{"hands_on_practices": [{"introduction": "The engine of a Gibbs sampler is its set of full conditional distributions. This foundational exercise guides you through the essential process of deriving these conditionals for a hierarchical Gaussian model, a structure ubiquitous in data assimilation and inverse problems. By working through both centered and noncentered parameterizations [@problem_id:3386530], you will gain direct experience with the algebraic manipulations required to implement a Gibbs sampler and appreciate how different model formulations impact the algorithm's structure.", "problem": "Consider a data assimilation setting with a network of $n$ sensors measuring a latent calibration offset field. Let the observed data be $y_{i} \\in \\mathbb{R}$ for $i=1,\\dots,n$, and consider the hierarchical random-effects inverse model\n- Observation (likelihood): $y_{i} \\mid x_{i} \\sim \\mathcal{N}(x_{i}, \\sigma^{2})$ independently for $i=1,\\dots,n$.\n- Random effects (state model): $x_{i} \\mid \\mu \\sim \\mathcal{N}(\\mu, \\tau^{2})$ independently for $i=1,\\dots,n$.\n- Hyperprior: $\\mu \\sim \\mathcal{N}(0, \\kappa^{2})$.\n\nAssume that $\\sigma^{2} > 0$, $\\tau^{2} > 0$, and $\\kappa^{2} > 0$ are all known. Two equivalent parameterizations are of interest for Gibbs sampling, a method within Markov chain Monte Carlo (MCMC):\n\n1) Centered parameterization: directly sample the latent effects $x_{1},\\dots,x_{n}$ and the hyperparameter $\\mu$.\n\n2) Noncentered parameterization: reparameterize as $x_{i} = \\mu + \\tau z_{i}$ with $z_{i} \\sim \\mathcal{N}(0,1)$ independently, and sample $z_{1},\\dots,z_{n}$ and $\\mu$.\n\nTasks:\n1) Starting from the joint density implied by the model and using only basic properties of the Gaussian distribution and conjugacy, derive the full conditional distributions for the centered parameterization: $x_{i} \\mid \\mu, y_{i}$ for each $i$, and $\\mu \\mid x_{1:n}, y_{1:n}$.\n\n2) Derive the full conditional distributions for the noncentered parameterization: $z_{i} \\mid \\mu, y_{i}$ for each $i$, and $\\mu \\mid z_{1:n}, y_{1:n}$.\n\n3) By analytically marginalizing the latent effects $x_{1:n}$, derive the observation model $y_{i} \\mid \\mu$ and use it to obtain the posterior distribution $\\mu \\mid y_{1:n}$. Provide your final answer as a single closed-form analytic expression for the posterior mean $\\mathbb{E}[\\mu \\mid y_{1:n}]$ in terms of $\\{y_{i}\\}_{i=1}^{n}$, $\\sigma^{2}$, $\\tau^{2}$, $\\kappa^{2}$, and $n$. Do not substitute numerical values; no rounding is required. The final answer must be a single analytic expression.", "solution": "The problem presents a standard Bayesian hierarchical model and asks for the derivation of several conditional and posterior distributions relevant to Gibbs sampling, along with the derivation of a marginal posterior mean. The model is scientifically grounded, well-posed, and all necessary information is provided. Therefore, the problem is valid and a solution can be derived.\n\nThe hierarchical model is defined as:\n- Observation model (likelihood): $y_{i} \\mid x_{i} \\sim \\mathcal{N}(x_{i}, \\sigma^{2})$ for $i=1,\\dots,n$.\n- State model: $x_{i} \\mid \\mu \\sim \\mathcal{N}(\\mu, \\tau^{2})$ for $i=1,\\dots,n$.\n- Hyperprior: $\\mu \\sim \\mathcal{N}(0, \\kappa^{2})$.\nThe variances $\\sigma^{2}$, $\\tau^{2}$, and $\\kappa^{2}$ are known constants.\n\nThe probability density function (PDF) of a normal distribution $\\mathcal{N}(m, s^2)$ is given by $p(x) = (2\\pi s^2)^{-1/2} \\exp(-\\frac{(x-m)^2}{2s^2})$. In derivations involving products of Gaussians, it is often sufficient to work with the terms in the exponent, as they uniquely determine the mean and variance of the resulting Gaussian distribution. A density $p(z)$ is Gaussian if $\\ln p(z)$ is a quadratic function of $z$. Specifically, if $p(z) \\propto \\exp(-\\frac{1}{2}(Az^2 - 2Bz))$, then $z$ is normally distributed with mean $B/A$ and variance $1/A$. This is equivalent to completing the square.\n\n**1) Full Conditionals for Centered Parameterization**\n\n**Derivation of $p(x_{i} \\mid \\mu, y_{i})$:**\nThe full conditional for $x_i$ is determined by its relationship with its Markov blanket, which in this model consists of $y_i$ and $\\mu$. Using Bayes' theorem, the conditional is proportional to the product of the likelihood and the prior for $x_i$:\n$$p(x_{i} \\mid \\mu, y_{i}) \\propto p(y_i, x_i, \\mu) \\propto p(y_{i} \\mid x_{i}) p(x_{i} \\mid \\mu)$$\nHere, we have $p(y_{i} \\mid x_{i}) \\propto \\exp\\left(-\\frac{(y_i - x_i)^2}{2\\sigma^2}\\right)$ and $p(x_{i} \\mid \\mu) \\propto \\exp\\left(-\\frac{(x_i - \\mu)^2}{2\\tau^2}\\right)$.\nThe product is proportional to the exponential of the sum of the arguments:\n$$p(x_{i} \\mid \\mu, y_{i}) \\propto \\exp\\left( -\\frac{(x_i - y_i)^2}{2\\sigma^2} - \\frac{(x_i - \\mu)^2}{2\\tau^2} \\right)$$\nTo find the parameters of the resulting Gaussian, we expand the quadratic terms in $x_i$ inside the exponent:\n$$-\\frac{1}{2\\sigma^2}(x_i^2 - 2x_iy_i + y_i^2) - \\frac{1}{2\\tau^2}(x_i^2 - 2x_i\\mu + \\mu^2)$$\nCollecting terms involving $x_i^2$ and $x_i$:\n$$-\\frac{1}{2} \\left[ x_i^2 \\left(\\frac{1}{\\sigma^2} + \\frac{1}{\\tau^2}\\right) - 2x_i \\left(\\frac{y_i}{\\sigma^2} + \\frac{\\mu}{\\tau^2}\\right) \\right] + C$$\nwhere $C$ contains terms not dependent on $x_i$. This is the exponent of a normal distribution for $x_i$. The precision (inverse variance) is the coefficient of $x_i^2/2$, and the mean is the coefficient of $x_i$ divided by the precision.\nThe precision is $\\frac{1}{\\sigma_{x_i}^2} = \\frac{1}{\\sigma^2} + \\frac{1}{\\tau^2} = \\frac{\\sigma^2+\\tau^2}{\\sigma^2\\tau^2}$. Thus, the variance is $\\sigma_{x_i}^2 = \\frac{\\sigma^2\\tau^2}{\\sigma^2+\\tau^2}$.\nThe mean $\\mu_{x_i}$ is given by $\\mu_{x_i} / \\sigma_{x_i}^2 = \\frac{y_i}{\\sigma^2} + \\frac{\\mu}{\\tau^2}$.\n$$\\mu_{x_i} = \\left(\\frac{\\sigma^2\\tau^2}{\\sigma^2+\\tau^2}\\right)\\left(\\frac{y_i}{\\sigma^2} + \\frac{\\mu}{\\tau^2}\\right) = \\frac{\\tau^2y_i + \\sigma^2\\mu}{\\sigma^2+\\tau^2}$$\nSo, the full conditional is $x_{i} \\mid \\mu, y_{i} \\sim \\mathcal{N}\\left(\\frac{y_i\\tau^2 + \\mu\\sigma^2}{\\sigma^2+\\tau^2}, \\frac{\\sigma^2\\tau^2}{\\sigma^2+\\tau^2}\\right)$.\n\n**Derivation of $p(\\mu \\mid x_{1:n}, y_{1:n})$:**\nGiven the latent effects $x_{1:n}$, $\\mu$ is conditionally independent of the observations $y_{1:n}$. Therefore, $p(\\mu \\mid x_{1:n}, y_{1:n}) = p(\\mu \\mid x_{1:n})$.\n$$p(\\mu \\mid x_{1:n}) \\propto p(x_{1:n} \\mid \\mu) p(\\mu) = \\left(\\prod_{i=1}^n p(x_i \\mid \\mu)\\right) p(\\mu)$$\nThe exponent is the sum of exponents from the individual PDFs:\n$$\\sum_{i=1}^n \\left(-\\frac{(x_i - \\mu)^2}{2\\tau^2}\\right) - \\frac{(\\mu - 0)^2}{2\\kappa^2} = -\\frac{1}{2\\tau^2}\\sum_{i=1}^n(x_i - \\mu)^2 - \\frac{\\mu^2}{2\\kappa^2}$$\nExpanding the terms quadratic in $\\mu$:\n$$-\\frac{1}{2\\tau^2}(\\sum x_i^2 - 2\\mu\\sum x_i + n\\mu^2) - \\frac{\\mu^2}{2\\kappa^2}$$\nCollecting terms involving $\\mu^2$ and $\\mu$:\n$$-\\frac{1}{2} \\left[ \\mu^2 \\left(\\frac{n}{\\tau^2} + \\frac{1}{\\kappa^2}\\right) - 2\\mu \\left(\\frac{\\sum_{i=1}^n x_i}{\\tau^2}\\right) \\right] + C'$$\nThe posterior precision for $\\mu$ is $\\frac{1}{\\sigma_{\\mu}^2} = \\frac{n}{\\tau^2} + \\frac{1}{\\kappa^2} = \\frac{n\\kappa^2+\\tau^2}{\\tau^2\\kappa^2}$. The variance is $\\sigma_{\\mu}^2 = \\frac{\\tau^2\\kappa^2}{n\\kappa^2+\\tau^2}$.\nThe posterior mean $\\mu_{\\mu}$ is given by $\\mu_{\\mu} / \\sigma_{\\mu}^2 = \\frac{\\sum x_i}{\\tau^2}$.\n$$\\mu_{\\mu} = \\left(\\frac{\\tau^2\\kappa^2}{n\\kappa^2+\\tau^2}\\right) \\left(\\frac{\\sum x_i}{\\tau^2}\\right) = \\frac{\\kappa^2 \\sum_{i=1}^n x_i}{n\\kappa^2+\\tau^2}$$\nSo, the full conditional is $\\mu \\mid x_{1:n} \\sim \\mathcal{N}\\left(\\frac{\\kappa^2 \\sum_{i=1}^n x_i}{n\\kappa^2+\\tau^2}, \\frac{\\tau^2\\kappa^2}{n\\kappa^2+\\tau^2}\\right)$.\n\n**2) Full Conditionals for Noncentered Parameterization**\n\nHere, we have $x_i = \\mu + \\tau z_i$ with $z_i \\sim \\mathcal{N}(0, 1)$. The likelihood becomes $y_i \\mid \\mu, z_i \\sim \\mathcal{N}(\\mu + \\tau z_i, \\sigma^2)$.\n\n**Derivation of $p(z_{i} \\mid \\mu, y_{i})$:**\nThe conditional for $z_i$ depends on $y_i$ and $\\mu$.\n$$p(z_i \\mid \\mu, y_i) \\propto p(y_i \\mid z_i, \\mu) p(z_i)$$\nThe exponent is $-\\frac{(y_i - (\\mu + \\tau z_i))^2}{2\\sigma^2} - \\frac{z_i^2}{2(1)^2}$. Expanding terms quadratic in $z_i$:\n$$-\\frac{1}{2\\sigma^2}((y_i-\\mu)^2 - 2\\tau z_i(y_i-\\mu) + \\tau^2 z_i^2) - \\frac{z_i^2}{2}$$\nCollecting terms involving $z_i^2$ and $z_i$:\n$$-\\frac{1}{2} \\left[ z_i^2 \\left(\\frac{\\tau^2}{\\sigma^2} + 1\\right) - 2z_i \\left(\\frac{\\tau(y_i-\\mu)}{\\sigma^2}\\right) \\right] + C''$$\nThe posterior precision for $z_i$ is $\\frac{1}{\\sigma_{z_i}^2} = \\frac{\\tau^2}{\\sigma^2} + 1 = \\frac{\\tau^2+\\sigma^2}{\\sigma^2}$. The variance is $\\sigma_{z_i}^2 = \\frac{\\sigma^2}{\\sigma^2+\\tau^2}$.\nThe posterior mean $\\mu_{z_i}$ is given by $\\mu_{z_i} / \\sigma_{z_i}^2 = \\frac{\\tau(y_i-\\mu)}{\\sigma^2}$.\n$$\\mu_{z_i} = \\left(\\frac{\\sigma^2}{\\sigma^2+\\tau^2}\\right) \\left(\\frac{\\tau(y_i-\\mu)}{\\sigma^2}\\right) = \\frac{\\tau(y_i - \\mu)}{\\sigma^2+\\tau^2}$$\nSo, $z_i \\mid \\mu, y_i \\sim \\mathcal{N}\\left(\\frac{\\tau(y_i - \\mu)}{\\sigma^2+\\tau^2}, \\frac{\\sigma^2}{\\sigma^2+\\tau^2}\\right)$.\n\n**Derivation of $p(\\mu \\mid z_{1:n}, y_{1:n})$:**\nThe conditional for $\\mu$ is found via $p(\\mu \\mid z_{1:n}, y_{1:n}) \\propto p(y_{1:n} \\mid \\mu, z_{1:n}) p(\\mu)$.\nThe likelihood term is $p(y_{1:n} \\mid \\mu, z_{1:n}) = \\prod_{i=1}^n p(y_i \\mid \\mu, z_i)$. Let's define $w_i = y_i - \\tau z_i$. Then $y_i - \\mu - \\tau z_i = w_i - \\mu$. The likelihood for a single observation is $p(y_i \\mid \\mu, z_i) \\propto \\exp(-\\frac{(w_i - \\mu)^2}{2\\sigma^2})$. This means we observe $w_i \\sim \\mathcal{N}(\\mu, \\sigma^2)$.\nThe inference problem is now to find the posterior for $\\mu$ given observations $w_{1:n}$ from a $\\mathcal{N}(\\mu, \\sigma^2)$ distribution and a $\\mathcal{N}(0, \\kappa^2)$ prior on $\\mu$. This is structurally identical to the derivation of $p(\\mu \\mid x_{1:n})$, but with $x_i$ replaced by $w_i$ and $\\tau^2$ replaced by $\\sigma^2$.\nFollowing the same logic, the posterior for $\\mu$ is a normal distribution with mean and variance:\n$$\\mu_{\\mu|z,y} = \\frac{\\kappa^2 \\sum_{i=1}^n w_i}{n\\kappa^2+\\sigma^2} = \\frac{\\kappa^2 \\sum_{i=1}^n (y_i - \\tau z_i)}{n\\kappa^2+\\sigma^2}$$\n$$\\sigma_{\\mu|z,y}^2 = \\frac{\\sigma^2\\kappa^2}{n\\kappa^2+\\sigma^2}$$\nThus, $\\mu \\mid z_{1:n}, y_{1:n} \\sim \\mathcal{N}\\left(\\frac{\\kappa^2 \\sum_{i=1}^n (y_i - \\tau z_i)}{n\\kappa^2+\\sigma^2}, \\frac{\\sigma^2\\kappa^2}{n\\kappa^2+\\sigma^2}\\right)$.\n\n**3) Marginal Posterior distribution for $\\mu$**\n\nFirst, we find the marginal distribution of $y_i$ given $\\mu$ by integrating out $x_i$:\n$$p(y_i \\mid \\mu) = \\int p(y_i \\mid x_i) p(x_i \\mid \\mu) dx_i$$\nThis is a convolution of two Gaussian distributions. Let $x_i = \\mu + \\delta_i$ where $\\delta_i \\sim \\mathcal{N}(0, \\tau^2)$, and $y_i = x_i + \\epsilon_i$ where $\\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$. Then $y_i = \\mu + \\delta_i + \\epsilon_i$.\nThe mean of $y_i$ given $\\mu$ is $\\mathbb{E}[y_i \\mid \\mu] = \\mathbb{E}[\\mu + \\delta_i + \\epsilon_i] = \\mu + 0 + 0 = \\mu$.\nThe variance of $y_i$ given $\\mu$, since $\\delta_i$ and $\\epsilon_i$ are independent, is $\\text{Var}(y_i \\mid \\mu) = \\text{Var}(\\delta_i) + \\text{Var}(\\epsilon_i) = \\tau^2 + \\sigma^2$.\nSo, the marginal observation model is $y_i \\mid \\mu \\sim \\mathcal{N}(\\mu, \\sigma^2 + \\tau^2)$.\n\nNow, we derive the posterior distribution $p(\\mu \\mid y_{1:n})$ using this marginal likelihood.\n$$p(\\mu \\mid y_{1:n}) \\propto p(y_{1:n} \\mid \\mu) p(\\mu) = \\left(\\prod_{i=1}^n p(y_i \\mid \\mu)\\right) p(\\mu)$$\nThe observations $y_i$ are independent given $\\mu$. We have a sample $y_{1:n}$ from $\\mathcal{N}(\\mu, \\sigma^2+\\tau^2)$ and a prior $\\mu \\sim \\mathcal{N}(0, \\kappa^2)$. This is again a standard conjugate-prior update. The problem is isomorphic to the derivation of $p(\\mu \\mid x_{1:n})$, with $x_i$ replaced by $y_i$ and $\\tau^2$ replaced by $\\sigma^2+\\tau^2$.\nThe posterior $p(\\mu \\mid y_{1:n})$ is a normal distribution. Its mean is $\\mathbb{E}[\\mu \\mid y_{1:n}]$. Using the formula derived in part 1:\n$$\\mathbb{E}[\\mu \\mid y_{1:n}] = \\frac{\\kappa^2 \\sum_{i=1}^n y_i}{n\\kappa^2 + (\\sigma^2 + \\tau^2)}$$\nThis is the final expression for the posterior mean. It can be rewritten as:\n$$\\mathbb{E}[\\mu \\mid y_{1:n}] = \\frac{n\\kappa^2}{n\\kappa^2 + \\sigma^2 + \\tau^2} \\left(\\frac{1}{n} \\sum_{i=1}^n y_i\\right) + \\frac{\\sigma^2 + \\tau^2}{n\\kappa^2 + \\sigma^2 + \\tau^2} (0)$$\nwhich shows it is a weighted average of the sample mean $\\bar{y}$ and the prior mean $0$.\n\nThe final answer is required as a single closed-form analytic expression for the posterior mean $\\mathbb{E}[\\mu \\mid y_{1:n}]$.", "answer": "$$\n\\boxed{\\frac{\\kappa^2 \\sum_{i=1}^{n} y_i}{n\\kappa^2 + \\sigma^2 + \\tau^2}}\n$$", "id": "3386530"}, {"introduction": "While elegant, the pure Gibbs sampler relies on the ability to sample directly from every full conditional, a condition that holds only for conjugate models. This practice problem confronts the common scenario where a full conditional is intractable, introducing the indispensable Metropolis-within-Gibbs technique [@problem_id:3386603]. By deriving the acceptance probability from the principle of detailed balance, you will learn how to construct a valid hybrid MCMC algorithm, greatly expanding the range of models you can tackle.", "problem": "Consider a Bayesian inverse problem in data assimilation, where an unknown state vector $x \\in \\mathbb{R}^{d}$ is inferred from observations $y \\in \\mathbb{R}^{m}$. Let the prior density be $p(x)$ and the likelihood be $L(y \\mid x)$, both strictly positive on their supports. The posterior density is defined by Bayes' theorem as $ \\pi(x \\mid y) \\propto L(y \\mid x)\\,p(x) $. You are tasked with sampling from $ \\pi(x \\mid y) $ via a component-wise Markov Chain Monte Carlo (MCMC) method.\n\nSuppose you intend to perform a Gibbs update of coordinate $x_{i}$ given $x_{-i}$ (the vector $x$ with the $i$-th component removed), but the full conditional density $ \\pi(x_{i} \\mid x_{-i}, y) $ is not available in a tractable form. Instead, you design a Metropolis-within-Gibbs (MwG) step that proposes a new value $x_{i}'$ from a proposal density $ q(x_{i}' \\mid x_{-i}) $ supported wherever $ \\pi(x \\mid y) $ is positive, and leaves $x_{-i}$ unchanged.\n\nUsing only the core definition of the Metropolis-Hastings (MH) method as a Markov kernel that enforces detailed balance with respect to the target distribution and the definition of the posterior $ \\pi(x \\mid y) \\propto L(y \\mid x)\\,p(x) $, derive the acceptance probability $ \\alpha_{i}(x_{i} \\to x_{i}' ; x_{-i}) $ for this MwG move. Your derivation must make explicit use of the detailed balance condition for the one-coordinate proposal that keeps $x_{-i}$ fixed. Express your final answer as a single closed-form analytic expression in terms of $ L(y \\mid x) $, $ p(x) $, and $ q(x_{i}' \\mid x_{-i}) $.\n\nProvide your final expression for the acceptance probability. The final answer must be a single closed-form analytic expression. Do not provide steps in the final answer.", "solution": "The problem requires the derivation of the acceptance probability for a Metropolis-within-Gibbs (MwG) step. The derivation must be based on the principle of detailed balance, which is the cornerstone of the Metropolis-Hastings (MH) algorithm.\n\nLet the target distribution be the posterior density $\\pi(x \\mid y)$, where $x \\in \\mathbb{R}^{d}$ is the state vector and $y \\in \\mathbb{R}^{m}$ are the observations. For any two states $x$ and $x'$, a Markov chain transition kernel $K(x' \\mid x)$ satisfies the detailed balance condition with respect to $\\pi(x \\mid y)$ if:\n$$ \\pi(x \\mid y) K(x' \\mid x) = \\pi(x' \\mid y) K(x \\mid x') $$\nThe MH algorithm constructs such a kernel. The transition from a state $x$ to a new state $x'$ is a two-step process:\n$1$. A candidate state $x'$ is generated from a proposal distribution $Q(x' \\mid x)$.\n$2$. The candidate state $x'$ is accepted with a probability $\\alpha(x \\to x')$. If rejected, the chain remains at state $x$.\n\nFor a transition where $x' \\neq x$, the kernel is given by $K(x' \\mid x) = Q(x' \\mid x) \\alpha(x \\to x')$. Substituting this into the detailed balance equation yields:\n$$ \\pi(x \\mid y) Q(x' \\mid x) \\alpha(x \\to x') = \\pi(x' \\mid y) Q(x \\mid x') \\alpha(x' \\to x) $$\nThe standard choice for the acceptance probability that satisfies this relationship is the Metropolis-Hastings acceptance probability:\n$$ \\alpha(x \\to x') = \\min \\left( 1, \\frac{\\pi(x' \\mid y) Q(x \\mid x')}{\\pi(x \\mid y) Q(x' \\mid x)} \\right) $$\n\nIn the context of the given problem, we are not updating the entire vector $x$ at once, but rather performing a component-wise update for a single coordinate $x_i$. This is a specific instance of the general MH framework. Let the current state be $x = (x_i, x_{-i})$, where $x_{-i}$ is the vector of all components except $x_i$. The proposed move only changes the $i$-th component. A new value $x_i'$ is proposed, leading to a candidate state $x' = (x_i', x_{-i})$. The other components remain fixed, so $x'_{-i} = x_{-i}$.\n\nThe proposal for the new component $x_i'$ is drawn from a density specified as $q(x_i' \\mid x_{-i})$. This defines the proposal distribution for the *full state vector*, which we denote as $Q_i(x' \\mid x)$ for this $i$-th component update step. Since the move from $x$ proposes a state $x'$ by stochastically choosing $x_i'$ while deterministically keeping $x_{-i}$ fixed, the forward proposal probability density is:\n$$ Q_i(x' \\mid x) = q(x_i' \\mid x_{-i}) $$\nIt is implicit that this proposal is for $x'=(x_i', x_{-i})$ given $x=(x_i, x_{-i})$.\n\nTo compute the acceptance probability, we also need the reverse proposal probability density, $Q_i(x \\mid x')$. This is the probability of proposing the original state $x$ starting from the new state $x'$. The reverse move is from $x' = (x_i', x_{-i})$ to $x = (x_i, x_{-i})$. According to the problem's definition, the proposal for a component's new value (in this case, $x_i$) depends on the other components of the state from which the proposal is made (in this case, $x_{-i}'$). Since $x_{-i}' = x_{-i}$, the reverse proposal probability density is:\n$$ Q_i(x \\mid x') = q(x_i \\mid x_{-i}' ) = q(x_i \\mid x_{-i}) $$\nThis shows that the proposal for the value of the $i$-th component depends on the values of the other components, but not on the current value of the $i$-th component itself.\n\nWe can now substitute these specific forms of the proposal distributions into the general MH acceptance formula. We will use the notation $\\alpha_i(x_i \\to x_i' ; x_{-i})$ for this component-specific acceptance probability.\n$$ \\alpha_{i}(x_{i} \\to x_{i}' ; x_{-i}) = \\min \\left( 1, \\frac{\\pi(x' \\mid y) Q_i(x \\mid x')}{\\pi(x \\mid y) Q_i(x' \\mid x)} \\right) = \\min \\left( 1, \\frac{\\pi(x' \\mid y) \\, q(x_i \\mid x_{-i})}{\\pi(x \\mid y) \\, q(x_i' \\mid x_{-i})} \\right) $$\n\nThe problem states that the posterior density is given by Bayes' theorem, $\\pi(x \\mid y) \\propto L(y \\mid x) p(x)$. The constant of proportionality is independent of $x$ and will cancel in the ratio. Therefore, we can replace $\\pi(z \\mid y)$ with the product $L(y \\mid z) p(z)$ for any state $z$:\n$$ \\alpha_{i}(x_{i} \\to x_{i}' ; x_{-i}) = \\min \\left( 1, \\frac{L(y \\mid x')p(x') \\, q(x_i \\mid x_{-i})}{L(y \\mid x)p(x) \\, q(x_i' \\mid x_{-i})} \\right) $$\n\nTo provide the final expression in a fully explicit form, we write the arguments of the likelihood $L$ and prior $p$ in terms of the components, recalling that $x = (x_i, x_{-i})$ and $x' = (x_i', x_{-i})$:\n$$ \\alpha_{i}(x_{i} \\to x_{i}' ; x_{-i}) = \\min \\left( 1, \\frac{L(y \\mid x_{i}', x_{-i}) p(x_{i}', x_{-i}) \\, q(x_{i} \\mid x_{-i})}{L(y \\mid x_{i}, x_{-i}) p(x_{i}, x_{-i}) \\, q(x_{i}' \\mid x_{-i})} \\right) $$\nThis is the final closed-form expression for the acceptance probability of the specified Metropolis-within-Gibbs step, derived directly from the detailed balance condition.", "answer": "$$\n\\boxed{\\min \\left( 1, \\frac{L(y \\mid x_{i}', x_{-i}) p(x_{i}', x_{-i}) \\, q(x_{i} \\mid x_{-i})}{L(y \\mid x_{i}, x_{-i}) p(x_{i}, x_{-i}) \\, q(x_{i}' \\mid x_{-i})} \\right)}\n$$", "id": "3386603"}, {"introduction": "Building a sampler is only half the battle; understanding its efficiency is crucial for reliable inference, especially in large-scale applications. This advanced exercise moves from implementation to analysis, exploring the convergence rate of a Gibbs sampler through its spectral gap [@problem_id:3386525]. By deriving an exact formula for the convergence rate in a Gaussian model, you will develop a theoretical understanding of how algorithmic choices, such as the scanning strategy, and posterior dependencies influence sampler performance.", "problem": "Consider a linear hierarchical Gaussian inverse problem in data assimilation. Let $x \\in \\mathbb{R}^{n}$ denote the unknown state, $z \\in \\mathbb{R}^{n}$ a latent mean field, and $d \\in \\mathbb{R}^{m}$ the observed data. The statistical model is\n$$\nd \\mid x \\sim \\mathcal{N}(A x, \\Gamma_{\\varepsilon}), \\quad x \\mid z \\sim \\mathcal{N}(z, \\Gamma_{x}), \\quad z \\sim \\mathcal{N}(0, \\Gamma_{z}),\n$$\nwhere $A \\in \\mathbb{R}^{m \\times n}$ is the forward operator, and $\\Gamma_{\\varepsilon} \\in \\mathbb{R}^{m \\times m}$, $\\Gamma_{x} \\in \\mathbb{R}^{n \\times n}$, $\\Gamma_{z} \\in \\mathbb{R}^{n \\times n}$ are symmetric positive definite covariance matrices. Conditioning on $d$, the joint posterior for $(x,z)$ is Gaussian with a symmetric positive definite posterior precision matrix\n$$\nJ \\;=\\; \\begin{pmatrix}\nJ_{11}  J_{12} \\\\\nJ_{21}  J_{22}\n\\end{pmatrix}\n\\quad \\text{with} \\quad\nJ_{11} = A^{\\top}\\Gamma_{\\varepsilon}^{-1}A + \\Gamma_{x}^{-1}, \\;\nJ_{12} = -\\Gamma_{x}^{-1}, \\;\nJ_{21} = -\\Gamma_{x}^{-1}, \\;\nJ_{22} = \\Gamma_{x}^{-1} + \\Gamma_{z}^{-1}.\n$$\nIn block Gibbs sampling, one iteratively updates $x$ from its conditional $\\mathcal{N}(\\mu_{x \\mid z}, J_{11}^{-1})$ and $z$ from its conditional $\\mathcal{N}(\\mu_{z \\mid x}, J_{22}^{-1})$, where the conditional means are linear functions of the other block. Define the two scan types: deterministic scan (systematic) alternates $x$ then $z$ every sweep, while random scan updates $x$ with probability $p \\in (0,1)$ and $z$ with probability $1-p$ at each step.\n\nStarting from the foundational definitions of multivariate Gaussian conditionals and the spectral gap of a Markov chain (defined as $1$ minus the spectral radius of its one-step Markov operator restricted to mean-zero functions), derive closed-form expressions for the exact one-step spectral gaps of the deterministic-scan and random-scan two-block Gibbs samplers for this posterior. Express your answers in terms of the largest eigenvalue\n$$\n\\mu_{\\max} \\;=\\; \\lambda_{\\max}\\!\\left( J_{22}^{-1/2} \\, J_{21} \\, J_{11}^{-1} \\, J_{12} \\, J_{22}^{-1/2} \\right),\n$$\nwhich is a symmetric positive semidefinite matrix built from the conditional precision blocks, and the random-scan block-update probability $p$. Provide the final expressions for both spectral gaps as a single row matrix. Then, succinctly discuss the practical implications of your formulas for algorithmic design in inverse problems and data assimilation, including the choice of scan type and the effect of coupling strength between $x$ and $z$. The final answer must be an exact analytic expression; do not approximate or round.", "solution": "The problem asks for the derivation of the one-step spectral gaps for both deterministic-scan and random-scan two-block Gibbs samplers for a specific linear hierarchical Gaussian posterior distribution. The spectral gap, $\\gamma$, of a Markov chain is defined as $1 - \\rho$, where $\\rho$ is the spectral radius of the one-step Markov operator restricted to the space of mean-zero functions. A larger spectral gap implies faster convergence to the stationary distribution.\n\nThe state of the sampler is the pair of variables $(x,z)$. The target posterior distribution $p(x,z|d)$ is a multivariate Gaussian with a precision matrix $J$ given in block form:\n$$ J \\;=\\; \\begin{pmatrix} J_{11}  J_{12} \\\\ J_{21}  J_{22} \\end{pmatrix} $$\nFor a Gaussian target, the convergence analysis of the Gibbs sampler simplifies considerably. The spectral radius of the transition operator is determined by the block components of the precision matrix.\n\nLet $e_x = x - \\bar{x}$ and $e_z = z - \\bar{z}$ be the deviations from the posterior mean $(\\bar{x}, \\bar{z})$. The conditional distributions for the Gibbs updates are:\n$x \\mid z,d \\sim \\mathcal{N}(\\bar{x} - J_{11}^{-1}J_{12}(z-\\bar{z}), J_{11}^{-1})$\n$z \\mid x,d \\sim \\mathcal{N}(\\bar{z} - J_{22}^{-1}J_{21}(x-\\bar{x}), J_{22}^{-1})$\nThe evolution of the expected error is key to determining the convergence rate.\n\n**1. Deterministic-Scan Gibbs Sampler**\nIn a deterministic (or systematic) scan, the variables are updated in a fixed order. Let's assume the order is $x$ then $z$. Let the state at sweep $k$ be $(x_k, z_k)$.\nFirst, $x_{k+1/2}$ is drawn conditional on $z_k$. The expectation of its error $e_x^{(k+1/2)} = x_{k+1/2} - \\bar{x}$ conditional on $e_z^{(k)}$ is:\n$$ E[e_x^{(k+1/2)} \\mid e_z^{(k)}] = -J_{11}^{-1}J_{12}e_z^{(k)} $$\nNext, $z_{k+1}$ is drawn conditional on $x_{k+1/2}$. The expectation of its error $e_z^{(k+1)}$ conditional on $e_x^{(k+1/2)}$ is:\n$$ E[e_z^{(k+1)} \\mid e_x^{(k+1/2)}] = -J_{22}^{-1}J_{21}e_x^{(k+1/2)} $$\nCombining these using the law of total expectation:\n$$ E[e_z^{(k+1)} \\mid e_z^{(k)}] = E[E[e_z^{(k+1)} \\mid e_x^{(k+1/2)}]\\mid e_z^{(k)}] = -J_{22}^{-1}J_{21} E[e_x^{(k+1/2)} \\mid e_z^{(k)}] = (-J_{22}^{-1}J_{21})(-J_{11}^{-1}J_{12})e_z^{(k)} $$\nThe mean error in the $z$ component is propagated by the matrix $M_{det} = J_{22}^{-1}J_{21}J_{11}^{-1}J_{12}$. The spectral radius of the deterministic-scan Gibbs sampler, $\\rho_{det}$, is the spectral radius of this matrix, $\\rho(M_{det})$.\n\nWe are given $\\mu_{\\max} = \\lambda_{\\max}(J_{22}^{-1/2} J_{21} J_{11}^{-1} J_{12} J_{22}^{-1/2})$. Let $M_s = J_{22}^{-1/2} J_{21} J_{11}^{-1} J_{12} J_{22}^{-1/2}$. The eigenvalues of $M_{det}$ and $M_s$ are identical. This can be seen as $M_s$ is a similarity transformation of $M_{det}$ if $J_{22}$ is invertible (which it is, being a sum of two positive definite matrices): $M_s = J_{22}^{1/2} M_{det} J_{22}^{-1/2}$. Thus, $\\rho(M_{det}) = \\rho(M_s)$.\n\nThe problem specifies that $J_{21} = J_{12} = -\\Gamma_x^{-1}$, where $\\Gamma_x$ is symmetric. Thus $J_{21} = J_{12}^\\top$. The matrix $M_s = J_{22}^{-1/2} J_{12}^{\\top} J_{11}^{-1} J_{12} J_{22}^{-1/2}$ is symmetric and positive semidefinite. Its eigenvalues are therefore real and non-negative. Its spectral radius is simply its largest eigenvalue, which is given as $\\mu_{\\max}$.\nTherefore, the spectral radius of the deterministic-scan sampler is $\\rho_{det} = \\mu_{\\max}$.\nThe spectral gap for the deterministic scan, $\\gamma_{det}$, is:\n$$ \\gamma_{det} = 1 - \\rho_{det} = 1 - \\mu_{\\max} $$\n\n**2. Random-Scan Gibbs Sampler**\nIn a random scan, at each step, we update $x$ with probability $p$ and $z$ with probability $1-p$. The one-step transition operator, acting on functions in $L^2(p(x,z|d))$, is $T_{rand} = p T_x + (1-p) T_z$, where $T_x = E[\\cdot|z,d]$ and $T_z = E[\\cdot|x,d]$ are the conditional expectation operators. These operators are self-adjoint projectors on the Hilbert space $L^2$. Thus, $T_{rand}$ is a self-adjoint operator, and its eigenvalues are real.\n\nThe eigenvalues of the operator $T_{rand}$ restricted to the space of mean-zero functions are given by a standard result for the sum of two projection operators. They are the roots of the quadratic equations\n$$ \\lambda^2 - \\lambda + p(1-p)(1-\\nu_k) = 0 $$\nfor each non-zero eigenvalue $\\nu_k$ of the operator product $T_x T_z$. In the Gaussian case, the eigenvalues $\\{\\nu_k\\}$ are precisely the eigenvalues of the matrix $M_{det}$ from the deterministic-scan analysis, which are also the eigenvalues of $M_s$. So, $\\nu_k \\in \\operatorname{spec}(M_s)$.\n\nThe roots of the quadratic equation for a given $\\nu$ are:\n$$ \\lambda(\\nu) = \\frac{1 \\pm \\sqrt{1 - 4p(1-p)(1-\\nu)}}{2} $$\nSince the operator is self-adjoint, its eigenvalues $\\lambda(\\nu)$ must be real. This implies the discriminant must be non-negative: $1 - 4p(1-p)(1-\\nu) \\ge 0$. As $p \\in (0,1)$, $4p(1-p) \\le 1$. Since we know from the analysis of $J$ that $0 \\le \\nu  1$, it follows that $0  1-\\nu \\le 1$. Thus $4p(1-p)(1-\\nu) \\le 1$, and the reality of the eigenvalues is guaranteed.\n\nLet $\\lambda_+(\\nu) = \\frac{1 + \\sqrt{1 - 4p(1-p)(1-\\nu)}}{2}$ and $\\lambda_-(\\nu) = \\frac{1 - \\sqrt{1 - 4p(1-p)(1-\\nu)}}{2}$. The spectral radius of the random-scan sampler, $\\rho_{rand}$, is the largest of all such eigenvalues over all $\\nu_k \\in \\operatorname{spec}(M_s)$. Since $\\lambda_+(\\nu) \\ge \\lambda_-(\\nu)$ for all relevant $\\nu$, we need to find the maximum of $\\lambda_+(\\nu)$:\n$$ \\rho_{rand} = \\sup_{\\nu \\in \\operatorname{spec}(M_s)} \\lambda_+(\\nu) $$\nTo find this maximum, we examine the derivative of $\\lambda_+(\\nu)$ with respect to $\\nu$:\n$$ \\frac{d\\lambda_+(\\nu)}{d\\nu} = \\frac{d}{d\\nu}\\left(\\frac{1 + \\sqrt{1 - 4p(1-p) + 4p(1-p)\\nu}}{2}\\right) = \\frac{1}{2} \\cdot \\frac{1}{2\\sqrt{\\dots}} \\cdot (4p(1-p)) = \\frac{p(1-p)}{\\sqrt{1-4p(1-p)(1-\\nu)}}  0 $$\nSince the derivative is positive, $\\lambda_+(\\nu)$ is an increasing function of $\\nu$. Therefore, its maximum value is achieved when $\\nu$ is at its maximum. The maximum eigenvalue of $M_s$ is given as $\\mu_{\\max}$.\nThus, the spectral radius of the random-scan sampler is:\n$$ \\rho_{rand} = \\lambda_+(\\mu_{\\max}) = \\frac{1 + \\sqrt{1-4p(1-p)(1-\\mu_{\\max})}}{2} $$\nThe spectral gap for the random scan, $\\gamma_{rand}$, is:\n$$ \\gamma_{rand} = 1 - \\rho_{rand} = 1 - \\frac{1 + \\sqrt{1-4p(1-p)(1-\\mu_{\\max})}}{2} = \\frac{1 - \\sqrt{1-4p(1-p)(1-\\mu_{\\max})}}{2} $$\n\n**3. Practical Implications**\nThe derived formulas for the spectral gaps have significant practical implications for the design and analysis of algorithms in inverse problems and data assimilation.\n- **Choice of Scan Type**: For $p=1/2$, we can compare $\\gamma_{det} = 1-\\mu_{\\max}$ with $\\gamma_{rand} = \\frac{1-\\sqrt{\\mu_{\\max}}}{2}$. Since $0 \\le \\mu_{\\max}  1$, let $\\sqrt{\\mu_{\\max}}=y$, where $0 \\le y  1$. We compare $1-y^2$ with $(1-y)/2$. As $1-y^2 = (1-y)(1+y)$ and $1+y \\ge 1  1/2$, it follows that $(1-y)(1+y)  (1-y)/2$, so $\\gamma_{det}  \\gamma_{rand}$. This shows that for two blocks, the deterministic scan converges faster than the random scan, a widely observed empirical result now confirmed analytically. Deterministic scanning should be preferred when feasible.\n- **Effect of Coupling Strength**: The quantity $\\mu_{\\max}$ measures the coupling between the blocks $(x,z)$. As $\\mu_{\\max} \\to 1$ (strong coupling), both $\\gamma_{det}$ and $\\gamma_{rand}$ approach $0$. This indicates that the convergence of both samplers degrades severely as the posterior correlation between variables increases. This highlights the fundamental challenge of MCMC methods in high-dimensional, strongly coupled systems. Algorithmic design should focus on finding reparameterizations or blocking strategies that reduce this coupling (i.e., make $\\mu_{\\max}$ smaller).\n- **Optimal Random Scan**: For the random-scan sampler, the convergence rate depends on the block update probability $p$. The gap $\\gamma_{rand}$ is maximized when $\\rho_{rand}$ is minimized. Since $\\rho_{rand}$ is a decreasing function of $4p(1-p)$, we should choose $p$ to maximize this term. The function $p(1-p)$ is maximized at $p=1/2$. Therefore, the optimal random-scan strategy is to update each block with equal probability, which equalizes the computational effort spent on each part of the state.\n\nIn summary, these closed-form expressions provide a quantitative tool for understanding sampler performance, guiding the choice between scanning strategies, and motivating the development of better-mixing algorithms through variance reduction and reparameterization techniques aimed at decreasing $\\mu_{\\max}$.\n\nFinal expressions for the spectral gaps are:\nDeterministic scan: $\\gamma_{det} = 1 - \\mu_{\\max}$\nRandom scan: $\\gamma_{rand} = \\frac{1 - \\sqrt{1-4p(1-p)(1-\\mu_{\\max})}}{2}$", "answer": "$$ \\boxed{ \\begin{pmatrix} 1 - \\mu_{\\max}  \\frac{1 - \\sqrt{1 - 4p(1-p)(1 - \\mu_{\\max})}}{2} \\end{pmatrix} } $$", "id": "3386525"}]}