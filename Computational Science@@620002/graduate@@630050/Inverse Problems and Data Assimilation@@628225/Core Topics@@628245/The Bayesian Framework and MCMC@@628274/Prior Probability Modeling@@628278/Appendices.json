{"hands_on_practices": [{"introduction": "Choosing a prior distribution is a cornerstone of Bayesian inference, but how do we proceed when our prior knowledge is limited to certain statistical properties? The principle of maximum entropy (MaxEnt) offers a rigorous and objective method to construct the least informative prior that is consistent with known constraints. This practice [@problem_id:3414194] guides you through a foundational derivation, demonstrating how the ubiquitous Laplace distribution, which is instrumental for promoting sparsity in inverse problem solutions, naturally emerges from constraining the expected $\\ell_1$ norm under an independence assumption.", "problem": "Consider an underdetermined linear inverse problem in which the unknown parameter vector is $u \\in \\mathbb{R}^{n}$. You need to model a prior distribution for $u$ in the context of inverse problems and data assimilation. Assume the following modeling requirements:\n- Independence across coordinates: the joint prior density factorizes as $\\pi(u) = \\prod_{i=1}^{n} p_{i}(u_{i})$.\n- Exchangeability of coordinates: each coordinate is modeled with the same marginal $p_{i} = p$.\n- The only available global constraint is the expected $\\ell_1$ norm, $\\mathbb{E}\\!\\left[\\|u\\|_{1}\\right] = c$ for a given $c > 0$, where $\\|u\\|_{1} = \\sum_{i=1}^{n} |u_{i}|$ and the expectation is taken with respect to the prior.\n\nUsing the Maximum Entropy (MaxEnt) principle with Shannon differential entropy, and starting from first principles of entropy maximization under moment constraints, derive the form of the entropy-maximizing independent prior over $\\mathbb{R}^{n}$ subject to the constraint $\\mathbb{E}\\!\\left[\\|u\\|_{1}\\right] = c$. In particular:\n1. Show that the MaxEnt marginal $p$ must depend on $|x|$ only and that the corresponding joint prior is a product of identical marginals.\n2. Derive the explicit functional form of the marginal density $p$ obtained by maximizing entropy subject to normalization and $\\mathbb{E}\\!\\left[|U|\\right] = c/n$, where $U \\sim p$ is a single coordinate.\n3. Compute the scale parameter of this marginal in terms of $c$ and $n$.\n\nYour final answer must be a single closed-form analytic expression for the scale parameter of the marginal prior, expressed in terms of $c$ and $n$. No rounding is required. Do not include any units in your final answer.", "solution": "The problem is to derive the form of a prior distribution $\\pi(u)$ for a vector $u \\in \\mathbb{R}^{n}$ using the Maximum Entropy (MaxEnt) principle, subject to a set of specified constraints. The solution will be constructed by following the logical steps of the MaxEnt procedure.\n\nFirst, we formalize the problem. We want to maximize the Shannon differential entropy of the joint prior distribution $\\pi(u)$, given by:\n$$H[\\pi] = -\\int_{\\mathbb{R}^{n}} \\pi(u) \\ln \\pi(u) \\, du$$\nThe problem states two modeling requirements for $\\pi(u)$:\n1.  Independence across coordinates: The joint density factorizes into a product of marginal densities, $\\pi(u) = \\prod_{i=1}^{n} p_{i}(u_{i})$.\n2.  Exchangeability of coordinates: The marginal densities are identical, $p_{i} = p$ for all $i=1, \\dots, n$.\n\nUnder these two requirements, the joint prior is $\\pi(u) = \\prod_{i=1}^{n} p(u_{i})$. We can simplify the expression for the joint entropy:\n$$H[\\pi] = -\\int_{\\mathbb{R}^{n}} \\left(\\prod_{j=1}^{n} p(u_{j})\\right) \\ln\\left(\\prod_{i=1}^{n} p(u_{i})\\right) \\, du$$\n$$H[\\pi] = -\\int_{\\mathbb{R}^{n}} \\left(\\prod_{j=1}^{n} p(u_j)\\right) \\left(\\sum_{i=1}^{n} \\ln p(u_i)\\right) \\, du$$\n$$H[\\pi] = -\\sum_{i=1}^{n} \\int_{\\mathbb{R}^{n}} \\ln p(u_i) \\left(\\prod_{j=1}^{n} p(u_j)\\right) \\, du_1 \\dots du_n$$\nFor each term $i$ in the sum, we can separate the integrals. The integrals over $u_j$ for $j \\neq i$ are $\\int_{\\mathbb{R}} p(u_j) \\, du_j$, which equals $1$ due to the normalization of the marginal probability density $p$. This leaves:\n$$H[\\pi] = -\\sum_{i=1}^{n} \\int_{\\mathbb{R}} p(u_i) \\ln p(u_i) \\, du_i$$\nEach term in the sum is the entropy of the marginal distribution $p$, denoted $H[p]$. Since there are $n$ identical terms, the total entropy is:\n$$H[\\pi] = n H[p] = -n \\int_{\\mathbb{R}} p(x) \\ln p(x) \\, dx$$\nMaximizing the joint entropy $H[\\pi]$ is therefore equivalent to maximizing the marginal entropy $H[p]$.\n\nNext, we process the global constraint on the expected $\\ell_1$ norm:\n$$\\mathbb{E}_{\\pi}\\!\\left[\\|u\\|_{1}\\right] = c$$\nwhere $\\|u\\|_{1} = \\sum_{i=1}^{n} |u_{i}|$. Using the linearity of expectation:\n$$\\mathbb{E}_{\\pi}\\!\\left[\\sum_{i=1}^{n} |u_{i}|\\right] = \\sum_{i=1}^{n} \\mathbb{E}_{\\pi}[|u_{i}|] = c$$\nThe expectation of $|u_i|$ with respect to the joint distribution $\\pi(u)$ is:\n$$\\mathbb{E}_{\\pi}[|u_{i}|] = \\int_{\\mathbb{R}^{n}} |u_i| \\pi(u) \\, du = \\int_{\\mathbb{R}^{n}} |u_i| \\prod_{j=1}^{n} p(u_j) \\, du_1 \\dots du_n$$\nAgain, integrating over all $u_j$ for $j \\neq i$ yields $1$, leaving:\n$$\\mathbb{E}_{\\pi}[|u_{i}|] = \\int_{\\mathbb{R}} |u_i| p(u_i) \\, du_i$$\nThis is the expectation of $|X|$ where $X$ is a random variable with distribution $p$. Let's denote this $\\mathbb{E}_{p}[|X|]$. Since the marginals are identical, this expectation is the same for all $i$. The constraint becomes:\n$$\\sum_{i=1}^{n} \\mathbb{E}_{p}[|X|] = n \\mathbb{E}_{p}[|X|] = c$$\nThis yields a constraint on the marginal density $p$:\n$$\\mathbb{E}_{p}[|X|] = \\int_{-\\infty}^{\\infty} |x| p(x) \\, dx = \\frac{c}{n}$$\n\nThe problem is now reduced to finding the marginal density $p(x)$ that maximizes $H[p]$ subject to two constraints:\n1.  Normalization: $\\int_{-\\infty}^{\\infty} p(x) \\, dx = 1$\n2.  Moment constraint: $\\int_{-\\infty}^{\\infty} |x| p(x) \\, dx = \\frac{c}{n}$\n\nWe solve this using the calculus of variations. We define the functional to be maximized, incorporating the constraints via Lagrange multipliers $\\lambda_0$ and $\\lambda_1$:\n$$L[p] = -\\int_{-\\infty}^{\\infty} p(x) \\ln p(x) \\, dx - (\\lambda_0-1) \\left(\\int_{-\\infty}^{\\infty} p(x) \\, dx - 1\\right) - \\lambda_1 \\left(\\int_{-\\infty}^{\\infty} |x| p(x) \\, dx - \\frac{c}{n}\\right)$$\nTaking the functional derivative of $L[p]$ with respect to $p(x)$ and setting it to zero gives:\n$$\\frac{\\delta L}{\\delta p(x)} = -\\ln p(x) - 1 - (\\lambda_0-1) - \\lambda_1 |x| = 0$$\n$$\\ln p(x) = -\\lambda_0 - \\lambda_1 |x|$$\n$$p(x) = \\exp(-\\lambda_0) \\exp(-\\lambda_1 |x|)$$\nThis form explicitly shows that $p(x)$ depends only on $|x|$, which addresses the first part of task 1. The factorization of the joint prior was an initial assumption, confirmed here to be consistent with the MaxEnt solution structure.\n\nNow, we determine the Lagrange multipliers. Let $Z = \\exp(\\lambda_0)$. The density is $p(x) = \\frac{1}{Z} \\exp(-\\lambda_1 |x|)$.\n\nUsing the normalization constraint:\n$$\\int_{-\\infty}^{\\infty} \\frac{1}{Z} \\exp(-\\lambda_1 |x|) \\, dx = 1$$\nThe integrand is an even function, so we can write:\n$$\\frac{2}{Z} \\int_{0}^{\\infty} \\exp(-\\lambda_1 x) \\, dx = 1$$\nFor the integral to converge, we require $\\lambda_1 > 0$.\n$$\\frac{2}{Z} \\left[-\\frac{1}{\\lambda_1} \\exp(-\\lambda_1 x)\\right]_{0}^{\\infty} = \\frac{2}{Z} \\left(0 - \\left(-\\frac{1}{\\lambda_1}\\right)\\right) = \\frac{2}{Z \\lambda_1} = 1$$\nThis gives $Z = \\frac{2}{\\lambda_1}$. Substituting this back into the expression for $p(x)$ gives its explicit functional form, which is the Laplace distribution:\n$$p(x) = \\frac{\\lambda_1}{2} \\exp(-\\lambda_1 |x|)$$\nThis completes task 2.\n\nUsing the moment constraint:\n$$\\int_{-\\infty}^{\\infty} |x| p(x) \\, dx = \\frac{c}{n}$$\n$$\\int_{-\\infty}^{\\infty} |x| \\frac{\\lambda_1}{2} \\exp(-\\lambda_1 |x|) \\, dx = \\lambda_1 \\int_{0}^{\\infty} x \\exp(-\\lambda_1 x) \\, dx = \\frac{c}{n}$$\nThe integral on the left can be solved using integration by parts. Let $u=x$ and $dv=\\exp(-\\lambda_1 x) dx$. Then $du=dx$ and $v = -\\frac{1}{\\lambda_1}\\exp(-\\lambda_1 x)$.\n$$\\int_{0}^{\\infty} x \\exp(-\\lambda_1 x) \\, dx = \\left[-\\frac{x}{\\lambda_1} \\exp(-\\lambda_1 x)\\right]_{0}^{\\infty} - \\int_{0}^{\\infty} \\left(-\\frac{1}{\\lambda_1}\\right) \\exp(-\\lambda_1 x) \\, dx$$\n$$= (0 - 0) + \\frac{1}{\\lambda_1} \\int_{0}^{\\infty} \\exp(-\\lambda_1 x) \\, dx = \\frac{1}{\\lambda_1} \\left[-\\frac{1}{\\lambda_1} \\exp(-\\lambda_1 x)\\right]_{0}^{\\infty} = \\frac{1}{\\lambda_1^2}$$\nSubstituting this result into the moment constraint equation:\n$$\\lambda_1 \\left(\\frac{1}{\\lambda_1^2}\\right) = \\frac{1}{\\lambda_1} = \\frac{c}{n}$$\nThis implies $\\lambda_1 = \\frac{n}{c}$. Since $n>0$ and $c>0$, the condition $\\lambda_1>0$ is satisfied.\n\nThe final form of the marginal density is:\n$$p(x) = \\frac{n}{2c} \\exp\\left(-\\frac{n|x|}{c}\\right)$$\n\nFinally, for task 3, we compute the scale parameter. The probability density function for a Laplace distribution with location parameter $\\mu$ and scale parameter $b > 0$ is:\n$$f(x; \\mu, b) = \\frac{1}{2b} \\exp\\left(-\\frac{|x-\\mu|}{b}\\right)$$\nIn our case, the location parameter $\\mu$ is $0$. Comparing our derived density $p(x)$ with the standard form $f(x; 0, b)$, we have:\n$$p(x) = \\frac{n}{2c} \\exp\\left(-\\frac{n|x|}{c}\\right) = \\frac{1}{2(c/n)} \\exp\\left(-\\frac{|x|}{c/n}\\right)$$\nBy direct comparison, the scale parameter $b$ is:\n$$b = \\frac{c}{n}$$\nThis is the scale parameter of the marginal prior, expressed in terms of $c$ and $n$.", "answer": "$$\\boxed{\\frac{c}{n}}$$", "id": "3414194"}, {"introduction": "Many inverse problems involve inferring continuous functions, such as a temperature field or an image, which must be discretized for numerical computation. A well-posed prior should be defined on the infinite-dimensional function space itself, ensuring that its discretized representation is consistent and stable as the computational mesh is refined. This practice [@problem_id:3414123] offers hands-on experience with this advanced concept, tasking you with implementing a continuous Gaussian prior using the finite element method and verifying its mesh-independence, a critical skill for developing robust computational solutions to function-space inverse problems.", "problem": "Consider a zero-mean Gaussian prior on the unit interval with homogeneous Dirichlet boundary conditions defined by the following precision bilinear form: for functions $u$ and $v$ in the Sobolev space $H_0^1([0,1])$, the precision operator $L$ induces the symmetric bilinear form\n$$\na(u,v) = \\kappa^2 \\int_0^1 u(x) v(x)\\, dx + \\int_0^1 \\frac{du}{dx}(x)\\frac{dv}{dx}(x)\\, dx,\n$$\nwhere $\\kappa > 0$ is a real parameter. The Gaussian prior is characterized by the covariance operator $C = L^{-1}$, interpreted in the weak sense.\n\nYou are asked to derive, implement, and compare discretized covariance matrices associated with the same continuous Gaussian prior using the finite element (FE) method with continuous, piecewise linear basis functions on two uniform meshes of the unit interval. The meshes correspond to $N$ elements for the coarse mesh and $2N$ elements for the fine mesh. The discretization must proceed from first principles using the weak formulation above and the standard Galerkin approach: assemble the FE stiffness and mass matrices by integrating the appropriate terms of $a(u,v)$ over elements. Enforce homogeneous Dirichlet boundary conditions by working in the subspace that excludes boundary basis functions.\n\nFrom these foundations, construct the discrete precision matrices on both meshes and obtain the corresponding discrete covariance matrices as inverses of the discrete precision matrices. To compare the two discretizations in a mesh-independent way, restrict the fine-mesh covariance matrix to the coarse-mesh nodal subset: since the fine mesh is a uniform refinement by a factor of two, each coarse interior node coincides with a fine interior node. Form the restricted fine covariance submatrix by selecting rows and columns of the fine covariance that correspond to the coarse interior nodes.\n\nDefine and compute, for each specified test case, the following two quantitative measures of discretization invariance comparing the coarse covariance $C_h$ to the restricted fine covariance $C_{h/2 \\to h}$:\n1. The maximum relative variance discrepancy\n$$\n\\delta_{\\mathrm{var}} = \\max_{i} \\left| \\frac{ \\left(C_h\\right)_{ii} - \\left(C_{h/2 \\to h}\\right)_{ii} }{ \\left(C_h\\right)_{ii} } \\right|.\n$$\n2. The relative Frobenius norm discrepancy\n$$\n\\delta_{\\mathrm{F}} = \\frac{ \\left\\| C_h - C_{h/2 \\to h} \\right\\|_F }{ \\left\\| C_h \\right\\|_F },\n$$\nwhere $\\|\\cdot\\|_F$ denotes the Frobenius norm.\n\nYour implementation must use the following test suite of parameter values, each consisting of a pair $(N,\\kappa)$:\n- Test case 1 (general case): $(N,\\kappa) = (16, 1.0)$.\n- Test case 2 (edge case with rougher field): $(N,\\kappa) = (8, 0.3)$.\n- Test case 3 (edge case with smoother field): $(N,\\kappa) = (32, 3.0)$.\n\nAll computations are purely mathematical; no physical units are involved, and no angle units are required.\n\nYour program must output a single line containing a flat list of six floating-point numbers ordered as\n$$\n\\left[ \\delta_{\\mathrm{var}}^{(1)}, \\delta_{\\mathrm{F}}^{(1)}, \\delta_{\\mathrm{var}}^{(2)}, \\delta_{\\mathrm{F}}^{(2)}, \\delta_{\\mathrm{var}}^{(3)}, \\delta_{\\mathrm{F}}^{(3)} \\right],\n$$\ncorresponding to the three test cases. Each number must be rounded to eight decimal places. The output format must be exactly a single line with the results as a comma-separated list enclosed in square brackets, for example,\n$$\n[\\delta_{\\mathrm{var}}^{(1)},\\delta_{\\mathrm{F}}^{(1)},\\delta_{\\mathrm{var}}^{(2)},\\delta_{\\mathrm{F}}^{(2)},\\delta_{\\mathrm{var}}^{(3)},\\delta_{\\mathrm{F}}^{(3)}].\n$$", "solution": "We begin with the definition of the prior. Let $u$ be a random function in $H_0^1([0,1])$ with a zero mean Gaussian distribution characterized by the precision operator $L$ whose weak form is given by\n$$\na(u,v) = \\kappa^2 \\int_0^1 u(x) v(x)\\, dx + \\int_0^1 \\frac{du}{dx}(x)\\frac{dv}{dx}(x)\\, dx.\n$$\nThe covariance operator $C = L^{-1}$ is defined as the inverse in the weak sense: for any $f,g$ in the dual space, $\\langle f, C g \\rangle$ solves the equation $a(C g, v) = \\langle g, v \\rangle$ for all $v$.\n\nTo discretize this prior on a mesh of $N$ elements, we select the standard finite element (FE) space of continuous, piecewise linear functions with nodes at $x_i = i h$, $i=0,1,\\ldots,N$ and $h = 1/N$. Homogeneous Dirichlet boundary conditions imply that the degrees of freedom correspond to interior nodes $x_i$ for $i = 1,\\ldots,N-1$. Let $\\{\\phi_i\\}_{i=1}^{N-1}$ denote the FE basis functions associated with these interior nodes. Using the Galerkin method, the discrete precision matrix $Q_h \\in \\mathbb{R}^{(N-1)\\times(N-1)}$ is defined by\n$$\n(Q_h)_{ij} = a(\\phi_i, \\phi_j) = \\kappa^2 \\int_0^1 \\phi_i(x)\\phi_j(x)\\, dx + \\int_0^1 \\frac{d\\phi_i}{dx}(x)\\frac{d\\phi_j}{dx}(x)\\, dx.\n$$\nThe first term is the FE mass matrix, and the second term is the FE stiffness matrix. It is a fundamental construction in the FE method that assembling the mass and stiffness matrices from the local element contributions yields these integrals. For a uniform mesh and piecewise linear basis, the stiffness matrix entries are derived from the gradient term and involve the inverse of the element length, while the mass matrix entries are derived from the $L^2$ inner product and involve the element length. Specifically, these matrices are tridiagonal due to the support of the basis functions overlapped only on neighboring elements.\n\nBy construction, $Q_h$ is symmetric positive definite. The discrete covariance matrix is the inverse of the discrete precision:\n$$\nC_h = Q_h^{-1}.\n$$\nThis discretized Gaussian distribution over the FE coefficients approximates the continuous prior, with the FE solution minimizing the energy induced by $a(\\cdot,\\cdot)$. The discretized prior is consistent in the sense that refining the mesh to $2N$ elements defines a space that contains the coarse space, and the Galerkin discretization on the fine mesh, with precision matrix $Q_{h/2}$ and covariance $C_{h/2} = Q_{h/2}^{-1}$, approximates the same continuous prior more finely.\n\nTo compare the coarse covariance $C_h$ with the fine covariance $C_{h/2}$ in a mesh-independent way, we restrict the fine covariance to the coarse nodal subset. Since the fine mesh is a uniform refinement by a factor of two, the coarse interior nodes coincide with fine interior nodes at positions $x_j = j h = 2 j h_f$ for $j=1,\\ldots,N-1$, where $h_f = 1/(2N)$ is the fine mesh size. In index terms, the coarse interior nodes correspond to fine interior indices $2j$ (in one-based indexing, or $2j-1$ in zero-based indexing for arrays). Define the restriction by selecting the submatrix of $C_{h/2}$ with rows and columns corresponding to these fine indices. Denote this restricted submatrix by $C_{h/2 \\to h}$.\n\nTo evaluate discretization invariance, we compute two metrics:\n1. The maximum relative variance discrepancy\n$$\n\\delta_{\\mathrm{var}} = \\max_{i} \\left| \\frac{ \\left(C_h\\right)_{ii} - \\left(C_{h/2 \\to h}\\right)_{ii} }{ \\left(C_h\\right)_{ii} } \\right|,\n$$\nwhich compares marginal variances at coarse nodes.\n2. The relative Frobenius norm discrepancy\n$$\n\\delta_{\\mathrm{F}} = \\frac{ \\left\\| C_h - C_{h/2 \\to h} \\right\\|_F }{ \\left\\| C_h \\right\\|_F },\n$$\nwhich measures the global difference in covariance structures.\n\nAlgorithmic steps aligned with these principles:\n- For a given $N$ and $\\kappa$, compute the uniform mesh size $h = 1/N$ and assemble the FE mass and stiffness matrices. For piecewise linear basis on a uniform mesh, the global mass matrix $M_h$ and stiffness matrix $K_h$ are tridiagonal with entries\n$$\n(M_h)_{ii} = \\frac{2h}{3},\\quad (M_h)_{i,i\\pm 1} = \\frac{h}{6},\\quad (K_h)_{ii} = \\frac{2}{h},\\quad (K_h)_{i,i\\pm 1} = -\\frac{1}{h},\n$$\nfor $i=1,\\ldots,N-1$, with off-diagonal entries zero whenever indices fall outside the interior range.\n- Form the discrete precision $Q_h = \\kappa^2 M_h + K_h$ and invert it numerically to obtain $C_h = Q_h^{-1}$.\n- Repeat the assembly for the fine mesh with $2N$ elements to obtain $C_{h/2}$.\n- Restrict $C_{h/2}$ to the coarse interior nodes to obtain $C_{h/2 \\to h}$ by selecting rows and columns with fine indices corresponding to the coarse interior nodes.\n- Compute $\\delta_{\\mathrm{var}}$ and $\\delta_{\\mathrm{F}}$ for each test case.\n\nThese steps implement the Galerkin discretization of the weak-form precision, enforce boundary conditions by excluding boundary degrees of freedom, and use the subspace relation between the coarse and fine FE spaces to provide a consistent restriction for comparison. Because the discretizations arise from the same continuous prior and the fine space contains the coarse space, $C_{h/2 \\to h}$ should closely approximate $C_h$ when $N$ is sufficiently large and the discretization is consistent, leading to small values of both discrepancy metrics. The test suite includes a general case and two edge cases intended to probe the behavior as $\\kappa$ varies, where smaller $\\kappa$ induces rougher fields and larger $\\kappa$ induces smoother fields.\n\nThe program will compute these metrics for the specified test cases and print a single line containing the six results ordered as $[\\delta_{\\mathrm{var}}^{(1)},\\delta_{\\mathrm{F}}^{(1)},\\delta_{\\mathrm{var}}^{(2)},\\delta_{\\mathrm{F}}^{(2)},\\delta_{\\mathrm{var}}^{(3)},\\delta_{\\mathrm{F}}^{(3)}]$, each rounded to eight decimal places.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef assemble_fe_matrices(N: int):\n    \"\"\"\n    Assemble mass and stiffness matrices for 1D FE with N uniform elements\n    on [0,1] and homogeneous Dirichlet boundary conditions.\n    Returns M, K of size (N-1) x (N-1).\n    \"\"\"\n    if N < 2:\n        raise ValueError(\"N must be at least 2 to have interior degrees of freedom.\")\n    n = N - 1  # number of interior nodes\n    h = 1.0 / N\n\n    # Mass matrix (tridiagonal)\n    diag_M = np.full(n, 2.0 * h / 3.0)\n    off_M = np.full(n - 1, h / 6.0)\n    M = np.diag(diag_M)\n    if n > 1:\n        M += np.diag(off_M, 1) + np.diag(off_M, -1)\n\n    # Stiffness matrix (tridiagonal)\n    diag_K = np.full(n, 2.0 / h)\n    off_K = np.full(n - 1, -1.0 / h)\n    K = np.diag(diag_K)\n    if n > 1:\n        K += np.diag(off_K, 1) + np.diag(off_K, -1)\n\n    return M, K\n\ndef covariance_matrix(N: int, kappa: float):\n    \"\"\"\n    Compute the discrete covariance matrix C_h = Q_h^{-1} for mesh with N elements.\n    \"\"\"\n    M, K = assemble_fe_matrices(N)\n    Q = (kappa ** 2) * M + K\n    # Invert Q using solve with identity for numerical stability\n    n = Q.shape[0]\n    I = np.eye(n)\n    C = np.linalg.solve(Q, I)\n    return C\n\ndef restrict_fine_to_coarse(C_fine: np.ndarray, N_coarse: int):\n    \"\"\"\n    Restrict the fine covariance matrix to the coarse interior nodes.\n    Fine mesh assumed to have 2*N_coarse elements.\n    Coarse interior indices j=1..N_coarse-1 correspond to fine indices i=2*j (1-based),\n    which are (2*j - 1) in 0-based array indexing.\n    \"\"\"\n    n_coarse = N_coarse - 1\n    fine_indices = [2 * j - 1 for j in range(1, N_coarse)]  # 0-based\n    C_restricted = C_fine[np.ix_(fine_indices, fine_indices)]\n    return C_restricted\n\ndef invariance_metrics(N: int, kappa: float):\n    \"\"\"\n    Compute the two discrepancy metrics:\n    - delta_var: maximum relative difference of variances (diagonal entries)\n    - delta_F: relative Frobenius norm difference\n    \"\"\"\n    # Coarse covariance\n    C_coarse = covariance_matrix(N, kappa)\n    # Fine covariance with twice as many elements\n    C_fine = covariance_matrix(2 * N, kappa)\n    # Restrict fine covariance to coarse nodes\n    C_fine_to_coarse = restrict_fine_to_coarse(C_fine, N)\n\n    # Variance discrepancy\n    diag_coarse = np.diag(C_coarse)\n    diag_fine_restricted = np.diag(C_fine_to_coarse)\n    # Avoid division by zero: diagonals of covariance should be positive\n    rel_var = np.abs((diag_coarse - diag_fine_restricted) / diag_coarse)\n    delta_var = float(np.max(rel_var)) if rel_var.size > 0 else 0.0\n\n    # Frobenius norm discrepancy\n    fro_diff = np.linalg.norm(C_coarse - C_fine_to_coarse, ord='fro')\n    fro_coarse = np.linalg.norm(C_coarse, ord='fro')\n    delta_F = float(fro_diff / fro_coarse) if fro_coarse > 0 else 0.0\n\n    return delta_var, delta_F\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (16, 1.0),  # general case\n        (8, 0.3),   # rougher field (smaller kappa)\n        (32, 3.0),  # smoother field (larger kappa)\n    ]\n\n    results = []\n    for N, kappa in test_cases:\n        d_var, d_F = invariance_metrics(N, kappa)\n        results.append(d_var)\n        results.append(d_F)\n\n    # Final print statement in the exact required format with 8 decimal places.\n    formatted = \",\".join(f\"{x:.8f}\" for x in results)\n    print(f\"[{formatted}]\")\n\nsolve()\n```", "id": "3414123"}, {"introduction": "In realistic modeling scenarios, we rarely know the exact parameters of our prior distribution, such as the variance of the unknown field. Hierarchical Bayesian models provide a powerful framework for managing this uncertainty by treating the prior's parameters (hyperparameters) as random variables governed by their own distributions (hyperpriors). This numerical exercise [@problem_id:3414106] will guide you through implementing a full hierarchical model and conducting a sensitivity analysis on the choice of hyperprior, providing crucial insight into how these deep modeling decisions propagate through the inference and affect the final solution.", "problem": "Consider the linear inverse problem with a Gaussian observation model and a hierarchical Gaussian prior. Let $A \\in \\mathbb{R}^{m \\times n}$ be a known forward operator, $x \\in \\mathbb{R}^n$ be the unknown parameter vector, and $y \\in \\mathbb{R}^m$ be the observed data. The observation model is\n$$\ny = A x + \\varepsilon,\n$$\nwhere $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_m)$ with known noise variance $\\sigma^2 > 0$ and $I_m$ the $m \\times m$ identity matrix.\n\nA hierarchical Gaussian prior on $x$ is given by\n$$\nx \\mid \\tau \\sim \\mathcal{N}(0, \\tau^2 I_n),\n$$\nwhere $\\tau > 0$ is an unknown scale (standard deviation) parameter. We will compare two hyperpriors for $\\tau$:\n\n1. An inverse-gamma hyperprior on $\\tau^2$, with shape $a > 1$ and scale $b > 0$:\n$$\n\\tau^2 \\sim \\text{Inverse-Gamma}(a, b).\n$$\n2. A half-Cauchy hyperprior on $\\tau$ with scale $s > 0$:\n$$\n\\tau \\sim \\text{Half-Cauchy}(s).\n$$\n\nWe will perform prior predictive checks and a sensitivity analysis by comparing posterior inferences under the two hyperpriors. The analysis must start from first principles: the definition of the Gaussian likelihood, the hierarchical prior, and the laws of marginalization.\n\nFundamental base definitions to be used:\n- The Gaussian density $\\mathcal{N}(m, \\Sigma)$ has density\n$$\np(z) = \\frac{1}{(2\\pi)^{k/2} \\det(\\Sigma)^{1/2}} \\exp\\left( -\\frac{1}{2} (z - m)^\\top \\Sigma^{-1} (z - m) \\right),\n$$\nfor $z \\in \\mathbb{R}^k$.\n- The law of total probability and conditional expectation:\n$$\np(y) = \\int p(y \\mid \\tau) p(\\tau) \\, d\\tau, \\quad \\mathbb{E}[x \\mid y] = \\int \\mathbb{E}[x \\mid y, \\tau] \\, p(\\tau \\mid y) \\, d\\tau.\n$$\n\nTasks:\n1. Derive the marginal likelihood $p(y \\mid \\tau)$ under the hierarchical Gaussian prior and Gaussian likelihood, expressed in terms of $A$, $y$, $\\sigma^2$, and $\\tau$.\n2. Derive the conditional posterior $x \\mid y, \\tau$ and identify its mean vector and covariance matrix in terms of $A$, $y$, $\\sigma^2$, and $\\tau$.\n3. For each hyperprior, define the hyperprior density $p(\\tau)$, ensuring correct transformation for the inverse-gamma prior placed on $\\tau^2$.\n4. Compute, for each hyperprior, the following posterior inferences:\n   - The posterior mean of $x$ integrated over the hyperprior via\n     $$\n     \\mathbb{E}[x \\mid y] = \\frac{\\int \\mathbb{E}[x \\mid y, \\tau] \\, p(y \\mid \\tau) \\, p(\\tau) \\, d\\tau}{\\int p(y \\mid \\tau) \\, p(\\tau) \\, d\\tau}.\n     $$\n   - The posterior expected trace of the covariance of $x$ integrated over the hyperprior via\n     $$\n     \\mathbb{E}[\\operatorname{tr}(\\operatorname{Cov}[x \\mid y, \\tau]) \\mid y] = \\frac{\\int \\operatorname{tr}(\\operatorname{Cov}[x \\mid y, \\tau]) \\, p(y \\mid \\tau) \\, p(\\tau) \\, d\\tau}{\\int p(y \\mid \\tau) \\, p(\\tau) \\, d\\tau}.\n     $$\n   - The prior predictive marginal density of the data $p(y)$ under the hyperprior via\n     $$\n     p(y) = \\int p(y \\mid \\tau) \\, p(\\tau) \\, d\\tau.\n     $$\n5. Using the above, conduct a sensitivity analysis by computing the Euclidean norm of the difference between the posterior means obtained under the two hyperpriors and the difference between the posterior expected trace of covariance under the two hyperpriors. Also report the log prior predictive marginal density $\\log p(y)$ under each hyperprior.\n\nYou must implement the above computations numerically. The integrals with respect to $\\tau$ in items 4 and 5 must be computed by one-dimensional numerical quadrature over $\\tau \\in (0, \\infty)$, using stable evaluation of the Gaussian likelihood and the hyperprior densities. For the inverse-gamma hyperprior, use $a = 2.5$ and $b = 1.0$. For the half-Cauchy hyperprior, use $s = 1.0$.\n\nTest Suite:\nEvaluate your program on the following three test cases. There are no physical units involved; all quantities are dimensionless.\n\n- Test case 1 (well-conditioned, moderate noise):\n  - $m = 3$, $n = 2$,\n  - $A = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{bmatrix}$,\n  - $y = \\begin{bmatrix} 1.2 \\\\ -0.3 \\\\ 0.7 \\end{bmatrix}$,\n  - $\\sigma = 0.5$.\n\n- Test case 2 (rank-deficient forward model, prior-dominated):\n  - $m = 3$, $n = 2$,\n  - $A = \\begin{bmatrix} 0 & 0 \\\\ 0 & 0 \\\\ 0 & 0 \\end{bmatrix}$,\n  - $y = \\begin{bmatrix} 0.1 \\\\ -0.2 \\\\ 0.05 \\end{bmatrix}$,\n  - $\\sigma = 0.5$.\n\n- Test case 3 (identity forward model, low noise, data-dominated):\n  - $m = 2$, $n = 2$,\n  - $A = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}$,\n  - $y = \\begin{bmatrix} 3.0 \\\\ -2.0 \\end{bmatrix}$,\n  - $\\sigma = 0.1$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case result must be a list of four floats in the order\n$$\n\\left[\\lVert \\mathbb{E}_{\\text{IG}}[x \\mid y] - \\mathbb{E}_{\\text{HC}}[x \\mid y] \\rVert_2, \\ \\mathbb{E}_{\\text{IG}}[\\operatorname{tr}(\\operatorname{Cov}[x \\mid y, \\tau]) \\mid y] - \\mathbb{E}_{\\text{HC}}[\\operatorname{tr}(\\operatorname{Cov}[x \\mid y, \\tau]) \\mid y], \\ \\log p_{\\text{IG}}(y), \\ \\log p_{\\text{HC}}(y) \\right],\n$$\nwhere the subscripts $\\text{IG}$ and $\\text{HC}$ indicate inverse-gamma and half-Cauchy hyperpriors, respectively. Thus, the entire output must be a list of three such lists, one per test case, for example:\n$$\n\\big[ [r_{11}, r_{12}, r_{13}, r_{14}], [r_{21}, r_{22}, r_{23}, r_{24}], [r_{31}, r_{32}, r_{33}, r_{34}] \\big].\n$$", "solution": "The problem requires a comprehensive Bayesian analysis of a linear inverse problem with a hierarchical Gaussian prior. The analysis involves comparing two different hyperpriors for the prior's scale parameter, $\\tau$. We will proceed by first deriving the necessary analytical expressions for the conditional likelihoods and posteriors, and then detailing the numerical strategy for marginalizing over the hyperparameter $\\tau$ to obtain the final posterior inferences.\n\nThe model setup is as follows:\n- Likelihood: $p(y \\mid x) = \\mathcal{N}(y; Ax, \\sigma^2 I_m)$, where $y \\in \\mathbb{R}^m$ are the data, $x \\in \\mathbb{R}^n$ are the parameters, $A \\in \\mathbb{R}^{m \\times n}$ is the forward operator, and $\\sigma^2 > 0$ is the known noise variance.\n- Prior: $p(x \\mid \\tau) = \\mathcal{N}(x; 0, \\tau^2 I_n)$, where $\\tau > 0$ is a hyperparameter.\n- Hyperprior $p(\\tau)$: Two cases are considered, an inverse-gamma distribution on $\\tau^2$ and a half-Cauchy distribution on $\\tau$.\n\nOur goal is to compute and compare posterior statistics for $x$ under these two hyperprior choices by marginalizing out the hyperparameter $\\tau$.\n\n1. Derivation of the Marginal Likelihood $p(y \\mid \\tau)$\n\nThe marginal likelihood of the data $y$ given the hyperparameter $\\tau$, denoted $p(y \\mid \\tau)$, is obtained by integrating the joint probability of $y$ and $x$ over all possible values of $x$:\n$$\np(y \\mid \\tau) = \\int_{\\mathbb{R}^n} p(y \\mid x) p(x \\mid \\tau) \\, dx.\n$$\nThis is a standard convolution of two Gaussian distributions. Since the model is linear and the distributions are Gaussian, the resulting marginal distribution for $y$ is also Gaussian. We can determine its parameters by computing its mean and covariance.\n\nThe mean of $y$ given $\\tau$ is:\n$$\n\\mathbb{E}[y \\mid \\tau] = \\mathbb{E}[Ax + \\varepsilon \\mid \\tau] = A \\mathbb{E}[x \\mid \\tau] + \\mathbb{E}[\\varepsilon] = A \\cdot 0 + 0 = 0.\n$$\nThe covariance of $y$ given $\\tau$, using the independence of $x$ and $\\varepsilon$, is:\n$$\n\\operatorname{Cov}(y \\mid \\tau) = \\operatorname{Cov}(Ax + \\varepsilon \\mid \\tau) = \\operatorname{Cov}(Ax \\mid \\tau) + \\operatorname{Cov}(\\varepsilon \\mid \\tau) = A \\operatorname{Cov}(x \\mid \\tau) A^\\top + \\sigma^2 I_m.\n$$\nSince $\\operatorname{Cov}(x \\mid \\tau) = \\tau^2 I_n$, the covariance becomes:\n$$\n\\Sigma_{y \\mid \\tau} = \\tau^2 A A^\\top + \\sigma^2 I_m.\n$$\nThus, the marginal likelihood is that of a multivariate Gaussian distribution:\n$$\np(y \\mid \\tau) = \\mathcal{N}(y; 0, \\Sigma_{y \\mid \\tau}) = \\frac{1}{(2\\pi)^{m/2} \\det(\\Sigma_{y \\mid \\tau})^{1/2}} \\exp\\left( -\\frac{1}{2} y^\\top \\Sigma_{y \\mid \\tau}^{-1} y \\right).\n$$\n\n2. Derivation of the Conditional Posterior $p(x \\mid y, \\tau)$\n\nUsing Bayes' theorem, the posterior distribution of $x$ conditioned on both the data $y$ and the hyperparameter $\\tau$ is:\n$$\np(x \\mid y, \\tau) \\propto p(y \\mid x, \\tau) p(x \\mid \\tau) = p(y \\mid x) p(x \\mid \\tau).\n$$\nThe log-posterior is proportional to the sum of the log-likelihood and the log-prior:\n$$\n\\log p(x \\mid y, \\tau) \\propto -\\frac{1}{2\\sigma^2} \\|y - Ax\\|_2^2 - \\frac{1}{2\\tau^2} \\|x\\|_2^2.\n$$\nExpanding the quadratic forms, we get:\n$$\n\\log p(x \\mid y, \\tau) \\propto -\\frac{1}{2} \\left( \\frac{1}{\\sigma^2}(y^\\top y - 2y^\\top Ax + x^\\top A^\\top A x) + \\frac{1}{\\tau^2} x^\\top x \\right).\n$$\nCollecting terms involving $x$:\n$$\n\\log p(x \\mid y, \\tau) \\propto -\\frac{1}{2} \\left( x^\\top \\left( \\frac{1}{\\sigma^2} A^\\top A + \\frac{1}{\\tau^2} I_n \\right) x - 2 \\frac{1}{\\sigma^2} y^\\top A x \\right).\n$$\nThis is a quadratic function of $x$, which implies that the posterior $p(x \\mid y, \\tau)$ is a Gaussian distribution, $\\mathcal{N}(\\mu_{x \\mid \\tau}, \\Sigma_{x \\mid \\tau})$. From the expression above, we can identify the posterior precision (inverse covariance) matrix as:\n$$\n\\Sigma_{x \\mid \\tau}^{-1} = \\frac{1}{\\sigma^2} A^\\top A + \\frac{1}{\\tau^2} I_n.\n$$\nThe posterior covariance matrix is its inverse:\n$$\n\\Sigma_{x \\mid \\tau} = \\operatorname{Cov}[x \\mid y, \\tau] = \\left( \\frac{1}{\\sigma^2} A^\\top A + \\frac{1}{\\tau^2} I_n \\right)^{-1}.\n$$\nThe posterior mean $\\mu_{x \\mid \\tau} = \\mathbb{E}[x \\mid y, \\tau]$ satisfies $\\Sigma_{x \\mid \\tau}^{-1} \\mu_{x \\mid \\tau} = \\frac{1}{\\sigma^2} A^\\top y$. Therefore,\n$$\n\\mu_{x \\mid \\tau} = \\left( \\frac{1}{\\sigma^2} A^\\top A + \\frac{1}{\\tau^2} I_n \\right)^{-1} \\frac{1}{\\sigma^2} A^\\top y.\n$$\n\n3. Hyperprior Densities $p(\\tau)$\n\nWe are given two hyperpriors for $\\tau > 0$.\n\na) Inverse-Gamma on $\\tau^2$: The hyperprior is specified as $\\tau^2 \\sim \\text{Inverse-Gamma}(a, b)$, with shape $a=2.5$ and scale $b=1.0$. The probability density function for a variable $\\lambda \\sim \\text{Inverse-Gamma}(a, b)$ is $p(\\lambda) = \\frac{b^a}{\\Gamma(a)} \\lambda^{-(a+1)} e^{-b/\\lambda}$. To find the density for $\\tau$, we use the change of variables formula. Let $\\lambda = \\tau^2$, so $\\frac{d\\lambda}{d\\tau} = 2\\tau$. For $\\tau > 0$:\n$$\np_{\\text{IG}}(\\tau) = p_{\\lambda}(\\tau^2) \\left| \\frac{d\\lambda}{d\\tau} \\right| = \\frac{b^a}{\\Gamma(a)} (\\tau^2)^{-(a+1)} e^{-b/\\tau^2} \\cdot (2\\tau) = \\frac{2b^a}{\\Gamma(a)} \\tau^{-2a-1} e^{-b/\\tau^2}.\n$$\n\nb) Half-Cauchy on $\\tau$: The hyperprior is $\\tau \\sim \\text{Half-Cauchy}(s)$, with scale $s=1.0$. This is a Cauchy distribution with location $0$ and scale $s$, truncated to the interval $(0, \\infty)$ and renormalized. The density is:\n$$\np_{\\text{HC}}(\\tau) = \\frac{2}{\\pi s(1 + (\\tau/s)^2)}.\n$$\n\n4. Numerical Computation of Posterior Inferences\n\nThe ultimate goal is to compute posterior quantities for $x$ by marginalizing over $\\tau$. The posterior for $\\tau$ is given by $p(\\tau \\mid y) \\propto p(y \\mid \\tau) p(\\tau)$. The required quantities are computed via the following integrals:\n- Prior predictive marginal density: $p(y) = \\int_0^\\infty p(y \\mid \\tau) p(\\tau) \\, d\\tau$.\n- Posterior mean of $x$: $\\mathbb{E}[x \\mid y] = \\int_0^\\infty \\mathbb{E}[x \\mid y, \\tau] p(\\tau \\mid y) \\, d\\tau = \\frac{\\int_0^\\infty \\mu_{x \\mid \\tau} p(y \\mid \\tau) p(\\tau) \\, d\\tau}{p(y)}$.\n- Posterior expected trace of covariance: $\\mathbb{E}[\\operatorname{tr}(\\operatorname{Cov}[x \\mid y, \\tau]) \\mid y] = \\frac{\\int_0^\\infty \\operatorname{tr}(\\Sigma_{x \\mid \\tau}) p(y \\mid \\tau) p(\\tau) \\, d\\tau}{p(y)}$.\n\nThese one-dimensional integrals over $\\tau \\in (0, \\infty)$ are computed using numerical quadrature. To ensure numerical stability, especially since the integrand $p(y \\mid \\tau) p(\\tau)$ can have a very large dynamic range, we work with logarithms. Let $L(\\tau) = \\log(p(y \\mid \\tau)) + \\log(p(\\tau))$. The integrand takes the form $f(\\tau) e^{L(\\tau)}$, where $f(\\tau)$ is either $1$, $\\mu_{x \\mid \\tau}$, or $\\operatorname{tr}(\\Sigma_{x \\mid \\tau})$.\n\nThe numerical integration strategy is as follows:\ni. Define the un-normalized log-posterior of $\\tau$, $L(\\tau)$.\nii. Find the value $\\tau_{\\text{max}}$ that maximizes $L(\\tau)$. This is the maximum a posteriori (MAP) estimate of $\\tau$. Let $L_{\\text{max}} = L(\\tau_{\\text{max}})$.\niii. Rewrite the integrals by factoring out the peak value. For example, $p(y) = \\int_0^\\infty e^{L(\\tau)} \\, d\\tau = e^{L_{\\text{max}}} \\int_0^\\infty e^{L(\\tau) - L_{\\text{max}}} \\, d\\tau$. The integrand $e^{L(\\tau) - L_{\\text{max}}}$ is now well-behaved, with a maximum value of $1$.\niv. Use a numerical quadrature routine (e.g., `scipy.integrate.quad` or `scipy.integrate.quad_vec`) to compute the normalized integrals.\nv. Combine the results to get the final quantities. For example, $\\log p(y) = L_{\\text{max}} + \\log\\left(\\int_0^\\infty e^{L(\\tau) - L_{\\text{max}}} \\, d\\tau\\right)$. For ratios like $\\mathbb{E}[x \\mid y]$, the $e^{L_{\\text{max}}}$ term cancels, simplifying the calculation to a ratio of integrals.\n\nThis procedure is applied for each hyperprior (IG and HC). The final step is to compute the differences and log-densities as specified in the problem statement for the sensitivity analysis. For Test Case 2 where $A=0$, the data provides no information about $x$, and $p(y \\mid \\tau)$ becomes independent of $\\tau$. Consequently, the posterior for $\\tau$ is just its prior, $p(\\tau \\mid y) = p(\\tau)$. The posterior expectations then become prior expectations. In this case, the heavy-tailed Half-Cauchy prior, which has an infinite second moment $\\mathbb{E}[\\tau^2]$, leads to an infinite posterior variance for $x$, a result that the numerical integration should capture.", "answer": "```python\nimport numpy as np\nfrom scipy.integrate import quad, quad_vec\nfrom scipy.optimize import minimize_scalar\nfrom scipy.special import gammaln\nimport math\n\ndef solve():\n    \"\"\"\n    Solves the Bayesian hierarchical inverse problem for three test cases\n    and compares results from Inverse-Gamma and Half-Cauchy hyperpriors.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"m\": 3, \"n\": 2,\n            \"A\": np.array([[1, 0], [0, 1], [1, 1]]),\n            \"y\": np.array([1.2, -0.3, 0.7]),\n            \"sigma\": 0.5,\n        },\n        {\n            \"m\": 3, \"n\": 2,\n            \"A\": np.array([[0, 0], [0, 0], [0, 0]]),\n            \"y\": np.array([0.1, -0.2, 0.05]),\n            \"sigma\": 0.5,\n        },\n        {\n            \"m\": 2, \"n\": 2,\n            \"A\": np.array([[1, 0], [0, 1]]),\n            \"y\": np.array([3.0, -2.0]),\n            \"sigma\": 0.1,\n        },\n    ]\n\n    all_results = []\n    \n    # Hyperprior parameters\n    ig_params = {\"a\": 2.5, \"b\": 1.0}\n    hc_params = {\"s\": 1.0}\n\n    for case in test_cases:\n        A, y, sigma = case[\"A\"], case[\"y\"], case[\"sigma\"]\n        m, n = case[\"m\"], case[\"n\"]\n        sigma_sq = sigma**2\n        \n        AtA = A.T @ A\n        AAt = A @ A.T\n        Aty = A.T @ y\n\n        # --- Helper functions for conditional quantities ---\n        def get_E_x_given_y_tau(tau, sigma_sq, AtA, Aty):\n            if tau == 0: return np.zeros_like(Aty)\n            tau_sq = tau**2\n            prec_x = (1 / sigma_sq) * AtA + (1 / tau_sq) * np.identity(n)\n            rhs = (1 / sigma_sq) * Aty\n            try:\n                mu_x = np.linalg.solve(prec_x, rhs)\n            except np.linalg.LinAlgError:\n                mu_x = np.linalg.pinv(prec_x) @ rhs\n            return mu_x\n\n        def get_tr_Cov_x_given_y_tau(tau, sigma_sq, AtA):\n            if tau == 0: return 0.0\n            tau_sq = tau**2\n            prec_x = (1 / sigma_sq) * AtA + (1 / tau_sq) * np.identity(n)\n            try:\n                cov_x = np.linalg.inv(prec_x)\n            except np.linalg.LinAlgError:\n                cov_x = np.linalg.pinv(prec_x)\n            return np.trace(cov_x)\n\n        def get_log_p_y_given_tau(tau, y, sigma_sq, AAt):\n            tau_sq = tau**2\n            cov_y = tau_sq * AAt + sigma_sq * np.identity(m)\n            try:\n                sign, log_det_cov_y = np.linalg.slogdet(cov_y)\n                if sign <= 0: return -np.inf\n                chol = np.linalg.cholesky(cov_y)\n                y_inv_cov_y = np.linalg.solve(chol.T, np.linalg.solve(chol, y))\n                quad_form = y @ y_inv_cov_y\n            except np.linalg.LinAlgError:\n                return -np.inf\n            \n            log_p = -0.5 * m * np.log(2 * np.pi) - 0.5 * log_det_cov_y - 0.5 * quad_form\n            return log_p\n\n        # --- Hyperprior log-densities ---\n        def log_p_tau_ig(tau, a, b):\n            if tau <= 0: return -np.inf\n            return np.log(2) + a * np.log(b) - gammaln(a) - (2 * a + 1) * np.log(tau) - b / (tau**2)\n\n        def log_p_tau_hc(tau, s):\n            if tau <= 0: return -np.inf\n            return np.log(2) - np.log(np.pi) - np.log(s) - np.log(1 + (tau / s)**2)\n\n        def compute_posterior_quantities(hyperprior_type, params):\n            if hyperprior_type == 'IG':\n                log_p_tau = lambda t: log_p_tau_ig(t, **params)\n            else: # HC\n                log_p_tau = lambda t: log_p_tau_hc(t, **params)\n\n            log_joint_unnorm = lambda t: get_log_p_y_given_tau(t, y, sigma_sq, AAt) + log_p_tau(t)\n            \n            # Find MAP of tau to stabilize integration\n            res = minimize_scalar(lambda t: -log_joint_unnorm(t), bounds=(1e-9, 1e6), method='bounded')\n            tau_max = res.x\n            max_log_val = log_joint_unnorm(tau_max)\n            \n            if not np.isfinite(max_log_val):\n                 # This can happen if the posterior is improper or flat, like A=0 case\n                 # Pick an arbitrary reference point\n                 tau_max = 1.0\n                 max_log_val = log_joint_unnorm(tau_max)\n\n\n            # Integrands for numerical quadrature\n            integrand_Z  = lambda t: np.exp(log_joint_unnorm(t) - max_log_val)\n            integrand_Nx = lambda t: get_E_x_given_y_tau(t, sigma_sq, AtA, Aty) * integrand_Z(t)\n            integrand_Ntr = lambda t: get_tr_Cov_x_given_y_tau(t, sigma_sq, AtA) * integrand_Z(t)\n\n            # Perform integration\n            integral_Z, _ = quad(integrand_Z, 0, np.inf, limit=200)\n            integral_Nx, _ = quad_vec(integrand_Nx, 0, np.inf, limit=200)\n            integral_Ntr, _ = quad(integrand_Ntr, 0, np.inf, limit=200)\n\n            # Compute final quantities\n            E_x = integral_Nx / integral_Z\n            E_tr_Cov = integral_Ntr / integral_Z\n            log_p_y = max_log_val + np.log(integral_Z)\n\n            return E_x, E_tr_Cov, log_p_y\n            \n        # Calculate for both hyperpriors\n        E_x_ig, E_tr_Cov_ig, log_p_y_ig = compute_posterior_quantities('IG', ig_params)\n        E_x_hc, E_tr_Cov_hc, log_p_y_hc = compute_posterior_quantities('HC', hc_params)\n        \n        # Compute sensitivity metrics\n        norm_diff_mean = np.linalg.norm(E_x_ig - E_x_hc)\n        diff_trace_cov = E_tr_Cov_ig - E_tr_Cov_hc\n        \n        case_results = [norm_diff_mean, diff_trace_cov, log_p_y_ig, log_p_y_hc]\n        all_results.append(case_results)\n\n    # Format output\n    result_str = \", \".join(\n        f\"[{', '.join(f'{val:.10f}' for val in res)}]\" for res in all_results\n    )\n    print(f\"[{result_str}]\")\n\n\nif __name__ == '__main__':\n    solve()\n\n```", "id": "3414106"}]}