{"hands_on_practices": [{"introduction": "Once a Markov chain appears to have reached its stationary distribution, we can use its samples to estimate posterior quantities like means or variances. However, unlike samples from direct simulation, MCMC samples are autocorrelated. This practice challenges you to compute the Effective Sample Size (ESS) and Monte Carlo Standard Error (MCSE), which are the correct metrics for quantifying the precision of MCMC estimates in the presence of autocorrelation [@problem_id:3372636]. Mastering this calculation is fundamental for reporting credible and accurate results from any MCMC analysis.", "problem": "In a Bayesian inverse problem for a linear observation model, a Markov chain Monte Carlo (MCMC) algorithm produces a stationary sequence $\\{Y_{t}\\}_{t=1}^{N}$ of evaluations of a scalar posterior functional $Y_{t} = g(\\theta_{t})$, with $N = 50000$. Assume that the Markov chain central limit theorem holds for $\\{Y_{t}\\}$ and that stationarity is achieved. You are given the empirical marginal variance estimate $s^{2} = 2.25$ and empirical autocorrelations $\\{\\hat{\\rho}_{k}\\}_{k=1}^{L}$ up to lag $L = 10$:\n$$\n\\hat{\\rho}_{1} = 0.65,\\quad \\hat{\\rho}_{2} = 0.48,\\quad \\hat{\\rho}_{3} = 0.36,\\quad \\hat{\\rho}_{4} = 0.27,\\quad \\hat{\\rho}_{5} = 0.20,\\\\\n\\hat{\\rho}_{6} = 0.14,\\quad \\hat{\\rho}_{7} = 0.10,\\quad \\hat{\\rho}_{8} = 0.07,\\quad \\hat{\\rho}_{9} = 0.04,\\quad \\hat{\\rho}_{10} = 0.02.\n$$\nStarting only from the definition of the autocovariance function of a strictly stationary process and the Markov chain central limit theoremâ€™s characterization of the asymptotic variance of the sample mean in terms of the spectral density at zero, derive a consistent large-sample estimator for the effective sample size (ESS) and the Monte Carlo standard error (MCSE) of the posterior mean $\\bar{Y}_{N} = \\frac{1}{N}\\sum_{t=1}^{N} Y_{t}$ that uses the empirical autocorrelations $\\{\\hat{\\rho}_{k}\\}_{k=1}^{L}$ via a finite-lag truncation. Then, using the numbers above, compute the numerical values of the ESS and the MCSE. Finally, justify in words how the overlapping batch means (OBM) method can stabilize estimation of the MCSE when the empirical autocorrelations are noisy at moderate and large lags, connecting your justification to consistency for the spectral density at frequency zero.\n\nReport only the MCSE value as your final numerical answer, rounded to four significant figures.", "solution": "The problem is valid as it is scientifically grounded in the statistical theory of Markov chain Monte Carlo (MCMC) methods, is well-posed with sufficient information for a unique solution, and is expressed in objective, formal language. It presents a standard, non-trivial task in the analysis of MCMC output.\n\nWe begin by deriving the estimators for the effective sample size (ESS) and the Monte Carlo standard error (MCSE). Let $\\{Y_{t}\\}_{t=1}^{N}$ be a weakly stationary sequence with mean $\\mu = \\mathbb{E}[Y_{t}]$, variance $\\sigma^{2} = \\text{Var}(Y_{t})$, and autocovariance function $\\gamma_{k} = \\text{Cov}(Y_{t}, Y_{t+k})$. The sample mean is given by $\\bar{Y}_{N} = \\frac{1}{N}\\sum_{t=1}^{N} Y_{t}$.\n\nThe variance of the sample mean is\n$$\n\\text{Var}(\\bar{Y}_{N}) = \\text{Var}\\left(\\frac{1}{N}\\sum_{t=1}^{N} Y_{t}\\right) = \\frac{1}{N^{2}}\\sum_{t=1}^{N}\\sum_{s=1}^{N} \\text{Cov}(Y_{t}, Y_{s}) = \\frac{1}{N^{2}}\\sum_{t=1}^{N}\\sum_{s=1}^{N} \\gamma_{t-s}.\n$$\nFor a large sample size $N$, this variance can be approximated by\n$$\n\\text{Var}(\\bar{Y}_{N}) \\approx \\frac{1}{N} \\sum_{k=-(N-1)}^{N-1} \\left(1 - \\frac{|k|}{N}\\right) \\gamma_{k} \\approx \\frac{1}{N} \\sum_{k=-\\infty}^{\\infty} \\gamma_{k}.\n$$\nThe Markov chain central limit theorem (CLT) states that, under suitable conditions,\n$$\n\\sqrt{N}(\\bar{Y}_{N} - \\mu) \\xrightarrow{d} \\mathcal{N}(0, \\sigma_{\\text{asy}}^{2}),\n$$\nwhere the asymptotic variance $\\sigma_{\\text{asy}}^{2}$ is given by the sum of all autocovariances:\n$$\n\\sigma_{\\text{asy}}^{2} = \\sum_{k=-\\infty}^{\\infty} \\gamma_{k}.\n$$\nThis connects to the spectral density of the process, $f(\\omega) = \\frac{1}{2\\pi}\\sum_{k=-\\infty}^{\\infty} \\gamma_{k} \\exp(-ik\\omega)$. At frequency $\\omega=0$, we have $f(0) = \\frac{1}{2\\pi}\\sum_{k=-\\infty}^{\\infty} \\gamma_{k}$, which directly yields $\\sigma_{\\text{asy}}^{2} = 2\\pi f(0)$.\n\nDue to stationarity, $\\gamma_{k} = \\gamma_{-k}$. We can rewrite the asymptotic variance using the autocorrelation function $\\rho_{k} = \\gamma_{k}/\\gamma_{0} = \\gamma_{k}/\\sigma^{2}$ as:\n$$\n\\sigma_{\\text{asy}}^{2} = \\gamma_{0} + \\sum_{k=1}^{\\infty} \\gamma_{k} + \\sum_{k=-\\infty}^{-1} \\gamma_{k} = \\gamma_{0} + 2\\sum_{k=1}^{\\infty} \\gamma_{k} = \\sigma^{2}\\left(1 + 2\\sum_{k=1}^{\\infty} \\rho_{k}\\right).\n$$\nThe term $\\tau = 1 + 2\\sum_{k=1}^{\\infty} \\rho_{k}$ is known as the integrated autocorrelation time (IACT).\n\nThe Monte Carlo standard error (MCSE) is the standard deviation of the estimator $\\bar{Y}_{N}$, which for large $N$ is\n$$\n\\text{MCSE}(\\bar{Y}_{N}) = \\sqrt{\\text{Var}(\\bar{Y}_{N})} \\approx \\sqrt{\\frac{\\sigma_{\\text{asy}}^{2}}{N}} = \\sqrt{\\frac{\\sigma^{2}}{N}\\left(1 + 2\\sum_{k=1}^{\\infty} \\rho_{k}\\right)}.\n$$\nThe effective sample size (ESS) is defined as the size of an equivalent independent and identically distributed (i.i.d.) sample that would yield the same variance for the sample mean. For an i.i.d. sample of size $M$, the variance of the mean is $\\sigma^{2}/M$. Equating this to the MCMC variance of the mean gives:\n$$\n\\frac{\\sigma^{2}}{\\text{ESS}} = \\text{Var}(\\bar{Y}_{N}) \\approx \\frac{\\sigma_{\\text{asy}}^{2}}{N} \\implies \\text{ESS} = \\frac{N\\sigma^{2}}{\\sigma_{\\text{asy}}^{2}} = \\frac{N}{1 + 2\\sum_{k=1}^{\\infty} \\rho_{k}}.\n$$\nTo construct consistent estimators, we replace the theoretical quantities with their empirical counterparts. The marginal variance $\\sigma^{2}$ is estimated by the sample variance $s^{2}$. The autocorrelations $\\rho_{k}$ are estimated by the sample autocorrelations $\\hat{\\rho}_{k}$. The problem specifies using a finite-lag truncation, which means we approximate the infinite sum of autocorrelations by a finite sum up to a lag $L$. The resulting estimator for the IACT is:\n$$\n\\hat{\\tau}_{L} = 1 + 2\\sum_{k=1}^{L} \\hat{\\rho}_{k}.\n$$\nThis leads to the following large-sample estimators:\n$$\n\\widehat{\\text{ESS}} = \\frac{N}{\\hat{\\tau}_{L}} = \\frac{N}{1 + 2\\sum_{k=1}^{L} \\hat{\\rho}_{k}}\n$$\n$$\n\\widehat{\\text{MCSE}}(\\bar{Y}_{N}) = \\sqrt{\\frac{s^{2}}{\\widehat{\\text{ESS}}}} = \\sqrt{\\frac{s^{2}\\hat{\\tau}_{L}}{N}} = \\sqrt{\\frac{s^{2}}{N}\\left(1 + 2\\sum_{k=1}^{L} \\hat{\\rho}_{k}\\right)}.\n$$\nNow, we compute the numerical values using the provided data: $N = 50000$, $s^{2} = 2.25$, and sample autocorrelations up to lag $L = 10$.\nFirst, we sum the given empirical autocorrelations:\n$$\n\\sum_{k=1}^{10} \\hat{\\rho}_{k} = 0.65 + 0.48 + 0.36 + 0.27 + 0.20 + 0.14 + 0.10 + 0.07 + 0.04 + 0.02 = 2.33.\n$$\nNext, we compute the estimated IACT:\n$$\n\\hat{\\tau}_{10} = 1 + 2\\sum_{k=1}^{10} \\hat{\\rho}_{k} = 1 + 2(2.33) = 1 + 4.66 = 5.66.\n$$\nUsing this, we calculate the effective sample size:\n$$\n\\widehat{\\text{ESS}} = \\frac{N}{\\hat{\\tau}_{10}} = \\frac{50000}{5.66} \\approx 8833.922.\n$$\nFinally, we compute the Monte Carlo standard error:\n$$\n\\widehat{\\text{MCSE}}(\\bar{Y}_{N}) = \\sqrt{\\frac{s^{2}}{\\widehat{\\text{ESS}}}} = \\sqrt{\\frac{2.25}{50000 / 5.66}} = \\sqrt{\\frac{2.25 \\times 5.66}{50000}} = \\sqrt{\\frac{12.735}{50000}} = \\sqrt{0.0002547} \\approx 0.0159593.\n$$\nRounding to four significant figures, the MCSE is $0.01596$.\n\nFinally, we address the role of the overlapping batch means (OBM) method. The simple finite-lag truncation estimator for the asymptotic variance is $\\hat{\\sigma}_{\\text{asy, L}}^{2} = s^{2}(1 + 2\\sum_{k=1}^{L} \\hat{\\rho}_{k}) = \\sum_{k=-L}^{L} \\hat{\\gamma}_{k}$. This estimator is known to have high variance because the sample autocovariances $\\hat{\\gamma}_{k}$ are themselves noisy, especially for moderate to large lags $k$. Summing these noisy terms directly can lead to unstable estimates of $\\sigma_{\\text{asy}}^{2}$. A negative estimate is even possible if the sum of negative sample correlations is large enough, which is nonsensical for a variance.\n\nThe OBM method offers a more stable alternative. It estimates $\\sigma_{\\text{asy}}^{2}$ using the variance of the means of overlapping batches of observations. For a batch size $b$, the OBM estimator for $\\sigma_{\\text{asy}}^{2}$ is consistent for $N \\to \\infty$ if $b \\to \\infty$ and $b/N \\to 0$. The critical insight is that the OBM estimator is equivalent to a kernel-based spectral density estimator at frequency zero. Specifically, the OBM variance estimator can be shown to be of the form:\n$$\n\\hat{\\sigma}_{\\text{OBM}}^{2} \\approx \\sum_{k=-(b-1)}^{b-1} K\\left(\\frac{k}{b}\\right) \\hat{\\gamma}_{k},\n$$\nwhere $K(x) = 1 - |x|$ for $|x| \\le 1$ is the Bartlett (or triangular) kernel. In contrast, the finite-lag truncation method corresponds to using a rectangular kernel, $K(x) = 1$ for $|x| \\le 1$.\n\nThe Bartlett kernel stabilizes the estimation of $\\sigma_{\\text{asy}}^{2} = 2\\pi f(0)$ by down-weighting the contributions from noisy, high-lag autocovariances. The weights decrease linearly from $1$ at lag $k=0$ to $0$ at lags $|k|=b$. This tapering reduces the variance of the overall estimator compared to the rectangular kernel, which gives equal weight to all lags up to the truncation point $L$. By systematically reducing the influence of the least reliable terms in the sum, the OBM method provides a more robust and statistically consistent estimate of the spectral density at zero, and therefore a more stable estimate of the MCSE.", "answer": "$$\\boxed{0.01596}$$", "id": "3372636"}, {"introduction": "A robust method for diagnosing non-convergence is to run multiple chains from dispersed initial positions and verify that they all explore the same distribution. The Gelman-Rubin diagnostic, or $\\hat{R}$, formalizes this comparison, but its true power is realized in its multivariate form, which can detect poor mixing hidden in specific combinations of parameters. This exercise guides you through the theoretical construction of a multivariate $\\hat{R}$ using linear algebra, providing deep insight into diagnosing convergence in the high-dimensional models common in modern data assimilation [@problem_id:3372662].", "problem": "Consider an inverse problem in data assimilation with a posterior distribution for a parameter vector $\\theta \\in \\mathbb{R}^p$. You run $m$ independent Markov chains, each of length $n$, targeting this posterior. Let $\\bar{\\theta}_{\\cdot j}$ denote the sample mean of chain $j \\in \\{1,\\dots,m\\}$ and $\\bar{\\theta}_{\\cdot\\cdot}$ the overall mean across chains. Let $S_j$ be the unbiased within-chain sample covariance from chain $j$. Define the within-chain covariance estimator $W = \\frac{1}{m} \\sum_{j=1}^m S_j$ and the between-chain covariance estimator $B = \\frac{n}{m-1} \\sum_{j=1}^m (\\bar{\\theta}_{\\cdot j} - \\bar{\\theta}_{\\cdot\\cdot})(\\bar{\\theta}_{\\cdot j} - \\bar{\\theta}_{\\cdot\\cdot})^\\top$. Consider the pooled covariance estimator $\\hat{V} = \\frac{n-1}{n} W + \\frac{1}{n} B$. For any nonzero $a \\in \\mathbb{R}^p$, define the scalar functional $y = a^\\top \\theta$ and the corresponding univariate within- and between-chain variance estimators $W_a = a^\\top W a$ and $B_a = a^\\top B a$, with pooled variance $\\hat{V}_a = \\frac{n-1}{n} W_a + \\frac{1}{n} B_a$.\n\nStarting from the definitions of covariance matrices and the Rayleigh quotient for symmetric matrices, reason about a multivariate potential scale reduction factor that captures the worst-case lack of convergence across all linear functionals $a^\\top \\theta$. Which of the following statements are correct? Select all that apply.\n\nA. A valid multivariate $\\hat{R}$ is given by $\\hat{R}_{\\mathrm{mv}} = \\sqrt{\\lambda_{\\max}(W^{-1} \\hat{V})}$, and it equals $\\sup_{a \\neq 0} \\hat{R}(a^\\top \\theta)$, where $\\hat{R}(a^\\top \\theta) = \\sqrt{\\hat{V}_a / W_a}$ is the univariate potential scale reduction factor for the scalar functional $a^\\top \\theta$.\n\nB. The alternative statistic $\\sqrt{\\operatorname{tr}(\\hat{V}) / \\operatorname{tr}(W)}$ upper-bounds $\\hat{R}(a^\\top \\theta)$ for every nonzero $a$, and it is invariant under any invertible linear reparameterization $\\tilde{\\theta} = T \\theta$ with $T \\in \\mathbb{R}^{p \\times p}$ invertible.\n\nC. The statistic in Option A is invariant under any invertible linear reparameterization $\\tilde{\\theta} = T \\theta$, in the sense that replacing $(W, \\hat{V})$ by $(T W T^\\top, T \\hat{V} T^\\top)$ leaves $\\lambda_{\\max}(W^{-1} \\hat{V})$ unchanged.\n\nD. The statistic $\\sqrt{ \\left( \\det \\hat{V} / \\det W \\right)^{1/p} }$ equals $\\sup_{a \\neq 0} \\hat{R}(a^\\top \\theta)$ and is therefore strictly more sensitive than Option A to poorly mixing directions.\n\nE. The leading eigenvector of $W^{-1} B$ identifies a linear functional $a^\\star$ such that $\\hat{R}(a^{\\star\\top} \\theta)$ attains the maximum over all $a \\neq 0$. In data assimilation, this connects the diagnostic to the worst-mixed linear predictions and can guide reparameterization or preconditioning to improve mixing along that direction.", "solution": "The problem statement will first be validated for scientific soundness and logical consistency.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- Parameter vector: $\\theta \\in \\mathbb{R}^p$\n- Number of independent Markov chains: $m$\n- Length of each chain: $n$\n- Sample mean of chain $j$: $\\bar{\\theta}_{\\cdot j}$ for $j \\in \\{1,\\dots,m\\}$\n- Overall sample mean: $\\bar{\\theta}_{\\cdot\\cdot}$\n- Unbiased within-chain sample covariance from chain $j$: $S_j$\n- Within-chain covariance estimator: $W = \\frac{1}{m} \\sum_{j=1}^m S_j$\n- Between-chain covariance estimator: $B = \\frac{n}{m-1} \\sum_{j=1}^m (\\bar{\\theta}_{\\cdot j} - \\bar{\\theta}_{\\cdot\\cdot})(\\bar{\\theta}_{\\cdot j} - \\bar{\\theta}_{\\cdot\\cdot})^\\top$\n- Pooled covariance estimator: $\\hat{V} = \\frac{n-1}{n} W + \\frac{1}{n} B$\n- A nonzero vector: $a \\in \\mathbb{R}^p$\n- A scalar functional: $y = a^\\top \\theta$\n- Univariate variance estimators: $W_a = a^\\top W a$, $B_a = a^\\top B a$, $\\hat{V}_a = \\frac{n-1}{n} W_a + \\frac{1}{n} B_a$\n- The task is to reason about a multivariate potential scale reduction factor that captures the worst-case lack of convergence across all linear functionals $a^\\top \\theta$.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem describes the multivariate extension of the Gelman-Rubin convergence diagnostic, often denoted $\\hat{R}$ or PSRF. The definitions of the within-chain covariance $W$, between-chain covariance $B$, and the pooled variance estimator $\\hat{V}$ are standard in the literature on MCMC diagnostics (e.g., Gelman et al., \"Bayesian Data Analysis\"; Brooks and Gelman, \"General Methods for Monitoring Convergence of Iterative Simulations\"). The concepts are fundamental to statistical computing and its application in fields like data assimilation. The problem is scientifically sound.\n- **Well-Posed:** The problem is well-defined. It asks to evaluate statements concerning a statistic that maximizes a specific ratio over all possible linear projections. This corresponds to a well-posed generalized eigenvalue problem in linear algebra, which has a unique and meaningful solution.\n- **Objective:** The problem is stated in precise mathematical language, free from any subjective or ambiguous terminology.\n- **Completeness and Consistency:** All necessary definitions and variables are provided. The relationships between the univariate and multivariate statistics are clearly laid out. The setup is internally consistent and complete.\n\n**Step 3: Verdict and Action**\nThe problem is valid. It is a well-formulated question about a standard, advanced topic in MCMC convergence diagnostics. The solution process will now proceed.\n\n### Derivation and Solution\n\nThe univariate potential scale reduction factor (PSRF) for a scalar quantity is given by $\\hat{R} = \\sqrt{\\frac{\\hat{V}}{W}}$, where $\\hat{V}$ is an estimate of the marginal posterior variance and $W$ is the average within-chain variance. For a linear functional $y = a^\\top \\theta$, the corresponding estimators for variance are $W_a = a^\\top W a$ and $\\hat{V}_a = a^\\top \\hat{V} a$. The univariate PSRF for this functional is thus:\n$$ \\hat{R}(a^\\top \\theta) = \\sqrt{\\frac{\\hat{V}_a}{W_a}} = \\sqrt{\\frac{a^\\top \\hat{V} a}{a^\\top W a}} $$\nThe problem asks for a multivariate measure that captures the \"worst-case\" lack of convergence. This corresponds to finding the linear functional $a^\\top \\theta$ that maximizes this ratio. We seek to compute:\n$$ \\hat{R}_{\\mathrm{mv}}^2 = \\sup_{a \\neq 0} \\frac{a^\\top \\hat{V} a}{a^\\top W a} $$\nThis expression is a generalized Rayleigh quotient for the pair of symmetric matrices $(\\hat{V}, W)$. Assuming the chains are not degenerate, the within-chain covariance matrix $W$ is positive definite and thus invertible. The supremum of this quotient is given by the largest eigenvalue, $\\lambda_{\\max}$, of the generalized eigenvalue problem $\\hat{V} a = \\lambda W a$. This can be rewritten as a standard eigenvalue problem:\n$$ W^{-1} \\hat{V} a = \\lambda a $$\nTherefore, the supremum is $\\lambda_{\\max}(W^{-1} \\hat{V})$. The worst-case potential scale reduction factor is the square root of this value:\n$$ \\hat{R}_{\\mathrm{mv}} = \\sqrt{\\lambda_{\\max}(W^{-1} \\hat{V})} $$\n\nWith this derivation, we can now evaluate each option.\n\n### Option-by-Option Analysis\n\n**A. A valid multivariate $\\hat{R}$ is given by $\\hat{R}_{\\mathrm{mv}} = \\sqrt{\\lambda_{\\max}(W^{-1} \\hat{V})}$, and it equals $\\sup_{a \\neq 0} \\hat{R}(a^\\top \\theta)$, where $\\hat{R}(a^\\top \\theta) = \\sqrt{\\hat{V}_a / W_a}$ is the univariate potential scale reduction factor for the scalar functional $a^\\top \\theta$.**\n\nAs shown in the derivation above, the multivariate PSRF that represents the worst-case (maximum) PSRF over all one-dimensional projections $a^\\top\\theta$ is precisely $\\sup_{a \\neq 0} \\hat{R}(a^\\top \\theta)$. We demonstrated that this supremum is equal to $\\sqrt{\\lambda_{\\max}(W^{-1} \\hat{V})}$. The statement is a direct and correct consequence of the definition of a \"worst-case\" multivariate diagnostic based on the univariate $\\hat{R}$.\n\n**Verdict: Correct.**\n\n**B. The alternative statistic $\\sqrt{\\operatorname{tr}(\\hat{V}) / \\operatorname{tr}(W)}$ upper-bounds $\\hat{R}(a^\\top \\theta)$ for every nonzero $a$, and it is invariant under any invertible linear reparameterization $\\tilde{\\theta} = T \\theta$ with $T \\in \\mathbb{R}^{p \\times p}$ invertible.**\n\nLet's test the first part: the claim that $\\frac{\\operatorname{tr}(\\hat{V})}{\\operatorname{tr}(W)} \\ge \\frac{a^\\top \\hat{V} a}{a^\\top W a}$ for all $a \\neq 0$. This is generally false. Consider a diagonal case where $W = \\begin{pmatrix} 1 & 0 \\\\ 0 & 100 \\end{pmatrix}$ and $\\hat{V} = \\begin{pmatrix} 10 & 0 \\\\ 0 & 100 \\end{pmatrix}$. We have $\\operatorname{tr}(W) = 101$ and $\\operatorname{tr}(\\hat{V}) = 110$. The ratio of traces is $\\frac{110}{101} \\approx 1.089$. Now, choose $a = (1, 0)^\\top$. The ratio $\\frac{a^\\top \\hat{V} a}{a^\\top W a} = \\frac{10}{1} = 10$. Since $10 > 1.089$, the ratio of traces does not provide an upper bound for the Rayleigh quotient.\n\nNow, let's test the second part: invariance under reparameterization $\\tilde{\\theta} = T \\theta$. The covariance matrices transform as $\\tilde{W} = T W T^\\top$ and $\\tilde{\\hat{V}} = T \\hat{V} T^\\top$. The statistic becomes $\\sqrt{\\operatorname{tr}(\\tilde{\\hat{V}}) / \\operatorname{tr}(\\tilde{W})} = \\sqrt{\\operatorname{tr}(T \\hat{V} T^\\top) / \\operatorname{tr}(T W T^\\top)}$. In general, $\\operatorname{tr}(AXA^\\top) \\neq \\operatorname{tr}(X)$. For example, if we rescale a single parameter, e.g., $T = \\mathrm{diag}(10, 1, \\dots, 1)$, the trace will change significantly, and so will the ratio. Therefore, the statistic is not invariant.\n\n**Verdict: Incorrect.**\n\n**C. The statistic in Option A is invariant under any invertible linear reparameterization $\\tilde{\\theta} = T \\theta$, in the sense that replacing $(W, \\hat{V})$ by $(T W T^\\top, T \\hat{V} T^\\top)$ leaves $\\lambda_{\\max}(W^{-1} \\hat{V})$ unchanged.**\n\nUnder the reparameterization $\\tilde{\\theta} = T \\theta$, the new covariance matrices are $\\tilde{W} = TWT^\\top$ and $\\tilde{\\hat{V}} = T\\hat{V}T^\\top$. The statistic from Option A, applied to the reparameterized problem, is $\\lambda_{\\max}((\\tilde{W})^{-1} \\tilde{\\hat{V}})$. Let's compute this matrix:\n$$ (\\tilde{W})^{-1} \\tilde{\\hat{V}} = (T W T^\\top)^{-1} (T \\hat{V} T^\\top) = ((T^\\top)^{-1} W^{-1} T^{-1}) (T \\hat{V} T^\\top) = (T^\\top)^{-1} W^{-1} (T^{-1} T) \\hat{V} T^\\top = (T^\\top)^{-1} (W^{-1} \\hat{V}) T^\\top $$\nLet $M = W^{-1} \\hat{V}$ and $P = T^\\top$. The new matrix is $P^{-1} M P$. This is a similarity transformation of the original matrix $M$. A fundamental property of similarity transformations is that they preserve eigenvalues. Therefore, the eigenvalues of $(\\tilde{W})^{-1} \\tilde{\\hat{V}}$ are identical to the eigenvalues of $W^{-1} \\hat{V}$. Consequently, their maximum eigenvalues are equal: $\\lambda_{\\max}((\\tilde{W})^{-1} \\tilde{\\hat{V}}) = \\lambda_{\\max}(W^{-1} \\hat{V})$. The statistic is indeed invariant under linear reparameterizations, which is a desirable property for a convergence diagnostic.\n\n**Verdict: Correct.**\n\n**D. The statistic $\\sqrt{ \\left( \\det \\hat{V} / \\det W \\right)^{1/p} }$ equals $\\sup_{a \\neq 0} \\hat{R}(a^\\top \\theta)$ and is therefore strictly more sensitive than Option A to poorly mixing directions.**\n\nThe first claim is that $\\sqrt{(\\det(\\hat{V}) / \\det(W))^{1/p}} = \\sup_{a \\neq 0} \\hat{R}(a^\\top \\theta)$. From Option A, we know the right-hand side is $\\sqrt{\\lambda_{\\max}(W^{-1} \\hat{V})}$. The claim is thus equivalent to $(\\det(\\hat{V}) / \\det(W))^{1/p} = \\lambda_{\\max}(W^{-1} \\hat{V})$.\nUsing the property $\\det(AB) = \\det(A)\\det(B)$, we have $\\det(\\hat{V}) / \\det(W) = \\det(\\hat{V}) \\det(W^{-1}) = \\det(W^{-1}\\hat{V})$.\nLet $M = W^{-1}\\hat{V}$. Its determinant is the product of its eigenvalues: $\\det(M) = \\prod_{i=1}^p \\lambda_i$.\nThe claim becomes $(\\prod_{i=1}^p \\lambda_i)^{1/p} = \\lambda_{\\max}$. The left side is the geometric mean of the eigenvalues of $M$. The geometric mean of a set of non-negative numbers is less than or equal to their maximum value, with equality holding only if all numbers are identical. In general, the eigenvalues will not be identical, so $(\\prod_{i=1}^p \\lambda_i)^{1/p} \\le \\lambda_{\\max}$. The first part of the statement is false.\nBecause the first part is false, the second part (\"and is therefore strictly more sensitive\") is nonsensical. In fact, by averaging over all eigendirections, the determinant-based statistic is generally *less* sensitive to a single poorly mixing direction than the maximum eigenvalue statistic.\n\n**Verdict: Incorrect.**\n\n**E. The leading eigenvector of $W^{-1} B$ identifies a linear functional $a^\\star$ such that $\\hat{R}(a^{\\star\\top} \\theta)$ attains the maximum over all $a \\neq 0$. In data assimilation, this connects the diagnostic to the worst-mixed linear predictions and can guide reparameterization or preconditioning to improve mixing along that direction.**\n\nThe vector $a^\\star$ that maximizes $\\hat{R}(a^\\top \\theta)$ is the eigenvector corresponding to the largest eigenvalue of $W^{-1}\\hat{V}$. Let's examine the relationship between the eigenvectors of $W^{-1} \\hat{V}$ and $W^{-1} B$.\nWe have the definition $\\hat{V} = \\frac{n-1}{n} W + \\frac{1}{n} B$. Pre-multiplying by $W^{-1}$ gives:\n$$ W^{-1} \\hat{V} = \\frac{n-1}{n} W^{-1}W + \\frac{1}{n} W^{-1}B = \\frac{n-1}{n} I + \\frac{1}{n} W^{-1}B $$\nwhere $I$ is the identity matrix. If $v$ is an eigenvector of $W^{-1}B$ with eigenvalue $\\mu$, then $(W^{-1}B)v = \\mu v$. It follows that:\n$$ (W^{-1}\\hat{V})v = \\left(\\frac{n-1}{n} I + \\frac{1}{n} W^{-1}B\\right)v = \\frac{n-1}{n}v + \\frac{1}{n}\\mu v = \\left(\\frac{n-1+\\mu}{n}\\right)v $$\nThis shows that $v$ is also an eigenvector of $W^{-1}\\hat{V}$, with eigenvalue $\\lambda = \\frac{n-1+\\mu}{n}$. Since this relationship is linear and increasing in $\\mu$, the eigenvector corresponding to the largest eigenvalue of $W^{-1} B$ is the same as the one corresponding to the largest eigenvalue of $W^{-1}\\hat{V}$. Thus, the leading eigenvector of $W^{-1} B$ does indeed identify the direction of worst convergence, $a^\\star$.\n\nThe second part of the statement provides the correct interpretation and application of this finding. Identifying the \"worst-mixed\" linear combination of parameters ($a^{\\star\\top} \\theta$) is extremely valuable for diagnosing MCMC performance. This direction points to the specific way in which the parameter space is difficult to explore (e.g., due to high posterior correlations). This information can then be used to improve the sampler, for instance by re-parameterizing the model to reduce these correlations or by building a more efficient proposal mechanism (preconditioning) for the MCMC algorithm. This is a key practical use of the multivariate PSRF diagnostic in complex applications like data assimilation.\n\n**Verdict: Correct.**", "answer": "$$\\boxed{ACE}$$", "id": "3372662"}, {"introduction": "Advanced samplers like Hamiltonian Monte Carlo (HMC) are powerful but have unique failure modes that require specialized diagnostics. HMC's performance is intimately linked to the physics of its simulated system, and diagnostics based on the system's total energy can reveal subtle issues with sampler tuning. In this hands-on coding challenge, you will implement and apply energy-based diagnostics like the Expected Bayesian Fraction of Missing Information (E-BFMI) to distinguish between different types of sampling problems and propose concrete improvements to the HMC algorithm [@problem_id:3372595].", "problem": "Consider a Bayesian linear inverse problem under Gaussian noise within the paradigm of Markov chain Monte Carlo (MCMC) for data assimilation. Let $y \\in \\mathbb{R}^m$ be observed data generated by the forward model $y = A \\theta + \\varepsilon$, where $A \\in \\mathbb{R}^{m \\times d}$ is a known matrix, $\\theta \\in \\mathbb{R}^d$ is the unknown parameter, and $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_m)$ is independent Gaussian noise with variance $\\sigma^2$. Assume a Gaussian prior $\\theta \\sim \\mathcal{N}(0, \\tau^2 I_d)$.\n\nThe posterior distribution $p(\\theta \\mid y)$ is proportional to $\\exp(-U(\\theta))$, where the potential energy is defined by\n$$\nU(\\theta) = \\frac{1}{2\\sigma^2}\\lVert A\\theta - y \\rVert_2^2 + \\frac{1}{2\\tau^2}\\lVert \\theta \\rVert_2^2.\n$$\nIn Hamiltonian Monte Carlo (HMC), a momentum variable $p \\in \\mathbb{R}^d$ is introduced with Gaussian kinetic energy\n$$\nK(p) = \\frac{1}{2} p^\\top M^{-1} p,\n$$\nwhere $M \\in \\mathbb{R}^{d \\times d}$ is a symmetric positive definite mass matrix. The Hamiltonian is\n$$\nH(\\theta, p) = U(\\theta) + K(p),\n$$\nand the leapfrog integrator with step size $\\epsilon$ and $L$ steps is used to propose transitions that are accepted or rejected via the Metropolis criterion.\n\nThe Expected Bayesian Fraction of Missing Information (E-BFMI) diagnostic assesses energy mobility under momentum resampling and its relationship to posterior exploration. Low energy mobility is often indicative of poor exploration of the posterior $p(\\theta \\mid y)$, particularly in the presence of model mismatch or ill-conditioning in $A$.\n\nTask:\n1. Starting from the definitions of the joint Hamiltonian system and stationarity of the Markov chain under HMC with momentum resampling, derive a computable estimator for E-BFMI expressed in terms of an empirical energy sequence $\\{E_t\\}_{t=1}^T$ where $E_t = H(\\theta_t, p_t)$ for the accepted states. Explain the interpretation of the estimator in relation to energy mobility and exploration of $p(\\theta \\mid y)$.\n2. Propose scientifically justified thresholds, derived from energy histograms and the empirical energy differences $\\Delta E_t = E_t - E_{t-1}$, to trigger either longer HMC trajectories (increasing $L$) or re-tuning (changing $M$). The thresholds must be formulated in terms of:\n   - A ratio involving the empirical variance of $E_t$ and empirical mean of $(\\Delta E_t)^2$.\n   - The fraction of near-zero energy changes, defined through a histogram-derived window relative to the empirical standard deviation of $E_t$.\n3. Implement a complete, runnable program that:\n   - Constructs the inverse problem instances with specified $A$, $\\sigma^2$, and $\\tau^2$.\n   - Runs HMC to generate a sequence of accepted states and their energies.\n   - Computes the E-BFMI estimator and the histogram-based fraction of near-zero energy changes.\n   - Uses your proposed thresholds to return a decision code for each test case:\n     - $0$: no change recommended,\n     - $1$: trigger longer trajectories,\n     - $2$: trigger mass matrix re-tuning,\n     - $3$: trigger both longer trajectories and mass matrix re-tuning.\n   - Produces a single line of output with the decision codes for the test suite as a comma-separated list enclosed in square brackets. For example, if there are four test cases, the output must be of the form $[c_1,c_2,c_3,c_4]$.\n\nUse the following test suite, each specified by $(d, m, \\sigma, \\tau, \\epsilon, L, \\text{mass}, \\text{A-spectrum}, N)$ where $N$ is the total number of iterations (including burn-in), and all scalars are real numbers:\n- Case $1$ (well-conditioned, identity mass): $(5, 5, 0.3, 1.0, 0.05, 40, \\text{I}, \\text{flat}, 2000)$, where $\\text{flat}$ means singular values of $A$ are all $1$.\n- Case $2$ (ill-conditioned, identity mass): $(20, 20, 0.1, 2.0, 0.03, 10, \\text{I}, \\text{logspace}, 2000)$, where $\\text{logspace}$ means singular values of $A$ are logarithmically spaced between $10^{-3}$ and $1$.\n- Case $3$ (ill-conditioned, posterior preconditioning): $(20, 20, 0.1, 2.0, 0.03, 10, \\text{posterior}, \\text{logspace}, 2000)$, where $\\text{posterior}$ means $M$ is set equal to the posterior covariance $(\\tau^{-2} I_d + \\sigma^{-2} A^\\top A)^{-1}$.\n- Case $4$ (boundary case, very short trajectory): $(5, 5, 0.3, 1.0, 0.005, 1, \\text{I}, \\text{flat}, 2000)$.\n\nYour program must:\n- Use Hamiltonian Monte Carlo (HMC) with momentum resampling at each iteration.\n- Use a burn-in of $500$ iterations and then compute diagnostics on the remaining iterations.\n- Base the decisions solely on energy-based diagnostics as described above.\n- Produce the final output as a single line: a comma-separated list of decision codes enclosed in square brackets.\n\nAll mathematical entities must be written in LaTeX notation in this problem statement. Angles are not involved; physical units do not apply. The final outputs are integers as defined by the decision code above.", "solution": "The problem is assessed to be valid. It is a well-posed, scientifically grounded problem in the field of computational statistics and inverse problems, free of the specified flaws. The minor ambiguity regarding the generation of synthetic data can be resolved using standard practices without contradicting the problem statement.\n\n### Part 1: Derivation and Interpretation of the E-BFMI Estimator\n\nThe problem asks for a computable estimator for the Expected Bayesian Fraction of Missing Information (E-BFMI) and its interpretation. The diagnostic is to be based on an empirical energy sequence $\\{E_t\\}_{t=1}^T$ from a Hamiltonian Monte Carlo (HMC) sampler and the corresponding energy differences $\\Delta E_t = E_t - E_{t-1}$.\n\nLet the sequence $\\{E_t\\}_{t=1}^T$ represent the total energy $H(\\theta, p)$ at the beginning of each HMC iteration, after momentum resampling. Specifically, at iteration $t$, the sampler is at state $\\theta_{t-1}$. A new momentum $p_{t-1}'$ is drawn from its Gaussian distribution, $p_{t-1}' \\sim \\mathcal{N}(0, M)$. The energy for this iteration is defined as $E_t = U(\\theta_{t-1}) + K(p_{t-1}')$. This sequence $\\{E_t\\}$ is generated for $t=1, \\dots, T$.\n\nThe problem suggests a diagnostic based on the ratio of the empirical variance of $E_t$ to the empirical mean of $(\\Delta E_t)^2$. Let us define this diagnostic as $D_1$:\n$$\nD_1 = \\frac{\\mathbb{V}[E_t]}{\\mathbb{E}[(\\Delta E_t)^2]}\n$$\nwhere $\\Delta E_t = E_t - E_{t-1}$. We can provide a statistical interpretation of this ratio. For any stationary time series $\\{x_t\\}$, there is a standard identity that relates the mean squared successive difference to the variance and the lag-$1$ autocorrelation, $\\rho_1$:\n$$\n\\mathbb{E}[(x_t - x_{t-1})^2] = \\mathbb{E}[x_t^2 - 2x_t x_{t-1} + x_{t-1}^2]\n$$\nBy stationarity, $\\mathbb{E}[x_t^2] = \\mathbb{E}[x_{t-1}^2] = \\mathbb{V}[x] + (\\mathbb{E}[x])^2$, and $\\mathbb{E}[x_t x_{t-1}] = \\text{Cov}(x_t, x_{t-1}) + (\\mathbb{E}[x])^2$. The lag-$1$ autocorrelation is $\\rho_1 = \\frac{\\text{Cov}(x_t, x_{t-1})}{\\mathbb{V}[x]}$. Substituting these gives:\n$$\n\\mathbb{E}[(x_t - x_{t-1})^2] = 2(\\mathbb{V}[x] + (\\mathbb{E}[x])^2) - 2(\\rho_1 \\mathbb{V}[x] + (\\mathbb{E}[x])^2) = 2\\mathbb{V}[x](1 - \\rho_1)\n$$\nApplying this identity to the energy sequence $\\{E_t\\}$, the diagnostic $D_1$ becomes:\n$$\nD_1 = \\frac{\\mathbb{V}[E_t]}{2\\mathbb{V}[E_t](1 - \\rho_1(E))} = \\frac{1}{2(1 - \\rho_1(E))}\n$$\nwhere $\\rho_1(E)$ is the lag-$1$ autocorrelation of the energy sequence.\n\n**Interpretation:**\nThis result directly links the diagnostic $D_1$ to the autocorrelation of the energy series.\n-   **Efficient Exploration:** An efficient HMC sampler explores the state space rapidly, leading to consecutive states that are nearly independent. In this scenario, the autocorrelation of any observable, including the energy, should be low (i.e., $\\rho_1(E) \\approx 0$). This results in $D_1 \\approx 1/2$.\n-   **Poor Exploration:** A poorly mixing sampler gets \"stuck\" in certain regions of the posterior distribution. This leads to high correlation between consecutive states, so $\\rho_1(E)$ approaches $1$. As $\\rho_1(E) \\to 1$, the denominator $2(1 - \\rho_1(E)) \\to 0$, causing the diagnostic $D_1$ to become very large.\n\nTherefore, a **large value of $D_1$ indicates low energy mobility and poor exploration** of the posterior distribution $p(\\theta \\mid y)$. This poor exploration can be caused by ill-conditioning of the posterior geometry or by HMC tuning parameters (like trajectory length) that are not well-suited to the problem.\n\nA computable estimator for $D_1$ from an empirical sequence $\\{E_t\\}_{t=1}^T$ is:\n$$\n\\widehat{D}_1 = \\frac{\\widehat{\\text{Var}}(E)}{\\widehat{\\mathbb{E}}[(\\Delta E)^2]} = \\frac{\\frac{1}{T-1} \\sum_{t=1}^T (E_t - \\bar{E})^2}{\\frac{1}{T-1} \\sum_{t=2}^T (E_t - E_{t-1})^2} = \\frac{\\sum_{t=1}^T (E_t - \\bar{E})^2}{\\sum_{t=2}^T (E_t - E_{t-1})^2}\n$$\nwhere $\\bar{E}$ is the sample mean of the sequence.\n\n### Part 2: Proposed Thresholds for HMC Tuning\n\nWe propose two diagnostics, $D_1$ as derived above, and a second diagnostic $D_2$, to guide HMC tuning. The decision logic aims to distinguish between problems requiring longer trajectories (increasing $L$) and those requiring a better mass matrix (re-tuning $M$).\n\n**Diagnostic 1 ($D_1$):** The autocorrelation diagnostic defined above. A high value of $D_1$ is a general indicator of a sampling problem. We set a threshold $T_{D1} = 5$, which corresponds to an energy autocorrelation of $\\rho_1(E) = 1 - 1/(2 \\times 5) = 0.9$. Values of $D_1 > T_{D1}$ suggest a significant sampling issue. We define a critical threshold $T_{D1,crit} = 15$ (corresponding to $\\rho_1(E) \\approx 0.967$) to indicate a severe problem that almost certainly involves geometric pathologies.\n\n**Diagnostic 2 ($D_2$):** The fraction of near-zero energy changes. This diagnostic is designed to detect random-walk behavior, which is often symptomatic of trajectories that are too short. We define a \"near-zero\" change as one whose magnitude is small relative to the overall energy fluctuation. Let $\\widehat{\\text{std}}(E)$ be the empirical standard deviation of the energy sequence. We define the change $\\Delta E_t$ as near-zero if $|\\Delta E_t| < \\alpha \\cdot \\widehat{\\text{std}}(E)$ for a small factor $\\alpha$. We choose $\\alpha=0.2$. The diagnostic $D_2$ is the fraction of such changes:\n$$\n\\widehat{D}_2 = \\frac{1}{T-1}\\sum_{t=2}^T \\mathbb{I}\\left(|\\Delta E_t| < 0.2 \\cdot \\widehat{\\text{std}}(E)\\right)\n$$\nA high value of $D_2$ indicates that the sampler is taking many small, tentative steps, characteristic of a short trajectory length $L$. We set a threshold $T_{D2} = 0.8$, meaning if over $80\\%$ of the energy jumps are \"small\", we flag an issue with trajectory length.\n\n**Decision Logic:**\nThe decision codes ($0, 1, 2, 3$) are determined by the following scientifically justified rules:\n1.  **Code $0$ (No change):** If $D_1 \\le T_{D1}$, the energy autocorrelation is acceptably low, indicating good mixing. No changes are recommended.\n2.  **Code $1$ (Increase $L$):** If $D_1 > T_{D1}$ (problem detected) and $D_2 > T_{D2}$ (symptoms of short trajectories), the primary recommendation is to increase the trajectory length $L$.\n3.  **Code $2$ (Re-tune $M$):** If $D_1 > T_{D1}$ but $D_2 \\le T_{D2}$, the sampler is struggling (high autocorrelation) but not because it's taking consistently tiny steps. This pattern is characteristic of a mismatch between the sampler's mass matrix $M$ and the posterior's geometry, causing it to get stuck and then occasionally jump. The recommendation is to re-tune $M$, for instance by using an estimate of the posterior covariance.\n4.  **Code $3$ (Both):** If the problem is extremely severe, signaled by $D_1 > T_{D1,crit}$, it indicates a pathological geometry that a simple increase in $L$ is unlikely to solve alone. In this case, we recommend re-tuning $M$ regardless of the $D_2$ value. If $D_2$ is also high, it suggests both issues are present, solidifying the recommendation for both actions. Our logic is: if $D_1 > T_{D1,crit}$, recommend re-tuning $M$. If, in addition, $D_2 > T_{D2}$, also recommend increasing $L$.\n\nThis logic provides a systematic, data-driven approach to diagnosing and addressing common HMC performance issues based on the mobility of the system's energy.\n\n### Part 3: Implementation\n\nThe following section provides the complete Python program that implements the HMC sampler, computes the diagnostics $\\widehat{D}_1$ and $\\widehat{D}_2$, and applies the proposed decision logic to the specified test cases.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the HMC diagnostics for all test cases.\n    \"\"\"\n    np.random.seed(42)  # For reproducibility\n\n    test_cases = [\n        # (d, m, sigma, tau, epsilon, L, mass_type, A_spectrum, N)\n        (5, 5, 0.3, 1.0, 0.05, 40, 'I', 'flat', 2000),\n        (20, 20, 0.1, 2.0, 0.03, 10, 'I', 'logspace', 2000),\n        (20, 20, 0.1, 2.0, 0.03, 10, 'posterior', 'logspace', 2000),\n        (5, 5, 0.3, 1.0, 0.005, 1, 'I', 'flat', 2000),\n    ]\n\n    results = []\n    for i, case in enumerate(test_cases):\n        decision_code = run_hmc_diagnostic_case(case)\n        results.append(decision_code)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef run_hmc_diagnostic_case(case_params):\n    \"\"\"\n    Sets up and runs a single HMC test case, returning a decision code.\n    \"\"\"\n    d, m, sigma, tau, epsilon, L, mass_type, A_spectrum, N = case_params\n    burn_in = 500\n\n    # 1. Construct the inverse problem instance\n    # Generate A matrix\n    U, _ = np.linalg.qr(np.random.randn(m, m))\n    V, _ = np.linalg.qr(np.random.randn(d, d))\n    if A_spectrum == 'flat':\n        s_vals = np.ones(min(d, m))\n    elif A_spectrum == 'logspace':\n        s_vals = np.logspace(-3, 0, min(d, m))\n    \n    S = np.zeros((m, d))\n    diag_len = len(s_vals)\n    S[:diag_len, :diag_len] = np.diag(s_vals)\n    A = U @ S @ V.T\n\n    # Generate synthetic data\n    theta_true = np.random.randn(d) * tau\n    noise = np.random.randn(m) * sigma\n    y = A @ theta_true + noise\n\n    # Construct mass matrix M and its inverse\n    if mass_type == 'I':\n        M = np.identity(d)\n        M_inv = np.identity(d)\n    elif mass_type == 'posterior':\n        # Posterior covariance inv: tau^-2 * I + sigma^-2 * A^T A\n        M_inv = (1 / tau**2) * np.identity(d) + (1 / sigma**2) * A.T @ A\n        M = np.linalg.inv(M_inv)\n\n    sigma_sq = sigma**2\n    tau_sq = tau**2\n    \n    # 2. Define HMC helper functions\n    def potential_energy(theta):\n        return 0.5 * np.sum((A @ theta - y)**2) / sigma_sq + 0.5 * np.sum(theta**2) / tau_sq\n\n    def grad_potential(theta):\n        return A.T @ (A @ theta - y) / sigma_sq + theta / tau_sq\n\n    # 3. Run HMC Sampler\n    theta_current = np.random.randn(d) * tau\n    energy_sequence = []\n\n    for i in range(N):\n        # Resample momentum\n        p_current = np.random.multivariate_normal(np.zeros(d), M)\n\n        # Record energy at start of trajectory\n        E_start = potential_energy(theta_current) + 0.5 * p_current.T @ M_inv @ p_current\n        energy_sequence.append(E_start)\n        \n        # Leapfrog integration\n        theta_prop = theta_current.copy()\n        p_prop = p_current.copy()\n        \n        # Half step for momentum\n        p_prop -= 0.5 * epsilon * grad_potential(theta_prop)\n        \n        for _ in range(L):\n            # Full step for position\n            theta_prop += epsilon * (M_inv @ p_prop)\n            # Full step for momentum (except last step)\n            grad = grad_potential(theta_prop)\n            p_prop -= epsilon * grad\n        \n        # Correct last half step for momentum\n        p_prop += 0.5 * epsilon * grad\n        \n        # Metropolis-Hastings acceptance step\n        E_prop = potential_energy(theta_prop) + 0.5 * p_prop.T @ M_inv @ p_prop\n        \n        log_alpha = E_start - E_prop\n        if np.log(np.random.rand()) < log_alpha:\n            theta_current = theta_prop\n\n    # 4. Compute diagnostics on post-burn-in samples\n    post_burn_energies = np.array(energy_sequence[burn_in:])\n    \n    # Diagnostic 1: D1 = Var(E) / E[(Delta E)^2]\n    var_E = np.var(post_burn_energies, ddof=1)\n    delta_E = np.diff(post_burn_energies)\n    mean_delta_E_sq = np.mean(delta_E**2)\n    \n    # Avoid division by zero if energies are constant\n    if mean_delta_E_sq < 1e-10:\n        D1 = np.inf\n    else:\n        D1 = var_E / mean_delta_E_sq\n\n    # Diagnostic 2: D2 = Fraction of near-zero energy changes\n    std_E = np.sqrt(var_E)\n    window = 0.2 * std_E\n    num_near_zero = np.sum(np.abs(delta_E) < window)\n    D2 = num_near_zero / len(delta_E)\n\n    # 5. Apply decision logic\n    T_D1 = 5.0\n    T_D1_crit = 15.0\n    T_D2 = 0.8\n\n    if D1 <= T_D1:\n        return 0  # No change recommended\n\n    recommend_L = (D2 > T_D2)\n    # If D1 is high, M is a suspect unless short L is a clear culprit.\n    # If D2 is low, M is the primary suspect.\n    recommend_M = not recommend_L\n    \n    # If D1 is critically high, M is definitely an issue.\n    if D1 > T_D1_crit:\n        recommend_M = True\n\n    if recommend_L and recommend_M:\n        return 3 # Trigger both\n    elif recommend_L:\n        return 1 # Trigger longer trajectories\n    elif recommend_M:\n        return 2 # Trigger mass matrix re-tuning\n    else: # Should not be reached with this logic but as a fallback\n        return 0\n\nsolve()\n\n```", "id": "3372595"}]}