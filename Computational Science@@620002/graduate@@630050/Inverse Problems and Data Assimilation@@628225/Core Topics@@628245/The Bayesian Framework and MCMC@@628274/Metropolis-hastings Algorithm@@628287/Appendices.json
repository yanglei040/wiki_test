{"hands_on_practices": [{"introduction": "A common challenge in Bayesian inference is that model parameters are often constrained. For instance, a standard deviation parameter $\\sigma$ must be positive. This practice explores a fundamental technique for handling such constraints: transforming the parameter to an unconstrained space and applying the Metropolis-Hastings algorithm there. You will derive the acceptance probability for an MCMC sampler targeting a transformed parameter, paying close attention to the crucial role of the Jacobian correction that arises from the change of variables [@problem_id:3402720].", "problem": "In a one-step sequential data assimilation setting, consider a forward model with fixed state estimate producing residuals relative to observations that are modeled as independent and identically distributed Gaussian noise with unknown standard deviation $\\sigma \\in (0,\\infty)$. Let the number of residuals be $n$, and denote by $S$ the sum of squared residuals, that is, $S = \\sum_{i=1}^{n} r_i^2$. Assume the likelihood for the residuals given $\\sigma$ is Gaussian and that the prior on $\\sigma$ is the Jeffreys prior, $p(\\sigma) \\propto \\sigma^{-1}$.\n\nTo construct a Markov Chain Monte Carlo (MCMC) update using the Metropolis–Hastings (MH) algorithm, enforce the positivity constraint by the transformation $u = \\ln(\\sigma)$ so that $u \\in \\mathbb{R}$. Use a Gaussian random-walk proposal in the transformed space, $q(u' \\mid u) = \\mathcal{N}(u, s^2)$, with some fixed $s > 0$. Starting from the fundamental definitions of Bayes’ theorem for the posterior density, the Gaussian likelihood for independent residuals, and the change-of-variables formula for probability densities, derive the expression for the MH acceptance probability $\\alpha(u \\rightarrow u')$ in the transformed variable $u$, explicitly accounting for the Jacobian of the transformation. Do not assume any specific $s$ beyond symmetry of the proposal in $u$.\n\nThen, evaluate the acceptance probability numerically for $n = 10$, $S = 25$, current $u = 0$ and proposed $u' = -0.2$. Round your final numerical answer for the acceptance probability to four significant figures.", "solution": "This solution proceeds in two parts as requested: first, the analytical derivation of the acceptance probability, and second, its numerical evaluation.\n\nPart 1: Derivation of the Metropolis-Hastings Acceptance Probability\n\nThe goal is to sample from the posterior distribution of the parameter $u = \\ln(\\sigma)$ using the Metropolis-Hastings (MH) algorithm. The posterior density, which is the target density for the MCMC sampler, is denoted by $\\pi(u)$. According to Bayes' theorem, the posterior is proportional to the product of the likelihood and the prior.\n\nFirst, we define the likelihood and the prior in terms of the parameter $\\sigma$.\nThe residuals, $r_i$ for $i=1, \\dots, n$, are modeled as independent and identically distributed (i.i.d.) draws from a Gaussian distribution with mean $0$ and variance $\\sigma^2$. The likelihood function is the joint probability density of the residuals given $\\sigma$:\n$$p(\\mathbf{r} | \\sigma) = \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{r_i^2}{2\\sigma^2}\\right)$$\n$$p(\\mathbf{r} | \\sigma) = (2\\pi\\sigma^2)^{-n/2} \\exp\\left(-\\frac{1}{2\\sigma^2}\\sum_{i=1}^{n} r_i^2\\right)$$\nUsing the given definition $S = \\sum_{i=1}^{n} r_i^2$, the likelihood becomes:\n$$p(\\mathbf{r} | \\sigma) = (2\\pi)^{-n/2} (\\sigma^2)^{-n/2} \\exp\\left(-\\frac{S}{2\\sigma^2}\\right)$$\nDropping the constant factors, the likelihood is proportional to:\n$$p(\\mathbf{r} | \\sigma) \\propto \\sigma^{-n} \\exp\\left(-\\frac{S}{2\\sigma^2}\\right)$$\nThe prior on $\\sigma$ is specified as the Jeffreys prior for a scale parameter:\n$$p(\\sigma) \\propto \\sigma^{-1}$$\nThe posterior density for $\\sigma$, $p(\\sigma | \\mathbf{r})$, is then proportional to the product of the likelihood and the prior:\n$$p(\\sigma | \\mathbf{r}) \\propto p(\\mathbf{r} | \\sigma) p(\\sigma)$$\n$$p(\\sigma | \\mathbf{r}) \\propto \\left[ \\sigma^{-n} \\exp\\left(-\\frac{S}{2\\sigma^2}\\right) \\right] \\cdot \\sigma^{-1}$$\n$$p(\\sigma | \\mathbf{r}) \\propto \\sigma^{-(n+1)} \\exp\\left(-\\frac{S}{2\\sigma^2}\\right)$$\nNext, we transform this density from the variable $\\sigma$ to the variable $u = \\ln(\\sigma)$. The inverse transformation is $\\sigma = \\exp(u)$. The change of variables formula for probability densities requires the Jacobian of the transformation:\n$$|\\frac{d\\sigma}{du}| = |\\frac{d}{du}(\\exp(u))| = |\\exp(u)| = \\exp(u)$$\nThe posterior density for $u$, denoted $\\pi(u)$, is obtained by substituting $\\sigma=\\exp(u)$ into the expression for $p(\\sigma | \\mathbf{r})$ and multiplying by the Jacobian:\n$$\\pi(u) \\propto p(\\sigma(u) | \\mathbf{r}) \\left|\\frac{d\\sigma}{du}\\right|$$\n$$\\pi(u) \\propto \\left[ (\\exp(u))^{-(n+1)} \\exp\\left(-\\frac{S}{2(\\exp(u))^2}\\right) \\right] \\cdot \\exp(u)$$\n$$\\pi(u) \\propto \\exp(-u(n+1)) \\exp\\left(-\\frac{S}{2\\exp(2u)}\\right) \\exp(u)$$\n$$\\pi(u) \\propto \\exp(-un - u + u) \\exp\\left(-\\frac{S}{2\\exp(2u)}\\right)$$\n$$\\pi(u) \\propto \\exp(-un) \\exp\\left(-\\frac{S}{2\\exp(2u)}\\right)$$\nThis is the unnormalized target posterior density for the transformed parameter $u$.\n\nThe MH algorithm acceptance probability for a move from a current state $u$ to a proposed state $u'$ is given by:\n$$\\alpha(u \\rightarrow u') = \\min\\left(1, \\frac{\\pi(u') q(u | u')}{\\pi(u) q(u' | u)}\\right)$$\nThe problem specifies a Gaussian random-walk proposal, $q(u'|u) = \\mathcal{N}(u, s^2)$, which has the form:\n$$q(u'|u) = \\frac{1}{\\sqrt{2\\pi s^2}} \\exp\\left(-\\frac{(u'-u)^2}{2s^2}\\right)$$\nThis proposal is symmetric, meaning $q(u'|u) = q(u|u')$, because $(u'-u)^2 = (u-u')^2$. Therefore, the ratio of proposal densities $q(u|u')/q(u'|u)$ is equal to $1$. The acceptance probability simplifies to:\n$$\\alpha(u \\rightarrow u') = \\min\\left(1, \\frac{\\pi(u')}{\\pi(u)}\\right)$$\nWe now compute the ratio of the posterior densities:\n$$\\frac{\\pi(u')}{\\pi(u)} = \\frac{\\exp(-u'n) \\exp\\left(-\\frac{S}{2\\exp(2u')}\\right)}{\\exp(-un) \\exp\\left(-\\frac{S}{2\\exp(2u)}\\right)}$$\n$$\\frac{\\pi(u')}{\\pi(u)} = \\exp(-u'n - (-un)) \\cdot \\exp\\left(-\\frac{S}{2\\exp(2u')} - \\left(-\\frac{S}{2\\exp(2u)}\\right)\\right)$$\n$$\\frac{\\pi(u')}{\\pi(u)} = \\exp(n(u-u')) \\cdot \\exp\\left(\\frac{S}{2}\\left(\\frac{1}{\\exp(2u)} - \\frac{1}{\\exp(2u')}\\right)\\right)$$\nCombining the terms inside the exponential gives the final expression for the ratio:\n$$\\frac{\\pi(u')}{\\pi(u)} = \\exp\\left( n(u-u') + \\frac{S}{2}\\left(\\exp(-2u) - \\exp(-2u')\\right) \\right)$$\nThus, the MH acceptance probability is:\n$$\\alpha(u \\rightarrow u') = \\min\\left(1, \\exp\\left( n(u-u') + \\frac{S}{2}\\left(\\exp(-2u) - \\exp(-2u')\\right) \\right)\\right)$$\n\nPart 2: Numerical Evaluation\n\nWe are given the following numerical values:\nNumber of residuals, $n = 10$.\nSum of squared residuals, $S = 25$.\nCurrent state, $u = 0$.\nProposed state, $u' = -0.2$.\n\nLet's compute the argument of the exponential function in the acceptance ratio, which we can call $A$:\n$$A = n(u-u') + \\frac{S}{2}\\left(\\exp(-2u) - \\exp(-2u')\\right)$$\nSubstituting the given values:\n$$A = 10(0 - (-0.2)) + \\frac{25}{2}\\left(\\exp(-2 \\cdot 0) - \\exp(-2 \\cdot (-0.2))\\right)$$\n$$A = 10(0.2) + 12.5\\left(\\exp(0) - \\exp(0.4)\\right)$$\n$$A = 2 + 12.5\\left(1 - \\exp(0.4)\\right)$$\nUsing a calculator for $\\exp(0.4)$:\n$$\\exp(0.4) \\approx 1.4918247$$\nSubstitute this value back into the expression for $A$:\n$$A \\approx 2 + 12.5(1 - 1.4918247)$$\n$$A \\approx 2 + 12.5(-0.4918247)$$\n$$A \\approx 2 - 6.14780875$$\n$$A \\approx -4.14780875$$\nThe acceptance ratio is $\\exp(A)$:\n$$\\exp(A) \\approx \\exp(-4.14780875) \\approx 0.015800445$$\nThe acceptance probability is the minimum of $1$ and this value:\n$$\\alpha(u \\rightarrow u') = \\min(1, 0.015800445) = 0.015800445$$\nThe problem requires rounding the final answer to four significant figures.\nThe value $0.015800445$ rounded to four significant figures is $0.01580$.", "answer": "$$\\boxed{0.01580}$$", "id": "3402720"}, {"introduction": "Real-world inverse problems often involve uncertainty not just in continuous parameters but in the underlying model structure itself. This exercise tackles a mixed discrete-continuous problem, where we must infer both the continuous parameters of a fault slip model and a discrete variable indicating the fault's very existence. You will design a sophisticated hybrid sampler that combines Gibbs sampling with a blocked Metropolis-Hastings update, a powerful technique that hinges on the analytical calculation of the marginal likelihood, or model evidence [@problem_id:3402753].", "problem": "Consider a linear observation model for a fault-slip inverse problem with a mixed discrete–continuous latent structure. Let the latent indicator be $z \\in \\{0,1\\}$ denoting the presence ($z=1$) or absence ($z=0$) of a fault. Let the continuous slip parameters be $\\theta \\in \\mathbb{R}^d$. Observations are $y \\in \\mathbb{R}^n$. The forward operator is linear: for a given design matrix $A \\in \\mathbb{R}^{n \\times d}$, the data model is\n$$\ny \\mid z,\\theta \\sim \\mathcal{N}\\!\\left(z\\,A\\theta,\\ \\Gamma\\right),\n$$\nwhere the data covariance is $\\Gamma = \\sigma^2 I_n$, with known noise standard deviation $\\sigma > 0$. The prior on $\\theta$ depends on $z$:\n$$\n\\theta \\mid z \\sim \\mathcal{N}\\!\\left(0,\\ C_z\\right),\n$$\nwith $C_1 = C$ a given positive definite covariance and $C_0 = \\varepsilon C$ with a small shrinkage factor $\\varepsilon \\in (0,1]$. The prior for $z$ is $\\mathbb{P}(z=1)=\\pi$ and $\\mathbb{P}(z=0)=1-\\pi$, with $\\pi \\in (0,1)$. All quantities are in purely mathematical units; no physical units are required.\n\nYou will combine a blocked Metropolis–Hastings (MH) update within a Gibbs sampler to target the joint posterior $p(z,\\theta \\mid y)$. Specifically, consider the following blocked proposal for $(z,\\theta)$ at fixed $y$:\n- Propose $z' \\sim q_z(z' \\mid z)$, where $q_z$ is a proposal on $\\{0,1\\}$ that, with probability $p_{\\text{flip}} \\in (0,1]$, flips $z$ and otherwise stays; assume $q_z$ is symmetric so that $q_z(1\\mid 0) = q_z(0\\mid 1)$.\n- Given the proposed $z'$, draw $\\theta' \\sim p(\\theta \\mid z', y)$, the exact conditional posterior of $\\theta$ under model $z'$.\n\nTask 1 (derivation of acceptance using fundamentals): Starting only from the definitions of the Metropolis–Hastings acceptance probability, Bayes’ rule, and standard Gaussian identities (completing the square for multivariate normal densities), derive the acceptance probability for the blocked proposal $(z,\\theta) \\to (z',\\theta')$. Show that it simplifies to a function of the model prior $p(z)$ and the marginal likelihood $p(y \\mid z)$ for each $z \\in \\{0,1\\}$, together with the proposal symmetry. Your derivation must not assume the final form in advance; it must start from the unnormalized joint posterior $p(y \\mid z,\\theta)\\,p(\\theta \\mid z)\\,p(z)$ and the proposal density $q_z(z' \\mid z)\\,p(\\theta' \\mid z',y)$, and proceed by explicit algebraic cancellation using Bayes’ rule.\n\nTask 2 (marginal likelihood under Gaussian integration): Using only well-established Gaussian integral results, derive a closed-form expression for the marginal likelihood $p(y \\mid z)$ by analytically integrating out $\\theta$ from $p(y \\mid z,\\theta)\\,p(\\theta \\mid z)$. Express the result in terms of a covariance $S_z$ and show how $S_z$ depends on $\\Gamma$, $A$, and $C_z$. Your derivation must proceed from first principles by completing the square and evaluating the Gaussian integral, and must not assume the answer.\n\nTask 3 (implementation and evaluation on a test suite): Implement a program that evaluates the posterior probability $\\mathbb{P}(z=1 \\mid y)$ for multiple test cases using the derived expressions. You must construct $A$ deterministically by\n$$\nn = 25,\\quad d = 2,\\quad A_{i,1} = 1,\\quad A_{i,2} = \\frac{i+1}{n},\\quad \\text{for } i=0,1,\\dots,n-1,\n$$\nso that $A \\in \\mathbb{R}^{25 \\times 2}$. Use\n$$\nC = \\mathrm{diag}(1,1),\\quad \\varepsilon = 10^{-6},\\quad \\pi = 0.5.\n$$\nFor each test case, form $y$ deterministically as specified below. For a given noise level $\\sigma$, define $\\Gamma = \\sigma^2 I_n$. The test suite is:\n- Case A (clear absence): $\\sigma = 0.1$, $y = 0 \\in \\mathbb{R}^{25}$.\n- Case B (clear presence): $\\sigma = 0.1$, let $\\theta^\\star = \\begin{bmatrix}1.5 \\\\ -0.7\\end{bmatrix}$ and set $y = A\\,\\theta^\\star \\in \\mathbb{R}^{25}$.\n- Case C (ambiguous, high noise): $\\sigma = 1.0$, let $\\theta^\\dagger = \\begin{bmatrix}1.0 \\\\ 0.5\\end{bmatrix}$ and set $y = 0.5\\,A\\,\\theta^\\dagger \\in \\mathbb{R}^{25}$.\n\nYour program must, for each case, compute the scalar value $\\mathbb{P}(z=1 \\mid y)$ implied by your Task 1–2 derivations. No random sampling is required for the final numerical answers. The final output format must be a single line containing the results for Cases A, B, and C, as a comma-separated list enclosed in square brackets, for example, $[r_A,r_B,r_C]$, where each $r_\\cdot$ is a floating-point number in standard decimal notation.\n\nDesign goals for the test suite:\n- The three cases test, respectively, a low-signal regime favoring $z=0$, a high-signal regime favoring $z=1$, and an ambiguous regime where the posterior mass is not concentrated.\n- Your implementation must be numerically stable for all three cases, using appropriate linear algebra for the evaluation of multivariate normal densities.\n\nYour submission must be a complete, runnable program that produces exactly one line of output in the format specified above, with no additional text.", "solution": "The solution is presented in three parts, corresponding to the three tasks in the problem description.\n\n### Task 1: Derivation of the Metropolis–Hastings Acceptance Probability\n\nThe goal is to derive the acceptance probability $\\alpha$ for a blocked Metropolis-Hastings (MH) update from state $(z, \\theta)$ to $(z', \\theta')$. The sampler targets the joint posterior distribution $p(z, \\theta \\mid y)$, which is proportional to the joint distribution of all variables:\n$$\np(z, \\theta \\mid y) \\propto p(y \\mid z, \\theta)\\,p(\\theta \\mid z)\\,p(z)\n$$\nThe proposal distribution for the transition $(z, \\theta) \\to (z', \\theta')$ is defined in two steps:\n1.  Propose a new model indicator $z' \\sim q_z(z' \\mid z)$.\n2.  Draw a new parameter vector $\\theta'$ from its exact conditional posterior under the proposed model $z'$, i.e., $\\theta' \\sim p(\\theta' \\mid z', y)$.\n\nThe full proposal distribution is therefore $Q((z', \\theta') \\mid (z, \\theta)) = q_z(z' \\mid z)\\,p(\\theta' \\mid z', y)$.\n\nThe MH acceptance probability $\\alpha((z, \\theta), (z', \\theta'))$ is given by:\n$$\n\\alpha = \\min\\left(1, \\frac{p(z', \\theta' \\mid y)\\,Q((z, \\theta) \\mid (z', \\theta'))}{p(z, \\theta \\mid y)\\,Q((z', \\theta') \\mid (z, \\theta))}\\right)\n$$\nWe express the posterior $p(z, \\theta \\mid y)$ in terms of the prior and likelihood, noting that the normalization constant $p(y)$ will cancel in the ratio. The proposal for the reverse move $(z', \\theta') \\to (z, \\theta)$ is $Q((z, \\theta) \\mid (z', \\theta')) = q_z(z \\mid z')\\,p(\\theta \\mid z, y)$. Substituting these expressions into the ratio gives:\n$$\n\\text{Ratio} = \\frac{p(y \\mid z', \\theta')\\,p(\\theta' \\mid z')\\,p(z')}{p(y \\mid z, \\theta)\\,p(\\theta \\mid z)\\,p(z)} \\times \\frac{q_z(z \\mid z')\\,p(\\theta \\mid z, y)}{q_z(z' \\mid z)\\,p(\\theta' \\mid z', y)}\n$$\nThe crucial step is to use Bayes' rule to rewrite the conditional posteriors for $\\theta$. The conditional posterior $p(\\theta \\mid z, y)$ is given by:\n$$\np(\\theta \\mid z, y) = \\frac{p(y \\mid z, \\theta)\\,p(\\theta \\mid z)}{p(y \\mid z)}\n$$\nwhere $p(y \\mid z) = \\int p(y \\mid z, \\theta)\\,p(\\theta \\mid z)\\,d\\theta$ is the marginal likelihood or model evidence. An analogous expression holds for $p(\\theta' \\mid z', y)$.\n\nSubstituting these into the ratio:\n$$\n\\text{Ratio} = \\frac{p(y \\mid z', \\theta')\\,p(\\theta' \\mid z')\\,p(z')}{p(y \\mid z, \\theta)\\,p(\\theta \\mid z)\\,p(z)} \\times \\frac{q_z(z \\mid z')}{q_z(z' \\mid z)} \\times \\frac{\\frac{p(y \\mid z, \\theta)\\,p(\\theta \\mid z)}{p(y \\mid z)}}{\\frac{p(y \\mid z', \\theta')\\,p(\\theta' \\mid z')}{p(y \\mid z')}}\n$$\nWe can now cancel terms. The terms $p(y \\mid z, \\theta)$ and $p(\\theta \\mid z)$ in the denominator of the first fraction cancel with the same terms in the numerator of the second fraction. Similarly, $p(y \\mid z', \\theta')$ and $p(\\theta' \\mid z')$ cancel. This leaves:\n$$\n\\text{Ratio} = \\frac{p(z')}{p(z)} \\times \\frac{q_z(z \\mid z')}{q_z(z' \\mid z)} \\times \\frac{1/p(y \\mid z)}{1/p(y \\mid z')} = \\frac{p(z')\\,p(y \\mid z')}{p(z)\\,p(y \\mid z)} \\times \\frac{q_z(z \\mid z')}{q_z(z' \\mid z)}\n$$\nThe problem states that the proposal $q_z$ is symmetric, meaning $q_z(z' \\mid z) = q_z(z \\mid z')$. Therefore, the proposal ratio is $1$. The acceptance ratio simplifies to:\n$$\n\\text{Ratio} = \\frac{p(y \\mid z')\\,p(z')}{p(y \\mid z)\\,p(z)}\n$$\nThe final acceptance probability is:\n$$\n\\alpha = \\min\\left(1, \\frac{p(y \\mid z')\\,p(z')}{p(y \\mid z)\\,p(z)}\\right)\n$$\nThis result demonstrates that the acceptance probability for this blocked proposal depends only on the model indicator $z$ and not on the continuous parameters $\\theta$, as required. The derivation proceeds from first principles without assuming the final form.\n\n### Task 2: Derivation of the Marginal Likelihood\n\nThe marginal likelihood $p(y \\mid z)$ is obtained by integrating the product of the likelihood and the prior over all possible values of $\\theta$:\n$$\np(y \\mid z) = \\int_{\\mathbb{R}^d} p(y \\mid z, \\theta)\\,p(\\theta \\mid z)\\,d\\theta\n$$\nThe components are the likelihood $p(y \\mid z, \\theta) = \\mathcal{N}(y; zA\\theta, \\Gamma)$ and the prior $p(\\theta \\mid z) = \\mathcal{N}(\\theta; 0, C_z)$. This is a standard linear-Gaussian model structure. The marginal distribution of $y$ given $z$ is also Gaussian. We can derive this from first principles by analyzing the exponent of the joint density $p(y, \\theta \\mid z) = p(y \\mid z, \\theta)p(\\theta \\mid z)$. Ignoring normalization constants, the density is proportional to $\\exp(-\\frac{1}{2} E)$, where the exponent $E$ is:\n$$\nE = (y - zA\\theta)^T \\Gamma^{-1} (y - zA\\theta) + \\theta^T C_z^{-1} \\theta\n$$\nExpanding and collecting terms in $\\theta$:\n$$\nE = y^T\\Gamma^{-1}y - 2z y^T\\Gamma^{-1}A\\theta + z^2 \\theta^T A^T \\Gamma^{-1} A \\theta + \\theta^T C_z^{-1} \\theta\n$$\nUsing $z^2 = z$ (since $z \\in \\{0,1\\}$), the terms quadratic and linear in $\\theta$ are:\n$$\n\\theta^T(z A^T \\Gamma^{-1} A + C_z^{-1})\\theta - 2z \\theta^T A^T \\Gamma^{-1} y\n$$\nThis is a quadratic form in $\\theta$. Completing the square identifies the mean $\\mu_\\theta$ and precision $P_\\theta$ of the posterior for $\\theta$, $p(\\theta \\mid y, z) = \\mathcal{N}(\\mu_\\theta, P_\\theta)$, as $P_\\theta^{-1} = z A^T \\Gamma^{-1} A + C_z^{-1}$ and $\\mu_\\theta = P_\\theta (z A^T \\Gamma^{-1} y)$.\n\nIntegrating over $\\theta$ is equivalent to using the standard identity for linear-Gaussian models: if $\\theta \\sim \\mathcal{N}(0, C_z)$ and $y \\mid \\theta \\sim \\mathcal{N}(zA\\theta, \\Gamma)$, then the marginal distribution of $y$ is $y \\mid z \\sim \\mathcal{N}(\\mu_y, S_z)$ where:\n$$\n\\mu_y = zA \\cdot \\mathbb{E}[\\theta] = zA \\cdot 0 = 0\n$$\n$$\nS_z = (zA) \\mathrm{Cov}(\\theta) (zA)^T + \\mathrm{Cov}(y \\mid \\theta) = z^2 A C_z A^T + \\Gamma\n$$\nThus, $p(y \\mid z) = \\mathcal{N}(y; 0, S_z)$. We now evaluate this for $z=0$ and $z=1$:\n-   For $z=1$: $S_1 = 1^2 A C_1 A^T + \\Gamma = A C_1 A^T + \\Gamma$.\n-   For $z=0$: $S_0 = 0^2 A C_0 A^T + \\Gamma = \\Gamma$.\n\nSo, the marginal likelihoods are:\n$$\np(y \\mid z=1) = \\mathcal{N}(y; 0, A C_1 A^T + \\Gamma)\n$$\n$$\np(y \\mid z=0) = \\mathcal{N}(y; 0, \\Gamma)\n$$\nThis completes the derivation. The covariance $S_z$ depends on $\\Gamma, A, C_z$ as shown.\n\n### Task 3: Implementation and Evaluation\n\nThe final task is to compute the posterior probability $\\mathbb{P}(z=1 \\mid y)$ for three test cases. Using Bayes' rule for the discrete variable $z$:\n$$\n\\mathbb{P}(z=1 \\mid y) = \\frac{p(y \\mid z=1)\\,\\mathbb{P}(z=1)}{p(y \\mid z=1)\\,\\mathbb{P}(z=1) + p(y \\mid z=0)\\,\\mathbb{P}(z=0)}\n$$\nLet $L_1 = p(y \\mid z=1)$, $L_0 = p(y \\mid z=0)$, $\\pi = \\mathbb{P}(z=1)$. The expression is:\n$$\n\\mathbb{P}(z=1 \\mid y) = \\frac{L_1 \\pi}{L_1 \\pi + L_0 (1-\\pi)} = \\frac{1}{1 + \\frac{L_0(1-\\pi)}{L_1 \\pi}}\n$$\nTo maintain numerical stability, we compute the log of the ratio $L_1/L_0$. The log-pdf of a multivariate normal distribution $\\mathcal{N}(x; \\mu, \\Sigma)$ is $\\log p(x) = -\\frac{k}{2}\\log(2\\pi) - \\frac{1}{2}\\log|\\Sigma| - \\frac{1}{2}(x-\\mu)^T\\Sigma^{-1}(x-\\mu)$.\nHere, the mean is $0$, so $\\log L_z = -\\frac{n}{2}\\log(2\\pi) - \\frac{1}{2}\\log|S_z| - \\frac{1}{2}y^T S_z^{-1} y$.\n\nFor $z=0$, $S_0 = \\Gamma = \\sigma^2 I_n$.\n$\\log L_0 = -\\frac{n}{2}\\log(2\\pi) - \\frac{1}{2}\\log|\\sigma^2 I_n| - \\frac{1}{2}y^T (\\sigma^2 I_n)^{-1} y = -\\frac{n}{2}\\log(2\\pi) - \\frac{n}{2}\\log(\\sigma^2) - \\frac{1}{2\\sigma^2}y^T y$.\n\nFor $z=1$, $S_1 = A C_1 A^T + \\sigma^2 I_n$. Since $C_1=C=I_d$, $S_1 = AA^T + \\sigma^2 I_n$. Direct computation of the determinant and inverse of this $n \\times n$ matrix is inefficient. We use the matrix determinant lemma and the Woodbury matrix identity for a numerically stable approach.\n-   $\\log|S_1| = \\log|AA^T + \\sigma^2 I_n| = \\log((\\sigma^2)^{n-d}|A^T A + \\sigma^2 I_d|) = (n-d)\\log(\\sigma^2) + \\log|A^T A + \\sigma^2 I_d|$.\n-   $y^T S_1^{-1} y = y^T (AA^T + \\sigma^2 I_n)^{-1} y = \\frac{1}{\\sigma^2}\\left(y^T y - (A^T y)^T(A^T A + \\sigma^2 I_d)^{-1}(A^T y)\\right)$.\n\nThe required log-likelihood ratio is $\\log(L_1/L_0) = \\log L_1 - \\log L_0$. After substitution and cancellation of common terms:\n$$\n\\log\\left(\\frac{L_1}{L_0}\\right) = \\frac{1}{2} \\left( d\\log(\\sigma^2) - \\log|A^T A + \\sigma^2 I_d| + \\frac{1}{\\sigma^2}(A^T y)^T(A^T A + \\sigma^2 I_d)^{-1}(A^T y) \\right)\n$$\nThis expression avoids large matrix operations, depending only on the $d \\times d$ matrix $A^T A + \\sigma^2 I_d$. The quadratic form is computed by solving a small linear system. Given $\\pi = 0.5$, the posterior probability is $\\mathbb{P}(z=1 \\mid y) = \\frac{1}{1 + \\exp(-\\log(L_1/L_0))}$. The following program implements this calculation for the specified test suite.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the posterior probability P(z=1|y) for three test cases.\n    \"\"\"\n    # Define problem constants\n    n = 25\n    d = 2\n    pi = 0.5\n    \n    # Construct the design matrix A\n    i_vals = np.arange(n)\n    A = np.zeros((n, d))\n    A[:, 0] = 1.0\n    A[:, 1] = (i_vals + 1) / n\n    \n    # Pre-compute A^T * A\n    ATA = A.T @ A\n\n    # Define the test cases from the problem statement.\n    theta_star = np.array([1.5, -0.7])\n    theta_dagger = np.array([1.0, 0.5])\n    \n    test_cases = [\n        # Case A (clear absence)\n        {'sigma': 0.1, 'y': np.zeros(n)},\n        # Case B (clear presence)\n        {'sigma': 0.1, 'y': A @ theta_star},\n        # Case C (ambiguous, high noise)\n        {'sigma': 1.0, 'y': 0.5 * (A @ theta_dagger)},\n    ]\n\n    results = []\n    for case in test_cases:\n        sigma = case['sigma']\n        y = case['y']\n        \n        sigma2 = sigma**2\n        \n        # Calculate log(L1/L0), the log Bayes Factor for model z=1 vs z=0.\n        \n        # 1. Form the d x d matrix M = A^T*A + sigma^2 * I\n        M = ATA + sigma2 * np.eye(d)\n        \n        # 2. Calculate the \"Occam's factor\" or complexity penalty term.\n        # This term comes from the ratio of determinants of the marginal likelihoods.\n        sign_det, log_det_M = np.linalg.slogdet(M)\n        log_occams_factor = 0.5 * (d * np.log(sigma2) - log_det_M)\n\n        # 3. Calculate the \"fit term\".\n        # This term measures how well the data y are explained by the model z=1.\n        ATy = A.T @ y\n        # To compute (ATy)^T * M_inv * ATy, we solve M*x = ATy for x,\n        # then compute dot product (ATy)^T * x. This is more stable than inversion.\n        x = np.linalg.solve(M, ATy)\n        fit_term_quadratic = ATy @ x\n        log_fit_term = 0.5 / sigma2 * fit_term_quadratic\n        \n        # The full log Bayes Factor\n        log_L1_div_L0 = log_occams_factor + log_fit_term\n        \n        # 4. Compute the posterior probability P(z=1|y).\n        # Since the prior P(z=1) = pi = 0.5, the posterior odds are equal to the Bayes Factor.\n        # P(z=1|y) = O_10 / (1 + O_10), where O_10 = L1/L0.\n        # This is equivalent to the logistic (sigmoid) function of the log-odds.\n        prob_z1 = 1.0 / (1.0 + np.exp(-log_L1_div_L0))\n        \n        results.append(prob_z1)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3402753"}, {"introduction": "When inverse problems are constrained by partial differential equations (PDEs), the computational cost of the forward model can render standard MCMC methods impractical. This practice introduces the multi-level Metropolis-Hastings algorithm, an advanced method for accelerating inference by coupling cheap, coarse-grid models with expensive, fine-grid models. You will implement a two-stage sampler for a groundwater flow problem, where the coarse model acts as a rapid filter to reject unpromising proposals, dramatically improving computational efficiency without introducing bias [@problem_id:3402701].", "problem": "Consider a Bayesian inverse problem for steady one-dimensional groundwater flow in a confined aquifer. The hydraulic head field $u(x)$ for $x \\in [0,1]$ satisfies the elliptic partial differential equation (PDE) $-\\frac{d}{dx}\\left(k(x)\\frac{du}{dx}\\right)=0$ with Dirichlet boundary conditions $u(0)=1$ and $u(1)=0$. The hydraulic conductivity $k(x)$ is modeled as piecewise constant via a two-parameter log-permeability representation $k(x)=\\exp(\\theta_0)$ for $x \\in [0,0.5]$ and $k(x)=\\exp(\\theta_1)$ for $x \\in (0.5,1]$, where $\\theta=(\\theta_0,\\theta_1) \\in \\mathbb{R}^2$ is the unknown parameter vector. Observations are modeled as noisy measurements of hydraulic head at locations $x_{\\text{obs}} \\subset (0,1)$: $y = G_h(\\theta) + \\eta$, where $G_h(\\theta)$ is the discretized forward model prediction on a fine grid of size $h$ and $\\eta \\sim \\mathcal{N}(0,\\sigma^2 I)$ with known noise level $\\sigma>0$. A Gaussian prior $\\theta \\sim \\mathcal{N}(\\mu, \\Sigma)$ with mean $\\mu \\in \\mathbb{R}^2$ and covariance $\\Sigma \\in \\mathbb{R}^{2 \\times 2}$ is assumed.\n\nYou are asked to derive and implement a two-stage multi-level Metropolis-Hastings (MH) Markov Chain Monte Carlo (MCMC) algorithm that couples a coarse grid posterior $\\pi_H(\\theta \\mid y)$ defined by a coarse discretization $G_H(\\theta)$ and the target fine grid posterior $\\pi_h(\\theta \\mid y)$ defined by $G_h(\\theta)$. The acceptance mechanism must rigorously ensure that the Markov chain has $\\pi_h(\\theta \\mid y)$ as its invariant distribution. The derivation must start from the definition of the MH algorithm with detailed balance for a given target probability density function (PDF) and must show how to incorporate a preliminary coarse-grid acceptance step without biasing the fine-grid target.\n\nFundamental modeling details to be used:\n- The likelihood on grid $h$ is proportional to $\\exp\\left(-\\frac{1}{2\\sigma^2}\\|y - G_h(\\theta)\\|_2^2\\right)$.\n- The likelihood on grid $H$ is proportional to $\\exp\\left(-\\frac{1}{2\\sigma^2}\\|y - G_H(\\theta)\\|_2^2\\right)$.\n- The prior is proportional to $\\exp\\left(-\\frac{1}{2}(\\theta-\\mu)^\\top \\Sigma^{-1}(\\theta-\\mu)\\right)$.\n- The target fine posterior is $\\pi_h(\\theta \\mid y) \\propto \\exp\\left(-\\frac{1}{2\\sigma^2}\\|y - G_h(\\theta)\\|_2^2\\right) \\exp\\left(-\\frac{1}{2}(\\theta-\\mu)^\\top \\Sigma^{-1}(\\theta-\\mu)\\right)$.\n- The coarse posterior is $\\pi_H(\\theta \\mid y) \\propto \\exp\\left(-\\frac{1}{2\\sigma^2}\\|y - G_H(\\theta)\\|_2^2\\right) \\exp\\left(-\\frac{1}{2}(\\theta-\\mu)^\\top \\Sigma^{-1}(\\theta-\\mu)\\right)$.\n\nYour implementation constraints:\n- Discretize the PDE on a uniform grid with $N_h+1$ nodes for the fine model and $N_H+1$ nodes for the coarse model, using a consistent second-order finite-difference scheme with harmonic averaging of the hydraulic conductivity at cell interfaces. The node spacing is $\\Delta x = 1/N$, and the interior equations are to be assembled from conservation of flux. Enforce the Dirichlet boundary conditions exactly at $x=0$ and $x=1$. The forward map $G(\\theta)$ returns head predictions at requested observation locations using linear interpolation from node values.\n- Use a symmetric Gaussian random-walk proposal $q(\\theta' \\mid \\theta) = \\mathcal{N}(\\theta, s^2 I)$ with step-size parameter $s>0$.\n- Derive, from first principles, a two-stage acceptance mechanism that performs a preliminary acceptance using the coarse posterior and then corrects to the fine posterior, while maintaining detailed balance with respect to the fine posterior.\n- Implement the algorithm so that it is fully deterministic given a random seed and so that results are reproducible.\n\nData generation and test configuration:\n- True parameter is $\\theta^\\star = (\\theta_0^\\star,\\theta_1^\\star) = (0.0,-1.0)$.\n- Prior is $\\mu=(0,0)$ and $\\Sigma = \\operatorname{diag}(\\tau^2,\\tau^2)$ with $\\tau=1.0$.\n- Observation locations are $x_{\\text{obs}} = (0.2, 0.5, 0.8)$.\n- Fine grid size is $N_h=200$ and coarse grid size is $N_H=20$.\n- For each test case, synthesize data by computing $y_{\\text{true}} = G_h(\\theta^\\star)$ and then drawing noise $\\eta \\sim \\mathcal{N}(0,\\sigma^2 I)$ with the provided seed and noise level $\\sigma$, finally setting $y=y_{\\text{true}}+\\eta$.\n\nImplement the two-stage multi-level MH algorithm with the following test suite, where each test case is a tuple $(\\text{seed}_{\\text{chain}}, \\text{seed}_{\\text{data}}, s, \\sigma, n_{\\text{iter}})$:\n- Case A (happy path): $(123, 2025, 0.2, 0.05, 2000)$.\n- Case B (larger proposal steps): $(456, 2026, 0.7, 0.05, 2000)$.\n- Case C (higher noise, fewer iterations): $(789, 2027, 0.2, 0.20, 1000)$.\n\nFor each test case, run the chain for $n_{\\text{iter}}$ iterations starting from $\\theta^{(0)}=(0,0)$ and discard the first half of the states as burn-in. Report the following four quantities in this order:\n- Stage-one acceptance rate (coarse-screen acceptance probability) as a float.\n- Overall acceptance rate (fraction of proposals that pass both stages) as a float.\n- Posterior mean of $\\theta_0$ after burn-in as a float.\n- Posterior mean of $\\theta_1$ after burn-in as a float.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, ordered by concatenating the four numbers for Case A, then for Case B, then for Case C. For example, it must look like $[r_1,r_2,r_3,r_4,r_5,r_6,r_7,r_8,r_9,r_{10},r_{11},r_{12}]$ with all entries printed as decimal floats rounded to six digits after the decimal point.\n- No other text should be printed.", "solution": "### 1. The Forward Problem and its Discretization\n\nThe physical system is governed by a one-dimensional steady-state flow equation in a confined aquifer, which is an elliptic partial differential equation (PDE):\n$$\n-\\frac{d}{dx}\\left(k(x)\\frac{du}{dx}\\right)=0, \\quad x \\in [0,1]\n$$\nwith Dirichlet boundary conditions $u(0)=1$ and $u(1)=0$. The hydraulic conductivity, $k(x)$, is modeled as a piecewise constant function determined by the parameter vector $\\theta=(\\theta_0, \\theta_1)$:\n$$\nk(x; \\theta) = \\begin{cases} \\exp(\\theta_0) & \\text{if } x \\in [0, 0.5] \\\\ \\exp(\\theta_1) & \\text{if } x \\in (0.5, 1] \\end{cases}\n$$\nTo solve this PDE numerically, we use a second-order finite difference scheme on a uniform grid with $N+1$ nodes, $x_i = i \\Delta x$ for $i=0, 1, \\dots, N$, where $\\Delta x = 1/N$. We apply the principle of conservation of flux over control volumes centered at each interior node $x_i$ for $i=1, \\dots, N-1$. The control volume for node $i$ is $[x_{i-1/2}, x_{i+1/2}]$, where $x_{i \\pm 1/2} = x_i \\pm \\Delta x/2$.\nThe flux at an interface $x_{i+1/2}$ is approximated as $F_{i+1/2} = -k_{i+1/2} \\frac{u_{i+1}-u_i}{\\Delta x}$. The conservation law $F_{i+1/2} - F_{i-1/2} = 0$ for the control volume around node $i$ gives:\n$$\n-k_{i+1/2} \\frac{u_{i+1}-u_i}{\\Delta x} + k_{i-1/2} \\frac{u_i-u_{i-1}}{\\Delta x} = 0\n$$\n$$\n-k_{i-1/2} u_{i-1} + (k_{i-1/2} + k_{i+1/2}) u_i - k_{i+1/2} u_{i+1} = 0\n$$\nThis equation holds for all interior nodes $i=1, \\dots, N-1$. The term $k_{i+1/2}$ represents the effective hydraulic conductivity at the interface between the control volumes for nodes $i$ and $i+1$. As specified, it is computed using the harmonic average of the conductivity values in the adjacent cells. Assuming cell $i$ has conductivity $k(x_i)$, the interface conductivity is $k_{i+1/2} = \\frac{2 k(x_i) k(x_{i+1})}{k(x_i) + k(x_{i+1})}$.\n\nThese $N-1$ linear equations for the unknown hydraulic heads $u_1, \\dots, u_{N-1}$ can be written in matrix form $A(\\theta)\\mathbf{u}_{\\text{int}} = \\mathbf{b}$. The boundary conditions $u_0=1$ and $u_N=0$ are incorporated as follows:\n- For $i=1$: $-k_{1/2}u_0 + (k_{1/2}+k_{3/2})u_1 - k_{3/2}u_2 = 0 \\implies (k_{1/2}+k_{3/2})u_1 - k_{3/2}u_2 = k_{1/2}u_0 = k_{1/2}$.\n- For $i=N-1$: $-k_{N-3/2}u_{N-2} + (k_{N-3/2}+k_{N-1/2})u_{N-1} - k_{N-1/2}u_N = 0 \\implies -k_{N-3/2}u_{N-2} + (k_{N-3/2}+k_{N-1/2})u_{N-1} = 0$.\n\nThe matrix $A(\\theta)$ is a symmetric positive-definite tridiagonal matrix, and the system can be efficiently solved. The forward model $G(\\theta)$ is the function that, for a given $\\theta$, solves this system for the nodal heads $\\mathbf{u} = [u_0, u_1, \\dots, u_N]^\\top$ and then uses linear interpolation to compute the head values at the specified observation locations $x_{\\text{obs}}$. This procedure is defined for both the fine grid ($N=N_h$) and the coarse grid ($N=N_H$), yielding forward maps $G_h(\\theta)$ and $G_H(\\theta)$, respectively.\n\n### 2. Bayesian Inverse Problem Formulation\n\nThe inverse problem is cast in a Bayesian framework. The posterior probability density function (PDF) for the parameters $\\theta$ given the data $y$ is given by Bayes' theorem:\n$$\n\\pi(\\theta \\mid y) \\propto L(y \\mid \\theta) \\pi(\\theta)\n$$\nwhere $L(y \\mid \\theta)$ is the likelihood and $\\pi(\\theta)$ is the prior.\n\nThe prior on $\\theta$ is a Gaussian distribution, $\\theta \\sim \\mathcal{N}(\\mu, \\Sigma)$, so its PDF is:\n$$\n\\pi(\\theta) \\propto \\exp\\left(-\\frac{1}{2}(\\theta-\\mu)^\\top \\Sigma^{-1}(\\theta-\\mu)\\right)\n$$\nThe observation model is $y = G(\\theta) + \\eta$, where the noise $\\eta$ is assumed to be zero-mean Gaussian with covariance $\\sigma^2 I$, i.e., $\\eta \\sim \\mathcal{N}(0, \\sigma^2 I)$. This yields a Gaussian likelihood function:\n$$\nL(y \\mid \\theta) \\propto \\exp\\left(-\\frac{1}{2\\sigma^2}\\|y - G(\\theta)\\|_2^2\\right)\n$$\nWe define two posteriors: a target posterior $\\pi_h$ based on the fine-grid forward model $G_h$, and an approximate posterior $\\pi_H$ based on the coarse-grid model $G_H$:\n$$\n\\pi_h(\\theta \\mid y) \\propto \\exp\\left(-\\frac{1}{2\\sigma^2}\\|y - G_h(\\theta)\\|_2^2 - \\frac{1}{2}(\\theta-\\mu)^\\top \\Sigma^{-1}(\\theta-\\mu)\\right)\n$$\n$$\n\\pi_H(\\theta \\mid y) \\propto \\exp\\left(-\\frac{1}{2\\sigma^2}\\|y - G_H(\\theta)\\|_2^2 - \\frac{1}{2}(\\theta-\\mu)^\\top \\Sigma^{-1}(\\theta-\\mu)\\right)\n$$\nOur goal is to generate samples from the target posterior $\\pi_h(\\theta \\mid y)$ using an MCMC method.\n\n### 3. The Two-Stage Multi-Level Metropolis-Hastings Algorithm\n\nThe standard Metropolis-Hastings (MH) algorithm constructs a Markov chain whose stationary distribution is the target PDF, $\\pi_h$. This is achieved by satisfying the detailed balance condition:\n$$\n\\pi_h(\\theta) P(\\theta' \\mid \\theta) = \\pi_h(\\theta') P(\\theta \\mid \\theta')\n$$\nwhere $P(\\theta' \\mid \\theta)$ is the transition kernel from state $\\theta$ to $\\theta'$. For a symmetric proposal distribution $q(\\theta' \\mid \\theta) = q(\\theta \\mid \\theta')$, the acceptance probability is:\n$$\n\\alpha_h(\\theta', \\theta) = \\min\\left(1, \\frac{\\pi_h(\\theta')}{\\pi_h(\\theta)}\\right)\n$$\nEvaluating $\\pi_h$ involves running the fine-grid forward model $G_h$, which can be computationally expensive. The two-stage multi-level (or Delayed Acceptance) MH algorithm aims to reduce computational cost by using the cheap-to-evaluate coarse posterior $\\pi_H$ to pre-screen proposals.\n\nThe algorithm proceeds as follows at each iteration, given the current state $\\theta$:\n1.  **Proposal:** Generate a candidate state $\\theta' \\sim q(\\theta' \\mid \\theta)$.\n2.  **Stage 1 (Coarse Screen):** Calculate the first-stage acceptance probability using the coarse model:\n    $$\n    \\alpha_1(\\theta', \\theta) = \\min\\left(1, \\frac{\\pi_H(\\theta')}{\\pi_H(\\theta)}\\right)\n    $$\n    With probability $\\alpha_1(\\theta', \\theta)$, the proposal passes this screen and proceeds to Stage 2. Otherwise, it is rejected, and the chain remains at $\\theta$.\n3.  **Stage 2 (Fine Correction):** If the proposal passes Stage 1, calculate the second-stage acceptance probability:\n    $$\n    \\alpha_2(\\theta', \\theta) = \\min\\left(1, \\frac{\\pi_h(\\theta') \\pi_H(\\theta)}{\\pi_h(\\theta) \\pi_H(\\theta')}\\right)\n    $$\n    With probability $\\alpha_2(\\theta', \\theta)$, the proposal is accepted, and the next state of the chain is $\\theta'$. Otherwise, it is rejected, and the chain remains at $\\theta$.\n\nThe overall probability of accepting the move from $\\theta$ to $\\theta'$ is the product of the probabilities of passing each stage:\n$$\n\\alpha_{DA}(\\theta', \\theta) = \\alpha_1(\\theta', \\theta) \\alpha_2(\\theta', \\theta) = \\min\\left(1, \\frac{\\pi_H(\\theta')}{\\pi_H(\\theta)}\\right) \\min\\left(1, \\frac{\\pi_h(\\theta') \\pi_H(\\theta)}{\\pi_h(\\theta) \\pi_H(\\theta')}\\right)\n$$\nTo verify that this algorithm correctly samples from $\\pi_h$, we must show that it satisfies the detailed balance condition with respect to $\\pi_h$. For a symmetric proposal $q$, this requires showing that $\\pi_h(\\theta) \\alpha_{DA}(\\theta', \\theta) = \\pi_h(\\theta') \\alpha_{DA}(\\theta, \\theta')$. Let's analyze the left-hand side:\n$$\n\\pi_h(\\theta) \\alpha_{DA}(\\theta', \\theta) = \\pi_h(\\theta) \\min\\left(1, \\frac{\\pi_H(\\theta')}{\\pi_H(\\theta)}\\right) \\min\\left(1, \\frac{\\pi_h(\\theta') \\pi_H(\\theta)}{\\pi_h(\\theta) \\pi_H(\\theta')}\\right)\n$$\nUsing the property $c \\min(a,b) = \\min(ca, cb)$ for $c>0$, we can move terms inside the minimum operators:\n$$\n= \\min\\left(\\pi_h(\\theta), \\pi_h(\\theta) \\frac{\\pi_H(\\theta')}{\\pi_H(\\theta)}\\right) \\min\\left(1, \\frac{\\pi_h(\\theta') \\pi_H(\\theta)}{\\pi_h(\\theta) \\pi_H(\\theta')}\\right)\n$$\n$$\n= \\min\\left(\\pi_h(\\theta), \\pi_H(\\theta') \\frac{\\pi_h(\\theta)}{\\pi_H(\\theta)}\\right) \\min\\left(1, \\frac{\\pi_h(\\theta') \\pi_H(\\theta)}{\\pi_h(\\theta) \\pi_H(\\theta')}\\right)\n$$\nLet's apply the $\\min$ operator property again, combining the two terms:\n$$\n= \\min\\left(\n\\pi_h(\\theta) \\cdot 1, \\quad\n\\pi_h(\\theta) \\cdot \\frac{\\pi_h(\\theta') \\pi_H(\\theta)}{\\pi_h(\\theta) \\pi_H(\\theta')}, \\quad\n\\pi_H(\\theta') \\frac{\\pi_h(\\theta)}{\\pi_H(\\theta)} \\cdot 1, \\quad\n\\pi_H(\\theta') \\frac{\\pi_h(\\theta)}{\\pi_H(\\theta)} \\cdot \\frac{\\pi_h(\\theta') \\pi_H(\\theta)}{\\pi_h(\\theta) \\pi_H(\\theta')}\n\\right)\n$$\nSimplifying the terms inside the minimum:\n$$\n= \\min\\left(\n\\pi_h(\\theta), \\quad\n\\pi_h(\\theta') \\frac{\\pi_H(\\theta)}{\\pi_H(\\theta')}, \\quad\n\\pi_h(\\theta) \\frac{\\pi_H(\\theta')}{\\pi_H(\\theta)}, \\quad\n\\pi_h(\\theta')\n\\right)\n$$\nLet this resulting expression be $f(\\theta, \\theta')$. By inspection, this expression is symmetric with respect to an exchange of $\\theta$ and $\\theta'$, i.e., $f(\\theta, \\theta') = f(\\theta', \\theta)$. Since we have shown $\\pi_h(\\theta) \\alpha_{DA}(\\theta', \\theta) = f(\\theta, \\theta')$, it immediately follows that $\\pi_h(\\theta') \\alpha_{DA}(\\theta, \\theta') = f(\\theta', \\theta) = f(\\theta, \\theta')$.\nThus, $\\pi_h(\\theta) \\alpha_{DA}(\\theta', \\theta) = \\pi_h(\\theta') \\alpha_{DA}(\\theta, \\theta')$, which confirms that detailed balance with respect to the target fine-grid posterior $\\pi_h$ is satisfied. The algorithm is unbiased and will produce samples from the correct target distribution.\nFor implementation, it is numerically more stable to work with logarithms of the PDFs (i.e., the negative log-posteriors, or potentials $\\Phi(\\theta) = -\\log \\pi(\\theta \\mid y) + \\text{const}$). The acceptance ratios become differences of potentials, and the comparisons are made between a log-uniform random variate and the log-ratio.", "answer": "```python\nimport numpy as np\nfrom scipy.linalg import solve_banded\n\ndef solve_pde(theta, N):\n    \"\"\"\n    Solves the 1D steady-state groundwater flow PDE using finite differences.\n    \"\"\"\n    delta_x = 1.0 / N\n    x_nodes = np.linspace(0, 1, N + 1)\n    \n    # Hydraulic conductivity at nodes\n    k_vals = np.exp(theta[0] * (x_nodes = 0.5) + theta[1] * (x_nodes > 0.5))\n\n    # Interface conductivities using harmonic mean\n    # k_interface[i] corresponds to interface at x_{i+1/2}\n    k_interface = np.zeros(N)\n    # The term k_vals[i] + k_vals[i+1] can be zero if conductivities are very low,\n    # but exp(theta_i) is always positive. Add a small epsilon for stability if needed,\n    # though not strictly necessary here.\n    k_interface = 2 * k_vals[:-1] * k_vals[1:] / (k_vals[:-1] + k_vals[1:])\n    \n    # Assemble the tridiagonal system A*u_int = b for interior nodes u_1, ..., u_{N-1}\n    # A is (N-1)x(N-1), but we use scipy's banded format\n    # ab has 3 rows: [0, upper_diag], [main_diag], [lower_diag, 0]\n    ab = np.zeros((3, N - 1))\n    \n    # Main diagonal: k_{i-1/2} + k_{i+1/2}\n    ab[1, :] = k_interface[:-1] + k_interface[1:]\n    \n    # Upper diagonal: -k_{i+1/2}\n    ab[0, 1:] = -k_interface[1:-1]\n    \n    # Lower diagonal: -k_{i-1/2}\n    ab[2, :-1] = -k_interface[1:-1]\n\n    # Right-hand side vector\n    b = np.zeros(N - 1)\n    b[0] = k_interface[0] * 1.0  # From u_0 = 1 boundary condition\n\n    # Solve for interior nodes\n    u_internal = solve_banded((1, 1), ab, b)\n    \n    # Combine with boundary values\n    u_full = np.concatenate(([1.0], u_internal, [0.0]))\n    \n    return x_nodes, u_full\n\ndef forward_model_G(theta, N, x_obs):\n    \"\"\"\n    Forward model G(theta), which solves the PDE and interpolates to observation points.\n    \"\"\"\n    x_grid, u_grid = solve_pde(theta, N)\n    return np.interp(x_obs, x_grid, u_grid)\n\ndef neg_log_posterior(theta, y_obs, sigma, N, x_obs, prior_mu, prior_tau_sq):\n    \"\"\"\n    Computes the negative log-posterior (up to a constant).\n    Phi(theta) = 0.5/sigma^2 * ||y - G(theta)||^2 + 0.5/tau^2 * ||theta - mu||^2\n    \"\"\"\n    # Log-likelihood term\n    y_pred = forward_model_G(theta, N, x_obs)\n    log_lik_term = 0.5 / (sigma**2) * np.sum((y_obs - y_pred)**2)\n    \n    # Log-prior term\n    log_prior_term = 0.5 / prior_tau_sq * np.sum((theta - prior_mu)**2)\n    \n    return log_lik_term + log_prior_term\n\ndef run_mcmc(case):\n    \"\"\"\n    Runs the two-stage multi-level MH MCMC for a given test case.\n    \"\"\"\n    seed_chain, seed_data, s, sigma, n_iter = case\n    \n    # --- Problem Setup ---\n    theta_true = np.array([0.0, -1.0])\n    prior_mu = np.array([0.0, 0.0])\n    prior_tau_sq = 1.0**2\n    x_obs = np.array([0.2, 0.5, 0.8])\n    N_h, N_H = 200, 20\n    \n    # --- Synthesize Data ---\n    data_rng = np.random.default_rng(seed_data)\n    y_true = forward_model_G(theta_true, N_h, x_obs)\n    noise = data_rng.normal(0, sigma, size=len(x_obs))\n    y_obs = y_true + noise\n\n    # --- MCMC Initialization ---\n    chain_rng = np.random.default_rng(seed_chain)\n    theta_current = np.array([0.0, 0.0])\n    samples = np.zeros((n_iter, 2))\n    \n    stage1_passes = 0\n    stage2_accepts = 0\n    \n    phi_h_current = neg_log_posterior(theta_current, y_obs, sigma, N_h, x_obs, prior_mu, prior_tau_sq)\n    phi_H_current = neg_log_posterior(theta_current, y_obs, sigma, N_H, x_obs, prior_mu, prior_tau_sq)\n    \n    # --- MCMC Loop ---\n    for i in range(n_iter):\n        # 1. Propose\n        theta_proposal = theta_current + chain_rng.normal(0, s, size=2)\n        \n        # 2. Stage 1 (Coarse Screen)\n        phi_H_proposal = neg_log_posterior(theta_proposal, y_obs, sigma, N_H, x_obs, prior_mu, prior_tau_sq)\n        log_alpha1_ratio = phi_H_current - phi_H_proposal\n        \n        if np.log(chain_rng.random())  log_alpha1_ratio:\n            stage1_passes += 1\n            \n            # 3. Stage 2 (Fine Correction)\n            phi_h_proposal = neg_log_posterior(theta_proposal, y_obs, sigma, N_h, x_obs, prior_mu, prior_tau_sq)\n            log_alpha2_ratio = (phi_h_current - phi_h_proposal) - (phi_H_current - phi_H_proposal)\n            \n            if np.log(chain_rng.random())  log_alpha2_ratio:\n                # Accept\n                stage2_accepts += 1\n                theta_current = theta_proposal\n                phi_h_current = phi_h_proposal\n                phi_H_current = phi_H_proposal\n        \n        # Store current state (which is the new state if accepted, old otherwise)\n        samples[i, :] = theta_current\n\n    # --- Post-processing ---\n    burn_in = n_iter // 2\n    post_burn_samples = samples[burn_in:, :]\n    \n    stage1_rate = stage1_passes / n_iter\n    overall_rate = stage2_accepts / n_iter\n    post_mean_theta0 = np.mean(post_burn_samples[:, 0])\n    post_mean_theta1 = np.mean(post_burn_samples[:, 1])\n    \n    return stage1_rate, overall_rate, post_mean_theta0, post_mean_theta1\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    # Test cases: (seed_chain, seed_data, s, sigma, n_iter)\n    test_cases = [\n        (123, 2025, 0.2, 0.05, 2000),  # Case A\n        (456, 2026, 0.7, 0.05, 2000),  # Case B\n        (789, 2027, 0.2, 0.20, 1000),  # Case C\n    ]\n    \n    results = []\n    for case in test_cases:\n        case_results = run_mcmc(case)\n        results.extend(case_results)\n        \n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3402701"}]}