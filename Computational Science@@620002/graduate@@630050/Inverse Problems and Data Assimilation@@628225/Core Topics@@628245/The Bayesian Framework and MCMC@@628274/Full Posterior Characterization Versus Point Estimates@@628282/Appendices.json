{"hands_on_practices": [{"introduction": "We begin our exploration by examining a scenario where point estimates fall short even in the simplest linear, Gaussian case. In many inverse problems, the available data are not equally informative about all aspects of the unknown parameters. This exercise [@problem_id:3383422] provides a clear, analytical demonstration of how a full posterior characterization reveals this anisotropic uncertainty, showing how the posterior covariance matrix distinguishes data-informed directions from unobserved ones, a crucial insight entirely missed by a single point estimate.", "problem": "Consider a linear Gaussian inverse problem with state vector $x \\in \\mathbb{R}^{3}$, observation vector $y \\in \\mathbb{R}^{2}$, forward operator $A \\in \\mathbb{R}^{2 \\times 3}$, and additive noise $\\epsilon \\in \\mathbb{R}^{2}$. The data model is $y = A x + \\epsilon$, where $\\epsilon \\sim \\mathcal{N}(0, \\Gamma)$ with $\\Gamma = \\sigma^{2} I_{2}$ for a given $\\sigma^{2} > 0$. The prior on $x$ is Gaussian $x \\sim \\mathcal{N}(m_{0}, C_{0})$ with mean $m_{0} = (m_{1}, m_{2}, m_{3})^{\\top}$ and covariance\n$$\nC_{0} = \\mathrm{diag}(\\beta^{2}, \\tau^{2}, \\tau^{2}),\n$$\nwhere $\\beta^{2} > 0$ and $\\tau^{2} > 0$ are given scalars. Let the forward operator be\n$$\nA = \\begin{pmatrix}\n1 & 0 & 0 \\\\\n0 & 0 & 0\n\\end{pmatrix},\n$$\nwhich is rank-deficient.\n\nStarting from Bayes’ rule and the definitions of Gaussian densities, derive the posterior covariance matrix of $x$ given $y$. Then, characterize the posterior uncertainty along the subspace $\\mathrm{Null}(A)$ and contrast it with the uncertainty along the data-informed direction. Specifically:\n\n- Compute the posterior covariance in closed form and identify $\\mathrm{Null}(A)$.\n- Let $v \\in \\mathrm{Null}(A)$ be any unit vector. Compute the posterior variance $v^{\\top} S v$, where $S$ denotes the posterior covariance.\n- Compute the posterior variance along the data-informed direction $e_{1} = (1, 0, 0)^{\\top}$.\n- Define the anisotropy (inflation) factor\n$$\nR := \\frac{\\text{posterior variance along } \\mathrm{Null}(A)}{\\text{posterior variance along the data-informed direction}} = \\frac{v^{\\top} S v}{e_{1}^{\\top} S e_{1}},\n$$\nand express $R$ in terms of $\\beta^{2}$, $\\tau^{2}$, and $\\sigma^{2}$ only.\n\nFinally, compute the Maximum A Posteriori (MAP) estimate of $x$ and explain, using your derived formulas, how the full posterior characterization reveals large uncertainty along $\\mathrm{Null}(A)$ that is not conveyed by the single MAP solution.\n\nYour final answer must be the closed-form analytic expression for $R$ with no numerical substitution or rounding.", "solution": "We begin from Bayes’ rule $p(x \\mid y) \\propto p(y \\mid x)\\,p(x)$, and the definitions of Gaussian densities. The likelihood is $p(y \\mid x) = \\mathcal{N}(A x, \\Gamma)$ with $\\Gamma = \\sigma^{2} I_{2}$, and the prior is $p(x) = \\mathcal{N}(m_{0}, C_{0})$ with $C_{0} = \\mathrm{diag}(\\beta^{2}, \\tau^{2}, \\tau^{2})$.\n\nA standard consequence of completing the square for the product of Gaussian densities in a linear model $y = A x + \\epsilon$ is that the posterior $p(x \\mid y)$ is Gaussian with precision (the inverse covariance) equal to the sum of the prior precision and the data precision pulled back to parameter space. Concretely, the posterior covariance $S$ satisfies\n$$\nS^{-1} = C_{0}^{-1} + A^{\\top} \\Gamma^{-1} A,\n$$\nand the posterior mean $m$ is\n$$\nm = S\\left(C_{0}^{-1} m_{0} + A^{\\top} \\Gamma^{-1} y \\right).\n$$\nWe derive $S$ explicitly for the given $A$, $\\Gamma$, and $C_{0}$.\n\nFirst, compute the prior precision:\n$$\nC_{0}^{-1} = \\mathrm{diag}\\!\\left(\\beta^{-2}, \\tau^{-2}, \\tau^{-2}\\right).\n$$\nNext, compute the data precision contribution:\n$$\n\\Gamma^{-1} = \\sigma^{-2} I_{2}, \\quad A^{\\top} \\Gamma^{-1} A = \\sigma^{-2} A^{\\top} A.\n$$\nSince\n$$\nA = \\begin{pmatrix}\n1 & 0 & 0 \\\\\n0 & 0 & 0\n\\end{pmatrix}, \\quad\nA^{\\top} = \\begin{pmatrix}\n1 & 0 \\\\\n0 & 0 \\\\\n0 & 0\n\\end{pmatrix},\n$$\nwe have\n$$\nA^{\\top} A = \\begin{pmatrix}\n1 & 0 & 0 \\\\\n0 & 0 & 0 \\\\\n0 & 0 & 0\n\\end{pmatrix},\n$$\nand therefore\n$$\nA^{\\top} \\Gamma^{-1} A = \\sigma^{-2} \\begin{pmatrix}\n1 & 0 & 0 \\\\\n0 & 0 & 0 \\\\\n0 & 0 & 0\n\\end{pmatrix}.\n$$\nHence,\n$$\nS^{-1} = \\mathrm{diag}\\!\\left(\\beta^{-2}, \\tau^{-2}, \\tau^{-2}\\right) + \\sigma^{-2} \\begin{pmatrix}\n1 & 0 & 0 \\\\\n0 & 0 & 0 \\\\\n0 & 0 & 0\n\\end{pmatrix}\n= \\begin{pmatrix}\n\\beta^{-2} + \\sigma^{-2} & 0 & 0 \\\\\n0 & \\tau^{-2} & 0 \\\\\n0 & 0 & \\tau^{-2}\n\\end{pmatrix}.\n$$\nInverting this diagonal matrix yields the posterior covariance\n$$\nS = \\begin{pmatrix}\n\\left(\\beta^{-2} + \\sigma^{-2}\\right)^{-1} & 0 & 0 \\\\\n0 & \\tau^{2} & 0 \\\\\n0 & 0 & \\tau^{2}\n\\end{pmatrix}.\n$$\n\nWe now identify the nullspace of $A$. The equation $A x = 0$ reads\n$$\n\\begin{pmatrix}\n1 & 0 & 0 \\\\\n0 & 0 & 0\n\\end{pmatrix}\n\\begin{pmatrix}\nx_{1} \\\\\nx_{2} \\\\\nx_{3}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\nx_{1} \\\\\n0\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n0 \\\\\n0\n\\end{pmatrix},\n$$\nwhich implies $x_{1} = 0$ and leaves $x_{2}$ and $x_{3}$ free. Therefore,\n$$\n\\mathrm{Null}(A) = \\left\\{ \\begin{pmatrix} 0 \\\\ t \\\\ s \\end{pmatrix} : t, s \\in \\mathbb{R} \\right\\}.\n$$\n\nLet $v \\in \\mathrm{Null}(A)$ be any unit vector. Since $S$ is diagonal with entries $\\left(\\beta^{-2} + \\sigma^{-2}\\right)^{-1}$, $\\tau^{2}$, and $\\tau^{2}$, and $v$ has zero in the first coordinate, we can write $v = (0, v_{2}, v_{3})^{\\top}$ with $v_{2}^{2} + v_{3}^{2} = 1$. Then\n$$\nv^{\\top} S v = \\begin{pmatrix} 0 & v_{2} & v_{3} \\end{pmatrix}\n\\begin{pmatrix}\n\\left(\\beta^{-2} + \\sigma^{-2}\\right)^{-1} & 0 & 0 \\\\\n0 & \\tau^{2} & 0 \\\\\n0 & 0 & \\tau^{2}\n\\end{pmatrix}\n\\begin{pmatrix} 0 \\\\ v_{2} \\\\ v_{3} \\end{pmatrix}\n= \\tau^{2} v_{2}^{2} + \\tau^{2} v_{3}^{2} = \\tau^{2}.\n$$\nThus, the posterior variance along any unit direction in $\\mathrm{Null}(A)$ equals $\\tau^{2}$.\n\nAlong the data-informed direction $e_{1} = (1, 0, 0)^{\\top}$, we have\n$$\ne_{1}^{\\top} S e_{1} = \\left(\\beta^{-2} + \\sigma^{-2}\\right)^{-1}.\n$$\n\nWe define the anisotropy (inflation) factor as\n$$\nR := \\frac{v^{\\top} S v}{e_{1}^{\\top} S e_{1}}\n= \\frac{\\tau^{2}}{\\left(\\beta^{-2} + \\sigma^{-2}\\right)^{-1}}\n= \\tau^{2}\\left(\\beta^{-2} + \\sigma^{-2}\\right).\n$$\nThis expression depends only on $\\beta^{2}$, $\\tau^{2}$, and $\\sigma^{2}$.\n\nFor contrast with a point estimate, we compute the Maximum A Posteriori (MAP) estimate. The posterior mean equals the MAP for Gaussian posteriors, and it is given by\n$$\nm = S\\left(C_{0}^{-1} m_{0} + A^{\\top} \\Gamma^{-1} y \\right).\n$$\nCompute $C_{0}^{-1} m_{0} = \\left(\\beta^{-2} m_{1}, \\tau^{-2} m_{2}, \\tau^{-2} m_{3}\\right)^{\\top}$ and $A^{\\top} \\Gamma^{-1} y = \\sigma^{-2} A^{\\top} y = \\sigma^{-2} \\begin{pmatrix} y_{1} \\\\ 0 \\\\ 0 \\end{pmatrix}$. Therefore,\n$$\nC_{0}^{-1} m_{0} + A^{\\top} \\Gamma^{-1} y = \\begin{pmatrix} \\beta^{-2} m_{1} + \\sigma^{-2} y_{1} \\\\ \\tau^{-2} m_{2} \\\\ \\tau^{-2} m_{3} \\end{pmatrix},\n$$\nand multiplying by $S$ yields\n$$\nm = \\begin{pmatrix}\n\\left(\\beta^{-2} + \\sigma^{-2}\\right)^{-1} \\left(\\beta^{-2} m_{1} + \\sigma^{-2} y_{1} \\right) \\\\\n\\tau^{2} \\cdot \\tau^{-2} m_{2} \\\\\n\\tau^{2} \\cdot \\tau^{-2} m_{3}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\left(\\beta^{-2} + \\sigma^{-2}\\right)^{-1} \\left(\\beta^{-2} m_{1} + \\sigma^{-2} y_{1} \\right) \\\\\nm_{2} \\\\\nm_{3}\n\\end{pmatrix}.\n$$\nThus, the MAP selects the single values $x_{2} = m_{2}$ and $x_{3} = m_{3}$ in the unobserved directions, even though the full posterior covariance shows that the uncertainty along $\\mathrm{Null}(A)$ is large and equal to $\\tau^{2}$. The anisotropy factor $R = \\tau^{2}(\\beta^{-2} + \\sigma^{-2})$ quantifies this inflation relative to the data-informed direction, highlighting how full posterior characterization captures uncertainty that a point estimate such as the MAP does not convey.\n\nThe requested final expression for the inflation factor is $R = \\tau^{2}(\\beta^{-2} + \\sigma^{-2})$.", "answer": "$$\\boxed{\\tau^{2}\\left(\\beta^{-2}+\\sigma^{-2}\\right)}$$", "id": "3383422"}, {"introduction": "Moving from linear models, the limitations of point estimates become even more striking in the context of nonlinear problems. A non-injective forward model, where different parameter values can produce the same output, often leads to a multimodal posterior distribution with multiple, distinct regions of high probability. This practice [@problem_id:3383451] uses a canonical non-injective model to show how a Maximum A Posteriori (MAP) estimate arbitrarily selects one solution while ignoring others, and allows you to quantify the superior predictive performance gained by integrating over the full posterior.", "problem": "Consider the scalar nonlinear inverse problem with observational model $y = f(\\theta) + \\epsilon$, where the parameter $\\theta \\in \\mathbb{R}$, the forward map is $f(\\theta) = \\theta^2$, and the noise $\\epsilon \\sim \\mathcal{N}(0,\\sigma^2)$ is independent of $\\theta$. Assume the prior $\\theta \\sim \\mathcal{N}(0,\\tau^2)$. This model is non-injective because $f(\\theta) = f(-\\theta)$, which can lead to a multimodal posterior for $\\theta$ when the observed data $y$ is sufficiently informative. Use the following fundamental base: Bayes' rule for the posterior $p(\\theta \\mid y) \\propto p(y \\mid \\theta) p(\\theta)$ and the law of total probability for the posterior predictive $p(y^\\ast \\mid y) = \\int p(y^\\ast \\mid \\theta) p(\\theta \\mid y) \\, d\\theta$, where $y^\\ast$ denotes a future observation under the same data model.\n\nYour tasks are:\n- Precisely define the Maximum A Posteriori (MAP) estimator $\\hat{\\theta}_{\\text{MAP}}$ as a maximizer of the posterior density $p(\\theta \\mid y)$ and determine the corresponding plug-in posterior predictive $p_{\\text{MAP}}(y^\\ast \\mid y)$ that uses a point-mass at $\\hat{\\theta}_{\\text{MAP}}$.\n- Precisely define the fully integrated posterior predictive $p_{\\text{int}}(y^\\ast \\mid y)$ by marginalizing over $p(\\theta \\mid y)$ and express it using only well-defined integrals and normalizing constants derived from Bayes' rule and the law of total probability.\n- Implement a robust numerical procedure to evaluate, for given $(y,\\sigma,\\tau)$ and a finite set of evaluation points $\\{y_k^\\ast\\}_{k=1}^K$, the average log predictive density difference\n$$\n\\Delta(y,\\sigma,\\tau;\\{y_k^\\ast\\}) \\;=\\; \\frac{1}{K} \\sum_{k=1}^K \\left[ \\log p_{\\text{int}}(y_k^\\ast \\mid y) \\;-\\; \\log p_{\\text{MAP}}(y_k^\\ast \\mid y) \\right].\n$$\nThe evaluation must be performed in a scientifically sound and numerically stable manner without resorting to any sampling-based approximation.\n\nBase definitions you may use (and must derive from them, not shortcut beyond them):\n- The likelihood $p(y \\mid \\theta)$ is Gaussian with mean $\\theta^2$ and variance $\\sigma^2$, that is $p(y \\mid \\theta) = \\mathcal{N}(y;\\theta^2,\\sigma^2)$.\n- The prior $p(\\theta)$ is Gaussian with mean $0$ and variance $\\tau^2$, that is $p(\\theta) = \\mathcal{N}(\\theta;0,\\tau^2)$.\n- The posterior $p(\\theta \\mid y) \\propto p(y \\mid \\theta)p(\\theta)$.\n- The MAP estimator $\\hat{\\theta}_{\\text{MAP}}$ is a maximizer of $p(\\theta \\mid y)$.\n- The plug-in predictive under a point estimate $\\hat{\\theta}$ is $p(y^\\ast \\mid y) \\approx p(y^\\ast \\mid \\theta=\\hat{\\theta})$.\n- The fully integrated predictive is $p(y^\\ast \\mid y) = \\int p(y^\\ast \\mid \\theta) p(\\theta \\mid y) \\, d\\theta$.\n\nReport the comparison in terms of the average log predictive density difference $\\Delta(y,\\sigma,\\tau;\\{y_k^\\ast\\})$ for the following test suite of parameter values and evaluation grids:\n- Case A (bimodal, low noise): $(y,\\sigma,\\tau) = (4.0, 0.2, 2.0)$ with $\\{y_k^\\ast\\} = [3.0, 4.0, 5.0]$.\n- Case B (near-coalescing modes): $(y,\\sigma,\\tau) = (0.5, 0.2, 2.0)$ with $\\{y_k^\\ast\\} = [0.1, 0.5, 1.0]$.\n- Case C (bimodal, higher noise): $(y,\\sigma,\\tau) = (4.0, 1.5, 2.0)$ with $\\{y_k^\\ast\\} = [0.0, 2.0, 4.0, 8.0]$.\n- Case D (edge case at zero): $(y,\\sigma,\\tau) = (0.0, 0.2, 2.0)$ with $\\{y_k^\\ast\\} = [0.0, 0.2, 1.0]$.\n\nNumerical and algorithmic requirements:\n- All quantities are dimensionless (no physical units are required).\n- Angles are not involved; no angle unit is required.\n- For each case, compute the MAP-based predictive assuming a point-mass at $\\hat{\\theta}_{\\text{MAP}}$ and the fully integrated predictive by numerically marginalizing over $\\theta \\in \\mathbb{R}$ using only deterministic numerical integration. Exploit any symmetry you can justify from first principles to improve numerical stability.\n- For each case, return the single float $\\Delta(y,\\sigma,\\tau;\\{y_k^\\ast\\})$, rounded to $6$ decimal places.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, ordered as $[\\Delta_A,\\Delta_B,\\Delta_C,\\Delta_D]$, where each $\\Delta$ corresponds to the respective case above and is rounded to $6$ decimal places, for example $[0.123456,0.234567,0.345678,0.456789]$.", "solution": "We start from Bayes' rule and the law of total probability, which serve as the foundational principles. The model is $y = \\theta^2 + \\epsilon$ with $\\epsilon \\sim \\mathcal{N}(0,\\sigma^2)$ and prior $\\theta \\sim \\mathcal{N}(0,\\tau^2)$. Therefore the likelihood is $p(y \\mid \\theta) = \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left(-\\frac{(y-\\theta^2)^2}{2\\sigma^2}\\right)$ and the prior is $p(\\theta) = \\frac{1}{\\sqrt{2\\pi}\\tau}\\exp\\left(-\\frac{\\theta^2}{2\\tau^2}\\right)$. The posterior is $p(\\theta \\mid y) \\propto p(y \\mid \\theta)p(\\theta)$, which yields the unnormalized log posterior\n$$\n\\ell(\\theta; y,\\sigma,\\tau) \\;=\\; -\\frac{(y-\\theta^2)^2}{2\\sigma^2} - \\frac{\\theta^2}{2\\tau^2} + \\text{constant}.\n$$\nBecause $f(\\theta)=\\theta^2$ is even, the likelihood is even in $\\theta$, and with a symmetric Gaussian prior, the posterior is also even. This implies that for sufficiently informative data values $y$ (relative to $\\sigma$ and $\\tau$), the posterior has two symmetric modes at $\\pm \\theta^\\star$; this is the source of multimodality.\n\nTo obtain the Maximum A Posteriori (MAP) estimator $\\hat{\\theta}_{\\text{MAP}}$, we maximize $\\ell(\\theta;y,\\sigma,\\tau)$. The derivative is\n$$\n\\frac{d\\ell}{d\\theta}(\\theta) = \\frac{2\\theta}{\\sigma^2}(y-\\theta^2) - \\frac{\\theta}{\\tau^2} \\;=\\; \\theta\\left[\\frac{2}{\\sigma^2}(y-\\theta^2) - \\frac{1}{\\tau^2}\\right].\n$$\nStationary points occur at $\\theta = 0$ or when $\\frac{2}{\\sigma^2}(y-\\theta^2) - \\frac{1}{\\tau^2} = 0$, i.e.,\n$$\n\\theta^2 \\;=\\; y - \\frac{\\sigma^2}{2\\tau^2}.\n$$\nThe second derivative is\n$$\n\\frac{d^2\\ell}{d\\theta^2}(\\theta) = \\left[\\frac{2}{\\sigma^2}(y-\\theta^2) - \\frac{1}{\\tau^2}\\right] - \\frac{4\\theta^2}{\\sigma^2}.\n$$\nFor $\\theta \\neq 0$ at stationary points, the bracketed term vanishes, and $\\frac{d^2\\ell}{d\\theta^2} = -\\frac{4\\theta^2}{\\sigma^2} < 0$, so these are local maxima. At $\\theta=0$, the curvature is $\\frac{d^2\\ell}{d\\theta^2}(0) = \\frac{2y}{\\sigma^2} - \\frac{1}{\\tau^2}$, which is negative (a local maximum at $\\theta=0$) if $y \\le \\frac{\\sigma^2}{2\\tau^2}$ and positive (a local minimum) if $y > \\frac{\\sigma^2}{2\\tau^2}$. Therefore:\n- If $y \\le \\frac{\\sigma^2}{2\\tau^2}$, then $\\hat{\\theta}_{\\text{MAP}} = 0$.\n- If $y > \\frac{\\sigma^2}{2\\tau^2}$, then there are two symmetric MAP values at $\\pm \\sqrt{y - \\frac{\\sigma^2}{2\\tau^2}}$. For a deterministic choice, we select the positive maximizer $\\hat{\\theta}_{\\text{MAP}} = +\\sqrt{y - \\frac{\\sigma^2}{2\\tau^2}}$.\n\nGiven a point estimate $\\hat{\\theta}$, the plug-in posterior predictive is\n$$\np_{\\text{MAP}}(y^\\ast \\mid y) \\approx p(y^\\ast \\mid \\theta = \\hat{\\theta}_{\\text{MAP}}) = \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left(-\\frac{(y^\\ast - \\hat{\\theta}_{\\text{MAP}}^2)^2}{2\\sigma^2}\\right).\n$$\nThis corresponds to collapsing posterior uncertainty onto a single point $\\hat{\\theta}_{\\text{MAP}}$ in parameter space, which in this nonlinear model collapses predictive uncertainty to only the observational noise variance.\n\nThe fully integrated posterior predictive, by the law of total probability, is\n$$\np_{\\text{int}}(y^\\ast \\mid y) \\;=\\; \\int_{\\mathbb{R}} p(y^\\ast \\mid \\theta)\\, p(\\theta \\mid y)\\, d\\theta \\;=\\; \\frac{\\int_{\\mathbb{R}} p(y^\\ast \\mid \\theta)\\, p(y \\mid \\theta)\\, p(\\theta)\\, d\\theta}{\\int_{\\mathbb{R}} p(y \\mid \\theta)\\, p(\\theta)\\, d\\theta}.\n$$\nSubstituting the Gaussian forms and canceling constants carefully yields\n$$\np_{\\text{int}}(y^\\ast \\mid y) \\;=\\; \\frac{1}{\\sqrt{2\\pi}\\sigma} \\cdot \\frac{\\int_{\\mathbb{R}} \\exp\\!\\left(-\\frac{(y-\\theta^2)^2}{2\\sigma^2} - \\frac{(y^\\ast-\\theta^2)^2}{2\\sigma^2} - \\frac{\\theta^2}{2\\tau^2}\\right)\\, d\\theta}{\\int_{\\mathbb{R}} \\exp\\!\\left(-\\frac{(y-\\theta^2)^2}{2\\sigma^2} - \\frac{\\theta^2}{2\\tau^2}\\right)\\, d\\theta}.\n$$\nBy symmetry, each integrand is an even function of $\\theta$, so we may compute each integral as twice the integral over $[0,\\infty)$, which improves numerical stability.\n\nAlgorithmic design:\n- Compute $\\hat{\\theta}_{\\text{MAP}}$ using the stationary point analysis above. The plug-in predictive $p_{\\text{MAP}}(y^\\ast \\mid y)$ is then a Gaussian density with mean $\\hat{\\theta}_{\\text{MAP}}^2$ and variance $\\sigma^2$.\n- For the integrated predictive, evaluate the denominator integral\n$$\nZ(y) = \\int_{\\mathbb{R}} \\exp\\!\\left(-\\frac{(y-\\theta^2)^2}{2\\sigma^2} - \\frac{\\theta^2}{2\\tau^2}\\right)\\, d\\theta = 2 \\int_{0}^{\\infty} \\exp\\!\\left(-\\frac{(y-\\theta^2)^2}{2\\sigma^2} - \\frac{\\theta^2}{2\\tau^2}\\right)\\, d\\theta,\n$$\nand for each $y^\\ast$ the numerator integral\n$$\nN(y^\\ast,y) = \\int_{\\mathbb{R}} \\exp\\!\\left(-\\frac{(y-\\theta^2)^2}{2\\sigma^2} - \\frac{(y^\\ast-\\theta^2)^2}{2\\sigma^2} - \\frac{\\theta^2}{2\\tau^2}\\right)\\, d\\theta = 2 \\int_{0}^{\\infty} \\exp\\!\\left(-\\frac{(y-\\theta^2)^2}{2\\sigma^2} - \\frac{(y^\\ast-\\theta^2)^2}{2\\sigma^2} - \\frac{\\theta^2}{2\\tau^2}\\right)\\, d\\theta.\n$$\nUse deterministic numerical quadrature with absolute and relative tolerances chosen to ensure accuracy. Because the integrands decay super-exponentially as $\\theta \\to \\infty$ due to the $\\theta^4$ term arising from $(y-\\theta^2)^2$, numerical integration over $[0,\\infty)$ is stable.\n\n- Compute $\\log p_{\\text{int}}(y^\\ast \\mid y) = -\\frac{1}{2}\\log(2\\pi) - \\log(\\sigma) + \\log N(y^\\ast,y) - \\log Z(y)$ and $\\log p_{\\text{MAP}}(y^\\ast \\mid y) = -\\frac{1}{2}\\log(2\\pi) - \\log(\\sigma) - \\frac{(y^\\ast-\\hat{\\theta}_{\\text{MAP}}^2)^2}{2\\sigma^2}$.\n- Average their differences over the provided grid $\\{y_k^\\ast\\}_{k=1}^K$ to obtain $\\Delta(y,\\sigma,\\tau;\\{y_k^\\ast\\})$.\n\nTest suite execution:\n- Case A: $(y,\\sigma,\\tau) = (4.0, 0.2, 2.0)$, $\\{y_k^\\ast\\} = [3.0, 4.0, 5.0]$.\n- Case B: $(y,\\sigma,\\tau) = (0.5, 0.2, 2.0)$, $\\{y_k^\\ast\\} = [0.1, 0.5, 1.0]$.\n- Case C: $(y,\\sigma,\\tau) = (4.0, 1.5, 2.0)$, $\\{y_k^\\ast\\} = [0.0, 2.0, 4.0, 8.0]$.\n- Case D: $(y,\\sigma,\\tau) = (0.0, 0.2, 2.0)$, $\\{y_k^\\ast\\} = [0.0, 0.2, 1.0]$.\n\nFinally, aggregate the four averaged differences into a single output line $[\\Delta_A,\\Delta_B,\\Delta_C,\\Delta_D]$, rounding each to $6$ decimal places. This comparison directly quantifies the advantage (if any) of full posterior characterization over point estimates for posterior predictive performance in a multimodal inverse problem, isolating the role of integrating over modes in $p(\\theta \\mid y)$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom math import sqrt, log, pi\nfrom scipy import integrate\n\ndef theta_map_squared(y, sigma, tau):\n    # Closed-form from stationary condition:\n    # theta_MAP^2 = max(y - sigma^2/(2*tau^2), 0)\n    val = y - (sigma**2) / (2.0 * (tau**2))\n    return max(val, 0.0)\n\ndef log_norm_pdf(x, mean, std):\n    return -0.5*log(2.0*pi) - log(std) - 0.5*((x - mean)/std)**2\n\ndef denom_integrand(theta, y, sigma, tau):\n    # Unnormalized posterior integrand: exp(- (y - theta^2)^2/(2*sigma^2) - theta^2/(2*tau^2))\n    t2 = theta*theta\n    return np.exp(- ((y - t2)**2) / (2.0*sigma**2) - t2 / (2.0*tau**2))\n\ndef numer_integrand(theta, y, ystar, sigma, tau):\n    # Numerator integrand for predictive: exp(- (y - theta^2)^2/(2*sigma^2) - (ystar - theta^2)^2/(2*sigma^2) - theta^2/(2*tau^2))\n    t2 = theta*theta\n    return np.exp(- ((y - t2)**2) / (2.0*sigma**2) - ((ystar - t2)**2) / (2.0*sigma**2) - t2 / (2.0*tau**2))\n\ndef log_pint(y, ystar, sigma, tau):\n    # Compute log p_int(ystar | y) = -0.5*log(2*pi) - log(sigma) + log(num) - log(den)\n    # Using symmetry: integrate from 0 to inf and double\n    den_half, den_err = integrate.quad(lambda th: denom_integrand(th, y, sigma, tau), 0.0, np.inf, epsabs=1e-10, epsrel=1e-9, limit=500)\n    num_half, num_err = integrate.quad(lambda th: numer_integrand(th, y, ystar, sigma, tau), 0.0, np.inf, epsabs=1e-10, epsrel=1e-9, limit=500)\n    den = 2.0 * den_half\n    num = 2.0 * num_half\n    # Safety checks to avoid log(0)\n    if den <= 0.0 or num <= 0.0:\n        # In rare numerical issues, return -inf\n        return -np.inf\n    return -0.5*log(2.0*pi) - log(sigma) + log(num) - log(den)\n\ndef average_logscore_difference(y, sigma, tau, ystars):\n    # Compute MAP-based log predictive at each y* and integrated log predictive, then average difference\n    theta2_map = theta_map_squared(y, sigma, tau)\n    diffs = []\n    for ys in ystars:\n        log_p_map = log_norm_pdf(ys, theta2_map, sigma)\n        log_p_int = log_pint(y, ys, sigma, tau)\n        diffs.append(log_p_int - log_p_map)\n    return float(np.mean(diffs))\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case: (y, sigma, tau, [y_star values])\n    test_cases = [\n        (4.0, 0.2, 2.0, [3.0, 4.0, 5.0]),           # Case A\n        (0.5, 0.2, 2.0, [0.1, 0.5, 1.0]),           # Case B\n        (4.0, 1.5, 2.0, [0.0, 2.0, 4.0, 8.0]),      # Case C\n        (0.0, 0.2, 2.0, [0.0, 0.2, 1.0]),           # Case D\n    ]\n\n    results = []\n    for (y, sigma, tau, ystars) in test_cases:\n        delta = average_logscore_difference(y, sigma, tau, ystars)\n        # Round to 6 decimals as required\n        results.append(f\"{delta:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3383451"}, {"introduction": "Beyond multimodality, another critical feature that point estimates cannot capture is the \"heavy-tailed\" nature of a distribution, which is paramount for risk assessment. The presence of outliers or non-Gaussian measurement error often results in a posterior with a higher probability of extreme values than a simple Gaussian would suggest. This exercise [@problem_id:3383393] guides you to analyze a posterior resulting from mixture noise and to directly compare the true tail risk against the dangerously misleading estimates produced by both a MAP point estimate and a standard Gaussian approximation.", "problem": "Consider a scalar inverse problem with a single observation modeled by $y = \\theta + \\epsilon$, where $\\theta \\in \\mathbb{R}$ is the unknown parameter and $\\epsilon$ is an observational error. Assume a Gaussian prior $\\theta \\sim \\mathcal{N}(\\mu_0, s_0^2)$ and a finite mixture noise model $\\epsilon \\sim \\sum_{k=1}^{K} \\pi_k \\mathcal{N}(0,\\sigma_k^2)$, with weights $\\pi_k > 0$ such that $\\sum_{k=1}^{K} \\pi_k = 1$, and standard deviations $\\sigma_k > 0$. Under this model, the likelihood is a mixture of Gaussian densities, and the posterior $p(\\theta \\mid y)$ is non-Gaussian and can exhibit heavier tails than a single Gaussian posterior.\n\nYour tasks are:\n\n1. Starting from Bayes' theorem $p(\\theta \\mid y) \\propto p(y \\mid \\theta) p(\\theta)$, with $p(y \\mid \\theta) = \\sum_{k=1}^{K} \\pi_k \\,\\mathcal{N}(y \\mid \\theta, \\sigma_k^2)$ and $p(\\theta) = \\mathcal{N}(\\theta \\mid \\mu_0, s_0^2)$, derive the analytic form of the posterior as a finite mixture of Gaussian components. Use only the definition of a Gaussian density and algebraic identities for products of Gaussian functions. Do not assume conjugacy beyond these definitions and do not invoke any unproven shortcut formulas.\n\n2. Define the tail risk functional for a given threshold $T > 0$ as $R_T = \\mathbb{E}_{p(\\theta \\mid y)}[\\mathbf{1}\\{|\\theta| > T\\}] = \\mathbb{P}(|\\theta| > T \\mid y)$. Express $R_T$ in terms of the derived posterior mixture representation and standard Gaussian cumulative distribution functions.\n\n3. Obtain the Maximum A Posteriori (MAP) estimate, defined as $\\hat{\\theta}_{\\text{MAP}} = \\arg\\max_{\\theta} \\log p(\\theta \\mid y)$. Use this to compute a degenerate point-estimate tail risk $\\tilde{R}_T^{\\text{point}} = \\mathbf{1}\\{|\\hat{\\theta}_{\\text{MAP}}| > T\\}$.\n\n4. Construct a Gaussian (Laplace) approximation to $p(\\theta \\mid y)$ centered at $\\hat{\\theta}_{\\text{MAP}}$ with variance given by the negative inverse of the second derivative of $\\log p(\\theta \\mid y)$ at $\\hat{\\theta}_{\\text{MAP}}$, namely $\\tilde{p}(\\theta \\mid y) \\approx \\mathcal{N}(\\hat{\\theta}_{\\text{MAP}}, \\tilde{s}^2)$ where $\\tilde{s}^2 = \\left(-\\frac{d^2}{d\\theta^2} \\log p(\\theta \\mid y) \\big|_{\\theta=\\hat{\\theta}_{\\text{MAP}}}\\right)^{-1}$. Use this approximation to compute the Gaussian-approximate tail risk $\\tilde{R}_T^{\\text{Gauss}} = \\mathbb{P}_{\\tilde{p}}(|\\theta| > T)$.\n\n5. Implement a single, self-contained program that, for each test case below, computes and returns a triple of values $[R_T, \\tilde{R}_T^{\\text{Gauss}}, \\tilde{R}_T^{\\text{point}}]$, where each element is a real number except $\\tilde{R}_T^{\\text{point}}$ which is either $0$ or $1$. Your program should produce a single line of output containing the list of these triples for all test cases as a comma-separated list enclosed in square brackets.\n\nUse the following test suite, which covers general scenarios, boundary conditions, and edge cases:\n\n- Test case 1 (heavy-tailed mixture, moderate prior): $y = 1.0$, $\\mu_0 = 0.0$, $s_0 = 1.0$, $(\\pi_1, \\pi_2) = (0.9, 0.1)$, $(\\sigma_1, \\sigma_2) = (0.2, 3.0)$, $T = 2.0$.\n- Test case 2 (pure Gaussian noise, more informative prior): $y = 0.5$, $\\mu_0 = 0.0$, $s_0 = 0.5$, $(\\pi_1) = (1.0)$, $(\\sigma_1) = (0.3)$, $T = 1.0$.\n- Test case 3 (rare but extreme outliers, heavy-tailed posterior): $y = 3.0$, $\\mu_0 = 0.0$, $s_0 = 1.0$, $(\\pi_1, \\pi_2) = (0.95, 0.05)$, $(\\sigma_1, \\sigma_2) = (0.1, 10.0)$, $T = 2.5$.\n- Test case 4 (negligible outlier component, very large threshold): $y = 0.1$, $\\mu_0 = 0.0$, $s_0 = 1.0$, $(\\pi_1, \\pi_2) = (0.99, 0.01)$, $(\\sigma_1, \\sigma_2) = (0.2, 10.0)$, $T = 4.0$.\n\nAll parameters are dimensionless real numbers. The final output format must be a single line containing a list of the per-test-case triples in the form:\n[[R_T1,tilde_R_T1_Gauss,tilde_R_T1_point],[R_T2,tilde_R_T2_Gauss,tilde_R_T2_point],...].\n\nYour implementation must use only real arithmetic and standard mathematical functions; angles are not used; probabilities must be reported as decimals in $[0,1]$ without percentage signs. The program must be self-contained and must not require any external input.", "solution": "The problem has been validated and is deemed valid. It is a well-posed problem in Bayesian statistics that is scientifically grounded, self-contained, and objective. The tasks require standard, albeit detailed, derivations and numerical computations that are central to the field of inverse problems and data assimilation, specifically concerning the comparison of full posterior characterization against point and approximate estimates.\n\nHerein, I provide a complete, reasoned solution.\n\n### Part 1: Derivation of the Posterior Distribution\n\nThe goal is to determine the posterior probability density function $p(\\theta \\mid y)$ using Bayes' theorem.\n\n$p(\\theta \\mid y) \\propto p(y \\mid \\theta) p(\\theta)$\n\nThe problem specifies a Gaussian prior for the parameter $\\theta$ and a Gaussian mixture model for the noise $\\epsilon$, which defines the likelihood $p(y \\mid \\theta)$.\nThe prior is given by:\n$$p(\\theta) = \\mathcal{N}(\\theta \\mid \\mu_0, s_0^2) = \\frac{1}{\\sqrt{2\\pi s_0^2}} \\exp\\left(-\\frac{(\\theta - \\mu_0)^2}{2s_0^2}\\right)$$\nThe likelihood, resulting from the noise model $\\epsilon \\sim \\sum_{k=1}^{K} \\pi_k \\mathcal{N}(0, \\sigma_k^2)$ and the observation model $y = \\theta + \\epsilon$, is:\n$$p(y \\mid \\theta) = \\sum_{k=1}^{K} \\pi_k \\mathcal{N}(y \\mid \\theta, \\sigma_k^2) = \\sum_{k=1}^{K} \\pi_k \\frac{1}{\\sqrt{2\\pi \\sigma_k^2}} \\exp\\left(-\\frac{(y - \\theta)^2}{2\\sigma_k^2}\\right)$$\nSubstituting these into Bayes' theorem:\n$$p(\\theta \\mid y) \\propto \\left( \\sum_{k=1}^{K} \\pi_k \\mathcal{N}(y \\mid \\theta, \\sigma_k^2) \\right) \\mathcal{N}(\\theta \\mid \\mu_0, s_0^2)$$\nBy distributing the product over the sum, we get:\n$$p(\\theta \\mid y) \\propto \\sum_{k=1}^{K} \\pi_k \\left( \\mathcal{N}(y \\mid \\theta, \\sigma_k^2) \\mathcal{N}(\\theta \\mid \\mu_0, s_0^2) \\right)$$\nLet's analyze the product of the two Gaussian functions for a single component $k$:\n$$\\mathcal{N}(y \\mid \\theta, \\sigma_k^2) \\mathcal{N}(\\theta \\mid \\mu_0, s_0^2) \\propto \\exp\\left(-\\frac{(y - \\theta)^2}{2\\sigma_k^2}\\right) \\exp\\left(-\\frac{(\\theta - \\mu_0)^2}{2s_0^2}\\right)$$\nThe product is proportional to the exponential of the sum of the exponents:\n$$-\\frac{1}{2} \\left( \\frac{(y - \\theta)^2}{\\sigma_k^2} + \\frac{(\\theta - \\mu_0)^2}{s_0^2} \\right) = -\\frac{1}{2} \\left( \\frac{\\theta^2 - 2y\\theta + y^2}{\\sigma_k^2} + \\frac{\\theta^2 - 2\\mu_0\\theta + \\mu_0^2}{s_0^2} \\right)$$\nWe collect terms in powers of $\\theta$ to complete the square.\nThe $\\theta^2$ term coefficient is $\\left(\\frac{1}{\\sigma_k^2} + \\frac{1}{s_0^2}\\right)$.\nThe $\\theta$ term coefficient is $-2\\left(\\frac{y}{\\sigma_k^2} + \\frac{\\mu_0}{s_0^2}\\right)$.\nA Gaussian density in $\\theta$, $\\mathcal{N}(\\theta \\mid \\mu, s^2)$, has an exponent of $-\\frac{1}{2s^2}(\\theta - \\mu)^2 = -\\frac{1}{2s^2}(\\theta^2 - 2\\mu\\theta + \\mu^2)$.\nBy matching coefficients, we define the posterior component's precision (inverse variance) and mean.\nThe precision is $s_{k,post}^{-2} = \\frac{1}{\\sigma_k^2} + \\frac{1}{s_0^2}$, so the variance is:\n$$s_{k,post}^2 = \\left(\\frac{1}{\\sigma_k^2} + \\frac{1}{s_0^2}\\right)^{-1} = \\frac{\\sigma_k^2 s_0^2}{\\sigma_k^2 + s_0^2}$$\nThe mean is given by $\\frac{\\mu_{k,post}}{s_{k,post}^2} = \\frac{y}{\\sigma_k^2} + \\frac{\\mu_0}{s_0^2}$, so:\n$$\\mu_{k,post} = s_{k,post}^2 \\left(\\frac{y}{\\sigma_k^2} + \\frac{\\mu_0}{s_0^2}\\right)$$\nThe product of the two Gaussians is thus proportional to a new Gaussian in $\\theta$: $\\mathcal{N}(\\theta \\mid \\mu_{k,post}, s_{k,post}^2)$. A full analysis of the product of two Gaussian PDFs shows that $\\mathcal{N}(x \\mid a, A) \\mathcal{N}(x \\mid b, B) = C \\cdot \\mathcal{N}(x \\mid c, C')$, where $C$ is a constant with respect to $x$. Applying this, we find:\n$$\\mathcal{N}(y \\mid \\theta, \\sigma_k^2) \\mathcal{N}(\\theta \\mid \\mu_0, s_0^2) = \\mathcal{N}(y \\mid \\mu_0, \\sigma_k^2 + s_0^2) \\cdot \\mathcal{N}(\\theta \\mid \\mu_{k,post}, s_{k,post}^2)$$\nThe term $C_k = \\mathcal{N}(y \\mid \\mu_0, \\sigma_k^2 + s_0^2)$ is the marginal likelihood of the observation $y$ under component $k$, also known as the evidence for that component.\nSubstituting this back into the expression for the posterior:\n$$p(\\theta \\mid y) \\propto \\sum_{k=1}^{K} \\pi_k C_k \\mathcal{N}(\\theta \\mid \\mu_{k,post}, s_{k,post}^2)$$\nTo make this a valid probability density, we must normalize it. Let $\\tilde{w}_k = \\pi_k C_k$. The normalization constant is $Z = \\int p(y \\mid \\theta) p(\\theta) d\\theta = \\sum_{k=1}^{K} \\tilde{w}_k$. The normalized posterior is a mixture of Gaussians:\n$$p(\\theta \\mid y) = \\sum_{k=1}^{K} w_k \\mathcal{N}(\\theta \\mid \\mu_{k,post}, s_{k,post}^2)$$\nwhere the posterior weights $w_k$ are given by:\n$$w_k = \\frac{\\tilde{w}_k}{\\sum_{j=1}^{K} \\tilde{w}_j} = \\frac{\\pi_k \\mathcal{N}(y \\mid \\mu_0, \\sigma_k^2 + s_0^2)}{\\sum_{j=1}^{K} \\pi_j \\mathcal{N}(y \\mid \\mu_0, \\sigma_j^2 + s_0^2)}$$\n\n### Part 2: Derivation of the Full Posterior Tail Risk ($R_T$)\n\nThe tail risk $R_T$ is the probability that the absolute value of the parameter $\\theta$ exceeds a threshold $T$, given the data $y$.\n$$R_T = \\mathbb{P}(|\\theta| > T \\mid y) = \\int_{|\\theta|>T} p(\\theta \\mid y) d\\theta$$\nThis is more easily computed as $1$ minus the probability of being within the interval $[-T, T]$:\n$$R_T = 1 - \\mathbb{P}(-T \\le \\theta \\le T \\mid y) = 1 - \\int_{-T}^{T} p(\\theta \\mid y) d\\theta$$\nSubstituting the derived posterior mixture:\n$$R_T = 1 - \\int_{-T}^{T} \\sum_{k=1}^{K} w_k \\mathcal{N}(\\theta \\mid \\mu_{k,post}, s_{k,post}^2) d\\theta$$\nBy linearity of integration, we can swap the sum and the integral:\n$$R_T = 1 - \\sum_{k=1}^{K} w_k \\int_{-T}^{T} \\mathcal{N}(\\theta \\mid \\mu_{k,post}, s_{k,post}^2) d\\theta$$\nThe integral for each component is the difference of the cumulative distribution function (CDF) values at the endpoints. Let $\\Phi(z)$ be the CDF of the standard normal distribution $\\mathcal{N}(0, 1)$. The CDF of $\\mathcal{N}(\\mu, s^2)$ is $\\Phi\\left(\\frac{x-\\mu}{s}\\right)$.\n$$\\int_{-T}^{T} \\mathcal{N}(\\theta \\mid \\mu_{k,post}, s_{k,post}^2) d\\theta = \\Phi\\left(\\frac{T-\\mu_{k,post}}{s_{k,post}}\\right) - \\Phi\\left(\\frac{-T-\\mu_{k,post}}{s_{k,post}}\\right)$$\nThus, the final expression for the exact tail risk is:\n$$R_T = 1 - \\sum_{k=1}^{K} w_k \\left[ \\Phi\\left(\\frac{T-\\mu_{k,post}}{s_{k,post}}\\right) - \\Phi\\left(\\frac{-T-\\mu_{k,post}}{s_{k,post}}\\right) \\right]$$\n\n### Part 3: MAP Estimation and Point-Estimate Risk ($\\tilde{R}_T^{\\text{point}}$)\n\nThe Maximum A Posteriori (MAP) estimate, $\\hat{\\theta}_{\\text{MAP}}$, is the value of $\\theta$ that maximizes the posterior probability density.\n$$\\hat{\\theta}_{\\text{MAP}} = \\underset{\\theta}{\\arg\\max} \\, p(\\theta \\mid y) = \\underset{\\theta}{\\arg\\max} \\, \\log p(\\theta \\mid y)$$\nMaximizing the log-posterior is equivalent and often numerically more stable. Since the normalization constant is independent of $\\theta$, we can maximize the unnormalized posterior $g(\\theta) = p(y \\mid \\theta)p(\\theta)$.\n$$L(\\theta) = \\log g(\\theta) = \\log\\left( \\sum_{k=1}^{K} \\pi_k C_k \\mathcal{N}(\\theta \\mid \\mu_{k,post}, s_{k,post}^2) \\right)$$\nTo find the maximum, we set the derivative of $L(\\theta)$ with respect to $\\theta$ to zero:\n$$\\frac{dL}{d\\theta} = \\frac{1}{g(\\theta)} \\frac{dg}{d\\theta} = 0 \\implies \\frac{dg}{d\\theta} = 0$$\n$$\\frac{dg}{d\\theta} = \\frac{d}{d\\theta} \\sum_{k=1}^{K} \\tilde{w}_k \\mathcal{N}(\\theta \\mid \\mu_{k,post}, s_{k,post}^2) = \\sum_{k=1}^{K} \\tilde{w}_k \\left( -\\frac{\\theta - \\mu_{k,post}}{s_{k,post}^2} \\right) \\mathcal{N}(\\theta \\mid \\mu_{k,post}, s_{k,post}^2) = 0$$\nThis is a nonlinear equation in $\\theta$ and generally does not have an analytical solution for $K > 1$. Therefore, $\\hat{\\theta}_{\\text{MAP}}$ must be found using numerical optimization or root-finding algorithms.\nThe point-estimate tail risk is then a binary indicator:\n$$\\tilde{R}_T^{\\text{point}} = \\mathbf{1}\\{|\\hat{\\theta}_{\\text{MAP}}| > T\\} = \\begin{cases} 1 & \\text{if } |\\hat{\\theta}_{\\text{MAP}}| > T \\\\ 0 & \\text{if } |\\hat{\\theta}_{\\text{MAP}}| \\le T \\end{cases}$$\n\n### Part 4: Laplace Approximation and Approximate Tail Risk ($\\tilde{R}_T^{\\text{Gauss}}$)\n\nThe Laplace approximation, or Gaussian approximation, approximates the posterior distribution with a Gaussian centered at its mode ($\\hat{\\theta}_{\\text{MAP}}$). The variance of this Gaussian is determined by the curvature of the log-posterior at the mode.\n$$\\tilde{p}(\\theta \\mid y) \\approx \\mathcal{N}(\\theta \\mid \\hat{\\theta}_{\\text{MAP}}, \\tilde{s}^2)$$\nThe variance $\\tilde{s}^2$ is the negative inverse of the second derivative of the log-posterior evaluated at the MAP estimate:\n$$\\tilde{s}^2 = \\left(-\\frac{d^2}{d\\theta^2} \\log p(\\theta \\mid y) \\Big|_{\\theta=\\hat{\\theta}_{\\text{MAP}}}\\right)^{-1}$$\nLet's compute the second derivative of $L(\\theta) = \\log g(\\theta)$:\n$$\\frac{d^2L}{d\\theta^2} = \\frac{g''(\\theta)g(\\theta) - (g'(\\theta))^2}{g(\\theta)^2}$$\nAt $\\theta = \\hat{\\theta}_{\\text{MAP}}$, we have $g'(\\hat{\\theta}_{\\text{MAP}}) = 0$, so the expression simplifies to:\n$$\\frac{d^2L}{d\\theta^2}\\bigg|_{\\hat{\\theta}_{\\text{MAP}}} = \\frac{g''(\\hat{\\theta}_{\\text{MAP}})}{g(\\hat{\\theta}_{\\text{MAP}})}$$\nWe need the second derivative of $g(\\theta)$:\n$$\\frac{d^2}{d\\theta^2}\\mathcal{N}(\\theta|\\mu,s^2) = \\left(\\frac{(\\theta-\\mu)^2}{s^4} - \\frac{1}{s^2}\\right) \\mathcal{N}(\\theta|\\mu,s^2)$$\nSo, $g''(\\theta) = \\sum_{k=1}^K \\tilde{w}_k \\left(\\frac{(\\theta-\\mu_{k,post})^2}{s_{k,post}^4} - \\frac{1}{s_{k,post}^2}\\right) \\mathcal{N}(\\theta \\mid \\mu_{k,post}, s_{k,post}^2)$.\nThe variance of the Laplace approximation is then:\n$$\\tilde{s}^2 = - \\frac{g(\\hat{\\theta}_{\\text{MAP}})}{g''(\\hat{\\theta}_{\\text{MAP}})}$$\nThe Gaussian-approximate tail risk, $\\tilde{R}_T^{\\text{Gauss}}$, is calculated using the CDF of this single Gaussian approximation, analogous to Part 2:\n$$\\tilde{R}_T^{\\text{Gauss}} = 1 - \\left[ \\Phi\\left(\\frac{T-\\hat{\\theta}_{\\text{MAP}}}{\\tilde{s}}\\right) - \\Phi\\left(\\frac{-T-\\hat{\\theta}_{\\text{MAP}}}{\\tilde{s}}\\right) \\right]$$\n\n### Part 5: Algorithmic Implementation\n\nThe program will implement the above derivations for each test case as follows:\n1.  Given the input parameters ($y, \\mu_0, s_0, \\{\\pi_k\\}, \\{\\sigma_k\\}, T$), compute the parameters of the posterior mixture components ($\\mu_{k,post}, s_{k,post}^2$) for each $k=1, \\dots, K$.\n2.  Compute the unnormalized weights $\\tilde{w}_k = \\pi_k \\mathcal{N}(y \\mid \\mu_0, \\sigma_k^2 + s_0^2)$ and the normalized weights $w_k$.\n3.  Calculate the exact tail risk $R_T$ using the mixture CDF formula from Part 2.\n4.  Numerically find $\\hat{\\theta}_{\\text{MAP}}$ by minimizing the negative of the unnormalized log-posterior function, $-\\log g(\\theta)$, using a bounded scalar optimizer.\n5.  Compute the point-estimate risk $\\tilde{R}_T^{\\text{point}}$ based on $\\hat{\\theta}_{\\text{MAP}}$.\n6.  Evaluate $g(\\hat{\\theta}_{\\text{MAP}})$ and $g''(\\hat{\\theta}_{\\text{MAP}})$ at the found MAP estimate.\n7.  Calculate the Laplace variance $\\tilde{s}^2$ and standard deviation $\\tilde{s}$.\n8.  Compute the Gaussian-approximated tail risk $\\tilde{R}_T^{\\text{Gauss}}$ using the CDF of the resulting $\\mathcal{N}(\\hat{\\theta}_{\\text{MAP}}, \\tilde{s}^2)$ distribution.\n9.  Collect the three values $[R_T, \\tilde{R}_T^{\\text{Gauss}}, \\tilde{R}_T^{\\text{point}}]$ into a list for each test case.\n10. Format the final output as a list of these lists.\n\nThis approach provides a complete solution to the problem by systematically deriving all required quantities and then outlining a clear computational strategy.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm\nfrom scipy.optimize import minimize_scalar\n\ndef solve():\n    \"\"\"\n    Solves the Bayesian inference problem for multiple test cases.\n\n    For each case, it calculates:\n    1. The exact posterior tail risk (R_T).\n    2. The tail risk from a Gaussian (Laplace) approximation (R_T_Gauss).\n    3. The tail risk from a point (MAP) estimate (R_T_point).\n    \"\"\"\n    test_cases = [\n        # Case 1: y=1.0, mu0=0.0, s0=1.0, pis=(0.9, 0.1), sigmas=(0.2, 3.0), T=2.0\n        {\"y\": 1.0, \"mu0\": 0.0, \"s0\": 1.0, \"pis\": [0.9, 0.1], \"sigmas\": [0.2, 3.0], \"T\": 2.0},\n        # Case 2: y=0.5, mu0=0.0, s0=0.5, pis=(1.0,), sigmas=(0.3,), T=1.0\n        {\"y\": 0.5, \"mu0\": 0.0, \"s0\": 0.5, \"pis\": [1.0], \"sigmas\": [0.3], \"T\": 1.0},\n        # Case 3: y=3.0, mu0=0.0, s0=1.0, pis=(0.95, 0.05), sigmas=(0.1, 10.0), T=2.5\n        {\"y\": 3.0, \"mu0\": 0.0, \"s0\": 1.0, \"pis\": [0.95, 0.05], \"sigmas\": [0.1, 10.0], \"T\": 2.5},\n        # Case 4: y=0.1, mu0=0.0, s0=1.0, pis=(0.99, 0.01), sigmas=(0.2, 10.0), T=4.0\n        {\"y\": 0.1, \"mu0\": 0.0, \"s0\": 1.0, \"pis\": [0.99, 0.01], \"sigmas\": [0.2, 10.0], \"T\": 4.0},\n    ]\n\n    results = []\n\n    for case in test_cases:\n        y, mu0, s0, pis, sigmas, T = case.values()\n        \n        K = len(pis)\n        sigmas = np.array(sigmas)\n        pis = np.array(pis)\n\n        # Step 1: Calculate posterior mixture parameters\n        s0_sq = s0**2\n        sigmas_sq = sigmas**2\n        \n        # Posterior component variances and means\n        s_post_sq = 1.0 / (1.0/s0_sq + 1.0/sigmas_sq)\n        s_post = np.sqrt(s_post_sq)\n        mu_post = s_post_sq * (mu0/s0_sq + y/sigmas_sq)\n\n        # Unnormalized posterior weights C_k = N(y | mu0, s0^2 + sigma_k^2)\n        C_k = norm.pdf(y, loc=mu0, scale=np.sqrt(s0_sq + sigmas_sq))\n        w_tilde = pis * C_k\n\n        # Normalized posterior weights\n        Z = np.sum(w_tilde)\n        w_k = w_tilde / Z\n\n        # Step 2: Calculate exact posterior tail risk R_T\n        prob_inside_T = np.sum(w_k * (norm.cdf(T, loc=mu_post, scale=s_post) - norm.cdf(-T, loc=mu_post, scale=s_post)))\n        R_T = 1.0 - prob_inside_T\n\n        # Step 3: Find MAP estimate and point-estimate risk\n        # The unnormalized posterior is g(theta) = sum_k w_tilde_k * N(theta|mu_post_k, s_post_k)\n        def neg_log_unnormalized_posterior(theta):\n            pdf_vals = norm.pdf(theta, loc=mu_post, scale=s_post)\n            g_theta = np.sum(w_tilde * pdf_vals)\n            # Add small epsilon to prevent log(0) for theta far in the tails\n            return -np.log(g_theta + 1e-300)\n\n        # A generous search range for the optimizer is sufficient\n        search_min = np.min(mu_post) - 10 * np.max(s_post)\n        search_max = np.max(mu_post) + 10 * np.max(s_post)\n        \n        # Use a numerical minimizer to find the mode (MAP)\n        res = minimize_scalar(neg_log_unnormalized_posterior, bounds=(search_min, search_max), method='bounded')\n        theta_map = res.x\n        \n        R_T_point = 1 if np.abs(theta_map) > T else 0\n\n        # Step 4: Calculate Laplace approximation and its tail risk\n        # g(theta_map)\n        g_theta_map_pdf_vals = norm.pdf(theta_map, loc=mu_post, scale=s_post)\n        g_theta_map = np.sum(w_tilde * g_theta_map_pdf_vals)\n\n        # g''(theta_map)\n        g_pp_term1 = ((theta_map - mu_post)**2 / s_post_sq**2)\n        g_pp_term2 = 1.0 / s_post_sq\n        g_pp_factors = (g_pp_term1 - g_pp_term2) * g_theta_map_pdf_vals\n        g_pp_theta_map = np.sum(w_tilde * g_pp_factors)\n\n        # The second derivative of log-posterior at a mode must be negative.\n        # This implies g''(theta_map) must be negative.\n        if g_pp_theta_map >= 0:\n            # This indicates a numerical issue or that the found point is not a proper maximum.\n            # For this problem's setup, this is not expected. A NaN signals failure.\n            s_tilde_sq = -1.0 \n        else:\n            s_tilde_sq = -g_theta_map / g_pp_theta_map\n        \n        if s_tilde_sq < 0:\n            R_T_gauss = np.nan # Should not happen in these a-cases\n        else:\n            s_tilde = np.sqrt(s_tilde_sq)\n            prob_inside_T_gauss = norm.cdf(T, loc=theta_map, scale=s_tilde) - norm.cdf(-T, loc=theta_map, scale=s_tilde)\n            R_T_gauss = 1.0 - prob_inside_T_gauss\n        \n        results.append([R_T, R_T_gauss, R_T_point])\n\n    # Format output as a list of lists strings\n    formatted_results = [f\"[{r[0]},{r[1]},{r[2]}]\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3383393"}]}