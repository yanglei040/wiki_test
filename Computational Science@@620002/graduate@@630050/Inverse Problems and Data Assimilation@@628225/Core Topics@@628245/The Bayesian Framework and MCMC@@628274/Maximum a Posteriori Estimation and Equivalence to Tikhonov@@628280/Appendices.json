{"hands_on_practices": [{"introduction": "The equivalence between Bayesian Maximum a Posteriori (MAP) estimation and Tikhonov regularization is a cornerstone of modern inverse problem theory. This first exercise provides a foundational derivation, guiding you from the probabilistic definitions of Gaussian likelihoods and priors to the deterministic objective function used in variational methods. By considering the specific case of diagonal covariance matrices, you will develop a clear intuition for how statistical uncertainties (variances) in your data and prior knowledge translate directly into the anisotropic weights of a Tikhonov functional.", "problem": "Consider a linear inverse problem in which a state vector $x \\in \\mathbb{R}^{n}$ is observed through a linear operator $H \\in \\mathbb{R}^{m \\times n}$ with additive noise, yielding observations $y \\in \\mathbb{R}^{m}$ that satisfy $y = H x + \\varepsilon$. Assume the observational noise $\\varepsilon$ is Gaussian with zero mean and covariance matrix $R \\in \\mathbb{R}^{m \\times m}$, and the prior distribution for $x$ is Gaussian with mean $x_{b} \\in \\mathbb{R}^{n}$ and covariance matrix $B \\in \\mathbb{R}^{n \\times n}$. Both $R$ and $B$ are symmetric positive definite and diagonal, with $R = \\operatorname{diag}(r_{1}, \\dots, r_{m})$ and $B = \\operatorname{diag}(b_{1}, \\dots, b_{n})$, where $r_{i} > 0$ and $b_{j} > 0$ for all indices.\n\nStarting from Bayes’ rule and the definition of the multivariate normal density, derive the Maximum a Posteriori (MAP) objective $J(x)$ defined as the negative logarithm of the posterior density up to an additive constant. Then rewrite $J(x)$ in a component-wise form that makes explicit the contribution of each observation and each state component when $R$ and $B$ are diagonal. Based on this component-wise form, identify the anisotropic Tikhonov weights that arise in the data-misfit and prior-penalty terms.\n\nYour final answer must be the single closed-form analytic expression for the MAP objective $J(x)$ written in component-wise form. No rounding is required. Do not include units in your final answer.", "solution": "The problem as stated is valid. It is scientifically grounded in Bayesian statistics and linear algebra, well-posed with a clear objective, objective in its language, and contains all necessary information for a rigorous derivation. We may therefore proceed with the solution.\n\nThe problem requires the derivation of the Maximum a Posteriori (MAP) objective function for a linear inverse problem with Gaussian assumptions. The MAP estimate of the state vector $x$ is the mode of the posterior probability distribution $p(x|y)$. According to Bayes' rule, the posterior is proportional to the product of the likelihood and the prior:\n$$p(x|y) \\propto p(y|x) p(x)$$\nwhere $p(y|x)$ is the likelihood of observing $y$ given the state $x$, and $p(x)$ is the prior probability of the state $x$.\n\nThe problem states that the observation model is $y = Hx + \\varepsilon$, where the noise term $\\varepsilon$ follows a multivariate normal distribution with zero mean and covariance matrix $R$, denoted as $\\varepsilon \\sim \\mathcal{N}(0, R)$. Consequently, the likelihood function $p(y|x)$ describes a random variable $y$ that is normally distributed with mean $Hx$ and covariance matrix $R$. The probability density function (PDF) for this likelihood is:\n$$p(y|x) = \\frac{1}{(2\\pi)^{m/2} (\\det R)^{1/2}} \\exp\\left(-\\frac{1}{2} (y - Hx)^T R^{-1} (y - Hx)\\right)$$\n\nThe prior distribution for the state $x$ is given as a multivariate normal distribution with mean $x_b$ and covariance matrix $B$, denoted as $x \\sim \\mathcal{N}(x_b, B)$. The PDF for the prior is:\n$$p(x) = \\frac{1}{(2\\pi)^{n/2} (\\det B)^{1/2}} \\exp\\left(-\\frac{1}{2} (x - x_b)^T B^{-1} (x - x_b)\\right)$$\n\nSubstituting these PDFs into Bayes' rule, we obtain the posterior distribution:\n$$p(x|y) \\propto \\exp\\left(-\\frac{1}{2} (y - Hx)^T R^{-1} (y - Hx)\\right) \\exp\\left(-\\frac{1}{2} (x - x_b)^T B^{-1} (x - x_b)\\right)$$\n$$p(x|y) \\propto \\exp\\left(-\\frac{1}{2} \\left[ (y - Hx)^T R^{-1} (y - Hx) + (x - x_b)^T B^{-1} (x - x_b) \\right] \\right)$$\n\nThe MAP estimate is the value of $x$ that maximizes $p(x|y)$. Because the natural logarithm is a monotonically increasing function, maximizing $p(x|y)$ is equivalent to maximizing $\\ln(p(x|y))$, which in turn is equivalent to minimizing $-\\ln(p(x|y))$. We define the MAP objective function $J(x)$ as the negative logarithm of the posterior, up to an additive constant that does not depend on $x$.\n$$J(x) = -\\ln(p(x|y)) + \\text{constant}$$\n$$J(x) = \\frac{1}{2} (y - Hx)^T R^{-1} (y - Hx) + \\frac{1}{2} (x - x_b)^T B^{-1} (x - x_b)$$\nThis expression is the general form of the MAP objective function for a linear-Gaussian problem. The first term is a measure of the data misfit, weighted by the inverse of the observation error covariance. The second term is a regularization or penalty term that measures the departure of the solution $x$ from the prior mean $x_b$, weighted by the inverse of the prior covariance.\n\nThe problem specifies that the covariance matrices $R \\in \\mathbb{R}^{m \\times m}$ and $B \\in \\mathbb{R}^{n \\times n}$ are diagonal:\n$$R = \\operatorname{diag}(r_1, r_2, \\dots, r_m)$$\n$$B = \\operatorname{diag}(b_1, b_2, \\dots, b_n)$$\nSince $R$ and $B$ are symmetric positive definite, all $r_i > 0$ and $b_j > 0$. Their inverses are also diagonal:\n$$R^{-1} = \\operatorname{diag}\\left(\\frac{1}{r_1}, \\frac{1}{r_2}, \\dots, \\frac{1}{r_m}\\right)$$\n$$B^{-1} = \\operatorname{diag}\\left(\\frac{1}{b_1}, \\frac{1}{b_2}, \\dots, \\frac{1}{b_n}\\right)$$\n\nWe can now rewrite the two quadratic forms in $J(x)$ in a component-wise manner.\nFor the data-misfit term, let the residual vector be $d = y - Hx$. Its $i$-th component is $d_i = y_i - (Hx)_i = y_i - \\sum_{j=1}^{n} H_{ij} x_j$. The quadratic form is:\n$$(y - Hx)^T R^{-1} (y - Hx) = d^T R^{-1} d = \\sum_{i=1}^{m} d_i (R^{-1})_{ii} d_i = \\sum_{i=1}^{m} \\frac{1}{r_i} d_i^2$$\n$$(y - Hx)^T R^{-1} (y - Hx) = \\sum_{i=1}^{m} \\frac{1}{r_i} \\left( y_i - \\sum_{j=1}^{n} H_{ij} x_j \\right)^2$$\n\nFor the prior-penalty term, let the deviation from the prior be $z = x - x_b$. Its $j$-th component is $z_j = x_j - (x_b)_j$. The quadratic form is:\n$$(x - x_b)^T B^{-1} (x - x_b) = z^T B^{-1} z = \\sum_{j=1}^{n} z_j (B^{-1})_{jj} z_j = \\sum_{j=1}^{n} \\frac{1}{b_j} z_j^2$$\n$$(x - x_b)^T B^{-1} (x - x_b) = \\sum_{j=1}^{n} \\frac{1}{b_j} (x_j - (x_b)_j)^2$$\n\nCombining these two component-wise expressions, the full MAP objective function $J(x)$ is:\n$$J(x) = \\frac{1}{2} \\sum_{i=1}^{m} \\frac{1}{r_i} \\left( y_i - \\sum_{j=1}^{n} H_{ij} x_j \\right)^2 + \\frac{1}{2} \\sum_{j=1}^{n} \\frac{1}{b_j} (x_j - (x_b)_j)^2$$\n\nThis form reveals the connection to anisotropic Tikhonov regularization. The objective function is a sum of squared norms, where each component of the sum is individually weighted.\nThe coefficients $\\{1/r_i\\}_{i=1}^m$ act as anisotropic weights for the data-misfit term. They are the inverse variances of the observation errors. An observation $y_i$ with a small error variance $r_i$ (i.e., a highly certain observation) is assigned a large weight $1/r_i$, penalizing deviations from it more heavily.\nSimilarly, the coefficients $\\{1/b_j\\}_{j=1}^n$ act as anisotropic weights for the prior-penalty (regularization) term. They are the inverse variances of the prior state components. A state component $x_j$ with a small prior variance $b_j$ (i.e., a highly certain prior estimate) is assigned a large weight $1/b_j$, penalizing deviations from its prior mean $(x_b)_j$ more heavily. The term Tikhonov regularization is justified as this objective function is equivalent to a weighted least-squares problem, which is a generalized form of Tikhonov regularization.", "answer": "$$\\boxed{J(x) = \\frac{1}{2} \\sum_{i=1}^{m} \\frac{1}{r_i} \\left( y_i - \\sum_{j=1}^{n} H_{ij} x_j \\right)^2 + \\frac{1}{2} \\sum_{j=1}^{n} \\frac{1}{b_j} (x_j - (x_b)_j)^2}$$", "id": "3401540"}, {"introduction": "While the assumption of Gaussian errors is mathematically convenient, real-world data is often corrupted by non-Gaussian noise, such as sporadic, large-magnitude outliers. This practice is a crucial exercise in robustness, demonstrating the sensitivity of the standard Tikhonov/MAP solution, which is based on an $\\ell_2$ norm, to such outliers. You will then explore a powerful alternative by replacing the Gaussian likelihood with a Laplace distribution, leading to an $\\ell_1$-norm data-misfit term that is inherently more robust and provides a practical tool for handling imperfect data.", "problem": "Consider a scalar linear inverse problem with identity forward operator. A single unknown parameter $x \\in \\mathbb{R}$ generates $M$ independent observations $y_{i} \\in \\mathbb{R}$ through $y_{i} = x + \\varepsilon_{i}$. In reality, the noise $\\varepsilon_{i}$ is heavy-tailed, occasionally producing large outliers, but the analyst incorrectly models it as Gaussian. The analyst also imposes a Gaussian prior on $x$. You are asked to analyze the Maximum A Posteriori (MAP) solution, connect it to Tikhonov regularization, and then propose and compute a robust alternative.\n\nUse the following fundamental bases:\n- Bayes’ rule for the posterior density: $\\pi(x \\mid y_{1:M}) \\propto \\pi(y_{1:M} \\mid x)\\,\\pi(x)$.\n- Gaussian density for a scalar random variable: if $z \\sim \\mathcal{N}(\\mu,\\sigma^{2})$ then $p(z) \\propto \\exp\\!\\big(-\\frac{1}{2\\sigma^{2}}(z-\\mu)^{2}\\big)$.\n- Laplace (double-exponential) density for a scalar random variable: if $z \\sim \\mathrm{Laplace}(0,b)$ then $p(z) \\propto \\exp\\!\\big(-\\frac{|z|}{b}\\big)$.\n\nTasks:\n1. Assume the analyst’s Gaussian likelihood model $\\varepsilon_{i} \\stackrel{\\text{i.i.d.}}{\\sim} \\mathcal{N}(0,\\sigma^{2})$ and Gaussian prior $x \\sim \\mathcal{N}(0,\\tau^{2})$. Starting from Bayes’ rule and the above densities, derive the MAP (Maximum A Posteriori) estimator as the minimizer of a deterministic objective, and show that it is equivalent to Tikhonov regularization. Then, derive a closed-form expression for the Gaussian-MAP estimator $\\hat{x}_{\\mathrm{G}}$ in terms of $M$, $\\sigma^{2}$, $\\tau^{2}$, and $\\{y_{i}\\}_{i=1}^{M}$.\n2. Consider the dataset with $M=3$ observations $y = \\{0,\\,0,\\,10\\}$, and modeling parameters $\\sigma^{2}=1$, $\\tau^{2}=1$. Compute the Gaussian-MAP estimate $\\hat{x}_{\\mathrm{G}}$ and briefly explain, via the objective structure, why an outlier drives sensitivity.\n3. To improve robustness, replace the Gaussian likelihood with a Laplace likelihood $\\varepsilon_{i} \\stackrel{\\text{i.i.d.}}{\\sim} \\mathrm{Laplace}(0,b)$ while retaining the Gaussian prior $x \\sim \\mathcal{N}(0,\\tau^{2})$. Derive the corresponding MAP objective $J_{\\mathrm{L}}(x)$ that must be minimized.\n4. For the same dataset $y = \\{0,\\,0,\\,10\\}$, with $b=1$ and $\\tau^{2}=1$, compute exactly the robust MAP estimate $\\hat{x}_{\\mathrm{L}}$ by satisfying first-order optimality conditions for a convex, possibly nonsmooth objective. Report $\\hat{x}_{\\mathrm{L}}$ as your final answer. No rounding is required.", "solution": "We proceed from first principles, beginning with Bayes’ rule and the specified likelihood and prior models.\n\n1. Under the analyst’s model, the likelihood and prior are\n$$\n\\pi(y_{1:M} \\mid x) \\;=\\; \\prod_{i=1}^{M} \\frac{1}{\\sqrt{2\\pi\\sigma^{2}}}\\,\\exp\\!\\left(-\\frac{1}{2\\sigma^{2}}(y_{i}-x)^{2}\\right),\n\\qquad\n\\pi(x) \\;=\\; \\frac{1}{\\sqrt{2\\pi\\tau^{2}}}\\,\\exp\\!\\left(-\\frac{1}{2\\tau^{2}}x^{2}\\right).\n$$\nBy Bayes’ rule, the posterior density satisfies\n$$\n\\pi(x \\mid y_{1:M}) \\;\\propto\\; \\pi(y_{1:M} \\mid x)\\,\\pi(x) \\;\\propto\\; \\exp\\!\\left(-\\frac{1}{2\\sigma^{2}}\\sum_{i=1}^{M} (y_{i}-x)^{2} - \\frac{1}{2\\tau^{2}} x^{2}\\right).\n$$\nMaximizing the posterior is equivalent to minimizing the negative log-posterior, which drops additive constants and yields the deterministic objective\n$$\nJ_{\\mathrm{G}}(x) \\;=\\; \\frac{1}{2\\sigma^{2}} \\sum_{i=1}^{M} (y_{i}-x)^{2} \\;+\\; \\frac{1}{2\\tau^{2}}\\,x^{2}.\n$$\nThis is a Tikhonov-regularized least-squares objective: the data misfit is quadratic and the prior contributes an $\\ell_{2}$-type penalty on $x$. To obtain the closed form $\\hat{x}_{\\mathrm{G}}$, we differentiate and set the derivative to zero:\n$$\n\\frac{\\mathrm{d} J_{\\mathrm{G}}}{\\mathrm{d}x} \\;=\\; \\frac{1}{\\sigma^{2}} \\sum_{i=1}^{M} (x - y_{i}) \\;+\\; \\frac{1}{\\tau^{2}} x\n\\;=\\; \\left(\\frac{M}{\\sigma^{2}} + \\frac{1}{\\tau^{2}}\\right)x \\;-\\; \\frac{1}{\\sigma^{2}} \\sum_{i=1}^{M} y_{i}.\n$$\nSetting $\\frac{\\mathrm{d} J_{\\mathrm{G}}}{\\mathrm{d}x}=0$ gives\n$$\n\\left(\\frac{M}{\\sigma^{2}} + \\frac{1}{\\tau^{2}}\\right)\\hat{x}_{\\mathrm{G}} \\;=\\; \\frac{1}{\\sigma^{2}} \\sum_{i=1}^{M} y_{i}\n\\quad\\Longrightarrow\\quad\n\\hat{x}_{\\mathrm{G}} \\;=\\; \\frac{\\sum_{i=1}^{M} y_{i}}{\\,M + \\sigma^{2}/\\tau^{2}\\,}.\n$$\n\n2. With $M=3$, $y=\\{0,\\,0,\\,10\\}$, $\\sigma^{2}=1$, and $\\tau^{2}=1$, we have $\\sum_{i=1}^{3} y_{i} = 10$ and $M + \\sigma^{2}/\\tau^{2} = 3 + 1 = 4$, so\n$$\n\\hat{x}_{\\mathrm{G}} \\;=\\; \\frac{10}{4} \\;=\\; 2.5.\n$$\nThe quadratic data misfit in $J_{\\mathrm{G}}(x)$ assigns high leverage to large residuals $(y_{i}-x)^{2}$, so a single outlier at $y_{3}=10$ substantially tilts the solution away from the bulk of the data at $0$ and toward $10$, illustrating sensitivity to outliers when heavy-tailed noise is misspecified as Gaussian.\n\n3. For robustness, consider the Laplace likelihood $\\varepsilon_{i} \\stackrel{\\text{i.i.d.}}{\\sim} \\mathrm{Laplace}(0,b)$, with density proportional to $\\exp\\!\\big(-|y_{i}-x|/b\\big)$, while retaining the Gaussian prior $x \\sim \\mathcal{N}(0,\\tau^{2})$. The posterior is then proportional to\n$$\n\\exp\\!\\left(-\\frac{1}{b} \\sum_{i=1}^{M} |y_{i}-x| \\right)\\;\\exp\\!\\left(-\\frac{1}{2\\tau^{2}} x^{2}\\right),\n$$\nso the MAP objective to minimize is\n$$\nJ_{\\mathrm{L}}(x) \\;=\\; \\frac{1}{b}\\,\\sum_{i=1}^{M} |y_{i}-x| \\;+\\; \\frac{1}{2\\tau^{2}}\\,x^{2}.\n$$\nThis objective employs an $\\ell_{1}$ data misfit that downweights outliers and a quadratic prior that stabilizes the estimate, yielding a convex, coercive problem.\n\n4. For $y=\\{0,\\,0,\\,10\\}$, $b=1$, and $\\tau^{2}=1$, the objective becomes\n$$\nJ_{\\mathrm{L}}(x) \\;=\\; |x-0| + |x-0| + |x-10| + \\frac{1}{2}x^{2} \\;=\\; 2|x| + |x-10| + \\frac{1}{2}x^{2}.\n$$\nBecause $J_{\\mathrm{L}}(x)$ is convex and piecewise differentiable, a necessary and sufficient condition for optimality is $0 \\in \\partial J_{\\mathrm{L}}(x)$, the subdifferential at $x$. The subgradient of $|x-a|$ is $\\partial |x-a| = \\{\\operatorname{sgn}(x-a)\\}$ if $x \\neq a$ and $\\partial |x-a| = [-1,1]$ if $x=a$. Therefore,\n$$\n\\partial J_{\\mathrm{L}}(x) \\;=\\; \\partial|x| + \\partial|x| + \\partial|x-10| + x.\n$$\nWe examine the possible regions:\n- If $x<0$, then $\\partial|x| = \\{-1\\}$ for both $|x|$ terms, and $\\partial|x-10| = \\{-1\\}$ since $x<10$, hence $\\partial J_{\\mathrm{L}}(x) = \\{-3 + x\\}$. Solving $-3 + x = 0$ gives $x=3$, which contradicts $x<0$, so no solution here.\n- If $0<x<10$, then $\\partial|x| = \\{+1\\}$ for both $|x|$ terms, and $\\partial|x-10| = \\{-1\\}$, hence $\\partial J_{\\mathrm{L}}(x) = \\{+1 + x\\}$. Solving $+1 + x = 0$ gives $x=-1$, which contradicts $0<x<10$, so no solution here.\n- If $x>10$, then $\\partial|x| = \\{+1\\}$ for both $|x|$ terms, and $\\partial|x-10| = \\{+1\\}$, hence $\\partial J_{\\mathrm{L}}(x) = \\{+3 + x\\}$. Solving $+3 + x = 0$ gives $x=-3$, which contradicts $x>10$, so no solution here.\n- At $x=10$, $\\partial|x| = \\{+1\\}$ for both, and $\\partial|x-10| = [-1,1]$, so $\\partial J_{\\mathrm{L}}(10) = [1,3] + 10 = [11,13]$, which does not contain $0$.\n- At $x=0$, $\\partial|x| = [-1,1]$ for each $|x|$, and $\\partial|x-10| = \\{-1\\}$, hence\n$$\n\\partial J_{\\mathrm{L}}(0) \\;=\\; [-1,1] + [-1,1] + \\{-1\\} + \\{0\\} \\;=\\; [-3,1].\n$$\nSince $0 \\in [-3,1]$, the subgradient optimality condition holds at $x=0$, and by strict convexity due to the quadratic term $(1/2)x^{2}$, this minimizer is unique.\n\nTherefore, the robust MAP estimate for the specified dataset and parameters is\n$$\n\\hat{x}_{\\mathrm{L}} \\;=\\; 0.\n$$\nThis robust estimator remains anchored by the two consistent observations at $0$ and is not pulled toward the outlier at $10$, illustrating reduced sensitivity under a heavy-tailed noise scenario modeled with a Laplace likelihood.\n\nThe requested final answer is the robust MAP estimate $\\hat{x}_{\\mathrm{L}}$ computed in this part.", "answer": "$$\\boxed{0}$$", "id": "3401499"}, {"introduction": "Moving from theory to practice, solving large-scale inverse problems requires efficient numerical algorithms that avoid forming and storing huge matrices. This exercise is designed to equip you with the essential tools for implementing modern, iterative optimization schemes. You will derive the gradient and the Hessian-vector product for the MAP objective function, expressing them in a 'matrix-free' form that relies only on the action of the forward operator and its adjoint, a critical skill for tackling high-dimensional problems in fields like data assimilation and medical imaging.", "problem": "Consider a linear observation model in an inverse problem and data assimilation setting. Let the unknown state vector be $x \\in \\mathbb{R}^{n}$ and the observed data be $y \\in \\mathbb{R}^{m}$. The observation operator is the linear map $A : \\mathbb{R}^{n} \\to \\mathbb{R}^{m}$. The observational noise $\\varepsilon \\in \\mathbb{R}^{m}$ is modeled as a realization of a zero-mean Gaussian random variable with covariance matrix $R \\in \\mathbb{R}^{m \\times m}$ that is symmetric positive definite. A Gaussian prior for the state $x$ is given by $x \\sim \\mathcal{N}(x_{b}, B)$, where $x_{b} \\in \\mathbb{R}^{n}$ is the background (prior mean) and $B \\in \\mathbb{R}^{n \\times n}$ is symmetric positive definite. Assume access to matrix-free operator actions for $A$, the adjoint $A^{T}$, and the inverses $R^{-1}$ and $B^{-1}$, but do not assume access to any explicit matrix factorizations or entries.\n\nStarting from Bayes' rule and the definitions of Gaussian probability density functions, derive the negative log-posterior objective for Maximum A Posteriori (MAP) estimation and show its equivalence to Tikhonov regularization. Then, derive expressions for the gradient and Hessian-vector product of this objective, explicitly in terms of the available operator actions $A$, $A^{T}$, $R^{-1}$, and $B^{-1}$. Your derivation must begin from the foundational definitions of the Gaussian likelihood and prior and proceed without invoking any pre-derived formulas.\n\nYou must express the final gradient as a function of $x$ and the Hessian-vector product as a function of an arbitrary vector $v \\in \\mathbb{R}^{n}$, both using only compositions of the available operator actions. Do not introduce any additional matrices beyond those specified. The target is a matrix-free implementation description: the expressions should be written so that each term corresponds to applying $A$, $A^{T}$, $R^{-1}$, and $B^{-1}$ to vectors.\n\nProvide your final answer as two closed-form analytic expressions: one for the gradient and one for the Hessian-vector product. The final answer must be a single row matrix containing these two expressions. No numerical computation is required.", "solution": "The problem statement will first be validated against the specified criteria.\n\n### Problem Validation\n\n#### Step 1: Extract Givens\n- Unknown state vector: $x \\in \\mathbb{R}^{n}$\n- Observed data: $y \\in \\mathbb{R}^{m}$\n- Observation operator (linear map): $A : \\mathbb{R}^{n} \\to \\mathbb{R}^{m}$\n- Linear observation model: $y = Ax + \\varepsilon$\n- Observational noise: $\\varepsilon \\in \\mathbb{R}^{m}$ is a realization of a zero-mean Gaussian random variable, so $\\varepsilon \\sim \\mathcal{N}(0, R)$.\n- Noise covariance matrix: $R \\in \\mathbb{R}^{m \\times m}$, symmetric positive definite.\n- Prior distribution for the state: $x \\sim \\mathcal{N}(x_{b}, B)$.\n- Prior mean (background state): $x_{b} \\in \\mathbb{R}^{n}$.\n- Prior covariance matrix: $B \\in \\mathbb{R}^{n \\times n}$, symmetric positive definite.\n- Available operator actions: $A$, $A^{T}$, $R^{-1}$, and $B^{-1}$ applied to vectors.\n\n#### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded:** The problem is a standard formulation in Bayesian inverse problems and data assimilation, forming the basis for methods like 3D-Var and 4D-Var. The assumptions of Gaussian priors and noise are fundamental and widely used in these fields. The problem is scientifically sound.\n- **Well-Posed:** The problem is a request for a derivation of standard results. The assumptions that $R$ and $B$ are symmetric positive definite ensure that the probability density functions are well-defined and the resulting optimization problem is strictly convex with a unique minimum. The problem is well-posed.\n- **Objective:** The problem is stated in precise mathematical language, free from subjectivity or ambiguity.\n\nThe problem does not violate any of the disqualifying criteria listed (Scientific Unsoundness, Non-Formalizable, Incomplete Setup, Unrealistic, Ill-Posed, Pseudo-Profound, Outside Verifiability). It is a standard, well-defined theoretical problem in computational science.\n\n#### Step 3: Verdict and Action\nThe problem is valid. A complete, reasoned solution will be provided.\n\n### Derivation of the MAP Objective and Equivalence to Tikhonov Regularization\n\nThe goal of Maximum A Posteriori (MAP) estimation is to find the state $x$ that maximizes the posterior probability density function (PDF) $p(x|y)$ given the observation $y$. According to Bayes' rule, the posterior is proportional to the product of the likelihood and the prior:\n$$p(x|y) \\propto p(y|x) p(x)$$\nMaximizing $p(x|y)$ is equivalent to maximizing its logarithm, $\\ln(p(x|y))$, which is in turn equivalent to minimizing its negative logarithm, $-\\ln(p(x|y))$. The MAP estimate is therefore given by:\n$$x_{\\text{MAP}} = \\arg\\max_{x} p(x|y) = \\arg\\min_{x} [-\\ln(p(y|x)) - \\ln(p(x))]$$\nWe now define the likelihood and prior terms based on the given Gaussian distributions. The PDF for a general multivariate Gaussian random variable $z \\in \\mathbb{R}^k$ with mean $\\mu$ and covariance $\\Sigma$ is:\n$$p(z) = \\frac{1}{\\sqrt{(2\\pi)^k \\det(\\Sigma)}} \\exp\\left(-\\frac{1}{2} (z-\\mu)^T \\Sigma^{-1} (z-\\mu)\\right)$$\n\n**1. Likelihood Term:**\nThe likelihood $p(y|x)$ is the PDF of the observation $y$ given the state $x$. The observation model is $y = Ax + \\varepsilon$, where $\\varepsilon \\sim \\mathcal{N}(0, R)$. This implies that for a given $x$, $y$ is a Gaussian random variable with mean $E[y|x] = E[Ax + \\varepsilon] = Ax + E[\\varepsilon] = Ax$ and covariance $\\text{Cov}(y|x) = \\text{Cov}(Ax + \\varepsilon) = \\text{Cov}(\\varepsilon) = R$. Thus, $y|x \\sim \\mathcal{N}(Ax, R)$. The likelihood function is:\n$$p(y|x) \\propto \\exp\\left(-\\frac{1}{2} (y-Ax)^T R^{-1} (y-Ax)\\right)$$\nThe negative log-likelihood, ignoring constant terms, is:\n$$-\\ln(p(y|x)) = \\frac{1}{2} (y-Ax)^T R^{-1} (y-Ax) + C_1$$\n\n**2. Prior Term:**\nThe prior distribution for the state $x$ is given as $x \\sim \\mathcal{N}(x_{b}, B)$. Its PDF is:\n$$p(x) \\propto \\exp\\left(-\\frac{1}{2} (x-x_{b})^T B^{-1} (x-x_{b})\\right)$$\nThe negative log-prior, ignoring constant terms, is:\n$$-\\ln(p(x)) = \\frac{1}{2} (x-x_{b})^T B^{-1} (x-x_{b}) + C_2$$\n\n**3. MAP Objective Function:**\nCombining the negative log-likelihood and negative log-prior, we obtain the objective function $J(x)$ to be minimized:\n$$J(x) = \\frac{1}{2} (y-Ax)^T R^{-1} (y-Ax) + \\frac{1}{2} (x-x_{b})^T B^{-1} (x-x_{b})$$\nThis is the negative log-posterior objective function for MAP estimation (up to an additive constant and a scaling factor of $2$).\n\n**Equivalence to Tikhonov Regularization:**\nTikhonov regularization is a method for solving ill-posed inverse problems, typically of the form $Ax=y$. The regularized solution is found by minimizing an objective function that combines a data-fidelity term and a regularization term:\n$$\\min_{x} \\left( \\|Ax - y\\|^2_{W} + \\|\\mathcal{L}(x - x_0)\\|^2_{P} \\right)$$\nwhere $\\|\\cdot\\|_M$ denotes the weighted norm defined by $\\|v\\|_M^2 = v^T M v$, and $\\mathcal{L}$ is a regularization operator.\n\nThe MAP objective function $J(x)$ can be written using this norm notation. By setting the weighting matrix for the data-fidelity term to $W=R^{-1}$, the regularization operator to the identity $\\mathcal{L}=I$, the reference solution to the prior mean $x_0 = x_b$, and the regularization weighting matrix to $P=B^{-1}$, our objective becomes:\n$$2J(x) = \\|Ax - y\\|^2_{R^{-1}} + \\|x - x_{b}\\|^2_{B^{-1}}$$\nThis demonstrates that MAP estimation under Gaussian assumptions is mathematically equivalent to Tikhonov regularization. The first term, $\\|Ax - y\\|^2_{R^{-1}}$, measures the misfit between the model prediction $Ax$ and the data $y$, weighted by the inverse of the observation error covariance. The second term, $\\|x - x_{b}\\|^2_{B^{-1}}$, is a regularization term that penalizes solutions deviating from the prior belief $x_b$, weighted by the inverse of the prior covariance.\n\n### Gradient and Hessian-Vector Product Derivation\n\n**1. Gradient of the Objective Function:**\nThe gradient $\\nabla J(x)$ is found by differentiating $J(x)$ with respect to the vector $x$. We differentiate the two terms of $J(x)$ separately.\nLet $J_d(x) = \\frac{1}{2} (y-Ax)^T R^{-1} (y-Ax)$ and $J_b(x) = \\frac{1}{2} (x-x_{b})^T B^{-1} (x-x_{b})$.\n\nFor the data term $J_d(x)$, we consider a small perturbation $x \\to x + \\delta x$.\nThe change in the residual is $y - A(x+\\delta x) = (y-Ax) - A\\delta x$.\n$$J_d(x+\\delta x) = \\frac{1}{2} ((y-Ax) - A\\delta x)^T R^{-1} ((y-Ax) - A\\delta x)$$\n$$J_d(x+\\delta x) = \\frac{1}{2} (y-Ax)^T R^{-1} (y-Ax) - \\frac{1}{2} (A\\delta x)^T R^{-1} (y-Ax) - \\frac{1}{2} (y-Ax)^T R^{-1} (A\\delta x) + O(\\|\\delta x\\|^2)$$\nSince $R$ and thus $R^{-1}$ are symmetric, the two linear terms are equal.\n$$J_d(x+\\delta x) = J_d(x) - (y-Ax)^T R^{-1} A \\delta x + O(\\|\\delta x\\|^2)$$\nThe directional derivative is $\\nabla J_d(x)^T \\delta x = (A^T R^{-1} (Ax-y))^T \\delta x$. Therefore, the gradient of the data term is:\n$$\\nabla J_d(x) = A^T R^{-1} (Ax - y)$$\n\nFor the background term $J_b(x)$, a similar derivation yields:\n$$\\nabla J_b(x) = B^{-1} (x - x_b)$$\n\nThe total gradient of the objective function is the sum of the gradients of its parts:\n$$\\nabla J(x) = \\nabla J_d(x) + \\nabla J_b(x) = A^T R^{-1} (Ax - y) + B^{-1} (x - x_b)$$\nThis expression is structured for matrix-free implementation. For instance, computing $A^T R^{-1} (Ax - y)$ involves applying $A$ to $x$, a vector subtraction, applying $R^{-1}$ to the result, and finally applying $A^T$.\n\n**2. Hessian-Vector Product:**\nThe Hessian matrix $H(x) = \\nabla^2 J(x)$ is obtained by differentiating the gradient $\\nabla J(x)$ with respect to $x$. The Hessian-vector product $H(x)v$ for an arbitrary vector $v \\in \\mathbb{R}^n$ can be derived by finding the directional derivative of the gradient, i.e., $H(x)v = \\frac{d}{d\\epsilon} [\\nabla J(x + \\epsilon v)]_{\\epsilon=0}$.\n\nLet's compute $\\nabla J(x+\\epsilon v)$:\n$$\\nabla J(x+\\epsilon v) = A^T R^{-1} (A(x+\\epsilon v) - y) + B^{-1} ((x+\\epsilon v) - x_b)$$\n$$\\nabla J(x+\\epsilon v) = A^T R^{-1} (Ax - y) + \\epsilon A^T R^{-1} Av + B^{-1} (x - x_b) + \\epsilon B^{-1} v$$\n$$\\nabla J(x+\\epsilon v) = \\nabla J(x) + \\epsilon (A^T R^{-1} Av + B^{-1} v)$$\nNow, we differentiate with respect to $\\epsilon$:\n$$\\frac{d}{d\\epsilon} [\\nabla J(x + \\epsilon v)] = A^T R^{-1} Av + B^{-1} v$$\nThis expression is independent of $\\epsilon$, so setting $\\epsilon=0$ gives the Hessian-vector product:\n$$H(x)v = A^T R^{-1} Av + B^{-1} v$$\nThe Hessian itself is $H(x) = A^T R^{-1} A + B^{-1}$, which is constant with respect to $x$. This is expected for a quadratic objective function. The computation of $H(x)v$ is matrix-free: it involves applying $A$ to $v$, then $R^{-1}$, then $A^T$, and adding the result of applying $B^{-1}$ to $v$.", "answer": "$$\\boxed{\\begin{pmatrix} A^{T} R^{-1} (Ax - y) + B^{-1} (x - x_{b}) & A^{T} R^{-1} Av + B^{-1} v \\end{pmatrix}}$$", "id": "3401544"}]}