{"hands_on_practices": [{"introduction": "This first practice provides a foundational exercise in the analytical construction of credible intervals. Using the Gamma distribution, a common form for posterior distributions of positive parameters, you will derive both the equal-tailed and the Highest Posterior Density (HPD) intervals from first principles [@problem_id:3373872]. This exercise is crucial for solidifying the definitions and appreciating the key optimality property of HPD regions—that they represent the shortest possible interval for a given credibility level.", "problem": "Consider a Bayesian inverse problem in which a positive scalar parameter $\\theta$ governs a data likelihood that yields a conjugate posterior of the form $\\theta \\mid y \\sim \\mathrm{Ga}(a,b)$ with shape $a>0$ and rate $b>0$, so that the posterior density is proportional to $\\theta^{a-1}\\exp(-b\\theta)$ on $(0,\\infty)$. In data assimilation contexts, this posterior can arise, for example, when aggregating independent exponential waiting-time data or Poisson process counts under a Gamma prior, but the specific data model is not required here. Let $F(\\theta)$ denote the cumulative distribution function (CDF) of the Gamma distribution with parameters $(a,b)$, and let $P(a,x)$ denote the regularized lower incomplete gamma function $P(a,x) \\equiv \\gamma(a,x)/\\Gamma(a)$, where $\\gamma(a,x)$ is the lower incomplete gamma function and $\\Gamma(a)$ is the gamma function. Recall that for $\\theta \\sim \\mathrm{Ga}(a,b)$, one has $F(\\theta) = P(a,b\\theta)$. Define $P^{-1}(a,q)$ as the inverse of $x \\mapsto P(a,x)$ at level $q \\in (0,1)$, namely $P(a,P^{-1}(a,q)) = q$.\n\nStarting from the definitions of equal-tailed $(1-\\alpha)$ credible intervals and Highest Posterior Density (HPD) $(1-\\alpha)$ credible intervals for unimodal and monotone posterior densities, do the following:\n\n1. Derive the endpoints $\\theta_{\\mathrm{L}}^{\\mathrm{ET}}$ and $\\theta_{\\mathrm{U}}^{\\mathrm{ET}}$ of the equal-tailed $(1-\\alpha)$ credible interval and express its length $L_{\\mathrm{ET}}(a,b,\\alpha)$ explicitly in terms of $P^{-1}(a,\\cdot)$ and $b$.\n\n2. Derive the HPD $(1-\\alpha)$ credible interval endpoints $\\theta_{\\mathrm{L}}^{\\mathrm{HPD}}$ and $\\theta_{\\mathrm{U}}^{\\mathrm{HPD}}$, distinguishing clearly between the cases $a \\leq 1$ (monotone decreasing posterior density with mode at $0$) and $a>1$ (unimodal posterior density with interior mode at $\\theta^{\\star} = (a-1)/b$). For $a \\leq 1$, express the HPD interval length $L_{\\mathrm{HPD}}(a,b,\\alpha)$ explicitly in terms of $P^{-1}(a,\\cdot)$ and $b$. For $a>1$, show that the HPD endpoints satisfy the equal-density condition $\\theta_{\\mathrm{L}}^{a-1}\\exp(-b\\theta_{\\mathrm{L}}) = \\theta_{\\mathrm{U}}^{a-1}\\exp(-b\\theta_{\\mathrm{U}})$ and the probability content condition $F(\\theta_{\\mathrm{U}}) - F(\\theta_{\\mathrm{L}}) = 1 - \\alpha$, and express $L_{\\mathrm{HPD}}(a,b,\\alpha)$ in terms of the unique solution to these equations.\n\n3. Compare the lengths by forming the ratio $R(a,b,\\alpha) \\equiv L_{\\mathrm{HPD}}(a,b,\\alpha) / L_{\\mathrm{ET}}(a,b,\\alpha)$ and simplify it as far as possible to exhibit its dependence on $(a,\\alpha)$ and the cancellation of the rate $b$.\n\nYour final answer must be the single analytic expression for $R(a,b,\\alpha)$, and no numerical evaluation is required.", "solution": "The problem requires the derivation and comparison of equal-tailed (ET) and highest posterior density (HPD) credible intervals for a parameter $\\theta$ whose posterior distribution is a Gamma distribution, $\\theta \\mid y \\sim \\mathrm{Ga}(a,b)$. The posterior probability density function (PDF) is given by $p(\\theta \\mid y) = \\frac{b^a}{\\Gamma(a)}\\theta^{a-1}\\exp(-b\\theta)$ for $\\theta > 0$. The corresponding cumulative distribution function (CDF) is $F(\\theta) = P(a,b\\theta)$, where $P(a,x)$ is the regularized lower incomplete gamma function.\n\nFirst, I will validate the problem statement.\n**Step 1: Extract Givens**\n- Parameter of interest: $\\theta$, a positive scalar.\n- Posterior distribution: $\\theta \\mid y \\sim \\mathrm{Ga}(a,b)$ with shape $a>0$ and rate $b>0$.\n- Posterior PDF proportionality: $p(\\theta \\mid y) \\propto \\theta^{a-1}\\exp(-b\\theta)$ on $(0,\\infty)$.\n- CDF notation: $F(\\theta)$.\n- Regularized lower incomplete gamma function: $P(a,x) \\equiv \\gamma(a,x)/\\Gamma(a)$.\n- CDF formula: $F(\\theta) = P(a,b\\theta)$.\n- Inverse function notation: $P^{-1}(a,q)$ is the inverse of $x \\mapsto P(a,x)$.\n- Credibility level: $(1-\\alpha)$.\n- Tasks: (1) Derive the endpoints and length of the $(1-\\alpha)$ ET interval. (2) Derive the endpoints and length of the $(1-\\alpha)$ HPD interval for cases $a \\leq 1$ and $a>1$. (3) Compute the ratio of the lengths $R(a,b,\\alpha) = L_{\\mathrm{HPD}}/L_{\\mathrm{ET}}$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded in Bayesian statistics and probability theory. The Gamma distribution, credible intervals (ET and HPD), and the regularized incomplete gamma function are all standard, well-defined concepts. The problem is well-posed, providing all necessary definitions to proceed with the derivations. The distinction between the cases $a \\le 1$ (monotonically decreasing posterior PDF) and $a > 1$ (unimodal posterior PDF) is correct and crucial for determining the HPD interval, indicating the problem is well-structured. It requires substantive derivation and is not trivial. All terms are objective and precisely defined. The problem is valid.\n\n**Step 3: Verdict and Action**\nThe problem is valid. I will proceed with the solution.\n\n**Part 1: Equal-Tailed (ET) Credible Interval**\nAn equal-tailed $(1-\\alpha)$ credible interval $[\\theta_{\\mathrm{L}}^{\\mathrm{ET}}, \\theta_{\\mathrm{U}}^{\\mathrm{ET}}]$ is defined by placing $\\alpha/2$ of the posterior probability in each tail.\nThe lower bound $\\theta_{\\mathrm{L}}^{\\mathrm{ET}}$ satisfies $P(\\theta  \\theta_{\\mathrm{L}}^{\\mathrm{ET}}) = F(\\theta_{\\mathrm{L}}^{\\mathrm{ET}}) = \\alpha/2$.\nUsing the given relation $F(\\theta) = P(a,b\\theta)$, we have:\n$$P(a, b\\theta_{\\mathrm{L}}^{\\mathrm{ET}}) = \\frac{\\alpha}{2}$$\nApplying the inverse function $P^{-1}(a,\\cdot)$ to both sides gives $b\\theta_{\\mathrm{L}}^{\\mathrm{ET}} = P^{-1}(a, \\alpha/2)$. Thus, the lower endpoint is:\n$$\\theta_{\\mathrm{L}}^{\\mathrm{ET}} = \\frac{1}{b} P^{-1}\\left(a, \\frac{\\alpha}{2}\\right)$$\nThe upper bound $\\theta_{\\mathrm{U}}^{\\mathrm{ET}}$ satisfies $P(\\theta > \\theta_{\\mathrm{U}}^{\\mathrm{ET}}) = 1 - F(\\theta_{\\mathrm{U}}^{\\mathrm{ET}}) = \\alpha/2$, which implies $F(\\theta_{\\mathrm{U}}^{\\mathrm{ET}}) = 1 - \\alpha/2$.\n$$P(a, b\\theta_{\\mathrm{U}}^{\\mathrm{ET}}) = 1 - \\frac{\\alpha}{2}$$\nSolving for $\\theta_{\\mathrm{U}}^{\\mathrm{ET}}$ yields:\n$$\\theta_{\\mathrm{U}}^{\\mathrm{ET}} = \\frac{1}{b} P^{-1}\\left(a, 1-\\frac{\\alpha}{2}\\right)$$\nThe length of the equal-tailed interval, $L_{\\mathrm{ET}}(a,b,\\alpha)$, is the difference between the endpoints:\n$$L_{\\mathrm{ET}}(a,b,\\alpha) = \\theta_{\\mathrm{U}}^{\\mathrm{ET}} - \\theta_{\\mathrm{L}}^{\\mathrm{ET}} = \\frac{1}{b} \\left[ P^{-1}\\left(a, 1-\\frac{\\alpha}{2}\\right) - P^{-1}\\left(a, \\frac{\\alpha}{2}\\right) \\right]$$\n\n**Part 2: Highest Posterior Density (HPD) Credible Interval**\nAn HPD interval is the shortest possible interval for a given credibility level. Its construction depends on the modality of the posterior PDF, $p(\\theta \\mid y) \\propto \\theta^{a-1}\\exp(-b\\theta)$. To determine the shape, we examine the sign of the derivative of the log-PDF, $g(\\theta) = (a-1)\\ln\\theta - b\\theta + \\text{const}$. The derivative is $g'(\\theta) = (a-1)/\\theta - b$.\n\nCase $a \\leq 1$:\nIf $a \\leq 1$, then $a-1 \\leq 0$. Since $b>0$ and $\\theta>0$, $g'(\\theta)  0$ for all $\\theta \\in (0,\\infty)$. The posterior PDF is therefore a monotonically decreasing function of $\\theta$. For such a density, the shortest interval capturing a probability of $(1-\\alpha)$ must start at the left boundary of the support, which is $\\theta=0$.\nThe HPD interval is of the form $[0, \\theta_{\\mathrm{U}}^{\\mathrm{HPD}}]$. The lower endpoint is $\\theta_{\\mathrm{L}}^{\\mathrm{HPD}} = 0$.\nThe upper endpoint $\\theta_{\\mathrm{U}}^{\\mathrm{HPD}}$ is determined by the probability content condition:\n$$P(0 \\leq \\theta \\leq \\theta_{\\mathrm{U}}^{\\mathrm{HPD}}) = F(\\theta_{\\mathrm{U}}^{\\mathrm{HPD}}) - F(0) = 1-\\alpha$$\nSince $F(0)=0$ for the Gamma distribution, this simplifies to $F(\\theta_{\\mathrm{U}}^{\\mathrm{HPD}}) = 1-\\alpha$. Using the CDF formula:\n$$P(a, b\\theta_{\\mathrm{U}}^{\\mathrm{HPD}}) = 1-\\alpha$$\nSolving for $\\theta_{\\mathrm{U}}^{\\mathrm{HPD}}$ gives:\n$$\\theta_{\\mathrm{U}}^{\\mathrm{HPD}} = \\frac{1}{b} P^{-1}(a, 1-\\alpha)$$\nThe length of the HPD interval for $a \\leq 1$ is:\n$$L_{\\mathrm{HPD}}(a,b,\\alpha) = \\theta_{\\mathrm{U}}^{\\mathrm{HPD}} - \\theta_{\\mathrm{L}}^{\\mathrm{HPD}} = \\frac{1}{b} P^{-1}(a, 1-\\alpha)$$\n\nCase $a > 1$:\nIf $a > 1$, then $a-1 > 0$. The mode of the distribution is found by setting $g'(\\theta)=0$, which yields $\\theta^{\\star} = (a-1)/b > 0$. The posterior PDF is unimodal, starting at $0$ at $\\theta=0$, increasing to a maximum at $\\theta^{\\star}$, and decreasing towards $0$ as $\\theta \\to \\infty$.\nFor a unimodal distribution, the $(1-\\alpha)$ HPD interval $[\\theta_{\\mathrm{L}}^{\\mathrm{HPD}}, \\theta_{\\mathrm{U}}^{\\mathrm{HPD}}]$ has the property that the posterior density is equal at its endpoints:\n$$p(\\theta_{\\mathrm{L}}^{\\mathrm{HPD}} \\mid y) = p(\\theta_{\\mathrm{U}}^{\\mathrm{HPD}} \\mid y)$$\nUsing the proportionality of the PDF, this gives the first condition as required:\n$$(\\theta_{\\mathrm{L}}^{\\mathrm{HPD}})^{a-1}\\exp(-b\\theta_{\\mathrm{L}}^{\\mathrm{HPD}}) = (\\theta_{\\mathrm{U}}^{\\mathrm{HPD}})^{a-1}\\exp(-b\\theta_{\\mathrm{U}}^{\\mathrm{HPD}})$$\nThe second condition is that the interval must contain $(1-\\alpha)$ of the posterior probability mass:\n$$\\int_{\\theta_{\\mathrm{L}}^{\\mathrm{HPD}}}^{\\theta_{\\mathrm{U}}^{\\mathrm{HPD}}} p(\\theta \\mid y) d\\theta = 1-\\alpha$$\nIn terms of the CDF $F$, this is the second required condition:\n$$F(\\theta_{\\mathrm{U}}^{\\mathrm{HPD}}) - F(\\theta_{\\mathrm{L}}^{\\mathrm{HPD}}) = 1-\\alpha$$\nThe endpoints $\\theta_{\\mathrm{L}}^{\\mathrm{HPD}}$ and $\\theta_{\\mathrm{U}}^{\\mathrm{HPD}}$ are the unique solution to this system of two nonlinear equations. The length of the HPD interval is then expressed in terms of this unique solution:\n$$L_{\\mathrm{HPD}}(a,b,\\alpha) = \\theta_{\\mathrm{U}}^{\\mathrm{HPD}} - \\theta_{\\mathrm{L}}^{\\mathrm{HPD}}$$\n\n**Part 3: Ratio of Lengths**\nThe ratio is $R(a,b,\\alpha) = L_{\\mathrm{HPD}}(a,b,\\alpha) / L_{\\mathrm{ET}}(a,b,\\alpha)$. To simplify this and show its independence from $b$, we introduce scaled, dimensionless endpoints. Let $x = b\\theta$. The distribution of $x$ is $\\mathrm{Ga}(a,1)$, with CDF $P(a,x)$.\n\nThe length of the ET interval in the scaled variable is $P^{-1}(a, 1-\\alpha/2) - P^{-1}(a, \\alpha/2)$.\nTherefore, $L_{\\mathrm{ET}}(a,b,\\alpha) = \\frac{1}{b} \\left[ P^{-1}\\left(a, 1-\\frac{\\alpha}{2}\\right) - P^{-1}\\left(a, \\frac{\\alpha}{2}\\right) \\right]$.\n\nThe length of the HPD interval depends on $a$. We define the scaled HPD endpoints, $x_{\\mathrm{L}}(a,\\alpha)$ and $x_{\\mathrm{U}}(a,\\alpha)$, such that $\\theta_{\\mathrm{L}}^{\\mathrm{HPD}} = x_{\\mathrm{L}}/b$ and $\\theta_{\\mathrm{U}}^{\\mathrm{HPD}} = x_{\\mathrm{U}}/b$.\n- For $a \\leq 1$:\n$x_{\\mathrm{L}}(a,\\alpha) = 0$\n$x_{\\mathrm{U}}(a,\\alpha) = P^{-1}(a, 1-\\alpha)$\n- For $a > 1$, $x_{\\mathrm{L}}(a,\\alpha)$ and $x_{\\mathrm{U}}(a,\\alpha)$ are the unique solution to the system:\n1. $(x_{\\mathrm{L}})^{a-1}\\exp(-x_{\\mathrm{L}}) = (x_{\\mathrm{U}})^{a-1}\\exp(-x_{\\mathrm{U}})$\n2. $P(a, x_{\\mathrm{U}}) - P(a, x_{\\mathrm{L}}) = 1-\\alpha$\n\nIn all cases, the length of the HPD interval can be written as $L_{\\mathrm{HPD}}(a,b,\\alpha) = \\frac{1}{b} (x_{\\mathrm{U}}(a,\\alpha) - x_{\\mathrm{L}}(a,\\alpha))$.\n\nNow we form the ratio $R(a,b,\\alpha)$:\n$$R(a,b,\\alpha) = \\frac{L_{\\mathrm{HPD}}(a,b,\\alpha)}{L_{\\mathrm{ET}}(a,b,\\alpha)} = \\frac{\\frac{1}{b}(x_{\\mathrm{U}}(a,\\alpha) - x_{\\mathrm{L}}(a,\\alpha))}{\\frac{1}{b}\\left[ P^{-1}\\left(a, 1-\\frac{\\alpha}{2}\\right) - P^{-1}\\left(a, \\frac{\\alpha}{2}\\right) \\right]}$$\nThe rate parameter $b$ cancels out, as expected. The resulting expression depends only on $a$ and $\\alpha$:\n$$R(a,\\alpha) = \\frac{x_{\\mathrm{U}}(a,\\alpha) - x_{\\mathrm{L}}(a,\\alpha)}{P^{-1}\\left(a, 1-\\frac{\\alpha}{2}\\right) - P^{-1}\\left(a, \\frac{\\alpha}{2}\\right)}$$\nThis is the single requested analytic expression, where the functions $x_{\\mathrm{L}}(a,\\alpha)$ and $x_{\\mathrm{U}}(a,\\alpha)$ are defined piece-wise based on the value of $a$ as established above. This expression represents the ratio of the width of the HPD region to the width of the ET region, in the scaled coordinates.", "answer": "$$\\boxed{\\frac{x_{\\mathrm{U}}(a,\\alpha) - x_{\\mathrm{L}}(a,\\alpha)}{P^{-1}\\left(a, 1-\\frac{\\alpha}{2}\\right) - P^{-1}\\left(a, \\frac{\\alpha}{2}\\right)}}$$", "id": "3373872"}, {"introduction": "In many real-world data assimilation problems, the posterior distribution is not available in a clean analytical form but is instead approximated by a set of weighted samples or 'particles'. This practice challenges you to translate the theoretical concept of an HPD region into a concrete set of algorithms for these empirical distributions [@problem_id:3373825]. By implementing methods to find HPD sets and shortest intervals, you will gain hands-on experience with the practical side of uncertainty quantification and learn to diagnose critical issues like particle degeneracy.", "problem": "You are given a Bayesian filtering context where the posterior distribution at time step $k$, denoted $p(x_k \\mid y_{1:k})$, is approximated by a discrete weighted set of particles $\\{(w_i, x_k^{(i)})\\}_{i=1}^N$ with nonnegative weights that sum to one. Starting from the fundamental definition of a highest posterior density (HPD) region as a set of the form $\\{x : p(x \\mid y_{1:k}) \\ge \\lambda\\}$ for some threshold $\\lambda$ chosen so that the posterior probability mass of the set equals a specified credibility level, derive an empirical method to construct an HPD region using the weighted particles. Your derivation must proceed from the definition of HPD regions and the discrete measure induced by the particle approximation. You must also derive a one-dimensional shortest-interval credible set based on ordering the particles by their state values and aggregating weights. Finally, you must quantify weight degeneracy and analyze its impact on the stability and coverage of the empirical HPD region.\n\nYour program must implement the following, grounded in the definitions:\n\n1. An empirical HPD set via thresholding on the discrete approximation:\n   - Using the discrete approximation $p(x \\mid y_{1:k}) \\approx \\sum_{i=1}^N w_i \\, \\delta(x - x^{(i)})$, show that an empirical HPD region at level $\\alpha \\in (0,1)$ can be constructed by selecting a minimal-cardinality subset of particles whose cumulative weight is at least $\\alpha$. This is achieved by sorting particles by weight in descending order and taking the smallest prefix whose weight sum exceeds $\\alpha$. Let the achieved mass of this set be $M_{\\mathrm{HPD}} \\ge \\alpha$, and define the overshoot as $M_{\\mathrm{HPD}} - \\alpha$.\n\n2. A one-dimensional shortest-interval credible set:\n   - When $x_k$ is scalar and particles are sorted by their state value $x^{(i)}$, derive that a shortest interval $[a,b]$ with minimal width $b-a$ that carries at least mass $\\alpha$ can be found by scanning pairs of indices and identifying the minimal-width index window whose cumulative weight is at least $\\alpha$. This sliding-window construction is valid on the empirical measure and coincides with the continuous HPD interval in unimodal cases.\n\n3. A quantitative measure of degeneracy:\n   - Use the effective sample size (ESS) defined for normalized weights by $\\mathrm{ESS} = 1 / \\sum_{i=1}^N w_i^2$ as a degeneracy indicator, and report the normalized effective sample size $\\mathrm{ESS}/N$.\n\n4. An empirical contiguity diagnostic for multimodality:\n   - Map the empirical HPD set from item $1$ onto the sorted-by-state order and count the number of contiguous index segments represented in the set. A value greater than one indicates non-contiguity, typical of multimodal posteriors.\n\nYour program must produce, for each test case, a list with the following six entries in this exact order:\n- The achieved mass $M_{\\mathrm{HPD}}$ of the empirical HPD set (float).\n- The overshoot $M_{\\mathrm{HPD}} - \\alpha$ (float).\n- The normalized effective sample size $\\mathrm{ESS}/N$ (float).\n- The number of particles in the empirical HPD set (integer).\n- The width $b-a$ of the shortest interval credible set (float; zero is permitted when a single particle already exceeds $\\alpha$).\n- The number of contiguous segments spanned by the empirical HPD set when particles are ordered by their state values (integer).\n\nAll floats must be rounded to exactly $6$ decimal places in the output.\n\nTest Suite. Your program must compute results for the following four independent cases, each posed in purely mathematical terms without physical units:\n\n- Case A (well-resolved unimodal posterior): $N = 801$ particles on a uniform grid $x^{(i)}$ from $-4$ to $4$ inclusive, with step $0.01$. Weights are proportional to the standard normal density $w_i \\propto \\exp(-\\tfrac{1}{2} (x^{(i)})^2)$ and normalized to sum to $1$. Use $\\alpha = 0.9$.\n- Case B (severe degeneracy): $N = 100$ particles at $x^{(i)}$ uniformly spaced from $0$ to $1$ inclusive. Let $w_1 = 0.91$ and for $i \\in \\{2,\\dots,N\\}$ set $w_i = 0.09 / (N-1)$. Use $\\alpha = 0.9$.\n- Case C (monotone posterior with skewed mass): $N = 500$ particles on a uniform grid $x^{(i)}$ from $0$ to $5$ inclusive. Weights are proportional to $\\exp(-x^{(i)})$ and normalized to sum to $1$. Use $\\alpha = 0.8$.\n- Case D (bimodal posterior): $N = 1201$ particles on a uniform grid $x^{(i)}$ from $-6$ to $6$ inclusive. Weights are proportional to the balanced Gaussian mixture $0.5 \\exp(-\\tfrac{1}{2}(x^{(i)}+2)^2) + 0.5 \\exp(-\\tfrac{1}{2}(x^{(i)}-2)^2)$ and normalized to sum to $1$. Use $\\alpha = 0.5$.\n\nFinal Output Format. Your program should produce a single line of output containing the results for the four cases as a comma-separated list of lists enclosed in square brackets, in the order A, B, C, D. Each inner list must contain the six numbers described above, with floats rounded to exactly $6$ decimal places. For example: \n\"[[M_A,overshoot_A,essnorm_A,k_A,width_A,segments_A],[M_B,overshoot_B,essnorm_B,k_B,width_B,segments_B],[M_C,overshoot_C,essnorm_C,k_C,width_C,segments_C],[M_D,overshoot_D,essnorm_D,k_D,width_D,segments_D]]\".", "solution": "The problem requires the derivation and implementation of several diagnostic tools for analyzing a posterior distribution approximated by a discrete set of weighted particles, $\\{(w_i, x_k^{(i)})\\}_{i=1}^N$. This discrete measure is given by $\\hat{p}(x) = \\sum_{i=1}^N w_i \\, \\delta(x - x^{(i)})$, where the weights are non-negative, $w_i \\ge 0$, and normalized to unity, $\\sum_{i=1}^N w_i = 1$. The credibility level is denoted by $\\alpha \\in (0,1)$.\n\n### 1. Empirical Highest Posterior Density (HPD) Set\n\nA highest posterior density (HPD) region is defined for a continuous probability density function $p(x)$ as the set $R_\\alpha = \\{x : p(x) \\ge \\lambda\\}$, where the threshold $\\lambda$ is chosen such that the probability mass in this region is exactly $\\alpha$, i.e., $\\int_{R_\\alpha} p(x) dx = \\alpha$. This means the HPD region includes all points where the posterior density is highest.\n\nFor our discrete measure $\\hat{p}(x)$, the \"density\" is non-zero only at the particle locations $x^{(i)}$, where it is infinite due to the Dirac delta function $\\delta(\\cdot)$. A direct application of the density threshold $\\lambda$ is ill-defined. Instead, we can adapt the core principle of HPD regions: to assemble a set of a given probability mass $\\alpha$ from the \"densest\" parts of the distribution. In the particle representation, the probability mass is concentrated at the particle locations, and the magnitude of this mass at each location is given by the particle's weight $w_i$. Therefore, the \"densest\" parts of the discrete distribution correspond to the particles with the largest weights.\n\nThis leads to the following construction for an empirical HPD set:\n1.  Sort the particles based on their weights in descending order. Let the sorted weights be $w_{(1)} \\ge w_{(2)} \\ge \\dots \\ge w_{(N)}$, and let the corresponding particles be $x^{((1))}, x^{((2))}, \\dots, x^{((N))}$.\n2.  Find the smallest integer $k_{\\mathrm{HPD}}$ such that the cumulative sum of the largest weights is at least $\\alpha$. That is, find the minimum $k$ satisfying:\n    $$ \\sum_{j=1}^{k} w_{(j)} \\ge \\alpha $$\n3.  The empirical HPD set is the collection of the $k_{\\mathrm{HPD}}$ particles with the highest weights: $\\{x^{((j))}\\}_{j=1}^{k_{\\mathrm{HPD}}}$. This construction naturally yields the set with the minimum number of particles (minimal cardinality) that achieves the desired probability mass.\n4.  The actual probability mass of this set, denoted $M_{\\mathrm{HPD}}$, is the sum of the weights of the selected particles:\n    $$ M_{\\mathrm{HPD}} = \\sum_{j=1}^{k_{\\mathrm{HPD}}} w_{(j)} $$\n    Due to the discrete nature of the weights, $M_{\\mathrm{HPD}}$ will generally be greater than or equal to $\\alpha$.\n5.  The overshoot is defined as the excess mass: $M_{\\mathrm{HPD}} - \\alpha$.\n\n### 2. One-Dimensional Shortest-Interval Credible Set\n\nFor a one-dimensional scalar state $x_k$, a credible interval is an interval $[a, b]$ such that the posterior probability of the state being in this interval is at least $\\alpha$. A common and desirable choice is the shortest such interval, which is often an HPD interval for unimodal distributions.\n\nGiven the discrete particle approximation, we seek an interval $[a, b]$ of minimal width $b-a$ such that $\\mathbb{P}(x_k \\in [a, b]) \\ge \\alpha$. For our discrete measure, this probability is $\\sum_{i: x^{(i)} \\in [a, b]} w_i$. The interval endpoints must coincide with particle locations to be minimal. The task is thus to find a pair of particles $(x^{(i)}, x^{(j)})$ that minimizes the width $x^{(j)} - x^{(i)}$ while their enclosed particles have a total weight sum of at least $\\alpha$.\n\nThe derivation of the algorithm is as follows:\n1.  First, sort the particles by their state values: $x_{(1)} \\le x_{(2)} \\le \\dots \\le x_{(N)}$. Let the corresponding weights be $\\{w'_{(i)}\\}_{i=1}^N$.\n2.  The problem is to find indices $i$ and $j$ ($1 \\le i \\le j \\le N$) that minimize the width $x_{(j)} - x_{(i)}$ subject to the constraint $\\sum_{k=i}^{j} w'_{(k)} \\ge \\alpha$.\n3.  A special case occurs if any single particle has a weight $w_i \\ge \\alpha$. In this situation, the shortest interval collapses to a single point, with width $0$.\n4.  If no single particle suffices, we can employ an efficient two-pointer (or sliding window) algorithm. Initialize a start pointer $i=1$ and an end pointer $j=1$. Maintain a running sum of weights from $i$ to $j$.\n    - Expand the window by incrementing $j$ until the cumulative weight $\\sum_{k=i}^{j} w'_{(k)} \\ge \\alpha$.\n    - Once the condition is met, calculate the width $x_{(j)} - x_{(i)}$ and record it if it is smaller than the minimum width found so far.\n    - Then, shrink the window from the left by incrementing $i$ and subtracting $w'_{(i)}$ from the running sum. Repeat the process until all possible start points have been considered.\nThis procedure guarantees finding the globally minimal width interval.\n\n### 3. Quantitative Measure of Degeneracy: Effective Sample Size (ESS)\n\nParticle degeneracy occurs when a small number of particles have very large weights, while the rest have negligible weights. This means the diversity of the particle set is low, and the approximation of the posterior is poor. The Effective Sample Size (ESS) is a widely used metric to quantify this phenomenon. For normalized weights where $\\sum_{i=1}^N w_i = 1$, the ESS is defined as:\n$$ \\mathrm{ESS} = \\frac{1}{\\sum_{i=1}^N w_i^2} $$\nThe value of ESS ranges from $1$ (complete degeneracy, one particle has weight $1$) to $N$ (no degeneracy, all weights are equal, $w_i = 1/N$). To provide a scale-invariant measure, the ESS is often normalized by the total number of particles, $N$. The normalized ESS, $\\mathrm{ESS}/N$, ranges from $1/N$ to $1$, making it easy to interpret the level of degeneracy regardless of the total particle count. A low value (e.g., less than $0.5$) is often a signal that resampling is needed in a particle filter.\n\n### 4. Empirical Contiguity Diagnostic for Multimodality\n\nThe HPD set identifies regions of high probability mass. In a one-dimensional unimodal posterior, this region will be a single contiguous interval. In a multimodal posterior, the HPD set may consist of several disjoint intervals. We can use this property to diagnose multimodality in our particle approximation.\n\nThe diagnostic algorithm is as follows:\n1.  Identify the set of particles belonging to the empirical HPD set, as derived in section 1. Let $I_{\\mathrm{HPD}}$ be the set of original indices of these particles.\n2.  Sort all particles by their state values: $x_{(1)} \\le x_{(2)} \\le \\dots \\le x_{(N)}$. Keep track of the original index of each particle in this sorted list.\n3.  Create a boolean sequence of length $N$. The $j$-th element of this sequence is true if the particle at position $j$ in the state-sorted list (i.e., $x_{(j)}$) is a member of the HPD set, and false otherwise.\n4.  Count the number of contiguous blocks of `true` values in this boolean sequence. This can be done by iterating through the sequence and counting the number of times a `true` value follows a `false` value. For instance, the sequence `[F, T, T, F, T, T, T, F]` contains two such blocks, or segments.\n5.  A count of $1$ suggests that the high-density region is contiguous, which is typical for a unimodal distribution. A count greater than $1$ indicates that the HPD set is disjoint in the state space, providing strong evidence for a multimodal posterior distribution.", "answer": "```python\nimport numpy as np\n\ndef calculate_diagnostics(x_particles, weights, alpha):\n    \"\"\"\n    Calculates various diagnostics for a particle approximation of a posterior.\n\n    Args:\n        x_particles (np.ndarray): 1D array of particle states.\n        weights (np.ndarray): 1D array of corresponding particle weights.\n        alpha (float): The credibility level.\n\n    Returns:\n        list: A list containing six diagnostic values in a specific order.\n    \"\"\"\n    N = len(weights)\n\n    # 1. Empirical HPD Set\n    w_sorted_indices = np.argsort(weights)[::-1]\n    w_sorted_cumsum = np.cumsum(weights[w_sorted_indices])\n    \n    # Find the smallest number of particles whose cumulative weight is = alpha\n    hpd_k_idx = np.searchsorted(w_sorted_cumsum, alpha)\n    num_particles_hpd = hpd_k_idx + 1\n    \n    achieved_mass_hpd = w_sorted_cumsum[hpd_k_idx]\n    overshoot_hpd = achieved_mass_hpd - alpha\n    \n    hpd_original_indices = w_sorted_indices[:num_particles_hpd]\n\n    # 2. Normalized Effective Sample Size (ESS)\n    sum_w_sq = np.sum(weights**2)\n    ess_normalized = (1.0 / sum_w_sq) / N if sum_w_sq > 0 else 0.0\n\n    # 3. One-dimensional Shortest-Interval Credible Set\n    x_sorted_indices = np.argsort(x_particles)\n    x_sorted = x_particles[x_sorted_indices]\n    w_sorted_by_x = weights[x_sorted_indices]\n    \n    # Check for a single particle with weight = alpha\n    if np.any(w_sorted_by_x >= alpha):\n        shortest_interval_width = 0.0\n    else:\n        min_width = np.inf\n        # Efficient two-pointer sliding window algorithm\n        current_weight = 0.0\n        i = 0\n        for j in range(N):\n            current_weight += w_sorted_by_x[j]\n            while current_weight >= alpha:\n                width = x_sorted[j] - x_sorted[i]\n                if width  min_width:\n                    min_width = width\n                current_weight -= w_sorted_by_x[i]\n                i += 1\n        shortest_interval_width = min_width\n\n    # 4. Empirical Contiguity Diagnostic\n    is_in_hpd = np.zeros(N, dtype=bool)\n    is_in_hpd[hpd_original_indices] = True\n    \n    # Reorder the HPD membership mask according to the state-sorted particles\n    in_hpd_sorted_by_x = is_in_hpd[x_sorted_indices]\n    \n    # Pad with False to handle edges correctly when using diff\n    padded_mask = np.concatenate(([False], in_hpd_sorted_by_x, [False]))\n    \n    # A segment starts where the diff is 1 (False - True transition)\n    diff_mask = np.diff(padded_mask.astype(int))\n    num_segments = np.sum(diff_mask == 1)\n\n    return [\n        round(achieved_mass_hpd, 6),\n        round(overshoot_hpd, 6),\n        round(ess_normalized, 6),\n        num_particles_hpd,\n        round(shortest_interval_width, 6),\n        num_segments\n    ]\n\ndef solve():\n    \"\"\"\n    Main function to define test cases, run calculations, and print results.\n    \"\"\"\n    test_cases = [\n        # Case A: Well-resolved unimodal posterior\n        {\n            'name': 'A', 'N': 801, 'alpha': 0.9,\n            'x_gen': lambda n: np.linspace(-4, 4, n),\n            'w_gen': lambda x: np.exp(-0.5 * x**2)\n        },\n        # Case B: Severe degeneracy\n        {\n            'name': 'B', 'N': 100, 'alpha': 0.9,\n            'x_gen': lambda n: np.linspace(0, 1, n),\n            'w_gen': lambda x: np.array([0.91] + [0.09 / (len(x) - 1)] * (len(x) - 1))\n        },\n        # Case C: Monotone posterior with skewed mass\n        {\n            'name': 'C', 'N': 500, 'alpha': 0.8,\n            'x_gen': lambda n: np.linspace(0, 5, n),\n            'w_gen': lambda x: np.exp(-x)\n        },\n        # Case D: Bimodal posterior\n        {\n            'name': 'D', 'N': 1201, 'alpha': 0.5,\n            'x_gen': lambda n: np.linspace(-6, 6, n),\n            'w_gen': lambda x: 0.5 * np.exp(-0.5 * (x + 2)**2) + 0.5 * np.exp(-0.5 * (x - 2)**2)\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        N = case['N']\n        alpha = case['alpha']\n        \n        x_particles = case['x_gen'](N)\n        raw_weights = case['w_gen'](x_particles)\n        \n        # Normalize weights if they don't sum to 1 (Case B weights are pre-normalized)\n        if not np.isclose(np.sum(raw_weights), 1.0):\n            weights = raw_weights / np.sum(raw_weights)\n        else:\n            weights = raw_weights\n\n        case_results = calculate_diagnostics(x_particles, weights, alpha)\n        results.append(case_results)\n    \n    # Format the final output string as specified\n    result_strings = []\n    for R in results:\n        # R[0], R[1], R[2], R[4] are floats, R[3], R[5] are ints\n        s = f\"[{R[0]:.6f},{R[1]:.6f},{R[2]:.6f},{R[3]},{R[4]:.6f},{R[5]}]\"\n        result_strings.append(s)\n        \n    print(f\"[{','.join(result_strings)}]\")\n\nsolve()\n```", "id": "3373825"}, {"introduction": "Having learned to construct credible regions, we now turn to a critical and profound question: what exactly do these regions guarantee? This final practice explores the behavior of Bayesian credible sets in the realistic scenario where the scientific model is misspecified—that is, it does not perfectly represent the true data-generating process [@problem_id:3373836]. This conceptual exercise will challenge your interpretation of Bayesian uncertainty, forcing you to confront the subtle yet crucial distinction between a $(1-\\alpha)$ credible level and a $(1-\\alpha)$ frequentist coverage probability.", "problem": "Consider a Bayesian inverse problem in data assimilation where a parameter vector $\\theta \\in \\mathbb{R}^{p}$ governs a forward model $G(\\theta)$ and an assumed observation model $P_{\\theta}$ with density $p_{\\theta}(y)$, used to form a likelihood for independent data $\\{Y_{i}\\}_{i=1}^{n}$. A prior density $\\pi(\\theta)$ is proper and continuous on a neighborhood of a unique minimizer of the expected negative log-likelihood under the true data-generating distribution $P_{0}$, which is not contained in the model family $\\{P_{\\theta} : \\theta \\in \\Theta\\}$. Specifically, assume:\n- The data are generated independently from $P_{0}$ with density $p_{0}(y)$, and the model is misspecified in the sense that $P_{0} \\notin \\{P_{\\theta} : \\theta \\in \\Theta\\}$.\n- The Kullback–Leibler divergence $\\mathrm{KL}(P_{0}\\,\\|\\,P_{\\theta}) = \\int \\log\\!\\left(\\frac{p_{0}(y)}{p_{\\theta}(y)}\\right) p_{0}(y)\\,dy$ admits a unique minimizer $\\theta^{\\ast} \\in \\Theta$, and regularity conditions ensure differentiability and positive-definite curvature of the expected log-likelihood at $\\theta^{\\ast}$.\n- The posterior density is $\\pi(\\theta \\mid y_{1:n}) \\propto \\pi(\\theta)\\prod_{i=1}^{n} p_{\\theta}(Y_{i})$.\n\nLet $C_{n,1-\\alpha}$ be a $(1-\\alpha)$ Bayesian credible set for $\\theta$ derived from the posterior, and let $H_{n,1-\\alpha}$ be a $(1-\\alpha)$ Highest Posterior Density (HPD) region, that is, the set of $\\theta$ values whose posterior density exceeds a threshold chosen so that the posterior mass of $H_{n,1-\\alpha}$ equals $1-\\alpha$. Consider the frequentist coverage probability $P_{0}\\!\\left(\\theta^{\\ast} \\in C_{n,1-\\alpha}\\right)$ and $P_{0}\\!\\left(\\theta^{\\ast} \\in H_{n,1-\\alpha}\\right)$ as $n \\to \\infty$, under misspecification.\n\nWhich of the following statements are correct in this misspecified setting?\n\nA. Even when the true data-generating process $P_{0}$ lies outside the model, $(1-\\alpha)$ posterior credible sets and HPD regions achieve asymptotic frequentist coverage $1-\\alpha$ for the true parameter governing $P_{0}$, provided the prior is continuous and positive near $\\theta^{\\ast}$.\n\nB. The posterior concentrates around the pseudo-true parameter $\\theta^{\\ast}$ and is asymptotically normal under regularity; however, because posterior curvature reflects the model’s expected log-likelihood rather than the sampling variability under $P_{0}$, unadjusted HPD regions generically fail to achieve nominal frequentist coverage, which is instead determined by the misspecification “sandwich” variability of the sampling distribution.\n\nC. Under misspecification, $(1-\\alpha)$ credible sets can achieve asymptotically correct frequentist coverage for $\\theta^{\\ast}$ if they are calibrated using a “Godambe information” or sandwich-type adjustment to account for model error; without such adjustment, they may under-cover or over-cover.\n\nD. In inverse problems, the use of informative priors or regularization alone suffices to compensate for likelihood misspecification, restoring nominal frequentist coverage of HPD regions in large samples.\n\nE. If the estimator of $\\theta$ is taken to be the posterior mean, then by the Central Limit Theorem the HPD region automatically has nominal frequentist coverage, because the posterior variability and sampling variability coincide asymptotically under misspecification.\n\nSelect all that apply. Provide your reasoning based on first principles of Bayesian posterior concentration, Kullback–Leibler projection, and the definition of frequentist coverage.", "solution": "**Problem Validation**\n\n**Step 1: Extract Givens**\n-   Parameter vector: $\\theta \\in \\mathbb{R}^{p}$.\n-   Forward model: $G(\\theta)$.\n-   Observation model family: $\\{P_{\\theta} : \\theta \\in \\Theta\\}$ with density $p_{\\theta}(y)$.\n-   Data: $\\{Y_{i}\\}_{i=1}^{n}$ are independent and identically distributed (i.i.d.).\n-   Prior: a proper and continuous density $\\pi(\\theta)$ on a neighborhood of $\\theta^{\\ast}$.\n-   True data-generating distribution: $P_{0}$ with density $p_{0}(y)$.\n-   Model misspecification: $P_{0} \\notin \\{P_{\\theta} : \\theta \\in \\Theta\\}$.\n-   Kullback–Leibler (KL) divergence: $\\mathrm{KL}(P_{0}\\,\\|\\,P_{\\theta}) = \\int \\log\\!\\left(\\frac{p_{0}(y)}{p_{\\theta}(y)}\\right) p_{0}(y)\\,dy$.\n-   Pseudo-true parameter: $\\theta^{\\ast} \\in \\Theta$ is the unique minimizer of $\\mathrm{KL}(P_{0}\\,\\|\\,P_{\\theta})$.\n-   Regularity conditions: Differentiability and positive-definite curvature of the expected log-likelihood at $\\theta^{\\ast}$ are assumed.\n-   Posterior density: $\\pi(\\theta \\mid y_{1:n}) \\propto \\pi(\\theta)\\prod_{i=1}^{n} p_{\\theta}(Y_{i})$.\n-   Credible set: $C_{n,1-\\alpha}$ is a $(1-\\alpha)$ Bayesian credible set.\n-   HPD region: $H_{n,1-\\alpha}$ is a $(1-\\alpha)$ Highest Posterior Density region.\n-   Quantity of interest: The frequentist coverage probability $P_{0}\\!\\left(\\theta^{\\ast} \\in C_{n,1-\\alpha}\\right)$ and $P_{0}\\!\\left(\\theta^{\\ast} \\in H_{n,1-\\alpha}\\right)$ as $n \\to \\infty$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem statement is scientifically grounded, well-posed, and objective. It outlines a canonical problem in statistical theory: the asymptotic frequentist properties of Bayesian procedures under model misspecification. The concepts employed—such as Kullback-Leibler divergence, pseudo-true parameters, posterior concentration, credible sets, and frequentist coverage—are all rigorously defined within the fields of statistics and information theory. The central assumption of misspecification, $P_{0} \\notin \\{P_{\\theta}\\}$, reflects a condition that is ubiquitous in practical applications, making the problem highly relevant. The setup is self-contained and provides the necessary assumptions (e.g., regularity conditions) to permit a standard asymptotic analysis. There are no scientific or logical contradictions, no ambiguities, and the question is a non-trivial inquiry into the relationship between Bayesian and frequentist inference.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A full solution will be derived.\n\n**Derivation of Asymptotic Behavior**\n\nLet $l(\\theta; y) = \\log p_{\\theta}(y)$ be the log-likelihood for a single observation $y$. The problem is situated in the large sample limit, as $n \\to \\infty$. Under the specified regularity conditions, the following standard asymptotic results hold.\n\n1.  **Posterior Concentration and Asymptotic Normality:** As $n \\to \\infty$, the posterior distribution $\\pi(\\theta \\mid y_{1:n})$ concentrates in any neighborhood of the pseudo-true parameter $\\theta^{\\ast}$. The parameter $\\theta^{\\ast}$ is defined as the unique minimizer of the KL-divergence $\\mathrm{KL}(P_{0}\\,\\|\\,P_{\\theta})$, which is equivalent to being the unique maximizer of the expected log-likelihood $E_{P_0}[l(\\theta; Y)]$.\n    Furthermore, a version of the Bernstein-von Mises theorem for misspecified models applies. The posterior distribution, centered at a consistent estimator like the maximum likelihood estimator $\\hat{\\theta}_n$ (which converges in probability to $\\theta^{\\ast}$), is asymptotically normal. The shape of this normal distribution is determined by the Hessian of the expected negative log-likelihood under the true distribution $P_0$. Let us define the matrix $A$:\n    $$ A = -E_{P_0}\\left[\\left.\\nabla^2_{\\theta} l(\\theta; Y)\\right|_{\\theta=\\theta^{\\ast}}\\right] = -\\int \\left[\\nabla^2_{\\theta} \\log p_{\\theta}(y)\\right]_{\\theta=\\theta^{\\ast}} p_0(y) dy $$\n    The posterior distribution for $\\theta$ is then asymptotically given by:\n    $$ \\mathcal{L}(\\theta \\mid y_{1:n}) \\approx N(\\hat{\\theta}_n, (nA)^{-1}) $$\n    where $\\mathcal{L}(\\cdot)$ denotes the law (distribution). This implies that a $(1-\\alpha)$ credible set, such as an HPD region $H_{n,1-\\alpha}$, will be an ellipsoid approximated by:\n    $$ H_{n,1-\\alpha} \\approx \\{ \\theta \\in \\mathbb{R}^p : n(\\theta-\\hat{\\theta}_n)^T A (\\theta-\\hat{\\theta}_n) \\le \\chi^2_{p, 1-\\alpha} \\} $$\n    where $\\chi^2_{p, 1-\\alpha}$ is the $(1-\\alpha)$-quantile of the chi-squared distribution with $p$ degrees of freedom.\n\n2.  **Frequentist Sampling Distribution:** The estimator $\\hat{\\theta}_n$ has a sampling distribution determined by the true data-generating process $P_0$. Standard theory for M-estimators shows that $\\hat{\\theta}_n$ is asymptotically normal around $\\theta^{\\ast}$:\n    $$ \\sqrt{n}(\\hat{\\theta}_n - \\theta^{\\ast}) \\xrightarrow{d} N(0, A^{-1} B A^{-1}) $$\n    where $A$ is defined as above, and $B$ is the covariance of the score function evaluated at $\\theta^{\\ast}$ under the true distribution $P_0$:\n    $$ B = E_{P_0}\\left[\\left(\\left.\\nabla_{\\theta} l(\\theta; Y)\\right|_{\\theta=\\theta^{\\ast}}\\right)\\left(\\left.\\nabla_{\\theta} l(\\theta; Y)\\right|_{\\theta=\\theta^{\\ast}}\\right)^T\\right] $$\n    The matrix $\\Sigma_S = A^{-1} B A^{-1}$ is the well-known \"sandwich\" covariance matrix.\n\n3.  **Frequentist Coverage:** The frequentist coverage of a credible set $C_{n, 1-\\alpha}$ (or HPD region $H_{n,1-\\alpha}$) is the probability $P_0(\\theta^{\\ast} \\in H_{n,1-\\alpha})$ over repeated datasets drawn from $P_0$. Asymptotically, we must evaluate:\n    $$ P_0\\left( n(\\theta^{\\ast}-\\hat{\\theta}_n)^T A (\\theta^{\\ast}-\\hat{\\theta}_n) \\le \\chi^2_{p, 1-\\alpha} \\right) $$\n    Let $Z_n = \\sqrt{n}(\\hat{\\theta}_n - \\theta^{\\ast})$. The quadratic form is $Z_n^T A Z_n$. As $n \\to \\infty$, $Z_n$ converges in distribution to $Z \\sim N(0, A^{-1} B A^{-1})$. Therefore, the asymptotic distribution of the quadratic form is that of $Z^T A Z$. This random variable does *not* follow a $\\chi^2_p$ distribution unless the covariance matrix of $Z$ is $A^{-1}$. This would require $A^{-1} B A^{-1} = A^{-1}$, which implies $B=A$.\n    The equality $A=B$ (the information matrix identity) holds if the model is correctly specified, i.e., if $P_0 = P_{\\theta^{\\ast}}$ (under regularity conditions). However, the problem explicitly states the model is misspecified ($P_0 \\notin \\{P_{\\theta}\\}$), so in general $A \\neq B$.\n    Consequently, the quadratic form $Z^T A Z$ does not follow a $\\chi^2_p$ distribution, and the frequentist coverage probability $P_0(\\theta^{\\ast} \\in H_{n,1-\\alpha})$ will not converge to $(1-\\alpha)$.\n\n**Option-by-Option Analysis**\n\n**A. Even when the true data-generating process $P_{0}$ lies outside the model, $(1-\\alpha)$ posterior credible sets and HPD regions achieve asymptotic frequentist coverage $1-\\alpha$ for the true parameter governing $P_{0}$, provided the prior is continuous and positive near $\\theta^{\\ast}$.**\nThis statement is incorrect. Since $P_0$ is outside the model family, a \"true parameter governing $P_0$\" does not exist in $\\Theta$. The object of inference is the pseudo-true parameter $\\theta^{\\ast}$. As derived above, due to model misspecification, the asymptotic posterior covariance $(nA)^{-1}$ and the frequentist sampling covariance of the estimator $n^{-1}A^{-1}BA^{-1}$ do not coincide. This mismatch ($A \\neq B$) causes the asymptotic frequentist coverage of credible sets for $\\theta^{\\ast}$ to deviate from the nominal level $(1-\\alpha)$.\n**Verdict: Incorrect.**\n\n**B. The posterior concentrates around the pseudo-true parameter $\\theta^{\\ast}$ and is asymptotically normal under regularity; however, because posterior curvature reflects the model’s expected log-likelihood rather than the sampling variability under $P_{0}$, unadjusted HPD regions generically fail to achieve nominal frequentist coverage, which is instead determined by the misspecification “sandwich” variability of the sampling distribution.**\nThis statement is an accurate and complete summary of the asymptotic phenomena.\n- \"The posterior concentrates around the pseudo-true parameter $\\theta^{\\ast}$ and is asymptotically normal under regularity\" is a standard result of Bayesian asymptotics under misspecification.\n- The phrase \"posterior curvature reflects the model’s expected log-likelihood\" correctly identifies that the asymptotic posterior covariance, $(nA)^{-1}$, is determined by the matrix $A$, which is the Hessian of the expected negative log-likelihood.\n- The phrase \"rather than the sampling variability under $P_{0}$\" correctly notes that the sampling variability is given by the sandwich covariance $n^{-1}A^{-1}BA^{-1}$, which is different.\n- The conclusion that \"unadjusted HPD regions generically fail to achieve nominal frequentist coverage\" is the direct consequence of the mismatch between the two covariance structures, as demonstrated in the derivation. The actual coverage is indeed determined by the properties of the sandwich matrix.\n**Verdict: Correct.**\n\n**C. Under misspecification, $(1-\\alpha)$ credible sets can achieve asymptotically correct frequentist coverage for $\\theta^{\\ast}$ if they are calibrated using a “Godambe information” or sandwich-type adjustment to account for model error; without such adjustment, they may under-cover or over-cover.**\nThis statement is correct. The claim that \"without such adjustment, they may under-cover or over-cover\" is a direct consequence of the analysis for option B; because the asymptotic coverage is generally not $(1-\\alpha)$, it can be either higher or lower depending on the structures of matrices $A$ and $B$. The assertion that calibration is possible is also correct. To achieve the nominal coverage $(1-\\alpha)$ for $\\theta^{\\ast}$, one must construct an interval or region using the sandwich covariance estimator. This is precisely the purpose of robust variance estimation in frequentist statistics. A similar adjustment can be made to Bayesian credible sets, for example by scaling the region based on an estimate of the sandwich variance. The \"Godambe information\" matrix, $A B^{-1} A$, is the inverse of the sandwich covariance matrix and is the key quantity needed for this adjustment.\n**Verdict: Correct.**\n\n**D. In inverse problems, the use of informative priors or regularization alone suffices to compensate for likelihood misspecification, restoring nominal frequentist coverage of HPD regions in large samples.**\nThis statement is incorrect. The problem concerns the large-sample limit ($n \\to \\infty$). In this regime, for any proper prior $\\pi(\\theta)$, the likelihood term $\\prod_{i=1}^{n} p_{\\theta}(Y_{i})$ dominates the posterior. The influence of the prior diminishes as $n$ grows, an effect known as the \"swamping\" of the prior by the data. The asymptotic properties of the posterior are determined by the likelihood and the true data-generating process. The mismatch between matrices $A$ and $B$ is an asymptotic feature of the likelihood under misspecification and is not corrected by the prior in the limit. Regularization, which is often mathematically equivalent to a log-prior, also has its effect \"washed out\" by the growing volume of data.\n**Verdict: Incorrect.**\n\n**E. If the estimator of $\\theta$ is taken to be the posterior mean, then by the Central Limit Theorem the HPD region automatically has nominal frequentist coverage, because the posterior variability and sampling variability coincide asymptotically under misspecification.**\nThis statement is incorrect. Its central premise, \"...the posterior variability and sampling variability coincide asymptotically under misspecification,\" is fundamentally false. As shown in the derivation, the asymptotic posterior covariance is $(nA)^{-1}$, while the asymptotic sampling covariance of the estimator (whether posterior mean, mode, or MLE, which are all asymptotically equivalent) is the sandwich matrix $n^{-1}A^{-1}BA^{-1}$. These two matrices are different when the model is misspecified. The Central Limit Theorem establishes the asymptotic normality of the *sampling distribution* of the estimator, while the Bernstein-von Mises theorem establishes the asymptotic normality of the *posterior distribution*. The fact that these two theorems yield different covariance structures is the very source of the coverage failure.\n**Verdict: Incorrect.**", "answer": "$$\\boxed{BC}$$", "id": "3373836"}]}