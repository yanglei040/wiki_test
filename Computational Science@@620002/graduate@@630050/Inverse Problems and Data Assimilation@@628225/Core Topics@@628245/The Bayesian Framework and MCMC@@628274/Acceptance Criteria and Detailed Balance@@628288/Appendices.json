{"hands_on_practices": [{"introduction": "The Independent Metropolis-Hastings (IMH) algorithm provides a powerful framework for Bayesian inference, especially when it is possible to draw samples directly from the prior distribution. This exercise [@problem_id:3362467] explores the application of IMH to a sequential data assimilation problem with a perfect, deterministic model. By deriving the acceptance probability when the prior is used as the proposal, you will gain a fundamental understanding of how the Metropolis-Hastings criterion serves to re-weight prior information with the evidence contained in the likelihood, isolating the precise contribution of the observational data.", "problem": "Consider a Bayesian sequential data assimilation problem with latent state $x_t \\in \\mathbb{R}^d$ for $t=0,1,\\dots,T$ and observations $y_{1:T} = (y_1,\\dots,y_T)$. Assume a perfect-model setting in which the latent dynamics are deterministic: there exists a known mapping $F_t:\\mathbb{R}^d \\to \\mathbb{R}^d$ such that $x_t = F_t(x_{t-1})$ for $t=1,\\dots,T$, and an initial prior density $p_0(x_0)$ on $x_0$. Observations are conditionally independent given the latent states with likelihood factors $\\ell_t(x_t) = p(y_t \\mid x_t)$, $t=1,\\dots,T$. The Bayesian posterior of the initial condition given the observations is thus proportional to $p_0(x_0)\\prod_{t=1}^T \\ell_t(x_t(x_0))$, where $x_t(x_0)$ denotes the deterministic state at time $t$ obtained by propagating $x_0$ through the model.\n\nYou implement an Independent Metropolis–Hastings (IMH) algorithm on the initial condition $x_0$ using the proposal $q(x_0') = p_0(x_0')$, and upon proposing $x_0'$ you deterministically propagate it via the model to obtain the candidate trajectory $x_{1:T}'$. Using only fundamental principles (Bayes’ rule, the Markov property of the model–observation structure, and the detailed balance condition that characterizes Metropolis–Hastings acceptance rules), derive a closed-form analytic expression for the Metropolis–Hastings acceptance probability $\\alpha(x_0 \\to x_0')$ that leaves the posterior on $x_0$ invariant in this perfect-model setting.\n\nThen, still starting from first principles, explain how the Hastings ratio would be modified if the latent dynamics included model error with a transition density $f(x_t \\mid x_{t-1})$ and you used an independent proposal that samples $x_0' \\sim p_0(\\cdot)$ and then propagates via a possibly different transition density $g(x_t \\mid x_{t-1})$ to produce a full proposed path $x_{1:T}'$. You must make clear which factors cancel and which survive in the Hastings ratio. Your final answer must be a single, closed-form analytic expression for the perfect-model acceptance probability $\\alpha(x_0 \\to x_0')$; no numerical values are required, and no rounding is needed.", "solution": "The problem statement has been validated and is deemed sound, well-posed, and scientifically grounded. It presents a standard, albeit non-trivial, application of the Metropolis-Hastings algorithm within the context of Bayesian data assimilation. We will proceed with a full derivation based on first principles.\n\nThe core of any Metropolis-Hastings (MH) algorithm is the detailed balance condition, which ensures that the resulting Markov chain has the desired target posterior distribution, $\\pi(x)$, as its stationary distribution. For a proposal distribution $q(x' \\mid x)$, the detailed balance condition is given by:\n$$ \\pi(x) P(x' \\mid x) = \\pi(x') P(x \\mid x') $$\nwhere $P(x' \\mid x) = q(x' \\mid x) \\alpha(x \\to x')$ is the transition probability of the Markov chain. This leads to the well-known expression for the acceptance probability $\\alpha(x \\to x')$:\n$$ \\alpha(x \\to x') = \\min\\left(1, \\frac{\\pi(x')q(x \\mid x')}{\\pi(x)q(x' \\mid x)}\\right) $$\nThe fraction within the minimum is known as the Hastings ratio.\n\n**Part 1: Perfect-Model Acceptance Probability**\n\nIn the first part of the problem, we operate under a perfect-model assumption. The state of the system is entirely determined by its initial condition, $x_0$. Therefore, the sampling problem is defined on the space of initial conditions, $\\mathbb{R}^d$.\n\nThe target distribution, $\\pi$, is the posterior probability density of the initial condition $x_0$ given the observations $y_{1:T}$. According to the problem statement and Bayes' rule, this is:\n$$ \\pi(x_0) = p(x_0 \\mid y_{1:T}) \\propto p(y_{1:T} \\mid x_0) p_0(x_0) $$\nThe term $p(y_{1:T} \\mid x_0)$ is the total likelihood of the observations given an initial state $x_0$. Since the model is deterministic ($x_t = x_t(x_0)$) and observations are conditionally independent, this likelihood is the product of the individual likelihood factors:\n$$ p(y_{1:T} \\mid x_0) = \\prod_{t=1}^T p(y_t \\mid x_t(x_0)) = \\prod_{t=1}^T \\ell_t(x_t(x_0)) $$\nThus, the unnormalized target posterior is:\n$$ \\tilde{\\pi}(x_0) = p_0(x_0) \\prod_{t=1}^T \\ell_t(x_t(x_0)) $$\nThe MH algorithm will sample from $\\pi(x_0)$ correctly even if we only use its unnormalized form $\\tilde{\\pi}(x_0)$, as the normalization constant cancels in the Hastings ratio.\n\nThe proposal distribution, $q$, is given as an Independent Metropolis-Hastings (IMH) proposal, where the proposed state $x_0'$ is drawn independently of the current state $x_0$. The specific proposal is the prior distribution itself:\n$$ q(x_0' \\mid x_0) = q(x_0') = p_0(x_0') $$\nConsequently, the reverse proposal probability is:\n$$ q(x_0 \\mid x_0') = q(x_0) = p_0(x_0) $$\n\nWe now construct the Hastings ratio for a move from a current state $x_0$ to a proposed state $x_0'$:\n$$ \\frac{\\pi(x_0')q(x_0 \\mid x_0')}{\\pi(x_0)q(x_0' \\mid x_0)} = \\frac{\\tilde{\\pi}(x_0')q(x_0 \\mid x_0')}{\\tilde{\\pi}(x_0)q(x_0' \\mid x_0)} $$\nSubstituting the expressions for the target and proposal densities:\n$$ \\frac{\\left( p_0(x_0') \\prod_{t=1}^T \\ell_t(x_t(x_0')) \\right) \\left( p_0(x_0) \\right)}{\\left( p_0(x_0) \\prod_{t=1}^T \\ell_t(x_t(x_0)) \\right) \\left( p_0(x_0') \\right)} $$\nWe can see that the prior density terms $p_0(x_0)$ and $p_0(x_0')$ cancel out. The term $p_0(x_0)$ from the reverse proposal cancels with the prior term in the target density at $x_0$, and similarly for $p_0(x_0')$. The simplified ratio becomes:\n$$ \\frac{\\prod_{t=1}^T \\ell_t(x_t(x_0'))}{\\prod_{t=1}^T \\ell_t(x_t(x_0))} $$\nThis is the ratio of the total likelihoods of the proposed trajectory versus the current trajectory.\n\nTherefore, the final acceptance probability $\\alpha(x_0 \\to x_0')$ is:\n$$ \\alpha(x_0 \\to x_0') = \\min\\left(1, \\frac{\\prod_{t=1}^T \\ell_t(x_t(x_0'))}{\\prod_{t=1}^T \\ell_t(x_t(x_0))}\\right) $$\nThis result is intuitively satisfying: since the proposal distribution is the prior, the MH acceptance step serves purely to re-weight the samples according to the information provided by the data, which is encapsulated entirely in the likelihood function.\n\n**Part 2: Modification for a Stochastic Model**\n\nIn the second scenario, the model dynamics are stochastic, meaning the entire path $X = x_{0:T} = (x_0, x_1, \\dots, x_T)$ is the latent variable. The sampling problem is now on the much higher-dimensional space of trajectories.\n\nThe target distribution $\\pi(X)$ is the posterior of the entire path given the observations, $p(x_{0:T} \\mid y_{1:T})$. From first principles (Bayes' rule and the Markov structure):\n$$ \\pi(X) = p(x_{0:T} \\mid y_{1:T}) \\propto p(y_{1:T} \\mid x_{0:T}) p(x_{0:T}) $$\nThe likelihood term, $p(y_{1:T} \\mid x_{0:T})$, simplifies to a product of likelihood factors as before:\n$$ p(y_{1:T} \\mid x_{0:T}) = \\prod_{t=1}^T p(y_t \\mid x_t) = \\prod_{t=1}^T \\ell_t(x_t) $$\nThe prior on the path, $p(x_{0:T})$, is given by the chain rule of probability, incorporating the initial prior and the true model transition density $f(x_t \\mid x_{t-1})$:\n$$ p(x_{0:T}) = p_0(x_0) \\prod_{t=1}^T f(x_t \\mid x_{t-1}) $$\nCombining these, the unnormalized target posterior for the path $X$ is:\n$$ \\tilde{\\pi}(X) = p_0(x_0) \\left( \\prod_{t=1}^T f(x_t \\mid x_{t-1}) \\right) \\left( \\prod_{t=1}^T \\ell_t(x_t) \\right) $$\n\nThe proposal mechanism generates a full path $X' = x_{0:T}'$ independently of the current path $X$. The proposal density $q(X')$ is constructed by first sampling $x_0' \\sim p_0(\\cdot)$ and then propagating forward with a possibly different transition density $g(x_t \\mid x_{t-1})$:\n$$ q(X') = p_0(x_0') \\prod_{t=1}^T g(x_t' \\mid x_{t-1}') $$\nSince this is an independent sampler, $q(X' \\mid X) = q(X')$ and the reverse proposal is $q(X \\mid X') = q(X)$.\n\nThe Hastings ratio is $\\frac{\\pi(X')q(X)}{\\pi(X)q(X')}$. Substituting the unnormalized expressions:\n$$ \\frac{\\tilde{\\pi}(X') q(X)}{\\tilde{\\pi}(X) q(X')} = \\frac{\\left[ p_0(x_0') \\prod_{t=1}^T f(x_t' \\mid x_{t-1}') \\prod_{t=1}^T \\ell_t(x_t') \\right] \\left[ p_0(x_0) \\prod_{t=1}^T g(x_t \\mid x_{t-1}) \\right]}{\\left[ p_0(x_0) \\prod_{t=1}^T f(x_t \\mid x_{t-1}) \\prod_{t=1}^T \\ell_t(x_t) \\right] \\left[ p_0(x_0') \\prod_{t=1}^T g(x_t' \\mid x_{t-1}') \\right]} $$\nThe factors of the initial prior, $p_0(x_0)$ and $p_0(x_0')$, cancel because it is used as the proposal for the initial state. The surviving terms constitute the modified Hastings ratio:\n$$ \\frac{\\left(\\prod_{t=1}^T \\ell_t(x_t')\\right) \\left(\\prod_{t=1}^T f(x_t' \\mid x_{t-1}')\\right) \\left(\\prod_{t=1}^T g(x_t \\mid x_{t-1})\\right)}{\\left(\\prod_{t=1}^T \\ell_t(x_t)\\right) \\left(\\prod_{t=1}^T f(x_t \\mid x_{t-1})\\right) \\left(\\prod_{t=1}^T g(x_t' \\mid x_{t-1}')\\right)} $$\nThis can be expressed illuminatingly as the ratio of importance weights, $\\frac{w(X')}{w(X)}$, where the weight for a path $X$ is $w(X) = \\frac{\\tilde{\\pi}(X)}{q(X)}$:\n$$ w(X) = \\frac{p_0(x_0) \\left(\\prod_{t=1}^T f(x_t \\mid x_{t-1})\\right) \\left(\\prod_{t=1}^T \\ell_t(x_t)\\right)}{p_0(x_0) \\prod_{t=1}^T g(x_t \\mid x_{t-1})} = \\left(\\prod_{t=1}^T \\ell_t(x_t)\\right) \\left(\\prod_{t=1}^T \\frac{f(x_t \\mid x_{t-1})}{g(x_t \\mid x_{t-1})}\\right) $$\nThe Hastings ratio is therefore:\n$$ \\frac{w(X')}{w(X)} = \\frac{\\left(\\prod_{t=1}^T \\ell_t(x_t')\\right) \\left(\\prod_{t=1}^T \\frac{f(x_t' \\mid x_{t-1}')}{g(x_t' \\mid x_{t-1}')}\\right)}{\\left(\\prod_{t=1}^T \\ell_t(x_t)\\right) \\left(\\prod_{t=1}^T \\frac{f(x_t \\mid x_{t-1})}{g(x_t \\mid x_{t-1})}\\right)}$$\nCompared to the perfect-model case, the ratio of total likelihoods, $\\frac{\\prod \\ell_t'}{\\prod \\ell_t}$, survives. However, it is now multiplied by a new factor. This new factor is the ratio of path dynamics weights, $\\frac{\\prod_t (f'/g')}{\\prod_t (f/g)}$, which corrects for the discrepancy between the proposal dynamics $g$ and the true model dynamics $f$. This correction term is necessary because the proposal for the path geometry, $\\prod_t g(\\cdot \\mid \\cdot)$, is not the same as the prior for the path geometry, $\\prod_t f(\\cdot \\mid \\cdot)$. In the perfect-model case, this correction was unnecessary as the proposal for the only stochastic component, $x_0$, was exactly its prior, $p_0$.", "answer": "$$\\boxed{\\min\\left(1, \\frac{\\prod_{t=1}^T \\ell_t(x_t(x_0'))}{\\prod_{t=1}^T \\ell_t(x_t(x_0))}\\right)}$$", "id": "3362467"}, {"introduction": "A frequent challenge in data assimilation and inverse problems is the presence of multimodal posterior distributions, which can cause standard MCMC samplers to become trapped in a single mode, a phenomenon known as metastability. This practice [@problem_id:3362440] investigates this issue by analyzing a simple one-dimensional bimodal posterior. You will compute the acceptance probability for a proposed jump between the modes, revealing how the low probability of proposing such a jump, rather than the acceptance rule itself, is often the primary cause of poor mixing and metastability.", "problem": "Consider a one-dimensional inverse problem arising in data assimilation, where a scalar parameter $x$ is to be inferred from observations under two competing physical regimes. The posterior density for $x$ is modeled as a Gaussian mixture,\n$$\n\\pi(x) \\propto w_{1}\\,\\frac{1}{\\sqrt{2\\pi}\\,\\sigma_{1}}\\exp\\!\\left(-\\frac{(x-\\mu_{1})^{2}}{2\\sigma_{1}^{2}}\\right) + w_{2}\\,\\frac{1}{\\sqrt{2\\pi}\\,\\sigma_{2}}\\exp\\!\\left(-\\frac{(x-\\mu_{2})^{2}}{2\\sigma_{2}^{2}}\\right),\n$$\nwhere $w_{1},w_{2}\\in(0,1)$ with $w_{1}+w_{2}=1$, the means $\\mu_{1},\\mu_{2}\\in\\mathbb{R}$ are well separated in the sense that $\\Delta\\equiv|\\mu_{2}-\\mu_{1}|\\gg \\max\\{\\sigma_{1},\\sigma_{2}\\}$, and $\\sigma_{1},\\sigma_{2}>0$ are the component standard deviations. Assume a Metropolis–Hastings (MH) Markov Chain Monte Carlo (MCMC) algorithm with a symmetric Gaussian random-walk proposal density $q(y\\,|\\,x)=\\frac{1}{\\sqrt{2\\pi}\\,s}\\exp\\!\\big(-\\frac{(y-x)^{2}}{2s^{2}}\\big)$, where the step size $s>0$ is fixed and satisfies $s\\ll \\Delta$. The proposal is therefore symmetric in the sense that $q(y\\,|\\,x)=q(x\\,|\\,y)$.\n\nStarting from the detailed balance condition for Markov chains, and using only foundational principles such as Bayes’s rule for the posterior and the definition of the Metropolis–Hastings transition kernel, derive the acceptance criterion that enforces detailed balance for this symmetric proposal. Then, characterize the metastability caused by multimodality and a fixed step size by formally conditioning on proposals that attempt to jump between the modes: define the event $A_{\\varepsilon}$ to mean a proposal from a current state $x$ with $|x-\\mu_{1}|\\le \\varepsilon$ to a proposed state $y$ with $|y-\\mu_{2}|\\le \\varepsilon$, where $\\varepsilon>0$ is a small neighborhood radius such that $\\varepsilon\\ll \\min\\{\\sigma_{1},\\sigma_{2}\\}$ and $\\varepsilon\\ll \\Delta$. In the limit $\\varepsilon\\to 0$, compute the expected acceptance probability conditioned on $A_{\\varepsilon}$ for the given MH algorithm with fixed step size $s$.\n\nYour final answer must be a single closed-form analytic expression in terms of $w_{1}$, $w_{2}$, $\\sigma_{1}$, and $\\sigma_{2}$. No numerical approximation is required. Also, briefly explain, from first principles, how multimodality and metastability enter through the acceptance rate and the proposal mechanism in this setting. The final answer must be provided exactly as an analytic expression; do not include any units.", "solution": "The posterior density is specified as a Gaussian mixture. From Bayes’s rule, for data assimilation, one has $\\pi(x)\\propto p(y_{\\text{obs}}\\,|\\,x)\\,p(x)$, and here that posterior is represented explicitly by the mixture. The Metropolis–Hastings (MH) algorithm defines a Markov chain with proposal density $q(y\\,|\\,x)$ and acceptance probability $\\alpha(x,y)$, yielding a transition probability from $x$ to $y$ given by $P(x\\to y)=q(y\\,|\\,x)\\,\\alpha(x,y)$ for $y\\neq x$ and $P(x\\to x)=1-\\int q(z\\,|\\,x)\\,\\alpha(x,z)\\,\\mathrm{d}z$. The requirement that the chain have $\\pi$ as its invariant distribution and be reversible with respect to $\\pi$ is the detailed balance condition:\n$$\n\\pi(x)\\,P(x\\to y)=\\pi(y)\\,P(y\\to x),\\quad \\text{for all }x,y.\n$$\nFor $x\\neq y$, substituting $P$ gives\n$$\n\\pi(x)\\,q(y\\,|\\,x)\\,\\alpha(x,y) = \\pi(y)\\,q(x\\,|\\,y)\\,\\alpha(y,x).\n$$\nThe standard MH construction chooses\n$$\n\\alpha(x,y)=\\min\\!\\left(1,\\frac{\\pi(y)\\,q(x\\,|\\,y)}{\\pi(x)\\,q(y\\,|\\,x)}\\right),\n$$\nwhich satisfies detailed balance and maximizes acceptance subject to that constraint. Because the proposal is symmetric, namely $q(y\\,|\\,x)=q(x\\,|\\,y)$, this simplifies to\n$$\n\\alpha(x,y)=\\min\\!\\left(1,\\frac{\\pi(y)}{\\pi(x)}\\right).\n$$\n\nWe are asked to compute the expected acceptance probability conditioned on proposals that attempt to jump between the two modes. Formally, define $A_{\\varepsilon}$ as\n$$\nA_{\\varepsilon}=\\left\\{|x-\\mu_{1}|\\le \\varepsilon,\\;|y-\\mu_{2}|\\le \\varepsilon\\right\\},\\quad \\varepsilon\\ll \\min\\{\\sigma_{1},\\sigma_{2}\\},\\;\\varepsilon\\ll \\Delta.\n$$\nUnder the well-separated condition $\\Delta\\gg \\max\\{\\sigma_{1},\\sigma_{2}\\}$ and the small neighborhoods of radius $\\varepsilon$, the contribution of the opposite Gaussian component to the posterior near each mode is negligible. Thus, for $x$ with $|x-\\mu_{1}|\\le \\varepsilon$,\n$$\n\\pi(x)\\approx C\\, w_{1}\\,\\frac{1}{\\sqrt{2\\pi}\\,\\sigma_{1}}\\exp\\!\\left(-\\frac{(x-\\mu_{1})^{2}}{2\\sigma_{1}^{2}}\\right),\n$$\nand for $y$ with $|y-\\mu_{2}|\\le \\varepsilon$,\n$$\n\\pi(y)\\approx C\\, w_{2}\\,\\frac{1}{\\sqrt{2\\pi}\\,\\sigma_{2}}\\exp\\!\\left(-\\frac{(y-\\mu_{2})^{2}}{2\\sigma_{2}^{2}}\\right),\n$$\nwhere $C>0$ is the (unknown) normalization constant of the posterior that cancels in ratios. In the limit $\\varepsilon\\to 0$, we have $x\\to \\mu_{1}$ and $y\\to \\mu_{2}$, so the exponential factors tend to $\\exp(0)=1$, and we obtain\n$$\n\\frac{\\pi(y)}{\\pi(x)} \\xrightarrow[\\varepsilon\\to 0]{} \\frac{w_{2}\\,\\frac{1}{\\sqrt{2\\pi}\\,\\sigma_{2}}}{w_{1}\\,\\frac{1}{\\sqrt{2\\pi}\\,\\sigma_{1}}}=\\frac{w_{2}\\,\\sigma_{1}}{w_{1}\\,\\sigma_{2}}.\n$$\nTherefore, for symmetric proposals,\n$$\n\\alpha(x,y)\\xrightarrow[\\varepsilon\\to 0]{}\\min\\!\\left(1,\\frac{w_{2}\\,\\sigma_{1}}{w_{1}\\,\\sigma_{2}}\\right).\n$$\nBecause this limiting value is constant on $A_{\\varepsilon}$ as $\\varepsilon\\to 0$, the conditional expectation of the acceptance probability given $A_{\\varepsilon}$ equals this constant:\n$$\n\\lim_{\\varepsilon\\to 0}\\,\\mathbb{E}\\!\\left[\\alpha(X,Y)\\,\\big|\\,A_{\\varepsilon}\\right] = \\min\\!\\left(1,\\frac{w_{2}\\,\\sigma_{1}}{w_{1}\\,\\sigma_{2}}\\right).\n$$\n\nFinally, we explain the interaction of multimodality and metastability with acceptance rates and fixed step sizes from first principles. Multimodality arises because $\\pi$ has two well-separated peaks at $\\mu_{1}$ and $\\mu_{2}$. Metastability in the Markov chain is a consequence of the proposal mechanism having a fixed step size $s$ that is small relative to the separation $\\Delta$. The probability of proposing a jump from a neighborhood of $\\mu_{1}$ into a neighborhood of $\\mu_{2}$ under a Gaussian random walk is exponentially small in $\\Delta^{2}/s^{2}$; more precisely, if $X\\approx \\mu_{1}$, then $Y\\sim \\mathcal{N}(X,s^{2})$ must realize a rare tail event $|Y-\\mu_{2}|\\le \\varepsilon$, whose probability scales like $\\exp\\!\\big(-\\Delta^{2}/(2s^{2})\\big)$ up to polynomial factors. Thus, even if the acceptance of such cross-mode proposals (when they occur) is high, as given by the expression above, the overall rate of successful inter-mode transitions is dominated by the rarity of proposing cross-mode moves. This leads to long residence times within a single mode (metastability), slow mixing, and strong dependence of inter-mode transition times on the ratio $\\Delta/s$. The acceptance criterion itself is determined by detailed balance and depends on the local ratio of posterior mass near the two modes, captured here by the weights and scales of the Gaussian components; the proposal mechanism determines how often such favorable proposals are actually attempted.", "answer": "$$\\boxed{\\min\\!\\left(1,\\frac{w_{2}\\,\\sigma_{1}}{w_{1}\\,\\sigma_{2}}\\right)}$$", "id": "3362440"}, {"introduction": "Many scientific models involve parameters that are subject to physical constraints, such as non-negativity, requiring MCMC samplers to respect these boundaries. This advanced practice [@problem_id:3362451] guides you through the process of designing valid Metropolis-Hastings proposals for a constrained parameter space using reflection and refraction techniques. The core of the exercise is to derive the proposal densities that arise from these boundary-handling mechanisms and then use them to construct the correct acceptance probability that rigorously preserves detailed balance, solidifying your understanding through direct implementation and numerical verification.", "problem": "Consider the following constrained Bayesian inverse problem with a hard feasibility constraint. A scalar parameter $\\theta \\in \\mathbb{R}$ has a Gaussian likelihood and a Gaussian prior, but the posterior support is truncated by a convex inequality constraint. The data model is $d \\mid \\theta \\sim \\mathcal{N}(\\theta,\\sigma_y^2)$ and the prior is $\\theta \\sim \\mathcal{N}(0,\\sigma_0^2)$, subject to the constraint $C(\\theta) \\le 0$ with $C(\\theta) = -\\theta$, so that the feasible set is $\\{\\theta \\in \\mathbb{R} : \\theta \\ge 0\\}$. The unnormalized truncated posterior is therefore\n$$\n\\pi_T(\\theta) \\propto \\exp\\left(-\\tfrac{1}{2}\\bigg(\\frac{(d-\\theta)^2}{\\sigma_y^2} + \\frac{\\theta^2}{\\sigma_0^2}\\bigg)\\right)\\,\\mathbf{1}_{\\{\\theta \\ge 0\\}},\n$$\nwhere $\\mathbf{1}_{\\{\\cdot\\}}$ is the indicator function.\n\nWe seek a Metropolis-Hastings (MH) transition kernel that preserves detailed balance with respect to $\\pi_T(\\theta)$ while proposing moves using a Gaussian random walk that is modified near the boundary to enforce the constraint.\n\nStarting from a current feasible state $x \\ge 0$, draw an unconstrained Gaussian increment $\\varepsilon \\sim \\mathcal{N}(0,\\tau^2)$ and set the unconstrained proposal $u = x + \\varepsilon$. Define two distinct boundary-handling mappings to produce a feasible proposed state $y \\ge 0$ from $u$:\n\n1. Reflection: $y = |u|$.\n\n2. Refraction with compression factor $\\rho \\in (0,1]$: \n   $$\n   y = \n   \\begin{cases}\n   u, & \\text{if } u \\ge 0,\\\\\n   -\\rho\\,u, & \\text{if } u < 0.\n   \\end{cases}\n   $$\n\nLet $\\varphi_\\tau(z) = \\dfrac{1}{\\sqrt{2\\pi}\\tau}\\exp\\!\\left(-\\dfrac{z^2}{2\\tau^2}\\right)$ denote the Gaussian density with standard deviation $\\tau$. The resulting proposal density $q(x \\to y)$ is the pushforward of $\\varepsilon$ through the mapping from $u$ to $y$ and may have multiple preimages due to folding at the boundary. Derive, from first principles and without appealing to any pre-derived acceptance rules, the correct Metropolis-Hastings acceptance probability\n$$\n\\alpha(x \\to y) = \\min\\!\\left(1,\\; \\frac{\\pi_T(y)\\,q(y \\to x)}{\\pi_T(x)\\,q(x \\to y)}\\right),\n$$\nby explicitly computing $q(x \\to y)$ for each boundary-handling mapping. Your derivation must start from the fundamental detailed balance condition, which for a target density $\\pi_T$ and a Markov transition kernel $K$ requires\n$$\n\\pi_T(x)\\,K(x,dy) = \\pi_T(y)\\,K(y,dx),\n$$\nand for Metropolis-Hastings with proposal kernel $q$ and acceptance $\\alpha$ reduces to\n$$\n\\pi_T(x)\\,q(x \\to y)\\,\\alpha(x \\to y) = \\pi_T(y)\\,q(y \\to x)\\,\\alpha(y \\to x).\n$$\nFrom this, obtain the expression for $\\alpha(x \\to y)$.\n\nSpecialize your expressions for the proposal density $q(x \\to y)$ in the scalar half-line case with boundary at $0$ as follows:\n\n- For reflection, show that\n$$\nq_{\\mathrm{refl}}(x \\to y) = \\varphi_\\tau(y-x) + \\varphi_\\tau(y+x), \\quad \\text{for } x \\ge 0,\\, y \\ge 0.\n$$\n\n- For refraction with factor $\\rho \\in (0,1]$, show that\n$$\nq_{\\mathrm{refr},\\rho}(x \\to y) = \\varphi_\\tau(y-x) + \\frac{1}{\\rho}\\,\\varphi_\\tau\\!\\Big(-\\frac{y}{\\rho} - x\\Big), \\quad \\text{for } x \\ge 0,\\, y \\ge 0.\n$$\n\nThen, implement a program that, for given parameters, computes the acceptance probability $\\alpha(x \\to y)$ using the derived expressions and also numerically verifies detailed balance for the pair $(x,y)$ via the scalar quantity\n$$\n\\Delta(x,y) = \\frac{\\pi_T(x)\\,q(x \\to y)\\,\\alpha(x \\to y) - \\pi_T(y)\\,q(y \\to x)\\,\\alpha(y \\to x)}{\\max\\big(\\pi_T(x)\\,q(x \\to y)\\,\\alpha(x \\to y),\\; \\pi_T(y)\\,q(y \\to x)\\,\\alpha(y \\to x)\\big)},\n$$\nwhich should be close to $0$ in magnitude when detailed balance holds.\n\nUse the following fixed data and hyperparameters: $d = 1$, $\\sigma_y = 0.5$ (so $\\sigma_y^2 = 0.25$), and $\\sigma_0 = 1$ (so $\\sigma_0^2 = 1$). For numerical stability, you may implement all quantities using logarithms and stable summation.\n\nYour program must evaluate the following test suite of five cases, each specified as a tuple $(\\text{method}, x, y, \\tau, \\rho)$, where the refraction factor $\\rho$ is ignored for the reflection method:\n\n1. Reflection interior move: $(\\text{reflect},\\, 0.8,\\, 0.9,\\, 0.3,\\, 1.0)$.\n\n2. Reflection crossing-dominant move: $(\\text{reflect},\\, 0.2,\\, 0.7,\\, 0.5,\\, 1.0)$.\n\n3. Reflection at the boundary: $(\\text{reflect},\\, 0.0,\\, 0.1,\\, 0.4,\\, 1.0)$.\n\n4. Refraction with compression $\\rho = 0.5$ crossing: $(\\text{refract},\\, 0.2,\\, 0.9,\\, 0.6,\\, 0.5)$.\n\n5. Refraction with compression $\\rho = 0.5$ interior: $(\\text{refract},\\, 0.9,\\, 1.1,\\, 0.4,\\, 0.5)$.\n\nFor each test case, compute and report the pair of floats $[\\alpha(x \\to y), \\Delta(x,y)]$ as defined above. Your program should produce a single line of output containing all results flattened into a single comma-separated list enclosed in square brackets, in the order of the test suite and with each test case contributing two consecutive entries. That is, the required output format is\n\"[a1,d1,a2,d2,a3,d3,a4,d4,a5,d5]\"\nwith no additional text. All angles, if any were present, would be measured in radians, but this problem contains no angles. All quantities are dimensionless. The output values must be real numbers expressed as standard decimal floats.", "solution": "The problem is assessed to be valid as it is scientifically grounded in the principles of Bayesian statistics and Markov chain Monte Carlo (MCMC) methods, is well-posed with a clear objective, and provides a complete and consistent set of information for its resolution.\n\nThe core of the problem is to derive the Metropolis-Hastings (MH) acceptance probability, $\\alpha(x \\to y)$, for two different proposal mechanisms designed to handle a boundary constraint. This requires deriving the proposal transition density, $q(x \\to y)$, for each mechanism.\n\nThe target distribution is a truncated posterior on $\\theta \\ge 0$:\n$$\n\\pi_T(\\theta) \\propto \\exp\\left(-\\tfrac{1}{2}\\bigg(\\frac{(d-\\theta)^2}{\\sigma_y^2} + \\frac{\\theta^2}{\\sigma_0^2}\\bigg)\\right)\\,\\mathbf{1}_{\\{\\theta \\ge 0\\}}\n$$\nFor convenience, we define the potential energy function $U(\\theta)$ such that $\\pi_T(\\theta) \\propto \\exp(-U(\\theta))$ for $\\theta \\ge 0$.\n$$\nU(\\theta) = \\frac{1}{2}\\left(\\frac{(d-\\theta)^2}{\\sigma_y^2} + \\frac{\\theta^2}{\\sigma_0^2}\\right)\n$$\nThe ratio of target densities is then $\\frac{\\pi_T(y)}{\\pi_T(x)} = \\exp(U(x) - U(y))$ for $x, y \\ge 0$.\n\n### Derivation of the Acceptance Probability Form\nThe detailed balance condition for a Metropolis-Hastings update is given as:\n$$\n\\pi_T(x)\\,q(x \\to y)\\,\\alpha(x \\to y) = \\pi_T(y)\\,q(y \\to x)\\,\\alpha(y \\to x)\n$$\nwhere $x$ and $y$ are states in the feasible domain, i.e., $x \\ge 0$ and $y \\ge 0$. We seek an acceptance function $\\alpha$ in the range $[0, 1]$ that satisfies this equation. Let us define the Metropolis ratio $R$:\n$$\nR(x, y) = \\frac{\\pi_T(y)\\,q(y \\to x)}{\\pi_T(x)\\,q(x \\to y)}\n$$\nThe detailed balance equation can be rewritten as $\\alpha(x \\to y) = R(x, y) \\cdot \\alpha(y \\to x)$. To maximize the acceptance rate, and thus the efficiency of the sampler, we should choose the largest possible values for $\\alpha(x \\to y)$ and $\\alpha(y \\to x)$ that satisfy this relation, subject to the constraint that they are probabilities (i.e., less than or equal to $1$).\n\nIf $R(x, y) < 1$, we can set $\\alpha(y \\to x) = 1$ (its maximum possible value). This choice then determines $\\alpha(x \\to y) = R(x, y)$.\nIf $R(x, y) \\ge 1$, we can set $\\alpha(x \\to y) = 1$. This implies $\\alpha(y \\to x) = 1 / R(x, y)$. Since $R(x, y) \\ge 1$, this value for $\\alpha(y \\to x)$ is also less than or equal to $1$.\n\nCombining these two cases, we obtain the standard Metropolis-Hastings acceptance probability:\n$$\n\\alpha(x \\to y) = \\min\\left(1, R(x,y)\\right) = \\min\\left(1, \\frac{\\pi_T(y)\\,q(y \\to x)}{\\pi_T(x)\\,q(x \\to y)}\\right)\n$$\nThis derivation fulfills the requirement of obtaining the expression for $\\alpha(x \\to y)$ from the provided detailed balance equation.\n\n### Derivation of the Proposal Density $q(x \\to y)$\nThe next step is to find the expression for the proposal density $q(x \\to y)$. The proposal is generated by first drawing an increment $\\varepsilon \\sim \\mathcal{N}(0, \\tau^2)$ with probability density function (PDF) $\\varphi_\\tau(\\varepsilon)$, forming an unconstrained proposal $u = x + \\varepsilon$, and then mapping $u$ to a feasible state $y = M(u) \\ge 0$. The density of $u$ given $x$ is $p(u|x) = \\varphi_\\tau(u-x)$.\n\nThe proposal density $q(x \\to y)$ is the pushforward of the density of $u$ under the mapping $M$. For a given proposed state $y > 0$, there may be multiple unconstrained values $u_i$ such that $M(u_i) = y$. The density at $y$ is the sum of contributions from all such preimages. By the change of variables formula for probability densities, the contribution from each preimage $u_i$ is $p(u_i|x) \\left| \\frac{du_i}{dy} \\right|$. Thus,\n$$\nq(x \\to y) = \\sum_{i \\text{ s.t. } M(u_i)=y} p(u_i|x) \\left| \\frac{du_i}{dy} \\right| = \\sum_{i \\text{ s.t. } M(u_i)=y} \\varphi_\\tau(u_i - x) \\left| \\frac{du_i}{dy} \\right|\n$$\nThis density is with respect to the Lebesgue measure on the feasible domain $[0, \\infty)$.\n\n#### 1. Reflection Mapping: $y = |u|$\nFor a given $y>0$, the preimages under the absolute value mapping $M(u)=|u|$ are $u_1 = y$ and $u_2 = -y$.\n\n- For $u_1 = y$, the required increment is $\\varepsilon_1 = y-x$. The Jacobian term is $|\\frac{du_1}{dy}| = |1| = 1$. The contribution to the density is $\\varphi_\\tau(y-x)$.\n- For $u_2 = -y$, the required increment is $\\varepsilon_2 = -y-x$. The Jacobian term is $|\\frac{du_2}{dy}| = |-1| = 1$. The contribution to the density is $\\varphi_\\tau(-y-x)$. Since the Gaussian PDF is an even function, $\\varphi_\\tau(-z) = \\varphi_\\tau(z)$, this is equal to $\\varphi_\\tau(y+x)$.\n\nSumming these two contributions gives the proposal density for reflection, valid for $x \\ge 0, y \\ge 0$:\n$$\nq_{\\mathrm{refl}}(x \\to y) = \\varphi_\\tau(y-x) + \\varphi_\\tau(y+x)\n$$\nThis confirms the expression provided in the problem statement.\n\n#### 2. Refraction Mapping: $y = u$ for $u \\ge 0$, $y = -\\rho u$ for $u < 0$\nFor a given $y>0$ and a compression factor $\\rho \\in (0,1]$, we find the preimages under the refraction mapping.\n\n- If the unconstrained proposal $u$ was non-negative ($u \\ge 0$), then $y=u$. The preimage is $u_1 = y$. Since we are considering $y>0$, $u_1>0$, which is consistent with the condition $u \\ge 0$. The Jacobian is $|\\frac{du_1}{dy}| = 1$. The density contribution is $\\varphi_\\tau(y-x)$.\n- If the unconstrained proposal $u$ was negative ($u < 0$), then $y = -\\rho u$. The preimage is $u_2 = -y/\\rho$. Since $y>0$ and $\\rho>0$, $u_2<0$, which is consistent with the condition $u<0$. The Jacobian is $|\\frac{du_2}{dy}| = |-1/\\rho| = 1/\\rho$. The density contribution is $\\varphi_\\tau(-y/\\rho - x) \\cdot \\frac{1}{\\rho}$.\n\nSumming these two contributions gives the proposal density for refraction, valid for $x \\ge 0, y \\ge 0$:\n$$\nq_{\\mathrm{refr},\\rho}(x \\to y) = \\varphi_\\tau(y-x) + \\frac{1}{\\rho}\\,\\varphi_\\tau\\!\\Big(-\\frac{y}{\\rho} - x\\Big)\n$$\nThis also confirms the expression provided in the problem statement.\n\n### Numerical Implementation\nThe calculations are performed in the logarithmic domain to enhance numerical stability. The acceptance probability $\\alpha(x \\to y)$ is computed via:\n$$\n\\log R(x,y) = (U(x) - U(y)) + (\\log q(y \\to x) - \\log q(x \\to y))\n$$\n$$\n\\alpha(x \\to y) = \\min(1, \\exp(\\log R(x,y)))\n$$\nThe terms $\\log q(x \\to y)$ are computed using a stable `log-sum-exp` operation, i.e., $\\log(A+B) = \\log(\\exp(\\log A) + \\exp(\\log B))$.\n\nThe detailed balance check quantity $\\Delta(x,y)$ is defined as:\n$$\n\\Delta(x,y) = \\frac{T_1 - T_2}{\\max(T_1, T_2)}, \\quad \\text{where } T_1 = \\pi_T(x)\\,q(x \\to y)\\,\\alpha(x \\to y) \\text{ and } T_2 = \\pi_T(y)\\,q(y \\to x)\\,\\alpha(y \\to x)\n$$\nBy construction, $T_1$ and $T_2$ should be identical. Any deviation arises from floating-point arithmetic errors. To compute $\\Delta(x,y)$ stably, we compute their logarithms, $\\log T_1$ and $\\log T_2$. Let $\\text{diff} = \\log T_1 - \\log T_2$. Then $\\Delta(x,y)$ can be computed as $\\exp(\\text{diff})-1$ if $T_2 > T_1$ and $1-\\exp(-\\text{diff})$ if $T_1 > T_2$. Using the `numpy.expm1` function, which computes $\\exp(z)-1$ accurately for small $z$, provides a robust implementation.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes MH acceptance probabilities and detailed balance checks for a\n    constrained Bayesian inverse problem.\n    \"\"\"\n    # Fixed parameters\n    d = 1.0\n    sigma_y = 0.5\n    sigma_0 = 1.0\n    sigma_y2 = sigma_y**2\n    sigma_02 = sigma_0**2\n\n    # Test cases from the problem statement\n    test_cases = [\n        ('reflect', 0.8, 0.9, 0.3, 1.0),\n        ('reflect', 0.2, 0.7, 0.5, 1.0),\n        ('reflect', 0.0, 0.1, 0.4, 1.0),\n        ('refract', 0.2, 0.9, 0.6, 0.5),\n        ('refract', 0.9, 1.1, 0.4, 0.5),\n    ]\n\n    def log_potential(theta):\n        \"\"\"Computes the log-potential U(theta) for theta >= 0.\"\"\"\n        if theta < 0:\n            return np.inf\n        return 0.5 * (((d - theta)**2 / sigma_y2) + (theta**2 / sigma_02))\n\n    def log_phi_tau(z, tau):\n        \"\"\"Computes the log of the Gaussian PDF with std dev tau.\"\"\"\n        return -np.log(tau * np.sqrt(2 * np.pi)) - (z**2) / (2 * tau**2)\n\n    def log_q(method, x, y, tau, rho):\n        \"\"\"Computes the log of the proposal density q(x -> y).\"\"\"\n        if method == 'reflect':\n            log_term1 = log_phi_tau(y - x, tau)\n            log_term2 = log_phi_tau(y + x, tau)\n            return np.logaddexp(log_term1, log_term2)\n        elif method == 'refract':\n            log_term1 = log_phi_tau(y - x, tau)\n            log_term2 = -np.log(rho) + log_phi_tau(-y / rho - x, tau)\n            return np.logaddexp(log_term1, log_term2)\n        else:\n            raise ValueError(\"Unknown method\")\n\n    results = []\n    for case in test_cases:\n        method, x, y, tau, rho = case\n\n        # Calculate acceptance probability alpha(x -> y)\n        U_x = log_potential(x)\n        U_y = log_potential(y)\n        log_pi_ratio = U_x - U_y\n\n        log_q_forward = log_q(method, x, y, tau, rho)\n        log_q_reverse = log_q(method, y, x, tau, rho)\n        \n        log_R_forward = log_pi_ratio + log_q_reverse - log_q_forward\n        alpha_forward = min(1.0, np.exp(log_R_forward))\n        \n        results.append(alpha_forward)\n\n        # Calculate detailed balance check Delta(x, y)\n        log_alpha_forward = np.log(alpha_forward)\n        \n        # We also need alpha(y -> x)\n        log_R_reverse = (U_y - U_x) + log_q_forward - log_q_reverse\n        alpha_reverse = min(1.0, np.exp(log_R_reverse))\n        log_alpha_reverse = np.log(alpha_reverse)\n\n        # Compute log transition probabilities T1 = pi(x)K(x,y) and T2 = pi(y)K(y,x)\n        log_T1 = -U_x + log_q_forward + log_alpha_forward\n        log_T2 = -U_y + log_q_reverse + log_alpha_reverse\n        \n        # Stably compute Delta = (T1-T2)/max(T1,T2)\n        diff = log_T1 - log_T2\n        if diff > 0:\n            # Delta = 1 - T2/T1 = 1 - exp(log_T2 - log_T1) = 1 - exp(-diff)\n            # -expm1(-x) is more numerically stable for small x than 1-exp(-x)\n            delta = -np.expm1(-diff)\n        else:\n            # Delta = T1/T2 - 1 = exp(log_T1 - log_T2) - 1 = exp(diff) - 1\n            delta = np.expm1(diff)\n            \n        results.append(delta)\n\n    # Format and print the final output\n    print(f\"[{','.join(f'{r:.15g}' for r in results)}]\")\n\nsolve()\n```", "id": "3362451"}]}