{"hands_on_practices": [{"introduction": "The convergence rate of the steepest descent method is highly sensitive to the conditioning of the problem's Hessian matrix, which in turn reflects the scaling of the underlying model parameters. This practice provides a concrete, analytical exploration of this sensitivity. By examining a simple, hypothetical two-dimensional problem [@problem_id:3422227], you will see firsthand how disparate scaling in the columns of the forward operator leads to an ill-conditioned Hessian, causing the steepest descent path to become inefficient. This exercise is foundational for appreciating why more advanced techniques, such as preconditioning, are often necessary.", "problem": "Consider a linear inverse problem with a two-parameter state vector $x \\in \\mathbb{R}^{2}$ and a measurement operator $A \\in \\mathbb{R}^{2 \\times 2}$ relating $x$ to measured data $b \\in \\mathbb{R}^{2}$ via $y = A x$. Adopt the Least Squares (LS) objective $F(x) = \\frac{1}{2} \\|A x - b\\|_{2}^{2}$ and a Steepest Descent (SD) iteration that, from a current $x$, moves along the negative gradient direction with an exact line search along that direction. It is known in inverse problems and data assimilation that SD can be sensitive to the scaling of the components of $x$.\n\nYou are asked to analyze sensitivity to scaling via a concrete example and a variable transformation. Let\n$$\nA(\\epsilon) = \\begin{pmatrix} 1 & 0 \\\\ 0 & \\epsilon \\end{pmatrix}, \\qquad b = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix},\n$$\nwhere $\\epsilon \\in (0,1)$ represents a strong scale disparity of the columns of $A(\\epsilon)$.\n\nTasks:\n1. Starting from the definitions of the LS objective and the gradient, explain why steepest descent is sensitive to the scaling of $x$-components when the columns of $A(\\epsilon)$ have very different Euclidean norms.\n2. Introduce a diagonal scaling matrix $S \\in \\mathbb{R}^{2 \\times 2}$ and the variable transformation $y = S x$. Show how the forward operator maps to $A' = A(\\epsilon) S^{-1}$ and argue for choosing $S$ via the Euclidean column norms of $A(\\epsilon)$ to equilibrate those columns.\n3. For the unscaled variables (i.e., with $S$ equal to the identity), starting at $x_{0} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$, perform one steepest descent step with an exact line search along the negative gradient direction. Denote the resulting iterate by $x_{1}$ and compute the LS objective value $F(x_{1})$ as a closed-form function of $\\epsilon$.\n4. Express your final answer as a single closed-form analytic expression in $\\epsilon$. No rounding is required.\n\nYour derivations must start from core definitions and well-tested facts (the LS objective, its gradient, and exact line search), without using any pre-stated shortcut formulas.", "solution": "The problem asks for an analysis of the steepest descent method's sensitivity to scaling for a linear least-squares problem, followed by a specific calculation for one iteration. We will first establish the general framework, then discuss the scaling sensitivity, and finally perform the detailed calculation.\n\nThe least-squares objective function is given by $F(x) = \\frac{1}{2} \\|Ax - b\\|_{2}^{2}$. We can write this in vector-transpose notation as $F(x) = \\frac{1}{2} (Ax - b)^T (Ax - b)$. Expanding this expression gives:\n$$\nF(x) = \\frac{1}{2} (x^T A^T - b^T)(Ax - b) = \\frac{1}{2} (x^T A^T A x - x^T A^T b - b^T A x + b^T b)\n$$\nSince $b^T A x$ is a scalar, it is equal to its transpose $(b^T A x)^T = x^T A^T b$. Thus, the objective function is:\n$$\nF(x) = \\frac{1}{2} (x^T A^T A x - 2 b^T A x + b^T b)\n$$\nTo find the direction of steepest descent, we compute the gradient of $F(x)$ with respect to $x$. Using standard rules of vector calculus ($\\nabla_x(x^T M x) = (M+M^T)x$ and $\\nabla_x(c^T x) = c$), and noting that $A^T A$ is symmetric, we get:\n$$\n\\nabla F(x) = \\frac{1}{2} (2 A^T A x - 2 A^T b) = A^T(Ax - b)\n$$\nThe steepest descent method updates the current estimate $x_k$ via the iteration $x_{k+1} = x_k + \\alpha_k p_k$, where the search direction $p_k$ is the negative gradient, $p_k = -\\nabla F(x_k)$, and $\\alpha_k$ is a step size.\n\nTask 1: Sensitivity to scaling.\nThe convergence rate of the steepest descent method is governed by the conditioning of the problem. The geometry of the level sets of the objective function $F(x)$ is determined by its Hessian matrix, $H_F = \\nabla^2 F(x)$. For the least-squares problem, the Hessian is constant:\n$$\nH_F = \\nabla (A^T(Ax-b)) = A^T A\n$$\nThe eigenvalues of the Hessian matrix $H_F$ determine the lengths of the principal axes of the ellipsoidal level sets of $F(x)$. If the eigenvalues are of vastly different magnitudes, the level sets are highly elongated (eccentric). The steepest descent direction, being normal to the level set, will be almost perpendicular to the direction of the minimum, leading to a characteristic zig-zagging pattern and slow convergence. The ratio of the largest to the smallest eigenvalue of $H_F$, $\\kappa(H_F) = \\frac{\\lambda_{\\max}}{\\lambda_{\\min}}$, is the condition number. A large condition number indicates poor scaling and slow convergence.\n\nFor the given problem, $A(\\epsilon) = \\begin{pmatrix} 1 & 0 \\\\ 0 & \\epsilon \\end{pmatrix}$. The Hessian is:\n$$\nH_F = A(\\epsilon)^T A(\\epsilon) = \\begin{pmatrix} 1 & 0 \\\\ 0 & \\epsilon \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 0 & \\epsilon \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 0 & \\epsilon^2 \\end{pmatrix}\n$$\nThe eigenvalues of this diagonal matrix are $\\lambda_1 = 1$ and $\\lambda_2 = \\epsilon^2$. The condition number is $\\kappa(H_F) = \\frac{1}{\\epsilon^2}$. Since $\\epsilon \\in (0,1)$, as $\\epsilon \\to 0$, the condition number $\\kappa(H_F) \\to \\infty$. This extreme ill-conditioning, arising from the disparate Euclidean norms of the columns of $A(\\epsilon)$ ($\\|a_1\\|_2=1$, $\\|a_2\\|_2=\\epsilon$), is the reason for the method's sensitivity.\n\nTask 2: Variable transformation.\nTo mitigate this, we introduce a diagonal scaling matrix $S$ and a new variable $y$ such that $x = S^{-1}y$. The forward model $Ax=b$ becomes $A S^{-1} y = b$. The new forward operator is $A' = A S^{-1}$, and the new least-squares problem is to minimize $F'(y) = \\frac{1}{2} \\|A' y - b\\|_2^2$. The Hessian of this new problem is $H_{F'} = (A')^T A' = (S^{-1})^T A^T A S^{-1}$. The goal is to choose $S$ to make $H_{F'}$ well-conditioned, ideally with $\\kappa(H_{F'}) \\approx 1$. A standard choice is to scale the columns of the operator to have unit norm. Let $A = [a_1, a_2]$ and $S = \\text{diag}(s_1, s_2)$. Then $A' = A S^{-1} = [s_1^{-1}a_1, s_2^{-1}a_2]$. To make the new columns have unit norm, we must choose $s_i = \\|a_i\\|_2$. For $A(\\epsilon)$, we have $\\|a_1\\|_2 = 1$ and $\\|a_2\\|_2 = \\epsilon$. Thus, an optimal scaling matrix is $S = \\begin{pmatrix} 1 & 0 \\\\ 0 & \\epsilon \\end{pmatrix}$. With this choice, $A' = A(\\epsilon) S^{-1} = \\begin{pmatrix} 1 & 0 \\\\ 0 & \\epsilon \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 0 & \\epsilon^{-1} \\end{pmatrix} = I$, the identity matrix. The new Hessian would be $I^T I = I$, which is perfectly conditioned.\n\nTask 3 & 4: One steepest descent step for the unscaled problem.\nWe now perform the calculation for the original, unscaled problem, starting from $x_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$.\nThe initial residual is $r_0 = Ax_0 - b = -b = \\begin{pmatrix} -1 \\\\ -1 \\end{pmatrix}$.\nThe gradient at $x_0$ is $\\nabla F(x_0) = A^T r_0 = A^T(-b)$:\n$$\n\\nabla F(x_0) = \\begin{pmatrix} 1 & 0 \\\\ 0 & \\epsilon \\end{pmatrix} \\begin{pmatrix} -1 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ -\\epsilon \\end{pmatrix}\n$$\nThe search direction is the negative gradient:\n$$\np_0 = -\\nabla F(x_0) = \\begin{pmatrix} 1 \\\\ \\epsilon \\end{pmatrix}\n$$\nThe next iterate is $x_1 = x_0 + \\alpha p_0 = \\alpha p_0$. We find the optimal step size $\\alpha$ by minimizing $F(x_0 + \\alpha p_0)$ with respect to $\\alpha$. Let $g(\\alpha) = F(x_0 + \\alpha p_0)$. The minimum is found where $\\frac{dg}{d\\alpha} = 0$.\n$$\n\\frac{dg}{d\\alpha} = \\frac{d}{d\\alpha} F(x_0+\\alpha p_0) = \\nabla F(x_0 + \\alpha p_0)^T p_0 = 0\n$$\nUsing the gradient formula, $\\nabla F(x_0+\\alpha p_0) = A^T(A(x_0+\\alpha p_0)-b) = A^T(Ax_0-b) + \\alpha A^T A p_0 = \\nabla F(x_0) + \\alpha A^T A p_0$.\nSubstituting this into the condition for the minimum:\n$$\n(\\nabla F(x_0) + \\alpha A^T A p_0)^T p_0 = 0 \\implies \\nabla F(x_0)^T p_0 + \\alpha p_0^T A^T A p_0 = 0\n$$\nUsing $p_0 = -\\nabla F(x_0)$, we get $-p_0^T p_0 + \\alpha p_0^T A^T A p_0 = 0$.\nSolving for $\\alpha$ gives the exact line search step size:\n$$\n\\alpha = \\frac{p_0^T p_0}{p_0^T A^T A p_0} = \\frac{\\|p_0\\|_2^2}{\\|A p_0\\|_2^2}\n$$\nWe now compute the terms in this expression for our specific problem.\nThe numerator is $\\|p_0\\|_2^2 = 1^2 + \\epsilon^2 = 1 + \\epsilon^2$.\nFor the denominator, we first compute $A p_0$:\n$$\nA p_0 = \\begin{pmatrix} 1 & 0 \\\\ 0 & \\epsilon \\end{pmatrix} \\begin{pmatrix} 1 \\\\ \\epsilon \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ \\epsilon^2 \\end{pmatrix}\n$$\nThen, $\\|A p_0\\|_2^2 = 1^2 + (\\epsilon^2)^2 = 1 + \\epsilon^4$.\nThe step size is therefore:\n$$\n\\alpha = \\frac{1 + \\epsilon^2}{1 + \\epsilon^4}\n$$\nThe new iterate $x_1$ is:\n$$\nx_1 = x_0 + \\alpha p_0 = \\frac{1 + \\epsilon^2}{1 + \\epsilon^4} \\begin{pmatrix} 1 \\\\ \\epsilon \\end{pmatrix} = \\begin{pmatrix} \\frac{1 + \\epsilon^2}{1 + \\epsilon^4} \\\\ \\frac{\\epsilon(1 + \\epsilon^2)}{1 + \\epsilon^4} \\end{pmatrix}\n$$\nFinally, we compute the objective function value at this new point, $F(x_1) = \\frac{1}{2} \\|A x_1 - b\\|_2^2$.\nThe new residual is $r_1 = A x_1 - b = A(\\alpha p_0) - b = \\alpha (A p_0) - b$:\n$$\nr_1 = \\frac{1 + \\epsilon^2}{1 + \\epsilon^4} \\begin{pmatrix} 1 \\\\ \\epsilon^2 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} \\frac{1 + \\epsilon^2}{1 + \\epsilon^4} - 1 \\\\ \\frac{\\epsilon^2(1 + \\epsilon^2)}{1 + \\epsilon^4} - 1 \\end{pmatrix} = \\begin{pmatrix} \\frac{1 + \\epsilon^2 - (1 + \\epsilon^4)}{1 + \\epsilon^4} \\\\ \\frac{\\epsilon^2 + \\epsilon^4 - (1 + \\epsilon^4)}{1 + \\epsilon^4} \\end{pmatrix} = \\begin{pmatrix} \\frac{\\epsilon^2 - \\epsilon^4}{1 + \\epsilon^4} \\\\ \\frac{\\epsilon^2 - 1}{1 + \\epsilon^4} \\end{pmatrix}\n$$\nNow we find the squared norm of this residual:\n$$\n\\|r_1\\|_2^2 = \\left( \\frac{\\epsilon^2(1 - \\epsilon^2)}{1 + \\epsilon^4} \\right)^2 + \\left( \\frac{-(1 - \\epsilon^2)}{1 + \\epsilon^4} \\right)^2 = \\frac{\\epsilon^4(1 - \\epsilon^2)^2}{(1 + \\epsilon^4)^2} + \\frac{(1 - \\epsilon^2)^2}{(1 + \\epsilon^4)^2}\n$$\nFactoring out the common terms:\n$$\n\\|r_1\\|_2^2 = \\frac{(1 - \\epsilon^2)^2 (\\epsilon^4 + 1)}{(1 + \\epsilon^4)^2} = \\frac{(1 - \\epsilon^2)^2}{1 + \\epsilon^4}\n$$\nThe value of the objective function at $x_1$ is half of this value:\n$$\nF(x_1) = \\frac{1}{2}\\|r_1\\|_2^2 = \\frac{(1 - \\epsilon^2)^2}{2(1 + \\epsilon^4)}\n$$\nThis is the final closed-form expression for the objective value after one step of steepest descent.", "answer": "$$\n\\boxed{\\frac{(1 - \\epsilon^2)^2}{2(1 + \\epsilon^4)}}\n$$", "id": "3422227"}, {"introduction": "Having established the challenge posed by ill-conditioning, we now turn to a powerful and practical solution: preconditioning. This practice advances into the domain of geophysical data assimilation, applying steepest descent to the Three-Dimensional Variational (3D-Var) objective function [@problem_id:3422256]. You will implement and compare both standard and preconditioned steepest descent, using the background-error covariance matrix $B$ as a physically meaningful preconditioner to see how it reshapes the optimization landscape and can dramatically accelerate convergence.", "problem": "Consider the Three-Dimensional Variational (3D-Var) data assimilation objective function\n$$\nf(x) = \\frac{1}{2}\\|H x - y\\|_{R^{-1}}^2 + \\frac{1}{2}\\|x - x_b\\|_{B^{-1}}^2,\n$$\nwhere $x \\in \\mathbb{R}^n$ is the state to be estimated, $x_b \\in \\mathbb{R}^n$ is the background state, $y \\in \\mathbb{R}^m$ are the observations, $H \\in \\mathbb{R}^{m \\times n}$ is the linear observation operator, $R \\in \\mathbb{R}^{m \\times m}$ is the observation-error covariance (assumed symmetric positive definite), and $B \\in \\mathbb{R}^{n \\times n}$ is the background-error covariance (assumed symmetric positive definite). The weighted norm $\\|v\\|_{M}^2$ is defined by $\\|v\\|_{M}^2 = v^\\top M v$ for any symmetric positive definite matrix $M$.\n\nStarting from the core definitions of weighted least squares and the derivative of quadratic forms, derive the steepest descent algorithm for minimizing $f(x)$ both without preconditioning and with the preconditioner $P = B$. Use exact line search at each iteration, computed from first principles (do not use canned formulas). Use the background state $x_b$ as the initial guess. Implement both algorithms and compare their behavior.\n\nYour program must, for each test case specified below, compute the following quantities:\n- The two-norm condition number of the symmetric positive definite matrix $A$ that acts as the Hessian of $f(x)$.\n- The two-norm condition number of the symmetrically preconditioned operator $B^{1/2} A B^{1/2}$, where $B^{1/2}$ is the unique symmetric square root of $B$.\n- The number of iterations required by unpreconditioned steepest descent (direction $-g$) to achieve $\\|x_k - x^\\star\\|_2 / \\|x^\\star\\|_2 \\le \\varepsilon$ with exact line search, where $x^\\star$ is the unique minimizer and $\\varepsilon = 10^{-8}$.\n- The number of iterations required by preconditioned steepest descent (direction $-B g$) to achieve the same tolerance with exact line search.\n- A boolean indicating whether preconditioning achieves strictly fewer iterations than the unpreconditioned method.\n\nUse real-valued computations and do not introduce any arbitrary unit conversions. Angles are not involved. All outputs should be numeric or boolean types. The test suite is as follows, with all matrices and vectors given explicitly:\n\n- Test case 1 (structure matched to the background):\n  - $n = 3$, $m = 3$.\n  - $B = \\mathrm{diag}(9, 1, 0.25)$.\n  - $R = I_3$.\n  - $H = 2 B^{-1/2} = \\mathrm{diag}\\left(\\frac{2}{3}, 2, 4\\right)$.\n  - $x_b = \\begin{bmatrix} 0.1 \\\\ -0.2 \\\\ 0.3 \\end{bmatrix}$, $y = \\begin{bmatrix} 1.0 \\\\ -2.0 \\\\ 0.5 \\end{bmatrix}$.\n\n- Test case 2 (identity observation operator with heterogeneous observation variances):\n  - $n = 3$, $m = 3$.\n  - $B = \\mathrm{diag}(1, 100, 10000)$.\n  - $R = \\mathrm{diag}(1, 0.01, 100)$.\n  - $H = I_3$.\n  - $x_b = \\begin{bmatrix} 1.0 \\\\ -1.0 \\\\ 0.0 \\end{bmatrix}$, $y = \\begin{bmatrix} 0.0 \\\\ 0.0 \\\\ 0.0 \\end{bmatrix}$.\n\n- Test case 3 (low-rank observation operator):\n  - $n = 3$, $m = 2$.\n  - $B = \\mathrm{diag}(4, 0.04, 25)$.\n  - $R = \\mathrm{diag}(0.5, 0.1)$.\n  - $H = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}$.\n  - $x_b = \\begin{bmatrix} 0.5 \\\\ -0.5 \\\\ 1.0 \\end{bmatrix}$, $y = \\begin{bmatrix} 1.0 \\\\ -2.0 \\end{bmatrix}$.\n\nYour implementation must:\n- Construct the quadratic form of $f(x)$ and its gradient from the definitions, identifying the symmetric positive definite matrix $A$ and the vector $b$ such that $\\nabla f(x) = A x - b$ and $x^\\star$ solves $A x^\\star = b$.\n- Use exact line search at each iteration, derived from first principles, for both the unpreconditioned ($P = I$) and preconditioned ($P = B$) steepest descent directions.\n- Compute the two-norm condition numbers by eigenvalue analysis for symmetric positive definite matrices.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with one entry per test case. Each entry should itself be a list $[\\kappa(A), \\kappa(B^{1/2} A B^{1/2}), N_{\\text{unprec}}, N_{\\text{prec}}, \\text{better}]$, where $\\kappa(\\cdot)$ denotes the two-norm condition number, $N_{\\text{unprec}}$ and $N_{\\text{prec}}$ are integers, and $\\text{better}$ is a boolean. For example, the final output format must be\n$$\n[\\,[\\kappa_1,\\kappa_{\\text{pre},1},N_{\\text{unprec},1},N_{\\text{prec},1},\\text{better}_1],\\,[\\kappa_2,\\kappa_{\\text{pre},2},N_{\\text{unprec},2},N_{\\text{prec},2},\\text{better}_2],\\,[\\kappa_3,\\kappa_{\\text{pre},3},N_{\\text{unprec},3},N_{\\text{prec},3},\\text{better}_3]\\,].\n$$", "solution": "The problem requires the derivation and implementation of the steepest descent algorithm to minimize the Three-Dimensional Variational (3D-Var) data assimilation objective function, both with and without preconditioning.\n\nThe objective function is given by\n$$\nf(x) = \\frac{1}{2}\\|H x - y\\|_{R^{-1}}^2 + \\frac{1}{2}\\|x - x_b\\|_{B^{-1}}^2\n$$\nwhere $x \\in \\mathbb{R}^n$ is the state vector, $x_b \\in \\mathbb{R}^n$ is the background state, $y \\in \\mathbb{R}^m$ are observations, $H \\in \\mathbb{R}^{m \\times n}$ is the linear observation operator, and $R \\in \\mathbb{R}^{m \\times m}$ and $B \\in \\mathbb{R}^{n \\times n}$ are the symmetric positive definite (SPD) observation-error and background-error covariance matrices, respectively. The weighted norm is defined as $\\|v\\|_M^2 = v^\\top M v$.\n\nFirst, we express the objective function in a standard quadratic form. By expanding the weighted norms, we get:\n$$\nf(x) = \\frac{1}{2}(H x - y)^\\top R^{-1} (H x - y) + \\frac{1}{2}(x - x_b)^\\top B^{-1} (x - x_b)\n$$\nExpanding both terms yields:\n$$\nf(x) = \\frac{1}{2}(x^\\top H^\\top R^{-1}Hx - x^\\top H^\\top R^{-1}y - y^\\top R^{-1}Hx + y^\\top R^{-1}y) + \\frac{1}{2}(x^\\top B^{-1}x - x^\\top B^{-1}x_b - x_b^\\top B^{-1}x + x_b^\\top B^{-1}x_b)\n$$\nSince $R$ and $B$ are symmetric, so are their inverses. Scalar transposition properties ($u^\\top M v = v^\\top M^\\top u$) allow us to combine terms involving $x$:\n$$\nf(x) = \\frac{1}{2}x^\\top (H^\\top R^{-1} H + B^{-1})x - x^\\top (H^\\top R^{-1}y + B^{-1}x_b) + C\n$$\nwhere $C = \\frac{1}{2}y^\\top R^{-1}y + \\frac{1}{2}x_b^\\top B^{-1}x_b$ is a constant that does not depend on $x$. This is the standard quadratic form $f(x) = \\frac{1}{2}x^\\top A x - x^\\top b + C$, where the Hessian matrix $A$ and the vector $b$ are identified as:\n$$\nA = H^\\top R^{-1} H + B^{-1}\n$$\n$$\nb = H^\\top R^{-1}y + B^{-1}x_b\n$$\nThe matrix $R^{-1}$ is SPD because $R$ is SPD. The matrix $H^\\top R^{-1} H$ is symmetric positive semi-definite. The matrix $B^{-1}$ is SPD because $B$ is SPD. The sum of a symmetric positive semi-definite matrix and a symmetric positive definite matrix is symmetric positive definite. Therefore, the Hessian $A$ is SPD, which guarantees that $f(x)$ is strictly convex and possesses a unique minimizer, denoted $x^\\star$.\n\nThe gradient of the objective function is:\n$$\n\\nabla f(x) = g(x) = A x - b\n$$\nThe minimizer $x^\\star$ is found by setting the gradient to zero, $\\nabla f(x^\\star) = 0$, which leads to the linear system:\n$$\nA x^\\star = b\n$$\n\nThe steepest descent algorithm is an iterative method for finding $x^\\star$. Starting from an initial guess $x_0 = x_b$, the iterates are updated according to $x_{k+1} = x_k + \\alpha_k p_k$, where $p_k$ is a search direction and $\\alpha_k$ is the step size. For an exact line search, $\\alpha_k$ is chosen to minimize $f(x_k + \\alpha p_k)$. The derivative of $\\phi(\\alpha) = f(x_k + \\alpha p_k)$ with respect to $\\alpha$ is $\\phi'(\\alpha) = \\nabla f(x_k + \\alpha p_k)^\\top p_k$. Setting this to zero gives:\n$$\n(A(x_k + \\alpha_k p_k) - b)^\\top p_k = 0 \\implies (A x_k - b + \\alpha_k A p_k)^\\top p_k = 0 \\implies (g_k + \\alpha_k A p_k)^\\top p_k = 0\n$$\nSolving for $\\alpha_k$ yields the general formula for the optimal step size:\n$$\n\\alpha_k = -\\frac{g_k^\\top p_k}{p_k^\\top A p_k}\n$$\n\n**1. Unpreconditioned Steepest Descent**\nThe search direction is the negative gradient, $p_k = -g_k$. Substituting this into the step size formula:\n$$\n\\alpha_k = -\\frac{g_k^\\top (-g_k)}{(-g_k)^\\top A (-g_k)} = \\frac{g_k^\\top g_k}{g_k^\\top A g_k}\n$$\nThe update rule is $x_{k+1} = x_k - \\alpha_k g_k$.\n\n**2. Preconditioned Steepest Descent with $P=B$**\nThe preconditioner $P=B$ is used to define the search direction $p_k = -P g_k = -B g_k$. Substituting this new direction into the formula for $\\alpha_k$:\n$$\n\\alpha_k = -\\frac{g_k^\\top (-B g_k)}{(-B g_k)^\\top A (-B g_k)} = \\frac{g_k^\\top B g_k}{g_k^\\top B^\\top A B g_k}\n$$\nSince $B$ is symmetric ($B^\\top = B$), this simplifies to:\n$$\n\\alpha_k = \\frac{g_k^\\top B g_k}{g_k^\\top B A B g_k}\n$$\nThe update rule is $x_{k+1} = x_k - \\alpha_k B g_k$.\n\n**Convergence Analysis**\nThe convergence rate of the steepest descent method for a quadratic function with an SPD Hessian $A$ is determined by the two-norm condition number of $A$, $\\kappa_2(A)$. For an SPD matrix $M$, $\\kappa_2(M) = \\lambda_{\\max}(M) / \\lambda_{\\min}(M)$, where $\\lambda_{\\max}$ and $\\lambda_{\\min}$ are the maximum and minimum eigenvalues. A larger condition number implies slower convergence.\n\nPreconditioning aims to transform the problem into one with a lower condition number. Using a symmetric preconditioner $P=B$ is equivalent to applying standard steepest descent to a transformed problem. With the change of variables $\\hat{x} = B^{-1/2}x$, the objective function becomes $\\hat{f}(\\hat{x}) = \\frac{1}{2}\\hat{x}^\\top (B^{1/2} A B^{1/2}) \\hat{x} - \\hat{x}^\\top (B^{1/2} b) + C$. The convergence rate of the preconditioned algorithm is therefore governed by the condition number of the symmetrically preconditioned Hessian, $\\kappa_2(B^{1/2} A B^{1/2})$. An effective preconditioner results in $\\kappa_2(B^{1/2} A B^{1/2}) \\ll \\kappa_2(A)$. The implementation will compute these condition numbers and count the iterations required for convergence for both methods.", "answer": "```python\nimport numpy as np\n\ndef steepest_descent(A, b, x_star, x_b, B=None, method='unpreconditioned', epsilon=1e-8, max_iter=500000):\n    \"\"\"\n    Performs steepest descent for the quadratic problem 1/2*x'Ax - b'x.\n    \"\"\"\n    x = x_b.copy()\n    \n    x_star_norm = np.linalg.norm(x_star)\n    \n    if x_star_norm < 1e-12:\n        # If the true solution is the zero vector, use absolute error\n        error_func = lambda x_k: np.linalg.norm(x_k)\n    else:\n        # Otherwise, use relative error\n        error_func = lambda x_k: np.linalg.norm(x_k - x_star) / x_star_norm\n\n    for k in range(max_iter + 1):\n        if error_func(x) <= epsilon:\n            return k\n\n        g = A @ x - b\n        \n        # In case the initial guess is the solution\n        if np.linalg.norm(g) < 1e-15:\n            return k\n\n        if method == 'unpreconditioned':\n            # Search direction p_k = -g_k\n            alpha_num = g.T @ g\n            alpha_den = g.T @ A @ g\n            if alpha_den == 0: break\n            alpha = alpha_num / alpha_den\n            x = x - alpha * g\n        elif method == 'preconditioned':\n            # Search direction p_k = -B @ g_k\n            p = B @ g\n            alpha_num = g.T @ p\n            alpha_den = p.T @ A @ p\n            if alpha_den == 0: break\n            alpha = alpha_num / alpha_den\n            x = x - alpha * p\n        else:\n            raise ValueError(\"Invalid method specified.\")\n            \n    return max_iter # Return max_iter to indicate non-convergence\n\ndef solve():\n    \"\"\"\n    Main function to run test cases and generate the final output.\n    \"\"\"\n    test_cases = [\n        # Test case 1\n        {\n            'n': 3, 'm': 3,\n            'B': np.diag([9.0, 1.0, 0.25]),\n            'R': np.diag([1.0, 1.0, 1.0]),\n            'H': np.diag([2.0/3.0, 2.0, 4.0]),\n            'x_b': np.array([0.1, -0.2, 0.3]),\n            'y': np.array([1.0, -2.0, 0.5])\n        },\n        # Test case 2\n        {\n            'n': 3, 'm': 3,\n            'B': np.diag([1.0, 100.0, 10000.0]),\n            'R': np.diag([1.0, 0.01, 100.0]),\n            'H': np.diag([1.0, 1.0, 1.0]),\n            'x_b': np.array([1.0, -1.0, 0.0]),\n            'y': np.array([0.0, 0.0, 0.0])\n        },\n        # Test case 3\n        {\n            'n': 3, 'm': 2,\n            'B': np.diag([4.0, 0.04, 25.0]),\n            'R': np.diag([0.5, 0.1]),\n            'H': np.array([[1.0, 0.0, 0.0], [0.0, 0.0, 1.0]]),\n            'x_b': np.array([0.5, -0.5, 1.0]),\n            'y': np.array([1.0, -2.0])\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        H, R, B, x_b, y = case['H'], case['R'], case['B'], case['x_b'], case['y']\n        \n        # Invert R and B\n        R_inv = np.linalg.inv(R)\n        B_inv = np.linalg.inv(B)\n        \n        # Formulate the Hessian A and vector b for the system Ax = b\n        A = H.T @ R_inv @ H + B_inv\n        b = H.T @ R_inv @ y + B_inv @ x_b\n        \n        # Calculate the true solution x_star\n        x_star = np.linalg.solve(A, b)\n        \n        # --- Condition number calculations ---\n        # 1. Unpreconditioned Hessian A\n        eigvals_A = np.linalg.eigvalsh(A)\n        kappa_A = np.max(eigvals_A) / np.min(eigvals_A)\n        \n        # 2. Symmetrically preconditioned Hessian\n        B_sqrt = np.sqrt(B) # B is diagonal, so sqrt is element-wise\n        A_pre = B_sqrt @ A @ B_sqrt\n        eigvals_A_pre = np.linalg.eigvalsh(A_pre)\n        kappa_A_pre = np.max(eigvals_A_pre) / np.min(eigvals_A_pre)\n        \n        # --- Iteration counts ---\n        # 3. Unpreconditioned steepest descent\n        N_unprec = steepest_descent(A, b, x_star, x_b, method='unpreconditioned')\n        \n        # 4. Preconditioned steepest descent\n        N_prec = steepest_descent(A, b, x_star, x_b, B=B, method='preconditioned')\n        \n        # 5. Comparison\n        better = N_prec < N_unprec\n        \n        results.append([kappa_A, kappa_A_pre, N_unprec, N_prec, better])\n\n    # Format the final output string precisely\n    output_strings = []\n    for res in results:\n        # Convert boolean to lowercase 'true'/'false' for consistent output format\n        res_str = f\"[{res[0]},{res[1]},{res[2]},{res[3]},{str(res[4]).lower()}]\"\n        output_strings.append(res_str)\n        \n    print(f\"[{','.join(output_strings)}]\")\n\nsolve()\n```", "id": "3422256"}, {"introduction": "Beyond convergence speed, the reliability of a solution in the presence of non-ideal data is paramount. The standard least-squares objective, which penalizes the squared L2-norm of the residual, is notoriously sensitive to outliers common in heavy-tailed noise distributions. This practice [@problem_id:3422261] introduces robust estimation by replacing the quadratic loss with the Huber loss, which behaves quadratically for small residuals but linearly for large ones. By comparing the gradient directions derived from these two different objective functions, you will quantify how a robust loss function effectively down-weights the influence of outliers, leading to more reliable parameter estimates.", "problem": "Consider a linear observation operator with matrix $A \\in \\mathbb{R}^{m \\times n}$ and a measurement vector $b \\in \\mathbb{R}^{m}$. Define the residual $r(x) = A x - b$. In classical least squares for inverse problems and data assimilation, one considers the objective function $f(x) = \\frac{1}{2}\\lVert r(x) \\rVert_2^2$, where $\\lVert \\cdot \\rVert_2$ denotes the Euclidean norm. Under heavy-tailed noise, large residuals (outliers) can dominate the descent direction of $f(x)$. To mitigate this, consider the Huber loss with threshold parameter $\\delta > 0$, defined elementwise for a scalar residual $t$ by\n$$\n\\phi_\\delta(t) = \n\\begin{cases}\n\\frac{1}{2} t^2, & \\text{if } |t| \\le \\delta, \\\\\n\\delta |t| - \\frac{1}{2} \\delta^2, & \\text{if } |t| > \\delta.\n\\end{cases}\n$$\nDefine the robust objective $g_\\delta(x) = \\sum_{i=1}^{m} \\phi_\\delta(r_i(x))$, where $r_i(x)$ denotes the $i$-th component of $r(x)$.\n\nThe steepest descent direction under the Euclidean geometry for a differentiable objective $h(x)$ at $x$ is the negative gradient $- \\nabla h(x)$. You must derive the gradient expressions from first principles and the chain rule starting from the definitions above, without assuming any pre-given gradient formulas.\n\nYour task is to implement a program that, for each test case specified below, computes:\n1. The steepest descent direction of the quadratic loss $f(x)$ at a given $x$, and the steepest descent direction of the Huber loss $g_\\delta(x)$ at the same $x$.\n2. The angle between these two directions measured in radians.\n3. A quantitative outlier sensitivity metric for each loss, defined as the ratio of the Euclidean norm of the contribution to the gradient coming from the set of outlier indices to the Euclidean norm of the full gradient. Specifically, define the outlier index set $O = \\{ i \\in \\{1,\\dots,m\\} : |r_i(x)| > \\delta \\}$. For the quadratic loss, the outlier contribution is obtained by zeroing all non-outlier residuals within $r(x)$ and recomputing the gradient. For the Huber loss, the outlier contribution is computed by zeroing all non-outlier elements within the vector of elementwise derivatives of the Huber loss with respect to residuals and then recomputing the gradient. If the full gradient is the zero vector, define both the angle and the ratios to be the scalar $0$ for that test case.\n\nYou must express the angle in radians. No physical units apply. All outputs must be real numbers. Your program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case contributes a three-element list $[\\text{angle}, \\text{outlier\\_fraction\\_L2}, \\text{outlier\\_fraction\\_Huber}]$ and the full output is a list of these lists, for example, $[[0.1,0.2,0.05],[\\dots],\\dots]$.\n\nUse the following test suite. For each test case, construct $b$ deterministically by $b = A x_{\\text{true}} + \\eta$, where $x_{\\text{true}}$ and $\\eta$ are given. Heavy-tailed noise is represented by two large entries within $\\eta$.\n\nLet $m = 10$ and $n = 4$. Define\n$$\nA = \\begin{bmatrix}\n1.0 & 0.0 & 2.0 & -1.0 \\\\\n0.5 & -1.0 & 0.0 & 1.5 \\\\\n-0.3 & 0.7 & 1.0 & 0.0 \\\\\n2.0 & -0.5 & 0.0 & 0.3 \\\\\n-1.5 & 0.0 & 1.2 & 0.8 \\\\\n0.0 & 1.0 & -0.5 & 2.0 \\\\\n-0.7 & 0.4 & 0.0 & 1.1 \\\\\n1.3 & 0.0 & -1.0 & 0.0 \\\\\n0.2 & -0.9 & 0.5 & -0.3 \\\\\n-0.1 & 0.2 & 1.5 & 1.0\n\\end{bmatrix},\n\\quad\nx_{\\text{true}} = \\begin{bmatrix} 1.5 \\\\ -0.5 \\\\ 0.8 \\\\ 0.0 \\end{bmatrix},\n\\quad\n\\eta = \\begin{bmatrix} 0.1 \\\\ -0.2 \\\\ 0.05 \\\\ 0.0 \\\\ 0.0 \\\\ -0.1 \\\\ 0.05 \\\\ 20.0 \\\\ -25.0 \\\\ 0.2 \\end{bmatrix}.\n$$\nForm $b = A x_{\\text{true}} + \\eta$.\n\nDefine the test cases as the tuples $(A, b, \\delta, x)$:\n- Case 1 (general heavy-tailed scenario): $(A, b, 1.0, x = \\mathbf{0})$, where $\\mathbf{0}$ is the zero vector in $\\mathbb{R}^4$.\n- Case 2 (no outliers boundary): $(A, b, 30.0, x = \\mathbf{0})$.\n- Case 3 (residuals equal to negative noise, stronger outlier impact): $(A, b, 0.5, x = x_{\\text{true}})$.\n\nFor each case, compute the three outputs as specified. Your program should produce a single line with the aggregated results formatted exactly as a comma-separated list enclosed in square brackets, with no extra whitespace, for example:\n$[[\\text{angle}_1,\\text{fracL2}_1,\\text{fracHuber}_1],[\\text{angle}_2,\\text{fracL2}_2,\\text{fracHuber}_2],[\\text{angle}_3,\\text{fracL2}_3,\\text{fracHuber}_3]]$.", "solution": "The user-provided problem is valid as it is scientifically grounded in numerical optimization and robust statistics, well-posed with all necessary data and definitions provided, and formulated objectively. We can therefore proceed with a formal solution.\n\nThe objective is to compare the steepest descent directions for a standard quadratic loss function and a robust Huber loss function in the context of a linear inverse problem. This involves deriving the gradients of both objective functions, and then using these to compute the angle between the descent directions and a metric for outlier sensitivity.\n\nLet the state vector be $x \\in \\mathbb{R}^n$, the observation operator be the matrix $A \\in \\mathbb{R}^{m \\times n}$, and the measurement vector be $b \\in \\mathbb{R}^m$. The residual vector is defined as $r(x) = Ax - b \\in \\mathbb{R}^m$.\n\n**1. Gradient of the Quadratic Objective Function $f(x)$**\n\nThe classical least-squares objective function is given by:\n$$ f(x) = \\frac{1}{2} \\lVert r(x) \\rVert_2^2 = \\frac{1}{2} r(x)^T r(x) = \\frac{1}{2} \\sum_{i=1}^{m} [r_i(x)]^2 $$\nTo find the gradient $\\nabla f(x)$, we compute the partial derivative with respect to each component $x_j$ of the vector $x$, for $j \\in \\{1, \\dots, n\\}$. Using the chain rule:\n$$ \\frac{\\partial f}{\\partial x_j} = \\frac{\\partial}{\\partial x_j} \\left( \\frac{1}{2} \\sum_{i=1}^{m} [r_i(x)]^2 \\right) = \\sum_{i=1}^{m} r_i(x) \\frac{\\partial r_i(x)}{\\partial x_j} $$\nThe $i$-th component of the residual is $r_i(x) = (\\sum_{k=1}^{n} A_{ik} x_k) - b_i$. Its partial derivative with respect to $x_j$ is:\n$$ \\frac{\\partial r_i(x)}{\\partial x_j} = \\frac{\\partial}{\\partial x_j} \\left( \\sum_{k=1}^{n} A_{ik} x_k - b_i \\right) = A_{ij} $$\nSubstituting this back into the expression for the partial derivative of $f(x)$:\n$$ \\frac{\\partial f}{\\partial x_j} = \\sum_{i=1}^{m} r_i(x) A_{ij} = \\sum_{i=1}^{m} (A^T)_{ji} r_i(x) $$\nThis is the $j$-th component of the matrix-vector product $A^T r(x)$. Therefore, the gradient of $f(x)$ is:\n$$ \\nabla f(x) = A^T r(x) = A^T (Ax - b) $$\nThe steepest descent direction for $f(x)$ at $x$ is the negative of the gradient, $d_f(x) = -\\nabla f(x) = -A^T (Ax - b)$.\n\n**2. Gradient of the Robust Huber Objective Function $g_\\delta(x)$**\n\nThe Huber objective function is defined as $g_\\delta(x) = \\sum_{i=1}^{m} \\phi_\\delta(r_i(x))$, where $\\phi_\\delta(t)$ is the Huber loss for a scalar argument $t$ with threshold $\\delta > 0$:\n$$\n\\phi_\\delta(t) = \n\\begin{cases}\n\\frac{1}{2} t^2, & \\text{if } |t| \\le \\delta, \\\\\n\\delta |t| - \\frac{1}{2} \\delta^2, & \\text{if } |t| > \\delta.\n\\end{cases}\n$$\nThe derivative of $\\phi_\\delta(t)$ with respect to $t$, denoted by $\\psi_\\delta(t)$, is:\n$$\n\\psi_\\delta(t) = \\frac{d\\phi_\\delta(t)}{dt} = \n\\begin{cases}\nt, & \\text{if } |t| \\le \\delta, \\\\\n\\delta \\cdot \\text{sgn}(t), & \\text{if } |t| > \\delta.\n\\end{cases}\n$$\nThis function is continuous. It can be expressed compactly as $\\psi_\\delta(t) = \\text{clip}(t, -\\delta, \\delta) = \\min(\\delta, \\max(-\\delta, t))$.\n\nUsing the chain rule to find the gradient $\\nabla g_\\delta(x)$:\n$$ \\frac{\\partial g_\\delta}{\\partial x_j} = \\frac{\\partial}{\\partial x_j} \\left( \\sum_{i=1}^{m} \\phi_\\delta(r_i(x)) \\right) = \\sum_{i=1}^{m} \\frac{d\\phi_\\delta(r_i(x))}{dr_i} \\frac{\\partial r_i(x)}{\\partial x_j} $$\nSubstituting $\\frac{d\\phi_\\delta(r_i(x))}{dr_i} = \\psi_\\delta(r_i(x))$ and $\\frac{\\partial r_i(x)}{\\partial x_j} = A_{ij}$:\n$$ \\frac{\\partial g_\\delta}{\\partial x_j} = \\sum_{i=1}^{m} \\psi_\\delta(r_i(x)) A_{ij} = \\sum_{i=1}^{m} (A^T)_{ji} \\psi_\\delta(r_i(x)) $$\nLet $\\Psi_\\delta(r)$ be the vector function that applies $\\psi_\\delta$ element-wise to the vector $r$. The gradient of $g_\\delta(x)$ is then:\n$$ \\nabla g_\\delta(x) = A^T \\Psi_\\delta(r(x)) = A^T \\Psi_\\delta(Ax - b) $$\nThe steepest descent direction for $g_\\delta(x)$ at $x$ is $d_g(x) = -\\nabla g_\\delta(x) = -A^T \\Psi_\\delta(Ax - b)$.\n\n**3. Angle Between Steepest Descent Directions**\n\nThe angle $\\theta$ in radians between the two steepest descent directions, $d_f$ and $d_g$, is calculated using the dot product formula:\n$$ \\theta = \\arccos\\left( \\frac{d_f^T d_g}{\\lVert d_f \\rVert_2 \\lVert d_g \\rVert_2} \\right) = \\arccos\\left( \\frac{(\\nabla f)^T (\\nabla g)}{\\lVert \\nabla f \\rVert_2 \\lVert \\nabla g \\rVert_2} \\right) $$\nAs per the problem specification, if either gradient is the zero vector (i.e., $\\lVert \\nabla f \\rVert_2 = 0$ or $\\lVert \\nabla g \\rVert_2 = 0$), the angle is defined to be $0$.\n\n**4. Outlier Sensitivity Metrics**\n\nThe outlier index set is $O = \\{ i \\in \\{1, \\dots, m\\} : |r_i(x)| > \\delta \\}$.\n\nFor the quadratic loss, the gradient contribution from outliers is computed using a masked residual vector $r_{\\text{outlier}}$, where $(r_{\\text{outlier}})_i = r_i$ if $i \\in O$ and $0$ otherwise. The contribution is $\\nabla f_{\\text{outlier}} = A^T r_{\\text{outlier}}$. The sensitivity metric is the ratio of norms:\n$$ \\text{fracL2} = \\frac{\\lVert \\nabla f_{\\text{outlier}} \\rVert_2}{\\lVert \\nabla f \\rVert_2} $$\n\nFor the Huber loss, the gradient contribution from outliers is computed using a masked vector $\\Psi_{\\delta, \\text{outlier}}(r)$, where $(\\Psi_{\\delta, \\text{outlier}}(r))_i = \\psi_\\delta(r_i)$ if $i \\in O$ and $0$ otherwise. The contribution is $\\nabla g_{\\delta, \\text{outlier}} = A^T \\Psi_{\\delta, \\text{outlier}}(r)$. The sensitivity metric is:\n$$ \\text{fracHuber} = \\frac{\\lVert \\nabla g_{\\delta, \\text{outlier}} \\rVert_2}{\\lVert \\nabla g_\\delta \\rVert_2} $$\nIn both cases, if the norm of the full gradient in the denominator is zero, the ratio is defined as $0$.\n\nThe following implementation computes these quantities for the specified test cases.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by computing metrics for quadratic and Huber losses\n    for a series of test cases.\n    \"\"\"\n    A = np.array([\n        [1.0, 0.0, 2.0, -1.0],\n        [0.5, -1.0, 0.0, 1.5],\n        [-0.3, 0.7, 1.0, 0.0],\n        [2.0, -0.5, 0.0, 0.3],\n        [-1.5, 0.0, 1.2, 0.8],\n        [0.0, 1.0, -0.5, 2.0],\n        [-0.7, 0.4, 0.0, 1.1],\n        [1.3, 0.0, -1.0, 0.0],\n        [0.2, -0.9, 0.5, -0.3],\n        [-0.1, 0.2, 1.5, 1.0]\n    ])\n    x_true = np.array([1.5, -0.5, 0.8, 0.0])\n    eta = np.array([0.1, -0.2, 0.05, 0.0, 0.0, -0.1, 0.05, 20.0, -25.0, 0.2])\n\n    b = A @ x_true + eta\n\n    test_cases = [\n        # Case 1: General heavy-tailed scenario\n        (A, b, 1.0, np.zeros(4)),\n        # Case 2: No outliers boundary\n        (A, b, 30.0, np.zeros(4)),\n        # Case 3: Residuals equal to negative noise\n        (A, b, 0.5, x_true)\n    ]\n\n    results = []\n    for case in test_cases:\n        A_case, b_case, delta, x_case = case\n        \n        # 1. Compute residual\n        r = A_case @ x_case - b_case\n\n        # 2. Compute gradients for both loss functions\n        grad_f = A_case.T @ r\n        \n        # psi_delta(t) = clip(t, -delta, delta)\n        psi_r = np.clip(r, -delta, delta)\n        grad_g = A_case.T @ psi_r\n        \n        norm_grad_f = np.linalg.norm(grad_f)\n        norm_grad_g = np.linalg.norm(grad_g)\n\n        # 3. Compute angle between gradient vectors\n        angle = 0.0\n        if norm_grad_f > 1e-12 and norm_grad_g > 1e-12:\n            cos_angle = np.dot(grad_f, grad_g) / (norm_grad_f * norm_grad_g)\n            # Clip for numerical stability\n            angle = np.arccos(np.clip(cos_angle, -1.0, 1.0))\n        \n        # 4. Compute outlier sensitivity metrics\n        outlier_indices_mask = np.abs(r) > delta\n        \n        # L2 sensitivity\n        frac_l2 = 0.0\n        if norm_grad_f > 1e-12:\n            r_outlier = np.where(outlier_indices_mask, r, 0.0)\n            grad_f_outlier = A_case.T @ r_outlier\n            frac_l2 = np.linalg.norm(grad_f_outlier) / norm_grad_f\n        \n        # Huber sensitivity\n        frac_huber = 0.0\n        if norm_grad_g > 1e-12:\n            psi_r_outlier = np.where(outlier_indices_mask, psi_r, 0.0)\n            grad_g_outlier = A_case.T @ psi_r_outlier\n            frac_huber = np.linalg.norm(grad_g_outlier) / norm_grad_g\n            \n        results.append([angle, frac_l2, frac_huber])\n\n    # Format the final output string without extra whitespace\n    formatted_results = []\n    for res in results:\n        formatted_results.append(f'[{res[0]},{res[1]},{res[2]}]')\n    \n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3422261"}]}