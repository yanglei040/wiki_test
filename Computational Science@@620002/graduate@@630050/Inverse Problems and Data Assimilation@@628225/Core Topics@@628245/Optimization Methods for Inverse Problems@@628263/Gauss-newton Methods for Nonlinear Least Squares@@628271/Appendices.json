{"hands_on_practices": [{"introduction": "The Gauss-Newton method's power stems from its linearization of a nonlinear problem, a step governed entirely by the Jacobian matrix. This exercise grounds the abstract concept of the Jacobian by revealing its entries as the physical sensitivities of measurements to changes in model parameters. By explicitly constructing the Jacobian for a simple geophysical model, you will gain a concrete understanding of how it dictates the local geometry of the inverse problem and the stability of the Gauss-Newton algorithm [@problem_id:3599237].", "problem": "Consider a two-parameter geophysical model $m=\\begin{pmatrix} m_1 \\\\ m_2 \\end{pmatrix}$, where $m_1$ represents a scalar property controlling a band-limited amplitude response and $m_2$ represents a coupling parameter associated with anisotropy. The forward map $F:\\mathbb{R}^2\\to\\mathbb{R}^2$ is defined by $F(m)=\\begin{pmatrix} \\sin(\\alpha m_1) \\\\ m_1 m_2 \\end{pmatrix}$, where $\\alpha>0$ is a known scaling constant with units chosen so that the argument of the sine function is dimensionless. The data weighting matrix is the identity $W_d=I$. The nonlinear least-squares misfit is defined as $\\phi(m)=\\tfrac{1}{2}\\|F(m)-d\\|_2^2$, where $d\\in\\mathbb{R}^2$ are the observed data.\n\nStarting from the core definitions of the Jacobian matrix of $F(m)$ and the Gauss-Newton linearization of the residual, derive the Jacobian $J(m)$ explicitly in terms of $m_1$, $m_2$, and $\\alpha$. Then, explain how each entry of $J(m)$ reflects the physical sensitivity of the data to the parameters, and interpret how the magnitude and sign of the entries influence the conditioning of the Gauss-Newton normal matrix. Your derivation should proceed from first principles without invoking pre-stated formulas for the Jacobian of $F(m)$ or the Gauss-Newton method.\n\nProvide as your final answer the explicit closed-form analytical expression for $J(m)$. No numerical rounding is required. If you introduce any angles, they must be in radians. Since the final answer is symbolic, do not include units in the final expression.", "solution": "The problem requires the derivation of the Jacobian matrix $J(m)$ for a given forward model $F(m)$, and an interpretation of its entries and their physical impact on the Gauss-Newton method. The validation of the problem statement confirms that it is scientifically grounded, well-posed, and objective. We may proceed with the solution.\n\nThe forward model is a function $F: \\mathbb{R}^2 \\to \\mathbb{R}^2$ that maps the model-parameter vector $m=\\begin{pmatrix} m_1 \\\\ m_2 \\end{pmatrix}$ to a data vector. The components of the forward map are given as:\n$$\nF(m) = \\begin{pmatrix} F_1(m_1, m_2) \\\\ F_2(m_1, m_2) \\end{pmatrix} = \\begin{pmatrix} \\sin(\\alpha m_1) \\\\ m_1 m_2 \\end{pmatrix}\n$$\nwhere $\\alpha > 0$ is a known constant.\n\nThe Jacobian matrix $J(m)$ of the vector-valued function $F(m)$ is, by definition, the matrix of all first-order partial derivatives. For a function mapping from $\\mathbb{R}^2$ to $\\mathbb{R}^2$, it is a $2 \\times 2$ matrix:\n$$\nJ(m) = \\begin{pmatrix} \\frac{\\partial F_1}{\\partial m_1} & \\frac{\\partial F_1}{\\partial m_2} \\\\ \\frac{\\partial F_2}{\\partial m_1} & \\frac{\\partial F_2}{\\partial m_2} \\end{pmatrix}\n$$\n\nWe proceed to calculate each entry of $J(m)$ from first principles using the rules of partial differentiation.\n\nThe first entry, $J_{11}$, is the partial derivative of $F_1(m_1, m_2) = \\sin(\\alpha m_1)$ with respect to $m_1$. Applying the chain rule, we have:\n$$\nJ_{11} = \\frac{\\partial}{\\partial m_1} \\left( \\sin(\\alpha m_1) \\right) = \\cos(\\alpha m_1) \\cdot \\frac{\\partial}{\\partial m_1}(\\alpha m_1) = \\alpha \\cos(\\alpha m_1)\n$$\n\nThe second entry, $J_{12}$, is the partial derivative of $F_1(m_1, m_2) = \\sin(\\alpha m_1)$ with respect to $m_2$. Since $F_1$ does not depend on $m_2$, this derivative is zero:\n$$\nJ_{12} = \\frac{\\partial}{\\partial m_2} \\left( \\sin(\\alpha m_1) \\right) = 0\n$$\n\nThe third entry, $J_{21}$, is the partial derivative of $F_2(m_1, m_2) = m_1 m_2$ with respect to $m_1$:\n$$\nJ_{21} = \\frac{\\partial}{\\partial m_1} \\left( m_1 m_2 \\right) = m_2\n$$\n\nThe fourth entry, $J_{22}$, is the partial derivative of $F_2(m_1, m_2) = m_1 m_2$ with respect to $m_2$:\n$$\nJ_{22} = \\frac{\\partial}{\\partial m_2} \\left( m_1 m_2 \\right) = m_1\n$$\n\nAssembling these partial derivatives into the matrix gives the explicit form of the Jacobian $J(m)$:\n$$\nJ(m) = \\begin{pmatrix} \\alpha \\cos(\\alpha m_1) & 0 \\\\ m_2 & m_1 \\end{pmatrix}\n$$\n\nNext, we interpret the physical meaning of these entries and their influence on the conditioning of the Gauss-Newton normal matrix. Each entry $J_{ij} = \\frac{\\partial F_i}{\\partial m_j}$ represents the sensitivity of the $i$-th data component to an infinitesimal change in the $j$-th model parameter.\n- $J_{11} = \\alpha \\cos(\\alpha m_1)$: This term quantifies the sensitivity of the first datum, $F_1 = \\sin(\\alpha m_1)$, to changes in the parameter $m_1$. The sensitivity is oscillatory and is maximal in magnitude when $|\\cos(\\alpha m_1)| = 1$, which occurs when $\\alpha m_1$ is an integer multiple of $\\pi$. At these points, the function $F_1$ is passing through zero and is steepest. Conversely, the sensitivity is zero when $\\cos(\\alpha m_1) = 0$, which occurs when $\\alpha m_1 = (n + \\frac{1}{2})\\pi$ for any integer $n$. These points correspond to the peaks and troughs of the sine wave, where a small change in $m_1$ results in a negligible change in $F_1$.\n- $J_{12} = 0$: This indicates that the first datum, $F_1$, is completely insensitive to changes in the parameter $m_2$. The measurement of the band-limited amplitude response is, according to this model, entirely decoupled from the anisotropy parameter.\n- $J_{21} = m_2$: This is the sensitivity of the second datum, $F_2 = m_1 m_2$, to perturbations in $m_1$. The sensitivity is directly proportional to the value of the coupling parameter $m_2$. If $m_2$ is close to zero, the second datum becomes insensitive to $m_1$, implying a weak coupling.\n- $J_{22} = m_1$: This is the sensitivity of the second datum $F_2$ to changes in $m_2$. This sensitivity is directly proportional to $m_1$. If $m_1$ is near zero, the second datum is insensitive to the anisotropy parameter $m_2$.\n\nThe Gauss-Newton method approximates the Hessian of the misfit function $\\phi(m)$ with the matrix $H_{GN} = J(m)^T J(m)$ (since the data weighting matrix $W_d = I$). The conditioning of this normal matrix is crucial for the stability and convergence of the inversion. A poorly conditioned or singular $H_{GN}$ leads to unstable parameter updates.\nThe normal matrix is:\n$$\nH_{GN} = J^T J = \\begin{pmatrix} \\alpha \\cos(\\alpha m_1) & m_2 \\\\ 0 & m_1 \\end{pmatrix} \\begin{pmatrix} \\alpha \\cos(\\alpha m_1) & 0 \\\\ m_2 & m_1 \\end{pmatrix} = \\begin{pmatrix} \\alpha^2 \\cos^2(\\alpha m_1) + m_2^2 & m_1 m_2 \\\\ m_1 m_2 & m_1^2 \\end{pmatrix}\n$$\nThe well-posedness of the local linear problem is determined by whether $H_{GN}$ is invertible. This is equivalent to the columns of $J(m)$ being linearly independent. The determinant of $H_{GN}$ provides a measure of this. Using the property $\\det(A^T A) = (\\det A)^2$ for a square matrix $A$, we can compute:\n$$\n\\det(J) = (\\alpha \\cos(\\alpha m_1))(m_1) - (0)(m_2) = \\alpha m_1 \\cos(\\alpha m_1)\n$$\nTherefore, the determinant of the normal matrix is:\n$$\n\\det(H_{GN}) = (\\det(J))^2 = \\alpha^2 m_1^2 \\cos^2(\\alpha m_1)\n$$\nThe normal matrix $H_{GN}$ becomes singular, and the inversion problem ill-conditioned, if $\\det(H_{GN}) = 0$. This occurs under two conditions:\n1. $m_1 = 0$: If the amplitude parameter $m_1$ is zero, the Jacobian becomes $J = \\begin{pmatrix} \\alpha & 0 \\\\ m_2 & 0 \\end{pmatrix}$, whose columns are linearly dependent. Physically, if $m_1=0$, then $F(m) = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ for any value of $m_2$. The data contain no information about $m_2$, making its recovery impossible.\n2. $\\cos(\\alpha m_1) = 0$: This happens when $\\alpha m_1_k = \\frac{\\pi}{2} + k\\pi$ for any integer $k$. In this case, the Jacobian is $J = \\begin{pmatrix} 0 & 0 \\\\ m_2 & m_1 \\end{pmatrix}$. The first row is zero, making the matrix rank-deficient. Physically, this corresponds to the points of zero sensitivity of a the first datum $d_1$ with respect to $m_1$, as discussed earlier. At these extremum points of $F_1$, a small perturbation in $m_1$ cannot be \"seen\" in the first datum, leading to a loss of information and an ill-conditioned system.\n\nThe magnitude of the entries also affects conditioning. If $m_1$ is very small, $\\det(H_{GN})$ becomes very small, leading to poor conditioning. Similarly, if the model is near a point where $\\cos(\\alpha m_1) \\approx 0$, the problem is nearly ill-conditioned. The signs of the Jacobian entries, while not directly affecting the conditioning of $J^T J$ (due to the squaring of terms), are critical for determining the direction of the parameter update step $\\delta m$ in the Gauss-Newton algorithm, as the update depends on the product $J^T (d - F(m))$.", "answer": "$$\n\\boxed{\nJ(m) = \\begin{pmatrix}\n\\alpha \\cos(\\alpha m_1) & 0 \\\\\nm_2 & m_1\n\\end{pmatrix}\n}\n$$", "id": "3599237"}, {"introduction": "Even with a well-defined Jacobian, an inversion can fail if the problem is ill-posed due to non-uniqueness, where different parameter sets yield identical data. This practice explores the concept of an \"identifiability ridge,\" a scenario where the cost function is flat in certain directions, rendering the Gauss-Newton Hessian singular and causing the algorithm to drift aimlessly. The core value is learning to diagnose this failure and implement regularization techniques, such as a Tikhonov prior, to introduce the necessary curvature for a stable and unique solution [@problem_id:3384262].", "problem": "Consider a nonlinear least-squares inverse problem in two parameters where the forward map depends only on a single identifiable combination. Let the parameter vector be $x = (x_1, x_2) \\in \\mathbb{R}^2$, and define the forward model $f: \\mathbb{R}^2 \\to \\mathbb{R}^2$ by\n$$\nf(x) = \\begin{bmatrix}\n\\alpha \\left( \\exp(x_1 + x_2) - 1 \\right) \\\\\n\\beta \\sin(x_1 + x_2)\n\\end{bmatrix},\n$$\nwith known positive scalars $\\alpha$ and $\\beta$. The observed data $y \\in \\mathbb{R}^2$ are generated synthetically by choosing a ground truth $x^{\\star}$ and setting $y = f(x^{\\star})$. The residual is $r(x) = f(x) - y$, and the cost is $F(x) = \\tfrac{1}{2} \\| r(x) \\|_2^2$. This structure produces an identifiability ridge because $f(x)$ depends only on $s = x_1 + x_2$, so $F(x)$ is flat along the orthogonal direction $v = x_1 - x_2$ whenever $s$ matches the identifiable combination encoded in $y$.\n\nYour tasks are:\n- Derive from first principles of linearization and least squares how the Gauss-Newton (GN) method is obtained for minimizing $F(x)$. Specifically, start from the definition of the residual $r(x)$, perform a first-order Taylor linearization $r(x + \\delta) \\approx r(x) + J(x)\\,\\delta$ where $J(x)$ is the Jacobian matrix of $r(x)$, and show how the GN step comes from solving the linearized least-squares subproblem at each iteration.\n- Argue, using the structure of $f(x)$, why there is an identifiability ridge along the direction of constant $s = x_1 + x_2$, and characterize the effect on the approximate Gauss-Newton Hessian $J(x)^{\\top} J(x)$.\n- Propose and implement curvature injection via either a quadratic prior or a structural constraint to eliminate the ridge. Use a quadratic prior that penalizes the non-identifiable direction $v = x_1 - x_2$ by choosing $L \\in \\mathbb{R}^{1 \\times 2}$ and a regularization weight $\\lambda > 0$, and derive the resulting modified Gauss-Newton normal equations.\n\nImplement a program that:\n- Uses the constants $\\alpha = 1$, $\\beta = \\tfrac{1}{2}$, and the ground truth $x^{\\star} = (0.5, -0.3)$, so that $y = f(x^{\\star})$. Angles are in radians.\n- Implements a Gauss-Newton solver to minimize $F(x)$ without regularization, for the following three initial guesses:\n  1. $x^{(0)}_{\\mathrm{A}} = (1.6, -1.0)$,\n  2. $x^{(0)}_{\\mathrm{B}} = (0.7, -0.5)$,\n  3. $x^{(0)}_{\\mathrm{C}} = (-0.5, 0.7)$.\n  For each run, report the final value of $v = x_1 - x_2$ after convergence. This demonstrates the drift along the identifiability ridge (that is, the effective insensitivity of $F(x)$ to $v$).\n- Implements a Gauss-Newton solver with quadratic prior (Tikhonov-type curvature injection) that penalizes the non-identifiable direction $v$, by minimizing\n$$\n\\Phi(x) = \\tfrac{1}{2} \\| r(x) \\|_2^2 + \\tfrac{\\lambda}{2} \\| L(x - x_{\\mathrm{ref}}) \\|_2^2,\n$$\nwith $L = \\begin{bmatrix} 1 & -1 \\end{bmatrix}$, $x_{\\mathrm{ref}} = (0,0)$, and $\\lambda = 1$. For the same three initial guesses as above, run the regularized Gauss-Newton solver and report the final value of $v = x_1 - x_2$ after convergence. This demonstrates how curvature injection via prior eliminates the drift by selecting a unique solution on the ridge.\n- Implements an additional edge-case structural test by fitting a slightly perturbed forward model\n$$\nf_{\\gamma}(x) = \\begin{bmatrix}\n\\alpha \\left( \\exp(x_1 + x_2) - 1 \\right) \\\\\n\\beta \\sin(x_1 + x_2) + \\gamma (x_1 - x_2)\n\\end{bmatrix},\n$$\nwith $\\gamma = 10^{-3}$, to the same data $y$ generated above (which came from $\\gamma = 0$). This mimics a structural constraint in the forward map that injects curvature along the previously flat direction. Run the unregularized Gauss-Newton solver for the two initial guesses $x^{(0)}_{\\mathrm{B}}$ and $x^{(0)}_{\\mathrm{C}}$, and report the final values of $v = x_1 - x_2$.\n\nNumerical and algorithmic requirements:\n- The Gauss-Newton subproblem at each iteration must be derived based on linearization and solved as a linear least-squares problem. You must not use closed-form solutions that bypass the principle of linearization-plus-least-squares.\n- Use a simple backtracking line search to ensure monotonic decrease of the objective if needed. Stop when the step norm falls below a small tolerance or a maximum number of iterations is reached.\n- There are no physical units in this problem.\n- Test suite:\n  - Case $1$: Unregularized Gauss-Newton, $\\gamma = 0$, starting at $x^{(0)}_{\\mathrm{A}}$; output the final $v$.\n  - Case $2$: Unregularized Gauss-Newton, $\\gamma = 0$, starting at $x^{(0)}_{\\mathrm{B}}$; output the final $v$.\n  - Case $3$: Unregularized Gauss-Newton, $\\gamma = 0$, starting at $x^{(0)}_{\\mathrm{C}}$; output the final $v$.\n  - Case $4$: Regularized Gauss-Newton with prior, $\\gamma = 0$, starting at $x^{(0)}_{\\mathrm{A}}$; output the final $v$.\n  - Case $5$: Regularized Gauss-Newton with prior, $\\gamma = 0$, starting at $x^{(0)}_{\\mathrm{B}}$; output the final $v$.\n  - Case $6$: Regularized Gauss-Newton with prior, $\\gamma = 0$, starting at $x^{(0)}_{\\mathrm{C}}$; output the final $v$.\n  - Case $7$: Unregularized Gauss-Newton with perturbed forward model $f_{\\gamma}$, $\\gamma = 10^{-3}$, starting at $x^{(0)}_{\\mathrm{B}}$; output the final $v$.\n  - Case $8$: Unregularized Gauss-Newton with perturbed forward model $f_{\\gamma}$, $\\gamma = 10^{-3}$, starting at $x^{(0)}_{\\mathrm{C}}$; output the final $v$.\n- Final output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the exact order of the cases above, for example $[v_1,v_2,v_3,v_4,v_5,v_6,v_7,v_8]$. Express each $v$ as a floating-point number.\n\nThe final answer of this problem must be a complete, runnable program that implements the described solvers and prints the required single-line output. No user input is required. Use radians for all trigonometric evaluations.", "solution": "The problem requires the derivation and implementation of the Gauss-Newton method to solve a nonlinear least-squares problem characterized by a structural non-identifiability, and to demonstrate how regularization can resolve this issue.\n\n### Part 1: Derivation of the Gauss-Newton Method\n\nThe objective is to find a parameter vector $x$ that minimizes the sum of squared residuals, given by the cost function:\n$$\nF(x) = \\frac{1}{2} \\|r(x)\\|_2^2 = \\frac{1}{2} (f(x) - y)^T (f(x) - y)\n$$\nThe Gauss-Newton method is an iterative algorithm that, at each iteration $k$, approximates the nonlinear residual function $r(x)$ with a linear one around the current estimate $x^{(k)}$. Let the proposed update be $\\delta$, such that the next estimate is $x^{(k+1)} = x^{(k)} + \\delta$. A first-order Taylor expansion of the residual vector $r(x)$ around $x^{(k)}$ is:\n$$\nr(x^{(k)} + \\delta) \\approx r(x^{(k)}) + J(x^{(k)}) \\delta\n$$\nwhere $J(x^{(k)})$ is the Jacobian matrix of the residual function $r(x)$ evaluated at $x^{(k)}$. Since $r(x) = f(x) - y$ and $y$ is a constant vector, the Jacobian of $r(x)$ is identical to the Jacobian of the forward model $f(x)$.\n\nSubstituting this linear approximation into the cost function $F(x)$, we obtain a quadratic model of the cost function around $x^{(k)}$:\n$$\nF(x^{(k)} + \\delta) \\approx \\frac{1}{2} \\|r(x^{(k)}) + J(x^{(k)}) \\delta\\|_2^2\n$$\nThe Gauss-Newton method determines the step $\\delta$ by minimizing this quadratic approximation at each iteration. This is a linear least-squares problem:\n$$\n\\delta^{(k)} = \\arg \\min_{\\delta} \\frac{1}{2} \\| J(x^{(k)}) \\delta - (-r(x^{(k)})) \\|_2^2\n$$\nThe solution to this standard linear least-squares problem is given by the normal equations:\n$$\n(J(x^{(k)})^T J(x^{(k)})) \\delta^{(k)} = -J(x^{(k)})^T r(x^{(k)})\n$$\nThe matrix $H_{GN}(x^{(k)}) = J(x^{(k)})^T J(x^{(k)})$ is the Gauss-Newton approximation of the Hessian of the cost function $F(x)$. The iterative update is thus given by $x^{(k+1)} = x^{(k)} + \\delta^{(k)}$, where $\\delta^{(k)}$ is the solution to the normal equations. A line search is often employed to determine a step size $\\eta^{(k)}$ such that $x^{(k+1)} = x^{(k)} + \\eta^{(k)} \\delta^{(k)}$ ensures a decrease in the cost function $F(x)$.\n\n### Part 2: Analysis of the Identifiability Ridge\n\nThe forward model is given by:\n$$\nf(x) = \\begin{bmatrix}\n\\alpha \\left( \\exp(x_1 + x_2) - 1 \\right) \\\\\n\\beta \\sin(x_1 + x_2)\n\\end{bmatrix}\n$$\nThis model depends only on the sum $s = x_1 + x_2$. The cost function $F(x) = \\frac{1}{2}\\|f(x_1, x_2) - y\\|_2^2$ will therefore be constant along any line where $x_1 + x_2$ is constant. This creates a \"ridge\" or valley of minimal cost. Perturbing $x$ in a direction that keeps $s$ constant does not change the value of $f(x)$ and hence does not change the cost. Such a direction is given by any vector orthogonal to the gradient of $s(x) = x_1 + x_2$, which is $\\nabla s = [1, 1]^T$. A basis for this direction is the vector $u = [1, -1]^T$. This direction corresponds to changing $v = x_1 - x_2$ while keeping $s$ fixed.\n\nThis structural non-identifiability manifests in the Gauss-Newton Hessian. The Jacobian of $f(x)$ is calculated using the chain rule: $J(x) = \\frac{\\partial f}{\\partial s} \\frac{\\partial s}{\\partial x}$.\n$$\n\\frac{\\partial f}{\\partial s} = \\begin{bmatrix} \\alpha \\exp(s) \\\\ \\beta \\cos(s) \\end{bmatrix}, \\quad \\frac{\\partial s}{\\partial x} = \\begin{bmatrix} 1 & 1 \\end{bmatrix}\n$$\nThus, the Jacobian is:\n$$\nJ(x) = \\begin{bmatrix} \\alpha \\exp(x_1+x_2) \\\\ \\beta \\cos(x_1+x_2) \\end{bmatrix} \\begin{bmatrix} 1 & 1 \\end{bmatrix} = \\begin{bmatrix} \\alpha e^{x_1+x_2} & \\alpha e^{x_1+x_2} \\\\ \\beta \\cos(x_1+x_2) & \\beta \\cos(x_1+x_2) \\end{bmatrix}\n$$\nThe two columns of $J(x)$ are identical, which means $J(x)$ has a rank of $1$. Its null space is spanned by the vector $u = [1, -1]^T$, since $J(x)u = 0$.\n\nThe Gauss-Newton Hessian is $H_{GN} = J(x)^T J(x)$. For any vector $u$ in the null space of $J(x)$:\n$$\nH_{GN} u = J(x)^T J(x) u = J(x)^T (0) = 0\n$$\nThis shows that $H_{GN}$ has a zero eigenvalue, corresponding to the eigenvector $u = [1, -1]^T$. The Hessian is singular. As a result, the normal equations $(J^T J) \\delta = -J^T r$ do not have a unique solution for the step $\\delta$. The component of $\\delta$ in the direction of the null space is undetermined, leading to drift along the identifiability ridge during the optimization.\n\n### Part 3: Curvature Injection\n\nTo obtain a unique solution, we must introduce information that distinguishes between points on the identifiability ridge. This can be done by adding curvature to the objective function in the deficient direction.\n\n**Method 1: Quadratic Prior (Tikhonov Regularization)**\n\nWe modify the cost function to include a penalty term:\n$$\n\\Phi(x) = \\frac{1}{2} \\| r(x) \\|_2^2 + \\frac{\\lambda}{2} \\| L(x - x_{\\mathrm{ref}}) \\|_2^2\n$$\nwhere $\\lambda > 0$ is a regularization weight, $L \\in \\mathbb{R}^{1 \\times 2}$ is a penalty operator, and $x_{\\mathrm{ref}}$ is a reference state. With $L = [1, -1]$ and $x_{\\mathrm{ref}} = 0$, the penalty term becomes $\\frac{\\lambda}{2} (x_1 - x_2)^2$. This term penalizes solutions where $v = x_1 - x_2$ is far from $0$, effectively selecting the solution on the ridge that is closest to the line $x_1 = x_2$.\n\nTo derive the modified Gauss-Newton step, we linearize both terms in $\\Phi(x)$ around $x^{(k)}$:\n$$\n\\Phi(x^{(k)} + \\delta) \\approx \\frac{1}{2} \\|r(x^{(k)}) + J(x^{(k)}) \\delta\\|_2^2 + \\frac{\\lambda}{2} \\|L(x^{(k)} - x_{\\mathrm{ref}}) + L \\delta\\|_2^2\n$$\nThis can be formulated as a single linear least-squares problem by defining an augmented residual and Jacobian:\n$$\n\\tilde{r}(x) = \\begin{bmatrix} r(x) \\\\ \\sqrt{\\lambda} L(x - x_{\\mathrm{ref}}) \\end{bmatrix}, \\quad \\tilde{J}(x) = \\begin{bmatrix} J(x) \\\\ \\sqrt{\\lambda} L \\end{bmatrix}\n$$\nThe step $\\delta^{(k)}$ minimizes $\\frac{1}{2} \\|\\tilde{r}(x^{(k)}) + \\tilde{J}(x^{(k)}) \\delta\\|_2^2$. The corresponding normal equations are $(\\tilde{J}^T \\tilde{J}) \\delta = -\\tilde{J}^T \\tilde{r}$. Expanding this yields:\n$$\n(J(x^{(k)})^T J(x^{(k)}) + \\lambda L^T L) \\delta^{(k)} = -(J(x^{(k)})^T r(x^{(k)}) + \\lambda L^T L(x^{(k)} - x_{\\mathrm{ref}}))\n$$\nThe new approximate Hessian is $H_{GN} + \\lambda L^T L$. The matrix $L^T L = \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix} [1, -1] = \\begin{bmatrix} 1 & -1 \\\\ -1 & 1 \\end{bmatrix}$ has a null space spanned by $[1, 1]^T$, which is orthogonal to the null space of $H_{GN}$. The sum of the two matrices is therefore positive definite and invertible, yielding a unique, well-defined step $\\delta^{(k)}$ that resolves the non-identifiability.\n\n**Method 2: Structural Constraint in the Forward Model**\n\nAn alternative is to modify the forward model to break the symmetry, for example:\n$$\nf_{\\gamma}(x) = \\begin{bmatrix}\n\\alpha \\left( \\exp(x_1 + x_2) - 1 \\right) \\\\\n\\beta \\sin(x_1 + x_2) + \\gamma (x_1 - x_2)\n\\end{bmatrix}\n$$\nFor a small $\\gamma \\neq 0$, the model now weakly depends on $v = x_1 - x_2$. The new Jacobian is:\n$$\nJ_{\\gamma}(x) = \\begin{bmatrix} \\alpha e^{x_1+x_2} & \\alpha e^{x_1+x_2} \\\\ \\beta \\cos(x_1+x_2) + \\gamma & \\beta \\cos(x_1+x_2) - \\gamma \\end{bmatrix}\n$$\nThe columns of $J_{\\gamma}(x)$ are now linearly independent. The vector $u = [1, -1]^T$ is no longer in the null space:\n$$\nJ_{\\gamma}(x) u = \\begin{bmatrix} 0 \\\\ 2\\gamma \\end{bmatrix} \\neq 0\n$$\nSince $J_{\\gamma}(x)$ has full rank, the Gauss-Newton Hessian $J_{\\gamma}(x)^T J_{\\gamma}(x)$ is non-singular, and the standard (unregularized) Gauss-Newton method will converge to a unique minimum. This demonstrates that even a slight structural dependence on the previously non-identifiable parameter combination is sufficient to regularize the problem.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves a nonlinear least-squares problem with an identifiability ridge\n    using Gauss-Newton, and demonstrates regularization techniques.\n    \"\"\"\n    # Problem Constants\n    alpha = 1.0\n    beta = 0.5\n    x_star = np.array([0.5, -0.3])\n    \n    # Generate synthetic data y = f(x_star)\n    s_star = x_star[0] + x_star[1]\n    y_obs = np.array([\n        alpha * (np.exp(s_star) - 1.0),\n        beta * np.sin(s_star)\n    ])\n\n    # Initial guesses\n    x0_A = np.array([1.6, -1.0])\n    x0_B = np.array([0.7, -0.5])\n    x0_C = np.array([-0.5, 0.7])\n    \n    # Regularization parameters\n    lmbda_reg = 1.0\n    L_reg = np.array([[1.0, -1.0]])\n    xref_reg = np.array([0.0, 0.0])\n    \n    # Structural perturbation parameter\n    gamma_pert = 1e-3\n\n    # Algorithmic parameters\n    max_iter = 100\n    tol = 1e-8\n\n    def gauss_newton_solver(x_init, gamma, lmbda, L, xref):\n        \"\"\"\n        Generic Gauss-Newton solver for the problem.\n        - gamma: structural perturbation parameter\n        - lmbda: Tikhonov regularization weight\n        - L, xref: Tikhonov operator and reference\n        \"\"\"\n        x = x_init.copy()\n        \n        for _ in range(max_iter):\n            s = x[0] + x[1]\n            v = x[0] - x[1]\n\n            # Forward model f(x)\n            f_val = np.array([\n                alpha * (np.exp(s) - 1.0),\n                beta * np.sin(s) + gamma * v\n            ])\n            \n            # Residual r(x)\n            r = f_val - y_obs\n\n            # Jacobian J(x)\n            J = np.array([\n                [alpha * np.exp(s), alpha * np.exp(s)],\n                [beta * np.cos(s) + gamma, beta * np.cos(s) - gamma]\n            ])\n\n            if lmbda == 0:\n                # Standard or structurally-regularized GN\n                # Solve the linear least squares problem: min ||J*delta - (-r)||^2\n                delta, _, _, _ = np.linalg.lstsq(J, -r, rcond=None)\n            else:\n                # Tikhonov-regularized GN (as augmented least squares)\n                # min || [ J_aug * delta - (-r_aug) ] ||^2\n                r_prior = np.sqrt(lmbda) * L @ (x - xref)\n                r_aug = np.concatenate([r, r_prior])\n                J_aug = np.vstack([J, np.sqrt(lmbda) * L])\n                delta, _, _, _ = np.linalg.lstsq(J_aug, -r_aug, rcond=None)\n\n            if np.linalg.norm(delta) < tol:\n                break\n                \n            # Backtracking line search\n            step_size = 1.0\n            def cost_func(xk):\n                sk = xk[0] + xk[1]\n                vk = xk[0] - xk[1]\n                fk = np.array([\n                    alpha * (np.exp(sk) - 1.0),\n                    beta * np.sin(sk) + gamma * vk\n                ])\n                res_cost = 0.5 * np.linalg.norm(fk - y_obs)**2\n                if lmbda > 0:\n                    reg_cost = (lmbda / 2.0) * np.linalg.norm(L @ (xk - xref))**2\n                    return res_cost + reg_cost\n                return res_cost\n\n            cost_current = cost_func(x)\n            while step_size > 1e-8:\n                x_new = x + step_size * delta\n                if cost_func(x_new) < cost_current:\n                    x = x_new\n                    break\n                step_size /= 2.0\n            else: # Line search failed to find a decrease\n                break\n        \n        return x[0] - x[1] # Return the final v\n\n    # Test Cases\n    results = []\n\n    # Cases 1-3: Unregularized GN (gamma=0, lambda=0)\n    v1 = gauss_newton_solver(x0_A, gamma=0.0, lmbda=0.0, L=L_reg, xref=xref_reg)\n    results.append(v1)\n    \n    v2 = gauss_newton_solver(x0_B, gamma=0.0, lmbda=0.0, L=L_reg, xref=xref_reg)\n    results.append(v2)\n    \n    v3 = gauss_newton_solver(x0_C, gamma=0.0, lmbda=0.0, L=L_reg, xref=xref_reg)\n    results.append(v3)\n    \n    # Cases 4-6: Regularized GN with prior (gamma=0, lambda=1)\n    v4 = gauss_newton_solver(x0_A, gamma=0.0, lmbda=lmbda_reg, L=L_reg, xref=xref_reg)\n    results.append(v4)\n    \n    v5 = gauss_newton_solver(x0_B, gamma=0.0, lmbda=lmbda_reg, L=L_reg, xref=xref_reg)\n    results.append(v5)\n    \n    v6 = gauss_newton_solver(x0_C, gamma=0.0, lmbda=lmbda_reg, L=L_reg, xref=xref_reg)\n    results.append(v6)\n\n    # Cases 7-8: Unregularized GN with perturbed model (gamma=1e-3, lambda=0)\n    v7 = gauss_newton_solver(x0_B, gamma=gamma_pert, lmbda=0.0, L=L_reg, xref=xref_reg)\n    results.append(v7)\n    \n    v8 = gauss_newton_solver(x0_C, gamma=gamma_pert, lmbda=0.0, L=L_reg, xref=xref_reg)\n    results.append(v8)\n\n    # Final output format\n    print(f\"[{','.join(f'{r:.6f}' for r in results)}]\")\n\nsolve()\n```", "id": "3384262"}, {"introduction": "Beyond ill-posedness, a primary challenge for local optimizers is non-convexity, where numerous local minima can trap the algorithm far from the true solution. This exercise simulates the classic problem of \"cycle skipping\" in seismic waveform inversion, demonstrating how a standard least-squares objective function comparing oscillatory signals creates a treacherous, multi-valleyed landscape. By contrasting the failing Gauss-Newton step with one derived from a smoother, envelope-based misfit, you will learn that the most effective strategy is often to reformulate the problem with a more suitable objective function [@problem_id:3599323].", "problem": "You are to implement a complete, runnable program that constructs a one-dimensional waveform inversion toy model in which the forward modeling operator generates a single reflected pulse as a time-shifted Ricker wavelet. The inversion target is a single unknown parameter, the constant acoustic velocity, and the data are synthetic seismograms measured at the surface. The goal is to examine the behavior of the Gauss–Newton method for Nonlinear Least Squares (NLS) under a standard amplitude-based least-squares residual and under an alternative envelope-based misfit, illustrating cycle skipping in the former and showing the change in the Gauss–Newton step in the latter.\n\nUse the following fundamental setup and definitions:\n\n- The observed data are acquired over a single, flat subsurface reflector at depth $z$, in a constant-velocity medium of unknown velocity $m$ (the model variable). The two-way travel time is $ \\tau(m) = \\dfrac{2 z}{m} $.\n- The source wavelet is the Ricker wavelet $ s(t) = \\left(1 - 2 a t^2\\right) \\exp\\left(- a t^2\\right) $, with $ a = \\left(\\pi f_0\\right)^2 $, where $ f_0 $ is the central frequency.\n- The noise-free forward model is the synthetic seismogram $ d\\!\\left(t; m\\right) = s\\!\\left(t - \\tau(m)\\right) $.\n- The amplitude-based least-squares objective is $ \\Phi_{\\mathrm{wf}}(m) = \\dfrac{1}{2} \\left\\| r_{\\mathrm{wf}}(m) \\right\\|_2^2 $ with residual $ r_{\\mathrm{wf}}(t; m) = d_{\\mathrm{obs}}(t) - d\\!\\left(t; m\\right) $.\n- The envelope-based alternative misfit uses the amplitude envelope $ e\\!\\left(t; m\\right) = \\sqrt{ d\\!\\left(t; m\\right)^2 + \\left( \\mathcal{H}\\{ d\\!\\left(t; m\\right) \\} \\right)^2 } $, where $ \\mathcal{H}\\{\\cdot\\} $ denotes the Hilbert transform, and the objective $ \\Phi_{\\mathrm{env}}(m) = \\dfrac{1}{2} \\left\\| r_{\\mathrm{env}}(m) \\right\\|_2^2 $ with residual $ r_{\\mathrm{env}}(t; m) = e_{\\mathrm{obs}}(t) - e\\!\\left(t; m\\right) $.\n\nFrom first principles, derive the Gauss–Newton step for each objective:\n\n- Start from the definition of the Nonlinear Least Squares objective $ \\Phi(m) = \\dfrac{1}{2} \\left\\| r(m) \\right\\|_2^2 $ and the Gauss–Newton approximation that replaces the exact Hessian with $ J(m)^\\top J(m) $, where $ J(m) $ is the Jacobian of the residual with respect to $ m $.\n- For the waveform residual, derive the Jacobian entry $ J_{\\mathrm{wf}}(t; m) = \\dfrac{\\partial}{\\partial m} d\\!\\left(t; m\\right) $, using the chain rule and the derivative of the Ricker wavelet. The derivative of the Ricker wavelet is $ s'(t) = \\exp\\!\\left(- a t^2\\right) \\, t \\left( - 6 a + 4 a^2 t^2 \\right) $. Provide $ J_{\\mathrm{wf}}(t; m) $ explicitly in terms of $ z $, $ m $, and $ s'\\!\\big(t - \\tau(m)\\big) $.\n- For the envelope residual, use the fact that $ \\mathcal{H}\\{\\cdot\\} $ is a linear operator to derive the Jacobian entry $ J_{\\mathrm{env}}(t; m) = \\dfrac{\\partial}{\\partial m} e\\!\\left(t; m\\right) $ via the chain rule: $ e(t; m) = \\sqrt{x(t; m)^2 + y(t; m)^2} $ with $ x = d(t; m) $ and $ y = \\mathcal{H}\\{ d(t; m) \\} $. Show that $ \\dfrac{\\partial e}{\\partial m}(t; m) = \\dfrac{ x \\, \\dfrac{\\partial x}{\\partial m} + y \\, \\dfrac{\\partial y}{\\partial m} }{ e } $ and relate $ \\dfrac{\\partial y}{\\partial m} $ to $ \\mathcal{H}\\!\\left\\{ \\dfrac{\\partial x}{\\partial m} \\right\\} $.\n\nThen, for each objective, write the Gauss–Newton update for the single parameter $ m $:\n$$\n\\Delta m_{\\bullet} = \\dfrac{ \\sum_t J_{\\bullet}(t; m_0) \\, r_{\\bullet}(t; m_0) }{ \\sum_t J_{\\bullet}(t; m_0)^2 },\n$$\nwhere $ \\bullet \\in \\{ \\mathrm{wf}, \\mathrm{env} \\} $ and $ m_0 $ is the current model iterate. Explain why, when $ m_0 $ is sufficiently far from the true value, the waveform-based residual $ r_{\\mathrm{wf}} $ can exhibit cycle skipping, which manifests as a multi-cycle phase mismatch causing $ J_{\\mathrm{wf}}^\\top r_{\\mathrm{wf}} $ to have misleading sign or small magnitude, yielding a poor $ \\Delta m_{\\mathrm{wf}} $. Contrast this with the envelope-based residual, which suppresses oscillatory phase effects, often providing a more robust step $ \\Delta m_{\\mathrm{env}} $ toward the true solution.\n\nPhysical and numerical parameters:\n\n- Depth $ z = 1000\\,\\mathrm{m} $.\n- True velocity $ m_\\star = 2000\\,\\mathrm{m/s} $.\n- Central frequency $ f_0 = 10\\,\\mathrm{Hz} $.\n- Time sampling interval $ \\Delta t = 0.001\\,\\mathrm{s} $.\n- Record length $ T = 2.5\\,\\mathrm{s} $.\n\nAngles, including any phase that may appear implicitly in the analytic signal, are to be considered in radians.\n\nTest suite of initial model velocities $ m_0 $:\n\n- Case $1$ (perfect match): $ m_0 = 2000\\,\\mathrm{m/s} $.\n- Case $2$ (near, below half-cycle mismatch): $ m_0 = 2100\\,\\mathrm{m/s} $.\n- Case $3$ (moderate cycle skip, around one period of mismatch): $ m_0 = 1800\\,\\mathrm{m/s} $.\n- Case $4$ (severe cycle skip, multiple periods): $ m_0 = 1500\\,\\mathrm{m/s} $.\n\nYour program must:\n\n- Construct $ d_{\\mathrm{obs}}(t) $ using $ m_\\star $ and compute, for each $ m_0 $ in the test suite, both $ \\Delta m_{\\mathrm{wf}} $ and $ \\Delta m_{\\mathrm{env}} $ according to the Gauss–Newton formulas above.\n- Use the Hilbert transform to compute the envelope and the envelope Jacobian, with appropriate numerical safeguards to avoid division by zero when the envelope amplitude is extremely small.\n- Express each $ \\Delta m $ in meters per second ($\\mathrm{m/s}$) and round to $6$ decimal places.\n\nFinal output format:\n\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element of this list corresponds to one test case and must be itself a two-element list containing the Gauss–Newton steps $ \\Delta m_{\\mathrm{wf}} $ and $ \\Delta m_{\\mathrm{env}} $ for that case, in that order. For example, the output should look like $ \\left[ [\\Delta m_{\\mathrm{wf}}^{(1)}, \\Delta m_{\\mathrm{env}}^{(1)}], \\ldots, [\\Delta m_{\\mathrm{wf}}^{(4)}, \\Delta m_{\\mathrm{env}}^{(4)}] \\right] $ where each number is printed with $6$ decimal places and implicitly in $\\mathrm{m/s}$.", "solution": "The user has provided a valid problem statement. The task is to analyze the Gauss-Newton optimization method for a one-dimensional waveform inversion problem, comparing a standard waveform-based objective function with an envelope-based alternative. The analysis focuses on the phenomenon of cycle skipping. The solution requires deriving the Gauss-Newton step for each objective function and then implementing a numerical simulation to compute these steps for several initial models.\n\nFirst, we establish the theoretical framework. The goal is to find the model parameter $m$ (acoustic velocity) that minimizes a nonlinear least-squares objective function of the form $\\Phi(m) = \\frac{1}{2} \\|r(m)\\|_2^2$, where $r(m)$ is the residual vector between observed and predicted data. For a discrete time series, this is $\\Phi(m) = \\frac{1}{2} \\sum_t [r(t; m)]^2$.\n\nThe Gauss-Newton method solves for an update step $\\delta m$ by linearizing the residual and minimizing the resulting quadratic objective. The standard update is $m_{k+1} = m_k + \\delta m$, where the step $\\delta m$ is the solution to the normal equations $(J_r^\\top J_r) \\delta m = -J_r^\\top r$, and $J_r$ is the Jacobian of the residual $r$.\n\nLet's define the Jacobian of the *forward model* $d$ as $J_d = \\frac{\\partial d}{\\partial m}$. Since the residual is $r(m) = d_{\\mathrm{obs}} - d(m)$, its Jacobian is $J_r = \\frac{\\partial r}{\\partial m} = -J_d$. Substituting these into the normal equations gives:\n$$\n((-J_d)^\\top (-J_d)) \\delta m = -(-J_d)^\\top r \\implies (J_d^\\top J_d) \\delta m = J_d^\\top r\n$$\nFor a single parameter $m$, this becomes $( \\sum_t J_d(t;m)^2 ) \\delta m = \\sum_t J_d(t;m) r(t;m)$. Solving for the step $\\delta m$ yields:\n$$\n\\delta m = \\frac{ \\sum_t J_d(t; m) r(t; m) }{ \\sum_t J_d(t; m)^2 }\n$$\nThis exactly matches the formula for $\\Delta m$ provided in the problem statement, with the understanding that the Jacobian $J_\\bullet$ in the prompt refers to the sensitivity of the forward model, not the residual. Therefore, the calculated $\\Delta m$ is the correct step to add to the current model: $m_{k+1} = m_k + \\Delta m$.\n\nThe forward model for the synthetic data is a time-shifted Ricker wavelet: $d(t; m) = s(t - \\tau(m))$, where $s(t) = (1 - 2 a t^2) \\exp(-a t^2)$ with $a = (\\pi f_0)^2$, and the two-way travel time is $\\tau(m) = \\frac{2z}{m}$.\n\n**1. Waveform-Based Objective $\\Phi_{\\mathrm{wf}}(m)$**\n\nThe residual is $r_{\\mathrm{wf}}(t; m) = d_{\\mathrm{obs}}(t) - d(t; m)$. The Jacobian of the forward model $d(t;m)$ with respect to $m$ is derived using the chain rule:\n$$\n\\frac{\\partial d(t; m)}{\\partial m} = \\frac{\\partial}{\\partial m} s(t - \\tau(m)) = s'(t - \\tau(m)) \\cdot \\left( -\\frac{\\partial \\tau(m)}{\\partial m} \\right)\n$$\nThe derivative of the travel time is $\\frac{\\partial \\tau(m)}{\\partial m} = \\frac{\\partial}{\\partial m}\\left(\\frac{2z}{m}\\right) = -\\frac{2z}{m^2}$.\nSubstituting this in, we get:\n$$\n\\frac{\\partial d(t; m)}{\\partial m} = s'(t - \\tau(m)) \\cdot \\left( - \\left( -\\frac{2z}{m^2} \\right) \\right) = \\frac{2z}{m^2} s'(t - \\tau(m))\n$$\nThis gives the Jacobian of the forward model, which we denote $J_{\\mathrm{wf}}(t; m)$:\n$$\nJ_{\\mathrm{wf}}(t; m) = \\frac{2z}{m^2} s'(t - \\tau(m))\n$$\nwhere the derivative of the Ricker wavelet is given as $s'(t) = \\exp(- a t^2) \\, t \\, (-6 a + 4 a^2 t^2)$.\nThe Gauss-Newton step is then:\n$$\n\\Delta m_{\\mathrm{wf}} = \\frac{\\sum_t J_{\\mathrm{wf}}(t; m_0) \\, r_{\\mathrm{wf}}(t; m_0)}{\\sum_t J_{\\mathrm{wf}}(t; m_0)^2}\n$$\n\n**2. Envelope-Based Objective $\\Phi_{\\mathrm{env}}(m)$**\n\nThe residual is $r_{\\mathrm{env}}(t; m) = e_{\\mathrm{obs}}(t) - e(t; m)$, where $e(t; m)$ is the amplitude envelope of the signal $d(t; m)$, defined as $e(t; m) = |d(t;m) + i \\mathcal{H}\\{d(t;m)\\}|$, with $\\mathcal{H}\\{\\cdot\\}$ denoting the Hilbert transform.\nLet $x(t; m) = d(t; m)$ and $y(t; m) = \\mathcal{H}\\{d(t; m)\\}$. Then $e(t; m) = \\sqrt{x^2 + y^2}$. The Jacobian of the forward model $e(t; m)$ is:\n$$\nJ_{\\mathrm{env}}(t; m) = \\frac{\\partial e}{\\partial m} = \\frac{1}{2\\sqrt{x^2+y^2}} \\left( 2x \\frac{\\partial x}{\\partial m} + 2y \\frac{\\partial y}{\\partial m} \\right) = \\frac{x \\frac{\\partial x}{\\partial m} + y \\frac{\\partial y}{\\partial m}}{e}\n$$\nWe have $\\frac{\\partial x}{\\partial m} = \\frac{\\partial d}{\\partial m} = J_{\\mathrm{wf}}(t; m)$. Since the Hilbert transform is a linear operator, its application commutes with differentiation with respect to $m$:\n$$\n\\frac{\\partial y}{\\partial m} = \\frac{\\partial}{\\partial m} \\mathcal{H}\\{d(t; m)\\} = \\mathcal{H}\\left\\{\\frac{\\partial d(t; m)}{\\partial m}\\right\\} = \\mathcal{H}\\{J_{\\mathrm{wf}}(t; m)\\}\n$$\nSubstituting these results, we get the explicit formula for the envelope Jacobian:\n$$\nJ_{\\mathrm{env}}(t; m) = \\frac{d(t; m) J_{\\mathrm{wf}}(t; m) + \\mathcal{H}\\{d(t; m)\\} \\mathcal{H}\\{J_{\\mathrm{wf}}(t; m)\\}}{e(t; m)}\n$$\nThe Gauss-Newton step for the envelope objective is therefore:\n$$\n\\Delta m_{\\mathrm{env}} = \\frac{\\sum_t J_{\\mathrm{env}}(t; m_0) \\, r_{\\mathrm{env}}(t; m_0)}{\\sum_t J_{\\mathrm{env}}(t; m_0)^2}\n$$\n\n**3. Cycle Skipping Analysis**\n\nThe waveform-based objective $\\Phi_{\\mathrm{wf}}$ is highly oscillatory as a function of the model parameter $m$, exhibiting numerous local minima. These minima arise when the predicted wavelet $d(t; m_0)$ is misaligned with the observed data $d_{\\mathrm{obs}}(t)$ by an integer multiple of the wavelet's half-period. The numerator of the $\\Delta m_{\\mathrm{wf}}$ formula, $\\sum_t J_{\\mathrm{wf}} r_{\\mathrm{wf}}$, represents the cross-correlation of the residual with the Jacobian. When the initial guess $m_0$ is far from the true value $m_\\star$, the time-shift $\\tau(m_0) - \\tau(m_\\star)$ can be large. If this shift exceeds approximately half the dominant period of the wavelet, the correlation can become small or even switch sign. A sign switch leads to a Gauss-Newton step in the wrong direction, away from the true solution. This failure to find the correct minimum is known as cycle skipping.\n\nIn contrast, the envelope $e(t; m)$ is a smooth, non-oscillatory function of time, whose maximum is located at the group arrival time $\\tau(m)$. The envelope-based objective $\\Phi_{\\mathrm{env}}$ is consequently much smoother and more convex over a wider range of $m$ values. Its basin of attraction around the global minimum is larger, making the optimization less susceptible to cycle skipping. The residual $r_{\\mathrm{env}}$ captures the mismatch in arrival time without the oscillatory interference, and its correlation with its Jacobian $J_{\\mathrm{env}}$ generally provides a robust update direction even for large initial errors in $m_0$.\n\nThe following program implements these calculations for the specified test cases, demonstrating the robustness of the envelope misfit compared to the waveform misfit. A small constant $\\epsilon$ is added to the envelope in the denominator of $J_{\\mathrm{env}}$ to ensure numerical stability.", "answer": "```python\nimport numpy as np\nfrom scipy.signal import hilbert\n\ndef solve():\n    \"\"\"\n    Computes and compares Gauss-Newton steps for waveform and envelope\n    misfit functions in a 1D seismic inversion toy problem.\n    \"\"\"\n    # Physical and numerical parameters\n    z = 1000.0  # Reflector depth in meters\n    m_true = 2000.0  # True velocity in m/s\n    f0 = 10.0  # Central frequency in Hz\n    dt = 0.001  # Time sampling interval in seconds\n    t_max = 2.5  # Record length in seconds\n    epsilon = 1e-9 # Small constant for numerical stability\n\n    # Time vector\n    t = np.arange(0, t_max, dt)\n\n    # Ricker wavelet and its derivative\n    a = (np.pi * f0)**2\n    def ricker(time_vec):\n        return (1.0 - 2.0 * a * time_vec**2) * np.exp(-a * time_vec**2)\n\n    def ricker_derivative(time_vec):\n        return np.exp(-a * time_vec**2) * time_vec * (-6.0 * a + 4.0 * a**2 * time_vec**2)\n\n    # Forward modeling operator\n    def forward_model(m, time_vec):\n        tau = 2.0 * z / m\n        return ricker(time_vec - tau)\n\n    # Generate observed data\n    d_obs = forward_model(m_true, t)\n\n    # Test cases for initial model velocities\n    test_cases = [2000.0, 2100.0, 1800.0, 1500.0]\n\n    results = []\n    \n    for m0 in test_cases:\n        # Calculate predicted data for the current model parameter m0\n        d_pred = forward_model(m0, t)\n\n        # --- 1. Waveform-based inversion step ---\n        r_wf = d_obs - d_pred\n        \n        # Calculate Jacobian for waveform misfit\n        tau0 = 2.0 * z / m0\n        # The Jacobian is d(d)/dm = s'(t-tau) * d(-tau)/dm = s'(t-tau) * (2z/m^2)\n        J_wf = (2.0 * z / m0**2) * ricker_derivative(t - tau0)\n\n        # Calculate Gauss-Newton step for waveform\n        numerator_wf = np.sum(J_wf * r_wf)\n        denominator_wf = np.sum(J_wf**2)\n        \n        if np.isclose(denominator_wf, 0):\n            delta_m_wf = 0.0\n        else:\n            # This is the standard Gauss-Newton step, delta_m = (J^T J)^-1 J^T r,\n            # where J is the Jacobian of the forward model and r is the residual.\n            # The update would be m_new = m_old + delta_m.\n            delta_m_wf = numerator_wf / denominator_wf\n\n        # --- 2. Envelope-based inversion step ---\n        \n        # Calculate analytic signals and envelopes\n        analytic_obs = hilbert(d_obs)\n        analytic_pred = hilbert(d_pred)\n        \n        e_obs = np.abs(analytic_obs)\n        e_pred = np.abs(analytic_pred)\n\n        r_env = e_obs - e_pred\n        \n        # Calculate Jacobian for envelope misfit\n        # J_env = d(e)/dm = [d*d(d)/dm + H{d}*d(H{d})/dm] / e\n        #       = [d*J_wf + H{d}*H{J_wf}] / e\n        analytic_J_wf = hilbert(J_wf)\n        \n        # The numerator is the real part of (analytic_pred_conj * analytic_J_wf)\n        numerator_J_env = np.real(np.conj(analytic_pred) * analytic_J_wf)\n        J_env = np.divide(numerator_J_env, e_pred + epsilon, \n                          out=np.zeros_like(numerator_J_env), \n                          where=(e_pred + epsilon) != 0)\n\n        # Calculate Gauss-Newton step for envelope\n        numerator_env = np.sum(J_env * r_env)\n        denominator_env = np.sum(J_env**2)\n\n        if np.isclose(denominator_env, 0):\n            delta_m_env = 0.0\n        else:\n            delta_m_env = numerator_env / denominator_env\n\n        results.append([round(delta_m_wf, 6), round(delta_m_env, 6)])\n\n    # Format the final output string\n    output_str = \"[\" + \",\".join([f\"[{res[0]:.6f},{res[1]:.6f}]\" for res in results]) + \"]\"\n    print(output_str)\n\nsolve()\n```", "id": "3599323"}]}