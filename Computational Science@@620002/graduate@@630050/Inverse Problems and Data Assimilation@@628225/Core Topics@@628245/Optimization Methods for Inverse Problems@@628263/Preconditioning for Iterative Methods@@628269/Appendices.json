{"hands_on_practices": [{"introduction": "To build a strong foundation, we begin with a classic analytical exercise using the one-dimensional Poisson equation as a model problem. This practice involves deriving the exact condition number for the Jacobi-preconditioned system, linking the abstract theory of eigenvalues to a concrete prediction of iterative performance. Mastering this canonical example is key to understanding how preconditioners fundamentally alter an operator's spectral properties to accelerate convergence [@problem_id:3412964].", "problem": "Consider the symmetric positive definite matrix $A \\in \\mathbb{R}^{n \\times n}$ arising from the standard second-order centered finite-difference discretization of the one-dimensional Poisson equation $-u''(x) = f(x)$ on $[0,1]$ with homogeneous Dirichlet boundary conditions, using $n$ interior grid points and uniform spacing $h = 1/(n+1)$. Thus $A = \\frac{1}{h^{2}} T$, where $T$ is the tridiagonal Toeplitz matrix with $2$ on the main diagonal and $-1$ on the first sub- and super-diagonals. Let $M = \\operatorname{diag}(A)$ be the Jacobi preconditioner.\n\n1) Starting from first principles for this discretization, derive an exact, closed-form expression in $n$ for the spectral condition number $\\kappa(M^{-1}A)$, where $\\kappa(B) = \\lambda_{\\max}(B) / \\lambda_{\\min}(B)$ for any symmetric positive definite matrix $B$. Then specialize your result to the case $n = 127$, and keep the expression exact.\n\n2) Consider applying the Preconditioned Conjugate Gradient (PCG) method to the linear system $A x = b$ with the preconditioner $M$, starting from an arbitrary initial guess. Let the prescribed tolerance be $\\epsilon = 10^{-6}$ in the relative $A$-norm, i.e., the stopping requirement is $\\|x_{k} - x_{\\star}\\|_{A} / \\|x_{0} - x_{\\star}\\|_{A} \\le \\epsilon$, where $x_{\\star}$ is the exact solution and $\\|y\\|_{A} = \\sqrt{y^{\\top} A y}$ denotes the $A$-norm. Using only worst-case information derived from $\\kappa(M^{-1}A)$, predict the smallest integer number of PCG iterations $k$ that guarantees this tolerance. Express your answer as the exact ceiling of a closed-form expression and then specialize it to $n = 127$ and $\\epsilon = 10^{-6}$.\n\nExpress the final answer as a row matrix with two entries: first, the closed-form expression for $\\kappa(M^{-1}A)$ specialized to $n = 127$; second, the smallest integer iteration count $k$ for the tolerance $\\epsilon = 10^{-6}$ under the worst-case bound. No units are required. Do not round any non-integer quantity; if an integer is required, provide the exact integer.", "solution": "The problem is first validated to ensure it is scientifically sound, well-posed, objective, and complete.\n\n### Step 1: Extract Givens\n- Matrix $A \\in \\mathbb{R}^{n \\times n}$ is symmetric positive definite (SPD), from the centered finite-difference discretization of $-u''(x) = f(x)$ on $[0,1]$ with homogeneous Dirichlet boundary conditions.\n- There are $n$ interior grid points.\n- The grid spacing is uniform, $h = 1/(n+1)$.\n- $A = \\frac{1}{h^{2}} T$, where $T$ is the tridiagonal Toeplitz matrix with $2$ on the main diagonal and $-1$ on the first sub- and super-diagonals.\n- The preconditioner is the Jacobi preconditioner, $M = \\operatorname{diag}(A)$.\n- Part 1 requires the derivation of a closed-form expression for the spectral condition number $\\kappa(M^{-1}A) = \\lambda_{\\max}(M^{-1}A) / \\lambda_{\\min}(M^{-1}A)$, and its specialization for $n = 127$.\n- Part 2 requires predicting the smallest integer number of PCG iterations, $k$, to achieve a tolerance $\\epsilon = 10^{-6}$ based on the worst-case convergence bound.\n- The stopping criterion is $\\|x_{k} - x_{\\star}\\|_{A} / \\|x_{0} - x_{\\star}\\|_{A} \\le \\epsilon$, where $x_{\\star}$ is the exact solution and $\\|y\\|_{A} = \\sqrt{y^{\\top} A y}$ is the $A$-norm.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded:** The problem is a standard exercise in numerical linear algebra, involving the analysis of a preconditioned system arising from the discretization of a well-known partial differential equation (the 1D Poisson equation). All concepts are fundamental to numerical analysis.\n- **Well-Posed:** The problem is clearly defined. The matrix $A$ is known to be SPD, ensuring its diagonal elements are positive, which in turn means the Jacobi preconditioner $M$ is SPD and invertible. The eigenvalues of $M^{-1}A$ are real and positive, making the condition number well-defined. The PCG iteration count is based on a standard a priori error bound. A unique, meaningful solution exists.\n- **Objective:** The problem is stated using precise, unambiguous mathematical language.\n- **Completeness and Consistency:** All necessary definitions and values (e.g., the structure of $A$, the definition of $M$, the value of $n$ and $\\epsilon$ for specialization) are provided. There are no contradictions.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\n### Part 1: Derivation of the Condition Number $\\kappa(M^{-1}A)$\n\nThe matrix $A$ is given by $A = \\frac{1}{h^2} T$, where $T$ is the $n \\times n$ matrix:\n$$\nT = \\begin{pmatrix}\n2 & -1 & & & \\\\\n-1 & 2 & -1 & & \\\\\n& \\ddots & \\ddots & \\ddots & \\\\\n& & -1 & 2 & -1 \\\\\n& & & -1 & 2\n\\end{pmatrix}\n$$\nThe Jacobi preconditioner $M$ is the diagonal of $A$. Since every diagonal element of $T$ is $2$, every diagonal element of $A$ is $2/h^2$. Thus, $M$ is a diagonal matrix:\n$$\nM = \\operatorname{diag}(A) = \\frac{2}{h^2} I\n$$\nwhere $I$ is the $n \\times n$ identity matrix. The inverse of $M$ is:\n$$\nM^{-1} = \\frac{h^2}{2} I\n$$\nNow, we can form the preconditioned matrix $M^{-1}A$:\n$$\nM^{-1}A = \\left(\\frac{h^2}{2} I\\right) \\left(\\frac{1}{h^2} T\\right) = \\frac{1}{2} T\n$$\nThe eigenvalues of $M^{-1}A$ are therefore half the eigenvalues of $T$. The eigenvalues of the matrix $T$ are a standard result in numerical analysis. For an $n \\times n$ matrix of this form, the eigenvalues $\\mu_j$ are given by:\n$$\n\\mu_j = 2 - 2\\cos\\left(\\frac{j\\pi}{n+1}\\right) \\quad \\text{for } j = 1, 2, \\ldots, n\n$$\nUsing the half-angle identity $1 - \\cos(\\theta) = 2\\sin^2(\\theta/2)$, we can rewrite this as:\n$$\n\\mu_j = 4\\sin^2\\left(\\frac{j\\pi}{2(n+1)}\\right) \\quad \\text{for } j = 1, 2, \\ldots, n\n$$\nThe eigenvalues $\\lambda_j$ of $M^{-1}A = \\frac{1}{2}T$ are then:\n$$\n\\lambda_j = \\frac{1}{2}\\mu_j = 2\\sin^2\\left(\\frac{j\\pi}{2(n+1)}\\right) \\quad \\text{for } j = 1, 2, \\ldots, n\n$$\nThe sine function is strictly increasing on the interval $(0, \\pi/2)$. The arguments of the sine function, $\\frac{j\\pi}{2(n+1)}$, for $j=1, \\ldots, n$, all lie within this interval. Therefore, the minimum eigenvalue of $M^{-1}A$ corresponds to $j=1$, and the maximum eigenvalue corresponds to $j=n$.\n$$\n\\lambda_{\\min}(M^{-1}A) = \\lambda_1 = 2\\sin^2\\left(\\frac{\\pi}{2(n+1)}\\right)\n$$\n$$\n\\lambda_{\\max}(M^{-1}A) = \\lambda_n = 2\\sin^2\\left(\\frac{n\\pi}{2(n+1)}\\right)\n$$\nThe spectral condition number $\\kappa(M^{-1}A)$ is the ratio of the maximum to the minimum eigenvalue:\n$$\n\\kappa(M^{-1}A) = \\frac{\\lambda_{\\max}(M^{-1}A)}{\\lambda_{\\min}(M^{-1}A)} = \\frac{2\\sin^2\\left(\\frac{n\\pi}{2(n+1)}\\right)}{2\\sin^2\\left(\\frac{\\pi}{2(n+1)}\\right)} = \\frac{\\sin^2\\left(\\frac{n\\pi}{2(n+1)}\\right)}{\\sin^2\\left(\\frac{\\pi}{2(n+1)}\\right)}\n$$\nWe can simplify the numerator using the identity $\\sin(\\theta) = \\cos(\\pi/2 - \\theta)$:\n$$\n\\sin\\left(\\frac{n\\pi}{2(n+1)}\\right) = \\sin\\left(\\frac{(n+1-1)\\pi}{2(n+1)}\\right) = \\sin\\left(\\frac{\\pi}{2} - \\frac{\\pi}{2(n+1)}\\right) = \\cos\\left(\\frac{\\pi}{2(n+1)}\\right)\n$$\nSubstituting this into the expression for the condition number gives:\n$$\n\\kappa(M^{-1}A) = \\frac{\\cos^2\\left(\\frac{\\pi}{2(n+1)}\\right)}{\\sin^2\\left(\\frac{\\pi}{2(n+1)}\\right)} = \\cot^2\\left(\\frac{\\pi}{2(n+1)}\\right)\n$$\nThis is the closed-form expression for the condition number in terms of $n$.\n\nFor the specific case $n=127$, we have $n+1=128$. The condition number is:\n$$\n\\kappa(M^{-1}A) = \\cot^2\\left(\\frac{\\pi}{2(128)}\\right) = \\cot^2\\left(\\frac{\\pi}{256}\\right)\n$$\n\n### Part 2: PCG Iteration Count\n\nThe convergence of the Preconditioned Conjugate Gradient (PCG) method for solving $Ax=b$ with preconditioner $M$ is governed by the condition number of the preconditioned matrix, $\\kappa \\equiv \\kappa(M^{-1}A)$. The standard worst-case bound for the error in the $A$-norm is given by:\n$$\n\\frac{\\|x_k - x_{\\star}\\|_{A}}{\\|x_0 - x_{\\star}\\|_{A}} \\le 2\\left(\\frac{\\sqrt{\\kappa} - 1}{\\sqrt{\\kappa} + 1}\\right)^k\n$$\nWe want to find the smallest integer $k$ that satisfies the stopping criterion:\n$$\n2\\left(\\frac{\\sqrt{\\kappa} - 1}{\\sqrt{\\kappa} + 1}\\right)^k \\le \\epsilon\n$$\nSolving for $k$, we first isolate the term raised to the power of $k$:\n$$\n\\left(\\frac{\\sqrt{\\kappa} - 1}{\\sqrt{\\kappa} + 1}\\right)^k \\le \\frac{\\epsilon}{2}\n$$\nTaking the natural logarithm of both sides. Since $\\frac{\\sqrt{\\kappa} - 1}{\\sqrt{\\kappa} + 1} < 1$, its logarithm is negative, so the inequality is reversed upon division:\n$$\nk \\ln\\left(\\frac{\\sqrt{\\kappa} - 1}{\\sqrt{\\kappa} + 1}\\right) \\le \\ln\\left(\\frac{\\epsilon}{2}\\right)\n$$\n$$\nk \\ge \\frac{\\ln(\\epsilon/2)}{\\ln\\left(\\frac{\\sqrt{\\kappa} - 1}{\\sqrt{\\kappa} + 1}\\right)} = \\frac{\\ln(2/\\epsilon)}{\\ln\\left(\\frac{\\sqrt{\\kappa} + 1}{\\sqrt{\\kappa} - 1}\\right)}\n$$\nThe smallest integer $k$ is the ceiling of this expression:\n$$\nk = \\left\\lceil \\frac{\\ln(2/\\epsilon)}{\\ln\\left(\\frac{\\sqrt{\\kappa} + 1}{\\sqrt{\\kappa} - 1}\\right)} \\right\\rceil\n$$\nNow we specialize to $n = 127$ and $\\epsilon = 10^{-6}$. From Part 1, we have $\\kappa = \\cot^2\\left(\\frac{\\pi}{256}\\right)$. Thus, $\\sqrt{\\kappa} = \\cot\\left(\\frac{\\pi}{256}\\right)$ since the argument is in $(0, \\pi/2)$.\nSubstituting these values into the expression for $k$:\n$$\nk = \\left\\lceil \\frac{\\ln(2/10^{-6})}{\\ln\\left(\\frac{\\cot(\\pi/256) + 1}{\\cot(\\pi/256) - 1}\\right)} \\right\\rceil = \\left\\lceil \\frac{\\ln(2 \\times 10^6)}{\\ln\\left(\\frac{\\cot(\\pi/256) + 1}{\\cot(\\pi/256) - 1}\\right)} \\right\\rceil\n$$\nWe now compute the value of the argument of the ceiling function.\nThe numerator is $\\ln(2 \\times 10^6) = \\ln(2) + 6\\ln(10) \\approx 0.693147 + 6 \\times 2.302585 \\approx 14.508657$.\nThe argument of the cotangent is $\\theta = \\pi/256 \\approx 0.0122718$ radians.\nSo, $\\sqrt{\\kappa} = \\cot(\\pi/256) \\approx 81.48735$.\nThe denominator is $\\ln\\left(\\frac{81.48735 + 1}{81.48735 - 1}\\right) = \\ln\\left(\\frac{82.48735}{80.48735}\\right) \\approx \\ln(1.024848) \\approx 0.024549$.\nThe ratio is $\\frac{14.508657}{0.024549} \\approx 591.011$.\nThe smallest integer number of iterations is the ceiling of this value:\n$$\nk = \\lceil 591.011 \\ldots \\rceil = 592\n$$", "answer": "$$\n\\boxed{\\begin{pmatrix} \\cot^{2}\\left(\\frac{\\pi}{256}\\right) & 592 \\end{pmatrix}}\n$$", "id": "3412964"}, {"introduction": "Theoretical guarantees for the Conjugate Gradient (CG) method hinge on the operator being Symmetric Positive-Definite (SPD). This computational exercise provides a crucial hands-on demonstration of why this property is not merely a theoretical footnote but a practical necessity. You will construct a case where a naive, non-symmetry-preserving preconditioner causes the CG algorithm to break down, and contrast it with the robust convergence of a properly formulated Preconditioned CG (PCG) [@problem_id:3566284].", "problem": "You are asked to construct and analyze a computational example that demonstrates the necessity of symmetry-preserving preconditioning for iterative methods in numerical linear algebra. The focus is on the Conjugate Gradient (CG) method, Preconditioned Conjugate Gradient (PCG), and the impact of applying a non-symmetric preconditioner on a Symmetric Positive Definite (SPD) linear system.\n\nBegin from the following fundamental base:\n- The definition that a matrix $A \\in \\mathbb{R}^{n \\times n}$ is Symmetric Positive Definite (SPD) if $A = A^{\\top}$ and $x^{\\top} A x \\gt 0$ for all nonzero $x \\in \\mathbb{R}^{n}$.\n- The requirement that the Conjugate Gradient (CG) method is derived for SPD operators, ensuring the existence of well-defined search directions and positive step-lengths when the operator is SPD.\n- The observation that naive left-preconditioning with an arbitrary invertible matrix $M$ replaces the SPD operator $A$ with $M^{-1} A$, which may not be symmetric (and may not define a valid energy functional), thus invalidating the assumptions underlying CG.\n- The Preconditioned Conjugate Gradient (PCG) method requires the preconditioner $M$ to be SPD and used in a symmetry-preserving way so that the operator remains $A$ while the inner product and residual updates are modified via the action of $M^{-1}$, maintaining positive definiteness of key quantities.\n\nTasks:\n1. Construct an SPD matrix $A$ from the standard $2$-dimensional discrete Laplacian with Dirichlet boundary conditions on a uniform $n \\times n$ grid. Use the five-point stencil to form $A$ of dimension $N = n^2$, with main diagonal entries equal to $4$ and off-diagonal entries equal to $-1$ for grid neighbors (no physical units are involved).\n2. Define a non-symmetric preconditioner $M = A J$, where $J \\in \\mathbb{R}^{N \\times N}$ is block diagonal with $2 \\times 2$ rotation-by-$90^\\circ$ blocks $K = \\begin{bmatrix} 0 & 1 \\\\ -1 & 0 \\end{bmatrix}$ repeated along the diagonal. Note that $J$ is orthogonal and skew-symmetric, so $J^{-1} = J^{\\top} = -J$, and the naively left-preconditioned operator $B = M^{-1} A = J^{-1}$ is skew-symmetric. Explain why running CG directly on the left-preconditioned system $B x = M^{-1} b$ violates the SPD requirement and leads to breakdown through the step-length denominator $p^{\\top} B p$, which equals $0$ for any nonzero $p$ when $B$ is skew-symmetric.\n3. Implement a symmetry-preserving PCG with an SPD preconditioner. Use a diagonal (Jacobi) preconditioner $M_{\\mathrm{sym}} = \\mathrm{diag}(A)$, which is SPD for the given $A$. In this configuration, keep $A$ as the operator and apply PCG using the preconditioned residual $z_k = M_{\\mathrm{sym}}^{-1} r_k$, ensuring positive invariants such as $r_k^{\\top} z_k \\gt 0$ and well-defined step lengths $\\alpha_k$ based on $p_k^{\\top} A p_k \\gt 0$.\n4. As a baseline, run plain CG (equivalent to PCG with the identity preconditioner) on $A$.\n\nFor all tasks, use the right-hand side $b = \\mathbf{1} \\in \\mathbb{R}^{N}$ (a vector of all ones). Use $n = 10$, so the dimension is $N = 100$. Use a relative residual tolerance $\\varepsilon = 10^{-10}$ and a maximum of $500$ iterations.\n\nDesign a test suite with three cases:\n- Case $1$: Naive left-preconditioned CG applied to $B = M^{-1} A$ with non-symmetric $M = A J$. Detect breakdown if the step-length denominator $p^{\\top} B p$ is non-positive or numerically zero (i.e., $|p^{\\top} B p| \\leq \\delta$ for a small $\\delta$), and return a sentinel result.\n- Case $2$: Symmetry-preserving PCG applied to $A$ with SPD preconditioner $M_{\\mathrm{sym}} = \\mathrm{diag}(A)$; stop when the relative residual $\\|r_k\\|_2 / \\|b\\|_2 \\leq \\varepsilon$ and report the iteration count.\n- Case $3$: Plain CG applied to $A$ with the identity preconditioner; stop under the same criterion and report the iteration count.\n\nOutput specification:\n- For each test case, return a single integer:\n    - For Case $1$: return $-1$ if breakdown is detected before convergence; otherwise return the iteration count until the tolerance is met (this should be expected to be $-1$ due to the specific construction where $B$ is skew-symmetric).\n    - For Case $2$: return the number of iterations taken by PCG to reach the tolerance.\n    - For Case $3$: return the number of iterations taken by plain CG to reach the tolerance.\n- Your program should produce a single line of output containing the three integer results as a comma-separated list enclosed in square brackets, for example, $\\left[ r_1, r_2, r_3 \\right]$ rendered as text in the exact format \"[r1,r2,r3]\".\n\nConstraints:\n- Use only precise floating-point arithmetic and linear algebra operations; no external randomness.\n- Ensure numerical stability by guarding denominators in the CG iteration, and define breakdown deterministically for Case $1$ through the skew-symmetry of $B$.\n\nThe final program must be complete and runnable without user input and must implement the three cases with the prescribed parameters, returning the results in the specified format.", "solution": "The problem statement is evaluated as valid. It is scientifically grounded in numerical linear algebra, well-posed, objective, and internally consistent. All required data and definitions are provided to construct and execute the computational experiment.\n\nThe objective is to demonstrate the necessity of symmetry-preserving preconditioning when using the Conjugate Gradient (CG) method. We will compare three scenarios: (1) a naive, non-symmetric preconditioning scheme that leads to algorithmic breakdown, (2) a proper, symmetry-preserving preconditioning scheme (PCG), and (3) the standard, unpreconditioned CG method as a baseline.\n\n### System, Vectors, and Parameters\nThe linear system to be solved is $Ax=b$.\n- The matrix $A \\in \\mathbb{R}^{N \\times N}$ represents the five-point finite difference discretization of the $2D$ negative Laplacian operator on a uniform $n \\times n$ grid with Dirichlet boundary conditions. The grid size is $n=10$, so the matrix dimension is $N = n^2 = 100$. The matrix $A$ is constructed with main diagonal entries of $4$ and off-diagonal entries of $-1$ corresponding to the four adjacent neighbors in the grid. This construction ensures that $A$ is Symmetric Positive Definite (SPD), a fundamental requirement for the convergence of the standard CG method.\n- The right-hand side vector is $b = \\mathbf{1} \\in \\mathbb{R}^{N}$, a vector of all ones.\n- The initial guess for the solution is $x_0 = \\mathbf{0} \\in \\mathbb{R}^{N}$.\n- The iterative methods are terminated when the relative residual norm satisfies $\\|r_k\\|_2 / \\|b\\|_2 \\leq \\varepsilon$, where the tolerance is $\\varepsilon = 10^{-10}$. The maximum number of iterations is set to $500$.\n\n### Case 1: Naive Left-Preconditioned CG with a Non-Symmetric Preconditioner\nThis case illustrates the failure of CG when its core assumptions are violated. We apply CG to a naively left-preconditioned system.\nThe preconditioner is defined as $M = AJ$, where $J \\in \\mathbb{R}^{N \\times N}$ is a block diagonal matrix composed of $N/2 = 50$ copies of the $2 \\times 2$ skew-symmetric rotation matrix $K = \\begin{bmatrix} 0 & 1 \\\\ -1 & 0 \\end{bmatrix}$.\n\nFirst, we analyze the properties of $J$:\n1.  **Skew-Symmetry**: The transpose of $K$ is $K^{\\top} = \\begin{bmatrix} 0 & -1 \\\\ 1 & 0 \\end{bmatrix} = -K$. Since $J$ is block diagonal with $K$ blocks, $J^{\\top}$ is block diagonal with $K^{\\top}$ blocks, which means $J^{\\top} = -J$. Thus, $J$ is skew-symmetric.\n2.  **Orthogonality**: We compute $K K^{\\top} = K(-K) = -K^2 = -\\begin{bmatrix} -1 & 0 \\\\ 0 & -1 \\end{bmatrix} = I_2$. Since $J J^{\\top}$ is block diagonal with $K K^{\\top}$ blocks, $J J^{\\top} = I_N$. Thus, $J$ is an orthogonal matrix, and its inverse is its transpose, $J^{-1} = J^{\\top}$.\n\nCombining these properties, we find $J^{-1} = J^{\\top} = -J$.\n\nThe left-preconditioned system is $M^{-1} A x = M^{-1} b$. The operator for the CG method becomes $B = M^{-1}A$.\n$$B = (AJ)^{-1}A = J^{-1}A^{-1}A = J^{-1}I = J^{-1}$$\nSo, the operator is $B = J^{-1}$. We can show that $B$ is skew-symmetric:\n$$B^{\\top} = (J^{-1})^{\\top} = (J^{\\top})^{-1} = (-J)^{-1} = -(J^{-1}) = -B$$\nThe CG algorithm requires the operator to be SPD. A key step in the iteration is the computation of the step length:\n$$\\alpha_k = \\frac{r_k^{\\top} r_k}{p_k^{\\top} B p_k}$$\nThe denominator is a quadratic form involving the operator $B$. For any skew-symmetric matrix $B$ and any non-zero vector $p$, this form is identically zero:\n$$p^{\\top}Bp = (p^{\\top}Bp)^{\\top} = p^{\\top}B^{\\top}p = p^{\\top}(-B)p = -p^{\\top}Bp$$\nThe only scalar value equal to its own negative is $0$. Therefore, $p_k^{\\top} B p_k = 0$ for any search direction $p_k$. This leads to division by zero, causing an immediate breakdown of the algorithm. We will detect this by checking if $|p_k^{\\top} B p_k| \\le \\delta$ for a small tolerance $\\delta$, and if so, report failure with a result of $-1$.\n\n### Case 2: Symmetry-Preserving Preconditioned CG (PCG)\nTo correctly apply preconditioning to an SPD system for use with CG, the preconditioner $M_{\\mathrm{sym}}$ must itself be SPD and be applied in a way that preserves the symmetry of the effective operator. The PCG algorithm achieves this.\n\nHere, we use a simple and effective SPD preconditioner: the Jacobi (or diagonal) preconditioner, $M_{\\mathrm{sym}} = \\mathrm{diag}(A)$. For our specific matrix $A$, all diagonal elements are $4$, so $M_{\\mathrm{sym}} = 4I$, which is clearly SPD. Its inverse is $M_{\\mathrm{sym}}^{-1} = \\frac{1}{4}I$.\n\nThe PCG algorithm maintains the original SPD operator $A$ in the step-length calculation ($p_k^\\top A p_k$), ensuring the denominator is always positive. The preconditioning step involves solving a system $M_{\\mathrm{sym}} z_k = r_k$ for the preconditioned residual $z_k$. This is used to update the search direction:\n$$p_{k+1} = z_{k+1} + \\beta_{k+1} p_k, \\quad \\text{where} \\quad \\beta_{k+1} = \\frac{r_{k+1}^{\\top} z_{k+1}}{r_k^{\\top} z_k}$$\nThis formulation guarantees that the method is well-defined and typically accelerates convergence compared to unpreconditioned CG. We will report the number of iterations required to meet the convergence tolerance.\n\n### Case 3: Standard Conjugate Gradient (CG)\nThis case serves as our baseline. We apply the standard CG algorithm to the original SPD system $Ax=b$. This is mathematically equivalent to PCG with the identity matrix as the preconditioner ($M=I$). It is expected to converge, as $A$ is SPD, but likely more slowly than the well-preconditioned PCG in Case 2. We will report the number of iterations for comparison.\n\nThe computational experiment will yield three integer results: the outcome for each of the three cases, demonstrating the critical importance of selecting and applying preconditioners correctly.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Constructs and solves the three test cases as specified in the problem.\n    \"\"\"\n    n = 10\n    N = n * n\n    tol = 1e-10\n    max_iter = 500\n\n    def construct_A(n_grid):\n        \"\"\"Constructs the discrete 2D Laplacian matrix A.\"\"\"\n        n_dim = n_grid * n_grid\n        A = np.zeros((n_dim, n_dim))\n        for k in range(n_dim):\n            i, j = k // n_grid, k % n_grid\n            A[k, k] = 4\n            if i > 0: A[k, k - n_grid] = -1\n            if i  n_grid - 1: A[k, k + n_grid] = -1\n            if j > 0: A[k, k - 1] = -1\n            if j  n_grid - 1: A[k, k + 1] = -1\n        return A\n\n    A = construct_A(n)\n    b = np.ones(N)\n\n    def case1_solver():\n        \"\"\"\n        Case 1: Naive left-preconditioned CG.\n        Expected to break down. Returns -1 on breakdown or max_iter.\n        \"\"\"\n        # Construct the skew-symmetric operator B\n        J = np.zeros((N, N))\n        for i in range(N // 2):\n            J[2 * i, 2 * i + 1] = 1\n            J[2 * i + 1, 2 * i] = -1\n        \n        # Operator B = M^{-1}A = J^{-1} = -J\n        B = -J \n        # RHS b' = M^{-1}b = J^{-1}b = -Jb\n        b_prime = -J @ b\n        \n        x = np.zeros(N)\n        r_prime = b_prime.copy()  # Residual of the preconditioned system\n        p = r_prime.copy()\n        rs_old = r_prime @ r_prime\n        \n        norm_b = np.linalg.norm(b)\n        \n        for k in range(max_iter):\n            Bp = B @ p\n            pBp = p @ Bp\n            \n            # Breakdown check: denominator is zero for skew-symmetric operator\n            if abs(pBp) = 1e-15:\n                return -1\n            \n            alpha = rs_old / pBp\n            x += alpha * p\n            r_prime -= alpha * Bp\n            \n            # Convergence check uses the true residual r = b - Ax\n            r_true = b - A @ x\n            if np.linalg.norm(r_true) / norm_b  tol:\n                return k + 1\n            \n            rs_new = r_prime @ r_prime\n            beta = rs_new / rs_old\n            p = r_prime + (rs_new / rs_old) * p\n            rs_old = rs_new\n        \n        return -1 # Failure (max iterations reached)\n\n    def case2_solver():\n        \"\"\"\n        Case 2: Symmetry-preserving PCG with Jacobi preconditioner.\n        Expected to converge. Returns iteration count.\n        \"\"\"\n        x = np.zeros(N)\n        r = b.copy()\n        norm_b = np.linalg.norm(b)\n\n        # Initial residual check\n        if np.linalg.norm(r) / norm_b  tol:\n            return 0\n        \n        # Preconditioning step: M_sym = diag(A) = 4I, so z = M_inv * r = r / 4.0\n        z = r / 4.0\n        p = z.copy()\n        rz_old = r @ z\n        \n        for k in range(max_iter):\n            Ap = A @ p\n            pAp = p @ Ap\n            \n            alpha = rz_old / pAp\n            x += alpha * p\n            r -= alpha * Ap\n            \n            if np.linalg.norm(r) / norm_b  tol:\n                return k + 1\n            \n            z = r / 4.0\n            rz_new = r @ z\n            beta = rz_new / rz_old\n            p = z + beta * p\n            rz_old = rz_new\n            \n        return max_iter # Did not converge within max iterations\n\n    def case3_solver():\n        \"\"\"\n        Case 3: Standard CG (no preconditioning).\n        Expected to converge, but slower than Case 2. Returns iteration count.\n        \"\"\"\n        x = np.zeros(N)\n        r = b.copy()\n        norm_b = np.linalg.norm(b)\n\n        if np.linalg.norm(r) / norm_b  tol:\n            return 0\n        \n        p = r.copy()\n        rs_old = r @ r\n        \n        for k in range(max_iter):\n            Ap = A @ p\n            pAp = p @ Ap\n            \n            alpha = rs_old / pAp\n            x += alpha * p\n            r -= alpha * Ap\n            \n            if np.linalg.norm(r) / norm_b  tol:\n                return k + 1\n            \n            rs_new = r @ r\n            beta = rs_new / rs_old\n            p = r + (rs_new / rs_old) * p\n            rs_old = rs_new\n            \n        return max_iter # Did not converge within max iterations\n\n    # Run the three cases and collect the results\n    results = [\n        case1_solver(),\n        case2_solver(),\n        case3_solver()\n    ]\n\n    # Print the final output in the required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3566284"}, {"introduction": "We now turn to a powerful technique deeply rooted in the statistical foundations of inverse problems: prior preconditioning. This practice explores how a change of variables, motivated by the prior covariance in a Bayesian framework, can be viewed as an elegant and highly effective preconditioning strategy. By implementing this \"prior-whitening\" transformation, you will quantitatively analyze how it dramatically improves the spectral properties of the problem's Hessian, often clustering many eigenvalues at unity and significantly accelerating convergence [@problem_id:3412970].", "problem": "Consider a Bayesian linear inverse problem with Gaussian prior and Gaussian observational noise. Let $A \\in \\mathbb{R}^{m \\times n}$ be the linear forward operator, $x \\in \\mathbb{R}^{n}$ the unknown, $y \\in \\mathbb{R}^{m}$ the data, $R \\in \\mathbb{R}^{m \\times m}$ the data error covariance (symmetric positive definite), and $C \\in \\mathbb{R}^{n \\times n}$ the prior covariance (symmetric positive definite). The Maximum A Posteriori (MAP) estimate is the minimizer of the strictly convex quadratic objective obtained from Bayes’ rule. The associated normal equations for the MAP are of the form\n$$\n\\left(A^{\\top} R^{-1} A + C^{-1}\\right) x = A^{\\top} R^{-1} y + C^{-1} x_{0},\n$$\nwhere $x_{0} \\in \\mathbb{R}^{n}$ is the prior mean. Let $H = A^{\\top} R^{-1} A + C^{-1}$ denote the Hessian of the objective in the variable $x$.\n\nYou are asked to study the effect of prior-whitening, defined by the change of variables $x = x_{0} + C^{1/2} z$, on the spectrum relevant to iterative solution by the Conjugate Gradient method for Symmetric Positive Definite (CG for SPD) systems. Starting from first principles of Gaussian models and basic linear algebra (spectral theorem and singular value decomposition), derive the symmetric positive definite system matrix whose spectrum governs CG in the prior-whitened variable $z$, and analyze how its eigenvalues depend on the interaction between the data term and the prior.\n\nYour program must implement the following, using only deterministic computations specified below.\n\n- For each test case, construct matrices $A$, $R$, and $C$ as follows.\n  - Use a fixed random seed $s$ to ensure reproducibility.\n  - Draw a standard normal matrix with independent entries and form a thin $QR$ factorization to obtain an orthonormal matrix $Q$. Use this to construct a symmetric positive definite covariance by $Q \\, \\mathrm{diag}(\\lambda) \\, Q^{\\top}$, where $\\mathrm{diag}(\\lambda)$ is diagonal with a prescribed geometric spectrum. Perform this construction separately to obtain $C$ and $R$ with their respective sizes and spectral ranges.\n  - Form $A$ with independent standard normal entries and scale by a prescribed scalar $\\gamma$.\n- For each test case, compute:\n  1. The spectrum of the original Hessian $H$ and its spectral condition number $\\kappa(H)$, defined as the ratio of the largest eigenvalue to the smallest eigenvalue.\n  2. The symmetric positive definite system matrix in the prior-whitened variable $z$ and its spectrum. From this spectrum compute:\n     - The spectral condition number $\\kappa$ of the prior-whitened system matrix.\n     - The fraction of eigenvalues that are exactly equal to $1$ up to an absolute tolerance $\\tau = 10^{-8}$ (express this as a decimal in $[0,1]$).\n     - The fraction of eigenvalues that are within an absolute tolerance $\\varepsilon = 10^{-2}$ of $1$ (express this as a decimal in $[0,1]$).\n     - The maximum absolute deviation from $1$ over all eigenvalues.\n  3. The ratio $\\kappa(H)/\\kappa$, where $\\kappa$ is the spectral condition number of the prior-whitened system matrix from item $2$.\n- Implementation requirements:\n  - All linear algebra must be performed in real arithmetic with double precision as provided by standard libraries.\n  - Inverting $R$ and $C$ explicitly is not permitted; use linear solves or factorization-based methods.\n  - Use the spectral theorem to construct matrix square roots needed for prior-whitening.\n  - Use the singular value decomposition or eigenvalue decomposition as appropriate for spectral quantities.\n\nUse the following test suite of parameter values, where each tuple represents $(n, m, \\gamma, c_{\\min}, c_{\\max}, r_{\\min}, r_{\\max}, s)$:\n- Test $1$: $(60, 40, 1.0, 10^{-3}, 10^{3}, 1.0, 1.0, 1)$.\n- Test $2$: $(60, 40, 0.05, 10^{-2}, 10^{2}, 1.0, 1.0, 2)$.\n- Test $3$: $(50, 80, 2.0, 10^{-3}, 10^{3}, 10^{-2}, 10^{2}, 3)$.\n\nFor each test, your program must output a list of four floating-point numbers in the following order:\n- $\\kappa(H)/\\kappa$,\n- fraction of exactly unit eigenvalues within tolerance $\\tau = 10^{-8}$,\n- fraction of near-unit eigenvalues within tolerance $\\varepsilon = 10^{-2}$,\n- maximum absolute deviation from $1$ of the prior-whitened system’s eigenvalues.\n\nFinal output format: Your program should produce a single line of output containing the results for all three tests as a comma-separated list of sublists, each sublist containing the four floating-point numbers for the corresponding test, enclosed in square brackets (e.g., $[\\,[a_{1},b_{1},c_{1},d_{1}],\\,[a_{2},b_{2},c_{2},d_{2}],\\,[a_{3},b_{3},c_{3},d_{3}]\\,]$). No additional text should be printed.", "solution": "The present problem addresses the analysis of prior preconditioning for a Bayesian linear inverse problem. We begin by establishing the theoretical framework and then proceed to the numerical implementation.\n\n### 1. Mathematical Formulation\n\nThe Maximum A Posteriori (MAP) estimate is the minimizer of the objective function $J(x)$, which combines the data likelihood and the prior information:\n$$ J(x) = \\frac{1}{2}(Ax - y)^\\top R^{-1}(Ax - y) + \\frac{1}{2}(x - x_0)^\\top C^{-1}(x - x_0) $$\nThe Hessian of this objective function with respect to $x$ is given by:\n$$ H = \\nabla_x^2 J(x) = A^\\top R^{-1} A + C^{-1} $$\nThis matrix is symmetric and positive definite (SPD), and its spectral properties, particularly its condition number $\\kappa(H)$, govern the convergence rate of iterative solvers like the Conjugate Gradient (CG) method.\n\n### 2. Prior-Whitening and the Preconditioned System\n\nThe problem introduces a change of variables known as prior-whitening or preconditioning in the state space:\n$$ x = x_0 + C^{1/2} z $$\nwhere $C^{1/2}$ is the unique symmetric positive definite square root of the prior covariance matrix $C$. The new unknown is $z \\in \\mathbb{R}^n$. If the prior on $x$ is $x \\sim \\mathcal{N}(x_0, C)$, then this transformation implies that $z = C^{-1/2}(x-x_0)$ has a standard normal distribution, $z \\sim \\mathcal{N}(0, I)$.\n\nWe reformulate the objective function $J(x)$ in terms of the new variable $z$. Let $\\tilde{J}(z) = J(x_0 + C^{1/2}z)$. The two terms in $J(x)$ become:\n1.  **Data Fidelity Term**:\n    $$ (A(x_0 + C^{1/2}z) - y) = A C^{1/2} z + (Ax_0 - y) $$\n    The term in the objective function becomes $\\frac{1}{2} (A C^{1/2} z + (Ax_0-y))^\\top R^{-1} (A C^{1/2} z + (Ax_0-y))$.\n2.  **Prior Term**:\n    $$ (x - x_0) = (x_0 + C^{1/2}z - x_0) = C^{1/2}z $$\n    The term in the objective function becomes $\\frac{1}{2} (C^{1/2}z)^\\top C^{-1} (C^{1/2}z) = \\frac{1}{2} z^\\top C^{1/2} C^{-1} C^{1/2} z = \\frac{1}{2} z^\\top I z = \\frac{1}{2} z^\\top z$.\n\nThe Hessian of the new objective function $\\tilde{J}(z)$ is found by taking the second derivative with respect to $z$. Only the quadratic terms in $z$ contribute. The quadratic part of $\\tilde{J}(z)$ is:\n$$ \\frac{1}{2} z^\\top \\left( (A C^{1/2})^\\top R^{-1} (A C^{1/2}) \\right) z + \\frac{1}{2} z^\\top z = \\frac{1}{2} z^\\top \\left( C^{1/2} A^\\top R^{-1} A C^{1/2} + I \\right) z $$\nThe Hessian in the $z$-coordinate system, which we denote $H_z$, is therefore:\n$$ H_z = \\nabla_z^2 \\tilde{J}(z) = C^{1/2} A^\\top R^{-1} A C^{1/2} + I $$\nThis is the SPD system matrix whose spectrum governs the convergence of the CG method applied to the minimization problem for $z$. This transformation is a form of symmetric preconditioning on the original Hessian $H$, where $H_z = C^{1/2} H C^{1/2}$.\n\n### 3. Spectral Analysis of the Preconditioned Hessian $H_z$\n\nLet's analyze the eigenvalues of $H_z$. Define a new matrix $\\tilde{A} = R^{-1/2} A C^{1/2}$. Since $R$ and $C$ are SPD, their square roots and inverse square roots are well-defined and symmetric. We can rewrite $H_z$ as:\n$$ H_z = (C^{1/2})^\\top A^\\top (R^{-1/2})^\\top R^{-1/2} A C^{1/2} + I = (R^{-1/2} A C^{1/2})^\\top (R^{-1/2} A C^{1/2}) + I = \\tilde{A}^\\top \\tilde{A} + I $$\nThe eigenvalues of $H_z$, denoted $\\lambda_i(H_z)$, are related to the singular values of $\\tilde{A}$, denoted $\\sigma_i(\\tilde{A})$, as follows:\n$$ \\lambda_i(H_z) = \\lambda_i(\\tilde{A}^\\top \\tilde{A} + I) = \\lambda_i(\\tilde{A}^\\top \\tilde{A}) + 1 = \\sigma_i^2(\\tilde{A}) + 1 $$\nThis relationship reveals key properties of the preconditioned system:\n- **Positive Definiteness**: Since $\\sigma_i^2(\\tilde{A}) \\ge 0$, all eigenvalues of $H_z$ are $\\ge 1$. This guarantees that $H_z$ is not only SPD but also well-conditioned with respect to inversion, as its smallest eigenvalue is bounded away from zero.\n- **Unit Eigenvalues**: An eigenvalue $\\lambda_i(H_z)$ is exactly $1$ if and only if the corresponding singular value $\\sigma_i(\\tilde{A}) = 0$. The number of zero singular values is equal to the dimension of the null space of $\\tilde{A}$. Since $R^{-1/2}$ and $C^{1/2}$ are invertible, $\\mathrm{rank}(\\tilde{A}) = \\mathrm{rank}(A)$. For an $m \\times n$ matrix $A$, the dimension of the null space is $n - \\mathrm{rank}(A)$. For a random matrix $A$ generated from a continuous distribution, its rank is almost surely $\\min(m, n)$. Therefore, the number of unit eigenvalues is expected to be $n - \\min(m, n) = \\max(0, n-m)$.\n- **Eigenvalue Clustering**: The eigenvalues of $H_z$ are clustered around $1$ if the singular values of $\\tilde{A}$ are small. This occurs when the data provides little information relative to the prior, for example, if the scaling factor $\\gamma$ of the operator $A$ is small. In such cases, the preconditioning is highly effective, leading to a condition number $\\kappa(H_z)$ close to $1$.\n\n### 4. Implementation Plan\n\nThe solution is implemented by following these steps for each test case:\n1.  **Matrix Construction**: A helper function creates SPD matrices $R$ and $C$ with specified dimensions and geometric spectra using a fixed random seed for reproducibility. The forward operator $A$ is also generated and scaled.\n2.  **Original Hessian $H$**: The matrix $H = A^\\top R^{-1} A + C^{-1}$ is assembled. Products with inverse matrices are computed using `numpy.linalg.solve` to avoid explicit inversion. The eigenvalues of $H$ are computed using `numpy.linalg.eigvalsh` to find its condition number $\\kappa(H)$.\n3.  **Preconditioned Hessian $H_z$**: The square root $C^{1/2}$ is computed from the spectral decomposition of $C$. Then, $H_z = C^{1/2} A^\\top R^{-1} A C^{1/2} + I$ is formed, again using `solve`. Its eigenvalues are computed to find $\\kappa(H_z)$ and analyze their distribution.\n4.  **Analysis**: The required metrics—the ratio $\\kappa(H)/\\kappa(H_z)$, the fraction of exact and near-unit eigenvalues, and the maximum deviation from $1$—are calculated and stored.", "answer": "```python\nimport numpy as np\n\ndef create_spd_matrix(dim, spec_min, spec_max, rng):\n    \"\"\"\n    Constructs a symmetric positive definite (SPD) matrix with a prescribed geometric spectrum.\n    \n    Args:\n        dim (int): The dimension of the square matrix.\n        spec_min (float): The minimum eigenvalue.\n        spec_max (float): The maximum eigenvalue.\n        rng (np.random.Generator): A NumPy random number generator.\n    \n    Returns:\n        np.ndarray: An SPD matrix of shape (dim, dim).\n    \"\"\"\n    # Generate a random matrix\n    random_matrix = rng.standard_normal((dim, dim))\n    # Get an orthonormal basis from its QR decomposition\n    q, _ = np.linalg.qr(random_matrix)\n    \n    # Create a diagonal matrix with a geometric spectrum\n    eigenvalues = np.geomspace(spec_min, spec_max, dim)\n    lambda_diag = np.diag(eigenvalues)\n    \n    # Construct the SPD matrix using the spectral theorem M = Q * Lambda * Q^T\n    spd_matrix = q @ lambda_diag @ q.T\n    return spd_matrix\n\ndef compute_matrix_sqrt(matrix):\n    \"\"\"\n    Computes the symmetric positive definite square root of a matrix.\n    \n    Args:\n        matrix (np.ndarray): The SPD matrix.\n    \n    Returns:\n        np.ndarray: The symmetric square root matrix.\n    \"\"\"\n    eigenvalues, eigenvectors = np.linalg.eigh(matrix)\n    sqrt_eigenvalues = np.sqrt(eigenvalues)\n    return eigenvectors @ np.diag(sqrt_eigenvalues) @ eigenvectors.T\n\ndef solve():\n    \"\"\"\n    Main function to run the test cases and produce the final output.\n    \"\"\"\n    # Tuples of (n, m, gamma, c_min, c_max, r_min, r_max, s)\n    test_cases = [\n        (60, 40, 1.0, 1e-3, 1e3, 1.0, 1.0, 1),\n        (60, 40, 0.05, 1e-2, 1e2, 1.0, 1.0, 2),\n        (50, 80, 2.0, 1e-3, 1e3, 1e-2, 1e2, 3),\n    ]\n\n    all_results = []\n\n    for n, m, gamma, c_min, c_max, r_min, r_max, seed in test_cases:\n        rng = np.random.default_rng(seed)\n\n        # 1. Construct matrices A, R, and C\n        C = create_spd_matrix(n, c_min, c_max, rng)\n        R = create_spd_matrix(m, r_min, r_max, rng)\n        A = gamma * rng.standard_normal((m, n))\n        \n        # 2. Compute original Hessian H and its spectrum\n        # H = A.T @ R_inv @ A + C_inv\n        # Avoid explicit inversion using solves\n        R_inv_A = np.linalg.solve(R, A)\n        C_inv = np.linalg.solve(C, np.identity(n))\n        H = A.T @ R_inv_A + C_inv\n        \n        eig_H = np.linalg.eigvalsh(H)\n        kappa_H = eig_H.max() / eig_H.min()\n\n        # 3. Compute prior-whitened Hessian H_z and its spectrum\n        # H_z = C^(1/2) * A.T * R^(-1) * A * C^(1/2) + I\n        C_sqrt = compute_matrix_sqrt(C)\n        \n        X = A @ C_sqrt\n        # Y = R^(-1) * X\n        Y = np.linalg.solve(R, X)\n        H_z = X.T @ Y + np.identity(n)\n\n        eig_Hz = np.linalg.eigvalsh(H_z)\n        \n        # 4. Analyze the spectrum of H_z\n        kappa_z = eig_Hz.max() / eig_Hz.min()\n        \n        # Fraction of eigenvalues equal to 1 (within tolerance tau)\n        tau = 1e-8\n        frac_exact_1 = np.sum(np.abs(eig_Hz - 1.0)  tau) / n\n        \n        # Fraction of eigenvalues near 1 (within tolerance epsilon)\n        epsilon = 1e-2\n        frac_near_1 = np.sum(np.abs(eig_Hz - 1.0)  epsilon) / n\n\n        # Maximum absolute deviation from 1\n        max_dev = np.max(np.abs(eig_Hz - 1.0))\n\n        # 5. Compute the ratio of condition numbers\n        ratio_kappa = kappa_H / kappa_z\n\n        all_results.append([ratio_kappa, frac_exact_1, frac_near_1, max_dev])\n\n    # Format the final output as specified\n    # e.g., [[a1,b1,c1,d1],[a2,b2,c2,d2],[a3,b3,c3,d3]]\n    output_str = \"[\" + \",\".join([f\"[{','.join(map(str, res))}]\" for res in all_results]) + \"]\"\n    print(output_str)\n\nsolve()\n```", "id": "3412970"}]}