{"hands_on_practices": [{"introduction": "The observability Gramian is a fundamental tool for analyzing linear time-invariant (LTI) systems. This exercise will guide you through connecting its integral definition, which provides deep theoretical insight, to the algebraic Lyapunov equation, a powerful computational alternative for stable systems [@problem_id:3421940]. By computing the Gramian and checking its positive definiteness, you will gain a hands-on understanding of how this matrix serves as a definitive test for system observability.", "problem": "Consider the continuous-time Linear Time-Invariant (LTI) state-space model with state matrix $A \\in \\mathbb{R}^{2 \\times 2}$ and output matrix $C \\in \\mathbb{R}^{1 \\times 2}$ given by\n$$\nA = \\begin{pmatrix} 0 & 1 \\\\ -1 & -2 \\end{pmatrix}, \\qquad C = \\begin{pmatrix} 1 & 0 \\end{pmatrix}.\n$$\nStarting from the definition of the infinite-horizon observability Gramian for a stable LTI system,\n$$\nW_{o} \\equiv \\int_{0}^{\\infty} \\exp(A^{\\top} t)\\, C^{\\top} C \\, \\exp(A t)\\, \\mathrm{d}t,\n$$\nderive any equivalent characterization that is valid under the stability of $A$ and use it to compute the exact symmetric matrix $W_{o} \\in \\mathbb{R}^{2 \\times 2}$. Then, determine whether the pair $(C,A)$ is observable by checking the positive definiteness of $W_{o}$ using first principles of matrix analysis.\n\nAs your final reported quantity, provide the exact value of $\\det(W_{o})$. Do not round; report the exact value. The final answer must be a single real number (no units).", "solution": "The problem is first validated to ensure it is well-posed, scientifically sound, and contains all necessary information.\n\n## Problem Validation\n\n### Step 1: Extract Givens\n- State matrix: $A = \\begin{pmatrix} 0 & 1 \\\\ -1 & -2 \\end{pmatrix} \\in \\mathbb{R}^{2 \\times 2}$\n- Output matrix: $C = \\begin{pmatrix} 1 & 0 \\end{pmatrix} \\in \\mathbb{R}^{1 \\times 2}$\n- Definition of infinite-horizon observability Gramian for a stable LTI system: $W_{o} \\equiv \\int_{0}^{\\infty} \\exp(A^{\\top} t)\\, C^{\\top} C \\, \\exp(A t)\\, \\mathrm{d}t$\n- Tasks:\n    1. Derive an equivalent characterization of $W_{o}$ valid for a stable matrix $A$.\n    2. Use this characterization to compute the exact symmetric matrix $W_o$.\n    3. Determine if the pair $(C,A)$ is observable by checking the positive definiteness of $W_o$.\n    4. Provide the exact value of $\\det(W_{o})$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded in linear systems and control theory. The concepts of state-space models, observability, and the observability Gramian are standard. The problem is well-posed and objective.\n\nA critical condition for the convergence of the integral defining $W_{o}$ is that the system matrix $A$ must be stable (or Hurwitz), meaning all its eigenvalues must have negative real parts. We must verify this condition. The characteristic equation is $\\det(\\lambda I - A) = 0$.\n$$\n\\det\\left(\\begin{pmatrix} \\lambda & 0 \\\\ 0 & \\lambda \\end{pmatrix} - \\begin{pmatrix} 0 & 1 \\\\ -1 & -2 \\end{pmatrix}\\right) = \\det\\begin{pmatrix} \\lambda & -1 \\\\ 1 & \\lambda+2 \\end{pmatrix} = 0\n$$\n$$\n\\lambda(\\lambda+2) - (1)(-1) = \\lambda^2 + 2\\lambda + 1 = (\\lambda+1)^2 = 0\n$$\nThe eigenvalues are $\\lambda_{1,2} = -1$, which are real and negative. Since all eigenvalues have real parts less than zero, the matrix $A$ is stable. Therefore, the integral for $W_o$ converges, and the problem is valid.\n\n## Solution\n\n### Equivalent Characterization of $W_o$\nThe infinite-horizon observability Gramian $W_o$ for a stable LTI system is known to be the unique, symmetric, positive semi-definite solution to the continuous-time algebraic Lyapunov equation:\n$$\nA^{\\top} W_o + W_o A = -C^{\\top} C\n$$\nTo derive this from the integral definition, we start with the Lyapunov equation and show that its solution corresponds to the integral. Let a symmetric matrix $X$ be the solution to $A^{\\top} X + X A = -C^{\\top} C$. We can write:\n$$\n\\int_{0}^{\\infty} \\exp(A^{\\top} t) (A^{\\top} X + X A) \\exp(A t) \\, \\mathrm{d}t = - \\int_{0}^{\\infty} \\exp(A^{\\top} t) C^{\\top} C \\exp(A t) \\, \\mathrm{d}t\n$$\nThe right-hand side is, by definition, $-W_o$. The left-hand side can be recognized as the integral of a derivative. Specifically, $\\frac{\\mathrm{d}}{\\mathrm{d}t} \\left(\\exp(A^{\\top} t) X \\exp(A t)\\right) = \\exp(A^{\\top} t) (A^{\\top} X + X A) \\exp(A t)$. Therefore, the integral becomes:\n$$\n\\int_{0}^{\\infty} \\frac{\\mathrm{d}}{\\mathrm{d}t} \\left(\\exp(A^{\\top} t) X \\exp(A t)\\right) \\, \\mathrm{d}t = \\left[ \\exp(A^{\\top} t) X \\exp(A t) \\right]_{0}^{\\infty}\n$$\nSince $A$ is stable, $\\lim_{t \\to \\infty} \\exp(A t) = 0$. The expression evaluates to:\n$$\n\\lim_{t \\to \\infty} \\left(\\exp(A^{\\top} t) X \\exp(A t)\\right) - \\exp(A^{\\top} \\cdot 0) X \\exp(A \\cdot 0) = 0 - I X I = -X\n$$\nEquating the two sides, we have $-X = -W_o$, which implies $X = W_o$. Thus, the observability Gramian $W_o$ is indeed the solution to the Lyapunov equation $A^{\\top} W_o + W_o A = -C^{\\top} C$. This is the requested equivalent characterization.\n\n### Computation of $W_o$\nWe now solve the Lyapunov equation for $W_o$. First, we compute the matrices involved:\n$A^{\\top} = \\begin{pmatrix} 0 & -1 \\\\ 1 & -2 \\end{pmatrix}$\n$C^{\\top} C = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix}$\nLet $W_o = \\begin{pmatrix} w_{11} & w_{12} \\\\ w_{21} & w_{22} \\end{pmatrix}$. Since $W_o$ must be symmetric, $w_{12} = w_{21}$.\nThe Lyapunov equation is:\n$$\n\\begin{pmatrix} 0 & -1 \\\\ 1 & -2 \\end{pmatrix} \\begin{pmatrix} w_{11} & w_{12} \\\\ w_{12} & w_{22} \\end{pmatrix} + \\begin{pmatrix} w_{11} & w_{12} \\\\ w_{12} & w_{22} \\end{pmatrix} \\begin{pmatrix} 0 & 1 \\\\ -1 & -2 \\end{pmatrix} = - \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix}\n$$\nPerforming the matrix multiplications:\n$$\n\\begin{pmatrix} -w_{12} & -w_{22} \\\\ w_{11}-2w_{12} & w_{12}-2w_{22} \\end{pmatrix} + \\begin{pmatrix} -w_{12} & w_{11}-2w_{12} \\\\ -w_{22} & w_{12}-2w_{22} \\end{pmatrix} = \\begin{pmatrix} -1 & 0 \\\\ 0 & 0 \\end{pmatrix}\n$$\nSumming the matrices on the left-hand side gives:\n$$\n\\begin{pmatrix} -2w_{12} & w_{11}-2w_{12}-w_{22} \\\\ w_{11}-2w_{12}-w_{22} & 2w_{12}-4w_{22} \\end{pmatrix} = \\begin{pmatrix} -1 & 0 \\\\ 0 & 0 \\end{pmatrix}\n$$\nEquating the corresponding elements yields a system of linear equations:\n1.  $-2w_{12} = -1$\n2.  $w_{11}-2w_{12}-w_{22} = 0$\n3.  $2w_{12}-4w_{22} = 0$\n\nFrom equation (1), we find $w_{12} = \\frac{1}{2}$.\nSubstituting this into equation (3): $2(\\frac{1}{2}) - 4w_{22} = 0 \\implies 1 - 4w_{22} = 0 \\implies w_{22} = \\frac{1}{4}$.\nSubstituting $w_{12}$ and $w_{22}$ into equation (2): $w_{11} - 2(\\frac{1}{2}) - \\frac{1}{4} = 0 \\implies w_{11} - 1 - \\frac{1}{4} = 0 \\implies w_{11} = \\frac{5}{4}$.\nSo, the observability Gramian is:\n$$\nW_o = \\begin{pmatrix} \\frac{5}{4} & \\frac{1}{2} \\\\ \\frac{1}{2} & \\frac{1}{4} \\end{pmatrix}\n$$\n\n### Observability Check\nThe pair $(C,A)$ is observable if and only if the observability Gramian $W_o$ is positive definite. For a $2 \\times 2$ symmetric matrix, positive definiteness is confirmed if its leading principal minors are positive (Sylvester's criterion).\nThe first leading principal minor is the top-left element:\n$$\nM_1 = w_{11} = \\frac{5}{4} > 0\n$$\nThe second leading principal minor is the determinant of the matrix:\n$$\nM_2 = \\det(W_o) = w_{11}w_{22} - w_{12}^2 = \\left(\\frac{5}{4}\\right)\\left(\\frac{1}{4}\\right) - \\left(\\frac{1}{2}\\right)^2 = \\frac{5}{16} - \\frac{1}{4} = \\frac{5}{16} - \\frac{4}{16} = \\frac{1}{16}\n$$\nSince $M_2 = \\det(W_o) > 0$, and $M_1 > 0$, the matrix $W_o$ is positive definite. Therefore, the system $(C,A)$ is observable.\n\nThe problem asks for the exact value of $\\det(W_o)$. As calculated above, this value is $\\frac{1}{16}$.", "answer": "$$\\boxed{\\frac{1}{16}}$$", "id": "3421940"}, {"introduction": "Moving beyond simple state estimation, many real-world applications require simultaneous estimation of unknown system parameters. This practice explores the crucial distinction between state observability and parameter identifiability within an augmented state-space framework [@problem_id:3421916]. You will discover how structural properties in the measurement model can render parameters unidentifiable, even when the underlying state dynamics are fully observable, a common challenge in inverse problems.", "problem": "Consider the following linear, time-invariant, discrete-time state-space model used in inverse problems and data assimilation for joint state-parameter estimation via state augmentation. The state evolves according to $x_{k+1} = A x_k$ and the observation is $y_k = C x_k + C_{\\theta} \\theta$, where $x_k \\in \\mathbb{R}^{2}$, $y_k \\in \\mathbb{R}^{2}$, and $\\theta \\in \\mathbb{R}^{2}$ is a constant parameter vector. Let\n$$\nA = \\begin{pmatrix} 0 & 1 \\\\ -2 & -3 \\end{pmatrix}, \\quad\nC = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}, \\quad\nC_{\\theta} = b\\, g^{\\top}, \\quad b = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}, \\quad g = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}.\n$$\nYou may assume the standard definitions: for a linear time-invariant system, the pair $(A, C)$ is observable if the classical observability matrix built from successive products of $C$ and $A$ has full column rank equal to the state dimension, and in an augmented formulation for joint state-parameter estimation, the local linear measurement mapping at time $k$ has Jacobian $[\\, C A^{k} \\;\\; C_{\\theta} \\,]$ with respect to the augmented vector $[\\, x_0^{\\top} \\;\\; \\theta^{\\top} \\,]^{\\top}$. Work with two consecutive observation times $k = 0$ and $k = 1$.\n\n1. Starting from these definitions, argue from first principles whether the states are observable for the given $(A, C)$.\n2. Explain why $C_{\\theta}$ has collinear columns and interpret the implication for parameter identifiability.\n3. Construct the augmented observability matrix by stacking the two local Jacobians for $k = 0$ and $k = 1$, and use rank-nullity to quantify the dimension of its nullspace.\n\nProvide as your final answer a single integer equal to the dimension of the nullspace of this $4 \\times 4$ augmented observability matrix. No units are required. Do not round.", "solution": "The problem asks for an analysis of a discrete-time state-space model with respect to state observability and parameter identifiability, culminating in the determination of the dimension of the nullspace of an augmented observability matrix.\n\nThe system is described by the equations:\n$$x_{k+1} = A x_k$$\n$$y_k = C x_k + C_{\\theta} \\theta$$\nwhere the state is $x_k \\in \\mathbb{R}^2$, the observation is $y_k \\in \\mathbb{R}^2$, and the parameter vector is $\\theta \\in \\mathbb{R}^2$. The matrices are given as:\n$$\nA = \\begin{pmatrix} 0 & 1 \\\\ -2 & -3 \\end{pmatrix}, \\quad\nC = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} = I_2, \\quad\nC_{\\theta} = b g^{\\top}, \\quad b = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}, \\quad g = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}.\n$$\n\nFirst, let's address the three parts of the problem sequentially.\n\n1.  **Observability of the state dynamics.**\nA linear time-invariant system $(A, C)$ is defined as observable if any initial state $x_0$ can be uniquely determined from a sequence of observations $y_0, y_1, \\dots$. From the state equation, we have $x_k = A^k x_0$. Assuming the parameters are known ($\\theta=0$ for this subproblem), the observation equation becomes $y_k = C x_k = C A^k x_0$. If two initial states, $x_0^{(1)}$ and $x_0^{(2)}$, produce identical output sequences, then $C A^k x_0^{(1)} = C A^k x_0^{(2)}$ for all $k \\ge 0$. This is equivalent to $C A^k (x_0^{(1)} - x_0^{(2)}) = 0$ for all $k \\ge 0$. The system is observable if and only if the only vector $\\delta x_0 = x_0^{(1)} - x_0^{(2)}$ that satisfies this condition is $\\delta x_0 = 0$. This requires the infinite observability matrix to have a trivial nullspace. For a system with state dimension $n$, the Cayley-Hamilton theorem implies that we only need to check the first $n$ time steps. The classical observability matrix is therefore defined as:\n$$ \\mathcal{O} = \\begin{pmatrix} C \\\\ CA \\\\ \\vdots \\\\ CA^{n-1} \\end{pmatrix} $$\nThe system is observable if and only if this matrix has full column rank, which is $\\text{rank}(\\mathcal{O}) = n$.\n\nIn this problem, the state dimension is $n=2$. The observability matrix is:\n$$ \\mathcal{O} = \\begin{pmatrix} C \\\\ CA \\end{pmatrix} $$\nWe are given $C = I_2$, so $CA = A$. The observability matrix is:\n$$ \\mathcal{O} = \\begin{pmatrix} I_2 \\\\ A \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 0 & 1 \\\\ -2 & -3 \\end{pmatrix} $$\nThis is a $4 \\times 2$ matrix. To check for full column rank ($2$), we must check if its columns are linearly independent. The two columns are $v_1 = \\begin{pmatrix} 1 & 0 & 0 & -2 \\end{pmatrix}^{\\top}$ and $v_2 = \\begin{pmatrix} 0 & 1 & 1 & -3 \\end{pmatrix}^{\\top}$. These vectors are not scalar multiples of each other, so they are linearly independent. Therefore, $\\text{rank}(\\mathcal{O}) = 2$. Since the rank equals the state dimension, the pair $(A, C)$ is observable.\n\n2.  **Analysis of the matrix $C_{\\theta}$ and its implications.**\nThe matrix $C_{\\theta}$ is defined by the outer product of two vectors, $b$ and $g$:\n$$ C_{\\theta} = b g^{\\top} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} \\begin{pmatrix} 1 & 2 \\end{pmatrix} = \\begin{pmatrix} 1 \\times 1 & 1 \\times 2 \\\\ 1 \\times 1 & 1 \\times 2 \\end{pmatrix} = \\begin{pmatrix} 1 & 2 \\\\ 1 & 2 \\end{pmatrix} $$\nAny outer product of two non-zero vectors results in a rank-$1$ matrix. A property of a rank-$1$ matrix is that all its columns are scalar multiples of a single vector (in this case, $b$), and all its rows are scalar multiples of a single vector (in this case, $g^{\\top}$). Here, the first column is $\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$ and the second column is $\\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix} = 2 \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$. The columns are thus collinear.\n\nThe implication for parameter identifiability stems from how the parameter vector $\\theta$ affects the observation $y_k$. The parameter-dependent part of the observation is $C_{\\theta} \\theta$:\n$$ C_{\\theta} \\theta = \\begin{pmatrix} 1 & 2 \\\\ 1 & 2 \\end{pmatrix} \\begin{pmatrix} \\theta_1 \\\\ \\theta_2 \\end{pmatrix} = \\begin{pmatrix} \\theta_1 + 2\\theta_2 \\\\ \\theta_1 + 2\\theta_2 \\end{pmatrix} = (\\theta_1 + 2\\theta_2) \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} $$\nThis expression shows that the parameter vector $\\theta = \\begin{pmatrix} \\theta_1 & \\theta_2 \\end{pmatrix}^{\\top}$ influences the measurements only through the linear combination $\\theta_1 + 2\\theta_2 = g^{\\top}\\theta$. We cannot distinguish between any two distinct parameter vectors $\\theta^{(1)}$ and $\\theta^{(2)}$ for which $g^{\\top}\\theta^{(1)} = g^{\\top}\\theta^{(2)}$. This means we cannot identify $\\theta_1$ and $\\theta_2$ individually. Any change to $\\theta$ that lies in the nullspace of $C_{\\theta}$ (or equivalently, is orthogonal to $g$) will be unobservable. The nullspace of $C_{\\theta}$ is one-dimensional, spanned by the vector $\\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix}$. Thus, the parameters are not identifiable.\n\n3.  **Augmented observability matrix and its nullspace dimension.**\nThe goal is to estimate the initial state $x_0$ and the parameter vector $\\theta$ simultaneously. The augmented vector is $\\begin{pmatrix} x_0^{\\top} & \\theta^{\\top} \\end{pmatrix}^{\\top} \\in \\mathbb{R}^4$. The observation at time $k$ is given by $y_k = C A^k x_0 + C_{\\theta} \\theta$. We can write this in matrix form:\n$$ y_k = \\begin{pmatrix} C A^k & C_{\\theta} \\end{pmatrix} \\begin{pmatrix} x_0 \\\\ \\theta \\end{pmatrix} $$\nThe matrix $\\begin{pmatrix} C A^k & C_{\\theta} \\end{pmatrix}$ is the Jacobian of the measurement function $y_k$ with respect to the augmented vector. We consider two consecutive observations at $k=0$ and $k=1$. We stack these observations:\n$$ \\mathcal{Y} = \\begin{pmatrix} y_0 \\\\ y_1 \\end{pmatrix} = \\begin{pmatrix} C A^0 & C_{\\theta} \\\\ C A^1 & C_{\\theta} \\end{pmatrix} \\begin{pmatrix} x_0 \\\\ \\theta \\end{pmatrix} $$\nThe augmented observability matrix, which we denote $\\mathcal{O}_{\\text{aug}}$, is the $4 \\times 4$ matrix relating the augmented vector to the stacked observations:\n$$ \\mathcal{O}_{\\text{aug}} = \\begin{pmatrix} C & C_{\\theta} \\\\ CA & C_{\\theta} \\end{pmatrix} $$\nSubstituting the given matrices with $C=I_2$, $A^0=I_2$, $A^1=A$:\n$$ \\mathcal{O}_{\\text{aug}} = \\begin{pmatrix} 1 & 0 & 1 & 2 \\\\ 0 & 1 & 1 & 2 \\\\ 0 & 1 & 1 & 2 \\\\ -2 & -3 & 1 & 2 \\end{pmatrix} $$\nWe need to find the dimension of the nullspace of this matrix. By the rank-nullity theorem, for a matrix $M \\in \\mathbb{R}^{m \\times n}$, we have $\\text{rank}(M) + \\text{dim}(\\text{null}(M)) = n$. Here, $n=4$. So, $\\text{dim}(\\text{null}(\\mathcal{O}_{\\text{aug}})) = 4 - \\text{rank}(\\mathcal{O}_{\\text{aug}})$.\n\nWe compute the rank of $\\mathcal{O}_{\\text{aug}}$ using Gaussian elimination. Notice that Row 2 and Row 3 are identical.\n$$ \\mathcal{O}_{\\text{aug}} = \\begin{pmatrix} 1 & 0 & 1 & 2 \\\\ 0 & 1 & 1 & 2 \\\\ 0 & 1 & 1 & 2 \\\\ -2 & -3 & 1 & 2 \\end{pmatrix} \\xrightarrow{R_3 \\to R_3-R_2} \\begin{pmatrix} 1 & 0 & 1 & 2 \\\\ 0 & 1 & 1 & 2 \\\\ 0 & 0 & 0 & 0 \\\\ -2 & -3 & 1 & 2 \\end{pmatrix} $$\n$$ \\xrightarrow{R_4 \\to R_4+2R_1} \\begin{pmatrix} 1 & 0 & 1 & 2 \\\\ 0 & 1 & 1 & 2 \\\\ 0 & 0 & 0 & 0 \\\\ 0 & -3 & 3 & 6 \\end{pmatrix} \\xrightarrow{R_4 \\to R_4+3R_2} \\begin{pmatrix} 1 & 0 & 1 & 2 \\\\ 0 & 1 & 1 & 2 \\\\ 0 & 0 & 0 & 0 \\\\ 0 & 0 & 6 & 12 \\end{pmatrix} $$\nSwapping Row 3 and Row 4 gives the row echelon form:\n$$ \\begin{pmatrix} 1 & 0 & 1 & 2 \\\\ 0 & 1 & 1 & 2 \\\\ 0 & 0 & 6 & 12 \\\\ 0 & 0 & 0 & 0 \\end{pmatrix} $$\nThis matrix clearly has three non-zero rows, so its rank is $3$.\n$$ \\text{rank}(\\mathcal{O}_{\\text{aug}}) = 3 $$\nApplying the rank-nullity theorem:\n$$ \\text{dim}(\\text{null}(\\mathcal{O}_{\\text{aug}})) = 4 - \\text{rank}(\\mathcal{O}_{\\text{aug}}) = 4 - 3 = 1 $$\nThe dimension of the nullspace is $1$. This non-trivial nullspace reflects the lack of full identifiability of the augmented state vector, originating from the rank-deficiency of $C_{\\theta}$ as established in part 2. Specifically, a vector $\\begin{pmatrix} x_0 \\\\ \\theta \\end{pmatrix}$ is in the nullspace if $x_0=0$ and $\\theta$ is in the nullspace of $C_\\theta$. The nullspace of $C_\\theta$ is one-dimensional, hence the nullspace of $\\mathcal{O}_{\\text{aug}}$ is also one-dimensional.", "answer": "$$\\boxed{1}$$", "id": "3421916"}, {"introduction": "In practice, observability is not just a binary property but a quantitative one; some states may be \"more observable\" than others. This computational exercise shifts our focus from a simple yes/no answer to quantifying the degree of observability using the singular values of the observability matrix [@problem_id:3421917]. By numerically analyzing how observability evolves over time for systems with poorly observed modes, you will develop an intuition for the practical challenges of state estimation in real-world scenarios.", "problem": "Consider a discrete-time, linear, time-invariant state-space model with state dimension $n$ and output dimension $p$, defined by the equations $x_{k+1} = A x_k$ and $y_k = C x_k$, where $A \\in \\mathbb{R}^{n \\times n}$ and $C \\in \\mathbb{R}^{p \\times n}$. The finite-horizon observability matrix of horizon $N$ is defined as the vertical stack\n$$\nO_N \\triangleq \\begin{bmatrix}\nC \\\\\nC A \\\\\n\\vdots \\\\\nC A^{N-1}\n\\end{bmatrix} \\in \\mathbb{R}^{(pN) \\times n}.\n$$\nThe finite-horizon observability Gramian is the symmetric positive semidefinite matrix\n$$\nW_N \\triangleq O_N^\\top O_N = \\sum_{k=0}^{N-1} (A^k)^\\top C^\\top C A^k \\in \\mathbb{R}^{n \\times n}.\n$$\nFor each horizon $N \\in \\{1,2,\\dots,10\\}$, define the quantity $s_{\\min}(N)$ to be the smallest singular value of the observability matrix $O_N$, which is equivalently $s_{\\min}(N) = \\sqrt{\\lambda_{\\min}(W_N)}$, where $\\lambda_{\\min}(W_N)$ denotes the smallest eigenvalue of $W_N$.\n\nYour task is to write a program that, for each of the test cases below, computes the list $\\{s_{\\min}(N)\\}_{N=1}^{10}$ and provides a simple interpretation of its trend. The interpretation must consist of:\n- A boolean indicating whether the sequence $\\{s_{\\min}(N)\\}$ is monotonically nondecreasing up to numerical tolerance $\\tau = 10^{-12}$, that is, whether $s_{\\min}(N+1) \\ge s_{\\min}(N) - \\tau$ holds for all $N \\in \\{1,2,\\dots,9\\}$.\n- A boolean indicating whether the system exhibits an effectively observable subspace over horizon $N=10$ according to the criterion $s_{\\min}(10) > \\epsilon$, with $\\epsilon = 10^{-12}$.\n\nThe program must not assume any specific structure beyond standard matrix algebra, and must rely on the fundamental definitions above. No physical units are involved in this problem. Angles are not used. Percentages are not used.\n\nTest Suite:\n- Case $1$ (happy path, two-dimensional, one poorly observed but observable mode): \n  $$\n  A_1 = \\begin{bmatrix} 0.95 & 0.0 \\\\ 0.0 & 0.70 \\end{bmatrix}, \\quad\n  C_1 = \\begin{bmatrix} 10^{-3} & 1.0 \\end{bmatrix}.\n  $$\n- Case $2$ (boundary condition, two-dimensional, extremely weakly coupled slow mode):\n  $$\n  A_2 = \\begin{bmatrix} 0.99 & 0.0 \\\\ 0.0 & 0.30 \\end{bmatrix}, \\quad\n  C_2 = \\begin{bmatrix} 10^{-8} & 1.0 \\end{bmatrix}.\n  $$\n- Case $3$ (edge case, two-dimensional, one strictly unobservable mode):\n  $$\n  A_3 = \\begin{bmatrix} 0.90 & 0.0 \\\\ 0.0 & 0.60 \\end{bmatrix}, \\quad\n  C_3 = \\begin{bmatrix} 0.0 & 1.0 \\end{bmatrix}.\n  $$\n\nFor each case, your program must:\n1. Construct $W_N$ for each $N \\in \\{1,\\dots,10\\}$ using the definition above, and compute $s_{\\min}(N) = \\sqrt{\\lambda_{\\min}(W_N)}$.\n2. Determine the two booleans described above using the specified tolerances $\\tau$ and $\\epsilon$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element corresponds to one test case and is itself a list of the form $[\\text{values}, \\text{is\\_monotone}, \\text{is\\_observable}]$, where $\\text{values}$ is the list $\\{s_{\\min}(N)\\}_{N=1}^{10}$ of $10$ floating-point numbers, $\\text{is\\_monotone}$ is a boolean, and $\\text{is\\_observable}$ is a boolean. For example, the final output must be of the form:\n$$\n[\\,[\\,[v_1,\\dots,v_{10}], \\text{True}, \\text{False}\\,],\\,[\\,[w_1,\\dots,w_{10}], \\text{True}, \\text{True}\\,],\\,[\\,[u_1,\\dots,u_{10}], \\text{True}, \\text{False}\\,]\\,]\n$$\nwith actual numerical values computed by your program for $v_i$, $w_i$, and $u_i$.", "solution": "The problem is valid as it is mathematically well-defined, scientifically grounded in linear systems theory, and all necessary data and conditions are provided for a unique solution.\n\nThe core of this problem lies in assessing the observability of a discrete-time linear time-invariant system. Observability refers to the ability to determine the initial state $x_0$ of the system by observing its outputs $y_k$ over a certain time horizon.\n\nThe system is defined by the equations:\n$$\nx_{k+1} = A x_k \\\\\ny_k = C x_k\n$$\nwhere $x_k \\in \\mathbb{R}^n$ is the state and $y_k \\in \\mathbb{R}^p$ is the output at time step $k$.\n\nThe sequence of outputs over a horizon of length $N$ is related to the initial state $x_0$ by:\n$$\nY_N = \\begin{bmatrix} y_0 \\\\ y_1 \\\\ \\vdots \\\\ y_{N-1} \\end{bmatrix} = \\begin{bmatrix} C \\\\ C A \\\\ \\vdots \\\\ C A^{N-1} \\end{bmatrix} x_0 = O_N x_0\n$$\nwhere $O_N \\in \\mathbb{R}^{(pN) \\times n}$ is the observability matrix for horizon $N$.\n\nThe quantity we are asked to compute, $s_{\\min}(N)$, is the smallest singular value of this matrix $O_N$. This value is a crucial metric for observability. If $s_{\\min}(N) > 0$, it implies that the matrix $O_N$ has full column rank, meaning that the mapping from $x_0$ to $Y_N$ is injective. In other words, every distinct initial state $x_0$ produces a unique sequence of outputs, and thus $x_0$ can be uniquely determined. If $s_{\\min}(N) = 0$, there exist non-zero initial states that produce zero output over the horizon $N$, making them indistinguishable from the zero state. The magnitude of $s_{\\min}(N)$ quantifies the \"degree\" of observability; a larger value indicates that the system is more robustly observable.\n\nThe problem directs us to compute $s_{\\min}(N)$ using the finite-horizon observability Gramian, $W_N$:\n$$\nW_N \\triangleq O_N^\\top O_N = \\sum_{k=0}^{N-1} (A^k)^\\top C^\\top C A^k\n$$\nThe singular values of $O_N$ are the square roots of the eigenvalues of $O_N^\\top O_N = W_N$. Therefore, the smallest singular value of $O_N$ is given by:\n$$\ns_{\\min}(N) = \\sqrt{\\lambda_{\\min}(W_N)}\n$$\nwhere $\\lambda_{\\min}(W_N)$ is the smallest eigenvalue of the Gramian $W_N$.\n\nA key insight for an efficient algorithm is the iterative nature of the Gramian. We can express $W_N$ in terms of $W_{N-1}$:\n$$\nW_N = \\left( \\sum_{k=0}^{N-2} (A^k)^\\top C^\\top C A^k \\right) + (A^{N-1})^\\top C^\\top C A^{N-1} = W_{N-1} + (A^{N-1})^\\top C^\\top C A^{N-1}\n$$\nThe term $(A^{N-1})^\\top C^\\top C A^{N-1}$ is a symmetric positive semidefinite matrix. According to Weyl's inequality for eigenvalues, adding a positive semidefinite matrix to another symmetric matrix cannot decrease any of its eigenvalues. Thus, $\\lambda_{\\min}(W_N) \\ge \\lambda_{\\min}(W_{N-1})$. Since the square root function is monotonically nondecreasing for non-negative arguments, it follows that:\n$$\ns_{\\min}(N) = \\sqrt{\\lambda_{\\min}(W_N)} \\ge \\sqrt{\\lambda_{\\min}(W_{N-1})} = s_{\\min}(N-1)\n$$\nThis proves that the sequence $\\{s_{\\min}(N)\\}$ is theoretically monotonically nondecreasing. The check for $s_{\\min}(N+1) \\ge s_{\\min}(N) - \\tau$ with a small tolerance $\\tau > 0$ correctly accounts for potential minor decreases due to floating-point arithmetic errors.\n\nThe algorithm for each test case proceeds as follows:\n1.  Initialize the Gramian $W$ as a zero matrix of size $n \\times n$ and the matrix power $A^k$ as the identity matrix (for $k=0$).\n2.  Pre-compute the constant matrix product $C^\\top C$.\n3.  Loop for each horizon $N$ from $1$ to $10$ (which corresponds to summing terms from $k=0$ to $9$). In each iteration $k$ of the sum (corresponding to horizon $N=k+1$):\n    a.  Add the next term, $(A^k)^\\top C^\\top C A^k$, to the accumulated Gramian $W$.\n    b.  Compute the eigenvalues of the updated symmetric Gramian $W$. The smallest eigenvalue, $\\lambda_{\\min}$, is found.\n    c.  Calculate $s_{\\min}(N) = \\sqrt{\\max(0, \\lambda_{\\min})}$. The `max` function provides numerical stability against potential small negative values for $\\lambda_{\\min}$ arising from floating-point inaccuracies.\n    d.  Store the computed $s_{\\min}(N)$.\n    e.  Update the matrix power from $A^k$ to $A^{k+1}$ for the next iteration.\n4.  After computing the full sequence $\\{s_{\\min}(N)\\}_{N=1}^{10}$, perform the two required boolean checks:\n    a.  **Monotonicity**: Iterate through the sequence to verify if $s_{\\min}(N+1) \\ge s_{\\min}(N) - \\tau$ for $N=1, \\dots, 9$.\n    b.  **Observability**: Check if the final value, $s_{\\min}(10)$, is greater than the threshold $\\epsilon = 10^{-12}$.\n\nThis procedure is implemented for each of the three test cases provided.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef compute_observability_metrics(A_in, C_in):\n    \"\"\"\n    Computes observability metrics for a given LTI system (A, C).\n\n    Args:\n        A_in (list of lists): The state transition matrix A.\n        C_in (list of lists): The observation matrix C.\n\n    Returns:\n        list: A list containing [s_min_values, is_monotone, is_observable].\n              - s_min_values: A list of the smallest singular values s_min(N) for N=1 to 10.\n              - is_monotone: A boolean indicating if the sequence is nondecreasing.\n              - is_observable: A boolean indicating if the system is effectively observable at N=10.\n    \"\"\"\n    N_max = 10\n    tau = 1e-12\n    epsilon = 1e-12\n\n    A = np.array(A_in, dtype=float)\n    C = np.array(C_in, dtype=float)\n    \n    n = A.shape[0]\n    W = np.zeros((n, n), dtype=float)\n    A_k = np.eye(n, dtype=float)\n    CTC = C.T @ C\n    \n    s_min_values = []\n\n    # Iteratively compute W_N and s_min(N) for N=1 to 10\n    for _ in range(N_max):\n        # The term for the current power of A is (A^k)^T C^T C (A^k)\n        term = A_k.T @ CTC @ A_k\n        W += term  # This gives W_{k+1}\n        \n        # W is symmetric, so eigh is efficient and returns sorted eigenvalues\n        eigenvalues = np.linalg.eigh(W)[0]\n        lambda_min = eigenvalues[0]\n        \n        # Guard against small negative eigenvalues from floating point error\n        s_min = np.sqrt(max(0.0, lambda_min))\n        s_min_values.append(s_min)\n        \n        # Update A^k to A^{k+1} for the next iteration\n        A_k = A @ A_k\n\n    # Check for monotonicity with tolerance tau\n    is_monotone = True\n    for i in range(N_max - 1):\n        if s_min_values[i+1]  s_min_values[i] - tau:\n            is_monotone = False\n            break\n\n    # Check for effective observability at N=10 with threshold epsilon\n    # s_min_values[9] corresponds to N=10\n    is_observable = s_min_values[9] > epsilon\n\n    return [s_min_values, is_monotone, is_observable]\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final result.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (\n            [[0.95, 0.0], [0.0, 0.70]],\n            [[1e-3, 1.0]]\n        ),\n        (\n            [[0.99, 0.0], [0.0, 0.30]],\n            [[1e-8, 1.0]]\n        ),\n        (\n            [[0.90, 0.0], [0.0, 0.60]],\n            [[0.0, 1.0]]\n        )\n    ]\n\n    results = []\n    for A, C in test_cases:\n        res = compute_observability_metrics(A, C)\n        results.append(res)\n\n    # Format the output string to match the specified no-space format\n    formatted_results = []\n    for res in results:\n        # res has the form [[values], bool, bool]\n        # Format the list of float values\n        values_str = f\"[{','.join(f'{v:.17g}' for v in res[0])}]\"\n        \n        # Format the entire case result string\n        case_str = f\"[{values_str},{res[1]},{res[2]}]\"\n        formatted_results.append(case_str)\n        \n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3421917"}]}