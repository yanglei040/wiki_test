{"hands_on_practices": [{"introduction": "The foundation of any estimation problem is a well-posed objective function that uniquely specifies a solution. This exercise provides a hands-on exploration of unobservability, where the objective is ill-posed, by connecting it to the nullspace of the posterior precision matrix in a linear smoothing context. You will learn to diagnose these unobserved modes and regularize the problem by introducing a minimal set of new observations. [@problem_id:3406074]", "problem": "You are asked to formalize and compute unobservability in linear-Gaussian smoothing objectives by constructing examples with large nullspaces, and then to propose and verify minimal additional observations that regularize the posterior. Work entirely in finite-dimensional linear algebra, treating the full space-time trajectory as a single stacked vector and all penalties as quadratic forms.\n\nConsider a discrete-time state sequence $\\{x_t\\}_{t=0}^T$ with $x_t \\in \\mathbb{R}^n$. Stack the trajectory as $z \\in \\mathbb{R}^{n(T+1)}$, where $z = [x_0^\\top, x_1^\\top, \\dots, x_T^\\top]^\\top$. A quadratic smoothing objective can be written as\n$$\nJ(z) \\;=\\; \\tfrac{1}{2}\\,\\|L z\\|_2^2 \\;+\\; \\tfrac{1}{2}\\,\\|C z - y\\|_2^2,\n$$\nwhere $L$ encodes smoothing or dynamical penalties and $C$ encodes available linear observations. Under a linear-Gaussian Bayesian interpretation, the Maximum A Posteriori (MAP) estimator coincides with the minimizer of $J(z)$, and the Hessian (posterior precision) is\n$$\n\\mathcal{I} \\;=\\; L^\\top L \\;+\\; C^\\top C.\n$$\nUnobservability is present when $\\mathcal{I}$ is singular; its nullspace $\\mathcal{N}(\\mathcal{I})$ identifies directions that are not penalized by dynamics/smoothing nor by the observations. The dimension of $\\mathcal{N}(\\mathcal{I})$ is the nullity of $\\mathcal{I}$, equivalently $n(T+1) - \\mathrm{rank}(\\mathcal{I})$.\n\nYou will study two canonical smoothing operators that yield large nullspaces:\n\n- First-difference (random-walk) smoothing: for $t \\in \\{0,\\dots,T-1\\}$, penalize $x_{t+1} - x_t$. This corresponds to $L \\in \\mathbb{R}^{nT \\times n(T+1)}$ with block structure such that the $t$-th row-block equals $[0,\\dots,0,-I_n, I_n, 0,\\dots,0]$ acting on $[x_t^\\top, x_{t+1}^\\top]^\\top$.\n- Second-difference (acceleration) smoothing: for $t \\in \\{1,\\dots,T-1\\}$, penalize $x_{t+1} - 2 x_t + x_{t-1}$. This corresponds to $L \\in \\mathbb{R}^{n(T-1) \\times n(T+1)}$ with block structure such that the $t$-th row-block equals $[0,\\dots,0, I_n, -2 I_n, I_n, 0,\\dots,0]$ acting on $[x_{t-1}^\\top, x_t^\\top, x_{t+1}^\\top]^\\top$.\n\nAssume all penalties are isotropic with identity weights for simplicity, so the $2$-norms above are Euclidean norms. Observations are pointwise component measurements: if you observe component indices $S \\subset \\{0,\\dots,n-1\\}$ of $x_t$ at time $t$, then $C$ contains one row $e_{t,i}^\\top$ per $(t,i) \\in \\{t\\} \\times S$, where $e_{t,i} \\in \\mathbb{R}^{n(T+1)}$ is the canonical basis vector selecting the $i$-th component at time $t$ (that is, it has a $1$ at the position corresponding to $(t,i)$ and $0$ elsewhere). Take measurement noise precision equal to identity, so $C^\\top C$ contributes directly to $\\mathcal{I}$.\n\nYour program must, for each specified test case:\n- Construct $L$ as either a first-difference or second-difference operator for the given $n$ and $T$.\n- Construct $C$ from the provided set of observed components at specified times.\n- Compute the nullity $k_0$ of $\\mathcal{I} = L^\\top L + C^\\top C$.\n- Propose a minimal set of additional linear observations to regularize the posterior. You must implement this by computing a basis $\\{u_j\\}_{j=1}^{k_0}$ for $\\mathcal{N}(\\mathcal{I})$ and taking the rows of the additional observation operator $C_{\\mathrm{add}}$ to be $u_j^\\top$. This choice uses $k_0$ scalar observations and guarantees that the updated precision\n$$\n\\mathcal{I}_{\\mathrm{new}} \\;=\\; \\mathcal{I} \\;+\\; C_{\\mathrm{add}}^\\top C_{\\mathrm{add}}\n$$\nis positive definite when $k_0 > 0$, and remains unchanged when $k_0 = 0$.\n- Set $k_{\\min} = k_0$ as the theoretically minimal number of additional scalar observations required, and verify success by checking that the nullity of $\\mathcal{I}_{\\mathrm{new}}$ is $0$.\n\nNumerical linear algebra hint for implementation (not a solution method shortcut): use symmetric eigendecomposition of $\\mathcal{I}$ to obtain its eigenvalues and eigenvectors, with a numerically reasonable threshold to define “zero” eigenvalues.\n\nTest suite. Your program must solve the following cases:\n\n- Case A: first-difference, $n = 3$, $T = 5$, no observations.\n- Case B: first-difference, $n = 3$, $T = 5$, observations at time $t = 0$ of components $\\{0,1\\}$.\n- Case C: second-difference, $n = 2$, $T = 6$, no observations.\n- Case D: second-difference, $n = 2$, $T = 6$, observations at time $t = 0$ of components $\\{0,1\\}$.\n- Case E: second-difference, $n = 2$, $T = 3$, observations at times $t = 0$ and $t = 3$ of component $\\{0\\}$ (that is, observe component $0$ at both endpoints).\n- Case F: second-difference, $n = 1$, $T = 4$, observations at times $t = 0$ and $t = 4$ of component $\\{0\\}$.\n\nRequired outputs. For each case, output a list $[k_0, k_{\\min}, s]$ where $k_0$ is the computed nullity of $\\mathcal{I}$, $k_{\\min}$ is the minimal number of additional scalar observations you propose (equal to $k_0$ by construction), and $s$ is a boolean indicating whether the updated precision $\\mathcal{I}_{\\mathrm{new}}$ is numerically full rank (nullity equal to $0$). Your program should produce a single line of output containing the results for Cases A–F as a comma-separated list of these lists, enclosed in square brackets; for example, a valid format is like $[[r_1],[r_2],\\dots]$ where each $[r_j]$ is the triplet for case $j$ in the specified order A–F.", "solution": "The problem requires an analysis of unobservability in linear smoothing problems by constructing and regularizing systems with singular posterior precision matrices. The entire space-time trajectory of the state $\\{x_t\\}_{t=0}^T$, where $x_t \\in \\mathbb{R}^n$, is represented as a single stacked vector $z = [x_0^\\top, x_1^\\top, \\dots, x_T^\\top]^\\top \\in \\mathbb{R}^{n(T+1)}$.\n\nThe smoothing problem is framed as the minimization of a quadratic objective function $J(z)$:\n$$\nJ(z) = \\tfrac{1}{2} \\|L z\\|_2^2 + \\tfrac{1}{2} \\|C z - y\\|_2^2\n$$\nThe first term corresponds to a prior or smoothing penalty, encoded by the matrix $L$. The second term represents the penalty for mismatching a set of linear observations $y$, encoded by the matrix $C$. In a Bayesian context, this corresponds to finding the Maximum A Posteriori (MAP) estimate for a linear-Gaussian model. The posterior precision matrix, which is the Hessian of $J(z)$, is given by:\n$$\n\\mathcal{I} = L^\\top L + C^\\top C\n$$\nUnobservability arises when $\\mathcal{I}$ is singular, meaning it has a non-trivial nullspace, $\\mathcal{N}(\\mathcal{I})$. The dimension of this nullspace, its nullity, quantifies the number of independent directions in the state space that are not constrained by either the dynamics (smoothing) or the observations. Any vector $v \\in \\mathcal{N}(\\mathcal{I})$ can be added to a solution $z^*$ without changing the cost $J$, since for any $v \\in \\mathcal{N}(\\mathcal{I})$, $v$ must be in $\\mathcal{N}(L)$ and $\\mathcal{N}(C)$ (assuming $y=0$ for the nullspace analysis), thus $L(\\alpha v)=0$ and $C(\\alpha v)=0$.\n\nThe procedure to solve each test case involves the following steps:\n\n1.  **Construct the Prior Precision Matrix $L^\\top L$**:\n    The total dimension of the problem is $d = n(T+1)$. The matrix $L^\\top L$ is a $d \\times d$ matrix.\n    -   For a **first-difference** smoother, the penalty is on $x_{t+1} - x_t$ for $t=0, \\dots, T-1$. This implies that the nullspace of $L$ consists of trajectories where $x_t$ is constant for all $t$. Specifically, if $z \\in \\mathcal{N}(L)$, then $x_0 = x_1 = \\dots = x_T$. This is an $n$-dimensional nullspace, spanned by trajectories of the form $[v^\\top, v^\\top, \\dots, v^\\top]^\\top$ for any $v \\in \\mathbb{R}^n$. The matrix $L$ can be constructed with $n T$ rows, where each block of $n$ rows corresponding to time $t$ has a $-I_n$ block at column block $t$ and an $I_n$ block at column block $t+1$.\n    -   For a **second-difference** smoother, the penalty is on $x_{t+1} - 2x_t + x_{t-1}$ for $t=1, \\dots, T-1$. The nullspace of $L$ consists of trajectories that evolve linearly with time, i.e., $x_t = x_0 + t(x_1 - x_0)$. Such a trajectory is fully determined by the initial two states $x_0$ and $x_1$. This constitutes a $2n$-dimensional nullspace. The matrix $L$ is constructed with $n(T-1)$ rows, where each block of $n$ rows for time $t$ has blocks $I_n, -2I_n, I_n$ at column blocks $t-1, t, t+1$ respectively.\n    In both cases, we compute $L$ and then form the prior precision $L^\\top L$.\n\n2.  **Construct the Observation Precision Matrix $C^\\top C$**:\n    Observations are pointwise measurements of specific components. An observation of the $i$-th component of $x_t$ corresponds to a row in $C$ given by $e_{t,i}^\\top$, where $e_{t,i}$ is the canonical basis vector in $\\mathbb{R}^{n(T+1)}$ with a $1$ at the position corresponding to the $i$-th component of $x_t$ and zeros elsewhere. With an identity measurement noise precision, the total contribution from observations to the posterior precision is $C^\\top C = \\sum_{(t,i) \\in \\text{obs}} e_{t,i} e_{t,i}^\\top$. This is a diagonal matrix with $1$s at indices corresponding to the observed components. We add this matrix to $L^\\top L$ to form the full posterior precision $\\mathcal{I}$. The global index for component $i$ of state $x_t$ is $t \\cdot n + i$.\n\n3.  **Compute Initial Nullity ($k_0$)**:\n    The posterior precision matrix $\\mathcal{I}$ is real and symmetric by construction. We compute its eigenvalues and eigenvectors using a symmetric eigensolver (`numpy.linalg.eigh`). The nullity $k_0$ is the number of eigenvalues that are numerically close to zero, judged by a small tolerance (e.g., $10^{-9}$). The corresponding eigenvectors $\\{u_j\\}_{j=1}^{k_0}$ form an orthonormal basis for the nullspace $\\mathcal{N}(\\mathcal{I})$.\n\n4.  **Propose and Apply Regularization**:\n    To make the problem well-posed, we must add new observations that penalize the unobserved modes. The problem specifies a minimal regularization strategy. We introduce $k_{\\min} = k_0$ new scalar observations. The new observation operator, $C_{\\mathrm{add}}$, is defined by taking its rows to be the basis vectors of the nullspace, i.e., $C_{\\mathrm{add}} = [u_1, u_2, \\dots, u_{k_0}]^\\top$. The contribution to the precision matrix is $C_{\\mathrm{add}}^\\top C_{\\mathrm{add}}$. Since the basis vectors $\\{u_j\\}$ are orthonormal, $C_{\\mathrm{add}}^\\top C_{\\mathrm{add}} = \\sum_{j=1}^{k_0} u_j u_j^\\top$, which is the projection matrix onto $\\mathcal{N}(\\mathcal{I})$.\n    The new, regularized precision matrix is:\n    $$\n    \\mathcal{I}_{\\mathrm{new}} = \\mathcal{I} + C_{\\mathrm{add}}^\\top C_{\\mathrm{add}}\n    $$\n\n5.  **Verify Regularization**:\n    We verify the success of the regularization by computing the nullity of $\\mathcal{I}_{\\mathrm{new}}$. For any vector $v$ in the original nullspace, $v = \\sum_k \\alpha_k u_k$. Then $\\mathcal{I}v=0$. The new matrix acts as:\n    $$\n    \\mathcal{I}_{\\mathrm{new}} v = (\\mathcal{I} + \\sum_{j=1}^{k_0} u_j u_j^\\top) (\\sum_{k=1}^{k_0} \\alpha_k u_k) = 0 + \\sum_{j,k} \\alpha_k u_j (u_j^\\top u_k) = \\sum_{j,k} \\alpha_k u_j \\delta_{jk} = \\sum_k \\alpha_k u_k = v\n    $$\n    This shows that the vectors spanning the original nullspace are now eigenvectors of $\\mathcal{I}_{\\mathrm{new}}$ with eigenvalue $1$. For any eigenvector $w$ of $\\mathcal{I}$ with non-zero eigenvalue $\\lambda$ (so $w \\perp \\mathcal{N}(\\mathcal{I})$), we have $\\mathcal{I}_{\\mathrm{new}} w = \\mathcal{I}w + (\\sum u_j u_j^\\top)w = \\lambda w + 0 = \\lambda w$. The other eigenvalues and eigenvectors remain unchanged. Thus, $\\mathcal{I}_{\\mathrm{new}}$ has no zero eigenvalues and is positive definite. We confirm this numerically by computing the eigenvalues of $\\mathcal{I}_{\\mathrm{new}}$ and checking that its nullity is $0$. The success is recorded as a boolean value $s$.\n\nThis complete procedure is implemented for each of the specified test cases.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the series of test cases for analyzing unobservability\n    in linear-Gaussian smoothing problems.\n    \"\"\"\n    \n    # Test suite as specified in the problem statement\n    test_cases = [\n        # Case A: first-difference, n=3, T=5, no observations\n        {'type': 'first_difference', 'n': 3, 'T': 5, 'obs': []},\n        # Case B: first-difference, n=3, T=5, obs at t=0 of components {0,1}\n        {'type': 'first_difference', 'n': 3, 'T': 5, 'obs': [(0, [0, 1])]},\n        # Case C: second-difference, n=2, T=6, no observations\n        {'type': 'second_difference', 'n': 2, 'T': 6, 'obs': []},\n        # Case D: second-difference, n=2, T=6, obs at t=0 of components {0,1}\n        {'type': 'second_difference', 'n': 2, 'T': 6, 'obs': [(0, [0, 1])]},\n        # Case E: second-difference, n=2, T=3, obs at t=0, t=3 of component {0}\n        {'type': 'second_difference', 'n': 2, 'T': 3, 'obs': [(0, [0]), (3, [0])]},\n        # Case F: second-difference, n=1, T=4, obs at t=0, t=4 of component {0}\n        {'type': 'second_difference', 'n': 1, 'T': 4, 'obs': [(0, [0]), (4, [0])]},\n    ]\n\n    results = []\n    for case in test_cases:\n        result = _solve_case(case['type'], case['n'], case['T'], case['obs'])\n        results.append(result)\n\n    # Format the final output string exactly as required\n    # e.g., [[k0_A,kmin_A,s_A],[k0_B,kmin_B,s_B],...]\n    result_strings = [f\"[{r[0]},{r[1]},{str(r[2]).lower()}]\" for r in results]\n    print(f\"[{','.join(result_strings)}]\")\n\ndef _solve_case(smoother_type, n, T, observations):\n    \"\"\"\n    Solves a single instance of the observability problem.\n    \"\"\"\n    # Total dimension of the stacked state vector z\n    dim = n * (T + 1)\n    \n    # Numerical tolerance for identifying zero eigenvalues\n    TOL = 1e-9\n\n    # 1. Construct the smoothing operator L\n    if smoother_type == 'first_difference':\n        # L has n*T rows and n*(T+1) columns\n        L = np.zeros((n * T, dim))\n        identity_n = np.eye(n)\n        for t in range(T):\n            row_slice = slice(t * n, (t + 1) * n)\n            col_slice_t = slice(t * n, (t + 1) * n)\n            col_slice_t1 = slice((t + 1) * n, (t + 2) * n)\n            L[row_slice, col_slice_t] = -identity_n\n            L[row_slice, col_slice_t1] = identity_n\n    \n    elif smoother_type == 'second_difference':\n        # L has n*(T-1) rows and n*(T+1) columns\n        L = np.zeros((n * (T - 1), dim))\n        identity_n = np.eye(n)\n        for t_idx, t in enumerate(range(1, T)):\n            row_slice = slice(t_idx * n, (t_idx + 1) * n)\n            col_slice_tm1 = slice((t - 1) * n, t * n)\n            col_slice_t = slice(t * n, (t + 1) * n)\n            col_slice_tp1 = slice((t + 1) * n, (t + 2) * n)\n            L[row_slice, col_slice_tm1] = identity_n\n            L[row_slice, col_slice_t] = -2 * identity_n\n            L[row_slice, col_slice_tp1] = identity_n\n    else:\n        raise ValueError(\"Unknown smoother type\")\n\n    # 2. Construct the posterior precision matrix I\n    # Start with the prior precision L^T L\n    I = L.T @ L\n    \n    # Add observation precision C^T C\n    for t, components in observations:\n        for i in components:\n            idx = t * n + i\n            I[idx, idx] += 1.0\n\n    # 3. Compute initial nullity k0 and find nullspace basis\n    eigenvalues, eigenvectors = np.linalg.eigh(I)\n    is_zero_eigenvalue = np.abs(eigenvalues) < TOL\n    k0 = int(np.sum(is_zero_eigenvalue))\n\n    # 4. Propose and apply regularization\n    k_min = k0\n    \n    if k0 == 0:\n        I_new = I\n    else:\n        nullspace_basis = eigenvectors[:, is_zero_eigenvalue]\n        # C_add.T @ C_add = sum(u_j @ u_j.T)\n        C_add_T_C_add = nullspace_basis @ nullspace_basis.T\n        I_new = I + C_add_T_C_add\n        \n    # 5. Verify successful regularization\n    new_eigenvalues, _ = np.linalg.eigh(I_new)\n    new_nullity = np.sum(np.abs(new_eigenvalues) < TOL)\n    s = (new_nullity == 0)\n    \n    return [k0, k_min, s]\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3406074"}, {"introduction": "While linear models offer clarity, most real-world systems are nonlinear, leading to complex, non-convex optimization landscapes with multiple possible solutions. This practice confronts this challenge directly, using a classic nonlinear model to demonstrate how different local minima can arise and how they can be found and compared. You will implement advanced numerical techniques, including a damped Newton's method and a homotopy continuation strategy, to robustly navigate the objective function and approximate the evidence for competing solutions. [@problem_id:3406013]", "problem": "Consider a discrete-time, scalar, nonlinear state-space model used for fixed-interval smoothing over times $t = 0, 1, \\dots, T$. The latent state sequence is $x_{0:T} \\equiv (x_0, x_1, \\dots, x_T) \\in \\mathbb{R}^{T+1}$. The model is specified by the following elements:\n\n- A Gaussian prior for the initial state: $x_0 \\sim \\mathcal{N}(m_0, P_0)$, with prior mean $m_0 \\in \\mathbb{R}$ and prior variance $P_0 \\in \\mathbb{R}_{>0}$.\n- A Markovian dynamics model: $x_{t+1} = f(x_t) + w_t$, where $f(x) = a x$ for a given $a \\in \\mathbb{R}$ and $w_t \\sim \\mathcal{N}(0, q)$ with process noise variance $q \\in \\mathbb{R}_{>0}$, independent across $t$.\n- A nonlinear observation model: $y_t = h(x_t) + v_t$, where $h(x) = x^2$ and $v_t \\sim \\mathcal{N}(0, r)$ with observation noise variance $r \\in \\mathbb{R}_{>0}$, independent across $t$ and independent of the process noise.\n\nAll distributions are understood to be conditionally independent according to the above structure. Let $y_{0:T} \\equiv (y_0, y_1, \\dots, y_T)$ denote the observed data.\n\nYour tasks are:\n\n1. Starting from Bayes' theorem and the definition of the joint probability density under the above Gaussian assumptions, derive the fixed-interval smoothing objective $J(x_{0:T})$ as the negative log of the posterior density $p(x_{0:T} \\mid y_{0:T})$ up to an additive constant that does not depend on $x_{0:T}$. Express $J(x_{0:T})$ in terms of the model parameters and residuals between the state sequence and the model/observations. Do not invoke unproven shortcut formulas; start from the factorization of the joint density and proceed via the properties of Gaussian densities.\n2. For the specific choice $f(x) = a x$ and $h(x) = x^2$, derive the exact gradient $\\nabla J(x_{0:T})$ and the exact Hessian matrix $\\nabla^2 J(x_{0:T})$ with respect to $x_{0:T}$. Your derivation must be explicit in $a$, $q$, $r$, $m_0$, $P_0$, and $y_{0:T}$.\n3. A local minimizer $x_{0:T}^\\star$ corresponds to a candidate for the Maximum A Posteriori (MAP) estimate. The Laplace approximation constructs a Gaussian approximation to the posterior near a local minimizer via the second-order Taylor expansion of $J(x_{0:T})$. Using your Hessian, write down the Gaussian approximation and the corresponding approximation to the local contribution to the model evidence, with logarithm given by\n$$\n\\log Z_{\\text{Lap}} \\approx -J(x_{0:T}^\\star) - \\tfrac{1}{2} \\log \\det\\left(\\nabla^2 J(x_{0:T}^\\star)\\right) - \\tfrac{(T+1)}{2} \\log(2\\pi),\n$$\nnoting that the final term arises from normalizing a $(T+1)$-dimensional Gaussian density.\n4. Propose a homotopy in the observation noise variance $r$ by introducing a scalar parameter $\\lambda \\in (0, 1]$ and defining $r(\\lambda) = r_{\\text{base}} / \\lambda$ with fixed $r_{\\text{base}} \\in \\mathbb{R}_{>0}$. Explain why decreasing $\\lambda$ increases $r(\\lambda)$ (weakening observation influence), and increasing $\\lambda$ decreases $r(\\lambda)$ (strengthening observation influence).\n5. Implement a continuation strategy in $\\lambda$ to track a family of local minimizers from a weak-observation regime (smaller $\\lambda$) to the target regime $\\lambda = 1$. At each $\\lambda$ on a specified discrete schedule, use Newton's method with a backtracking line search and Levenberg–Marquardt-style damping to find a local minimizer, initialized from the previous $\\lambda$'s solution. Ensure that at the final $\\lambda = 1$, your candidate is a local minimum by verifying that the Hessian is positive definite.\n\nFor numerical study and automated testing, use the following test suite. For each case, you must:\n\n- Construct the smoothing objective for the given parameters.\n- Use two multi-start initializations at $\\lambda = 1$: one with $x_t^{(0)} = +\\sqrt{y_t}$ for all $t$, and one with $x_t^{(0)} = -\\sqrt{y_t}$ for all $t$. Run Newton's method from both initializations to seek distinct local minima. Identify distinct minima by an $\\ell_2$-distance threshold of $10^{-3}$ and retain only those with positive definite Hessians.\n- Compute the Laplace-approximated log-evidence at each distinct local minimum using your Hessian.\n- Run the homotopy tracking from the smaller $\\lambda$ to $\\lambda = 1$, starting from the positive initialization at the smallest $\\lambda$, and report the sign of the time-average of the final tracked solution as $+1$ if the average is positive and $-1$ if the average is negative.\n- At the final homotopy step, compute the condition number of the Hessian as the ratio of its largest to smallest eigenvalue.\n\nTest suite:\n\n- Case $\\#1$ (symmetric, two-minima regime):\n  - $T = 4$,\n  - $a = 0.9$,\n  - $q = 0.05$,\n  - $r_{\\text{base}} = 0.02$,\n  - $m_0 = 0.0$,\n  - $P_0 = 1.0$,\n  - $y_{0:4} = (1.05, 0.95, 1.10, 0.90, 1.00)$,\n  - Homotopy schedule $\\lambda \\in \\{0.2, 0.4, 0.7, 1.0\\}$, so $r(\\lambda) = r_{\\text{base}}/\\lambda$.\n- Case $\\#2$ (prior breaks symmetry, single-minimum regime likely):\n  - $T = 4$,\n  - $a = 0.9$,\n  - $q = 0.05$,\n  - $r_{\\text{base}} = 0.02$,\n  - $m_0 = 0.5$,\n  - $P_0 = 0.01$,\n  - $y_{0:4} = (1.05, 0.95, 1.10, 0.90, 1.00)$,\n  - Homotopy schedule $\\lambda \\in \\{0.2, 0.4, 0.7, 1.0\\}$.\n- Case $\\#3$ (weak dynamics coupling, multiple nearly-decoupled modes):\n  - $T = 4$,\n  - $a = 0.9$,\n  - $q = 10.0$,\n  - $r_{\\text{base}} = 0.02$,\n  - $m_0 = 0.0$,\n  - $P_0 = 1.0$,\n  - $y_{0:4} = (1.05, 0.95, 1.10, 0.90, 1.00)$,\n  - Homotopy schedule $\\lambda \\in \\{0.2, 0.4, 0.7, 1.0\\}$.\n\nRequired final output for each case is a list of four values:\n- The integer number of distinct local minima found at $\\lambda = 1$ from the two prescribed initializations.\n- The floating-point difference in Laplace-approximated log-evidence between the positive-basin and negative-basin minima at $\\lambda = 1$, computed as $\\log Z_{\\text{Lap}}^{(+)} - \\log Z_{\\text{Lap}}^{(-)}$. If only one minimum is found, report $0.0$.\n- The integer sign of the time-average of the final homotopy-tracked solution at $\\lambda = 1$, reported as $+1$ or $-1$.\n- The floating-point condition number of the Hessian at the final homotopy-tracked solution at $\\lambda = 1$.\n\nYour program should produce a single line of output containing the results for the three cases as a comma-separated list of the per-case lists, with no spaces, enclosed in square brackets. For example: $[\\,[1,0.0,1,12.345678],[\\dots],[\\dots]\\,]$. No physical units are involved in this problem, and all numbers should be printed as plain decimals.", "solution": "This problem requires the derivation and implementation of a fixed-interval smoothing algorithm for a nonlinear state-space model. The solution involves finding Maximum A Posteriori (MAP) estimates of the state trajectory by minimizing a cost function, which corresponds to the negative log-posterior probability density. The specific model involves linear dynamics and quadratic observations, leading to a non-convex optimization problem with potentially multiple local minima.\n\n### Task 1: Derivation of the Smoothing Objective $J(x_{0:T})$\n\nThe objective is to find the state trajectory $x_{0:T} = (x_0, x_1, \\dots, x_T)$ that maximizes the posterior probability density $p(x_{0:T} \\mid y_{0:T})$. By Bayes' theorem, the posterior is proportional to the product of the likelihood and the prior:\n$$\np(x_{0:T} \\mid y_{0:T}) \\propto p(y_{0:T} \\mid x_{0:T}) p(x_{0:T})\n$$\nMaximizing the posterior is equivalent to maximizing its logarithm, or minimizing its negative logarithm. We define the objective function $J(x_{0:T})$ as the negative log-posterior, up to an additive constant that does not depend on the state sequence $x_{0:T}$.\n$$\nJ(x_{0:T}) = -\\log p(x_{0:T} \\mid y_{0:T}) + C = -\\log p(y_{0:T} \\mid x_{0:T}) - \\log p(x_{0:T}) + C'\n$$\nThe state-space model's structure implies conditional independencies, which allow us to factor the likelihood and prior terms.\nThe prior on the state trajectory $p(x_{0:T})$ is given by the Markov property:\n$$\np(x_{0:T}) = p(x_0) \\prod_{t=0}^{T-1} p(x_{t+1} \\mid x_t)\n$$\nGiven the states $x_{0:T}$, the observations $y_{0:T}$ are conditionally independent:\n$$\np(y_{0:T} \\mid x_{0:T}) = \\prod_{t=0}^{T} p(y_t \\mid x_t)\n$$\nSubstituting these into the expression for $J(x_{0:T})$ gives:\n$$\nJ(x_{0:T}) = -\\log p(x_0) - \\sum_{t=0}^{T-1} \\log p(x_{t+1} \\mid x_t) - \\sum_{t=0}^{T} \\log p(y_t \\mid x_t) + C'\n$$\nWe now substitute the given Gaussian probability densities:\n-   Prior: $p(x_0) = \\mathcal{N}(x_0; m_0, P_0) \\propto \\exp\\left(-\\frac{1}{2P_0}(x_0 - m_0)^2\\right)$\n-   Dynamics: $p(x_{t+1} \\mid x_t) = \\mathcal{N}(x_{t+1}; f(x_t), q) = \\mathcal{N}(x_{t+1}; ax_t, q) \\propto \\exp\\left(-\\frac{1}{2q}(x_{t+1} - ax_t)^2\\right)$\n-   Observations: $p(y_t \\mid x_t) = \\mathcal{N}(y_t; h(x_t), r) = \\mathcal{N}(y_t; x_t^2, r) \\propto \\exp\\left(-\\frac{1}{2r}(y_t - x_t^2)^2\\right)$\n\nTaking the negative logarithm of each term and summing, while ignoring normalization constants (e.g., factors of $1/\\sqrt{2\\pi\\sigma^2}$), we obtain the smoothing objective:\n$$\nJ(x_{0:T}) = \\frac{1}{2P_0}(x_0 - m_0)^2 + \\sum_{t=0}^{T-1} \\frac{1}{2q}(x_{t+1} - ax_t)^2 + \\sum_{t=0}^{T} \\frac{1}{2r}(y_t - x_t^2)^2\n$$\nThis is a standard least-squares objective, composed of a prior term penalizing deviation from the prior mean, a dynamics term penalizing deviations from the state transition model, and an observation term penalizing mismatch with the measurements.\n\n### Task 2: Gradient $\\nabla J(x_{0:T})$ and Hessian $\\nabla^2 J(x_{0:T})$\n\nTo minimize $J(x_{0:T})$ using a gradient-based method like Newton's method, we need its gradient vector $\\nabla J(x_{0:T})$ and Hessian matrix $\\nabla^2 J(x_{0:T})$. The state vector is $x = (x_0, \\dots, x_T)^T$.\n\nThe gradient is the vector of partial derivatives $\\frac{\\partial J}{\\partial x_k}$ for $k=0, \\dots, T$.\n-   For $k=0$:\n    $$\n    \\frac{\\partial J}{\\partial x_0} = \\frac{1}{P_0}(x_0 - m_0) + \\frac{1}{q}(x_{1} - ax_0)(-a) + \\frac{1}{r}(y_0 - x_0^2)(-2x_0) = \\frac{x_0 - m_0}{P_0} - \\frac{a}{q}(x_1 - ax_0) - \\frac{2x_0}{r}(y_0 - x_0^2)\n    $$\n-   For $k \\in \\{1, \\dots, T-1\\}$:\n    $$\n    \\frac{\\partial J}{\\partial x_k} = \\frac{1}{q}(x_k - ax_{k-1}) + \\frac{1}{q}(x_{k+1} - ax_k)(-a) + \\frac{1}{r}(y_k - x_k^2)(-2x_k) = \\frac{1}{q}(x_k - ax_{k-1}) - \\frac{a}{q}(x_{k+1} - ax_k) - \\frac{2x_k}{r}(y_k - x_k^2)\n    $$\n-   For $k=T$:\n    $$\n    \\frac{\\partial J}{\\partial x_T} = \\frac{1}{q}(x_T - ax_{T-1}) + \\frac{1}{r}(y_T - x_T^2)(-2x_T) = \\frac{1}{q}(x_T - ax_{T-1}) - \\frac{2x_T}{r}(y_T - x_T^2)\n    $$\n\nThe Hessian is the symmetric matrix of second partial derivatives $H_{ij} = \\frac{\\partial^2 J}{\\partial x_i \\partial x_j}$. Due to the Markovian structure of the dynamics, this matrix is tridiagonal.\nThe diagonal elements $H_{k,k}$ are:\n-   For $k=0$:\n    $$\n    \\frac{\\partial^2 J}{\\partial x_0^2} = \\frac{1}{P_0} + \\frac{a^2}{q} + \\frac{\\partial}{\\partial x_0}\\left(-\\frac{2x_0y_0}{r} + \\frac{2x_0^3}{r}\\right) = \\frac{1}{P_0} + \\frac{a^2}{q} - \\frac{2y_0}{r} + \\frac{6x_0^2}{r} = \\frac{1}{P_0} + \\frac{a^2}{q} + \\frac{2}{r}(3x_0^2 - y_0)\n    $$\n-   For $k \\in \\{1, \\dots, T-1\\}$:\n    $$\n    \\frac{\\partial^2 J}{\\partial x_k^2} = \\frac{1}{q} + \\frac{a^2}{q} + \\frac{2}{r}(3x_k^2 - y_k) = \\frac{1+a^2}{q} + \\frac{2}{r}(3x_k^2 - y_k)\n    $$\n-   For $k=T$:\n    $$\n    \\frac{\\partial^2 J}{\\partial x_T^2} = \\frac{1}{q} + \\frac{2}{r}(3x_T^2 - y_T)\n    $$\nThe off-diagonal elements $H_{k, k+1} = H_{k+1, k}$ for $k=0, \\dots, T-1$ are:\n$$\n\\frac{\\partial^2 J}{\\partial x_k \\partial x_{k+1}} = \\frac{\\partial}{\\partial x_{k+1}}\\left( \\dots - \\frac{a}{q}(x_{k+1} - ax_k) \\dots \\right) = -\\frac{a}{q}\n$$\nAll other off-diagonal elements $H_{i,j}$ where $|i-j|>1$ are zero.\n\n### Task 3: Laplace Approximation\n\nThe Laplace approximation provides a Gaussian approximation to the posterior distribution around a MAP estimate $x_{0:T}^\\star$, which is a local minimum of $J(x_{0:T})$. The objective function is expanded to second order around $x_{0:T}^\\star$:\n$$\nJ(x_{0:T}) \\approx J(x_{0:T}^\\star) + \\nabla J(x_{0:T}^\\star)^T (x_{0:T} - x_{0:T}^\\star) + \\frac{1}{2}(x_{0:T} - x_{0:T}^\\star)^T \\nabla^2 J(x_{0:T}^\\star) (x_{0:T} - x_{0:T}^\\star)\n$$\nSince $x_{0:T}^\\star$ is a minimum, $\\nabla J(x_{0:T}^\\star) = 0$. The posterior is then approximated as:\n$$\np(x_{0:T} \\mid y_{0:T}) \\propto \\exp(-J(x_{0:T})) \\approx \\exp(-J(x_{0:T}^\\star)) \\exp\\left(-\\frac{1}{2}(x_{0:T} - x_{0:T}^\\star)^T H^\\star (x_{0:T} - x_{0:T}^\\star)\\right)\n$$\nwhere $H^\\star = \\nabla^2 J(x_{0:T}^\\star)$. This shows the posterior is approximately Gaussian: $\\mathcal{N}(x_{0:T}^\\star, (H^\\star)^{-1})$. The model evidence (or marginal likelihood) $p(y_{0:T}) = \\int p(y_{0:T}, x_{0:T}) dx_{0:T}$ can be approximated by integrating the unnormalized posterior density. As given, the local contribution to the log evidence is:\n$$\n\\log Z_{\\text{Lap}} \\approx -J(x_{0:T}^\\star) - \\tfrac{1}{2} \\log \\det\\left(\\nabla^2 J(x_{0:T}^\\star)\\right) - \\tfrac{T+1}{2} \\log(2\\pi)\n$$\nThis formula combines the height of the posterior peak (via $-J(x_{0:T}^\\star)$) and the volume of the peak (via $-\\frac{1}{2}\\log\\det H^\\star$, which measures its 'width'). When comparing the evidence of different local minima, terms not depending on $x_{0:T}^\\star$ cancel out, making the choice of which constants are included in $J$ unimportant for relative comparison.\n\n### Task 4: Homotopy in Observation Noise Variance\n\nA homotopy, or continuation method, is proposed to solve the optimization problem. The observation noise variance $r$ is parameterized by $\\lambda \\in (0, 1]$ as $r(\\lambda) = r_{\\text{base}} / \\lambda$.\n-   When $\\lambda$ is small (close to $0$), $r(\\lambda)$ becomes very large. A large observation variance $r$ implies that the observations $y_t$ are uninformative. The term $\\frac{1}{2r(\\lambda)}(y_t - x_t^2)^2$ in the objective function becomes small, so the posterior is dominated by the prior and dynamics, which define a convex problem. This \"weak-observation\" problem is easier to solve and typically has a single minimum close to the prior mean.\n-   As $\\lambda$ increases towards $1$, $r(\\lambda)$ decreases, approaching the target value $r_{\\text{base}}$. The influence of the non-convex observation term $\\frac{1}{2r(\\lambda)}(y_t - x_t^2)^2$ increases, and the objective function deforms from the simple, convex-like shape to the complex, possibly multi-modal target shape.\n\nThe strategy is to start from the solution of the easy problem at a small $\\lambda$ and use it as the initial guess for a slightly larger $\\lambda$. By gradually stepping $\\lambda$ from a small value to $1$, we can track a local minimum through the deformation, which is generally more robust than starting the optimization directly at $\\lambda=1$.\n\n### Task 5: Numerical Implementation\n\nThe core of the implementation is a damped Newton's method to find local minima of $J(x_{0:T})$.\n-   **Newton Step**: At each iteration, we solve the linear system $H_k p_k = -g_k$ for the Newton step $p_k$, where $g_k$ and $H_k$ are the gradient and Hessian at the current estimate $x_k$.\n-   **Damping**: The Hessian may not be positive definite away from a minimum. A Levenberg-Marquardt style modification is used: we solve $(H_k + \\mu I) p_k = -g_k$, where $\\mu \\ge 0$ is a damping parameter. $\\mu$ is chosen to be zero if $H_k$ is positive definite, and increased otherwise until $(H_k + \\mu I)$ is positive definite, ensuring a descent direction.\n-   **Line Search**: A backtracking line search is used to determine the step size $\\alpha$. We start with $\\alpha=1$ and reduce it until the Armijo condition $J(x_k + \\alpha p_k) \\le J(x_k) + c \\alpha g_k^T p_k$ is satisfied for a small constant $c$.\n-   **Multi-start and Homotopy**: This Newton solver is used for two purposes:\n    1.  At $\\lambda=1$, it is run from two different initial guesses ($x_t^{(0)} = \\pm\\sqrt{y_t}$) to find potentially distinct local minima.\n    2.  It is used at each step of the homotopy schedule, with the solution from the previous $\\lambda$ value providing the initial guess for the current $\\lambda$.\n\nThe final step for each identified minimum involves checking if the Hessian is positive definite (all eigenvalues are positive) and then computing the required quantities: Laplace log-evidence, sign of the average state, and Hessian condition number.", "answer": "```python\nimport numpy as np\nimport scipy.linalg\n\ndef solve():\n    \"\"\"\n    Main function to run the smoothing problem for all test cases.\n    \"\"\"\n    \n    test_cases = [\n        # Case #1 (symmetric, two-minima regime)\n        {\n            \"T\": 4, \"a\": 0.9, \"q\": 0.05, \"r_base\": 0.02, \"m0\": 0.0, \"P0\": 1.0,\n            \"y\": np.array([1.05, 0.95, 1.10, 0.90, 1.00]),\n            \"lambda_schedule\": [0.2, 0.4, 0.7, 1.0]\n        },\n        # Case #2 (prior breaks symmetry, single-minimum regime likely)\n        {\n            \"T\": 4, \"a\": 0.9, \"q\": 0.05, \"r_base\": 0.02, \"m0\": 0.5, \"P0\": 0.01,\n            \"y\": np.array([1.05, 0.95, 1.10, 0.90, 1.00]),\n            \"lambda_schedule\": [0.2, 0.4, 0.7, 1.0]\n        },\n        # Case #3 (weak dynamics coupling, multiple nearly-decoupled modes)\n        {\n            \"T\": 4, \"a\": 0.9, \"q\": 10.0, \"r_base\": 0.02, \"m0\": 0.0, \"P0\": 1.0,\n            \"y\": np.array([1.05, 0.95, 1.10, 0.90, 1.00]),\n            \"lambda_schedule\": [0.2, 0.4, 0.7, 1.0]\n        }\n    ]\n\n    all_results = []\n\n    for case in test_cases:\n        p = case\n        dim = p[\"T\"] + 1\n\n        def J_objective(x, r, p):\n            term_prior = 0.5 * (x[0] - p[\"m0\"])**2 / p[\"P0\"]\n            term_dyn = 0.5 * np.sum((x[1:] - p[\"a\"] * x[:-1])**2) / p[\"q\"]\n            term_obs = 0.5 * np.sum((p[\"y\"] - x**2)**2) / r\n            return term_prior + term_dyn + term_obs\n\n        def grad_J(x, r, p):\n            grad = np.zeros_like(x)\n            # Observation part\n            grad += -2 * x * (p[\"y\"] - x**2) / r\n            \n            # Prior part\n            grad[0] += (x[0] - p[\"m0\"]) / p[\"P0\"]\n            \n            # Dynamics part\n            dyn_res = x[1:] - p[\"a\"] * x[:-1]\n            grad[1:] += dyn_res / p[\"q\"]\n            grad[:-1] -= p[\"a\"] * dyn_res / p[\"q\"]\n            \n            return grad\n\n        def hess_J(x, r, p):\n            hess = np.zeros((dim, dim))\n            \n            # Diagonal\n            diag = np.zeros(dim)\n            diag += 2 * (3 * x**2 - p[\"y\"]) / r\n            diag[0] += 1 / p[\"P0\"] + p[\"a\"]**2 / p[\"q\"]\n            diag[1:p[\"T\"]] += (1 + p[\"a\"]**2) / p[\"q\"]\n            diag[p[\"T\"]] += 1 / p[\"q\"]\n            np.fill_diagonal(hess, diag)\n            \n            # Off-diagonal\n            off_diag_val = -p[\"a\"] / p[\"q\"]\n            np.fill_diagonal(hess[1:], off_diag_val)\n            np.fill_diagonal(hess[:, 1:], off_diag_val)\n            \n            return hess\n\n        def newton_solver(x_init, lam, p, max_iter=100, tol=1e-8):\n            x = x_init.copy()\n            r = p[\"r_base\"] / lam\n            \n            for _ in range(max_iter):\n                g = grad_J(x, r, p)\n                if np.linalg.norm(g) < tol:\n                    break\n                \n                H = hess_J(x, r, p)\n                \n                # Levenberg-Marquardt Damping\n                mu = 0\n                eigvals_H = np.linalg.eigvalsh(H)\n                if eigvals_H[0] <= 0:\n                    mu = -eigvals_H[0] + 1e-6\n                \n                H_lm = H + mu * np.eye(dim)\n                \n                try:\n                    # Solve linear system H_lm * step = -g\n                    step = np.linalg.solve(H_lm, -g)\n                except np.linalg.LinAlgError:\n                    break\n\n                if np.linalg.norm(step) < tol:\n                    break\n\n                # Backtracking Line Search\n                alpha = 1.0\n                c = 1e-4\n                J_current = J_objective(x, r, p)\n                while alpha > 1e-8:\n                    x_new = x + alpha * step\n                    if J_objective(x_new, r, p) < J_current + c * alpha * np.dot(g, step):\n                        break\n                    alpha *= 0.5\n                \n                x = x + alpha * step\n\n            return x\n\n        # 1. Multi-start Newton's at lambda=1\n        x_init_pos = np.sqrt(np.abs(p[\"y\"]))\n        x_init_neg = -np.sqrt(np.abs(p[\"y\"]))\n        \n        x_sol_pos = newton_solver(x_init_pos, 1.0, p)\n        x_sol_neg = newton_solver(x_init_neg, 1.0, p)\n\n        minima = []\n        r_final = p[\"r_base\"]\n        \n        # Check first minimum\n        H_pos = hess_J(x_sol_pos, r_final, p)\n        try:\n            eigvals_pos = np.linalg.eigvalsh(H_pos)\n            if np.all(eigvals_pos > 1e-9):\n                minima.append({'x': x_sol_pos, 'H': H_pos, 'origin': 'pos', 'eigvals':eigvals_pos})\n        except np.linalg.LinAlgError:\n            pass\n\n        # Check second minimum\n        is_new = True\n        for m in minima:\n            if np.linalg.norm(m['x'] - x_sol_neg) < 1e-3:\n                is_new = False\n                break\n        \n        if is_new:\n            H_neg = hess_J(x_sol_neg, r_final, p)\n            try:\n                eigvals_neg = np.linalg.eigvalsh(H_neg)\n                if np.all(eigvals_neg > 1e-9):\n                    minima.append({'x': x_sol_neg, 'H': H_neg, 'origin': 'neg', 'eigvals':eigvals_neg})\n            except np.linalg.LinAlgError:\n                pass\n        \n        num_minima = len(minima)\n\n        # 2. Laplace log-evidence difference\n        log_evidence_diff = 0.0\n        if num_minima == 2:\n            m_pos = next(m for m in minima if m['origin'] == 'pos')\n            m_neg = next(m for m in minima if m['origin'] == 'neg')\n\n            J_pos = J_objective(m_pos['x'], r_final, p)\n            logdet_pos = np.sum(np.log(m_pos['eigvals']))\n            \n            J_neg = J_objective(m_neg['x'], r_final, p)\n            logdet_neg = np.sum(np.log(m_neg['eigvals']))\n            \n            log_z_pos = -J_pos - 0.5 * logdet_pos\n            log_z_neg = -J_neg - 0.5 * logdet_neg\n            \n            log_evidence_diff = log_z_pos - log_z_neg\n\n        # 3. Homotopy tracking\n        x_homotopy = np.sqrt(np.abs(p[\"y\"]))\n        for lam in p[\"lambda_schedule\"]:\n            x_homotopy = newton_solver(x_homotopy, lam, p)\n        \n        sign_avg_homotopy = int(np.sign(np.mean(x_homotopy)))\n\n        # 4. Condition number\n        H_homotopy = hess_J(x_homotopy, r_final, p)\n        cond_num = np.inf\n        try:\n            eigvals_homotopy = np.linalg.eigvalsh(H_homotopy)\n            if np.all(eigvals_homotopy > 1e-9):\n                cond_num = eigvals_homotopy[-1] / eigvals_homotopy[0]\n        except np.linalg.LinAlgError:\n            pass\n        \n        all_results.append(f\"[{num_minima},{log_evidence_diff:.8f},{sign_avg_homotopy},{cond_num:.8f}]\")\n\n    print(f\"[{','.join(all_results)}]\")\n\nsolve()\n```", "id": "3406013"}, {"introduction": "A state estimate is only as reliable as the model used to produce it, making it crucial to understand how sensitive the solution is to modeling assumptions. This exercise introduces the powerful technique of sensitivity analysis, guiding you through the derivation of how the smoothed state estimate responds to changes in the assumed model error statistics. By calculating the derivative of the solution with respect to a model hyperparameter, you will gain quantitative insight into the robustness and structural properties of the data assimilation system. [@problem_id:3406001]", "problem": "Consider a scalar linear Gaussian state-space model used for smoothing in data assimilation. The state evolves according to $x_{k+1} = F x_{k} + w_{k}$ with process noise $w_{k} \\sim \\mathcal{N}(0, \\alpha Q)$, and the observation model is $y_{k} = H x_{k} + v_{k}$ with observation noise $v_{k} \\sim \\mathcal{N}(0, R)$. The initial state prior is $x_{0} \\sim \\mathcal{N}(m_{0}, P_{0})$. The smoothed state $x_{0:K}^{s}$ and its smoothed covariance $P^{s}$ are defined as the posterior mean and posterior covariance of the joint state given the full observation sequence $y_{0:K}$.\n\nStarting from the definition of the posterior as a product of Gaussian densities and the objective of state estimation and smoothing as the minimization of the negative log-posterior, derive how scaling the model-error covariance by the factor $\\alpha$ (i.e., replacing $Q$ with $\\alpha Q$) affects the smoothed estimate and covariance. Specifically:\n\n1. For the case $K=1$ with $H = 1$, write the negative log-posterior objective in terms of $x_{0}$ and $x_{1}$ and derive an analytic expression for $x_{0}^{s}(\\alpha)$ by eliminating $x_{1}$ and minimizing with respect to $x_{0}$. Then, differentiate this expression with respect to $\\alpha$ to obtain $\\frac{d}{d\\alpha} x_{0}^{s}(\\alpha)$.\n\n2. Using the characterization of the smoothed covariance as the inverse of the Hessian (posterior precision) of the negative log-posterior, derive an expression for $\\frac{d}{d\\alpha} P^{s}(\\alpha)$ in terms of the posterior precision matrix and its derivative with respect to $\\alpha$.\n\nFinally, evaluate the derivative $\\frac{d}{d\\alpha} x_{0}^{s}(\\alpha)$ at $\\alpha = 1$ for the following parameter values:\n- $F = \\frac{9}{10}$,\n- $H = 1$,\n- $Q = \\frac{1}{2}$,\n- $R = \\frac{2}{5}$,\n- $m_{0} = \\frac{1}{5}$,\n- $P_{0} = 1$,\n- $y_{0} = \\frac{13}{10}$,\n- $y_{1} = \\frac{3}{5}$.\n\nExpress the final answer as an exact value. No rounding is required. There are no physical units associated with the answer.", "solution": "The user wants to analyze the sensitivity of a smoothed state estimate and its covariance to a scaling factor $\\alpha$ applied to the process noise covariance in a linear Gaussian state-space model.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- State model: $x_{k+1} = F x_{k} + w_{k}$, with process noise $w_{k} \\sim \\mathcal{N}(0, \\alpha Q)$.\n- Observation model: $y_{k} = H x_{k} + v_{k}$, with observation noise $v_{k} \\sim \\mathcal{N}(0, R)$.\n- Prior on the initial state: $x_{0} \\sim \\mathcal{N}(m_{0}, P_{0})$.\n- Definition: The smoothed state $x_{0:K}^{s}$ and covariance $P^{s}$ are the posterior mean and covariance of the joint state given the full observation sequence $y_{0:K}$.\n- Objective: Minimize the negative log-posterior.\n- Task 1: For $K=1$ and $H=1$, find the negative log-posterior $J(x_0, x_1)$, derive an analytic expression for $x_{0}^{s}(\\alpha)$, and then calculate $\\frac{d}{d\\alpha} x_{0}^{s}(\\alpha)$.\n- Task 2: Derive an expression for $\\frac{d}{d\\alpha} P^{s}(\\alpha)$ using the characterization of $P^s$ as the inverse of the Hessian of the negative log-posterior.\n- Task 3: Evaluate $\\frac{d}{d\\alpha} x_{0}^{s}(\\alpha)$ at $\\alpha = 1$ using the parameter values: $F = \\frac{9}{10}$, $H = 1$, $Q = \\frac{1}{2}$, $R = \\frac{2}{5}$, $m_{0} = \\frac{1}{5}$, $P_{0} = 1$, $y_{0} = \\frac{13}{10}$, $y_{1} = \\frac{3}{5}$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, being a standard topic in data assimilation, control theory, and statistics, specifically relating to variational methods (like 4D-Var) and sensitivity analysis. It is well-posed, stating a clear objective with all necessary parameters and conditions provided. The language is objective and precise. The problem does not violate any of the invalidity criteria.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A full solution will be provided.\n\n### Solution\n\nThe posterior probability of the state trajectory $x_{0:K}$ given the observations $y_{0:K}$ is given by Bayes' theorem:\n$$p(x_{0:K} | y_{0:K}) \\propto p(y_{0:K} | x_{0:K}) p(x_{0:K})$$\nUsing the Markov property of the state-space model, this can be expanded as:\n$$p(x_{0:K} | y_{0:K}) \\propto p(x_0) \\left( \\prod_{k=0}^{K-1} p(x_{k+1}|x_k) \\right) \\left( \\prod_{k=0}^{K} p(y_k|x_k) \\right)$$\nThe smoothed estimate is the mean of this posterior distribution. For a Gaussian system, the mean coincides with the mode, which is found by minimizing the negative logarithm of the posterior. The negative log-posterior (or cost function) $J(x_{0:K})$ is, up to an additive constant:\n$$J(x_{0:K}) = \\frac{1}{2} \\left( (x_0 - m_0)^T P_0^{-1} (x_0 - m_0) + \\sum_{k=0}^{K-1} (x_{k+1} - F x_k)^T (\\alpha Q)^{-1} (x_{k+1} - F x_k) + \\sum_{k=0}^{K} (y_k - H x_k)^T R^{-1} (y_k - H x_k) \\right)$$\n\n**Part 1: Derivation of $x_{0}^{s}(\\alpha)$ and its derivative**\n\nFor the case $K=1$, with scalar states and $H=1$, the state vector is $(x_0, x_1)$. The cost function becomes:\n$$J(x_0, x_1) = \\frac{1}{2} \\left( (x_0 - m_0)^2 P_0^{-1} + (x_1 - F x_0)^2 (\\alpha Q)^{-1} + (y_0 - x_0)^2 R^{-1} + (y_1 - x_1)^2 R^{-1} \\right)$$\nThe smoothed estimates $(x_0^s, x_1^s)$ are the values of $(x_0, x_1)$ that minimize $J(x_0, x_1)$. We find them by setting the gradient of $J$ to zero:\n$$\\frac{\\partial J}{\\partial x_0} = P_0^{-1}(x_0 - m_0) - F(\\alpha Q)^{-1}(x_1 - Fx_0) - R^{-1}(y_0 - x_0) = 0$$\n$$\\frac{\\partial J}{\\partial x_1} = (\\alpha Q)^{-1}(x_1 - Fx_0) - R^{-1}(y_1 - x_1) = 0$$\nThis is a system of two linear equations for $(x_0^s, x_1^s)$. Rearranging the terms, we get:\n$$(P_0^{-1} + F^2(\\alpha Q)^{-1} + R^{-1})x_0^s - F(\\alpha Q)^{-1}x_1^s = P_0^{-1}m_0 + R^{-1}y_0$$\n$$-F(\\alpha Q)^{-1}x_0^s + ((\\alpha Q)^{-1} + R^{-1})x_1^s = R^{-1}y_1$$\nTo find $x_0^s(\\alpha)$ as requested, we solve this system, which is equivalent to eliminating $x_1$ and minimizing with respect to $x_0$. From the second equation, we express $x_1^s$ in terms of $x_0^s$:\n$$x_1^s = \\frac{F(\\alpha Q)^{-1}x_0^s + R^{-1}y_1}{(\\alpha Q)^{-1} + R^{-1}}$$\nSubstituting this into the first equation and solving for $x_0^s$ yields:\n$$x_0^s(\\alpha) = \\frac{((\\alpha Q)^{-1} + R^{-1})(P_0^{-1}m_0 + R^{-1}y_0) + F(\\alpha Q)^{-1}R^{-1}y_1}{(P_0^{-1} + R^{-1})((\\alpha Q)^{-1} + R^{-1}) + F^2(\\alpha Q)^{-1}R^{-1}}$$\nLet $u(\\alpha) = (\\alpha Q)^{-1}$. Then $\\frac{du}{d\\alpha} = -(\\alpha^2 Q)^{-1} = -u/\\alpha$. The expression for $x_0^s$ is of the form $x_0^s(\\alpha) = \\frac{N(\\alpha)}{D(\\alpha)}$, where:\n$$N(\\alpha) = (u + R^{-1})(P_0^{-1}m_0 + R^{-1}y_0) + F u R^{-1}y_1$$\n$$D(\\alpha) = (P_0^{-1} + R^{-1})(u + R^{-1}) + F^2 u R^{-1}$$\nUsing the quotient rule, the derivative is $\\frac{dx_0^s}{d\\alpha} = \\frac{\\frac{dN}{d\\alpha}D - N\\frac{dD}{d\\alpha}}{D^2}$.\nThe derivatives of $N$ and $D$ with respect to $\\alpha$ are:\n$$\\frac{dN}{d\\alpha} = \\frac{du}{d\\alpha} (P_0^{-1}m_0 + R^{-1}y_0 + F R^{-1}y_1) = -\\frac{u}{\\alpha} (P_0^{-1}m_0 + R^{-1}y_0 + F R^{-1}y_1)$$\n$$\\frac{dD}{d\\alpha} = \\frac{du}{d\\alpha} (P_0^{-1} + R^{-1} + F^2 R^{-1}) = -\\frac{u}{\\alpha} (P_0^{-1} + R^{-1} + F^2 R^{-1})$$\nThus, the derivative of the smoothed estimate is:\n$$\\frac{d x_0^s}{d\\alpha} = \\frac{1}{D(\\alpha)} \\left( \\frac{dN}{d\\alpha} - \\frac{N(\\alpha)}{D(\\alpha)} \\frac{dD}{d\\alpha} \\right) = \\frac{1}{D(\\alpha)} \\left( \\frac{dN}{d\\alpha} - x_0^s(\\alpha) \\frac{dD}{d\\alpha} \\right)$$\n$$\\frac{d x_0^s}{d\\alpha} = \\frac{-u/\\alpha}{D(\\alpha)} \\left[ (P_0^{-1}m_0 + R^{-1}y_0 + F R^{-1}y_1) - x_0^s(\\alpha)(P_0^{-1} + R^{-1} + F^2 R^{-1}) \\right]$$\n\n**Part 2: Derivation of $\\frac{d}{d\\alpha} P^s(\\alpha)$**\nThe smoothed covariance $P^s$ is the inverse of the Hessian matrix of the cost function $J$. For the case $K=1$, the Hessian matrix $\\mathbf{A} = \\nabla^2 J$ is the matrix of coefficients of the linear system derived above:\n$$\\mathbf{A}(\\alpha) = \\begin{pmatrix} P_0^{-1} + F^2(\\alpha Q)^{-1} + R^{-1} & -F(\\alpha Q)^{-1} \\\\ -F(\\alpha Q)^{-1} & (\\alpha Q)^{-1} + R^{-1} \\end{pmatrix}$$\nSo, $P^s(\\alpha) = \\mathbf{A}(\\alpha)^{-1}$. Using the identity for the derivative of a matrix inverse, $\\frac{d}{dt}\\mathbf{M}^{-1} = -\\mathbf{M}^{-1} \\frac{d\\mathbf{M}}{dt} \\mathbf{M}^{-1}$, we get:\n$$\\frac{d}{d\\alpha} P^s(\\alpha) = -P^s(\\alpha) \\left( \\frac{d\\mathbf{A}}{d\\alpha} \\right) P^s(\\alpha)$$\nThe derivative of the precision matrix $\\mathbf{A}(\\alpha)$ with respect to $\\alpha$ is:\n$$\\frac{d\\mathbf{A}}{d\\alpha} = \\frac{d}{d\\alpha} \\left( (\\alpha Q)^{-1} \\right) \\begin{pmatrix} F^2 & -F \\\\ -F & 1 \\end{pmatrix} = -(\\alpha^2 Q)^{-1} \\begin{pmatrix} F^2 & -F \\\\ -F & 1 \\end{pmatrix}$$\nThis completes the derivation.\n\n**Part 3: Evaluation of the derivative at $\\alpha=1$**\nWe are given the values: $F = \\frac{9}{10}$, $H = 1$, $Q = \\frac{1}{2}$, $R = \\frac{2}{5}$, $m_{0} = \\frac{1}{5}$, $P_{0} = 1$, $y_{0} = \\frac{13}{10}$, $y_{1} = \\frac{3}{5}$. We evaluate at $\\alpha=1$.\nFirst, we compute the inverse parameters:\n$P_0^{-1} = 1$, $Q^{-1} = 2$, $R^{-1} = \\frac{5}{2}$. Also, $F^2 = (\\frac{9}{10})^2 = \\frac{81}{100}$.\nAt $\\alpha=1$, $u=Q^{-1}=2$.\n\nWe first compute $D(1)$ and $N(1)$ to find $x_0^s(1)$.\n$$D(1) = \\left(1 + \\frac{5}{2}\\right)\\left(2 + \\frac{5}{2}\\right) + \\frac{81}{100}(2)\\left(\\frac{5}{2}\\right) = \\left(\\frac{7}{2}\\right)\\left(\\frac{9}{2}\\right) + \\frac{81}{20} = \\frac{63}{4} + \\frac{81}{20} = \\frac{315+81}{20} = \\frac{396}{20} = \\frac{99}{5}$$\n$$N(1) = \\left(2 + \\frac{5}{2}\\right)\\left(1\\cdot\\frac{1}{5} + \\frac{5}{2}\\cdot\\frac{13}{10}\\right) + \\frac{9}{10}(2)\\left(\\frac{5}{2}\\right)\\frac{3}{5} = \\left(\\frac{9}{2}\\right)\\left(\\frac{1}{5} + \\frac{13}{4}\\right) + \\frac{27}{10} = \\frac{9}{2}\\left(\\frac{4+65}{20}\\right) + \\frac{27}{10} = \\frac{9}{2}\\frac{69}{20} + \\frac{27}{10} = \\frac{621}{40} + \\frac{108}{40} = \\frac{729}{40}$$\n$$x_0^s(1) = \\frac{N(1)}{D(1)} = \\frac{729/40}{99/5} = \\frac{729}{40} \\cdot \\frac{5}{99} = \\frac{729}{8 \\cdot 99} = \\frac{81 \\cdot 9}{8 \\cdot 11 \\cdot 9} = \\frac{81}{88}$$\nNow we evaluate the terms for the derivative formula at $\\alpha=1$:\n$$\\frac{dx_0^s}{d\\alpha}\\bigg|_{\\alpha=1} = \\frac{-Q^{-1}}{D(1)} \\left[ (P_0^{-1}m_0 + R^{-1}y_0 + F R^{-1}y_1) - x_0^s(1)(P_0^{-1} + R^{-1} + F^2 R^{-1}) \\right]$$\nLet's compute the two parts inside the bracket:\nPart 1: $P_0^{-1}m_0 + R^{-1}y_0 + F R^{-1}y_1 = \\left(\\frac{1}{5} + \\frac{13}{4}\\right) + \\frac{9}{10}\\frac{5}{2}\\frac{3}{5} = \\frac{69}{20} + \\frac{27}{20} = \\frac{96}{20} = \\frac{24}{5}$.\nPart 2: $x_0^s(1)(P_0^{-1} + R^{-1} + F^2 R^{-1}) = \\frac{81}{88} \\left(1 + \\frac{5}{2} + \\frac{81}{100}\\frac{5}{2}\\right) = \\frac{81}{88} \\left(\\frac{7}{2} + \\frac{81}{40}\\right) = \\frac{81}{88} \\left(\\frac{140+81}{40}\\right) = \\frac{81}{88}\\frac{221}{40}$.\nSubstituting back into the derivative formula:\n$$\\frac{dx_0^s}{d\\alpha}\\bigg|_{\\alpha=1} = \\frac{-2}{99/5} \\left[ \\frac{24}{5} - \\frac{81 \\cdot 221}{88 \\cdot 40} \\right] = \\frac{-10}{99} \\left[ \\frac{24 \\cdot 704}{3520} - \\frac{17901}{3520} \\right]$$\n$$= \\frac{-10}{99} \\left[ \\frac{16896 - 17901}{3520} \\right] = \\frac{-10}{99} \\left[ \\frac{-1005}{3520} \\right] = \\frac{10 \\cdot 1005}{99 \\cdot 3520}$$\nWe simplify the fraction:\n$$= \\frac{10050}{348480} = \\frac{1005}{34848}$$\nSince $1+0+0+5=6$ and $3+4+8+4+8=27$, both are divisible by $3$.\n$$= \\frac{1005/3}{34848/3} = \\frac{335}{11616}$$\nThe prime factorization of the numerator is $335 = 5 \\times 67$. The denominator $11616$ is not divisible by $5$ or $67$. Thus, the fraction is irreducible.", "answer": "$$\\boxed{\\frac{335}{11616}}$$", "id": "3406001"}]}