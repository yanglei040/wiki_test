{"hands_on_practices": [{"introduction": "This exercise provides a foundational practice in the mechanics of the EKF derivation by extending the standard filter to second order. You will work from first principles to incorporate second-derivative information, directly observing how these terms correct for local nonlinearity in the predicted measurement and its covariance. Applying this to a concrete quadratic model [@problem_id:3375506] solidifies the link between a function's curvature and the filter's optimal gain.", "problem": "Consider a single-step measurement update in a nonlinear Bayesian data assimilation problem. Let the state be scalar with prior distribution $x \\sim \\mathcal{N}(m, P)$, and let the measurement model be $y = h(x) + v$, where $v \\sim \\mathcal{N}(0, R)$ is independent of $x$. Your task is to derive the second-order Extended Kalman Filter (EKF) measurement update quantities using a second-order Taylor expansion and Gaussian moment identities, starting from first principles. Specifically:\n\n1. Using a second-order Taylor expansion of $h(x)$ around the prior mean $m$, derive expressions for the predicted measurement mean and innovation variance that are accurate up to and including terms involving the second derivative $h''(m)$. You must show how the second derivative of $h$ contributes to the innovation mean and covariance, and justify any vanishing terms using the properties of Gaussian central moments.\n2. Show that, to this order, the state–measurement cross-covariance entering the Kalman gain remains $P h'(m)$.\n3. Define the second-order EKF gain $K_{2}$ as the ratio of this cross-covariance to the innovation variance you derived in part 1.\n4. Now specialize to the quadratic measurement function $h(x) = c x^{2}$, with a fixed nonzero constant $c \\in \\mathbb{R}$. Compute $K_{2}$ for this measurement and, for comparison, compute the first-order EKF gain $K_{1}$ obtained by truncating the expansion after the first derivative. \n\nProvide as your final answer the closed-form analytic expression for the ratio $K_{2}/K_{1}$ as a function of $c$, $m$, $P$, and $R$. No numerical evaluation is required, and no units are needed. Express your final answer as a single simplified algebraic expression.", "solution": "The state is a scalar random variable $x$ with a prior Gaussian distribution $x \\sim \\mathcal{N}(m, P)$, where $m$ is the prior mean and $P$ is the prior variance. The measurement model is given by $y = h(x) + v$, where the measurement noise $v$ is also Gaussian, $v \\sim \\mathcal{N}(0, R)$, and is independent of $x$.\n\nThe core of the second-order EKF is to approximate the nonlinear function $h(x)$ using a second-order Taylor series expansion around the prior mean $m$:\n$$\nh(x) \\approx h(m) + h'(m)(x-m) + \\frac{1}{2}h''(m)(x-m)^2\n$$\nwhere $h'(m)$ and $h''(m)$ are the first and second derivatives of $h(x)$ evaluated at $x=m$.\n\n### Part 1: Predicted Measurement Mean and Innovation Variance\n\nFirst, we derive the predicted measurement mean, $\\mu_y = E[y]$. Since $y = h(x) + v$ and $v$ has zero mean and is independent of $x$, we have $E[y] = E[h(x)] + E[v] = E[h(x)]$. Using the Taylor expansion for $h(x)$:\n$$\n\\mu_y = E[y] \\approx E\\left[h(m) + h'(m)(x-m) + \\frac{1}{2}h''(m)(x-m)^2\\right]\n$$\nBy the linearity of the expectation operator and using the Gaussian central moments $E[x-m] = 0$ and $E[(x-m)^2] = P$:\n$$\n\\mu_y \\approx h(m) + h'(m)E[x-m] + \\frac{1}{2}h''(m)E[(x-m)^2] = h(m) + \\frac{1}{2}h''(m)P\n$$\nThis shows that the second derivative $h''(m)$ introduces a bias correction term $\\frac{1}{2}h''(m)P$ to the predicted measurement mean.\n\nNext, we derive the innovation variance, $S = \\text{Var}(y)$. Since $x$ and $v$ are independent, $\\text{Var}(y) = \\text{Var}(h(x)) + \\text{Var}(v) = \\text{Var}(h(x)) + R$. We compute $\\text{Var}(h(x))$ using $\\text{Var}(Z) = E[(Z - E[Z])^2]$:\n$$\nh(x) - E[h(x)] \\approx \\left(h(m) + h'(m)(x-m) + \\frac{1}{2}h''(m)(x-m)^2\\right) - \\left(h(m) + \\frac{1}{2}h''(m)P\\right)\n$$\n$$\nh(x) - E[h(x)] \\approx h'(m)(x-m) + \\frac{1}{2}h''(m)\\left((x-m)^2 - P\\right)\n$$\nSquaring this expression and taking the expectation gives $\\text{Var}(h(x))$. The cross term vanishes because the third central moment of a Gaussian is zero ($E[(x-m)^3] = 0$). We use the fourth central moment $E[(x-m)^4] = 3P^2$.\n$$\n\\text{Var}(h(x)) \\approx E\\left[ (h'(m))^2(x-m)^2 \\right] + E\\left[ \\frac{1}{4}(h''(m))^2((x-m)^2 - P)^2 \\right]\n$$\n$$\n\\text{Var}(h(x)) \\approx (h'(m))^2 P + \\frac{1}{4}(h''(m))^2 E[(x-m)^4 - 2P(x-m)^2 + P^2]\n$$\n$$\n\\text{Var}(h(x)) \\approx (h'(m))^2 P + \\frac{1}{4}(h''(m))^2 (3P^2 - 2P^2 + P^2) = (h'(m))^2 P + \\frac{1}{2}(h''(m))^2 P^2\n$$\nThe total innovation variance is therefore:\n$$\nS \\approx (h'(m))^2 P + \\frac{1}{2}(h''(m))^2 P^2 + R\n$$\n\n### Part 2: State-Measurement Cross-Covariance\n\nThe cross-covariance is $\\text{Cov}(x, y) = \\text{Cov}(x, h(x)) = E[(x-m)(h(x) - E[h(x)])]$.\n$$\n\\text{Cov}(x, y) \\approx E\\left[ (x-m) \\left( h'(m)(x-m) + \\frac{1}{2}h''(m)\\left((x-m)^2 - P\\right) \\right) \\right]\n$$\n$$\n\\text{Cov}(x, y) \\approx h'(m)E[(x-m)^2] + \\frac{1}{2}h''(m)E[(x-m)^3 - P(x-m)]\n$$\nSince odd central moments are zero, the second term vanishes, leaving:\n$$\n\\text{Cov}(x, y) \\approx P h'(m)\n$$\n\n### Part 3: Second-Order EKF Gain\n\nThe second-order EKF gain, $K_2$, is the ratio of the cross-covariance to the innovation variance:\n$$\nK_2 = \\frac{\\text{Cov}(x, y)}{S} \\approx \\frac{P h'(m)}{(h'(m))^2 P + \\frac{1}{2}(h''(m))^2 P^2 + R}\n$$\n\n### Part 4: Specialization to $h(x) = c x^2$\n\nFor $h(x) = c x^2$, the derivatives are $h'(m) = 2cm$ and $h''(m) = 2c$.\nSubstituting these into the expression for $K_2$:\n$$\nK_2 = \\frac{P (2cm)}{(2cm)^2 P + \\frac{1}{2}(2c)^2 P^2 + R} = \\frac{2cmP}{4c^2m^2P + 2c^2P^2 + R}\n$$\nThe first-order EKF gain, $K_1$, uses an innovation variance that omits the second-derivative term: $S_1 = (h'(m))^2 P + R$.\n$$\nK_1 = \\frac{P h'(m)}{(h'(m))^2 P + R} = \\frac{P(2cm)}{(2cm)^2 P + R} = \\frac{2cmP}{4c^2m^2P + R}\n$$\nThe ratio $K_2 / K_1$ is:\n$$\n\\frac{K_2}{K_1} = \\frac{\\frac{2cmP}{4c^2m^2P + 2c^2P^2 + R}}{\\frac{2cmP}{4c^2m^2P + R}} = \\frac{4c^2m^2P + R}{4c^2m^2P + 2c^2P^2 + R}\n$$", "answer": "$$\n\\boxed{\\frac{4c^2m^2P + R}{4c^2m^2P + 2c^2P^2 + R}}\n$$", "id": "3375506"}, {"introduction": "The EKF's accuracy is fundamentally limited by its linear approximation of nonlinear dynamics. This practice guides you through a rigorous analysis to quantify the bias introduced by this first-order Taylor series truncation. By deriving worst-case error bounds [@problem_id:3375491], you will develop a quantitative understanding of the approximation error and the conditions under which it becomes significant, highlighting when higher-order filter variants are necessary.", "problem": "Consider a scalar nonlinear observation model in a data assimilation setting: a state $x \\in \\mathbb{R}$ has a Gaussian prior $x \\sim \\mathcal{N}(m, P)$ with variance $P \\in \\mathbb{R}_{>0}$, and the observation is $y = h(x) + v$, where $v$ is zero-mean observation noise independent of $x$ and $h:\\mathbb{R} \\to \\mathbb{R}$ is thrice continuously differentiable. The Extended Kalman Filter (EKF) predicts the observation via first-order linearization $h(x) \\approx h(m) + h^{\\prime}(m)(x - m)$, while a second-order EKF augments this with the quadratic term obtained by taking the prior expectation, that is $h(x) \\approx h(m) + h^{\\prime}(m)(x - m) + \\tfrac{1}{2} h^{\\prime\\prime}(m) (x - m)^{2}$, whose expectation contributes $\\tfrac{1}{2} h^{\\prime\\prime}(m) P$.\n\nDefine the EKF linearization bias as $b_{\\mathrm{EKF}} := \\mathbb{E}[h(x)] - h(m)$ and the second-order linearization bias as $b_{\\mathrm{SO}} := \\mathbb{E}[h(x)] - \\big(h(m) + \\tfrac{1}{2} h^{\\prime\\prime}(m) P\\big)$. Suppose that for all $x$ in an interval containing the bulk of the Gaussian prior mass, the following curvature bounds hold: $|h^{\\prime\\prime}(x)| \\leq K_{2}$ and $|h^{(3)}(x)| \\leq K_{3}$ for some known $K_{2}, K_{3} \\in \\mathbb{R}_{>0}$.\n\nUsing Taylor’s theorem with the Lagrange form of the remainder and basic Gaussian moment identities, derive tight worst-case upper bounds $B_{\\mathrm{EKF}}(P)$ and $B_{\\mathrm{SO}}(P)$ such that $|b_{\\mathrm{EKF}}| \\leq B_{\\mathrm{EKF}}(P)$ and $|b_{\\mathrm{SO}}| \\leq B_{\\mathrm{SO}}(P)$, expressed only in terms of $P$, $K_{2}$, and $K_{3}$. Then define the bias reduction factor $R(P) := \\dfrac{B_{\\mathrm{SO}}(P)}{B_{\\mathrm{EKF}}(P)}$ and provide its exact closed-form expression. State explicitly the condition on $P$, $K_{2}$, and $K_{3}$ under which second-order terms significantly reduce the linearization bias, interpreted as $R(P) < 1$.\n\nYour final answer must be the exact closed-form expression for $R(P)$, simplified as much as possible. No numerical rounding is required, and no units are involved.", "solution": "We derive the worst-case upper bounds for the linearization bias in first- and second-order Extended Kalman Filters (EKF) using Taylor's theorem with the Lagrange form of the remainder.\n\n**1. First-Order EKF Bias Bound ($B_{\\mathrm{EKF}}(P)$)**\n\nThe first-order EKF linearization bias is defined as $b_{\\mathrm{EKF}} = \\mathbb{E}[h(x)] - h(m)$. We expand $h(x)$ around the prior mean $m$ up to the first-order term:\n$$h(x) = h(m) + h^{\\prime}(m)(x - m) + \\frac{1}{2} h^{\\prime\\prime}(\\xi)(x - m)^{2}$$\nwhere $\\xi$ is a value between $m$ and $x$. Taking the expectation over $x \\sim \\mathcal{N}(m, P)$ and using $\\mathbb{E}[x - m] = 0$:\n$$\\mathbb{E}[h(x)] = h(m) + \\frac{1}{2}\\mathbb{E}[h^{\\prime\\prime}(\\xi)(x - m)^{2}]$$\nThe bias is therefore $b_{\\mathrm{EKF}} = \\frac{1}{2}\\mathbb{E}[h^{\\prime\\prime}(\\xi)(x - m)^{2}]$. To find an upper bound on its magnitude:\n$$|b_{\\mathrm{EKF}}| = \\left| \\frac{1}{2}\\mathbb{E}[h^{\\prime\\prime}(\\xi)(x - m)^{2}] \\right| \\leq \\frac{1}{2}\\mathbb{E}[|h^{\\prime\\prime}(\\xi)|(x - m)^{2}]$$\nUsing the given bound $|h^{\\prime\\prime}(x)| \\leq K_{2}$ and the fact that $\\mathbb{E}[(x - m)^{2}] = P$:\n$$|b_{\\mathrm{EKF}}| \\leq \\frac{1}{2} K_{2} \\mathbb{E}[(x - m)^{2}] = \\frac{1}{2}K_{2}P$$\nThis gives the tight worst-case upper bound $B_{\\mathrm{EKF}}(P) = \\frac{1}{2}K_{2}P$.\n\n**2. Second-Order Bias Bound ($B_{\\mathrm{SO}}(P)$)**\n\nThe second-order bias is $b_{\\mathrm{SO}} = \\mathbb{E}[h(x)] - \\left(h(m) + \\frac{1}{2} h^{\\prime\\prime}(m) P\\right)$. We use a higher-order Taylor expansion:\n$$h(x) = h(m) + h^{\\prime}(m)(x - m) + \\frac{1}{2}h^{\\prime\\prime}(m)(x - m)^{2} + \\frac{1}{6}h^{(3)}(\\zeta)(x - m)^{3}$$\nwhere $\\zeta$ is a value between $m$ and $x$. Taking the expectation:\n$$\\mathbb{E}[h(x)] = h(m) + \\frac{1}{2}h^{\\prime\\prime}(m)P + \\frac{1}{6}\\mathbb{E}[h^{(3)}(\\zeta)(x - m)^{3}]$$\nThe bias is $b_{\\mathrm{SO}} = \\frac{1}{6}\\mathbb{E}[h^{(3)}(\\zeta)(x - m)^{3}]$. Bounding its magnitude using $|h^{(3)}(x)| \\leq K_{3}$:\n$$|b_{\\mathrm{SO}}| \\leq \\frac{1}{6}\\mathbb{E}[|h^{(3)}(\\zeta)||x - m|^{3}] \\leq \\frac{K_{3}}{6}\\mathbb{E}[|x - m|^{3}]$$\nThe third absolute central moment of a Gaussian distribution $\\mathcal{N}(0, P)$ is $\\mathbb{E}[|x - m|^3] = 2\\sqrt{\\frac{2}{\\pi}} P^{3/2}$. Substituting this in:\n$$|b_{\\mathrm{SO}}| \\leq \\frac{K_{3}}{6} \\left( 2\\sqrt{\\frac{2}{\\pi}} P^{3/2} \\right) = \\frac{K_{3}}{3}\\sqrt{\\frac{2}{\\pi}} P^{3/2}$$\nThis gives the tight worst-case upper bound $B_{\\mathrm{SO}}(P) = \\frac{K_{3}}{3}\\sqrt{\\frac{2}{\\pi}} P^{3/2}$.\n\n**3. Bias Reduction Factor ($R(P)$)**\n\nThe bias reduction factor is the ratio of the two bounds:\n$$R(P) = \\frac{B_{\\mathrm{SO}}(P)}{B_{\\mathrm{EKF}}(P)} = \\frac{\\frac{K_{3}}{3}\\sqrt{\\frac{2}{\\pi}} P^{3/2}}{\\frac{1}{2}K_{2}P} = \\frac{2K_{3}}{3K_{2}} \\sqrt{\\frac{2}{\\pi}} \\sqrt{P} = \\frac{2K_{3}}{3K_{2}} \\sqrt{\\frac{2P}{\\pi}}$$\nThe condition for the second-order term to reduce the bias bound is $R(P) < 1$.", "answer": "$$\\boxed{\\frac{2K_{3}}{3K_{2}} \\sqrt{\\frac{2P}{\\pi}}}$$", "id": "3375491"}, {"introduction": "While algebraically elegant, the standard EKF equations can suffer from numerical instability in practice. This exercise examines two different but related formulas for the posterior covariance update: the simplified \"compact\" form and the numerically robust \"Joseph\" form. By demonstrating their algebraic equivalence when using the optimal Kalman gain [@problem_id:3375487], you will uncover the subtle but critical reasons why the Joseph form is indispensable for ensuring the covariance matrix remains symmetric and positive-definite in finite-precision computer implementations.", "problem": "Consider a discrete-time data assimilation setting with a nonlinear observation model used in the Extended Kalman Filter (EKF). Let the true state at time index $k$ be $x_k \\in \\mathbb{R}^n$ and the measurement be $y_k \\in \\mathbb{R}^m$ with model $y_k = h(x_k) + v_k$, where $h:\\mathbb{R}^n \\to \\mathbb{R}^m$ is differentiable and $v_k$ is zero-mean measurement noise with covariance $R_k \\in \\mathbb{S}_{+}^{m}$ (the set of symmetric positive semidefinite $m \\times m$ matrices). Suppose we have a prior estimate $x_{k|k-1}$ with prior error covariance $P_{k|k-1} \\in \\mathbb{S}_{+}^{n}$. Assume the usual independence assumptions of the EKF measurement update: the prior estimation error $e_{k|k-1} := x_k - x_{k|k-1}$ is independent of $v_k$, and both are zero mean. Let $H_k := \\left.\\frac{\\partial h}{\\partial x}\\right|_{x_{k|k-1}}$ denote the Jacobian of $h$ at the linearization point $x_{k|k-1}$.\n\nUnder the standard linearization used in the EKF measurement update and the above assumptions, the posterior error $e_{k|k} := x_k - x_{k|k}$ is an affine function of the prior error $e_{k|k-1}$ and of $v_k$, and the posterior covariance $P_{k|k}$ can be written in a form that is manifestly symmetric and positive semidefinite (often called the Joseph form). An alternative algebraic expression, sometimes used in implementations, omits an additive term and appears more compact.\n\nWhich of the following statements about the exact algebraic relationship between these two covariance update expressions in the EKF measurement update is correct?\n\nA. If the Kalman gain $K_k$ is computed as $K_k = P_{k|k-1} H_k^\\top S_k^{-1}$ with innovation covariance $S_k = H_k P_{k|k-1} H_k^\\top + R_k$ (assumed invertible), then the Joseph form and the compact expression are exactly equal in exact arithmetic; any discrepancy in practice is due to finite-precision effects. In particular, the Joseph form preserves symmetry and positive semidefiniteness by construction.\n\nB. The Joseph form and the compact expression are exactly equal if and only if $H_k$ is orthonormal and $R_k = 0$. Otherwise, the Joseph form is strictly larger because of the additive term involving $R_k$.\n\nC. The Joseph form and the compact expression are exactly equal only in the linear Kalman filter with linear $h$ and Gaussian noises, and only when the innovation happens to be zero at time $k$; in the EKF they generally differ even in exact arithmetic.\n\nD. The Joseph form and the compact expression can never be exactly equal for any nontrivial $R_k \\succ 0$; the compact expression is a heuristic approximation that always underestimates the true posterior covariance, regardless of how $K_k$ is computed.", "solution": "To determine the correct statement, we must derive the algebraic relationship between the two common forms of the EKF posterior covariance update.\n\nThe posterior error $e_{k|k}$ is defined as $e_{k|k} = x_k - x_{k|k}$. Using the standard EKF update rule $x_{k|k} = x_{k|k-1} + K_k(y_k - h(x_{k|k-1}))$ and the linearization $y_k \\approx h(x_{k|k-1}) + H_k(x_k - x_{k|k-1}) + v_k$, we can express the posterior error in terms of the prior error $e_{k|k-1} = x_k - x_{k|k-1}$ and the measurement noise $v_k$:\n$$ e_{k|k} \\approx (I - K_k H_k)e_{k|k-1} - K_k v_k $$\nThe posterior covariance is $P_{k|k} = E[e_{k|k}e_{k|k}^\\top]$. Since the prior error $e_{k|k-1}$ and measurement noise $v_k$ are assumed to be independent and zero-mean, the cross-term vanishes when we compute the expectation:\n$$ P_{k|k} = E[((I - K_k H_k)e_{k|k-1} - K_k v_k)((I - K_k H_k)e_{k|k-1} - K_k v_k)^\\top] $$\n$$ P_{k|k} = (I - K_k H_k) E[e_{k|k-1}e_{k|k-1}^\\top] (I - K_k H_k)^\\top + K_k E[v_k v_k^\\top] K_k^\\top $$\nSubstituting $P_{k|k-1} = E[e_{k|k-1}e_{k|k-1}^\\top]$ and $R_k = E[v_k v_k^\\top]$, we obtain the **Joseph form** of the covariance update, which is valid for any gain matrix $K_k$:\n$$ P_{k|k} = (I - K_k H_k) P_{k|k-1} (I - K_k H_k)^\\top + K_k R_k K_k^\\top $$\nThis form is numerically robust because it is manifestly symmetric and ensures positive semidefiniteness.\n\nNow, let's see how this relates to the \"compact\" form. We expand the Joseph form:\n$$ P_{k|k} = P_{k|k-1} - K_k H_k P_{k|k-1} - P_{k|k-1} H_k^\\top K_k^\\top + K_k H_k P_{k|k-1} H_k^\\top K_k^\\top + K_k R_k K_k^\\top $$\n$$ P_{k|k} = P_{k|k-1} - K_k H_k P_{k|k-1} - P_{k|k-1} H_k^\\top K_k^\\top + K_k (H_k P_{k|k-1} H_k^\\top + R_k) K_k^\\top $$\nLet $S_k = H_k P_{k|k-1} H_k^\\top + R_k$ be the innovation covariance. The expression is:\n$$ P_{k|k} = P_{k|k-1} - K_k H_k P_{k|k-1} - P_{k|k-1} H_k^\\top K_k^\\top + K_k S_k K_k^\\top $$\nThis equation holds for any gain $K_k$. If we specifically use the **optimal Kalman gain**, defined as $K_k = P_{k|k-1} H_k^\\top S_k^{-1}$, we can simplify further. From the gain definition, we have $K_k S_k = P_{k|k-1} H_k^\\top$. Substituting this into the previous equation:\n$$ P_{k|k} = P_{k|k-1} - K_k H_k P_{k|k-1} - (K_k S_k) K_k^\\top + K_k S_k K_k^\\top $$\nThe last two terms cancel out, yielding:\n$$ P_{k|k} = P_{k|k-1} - K_k H_k P_{k|k-1} = (I - K_k H_k) P_{k|k-1} $$\nThis is the **compact form**. Our derivation shows that the Joseph form and the compact form are algebraically identical *if and only if* the optimal Kalman gain is used.\n\nBased on this derivation:\n- **Option A** is correct. It states that the two forms are exactly equal in exact arithmetic when the optimal Kalman gain is used, and that the Joseph form has superior numerical properties. This matches our findings.\n- **Option B** is incorrect. The equivalence does not depend on $H_k$ being orthonormal or $R_k$ being zero.\n- **Option C** is incorrect. The equivalence holds for the EKF linearization and is independent of the innovation's value.\n- **Option D** is incorrect. The forms can be exactly equal, and the compact form is a direct algebraic consequence, not a heuristic.", "answer": "$$\\boxed{A}$$", "id": "3375487"}]}