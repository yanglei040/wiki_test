{"hands_on_practices": [{"introduction": "This exercise illuminates a fundamental advantage of the Unscented Kalman Filter over the Extended Kalman Filter. By analyzing a simple quadratic system, you will analytically demonstrate how the UKF's symmetric sampling scheme correctly captures the second-order effects that the EKF's linearization ignores. This practice reveals the source of the UKF's improved accuracy for many nonlinear systems [@problem_id:3380789].", "problem": "Consider a scalar, discrete-time, noise-driven nonlinear dynamical system for a state $x \\in \\mathbb{R}$ given by\n$$\nx_{k+1} = f(x_k) + w_k, \\quad f(x) = x + \\beta x^{2},\n$$\nwhere $\\beta \\in \\mathbb{R}$ is a known parameter, and $w_k$ is zero-mean process noise that is independent of $x_k$. Suppose the prior distribution at time $k$ is Gaussian with mean and variance\n$$\nx_k \\sim \\mathcal{N}(m, P), \\quad P > 0.\n$$\nYou are asked to analyze the predictive mean under two Gaussian filtering approximations.\n\nTasks:\n1. Using the definition of the Extended Kalman Filter (EKF), which linearizes $f$ to first order about the prior mean, derive the predictive mean $\\mu_{\\mathrm{EKF}}$.\n2. Using the Unscented Kalman Filter (UKF) with the Unscented Transform (UT), dimension $L=1$, and general scaling parameters $\\alpha > 0$ and $\\kappa \\in \\mathbb{R}$, derive the predictive mean $\\mu_{\\mathrm{UKF}}$. Use the standard sigma-point construction for $L=1$:\n   - Define $\\lambda = \\alpha^{2}(L+\\kappa) - L$.\n   - Define sigma points\n     $$\n     \\chi_{0} = m, \\quad \\chi_{1} = m + \\sqrt{(L+\\lambda)P}, \\quad \\chi_{2} = m - \\sqrt{(L+\\lambda)P}.\n     $$\n   - Use mean weights\n     $$\n     W_{0}^{(m)} = \\frac{\\lambda}{L+\\lambda}, \\quad W_{1}^{(m)} = W_{2}^{(m)} = \\frac{1}{2(L+\\lambda)}.\n     $$\n   The third UT parameter that appears in covariance formulas is not needed here. Show any dependence or independence on the UT parameters explicitly.\n3. Using only the properties of Gaussian expectations of polynomials, write the exact predictive mean $\\mu_{\\mathrm{exact}} = \\mathbb{E}[f(x_k)]$ as a function of $m$, $P$, and $\\beta$.\n4. By comparing your EKF and UKF expressions, quantify the second-order bias difference, defined here as the difference in predictive means $\\mu_{\\mathrm{UKF}} - \\mu_{\\mathrm{EKF}}$, as a function of $P$, $m$, and $\\beta$. Provide your final answer as a single simplified analytical expression. No numerical evaluation is required.\n\nAnswer form requirement: Your final submitted answer must be a single closed-form analytical expression. Do not include units. Do not round.", "solution": "The problem asks for an analysis of the predictive mean for a scalar nonlinear system under three different computation methods: the Extended Kalman Filter (EKF), the Unscented Kalman Filter (UKF), and the exact analytical expectation. The final goal is to quantify the difference between the UKF and EKF predictive means.\n\nThe system is described by the state-space model $x_{k+1} = f(x_k) + w_k$, with the nonlinear function $f(x) = x + \\beta x^2$. The state at time $k$, $x_k$, is a random variable following a Gaussian distribution, $x_k \\sim \\mathcal{N}(m, P)$, where $m$ is the mean and $P$ is the variance. The process noise $w_k$ is zero-mean, $\\mathbb{E}[w_k] = 0$, and independent of $x_k$. The predictive mean at time $k+1$ is $\\mathbb{E}[x_{k+1}] = \\mathbb{E}[f(x_k) + w_k] = \\mathbb{E}[f(x_k)] + \\mathbb{E}[w_k] = \\mathbb{E}[f(x_k)]$. Therefore, the task reduces to computing the expectation of the transformed state, $\\mathbb{E}[f(x_k)]$, using the specified methods.\n\n1. Derivation of the EKF Predictive Mean, $\\mu_{\\mathrm{EKF}}$\n\nThe Extended Kalman Filter approximates the nonlinear function $f(x)$ with a first-order Taylor series expansion around the mean of the state, $m$. The approximation is:\n$$\nf(x_k) \\approx f(m) + f'(m)(x_k - m)\n$$\nThe derivative of $f(x) = x + \\beta x^2$ with respect to $x$ is $f'(x) = 1 + 2\\beta x$. Evaluating this at $x=m$ gives $f'(m) = 1 + 2\\beta m$.\n\nThe EKF predictive mean, $\\mu_{\\mathrm{EKF}}$, is the expectation of this linearized function:\n$$\n\\mu_{\\mathrm{EKF}} = \\mathbb{E}[f(m) + f'(m)(x_k - m)]\n$$\nBy the linearity of expectation:\n$$\n\\mu_{\\mathrm{EKF}} = \\mathbb{E}[f(m)] + \\mathbb{E}[f'(m)(x_k - m)]\n$$\nSince $m$ is a constant, $f(m)$ and $f'(m)$ are also constants with respect to the expectation over $x_k$. Thus, we can write:\n$$\n\\mu_{\\mathrm{EKF}} = f(m) + f'(m)\\mathbb{E}[x_k - m]\n$$\nBy definition, the expectation of $x_k$ is $m$, so $\\mathbb{E}[x_k - m] = \\mathbb{E}[x_k] - m = m - m = 0$.\nTherefore, the second term vanishes:\n$$\n\\mu_{\\mathrm{EKF}} = f(m) = m + \\beta m^2\n$$\n\n2. Derivation of the UKF Predictive Mean, $\\mu_{\\mathrm{UKF}}$\n\nThe Unscented Kalman Filter uses the Unscented Transform (UT) to estimate the mean of the transformed distribution. It involves propagating a set of deterministically chosen sigma points through the true nonlinear function $f(x)$ and computing a weighted average.\n\nThe parameters for the UT are given for dimension $L=1$:\n- Scaling parameter $\\lambda = \\alpha^2(L+\\kappa) - L = \\alpha^2(1+\\kappa) - 1$.\n- The term $L+\\lambda = \\alpha^2(1+\\kappa)$.\n- The sigma points are:\n  - $\\chi_0 = m$\n  - $\\chi_1 = m + \\sqrt{(L+\\lambda)P} = m + \\sqrt{\\alpha^2(1+\\kappa)P}$\n  - $\\chi_2 = m - \\sqrt{(L+\\lambda)P} = m - \\sqrt{\\alpha^2(1+\\kappa)P}$\n- The weights for the mean are:\n  - $W_0^{(m)} = \\frac{\\lambda}{L+\\lambda} = \\frac{\\lambda}{1+\\lambda}$\n  - $W_1^{(m)} = W_2^{(m)} = \\frac{1}{2(L+\\lambda)} = \\frac{1}{2(1+\\lambda)}$\n\nThe sum of the weights is $W_0^{(m)} + W_1^{(m)} + W_2^{(m)} = \\frac{\\lambda}{1+\\lambda} + 2\\left(\\frac{1}{2(1+\\lambda)}\\right) = \\frac{\\lambda+1}{1+\\lambda} = 1$.\n\nThe UKF predictive mean is $\\mu_{\\mathrm{UKF}} = \\sum_{i=0}^{2} W_i^{(m)} f(\\chi_i)$.\nFirst, evaluate $f(x) = x + \\beta x^2$ at each sigma point:\n- $f(\\chi_0) = f(m) = m + \\beta m^2$\n- $f(\\chi_1) = (m + \\sqrt{(1+\\lambda)P}) + \\beta(m + \\sqrt{(1+\\lambda)P})^2 = m + \\sqrt{(1+\\lambda)P} + \\beta(m^2 + 2m\\sqrt{(1+\\lambda)P} + (1+\\lambda)P)$\n- $f(\\chi_2) = (m - \\sqrt{(1+\\lambda)P}) + \\beta(m - \\sqrt{(1+\\lambda)P})^2 = m - \\sqrt{(1+\\lambda)P} + \\beta(m^2 - 2m\\sqrt{(1+\\lambda)P} + (1+\\lambda)P)$\n\nNow, we compute the weighted sum:\n$$\n\\mu_{\\mathrm{UKF}} = W_0^{(m)}f(\\chi_0) + W_1^{(m)}f(\\chi_1) + W_2^{(m)}f(\\chi_2)\n$$\nSince $W_1^{(m)} = W_2^{(m)}$, we can group terms:\n$$\n\\mu_{\\mathrm{UKF}} = W_0^{(m)}f(\\chi_0) + W_1^{(m)}(f(\\chi_1) + f(\\chi_2))\n$$\nLet's compute the sum $f(\\chi_1) + f(\\chi_2)$:\n$$\nf(\\chi_1) + f(\\chi_2) = (2m) + \\beta( (m^2 + 2m\\sqrt{\\dots} + (1+\\lambda)P) + (m^2 - 2m\\sqrt{\\dots} + (1+\\lambda)P) )\n$$\n$$\nf(\\chi_1) + f(\\chi_2) = 2m + \\beta(2m^2 + 2(1+\\lambda)P) = 2(m + \\beta m^2) + 2\\beta(1+\\lambda)P\n$$\nSubstituting this back into the expression for $\\mu_{\\mathrm{UKF}}$:\n$$\n\\mu_{\\mathrm{UKF}} = W_0^{(m)}(m + \\beta m^2) + W_1^{(m)}[2(m + \\beta m^2) + 2\\beta(1+\\lambda)P]\n$$\n$$\n\\mu_{\\mathrm{UKF}} = (W_0^{(m)} + 2W_1^{(m)})(m + \\beta m^2) + 2W_1^{(m)}\\beta(1+\\lambda)P\n$$\nAs the sum of weights is $W_0^{(m)} + 2W_1^{(m)} = 1$, and $2W_1^{(m)} = 2\\left(\\frac{1}{2(1+\\lambda)}\\right) = \\frac{1}{1+\\lambda}$, this simplifies to:\n$$\n\\mu_{\\mathrm{UKF}} = 1 \\cdot (m + \\beta m^2) + \\frac{1}{1+\\lambda}\\beta(1+\\lambda)P = m + \\beta m^2 + \\beta P\n$$\nThe UKF predictive mean is $\\mu_{\\mathrm{UKF}} = m + \\beta m^2 + \\beta P$. Notably, this result is independent of the UT scaling parameters $\\alpha$ and $\\kappa$.\n\n3. Derivation of the Exact Predictive Mean, $\\mu_{\\mathrm{exact}}$\n\nThe exact predictive mean is the true expectation of $f(x_k)$ given that $x_k \\sim \\mathcal{N}(m, P)$.\n$$\n\\mu_{\\mathrm{exact}} = \\mathbb{E}[f(x_k)] = \\mathbb{E}[x_k + \\beta x_k^2]\n$$\nUsing the linearity of expectation:\n$$\n\\mu_{\\mathrm{exact}} = \\mathbb{E}[x_k] + \\beta \\mathbb{E}[x_k^2]\n$$\nWe are given $\\mathbb{E}[x_k] = m$. The second moment $\\mathbb{E}[x_k^2]$ is related to the mean and variance by the formula $\\mathrm{Var}(x_k) = \\mathbb{E}[x_k^2] - (\\mathbb{E}[x_k])^2$.\nWith $\\mathrm{Var}(x_k) = P$, we have:\n$$\n\\mathbb{E}[x_k^2] = \\mathrm{Var}(x_k) + (\\mathbb{E}[x_k])^2 = P + m^2\n$$\nSubstituting this into the expression for $\\mu_{\\mathrm{exact}}$:\n$$\n\\mu_{\\mathrm{exact}} = m + \\beta (P + m^2) = m + \\beta m^2 + \\beta P\n$$\nThis demonstrates that for a quadratic nonlinearity, the Unscented Transform provides the exact predictive mean, i.e., $\\mu_{\\mathrm{UKF}} = \\mu_{\\mathrm{exact}}$.\n\n4. Calculation of the Second-Order Bias Difference\n\nThe problem defines the second-order bias difference as the difference between the UKF and EKF predictive means, $\\mu_{\\mathrm{UKF}} - \\mu_{\\mathrm{EKF}}$. Using the results from the previous parts:\n- $\\mu_{\\mathrm{EKF}} = m + \\beta m^2$\n- $\\mu_{\\mathrm{UKF}} = m + \\beta m^2 + \\beta P$\n\nThe difference is:\n$$\n\\mu_{\\mathrm{UKF}} - \\mu_{\\mathrm{EKF}} = (m + \\beta m^2 + \\beta P) - (m + \\beta m^2)\n$$\n$$\n\\mu_{\\mathrm{UKF}} - \\mu_{\\mathrm{EKF}} = \\beta P\n$$\nThis term, $\\beta P$, represents the leading-order error, or bias, in the EKF's mean prediction. This bias arises because the EKF approximation linearizes the function and thus neglects the effects of its curvature (second derivative) on the mean of the transformed distribution. The UKF, through its symmetric set of sigma points, correctly captures this second-order term, resulting in a more accurate mean estimate that, in this quadratic case, is exact. The difference between the two filters' predictions is precisely this second-order term.\nThe final required expression is the analytical form of this difference.", "answer": "$$\\boxed{\\beta P}$$", "id": "3380789"}, {"introduction": "This practice moves from theory to a concrete numerical application for a two-dimensional system. You will perform a single prediction step, which involves generating sigma points, propagating them through nonlinear dynamics, and reconstructing the predicted state mean. This exercise reinforces the core mechanics of the Unscented Transform and its implementation details [@problem_id:2888306].", "problem": "Consider the discrete-time nonlinear stochastic system\n$$\nx_{k+1} \\;=\\; \\begin{bmatrix} x_{1,k} + \\sin(x_{2,k}) \\\\ x_{2,k} \\end{bmatrix} \\;+\\; w_k,\n\\qquad\ny_k \\;=\\; \\begin{bmatrix} x_{1,k}^2 \\end{bmatrix} \\;+\\; v_k,\n$$\nwhere $w_k$ and $v_k$ are mutually independent, zero-mean, Gaussian white noises with covariance matrices $Q$ and $R$, respectively. Assume $Q = 0.01 I_2$, and note that the process noise enters additively. At time $k$, the prior state estimate is Gaussian with mean\n$$\n\\hat{x}_{k|k} \\;=\\; \\begin{bmatrix} 0.5 \\\\ 0.1 \\end{bmatrix}\n$$\nand covariance\n$$\nP_{k|k} \\;=\\; \\begin{bmatrix} 0.04 & 0.01 \\\\ 0.01 & 0.09 \\end{bmatrix}.\n$$\nYou will perform one prediction step of the Unscented Kalman Filter (UKF), using the Unscented Transform with parameters $\\alpha = 10^{-3}$, $\\beta = 2$, $\\kappa = 0$, and the standard sigma-point construction. Take the argument of the sine function in radians.\n\nStarting from the foundational definition that the prediction step seeks to approximate the first two moments of the random variable $x_{k+1} = f(x_k) + w_k$ (with $f(x) = [x_1 + \\sin(x_2),\\; x_2]^\\top$) by matching moments of a transformed set of deterministically chosen sigma points drawn from the Gaussian prior, derive the Unscented Transform–based predicted mean of the first state component, $\\hat{x}_{1,k+1|k}$.\n\nReport the single scalar $\\hat{x}_{1,k+1|k}$. Round your answer to eight significant figures. No units are required.", "solution": "The problem requires the calculation of the predicted state mean, specifically the first component $\\hat{x}_{1,k+1|k}$, for a given discrete-time nonlinear system using the Unscented Kalman Filter (UKF) prediction step.\n\nThe problem is first validated.\nGivens:\n- State dynamics: $x_{k+1} = f(x_k) + w_k$, where $f(x) = \\begin{bmatrix} x_1 + \\sin(x_2) \\\\ x_2 \\end{bmatrix}$.\n- Process noise: $w_k \\sim \\mathcal{N}(0, Q)$, with $Q = 0.01 I_2 = \\begin{bmatrix} 0.01 & 0 \\\\ 0 & 0.01 \\end{bmatrix}$.\n- Measurement equation: $y_k = h(x_k) + v_k$, where $h(x) = [x_1^2]$.\n- Mean of state estimate at time $k$: $\\hat{x}_{k|k} = \\begin{bmatrix} 0.5 \\\\ 0.1 \\end{bmatrix}$.\n- Covariance of state estimate at time $k$: $P_{k|k} = \\begin{bmatrix} 0.04 & 0.01 \\\\ 0.01 & 0.09 \\end{bmatrix}$.\n- UKF parameters: $\\alpha = 10^{-3}$, $\\beta = 2$, $\\kappa = 0$. Sine argument is in radians.\n\nValidation Verdict:\nThe problem is scientifically grounded, well-posed, objective, and complete for the task required. It is a standard application of the Unscented Kalman Filter. The provided data is consistent and sufficient to perform the prediction step. The problem is deemed valid.\n\nThe UKF prediction step computes the predicted mean $\\hat{x}_{k+1|k}$ and covariance $P_{k+1|k}$ by propagating a set of deterministically chosen \"sigma points\" through the nonlinear state dynamics. Since the process noise $w_k$ is additive and zero-mean, its contribution to the predicted mean is zero. The predicted mean is computed as the weighted average of the propagated sigma points:\n$$ \\hat{x}_{k+1|k} = \\sum_{i=0}^{2n} W_i^{(m)} \\mathcal{X}_{i, k+1|k}^* $$\nwhere $\\mathcal{X}_{i, k+1|k}^* = f(\\mathcal{X}_{i, k|k})$ are the propagated sigma points and $W_i^{(m)}$ are the weights for the mean.\n\nStep 1: Calculate UKF Weights\nThe state dimension is $n=2$. The scaling parameter $\\lambda$ is given by:\n$$ \\lambda = \\alpha^2(n + \\kappa) - n $$\nSubstituting the given values:\n$$ \\lambda = (10^{-3})^2(2 + 0) - 2 = 2 \\times 10^{-6} - 2 = -1.999998 $$\nThe weights for the mean, $W_i^{(m)}$, are:\n$$ W_0^{(m)} = \\frac{\\lambda}{n+\\lambda} = \\frac{-1.999998}{2 - 1.999998} = \\frac{-1.999998}{0.000002} = -999999 $$\n$$ W_i^{(m)} = \\frac{1}{2(n+\\lambda)} = \\frac{1}{2(0.000002)} = 250000 \\quad \\text{for } i=1, \\dots, 2n $$\nNote that the weights sum to $1$: $W_0^{(m)} + 2n \\cdot W_i^{(m)} = -999999 + 4 \\cdot 250000 = 1$. The large magnitudes of these weights warn of potential numerical instability due to catastrophic cancellation if computed naively.\n\nStep 2: Generate Sigma Points\nThe $2n+1 = 5$ sigma points $\\mathcal{X}_{i, k|k}$ are generated based on the mean $\\hat{x}_{k|k}$ and covariance $P_{k|k}$.\n$$ \\mathcal{X}_{0, k|k} = \\hat{x}_{k|k} $$\n$$ \\mathcal{X}_{i, k|k} = \\hat{x}_{k|k} + (\\sqrt{(n+\\lambda)P_{k|k}})_i \\quad \\text{for } i=1, \\dots, n $$\n$$ \\mathcal{X}_{i+n, k|k} = \\hat{x}_{k|k} - (\\sqrt{(n+\\lambda)P_{k|k}})_i \\quad \\text{for } i=1, \\dots, n $$\nwhere $(\\cdot)_i$ denotes the $i$-th column of the matrix square root, typically computed via Cholesky decomposition. Let $S = \\sqrt{(n+\\lambda)P_{k|k}}$.\n$$ (n+\\lambda)P_{k|k} = 2 \\times 10^{-6} \\begin{bmatrix} 0.04 & 0.01 \\\\ 0.01 & 0.09 \\end{bmatrix} = \\begin{bmatrix} 8 \\times 10^{-8} & 2 \\times 10^{-8} \\\\ 2 \\times 10^{-8} & 1.8 \\times 10^{-7} \\end{bmatrix} $$\nThe Cholesky decomposition $S S^\\top = (n+\\lambda)P_{k|k}$ yields a lower triangular matrix $S$:\n$$ S_{11} = \\sqrt{8 \\times 10^{-8}} = 2\\sqrt{2} \\times 10^{-4} \\approx 2.8284 \\times 10^{-4} $$\n$$ S_{21} = \\frac{2 \\times 10^{-8}}{S_{11}} = \\frac{1}{\\sqrt{2}} \\times 10^{-4} \\approx 0.7071 \\times 10^{-4} $$\n$$ S_{12} = 0 $$\n$$ S_{22} = \\sqrt{(1.8 \\times 10^{-7}) - S_{21}^2} = \\sqrt{1.8 \\times 10^{-7} - 0.5 \\times 10^{-8}} = \\sqrt{17.5 \\times 10^{-8}} \\approx 4.1833 \\times 10^{-4} $$\nThe displacement vectors (columns of $S$) are $d_1 = [S_{11}, S_{21}]^\\top$ and $d_2 = [S_{12}, S_{22}]^\\top = [0, S_{22}]^\\top$.\n\nStep 3: Propagate Sigma Points and Calculate Predicted Mean\nThe sigma points are propagated through the state transition function $f(x) = [x_1 + \\sin(x_2), x_2]^\\top$. We are interested in the first component of the predicted mean, $\\hat{x}_{1, k+1|k}$.\nThe propagated points' first components are:\n$$ \\mathcal{X}_{1,0}^* = (\\hat{x}_{1,k|k}) + \\sin(\\hat{x}_{2,k|k}) = 0.5 + \\sin(0.1) $$\n$$ \\mathcal{X}_{1,1}^* = (\\hat{x}_{1,k|k} + S_{11}) + \\sin(\\hat{x}_{2,k|k} + S_{21}) $$\n$$ \\mathcal{X}_{1,2}^* = (\\hat{x}_{1,k|k} + S_{12}) + \\sin(\\hat{x}_{2,k|k} + S_{22}) = 0.5 + \\sin(0.1 + S_{22}) $$\n$$ \\mathcal{X}_{1,3}^* = (\\hat{x}_{1,k|k} - S_{11}) + \\sin(\\hat{x}_{2,k|k} - S_{21}) $$\n$$ \\mathcal{X}_{1,4}^* = (\\hat{x}_{1,k|k} - S_{12}) + \\sin(\\hat{x}_{2,k|k} - S_{22}) = 0.5 + \\sin(0.1 - S_{22}) $$\nThe predicted mean is the weighted sum:\n$$ \\hat{x}_{1,k+1|k} = W_0^{(m)} \\mathcal{X}_{1,0}^* + \\sum_{i=1}^{4} W_i^{(m)} \\mathcal{X}_{1,i}^* $$\nTo avoid numerical instability from subtracting large numbers, we simplify this expression. Using $W_0^{(m)} = 1 - 4W_i^{(m)}$:\n$$ \\hat{x}_{1,k+1|k} = (1 - 4W_i^{(m)})\\mathcal{X}_{1,0}^* + W_i^{(m)} \\sum_{i=1}^{4} \\mathcal{X}_{1,i}^* = \\mathcal{X}_{1,0}^* + W_i^{(m)} \\left(\\sum_{i=1}^{4} \\mathcal{X}_{1,i}^* - 4\\mathcal{X}_{1,0}^* \\right) $$\nLet's analyze the sum term:\n$$ \\sum_{i=1}^{4} \\mathcal{X}_{1,i}^* = (0.5+S_{11})+(0.5)+(0.5-S_{11})+(0.5) + \\sin(0.1+S_{21}) + \\sin(0.1+S_{22}) + \\sin(0.1-S_{21}) + \\sin(0.1-S_{22}) $$\n$$ = 2 + [\\sin(0.1+S_{21}) + \\sin(0.1-S_{21})] + [\\sin(0.1+S_{22}) + \\sin(0.1-S_{22})] $$\nUsing the identity $\\sin(A+B)+\\sin(A-B) = 2\\sin(A)\\cos(B)$:\n$$ = 2 + 2\\sin(0.1)\\cos(S_{21}) + 2\\sin(0.1)\\cos(S_{22}) $$\nThe term in the parenthesis of the mean formula becomes:\n$$ \\sum_{i=1}^{4} \\mathcal{X}_{1,i}^* - 4\\mathcal{X}_{1,0}^* = 2 + 2\\sin(0.1)[\\cos(S_{21}) + \\cos(S_{22})] - 4(0.5 + \\sin(0.1)) $$\n$$ = 2 + 2\\sin(0.1)[\\cos(S_{21}) + \\cos(S_{22})] - 2 - 4\\sin(0.1) $$\n$$ = 2\\sin(0.1)[\\cos(S_{21}) + \\cos(S_{22}) - 2] $$\nSubstituting this back into the formula for $\\hat{x}_{1,k+1|k}$ and using $W_i^{(m)} = 1/(2(n+\\lambda))$:\n$$ \\hat{x}_{1,k+1|k} = \\mathcal{X}_{1,0}^* + \\frac{1}{2(n+\\lambda)} \\left( 2\\sin(0.1)[\\cos(S_{21}) + \\cos(S_{22}) - 2] \\right) $$\n$$ \\hat{x}_{1,k+1|k} = 0.5 + \\sin(0.1) + \\frac{\\sin(0.1)}{n+\\lambda} [\\cos(S_{21}) + \\cos(S_{22}) - 2] $$\nSince $S_{21}$ and $S_{22}$ are very small, we use the Taylor expansion $\\cos(x) \\approx 1 - x^2/2$. The term in brackets becomes:\n$$ [\\cos(S_{21}) + \\cos(S_{22}) - 2] \\approx (1 - S_{21}^2/2) + (1 - S_{22}^2/2) - 2 = -\\frac{1}{2}(S_{21}^2 + S_{22}^2) $$\nFrom the properties of Cholesky decomposition, $S_{21}^2 + S_{22}^2 = ((n+\\lambda)P_{k|k})_{22} = (n+\\lambda)P_{22,k|k}$.\n$$ [\\cos(S_{21}) + \\cos(S_{22}) - 2] \\approx -\\frac{1}{2}(n+\\lambda)P_{22,k|k} $$\nThis approximation is extremely accurate for the given parameters. Substituting into the expression for the mean:\n$$ \\hat{x}_{1,k+1|k} \\approx 0.5 + \\sin(0.1) + \\frac{\\sin(0.1)}{n+\\lambda} \\left[ -\\frac{1}{2}(n+\\lambda)P_{22,k|k} \\right] $$\n$$ \\hat{x}_{1,k+1|k} \\approx 0.5 + \\sin(0.1) - \\frac{1}{2} P_{22,k|k} \\sin(0.1) $$\n$$ \\hat{x}_{1,k+1|k} \\approx 0.5 + \\sin(0.1) \\left( 1 - \\frac{P_{22,k|k}}{2} \\right) $$\nNow, we substitute the numerical values: $P_{22,k|k}=0.09$.\n$$ \\hat{x}_{1,k+1|k} \\approx 0.5 + \\sin(0.1) \\left( 1 - \\frac{0.09}{2} \\right) = 0.5 + 0.955 \\sin(0.1) $$\nUsing a high-precision value for $\\sin(0.1) \\approx 0.09983341664682815$:\n$$ \\hat{x}_{1,k+1|k} \\approx 0.5 + 0.955 \\times 0.09983341664682815 $$\n$$ \\hat{x}_{1,k+1|k} \\approx 0.5 + 0.0953409128977209 $$\n$$ \\hat{x}_{1,k+1|k} \\approx 0.5953409128977209 $$\nRounding to eight significant figures, the result is $0.59534091$.\n\nThis result is consistent with the second-order Taylor series expansion of the expected value, which the Unscented Transform is designed to approximate accurately. The symbolic simplification was necessary to obtain a numerically stable calculation.", "answer": "$$\\boxed{0.59534091}$$", "id": "2888306"}, {"introduction": "While powerful, the UKF is not infallible, and understanding its limitations is crucial for robust application. This exercise explores a scenario with a non-differentiable function where sigma points can straddle the discontinuity, leading to significant errors while the simpler EKF remains exact. This cautionary tale highlights the critical interaction between UKF parameters, state uncertainty, and the system's nonlinearity [@problem_id:3429813].", "problem": "Consider a deterministic, discrete-time, one-dimensional state-space system without process or observation noise. Let the state be denoted by $x_k \\in \\mathbb{R}$ and evolve according to a piecewise-linear switched map that composes a linear map with a sign switch. Specifically, define the dynamics function $f$ and the observation function $h$ by\n- $f(x) = a \\, s(x) \\, x$, where $s(x) = 1$ if $x \\ge 0$ and $s(x) = -1$ otherwise, and $a \\in \\mathbb{R}$ is a known constant,\n- $h(x) = x$.\nAssume there is no process noise and no measurement noise anywhere in the system, and no measurements are assimilated; only time propagation is performed.\n\nYou are given an initial Gaussian prior state distribution at time $k=0$, denoted by $\\mathcal{N}(m_0, P_0)$, with $m_0 \\in \\mathbb{R}$ and $P_0 > 0$. The true state $x_k$ evolves deterministically via $x_{k+1} = f(x_k)$ from $x_0 = m_0$, and because of the sign structure of $s(x)$ and the chosen parameters below, the true state remains on a single linear branch for all times of interest.\n\nYour task is to:\n- Implement the Extended Kalman Filter (EKF) and the Unscented Kalman Filter (UKF) mean and covariance propagators for the system above. For the UKF, use the standard unscented transform with parameters $\\alpha$, $\\kappa$, and $\\beta$, where $n=1$ is the state dimension. The parameter $\\beta$ should be set to the value that is standard for Gaussian priors.\n- Propagate both filters for $T$ time steps without any measurement updates.\n- For each tested pair $(\\alpha,\\kappa)$, compute the absolute error of the UKF mean at time $T$ with respect to the true state $x_T$ started from $x_0 = m_0$. Use the EKF to verify that it remains exact in this scenario (this verification is internal and does not need to be reported in the final output; it must be used in your code to ensure scientific and numerical correctness).\n- Return, for each $(\\alpha,\\kappa)$, a Boolean flag indicating whether the UKF diverges according to the criterion below.\n\nFundamental base and constraints:\n- Deterministic propagation: $x_{k+1} = f(x_k)$ with $f(x) = a \\, s(x) \\, x$ and no noise anywhere.\n- Gaussian prior propagation under a deterministic map: the exact mean evolves as $m_{k+1} = \\mathbb{E}[f(X_k)]$ for $X_k \\sim \\mathcal{N}(m_k, P_k)$, and the EKF and UKF approximate this mean differently.\n- The Extended Kalman Filter (EKF) linearizes the dynamics at the current mean and propagates mean and covariance using the local Jacobian as if the map were linear in a neighborhood of the mean.\n- The Unscented Kalman Filter (UKF) approximates the mean and covariance via the unscented transform from $2n+1$ sigma points with weights determined by $\\alpha$, $\\kappa$, and $\\beta$.\n\nScenario parameters to be used:\n- $n = 1$,\n- $a = 1.5$ (unitless),\n- $m_0 = 0.05$ (unitless),\n- $P_0 = 0.04$ (unitless),\n- $T = 10$ (unitless time steps),\n- $\\beta = 2$ (standard choice for Gaussian priors).\n\nDefinition of divergence:\n- Define the divergence threshold $\\tau = 0.5$ (unitless). The UKF is said to diverge for a given $(\\alpha,\\kappa)$ if the absolute error $|m_T^{\\text{UKF}} - x_T|$ exceeds $\\tau$, where $m_T^{\\text{UKF}}$ is the UKF mean after $T$ propagations, and $x_T$ is the true state after $T$ steps starting from $x_0 = m_0$.\n\nTest suite:\nEvaluate the divergence flag for the following $(\\alpha,\\kappa)$ pairs:\n- Case $1$: $(\\alpha,\\kappa) = (0.1, 0.0)$,\n- Case $2$: $(\\alpha,\\kappa) = (0.5, 0.0)$,\n- Case $3$: $(\\alpha,\\kappa) = (1.0, 0.0)$,\n- Case $4$: $(\\alpha,\\kappa) = (0.9, -0.5)$,\n- Case $5$: $(\\alpha,\\kappa) = (0.05, 2.0)$.\n\nAdditional requirements and notes:\n- Ensure that for all test cases the unscented transform scaling quantity $n+\\lambda$ is positive, where $\\lambda = \\alpha^2 (n+\\kappa) - n$, so that sigma points are well-defined. The test values above satisfy this condition.\n- Angles are not involved; no special unit handling is needed beyond the unitless scalars provided.\n- The program must compute, for each test case, a Boolean result indicating whether the UKF diverges (true) or not (false) according to the criterion above.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with Boolean values as produced by the programming language’s native Boolean string formatting (for example, $[True,False,True,False,True]$). The order must match the five cases above.", "solution": "The problem statement is evaluated as valid. It is scientifically grounded in estimation theory, mathematically well-posed, and all parameters and conditions are specified unambiguously.\n\nThe central task is to analyze the performance of the Unscented Kalman Filter (UKF) for a one-dimensional nonlinear system by propagating an initial Gaussian distribution over time and comparing the filter's predicted mean to the true deterministic state evolution.\n\n**1. System Dynamics and True State Evolution**\n\nThe state of the system evolves according to the discrete-time mapping $x_{k+1} = f(x_k)$. The function $f(x)$ is defined as $f(x) = a \\cdot s(x) \\cdot x$, where $a = 1.5$ and $s(x)$ is a sign-like function given by $s(x) = 1$ for $x \\ge 0$ and $s(x) = -1$ for $x < 0$. We can express $f(x)$ more compactly.\n- If $x \\ge 0$, $f(x) = a \\cdot (1) \\cdot x = ax = a|x|$.\n- If $x < 0$, $f(x) = a \\cdot (-1) \\cdot x = -ax = a|x|$.\nTherefore, for all $x \\in \\mathbb{R}$, the dynamics are given by the function $f(x) = a|x|$.\n\nThe true state is initialized at $x_0 = m_0 = 0.05$ and evolves deterministically. Since $x_0 > 0$ and $a = 1.5 > 1$, the first state is $x_1 = f(x_0) = 1.5 \\cdot |0.05| = 0.075$. By induction, if $x_k > 0$, then $x_{k+1} = a x_k > x_k > 0$. Thus, the true state $x_k$ remains positive for all $k \\ge 0$. The evolution is a simple geometric progression:\n$$x_k = a^k x_0$$\nAfter $T=10$ time steps, the true state is $x_T = a^T m_0 = (1.5)^{10} \\cdot 0.05$. This value serves as the ground truth for evaluating the UKF's accuracy.\n\n**2. Extended Kalman Filter (EKF) as a Reference**\n\nThe EKF propagates the mean $m_k$ and covariance $P_k$ by linearizing the dynamics around the current mean. The predicted mean is given by $m_{k+1}^{\\text{EKF}} = f(m_k^{\\text{EKF}})$.\nStarting with $m_0^{\\text{EKF}} = m_0 = 0.05 > 0$, the EKF mean also remains positive for all subsequent steps, following the same logic as the true state. Therefore, $f(m_k^{\\text{EKF}}) = a \\cdot |m_k^{\\text{EKF}}| = a \\cdot m_k^{\\text{EKF}}$. The EKF mean evolves as:\n$$m_{k+1}^{\\text{EKF}} = a \\cdot m_k^{\\text{EKF}}$$\nThis is the same recurrence relation as the true state $x_k$. Since they start from the same initial condition ($m_0^{\\text{EKF}} = x_0$), the EKF mean perfectly tracks the true state at all times: $m_k^{\\text{EKF}} = x_k$.\nThe problem's reference to the EKF being \"exact\" pertains to this fact: its mean estimate $m_T^{\\text{EKF}}$ is identical to the true deterministic state $x_T$. The error $|m_T^{\\text{UKF}} - x_T|$ is thus equivalent to $|m_T^{\\text{UKF}} - m_T^{\\text{EKF}}|$.\n\n**3. Unscented Kalman Filter (UKF) Propagation Algorithm**\n\nThe UKF avoids direct linearization by propagating a set of deterministically chosen \"sigma points\" through the true nonlinear function. The mean and covariance are then reconstructed from these propagated points.\n\nFor a state dimension of $n=1$, the UKF uses $2n+1=3$ sigma points. Their locations are determined by the current mean $m_k$, covariance $P_k$, and parameters $\\alpha$, $\\kappa$. The scaling parameter $\\lambda$ is computed as:\n$$\\lambda = \\alpha^2 (n+\\kappa) - n = \\alpha^2 (1+\\kappa) - 1$$\nThe sigma points for step $k$ are:\n$$\n\\begin{align*}\n\\mathcal{X}_k^{(0)} &= m_k^{\\text{UKF}} \\\\\n\\mathcal{X}_k^{(1)} &= m_k^{\\text{UKF}} + \\sqrt{(n+\\lambda)P_k^{\\text{UKF}}} \\\\\n\\mathcal{X}_k^{(2)} &= m_k^{\\text{UKF}} - \\sqrt{(n+\\lambda)P_k^{\\text{UKF}}}\n\\end{align*}\n$$\nThe weights for reconstructing the mean ($W^{(m)}$) and covariance ($W^{(c)}$) are defined using $\\lambda$, $\\alpha$, and $\\beta=2$:\n$$\n\\begin{align*}\nW_0^{(m)} &= \\frac{\\lambda}{n+\\lambda} & W_i^{(m)} &= \\frac{1}{2(n+\\lambda)} \\quad \\text{for } i=1,2 \\\\\nW_0^{(c)} &= \\frac{\\lambda}{n+\\lambda} + (1-\\alpha^2+\\beta) & W_i^{(c)} &= \\frac{1}{2(n+\\lambda)} \\quad \\text{for } i=1,2\n\\end{align*}\n$$\nThe propagation procedure at each time step $k$ is:\n1.  **Propagate Points**: Each sigma point is passed through the nonlinear function $f(x)=a|x|$:\n    $$\\mathcal{Y}_k^{(i)} = f(\\mathcal{X}_k^{(i)}) = a|\\mathcal{X}_k^{(i)}| \\quad \\text{for } i=0,1,2$$\n2.  **Predict Mean**: The predicted mean $m_{k+1}^{\\text{UKF}}$ is a weighted average of the propagated points:\n    $$m_{k+1}^{\\text{UKF}} = \\sum_{i=0}^{2} W_i^{(m)} \\mathcal{Y}_k^{(i)}$$\n3.  **Predict Covariance**: The predicted covariance $P_{k+1}^{\\text{UKF}}$ is the weighted sum of squared deviations:\n    $$P_{k+1}^{\\text{UKF}} = \\sum_{i=0}^{2} W_i^{(c)} (\\mathcal{Y}_k^{(i)} - m_{k+1}^{\\text{UKF}})^2$$\n\n**4. Analysis of UKF Behavior and Divergence**\n\nThe key to the UKF's behavior lies in how its sigma points interact with the nonlinearity at $x=0$. The spread of the sigma points is controlled by the term $\\sqrt{(n+\\lambda)P_k} = \\sqrt{\\alpha^2(1+\\kappa)P_k} = |\\alpha|\\sqrt{(1+\\kappa)P_k}$.\n-   If all three sigma points $\\mathcal{X}_k^{(0)}, \\mathcal{X}_k^{(1)}, \\mathcal{X}_k^{(2)}$ are non-negative, the transformation is effectively linear ($f(x)=ax$) across these points. In this case, the UKF mean update yields $m_{k+1}^{\\text{UKF}} = a m_k^{\\text{UKF}}$, behaving identically to the EKF. This occurs when the spread is small enough such that $m_k^{\\text{UKF}} - |\\alpha|\\sqrt{(1+\\kappa)P_k^{\\text{UKF}}} \\ge 0$.\n-   If the spread is large enough, the lower sigma point $\\mathcal{X}_k^{(2)}$ can become negative, while the central point $m_k^{\\text{UKF}}$ remains positive. The function $f(x)=a|x|$ \"folds\" this negative point to a positive value: $\\mathcal{Y}_k^{(2)} = a|\\mathcal{X}_k^{(2)}| > 0$. Because the propagated points $\\mathcal{Y}_k^{(1)}$ and $\\mathcal{Y}_k^{(2)}$ are now asymmetrically located relative to the propagated central point $\\mathcal{Y}_k^{(0)}$, the reconstructed mean $m_{k+1}^{\\text{UKF}}$ is skewed, typically to a much larger value than the true state evolution would suggest.\n\nThis error accumulates over the $T=10$ steps. Parameters $(\\alpha, \\kappa)$ that lead to a larger spread of sigma points will cause this crossing to happen earlier and more severely, increasing the final error $|m_T^{\\text{UKF}} - x_T|$. If this error exceeds the threshold $\\tau=0.5$, the filter is considered to have diverged.\n\n**5. Implementation and Evaluation**\n\nThe described UKF propagation algorithm is implemented for each of the five test cases $(\\alpha, \\kappa)$. Starting with the initial state $m_0=0.05$ and $P_0=0.04$, the filter is iterated for $T=10$ steps. The final UKF mean $m_T^{\\text{UKF}}$ is computed and compared against the true state $x_T=(1.5)^{10} \\cdot 0.05$. The boolean flag for divergence, $|m_T^{\\text{UKF}} - x_T| > 0.5$, is then determined for each case. The program will output these boolean flags in the specified order.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements the Unscented Kalman Filter (UKF) time propagation for a \n    1D nonlinear system and checks for divergence against the true state.\n    \"\"\"\n    # Define problem constants and parameters\n    a = 1.5         # Dynamics parameter\n    m0 = 0.05       # Initial mean\n    P0 = 0.04       # Initial covariance\n    T = 10          # Number of time steps\n    beta = 2.0      # UKF parameter for Gaussian priors\n    tau = 0.5       # Divergence threshold\n    n = 1.0         # State dimension (as float for calculations)\n\n    # Test suite of (alpha, kappa) pairs\n    test_cases = [\n        (0.1, 0.0),\n        (0.5, 0.0),\n        (1.0, 0.0),\n        (0.9, -0.5),\n        (0.05, 2.0),\n    ]\n\n    # Calculate the true final state x_T after T steps of deterministic evolution.\n    # The true state x_k remains positive since x0 > 0 and a > 1, so x_{k+1} = a*x_k.\n    x_T = m0 * (a ** T)\n\n    results = []\n    \n    # The dynamics function f(x) = a * s(x) * x simplifies to a * |x|.\n    def f_dynamics(x, a_param):\n        return a_param * np.abs(x)\n\n    # Iterate through each test case\n    for alpha, kappa in test_cases:\n        # Initialize UKF mean and covariance for the current case\n        m_ukf = m0\n        P_ukf = P0\n\n        # --- UKF Parameter and Weight Calculation ---\n        # Calculate the composite scaling parameter lambda\n        lambda_ = alpha**2 * (n + kappa) - n\n\n        # Calculate weights for the mean (Wm) and covariance (Wc)\n        # We need 2n+1 = 3 weights\n        Wm = np.full(3, 0.5 / (n + lambda_))\n        Wc = np.full(3, 0.5 / (n + lambda_))\n        Wm[0] = lambda_ / (n + lambda_)\n        Wc[0] = lambda_ / (n + lambda_) + (1 - alpha**2 + beta)\n\n        # --- UKF Time Propagation Loop ---\n        for _ in range(T):\n            # 1. Generate sigma points\n            # The square root of the matrix P_ukf must be handled carefully.\n            # For 1D, this is just the standard deviation.\n            # The scaling factor gamma is sqrt(n + lambda).\n            gamma = np.sqrt(n + lambda_)\n            sigma_offset = gamma * np.sqrt(P_ukf)\n            \n            # Sigma points for n=1 state\n            X0 = m_ukf\n            X1 = m_ukf + sigma_offset\n            X2 = m_ukf - sigma_offset\n            \n            # 2. Propagate sigma points through the nonlinear dynamics\n            Y0 = f_dynamics(X0, a)\n            Y1 = f_dynamics(X1, a)\n            Y2 = f_dynamics(X2, a)\n\n            # 3. Predict the new mean from propagated points\n            m_ukf = Wm[0] * Y0 + Wm[1] * Y1 + Wm[2] * Y2\n\n            # 4. Predict the new covariance\n            P_ukf = (Wc[0] * (Y0 - m_ukf)**2 +\n                     Wc[1] * (Y1 - m_ukf)**2 +\n                     Wc[2] * (Y2 - m_ukf)**2)\n\n        # After T steps, compare the final UKF mean to the true state\n        error = np.abs(m_ukf - x_T)\n        diverged = error > tau\n        results.append(diverged)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(str(r) for r in results)}]\")\n\nsolve()\n```", "id": "3429813"}]}