{"hands_on_practices": [{"introduction": "This first practice explores one of the most powerful features of the Kalman filter: its ability to update our knowledge of a state variable we don't even measure. By working through a simple two-dimensional system [@problem_id:779302], you will derive how the correlation between state components allows the Kalman gain to propagate information from a measured variable to an unmeasured one, thereby reducing its uncertainty. This exercise is fundamental to understanding how the filter leverages the entire covariance structure to produce an optimal estimate.", "problem": "Consider a two-dimensional system whose state is described by the vector $x = [x_1, x_2]^T$. We have a prior estimate of this state, $\\hat{x}^{-}$, with an associated prior error covariance matrix $P^{-}$. This matrix is given by:\n$$\nP^{-} = \\begin{pmatrix} \\sigma_1^2  \\rho \\sigma_1 \\sigma_2 \\\\ \\rho \\sigma_1 \\sigma_2  \\sigma_2^2 \\end{pmatrix}\n$$\nHere, $\\sigma_1^2$ and $\\sigma_2^2$ are the prior variances of the state components $x_1$ and $x_2$ respectively, and $\\rho$ is the correlation coefficient between them.\n\nA single measurement $z$ is obtained, which is a noisy observation of only the first state component, $x_1$. The measurement model is linear:\n$$\nz = Hx + v\n$$\nwhere $H = \\begin{pmatrix} 1  0 \\end{pmatrix}$, and $v$ is a zero-mean measurement noise with variance $R = \\sigma_v^2$.\n\nThe Kalman filter measurement update equations are used to obtain the posterior state estimate $\\hat{x}^{+}$ and its covariance $P^{+}$:\n1. Kalman Gain: $K = P^{-} H^T (H P^{-} H^T + R)^{-1}$\n2. Posterior Covariance: $P^{+} = (I - KH)P^{-}$\n\nThe measurement of $x_1$ provides information that not only reduces the uncertainty in our estimate of $x_1$, but also reduces the uncertainty in our estimate of the unmeasured state $x_2$, provided that $x_1$ and $x_2$ are correlated ($\\rho \\neq 0$).\n\nDefine a parameter $\\gamma = \\sigma_1^2 / \\sigma_v^2$, which represents the ratio of the prior variance of the measured state to the measurement noise variance.\n\nDerive an expression for the ratio of the posterior variance of the unmeasured state component, $x_2$, to its prior variance. That is, find an expression for $\\frac{P_{22}^{+}}{P_{22}^{-}}$ in terms of $\\rho$ and $\\gamma$.", "solution": "The goal is to compute the ratio of the posterior variance of the unmeasured state, $P_{22}^{+}$, to its prior variance, $P_{22}^{-}$. The prior variance is given by the $(2,2)$ element of the prior covariance matrix $P^{-}$, which is $P_{22}^{-} = \\sigma_2^2$.\n\nWe begin by computing the Kalman gain, $K$. The components of the Kalman gain calculation are:\nThe state-to-measurement matrix is $H = \\begin{pmatrix} 1  0 \\end{pmatrix}$, so its transpose is $H^T = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$.\nThe prior covariance matrix is $P^{-} = \\begin{pmatrix} \\sigma_1^2  \\rho \\sigma_1 \\sigma_2 \\\\ \\rho \\sigma_1 \\sigma_2  \\sigma_2^2 \\end{pmatrix}$.\nThe measurement noise covariance is $R = \\sigma_v^2$.\n\nFirst, we calculate the term $P^{-}H^T$:\n$$\nP^{-}H^T = \\begin{pmatrix} \\sigma_1^2  \\rho \\sigma_1 \\sigma_2 \\\\ \\rho \\sigma_1 \\sigma_2  \\sigma_2^2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\sigma_1^2 \\\\ \\rho \\sigma_1 \\sigma_2 \\end{pmatrix}\n$$\n\nNext, we calculate the innovation covariance, $S = H P^{-} H^T + R$:\n$$\nH P^{-} H^T = \\begin{pmatrix} 1  0 \\end{pmatrix} \\begin{pmatrix} \\sigma_1^2 \\\\ \\rho \\sigma_1 \\sigma_2 \\end{pmatrix} = \\sigma_1^2\n$$\nSo, the innovation covariance is $S = \\sigma_1^2 + \\sigma_v^2$. Since this is a scalar, its inverse is $S^{-1} = \\frac{1}{\\sigma_1^2 + \\sigma_v^2}$.\n\nNow we can compute the Kalman gain $K$:\n$$\nK = P^{-}H^T S^{-1} = \\begin{pmatrix} \\sigma_1^2 \\\\ \\rho \\sigma_1 \\sigma_2 \\end{pmatrix} \\frac{1}{\\sigma_1^2 + \\sigma_v^2} = \\frac{1}{\\sigma_1^2 + \\sigma_v^2} \\begin{pmatrix} \\sigma_1^2 \\\\ \\rho \\sigma_1 \\sigma_2 \\end{pmatrix}\n$$\n\nWith the Kalman gain, we can find the posterior covariance matrix $P^{+} = (I - KH)P^{-}$.\nFirst, we compute the term $(I - KH)$:\n$$\nKH = \\frac{1}{\\sigma_1^2 + \\sigma_v^2} \\begin{pmatrix} \\sigma_1^2 \\\\ \\rho \\sigma_1 \\sigma_2 \\end{pmatrix} \\begin{pmatrix} 1  0 \\end{pmatrix} = \\frac{1}{\\sigma_1^2 + \\sigma_v^2} \\begin{pmatrix} \\sigma_1^2  0 \\\\ \\rho \\sigma_1 \\sigma_2  0 \\end{pmatrix}\n$$\n$$\nI - KH = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} - \\frac{1}{\\sigma_1^2 + \\sigma_v^2} \\begin{pmatrix} \\sigma_1^2  0 \\\\ \\rho \\sigma_1 \\sigma_2  0 \\end{pmatrix} = \\begin{pmatrix} 1 - \\frac{\\sigma_1^2}{\\sigma_1^2 + \\sigma_v^2}  0 \\\\ -\\frac{\\rho \\sigma_1 \\sigma_2}{\\sigma_1^2 + \\sigma_v^2}  1 \\end{pmatrix}\n$$\nSimplifying the $(1,1)$ element:\n$$\n1 - \\frac{\\sigma_1^2}{\\sigma_1^2 + \\sigma_v^2} = \\frac{\\sigma_1^2 + \\sigma_v^2 - \\sigma_1^2}{\\sigma_1^2 + \\sigma_v^2} = \\frac{\\sigma_v^2}{\\sigma_1^2 + \\sigma_v^2}\n$$\nSo,\n$$\nI - KH = \\begin{pmatrix} \\frac{\\sigma_v^2}{\\sigma_1^2 + \\sigma_v^2}  0 \\\\ -\\frac{\\rho \\sigma_1 \\sigma_2}{\\sigma_1^2 + \\sigma_v^2}  1 \\end{pmatrix}\n$$\n\nNow we compute the posterior covariance matrix $P^{+}$:\n$$\nP^{+} = (I - KH)P^{-} = \\begin{pmatrix} \\frac{\\sigma_v^2}{\\sigma_1^2 + \\sigma_v^2}  0 \\\\ -\\frac{\\rho \\sigma_1 \\sigma_2}{\\sigma_1^2 + \\sigma_v^2}  1 \\end{pmatrix} \\begin{pmatrix} \\sigma_1^2  \\rho \\sigma_1 \\sigma_2 \\\\ \\rho \\sigma_1 \\sigma_2  \\sigma_2^2 \\end{pmatrix}\n$$\nWe are interested in the $(2,2)$ element of $P^{+}$, which is $P_{22}^{+}$:\n$$\nP_{22}^{+} = \\left(-\\frac{\\rho \\sigma_1 \\sigma_2}{\\sigma_1^2 + \\sigma_v^2}\\right) (\\rho \\sigma_1 \\sigma_2) + (1)(\\sigma_2^2)\n$$\n$$\nP_{22}^{+} = \\sigma_2^2 - \\frac{\\rho^2 \\sigma_1^2 \\sigma_2^2}{\\sigma_1^2 + \\sigma_v^2}\n$$\n\nThe problem asks for the ratio $\\frac{P_{22}^{+}}{P_{22}^{-}}$. We know that $P_{22}^{-} = \\sigma_2^2$.\n$$\n\\frac{P_{22}^{+}}{P_{22}^{-}} = \\frac{\\sigma_2^2 - \\frac{\\rho^2 \\sigma_1^2 \\sigma_2^2}{\\sigma_1^2 + \\sigma_v^2}}{\\sigma_2^2} = 1 - \\frac{\\rho^2 \\sigma_1^2}{\\sigma_1^2 + \\sigma_v^2}\n$$\nFinally, we substitute the parameter $\\gamma = \\sigma_1^2 / \\sigma_v^2$. To do this, we can divide the numerator and denominator of the fraction by $\\sigma_v^2$:\n$$\n\\frac{P_{22}^{+}}{P_{22}^{-}} = 1 - \\frac{\\rho^2 (\\sigma_1^2 / \\sigma_v^2)}{(\\sigma_1^2 / \\sigma_v^2) + (\\sigma_v^2 / \\sigma_v^2)} = 1 - \\frac{\\rho^2 \\gamma}{\\gamma + 1}\n$$", "answer": "$$\n\\boxed{1 - \\frac{\\rho^2 \\gamma}{1 + \\gamma}}\n$$", "id": "779302"}, {"introduction": "Building upon intuition, this exercise guides you through the formal derivation of the Kalman gain matrix from first principles. Starting with the core concept of Linear Minimum Mean-Squared Error (LMMSE) estimation and the orthogonality principle, you will derive the general form of the gain for a system with multiple simultaneous measurements [@problem_id:2753313]. This practice solidifies the theoretical underpinning of the Kalman gain as the optimal linear operator that minimizes posterior estimation error, and a concrete numerical example will help you translate the theory into practice.", "problem": "Consider a single measurement-update step of the Kalman filter for a linear Gaussian state estimation problem. Let the prior state be modeled as a random vector $x \\in \\mathbb{R}^{2}$ with prior mean $\\hat{x}^{-} \\in \\mathbb{R}^{2}$ and prior covariance $P \\in \\mathbb{R}^{2 \\times 2}$. Two independent measurements are obtained simultaneously according to the linear model\n$$\ny_{1} = H_{1} x + v_{1}, \\quad y_{2} = H_{2} x + v_{2},\n$$\nwhere $H_{1} \\in \\mathbb{R}^{1 \\times 2}$, $H_{2} \\in \\mathbb{R}^{1 \\times 2}$ are measurement matrices, and $v_{1} \\sim \\mathcal{N}(0, R_{1})$, $v_{2} \\sim \\mathcal{N}(0, R_{2})$ are mutually independent, zero-mean, Gaussian noises, independent of $x$, with covariances $R_{1} \\in \\mathbb{R}$ and $R_{2} \\in \\mathbb{R}$. Stack the measurements as $y = \\begin{bmatrix} y_{1} \\\\ y_{2} \\end{bmatrix}$ with a corresponding stacked measurement matrix $H \\in \\mathbb{R}^{2 \\times 2}$ and block-diagonal noise covariance $R \\in \\mathbb{R}^{2 \\times 2}$. Using only foundational principles of linear minimum mean-squared error (LMMSE) estimation for jointly Gaussian variables—namely, the orthogonality principle and properties of conditional Gaussians—derive the expression for the joint optimal linear update $ \\hat{x}^{+} = \\hat{x}^{-} + K (y - H \\hat{x}^{-})$, identify the optimal joint Kalman gain $K \\in \\mathbb{R}^{2 \\times 2}$, and derive the corresponding posterior covariance $P^{+} \\in \\mathbb{R}^{2 \\times 2}$ in terms of $P$, $H$, and $R$.\n\nThen, evaluate these expressions for the specific numerical case\n$$\nP = \\begin{bmatrix} 3  0 \\\\ 0  2 \\end{bmatrix}, \\quad H_{1} = \\begin{bmatrix} 1  0 \\end{bmatrix}, \\quad R_{1} = 1, \\quad H_{2} = \\begin{bmatrix} 0  1 \\end{bmatrix}, \\quad R_{2} = 4,\n$$\nwith the understanding that $v_{1}$ and $v_{2}$ are independent and independent of $x$. Compute the joint Kalman gain $K$ and the posterior covariance $P^{+}$ for this numerical instance.\n\nYour final reported answer must be a single scalar: the determinant of the posterior covariance $P^{+}$. No rounding is required.", "solution": "**Derivation of the Kalman Filter Update Equations**\n\nWe are given a linear system with jointly Gaussian variables. The state $x$ has a prior distribution with mean $\\mathbb{E}[x] = \\hat{x}^{-}$ and covariance $\\text{cov}(x) = P$. The measurement model is stacked into a single equation:\n$$y = Hx + v$$\nwhere $y = \\begin{bmatrix} y_1 \\\\ y_2 \\end{bmatrix}$, $H = \\begin{bmatrix} H_1 \\\\ H_2 \\end{bmatrix}$, and $v = \\begin{bmatrix} v_1 \\\\ v_2 \\end{bmatrix}$. The noise $v$ is zero-mean, $\\mathbb{E}[v] = 0$, and has covariance $R = \\mathbb{E}[vv^T]$. Since $v_1$ and $v_2$ are independent, $R$ is block-diagonal:\n$$R = \\begin{bmatrix} R_1  0 \\\\ 0  R_2 \\end{bmatrix}$$\nThe noise $v$ is independent of the state $x$. The optimal LMMSE estimate of $x$ given the measurement $y$, denoted $\\hat{x}^{+}$, is the conditional mean $\\mathbb{E}[x|y]$. The problem specifies the form of the linear estimator:\n$$\\hat{x}^{+} = \\hat{x}^{-} + K(y - H\\hat{x}^{-})$$\nThe vector $\\tilde{y} = y - H\\hat{x}^{-}$ is the measurement innovation. The optimal gain matrix $K$ is found by applying the orthogonality principle, which states that the posterior estimation error, $e^{+} = x - \\hat{x}^{+}$, must be orthogonal to the innovation $\\tilde{y}$.\n$$\\mathbb{E}[e^{+} \\tilde{y}^T] = 0$$\nFirst, let's express the error $e^{+}$ and innovation $\\tilde{y}$ in terms of the prior error $e^{-} = x - \\hat{x}^{-}$ and the noise $v$.\nThe innovation is:\n$$\\tilde{y} = y - H\\hat{x}^{-} = (Hx + v) - H\\hat{x}^{-} = H(x - \\hat{x}^{-}) + v = He^{-} + v$$\nThe posterior error is:\n$$e^{+} = x - \\hat{x}^{+} = x - (\\hat{x}^{-} + K\\tilde{y}) = (x - \\hat{x}^{-}) - K\\tilde{y} = e^{-} - K(He^{-} + v) = (I - KH)e^{-} - Kv$$\nNow, apply the orthogonality condition:\n$$\\mathbb{E}[((I - KH)e^{-} - Kv)(He^{-} + v)^T] = 0$$\nExpanding the expression:\n$$\\mathbb{E}[(I - KH)e^{-}(e^{-})^T H^T + (I - KH)e^{-}v^T - Kve^{-T}H^T - Kvv^T] = 0$$\nThe prior error $e^{-}$ has zero mean and is uncorrelated with the measurement noise $v$. Therefore, $\\mathbb{E}[e^{-}v^T] = 0$ and $\\mathbb{E}[ve^{-T}] = 0$. The covariances are $\\mathbb{E}[e^{-}(e^{-})^T] = P$ and $\\mathbb{E}[vv^T] = R$. The equation simplifies to:\n$$(I - KH)\\mathbb{E}[e^{-}(e^{-})^T]H^T - K\\mathbb{E}[vv^T] = 0$$\n$$(I - KH)PH^T - KR = 0$$\n$$PH^T - K H P H^T - KR = 0$$\n$$PH^T = K(H P H^T + R)$$\nSolving for the gain $K$, assuming the innovation covariance matrix $S = H P H^T + R$ is invertible:\n$$K = P H^T (H P H^T + R)^{-1}$$\nThis is the expression for the optimal Kalman gain.\n\nNext, we derive the posterior covariance $P^{+} = \\mathbb{E}[e^{+}(e^{+})^T]$.\n$$P^{+} = \\mathbb{E}[((I - KH)e^{-} - Kv)((I - KH)e^{-} - Kv)^T]$$\nExpanding and using the uncorrelation of $e^{-}$ and $v$:\n$$P^{+} = \\mathbb{E}[(I - KH)e^{-}(e^{-})^T(I - KH)^T] + \\mathbb{E}[Kvv^T K^T]$$\n$$P^{+} = (I - KH) \\mathbb{E}[e^{-}(e^{-})^T] (I - KH)^T + K \\mathbb{E}[vv^T] K^T$$\n$$P^{+} = (I - KH) P (I - KH)^T + K R K^T$$\nThis is the Joseph form of the covariance update. A simpler form is $P^{+} = (I - KH)P$. To show this, substitute $K = P H^T S^{-1}$ into the Joseph form. A more direct derivation:\n$$e^{+} = (I - KH)e^{-} - Kv$$\n$$P^{+} = \\mathbb{E}[e^{+}e^{+T}] = (I - KH)P(I - KH)^T + KRK^T$$\n$$P^{+} = P - KHP - PH^T(I-KH)^T + KHP H^T (I-KH)^T + KRK^T$$\n$$P^{+} = P - KHP - PH^T + PH^T H^T K^T + K(HPH^T + R)K^T$$\nUsing $PH^T = K(HPH^T + R) = KS$:\n$$P^{+} = P - KS H^T - PH^T + (KS)H^T K^T + K S K^T = P - PH^T - PH^T + PH^T(H^T K^T) + PH^T K^T$$\nThis is getting complicated. Let's use the simpler derivation:\n$P^{+} = \\mathbb{E}[e^{+}e^{+T}] = \\mathbb{E}[(e^{-} - K\\tilde{y})(e^{-} - K\\tilde{y})^T] = P - \\mathbb{E}[e^{-} \\tilde{y}^T] K^T - K\\mathbb{E}[\\tilde{y} e^{-T}] + K\\mathbb{E}[\\tilde{y}\\tilde{y}^T]K^T$\n$\\mathbb{E}[\\tilde{y}\\tilde{y}^T] = S = HPH^T+R$.\n$\\mathbb{E}[\\tilde{y}e^{-T}] = \\mathbb{E}[(He^{-}+v)e^{-T}] = H P$.\n$P^{+} = P - (PH^T)K^T - K(HP) + KSK^T = P - PH^TK^T - KHP + K(HPH^T+R)K^T$.\n$P^{+} = P - PH^TK^T - KHP + K H P H^T K^T + K R K^T$.\nSince $K = P H^T S^{-1}$, $KS = PH^T$, and $K(HP) = P H^T S^{-1} HP$. This is not simple.\nLet's stick with $P^{+} = (I-KH)P$.\n$e^{+} = e^{-} - K(He^{-} + v) = (I-KH)e^{-} - Kv$.\n$e^{+} = x - \\hat{x}^{+}$. We want $\\mathbb{E}[e^{+}e^{+T}]$.\n$P^{+} = \\text{cov}(x | y) = \\text{cov}(x) - \\text{cov}(x,y)\\text{cov}(y)^{-1}\\text{cov}(y,x)$.\n$\\text{cov}(x,y) = \\text{cov}(x, Hx+v) = \\text{cov}(x,Hx) = PH^T$.\n$\\text{cov}(y) = \\text{cov}(Hx+v) = HPH^T + R = S$.\n$P^{+} = P - PH^T S^{-1} (PH^T)^T = P - PH^T S^{-1} H P = P - K H P$.\nThis confirms $P^{+} = (I-KH)P$ is correct.\n\n**Numerical Calculation**\n\nWe are given the specific values:\n$$P = \\begin{bmatrix} 3  0 \\\\ 0  2 \\end{bmatrix}, \\quad H_{1} = \\begin{bmatrix} 1  0 \\end{bmatrix}, \\quad R_{1} = 1, \\quad H_{2} = \\begin{bmatrix} 0  1 \\end{bmatrix}, \\quad R_{2} = 4$$\nFirst, we construct the stacked measurement matrix $H$ and stacked noise covariance matrix $R$:\n$$H = \\begin{bmatrix} H_1 \\\\ H_2 \\end{bmatrix} = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix} = I_2$$\n$$R = \\begin{bmatrix} R_1  0 \\\\ 0  R_2 \\end{bmatrix} = \\begin{bmatrix} 1  0 \\\\ 0  4 \\end{bmatrix}$$\nNow, we compute the Kalman gain $K = P H^T (H P H^T + R)^{-1}$.\nWith $H = I_2$, this simplifies to $K = P (P + R)^{-1}$.\nThe innovation covariance $S = P+R$ is:\n$$S = \\begin{bmatrix} 3  0 \\\\ 0  2 \\end{bmatrix} + \\begin{bmatrix} 1  0 \\\\ 0  4 \\end{bmatrix} = \\begin{bmatrix} 4  0 \\\\ 0  6 \\end{bmatrix}$$\nThe inverse of $S$ is:\n$$S^{-1} = \\begin{bmatrix} 4  0 \\\\ 0  6 \\end{bmatrix}^{-1} = \\begin{bmatrix} \\frac{1}{4}  0 \\\\ 0  \\frac{1}{6} \\end{bmatrix}$$\nNow we compute the Kalman gain $K$:\n$$K = P S^{-1} = \\begin{bmatrix} 3  0 \\\\ 0  2 \\end{bmatrix} \\begin{bmatrix} \\frac{1}{4}  0 \\\\ 0  \\frac{1}{6} \\end{bmatrix} = \\begin{bmatrix} \\frac{3}{4}  0 \\\\ 0  \\frac{2}{6} \\end{bmatrix} = \\begin{bmatrix} \\frac{3}{4}  0 \\\\ 0  \\frac{1}{3} \\end{bmatrix}$$\nNext, we compute the posterior covariance $P^{+} = (I - KH)P$.\nWith $H = I_2$, this is $P^{+} = (I - K)P$.\n$$I - K = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix} - \\begin{bmatrix} \\frac{3}{4}  0 \\\\ 0  \\frac{1}{3} \\end{bmatrix} = \\begin{bmatrix} 1 - \\frac{3}{4}  0 \\\\ 0  1 - \\frac{1}{3} \\end{bmatrix} = \\begin{bmatrix} \\frac{1}{4}  0 \\\\ 0  \\frac{2}{3} \\end{bmatrix}$$\n$$P^{+} = (I - K)P = \\begin{bmatrix} \\frac{1}{4}  0 \\\\ 0  \\frac{2}{3} \\end{bmatrix} \\begin{bmatrix} 3  0 \\\\ 0  2 \\end{bmatrix} = \\begin{bmatrix} \\frac{3}{4}  0 \\\\ 0  \\frac{4}{3} \\end{bmatrix}$$\nThe posterior covariance matrix is $P^{+} = \\begin{bmatrix} \\frac{3}{4}  0 \\\\ 0  \\frac{4}{3} \\end{bmatrix}$.\n\nThe problem requires the determinant of the posterior covariance, $\\det(P^{+})$.\nSince $P^{+}$ is a diagonal matrix, its determinant is the product of its diagonal elements.\n$$\\det(P^{+}) = \\det\\left(\\begin{bmatrix} \\frac{3}{4}  0 \\\\ 0  \\frac{4}{3} \\end{bmatrix}\\right) = \\left(\\frac{3}{4}\\right) \\times \\left(\\frac{4}{3}\\right) = 1$$\nThe determinant is exactly $1$.", "answer": "$$\n\\boxed{1}\n$$", "id": "2753313"}, {"introduction": "To truly master the Kalman gain, it is crucial to understand its behavior under different conditions. This final practice presents a thought experiment [@problem_id:2912316] exploring the limiting case of a \"perfect\" measurement, where the measurement noise covariance approaches zero ($R_k \\to 0$). By analyzing how the Kalman gain and the state update behave in this limit, you will gain deeper insight into the gain's role as a dynamic weighting factor that intelligently balances the confidence in the prior prediction against the trustworthiness of the new data.", "problem": "Consider a discrete-time linear Gaussian state-space model at time $k$ given by\n$$\nx_k = F_k x_{k-1} + w_{k-1}, \\quad y_k = H_k x_k + v_k,\n$$\nwhere $x_k \\in \\mathbb{R}^n$, $y_k \\in \\mathbb{R}^m$, $w_{k-1} \\sim \\mathcal{N}(0,Q_k)$ with $Q_k \\succeq 0$, and $v_k \\sim \\mathcal{N}(0,R_k)$ with $R_k \\succeq 0$. Assume $w_{k-1}$ and $v_k$ are mutually independent and independent of $x_{k-1}$. Suppose a prior Gaussian belief at time $k$ is available with mean $\\hat x_{k|k-1}$ and covariance $P_{k|k-1} \\succ 0$. Let $H_k \\in \\mathbb{R}^{m \\times n}$ have full row rank $m \\le n$, and assume that $H_k P_{k|k-1} H_k^\\top$ is positive definite.\n\nYou are interested in the behavior of the discrete-time Kalman filter when the measurement noise covariance is driven to zero, in the sense $R_k = \\epsilon R_{0,k}$ with $\\epsilon \\to 0^+$ and $R_{0,k} \\succ 0$ fixed. Consider the limiting effect on the Kalman gain $K_k$, the updated state estimate $\\hat x_{k|k}$, and the updated covariance $P_{k|k}$, all at the same time $k$. Choose the single option that correctly characterizes this limit under the stated assumptions.\n\nA. As $R_k \\to 0$, one has $K_k \\to P_{k|k-1} H_k^\\top (H_k P_{k|k-1} H_k^\\top)^{-1}$, the updated estimate satisfies $H_k \\hat x_{k|k} = y_k$, and $P_{k|k} \\to P_{k|k-1} - P_{k|k-1} H_k^\\top (H_k P_{k|k-1} H_k^\\top)^{-1} H_k P_{k|k-1}$.\n\nB. As $R_k \\to 0$, one has $K_k \\to 0$ because the innovation covariance shrinks, so the filter ignores the measurement and trusts the prediction.\n\nC. As $R_k \\to 0$, one has $\\hat x_{k|k} \\to y_k$ (independently of $H_k$) and $P_{k|k} \\to 0$.\n\nD. As $R_k \\to 0$, one has $\\|K_k\\| \\to \\infty$ so the update becomes undefined due to an ill-conditioned inversion.\n\nE. As $R_k \\to 0$, one has $P_{k|k} = P_{k|k-1}$ because the term proportional to $R_k$ vanishes in the covariance update.", "solution": "The problem asks for the limiting behavior of the Kalman filter update step as the measurement noise covariance $R_k$ approaches the zero matrix. The standard Kalman filter update equations at time $k$ are:\nThe innovation covariance:\n$$ S_k = H_k P_{k|k-1} H_k^\\top + R_k $$\nThe Kalman gain:\n$$ K_k = P_{k|k-1} H_k^\\top S_k^{-1} = P_{k|k-1} H_k^\\top (H_k P_{k|k-1} H_k^\\top + R_k)^{-1} $$\nThe updated state estimate:\n$$ \\hat x_{k|k} = \\hat x_{k|k-1} + K_k (y_k - H_k \\hat x_{k|k-1}) $$\nThe updated error covariance:\n$$ P_{k|k} = (I - K_k H_k) P_{k|k-1} $$\n\nWe are given that $R_k = \\epsilon R_{0,k}$ where $R_{0,k} \\succ 0$ is a fixed positive definite matrix and $\\epsilon \\to 0^+$. The problem also states that $P_{k|k-1} \\succ 0$ and $H_k$ has full row rank $m \\le n$. This implies that the matrix $H_k P_{k|k-1} H_k^\\top$ is positive definite, and thus invertible. Let us denote this matrix by $\\Sigma_k \\equiv H_k P_{k|k-1} H_k^\\top$.\n\nFirst, we analyze the limit of the Kalman gain $K_k$.\n$$ \\lim_{\\epsilon \\to 0^+} K_k = \\lim_{\\epsilon \\to 0^+} P_{k|k-1} H_k^\\top (H_k P_{k|k-1} H_k^\\top + \\epsilon R_{0,k})^{-1} $$\nSince matrix inversion is a continuous operation on the space of invertible matrices and $\\Sigma_k = H_k P_{k|k-1} H_k^\\top$ is invertible, we can take the limit inside the inverse:\n$$ \\lim_{\\epsilon \\to 0^+} K_k = P_{k|k-1} H_k^\\top \\left(\\lim_{\\epsilon \\to 0^+} (H_k P_{k|k-1} H_k^\\top + \\epsilon R_{0,k})\\right)^{-1} $$\n$$ \\lim_{\\epsilon \\to 0^+} K_k = P_{k|k-1} H_k^\\top (H_k P_{k|k-1} H_k^\\top)^{-1} $$\nLet us denote this limiting gain as $K_k^*$.\n\nNext, we analyze the property of the limiting updated state estimate, $\\hat x_{k|k}^* = \\lim_{\\epsilon \\to 0^+} \\hat x_{k|k}$.\n$$ \\hat x_{k|k}^* = \\hat x_{k|k-1} + K_k^* (y_k - H_k \\hat x_{k|k-1}) $$\nWe examine the quantity $H_k \\hat x_{k|k}^*$ by premultiplying the equation by $H_k$:\n$$ H_k \\hat x_{k|k}^* = H_k \\hat x_{k|k-1} + H_k K_k^* (y_k - H_k \\hat x_{k|k-1}) $$\nLet us evaluate the term $H_k K_k^*$:\n$$ H_k K_k^* = H_k \\left( P_{k|k-1} H_k^\\top (H_k P_{k|k-1} H_k^\\top)^{-1} \\right) = (H_k P_{k|k-1} H_k^\\top) (H_k P_{k|k-1} H_k^\\top)^{-1} = I_m $$\nwhere $I_m$ is the $m \\times m$ identity matrix. Substituting this result back:\n$$ H_k \\hat x_{k|k}^* = H_k \\hat x_{k|k-1} + I_m (y_k - H_k \\hat x_{k|k-1}) = H_k \\hat x_{k|k-1} + y_k - H_k \\hat x_{k|k-1} = y_k $$\nThis demonstrates that in the limit of a perfect measurement, the updated state estimate perfectly honors the measurement equation.\n\nFinally, we analyze the limit of the updated error covariance, $P_{k|k}^* = \\lim_{\\epsilon \\to 0^+} P_{k|k}$.\nUsing the formula $P_{k|k} = (I - K_k H_k) P_{k|k-1}$:\n$$ P_{k|k}^* = \\lim_{\\epsilon \\to 0^+} (I - K_k H_k) P_{k|k-1} = (I - K_k^* H_k) P_{k|k-1} $$\nDistributing $P_{k|k-1}$, we get:\n$$ P_{k|k}^* = P_{k|k-1} - K_k^* H_k P_{k|k-1} $$\nSubstituting the expression for $K_k^*$:\n$$ P_{k|k}^* = P_{k|k-1} - \\left( P_{k|k-1} H_k^\\top (H_k P_{k|k-1} H_k^\\top)^{-1} \\right) H_k P_{k|k-1} $$\nThis expression represents the residual covariance after perfectly incorporating the information from the measurement. The uncertainty is eliminated in the directions observable by $H_k$.\n\nNow let us evaluate each option based on our derivation.\n\nA. As $R_k \\to 0$, one has $K_k \\to P_{k|k-1} H_k^\\top (H_k P_{k|k-1} H_k^\\top)^{-1}$, the updated estimate satisfies $H_k \\hat x_{k|k} = y_k$, and $P_{k|k} \\to P_{k|k-1} - P_{k|k-1} H_k^\\top (H_k P_{k|k-1} H_k^\\top)^{-1} H_k P_{k|k-1}$.\nOur derivation confirms all three parts of this statement.\n- The limit of $K_k$ is correct.\n- The property of the limiting estimate $\\hat x_{k|k}^*$ is correct.\n- The limit of $P_{k|k}$ is correct.\nTherefore, this option is **Correct**.\n\nB. As $R_k \\to 0$, one has $K_k \\to 0$ because the innovation covariance shrinks, so the filter ignores the measurement and trusts the prediction.\nThis is incorrect. The innovation covariance $S_k = H_k P_{k|k-1} H_k^\\top + R_k$ converges to $H_k P_{k|k-1} H_k^\\top$, which is a non-zero positive definite matrix. The Kalman gain $K_k$ converges to a non-zero matrix $K_k^*$, not the zero matrix. The physical interpretation is also reversed: as $R_k \\to 0$, the measurement becomes infinitely trustworthy, so the filter relies on it *more*, not less. The filter would ignore the measurement ($K_k \\to 0$) in the opposite case, where $R_k \\to \\infty$. Therefore, this option is **Incorrect**.\n\nC. As $R_k \\to 0$, one has $\\hat x_{k|k} \\to y_k$ (independently of $H_k$) and $P_{k|k} \\to 0$.\nThe first claim, $\\hat x_{k|k} \\to y_k$, is dimensionally inconsistent unless $n=m$. The state estimate $\\hat x_{k|k}$ is in $\\mathbb{R}^n$, while the measurement $y_k$ is in $\\mathbb{R}^m$. The correct relationship is $H_k \\hat x_{k|k}^* = y_k$. The second claim, $P_{k|k} \\to 0$, is true only if $m=n$ (and $H_k$ is invertible). If $m  n$, the measurement only provides information about an $m$-dimensional subspace of the state. The uncertainty in the remaining $n-m$ dimensions is not eliminated, so $P_{k|k}^*_k$ is a non-zero, singular matrix of rank $n-m$. Therefore, this option is **Incorrect**.\n\nD. As $R_k \\to 0$, one has $\\|K_k\\| \\to \\infty$ so the update becomes undefined due to an ill-conditioned inversion.\nThis is false. The inversion in the Kalman gain formula is of the matrix $S_k = H_k P_{k|k-1} H_k^\\top + R_k$. As $R_k \\to 0$, this matrix converges to $H_k P_{k|k-1} H_k^\\top$, which is positive definite and thus invertible under the problem's assumptions. The inversion is well-defined in the limit, and the norm of the limiting gain $\\|K_k^*\\|$ is finite. The matrix $R_k$ itself is not inverted in this formulation. Therefore, this option is **Incorrect**.\n\nE. As $R_k \\to 0$, one has $P_{k|k} = P_{k|k-1}$ because the term proportional to $R_k$ vanishes in the covariance update.\nThis reflects a severe misunderstanding of the covariance update. This would imply that no information is gained from the measurement, which is the case when $K_k = 0$. Here, $K_k$ is non-zero in the limit. The state $P_{k|k} = P_{k|k-1}$ holds when the measurement noise is infinite ($R_k \\to \\infty$), not zero. While it is true that in the Joseph form of the covariance update, $P_{k|k} = (I - K_k H_k) P_{k|k-1} (I - K_k H_k)^\\top + K_k R_k K_k^\\top$, the term $K_k R_k K_k^\\top$ vanishes, the main reduction in covariance comes from the first term, as $(I - K_k H_k)$ is not the identity matrix. Therefore, this option is **Incorrect**.\n\nBased on this comprehensive analysis, only option A is correct.", "answer": "$$\\boxed{A}$$", "id": "2912316"}]}