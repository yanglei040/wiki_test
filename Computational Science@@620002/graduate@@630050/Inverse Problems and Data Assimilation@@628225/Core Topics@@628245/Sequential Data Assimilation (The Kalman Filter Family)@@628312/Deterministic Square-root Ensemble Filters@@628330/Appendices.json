{"hands_on_practices": [{"introduction": "Before applying any new method, it's crucial to verify that its machinery works as intended. This first practice is a fundamental verification exercise designed to build confidence in the deterministic square-root filter's formulation. You will implement the analysis covariance update through two different pathways—one using the anomaly transform matrix $T$ and the other using the classic Kalman update formula—and confirm they produce numerically identical results [@problem_id:3376014].", "problem": "Consider a linear-Gaussian data assimilation setting with a state vector in $\\mathbb{R}^n$ and an observation vector in $\\mathbb{R}^p$. The prior (forecast) state ensemble has $m$ members and is represented by a matrix of ensemble anomalies $X^f \\in \\mathbb{R}^{n \\times m}$, whose columns are the deviations of ensemble members from the ensemble mean. The sample covariance of the prior is $P^f = \\frac{1}{m-1} X^f (X^f)^\\top$. The observation operator is a known matrix $H \\in \\mathbb{R}^{p \\times n}$, and the observation error covariance $R \\in \\mathbb{R}^{p \\times p}$ is symmetric positive definite. The Deterministic Square-Root Ensemble Kalman Filter (commonly referred to as Ensemble Transform Kalman Filter (ETKF)) updates anomalies via a transform matrix $T \\in \\mathbb{R}^{m \\times m}$, producing analysis anomalies $X^a = X^f T$ and an analysis covariance $P^a_{\\text{anom}} = \\frac{1}{m-1} X^a (X^a)^\\top$. The transform $T$ is chosen to be the unique symmetric matrix that makes $P^a_{\\text{anom}}$ consistent with the posterior covariance implied by the linear-Gaussian Bayesian update, when computed in ensemble space using only $X^f$, $H$, and $R$.\n\nTasks:\n- Using only $X^f$, $H$, and $R$, construct the whitened ensemble observation anomalies $S = R^{-1/2} H X^f / \\sqrt{m-1}$ and then construct the symmetric transform $T$ as the inverse square root of the matrix $I_m + S^\\top S$, where $I_m$ is the $m \\times m$ identity. Use $X^a = X^f T$ and compute $P^a_{\\text{anom}} = \\frac{1}{m-1} X^a (X^a)^\\top$.\n- Independently compute the Bayesian posterior covariance $P^a_{\\text{post}}$ from $P^f$, $H$, and $R$ without forming any Kalman gain explicitly; use only the prior covariance, the observation operator, and the observation error covariance through the linear-Gaussian posterior law.\n- Define the discrepancy metric $e = \\| P^a_{\\text{anom}} - P^a_{\\text{post}} \\|_F / \\| P^a_{\\text{post}} \\|_F$, where $\\| \\cdot \\|_F$ denotes the Frobenius norm.\n\nYour program must compute $e$ for each of the following test cases. All random quantities must be generated using the specified seed for reproducibility. Any symmetric positive definite matrix square roots and inverses must be computed via an eigen-decomposition to ensure numerical stability. Angles do not appear; no physical units are involved.\n\nTest suite:\n1. Happy path with moderate dimensions and a reasonably large ensemble:\n   - $n = 50$, $p = 20$, $m = 100$.\n   - $H$ is drawn with independent standard normal entries and scaled by $1/\\sqrt{n}$.\n   - $R$ is diagonal with entries $0.5 + u_i$, where $u_i$ are independent draws from $\\mathrm{Uniform}(0, 1)$.\n   - Prior ensemble anomalies $X^f$ are constructed by drawing $n \\times m$ independent standard normal entries and subtracting the column-wise ensemble mean from each column to produce anomalies.\n   - Seed: $0$.\n\n2. Small ensemble to induce strong sampling noise:\n   - $n = 50$, $p = 20$, $m = 15$.\n   - $H$ as in Case 1 with scaling $1/\\sqrt{n}$.\n   - $R$ as in Case 1.\n   - $X^f$ constructed as in Case 1.\n   - Seed: $1$.\n\n3. Identity observation operator with low observation noise and a large ensemble:\n   - $n = 30$, $p = 30$, $m = 300$.\n   - $H = I_n$ (the $n \\times n$ identity).\n   - $R$ is diagonal with all entries equal to $0.1$.\n   - $X^f$ constructed as in Case 1.\n   - Seed: $2$.\n\n4. Correlated observation errors:\n   - $n = 40$, $p = 25$, $m = 80$.\n   - $H$ as in Case 1 with scaling $1/\\sqrt{n}$.\n   - $R$ is constructed as $R = B B^\\top + 0.5 I_p$, where $B$ has independent standard normal entries.\n   - $X^f$ constructed as in Case 1.\n   - Seed: $3$.\n\n5. Nearly rank-deficient prior anomalies:\n   - $n = 60$, $p = 15$, $m = 10$.\n   - $H$ as in Case 1 with scaling $1/\\sqrt{n}$.\n   - $R$ as in Case 1.\n   - $X^f$ is constructed by first drawing $U \\in \\mathbb{R}^{n \\times r}$ and $V \\in \\mathbb{R}^{r \\times m}$ with independent standard normal entries for $r = 5$, forming $X = U V$, and then subtracting the column-wise ensemble mean to produce anomalies $X^f$.\n   - Seed: $4$.\n\nOutput specification:\n- Your program should produce a single line of output containing the five discrepancy values, one per test case, as a comma-separated list enclosed in square brackets (e.g., \"[e1,e2,e3,e4,e5]\").\n- Each $e_i$ must be output as a floating-point number.", "solution": "The user has provided a problem statement that requires validation and, if valid, a solution.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\n- **State dimension**: $n$\n- **Observation dimension**: $p$\n- **Ensemble size**: $m$\n- **Prior ensemble anomalies**: $X^f \\in \\mathbb{R}^{n \\times m}$\n- **Prior sample covariance**: $P^f = \\frac{1}{m-1} X^f (X^f)^\\top$\n- **Observation operator**: $H \\in \\mathbb{R}^{p \\times n}$\n- **Observation error covariance**: $R \\in \\mathbb{R}^{p \\times p}$, symmetric positive definite (SPD).\n- **Analysis ensemble anomalies**: $X^a = X^f T$\n- **Transform matrix**: $T = (I_m + S^\\top S)^{-1/2}$, symmetric.\n- **Whitened ensemble observation anomalies**: $S = R^{-1/2} H X^f / \\sqrt{m-1}$\n- **Analysis covariance (from anomalies)**: $P^a_{\\text{anom}} = \\frac{1}{m-1} X^a (X^a)^\\top$\n- **Analysis covariance (from posterior law)**: $P^a_{\\text{post}}$ computed from the linear-Gaussian Bayesian update using $P^f$, $H$, and $R$.\n- **Discrepancy metric**: $e = \\| P^a_{\\text{anom}} - P^a_{\\text{post}} \\|_F / \\| P^a_{\\text{post}} \\|_F$\n- **Numerical constraint**: SPD matrix square roots and inverses must be computed via eigen-decomposition.\n- **Test Suite**: Five completely specified test cases are provided, including dimensions, matrix generation procedures, and random seeds for reproducibility.\n\n**Step 2: Validate Using Extracted Givens**\n\n1.  **Scientifically Grounded**: The problem is rooted in the field of data assimilation, specifically in the theory of the Ensemble Kalman Filter (EnKF) and its deterministic square-root variants, such as the Ensemble Transform Kalman Filter (ETKF). The equations for the ETKF transform matrix and the Bayesian posterior covariance are standard and correct. **(Valid)**\n2.  **Well-Posed**: The problem is well-posed. Each test case is fully specified with all necessary parameters, initial conditions (via random seeds), and matrix definitions. The task is to compute a uniquely defined numerical quantity. The conditions on the matrices (e.g., $R$ is SPD) ensure that the required matrix operations (inversion, square root) are well-defined. **(Valid)**\n3.  **Objective**: The problem is stated using precise, objective mathematical language, free from ambiguity or subjective claims. **(Valid)**\n4.  **Scientific or Factual Unsoundness**: The problem contains no scientific or factual errors. The presented framework is a standard representation of the ETKF. **(Valid)**\n5.  **Incomplete or Contradictory Setup**: The problem is self-contained and provides all necessary information. There are no contradictions in the definitions or constraints. **(Valid)**\n\n**Step 3: Verdict and Action**\n\nThe problem is valid. A complete, reasoned solution will be provided.\n\n### Solution\n\nThe problem requires us to compute and compare two formulations of the analysis covariance matrix in a linear-Gaussian data assimilation setting. The first, $P^a_{\\text{anom}}$, is derived from the updated ensemble anomalies of a Deterministic Square-Root Filter (specifically, the ETKF). The second, $P^a_{\\text{post}}$, is the theoretical posterior covariance from the Bayesian update law.\n\nFirst, we will establish the theoretical equivalence of these two quantities. The analysis covariance from the ensemble anomalies is given by:\n$$ P^a_{\\text{anom}} = \\frac{1}{m-1} X^a (X^a)^\\top $$\nSubstituting $X^a = X^f T$ and noting that $T$ is symmetric ($T=T^\\top$), we get:\n$$ P^a_{\\text{anom}} = \\frac{1}{m-1} (X^f T) (X^f T)^\\top = \\frac{1}{m-1} X^f T T^\\top (X^f)^\\top = \\frac{1}{m-1} X^f T^2 (X^f)^\\top $$\nThe transform matrix squared, $T^2$, is defined by the problem statement as:\n$$ T^2 = \\left( (I_m + S^\\top S)^{-1/2} \\right)^2 = (I_m + S^\\top S)^{-1} $$\nSubstituting the definition of $S = R^{-1/2} H X^f / \\sqrt{m-1}$:\n$$ S^\\top S = \\left(\\frac{1}{\\sqrt{m-1}} R^{-1/2} H X^f\\right)^\\top \\left(\\frac{1}{\\sqrt{m-1}} R^{-1/2} H X^f\\right) = \\frac{1}{m-1} (X^f)^\\top H^\\top (R^{-1/2})^\\top R^{-1/2} H X^f $$\nSince $R$ is symmetric, $R^{-1/2}$ is also symmetric. Thus, $(R^{-1/2})^\\top = R^{-1/2}$, and $(R^{-1/2})^2 = R^{-1}$.\n$$ S^\\top S = \\frac{1}{m-1} (X^f)^\\top H^\\top R^{-1} H X^f $$\nSo, $T^2$ becomes:\n$$ T^2 = \\left( I_m + \\frac{1}{m-1} (X^f)^\\top H^\\top R^{-1} H X^f \\right)^{-1} $$\nAnd $P^a_{\\text{anom}}$ is:\n$$ P^a_{\\text{anom}} = \\frac{1}{m-1} X^f \\left( I_m + \\frac{1}{m-1} (X^f)^\\top H^\\top R^{-1} H X^f \\right)^{-1} (X^f)^\\top $$\nTo simplify this expression, we apply the Woodbury matrix identity, which states that $(A+UCV)^{-1} = A^{-1} - A^{-1}U(C^{-1}+VA^{-1}U)^{-1}VA^{-1}$. A useful variant is $U(C+VAV^\\top)^{-1}V = A^{-1} - (A+U^\\top C^{-1} U)^{-1}$. Let's use the form $X(I+Y X)^{-1} = (I+XY)^{-1}X$.\nLet's apply the identity $(I + UV)^{-1} = I - U(I+VU)^{-1}V$ to $T^2$.\nLet $U = \\frac{1}{m-1}(X^f)^\\top H^\\top R^{-1}$ and $V = H X^f$.\nThen $T^2 = (I_m + UV)^{-1} = I_m - U(I_p + VU)^{-1}V$.\nThe term $VU$ is $H X^f \\frac{1}{m-1}(X^f)^\\top H^\\top R^{-1} = H P^f H^\\top R^{-1}$.\nSo, $(I_p + VU)^{-1} = (I_p + H P^f H^\\top R^{-1})^{-1} = ((R + H P^f H^\\top)R^{-1})^{-1} = R(R+H P^f H^\\top)^{-1}$.\nSubstituting this back into the expression for $T^2$:\n$$ T^2 = I_m - \\frac{1}{m-1}(X^f)^\\top H^\\top R^{-1} \\left(R(R+H P^f H^\\top)^{-1}\\right) H X^f $$\n$$ T^2 = I_m - \\frac{1}{m-1}(X^f)^\\top H^\\top (R+H P^f H^\\top)^{-1} H X^f $$\nNow, we substitute this into the formula for $P^a_{\\text{anom}}$:\n$$ P^a_{\\text{anom}} = \\frac{1}{m-1} X^f T^2 (X^f)^\\top = \\frac{1}{m-1} X^f \\left( I_m - \\frac{1}{m-1}(X^f)^\\top H^\\top (R+H P^f H^\\top)^{-1} H X^f \\right) (X^f)^\\top $$\n$$ P^a_{\\text{anom}} = \\frac{1}{m-1} X^f (X^f)^\\top - \\frac{1}{m-1} X^f \\frac{1}{m-1}(X^f)^\\top H^\\top (R+H P^f H^\\top)^{-1} H X^f (X^f)^\\top $$\nRecognizing $P^f = \\frac{1}{m-1} X^f (X^f)^\\top$, this simplifies to:\n$$ P^a_{\\text{anom}} = P^f - P^f H^\\top (R+H P^f H^\\top)^{-1} H P^f $$\nThis is precisely the Kalman update formula for the posterior covariance, derived from the information form $( (P^f)^{-1} + H^\\top R^{-1} H )^{-1}$ via the Woodbury identity, which is robust to rank-deficient $P^f$. Let's call this $P^a_{\\text{post}}$.\n$$ P^a_{\\text{post}} = P^f - P^f H^\\top (H P^f H^\\top + R)^{-1} H P^f $$\nThus, we have analytically shown that $P^a_{\\text{anom}} = P^a_{\\text{post}}$. The problem, therefore, is a numerical verification exercise. The computed discrepancy metric $e$ will be non-zero due to the accumulation of floating-point errors through two distinct computational pathways.\n\nThe algorithm to compute $e$ for each test case is as follows:\n1.  For each test case, generate the matrices $X^f$, $H$, and $R$ according to the specified dimensions, procedures, and random seed. The columns of $X^f$ must be centered.\n2.  **Compute $P^a_{\\text{anom}}$**:\n    a. Compute $R^{-1/2}$ via eigen-decomposition: $R^{-1/2} = U_R \\Lambda_R^{-1/2} U_R^\\top$.\n    b. Form $S = R^{-1/2} H X^f / \\sqrt{m-1}$.\n    c. Form $A = I_m + S^\\top S$.\n    d. Compute the transform $T = A^{-1/2}$ via eigen-decomposition: $T = U_A \\Lambda_A^{-1/2} U_A^\\top$.\n    e. Compute analysis anomalies $X^a = X^f T$.\n    f. Compute $P^a_{\\text{anom}} = \\frac{1}{m-1} X^a (X^a)^\\top$.\n3.  **Compute $P^a_{\\text{post}}$**:\n    a. Compute the prior sample covariance $P^f = \\frac{1}{m-1} X^f (X^f)^\\top$.\n    b. Compute the innovation covariance $C_{inv} = H P^f H^\\top + R$.\n    c. Compute $C_{inv}^{-1}$ via eigen-decomposition: $C_{inv}^{-1} = U_C \\Lambda_C^{-1} U_C^\\top$.\n    d. Finally, compute $P^a_{\\text{post}} = P^f - P^f H^\\top C_{inv}^{-1} H P^f$.\n4.  **Compute the Discrepancy**:\n    a. Calculate the Frobenius norms $d = \\| P^a_{\\text{anom}} - P^a_{\\text{post}} \\|_F$ and $N = \\| P^a_{\\text{post}} \\|_F$.\n    b. The discrepancy is $e = d/N$.\n\nThis procedure is implemented for each of the five test cases to produce the final results.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef spd_matrix_op(A, op='inv_sqrt'):\n    \"\"\"\n    Computes A^{-1/2}, A^{1/2}, or A^{-1} for a symmetric matrix A\n    using eigen-decomposition.\n    \"\"\"\n    s, U = np.linalg.eigh(A)\n    if np.any(s <= 1e-12): # Use a tolerance for near-zero eigenvalues\n        s[s <= 1e-12] = 1e-12 # Clamp small eigenvalues to avoid division by zero/sqrt of negative\n\n    if op == 'inv_sqrt':\n        s_op = 1.0 / np.sqrt(s)\n    elif op == 'inv':\n        s_op = 1.0 / s\n    elif op == 'sqrt':\n        s_op = np.sqrt(s)\n    else:\n        raise ValueError(\"Unsupported operation\")\n        \n    return U @ np.diag(s_op) @ U.T\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all test cases.\n    \"\"\"\n\n    def compute_discrepancy(n, p, m, seed, H_func, R_func, Xf_func):\n        \"\"\"\n        Computes the discrepancy for a single test case.\n        \"\"\"\n        rng = np.random.default_rng(seed)\n\n        # 1. Generate matrices\n        H = H_func(n, p, rng)\n        R = R_func(p, rng)\n        Xf = Xf_func(n, m, rng)\n        \n        m_minus_1 = float(m - 1)\n\n        # --- Part 1: Compute P^a_anom ---\n        # S = R^{-1/2} H Xf / sqrt(m-1)\n        R_inv_sqrt = spd_matrix_op(R, 'inv_sqrt')\n        S = (R_inv_sqrt @ H @ Xf) / np.sqrt(m_minus_1)\n\n        # T = (I + S'S)^{-1/2}\n        T_base = np.eye(m) + S.T @ S\n        T = spd_matrix_op(T_base, 'inv_sqrt')\n\n        # Xa = Xf T\n        Xa = Xf @ T\n        \n        # Pa_anom = Xa Xa' / (m-1)\n        Pa_anom = (Xa @ Xa.T) / m_minus_1\n\n        # --- Part 2: Compute P^a_post ---\n        # Pf = Xf Xf' / (m-1)\n        Pf = (Xf @ Xf.T) / m_minus_1\n\n        # Pa_post = Pf - Pf H' (H Pf H' + R)^{-1} H Pf\n        Innov_cov = H @ Pf @ H.T + R\n        Innov_cov_inv = spd_matrix_op(Innov_cov, 'inv')\n        \n        # Using the formulation (I - KH)Pf is more stable\n        K = Pf @ H.T @ Innov_cov_inv\n        Pa_post = (np.eye(n) - K @ H) @ Pf\n\n        # --- Part 3: Compute discrepancy ---\n        diff_norm = np.linalg.norm(Pa_anom - Pa_post, 'fro')\n        post_norm = np.linalg.norm(Pa_post, 'fro')\n\n        if post_norm == 0:\n            return 0.0 if diff_norm == 0.0 else np.inf\n\n        return diff_norm / post_norm\n\n    # --- Define Test Cases ---\n\n    test_cases = [\n        # Case 1\n        {'n': 50, 'p': 20, 'm': 100, 'seed': 0,\n         'H': lambda n, p, rng: rng.normal(size=(p, n)) / np.sqrt(n),\n         'R': lambda p, rng: np.diag(0.5 + rng.uniform(size=p)),\n         'Xf': lambda n, m, rng: (X_raw := rng.normal(size=(n, m))) - X_raw.mean(axis=1, keepdims=True)},\n        # Case 2\n        {'n': 50, 'p': 20, 'm': 15, 'seed': 1,\n         'H': lambda n, p, rng: rng.normal(size=(p, n)) / np.sqrt(n),\n         'R': lambda p, rng: np.diag(0.5 + rng.uniform(size=p)),\n         'Xf': lambda n, m, rng: (X_raw := rng.normal(size=(n, m))) - X_raw.mean(axis=1, keepdims=True)},\n        # Case 3\n        {'n': 30, 'p': 30, 'm': 300, 'seed': 2,\n         'H': lambda n, p, rng: np.eye(n),\n         'R': lambda p, rng: np.diag(np.full(p, 0.1)),\n         'Xf': lambda n, m, rng: (X_raw := rng.normal(size=(n, m))) - X_raw.mean(axis=1, keepdims=True)},\n        # Case 4\n        {'n': 40, 'p': 25, 'm': 80, 'seed': 3,\n         'H': lambda n, p, rng: rng.normal(size=(p, n)) / np.sqrt(n),\n         'R': lambda p, rng: (B := rng.normal(size=(p, p))) @ B.T + 0.5 * np.eye(p),\n         'Xf': lambda n, m, rng: (X_raw := rng.normal(size=(n, m))) - X_raw.mean(axis=1, keepdims=True)},\n        # Case 5\n        {'n': 60, 'p': 15, 'm': 10, 'seed': 4,\n         'H': lambda n, p, rng: rng.normal(size=(p, n)) / np.sqrt(n),\n         'R': lambda p, rng: np.diag(0.5 + rng.uniform(size=p)),\n         'Xf': lambda n, m, rng: (X_raw := (rng.normal(size=(n, 5)) @ rng.normal(size=(5, m)))) - X_raw.mean(axis=1, keepdims=True)}\n    ]\n\n    results = []\n    for case in test_cases:\n        e = compute_discrepancy(case['n'], case['p'], case['m'], case['seed'],\n                                case['H'], case['R'], case['Xf'])\n        results.append(e)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3376014"}, {"introduction": "The deterministic square-root approach is not a single algorithm but a family of methods that preserve the posterior covariance. This exercise explores two distinct implementations: the common right-multiplying Ensemble Transform Kalman Filter (ETKF) and an alternative left-multiplying update derived from matrix square roots. By deriving and implementing both, you will gain a deeper appreciation for the mathematical structures that guarantee a consistent analysis update [@problem_id:3376040].", "problem": "Consider a linear-Gaussian data assimilation problem with state vector $\\mathbf{x} \\in \\mathbb{R}^{d}$, observation vector $\\mathbf{y} \\in \\mathbb{R}^{m}$, a linear observation operator $\\mathbf{H} \\in \\mathbb{R}^{m \\times d}$, and additive Gaussian observation noise $\\mathbf{v} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{R})$, where $\\mathbf{R} \\in \\mathbb{R}^{m \\times m}$ is symmetric positive definite. The prior information is represented by an ensemble of $N$ state realizations $\\{\\mathbf{x}^{b}_{i}\\}_{i=1}^{N}$ with sample mean $\\bar{\\mathbf{x}}^{b}$ and sample covariance $\\mathbf{B}$. The fundamental base you must start from is the standard linear-Gaussian Bayesian update, equivalently the Kalman filter update, which states that the analysis (posterior) mean $\\bar{\\mathbf{x}}^{a}$ and covariance $\\mathbf{B}^{a}$ are given by the well-tested formulas\n$$\n\\bar{\\mathbf{x}}^{a} = \\bar{\\mathbf{x}}^{b} + \\mathbf{K}\\,(\\mathbf{y} - \\mathbf{H}\\,\\bar{\\mathbf{x}}^{b}), \\quad \\mathbf{K} = \\mathbf{B}\\,\\mathbf{H}^{\\top}\\,(\\mathbf{H}\\,\\mathbf{B}\\,\\mathbf{H}^{\\top} + \\mathbf{R})^{-1},\n$$\nand\n$$\n\\mathbf{B}^{a} = (\\mathbf{I} - \\mathbf{K}\\,\\mathbf{H})\\,\\mathbf{B}\\,(\\mathbf{I} - \\mathbf{K}\\,\\mathbf{H})^{\\top} + \\mathbf{K}\\,\\mathbf{R}\\,\\mathbf{K}^{\\top}.\n$$\nYou are to derive, from these bases and core definitions of ensemble representation and sample covariance, two deterministic square-root ensemble analysis updates for the anomalies matrix that, when combined with the mean update above, produce an analysis ensemble whose sample covariance equals the target $\\mathbf{B}^{a}$. Let the prior anomaly matrix be $\\mathbf{A} = [\\mathbf{x}^{b}_{1} - \\bar{\\mathbf{x}}^{b}, \\dots, \\mathbf{x}^{b}_{N} - \\bar{\\mathbf{x}}^{b}] \\in \\mathbb{R}^{d \\times N}$, so that $\\mathbf{B} = \\frac{1}{N-1}\\,\\mathbf{A}\\,\\mathbf{A}^{\\top}$.\n\nYour tasks:\n\n- Derive a right-multiplying deterministic square-root update (often referred to as the Ensemble Transform Kalman Filter (ETKF) in the literature, but you must derive it anew here) that finds a symmetric positive-definite transform $\\mathbf{T}_{r} \\in \\mathbb{R}^{N \\times N}$ such that the analysis anomalies $\\mathbf{A}^{a} = \\mathbf{A}\\,\\mathbf{T}_{r}$ satisfy $\\frac{1}{N-1}\\,\\mathbf{A}^{a}\\,(\\mathbf{A}^{a})^{\\top} = \\mathbf{B}^{a}$.\n- Derive a left-multiplying symmetric square-root update that constructs a linear operator $\\mathbf{L} \\in \\mathbb{R}^{d \\times d}$ from symmetric square roots of $\\mathbf{B}$ and $\\mathbf{B}^{a}$ (using the Moore–Penrose pseudo-inverse where appropriate for rank-deficient cases) such that $\\mathbf{A}^{a} = \\mathbf{L}\\,\\mathbf{A}$ also satisfies $\\frac{1}{N-1}\\,\\mathbf{A}^{a}\\,(\\mathbf{A}^{a})^{\\top} = \\mathbf{B}^{a}$.\n\nUsing these two derivations, implement a single program that:\n- Computes the analysis mean using the Kalman filter equations above, with $\\mathbf{B}$ taken as the sample covariance from the given prior ensemble.\n- Computes the two sets of analysis anomalies via the two deterministic square-root updates you derived.\n- Forms two analysis ensembles by adding their anomalies to the analysis mean.\n- Verifies, for each method, that the sample analysis covariance equals $\\mathbf{B}^{a}$ and that the analysis ensemble mean equals the Kalman analysis mean.\n\nYour program must implement and evaluate these methods on the following deterministic test suite. For each case, you must set the random seed to the specified integer before generating any random quantities for that case. For each case:\n- Generate $\\mathbf{B}_{\\text{true}}$ as follows: draw $\\mathbf{G} \\in \\mathbb{R}^{d \\times d}$ with independent standard normal entries, and set $\\mathbf{B}_{\\text{true}} = \\frac{1}{d}\\mathbf{G}\\,\\mathbf{G}^{\\top} + 0.5\\,\\mathbf{I}_{d}$.\n- Generate the prior mean $\\bar{\\mathbf{x}}^{b}$ by drawing a vector in $\\mathbb{R}^{d}$ with independent standard normal entries.\n- Generate an ensemble by first computing a Cholesky factor $\\mathbf{L}_{\\text{true}}$ of $\\mathbf{B}_{\\text{true}}$ (lower-triangular), drawing $\\mathbf{Z} \\in \\mathbb{R}^{d \\times N}$ with independent standard normal entries, and setting $\\mathbf{X}^{b} = \\bar{\\mathbf{x}}^{b}\\,\\mathbf{1}^{\\top} + \\mathbf{L}_{\\text{true}}\\,\\mathbf{Z}$, where $\\mathbf{1} \\in \\mathbb{R}^{N}$ denotes the vector of ones.\n- Draw $\\mathbf{H} \\in \\mathbb{R}^{m \\times d}$ with independent standard normal entries.\n- Draw a diagonal $\\mathbf{R} \\in \\mathbb{R}^{m \\times m}$ by first drawing a vector $\\mathbf{r} \\in \\mathbb{R}^{m}$ with independent standard normal entries and setting $\\mathbf{R} = \\mathrm{diag}(0.1 + |\\mathbf{r}|)$.\n- Draw a “true” state $\\mathbf{x}^{\\dagger} \\in \\mathbb{R}^{d}$ with independent standard normal entries. Draw noise $\\boldsymbol{\\epsilon} \\in \\mathbb{R}^{m}$ with independent entries having variances equal to the diagonal entries of $\\mathbf{R}$, and set $\\mathbf{y} = \\mathbf{H}\\,\\mathbf{x}^{\\dagger} + \\boldsymbol{\\epsilon}$.\n\nThe test suite consists of the following cases $(d, N, m, \\text{seed})$:\n- Case A: $(5, 20, 4, 42)$.\n- Case B: $(10, 6, 7, 7)$.\n- Case C: $(8, 8, 8, 202)$.\n- Case D: $(12, 30, 3, 99)$.\n\nFor each case and for each method (right-multiplying and left-multiplying), compute two relative errors:\n- The relative mean error $e_{\\text{mean}} = \\frac{\\lVert \\bar{\\mathbf{x}}^{a}_{\\text{ens}} - \\bar{\\mathbf{x}}^{a} \\rVert_{2}}{\\max(1, \\lVert \\bar{\\mathbf{x}}^{a} \\rVert_{2})}$, where $\\bar{\\mathbf{x}}^{a}_{\\text{ens}}$ is the sample mean of the analysis ensemble.\n- The relative covariance error $e_{\\text{cov}} = \\frac{\\lVert \\mathbf{B}^{a}_{\\text{ens}} - \\mathbf{B}^{a} \\rVert_{F}}{\\max(1, \\lVert \\mathbf{B}^{a} \\rVert_{F})}$, where $\\mathbf{B}^{a}_{\\text{ens}}$ is the sample covariance of the analysis ensemble.\n\nFor each method in each case, report a single float error equal to $\\max(e_{\\text{mean}}, e_{\\text{cov}})$. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[result\\_1,result\\_2,\\dots]$), ordered as\n$$\n[\\text{err}_{\\text{right},A}, \\text{err}_{\\text{left},A}, \\text{err}_{\\text{right},B}, \\text{err}_{\\text{left},B}, \\text{err}_{\\text{right},C}, \\text{err}_{\\text{left},C}, \\text{err}_{\\text{right},D}, \\text{err}_{\\text{left},D}].\n$$\nAll quantities are dimensionless; no physical units or angles are involved. The final outputs must be floats. The problem requires reasoning from first principles by deriving the deterministic square-root transforms using the stated bases, and implementing them robustly for potentially rank-deficient prior covariance with appropriate pseudo-inverses where needed. No shortcut formulas may be assumed without derivation in your solution. The ensemble sizes and dimensions are chosen to include a happy path, rank-deficient prior covariance, a square observation operator, and an underdetermined observation scenario.", "solution": "The user-provided problem is assessed to be **valid**. It is a well-posed, scientifically grounded, and objective task within the domain of data assimilation and ensemble Kalman filtering. The problem statement provides a complete and consistent set of definitions, base equations, and objectives, allowing for a rigorous derivation and implementation.\n\nThe solution proceeds in three parts: first, a common framework is established based on the provided Kalman filter equations. Second, the right-multiplying square-root transform is derived. Third, the left-multiplying square-root transform is derived.\n\n### 1. Preliminaries and Analysis Equations\n\nLet the prior ensemble be represented by the matrix $\\mathbf{X}^{b} = [\\mathbf{x}^{b}_{1}, \\dots, \\mathbf{x}^{b}_{N}] \\in \\mathbb{R}^{d \\times N}$. The sample mean and anomaly matrix are:\n$$\n\\bar{\\mathbf{x}}^{b} = \\frac{1}{N} \\mathbf{X}^{b} \\mathbf{1}\n$$\n$$\n\\mathbf{A} = \\mathbf{X}^{b} - \\bar{\\mathbf{x}}^{b} \\mathbf{1}^{\\top}\n$$\nwhere $\\mathbf{1} \\in \\mathbb{R}^{N}$ is a vector of ones. By construction, the sum of the columns of $\\mathbf{A}$ is zero, i.e., $\\mathbf{A}\\mathbf{1} = \\mathbf{0}$. The prior sample covariance is $\\mathbf{B} = \\frac{1}{N-1}\\mathbf{A}\\mathbf{A}^{\\top}$.\n\nThe analysis (posterior) mean $\\bar{\\mathbf{x}}^{a}$ and covariance $\\mathbf{B}^{a}$ are given by the standard Kalman update equations:\n$$\n\\bar{\\mathbf{x}}^{a} = \\bar{\\mathbf{x}}^{b} + \\mathbf{K}\\,(\\mathbf{y} - \\mathbf{H}\\,\\bar{\\mathbf{x}}^{b})\n$$\nwhere the Kalman gain $\\mathbf{K}$ is:\n$$\n\\mathbf{K} = \\mathbf{B}\\,\\mathbf{H}^{\\top}\\,(\\mathbf{H}\\,\\mathbf{B}\\,\\mathbf{H}^{\\top} + \\mathbf{R})^{-1}\n$$\nThe problem specifies using the Joseph form for the analysis covariance, which is robust and symmetric by construction:\n$$\n\\mathbf{B}^{a} = (\\mathbf{I} - \\mathbf{K}\\,\\mathbf{H})\\,\\mathbf{B}\\,(\\mathbf{I} - \\mathbf{K}\\,\\mathbf{H})^{\\top} + \\mathbf{K}\\,\\mathbf{R}\\,\\mathbf{K}^{\\top}\n$$\nThis form is mathematically equivalent to the simpler form $\\mathbf{B}^{a} = (\\mathbf{I} - \\mathbf{K}\\mathbf{H})\\mathbf{B}$ when $\\mathbf{K}$ is the optimal Kalman gain. We will use the Joseph form for numerical computation as requested, and the simpler form for algebraic derivation where convenient.\n\nAn analysis ensemble $\\{\\mathbf{x}^{a}_{i}\\}_{i=1}^{N}$ is constructed from the analysis mean $\\bar{\\mathbf{x}}^{a}$ and a new analysis anomaly matrix $\\mathbf{A}^{a}$ as $\\mathbf{X}^{a} = \\bar{\\mathbf{x}}^{a}\\mathbf{1}^{\\top} + \\mathbf{A}^{a}$. For this ensemble to be consistent, it must satisfy two conditions:\n1.  **Correct Mean**: The sample mean must be $\\bar{\\mathbf{x}}^{a}$. This implies $\\mathbf{A}^{a}\\mathbf{1} = \\mathbf{0}$.\n2.  **Correct Covariance**: The sample covariance must be $\\mathbf{B}^{a}$. This implies $\\frac{1}{N-1}\\mathbf{A}^{a}(\\mathbf{A}^{a})^{\\top} = \\mathbf{B}^{a}$.\n\n### 2. Derivation of the Right-Multiplying Update\n\nWe seek a symmetric positive-definite transform $\\mathbf{T}_{r} \\in \\mathbb{R}^{N \\times N}$ such that the analysis anomalies $\\mathbf{A}^{a} = \\mathbf{A}\\,\\mathbf{T}_{r}$ satisfy the covariance condition.\nSubstituting $\\mathbf{A}^{a} = \\mathbf{A}\\,\\mathbf{T}_{r}$ into the covariance condition yields:\n$$\n\\frac{1}{N-1} (\\mathbf{A}\\mathbf{T}_{r})(\\mathbf{A}\\mathbf{T}_{r})^{\\top} = \\mathbf{B}^{a} \\implies \\frac{1}{N-1} \\mathbf{A}\\mathbf{T}_{r}(\\mathbf{T}_{r})^{\\top}\\mathbf{A}^{\\top} = \\mathbf{B}^{a}\n$$\nSince $\\mathbf{T}_{r}$ is symmetric, $(\\mathbf{T}_{r})^{\\top} = \\mathbf{T}_{r}$, so $\\mathbf{T}_{r}(\\mathbf{T}_{r})^{\\top} = \\mathbf{T}_{r}^2$. Thus, we need to find $\\mathbf{T}_{r}$ such that $\\frac{1}{N-1} \\mathbf{A}\\mathbf{T}_{r}^2\\mathbf{A}^{\\top} = \\mathbf{B}^{a}$.\n\nTo solve for $\\mathbf{T}_{r}$, we express $\\mathbf{B}^{a}$ in a form that facilitates comparison. Using the equivalent form $\\mathbf{B}^{a} = (\\mathbf{I} - \\mathbf{K}\\mathbf{H})\\mathbf{B}$ and substituting the ensemble expressions for $\\mathbf{B}$ and $\\mathbf{K}$:\n$$\n\\mathbf{B}^{a} = \\mathbf{B} - \\mathbf{B}\\mathbf{H}^{\\top}(\\mathbf{H}\\mathbf{B}\\mathbf{H}^{\\top}+\\mathbf{R})^{-1}\\mathbf{H}\\mathbf{B}\n$$\nLet $\\mathbf{Y} = \\mathbf{H}\\mathbf{A}$ be the matrix of projected anomalies. Then $\\mathbf{H}\\mathbf{B}\\mathbf{H}^{\\top} = \\frac{1}{N-1}\\mathbf{Y}\\mathbf{Y}^{\\top}$.\n$$\n\\mathbf{B}^{a} = \\frac{1}{N-1}\\mathbf{A}\\mathbf{A}^{\\top} - \\frac{1}{N-1}\\mathbf{A}\\mathbf{Y}^{\\top}\\left(\\frac{1}{N-1}\\mathbf{Y}\\mathbf{Y}^{\\top}+\\mathbf{R}\\right)^{-1}\\frac{1}{N-1}\\mathbf{Y}\\mathbf{A}^{\\top}\n$$\nUsing the Woodbury matrix identity on the inverse term, a more direct derivation shows that:\n$$\n\\mathbf{B}^{a} = \\frac{1}{N-1} \\mathbf{A} \\left( \\mathbf{I} + \\frac{1}{N-1}\\mathbf{Y}^{\\top}\\mathbf{R}^{-1}\\mathbf{Y} \\right)^{-1} \\mathbf{A}^{\\top}\n$$\nComparing this with our target expression $\\mathbf{B}^{a} = \\frac{1}{N-1} \\mathbf{A}\\mathbf{T}_{r}^2\\mathbf{A}^{\\top}$, we can identify:\n$$\n\\mathbf{T}_{r}^2 = \\left( \\mathbf{I} + \\frac{1}{N-1}\\mathbf{Y}^{\\top}\\mathbf{R}^{-1}\\mathbf{Y} \\right)^{-1}\n$$\nSince $\\mathbf{Y}^{\\top}\\mathbf{R}^{-1}\\mathbf{Y}$ is positive semi-definite, the matrix inside the parenthesis is symmetric positive-definite. Hence, we can take its unique symmetric positive-definite square root:\n$$\n\\mathbf{T}_{r} = \\left( \\mathbf{I} + \\frac{1}{N-1}\\mathbf{Y}^{\\top}\\mathbf{R}^{-1}\\mathbf{Y} \\right)^{-1/2}\n$$\nThe matrix square root $(\\cdot)^{-1/2}$ is computed via eigendecomposition: if $\\mathbf{M} = \\mathbf{U}\\mathbf{D}\\mathbf{U}^{\\top}$, then $\\mathbf{M}^{-1/2} = \\mathbf{U}\\mathbf{D}^{-1/2}\\mathbf{U}^{\\top}$.\n\nFinally, we verify the mean condition. Since $\\mathbf{A}\\mathbf{1} = \\mathbf{0}$, we have $\\mathbf{Y}\\mathbf{1} = \\mathbf{H}\\mathbf{A}\\mathbf{1} = \\mathbf{0}$. Let $\\mathbf{M} = \\mathbf{I} + \\frac{1}{N-1}\\mathbf{Y}^{\\top}\\mathbf{R}^{-1}\\mathbf{Y}$. Then $\\mathbf{M}\\mathbf{1} = \\mathbf{1} + \\frac{1}{N-1}\\mathbf{Y}^{\\top}\\mathbf{R}^{-1}(\\mathbf{Y}\\mathbf{1}) = \\mathbf{1}$. This shows $\\mathbf{1}$ is an eigenvector of $\\mathbf{M}$ with eigenvalue $1$. Consequently, $\\mathbf{1}$ is also an eigenvector of $\\mathbf{T}_{r} = \\mathbf{M}^{-1/2}$ with eigenvalue $1$. Therefore, $\\mathbf{A}^{a}\\mathbf{1} = \\mathbf{A}\\mathbf{T}_{r}\\mathbf{1} = \\mathbf{A}\\mathbf{1} = \\mathbf{0}$, satisfying the mean condition.\n\n### 3. Derivation of the Left-Multiplying Update\n\nWe seek a linear operator $\\mathbf{L} \\in \\mathbb{R}^{d \\times d}$ such that $\\mathbf{A}^{a} = \\mathbf{L}\\mathbf{A}$ satisfies the covariance condition:\n$$\n\\frac{1}{N-1} (\\mathbf{L}\\mathbf{A})(\\mathbf{L}\\mathbf{A})^{\\top} = \\mathbf{B}^{a} \\implies \\mathbf{L}\\left(\\frac{1}{N-1}\\mathbf{A}\\mathbf{A}^{\\top}\\right)\\mathbf{L}^{\\top} = \\mathbf{B}^{a} \\implies \\mathbf{L}\\mathbf{B}\\mathbf{L}^{\\top} = \\mathbf{B}^{a}\n$$\nThe problem suggests constructing $\\mathbf{L}$ from symmetric square roots of $\\mathbf{B}$ and $\\mathbf{B}^{a}$. Let $\\mathbf{B} = \\mathbf{S}_{b}\\mathbf{S}_{b}$ and $\\mathbf{B}^{a} = \\mathbf{S}_{a}\\mathbf{S}_{a}$, where $\\mathbf{S}_{b}$ and $\\mathbf{S}_{a}$ are the unique symmetric positive semi-definite square roots of $\\mathbf{B}$ and $\\mathbf{B}^{a}$, respectively. A natural candidate for $\\mathbf{L}$ is:\n$$\n\\mathbf{L} = \\mathbf{S}_{a} \\mathbf{S}_{b}^{+}\n$$\nwhere $\\mathbf{S}_{b}^{+}$ is the Moore-Penrose pseudo-inverse of $\\mathbf{S}_{b}$. Substituting this into the desired relation:\n$$\n\\mathbf{L}\\mathbf{B}\\mathbf{L}^{\\top} = (\\mathbf{S}_{a}\\mathbf{S}_{b}^{+}) \\mathbf{B} (\\mathbf{S}_{a}\\mathbf{S}_{b}^{+})^{\\top} = \\mathbf{S}_{a}\\mathbf{S}_{b}^{+} \\mathbf{S}_{b}^{2} (\\mathbf{S}_{b}^{+})^{\\top} \\mathbf{S}_{a}^{\\top}\n$$\nSince $\\mathbf{S}_{b}$ and $\\mathbf{S}_{b}^{+}$ are symmetric, this simplifies to $\\mathbf{S}_{a}(\\mathbf{S}_{b}^{+}\\mathbf{S}_{b}\\mathbf{S}_{b}\\mathbf{S}_{b}^{+})\\mathbf{S}_{a}$. Let $\\mathbf{P} = \\mathbf{S}_{b}\\mathbf{S}_{b}^{+}$. $\\mathbf{P}$ is the orthogonal projector onto the range of $\\mathbf{S}_{b}$ (which is the same as the range of $\\mathbf{B}$). The expression becomes $\\mathbf{S}_{a}\\mathbf{P}\\mathbf{S}_{a}$.\n\nFrom the covariance update formula $\\mathbf{B}^{a} = (\\mathbf{I}-\\mathbf{K}\\mathbf{H})\\mathbf{B}$, it is clear that any vector in the range of $\\mathbf{B}^{a}$ is a linear transformation of a vector in the range of $\\mathbf{B}$. Thus, $\\text{range}(\\mathbf{B}^{a}) \\subseteq \\text{range}(\\mathbf{B})$. This implies that the columns of $\\mathbf{S}_{a}$ lie within the range of $\\mathbf{S}_{b}$. Therefore, the projector $\\mathbf{P}$ acts as the identity on the columns of $\\mathbf{S}_{a}$, i.e., $\\mathbf{P}\\mathbf{S}_{a} = \\mathbf{S}_{a}$.\nThe expression thus simplifies to $\\mathbf{S}_{a}\\mathbf{S}_{a} = \\mathbf{B}^{a}$, confirming our choice of $\\mathbf{L}$ is correct.\n\nThe mean condition is trivially satisfied: $\\mathbf{A}^{a}\\mathbf{1} = \\mathbf{L}\\mathbf{A}\\mathbf{1} = \\mathbf{L}\\mathbf{0} = \\mathbf{0}$.\n\nThe square roots $\\mathbf{S}_b, \\mathbf{S}_a$ are computed via eigendecomposition (e.g., $\\mathbf{S}_b = \\mathbf{U}_b \\mathbf{D}_b^{1/2} \\mathbf{U}_b^\\top$ from $\\mathbf{B}=\\mathbf{U}_b\\mathbf{D}_b\\mathbf{U}_b^\\top$). The pseudo-inverse $\\mathbf{S}_{b}^{+}$ is computed using `numpy.linalg.pinv`.", "answer": "```python\nimport numpy as np\n\ndef _sqrtm_psd(X):\n    \"\"\"\n    Computes the unique symmetric positive semi-definite square root of a matrix X.\n    This is achieved via eigendecomposition.\n    \"\"\"\n    eigvals, eigvecs = np.linalg.eigh(X)\n    # Clamp small negative eigenvalues that can appear from numerical noise\n    eigvals[eigvals < 0] = 0\n    return eigvecs @ np.diag(np.sqrt(eigvals)) @ eigvecs.T\n\ndef solve():\n    \"\"\"\n    Derives and implements two deterministic square-root ensemble analysis updates,\n    and evaluates their accuracy on a suite of test cases.\n    \"\"\"\n    test_cases = [\n        # (d, N, m, seed)\n        (5, 20, 4, 42),   # Case A: d < N-1\n        (10, 6, 7, 7),    # Case B: d > N-1 (rank-deficient B)\n        (8, 8, 8, 202),   # Case C: d = N (rank-deficient B)\n        (12, 30, 3, 99),  # Case D: d < N-1\n    ]\n\n    results = []\n\n    for d, N, m, seed in test_cases:\n        np.random.seed(seed)\n\n        # 1. Generate test data according to problem specification\n        # Generate a \"true\" positive definite covariance matrix\n        G = np.random.standard_normal((d, d))\n        B_true = (G @ G.T) / d + 0.5 * np.eye(d)\n\n        # Generate a prior ensemble with a specified \"true\" mean and covariance\n        # The x_b_dist_mean is the mean of the generating distribution, not the ensemble sample mean.\n        x_b_dist_mean = np.random.standard_normal(d)\n        L_true = np.linalg.cholesky(B_true)\n        Z = np.random.standard_normal((d, N))\n        X_b = x_b_dist_mean[:, np.newaxis] + L_true @ Z\n        \n        # Generate observation model components\n        H = np.random.standard_normal((m, d))\n        r_diag = np.random.standard_normal(m)\n        R = np.diag(0.1 + np.abs(r_diag))\n\n        # Generate a \"true\" state and a corresponding observation\n        x_true = np.random.standard_normal(d)\n        epsilon = np.random.multivariate_normal(np.zeros(m), R)\n        y = H @ x_true + epsilon\n\n        # 2. Compute prior statistics from the generated ensemble\n        # The Kalman filter operates on the sample statistics of the given ensemble.\n        xb_bar = np.mean(X_b, axis=1)\n        A = X_b - xb_bar[:, np.newaxis]\n        B = (A @ A.T) / (N - 1)\n\n        # 3. Compute the theoretical Kalman analysis mean and covariance\n        HBH_T = H @ B @ H.T\n        S = HBH_T + R\n        K = B @ H.T @ np.linalg.inv(S)\n        \n        # This is the target analysis mean\n        xa_bar = xb_bar + K @ (y - H @ xb_bar)\n        \n        # This is the target analysis covariance (Joseph form, as specified)\n        Id = np.eye(d)\n        Ba = (Id - K @ H) @ B @ (Id - K @ H).T + K @ R @ K.T\n\n        # 4. Apply and verify the two deterministic square-root update methods\n\n        # --- Method 1: Right-Multiplying Transform (ETKF-style) ---\n        Y = H @ A\n        R_inv = np.linalg.inv(R)\n        M_transform = np.eye(N) + (Y.T @ R_inv @ Y) / (N - 1)\n        \n        eigvals_M, U_M = np.linalg.eigh(M_transform)\n        eigvals_M[eigvals_M < 0] = 0 # Ensure non-negativity\n        D_inv_sqrt_M = np.diag(1.0 / np.sqrt(eigvals_M))\n        Tr = U_M @ D_inv_sqrt_M @ U_M.T\n        \n        Aa_right = A @ Tr\n        Xa_right = xa_bar[:, np.newaxis] + Aa_right\n\n        # Verification for right-multiplying method\n        xa_bar_ens_right = np.mean(Xa_right, axis=1)\n        Aa_ens_right = Xa_right - xa_bar_ens_right[:, np.newaxis]\n        Ba_ens_right = (Aa_ens_right @ Aa_ens_right.T) / (N - 1)\n        \n        norm_xa_bar = np.linalg.norm(xa_bar)\n        e_mean_right = np.linalg.norm(xa_bar_ens_right - xa_bar) / max(1.0, norm_xa_bar)\n        \n        norm_Ba = np.linalg.norm(Ba, 'fro')\n        e_cov_right = np.linalg.norm(Ba_ens_right - Ba, 'fro') / max(1.0, norm_Ba)\n        \n        err_right = max(e_mean_right, e_cov_right)\n        results.append(err_right)\n\n        # --- Method 2: Left-Multiplying Transform ---\n        Sb_sqrt = _sqrtm_psd(B)\n        Sb_pinv = np.linalg.pinv(Sb_sqrt)\n        Sa_sqrt = _sqrtm_psd(Ba)\n        \n        L = Sa_sqrt @ Sb_pinv\n        Aa_left = L @ A\n        Xa_left = xa_bar[:, np.newaxis] + Aa_left\n\n        # Verification for left-multiplying method\n        xa_bar_ens_left = np.mean(Xa_left, axis=1)\n        Aa_ens_left = Xa_left - xa_bar_ens_left[:, np.newaxis]\n        Ba_ens_left = (Aa_ens_left @ Aa_ens_left.T) / (N - 1)\n        \n        e_mean_left = np.linalg.norm(xa_bar_ens_left - xa_bar) / max(1.0, norm_xa_bar)\n        e_cov_left = np.linalg.norm(Ba_ens_left - Ba, 'fro') / max(1.0, norm_Ba)\n        \n        err_left = max(e_mean_left, e_cov_left)\n        results.append(err_left)\n\n    print(f\"[{','.join(f'{r:.15e}' for r in results)}]\")\n\nsolve()\n```", "id": "3376040"}, {"introduction": "A key motivation for using deterministic square-root filters is their improved numerical stability compared to stochastic counterparts that require perturbing observations. This practice provides a direct, hands-on comparison of these two approaches in a dynamically unstable system where ensemble degeneracy, or 'collapse,' is a real risk. By simulating the probability of collapse for both filters, you will quantify the practical benefits of the deterministic update scheme [@problem_id:3375995].", "problem": "You are given a linear, time-invariant, discrete-time state-space model for data assimilation with an ensemble-based filter. The state evolution is specified by a square matrix $M \\in \\mathbb{R}^{d \\times d}$ and the observation operator is $H \\in \\mathbb{R}^{p \\times d}$. Observations are generated by $y_k = H x_k + v_k$ with zero-mean Gaussian noise $v_k \\sim \\mathcal{N}(0, R)$, where $R \\in \\mathbb{R}^{p \\times p}$ is symmetric positive definite. The ensemble size is $N_e$, and the ensemble anomalies are defined by forming the mean-centered ensemble and dividing by $\\sqrt{N_e - 1}$ so that the sample covariance equals the anomaly product. The forward dynamics $M$ is chosen so that at least one Lyapunov exponent is positive, making the system unstable in at least one direction. For a constant linear operator $M$, define the Lyapunov exponents as $\\lambda_i = \\log \\sigma_i(M)$, where $\\sigma_i(M)$ are the singular values of $M$. The analysis step is performed by either a deterministic square-root ensemble filter or a stochastic perturbed-observations ensemble filter. Define the event of ensemble collapse if, at any analysis step $k$, the smallest singular value of the current analysis anomaly matrix falls below a prescribed tolerance $\\tau$ times a reference scale taken from the initial anomalies.\n\nYour task is to compute, by Monte Carlo simulation with a fixed pseudorandom seed, the estimated probabilities that the ensemble collapses within a fixed number of analysis cycles $L$ for two filters:\n(i) the deterministic square-root ensemble transform Kalman filter (also known as the Ensemble Transform Kalman Filter), which updates anomalies via a deterministic right-multiplying transform that preserves the sample analysis covariance, and\n(ii) the stochastic Ensemble Kalman Filter with perturbed observations, which updates each member using perturbed innovations and in expectation matches the analysis covariance.\n\nBase your derivation on the following fundamental facts:\n(i) linear Gaussian state-space modeling and the Kalman filter update for mean and covariance at the analysis step,\n(ii) ensemble representation of covariances via mean-centered anomalies scaled by $\\sqrt{N_e - 1}$, and\n(iii) the definition of Lyapunov exponents for linear operators via singular values. Do not assume any special structure beyond symmetry and positive definiteness of $R$ and do not introduce any unphysical assumptions. Use angles in radians.\n\nUse the following precise and self-consistent test suite, which is designed to test different regimes including an unstable direction, moderate instability, and a near-neutral case. In all tests, initialize the true state at the origin so that $x_0 = 0$, draw the initial ensemble members independently from $\\mathcal{N}(0, \\sigma_0^2 I_d)$ with $\\sigma_0 = 1$, and use zero model noise. In all cases, define collapse as occurring if the smallest singular value $s_{\\min}$ of the analysis anomaly matrix at any cycle satisfies $s_{\\min} < \\tau s_{\\mathrm{ref}}$, where $s_{\\mathrm{ref}}$ is the mean singular value of the initial anomaly matrix for that Monte Carlo realization. Define the Monte Carlo estimate of the collapse probability as the fraction of realizations that collapse within $L$ cycles.\n\nTest case $1$ (happy path with one unstable and one stable direction):\n- State dimension $d = 2$, ensemble size $N_e = 10$, number of cycles $L = 20$.\n- Dynamics $M = Q \\, \\mathrm{diag}(1.5, 0.9) \\, Q^\\top$, where $Q$ is a rotation by angle $\\theta = 0.5$ radians, i.e., $Q = \\begin{bmatrix} \\cos \\theta & -\\sin \\theta \\\\ \\sin \\theta & \\cos \\theta \\end{bmatrix}$.\n- Observation operator $H = \\begin{bmatrix} 1 & 0 \\end{bmatrix}$, observation noise covariance $R = \\sigma_y^2 I_p$ with $\\sigma_y = 0.1$, so $p = 1$.\n- Tolerance $\\tau = 10^{-6}$, Monte Carlo realizations $S = 400$.\n\nTest case $2$ (moderate instability in three dimensions with two observed components):\n- State dimension $d = 3$, ensemble size $N_e = 12$, number of cycles $L = 20$.\n- Dynamics $M = Q \\, \\mathrm{diag}(1.8, 1.2, 0.8) \\, Q^\\top$, where $Q = R_z(\\alpha) R_y(\\beta) R_x(\\gamma)$ with $\\alpha = 0.4$, $\\beta = 0.3$, $\\gamma = 0.2$, and\n$R_x(\\gamma) = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & \\cos \\gamma & -\\sin \\gamma \\\\ 0 & \\sin \\gamma & \\cos \\gamma \\end{bmatrix}$,\n$R_y(\\beta) = \\begin{bmatrix} \\cos \\beta & 0 & \\sin \\beta \\\\ 0 & 1 & 0 \\\\ -\\sin \\beta & 0 & \\cos \\beta \\end{bmatrix}$,\n$R_z(\\alpha) = \\begin{bmatrix} \\cos \\alpha & -\\sin \\alpha & 0 \\\\ \\sin \\alpha & \\cos \\alpha & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}$.\n- Observation operator $H = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\end{bmatrix}$, observation noise covariance $R = \\sigma_y^2 I_p$ with $\\sigma_y = 0.05$, so $p = 2$.\n- Tolerance $\\tau = 10^{-6}$, Monte Carlo realizations $S = 400$.\n\nTest case $3$ (near-neutral dynamics with full observation and larger noise):\n- State dimension $d = 2$, ensemble size $N_e = 8$, number of cycles $L = 15$.\n- Dynamics $M = Q \\, \\mathrm{diag}(1.0, 0.99) \\, Q^\\top$, where $Q$ is a rotation by angle $\\theta = 0.3$ radians as in test $1$.\n- Observation operator $H = I_2$, observation noise covariance $R = \\sigma_y^2 I_p$ with $\\sigma_y = 0.5$, so $p = 2$.\n- Tolerance $\\tau = 10^{-6}$, Monte Carlo realizations $S = 400$.\n\nFor each test case, compute two numbers: the estimated collapse probability for the deterministic square-root filter and the estimated collapse probability for the stochastic filter with perturbed observations. Use the same fixed pseudorandom seed for all cases to ensure reproducibility.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, ordered as\n$[\\text{test1\\_det}, \\text{test1\\_stoch}, \\text{test2\\_det}, \\text{test2\\_stoch}, \\text{test3\\_det}, \\text{test3\\_stoch}]$,\nwhere each entry is a floating-point number in decimal form. No other output is allowed.", "solution": "The user-provided problem is assessed to be valid. It is a well-posed, scientifically grounded numerical task within the domain of data assimilation. It is self-contained, with all necessary parameters, models, and definitions provided. The problem is free of contradictions, ambiguities, or pseudo-scientific claims. A complete solution will now be presented.\n\n### 1. Mathematical Framework\n\n**State-Space Model**:\nThe state of the system, $x_k \\in \\mathbb{R}^d$ at a discrete time step $k$, evolves according to the linear forecast model:\n$$x_k = M x_{k-1}$$\nwhere $M \\in \\mathbb{R}^{d \\times d}$ is the dynamics matrix. The model is assumed to have no process noise.\nObservations $y_k \\in \\mathbb{R}^p$ are related to the true state through a linear observation operator $H \\in \\mathbb{R}^{p \\times d}$:\n$$y_k = H x_k + v_k$$\nwhere $v_k$ is a random observation noise vector drawn from a zero-mean Gaussian distribution, $v_k \\sim \\mathcal{N}(0, R)$, with a symmetric positive-definite covariance matrix $R \\in \\mathbb{R}^{p \\times p}$.\n\n**Ensemble Representation**:\nEnsemble-based filters approximate the state distribution using a finite set of $N_e$ samples, called ensemble members, $\\{x_i\\}_{i=1}^{N_e}$. The collection of members forms the ensemble matrix $X \\in \\mathbb{R}^{d \\times N_e}$. The state estimate is given by the ensemble mean:\n$$\\bar{x} = \\frac{1}{N_e} \\sum_{i=1}^{N_e} x_i$$\nThe uncertainty of the estimate is represented by the sample covariance matrix $P$:\n$$P = \\frac{1}{N_e-1} \\sum_{i=1}^{N_e} (x_i - \\bar{x})(x_i - \\bar{x})^\\top$$\nThe problem defines a scaled anomaly matrix $A \\in \\mathbb{R}^{d \\times N_e}$ whose columns are $a_i = (x_i - \\bar{x})/\\sqrt{N_e-1}$. With this definition, the sample covariance can be compactly written as:\n$$P = A A^\\top$$\n\n### 2. Monte Carlo Simulation\n\nThe probability of ensemble collapse is estimated by running $S$ independent simulations (realizations). For each realization, we perform the following steps:\n\n**Initialization**:\n1.  Set the initial true state to the origin: $x_0 = 0$.\n2.  Generate an initial ensemble $X_0$ by drawing $N_e$ members independently from $\\mathcal{N}(0, \\sigma_0^2 I_d)$, where $\\sigma_0=1$.\n3.  Compute the initial ensemble mean $\\bar{x}_0$ and anomaly matrix $A_0$.\n4.  Calculate the reference scale $s_{\\mathrm{ref}}$ as the mean of the singular values of the initial anomaly matrix $A_0$. The collapse threshold is then $\\tau s_{\\mathrm{ref}}$.\n5.  Initialize separate states for the deterministic and stochastic filters using the same initial ensemble.\n\n**Time Evolution (for $L$ cycles)**:\nAt each cycle $k=1, \\dots, L$:\n1.  **Forecast**: Propagate the true state and both ensembles forward in time using the dynamics matrix $M$. For any ensemble $X_{k-1}^a$, the forecast ensemble is $X_k^f = M X_{k-1}^a$. This linearity implies that the forecast mean is $\\bar{x}_k^f = M \\bar{x}_{k-1}^a$ and the forecast anomaly matrix is $A_k^f = M A_{k-1}^a$.\n2.  **Observation**: Generate a synthetic observation $y_k$ using the propagated true state $x_k$ and a random noise sample $v_k \\sim \\mathcal{N}(0, R)$.\n3.  **Analysis**: Update each filter's state (mean and anomalies for the deterministic filter; ensemble members for the stochastic filter) using the observation $y_k$. The specific update algorithms are detailed below.\n4.  **Collapse Check**: For each filter, compute the singular values of its new analysis anomaly matrix $A_k^a$. If the smallest singular value $s_{\\min}$ falls below the threshold $\\tau s_{\\mathrm{ref}}$, the ensemble for that filter is considered to have collapsed. The simulation for that filter's path can be stopped.\n\n**Aggregation**:\nAfter running $S$ realizations, the collapse probability for each filter is estimated as the fraction of realizations in which a collapse occurred.\n$$P_{\\text{collapse}} = \\frac{\\text{Number of collapsed realizations}}{S}$$\n\n### 3. Filter Algorithms\n\nBoth filters use the same forecast step. They differ in the analysis step.\n\n**A. Deterministic Square-Root Filter (ETKF)**\n\nThe ETKF updates the ensemble mean and anomalies deterministically.\n1.  **Analysis Mean**: The analysis mean $\\bar{x}_k^a$ is updated using the standard Kalman gain $K_k$:\n    $$\\bar{x}_k^a = \\bar{x}_k^f + K_k (y_k - H \\bar{x}_k^f)$$\n    where $K_k = P_k^f H^\\top (H P_k^f H^\\top + R)^{-1}$. Using the anomaly representation $P_k^f = A_k^f (A_k^f)^\\top$, this becomes:\n    $$K_k = A_k^f (H A_k^f)^\\top (H A_k^f (H A_k^f)^\\top + R)^{-1}$$\n\n2.  **Analysis Anomalies**: The analysis anomalies $A_k^a$ are obtained by right-multiplying the forecast anomalies $A_k^f$ by a transform matrix $T$:\n    $$A_k^a = A_k^f T$$\n    The matrix $T \\in \\mathbb{R}^{N_e \\times N_e}$ is chosen to ensure the analysis covariance $P_k^a = A_k^a(A_k^a)^\\top$ matches the theoretical Kalman analysis covariance, $P_k^a = ((P_k^f)^{-1} + H^\\top R^{-1}H)^{-1}$. This leads to the condition:\n    $$T T^\\top = \\left( I + (H A_k^f)^\\top R^{-1} (H A_k^f) \\right)^{-1}$$\n    A unique, symmetric, positive-definite solution for $T$ is the matrix square root of the right-hand side. We compute $T$ as $T = \\text{sqrtm}\\left( \\left( I + (H A_k^f)^\\top R^{-1} (H A_k^f) \\right)^{-1} \\right)$, where $\\text{sqrtm}$ denotes the principal matrix square root.\n\n**B. Stochastic EnKF with Perturbed Observations**\n\nThis filter updates each ensemble member individually.\n1.  **Kalman Gain**: Calculate the Kalman gain $K_k$ once, using the full forecast ensemble statistics, identical to the ETKF gain.\n2.  **Member Update**: For each ensemble member $i=1, \\dots, N_e$:\n    a. Generate a unique perturbation for the observation: $v_{k,i} \\sim \\mathcal{N}(0, R)$.\n    b. Create a perturbed observation: $y_{k,i} = y_k + v_{k,i}$.\n    c. Update the member:\n    $$x_{k,i}^a = x_{k,i}^f + K_k (y_{k,i} - H x_{k,i}^f)$$\nThe collection of the updated members $\\{x_{k,i}^a\\}_{i=1}^{N_e}$ forms the new analysis ensemble $X_k^a$. The analysis anomaly matrix $A_k^a$ required for the collapse check is then computed from $X_k^a$. The introduction of random noise in the update step is the key difference from the deterministic filter and influences the ensemble's statistical properties and stability.", "answer": "```python\nimport numpy as np\nfrom scipy import linalg\n\ndef solve():\n    \"\"\"\n    Main function to run the simulations for all test cases and print the results.\n    \"\"\"\n    # Use a fixed seed for reproducibility as required by the problem.\n    SEED = 42\n\n    test_cases = [\n        {\n            \"d\": 2, \"Ne\": 10, \"L\": 20, \"S\": 400, \"tau\": 1e-6,\n            \"M_params\": {\"type\": \"rotation_2d\", \"theta\": 0.5, \"diag\": np.array([1.5, 0.9])},\n            \"H\": np.array([[1.0, 0.0]]),\n            \"sigma_y\": 0.1,\n        },\n        {\n            \"d\": 3, \"Ne\": 12, \"L\": 20, \"S\": 400, \"tau\": 1e-6,\n            \"M_params\": {\"type\": \"rotation_3d\", \"angles\": (0.4, 0.3, 0.2), \"diag\": np.array([1.8, 1.2, 0.8])},\n            \"H\": np.array([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0]]),\n            \"sigma_y\": 0.05,\n        },\n        {\n            \"d\": 2, \"Ne\": 8, \"L\": 15, \"S\": 400, \"tau\": 1e-6,\n            \"M_params\": {\"type\": \"rotation_2d\", \"theta\": 0.3, \"diag\": np.array([1.0, 0.99])},\n            \"H\": np.eye(2),\n            \"sigma_y\": 0.5,\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        p_det, p_stoch = run_simulation_for_case(case, SEED)\n        results.extend([p_det, p_stoch])\n\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef _create_M_matrix(params):\n    \"\"\"Helper function to create the dynamics matrix M.\"\"\"\n    if params[\"type\"] == \"rotation_2d\":\n        theta = params[\"theta\"]\n        c, s = np.cos(theta), np.sin(theta)\n        Q = np.array([[c, -s], [s, c]])\n        D = np.diag(params[\"diag\"])\n        return Q @ D @ Q.T\n    elif params[\"type\"] == \"rotation_3d\":\n        alpha, beta, gamma = params[\"angles\"]\n        ca, sa = np.cos(alpha), np.sin(alpha)\n        cb, sb = np.cos(beta), np.sin(beta)\n        cg, sg = np.cos(gamma), np.sin(gamma)\n        Rx = np.array([[1, 0, 0], [0, cg, -sg], [0, sg, cg]])\n        Ry = np.array([[cb, 0, sb], [0, 1, 0], [-sb, 0, cb]])\n        Rz = np.array([[ca, -sa, 0], [sa, ca, 0], [0, 0, 1]])\n        Q = Rz @ Ry @ Rx\n        D = np.diag(params[\"diag\"])\n        return Q @ D @ Q.T\n    else:\n        raise ValueError(\"Unknown M_params type\")\n\ndef run_simulation_for_case(case_params, seed):\n    \"\"\"\n    Runs the Monte Carlo simulation for a single test case.\n    \"\"\"\n    d, Ne, L, S, tau = case_params[\"d\"], case_params[\"Ne\"], case_params[\"L\"], case_params[\"S\"], case_params[\"tau\"]\n    M = _create_M_matrix(case_params[\"M_params\"])\n    H = case_params[\"H\"]\n    p = H.shape[0]\n    sigma_y = case_params[\"sigma_y\"]\n    R = (sigma_y**2) * np.eye(p)\n    R_inv = np.linalg.inv(R)\n\n    rng = np.random.default_rng(seed)\n    \n    sigma_0 = 1.0\n\n    collapse_count_det = 0\n    collapse_count_stoch = 0\n\n    for _ in range(S):\n        # --- Initialization for one realization ---\n        x_true_k = np.zeros(d)\n        \n        X0 = rng.normal(loc=0.0, scale=sigma_0, size=(d, Ne))\n\n        x_mean0 = np.mean(X0, axis=1, keepdims=True)\n        A0 = (X0 - x_mean0) / np.sqrt(Ne - 1)\n        \n        s_ref = np.mean(np.linalg.svd(A0, compute_uv=False))\n        collapse_threshold = tau * s_ref\n        \n        # --- Initialize states for both filters ---\n        # Deterministic ETKF state\n        x_mean_det_k = x_mean0.flatten()\n        A_det_k = A0\n        \n        # Stochastic EnKF state\n        X_stoch_k = X0\n\n        has_collapsed_det = False\n        has_collapsed_stoch = False\n\n        for _ in range(L):\n            if has_collapsed_det and has_collapsed_stoch:\n                break\n            \n            # --- Forecast Step ---\n            x_true_k = M @ x_true_k\n            \n            # ETKF Forecast\n            x_mean_f_det = M @ x_mean_det_k\n            A_f_det = M @ A_det_k\n            \n            # Stochastic EnKF Forecast\n            X_f_stoch = M @ X_stoch_k\n\n            # --- Observation Step ---\n            v = rng.multivariate_normal(np.zeros(p), R)\n            y_obs = H @ x_true_k + v\n\n            # --- Analysis Step ---\n            # ETKF Analysis\n            if not has_collapsed_det:\n                HAf_det = H @ A_f_det\n                PfHt_det = A_f_det @ HAf_det.T\n                S_k_inv_det = np.linalg.inv(HAf_det @ HAf_det.T + R)\n                K_det = PfHt_det @ S_k_inv_det\n                \n                x_mean_a_det = x_mean_f_det + K_det @ (y_obs - H @ x_mean_f_det)\n                \n                # Transform matrix T for anomalies\n                T_inv_sq_arg = np.eye(Ne) + HAf_det.T @ R_inv @ HAf_det\n                T_inv = linalg.sqrtm(T_inv_sq_arg)\n                T = np.linalg.inv(T_inv)\n                A_a_det = A_f_det @ T.real\n                \n                # Check for collapse\n                if np.min(np.linalg.svd(A_a_det, compute_uv=False)) < collapse_threshold:\n                    has_collapsed_det = True\n                    collapse_count_det += 1\n                \n                x_mean_det_k = x_mean_a_det\n                A_det_k = A_a_det\n\n            # Stochastic EnKF Analysis\n            if not has_collapsed_stoch:\n                A_f_stoch = (X_f_stoch - np.mean(X_f_stoch, axis=1, keepdims=True)) / np.sqrt(Ne - 1)\n                Pf_stoch = A_f_stoch @ A_f_stoch.T\n                S_k_inv_stoch = np.linalg.inv(H @ Pf_stoch @ H.T + R)\n                K_stoch = Pf_stoch @ H.T @ S_k_inv_stoch\n                \n                obs_perturbations = rng.multivariate_normal(np.zeros(p), R, size=Ne)\n                \n                X_a_stoch = np.zeros_like(X_f_stoch)\n                for i in range(Ne):\n                    y_p = y_obs + obs_perturbations[i]\n                    X_a_stoch[:, i] = X_f_stoch[:, i] + K_stoch @ (y_p - H @ X_f_stoch[:, i])\n                \n                X_stoch_k = X_a_stoch\n                \n                # Check for collapse\n                A_a_stoch = (X_stoch_k - np.mean(X_stoch_k, axis=1, keepdims=True)) / np.sqrt(Ne - 1)\n                if np.min(np.linalg.svd(A_a_stoch, compute_uv=False)) < collapse_threshold:\n                    has_collapsed_stoch = True\n                    collapse_count_stoch += 1\n\n    prob_det = collapse_count_det / S\n    prob_stoch = collapse_count_stoch / S\n    \n    return prob_det, prob_stoch\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3375995"}]}