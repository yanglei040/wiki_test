{"hands_on_practices": [{"introduction": "The forecast step of the Kalman filter projects our knowledge of a system forward in time. This involves predicting not only the most likely future state but also the uncertainty associated with that prediction, captured by the forecast error covariance. This first practice [@problem_id:3381758] focuses on deriving the fundamental equation for covariance propagation from first principles, providing a solid theoretical foundation for all subsequent analysis and connecting it to the underlying system dynamics.", "problem": "Consider a linear Gaussian state-transition model used in the forecast step of the Kalman filter (KF) in inverse problems and data assimilation. The state evolves as $x_{k} = A x_{k-1} + w_{k-1}$, where $x_{k} \\in \\mathbb{R}^{n}$ is the state at time $k$, $A \\in \\mathbb{R}^{n \\times n}$ is the linear propagator, and $w_{k-1}$ is a zero-mean process noise independent of $x_{k-1}$. Let the previous posterior covariance be $P_{k-1|k-1} = \\operatorname{Cov}(x_{k-1} \\mid y_{1:k-1})$.\n\nStarting from the definition of covariance and using only the properties of expectations, independence, and linear transformations of random vectors, derive the expression for the forecast covariance $P_{k|k-1} = \\operatorname{Cov}(x_k \\mid y_{1:k-1})$ and compute it for the following numerical case with $n=2$:\n$$\nA = \\begin{pmatrix}\n1.5  0.4 \\\\\n0.2  0.7\n\\end{pmatrix}, \\quad\nP_{k-1|k-1} = \\begin{pmatrix}\n2.0  0.5 \\\\\n0.5  1.0\n\\end{pmatrix}, \\quad\nQ = \\begin{pmatrix}\n0.3  0.0 \\\\\n0.0  0.2\n\\end{pmatrix}.\n$$\nThen, based on first principles of how covariance transforms under a linear map, justify whether the operator $A$ amplifies or damps forecast uncertainty along its principal directions, and explain why.\n\nProvide your numerical matrix for $P_{k|k-1}$ exactly; no rounding is required. Your final answer must be the matrix $P_{k|k-1}$.", "solution": "The problem statement is critically validated before attempting a solution.\n\n### Step 1: Extract Givens\n-   State evolution model: $x_{k} = A x_{k-1} + w_{k-1}$, where $x_{k} \\in \\mathbb{R}^{n}$.\n-   $A \\in \\mathbb{R}^{n \\times n}$ is the linear propagator.\n-   $w_{k-1}$ is a zero-mean process noise, independent of $x_{k-1}$.\n-   The previous posterior covariance is $P_{k-1|k-1} = \\operatorname{Cov}(x_{k-1} \\mid y_{1:k-1})$.\n-   The forecast covariance is $P_{k|k-1} = \\operatorname{Cov}(x_k \\mid y_{1:k-1})$.\n-   The process noise covariance is $Q = \\operatorname{Cov}(w_{k-1})$. From the problem context, this is the given matrix $Q$.\n-   Numerical case for $n=2$:\n    -   Propagator matrix: $A = \\begin{pmatrix} 1.5  0.4 \\\\ 0.2  0.7 \\end{pmatrix}$\n    -   Previous posterior covariance: $P_{k-1|k-1} = \\begin{pmatrix} 2.0  0.5 \\\\ 0.5  1.0 \\end{pmatrix}$\n    -   Process noise covariance: $Q = \\begin{pmatrix} 0.3  0.0 \\\\ 0.0  0.2 \\end{pmatrix}$\n\n### Step 2: Validate Using Extracted Givens\n-   **Scientifically Grounded:** The problem describes the forecast (or prediction) step of the Kalman filter, a fundamental algorithm in estimation theory and data assimilation. The state-space model, definitions, and objective are standard and scientifically correct.\n-   **Well-Posed:** The problem asks for a standard derivation followed by a numerical calculation and a qualitative analysis. All necessary information is provided, and a unique, meaningful solution exists.\n-   **Objective:** The problem is stated using precise mathematical language, free from subjectivity.\n-   **Completeness and Consistency:** All required matrices ($A$, $P_{k-1|k-1}$, $Q$) are provided with consistent dimensions for $n=2$. The covariance matrices $P_{k-1|k-1}$ and $Q$ are symmetric, as required. The properties of the process noise $w_{k-1}$ are clearly defined. The problem is self-contained.\n\n### Step 3: Verdict and Action\nThe problem is valid. It is scientifically sound, well-posed, and all necessary information is provided. We proceed with the solution.\n\nThe forecast step of the Kalman filter propagates the state estimate and its covariance forward in time, from time $k-1$ to time $k$, before the measurement at time $k$ is incorporated. The solution is divided into three parts as requested: derivation of the forecast covariance equation, numerical computation for the given case, and analysis of the operator $A$.\n\n**Part 1: Derivation of the Forecast Covariance $P_{k|k-1}$**\n\nThe forecast covariance $P_{k|k-1}$ is defined as the covariance of the state $x_k$ conditioned on all observations up to time $k-1$, denoted by $y_{1:k-1}$.\n$$P_{k|k-1} = \\operatorname{Cov}(x_k \\mid y_{1:k-1}) = E[(x_k - E[x_k \\mid y_{1:k-1}])(x_k - E[x_k \\mid y_{1:k-1}])^T \\mid y_{1:k-1}]$$\nLet us define the forecast state estimate as $\\hat{x}_{k|k-1} = E[x_k \\mid y_{1:k-1}]$ and the previous posterior state estimate as $\\hat{x}_{k-1|k-1} = E[x_{k-1} \\mid y_{1:k-1}]$.\n\nFirst, we find the expression for the forecast state estimate $\\hat{x}_{k|k-1}$ by taking the conditional expectation of the state evolution equation:\n$$\\hat{x}_{k|k-1} = E[A x_{k-1} + w_{k-1} \\mid y_{1:k-1}]$$\nBy the linearity of expectation:\n$$\\hat{x}_{k|k-1} = E[A x_{k-1} \\mid y_{1:k-1}] + E[w_{k-1} \\mid y_{1:k-1}]$$\nSince $A$ is a constant matrix:\n$$\\hat{x}_{k|k-1} = A E[x_{k-1} \\mid y_{1:k-1}] + E[w_{k-1} \\mid y_{1:k-1}]$$\nThe first term is $A \\hat{x}_{k-1|k-1}$. For the second term, the process noise $w_{k-1}$ is independent of past states and thus independent of past observations $y_{1:k-1}$. It is also given to be zero-mean. Therefore, $E[w_{k-1} \\mid y_{1:k-1}] = E[w_{k-1}] = 0$.\nThus, the state forecast is:\n$$\\hat{x}_{k|k-1} = A \\hat{x}_{k-1|k-1}$$\n\nNow, we substitute this back into the covariance definition. The forecast error is $x_k - \\hat{x}_{k|k-1}$.\n$$x_k - \\hat{x}_{k|k-1} = (A x_{k-1} + w_{k-1}) - (A \\hat{x}_{k-1|k-1}) = A(x_{k-1} - \\hat{x}_{k-1|k-1}) + w_{k-1}$$\nLet $e_{k-1|k-1} = x_{k-1} - \\hat{x}_{k-1|k-1}$ be the posterior error at time $k-1$. Then the forecast error is $A e_{k-1|k-1} + w_{k-1}$.\nThe forecast covariance is the expected outer product of this error:\n$$P_{k|k-1} = E[(A e_{k-1|k-1} + w_{k-1})(A e_{k-1|k-1} + w_{k-1})^T \\mid y_{1:k-1}]$$\nExpanding the term inside the expectation:\n$$P_{k|k-1} = E[A e_{k-1|k-1} e_{k-1|k-1}^T A^T + A e_{k-1|k-1} w_{k-1}^T + w_{k-1} e_{k-1|k-1}^T A^T + w_{k-1} w_{k-1}^T \\mid y_{1:k-1}]$$\nUsing linearity of expectation again:\n$$P_{k|k-1} = E[A e_{k-1|k-1} e_{k-1|k-1}^T A^T] + E[A e_{k-1|k-1} w_{k-1}^T] + E[w_{k-1} e_{k-1|k-1}^T A^T] + E[w_{k-1} w_{k-1}^T]$$\n(where the conditioning on $y_{1:k-1}$ is implicit in the expectation $E[\\cdot]$).\nWe analyze each term:\n1.  $E[A e_{k-1|k-1} e_{k-1|k-1}^T A^T] = A E[e_{k-1|k-1} e_{k-1|k-1}^T] A^T = A P_{k-1|k-1} A^T$. This is because $P_{k-1|k-1} = \\operatorname{Cov}(x_{k-1} \\mid y_{1:k-1}) = E[e_{k-1|k-1}e_{k-1|k-1}^T \\mid y_{1:k-1}]$, since the error $e_{k-1|k-1}$ has zero mean.\n2.  The cross-terms are zero. For example, $E[A e_{k-1|k-1} w_{k-1}^T] = A E[e_{k-1|k-1} w_{k-1}^T]$. The error $e_{k-1|k-1}$ is a function of the history up to time $k-1$. The process noise $w_{k-1}$ is independent of this history. Since $w_{k-1}$ is also zero-mean, the expectation of the product is the product of expectations: $A E[e_{k-1|k-1}]E[w_{k-1}^T] = A \\cdot 0 \\cdot 0^T = 0$. Similarly, $E[w_{k-1} e_{k-1|k-1}^T A^T] = 0$.\n3.  $E[w_{k-1} w_{k-1}^T]$. Since $w_{k-1}$ is independent of past history and has zero mean, this is simply the covariance of the process noise, $Q = \\operatorname{Cov}(w_{k-1})$.\n\nCombining these results, we obtain the forecast covariance update equation:\n$$P_{k|k-1} = A P_{k-1|k-1} A^T + Q$$\n\n**Part 2: Numerical Computation**\n\nWe are given:\n$A = \\begin{pmatrix} 1.5  0.4 \\\\ 0.2  0.7 \\end{pmatrix}$, $P_{k-1|k-1} = \\begin{pmatrix} 2.0  0.5 \\\\ 0.5  1.0 \\end{pmatrix}$, $Q = \\begin{pmatrix} 0.3  0.0 \\\\ 0.0  0.2 \\end{pmatrix}$.\nFirst, we compute the term $A P_{k-1|k-1} A^T$.\n$A^T = \\begin{pmatrix} 1.5  0.2 \\\\ 0.4  0.7 \\end{pmatrix}$.\nNext, we calculate the product $A P_{k-1|k-1}$:\n$$A P_{k-1|k-1} = \\begin{pmatrix} 1.5  0.4 \\\\ 0.2  0.7 \\end{pmatrix} \\begin{pmatrix} 2.0  0.5 \\\\ 0.5  1.0 \\end{pmatrix} = \\begin{pmatrix} (1.5)(2.0)+(0.4)(0.5)  (1.5)(0.5)+(0.4)(1.0) \\\\ (0.2)(2.0)+(0.7)(0.5)  (0.2)(0.5)+(0.7)(1.0) \\end{pmatrix}$$\n$$A P_{k-1|k-1} = \\begin{pmatrix} 3.0+0.2  0.75+0.4 \\\\ 0.4+0.35  0.1+0.7 \\end{pmatrix} = \\begin{pmatrix} 3.2  1.15 \\\\ 0.75  0.8 \\end{pmatrix}$$\nNow, we multiply by $A^T$:\n$$A P_{k-1|k-1} A^T = \\begin{pmatrix} 3.2  1.15 \\\\ 0.75  0.8 \\end{pmatrix} \\begin{pmatrix} 1.5  0.2 \\\\ 0.4  0.7 \\end{pmatrix} = \\begin{pmatrix} (3.2)(1.5)+(1.15)(0.4)  (3.2)(0.2)+(1.15)(0.7) \\\\ (0.75)(1.5)+(0.8)(0.4)  (0.75)(0.2)+(0.8)(0.7) \\end{pmatrix}$$\n$$A P_{k-1|k-1} A^T = \\begin{pmatrix} 4.8+0.46  0.64+0.805 \\\\ 1.125+0.32  0.15+0.56 \\end{pmatrix} = \\begin{pmatrix} 5.26  1.445 \\\\ 1.445  0.71 \\end{pmatrix}$$\nFinally, we add the process noise covariance $Q$:\n$$P_{k|k-1} = A P_{k-1|k-1} A^T + Q = \\begin{pmatrix} 5.26  1.445 \\\\ 1.445  0.71 \\end{pmatrix} + \\begin{pmatrix} 0.3  0.0 \\\\ 0.0  0.2 \\end{pmatrix}$$\n$$P_{k|k-1} = \\begin{pmatrix} 5.26+0.3  1.445+0.0 \\\\ 1.445+0.0  0.71+0.2 \\end{pmatrix} = \\begin{pmatrix} 5.56  1.445 \\\\ 1.445  0.91 \\end{pmatrix}$$\n\n**Part 3: Analysis of Uncertainty Propagation by Operator $A$**\n\nThe forecast covariance update has two components: $A P_{k-1|k-1} A^T$, which represents the propagation of existing uncertainty by the system dynamics, and $Q$, which represents the addition of new uncertainty from the process noise. To analyze the effect of the operator $A$ itself, we must examine how it transforms uncertainty.\n\nFrom first principles, a linear map $x \\mapsto Ax$ transforms an error component. If the error is in a direction $v$ that is an eigenvector of $A$ (i.e., $Av = \\lambda v$), then the transformed error is $A(\\epsilon v) = \\epsilon(Av) = (\\epsilon \\lambda)v$. The magnitude of the error component in this direction is scaled by the magnitude of the eigenvalue, $|\\lambda|$. The principal directions of the operator $A$ are its eigendirections.\n-   If $|\\lambda|  1$, the operator $A$ amplifies any uncertainty component along the corresponding eigenvector $v$.\n-   If $|\\lambda|  1$, the operator $A$ damps (reduces) any uncertainty component along the corresponding eigenvector $v$.\n-   If $|\\lambda| = 1$, the uncertainty component is preserved in magnitude.\n\nWe compute the eigenvalues of $A$ from its characteristic equation, $\\det(A-\\lambda I) = 0$:\n$$\\det\\begin{pmatrix} 1.5 - \\lambda  0.4 \\\\ 0.2  0.7 - \\lambda \\end{pmatrix} = 0$$\n$$(1.5 - \\lambda)(0.7 - \\lambda) - (0.4)(0.2) = 0$$\n$$\\lambda^2 - 2.2\\lambda + 1.05 - 0.08 = 0$$\n$$\\lambda^2 - 2.2\\lambda + 0.97 = 0$$\nUsing the quadratic formula, $\\lambda = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$:\n$$\\lambda = \\frac{2.2 \\pm \\sqrt{(-2.2)^2 - 4(1)(0.97)}}{2} = \\frac{2.2 \\pm \\sqrt{4.84 - 3.88}}{2} = \\frac{2.2 \\pm \\sqrt{0.96}}{2}$$\nThe two eigenvalues are $\\lambda_1 = 1.1 + \\sqrt{0.24}$ and $\\lambda_2 = 1.1 - \\sqrt{0.24}$.\nSince $A$ has real entries, its eigenvectors corresponding to real eigenvalues are real vectors, defining real directions in $\\mathbb{R}^2$.\nLet's analyze their magnitudes:\n-   For $\\lambda_1 = 1.1 + \\sqrt{0.24}$: Since $\\sqrt{0.24}  0$, it is clear that $\\lambda_1  1.1$, so $|\\lambda_1|  1$.\n-   For $\\lambda_2 = 1.1 - \\sqrt{0.24}$: We need to determine if this is greater or less than $1$. This is equivalent to comparing $0.1$ and $\\sqrt{0.24}$. Squaring both positive numbers, we compare $0.1^2=0.01$ and $(\\sqrt{0.24})^2=0.24$. Since $0.01  0.24$, it follows that $0.1  \\sqrt{0.24}$, which means $1.1 - \\sqrt{0.24}  1.0$. Also, since $\\sqrt{0.24}  \\sqrt{1.21} = 1.1$, $\\lambda_2  0$. So, $0  \\lambda_2  1$, and thus $|\\lambda_2|  1$.\n\nConclusion: The operator $A$ has two distinct principal directions (its eigenvectors). Along one direction (corresponding to eigenvalue $\\lambda_1 \\approx 1.59$), uncertainty is amplified. Along the other direction (corresponding to eigenvalue $\\lambda_2 \\approx 0.61$), uncertainty is damped. Therefore, the operator $A$ has a mixed effect: it is unstable or expanding in one direction while being stable or contracting in another.", "answer": "$$\\boxed{\\begin{pmatrix} 5.56  1.445 \\\\ 1.445  0.91 \\end{pmatrix}}$$", "id": "3381758"}, {"introduction": "While the eigenvalues of a system's dynamics matrix, $F$, tell us about its long-term exponential growth or decay, they do not tell the whole story. This exercise explores the subtle but critical case of non-diagonalizable dynamics, where transient or even polynomial growth in uncertainty can occur despite all eigenvalues lying on the unit circle. By working through this hypothetical scenario with a Jordan block [@problem_id:3381710], you will gain a deeper appreciation for how the geometric structure of the system dynamics, not just its spectrum, governs the evolution of uncertainty.", "problem": "Consider a discrete-time linear Gaussian state-space model used in the forecast step of the Kalman filter (KF). The state evolves according to the linear dynamics\n$$\nx_{k+1} = F x_k + w_k,\n$$\nwhere $x_k \\in \\mathbb{R}^{2}$, the process noise $w_k$ is zero-mean, independent across time, and independent of $x_0$, with covariance $\\operatorname{Cov}(w_k) = Q$. The initial state $x_0$ has zero mean and covariance $\\operatorname{Cov}(x_0) = P_0$. All quantities are specified in the Jordan basis of $F$.\n\nYou are given that $F$ is the $2 \\times 2$ Jordan block with eigenvalue $1$, namely\n$$\nF = \\begin{pmatrix} 1  1 \\\\ 0  1 \\end{pmatrix},\n$$\nand that $Q = q I_2$ and $P_0 = p_0 I_2$ with scalars $q  0$ and $p_0  0$, where $I_2$ denotes the $2 \\times 2$ identity matrix. There are no analysis updates between steps; that is, the forecast covariance is generated purely by propagation and process noise accumulation from the initial time.\n\nStarting only from the fundamental definitions that (i) the covariance of a linearly transformed random vector satisfies $\\operatorname{Cov}(A x) = A \\operatorname{Cov}(x) A^{\\top}$ for any matrix $A$, and (ii) independent contributions add in covariance, $\\operatorname{Cov}(x + y) = \\operatorname{Cov}(x) + \\operatorname{Cov}(y)$ when $x$ and $y$ are independent, derive an explicit closed-form expression for the forecast variance of the first coordinate at time $k$, namely the $(1,1)$ entry of the forecast covariance matrix after $k$ steps,\n$$\n\\left[P_{k|0}\\right]_{11} = \\operatorname{Var}\\big((x_k)_1\\big),\n$$\nas a function of $k$, $p_0$, and $q$. Express your final answer as a single closed-form analytic expression; no rounding is required.", "solution": "The problem requires the derivation of a closed-form expression for the forecast variance of the first coordinate of a state vector, $[P_{k|0}]_{11}$, evolving according to a discrete-time linear Gaussian state-space model. There are no analysis updates, so the covariance at time $k$ is found by propagating the initial state covariance and accumulating the process noise covariance.\n\nThe state evolution is governed by the equation:\n$$\nx_{k+1} = F x_k + w_k\n$$\nwhere $x_k \\in \\mathbb{R}^2$ is the state vector at time $k$, $F$ is the state transition matrix, and $w_k$ is the process noise. The initial state is $x_0$.\n\nWe can express the state $x_k$ at an arbitrary time step $k$ in terms of the initial state $x_0$ and the sequence of process noise vectors $w_0, w_1, \\dots, w_{k-1}$ by unrolling the recursion:\n$x_1 = F x_0 + w_0$\n$x_2 = F x_1 + w_1 = F(F x_0 + w_0) + w_1 = F^2 x_0 + F w_0 + w_1$\nBy induction, we arrive at the general expression for $x_k$:\n$$\nx_k = F^k x_0 + \\sum_{i=0}^{k-1} F^{k-1-i} w_i\n$$\nThe problem asks for the $(1,1)$ entry of the forecast covariance matrix $P_{k|0}$, which we denote as $P_k = \\operatorname{Cov}(x_k)$ since there are no intermediate updates.\n$$\nP_k = \\operatorname{Cov}\\left(F^k x_0 + \\sum_{i=0}^{k-1} F^{k-1-i} w_i\\right)\n$$\nThe problem states that the initial state $x_0$ and the process noise terms $w_i$ are all mutually independent. Therefore, using the provided rule that covariances of independent random vectors add, $\\operatorname{Cov}(x+y) = \\operatorname{Cov}(x) + \\operatorname{Cov}(y)$, we can write:\n$$\nP_k = \\operatorname{Cov}(F^k x_0) + \\operatorname{Cov}\\left(\\sum_{i=0}^{k-1} F^{k-1-i} w_i\\right) = \\operatorname{Cov}(F^k x_0) + \\sum_{i=0}^{k-1} \\operatorname{Cov}(F^{k-1-i} w_i)\n$$\nUsing the rule for linear transformation of covariance, $\\operatorname{Cov}(A z) = A \\operatorname{Cov}(z) A^{\\top}$, we get:\n$$\nP_k = F^k \\operatorname{Cov}(x_0) (F^k)^{\\top} + \\sum_{i=0}^{k-1} \\left( F^{k-1-i} \\operatorname{Cov}(w_i) (F^{k-1-i})^{\\top} \\right)\n$$\nThe problem specifies the initial covariance $\\operatorname{Cov}(x_0) = P_0 = p_0 I_2$ and the process noise covariance $\\operatorname{Cov}(w_i) = Q = q I_2$ for all $i$. Substituting these into the equation yields:\n$$\nP_k = F^k (p_0 I_2) (F^k)^{\\top} + \\sum_{i=0}^{k-1} F^{k-1-i} (q I_2) (F^{k-1-i})^{\\top}\n$$\nSince $p_0$ and $q$ are scalars, they can be factored out:\n$$\nP_k = p_0 F^k (F^k)^{\\top} + q \\sum_{i=0}^{k-1} F^{k-1-i} (F^{k-1-i})^{\\top}\n$$\nThe state transition matrix is given as the Jordan block $F = \\begin{pmatrix} 1  1 \\\\ 0  1 \\end{pmatrix}$. We must find an expression for its powers, $F^j$. We can write $F = I_2 + N$ where $N = \\begin{pmatrix} 0  1 \\\\ 0  0 \\end{pmatrix}$. The matrix $N$ is nilpotent with $N^2 = 0$. Since $I_2$ and $N$ commute, we can use the binomial theorem:\n$$\nF^j = (I_2 + N)^j = \\binom{j}{0}I_2^j N^0 + \\binom{j}{1}I_2^{j-1} N^1 + \\dots = I_2 + jN = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} + j\\begin{pmatrix} 0  1 \\\\ 0  0 \\end{pmatrix} = \\begin{pmatrix} 1  j \\\\ 0  1 \\end{pmatrix}\n$$\nThe transpose is $(F^j)^{\\top} = \\begin{pmatrix} 1  0 \\\\ j  1 \\end{pmatrix}$.\nNow we compute the product $F^j (F^j)^{\\top}$:\n$$\nF^j (F^j)^{\\top} = \\begin{pmatrix} 1  j \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} 1  0 \\\\ j  1 \\end{pmatrix} = \\begin{pmatrix} 1+j^2  j \\\\ j  1 \\end{pmatrix}\n$$\nWe can now calculate the two components of $P_k$.\nThe first term, arising from the initial condition, is:\n$$\np_0 F^k (F^k)^{\\top} = p_0 \\begin{pmatrix} 1+k^2  k \\\\ k  1 \\end{pmatrix}\n$$\nThe $(1,1)$ entry of this term is $p_0(1+k^2)$.\n\nThe second term, arising from the accumulated process noise, is a sum. Let us change the summation index by setting $j = k-1-i$. As $i$ ranges from $0$ to $k-1$, $j$ ranges from $k-1$ down to $0$. The sum becomes:\n$$\nq \\sum_{j=0}^{k-1} F^j (F^j)^{\\top} = q \\sum_{j=0}^{k-1} \\begin{pmatrix} 1+j^2  j \\\\ j  1 \\end{pmatrix} = q \\begin{pmatrix} \\sum_{j=0}^{k-1}(1+j^2)  \\sum_{j=0}^{k-1}j \\\\ \\sum_{j=0}^{k-1}j  \\sum_{j=0}^{k-1}1 \\end{pmatrix}\n$$\nWe need the $(1,1)$ entry of this matrix, which is $q \\sum_{j=0}^{k-1}(1+j^2)$.\nWe evaluate the sum using standard formulas:\n$$\n\\sum_{j=0}^{k-1}(1+j^2) = \\sum_{j=0}^{k-1}1 + \\sum_{j=0}^{k-1}j^2\n$$\nThe first part is $\\sum_{j=0}^{k-1}1 = k$.\nThe second part is the sum of the first $k-1$ squares (since $0^2=0$): $\\sum_{j=0}^{k-1}j^2 = \\sum_{j=1}^{k-1}j^2$. The formula for the sum of the first $n$ squares is $\\frac{n(n+1)(2n+1)}{6}$. Setting $n=k-1$:\n$$\n\\sum_{j=0}^{k-1}j^2 = \\frac{(k-1)((k-1)+1)(2(k-1)+1)}{6} = \\frac{(k-1)k(2k-1)}{6}\n$$\nSo, the total sum is:\n$$\n\\sum_{j=0}^{k-1}(1+j^2) = k + \\frac{k(k-1)(2k-1)}{6}\n$$\nWe can simplify this expression:\n$$\nk + \\frac{k(2k^2 - 3k + 1)}{6} = \\frac{6k + 2k^3 - 3k^2 + k}{6} = \\frac{2k^3 - 3k^2 + 7k}{6} = \\frac{k(2k^2-3k+7)}{6}\n$$\nThe $(1,1)$ entry of the second term of $P_k$ is thus $q \\frac{k(2k^2-3k+7)}{6}$.\n\nFinally, the desired quantity, $[P_{k|0}]_{11}$, is the sum of the $(1,1)$ entries from both terms:\n$$\n[P_{k|0}]_{11} = p_0(1+k^2) + q \\frac{k(2k^2-3k+7)}{6}\n$$\nThis is the closed-form expression for the forecast variance of the first state coordinate as a function of $k$, $p_0$, and $q$.", "answer": "$$\n\\boxed{p_0(1+k^2) + q \\frac{k(2k^2-3k+7)}{6}}\n$$", "id": "3381710"}, {"introduction": "In theory, the covariance propagation formula is straightforward; in practice, its direct implementation can be a source of major numerical instability, especially in the high-dimensional systems common to fields like meteorology and oceanography. This practice moves from abstract equations to robust numerical algorithms by introducing the square-root filter. You will derive and implement a method [@problem_id:3381740] that propagates the 'square root' of the covariance matrix, thereby preserving its mathematical properties and avoiding the numerical pitfalls of working with the full covariance matrix.", "problem": "You are given a linear, discrete-time dynamical system in the standard form of the forecast step in the Kalman filter. Let the state be an $n$-dimensional vector, and let the forecast model be linear:\n$$\n\\mathbf{x}_{k+1} = \\mathbf{F}\\,\\mathbf{x}_k + \\mathbf{w}_k,\n$$\nwhere $\\mathbf{F} \\in \\mathbb{R}^{n \\times n}$ is the model operator and $\\mathbf{w}_k$ is the process noise with covariance $\\mathbf{Q} \\in \\mathbb{R}^{n \\times n}$ that is symmetric positive semi-definite. In ensemble-based data assimilation, we represent the state covariance by a square-root factor $\\mathbf{S} \\in \\mathbb{R}^{n \\times r}$ such that\n$$\n\\mathbf{P} = \\mathbf{S}\\,\\mathbf{S}^\\top,\n$$\nwhere $r$ is the factor rank. The forecast covariance should satisfy\n$$\n\\mathbf{P}^{f} = \\mathbf{F}\\,\\mathbf{P}\\,\\mathbf{F}^\\top + \\mathbf{Q}.\n$$\n\nYour task is to derive and implement a square-root ensemble forecast algorithm that computes a forecast square-root factor $\\mathbf{S}^{f}$ satisfying\n$$\n\\mathbf{S}^{f} \\left(\\mathbf{S}^{f}\\right)^\\top = \\mathbf{F}\\,\\mathbf{S}\\,\\mathbf{S}^\\top\\,\\mathbf{F}^\\top + \\mathbf{Q},\n$$\nwhile explicitly avoiding the formation of any covariance matrices. Your implementation must use a single thin (economy) QR factorization of a suitably constructed matrix that depends only on $\\mathbf{F}$, $\\mathbf{S}$, and a square-root factor of $\\mathbf{Q}$, and must not form $\\mathbf{P}$ or $\\mathbf{P}^{f}$ directly to compute $\\mathbf{S}^{f}$.\n\nStarting from first principles—namely, the core definitions of the forecast step in the Kalman filter, properties of square-root factorizations, and well-tested facts about QR factorization of Gram matrices—you must:\n- Derive why a single QR factorization of an augmented matrix that combines the propagated prior square-root factor and a square-root factor of the process noise yields a valid forecast square-root factor.\n- Explain, without forming any covariance matrix, how this avoids numerical issues associated with covariance formation in high dimensions.\n- Implement the derived algorithm and verify numerically that the resulting forecast square-root factor reproduces the forecast covariance implied by the model operator and process noise.\n\nDesign a deterministic test suite consisting of three test cases. For each test case, construct matrices using the specified random seeds and procedures so that the scenario is scientifically plausible:\n\n- Test Case $1$ (happy path):\n  - Dimension $n = 5$.\n  - Random seed $s = 42$.\n  - Construct $\\mathbf{P} = \\mathbf{S}\\,\\mathbf{S}^\\top$ with $\\mathbf{S}$ being the Cholesky factor of a symmetric positive-definite matrix built as $\\mathbf{R}\\,\\mathbf{R}^\\top + \\varepsilon\\,\\mathbf{I}$ with $\\varepsilon = 10^{-3}$ and $\\mathbf{R}$ with independent and identically distributed standard normal entries.\n  - Construct a stable $\\mathbf{F}$ by generating a random $\\mathbf{G}$ with independent and identically distributed standard normal entries and scaling $\\mathbf{G}$ by a factor $\\alpha = 0.8 / \\rho(\\mathbf{G})$, where $\\rho(\\mathbf{G})$ is the spectral radius of $\\mathbf{G}$, so that $\\rho(\\mathbf{F}) \\approx 0.8$.\n  - Construct $\\mathbf{Q}$ as $\\mathbf{B}\\,\\mathbf{B}^\\top + \\varepsilon\\,\\mathbf{I}$ with $\\mathbf{B}$ having independent and identically distributed standard normal entries and the same $\\varepsilon$; take its Cholesky factor as a square-root factor of $\\mathbf{Q}$.\n\n- Test Case $2$ (high-dimensional regime):\n  - Dimension $n = 100$.\n  - Random seed $s = 1234$.\n  - Construct $\\mathbf{S}$, $\\mathbf{F}$, and $\\mathbf{Q}$ analogously to Test Case $1$ with the same $\\varepsilon = 10^{-3}$ and stable scaling $\\alpha = 0.8 / \\rho(\\mathbf{G})$.\n\n- Test Case $3$ (edge case with rank-deficient process noise):\n  - Dimension $n = 40$.\n  - Random seed $s = 7$.\n  - Construct $\\mathbf{S}$ as in Test Case $1$.\n  - Construct a rank-deficient square-root factor of the process noise by first generating $\\mathbf{B}$ with independent and identically distributed standard normal entries, then setting the last $\\lfloor n/4 \\rfloor$ columns of $\\mathbf{B}$ to zero to reduce rank, and defining $\\mathbf{Q} = \\mathbf{B}\\,\\mathbf{B}^\\top$ with $\\mathbf{B}$ serving as its square-root factor.\n  - Construct a stable $\\mathbf{F}$ as in Test Case $1$.\n\nFor each test case, compute the forecast square-root factor $\\mathbf{S}^{f}$ using only the QR-based algorithm you derive, and then, for verification only, compute the explicit forecast covariance $\\mathbf{P}^{f} = \\mathbf{F}\\,\\mathbf{S}\\,\\mathbf{S}^\\top\\,\\mathbf{F}^\\top + \\mathbf{Q}$. Report, for each test case, the relative Frobenius-norm error between the covariance reconstructed from the QR-based square root and the explicit forecast covariance:\n$$\ne = \\frac{\\left\\| \\mathbf{S}^{f}\\left(\\mathbf{S}^{f}\\right)^\\top - \\mathbf{P}^{f} \\right\\|_F}{\\left\\| \\mathbf{P}^{f} \\right\\|_F}.\n$$\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[\\text{result}_1,\\text{result}_2,\\text{result}_3]$). Each $\\text{result}_i$ must be a floating-point number equal to the relative error $e$ for the corresponding test case. No physical units or angle units are involved in this problem; all outputs are dimensionless real numbers.", "solution": "The problem requires the derivation and implementation of a square-root forecast algorithm for a linear discrete-time dynamical system. This algorithm must compute the square-root factor of the forecast state covariance, $\\mathbf{S}^f$, without explicitly forming the full covariance matrices.\n\nThe state of the system evolves according to the linear model:\n$$\n\\mathbf{x}_{k+1} = \\mathbf{F}\\,\\mathbf{x}_k + \\mathbf{w}_k\n$$\nwhere $\\mathbf{x}_k \\in \\mathbb{R}^n$ is the state vector at time $k$, $\\mathbf{F} \\in \\mathbb{R}^{n \\times n}$ is the model operator, and $\\mathbf{w}_k$ is a zero-mean process noise with a symmetric positive semi-definite covariance matrix $\\mathbf{Q} \\in \\mathbb{R}^{n \\times n}$.\n\nThe prior state error covariance matrix, $\\mathbf{P}$, is represented by a square-root factor $\\mathbf{S} \\in \\mathbb{R}^{n \\times r}$ such that $\\mathbf{P} = \\mathbf{S}\\mathbf{S}^\\top$. The forecast error covariance, $\\mathbf{P}^f$, is given by the propagation equation:\n$$\n\\mathbf{P}^{f} = \\mathbf{F}\\,\\mathbf{P}\\,\\mathbf{F}^\\top + \\mathbf{Q}\n$$\nOur objective is to find a forecast square-root factor $\\mathbf{S}^f$ that satisfies $\\mathbf{S}^{f} \\left(\\mathbf{S}^{f}\\right)^\\top = \\mathbf{P}^{f}$.\n\n### Derivation from First Principles\n\nThe derivation proceeds from the fundamental definitions of the forecast covariance and matrix factorizations.\n\n1.  **Substitute Square-Root Factors**: We are given square-root factors for the prior covariance, $\\mathbf{P} = \\mathbf{S}\\mathbf{S}^\\top$, and the process noise covariance, for which we can define a factor $\\mathbf{S}_Q \\in \\mathbb{R}^{n \\times q}$ such that $\\mathbf{Q} = \\mathbf{S}_Q \\mathbf{S}_Q^\\top$. Substituting these into the forecast covariance equation yields:\n    $$\n    \\mathbf{P}^{f} = \\mathbf{F}\\,(\\mathbf{S}\\mathbf{S}^\\top)\\,\\mathbf{F}^\\top + \\mathbf{S}_Q \\mathbf{S}_Q^\\top\n    $$\n\n2.  **Rearrange into a Gram-like structure**: Using the matrix transpose property $(AB)^\\top = B^\\top A^\\top$, the first term can be rewritten as:\n    $$\n    \\mathbf{F}\\mathbf{S}\\mathbf{S}^\\top\\mathbf{F}^\\top = (\\mathbf{F}\\mathbf{S})(\\mathbf{S}^\\top\\mathbf{F}^\\top) = (\\mathbf{F}\\mathbf{S})(\\mathbf{F}\\mathbf{S})^\\top\n    $$\n    The forecast covariance is now a sum of two outer products:\n    $$\n    \\mathbf{P}^{f} = (\\mathbf{F}\\mathbf{S})(\\mathbf{F}\\mathbf{S})^\\top + \\mathbf{S}_Q \\mathbf{S}_Q^\\top\n    $$\n\n3.  **Construct an Augmented Matrix**: This sum can be expressed as a single matrix outer product by forming an augmented matrix. Let us define a new matrix $\\mathbf{M}$ by horizontally concatenating the propagated prior factor $\\mathbf{F}\\mathbf{S}$ and the process noise factor $\\mathbf{S}_Q$:\n    $$\n    \\mathbf{M} = \\begin{bmatrix} \\mathbf{F}\\mathbf{S}  \\mathbf{S}_Q \\end{bmatrix}\n    $$\n    The dimensions of $\\mathbf{M}$ are $n \\times (r+q)$, where $r$ and $q$ are the number of columns in $\\mathbf{S}$ and $\\mathbf{S}_Q$, respectively. The outer product of $\\mathbf{M}$ with itself is:\n    $$\n    \\mathbf{M}\\mathbf{M}^\\top = \\begin{bmatrix} \\mathbf{F}\\mathbf{S}  \\mathbf{S}_Q \\end{bmatrix} \\begin{bmatrix} (\\mathbf{F}\\mathbf{S})^\\top \\\\ \\mathbf{S}_Q^\\top \\end{bmatrix} = (\\mathbf{F}\\mathbf{S})(\\mathbf{F}\\mathbf{S})^\\top + \\mathbf{S}_Q \\mathbf{S}_Q^\\top\n    $$\n    This demonstrates the crucial identity: $\\mathbf{P}^f = \\mathbf{M}\\mathbf{M}^\\top$.\n\n4.  **Application of QR Factorization**: We now seek a matrix $\\mathbf{S}^f$ such that $\\mathbf{S}^f(\\mathbf{S}^f)^\\top = \\mathbf{M}\\mathbf{M}^\\top$. Instead of forming the product $\\mathbf{M}\\mathbf{M}^\\top$, which is precisely what we must avoid, we can use a QR factorization. Let's perform a thin (economy) QR factorization on the transpose of the augmented matrix, $\\mathbf{M}^\\top$:\n    $$\n    \\mathbf{M}^\\top = \\mathbf{Q}_{qr} \\mathbf{R}_{qr}\n    $$\n    Here, $\\mathbf{Q}_{qr}$ is a matrix with orthonormal columns (i.e., $\\mathbf{Q}_{qr}^\\top \\mathbf{Q}_{qr} = \\mathbf{I}$), and $\\mathbf{R}_{qr}$ is an upper triangular (or upper trapezoidal) matrix. Transposing this equation back gives $\\mathbf{M} = (\\mathbf{Q}_{qr} \\mathbf{R}_{qr})^\\top = \\mathbf{R}_{qr}^\\top \\mathbf{Q}_{qr}^\\top$.\n\n5.  **Identify the Forecast Square-Root Factor**: Now, we substitute this form of $\\mathbf{M}$ back into the expression for $\\mathbf{P}^f$:\n    $$\n    \\mathbf{P}^f = \\mathbf{M}\\mathbf{M}^\\top = (\\mathbf{R}_{qr}^\\top \\mathbf{Q}_{qr}^\\top)(\\mathbf{Q}_{qr} \\mathbf{R}_{qr}) = \\mathbf{R}_{qr}^\\top (\\mathbf{Q}_{qr}^\\top \\mathbf{Q}_{qr}) \\mathbf{R}_{qr}\n    $$\n    Since $\\mathbf{Q}_{qr}$ has orthonormal columns, $\\mathbf{Q}_{qr}^\\top \\mathbf{Q}_{qr} = \\mathbf{I}$. This simplifies the expression to:\n    $$\n    \\mathbf{P}^f = \\mathbf{R}_{qr}^\\top \\mathbf{I} \\mathbf{R}_{qr} = \\mathbf{R}_{qr}^\\top \\mathbf{R}_{qr}\n    $$\n    We are seeking a matrix $\\mathbf{S}^f$ that fulfills $\\mathbf{S}^f(\\mathbf{S}^f)^\\top = \\mathbf{P}^f$. If we define our forecast square-root factor as $\\mathbf{S}^f = \\mathbf{R}_{qr}^\\top$, we can verify this condition:\n    $$\n    \\mathbf{S}^f(\\mathbf{S}^f)^\\top = (\\mathbf{R}_{qr}^\\top)(\\mathbf{R}_{qr}^\\top)^\\top = \\mathbf{R}_{qr}^\\top \\mathbf{R}_{qr} = \\mathbf{P}^f\n    $$\n    This confirms that $\\mathbf{S}^f = \\mathbf{R}_{qr}^\\top$ is a valid forecast square-root factor. The algorithm computes this factor using only matrix-matrix multiplications and a single QR factorization, without ever forming $\\mathbf{P}$, $\\mathbf{Q}$, or $\\mathbf{P}^f$.\n\n### Avoiding Numerical Instability\n\nIn high-dimensional systems, covariance matrices are often ill-conditioned, meaning their singular values span many orders of magnitude. The process of forming a covariance matrix from its square-root factor, $\\mathbf{P} = \\mathbf{S}\\mathbf{S}^\\top$, involves a squaring operation. This squaring effect doubles the range of the logarithm of the singular values and squares the condition number of the matrix, i.e., $\\kappa(\\mathbf{P}) \\approx \\kappa(\\mathbf{S})^2$.\n\nFor example, if $\\mathbf{S}$ has a condition number of $10^9$, $\\mathbf{P}$ will have a condition number of $10^{18}$. In standard double-precision floating-point arithmetic (which has about $16$ decimal digits of precision), the smallest singular values of $\\mathbf{P}$ would be numerically indistinguishable from zero, leading to an effective loss of rank and information. Any subsequent operations involving $\\mathbf{P}$, such as inversion or addition, can suffer from large numerical errors or catastrophic cancellation.\n\nThe derived square-root algorithm circumvents this problem by operating directly on the factors $\\mathbf{S}$ and $\\mathbf{S}_Q$. The core operations are matrix multiplication and QR factorization. QR factorization is an orthogonal transformation, which is known to be backward stable and preserves the condition number of a matrix. By concatenating the factors into $\\mathbf{M}$ and then applying this stable transformation, we effectively combine the uncertainties represented by $\\mathbf{S}$ and $\\mathbf{S}_Q$ without squaring their condition numbers. This ensures that the numerical precision is maintained, making the algorithm robust even for very large and ill-conditioned problems typical in geosciences, engineering, and other fields.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import linalg\n\ndef solve():\n    \"\"\"\n    Solves the three test cases specified in the problem and prints the results.\n    \"\"\"\n    \n    test_cases = [\n        # (n, seed, case_type)\n        (5, 42, 'happy_path'),\n        (100, 1234, 'high_dim'),\n        (40, 7, 'rank_deficient'),\n    ]\n\n    results = []\n    for n, seed, case_type in test_cases:\n        error = run_test_case(n, seed, case_type)\n        results.append(error)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef run_test_case(n, seed, case_type):\n    \"\"\"\n    Generates matrices for a single test case, runs the QR-based square-root\n    forecast algorithm, and computes the verification error.\n    \n    Args:\n        n (int): The dimension of the state space.\n        seed (int): The random seed for reproducibility.\n        case_type (str): The type of test case to run.\n\n    Returns:\n        float: The relative Frobenius-norm error.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    epsilon = 1e-3\n\n    # 1. Construct the prior square-root factor S\n    # P = S S^T. We use Cholesky L, so P = L L^T. Thus S = L.\n    R_prior = rng.standard_normal((n, n))\n    P_tmp = R_prior @ R_prior.T + epsilon * np.identity(n)\n    S = linalg.cholesky(P_tmp, lower=True)\n\n    # 2. Construct a stable model operator F\n    G = rng.standard_normal((n, n))\n    eigvals = linalg.eigvals(G)\n    rho_G = np.max(np.abs(eigvals))\n    F = (0.8 / rho_G) * G\n\n    # 3. Construct the process noise covariance Q and its square-root factor S_Q\n    if case_type in ['happy_path', 'high_dim']:\n        B = rng.standard_normal((n, n))\n        Q = B @ B.T + epsilon * np.identity(n)\n        S_Q = linalg.cholesky(Q, lower=True)\n    elif case_type == 'rank_deficient':\n        B = rng.standard_normal((n, n))\n        num_zero_cols = n // 4\n        B[:, -num_zero_cols:] = 0.0\n        S_Q = B\n        # For verification, we need the explicit Q\n        Q = S_Q @ S_Q.T\n    else:\n        raise ValueError(\"Invalid case type specified.\")\n\n    # --- QR-Based Square-Root Forecast Algorithm ---\n    # Propagate the prior square-root factor\n    FS = F @ S\n    \n    # Form the augmented matrix M = [FS, S_Q]\n    M = np.hstack([FS, S_Q])\n    \n    # Perform thin QR factorization of M^T = Q_qr R_qr\n    # Using scipy.linalg.qr mode='economic' for thin QR\n    Q_qr, R_qr = linalg.qr(M.T, mode='economic')\n    \n    # The new forecast square-root factor is S_f = R_qr^T\n    S_f = R_qr.T\n    \n    # --- Verification ---\n    # Reconstruct the forecast covariance from the computed square-root factor\n    P_f_sqrt = S_f @ S_f.T\n\n    # Compute the explicit forecast covariance for direct comparison\n    P = S @ S.T\n    P_f_explicit = F @ P @ F.T + Q\n\n    # Compute the relative Frobenius-norm error\n    # e = || S_f S_f^T - P_f ||_F / || P_f ||_F\n    numerator = linalg.norm(P_f_sqrt - P_f_explicit, 'fro')\n    denominator = linalg.norm(P_f_explicit, 'fro')\n    \n    if denominator == 0:\n        return 0.0 if numerator == 0 else np.inf\n        \n    relative_error = numerator / denominator\n    return relative_error\n\nsolve()\n```", "id": "3381740"}]}