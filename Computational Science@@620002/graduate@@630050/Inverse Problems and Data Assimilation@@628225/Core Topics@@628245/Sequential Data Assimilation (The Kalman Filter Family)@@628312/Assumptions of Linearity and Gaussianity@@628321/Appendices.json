{"hands_on_practices": [{"introduction": "The Kalman filter provides the optimal state estimate for any linear system with Gaussian noise, forming the bedrock of modern data assimilation. To truly grasp its mechanics, it is essential to understand how its components arise from foundational Bayesian principles. This exercise [@problem_id:3365459] guides you through the derivation of the steady-state Kalman gain from first principles, revealing how the assumptions of linearity and Gaussianity lead directly to the filter's elegant recursive structure.", "problem": "Consider a scalar linear-Gaussian state-space model used in inverse problems and data assimilation, given by the dynamics and observation equations $x_{k+1} = a x_{k} + w_{k}$ and $y_{k} = b x_{k} + \\epsilon_{k}$, where $w_{k} \\sim \\mathcal{N}(0, Q)$ and $\\epsilon_{k} \\sim \\mathcal{N}(0, R)$ are mutually independent and independent across time. Assume $Q > 0$, $R > 0$, and $b \\neq 0$. Assume further that the parameters satisfy the standard detectability and stabilizability conditions so that the steady-state Kalman filter exists.\n\nStarting only from the linearity of the model and the Gaussianity of all uncertainties and using Bayes’ theorem together with the rule for the product of Gaussian densities, derive the steady-state scalar Kalman gain purely as a function of $a$, $b$, $Q$, and $R$. Your derivation must proceed from first principles by:\n- Establishing the Gaussian form of the one-step predictive distribution and the measurement likelihood.\n- Forming the posterior distribution and determining its mean in terms of the innovation and a gain factor.\n- Deriving the fixed-point relation for the steady-state error covariances implied by the Gaussian posterior and the linear dynamics.\n- Solving the resulting algebraic relation to eliminate any covariance variables and obtain a closed-form analytic expression for the steady-state Kalman gain solely in terms of $a$, $b$, $Q$, and $R$.\n\nProvide the final expression for the steady-state Kalman gain. No numerical evaluation is required, and no rounding is needed. The final answer must be a single closed-form analytic expression in terms of $a$, $b$, $Q$, and $R$ only, with no remaining auxiliary variables.", "solution": "The problem requires the derivation of the steady-state Kalman gain for a scalar linear-Gaussian state-space model. The derivation must proceed from first principles, namely Bayes' theorem and the properties of Gaussian distributions.\n\nThe state-space model is given by:\nState dynamics: $x_{k+1} = a x_{k} + w_{k}$, where $w_{k} \\sim \\mathcal{N}(0, Q)$\nObservation model: $y_{k} = b x_{k} + \\epsilon_{k}$, where $\\epsilon_{k} \\sim \\mathcal{N}(0, R)$\n\nHere, $x_k$ is the hidden state at time $k$, and $y_k$ is the observation. The process noise $w_k$ and measurement noise $\\epsilon_k$ are assumed to be independent, zero-mean Gaussian white noise sequences with variances $Q > 0$ and $R > 0$, respectively. The parameters $a$ and $b$ are scalars, with $b \\neq 0$.\n\nThe core of the Kalman filter is a Bayesian update cycle, where the posterior distribution of the state at time $k-1$ is used to predict the state at time $k$, which then serves as a prior for incorporating the measurement at time $k$.\n\nLet the posterior distribution of the state $x_{k-1}$ given all observations up to time $k-1$, denoted $y_{1:k-1}$, be Gaussian:\n$$p(x_{k-1} | y_{1:k-1}) = \\mathcal{N}(x_{k-1}; \\hat{x}_{k-1|k-1}, P_{k-1|k-1})$$\nwhere $\\hat{x}_{k-1|k-1}$ is the filtered state estimate and $P_{k-1|k-1}$ is its error covariance.\n\n**Step 1: Prediction (Time Update)**\nThe first step is to derive the one-step predictive distribution, $p(x_k | y_{1:k-1})$, which serves as the prior for the update step at time $k$. Using the state dynamics equation, $x_k = a x_{k-1} + w_{k-1}$, we find the mean and variance of $x_k$. Since $x_{k-1}$ and $w_{k-1}$ are independent and Gaussian, $x_k$ is also Gaussian.\nThe predicted mean is:\n$$\\hat{x}_{k|k-1} = E[x_k | y_{1:k-1}] = E[a x_{k-1} + w_{k-1} | y_{1:k-1}] = a E[x_{k-1} | y_{1:k-1}] + E[w_{k-1}] = a \\hat{x}_{k-1|k-1}$$\nThe predicted error covariance is:\n$$P_{k|k-1} = \\text{Var}(x_k - \\hat{x}_{k|k-1}) = \\text{Var}(a(x_{k-1} - \\hat{x}_{k-1|k-1}) + w_{k-1}) = a^2 P_{k-1|k-1} + Q$$\nThus, the predictive distribution (prior) is:\n$$p(x_k | y_{1:k-1}) = \\mathcal{N}(x_k; \\hat{x}_{k|k-1}, P_{k|k-1})$$\n\n**Step 2: Update (Measurement Update)**\nThe second step is to update the prior with the new information from the measurement $y_k$. According to Bayes' theorem, the posterior distribution is proportional to the product of the likelihood and the prior:\n$$p(x_k | y_{1:k}) \\propto p(y_k | x_k) p(x_k | y_{1:k-1})$$\nThe measurement likelihood, from $y_k = b x_k + \\epsilon_k$, is also Gaussian:\n$$p(y_k | x_k) = \\mathcal{N}(y_k; b x_k, R)$$\nSince the product of two Gaussian probability density functions (PDFs) is an unnormalized Gaussian PDF, the posterior $p(x_k | y_{1:k})$ is Gaussian. We can find its mean and variance by analyzing the exponent of the product:\n$$\\ln p(x_k | y_{1:k}) \\propto -\\frac{1}{2}\\left( \\frac{(x_k - \\hat{x}_{k|k-1})^2}{P_{k|k-1}} + \\frac{(y_k - b x_k)^2}{R} \\right)$$\nExpanding the quadratic terms in $x_k$:\n$$\\text{exponent} \\propto x_k^2 \\left(\\frac{1}{P_{k|k-1}} + \\frac{b^2}{R}\\right) - 2x_k \\left(\\frac{\\hat{x}_{k|k-1}}{P_{k|k-1}} + \\frac{b y_k}{R}\\right)$$\nThe posterior PDF is $p(x_k | y_{1:k}) = \\mathcal{N}(x_k; \\hat{x}_{k|k}, P_{k|k})$, whose exponent is proportional to $-\\frac{1}{2 P_{k|k}}(x_k - \\hat{x}_{k|k})^2 \\propto \\frac{x_k^2}{P_{k|k}} - \\frac{2x_k \\hat{x}_{k|k}}{P_{k|k}}$.\nBy comparing the coefficients of $x_k^2$, we find the inverse of the posterior covariance $P_{k|k}$:\n$$\\frac{1}{P_{k|k}} = \\frac{1}{P_{k|k-1}} + \\frac{b^2}{R} \\implies P_{k|k} = \\left(\\frac{1}{P_{k|k-1}} + \\frac{b^2}{R}\\right)^{-1} = \\frac{P_{k|k-1}R}{R + b^2 P_{k|k-1}}$$\nBy comparing the coefficients of $x_k$, we find the posterior mean $\\hat{x}_{k|k}$:\n$$\\frac{\\hat{x}_{k|k}}{P_{k|k}} = \\frac{\\hat{x}_{k|k-1}}{P_{k|k-1}} + \\frac{b y_k}{R} \\implies \\hat{x}_{k|k} = P_{k|k} \\left(\\frac{\\hat{x}_{k|k-1}}{P_{k|k-1}} + \\frac{b y_k}{R}\\right)$$\nTo obtain the standard Kalman filter form, we substitute the expression for $P_{k|k}$:\n$$\\hat{x}_{k|k} = \\frac{P_{k|k-1}R}{R + b^2 P_{k|k-1}} \\left(\\frac{\\hat{x}_{k|k-1}}{P_{k|k-1}} + \\frac{b y_k}{R}\\right) = \\frac{R}{R + b^2 P_{k|k-1}} \\hat{x}_{k|k-1} + \\frac{b P_{k|k-1}}{R + b^2 P_{k|k-1}} y_k$$\nThis can be rewritten in the familiar form involving the Kalman gain $K_k$:\n$$\\hat{x}_{k|k} = \\hat{x}_{k|k-1} + K_k (y_k - b \\hat{x}_{k|k-1})$$\nBy comparing the two expressions for $\\hat{x}_{k|k}$, specifically the coefficient of $y_k$, we identify the Kalman gain:\n$$K_k = \\frac{b P_{k|k-1}}{R + b^2 P_{k|k-1}}$$\n\n**Step 3: Steady-State Analysis**\nThe problem asks for the steady-state Kalman gain. In steady-state, the error covariances converge to constant values as $k \\to \\infty$. Let $P_{k|k-1} \\to P_p$ and $P_{k|k} \\to P_a$. The Kalman gain also converges to a constant value $K$. The recursive equations for the covariances become algebraic equations:\n1. Prediction: $P_p = a^2 P_a + Q$\n2. Update: $P_a = \\frac{P_p R}{R + b^2 P_p}$\nThe steady-state gain is $K = \\frac{b P_p}{R + b^2 P_p}$.\n\nTo find $K$ purely in terms of the model parameters, we must eliminate the covariance variables. We can derive a single algebraic equation for $K$. From the gain equation, we express $P_p$ in terms of $K$:\n$$K(R + b^2 P_p) = b P_p \\implies KR + K b^2 P_p = b P_p \\implies KR = P_p(b - K b^2) = P_p b(1 - K b)$$\n$$P_p = \\frac{KR}{b(1-Kb)}$$\nAlso, from the gain equation, we have $\\frac{P_p}{R + b^2 P_p} = \\frac{K}{b}$. Substituting this into the update equation for $P_a$:\n$$P_a = R \\left( \\frac{K}{b} \\right)$$\nNow substitute these expressions for $P_p$ and $P_a$ into the prediction equation:\n$$P_p = a^2 P_a + Q \\implies \\frac{KR}{b(1-Kb)} = a^2 \\left(\\frac{RK}{b}\\right) + Q$$\nTo solve for $K$, we multiply the equation by $b(1-Kb)$:\n$$KR = a^2 R K (1-Kb) + Q b (1-Kb)$$\n$$KR = a^2 R K - a^2 b R K^2 + Qb - Q b^2 K$$\nRearranging the terms yields a quadratic equation for the steady-state gain $K$:\n$$(a^2 b R) K^2 + (R - a^2 R + Q b^2) K - Q b = 0$$\n$$(a^2 b R) K^2 + (R(1 - a^2) + Q b^2) K - Q b = 0$$\n\n**Step 4: Solving for the Kalman Gain**\nThis is a quadratic equation of the form $\\mathcal{A}K^2 + \\mathcal{B}K + \\mathcal{C} = 0$, with:\n$\\mathcal{A} = a^2 b R$\n$\\mathcal{B} = R(1 - a^2) + Q b^2$\n$\\mathcal{C} = -Q b$\n\nThe solution is $K = \\frac{-\\mathcal{B} \\pm \\sqrt{\\mathcal{B}^2 - 4\\mathcal{A}\\mathcal{C}}}{2\\mathcal{A}}$. The existence of a steady-state solution implies we must select the root that corresponds to a stable filter, i.e., one for which the error dynamics are stable. This corresponds to a unique gain $K$ for which the sign matches the sign of $b$. To avoid issues with catastrophic cancellation when $a$ is small and to write the solution in a form that is well-defined for $a=0$, we multiply the numerator and denominator by the conjugate of the numerator:\n$$K = \\frac{-\\mathcal{B} + \\sqrt{\\mathcal{B}^2 - 4\\mathcal{A}\\mathcal{C}}}{2\\mathcal{A}} = \\frac{4\\mathcal{A}\\mathcal{C}}{2\\mathcal{A}(-\\mathcal{B} - \\sqrt{\\mathcal{B}^2 - 4\\mathcal{A}\\mathcal{C}})} = \\frac{2\\mathcal{C}}{-\\mathcal{B} - \\sqrt{\\mathcal{B}^2 - 4\\mathcal{A}\\mathcal{C}}}$$\nSubstituting the expressions for $\\mathcal{A}$, $\\mathcal{B}$, and $\\mathcal{C}$:\n$$K = \\frac{2(-Qb)}{-(R(1 - a^2) + Q b^2) - \\sqrt{(R(1 - a^2) + Q b^2)^2 - 4(a^2 b R)(-Qb)}}$$\n$$K = \\frac{-2Qb}{-(R(1 - a^2) + Q b^2) - \\sqrt{(R(1 - a^2) + Q b^2)^2 + 4 a^2 b^2 Q R}}$$\n$$K = \\frac{2Qb}{R(1 - a^2) + Q b^2 + \\sqrt{(R(1 - a^2) + Q b^2)^2 + 4 a^2 b^2 Q R}}$$\nThis is the closed-form analytic expression for the steady-state scalar Kalman gain.", "answer": "$$\n\\boxed{\\frac{2Qb}{R(1 - a^2) + Q b^2 + \\sqrt{\\left(R(1 - a^2) + Q b^2\\right)^2 + 4 a^2 b^2 Q R}}}\n$$", "id": "3365459"}, {"introduction": "The Gaussian assumption extends to scenarios with correlated observation errors, which are modeled by a non-diagonal covariance matrix $R$. This exercise [@problem_id:3365471] demystifies the role of the inverse covariance matrix in the data misfit function by guiding you through the construction of a whitening transform. By converting a problem with correlated noise into an equivalent one with uncorrelated, unit-variance noise, you will gain a practical understanding of generalized least squares.", "problem": "Consider a linear observation model in inverse problems under the assumptions of linearity and Gaussianity. Let the observations be modeled by $y = H x + \\epsilon$, where $y \\in \\mathbb{R}^{3}$ are the observations, $x \\in \\mathbb{R}^{2}$ is the state, $H \\in \\mathbb{R}^{3 \\times 2}$ is the linear observation operator, and $\\epsilon \\in \\mathbb{R}^{3}$ is the observation error with a multivariate normal distribution $\\epsilon \\sim \\mathcal{N}(0, R)$ having symmetric positive definite (SPD) covariance matrix $R \\in \\mathbb{R}^{3 \\times 3}$. Assume the following specific, scientifically realistic matrices and vectors:\n$$\nR = \\begin{pmatrix}\n4 & 2 & 0 \\\\\n2 & 10 & 6 \\\\\n0 & 6 & 5\n\\end{pmatrix}, \\quad\nH = \\begin{pmatrix}\n1 & 0 \\\\\n1 & 1 \\\\\n0 & 2\n\\end{pmatrix}, \\quad\ny = \\begin{pmatrix}\n4 \\\\\n0 \\\\\n3\n\\end{pmatrix}, \\quad\nx = \\begin{pmatrix}\n2 \\\\\n-1\n\\end{pmatrix}.\n$$\nStarting only from the definition of the multivariate normal Probability Density Function (PDF) and the linear observation model, perform the following:\n\n1. Derive the negative log-likelihood (data misfit) consistent with linearity and Gaussianity, in terms of the residual $d = y - H x$ and the covariance $R$.\n\n2. Construct a whitening transform, i.e., find a matrix $W \\in \\mathbb{R}^{3 \\times 3}$ such that $W R W^{\\top} = I$, by using the Cholesky factorization of $R$ into a lower-triangular matrix $L$ with positive diagonal entries, where $R = L L^{\\top}$, and then setting $W = L^{-1}$.\n\n3. Using your constructed $W$, define the whitened variables $\\tilde{y} = W y$ and $\\tilde{H} = W H$, and show the effect of whitening on the data misfit by expressing it as the Euclidean norm of the whitened residual, i.e., show that it becomes a standard least-squares form.\n\n4. Evaluate the whitened data misfit numerically for the given $y$, $H$, and $x$, using exact arithmetic and simplifying to an exact rational number where possible.\n\nYour final answer must be a single closed-form analytic expression: the explicit whitening matrix $W$. No rounding is required, and no units are associated with the quantities in this problem.", "solution": "**1. Derivation of the Negative Log-Likelihood**\n\nThe problem assumes a linear observation model $y = H x + \\epsilon$, where the observation error $\\epsilon$ follows a multivariate normal distribution with zero mean and covariance matrix $R$, denoted as $\\epsilon \\sim \\mathcal{N}(0, R)$. Given a state $x$, the observations $y$ are thus also normally distributed, $y \\sim \\mathcal{N}(Hx, R)$.\n\nThe probability density function (PDF) for a multivariate normal random vector $z \\in \\mathbb{R}^k$ with mean $\\mu$ and covariance matrix $\\Sigma$ is given by:\n$$\np(z) = \\frac{1}{\\sqrt{(2\\pi)^k \\det(\\Sigma)}} \\exp\\left(-\\frac{1}{2} (z-\\mu)^{\\top} \\Sigma^{-1} (z-\\mu)\\right)\n$$\nIn our context, the vector of observations $y$ is in $\\mathbb{R}^3$, so $k=3$. The mean is $\\mu = Hx$ and the covariance is $\\Sigma=R$. The likelihood of observing $y$ given the state $x$ is therefore:\n$$\n\\mathcal{L}(x|y) = p(y|x) = \\frac{1}{\\sqrt{(2\\pi)^3 \\det(R)}} \\exp\\left(-\\frac{1}{2} (y-Hx)^{\\top} R^{-1} (y-Hx)\\right)\n$$\nThe negative log-likelihood is obtained by taking the natural logarithm of the likelihood function and negating it:\n$$\n-\\ln(\\mathcal{L}(x|y)) = -\\ln\\left(\\frac{1}{\\sqrt{(2\\pi)^3 \\det(R)}}\\right) - \\ln\\left(\\exp\\left(-\\frac{1}{2} (y-Hx)^{\\top} R^{-1} (y-Hx)\\right)\\right)\n$$\n$$\n-\\ln(\\mathcal{L}(x|y)) = \\ln\\left(\\sqrt{(2\\pi)^3 \\det(R)}\\right) + \\frac{1}{2} (y-Hx)^{\\top} R^{-1} (y-Hx)\n$$\n$$\n-\\ln(\\mathcal{L}(x|y)) = \\frac{3}{2}\\ln(2\\pi) + \\frac{1}{2}\\ln(\\det(R)) + \\frac{1}{2} (y-Hx)^{\\top} R^{-1} (y-Hx)\n$$\nIn optimization and data assimilation, the data misfit function, often denoted $J(x)$, consists of the terms in the negative log-likelihood that depend on the state $x$. The constant terms are typically dropped. Thus, the data misfit is proportional to the quadratic term. We define the misfit as:\n$$\nJ(x) = \\frac{1}{2} (y-Hx)^{\\top} R^{-1} (y-Hx)\n$$\nDefining the residual as $d = y - Hx$, the data misfit is expressed as:\n$$\nJ(x) = \\frac{1}{2} d^{\\top} R^{-1} d\n$$\n\n**2. Construction of the Whitening Transform**\n\nWe are tasked with finding a whitening matrix $W$ such that $W R W^{\\top} = I$. This is achieved by first finding the Cholesky factorization of $R$ into $R = L L^{\\top}$, where $L$ is a lower-triangular matrix with positive diagonal entries, and then setting $W = L^{-1}$.\n\nGiven $R = \\begin{pmatrix} 4 & 2 & 0 \\\\ 2 & 10 & 6 \\\\ 0 & 6 & 5 \\end{pmatrix}$, we seek $L = \\begin{pmatrix} L_{11} & 0 & 0 \\\\ L_{21} & L_{22} & 0 \\\\ L_{31} & L_{32} & L_{33} \\end{pmatrix}$ such that $L L^{\\top} = R$.\n\nThe components of $L$ are found sequentially:\n- $L_{11}^2 = R_{11} = 4 \\implies L_{11} = 2$.\n- $L_{21}L_{11} = R_{21} = 2 \\implies L_{21}(2) = 2 \\implies L_{21} = 1$.\n- $L_{31}L_{11} = R_{31} = 0 \\implies L_{31}(2) = 0 \\implies L_{31} = 0$.\n- $L_{21}^2 + L_{22}^2 = R_{22} = 10 \\implies 1^2 + L_{22}^2 = 10 \\implies L_{22}^2 = 9 \\implies L_{22} = 3$.\n- $L_{32}L_{22} + L_{31}L_{21} = R_{32} = 6 \\implies L_{32}(3) + (0)(1) = 6 \\implies L_{32} = 2$.\n- $L_{31}^2 + L_{32}^2 + L_{33}^2 = R_{33} = 5 \\implies 0^2 + 2^2 + L_{33}^2 = 5 \\implies L_{33}^2 = 1 \\implies L_{33} = 1$.\n\nThus, the Cholesky factor is $L = \\begin{pmatrix} 2 & 0 & 0 \\\\ 1 & 3 & 0 \\\\ 0 & 2 & 1 \\end{pmatrix}$.\n\nNext, we find the whitening matrix $W = L^{-1}$ by solving $L W = I$ for $W$ using forward substitution. Let $W = (w_{ij})$.\n$$\n\\begin{pmatrix} 2 & 0 & 0 \\\\ 1 & 3 & 0 \\\\ 0 & 2 & 1 \\end{pmatrix} \\begin{pmatrix} w_{11} & w_{12} & w_{13} \\\\ w_{21} & w_{22} & w_{23} \\\\ w_{31} & w_{32} & w_{33} \\end{pmatrix} = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}\n$$\nSolving for each column of $W$:\n- Column 1: $2w_{11}=1 \\implies w_{11}=\\frac{1}{2}$. $w_{11}+3w_{21}=0 \\implies \\frac{1}{2}+3w_{21}=0 \\implies w_{21}=-\\frac{1}{6}$. $2w_{21}+w_{31}=0 \\implies 2(-\\frac{1}{6})+w_{31}=0 \\implies w_{31}=\\frac{1}{3}$. Also $w_{12}=w_{13}=0$.\n- Column 2: $3w_{22}=1 \\implies w_{22}=\\frac{1}{3}$. $2w_{22}+w_{32}=0 \\implies 2(\\frac{1}{3})+w_{32}=0 \\implies w_{32}=-\\frac{2}{3}$. Also $w_{23}=0$.\n- Column 3: $w_{33}=1$.\n\nThe resulting whitening matrix is $W = L^{-1} = \\begin{pmatrix} \\frac{1}{2} & 0 & 0 \\\\ -\\frac{1}{6} & \\frac{1}{3} & 0 \\\\ \\frac{1}{3} & -\\frac{2}{3} & 1 \\end{pmatrix}$.\n\n**3. Effect of Whitening on Data Misfit**\n\nWe start with the data misfit expression $J(x) = \\frac{1}{2} (y-Hx)^{\\top} R^{-1} (y-Hx)$.\nFrom $R = L L^{\\top}$, we have $R^{-1} = (L L^{\\top})^{-1} = (L^{\\top})^{-1}L^{-1}$. Since the inverse of a transpose is the transpose of the inverse, $(L^{\\top})^{-1} = (L^{-1})^{\\top}$. With $W=L^{-1}$, we get $R^{-1} = W^{\\top}W$.\nSubstituting this into the misfit function:\n$$\nJ(x) = \\frac{1}{2} (y-Hx)^{\\top} (W^{\\top}W) (y-Hx)\n$$\nUsing the property $(AB)^{\\top} = B^{\\top}A^{\\top}$, we can regroup the terms:\n$$\nJ(x) = \\frac{1}{2} [W(y-Hx)]^{\\top} [W(y-Hx)]\n$$\nDefining the whitened observations $\\tilde{y} = Wy$, the whitened operator $\\tilde{H} = WH$, and the whitened residual $\\tilde{d} = \\tilde{y} - \\tilde{H}x = W(y-Hx)$, the expression becomes:\n$$\nJ(x) = \\frac{1}{2} \\tilde{d}^{\\top}\\tilde{d} = \\frac{1}{2} ||\\tilde{d}||_2^2 = \\frac{1}{2} ||\\tilde{y} - \\tilde{H}x||_2^2\n$$\nThis demonstrates that the generalized least-squares cost function, weighted by the inverse of the covariance matrix, is transformed into a standard, unweighted least-squares problem involving the sum of squared whitened residuals.\n\n**4. Numerical Evaluation of the Whitened Data Misfit**\n\nFirst, we compute the residual $d = y - Hx$ with the given values:\n$H = \\begin{pmatrix} 1 & 0 \\\\ 1 & 1 \\\\ 0 & 2 \\end{pmatrix}$, $x = \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix}$, $y = \\begin{pmatrix} 4 \\\\ 0 \\\\ 3 \\end{pmatrix}$.\n$$\nHx = \\begin{pmatrix} 1 & 0 \\\\ 1 & 1 \\\\ 0 & 2 \\end{pmatrix}\\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} (1)(2) + (0)(-1) \\\\ (1)(2) + (1)(-1) \\\\ (0)(2) + (2)(-1) \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 1 \\\\ -2 \\end{pmatrix}\n$$\n$$\nd = y - Hx = \\begin{pmatrix} 4 \\\\ 0 \\\\ 3 \\end{pmatrix} - \\begin{pmatrix} 2 \\\\ 1 \\\\ -2 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ -1 \\\\ 5 \\end{pmatrix}\n$$\nNext, we compute the whitened residual $\\tilde{d} = W d$:\n$$\n\\tilde{d} = \\begin{pmatrix} \\frac{1}{2} & 0 & 0 \\\\ -\\frac{1}{6} & \\frac{1}{3} & 0 \\\\ \\frac{1}{3} & -\\frac{2}{3} & 1 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ -1 \\\\ 5 \\end{pmatrix} = \\begin{pmatrix} (\\frac{1}{2})(2) + (0)(-1) + (0)(5) \\\\ (-\\frac{1}{6})(2) + (\\frac{1}{3})(-1) + (0)(5) \\\\ (\\frac{1}{3})(2) + (-\\frac{2}{3})(-1) + (1)(5) \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -\\frac{1}{3} - \\frac{1}{3} \\\\ \\frac{2}{3} + \\frac{2}{3} + 5 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -\\frac{2}{3} \\\\ \\frac{4}{3} + \\frac{15}{3} \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -\\frac{2}{3} \\\\ \\frac{19}{3} \\end{pmatrix}\n$$\nFinally, we evaluate the whitened data misfit $J = \\frac{1}{2}||\\tilde{d}||_2^2$:\n$$\nJ = \\frac{1}{2} \\left( (1)^2 + \\left(-\\frac{2}{3}\\right)^2 + \\left(\\frac{19}{3}\\right)^2 \\right) = \\frac{1}{2} \\left( 1 + \\frac{4}{9} + \\frac{361}{9} \\right)\n$$\n$$\nJ = \\frac{1}{2} \\left( \\frac{9}{9} + \\frac{4}{9} + \\frac{361}{9} \\right) = \\frac{1}{2} \\left( \\frac{9+4+361}{9} \\right) = \\frac{1}{2} \\left( \\frac{374}{9} \\right) = \\frac{187}{9}\n$$\nThe numerical value of the data misfit is $\\frac{187}{9}$. The problem asks for the whitening matrix $W$ as the final answer.", "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{1}{2} & 0 & 0 \\\\ -\\frac{1}{6} & \\frac{1}{3} & 0 \\\\ \\frac{1}{3} & -\\frac{2}{3} & 1 \\end{pmatrix}}\n$$", "id": "3365471"}, {"introduction": "While mathematically convenient, the Gaussian assumption is not universally valid, and its failure can severely degrade estimator performance, especially in the presence of outliers. This practice [@problem_id:3365464] explores a robust alternative by replacing the Gaussian likelihood with a heavy-tailed Student-$t$ distribution. Through a concrete numerical experiment, you will quantify the breakdown of the Gaussian-based estimator and implement a robust estimator that correctly handles outliers.", "problem": "Consider a linear inverse problem with a forward operator represented by a known matrix $H \\in \\mathbb{R}^{m \\times n}$, an unknown state vector $x \\in \\mathbb{R}^{n}$, and observed data $y \\in \\mathbb{R}^{m}$. The observation model is $y = H x + \\varepsilon$, where $\\varepsilon$ represents measurement noise. You are to analyze the Maximum A Posteriori (MAP) estimator under two noise-modeling assumptions: a mis-specified Gaussian likelihood and a correctly specified heavy-tailed Student-$t$ likelihood, and quantify the bias of each MAP estimate relative to a known ground truth.\n\nUse Bayes’ rule and the definitions of the Gaussian and Student-$t$ probability density functions (PDFs) as the fundamental base. Specifically:\n- The Gaussian PDF for a scalar with variance parameter $\\sigma^{2}$ is $p(r) \\propto \\exp\\left(-\\frac{r^{2}}{2 \\sigma^{2}}\\right)$.\n- The Student-$t$ PDF with degrees of freedom $\\nu$ and scale $\\sigma$ is $p(r) \\propto \\left(1 + \\frac{r^{2}}{\\nu \\sigma^{2}}\\right)^{-\\frac{\\nu + 1}{2}}$.\n\nAssume a Gaussian prior for $x$, namely $x \\sim \\mathcal{N}(m, P)$, where $m \\in \\mathbb{R}^{n}$ and $P \\in \\mathbb{R}^{n \\times n}$ is symmetric positive definite. Under the Gaussian likelihood, the MAP estimate minimizes a convex quadratic objective. Under the Student-$t$ likelihood, the MAP estimate minimizes a robust objective based on the sum of logarithmic terms. Derive the MAP estimators from first principles and design an algorithm that computes both MAP estimates for given $H$, $y$, $m$, $P$, $\\sigma$, and $\\nu$. Then, compare their biases relative to a known ground truth $x^{\\star}$.\n\nYou must write a complete, runnable program that:\n1. Constructs the following fixed problem instance:\n   - Dimensions: $m = 8$, $n = 2$.\n   - Forward matrix $H$ (rows listed explicitly):\n     - Row $1$: $\\left[1.0,\\,0.5\\right]$\n     - Row $2$: $\\left[0.8,\\,-0.3\\right]$\n     - Row $3$: $\\left[-0.6,\\,1.2\\right]$\n     - Row $4$: $\\left[0.0,\\,1.0\\right]$\n     - Row $5$: $\\left[1.5,\\,-0.7\\right]$\n     - Row $6$: $\\left[-1.0,\\,-0.2\\right]$\n     - Row $7$: $\\left[0.3,\\,0.8\\right]$\n     - Row $8$: $\\left[-0.4,\\,0.5\\right]$\n   - Ground truth state $x^{\\star} = \\left[1.0,\\,-2.0\\right]$.\n   - Prior mean $m = \\left[0.0,\\,0.0\\right]$.\n   - Prior covariance $P = \\mathrm{diag}\\left(10.0,\\,10.0\\right)$.\n   - Noise scale $\\sigma = 1.0$ (note: no physical units are involved).\n2. Forms the test suite of four cases by creating $y$ via $y = H x^{\\star} + r$ with fixed residual vectors $r$, and specifying $\\nu$ for the Student-$t$ likelihood:\n   - Case $1$ (heavy-tailed residuals, moderate degrees of freedom):\n     - $\\nu = 3$\n     - $r = \\left[0.2,\\,-0.1,\\,0.15,\\,-0.05,\\,0.1,\\,-0.1,\\,12.0,\\,-15.0\\right]$\n   - Case $2$ (near-Gaussian behavior):\n     - $\\nu = 30$\n     - $r = \\left[0.05,\\,-0.02,\\,0.03,\\,-0.04,\\,0.01,\\,-0.01,\\,0.02,\\,-0.03\\right]$\n   - Case $3$ (extremely heavy-tailed, multiple large outliers):\n     - $\\nu = 1.5$\n     - $r = \\left[20.0,\\,-25.0,\\,0.2,\\,0.0,\\,-0.1,\\,18.0,\\,-22.0,\\,0.3\\right]$\n   - Case $4$ (baseline with no residuals):\n     - $\\nu = 3$\n     - $r = \\left[0.0,\\,0.0,\\,0.0,\\,0.0,\\,0.0,\\,0.0,\\,0.0,\\,0.0\\right]$\n3. Computes, for each case:\n   - The Gaussian-likelihood MAP estimate $x_{\\mathrm{G}}$.\n   - The Student-$t$-likelihood MAP estimate $x_{\\mathrm{T}}$ using a robust algorithm derived from first principles and implemented via a convergent iterative scheme.\n   - The Euclidean norm of the bias under each assumption: $\\left\\|x_{\\mathrm{G}} - x^{\\star}\\right\\|_{2}$ and $\\left\\|x_{\\mathrm{T}} - x^{\\star}\\right\\|_{2}$.\n   - The relative improvement ratio $\\rho = \\frac{\\left\\|x_{\\mathrm{G}} - x^{\\star}\\right\\|_{2}}{\\left\\|x_{\\mathrm{T}} - x^{\\star}\\right\\|_{2}}$ (if the denominator is $0$, set $\\rho$ to $+\\infty$).\n   - The difference norm between the two estimates $\\left\\|x_{\\mathrm{G}} - x_{\\mathrm{T}}\\right\\|_{2}$.\n4. Produces a single line of output containing the results aggregated across all four cases as a comma-separated list enclosed in square brackets. Each case’s result must be a list of four floating-point numbers in the order:\n   - $\\left\\|x_{\\mathrm{G}} - x^{\\star}\\right\\|_{2}$,\n   - $\\left\\|x_{\\mathrm{T}} - x^{\\star}\\right\\|_{2}$,\n   - $\\rho$,\n   - $\\left\\|x_{\\mathrm{G}} - x_{\\mathrm{T}}\\right\\|_{2}$.\nFor example, the final output must have the format $\\left[\\left[a_{1},b_{1},c_{1},d_{1}\\right],\\left[a_{2},b_{2},c_{2},d_{2}\\right],\\left[a_{3},b_{3},c_{3},d_{3}\\right],\\left[a_{4},b_{4},c_{4},d_{4}\\right]\\right]$.\n\nDesign for coverage:\n- Case $1$ tests the effect of a few large outliers under moderate heavy-tailed modeling.\n- Case $2$ approximates the Gaussian regime.\n- Case $3$ tests extreme heavy tails with multiple large outliers.\n- Case $4$ is a boundary case with no residuals.\n\nAngles and physical units are not involved; report all quantities as pure floating-point numbers without units. Your implementation must be self-contained and must not read any input. The final program must deterministically compute the specified outputs for the given test suite.", "solution": "The posed problem requires the derivation and comparison of two Maximum A Posteriori (MAP) estimators for a linear inverse problem, based on different assumptions about the measurement noise statistics. The core of the task is to apply Bayes' rule to find the estimator that maximizes the posterior probability of the state vector $x$ given the measurements $y$.\n\nThe observation model is given by $y = Hx + \\varepsilon$, where $y \\in \\mathbb{R}^{m}$ are the observations, $x \\in \\mathbb{R}^{n}$ is the unknown state vector, $H \\in \\mathbb{R}^{m \\times n}$ is the forward operator, and $\\varepsilon \\in \\mathbb{R}^{m}$ is the measurement noise.\n\nAccording to Bayes' rule, the posterior probability density function (PDF) of $x$ given $y$ is:\n$$p(x|y) \\propto p(y|x) p(x)$$\nwhere $p(y|x)$ is the likelihood and $p(x)$ is the prior PDF for the state vector.\n\nThe MAP estimate, $x_{\\mathrm{MAP}}$, is the value of $x$ that maximizes this posterior probability. Maximizing $p(x|y)$ is equivalent to minimizing its negative logarithm. We define the objective function $J(x)$ as:\n$$J(x) = -\\ln(p(y|x)) - \\ln(p(x))$$\nThe MAP estimate is therefore $x_{\\mathrm{MAP}} = \\arg\\min_x J(x)$.\n\nThe problem specifies a Gaussian prior on $x$, such that $x \\sim \\mathcal{N}(m, P)$. The prior PDF is:\n$$p(x) \\propto \\exp\\left(-\\frac{1}{2}(x-m)^T P^{-1} (x-m)\\right)$$\nThe corresponding negative log-prior term in the objective function is:\n$$J_{\\mathrm{prior}}(x) = -\\ln(p(x)) = \\frac{1}{2}(x-m)^T P^{-1} (x-m) + \\mathrm{const.}$$\n\nThe likelihood term depends on the assumed distribution of the noise $\\varepsilon$. We assume the noise components $\\varepsilon_i = y_i - (Hx)_i$ are independent and identically distributed. The negative log-likelihood is then a sum over the individual components:\n$$J_{\\mathrm{likelihood}}(x) = -\\ln(p(y|x)) = -\\sum_{i=1}^{m} \\ln p(\\varepsilon_i) = -\\sum_{i=1}^{m} \\ln p(y_i - (Hx)_i)$$\n\nWe now derive the estimators for the two specified noise models.\n\n**1. Gaussian-Likelihood MAP Estimator ($x_{\\mathrm{G}}$)**\n\nUnder the assumption of Gaussian noise, each component $\\varepsilon_i$ is distributed according to $p(\\varepsilon_i) \\propto \\exp\\left(-\\frac{\\varepsilon_i^2}{2\\sigma^2}\\right)$. The negative log-likelihood term is:\n$$J_{\\mathrm{G-like}}(x) = \\sum_{i=1}^{m} \\frac{(y_i - (Hx)_i)^2}{2\\sigma^2} = \\frac{1}{2\\sigma^2}\\|y - Hx\\|_2^2$$\nThe total objective function to minimize for the Gaussian MAP estimate $x_{\\mathrm{G}}$ is:\n$$J_{\\mathrm{G}}(x) = \\frac{1}{2\\sigma^2}\\|y - Hx\\|_2^2 + \\frac{1}{2}(x-m)^T P^{-1} (x-m)$$\nThis is a quadratic function of $x$. Its minimum is found by setting its gradient with respect to $x$ to zero:\n$$\\nabla_x J_{\\mathrm{G}}(x) = \\frac{1}{\\sigma^2} H^T (Hx - y) + P^{-1}(x - m) = 0$$\nRearranging the terms to solve for $x$ gives:\n$$(H^T H + \\sigma^2 P^{-1}) x = H^T y + \\sigma^2 P^{-1} m$$\nThis is a linear system of equations. The unique solution is the Gaussian MAP estimate:\n$$x_{\\mathrm{G}} = (H^T H + \\sigma^2 P^{-1})^{-1} (H^T y + \\sigma^2 P^{-1} m)$$\nThis is a closed-form solution that can be computed directly.\n\n**2. Student-$t$-Likelihood MAP Estimator ($x_{\\mathrm{T}}$)**\n\nUnder the assumption of Student-$t$ distributed noise, each component $\\varepsilon_i$ has a PDF $p(\\varepsilon_i) \\propto \\left(1 + \\frac{\\varepsilon_i^2}{\\nu \\sigma^2}\\right)^{-(\\nu+1)/2}$. The negative log-likelihood term becomes:\n$$J_{\\mathrm{T-like}}(x) = \\sum_{i=1}^{m} \\frac{\\nu+1}{2} \\ln\\left(1 + \\frac{(y_i - (Hx)_i)^2}{\\nu \\sigma^2}\\right)$$\nThe total objective function for the Student-$t$ MAP estimate $x_{\\mathrm{T}}$ is:\n$$J_{\\mathrm{T}}(x) = \\sum_{i=1}^{m} \\frac{\\nu+1}{2} \\ln\\left(1 + \\frac{(y_i - (Hx)_i)^2}{\\nu \\sigma^2}\\right) + \\frac{1}{2}(x-m)^T P^{-1} (x-m)$$\nThis objective function is convex but non-quadratic, so an iterative method is required to find its minimum. We again set the gradient to zero:\n$$\\nabla_x J_{\\mathrm{T}}(x) = -\\sum_{i=1}^{m} \\left(\\frac{\\nu+1}{\\nu\\sigma^2 + (y_i - (Hx)_i)^2}\\right) (y_i - (Hx)_i) H_i^T + P^{-1}(x-m) = 0$$\nwhere $H_i$ is the $i$-th row of $H$. Let us define a set of weights $w_i(x)$ that depend on the current estimate of $x$:\n$$w_i(x) = \\frac{\\nu+1}{\\nu\\sigma^2 + (y_i - (Hx)_i)^2}$$\nThese weights represent the influence of each measurement. Outliers with large residuals $|y_i - (Hx)_i|$ will receive a small weight, which gives the estimator its robustness.\nWith these weights, the gradient condition can be rewritten in matrix form, where $W(x)$ is a diagonal matrix with entries $w_i(x)$:\n$$H^T W(x) (Hx - y) + P^{-1}(x - m) = 0$$\n$$(H^T W(x) H + P^{-1}) x = H^T W(x) y + P^{-1} m$$\nThis equation's structure suggests an Iteratively Reweighted Least Squares (IRLS) algorithm. Starting with an initial guess $x^{(0)}$, we can iterate as follows for $k=0, 1, 2, \\dots$:\n1.  Compute the residuals: $r^{(k)} = y - Hx^{(k)}$.\n2.  Compute the weights: $W^{(k)} = \\mathrm{diag}\\left(\\frac{\\nu+1}{\\nu\\sigma^2 + (r_i^{(k)})^2}\\right)$.\n3.  Solve the linear system for the next estimate $x^{(k+1)}$:\n    $$x^{(k+1)} = (H^T W^{(k)} H + P^{-1})^{-1} (H^T W^{(k)} y + P^{-1} m)$$\nThis iteration is repeated until convergence, i.e., until the change $\\|x^{(k+1)} - x^{(k)}\\|_2$ is below a small tolerance. A suitable initial guess is the Gaussian MAP estimate, $x^{(0)} = x_{\\mathrm{G}}$.\n\nThe final program implements these two estimators and computes the required comparison metrics for the specified test cases, demonstrating the superior performance of the robust Student-$t$ model in the presence of heavy-tailed noise (outliers).", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes and compares MAP estimates under Gaussian and Student-t likelihoods\n    for a linear inverse problem across four test cases.\n    \"\"\"\n    # 1. Construct the fixed problem instance\n    M, N = 8, 2\n    H = np.array([\n        [1.0, 0.5],\n        [0.8, -0.3],\n        [-0.6, 1.2],\n        [0.0, 1.0],\n        [1.5, -0.7],\n        [-1.0, -0.2],\n        [0.3, 0.8],\n        [-0.4, 0.5]\n    ])\n    x_star = np.array([1.0, -2.0])\n    m_prior = np.array([0.0, 0.0])\n    P_prior = np.diag([10.0, 10.0])\n    sigma = 1.0\n\n    # Pre-compute the inverse of the prior covariance matrix\n    P_inv = np.linalg.inv(P_prior)\n    H_T = H.T\n\n    # 2. Define the test suite\n    test_cases = [\n        # Case 1: Heavy-tailed residuals, moderate nu\n        {'nu': 3.0, 'r': np.array([0.2, -0.1, 0.15, -0.05, 0.1, -0.1, 12.0, -15.0])},\n        # Case 2: Near-Gaussian behavior\n        {'nu': 30.0, 'r': np.array([0.05, -0.02, 0.03, -0.04, 0.01, -0.01, 0.02, -0.03])},\n        # Case 3: Extremely heavy-tailed, multiple large outliers\n        {'nu': 1.5, 'r': np.array([20.0, -25.0, 0.2, 0.0, -0.1, 18.0, -22.0, 0.3])},\n        # Case 4: Baseline with no residuals\n        {'nu': 3.0, 'r': np.array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])}\n    ]\n\n    all_case_results = []\n\n    # 3. Compute results for each case\n    for case in test_cases:\n        nu = case['nu']\n        r = case['r']\n        \n        # Form the observation vector y\n        y = H @ x_star + r\n\n        # Compute the Gaussian-likelihood MAP estimate (x_G)\n        A_G = H_T @ H + (sigma**2) * P_inv\n        b_G = H_T @ y + (sigma**2) * P_inv @ m_prior\n        x_G = np.linalg.solve(A_G, b_G)\n\n        # Compute the Student-t-likelihood MAP estimate (x_T) using IRLS\n        # Initialize the iteration with the Gaussian estimate\n        x_T = np.copy(x_G)\n        num_iterations = 50  # Sufficient for convergence in this problem\n        \n        for _ in range(num_iterations):\n            residuals = y - H @ x_T\n            \n            # Calculate weights for the current estimate\n            weights_diag = (nu + 1) / (nu * sigma**2 + residuals**2)\n            W = np.diag(weights_diag)\n            \n            # Form and solve the linear system for the next estimate\n            A_T = H_T @ W @ H + P_inv\n            b_T = H_T @ W @ y + P_inv @ m_prior\n            \n            x_T_new = np.linalg.solve(A_T, b_T)\n            \n            # Check for convergence\n            if np.linalg.norm(x_T_new - x_T) < 1e-12:\n                x_T = x_T_new\n                break\n            \n            x_T = x_T_new\n        \n        # Calculate the required metrics\n        bias_G_norm = np.linalg.norm(x_G - x_star)\n        bias_T_norm = np.linalg.norm(x_T - x_star)\n        \n        # The prior ensures the denominator is non-zero, but handle for robustness\n        if bias_T_norm == 0.0:\n            rho = float('inf')\n        else:\n            rho = bias_G_norm / bias_T_norm\n            \n        diff_norm = np.linalg.norm(x_G - x_T)\n        \n        single_case_result = [bias_G_norm, bias_T_norm, rho, diff_norm]\n        all_case_results.append(single_case_result)\n\n    # Format and print the final output string\n    # E.g., [[a1,b1,c1,d1],[a2,b2,c2,d2]] with no spaces\n    output_str = ','.join(str(res).replace(' ', '') for res in all_case_results)\n    print(f\"[{output_str}]\")\n\nsolve()\n```", "id": "3365464"}]}