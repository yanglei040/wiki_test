{"hands_on_practices": [{"introduction": "The EKF's prediction step often involves discretizing a continuous-time dynamical system. This practice explores a crucial limitation stemming from this process: the error introduced by the common first-order Euler approximation of the system dynamics. By comparing the EKF's covariance propagation to a high-accuracy reference solution, you will empirically measure how this error scales with the time step $\\Delta t$, providing insight into how nonlinearity and step size interact to degrade filter performance [@problem_id:3397742].", "problem": "Consider a continuous–time nonlinear dynamical system defined by the ordinary differential equation $x'(t) = f(x(t))$ on $\\mathbb{R}^n$, where $f$ is sufficiently smooth. In the Extended Kalman Filter (EKF), the time-update step in a continuous–discrete setting linearizes the dynamics about the predicted trajectory and propagates the error covariance under a white-noise model. The continuous-time covariance propagation is governed by the matrix differential equation $P'(t) = J(x(t)) P(t) + P(t) J(x(t))^\\top + Q_c$, where $J(x) = \\frac{\\partial f}{\\partial x}(x)$ is the Jacobian of $f$ and $Q_c$ is the spectral density of the process noise. When implementing a discrete-time EKF with time step $\\Delta t$, the commonly used first-order discretization approximates the discrete transition matrix by $F \\approx I + \\Delta t \\, J(x_k)$ and the discrete process covariance by $Q_d \\approx \\Delta t \\, Q_c$, where $I$ is the identity matrix, $x_k$ is the state at the beginning of the step, and $P_{k+1} \\approx F P_k F^\\top + Q_d$.\n\nYour task is to empirically investigate how the EKF's linearization and discretization errors in the covariance propagation depend on the step size $\\Delta t$. Use the following setup:\n\n- Dimension $n = 2$.\n- Dynamics $f(x) = \\begin{bmatrix} x_2 \\\\ -\\omega^2 x_1 + \\gamma x_1^3 \\end{bmatrix}$ with parameters $\\omega > 0$ and $\\gamma \\ge 0$.\n- Jacobian $J(x) = \\begin{bmatrix} 0  1 \\\\ -\\omega^2 + 3 \\gamma x_1^2  0 \\end{bmatrix}$.\n- Continuous-time covariance dynamics $P'(t) = J(x(t)) P(t) + P(t) J(x(t))^\\top + Q_c$ with constant $Q_c$ and $P(0) = P_0$.\n\nFor each test case, perform the following steps for a set of step sizes $\\Delta t$:\n\n1. Compute a reference covariance $P_{\\text{ref}}(\\Delta t)$ by numerically integrating the coupled system for the state $x(t)$ and the covariance $P(t)$ from $t=0$ to $t=\\Delta t$ using a high-accuracy fourth-order Runge–Kutta scheme with sufficiently fine substeps to approximate the continuous-time covariance propagation accurately.\n2. Compute the first-order EKF discrete-time approximation $P_{\\text{eul}}(\\Delta t) = F P_0 F^\\top + Q_d$, where $F = I + \\Delta t \\, J(x_0)$ and $Q_d = \\Delta t \\, Q_c$.\n3. Measure the covariance propagation error $E(\\Delta t) = \\| P_{\\text{eul}}(\\Delta t) - P_{\\text{ref}}(\\Delta t) \\|_F$, where $\\| \\cdot \\|_F$ denotes the Frobenius norm.\n4. Estimate the scaling exponent $n$ by fitting a straight line to $\\log(E(\\Delta t))$ versus $\\log(\\Delta t)$ via least squares and taking the slope as the estimate of $n$.\n\nUse the following test suite of parameter values to assess different regimes and boundary conditions:\n\n- Test Case 1 (happy path, linear dynamics): $\\omega = 1.5$, $\\gamma = 0$, $x_0 = \\begin{bmatrix} 1.0 \\\\ 0.0 \\end{bmatrix}$, $P_0 = \\begin{bmatrix} 0.2  0.0 \\\\ 0.0  0.2 \\end{bmatrix}$, $Q_c = \\begin{bmatrix} 0.05  0.0 \\\\ 0.0  0.05 \\end{bmatrix}$, step sizes $\\Delta t \\in \\{ 0.5, 0.25, 0.125, 0.0625, 0.03125 \\}$.\n- Test Case 2 (moderate nonlinearity): $\\omega = 1.5$, $\\gamma = 0.1$, $x_0 = \\begin{bmatrix} 0.7 \\\\ -0.3 \\end{bmatrix}$, $P_0 = \\begin{bmatrix} 0.05  0.0 \\\\ 0.0  0.08 \\end{bmatrix}$, $Q_c = \\begin{bmatrix} 0.05  0.0 \\\\ 0.0  0.05 \\end{bmatrix}$, step sizes $\\Delta t \\in \\{ 0.5, 0.25, 0.125, 0.0625, 0.03125 \\}$.\n- Test Case 3 (stronger nonlinearity): $\\omega = 1.5$, $\\gamma = 1.0$, $x_0 = \\begin{bmatrix} 0.4 \\\\ 0.4 \\end{bmatrix}$, $P_0 = \\begin{bmatrix} 0.5  0.0 \\\\ 0.0  0.1 \\end{bmatrix}$, $Q_c = \\begin{bmatrix} 0.05  0.0 \\\\ 0.0  0.05 \\end{bmatrix}$, step sizes $\\Delta t \\in \\{ 0.5, 0.25, 0.125, 0.0625, 0.03125 \\}$.\n\nYour program must implement the numerical integration and estimation described above and produce a single float per test case: the estimated scaling exponent $n$. The final output format must be a single line containing a comma-separated list of these three floats enclosed in square brackets, with no additional characters or spaces, for example, $[n_1,n_2,n_3]$.\n\nNo physical units are involved; all variables are dimensionless. Angles do not appear. Express all computed quantities as real numbers. The program must be self-contained and must not require any input.", "solution": "The user requires an empirical investigation into the scaling of the linearization and discretization errors in the Extended Kalman Filter (EKF) covariance propagation step with respect to the time step size $\\Delta t$. The problem is well-posed and scientifically grounded in the fields of numerical analysis and state estimation.\n\n### Theoretical Error Analysis\n\nThe task is to determine the exponent $n$ in the relationship $E(\\Delta t) \\propto (\\Delta t)^n$, where $E(\\Delta t)$ is the error in the covariance matrix after one time step. This error is defined as the Frobenius norm of the difference between a high-accuracy reference solution, $P_{\\text{ref}}(\\Delta t)$, and the first-order EKF approximation, $P_{\\text{eul}}(\\Delta t)$.\n\nThe continuous-time covariance propagation is governed by the matrix differential equation:\n$$P'(t) = J(x(t)) P(t) + P(t) J(x(t))^\\top + Q_c$$\nThe solution to this ODE over a small interval $[0, \\Delta t]$ can be expressed via a Taylor series expansion around $t=0$:\n$$P(\\Delta t) = P(0) + \\Delta t P'(0) + \\frac{(\\Delta t)^2}{2} P''(0) + O((\\Delta t)^3)$$\nUsing the initial conditions $P(0) = P_0$ and $x(0) = x_0$, the first derivative at $t=0$ is $P'(0) = J(x_0) P_0 + P_0 J(x_0)^\\top + Q_c$. Substituting this gives:\n$$P(\\Delta t) = P_0 + \\Delta t (J(x_0) P_0 + P_0 J(x_0)^\\top + Q_c) + \\frac{(\\Delta t)^2}{2} P''(0) + O((\\Delta t)^3)$$\n\nThe first-order EKF discrete-time approximation is defined as:\n$$P_{\\text{eul}}(\\Delta t) = F P_0 F^\\top + Q_d$$\nwhere the state transition matrix is approximated as $F = I + \\Delta t J(x_0)$ and the discrete process noise covariance is $Q_d = \\Delta t Q_c$. Substituting these expressions yields:\n$$P_{\\text{eul}}(\\Delta t) = (I + \\Delta t J(x_0)) P_0 (I + \\Delta t J(x_0))^\\top + \\Delta t Q_c$$\nLetting $J_0 = J(x_0)$, we can expand the expression:\n$$P_{\\text{eul}}(\\Delta t) = (I + \\Delta t J_0) P_0 (I + \\Delta t J_0^\\top) + \\Delta t Q_c$$\n$$P_{\\text{eul}}(\\Delta t) = P_0 + \\Delta t J_0 P_0 + \\Delta t P_0 J_0^\\top + (\\Delta t)^2 J_0 P_0 J_0^\\top + \\Delta t Q_c$$\nGrouping terms by powers of $\\Delta t$:\n$$P_{\\text{eul}}(\\Delta t) = P_0 + \\Delta t (J_0 P_0 + P_0 J_0^\\top + Q_c) + (\\Delta t)^2 J_0 P_0 J_0^\\top$$\nThis can be written in terms of $P'(0)$ as:\n$$P_{\\text{eul}}(\\Delta t) = P_0 + \\Delta t P'(0) + (\\Delta t)^2 J_0 P_0 J_0^\\top$$\n\nThe error matrix is the difference between the approximation and the true solution, $P_{\\text{eul}}(\\Delta t) - P(\\Delta t)$. For small $\\Delta t$, we use the Taylor series for $P(\\Delta t)$:\n$$\n\\begin{align*}\nP_{\\text{eul}}(\\Delta t) - P(\\Delta t) = \\left( P_0 + \\Delta t P'(0) + (\\Delta t)^2 J_0 P_0 J_0^\\top \\right) \\\\\n\\quad - \\left( P_0 + \\Delta t P'(0) + \\frac{(\\Delta t)^2}{2} P''(0) + O((\\Delta t)^3) \\right) \\\\\n= (\\Delta t)^2 \\left( J_0 P_0 J_0^\\top - \\frac{1}{2} P''(0) \\right) + O((\\Delta t)^3)\n\\end{align*}\n$$\nThe dominant term in the error matrix is of order $(\\Delta t)^2$. Consequently, the Frobenius norm of this error, $E(\\Delta t) = \\| P_{\\text{eul}}(\\Delta t) - P_{\\text{ref}}(\\Delta t) \\|_F$, is expected to scale as $(\\Delta t)^2$. This implies that the scaling exponent $n$ should be approximately $2$. This is the expected result for a method with a local truncation error of order $(\\Delta t)^2$, which is characteristic of first-order integration schemes like the forward Euler method used implicitly in the EKF formulation.\n\nTo find $n$ empirically, we assume $E(\\Delta t) = C(\\Delta t)^n$ for some constant $C$. Taking the natural logarithm of both sides gives:\n$$\\log(E(\\Delta t)) = n \\log(\\Delta t) + \\log(C)$$\nThis is a linear relationship between $\\log(E)$ and $\\log(\\Delta t)$, where the slope is the desired exponent $n$.\n\n### Numerical Implementation Strategy\n\nA Python program will be implemented to perform the required numerical experiment for each test case.\n\n1.  **Reference Solution Computation**:\n    A fourth-order Runge-Kutta (RK4) integrator is implemented to solve the coupled system of ordinary differential equations for the state $x(t) \\in \\mathbb{R}^2$ and the covariance matrix $P(t) \\in \\mathbb{R}^{2 \\times 2}$. The system state is a $6$-dimensional vector $Y(t) = [x_1(t), x_2(t), P_{11}(t), P_{12}(t), P_{21}(t), P_{22}(t)]^\\top$. The derivative function $Y'(t) = g(Y(t))$ is defined by combining the state dynamics $x'(t) = f(x(t))$ and the covariance dynamics $P'(t) = J(x(t))P(t) + P(t)J(x(t))^\\top + Q_c$.\n    To ensure accuracy, the integration from $t=0$ to $t=\\Delta t$ is performed using a large number of substeps (e.g., $N=1000$), so the substep size $h = \\Delta t/N$ is very small. The final covariance matrix from this integration constitutes $P_{\\text{ref}}(\\Delta t)$.\n\n2.  **First-Order Approximation**:\n    The EKF's first-order approximation, $P_{\\text{eul}}(\\Delta t)$, is calculated directly from its defining formula using the initial conditions $x_0$, $P_0$, and the fixed parameters for each case.\n\n3.  **Error Analysis and Exponent Estimation**:\n    For each $\\Delta t$ provided, the error $E(\\Delta t) = \\| P_{\\text{eul}}(\\Delta t) - P_{\\text{ref}}(\\Delta t) \\|_F$ is calculated. The pairs of $(\\log(\\Delta t), \\log(E(\\Delta t)))$ are collected. Finally, `numpy.polyfit` is used to perform a linear least-squares regression on these log-transformed data points. The slope of the resulting line is the estimated scaling exponent $n$.\n\nThis procedure is applied to each of the three specified test cases, and the resulting exponents are collected for the final output.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the EKF error scaling analysis for all test cases.\n    \"\"\"\n\n    # Test Case 1 (happy path, linear dynamics)\n    case1 = {\n        \"omega\": 1.5, \"gamma\": 0.0,\n        \"x0\": np.array([1.0, 0.0]),\n        \"P0\": np.array([[0.2, 0.0], [0.0, 0.2]]),\n        \"Qc\": np.array([[0.05, 0.0], [0.0, 0.05]]),\n        \"delta_ts\": np.array([0.5, 0.25, 0.125, 0.0625, 0.03125])\n    }\n\n    # Test Case 2 (moderate nonlinearity)\n    case2 = {\n        \"omega\": 1.5, \"gamma\": 0.1,\n        \"x0\": np.array([0.7, -0.3]),\n        \"P0\": np.array([[0.05, 0.0], [0.0, 0.08]]),\n        \"Qc\": np.array([[0.05, 0.0], [0.0, 0.05]]),\n        \"delta_ts\": np.array([0.5, 0.25, 0.125, 0.0625, 0.03125])\n    }\n\n    # Test Case 3 (stronger nonlinearity)\n    case3 = {\n        \"omega\": 1.5, \"gamma\": 1.0,\n        \"x0\": np.array([0.4, 0.4]),\n        \"P0\": np.array([[0.5, 0.0], [0.0, 0.1]]),\n        \"Qc\": np.array([[0.05, 0.0], [0.0, 0.05]]),\n        \"delta_ts\": np.array([0.5, 0.25, 0.125, 0.0625, 0.03125])\n    }\n\n    test_cases = [case1, case2, case3]\n    results = []\n\n    for case in test_cases:\n        exponent = _compute_scaling_exponent(**case)\n        results.append(exponent)\n\n    # Format the output as specified: a list of floats, no spaces.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef _compute_scaling_exponent(omega, gamma, x0, P0, Qc, delta_ts):\n    \"\"\"\n    Computes the error scaling exponent for a single test case.\n    \"\"\"\n    log_delta_ts = []\n    log_errors = []\n    \n    # Number of substeps for high-accuracy RK4 integration.\n    # Chosen to be sufficiently large for the reference solution to converge.\n    num_substeps = 1000\n\n    def dYdt(Y, omega_val, gamma_val, Qc_val):\n        \"\"\"\n        Computes the derivative of the combined state vector [x, vec(P)].\n        Y: 6x1 state vector [x1, x2, p11, p12, p21, p22]\n        \"\"\"\n        x = Y[0:2]\n        P = Y[2:].reshape((2, 2))\n\n        # Nonlinear dynamics f(x)\n        dx_dt = np.array([x[1], -omega_val**2 * x[0] + gamma_val * x[0]**3])\n        \n        # Jacobian J(x)\n        J = np.array([[0.0, 1.0],\n                      [-omega_val**2 + 3 * gamma_val * x[0]**2, 0.0]])\n        \n        # Covariance differential equation (Lyapunov equation)\n        dP_dt = J @ P + P @ J.T + Qc_val\n        \n        return np.concatenate((dx_dt, dP_dt.flatten()))\n\n    for dt in delta_ts:\n        # 1. Compute reference covariance P_ref(dt) using RK4\n        substep_h = dt / num_substeps\n        \n        # Initial combined state: Y(0) = [x0, vec(P0)]\n        Y_current = np.concatenate((x0, P0.flatten()))\n        \n        for _ in range(num_substeps):\n            k1 = substep_h * dYdt(Y_current, omega, gamma, Qc)\n            k2 = substep_h * dYdt(Y_current + 0.5 * k1, omega, gamma, Qc)\n            k3 = substep_h * dYdt(Y_current + 0.5 * k2, omega, gamma, Qc)\n            k4 = substep_h * dYdt(Y_current + k3, omega, gamma, Qc)\n            Y_current += (k1 + 2*k2 + 2*k3 + k4) / 6.0\n        \n        P_ref = Y_current[2:].reshape((2, 2))\n\n        # 2. Compute first-order EKF approximation P_eul(dt)\n        J0 = np.array([[0.0, 1.0], [-omega**2 + 3 * gamma * x0[0]**2, 0.0]])\n        F = np.eye(2) + dt * J0\n        Qd = dt * Qc\n        P_eul = F @ P0 @ F.T + Qd\n        \n        # 3. Measure the covariance propagation error\n        error = np.linalg.norm(P_eul - P_ref, 'fro')\n        \n        # 4. Store log-transformed values for regression\n        log_delta_ts.append(np.log(dt))\n        log_errors.append(np.log(error))\n        \n    # 5. Estimate the scaling exponent n via least squares\n    # np.polyfit returns [slope, intercept] for degree 1\n    slope, _ = np.polyfit(log_delta_ts, log_errors, 1)\n    \n    return slope\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3397742"}, {"introduction": "A core assumption of the EKF is that the state distribution can be adequately approximated by a Gaussian at every step. This exercise demonstrates how this assumption can fail during the measurement update when the observation model $h(x)$ is nonlinear. You will quantify the bias in the EKF's estimate by comparing it to the exact Bayesian posterior mean, revealing how model curvature can lead to systematic errors even when the filter appears to be perfectly tracking the system [@problem_id:3397756].", "problem": "Consider a single-step data assimilation problem with a scalar latent state $x \\in \\mathbb{R}$. The prior distribution is Gaussian $x \\sim \\mathcal{N}(\\mu, P)$ with mean $\\mu$ and variance $P$. The observation model is $y = h(x) + v$ where $h:\\mathbb{R} \\to \\mathbb{R}$ is a twice-differentiable nonlinear function and $v \\sim \\mathcal{N}(0, R)$ is zero-mean Gaussian measurement noise with variance $R$. The Extended Kalman Filter (EKF) proceeds by linearizing the observation function $h(x)$ at the current estimate and applying the linear Kalman filter update. In contrast, the exact Bayesian posterior mean is given by integrating against the exact likelihood and prior via Bayes' rule.\n\nStarting from the foundational definitions of a Gaussian prior, a Gaussian noise model, Bayes' rule for the posterior, and first-order Taylor linearization, your task is to:\n\n- Implement the EKF measurement update for a single assimilation step using a linearization of $h(x)$ at $x = \\mu$.\n- Compute the exact Bayesian posterior mean $m^\\star(y)$ defined by\n$$\nm^\\star(y) \\equiv \\frac{\\int_{-\\infty}^{\\infty} x \\, \\exp\\!\\left(-\\frac{(x-\\mu)^2}{2P}\\right) \\, \\exp\\!\\left(-\\frac{(y - h(x))^2}{2R}\\right) \\, dx}{\\int_{-\\infty}^{\\infty} \\exp\\!\\left(-\\frac{(x-\\mu)^2}{2P}\\right) \\, \\exp\\!\\left(-\\frac{(y - h(x))^2}{2R}\\right) \\, dx},\n$$\nwhich is the Gaussian-prior-weighted expectation of the state under the exact nonlinear likelihood, omitting normalization constants that cancel between numerator and denominator.\n- Quantify the bias in the EKF posterior mean estimate as\n$$\nb \\equiv m^\\star(y) - m^{\\text{EKF}}(y),\n$$\nwhere $m^{\\text{EKF}}(y)$ is the EKF posterior mean after the measurement update. The bias $b$ is a direct measure of the limitation of the EKF due to linearization and model curvature.\n\nFor all tests below, use the convention that angles are measured in radians. To isolate the effect of nonlinearity on bias, set the realized measurement equal to the observation function evaluated at the prior mean, i.e., $y = h(\\mu)$, in each test case. Under this choice, the EKF innovation will be zero by construction, but the exact Bayesian posterior mean may shift due to the curvature of $h(x)$, revealing EKF bias.\n\nImplement a program that computes the bias $b$ for each of the following test cases:\n\n- Test Case 1 (linear observation, \"happy path\"): $h(x) = a x + b$ with $a = 1.3$, $b = -0.2$, prior mean $\\mu = 0.5$, variance $P = 0.09$, noise variance $R = 0.04$, and measurement $y = h(\\mu)$.\n- Test Case 2 (quadratic observation): $h(x) = x^2$, prior mean $\\mu = 0.5$, variance $P = 0.2$, noise variance $R = 0.01$, and measurement $y = h(\\mu)$.\n- Test Case 3 (sinusoidal observation): $h(x) = \\sin(x)$, prior mean $\\mu = 0.3$, variance $P = 0.15$, noise variance $R = 0.02$, and measurement $y = h(\\mu)$. Angles are in radians.\n- Test Case 4 (saturating observation): $h(x) = \\tanh(x)$, prior mean $\\mu = 0.8$, variance $P = 0.3$, noise variance $R = 0.02$, and measurement $y = h(\\mu)$.\n- Test Case 5 (strong nonlinearity and large prior variance): $h(x) = x^2$, prior mean $\\mu = 1.0$, variance $P = 1.0$, noise variance $R = 0.1$, and measurement $y = h(\\mu)$.\n\nFor each test case, compute the exact Bayesian posterior mean $m^\\star(y)$ via numerical integration over $x \\in (-\\infty, \\infty)$ as specified above, compute the EKF posterior mean $m^{\\text{EKF}}(y)$ using a linearization of $h(x)$ at $x=\\mu$ and a standard linear Kalman update, and then report the bias $b$ as a floating-point number. Your program should produce a single line of output containing the biases for the five test cases as a comma-separated list enclosed in square brackets. Express each bias as a decimal number and round to six decimal places in the final output. The final output format must be exactly\n$$\n[\\text{bias}_1,\\text{bias}_2,\\text{bias}_3,\\text{bias}_4,\\text{bias}_5].\n$$", "solution": "The problem is assessed to be valid. It is a well-posed and scientifically grounded exercise in nonlinear state estimation, asking for a quantitative comparison between the Extended Kalman Filter (EKF) and the exact Bayesian posterior. All necessary functions, parameters, and definitions are provided, and there are no contradictions or factual errors. The tautological statement \"EKF, which stands for Extended Kalman Filter (EKF)\" is a minor linguistic flaw but has no bearing on the problem's mathematical or scientific validity.\n\nThe task is to compute the bias of the EKF posterior mean, defined as $b \\equiv m^\\star(y) - m^{\\text{EKF}}(y)$, for a scalar system under a specific measurement condition. Here, $m^{\\text{EKF}}(y)$ is the posterior mean from the EKF update, and $m^\\star(y)$ is the exact Bayesian posterior mean.\n\n### 1. Derivation of the EKF Posterior Mean $m^{\\text{EKF}}(y)$\n\nThe EKF approximates a nonlinear system with a linear one by performing a first-order Taylor series expansion of the nonlinear functions around the current state estimate. For the measurement update, the observation function $h(x)$ is linearized around the prior mean, $x=\\mu$.\n\nThe state and its uncertainty before the measurement are given by the prior distribution, $x \\sim \\mathcal{N}(\\mu, P)$. Thus, the prior estimate is $\\mu$ with variance $P$.\n\nThe first-order Taylor expansion of $h(x)$ around $x=\\mu$ is:\n$$\nh(x) \\approx h(\\mu) + H(x - \\mu)\n$$\nwhere $H$ is the Jacobian of $h(x)$ evaluated at $x=\\mu$. Since $x$ is a scalar, $H$ is the derivative:\n$$\nH = \\left. \\frac{dh}{dx} \\right|_{x=\\mu}\n$$\nThe observation model $y = h(x) + v$ is then approximated as:\n$$\ny \\approx h(\\mu) + H(x - \\mu) + v\n$$\nThis is a linear observation model for the state $x$ with respect to the \"linearized\" measurement $y' = y - h(\\mu) + H\\mu$. More directly, we can apply the standard linear Kalman update equations.\n\nThe key components of the Kalman filter measurement update are:\n1.  **Innovation** (or measurement residual): $\\nu = y - \\hat{y}$, where $\\hat{y}$ is the predicted measurement. For the EKF, the predicted measurement is the observation function evaluated at the prior mean, so $\\hat{y} = h(\\mu)$.\n    $$\n    \\nu = y - h(\\mu)\n    $$\n2.  **Innovation Covariance**: $S = H P H^T + R$. For our scalar case, this is:\n    $$\n    S = H^2 P + R\n    $$\n3.  **Kalman Gain**: $K = P H^T S^{-1}$. In the scalar case:\n    $$\n    K = \\frac{PH}{H^2 P + R}\n    $$\n4.  **Updated State Mean**: The posterior mean $m^{\\text{EKF}}(y)$ is given by the prior mean plus the innovation corrected by the Kalman gain.\n    $$\n    m^{\\text{EKF}}(y) = \\mu + K \\nu = \\mu + K (y - h(\\mu))\n    $$\n\nThe problem specifies a crucial condition: the measurement $y$ is set to the value of the observation function at the prior mean, i.e., $y = h(\\mu)$. Substituting this into the innovation equation gives:\n$$\n\\nu = h(\\mu) - h(\\mu) = 0\n$$\nConsequently, the EKF state update becomes:\n$$\nm^{\\text{EKF}}(y) = \\mu + K \\cdot 0 = \\mu\n$$\nThis result is fundamental to the problem's design. With the measurement set to $h(\\mu)$, the EKF posterior mean is identical to the prior mean, as the linearized model sees no new information. Any deviation of the true posterior mean from $\\mu$ is therefore a direct measure of the error introduced by the linearization.\n\n### 2. The Exact Bayesian Posterior Mean $m^\\star(y)$\n\nBayes' rule states that the posterior probability density function $p(x|y)$ is proportional to the product of the likelihood $p(y|x)$ and the prior $p(x)$:\n$$\np(x|y) \\propto p(y|x) p(x)\n$$\nGiven the problem definitions:\n-   The prior is Gaussian: $p(x) = \\mathcal{N}(x; \\mu, P) \\propto \\exp\\left(-\\frac{(x - \\mu)^2}{2P}\\right)$.\n-   The likelihood, from the noise model $v \\sim \\mathcal{N}(0, R)$, is also Gaussian: $p(y|x) = \\mathcal{N}(y; h(x), R) \\propto \\exp\\left(-\\frac{(y - h(x))^2}{2R}\\right)$.\n\nCombining these gives the unnormalized posterior:\n$$\np(x|y) \\propto \\exp\\left(-\\frac{(x-\\mu)^2}{2P} - \\frac{(y-h(x))^2}{2R}\\right)\n$$\nThe exact posterior mean $m^\\star(y)$ is the expected value of $x$ with respect to this posterior distribution:\n$$\nm^\\star(y) = \\mathbb{E}[x|y] = \\frac{\\int_{-\\infty}^{\\infty} x p(x|y) dx}{\\int_{-\\infty}^{\\infty} p(x|y) dx}\n$$\nSubstituting the expression for the unnormalized posterior yields the formula given in the problem statement:\n$$\nm^\\star(y) = \\frac{\\int_{-\\infty}^{\\infty} x \\, \\exp\\!\\left(-\\frac{(x-\\mu)^2}{2P}\\right) \\, \\exp\\!\\left(-\\frac{(y - h(x))^2}{2R}\\right) \\, dx}{\\int_{-\\infty}^{\\infty} \\exp\\!\\left(-\\frac{(x-\\mu)^2}{2P}\\right) \\, \\exp\\!\\left(-\\frac{(y - h(x))^2}{2R}\\right) \\, dx}\n$$\nWhen $h(x)$ is nonlinear, these integrals generally do not have a closed-form solution and must be computed numerically.\n\n### 3. Bias Calculation\n\nThe bias $b$ is the difference between the exact and the EKF-approximated posterior means:\n$$\nb = m^\\star(y) - m^{\\text{EKF}}(y)\n$$\nUsing our derived results, where $m^{\\text{EKF}}(y) = \\mu$:\n$$\nb = m^\\star(y) - \\mu\n$$\nThis bias directly quantifies the error due to the EKF's linearization, isolated from the effects of a random measurement innovation. The bias will be non-zero if the posterior distribution $p(x|y)$ becomes skewed due to the nonlinearity (curvature) of $h(x)$.\n\nFor the linear case, $h(x) = ax+b$, the posterior exponent is a quadratic function of $x$. This means the posterior $p(x|y)$ is exactly Gaussian. The mean of a Gaussian distribution is its mode (the point of maximum probability). In this case, the mean `m*(y)` will be identical to the Kalman filter mean, and thus the bias will be $0$. For nonlinear $h(x)$, the posterior is non-Gaussian, and a bias is expected.\n\n### 4. Computational Procedure for Each Test Case\n\nFor each test case with parameters $(\\mu, P, R)$ and function $h(x)$:\n1.  Set the measurement $y = h(\\mu)$.\n2.  The EKF posterior mean is $m^{\\text{EKF}}(y) = \\mu$.\n3.  Define the integrand for the numerator of $m^\\star(y)$:\n    $$\n    \\text{integrand}_{\\text{num}}(x) = x \\exp\\left(-\\frac{(x-\\mu)^2}{2P} - \\frac{(h(\\mu)-h(x))^2}{2R}\\right)\n    $$\n4.  Define the integrand for the denominator of $m^\\star(y)$:\n    $$\n    \\text{integrand}_{\\text{den}}(x) = \\exp\\left(-\\frac{(x-\\mu)^2}{2P} - \\frac{(h(\\mu)-h(x))^2}{2R}\\right)\n    $$\n5.  Numerically compute the integrals over $(-\\infty, \\infty)$:\n    $$\n    N = \\int_{-\\infty}^{\\infty} \\text{integrand}_{\\text{num}}(x) \\, dx\n    $$\n    $$\n    D = \\int_{-\\infty}^{\\infty} \\text{integrand}_{\\text{den}}(x) \\, dx\n    $$\n6.  Calculate the exact posterior mean: $m^\\star(y) = N/D$.\n7.  Calculate the bias: $b = m^\\star(y) - \\mu$.\nThis procedure will be implemented for all five test cases provided.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\n# language: Python\n# version: 3.12\n# libraries:\n#   name: numpy, version: 1.23.5\n#   name: scipy, version: 1.11.4\nimport numpy as np\nfrom scipy.integrate import quad\n\ndef solve():\n    \"\"\"\n    Computes the bias of the Extended Kalman Filter (EKF) posterior mean for\n    several test cases with nonlinear observation models.\n    \"\"\"\n\n    # Define the observation functions h(x) for each test case.\n    def h_case1(x):\n        a, b = 1.3, -0.2\n        return a * x + b\n\n    def h_case2(x):\n        return x**2\n\n    def h_case3(x):\n        return np.sin(x)\n\n    def h_case4(x):\n        return np.tanh(x)\n\n    # Test cases defined as a list of dictionaries for clarity.\n    # Each dictionary contains the observation function h, prior mean mu,\n    # prior variance P, and measurement noise variance R.\n    test_cases = [\n        # Test Case 1: Linear observation model\n        {'h': h_case1, 'mu': 0.5, 'P': 0.09, 'R': 0.04},\n        # Test Case 2: Quadratic observation model\n        {'h': h_case2, 'mu': 0.5, 'P': 0.2, 'R': 0.01},\n        # Test Case 3: Sinusoidal observation model\n        {'h': h_case3, 'mu': 0.3, 'P': 0.15, 'R': 0.02},\n        # Test Case 4: Saturating observation model\n        {'h': h_case4, 'mu': 0.8, 'P': 0.3, 'R': 0.02},\n        # Test Case 5: Strong nonlinearity and large prior variance\n        {'h': h_case2, 'mu': 1.0, 'P': 1.0, 'R': 0.1},\n    ]\n\n    results = []\n    for case in test_cases:\n        h = case['h']\n        mu = case['mu']\n        P = case['P']\n        R = case['R']\n\n        # Per the problem statement, the measurement y is set to h(mu).\n        y = h(mu)\n\n        # The EKF posterior mean m_ekf simplifies to mu under this condition,\n        # as the innovation is zero.\n        m_ekf = mu\n\n        # Define the unnormalized posterior PDF, which is proportional to\n        # exp(-((x-mu)^2)/(2P) - ((y-h(x))^2)/(2R)).\n        # We define a function for the exponent to avoid re-calculation and\n        # potential underflow with the full exponential.\n        def posterior_log_pdf_unnormalized(x):\n            prior_term = (x - mu)**2 / (2 * P)\n            likelihood_term = (y - h(x))**2 / (2 * R)\n            return -prior_term - likelihood_term\n\n        # Integrand for the numerator of the exact posterior mean E[x|y]\n        def numerator_integrand(x):\n            return x * np.exp(posterior_log_pdf_unnormalized(x))\n\n        # Integrand for the denominator (normalization constant)\n        def denominator_integrand(x):\n            return np.exp(posterior_log_pdf_unnormalized(x))\n\n        # Perform numerical integration over (-inf, inf)\n        # The quad function returns the integral result and an error estimate.\n        numerator_val, _ = quad(numerator_integrand, -np.inf, np.inf)\n        denominator_val, _ = quad(denominator_integrand, -np.inf, np.inf)\n        \n        # Calculate the exact Bayesian posterior mean m_star\n        if denominator_val == 0:\n            # This case should not be reached with the given parameters,\n            # but is included for robustness.\n            m_star = mu\n        else:\n            m_star = numerator_val / denominator_val\n            \n        # The bias is the difference between the exact mean and the EKF mean.\n        bias = m_star - m_ekf\n        results.append(bias)\n\n    # Format the final output as a comma-separated list of biases,\n    # rounded to six decimal places, enclosed in square brackets.\n    print(f\"[{','.join([f'{r:.6f}' for r in results])}]\")\n\nsolve()\n```", "id": "3397756"}, {"introduction": "Beyond theoretical approximations, the EKF's performance is also limited by the numerical stability of its implementation. This practice investigates a critical vulnerability in the standard covariance update formula, which can lose the essential property of positive-definiteness due to floating-point errors. By implementing and comparing it with the robust \"Joseph form\" of the covariance update, you will learn to diagnose and prevent a common cause of filter divergence in real-world applications [@problem_id:3397785].", "problem": "Consider a discrete-time nonlinear state-space model used in inverse problems and data assimilation:\n$$\nx_{k+1} = f(x_k) + w_k, \\quad z_k = h(x_k) + v_k,\n$$\nwhere $x_k \\in \\mathbb{R}^2$ is the state, $z_k \\in \\mathbb{R}$ is the measurement, $w_k \\sim \\mathcal{N}(0,Q)$ is the process noise, and $v_k \\sim \\mathcal{N}(0,R)$ is the measurement noise. The covariance matrices $Q \\in \\mathbb{R}^{2 \\times 2}$ and $R \\in \\mathbb{R}$ are symmetric positive semidefinite. The Extended Kalman Filter (EKF) linearizes the nonlinear model using Jacobians. A valid covariance must remain symmetric positive semidefinite after prediction and update.\n\nIn practice, it is known that certain algebraically simplified covariance update formulas can lose positive semidefiniteness due to linearization error and floating-point roundoff, whereas the Joseph stabilized form preserves symmetry and positive semidefiniteness. This problem asks you to derive, implement, and empirically verify the limitations of the algebraically simplified covariance update versus the Joseph stabilized form, using well-defined nonlinear dynamics and measurement functions.\n\nUse the following nonlinear functions and their Jacobians:\n$$\nf(x) = \\begin{bmatrix} x_1 + \\Delta t \\, x_2 \\\\ x_2 + \\Delta t \\, \\alpha \\, \\sin(x_1) \\end{bmatrix}, \\quad\nF(x) = \\frac{\\partial f}{\\partial x}(x) = \\begin{bmatrix} 1  \\Delta t \\\\ \\Delta t \\, \\alpha \\, \\cos(x_1)  1 \\end{bmatrix},\n$$\n$$\nh(x) = x_1^2 + \\tfrac{1}{2} x_2, \\quad\nH(x) = \\frac{\\partial h}{\\partial x}(x) = \\begin{bmatrix} 2 x_1  \\tfrac{1}{2} \\end{bmatrix}.\n$$\n\nStarting from the definitions of covariance, linear Gaussian assimilation, and the EKF linearization, derive two distinct covariance update expressions for the analysis step:\n- The algebraically simplified update commonly used in implementations that eliminates a symmetric term.\n- The Joseph stabilized form that explicitly retains terms necessary for symmetry and positive semidefiniteness.\n\nThen implement a single EKF predict-update cycle as follows:\n1. Prediction:\n   - Compute $x_{k|k-1} = f(x_{k-1|k-1})$ and $F = F(x_{k-1|k-1})$.\n   - Compute $P_{k|k-1} = F P_{k-1|k-1} F^\\top + Q$.\n\n2. Update:\n   - Compute $H = H(x_{k|k-1})$, the innovation $y_k = z_k - h(x_{k|k-1})$, and the innovation covariance $S_k = H P_{k|k-1} H^\\top + R$.\n   - Compute the Kalman gain $K_k = P_{k|k-1} H^\\top S_k^{-1}$.\n   - Compute the updated state $x_{k|k} = x_{k|k-1} + K_k y_k$.\n   - Compute two versions of the updated covariance:\n     - The algebraically simplified covariance update.\n     - The Joseph stabilized covariance update.\n\nFor each updated covariance matrix, assess:\n- Symmetry error $e_{\\text{sym}} = \\|P - P^\\top\\|_F$ using the Frobenius norm.\n- The minimum eigenvalue $\\lambda_{\\min}$ of the symmetrized covariance $P_{\\text{sym}} = (P + P^\\top)/2$.\n- A boolean for positive semidefiniteness defined as $e_{\\text{sym}} \\leq 10^{-12}$ and $\\lambda_{\\min} \\geq -10^{-12}$.\n\nPerform the procedure for the following test suite, where each test case specifies $(\\Delta t, \\alpha, x_{0}, P_0, Q, R, z_1)$, and all scalar angles are in radians:\n\n- Test case 1 (happy path):\n  - $\\Delta t = 0.1$, $\\alpha = 5.0$,\n  - $x_0 = \\begin{bmatrix} 0.5 \\\\ -0.2 \\end{bmatrix}$,\n  - $P_0 = \\begin{bmatrix} 0.5  0.1 \\\\ 0.1  0.5 \\end{bmatrix}$,\n  - $Q = \\begin{bmatrix} 10^{-3}  0 \\\\ 0  10^{-3} \\end{bmatrix}$,\n  - $R = 0.05$,\n  - $z_1 = h(x_0) + 0.1$.\n\n- Test case 2 (near-singular prior):\n  - $\\Delta t = 0.1$, $\\alpha = 5.0$,\n  - $x_0 = \\begin{bmatrix} 0.01 \\\\ 0.0 \\end{bmatrix}$,\n  - $P_0 = \\begin{bmatrix} 10^{-8}  0 \\\\ 0  10^{-4} \\end{bmatrix}$,\n  - $Q = \\begin{bmatrix} 10^{-12}  0 \\\\ 0  10^{-10} \\end{bmatrix}$,\n  - $R = 10^{-8}$,\n  - $z_1 = h(x_0) - 0.05$.\n\n- Test case 3 (strong nonlinearity and high correlation):\n  - $\\Delta t = 0.2$, $\\alpha = 15.0$,\n  - $x_0 = \\begin{bmatrix} 2.0 \\\\ 1.0 \\end{bmatrix}$,\n  - $P_0 = \\begin{bmatrix} 1.0  0.99 \\\\ 0.99  1.0 \\end{bmatrix}$,\n  - $Q = \\begin{bmatrix} 10^{-3}  0 \\\\ 0  10^{-3} \\end{bmatrix}$,\n  - $R = 10^{-6}$,\n  - $z_1 = h(x_0) - 0.2$.\n\n- Test case 4 (ill-conditioned innovation):\n  - $\\Delta t = 0.05$, $\\alpha = 7.0$,\n  - $x_0 = \\begin{bmatrix} -3.0 \\\\ 0.1 \\end{bmatrix}$,\n  - $P_0 = \\begin{bmatrix} 0.2  -0.19 \\\\ -0.19  0.2 \\end{bmatrix}$,\n  - $Q = \\begin{bmatrix} 10^{-5}  0 \\\\ 0  10^{-5} \\end{bmatrix}$,\n  - $R = 10^{-4}$,\n  - $z_1 = h(x_0)$.\n\nFor each test case, your program must output a list\n$$\n[\\text{is\\_naive\\_psd}, \\ \\text{is\\_joseph\\_psd}, \\ \\lambda_{\\min}^{\\text{naive}}, \\ \\lambda_{\\min}^{\\text{joseph}}, \\ e_{\\text{sym}}^{\\text{naive}}, \\ e_{\\text{sym}}^{\\text{joseph}}],\n$$\nwhere the booleans indicate positive semidefiniteness for the respective covariance updates, the eigenvalues are floats, and the symmetry errors are floats.\n\nFinal output format specification:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3]\"). Each \"result\" must itself be the list described above for its corresponding test case, in the order of the test suite provided.", "solution": "The task is to derive, implement, and compare two covariance update formulas for the Extended Kalman Filter (EKF) under specific nonlinear dynamics, and to evaluate their numerical stability.\n\n### Part 1: Derivation of Covariance Update Formulas\n\nWe consider a nonlinear system with state $x_k \\in \\mathbb{R}^n$ and measurement $z_k \\in \\mathbb{R}^m$:\n$$\nx_{k+1} = f(x_k) + w_k, \\quad w_k \\sim \\mathcal{N}(0,Q)\n$$\n$$\nz_k = h(x_k) + v_k, \\quad v_k \\sim \\mathcal{N}(0,R)\n$$\nThe EKF update step linearizes the measurement model around the predicted state $x_{k|k-1}$. The analysis state $x_{k|k}$ and its covariance $P_{k|k}$ are computed from the predicted state $x_{k|k-1}$, its covariance $P_{k|k-1}$, the measurement $z_k$, and the linearized measurement matrix $H_k = \\frac{\\partial h}{\\partial x}(x_{k|k-1})$.\n\nThe key components of the update are:\n- Innovation: $y_k = z_k - h(x_{k|k-1})$\n- Innovation covariance: $S_k = H_k P_{k|k-1} H_k^\\top + R$\n- Kalman gain: $K_k = P_{k|k-1} H_k^\\top S_k^{-1}$\n- State update: $x_{k|k} = x_{k|k-1} + K_k y_k$\n\nFrom these, we derive two forms for the covariance update $P_{k|k}$.\n\n**1. Algebraically Simplified Covariance Update**\n\nThis is the most common form in introductory texts, derived from the Kalman gain expression. It starts from the more fundamental expression for the posterior covariance, which can be obtained via the Woodbury matrix identity:\n$$\nP_{k|k} = P_{k|k-1} - P_{k|k-1} H_k^\\top (H_k P_{k|k-1} H_k^\\top + R)^{-1} H_k P_{k|k-1}\n$$\nSubstituting the expressions for $S_k$ and $K_k$:\n$$\nP_{k|k} = P_{k|k-1} - (P_{k|k-1} H_k^\\top S_k^{-1}) H_k P_{k|k-1}\n$$\n$$\nP_{k|k} = P_{k|k-1} - K_k H_k P_{k|k-1}\n$$\nThis can be factored into the **algebraically simplified form**:\n$$\nP_{k|k} = (I - K_k H_k) P_{k|k-1}\n$$\nAlthough algebraically correct, this formula is numerically unstable. The subtraction $P_{k|k-1} - K_k H_k P_{k|k-1}$ can lead to a loss of positive definiteness due to finite-precision arithmetic (catastrophic cancellation). Furthermore, since the matrix product $K_k H_k P_{k|k-1}$ is not generally symmetric, small floating-point errors can cause $P_{k|k}$ to lose its symmetry property, even if $P_{k|k-1}$ is perfectly symmetric.\n\n**2. Joseph Stabilized Covariance Update**\n\nThis form is derived directly from the propagation of the estimation error. The analysis error is $e_{k|k} = x_k - x_{k|k}$. Substituting the state update equation and the linearized measurement model:\n$$\ne_{k|k} = x_k - \\left(x_{k|k-1} + K_k (z_k - h(x_{k|k-1}))\\right)\n$$\n$$\ne_{k|k} \\approx x_k - \\left(x_{k|k-1} + K_k (H_k(x_k - x_{k|k-1}) + v_k)\\right)\n$$\nLetting $e_{k|k-1} = x_k - x_{k|k-1}$ be the prediction error, we get:\n$$\ne_{k|k} \\approx e_{k|k-1} - K_k H_k e_{k|k-1} - K_k v_k = (I - K_k H_k) e_{k|k-1} - K_k v_k\n$$\nThe analysis covariance is $P_{k|k} = E[e_{k|k} e_{k|k}^\\top]$. Since the prediction error $e_{k|k-1}$ and the measurement noise $v_k$ are uncorrelated, their cross-term vanishes:\n$$\nP_{k|k} = E\\left[ \\left((I - K_k H_k) e_{k|k-1}\\right)\\left((I - K_k H_k) e_{k|k-1}\\right)^\\top \\right] + E\\left[ (- K_k v_k)(- K_k v_k)^\\top \\right]\n$$\n$$\nP_{k|k} = (I - K_k H_k) E[e_{k|k-1}e_{k|k-1}^\\top] (I - K_k H_k)^\\top + K_k E[v_k v_k^\\top] K_k^\\top\n$$\nSubstituting $P_{k|k-1} = E[e_{k|k-1}e_{k|k-1}^\\top]$ and $R = E[v_k v_k^\\top]$ yields the **Joseph stabilized form**:\n$$\nP_{k|k} = (I - K_k H_k) P_{k|k-1} (I - K_k H_k)^\\top + K_k R K_k^\\top\n$$\nThis form is inherently symmetric by construction. If $P_{k|k-1}$ and $R$ are symmetric-positive-semidefinite (SPSD), then each term in the sum is of the form $M \\Sigma M^\\top$, which is also SPSD. The sum of SPSD matrices is SPSD, so the result $P_{k|k}$ is guaranteed to be SPSD. This formulation avoids the subtractive cancellation that plagues the simplified form, ensuring numerical stability.\n\n### Part 2: Implementation and Empirical Verification\n\nThe provided problem is solved by implementing a single predict-update cycle of the EKF for four different test cases. Each case uses the two derived covariance update formulas, and the resulting covariance matrices are assessed for symmetry and positive semidefiniteness.\n\n**Procedure:**\nFor each test case with parameters $(\\Delta t, \\alpha, x_{0}, P_0, Q, R, z_1)$:\n1.  **Prediction**: The state is predicted forward using the nonlinear dynamics function $f$, and the covariance is predicted using the linearized dynamics matrix $F$.\n    -   $x_{1|0} = f(x_0)$\n    -   $F_0 = F(x_0)$\n    -   $P_{1|0} = F_0 P_0 F_0^\\top + Q$\n2.  **Update**: An update is performed using the measurement $z_1$.\n    -   $H_1 = H(x_{1|0})$\n    -   Innovation covariance: $S_1 = H_1 P_{1|0} H_1^\\top + R$\n    -   Kalman gain: $K_1 = P_{1|0} H_1^\\top S_1^{-1}$\n3.  **Covariance Calculation**: Two versions of the updated covariance $P_{1|1}$ are computed.\n    -   Naive form: $P_{\\text{naive}} = (I - K_1 H_1) P_{1|0}$\n    -   Joseph form: $P_{\\text{joseph}} = (I - K_1 H_1) P_{1|0} (I - K_1 H_1)^\\top + K_1 R K_1^\\top$\n4.  **Assessment**: Each resulting covariance matrix $P$ is evaluated based on:\n    -   Symmetry error: $e_{\\text{sym}} = \\|P - P^\\top\\|_F$\n    -   Minimum eigenvalue: $\\lambda_{\\min}$ of the symmetrized matrix $(P + P^\\top)/2$\n    -   Positive semidefiniteness (PSD): A boolean flag is set to true if $e_{\\text{sym}} \\leq 10^{-12}$ and $\\lambda_{\\min} \\geq -10^{-12}$.\n\nThe implementation will show that while both forms produce similar results in well-conditioned cases, the naive form can fail to maintain symmetry and positive semidefiniteness in challenging scenarios involving high correlation or ill-conditioned matrices, whereas the Joseph form remains robust.", "answer": "```python\nimport numpy as np\nfrom numpy.linalg import norm, eigvalsh\n\ndef solve():\n    \"\"\"\n    Main function to run the EKF cycle for all test cases and print results.\n    \"\"\"\n\n    # Helper functions for h(x) to compute z1 values\n    def h_func(x):\n        return x[0]**2 + 0.5 * x[1]\n\n    # Test cases as defined in the problem statement\n    test_cases = [\n        {\n            \"dt\": 0.1, \"alpha\": 5.0,\n            \"x0\": np.array([0.5, -0.2]),\n            \"P0\": np.array([[0.5, 0.1], [0.1, 0.5]]),\n            \"Q\": np.array([[1e-3, 0], [0, 1e-3]]),\n            \"R\": 0.05,\n            \"z1\": h_func(np.array([0.5, -0.2])) + 0.1,\n        },\n        {\n            \"dt\": 0.1, \"alpha\": 5.0,\n            \"x0\": np.array([0.01, 0.0]),\n            \"P0\": np.array([[1e-8, 0], [0, 1e-4]]),\n            \"Q\": np.array([[1e-12, 0], [0, 1e-10]]),\n            \"R\": 1e-8,\n            \"z1\": h_func(np.array([0.01, 0.0])) - 0.05,\n        },\n        {\n            \"dt\": 0.2, \"alpha\": 15.0,\n            \"x0\": np.array([2.0, 1.0]),\n            \"P0\": np.array([[1.0, 0.99], [0.99, 1.0]]),\n            \"Q\": np.array([[1e-3, 0], [0, 1e-3]]),\n            \"R\": 1e-6,\n            \"z1\": h_func(np.array([2.0, 1.0])) - 0.2,\n        },\n        {\n            \"dt\": 0.05, \"alpha\": 7.0,\n            \"x0\": np.array([-3.0, 0.1]),\n            \"P0\": np.array([[0.2, -0.19], [-0.19, 0.2]]),\n            \"Q\": np.array([[1e-5, 0], [0, 1e-5]]),\n            \"R\": 1e-4,\n            \"z1\": h_func(np.array([-3.0, 0.1])),\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        result = run_ekf_cycle(\n            case[\"dt\"], case[\"alpha\"], case[\"x0\"], case[\"P0\"],\n            case[\"Q\"], case[\"R\"], case[\"z1\"]\n        )\n        results.append(result)\n\n    # Format the final output as a string representing a list of lists.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef run_ekf_cycle(dt, alpha, x0, P0, Q, R, z1):\n    \"\"\"\n    Performs a single EKF predict-update cycle for a given test case.\n\n    Args:\n        dt (float): Time step.\n        alpha (float): Parameter in the dynamic model.\n        x0 (np.ndarray): Initial state vector x_{0|0}.\n        P0 (np.ndarray): Initial covariance matrix P_{0|0}.\n        Q (np.ndarray): Process noise covariance matrix.\n        R (float): Measurement noise variance.\n        z1 (float): Measurement at step 1.\n\n    Returns:\n        list: A list containing the assessment metrics for naive and Joseph forms.\n              [is_naive_psd, is_joseph_psd, lambda_min_naive, \n               lambda_min_joseph, e_sym_naive, e_sym_joseph]\n    \"\"\"\n    \n    # State and measurement model functions\n    def f(x, dt, alpha):\n        x1, x2 = x\n        return np.array([x1 + dt * x2, x2 + dt * alpha * np.sin(x1)])\n\n    def h(x):\n        x1, x2 = x\n        return x1**2 + 0.5 * x2\n\n    # Jacobian functions\n    def F_jac(x, dt, alpha):\n        x1, _ = x\n        return np.array([[1.0, dt], [dt * alpha * np.cos(x1), 1.0]])\n\n    def H_jac(x):\n        x1, _ = x\n        return np.array([[2.0 * x1, 0.5]])\n\n    # ---- 1. Prediction Step ----\n    # Predicted state x_{1|0}\n    x_pred = f(x0, dt, alpha)\n    # Jacobian of dynamics at x0\n    F0 = F_jac(x0, dt, alpha)\n    # Predicted covariance P_{1|0}\n    P_pred = F0 @ P0 @ F0.T + Q\n\n    # ---- 2. Update Step ----\n    # Jacobian of measurement at x_pred\n    H1 = H_jac(x_pred)\n    # Innovation\n    y = z1 - h(x_pred)\n    # Innovation covariance (S is a 1x1 matrix)\n    S = H1 @ P_pred @ H1.T + R\n    S_inv = 1.0 / S[0, 0]\n    # Kalman gain K\n    K = P_pred @ H1.T * S_inv\n\n    # ---- 3. Covariance Update (Two Forms) ----\n    I = np.identity(2)\n    # Algebraically simplified (naive) form\n    P_naive = (I - K @ H1) @ P_pred\n    # Joseph stabilized form\n    term1 = (I - K @ H1) @ P_pred @ (I - K @ H1).T\n    term2 = K @ np.array([[R]]) @ K.T\n    P_joseph = term1 + term2\n\n    # ---- 4. Assessment ----\n    def assess_covariance(P):\n        # Symmetry error\n        e_sym = norm(P - P.T, 'fro')\n        # Symmetrize for eigenvalue calculation\n        P_sym = 0.5 * (P + P.T)\n        # Minimum eigenvalue\n        try:\n            # eigvalsh is for symmetric/Hermitian matrices\n            min_eig = np.min(eigvalsh(P_sym))\n        except np.linalg.LinAlgError:\n            min_eig = -np.inf\n            \n        # Check for positive semidefiniteness (PSD)\n        is_psd = (e_sym = 1e-12) and (min_eig >= -1e-12)\n        return is_psd, min_eig, e_sym\n\n    is_naive_psd, lambda_min_naive, e_sym_naive = assess_covariance(P_naive)\n    is_joseph_psd, lambda_min_joseph, e_sym_joseph = assess_covariance(P_joseph)\n    \n    return [is_naive_psd, is_joseph_psd, lambda_min_naive, lambda_min_joseph, e_sym_naive, e_sym_joseph]\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3397785"}]}