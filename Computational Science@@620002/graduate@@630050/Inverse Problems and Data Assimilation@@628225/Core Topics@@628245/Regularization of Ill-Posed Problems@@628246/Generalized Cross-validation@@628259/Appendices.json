{"hands_on_practices": [{"introduction": "The first and most fundamental step in using Generalized Cross-Validation (GCV) is translating its mathematical definition into an efficient numerical algorithm. This practice [@problem_id:3419911] guides you through building a GCV solver from the ground up, leveraging the Singular Value Decomposition (SVD) for a stable and computationally efficient implementation. By applying your solver to various test cases, you will gain practical experience in choosing the Tikhonov regularization parameter for linear inverse problems.", "problem": "Consider the linear discrete inverse problem of estimating an unknown state vector $x \\in \\mathbb{R}^n$ from noisy observations $b \\in \\mathbb{R}^m$ related by a known forward operator $A \\in \\mathbb{R}^{m \\times n}$ through the model $b = A x + \\varepsilon$, where $\\varepsilon$ represents additive measurement noise. We focus on zero-order Tikhonov regularization, in which $x$ is estimated by minimizing the Tikhonov functional $\\|A x - b\\|_2^2 + \\alpha^2 \\|x\\|_2^2$ for a regularization parameter $\\alpha > 0$. The estimation must be expressed using the Singular Value Decomposition (SVD), and the interpretation must be given in terms of filter factors. The quality of fit should be assessed using Generalized Cross-Validation (GCV) (Generalized Cross-Validation (GCV) penalizes influence by the linear prediction operator).\n\nStarting from the following fundamental base:\n- The discrete forward model $b = A x + \\varepsilon$ with $A \\in \\mathbb{R}^{m \\times n}$, $x \\in \\mathbb{R}^n$, $b \\in \\mathbb{R}^m$.\n- The zero-order Tikhonov regularized solution $\\hat{x}_\\alpha$ solves $\\min_{x} \\|A x - b\\|_2^2 + \\alpha^2 \\|x\\|_2^2$ for $\\alpha > 0$.\n- The Singular Value Decomposition (SVD) of $A$ is $A = U \\Sigma V^\\top$, where $U \\in \\mathbb{R}^{m \\times r}$, $\\Sigma \\in \\mathbb{R}^{r \\times r}$ is diagonal with positive singular values $\\sigma_i$, $V \\in \\mathbb{R}^{n \\times r}$, and $r = \\mathrm{rank}(A) \\le \\min(m,n)$.\n- The influence matrix (also called the hat matrix) is $H_\\alpha \\in \\mathbb{R}^{m \\times m}$ such that the predicted data is $\\hat{b}_\\alpha = A \\hat{x}_\\alpha = H_\\alpha b$.\n\nYour task is to:\n1. Derive, from first principles, the SVD-based form of the Tikhonov solution $\\hat{x}_\\alpha$ and identify the corresponding filter factors in terms of the singular values $\\sigma_i$.\n2. Derive the influence matrix $H_\\alpha$ and express its trace in terms of the singular values $\\sigma_i$ and the regularization parameter $\\alpha$.\n3. Derive the Generalized Cross-Validation (GCV) functional $G(\\alpha)$ for the zero-order Tikhonov estimator, solely in terms of quantities computed from the SVD of $A$ and the data vector $b$, without forming any large dense matrix beyond what is necessary for the SVD. The GCV functional must be derived as a function that depends on the residual norm $\\|A \\hat{x}_\\alpha - b\\|_2$ and $\\mathrm{trace}(H_\\alpha)$.\n4. Implement a numerical algorithm that:\n   - Computes the SVD of $A$.\n   - Evaluates $G(\\alpha)$ over a logarithmically spaced grid of $\\alpha \\in [10^{-8}, 10^{2}]$ using $200$ points, then refines by searching a new logarithmic grid of $200$ points centered around the minimizing $\\alpha$ within a multiplicative factor of $10$ (respecting the original bounds).\n   - Returns, for each test case, the minimizing regularization parameter $\\alpha^\\star$, the squared residual norm $\\|A \\hat{x}_{\\alpha^\\star} - b\\|_2^2$, and $\\mathrm{trace}(H_{\\alpha^\\star})$; all values must be computed in double precision.\n5. Use radians for any trigonometric functions.\n\nTest Suite:\n- Case $1$ (happy path, mildly ill-conditioned, overdetermined): Let $m = 50$, $n = 30$. Define the matrix $A \\in \\mathbb{R}^{50 \\times 30}$ by\n  $$A_{ij} = \\frac{1}{1 + |i - j|} + 10^{-3} \\cdot \\sin\\left(\\frac{i + j}{10}\\right), \\quad 1 \\le i \\le 50, \\; 1 \\le j \\le 30,$$\n  where the sine argument is in radians. Define the true state $x_{\\mathrm{true}} \\in \\mathbb{R}^{30}$ by\n  $$x_{\\mathrm{true},j} = \\sin\\left(\\frac{j}{4}\\right), \\quad 1 \\le j \\le 30,$$\n  with sine in radians. Define the data $b \\in \\mathbb{R}^{50}$ by\n  $$b_i = \\sum_{j=1}^{30} A_{ij} x_{\\mathrm{true},j} + 10^{-3} \\cdot (-1)^i, \\quad 1 \\le i \\le 50.$$\n- Case $2$ (ill-posed blur, square system, stronger noise): Let $m = n = 40$. Define the matrix $A \\in \\mathbb{R}^{40 \\times 40}$ by\n  $$A_{ij} = \\exp\\left( - \\frac{(i - j)^2}{2 \\cdot 25} \\right), \\quad 1 \\le i,j \\le 40.$$\n  Define the true state $x_{\\mathrm{true}} \\in \\mathbb{R}^{40}$ by\n  $$x_{\\mathrm{true},j} = \\cos\\left(\\frac{j}{8}\\right), \\quad 1 \\le j \\le 40,$$\n  with cosine in radians. Define the data $b \\in \\mathbb{R}^{40}$ by\n  $$b_i = \\sum_{j=1}^{40} A_{ij} x_{\\mathrm{true},j} + 10^{-2} \\cdot \\sin\\left(\\frac{i}{3}\\right), \\quad 1 \\le i \\le 40,$$\n  with sine in radians.\n- Case $3$ (edge case, no information in $A$): Let $m = 20$, $n = 10$. Define $A$ by\n  $$A_{ij} = 0, \\quad 1 \\le i \\le 20, \\; 1 \\le j \\le 10,$$\n  and define $b \\in \\mathbb{R}^{20}$ by\n  $$b_i = (-1)^i, \\quad 1 \\le i \\le 20.$$\n\nFinal Output Format:\n- Your program should produce a single line of output containing the results for all three test cases as a comma-separated list of lists enclosed in square brackets. For each test case, output the triple $[\\alpha^\\star, \\|A \\hat{x}_{\\alpha^\\star} - b\\|_2^2, \\mathrm{trace}(H_{\\alpha^\\star})]$. Each floating-point number must be formatted with exactly $6$ digits after the decimal point.\n- For example, the output structure must be of the form\n  $$\\left[ [\\alpha^\\star_1, r^2_1, t_1], [\\alpha^\\star_2, r^2_2, t_2], [\\alpha^\\star_3, r^2_3, t_3] \\right],$$\n  printed as a single line: \n  $$[[\\alpha^\\star_1,r^2_1,t_1],[\\alpha^\\star_2,r^2_2,t_2],[\\alpha^\\star_3,r^2_3,t_3]].$$\nAll trigonometric function arguments are in radians, and no physical units are involved. Each returned value is a float.", "solution": "The problem is critically validated and deemed valid. It is a well-posed, scientifically grounded problem in the field of inverse problems, with all necessary information provided and no contradictions apparent.\n\nWe are tasked with finding the zero-order Tikhonov regularized solution $\\hat{x}_\\alpha$ to the linear inverse problem $b = Ax + \\varepsilon$. The solution $\\hat{x}_\\alpha$ is the minimizer of the Tikhonov functional:\n$$\nJ(x) = \\|A x - b\\|_2^2 + \\alpha^2 \\|x\\|_2^2\n$$\nwhere $A \\in \\mathbb{R}^{m \\times n}$, $x \\in \\mathbb{R}^n$, $b \\in \\mathbb{R}^m$, and $\\alpha > 0$ is the regularization parameter. The solution must be expressed using the Singular Value Decomposition (SVD) of $A$, and the optimal $\\alpha$ is to be found by minimizing the Generalized Cross-Validation (GCV) functional.\n\n**1. SVD-based Tikhonov Solution and Filter Factors**\n\nThe minimizer $\\hat{x}_\\alpha$ of $J(x)$ satisfies the condition that the gradient $\\nabla_x J(x)$ is zero. Calculating the gradient yields:\n$$\n\\nabla_x J(x) = \\nabla_x ( (Ax-b)^\\top(Ax-b) + \\alpha^2 x^\\top x ) = 2 A^\\top (Ax - b) + 2 \\alpha^2 x\n$$\nSetting the gradient to zero gives the normal equations for the Tikhonov problem:\n$$\n(A^\\top A + \\alpha^2 I_n) \\hat{x}_\\alpha = A^\\top b\n$$\nwhere $I_n$ is the $n \\times n$ identity matrix.\n\nLet the rank of $A$ be $r \\le \\min(m, n)$. The SVD of $A$ is given by $A = U \\Sigma V^\\top$, where $U \\in \\mathbb{R}^{m \\times r}$ has orthonormal columns ($u_1, \\ldots, u_r$), $V \\in \\mathbb{R}^{n \\times r}$ has orthonormal columns ($v_1, \\ldots, v_r$), and $\\Sigma \\in \\mathbb{R}^{r \\times r}$ is a diagonal matrix with positive singular values $\\sigma_1 \\ge \\sigma_2 \\ge \\ldots \\ge \\sigma_r > 0$.\n\nUsing the SVD, we can express the terms in the normal equations:\n$$\nA^\\top A = (U \\Sigma V^\\top)^\\top (U \\Sigma V^\\top) = V \\Sigma^\\top U^\\top U \\Sigma V^\\top = V \\Sigma^2 V^\\top\n$$\n$$\nA^\\top b = (U \\Sigma V^\\top)^\\top b = V \\Sigma U^\\top b\n$$\nSubstituting these into the normal equations gives:\n$$\n(V \\Sigma^2 V^\\top + \\alpha^2 I_n) \\hat{x}_\\alpha = V \\Sigma U^\\top b\n$$\nThe solution $\\hat{x}_\\alpha$ must lie in the span of the columns of $V$, which is the orthogonal complement of the null space of $A$. Any component of $x$ in the null space of $A$ would increase the penalty term $\\alpha^2 \\|x\\|_2^2$ without decreasing the residual term $\\|A x - b\\|_2^2$. Thus, we can write the solution as a linear combination of the basis vectors $v_i$: $\\hat{x}_\\alpha = \\sum_{i=1}^r c_i v_i = Vc$ for some coefficient vector $c \\in \\mathbb{R}^r$. Substituting this into the equation and using $V^\\top V = I_r$:\n$$\n(V \\Sigma^2 V^\\top + \\alpha^2 I_n) V c = V \\Sigma U^\\top b\n$$\nLeft-multiplying by $V^\\top$:\n$$\nV^\\top(V \\Sigma^2 V^\\top + \\alpha^2 I_n) V c = V^\\top(V \\Sigma U^\\top b)\n$$\n$$\n(\\Sigma^2 + \\alpha^2 I_r) c = \\Sigma U^\\top b\n$$\nSince $\\sigma_i > 0$ and $\\alpha > 0$, the matrix $(\\Sigma^2 + \\alpha^2 I_r)$ is diagonal and invertible. We can solve for $c$:\n$$\nc = (\\Sigma^2 + \\alpha^2 I_r)^{-1} \\Sigma U^\\top b\n$$\nThe Tikhonov solution is then $\\hat{x}_\\alpha = Vc$:\n$$\n\\hat{x}_\\alpha = V (\\Sigma^2 + \\alpha^2 I_r)^{-1} \\Sigma U^\\top b\n$$\nIn summation form, the $i$-th component of $c$ is $c_i = \\frac{\\sigma_i}{\\sigma_i^2 + \\alpha^2} (u_i^\\top b)$. The solution becomes:\n$$\n\\hat{x}_\\alpha = \\sum_{i=1}^r c_i v_i = \\sum_{i=1}^r \\frac{\\sigma_i}{\\sigma_i^2 + \\alpha^2} (u_i^\\top b) v_i\n$$\nThe standard (unregularized) pseudoinverse solution is $x^\\dagger = A^\\dagger b = \\sum_{i=1}^r \\frac{1}{\\sigma_i} (u_i^\\top b) v_i$. We can express $\\hat{x}_\\alpha$ in terms of the components of $x^\\dagger$:\n$$\n\\hat{x}_\\alpha = \\sum_{i=1}^r \\left(\\frac{\\sigma_i^2}{\\sigma_i^2 + \\alpha^2}\\right) \\frac{u_i^\\top b}{\\sigma_i} v_i\n$$\nThe terms $f_i(\\alpha) = \\frac{\\sigma_i^2}{\\sigma_i^2 + \\alpha^2}$ are the **filter factors**. They filter the spectral components of the solution, damping those associated with small singular values, which stabilizes the solution against noise.\n\n**2. Influence Matrix and its Trace**\n\nThe predicted data is $\\hat{b}_\\alpha = A \\hat{x}_\\alpha$. The influence matrix (or hat matrix) $H_\\alpha$ relates the original data $b$ to the predicted data $\\hat{b}_\\alpha$ via $\\hat{b}_\\alpha = H_\\alpha b$. Substituting the SVD expressions for $A$ and $\\hat{x}_\\alpha$:\n$$\n\\hat{b}_\\alpha = (U \\Sigma V^\\top) \\left( V (\\Sigma^2 + \\alpha^2 I_r)^{-1} \\Sigma U^\\top b \\right)\n$$\nUsing $V^\\top V = I_r$, this simplifies to:\n$$\n\\hat{b}_\\alpha = U \\Sigma (\\Sigma^2 + \\alpha^2 I_r)^{-1} \\Sigma U^\\top b = U \\Sigma^2 (\\Sigma^2 + \\alpha^2 I_r)^{-1} U^\\top b\n$$\nFrom this, we identify the influence matrix:\n$$\nH_\\alpha = U \\Sigma^2 (\\Sigma^2 + \\alpha^2 I_r)^{-1} U^\\top = \\sum_{i=1}^r \\frac{\\sigma_i^2}{\\sigma_i^2 + \\alpha^2} u_i u_i^\\top\n$$\nThe trace of the influence matrix is found using the linearity of the trace operator and the property $\\mathrm{trace}(uv^\\top) = v^\\top u$:\n$$\n\\mathrm{trace}(H_\\alpha) = \\mathrm{trace} \\left(\\sum_{i=1}^r \\frac{\\sigma_i^2}{\\sigma_i^2 + \\alpha^2} u_i u_i^\\top \\right) = \\sum_{i=1}^r \\frac{\\sigma_i^2}{\\sigma_i^2 + \\alpha^2} \\mathrm{trace}(u_i u_i^\\top)\n$$\nSince $u_i^\\top u_i = 1$, we have $\\mathrm{trace}(u_i u_i^\\top) = 1$. Therefore:\n$$\n\\mathrm{trace}(H_\\alpha) = \\sum_{i=1}^r \\frac{\\sigma_i^2}{\\sigma_i^2 + \\alpha^2} = \\sum_{i=1}^r f_i(\\alpha)\n$$\nThis quantity is often interpreted as the effective degrees of freedom of the regularized model.\n\n**3. Generalized Cross-Validation (GCV) Functional**\n\nThe GCV functional provides a means to estimate the optimal $\\alpha$ by minimizing:\n$$\nG(\\alpha) = \\frac{m \\|A \\hat{x}_\\alpha - b\\|_2^2}{(m - \\mathrm{trace}(H_\\alpha))^2}\n$$\nWe need to express the numerator, the squared norm of the residual $r_\\alpha = A\\hat{x}_\\alpha - b$, in terms of SVD components. The residual can be written as $r_\\alpha = \\hat{b}_\\alpha - b = (H_\\alpha - I_m)b$.\nLet's decompose the data vector $b$ into its projection onto the column space of $A$ (which is the span of the columns of $U$) and its orthogonal complement:\n$$\nb = UU^\\top b + (I_m - UU^\\top)b = \\sum_{i=1}^r (u_i^\\top b) u_i + b_{ortho}\n$$\nApplying $(H_\\alpha - I_m)$ to $b$:\n$$\nr_\\alpha = \\left( \\sum_{i=1}^r f_i(\\alpha) u_i u_i^\\top \\right) b - b = \\sum_{i=1}^r f_i(\\alpha) (u_i^\\top b) u_i - \\left(\\sum_{i=1}^r (u_i^\\top b) u_i + b_{ortho}\\right)\n$$\n$$\nr_\\alpha = \\sum_{i=1}^r (f_i(\\alpha) - 1) (u_i^\\top b) u_i - b_{ortho}\n$$\nSubstituting $f_i(\\alpha) - 1 = \\frac{\\sigma_i^2}{\\sigma_i^2 + \\alpha^2} - 1 = \\frac{-\\alpha^2}{\\sigma_i^2 + \\alpha^2}$:\n$$\nr_\\alpha = \\sum_{i=1}^r \\frac{-\\alpha^2}{\\sigma_i^2 + \\alpha^2} (u_i^\\top b) u_i - b_{ortho}\n$$\nThe terms in the summation are orthogonal to $b_{ortho}$. By the Pythagorean theorem, the squared norm is:\n$$\n\\|r_\\alpha\\|_2^2 = \\left\\| \\sum_{i=1}^r \\frac{-\\alpha^2}{\\sigma_i^2 + \\alpha^2} (u_i^\\top b) u_i \\right\\|_2^2 + \\|b_{ortho}\\|_2^2\n$$\nSince the vectors $u_i$ are orthonormal, this becomes:\n$$\n\\|A \\hat{x}_\\alpha - b\\|_2^2 = \\sum_{i=1}^r \\left( \\frac{\\alpha^2}{\\sigma_i^2 + \\alpha^2} \\right)^2 (u_i^\\top b)^2 + \\|(I_m - UU^\\top) b\\|_2^2\n$$\nThe second term, $\\|b_{ortho}\\|_2^2$, is the squared norm of the component of $b$ that is orthogonal to the column space of $A$. It can be computed efficiently as $\\|b\\|_2^2 - \\|UU^\\top b\\|_2^2 = \\|b\\|_2^2 - \\sum_{i=1}^r(u_i^\\top b)^2$.\n\nAll components of the GCV functional $G(\\alpha)$ can now be computed using only the singular values $\\sigma_i$, the projected data components $u_i^\\top b$, the total size $m$, and the norm of $b$. This avoids the explicit formation of large matrices like $H_\\alpha$ or $A^\\top A$. The numerical implementation will follow this SVD-based formulation.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the Tikhonov regularization problem for all test cases.\n    \"\"\"\n\n    def generate_test_cases():\n        \"\"\"\n        Generates the A matrices and b vectors for the three test cases.\n        \"\"\"\n        # Case 1\n        m1, n1 = 50, 30\n        A1 = np.zeros((m1, n1), dtype=np.float64)\n        i_idx, j_idx = np.ogrid[1:m1+1, 1:n1+1]\n        A1 = 1 / (1 + np.abs(i_idx - j_idx)) + 1e-3 * np.sin((i_idx + j_idx) / 10)\n        \n        j_vec1 = np.arange(1, n1 + 1)\n        xtrue1 = np.sin(j_vec1 / 4)\n        \n        i_vec1 = np.arange(1, m1 + 1)\n        noise1 = 1e-3 * ((-1)**i_vec1)\n        b1 = A1 @ xtrue1 + noise1\n\n        # Case 2\n        m2, n2 = 40, 40\n        i_idx2, j_idx2 = np.ogrid[1:m2+1, 1:n2+1]\n        A2 = np.exp(-((i_idx2 - j_idx2)**2) / (2 * 25))\n        \n        j_vec2 = np.arange(1, n2 + 1)\n        xtrue2 = np.cos(j_vec2 / 8)\n        \n        i_vec2 = np.arange(1, m2 + 1)\n        noise2 = 1e-2 * np.sin(i_vec2 / 3)\n        b2 = A2 @ xtrue2 + noise2\n        \n        # Case 3\n        m3, n3 = 20, 10\n        A3 = np.zeros((m3, n3), dtype=np.float64)\n        \n        i_vec3 = np.arange(1, m3 + 1)\n        b3 = (-1)**i_vec3\n        \n        return [(A1, b1), (A2, b2), (A3, b3)]\n\n    def compute_gcv_outputs(A, b):\n        \"\"\"\n        Computes the optimal alpha and corresponding outputs using GCV.\n        \"\"\"\n        m, n = A.shape\n        \n        # Step 1: Compute economy SVD\n        U, s, Vh = np.linalg.svd(A, full_matrices=False)\n        \n        # Step 2: Pre-compute SVD-based quantities\n        btilde = U.T @ b  # Components u_i^T * b\n        b_norm_sq = np.linalg.norm(b)**2\n        b_ortho_norm_sq = b_norm_sq - np.sum(btilde**2)\n        s_sq = s**2\n        k = len(s)\n\n        def gcv_func_vectorized(alphas):\n            \"\"\"\n            Calculates GCV values for a vector of alphas.\n            \"\"\"\n            alphas_sq = alphas[:, np.newaxis]**2  # Shape (num_alphas, 1)\n            f = s_sq / (s_sq + alphas_sq)         # Shape (num_alphas, k) using broadcasting\n            \n            trace_H = np.sum(f, axis=1) # Shape (num_alphas,)\n            \n            # Residual norm calculation\n            # (1-f_i)^2 = (alpha^2 / (sigma_i^2 + alpha^2))^2\n            term1 = np.sum(((1 - f)**2) * (btilde**2), axis=1) # Shape (num_alphas,)\n            residual_norm_sq = term1 + b_ortho_norm_sq\n            \n            denom = m - trace_H\n            # Handle potential division by zero\n            gcv_vals = np.full_like(denom, np.inf)\n            safe_indices = ~np.isclose(denom, 0)\n            gcv_vals[safe_indices] = m * residual_norm_sq[safe_indices] / (denom[safe_indices]**2)\n            \n            return gcv_vals\n\n        # Step 3: Search for optimal alpha\n        # Coarse Search\n        alphas1 = np.logspace(-8, 2, 200)\n        gcv_values1 = gcv_func_vectorized(alphas1)\n        min_idx1 = np.argmin(gcv_values1)\n        alpha_min1 = alphas1[min_idx1]\n        \n        # Refined Search\n        lower_bound = max(1e-8, alpha_min1 / 10)\n        upper_bound = min(1e2, alpha_min1 * 10)\n        alphas2 = np.logspace(np.log10(lower_bound), np.log10(upper_bound), 200)\n        gcv_values2 = gcv_func_vectorized(alphas2)\n        min_idx2 = np.argmin(gcv_values2)\n        alpha_star = alphas2[min_idx2]\n        \n        # Step 4: Calculate final results for alpha_star\n        alpha_star_sq = alpha_star**2\n        f_star = s_sq / (s_sq + alpha_star_sq)\n        \n        trace_H_star = np.sum(f_star)\n        \n        term1_star = np.sum(((1 - f_star)**2) * (btilde**2))\n        residual_norm_sq_star = term1_star + b_ortho_norm_sq\n        \n        return alpha_star, residual_norm_sq_star, trace_H_star\n\n    test_cases = generate_test_cases()\n    results = []\n\n    for A, b in test_cases:\n        alpha_star, res_norm_sq, trace_H = compute_gcv_outputs(A, b)\n        results.append(f\"[{alpha_star:.6f},{res_norm_sq:.6f},{trace_H:.6f}]\")\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3419911"}, {"introduction": "After implementing GCV, a crucial question arises: how effective is it? Since GCV serves as a proxy for minimizing the unknown predictive error, it's valuable to assess its performance in a controlled setting. In this exercise [@problem_id:3283866], you will compare the parameter chosen by GCV against the \"oracle\" parameter—the one that truly minimizes the error with a known solution—to quantify GCV's effectiveness and build intuition for its performance.", "problem": "Consider a linear inverse problem in which an unknown vector $x \\in \\mathbb{R}^{n}$ is estimated from data $b \\in \\mathbb{R}^{n}$ related by a known matrix $A \\in \\mathbb{R}^{n \\times n}$ through the model $b = A x + \\varepsilon$, where $\\varepsilon$ represents additive noise. Assume the unknown true solution $x_{true}$ is known for the purposes of evaluation. The estimate $x_{\\alpha}$ is obtained by minimizing the quadratic functional $J_{\\alpha}(x) = \\lVert A x - b \\rVert_{2}^{2} + \\alpha^{2} \\lVert x \\rVert_{2}^{2}$ with respect to $x$, where $\\alpha > 0$ is a regularization parameter. Your task is to choose $\\alpha$ to minimize the discrepancy $\\lVert x_{\\alpha} - x_{true} \\rVert_{2}$ and compare this choice to the $\\alpha$ selected by Generalized Cross-Validation (GCV). Generalized Cross-Validation (GCV) is a method that selects $\\alpha$ by minimizing the function $G(\\alpha) = \\lVert (I - S(\\alpha)) b \\rVert_{2}^{2} / \\left(\\operatorname{trace}(I - S(\\alpha))\\right)^{2}$, where $S(\\alpha)$ is the linear smoother that maps $b$ to the fitted data $A x_{\\alpha}$.\n\nFundamental base to use:\n- The minimizer of a strictly convex quadratic functional is characterized by setting its gradient to zero.\n- The smoothing matrix for a linear estimator is the linear operator $S(\\alpha)$ that maps the data $b$ to the fitted values $A x_{\\alpha}$.\n\nImplementation details:\n- For all test cases, use the square matrix $A$ defined by diagonal singular values on the diagonal. Specifically, for index $i$ with $1 \\leq i \\leq n$, set $A_{ii} = s_{i}$ and $A_{ij} = 0$ for $i \\neq j$, where the sequence $\\{s_{i}\\}$ is described per test case below.\n- Define the true solution as $x_{true} \\in \\mathbb{R}^{n}$ with components $x_{true,i} = \\sin\\left( \\frac{2\\pi i}{n} \\right) + \\frac{1}{2} \\cos\\left( \\frac{\\pi i}{n} \\right)$ for $i = 1, 2, \\dots, n$. All angles must be in radians.\n- Define the noise vector $\\varepsilon \\in \\mathbb{R}^{n}$ with components $\\varepsilon_{j} = \\sigma \\sin(j)$ for $j = 1, 2, \\dots, n$, where $\\sigma$ is the noise level specified per test case. All angles must be in radians.\n- For evaluation of $\\alpha$, use a grid of $N_{\\alpha}$ logarithmically spaced values in the interval $[10^{-8}, 10^{0}]$, excluding $\\alpha = 0$. Specifically, set $N_{\\alpha} = 121$ and sample $\\alpha$ values uniformly in logarithmic scale from $10^{-8}$ to $10^{0}$.\n\nFor each $\\alpha$ in the grid:\n- Compute $x_{\\alpha}$ by minimizing $J_{\\alpha}(x)$.\n- Compute the error norm $\\lVert x_{\\alpha} - x_{true} \\rVert_{2}$.\n- Compute the smoothing matrix $S(\\alpha)$ associated with the estimator $A x_{\\alpha}$ and then compute the GCV score $G(\\alpha)$.\n\nFor each test case:\n- Select the optimal $\\alpha$ (denoted $\\alpha_{opt}$) that minimizes $\\lVert x_{\\alpha} - x_{true} \\rVert_{2}$ over the grid.\n- Select the $\\alpha$ chosen by Generalized Cross-Validation (denoted $\\alpha_{gcv}$) that minimizes $G(\\alpha)$ over the grid.\n- Compute the ratio $R = \\lVert x_{\\alpha_{gcv}} - x_{true} \\rVert_{2} / \\lVert x_{\\alpha_{opt}} - x_{true} \\rVert_{2}$.\n\nTest suite:\n- Test case $1$: $n = 20$, $s_{i} = i^{-2}$, $\\sigma = 10^{-2}$.\n- Test case $2$: $n = 20$, $s_{i} = 10^{-i/4}$, $\\sigma = 0$.\n- Test case $3$: $n = 30$, $s_{i} = i^{-3}$, $\\sigma = 5 \\cdot 10^{-2}$.\n\nYour program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case result must itself be a list of three floats $[\\alpha_{opt}, \\alpha_{gcv}, R]$, with each float rounded to $6$ decimal places. Therefore, the final output must have the form $[[\\alpha_{opt}^{(1)}, \\alpha_{gcv}^{(1)}, R^{(1)}],[\\alpha_{opt}^{(2)}, \\alpha_{gcv}^{(2)}, R^{(2)}],[\\alpha_{opt}^{(3)}, \\alpha_{gcv}^{(3)}, R^{(3)}]]$.", "solution": "The problem requires the determination of the Tikhonov regularization parameter $\\alpha$ for a linear inverse problem. We must find the parameter $\\alpha_{opt}$ that minimizes the true error norm $\\lVert x_{\\alpha} - x_{true} \\rVert_{2}$ and compare it to the parameter $\\alpha_{gcv}$ selected by the Generalized Cross-Validation (GCV) method.\n\nFirst, we derive the expression for the regularized solution $x_{\\alpha}$. The solution is defined as the minimizer of the Tikhonov functional:\n$$J_{\\alpha}(x) = \\lVert A x - b \\rVert_{2}^{2} + \\alpha^{2} \\lVert x \\rVert_{2}^{2}$$\nwhere $\\alpha > 0$ is the regularization parameter. This is a quadratic functional in $x$. We can expand it as:\n$$J_{\\alpha}(x) = (A x - b)^T (A x - b) + \\alpha^2 x^T x = x^T A^T A x - 2 b^T A x + b^T b + \\alpha^2 x^T x$$\nThe functional $J_{\\alpha}(x)$ is strictly convex for $\\alpha > 0$, so its unique minimum can be found by setting its gradient with respect to $x$ to zero.\n$$\\nabla_x J_{\\alpha}(x) = 2 A^T A x - 2 A^T b + 2 \\alpha^2 I x = 0$$\nwhere $I$ is the identity matrix. Rearranging the terms, we get:\n$$(A^T A + \\alpha^2 I) x = A^T b$$\nThe regularized solution $x_{\\alpha}$ is therefore given by:\n$$x_{\\alpha} = (A^T A + \\alpha^2 I)^{-1} A^T b$$\nThis is the standard form of the Tikhonov-regularized solution. The problem specifies that the matrix $A$ is a square diagonal matrix with entries $A_{ii} = s_i$ and $A_{ij} = 0$ for $i \\neq j$. Thus, $A = \\mathrm{diag}(s_1, s_2, \\dots, s_n)$. Since $A$ is real and diagonal, it is symmetric, so $A^T = A$. The expression for $x_{\\alpha}$ simplifies.\nThe matrices involved are diagonal:\n- $A^T A = A^2 = \\mathrm{diag}(s_1^2, s_2^2, \\dots, s_n^2)$\n- $A^T A + \\alpha^2 I = \\mathrm{diag}(s_1^2 + \\alpha^2, s_2^2 + \\alpha^2, \\dots, s_n^2 + \\alpha^2)$\n- $(A^T A + \\alpha^2 I)^{-1} = \\mathrm{diag}\\left(\\frac{1}{s_1^2 + \\alpha^2}, \\dots, \\frac{1}{s_n^2 + \\alpha^2}\\right)$\nThe solution vector $x_{\\alpha}$ can be computed component-wise. The $i$-th component of $x_{\\alpha}$ is:\n$$(x_{\\alpha})_i = \\left(\\frac{1}{s_i^2 + \\alpha^2}\\right) (s_i) (b_i) = \\frac{s_i}{s_i^2 + \\alpha^2} b_i$$\nThese terms $\\frac{s_i}{s_i^2 + \\alpha^2}$ are known as filter factors.\n\nNext, we address the Generalized Cross-Validation (GCV) method. The GCV function is defined as:\n$$G(\\alpha) = \\frac{\\lVert (I - S(\\alpha)) b \\rVert_{2}^{2}}{\\left(\\operatorname{trace}(I - S(\\alpha))\\right)^{2}}$$\nwhere $S(\\alpha)$ is the smoothing matrix that maps the data vector $b$ to the fitted data $A x_{\\alpha}$, i.e., $A x_{\\alpha} = S(\\alpha) b$. From the expression for $x_{\\alpha}$, we can derive $S(\\alpha)$:\n$$S(\\alpha) = A (A^T A + \\alpha^2 I)^{-1} A^T$$\nGiven that $A$ is diagonal, $S(\\alpha)$ is also diagonal:\n$$S(\\alpha) = \\mathrm{diag}(s_i) \\cdot \\mathrm{diag}\\left(\\frac{1}{s_i^2 + \\alpha^2}\\right) \\cdot \\mathrm{diag}(s_i) = \\mathrm{diag}\\left(\\frac{s_i^2}{s_i^2 + \\alpha^2}\\right)$$\nNow we can evaluate the two parts of the GCV function. The term $I - S(\\alpha)$ is a diagonal matrix:\n$$I - S(\\alpha) = \\mathrm{diag}\\left(1 - \\frac{s_i^2}{s_i^2 + \\alpha^2}\\right) = \\mathrm{diag}\\left(\\frac{\\alpha^2}{s_i^2 + \\alpha^2}\\right)$$\nThe trace of this matrix is the sum of its diagonal elements:\n$$\\operatorname{trace}(I - S(\\alpha)) = \\sum_{i=1}^{n} \\frac{\\alpha^2}{s_i^2 + \\alpha^2}$$\nThe numerator of $G(\\alpha)$ is the squared norm of the residual vector $(I - S(\\alpha)) b$:\n$$\\lVert (I - S(\\alpha)) b \\rVert_{2}^{2} = \\sum_{i=1}^{n} \\left( \\left(\\frac{\\alpha^2}{s_i^2 + \\alpha^2}\\right) b_i \\right)^2$$\nThus, the GCV function to be minimized is:\n$$G(\\alpha) = \\frac{\\sum_{i=1}^{n} \\left(\\frac{\\alpha^2 b_i}{s_i^2 + \\alpha^2}\\right)^2}{\\left(\\sum_{i=1}^{n} \\frac{\\alpha^2}{s_i^2 + \\alpha^2}\\right)^2}$$\n\nThe overall algorithm proceeds as follows for each test case:\n1.  Define the parameters $n$, $\\sigma$, and the function for singular values $s_i$.\n2.  Construct the vectors $s_i$, $x_{true,i}$, and $\\varepsilon_i$ according to the problem specifications for $i=1, \\dots, n$.\n3.  Compute the data vector $b$ using the model $b_i = s_i x_{true,i} + \\varepsilon_i$.\n4.  Create a logarithmically spaced grid of $N_{\\alpha}=121$ values for $\\alpha$ from $10^{-8}$ to $10^{0}$.\n5.  For each $\\alpha$ in the grid:\n    a. Compute the regularized solution vector $x_{\\alpha}$ using $(x_{\\alpha})_i = \\frac{s_i}{s_i^2 + \\alpha^2} b_i$.\n    b. Compute and store the error norm $\\lVert x_{\\alpha} - x_{true} \\rVert_2$.\n    c. Compute and store the GCV score $G(\\alpha)$ using the derived formula.\n6.  Find the value $\\alpha_{opt}$ in the grid that minimizes the error norm. This corresponds to the minimal value in the stored list of errors.\n7.  Find the value $\\alpha_{gcv}$ in the grid that minimizes the GCV score. This corresponds to the minimal value in the stored list of GCV scores.\n8.  Calculate the ratio $R = \\frac{\\lVert x_{\\alpha_{gcv}} - x_{true} \\rVert_{2}}{\\lVert x_{\\alpha_{opt}} - x_{true} \\rVert_{2}}$, where $\\lVert x_{\\alpha_{opt}} - x_{true} \\rVert_{2}$ is the minimum error found in step 6, and $\\lVert x_{\\alpha_{gcv}} - x_{true} \\rVert_{2}$ is the error corresponding to $\\alpha_{gcv}$.\n9.  Store the results $[\\alpha_{opt}, \\alpha_{gcv}, R]$ for the test case.\n\nThis procedure is repeated for all three test cases, and the results are aggregated and formatted as specified.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final result.\n    \"\"\"\n\n    def process_case(n, s_func, sigma):\n        \"\"\"\n        Solves a single test case for Tikhonov regularization and GCV.\n\n        Args:\n            n (int): The dimension of the problem.\n            s_func (function): A function that takes an array of indices i and returns singular values s_i.\n            sigma (float): The noise level.\n\n        Returns:\n            list: A list containing [alpha_opt, alpha_gcv, R] rounded to 6 decimal places.\n        \"\"\"\n        # 1. Generate problem data: s, x_true, noise, b\n        i_vals = np.arange(1.0, n + 1.0)\n        s = s_func(i_vals)\n        \n        x_true = np.sin(2 * np.pi * i_vals / n) + 0.5 * np.cos(np.pi * i_vals / n)\n        \n        if sigma == 0.0:\n            noise = np.zeros(n)\n        else:\n            noise = sigma * np.sin(i_vals)\n            \n        b = s * x_true + noise\n\n        # 2. Setup alpha grid for regularization parameter search\n        alphas = np.logspace(-8, 0, 121)\n        \n        errors = []\n        gcv_scores = []\n        \n        s2 = s**2\n\n        # 3. Loop over all alpha values to compute errors and GCV scores\n        for alpha in alphas:\n            alpha2 = alpha**2\n            \n            # Compute the Tikhonov-regularized solution x_alpha\n            # (x_alpha)_i = (s_i / (s_i^2 + alpha^2)) * b_i\n            filter_factors = s / (s2 + alpha2)\n            x_alpha = filter_factors * b\n            \n            # Compute the error norm ||x_alpha - x_true||_2\n            error = np.linalg.norm(x_alpha - x_true)\n            errors.append(error)\n            \n            # Compute the GCV score G(alpha)\n            # Numerator: ||(I - S(alpha))b||^2 = sum_i ((alpha^2 * b_i) / (s_i^2 + alpha^2))^2\n            # Denominator: (tr(I - S(alpha)))^2 = (sum_i alpha^2 / (s_i^2 + alpha^2))^2\n            common_term_gcv = alpha2 / (s2 + alpha2)\n            \n            gcv_num = np.sum((common_term_gcv * b)**2)\n            gcv_den = np.sum(common_term_gcv)**2\n            \n            if gcv_den == 0.0:\n                gcv_score = np.inf\n            else:\n                gcv_score = gcv_num / gcv_den\n            gcv_scores.append(gcv_score)\n\n        # 4. Find optimal alpha and GCV-chosen alpha from the grid\n        errors = np.array(errors)\n        gcv_scores = np.array(gcv_scores)\n        \n        # Find alpha_opt, which minimizes the true error\n        idx_opt = np.argmin(errors)\n        alpha_opt = alphas[idx_opt]\n        min_error = errors[idx_opt]\n        \n        # Find alpha_gcv, which minimizes the GCV score\n        idx_gcv = np.argmin(gcv_scores)\n        alpha_gcv = alphas[idx_gcv]\n        \n        # 5. Compute the performance ratio R\n        error_at_gcv = errors[idx_gcv]\n        \n        if min_error == 0.0:\n            R = 1.0 if error_at_gcv == 0.0 else np.inf\n        else:\n            R = error_at_gcv / min_error\n            \n        return [alpha_opt, alpha_gcv, R]\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (n, s_func, sigma)\n        {'n': 20, 's_func': lambda i: i**(-2), 'sigma': 1e-2},\n        {'n': 20, 's_func': lambda i: 10**(-i/4.0), 'sigma': 0.0},\n        {'n': 30, 's_func': lambda i: i**(-3), 'sigma': 5e-2},\n    ]\n\n    all_results = []\n    for case in test_cases:\n        result = process_case(case['n'], case['s_func'], case['sigma'])\n        all_results.append(result)\n\n    # Final print statement in the exact required format.\n    inner_parts = []\n    for r in all_results:\n        inner_parts.append(f\"[{r[0]:.6f},{r[1]:.6f},{r[2]:.6f}]\")\n    output_str = f\"[{','.join(inner_parts)}]\"\n    print(output_str)\n\nsolve()\n```", "id": "3283866"}, {"introduction": "The theoretical underpinnings of GCV are based on assumptions of linearity and statistical noise, which may not hold in all practical scenarios. This final practice [@problem_id:3385851] delves into a critical limitation by exploring the naive application of GCV within a nonlinear Gauss-Newton iterative scheme. You will construct a counterexample demonstrating how GCV can misinterpret deterministic linearization error as noise, leading to over-smoothing and solver stagnation, a crucial lesson for any practitioner.", "problem": "Consider the nonlinear inverse problem of estimating the unknown parameter vector $x \\in \\mathbb{R}^n$ from data $y \\in \\mathbb{R}^m$ under the forward map $F: \\mathbb{R}^n \\to \\mathbb{R}^m$. Assume the observation model is noiseless, that is, $y = F(x^\\star)$ for a fixed true parameter $x^\\star$. The goal is to analyze the behavior of a Gauss–Newton scheme that, at iteration $k$, linearizes the forward map around the current iterate $x_k$, computes the Jacobian $J_k = \\nabla F(x_k)$, and determines a step $d_k$ by solving a Tikhonov-regularized linear least-squares subproblem of the form\n$$\n\\min_{d \\in \\mathbb{R}^n} \\ \\|J_k d - r_k\\|_2^2 + \\alpha_k^2 \\|d\\|_2^2,\n$$\nwhere $r_k = y - F(x_k)$ is the current residual and $\\alpha_k \\ge 0$ is a regularization parameter. In the naive generalized cross-validation (GCV) approach applied to the nonlinear problem, $\\alpha_k$ is selected at each iteration by minimizing the generalized cross-validation functional associated with the linearized subproblem based on $J_k$ and $r_k$.\n\nConstruct a counterexample in which the naive use of generalized cross-validation in the nonlinear setting selects overly large $\\alpha_k$, thereby over-smoothing along informative directions and causing stagnation of the Gauss–Newton iterations. Your construction must be explicit and algorithmic, and it must be demonstrably verifiable by computation.\n\nUse the following explicit forward map with $n = 2$ and $m = 3$:\n$$\nF(x) = \\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\ns \\tanh\\!\\big(\\gamma (x_1 + x_2)\\big)\n\\end{bmatrix},\n$$\nwhere $x = (x_1,x_2)^\\top$, $s > 0$ is a scale parameter, and $\\gamma > 0$ controls the strength of the nonlinearity. The Jacobian is\n$$\nJ(x) = \\begin{bmatrix}\n1 & 0 \\\\\n0 & 1 \\\\\ns \\gamma \\operatorname{sech}^2\\!\\big(\\gamma (x_1 + x_2)\\big) & s \\gamma \\operatorname{sech}^2\\!\\big(\\gamma (x_1 + x_2)\\big)\n\\end{bmatrix}.\n$$\n\nThe target solution is fixed to $x^\\star = (1,1)^\\top$ and the observations are $y = F(x^\\star)$. Initialize the Gauss–Newton method at a specified $x_0$ and update via $x_{k+1} = x_k + d_k$. At each iteration, choose $\\alpha_k$ via naive generalized cross-validation applied to the linearized subproblem (that is, treating $J_k$ and $r_k$ as if they arise from a linear model with Tikhonov regularization with identity penalty). Run the iteration until either a maximum iteration count is reached or a stagnation criterion is met.\n\nYour program must implement two schemes:\n- A naive generalized cross-validation scheme that selects $\\alpha_k$ at each iteration solely from the linearized subproblem at $x_k$.\n- A fixed-regularization baseline Gauss–Newton scheme with a small constant $\\alpha > 0$ at all iterations.\n\nDefine stagnation of the naive generalized cross-validation scheme as follows: the final residual norm ratio $\\|r_{\\text{final}}\\|_2 / \\|r_{\\text{initial}}\\|_2$ is greater than or equal to $0.95$, while the baseline fixed-regularization scheme achieves a residual norm ratio less than or equal to $0.70$. This criterion captures the phenomenon where generalized cross-validation over-smooths along informative directions (here the first two components) and impedes progress, whereas a modest fixed-regularization allows progress.\n\nUse the following test suite, which varies the nonlinearity strength $\\gamma$ and the initial point $x_0$, with $s = 1$ fixed:\n- Test case $1$ (strong nonlinearity, far initial point): $\\gamma = 10$, $x_0 = (-1,-1)^\\top$.\n- Test case $2$ (moderate nonlinearity, far initial point): $\\gamma = 0.5$, $x_0 = (-1,-1)^\\top$.\n- Test case $3$ (strong nonlinearity, near initial point): $\\gamma = 10$, $x_0 = (0.9,0.9)^\\top$.\n\nFor each test case, run both schemes with the same maximum iteration count and update rule. The final output must be a single line containing a list of booleans indicating, for each test case in order, whether the naive generalized cross-validation scheme stagnates according to the criterion defined above.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[true,false,true]\"). The booleans must be formatted as Python boolean literals, that is, \"True\" or \"False\".", "solution": "The problem of estimating an unknown parameter vector $x \\in \\mathbb{R}^n$ from data $y \\in \\mathbb{R}^m$ via a nonlinear forward map $F(x)$ is a cornerstone of scientific computing. The Gauss–Newton method is a standard iterative technique for solving such nonlinear least-squares problems. At each iteration $k$, the method linearizes the problem around the current estimate $x_k$ and solves a linear least-squares subproblem to find an update step $d_k$. To stabilize the solution in the presence of ill-conditioning, this subproblem is often regularized, for instance, using the Tikhonov method:\n$$\n\\min_{d \\in \\mathbb{R}^n} \\ \\|J_k d - r_k\\|_2^2 + \\alpha_k^2 \\|d\\|_2^2,\n$$\nwhere $J_k = \\nabla F(x_k)$ is the Jacobian and $r_k = y - F(x_k)$ is the residual. A critical aspect of this approach is the choice of the regularization parameter $\\alpha_k$.\n\nGeneralized Cross-Validation (GCV) is a powerful method for choosing $\\alpha_k$ in linear inverse problems of the form $y = Kx + \\epsilon$, where $\\epsilon$ is white noise. The GCV functional $V(\\alpha)$ is designed to estimate the predictive error and is minimized to find an optimal $\\alpha$. Its derivation assumes that the residual vector for the linear system is dominated by statistical noise.\n\nThe \"naive\" application of GCV in the nonlinear Gauss–Newton context involves treating the linearized subproblem at each step as a self-contained linear inverse problem. That is, for the system $J_k d_k \\approx r_k$, GCV is used to find $\\alpha_k$ by minimizing the functional:\n$$\nV(\\alpha_k) = \\frac{\\|J_k d_k(\\alpha_k) - r_k\\|_2^2}{\\left( \\operatorname{Tr}\\left(I - A_k(\\alpha_k)\\right) \\right)^2},\n$$\nwhere $A_k(\\alpha_k) = J_k(J_k^\\top J_k + \\alpha_k^2 I)^{-1}J_k^\\top$ is the influence matrix.\n\nThe fundamental flaw in this naive approach, which this counterexample is designed to expose, is that the residual $r_k = F(x^\\star) - F(x_k)$ in a nonlinear problem is not analogous to statistical noise. It is dominated by **linearization error**, especially when the iterate $x_k$ is far from the true solution $x^\\star$ and the function $F$ is highly nonlinear. That is, $r_k = J_k(x^\\star - x_k) + \\mathcal{O}(\\|x^\\star - x_k\\|^2)$. The higher-order terms are deterministic, not stochastic. GCV, unable to distinguish this deterministic structure from noise, misinterprets large linearization errors as large noise. It consequently selects an overly large regularization parameter $\\alpha_k$ to \"smooth\" this perceived noise, which excessively damps the update step $d_k$ and causes the iteration to stall, or stagnate.\n\nWe construct the counterexample using the specified forward map $F(x) = [x_1, x_2, s \\tanh(\\gamma(x_1+x_2))]^\\top$. The parameter $\\gamma$ controls the nonlinearity. When $\\gamma$ is large, the $\\tanh$ function saturates quickly, meaning its derivative, $\\operatorname{sech}^2$, becomes nearly zero for arguments with large magnitude. This structural property is key.\n\nLet us analyze Test Case 1: $\\gamma=10$, $x_0 = (-1,-1)^\\top$, $s=1$, and $x^\\star = (1,1)^\\top$.\nThe initial iterate $x_0$ is \"far\" from the solution $x^\\star$. The argument to the $\\tanh$ function at $x_0$ is $\\gamma(x_{0,1}+x_{0,2}) = 10(-2) = -20$. This is deep in the saturation region of the $\\tanh$ function.\nThe Jacobian at $x_0$ is:\n$$\nJ(x_0) = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 10 \\operatorname{sech}^2(-20) & 10 \\operatorname{sech}^2(-20) \\end{bmatrix}.\n$$\nSince $\\operatorname{sech}^2(-20) \\approx 4.2 \\times 10^{-18}$, the third row of the Jacobian is effectively zero. Thus, $J_0 \\approx \\begin{bsmallmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 0 & 0 \\end{bsmallmatrix}$. The range of $J_0$ is essentially the $xy$-plane in $\\mathbb{R}^3$.\n\nThe true data is $y = F(x^\\star) = [1, 1, \\tanh(20)]^\\top \\approx [1, 1, 1]^\\top$.\nThe model output at $x_0$ is $F(x_0) = [-1, -1, \\tanh(-20)]^\\top \\approx [-1, -1, -1]^\\top$.\nThe initial residual is $r_0 = y - F(x_0) \\approx [2, 2, 2]^\\top$.\n\nThe crucial observation is that $r_0$ has a large component, $[0,0,2]^\\top$, that is orthogonal to the range of $J_0$. The linear model $J_0 d$ is incapable of explaining this component of the residual. The GCV functional interprets this large, unexplainable residual component as evidence of massive noise, and its minimization procedure dictates a very large value for $\\alpha_0$ to suppress it. For a minimizer of the GCV functional that is very large ($\\alpha_0 \\to \\infty$), the resulting regularized step is $d_0 \\approx (J_0^\\top J_0 + \\alpha_0^2 I)^{-1} J_0^\\top r_0 \\approx \\frac{1}{\\alpha_0^2} J_0^\\top r_0$, which becomes vanishingly small. The Gauss–Newton iteration therefore makes negligible progress, i.e., $x_1 \\approx x_0$, and the process stagnates.\n\nIn contrast, the baseline scheme with a small, fixed $\\alpha$ is essentially performing a standard Gauss–Newton step: $d_0 \\approx J_0^\\dagger r_0 = [2,2]^\\top$. This yields the update $x_1 = x_0 + d_0 = (-1,-1)^\\top + (2,2)^\\top = (1,1)^\\top = x^\\star$, achieving convergence in a single iteration. This starkly demonstrates the failure of naive GCV.\n\nFor Test Case 2 ($\\gamma=0.5$), the nonlinearity is weak. The Jacobian is well-conditioned everywhere, and the linearization error is small. GCV performs well, choosing a reasonable $\\alpha_k$, and does not stagnate.\nFor Test Case 3 ($\\gamma=10$, $x_0=(0.9,0.9)^\\top$), although nonlinearity is strong, the initial guess is close to the solution. The residual $r_0 \\approx [0.1, 0.1, 0]^\\top$ is small and lies almost entirely within the range of $J_0$. GCV correctly identifies that little to no smoothing is needed, selects $\\alpha_0 \\approx 0$, and the method converges rapidly.\n\nThe provided program computationally implements these two schemes—naive GCV and fixed-alpha baseline—and applies them to the three test cases. It calculates the final-to-initial residual norm ratio for each and evaluates the prescribed stagnation criterion, thereby verifying the predicted stagnation in the first case and successful convergence in the others.", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import minimize_scalar\n\ndef solve():\n    \"\"\"\n    Implements and tests two Gauss-Newton schemes to demonstrate the\n    failure of naive Generalized Cross-Validation (GCV) for a specific\n    nonlinear inverse problem.\n    \"\"\"\n    # --- Constants  Parameters from problem statement and implementation choices ---\n    S = 1.0\n    X_STAR = np.array([1.0, 1.0])\n    MAX_ITER = 50\n    FIXED_ALPHA = 1e-6\n    STAGNATION_THRESHOLD_GCV = 0.95\n    CONVERGENCE_THRESHOLD_FIXED = 0.70\n\n    # --- Problem-specific functions: Forward Map and Jacobian ---\n    def forward_map(x, s, gamma):\n        x1, x2 = x\n        val = s * np.tanh(gamma * (x1 + x2))\n        return np.array([x1, x2, val])\n\n    def jacobian(x, s, gamma):\n        x1, x2 = x\n        cosh_arg = gamma * (x1 + x2)\n        # Avoid overflow for large arguments in cosh\n        if np.abs(cosh_arg) > 30:\n            sech_val = 0.0\n        else:\n            sech_val = 1.0 / np.cosh(cosh_arg)\n        \n        d_tanh = s * gamma * sech_val**2\n        return np.array([\n            [1.0, 0.0],\n            [0.0, 1.0],\n            [d_tanh, d_tanh]\n        ])\n\n    # --- GCV Functional Implementation ---\n    def gcv_function(alpha, J, r):\n        m, n = J.shape\n        alpha2 = alpha**2\n        \n        try:\n            # SVD is the standard, numerically stable way to analyze the GCV functional.\n            U, s_vals, _ = np.linalg.svd(J, full_matrices=True)\n        except np.linalg.LinAlgError:\n            return np.inf\n\n        s2 = s_vals**2\n        r_hat = U.T @ r\n\n        # Numerator: ||(I - A)r||^2 where A is the influence matrix\n        f_res = alpha2 / (s2 + alpha2)\n        num_term1 = np.sum((f_res * r_hat[:n])**2) # Component in range(J)\n        num_term2 = np.sum(r_hat[n:]**2)      # Component in null(J^T)\n        numerator = num_term1 + num_term2\n\n        # Denominator: (Tr(I - A))^2\n        trace_I_minus_A = (m - n) + np.sum(alpha2 / (s2 + alpha2))\n        \n        if trace_I_minus_A  1e-15:\n            return np.inf\n            \n        denominator = trace_I_minus_A**2\n        \n        return numerator / denominator\n\n    # --- Gauss-Newton Solver ---\n    def solve_gauss_newton(x0, gamma, s, y_true, method, fixed_alpha, max_iter):\n        x_k = np.copy(x0.astype(np.float64))\n        \n        initial_residual = y_true - forward_map(x_k, s, gamma)\n        initial_residual_norm = np.linalg.norm(initial_residual)\n\n        if initial_residual_norm  1e-12: # Already converged\n            return 0.0\n\n        for _ in range(max_iter):\n            r_k = y_true - forward_map(x_k, s, gamma)\n            J_k = jacobian(x_k, s, gamma)\n\n            if method == 'gcv':\n                # Find alpha_k by minimizing the GCV functional on a log-spaced grid or via optimizer\n                res = minimize_scalar(\n                    lambda alpha: gcv_function(alpha, J_k, r_k),\n                    bounds=(1e-10, 1e4), # A wide search range for alpha\n                    method='bounded'\n                )\n                alpha_k = res.x\n            elif method == 'fixed':\n                alpha_k = fixed_alpha\n            else:\n                raise ValueError(\"Invalid method specified.\")\n\n            # Solve the Tikhonov-regularized normal equations for the step d_k\n            # (J_k^T J_k + alpha_k^2 I) d_k = J_k^T r_k\n            A = J_k.T @ J_k + (alpha_k**2) * np.identity(x_k.shape[0])\n            b = J_k.T @ r_k\n            \n            try:\n                d_k = np.linalg.solve(A, b)\n            except np.linalg.LinAlgError:\n                # If matrix is singular, iteration cannot proceed.\n                break\n\n            x_k += d_k\n        \n        final_residual_norm = np.linalg.norm(y_true - forward_map(x_k, s, gamma))\n        \n        return final_residual_norm / initial_residual_norm\n\n    # --- Main Driver Script ---\n    test_cases = [\n        # (gamma, x0)\n        (10.0, np.array([-1.0, -1.0])),\n        (0.5, np.array([-1.0, -1.0])),\n        (10.0, np.array([0.9, 0.9]))\n    ]\n    \n    results = []\n\n    for gamma, x0 in test_cases:\n        # The true data y depends on the forward map's parameters\n        y_true = forward_map(X_STAR, S, gamma)\n\n        # Run with Naive GCV\n        ratio_gcv = solve_gauss_newton(\n            x0=x0, gamma=gamma, s=S, y_true=y_true,\n            method='gcv', fixed_alpha=None, max_iter=MAX_ITER\n        )\n\n        # Run with Fixed Alpha Baseline\n        ratio_fixed = solve_gauss_newton(\n            x0=x0, gamma=gamma, s=S, y_true=y_true,\n            method='fixed', fixed_alpha=FIXED_ALPHA, max_iter=MAX_ITER\n        )\n\n        # Evaluate the stagnation criterion from the problem description\n        stagnated = (\n            ratio_gcv >= STAGNATION_THRESHOLD_GCV and\n            ratio_fixed = CONVERGENCE_THRESHOLD_FIXED\n        )\n        results.append(stagnated)\n\n    # Print the final result in the exact specified format\n    print(f\"[{','.join(str(r).capitalize() for r in results)}]\")\n\nsolve()\n```", "id": "3385851"}]}