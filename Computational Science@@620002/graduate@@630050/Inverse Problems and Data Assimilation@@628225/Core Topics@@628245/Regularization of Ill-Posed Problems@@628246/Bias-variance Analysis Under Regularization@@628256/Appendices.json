{"hands_on_practices": [{"introduction": "Regularization integrates prior knowledge into an inverse problem, fundamentally altering the properties of the estimator. This can be achieved through 'hard' constraints that force the solution into a specific subspace, or 'soft' penalties that discourage but do not forbid certain features. This foundational exercise [@problem_id:3368100] provides a direct comparison of these two approaches, guiding you to derive the exact bias and variance introduced by each, offering a crystal-clear illustration of the bias-variance trade-off in action.", "problem": "Consider a linear inverse problem in three dimensions where a state vector $x \\in \\mathbb{R}^{3}$ is observed through noisy identity measurements. The observation model is $y = x^{\\star} + \\varepsilon$, where the true state is $x^{\\star} = \\begin{pmatrix} 2 \\\\ -1 \\\\ 3 \\end{pmatrix}$ and the noise $\\varepsilon$ has a multivariate Normal distribution with zero mean and covariance $\\sigma^{2} I_{3}$, with $\\sigma^{2} = 1$ and $I_{3}$ the $3 \\times 3$ identity matrix. Let $L \\in \\mathbb{R}^{2 \\times 3}$ be the linear operator\n$$\nL = \\begin{pmatrix}\n1 & 0 & 0 \\\\\n0 & 1 & 0\n\\end{pmatrix},\n$$\nwhich enforces a feasibility restriction on the first two components of $x$. Two regularization strategies are considered:\n\n1. A hard feasibility constraint $L x = 0$, which restricts admissible estimates to the null space of $L$.\n\n2. A soft quadratic penalty with strength $\\lambda = 2$, which adds $\\lambda \\|L x\\|^{2}$ to the data misfit.\n\nStarting from the definitions of least squares estimation, linear constraints, and the properties of multivariate Normal noise (zero-mean, covariance $\\sigma^{2} I_{3}$), derive the estimators under both strategies and compute:\n- The bias vector (defined as the expected estimator minus $x^{\\star}$) induced by the hard feasibility restriction.\n- The change in total variance (defined as the difference in the trace of the estimator’s covariance matrix relative to the unconstrained least squares estimator) due to the hard feasibility restriction.\n- The bias vector induced by the soft quadratic penalty with $\\lambda = 2$.\n- The change in total variance relative to the unconstrained least squares estimator under the soft quadratic penalty with $\\lambda = 2$.\n\nExpress the final answer as a single row matrix containing eight entries in the following order: the three components of the hard-constraint bias vector, the three components of the soft-penalty bias vector, the hard-constraint variance change, and the soft-penalty variance change. Use exact values; no rounding is required. No physical units are involved.", "solution": "The problem is well-posed, scientifically grounded, and provides all necessary information for a unique solution. We proceed with the derivation.\n\nThe inverse problem is described by the observation model $y = x^{\\star} + \\varepsilon$, where $x^{\\star} \\in \\mathbb{R}^{3}$ is the true state, $y \\in \\mathbb{R}^{3}$ is the observation, and $\\varepsilon$ is a noise vector. We are given:\n- The true state: $x^{\\star} = \\begin{pmatrix} 2 \\\\ -1 \\\\ 3 \\end{pmatrix}$.\n- The noise distribution: $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{3})$, with mean $\\mathbb{E}[\\varepsilon] = 0$ and covariance $\\mathrm{Cov}(\\varepsilon) = \\sigma^{2} I_{3}$.\n- The noise variance: $\\sigma^{2} = 1$. Thus, $\\mathrm{Cov}(\\varepsilon) = I_{3}$.\n- The observation model can be written as $y = H x^{\\star} + \\varepsilon$ with the forward operator being the identity matrix, $H = I_{3}$.\n\nWe first analyze the unconstrained least squares estimator, which will serve as a baseline for variance comparison.\n\n**Unconstrained Least Squares Estimator**\nThe unconstrained least squares estimator, $\\hat{x}_{\\text{LS}}$, minimizes the data misfit (or residual sum of squares) $J(x) = \\|y - x\\|^{2}$. The solution is obtained by setting the gradient $\\nabla_{x} J(x) = -2(y-x)$ to zero, which yields:\n$$\n\\hat{x}_{\\text{LS}} = y\n$$\nThe expectation of this estimator is $\\mathbb{E}[\\hat{x}_{\\text{LS}}] = \\mathbb{E}[y] = \\mathbb{E}[x^{\\star} + \\varepsilon] = x^{\\star} + \\mathbb{E}[\\varepsilon] = x^{\\star}$. The bias is $\\text{Bias}(\\hat{x}_{\\text{LS}}) = \\mathbb{E}[\\hat{x}_{\\text{LS}}] - x^{\\star} = 0$, so it is an unbiased estimator.\nThe covariance of this estimator is $\\mathrm{Cov}(\\hat{x}_{\\text{LS}}) = \\mathrm{Cov}(y) = \\mathrm{Cov}(x^{\\star} + \\varepsilon) = \\mathrm{Cov}(\\varepsilon) = \\sigma^{2} I_{3} = I_{3}$.\nThe total variance is the trace of the covariance matrix:\n$$\n\\mathrm{Tr}(\\mathrm{Cov}(\\hat{x}_{\\text{LS}})) = \\mathrm{Tr}(I_{3}) = 1 + 1 + 1 = 3\n$$\n\n**1. Hard Feasibility Constraint**\nThis strategy seeks to minimize $\\|y - x\\|^{2}$ subject to the linear constraint $Lx = 0$. The operator is given by $L = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\end{pmatrix}$.\nThe constraint $Lx=0$ implies:\n$$\n\\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} \\implies x_1 = 0 \\text{ and } x_2 = 0\n$$\nThe estimator must lie in the null space of $L$, which is spanned by the vector $\\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix}$. Thus, the estimator has the form $\\hat{x}_{\\text{hard}} = \\begin{pmatrix} 0 \\\\ 0 \\\\ \\hat{x}_{3} \\end{pmatrix}$.\nSubstituting this into the objective function:\n$$\nJ(x) = (y_1 - 0)^{2} + (y_2 - 0)^{2} + (y_3 - x_3)^{2}\n$$\nTo minimize $J(x)$ with respect to $x_3$, we set the derivative to zero: $\\frac{\\partial J}{\\partial x_3} = -2(y_3 - x_3) = 0$, which gives $\\hat{x}_3 = y_3$.\nThe hard-constrained estimator is therefore:\n$$\n\\hat{x}_{\\text{hard}} = \\begin{pmatrix} 0 \\\\ 0 \\\\ y_3 \\end{pmatrix}\n$$\n- **Bias Vector (Hard Constraint):**\nThe expectation of the estimator is $\\mathbb{E}[\\hat{x}_{\\text{hard}}] = \\mathbb{E}\\left[\\begin{pmatrix} 0 \\\\ 0 \\\\ y_3 \\end{pmatrix}\\right] = \\begin{pmatrix} 0 \\\\ 0 \\\\ \\mathbb{E}[y_3] \\end{pmatrix}$.\nSince $y_3 = x^{\\star}_3 + \\varepsilon_3$, we have $\\mathbb{E}[y_3] = x^{\\star}_3 + \\mathbb{E}[\\varepsilon_3] = 3 + 0 = 3$.\nSo, $\\mathbb{E}[\\hat{x}_{\\text{hard}}] = \\begin{pmatrix} 0 \\\\ 0 \\\\ 3 \\end{pmatrix}$.\nThe bias vector is:\n$$\n\\text{Bias}_{\\text{hard}} = \\mathbb{E}[\\hat{x}_{\\text{hard}}] - x^{\\star} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 3 \\end{pmatrix} - \\begin{pmatrix} 2 \\\\ -1 \\\\ 3 \\end{pmatrix} = \\begin{pmatrix} -2 \\\\ 1 \\\\ 0 \\end{pmatrix}\n$$\n\n- **Change in Total Variance (Hard Constraint):**\nThe covariance matrix of the estimator is $\\mathrm{Cov}(\\hat{x}_{\\text{hard}}) = \\mathrm{Cov}\\left(\\begin{pmatrix} 0 \\\\ 0 \\\\ y_3 \\end{pmatrix}\\right)$.\nThe components of the covariance matrix are $\\mathrm{Cov}(\\hat{x}_{\\text{hard}})_{ij} = \\mathrm{Cov}((\\hat{x}_{\\text{hard}})_i, (\\hat{x}_{\\text{hard}})_j)$.\nThe first two components are constants ($0$), so their variances and covariances with any other variable are $0$. The only non-zero element is:\n$$\n\\mathrm{Var}((\\hat{x}_{\\text{hard}})_3) = \\mathrm{Var}(y_3) = \\mathrm{Var}(x^{\\star}_3 + \\varepsilon_3) = \\mathrm{Var}(\\varepsilon_3) = \\sigma^{2} = 1\n$$\nThe covariance matrix is:\n$$\n\\mathrm{Cov}(\\hat{x}_{\\text{hard}}) = \\begin{pmatrix} 0 & 0 & 0 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}\n$$\nThe total variance is $\\mathrm{Tr}(\\mathrm{Cov}(\\hat{x}_{\\text{hard}})) = 0 + 0 + 1 = 1$.\nThe change in total variance relative to the unconstrained-estimator is:\n$$\n\\Delta\\mathrm{Var}_{\\text{hard}} = \\mathrm{Tr}(\\mathrm{Cov}(\\hat{x}_{\\text{hard}})) - \\mathrm{Tr}(\\mathrm{Cov}(\\hat{x}_{\\text{LS}})) = 1 - 3 = -2\n$$\n\n**2. Soft Quadratic Penalty (Tikhonov Regularization)**\nThis strategy seeks to minimize $J(x) = \\|y - x\\|^{2} + \\lambda \\|L x\\|^{2}$ with $\\lambda=2$. This is a Tikhonov regularization problem. The objective function is:\n$$\nJ(x) = (y-x)^{\\top}(y-x) + \\lambda x^{\\top}L^{\\top}Lx\n$$\nSetting the gradient $\\nabla_x J(x) = -2(y-x) + 2\\lambda L^{\\top}Lx$ to zero:\n$$\n(I + \\lambda L^{\\top}L)x = y\n$$\nThe estimator is $\\hat{x}_{\\text{soft}} = (I + \\lambda L^{\\top}L)^{-1} y$.\nLet's compute the matrix $M = (I + \\lambda L^{\\top}L)^{-1}$. First, we compute $L^{\\top}L$:\n$$\nL^{\\top}L = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 0 & 0 \\end{pmatrix} \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix}\n$$\nWith $\\lambda=2$, we have:\n$$\nI + \\lambda L^{\\top}L = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} + 2 \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 3 & 0 & 0 \\\\ 0 & 3 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}\n$$\nThe inverse is:\n$$\nM = (I + \\lambda L^{\\top}L)^{-1} = \\begin{pmatrix} 1/3 & 0 & 0 \\\\ 0 & 1/3 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}\n$$\nThe estimator is $\\hat{x}_{\\text{soft}} = M y = \\begin{pmatrix} y_1/3 \\\\ y_2/3 \\\\ y_3 \\end{pmatrix}$.\n\n- **Bias Vector (Soft Penalty):**\nThe expectation of the estimator is $\\mathbb{E}[\\hat{x}_{\\text{soft}}] = \\mathbb{E}[M y] = M \\mathbb{E}[y] = M x^{\\star}$.\n$$\n\\mathbb{E}[\\hat{x}_{\\text{soft}}] = \\begin{pmatrix} 1/3 & 0 & 0 \\\\ 0 & 1/3 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ -1 \\\\ 3 \\end{pmatrix} = \\begin{pmatrix} 2/3 \\\\ -1/3 \\\\ 3 \\end{pmatrix}\n$$\nThe bias vector is:\n$$\n\\text{Bias}_{\\text{soft}} = \\mathbb{E}[\\hat{x}_{\\text{soft}}] - x^{\\star} = \\begin{pmatrix} 2/3 \\\\ -1/3 \\\\ 3 \\end{pmatrix} - \\begin{pmatrix} 2 \\\\ -1 \\\\ 3 \\end{pmatrix} = \\begin{pmatrix} 2/3 - 6/3 \\\\ -1/3 + 3/3 \\\\ 3 - 3 \\end{pmatrix} = \\begin{pmatrix} -4/3 \\\\ 2/3 \\\\ 0 \\end{pmatrix}\n$$\n\n- **Change in Total Variance (Soft Penalty):**\nThe covariance of the estimator is $\\mathrm{Cov}(\\hat{x}_{\\text{soft}}) = \\mathrm{Cov}(M y) = \\mathrm{Cov}(M(x^{\\star}+\\varepsilon)) = M \\mathrm{Cov}(\\varepsilon) M^{\\top}$.\nSince $\\mathrm{Cov}(\\varepsilon) = I_3$ and $M$ is symmetric ($M=M^{\\top}$):\n$$\n\\mathrm{Cov}(\\hat{x}_{\\text{soft}}) = M I_3 M = M^2\n$$\n$$\n\\mathrm{Cov}(\\hat{x}_{\\text{soft}}) = \\begin{pmatrix} 1/3 & 0 & 0 \\\\ 0 & 1/3 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}^2 = \\begin{pmatrix} 1/9 & 0 & 0 \\\\ 0 & 1/9 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}\n$$\nThe total variance is $\\mathrm{Tr}(\\mathrm{Cov}(\\hat{x}_{\\text{soft}})) = 1/9 + 1/9 + 1 = 2/9 + 9/9 = 11/9$.\nThe change in total variance relative to the unconstrained estimator is:\n$$\n\\Delta\\mathrm{Var}_{\\text{soft}} = \\mathrm{Tr}(\\mathrm{Cov}(\\hat{x}_{\\text{soft}})) - \\mathrm{Tr}(\\mathrm{Cov}(\\hat{x}_{\\text{LS}})) = \\frac{11}{9} - 3 = \\frac{11}{9} - \\frac{27}{9} = -\\frac{16}{9}\n$$\n\n**Summary of Results**\n- Hard-constraint bias vector: `(−2, 1, 0)`\n- Soft-penalty bias vector: `(−4/3, 2/3, 0)`\n- Hard-constraint variance change: `−2`\n- Soft-penalty variance change: `−16/9`\n\nThe final answer is a row matrix containing these eight values in the specified order.", "answer": "$$\n\\boxed{\\begin{pmatrix} -2 & 1 & 0 & -\\frac{4}{3} & \\frac{2}{3} & 0 & -2 & -\\frac{16}{9} \\end{pmatrix}}\n$$", "id": "3368100"}, {"introduction": "Spectral methods like Truncated Singular Value Decomposition (TSVD) regularize a problem by filtering out the unstable singular modes associated with small singular values. A central challenge is determining the optimal truncation level, and this practice [@problem_id:3368040] introduces a powerful heuristic known as the Discrepancy Principle for this purpose. By applying this principle to a concrete scenario, you will not only select the regularization parameter but also directly compute the resulting parameter-space bias, forging a clear link between the chosen model complexity and its systematic error.", "problem": "Consider a linear inverse problem with a forward operator $A \\in \\mathbb{R}^{m \\times n}$, where $m=n=6$, and a measurement model $y = A x + \\varepsilon$, with $\\varepsilon \\sim \\mathcal{N}(0,\\sigma^{2} I_{m})$ independent of $x$. Let the Singular Value Decomposition (SVD) of $A$ be $A = U \\Sigma V^{\\top}$, where $U \\in \\mathbb{R}^{6 \\times 6}$ and $V \\in \\mathbb{R}^{6 \\times 6}$ are orthogonal matrices, and $\\Sigma = \\operatorname{diag}(s_{1}, s_{2}, s_{3}, s_{4}, s_{5}, s_{6})$ with singular values $s_{1} \\geq s_{2} \\geq s_{3} \\geq s_{4} \\geq s_{5} \\geq s_{6} > 0$. Define the Truncated Singular Value Decomposition (TSVD) estimator $\\hat{x}_{k} \\in \\mathbb{R}^{6}$ at truncation level $k \\in \\{0,1,2,3,4,5,6\\}$ as the estimator that retains only the first $k$ singular components.\n\nYou are given the singular values $s_{1} = 20$, $s_{2} = 10$, $s_{3} = 5$, $s_{4} = 2.5$, $s_{5} = 1.25$, $s_{6} = 0.6$, and the coefficients of the true parameter $x$ in the right singular vector basis $V$:\n$$\n\\alpha_{i} \\equiv v_{i}^{\\top} x, \\quad \\alpha_{1} = 1,\\ \\alpha_{2} = \\frac{1}{2},\\ \\alpha_{3} = \\frac{1}{4},\\ \\alpha_{4} = \\frac{1}{8},\\ \\alpha_{5} = \\frac{1}{16},\\ \\alpha_{6} = \\frac{1}{32}.\n$$\nAssume the measurement noise variance is $\\sigma^{2} = 1$ (so $m \\sigma^{2} = 6$). The realized measurement $y$ satisfies $u_{i}^{\\top} y = \\gamma_{i}$, where the left singular vector coefficients are\n$$\n\\gamma_{1} = 21,\\quad \\gamma_{2} = 3,\\quad \\gamma_{3} = 4.25,\\quad \\gamma_{4} = -0.6875,\\quad \\gamma_{5} = 0.578125,\\quad \\gamma_{6} = 0.01875,\n$$\nand these are consistent with the model via $u_{i}^{\\top} y = s_{i} \\alpha_{i} + \\varepsilon_{i}$ with $u_{i}^{\\top} \\varepsilon = \\varepsilon_{i}$ given by $\\varepsilon_{1} = 1$, $\\varepsilon_{2} = -2$, $\\varepsilon_{3} = 3$, $\\varepsilon_{4} = -1$, $\\varepsilon_{5} = \\frac{1}{2}$, $\\varepsilon_{6} = 0$.\n\nStarting from first principles of the SVD and the TSVD construction, apply the discrepancy principle to select the truncation level $k$ by enforcing the inequality $\\|A \\hat{x}_{k} - y\\|^{2} \\leq m \\sigma^{2}$ and choosing the smallest integer $k$ for which this condition holds. Then, compute the induced parameter-space bias of the TSVD estimator at the selected $k$, defined as\n$$\n\\sum_{i>k} \\left(v_{i}^{\\top} x\\right)^{2}.\n$$\nProvide only the numerical value of this bias as your final answer. No rounding is required; report the exact value.", "solution": "The problem asks us to first determine the optimal truncation level $k$ for a Truncated Singular Value Decomposition (TSVD) estimator using the discrepancy principle, and then to compute the corresponding parameter-space bias, which is given by a specific formula.\n\nLet the linear inverse problem be described by the model $y = A x + \\varepsilon$, where $y \\in \\mathbb{R}^{m}$ represents the measurements, $x \\in \\mathbb{R}^{n}$ is the vector of unknown parameters, $A \\in \\mathbb{R}^{m \\times n}$ is the forward operator, and $\\varepsilon \\in \\mathbb{R}^{m}$ is a noise vector. We are given $m=n=6$. The noise is assumed to follow a normal distribution $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_m)$, with $\\sigma^2=1$.\n\nThe Singular Value Decomposition (SVD) of the matrix $A$ is $A = U \\Sigma V^{\\top}$, where $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{n \\times n}$ are orthogonal matrices whose columns are the left singular vectors $\\{u_i\\}$ and right singular vectors $\\{v_i\\}$, respectively. $\\Sigma \\in \\mathbb{R}^{m \\times n}$ is a diagonal matrix with non-negative singular values $s_i$ on its diagonal, ordered such that $s_1 \\geq s_2 \\geq \\dots \\geq s_n > 0$.\n\nThe TSVD estimator for $x$ at a truncation level $k$ is defined as\n$$\n\\hat{x}_{k} = \\sum_{i=1}^{k} \\frac{u_{i}^{\\top} y}{s_{i}} v_{i}\n$$\nThe problem provides the coefficients $\\gamma_i = u_i^\\top y$, so the estimator can be written as\n$$\n\\hat{x}_{k} = \\sum_{i=1}^{k} \\frac{\\gamma_{i}}{s_{i}} v_{i}\n$$\nThe next step is to evaluate the residual $A\\hat{x}_k - y$ associated with this estimator.\n$$\nA\\hat{x}_{k} = \\left( \\sum_{j=1}^{m} s_j u_j v_j^\\top \\right) \\left( \\sum_{i=1}^{k} \\frac{\\gamma_{i}}{s_{i}} v_{i} \\right)\n$$\nUsing the orthogonality of the right singular vectors, $v_j^\\top v_i = \\delta_{ij}$ (the Kronecker delta), we get\n$$\nA\\hat{x}_{k} = \\sum_{j=1}^{m} \\sum_{i=1}^{k} s_j u_j \\frac{\\gamma_i}{s_i} \\delta_{ij} = \\sum_{i=1}^{k} s_i u_i \\frac{\\gamma_i}{s_i} = \\sum_{i=1}^{k} \\gamma_i u_i\n$$\nThe data vector $y$ can be expanded in the basis of left singular vectors as $y = \\sum_{i=1}^{m} (u_i^\\top y) u_i = \\sum_{i=1}^{m} \\gamma_i u_i$.\nThe residual is therefore\n$$\nA\\hat{x}_{k} - y = \\sum_{i=1}^{k} \\gamma_i u_i - \\sum_{i=1}^{m} \\gamma_i u_i = - \\sum_{i=k+1}^{m} \\gamma_i u_i\n$$\nThe squared Euclidean norm of the residual, using the orthogonality of the left singular vectors ($u_i^\\top u_j = \\delta_{ij}$), is\n$$\n\\|A\\hat{x}_{k} - y\\|^2 = \\left\\| - \\sum_{i=k+1}^{m} \\gamma_i u_i \\right\\|^2 = \\sum_{i=k+1}^{m} \\gamma_i^2\n$$\nThe discrepancy principle states that we should choose the smallest integer $k$ for which the squared residual norm does not exceed the total expected contribution from the noise, which is $\\|A\\hat{x}_{k} - y\\|^2 \\leq m \\sigma^2$. With $m=6$ and $\\sigma^2=1$, the condition is\n$$\n\\sum_{i=k+1}^{6} \\gamma_i^2 \\leq 6\n$$\nWe are given the values of $\\gamma_i$:\n$\\gamma_1 = 21$, $\\gamma_2 = 3$, $\\gamma_3 = 4.25$, $\\gamma_4 = -0.6875$, $\\gamma_5 = 0.578125$, $\\gamma_6 = 0.01875$.\nLet's compute their squares:\n$\\gamma_1^2 = 21^2 = 441$\n$\\gamma_2^2 = 3^2 = 9$\n$\\gamma_3^2 = (4.25)^2 = 18.0625$\n$\\gamma_4^2 = (-0.6875)^2 = 0.47265625$\n$\\gamma_5^2 = (0.578125)^2 = 0.334228515625$\n$\\gamma_6^2 = (0.01875)^2 = 0.0003515625$\n\nNow we check the condition for increasing values of $k \\in \\{0, 1, 2, 3, 4, 5, 6\\}$:\nFor $k=0$: $\\sum_{i=1}^{6} \\gamma_i^2 = 441 + 9 + 18.0625 + \\dots = 468.869... > 6$. Condition fails.\nFor $k=1$: $\\sum_{i=2}^{6} \\gamma_i^2 = 9 + 18.0625 + \\dots = 27.869... > 6$. Condition fails.\nFor $k=2$: $\\sum_{i=3}^{6} \\gamma_i^2 = 18.0625 + 0.47265625 + \\dots = 18.869... > 6$. Condition fails.\nFor $k=3$: $\\sum_{i=4}^{6} \\gamma_i^2 = \\gamma_4^2 + \\gamma_5^2 + \\gamma_6^2 = 0.47265625 + 0.334228515625 + 0.0003515625 = 0.807236328125$.\nSince $0.807236328125 \\leq 6$, the condition is met. As this is the smallest $k$ for which the condition holds, the selected truncation level is $k=3$.\n\nThe problem asks for the value of the parameter-space bias, which is explicitly defined as $\\sum_{i>k} (v_i^\\top x)^2$. With the selected $k=3$, we need to compute\n$$\n\\text{Bias} = \\sum_{i=4}^{6} (v_i^\\top x)^2\n$$\nThe coefficients $\\alpha_i = v_i^\\top x$ are given:\n$\\alpha_4 = \\frac{1}{8}$\n$\\alpha_5 = \\frac{1}{16}$\n$\\alpha_6 = \\frac{1}{32}$\n\nSubstituting these values into the expression for the bias:\n$$\n\\text{Bias} = \\alpha_4^2 + \\alpha_5^2 + \\alpha_6^2 = \\left(\\frac{1}{8}\\right)^2 + \\left(\\frac{1}{16}\\right)^2 + \\left(\\frac{1}{32}\\right)^2\n$$\n$$\n\\text{Bias} = \\frac{1}{64} + \\frac{1}{256} + \\frac{1}{1024}\n$$\nTo sum these fractions, we use the common denominator $1024$:\n$$\n\\frac{1}{64} = \\frac{1 \\times 16}{64 \\times 16} = \\frac{16}{1024}\n$$\n$$\n\\frac{1}{256} = \\frac{1 \\times 4}{256 \\times 4} = \\frac{4}{1024}\n$$\nThe total sum is\n$$\n\\text{Bias} = \\frac{16}{1024} + \\frac{4}{1024} + \\frac{1}{1024} = \\frac{16+4+1}{1024} = \\frac{21}{1024}\n$$\nThis is the final exact numerical value.", "answer": "$$\n\\boxed{\\frac{21}{1024}}\n$$", "id": "3368040"}, {"introduction": "Moving from single-point analysis to a global perspective, this capstone computational exercise [@problem_id:3368069] explores the entire Tikhonov regularization path. You will implement a numerical procedure to connect the theoretical optimum for the regularization parameter $\\lambda$, found by minimizing the statistical risk, with a widely used practical heuristic—the corner of the 'L-curve'. This practice reveals the underlying mechanics of regularization by visualizing how singular modes are progressively activated, providing a powerful, hands-on synthesis of the chapter's key concepts.", "problem": "Consider a linear inverse problem with additive noise in a finite-dimensional Euclidean space: given a matrix $A \\in \\mathbb{R}^{m \\times n}$, a true state $x_{\\mathrm{true}} \\in \\mathbb{R}^{n}$, and noise $ \\varepsilon \\in \\mathbb{R}^{m}$ such that the observed data is $y = A x_{\\mathrm{true}} + \\varepsilon$, with the noise modeled as zero-mean and independent with variance $\\sigma_{\\varepsilon}^{2}$. For a regularization parameter $\\lambda > 0$, define the Tikhonov-regularized solution $x_{\\lambda}$ as the minimizer of the functional $\\|A x - y\\|_{2}^{2} + \\lambda^{2} \\|x\\|_{2}^{2}$. Assume that $A$ has a singular value decomposition $A = U \\Sigma V^{\\top}$ with singular values $\\{\\sigma_{i}\\}_{i=1}^{r}$, where $r = \\min(m,n)$ and $\\sigma_{1} \\ge \\cdots \\ge \\sigma_{r} > 0$.\n\nYour tasks are as follows, and all derivations must begin from the definitions of Tikhonov regularization, the singular value decomposition, and the standard bias-variance decomposition of the mean-squared error; do not use any pre-packaged risk or curvature formulas not derived from these bases.\n\n- Define the regularization path $\\{x_{\\lambda} : \\lambda > 0\\}$ and characterize the activation of singular modes along this path by identifying, for each singular value $\\sigma_{i}$, a deterministic threshold in $\\lambda$ at which the contribution of the corresponding singular direction transitions from being suppressed to being substantially present. Formalize this threshold using a principled criterion based on the filter factor implied by the Tikhonov solution in the singular vector basis, and treat this threshold as a phase transition point for mode $i$. Explicitly state these thresholds in terms of the singular values.\n\n- Define the L-curve as the parametric planar curve $(\\log \\|A x_{\\lambda} - y\\|_{2}, \\log \\|x_{\\lambda}\\|_{2})$ parameterized by $\\log \\lambda$. Using the definition of curvature for a planar parametric curve in terms of its first and second derivatives with respect to the parameter, implement a numerical procedure to estimate the curvature over a grid of $\\lambda$ values and identify the L-curve corner as the regularization parameter $\\lambda$ where the curvature attains its maximum. You must parameterize the curve by $\\log \\lambda$ and use numerically stable finite differences.\n\n- Using the bias-variance decomposition of the expected squared error $\\mathbb{E}\\left[\\|x_{\\lambda} - x_{\\mathrm{true}}\\|_{2}^{2}\\right]$ in the singular vector basis, derive and implement an expression for the mean-squared risk as a function of $\\lambda$ and identify the risk-minimizing regularization parameter $\\lambda_{\\mathrm{risk}}$ over a logarithmically spaced grid. Your derivation must explicitly separate the contribution from the bias and the variance.\n\n- Connect the phase transition locations, the L-curve corner, and the risk minimum by reporting, for each test case: the L-curve corner $\\lambda_{\\mathrm{L}}$, the risk minimum $\\lambda_{\\mathrm{risk}}$, the ratio $\\lambda_{\\mathrm{L}} / \\lambda_{\\mathrm{risk}}$, the number of activated modes at $\\lambda_{\\mathrm{risk}}$ under your activation criterion, and a boolean indicating whether $\\lambda_{\\mathrm{L}}$ and $\\lambda_{\\mathrm{risk}}$ are within a factor of $2$ of each other.\n\nImplementation requirements and test suite:\n\n- You must construct $A$ as a diagonal matrix with prescribed singular values (so $U = I$ and $V = I$), and you must use the exact Tikhonov solution defined by the given objective. The numerical curvature must be computed with central finite differences on a uniform grid in $\\log \\lambda$.\n\n- Use a logarithmically spaced grid for $\\lambda$ with $N_{\\lambda} = 800$ points between $\\lambda_{\\min} = 10^{-6} \\cdot \\max_{i} \\sigma_{i}$ and $\\lambda_{\\max} = 10^{2} \\cdot \\max_{i} \\sigma_{i}$, inclusive.\n\n- For the L-curve, use the realized data $y$ constructed from the specified $\\varepsilon$ in each test case. For the risk, use the expected risk computed with the supplied $\\sigma_{\\varepsilon}^{2}$.\n\n- Define the activation threshold for mode $i$ as the $\\lambda$ at which the corresponding Tikhonov filter factor for that mode equals $1/2$, and count a mode as activated at a given $\\lambda$ if its filter factor is at least $1/2$.\n\n- Use the following three test cases, each with $m = n$, and the specified deterministic noise vector $\\varepsilon$:\n    - Test case $1$:\n        - Singular values: $\\sigma = [10.0, 1.0, 0.1]$.\n        - True state: $x_{\\mathrm{true}} = [1.0, 1.0, 1.0]$.\n        - Noise variance: $\\sigma_{\\varepsilon}^{2} = 0.01$.\n        - Noise vector: $\\varepsilon = \\sqrt{\\sigma_{\\varepsilon}^{2}} \\cdot [1.0, -1.0, 1.0]$.\n    - Test case $2$:\n        - Singular values: $\\sigma = [5.0, 4.0, 3.0, 2.0, 1.0]$.\n        - True state: $x_{\\mathrm{true}} = [1.0, 0.5, 0.25, 0.125, 0.0625]$.\n        - Noise variance: $\\sigma_{\\varepsilon}^{2} = 0.04$.\n        - Noise vector: $\\varepsilon = \\sqrt{\\sigma_{\\varepsilon}^{2}} \\cdot [1.0, 0.0, -1.0, 0.0, 1.0]$.\n    - Test case $3$:\n        - Singular values: $\\sigma = [1.0, 0.3, 0.09, 0.027]$.\n        - True state: $x_{\\mathrm{true}} = [1.0, 0.8, 0.6, 0.4]$.\n        - Noise variance: $\\sigma_{\\varepsilon}^{2} = 0.0001$.\n        - Noise vector: $\\varepsilon = \\sqrt{\\sigma_{\\varepsilon}^{2}} \\cdot [1.0, -2.0, 1.0, -2.0]$.\n\nFinal output format:\n\n- Your program should produce a single line of output containing the results for the three test cases as a comma-separated list enclosed in square brackets. The list must concatenate, in order for test cases $1$, $2$, and $3$, the following five items per case:\n    - $\\lambda_{\\mathrm{L}}$ as a float,\n    - $\\lambda_{\\mathrm{risk}}$ as a float,\n    - $\\lambda_{\\mathrm{L}} / \\lambda_{\\mathrm{risk}}$ as a float,\n    - the integer number of activated modes at $\\lambda_{\\mathrm{risk}}$,\n    - a boolean indicating whether $\\max\\left(\\lambda_{\\mathrm{L}} / \\lambda_{\\mathrm{risk}}, \\lambda_{\\mathrm{risk}} / \\lambda_{\\mathrm{L}}\\right) \\le 2$.\n\nThus the output list contains $15$ entries: $5$ entries per test case, concatenated across the $3$ cases, and printed as a single Python-style list on one line, with no additional text.", "solution": "The problem requires a comprehensive analysis of Tikhonov regularization for a linear inverse problem, focusing on the connection between the regularization parameter $\\lambda$, model complexity (activated singular modes), predictive error (L-curve), and estimation error (statistical risk). The analysis proceeds from first principles.\n\n### 1. Tikhonov Regularization and the Regularization Path\n\nThe problem is to find a stable solution $x$ to the linear system $y = Ax_{\\mathrm{true}} + \\varepsilon$, where $y \\in \\mathbb{R}^m$ is the observed data, $A \\in \\mathbb{R}^{m \\times n}$ is the forward operator, $x_{\\mathrm{true}} \\in \\mathbb{R}^n$ is the true underlying state, and $\\varepsilon \\in \\mathbb{R}^m$ is additive noise.\n\nTikhonov regularization seeks to find a solution by minimizing the functional:\n$$\nJ_{\\lambda}(x) = \\|Ax - y\\|_{2}^{2} + \\lambda^{2} \\|x\\|_{2}^{2}\n$$\nwhere $\\lambda > 0$ is the regularization parameter. The first term enforces fidelity to the data, while the second term penalizes large solutions, thereby enforcing stability. The solution $x_{\\lambda}$ is found by setting the gradient of $J_{\\lambda}(x)$ with respect to $x$ to zero:\n$$\n\\nabla_{x} J_{\\lambda}(x) = 2A^{\\top}(Ax - y) + 2\\lambda^{2}x = 0\n$$\nThis leads to the normal equations:\n$$\n(A^{\\top}A + \\lambda^{2}I)x = A^{\\top}y\n$$\nThe Tikhonov-regularized solution is therefore:\n$$\nx_{\\lambda} = (A^{\\top}A + \\lambda^{2}I)^{-1}A^{\\top}y\n$$\nThe set of solutions $\\{x_{\\lambda} : \\lambda > 0\\}$ constitutes the regularization path.\n\nTo analyze the behavior of $x_{\\lambda}$, we use the singular value decomposition (SVD) of $A$: $A = U \\Sigma V^{\\top}$, where $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{n \\times n}$ are orthogonal matrices, and $\\Sigma \\in \\mathbb{R}^{m \\times n}$ is a rectangular diagonal matrix with non-negative singular values $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge \\sigma_r > 0$ on its diagonal, where $r = \\text{rank}(A)$.\n\nSubstituting the SVD into the expression for $x_{\\lambda}$:\n$A^{\\top}A = (U\\Sigma V^{\\top})^{\\top}(U\\Sigma V^{\\top}) = V\\Sigma^{\\top}U^{\\top}U\\Sigma V^{\\top} = V\\Sigma^{\\top}\\Sigma V^{\\top}$.\n$A^{\\top}y = (U\\Sigma V^{\\top})^{\\top}y = V\\Sigma^{\\top}U^{\\top}y$.\nSo, $(V\\Sigma^{\\top}\\Sigma V^{\\top} + \\lambda^2 I)x_\\lambda = V\\Sigma^{\\top}U^{\\top}y$.\nMultiplying by $V^{\\top}$ from the left and noting $V^{\\top}V=I$:\n$(\\Sigma^{\\top}\\Sigma + \\lambda^2 I)V^{\\top}x_\\lambda = \\Sigma^{\\top}U^{\\top}y$.\nLet $\\hat{x}_{\\lambda} = V^{\\top}x_{\\lambda}$ and $\\hat{y} = U^{\\top}y$ be the solution and data projected onto the singular vector bases. The matrix $\\Sigma^{\\top}\\Sigma$ is an $n \\times n$ diagonal matrix with diagonal entries $\\sigma_i^2$ for $i=1, \\dots, r$ and zero otherwise. The equation becomes diagonal:\n$$\n(\\sigma_i^2 + \\lambda^2)(\\hat{x}_{\\lambda})_i = \\sigma_i (\\hat{y})_i \\quad \\text{for } i \\le r\n$$\nThe solution for the $i$-th component in the $V$-basis is:\n$$\n(\\hat{x}_{\\lambda})_i = \\frac{\\sigma_i}{\\sigma_i^2 + \\lambda^2}(\\hat{y})_i = \\left(\\frac{\\sigma_i^2}{\\sigma_i^2 + \\lambda^2}\\right) \\frac{(\\hat{y})_i}{\\sigma_i}\n$$\nThe term $f_i(\\lambda) = \\frac{\\sigma_i^2}{\\sigma_i^2 + \\lambda^2}$ is the Tikhonov filter factor for the $i$-th singular mode. It determines how much the $i$-th data component $(\\hat{y})_i / \\sigma_i$ contributes to the solution component $(\\hat{x}_{\\lambda})_i$.\nFor $\\lambda \\ll \\sigma_i$, $f_i(\\lambda) \\approx 1$, and the mode is passed through.\nFor $\\lambda \\gg \\sigma_i$, $f_i(\\lambda) \\approx 0$, and the mode is filtered out.\nThe problem defines the activation threshold as the point where the filter factor is $1/2$:\n$$\nf_i(\\lambda) = \\frac{\\sigma_i^2}{\\sigma_i^2 + \\lambda^2} = \\frac{1}{2} \\implies 2\\sigma_i^2 = \\sigma_i^2 + \\lambda^2 \\implies \\lambda^2 = \\sigma_i^2\n$$\nSince $\\lambda > 0$ and $\\sigma_i > 0$, the activation threshold for mode $i$ is $\\lambda = \\sigma_i$.\nA mode is counted as activated at a given $\\lambda$ if $f_i(\\lambda) \\ge 1/2$, which is equivalent to $\\sigma_i^2 \\ge \\lambda^2$, or $\\sigma_i \\ge \\lambda$.\n\n### 2. L-Curve Corner Identification\n\nThe L-curve is the parametric plot of $(\\rho_\\lambda, \\eta_\\lambda)$ where $\\rho_\\lambda = \\log \\|A x_{\\lambda} - y\\|_{2}$ and $\\eta_\\lambda = \\log \\|x_{\\lambda}\\|_{2}$, parameterized by $p = \\log \\lambda$. The \"corner\" of the L-curve often corresponds to a good choice of $\\lambda$, balancing data fidelity and solution stability. The corner is located by finding the point of maximum curvature.\n\nThe problem specifies $A$ to be a diagonal matrix, so $U=V=I$ and $m=n=r$. The SVD is trivial, with $A_{ii} = \\sigma_i$. The solution and residual components are:\n$$\n(x_{\\lambda})_i = \\frac{\\sigma_i}{\\sigma_i^2 + \\lambda^2} y_i\n$$\n$$\n(Ax_{\\lambda} - y)_i = \\sigma_i (x_{\\lambda})_i - y_i = \\left(\\frac{\\sigma_i^2}{\\sigma_i^2 + \\lambda^2} - 1\\right) y_i = -\\frac{\\lambda^2}{\\sigma_i^2 + \\lambda^2} y_i\n$$\nThe squared norms required for the L-curve are:\n$$\nS_\\lambda = \\|x_{\\lambda}\\|_2^2 = \\sum_{i=1}^n \\left( \\frac{\\sigma_i y_i}{\\sigma_i^2 + \\lambda^2} \\right)^2\n$$\n$$\nR_\\lambda = \\|Ax_{\\lambda} - y\\|_2^2 = \\sum_{i=1}^n \\left( \\frac{\\lambda^2 y_i}{\\sigma_i^2 + \\lambda^2} \\right)^2\n$$\nThe L-curve points are $(\\rho(\\lambda), \\eta(\\lambda)) = (\\frac{1}{2}\\log R_\\lambda, \\frac{1}{2}\\log S_\\lambda)$. The curvature $K$ of a parametric curve $(x(p), y(p))$ is given by:\n$$\nK(p) = \\frac{|x'y'' - y'x''|}{(x'^2 + y'^2)^{3/2}}\n$$\nwhere primes denote derivatives with respect to the parameter $p = \\log \\lambda$.\nNumerically, we compute the curve points $(\\rho_j, \\eta_j)$ over a uniform grid of $p_j = \\log \\lambda_j$. The derivatives are approximated using central finite differences. With $\\Delta p = p_{j+1} - p_j$:\n$$\n\\rho'_j \\approx \\frac{\\rho_{j+1} - \\rho_{j-1}}{2 \\Delta p}, \\quad \\rho''_j \\approx \\frac{\\rho_{j+1} - 2\\rho_j + \\rho_{j-1}}{(\\Delta p)^2}\n$$\nand similarly for $\\eta_j$. These approximations are substituted into the curvature formula to find $K_j$. The regularization parameter $\\lambda_L$ is the value of $\\lambda_j$ that maximizes $K_j$.\n\n### 3. Bias-Variance Decomposition and Risk Minimization\n\nThe quality of the estimator $x_\\lambda$ is measured by the mean-squared error, or risk, defined as $Risk(\\lambda) = \\mathbb{E}[\\|x_\\lambda - x_{\\mathrm{true}}\\|_2^2]$, where the expectation is over the distribution of the noise $\\varepsilon$. The noise is zero-mean ($\\mathbb{E}[\\varepsilon] = 0$) with independent components and variance $\\sigma_\\varepsilon^2$ ($\\mathbb{E}[\\varepsilon\\varepsilon^\\top] = \\sigma_\\varepsilon^2 I$).\n\nThe risk can be decomposed into squared bias and variance:\n$$\nRisk(\\lambda) = \\underbrace{\\|\\mathbb{E}[x_\\lambda] - x_{\\mathrm{true}}\\|_2^2}_{\\text{Squared Bias}} + \\underbrace{\\mathbb{E}[\\|x_\\lambda - \\mathbb{E}[x_\\lambda]\\|_2^2]}_{\\text{Variance}}\n$$\n\nFirst, we compute the expected value of the solution. Since $y = Ax_{\\mathrm{true}} + \\varepsilon$ and linearity of expectation:\n$$\n\\mathbb{E}[x_\\lambda] = \\mathbb{E}[(A^\\top A + \\lambda^2 I)^{-1} A^\\top (A x_{\\mathrm{true}} + \\varepsilon)] = (A^\\top A + \\lambda^2 I)^{-1} A^\\top A x_{\\mathrm{true}}\n$$\nThe bias is $b(\\lambda) = \\mathbb{E}[x_\\lambda] - x_{\\mathrm{true}} = [(A^\\top A + \\lambda^2 I)^{-1} A^\\top A - I]x_{\\mathrm{true}}$.\nUsing the SVD components in the basis of $V$, the $i$-th component of the bias is:\n$$\n(\\hat{b}(\\lambda))_i = \\left(\\frac{\\sigma_i^2}{\\sigma_i^2 + \\lambda^2} - 1\\right)(\\hat{x}_{\\mathrm{true}})_i = -\\frac{\\lambda^2}{\\sigma_i^2 + \\lambda^2}(\\hat{x}_{\\mathrm{true}})_i\n$$\nThe squared bias norm is $\\|\\hat{b}(\\lambda)\\|_2^2$, which for the problem's diagonal $A$ ($V=I, \\hat{x}_{\\text{true}}=x_{\\text{true}}$) is:\n$$\n\\text{Squared Bias} = \\sum_{i=1}^n \\left( \\frac{\\lambda^2 (x_{\\mathrm{true}})_i}{\\sigma_i^2 + \\lambda^2} \\right)^2\n$$\nNext, we compute the variance. The deviation from the mean is:\n$$\nx_\\lambda - \\mathbb{E}[x_\\lambda] = (A^\\top A + \\lambda^2 I)^{-1} A^\\top \\varepsilon\n$$\nThe variance is the expected squared norm of this quantity:\n$$\n\\text{Variance} = \\mathbb{E}[\\varepsilon^\\top A (A^\\top A + \\lambda^2 I)^{-2} A^\\top \\varepsilon]\n$$\nUsing the trace identity for quadratic forms of random vectors, $\\mathbb{E}[z^\\top Q z]=\\text{Tr}(Q \\text{Cov}(z))$, where $z=\\varepsilon$ and $\\text{Cov}(\\varepsilon) = \\sigma_\\varepsilon^2 I$:\n$$\n\\text{Variance} = \\text{Tr}(A(A^\\top A + \\lambda^2 I)^{-2} A^\\top \\sigma_\\varepsilon^2 I) = \\sigma_\\varepsilon^2 \\text{Tr}(A(A^\\top A + \\lambda^2 I)^{-2} A^\\top)\n$$\nUsing the cyclic property of the trace and the SVD:\n$$\n\\text{Tr}(A(A^\\top A + \\lambda^2 I)^{-2} A^\\top) = \\text{Tr}((A^\\top A + \\lambda^2 I)^{-2} A^\\top A) = \\sum_{i=1}^n \\frac{\\sigma_i^2}{(\\sigma_i^2 + \\lambda^2)^2}\n$$\nSo the variance is:\n$$\n\\text{Variance} = \\sigma_\\varepsilon^2 \\sum_{i=1}^n \\frac{\\sigma_i^2}{(\\sigma_i^2 + \\lambda^2)^2}\n$$\nThe total risk is the sum of the squared bias and the variance:\n$$\nRisk(\\lambda) = \\sum_{i=1}^n \\left( \\frac{\\lambda^2 (x_{\\mathrm{true}})_i}{\\sigma_i^2 + \\lambda^2} \\right)^2 + \\sigma_\\varepsilon^2 \\sum_{i=1}^n \\frac{\\sigma_i^2}{(\\sigma_i^2 + \\lambda^2)^2}\n$$\nThe risk-minimizing parameter $\\lambda_{\\mathrm{risk}}$ is found by minimizing this function over the grid of $\\lambda$ values.\n\n### 4. Synthesis and Reporting\n\nThe final step is to integrate these analyses for each test case.\n1.  Compute the L-curve and its curvature to find $\\lambda_L$.\n2.  Compute the risk function and find its minimizer $\\lambda_{\\mathrm{risk}}$.\n3.  Calculate the ratio $\\lambda_L / \\lambda_{\\mathrm{risk}}$.\n4.  Count the number of activated modes at $\\lambda = \\lambda_{\\mathrm{risk}}$ by counting how many singular values $\\sigma_i$ satisfy $\\sigma_i \\ge \\lambda_{\\mathrm{risk}}$.\n5.  Determine if $\\lambda_L$ and $\\lambda_{\\mathrm{risk}}$ are within a factor of $2$ of each other, i.e., $1/2 \\le \\lambda_L / \\lambda_{\\mathrm{risk}} \\le 2$.\n\nThis procedure provides a multi-faceted view of regularization, connecting a heuristic method (L-curve), a statistical optimality criterion (risk minimization), and a model complexity interpretation (number of activated modes).", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the Tikhonov regularization analysis problem for three test cases.\n\n    For each case, it calculates:\n    1. lambda_L: The regularization parameter at the L-curve corner.\n    2. lambda_risk: The risk-minimizing regularization parameter.\n    3. The ratio lambda_L / lambda_risk.\n    4. The number of activated singular modes at lambda_risk.\n    5. A boolean indicating if lambda_L and lambda_risk are within a factor of 2.\n    \"\"\"\n    \n    test_cases = [\n        {\n            \"sigma\": np.array([10.0, 1.0, 0.1]),\n            \"x_true\": np.array([1.0, 1.0, 1.0]),\n            \"sigma_eps_sq\": 0.01,\n            \"epsilon\": np.sqrt(0.01) * np.array([1.0, -1.0, 1.0])\n        },\n        {\n            \"sigma\": np.array([5.0, 4.0, 3.0, 2.0, 1.0]),\n            \"x_true\": np.array([1.0, 0.5, 0.25, 0.125, 0.0625]),\n            \"sigma_eps_sq\": 0.04,\n            \"epsilon\": np.sqrt(0.04) * np.array([1.0, 0.0, -1.0, 0.0, 1.0])\n        },\n        {\n            \"sigma\": np.array([1.0, 0.3, 0.09, 0.027]),\n            \"x_true\": np.array([1.0, 0.8, 0.6, 0.4]),\n            \"sigma_eps_sq\": 0.0001,\n            \"epsilon\": np.sqrt(0.0001) * np.array([1.0, -2.0, 1.0, -2.0])\n        }\n    ]\n\n    all_results = []\n    \n    N_lambda = 800\n\n    for case in test_cases:\n        sigma = case[\"sigma\"]\n        x_true = case[\"x_true\"]\n        sigma_eps_sq = case[\"sigma_eps_sq\"]\n        epsilon = case[\"epsilon\"]\n        \n        n = len(sigma)\n        \n        # Construct diagonal matrix A and observed data y\n        A = np.diag(sigma)\n        y = A @ x_true + epsilon\n        \n        # Set up logarithmically spaced grid for lambda\n        lambda_min = 1e-6 * np.max(sigma)\n        lambda_max = 1e2 * np.max(sigma)\n        lambda_grid = np.logspace(np.log10(lambda_min), np.log10(lambda_max), N_lambda)\n        \n        # Vectorized calculations over the lambda grid\n        # Reshape for broadcasting: sigma(n,1), lambda_grid(1, N)\n        sigma_col = sigma[:, np.newaxis]\n        x_true_col = x_true[:, np.newaxis]\n        y_col = y[:, np.newaxis]\n        lambda_sq = lambda_grid[np.newaxis, :]**2\n        \n        # --- Bias-Variance and Risk Calculation ---\n        # Denominator term for all calculations\n        denom = sigma_col**2 + lambda_sq # Shape (n, N_lambda)\n        \n        # Squared Bias\n        bias_sq_terms = (lambda_sq * x_true_col / denom)**2\n        bias_sq = np.sum(bias_sq_terms, axis=0)\n        \n        # Variance\n        variance_terms = sigma_col**2 / (denom**2)\n        variance = sigma_eps_sq * np.sum(variance_terms, axis=0)\n        \n        # Risk\n        risk = bias_sq + variance\n        \n        # Find risk-minimizing lambda\n        risk_min_idx = np.argmin(risk)\n        lambda_risk = lambda_grid[risk_min_idx]\n        \n        # --- L-Curve Calculation ---\n        # Solution norm squared\n        sol_norm_sq_terms = (sigma_col * y_col / denom)**2\n        sol_norm_sq = np.sum(sol_norm_sq_terms, axis=0)\n        \n        # Residual norm squared\n        res_norm_sq_terms = (lambda_sq * y_col / denom)**2\n        res_norm_sq = np.sum(res_norm_sq_terms, axis=0)\n        \n        # L-curve coordinates (log norms)\n        # Add a small epsilon to avoid log(0) if norms are numerically zero\n        rho = np.log(np.sqrt(res_norm_sq) + 1e-16)\n        eta = np.log(np.sqrt(sol_norm_sq) + 1e-16)\n        \n        # Parameter p = log(lambda)\n        p_grid = np.log(lambda_grid)\n        dp = p_grid[1] - p_grid[0]\n        \n        # First and second derivatives using central differences\n        rho_p = np.gradient(rho, dp)\n        eta_p = np.gradient(eta, dp)\n        rho_pp = np.gradient(rho_p, dp)\n        eta_pp = np.gradient(eta_p, dp)\n        \n        # Curvature\n        # Calculate on interior points to avoid edge effects of np.gradient\n        numerator = np.abs(rho_p * eta_pp - eta_p * rho_pp)\n        denominator = (rho_p**2 + eta_p**2)**1.5\n        # Avoid division by zero\n        curvature = np.divide(numerator, denominator, out=np.zeros_like(numerator), where=denominator!=0)\n        \n        # Find lambda at max curvature (L-curve corner)\n        # Search on interior of grid to match derivative calculation\n        lcurve_corner_idx = np.argmax(curvature[1:-1]) + 1\n        lambda_L = lambda_grid[lcurve_corner_idx]\n        \n        # --- Synthesize Results ---\n        # Ratio of lambdas\n        ratio = lambda_L / lambda_risk\n        \n        # Number of activated modes at lambda_risk\n        # Mode i is activated if sigma_i >= lambda\n        activated_modes = np.sum(sigma >= lambda_risk)\n        \n        # Boolean check: within factor of 2\n        is_close = (0.5 <= ratio <= 2.0)\n        \n        all_results.extend([lambda_L, lambda_risk, ratio, int(activated_modes), is_close])\n\n    # Format and print the final output as a single list string\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n\n```\n```", "id": "3368069"}]}