{"hands_on_practices": [{"introduction": "This exercise delves into the theoretical core of iterative regularization. You will derive the relationship between the optimal number of iterations $k$, the noise level $\\delta$, and the smoothness of the true solution, demonstrating how early stopping acts as a regularization mechanism by balancing approximation and noise-induced errors. This analysis forms the foundation for understanding why and how iterative methods successfully solve ill-posed problems [@problem_id:3392708].", "problem": "Consider a linear inverse problem in separable Hilbert spaces $\\mathcal{X}$ and $\\mathcal{Y}$ with a bounded linear operator $A:\\mathcal{X}\\to\\mathcal{Y}$, and assume the measurement model $y=A x^{\\dagger}$ with noisy data $y^{\\delta}\\in\\mathcal{Y}$ satisfying $\\|y^{\\delta}-y\\|\\leq \\delta$. Assume, without loss of generality by scaling, that $\\|A\\|\\leq 1$. Let $(x_{k}^{\\delta})_{k\\in\\mathbb{N}}$ be the Landweber iteration with unit stepsize, initialized at $x_{0}^{\\delta}=0$, defined by $x_{k+1}^{\\delta}=x_{k}^{\\delta}+A^{*}(y^{\\delta}-A x_{k}^{\\delta})$, where $A^{*}$ denotes the adjoint of $A$. It is known that this iteration admits a spectral filter representation $x_{k}^{\\delta}=g_{k}(A^{*}A)A^{*}y^{\\delta}$ with the filter function $g_{k}(\\lambda)=\\frac{1-(1-\\lambda)^{k}}{\\lambda}$ and the corresponding residual function $r_{k}(\\lambda)=1-\\lambda g_{k}(\\lambda)=(1-\\lambda)^{k}$.\n\nAssume the source condition $x^{\\dagger}=\\phi(A^{*}A)w$ for some $w\\in\\mathcal{X}$ and an index function $\\phi:[0,1]\\to[0,\\infty)$; in this problem, specialize to the Hölder-type source condition $\\phi(\\lambda)=\\lambda^{\\nu}$ with $\\nu\\in(0,1]$. Using the spectral theorem for bounded self-adjoint operators and the given $g_{k}$ and $r_{k}$, derive the decomposition of the total error $x_{k}^{\\delta}-x^{\\dagger}$ into an approximation part and a noise part, and obtain asymptotic rate conditions linking the iteration count $k$, the noise level $\\delta$, and the index function $\\phi$.\n\nYour task is to:\n- Start from the source condition and the spectral calculus to derive bounds for the approximation error $\\|r_{k}(A^{*}A)x^{\\dagger}\\|$ in terms of $k$ and $\\nu$.\n- Bound the propagated noise $\\|g_{k}(A^{*}A)A^{*}(y^{\\delta}-y)\\|$ in terms of $k$ and $\\delta$.\n- Balance these two bounds to obtain the iteration count $k(\\delta)$ that asymptotically minimizes the total error as $\\delta\\to 0$, ignoring multiplicative constants independent of $\\delta$ and $k$.\n\nAnswer form requirement: Provide a single closed-form analytic expression for the asymptotically optimal choice $k(\\delta)$ in terms of $\\delta$ and $\\nu$. No units are required. Do not include inequalities or big-$O$ notation. The final answer must be expressed exactly without rounding.", "solution": "The problem statement has been validated and is deemed a well-posed and standard problem in the theory of inverse problems and regularization. We may proceed with the solution.\n\nThe goal is to determine the asymptotically optimal iteration count $k(\\delta)$ for the Landweber iteration by balancing the approximation error and the propagated noise error.\n\nThe total error is given by $x_{k}^{\\delta}-x^{\\dagger}$. Using the provided spectral filter representation for the Landweber iterate, $x_{k}^{\\delta}=g_{k}(A^{*}A)A^{*}y^{\\delta}$, we can decompose the error as follows.\nFirst, we substitute $y^{\\delta} = y + (y^{\\delta}-y)$ and use the fact that the exact solution $x^{\\dagger}$ satisfies $A x^{\\dagger} = y$, which implies $A^{*}A x^{\\dagger} = A^{*}y$.\n\n$$\n\\begin{aligned}\nx_{k}^{\\delta}-x^{\\dagger} &= g_{k}(A^{*}A)A^{*}y^{\\delta} - x^{\\dagger} \\\\\n&= g_{k}(A^{*}A)A^{*}(y + (y^{\\delta}-y)) - x^{\\dagger} \\\\\n&= \\left( g_{k}(A^{*}A)A^{*}y - x^{\\dagger} \\right) + g_{k}(A^{*}A)A^{*}(y^{\\delta}-y) \\\\\n&= \\left( g_{k}(A^{*}A)A^{*}Ax^{\\dagger} - x^{\\dagger} \\right) + g_{k}(A^{*}A)A^{*}(y^{\\delta}-y)\n\\end{aligned}\n$$\n\nLet $B = A^{*}A$. The first term is $(g_{k}(B)B - I)x^{\\dagger}$, where $I$ is the identity operator. Using the given residual function $r_{k}(\\lambda) = 1-\\lambda g_{k}(\\lambda)$, we can identify this term as the approximation error:\n$$\ng_{k}(B)B - I = - (I - g_{k}(B)B) = -r_{k}(B)\n$$\nThus, the error decomposition is:\n$$\nx_{k}^{\\delta}-x^{\\dagger} = -r_{k}(A^{*}A)x^{\\dagger} + g_{k}(A^{*}A)A^{*}(y^{\\delta}-y)\n$$\nThe first term, $-r_{k}(A^{*}A)x^{\\dagger}$, is the approximation error (or bias), and the second term, $g_{k}(A^{*}A)A^{*}(y^{\\delta}-y)$, is the propagated noise error (or variance). By the triangle inequality, the norm of the total error is bounded by the sum of the norms of these two components:\n$$\n\\|x_{k}^{\\delta}-x^{\\dagger}\\| \\leq \\|r_{k}(A^{*}A)x^{\\dagger}\\| + \\|g_{k}(A^{*}A)A^{*}(y^{\\delta}-y)\\|\n$$\nWe will now bound each term separately.\n\n1.  **Bound on the Approximation Error**\n\nWe are given the Hölder-type source condition $x^{\\dagger} = \\phi(A^{*}A)w = (A^{*}A)^{\\nu}w$ for some $w \\in \\mathcal{X}$ and $\\nu \\in (0, 1]$. The norm of the approximation error is:\n$$\n\\|r_{k}(A^{*}A)x^{\\dagger}\\| = \\|r_{k}(A^{*}A)(A^{*}A)^{\\nu}w\\| \\leq \\|r_{k}(A^{*}A)(A^{*}A)^{\\nu}\\| \\|w\\|\n$$\nUsing the spectral theorem for bounded self-adjoint operators (functional calculus), the norm of the operator is the supremum of the corresponding function over the spectrum of $A^{*}A$. The spectrum $\\sigma(A^{*}A)$ is contained in $[0, \\|A^{*}A\\|] \\subseteq [0, \\|A\\|^{2}] \\subseteq [0, 1]$.\n$$\n\\|r_{k}(A^{*}A)(A^{*}A)^{\\nu}\\| = \\sup_{\\lambda \\in \\sigma(A^{*}A)} |r_{k}(\\lambda)\\lambda^{\\nu}| \\leq \\sup_{\\lambda \\in [0, 1]} |(1-\\lambda)^{k} \\lambda^{\\nu}|\n$$\nTo find the supremum of the function $f(\\lambda) = (1-\\lambda)^{k}\\lambda^{\\nu}$ on $[0, 1]$, we take its derivative with respect to $\\lambda$ and set it to $0$:\n$$\nf'(\\lambda) = -k(1-\\lambda)^{k-1}\\lambda^{\\nu} + \\nu(1-\\lambda)^{k}\\lambda^{\\nu-1} = (1-\\lambda)^{k-1}\\lambda^{\\nu-1}(-k\\lambda + \\nu(1-\\lambda)) = 0\n$$\nFor $\\lambda \\in (0, 1)$, this implies $-k\\lambda + \\nu - \\nu\\lambda = 0$, which gives the location of the maximum as $\\lambda_{\\text{max}} = \\frac{\\nu}{\\nu+k}$.\nSubstituting this back into $f(\\lambda)$, we get the maximum value:\n$$\nf(\\lambda_{\\text{max}}) = \\left(1-\\frac{\\nu}{\\nu+k}\\right)^{k} \\left(\\frac{\\nu}{\\nu+k}\\right)^{\\nu} = \\left(\\frac{k}{k+\\nu}\\right)^{k} \\left(\\frac{\\nu}{k+\\nu}\\right)^{\\nu} = \\left(1+\\frac{\\nu}{k}\\right)^{-k} \\frac{\\nu^{\\nu}}{(k+\\nu)^{\\nu}}\n$$\nFor large $k$, we have the asymptotic behavior:\n- $\\left(1+\\frac{\\nu}{k}\\right)^{-k} \\to \\exp(-\\nu)$\n- $(k+\\nu)^{\\nu} = k^{\\nu}(1+\\frac{\\nu}{k})^{\\nu} \\sim k^{\\nu}$\nSo, $f(\\lambda_{\\text{max}}) \\sim \\exp(-\\nu) \\nu^{\\nu} k^{-\\nu}$. This shows that the supremum is of the order $k^{-\\nu}$. Therefore, the approximation error has the bound:\n$$\n\\|r_{k}(A^{*}A)x^{\\dagger}\\| \\leq C_{1} k^{-\\nu}\n$$\nfor some constant $C_{1}$ that is independent of $k$ and $\\delta$.\n\n2.  **Bound on the Propagated Noise Error**\n\nThe propagated noise error is bounded using the noise condition $\\|y^{\\delta}-y\\| \\leq \\delta$:\n$$\n\\|g_{k}(A^{*}A)A^{*}(y^{\\delta}-y)\\| \\leq \\|g_{k}(A^{*}A)A^{*}\\| \\|y^{\\delta}-y\\| \\leq \\|g_{k}(A^{*}A)A^{*}\\| \\delta\n$$\nWe need to bound the operator norm $\\|g_{k}(A^{*}A)A^{*}\\|$. We have:\n$$\n\\|g_{k}(A^{*}A)A^{*}\\|^{2} = \\|(g_{k}(A^{*}A)A^{*})(g_{k}(A^{*}A)A^{*})^{*}\\| = \\|g_{k}(A^{*}A)A^{*}A g_{k}(A^{*}A)\\| = \\|(A^{*}A) (g_{k}(A^{*}A))^{2}\\|\n$$\nUsing functional calculus again, this norm is:\n$$\n\\sup_{\\lambda \\in \\sigma(A^{*}A)} |\\lambda (g_{k}(\\lambda))^{2}| \\leq \\sup_{\\lambda \\in [0, 1]} \\frac{(1-(1-\\lambda)^{k})^{2}}{\\lambda}\n$$\nLet's analyze the asymptotic behavior of this supremum with respect to $k$. Consider the substitution $\\lambda = z/k$:\n$$\n\\sup_{\\lambda \\in [0, 1]} \\lambda (g_{k}(\\lambda))^{2} \\asymp \\sup_{z>0} \\frac{z}{k} \\left(\\frac{1-(1-z/k)^{k}}{z/k}\\right)^{2} = \\frac{1}{k} \\sup_{z>0} \\frac{(1-(1-z/k)^{k})^{2}}{z/k^{2}} \\cdot k^2 = k \\sup_{z>0} \\frac{(1-(1-z/k)^{k})^{2}}{z}\n$$\nFor large $k$, $(1-z/k)^{k} \\approx \\exp(-z)$. Thus, the supremum scales as:\n$$\n\\asymp k \\sup_{z>0} \\frac{(1-\\exp(-z))^{2}}{z}\n$$\nThe function $\\frac{(1-\\exp(-z))^{2}}{z}$ is positive and its supremum over $z>0$ is a finite positive constant. Therefore, we have:\n$$\n\\sup_{\\lambda \\in [0, 1]} \\lambda (g_{k}(\\lambda))^{2} \\leq C'_{2} k\n$$\nThis implies $\\|g_{k}(A^{*}A)A^{*}\\| \\leq \\sqrt{C'_{2} k} = C_{2} \\sqrt{k}$. The propagated noise error is thus bounded by:\n$$\n\\|g_{k}(A^{*}A)A^{*}(y^{\\delta}-y)\\| \\leq C_{2} \\delta \\sqrt{k}\n$$\nfor some constant $C_{2}$ independent of $k$ and $\\delta$.\n\n3.  **Balancing the Error Bounds**\n\nThe total error norm is bounded by the sum of the two derived bounds:\n$$\n\\|x_{k}^{\\delta}-x^{\\dagger}\\| \\leq C_{1} k^{-\\nu} + C_{2} \\delta \\sqrt{k}\n$$\nTo find the iteration count $k$ that asymptotically minimizes this upper bound as a function of the noise level $\\delta$, we use the balancing principle. This principle states that the optimal choice is made when the two error components are of the same order of magnitude.\n$$\nC_{1} k^{-\\nu} \\asymp C_{2} \\delta \\sqrt{k}\n$$\nIgnoring the constants $C_1$ and $C_2$, we set the orders equal:\n$$\nk^{-\\nu} \\asymp \\delta k^{1/2}\n$$\nRearranging the terms to solve for $k$:\n$$\nk^{-\\nu - 1/2} \\asymp \\delta\n$$\n$$\nk^{-(\\frac{2\\nu+1}{2})} \\asymp \\delta\n$$\n$$\nk^{\\frac{2\\nu+1}{2}} \\asymp \\delta^{-1}\n$$\n$$\nk \\asymp (\\delta^{-1})^{\\frac{2}{2\\nu+1}} = \\delta^{-\\frac{2}{2\\nu+1}}\n$$\nThus, the asymptotically optimal choice for the iteration count $k$ as a function of $\\delta$ is $k(\\delta) \\propto \\delta^{-\\frac{2}{2\\nu+1}}$. The problem asks for the analytic expression representing this asymptotic relationship, ignoring constants.", "answer": "$$\n\\boxed{\\delta^{-\\frac{2}{2\\nu+1}}}\n$$", "id": "3392708"}, {"introduction": "Building on the theoretical understanding of error balancing, this hands-on coding exercise makes the concept of semi-convergence tangible. You will construct an ill-posed problem with adversarial noise and compare an iteration stopped by the discrepancy principle against one that is allowed to run too long. This practice provides a striking visual and quantitative demonstration of how early stopping prevents \"overfitting\" to noise, a critical skill in data assimilation and inverse problems [@problem_id:3392768].", "problem": "Consider the linear inverse problem in finite dimensions with additive noise. Let $A \\in \\mathbb{R}^{n \\times n}$ be a square, ill-conditioned matrix, $x^\\dagger \\in \\mathbb{R}^n$ the exact solution, and $y = A x^\\dagger$ the noise-free data. The noisy data is $y^\\delta = y + e^\\delta$ with $\\lVert e^\\delta \\rVert_2 = \\delta$. You will study the behavior of two iterative methods: (i) Landweber iteration with discrepancy-principle stopping, and (ii) unconstrained gradient descent continued for a large, fixed number of iterations. Your goal is to construct an adversarial noise scenario and quantify how discrepancy-based stopping avoids overfitting relative to unconstrained descent.\n\nFundamental basis and definitions:\n- The singular value decomposition (SVD) of $A$ is $A = U \\Sigma V^\\top$, where $U \\in \\mathbb{R}^{n \\times n}$ and $V \\in \\mathbb{R}^{n \\times n}$ are orthogonal, and $\\Sigma = \\mathrm{diag}(\\sigma_1,\\ldots,\\sigma_n)$ with $\\sigma_1 \\ge \\cdots \\ge \\sigma_n > 0$.\n- The Landweber iteration for minimizing the data misfit $\\frac{1}{2} \\lVert A x - y^\\delta \\rVert_2^2$ with step size $\\omega \\in (0, 2/\\lVert A \\rVert_2^2)$ is $x_{k+1}^\\delta = x_k^\\delta + \\omega A^\\top (y^\\delta - A x_k^\\delta)$, with $x_0^\\delta = 0$.\n- The discrepancy principle with parameter $\\tau > 1$ stops at the first index $k_\\ast$ such that $\\lVert A x_{k_\\ast}^\\delta - y^\\delta \\rVert_2 \\le \\tau \\delta$.\n- Unconstrained gradient descent here is the same Landweber update but continued for a prescribed large number of iterations $K$ without using $\\delta$.\n\nAdversarial noise construction:\n- You will construct $A$ with prescribed singular value decay via an SVD model: $A = U \\Sigma V^\\top$, where $U$ and $V$ are random orthogonal matrices, and $\\Sigma = \\mathrm{diag}(\\sigma_i)$ with $\\sigma_i = i^{-p}$ for an exponent $p > 0$.\n- You will construct $x^\\dagger$ by assigning coefficients in the right singular vector basis as $(\\alpha_i)_{i=1}^n$ with $\\alpha_i = i^{-q}$ for an exponent $q > 0$, and then normalizing so that $\\lVert x^\\dagger \\rVert_2 = 1$, that is $x^\\dagger = V \\alpha / \\lVert \\alpha \\rVert_2$.\n- You will choose adversarial noise aligned with the left singular vector corresponding to the smallest singular value: $e^\\delta = \\delta \\, u_n$, where $u_n$ is the $n$-th column of $U$. This makes $\\lVert e^\\delta \\rVert_2 = \\delta$ and concentrates the noise in the most amplified solution component.\n\nNumerical tasks to implement for each test case:\n1. Build $A = U \\Sigma V^\\top$ as described, with $U$ and $V$ obtained by orthonormalizing random Gaussian matrices using a fixed seed.\n2. Construct $x^\\dagger$ and $y = A x^\\dagger$, as described.\n3. Construct the adversarial noise $e^\\delta = \\delta u_n$ and the noisy data $y^\\delta = y + e^\\delta$.\n4. Choose the Landweber step size $\\omega = \\frac{1.9}{\\sigma_1^2}$, where $\\sigma_1$ is the largest singular value in $\\Sigma$.\n5. Run Landweber iteration with discrepancy-principle stopping: stop at the first index $k_\\ast$ such that $\\lVert A x_{k_\\ast}^\\delta - y^\\delta \\rVert_2 \\le \\tau \\delta$, with a sufficiently large safety cap on the number of iterations to ensure termination if possible.\n6. Run the same iteration for a fixed large number of iterations $K$ (unconstrained gradient descent).\n7. Compute the relative reconstruction errors $\\varepsilon_{\\mathrm{reg}} = \\lVert x_{k_\\ast}^\\delta - x^\\dagger \\rVert_2 / \\lVert x^\\dagger \\rVert_2$ and $\\varepsilon_{\\mathrm{unc}} = \\lVert x_{K}^\\delta - x^\\dagger \\rVert_2 / \\lVert x^\\dagger \\rVert_2$, and report the ratio $r = \\varepsilon_{\\mathrm{unc}} / \\varepsilon_{\\mathrm{reg}}$.\n\nTest suite:\nFor each tuple $(n, p, q, \\delta, \\tau, \\mathrm{seed}, K)$ below, carry out the steps above and return the ratio $r$.\n\n- Case A (happy path, moderate ill-posedness): $(n, p, q, \\delta, \\tau, \\mathrm{seed}, K) = (50, 1.5, 1.0, 10^{-3}, 1.1, 0, 20000)$.\n- Case B (severe ill-posedness): $(50, 2.5, 1.0, 10^{-3}, 1.1, 1, 40000)$.\n- Case C (larger noise level and milder decay): $(80, 1.2, 0.5, 10^{-2}, 1.05, 2, 15000)$.\n- Case D (very severe ill-posedness, small noise): $(50, 3.0, 1.5, 10^{-4}, 1.2, 3, 60000)$.\n\nAnswer specification and output format:\n- For each case, the result is a single float $r$.\n- Your program must produce a single line of output containing the list of the four ratios $[r_A, r_B, r_C, r_D]$ as a comma-separated list enclosed in square brackets. Each ratio must be rounded to six decimal places.\n- No physical units are involved. All angles, if any, are irrelevant here.\n\nYour program must be self-contained and produce the exact specified final output format without any additional text.", "solution": "The user-provided problem is valid. It is a well-posed numerical experiment in the field of inverse problems, designed to illustrate the concept of iterative regularization and the phenomenon of overfitting. The problem is scientifically grounded in the theory of linear inverse problems and numerical optimization, with all parameters and procedures clearly and objectively defined.\n\nThe problem investigates a linear inverse problem of the form $y^\\delta = A x^\\dagger + e^\\delta$, where the goal is to recover the true solution $x^\\dagger \\in \\mathbb{R}^n$ from noisy data $y^\\delta \\in \\mathbb{R}^n$. The matrix $A \\in \\mathbb{R}^{n \\times n}$ is ill-conditioned, meaning its singular values decay rapidly. This ill-conditioning is a hallmark of inverse problems, where small perturbations in the data can lead to large errors in a naive solution. The noise term $e^\\delta$ has a known norm $\\lVert e^\\delta \\rVert_2 = \\delta$.\n\nWe use the singular value decomposition (SVD) of $A$, $A = U \\Sigma V^\\top$, as the fundamental tool for analysis. Here, $U = [u_1, \\dots, u_n]$ and $V = [v_1, \\dots, v_n]$ are orthogonal matrices whose columns are the left and right singular vectors, respectively. The matrix $\\Sigma = \\mathrm{diag}(\\sigma_1, \\dots, \\sigma_n)$ contains the singular values, ordered non-increasingly: $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge \\sigma_n > 0$. The ill-conditioning of $A$ implies that the ratio $\\sigma_1/\\sigma_n$ is large. A naive solution, $x_{\\mathrm{naive}} = A^{-1} y^\\delta$, would amplify the noise. Using the SVD, this is $x_{\\mathrm{naive}} = V \\Sigma^{-1} U^\\top y^\\delta$. The noise component in the direction of $u_n$, where the singular value $\\sigma_n$ is smallest, gets amplified by a factor of $1/\\sigma_n$. The problem constructs an adversarial noise $e^\\delta = \\delta u_n$ precisely to maximize this effect.\n\nThe problem compares two iterative approaches to solve for $x$. Both are based on the Landweber iteration, which is a form of gradient descent applied to minimize the data misfit functional $J(x) = \\frac{1}{2} \\lVert Ax - y^\\delta \\rVert_2^2$. The iteration is given by:\n$$x_{k+1}^\\delta = x_k^\\delta - \\omega \\nabla J(x_k^\\delta) = x_k^\\delta + \\omega A^\\top (y^\\delta - A x_k^\\delta)$$\nstarting from $x_0^\\delta = 0$. The step size $\\omega$ must be chosen in $(0, 2/\\lVert A \\rVert_2^2) = (0, 2/\\sigma_1^2)$ to ensure convergence. The problem specifies $\\omega = 1.9/\\sigma_1^2$.\n\nIn the SVD basis, the Landweber iterate can be expressed as $x_k^\\delta = \\sum_{i=1}^n c_{k,i} v_i$. The coefficients evolve according to:\n$$c_{k,i} = \\frac{\\langle y^\\delta, u_i \\rangle}{\\sigma_i} \\left(1 - (1 - \\omega \\sigma_i^2)^k\\right)$$\nAs the number of iterations $k \\to \\infty$, the filter factors $(1 - (1 - \\omega \\sigma_i^2)^k)$ approach $1$. The solution coefficients $c_{k,i}$ converge to $\\langle y^\\delta, u_i \\rangle / \\sigma_i$, which are the coefficients of the naive solution $A^{-1} y^\\delta$. For the component $i=n$, the noise $e^\\delta = \\delta u_n$ results in $\\langle y^\\delta, u_n \\rangle = \\langle y, u_n \\rangle + \\delta$. The coefficient $c_{k,n}$ converges to $\\frac{\\langle y, u_n \\rangle}{\\sigma_n} + \\frac{\\delta}{\\sigma_n}$. The term $\\delta/\\sigma_n$ is enormous due to the smallness of $\\sigma_n$, leading to catastrophic noise amplification. This is what happens in the \"unconstrained gradient descent\" case, where the iteration is run for a large, fixed number of steps $K$, causing it to \"overfit\" to the noise. The resulting error $\\varepsilon_{\\mathrm{unc}}$ is expected to be large.\n\nThe second approach uses the Landweber iteration but incorporates a stopping rule known as the discrepancy principle. This principle, a form of regularization, dictates that the iteration should be stopped as soon as the data misfit of the iterate becomes comparable to the noise level. Specifically, we stop at the first iteration $k_\\ast$ such that:\n$$\\lVert A x_{k_\\ast}^\\delta - y^\\delta \\rVert_2 \\le \\tau \\delta$$\nwhere $\\tau > 1$ is a safety factor. This stopping rule prevents the iteration from proceeding long enough to significantly amplify the high-frequency noise components. The filter factors $(1 - (1 - \\omega \\sigma_i^2)^{k_\\ast})$ for small $\\sigma_i$ (i.e., high-frequency components) will be close to $0$ because $k_\\ast$ is relatively small. This damps the noisy components, especially the one corresponding to $u_n$, resulting in a stable, regularized solution $x_{k_\\ast}^\\delta$ with a much smaller error $\\varepsilon_{\\mathrm{reg}}$.\n\nThe numerical task is to implement this scenario for four different parameter sets. For each case, we construct the matrix $A$, the true solution $x^\\dagger$, and the noisy data $y^\\delta$ according to the SVD-based model. Then we run the two iterative schemes: one stopped by the discrepancy principle and one run for a fixed large number of iterations $K$. Finally, we compute the relative reconstruction errors, $\\varepsilon_{\\mathrm{reg}}$ and $\\varepsilon_{\\mathrm{unc}}$, and their ratio $r = \\varepsilon_{\\mathrm{unc}} / \\varepsilon_{\\mathrm{reg}}$, which quantifies the benefit of using a proper regularization strategy. This ratio is expected to be significantly larger than $1$.\n\nThe implementation will proceed as follows:\n1.  A function will be defined to handle a single test case, taking the parameters $(n, p, q, \\delta, \\tau, \\mathrm{seed}, K)$ as input.\n2.  Inside this function, a random number generator seed is set for reproducibility.\n3.  Random orthogonal matrices $U$ and $V$ of size $n \\times n$ are generated by applying QR decomposition to random Gaussian matrices.\n4.  The diagonal matrix of singular values $\\Sigma$ is constructed with entries $\\sigma_i = i^{-p}$ for $i \\in \\{1, \\dots, n\\}$.\n5.  The matrix $A$ is assembled as $A = U \\Sigma V^\\top$.\n6.  The true solution $x^\\dagger$ is constructed from coefficients $\\alpha_i = i^{-q}$, normalized such that $\\lVert x^\\dagger \\rVert_2 = 1$, and transformed into the standard basis: $x^\\dagger = V (\\alpha / \\lVert \\alpha \\rVert_2)$. The noise-free data is $y = A x^\\dagger$.\n7.  The adversarial noise $e^\\delta = \\delta u_n$ is created, where $u_n$ is the last column of $U$. The noisy data becomes $y^\\delta = y + e^\\delta$.\n8.  The Landweber step size is set to $\\omega = 1.9 / \\sigma_1^2$.\n9.  The Landweber iteration with the discrepancy principle is run. Starting with $x_0^\\delta = 0$, the iteration proceeds until $\\lVert A x_k^\\delta - y^\\delta \\rVert_2 \\le \\tau \\delta$. The resulting solution is $x_{\\mathrm{reg}} = x_{k_\\ast}^\\delta$.\n10. The unconstrained Landweber iteration is run for a fixed $K$ steps, yielding the solution $x_{\\mathrm{unc}} = x_K^\\delta$.\n11. The relative errors $\\varepsilon_{\\mathrm{reg}} = \\lVert x_{\\mathrm{reg}} - x^\\dagger \\rVert_2 / \\lVert x^\\dagger \\rVert_2$ and $\\varepsilon_{\\mathrm{unc}} = \\lVert x_{\\mathrm{unc}} - x^\\dagger \\rVert_2 / \\lVert x^\\dagger \\rVert_2$ are calculated.\n12. The ratio $r = \\varepsilon_{\\mathrm{unc}} / \\varepsilon_{\\mathrm{reg}}$ is computed and returned.\n\nThis procedure will be repeated for all four test cases, and the final list of ratios will be formatted as requested.", "answer": "```python\nimport numpy as np\n\ndef run_case(n, p, q, delta, tau, seed, K):\n    \"\"\"\n    Solves a single test case for the iterative regularization problem.\n    \"\"\"\n    # 1. Build A = U Sigma V^T\n    np.random.seed(seed)\n    \n    # Generate random orthogonal matrices U and V\n    H1 = np.random.randn(n, n)\n    U, _ = np.linalg.qr(H1)\n    \n    H2 = np.random.randn(n, n)\n    V, _ = np.linalg.qr(H2)\n    \n    # Generate singular values and Sigma matrix\n    i_vals = np.arange(1, n + 1)\n    sigma_vals = i_vals**(-p)\n    Sigma = np.diag(sigma_vals)\n    \n    A = U @ Sigma @ V.T\n\n    # 2. Construct x_dagger and y\n    alpha_coeffs = i_vals**(-q)\n    alpha_norm_factor = np.linalg.norm(alpha_coeffs)\n    alpha_normalized = alpha_coeffs / alpha_norm_factor\n    \n    x_dagger = V @ alpha_normalized\n    y = A @ x_dagger\n\n    # 3. Construct adversarial noise and noisy data\n    u_n = U[:, -1]\n    e_delta = delta * u_n\n    y_delta = y + e_delta\n\n    # 4. Choose Landweber step size\n    sigma_1 = sigma_vals[0]\n    omega = 1.9 / (sigma_1**2)\n\n    # 5. Run Landweber with discrepancy principle\n    x_k_reg = np.zeros(n)\n    # Use a large enough safety cap for iterations\n    max_iter_reg = max(100000, 2 * K)\n    \n    for _ in range(max_iter_reg):\n        residual = y_delta - A @ x_k_reg\n        residual_norm = np.linalg.norm(residual)\n        \n        if residual_norm <= tau * delta:\n            break\n        \n        x_k_reg = x_k_reg + omega * (A.T @ residual)\n    else:\n        # This part should not be reached if the problem is well-posed.\n        # It indicates failure of the discrepancy principle to stop.\n        # Assign NaN or raise an error to signal failure.\n        x_k_reg = np.full(n, np.nan)\n    \n    x_reg = x_k_reg\n\n    # 6. Run unconstrained Landweber for K iterations\n    x_k_unc = np.zeros(n)\n    for _ in range(K):\n        residual = y_delta - A @ x_k_unc\n        x_k_unc = x_k_unc + omega * (A.T @ residual)\n        \n    x_unc = x_k_unc\n\n    # 7. Compute errors and ratio\n    norm_x_dagger = np.linalg.norm(x_dagger) # Should be 1.0 by construction\n    \n    err_reg = np.linalg.norm(x_reg - x_dagger)\n    err_unc = np.linalg.norm(x_unc - x_dagger)\n    \n    eps_reg = err_reg / norm_x_dagger\n    eps_unc = err_unc / norm_x_dagger\n    \n    ratio = eps_unc / eps_reg\n    return ratio\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        # (n, p, q, delta, tau, seed, K)\n        (50, 1.5, 1.0, 10**-3, 1.1, 0, 20000),  # Case A\n        (50, 2.5, 1.0, 10**-3, 1.1, 1, 40000),  # Case B\n        (80, 1.2, 0.5, 10**-2, 1.05, 2, 15000), # Case C\n        (50, 3.0, 1.5, 10**-4, 1.2, 3, 60000),  # Case D\n    ]\n\n    results = []\n    for case in test_cases:\n        n, p, q, delta, tau, seed, K = case\n        result = run_case(n, p, q, delta, tau, seed, K)\n        results.append(result)\n\n    # Format output as required\n    formatted_results = [f'{r:.6f}' for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3392768"}, {"introduction": "While methods like Landweber iteration are excellent for illustrating theoretical principles, practical applications often demand more efficient algorithms. This exercise compares the performance of the simple Landweber method with the more powerful Conjugate Gradient for Least Squares (CGLS) method on an ill-posed problem. By implementing both, you will gain insight into the significant practical advantages of accelerated methods and observe how they achieve regularization much more rapidly [@problem_id:3392772].", "problem": "Consider the linear inverse problem defined by a square diagonal matrix $A \\in \\mathbb{R}^{50 \\times 50}$ with $A = \\mathrm{diag}(\\sigma_j)$ where $\\sigma_j = 10^{-j}$ for $j \\in \\{1,\\dots,50\\}$. Let the exact data be $b^\\dagger = A x^\\dagger$ for an exact solution $x^\\dagger \\in \\mathbb{R}^{50}$. The measured data are $b^\\delta = b^\\dagger + e^\\delta$, where the noise vector $e^\\delta \\in \\mathbb{R}^{50}$ satisfies $\\lVert e^\\delta \\rVert_2 = \\delta$ and is constructed deterministically as\n$$\nu_j = (-1)^j \\, j,\\quad j \\in \\{1,\\dots,50\\},\\qquad e^\\delta = \\delta \\, \\frac{u}{\\lVert u \\rVert_2}.\n$$\nYou will study iterative regularization via two methods: the Conjugate Gradient for Least Squares (CGLS) method and the Landweber iteration. The CGLS method is defined as the Conjugate Gradient algorithm applied to the normal equations $A^\\top A x = A^\\top b^\\delta$ starting from $x_0 = 0$, with the canonical residuals and search directions determined by the method. The Landweber iteration is defined as gradient descent on the least-squares objective $\\frac{1}{2}\\lVert A x - b^\\delta \\rVert_2^2$, with iteration\n$$\nx_{k+1} = x_k + \\tau A^\\top \\left(b^\\delta - A x_k\\right),\n$$\nstarting from $x_0 = 0$ and a fixed stepsize $\\tau$ chosen to satisfy $0 < \\tau < \\frac{2}{\\lVert A \\rVert_2^2}$.\n\nYour task is to:\n- Implement the first $10$ iterations of the CGLS method and the Landweber iteration for the same computational cost, where computational cost is measured by the total number of applications of $A$ and $A^\\top$. Use $x_0 = 0$ for both methods and the same number of iterations for both methods.\n- Set the Landweber stepsize to $\\tau = \\frac{1}{\\lVert A \\rVert_2^2}$, where $\\lVert A \\rVert_2$ is the spectral norm of $A$.\n- For each iteration $k \\in \\{1,\\dots,10\\}$ and each method, compute the relative error $\\frac{\\lVert x_k - x^\\dagger \\rVert_2}{\\lVert x^\\dagger \\rVert_2}$.\n- For each method, determine the iteration index $k$ in $\\{1,\\dots,10\\}$ that minimizes the relative error and record that index and the corresponding minimal relative error. This demonstrates semi-convergence behavior.\n\nUse the following test suite of exact solutions $x^\\dagger$ and noise levels $\\delta$:\n1. Case A: $x^\\dagger_j = \\frac{1}{j}$ for $j \\in \\{1,\\dots,50\\}$, $\\delta = 10^{-3}$.\n2. Case B: $x^\\dagger_j = 1$ for $j \\in \\{1,\\dots,50\\}$, $\\delta = 10^{-3}$.\n3. Case C: $x^\\dagger_j = \\frac{1}{j}$ for $j \\in \\{1,\\dots,50\\}$, $\\delta = 10^{-6}$.\n4. Case D: $x^\\dagger_j = 1$ for $j \\in \\{1,\\dots,50\\}$, $\\delta = 10^{-2}$.\n\nConstraints and definitions to use:\n- The Landweber stepsize is fixed to $\\tau = \\frac{1}{\\lVert A \\rVert_2^2}$, where $\\lVert A \\rVert_2 = \\max_{j} \\sigma_j$.\n- The CGLS method must be implemented directly as Conjugate Gradient on the normal equations without any external black-box solver.\n- The initial iterate for all methods is $x_0 = 0$.\n- All norms are the Euclidean norm $\\lVert \\cdot \\rVert_2$.\n- Angles do not appear; no angle units are needed. No physical units are involved.\n\nFor each case in the test suite, produce a result of the form $[k_{\\mathrm{CGLS}}, \\mathrm{err}_{\\mathrm{CGLS}}, k_{\\mathrm{LW}}, \\mathrm{err}_{\\mathrm{LW}}]$, where $k_{\\mathrm{CGLS}}$ and $k_{\\mathrm{LW}}$ are integers in $\\{1,\\dots,10\\}$, and $\\mathrm{err}_{\\mathrm{CGLS}}$ and $\\mathrm{err}_{\\mathrm{LW}}$ are floats equal to the minimal relative errors over the first $10$ iterations for the respective methods.\n\nFinal output format requirement:\n- Your program should produce a single line of output containing the results for the $4$ cases as a comma-separated list of lists, enclosed in square brackets. For example: $[[\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot]]$.\n- Each float must be formatted in scientific notation with six digits after the decimal point using a lower-case $e$ (for example, $1.234567e-04$).\n- The final printed line must be exactly in this format, with no additional text.\n\nThis problem is purely mathematical and algorithmic; no physical units are involved. Ensure all calculations use real arithmetic with full double precision.", "solution": "The problem presented is a well-posed numerical exercise in the field of inverse problems, specifically focusing on the comparison of two iterative regularization methods: Landweber iteration and the Conjugate Gradient for Least Squares (CGLS) method. The problem is mathematically sound, self-contained, and all parameters and objectives are clearly defined. I will therefore proceed with a complete solution.\n\n### 1. Theoretical Framework\n\nThe core of the problem is the linear system $A x = b^\\delta$, where we seek to find a stable and meaningful approximation of an unknown true solution $x^\\dagger$ given noisy data $b^\\delta$. The matrix $A$ is a diagonal matrix with entries $\\sigma_j = 10^{-j}$ for $j \\in \\{1, \\dots, 50\\}$. These values, which are the singular values of $A$, decay rapidly towards zero. This property makes the inverse problem severely ill-posed: small perturbations in the data $b^\\delta$ (due to noise $e^\\delta$) can lead to large, unphysical oscillations in the naively computed solution, $x = A^{-1} b^\\delta$. This is because the inverse matrix $A^{-1}$ has singular values $1/\\sigma_j = 10^j$, which amplify noise components aligned with the corresponding singular vectors.\n\nRegularization methods are designed to counteract this instability. Iterative methods, such as Landweber and CGLS, provide regularization by projecting the problem onto a sequence of subspaces of increasing dimension. The iteration count $k$ acts as the regularization parameter. By stopping the iteration early (i.e., before the algorithm starts to fit the noise), we can obtain a stable approximation of $x^\\dagger$. This phenomenon, where the error $\\lVert x_k - x^\\dagger \\rVert_2$ first decreases and then increases with $k$, is known as semi-convergence. The task is to find the optimal iteration count $k$ within the first $10$ iterations that minimizes this error.\n\nThe problem uses a fixed problem size $N=50$. The matrix $A$ is diagonal, so $A = A^\\top$ and its spectral norm is $\\lVert A \\rVert_2 = \\max_j |\\sigma_j| = \\sigma_1 = 10^{-1}$.\n\n### 2. Method Implementation\n\nWe will implement two iterative methods starting with the initial guess $x_0 = 0$. For each method and each test case, we will compute the iterates $x_1, \\dots, x_{10}$ and their corresponding relative errors $\\frac{\\lVert x_k - x^\\dagger \\rVert_2}{\\lVert x^\\dagger \\rVert_2}$.\n\n#### 2.1. Landweber Iteration\n\nThe Landweber iteration is a form of gradient descent applied to the least-squares functional $J(x) = \\frac{1}{2}\\lVert Ax - b^\\delta \\rVert_2^2$. The gradient is $\\nabla J(x) = A^\\top(Ax - b^\\delta)$. The iterative update rule is:\n$$\nx_{k+1} = x_k - \\tau \\nabla J(x_k) = x_k + \\tau A^\\top(b^\\delta - Ax_k)\n$$\nThe problem specifies a stepsize $\\tau = \\frac{1}{\\lVert A \\rVert_2^2}$. With $\\lVert A \\rVert_2 = 10^{-1}$, we have $\\tau = \\frac{1}{(10^{-1})^2} = 100$. Since $A$ is a real diagonal matrix, $A^\\top = A$. The iteration starting from $x_0=0$ is implemented by repeatedly applying this formula for $k=0, 1, \\dots, 9$.\n\n#### 2.2. Conjugate Gradient for Least Squares (CGLS)\n\nThe CGLS method applies the Conjugate Gradient (CG) algorithm to solve the normal equations $A^\\top A x = A^\\top b^\\delta$. This is significantly more efficient than simple gradient descent. The standard algorithm, adapted for stability and starting with $x_0 = 0$, is as follows:\n\n1.  Initialize:\n    $x_0 = 0$\n    $r_0 = b^\\delta - A x_0 = b^\\delta$\n    $g_0 = A^\\top r_0$\n    $p_0 = g_0$\n\n2.  Iterate for $k = 0, 1, \\dots, 9$:\n    $q_k = A p_k$\n    $\\alpha_k = \\frac{\\lVert g_k \\rVert_2^2}{\\lVert q_k \\rVert_2^2}$\n    $x_{k+1} = x_k + \\alpha_k p_k$\n    $r_{k+1} = r_k - \\alpha_k q_k$\n    $g_{k+1} = A^\\top r_{k+1}$\n    $\\beta_k = \\frac{\\lVert g_{k+1} \\rVert_2^2}{\\lVert g_k \\rVert_2^2}$\n    $p_{k+1} = g_{k+1} + \\beta_k p_k$\n\nEach iteration of both Landweber and CGLS requires one application of $A$ and one of $A^\\top$, making their per-iteration computational cost comparable, as specified in the problem statement.\n\n### 3. Execution Plan\n\nThe overall procedure for each of the four test cases is:\n\n1.  **Setup**: Construct the vector of singular values $\\sigma \\in \\mathbb{R}^{50}$ where $\\sigma_j = 10^{-j}$. Construct the specified true solution $x^\\dagger$ and calculate its norm $\\lVert x^\\dagger \\rVert_2$. Compute the true data $b^\\dagger = A x^\\dagger$. Construct the deterministic noise vector $e^\\delta$ with norm $\\delta$. Compute the noisy data $b^\\delta = b^\\dagger + e^\\delta$.\n\n2.  **Iteration and Error Calculation**:\n    -   Run the Landweber iteration for $10$ steps, storing the relative error $\\frac{\\lVert x_k - x^\\dagger \\rVert_2}{\\lVert x^\\dagger \\rVert_2}$ for each iterate $x_k$, $k \\in \\{1, \\dots, 10\\}$.\n    -   Run the CGLS iteration for $10$ steps, storing the relative error for each iterate $x_k$, $k \\in \\{1, \\dots, 10\\}$.\n\n3.  **Result Extraction**: For each method, find the minimum relative error among the $10$ computed values and the corresponding iteration index $k \\in \\{1, \\dots, 10\\}$.\n\n4.  **Formatting**: The final results for each test case, $[k_{\\mathrm{CGLS}}, \\mathrm{err}_{\\mathrm{CGLS}}, k_{\\mathrm{LW}}, \\mathrm{err}_{\\mathrm{LW}}]$, are assembled into a list for final output according to the specified format. The Python code in the `<final_answer>` section implements this logic.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_one_case(x_dagger_type: str, delta: float):\n    \"\"\"\n    Runs one test case for the inverse problem, comparing Landweber and CGLS.\n    \"\"\"\n    N = 50\n    # Use 1-based indexing for correspondence with problem statement\n    j_indices = np.arange(1, N + 1)\n\n    # Setup matrix A (as a vector of singular values)\n    sigma = 10.0**(-j_indices)\n\n    # Setup exact solution x_dagger\n    if x_dagger_type == '1/j':\n        x_dagger = 1.0 / j_indices\n    elif x_dagger_type == '1':\n        x_dagger = np.ones(N)\n    else:\n        raise ValueError(\"Unknown x_dagger_type\")\n    \n    norm_x_dagger = np.linalg.norm(x_dagger)\n\n    # Setup data b_dagger and b_delta\n    b_dagger = sigma * x_dagger\n    \n    u = ((-1.0)**j_indices) * j_indices\n    norm_u = np.linalg.norm(u)\n    e_delta = delta * u / norm_u\n    \n    b_delta = b_dagger + e_delta\n\n    # --- Landweber Iteration ---\n    errors_lw = []\n    x_lw = np.zeros(N)\n    # Spectral norm ||A||_2 is max(sigma) = 10^-1 = 0.1\n    # tau = 1 / ||A||_2^2 = 1 / (0.1)^2 = 100\n    tau = 100.0\n    \n    for _ in range(10):\n        residual = b_delta - sigma * x_lw\n        # Since A is diagonal real, A^T = A\n        grad = sigma * residual\n        x_lw = x_lw + tau * grad\n        \n        rel_error = np.linalg.norm(x_lw - x_dagger) / norm_x_dagger\n        errors_lw.append(rel_error)\n\n    min_err_lw = min(errors_lw)\n    k_lw = errors_lw.index(min_err_lw) + 1\n\n    # --- CGLS Iteration ---\n    errors_cgls = []\n    x_cgls = np.zeros(N)\n    r = b_delta  # r_0 = b - A*x_0 = b since x_0=0\n    g = sigma * r  # g_0 = A^T * r_0\n    p = g        # p_0 = g_0\n    norm_g_sq = np.dot(g, g)\n\n    for _ in range(10):\n        q = sigma * p # q_k = A*p_k\n        alpha = norm_g_sq / np.dot(q, q)\n        \n        x_cgls = x_cgls + alpha * p\n        \n        r = r - alpha * q\n        g_next = sigma * r\n        norm_g_next_sq = np.dot(g_next, g_next)\n        \n        beta = norm_g_next_sq / norm_g_sq\n        p = g_next + beta * p\n        \n        g = g_next\n        norm_g_sq = norm_g_next_sq\n        \n        rel_error = np.linalg.norm(x_cgls - x_dagger) / norm_x_dagger\n        errors_cgls.append(rel_error)\n    \n    min_err_cgls = min(errors_cgls)\n    k_cgls = errors_cgls.index(min_err_cgls) + 1\n    \n    return [k_cgls, min_err_cgls, k_lw, min_err_lw]\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {'x_spec': '1/j', 'delta': 1e-3}, # Case A\n        {'x_spec': '1',   'delta': 1e-3}, # Case B\n        {'x_spec': '1/j', 'delta': 1e-6}, # Case C\n        {'x_spec': '1',   'delta': 1e-2}, # Case D\n    ]\n\n    results = []\n    for case in test_cases:\n        result = run_one_case(case['x_spec'], case['delta'])\n        results.append(result)\n\n    # Format the final output string\n    formatted_results = []\n    for res in results:\n        k_cgls, err_cgls, k_lw, err_lw = res\n        s = f\"[{k_cgls},{err_cgls:.6e},{k_lw},{err_lw:.6e}]\"\n        formatted_results.append(s)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3392772"}]}