{"hands_on_practices": [{"introduction": "Understanding the boundaries of an algorithm's stability is a crucial first step in its practical application. This exercise provides a hands-on construction of a scenario where the Landweber iteration diverges. By explicitly violating the stepsize condition, you will directly observe how error components can be amplified rather than reduced, reinforcing the theoretical necessity of the condition $0  \\omega  2/\\|A\\|^2$ [@problem_id:3372411].", "problem": "Consider a linear inverse problem in a finite-dimensional Euclidean space where the data model is $y = A x^{\\ast}$ with $A \\in \\mathbb{R}^{2 \\times 2}$ and the true state $x^{\\ast} \\in \\mathbb{R}^{2}$. The Landweber iteration for estimating $x^{\\ast}$ is defined by $x_{k+1} = x_{k} + \\omega A^{\\top} \\big( y - A x_{k} \\big)$, where $\\omega > 0$ is a fixed stepsize. Use the standard Euclidean norm for vectors and the induced operator norm for matrices, so that $\\|A\\|$ equals the largest singular value of $A$.\n\nConstruct an explicit example with \n- $A = \\sigma u v^{\\top}$ for some $\\sigma > 0$, where $u, v \\in \\mathbb{R}^{2}$ are unit vectors, \n- choose $u = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$ and $v = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$ so that $A = \\begin{pmatrix} \\sigma  0 \\\\ 0  0 \\end{pmatrix}$ has a single positive singular value equal to $\\|A\\| = \\sigma$ and the other singular value is zero,\n- set the data to be $y = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ so that the minimum-norm solution is $x^{\\dagger} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$,\n- initialize the iteration at $x_{0} = v = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$,\n- and choose the stepsize $\\omega = \\dfrac{3}{\\|A\\|^{2}}$.\n\nStarting from the fundamental definition of the Landweber iteration and the properties of the singular value decomposition (SVD), derive the one-step error recurrence along the nonzero singular direction and compute the magnitude of the corresponding scalar amplification factor. Your final answer must be the single real number equal to that magnitude. No rounding is required.", "solution": "The problem statement is validated as scientifically grounded, well-posed, objective, and self-consistent. All necessary information is provided to determine a unique solution.\n\nThe linear inverse problem is given by the data model $y = A x^{\\ast}$, where $y \\in \\mathbb{R}^{2}$ is the observed data, $A \\in \\mathbb{R}^{2 \\times 2}$ is the forward operator, and $x^{\\ast} \\in \\mathbb{R}^{2}$ is the true state to be estimated. The Landweber iteration is used to find an estimate $x_k$ of $x^{\\ast}$ and is defined by the recurrence relation:\n$$\nx_{k+1} = x_{k} + \\omega A^{\\top} \\big( y - A x_{k} \\big)\n$$\nwhere $k$ is the iteration index, $x_0$ is an initial guess, $\\omega > 0$ is a fixed stepsize (relaxation parameter), and $A^{\\top}$ is the transpose of $A$.\n\nThe analysis of the iteration's convergence is performed by studying the evolution of the error vector, defined as $e_k = x_k - x^{\\dagger}$, where $x^{\\dagger}$ is the true solution being sought. In this problem, it is specified that $y = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$. The set of all solutions to $Ax=y$ is the null space of $A$, denoted $\\text{ker}(A)$. The minimum-norm solution $x^{\\dagger}$ is the element in $\\text{ker}(A)$ with the smallest Euclidean norm. With $A = \\begin{pmatrix} \\sigma  0 \\\\ 0  0 \\end{pmatrix}$ and $\\sigma>0$, the equation $Ax=y$ becomes $\\begin{pmatrix} \\sigma  0 \\\\ 0  0 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$, which implies $\\sigma x_1 = 0$, so $x_1=0$. The solution set is $\\{ (0,c) \\mid c \\in \\mathbb{R} \\}$. The minimum-norm solution occurs when $c=0$, thus $x^{\\dagger} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$. Consequently, the error vector is simply $e_k = x_k$.\n\nWe can now derive the error recurrence relation. Since $y = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ and $x^{\\dagger} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$, we have $y = A x^{\\dagger}$. We substitute this into the Landweber iteration formula:\n$$\nx_{k+1} = x_k + \\omega A^{\\top} (A x^{\\dagger} - A x_k)\n$$\nSubtracting $x^{\\dagger}$ from both sides gives:\n$$\nx_{k+1} - x^{\\dagger} = x_k - x^{\\dagger} - \\omega A^{\\top} A (x_k - x^{\\dagger})\n$$\nThis yields the error propagation equation:\n$$\ne_{k+1} = e_k - \\omega A^{\\top} A e_k = \\left(I - \\omega A^{\\top} A\\right) e_k\n$$\nwhere $I$ is the $2 \\times 2$ identity matrix. The matrix $G = I - \\omega A^{\\top} A$ is the error propagation operator.\n\nThe problem's behavior is best analyzed in the coordinate system defined by the singular value decomposition (SVD) of $A$. Let the SVD of $A$ be $A = U \\Sigma V^{\\top}$, where $U$ and $V$ are orthogonal matrices whose columns are the left and right singular vectors, respectively, and $\\Sigma$ is a diagonal matrix of singular values $\\sigma_j$. The right singular vectors $\\{v_j\\}$ form an orthonormal basis for the domain $\\mathbb{R}^2$. They are the eigenvectors of $A^{\\top} A$, since $A^{\\top} A = (V \\Sigma^{\\top} U^{\\top}) (U \\Sigma V^{\\top}) = V (\\Sigma^{\\top} \\Sigma) V^{\\top}$. The matrix $\\Sigma^{\\top} \\Sigma$ is a diagonal matrix with entries $\\sigma_j^2$. Therefore, $A^{\\top} A v_j = \\sigma_j^2 v_j$.\n\nThe error propagation operator $G$ acts on the right singular vectors $v_j$ as follows:\n$$\nG v_j = (I - \\omega A^{\\top} A) v_j = I v_j - \\omega (A^{\\top} A v_j) = v_j - \\omega \\sigma_j^2 v_j = (1 - \\omega \\sigma_j^2) v_j\n$$\nThis shows that the right singular vectors $v_j$ of $A$ are also the eigenvectors of the error propagation operator $G$. The corresponding eigenvalues are $\\lambda_j = 1 - \\omega \\sigma_j^2$. If the error at step $k$ has a component along $v_j$, say $\\alpha_{k,j} v_j$, then at step $k+1$ this component becomes $\\alpha_{k+1,j} v_j = \\lambda_j (\\alpha_{k,j} v_j)$. The value $\\lambda_j$ is therefore the scalar amplification factor for the error component in the direction of $v_j$.\n\nThe problem provides specific values. The matrix is $A = \\begin{pmatrix} \\sigma  0 \\\\ 0  0 \\end{pmatrix}$ with $\\sigma > 0$. The singular values are $\\sigma_1 = \\sigma$ and $\\sigma_2 = 0$. The corresponding right singular vectors can be chosen as $v_1 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$ and $v_2 = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$. The operator norm is $\\|A\\| = \\max(\\sigma_1, \\sigma_2) = \\sigma$.\n\nThe \"nonzero singular direction\" corresponds to the singular value $\\sigma_1 = \\sigma$. The problem asks for the scalar amplification factor along this direction, which is $\\lambda_1 = 1 - \\omega \\sigma_1^2 = 1 - \\omega \\sigma^2$.\n\nThe stepsize is given as $\\omega = \\dfrac{3}{\\|A\\|^{2}}$. Since $\\|A\\| = \\sigma$, we have $\\omega = \\dfrac{3}{\\sigma^2}$.\n\nSubstituting this value of $\\omega$ into the expression for the amplification factor:\n$$\n\\lambda_1 = 1 - \\left(\\frac{3}{\\sigma^2}\\right) \\sigma^2 = 1 - 3 = -2\n$$\nThe scalar amplification factor for the error component along the nonzero singular direction is $-2$.\n\nThe problem asks for the magnitude of this scalar amplification factor.\n$$\n| \\lambda_1 | = | -2 | = 2\n$$\nThis result indicates that the error component in the direction of the first singular vector is amplified by a factor of $2$ at each iteration, causing the iteration to diverge. This is expected, as the provided stepsize $\\omega = \\frac{3}{\\|A\\|^2}$ is outside the standard convergence interval for Landweber iteration, which is $0  \\omega  \\frac{2}{\\|A\\|^2}$.", "answer": "$$\\boxed{2}$$", "id": "3372411"}, {"introduction": "While the standard Landweber iteration provides a foundational approach, its performance can be significantly enhanced by incorporating prior knowledge about the solution. This practice explores such an improvement by using projection onto a convex set [@problem_id:3372385]. You will analyze a model problem where the signal and noise components are distinct and quantify how projecting out the noise-contaminated mode at each step prevents error amplification, leading to a more accurate reconstruction.", "problem": "Consider a linear inverse problem posed on a real Hilbert space $\\mathcal{H}$ with inner product $\\langle \\cdot, \\cdot \\rangle$ and norm $\\|\\cdot\\|$. Let $\\{\\varphi_{L}, \\varphi_{H}\\}$ be an orthonormal basis for a two-dimensional model subspace of $\\mathcal{H}$, where $\\varphi_{L}$ is a low-frequency mode and $\\varphi_{H}$ is a high-frequency mode. Let $A : \\mathcal{H} \\to \\mathcal{H}$ be a bounded, self-adjoint, positive operator modeling a smoothing forward map such that\n$$\nA \\varphi_{L} = a_{L} \\varphi_{L}, \\qquad A \\varphi_{H} = a_{H} \\varphi_{H},\n$$\nwith $a_{L}, a_{H} \\in (0,1]$ and $a_{H}  a_{L}$. The observed data $y \\in \\mathcal{H}$ are generated according to the additive noise model $y = A x^{\\dagger} + \\eta$, where the true solution is $x^{\\dagger} = s \\varphi_{L}$ with $s \\in \\mathbb{R}$, and the noise $\\eta = \\nu \\varphi_{H}$ with $\\nu \\in \\mathbb{R}$ excites only the high-frequency mode.\n\nDefine the Landweber iteration (with initial iterate $x^{0} = 0$) by\n$$\nx^{n+1} = x^{n} + \\omega A \\big( y - A x^{n} \\big), \\qquad n \\in \\mathbb{N},\n$$\nwhere the stepsize $\\omega$ satisfies $0  \\omega  \\frac{2}{\\|A\\|^{2}}$, and $\\|A\\| = a_{L}$. Consider also the projected Landweber iteration onto the convex set\n$$\nC = \\big\\{ x \\in \\mathcal{H} : \\langle x, \\varphi_{H} \\rangle = 0 \\big\\},\n$$\ngiven by\n$$\nx^{n+1}_{\\mathrm{proj}} = P_{C}\\!\\left( x^{n}_{\\mathrm{proj}} + \\omega A \\big( y - A x^{n}_{\\mathrm{proj}} \\big) \\right),\n$$\nwhere $P_{C}$ denotes the orthogonal projection onto $C$.\n\nUsing only the core definitions and the model specified above, derive the squared reconstruction error $\\|x^{N} - x^{\\dagger}\\|^{2}$ for the unconstrained Landweber iteration and the squared reconstruction error $\\|x^{N}_{\\mathrm{proj}} - x^{\\dagger}\\|^{2}$ for the projected Landweber iteration at a fixed iteration index $N \\in \\mathbb{N}$. Then, quantify the improvement due to projection by computing the reduction\n$$\n\\Delta E(N) = \\|x^{N} - x^{\\dagger}\\|^{2} - \\|x^{N}_{\\mathrm{proj}} - x^{\\dagger}\\|^{2}.\n$$\nProvide your final answer as a single closed-form analytic expression for $\\Delta E(N)$ in terms of $a_{H}$, $\\omega$, $\\nu$, and $N$ only. No numerical evaluation is required. Express your final answer without units.", "solution": "The user wants to analyze the effect of projection on the Landweber iteration for a specific linear inverse problem.\n\n## PROBLEM VALIDATION\n### Step 1: Extract Givens\n-   **Hilbert Space:** $\\mathcal{H}$ is a real Hilbert space with inner product $\\langle \\cdot, \\cdot \\rangle$ and norm $\\|\\cdot\\|$.\n-   **Basis:** $\\{\\varphi_{L}, \\varphi_{H}\\}$ is an orthonormal basis for a two-dimensional subspace of $\\mathcal{H}$.\n-   **Forward Operator:** $A : \\mathcal{H} \\to \\mathcal{H}$ is a bounded, self-adjoint, positive operator.\n-   **Eigen-relations:** $A \\varphi_{L} = a_{L} \\varphi_{L}$ and $A \\varphi_{H} = a_{H} \\varphi_{H}$, with $a_{L}, a_{H} \\in (0,1]$ and $a_{H}  a_{L}$.\n-   **Data Model:** $y = A x^{\\dagger} + \\eta$.\n-   **True Solution:** $x^{\\dagger} = s \\varphi_{L}$, for some $s \\in \\mathbb{R}$.\n-   **Noise:** $\\eta = \\nu \\varphi_{H}$, for some $\\nu \\in \\mathbb{R}$.\n-   **Unconstrained Landweber Iteration:** $x^{n+1} = x^{n} + \\omega A ( y - A x^{n} )$ with initial iterate $x^{0} = 0$.\n-   **Stepsize Condition:** $0  \\omega  \\frac{2}{\\|A\\|^{2}}$, with $\\|A\\| = a_{L}$.\n-   **Convex Set:** $C = \\{ x \\in \\mathcal{H} : \\langle x, \\varphi_{H} \\rangle = 0 \\}$. This is the subspace spanned by $\\varphi_L$.\n-   **Projected Landweber Iteration:** $x^{n+1}_{\\mathrm{proj}} = P_{C}( x^{n}_{\\mathrm{proj}} + \\omega A ( y - A x^{n}_{\\mathrm{proj}} ) )$ with initial iterate $x^{0}_{\\mathrm{proj}} = 0$, where $P_C$ is the orthogonal projection onto $C$.\n-   **Objective:** Compute $\\Delta E(N) = \\|x^{N} - x^{\\dagger}\\|^{2} - \\|x^{N}_{\\mathrm{proj}} - x^{\\dagger}\\|^{2}$ in terms of $a_{H}$, $\\omega$, $\\nu$, and $N$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is mathematically well-defined and self-contained. It describes a standard scenario in the analysis of iterative regularization methods for inverse problems, specifically comparing the standard Landweber iteration to a projected variant. The setup is a simplified but rigorous \"toy model\" commonly used to understand the behavior of such algorithms. All terms and conditions are precisely defined. The problem is scientifically grounded in the field of inverse problems, objective, and well-posed. No flaws are identified.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\n## SOLUTION DERIVATION\nThe core of the task is to derive closed-form expressions for the squared reconstruction errors of the unconstrained and projected Landweber iterations at a fixed iteration index $N$, and then to compute their difference.\n\nWe will work with the error vectors $e^n = x^n - x^\\dagger$ and $e^n_{\\mathrm{proj}} = x^n_{\\mathrm{proj}} - x^\\dagger$. The problem is simplified by working in the orthonormal basis $\\{\\varphi_L, \\varphi_H\\}$. Any vector $x \\in \\text{span}\\{\\varphi_L, \\varphi_H\\}$ can be written as $x = x_L \\varphi_L + x_H \\varphi_H$. The action of the operator $A$ on such a vector is $A x = a_L x_L \\varphi_L + a_H x_H \\varphi_H$.\n\n### 1. Analysis of the Unconstrained Landweber Iteration\nThe error $e^n = x^n - x^\\dagger$ propagates according to the recurrence:\n$$e^{n+1} = x^{n+1} - x^\\dagger = \\left(x^n + \\omega A(y - Ax^n)\\right) - x^\\dagger$$\nSubstituting $y = Ax^\\dagger + \\eta$:\n$$e^{n+1} = x^n + \\omega A(Ax^\\dagger + \\eta - Ax^n) - x^\\dagger = (x^n - x^\\dagger) - \\omega A^2(x^n - x^\\dagger) + \\omega A\\eta$$\n$$e^{n+1} = (I - \\omega A^2)e^n + \\omega A\\eta$$\nThe initial error is $e^0 = x^0 - x^\\dagger = 0 - x^\\dagger = -s\\varphi_L$.\nThis is a linear recurrence for the error vector $e^n$. Its solution at step $N$ is:\n$$e^N = (I - \\omega A^2)^N e^0 + \\sum_{k=0}^{N-1} (I - \\omega A^2)^k (\\omega A\\eta)$$\nSince the operator $I - \\omega A^2$ is diagonal in the basis $\\{\\varphi_L, \\varphi_H\\}$ with eigenvalues $(1-\\omega a_L^2)$ and $(1-\\omega a_H^2)$, we can analyze the components of the error vector.\nThe components of the initial error $e^0$ are $e^0_L = -s$ and $e^0_H = 0$.\nThe noise term is $\\omega A\\eta = \\omega A(\\nu\\varphi_H) = \\omega \\nu a_H \\varphi_H$. Its components are $(\\omega A\\eta)_L = 0$ and $(\\omega A\\eta)_H = \\omega \\nu a_H$.\n\nThe component of the error along $\\varphi_L$ is:\n$$e^N_L = (1 - \\omega a_L^2)^N e^0_L + 0 = -s(1 - \\omega a_L^2)^N$$\nThe component of the error along $\\varphi_H$ is:\n$$e^N_H = (1 - \\omega a_H^2)^N e^0_H + \\sum_{k=0}^{N-1} (1 - \\omega a_H^2)^k (\\omega \\nu a_H) = \\omega \\nu a_H \\sum_{k=0}^{N-1} (1 - \\omega a_H^2)^k$$\nThis is a geometric series. Using the sum formula $\\sum_{k=0}^{N-1} r^k = \\frac{1-r^N}{1-r}$:\n$$e^N_H = \\omega \\nu a_H \\frac{1 - (1 - \\omega a_H^2)^N}{1 - (1 - \\omega a_H^2)} = \\omega \\nu a_H \\frac{1 - (1 - \\omega a_H^2)^N}{\\omega a_H^2} = \\frac{\\nu}{a_H} \\left(1 - (1 - \\omega a_H^2)^N\\right)$$\nThe total error vector is $e^N = e^N_L \\varphi_L + e^N_H \\varphi_H$. The squared reconstruction error is $\\|e^N\\|^2 = (e^N_L)^2 + (e^N_H)^2$ due to orthonormality.\n$$\\|x^N - x^\\dagger\\|^2 = \\left(-s(1-\\omega a_L^2)^N\\right)^2 + \\left(\\frac{\\nu}{a_H}\\left(1 - (1 - \\omega a_H^2)^N\\right)\\right)^2$$\n$$\\|x^N - x^\\dagger\\|^2 = s^2(1 - \\omega a_L^2)^{2N} + \\frac{\\nu^2}{a_H^2}\\left(1 - (1 - \\omega a_H^2)^N\\right)^2$$\n\n### 2. Analysis of the Projected Landweber Iteration\nThe set $C$ is the one-dimensional subspace spanned by $\\varphi_L$. The orthogonal projection $P_C$ acts on a vector $x = x_L\\varphi_L + x_H\\varphi_H$ as $P_C(x) = x_L\\varphi_L$.\nThe error propagation for the projected iteration is:\n$$e^{n+1}_{\\mathrm{proj}} = x^{n+1}_{\\mathrm{proj}} - x^\\dagger = P_C(x^n_{\\mathrm{proj}} + \\omega A (y - A x^n_{\\mathrm{proj}})) - x^\\dagger$$\nSince $x^\\dagger = s\\varphi_L \\in C$, we have $P_C(x^\\dagger) = x^\\dagger$. We can write $x^\\dagger = P_C(x^\\dagger)$ and use the linearity of $P_C$:\n$$e^{n+1}_{\\mathrm{proj}} = P_C\\left(x^n_{\\mathrm{proj}} - x^\\dagger + \\omega A(Ax^\\dagger + \\eta - Ax^n_{\\mathrm{proj}})\\right) = P_C\\left((I-\\omega A^2)e^n_{\\mathrm{proj}} + \\omega A\\eta\\right)$$\nThe initial error is $e^0_{\\mathrm{proj}} = x^0_{\\mathrm{proj}} - x^\\dagger = 0 - s\\varphi_L = -s\\varphi_L$, which is in $C$.\nLet's proceed by induction. Assume $e^n_{\\mathrm{proj}} \\in C$. Then $e^n_{\\mathrm{proj}} = \\alpha_n \\varphi_L$ for some scalar $\\alpha_n$.\nThe term $(I - \\omega A^2)e^n_{\\mathrm{proj}} = (I - \\omega A^2)(\\alpha_n \\varphi_L) = (1 - \\omega a_L^2)\\alpha_n \\varphi_L$, which is also in $C$.\nThe noise term is $\\omega A\\eta = \\omega \\nu a_H \\varphi_H$, which is orthogonal to $C$.\nApplying the projector $P_C$:\n$$e^{n+1}_{\\mathrm{proj}} = P_C\\left((1 - \\omega a_L^2)e^n_{\\mathrm{proj}} + \\omega \\nu a_H \\varphi_H\\right) = (1 - \\omega a_L^2)e^n_{\\mathrm{proj}} + P_C(\\omega \\nu a_H \\varphi_H)$$\nSince $\\omega \\nu a_H \\varphi_H$ is orthogonal to the subspace $C$, its projection is zero. Thus:\n$$e^{n+1}_{\\mathrm{proj}} = (1 - \\omega a_L^2)e^n_{\\mathrm{proj}}$$\nThis is a simple geometric recurrence. Starting with $e^0_{\\mathrm{proj}} = -s\\varphi_L$:\n$$e^N_{\\mathrm{proj}} = (1 - \\omega a_L^2)^N e^0_{\\mathrm{proj}} = -s(1 - \\omega a_L^2)^N \\varphi_L$$\nThe squared reconstruction error for the projected method is:\n$$\\|x^N_{\\mathrm{proj}} - x^\\dagger\\|^2 = \\|e^N_{\\mathrm{proj}}\\|^2 = \\|-s(1 - \\omega a_L^2)^N \\varphi_L\\|^2 = s^2(1 - \\omega a_L^2)^{2N}$$\n\n### 3. Computation of the Error Reduction $\\Delta E(N)$\nThe improvement due to projection is the difference between the two squared errors:\n$$\\Delta E(N) = \\|x^{N} - x^{\\dagger}\\|^{2} - \\|x^{N}_{\\mathrm{proj}} - x^{\\dagger}\\|^{2}$$\nSubstituting the derived expressions:\n$$\\Delta E(N) = \\left(s^2(1 - \\omega a_L^2)^{2N} + \\frac{\\nu^2}{a_H^2}\\left(1 - (1 - \\omega a_H^2)^N\\right)^2\\right) - \\left(s^2(1 - \\omega a_L^2)^{2N}\\right)$$\nThe terms involving the true signal amplitude $s$ and the low-frequency eigenvalue $a_L$ cancel out. This is because both methods reconstruct the signal part identically; the difference lies entirely in how they handle the noise component.\n$$\\Delta E(N) = \\frac{\\nu^2}{a_H^2}\\left(1 - (1 - \\omega a_H^2)^N\\right)^2$$\nThis expression quantifies the reduction in error, which is precisely the squared error contribution from the amplified noise component that the projection successfully eliminates. It depends only on the noise amplitude $\\nu$, the high-frequency eigenvalue $a_H$, the stepsize $\\omega$, and the number of iterations $N$, as requested.", "answer": "$$\\boxed{\\frac{\\nu^2}{a_H^2} \\left( 1 - \\left(1 - \\omega a_H^2\\right)^N \\right)^2}$$", "id": "3372385"}, {"introduction": "Theoretical convergence rates provide powerful predictions, but verifying them through numerical simulation solidifies understanding and builds practical skills. This final exercise transitions from analytical derivation to computational practice [@problem_id:3395632]. You will implement the Landweber iteration to empirically measure its convergence rate and compare it against the theoretical predictions derived from the problem's smoothness and spectral properties, bridging the gap between abstract theory and concrete numerical behavior.", "problem": "Consider a linear inverse problem in a finite-dimensional Euclidean space with a compact, self-adjoint, positive semidefinite operator represented by a matrix with a prescribed singular value decay. Let the forward model be $y = A x^\\dagger$, where $A \\in \\mathbb{R}^{n \\times n}$ is diagonalizable with singular values $\\sigma_i$ that follow a power-law decay $\\sigma_i \\asymp i^{-p}$ for some $p > 0$, and the true solution $x^\\dagger$ has coefficients in the singular value decomposition (SVD) basis that satisfy $c_i \\asymp i^{-(\\mu + 1/2)}$ for some $\\mu > 0$. Consider the Landweber iteration, initialized at $x^{(0)} = 0$, with a constant relaxation parameter $\\omega \\in (0, 2 / \\|A\\|^2)$, defined by the recursion $x^{(k+1)} = x^{(k)} + \\omega A^\\top \\left( y - A x^{(k)} \\right)$ for integers $k \\ge 0$. You will empirically verify the predicted algebraic convergence rate of the Landweber iteration error in terms of the iteration count $k$ under the prescribed spectral decay and source condition.\n\nYour program must:\n- Construct, for each test case, a diagonal matrix $A \\in \\mathbb{R}^{n \\times n}$ with diagonal entries $\\sigma_i = i^{-p}$ for $i \\in \\{1, 2, \\dots, n\\}$, with $n$ specified per the test suite.\n- Construct $x^\\dagger \\in \\mathbb{R}^n$ with entries $c_i = i^{-(\\mu + 1/2)}$ in the canonical basis that coincides with the SVD right singular vectors.\n- Form exact data $y = A x^\\dagger$.\n- Implement the Landweber iteration with $x^{(0)} = 0$ and $\\omega = 0.9 / \\|A\\|^2$. Note that since $A$ is diagonal with largest singular value $\\sigma_1 = 1$, one has $\\|A\\|^2 = 1$ and thus $\\omega = 0.9$.\n- Compute the error norm $\\|x^{(k)} - x^\\dagger\\|_2$ for a prescribed set of iteration counts $k$ in each test case.\n- Estimate the empirical convergence exponent $\\alpha_{\\mathrm{est}}$ by performing a least-squares linear regression of $\\log \\left( \\|x^{(k)} - x^\\dagger\\|_2 \\right)$ versus $\\log(k)$ over the provided $k$ values. Interpret the model as $\\|x^{(k)} - x^\\dagger\\|_2 \\approx C k^{-\\alpha}$, so that the slope of the regression line equals $-\\alpha_{\\mathrm{est}}$.\n- For each test case, compare the empirically estimated exponent $\\alpha_{\\mathrm{est}}$ with the theoretical prediction $\\alpha_{\\mathrm{th}} = \\mu / (2 p)$ that arises from combining the power-law spectral decay and the source condition under Landweber iteration.\n\nUse the following test suite, which is designed to probe a range of ill-posedness and smoothness regimes while avoiding degenerate parameter choices:\n- Test case $1$ (general case): $n = 16384$, $p = 1.5$, $\\mu = 1.0$, $\\omega = 0.9$, and iteration counts $k \\in \\{50, 100, 200, 400, 800, 1600\\}$.\n- Test case $2$ (mildly ill-posed, higher smoothness): $n = 16384$, $p = 0.5$, $\\mu = 1.5$, $\\omega = 0.9$, and iteration counts $k \\in \\{50, 100, 200, 400, 800, 1600\\}$.\n- Test case $3$ (severely ill-posed, higher smoothness): $n = 16384$, $p = 2.0$, $\\mu = 2.0$, $\\omega = 0.9$, and iteration counts $k \\in \\{50, 100, 200, 400, 800, 1600\\}$.\n\nFor numerical stability and efficiency, you may exploit the diagonal structure of $A$ and the closed-form expression of Landweber iterates in the SVD basis, but you must not use any formula in the code or the analysis that presupposes the target convergence rate; the verification must result from the implemented computation.\n\nYour program must output, for each of the above three test cases, a single floating-point number equal to the absolute difference $|\\alpha_{\\mathrm{est}} - \\alpha_{\\mathrm{th}}|$, rounded to three decimal places. Aggregate these three numbers into a single line as a comma-separated list enclosed in square brackets, with no spaces. For example, if the three absolute differences are $d_1$, $d_2$, and $d_3$, the output format must be $[d_1,d_2,d_3]$.\n\nThere are no physical units involved in this problem. All angles, if any appear, must be in radians, but no angles are used here.\n\nYour final program must produce exactly one line of output in the specified format. It must not read any input or write any files.", "solution": "The problem requires an empirical verification of the theoretical convergence rate of the Landweber iteration for a specific class of linear inverse problems. The verification is to be performed by simulating the iteration, estimating the convergence rate from the numerical results, and comparing it to the theoretical prediction.\n\nThe problem is defined in a finite-dimensional space $\\mathbb{R}^n$. The forward model is a linear equation $y = A x^\\dagger$, where $y \\in \\mathbb{R}^n$ represents the observed data, $x^\\dagger \\in \\mathbb{R}^n$ is the true solution to be recovered, and $A \\in \\mathbb{R}^{n \\times n}$ is the forward operator. The problem is constructed such that the underlying inverse problem is ill-posed, meaning that small perturbations in $y$ can lead to large errors in the solution.\n\nThe operator $A$ is specified as a diagonal matrix with diagonal entries $\\sigma_i$ representing its singular values. These are defined by a power-law decay:\n$$\n\\sigma_i = i^{-p} \\quad \\text{for } i = 1, 2, \\dots, n\n$$\nwhere $p > 0$ controls the degree of ill-posedness. Since $A$ is a real diagonal matrix, it is self-adjoint ($A = A^\\top$). Its eigenvalues are positive, so it is a positive definite operator.\n\nThe true solution $x^\\dagger$ has a specific smoothness, characterized by the decay of its coefficients $c_i$ in the canonical basis (which, for a diagonal $A$, is the basis of singular vectors):\n$$\nc_i = i^{-(\\mu + 1/2)} \\quad \\text{for } i = 1, 2, \\dots, n\n$$\nwhere $\\mu > 0$ is a smoothness parameter. Larger values of $\\mu$ correspond to \"smoother\" solutions. The data $y$ are exact, computed as $y = A x^\\dagger$. Since both $A$ and $x^\\dagger$ are defined component-wise in the same basis, this is simply $y_i = \\sigma_i c_i$.\n\nThe Landweber iteration is an iterative regularization method for solving such inverse problems. It is defined by the recurrence relation:\n$$\nx^{(k+1)} = x^{(k)} + \\omega A^\\top \\left( y - A x^{(k)} \\right)\n$$\nwith an initial guess $x^{(0)} = 0$. The parameter $\\omega$ is a relaxation or step-size parameter, which must satisfy $\\omega \\in (0, 2/\\|A\\|^2)$ to ensure convergence. For the given operator $A$, the operator norm (largest singular value) is $\\|A\\| = \\sigma_1 = 1^{-p} = 1$. The problem specifies $\\omega = 0.9 / \\|A\\|^2 = 0.9$.\n\nFor a diagonal operator $A$, the iteration decouples into $n$ independent scalar recursions, one for each component $x_i^{(k)}$:\n$$\nx_i^{(k+1)} = x_i^{(k)} + \\omega \\sigma_i (y_i - \\sigma_i x_i^{(k)}) = (1 - \\omega \\sigma_i^2) x_i^{(k)} + \\omega \\sigma_i y_i\n$$\nWith $y_i = \\sigma_i c_i$ and the initial condition $x_i^{(0)} = 0$, this linear recurrence has the closed-form solution:\n$$\nx_i^{(k)} = c_i \\left( 1 - (1 - \\omega \\sigma_i^2)^k \\right)\n$$\nThis expression allows for the direct computation of the iterate $x^{(k)}$ at any step $k$ without performing the full iteration. The error vector is $x^{(k)} - x^\\dagger$, and its $i$-th component is $x_i^{(k)} - c_i = -c_i (1 - \\omega \\sigma_i^2)^k$. The squared L2-norm of the error is therefore:\n$$\n\\|x^{(k)} - x^\\dagger\\|_2^2 = \\sum_{i=1}^{n} (x_i^{(k)} - c_i)^2 = \\sum_{i=1}^{n} c_i^2 \\left( (1 - \\omega \\sigma_i^2)^k \\right)^2 = \\sum_{i=1}^{n} c_i^2 (1 - \\omega \\sigma_i^2)^{2k}\n$$\nThe program computes this error norm for each specified iteration count $k$.\n\nTo estimate the convergence rate, we assume the error behaves according to an algebraic decay law: $\\|x^{(k)} - x^\\dagger\\|_2 \\approx C k^{-\\alpha}$ for some constant $C$ and exponent $\\alpha > 0$. Taking the natural logarithm of both sides linearizes this relationship:\n$$\n\\log\\left(\\|x^{(k)} - x^\\dagger\\|_2\\right) \\approx \\log(C) - \\alpha \\log(k)\n$$\nThis equation shows a linear relationship between $Y = \\log\\left(\\|x^{(k)} - x^\\dagger\\|_2\\right)$ and $X = \\log(k)$, with a slope of $-\\alpha$. The empirical exponent $\\alpha_{\\mathrm{est}}$ is determined by performing a least-squares linear regression on the set of points $\\{(\\log(k_j), \\log(\\|x^{(k_j)} - x^\\dagger\\|_2))\\}$ for the given iteration counts $k_j$. The estimated exponent is the negative of the slope of the fitted line.\n\nFor Landweber iteration applied to problems with spectral decay $\\sigma_i \\asymp i^{-p}$ and a solution smoothness (source condition) of $x^\\dagger_i \\asymp i^{-(\\mu+1/2)}$, regularization theory predicts a convergence rate of the error norm. The predicted exponent is:\n$$\n\\alpha_{\\mathrm{th}} = \\frac{\\mu}{2p}\n$$\nThe final step for each test case is to compute the absolute difference $|\\alpha_{\\mathrmest} - \\alpha_{\\mathrm{th}}|$ to quantify the agreement between the empirical measurement and the theoretical prediction.\n\nThe program implements this entire procedure. For each test case, it constructs the vectors corresponding to $\\sigma_i$ and $c_i$, calculates the error norms at the specified values of $k$ using the closed-form expression, performs a log-log linear regression to find $\\alpha_{\\mathrm{est}}$, calculates $\\alpha_{\\mathrm{th}}$, and outputs their absolute difference, rounded to three decimal places.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and verify Landweber iteration convergence rates.\n    \"\"\"\n\n    # Test suite as defined in the problem statement.\n    # Each tuple is (n, p, mu, omega, k_values)\n    test_cases = [\n        (16384, 1.5, 1.0, 0.9, np.array([50, 100, 200, 400, 800, 1600])),\n        (16384, 0.5, 1.5, 0.9, np.array([50, 100, 200, 400, 800, 1600])),\n        (16384, 2.0, 2.0, 0.9, np.array([50, 100, 200, 400, 800, 1600])),\n    ]\n\n    results = []\n    for n, p, mu, omega, k_values in test_cases:\n        # Construct the problem in the SVD basis (canonical basis here).\n        # The operator A is diagonal with entries sigma_i.\n        # The true solution x_dagger has entries c_i.\n        i = np.arange(1, n + 1, dtype=np.float64)\n        sigma = i**(-p)\n        c = i**(-(mu + 0.5))\n\n        # Compute the error norm ||x^(k) - x_dagger||_2 for each k.\n        # This is done using the closed-form expression for the error in the SVD basis.\n        # Error_i(k) = -c_i * (1 - omega * sigma_i^2)^k\n        # ||Error(k)||_2 = sqrt(sum_i (Error_i(k))^2)\n        error_norms = []\n        for k in k_values:\n            # The term (1 - omega * sigma^2) is raised to the power of k.\n            # This represents the decay of the error components.\n            # err_vec_k = c_i * (1 - omega * sigma_i^2)^k\n            err_vec_k = c * np.power(1.0 - omega * sigma**2, k)\n            \n            # The L2 norm is calculated using numpy.linalg.norm.\n            norm_k = np.linalg.norm(err_vec_k)\n            error_norms.append(norm_k)\n\n        # Estimate the empirical convergence exponent alpha_est.\n        # This is done by fitting a line to log(error) vs log(k).\n        # The model is: log(error) approx log(C) - alpha * log(k).\n        # The slope of the line is -alpha.\n        log_k = np.log(k_values)\n        log_error = np.log(np.array(error_norms))\n\n        # np.polyfit(x, y, 1) returns [slope, intercept] for the best-fit line.\n        slope, _ = np.polyfit(log_k, log_error, 1)\n        alpha_est = -slope\n        \n        # Calculate the theoretical convergence exponent.\n        alpha_th = mu / (2.0 * p)\n\n        # Calculate the absolute difference and round to three decimal places.\n        difference = abs(alpha_est - alpha_th)\n        results.append(round(difference, 3))\n\n    # Format the final output as a comma-separated list in brackets.\n    output_string = \",\".join(map(str, results))\n    print(f\"[{output_string}]\")\n\nsolve()\n```", "id": "3395632"}]}