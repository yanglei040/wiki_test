{"hands_on_practices": [{"introduction": "This first practice grounds the theory of Tikhonov regularization in a concrete calculation. Before automating parameter selection with sophisticated code, it is invaluable to develop an intuition for how the regularization parameter $\\alpha$ impacts the solution and the residual norm. This exercise guides you through the manual computation of regularized solutions for a simple, ill-conditioned system, demonstrating the monotonic relationship between $\\alpha$ and the residual, and providing a hands-on application of the Morozov Discrepancy Principle.", "problem": "Consider the linear inverse problem with observational noise described by $y^{\\delta} = A x^{\\dagger} + \\eta$, where $A \\in \\mathbb{R}^{2 \\times 2}$ is ill-conditioned, $x^{\\dagger} \\in \\mathbb{R}^{2}$ is the unknown true state, and $\\eta \\in \\mathbb{R}^{2}$ is the noise. Let the measurement matrix be\n$$\nA = \\begin{pmatrix}\n1  0 \\\\\n0  10^{-2}\n\\end{pmatrix},\n$$\nand the noisy data be\n$$\ny^{\\delta} = \\begin{pmatrix}\n1 \\\\\n0.2\n\\end{pmatrix}.\n$$\nFor Tikhonov regularization with identity penalty, define for any $\\alpha  0$ the regularized estimator $x_{\\alpha}^{\\delta}$ as the minimizer of\n$$\n\\|A x - y^{\\delta}\\|_{2}^{2} + \\alpha \\|x\\|_{2}^{2},\n$$\nand recall that $x_{\\alpha}^{\\delta}$ satisfies the normal equations\n$$\n(A^{\\top} A + \\alpha I) x_{\\alpha}^{\\delta} = A^{\\top} y^{\\delta}.\n$$\nYou are given the candidate set of regularization parameters\n$$\n\\mathcal{A} = \\{10^{-6}, 10^{-4}, 3 \\times 10^{-4}, 10^{-3}, 10^{-2}\\}.\n$$\nPerform the following tasks:\n- For each $\\alpha \\in \\mathcal{A}$, compute $x_{\\alpha}^{\\delta}$ and the residual norm $\\|A x_{\\alpha}^{\\delta} - y^{\\delta}\\|_{2}$.\n- Using first principles and the singular value decomposition (SVD) framework for $A$, discuss whether $\\|A x_{\\alpha}^{\\delta} - y^{\\delta}\\|_{2}$ is monotone in $\\alpha$ and justify your conclusion rigorously.\n\nAssume a known noise bound $\\|\\eta\\|_{2} \\le \\delta$ with $\\delta = 0.135$ and use the Morozov discrepancy principle with safety factor $\\tau = 1.1$, i.e., select $\\alpha^{\\star}$ such that $\\|A x_{\\alpha^{\\star}}^{\\delta} - y^{\\delta}\\|_{2} \\approx \\tau \\delta$ and, among the discrete set $\\mathcal{A}$, choose the smallest $\\alpha$ for which $\\|A x_{\\alpha}^{\\delta} - y^{\\delta}\\|_{2} \\ge \\tau \\delta$ holds. Report the selected value $\\alpha^{\\star}$ as your final answer. The final answer must be a single real-valued number.", "solution": "The problem statement is reviewed and found to be valid. It is scientifically grounded in the established theory of Tikhonov regularization for linear inverse problems, well-posed with all necessary information provided, and expressed in objective, mathematically precise language.\n\nThe core of the problem is to analyze the Tikhonov-regularized solution $x_{\\alpha}^{\\delta}$ for the linear system $y^{\\delta} = A x^{\\dagger} + \\eta$. The regularized solution minimizes the functional $\\|A x - y^{\\delta}\\|_{2}^{2} + \\alpha \\|x\\|_{2}^{2}$, and is given by the solution to the normal equations $(A^{\\top} A + \\alpha I) x_{\\alpha}^{\\delta} = A^{\\top} y^{\\delta}$.\n\n**Part 1: Computation of Regularized Solutions and Residuals**\n\nFirst, we define the matrices and vectors based on the problem statement.\nThe matrix $A$ and data vector $y^{\\delta}$ are:\n$$\nA = \\begin{pmatrix} 1  0 \\\\ 0  10^{-2} \\end{pmatrix}, \\quad y^{\\delta} = \\begin{pmatrix} 1 \\\\ 0.2 \\end{pmatrix}\n$$\nSince $A$ is a symmetric matrix, $A^{\\top} = A$. We compute the components of the normal equations:\n$$\nA^{\\top}A = A^2 = \\begin{pmatrix} 1^2  0 \\\\ 0  (10^{-2})^2 \\end{pmatrix} = \\begin{pmatrix} 1  0 \\\\ 0  10^{-4} \\end{pmatrix}\n$$\n$$\nA^{\\top}y^{\\delta} = Ay^{\\delta} = \\begin{pmatrix} 1  0 \\\\ 0  10^{-2} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0.2 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 2 \\times 10^{-3} \\end{pmatrix}\n$$\nThe normal equations become:\n$$\n\\left( \\begin{pmatrix} 1  0 \\\\ 0  10^{-4} \\end{pmatrix} + \\alpha \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} \\right) x_{\\alpha}^{\\delta} = \\begin{pmatrix} 1 \\\\ 2 \\times 10^{-3} \\end{pmatrix}\n$$\n$$\n\\begin{pmatrix} 1+\\alpha  0 \\\\ 0  10^{-4}+\\alpha \\end{pmatrix} x_{\\alpha}^{\\delta} = \\begin{pmatrix} 1 \\\\ 2 \\times 10^{-3} \\end{pmatrix}\n$$\nFor any $\\alpha  0$, the matrix on the left is diagonal and invertible, yielding the solution:\n$$\nx_{\\alpha}^{\\delta} = \\begin{pmatrix} \\frac{1}{1+\\alpha} \\\\ \\frac{2 \\times 10^{-3}}{10^{-4}+\\alpha} \\end{pmatrix}\n$$\nThe residual is $r_{\\alpha}^{\\delta} = A x_{\\alpha}^{\\delta} - y^{\\delta}$. The squared norm of the residual is:\n$$\n\\|A x_{\\alpha}^{\\delta} - y^{\\delta}\\|_{2}^{2} = \\left\\|\\begin{pmatrix} \\frac{1}{1+\\alpha} \\\\ \\frac{2 \\times 10^{-5}}{10^{-4}+\\alpha} \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 0.2 \\end{pmatrix}\\right\\|_{2}^{2} = \\left\\|\\begin{pmatrix} \\frac{-\\alpha}{1+\\alpha} \\\\ \\frac{2 \\times 10^{-5} - 0.2(10^{-4}+\\alpha)}{10^{-4}+\\alpha} \\end{pmatrix}\\right\\|_{2}^{2} = \\left\\|\\begin{pmatrix} \\frac{-\\alpha}{1+\\alpha} \\\\ \\frac{-0.2\\alpha}{10^{-4}+\\alpha} \\end{pmatrix}\\right\\|_{2}^{2}\n$$\n$$\n\\|A x_{\\alpha}^{\\delta} - y^{\\delta}\\|_{2}^{2} = \\alpha^2 \\left( \\frac{1}{(1+\\alpha)^2} + \\frac{0.04}{(10^{-4}+\\alpha)^2} \\right)\n$$\nWe compute $x_{\\alpha}^{\\delta}$ and $\\|A x_{\\alpha}^{\\delta} - y^{\\delta}\\|_{2}$ for each $\\alpha$ in the candidate set $\\mathcal{A} = \\{10^{-6}, 10^{-4}, 3 \\times 10^{-4}, 10^{-3}, 10^{-2}\\}$.\n\nFor $\\alpha = 10^{-6}$:\n$x_{10^{-6}}^{\\delta} = \\begin{pmatrix} \\frac{1}{1+10^{-6}} \\\\ \\frac{2 \\times 10^{-3}}{10^{-4}+10^{-6}} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{1.000001} \\\\ \\frac{200}{10.1} \\end{pmatrix}$.\n$\\|A x_{10^{-6}}^{\\delta} - y^{\\delta}\\|_{2} = \\sqrt{(10^{-6})^2 \\left( \\frac{1}{(1.000001)^2} + \\frac{0.04}{(1.01 \\times 10^{-4})^2} \\right)} \\approx 0.00198$.\n\nFor $\\alpha = 10^{-4}$:\n$x_{10^{-4}}^{\\delta} = \\begin{pmatrix} \\frac{1}{1+10^{-4}} \\\\ \\frac{2 \\times 10^{-3}}{10^{-4}+10^{-4}} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{1.0001} \\\\ 10 \\end{pmatrix}$.\n$\\|A x_{10^{-4}}^{\\delta} - y^{\\delta}\\|_{2} = \\sqrt{(10^{-4})^2 \\left( \\frac{1}{(1.0001)^2} + \\frac{0.04}{(2 \\times 10^{-4})^2} \\right)} = \\sqrt{\\frac{10^{-8}}{(1.0001)^2} + 0.01}  0.1$. A closer calculation gives $\\approx 0.10000005$.\n\nFor $\\alpha = 3 \\times 10^{-4}$:\n$x_{3 \\times 10^{-4}}^{\\delta} = \\begin{pmatrix} \\frac{1}{1+3 \\times 10^{-4}} \\\\ \\frac{2 \\times 10^{-3}}{10^{-4}+3 \\times 10^{-4}} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{1.0003} \\\\ 5 \\end{pmatrix}$.\n$\\|A x_{3 \\times 10^{-4}}^{\\delta} - y^{\\delta}\\|_{2} = \\sqrt{(3 \\times 10^{-4})^2 \\left( \\frac{1}{(1.0003)^2} + \\frac{0.04}{(4 \\times 10^{-4})^2} \\right)} = \\sqrt{\\frac{9 \\times 10^{-8}}{(1.0003)^2} + 0.0225}  0.15$. A closer calculation gives $\\approx 0.1500003$.\n\nFor $\\alpha = 10^{-3}$:\n$x_{10^{-3}}^{\\delta} = \\begin{pmatrix} \\frac{1}{1+10^{-3}} \\\\ \\frac{2 \\times 10^{-3}}{10^{-4}+10^{-3}} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{1.001} \\\\ \\frac{20}{11} \\end{pmatrix}$.\n$\\|A x_{10^{-3}}^{\\delta} - y^{\\delta}\\|_{2} = \\sqrt{(10^{-3})^2 \\left( \\frac{1}{(1.001)^2} + \\frac{0.04}{(1.1 \\times 10^{-3})^2} \\right)} \\approx 0.1818$.\n\nFor $\\alpha = 10^{-2}$:\n$x_{10^{-2}}^{\\delta} = \\begin{pmatrix} \\frac{1}{1+10^{-2}} \\\\ \\frac{2 \\times 10^{-3}}{10^{-4}+10^{-2}} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{1.01} \\\\ \\frac{2}{10.1} \\end{pmatrix}$.\n$\\|A x_{10^{-2}}^{\\delta} - y^{\\delta}\\|_{2} = \\sqrt{(10^{-2})^2 \\left( \\frac{1}{(1.01)^2} + \\frac{0.04}{(1.01 \\times 10^{-2})^2} \\right)} \\approx 0.1983$.\n\n**Part 2: Monotonicity of the Residual Norm**\n\nWe rigorously establish that the residual norm $\\rho(\\alpha) = \\|A x_{\\alpha}^{\\delta} - y^{\\delta}\\|_{2}$ is a monotone function of $\\alpha$. Consider the SVD of $A = U \\Sigma V^{\\top}$ with singular values $\\sigma_i  0$. The squared residual norm is $\\rho(\\alpha)^2 = \\alpha^2 \\sum_{i} \\frac{(u_i^{\\top} y^{\\delta})^2}{(\\sigma_i^2+\\alpha)^2}$. Differentiating with respect to $\\alpha$:\n$$\n\\frac{d}{d\\alpha} \\rho(\\alpha)^2 = \\sum_i (u_i^{\\top} y^{\\delta})^2 \\frac{d}{d\\alpha} \\left( \\frac{\\alpha^2}{(\\sigma_i^2+\\alpha)^2} \\right)\n$$\nThe derivative of the inner term is:\n$$\n\\frac{d}{d\\alpha} \\frac{\\alpha^2}{(\\sigma_i^2+\\alpha)^2} = \\frac{2\\alpha(\\sigma_i^2+\\alpha)^2 - \\alpha^2 \\cdot 2(\\sigma_i^2+\\alpha)(1)}{(\\sigma_i^2+\\alpha)^4} = \\frac{2\\alpha(\\sigma_i^2+\\alpha) - 2\\alpha^2}{(\\sigma_i^2+\\alpha)^3} = \\frac{2\\alpha\\sigma_i^2}{(\\sigma_i^2+\\alpha)^3}\n$$\nTherefore, the derivative of the squared residual norm is:\n$$\n\\frac{d}{d\\alpha} \\rho(\\alpha)^2 = \\sum_i (u_i^{\\top} y^{\\delta})^2 \\frac{2\\alpha\\sigma_i^2}{(\\sigma_i^2+\\alpha)^3}\n$$\nFor $\\alpha  0$, every term in this sum is non-negative. The derivative is strictly positive provided that for at least one index $i$, we have $\\sigma_i  0$ and the data component $u_i^{\\top} y^{\\delta} \\neq 0$. In this problem, $A$ is diagonal, so its singular values are $\\sigma_1=1$ and $\\sigma_2=10^{-2}$, and the singular vectors $u_i$ are the standard basis vectors $e_i$. The data components are $e_1^{\\top} y^{\\delta} = 1 \\neq 0$ and $e_2^{\\top} y^{\\delta} = 0.2 \\neq 0$. Thus, $\\frac{d}{d\\alpha} \\rho(\\alpha)^2  0$ for all $\\alpha  0$.\nThis proves that $\\rho(\\alpha)^2$ is a strictly monotonically increasing function of $\\alpha$. Since $\\rho(\\alpha)$ is non-negative, and the square-root function is strictly increasing for non-negative arguments, $\\rho(\\alpha)$ is also a strictly monotonically increasing function of $\\alpha$.\n\n**Part 3: Morozov Discrepancy Principle**\n\nThe Morozov discrepancy principle requires choosing the smallest $\\alpha$ from the discrete set $\\mathcal{A}$ such that $\\|A x_{\\alpha}^{\\delta} - y^{\\delta}\\|_{2} \\ge \\tau \\delta$.\nGiven $\\delta = 0.135$ and $\\tau = 1.1$, the target discrepancy is:\n$$\n\\tau \\delta = 1.1 \\times 0.135 = 0.1485\n$$\nWe check this condition for our calculated residual norms:\n- $\\alpha = 10^{-6}$: $\\|r_{\\alpha}\\|_{2} \\approx 0.00198  0.1485$.\n- $\\alpha = 10^{-4}$: $\\|r_{\\alpha}\\|_{2} \\approx 0.10000005  0.1485$.\n- $\\alpha = 3 \\times 10^{-4}$: $\\|r_{\\alpha}\\|_{2} \\approx 0.1500003 \\ge 0.1485$.\n- $\\alpha = 10^{-3}$: $\\|r_{\\alpha}\\|_{2} \\approx 0.1818 \\ge 0.1485$.\n- $\\alpha = 10^{-2}$: $\\|r_{\\alpha}\\|_{2} \\approx 0.1983 \\ge 0.1485$.\n\nThe parameters satisfying the condition are $\\{3 \\times 10^{-4}, 10^{-3}, 10^{-2}\\}$. The rule is to select the smallest of these.\nTherefore, the selected regularization parameter is $\\alpha^{\\star} = 3 \\times 10^{-4}$.", "answer": "$$\\boxed{3 \\times 10^{-4}}$$", "id": "3361699"}, {"introduction": "Building upon a foundational understanding, we now move to computational implementation, a core skill for any practitioner. This exercise challenges you to implement and compare two of the most widely-used a posteriori rules: the L-curve criterion and the Morozov Discrepancy Principle (MDP). You will computationally generate the famous L-curve, identify its \"corner\" by maximizing curvature, and contrast this geometric approach with the data-driven residual-matching of MDP, gaining practical insight into their behavior and relative performance.", "problem": "You are given a synthetic family of small linear inverse problems with a prescribed singular value structure and coefficients, to be regularized via Tikhonov regularization. For each case, you must compute the L-curve, estimate the L-curve corner for the Tikhonov regularization parameter, and compare it against the selection obtained by the Morozov Discrepancy Principle (MDP).\n\nAll problems are defined in the following purely mathematical setting. Consider a linear system with a known Singular Value Decomposition (SVD) basis, specialized to the identity for simplicity, so that the forward operator is diagonal. Let the forward operator be given by a diagonal matrix $A \\in \\mathbb{R}^{n \\times n}$ with diagonal entries equal to prescribed singular values $\\{\\sigma_i\\}_{i=1}^n$, and let the true unknown vector be represented in the right singular vector basis with coefficients $\\{c_i\\}_{i=1}^n$. The clean data are then given by $b_{\\mathrm{clean}} = A x_{\\mathrm{true}}$, where $x_{\\mathrm{true}}$ has coordinates $c_i$ in that basis. Define a deterministic noise direction $d \\in \\mathbb{R}^n$ and a noise level $\\delta  0$. Let $b = b_{\\mathrm{clean}} + \\eta$ with $\\eta = \\delta \\, d / \\lVert d \\rVert_2$, so that $\\lVert \\eta \\rVert_2 = \\delta$.\n\nFor Tikhonov regularization with identity penalty, the solution $x_\\alpha$ for a given parameter $\\alpha  0$ minimizes $\\lVert A x - b \\rVert_2^2 + \\alpha^2 \\lVert x \\rVert_2^2$. In the SVD basis with left and right singular vector matrices equal to the identity, the solution can be expressed using the data coordinates $\\beta_i = b_i$ as\n$$\nx_\\alpha = \\sum_{i=1}^n \\frac{\\sigma_i}{\\sigma_i^2 + \\alpha^2} \\, \\beta_i \\, e_i,\n$$\nwhere $e_i$ is the $i$-th canonical basis vector. The residual norm and solution norm can be computed via\n$$\n\\lVert A x_\\alpha - b \\rVert_2^2 = \\sum_{i=1}^n \\left(\\frac{\\alpha^2}{\\sigma_i^2 + \\alpha^2}\\right)^2 \\beta_i^2, \\quad\n\\lVert x_\\alpha \\rVert_2^2 = \\sum_{i=1}^n \\left(\\frac{\\sigma_i}{\\sigma_i^2 + \\alpha^2}\\right)^2 \\beta_i^2.\n$$\n\nThe L-curve is the parametric curve of points $\\left(\\log \\lVert A x_\\alpha - b \\rVert_2, \\log \\lVert x_\\alpha \\rVert_2\\right)$ as $\\alpha$ varies. The \"corner\" of the L-curve can be estimated by maximizing the curvature of this parametric curve in the plane. If we define the parameterization $t = \\log \\alpha$, $X(t) = \\log \\lVert A x_{e^t} - b \\rVert_2$, and $Y(t) = \\log \\lVert x_{e^t} \\rVert_2$, the curvature at $t$ is\n$$\n\\kappa(t) = \\frac{X'(t)Y''(t) - Y'(t)X''(t)}{\\left( \\left(X'(t)\\right)^2 + \\left(Y'(t)\\right)^2 \\right)^{3/2}}.\n$$\nIn practice, you should evaluate the curvature on a discrete grid of $t$ values using second-order central finite differences and select the parameter $\\alpha$ at the grid point with the maximal absolute curvature.\n\nThe Morozov Discrepancy Principle (MDP) selects the regularization parameter $\\alpha$ such that the residual norm matches the noise level, i.e.,\n$$\n\\lVert A x_\\alpha - b \\rVert_2 = \\delta.\n$$\nFor the diagonal operator considered here with full column rank, the residual norm is a strictly increasing function of $\\alpha \\in [0,\\infty)$ with limits $0$ as $\\alpha \\to 0^+$ and $\\lVert b \\rVert_2$ as $\\alpha \\to \\infty$. Therefore, a unique $\\alpha$ exists for any $\\delta \\in (0, \\lVert b \\rVert_2)$.\n\nYour task is to implement a program that, for each test case below, performs the following steps:\n1. Constructs $A$, $x_{\\mathrm{true}}$, $b_{\\mathrm{clean}}$, and $b = b_{\\mathrm{clean}} + \\eta$ with $\\lVert \\eta \\rVert_2 = \\delta$ using the specified $\\{\\sigma_i\\}$, $\\{c_i\\}$, noise direction $d$, and noise fraction $f$ where $\\delta = f \\, \\lVert b_{\\mathrm{clean}} \\rVert_2$.\n2. Computes the L-curve on a grid of $\\alpha$ values uniformly spaced in $\\log_{10} \\alpha$ between the specified $\\alpha_{\\min}$ and $\\alpha_{\\max}$ with the specified number of points $N_\\alpha$. Uses discrete second-order central differences with respect to $t = \\log \\alpha$ to approximate derivatives, evaluates curvature at interior points, and selects the L-curve corner estimate $\\alpha_{\\mathrm{LC}}$ as the $\\alpha$ corresponding to the maximal absolute curvature.\n3. Computes the MDP selection $\\alpha_{\\mathrm{MDP}}$ by solving $\\lVert A x_\\alpha - b \\rVert_2 = \\delta$ for $\\alpha$, using a robust bracketing root-finding method. If necessary, enlarge the bracket exponentially until a sign change is found. Select the smallest positive root (which is unique here).\n4. Produces, for each test case, the triple of real numbers $[\\alpha_{\\mathrm{LC}}, \\alpha_{\\mathrm{MDP}}, \\mathrm{rel\\_err}]$ where $\\mathrm{rel\\_err} = \\lvert \\alpha_{\\mathrm{LC}} - \\alpha_{\\mathrm{MDP}} \\rvert / \\alpha_{\\mathrm{MDP}}$, returned as floating-point numbers.\n\nImportant implementation details and constraints:\n- All computations are unitless; no physical units are involved.\n- When computing logarithms of norms, ensure the arguments are strictly positive. If numerical underflow occurs, clip norms to a positive floor such as $10^{-300}$ before taking logarithms to avoid undefined values.\n- Use only the data and methods described; do not assume any external hints or formulas.\n- The program must be self-contained and must not read input or write files.\n\nTest Suite:\nUse the following four test cases. For each case, $n = 6$, and the noise direction is $d = [1,-1,1,-1,1,-1]^T$.\n- Case 1 (moderate noise, geometric decay):\n  - $\\sigma = [1, 0.5, 0.25, 0.125, 0.0625, 0.03125]$\n  - $c = [1, 1, 1, 1, 1, 1]$\n  - Noise fraction $f = 0.05$\n  - Grid: $\\alpha_{\\min} = 10^{-6}$, $\\alpha_{\\max} = 10^{1}$, $N_\\alpha = 400$\n- Case 2 (very small noise, geometric decay):\n  - $\\sigma = [1, 0.5, 0.25, 0.125, 0.0625, 0.03125]$\n  - $c = [1, 1, 1, 1, 1, 1]$\n  - Noise fraction $f = 10^{-6}$\n  - Grid: $\\alpha_{\\min} = 10^{-8}$, $\\alpha_{\\max} = 10^{0}$, $N_\\alpha = 400$\n- Case 3 (large noise, geometric decay):\n  - $\\sigma = [1, 0.5, 0.25, 0.125, 0.0625, 0.03125]$\n  - $c = [1, 1, 1, 1, 1, 1]$\n  - Noise fraction $f = 0.4$\n  - Grid: $\\alpha_{\\min} = 10^{-6}$, $\\alpha_{\\max} = 10^{2}$, $N_\\alpha = 600$\n- Case 4 (severely ill-posed spectrum, moderate noise):\n  - $\\sigma = [1, 10^{-2}, 10^{-4}, 10^{-6}, 10^{-8}, 10^{-10}]$\n  - $c = [1, 1, 1, 1, 1, 1]$\n  - Noise fraction $f = 10^{-3}$\n  - Grid: $\\alpha_{\\min} = 10^{-10}$, $\\alpha_{\\max} = 10^{2}$, $N_\\alpha = 600$\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a JSON-like list of four items, one per test case, where each item is the list $[\\alpha_{\\mathrm{LC}}, \\alpha_{\\mathrm{MDP}}, \\mathrm{rel\\_err}]$. For example:\n[[aL1,aM1,e1],[aL2,aM2,e2],[aL3,aM3,e3],[aL4,aM4,e4]]\nEach $aLk$, $aMk$, and $ek$ must be a floating-point number. No additional text or lines should be printed.", "solution": "The problem presented is a valid and well-posed exercise in the numerical analysis of inverse problems. It asks for the implementation and comparison of two standard *a posteriori* regularization parameter choice rules—the L-curve criterion and the Morozov Discrepancy Principle—for Tikhonov regularization. The problem is scientifically grounded, self-contained, and algorithmically specified. We will proceed with a full solution.\n\nThe core task is to determine the Tikhonov regularization parameter $\\alpha$ for a linear system $Ax = b$, where the forward operator $A$ and the true solution $x_{\\mathrm{true}}$ are known, but the data $b$ are corrupted by noise.\n\n**1. Governing Equations and Problem Formulation**\n\nWe consider a linear inverse problem in $\\mathbb{R}^n$ defined by the system $Ax = b$. The forward operator $A \\in \\mathbb{R}^{n \\times n}$ is given as a diagonal matrix with diagonal entries corresponding to a set of singular values $\\{\\sigma_i\\}_{i=1}^n$, where $\\sigma_i  0$.\n$$\nA = \\mathrm{diag}(\\sigma_1, \\sigma_2, \\dots, \\sigma_n)\n$$\nThe true, unknown solution vector $x_{\\mathrm{true}}$ is specified by its coefficients $\\{c_i\\}_{i=1}^n$ in the canonical basis, such that $x_{\\mathrm{true}} = [c_1, c_2, \\dots, c_n]^T$. The clean data vector $b_{\\mathrm{clean}}$ is then given by\n$$\nb_{\\mathrm{clean}} = A x_{\\mathrm{true}}, \\quad \\text{with components} \\quad (b_{\\mathrm{clean}})_i = \\sigma_i c_i\n$$\nThe observed data $b$ are a perturbation of the clean data by additive noise $\\eta$, i.e., $b = b_{\\mathrm{clean}} + \\eta$. The noise vector $\\eta$ is constructed to have a specific magnitude $\\delta  0$ and direction $d \\in \\mathbb{R}^n$:\n$$\n\\eta = \\delta \\frac{d}{\\lVert d \\rVert_2}, \\quad \\text{so that} \\quad \\lVert \\eta \\rVert_2 = \\delta\n$$\nThe noise level $\\delta$ is defined as a fraction $f$ of the norm of the clean data:\n$$\n\\delta = f \\cdot \\lVert b_{\\mathrm{clean}} \\rVert_2\n$$\nThe problem is to find a stable approximation of $x_{\\mathrm{true}}$ from the noisy data $b$.\n\n**2. Tikhonov Regularization and Associated Norms**\n\nTikhonov regularization seeks to solve this ill-posed problem by finding a solution $x_\\alpha$ that minimizes a composite objective function, balancing data fidelity with solution stability. For a regularization parameter $\\alpha  0$, the solution $x_\\alpha$ is the minimizer of:\n$$\nJ_\\alpha(x) = \\lVert Ax - b \\rVert_2^2 + \\alpha^2 \\lVert x \\rVert_2^2\n$$\nSince $A$ is a diagonal matrix, the problem decouples. The $i$-th component of the regularized solution, $(x_\\alpha)_i$, is found to be:\n$$\n(x_\\alpha)_i = \\frac{\\sigma_i b_i}{\\sigma_i^2 + \\alpha^2}\n$$\nThe quality of a regularized solution is assessed by the residual norm, $\\lVert Ax_\\alpha - b \\rVert_2$, and the solution norm, $\\lVert x_\\alpha \\rVert_2$. Their squared values, which we denote as $\\rho(\\alpha)$ and $\\xi(\\alpha)$ respectively, have convenient closed-form expressions:\n$$\n\\rho(\\alpha) = \\lVert Ax_\\alpha - b \\rVert_2^2 = \\sum_{i=1}^n \\left( \\sigma_i (x_\\alpha)_i - b_i \\right)^2 = \\sum_{i=1}^n \\left( \\frac{\\alpha^2 b_i}{\\sigma_i^2 + \\alpha^2} \\right)^2\n$$\n$$\n\\xi(\\alpha) = \\lVert x_\\alpha \\rVert_2^2 = \\sum_{i=1}^n \\left( (x_\\alpha)_i \\right)^2 = \\sum_{i=1}^n \\left( \\frac{\\sigma_i b_i}{\\sigma_i^2 + \\alpha^2} \\right)^2\n$$\nThe central challenge is to select an appropriate value for $\\alpha$.\n\n**3. L-Curve Criterion for Parameter Selection**\n\nThe L-curve is a parametric plot of the (logarithm of) the solution norm versus the (logarithm of) the residual norm. As specified, the curve is parameterized by $\\alpha  0$:\n$$\n\\left( \\log \\lVert A x_\\alpha - b \\rVert_2, \\log \\lVert x_\\alpha \\rVert_2 \\right) = \\left( \\frac{1}{2}\\log \\rho(\\alpha), \\frac{1}{2}\\log \\xi(\\alpha) \\right)\n$$\nThis curve typically has a characteristic 'L' shape. The corner of the 'L' is often considered to represent a good balance between minimizing the residual (data fidelity) and minimizing the solution norm (regularity). We identify this corner by finding the point of maximum curvature.\n\nTo compute the curvature, we re-parameterize the curve using $t = \\log \\alpha$. Let $X(t) = \\log \\lVert A x_{e^t} - b \\rVert_2$ and $Y(t) = \\log \\lVert x_{e^t} \\rVert_2$. The curvature $\\kappa(t)$ is given by the standard formula for a plane curve:\n$$\n\\kappa(t) = \\frac{X'(t)Y''(t) - Y'(t)X''(t)}{\\left( (X'(t))^2 + (Y'(t))^2 \\right)^{3/2}}\n$$\nWe compute $\\alpha_{\\mathrm{LC}}$ by performing the following steps:\n1.  Generate a grid of $N_\\alpha$ values of $\\alpha$ spaced logarithmically between $\\alpha_{\\min}$ and $\\alpha_{\\max}$. This corresponds to a uniform grid for $t = \\log \\alpha$ with spacing $\\Delta t$.\n2.  For each $\\alpha$ on the grid, calculate $\\rho(\\alpha)$ and $\\xi(\\alpha)$, and then the functions $X(t)$ and $Y(t)$. A floor of $10^{-300}$ is used for the norm arguments of the logarithm to prevent numerical errors.\n3.  Approximate the first and second derivatives, $X'(t)$, $X''(t)$, $Y'(t)$, and $Y''(t)$, at the interior points of the grid using second-order central finite differences:\n    $$\n    F'(t_j) \\approx \\frac{F(t_{j+1}) - F(t_{j-1})}{2 \\Delta t}, \\quad F''(t_j) \\approx \\frac{F(t_{j+1}) - 2F(t_j) + F(t_{j-1})}{(\\Delta t)^2}\n    $$\n4.  Substitute these approximate derivatives into the curvature formula $\\kappa(t)$.\n5.  The regularization parameter $\\alpha_{\\mathrm{LC}}$ is chosen as the grid value of $\\alpha$ corresponding to the maximum absolute curvature $|\\kappa(t)|$.\n\n**4. Morozov Discrepancy Principle (MDP)**\n\nThe Morozov Discrepancy Principle provides an alternative rule for selecting $\\alpha$. It is based on the premise that an ideal regularized solution should reproduce the data only up to the level of noise. Mathematically, it selects the parameter $\\alpha$ that satisfies:\n$$\n\\lVert A x_\\alpha - b \\rVert_2 = \\delta \\quad \\text{or equivalently} \\quad \\rho(\\alpha) = \\delta^2\n$$\nTo find the parameter $\\alpha_{\\mathrm{MDP}}$, we must solve the nonlinear scalar equation $g(\\alpha) = 0$, where $g(\\alpha) = \\rho(\\alpha) - \\delta^2$. The function $\\rho(\\alpha)$ is a strictly monotonic increasing function of $\\alpha$ for $\\alpha  0$, ranging from $\\rho(0)=0$ to $\\rho(\\infty) = \\lVert b \\rVert_2^2$. Thus, for any $\\delta \\in (0, \\lVert b \\rVert_2)$, a unique positive solution $\\alpha_{\\mathrm{MDP}}$ exists. This solution is found numerically using a robust bracketing root-finding algorithm, such as the Brent-Dekker method. If an initial search interval does not bracket a root, it is expanded exponentially until a sign change in $g(\\alpha)$ is detected.\n\n**5. Algorithmic Implementation Summary**\n\nFor each test case provided, the following computational procedure is executed:\n1.  The problem parameters $\\{\\sigma_i\\}$, $\\{c_i\\}$, $d$, and $f$ are used to construct the vectors $x_{\\mathrm{true}}$, $b_{\\mathrm{clean}}$, and $b$, and the scalar noise level $\\delta$.\n2.  To find $\\alpha_{\\mathrm{LC}}$, the quantities $\\rho(\\alpha)$ and $\\xi(\\alpha)$ are computed on the specified logarithmic grid for $\\alpha$. The derivatives and curvature are calculated at interior grid points using finite differences, and the $\\alpha$ corresponding to the maximum absolute curvature is selected.\n3.  To find $\\alpha_{\\mathrm{MDP}}$, the function $g(\\alpha) = \\rho(\\alpha) - \\delta^2$ is defined. A root-finding algorithm is employed to locate the unique positive root of this function, yielding $\\alpha_{\\mathrm{MDP}}$.\n4.  The two parameter estimates are compared by computing the relative error $\\mathrm{rel\\_err} = |\\alpha_{\\mathrm{LC}} - \\alpha_{\\mathrm{MDP}}| / \\alpha_{\\mathrm{MDP}}$.\n5.  The resulting triplet $[\\alpha_{\\mathrm{LC}}, \\alpha_{\\mathrm{MDP}}, \\mathrm{rel\\_err}]$ constitutes the output for the test case.\nThe entire procedure is encapsulated in a self-contained program.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import brentq\n\ndef solve():\n    \"\"\"\n    Solves the regularization parameter selection problem for a series of test cases.\n    For each case, it calculates the L-curve corner alpha, the Morozov Discrepancy\n    Principle alpha, and their relative error.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"name\": \"Case 1\",\n            \"sigma\": np.array([1, 0.5, 0.25, 0.125, 0.0625, 0.03125]),\n            \"c\": np.ones(6),\n            \"f\": 0.05,\n            \"grid\": (1e-6, 1e1, 400)\n        },\n        {\n            \"name\": \"Case 2\",\n            \"sigma\": np.array([1, 0.5, 0.25, 0.125, 0.0625, 0.03125]),\n            \"c\": np.ones(6),\n            \"f\": 1e-6,\n            \"grid\": (1e-8, 1e0, 400)\n        },\n        {\n            \"name\": \"Case 3\",\n            \"sigma\": np.array([1, 0.5, 0.25, 0.125, 0.0625, 0.03125]),\n            \"c\": np.ones(6),\n            \"f\": 0.4,\n            \"grid\": (1e-6, 1e2, 600)\n        },\n        {\n            \"name\": \"Case 4\",\n            \"sigma\": np.array([1, 1e-2, 1e-4, 1e-6, 1e-8, 1e-10]),\n            \"c\": np.ones(6),\n            \"f\": 1e-3,\n            \"grid\": (1e-10, 1e2, 600)\n        }\n    ]\n\n    results = []\n    \n    d = np.array([1.0, -1.0, 1.0, -1.0, 1.0, -1.0])\n    d_normalized = d / np.linalg.norm(d)\n    \n    # Positive floor for log calculation to avoid undefined values.\n    LOG_FLOOR = 1e-300\n\n    for case in test_cases:\n        sigma = case[\"sigma\"]\n        c = case[\"c\"]\n        f = case[\"f\"]\n        alpha_min, alpha_max, n_alpha = case[\"grid\"]\n\n        # 1. Construct b_clean, delta, and b\n        x_true = c\n        b_clean = sigma * x_true\n        norm_b_clean = np.linalg.norm(b_clean)\n        delta = f * norm_b_clean\n        eta = delta * d_normalized\n        b = b_clean + eta\n\n        # 2. Compute L-curve and estimate the corner alpha_LC\n        alphas = np.logspace(np.log10(alpha_min), np.log10(alpha_max), n_alpha)\n        \n        # Vectorized calculation of norms\n        sigma_sq = sigma**2\n        b_sq = b**2\n        alphas_sq = alphas[:, np.newaxis]**2\n        \n        denominators = sigma_sq + alphas_sq\n        \n        # Residual norm squared rho(alpha)\n        rho_terms = (alphas_sq**2 / denominators**2) * b_sq\n        rho_sq_vals = np.sum(rho_terms, axis=1)\n\n        # Solution norm squared xi(alpha)\n        xi_terms = (sigma_sq / denominators**2) * b_sq\n        xi_sq_vals = np.sum(xi_terms, axis=1)\n\n        # L-curve coordinates in log-log scale\n        X = 0.5 * np.log(np.maximum(rho_sq_vals, LOG_FLOOR)) # X = log(residual norm)\n        Y = 0.5 * np.log(np.maximum(xi_sq_vals, LOG_FLOOR))  # Y = log(solution norm)\n        \n        # Curvature calculation using central differences\n        t = np.log(alphas)\n        delta_t = t[1] - t[0]\n\n        # Derivatives for interior points (from index 1 to n_alpha-2)\n        Xp = (X[2:] - X[:-2]) / (2 * delta_t)\n        Yp = (Y[2:] - Y[:-2]) / (2 * delta_t)\n        Xpp = (X[2:] - 2 * X[1:-1] + X[:-2]) / (delta_t**2)\n        Ypp = (Y[2:] - 2 * Y[1:-1] + Y[:-2]) / (delta_t**2)\n\n        numerator = Xp * Ypp - Yp * Xpp\n        denominator = (Xp**2 + Yp**2)**1.5\n\n        kappa = np.zeros_like(numerator)\n        # Avoid division by zero for points where derivatives might be nil.\n        valid_denom = denominator > LOG_FLOOR\n        kappa[valid_denom] = numerator[valid_denom] / denominator[valid_denom]\n\n        # Find alpha corresponding to max absolute curvature\n        max_kappa_idx = np.argmax(np.abs(kappa))\n        # The index must be shifted by 1 to map back to the original alpha grid\n        alpha_lc = alphas[max_kappa_idx + 1]\n\n        # 3. Compute MDP selection alpha_MDP\n        def rho_sq_func(alpha):\n            alpha_sq = alpha**2\n            den = sigma_sq + alpha_sq\n            terms = (alpha_sq**2 / den**2) * b_sq\n            return np.sum(terms)\n            \n        def g(alpha):\n            return rho_sq_func(alpha) - delta**2\n            \n        # Find a bracket for the root-finding algorithm\n        a, b_val = alpha_min, alpha_max\n        try:\n            val_a = g(a)\n            val_b = g(b_val)\n            # Exponentially expand bracket until a sign change is found\n            while val_a * val_b > 0:\n                if val_b  0:\n                    b_val *= 10\n                    val_b = g(b_val)\n                elif val_a > 0:\n                    a /= 10\n                    val_a = g(a)\n                else:\n                    # This case should not be reached with monotonic rho\n                    break\n        except OverflowError:\n            # Handle cases where alpha becomes too large or small\n            # For this problem, the initial grid is sufficient.\n            pass\n\n        alpha_mdp = brentq(g, a, b_val, xtol=1e-15, rtol=1e-15)\n\n        # 4. Compute relative error and store results\n        rel_err = np.abs(alpha_lc - alpha_mdp) / alpha_mdp\n        results.append([alpha_lc, alpha_mdp, rel_err])\n\n    # Final print statement in the exact required format.\n    # Produces for example: [[aL1,aM1,e1],[aL2,aM2,e2],...]\n    formatted_results = [f\"[{','.join(map(str, r))}]\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3361732"}, {"introduction": "A robust toolkit for inverse problems requires familiarity with multiple parameter-selection strategies, as no single method is universally optimal. This practice introduces the quasi-optimality rule, a valuable heuristic that selects $\\alpha$ by monitoring the stability of the solution as the parameter changes, notably without requiring an estimate of the noise level. You will implement this rule for general, non-diagonal systems and compare its selection against the Morozov Discrepancy Principle, enhancing your ability to tackle a broader class of inverse problems.", "problem": "Consider the linear inverse problem defined by a real matrix $A \\in \\mathbb{R}^{m \\times n}$ and noisy data $y^\\delta \\in \\mathbb{R}^m$. For a given grid of regularization parameters $\\{\\alpha_k\\}_{k=0}^{K-1}$, where the $\\alpha_k$ are strictly decreasing and positive, compute the Tikhonov-regularized solutions $x_{\\alpha_k}^\\delta \\in \\mathbb{R}^n$ obtained by minimizing the functional\n$$\nJ_\\alpha(x) \\equiv \\|A x - y^\\delta\\|_2^2 + \\alpha \\|x\\|_2^2.\n$$\nUse these to select a regularization parameter via the quasi-optimality rule, and, when a noise level $\\delta \\ge 0$ is provided, compare this selection with the Morozov Discrepancy Principle (MDP).\n\nThe quasi-optimality rule on a discrete grid selects the index $k^\\star \\in \\{1,\\dots,K-1\\}$ that minimizes the discrete quasi-optimality functional\n$$\n\\psi(\\alpha_k) \\equiv \\|x_{\\alpha_k}^\\delta - x_{\\alpha_{k-1}}^\\delta\\|_2,\n$$\nand returns $\\alpha_{k^\\star}$.\n\nThe Morozov Discrepancy Principle (MDP) with safety factor $\\tau  1$ targets a residual norm $\\|A x_{\\alpha}^\\delta - y^\\delta\\|_2 \\approx \\tau \\delta$. On a discrete grid, define the selected index $k_{\\mathrm{MDP}}$ as follows:\n- If a noise level $\\delta$ is provided (i.e., known), let $t \\equiv \\tau \\delta$. Choose the smallest $k \\in \\{0,\\dots,K-1\\}$ such that $\\|A x_{\\alpha_k}^\\delta - y^\\delta\\|_2 \\le t$. If no such $k$ exists, choose $k$ that minimizes $|\\|A x_{\\alpha_k}^\\delta - y^\\delta\\|_2 - t|$.\n- If no noise level is provided (i.e., unknown), do not select via MDP.\n\nIn all computations, use the Euclidean norm $\\|\\cdot\\|_2$. Use $\\tau = 1.05$.\n\nImplement the complete computation, starting from the definition of Tikhonov regularization, without assuming any shortcut formulas. Derive any needed expressions from first principles in your solution, and implement a numerically stable algorithm to evaluate $x_{\\alpha_k}^\\delta$ for each $\\alpha_k$ in the grid.\n\nTest Suite. Use the following four test cases. In each, the matrix $A$, a vector $x^\\dagger$ defining noise-free data $y \\equiv A x^\\dagger$, a noise vector $w$, the noisy data $y^\\delta \\equiv y + w$, the noise level $\\delta \\equiv \\|w\\|_2$ when provided, and the grid $\\{\\alpha_k\\}$ are specified. When the noise level is not provided, treat $\\delta$ as unknown. All matrices and vectors are real.\n\n- Case 1 (tall, moderately ill-conditioned, known noise):\n  - $A_1 \\in \\mathbb{R}^{8 \\times 5}$ is\n    $$\n    A_1 =\n    \\begin{bmatrix}\n    1  0  0  0  0 \\\\\n    0.9  0.1  0  0  0 \\\\\n    0.8  0.2  0.1  0  0 \\\\\n    0.7  0.3  0.1  0.1  0 \\\\\n    0.6  0.4  0.1  0.1  0.1 \\\\\n    0.5  0.5  0.1  0.1  0.1 \\\\\n    0.4  0.5  0.2  0.1  0.1 \\\\\n    0.3  0.5  0.2  0.2  0.1\n    \\end{bmatrix}.\n    $$\n  - $x_1^\\dagger = \\begin{bmatrix} 1 \\\\ -0.5 \\\\ 0.3 \\\\ 0 \\\\ 0.2 \\end{bmatrix}$.\n  - $w_1 = \\begin{bmatrix} -0.01 \\\\ 0.02 \\\\ -0.005 \\\\ 0 \\\\ 0.004 \\\\ -0.003 \\\\ 0.001 \\\\ -0.002 \\end{bmatrix}$.\n  - $y_1^\\delta = A_1 x_1^\\dagger + w_1$, and $\\delta_1 = \\|w_1\\|_2$ is provided.\n  - Grid $\\{\\alpha_k\\}_1 = \\{ 10^0, 10^{-1}, 10^{-2}, 10^{-3}, 10^{-4} \\}$.\n\n- Case 2 (square Hilbert matrix, known noise):\n  - $A_2 \\in \\mathbb{R}^{6 \\times 6}$ is the Hilbert matrix, $(A_2)_{ij} = \\frac{1}{i + j - 1}$ for $i,j \\in \\{1,\\dots,6\\}$.\n  - $x_2^\\dagger = \\begin{bmatrix} 1 \\\\ -1 \\\\ 1 \\\\ -1 \\\\ 1 \\\\ -1 \\end{bmatrix}$.\n  - $w_2 = \\begin{bmatrix} 0.001 \\\\ -0.002 \\\\ 0.0015 \\\\ -0.001 \\\\ 0.0005 \\\\ -0.0005 \\end{bmatrix}$.\n  - $y_2^\\delta = A_2 x_2^\\dagger + w_2$, and $\\delta_2 = \\|w_2\\|_2$ is provided.\n  - Grid $\\{\\alpha_k\\}_2 = \\{ 10^{-6}, 10^{-5}, 10^{-4}, 10^{-3}, 10^{-2}, 10^{-1} \\}$.\n\n- Case 3 (rectangular, unknown noise level):\n  - $A_3 \\in \\mathbb{R}^{7 \\times 4}$ is\n    $\n    A_3 =\n    \\begin{bmatrix}\n    1  0  0  0 \\\\\n    0.9  0.1  0  0 \\\\\n    0.8  0.2  0.1  0 \\\\\n    0.7  0.3  0.1  0.1 \\\\\n    0.6  0.4  0.1  0.1 \\\\\n    0.5  0.4  0.2  0.1 \\\\\n    0.4  0.5  0.2  0.1\n    \\end{bmatrix}.\n    $\n  - $x_3^\\dagger = \\begin{bmatrix} 0.5 \\\\ 0.2 \\\\ -0.1 \\\\ 0 \\end{bmatrix}$.\n  - $w_3 = \\begin{bmatrix} 0.01 \\\\ -0.005 \\\\ 0.003 \\\\ -0.002 \\\\ 0.001 \\\\ -0.001 \\\\ 0 \\end{bmatrix}$.\n  - $y_3^\\delta = A_3 x_3^\\dagger + w_3$, and the noise level is unknown (treat $\\delta_3$ as not provided).\n  - Grid $\\{\\alpha_k\\}_3 = \\{ 10^0, 10^{-1}, 10^{-2}, 10^{-3}, 10^{-4} \\}$.\n\n- Case 4 (diagonal, noiseless data as a boundary case for MDP fallback):\n  - $A_4 \\in \\mathbb{R}^{4 \\times 4}$ is diagonal,\n    $\n    A_4 = \\mathrm{diag}(1, 10^{-1}, 10^{-2}, 10^{-3}).\n    $\n  - $x_4^\\dagger = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{bmatrix}$.\n  - $w_4 = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix}$.\n  - $y_4^\\delta = A_4 x_4^\\dagger + w_4$, and $\\delta_4 = \\|w_4\\|_2 = 0$ is provided.\n  - Grid $\\{\\alpha_k\\}_4 = \\{ 10^0, 10^{-1}, 10^{-2}, 10^{-3}, 10^{-4}, 10^{-5} \\}$.\n\nProgram requirements:\n- For each case, compute $x_{\\alpha_k}^\\delta$ for all $\\alpha_k$ on the given grid; select $\\alpha$ via quasi-optimality; if $\\delta$ is provided, also select $\\alpha$ via Morozov Discrepancy Principle (MDP) with $\\tau = 1.05$ and the grid rule described above. If $\\delta$ is unknown, do not provide an MDP selection.\n- Output format: Your program should produce a single line of output containing a list of four results (one per case). Each result must itself be a list of four entries:\n  1. the quasi-optimality-selected $\\alpha$ (rounded to $8$ decimal places),\n  2. the quasi-optimality-selected index $k$ using zero-based indexing,\n  3. the MDP-selected $\\alpha$ (rounded to $8$ decimal places) or the sentinel $-1$ if $\\delta$ is unknown, and\n  4. the MDP-selected index $k$ using zero-based indexing, or the sentinel $-1$ if $\\delta$ is unknown.\n- The final print must be a single line, exactly the single list, with no extra spaces.", "solution": "The problem statement has been rigorously assessed and is determined to be **valid**. It is scientifically grounded in the established theory of inverse problems, well-posed, objective, and provides a complete and consistent set of specifications for computation. We may therefore proceed with a formal solution.\n\nThe core of the problem lies in finding a stable approximate solution to a linear system $Ax = y^\\delta$, where the data vector $y^\\delta$ is contaminated with noise. Tikhonov regularization is employed to achieve this by seeking a minimizer $x_\\alpha^\\delta$ for the functional:\n$$\nJ_\\alpha(x) \\equiv \\|Ax - y^\\delta\\|_2^2 + \\alpha \\|x\\|_2^2\n$$\nHere, $A \\in \\mathbb{R}^{m \\times n}$, $x \\in \\mathbb{R}^n$, $y^\\delta \\in \\mathbb{R}^m$, and $\\alpha  0$ is the regularization parameter.\n\n**1. Derivation of the Tikhonov Solution**\n\nTo find the minimizer of the convex functional $J_\\alpha(x)$, we compute its gradient with respect to $x$ and set it to the zero vector. First, we expand the functional using the definition of the Euclidean norm, $\\|v\\|_2^2 = v^T v$:\n$$\nJ_\\alpha(x) = (Ax - y^\\delta)^T(Ax - y^\\delta) + \\alpha x^T x\n$$\nExpanding the first term gives:\n$$\nJ_\\alpha(x) = (x^T A^T - (y^\\delta)^T)(Ax - y^\\delta) + \\alpha x^T x = x^T A^T Ax - x^T A^T y^\\delta - (y^\\delta)^T Ax + (y^\\delta)^T y^\\delta + \\alpha x^T x\n$$\nSince the term $(y^\\delta)^T Ax$ is a scalar, it is equal to its transpose, $(x^T A^T y^\\delta)^T = (y^\\delta)^T (A^T)^T (x^T)^T = (y^\\delta)^T A x$. Thus, we can combine the cross-terms:\n$$\nJ_\\alpha(x) = x^T (A^T A) x - 2 x^T A^T y^\\delta + \\|y^\\delta\\|_2^2 + \\alpha x^T x\n$$\nThe gradient of $J_\\alpha(x)$ with respect to the vector $x$ is:\n$$\n\\nabla_x J_\\alpha(x) = \\nabla_x (x^T (A^T A) x) - \\nabla_x (2 x^T A^T y^\\delta) + \\nabla_x (\\|y^\\delta\\|_2^2) + \\nabla_x (\\alpha x^T x)\n$$\nUsing the standard matrix calculus identities $\\nabla_x(x^T B x) = (B+B^T)x$ and $\\nabla_x(b^T x) = \\nabla_x(x^T b) = b$, and noting that $A^T A$ is symmetric, we obtain:\n$$\n\\nabla_x J_\\alpha(x) = 2 A^T Ax - 2 A^T y^\\delta + 0 + 2 \\alpha x\n$$\nSetting the gradient to zero to find the minimum yields the Tikhonov normal equations:\n$$\n2 A^T Ax - 2 A^T y^\\delta + 2 \\alpha x = 0\n$$\n$$\n(A^T A + \\alpha I) x = A^T y^\\delta\n$$\nwhere $I$ is the identity matrix of size $n \\times n$. For any $\\alpha  0$, the matrix $(A^T A + \\alpha I)$ is symmetric and positive definite, and therefore invertible. The unique Tikhonov-regularized solution $x_\\alpha^\\delta$ is given by:\n$$\nx_\\alpha^\\delta = (A^T A + \\alpha I)^{-1} A^T y^\\delta\n$$\n\n**2. Numerical Algorithm**\n\nFor each given regularization parameter $\\alpha_k$ from the discrete grid $\\{\\alpha_k\\}_{k=0}^{K-1}$, we must compute the corresponding solution $x_{\\alpha_k}^\\delta$. A direct computation involving the matrix inverse $(A^T A + \\alpha_k I)^{-1}$ is numerically unstable, particularly for ill-conditioned matrices $A$, and computationally inefficient. A superior method is to solve the linear system of equations directly.\n\nThe procedure for each test case is as follows:\n1.  Construct the matrix $A$ and vectors $x^\\dagger$ and $w$ as specified.\n2.  Compute the noisy data $y^\\delta = A x^\\dagger + w$.\n3.  For problems where the noise level is specified, compute $\\delta = \\|w\\|_2$.\n4.  Pre-compute the matrix $A^T A$ and the vector $A^T y^\\delta$, as these are constant for all $\\alpha_k$.\n5.  Iterate through the grid of regularization parameters $\\alpha_k$:\n    a. Form the matrix of the normal equations: $B_k = A^T A + \\alpha_k I$.\n    b. Solve the linear system $B_k x = A^T y^\\delta$ for $x_{\\alpha_k}^\\delta$. This is accomplished using a robust numerical solver, such as that provided by `numpy.linalg.solve`.\n    c. Store the sequence of solutions $\\{x_{\\alpha_k}^\\delta\\}_{k=0}^{K-1}$.\n\n**3. Parameter Selection Rules**\n\nOnce the set of candidate solutions $\\{x_{\\alpha_k}^\\delta\\}$ is computed, we apply two distinct a posteriori rules to select an appropriate regularization parameter.\n\n**a) Quasi-Optimality Rule**\nThis heuristic rule selects the parameter $\\alpha$ that marks a transition in the stability of the solution. The discrete quasi-optimality functional is defined as:\n$$\n\\psi(\\alpha_k) = \\|x_{\\alpha_k}^\\delta - x_{\\alpha_{k-1}}^\\delta\\|_2, \\quad \\text{for } k \\in \\{1, \\dots, K-1\\}\n$$\nThe rule selects the index $k^\\star$ that minimizes this functional. The rationale is that this value of $k$ corresponds to an $\\alpha_k$ just before the solution starts being significantly amplified by noise (which would cause a large change from $x_{\\alpha_{k-1}}^\\delta$ to $x_{\\alpha_k}^\\delta$). The algorithm is:\n1.  Compute $\\psi(\\alpha_k)$ for each $k$ from $1$ to $K-1$.\n2.  Find the index $k^\\star = \\arg\\min_{k \\in \\{1, \\dots, K-1\\}} \\psi(\\alpha_k)$.\n3.  The selected parameter is $\\alpha_{k^\\star}$ with index $k^\\star$.\n\n**b) Morozov Discrepancy Principle (MDP)**\nThis principle applies when the noise level $\\delta = \\|y^\\delta - Ax^\\dagger\\|_2$ is known. It aims to find a parameter $\\alpha$ such that the residual norm of the regularized solution matches the expected noise level, scaled by a safety factor $\\tau  1$. The target is to satisfy:\n$$\n\\|Ax_\\alpha^\\delta - y^\\delta\\|_2 = \\tau \\delta\n$$\nFor a discrete grid $\\{\\alpha_k\\}$ and a given safety factor $\\tau=1.05$, the implementation is as follows:\n1.  If the noise level $\\delta$ is not provided, the MDP is not applicable.\n2.  If $\\delta$ is provided, calculate the target discrepancy $t = \\tau \\delta$.\n3.  For each solution $x_{\\alpha_k}^\\delta$, compute the residual norm $r_k = \\|A x_{\\alpha_k}^\\delta - y^\\delta\\|_2$.\n4.  Search for the smallest index $k \\in \\{0, \\dots, K-1\\}$ such that the condition $r_k \\le t$ is met. This corresponds to the largest $\\alpha_k$ that sufficiently fits the data.\n5.  If no such index exists (i.e., $r_k  t$ for all $k$), the fallback rule is to choose the index $k$ that minimizes the absolute difference $|r_k - t|$.\n6.  The selected parameter is $\\alpha_{k_{\\mathrm{MDP}}}$ with index $k_{\\mathrm{MDP}}$.\n\nThese procedures are applied to each of the four test cases specified in the problem statement to produce the required results.", "answer": "```python\nimport numpy as np\n\ndef solve_tikhonov(A, y_delta, alpha):\n    \"\"\"\n    Solves the Tikhonov regularization problem for a given alpha.\n    (A^T A + alpha * I) x = A^T y\n    \"\"\"\n    m, n = A.shape\n    AtA = A.T @ A\n    Aty = A.T @ y_delta\n    \n    # Form the matrix for the normal equations\n    B = AtA + alpha * np.identity(n)\n    \n    # Solve the linear system\n    x_alpha = np.linalg.solve(B, Aty)\n    return x_alpha\n\ndef select_quasi_optimality(x_solutions, alphas):\n    \"\"\"\n    Selects alpha using the quasi-optimality rule on a discrete grid.\n    \"\"\"\n    psi_values = []\n    # QO is defined for k = 1, ..., K-1\n    for k in range(1, len(alphas)):\n        psi = np.linalg.norm(x_solutions[k] - x_solutions[k-1])\n        psi_values.append(psi)\n    \n    # Find the index k in {1, ..., K-1} that minimizes psi.\n    # The index relative to psi_values is k-1.\n    min_idx_psi = np.argmin(psi_values)\n    best_k = min_idx_psi + 1\n    \n    return alphas[best_k], best_k\n\ndef select_morozov_discrepancy(A, y_delta, x_solutions, alphas, delta, delta_known, tau):\n    \"\"\"\n    Selects alpha using the Morozov Discrepancy Principle.\n    \"\"\"\n    if not delta_known:\n        return -1, -1\n\n    target = tau * delta\n    \n    residuals = np.array([np.linalg.norm(A @ x - y_delta) for x in x_solutions])\n    \n    # Find smallest k such that residual[k] = target\n    eligible_indices = np.where(residuals = target)[0]\n    \n    if len(eligible_indices)  0:\n        best_k = np.min(eligible_indices)\n    else:\n        # Fallback: minimize |residual[k] - target|\n        best_k = np.argmin(np.abs(residuals - target))\n        \n    return alphas[best_k], int(best_k)\n\ndef process_case(case_data):\n    \"\"\"\n    Processes a single test case.\n    \"\"\"\n    A = case_data[\"A\"]\n    x_dagger = case_data[\"x_dagger\"]\n    w = case_data[\"w\"]\n    alphas = case_data[\"alphas\"]\n    delta_known = case_data[\"delta_known\"]\n    tau = 1.05\n\n    y_delta = A @ x_dagger + w\n    delta = np.linalg.norm(w)\n    \n    # 1. Compute all Tikhonov solutions\n    x_solutions = [solve_tikhonov(A, y_delta, alpha) for alpha in alphas]\n    \n    # 2. Apply Quasi-Optimality rule\n    qo_alpha, qo_k = select_quasi_optimality(x_solutions, alphas)\n    \n    # 3. Apply Morozov Discrepancy Principle\n    mdp_alpha, mdp_k = select_morozov_discrepancy(A, y_delta, x_solutions, alphas, delta, delta_known, tau)\n\n    # Format output\n    if mdp_alpha == -1:\n        mdp_alpha_str = \"-1\"\n    else:\n        mdp_alpha_str = f\"{mdp_alpha:.8f}\"\n\n    return f\"[{qo_alpha:.8f},{qo_k},{mdp_alpha_str},{mdp_k}]\"\n    \ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    test_cases = [\n        {\n            \"A\": np.array([\n                [1.0, 0.0, 0.0, 0.0, 0.0],\n                [0.9, 0.1, 0.0, 0.0, 0.0],\n                [0.8, 0.2, 0.1, 0.0, 0.0],\n                [0.7, 0.3, 0.1, 0.1, 0.0],\n                [0.6, 0.4, 0.1, 0.1, 0.1],\n                [0.5, 0.5, 0.1, 0.1, 0.1],\n                [0.4, 0.5, 0.2, 0.1, 0.1],\n                [0.3, 0.5, 0.2, 0.2, 0.1]\n            ]),\n            \"x_dagger\": np.array([1.0, -0.5, 0.3, 0.0, 0.2]),\n            \"w\": np.array([-0.01, 0.02, -0.005, 0.0, 0.004, -0.003, 0.001, -0.002]),\n            \"alphas\": np.array([10.0**0, 10.0**-1, 10.0**-2, 10.0**-3, 10.0**-4]),\n            \"delta_known\": True\n        },\n        {\n            \"A\": np.array([[1.0 / (i + j + 1) for j in range(6)] for i in range(6)]),\n            \"x_dagger\": np.array([1.0, -1.0, 1.0, -1.0, 1.0, -1.0]),\n            \"w\": np.array([0.001, -0.002, 0.0015, -0.001, 0.0005, -0.0005]),\n            \"alphas\": np.array([10.0**-6, 10.0**-5, 10.0**-4, 10.0**-3, 10.0**-2, 10.0**-1]),\n            \"delta_known\": True\n        },\n        {\n            \"A\": np.array([\n                [1.0, 0.0, 0.0, 0.0],\n                [0.9, 0.1, 0.0, 0.0],\n                [0.8, 0.2, 0.1, 0.0],\n                [0.7, 0.3, 0.1, 0.1],\n                [0.6, 0.4, 0.1, 0.1],\n                [0.5, 0.4, 0.2, 0.1],\n                [0.4, 0.5, 0.2, 0.1]\n            ]),\n            \"x_dagger\": np.array([0.5, 0.2, -0.1, 0.0]),\n            \"w\": np.array([0.01, -0.005, 0.003, -0.002, 0.001, -0.001, 0.0]),\n            \"alphas\": np.array([10.0**0, 10.0**-1, 10.0**-2, 10.0**-3, 10.0**-4]),\n            \"delta_known\": False\n        },\n        {\n            \"A\": np.diag([1.0, 1e-1, 1e-2, 1e-3]),\n            \"x_dagger\": np.array([1.0, 1.0, 1.0, 1.0]),\n            \"w\": np.array([0.0, 0.0, 0.0, 0.0]),\n            \"alphas\": np.array([10.0**0, 10.0**-1, 10.0**-2, 10.0**-3, 10.0**-4, 10.0**-5]),\n            \"delta_known\": True\n        }\n    ]\n\n    results = [process_case(case) for case in test_cases]\n    \n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3361745"}]}