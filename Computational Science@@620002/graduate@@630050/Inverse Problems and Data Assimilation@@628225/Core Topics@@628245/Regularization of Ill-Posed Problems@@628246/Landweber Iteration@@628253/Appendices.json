{"hands_on_practices": [{"introduction": "Before diving into complex, high-dimensional inverse problems, it is instructive to grasp the mechanics of the Landweber iteration in its simplest form. This exercise applies the iteration to a basic scalar equation, stripping away the complexities of matrix algebra to reveal the core concept: updating an estimate in the direction that reduces the error. By performing a single step by hand, you will build a concrete intuition for how the residual drives the solution towards the correct value. [@problem_id:539080]", "problem": "Consider the Landweber iteration, an iterative regularization method used to solve linear equations of the form $A\\mathbf{x} = \\mathbf{b}$. For a scalar equation $a x = b$ with $a, b \\in \\mathbb{R}$, the iteration scheme is defined as:\n$$\nx_{k+1} = x_k + \\tau a (b - a x_k),\n$$\nwhere $\\tau$ is the step size and $x_k$ denotes the $k$-th iterate.  \n\nGiven the scalar equation $2x = 3$, compute the first iterate $x_1$ using the Landweber iteration with step size $\\tau = \\frac{1}{4}$ and initial guess $x_0 = 0$. Provide the exact value of $x_1$.", "solution": "1. The Landweber iteration for the scalar problem $a x = b$ is\n$$\nx_{k+1} \\;=\\; x_k \\;+\\; \\tau\\,a\\,(b - a\\,x_k).\n$$\n2. For $a=2$, $b=3$, $\\tau=\\tfrac14$, and $x_0=0$, we have\n$$\nb - a\\,x_0 = 3 - 2\\cdot 0 = 3.\n$$\n3. Hence\n$$\nx_1 = x_0 + \\tau\\,a\\,(b - a\\,x_0)\n    = 0 + \\frac14\\cdot 2\\cdot 3\n    = \\frac{6}{4}\n    = \\frac{3}{2}.\n$$", "answer": "$$\\boxed{3/2}$$", "id": "539080"}, {"introduction": "The Landweber iteration's convergence is not guaranteed; it hinges critically on the choice of the step size, often denoted by $\\omega$. This exercise provides a hands-on demonstration of what happens when the theoretical stability bound, $0 \\lt \\omega \\lt \\frac{2}{\\lVert A \\rVert^2}$, is violated. By analyzing a simple $2 \\times 2$ system, you will see precisely how an oversized step can amplify errors rather than reduce them, leading to divergence, and gain a concrete understanding of the method's convergence properties. [@problem_id:3372411]", "problem": "Consider a linear inverse problem in a finite-dimensional Euclidean space where the data model is $y = A x^{\\ast}$ with $A \\in \\mathbb{R}^{2 \\times 2}$ and the true state $x^{\\ast} \\in \\mathbb{R}^{2}$. The Landweber iteration for estimating $x^{\\ast}$ is defined by $x_{k+1} = x_{k} + \\omega A^{\\top} \\big( y - A x_{k} \\big)$, where $\\omega  0$ is a fixed stepsize. Use the standard Euclidean norm for vectors and the induced operator norm for matrices, so that $\\|A\\|$ equals the largest singular value of $A$.\n\nConstruct an explicit example with \n- $A = \\sigma u v^{\\top}$ for some $\\sigma  0$, where $u, v \\in \\mathbb{R}^{2}$ are unit vectors, \n- choose $u = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$ and $v = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$ so that $A = \\begin{pmatrix} \\sigma  0 \\\\ 0  0 \\end{pmatrix}$ has a single positive singular value equal to $\\|A\\| = \\sigma$ and the other singular value is zero,\n- set the data to be $y = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ so that the minimum-norm solution is $x^{\\dagger} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$,\n- initialize the iteration at $x_{0} = v = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$,\n- and choose the stepsize $\\omega = \\dfrac{3}{\\|A\\|^{2}}$.\n\nStarting from the fundamental definition of the Landweber iteration and the properties of the singular value decomposition (SVD), derive the one-step error recurrence along the nonzero singular direction and compute the magnitude of the corresponding scalar amplification factor. Your final answer must be the single real number equal to that magnitude. No rounding is required.", "solution": "The problem statement is validated as scientifically grounded, well-posed, objective, and self-consistent. All necessary information is provided to determine a unique solution.\n\nThe linear inverse problem is given by the data model $y = A x^{\\ast}$, where $y \\in \\mathbb{R}^{2}$ is the observed data, $A \\in \\mathbb{R}^{2 \\times 2}$ is the forward operator, and $x^{\\ast} \\in \\mathbb{R}^{2}$ is the true state to be estimated. The Landweber iteration is used to find an estimate $x_k$ of $x^{\\ast}$ and is defined by the recurrence relation:\n$$\nx_{k+1} = x_{k} + \\omega A^{\\top} \\big( y - A x_{k} \\big)\n$$\nwhere $k$ is the iteration index, $x_0$ is an initial guess, $\\omega  0$ is a fixed stepsize (relaxation parameter), and $A^{\\top}$ is the transpose of $A$.\n\nThe analysis of the iteration's convergence is performed by studying the evolution of the error vector, defined as $e_k = x_k - x^{\\dagger}$, where $x^{\\dagger}$ is the true solution being sought. In this problem, it is specified that $y = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$. The set of all solutions to $Ax=y$ is the null space of $A$, denoted $\\text{ker}(A)$. The minimum-norm solution $x^{\\dagger}$ is the element in $\\text{ker}(A)$ with the smallest Euclidean norm. With $A = \\begin{pmatrix} \\sigma  0 \\\\ 0  0 \\end{pmatrix}$ and $\\sigma0$, the equation $Ax=y$ becomes $\\begin{pmatrix} \\sigma  0 \\\\ 0  0 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$, which implies $\\sigma x_1 = 0$, so $x_1=0$. The solution set is $\\{ (0,c) \\mid c \\in \\mathbb{R} \\}$. The minimum-norm solution occurs when $c=0$, thus $x^{\\dagger} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$. Consequently, the error vector is simply $e_k = x_k$.\n\nWe can now derive the error recurrence relation. Since $y = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ and $x^{\\dagger} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$, we have $y = A x^{\\dagger}$. We substitute this into the Landweber iteration formula:\n$$\nx_{k+1} = x_k + \\omega A^{\\top} (A x^{\\dagger} - A x_k)\n$$\nSubtracting $x^{\\dagger}$ from both sides gives:\n$$\nx_{k+1} - x^{\\dagger} = x_k - x^{\\dagger} - \\omega A^{\\top} A (x_k - x^{\\dagger})\n$$\nThis yields the error propagation equation:\n$$\ne_{k+1} = e_k - \\omega A^{\\top} A e_k = \\left(I - \\omega A^{\\top} A\\right) e_k\n$$\nwhere $I$ is the $2 \\times 2$ identity matrix. The matrix $G = I - \\omega A^{\\top} A$ is the error propagation operator.\n\nThe problem's behavior is best analyzed in the coordinate system defined by the singular value decomposition (SVD) of $A$. Let the SVD of $A$ be $A = U \\Sigma V^{\\top}$, where $U$ and $V$ are orthogonal matrices whose columns are the left and right singular vectors, respectively, and $\\Sigma$ is a diagonal matrix of singular values $\\sigma_j$. The right singular vectors $\\{v_j\\}$ form an orthonormal basis for the domain $\\mathbb{R}^2$. They are the eigenvectors of $A^{\\top} A$, since $A^{\\top} A = (V \\Sigma^{\\top} U^{\\top}) (U \\Sigma V^{\\top}) = V (\\Sigma^{\\top} \\Sigma) V^{\\top}$. The matrix $\\Sigma^{\\top} \\Sigma$ is a diagonal matrix with entries $\\sigma_j^2$. Therefore, $A^{\\top} A v_j = \\sigma_j^2 v_j$.\n\nThe error propagation operator $G$ acts on the right singular vectors $v_j$ as follows:\n$$\nG v_j = (I - \\omega A^{\\top} A) v_j = I v_j - \\omega (A^{\\top} A v_j) = v_j - \\omega \\sigma_j^2 v_j = (1 - \\omega \\sigma_j^2) v_j\n$$\nThis shows that the right singular vectors $v_j$ of $A$ are also the eigenvectors of the error propagation operator $G$. The corresponding eigenvalues are $\\lambda_j = 1 - \\omega \\sigma_j^2$. If the error at step $k$ has a component along $v_j$, say $\\alpha_{k,j} v_j$, then at step $k+1$ this component becomes $\\alpha_{k+1,j} v_j = \\lambda_j (\\alpha_{k,j} v_j)$. The value $\\lambda_j$ is therefore the scalar amplification factor for the error component in the direction of $v_j$.\n\nThe problem provides specific values. The matrix is $A = \\begin{pmatrix} \\sigma  0 \\\\ 0  0 \\end{pmatrix}$ with $\\sigma  0$. The singular values are $\\sigma_1 = \\sigma$ and $\\sigma_2 = 0$. The corresponding right singular vectors can be chosen as $v_1 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$ and $v_2 = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$. The operator norm is $\\|A\\| = \\max(\\sigma_1, \\sigma_2) = \\sigma$.\n\nThe \"nonzero singular direction\" corresponds to the singular value $\\sigma_1 = \\sigma$. The problem asks for the scalar amplification factor along this direction, which is $\\lambda_1 = 1 - \\omega \\sigma_1^2 = 1 - \\omega \\sigma^2$.\n\nThe stepsize is given as $\\omega = \\dfrac{3}{\\|A\\|^{2}}$. Since $\\|A\\| = \\sigma$, we have $\\omega = \\dfrac{3}{\\sigma^2}$.\n\nSubstituting this value of $\\omega$ into the expression for the amplification factor:\n$$\n\\lambda_1 = 1 - \\left(\\frac{3}{\\sigma^2}\\right) \\sigma^2 = 1 - 3 = -2\n$$\nThe scalar amplification factor for the error component along the nonzero singular direction is $-2$.\n\nThe problem asks for the magnitude of this scalar amplification factor.\n$$\n| \\lambda_1 | = | -2 | = 2\n$$\nThis result indicates that the error component in the direction of the first singular vector is amplified by a factor of $2$ at each iteration, causing the iteration to diverge. This is expected, as the provided stepsize $\\omega = \\frac{3}{\\|A\\|^2}$ is outside the standard convergence interval for Landweber iteration, which is $0  \\omega  \\frac{2}{\\|A\\|^2}$.", "answer": "$$\\boxed{2}$$", "id": "3372411"}, {"introduction": "A cornerstone of regularization theory is predicting how fast an iterative method converges, which depends on both the operator's properties and the solution's smoothness. This computational exercise bridges the gap between abstract theory and practical performance by tasking you with empirically verifying the convergence rate of the Landweber iteration. You will construct a problem with specific spectral properties and source conditions, run the iteration, and use data analysis to estimate the convergence exponent, comparing your findings directly with the theoretical prediction. [@problem_id:3395632]", "problem": "Consider a linear inverse problem in a finite-dimensional Euclidean space with a compact, self-adjoint, positive semidefinite operator represented by a matrix with a prescribed singular value decay. Let the forward model be $y = A x^\\dagger$, where $A \\in \\mathbb{R}^{n \\times n}$ is diagonalizable with singular values $\\sigma_i$ that follow a power-law decay $\\sigma_i \\asymp i^{-p}$ for some $p  0$, and the true solution $x^\\dagger$ has coefficients in the singular value decomposition (SVD) basis that satisfy $c_i \\asymp i^{-(\\mu + 1/2)}$ for some $\\mu  0$. Consider the Landweber iteration, initialized at $x^{(0)} = 0$, with a constant relaxation parameter $\\omega \\in (0, 2 / \\|A\\|^2)$, defined by the recursion $x^{(k+1)} = x^{(k)} + \\omega A^\\top \\left( y - A x^{(k)} \\right)$ for integers $k \\ge 0$. You will empirically verify the predicted algebraic convergence rate of the Landweber iteration error in terms of the iteration count $k$ under the prescribed spectral decay and source condition.\n\nYour program must:\n- Construct, for each test case, a diagonal matrix $A \\in \\mathbb{R}^{n \\times n}$ with diagonal entries $\\sigma_i = i^{-p}$ for $i \\in \\{1, 2, \\dots, n\\}$, with $n$ specified per the test suite.\n- Construct $x^\\dagger \\in \\mathbb{R}^n$ with entries $c_i = i^{-(\\mu + 1/2)}$ in the canonical basis that coincides with the SVD right singular vectors.\n- Form exact data $y = A x^\\dagger$.\n- Implement the Landweber iteration with $x^{(0)} = 0$ and $\\omega = 0.9 / \\|A\\|^2$. Note that since $A$ is diagonal with largest singular value $\\sigma_1 = 1$, one has $\\|A\\|^2 = 1$ and thus $\\omega = 0.9$.\n- Compute the error norm $\\|x^{(k)} - x^\\dagger\\|_2$ for a prescribed set of iteration counts $k$ in each test case.\n- Estimate the empirical convergence exponent $\\alpha_{\\mathrm{est}}$ by performing a least-squares linear regression of $\\log \\left( \\|x^{(k)} - x^\\dagger\\|_2 \\right)$ versus $\\log(k)$ over the provided $k$ values. Interpret the model as $\\|x^{(k)} - x^\\dagger\\|_2 \\approx C k^{-\\alpha}$, so that the slope of the regression line equals $-\\alpha_{\\mathrm{est}}$.\n- For each test case, compare the empirically estimated exponent $\\alpha_{\\mathrm{est}}$ with the theoretical prediction $\\alpha_{\\mathrm{th}} = \\mu / (2 p)$ that arises from combining the power-law spectral decay and the source condition under Landweber iteration.\n\nUse the following test suite, which is designed to probe a range of ill-posedness and smoothness regimes while avoiding degenerate parameter choices:\n- Test case $1$ (general case): $n = 16384$, $p = 1.5$, $\\mu = 1.0$, $\\omega = 0.9$, and iteration counts $k \\in \\{50, 100, 200, 400, 800, 1600\\}$.\n- Test case $2$ (mildly ill-posed, higher smoothness): $n = 16384$, $p = 0.5$, $\\mu = 1.5$, $\\omega = 0.9$, and iteration counts $k \\in \\{50, 100, 200, 400, 800, 1600\\}$.\n- Test case $3$ (severely ill-posed, higher smoothness): $n = 16384$, $p = 2.0$, $\\mu = 2.0$, $\\omega = 0.9$, and iteration counts $k \\in \\{50, 100, 200, 400, 800, 1600\\}$.\n\nFor numerical stability and efficiency, you may exploit the diagonal structure of $A$ and the closed-form expression of Landweber iterates in the SVD basis, but you must not use any formula in the code or the analysis that presupposes the target convergence rate; the verification must result from the implemented computation.\n\nYour program must output, for each of the above three test cases, a single floating-point number equal to the absolute difference $|\\alpha_{\\mathrm{est}} - \\alpha_{\\mathrm{th}}|$, rounded to three decimal places. Aggregate these three numbers into a single line as a comma-separated list enclosed in square brackets, with no spaces. For example, if the three absolute differences are $d_1$, $d_2$, and $d_3$, the output format must be $[d_1,d_2,d_3]$.\n\nThere are no physical units involved in this problem. All angles, if any appear, must be in radians, but no angles are used here.\n\nYour final program must produce exactly one line of output in the specified format. It must not read any input or write any files.", "solution": "The problem requires an empirical verification of the theoretical convergence rate of the Landweber iteration for a specific class of linear inverse problems. The verification is to be performed by simulating the iteration, estimating the convergence rate from the numerical results, and comparing it to the theoretical prediction.\n\nThe problem is defined in a finite-dimensional space $\\mathbb{R}^n$. The forward model is a linear equation $y = A x^\\dagger$, where $y \\in \\mathbb{R}^n$ represents the observed data, $x^\\dagger \\in \\mathbb{R}^n$ is the true solution to be recovered, and $A \\in \\mathbb{R}^{n \\times n}$ is the forward operator. The problem is constructed such that the underlying inverse problem is ill-posed, meaning that small perturbations in $y$ can lead to large errors in the solution.\n\nThe operator $A$ is specified as a diagonal matrix with diagonal entries $\\sigma_i$ representing its singular values. These are defined by a power-law decay:\n$$\n\\sigma_i = i^{-p} \\quad \\text{for } i = 1, 2, \\dots, n\n$$\nwhere $p  0$ controls the degree of ill-posedness. Since $A$ is a real diagonal matrix, it is self-adjoint ($A = A^\\top$). Its eigenvalues are positive, so it is a positive definite operator.\n\nThe true solution $x^\\dagger$ has a specific smoothness, characterized by the decay of its coefficients $c_i$ in the canonical basis (which, for a diagonal $A$, is the basis of singular vectors):\n$$\nc_i = i^{-(\\mu + 1/2)} \\quad \\text{for } i = 1, 2, \\dots, n\n$$\nwhere $\\mu  0$ is a smoothness parameter. Larger values of $\\mu$ correspond to \"smoother\" solutions. The data $y$ are exact, computed as $y = A x^\\dagger$. Since both $A$ and $x^\\dagger$ are defined component-wise in the same basis, this is simply $y_i = \\sigma_i c_i$.\n\nThe Landweber iteration is an iterative regularization method for solving such inverse problems. It is defined by the recurrence relation:\n$$\nx^{(k+1)} = x^{(k)} + \\omega A^\\top \\left( y - A x^{(k)} \\right)\n$$\nwith an initial guess $x^{(0)} = 0$. The parameter $\\omega$ is a relaxation or step-size parameter, which must satisfy $\\omega \\in (0, 2/\\|A\\|^2)$ to ensure convergence. For the given operator $A$, the operator norm (largest singular value) is $\\|A\\| = \\sigma_1 = 1^{-p} = 1$. The problem specifies $\\omega = 0.9 / \\|A\\|^2 = 0.9$.\n\nFor a diagonal operator $A$, the iteration decouples into $n$ independent scalar recursions, one for each component $x_i^{(k)}$:\n$$\nx_i^{(k+1)} = x_i^{(k)} + \\omega \\sigma_i (y_i - \\sigma_i x_i^{(k)}) = (1 - \\omega \\sigma_i^2) x_i^{(k)} + \\omega \\sigma_i y_i\n$$\nWith $y_i = \\sigma_i c_i$ and the initial condition $x_i^{(0)} = 0$, this linear recurrence has the closed-form solution:\n$$\nx_i^{(k)} = c_i \\left( 1 - (1 - \\omega \\sigma_i^2)^k \\right)\n$$\nThis expression allows for the direct computation of the iterate $x^{(k)}$ at any step $k$ without performing the full iteration. The error vector is $x^{(k)} - x^\\dagger$, and its $i$-th component is $x_i^{(k)} - c_i = -c_i (1 - \\omega \\sigma_i^2)^k$. The squared L2-norm of the error is therefore:\n$$\n\\|x^{(k)} - x^\\dagger\\|_2^2 = \\sum_{i=1}^{n} (x_i^{(k)} - c_i)^2 = \\sum_{i=1}^{n} c_i^2 \\left( (1 - \\omega \\sigma_i^2)^k \\right)^2 = \\sum_{i=1}^{n} c_i^2 (1 - \\omega \\sigma_i^2)^{2k}\n$$\nThe program computes this error norm for each specified iteration count $k$.\n\nTo estimate the convergence rate, we assume the error behaves according to an algebraic decay law: $\\|x^{(k)} - x^\\dagger\\|_2 \\approx C k^{-\\alpha}$ for some constant $C$ and exponent $\\alpha  0$. Taking the natural logarithm of both sides linearizes this relationship:\n$$\n\\log\\left(\\|x^{(k)} - x^\\dagger\\|_2\\right) \\approx \\log(C) - \\alpha \\log(k)\n$$\nThis equation shows a linear relationship between $Y = \\log\\left(\\|x^{(k)} - x^\\dagger\\|_2\\right)$ and $X = \\log(k)$, with a slope of $-\\alpha$. The empirical exponent $\\alpha_{\\mathrm{est}}$ is determined by performing a least-squares linear regression on the set of points $\\{(\\log(k_j), \\log(\\|x^{(k_j)} - x^\\dagger\\|_2))\\}$ for the given iteration counts $k_j$. The estimated exponent is the negative of the slope of the fitted line.\n\nFor Landweber iteration applied to problems with spectral decay $\\sigma_i \\asymp i^{-p}$ and a solution smoothness (source condition) of $x^\\dagger_i \\asymp i^{-(\\mu+1/2)}$, regularization theory predicts a convergence rate of the error norm. The predicted exponent is:\n$$\n\\alpha_{\\mathrm{th}} = \\frac{\\mu}{2p}\n$$\nThe final step for each test case is to compute the absolute difference $|\\alpha_{\\mathrm{est}} - \\alpha_{\\mathrm{th}}|$ to quantify the agreement between the empirical measurement and the theoretical prediction.\n\nThe program implements this entire procedure. For each test case, it constructs the vectors corresponding to $\\sigma_i$ and $c_i$, calculates the error norms at the specified values of $k$ using the closed-form expression, performs a log-log linear regression to find $\\alpha_{\\mathrm{est}}$, calculates $\\alpha_{\\mathrm{th}}$, and outputs their absolute difference, rounded to three decimal places.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and verify Landweber iteration convergence rates.\n    \"\"\"\n\n    # Test suite as defined in the problem statement.\n    # Each tuple is (n, p, mu, omega, k_values)\n    test_cases = [\n        (16384, 1.5, 1.0, 0.9, np.array([50, 100, 200, 400, 800, 1600])),\n        (16384, 0.5, 1.5, 0.9, np.array([50, 100, 200, 400, 800, 1600])),\n        (16384, 2.0, 2.0, 0.9, np.array([50, 100, 200, 400, 800, 1600])),\n    ]\n\n    results = []\n    for n, p, mu, omega, k_values in test_cases:\n        # Construct the problem in the SVD basis (canonical basis here).\n        # The operator A is diagonal with entries sigma_i.\n        # The true solution x_dagger has entries c_i.\n        i = np.arange(1, n + 1, dtype=np.float64)\n        sigma = i**(-p)\n        c = i**(-(mu + 0.5))\n\n        # Compute the error norm ||x^(k) - x_dagger||_2 for each k.\n        # This is done using the closed-form expression for the error in the SVD basis.\n        # Error_i(k) = -c_i * (1 - omega * sigma_i^2)^k\n        # ||Error(k)||_2 = sqrt(sum_i (Error_i(k))^2)\n        error_norms = []\n        for k in k_values:\n            # The term (1 - omega * sigma^2) is raised to the power of k.\n            # This represents the decay of the error components.\n            # err_vec_k = c_i * (1 - omega * sigma_i^2)^k\n            err_vec_k = c * np.power(1.0 - omega * sigma**2, k)\n            \n            # The L2 norm is calculated using numpy.linalg.norm.\n            norm_k = np.linalg.norm(err_vec_k)\n            error_norms.append(norm_k)\n\n        # Estimate the empirical convergence exponent alpha_est.\n        # This is done by fitting a line to log(error) vs log(k).\n        # The model is: log(error) approx log(C) - alpha * log(k).\n        # The slope of the line is -alpha.\n        log_k = np.log(k_values)\n        log_error = np.log(np.array(error_norms))\n\n        # np.polyfit(x, y, 1) returns [slope, intercept] for the best-fit line.\n        slope, _ = np.polyfit(log_k, log_error, 1)\n        alpha_est = -slope\n        \n        # Calculate the theoretical convergence exponent.\n        alpha_th = mu / (2.0 * p)\n\n        # Calculate the absolute difference and round to three decimal places.\n        difference = abs(alpha_est - alpha_th)\n        results.append(round(difference, 3))\n\n    # Format the final output as a comma-separated list in brackets.\n    output_string = \",\".join(map(str, results))\n    print(f\"[{output_string}]\")\n\nsolve()\n```", "id": "3395632"}]}