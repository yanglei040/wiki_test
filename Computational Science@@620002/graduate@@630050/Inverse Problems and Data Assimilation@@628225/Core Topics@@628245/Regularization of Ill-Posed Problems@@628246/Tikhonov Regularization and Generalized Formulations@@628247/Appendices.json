{"hands_on_practices": [{"introduction": "Mastering generalized Tikhonov regularization begins with understanding its core mechanics. This exercise provides a foundational, hands-on calculation where you will directly minimize the Tikhonov functional for a small-scale linear system. By incorporating a discrete derivative operator, this problem [@problem_id:1031979] highlights the fundamental trade-off between fitting the data and controlling the smoothness of the solution.", "problem": "Consider the overdetermined linear system $A \\mathbf{u} = \\mathbf{b}$, where $A$ is the $3 \\times 2$ matrix  \n$$  \nA = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{bmatrix},  \n$$  \nand $\\mathbf{b}$ is the vector $\\mathbf{b} = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}$. The generalized Tikhonov regularization functional incorporates a derivative operator $L$ and is defined as  \n$$  \nJ(\\mathbf{u}) = \\| A \\mathbf{u} - \\mathbf{b} \\|^2 + \\lambda \\| L \\mathbf{u} \\|^2,  \n$$  \nwhere $\\mathbf{u} = \\begin{bmatrix} u_1 \\\\ u_2 \\end{bmatrix} \\in \\mathbb{R}^2$, $\\lambda > 0$ is a regularization parameter, and $L$ is the first-order derivative operator given by the $1 \\times 2$ matrix $L = \\begin{bmatrix} -1 & 1 \\end{bmatrix}$. Compute the minimum value of $J(\\mathbf{u})$ over all $\\mathbf{u} \\in \\mathbb{R}^2$.", "solution": "1.  The minimizer of the Tikhonov functional $J(\\mathbf{u}) = \\| A \\mathbf{u} - \\mathbf{b} \\|^2 + \\lambda \\| L \\mathbf{u} \\|^2$ satisfies the normal equations $(A^\\top A + \\lambda L^\\top L) \\mathbf{u} = A^\\top \\mathbf{b}$.\n\n2.  First, we compute the required matrices:\n$$ A^\\top A = \\begin{bmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\end{bmatrix} \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{bmatrix} = \\begin{bmatrix} 2 & 1 \\\\ 1 & 2 \\end{bmatrix} $$\n$$ L^\\top L = \\begin{bmatrix} -1 \\\\ 1 \\end{bmatrix} \\begin{bmatrix} -1 & 1 \\end{bmatrix} = \\begin{bmatrix} 1 & -1 \\\\ -1 & 1 \\end{bmatrix} $$\n$$ A^\\top \\mathbf{b} = \\begin{bmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} $$\n3.  The normal equations become:\n$$ \\left( \\begin{bmatrix} 2 & 1 \\\\ 1 & 2 \\end{bmatrix} + \\lambda \\begin{bmatrix} 1 & -1 \\\\ -1 & 1 \\end{bmatrix} \\right) \\begin{bmatrix} u_1 \\\\ u_2 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} $$\n$$ \\begin{bmatrix} 2+\\lambda & 1-\\lambda \\\\ 1-\\lambda & 2+\\lambda \\end{bmatrix} \\begin{bmatrix} u_1 \\\\ u_2 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} $$\n4.  Solving this $2 \\times 2$ system yields the optimal solution $\\mathbf{u}$. From the second row, $(1-\\lambda)u_1 + (2+\\lambda)u_2 = 0 \\implies u_2 = -\\frac{1-\\lambda}{2+\\lambda}u_1$. Substituting into the first row gives $(2+\\lambda)u_1 - \\frac{(1-\\lambda)^2}{2+\\lambda}u_1 = 1$, which simplifies to $u_1 \\frac{(2+\\lambda)^2 - (1-\\lambda)^2}{2+\\lambda} = 1 \\implies u_1 \\frac{3+6\\lambda}{2+\\lambda} = 1$. The components of the solution vector are:\n$$ u_1 = \\frac{2+\\lambda}{3(1+2\\lambda)}, \\quad u_2 = -\\frac{1-\\lambda}{3(1+2\\lambda)} $$\n5.  To find the minimum value $J_{\\min}$, we substitute $\\mathbf{u}$ back into the original functional $J(\\mathbf{u}) = (u_1-1)^2 + u_2^2 + (u_1+u_2)^2 + \\lambda(u_2-u_1)^2$. After algebraic simplification, this evaluates to:\n$$ J_{\\min} = \\frac{1+7\\lambda+10\\lambda^2}{3(1+2\\lambda)^2} = \\frac{(1+5\\lambda)(1+2\\lambda)}{3(1+2\\lambda)^2} = \\frac{5\\lambda+1}{3(2\\lambda+1)} $$\nHence the minimum of $J$ is $\\frac{5\\lambda+1}{3(2\\lambda+1)}$.", "answer": "$$\\boxed{\\frac{5\\lambda+1}{3(2\\lambda+1)}}$$", "id": "1031979"}, {"introduction": "Moving from abstract theory to practical application, many inverse problems require discretizing a continuous model. This practice [@problem_id:3427382] guides you through implementing a Tikhonov-regularized inversion for a deconvolution problem, a common task in signal processing and imaging. More importantly, it demonstrates the \"inverse crime,\" a critical methodological pitfall where using identical discretizations for data generation and inversion yields unrealistically optimistic results, and shows how to design a more robust test.", "problem": "Consider the linear inverse problem of recovering an unknown function $x$ on the unit interval $[0,1]$ from blurred observations modeled by the Fredholm integral of the first kind\n$$\ny(s) \\;=\\; \\int_{0}^{1} k(s,t)\\,x(t)\\,dt, \\quad s \\in [0,1],\n$$\nwith a Gaussian blur kernel\n$$\nk(s,t) \\;=\\; \\frac{1}{\\sqrt{2\\pi}\\,\\sigma}\\,\\exp\\!\\Big(-\\frac{(s-t)^2}{2\\sigma^2}\\Big),\n$$\nwhere $\\sigma > 0$ is the blur width. The discrete Tikhonov regularization problem seeks an $x \\in \\mathbb{R}^N$ that minimizes\n$$\nJ(x) \\;=\\; \\|A x - y\\|_2^2 \\;+\\; \\alpha^2 \\|L x\\|_2^2,\n$$\nwhere $A \\in \\mathbb{R}^{N \\times N}$ discretizes the forward operator, $L \\in \\mathbb{R}^{(N-1) \\times N}$ discretizes a first-derivative penalty, $\\alpha > 0$ is the regularization parameter, and $y \\in \\mathbb{R}^N$ is the observed data.\n\nYour task is to build a complete “inverse crime” test and a staggered-discretization diagnostic to compare reconstructions under two data-generation regimes:\n\n- Inverse crime (grid-matching): synthetic data are generated by applying the same discrete operator used in the inversion to a discretized ground truth on the same grid. This practice can exaggerate apparent performance.\n- Staggered discretization (non-matching): synthetic data are generated by a finer and shifted quadrature that does not align with the inversion grid, thereby revealing modeling error and diagnosing true regularization effects.\n\nStart from the following fundamental base:\n\n- The definition of Tikhonov regularization as the minimizer of the convex functional $J(x)$.\n- First-order optimality (Euler–Lagrange conditions) for a strictly convex quadratic leading to the normal equations\n$$\n(A^\\top A + \\alpha^2 L^\\top L)\\,x^\\star \\;=\\; A^\\top y.\n$$\n- Standard rectangle quadrature on a uniform grid for discretizing the integral, and forward finite differences for discretizing the first derivative.\n\nDiscretization details to be implemented:\n\n- Inversion grid: choose $N$ points at midpoints $s_i = (i+\\tfrac{1}{2})/N$ for $i=0,\\dots,N-1$. Represent $x$ by its samples at the same midpoints. The inversion matrix $A \\in \\mathbb{R}^{N\\times N}$ uses rectangle quadrature with weight $w = 1/N$,\n$$\nA_{ij} \\;=\\; k\\!\\big(s_i, s_j\\big)\\,w \\quad \\text{for} \\quad 0 \\le i,j \\le N-1.\n$$\n- Regularization operator $L \\in \\mathbb{R}^{(N-1)\\times N}$ is the forward finite-difference approximation of the first derivative on the same inversion grid with spacing $h = 1/N$,\n$$\n(Lx)_j \\;=\\; \\frac{x_{j+1}-x_j}{h}, \\quad j = 0,\\dots,N-2.\n$$\n- Ground truth: use the smooth function\n$$\nx_{\\text{true}}(t) \\;=\\; \\exp\\!\\big(-50\\,(t-0.3)^2\\big) \\;+\\; 0.5\\,\\sin(6\\pi t) \\;+\\; 0.2,\n$$\nwith angles in radians. For comparison, the reference vector on the inversion grid is $x_{\\text{true,inv}} = \\big(x_{\\text{true}}(s_i)\\big)_{i=0}^{N-1}$.\n- Data generation:\n  - Inverse crime: $y_{\\text{clean}} = A\\,x_{\\text{true,inv}}$.\n  - Staggered discretization: for each inversion grid point $s_i$, approximate the integral using a finer, shifted quadrature with $N_f$ points $t_\\ell = (\\ell + \\delta)/N_f$ for $\\ell=0,\\dots,N_f-1$, weight $w_f = 1/N_f$, and a fixed shift $\\delta = 0.125$,\n  $$\n  y_{\\text{clean},i} \\;=\\; \\sum_{\\ell=0}^{N_f-1} k\\!\\big(s_i, t_\\ell\\big)\\,x_{\\text{true}}(t_\\ell)\\,w_f.\n  $$\n  Unless otherwise stated, take $N_f = 4N$ for staggered cases.\n- Noise model: additive independent Gaussian noise $n \\sim \\mathcal{N}(0,\\sigma_n^2 I)$ with per-component standard deviation\n$$\n\\sigma_n \\;=\\; \\eta\\,\\frac{\\|y_{\\text{clean}}\\|_2}{\\sqrt{N}},\n$$\nwhere $\\eta$ is the prescribed relative noise level and $I$ is the identity matrix.\n- Reconstruction: for given $(A,L,\\alpha,y)$, compute $x^\\star$ from the normal equations and report the relative reconstruction error\n$$\n\\varepsilon \\;=\\; \\frac{\\|x^\\star - x_{\\text{true,inv}}\\|_2}{\\|x_{\\text{true,inv}}\\|_2}.\n$$\n\nTest suite and coverage:\n\nImplement the following five test cases, each specified by $(N, N_f, \\sigma, \\alpha, \\eta, \\text{mode})$, where $\\text{mode} \\in \\{\\text{crime},\\text{staggered}\\}$ indicates the data-generation regime:\n\n- Case $1$ (happy path, inverse crime): $(N, N_f, \\sigma, \\alpha, \\eta, \\text{mode}) = (80, 80, 0.03, 0.01, 0.001, \\text{crime})$.\n- Case $2$ (happy path, staggered): $(N, N_f, \\sigma, \\alpha, \\eta, \\text{mode}) = (80, 320, 0.03, 0.01, 0.001, \\text{staggered})$.\n- Case $3$ (stronger blur and noise, staggered): $(N, N_f, \\sigma, \\alpha, \\eta, \\text{mode}) = (80, 320, 0.05, 0.05, 0.01, \\text{staggered})$.\n- Case $4$ (coarser inversion grid, staggered): $(N, N_f, \\sigma, \\alpha, \\eta, \\text{mode}) = (40, 160, 0.03, 0.01, 0.005, \\text{staggered})$.\n- Case $5$ (under-regularized, inverse crime): $(N, N_f, \\sigma, \\alpha, \\eta, \\text{mode}) = (80, 80, 0.03, 0.00001, 0.001, \\text{crime})$.\n\nRandomness:\n\n- Use a fixed seed for reproducibility: initialize a pseudorandom number generator with seed $0$ for all noise draws.\n\nRequired program output:\n\n- For each case, compute the relative error $\\varepsilon$ as defined above.\n- Your program should produce a single line of output containing the five results as a comma-separated list enclosed in square brackets, rounded to $6$ decimal places (e.g., $[0.123456,0.234567,0.345678,0.456789,0.567890]$).\n- No physical units are involved, and all trigonometric angles must be in radians.\n\nYour implementation must be a complete, runnable program that constructs $A$, $L$, generates $y$ according to the regime, solves the Tikhonov normal equations, and reports the errors for all test cases in the specified format.", "solution": "The user-provided problem is a well-defined numerical exercise in the field of inverse problems, specifically focusing on Tikhonov regularization. It is scientifically sound, mathematically consistent, and all parameters and procedures are explicitly defined. The task is to implement a numerical scheme to solve a Fredholm integral equation of the first kind, test it under different data-generation scenarios (\"inverse crime\" vs. \"staggered discretization\"), and quantify the reconstruction error. The problem is valid and a solution can be constructed.\n\nThe solution proceeds by discretizing the continuous model, constructing the linear-algebraic operators, generating synthetic data according to the specified regime, solving the Tikhonov-regularized normal equations, and finally, computing the reconstruction error.\n\n$1$. **Discretization of Grids and Ground Truth**:\nThe problem is discretized on a uniform grid of $N$ points. The inversion grid points $s_i$ are midpoints of subintervals of $[0,1]$:\n$$s_i = \\frac{i + 0.5}{N}, \\quad i = 0, 1, \\dots, N-1.$$\nThe ground truth function is given by:\n$$x_{\\text{true}}(t) = \\exp\\big(-50(t - 0.3)^2\\big) + 0.5 \\sin(6\\pi t) + 0.2.$$\nThe reference solution on the inversion grid, $x_{\\text{true,inv}} \\in \\mathbb{R}^N$, is obtained by sampling $x_{\\text{true}}(t)$ at the grid points $s_i$:\n$$(x_{\\text{true,inv}})_i = x_{\\text{true}}(s_i).$$\n\n$2$. **Discretization of Operators**:\nThe integral operator is discretized using the midpoint rectangle rule on the inversion grid. The weight for each subinterval of length $h = \\frac{1}{N}$ is $w=\\frac{1}{N}$. The matrix $A \\in \\mathbb{R}^{N \\times N}$ has entries:\n$$A_{ij} = k(s_i, s_j) w = \\frac{1}{N} k(s_i, s_j),$$\nwhere the kernel is $k(s,t) = \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\Big(-\\frac{(s-t)^2}{2\\sigma^2}\\Big)$.\n\nThe first-derivative regularization operator, $L$, is discretized using a forward finite-difference scheme. For a vector $x \\in \\mathbb{R}^N$, the $j$-th component of $Lx$ is an approximation of the derivative at point $s_j$:\n$$(Lx)_j = \\frac{x_{j+1} - x_j}{h} = N(x_{j+1} - x_j), \\quad j = 0, 1, \\dots, N-2.$$\nThis defines the matrix $L \\in \\mathbb{R}^{(N-1) \\times N}$. For a given row $j$, this matrix has an entry of $-N$ at column $j$ and an entry of $N$ at column $j+1$.\n\n$3$. **Data Generation ($y$)**:\nThe observed data $y \\in \\mathbb{R}^N$ is generated by first creating \"clean\" data, $y_{\\text{clean}}$, and then adding noise.\n\n-   **Inverse Crime Regime**: The data is generated using the same discretization as the inversion.\n    $$y_{\\text{clean}} = A x_{\\text{true,inv}}.$$\n    This is termed an \"inverse crime\" because the model used for inversion perfectly matches the data generation process, which is rarely true in practice and can lead to unrealistically good results.\n\n-   **Staggered Discretization Regime**: The data is generated using a finer, shifted grid to simulate model mismatch. A fine grid of $N_f$ points, $t_\\ell = \\frac{\\ell + \\delta}{N_f}$ for $\\ell=0,\\dots,N_f-1$ with shift $\\delta=0.125$, is used. The clean data is an approximation of the integral:\n    $$ (y_{\\text{clean}})_i = \\sum_{\\ell=0}^{N_f-1} k(s_i, t_\\ell) x_{\\text{true}}(t_\\ell) w_f, $$\n    where $w_f = \\frac{1}{N_f}$. This more realistic approach helps diagnose the true performance of the regularization method.\n\n-   **Noise Model**: Additive Gaussian noise $n \\sim \\mathcal{N}(0, \\sigma_n^2 I)$ is added to $y_{\\text{clean}}$. The standard deviation of the noise, $\\sigma_n$, is determined by the relative noise level $\\eta$:\n    $$\\sigma_n = \\eta \\frac{\\|y_{\\text{clean}}\\|_2}{\\sqrt{N}}.$$\n    The final observed data is $y = y_{\\text{clean}} + n$. A fixed random seed of $0$ is used for reproducibility.\n\n$4$. **Solving for the Reconstruction**:\nThe Tikhonov-regularized solution $x^\\star$ minimizes $J(x) = \\|Ax - y\\|_2^2 + \\alpha^2 \\|Lx\\|_2^2$. The minimizer is found by solving the normal equations, which form a well-posed linear system:\n$$(A^\\top A + \\alpha^2 L^\\top L) x^\\star = A^\\top y.$$\nWe construct the matrix $M = A^\\top A + \\alpha^2 L^\\top L$ and the vector $b = A^\\top y$, and solve the system $Mx^\\star = b$ for $x^\\star$ using a standard linear solver.\n\n$5$. **Error Calculation**:\nThe quality of the reconstruction $x^\\star$ is measured by the relative error $\\varepsilon$ with respect to the true solution sampled on the inversion grid, $x_{\\text{true,inv}}$:\n    $$\\varepsilon = \\frac{\\|x^\\star - x_{\\text{true,inv}}\\|_2}{\\|x_{\\text{true,inv}}\\|_2}.$$\n\nThe implementation will proceed by executing these steps for each of the five test cases provided. A Python script utilizing the `numpy` library is well-suited for these matrix and vector operations. For each case, we will configure the parameters $(N, N_f, \\sigma, \\alpha, \\eta, \\text{mode})$, construct the necessary components, solve for $x^\\star$, and compute $\\varepsilon$. The final results will be collected and formatted as specified.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the Tikhonov regularization problem for a series of test cases\n    and prints the relative reconstruction errors.\n    \"\"\"\n    \n    # Use a fixed seed for reproducibility for all noise draws.\n    RNG = np.random.default_rng(0)\n    \n    # Shift for staggered grid, as specified in the problem.\n    DELTA = 0.125\n\n    def x_true_func(t):\n        \"\"\"Computes the ground truth function x_true(t).\"\"\"\n        return np.exp(-50 * (t - 0.3)**2) + 0.5 * np.sin(6 * np.pi * t) + 0.2\n\n    def kernel_func(s, t, sigma):\n        \"\"\"Computes the Gaussian blur kernel k(s,t).\"\"\"\n        return (1.0 / (np.sqrt(2 * np.pi) * sigma)) * np.exp(-(s - t)**2 / (2 * sigma**2))\n\n    def compute_reconstruction_error(params):\n        \"\"\"Computes the reconstruction error for a single test case.\"\"\"\n        N, Nf, sigma, alpha, eta, mode = params\n\n        # 1. Discretization of Grids and Ground Truth\n        h = 1.0 / N\n        s_grid = (np.arange(N) + 0.5) * h\n        x_true_inv = x_true_func(s_grid)\n\n        # 2. Discretization of Operators\n        # Forward operator A for inversion\n        s_i_mesh, s_j_mesh = np.meshgrid(s_grid, s_grid, indexing='ij')\n        A = kernel_func(s_i_mesh, s_j_mesh, sigma) * h\n\n        # Regularization operator L\n        L = np.zeros((N - 1, N))\n        idx = np.arange(N - 1)\n        L[idx, idx] = -1.0\n        L[idx, idx + 1] = 1.0\n        L /= h\n\n        # 3. Data Generation (y)\n        if mode == 'crime':\n            y_clean = A @ x_true_inv\n        elif mode == 'staggered':\n            h_f = 1.0 / Nf\n            t_grid = (np.arange(Nf) + DELTA) * h_f\n            x_true_fine = x_true_func(t_grid)\n            \n            s_mesh, t_mesh = np.meshgrid(s_grid, t_grid, indexing='ij')\n            K_staggered = kernel_func(s_mesh, t_mesh, sigma)\n            \n            y_clean = K_staggered @ x_true_fine * h_f\n        else:\n            raise ValueError(f\"Invalid mode: {mode}\")\n\n        # Add noise\n        norm_y_clean = np.linalg.norm(y_clean)\n        if norm_y_clean == 0:\n            sigma_n = 0.0\n        else:\n            sigma_n = eta * norm_y_clean / np.sqrt(N)\n        \n        noise = RNG.normal(loc=0.0, scale=sigma_n, size=N)\n        y = y_clean + noise\n\n        # 4. Solving for the Reconstruction\n        M = A.T @ A + alpha**2 * (L.T @ L)\n        b = A.T @ y\n        x_star = np.linalg.solve(M, b)\n\n        # 5. Error Calculation\n        error_num = np.linalg.norm(x_star - x_true_inv)\n        error_den = np.linalg.norm(x_true_inv)\n        epsilon = error_num / error_den\n        \n        return epsilon\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (N, Nf, sigma, alpha, eta, mode)\n        (80, 80, 0.03, 0.01, 0.001, 'crime'),\n        (80, 320, 0.03, 0.01, 0.001, 'staggered'),\n        (80, 320, 0.05, 0.05, 0.01, 'staggered'),\n        (40, 160, 0.03, 0.01, 0.005, 'staggered'),\n        (80, 80, 0.03, 1e-5, 0.001, 'crime'),\n    ]\n\n    results = []\n    for case in test_cases:\n        result = compute_reconstruction_error(case)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join([f'{r:.6f}' for r in results])}]\")\n\nsolve()\n```", "id": "3427382"}, {"introduction": "The performance of Tikhonov regularization critically depends on the choice of the regularization parameter $\\lambda$. This advanced practice [@problem_id:3427389] tackles this challenge head-on by introducing an automated, data-driven method for hyperparameter tuning. You will implement a bilevel optimization scheme, where the optimal $\\lambda$ is found by minimizing an error metric on a held-out validation set, a powerful technique that lies at the heart of modern machine learning and computational science.", "problem": "You are tasked with implementing a bilevel optimization procedure for selecting the hyperparameter $\\lambda$ in generalized Tikhonov regularization by minimizing a validation risk on held-out data. The inner problem is a generalized Tikhonov estimator defined by minimizing the objective $\\|A x - y\\|^2 + \\lambda^2 \\|L x\\|^2$ with respect to $x$, where $A \\in \\mathbb{R}^{m \\times n}$ is the training design matrix, $y \\in \\mathbb{R}^m$ is the training observation vector, and $L \\in \\mathbb{R}^{p \\times n}$ is a linear regularization operator. The outer problem selects $\\lambda \\ge 0$ to minimize the validation risk $R_{\\mathrm{val}}(x_\\lambda) = \\|A_{\\mathrm{val}} x_\\lambda - y_{\\mathrm{val}}\\|^2$, where $A_{\\mathrm{val}} \\in \\mathbb{R}^{m_{\\mathrm{val}} \\times n}$ and $y_{\\mathrm{val}} \\in \\mathbb{R}^{m_{\\mathrm{val}}}$ represent held-out validation data, and $x_\\lambda$ denotes the unique minimizer of the inner problem for a given $\\lambda$. Your implementation must be based on first principles and must derive and use the gradient $\\partial x_\\lambda / \\partial \\lambda$ to perform hyperparameter tuning on $\\lambda$ via projected gradient descent onto $\\mathbb{R}_{\\ge 0}$.\n\nStarting point: Use the fundamental least-squares normal equations that characterize the minimizer of a convex quadratic objective. For the inner problem, the minimizer $x_\\lambda$ satisfies the first-order optimality (normal) equations $(A^\\top A + \\lambda^2 L^\\top L) x_\\lambda = A^\\top y$, assuming $A^\\top A + \\lambda^2 L^\\top L$ is invertible. The outer problem objective is the validation risk $R_{\\mathrm{val}}(x_\\lambda) = \\|A_{\\mathrm{val}} x_\\lambda - y_{\\mathrm{val}}\\|^2$. From these bases, derive $\\partial x_\\lambda / \\partial \\lambda$ and implement the gradient of the validation risk with respect to $\\lambda$, $\\mathrm{d}R_{\\mathrm{val}}/\\mathrm{d}\\lambda$, without using pre-derived shortcut formulas.\n\nYour program should:\n- Construct synthetic training and validation datasets $(A, y)$ and $(A_{\\mathrm{val}}, y_{\\mathrm{val}})$ for each test case using fixed random seeds and a known ground-truth vector $x^\\star$.\n- Implement the inner solver for $x_\\lambda$ using the normal equations.\n- Derive and implement the sensitivity $\\partial x_\\lambda / \\partial \\lambda$ via implicit differentiation of the normal equations.\n- Compute the gradient of the validation risk and run projected gradient descent on $\\lambda$ (projection onto $\\mathbb{R}_{\\ge 0}$) for a fixed number of iterations with a specified step size.\n- For the gradient verification test, compare the analytic gradient against a central finite-difference approximation and report the relative discrepancy.\n\nUnits and representation:\n- There are no physical units in this problem.\n- All angles, if any, are to be interpreted in radians; however, no trigonometric angles of measurement are required in the final outputs.\n\nTest Suite Specification:\nImplement the following four test cases. In all cases, generate $A$ and $A_{\\mathrm{val}}$ with independent entries drawn from the standard normal distribution $\\mathcal{N}(0,1)$ using the specified random seed, and let the ground-truth vector $x^\\star \\in \\mathbb{R}^n$ be defined component-wise by $x^\\star_i = \\sin\\left(\\frac{\\pi i}{n}\\right) + \\frac{1}{2} \\cos\\left(\\frac{2 \\pi i}{n}\\right)$ for $i = 1, \\dots, n$. For a given noise standard deviation $\\sigma$, generate $y = A x^\\star + \\varepsilon_{\\mathrm{train}}$ and $y_{\\mathrm{val}} = A_{\\mathrm{val}} x^\\star + \\varepsilon_{\\mathrm{val}}$ with independent $\\varepsilon_{\\mathrm{train}} \\sim \\mathcal{N}(0, \\sigma^2 I)$ and $\\varepsilon_{\\mathrm{val}} \\sim \\mathcal{N}(0, \\sigma^2 I)$.\n\n- Test Case $1$ (Happy path; generalized $L$ as first-difference):\n  - Dimensions: $m_{\\mathrm{train}} = 40$, $m_{\\mathrm{val}} = 45$, $n = 20$.\n  - Random seed: $1$.\n  - Regularization operator $L \\in \\mathbb{R}^{(n-1) \\times n}$ is the first-difference operator: $L_{i,i} = -1$, $L_{i,i+1} = 1$ for $i = 1, \\dots, n-1$, with all other entries $0$.\n  - Noise standard deviation: $\\sigma = 10^{-2}$.\n  - Initialization and optimization: $\\lambda_0 = 10^{-1}$, step size $\\alpha = 10^{-2}$, number of iterations $200$.\n\n  Output the final tuned $\\lambda$ as a float.\n\n- Test Case $2$ (Boundary condition; $\\lambda$ initialized at zero with $L = I$):\n  - Dimensions: $m_{\\mathrm{train}} = 50$, $m_{\\mathrm{val}} = 55$, $n = 25$.\n  - Random seed: $3$.\n  - Regularization operator $L \\in \\mathbb{R}^{n \\times n}$ is the identity matrix.\n  - Noise standard deviation: $\\sigma = 5 \\cdot 10^{-3}$.\n  - Initialization and optimization: $\\lambda_0 = 0$, step size $\\alpha = 10^{-2}$, number of iterations $50$.\n\n  Output the final tuned $\\lambda$ as a float.\n\n- Test Case $3$ (Edge case; singular $L$ as second-difference):\n  - Dimensions: $m_{\\mathrm{train}} = 60$, $m_{\\mathrm{val}} = 65$, $n = 30$.\n  - Random seed: $4$.\n  - Regularization operator $L \\in \\mathbb{R}^{(n-2) \\times n}$ is the second-difference operator: $L_{i,i} = 1$, $L_{i,i+1} = -2$, $L_{i,i+2} = 1$ for $i = 1, \\dots, n-2$, with all other entries $0$.\n  - Noise standard deviation: $\\sigma = 10^{-2}$.\n  - Initialization and optimization: $\\lambda_0 = 5 \\cdot 10^{-2}$, step size $\\alpha = 2 \\cdot 10^{-2}$, number of iterations $300$.\n\n  Output the final tuned $\\lambda$ as a float.\n\n- Test Case $4$ (Gradient verification via finite differences):\n  - Dimensions: $m_{\\mathrm{train}} = 6$, $m_{\\mathrm{val}} = 7$, $n = 5$.\n  - Random seed: $2$.\n  - Regularization operator $L \\in \\mathbb{R}^{n \\times n}$ is the identity matrix.\n  - Noise standard deviation: $\\sigma = 10^{-3}$.\n  - Evaluate at a fixed $\\lambda = 0.15$.\n  - Compute the analytic gradient $\\mathrm{d}R_{\\mathrm{val}}/\\mathrm{d}\\lambda$ at $\\lambda$ and compare against the central finite-difference approximation with step $h = 10^{-6}$:\n    $$\\frac{R_{\\mathrm{val}}(\\lambda + h) - R_{\\mathrm{val}}(\\lambda - h)}{2 h}.$$\n  - Output the relative discrepancy defined as\n    $$\\frac{\\left|\\left(\\frac{R_{\\mathrm{val}}(\\lambda + h) - R_{\\mathrm{val}}(\\lambda - h)}{2 h}\\right) - \\frac{\\mathrm{d}R_{\\mathrm{val}}}{\\mathrm{d}\\lambda}\\right|}{\\max\\left(10^{-12}, \\left|\\frac{R_{\\mathrm{val}}(\\lambda + h) - R_{\\mathrm{val}}(\\lambda - h)}{2 h}\\right|\\right)}$$\n    as a float.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of Test Cases $1$ through $4$. For example, the format must be like $[r_1,r_2,r_3,r_4]$, where $r_1$, $r_2$, and $r_3$ are floats for the tuned $\\lambda$ values of Test Cases $1$–$3$, and $r_4$ is the float for the relative discrepancy in Test Case $4$.", "solution": "The problem statement is assessed to be valid. It presents a well-posed and scientifically sound task in the field of numerical optimization and inverse problems. All necessary parameters, definitions, and conditions are provided, and the problem is free of contradictions, ambiguities, or factual errors. The task is to implement a bilevel optimization scheme for hyperparameter selection in generalized Tikhonov regularization, which is a standard and important problem.\n\nThe solution proceeds in two stages. First, we derive the analytical gradient of the validation risk with respect to the regularization hyperparameter $\\lambda$. Second, we implement this gradient within a projected gradient descent algorithm to solve the specified test cases.\n\n**1. Derivation of the Gradient for Bilevel Optimization**\n\nThe problem is structured as a bilevel optimization problem. The inner problem finds the regularized solution $x_\\lambda$ for a given hyperparameter $\\lambda$, and the outer problem optimizes $\\lambda$ by minimizing a validation metric.\n\n**Inner Problem:** The generalized Tikhonov regularized solution, $x_\\lambda$, is the unique minimizer of the objective function:\n$$ J(x; \\lambda) = \\|A x - y\\|^2_2 + \\lambda^2 \\|L x\\|^2_2 $$\nwhere $A \\in \\mathbb{R}^{m \\times n}$ is the training design matrix, $y \\in \\mathbb{R}^m$ is the training data, $L \\in \\mathbb{R}^{p \\times n}$ is the regularization operator, and $\\lambda \\ge 0$ is the regularization hyperparameter. The objective $J(x; \\lambda)$ is a convex quadratic function of $x$. Its minimizer is found by setting its gradient with respect to $x$ to zero:\n$$ \\nabla_x J(x; \\lambda) = 2 A^\\top (A x - y) + 2 \\lambda^2 L^\\top L x = 0 $$\nRearranging these terms gives the normal equations for the regularized least-squares problem:\n$$ (A^\\top A + \\lambda^2 L^\\top L) x_\\lambda = A^\\top y $$\nWe denote the system matrix as $M_\\lambda = A^\\top A + \\lambda^2 L^\\top L$ and the right-hand side as $b = A^\\top y$. The solution $x_\\lambda$ is thus given by $x_\\lambda = M_\\lambda^{-1} b$, assuming $M_\\lambda$ is invertible. This is guaranteed if $\\lambda > 0$ and the null spaces of $A$ and $L$ intersect only at the origin, i.e., $\\ker(A) \\cap \\ker(L) = \\{0\\}$, a standard condition. For $\\lambda = 0$, invertibility requires $A^\\top A$ to be invertible, which is true with probability $1$ for the randomly generated matrices in this problem where $m \\ge n$.\n\n**Outer Problem:** The outer problem is to find the optimal $\\lambda$ that minimizes the validation risk, defined as the squared error on a held-out validation dataset $(A_{\\mathrm{val}}, y_{\\mathrm{val}})$:\n$$ R_{\\mathrm{val}}(\\lambda) = \\|A_{\\mathrm{val}} x_\\lambda - y_{\\mathrm{val}}\\|^2_2 $$\nTo optimize $R_{\\mathrm{val}}(\\lambda)$ using gradient descent, we must compute its derivative with respect to $\\lambda$, $\\frac{\\mathrm{d}R_{\\mathrm{val}}}{\\mathrm{d}\\lambda}$.\n\n**Gradient Derivation:** We start by applying the chain rule to $R_{\\mathrm{val}}(\\lambda)$:\n$$ R_{\\mathrm{val}}(\\lambda) = (A_{\\mathrm{val}} x_\\lambda - y_{\\mathrm{val}})^\\top (A_{\\mathrm{val}} x_\\lambda - y_{\\mathrm{val}}) $$\n$$ \\frac{\\mathrm{d}R_{\\mathrm{val}}}{\\mathrm{d}\\lambda} = 2 (A_{\\mathrm{val}} x_\\lambda - y_{\\mathrm{val}})^\\top \\frac{\\mathrm{d}}{\\mathrm{d}\\lambda}(A_{\\mathrm{val}} x_\\lambda - y_{\\mathrm{val}}) = 2 (A_{\\mathrm{val}} x_\\lambda - y_{\\mathrm{val}})^\\top A_{\\mathrm{val}} \\frac{\\mathrm{d}x_\\lambda}{\\mathrm{d}\\lambda} $$\nThe core of the derivation is to find the sensitivity vector $\\frac{\\mathrm{d}x_\\lambda}{\\mathrm{d}\\lambda}$. This is achieved by implicitly differentiating the normal equation $M_\\lambda x_\\lambda = b$ with respect to $\\lambda$:\n$$ \\frac{\\mathrm{d}}{\\mathrm{d}\\lambda} (M_\\lambda x_\\lambda) = \\frac{\\mathrm{d}b}{\\mathrm{d}\\lambda} $$\nThe right-hand side is zero, as $b=A^\\top y$ is independent of $\\lambda$. Applying the product rule to the left-hand side yields:\n$$ \\left( \\frac{\\mathrm{d}M_\\lambda}{\\mathrm{d}\\lambda} \\right) x_\\lambda + M_\\lambda \\left( \\frac{\\mathrm{d}x_\\lambda}{\\mathrm{d}\\lambda} \\right) = 0 $$\nThe derivative of the system matrix $M_\\lambda$ is:\n$$ \\frac{\\mathrm{d}M_\\lambda}{\\mathrm{d}\\lambda} = \\frac{\\mathrm{d}}{\\mathrm{d}\\lambda} (A^\\top A + \\lambda^2 L^\\top L) = 2\\lambda L^\\top L $$\nSubstituting this back, we get:\n$$ (2\\lambda L^\\top L) x_\\lambda + M_\\lambda \\left( \\frac{\\mathrm{d}x_\\lambda}{\\mathrm{d}\\lambda} \\right) = 0 $$\nSolving for the sensitivity $\\frac{\\mathrm{d}x_\\lambda}{\\mathrm{d}\\lambda}$:\n$$ \\frac{\\mathrm{d}x_\\lambda}{\\mathrm{d}\\lambda} = -M_\\lambda^{-1} (2\\lambda L^\\top L x_\\lambda) = -2\\lambda (A^\\top A + \\lambda^2 L^\\top L)^{-1} (L^\\top L) x_\\lambda $$\nFinally, we substitute this expression for $\\frac{\\mathrm{d}x_\\lambda}{\\mathrm{d}\\lambda}$ into the equation for $\\frac{\\mathrm{d}R_{\\mathrm{val}}}{\\mathrm{d}\\lambda}$:\n$$ \\frac{\\mathrm{d}R_{\\mathrm{val}}}{\\mathrm{d}\\lambda} = 2 (A_{\\mathrm{val}} x_\\lambda - y_{\\mathrm{val}})^\\top A_{\\mathrm{val}} \\left( -2\\lambda (A^\\top A + \\lambda^2 L^\\top L)^{-1} (L^\\top L) x_\\lambda \\right) $$\n$$ \\frac{\\mathrm{d}R_{\\mathrm{val}}}{\\mathrm{d}\\lambda} = -4\\lambda (A_{\\mathrm{val}} x_\\lambda - y_{\\mathrm{val}})^\\top A_{\\mathrm{val}} (A^\\top A + \\lambda^2 L^\\top L)^{-1} (L^\\top L) x_\\lambda $$\nThis is the analytical gradient of the validation risk. Note that if $\\lambda = 0$, the gradient is $0$. This is expected, as the objective function is defined in terms of $\\lambda^2$, making $R_{\\mathrm{val}}(\\lambda)$ an even function of $\\lambda$ whose derivative at $0$ must be $0$.\n\n**2. Algorithm and Implementation**\n\nTo compute the gradient efficiently, we avoid explicit matrix inversion. The computation proceeds as follows for a given $\\lambda$:\n1.  Pre-compute constant matrices: $A^\\top A$, $A^\\top y$, and $L^\\top L$.\n2.  Form the matrix $M_\\lambda = A^\\top A + \\lambda^2 L^\\top L$.\n3.  Solve the linear system $M_\\lambda x_\\lambda = A^\\top y$ to find $x_\\lambda$.\n4.  Compute the vector $v = (L^\\top L) x_\\lambda$.\n5.  Solve the linear system $M_\\lambda w = v$ to find $w = M_\\lambda^{-1} v$. This reuses the matrix $M_\\lambda$ (or its factorization).\n6.  The gradient is then calculated as $\\frac{\\mathrm{d}R_{\\mathrm{val}}}{\\mathrm{d}\\lambda} = -4\\lambda (A_{\\mathrm{val}} x_\\lambda - y_{\\mathrm{val}})^\\top A_{\\mathrm{val}} w$.\n\nThe hyperparameter $\\lambda$ is updated via projected gradient descent. For a given step size $\\alpha > 0$, the update rule is:\n$$ \\lambda_{k+1} = \\lambda_k - \\alpha \\frac{\\mathrm{d}R_{\\mathrm{val}}}{\\mathrm{d}\\lambda}\\bigg|_{\\lambda=\\lambda_k} $$\n$$ \\lambda_{k+1} = \\max(0, \\lambda_{k+1}) $$\nThe projection step $\\max(0, \\cdot)$ ensures that $\\lambda$ remains non-negative. This iterative process is repeated for a fixed number of iterations as specified in each test case.\n\nFor the gradient verification in Test Case $4$, the analytical gradient is compared to a central finite-difference approximation:\n$$ \\left. \\frac{\\mathrm{d}R_{\\mathrm{val}}}{\\mathrm{d}\\lambda} \\right|_{\\mathrm{FD}} = \\frac{R_{\\mathrm{val}}(\\lambda + h) - R_{\\mathrm{val}}(\\lambda - h)}{2h} $$\nfor a small step $h$. The relative discrepancy is then computed as specified. The data synthesis, operator definitions, and algorithm parameters for all test cases are implemented precisely as described in the problem statement.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        {\n            \"case_id\": 1, \"m_train\": 40, \"m_val\": 45, \"n\": 20, \"seed\": 1,\n            \"L_type\": \"first_diff\", \"sigma\": 1e-2, \"lambda_0\": 1e-1,\n            \"alpha\": 1e-2, \"n_iter\": 200, \"mode\": \"optimize\"\n        },\n        {\n            \"case_id\": 2, \"m_train\": 50, \"m_val\": 55, \"n\": 25, \"seed\": 3,\n            \"L_type\": \"identity\", \"sigma\": 5e-3, \"lambda_0\": 0.0,\n            \"alpha\": 1e-2, \"n_iter\": 50, \"mode\": \"optimize\"\n        },\n        {\n            \"case_id\": 3, \"m_train\": 60, \"m_val\": 65, \"n\": 30, \"seed\": 4,\n            \"L_type\": \"second_diff\", \"sigma\": 1e-2, \"lambda_0\": 5e-2,\n            \"alpha\": 2e-2, \"n_iter\": 300, \"mode\": \"optimize\"\n        },\n        {\n            \"case_id\": 4, \"m_train\": 6, \"m_val\": 7, \"n\": 5, \"seed\": 2,\n            \"L_type\": \"identity\", \"sigma\": 1e-3, \"lambda_val\": 0.15,\n            \"h\": 1e-6, \"mode\": \"verify_gradient\"\n        }\n    ]\n\n    results = []\n    for params in test_cases:\n        result = handle_case(params)\n        results.append(result)\n\n    print(f\"[{','.join(f'{r:.12g}' for r in results)}]\")\n\ndef handle_case(params):\n    \"\"\"Handles a single test case, either optimization or gradient verification.\"\"\"\n    # Generate data\n    rng = np.random.default_rng(params[\"seed\"])\n    n = params[\"n\"]\n    m_train = params[\"m_train\"]\n    m_val = params[\"m_val\"]\n    sigma = params[\"sigma\"]\n\n    A = rng.normal(size=(m_train, n))\n    A_val = rng.normal(size=(m_val, n))\n\n    i_vals = np.arange(1, n + 1)\n    x_star = np.sin(np.pi * i_vals / n) + 0.5 * np.cos(2 * np.pi * i_vals / n)\n\n    y_train = A @ x_star + sigma * rng.normal(size=m_train)\n    y_val = A_val @ x_star + sigma * rng.normal(size=m_val)\n\n    if params[\"L_type\"] == \"identity\":\n        L = np.identity(n)\n    elif params[\"L_type\"] == \"first_diff\":\n        L = np.zeros((n - 1, n))\n        np.fill_diagonal(L, -1)\n        np.fill_diagonal(L[:, 1:], 1)\n    elif params[\"L_type\"] == \"second_diff\":\n        L = np.zeros((n - 2, n))\n        np.fill_diagonal(L, 1)\n        np.fill_diagonal(L[:, 1:], -2)\n        np.fill_diagonal(L[:, 2:], 1)\n\n    # Pre-compute constant matrices\n    AtA = A.T @ A\n    LTL = L.T @ L\n    Aty = A.T @ y_train\n\n    data = (AtA, Aty, LTL, A_val, y_val)\n\n    if params[\"mode\"] == \"optimize\":\n        lam = params[\"lambda_0\"]\n        alpha = params[\"alpha\"]\n        n_iter = params[\"n_iter\"]\n        for _ in range(n_iter):\n            grad = compute_gradient(lam, data)\n            lam -= alpha * grad\n            lam = max(0.0, lam)\n        return lam\n    elif params[\"mode\"] == \"verify_gradient\":\n        lam = params[\"lambda_val\"]\n        h = params[\"h\"]\n        analytic_grad = compute_gradient(lam, data)\n\n        def R_val(lambda_val, data_tuple):\n            AtA_loc, Aty_loc, LTL_loc, A_val_loc, y_val_loc = data_tuple\n            x_lam = solve_inner(lambda_val, AtA_loc, Aty_loc, LTL_loc)\n            return np.linalg.norm(A_val_loc @ x_lam - y_val_loc)**2\n\n        R_plus = R_val(lam + h, data)\n        R_minus = R_val(lam - h, data)\n        fd_approx = (R_plus - R_minus) / (2 * h)\n\n        discrepancy = np.abs(fd_approx - analytic_grad) / np.maximum(1e-12, np.abs(fd_approx))\n        return discrepancy\n\ndef solve_inner(lam, AtA, Aty, LTL):\n    \"\"\"Solves the inner problem for x_lambda.\"\"\"\n    M = AtA + lam**2 * LTL\n    x_lam = np.linalg.solve(M, Aty)\n    return x_lam\n\ndef compute_gradient(lam, data):\n    \"\"\"Computes the analytical gradient of the validation risk.\"\"\"\n    AtA, Aty, LTL, A_val, y_val = data\n    \n    if lam == 0.0:\n        return 0.0\n\n    # 1. Solve for x_lambda\n    x_lam = solve_inner(lam, AtA, Aty, LTL)\n\n    # 2. Form M_lambda\n    M = AtA + lam**2 * LTL\n    \n    # 3. Solve for sensitivity component w\n    # We want to compute dR/dλ = -4λ (e_val)ᵀ A_val (M⁻¹ LᵀL x_λ)\n    # Let v = LᵀL x_λ. Let w = M⁻¹ v. Then dR/dλ = -4λ (e_val)ᵀ A_val w.\n    v = LTL @ x_lam\n    w = np.linalg.solve(M, v)\n\n    # 4. Compute final gradient\n    e_val = A_val @ x_lam - y_val\n    grad = -4 * lam * (e_val.T @ A_val @ w)\n    \n    return grad\n\nif __name__ == \"__main__\":\n    solve()\n\n```", "id": "3427389"}]}