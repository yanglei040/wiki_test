{"hands_on_practices": [{"introduction": "In variational data assimilation, gradients are computed efficiently using an adjoint model, which is the cornerstone of large-scale optimization. However, any error in the adjoint implementation will lead the optimization astray, making verification an essential first step. This practice guides you through the 'dot-product test,' a fundamental numerical procedure for verifying that your coded tangent linear and adjoint operators correctly satisfy the defining mathematical relationship to machine precision [@problem_id:3430448].", "problem": "You are asked to implement and verify a discrete dot-product adjoint test for variational data assimilation using a set of finite-dimensional models. The goal is to check numerically that, for a nonlinear mapping between Euclidean spaces endowed with possibly weighted inner products, the adjoint of the tangent linear operator satisfies the defining equality of adjoints to machine precision.\n\nStart from the following fundamental base:\n- The directional (Gateaux) derivative of a mapping between finite-dimensional vector spaces, the definition of Jacobian matrices, and their action on perturbations.\n- The definition of an adjoint operator with respect to an inner product: given inner products on the domain and codomain, the adjoint is the unique linear map that satisfies the inner-product equality for all directions.\n- Linear algebra facts about symmetric positive definite matrices and weighted inner products in finite-dimensional spaces.\n\nLet $M:\\mathbb{R}^n\\to\\mathbb{R}^m$ be a differentiable mapping, and let $x^\\ast\\in\\mathbb{R}^n$ be a reference state. Let $M'(x^\\ast)$ denote the Jacobian (tangent linear) operator evaluated at $x^\\ast$. Equip the domain $\\mathbb{R}^n$ with an inner product $\\langle u,v\\rangle_X = u^\\top W_X v$ and the codomain $\\mathbb{R}^m$ with an inner product $\\langle y,z\\rangle_Y = y^\\top W_Y z$, where $W_X\\in\\mathbb{R}^{n\\times n}$ and $W_Y\\in\\mathbb{R}^{m\\times m}$ are symmetric positive definite matrices. The adjoint of $M'(x^\\ast)$ with respect to these inner products, denoted $M'(x^\\ast)^\\star:\\mathbb{R}^m\\to\\mathbb{R}^n$, is defined as the unique linear map that satisfies\n$$\n\\langle M'(x^\\ast)\\,\\delta x, \\delta y \\rangle_Y \\;=\\; \\langle \\delta x, M'(x^\\ast)^\\star \\,\\delta y \\rangle_X\n$$\nfor all $\\delta x\\in\\mathbb{R}^n$ and $\\delta y\\in\\mathbb{R}^m$.\n\nYour task is to implement a program that, for each of the test cases below, computes a numerical estimate of the mismatch\n$$\n\\varepsilon \\;=\\; \\frac{\\left|\\langle M'(x^\\ast)\\,\\delta x, \\delta y \\rangle_Y \\;-\\; \\langle \\delta x, M'(x^\\ast)^\\star \\,\\delta y \\rangle_X\\right|}{\\max\\!\\left(1,\\left|\\langle M'(x^\\ast)\\,\\delta x, \\delta y \\rangle_Y\\right|,\\left|\\langle \\delta x, M'(x^\\ast)^\\star \\,\\delta y \\rangle_X\\right|\\right)}\\,,\n$$\nand verifies that $\\varepsilon$ is at the level of machine precision. You must derive the adjoint action $M'(x^\\ast)^\\star\\,\\delta y$ directly from the adjoint definition and the weighted inner products, using only the fundamental base above, without relying on any pre-given formula.\n\nImplement the following models and test cases. All random quantities must be generated deterministically using the specified seeds. Angles are not involved. There are no physical units to report.\n\nDefinitions:\n- The elementwise hyperbolic tangent is denoted by $\\tanh(\\cdot)$, the elementwise sine by $\\sin(\\cdot)$, the elementwise cosine by $\\cos(\\cdot)$, and the elementwise hyperbolic secant squared by $\\operatorname{sech}^2(z)=1-\\tanh^2(z)$.\n- For any vector $a\\in\\mathbb{R}^k$, $\\operatorname{diag}(a)$ denotes the $k\\times k$ diagonal matrix with the entries of $a$ on its diagonal.\n\nModels:\n- Linear model: $M(x)=A\\,x$ with $A\\in\\mathbb{R}^{m\\times n}$.\n- Nonlinear model: $M(x)=\\tanh(L\\,x + b)+ D\\,\\sin(x)$ with $L\\in\\mathbb{R}^{m\\times n}$, $b\\in\\mathbb{R}^m$, $D\\in\\mathbb{R}^{m\\times n}$, where all functions are applied elementwise.\n\nTangent linear operators:\n- For the linear model, $M'(x^\\ast)\\,\\delta x = A\\,\\delta x$.\n- For the nonlinear model, define $z=L\\,x^\\ast + b$ and $g=\\operatorname{sech}^2(z)$. Then\n$$\nM'(x^\\ast)\\,\\delta x \\;=\\; \\operatorname{diag}(g)\\,L\\,\\delta x \\;+\\; D\\,\\operatorname{diag}(\\cos(x^\\ast))\\,\\delta x\\,.\n$$\n\nAdjoint operators:\n- You must derive and implement $M'(x^\\ast)^\\star$ for each case under the given inner products, starting from the adjoint definition above. Do not assume any pre-remembered shortcut formula; instead, derive the algebraic form from first principles.\n\nTest suite:\n- Case $\\mathbf{A}$ (linear, Euclidean inner products):\n  - Dimensions: $n=5$, $m=4$.\n  - Random seeds: matrix $A$ from seed $s_A=0$; perturbations $\\delta x$ and $\\delta y$ from seeds $s_x=1$ and $s_y=2$.\n  - Weights: $W_X=I_n$, $W_Y=I_m$, where $I_k$ is the $k\\times k$ identity.\n- Case $\\mathbf{B}$ (linear, weighted inner products):\n  - Dimensions: $n=6$, $m=6$.\n  - Random seeds: matrix $A$ from seed $s_A=3$; perturbations $\\delta x$ and $\\delta y$ from seeds $s_x=6$ and $s_y=7$.\n  - Weights: $W_X=R_X^\\top R_X+\\alpha I_n$ and $W_Y=R_Y^\\top R_Y+\\beta I_m$, with $R_X$ from seed $s_{RX}=4$, $R_Y$ from seed $s_{RY}=5$, and scalars $\\alpha=n$, $\\beta=m$.\n- Case $\\mathbf{C}$ (nonlinear, Euclidean inner products):\n  - Dimensions: $n=8$, $m=7$.\n  - Random seeds: matrices $L$ and $D$ from seeds $s_L=10$ and $s_D=11$; vector $b$ from seed $s_b=12$; reference state $x^\\ast$ from seed $s_{x^\\ast}=13$; perturbations $\\delta x$ and $\\delta y$ from seeds $s_x=14$ and $s_y=15$.\n  - Weights: $W_X=I_n$, $W_Y=I_m$.\n- Case $\\mathbf{D}$ (edge case with zero perturbation):\n  - Dimensions: $n=4$, $m=3$.\n  - Random seeds: matrix $A$ from seed $s_A=8$; perturbations $\\delta x=\\mathbf{0}$ (the zero vector) and $\\delta y$ from seed $s_y=9$.\n  - Weights: $W_X=I_n$, $W_Y=I_m$.\n\nFor each case, compute the mismatch $\\varepsilon$ as defined above. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order $\\left[\\varepsilon_A,\\varepsilon_B,\\varepsilon_C,\\varepsilon_D\\right]$. The entries must be floating-point numbers. No other text should be printed. The numerical results must be consistent with machine precision for the test to be considered passed.", "solution": "The objective is to perform a numerical verification of the adjoint property for several discrete models, a procedure commonly known as the dot-product test. This test is a fundamental debugging and validation tool in fields that rely on adjoint models, such as variational data assimilation and large-scale optimization. The verification must confirm that for a given linear operator and specified inner products on its domain and codomain, the defining equality for the adjoint holds to within machine precision.\n\nLet $M: \\mathbb{R}^n \\to \\mathbb{R}^m$ be a differentiable mapping. Its linearization at a reference state $x^\\ast \\in \\mathbb{R}^n$ is the tangent linear operator, which is a linear map represented by the Jacobian matrix $K = M'(x^\\ast) \\in \\mathbb{R}^{m \\times n}$. The domain $\\mathbb{R}^n$ and codomain $\\mathbb{R}^m$ are endowed with weighted inner products:\n$$ \\langle u, v \\rangle_X = u^\\top W_X v \\quad \\text{for } u, v \\in \\mathbb{R}^n $$\n$$ \\langle y, z \\rangle_Y = y^\\top W_Y z \\quad \\text{for } y, z \\in \\mathbb{R}^m $$\nwhere $W_X \\in \\mathbb{R}^{n \\times n}$ and $W_Y \\in \\mathbb{R}^{m \\times m}$ are symmetric positive definite matrices.\n\nThe adjoint of the operator $K$, denoted $K^\\star: \\mathbb{R}^m \\to \\mathbb{R}^n$, is uniquely defined by the relation:\n$$ \\langle K \\delta x, \\delta y \\rangle_Y = \\langle \\delta x, K^\\star \\delta y \\rangle_X $$\nwhich must hold for all perturbations $\\delta x \\in \\mathbb{R}^n$ and $\\delta y \\in \\mathbb{R}^m$.\n\nTo implement the dot-product test, we must first derive an explicit expression for the action of the adjoint, $K^\\star \\delta y$. We begin by substituting the definitions of the inner products into the defining relation.\n\nThe left-hand side (LHS) is:\n$$ \\langle K \\delta x, \\delta y \\rangle_Y = (K \\delta x)^\\top W_Y \\delta y $$\nUsing the transpose property $(AB)^\\top = B^\\top A^\\top$, this becomes:\n$$ \\text{LHS} = (\\delta x)^\\top K^\\top W_Y \\delta y $$\n\nThe right-hand side (RHS) is:\n$$ \\langle \\delta x, K^\\star \\delta y \\rangle_X = (\\delta x)^\\top W_X (K^\\star \\delta y) $$\n\nEquating the two expressions, we obtain:\n$$ (\\delta x)^\\top K^\\top W_Y \\delta y = (\\delta x)^\\top W_X (K^\\star \\delta y) $$\nThis equality must hold for any choice of vector $\\delta x$. This implies that the vectors multiplying $(\\delta x)^\\top$ on both sides must be identical:\n$$ K^\\top W_Y \\delta y = W_X (K^\\star \\delta y) $$\nSince the weight matrix $W_X$ is symmetric positive definite, it is invertible. We can therefore solve for the action $K^\\star \\delta y$ by pre-multiplying by the inverse of $W_X$, denoted $W_X^{-1}$:\n$$ K^\\star \\delta y = W_X^{-1} K^\\top W_Y \\delta y $$\nThis derived expression provides the concrete computational formula for the action of the adjoint operator. Numerically, calculating this action is best performed by solving the linear system $W_X z = v$, where $z = K^\\star \\delta y$ and $v = K^\\top W_Y \\delta y$, thus avoiding the potential numerical instability of explicit matrix inversion.\n\nFor the special case of standard Euclidean inner products, the weight matrices are identity matrices: $W_X = I_n$ and $W_Y = I_m$. In this configuration, the formula for the adjoint action simplifies considerably:\n$$ K^\\star \\delta y = (I_n)^{-1} K^\\top I_m \\delta y = K^\\top \\delta y $$\nThus, for Euclidean inner products, the adjoint operator is simply the transpose of the original linear operator.\n\nThe problem specifies two models for which this verification must be carried out.\n1.  **Linear Model**: $M(x) = Ax$. The tangent linear operator is constant and independent of the reference state $x^\\ast$, i.e., $K = M'(x^\\ast) = A$.\n2.  **Nonlinear Model**: $M(x) = \\tanh(Lx + b) + D\\sin(x)$. The tangent linear operator, evaluated at $x^\\ast$, is given by $K = \\operatorname{diag}(g)L + D\\operatorname{diag}(\\cos(x^\\ast))$, where $g = \\operatorname{sech}^2(Lx^\\ast + b) = 1 - \\tanh^2(Lx^\\ast + b)$.\n\nThe numerical verification consists of computing both sides of the adjoint definition and evaluating their difference. A normalized error metric, $\\varepsilon$, is used to quantify the mismatch:\n$$ \\varepsilon = \\frac{|\\text{LHS} - \\text{RHS}|}{\\max(1, |\\text{LHS}|, |\\text{RHS}|)} = \\frac{\\left|\\langle K \\delta x, \\delta y \\rangle_Y - \\langle \\delta x, K^\\star \\delta y \\rangle_X\\right|}{\\max(1, |\\langle K \\delta x, \\delta y \\rangle_Y|, |\\langle \\delta x, K^\\star \\delta y \\rangle_X|)} $$\nFor a correct implementation, the value of $\\varepsilon$ should be on the order of machine precision (typically around $10^{-15}$ to $10^{-16}$ for double-precision floating-point arithmetic), provided the operands are not zero. For the edge case where one of the perturbations is a zero vector (e.g., $\\delta x = \\mathbf{0}$), both the LHS and RHS evaluate to exactly zero, resulting in $\\varepsilon=0$. The implementation will proceed with these derived formulas to compute $\\varepsilon$ for each test case.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and runs a discrete dot-product adjoint test for a set of\n    finite-dimensional models to verify the adjoint property numerically.\n    \"\"\"\n\n    test_cases = [\n        {\n            'case_id': 'A', 'model': 'linear', 'n': 5, 'm': 4,\n            's_A': 0, 's_x': 1, 's_y': 2, 'weights': 'euclidean'\n        },\n        {\n            'case_id': 'B', 'model': 'linear', 'n': 6, 'm': 6,\n            's_A': 3, 's_x': 6, 's_y': 7, 'weights': 'weighted',\n            's_RX': 4, 's_RY': 5\n        },\n        {\n            'case_id': 'C', 'model': 'nonlinear', 'n': 8, 'm': 7,\n            's_L': 10, 's_D': 11, 's_b': 12, 's_x_star': 13,\n            's_x': 14, 's_y': 15, 'weights': 'euclidean'\n        },\n        {\n            'case_id': 'D', 'model': 'linear', 'n': 4, 'm': 3,\n            's_A': 8, 's_y': 9, 'weights': 'euclidean', 'dx_zero': True\n        }\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        n, m = case['n'], case['m']\n\n        # Generate perturbations\n        rng_y = np.random.default_rng(case['s_y'])\n        delta_y = rng_y.standard_normal((m, 1))\n\n        if case.get('dx_zero', False):\n            delta_x = np.zeros((n, 1))\n        else:\n            rng_x = np.random.default_rng(case['s_x'])\n            delta_x = rng_x.standard_normal((n, 1))\n\n        # Construct weight matrices\n        if case['weights'] == 'euclidean':\n            W_X = np.eye(n)\n            W_Y = np.eye(m)\n        else: # weighted\n            alpha, beta = n, m\n            \n            rng_RX = np.random.default_rng(case['s_RX'])\n            R_X = rng_RX.standard_normal((n, n))\n            W_X = R_X.T @ R_X + alpha * np.eye(n)\n\n            rng_RY = np.random.default_rng(case['s_RY'])\n            R_Y = rng_RY.standard_normal((m, m))\n            W_Y = R_Y.T @ R_Y + beta * np.eye(m)\n            \n        # Construct tangent linear operator K\n        if case['model'] == 'linear':\n            rng_A = np.random.default_rng(case['s_A'])\n            A = rng_A.standard_normal((m, n))\n            K = A\n        else: # nonlinear\n            rng_L = np.random.default_rng(case['s_L'])\n            L = rng_L.standard_normal((m, n))\n            \n            rng_D = np.random.default_rng(case['s_D'])\n            D = rng_D.standard_normal((m, n))\n\n            rng_b = np.random.default_rng(case['s_b'])\n            b = rng_b.standard_normal((m, 1))\n\n            rng_x_star = np.random.default_rng(case['s_x_star'])\n            x_star = rng_x_star.standard_normal((n, 1))\n            \n            z = L @ x_star + b\n            tanh_z = np.tanh(z)\n            g = 1 - tanh_z**2\n            cos_x_star = np.cos(x_star)\n            \n            K = np.diag(g.flatten()) @ L + D @ np.diag(cos_x_star.flatten())\n            \n        # Calculate LHS: <K*delta_x, delta_y>_Y\n        lhs_val = (K @ delta_x).T @ W_Y @ delta_y\n        lhs = lhs_val.item()\n\n        # Calculate RHS: <delta_x, K_star*delta_y>_X\n        # K_star*delta_y = inv(W_X) * K.T * W_Y * delta_y\n        v = K.T @ W_Y @ delta_y\n        adj_action = np.linalg.solve(W_X, v)\n        rhs_val = delta_x.T @ W_X @ adj_action\n        rhs = rhs_val.item()\n\n        # Calculate mismatch epsilon\n        numerator = abs(lhs - rhs)\n        denominator = max(1, abs(lhs), abs(rhs))\n        epsilon = numerator / denominator\n        \n        results.append(epsilon)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3430448"}, {"introduction": "The incremental formulation of 4D-Var linearizes the nonlinear forecast model, approximating its behavior with a tangent linear model. This approximation is only valid for small perturbations, and this practice provides a hands-on method to test its limits. You will quantify the linearization error and verify its expected quadratic scaling, thereby empirically determining the 'trust region' within which the linear model is a faithful representation of the full dynamics [@problem_id:3430513].", "problem": "You are to implement a computational test of the tangent linear hypothesis in the context of variational data assimilation for a nonlinear forward map. The test quantifies the linearization error and identifies a trust region within which the linear approximation remains valid. The work is to be conducted in purely mathematical terms and expressed numerically as specified, with angles in radians.\n\nConsider the nonlinear map $M:\\mathbb{R}^2\\rightarrow\\mathbb{R}^2$ defined by\n$$\nM(\\mathbf{x}) = \\begin{bmatrix}\nx_1 + x_2^2 \\\\\n\\sin(x_1) + \\cosh(x_2)\n\\end{bmatrix},\n$$\nwhere $\\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix}$ and the sine function uses angles in radians. Let $M'(\\mathbf{x}^\\ast)$ denote the Fréchet derivative (Jacobian) of $M$ at the base point $\\mathbf{x}^\\ast$. For a perturbation vector $\\boldsymbol{\\delta} \\in \\mathbb{R}^2$, define the linearization error by\n$$\ne(\\boldsymbol{\\delta}) = \\left\\| M(\\mathbf{x}^\\ast + \\boldsymbol{\\delta}) - M(\\mathbf{x}^\\ast) - M'(\\mathbf{x}^\\ast) \\boldsymbol{\\delta} \\right\\|_2,\n$$\nwhere $\\|\\cdot\\|_2$ denotes the Euclidean norm.\n\nThe goal is to design and implement a test that:\n- Computes $e(s \\,\\mathbf{d})$ for a set of perturbation magnitudes $s$ along a given direction $\\mathbf{d}$, where $\\mathbf{d}$ is normalized to have unit Euclidean norm.\n- Verifies the quadratic behavior of $e(s \\,\\mathbf{d})$ by examining the scaling of $e(s \\,\\mathbf{d})$ with respect to $s^2$. Specifically, define the curvature ratio\n$$\nR(s; \\mathbf{d}) = \\frac{e(s \\,\\mathbf{d})}{s^2},\n$$\nand identify the largest $s$ within a prescribed range such that $R(s; \\mathbf{d})$ remains within a specified relative tolerance of a baseline value computed at the smallest tested $s$.\n- Enforces a smallness condition by requiring $e(s \\,\\mathbf{d}) \\le \\varepsilon_{\\text{small}}$ for accepted $s$.\n- Determines the trust region radius $r(\\mathbf{x}^\\ast, \\mathbf{d})$ as the largest $s$ satisfying both the quadratic behavior and smallness conditions.\n\nYou must derive $M'(\\mathbf{x})$ from the definition of the Fréchet derivative and implement the computation of $e(\\boldsymbol{\\delta})$ and $R(s; \\mathbf{d})$. Use a logarithmically spaced set of $s$ values spanning a prescribed interval $[s_{\\min}, s_{\\max}]$ with a specified number of samples $N_s$, and choose the baseline curvature ratio $R_{\\text{ref}}$ as $R(s_{\\min}; \\mathbf{d})$. The quadratic behavior condition is defined as\n$$\n\\left| \\frac{R(s; \\mathbf{d}) - R_{\\text{ref}}}{R_{\\text{ref}}} \\right| \\le \\tau,\n$$\nfor a given tolerance $\\tau > 0$, and the smallness condition is $e(s \\,\\mathbf{d}) \\le \\varepsilon_{\\text{small}}$.\n\nImplement the above for the following test suite of parameter values, which covers a general case, strong nonlinearity, an extreme smallness constraint, and high curvature:\n\n- Test case $1$: $\\mathbf{x}^\\ast = \\begin{bmatrix} 0.5 \\\\ -0.75 \\end{bmatrix}$, $\\mathbf{d} = \\begin{bmatrix} 1.0 \\\\ 0.2 \\end{bmatrix}$, $s_{\\min} = 10^{-6}$, $s_{\\max} = 10^{-1}$, $N_s = 30$, $\\tau = 0.25$, $\\varepsilon_{\\text{small}} = 10^{-3}$.\n- Test case $2$: $\\mathbf{x}^\\ast = \\begin{bmatrix} 2.0 \\\\ 2.0 \\end{bmatrix}$, $\\mathbf{d} = \\begin{bmatrix} 1.0 \\\\ -1.0 \\end{bmatrix}$, $s_{\\min} = 10^{-8}$, $s_{\\max} = 10^{-2}$, $N_s = 40$, $\\tau = 0.15$, $\\varepsilon_{\\text{small}} = 10^{-4}$.\n- Test case $3$: $\\mathbf{x}^\\ast = \\begin{bmatrix} 0.0 \\\\ 0.0 \\end{bmatrix}$, $\\mathbf{d} = \\begin{bmatrix} 0.0 \\\\ 1.0 \\end{bmatrix}$, $s_{\\min} = 10^{-9}$, $s_{\\max} = 10^{-1}$, $N_s = 25$, $\\tau = 0.10$, $\\varepsilon_{\\text{small}} = 10^{-12}$.\n- Test case $4$: $\\mathbf{x}^\\ast = \\begin{bmatrix} -1.5 \\\\ 3.0 \\end{bmatrix}$, $\\mathbf{d} = \\begin{bmatrix} 0.3 \\\\ 1.0 \\end{bmatrix}$, $s_{\\min} = 10^{-10}$, $s_{\\max} = 10^{-3}$, $N_s = 50$, $\\tau = 0.20$, $\\varepsilon_{\\text{small}} = 10^{-6}$.\n\nFor each test case, compute and return the trust region radius $r(\\mathbf{x}^\\ast, \\mathbf{d})$ as a real number. If no tested $s$ satisfies both conditions, define $r(\\mathbf{x}^\\ast, \\mathbf{d}) = 0.0$.\n\nYour program should produce a single line of output containing the results for the four test cases as a comma-separated list enclosed in square brackets, for example, $\\left[ r_1, r_2, r_3, r_4 \\right]$, with each $r_i$ a floating-point number. No additional text should be printed.", "solution": "The objective is to conduct a computational test of the tangent linear hypothesis for a given nonlinear map, $M$. This test involves quantifying the linearization error and determining the radius of a trust region, a zone within which the linear approximation of the map is valid according to specified criteria. The process requires deriving the Jacobian of the map and implementing a numerical algorithm to evaluate the validity conditions.\n\nThe nonlinear map $M$ from $\\mathbb{R}^2$ to $\\mathbb{R}^2$ is defined as:\n$$\nM(\\mathbf{x}) = \\begin{bmatrix} M_1(\\mathbf{x}) \\\\ M_2(\\mathbf{x}) \\end{bmatrix} = \\begin{bmatrix} x_1 + x_2^2 \\\\ \\sin(x_1) + \\cosh(x_2) \\end{bmatrix}\n$$\nwhere $\\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix}$ is a vector in $\\mathbb{R}^2$.\n\nFirst, we derive the Fréchet derivative (Jacobian matrix) of $M$, denoted by $M'(\\mathbf{x})$. The elements of the Jacobian are the partial derivatives of the components of $M$:\n$$\nM'(\\mathbf{x}) = \\begin{bmatrix} \\frac{\\partial M_1}{\\partial x_1} & \\frac{\\partial M_1}{\\partial x_2} \\\\ \\frac{\\partial M_2}{\\partial x_1} & \\frac{\\partial M_2}{\\partial x_2} \\end{bmatrix}\n$$\nComputing each partial derivative:\n- $\\frac{\\partial M_1}{\\partial x_1} = \\frac{\\partial}{\\partial x_1}(x_1 + x_2^2) = 1$\n- $\\frac{\\partial M_1}{\\partial x_2} = \\frac{\\partial}{\\partial x_2}(x_1 + x_2^2) = 2x_2$\n- $\\frac{\\partial M_2}{\\partial x_1} = \\frac{\\partial}{\\partial x_1}(\\sin(x_1) + \\cosh(x_2)) = \\cos(x_1)$\n- $\\frac{\\partial M_2}{\\partial x_2} = \\frac{\\partial}{\\partial x_2}(\\sin(x_1) + \\cosh(x_2)) = \\sinh(x_2)$\n\nThus, the Jacobian matrix is:\n$$\nM'(\\mathbf{x}) = \\begin{bmatrix} 1 & 2x_2 \\\\ \\cos(x_1) & \\sinh(x_2) \\end{bmatrix}\n$$\n\nThe core of the analysis is the linearization error, $e(\\boldsymbol{\\delta})$, which measures the discrepancy between the full nonlinear change in $M$ and its linear approximation. For a perturbation $\\boldsymbol{\\delta}$ from a base point $\\mathbf{x}^\\ast$, this error is given by the Euclidean norm of the difference:\n$$\ne(\\boldsymbol{\\delta}) = \\left\\| M(\\mathbf{x}^\\ast + \\boldsymbol{\\delta}) - M(\\mathbf{x}^\\ast) - M'(\\mathbf{x}^\\ast) \\boldsymbol{\\delta} \\right\\|_2\n$$\nAccording to Taylor's theorem, for a sufficiently smooth map $M$, this error is of the order of the square of the norm of the perturbation, i.e., $e(\\boldsymbol{\\delta}) = O(\\|\\boldsymbol{\\delta}\\|_2^2)$.\n\nTo verify this quadratic behavior, we examine perturbations along a specific direction $\\mathbf{d}$ with varying magnitudes $s$, such that $\\boldsymbol{\\delta} = s \\mathbf{d}$. The curvature ratio, $R(s; \\mathbf{d})$, is defined to normalize the error by $s^2$:\n$$\nR(s; \\mathbf{d}) = \\frac{e(s \\mathbf{d})}{s^2}\n$$\nFor small $s$, $R(s; \\mathbf{d})$ should be approximately constant, approaching a limit related to the second derivative of $M$.\n\nThe computational procedure to find the trust region radius $r(\\mathbf{x}^\\ast, \\mathbf{d})$ for each test case is as follows:\n\n1.  **Initialization**: The parameters for each test case are provided: the base point $\\mathbf{x}^\\ast$, the direction vector $\\mathbf{d}$, the range of perturbation magnitudes $[s_{\\min}, s_{\\max}]$, the number of samples $N_s$, the relative tolerance $\\tau$, and the smallness threshold $\\varepsilon_{\\text{small}}$.\n\n2.  **Normalization of Direction Vector**: The given direction vector $\\mathbf{d}$ is normalized to have a unit Euclidean norm to ensure that $s$ represents the true magnitude of the perturbation: $\\mathbf{d}_{\\text{norm}} = \\mathbf{d} / \\|\\mathbf{d}\\|_2$.\n\n3.  **Discretization of Perturbation Magnitudes**: A set of $N_s$ perturbation magnitudes $\\{ s_i \\}$ is generated by logarithmically spacing points in the interval $[s_{\\min}, s_{\\max}]$. This allows for efficient probing of behavior across several orders of magnitude.\n\n4.  **Baseline Calculation**: A reference curvature ratio, $R_{\\text{ref}}$, is computed using the smallest perturbation magnitude, $s_{\\min}$. This value serves as the baseline for \"quadratic\" behavior.\n    $$\n    R_{\\text{ref}} = R(s_{\\min}; \\mathbf{d}_{\\text{norm}}) = \\frac{e(s_{\\min} \\mathbf{d}_{\\text{norm}})}{s_{\\min}^2}\n    $$\n    This requires evaluating $M$ and $M'$ at $\\mathbf{x}^\\ast$.\n\n5.  **Iterative Search and Validation**: The algorithm iterates through the ordered set of magnitudes $\\{ s_i \\}$ from smallest to largest. For each $s_i$, two conditions must be satisfied:\n    - **Quadratic Behavior Condition**: The curvature ratio $R(s_i; \\mathbf{d}_{\\text{norm}})$ must remain close to the reference ratio $R_{\\text{ref}}$. The relative difference must be within the tolerance $\\tau$:\n      $$\n      \\left| \\frac{R(s_i; \\mathbf{d}_{\\text{norm}}) - R_{\\text{ref}}}{R_{\\text{ref}}} \\right| \\le \\tau\n      $$\n    - **Smallness Condition**: The absolute linearization error must be small enough, ensuring the linear approximation is not just scaling correctly but is also accurate in an absolute sense:\n      $$\n      e(s_i \\mathbf{d}_{\\text{norm}}) \\le \\varepsilon_{\\text{small}}\n      $$\n\n6.  **Determination of Trust Region Radius**: The trust region radius, $r(\\mathbf{x}^\\ast, \\mathbf{d})$, is defined as the largest value of $s_i$ from the generated set that satisfies both the quadratic behavior and smallness conditions. Since the magnitudes $\\{s_i\\}$ are processed in increasing order, the radius is updated each time a value $s_i$ is found to be valid. The final updated value is the result. If no $s_i$ satisfies both conditions, the radius is taken to be $0.0$.\n\nThis procedure is implemented numerically for each specified test case.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases for the tangent linear hypothesis test.\n    \"\"\"\n\n    def solve_for_case(x_star_list, d_list, s_min, s_max, N_s, tau, epsilon_small):\n        \"\"\"\n        Computes the trust region radius for a single set of parameters.\n\n        Args:\n            x_star_list (list): The base point x*.\n            d_list (list): The perturbation direction d.\n            s_min (float): The minimum perturbation magnitude.\n            s_max (float): The maximum perturbation magnitude.\n            N_s (int): The number of samples for s.\n            tau (float): The relative tolerance for the curvature ratio.\n            epsilon_small (float): The threshold for the linearization error.\n\n        Returns:\n            float: The computed trust region radius.\n        \"\"\"\n        # Convert inputs to numpy arrays for vector operations\n        x_star = np.array(x_star_list, dtype=np.float64)\n        d = np.array(d_list, dtype=np.float64)\n\n        # Step 1: Normalize the direction vector d\n        d_norm = d / np.linalg.norm(d)\n\n        # Step 2: Define the nonlinear map M and its Jacobian M'\n        def M(x):\n            x1, x2 = x\n            return np.array([x1 + x2**2, np.sin(x1) + np.cosh(x2)], dtype=np.float64)\n\n        def M_prime(x):\n            x1, x2 = x\n            return np.array([[1.0, 2.0 * x2],\n                             [np.cos(x1), np.sinh(x2)]], dtype=np.float64)\n\n        # Pre-calculate terms that are constant throughout the loop\n        M_at_x_star = M(x_star)\n        Jacobian_at_x_star = M_prime(x_star)\n\n        # Step 3: Generate logarithmically spaced perturbation magnitudes\n        s_values = np.logspace(np.log10(s_min), np.log10(s_max), N_s)\n\n        # Step 4: Calculate the reference linearization error and curvature ratio\n        s_ref = s_values[0]\n        delta_ref = s_ref * d_norm\n        \n        # e(delta) = || M(x* + delta) - M(x*) - M'(x*)delta ||\n        error_vector_ref = M(x_star + delta_ref) - M_at_x_star - Jacobian_at_x_star @ delta_ref\n        e_ref = np.linalg.norm(error_vector_ref)\n        \n        # R = e / s^2\n        R_ref = e_ref / (s_ref**2)\n\n        # Step 5 & 6: Iterate through s values to find the largest one satisfying the conditions\n        trust_region_radius = 0.0\n        \n        if R_ref == 0.0:\n            # Degenerate case where the second-order term is zero in direction d.\n            # The quadratic behavior test as defined will likely only pass for s where R(s) is also 0.\n            # For this problem's context, any s satisfying the smallness condition could be accepted\n            # if R(s) remains 0.\n            for s in s_values:\n                delta = s * d_norm\n                error_vector = M(x_star + delta) - M_at_x_star - Jacobian_at_x_star @ delta\n                e_s = np.linalg.norm(error_vector)\n                R_s = e_s / (s**2)\n                if e_s <= epsilon_small and R_s == 0.0:\n                    trust_region_radius = s\n            return trust_region_radius\n        \n        for s in s_values:\n            delta = s * d_norm\n            error_vector = M(x_star + delta) - M_at_x_star - Jacobian_at_x_star @ delta\n            e_s = np.linalg.norm(error_vector)\n\n            # Condition 1: Smallness condition\n            smallness_condition_met = (e_s <= epsilon_small)\n            \n            # Condition 2: Quadratic behavior condition\n            R_s = e_s / (s**2)\n            relative_diff = np.abs((R_s - R_ref) / R_ref)\n            quadratic_condition_met = (relative_diff <= tau)\n\n            # If both conditions are met, this s is a candidate. Since s_values is sorted,\n            # the last one found will be the largest.\n            if smallness_condition_met and quadratic_condition_met:\n                trust_region_radius = s\n\n        return trust_region_radius\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test case 1: General case\n        {'x_star_list': [0.5, -0.75], 'd_list': [1.0, 0.2], 's_min': 1e-6, 's_max': 1e-1, 'N_s': 30, 'tau': 0.25, 'epsilon_small': 1e-3},\n        # Test case 2: Strong nonlinearity\n        {'x_star_list': [2.0, 2.0], 'd_list': [1.0, -1.0], 's_min': 1e-8, 's_max': 1e-2, 'N_s': 40, 'tau': 0.15, 'epsilon_small': 1e-4},\n        # Test case 3: Extreme smallness constraint\n        {'x_star_list': [0.0, 0.0], 'd_list': [0.0, 1.0], 's_min': 1e-9, 's_max': 1e-1, 'N_s': 25, 'tau': 0.10, 'epsilon_small': 1e-12},\n        # Test case 4: High curvature\n        {'x_star_list': [-1.5, 3.0], 'd_list': [0.3, 1.0], 's_min': 1e-10, 's_max': 1e-3, 'N_s': 50, 'tau': 0.20, 'epsilon_small': 1e-6},\n    ]\n\n    results = []\n    for case in test_cases:\n        result = solve_for_case(**case)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3430513"}, {"introduction": "Once the validity of the linear model is understood, we can build robust optimization algorithms that explicitly use this information. This practice introduces the trust-region framework, a powerful method for solving the nonlinear least-squares problem in 4D-Var. You will implement a strategy that selects an optimal step size by balancing the aggressive, fast-converging Gauss-Newton step with the safe, reliable steepest-descent direction, ensuring stability even when the problem is highly nonlinear [@problem_id:3430474].", "problem": "Consider the incremental form of four-dimensional variational (4D-Var) data assimilation. Let the state increment be denoted by $\\delta x \\in \\mathbb{R}^n$. The background error covariance is $\\mathbf{B} \\in \\mathbb{R}^{n \\times n}$, assumed symmetric positive definite. For time indices $i \\in \\{1,\\dots, m\\}$, let the tangent-linear model mapping from the initial time to time $i$ be $\\mathbf{M}_{0,i} \\in \\mathbb{R}^{n \\times n}$, the linearized observation operator be $\\mathbf{H}_i \\in \\mathbb{R}^{p_i \\times n}$, the observation error covariance be $\\mathbf{R}_i \\in \\mathbb{R}^{p_i \\times p_i}$, symmetric positive definite, and the observed innovation be $d_i \\in \\mathbb{R}^{p_i}$. The incremental cost function is\n$$\nJ(\\delta x) = \\tfrac{1}{2}\\,\\|\\delta x\\|_{\\mathbf{B}^{-1}}^2 + \\tfrac{1}{2}\\,\\sum_{i=1}^m \\|\\mathbf{H}_i \\mathbf{M}_{0,i}\\,\\delta x - d_i\\|_{\\mathbf{R}_i^{-1}}^2,\n$$\nwith $\\|u\\|_{\\mathbf{W}}^2 = u^\\top \\mathbf{W}\\, u$ for a symmetric positive definite matrix $\\mathbf{W}$. At the current linearization point $\\delta x = 0$, define the Gauss–Newton quadratic model\n$$\nq(s) = J(0) + \\nabla J(0)^\\top s + \\tfrac{1}{2}\\, s^\\top \\mathbf{H}_{\\text{GN}}\\, s,\n$$\nwhere the Gauss–Newton Hessian is\n$$\n\\mathbf{H}_{\\text{GN}} = \\mathbf{B}^{-1} + \\sum_{i=1}^m (\\mathbf{H}_i \\mathbf{M}_{0,i})^\\top \\mathbf{R}_i^{-1} (\\mathbf{H}_i \\mathbf{M}_{0,i}),\n$$\nand the gradient at $\\delta x = 0$ is\n$$\n\\nabla J(0) = \\mathbf{B}^{-1}\\, 0 + \\sum_{i=1}^m (\\mathbf{H}_i \\mathbf{M}_{0,i})^\\top \\mathbf{R}_i^{-1}\\,(\\mathbf{H}_i \\mathbf{M}_{0,i}\\,0 - d_i) = - \\sum_{i=1}^m (\\mathbf{H}_i \\mathbf{M}_{0,i})^\\top \\mathbf{R}_i^{-1}\\, d_i.\n$$\nYou will construct a trust-region Gauss–Newton method for the quadratic model $q(s)$ with a radius $\\Delta$ selected by a predicted-reduction criterion that uses the Gauss–Newton Hessian. Specifically:\n- The Gauss–Newton step $s_{\\text{GN}}$ is the minimizer of $q(s)$ over $\\mathbb{R}^n$.\n- The model-predicted reduction for a step $s$ is defined by $\\mathrm{pred}(s) = q(0) - q(s)$.\n- The steepest-descent (Cauchy) direction at $s=0$ is $- \\nabla J(0)$, and for a given radius $r > 0$, the Cauchy step constrained to the ball of radius $r$ is $s_C(r) = - \\alpha(r)\\, \\nabla J(0)$ with $\\alpha(r) = r / \\|\\nabla J(0)\\|_2$.\n\nTrust-region radius selection by predicted reduction:\n- Given a target fraction $\\gamma \\in (0,1]$ and a maximum radius $\\Delta_{\\max} > 0$, select $\\Delta$ to be the smallest radius $r \\in (0, \\Delta_{\\max}]$ such that the predicted reduction along the Cauchy step at radius $r$ equals $\\gamma$ times the predicted reduction achieved by the Gauss–Newton step, i.e., enforce $\\mathrm{pred}(s_C(r)) = \\gamma \\,\\mathrm{pred}(s_{\\text{GN}})$, whenever such an $r$ exists. If no such $r$ exists (because the maximum achievable predicted reduction along the Cauchy direction is smaller than $\\gamma \\,\\mathrm{pred}(s_{\\text{GN}})$), then choose $r$ to be the radius that maximizes the predicted reduction along the Cauchy direction, and finally set $\\Delta = \\min\\{r, \\Delta_{\\max}\\}$.\n\nAnalysis goal:\n- Using only fundamental definitions and linear algebra, determine, for each test case below, whether the unconstrained Gauss–Newton step $s_{\\text{GN}}$ lies strictly outside the selected trust region, i.e., whether $\\|s_{\\text{GN}}\\|_2 > \\Delta$. Report a boolean indicator for this event, and also report the norm $\\|s_{\\text{GN}}\\|_2$, the selected radius $\\Delta$, the predicted reduction $\\mathrm{pred}(s_{\\text{GN}})$, the predicted reduction $\\mathrm{pred}(s_C(\\Delta))$, and the ratio $\\mathrm{pred}(s_C(\\Delta))\\,/\\,\\mathrm{pred}(s_{\\text{GN}})$.\n\nYour program must implement the following steps from first principles:\n- Assemble $\\mathbf{H}_{\\text{GN}}$ and $\\nabla J(0)$ from the given $\\mathbf{B}$, $\\mathbf{M}_{0,i}$, $\\mathbf{H}_i$, $\\mathbf{R}_i$, and $d_i$.\n- Compute the Gauss–Newton step $s_{\\text{GN}}$ as the unique global minimizer of $q(s)$ in $\\mathbb{R}^n$.\n- Compute the model-predicted reductions $\\mathrm{pred}(s_{\\text{GN}})$ and $\\mathrm{pred}(s_C(r))$ for a general radius $r$.\n- Implement the radius-selection rule stated above to obtain $\\Delta$ based on the target fraction $\\gamma$ and the cap $\\Delta_{\\max}$, using only the definitions of predicted reduction and the Cauchy step.\n- Decide whether $\\|s_{\\text{GN}}\\|_2 > \\Delta$.\n\nTest suite:\nFor each case, $n$ is the state dimension, $m$ is the number of observation times, and all matrices and vectors are given numerically. Use the exact values listed here.\n\nCase $1$ (boundary case with isotropic curvature):\n- $n = 2$, $m = 1$.\n- $\\mathbf{B} = \\begin{bmatrix} 1.0 & 0.0 \\\\ 0.0 & 1.0 \\end{bmatrix}$.\n- $\\mathbf{M}_{0,1} = \\begin{bmatrix} 1.0 & 0.0 \\\\ 0.0 & 1.0 \\end{bmatrix}$.\n- $\\mathbf{H}_1 = \\begin{bmatrix} 1.0 & 0.0 \\\\ 0.0 & 1.0 \\end{bmatrix}$.\n- $\\mathbf{R}_1 = \\begin{bmatrix} 0.1 & 0.0 \\\\ 0.0 & 0.1 \\end{bmatrix}$.\n- $d_1 = \\begin{bmatrix} 1.0 \\\\ 1.0 \\end{bmatrix}$.\n- $\\gamma = 1.0$, $\\Delta_{\\max} = 10.0$.\n\nCase $2$ (leaving due to strict fraction):\n- Same as Case $1$ but with $\\gamma = 0.25$, $\\Delta_{\\max} = 10.0$.\n\nCase $3$ (ill-conditioned, multi-time, anisotropic):\n- $n = 2$, $m = 2$.\n- $\\mathbf{B} = \\begin{bmatrix} 100.0 & 0.0 \\\\ 0.0 & 0.1 \\end{bmatrix}$.\n- $\\mathbf{M}_{0,1} = \\begin{bmatrix} 1.0 & 2.0 \\\\ 0.01 & 1.0 \\end{bmatrix}$, $\\mathbf{M}_{0,2} = \\begin{bmatrix} 0.5 & -0.3 \\\\ 0.2 & 1.5 \\end{bmatrix}$.\n- $\\mathbf{H}_1 = \\begin{bmatrix} 1.0 & 0.0 \\\\ 0.0 & 1.0 \\end{bmatrix}$, $\\mathbf{H}_2 = \\begin{bmatrix} 1.0 & 0.0 \\\\ 0.0 & 1.0 \\end{bmatrix}$.\n- $\\mathbf{R}_1 = \\begin{bmatrix} 0.5 & 0.0 \\\\ 0.0 & 2.0 \\end{bmatrix}$, $\\mathbf{R}_2 = \\begin{bmatrix} 0.1 & 0.0 \\\\ 0.0 & 0.2 \\end{bmatrix}$.\n- $d_1 = \\begin{bmatrix} 1.0 \\\\ -1.0 \\end{bmatrix}$, $d_2 = \\begin{bmatrix} 0.2 \\\\ -0.5 \\end{bmatrix}$.\n- $\\gamma = 1.0$, $\\Delta_{\\max} = 10.0$.\n\nCase $4$ (capped radius):\n- Same as Case $1$ but with $\\gamma = 1.0$, $\\Delta_{\\max} = 0.5$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with no whitespace. Each test case contributes a list of six entries in the order $[\\text{leave}, \\|s_{\\text{GN}}\\|_2, \\Delta, \\mathrm{pred}(s_{\\text{GN}}), \\mathrm{pred}(s_C(\\Delta)), \\mathrm{pred}(s_C(\\Delta))/\\mathrm{pred}(s_{\\text{GN}})]$, where $\\text{leave}$ is $1$ if $\\|s_{\\text{GN}}\\|_2 > \\Delta$ and $0$ otherwise. The overall output is thus a list of lists, for example, $[[\\dots],[\\dots],[\\dots],[\\dots]]$.", "solution": "The user-provided problem is a well-posed and scientifically grounded exercise in numerical optimization, specifically concerning the application of a trust-region Gauss-Newton method to the incremental 4D-Var data assimilation cost function. The problem asks for the implementation and analysis of a specific trust-region radius selection strategy based on the predicted reduction. We will proceed by first deriving the general formulas for the quantities of interest, followed by their application to the specific test cases provided.\n\nThe core of the problem lies in minimizing the Gauss-Newton quadratic model of the cost function $J(\\delta x)$ around the point $\\delta x = 0$. The model is given by:\n$$\nq(s) = J(0) + \\nabla J(0)^\\top s + \\tfrac{1}{2}\\, s^\\top \\mathbf{H}_{\\text{GN}}\\, s\n$$\nFor brevity, let us denote the gradient at $\\delta x = 0$ as $g \\equiv \\nabla J(0)$ and the Gauss-Newton Hessian as $H \\equiv \\mathbf{H}_{\\text{GN}}$. The quadratic model becomes:\n$$\nq(s) = J(0) + g^\\top s + \\tfrac{1}{2}\\, s^\\top H s\n$$\nThe gradient $g$ and Hessian $H$ are assembled from the problem components as follows:\n$$\ng = - \\sum_{i=1}^m (\\mathbf{H}_i \\mathbf{M}_{0,i})^\\top \\mathbf{R}_i^{-1}\\, d_i\n$$\n$$\nH = \\mathbf{B}^{-1} + \\sum_{i=1}^m (\\mathbf{H}_i \\mathbf{M}_{0,i})^\\top \\mathbf{R}_i^{-1} (\\mathbf{H}_i \\mathbf{M}_{0,i})\n$$\nSince $\\mathbf{B}$ and all $\\mathbf{R}_i$ are symmetric positive definite, their inverses $\\mathbf{B}^{-1}$ and $\\mathbf{R}_i^{-1}$ are also symmetric positive definite. The Hessian $H$ is a sum of a symmetric positive definite matrix ($\\mathbf{B}^{-1}$) and one or more symmetric positive semi-definite matrices ($(\\mathbf{H}_i \\mathbf{M}_{0,i})^\\top \\mathbf{R}_i^{-1} (\\mathbf{H}_i \\mathbf{M}_{0,i})$). Consequently, $H$ is symmetric positive definite, which guarantees that the quadratic model $q(s)$ is strictly convex and has a unique global minimizer.\n\nThe first step is to compute the unconstrained Gauss-Newton step, $s_{\\text{GN}}$. This step minimizes $q(s)$ over $\\mathbb{R}^n$ and is found by setting the gradient of $q(s)$ to zero:\n$$\n\\nabla q(s) = g + H s = 0 \\implies s_{\\text{GN}} = -H^{-1} g\n$$\n\nNext, we evaluate the model-predicted reduction, $\\mathrm{pred}(s)$, which measures the decrease in the model value $q(s)$ from its value at $s=0$:\n$$\n\\mathrm{pred}(s) = q(0) - q(s) = q(0) - (J(0) + g^\\top s + \\tfrac{1}{2}\\, s^\\top H s) = -g^\\top s - \\tfrac{1}{2}\\, s^\\top H s\n$$\nFor the Gauss-Newton step $s_{\\text{GN}}$, the predicted reduction is:\n$$\n\\mathrm{pred}(s_{\\text{GN}}) = -g^\\top(-H^{-1}g) - \\tfrac{1}{2}(-H^{-1}g)^\\top H (-H^{-1}g) = g^\\top H^{-1} g - \\tfrac{1}{2}g^\\top H^{-1} H H^{-1} g = \\tfrac{1}{2}g^\\top H^{-1} g\n$$\nAn equivalent expression, often useful for computation, is $\\mathrm{pred}(s_{\\text{GN}}) = -\\frac{1}{2}g^\\top s_{\\text{GN}}$.\n\nThe trust-region method constrains the minimization of $q(s)$ to a ball of radius $\\Delta$. The radius selection rule depends on the behavior of the model along the steepest-descent (Cauchy) direction, which is $p = -g$. A step along this direction with length $r$ is $s_C(r) = \\alpha(r) p = - \\alpha(r) g$. To have $\\|s_C(r)\\|_2 = r$, we must choose $\\alpha(r) = r / \\|g\\|_2$. Thus,\n$$\ns_C(r) = -\\frac{r}{\\|g\\|_2} g\n$$\nThe predicted reduction for this Cauchy step is a function of the radius $r$:\n$$\n\\mathrm{pred}(s_C(r)) = -g^\\top \\left(-\\frac{r}{\\|g\\|_2} g\\right) - \\tfrac{1}{2} \\left(-\\frac{r}{\\|g\\|_2} g\\right)^\\top H \\left(-\\frac{r}{\\|g\\|_2} g\\right)\n$$\n$$\n\\mathrm{pred}(s_C(r)) = \\frac{r}{\\|g\\|_2} (g^\\top g) - \\frac{1}{2} \\frac{r^2}{\\|g\\|_2^2} (g^\\top H g) = r \\|g\\|_2 - \\frac{g^\\top H g}{2 \\|g\\|_2^2} r^2\n$$\nLet's denote this quadratic function of $r$ as $P(r)$. The trust-region radius $\\Delta$ is selected by finding the smallest radius $r \\in (0, \\Delta_{\\max}]$ that satisfies the condition:\n$$\nP(r) = \\gamma \\,\\mathrm{pred}(s_{\\text{GN}})\n$$\nLet $C_{\\text{target}} = \\gamma \\,\\mathrm{pred}(s_{\\text{GN}})$. We must solve the quadratic equation for $r$:\n$$\n\\left(\\frac{g^\\top H g}{2 \\|g\\|_2^2}\\right) r^2 - \\|g\\|_2 r + C_{\\text{target}} = 0\n$$\nThe discriminant of this equation is $\\mathcal{D} = (\\|g\\|_2)^2 - 4 \\left(\\frac{g^\\top H g}{2 \\|g\\|_2^2}\\right) C_{\\text{target}} = \\|g\\|_2^2 - 2 \\frac{g^\\top H g}{\\|g\\|_2^2} C_{\\text{target}}$.\n- If $\\mathcal{D} \\ge 0$, real solutions for $r$ exist. Since the quadratic term's coefficient is positive and the linear term's coefficient is negative, and $C_{\\text{target}} > 0$ (assuming $g \\neq 0$), both roots are positive. We choose the smaller root:\n  $$\n  r^* = \\frac{\\|g\\|_2 - \\sqrt{\\mathcal{D}}}{g^\\top H g / \\|g\\|_2^2}\n  $$\n- If $\\mathcal{D} < 0$, no real solution exists. This physically means that the target reduction $C_{\\text{target}}$ is greater than the maximum achievable reduction along the Cauchy direction. The problem statement specifies that in this case, we should choose the radius $r$ that maximizes $P(r)$. This radius is found by setting $P'(r) = 0$:\n  $$\n  P'(r) = \\|g\\|_2 - r \\frac{g^\\top H g}{\\|g\\|_2^2} = 0 \\implies r^* = \\frac{\\|g\\|_2^3}{g^\\top H g}\n  $$\n  This radius $r^*$ corresponds to the length of the \"Cauchy point\".\n\nAfter determining the candidate radius $r^*$, the final trust-region radius $\\Delta$ is capped by $\\Delta_{\\max}$:\n$$\n\\Delta = \\min(r^*, \\Delta_{\\max})\n$$\nFinally, for each test case, we compute the required quantities: $\\|s_{\\text{GN}}\\|_2$, $\\Delta$, $\\mathrm{pred}(s_{\\text{GN}})$, and $\\mathrm{pred}(s_C(\\Delta))$, and determine if the Gauss-Newton step lies outside the trust region, i.e., if $\\|s_{\\text{GN}}\\|_2 > \\Delta$. The ratio $\\mathrm{pred}(s_C(\\Delta)) / \\mathrm{pred}(s_{\\text{GN}})$ is also computed. The implementation will perform these calculations for each case.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the trust-region subproblem for each test case.\n    \"\"\"\n\n    test_cases = [\n        # Case 1: boundary case with isotropic curvature\n        {\n            \"n\": 2, \"m\": 1,\n            \"B\": np.array([[1.0, 0.0], [0.0, 1.0]]),\n            \"M0_list\": [np.array([[1.0, 0.0], [0.0, 1.0]])],\n            \"H_list\": [np.array([[1.0, 0.0], [0.0, 1.0]])],\n            \"R_list\": [np.array([[0.1, 0.0], [0.0, 0.1]])],\n            \"d_list\": [np.array([1.0, 1.0])],\n            \"gamma\": 1.0,\n            \"delta_max\": 10.0\n        },\n        # Case 2: leaving due to strict fraction\n        {\n            \"n\": 2, \"m\": 1,\n            \"B\": np.array([[1.0, 0.0], [0.0, 1.0]]),\n            \"M0_list\": [np.array([[1.0, 0.0], [0.0, 1.0]])],\n            \"H_list\": [np.array([[1.0, 0.0], [0.0, 1.0]])],\n            \"R_list\": [np.array([[0.1, 0.0], [0.0, 0.1]])],\n            \"d_list\": [np.array([1.0, 1.0])],\n            \"gamma\": 0.25,\n            \"delta_max\": 10.0\n        },\n        # Case 3: ill-conditioned, multi-time, anisotropic\n        {\n            \"n\": 2, \"m\": 2,\n            \"B\": np.array([[100.0, 0.0], [0.0, 0.1]]),\n            \"M0_list\": [np.array([[1.0, 2.0], [0.01, 1.0]]), np.array([[0.5, -0.3], [0.2, 1.5]])],\n            \"H_list\": [np.array([[1.0, 0.0], [0.0, 1.0]]), np.array([[1.0, 0.0], [0.0, 1.0]])],\n            \"R_list\": [np.array([[0.5, 0.0], [0.0, 2.0]]), np.array([[0.1, 0.0], [0.0, 0.2]])],\n            \"d_list\": [np.array([1.0, -1.0]), np.array([0.2, -0.5])],\n            \"gamma\": 1.0,\n            \"delta_max\": 10.0\n        },\n        # Case 4: capped radius\n        {\n            \"n\": 2, \"m\": 1,\n            \"B\": np.array([[1.0, 0.0], [0.0, 1.0]]),\n            \"M0_list\": [np.array([[1.0, 0.0], [0.0, 1.0]])],\n            \"H_list\": [np.array([[1.0, 0.0], [0.0, 1.0]])],\n            \"R_list\": [np.array([[0.1, 0.0], [0.0, 0.1]])],\n            \"d_list\": [np.array([1.0, 1.0])],\n            \"gamma\": 1.0,\n            \"delta_max\": 0.5\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        result = process_case(case)\n        all_results.append(result)\n\n    # Format the final output string\n    # e.g. [[1,0.632,0.531,0.823,0.700,0.850]]\n    output_str = \"[\"\n    for i, res in enumerate(all_results):\n        leave_val = 1 if res['leave'] else 0\n        res_str = f\"[{leave_val},{res['norm_s_gn']:.7f},{res['delta']:.7f},{res['pred_s_gn']:.7f},{res['pred_s_c_delta']:.7f},{res['ratio']:.7f}]\"\n        output_str += res_str\n        if i < len(all_results) - 1:\n            output_str += \",\"\n    output_str += \"]\"\n    \n    print(output_str)\n\n\ndef process_case(case_data):\n    \"\"\"\n    Computes all required quantities for a single test case.\n    \"\"\"\n    B = case_data['B']\n    M0_list = case_data['M0_list']\n    H_list = case_data['H_list']\n    R_list = case_data['R_list']\n    d_list = case_data['d_list']\n    gamma = case_data['gamma']\n    delta_max = case_data['delta_max']\n    n = case_data['n']\n    m = case_data['m']\n\n    # Assemble gradient and Hessian\n    B_inv = np.linalg.inv(B)\n    g = np.zeros(n)\n    H_gn = np.copy(B_inv)\n\n    for i in range(m):\n        M0i = M0_list[i]\n        Hi = H_list[i]\n        Ri = R_list[i]\n        di = d_list[i]\n        \n        Ri_inv = np.linalg.inv(Ri)\n        HiM0i = Hi @ M0i\n\n        g -= (HiM0i.T @ Ri_inv @ di)\n        H_gn += (HiM0i.T @ Ri_inv @ HiM0i)\n\n    # Compute Gauss-Newton step and its norm\n    s_gn = np.linalg.solve(H_gn, -g)\n    norm_s_gn = np.linalg.norm(s_gn)\n\n    # Compute predicted reduction for GN step\n    pred_s_gn = -0.5 * g.T @ s_gn\n\n    # Radius selection logic\n    if np.linalg.norm(g) < 1e-12:\n        # If gradient is zero, optimal step is zero, delta is arbitrary but > 0\n        r_star = delta_max \n    else:\n        C_target = gamma * pred_s_gn\n        norm_g = np.linalg.norm(g)\n        norm_g_sq = norm_g**2\n        gHg = g.T @ H_gn @ g\n\n        # We solve (gHg / (2*norm_g_sq)) r^2 - norm_g r + C_target = 0\n        a = 0.5 * gHg / norm_g_sq\n        b = -norm_g\n        c = C_target\n        \n        discriminant = b**2 - 4 * a * c\n        \n        if discriminant >= 0:\n            # Smallest positive root\n            r_star = (-b - np.sqrt(discriminant)) / (2 * a)\n        else:\n            # Maximizer of pred(s_C(r))\n            r_star = norm_g**3 / gHg\n\n    delta = min(r_star, delta_max)\n\n    # Final calculations\n    leave = norm_s_gn > delta\n    \n    # Predicted reduction for Cauchy step of length delta\n    norm_g = np.linalg.norm(g)\n    norm_g_sq = norm_g**2 if norm_g > 1e-12 else 1.0\n    gHg = g.T @ H_gn @ g\n    pred_s_c_delta = delta * norm_g - 0.5 * delta**2 * gHg / norm_g_sq\n\n    # Ratio of predicted reductions\n    if abs(pred_s_gn) < 1e-15:\n        # Avoid division by zero, although not expected in these cases\n        ratio = 0.0 if abs(pred_s_c_delta) < 1e-15 else np.inf\n    else:\n        ratio = pred_s_c_delta / pred_s_gn\n\n    return {\n        \"leave\": leave,\n        \"norm_s_gn\": norm_s_gn,\n        \"delta\": delta,\n        \"pred_s_gn\": pred_s_gn,\n        \"pred_s_c_delta\": pred_s_c_delta,\n        \"ratio\": ratio\n    }\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3430474"}]}