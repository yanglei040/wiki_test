{"hands_on_practices": [{"introduction": "The backbone of any variational data assimilation system is the implementation of the tangent-linear and adjoint models. This exercise provides a comprehensive, practical guide to this essential process using a physically-based radiative transfer model. You will not only implement the forward, tangent-linear, and adjoint codes from first principles but also master the standard verification techniques—the Taylor test and the adjoint inner-product test—that are crucial for ensuring your code is correct. [@problem_id:3398784]", "problem": "Consider a one-dimensional, plane-parallel, purely absorbing, non-scattering discrete-layer radiative transfer forward/observation model $H:\\mathbb{R}^N\\to\\mathbb{R}^K$ defined by the following physically consistent construction based on the Beer–Lambert law. Let the state vector be $x\\in\\mathbb{R}^N$, with layer optical depths given by $\\tau_i=\\exp(x_i)$ to enforce positivity. For each spectral channel $k=1,\\dots,K$, define channel-layer optical depths $\\Delta\\tau_{k,i}=w_{k,i}\\,\\tau_i$, where $w_{k,i}0$ are known channel-layer weighting coefficients. Let the top-of-atmosphere solar source term be $s_k0$ for channel $k$, and let each layer $i$ have a channel-dependent emission/source term $b_{k,i}\\ge 0$ assumed constant within the layer. Define the upward transmission from the top of layer $i$ to the top of layer $i+1$ as $Z_{k,i}=\\exp(-\\Delta\\tau_{k,i})$, the cumulative upward transmission to just above layer $i$ as $T_{k,i-1}=\\prod_{j=1}^{i-1} Z_{k,j}$ with $T_{k,0}=1$, and the layer $i$ emission escape fraction as $L_{k,i}=1-Z_{k,i}$. The top-of-atmosphere upwelling radiance for channel $k$ is then\n$$\ny_k = s_k \\exp\\Big(-\\sum_{i=1}^N \\Delta\\tau_{k,i}\\Big) + \\sum_{i=1}^N b_{k,i}\\,L_{k,i}\\,T_{k,i-1}.\n$$\nCollecting all channels, the forward model is $H(x)=(y_1,\\dots,y_K)^\\top$.\n\nYou must implement:\n1. The forward model $H(x)$ as defined above.\n2. The directional derivative (tangent-linear action) $D H_x(\\delta)$ for any direction $\\delta\\in\\mathbb{R}^N$, using first principles and the chain rule. The directional derivative must be computed without forming the full Jacobian matrix explicitly.\n3. The adjoint action $A_x(\\eta)=\\big(D H_x\\big)^\\top \\eta$ for any $\\eta\\in\\mathbb{R}^K$, by deriving and coding a mathematically exact reverse-mode/adjoint accumulation that satisfies the inner-product identity\n$$\n\\langle D H_x(\\delta),\\,\\eta\\rangle_{\\mathbb{R}^K}=\\langle \\delta,\\,A_x(\\eta)\\rangle_{\\mathbb{R}^N}.\n$$\n\nVerification via Taylor test: For a set of positive step sizes $\\alpha$, verify that the Taylor remainder metric\n$$\nR(\\alpha) = \\frac{\\left\\|H(x+\\alpha \\delta)-H(x)-D H_x(\\alpha \\delta)\\right\\|_2}{\\alpha}\n$$\nscales linearly in $\\alpha$, i.e., $R(\\alpha)=\\mathcal{O}(\\alpha)$, which corresponds to a slope near $1$ in the log-log relation $\\log R(\\alpha)$ versus $\\log \\alpha$. Implement a numerical estimation of the slope by least-squares fitting of $\\log_{10} R(\\alpha)$ against $\\log_{10} \\alpha$ over the given $\\alpha$ values.\n\nAdjoint-code diagnosis: Implement an adjoint inner-product test by computing the relative discrepancy\n$$\n\\mathrm{err}_{\\mathrm{adj}}=\\frac{\\left|\\langle D H_x(\\delta),\\,\\eta\\rangle - \\langle \\delta,\\,A_x(\\eta)\\rangle\\right|}{\\max\\left(10^{-16},\\,\\left|\\langle D H_x(\\delta),\\,\\eta\\rangle\\right|+\\left|\\langle \\delta,\\,A_x(\\eta)\\rangle\\right|\\right)}.\n$$\nAdditionally, implement a “faulty” adjoint variant that omits the solar-term contribution in the adjoint accumulation, and demonstrate that the inner-product identity fails for this faulty variant while the Taylor test for the forward/tangent-linear remains correct. If the Taylor scaling fails (slope far from $1$), report the adjoint relative error to help diagnose whether the adjoint was implemented incorrectly.\n\nUse the following test suite of parameter sets, each providing $(N,K,w,s,b,x,\\delta,\\eta,\\text{faulty})$, where $N$ is the number of layers, $K$ is the number of channels, $w\\in\\mathbb{R}^{K\\times N}$, $s\\in\\mathbb{R}^K$, $b\\in\\mathbb{R}^{K\\times N}$, $x\\in\\mathbb{R}^N$, $\\delta\\in\\mathbb{R}^N$, and $\\eta\\in\\mathbb{R}^K$. All quantities are dimensionless. The list of step sizes for the Taylor test is $\\alpha\\in\\{10^{-1},\\,5\\times 10^{-2},\\,2.5\\times 10^{-2},\\,1.25\\times 10^{-2},\\,6.25\\times 10^{-3}\\}$:\n\n- Case 1 (general “happy path”, correct adjoint):\n    - $N=6$, $K=4$.\n    - $w_{k,i}=0.2+0.05\\,k+0.03\\,i$ for $k\\in\\{1,2,3,4\\}$, $i\\in\\{1,\\dots,6\\}$.\n    - $s_k=1.0+0.2\\,k$.\n    - $b_{k,i}=0.1+0.05\\,i+0.03\\,k$.\n    - $x=[-0.8,\\,-0.3,\\,0.2,\\,0.7,\\,-0.5,\\,0.1]^\\top$.\n    - $\\delta=[0.1,\\,-0.2,\\,0.05,\\,-0.1,\\,0.2,\\,-0.05]^\\top$.\n    - $\\eta=[0.3,\\,-0.5,\\,0.7,\\,-0.2]^\\top$.\n    - faulty = False.\n\n- Case 2 (boundary: very small optical depths, correct adjoint):\n    - $N=6$, $K=4$.\n    - $w_{k,i}=0.05+0.02\\,k+0.01\\,i$.\n    - $s_k=0.2+0.1\\,k$.\n    - $b_{k,i}=0.02+0.03\\,i+0.01\\,k$.\n    - $x=[-5.5,\\,-4.8,\\,-4.2,\\,-3.9,\\,-3.5,\\,-3.2]^\\top$.\n    - $\\delta=[0.02,\\,-0.01,\\,0.03,\\,-0.02,\\,0.01,\\,-0.03]^\\top$.\n    - $\\eta=[1.0,\\,-0.8,\\,0.6,\\,-0.4]^\\top$.\n    - faulty = False.\n\n- Case 3 (edge: very large optical depths, correct adjoint):\n    - $N=6$, $K=4$.\n    - $w_{k,i}=0.7+0.1\\,k+0.2\\,i$.\n    - $s_k=1.0+0.3\\,k$.\n    - $b_{k,i}=0.4+0.1\\,i+0.05\\,k$.\n    - $x=[3.0,\\,3.5,\\,4.0,\\,4.5,\\,5.0,\\,3.8]^\\top$.\n    - $\\delta=[-0.05,\\,0.1,\\,-0.08,\\,0.06,\\,-0.04,\\,0.02]^\\top$.\n    - $\\eta=[-0.5,\\,0.4,\\,-0.3,\\,0.2]^\\top$.\n    - faulty = False.\n\n- Case 4 (diagnosis: same as Case 1 but faulty adjoint):\n    - Use Case 1 parameters and set faulty = True.\n\nFor each case, compute:\n- The estimated slope (a float) of $\\log_{10} R(\\alpha)$ versus $\\log_{10} \\alpha$ using least-squares over the provided $\\alpha$ values.\n- A boolean indicating whether the Taylor test passed, defined by slope within the interval $[0.9,\\,1.1]$.\n- The adjoint inner-product relative error (a float) computed from the exact adjoint if faulty = False and from the faulty adjoint if faulty = True.\n- A boolean indicating whether the adjoint inner-product test passed, defined by $\\mathrm{err}_{\\mathrm{adj}}10^{-10}$.\n\nYour program should produce a single line of output containing the results of all cases as a comma-separated list of per-case result lists, each per-case list in the order [slope, taylor_pass_boolean, adjoint_relative_error, adjoint_pass_boolean], enclosed in square brackets. For example, the output must have the form\n```\n[[slope_1,True,err_1,True],[slope_2,True,err_2,True],[slope_3,True,err_3,True],[slope_4,True,err_4,False]]\n```\nwhere the booleans and floats reflect your computed values. No physical units are required, and angles do not appear; all quantities are dimensionless. The program must be self-contained and require no input.", "solution": "The problem statement is assessed to be valid. It is scientifically grounded in the principles of radiative transfer, mathematically well-posed, and expressed in objective, formal language. It presents a complete and consistent setup for a non-trivial computational task involving the implementation of a forward model, its tangent-linear counterpart, and the corresponding adjoint model, along with standard verification procedures.\n\nThe solution proceeds by first deriving the mathematical expressions for the tangent-linear and adjoint models from first principles, and then outlining their implementation.\n\nLet the state vector be $x \\in \\mathbb{R}^N$ and the measurement vector be $y \\in \\mathbb{R}^K$. The forward model is a function $H:\\mathbb{R}^N \\to \\mathbb{R}^K$ mapping the state to the measurements. The components of the state vector, $x_i$, are related to the layer optical depths $\\tau_i$ by the transformation $\\tau_i = \\exp(x_i)$. This ensures that $\\tau_i  0$.\n\nThe upwelling radiance $y_k$ for channel $k$ is given by:\n$$\ny_k = s_k \\exp\\Big(-\\sum_{i=1}^N \\Delta\\tau_{k,i}\\Big) + \\sum_{i=1}^N b_{k,i}\\,L_{k,i}\\,T_{k,i-1}\n$$\nwhere the intermediate quantities are defined as:\n- Channel-layer optical depth: $\\Delta\\tau_{k,i} = w_{k,i}\\,\\tau_i = w_{k,i} \\exp(x_i)$\n- Layer upward transmission: $Z_{k,i} = \\exp(-\\Delta\\tau_{k,i})$\n- Layer emission escape fraction: $L_{k,i} = 1 - Z_{k,i}$\n- Cumulative upward transmission to just above layer $i$: $T_{k,i} = \\prod_{j=1}^{i} Z_{k,j}$, with the base case $T_{k,0} = 1$. The term $T_{k,i-1}$ in the main equation is thus an empty product for $i=1$, correctly evaluating to $1$.\n\n**1. Forward Model Implementation**\nThe forward model $H(x)$ is implemented by computing the sequence of intermediate variables for each channel $k=1,\\dots,K$ and each layer $i=1,\\dots,N$:\n1.  Compute layer optical depths $\\tau_i = \\exp(x_i)$ for $i=1,\\dots,N$.\n2.  For each channel $k$, compute channel-layer optical depths $\\Delta\\tau_{k,i} = w_{k,i}\\tau_i$.\n3.  For each channel $k$, compute layer transmissions $Z_{k,i} = \\exp(-\\Delta\\tau_{k,i})$.\n4.  For each channel $k$, compute cumulative transmissions $T_{k,i}$ recursively: $T_{k,0}=1$ and $T_{k,i} = T_{k,i-1} Z_{k,i}$ for $i=1,\\dots,N$.\n5.  The radiance $y_k$ is then calculated using the main equation, which can be expressed in terms of the cumulative transmissions as $y_k = s_k T_{k,N} + \\sum_{i=1}^N b_{k,i}(T_{k,i-1} - T_{k,i})$.\nAll intermediate quantities ($\\tau_i, \\Delta\\tau_{k,i}, Z_{k,i}, T_{k,i}$) are stored for use in the tangent-linear and adjoint calculations.\n\n**2. Tangent-Linear Model: Directional Derivative $D H_x(\\delta)$**\nThe tangent-linear model computes the action of the Jacobian of $H$ on a perturbation vector $\\delta \\in \\mathbb{R}^N$, denoted $\\delta y = DH_x(\\delta)$, without explicitly forming the Jacobian matrix. This is achieved by propagating the initial perturbation $\\delta x = \\delta$ through the chain of operations in the forward model. Let $\\delta v$ denote the perturbation of a variable $v$.\n\n1.  Perturbation of layer optical depth: $\\delta\\tau_i = \\frac{d\\tau_i}{dx_i}\\delta x_i = \\exp(x_i)\\delta x_i = \\tau_i \\delta x_i$.\n2.  Perturbation of channel-layer optical depth: $\\delta(\\Delta\\tau_{k,i}) = w_{k,i}\\delta\\tau_i = w_{k,i}\\tau_i\\delta x_i = \\Delta\\tau_{k,i}\\delta x_i$.\n3.  Perturbation of layer transmission: $\\delta Z_{k,i} = \\frac{dZ_{k,i}}{d\\Delta\\tau_{k,i}}\\delta(\\Delta\\tau_{k,i}) = -\\exp(-\\Delta\\tau_{k,i})\\delta(\\Delta\\tau_{k,i}) = -Z_{k,i}\\delta(\\Delta\\tau_{k,i})$.\n4.  Perturbation of cumulative transmission: From $T_{k,i} = T_{k,i-1}Z_{k,i}$, the product rule gives $\\delta T_{k,i} = \\delta T_{k,i-1}Z_{k,i} + T_{k,i-1}\\delta Z_{k,i}$. This is computed recursively starting with $\\delta T_{k,0} = 0$.\n5.  The final radiance perturbation is $\\delta y_k = s_k \\delta T_{k,N} + \\sum_{i=1}^N b_{k,i}(\\delta T_{k,i-1} - \\delta T_{k,i})$.\n\nThis forward propagation of perturbations yields the vector $\\delta y = (\\delta y_1, \\dots, \\delta y_K)^\\top$.\n\n**3. Adjoint Model: Transposed Action $A_x(\\eta) = (D H_x)^\\top \\eta$**\nThe adjoint model calculates the action of the transpose of the Jacobian on a vector $\\eta \\in \\mathbb{R}^K$. This is implemented using reverse-mode differentiation, which propagates sensitivities backwards from the output to the input. Let $\\bar{v}$ denote the derivative of the final scalar objective function $L = \\langle y, \\eta \\rangle = \\sum_k y_k \\eta_k$ with respect to an intermediate variable $v$. The final result is the vector of derivatives $\\bar{x}_j = \\partial L / \\partial x_j$.\n\nThe algorithm proceeds as follows:\n1.  Initialize all adjoint variables ($\\bar{\\tau}_i, \\bar{\\Delta\\tau}_{k,i}, \\dots$) to $0$. The input to the adjoint model is $\\bar{y}_k = \\eta_k$ for $k=1,\\dots,K$.\n2.  The calculation is performed per channel $k$ and the contributions to $\\bar{x}$ are accumulated. For each channel $k$:\n    a. Initialize an array for adjoint cumulative transmissions, $\\bar{T}_k$, of size $N+1$ to zeros.\n    b. For $y_k = s_k T_{k,N} + \\sum_{i=1}^N b_{k,i}(T_{k,i-1} - T_{k,i})$, the adjoint contributions to $\\bar{T}_k$ are:\n       - $\\bar{T}_{k,N} \\mathrel{+}= \\eta_k s_k$. This step is omitted for the faulty adjoint.\n       - For $i=1,\\dots,N$: $\\bar{T}_{k,i-1} \\mathrel{+}= \\eta_k b_{k,i}$ and $\\bar{T}_{k,i} \\mathrel{-}= \\eta_k b_{k,i}$.\n    c. Propagate sensitivities backward through the cumulative transmission calculation. For $i=N,\\dots,1$:\n       From $T_{k,i} = T_{k,i-1}Z_{k,i}$, we have:\n       - $\\bar{Z}_{k,i} \\mathrel{+}= \\bar{T}_{k,i} T_{k,i-1}$\n       - $\\bar{T}_{k,i-1} \\mathrel{+}= \\bar{T}_{k,i} Z_{k,i}$\n    d. Propagate from $\\bar{Z}_{k,i}$ to $\\bar{\\Delta\\tau}_{k,i}$ for $i=1,\\dots,N$:\n       From $Z_{k,i} = \\exp(-\\Delta\\tau_{k,i})$, we have $\\bar{\\Delta\\tau}_{k,i} \\mathrel{+}= \\bar{Z}_{k,i} (-Z_{k,i})$.\n3.  After iterating over all channels $k$, accumulate contributions to $\\bar{\\tau}_i$:\n    From $\\Delta\\tau_{k,i} = w_{k,i}\\tau_i$, we have $\\bar{\\tau}_i \\mathrel{+}= \\sum_k \\bar{\\Delta\\tau}_{k,i} w_{k,i}$.\n4.  Finally, propagate from $\\bar{\\tau}_i$ to $\\bar{x}_i$:\n    From $\\tau_i = \\exp(x_i)$, we have $\\bar{x}_i = \\bar{\\tau}_i \\exp(x_i) = \\bar{\\tau}_i \\tau_i$.\n\nThe resulting vector $\\bar{x}$ is the desired adjoint action $A_x(\\eta)$.\n\n**4. Verification Procedures**\n- **Taylor Test:** The quality of the tangent-linear model is checked by verifying that the model is first-order accurate. The Taylor remainder metric $R(\\alpha) = \\|H(x+\\alpha\\delta) - H(x) - \\alpha DH_x(\\delta)\\|_2 / \\alpha$ should be $\\mathcal{O}(\\alpha)$. This implies that a log-log plot of $R(\\alpha)$ versus $\\alpha$ will have a slope of $1$. We estimate this slope via linear least-squares regression. A slope in the range $[0.9, 1.1]$ indicates a pass.\n- **Adjoint Test:** The correctness of the adjoint model is verified using the fundamental inner product identity $\\langle DH_x(\\delta), \\eta \\rangle = \\langle \\delta, (DH_x)^\\top\\eta \\rangle$. The relative discrepancy between the two sides of this identity is computed. A value below $10^{-10}$ indicates a pass, confirming that the adjoint code is a true transpose of the tangent-linear code. The faulty adjoint, which omits the solar term, is expected to fail this test.", "answer": "```python\nimport numpy as np\n\nclass RadiativeTransferModel:\n    \"\"\"\n    Implements a discrete-layer radiative transfer model and its derivatives.\n    \"\"\"\n    def __init__(self, N, K, w, s, b):\n        self.N = N\n        self.K = K\n        self.w = w\n        self.s = s\n        self.b = b\n\n        # Pre-allocate for intermediate variables\n        self.tau = np.zeros(N)\n        self.Delta_tau = np.zeros((K, N))\n        self.Z = np.zeros((K, N))\n        self.T = np.zeros((K, N + 1))\n\n    def forward(self, x):\n        \"\"\"Computes the forward model H(x) and stores intermediate values.\"\"\"\n        self.tau = np.exp(x)\n        self.Delta_tau = self.w * self.tau\n        self.Z = np.exp(-self.Delta_tau)\n        \n        self.T[:, 0] = 1.0\n        for i in range(self.N):\n            self.T[:, i + 1] = self.T[:, i] * self.Z[:, i]\n            \n        y = self.s * self.T[:, self.N]\n        for i in range(self.N):\n            # L_ki * T_{k,i-1} = (1 - Z_ki) * T_{k,i-1} = T_{k,i-1} - T_{k,i}\n            y += self.b[:, i] * (self.T[:, i] - self.T[:, i + 1])\n        \n        return y\n\n    def tangent_linear(self, x, delta_x):\n        \"\"\"Computes the directional derivative DH_x(delta_x).\"\"\"\n        # Ensure forward pass has been run for base state x\n        self.forward(x)\n\n        delta_tau = self.tau * delta_x\n        delta_Delta_tau = self.w * delta_tau\n        delta_Z = -self.Z * delta_Delta_tau\n        \n        delta_T = np.zeros((self.K, self.N + 1))\n        # delta_T[:, 0] is always 0\n        for i in range(self.N):\n            delta_T[:, i + 1] = delta_T[:, i] * self.Z[:, i] + self.T[:, i] * delta_Z[:, i]\n\n        delta_y = self.s * delta_T[:, self.N]\n        for i in range(self.N):\n            delta_y += self.b[:, i] * (delta_T[:, i] - delta_T[:, i + 1])\n            \n        return delta_y\n\n    def adjoint(self, x, eta, faulty=False):\n        \"\"\"Computes the adjoint action (DH_x)^T * eta.\"\"\"\n        # Ensure forward pass has been run for base state x\n        self.forward(x)\n        \n        bar_tau = np.zeros(self.N)\n\n        for k in range(self.K):\n            bar_T_k = np.zeros(self.N + 1)\n            eta_k = eta[k]\n\n            # Contributions from y_k = s_k*T_kN + sum_i b_ki*(T_{k,i-1}-T_{k,i})\n            if not faulty:\n                bar_T_k[self.N] += eta_k * self.s[k]\n            \n            for i in range(self.N): # 1-based index i=1,...,N\n                i_idx = i\n                bar_T_k[i_idx] += eta_k * self.b[k, i_idx]\n                bar_T_k[i_idx + 1] -= eta_k * self.b[k, i_idx]\n\n            bar_Delta_tau_k = np.zeros(self.N)\n            # Propagate backwards from T\n            for i in range(self.N, 0, -1): # 1-based index i=N,...,1\n                i_idx = i - 1\n                \n                # From T_k,i = T_{k,i-1} * Z_{k,i}\n                bar_Z_ki = bar_T_k[i] * self.T[k, i_idx]\n                bar_T_k[i_idx] += bar_T_k[i] * self.Z[k, i_idx]\n\n                # From Z_k,i = exp(-Delta_tau_{k,i})\n                bar_Delta_tau_k[i_idx] += bar_Z_ki * (-self.Z[k, i_idx])\n\n            # Accumulate contributions to bar_tau for channel k\n            # From Delta_tau_ki = w_ki * tau_i\n            bar_tau += bar_Delta_tau_k * self.w[k, :]\n\n        # Final propagation from tau to x\n        # From tau_i = exp(x_i)\n        bar_x = bar_tau * self.tau\n        \n        return bar_x\n\ndef solve():\n    alpha_steps = np.array([1e-1, 5e-2, 2.5e-2, 1.25e-2, 6.25e-3])\n\n    # --- Test Case Data Generation ---\n    test_cases_params = []\n    \n    # Case 1\n    N, K = 6, 4\n    w = np.array([[0.2 + 0.05*(k+1) + 0.03*(i+1) for i in range(N)] for k in range(K)])\n    s = np.array([1.0 + 0.2*(k+1) for k in range(K)])\n    b = np.array([[0.1 + 0.05*(i+1) + 0.03*(k+1) for i in range(N)] for k in range(K)])\n    x = np.array([-0.8, -0.3, 0.2, 0.7, -0.5, 0.1])\n    delta = np.array([0.1, -0.2, 0.05, -0.1, 0.2, -0.05])\n    eta = np.array([0.3, -0.5, 0.7, -0.2])\n    test_cases_params.append({'N':N, 'K':K, 'w':w, 's':s, 'b':b, 'x':x, 'delta':delta, 'eta':eta, 'faulty':False})\n\n    # Case 2\n    N, K = 6, 4\n    w = np.array([[0.05 + 0.02*(k+1) + 0.01*(i+1) for i in range(N)] for k in range(K)])\n    s = np.array([0.2 + 0.1*(k+1) for k in range(K)])\n    b = np.array([[0.02 + 0.03*(i+1) + 0.01*(k+1) for i in range(N)] for k in range(K)])\n    x = np.array([-5.5, -4.8, -4.2, -3.9, -3.5, -3.2])\n    delta = np.array([0.02, -0.01, 0.03, -0.02, 0.01, -0.03])\n    eta = np.array([1.0, -0.8, 0.6, -0.4])\n    test_cases_params.append({'N':N, 'K':K, 'w':w, 's':s, 'b':b, 'x':x, 'delta':delta, 'eta':eta, 'faulty':False})\n\n    # Case 3\n    N, K = 6, 4\n    w = np.array([[0.7 + 0.1*(k+1) + 0.2*(i+1) for i in range(N)] for k in range(K)])\n    s = np.array([1.0 + 0.3*(k+1) for k in range(K)])\n    b = np.array([[0.4 + 0.1*(i+1) + 0.05*(k+1) for i in range(N)] for k in range(K)])\n    x = np.array([3.0, 3.5, 4.0, 4.5, 5.0, 3.8])\n    delta = np.array([-0.05, 0.1, -0.08, 0.06, -0.04, 0.02])\n    eta = np.array([-0.5, 0.4, -0.3, 0.2])\n    test_cases_params.append({'N':N, 'K':K, 'w':w, 's':s, 'b':b, 'x':x, 'delta':delta, 'eta':eta, 'faulty':False})\n\n    # Case 4 (same as 1, but faulty adjoint)\n    params1_copy = test_cases_params[0].copy()\n    params1_copy['faulty'] = True\n    test_cases_params.append(params1_copy)\n    \n    # --- Main Loop ---\n    results = []\n    for params in test_cases_params:\n        p = params\n        model = RadiativeTransferModel(p['N'], p['K'], p['w'], p['s'], p['b'])\n        \n        # --- Taylor Test ---\n        y0 = model.forward(p['x'])\n        dy = model.tangent_linear(p['x'], p['delta'])\n        \n        remainders = []\n        for alpha in alpha_steps:\n            y_alpha = model.forward(p['x'] + alpha * p['delta'])\n            remainder_norm = np.linalg.norm(y_alpha - y0 - alpha * dy)\n            remainders.append(remainder_norm / alpha)\n        \n        log_alphas = np.log10(alpha_steps)\n        log_remainders = np.log10(remainders)\n        slope = np.polyfit(log_alphas, log_remainders, 1)[0]\n        taylor_pass = 0.9 = slope = 1.1\n\n        # --- Adjoint Test ---\n        adj = model.adjoint(p['x'], p['eta'], faulty=p['faulty'])\n        \n        ip1 = np.dot(dy, p['eta'])\n        ip2 = np.dot(p['delta'], adj)\n        \n        adjoint_err = np.abs(ip1 - ip2) / max(1e-16, np.abs(ip1) + np.abs(ip2))\n        adjoint_pass = adjoint_err  1e-10\n        \n        results.append([slope, taylor_pass, adjoint_err, adjoint_pass])\n\n    # --- Formatting Output ---\n    case_results_str = [f'[{r[0]},{r[1]},{r[2]},{r[3]}]' for r in results]\n    final_str = f\"[{','.join(case_results_str)}]\"\n    print(final_str)\n\nsolve()\n```", "id": "3398784"}, {"introduction": "Linearization is an approximation, and understanding its limits is key to building robust assimilation systems. This practice moves beyond the first-order approximation to explore the impact of the second-order terms that are ignored by common methods like the Gauss-Newton algorithm. By deriving and computing a quantitative measure of nonlinearity, you will gain a concrete understanding of when a simple linear approximation is sufficient and when a more sophisticated approach, like a full Newton method, becomes necessary. [@problem_id:3398774]", "problem": "Consider a nonlinear forecast model and a nonlinear observation operator composed as follows. The forecast model is a map $M:\\mathbb{R}^2 \\to \\mathbb{R}^2$ given by $M(x) = \\begin{bmatrix} M_1(x) \\\\ M_2(x) \\end{bmatrix}$ with $M_1(x) = x_1$ and $M_2(x) = a \\tanh(x_2)$, where $x = (x_1,x_2)^{\\top}$ and $a \\in \\mathbb{R}$ is a positive scalar parameter controlling the nonlinearity. The observation operator is $H:\\mathbb{R}^2 \\to \\mathbb{R}$ given by $H(z) = z_1^2 + z_2$. Define the composed forward map $h:\\mathbb{R}^2 \\to \\mathbb{R}$ by $h(x) = (H \\circ M)(x)$. Suppose we are given a scalar observation $y \\in \\mathbb{R}$ and we consider the nonlinear least-squares objective $f(x) = \\tfrac{1}{2}(h(x) - y)^2$.\n\nYour tasks are:\n\n- Starting from the second-order Taylor expansion and the chain rule for derivatives, use only foundational principles to derive an explicit expression for the second-order correction term $\\tfrac{1}{2} D^2(H \\circ M)_x[\\delta,\\delta]$ in the direction $\\delta \\in \\mathbb{R}^2$. Your derivation must be based solely on the definitions of the Jacobian and Hessian, the chain rule for compositions, and the identification of $D^2(H \\circ M)_x$ in terms of the Jacobian of $M$, the gradient and Hessian of $H$, and the Hessians of the components of $M$.\n\n- Using the derived expression, implement a program that, for each specified test case below, performs the following computations at a given linearization point $x$:\n  1. Compute the residual $r = h(x) - y$.\n  2. Compute the gradient of the composed forward map $J = \\nabla h(x) \\in \\mathbb{R}^2$ using only the Jacobian of $M$ and the gradient of $H$ via the chain rule.\n  3. Compute the Gauss–Newton search direction $\\delta_{\\mathrm{GN}}$ as the solution of $(J^{\\top}J)\\delta = -J^{\\top} r$, with Tikhonov regularization by adding a small $\\ell_2$ penalty $\\lambda I$ to $J^{\\top}J$ if needed to ensure invertibility. Use $\\lambda = 10^{-12}$.\n  4. Compute the second-order correction term $c = \\tfrac{1}{2} D^2(H \\circ M)_x[\\delta_{\\mathrm{GN}},\\delta_{\\mathrm{GN}}]$.\n  5. Compute the first-order predicted change in the observation along $\\delta_{\\mathrm{GN}}$, namely $p = J \\cdot \\delta_{\\mathrm{GN}}$.\n  6. Compute the nonlinearity ratio $\\rho = |c|/\\max(\\varepsilon, |p|)$ with $\\varepsilon = 10^{-12}$.\n  7. Classify a Gauss–Newton linearization as unreliable, and a full Newton method as preferable, when $\\rho \\ge \\tau$, where the threshold is $\\tau = 0.5$. In that case, set a failure-versus-success indicator $\\kappa = 1$; otherwise set $\\kappa = 0$.\n\n- Your program must produce, for all test cases, a single line of output consisting of a flat list that alternates the nonlinearity ratio and the indicator, i.e., $[\\rho_1,\\kappa_1,\\rho_2,\\kappa_2,\\dots]$. All angles are in radians. There are no physical units in this problem.\n\nUse the following test suite. In all cases, the composed forward map and all derivatives are evaluated at the given $x$ for the specified $a$, and the observation $y$ is as stated:\n\n- Test case A (near-linear regime): $a = 3.0$, $x = (0.9,\\,0.1)^{\\top}$, $y = h(x_{\\star})$ with $x_{\\star} = (1.0,\\,0.1)^{\\top}$.\n- Test case B (highly nonlinear, positive saturation): $a = 5.0$, $x = (0.01,\\,3.0)^{\\top}$, $y = 2.0$.\n- Test case C (highly nonlinear, negative saturation): $a = 5.0$, $x = (0.01,\\,-3.0)^{\\top}$, $y = 2.0$.\n- Test case D (exact solution, zero residual): $a = 3.0$, $x = (0.5,\\,0.1)^{\\top}$, $y = h(x)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the exact order $[\\rho_{\\mathrm{A}},\\kappa_{\\mathrm{A}},\\rho_{\\mathrm{B}},\\kappa_{\\mathrm{B}},\\rho_{\\mathrm{C}},\\kappa_{\\mathrm{C}},\\rho_{\\mathrm{D}},\\kappa_{\\mathrm{D}}]$. Each $\\rho$ must be a floating point number and each $\\kappa$ must be an integer equal to either $0$ or $1$.", "solution": "The user wants to analyze the nonlinearity of a composite model used in an inverse problem, specifically to determine when a Gauss-Newton linearization is unreliable.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\n- **Forecast Model**: $M:\\mathbb{R}^2 \\to \\mathbb{R}^2$, $M(x) = \\begin{bmatrix} M_1(x) \\\\ M_2(x) \\end{bmatrix} = \\begin{bmatrix} x_1 \\\\ a \\tanh(x_2) \\end{bmatrix}$ for $x = (x_1,x_2)^{\\top}$ and $a > 0$.\n- **Observation Operator**: $H:\\mathbb{R}^2 \\to \\mathbb{R}$, $H(z) = z_1^2 + z_2$.\n- **Composed Forward Map**: $h:\\mathbb{R}^2 \\to \\mathbb{R}$, $h(x) = (H \\circ M)(x)$.\n- **Objective Function**: $f(x) = \\tfrac{1}{2}(h(x) - y)^2$, for an observation $y \\in \\mathbb{R}$.\n- **Second-Order Correction Term**: $c = \\tfrac{1}{2} D^2(H \\circ M)_x[\\delta_{\\mathrm{GN}},\\delta_{\\mathrm{GN}}]$.\n- **Residual**: $r = h(x) - y$.\n- **Gradient/Jacobian**: $J = \\nabla h(x) \\in \\mathbb{R}^2$. The problem uses this notation but contextually implies $J$ is the $1 \\times 2$ Jacobian matrix of $h$.\n- **Gauss-Newton Search Direction**: $\\delta_{\\mathrm{GN}}$ is the solution to $(J^{\\top}J + \\lambda I)\\delta = -J^{\\top} r$.\n- **Regularization Parameter**: $\\lambda = 10^{-12}$.\n- **First-Order Predicted Change**: $p = J \\cdot \\delta_{\\mathrm{GN}}$.\n- **Nonlinearity Ratio**: $\\rho = |c|/\\max(\\varepsilon, |p|)$.\n- **Denominator Floor**: $\\varepsilon = 10^{-12}$.\n- **Reliability Threshold**: $\\tau = 0.5$.\n- **Indicator Variable**: $\\kappa = 1$ if $\\rho \\ge \\tau$, $\\kappa = 0$ otherwise.\n\n- **Test Cases**:\n  - Test case A: $a = 3.0$, $x = (0.9,\\,0.1)^{\\top}$, $y = h(x_{\\star})$ with $x_{\\star} = (1.0,\\,0.1)^{\\top}$.\n  - Test case B: $a = 5.0$, $x = (0.01,\\,3.0)^{\\top}$, $y = 2.0$.\n  - Test case C: $a = 5.0$, $x = (0.01,\\,-3.0)^{\\top}$, $y = 2.0$.\n  - Test case D: $a = 3.0$, $x = (0.5,\\,0.1)^{\\top}$, $y = h(x)$.\n\n**Step 2: Validate Using Extracted Givens**\n\n- **Scientifically Grounded**: The problem is an application of standard concepts from multivariable calculus (Taylor series, chain rule, Jacobians, Hessians) and numerical optimization (Gauss-Newton method). These are fundamental tools in the field of inverse problems and data assimilation. The problem is scientifically and mathematically sound.\n- **Well-Posed**: All functions ($M$, $H$, $h$) are well-defined and infinitely differentiable. The computational tasks are specified algorithmically. The use of Tikhonov regularization with a small positive parameter $\\lambda$ ensures that the linear system for the Gauss-Newton direction $\\delta_{\\mathrm{GN}}$ is always uniquely solvable, even if $J^\\top J$ is singular. The problem is therefore well-posed.\n- **Objective**: The problem is formulated using precise mathematical language and definitions, with no subjective or ambiguous statements.\n\nThe problem is self-contained, consistent, and meets all criteria for validity.\n\n**Step 3: Verdict and Action**\n\nThe problem is valid. I will proceed with the derivation and solution.\n\n### Derivation of the Second-Order Correction Term\n\nThe core of the problem is to quantify the nonlinearity of the forward map $h(x) = (H \\circ M)(x)$. This is done by comparing the first- and second-order terms of its Taylor expansion around a point $x$ in a specific direction $\\delta$. The Taylor expansion of $h(x+\\delta)$ is:\n$$ h(x+\\delta) = h(x) + Dh(x)[\\delta] + \\frac{1}{2} D^2h(x)[\\delta, \\delta] + O(\\|\\delta\\|^3) $$\nThe term $p = Dh(x)[\\delta_{\\mathrm{GN}}]$ is the first-order predicted change in the observation. The term $c = \\frac{1}{2} D^2h(x)[\\delta_{\\mathrm{GN}}, \\delta_{\\mathrm{GN}}]$ is the second-order correction. The problem asks for an explicit derivation of this term.\n\nWe use the chain rule for second derivatives (Faà di Bruno's formula). For a composition $h(x) = H(M(x))$, the second-order differential is given by:\n$$ D^2h_x[\\delta, \\delta] = D^2H_{M(x)}[DM_x[\\delta], DM_x[\\delta]] + DH_{M(x)}[D^2M_x[\\delta, \\delta]] $$\nIn matrix notation, where $\\mathbf{M}(x)$ is the Jacobian of $M$, $\\mathbf{H}_H(z)$ is the Hessian of $H$, and $\\mathbf{H}_{M_i}(x)$ is the Hessian of the $i$-th component of $M$, the Hessian of $h$, which we denote $\\mathbf{H}_h(x)$, is:\n$$ \\mathbf{H}_h(x) = \\mathbf{M}(x)^{\\top} \\mathbf{H}_H(M(x)) \\mathbf{M}(x) + \\sum_{i=1}^{k} \\left( \\nabla H(M(x)) \\right)_i \\mathbf{H}_{M_i}(x) $$\nwhere $k=2$ is the dimension of the intermediate space. The second-order term is then $c = \\frac{1}{2}\\delta_{\\mathrm{GN}}^{\\top} \\mathbf{H}_h(x) \\delta_{\\mathrm{GN}}$.\n\nWe now compute the required derivatives for the given functions:\n1.  **Function definitions**:\n    $M(x) = \\begin{bmatrix} x_1 \\\\ a \\tanh(x_2) \\end{bmatrix}$, $H(z) = z_1^2 + z_2$.\n\n2.  **Derivatives of $M$**:\n    - Jacobian of $M$: $\\mathbf{M}(x) = \\begin{bmatrix} \\frac{\\partial M_1}{\\partial x_1}  \\frac{\\partial M_1}{\\partial x_2} \\\\ \\frac{\\partial M_2}{\\partial x_1}  \\frac{\\partial M_2}{\\partial x_2} \\end{bmatrix} = \\begin{bmatrix} 1  0 \\\\ 0  a \\, \\mathrm{sech}^2(x_2) \\end{bmatrix}$.\n    - Hessian of $M_1(x) = x_1$: $\\mathbf{H}_{M_1}(x) = \\begin{bmatrix} 0  0 \\\\ 0  0 \\end{bmatrix}$.\n    - Hessian of $M_2(x) = a \\tanh(x_2)$:\n      $\\frac{\\partial^2 M_2}{\\partial x_1^2} = 0$, $\\frac{\\partial^2 M_2}{\\partial x_1 \\partial x_2} = 0$.\n      $\\frac{\\partial^2 M_2}{\\partial x_2^2} = \\frac{d}{dx_2} (a \\, \\mathrm{sech}^2(x_2)) = a \\cdot 2 \\mathrm{sech}(x_2) \\cdot (-\\mathrm{sech}(x_2)\\tanh(x_2)) = -2a \\, \\mathrm{sech}^2(x_2) \\tanh(x_2)$.\n      So, $\\mathbf{H}_{M_2}(x) = \\begin{bmatrix} 0  0 \\\\ 0  -2a \\, \\mathrm{sech}^2(x_2) \\tanh(x_2) \\end{bmatrix}$.\n\n3.  **Derivatives of $H$**:\n    Let $z = M(x) = (x_1, a\\tanh(x_2))^{\\top}$.\n    - Gradient of $H$: $\\nabla H(z) = \\begin{bmatrix} \\frac{\\partial H}{\\partial z_1} \\\\ \\frac{\\partial H}{\\partial z_2} \\end{bmatrix} = \\begin{bmatrix} 2z_1 \\\\ 1 \\end{bmatrix}$. Evaluated at $z=M(x)$, this becomes $\\nabla H(M(x)) = \\begin{bmatrix} 2x_1 \\\\ 1 \\end{bmatrix}$.\n    - Hessian of $H$: $\\mathbf{H}_H(z) = \\begin{bmatrix} \\frac{\\partial^2 H}{\\partial z_1^2}  \\frac{\\partial^2 H}{\\partial z_1 \\partial z_2} \\\\ \\frac{\\partial^2 H}{\\partial z_2 \\partial z_1}  \\frac{\\partial^2 H}{\\partial z_2^2} \\end{bmatrix} = \\begin{bmatrix} 2  0 \\\\ 0  0 \\end{bmatrix}$.\n\n4.  **Assemble the Hessian of $h$**:\n    We substitute these components into the formula for $\\mathbf{H}_h(x)$:\n    $$ \\mathbf{H}_h(x) = \\underbrace{\\begin{bmatrix} 1  0 \\\\ 0  a\\,\\mathrm{sech}^2(x_2) \\end{bmatrix} \\begin{bmatrix} 2  0 \\\\ 0  0 \\end{bmatrix} \\begin{bmatrix} 1  0 \\\\ 0  a\\,\\mathrm{sech}^2(x_2) \\end{bmatrix}}_{\\text{Part 1}} + \\underbrace{(2x_1)\\mathbf{H}_{M_1}(x) + (1)\\mathbf{H}_{M_2}(x)}_{\\text{Part 2}} $$\n    - Part 1: $\\begin{bmatrix} 1  0 \\\\ 0  a\\,\\mathrm{sech}^2(x_2) \\end{bmatrix} \\begin{bmatrix} 2  0 \\\\ 0  0 \\end{bmatrix} = \\begin{bmatrix} 2  0 \\\\ 0  0 \\end{bmatrix}$.\n    - Part 2: $(2x_1)\\begin{bmatrix} 0  0 \\\\ 0  0 \\end{bmatrix} + (1)\\begin{bmatrix} 0  0 \\\\ 0  -2a\\,\\mathrm{sech}^2(x_2)\\tanh(x_2) \\end{bmatrix} = \\begin{bmatrix} 0  0 \\\\ 0  -2a\\,\\mathrm{sech}^2(x_2)\\tanh(x_2) \\end{bmatrix}$.\n    - Combining the parts:\n    $$ \\mathbf{H}_h(x) = \\begin{bmatrix} 2  0 \\\\ 0  0 \\end{bmatrix} + \\begin{bmatrix} 0  0 \\\\ 0  -2a\\,\\mathrm{sech}^2(x_2)\\tanh(x_2) \\end{bmatrix} = \\begin{bmatrix} 2  0 \\\\ 0  -2a\\,\\mathrm{sech}^2(x_2)\\tanh(x_2) \\end{bmatrix} $$\n\n5.  **Expression for the correction term $c$**:\n    Let $\\delta_{\\mathrm{GN}} = (\\delta_1, \\delta_2)^{\\top}$. The second-order correction term is:\n    $$ c = \\frac{1}{2} \\delta_{\\mathrm{GN}}^{\\top} \\mathbf{H}_h(x) \\delta_{\\mathrm{GN}} = \\frac{1}{2} \\begin{bmatrix} \\delta_1  \\delta_2 \\end{bmatrix} \\begin{bmatrix} 2  0 \\\\ 0  -2a\\,\\mathrm{sech}^2(x_2)\\tanh(x_2) \\end{bmatrix} \\begin{bmatrix} \\delta_1 \\\\ \\delta_2 \\end{bmatrix} $$\n    $$ c = \\frac{1}{2} (2\\delta_1^2 - 2a\\,\\delta_2^2\\,\\mathrm{sech}^2(x_2)\\tanh(x_2)) $$\n    $$ c = \\delta_1^2 - a\\,\\delta_2^2\\,\\mathrm{sech}^2(x_2)\\tanh(x_2) $$\n    This is the explicit expression for the second-order correction term. The implementation will follow the algorithmic steps specified in the problem statement using this derived formula.\n\n### Algorithmic Steps and Implementation\nThe procedure for each test case is as follows. Note that the problem's notation $J=\\nabla h(x)$ is interpreted as $J$ being the $1 \\times 2$ Jacobian matrix (a row vector), consistent with the standard Gauss-Newton formulation for a scalar observation function.\n\n1.  **Compute $h(x)$ and residual $r$**:\n    $h(x) = x_1^2 + a \\tanh(x_2)$.\n    $r = h(x) - y$.\n\n2.  **Compute Jacobian $J$**:\n    By direct differentiation or the chain rule: $J = \\nabla h(x)^{\\top} = [2x_1, a \\, \\mathrm{sech}^2(x_2)]$.\n\n3.  **Compute Gauss-Newton direction $\\delta_{\\mathrm{GN}}$**:\n    Solve the $2 \\times 2$ linear system $(J^{\\top}J + \\lambda I)\\delta = -J^{\\top} r$ for $\\delta = \\delta_{\\mathrm{GN}}$.\n\n4.  **Compute second-order correction term $c$**:\n    Using the derived expression: $c = \\delta_{\\mathrm{GN},1}^2 - a\\,\\delta_{\\mathrm{GN},2}^2\\,\\mathrm{sech}^2(x_2)\\tanh(x_2)$.\n\n5.  **Compute first-order predicted change $p$**:\n    $p = J \\cdot \\delta_{\\mathrm{GN}} = J_1 \\delta_{\\mathrm{GN},1} + J_2 \\delta_{\\mathrm{GN},2}$.\n\n6.  **Compute nonlinearity ratio $\\rho$**:\n    $\\rho = |c|/\\max(\\varepsilon, |p|)$.\n\n7.  **Compute indicator $\\kappa$**:\n    $\\kappa = 1$ if $\\rho \\ge 0.5$, otherwise $\\kappa = 0$.\n\nThe following Python code implements this algorithm for all specified test cases.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by analyzing the nonlinearity of a composite model\n    for several test cases.\n    \"\"\"\n\n    def compute_rho_kappa(a, x_val, y_val, lambda_reg=1e-12, epsilon=1e-12, tau=0.5):\n        \"\"\"\n        Performs the required computations for a single test case.\n\n        Args:\n            a (float): The scalar parameter for the forecast model.\n            x_val (np.ndarray): The linearization point, shape (2,).\n            y_val (float): The scalar observation.\n            lambda_reg (float): Tikhonov regularization parameter.\n            epsilon (float): Small constant to avoid division by zero.\n            tau (float): Threshold for classifying nonlinearity.\n\n        Returns:\n            tuple: A tuple containing the nonlinearity ratio (rho) and the\n                   indicator (kappa).\n        \"\"\"\n        x1, x2 = x_val[0], x_val[1]\n\n        # 1. Compute residual r = h(x) - y\n        h_x = x1**2 + a * np.tanh(x2)\n        r = h_x - y_val\n\n        # 2. Compute the Jacobian J of h(x)\n        # J is a 1x2 row vector (matrix)\n        sech2_x2 = (1.0 / np.cosh(x2))**2\n        J = np.array([[2 * x1, a * sech2_x2]])\n\n        # 3. Compute the Gauss-Newton search direction delta_GN\n        # Solve (J^T J + lambda*I) delta = -J^T r\n        # J is (1,2), J.T is (2,1). J.T @ J is (2,2).\n        J_T_J = J.T @ J\n        A = J_T_J + lambda_reg * np.eye(2)\n        # J.T is (2,1), r is a scalar. rhs is (2,1).\n        rhs = -J.T * r\n        # delta_gn is a column vector (2,1)\n        delta_gn = np.linalg.solve(A, rhs)\n        delta_1, delta_2 = delta_gn[0, 0], delta_gn[1, 0]\n\n        # 4. Compute the second-order correction term c\n        # c = delta_1^2 - a * delta_2^2 * sech^2(x2) * tanh(x2)\n        tanh_x2 = np.tanh(x2)\n        c = delta_1**2 - a * (delta_2**2) * sech2_x2 * tanh_x2\n\n        # 5. Compute the first-order predicted change p\n        # p = J . delta_GN\n        # J is (1,2), delta_gn (2,1), so p is (1,1) matrix\n        p = (J @ delta_gn)[0, 0]\n\n        # 6. Compute the nonlinearity ratio rho\n        rho = np.abs(c) / np.maximum(epsilon, np.abs(p))\n\n        # 7. Classify reliability and compute indicator kappa\n        kappa = 1 if rho >= tau else 0\n\n        return rho, kappa\n\n    # Define test cases\n    # For h(x_star), the parameter 'a' must match the test case 'a'\n    h_func = lambda a_val, x_star_val: x_star_val[0]**2 + a_val * np.tanh(x_star_val[1])\n\n    test_cases = [\n        # Case A: Near-linear regime\n        {\n            \"a\": 3.0,\n            \"x\": np.array([0.9, 0.1]),\n            \"y\": h_func(3.0, np.array([1.0, 0.1]))\n        },\n        # Case B: Highly nonlinear, positive saturation\n        {\n            \"a\": 5.0,\n            \"x\": np.array([0.01, 3.0]),\n            \"y\": 2.0\n        },\n        # Case C: Highly nonlinear, negative saturation\n        {\n            \"a\": 5.0,\n            \"x\": np.array([0.01, -3.0]),\n            \"y\": 2.0\n        },\n        # Case D: Exact solution, zero residual\n        {\n            \"a\": 3.0,\n            \"x\": np.array([0.5, 0.1]),\n            \"y\": h_func(3.0, np.array([0.5, 0.1]))\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        rho, kappa = compute_rho_kappa(case[\"a\"], case[\"x\"], case[\"y\"])\n        results.extend([rho, kappa])\n\n    # Format and print the final output\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3398774"}, {"introduction": "When the linear approximation is poor, how can we still make progress in the optimization? This advanced practice introduces the trust-region method, a powerful framework for managing linearization error. You will derive a 'safe' step size based on a bound on the model's nonlinearity and design a strategy for accepting or rejecting steps, ensuring robust convergence even in highly nonlinear regimes. This exercise connects theoretical error bounds to practical algorithmic design, a hallmark of modern numerical optimization. [@problem_id:3398775]", "problem": "Consider a Four-Dimensional Variational (4D-Var) incremental formulation where the forecast model is a mapping $M:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}^{p}$ that is twice Fréchet differentiable, and the observation operator is linear $\\mathcal{H}:\\mathbb{R}^{p}\\rightarrow\\mathbb{R}^{q}$. At a linearization point $x\\in\\mathbb{R}^{n}$, define the residual $r_{0}\\in\\mathbb{R}^{q}$ by $r_{0}=\\mathcal{H}M(x)-y$ for given observations $y\\in\\mathbb{R}^{q}$, and define the linearized observation mapping $A\\in\\mathbb{R}^{q\\times n}$ by $A=\\mathcal{H}\\,D M_{x}$, where $D M_{x}$ denotes the Fréchet derivative of $M$ at $x$. The incremental least-squares model for the observation misfit is then the quadratic functional $m(\\delta)=\\tfrac{1}{2}\\|r_{0}+A\\,\\delta\\|_{2}^{2}$ for increments $\\delta\\in\\mathbb{R}^{n}$.\n\nYou are tasked to design a trust-region strategy that selects increments $\\delta$ satisfying the linearization error safeguard\n$$\n\\|M(x+\\delta)-M(x)-D M_{x}\\,\\delta\\|_{2}\\le \\varepsilon,\n$$\nfor a prescribed tolerance $\\varepsilon0$. The only information available about the local nonlinearity of $M$ is a local uniform bound on the operator norm of the second derivative within a ball $\\mathcal{B}(x,R)=\\{z\\in\\mathbb{R}^{n}:\\|z-x\\|_{2}\\le R\\}$, namely\n$$\n\\sup_{\\xi\\in\\mathcal{B}(x,R)}\\|D^{2}M(\\xi)\\|_{\\mathrm{op}}\\le L_{2},\n$$\nwhere $L_{2}0$ is given and $\\|\\cdot\\|_{\\mathrm{op}}$ denotes the induced operator norm on bilinear forms.\n\nYour program must, for each test case, perform the following tasks starting only from: (i) Taylor’s theorem with integral remainder for Fréchet differentiable mappings, (ii) the definition of Lipschitz continuity and operator norms, and (iii) standard convex trust-region optimality conditions for quadratic models.\n\n1. From the facts above, determine a trust-region radius $\\Delta0$ such that every step $\\delta$ with $\\|\\delta\\|_{2}\\le \\Delta$ is guaranteed to satisfy the linearization error safeguard. Express $\\Delta$ in terms of $L_{2}$ and $\\varepsilon$ without using any unproven shortcut formulas.\n\n2. Compute the trust-region step $\\delta_{\\star}$ that minimizes the quadratic model $m(\\delta)=\\tfrac{1}{2}\\|r_{0}+A\\,\\delta\\|_{2}^{2}$ subject to the Euclidean norm constraint $\\|\\delta\\|_{2}\\le \\Delta$. Use only well-founded optimality conditions for convex quadratic trust-region problems.\n\n3. Compute the predicted reduction $\\mathrm{pred}=\\tfrac{1}{2}\\|r_{0}\\|_{2}^{2}-\\tfrac{1}{2}\\|r_{0}+A\\,\\delta_{\\star}\\|_{2}^{2}$.\n\n4. Using only the bound $L_{2}$ and the operator norm of the observation operator $\\|\\mathcal{H}\\|_{\\mathrm{op}}$ (denote this scalar by $H_{\\mathrm{op}}0$), derive a rigorous bound on the discrepancy between the actual observation-space reduction and the predicted reduction, and then produce a conservative acceptance ratio lower bound $\\rho_{\\mathrm{low}}\\in[0,\\infty)$ that depends only on computable quantities. The acceptance ratio lower bound must be constructed so that if $\\rho_{\\mathrm{low}}\\ge \\eta$ for a given threshold $\\eta\\in(0,1)$, then acceptance of the step is guaranteed to be justified with respect to the observation misfit decrease in the presence of model linearization error consistent with the given bounds.\n\n5. Return, for each test case, a triple consisting of the trust-region radius $\\Delta$, the acceptance ratio lower bound $\\rho_{\\mathrm{low}}$, and a boolean acceptance decision $\\rho_{\\mathrm{low}}\\ge \\eta$.\n\nYour solution must treat all vectors and matrices with Euclidean norms and induced operator norms, and must not rely on any unproven or context-external identities. Angles are not involved. No physical units are involved. All scalars must be treated as real numbers. The observation operator norm $H_{\\mathrm{op}}$ is provided directly.\n\nTest Suite. Implement your program to compute results for the following three test cases. For each case, use the data exactly as specified.\n\n- Test case $1$:\n  - $A=\\begin{bmatrix}1.0  0.0\\\\ 0.0  2.0\\end{bmatrix}$,\n  - $r_{0}=\\begin{bmatrix}1.0\\\\ -1.0\\end{bmatrix}$,\n  - $L_{2}=4.0$,\n  - $\\varepsilon=0.05$,\n  - $H_{\\mathrm{op}}=1.5$,\n  - $\\eta=0.3$.\n\n- Test case $2$:\n  - $A=\\begin{bmatrix}2.0  0.5\\\\ 0.0  1.0\\end{bmatrix}$,\n  - $r_{0}=\\begin{bmatrix}2.0\\\\ 1.5\\end{bmatrix}$,\n  - $L_{2}=3.0$,\n  - $\\varepsilon=0.5$,\n  - $H_{\\mathrm{op}}=2.0$,\n  - $\\eta=0.5$.\n\n- Test case $3$:\n  - $A=\\begin{bmatrix}0.1  0.0\\\\ 0.0  0.1\\end{bmatrix}$,\n  - $r_{0}=\\begin{bmatrix}0.05\\\\ -0.02\\end{bmatrix}$,\n  - $L_{2}=100.0$,\n  - $\\varepsilon=0.0001$,\n  - $H_{\\mathrm{op}}=1.0$,\n  - $\\eta=0.1$.\n\nFinal Output Format. Your program should produce a single line of output containing a list of three lists, one per test case, each inner list containing three entries: the trust-region radius $\\Delta$ as a float rounded to six decimal places, the acceptance ratio lower bound $\\rho_{\\mathrm{low}}$ as a float rounded to six decimal places, and the boolean acceptance decision. For example, the output must have the format\n$[[\\Delta_{1},\\rho_{\\mathrm{low},1},\\mathrm{decision}_{1}],[\\Delta_{2},\\rho_{\\mathrm{low},2},\\mathrm{decision}_{2}],[\\Delta_{3},\\rho_{\\mathrm{low},3},\\mathrm{decision}_{3}]]$\nwith floats rounded to six decimal places.", "solution": "The problem requires the design and analysis of a trust-region strategy for an incremental 4D-Var formulation. The solution is derived from first principles as specified.\n\n### Step 1: Determination of the Trust-Region Radius $\\Delta$\n\nThe trust-region radius $\\Delta$ must be chosen to satisfy the linearization error safeguard:\n$$\n\\|M(x+\\delta)-M(x)-D M_{x}\\,\\delta\\|_{2}\\le \\varepsilon\n$$\nfor all increments $\\delta$ such that $\\|\\delta\\|_{2}\\le \\Delta$. We are given that the mapping $M:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}^{p}$ is twice Fréchet differentiable. By Taylor's theorem with integral remainder, the linearization error term can be written as:\n$$\nM(x+\\delta) - M(x) - D M_{x}\\,\\delta = \\int_{0}^{1}(1-t) D^{2}M(x+t\\delta)(\\delta, \\delta) \\, dt\n$$\nwhere $D^{2}M(\\xi)$ is the second Fréchet derivative (a bilinear form) of $M$ at a point $\\xi$. Taking the Euclidean norm of both sides and applying the triangle inequality for Bochner integrals, we have:\n$$\n\\|M(x+\\delta) - M(x) - D M_{x}\\,\\delta\\|_{2} = \\left\\| \\int_{0}^{1}(1-t) D^{2}M(x+t\\delta)(\\delta, \\delta) \\, dt \\right\\|_{2} \\le \\int_{0}^{1} \\|(1-t) D^{2}M(x+t\\delta)(\\delta, \\delta)\\|_{2} \\, dt\n$$\nSince $t \\in [0,1]$, $(1-t)$ is a non-negative scalar. Using the definition of the operator norm for a bilinear form, $\\|B(v,v)\\| \\le \\|B\\|_{\\mathrm{op}}\\|v\\|^2$, we get:\n$$\n\\int_{0}^{1} (1-t) \\|D^{2}M(x+t\\delta)(\\delta, \\delta)\\|_{2} \\, dt \\le \\int_{0}^{1} (1-t) \\|D^{2}M(x+t\\delta)\\|_{\\mathrm{op}}\\|\\delta\\|_{2}^{2} \\, dt\n$$\nThe problem provides a uniform bound on the operator norm of the second derivative, $\\sup_{\\xi\\in\\mathcal{B}(x,R)}\\|D^{2}M(\\xi)\\|_{\\mathrm{op}}\\le L_{2}$. If we restrict $\\delta$ such that $\\|\\delta\\|_{2} \\le R$, then for all $t\\in[0,1]$, the point $x+t\\delta$ lies within the ball $\\mathcal{B}(x,R)$, and we can use the bound $L_2$.\n$$\n\\|M(x+\\delta) - M(x) - D M_{x}\\,\\delta\\|_{2} \\le \\int_{0}^{1} (1-t) L_{2} \\|\\delta\\|_{2}^{2} \\, dt = L_{2} \\|\\delta\\|_{2}^{2} \\int_{0}^{1} (1-t) \\, dt\n$$\nThe integral evaluates to $\\int_{0}^{1} (1-t) \\, dt = [t - \\frac{t^2}{2}]_{0}^{1} = \\frac{1}{2}$. This yields the standard second-order Taylor error bound:\n$$\n\\|M(x+\\delta) - M(x) - D M_{x}\\,\\delta\\|_{2} \\le \\frac{1}{2} L_{2} \\|\\delta\\|_{2}^{2}\n$$\nTo satisfy the safeguard $\\|M(x+\\delta)-M(x)-D M_{x}\\,\\delta\\|_{2}\\le \\varepsilon$, it is sufficient to impose the condition:\n$$\n\\frac{1}{2} L_{2} \\|\\delta\\|_{2}^{2} \\le \\varepsilon \\implies \\|\\delta\\|_{2}^{2} \\le \\frac{2\\varepsilon}{L_{2}}\n$$\nThus, any $\\delta$ satisfying $\\|\\delta\\|_{2} \\le \\sqrt{2\\varepsilon/L_{2}}$ will meet the safeguard. We define the trust-region radius as this upper bound:\n$$\n\\Delta = \\sqrt{\\frac{2\\varepsilon}{L_{2}}}\n$$\n\n### Step 2: Computation of the Trust-Region Step $\\delta_{\\star}$\n\nThe trust-region step $\\delta_{\\star}$ is the solution to the convex quadratic minimization problem:\n$$\n\\min_{\\delta \\in \\mathbb{R}^n} \\quad m(\\delta) = \\frac{1}{2} \\|r_{0} + A\\delta\\|_{2}^{2} \\quad \\text{subject to} \\quad \\|\\delta\\|_{2} \\le \\Delta\n$$\nThe objective function can be expanded as $m(\\delta) = \\frac{1}{2} (r_0^T r_0 + 2r_0^T A\\delta + \\delta^T A^T A \\delta)$. Let $g = A^T r_0$ and $B = A^T A$. The problem becomes $\\min_{\\delta} g^T\\delta + \\frac{1}{2}\\delta^T B \\delta$ subject to $\\|\\delta\\|_2 \\le \\Delta$, ignoring the constant term. The Hessian $B=A^TA$ is symmetric and positive semi-definite, making the problem convex.\n\nThe Karush-Kuhn-Tucker (KKT) optimality conditions state that there exists a Lagrange multiplier $\\lambda \\ge 0$ such that the optimal solution $\\delta_{\\star}$ satisfies:\n1.  $(B + \\lambda I)\\delta_{\\star} = -g$\n2.  $\\|\\delta_{\\star}\\|_{2} \\le \\Delta$\n3.  $\\lambda \\ge 0$\n4.  $\\lambda(\\|\\delta_{\\star}\\|_{2} - \\Delta) = 0$\n\nTwo cases arise:\n-   **Interior solution**: First, we compute the unconstrained minimizer $\\delta_u$, which corresponds to $\\lambda=0$. This requires solving $B\\delta_u = -g$. If $B$ is invertible (which it is for the given test cases, as $A$ is full rank), $\\delta_u = -B^{-1}g$. If $\\|\\delta_u\\|_{2}  \\Delta$, this is the optimal solution, so $\\delta_{\\star} = \\delta_u$.\n-   **Boundary solution**: If $\\|\\delta_u\\|_{2} \\ge \\Delta$, the constraint must be active, meaning $\\|\\delta_{\\star}\\|_{2} = \\Delta$ and $\\lambda  0$. The solution is given by $\\delta_{\\star}(\\lambda) = -(B + \\lambda I)^{-1}g$. We must find the unique $\\lambda  0$ that satisfies the secular equation $\\|\\delta_{\\star}(\\lambda)\\|_{2} = \\Delta$. This is a nonlinear scalar equation that can be solved efficiently using a root-finding algorithm like the bisection method or Brent's method on the function $\\phi(\\lambda) = \\|-(B + \\lambda I)^{-1}g\\|_{2} - \\Delta$.\n\n### Step 3: Computation of the Predicted Reduction `pred`\n\nThe predicted reduction is the decrease in the quadratic model $m(\\delta)$ when moving from $\\delta=0$ to $\\delta=\\delta_{\\star}$:\n$$\n\\mathrm{pred} = m(0) - m(\\delta_{\\star}) = \\frac{1}{2}\\|r_{0}\\|_{2}^{2} - \\frac{1}{2}\\|r_{0} + A\\delta_{\\star}\\|_{2}^{2}\n$$\nSince $\\delta_{\\star}$ minimizes $m(\\delta)$ over the trust region, and $\\delta=0$ is in the trust region, it is guaranteed that $m(\\delta_{\\star}) \\le m(0)$, so $\\mathrm{pred} \\ge 0$.\n\n### Step 4: Derivation of the Acceptance Ratio Lower Bound $\\rho_{\\mathrm{low}}$\n\nThe actual reduction is the decrease in the true observation misfit:\n$$\n\\mathrm{ared} = \\frac{1}{2}\\|r_{0}\\|_{2}^{2} - \\frac{1}{2}\\|\\mathcal{H}M(x+\\delta_{\\star}) - y\\|_{2}^{2}\n$$\nThe acceptance ratio is $\\rho = \\mathrm{ared} / \\mathrm{pred}$. We seek a computable lower bound $\\rho_{\\mathrm{low}}$.\nLet's analyze the term for the new residual:\n$$\n\\mathcal{H}M(x+\\delta_{\\star}) - y = \\mathcal{H}M(x) - y + \\mathcal{H}(M(x+\\delta_{\\star}) - M(x)) = r_0 + \\mathcal{H}(D M_{x}\\delta_{\\star} + e_M(\\delta_{\\star}))\n$$\nwhere $e_M(\\delta_{\\star}) = M(x+\\delta_{\\star}) - M(x) - D M_{x}\\delta_{\\star}$ is the model linearization error. Using the linearity of $\\mathcal{H}$ and the definition $A = \\mathcal{H} D M_x$:\n$$\n\\mathcal{H}M(x+\\delta_{\\star}) - y = r_0 + A\\delta_{\\star} + \\mathcal{H}e_M(\\delta_{\\star})\n$$\nThe discrepancy between the actual and predicted reductions is:\n$$\n\\mathrm{ared} - \\mathrm{pred} = \\left(\\frac{1}{2}\\|r_{0}\\|_{2}^{2} - \\frac{1}{2}\\|\\dots\\|_{2}^{2}\\right) - \\left(\\frac{1}{2}\\|r_{0}\\|_{2}^{2} - \\frac{1}{2}\\|r_{0} + A\\delta_{\\star}\\|_{2}^{2}\\right) = \\frac{1}{2}\\|r_{0} + A\\delta_{\\star}\\|_{2}^{2} - \\frac{1}{2}\\|r_{0} + A\\delta_{\\star} + \\mathcal{H}e_M(\\delta_{\\star})\\|_{2}^{2}\n$$\nLet $u = r_0 + A\\delta_{\\star}$ and $v = \\mathcal{H}e_M(\\delta_{\\star})$. The difference is $\\frac{1}{2}(\\|u\\|_{2}^{2} - \\|u+v\\|_{2}^{2}) = -u^T v - \\frac{1}{2}\\|v\\|_{2}^{2}$.\nTo find a lower bound on `ared`, we need an upper bound on the absolute value of this difference:\n$$\n|\\mathrm{ared} - \\mathrm{pred}| = |-u^T v - \\frac{1}{2}\\|v\\|_{2}^{2}| \\le |u^T v| + \\frac{1}{2}\\|v\\|_{2}^{2} \\le \\|u\\|_{2}\\|v\\|_{2} + \\frac{1}{2}\\|v\\|_{2}^{2}\n$$\nWe have bounds on $\\|v\\|_{2}$:\n$$\n\\|v\\|_{2} = \\|\\mathcal{H}e_M(\\delta_{\\star})\\|_{2} \\le \\|\\mathcal{H}\\|_{\\mathrm{op}} \\|e_M(\\delta_{\\star})\\|_{2} = H_{\\mathrm{op}} \\|e_M(\\delta_{\\star})\\|_{2}\n$$\nFrom Step 1, $\\|e_M(\\delta_{\\star})\\|_{2} \\le \\frac{1}{2}L_{2}\\|\\delta_{\\star}\\|_{2}^{2}$. Combining these, we get an upper bound on $\\|v\\|_2$:\n$$\n\\|v\\|_{2} \\le \\frac{1}{2} H_{\\mathrm{op}} L_{2} \\|\\delta_{\\star}\\|_{2}^{2}\n$$\nLet this bound be $C_v = \\frac{1}{2} H_{\\mathrm{op}} L_2 \\|\\delta_{\\star}\\|_{2}^{2}$. Then the absolute difference between `ared` and `pred` is bounded by:\n$$\n|\\mathrm{ared} - \\mathrm{pred}| \\le \\|r_{0} + A\\delta_{\\star}\\|_{2} C_v + \\frac{1}{2}C_v^2\n$$\nLet this upper bound be $C_{err}$. Since $\\mathrm{ared} \\ge \\mathrm{pred} - C_{err}$, we can form a lower bound on the ratio $\\rho = \\mathrm{ared}/\\mathrm{pred}$ (assuming $\\mathrm{pred}  0$):\n$$\n\\rho \\ge \\frac{\\mathrm{pred} - C_{err}}{\\mathrm{pred}} = 1 - \\frac{C_{err}}{\\mathrm{pred}}\n$$\nGiven the requirement that $\\rho_{\\mathrm{low}} \\in [0, \\infty)$, we define our conservative lower bound as:\n$$\n\\rho_{\\mathrm{low}} = \\max\\left(0, 1 - \\frac{\\|r_{0} + A\\delta_{\\star}\\|_{2} C_v + \\frac{1}{2}C_v^2}{\\mathrm{pred}}\\right), \\quad \\text{where } C_v = \\frac{1}{2}H_{\\mathrm{op}}L_{2}\\|\\delta_{\\star}\\|_{2}^{2}\n$$\nIf $\\mathrm{pred}=0$, which occurs if and only if $\\delta_{\\star}=0$ (for the given full-rank $A$), then $\\mathrm{ared}$ is also $0$. The ratio is indeterminate $0/0$, conventionally taken as $1$. In this case, $C_v=0$ and $C_{err}=0$, so our formula is consistent if we define $\\rho_{\\mathrm{low}}=1$ when $\\mathrm{pred}=0$.", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import brentq\n\ndef solve():\n    \"\"\"\n    Solves the trust-region problem for the given test cases.\n    \"\"\"\n    test_cases = [\n        {\"A\": np.array([[1.0, 0.0], [0.0, 2.0]]), \"r0\": np.array([1.0, -1.0]), \"L2\": 4.0, \"epsilon\": 0.05, \"H_op\": 1.5, \"eta\": 0.3},\n        {\"A\": np.array([[2.0, 0.5], [0.0, 1.0]]), \"r0\": np.array([2.0, 1.5]), \"L2\": 3.0, \"epsilon\": 0.5, \"H_op\": 2.0, \"eta\": 0.5},\n        {\"A\": np.array([[0.1, 0.0], [0.0, 0.1]]), \"r0\": np.array([0.05, -0.02]), \"L2\": 100.0, \"epsilon\": 0.0001, \"H_op\": 1.0, \"eta\": 0.1},\n    ]\n\n    results = []\n    for case in test_cases:\n        A = case[\"A\"]\n        r0 = case[\"r0\"]\n        L2 = case[\"L2\"]\n        epsilon = case[\"epsilon\"]\n        H_op = case[\"H_op\"]\n        eta = case[\"eta\"]\n\n        Delta = np.sqrt(2 * epsilon / L2)\n        \n        B = A.T @ A\n        g = A.T @ r0\n        \n        delta_u = -np.linalg.solve(B, g)\n        norm_delta_u = np.linalg.norm(delta_u)\n        \n        if norm_delta_u = Delta:\n            delta_star = delta_u\n        else:\n            def phi(lam):\n                delta_lam = -np.linalg.solve(B + lam * np.identity(B.shape[0]), g)\n                return np.linalg.norm(delta_lam) - Delta\n            \n            lambda_upper_bound = 2 * np.linalg.norm(g) / Delta + 1000\n            try:\n                lam_star = brentq(phi, 1e-9, lambda_upper_bound)\n            except ValueError:\n                lam_star = brentq(phi, 0, lambda_upper_bound)\n                \n            delta_star = -np.linalg.solve(B + lam_star * np.identity(B.shape[0]), g)\n\n        norm_r0_sq = r0.T @ r0\n        res_delta_star = r0 + A @ delta_star\n        norm_res_delta_star_sq = res_delta_star.T @ res_delta_star\n        pred = 0.5 * (norm_r0_sq - norm_res_delta_star_sq)\n\n        if pred  1e-12:\n            rho_low = 1.0\n        else:\n            norm_delta_star = np.linalg.norm(delta_star)\n            Cv = 0.5 * H_op * L2 * norm_delta_star**2\n            C_err = np.linalg.norm(res_delta_star) * Cv + 0.5 * Cv**2\n            rho_low = max(0.0, 1.0 - C_err / pred)\n\n        decision = bool(rho_low >= eta)\n        results.append([Delta, rho_low, decision])\n\n    final_output_str = \",\".join(\n        [\n            f\"[{r[0]:.6f},{r[1]:.6f},{r[2]}]\"\n            for r in results\n        ]\n    )    \n    print(f\"[{final_output_str}]\".replace(\" \",\"\"))\n\nsolve()\n```", "id": "3398775"}]}