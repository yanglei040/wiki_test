{"hands_on_practices": [{"introduction": "The inner loop is the computational engine of incremental variational methods, where a simplified, quadratic version of the full data assimilation problem is solved. This exercise focuses on calculating the essential ingredients of this inner loop: the gradient of the cost function at the starting point, and the optimal update step (the Gauss-Newton step) that minimizes the quadratic approximation. Mastering this calculation provides a concrete understanding of how observations and background information are blended to produce an analysis increment.[@problem_id:3409171]", "problem": "Consider a single-observation incremental variational data assimilation inner-loop arising within an outer-loop/inner-loop strategy. Let the observation operator be linear, given by the row vector $\\,\\mathbf{H} = [\\,1,\\,2\\,]\\,$ mapping $\\mathbb{R}^{2}\\rightarrow\\mathbb{R}$. The background-error covariance is $\\,\\mathbf{B}=\\mathrm{diag}(1,4)\\,$ and the observation-error variance is $\\,\\mathbf{R}=0.5\\,$ (a scalar). Suppose the current outer-loop linearization point is $\\,\\mathbf{x}_{k}=\\begin{pmatrix}1\\\\-1\\end{pmatrix}\\,$, the background state is $\\,\\mathbf{x}_{b}=\\begin{pmatrix}0\\\\0\\end{pmatrix}\\,$, and the observed value is $\\,y=1\\,$. \n\nDefine the incremental inner-loop cost (as used in the Three-Dimensional Variational (3D-Var) setting after linearization of a possibly nonlinear observation operator) by\n$$\nJ(\\delta \\mathbf{x}) \\;=\\; \\tfrac{1}{2}\\,(\\delta \\mathbf{x}-\\delta \\mathbf{x}_{b})^{\\top}\\,\\mathbf{B}^{-1}\\,(\\delta \\mathbf{x}-\\delta \\mathbf{x}_{b})\n\\;+\\;\\tfrac{1}{2}\\,\\bigl(\\mathbf{H}\\,\\delta \\mathbfx-\\mathbf{d}\\bigr)^{\\top}\\,\\mathbf{R}^{-1}\\,\\bigl(\\mathbf{H}\\,\\delta \\mathbf{x}-\\mathbf{d}\\bigr),\n$$\nwhere $\\,\\delta \\mathbf{x}=\\mathbf{x}-\\mathbf{x}_{k}\\,$, $\\,\\delta \\mathbf{x}_{b}=\\mathbf{x}_{b}-\\mathbf{x}_{k}\\,$, and the innovation at the linearization point is $\\,\\mathbf{d} = y - \\mathbf{H}\\,\\mathbf{x}_{k}\\,$.\n\n- Compute the inner-loop gradient $\\,\\nabla J(\\delta \\mathbf{x})\\,$ evaluated at the initial inner-loop iterate $\\,\\delta \\mathbf{x}=\\mathbf{0}\\,$.\n- Compute the Gauss–Newton step $\\,\\delta \\mathbf{x}^{\\star}\\,$ that minimizes $\\,J(\\delta \\mathbf{x})\\,$.\n\nProvide the final answer as a single row matrix $\\,\\begin{pmatrix} g_{1} & g_{2} & \\delta x_{1}^{\\star} & \\delta x_{2}^{\\star}\\end{pmatrix}\\,$ containing the two components of the gradient at $\\,\\delta \\mathbf{x}=\\mathbf{0}\\,$ followed by the two components of the minimizing step. Express all values exactly; no rounding is required.", "solution": "The problem asks for two quantities related to an incremental variational data assimilation cost function, $J(\\delta \\mathbf{x})$: the gradient at the start of the inner loop ($\\delta \\mathbf{x} = \\mathbf{0}$), and the minimizing Gauss-Newton step, $\\delta \\mathbf{x}^{\\star}$.\n\nThe cost function is given by:\n$$\nJ(\\delta \\mathbf{x}) = \\frac{1}{2}(\\delta \\mathbf{x}-\\delta \\mathbf{x}_{b})^{\\top}\\mathbf{B}^{-1}(\\delta \\mathbf{x}-\\delta \\mathbf{x}_{b}) + \\frac{1}{2}(\\mathbf{H}\\delta \\mathbf{x}-\\mathbf{d})^{\\top}\\mathbf{R}^{-1}(\\mathbf{H}\\delta \\mathbf{x}-\\mathbf{d})\n$$\nThe provided data are:\n- $\\mathbf{H} = \\begin{pmatrix} 1 & 2 \\end{pmatrix}$\n- $\\mathbf{B} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 4 \\end{pmatrix}$\n- $R = 0.5$\n- $\\mathbf{x}_{k} = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$\n- $\\mathbf{x}_{b} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$\n- $y = 1$\n\nFirst, we compute the defined increments $\\delta \\mathbf{x}_{b}$ and $\\mathbf{d}$.\nThe background increment is $\\delta \\mathbf{x}_{b} = \\mathbf{x}_{b} - \\mathbf{x}_{k}$:\n$$\n\\delta \\mathbf{x}_{b} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix}\n$$\nThe innovation at the linearization point, $d$, is defined as $d = y - \\mathbf{H}\\mathbf{x}_{k}$. Since $y$ is a scalar and $\\mathbf{H}\\mathbf{x}_k$ is a scalar, $d$ is a scalar.\n$$\n\\mathbf{H}\\mathbf{x}_{k} = \\begin{pmatrix} 1 & 2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = (1)(1) + (2)(-1) = 1 - 2 = -1\n$$\n$$\nd = 1 - (-1) = 2\n$$\nThe gradient of the cost function is:\n$$\n\\nabla J(\\delta \\mathbf{x}) = \\mathbf{B}^{-1}(\\delta \\mathbf{x} - \\delta \\mathbf{x}_{b}) + \\mathbf{H}^{\\top}R^{-1}(\\mathbf{H}\\delta \\mathbf{x} - d)\n$$\nWe evaluate this gradient at $\\delta \\mathbf{x} = \\mathbf{0}$:\n$$\n\\nabla J(\\mathbf{0}) = -\\mathbf{B}^{-1}\\delta \\mathbf{x}_{b} - \\mathbf{H}^{\\top}R^{-1}d\n$$\nThe inverse matrices are:\n$$\n\\mathbf{B}^{-1} = \\begin{pmatrix} 1 & 0 \\\\ 0 & \\frac{1}{4} \\end{pmatrix}, \\quad R^{-1} = \\frac{1}{0.5} = 2\n$$\nSubstituting values into the gradient expression:\n$$\n\\nabla J(\\mathbf{0}) = - \\begin{pmatrix} 1 & 0 \\\\ 0 & \\frac{1}{4} \\end{pmatrix} \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} (2) (2) = - \\begin{pmatrix} -1 \\\\ \\frac{1}{4} \\end{pmatrix} - \\begin{pmatrix} 4 \\\\ 8 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -\\frac{1}{4} \\end{pmatrix} - \\begin{pmatrix} 4 \\\\ 8 \\end{pmatrix}\n$$\n$$\n\\nabla J(\\mathbf{0}) = \\begin{pmatrix} 1 - 4 \\\\ -\\frac{1}{4} - 8 \\end{pmatrix} = \\begin{pmatrix} -3 \\\\ -\\frac{1}{4} - \\frac{32}{4} \\end{pmatrix} = \\begin{pmatrix} -3 \\\\ -\\frac{33}{4} \\end{pmatrix}\n$$\nSo, the gradient components are $g_1 = -3$ and $g_2 = -\\frac{33}{4}$.\n\nTo find the minimizer $\\delta \\mathbf{x}^{\\star}$, we set the gradient to zero, which gives the normal equations $\\nabla^2 J \\, \\delta \\mathbf{x}^{\\star} = -\\nabla J(\\mathbf{0})$. The Hessian is $\\nabla^2 J = \\mathbf{B}^{-1} + \\mathbf{H}^{\\top}R^{-1}\\mathbf{H}$.\n$$\n\\mathbf{H}^{\\top}R^{-1}\\mathbf{H} = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} (2) \\begin{pmatrix} 1 & 2 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 4 \\end{pmatrix} \\begin{pmatrix} 1 & 2 \\end{pmatrix} = \\begin{pmatrix} 2 & 4 \\\\ 4 & 8 \\end{pmatrix}\n$$\n$$\n\\nabla^2 J = \\begin{pmatrix} 1 & 0 \\\\ 0 & \\frac{1}{4} \\end{pmatrix} + \\begin{pmatrix} 2 & 4 \\\\ 4 & 8 \\end{pmatrix} = \\begin{pmatrix} 3 & 4 \\\\ 4 & 8+\\frac{1}{4} \\end{pmatrix} = \\begin{pmatrix} 3 & 4 \\\\ 4 & \\frac{33}{4} \\end{pmatrix}\n$$\nThe linear system for $\\delta \\mathbf{x}^{\\star} = \\begin{pmatrix} \\delta x_1^{\\star} \\\\ \\delta x_2^{\\star} \\end{pmatrix}$ is:\n$$\n\\begin{pmatrix} 3 & 4 \\\\ 4 & \\frac{33}{4} \\end{pmatrix} \\begin{pmatrix} \\delta x_1^{\\star} \\\\ \\delta x_2^{\\star} \\end{pmatrix} = -\\nabla J(\\mathbf{0}) = \\begin{pmatrix} 3 \\\\ \\frac{33}{4} \\end{pmatrix}\n$$\nThis gives two simultaneous equations:\n1) $3\\delta x_1^{\\star} + 4\\delta x_2^{\\star} = 3$\n2) $4\\delta x_1^{\\star} + \\frac{33}{4}\\delta x_2^{\\star} = \\frac{33}{4}$\n\nFrom equation (1), we express $\\delta x_1^{\\star} = \\frac{3 - 4\\delta x_2^{\\star}}{3} = 1 - \\frac{4}{3}\\delta x_2^{\\star}$.\nSubstitute this into equation (2):\n$$\n4\\left(1 - \\frac{4}{3}\\delta x_2^{\\star}\\right) + \\frac{33}{4}\\delta x_2^{\\star} = \\frac{33}{4}\n$$\n$$\n4 - \\frac{16}{3}\\delta x_2^{\\star} + \\frac{33}{4}\\delta x_2^{\\star} = \\frac{33}{4}\n$$\nRearranging terms to solve for $\\delta x_2^{\\star}$:\n$$\n4 = \\frac{16}{3}\\delta x_2^{\\star} - \\frac{33}{4}\\delta x_2^{\\star}\n$$\n$$\n4 = \\left(\\frac{16 \\cdot 4 - 33 \\cdot 3}{12}\\right)\\delta x_2^{\\star} = \\left(\\frac{64 - 99}{12}\\right)\\delta x_2^{\\star} = -\\frac{35}{12}\\delta x_2^{\\star}\n$$\nThis is an error in the original calculation. Let's re-group the terms correctly:\n$$\n\\left(\\frac{33}{4} - \\frac{16}{3}\\right)\\delta x_2^{\\star} = \\frac{33}{4} - 4\n$$\n$$\n\\left(\\frac{99 - 64}{12}\\right)\\delta x_2^{\\star} = \\frac{33 - 16}{4}\n$$\n$$\n\\frac{35}{12}\\delta x_2^{\\star} = \\frac{17}{4}\n$$\n$$\n\\delta x_2^{\\star} = \\frac{17}{4} \\cdot \\frac{12}{35} = \\frac{17 \\cdot 3}{35} = \\frac{51}{35}\n$$\nNow, substitute $\\delta x_2^{\\star}$ back to find $\\delta x_1^{\\star}$:\n$$\n\\delta x_1^{\\star} = 1 - \\frac{4}{3}\\left(\\frac{51}{35}\\right) = 1 - \\frac{4 \\cdot 17}{35} = 1 - \\frac{68}{35} = \\frac{35-68}{35} = -\\frac{33}{35}\n$$\nSo the minimizer is $\\delta \\mathbf{x}^{\\star} = \\begin{pmatrix} -\\frac{33}{35} \\\\ \\frac{51}{35} \\end{pmatrix}$.\n\nThe final answer is $\\begin{pmatrix} g_1 & g_2 & \\delta x_1^{\\star} & \\delta x_2^{\\star} \\end{pmatrix}$:\n$$\n\\begin{pmatrix} -3 & -\\frac{33}{4} & -\\frac{33}{35} & \\frac{51}{35} \\end{pmatrix}\n$$", "answer": "$$\n\\boxed{\\begin{pmatrix} -3 & -\\frac{33}{4} & -\\frac{33}{35} & \\frac{51}{35} \\end{pmatrix}}\n$$", "id": "3409171"}, {"introduction": "The true power of the outer-loop/inner-loop strategy lies in its ability to handle the nonlinear observation operators common in realistic models, where a single solution step is insufficient. This practice demonstrates a complete outer-loop iteration where you will linearize a nonlinear function and solve for an increment to update the state estimate.[@problem_id:3409194] Completing this cycle reveals how the iterative process refines the solution by successively improving the linearization point, moving closer to the minimum of the true nonlinear cost function.", "problem": "Consider a scalar variational data assimilation problem in which the state variable is $x \\in \\mathbb{R}$ and the observation operator is nonlinear, given by $\\mathcal{H}(x)=\\sin(x)$. The standard variational data assimilation cost functional is\n$$\nJ(x) = \\frac{1}{2} (x - x_{b})^{\\top} B^{-1} (x - x_{b}) + \\frac{1}{2} \\big(y - \\mathcal{H}(x)\\big)^{\\top} R^{-1} \\big(y - \\mathcal{H}(x)\\big),\n$$\nwhere $x_{b}$ is the background (prior) state, $B$ is the background error covariance, $y$ is the observation, and $R$ is the observation error covariance. In the incremental outer-loop and inner-loop strategy, one linearizes the observation operator at the current outer-loop linearization point $x_{0}$, writes $x = x_{0} + \\delta x$, and solves the inner-loop quadratic problem for the increment $\\delta x$ before updating $x$ and relinearizing.\n\nGiven the scalar data $x_{b} = 0$, $B = 1$, $R = 0.1$, $y = 0.5$, perform a single outer-loop step starting from $x_{0} = x_{b}$ as follows:\n- Using a first-order Taylor linearization of $\\mathcal{H}(x)$ at $x_{0}$, formulate the corresponding inner-loop quadratic cost in terms of the increment $\\delta x$, and analytically minimize it to obtain the inner-loop solution $\\delta x$.\n- Apply the outer-loop update $x_{1} = x_{0} + \\delta x$.\n\nExpress your final numerical values exactly as fractions if possible. No rounding is required. The final answer must consist of the pair $\\delta x$ and $x_{1}$ written as a single row matrix.", "solution": "The task is to perform one outer-loop iteration of an incremental variational data assimilation scheme for a scalar problem. This involves linearizing the nonlinear observation operator, forming a quadratic cost function for the increment, minimizing it, and updating the state.\n\nThe general cost functional for this scalar problem is:\n$$\nJ(x) = \\frac{1}{2} \\frac{(x - x_{b})^{2}}{B} + \\frac{1}{2} \\frac{(y - \\mathcal{H}(x))^{2}}{R}\n$$\nThe incremental approach approximates this by setting $x = x_0 + \\delta x$ and linearizing the nonlinear operator $\\mathcal{H}(x) = \\sin(x)$ around the starting point $x_0 = x_b = 0$.\nThe first-order Taylor expansion is $\\mathcal{H}(x_0 + \\delta x) \\approx \\mathcal{H}(x_0) + H \\delta x$, where $H = \\mathcal{H}'(x_0)$.\nThe derivative is $\\mathcal{H}'(x) = \\cos(x)$, so at $x_0 = 0$, the linearized operator is $H = \\cos(0) = 1$.\nThe value of the operator at the linearization point is $\\mathcal{H}(x_0) = \\sin(0) = 0$.\n\nWe substitute this linearization into the cost functional to get the inner-loop cost function, $J_{inner}(\\delta x)$:\n$$\nJ_{inner}(\\delta x) = \\frac{1}{2} \\frac{(x_0 + \\delta x - x_b)^{2}}{B} + \\frac{1}{2} \\frac{(y - (\\mathcal{H}(x_0) + H \\delta x))^{2}}{R}\n$$\nSubstituting the given values ($x_b = 0, B = 1, y = 0.5, R = 0.1, x_0=0, \\mathcal{H}(x_0)=0, H=1$):\n$$\nJ_{inner}(\\delta x) = \\frac{1}{2} \\frac{(0 + \\delta x - 0)^{2}}{1} + \\frac{1}{2} \\frac{(0.5 - (0 + 1 \\cdot \\delta x))^{2}}{0.1}\n$$\n$$\nJ_{inner}(\\delta x) = \\frac{1}{2} (\\delta x)^{2} + \\frac{1}{2 \\cdot 0.1} (0.5 - \\delta x)^{2} = \\frac{1}{2} (\\delta x)^{2} + 5 \\left(\\frac{1}{2} - \\delta x\\right)^{2}\n$$\nTo find the increment $\\delta x$ that minimizes this quadratic cost function, we compute its derivative with respect to $\\delta x$ and set it to zero.\n$$\n\\frac{d J_{inner}}{d (\\delta x)} = \\delta x + 5 \\cdot 2 \\left(\\frac{1}{2} - \\delta x\\right) \\cdot (-1) = \\delta x - 10\\left(\\frac{1}{2} - \\delta x\\right)\n$$\n$$\n\\frac{d J_{inner}}{d (\\delta x)} = \\delta x - 5 + 10 \\delta x = 11 \\delta x - 5\n$$\nSet the derivative to zero to find the minimum:\n$$\n11 \\delta x - 5 = 0 \\implies \\delta x = \\frac{5}{11}\n$$\nThe second derivative is $\\frac{d^{2} J_{inner}}{d (\\delta x)^{2}} = 11 > 0$, confirming this is a minimum.\n\nThe final step is the outer-loop update to obtain the new state estimate, $x_{1}$:\n$$\nx_{1} = x_{0} + \\delta x = 0 + \\frac{5}{11} = \\frac{5}{11}\n$$\nThe requested pair is $(\\delta x, x_1)$, which is $(\\frac{5}{11}, \\frac{5}{11})$.", "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{5}{11} & \\frac{5}{11} \\end{pmatrix}}\n$$", "id": "3409194"}, {"introduction": "Beyond estimating the state of a system, data assimilation is often used to estimate uncertain model parameters, and the outer-loop/inner-loop framework is flexible enough to handle this joint estimation problem. This exercise extends the method to an augmented state vector that includes both the system state $x$ and a model parameter $p$, where the observation operator $h(x,p) = px$ is nonlinear in the joint space.[@problem_id:3409148] This practice illustrates the power and generality of the incremental method, showing how it can simultaneously improve estimates of both the state and the underlying model physics.", "problem": "Consider joint state–parameter variational data assimilation for a single scalar observation with the observation operator defined by $h(x,p)=p\\,x$. Let the augmented control vector be $z=\\begin{pmatrix}x & p\\end{pmatrix}^{\\top}$. Assume a Gaussian prior $z\\sim\\mathcal{N}(z_{b},B)$ with $z_{b}=\\begin{pmatrix}x_{b} & p_{b}\\end{pmatrix}^{\\top}$ and $B=\\mathrm{diag}(1,1)$, and a Gaussian likelihood $y\\mid z\\sim\\mathcal{N}(h(x,p),R)$ with $R=0.1$. The Maximum A Posteriori (MAP) estimator arises from minimizing the quadratic Bayesian objective\n$$\nJ(z)=\\frac{1}{2}\\,(z-z_{b})^{\\top}B^{-1}(z-z_{b})+\\frac{1}{2}\\,\\big(y-h(x,p)\\big)^{\\top}R^{-1}\\big(y-h(x,p)\\big).\n$$\nAn outer-loop and inner-loop strategy is employed: at outer-loop index $k$, the observation operator is linearized at the current iterate $z^{k}$, and the inner loop solves exactly the linearized Gaussian least-squares problem for the increment $\\delta z=z-z^{k}$ (equivalently, solves the inner symmetric block system associated with the linearized Karush–Kuhn–Tucker conditions). Starting from the background $x_{b}=1$ and $p_{b}=2$ with the scalar observation $y=3$, perform one outer-loop update by linearizing at $z^{0}=z_{b}$ and solving the inner problem exactly to obtain $z^{1}=z^{0}+\\delta z$. Express your final answer as the exact pair $\\begin{pmatrix}x^{1} & p^{1}\\end{pmatrix}$ using row-matrix notation. No rounding is required, and no physical units are involved.", "solution": "The problem is to find the Maximum A Posteriori (MAP) estimate for an augmented state vector $z=\\begin{pmatrix}x & p\\end{pmatrix}^{\\top}$ by minimizing the cost function:\n$$\nJ(z)=\\frac{1}{2}\\,(z-z_{b})^{\\top}B^{-1}(z-z_{b})+\\frac{1}{2}\\,\\big(y-h(x,p)\\big)^{\\top}R^{-1}\\big(y-h(x,p)\\big),\n$$\nwhere $h(x,p)=px$. We perform one outer-loop iteration starting from $z^0 = z_b$. This involves linearizing $h(x,p)$ at $z^0$ and solving for the increment $\\delta z$.\n\nThe linearization of $h$ at an iterate $z^k$ is $h(z) \\approx h(z^k) + G^k \\delta z$, where $G^k$ is the Jacobian of $h$ evaluated at $z^k$:\n$$\nG^k = \\begin{pmatrix}\\frac{\\partial h}{\\partial x} & \\frac{\\partial h}{\\partial p}\\end{pmatrix}\\bigg|_{z^k} = \\begin{pmatrix}p^k & x^k\\end{pmatrix}.\n$$\nThe incremental cost function for $\\delta z$, starting from $z^k=z_b$, is:\n$$\nJ_k(\\delta z) = \\frac{1}{2}\\,\\delta z^{\\top}B^{-1}\\delta z + \\frac{1}{2}\\,\\big(d^k-G^k\\delta z\\big)^{\\top}R^{-1}\\big(d^k-G^k\\delta z\\big),\n$$\nwhere $d^k = y-h(z^k)$ is the innovation. Setting the gradient of $J_k(\\delta z)$ to zero gives the normal equations:\n$$\n\\big(B^{-1}+(G^{k})^{\\top}R^{-1}G^{k}\\big)\\,\\delta z=(G^{k})^{\\top}R^{-1}d^{k}.\n$$\nWe instantiate the data for the first step, where $k=0$ and $z^0 = z_b = \\begin{pmatrix}1 \\\\ 2\\end{pmatrix}$. So $x^0=1, p^0=2$.\nThe other data are $B=\\mathrm{diag}(1,1)$, $y=3$, and $R=0.1$.\nFirst, compute the Jacobian at $z^0$:\n$$\nG^{0}=\\begin{pmatrix}p^{0} & x^{0}\\end{pmatrix}=\\begin{pmatrix}2 & 1\\end{pmatrix}.\n$$\nNext, compute the innovation:\n$$\nd^{0}=y-h(x^{0},p^{0})=3-(p^{0}x^{0})=3-(2\\cdot 1)=1.\n$$\nNow, assemble the matrices for the normal equations. $B^{-1}=\\mathrm{diag}(1,1)$ and $R^{-1}=1/0.1=10$.\nThe Hessian of the inner-loop problem is $M = B^{-1}+(G^{0})^{\\top}R^{-1}G^{0}$:\n$$\n(G^{0})^{\\top}R^{-1}G^{0} = \\begin{pmatrix}2 \\\\ 1\\end{pmatrix}(10)\\begin{pmatrix}2 & 1\\end{pmatrix} = 10\\begin{pmatrix}4 & 2 \\\\ 2 & 1\\end{pmatrix} = \\begin{pmatrix}40 & 20 \\\\ 20 & 10\\end{pmatrix}.\n$$\n$$\nM = \\begin{pmatrix}1 & 0 \\\\ 0 & 1\\end{pmatrix}+\\begin{pmatrix}40 & 20 \\\\ 20 & 10\\end{pmatrix}=\\begin{pmatrix}41 & 20 \\\\ 20 & 11\\end{pmatrix}.\n$$\nThe right-hand side is $b=(G^{0})^{\\top}R^{-1}d^{0}$:\n$$\nb = \\begin{pmatrix}2 \\\\ 1\\end{pmatrix}(10)(1)=\\begin{pmatrix}20 \\\\ 10\\end{pmatrix}.\n$$\nWe solve the $2 \\times 2$ linear system $M\\,\\delta z=b$. The determinant of $M$ is:\n$$\n\\det(M)=41\\cdot 11-20\\cdot 20=451-400=51.\n$$\nThe inverse is:\n$$\nM^{-1}=\\frac{1}{51}\\begin{pmatrix}11 & -20 \\\\ -20 & 41\\end{pmatrix}.\n$$\nThe increment $\\delta z$ is:\n$$\n\\delta z=M^{-1}b=\\frac{1}{51}\\begin{pmatrix}11 & -20 \\\\ -20 & 41\\end{pmatrix}\\begin{pmatrix}20 \\\\ 10\\end{pmatrix}=\\frac{1}{51}\\begin{pmatrix}11\\cdot 20-20\\cdot 10 \\\\ -20\\cdot 20+41\\cdot 10\\end{pmatrix}=\\frac{1}{51}\\begin{pmatrix}220-200 \\\\ -400+410\\endpmatrix}=\\frac{1}{51}\\begin{pmatrix}20 \\\\ 10\\end{pmatrix}.\n$$\nThus, $\\delta x=20/51$ and $\\delta p=10/51$.\nThe outer-loop update gives the new state $z^1$:\n$$\nz^{1}=z^{0}+\\delta z=\\begin{pmatrix}x^{0} \\\\ p^{0}\\end{pmatrix}+\\begin{pmatrix}\\delta x \\\\ \\delta p\\end{pmatrix}=\\begin{pmatrix}1 \\\\ 2\\end{pmatrix}+\\begin{pmatrix}\\frac{20}{51} \\\\ \\frac{10}{51}\\end{pmatrix}=\\begin{pmatrix}1+\\frac{20}{51} \\\\ 2+\\frac{10}{51}\\end{pmatrix}=\\begin{pmatrix}\\frac{71}{51} \\\\ \\frac{112}{51}\\end{pmatrix}.\n$$\nThe result for the pair $\\begin{pmatrix}x^{1} & p^{1}\\end{pmatrix}$ is $\\begin{pmatrix}\\frac{71}{51} & \\frac{112}{51}\\end{pmatrix}$.", "answer": "$$\\boxed{\\begin{pmatrix}\\frac{71}{51} & \\frac{112}{51}\\end{pmatrix}}$$", "id": "3409148"}]}