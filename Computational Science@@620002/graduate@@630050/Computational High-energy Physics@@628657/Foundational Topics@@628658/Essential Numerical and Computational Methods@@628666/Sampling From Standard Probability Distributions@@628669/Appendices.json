{"hands_on_practices": [{"introduction": "Generating standard normal variates is a foundational task in any simulation. This exercise compares the classic Box-Muller and Marsaglia polar methods, requiring a derivation of each from first principles. More importantly, it pushes you to analyze their performance in the context of modern parallel computing, a critical consideration for large-scale physics simulations [@problem_id:3532699]. You will quantify the trade-offs between the branch-free but computationally intensive Box-Muller method and the rejection-based but arithmetically simpler Marsaglia method, providing deep insight into algorithm choice on hardware like GPUs.", "problem": "You are to build a complete, runnable program that compares the Box–Muller and Marsaglia polar methods for sampling from a standard normal distribution in a computational high-energy physics context. Starting from the fundamental base of the probability density function of the standard normal distribution and the definitions of the uniform distribution on the unit interval, derive the transformations from independent uniform pairs to independent standard normal pairs and quantify branch divergence and expected runtime under a Single Instruction, Multiple Threads (SIMT) graphics processing unit (GPU) execution model. All angles must be treated in radians.\n\nYour program must implement the following tasks, derived from first principles:\n\n- Base definitions:\n  - Two independent uniform random variables $U_1$ and $U_2$ on the unit interval follow $U_1,U_2 \\sim \\mathrm{Uniform}(0,1)$.\n  - A standard normal random variable $Z$ has the probability density function $f_Z(z) = \\frac{1}{\\sqrt{2\\pi}} \\exp(-z^2/2)$.\n\n- Box–Muller method:\n  - Derive and justify the transformation from $U_1,U_2$ to two independent standard normal variates using change of variables and the polar representation of the joint Gaussian in two dimensions.\n  - The method must be free of accept/reject branching. You must assume angles are in radians.\n\n- Marsaglia polar method:\n  - Derive and justify the sampling rule from two independent draws on the square via an accept/reject step on the unit disc and a radial rescaling using the probability conservation under smooth mappings in two dimensions.\n  - Using geometric probability, derive the acceptance probability $p$ and the distribution of the number of iterations per thread until the first acceptance. Model the latter as a geometric random variable with support on the positive integers.\n\n- SIMT warp model for expected iterations and divergence:\n  - Consider a warp of $W$ independent threads, each requiring one accepted pair. Let $G_i$ denote the number of accept/reject iterations taken by thread $i$ until its first acceptance under the Marsaglia polar method.\n  - Derive an exact expression for the distribution function of the maximum $M_W = \\max\\{G_1,\\dots,G_W\\}$ from basic probability and independence, and from this derive a convergent series expression for the expectation $\\mathbb{E}[M_W]$.\n  - Define the initial per-iteration branch divergence probability $D_0$ when all $W$ threads are active as the probability that, in the first iteration, not all threads make the same branch decision. Express $D_0$ in terms of $p$ and $W$.\n  - Define the expected number of iterations within a warp where the accept branch is executed at least once (i.e., iterations with at least one acceptance among the active threads). Derive a convergent series for its expectation $\\mathbb{E}[K_W]$ using linearity of expectation and independence.\n\n- Operation-count-based cost model:\n  - Assume the following operation counts per algorithm:\n    - Box–Muller per two-normal pair:\n      - Uniform draws: $2$ draws.\n      - Transcendentals: one $\\log$ and one $\\sqrt$ together counted as a combined cost; one $\\sin$ and one $\\cos$ together counted as a combined cost.\n      - Basic arithmetic operations: $4$ operations.\n    - Marsaglia polar per one iteration attempt (executed by all active threads every iteration):\n      - Uniform draws: $2$ draws.\n      - Basic arithmetic operations: $7$ operations to map to the square, compute $S$, and compare.\n    - Marsaglia polar extra per accept path (executed on iterations where at least one acceptance occurs in the warp):\n      - Transcendentals: one $\\log$ and one $\\sqrt$ together counted as a combined cost.\n      - Basic arithmetic operations: $4$ operations to scale and form the outputs.\n  - Define cost parameters:\n    - $c_{\\mathrm{uni}}$ = cost of one uniform draw.\n    - $c_{\\mathrm{basic}}$ = cost of one basic arithmetic operation.\n    - $c_{\\log\\sqrt}$ = combined cost of one $\\log$ and one $\\sqrt$.\n    - $c_{\\mathrm{trigpair}}$ = combined cost of one $\\sin$ and one $\\cos$.\n  - From these counts and costs, define:\n    - Box–Muller warp time per two-normal pair: $T_{\\mathrm{BM}} = 2 c_{\\mathrm{uni}} + c_{\\log\\sqrt} + c_{\\mathrm{trigpair}} + 4 c_{\\mathrm{basic}}$.\n    - Marsaglia polar warp time per two-normal pair: $T_{\\mathrm{MP}} = \\mathbb{E}[M_W]\\,(2 c_{\\mathrm{uni}} + 7 c_{\\mathrm{basic}}) + \\mathbb{E}[K_W]\\,(c_{\\log\\sqrt} + 4 c_{\\mathrm{basic}})$.\n  - Define the occupancy overhead factor $F_{\\mathrm{occ}} = \\mathbb{E}[M_W]\\,p$ to quantify the slowdown relative to the single-thread expectation under identical conditions.\n\n- Required outputs per test case:\n  - The acceptance probability $p$.\n  - The expected number of warp iterations $\\mathbb{E}[M_W]$ for the Marsaglia polar method.\n  - The initial divergence probability $D_0$ with all $W$ threads active.\n  - The occupancy overhead factor $F_{\\mathrm{occ}}$.\n  - The Box–Muller warp time per pair $T_{\\mathrm{BM}}$.\n  - The Marsaglia polar warp time per pair $T_{\\mathrm{MP}}$.\n  - The ratio $R = T_{\\mathrm{MP}}/T_{\\mathrm{BM}}$.\n\nTest suite:\n- Each test case specifies $(W, c_{\\mathrm{uni}}, c_{\\mathrm{basic}}, c_{\\log\\sqrt}, c_{\\mathrm{trigpair}})$.\n- Use the following four cases:\n  - Case A (single-thread boundary): $W = 1$, $c_{\\mathrm{uni}} = 1.0$, $c_{\\mathrm{basic}} = 0.25$, $c_{\\log\\sqrt} = 10.0$, $c_{\\mathrm{trigpair}} = 8.0$.\n  - Case B (small warp): $W = 8$, $c_{\\mathrm{uni}} = 1.0$, $c_{\\mathrm{basic}} = 0.25$, $c_{\\log\\sqrt} = 10.0$, $c_{\\mathrm{trigpair}} = 8.0$.\n  - Case C (typical NVIDIA warp): $W = 32$, $c_{\\mathrm{uni}} = 1.0$, $c_{\\mathrm{basic}} = 0.25$, $c_{\\log\\sqrt} = 10.0$, $c_{\\mathrm{trigpair}} = 8.0$.\n  - Case D (typical AMD wavefront): $W = 64$, $c_{\\mathrm{uni}} = 1.0$, $c_{\\mathrm{basic}} = 0.25$, $c_{\\log\\sqrt} = 10.0$, $c_{\\mathrm{trigpair}} = 8.0$.\n\nAngle unit specification:\n- All trigonometric functions must use angles in radians.\n\nFinal output format:\n- Your program must produce a single line containing a list of four inner lists, one per test case. Each inner list must be ordered as $[p,\\ \\mathbb{E}[M_W],\\ D_0,\\ F_{\\mathrm{occ}},\\ T_{\\mathrm{BM}},\\ T_{\\mathrm{MP}},\\ R]$, with all entries as decimal floats. The outer list must be enclosed in square brackets, and inner lists must be comma-separated. For example, a structurally correct output would look like $[[x_1,x_2,\\dots,x_7],[y_1,y_2,\\dots,y_7],[z_1,\\dots,z_7],[w_1,\\dots,w_7]]$ where each $x_i,y_i,z_i,w_i$ is a float.", "solution": "The problem is valid as it is scientifically grounded in established probability theory and computational modeling, is well-posed with sufficient and consistent information, and is formalizable into a solvable structure. The solution proceeds by deriving the necessary mathematical expressions from first principles and then implementing them numerically to evaluate the specified performance metrics for the given test cases.\n\n### 1. Box–Muller Method: Derivation\nThe Box–Muller method generates two independent standard normal variates, $Z_1$ and $Z_2$, from two independent uniform variates, $U_1, U_2 \\sim \\mathrm{Uniform}(0,1)$.\n\nLet $Z_1$ and $Z_2$ be independent standard normal random variables. Their joint probability density function (PDF) is given by the product of their individual PDFs:\n$$f_{Z_1, Z_2}(z_1, z_2) = f_{Z_1}(z_1) f_{Z_2}(z_2) = \\left(\\frac{1}{\\sqrt{2\\pi}} e^{-z_1^2/2}\\right) \\left(\\frac{1}{\\sqrt{2\\pi}} e^{-z_2^2/2}\\right) = \\frac{1}{2\\pi} e^{-(z_1^2 + z_2^2)/2}$$\nWe perform a change of variables to polar coordinates $(R, \\Theta)$, where $z_1 = r\\cos\\theta$ and $z_2 = r\\sin\\theta$. The domain is $r \\in [0, \\infty)$ and, per problem specification of using radians, $\\theta \\in [0, 2\\pi)$. The squared radius is $r^2 = z_1^2 + z_2^2$.\n\nThe Jacobian of this transformation from $(r, \\theta)$ to $(z_1, z_2)$ is:\n$$|J| = \\left| \\det \\begin{pmatrix} \\frac{\\partial z_1}{\\partial r} & \\frac{\\partial z_1}{\\partial \\theta} \\\\ \\frac{\\partial z_2}{\\partial r} & \\frac{\\partial z_2}{\\partial \\theta} \\end{pmatrix} \\right| = \\left| \\det \\begin{pmatrix} \\cos\\theta & -r\\sin\\theta \\\\ \\sin\\theta & r\\cos\\theta \\end{pmatrix} \\right| = r\\cos^2\\theta + r\\sin^2\\theta = r$$\nThe joint PDF in polar coordinates is $g(r, \\theta) = f_{Z_1, Z_2}(r\\cos\\theta, r\\sin\\theta) |J|$, which gives:\n$$g(r, \\theta) = \\frac{1}{2\\pi} e^{-r^2/2} \\cdot r = \\left(r e^{-r^2/2}\\right) \\left(\\frac{1}{2\\pi}\\right)$$\nThis PDF is separable into a function of $r$ and a function of $\\theta$, which implies that the random variables $R$ and $\\Theta$ are independent. The marginal PDF for $\\Theta$ is $g_\\Theta(\\theta) = \\frac{1}{2\\pi}$ for $\\theta \\in [0, 2\\pi)$, which is a uniform distribution. The marginal PDF for $R$ is $g_R(r) = r e^{-r^2/2}$ for $r \\ge 0$, which is a Rayleigh distribution.\n\nTo generate samples for $R$ and $\\Theta$, we use the inverse transform sampling method.\nFor the angle $\\Theta$, we can draw $U_2 \\sim \\mathrm{Uniform}(0,1)$ and set $\\Theta = 2\\pi U_2$.\nFor the radius $R$, it is easier to first find the distribution of $S = R^2$. Using the change of variable formula for $s=r^2$ (so $r=\\sqrt{s}$ and $dr/ds = 1/(2\\sqrt{s})$):\n$$g_S(s) = g_R(\\sqrt{s}) \\left|\\frac{dr}{ds}\\right| = \\sqrt{s} e^{-s/2} \\cdot \\frac{1}{2\\sqrt{s}} = \\frac{1}{2}e^{-s/2}$$\nThis is the PDF of an exponential distribution with rate parameter $\\lambda=1/2$. We can generate a sample $s$ from this distribution by drawing $U_1 \\sim \\mathrm{Uniform}(0,1)$ and setting $s = F_S^{-1}(U_1)$. The CDF is $F_S(s) = 1 - e^{-s/2}$. Inverting this gives $s = -2\\ln(1-U_1)$. As $1-U_1$ is also uniform on $(0,1)$, we can simplify this to $S = -2\\ln(U_1)$.\n\nThus, the full transformation is:\n$1.$ Generate $U_1, U_2 \\sim \\mathrm{Uniform}(0,1)$.\n$2.$ Compute $R = \\sqrt{-2\\ln U_1}$ and $\\Theta = 2\\pi U_2$.\n$3.$ Transform back to Cartesian coordinates to get the standard normal variates:\n$$Z_1 = R \\cos(\\Theta) = \\sqrt{-2\\ln U_1} \\cos(2\\pi U_2)$$\n$$Z_2 = R \\sin(\\Theta) = \\sqrt{-2\\ln U_1} \\sin(2\\pi U_2)$$\nThis method is free of branching.\n\n### 2. Marsaglia Polar Method: Derivation\nThis method avoids the trigonometric functions of the Box-Muller method.\n$1.$ Generate two independent uniform variates $V_1, V_2 \\sim \\mathrm{Uniform}(-1,1)$. This is done by drawing $U_1', U_2' \\sim \\mathrm{Uniform}(0,1)$ and setting $V_1=2U_1'-1$, $V_2=2U_2'-1$.\n$2.$ Calculate $S = V_1^2 + V_2^2$.\n$3.$ If $S=0$ or $S>1$, the point $(V_1, V_2)$ is rejected, and we return to step $1$. Otherwise, the point is accepted.\n\n**Acceptance Probability ($p$):**\nThe points $(V_1, V_2)$ are uniformly distributed in the square $[-1,1] \\times [-1,1]$, which has area $A_{\\mathrm{square}}=2 \\times 2 = 4$. The acceptance region is the unit disk $S \\le 1$, which has area $A_{\\mathrm{disk}}=\\pi (1)^2 = \\pi$. The acceptance probability $p$ is the ratio of these areas:\n$$p = \\frac{A_{\\mathrm{disk}}}{A_{\\mathrm{square}}} = \\frac{\\pi}{4}$$\n\n**Transformation:**\nAn accepted point $(V_1, V_2)$ is uniformly distributed in the unit disk. Let this point be $(X,Y)$. We want to find the distribution of its squared radius $S=X^2+Y^2$. The CDF of $R=\\sqrt{S}$ is $F_R(r) = P(R \\le r)$. For a uniform distribution on the unit disk, this probability is the ratio of the area of a disk of radius $r$ to the area of the unit disk: $F_R(r) = \\frac{\\pi r^2}{\\pi (1)^2} = r^2$ for $r \\in [0,1]$.\nThe CDF of $S=R^2$ is then $F_S(s) = P(S \\le s) = P(R^2 \\le s) = P(R \\le \\sqrt{s}) = (\\sqrt{s})^2 = s$ for $s \\in [0,1]$. This is the CDF of a $\\mathrm{Uniform}(0,1)$ distribution.\n\nTherefore, the value $S=V_1^2+V_2^2$ for an accepted pair is a random variate from $\\mathrm{Uniform}(0,1)$. We can use this $S$ in place of $U_1$ in the Box-Muller logic. We set a radial factor $\\sqrt{-2\\ln S}$. The angle $\\Theta$ of the point $(V_1, V_2)$ has $\\cos\\Theta = V_1/\\sqrt{S}$ and $\\sin\\Theta = V_2/\\sqrt{S}$. Substituting these into the Box-Muller formulas gives:\n$$Z_1 = \\sqrt{-2\\ln S} \\cos(\\Theta) = \\sqrt{-2\\ln S} \\frac{V_1}{\\sqrt{S}} = V_1 \\sqrt{\\frac{-2\\ln S}{S}}$$\n$$Z_2 = \\sqrt{-2\\ln S} \\sin(\\Theta) = \\sqrt{-2\\ln S} \\frac{V_2}{\\sqrt{S}} = V_2 \\sqrt{\\frac{-2\\ln S}{S}}$$\nThis transformation yields two independent standard normal variates without computing trigonometric functions. The cost is the accept/reject loop. The number of iterations $G_i$ for a thread $i$ to get its first acceptance follows a geometric distribution with success probability $p=\\pi/4$. The PDF is $P(G_i=k) = (1-p)^{k-1}p$ for $k \\in \\{1, 2, 3, \\dots\\}$.\n\n### 3. SIMT Warp Model: Derivations\nConsider a warp of $W$ threads, where thread $i$ requires $G_i$ iterations. The entire warp completes when the last thread finishes, which takes $M_W = \\max\\{G_1, \\dots, G_W\\}$ iterations.\n\n**Expectation of an upper bound on iterations, $\\mathbb{E}[M_W]$:**\nThe CDF of a single geometric variable $G_i$ is $F_G(k) = P(G_i \\le k) = \\sum_{j=1}^k (1-p)^{j-1}p = 1 - (1-p)^k$.\nSince the $G_i$ are independent and identically distributed, the CDF of their maximum $M_W$ is:\n$$F_{M_W}(k) = P(M_W \\le k) = P(G_1 \\le k, \\dots, G_W \\le k) = (F_G(k))^W = (1 - (1-p)^k)^W$$\nThe expectation of a random variable taking values in $\\{1, 2, \\dots\\}$ is $\\mathbb{E}[X] = \\sum_{k=1}^\\infty P(X \\ge k)$. This can be written as $\\mathbb{E}[X] = \\sum_{k=0}^\\infty (1 - F_X(k))$. For $M_W$:\n$$\\mathbb{E}[M_W] = \\sum_{k=0}^{\\infty} (1 - F_{M_W}(k)) = \\sum_{k=0}^{\\infty} \\left[1 - (1 - (1-p)^k)^W\\right]$$\nThis infinite series converges and will be computed numerically.\n\n**Initial Branch Divergence Probability, $D_0$:**\nIn the first iteration, all $W$ threads are active. Each thread either accepts (with probability $p$) or rejects (with probability $1-p$). The warp does not diverge if all threads make the same decision.\n$P(\\text{all accept}) = p^W$\n$P(\\text{all reject}) = (1-p)^W$\nThe probability of divergence is $D_0 = 1 - (P(\\text{all accept}) + P(\\text{all reject}))$.\n$$D_0 = 1 - (p^W + (1-p)^W)$$\n\n**Expected Iterations with Acceptances, $\\mathbb{E}[K_W]$:**\nA cost is incurred on iterations where at least one thread in the warp accepts. Let $K_W$ be the number of such iterations. We can find its expectation using linearity: $\\mathbb{E}[K_W] = \\sum_{k=1}^\\infty P(\\text{iteration } k \\text{ has at least one acceptance})$.\nThe event \"at least one thread accepts on iteration $k$\" is the union of events $\\{G_i=k\\}$ for $i=1, \\dots, W$. These events are independent.\n$P(G_i=k) = (1-p)^{k-1}p$.\n$P(\\text{at least one acceptance on iter } k) = 1 - P(\\text{no acceptances on iter } k) = 1 - \\prod_{i=1}^W P(G_i \\ne k)$.\n$P(G_i \\ne k) = 1 - (1-p)^{k-1}p$.\nSo, $P(\\text{at least one acceptance on iter } k) = 1 - (1 - (1-p)^{k-1}p)^W$.\nThe expectation is the sum over all possible iterations:\n$$\\mathbb{E}[K_W] = \\sum_{k=1}^{\\infty} \\left[1 - (1 - (1-p)^{k-1}p)^W\\right]$$\nThis is also a convergent series to be computed numerically.\n\n### 4. Cost Model and Final Calculations\nThe quantities to be calculated are defined as follows:\n- **Acceptance probability:** $p = \\pi/4$.\n- **Expected warp iterations:** $\\mathbb{E}[M_W] = \\sum_{k=0}^{\\infty} [1 - (1 - (1-p)^k)^W]$.\n- **Initial divergence probability:** $D_0 = 1 - p^W - (1-p)^W$.\n- **Occupancy overhead factor:** $F_{\\mathrm{occ}} = \\mathbb{E}[M_W] \\, p$. This measures the slowdown relative to an idealized single-thread case where the mean number of iterations is $1/p$.\n- **Box-Muller warp time:** $T_{\\mathrm{BM}} = 2 c_{\\mathrm{uni}} + c_{\\log\\sqrt} + c_{\\mathrm{trigpair}} + 4 c_{\\mathrm{basic}}$.\n- **Marsaglia polar warp time:** $T_{\\mathrm{MP}} = \\mathbb{E}[M_W]\\,(2 c_{\\mathrm{uni}} + 7 c_{\\mathrm{basic}}) + \\mathbb{E}[K_W]\\,(c_{\\log\\sqrt} + 4 c_{\\mathrm{basic}})$.\n- **Ratio:** $R = T_{\\mathrm{MP}}/T_{\\mathrm{BM}}$.\n\nThese quantities will be computed for each test case.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Derives and compares the Box-Muller and Marsaglia polar methods for\n    sampling from a standard normal distribution under a SIMT GPU execution model.\n    \"\"\"\n\n    test_cases = [\n        # (W, c_uni, c_basic, c_logsqrt, c_trigpair)\n        (1, 1.0, 0.25, 10.0, 8.0),   # Case A: Single thread\n        (8, 1.0, 0.25, 10.0, 8.0),   # Case B: Small warp\n        (32, 1.0, 0.25, 10.0, 8.0),  # Case C: NVIDIA warp\n        (64, 1.0, 0.25, 10.0, 8.0),  # Case D: AMD wavefront\n    ]\n\n    results = []\n    \n    # Acceptance probability for Marsaglia polar method\n    p = np.pi / 4.0\n\n    # Numerical summation tolerance\n    tolerance = 1e-17\n\n    for case in test_cases:\n        W, c_uni, c_basic, c_logsqrt, c_trigpair = case\n\n        # Calculate E[M_W]\n        # E[M_W] = sum_{k=0 to inf} (1 - (1 - (1-p)^k)^W)\n        q = 1.0 - p\n        expected_m_w = 0.0\n        k = 0\n        while True:\n            term = 1.0 - (1.0 - q**k)**W\n            if k > 0 and term  tolerance:\n                break\n            expected_m_w += term\n            k += 1\n            # Safety break for very slow convergence, though not expected here\n            if k > 20000:\n                break\n        \n        # Calculate E[K_W]\n        # E[K_W] = sum_{k=1 to inf} [1 - (1 - (1-p)^(k-1)*p)^W]\n        expected_k_w = 0.0\n        k = 1\n        while True:\n            term_base = 1.0 - (q**(k - 1)) * p\n            term = 1.0 - term_base**W\n            if k > 1 and term  tolerance:\n                break\n            expected_k_w += term\n            k += 1\n            if k > 20000:\n                break\n\n        # Calculate initial divergence probability D_0\n        d_0 = 1.0 - (p**W + (1.0 - p)**W)\n\n        # Calculate occupancy overhead factor F_occ\n        f_occ = expected_m_w * p\n        \n        # Calculate Box-Muller warp time T_BM\n        t_bm = 2 * c_uni + c_logsqrt + c_trigpair + 4 * c_basic\n\n        # Calculate Marsaglia polar warp time T_MP\n        t_mp_loop = expected_m_w * (2 * c_uni + 7 * c_basic)\n        t_mp_accept = expected_k_w * (c_logsqrt + 4 * c_basic)\n        t_mp = t_mp_loop + t_mp_accept\n        \n        # Calculate the ratio R\n        r = t_mp / t_bm\n\n        case_results = [\n            p,\n            expected_m_w,\n            d_0,\n            f_occ,\n            t_bm,\n            t_mp,\n            r\n        ]\n        results.append(case_results)\n\n    # Format output as specified\n    formatted_results = []\n    for res_list in results:\n        formatted_list = f\"[{','.join(f'{x:.7f}' for x in res_list)}]\"\n        formatted_results.append(formatted_list)\n    \n    print(f\"[{','.join(formatted_results)}]\")\n\n\nsolve()\n```", "id": "3532699"}, {"introduction": "While libraries provide samplers for common distributions, physicists often need to sample from distributions derived from first-principles models. This practice focuses on the powerful inverse transform sampling method, a general technique for drawing samples from any distribution whose cumulative distribution function ($CDF$) can be inverted. You will first derive the probability distribution for transverse momentum, $p_T$, a key observable in collider physics, and then implement a sampler based on its inverse $CDF$, including considerations for numerical stability and statistical validation [@problem_id:3532740].", "problem": "You are modeling isotropic soft background in a collider event, where the transverse momentum components $p_{x}$ and $p_{y}$ of a reconstructed track are modeled as independent and identically distributed Gaussian random variables with zero mean and variance $\\sigma^{2}$. Treat all variables as dimensionless for this exercise. Starting from the joint density of independent Gaussian variables and the change-of-variables theorem, derive the distribution of the transverse momentum magnitude $p_{T} = \\sqrt{p_{x}^{2} + p_{y}^{2}}$. Your derivation must begin from the fundamental definition of the joint density of independent Gaussian variables and the Jacobian determinant for the transformation to polar coordinates, without using any pre-stated form of the target distribution. Then, obtain the cumulative distribution function (CDF) and derive its inverse to enable inverse transform sampling of $p_{T}$ from a Uniform distribution on $[0,1)$.\n\nImplement two samplers for $p_{T}$:\n- A sampler by transforming Gaussian components: draw $p_{x} \\sim \\mathcal{N}(0,\\sigma^{2})$ and $p_{y} \\sim \\mathcal{N}(0,\\sigma^{2})$ independently, then return $p_{T}=\\sqrt{p_{x}^{2}+p_{y}^{2}}$.\n- An inverse transform sampler: draw $U \\sim \\mathrm{Uniform}[0,1)$ and transform $U$ using the inverse Cumulative Distribution Function (CDF) you derived. For numerical stability near $U \\approx 0$, implement this using a formulation based on $\\log1p$ to avoid catastrophic cancellation.\n\nUse the following test suite with fixed seeds and sample sizes. All random number generation must use a modern pseudorandom number generator with a fixed seed to ensure reproducibility.\n\nDefinitions for reference:\n- The Probability Density Function (PDF) of $\\mathcal{N}(0,\\sigma^{2})$ is $f(x) = \\dfrac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\!\\left(-\\dfrac{x^{2}}{2\\sigma^{2}}\\right)$.\n- The Cumulative Distribution Function (CDF) is the integral of the PDF from $-\\infty$ to the argument.\n- The Kolmogorov–Smirnov (KS) statistic is the supremum of the absolute difference between the empirical CDF and the theoretical CDF; the Kolmogorov–Smirnov (KS) test provides a $p$-value for goodness-of-fit.\n\nTest suite and required outputs:\n- Case A (mean check): Using the inverse transform sampler with $\\sigma = 1.25$, sample size $N = 2\\times 10^{5}$, and seed $s_{1} = 123456$. Compute the empirical mean $\\hat{\\mu}$ and compare to the theoretical mean $\\mu_{\\mathrm{th}}$ of the derived distribution. Output a boolean indicating whether the relative error $\\left|\\hat{\\mu}-\\mu_{\\mathrm{th}}\\right|/\\mu_{\\mathrm{th}}$ is less than $\\epsilon_{\\mu} = 0.01$ (expressed as a decimal).\n- Case B (variance check): With the same sample as in Case A, compute the empirical variance $\\widehat{\\mathrm{Var}}$ and compare to the theoretical variance $\\mathrm{Var}_{\\mathrm{th}}$. Output a boolean indicating whether the relative error $\\left|\\widehat{\\mathrm{Var}}-\\mathrm{Var}_{\\mathrm{th}}\\right|/\\mathrm{Var}_{\\mathrm{th}}$ is less than $\\epsilon_{\\mathrm{var}} = 0.02$ (expressed as a decimal).\n- Case C (one-sample Kolmogorov–Smirnov goodness-of-fit): Using the same sample as in Case A, compute the Kolmogorov–Smirnov $p$-value for the null hypothesis that the sample follows the derived distribution with the given $\\sigma$. Output the $p$-value as a float rounded to $6$ decimal places.\n- Case D (inverse transform numerical stability): For $\\sigma = 2.0$ and the Uniform inputs $U \\in \\{0,\\;10^{-12},\\;0.5,\\;1-10^{-12}\\}$, compute the inverse transform sampler outputs and return these four values as a list of floats, each rounded to $6$ decimal places, in the order corresponding to the listed $U$ values.\n- Case E (two-sample distributional consistency): Let $\\sigma = 0.8$, and $N = 1.5\\times 10^{5}$. Generate one sample by the Gaussian-components method with seed $s_{2} = 20231017$, and another independent sample by the inverse transform sampler with seed $s_{3} = 424242$. Compute the two-sample Kolmogorov–Smirnov statistic between these two samples. Output a boolean indicating whether this statistic is less than $\\delta = 0.02$ (expressed as a decimal).\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with the entries in the following order:\n$[\\text{mean\\_ok},\\text{var\\_ok},\\text{ks\\_p\\_value},\\text{inverse\\_values},\\text{twosample\\_ok}]$,\nwhere $\\text{mean\\_ok}$, $\\text{var\\_ok}$, and $\\text{twosample\\_ok}$ are booleans; $\\text{ks\\_p\\_value}$ is a float rounded to $6$ decimal places; and $\\text{inverse\\_values}$ is a list of four floats each rounded to $6$ decimal places. For example, a syntactically correct line would look like $[{\\rm True},{\\rm True},0.123456,[0.000000,1.234567,2.345678,9.876543],{\\rm True}]$ (the numbers here are illustrative only).", "solution": "The problem requires the derivation of the probability distribution for the transverse momentum magnitude $p_{T}$, followed by the implementation and testing of two sampling methods. The derivation must start from first principles.\n\n### Part 1: Derivation of the $p_{T}$ Distribution\n\nWe are given that the transverse momentum components $p_x$ and $p_y$ are independent and identically distributed (i.i.d.) random variables from a Gaussian distribution with mean $0$ and variance $\\sigma^2$. The probability density function (PDF) for such a variable, say $z$, is:\n$$\nf(z; \\sigma) = \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left(-\\frac{z^2}{2\\sigma^2}\\right)\n$$\nSince $p_x$ and $p_y$ are independent, their joint PDF, $f(p_x, p_y)$, is the product of their individual PDFs:\n$$\nf(p_x, p_y) = f(p_x; \\sigma) f(p_y; \\sigma) = \\left(\\frac{1}{\\sqrt{2\\pi}\\sigma}\\right) \\exp\\left(-\\frac{p_x^2}{2\\sigma^2}\\right) \\left(\\frac{1}{\\sqrt{2\\pi}\\sigma}\\right) \\exp\\left(-\\frac{p_y^2}{2\\sigma^2}\\right)\n$$\n$$\nf(p_x, p_y) = \\frac{1}{2\\pi\\sigma^2} \\exp\\left(-\\frac{p_x^2 + p_y^2}{2\\sigma^2}\\right)\n$$\nThe transverse momentum magnitude is defined as $p_T = \\sqrt{p_x^2 + p_y^2}$. To find the distribution of $p_T$, we perform a change of variables from Cartesian coordinates $(p_x, p_y)$ to polar coordinates $(p_T, \\theta)$, where:\n$$\np_x = p_T \\cos\\theta\n$$\n$$\np_y = p_T \\sin\\theta\n$$\nThe transformation ranges are $p_T \\in [0, \\infty)$ and $\\theta \\in [0, 2\\pi)$. The change of variables theorem states that the new joint PDF, $g(p_T, \\theta)$, is related to the original PDF $f(p_x, p_y)$ by the absolute value of the Jacobian determinant of the transformation, $|J|$.\n$$\ng(p_T, \\theta) = f(p_x(p_T, \\theta), p_y(p_T, \\theta)) |J|\n$$\nThe Jacobian determinant $J$ is:\n$$\nJ = \\det \\begin{pmatrix} \\frac{\\partial p_x}{\\partial p_T}  \\frac{\\partial p_x}{\\partial \\theta} \\\\ \\frac{\\partial p_y}{\\partial p_T}  \\frac{\\partial p_y}{\\partial \\theta} \\end{pmatrix} = \\det \\begin{pmatrix} \\cos\\theta  -p_T \\sin\\theta \\\\ \\sin\\theta  p_T \\cos\\theta \\end{pmatrix} = (p_T \\cos^2\\theta) - (-p_T \\sin^2\\theta) = p_T(\\cos^2\\theta + \\sin^2\\theta) = p_T\n$$\nSince $p_T \\ge 0$, we have $|J| = p_T$. We also note that $p_x^2 + p_y^2 = (p_T\\cos\\theta)^2 + (p_T\\sin\\theta)^2 = p_T^2$. Substituting these into the transformation formula gives the joint PDF for $p_T$ and $\\theta$:\n$$\ng(p_T, \\theta) = \\frac{1}{2\\pi\\sigma^2} \\exp\\left(-\\frac{p_T^2}{2\\sigma^2}\\right) p_T\n$$\nTo find the marginal PDF for $p_T$, which we denote $h(p_T)$, we integrate the joint PDF $g(p_T, \\theta)$ over all possible values of $\\theta$:\n$$\nh(p_T) = \\int_0^{2\\pi} g(p_T, \\theta) \\,d\\theta = \\int_0^{2\\pi} \\frac{p_T}{2\\pi\\sigma^2} \\exp\\left(-\\frac{p_T^2}{2\\sigma^2}\\right) \\,d\\theta\n$$\nThe integrand is independent of $\\theta$, so we have:\n$$\nh(p_T) = \\frac{p_T}{2\\pi\\sigma^2} \\exp\\left(-\\frac{p_T^2}{2\\sigma^2}\\right) \\int_0^{2\\pi} 1 \\,d\\theta = \\frac{p_T}{2\\pi\\sigma^2} \\exp\\left(-\\frac{p_T^2}{2\\sigma^2}\\right) [2\\pi]\n$$\n$$\nh(p_T) = \\frac{p_T}{\\sigma^2} \\exp\\left(-\\frac{p_T^2}{2\\sigma^2}\\right), \\quad \\text{for } p_T \\ge 0\n$$\nThis is the PDF of the Rayleigh distribution with scale parameter $\\sigma$.\n\n### Part 2: Cumulative Distribution Function (CDF) and Inverse Transform Sampling\n\nThe CDF, $F(p_T)$, is the integral of the PDF from $0$ to a value $p_T$:\n$$\nF(p_T) = \\int_0^{p_T} h(t) \\,dt = \\int_0^{p_T} \\frac{t}{\\sigma^2} \\exp\\left(-\\frac{t^2}{2\\sigma^2}\\right) \\,dt\n$$\nWe use the substitution $u = \\frac{t^2}{2\\sigma^2}$, which implies $du = \\frac{2t}{2\\sigma^2} dt = \\frac{t}{\\sigma^2} dt$. The limits of integration change from $t=0$ to $u=0$ and from $t=p_T$ to $u = p_T^2/(2\\sigma^2)$.\n$$\nF(p_T) = \\int_0^{p_T^2/(2\\sigma^2)} e^{-u} \\,du = \\left[-e^{-u}\\right]_0^{p_T^2/(2\\sigma^2)} = -e^{-p_T^2/(2\\sigma^2)} - (-e^0) = 1 - \\exp\\left(-\\frac{p_T^2}{2\\sigma^2}\\right)\n$$\nFor inverse transform sampling, we set the CDF equal to a random variable $U$ drawn from a uniform distribution on $[0,1)$, i.e., $U = F(p_T)$, and solve for $p_T$:\n$$\nU = 1 - \\exp\\left(-\\frac{p_T^2}{2\\sigma^2}\\right)\n$$\n$$\n1-U = \\exp\\left(-\\frac{p_T^2}{2\\sigma^2}\\right)\n$$\n$$\n\\ln(1-U) = -\\frac{p_T^2}{2\\sigma^2}\n$$\n$$\np_T^2 = -2\\sigma^2 \\ln(1-U)\n$$\n$$\np_T = \\sqrt{-2\\sigma^2 \\ln(1-U)}\n$$\nThis is the inverse CDF, $p_T = F^{-1}(U)$. For numerical stability when $U$ is close to $0$, the term $\\ln(1-U)$ can suffer from catastrophic cancellation. We use the function $\\mathrm{log1p}(x) = \\ln(1+x)$, which is implemented to be accurate for small $|x|$. We can write $\\ln(1-U)$ as $\\mathrm{log1p}(-U)$. The numerically stable formula is:\n$$\np_T = \\sqrt{-2\\sigma^2 \\mathrm{log1p}(-U)}\n$$\n\n### Part 3: Theoretical Moments\n\nFor the validation tests, we need the theoretical mean ($\\mu_{\\mathrm{th}}$) and variance ($\\mathrm{Var}_{\\mathrm{th}}$) of the Rayleigh distribution.\nThe mean is the expected value of $p_T$:\n$$\n\\mu_{\\mathrm{th}} = E[p_T] = \\int_0^\\infty p_T h(p_T) \\,dp_T = \\int_0^\\infty \\frac{p_T^2}{\\sigma^2} \\exp\\left(-\\frac{p_T^2}{2\\sigma^2}\\right) \\,dp_T\n$$\nThis integral evaluates to:\n$$\n\\mu_{\\mathrm{th}} = \\sigma \\sqrt{\\frac{\\pi}{2}}\n$$\nThe second moment is the expected value of $p_T^2$:\n$$\nE[p_T^2] = \\int_0^\\infty p_T^2 h(p_T) \\,dp_T = \\int_0^\\infty \\frac{p_T^3}{\\sigma^2} \\exp\\left(-\\frac{p_T^2}{2\\sigma^2}\\right) \\,dp_T\n$$\nThis integral evaluates to:\n$$\nE[p_T^2] = 2\\sigma^2\n$$\nThe variance is then given by $\\mathrm{Var}_{\\mathrm{th}} = E[p_T^2] - (E[p_T])^2$:\n$$\n\\mathrm{Var}_{\\mathrm{th}} = 2\\sigma^2 - \\left(\\sigma\\sqrt{\\frac{\\pi}{2}}\\right)^2 = 2\\sigma^2 - \\sigma^2 \\frac{\\pi}{2} = \\sigma^2\\left(2 - \\frac{\\pi}{2}\\right)\n$$\nThese formulae will be used to validate the generated samples.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import stats\n\ndef sampler_gauss_components(sigma, n_samples, rng):\n    \"\"\"\n    Generates pT samples by transforming Gaussian-distributed components.\n    p_x, p_y ~ N(0, sigma^2)\n    pT = sqrt(p_x^2 + p_y^2)\n    \"\"\"\n    px = rng.normal(loc=0.0, scale=sigma, size=n_samples)\n    py = rng.normal(loc=0.0, scale=sigma, size=n_samples)\n    return np.sqrt(px**2 + py**2)\n\ndef sampler_inverse_transform(sigma, n_samples, rng, u_values=None):\n    \"\"\"\n    Generates pT samples using inverse transform sampling from the derived CDF.\n    Uses log1p for numerical stability.\n    If u_values is provided, it transforms those instead of generating new ones.\n    \"\"\"\n    if u_values is None:\n        u_values = rng.uniform(low=0.0, high=1.0, size=n_samples)\n    \n    # The term np.log1p(-u_values) is equivalent to np.log(1-u_values) but\n    # provides higher precision for small u_values.\n    return np.sqrt(-2 * sigma**2 * np.log1p(-u_values))\n\ndef solve():\n    \"\"\"\n    Performs all derivations, implementations, and tests as specified in the problem.\n    \"\"\"\n    \n    results = []\n\n    # Case A  B  C setup\n    sigma_A = 1.25\n    N_A = 2 * 10**5\n    seed_1 = 123456\n    rng_A = np.random.default_rng(seed_1)\n    \n    # Generate one sample for cases A, B, C\n    samples_A = sampler_inverse_transform(sigma_A, N_A, rng_A)\n\n    # --- Case A: Mean Check ---\n    mu_th = sigma_A * np.sqrt(np.pi / 2.0)\n    mu_hat = np.mean(samples_A)\n    rel_error_mu = np.abs(mu_hat - mu_th) / mu_th\n    epsilon_mu = 0.01\n    mean_ok = rel_error_mu  epsilon_mu\n    \n    # --- Case B: Variance Check ---\n    var_th = sigma_A**2 * (2.0 - np.pi / 2.0)\n    var_hat = np.var(samples_A)\n    rel_error_var = np.abs(var_hat - var_th) / var_th\n    epsilon_var = 0.02\n    var_ok = rel_error_var  epsilon_var\n\n    # --- Case C: One-sample KS Goodness-of-Fit ---\n    # The derived distribution is a Rayleigh distribution with scale=sigma.\n    # We use scipy.stats.rayleigh for the theoretical CDF.\n    # The parameters for scipy.stats.rayleigh are (loc, scale).\n    _, ks_p_value = stats.kstest(samples_A, 'rayleigh', args=(0, sigma_A))\n\n    # --- Case D: Inverse Transform Numerical Stability ---\n    sigma_D = 2.0\n    u_values_D = np.array([0.0, 1e-12, 0.5, 1.0 - 1e-12])\n    inverse_values = sampler_inverse_transform(sigma_D, n_samples=None, rng=None, u_values=u_values_D)\n\n    # --- Case E: Two-sample Distributional Consistency ---\n    sigma_E = 0.8\n    N_E = int(1.5 * 10**5)\n    seed_2 = 20231017\n    seed_3 = 424242\n    delta = 0.02\n\n    rng_E2 = np.random.default_rng(seed_2)\n    sample_E_gauss = sampler_gauss_components(sigma_E, N_E, rng_E2)\n\n    rng_E3 = np.random.default_rng(seed_3)\n    sample_E_inv = sampler_inverse_transform(sigma_E, N_E, rng_E3)\n\n    ks_stat_2samp, _ = stats.ks_2samp(sample_E_gauss, sample_E_inv)\n    twosample_ok = ks_stat_2samp  delta\n\n    # --- Format and Print Final Output ---\n    # Format the list of floats for Case D as a string to match the required output style\n    inverse_values_str = f\"[{','.join([f'{v:.6f}' for v in inverse_values])}]\"\n\n    # Assemble the final result string\n    final_output_str = (\n        f\"[{mean_ok},\"\n        f\"{var_ok},\"\n        f\"{ks_p_value:.6f},\"\n        f\"{inverse_values_str},\"\n        f\"{twosample_ok}]\"\n    )\n    \n    # Capitalize booleans as required\n    print(final_output_str.replace(\"True\", \"True\").replace(\"False\", \"False\"))\n\nsolve()\n```", "id": "3532740"}, {"introduction": "When the inverse $CDF$ of a target distribution is unknown or intractable, rejection sampling provides an elegant and exact alternative. However, its efficiency hinges critically on the choice of the proposal distribution. This exercise challenges you to design and optimize a rejection sampler for a truncated Cauchy distribution, a model relevant to resonance physics, by analytically minimizing the rejection constant [@problem_id:3532780]. This process will give you a deep appreciation for the theoretical underpinnings of sampler efficiency and how to maximize performance for non-standard probability densities.", "problem": "In unbinned resonance modeling for computational high-energy physics, a common line-shape is the symmetric Breit–Wigner, which coincides with a Cauchy distribution. Suppose a detector-level acceptance enforces a symmetric window of half-width $Lgt;0$ in units of the resonance width, so that the target sampling density is the truncated Cauchy on $\\left[-L,L\\right]$ with probability density function $f(x)\\propto \\frac{1}{1+x^{2}}$ for $|x|\\leq L$ and $f(x)=0$ for $|x|gt;L$. You are tasked with designing an efficient independent rejection sampler for $f$ using as an envelope a scale family of standard distributions that are easily sampled in high-energy Monte Carlo pipelines.\n\nStarting from the definition of rejection sampling and basic calculus, consider the envelope family $g_{\\sigma}(x)=\\frac{1}{\\pi\\,\\sigma\\left(1+\\left(x/\\sigma\\right)^{2}\\right)}$ for $\\sigmagt;0$, which is the Cauchy distribution of scale $\\sigma$ on $\\mathbb{R}$. For a given $L$, determine the rejection constant $M(\\sigma,L)$ such that $f(x)\\leq M(\\sigma,L)\\,g_{\\sigma}(x)$ for all $x\\in\\mathbb{R}$, and then minimize $M(\\sigma,L)$ over $\\sigmagt;0$. From first principles:\n\n- Normalize $f$ on $\\left[-L,L\\right]$.\n- Derive $M(\\sigma,L)=\\sup_{x\\in\\mathbb{R}}\\frac{f(x)}{g_{\\sigma}(x)}$.\n- Minimize $M(\\sigma,L)$ over $\\sigmagt;0$ to obtain the optimal value $M^{\\star}(L)$ and the corresponding acceptance probability $A^{\\star}(L)=\\frac{1}{M^{\\star}(L)}$.\n\nFor comparison, the tailored transformation method that uses the inverse cumulative distribution function of the truncated Cauchy has unit acceptance probability. Using this fact, compute the ratio $R(L)$ of expected proposal counts between the optimized rejection sampler and the tailored transformation method, under the standard cost model where the number of proposals is inversely proportional to the acceptance probability.\n\nExpress your final answer as a single row matrix containing, in order, the minimal rejection constant $M^{\\star}(L)$, the corresponding acceptance probability $A^{\\star}(L)$, and the proposal-count ratio $R(L)$, all as exact analytic functions of $L$. No units are required, and no numerical rounding is needed.", "solution": "The user has provided a problem in computational physics concerning the optimization of a rejection sampler for a truncated Cauchy distribution. The problem is valid as it is scientifically grounded in standard statistical methods, well-posed, objective, and internally consistent. I will proceed with a full derivation.\n\nThe problem asks for three quantities: the minimal rejection constant $M^{\\star}(L)$, the corresponding acceptance probability $A^{\\star}(L)$, and a performance ratio $R(L)$, all as functions of the truncation half-width $L$.\n\n**Step 1: Normalize the target probability density function (PDF)**\n\nThe target distribution has a PDF $f(x)$ proportional to $\\frac{1}{1+x^2}$ on the interval $\\left[-L, L\\right]$ and is zero elsewhere. First, we must find the normalization constant, $C$.\n$$\nC = \\int_{-L}^{L} \\frac{1}{1+x^2} \\, dx\n$$\nThe antiderivative of $\\frac{1}{1+x^2}$ is $\\arctan(x)$.\n$$\nC = \\left[ \\arctan(x) \\right]_{-L}^{L} = \\arctan(L) - \\arctan(-L)\n$$\nSince $\\arctan(x)$ is an odd function, $\\arctan(-L) = -\\arctan(L)$.\n$$\nC = \\arctan(L) - (-\\arctan(L)) = 2\\arctan(L)\n$$\nThe normalized PDF $f(x)$ is therefore:\n$$\nf(x) = \\begin{cases} \\frac{1}{2\\arctan(L)} \\frac{1}{1+x^2}  \\text{if } |x| \\leq L \\\\ 0  \\text{if } |x|  L \\end{cases}\n$$\n\n**Step 2: Derive the rejection constant $M(\\sigma, L)$**\n\nThe proposal distribution is the Cauchy distribution $g_{\\sigma}(x)$ with scale parameter $\\sigma  0$:\n$$\ng_{\\sigma}(x) = \\frac{1}{\\pi\\sigma\\left(1 + (x/\\sigma)^2\\right)}\n$$\nThe rejection constant $M(\\sigma, L)$ is defined as the supremum of the ratio of the target PDF to the proposal PDF over the entire real line.\n$$\nM(\\sigma, L) = \\sup_{x \\in \\mathbb{R}} \\frac{f(x)}{g_{\\sigma}(x)}\n$$\nSince $f(x)=0$ for $|x|  L$, we only need to consider $x \\in [-L, L]$. For $x$ in this interval, the ratio is:\n$$\n\\frac{f(x)}{g_{\\sigma}(x)} = \\frac{\\frac{1}{2\\arctan(L)(1+x^2)}}{\\frac{1}{\\pi\\sigma(1+x^2/\\sigma^2)}} = \\frac{\\pi\\sigma}{2\\arctan(L)} \\frac{1+x^2/\\sigma^2}{1+x^2}\n$$\nTo find the supremum, we must maximize the term that depends on $x$, which we denote $q(x, \\sigma)$:\n$$\nq(x, \\sigma) = \\frac{1+x^2/\\sigma^2}{1+x^2}\n$$\nSince $q(x, \\sigma)$ is an even function of $x$, we can analyze its behavior for $x \\in [0, L]$. We compute the derivative with respect to $x$:\n$$\n\\frac{dq}{dx} = \\frac{(2x/\\sigma^2)(1+x^2) - (1+x^2/\\sigma^2)(2x)}{(1+x^2)^2} = \\frac{2x(1/\\sigma^2 - 1)}{(1+x^2)^2} = \\frac{2x(1-\\sigma^2)}{\\sigma^2(1+x^2)^2}\n$$\nThe sign of the derivative for $x0$ is determined by the sign of $1-\\sigma^2$.\n- If $\\sigma^2  1$ (i.e., $0  \\sigma  1$), then $1-\\sigma^2  0$, so $\\frac{dq}{dx}  0$. The function $q(x, \\sigma)$ is increasing on $[0, L]$. The maximum occurs at $x=L$.\n- If $\\sigma^2  1$ (i.e., $\\sigma  1$), then $1-\\sigma^2  0$, so $\\frac{dq}{dx}  0$. The function $q(x, \\sigma)$ is decreasing on $[0, L]$. The maximum occurs at $x=0$.\n- If $\\sigma^2 = 1$ (i.e., $\\sigma = 1$), then $\\frac{dq}{dx}=0$. The function $q(x, \\sigma)$ is constant, $q(x,1)=1$.\n\nThe supremum of $q(x, \\sigma)$ on $[-L,L]$ is:\n$$\n\\sup_{x \\in [-L,L]} q(x, \\sigma) = \\begin{cases} q(L, \\sigma) = \\frac{1+L^2/\\sigma^2}{1+L^2}  \\text{if } 0  \\sigma \\leq 1 \\\\ q(0, \\sigma) = 1  \\text{if } \\sigma  1 \\end{cases}\n$$\nSubstituting this back into the expression for $M(\\sigma, L)$:\n$$\nM(\\sigma, L) = \\frac{\\pi\\sigma}{2\\arctan(L)} \\times \\begin{cases} \\frac{1+L^2/\\sigma^2}{1+L^2}  \\text{if } 0  \\sigma \\leq 1 \\\\ 1  \\text{if } \\sigma  1 \\end{cases}\n$$\nFor $0  \\sigma \\leq 1$:\n$$\nM(\\sigma, L) = \\frac{\\pi\\sigma}{2\\arctan(L)(1+L^2)} \\left(\\frac{\\sigma^2+L^2}{\\sigma^2}\\right) = \\frac{\\pi}{2\\arctan(L)(1+L^2)} \\left(\\sigma + \\frac{L^2}{\\sigma}\\right)\n$$\nFor $\\sigma  1$:\n$$\nM(\\sigma, L) = \\frac{\\pi\\sigma}{2\\arctan(L)}\n$$\n\n**Step 3: Minimize $M(\\sigma, L)$ over $\\sigma  0$**\n\nWe now minimize $M(\\sigma, L)$ with respect to $\\sigma$ to find the optimal rejection constant $M^{\\star}(L)$. We analyze the two cases for $\\sigma$.\n\nCase 1: $0  \\sigma \\leq 1$. Let $M_1(\\sigma) = \\frac{\\pi}{2\\arctan(L)(1+L^2)} \\left(\\sigma + \\frac{L^2}{\\sigma}\\right)$.\nWe find the minimum by taking the derivative with respect to $\\sigma$:\n$$\n\\frac{dM_1}{d\\sigma} = \\frac{\\pi}{2\\arctan(L)(1+L^2)} \\left(1 - \\frac{L^2}{\\sigma^2}\\right)\n$$\nSetting the derivative to zero gives $\\sigma^2=L^2$, so $\\sigma=L$ (since $\\sigma, L  0$).\n- If $0  L \\leq 1$, the critical point $\\sigma=L$ is in the interval $(0, 1]$. The second derivative test confirms this is a minimum.\n- If $L  1$, then for any $\\sigma \\in (0,1]$, we have $\\sigma  L$, so $1 - L^2/\\sigma^2  0$. $M_1(\\sigma)$ is a decreasing function on $(0,1]$. The minimum on this interval occurs at the boundary $\\sigma=1$.\n\nCase 2: $\\sigma  1$. Let $M_2(\\sigma) = \\frac{\\pi\\sigma}{2\\arctan(L)}$. This is a linear function of $\\sigma$ with a positive slope. It is strictly increasing for $\\sigma  1$. Its minimum value on this interval is approached as $\\sigma \\to 1^+$.\n\nCombining these results to find the global minimum $M^{\\star}(L) = \\min_{\\sigma0} M(\\sigma, L)$:\n- If $0  L \\leq 1$: The minimum on $(0, 1]$ is at $\\sigma=L$, with value $M(L, L) = \\frac{\\pi}{2\\arctan(L)(1+L^2)}(L+L^2/L) = \\frac{\\pi L}{\\arctan(L)(1+L^2)}$. The function is increasing for $\\sigma  1$ from the value $M(1, L) = \\frac{\\pi}{2\\arctan(L)}$. Since $\\frac{2L}{1+L^2} \\leq 1$ for all $L0$ (and equality holds only at $L=1$), we have $\\frac{\\pi L}{\\arctan(L)(1+L^2)} \\leq \\frac{\\pi}{2\\arctan(L)}$. Thus, the global minimum is at $\\sigma=L$.\n- If $L  1$: The minimum on $(0, 1]$ occurs at $\\sigma=1$. For $\\sigma  1$, the function is increasing. Thus, the global minimum is at $\\sigma=1$. The value is $M(1, L) = \\frac{\\pi}{2\\arctan(L)}$.\n\nSo, the minimal rejection constant $M^{\\star}(L)$ is:\n$$\nM^{\\star}(L) = \\begin{cases} \\frac{\\pi L}{(1+L^2)\\arctan(L)}  \\text{if } 0  L \\leq 1 \\\\ \\frac{\\pi}{2\\arctan(L)}  \\text{if } L  1 \\end{cases}\n$$\n\n**Step 4: Determine the acceptance probability $A^{\\star}(L)$ and the ratio $R(L)$**\n\nThe maximum acceptance probability for the rejection sampler is $A^{\\star}(L) = \\frac{1}{M^{\\star}(L)}$.\n$$\nA^{\\star}(L) = \\begin{cases} \\frac{(1+L^2)\\arctan(L)}{\\pi L}  \\text{if } 0  L \\leq 1 \\\\ \\frac{2\\arctan(L)}{\\pi}  \\text{if } L  1 \\end{cases}\n$$\nThe ratio $R(L)$ is the ratio of expected proposal counts. The expected number of proposals is the reciprocal of the acceptance probability. The tailored transformation method has an acceptance probability of $A_{\\text{trans}}=1$.\n$$\nR(L) = \\frac{\\text{Expected Proposals (Rejection Sampling)}}{\\text{Expected Proposals (Transformation Method)}} = \\frac{1/A^{\\star}(L)}{1/A_{\\text{trans}}} = \\frac{1/A^{\\star}(L)}{1} = \\frac{1}{A^{\\star}(L)}\n$$\nTherefore, $R(L) = M^{\\star}(L)$.\n\nThe three required quantities are:\n1. $M^{\\star}(L) = \\begin{cases} \\frac{\\pi L}{(1+L^2)\\arctan(L)}  \\text{if } 0  L \\leq 1 \\\\ \\frac{\\pi}{2\\arctan(L)}  \\text{if } L  1 \\end{cases}$\n2. $A^{\\star}(L) = \\begin{cases} \\frac{(1+L^2)\\arctan(L)}{\\pi L}  \\text{if } 0  L \\leq 1 \\\\ \\frac{2\\arctan(L)}{\\pi}  \\text{if } L  1 \\end{cases}$\n3. $R(L) = M^{\\star}(L) = \\begin{cases} \\frac{\\pi L}{(1+L^2)\\arctan(L)}  \\text{if } 0  L \\leq 1 \\\\ \\frac{\\pi}{2\\arctan(L)}  \\text{if } L  1 \\end{cases}$\n\nThese are collected into a single row matrix for the final answer.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\begin{cases} \\frac{\\pi L}{(1+L^2)\\arctan(L)}  \\text{if } 0  L \\leq 1 \\\\ \\frac{\\pi}{2\\arctan(L)}  \\text{if } L  1 \\end{cases}  \\begin{cases} \\frac{(1+L^2)\\arctan(L)}{\\pi L}  \\text{if } 0  L \\leq 1 \\\\ \\frac{2\\arctan(L)}{\\pi}  \\text{if } L  1 \\end{cases}  \\begin{cases} \\frac{\\pi L}{(1+L^2)\\arctan(L)}  \\text{if } 0  L \\leq 1 \\\\ \\frac{\\pi}{2\\arctan(L)}  \\text{if } L  1 \\end{cases}\n\\end{pmatrix}\n}\n$$", "id": "3532780"}]}