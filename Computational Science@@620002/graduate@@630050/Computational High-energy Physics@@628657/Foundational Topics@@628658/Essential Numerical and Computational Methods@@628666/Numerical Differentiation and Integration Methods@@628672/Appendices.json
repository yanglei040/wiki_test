{"hands_on_practices": [{"introduction": "The central-difference formula is a cornerstone of numerical differentiation, but its apparent simplicity belies a crucial subtlety in its practical application. The accuracy of the approximation is a delicate balance between truncation error, which decreases with smaller step sizes, and floating-point round-off error, which is amplified. This exercise guides you through a theoretical and numerical investigation to find the optimal step size, $h_{\\mathrm{opt}}$, that minimizes the total error, providing deep insight into the practical limits of finite-difference methods [@problem_id:3525181].", "problem": "Consider the numerical evaluation of derivatives that arises in computational high-energy physics when differentiating exponential factors in generating functionals and propagators. You will analyze the central finite-difference approximation for the derivative and determine the step size that balances truncation and floating-point round-off errors in Institute of Electrical and Electronics Engineers (IEEE) double precision arithmetic.\n\nStarting from first principles appropriate to numerical analysis and physics computation:\n- Use the Taylor expansion about a point for a smooth scalar function $f(x)$ and the definition of the derivative $f'(x)$.\n- Model floating-point arithmetic under IEEE double precision with unit roundoff $\\epsilon$ satisfying $\\epsilon \\approx 2^{-53}$.\n\nYour tasks are:\n1. Derive, from Taylor expansion and the definition of the derivative, the leading-order truncation error for the central difference formula\n$$\nD_h f(x) \\equiv \\frac{f(x+h) - f(x-h)}{2h}\n$$\nas an approximation to $f'(x)$, identifying an explicit leading coefficient multiplying $h^2$ in terms of derivatives of $f$ evaluated at $x$.\n2. Using a standard floating-point rounding model in which each elementary function evaluation incurs a relative error on the order of $\\epsilon$, and noting the subtraction of nearly equal quantities in the numerator, construct a leading-order rounding error model for $D_h f(x)$ of the form $C\\epsilon/h$ with a coefficient $C$ expressed in terms of $f(x)$.\n3. Combine the truncation and rounding error models into a single leading-order error estimate $E(h)$ and analytically minimize $E(h)$ with respect to $h$ to obtain a formula for the optimal step $h_{\\mathrm{opt}}$ in terms of $f(x)$, $f'''(x)$, and $\\epsilon$.\n4. Specialize your result to the case $f(x) = \\exp(k x)$ at $x=1$, where $k$ is a real parameter. Using IEEE double precision with $\\epsilon = 2^{-53}$, compute $h_{\\mathrm{opt}}$ numerically from your formula.\n5. For each parameter value, estimate the expected number of correct base-$10$ digits in $f'(1)$ when using the central difference approximation with your computed $h_{\\mathrm{opt}}$. Use the leading-order error model to obtain the expected relative error $\\delta_{\\mathrm{exp}}$, and then report the expected digit count as $-\\log_{10}(\\delta_{\\mathrm{exp}})$.\n6. Validate your model by actually computing the central difference approximation $D_{h_{\\mathrm{opt}}} f(1)$ in IEEE double precision and measuring the realized relative error $\\delta_{\\mathrm{meas}}$ against the exact derivative $f'(1)$, then reporting the measured digit count as $-\\log_{10}(\\delta_{\\mathrm{meas}})$.\n\nTest Suite:\n- Use the following parameter values for $k$ at $x=1$:\n    - Case A (general reference scale): $k = 1$.\n    - Case B (steep exponential relevant to stiff scales): $k = 10$.\n    - Case C (gentle exponential): $k = 0.1$.\n    - Case D (very steep exponential, but without overflow in double precision): $k = 100$.\n\nFor each case, compute and report:\n- The optimal step $h_{\\mathrm{opt}}$.\n- The expected digit count $-\\log_{10}(\\delta_{\\mathrm{exp}})$.\n- The measured digit count $-\\log_{10}(\\delta_{\\mathrm{meas}})$ computed using the central difference at $h_{\\mathrm{opt}}$.\n- A boolean that is $\\mathrm{True}$ if and only if the measured digit count differs from the expected digit count by no more than $0.5$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain, in order for Cases A, B, C, and D respectively, the quadruples flattened into a single list:\n$$\n[ h_A, d^{\\mathrm{exp}}_A, d^{\\mathrm{meas}}_A, \\mathrm{ok}_A, h_B, d^{\\mathrm{exp}}_B, d^{\\mathrm{meas}}_B, \\mathrm{ok}_B, h_C, d^{\\mathrm{exp}}_C, d^{\\mathrm{meas}}_C, \\mathrm{ok}_C, h_D, d^{\\mathrm{exp}}_D, d^{\\mathrm{meas}}_D, \\mathrm{ok}_D ]\n$$\nwhere each $h$ is a float, each $d$ is a float, and each $\\mathrm{ok}$ is a boolean. No physical units are required, and angles are not involved. Express all digit counts as real numbers (floats), not as percentages.", "solution": "The problem statement has been critically validated and is deemed to be scientifically grounded, well-posed, objective, and complete. It represents a standard and instructive exercise in the numerical analysis of scientific computations. We may therefore proceed with a formal solution.\n\nThe task is to analyze the central finite-difference approximation for the first derivative of a function $f(x)$, denoted $D_h f(x)$, balancing its inherent truncation error with the rounding error introduced by floating-point arithmetic.\n\n$$\nD_h f(x) \\equiv \\frac{f(x+h) - f(x-h)}{2h}\n$$\n\n### 1. Derivation of the Truncation Error\n\nThe truncation error is the mathematical error incurred by approximating the true derivative $f'(x)$ with the finite-difference formula, assuming exact arithmetic. We derive this error by expanding the function $f(x)$ in a Taylor series about the point $x$. For a sufficiently smooth function, the expansions for $f(x+h)$ and $f(x-h)$ are:\n\n$$\nf(x+h) = f(x) + hf'(x) + \\frac{h^2}{2!}f''(x) + \\frac{h^3}{3!}f'''(x) + \\frac{h^4}{4!}f^{(4)}(x) + \\frac{h^5}{5!}f^{(5)}(x) + \\mathcal{O}(h^6)\n$$\n$$\nf(x-h) = f(x) - hf'(x) + \\frac{h^2}{2!}f''(x) - \\frac{h^3}{3!}f'''(x) + \\frac{h^4}{4!}f^{(4)}(x) - \\frac{h^5}{5!}f^{(5)}(x) + \\mathcal{O}(h^6)\n$$\n\nSubtracting the second expansion from the first cancels all even-powered terms in $h$:\n$$\nf(x+h) - f(x-h) = 2hf'(x) + 2\\frac{h^3}{3!}f'''(x) + 2\\frac{h^5}{5!}f^{(5)}(x) + \\mathcal{O}(h^7)\n$$\n\nDividing by $2h$ gives an expression for the central difference formula:\n$$\n\\frac{f(x+h) - f(x-h)}{2h} = f'(x) + \\frac{h^2}{6}f'''(x) + \\frac{h^4}{120}f^{(5)}(x) + \\mathcal{O}(h^6)\n$$\n\nThe truncation error, $E_{\\mathrm{trunc}}(h)$, is the difference between the approximation and the exact value. The leading-order term dominates for small $h$:\n$$\nE_{\\mathrm{trunc}}(h) = D_h f(x) - f'(x) = \\frac{h^2}{6}f'''(x) + \\mathcal{O}(h^4)\n$$\n\nThe magnitude of the leading-order truncation error is thus $|E_{\\mathrm{trunc}}(h)| \\approx \\left|\\frac{f'''(x)}{6}\\right| h^2$. The coefficient multiplying $h^2$ is $\\frac{1}{6}f'''(x)$.\n\n### 2. Modeling the Rounding Error\n\nIn floating-point arithmetic, the evaluation of a function $f(z)$ yields a computed value $\\hat{f}(z)$ with a relative error on the order of the machine precision or unit roundoff, $\\epsilon$. We model this as $\\hat{f}(z) = f(z)(1+\\delta)$, where $|\\delta| \\le \\epsilon$. The problem specifies IEEE double precision, for which $\\epsilon = 2^{-53}$.\n\nThe numerator of $D_h f(x)$ involves the subtraction of two nearly equal quantities, $f(x+h)$ and $f(x-h)$, which is a primary source of rounding error. Let the computed values be $\\hat{f}(x+h) = f(x+h)(1+\\delta_1)$ and $\\hat{f}(x-h) = f(x-h)(1+\\delta_2)$. The error in the computed numerator is approximately:\n$$\n\\Delta_{\\mathrm{num}} \\approx f(x+h)\\delta_1 - f(x-h)\\delta_2\n$$\n\nFor small $h$, we have $f(x+h) \\approx f(x-h) \\approx f(x)$. The magnitude of the absolute error in the numerator is then bounded by:\n$$\n|\\Delta_{\\mathrm{num}}| \\lesssim |f(x)||\\delta_1| + |f(x)||\\delta_2| \\approx 2\\epsilon|f(x)|\n$$\n\nA more direct analysis considers that the floating-point subtraction $a \\ominus b$ of two nearly equal numbers $a \\approx b$ has a rounding error whose magnitude is on the order of $\\epsilon|a|$. The absolute rounding error in the final calculation of $D_h f(x)$ is this numerator error divided by $2h$. Thus, the magnitude of the rounding error $E_{\\mathrm{round}}(h)$ is:\n$$\n|E_{\\mathrm{round}}(h)| \\approx \\frac{\\epsilon |f(x)|}{h}\n$$\n\nThis matches the requested form $C\\epsilon/h$ with the coefficient $C = |f(x)|$.\n\n### 3. Derivation of the Optimal Step Size\n\nThe total error $E(h)$ is the sum of the magnitudes of the truncation and rounding errors:\n$$\nE(h) \\approx |E_{\\mathrm{trunc}}(h)| + |E_{\\mathrm{round}}(h)| = \\left|\\frac{f'''(x)}{6}\\right| h^2 + \\frac{|f(x)| \\epsilon}{h}\n$$\n\nTo find the step size $h_{\\mathrm{opt}}$ that minimizes this total error, we differentiate $E(h)$ with respect to $h$ and set the result to zero:\n$$\n\\frac{dE}{dh} = 2\\left|\\frac{f'''(x)}{6}\\right| h - \\frac{|f(x)| \\epsilon}{h^2} = 0\n$$\n\nSolving for $h$ gives the optimal step size $h_{\\mathrm{opt}}$:\n$$\n2\\left|\\frac{f'''(x)}{6}\\right| h^3 = |f(x)| \\epsilon \\implies h^3 = \\frac{3|f(x)|\\epsilon}{|f'''(x)|}\n$$\n$$\nh_{\\mathrm{opt}} = \\left( \\frac{3|f(x)|\\epsilon}{|f'''(x)|} \\right)^{1/3}\n$$\n\n### 4. Specialization to $f(x) = \\exp(kx)$\n\nWe now specialize this result for the function $f(x) = \\exp(kx)$ evaluated at $x=1$. The required derivatives are:\n$$\nf(x) = e^{kx} \\implies f(1) = e^k\n$$\n$$\nf'(x) = ke^{kx} \\implies f'(1) = ke^k\n$$\n$$\nf'''(x) = k^3e^{kx} \\implies f'''(1) = k^3e^k\n$$\n\nSubstituting these into the formula for $h_{\\mathrm{opt}}$:\n$$\nh_{\\mathrm{opt}} = \\left( \\frac{3|e^k|\\epsilon}{|k^3e^k|} \\right)^{1/3} = \\left( \\frac{3\\epsilon}{|k|^3} \\right)^{1/3} = \\frac{(3\\epsilon)^{1/3}}{|k|}\n$$\nThe value of machine epsilon for IEEE double precision is $\\epsilon = 2^{-53}$.\n\n### 5. Expected Number of Correct Digits\n\nThe expected optimal relative error, $\\delta_{\\mathrm{exp}}$, is the minimum total error $E(h_{\\mathrm{opt}})$ divided by the magnitude of the true derivative, $|f'(1)|$. This minimum error $E(h_{\\mathrm{opt}})$ can be found by substituting $h_{\\mathrm{opt}}$ back into the total error expression.\nThe total absolute error is $E(h) = A h^2 + B/h$ where $A = |f'''(1)/6|$ and $B = |f(1)|\\epsilon$.\nThe optimal error is $E(h_{\\mathrm{opt}}) = A h_{\\mathrm{opt}}^2 + B/h_{\\mathrm{opt}}$.\nThe relative error is $\\delta_{\\mathrm{opt}} = E(h_{\\mathrm{opt}}) / |f'(1)|$.\nFor $f(x)=e^{kx}$, we have $A=|k^3e^k/6|$, $B=|e^k|\\epsilon$, $|f'(1)|=|ke^k|$, and $h_{\\mathrm{opt}}=(3\\epsilon/|k|^3)^{1/3}$.\n$$\n\\delta_{\\mathrm{exp}} = \\frac{1}{|ke^k|} \\left( \\left|\\frac{k^3e^k}{6}\\right| \\left(\\frac{3\\epsilon}{|k|^3}\\right)^{2/3} + \\frac{|e^k|\\epsilon}{\\left(\\frac{3\\epsilon}{|k|^3}\\right)^{1/3}} \\right)\n$$\n$$\n\\delta_{\\mathrm{exp}} = \\frac{1}{|k|e^k} \\left( \\frac{|k|^3e^k}{6} \\frac{(3\\epsilon)^{2/3}}{|k|^2} + |e^k|\\epsilon \\frac{|k|}{(3\\epsilon)^{1/3}} \\right)\n$$\n$$\n\\delta_{\\mathrm{exp}} = \\frac{|k|}{6}(3\\epsilon)^{2/3} + \\frac{1}{|k|} \\frac{|k|\\epsilon}{(3\\epsilon)^{1/3}} = \\frac{|k|}{6}(3\\epsilon)^{2/3} + \\frac{3\\epsilon}{3(3\\epsilon)^{1/3}} = \\frac{|k|}{6}(3\\epsilon)^{2/3} + \\frac{(3\\epsilon)^{2/3}}{3}\n$$\nThis calculation seems to have gone awry in the original text. A cleaner path is to use the general formula for minimum error:\n$$\n\\delta_{\\mathrm{opt}} = \\frac{1}{|f'(1)|} |f'''(1)|^{1/3} |f(1)|^{2/3} \\frac{3^{2/3}}{2^{2/3}} \\epsilon^{2/3} = \\frac{|k^3e^k|^{1/3}|e^k|^{2/3}}{|ke^k|} \\frac{3^{2/3}}{2} \\epsilon^{2/3} = \\frac{|k|e^{k/3}e^{2k/3}}{|k|e^k} \\frac{3^{2/3}}{2} \\epsilon^{2/3} = \\frac{3^{2/3}}{2} \\epsilon^{2/3}\n$$\n$$\n\\delta_{\\mathrm{exp}} = \\frac{(9\\epsilon^2)^{1/3}}{2}\n$$\nRemarkably, the expected optimal relative error is independent of the parameter $k$.\nThe expected number of correct decimal digits is given by $d^{\\mathrm{exp}} = -\\log_{10}(\\delta_{\\mathrm{exp}})$.\n\n### 6. Measured Number of Correct Digits\n\nFor each value of $k$, we will numerically compute $h_{\\mathrm{opt}}$, then evaluate $D_{h_{\\mathrm{opt}}} f(1)$ using standard double-precision arithmetic. The measured relative error $\\delta_{\\mathrm{meas}}$ is then calculated as:\n$$\n\\delta_{\\mathrm{meas}} = \\frac{|D_{h_{\\mathrm{opt}}}f(1) - f'(1)|}{|f'(1)|}\n$$\nThe measured digit count is $d^{\\mathrm{meas}} = -\\log_{10}(\\delta_{\\mathrm{meas}})$. The comparison of $d^{\\mathrm{exp}}$ and $d^{\\mathrm{meas}}$ validates the robustness of the error model. Catastrophic cancellation is expected to degrade the accuracy for large values of $k$, potentially causing the measured error to be significantly larger (and $d^{\\mathrm{meas}}$ smaller) than predicted by our simplified model.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Analyzes the central finite-difference method for f(x) = exp(kx),\n    calculating the optimal step size and comparing theoretical vs. measured error.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Case parameters are values of k, with x=1 fixed.\n    test_cases = [\n        1.0,    # Case A: General reference scale\n        10.0,   # Case B: Steep exponential\n        0.1,    # Case C: Gentle exponential\n        100.0   # Case D: Very steep exponential\n    ]\n\n    # IEEE 754 double precision unit roundoff\n    epsilon = 2**-53\n\n    results = []\n    \n    # The expected relative error, derived in the solution, is independent of k.\n    # δ_exp = (3^(2/3) / 2) * ε^(2/3)\n    delta_exp = (3**(2/3) / 2) * (epsilon**(2/3))\n    \n    # The expected number of correct digits.\n    d_exp = -np.log10(delta_exp)\n\n    for k in test_cases:\n        x = 1.0\n\n        # Task 4: Compute the optimal step size h_opt for f(x) = exp(kx).\n        # h_opt = (|k|)^-1 * (3*ε)^(1/3)\n        # Using abs(k) to handle potential negative k, though not in test suite.\n        h_opt = (1 / np.abs(k)) * (3 * epsilon)**(1/3)\n\n        # Task 5: The expected number of correct digits, d_exp, is already computed.\n\n        # Task 6: Validate the model by direct computation.\n        # Exact derivative: f'(x) = k * exp(kx)\n        f_prime_exact = k * np.exp(k * x)\n\n        # Numerical derivative using the central difference formula.\n        # All calculations are implicitly in float64 (double precision).\n        f_plus_h = np.exp(k * (x + h_opt))\n        f_minus_h = np.exp(k * (x - h_opt))\n        f_prime_numerical = (f_plus_h - f_minus_h) / (2 * h_opt)\n\n        # Calculate the measured relative error.\n        # Add a small quantity to the denominator to prevent division by zero\n        # if the exact derivative happens to be zero.\n        delta_meas = np.abs(f_prime_numerical - f_prime_exact) / (np.abs(f_prime_exact) + epsilon)\n        \n        # Calculate the measured number of correct digits.\n        # If delta_meas is zero, it implies precision to the limits of float64.\n        # In that rare case, we can use a proxy for a very large number of digits.\n        if delta_meas == 0.0:\n            d_meas = -np.log10(epsilon) # A reasonable upper bound on measurable precision\n        else:\n            d_meas = -np.log10(delta_meas)\n\n        # Compare expected vs. measured digit counts.\n        ok = np.abs(d_exp - d_meas) = 0.5\n        \n        results.extend([h_opt, d_exp, d_meas, ok])\n\n    # Final print statement in the exact required format.\n    # The boolean values will be correctly converted to 'True' or 'False' by str().\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3525181"}, {"introduction": "Having explored the inherent limitations of the central-difference method, we now turn to a more advanced technique that elegantly sidesteps the problem of subtractive cancellation. The complex-step method leverages complex analysis to compute derivatives to near machine precision, free from the trade-off between truncation and round-off error. This practice demonstrates the remarkable accuracy and stability of the complex-step derivative and contrasts it directly with the finite-difference approach, highlighting its value in high-precision scientific computing [@problem_id:3525175].", "problem": "Consider the real-valued function $f(x) = \\exp(x)\\cos(x)$ that appears in simplified models of oscillatory kernels in computational high-energy physics where exponential damping and phase oscillations co-exist. In sensitivity analyses and automatic differentiation of parameterized integrals, one often requires high-accuracy first derivatives at specific points without incurring subtractive cancellation. The complex-step differentiation method is a robust way to achieve this in IEEE 754 binary64 (double-precision) arithmetic.\n\nStarting from the definition of the derivative as the limit $f'(x) = \\lim_{h \\to 0} \\frac{f(x+h) - f(x)}{h}$ and the analyticity of $f$ in the complex plane, construct and apply the complex-step differentiation method to compute $f'(1)$ using a purely imaginary perturbation $h = 10^{-20}$. All trigonometric functions must use radians.\n\nAdditionally, to situate differentiation accuracy alongside numerical integration that commonly arises in computational high-energy physics, compute the definite integral $\\int_{0}^{1} \\exp(x)\\cos(x)\\,dx$ using a high-accuracy adaptive quadrature, and compare it to the analytically-derived exact value. Further, compute the convergent improper integral $\\int_{0}^{\\infty} \\exp(-x)\\cos(x)\\,dx$ and compare it to its exact value obtained from fundamental transform methods. All trigonometric functions must use radians.\n\nYour program must adhere to IEEE 754 binary64 arithmetic by using the default double-precision types and must not alter floating-point precision. To make the task testable and comprehensive, implement the following test suite and produce results for each test case:\n\n- Test case 1 (happy path for complex-step): Compute the complex-step derivative at $x=1$ using $h = 10^{-20}$ and report the numerical value of $f'(1)$ from complex-step along with its absolute error relative to the exact derivative computed from first principles.\n- Test case 2 (finite-difference comparison, moderately small step): Compute the symmetric central-difference derivative at $x=1$ using $h = 10^{-8}$ and report the absolute error relative to the exact derivative.\n- Test case 3 (finite-difference comparison, larger step): Compute the symmetric central-difference derivative at $x=1$ using $h = 10^{-4}$ and report the absolute error relative to the exact derivative.\n- Test case 4 (boundary case for complex-step): Compute the complex-step derivative at $x=1$ using $h = 10^{-300}$ and report the absolute error relative to the exact derivative.\n- Test case 5 (definite integral on a finite interval): Compute $\\int_{0}^{1} \\exp(x)\\cos(x)\\,dx$ using a high-accuracy adaptive quadrature method and report the absolute error relative to the exact value obtained by analytic integration rules.\n- Test case 6 (improper integral on an infinite interval): Compute $\\int_{0}^{\\infty} \\exp(-x)\\cos(x)\\,dx$ using a high-accuracy adaptive quadrature method suitable for infinite limits and report the absolute error relative to the exact value obtained from fundamental transform formulas.\n\nAngles must be in radians. No physical units are involved. All reported errors must be real numbers. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, ordered as follows:\n$[d_{\\text{cs}}(h=10^{-20}),\\varepsilon_{\\text{cs}}(h=10^{-20}),\\varepsilon_{\\text{cd}}(h=10^{-8}),\\varepsilon_{\\text{cd}}(h=10^{-4}),\\varepsilon_{\\text{cs}}(h=10^{-300}),\\varepsilon_{I,[0,1]},\\varepsilon_{I,[0,\\infty)}]$,\nwhere $d_{\\text{cs}}$ is the complex-step derivative value and $\\varepsilon_{\\cdot}$ denotes absolute error.", "solution": "The problem is subjected to validation.\n\n### Step 1: Extract Givens\n- **Function**: $f(x) = \\exp(x)\\cos(x)$, a real-valued function of a real variable, analytic in the complex plane.\n- **Task 1 (Numerical Differentiation)**: Compute the first derivative $f'(1)$ using the complex-step method with perturbation $h = 10^{-20}$.\n- **Task 2 (Numerical Differentiation)**: Compute $f'(1)$ using the symmetric central-difference method with perturbations $h = 10^{-8}$ and $h = 10^{-4}$.\n- **Task 3 (Numerical Differentiation)**: Compute $f'(1)$ using the complex-step method with perturbation $h = 10^{-300}$.\n- **Task 4 (Numerical Integration)**: Compute the definite integral $\\int_{0}^{1} \\exp(x)\\cos(x)\\,dx$ using high-accuracy adaptive quadrature.\n- **Task 5 (Numerical Integration)**: Compute the improper integral $\\int_{0}^{\\infty} \\exp(-x)\\cos(x)\\,dx$ using high-accuracy adaptive quadrature.\n- **Comparison**: All numerical results are to be compared against their respective analytically-derived exact values to compute the absolute error.\n- **Arithmetic and Units**: All computations must use IEEE $754$ binary64 (double-precision) floating-point arithmetic. All trigonometric functions must use radians.\n- **Test Cases  Output**: The program must compute and report the following quantities in a specific order and format:\n    1.  $d_{\\text{cs}}(h=10^{-20})$: The numerical value of the derivative from the complex-step method with $h = 10^{-20}$.\n    2.  $\\varepsilon_{\\text{cs}}(h=10^{-20})$: The absolute error for the complex-step derivative with $h = 10^{-20}$.\n    3.  $\\varepsilon_{\\text{cd}}(h=10^{-8})$: The absolute error for the central-difference derivative with $h = 10^{-8}$.\n    4.  $\\varepsilon_{\\text{cd}}(h=10^{-4})$: The absolute error for the central-difference derivative with $h = 10^{-4}$.\n    5.  $\\varepsilon_{\\text{cs}}(h=10^{-300})$: The absolute error for the complex-step derivative with $h = 10^{-300}$.\n    6.  $\\varepsilon_{I,[0,1]}$: The absolute error for the definite integral over $[0, 1]$.\n    7.  $\\varepsilon_{I,[0,\\infty)}$: The absolute error for the improper integral over $[0, \\infty)$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically and mathematically sound. The function $f(x) = \\exp(x)\\cos(x)$ is entire (analytic over the whole complex plane), which is a necessary condition for the validity of the complex-step method. The tasks involve standard, well-established numerical methods (complex-step differentiation, central-difference, adaptive quadrature) and analytical techniques (calculus of derivatives and integrals). The problem is self-contained, with all necessary parameters ($h$ values, integration limits) specified. The comparison between methods and step sizes constitutes a valid and instructive exercise in numerical analysis, highlighting the trade-offs between truncation error and round-off error, and demonstrating the particular strengths of the complex-step method. All aspects of the problem are well-posed, objective, and computationally feasible.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\n### Analytical Preliminaries\nFirst, we establish the exact analytical values that will serve as the ground truth for our error calculations.\n\n**1. Exact Derivative**\nThe function is $f(x) = \\exp(x)\\cos(x)$. Using the product rule for differentiation, $(uv)' = u'v + uv'$, we get:\n$$ f'(x) = \\frac{d}{dx}(\\exp(x)\\cos(x)) = (\\exp(x))'\\cos(x) + \\exp(x)(\\cos(x))' $$\n$$ f'(x) = \\exp(x)\\cos(x) - \\exp(x)\\sin(x) = \\exp(x)(\\cos(x) - \\sin(x)) $$\nAt $x=1$, the exact value of the derivative is:\n$$ f'(1) = \\exp(1)(\\cos(1) - \\sin(1)) $$\n\n**2. Exact Definite Integral on $[0, 1]$**\nWe must evaluate $I_1 = \\int_{0}^{1} \\exp(x)\\cos(x)\\,dx$. This is a standard integral solved using integration by parts twice. A more direct method is to recognize $\\exp(x)\\cos(x)$ as the real part of a complex exponential: $\\text{Re}(\\exp(x) \\exp(ix)) = \\text{Re}(\\exp((1+i)x))$.\n$$ \\int \\exp((1+i)x)\\,dx = \\frac{\\exp((1+i)x)}{1+i} = \\frac{(1-i)\\exp(x)(\\cos(x)+i\\sin(x))}{2} = \\frac{\\exp(x)}{2}[(\\cos(x)+\\sin(x)) + i(\\sin(x)-\\cos(x))] $$\nTaking the real part of the indefinite integral gives:\n$$ \\int \\exp(x)\\cos(x)\\,dx = \\frac{1}{2}\\exp(x)(\\cos(x) + \\sin(x)) $$\nEvaluating the definite integral:\n$$ I_1 = \\left[ \\frac{1}{2}\\exp(x)(\\cos(x) + \\sin(x)) \\right]_0^1 = \\frac{1}{2}\\exp(1)(\\cos(1) + \\sin(1)) - \\frac{1}{2}\\exp(0)(\\cos(0) + \\sin(0)) $$\n$$ I_1 = \\frac{1}{2}(e(\\cos(1) + \\sin(1)) - 1) $$\n\n**3. Exact Improper Integral on $[0, \\infty)$**\nWe must evaluate $I_2 = \\int_{0}^{\\infty} \\exp(-x)\\cos(x)\\,dx$. A similar approach can be used. The indefinite integral is:\n$$ \\int \\exp(-x)\\cos(x)\\,dx = \\frac{1}{2}\\exp(-x)(\\sin(x) - \\cos(x)) $$\nEvaluating the improper integral:\n$$ I_2 = \\left[ \\frac{1}{2}\\exp(-x)(\\sin(x) - \\cos(x)) \\right]_0^\\infty = \\lim_{b \\to \\infty} \\frac{1}{2}\\exp(-b)(\\sin(b) - \\cos(b)) - \\frac{1}{2}\\exp(0)(\\sin(0) - \\cos(0)) $$\nSince $\\exp(-b) \\to 0$ as $b \\to \\infty$ and $\\sin(b), \\cos(b)$ are bounded, the first term vanishes.\n$$ I_2 = 0 - \\frac{1}{2}(1)(0 - 1) = \\frac{1}{2} $$\n\n### Numerical Methods\n\n**1. Complex-Step Differentiation**\nFor an analytic function $f$, its Taylor series expansion around $x$ is:\n$$ f(x+k) = f(x) + kf'(x) + \\frac{k^2}{2!}f''(x) + \\frac{k^3}{3!}f'''(x) + \\dots $$\nLet us choose a purely imaginary step $k = ih$, where $h$ is a small real number.\n$$ f(x+ih) = f(x) + ihf'(x) + \\frac{(ih)^2}{2!}f''(x) + \\frac{(ih)^3}{3!}f'''(x) + \\dots $$\n$$ f(x+ih) = f(x) + ihf'(x) - \\frac{h^2}{2!}f''(x) - i\\frac{h^3}{3!}f'''(x) + \\dots $$\nGrouping the real and imaginary parts:\n$$ f(x+ih) = \\left(f(x) - \\frac{h^2}{2}f''(x) + O(h^4)\\right) + i\\left(hf'(x) - \\frac{h^3}{6}f'''(x) + O(h^5)\\right) $$\nBy taking the imaginary part of both sides and dividing by $h$, we can isolate $f'(x)$:\n$$ \\frac{\\text{Im}[f(x+ih)]}{h} = f'(x) - \\frac{h^2}{6}f'''(x) + O(h^4) $$\nThis gives the complex-step approximation for the first derivative:\n$$ f'(x) \\approx \\frac{\\text{Im}[f(x+ih)]}{h} $$\nThe truncation error is of order $O(h^2)$. Crucially, this formula does not involve the subtraction of two nearly equal numbers, thus avoiding the subtractive cancellation that plagues finite-difference methods. The main source of error for a sufficiently small $h$ is the precision of the floating-point arithmetic itself, not the step size.\n\n**2. Central-Difference Differentiation**\nThe symmetric central-difference formula is derived from two Taylor expansions:\n$$ f(x+h) = f(x) + hf'(x) + \\frac{h^2}{2}f''(x) + \\frac{h^3}{6}f'''(x) + O(h^4) $$\n$$ f(x-h) = f(x) - hf'(x) + \\frac{h^2}{2}f''(x) - \\frac{h^3}{6}f'''(x) + O(h^4) $$\nSubtracting the second from the first yields:\n$$ f(x+h) - f(x-h) = 2hf'(x) + \\frac{h^3}{3}f'''(x) + O(h^5) $$\nRearranging gives the approximation:\n$$ f'(x) \\approx \\frac{f(x+h) - f(x-h)}{2h} $$\nThe truncation error is of order $O(h^2)$. However, as $h \\to 0$, $f(x+h)$ and $f(x-h)$ become very close. Their subtraction, $f(x+h) - f(x-h)$, suffers from catastrophic cancellation (a form of round-off error), which is then amplified by division by the small $2h$. This leads to an optimal $h$ that balances truncation and round-off errors, below which the total error increases.\n\n**3. Adaptive Quadrature**\nFor the numerical integrals, we will use a high-accuracy, general-purpose adaptive quadrature routine as provided by `scipy.integrate.quad`. This function uses a Clenshaw-Curtis or more commonly a Gauss-Kronrod quadrature formula. It adaptively subdivides the integration interval in regions where the integrand is difficult to approximate, aiming to achieve a requested error tolerance. It is robust for a wide class of functions, including those with integrable singularities, and can handle infinite integration limits.\n\n### Implementation and Calculation of Test Cases\nThe following steps are implemented in the provided Python code.\n1.  Define the function $f(x) = \\exp(x)\\cos(x)$ and its variant for the improper integral, $g(x) = \\exp(-x)\\cos(x)$.\n2.  Calculate the exact analytical values for $f'(1)$, $\\int_{0}^{1} f(x)dx$, and $\\int_{0}^{\\infty} g(x)dx$ to be used as benchmarks.\n3.  **Test Case 1  4**: Compute $d_{\\text{cs}}$ using the complex-step formula for $h=10^{-20}$ and $h=10^{-300}$. The absolute errors $\\varepsilon_{\\text{cs}}$ are calculated against the exact $f'(1)$.\n4.  **Test Case 2  3**: Compute the central-difference approximation for $h=10^{-8}$ and $h=10^{-4}$. The absolute errors $\\varepsilon_{\\text{cd}}$ are calculated against the exact $f'(1)$.\n5.  **Test Case 5**: Compute $\\int_{0}^{1} f(x)dx$ using `scipy.integrate.quad` and calculate the absolute error $\\varepsilon_{I,[0,1]}$.\n6.  **Test Case 6**: Compute $\\int_{0}^{\\infty} g(x)dx$ using `scipy.integrate.quad` with an infinite upper limit and calculate the absolute error $\\varepsilon_{I,[0,\\infty)}$.\n7.  Collect the seven required results in a list and format the output as specified.\nThe results will demonstrate the remarkable accuracy and stability of the complex-step method, even for extremely small step sizes, compared to the central-difference method, which shows a degradation in accuracy for a very small step size ($h=10^{-8}$) due to round-off error. They will also confirm the high accuracy of the adaptive quadrature routine.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import integrate\n\ndef solve():\n    \"\"\"\n    Performs numerical differentiation and integration as specified in the problem,\n    calculating values and their absolute errors relative to analytical solutions.\n    \"\"\"\n\n    # Define the primary function f(x) = exp(x)*cos(x)\n    # This function is defined to handle complex inputs for the complex-step method.\n    def f(z):\n        return np.exp(z) * np.cos(z)\n\n    # Define the function for the improper integral g(x) = exp(-x)*cos(x)\n    def g(x):\n        return np.exp(-x) * np.cos(x)\n    \n    # Point of evaluation for the derivative\n    x_val = 1.0\n\n    # --- Analytical (Exact) Solutions ---\n    \n    # Exact first derivative: f'(x) = exp(x)*(cos(x) - sin(x))\n    exact_derivative = np.exp(x_val) * (np.cos(x_val) - np.sin(x_val))\n    \n    # Exact definite integral: Integral[exp(x)cos(x)dx] from 0 to 1\n    # = 0.5 * [exp(x)*(cos(x) + sin(x))] from 0 to 1\n    exact_integral_finite = 0.5 * (np.exp(1.0) * (np.cos(1.0) + np.sin(1.0)) - 1.0)\n    \n    # Exact improper integral: Integral[exp(-x)cos(x)dx] from 0 to inf\n    # This evaluates to 0.5\n    exact_integral_improper = 0.5\n\n    # --- Test Case Calculations ---\n\n    results = []\n\n    # Test Case 1: Complex-step derivative with h = 1e-20\n    h1 = 1e-20\n    cs_derivative_h1 = np.imag(f(x_val + 1j * h1)) / h1\n    cs_error_h1 = np.abs(cs_derivative_h1 - exact_derivative)\n    \n    # Add the value and the error to the results\n    results.append(cs_derivative_h1)\n    results.append(cs_error_h1)\n    \n    # Test Case 2: Central-difference derivative with h = 1e-8\n    h2 = 1e-8\n    cd_derivative_h2 = (f(x_val + h2) - f(x_val - h2)) / (2 * h2)\n    cd_error_h2 = np.abs(cd_derivative_h2 - exact_derivative)\n    results.append(cd_error_h2)\n\n    # Test Case 3: Central-difference derivative with h = 1e-4\n    h3 = 1e-4\n    cd_derivative_h3 = (f(x_val + h3) - f(x_val - h3)) / (2 * h3)\n    cd_error_h3 = np.abs(cd_derivative_h3 - exact_derivative)\n    results.append(cd_error_h3)\n\n    # Test Case 4: Complex-step derivative with h = 1e-300 (boundary case)\n    h4 = 1e-300\n    cs_derivative_h4 = np.imag(f(x_val + 1j * h4)) / h4\n    cs_error_h4 = np.abs(cs_derivative_h4 - exact_derivative)\n    results.append(cs_error_h4)\n    \n    # Test Case 5: Definite integral over [0, 1]\n    # The second return value from quad is the estimated absolute error, which we ignore\n    # in favor of comparison with our analytically derived exact value.\n    numerical_integral_finite, _ = integrate.quad(f, 0, 1)\n    integral_error_finite = np.abs(numerical_integral_finite - exact_integral_finite)\n    results.append(integral_error_finite)\n\n    # Test Case 6: Improper integral over [0, inf)\n    numerical_integral_improper, _ = integrate.quad(g, 0, np.inf)\n    integral_error_improper = np.abs(numerical_integral_improper - exact_integral_improper)\n    results.append(integral_error_improper)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3525175"}, {"introduction": "Moving from differentiation to integration, we confront one of the central challenges in computational physics: evaluating integrals in multiple dimensions. Naively extending one-dimensional methods via tensor products leads to an exponential increase in computational cost known as the \"curse of dimensionality.\" This practice introduces Smolyak sparse grids as a powerful strategy to mitigate this issue, allowing for accurate integration in higher dimensions with a much more manageable number of function evaluations. You will implement and compare a tensor-product rule against a sparse grid, gaining first-hand experience with these essential tools for tackling multi-dimensional problems like phase-space integrals [@problem_id:3525193].", "problem": "Consider the integral of a separable exponential weight over the three-dimensional unit cube, defined as $\\int_{[0,1]^3} \\exp(-\\alpha \\sum_{i=1}^3 x_i)\\,dx$, where $\\alpha$ is a positive parameter and $(x_1,x_2,x_3)\\in[0,1]^3$. In computational high-energy physics, such integrals model simplified phase-space averages with exponential suppression, and their accurate numerical evaluation is essential when exact expressions are available for validation. Starting from fundamental facts of calculus and numerical analysis, design and implement a program that compares two numerical integration strategies for this integral at the fixed parameter value $\\alpha=10$: a tensor-product Gaussian quadrature and a Smolyak sparse grid of level $2$ in three dimensions.\n\nYour program must adhere to the following requirements:\n\n1. Implement a tensor-product Gaussian quadrature using the Gauss–Legendre family on $[-1,1]$ transformed to $[0,1]$, with $N$ points per spatial dimension. The three-dimensional tensor-product rule should use the Cartesian product of the one-dimensional nodes and weights.\n\n2. Implement a level-$2$ Smolyak sparse grid in $d=3$ dimensions using a sequence of one-dimensional Gauss–Legendre rules $U^{(l)}$ with $N(l)=2l-1$ nodes for $l\\in\\{1,2\\}$, transformed to $[0,1]$. Construct the Smolyak operator at level $2$ via the canonical difference-of-levels formulation using $\\Delta^{(l)}=U^{(l)}-U^{(l-1)}$ with $U^{(0)}=0$, combining all multi-indices $(l_1,l_2,l_3)$ satisfying $l_i\\ge 1$ and $l_1+l_2+l_3=4$. Your implementation must explicitly build the combined three-dimensional quadrature nodes and weights, correctly accounting for negative weights and merging duplicate nodes by summing their weights, and it must report the total number of unique function evaluation points used.\n\n3. Compute the exact value of the integral analytically using first principles of calculus and the separability of the integrand, without relying on external resources or pre-tabulated results. Use this exact value to compute absolute errors for each numerical method.\n\n4. Use the fixed parameter value $\\alpha=10$ for all computations. No physical unit is involved in this integral; all quantities are dimensionless.\n\n5. Test Suite:\n   - Evaluate the tensor-product Gaussian quadrature for $N\\in\\{1,3,5\\}$ points per dimension.\n   - For each $N$ in the test suite, compute:\n     - The absolute error of the tensor-product Gaussian quadrature.\n     - The absolute error of the level-$2$ Smolyak sparse grid (which is independent of $N$).\n     - The total number of points used by the tensor-product rule, which is $N^3$.\n     - The total number of unique points used by the Smolyak sparse grid implementation (as constructed with the specified rules and merging duplicates).\n   This test suite covers a boundary case ($N=1$), a typical moderate-accuracy case ($N=3$), and a higher-accuracy case ($N=5$).\n\n6. Final Output Format:\n   Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case contributes a sub-list of the form $[E_{\\mathrm{tensor}},E_{\\mathrm{smolyak}},P_{\\mathrm{tensor}},P_{\\mathrm{smolyak}}]$, where $E_{\\mathrm{tensor}}$ and $E_{\\mathrm{smolyak}}$ are floats (absolute errors), and $P_{\\mathrm{tensor}}$ and $P_{\\mathrm{smolyak}}$ are integers (point counts). For example, the final printed line must look like `[[e_1,e_2,p_1,p_2],[e_3,e_4,p_3,p_4],...]`.\n\nYour implementation must be a complete, runnable program and must not read input from files or the network. The program must compute the exact integral internally, and perform all numerical quadrature computations as specified.", "solution": "The problem is validated as scientifically grounded, well-posed, and objective. It poses a specific and formalizable task in numerical analysis, comparing two standard quadrature methods for a well-defined integral. All necessary parameters and definitions are provided.\n\n### 1. Analytical Solution\n\nThe integral to be evaluated is:\n$$\nI = \\int_{[0,1]^3} \\exp\\left(-\\alpha \\sum_{i=1}^3 x_i\\right) d\\mathbf{x}\n$$\nwhere $d\\mathbf{x} = dx_1 dx_2 dx_3$ and the domain is the unit cube $[0,1]^3$. The parameter $\\alpha$ is given as $10$.\n\nThe integrand $f(x_1, x_2, x_3) = \\exp(-\\alpha(x_1+x_2+x_3))$ is a separable function, as it can be written as a product of functions of a single variable:\n$$\nf(x_1, x_2, x_3) = e^{-\\alpha x_1} e^{-\\alpha x_2} e^{-\\alpha x_3}\n$$\nDue to the separability of the integrand and the rectangular domain of integration, the three-dimensional integral can be expressed as the cube of a one-dimensional integral:\n$$\nI = \\left( \\int_0^1 e^{-\\alpha x} dx \\right) \\left( \\int_0^1 e^{-\\alpha y} dy \\right) \\left( \\int_0^1 e^{-\\alpha z} dz \\right) = \\left( \\int_0^1 e^{-\\alpha x} dx \\right)^3\n$$\nThe one-dimensional integral is evaluated using the fundamental theorem of calculus:\n$$\n\\int_0^1 e^{-\\alpha x} dx = \\left[ -\\frac{1}{\\alpha} e^{-\\alpha x} \\right]_0^1 = -\\frac{1}{\\alpha} (e^{-\\alpha \\cdot 1} - e^{-\\alpha \\cdot 0}) = -\\frac{1}{\\alpha} (e^{-\\alpha} - 1) = \\frac{1 - e^{-\\alpha}}{\\alpha}\n$$\nSubstituting this result back, the exact value of the three-dimensional integral is:\n$$\nI_{\\text{exact}} = \\left( \\frac{1 - e^{-\\alpha}}{\\alpha} \\right)^3\n$$\nFor the given parameter $\\alpha=10$, the exact value is:\n$$\nI_{\\text{exact}} = \\left( \\frac{1 - e^{-10}}{10} \\right)^3 \\approx (0.09999546...)^3 \\approx 9.9986389... \\times 10^{-4}\n$$\n\n### 2. Numerical Integration Methods\n\nWe will implement and compare two numerical quadrature methods. For any quadrature rule, the integral of a function $g(\\mathbf{p})$ over a domain $\\Omega$ is approximated by a weighted sum of function evaluations at specific points (nodes) $\\mathbf{p}_j$:\n$$\n\\int_\\Omega g(\\mathbf{p}) d\\mathbf{p} \\approx \\sum_{j=1}^M w_j g(\\mathbf{p}_j)\n$$\n\nThe one-dimensional Gauss-Legendre quadrature rules are defined on the interval $[-1,1]$. To apply them to an integral on $[0,1]$, the nodes $x_i \\in [-1,1]$ and weights $w_i$ must be transformed. The linear mapping is $x' = (x+1)/2$. The new nodes $x'_i$ and weights $w'_i$ for the interval $[0,1]$ are:\n$$\nx'_i = \\frac{x_i + 1}{2}, \\quad w'_i = \\frac{w_i}{2}\n$$\n\n#### 2.1. Tensor-Product Gaussian Quadrature\n\nA three-dimensional tensor-product rule is constructed from a one-dimensional rule. Given a 1D Gauss-Legendre rule with $N$ nodes $\\{x'_i\\}_{i=1}^N$ and weights $\\{w'_i\\}_{i=1}^N$ on $[0,1]$, the 3D tensor-product rule uses $N^3$ points. The nodes are the Cartesian product of the 1D nodes, forming a grid: $\\mathbf{p}_{ijk} = (x'_i, x'_j, x'_k)$. The corresponding weights are the products of the 1D weights: $W_{ijk} = w'_i w'_j w'_k$. The integral approximation is:\n$$\nI_{\\text{tensor}} \\approx \\sum_{i=1}^N \\sum_{j=1}^N \\sum_{k=1}^N (w'_i w'_j w'_k) f(x'_i, x'_j, x'_k)\n$$\nThe total number of function evaluation points for this method is $P_{\\text{tensor}} = N^3$.\n\n#### 2.2. Smolyak Sparse Grid Quadrature\n\nThe problem specifies a particular construction for a level-$2$ Smolyak sparse grid in $d=3$ dimensions. The construction is based on a sequence of 1D Gauss-Legendre rules, $U^{(l)}$, where the $l$-th rule uses $N(l) = 2l-1$ points.\n-   $l=1 \\implies N(1)=1$ point (midpoint rule).\n-   $l=2 \\implies N(2)=3$ points.\n\nThe construction uses the difference operator $\\Delta^{(l)} = U^{(l)} - U^{(l-1)}$, with the convention $U^{(0)}=0$. Thus, $\\Delta^{(1)} = U^{(1)}$.\nThe specified Smolyak operator is formulated by combining tensor products of these difference operators for all multi-indices $\\mathbf{l} = (l_1, l_2, l_3)$ with $l_i \\ge 1$ such that their sum is $|\\mathbf{l}|_1 = l_1+l_2+l_3=4$. The set of qualifying multi-indices is $\\{(2,1,1), (1,2,1), (1,1,2)\\}$.\n\nThe Smolyak quadrature rule $Q_{\\text{smolyak}}$ is therefore:\n$$\nQ_{\\text{smolyak}} = \\Delta^{(2)} \\otimes \\Delta^{(1)} \\otimes \\Delta^{(1)} + \\Delta^{(1)} \\otimes \\Delta^{(2)} \\otimes \\Delta^{(1)} + \\Delta^{(1)} \\otimes \\Delta^{(1)} \\otimes \\Delta^{(2)}\n$$\nSubstituting $\\Delta^{(1)} = U^{(1)}$ and $\\Delta^{(2)} = U^{(2)} - U^{(1)}$, this expands to:\n$$\nQ_{\\text{smolyak}} = (U^{(2)}-U^{(1)}) \\otimes U^{(1)} \\otimes U^{(1)} + U^{(1)} \\otimes (U^{(2)}-U^{(1)}) \\otimes U^{(1)} + U^{(1)} \\otimes U^{(1)} \\otimes (U^{(2)}-U^{(1)})\n$$\n\nTo implement this, we compute the nodes and weights for each term and sum them, merging duplicate nodes by adding their weights.\n-   $U^{(1)}$ on $[0,1]$: 1 node $x_1^{(1)}=0.5$, weight $w_1^{(1)}=1.0$.\n-   $U^{(2)}$ on $[0,1]$: 3 nodes $\\{x_1^{(2)}, x_2^{(2)}, x_3^{(2)}\\} = \\{(1-\\sqrt{3/5})/2, 0.5, (1+\\sqrt{3/5})/2\\}$, weights $\\{w_1^{(2)}, w_2^{(2)}, w_3^{(2)}\\} = \\{5/18, 8/18, 5/18\\}$.\n\nThe operator $\\Delta^{(2)} = U^{(2)} - U^{(1)}$ uses the nodes of $U^{(2)}$. The weight for the node $x_2^{(2)}=0.5$, which is also the node of $U^{(1)}$, is modified: $w_2^{\\Delta(2)} = w_2^{(2)} - w_1^{(1)} = 8/18 - 1 = -10/18$. The other weights are unchanged. The nodes and weights for $\\Delta^{(2)}$ are:\n-   Nodes: $\\{(1-\\sqrt{3/5})/2, 0.5, (1+\\sqrt{3/5})/2\\}$\n-   Weights: $\\{5/18, -10/18, 5/18\\}$\n\nThe full sparse grid is built by creating a list of all points and their corresponding weights from the three tensor product terms in the expression for $Q_{\\text{smolyak}}$.\n-   $\\Delta^{(2)}\\otimes\\Delta^{(1)}\\otimes\\Delta^{(1)}$ contributes 3 points of the form (node from $\\Delta^{(2)}$, $0.5, 0.5$).\n-   $\\Delta^{(1)}\\otimes\\Delta^{(2)}\\otimes\\Delta^{(1)}$ contributes 3 points of the form ($0.5$, node from $\\Delta^{(2)}$, $0.5$).\n-   $\\Delta^{(1)}\\otimes\\Delta^{(1)}\\otimes\\Delta^{(2)}$ contributes 3 points of the form ($0.5, 0.5$, node from $\\Delta^{(2)}$).\n\nThis results in a total of $3+3+3=9$ initial points. Several points are duplicates. For example, $(0.5, 0.5, 0.5)$ is generated by all three terms. After combining duplicates and summing their weights, we are left with $7$ unique points.\n-   1 central point: $(0.5, 0.5, 0.5)$. Its weight is the sum of the weights from the three terms: $3 \\times (-10/18) = -30/18$.\n-   6 \"axial\" points: e.g., $((1\\pm\\sqrt{3/5})/2, 0.5, 0.5)$ and its permutations. Each of these unique points appears once, with a weight of $5/18$.\n\nThe total number of unique function evaluations is $P_{\\text{smolyak}} = 7$.\n\n### 3. Error Computation\n\nThe absolute error for each numerical method is calculated as the absolute difference between the numerical approximation and the exact analytical value:\n$$\nE = |I_{\\text{numerical}} - I_{\\text{exact}}|\n$$\nThis will be computed for both $I_{\\text{tensor}}$ and $I_{\\text{smolyak}}$.\n\n### 4. Test Cases\n\nThe test suite requires these computations for each $N \\in \\{1, 3, 5\\}$:\n1.  Absolute error $E_{\\text{tensor}}$ for the $N$-point tensor-product rule.\n2.  Absolute error $E_{\\text{smolyak}}$ for the level-2 Smolyak rule (this will be constant).\n3.  Total points $P_{\\text{tensor}} = N^3$.\n4.  Total points $P_{\\text{smolyak}} = 7$ (this will be constant).\n\nThe final program will execute these calculations and format the output as specified.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import roots_legendre\n\ndef solve():\n    \"\"\"\n    Computes and compares tensor-product and Smolyak sparse grid quadratures\n    for a 3D exponential integral.\n    \"\"\"\n    # Define the problem parameter and test cases from the problem statement.\n    ALPHA = 10.0\n    TEST_N_VALUES = [1, 3, 5]\n    \n    # 1. ANALYTICAL SOLUTION\n    # The exact value of the integral I = ( (1 - exp(-alpha)) / alpha )^3\n    exact_value = ((1.0 - np.exp(-ALPHA)) / ALPHA)**3\n\n    # Define the integrand function\n    # It accepts a batch of points (N, 3) and returns (N,) results.\n    def integrand(points):\n        return np.exp(-ALPHA * np.sum(points, axis=1))\n\n    def get_transformed_gl_rule(n):\n        \"\"\"\n        Gets Gauss-Legendre nodes and weights for interval [0, 1].\n        \"\"\"\n        nodes_std, weights_std = roots_legendre(n)\n        # Transform from [-1, 1] to [0, 1]\n        nodes_transformed = (nodes_std + 1.0) / 2.0\n        weights_transformed = weights_std / 2.0\n        return nodes_transformed, weights_transformed\n\n    # 2. SMOLYAK SPARSE GRID IMPLEMENTATION\n    def calculate_smolyak_quadrature():\n        \"\"\"\n        Constructs the specified level-2 Smolyak sparse grid and computes the integral.\n        This is a fixed rule, so its result is computed once.\n        \"\"\"\n        # Get 1D base rules U^(l) for l=1, 2 on [0,1]\n        # U^(1) corresponds to n=1, U^(2) to n=3 (since N(l)=2l-1)\n        u1_nodes, u1_weights = get_transformed_gl_rule(1)\n        u2_nodes, u2_weights = get_transformed_gl_rule(3)\n        \n        # Define difference operators Delta^(l)\n        # Delta^(1) = U^(1)\n        d1_nodes, d1_weights = u1_nodes, u1_weights\n        \n        # Delta^(2) = U^(2) - U^(1)\n        d2_nodes = u2_nodes\n        d2_weights = u2_weights.copy()\n        # Find the index of the common node (0.5) in U^(2)\n        common_node_idx = np.where(u2_nodes == u1_nodes[0])[0][0]\n        d2_weights[common_node_idx] -= u1_weights[0]\n\n        # Use a dictionary to store points and sum weights for duplicates\n        # Rounding keys to handle floating point inaccuracies\n        sparse_grid = {}\n        \n        # Rule is sum over permutations of (Delta^2, Delta^1, Delta^1)\n        deltas = [(d1_nodes, d1_weights), (d2_nodes, d2_weights)]\n        multi_indices = [(2, 1, 1), (1, 2, 1), (1, 1, 2)]\n\n        for l1, l2, l3 in multi_indices:\n            nodes_x, weights_x = deltas[l1 - 1]\n            nodes_y, weights_y = deltas[l2 - 1]\n            nodes_z, weights_z = deltas[l3 - 1]\n            \n            for i, (nx, wx) in enumerate(zip(nodes_x, weights_x)):\n                for j, (ny, wy) in enumerate(zip(nodes_y, weights_y)):\n                    for k, (nz, wz) in enumerate(zip(nodes_z, weights_z)):\n                        point = (nx, ny, nz)\n                        # Round to ensure consistent keys for duplicate points\n                        key = tuple(np.round(point, 15))\n                        weight = wx * wy * wz\n                        sparse_grid[key] = sparse_grid.get(key, 0.0) + weight\n\n        # Extract unique points and final weights\n        final_points = np.array(list(sparse_grid.keys()))\n        final_weights = np.array(list(sparse_grid.values()))\n        \n        # Calculate the integral value\n        integral_value = np.sum(final_weights * integrand(final_points))\n        num_points = len(final_points)\n        \n        return integral_value, num_points\n\n    # 3. TENSOR-PRODUCT GAUSSIAN QUADRATURE IMPLEMENTATION\n    def calculate_tensor_product_quadrature(N):\n        \"\"\"\n        Constructs an N-point (per dim) tensor-product grid and computes the integral.\n        \"\"\"\n        nodes_1d, weights_1d = get_transformed_gl_rule(N)\n        \n        # Create 3D grid points and weights\n        x, y, z = np.meshgrid(nodes_1d, nodes_1d, nodes_1d, indexing='ij')\n        points = np.vstack([x.ravel(), y.ravel(), z.ravel()]).T\n        \n        wx, wy, wz = np.meshgrid(weights_1d, weights_1d, weights_1d, indexing='ij')\n        weights = (wx * wy * wz).ravel()\n\n        # Calculate the integral value\n        integral_value = np.sum(weights * integrand(points))\n        num_points = N**3\n        \n        return integral_value, num_points\n\n    # 4. EXECUTE TEST SUITE AND COLLECT RESULTS\n    results = []\n\n    # Pre-calculate the Smolyak result as it's constant for all test cases\n    smolyak_value, smolyak_points_count = calculate_smolyak_quadrature()\n    error_smolyak = abs(smolyak_value - exact_value)\n\n    for N in TEST_N_VALUES:\n        # Calculate tensor product quadrature for the current N\n        tensor_value, tensor_points_count = calculate_tensor_product_quadrature(N)\n        error_tensor = abs(tensor_value - exact_value)\n        \n        # Store results for this test case\n        case_result = [\n            error_tensor,\n            error_smolyak,\n            tensor_points_count,\n            smolyak_points_count\n        ]\n        results.append(case_result)\n\n    # 5. FINAL OUTPUT FORMATTING\n    # Format each sub-list: [val1,val2,...]\n    sub_lists_str = [f\"[{','.join(map(str, r))}]\" for r in results]\n    # Join sub-lists into the final format: [[...],[...],...]\n    final_output = f\"[{','.join(sub_lists_str)}]\"\n    \n    print(final_output)\n\nsolve()\n```", "id": "3525193"}]}