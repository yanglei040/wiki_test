{"hands_on_practices": [{"introduction": "A foundational task in particle physics is calculating the rates of particle decays and scattering processes, which requires integrating over the available final-state momentum configurations. This exercise guides you through the process of calculating the Lorentz-invariant three-body phase-space volume, a key component of any three-body decay rate, using Monte Carlo integration from first principles [@problem_id:3523401]. By reducing a complex multi-dimensional integral into a one-dimensional form through a strategic change of variables, this practice develops essential skills for tackling the high-dimensional integrals common in high-energy physics.", "problem": "You are tasked with designing and implementing a Monte Carlo integration algorithm to compute the Lorentz-invariant three-body phase-space volume in high-energy physics for a process with total center-of-mass four-momentum squared $s$ and three final-state particles of masses $m_1$, $m_2$, and $m_3$. The problem must be framed from first principles of Monte Carlo integration and the Lorentz-invariant phase-space measure. You should build your solution starting from the definition of the Lorentz-invariant phase-space for three particles as a distributional integral over final-state three-momenta constrained by energy-momentum conservation, without relying on shortcut formulas. Your program must carry out a valid change of variables to a single invariant and use Monte Carlo sampling in the transformed variable to estimate the integral.\n\nFundamental base for the problem: The Lorentz-invariant $n$-body phase space is defined by\n$$\n\\mathrm{d}\\Phi_n(P; m_1,\\dots,m_n) \\equiv (2\\pi)^4\\,\\delta^{(4)}\\!\\left(P - \\sum_{i=1}^{n} p_i\\right)\\,\\prod_{i=1}^{n}\\frac{\\mathrm{d}^3 \\mathbf{p}_i}{(2\\pi)^3\\,2E_i},\n$$\nwhere $P$ is the total incoming four-momentum, $p_i$ are outgoing on-shell four-momenta satisfying $p_i^2 = m_i^2$, $E_i$ are the corresponding energies, and $\\delta^{(4)}$ is the four-dimensional Dirac delta function. Your target is the integrated three-body phase-space volume\n$$\n\\Phi_3(s; m_1,m_2,m_3) \\equiv \\int \\mathrm{d}\\Phi_3(P; m_1,m_2,m_3),\n$$\nexpressed as a function of the center-of-mass invariant $s \\equiv P^2$ and the final-state masses. You must derive a physically valid factorization of the measure using a sequential two-body decay picture and a change of variables to a single invariant mass squared $s_{12} \\equiv (p_1+p_2)^2$, identify the correct kinematic limits for $s_{12}$, and compute the Jacobian to convert the original measure to an integral over $s_{12}$ alone after the angular integrations.\n\nMonte Carlo requirement: Implement a Monte Carlo estimator for the one-dimensional integral over $s_{12}$ using uniform sampling in a unit interval mapped to the physically allowed range $\\left[s_{12}^{\\min}, s_{12}^{\\max}\\right]$, and multiply by the appropriate Jacobian and constant factors dictated by the derived measure. Use a fixed pseudorandom number generator seed for reproducibility. Report the estimated value of $\\Phi_3(s; m_1,m_2,m_3)$ as a floating-point number.\n\nUnits requirement: All energies and masses must be treated in gigaelectronvolts (GeV). Express the final phase-space volume $\\Phi_3$ in GeV$^2$.\n\nAngle unit requirement: Any angles introduced in the derivation must be understood as measured in radians.\n\nYour program must implement the following:\n- A function that computes the Källén function $\\lambda(a,b,c) \\equiv a^2 + b^2 + c^2 - 2ab - 2ac - 2bc$ and uses it consistently within the Jacobian after the correct variable transformation and angular integrations.\n- A Monte Carlo estimator that draws $N$ independent samples uniformly from the unit interval and maps them to $s_{12}$ in $\\left[s_{12}^{\\min}, s_{12}^{\\max}\\right]$ using a linear transformation, accumulates the integrand implied by the derived measure, and outputs the final estimate as the scaled average. Include a safeguard that returns $0$ when the kinematic threshold $s < \\left(m_1 + m_2 + m_3\\right)^2$ makes the phase space vanish.\n- Use a fixed seed $12345$ for the pseudorandom number generator.\n\nTest suite:\nProvide estimates for the following parameter sets, all expressed in GeV and GeV$^2$ as appropriate, with $N$ samples per case as specified:\n1. $s = 100$, $m_1 = 1$, $m_2 = 2$, $m_3 = 3$, $N = 200000$.\n2. $s = 25$, $m_1 = 0$, $m_2 = 0$, $m_3 = 0$, $N = 200000$.\n3. $s = (6.1)^2$, $m_1 = 2$, $m_2 = 2$, $m_3 = 2$, $N = 200000$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each entry is the Monte Carlo estimate of $\\Phi_3$ in GeV$^2$ for the corresponding test case, in order. For example, your output should look like $[\\varphi_1,\\varphi_2,\\varphi_3]$, where each $\\varphi_i$ is a floating-point number.", "solution": "The problem requires the design and implementation of a Monte Carlo algorithm to compute the Lorentz-invariant three-body phase-space volume, $\\Phi_3$. The approach must be derived from first principles, starting with the definition of the Lorentz-invariant phase-space (LIPS) measure, and proceeding through a valid change of variables to obtain a one-dimensional integral suitable for Monte Carlo estimation.\n\nThe starting point is the definition of the $n$-body LIPS for a system with total four-momentum $P$ decaying into $n$ particles with four-momenta $p_i$ and masses $m_i$:\n$$\n\\mathrm{d}\\Phi_n(P; m_1,\\dots,m_n) \\equiv (2\\pi)^4\\,\\delta^{(4)}\\!\\left(P - \\sum_{i=1}^{n} p_i\\right)\\,\\prod_{i=1}^{n}\\frac{\\mathrm{d}^3 \\mathbf{p}_i}{(2\\pi)^3\\,2E_i}\n$$\nwhere $p_i^2 = m_i^2$. We are interested in the total phase-space volume for $n=3$, which is the integral of this measure over all final-state momenta:\n$$\n\\Phi_3(s; m_1,m_2,m_3) \\equiv \\int \\mathrm{d}\\Phi_3(P; m_1,m_2,m_3)\n$$\nHere, $s=P^2$ is the square of the center-of-mass energy. In the center-of-mass (CM) frame, $P=(\\sqrt{s}, \\mathbf{0})$. The dimensionality of $\\mathrm{d}\\Phi_n$ is $(\\text{Energy})^{2n-4}$, which for $n=3$ results in $(\\text{Energy})^2$, consistent with the required output unit of $\\text{GeV}^2$.\n\nThe key to simplifying this nine-dimensional momentum integral is to factorize the three-body decay into a sequence of two-body decays. We treat the decay as a two-step process: first, the initial state decays into particle $3$ and an intermediate virtual particle with four-momentum $p_{12} = p_1+p_2$. This intermediate state then decays into particles $1$ and $2$. The invariant mass squared of this intermediate state is $s_{12} \\equiv p_{12}^2 = (p_1+p_2)^2$.\n\nWe can express the total three-body phase-space integral by first integrating over the phase space of particles $1$ and $2$ for a fixed intermediate momentum $p_{12}$, and then integrating over the phase space of $p_{12}$ and $p_3$. This can be formally shown by inserting an identity into the integral, which leads to the following factorization:\n$$\n\\Phi_3 = \\int \\frac{\\mathrm{d}^3\\mathbf{p}_3}{(2\\pi)^3 2E_3} \\left[ \\int (2\\pi)^4 \\delta^{(4)}(P-p_3-p_{12}) \\frac{\\mathrm{d}^3\\mathbf{p}_1}{(2\\pi)^3 2E_1} \\frac{\\mathrm{d}^3\\mathbf{p}_2}{(2\\pi)^3 2E_2} \\right]\n$$\nwhere $p_{12}$ is fixed by the outer delta function to $p_{12}=P-p_3$. The term in the square brackets is the two-body phase space for the decay of a virtual particle of momentum $p_{12}$ into particles $1$ and $2$. This is $\\Phi_2(s_{12}; m_1, m_2)$, where $s_{12} = (P-p_3)^2 = s+m_3^2-2\\sqrt{s}E_3$. The integrated two-body phase-space volume is given by the well-known formula:\n$$\n\\Phi_2(Q^2; m_a, m_b) = \\frac{\\sqrt{\\lambda(Q^2, m_a^2, m_b^2)}}{8\\pi Q^2}\n$$\nwhere $\\lambda(x,y,z) = x^2+y^2+z^2-2xy-2xz-2yz$ is the Källén kinematic function. This result is dimensionless. Applying this, our expression for $\\Phi_3$ becomes:\n$$\n\\Phi_3 = \\int \\frac{\\mathrm{d}^3\\mathbf{p}_3}{(2\\pi)^3 2E_3} \\Phi_2(s-2\\sqrt{s}E_3+m_3^2; m_1, m_2)\n$$\nThe integral over $\\mathrm{d}^3\\mathbf{p}_3$ is performed in the CM frame. Using spherical coordinates, $\\mathrm{d}^3\\mathbf{p}_3 = |\\mathbf{p}_3|^2 \\mathrm{d}|\\mathbf{p}_3| \\mathrm{d}\\Omega_3$. The angular integral gives a factor of $4\\pi$. Using the relation $E_3^2 = |\\mathbf{p}_3|^2 + m_3^2 \\implies E_3\\mathrm{d}E_3 = |\\mathbf{p}_3|\\mathrm{d}|\\mathbf{p}_3|$, the integration measure becomes:\n$$\n\\frac{4\\pi |\\mathbf{p}_3|E_3 \\mathrm{d}E_3}{(2\\pi)^3 2E_3} = \\frac{|\\mathbf{p}_3|}{4\\pi^2} \\mathrm{d}E_3\n$$\nWe now change the integration variable from $E_3$ to $s_{12}$. From $s_{12} = s+m_3^2-2\\sqrt{s}E_3$, we have $|\\mathrm{d}E_3| = \\frac{\\mathrm{d}s_{12}}{2\\sqrt{s}}$. The momentum $|\\mathbf{p}_3|$ in the CM frame can also be expressed in terms of invariants: it is the momentum of either particle in the decay $P \\to p_3 + p_{12}$, so $|\\mathbf{p}_3| = \\frac{\\sqrt{\\lambda(s, m_3^2, s_{12})}}{2\\sqrt{s}}$.\n\nSubstituting these expressions back into the integral for $\\Phi_3$:\n$$\n\\Phi_3 = \\int \\left( \\frac{1}{4\\pi^2} \\frac{\\sqrt{\\lambda(s, m_3^2, s_{12})}}{2\\sqrt{s}} \\right) \\left( \\frac{\\sqrt{\\lambda(s_{12}, m_1^2, m_2^2)}}{8\\pi s_{12}} \\right) \\left( \\frac{\\mathrm{d}s_{12}}{2\\sqrt{s}} \\right)\n$$\nCombining the constant factors and terms depending on $s$ yields the final one-dimensional integral:\n$$\n\\Phi_3 = \\frac{1}{128\\pi^3 s} \\int_{s_{12}^{\\min}}^{s_{12}^{\\max}} \\frac{\\sqrt{\\lambda(s, s_{12}, m_3^2)} \\sqrt{\\lambda(s_{12}, m_1^2, m_2^2)}}{s_{12}} \\mathrm{d}s_{12}\n$$\nThe kinematic limits for $s_{12}$ are determined by physical constraints. The minimum value of $s_{12}$ is the threshold for producing particles $1$ and $2$, which is when they are created at rest with respect to each other: $s_{12}^{\\min} = (m_1+m_2)^2$. The maximum value occurs when particle $3$ is produced with the minimum possible momentum, i.e., at rest with respect to the CM frame of particles $1$ and $2$. This corresponds to the total available energy minus the rest mass of particle $3$ being devoted to the pair $(1,2)$, so $\\sqrt{s_{12}^{\\max}} = \\sqrt{s}-m_3$, which gives $s_{12}^{\\max} = (\\sqrt{s}-m_3)^2$. If $\\sqrt{s} < m_1+m_2+m_3$, then $s_{12}^{\\max} < s_{12}^{\\min}$, the integration range is empty, and $\\Phi_3=0$, correctly reflecting the kinematic threshold.\n\nTo estimate this integral using the Monte Carlo method, we define the integrand as:\n$$\nf(s_{12}) = \\frac{\\sqrt{\\lambda(s, s_{12}, m_3^2)} \\sqrt{\\lambda(s_{12}, m_1^2, m_2^2)}}{s_{12}}\n$$\nThe integral $I = \\int_{s_{12}^{\\min}}^{s_{12}^{\\max}} f(s_{12}) \\mathrm{d}s_{12}$ can be estimated as:\n$$\nI \\approx \\frac{s_{12}^{\\max} - s_{12}^{\\min}}{N} \\sum_{i=1}^{N} f(x_i)\n$$\nwhere $x_i$ are $N$ random numbers drawn from a uniform distribution over the interval $[s_{12}^{\\min}, s_{12}^{\\max}]$. This is achieved by generating random numbers $u_i$ from a uniform distribution on $[0,1]$ and applying the linear transformation $x_i = s_{12}^{\\min} + u_i (s_{12}^{\\max} - s_{12}^{\\min})$. The final estimate for $\\Phi_3$ is then obtained by multiplying the estimate of $I$ by the prefactor $\\frac{1}{128\\pi^3 s}$. The algorithm must implement this logic, including the Källén function, the calculation of kinematic limits, and the threshold check. For reproducibility, a fixed seed is used for the random number generator.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite for three-body phase-space volume estimation.\n    \"\"\"\n    \n    test_cases = [\n        # (s, m1, m2, m3, N)\n        (100.0, 1.0, 2.0, 3.0, 200000),\n        (25.0, 0.0, 0.0, 0.0, 200000),\n        ((6.1)**2, 2.0, 2.0, 2.0, 200000),\n    ]\n\n    results = []\n    seed = 12345\n\n    for case in test_cases:\n        s, m1, m2, m3, N = case\n        phi3_estimate = estimate_phi3(s, m1, m2, m3, N, seed)\n        results.append(phi3_estimate)\n\n    print(f\"[{','.join(f'{r:.8f}' for r in results)}]\")\n\ndef kallen_lambda(a, b, c):\n    \"\"\"\n    Calculates the Källén (or triangle) function lambda(a, b, c).\n    This function can accept scalar or numpy array inputs.\n    \"\"\"\n    return a**2 + b**2 + c**2 - 2*a*b - 2*a*c - 2*b*c\n\ndef estimate_phi3(s, m1, m2, m3, N, seed):\n    \"\"\"\n    Estimates the three-body phase-space volume Phi_3 using Monte Carlo integration.\n    \n    Args:\n        s (float): The square of the center-of-mass energy in GeV^2.\n        m1 (float): Mass of particle 1 in GeV.\n        m2 (float): Mass of particle 2 in GeV.\n        m3 (float): Mass of particle 3 in GeV.\n        N (int): The number of Monte Carlo samples.\n        seed (int): The seed for the random number generator.\n        \n    Returns:\n        float: The estimated value of Phi_3 in GeV^2.\n    \"\"\"\n    sqrts = np.sqrt(s)\n    \n    # Kinematic threshold check\n    if sqrts < (m1 + m2 + m3):\n        return 0.0\n\n    # Calculate kinematic limits for the invariant mass squared s_12\n    s12_min = (m1 + m2)**2\n    s12_max = (sqrts - m3)**2\n    \n    # If the integration range is non-positive, phase space is zero.\n    if s12_max <= s12_min:\n        return 0.0\n\n    # Set up the random number generator with a fixed seed for reproducibility.\n    rng = np.random.default_rng(seed)\n    \n    # Generate N uniform random samples in [0, 1)\n    u_samples = rng.uniform(0, 1, N)\n    \n    # Map samples to the integration domain [s12_min, s12_max]\n    s12_samples = s12_min + (s12_max - s12_min) * u_samples\n\n    # Pre-calculate squared masses\n    m1_sq = m1**2\n    m2_sq = m2**2\n    m3_sq = m3**2\n\n    # Calculate the two Källén functions\n    # Handle potential small negative values from floating point inaccuracies\n    lambda1 = kallen_lambda(s12_samples, m1_sq, m2_sq)\n    lambda1[lambda1 < 0] = 0.0\n    \n    lambda2 = kallen_lambda(s, s12_samples, m3_sq)\n    lambda2[lambda2 < 0] = 0.0\n    \n    # Calculate the integrand value for each sample of s12\n    # Ensure s12 is not zero to avoid division by zero. s12_samples will be > s12_min >= 0\n    # In the massless case, s12_min is 0, so we need to handle the s12=0 point.\n    # The integrand at s12=0 limit is 0, so we can safely replace NaNs with 0.\n    with np.errstate(divide='ignore', invalid='ignore'):\n        integrand_values = (np.sqrt(lambda1) * np.sqrt(lambda2)) / s12_samples\n    integrand_values = np.nan_to_num(integrand_values, nan=0.0, posinf=0.0, neginf=0.0)\n\n    # Calculate the Monte Carlo estimate of the integral part\n    integral_volume = s12_max - s12_min\n    integral_estimate = integral_volume * np.mean(integrand_values)\n\n    #\n    # Final formula for Phi_3:\n    # Phi_3 = (1 / (128 * pi^3 * s)) * Integral(...)\n    #\n    prefactor = 1.0 / (128.0 * np.pi**3 * s)\n    \n    phi3 = prefactor * integral_estimate\n    \n    return phi3\n\nsolve()\n```", "id": "3523401"}, {"introduction": "While simple Monte Carlo integration is powerful, its efficiency can often be dramatically improved. This practice explores stratified sampling, a premier variance reduction technique, by applying it to a common scenario in collider physics: categorizing events by jet multiplicity [@problem_id:3523390]. You will derive the optimal sample allocation, known as Neyman allocation, demonstrating how to strategically distribute computational effort across different strata to achieve the most precise estimate for a given total number of samples.", "problem": "You are tasked with analyzing Monte Carlo integration strategies for estimating an inclusive cross section in computational high-energy physics. Monte Carlo (MC) integration is used to estimate the expectation of random event weights. In a collider analysis, events are categorized by jet multiplicity, denoted $n_{\\text{jet}} \\in \\{0,1,2,3+\\}$, and we may stratify sampling across these categories. The inclusive cross section is defined as the expectation of the per-event weight under the event-generation distribution.\n\nStart from the following foundations:\n- The unbiased estimator of an expectation is the sample mean of independent and identically distributed draws.\n- The variance of an estimator for an expectation under simple random sampling is the population variance divided by the sample size.\n- The law of total variance states that for a random variable $Y$ and stratum index $H$, one has $\\mathrm{Var}(Y) = \\mathbb{E}[\\mathrm{Var}(Y \\mid H)] + \\mathrm{Var}(\\mathbb{E}[Y \\mid H])$.\n- In stratified sampling with strata weights $W_h = \\mathbb{P}(H = h)$ and per-stratum sample counts $n_h$, the stratified estimator of the mean is the weighted sum of per-stratum sample means.\n\nConsider the per-event weight $Y$ that contributes to the inclusive cross section. Let there be $H$ strata corresponding to the jet multiplicity categories $n_{\\text{jet}}$. For each stratum $h \\in \\{1,\\dots,H\\}$, let $W_h$ denote the stratum probability (fraction of events falling into the stratum), let $m_h = \\mathbb{E}[Y \\mid H = h]$ denote the conditional mean of per-event weight, and let $s_h = \\sqrt{\\mathrm{Var}(Y \\mid H = h)}$ denote the conditional standard deviation. Let $N$ denote the total available sample size. Your tasks are:\n1. Derive, from the foundations listed above, the variance of the simple (unstratified) MC estimator of the inclusive cross section and the variance of the stratified estimator as a function of $(W_h, m_h, s_h, n_h)$.\n2. Using a mathematically rigorous optimization argument, derive the per-stratum allocation $n_h$ that minimizes the variance of the stratified estimator subject to the constraint that $\\sum_{h=1}^H n_h = N$. This allocation is known from classical sampling theory as Neyman allocation.\n3. Derive the resulting minimal variance for the stratified estimator under your optimal allocation and express the variance reduction factor as the ratio between the stratified minimal variance and the unstratified variance.\n4. Implement a complete, runnable program that, for each test case below, computes the variance reduction factor as a float.\n\nAll answers in your derivation must be expressed in purely mathematical terms, and any discussion referring to physics should be framed only as motivation without changing the mathematical content. Angles are not involved, and the requested outputs are dimensionless, so no physical units are required. The final program output must strictly adhere to the output format specification given at the end of this problem.\n\nTest suite:\n- Test case A (four strata for $n_{\\text{jet}} \\in \\{0,1,2,3+\\}$):\n  - Stratum weights $W = [0.58, 0.26, 0.11, 0.05]$.\n  - Conditional means $m = [18.0, 28.0, 50.0, 95.0]$.\n  - Conditional standard deviations $s = [7.5, 12.0, 30.0, 75.0]$.\n  - Total samples $N = 100000$.\n- Test case B (four strata, equal means and equal standard deviations to probe the boundary where stratification provides no improvement):\n  - Stratum weights $W = [0.40, 0.30, 0.20, 0.10]$.\n  - Conditional means $m = [20.0, 20.0, 20.0, 20.0]$.\n  - Conditional standard deviations $s = [10.0, 10.0, 10.0, 10.0]$.\n  - Total samples $N = 25000$.\n- Test case C (three strata, including an effectively deterministic category with zero conditional variance):\n  - Stratum weights $W = [0.70, 0.20, 0.10]$.\n  - Conditional means $m = [25.0, 60.0, 40.0]$.\n  - Conditional standard deviations $s = [8.0, 0.0, 12.0]$.\n  - Total samples $N = 50000$.\n- Test case D (four strata with a rare, high-variance tail category):\n  - Stratum weights $W = [0.70, 0.20, 0.08, 0.02]$.\n  - Conditional means $m = [10.0, 35.0, 55.0, 150.0]$.\n  - Conditional standard deviations $s = [5.0, 20.0, 35.0, 200.0]$.\n  - Total samples $N = 200000$.\n\nFor each test case, compute the variance of the unstratified estimator of the inclusive cross section, the minimal variance of the stratified estimator under Neyman allocation, and return the variance reduction factor defined as the ratio of stratified minimal variance to unstratified variance, expressed as a float.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[result1,result2,result3]$). Each element must be the variance reduction factor for the corresponding test case in the order A, B, C, D.", "solution": "The problem as stated is scientifically grounded, well-posed, objective, and provides all necessary information for a unique solution. It is a standard application of sampling theory, specifically stratified sampling and Neyman allocation, framed within a computational physics context. All provided data is consistent and the tasks are mathematically formalizable. Therefore, the problem is valid. We proceed with the derivation and solution.\n\nThe objective is to analyze the variance of two Monte Carlo estimators for an inclusive cross section, which is mathematically equivalent to estimating the expectation $\\mu = \\mathbb{E}[Y]$ of a random variable $Y$ representing the per-event weight. The available information for $H$ strata consists of the stratum probabilities $W_h = \\mathbb{P}(H=h)$, conditional means $m_h = \\mathbb{E}[Y \\mid H=h]$, conditional standard deviations $s_h = \\sqrt{\\mathrm{Var}(Y \\mid H=h)}$, and the total sample size $N$.\n\n### Task 1: Variance of Simple and Stratified Estimators\n\nFirst, we derive the variance of the simple (unstratified) Monte Carlo estimator. The estimator for $\\mu = \\mathbb{E}[Y]$ based on a simple random sample of size $N$ is the sample mean:\n$$\n\\hat{\\mu}_{\\text{simple}} = \\frac{1}{N} \\sum_{i=1}^{N} Y_i\n$$\nThe variance of this estimator is given by:\n$$\n\\mathrm{Var}(\\hat{\\mu}_{\\text{simple}}) = \\frac{\\mathrm{Var}(Y)}{N}\n$$\nTo express $\\mathrm{Var}(Y)$ in terms of the given per-stratum quantities, we use the law of total variance:\n$$\n\\mathrm{Var}(Y) = \\mathbb{E}[\\mathrm{Var}(Y \\mid H)] + \\mathrm{Var}(\\mathbb{E}[Y \\mid H])\n$$\nThe first term, the expected conditional variance, is the weighted average of the per-stratum variances $s_h^2$:\n$$\n\\mathbb{E}[\\mathrm{Var}(Y \\mid H)] = \\sum_{h=1}^{H} W_h \\mathrm{Var}(Y \\mid H=h) = \\sum_{h=1}^{H} W_h s_h^2\n$$\nThe second term is the variance of the conditional expectation. The conditional expectation $\\mathbb{E}[Y \\mid H]$ is a discrete random variable that takes the value $m_h$ with probability $W_h$. Its variance is:\n$$\n\\mathrm{Var}(\\mathbb{E}[Y \\mid H]) = \\mathbb{E}[(\\mathbb{E}[Y \\mid H])^2] - (\\mathbb{E}[\\mathbb{E}[Y \\mid H]])^2\n$$\nThe terms on the right-hand side are:\n$$\n\\mathbb{E}[(\\mathbb{E}[Y \\mid H])^2] = \\sum_{h=1}^{H} W_h m_h^2\n$$\n$$\n\\mathbb{E}[\\mathbb{E}[Y \\mid H]] = \\sum_{h=1}^{H} W_h m_h\n$$\nCombining these, the variance of the conditional expectation is:\n$$\n\\mathrm{Var}(\\mathbb{E}[Y \\mid H]) = \\left(\\sum_{h=1}^{H} W_h m_h^2\\right) - \\left(\\sum_{h=1}^{H} W_h m_h\\right)^2\n$$\nThus, the total variance of $Y$ is:\n$$\n\\mathrm{Var}(Y) = \\sum_{h=1}^{H} W_h s_h^2 + \\sum_{h=1}^{H} W_h m_h^2 - \\left(\\sum_{h=1}^{H} W_h m_h\\right)^2\n$$\nAnd the variance of the simple estimator is:\n$$\n\\mathrm{Var}(\\hat{\\mu}_{\\text{simple}}) = \\frac{1}{N}\\left[ \\sum_{h=1}^{H} W_h s_h^2 + \\sum_{h=1}^{H} W_h m_h^2 - \\left(\\sum_{h=1}^{H} W_h m_h\\right)^2 \\right]\n$$\nNext, we consider the stratified estimator. Given per-stratum sample sizes $n_h$ such that $\\sum n_h = N$, the per-stratum sample mean is $\\hat{m}_h$. The stratified estimator for $\\mu$ is:\n$$\n\\hat{\\mu}_{\\text{strat}} = \\sum_{h=1}^{H} W_h \\hat{m}_h\n$$\nSince sampling across strata is independent, the variance of this estimator is the sum of the variances of each term:\n$$\n\\mathrm{Var}(\\hat{\\mu}_{\\text{strat}}) = \\mathrm{Var}\\left(\\sum_{h=1}^{H} W_h \\hat{m}_h\\right) = \\sum_{h=1}^{H} W_h^2 \\mathrm{Var}(\\hat{m}_h)\n$$\nThe variance of the sample mean in stratum $h$ is $\\mathrm{Var}(\\hat{m}_h) = \\frac{\\mathrm{Var}(Y \\mid H=h)}{n_h} = \\frac{s_h^2}{n_h}$. Substituting this in, we obtain the variance of the stratified estimator:\n$$\n\\mathrm{Var}(\\hat{\\mu}_{\\text{strat}}) = \\sum_{h=1}^{H} \\frac{W_h^2 s_h^2}{n_h}\n$$\n\n### Task 2: Optimal Sample Allocation (Neyman Allocation)\n\nWe seek to find the allocation of samples $\\{n_h\\}_{h=1}^H$ that minimizes the variance $\\mathrm{Var}(\\hat{\\mu}_{\\text{strat}})$ subject to the constraint $\\sum_{h=1}^H n_h = N$. We employ the method of Lagrange multipliers. The objective function to minimize is $V(n_1, \\dots, n_H) = \\sum_{h=1}^H \\frac{W_h^2 s_h^2}{n_h}$ subject to the constraint $g(n_1, \\dots, n_H) = \\sum_{h=1}^H n_h - N = 0$.\n\nThe Lagrangian is:\n$$\n\\mathcal{L}(n_1, \\dots, n_H, \\lambda) = \\sum_{h=1}^{H} \\frac{W_h^2 s_h^2}{n_h} + \\lambda \\left(\\sum_{h=1}^{H} n_h - N\\right)\n$$\nTo find the minimum, we set the partial derivatives with respect to each $n_k$ to zero:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial n_k} = -\\frac{W_k^2 s_k^2}{n_k^2} + \\lambda = 0 \\implies n_k^2 = \\frac{W_k^2 s_k^2}{\\lambda}\n$$\nAssuming $n_k > 0$, we take the positive square root:\n$$\nn_k = \\frac{W_k s_k}{\\sqrt{\\lambda}}\n$$\nThis shows that the optimal number of samples $n_k$ in a stratum is proportional to the product of its weight $W_k$ and standard deviation $s_k$. To find the value of $\\sqrt{\\lambda}$, we use the constraint:\n$$\n\\sum_{k=1}^{H} n_k = \\sum_{k=1}^{H} \\frac{W_k s_k}{\\sqrt{\\lambda}} = \\frac{1}{\\sqrt{\\lambda}} \\sum_{k=1}^{H} W_k s_k = N\n$$\nSolving for $\\sqrt{\\lambda}$:\n$$\n\\sqrt{\\lambda} = \\frac{\\sum_{j=1}^{H} W_j s_j}{N}\n$$\nSubstituting this back into the expression for $n_k$, we find the optimal allocation, known as Neyman allocation:\n$$\nn_k = N \\frac{W_k s_k}{\\sum_{j=1}^{H} W_j s_j}\n$$\n\n### Task 3: Minimal Variance and Reduction Factor\n\nWe now derive the minimal variance of the stratified estimator by substituting the optimal allocation $n_h$ back into the variance formula:\n$$\n\\mathrm{Var}_{\\text{min}}(\\hat{\\mu}_{\\text{strat}}) = \\sum_{h=1}^{H} \\frac{W_h^2 s_h^2}{n_h} = \\sum_{h=1}^{H} \\frac{W_h^2 s_h^2}{N \\frac{W_h s_h}{\\sum_{j} W_j s_j}}\n$$\nSimplifying the expression within the sum:\n$$\n\\frac{W_h^2 s_h^2}{N \\frac{W_h s_h}{\\sum_{j} W_j s_j}} = \\frac{W_h s_h}{N} \\left(\\sum_{j=1}^{H} W_j s_j\\right)\n$$\nSumming over $h$:\n$$\n\\mathrm{Var}_{\\text{min}}(\\hat{\\mu}_{\\text{strat}}) = \\frac{1}{N} \\left(\\sum_{j=1}^{H} W_j s_j\\right) \\left(\\sum_{h=1}^{H} W_h s_h\\right) = \\frac{1}{N} \\left(\\sum_{h=1}^{H} W_h s_h\\right)^2\n$$\nThe variance reduction factor is the ratio of the minimal stratified variance to the unstratified variance. The factor of $1/N$ cancels from the numerator and denominator:\n$$\nR = \\frac{\\mathrm{Var}_{\\text{min}}(\\hat{\\mu}_{\\text{strat}})}{\\mathrm{Var}(\\hat{\\mu}_{\\text{simple}})} = \\frac{\\frac{1}{N} \\left(\\sum_{h=1}^{H} W_h s_h\\right)^2}{\\frac{1}{N} \\mathrm{Var}(Y)} = \\frac{\\left(\\sum_{h=1}^{H} W_h s_h\\right)^2}{\\mathrm{Var}(Y)}\n$$\nSubstituting the full expression for $\\mathrm{Var}(Y)$, the final formula for the variance reduction factor is:\n$$\nR = \\frac{\\left(\\sum_{h=1}^{H} W_h s_h\\right)^2}{\\sum_{h=1}^{H} W_h s_h^2 + \\sum_{h=1}^{H} W_h m_h^2 - \\left(\\sum_{h=1}^{H} W_h m_h\\right)^2}\n$$\nThis formula will be implemented to compute the results for the given test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Calculates the variance reduction factor for stratified sampling\n    under Neyman allocation compared to simple random sampling for several test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test case A\n        {'W': np.array([0.58, 0.26, 0.11, 0.05]),\n         'm': np.array([18.0, 28.0, 50.0, 95.0]),\n         's': np.array([7.5, 12.0, 30.0, 75.0]),\n         'N': 100000},\n        # Test case B\n        {'W': np.array([0.40, 0.30, 0.20, 0.10]),\n         'm': np.array([20.0, 20.0, 20.0, 20.0]),\n         's': np.array([10.0, 10.0, 10.0, 10.0]),\n         'N': 25000},\n        # Test case C\n        {'W': np.array([0.70, 0.20, 0.10]),\n         'm': np.array([25.0, 60.0, 40.0]),\n         's': np.array([8.0, 0.0, 12.0]),\n         'N': 50000},\n        # Test case D\n        {'W': np.array([0.70, 0.20, 0.08, 0.02]),\n         'm': np.array([10.0, 35.0, 55.0, 150.0]),\n         's': np.array([5.0, 20.0, 35.0, 200.0]),\n         'N': 200000}\n    ]\n\n    results = []\n    for case in test_cases:\n        W = case['W']\n        m = case['m']\n        s = case['s']\n\n        # Numerator: Squared per-stratum weighted average of standard deviations.\n        # This is proportional to the minimal variance under Neyman allocation.\n        # Var_min_strat = (1/N) * (sum(W_h * s_h))^2\n        numerator = (np.sum(W * s))**2\n\n        # Denominator: Total variance of the random variable Y.\n        # This is proportional to the variance of the simple random sampling estimator.\n        # Var(Y) = E[Var(Y|H)] + Var(E[Y|H])\n        \n        # E[Var(Y|H)] = sum(W_h * s_h^2)\n        expected_cond_var = np.sum(W * s**2)\n        \n        # Var(E[Y|H]) = E[(E[Y|H])^2] - (E[E[Y|H]])^2\n        # E[Y|H] is a random variable taking value m_h with probability W_h.\n        var_cond_exp = np.sum(W * m**2) - (np.sum(W * m))**2\n        \n        # Total variance Var(Y)\n        denominator = expected_cond_var + var_cond_exp\n        \n        # The variance reduction factor is the ratio. The 1/N factor cancels.\n        if denominator == 0:\n            # Handle the case where total variance is zero (all s_h=0 and all m_h are equal)\n            # In this case, stratified variance is also zero, so reduction is perfect.\n            # However, if s_h are all zero but m_h differ, Var(Y)>0 but Var_strat_min=0.\n            # A divide-by-zero isn't expected for the given test cases, but it's good practice.\n            # For Test Case B, denominator is non-zero (100).\n            variance_reduction_factor = 0.0 if numerator == 0 else float('inf')\n        else:\n            variance_reduction_factor = numerator / denominator\n        \n        results.append(variance_reduction_factor)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3523390"}, {"introduction": "Modern theoretical predictions in high-energy physics, such as those at Next-to-Leading Order (NLO), often involve Monte Carlo integrations with both positive and negative weights, posing a significant challenge to variance control. This exercise tackles this advanced problem by modeling the calculation as a mixture of positive and negative weight contributions and applying the principles of stratified sampling to minimize the overall variance [@problem_id:3523424]. By deriving the optimal allocation for this specific case, you will learn how to adapt fundamental variance reduction techniques to handle the complex structure of state-of-the-art computational problems.", "problem": "You are given a Monte Carlo integration problem representative of total cross section evaluation at Next-to-Leading Order (NLO) in high-energy physics, where event weights can be positive or negative due to subtraction terms. The goal is to construct an unbiased, minimal-variance estimator for the total cross section when the event sample contains mixed positive and negative weights, and to analyze the effect of stratifying the sample by the sign of the weight.\n\nConsider a generator that emits events with a signed weight random variable denoted by $W$. The generator is a two-component mixture: with probability $p_{+}$ one draws a positive event with weight $W=+U_{+}$, and with probability $p_{-}=1-p_{+}$ one draws a negative event with weight $W=-U_{-}$. Conditional on the sign, the absolute weights are independent and follow Gamma distributions,\n$$U_{+} \\sim \\mathrm{Gamma}(k_{+},\\theta_{+}), \\quad U_{-} \\sim \\mathrm{Gamma}(k_{-},\\theta_{-}),$$\nwith shape parameters $k_{+}, k_{-} \\in \\mathbb{R}_{>0}$ and scale parameters $\\theta_{+}, \\theta_{-} \\in \\mathbb{R}_{>0}$. All random draws are independent. The total cross section (in arbitrary units) is modeled as the expectation\n$$I \\equiv \\mathbb{E}[W] = p_{+}\\,\\mathbb{E}[U_{+}] - p_{-}\\,\\mathbb{E}[U_{-}].$$\n\nYou are to work from fundamental Monte Carlo principles and basic probability facts only. The permitted starting points are: (i) linearity of expectation, (ii) the variance definition $\\mathrm{Var}(X)=\\mathbb{E}[X^{2}]-\\mathbb{E}[X]^{2}$, (iii) independence properties for sums and sample means, and (iv) the Gamma distribution moments $\\mathbb{E}[U]=k\\theta$ and $\\mathrm{Var}(U)=k\\theta^{2}$ for $U\\sim \\mathrm{Gamma}(k,\\theta)$. Do not assume any specialized variance-reduction formulas beyond these fundamentals.\n\nTasks:\n1. Using only the principles above, define an unbiased Monte Carlo estimator for $I$ based on $N$ independent draws from the mixture generator (no stratification). Derive its variance in terms of $p_{+}$, $k_{+}$, $\\theta_{+}$, $k_{-}$, $\\theta_{-}$, and $N$.\n2. Now consider stratification by sign. Allocate $n_{+}$ samples to the positive stratum and $n_{-}$ samples to the negative stratum, with $n_{+}+n_{-}=N$, and construct an unbiased stratified estimator of $I$ that uses the sign-strata means. Derive its variance as a function of $n_{+}$ and $n_{-}$. Then, determine the allocation $(n_{+},n_{-})$ that minimizes this variance subject to $n_{+}+n_{-}=N$ and $n_{+},n_{-} \\in \\mathbb{R}_{>0}$.\n3. Express the minimizing allocation as an optimal positive-stratum fraction $f_{+}^{\\star} \\equiv n_{+}^{\\star}/N$. Using your derivations, also express the minimal achievable variance under sign-stratification.\n\nImplementation requirements:\n- Implement a complete program that, using your closed-form expressions, computes for each test case:\n  - the optimal positive-stratum fraction $f_{+}^{\\star}$,\n  - the ratio $R_{\\mathrm{equal}} \\equiv \\mathrm{Var}_{\\mathrm{equal}}/\\mathrm{Var}_{\\mathrm{naive}}$, where $\\mathrm{Var}_{\\mathrm{equal}}$ is the variance with equal allocation $n_{+}=\\lfloor N/2 \\rfloor$, $n_{-}=N-\\lfloor N/2 \\rfloor$,\n  - the ratio $R_{\\mathrm{opt}} \\equiv \\mathrm{Var}_{\\mathrm{opt}}/\\mathrm{Var}_{\\mathrm{naive}}$, where $\\mathrm{Var}_{\\mathrm{opt}}$ is the minimal stratified variance under your optimal allocation.\n- All outputs are dimensionless real numbers.\n- Your program should produce a single line of output containing the results as a list of lists, one inner list per test case, each inner list in the order $[f_{+}^{\\star},R_{\\mathrm{equal}},R_{\\mathrm{opt}}]$. Numerically round each value to six decimal places before printing.\n\nTest suite:\n- Case A (balanced signs, moderate cancellation): $p_{+}=0.5$, $k_{+}=2.0$, $\\theta_{+}=1.0$, $k_{-}=5.0$, $\\theta_{-}=0.4$, $N=10000$.\n- Case B (rare but noisy negatives): $p_{+}=0.99$, $k_{+}=2.0$, $\\theta_{+}=1.0$, $k_{-}=3.0$, $\\theta_{-}=5.0$, $N=10000$.\n- Case C (near-exact cancellation with large means): $p_{+}=0.5$, $k_{+}=50.0$, $\\theta_{+}=0.2$, $k_{-}=50.0$, $\\theta_{-}=0.2$, $N=10000$.\n- Case D (all-positive boundary): $p_{+}=1.0$, $k_{+}=2.0$, $\\theta_{+}=1.0$, $k_{-}=1.0$, $\\theta_{-}=1.0$, $N=10000$.\n\nYour program must compute and print the list $[ [f_{+}^{\\star},R_{\\mathrm{equal}},R_{\\mathrm{opt}}]_{\\text{Case A}}, [f_{+}^{\\star},R_{\\mathrm{equal}},R_{\\mathrm{opt}}]_{\\text{Case B}}, [f_{+}^{\\star},R_{\\mathrm{equal}},R_{\\mathrm{opt}}]_{\\text{Case C}}, [f_{+}^{\\star},R_{\\mathrm{equal}},R_{\\mathrm{opt}}]_{\\text{Case D}} ]$ on a single line, with all floating-point numbers rounded to six decimal places, and with no additional text.", "solution": "We formalize the target as the expectation $I \\equiv \\mathbb{E}[W]$ under a two-component signed-weight mixture. By definition of the mixture and linearity of expectation,\n$$I = \\mathbb{E}[W] = p_{+}\\,\\mathbb{E}[U_{+}] - p_{-}\\,\\mathbb{E}[U_{-}].$$\nFor $U_{\\pm} \\sim \\mathrm{Gamma}(k_{\\pm},\\theta_{\\pm})$, the Gamma moments yield $\\mathbb{E}[U_{\\pm}] = k_{\\pm}\\theta_{\\pm}$ and $\\mathrm{Var}(U_{\\pm}) = k_{\\pm}\\theta_{\\pm}^{2}$. Let us denote the stratum means and variances by\n$$\\mu_{+} \\equiv \\mathbb{E}[U_{+}] = k_{+}\\theta_{+},\\quad \\sigma_{+}^{2} \\equiv \\mathrm{Var}(U_{+}) = k_{+}\\theta_{+}^{2},$$\n$$\\mu_{-} \\equiv \\mathbb{E}[U_{-}] = k_{-}\\theta_{-},\\quad \\sigma_{-}^{2} \\equiv \\mathrm{Var}(U_{-}) = k_{-}\\theta_{-}^{2}.$$\n\nTask 1 (naive mixture estimator). Draw $N$ independent samples $W_{1},\\dots,W_{N}$ from the mixture. The sample mean\n$$\\hat{I}_{\\mathrm{naive}} \\equiv \\frac{1}{N}\\sum_{i=1}^{N} W_{i}$$\nis unbiased by linearity of expectation, $\\mathbb{E}[\\hat{I}_{\\mathrm{naive}}]=I$. For its variance, independence gives\n$$\\mathrm{Var}(\\hat{I}_{\\mathrm{naive}}) = \\frac{1}{N}\\mathrm{Var}(W),$$\nso it suffices to compute $\\mathrm{Var}(W)$. By the law of total expectation and the definition $\\mathrm{Var}(W)=\\mathbb{E}[W^{2}]-\\mathbb{E}[W]^{2}$, and noting that $W^{2}$ equals $U_{+}^{2}$ in the positive stratum and $U_{-}^{2}$ in the negative stratum, we obtain\n$$\\mathbb{E}[W^{2}] = p_{+}\\,\\mathbb{E}[U_{+}^{2}] + p_{-}\\,\\mathbb{E}[U_{-}^{2}] = p_{+}\\,(\\sigma_{+}^{2}+\\mu_{+}^{2}) + p_{-}\\,(\\sigma_{-}^{2}+\\mu_{-}^{2}),$$\nwhere we used $\\mathbb{E}[U^{2}] = \\mathrm{Var}(U)+(\\mathbb{E}[U])^{2}$. Therefore,\n$$\\mathrm{Var}(\\hat{I}_{\\mathrm{naive}}) = \\frac{1}{N}\\left(p_{+}\\,(\\sigma_{+}^{2}+\\mu_{+}^{2}) + p_{-}\\,(\\sigma_{-}^{2}+\\mu_{-}^{2}) - I^{2}\\right),$$\nwith $I = p_{+}\\mu_{+} - p_{-}\\mu_{-}$.\n\nTask 2 (sign-stratified estimator and its variance). Allocate $n_{+}$ samples to the positive stratum and $n_{-}$ to the negative, with $n_{+}+n_{-}=N$. Within each stratum, form the sample means\n$$\\bar{U}_{+} \\equiv \\frac{1}{n_{+}}\\sum_{i=1}^{n_{+}} U_{+,i}, \\quad \\bar{U}_{-} \\equiv \\frac{1}{n_{-}}\\sum_{j=1}^{n_{-}} U_{-,j}.$$\nDefine the stratified estimator\n$$\\hat{I}_{\\mathrm{strat}}(n_{+},n_{-}) \\equiv p_{+}\\,\\bar{U}_{+} - p_{-}\\,\\bar{U}_{-}.$$\nBy linearity of expectation and the stratum means, this estimator is unbiased:\n$$\\mathbb{E}[\\hat{I}_{\\mathrm{strat}}] = p_{+}\\,\\mathbb{E}[\\bar{U}_{+}] - p_{-}\\,\\mathbb{E}[\\bar{U}_{-}] = p_{+}\\mu_{+} - p_{-}\\mu_{-} = I.$$\nThe variance, using independence between strata and within-stratum sample means, is\n$$\\mathrm{Var}(\\hat{I}_{\\mathrm{strat}}) = p_{+}^{2}\\,\\mathrm{Var}(\\bar{U}_{+}) + p_{-}^{2}\\,\\mathrm{Var}(\\bar{U}_{-}) = p_{+}^{2}\\,\\frac{\\sigma_{+}^{2}}{n_{+}} + p_{-}^{2}\\,\\frac{\\sigma_{-}^{2}}{n_{-}}.$$\nWe minimize this variance with respect to $n_{+},n_{-}$ subject to $n_{+}+n_{-}=N$ and $n_{+},n_{-} \\in \\mathbb{R}_{>0}$. Introducing a Lagrange multiplier $\\lambda$ for the constraint, minimize\n$$\\mathcal{L}(n_{+},n_{-},\\lambda) = p_{+}^{2}\\,\\frac{\\sigma_{+}^{2}}{n_{+}} + p_{-}^{2}\\,\\frac{\\sigma_{-}^{2}}{n_{-}} + \\lambda\\,(n_{+}+n_{-}-N).$$\nSetting partial derivatives to zero,\n$$\\frac{\\partial \\mathcal{L}}{\\partial n_{+}} = -p_{+}^{2}\\,\\frac{\\sigma_{+}^{2}}{n_{+}^{2}} + \\lambda = 0,\\quad \\frac{\\partial \\mathcal{L}}{\\partial n_{-}} = -p_{-}^{2}\\,\\frac{\\sigma_{-}^{2}}{n_{-}^{2}} + \\lambda = 0,$$\nwhich implies\n$$\\frac{p_{+}^{2}\\sigma_{+}^{2}}{n_{+}^{2}} = \\frac{p_{-}^{2}\\sigma_{-}^{2}}{n_{-}^{2}} \\quad \\Rightarrow \\quad \\frac{n_{+}}{n_{-}} = \\frac{p_{+}\\sigma_{+}}{p_{-}\\sigma_{-}}.$$\nUsing $n_{+}+n_{-}=N$, the optimal allocation is proportional to $p_{h}\\sigma_{h}$ in each stratum. Writing $S \\equiv p_{+}\\sigma_{+} + p_{-}\\sigma_{-}$, we obtain\n$$n_{+}^{\\star} = N\\,\\frac{p_{+}\\sigma_{+}}{S},\\quad n_{-}^{\\star} = N\\,\\frac{p_{-}\\sigma_{-}}{S}.$$\n\nTask 3 (optimal fraction and minimal variance). The optimal positive-stratum fraction is\n$$f_{+}^{\\star} \\equiv \\frac{n_{+}^{\\star}}{N} = \\frac{p_{+}\\,\\sigma_{+}}{p_{+}\\sigma_{+}+p_{-}\\sigma_{-}}.$$\nSubstituting $n_{+}^{\\star},n_{-}^{\\star}$ into the stratified variance yields the minimal variance\n$$\\mathrm{Var}(\\hat{I}_{\\mathrm{opt}}) = p_{+}^{2}\\frac{\\sigma_{+}^{2}}{n_{+}^{\\star}} + p_{-}^{2}\\frac{\\sigma_{-}^{2}}{n_{-}^{\\star}} = \\frac{(p_{+}\\sigma_{+}+p_{-}\\sigma_{-})^{2}}{N}.$$\nFor comparison, the equal-allocation stratification with $n_{+}=\\lfloor N/2 \\rfloor$ and $n_{-}=N-\\lfloor N/2 \\rfloor$ has variance\n$$\\mathrm{Var}(\\hat{I}_{\\mathrm{equal}}) = p_{+}^{2}\\frac{\\sigma_{+}^{2}}{n_{+}} + p_{-}^{2}\\frac{\\sigma_{-}^{2}}{n_{-}}.$$\nThe naive mixture variance is\n$$\\mathrm{Var}(\\hat{I}_{\\mathrm{naive}}) = \\frac{1}{N}\\left(p_{+}\\,(\\sigma_{+}^{2}+\\mu_{+}^{2}) + p_{-}\\,(\\sigma_{-}^{2}+\\mu_{-}^{2}) - (p_{+}\\mu_{+}-p_{-}\\mu_{-})^{2}\\right).$$\nTherefore, the requested ratios are\n$$R_{\\mathrm{equal}} \\equiv \\frac{\\mathrm{Var}(\\hat{I}_{\\mathrm{equal}})}{\\mathrm{Var}(\\hat{I}_{\\mathrm{naive}})},\\quad R_{\\mathrm{opt}} \\equiv \\frac{\\mathrm{Var}(\\hat{I}_{\\mathrm{opt}})}{\\mathrm{Var}(\\hat{I}_{\\mathrm{naive}})}.$$\n\nEdge-case behavior and interpretation:\n- If $p_{-}=0$, then $f_{+}^{\\star}=1$ and $\\mathrm{Var}(\\hat{I}_{\\mathrm{opt}})=\\sigma_{+}^{2}/N$, equal to the naive variance, while equal allocation wastes half the budget and doubles the variance.\n- If cancellation is strong (for example $p_{+}\\mu_{+}\\approx p_{-}\\mu_{-}$), $\\mathrm{Var}(\\hat{I}_{\\mathrm{naive}})$ can be dominated by $\\mathbb{E}[W^{2}]$, and stratification by sign can dramatically reduce variance by removing within-sample sign cancellations.\n- If $\\sigma_{+}=\\sigma_{-}$, then $f_{+}^{\\star}=p_{+}$ and stratification achieves $\\mathrm{Var}(\\hat{I}_{\\mathrm{opt}})=(\\sigma_{+})^{2}/N$, which typically improves upon the naive variance when $\\mu_{\\pm}$ are large in magnitude with near-cancellation.\n\nImplementation details. For each test case, compute $\\mu_{\\pm}=k_{\\pm}\\theta_{\\pm}$ and $\\sigma_{\\pm}=\\sqrt{k_{\\pm}}\\,\\theta_{\\pm}$. Then compute\n$$I=p_{+}\\mu_{+}-p_{-}\\mu_{-},\\quad \\mathrm{Var}(\\hat{I}_{\\mathrm{naive}})=\\frac{p_{+}(\\sigma_{+}^{2}+\\mu_{+}^{2})+p_{-}(\\sigma_{-}^{2}+\\mu_{-}^{2})-I^{2}}{N},$$\n$$\\mathrm{Var}(\\hat{I}_{\\mathrm{equal}})=p_{+}^{2}\\frac{\\sigma_{+}^{2}}{\\lfloor N/2 \\rfloor}+p_{-}^{2}\\frac{\\sigma_{-}^{2}}{N-\\lfloor N/2 \\rfloor},\\quad f_{+}^{\\star}=\\frac{p_{+}\\sigma_{+}}{p_{+}\\sigma_{+}+p_{-}\\sigma_{-}},$$\n$$\\mathrm{Var}(\\hat{I}_{\\mathrm{opt}})=\\frac{(p_{+}\\sigma_{+}+p_{-}\\sigma_{-})^{2}}{N},\\quad R_{\\mathrm{equal}}=\\frac{\\mathrm{Var}(\\hat{I}_{\\mathrm{equal}})}{\\mathrm{Var}(\\hat{I}_{\\mathrm{naive}})},\\quad R_{\\mathrm{opt}}=\\frac{\\mathrm{Var}(\\hat{I}_{\\mathrm{opt}})}{\\mathrm{Var}(\\hat{I}_{\\mathrm{naive}})}.$$\nFinally, round $f_{+}^{\\star}$, $R_{\\mathrm{equal}}$, and $R_{\\mathrm{opt}}$ to six decimal places and print them for each test case in the specified single-line list-of-lists format.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef compute_case(p_plus, k_plus, theta_plus, k_minus, theta_minus, N):\n    # Derived quantities\n    p_minus = 1.0 - p_plus\n\n    # Means and variances of Gamma(k, theta): mean=k*theta, var=k*theta^2\n    mu_plus = k_plus * theta_plus\n    var_plus = k_plus * (theta_plus ** 2)\n    sigma_plus = np.sqrt(var_plus)\n\n    mu_minus = k_minus * theta_minus\n    var_minus = k_minus * (theta_minus ** 2)\n    sigma_minus = np.sqrt(var_minus)\n\n    # Target mean\n    I = p_plus * mu_plus - p_minus * mu_minus\n\n    # Naive (mixture) estimator variance: Var(W)/N\n    EW2 = p_plus * (var_plus + mu_plus**2) + p_minus * (var_minus + mu_minus**2)\n    var_naive = (EW2 - I**2) / N\n\n    # Equal allocation stratification\n    n_plus_eq = N // 2\n    n_minus_eq = N - n_plus_eq\n    # Guard against division by zero in pathological N=0 (not present in tests)\n    var_equal = 0.0\n    if n_plus_eq > 0:\n        var_equal += (p_plus**2) * var_plus / n_plus_eq\n    if n_minus_eq > 0:\n        var_equal += (p_minus**2) * var_minus / n_minus_eq\n\n    # Optimal allocation (continuous) fraction and variance\n    denom = p_plus * sigma_plus + p_minus * sigma_minus\n    if denom == 0.0:\n        # Degenerate case: zero within-stratum std devs; variance is zero\n        f_opt = 0.5  # arbitrary since variance is zero; choose symmetric\n        var_opt = 0.0\n    else:\n        f_opt = (p_plus * sigma_plus) / denom\n        var_opt = (denom ** 2) / N\n\n    # Ratios; handle possible zero var_naive\n    if var_naive == 0.0:\n        # If var_naive is zero, ratios are undefined/infinite; set to 0 if numerators also zero else large\n        R_equal = 0.0 if var_equal == 0.0 else float('inf')\n        R_opt = 0.0 if var_opt == 0.0 else float('inf')\n    else:\n        R_equal = var_equal / var_naive\n        R_opt = var_opt / var_naive\n\n    # Clamp f_opt to [0,1] to avoid tiny numerical excursions\n    f_opt = min(max(f_opt, 0.0), 1.0)\n\n    # Round to six decimals\n    f_opt = round(f_opt, 6)\n    R_equal = round(R_equal, 6)\n    R_opt = round(R_opt, 6)\n\n    return [f_opt, R_equal, R_opt]\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each tuple: (p_plus, k_plus, theta_plus, k_minus, theta_minus, N)\n    test_cases = [\n        (0.5, 2.0, 1.0, 5.0, 0.4, 10000),   # Case A\n        (0.99, 2.0, 1.0, 3.0, 5.0, 10000),  # Case B\n        (0.5, 50.0, 0.2, 50.0, 0.2, 10000), # Case C\n        (1.0, 2.0, 1.0, 1.0, 1.0, 10000),   # Case D\n    ]\n\n    results = []\n    for case in test_cases:\n        p_plus, k_plus, theta_plus, k_minus, theta_minus, N = case\n        result = compute_case(p_plus, k_plus, theta_plus, k_minus, theta_minus, N)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    # Print as a single-line list of lists with six-decimal floats.\n    def format_inner(lst):\n        return \"[\" + \",\".join(f\"{x:.6f}\" for x in lst) + \"]\"\n\n    print(\"[\" + \",\".join(format_inner(r) for r in results) + \"]\")\n\nsolve()\n```", "id": "3523424"}]}