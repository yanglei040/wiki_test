{"hands_on_practices": [{"introduction": "The efficiency of a Metropolis-Hastings sampler critically depends on the properties of its proposal distribution. For a random-walk proposal, the step size is a crucial tuning parameter that determines how quickly the algorithm explores the target distribution. This exercise delves into the heart of MCMC efficiency analysis by tasking you with deriving the asymptotically optimal acceptance rate for a high-dimensional problem, a landmark result in the field [@problem_id:3521352]. By working through this derivation, you will gain a deep appreciation for the interplay between the Central Limit Theorem and MCMC performance, culminating in the famous target rate of approximately $0.234$.", "problem": "Consider a high-dimensional target distribution in $\\mathbb{R}^{d}$ that factors as a product $\\pi_{d}(x) = \\prod_{i=1}^{d} f(x_{i})$, where $f$ is a one-dimensional probability density function with twice continuously differentiable logarithm. Define $g(x) = \\frac{d}{dx} \\ln f(x)$ and assume the Fisher information $I = \\mathbb{E}_{X \\sim f}\\!\\left[g(X)^{2}\\right]$ is finite, and $\\mathbb{E}_{X \\sim f}\\!\\left[g'(X)\\right]$ exists. A Metropolis-Hastings (MH) random-walk proposal is used with $Y = X + \\sigma Z$, where $X \\in \\mathbb{R}^{d}$ is the current state, $Z \\sim \\mathcal{N}(0, I_{d})$ is a standard multivariate normal, and $\\sigma$ is a scalar step size. The acceptance probability is given by the standard MH rule $\\alpha(X,Y) = \\min\\!\\left\\{1, \\frac{\\pi_{d}(Y)}{\\pi_{d}(X)}\\right\\}$.\n\nYou are tasked with analyzing the asymptotic behavior of the random-walk Metropolis under the high-dimensional limit $d \\to \\infty$, in the scaling regime $\\sigma = \\frac{l}{\\sqrt{d}}$ for a fixed $l  0$. Work under stationarity $X \\sim \\pi_{d}$. Starting only from core definitions and well-tested facts (the Metropolis-Hastings acceptance rule, Taylor expansion of $\\ln f$, the Central Limit Theorem, and the integration-by-parts identity $\\mathbb{E}\\!\\left[g'(X)\\right] + \\mathbb{E}\\!\\left[g(X)^{2}\\right] = 0$ that holds for smooth $f$), perform the following steps:\n\n1. Derive the high-dimensional limit of the log-acceptance ratio and show it converges in distribution to a Gaussian shift. Use this to obtain a closed-form expression for the limiting average acceptance probability $\\alpha(l)$ as a function of $l$ and the Fisher information $I$.\n\n2. Using the diffusion-limit motivation commonly adopted in Markov chain Monte Carlo (MCMC), define the efficiency of the algorithm to be proportional to the expected squared jump distance, and in this scaling regime reduce it to a function of the form $S(l) = C \\, l^{2} \\alpha(l)$ for a constant $C  0$ independent of $l$. Determine the $l$ that maximizes $S(l)$.\n\n3. From the optimizing $l$, deduce the corresponding asymptotic optimal acceptance rate $\\alpha^{\\star}$ in the $d \\to \\infty$ limit. Provide the numerical value of $\\alpha^{\\star}$ rounded to three significant figures. Express the final answer as a pure number without units.", "solution": "The problem asks for an analysis of the random-walk Metropolis-Hastings algorithm in the high-dimensional limit $d \\to \\infty$. We will proceed in three steps as requested: first, deriving the limiting average acceptance probability; second, finding the optimal scaling parameter; and third, calculating the corresponding optimal acceptance rate.\n\n**Part 1: Limiting Average Acceptance Probability**\n\nThe target distribution is $\\pi_{d}(x) = \\prod_{i=1}^{d} f(x_{i})$ and the proposal is $Y = X + \\sigma Z$, where $Z \\sim \\mathcal{N}(0, I_d)$. The proposal density is symmetric, $q(Y|X) = q(X|Y)$, so the Metropolis-Hastings acceptance probability is $\\alpha(X,Y) = \\min\\!\\left\\{1, \\frac{\\pi_{d}(Y)}{\\pi_{d}(X)}\\right\\}$. We are interested in the behavior of the log-acceptance ratio, $\\Delta = \\ln\\left(\\frac{\\pi_{d}(Y)}{\\pi_{d}(X)}\\right)$, as $d \\to \\infty$.\n\nGiven the product form of the target, the log-ratio is a sum:\n$$\n\\Delta = \\ln\\left(\\frac{\\prod_{i=1}^{d} f(Y_{i})}{\\prod_{i=1}^{d} f(X_{i})}\\right) = \\sum_{i=1}^{d} \\left( \\ln f(Y_{i}) - \\ln f(X_{i}) \\right)\n$$\nThe proposal for each component is $Y_i = X_i + \\sigma Z_i$, where $Z_i \\sim \\mathcal{N}(0,1)$ are independent. We are in the scaling regime $\\sigma = \\frac{l}{\\sqrt{d}}$ for some constant $l  0$. Since $\\sigma \\to 0$ as $d \\to \\infty$, we can perform a Taylor expansion of $\\ln f(Y_i)$ around $X_i$:\n$$\n\\ln f(Y_i) = \\ln f(X_i) + (Y_i - X_i) \\left.\\frac{d}{dx}\\ln f(x)\\right|_{x=X_i} + \\frac{1}{2}(Y_i - X_i)^2 \\left.\\frac{d^2}{dx^2}\\ln f(x)\\right|_{x=X_i} + O((Y_i-X_i)^3)\n$$\nLet $g(x) = \\frac{d}{dx}\\ln f(x)$. The expansion becomes:\n$$\n\\ln f(Y_i) - \\ln f(X_i) = (\\sigma Z_i) g(X_i) + \\frac{1}{2}(\\sigma Z_i)^2 g'(X_i) + O(\\sigma^3)\n$$\nSubstituting this into the sum for $\\Delta$ and using $\\sigma = l/\\sqrt{d}$:\n$$\n\\Delta = \\sum_{i=1}^{d} \\left[ \\frac{l}{\\sqrt{d}} Z_i g(X_i) + \\frac{l^2}{2d} Z_i^2 g'(X_i) + O\\left(d^{-3/2}\\right) \\right]\n$$\n$$\n\\Delta = \\frac{l}{\\sqrt{d}} \\sum_{i=1}^{d} Z_i g(X_i) + \\frac{l^2}{2d} \\sum_{i=1}^{d} Z_i^2 g'(X_i) + O\\left(d^{-1/2}\\right)\n$$\nWe analyze the two sums in the limit $d \\to \\infty$. The algorithm is assumed to be in stationarity, so the current state $X$ is drawn from $\\pi_d$, meaning the components $X_i$ are independent and identically distributed (i.i.d.) draws from $f$. The $Z_i$ terms are i.i.d. $\\mathcal{N}(0,1)$ and are independent of the $X_i$.\n\nLet $W_i = Z_i g(X_i)$. The terms $W_i$ are i.i.d. random variables. We calculate their mean and variance. The expectation of $g(X_i)$ is $\\mathbb{E}[g(X_i)] = \\int_{-\\infty}^{\\infty} g(x)f(x)dx = \\int_{-\\infty}^{\\infty} f'(x)dx = [f(x)]_{-\\infty}^{\\infty} = 0$, since $f$ is a probability density function. Thus, $\\mathbb{E}[W_i] = \\mathbb{E}[Z_i]\\mathbb{E}[g(X_i)] = 0 \\cdot 0 = 0$.\nThe variance is $\\text{Var}(W_i) = \\mathbb{E}[W_i^2] - (\\mathbb{E}[W_i])^2 = \\mathbb{E}[Z_i^2 g(X_i)^2] = \\mathbb{E}[Z_i^2]\\mathbb{E}[g(X_i)^2] = 1 \\cdot I = I$, where $I$ is the Fisher information.\nThe first term in $\\Delta$ is $T_1 = l \\left( \\frac{1}{\\sqrt{d}} \\sum_{i=1}^{d} W_i \\right)$. By the Central Limit Theorem, $\\frac{1}{\\sqrt{d}} \\sum_{i=1}^{d} W_i \\xrightarrow{d} \\mathcal{N}(0, I)$. Therefore, $T_1 \\xrightarrow{d} \\mathcal{N}(0, l^2 I)$.\n\nThe second term is $T_2 = \\frac{l^2}{2} \\left( \\frac{1}{d} \\sum_{i=1}^{d} Z_i^2 g'(X_i) \\right)$. By the Law of Large Numbers, the average converges in probability to its expectation:\n$$\n\\frac{1}{d} \\sum_{i=1}^{d} Z_i^2 g'(X_i) \\xrightarrow{p} \\mathbb{E}[Z_i^2 g'(X_i)] = \\mathbb{E}[Z_i^2]\\mathbb{E}[g'(X_i)] = 1 \\cdot \\mathbb{E}[g'(X_i)]\n$$\nUsing the provided identity $\\mathbb{E}[g'(X)] + \\mathbb{E}[g(X)^2] = 0$, we have $\\mathbb{E}[g'(X_i)] = -\\mathbb{E}[g(X_i)^2] = -I$.\nThus, $T_2 \\xrightarrow{p} \\frac{l^2}{2} (-I) = -\\frac{1}{2}l^2 I$.\n\nBy Slutsky's theorem, the limiting distribution of $\\Delta = T_1 + T_2 + O(d^{-1/2})$ is the sum of the limits of $T_1$ and $T_2$:\n$$\n\\Delta \\xrightarrow{d} \\mathcal{N}\\left(-\\frac{1}{2}l^2 I, l^2 I\\right)\n$$\nThe average acceptance probability in the limit is $\\alpha(l) = \\mathbb{E}[\\min(1, \\exp(\\Delta_{\\infty}))]$ where $\\Delta_{\\infty} \\sim \\mathcal{N}\\left(-\\frac{1}{2}l^2 I, l^2 I\\right)$. Let $\\Delta_{\\infty} = -\\frac{1}{2}l^2 I + l\\sqrt{I} \\zeta$, where $\\zeta \\sim \\mathcal{N}(0,1)$. The expectation is:\n$$\n\\alpha(l) = \\int_{-\\infty}^{\\infty} \\min(1, \\exp(-\\frac{1}{2}l^2 I + l\\sqrt{I}\\zeta)) \\phi(\\zeta) d\\zeta\n$$\nwhere $\\phi(\\zeta)$ is the standard normal PDF. The term $\\exp(-\\frac{1}{2}l^2 I + l\\sqrt{I}\\zeta)$ is less than $1$ when its argument is negative, i.e., $l\\sqrt{I}\\zeta  \\frac{1}{2}l^2 I$, which simplifies to $\\zeta  \\frac{l\\sqrt{I}}{2}$.\n$$\n\\alpha(l) = \\int_{-\\infty}^{l\\sqrt{I}/2} \\exp(-\\frac{1}{2}l^2 I + l\\sqrt{I}\\zeta) \\phi(\\zeta) d\\zeta + \\int_{l\\sqrt{I}/2}^{\\infty} 1 \\cdot \\phi(\\zeta) d\\zeta\n$$\nThe second integral is the tail probability $P(\\zeta  l\\sqrt{I}/2) = 1 - \\Phi(l\\sqrt{I}/2) = \\Phi(-l\\sqrt{I}/2)$, where $\\Phi$ is the standard normal CDF.\nFor the first integral, we complete the square in the exponent. The combined exponent inside the integral is $-\\frac{1}{2}l^2 I + l\\sqrt{I}\\zeta - \\frac{\\zeta^2}{2}$. This is equal to $-\\frac{1}{2}(\\zeta - l\\sqrt{I})^2$. So the integrand is $\\frac{1}{\\sqrt{2\\pi}} \\exp(-\\frac{1}{2}(\\zeta-l\\sqrt{I})^2)$, which is the PDF of a $\\mathcal{N}(l\\sqrt{I}, 1)$ variable.\nThe integral becomes $\\int_{-\\infty}^{l\\sqrt{I}/2} \\frac{1}{\\sqrt{2\\pi}} \\exp(-\\frac{1}{2}(\\zeta-l\\sqrt{I})^2) d\\zeta$. Let $u = \\zeta-l\\sqrt{I}$. The integral is $\\int_{-\\infty}^{-l\\sqrt{I}/2} \\phi(u) du = \\Phi(-l\\sqrt{I}/2)$.\nTherefore, $\\alpha(l) = \\Phi(-l\\sqrt{I}/2) + \\Phi(-l\\sqrt{I}/2) = 2\\Phi(-l\\sqrt{I}/2)$.\n\n**Part 2: Optimization of MCMC Efficiency**\n\nThe efficiency of the sampler is taken to be proportional to the expected squared jump distance, $S(l) \\propto \\mathbb{E}[\\alpha(X,Y) \\|Y-X\\|^2]$. In the high-dimensional limit, the acceptance probability $\\alpha(X,Y)$ converges to the deterministic value $\\alpha(l)$ derived above. The jump vector is $Y-X = \\sigma Z = \\frac{l}{\\sqrt{d}}Z$. The squared jump distance is $\\|Y-X\\|^2 = \\frac{l^2}{d}\\|Z\\|^2 = \\frac{l^2}{d}\\sum_{i=1}^d Z_i^2$. The expected squared jump distance is $\\mathbb{E}[\\|Y-X\\|^2] = \\frac{l^2}{d} \\sum_{i=1}^d \\mathbb{E}[Z_i^2] = \\frac{l^2}{d} \\cdot d = l^2$.\nThe efficiency function is thus proportional to $l^2 \\alpha(l)$. We seek to maximize $S(l) = l^2 \\alpha(l) = 2l^2 \\Phi(-l\\sqrt{I}/2)$. To simplify, let $k = l\\sqrt{I}$, and we maximize $f(k) \\propto k^2 \\Phi(-k/2)$ with respect to $k0$.\nWe set the derivative to zero: $\\frac{d}{dk} \\left(k^2 \\Phi(-k/2)\\right) = 0$.\n$$\n2k \\Phi(-k/2) + k^2 \\cdot \\phi(-k/2) \\cdot \\left(-\\frac{1}{2}\\right) = 0\n$$\nSince $k0$, we can divide by $k$. Using $\\phi(-u)=\\phi(u)$:\n$$\n2 \\Phi(-k/2) - \\frac{k}{2} \\phi(k/2) = 0\n$$\nLetting $x = k/2 = l\\sqrt{I}/2$, the condition for the optimal $l$ is given by the solution to the transcendental equation:\n$$\n2\\Phi(-x) = x\\phi(x)\n$$\nThis equation implicitly defines the optimal scaled parameter $l$.\n\n**Part 3: Asymptotic Optimal Acceptance Rate**\n\nThe optimal acceptance rate, $\\alpha^\\star$, corresponds to the value of $\\alpha(l)$ at the optimal scaling parameter $l^\\star$ found in Part 2.\nFrom Part 1, the acceptance rate is $\\alpha(l) = 2\\Phi(-l\\sqrt{I}/2)$.\nAt the optimum, we have $\\alpha^\\star = 2\\Phi(-l^\\star\\sqrt{I}/2)$.\nFrom Part 2, the optimality condition is $2\\Phi(-l^\\star\\sqrt{I}/2) = \\frac{l^\\star\\sqrt{I}}{2} \\phi(l^\\star\\sqrt{I}/2)$.\nLetting $x^\\star = l^\\star\\sqrt{I}/2$, we can write the optimal acceptance rate as:\n$$\n\\alpha^\\star = x^\\star \\phi(x^\\star)\n$$\nwhere $x^\\star$ is the positive solution to $2\\Phi(-x) = x\\phi(x)$. This equation must be solved numerically. Let $h(x) = x\\phi(x) - 2\\Phi(-x)$. Using a numerical root-finding method (such as Newton's method), we find the solution to be $x^\\star \\approx 1.1906$.\nWe can now calculate the numerical value of the optimal acceptance rate:\n$$\n\\alpha^\\star = x^\\star\\phi(x^\\star) = x^\\star \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{(x^\\star)^2}{2}\\right)\n$$\nSubstituting $x^\\star \\approx 1.1906$:\n$$\n\\alpha^\\star \\approx 1.1906 \\cdot \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{1.1906^2}{2}\\right) \\approx 1.1906 \\cdot \\frac{1}{2.5066} \\exp(-0.70877) \\approx 1.1906 \\cdot 0.39894 \\cdot 0.49220 \\approx 0.23377\n$$\nRounding to three significant figures, the optimal acceptance rate is $0.234$.", "answer": "$$\n\\boxed{0.234}\n$$", "id": "3521352"}, {"introduction": "Many problems in high-energy physics involve parameters that are subject to constraints, such as branching fractions which must be positive and sum to one. Standard MCMC proposals on an unconstrained space are not directly applicable. This exercise introduces a powerful and standard technique: reparameterizing the constrained space onto an unconstrained one, allowing for simple proposals [@problem_id:3521320]. Your task is to derive the Jacobian determinant for the additive log-ratio transform, a key component in the change-of-variables formula needed to correctly define the target density in the new, unconstrained space.", "problem": "A heavy neutral resonance in a high-energy collider experiment decays into $d$ exclusive channels. Let the branching fraction vector be $b = (b_1, b_2, \\dots, b_d)$ constrained to the unit simplex, so that $b_i  0$ for all $i$ and $\\sum_{i=1}^{d} b_i = 1$. You observe counts $n_1, n_2, \\dots, n_d$ from $N = \\sum_{i=1}^{d} n_i$ decays, and you model the likelihood by a multinomial distribution $L(n \\mid b) \\propto \\prod_{i=1}^{d} b_i^{n_i}$ with a Dirichlet prior $p(b) \\propto \\prod_{i=1}^{d} b_i^{a_i - 1}$, where the hyperparameters $a_i  0$ are fixed. You wish to sample from the posterior distribution $\\pi_Y(b \\mid n) \\propto L(n \\mid b) p(b)$ using Markov chain Monte Carlo (MCMC), specifically the Metropolis-Hastings (MH) method, while respecting the simplex constraints.\n\nTo build an unconstrained parameterization, consider the additive log-ratio logistic transform from the simplex to $\\mathbb{R}^{d-1}$ defined by\n$$\nz_i = \\ln\\!\\left(\\frac{b_i}{b_d}\\right), \\quad i = 1, 2, \\dots, d-1,\n$$\nwith inverse map\n$$\nb_i(z) = \\frac{\\exp(z_i)}{1 + \\sum_{k=1}^{d-1} \\exp(z_k)}, \\quad i = 1, \\dots, d-1, \\qquad\nb_d(z) = \\frac{1}{1 + \\sum_{k=1}^{d-1} \\exp(z_k)}.\n$$\nYou design an MH scheme that proposes in the unconstrained space via a symmetric Gaussian random walk $q(z' \\mid z)$ on $\\mathbb{R}^{d-1}$, and then maps back to $b' = b(z')$ for evaluation in the physical space. By the change-of-variables formula, the target density in $z$-space is\n$$\n\\pi_Z(z \\mid n) \\propto \\pi_Y(b(z) \\mid n) \\left| \\det\\!\\left(\\frac{\\partial b}{\\partial z}\\right) \\right|,\n$$\nwhere $\\frac{\\partial b}{\\partial z}$ denotes the $(d-1) \\times (d-1)$ Jacobian matrix of the inverse map $z \\mapsto (b_1, \\dots, b_{d-1})$.\n\nStarting from the fundamental definitions of the Metropolis-Hastings acceptance rule and the change-of-variables theorem for probability densities, derive the required Jacobian factor $\\left| \\det\\!\\left(\\frac{\\partial b}{\\partial z}\\right) \\right|$ as a closed-form expression in terms of $b_1, \\dots, b_d$. Express your final answer as a single analytic expression. No numerical approximation is required.", "solution": "The problem requires the derivation of the Jacobian factor $\\left| \\det\\left(\\frac{\\partial b}{\\partial z}\\right) \\right|$ for a change of variables from a constrained simplex space to an unconstrained Euclidean space. This factor is essential for correctly formulating the Metropolis-Hastings acceptance probability when proposing moves in the unconstrained space.\n\nLet the vector of unconstrained variables be $z = (z_1, z_2, \\dots, z_{d-1}) \\in \\mathbb{R}^{d-1}$. The problem defines the mapping from this space back to the first $d-1$ components of the branching fraction vector $b = (b_1, b_2, \\dots, b_d)$. The vector $b$ lives on the $(d-1)$-simplex $\\mathcal{S}^{d-1}$, defined by $b_i  0$ for all $i=1, \\dots, d$ and $\\sum_{i=1}^{d} b_i = 1$. The inverse map is given by:\n$$\nb_i(z) = \\frac{\\exp(z_i)}{1 + \\sum_{k=1}^{d-1} \\exp(z_k)}, \\quad i = 1, \\dots, d-1\n$$\n$$\nb_d(z) = \\frac{1}{1 + \\sum_{k=1}^{d-1} \\exp(z_k)}\n$$\nThe quantity to be calculated is the absolute value of the determinant of the Jacobian matrix $J = \\frac{\\partial b}{\\partial z}$, where $J$ is a $(d-1) \\times (d-1)$ matrix with entries $J_{ij} = \\frac{\\partial b_i}{\\partial z_j}$ for $i, j \\in \\{1, 2, \\dots, d-1\\}$.\n\nFirst, we calculate the partial derivatives $J_{ij}$. Let us define the denominator as $S(z) = 1 + \\sum_{k=1}^{d-1} \\exp(z_k)$. From the provided map definitions, we can establish a direct relationship between $b_i$ and $S$.\nNotice that $b_d = 1/S$ and $b_i = b_d \\exp(z_i)$ for $i=1, \\dots, d-1$. This implies $\\exp(z_i) = b_i/b_d$.\nWe can verify the consistency of $S(z)$:\n$$\nS(z) = 1 + \\sum_{k=1}^{d-1} \\exp(z_k) = 1 + \\sum_{k=1}^{d-1} \\frac{b_k}{b_d} = \\frac{b_d + \\sum_{k=1}^{d-1} b_k}{b_d} = \\frac{\\sum_{k=1}^{d} b_k}{b_d} = \\frac{1}{b_d}\n$$\nThis confirms that $b_i = \\exp(z_i) / S = \\exp(z_i) b_d$.\n\nNow, we compute the partial derivatives $\\frac{\\partial b_i}{\\partial z_j}$ for $i, j \\in \\{1, \\dots, d-1\\}$ using the quotient rule.\n\nCase 1: $i = j$.\n$$\n\\frac{\\partial b_i}{\\partial z_i} = \\frac{\\partial}{\\partial z_i} \\left( \\frac{\\exp(z_i)}{S(z)} \\right) = \\frac{\\frac{\\partial \\exp(z_i)}{\\partial z_i} S(z) - \\exp(z_i) \\frac{\\partial S(z)}{\\partial z_i}}{S(z)^2}\n$$\nSince $\\frac{\\partial S(z)}{\\partial z_i} = \\exp(z_i)$, we have:\n$$\n\\frac{\\partial b_i}{\\partial z_i} = \\frac{\\exp(z_i) S(z) - \\exp(z_i) \\exp(z_i)}{S(z)^2} = \\frac{\\exp(z_i)}{S(z)} - \\left( \\frac{\\exp(z_i)}{S(z)} \\right)^2\n$$\nSubstituting $b_i = \\exp(z_i)/S(z)$, we get:\n$$\n\\frac{\\partial b_i}{\\partial z_i} = b_i - b_i^2 = b_i(1 - b_i)\n$$\n\nCase 2: $i \\neq j$.\n$$\n\\frac{\\partial b_i}{\\partial z_j} = \\frac{\\partial}{\\partial z_j} \\left( \\frac{\\exp(z_i)}{S(z)} \\right) = \\frac{0 \\cdot S(z) - \\exp(z_i) \\frac{\\partial S(z)}{\\partial z_j}}{S(z)^2}\n$$\nSince $\\frac{\\partial S(z)}{\\partial z_j} = \\exp(z_j)$, we have:\n$$\n\\frac{\\partial b_i}{\\partial z_j} = \\frac{-\\exp(z_i) \\exp(z_j)}{S(z)^2} = - \\left( \\frac{\\exp(z_i)}{S(z)} \\right) \\left( \\frac{\\exp(z_j)}{S(z)} \\right)\n$$\nSubstituting $b_i = \\exp(z_i)/S(z)$ and $b_j = \\exp(z_j)/S(z)$, we get:\n$$\n\\frac{\\partial b_i}{\\partial z_j} = -b_i b_j\n$$\n\nThe entries of the Jacobian matrix $J$ are therefore:\n$$\nJ_{ij} = \\frac{\\partial b_i}{\\partial z_j} =\n\\begin{cases}\nb_i(1 - b_i)  \\text{if } i = j \\\\\n-b_i b_j  \\text{if } i \\neq j\n\\end{cases}\n$$\nThis can be written compactly as $J_{ij} = \\delta_{ij} b_i - b_i b_j$, where $\\delta_{ij}$ is the Kronecker delta.\n\nWe now compute the determinant of this $(d-1) \\times (d-1)$ matrix $J$. We can write $J = \\text{diag}(b_1, \\dots, b_{d-1}) - \\mathbf{b}\\mathbf{b}^T$, where $\\mathbf{b} = (b_1, \\dots, b_{d-1})^T$. Using the matrix determinant lemma, $\\det(A - uv^T) = \\det(A)(1 - v^T A^{-1} u)$, with $A = \\text{diag}(\\mathbf{b})$ and $u=v=\\mathbf{b}$.\n$$\n\\det(J) = \\det(\\text{diag}(\\mathbf{b})) \\left(1 - \\mathbf{b}^T (\\text{diag}(\\mathbf{b}))^{-1} \\mathbf{b}\\right)\n$$\nThe determinant of the diagonal part is $\\prod_{i=1}^{d-1} b_i$. The term in the parenthesis is:\n$$\n1 - (b_1, \\dots, b_{d-1}) \\begin{pmatrix} 1/b_1   0 \\\\  \\ddots  \\\\ 0   1/b_{d-1} \\end{pmatrix} \\begin{pmatrix} b_1 \\\\ \\vdots \\\\ b_{d-1} \\end{pmatrix} = 1 - (1, \\dots, 1) \\begin{pmatrix} b_1 \\\\ \\vdots \\\\ b_{d-1} \\end{pmatrix} = 1 - \\sum_{k=1}^{d-1} b_k\n$$\nUsing the simplex constraint $\\sum_{k=1}^{d} b_k = 1$, we have $1 - \\sum_{k=1}^{d-1} b_k = b_d$.\nCombining these results, the determinant of the Jacobian matrix $J$ is:\n$$\n\\det(J) = \\left( \\prod_{i=1}^{d-1} b_i \\right) b_d = \\prod_{i=1}^{d} b_i\n$$\nThe final step is to take the absolute value as required for the change of variables formula for probability densities. The problem states that $b_i  0$ for all $i$. Consequently, the product $\\prod_{i=1}^{d} b_i$ is strictly positive.\nThus, the required Jacobian factor is:\n$$\n\\left| \\det\\left(\\frac{\\partial b}{\\partial z}\\right) \\right| = \\left| \\prod_{i=1}^{d} b_i \\right| = \\prod_{i=1}^{d} b_i\n$$\nThis is the closed-form expression for the Jacobian determinant in terms of the branching fractions $b_1, \\dots, b_d$.", "answer": "$$\\boxed{\\prod_{i=1}^{d} b_i}$$", "id": "3521320"}, {"introduction": "When sampling from distributions with multiple, well-separated modes—a common feature in lattice gauge theories with distinct topological sectors—standard MCMC algorithms can become trapped, failing to explore the full state space. Parallel tempering, or replica-exchange MCMC, is a powerful technique designed to overcome this challenge by simulating the system at multiple temperatures. The key to an efficient implementation is the design of the temperature ladder itself, which is the focus of this practical coding exercise [@problem_id:3521299]. You will derive and implement an algorithm to construct an optimal temperature ladder based on a physically motivated model, translating theoretical principles into a working solution.", "problem": "You are tasked with constructing a complete, runnable program that designs a temperature ladder for tempered transitions tailored to sampling distinct instanton sectors in a lattice gauge theory setting. The target distribution over gauge fields is given by a Boltzmann weight with action $S(U)$, so that configurations are distributed according to $\\pi_{\\beta}(U) \\propto \\exp(-\\beta S(U))$ where $\\beta = 1/T$ is the inverse temperature and $T$ is the temperature. Consider a set of $m$ replicas at temperatures $T_1  T_2  \\dots  T_m$, with replica exchange moves allowed between adjacent temperatures. For a swap proposal that exchanges configurations between replicas $i$ and $j$, the Metropolis-Hastings acceptance probability is\n$$\n\\alpha_{i \\leftrightarrow j} = \\min\\left\\{1, \\exp\\left[(1/T_i - 1/T_j)(S(U_j) - S(U_i))\\right]\\right\\}.\n$$\nYour objective is to design the temperature ladder $T_1  T_2  \\dots  T_m$ that maximizes the expected swap acceptance between adjacent replicas subject to a fixed computational budget determined by a fixed number of replicas $m$ and fixed boundary temperatures $T_1$ and $T_m$. You must build the ladder from fundamental principles of Markov Chain Monte Carlo (MCMC), particularly the Metropolis-Hastings method, and well-tested thermodynamic facts.\n\nStarting point and modeling assumptions:\n- Use the canonical ensemble with inverse temperature $\\beta = 1/T$ and Boltzmann constant set to $k_B = 1$.\n- Model the action as a random variable $E$ with a Gaussian distribution that is consistent with constant heat capacity $C_v$:\n  - Mean $\\mu(T) = E_0 + C_v T$,\n  - Variance $\\sigma^2(T) = C_v T^2$.\n- These relations agree with standard canonical identities: $\\mathrm{Var}(E) = C_v T^2$ and $d\\langle E\\rangle/dT = C_v$, together with $d\\langle E\\rangle/d\\beta = -\\mathrm{Var}(E)$, ensuring scientific realism within computational high-energy physics.\n\nTasks:\n1. From first principles, design an algorithm that, under small inter-replica temperature gaps, approximately maximizes the expected swap acceptance between adjacent replicas by appropriately spacing the inverse temperatures $\\beta_i = 1/T_i$ along the ladder. You must derive the spacing rule starting from the Metropolis-Hastings acceptance definition and the Gaussian action model, using a principled approximation that is valid for small increments.\n2. Implement a numerically stable function that computes the expected swap acceptance between two temperatures $T_i$ and $T_j$ under the Gaussian model, without simulating individual gauge configurations. The expectation must be computed analytically by integrating over the Gaussian distributions of $E_i \\sim \\mathcal{N}(\\mu(T_i),\\sigma^2(T_i))$ and $E_j \\sim \\mathcal{N}(\\mu(T_j),\\sigma^2(T_j))$.\n3. Using your derived spacing rule, construct the temperature ladder for each test case below. Ensure the temperatures are strictly increasing and lie within the prescribed bounds.\n4. For each test case, output the temperature ladder as a list of floating-point numbers rounded to six decimal places.\n\nTest suite:\n- Case A (general case): $m=6$, $T_1=0.5$, $T_m=2.0$, $C_v=20$, $E_0=0.0$.\n- Case B (different range and offset): $m=5$, $T_1=0.7$, $T_m=1.8$, $C_v=15$, $E_0=1.0$.\n- Case C (edge case with tight ladder at higher heat capacity): $m=4$, $T_1=0.9$, $T_m=1.2$, $C_v=50$, $E_0=0.0$.\n\nFinal output format:\n- Your program should produce a single line of output containing the ladders for all test cases as a comma-separated list enclosed in square brackets, where each element is itself a list of temperatures for one test case (e.g., $[[t_{1,1},t_{1,2},\\dots,t_{1,m}], [t_{2,1},\\dots,t_{2,m}], [t_{3,1},\\dots,t_{3,m}]]$).\n- All temperatures must be rounded to six decimal places.\n- Temperatures are dimensionless in this formulation, so no physical unit needs to be printed.", "solution": "The problem requires the design of an optimal temperature ladder for replica exchange Monte Carlo, also known as parallel tempering. The objective is to maximize the swap acceptance rate between adjacent replicas, given a fixed number of replicas $m$ and boundary temperatures $T_1$ and $T_m$. The system's action (energy) $S(U)$ at a given temperature $T$ is modeled as a Gaussian random variable $E$ with mean $\\mu(T) = E_0 + C_v T$ and variance $\\sigma^2(T) = C_v T^2$.\n\n### Principle-Based Derivation of the Temperature Spacing Rule\n\n**1. Maximizing Swap Acceptance**\n\nThe Metropolis-Hastings acceptance probability for a swap of configurations between replicas at temperatures $T_i$ and $T_{i+1}$ (with corresponding inverse temperatures $\\beta_i=1/T_i$ and $\\beta_{i+1}=1/T_{i+1}$) is given by:\n$$\n\\alpha_{i \\leftrightarrow i+1} = \\min\\left\\{1, \\exp\\left[(\\beta_i - \\beta_{i+1})(E_{i+1} - E_i)\\right]\\right\\}\n$$\nwhere $E_i$ and $E_{i+1}$ are the energies of the configurations in the respective replicas. To ensure an efficient random walk through temperature space, it is desirable to have a uniform (and high) acceptance probability $\\langle \\alpha_{i \\leftrightarrow i+1} \\rangle$ across all adjacent pairs $(i, i+1)$. A bottleneck with a low acceptance rate anywhere in the ladder would trap configurations and hinder sampling efficiency.\n\n**2. The Thermodynamic Distance Heuristic**\n\nDirectly solving the equation $\\langle \\alpha_{i \\leftrightarrow i+1} \\rangle = \\text{const}$ for all $i$ is analytically challenging. A widely used and effective heuristic is to instead require that the \"thermodynamic distance\" between adjacent temperatures is constant. This is based on the reasoning that for small temperature gaps, the acceptance probability is a monotonic function of the statistical overlap between the energy distributions of adjacent replicas. By keeping this overlap constant, we expect the acceptance rate to also be roughly constant.\n\n**3. Defining the Metric**\n\nA natural way to define the distance between two statistical systems at infinitesimally different inverse temperatures $\\beta$ and $\\beta+d\\beta$ is through an information-geometric metric. The metric element $ds$ can be chosen to be proportional to the change $d\\beta$ scaled by a factor that accounts for the typical energy fluctuations. A standard choice, which can be derived from the Fisher information metric for the canonical ensemble, is to relate the step size to the standard deviation of the energy, $\\sigma_E(T)$. We define the infinitesimal distance as:\n$$\nds \\propto d\\beta \\cdot \\sigma_E(T)\n$$\nUsing the provided model, $\\sigma_E(T) = \\sqrt{\\mathrm{Var}(E)} = \\sqrt{C_v T^2} = \\sqrt{C_v} T$. Substituting $T=1/\\beta$:\n$$\nds = k \\cdot d\\beta \\cdot \\frac{\\sqrt{C_v}}{\\beta}\n$$\nwhere $k$ is an arbitrary constant of proportionality. For the spacing to be optimal, we need to take equal steps in this new coordinate $s$.\n\n**4. Integration and Discretization**\n\nTo create $m-1$ equal intervals between $T_1$ (or $\\beta_1$) and $T_m$ (or $\\beta_m$), we first compute the total thermodynamic distance $S_{total}$ by integrating $ds$ over the entire range. Since $T_1  T_m$, it follows that $\\beta_1  \\beta_m$.\n$$\nS_{total} = \\int_{\\beta_m}^{\\beta_1} k \\frac{\\sqrt{C_v}}{\\beta} d\\beta = k\\sqrt{C_v} [\\ln \\beta]_{\\beta_m}^{\\beta_1} = k\\sqrt{C_v} (\\ln \\beta_1 - \\ln \\beta_m)\n$$\nWe divide this total distance into $m-1$ equal segments, each of length $\\Delta S = \\frac{S_{total}}{m-1}$. The condition for the temperatures in the ladder is that the distance between any adjacent pair $(\\beta_{i+1}, \\beta_i)$ is equal to $\\Delta S$:\n$$\n\\int_{\\beta_{i+1}}^{\\beta_i} k \\frac{\\sqrt{C_v}}{\\beta} d\\beta = \\Delta S\n$$\nThis yields:\n$$\nk\\sqrt{C_v} (\\ln \\beta_i - \\ln \\beta_{i+1}) = \\frac{k\\sqrt{C_v} (\\ln \\beta_1 - \\ln \\beta_m)}{m-1}\n$$\nSimplifying, we obtain the core spacing rule:\n$$\n\\ln \\beta_i - \\ln \\beta_{i+1} = \\frac{\\ln \\beta_1 - \\ln \\beta_m}{m-1} = \\text{const}\n$$\nThis demonstrates that the logarithms of the inverse temperatures, $\\ln \\beta_i$, must be linearly spaced.\n\n**5. The Geometric Progression Rule**\n\nThe linear spacing of $\\ln \\beta_i$ implies a geometric progression for the $\\beta_i$ values themselves. Let the constant difference be $C = \\ln\\beta_i - \\ln\\beta_{i+1}$. Then $\\ln\\beta_{i+1} = \\ln\\beta_i - C$, which means $\\beta_{i+1} = \\beta_i e^{-C}$. The inverse temperatures form a geometric series.\n\nThe $i$-th inverse temperature $\\beta_i$ (for $i=1, \\dots, m$) can be expressed as:\n$$\n\\beta_i = \\beta_1 \\cdot r^{i-1}\n$$\nwhere $r$ is the common ratio. We can find $r$ using the boundary condition at $i=m$:\n$$\n\\beta_m = \\beta_1 \\cdot r^{m-1} \\implies r = \\left(\\frac{\\beta_m}{\\beta_1}\\right)^{\\frac{1}{m-1}}\n$$\nSince $T_i = 1/\\beta_i$, the temperatures also form a geometric progression:\n$$\nT_i = \\frac{1}{\\beta_1 \\cdot r^{i-1}} = T_1 \\cdot \\left(\\frac{1}{r}\\right)^{i-1}\n$$\nLet the temperature ratio be $q = 1/r$. Substituting the expression for $r$:\n$$\nq = \\left(\\frac{\\beta_1}{\\beta_m}\\right)^{\\frac{1}{m-1}} = \\left(\\frac{T_m}{T_1}\\right)^{\\frac{1}{m-1}}\n$$\nThus, the temperature ladder is given by the geometric progression:\n$$\nT_i = T_1 \\cdot q^{i-1} = T_1 \\cdot \\left(\\frac{T_m}{T_1}\\right)^{\\frac{i-1}{m-1}} \\quad \\text{for } i = 1, 2, \\dots, m\n$$\nThis algorithm depends only on the boundary temperatures $T_1$, $T_m$, and the number of replicas $m$. The specific value of the constant heat capacity $C_v$ was essential for the derivation (specifically, that $C_v$ is constant), but it cancels out of the final formula for the relative spacing.\n\n### Analytical Formula for Expected Acceptance\n\nAs a secondary task, we derive the analytical expression for the expected swap acceptance probability $\\langle \\alpha_{a \\leftrightarrow b} \\rangle$ between two temperatures $T_a$ and $T_b$. Let $T_b  T_a$, so $\\beta_b  \\beta_a$. The swap exponent is $X = (\\beta_a - \\beta_b)(E_b - E_a)$. Since $E_a$ and $E_b$ are independent Gaussian variables, $E_a \\sim \\mathcal{N}(\\mu_a, \\sigma_a^2)$ and $E_b \\sim \\mathcal{N}(\\mu_b, \\sigma_b^2)$, their difference $E_b - E_a$ is also Gaussian. Thus, $X$ is a Gaussian random variable, $X \\sim \\mathcal{N}(\\mu_X, \\sigma_X^2)$, with parameters:\n$$\n\\mu_X = (\\beta_a - \\beta_b)(\\mu_b - \\mu_a) = C_v \\frac{(T_b - T_a)^2}{T_a T_b}\n$$\n$$\n\\sigma_X^2 = (\\beta_a - \\beta_b)^2 (\\sigma_a^2 + \\sigma_b^2) = C_v \\frac{(T_b - T_a)^2 (T_a^2 + T_b^2)}{(T_a T_b)^2}\n$$\nThe expected acceptance is the integral over the distribution $p(x)$ of $X$:\n$$\n\\langle \\alpha \\rangle = \\int_{-\\infty}^{\\infty} \\min(1, e^x) p(x) dx = \\int_{-\\infty}^{0} e^x p(x) dx + \\int_{0}^{\\infty} p(x) dx\n$$\nThe evaluation of these integrals yields the closed-form expression:\n$$\n\\langle \\alpha \\rangle = e^{\\mu_X + \\sigma_X^2/2} \\Phi\\left(-\\frac{\\mu_X + \\sigma_X^2}{\\sigma_X}\\right) + \\Phi\\left(\\frac{\\mu_X}{\\sigma_X}\\right)\n$$\nwhere $\\Phi$ is the cumulative distribution function (CDF) of the standard normal distribution. This formula can be implemented for verification but is not used in constructing the ladder itself.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\ndef calculate_expected_acceptance(Ta, Tb, Cv, E0):\n    \"\"\"\n    Computes the expected swap acceptance probability between two replicas\n    at temperatures Ta and Tb, based on the analytical formula for a\n    Gaussian energy model.\n\n    This function is implemented as per the problem description but is not\n    used for generating the temperature ladder itself.\n\n    Args:\n        Ta (float): Temperature of the first replica.\n        Tb (float): Temperature of the second replica.\n        Cv (float): Constant volume heat capacity.\n        E0 (float): Energy offset.\n\n    Returns:\n        float: The expected acceptance probability.\n    \"\"\"\n    if Ta == Tb:\n        return 1.0\n\n    # Ensure Ta  Tb for consistency\n    if Ta > Tb:\n        Ta, Tb = Tb, Ta\n\n    beta_a = 1.0 / Ta\n    beta_b = 1.0 / Tb\n\n    # Parameters for the energy distributions E_a and E_b\n    # mu_a = E0 + Cv * Ta\n    # mu_b = E0 + Cv * Tb\n    # sigma_sq_a = Cv * Ta**2\n    # sigma_sq_b = Cv * Tb**2\n    \n    # The random variable for the exponent is X = (beta_a - beta_b) * (E_b - E_a)\n    # X follows a normal distribution N(mu_X, sigma_sq_X)\n    \n    # Calculate mu_X\n    mu_b_minus_mu_a = Cv * (Tb - Ta)\n    beta_a_minus_beta_b = (1.0 / Ta) - (1.0 / Tb)\n    mu_X = beta_a_minus_beta_b * mu_b_minus_mu_a\n\n    # Calculate sigma_sq_X\n    sigma_sq_a_plus_sigma_sq_b = Cv * (Ta**2 + Tb**2)\n    sigma_sq_X = (beta_a_minus_beta_b**2) * sigma_sq_a_plus_sigma_sq_b\n\n    if sigma_sq_X  1e-12: # Avoid division by zero if temperatures are very close\n        return 1.0 if mu_X >=0 else np.exp(mu_X)\n\n    sigma_X = np.sqrt(sigma_sq_X)\n\n    # The formula for expected acceptance is:\n    # E[alpha] = exp(mu_X + sigma_sq_X/2) * Phi(-(mu_X + sigma_sq_X)/sigma_X) + Phi(mu_X/sigma_X)\n    # where Phi is the standard normal CDF.\n\n    # This can be numerically unstable if the arguments are large.\n    # We use log-space calculations for the first term.\n    arg1_cdf = -(mu_X + sigma_sq_X) / sigma_X\n    log_term1 = mu_X + sigma_sq_X / 2.0 + norm.logcdf(arg1_cdf)\n    term1 = np.exp(log_term1)\n    \n    arg2_cdf = mu_X / sigma_X\n    term2 = norm.cdf(arg2_cdf)\n\n    return term1 + term2\n\ndef construct_temperature_ladder(m, T1, Tm, Cv, E0):\n    \"\"\"\n    Constructs a temperature ladder with m replicas between T1 and Tm.\n\n    The ladder is a geometric progression in temperature, derived from the\n    principle of keeping the \"thermodynamic distance\" between adjacent\n    replicas constant for a system with constant heat capacity Cv.\n\n    Args:\n        m (int): The number of replicas.\n        T1 (float): The lowest temperature.\n        Tm (float): The highest temperature.\n        Cv (float): Constant volume heat capacity (used in derivation, not formula).\n        E0 (float): Energy offset (used in derivation, not formula).\n\n    Returns:\n        list[float]: A list of m temperatures forming the ladder.\n    \"\"\"\n    if m  2:\n        return [T1] if m == 1 else []\n        \n    # The temperatures {T_i} form a geometric progression T_i = T_1 * q^(i-1)\n    # The ratio q is determined by the boundary conditions T_1 and T_m.\n    # T_m = T_1 * q^(m-1) => q = (T_m / T_1)^(1 / (m-1))\n    \n    ratio = (Tm / T1)**(1.0 / (m - 1))\n    \n    temperatures = [T1 * (ratio**i) for i in range(m)]\n    \n    return temperatures\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all test cases and print the output.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A: m=6, T1=0.5, Tm=2.0, Cv=20, E0=0.0\n        {'m': 6, 'T1': 0.5, 'Tm': 2.0, 'Cv': 20, 'E0': 0.0},\n        # Case B: m=5, T1=0.7, Tm=1.8, Cv=15, E0=1.0\n        {'m': 5, 'T1': 0.7, 'Tm': 1.8, 'Cv': 15, 'E0': 1.0},\n        # Case C: m=4, T1=0.9, Tm=1.2, Cv=50, E0=0.0\n        {'m': 4, 'T1': 0.9, 'Tm': 1.2, 'Cv': 50, 'E0': 0.0},\n    ]\n\n    all_results = []\n    for case in test_cases:\n        ladder = construct_temperature_ladder(case['m'], case['T1'], case['Tm'], case['Cv'], case['E0'])\n        # Format results to six decimal places as required.\n        formatted_ladder = [f\"{temp:.6f}\" for temp in ladder]\n        all_results.append(formatted_ladder)\n\n    # Format the final output string to be exactly as specified:\n    # [[t1,t2,...],[...]] with no spaces.\n    inner_strings = []\n    for res_list in all_results:\n        inner_str = '[' + ','.join(res_list) + ']'\n        inner_strings.append(inner_str)\n    \n    final_output_string = '[' + ','.join(inner_strings) + ']'\n    \n    print(final_output_string)\n\nsolve()\n```", "id": "3521299"}]}