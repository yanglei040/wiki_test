{"hands_on_practices": [{"introduction": "To build a solid foundation, our first practice starts with the bedrock of frequentist statistics: the Neyman construction. This exercise guides you through the process of building an \"exact\" confidence interval for a Poisson signal in the presence of a known background. By numerically inverting the test for each possible signal value, you will not only construct the interval but also investigate the crucial concept of coverage, directly observing how the discrete nature of counting data leads to over-coverage, a key property of these intervals [@problem_id:3509404].", "problem": "Consider a counting experiment in computational high-energy physics where the observed event count $n$ is modeled as $n \\sim \\text{Poisson}(s+b)$ with signal mean $s \\ge 0$ and known background mean $b \\ge 0$. The goal is to construct an exact two-sided confidence interval for the parameter $s$ at Confidence Level (CL) $1-\\alpha$ by inverting the cumulative distribution function (CDF) of the Poisson distribution and then to quantify the frequentist coverage and over-coverage behavior at small counts.\n\nStart from the fundamental definitions: the model likelihood is based on the Poisson distribution for the observable $n$, and a Neyman confidence interval for $s$ is obtained by inverting a test of the parameter using the sampling distribution of $n$. Use an equal-tailed construction in the observable space, and invert it numerically to obtain the interval in parameter space.\n\nImplement the following tasks:\n1. For a given observed count $n$, background $b$, and significance level $\\alpha$, construct the exact two-sided interval for $s$ by numerically inverting the Poisson CDF of $n \\sim \\text{Poisson}(s+b)$ with equal tails of size $\\alpha/2$ in the observable $n$. Enforce the physical constraint $s \\ge 0$ by truncating the lower endpoint at $0$ if needed. Express the interval endpoints as real numbers without units.\n2. For a given true signal $s_{\\text{true}}$, compute the frequentist coverage of the interval procedure as the sum over all $n \\in \\{0,1,2,\\dots\\}$ of the indicator that $s_{\\text{true}}$ lies inside the interval built from $n$ times the probability mass function of $n \\sim \\text{Poisson}(s_{\\text{true}}+b)$. Quantify over-coverage as the difference between the coverage and the nominal confidence level $1-\\alpha$. Express this difference as a decimal number (not a percentage), rounded to a specified precision.\n3. Use robust numerical root finding for the inversion, justified by monotonicity of the Poisson CDF in the mean. For the coverage summation, truncate the infinite sum at a sufficiently large upper limit where the remaining tail probability is negligible, ensuring numerical accuracy.\n\nYour program must implement the above and run the following test suite of parameter sets:\n- Test case $1$: $b=0.5$, $\\alpha=0.10$, $n_{\\text{obs}}=0$, $s_{\\text{true}}=0.0$.\n- Test case $2$: $b=1.0$, $\\alpha=0.05$, $n_{\\text{obs}}=1$, $s_{\\text{true}}=0.2$.\n- Test case $3$: $b=2.3$, $\\alpha=0.05$, $n_{\\text{obs}}=3$, $s_{\\text{true}}=0.0$.\n- Test case $4$: $b=0.0$, $\\alpha=0.05$, $n_{\\text{obs}}=5$, $s_{\\text{true}}=1.5$.\n\nFor each test case, your program must output a list of three values $[s_{\\text{L}}, s_{\\text{U}}, \\Delta]$ where $s_{\\text{L}}$ is the lower interval endpoint for $s$, $s_{\\text{U}}$ is the upper interval endpoint for $s$, and $\\Delta$ is the over-coverage defined as coverage minus $1-\\alpha$. All three values must be rounded to six decimal places.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case result is itself a comma-separated list enclosed in square brackets. For example, output should look like $[[x_1,y_1,z_1],[x_2,y_2,z_2],[x_3,y_3,z_3],[x_4,y_4,z_4]]$ with each $x_i$, $y_i$, and $z_i$ a decimal number rounded to six places.", "solution": "The problem statement is assessed to be valid. It is scientifically grounded in the principles of frequentist statistics as applied to high-energy physics, specifically concerning the construction of confidence intervals for a Poisson process with background. The problem is well-posed, with all necessary parameters and definitions provided, and contains no internal contradictions or scientifically unsound premises. The tasks are computationally feasible and test a standard, albeit simplified, procedure for setting limits.\n\nThis solution will proceed by first detailing the theoretical framework for constructing the confidence interval and calculating its frequentist coverage, followed by an outline of the numerical implementation.\n\n### Theoretical Framework\n\n#### 1. Statistical Model\nThe experiment observes a count $n$, which is assumed to follow a Poisson distribution. The expected number of events, $\\mu$, is the sum of an unknown non-negative signal mean, $s$, and a known non-negative background mean, $b$. The likelihood of observing $n$ events is given by the Poisson probability mass function (PMF):\n$$\nP(n | s, b) = \\frac{e^{-(s+b)}(s+b)^n}{n!}\n$$\nThe parameter of interest is $s$, with the constraint $s \\ge 0$.\n\n#### 2. Confidence Interval Construction\nThe confidence interval for $s$ at a confidence level (CL) of $1-\\alpha$ is constructed by inverting a hypothesis test for each possible value of $s$. The problem specifies a method based on numerically inverting the cumulative distribution function (CDF) of the Poisson-distributed observable $n$. For a given observation $n_{\\text{obs}}$, the interval $[s_L, s_U]$ is defined by the set of all values of $s$ for which $n_{\\text{obs}}$ is not in the extreme tails of the distribution $P(n|s,b)$. The \"equal-tailed\" construction defines the endpoints by finding values of the mean $\\mu = s+b$ that place the observation $n_{\\text{obs}}$ at the edge of the lower and upper tails, each of size $\\alpha/2$.\n\n**Upper Endpoint $s_U$:**\nThe upper limit $s_U$ corresponds to a signal strength so large that observing a count as low as $n_{\\text{obs}}$ is improbable. We find the mean $\\mu_U = s_U + b$ for which the probability of observing $n_{\\text{obs}}$ or fewer events is exactly $\\alpha/2$:\n$$\nP(n \\le n_{\\text{obs}} | \\mu_U) = \\sum_{k=0}^{n_{\\text{obs}}} \\frac{e^{-\\mu_U}\\mu_U^k}{k!} = \\frac{\\alpha}{2}\n$$\nThis is the Poisson CDF. Because the Poisson CDF is a monotonically decreasing function of its mean $\\mu$, a unique solution for $\\mu_U$ exists for any $\\alpha \\in (0,1)$ and can be found using a numerical root-finding algorithm. Once $\\mu_U$ is found, the upper endpoint for the signal is $s_U = \\mu_U - b$.\n\n**Lower Endpoint $s_L$:**\nThe lower limit $s_L$ corresponds to a signal strength so small that observing a count as high as $n_{\\text{obs}}$ is improbable. We find the mean $\\mu_L = s_L + b$ for which the probability of observing $n_{\\text{obs}}$ or more events is exactly $\\alpha/2$:\n$$\nP(n \\ge n_{\\text{obs}} | \\mu_L) = \\sum_{k=n_{\\text{obs}}}^{\\infty} \\frac{e^{-\\mu_L}\\mu_L^k}{k!} = \\frac{\\alpha}{2}\n$$\nThis can be rewritten using the CDF as $1 - P(n \\le n_{\\text{obs}}-1 | \\mu_L) = \\alpha/2$, or:\n$$\nP(n \\le n_{\\text{obs}}-1 | \\mu_L) = 1 - \\frac{\\alpha}{2}\n$$\nThis equation is solved for $\\mu_L$ to find $s_L = \\mu_L - b$.\n\nA special case arises when $n_{\\text{obs}} = 0$. The condition for the lower limit becomes $P(n \\ge 0 | \\mu_L) = 1 = \\alpha/2$, which is impossible. In this situation, the data provides no evidence against arbitrarily small values of the mean, so we take $\\mu_L = 0$. This yields a raw lower limit of $s_L = -b$.\n\n**Physical Constraint:**\nThe signal parameter $s$ must be non-negative. The problem mandates enforcing this by truncating the lower endpoint at $0$. Therefore, the final lower limit is given by $s_L = \\max(0, \\mu_L - b)$. The upper limit $s_U$ is not explicitly truncated, allowing for the possibility of an empty interval if $s_U < s_L$.\n\n#### 3. Frequentist Coverage\nThe frequentist coverage of a confidence interval procedure is the long-run probability that the constructed interval contains the true value of the parameter, $s_{\\text{true}}$. For a discrete observable like $n$, the coverage is calculated by summing the probabilities of all outcomes $n$ that lead to an interval $[s_L(n), s_U(n)]$ containing $s_{\\text{true}}$:\n$$\n\\text{Coverage}(s_{\\text{true}}) = \\sum_{n=0}^{\\infty} I(s_L(n) \\le s_{\\text{true}} \\le s_U(n)) \\cdot P(n | s_{\\text{true}}, b)\n$$\nwhere $I(\\cdot)$ is the indicator function, which is $1$ if its argument is true and $0$ otherwise. The probability $P(n|s_{\\text{true}}, b)$ is calculated using the Poisson PMF with mean $\\mu_{\\text{true}} = s_{\\text{true}} + b$.\n\nDue to the discrete nature of the Poisson distribution, the actual coverage of an \"exact\" interval procedure is not guaranteed to be exactly $1-\\alpha$. It is guaranteed to be at least $1-\\alpha$ (i.e., it over-covers). The amount of over-coverage is quantified by:\n$$\n\\Delta = \\text{Coverage}(s_{\\text{true}}) - (1-\\alpha)\n$$\n\n### Numerical Implementation Strategy\nThe solution is implemented in Python using the `numpy` and `scipy` libraries.\n\n1.  **Interval Calculation (`get_interval`)**:\n    -   A function is created to compute $[s_L, s_U]$ for a given $n_{\\text{obs}}$, $b$, and $\\alpha$.\n    -   The equations for $\\mu_L$ and $\\mu_U$ are solved using the `scipy.optimize.brentq` root-finding algorithm. This choice is justified because the Poisson CDF is monotonic with respect to its mean parameter, guaranteeing a unique root within a bounded interval. The target functions for `brentq` are lambda functions wrapping `scipy.stats.poisson.cdf`.\n    -   The special case for $n_{\\text{obs}}=0$ is handled explicitly for the lower bound calculation.\n    -   The physical constraint $s_L \\ge 0$ is applied as the final step for the lower bound.\n\n2.  **Coverage Calculation (`calculate_coverage`)**:\n    -   A function computes the coverage for a given $s_{\\text{true}}$, $b$, and $\\alpha$.\n    -   The infinite sum is truncated at a sufficiently large value $N_{\\max}$, determined such that the remaining tail probability $P(n > N_{\\max} | \\mu_{\\text{true}})$ is negligible (e.g., less than $10^{-16}$). $N_{\\max}$ is found using the percent point function (`ppf`) of the Poisson distribution.\n    -   The function iterates from $n=0$ to $N_{\\max}$. In each iteration, it calls `get_interval` to find the interval for that $n$, checks if $s_{\\text{true}}$ is covered, and if so, adds the corresponding probability `scipy.stats.poisson.pmf(n, mu_true)` to the total coverage.\n\n3.  **Main Routine (`solve`)**:\n    -   The main function iterates through the provided test cases.\n    -   For each case, it calls the functions for interval calculation (using $n_{\\text{obs}}$) and coverage calculation (using $s_{\\text{true}}$).\n    -   It computes the over-coverage $\\Delta$.\n    -   The results $[s_L, s_U, \\Delta]$ are formatted to six decimal places and collected.\n    -   The final output is printed as a single line in the specified list-of-lists format.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import poisson\nfrom scipy.optimize import brentq\n\ndef get_interval(n_obs: int, b: float, alpha: float) -> list[float]:\n    \"\"\"\n    Constructs the exact two-sided confidence interval for the signal s.\n    \n    Args:\n        n_obs: The observed event count.\n        b: The known background mean.\n        alpha: The significance level (e.g., 0.10 for 90% CL).\n        \n    Returns:\n        A list [s_L, s_U] with the lower and upper interval endpoints.\n    \"\"\"\n    alpha_2 = alpha / 2.0\n\n    # Upper Endpoint s_U from mu_U\n    # Solve P(n <= n_obs | mu_U) = alpha/2, which is poisson.cdf(n_obs, mu_U) = alpha/2.\n    # The function f(mu) = poisson.cdf(n_obs, mu) is monotonic, decreasing from ~1 to 0.\n    # A root must exist for alpha_2 in (0, 1).\n    # We need a search interval [a, c] for brentq where f(a)*f(c) < 0.\n    # f(1e-9) is > 0. We need a c where f(c) < 0.\n    # A generous upper search bound is robust.\n    upper_search_bound = n_obs + b + 500\n    try:\n        mu_U = brentq(lambda mu: poisson.cdf(n_obs, mu) - alpha_2, 1e-9, upper_search_bound)\n    except ValueError:\n        # Should not be reached with a sufficiently large search bound.\n        mu_U = np.inf\n    s_U = mu_U - b\n\n    # Lower Endpoint s_L from mu_L\n    # Solve P(n >= n_obs | mu_L) = alpha/2 => P(n <= n_obs-1 | mu_L) = 1 - alpha/2.\n    if n_obs == 0:\n        # P(n >= 0 | mu_L) = 1. The equation 1 = alpha/2 has no solution for mu_L.\n        # The limit is mu_L -> 0.\n        mu_L = 0.0\n    else:\n        # The function g(mu) = poisson.cdf(n_obs-1, mu) - (1-alpha/2) is monotonic.\n        # g(1e-9) is > 0. A large enough upper bound ensures g(c) < 0.\n        try:\n            mu_L = brentq(lambda mu: poisson.cdf(n_obs - 1, mu) - (1.0 - alpha_2), 1e-9, upper_search_bound)\n        except ValueError:\n            # Should not be reached.\n            mu_L = 0.0\n            \n    s_L_raw = mu_L - b\n\n    # Enforce physical constraint s >= 0 by truncating the lower endpoint.\n    s_L = max(0.0, s_L_raw)\n    \n    return [s_L, s_U]\n\ndef calculate_coverage(s_true: float, b: float, alpha: float) -> float:\n    \"\"\"\n    Computes the frequentist coverage for a given true signal s_true.\n    \n    Args:\n        s_true: The true value of the signal parameter.\n        b: The known background mean.\n        alpha: The significance level.\n        \n    Returns:\n        The coverage probability.\n    \"\"\"\n    mu_true = s_true + b\n    \n    # The sum is over all n, truncated at n_max where the tail is negligible.\n    if mu_true > 1e-9:\n        n_max = int(poisson.ppf(1.0 - 1e-16, mu=mu_true)) + 10\n    else:\n        # If mu_true is 0, only n=0 has non-zero probability.\n        n_max = 10\n\n    total_coverage = 0.0\n    for n_iter in range(n_max + 1):\n        s_L, s_U = get_interval(n_iter, b, alpha)\n        \n        # Check if s_true is inside the calculated interval [s_L, s_U].\n        if s_L <= s_true <= s_U:\n            total_coverage += poisson.pmf(n_iter, mu=mu_true)\n            \n    return total_coverage\n\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final result.\n    \"\"\"\n    test_cases = [\n        # (b, alpha, n_obs, s_true)\n        (0.5, 0.10, 0, 0.0),\n        (1.0, 0.05, 1, 0.2),\n        (2.3, 0.05, 3, 0.0),\n        (0.0, 0.05, 5, 1.5),\n    ]\n\n    all_results = []\n    for b, alpha, n_obs, s_true in test_cases:\n        # Task 1: Construct the confidence interval for the given n_obs.\n        s_L, s_U = get_interval(n_obs, b, alpha)\n        \n        # Task 2: Compute the frequentist coverage and over-coverage for s_true.\n        coverage = calculate_coverage(s_true, b, alpha)\n        over_coverage = coverage - (1.0 - alpha)\n        \n        # Store results rounded to six decimal places.\n        all_results.append([s_L, s_U, over_coverage])\n\n    # Format the final output string exactly as specified.\n    results_str_parts = []\n    for res in all_results:\n        # Use a formatter to ensure 6 decimal places, including trailing zeros.\n        formatted_res = ','.join(f'{v:.6f}' for v in res)\n        results_str_parts.append(f'[{formatted_res}]')\n        \n    final_output = f\"[{','.join(results_str_parts)}]\"\n    print(final_output)\n\nsolve()\n\n```", "id": "3509404"}, {"introduction": "Building on the foundational Neyman construction, this exercise introduces the powerful Feldman-Cousins method, a technique developed to address challenges that arise near physical boundaries. You will implement the likelihood-ratio ordering principle, which provides a statistically motivated way to define the acceptance region for each possible signal strength, ensuring proper coverage and avoiding unphysical empty intervals. This practice is essential for understanding how to robustly report results for signals that may be small or non-existent [@problem_id:3509483].", "problem": "Consider a counting experiment producing an observed non-negative integer count $n$ modeled as a Poisson random variable with mean $\\mu = s + b$, where $s \\ge 0$ is the unknown non-negative signal rate and $b$ is a known non-negative background rate. You are to construct confidence intervals using the Likelihood Ratio Ordering procedure commonly known as the Feldman–Cousins method. The method requires: (i) the Poisson probability mass function $P(n \\mid \\mu)$, (ii) the maximum likelihood estimate of the signal $s$ under the constraint $s \\ge 0$, and (iii) an acceptance ordering based on the likelihood ratio for each fixed $s$. Use a Confidence Level (CL) of $0.9$ (expressed as the decimal number $0.9$). The background is known and fixed at $b = 0.5$.\n\nStarting from the fundamental definitions and without using any pre-solved shortcut formulas, implement the following:\n\n- For each fixed $s \\ge 0$, define an ordering over all possible counts $n \\in \\{0,1,2,\\dots\\}$ by the likelihood ratio $R(n \\mid s) = \\dfrac{P(n \\mid s+b)}{P(n \\mid s_{\\text{best}}(n)+b)}$, where $s_{\\text{best}}(n)$ is the value of $s$ that maximizes the likelihood $P(n \\mid s+b)$ subject to $s \\ge 0$. Include $n$ values into the acceptance region in order of descending $R(n \\mid s)$ until the cumulative probability $\\sum_{n \\in \\mathcal{A}(s)} P(n \\mid s+b)$ first reaches or exceeds the Confidence Level $0.9$. This yields an acceptance region $\\mathcal{A}(s)$ for that $s$.\n- The Feldman–Cousins confidence belt is obtained by inverting this construction: for a given observed count $n_{\\text{obs}}$, the $0.9$ confidence interval for $s$ is the set of all $s \\ge 0$ such that $n_{\\text{obs}} \\in \\mathcal{A}(s)$.\n\nYour task is to implement this belt construction computationally and to report the $0.9$ confidence intervals for the three observed counts: $n = 0$, $n = 3$, and $n = 7$, using the known background $b = 0.5$.\n\nRequirements and constraints:\n\n- Use only the Poisson probability mass function and maximum likelihood under the constraint $s \\ge 0$ as foundational starting points. Avoid introducing ad hoc approximations beyond systematic numerical discretization of $s$ and truncation of the count range to ensure numerical completeness of the probability mass.\n- Ensure scientific realism by choosing a sufficiently large upper bound for $n$ and a sufficiently wide and fine grid of $s$ to capture the acceptance regions with accuracy. Then refine interval endpoints by a robust one-dimensional search to achieve numerical precision.\n- There are no physical units involved; all values are dimensionless counts or rates. The Confidence Level must be handled as the decimal number $0.9$.\n- Design a test suite comprising the following observed counts:\n    - $n = 0$ (boundary case with very low statistics),\n    - $n = 3$ (moderate statistics), and\n    - $n = 7$ (higher statistics).\n  This suite probes different facets: near-boundary behavior, typical mid-range behavior, and larger-count behavior.\n- Output specification: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is the two-element list $[s_{\\text{low}}, s_{\\text{high}}]$ for the corresponding observed $n$, in the order $n = 0$, $n = 3$, $n = 7$. Each numerical endpoint must be rounded to $4$ decimal places. For example, the output format must look like $[[s_{0,\\text{low}},s_{0,\\text{high}}],[s_{3,\\text{low}},s_{3,\\text{high}}],[s_{7,\\text{low}},s_{7,\\text{high}}]]$.\n\nYour implementation must be a complete, runnable program that computes these intervals from first principles under the constraints stated above and prints the single-line output in the exact format specified.", "solution": "The problem requires the construction of confidence intervals for an unknown signal rate $s$ using the Feldman-Cousins method. This is a standard procedure in high-energy physics for a counting experiment where the observed count $n$ follows a Poisson distribution with mean $\\mu = s+b$, given a known background rate $b$.\n\n### 1. Theoretical Framework\n\nThe foundation of the method rests on the Poisson probability mass function (PMF), the principle of maximum likelihood, and a specific likelihood-ratio ordering to construct acceptance regions.\n\n**1.1. The Statistical Model**\nThe probability of observing $n$ events is given by the Poisson PMF:\n$$ P(n \\mid \\mu) = \\frac{\\mu^n e^{-\\mu}}{n!} $$\nwhere the mean $\\mu$ is the sum of the unknown signal rate $s$ and the known background rate $b$, so $\\mu(s) = s+b$. The signal rate is physically constrained to be non-negative, $s \\ge 0$.\n\n**1.2. Maximum Likelihood Estimation**\nFor a given observation $n$, the likelihood function is $L(s) = P(n \\mid s+b)$. To find the value of $s$ that best explains the data, we maximize this likelihood. It is equivalent and simpler to maximize the log-likelihood:\n$$ \\ln L(s) = n \\ln(s+b) - (s+b) - \\ln(n!) $$\nDifferentiating with respect to $s$ and setting the result to zero yields:\n$$ \\frac{d}{ds} \\ln L(s) = \\frac{n}{s+b} - 1 = 0 \\implies \\hat{s} = n - b $$\nThis is the unconstrained maximum likelihood estimate (MLE). However, the signal rate $s$ must be non-negative. Therefore, the physically constrained MLE, which we denote $s_{\\text{best}}(n)$, is:\n$$ s_{\\text{best}}(n) = \\max(0, n-b) $$\nThe corresponding best-fit mean parameter is $\\mu_{\\text{best}}(n) = s_{\\text{best}}(n) + b = \\max(n, b)$.\n\n**1.3. Likelihood Ratio Ordering**\nThe Feldman-Cousins method ranks possible observations $n$ for a given hypothesized true signal $s$ using the likelihood ratio $R(n \\mid s)$:\n$$ R(n \\mid s) = \\frac{P(n \\mid s+b)}{P(n \\mid s_{\\text{best}}(n)+b)} = \\frac{P(n \\mid \\mu(s))}{P(n \\mid \\mu_{\\text{best}}(n))} $$\nThis ratio compares the likelihood of observing $n$ given the true signal $s$ to the maximum possible likelihood for that $n$ under any valid physical hypothesis. A higher value of $R$ indicates that the observation $n$ is more consistent with the hypothesized $s$.\n\n### 2. Construction of Acceptance and Confidence Regions\n\n**2.1. Acceptance Regions**\nFor each fixed value of the true signal $s$, an acceptance region $\\mathcal{A}(s)$ is constructed. This region is a set of observable counts $n$ that would be considered consistent with that $s$. The construction proceeds as follows:\n1.  For the given $s$, calculate the rank $R(n \\mid s)$ for all possible integer counts $n=0, 1, 2, \\dots$.\n2.  Add counts $n$ to the acceptance region $\\mathcal{A}(s)$ in descending order of their rank $R(n \\mid s)$.\n3.  Continue adding counts until the cumulative probability first meets or exceeds the specified confidence level (CL).\n$$ \\sum_{n \\in \\mathcal{A}(s)} P(n \\mid s+b) \\ge \\text{CL} $$\nThe problem specifies a CL of $0.9$ and a background $b=0.5$.\n\n**2.2. Confidence Intervals via Inversion**\nThe confidence interval for $s$, given an actual observation $n_{\\text{obs}}$, is found by \"inverting\" the collection of acceptance regions. The interval consists of all values of $s$ for which the observed count $n_{\\text{obs}}$ falls within the corresponding acceptance region:\n$$ [s_{\\text{low}}, s_{\\text{high}}] = \\{ s \\ge 0 \\mid n_{\\text{obs}} \\in \\mathcal{A}(s) \\} $$\n\n### 3. Computational Strategy\n\nThe interval boundaries $[s_{\\text{low}}, s_{\\text{high}}]$ are the specific values of $s$ where $n_{\\text{obs}}$ is precisely on the threshold of being included in or excluded from the acceptance region $\\mathcal{A}(s)$. This occurs when the sum of probabilities of all outcomes $n'$ that have a strictly higher rank than $n_{\\text{obs}}$ equals the confidence level. This condition defines the boundaries of the interval. We can define a function whose roots correspond to these boundaries:\n$$ f(s; n_{\\text{obs}}) = \\left( \\sum_{n' \\text{ s.t. } R(n'|s) > R(n_{\\text{obs}}|s)} P(n'|s+b) \\right) - \\text{CL} = 0 $$\nThe solution involves implementing a numerical procedure to find the roots of this equation for each given $n_{\\text{obs}} \\in \\{0, 3, 7\\}$.\n\n-   **Numerical Evaluation:** The sum is computed over a sufficiently large range of integer counts (e.g., $n' \\in [0, 100]$) to ensure the truncated probabilities are negligible. For numerical stability, the likelihood ratio $R$ is computed using log-likelihoods:\n    $$ \\ln R(n \\mid s) = n \\left(\\ln(s+b) - \\ln(\\max(n,b))\\right) - (s- \\max(0, n-b)) $$\n-   **Root Finding:** A robust one-dimensional root-finding algorithm, such as Brent's method, is employed to solve $f(s; n_{\\text{obs}}) = 0$ for $s$.\n    -   For $n_{\\text{obs}} > b$, the function $f(s)$ is typically positive for very small and very large $s$, and negative in between. It thus has two roots, which are $s_{\\text{low}}$ and $s_{\\text{high}}$. The search for these roots is split around the point $s \\approx n_{\\text{obs}} - b$, where $f(s)$ is minimal.\n    -   For the special case $n_{\\text{obs}}=0$, it can be shown that $s_{\\text{low}}=0$. The function $f(s; 0)$ is monotonic for $s \\ge 0$ and has a single root, which corresponds to $s_{\\text{high}}$.\n-   **Implementation:** The algorithm is implemented in Python using the `numpy` library for numerical operations and `scipy` for its Poisson PMF implementation and the `brentq` root-finding function. The final interval endpoints are rounded to four decimal places as required.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import poisson\nfrom scipy.optimize import brentq\nimport sys\n\n# Suppress potential RuntimeWarning from log(0) in edge cases, which are handled.\nnp.seterr(divide='ignore', invalid='ignore')\n\ndef solve():\n    \"\"\"\n    Computes Feldman-Cousins confidence intervals for a Poisson signal\n    with known background.\n    \"\"\"\n    \n    # Problem definition\n    B = 0.5  # Known background rate\n    CL = 0.9 # Confidence Level\n    OBSERVED_COUNTS = [0, 3, 7] # Test cases for n_obs\n\n    # Numerical parameters\n    N_MAX = 100 # Range of n to sum over, must be large enough\n    \n    def get_boundary_function(n_obs: int, b: float, cl: float, n_max: int):\n        \"\"\"\n        Returns a function f(s) whose roots are the confidence interval boundaries.\n        f(s) = (sum of P(n') for n' with rank > rank of n_obs) - CL\n        \"\"\"\n\n        def boundary_func(s: float) -> float:\n            \"\"\"\n            The function to find roots of.\n            s: The hypothesized signal strength.\n            \"\"\"\n            # Physical constraint s >= 0. Root finder is bounded to this.\n            if s < 0:\n                return 1.0  # Return a large positive value to constrain search\n            \n            mu_true = s + b\n\n            # Calculate log of the rank for the observed count n_obs\n            # ln(R) = n*ln(mu_true/mu_best) - (mu_true-mu_best)\n            mu_best_obs = max(n_obs, b)\n            \n            # Use logs for numerical stability. Handle s=0 where mu_true = b.\n            if np.isclose(mu_true, mu_best_obs):\n                log_R_obs = 0.0\n            else:\n                log_R_obs = n_obs * (np.log(mu_true) - np.log(mu_best_obs)) - (mu_true - mu_best_obs)\n\n            prob_sum_greater = 0.0\n            \n            # Sum probabilities for all n' with a higher rank than n_obs\n            for n_prime in range(n_max + 1):\n                if n_prime == n_obs:\n                    continue\n\n                mu_best_n_prime = max(n_prime, b)\n                \n                if np.isclose(mu_true, mu_best_n_prime):\n                    log_R_n_prime = 0.0\n                else:\n                    log_R_n_prime = n_prime * (np.log(mu_true) - np.log(mu_best_n_prime)) - (mu_true - mu_best_n_prime)\n\n                # Add probability if rank is strictly greater.\n                # A small tolerance helps with floating point comparisons.\n                if log_R_n_prime > log_R_obs + 1e-12:\n                    prob_sum_greater += poisson.pmf(n_prime, mu_true)\n\n            return prob_sum_greater - cl\n\n        return boundary_func\n\n    results = []\n    for n_obs in OBSERVED_COUNTS:\n        boundary_func = get_boundary_function(n_obs, B, CL, N_MAX)\n        s_low, s_high = 0.0, 0.0\n\n        if n_obs == 0:\n            # For n_obs=0, the lower bound is always s_low=0.\n            # The boundary function is monotonic, so there's one root for s_high.\n            s_low = 0.0\n            try:\n                s_high = brentq(boundary_func, 0, 15)\n            except ValueError:\n                # Should not happen for this well-posed problem\n                s_high = np.nan\n        else: # For n_obs > 0\n            # The boundary function typically has two roots (for s_low and s_high)\n            # which we find by splitting the search domain around the minimum.\n            # The minimum occurs approximately where s_best = n_obs - b.\n            s_peak_approx = max(1e-5, n_obs - B)\n\n            # --- Find s_low ---\n            # Check if s=0 is included in the interval. If f(0)<0, it is.\n            if boundary_func(1e-9) < 0:\n                s_low = 0.0\n            else:\n                # Search for the root between 0 and the peak approximation\n                try:\n                    s_low = brentq(boundary_func, 1e-9, s_peak_approx)\n                except ValueError:\n                    s_low = np.nan\n            \n            # --- Find s_high ---\n            # Search for the root from the peak approximation upwards\n            try:\n                # A sufficiently large upper bracket for the search\n                s_high = brentq(boundary_func, s_peak_approx, n_obs + 40)\n            except ValueError:\n                s_high = np.nan\n\n        results.append([round(s_low, 4), round(s_high, 4)])\n\n    # Format the output exactly as specified\n    output_str = \"[\"\n    for i, res in enumerate(results):\n        output_str += f\"[{res[0]:.4f},{res[1]:.4f}]\"\n        if i < len(results) - 1:\n            output_str += \",\"\n    output_str += \"]\"\n        \n    print(output_str)\n\nsolve()\n```", "id": "3509483"}, {"introduction": "This practice moves us from simplified single-bin models to the more realistic scenario of a multi-bin analysis with systematic uncertainties. You will construct a complete likelihood function that includes nuisance parameters, constrained by Gaussian terms, to model these uncertainties. The core of this exercise is to implement the profile likelihood ratio and compute the $q_{\\mu}$ test statistic, which is the workhorse for setting limits in modern high-energy physics analyses and a critical skill for handling complex, multi-parameter models [@problem_id:3509408].", "problem": "Consider a counting experiment partitioned into multiple analysis bins, in which the observed counts are modeled as independent Poisson random variables. Let there be $N_{\\mathrm{bins}}$ bins indexed by $i \\in \\{1,\\dots,N_{\\mathrm{bins}}\\}$. For each bin $i$, let $n_i$ denote the observed count, $s_i$ denote the nominal signal template yield for unit signal strength, and $b_i$ denote the nominal background template yield. The parameter of interest is the nonnegative signal strength $\\mu \\ge 0$. Two systematic effects are included via Gaussian-constrained nuisance parameters: a global signal efficiency nuisance $\\theta_s$ and a global background normalization nuisance $\\theta_b$, each constrained by standard normal distributions. To ensure positivity and numerical stability in the presence of multiplicative systematics, adopt a log-normal morphing with fractional scales $k_s$ for signal and $k_b$ for background. Specifically, define the expected yield in bin $i$ as\n$$\n\\lambda_i(\\mu,\\theta_s,\\theta_b) = \\mu\\, s_i \\,\\exp(k_s \\theta_s) + b_i\\, \\exp(k_b \\theta_b).\n$$\nAssuming independent Poisson counting and independent Gaussian constraints on the nuisances, the full likelihood (up to bin-independent Poisson constants that cancel in likelihood ratios) is\n$$\nL(\\mu,\\theta_s,\\theta_b) \\propto \\left[\\prod_{i=1}^{N_{\\mathrm{bins}}} \\mathrm{Pois}\\big(n_i \\mid \\lambda_i(\\mu,\\theta_s,\\theta_b)\\big)\\right] \\times \\exp\\!\\left(-\\frac{\\theta_s^2}{2}\\right) \\times \\exp\\!\\left(-\\frac{\\theta_b^2}{2}\\right),\n$$\nand its log-likelihood (dropping additive constants independent of $(\\mu,\\theta_s,\\theta_b)$) is\n$$\n\\ell(\\mu,\\theta_s,\\theta_b) = \\sum_{i=1}^{N_{\\mathrm{bins}}} \\big[n_i \\ln \\lambda_i(\\mu,\\theta_s,\\theta_b) - \\lambda_i(\\mu,\\theta_s,\\theta_b)\\big] - \\frac{1}{2}\\big(\\theta_s^2 + \\theta_b^2\\big).\n$$\nDefine the profile likelihood ratio\n$$\n\\lambda(\\mu) = \\frac{\\max_{\\theta_s,\\theta_b}\\, L(\\mu,\\theta_s,\\theta_b)}{\\max_{\\mu \\ge 0,\\ \\theta_s,\\theta_b} \\, L(\\mu,\\theta_s,\\theta_b)},\n$$\nand the one-sided test statistic for upper limits\n$$\nq_\\mu = \n\\begin{cases}\n-2 \\ln \\lambda(\\mu), & \\hat{\\mu} \\le \\mu,\\\\\n0, & \\hat{\\mu} > \\mu,\n\\end{cases}\n$$\nwhere $(\\hat{\\mu},\\hat{\\theta}_s,\\hat{\\theta}_b)$ are the unconditional maximum likelihood estimators that maximize $L(\\mu,\\theta_s,\\theta_b)$ under the constraint $\\mu \\ge 0$, and $(\\hat{\\theta}_s(\\mu),\\hat{\\theta}_b(\\mu))$ are the profiled nuisance estimators at fixed $\\mu$ that maximize $L(\\mu,\\theta_s,\\theta_b)$ with respect to $(\\theta_s,\\theta_b)$.\n\nYour task is to implement a complete numerical computation of $q_\\mu$ for several specified pseudo-datasets, based on the above model. The computation must:\n- Construct $\\ell(\\mu,\\theta_s,\\theta_b)$ using the given $n_i$, $s_i$, $b_i$, $k_s$, and $k_b$.\n- Perform a constrained numerical optimization to obtain $(\\hat{\\mu},\\hat{\\theta}_s,\\hat{\\theta}_b)$ and $(\\hat{\\theta}_s(\\mu),\\hat{\\theta}_b(\\mu))$.\n- Evaluate $q_\\mu$ according to the one-sided definition above.\n- Be robust with respect to numerical stability and optimization initialization, and include reasonable bounds to prevent overflow while preserving physical realism.\n\nUse the following fractional scales for the log-normal morphing: $k_s = \\ln(1 + 0.10)$ and $k_b = \\ln(1 + 0.20)$, representing $10$-percent and $20$-percent one-sigma fractional uncertainties, respectively.\n\nImplement your program to process the following test suite, each case consisting of $(\\{s_i\\},\\{b_i\\},\\{n_i\\},\\mu)$:\n\n- Case A (general multi-bin fit, moderate counts):\n  - $N_{\\mathrm{bins}} = 4$\n  - $s = [\\,5.0,\\,9.0,\\,4.0,\\,2.0\\,]$\n  - $b = [\\,20.0,\\,15.0,\\,10.0,\\,5.0\\,]$\n  - $n = [\\,27,\\,25,\\,15,\\,8\\,]$\n  - $\\mu = 1.0$\n\n- Case B (boundary behavior where the global maximum may favor $\\hat{\\mu} = 0$):\n  - $N_{\\mathrm{bins}} = 4$\n  - $s = [\\,8.0,\\,12.0,\\,5.0,\\,1.0\\,]$\n  - $b = [\\,18.0,\\,14.0,\\,11.0,\\,6.0\\,]$\n  - $n = [\\,13,\\,10,\\,7,\\,4\\,]$\n  - $\\mu = 1.0$\n\n- Case C (low counts and zeros, testing numerical stability):\n  - $N_{\\mathrm{bins}} = 4$\n  - $s = [\\,0.5,\\,1.0,\\,0.2,\\,0.1\\,]$\n  - $b = [\\,0.2,\\,0.5,\\,0.2,\\,0.05\\,]$\n  - $n = [\\,0,\\,1,\\,0,\\,2\\,]$\n  - $\\mu = 0.5$\n\nYour program should compute $q_\\mu$ for each case and produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $\"[q_{\\mu,A},q_{\\mu,B},q_{\\mu,C}]\"$). All outputs must be floating-point numbers. No physical units are involved and all angles, if any appear, must be interpreted as dimensionless real numbers. The program must be completely self-contained, require no user input, and adhere to the specified execution environment.", "solution": "The problem statement has been validated and is deemed a well-posed, scientifically grounded task in computational statistics as applied to high-energy physics. It is self-contained, objective, and free of contradictions or ambiguities.\n\nThe objective is to compute the one-sided test statistic $q_\\mu$ for a given signal strength hypothesis $\\mu$. The test statistic is defined in terms of the profile likelihood ratio $\\lambda(\\mu)$:\n$$\nq_\\mu = \n\\begin{cases}\n-2 \\ln \\lambda(\\mu), & \\hat{\\mu} \\le \\mu,\\\\\n0, & \\hat{\\mu} > \\mu.\n\\end{cases}\n$$\nThe profile likelihood ratio is given by\n$$\n\\lambda(\\mu) = \\frac{\\max_{\\theta_s,\\theta_b}\\, L(\\mu,\\theta_s,\\theta_b)}{\\max_{\\mu' \\ge 0,\\ \\theta_s,\\theta_b} \\, L(\\mu',\\theta_s,\\theta_b)}.\n$$\nHere, $L(\\mu,\\theta_s,\\theta_b)$ is the likelihood function, and $(\\hat{\\mu}, \\hat{\\theta}_s, \\hat{\\theta}_b)$ are the maximum likelihood estimators (MLEs) that globally maximize $L$ under the constraint $\\mu \\ge 0$.\n\nSubstituting the definition of the log-likelihood, $\\ell = \\ln L$, the test statistic can be written as:\n$$\nq_\\mu = -2 \\left[ \\ell(\\mu, \\hat{\\theta}_s(\\mu), \\hat{\\theta}_b(\\mu)) - \\ell(\\hat{\\mu}, \\hat{\\theta}_s, \\hat{\\theta}_b) \\right], \\quad \\text{for } \\hat{\\mu} \\le \\mu,\n$$\nwhere $(\\hat{\\theta}_s(\\mu), \\hat{\\theta}_b(\\mu))$ are the estimators for the nuisance parameters $(\\theta_s, \\theta_b)$ that maximize the likelihood for a fixed value of $\\mu$.\n\nFor numerical computation, it is standard practice to perform minimization of the negative log-likelihood (NLL), denoted $f = -\\ell$. The NLL function is:\n$$\nf(\\mu, \\theta_s, \\theta_b) = -\\ell(\\mu, \\theta_s, \\theta_b) = \\sum_{i=1}^{N_{\\mathrm{bins}}} \\left[ \\lambda_i(\\mu,\\theta_s,\\theta_b) - n_i \\ln \\lambda_i(\\mu,\\theta_s,\\theta_b) \\right] + \\frac{1}{2}(\\theta_s^2 + \\theta_b^2),\n$$\nwhere $\\lambda_i(\\mu,\\theta_s,\\theta_b) = \\mu\\, s_i \\,\\exp(k_s \\theta_s) + b_i\\, \\exp(k_b \\theta_b)$. The term $n_i \\ln \\lambda_i$ is taken to be $0$ if $n_i=0$, which is handled robustly in the implementation.\n\nIn terms of the NLL, the test statistic becomes:\n$$\nq_\\mu = 2 \\left[ f_{\\text{cond}}(\\mu) - f_{\\text{global}} \\right], \\quad \\text{for } \\hat{\\mu} \\le \\mu,\n$$\nwhere $f_{\\text{cond}}(\\mu)$ is the minimum of the NLL for a fixed $\\mu$, and $f_{\\text{global}}$ is the global minimum of the NLL.\n\nThe computation of $q_\\mu$ therefore involves two distinct numerical optimization steps:\n$1$. **Unconditional (Global) Minimization:** We find the global minimum of the NLL function, $f_{\\text{global}}$, by minimizing $f(\\mu, \\theta_s, \\theta_b)$ with respect to all three parameters $(\\mu, \\theta_s, \\theta_b)$ subject to the physical constraint $\\mu \\ge 0$. The value of $\\mu$ at this minimum is the MLE, $\\hat{\\mu}$.\n$$\nf_{\\text{global}} = \\min_{\\mu \\ge 0, \\, \\theta_s, \\, \\theta_b} f(\\mu, \\theta_s, \\theta_b)\n$$\nThis yields both $f_{\\text{global}}$ and $\\hat{\\mu}$.\n\n$2$. **Conditional (Profiled) Minimization:** For the specific value of $\\mu$ being tested, we find the conditional minimum of the NLL, $f_{\\text{cond}}(\\mu)$, by minimizing $f(\\mu, \\theta_s, \\theta_b)$ with respect to only the nuisance parameters $(\\theta_s, \\theta_b)$.\n$$\nf_{\\text{cond}}(\\mu) = \\min_{\\theta_s, \\, \\theta_b} f(\\mu, \\theta_s, \\theta_b)\n$$\n\nFinally, we assemble the test statistic. First, we compare the globally fitted $\\hat{\\mu}$ with the test value $\\mu$. If $\\hat{\\mu} > \\mu$, the data favors a signal strength larger than the one being tested, so by definition, $q_\\mu=0$. Otherwise, if $\\hat{\\mu} \\le \\mu$, the value is calculated as $q_\\mu = 2(f_{\\text{cond}}(\\mu) - f_{\\text{global}})$. We ensure the result is non-negative to account for any minor floating-point inaccuracies where the true value is zero.\n\nThese minimizations are performed using the `scipy.optimize.minimize` function with the `L-BFGS-B` algorithm, which is well-suited for bound-constrained quasi-Newton optimization. Reasonable bounds, such as $[-5, 5]$, are placed on the nuisance parameters $\\theta_s$ and $\\theta_b$ to improve numerical stability, reflecting the expectation that they are unlikely to deviate by more than $5$ standard deviations from their central value of $0$.\nThe constants $k_s$ and $k_b$ are set to $k_s = \\ln(1 + 0.10)$ and $k_b = \\ln(1 + 0.20)$ as specified.", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef solve():\n    \"\"\"\n    Computes the q_mu test statistic for several counting experiments.\n    \"\"\"\n\n    # Define fractional scales for systematics from the problem statement.\n    # k_s corresponds to a 10% uncertainty, k_b to a 20% uncertainty.\n    k_s = np.log(1.0 + 0.10)\n    k_b = np.log(1.0 + 0.20)\n\n    # Test cases as defined in the problem.\n    # Each case is a tuple: (s_i, b_i, n_i, mu_test)\n    test_cases = [\n        # Case A\n        (\n            np.array([5.0, 9.0, 4.0, 2.0]),\n            np.array([20.0, 15.0, 10.0, 5.0]),\n            np.array([27, 25, 15, 8]),\n            1.0\n        ),\n        # Case B\n        (\n            np.array([8.0, 12.0, 5.0, 1.0]),\n            np.array([18.0, 14.0, 11.0, 6.0]),\n            np.array([13, 10, 7, 4]),\n            1.0\n        ),\n        # Case C\n        (\n            np.array([0.5, 1.0, 0.2, 0.1]),\n            np.array([0.2, 0.5, 0.2, 0.05]),\n            np.array([0, 1, 0, 2]),\n            0.5\n        )\n    ]\n\n    def neg_log_likelihood(params, s, b, n, ks, kb):\n        \"\"\"\n        Calculates the negative log-likelihood.\n        This function is the core objective function for minimization.\n        \n        params: Tuple or array of parameters (mu, theta_s, theta_b).\n        s, b, n: Signal, background, and observed counts arrays.\n        ks, kb: Log-normal morphing scales for systematics.\n        \"\"\"\n        mu, theta_s, theta_b = params\n\n        # Calculate expected yields per bin\n        # The model ensures lambda_i > 0 for mu >= 0 and b_i > 0\n        lambda_i = mu * s * np.exp(ks * theta_s) + b * np.exp(kb * theta_b)\n        \n        # Guard against non-positive lambda_i which can occur during optimization steps\n        # If any lambda_i is non-positive where n_i is positive, log-likelihood is -inf, NLL is +inf.\n        if np.any(lambda_i <= 0):\n             if np.any(n[lambda_i <= 0] > 0):\n                 return np.inf\n\n        # Calculate Poisson part of NLL: sum(lambda_i - n_i * log(lambda_i))\n        # This implementation is numerically stable for n_i = 0 cases.\n        log_lambda_terms = np.where(n > 0, n * np.log(lambda_i), 0.)\n        poisson_nll = np.sum(lambda_i - log_lambda_terms)\n\n        # Add Gaussian constraint terms for nuisance parameters\n        constraints_nll = 0.5 * (theta_s**2 + theta_b**2)\n\n        return poisson_nll + constraints_nll\n    \n    def calculate_q_mu(s, b, n, mu_test, ks, kb):\n        \"\"\"\n        Performs the full calculation of q_mu for a single case.\n        \"\"\"\n        # --- 1. Unconditional (Global) Minimization ---\n        # Objective function for global fit, taking a single parameter vector.\n        def objective_global(p):\n            return neg_log_likelihood(p, s, b, n, ks, kb)\n\n        # Initial guess and bounds for [mu, theta_s, theta_b]\n        # Bounds on thetas are for stability; mu>=0 is a physical constraint.\n        x0_global = [1.0, 0.0, 0.0]\n        bounds_global = [(0, None), (-5, 5), (-5, 5)]\n\n        res_global = minimize(\n            objective_global,\n            x0=x0_global,\n            method='L-BFGS-B',\n            bounds=bounds_global\n        )\n        \n        nll_global = res_global.fun\n        mu_hat = res_global.x[0]\n\n        # --- 2. Conditional (Profiled) Minimization ---\n        # Objective function for conditional fit at fixed mu_test.\n        def objective_conditional(p):\n            # p is [theta_s, theta_b]\n            params = (mu_test, p[0], p[1])\n            return neg_log_likelihood(params, s, b, n, ks, kb)\n\n        # Initial guess and bounds for [theta_s, theta_b]\n        x0_cond = [0.0, 0.0]\n        bounds_cond = [(-5, 5), (-5, 5)]\n\n        res_cond = minimize(\n            objective_conditional,\n            x0=x0_cond,\n            method='L-BFGS-B',\n            bounds=bounds_cond\n        )\n        \n        nll_conditional = res_cond.fun\n\n        # --- 3. Compute q_mu ---\n        if mu_hat > mu_test:\n            q_mu = 0.0\n        else:\n            # Ensure q_mu >= 0 to handle potential floating-point inaccuracies\n            q_mu = max(0.0, 2 * (nll_conditional - nll_global))\n        \n        return q_mu\n\n    results = []\n    for case in test_cases:\n        s_case, b_case, n_case, mu_test_case = case\n        q_mu_val = calculate_q_mu(s_case, b_case, n_case, mu_test_case, k_s, k_b)\n        results.append(q_mu_val)\n\n    # Format and print the final output as a single line.\n    print(f\"[{','.join(f'{r:.8f}' for r in results)}]\")\n\nsolve()\n```", "id": "3509408"}]}