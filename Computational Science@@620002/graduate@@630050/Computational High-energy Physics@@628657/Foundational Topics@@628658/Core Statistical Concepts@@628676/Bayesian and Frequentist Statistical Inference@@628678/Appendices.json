{"hands_on_practices": [{"introduction": "This first practice plunges us into a quintessential high-energy physics analysis scenario: a counting experiment where the background is constrained by an auxiliary measurement. The exercise is a cornerstone of frequentist inference, guiding you through the derivation of maximum likelihood estimates for signal and background yields. More critically, it demonstrates how to compute their full asymptotic covariance matrix, revealing the uncertainties and, just as importantly, the correlations between these estimated parameters [@problem_id:3506272].", "problem": "A single-bin counting experiment is conducted in a search for a heavy resonance in the dimuon channel at the Large Hadron Collider (LHC). The analysis proceeds under the following generative model: the observed count in the signal region, denoted by `n`, is assumed to follow a Poisson distribution with mean $s + b$, where $s \\ge 0$ is the unknown mean signal yield and $b \\ge 0$ is the unknown mean background yield. An independent auxiliary background calibration produces a single Gaussian measurement `x` of the background with known standard deviation `sigma`, modeled as $x \\sim \\mathcal{N}(b, \\sigma^{2})$. Independence is assumed between the Poisson count and the Gaussian calibration conditional on `b` and `s`.\n\nThe well-tested facts and definitions to be used are: the Poisson probability mass function $P(n \\mid \\lambda) = \\exp(-\\lambda)\\lambda^{n}/n!$ for integer $n \\ge 0$ and mean $\\lambda > 0$, the Gaussian probability density $f(x \\mid \\mu,\\sigma^{2}) = (2\\pi\\sigma^{2})^{-1/2}\\exp\\!\\big(- (x - \\mu)^{2}/(2\\sigma^{2})\\big)$ for $x \\in \\mathbb{R}$, and the definition of the maximum likelihood estimate (MLE) as the parameter value that maximizes the likelihood function. The asymptotic covariance matrix of the MLE is obtained by inverting the observed information matrix, defined as the negative Hessian (matrix of second derivatives) of the log-likelihood evaluated at the MLE, under standard regularity conditions.\n\nYou are given the observed data $n = 35$, the auxiliary measurement $x = 28$, and the known standard deviation $\\sigma = 6$. Starting only from the above definitions and facts, derive the joint MLEs for $(s,b)$ and obtain their asymptotic covariance matrix by inverting the observed information at the MLE. Assume the interior solution $s > 0$ and $b > 0$ is valid for the provided data. Express your final answer as a single row matrix containing, in order, $s^{\\ast}$, $b^{\\ast}$, $\\operatorname{Var}(s^{\\ast})$, $\\operatorname{Cov}(s^{\\ast}, b^{\\ast})$, $\\operatorname{Cov}(b^{\\ast}, s^{\\ast})$, and $\\operatorname{Var}(b^{\\ast})$. No rounding is required, and no physical units are to be included in the final numeric values.", "solution": "The problem statement is formally validated and found to be self-contained, scientifically grounded, and well-posed. All necessary data, models, and definitions are provided, and there are no internal contradictions or ambiguities. The problem is a standard application of maximum likelihood estimation in a context representative of data analysis in high-energy physics. We may therefore proceed with the solution.\n\nThe goal is to find the maximum likelihood estimates (MLEs) for the parameters $s$ (signal) and $b$ (background), and their asymptotic covariance matrix. The data consist of an observed count $n$ and an auxiliary measurement $x$.\n\nThe statistical model is defined by two components:\n1.  The count $n$ follows a Poisson distribution with mean $s+b$: $n \\sim \\text{Poisson}(s+b)$. The probability mass function is $P(n \\mid s, b) = \\frac{\\exp(-(s+b))(s+b)^n}{n!}$.\n2.  The auxiliary measurement $x$ follows a Gaussian distribution with mean $b$ and known variance $\\sigma^2$: $x \\sim \\mathcal{N}(b, \\sigma^2)$. The probability density function is $f(x \\mid b, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x-b)^2}{2\\sigma^2}\\right)$.\n\nGiven that the two measurements are conditionally independent, the joint likelihood function $L(s, b)$ for observing the data pair $(n, x)$ is the product of the individual probability functions:\n$$L(s, b) = P(n \\mid s, b) \\times f(x \\mid b, \\sigma^2) = \\frac{\\exp(-(s+b))(s+b)^n}{n!} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x-b)^2}{2\\sigma^2}\\right)$$\nIt is computationally more convenient to work with the natural logarithm of the likelihood, the log-likelihood function $\\ell(s, b) = \\ln L(s, b)$:\n$$\\ell(s, b) = -(s+b) + n\\ln(s+b) - \\ln(n!) - \\frac{1}{2}\\ln(2\\pi\\sigma^2) - \\frac{(x-b)^2}{2\\sigma^2}$$\nTo find the MLEs, we maximize this function with respect to $s$ and $b$. We can disregard the terms $-\\ln(n!)$ and $-\\frac{1}{2}\\ln(2\\pi\\sigma^2)$ as they are constants that do not depend on the parameters $s$ and $b$. The relevant part of the log-likelihood is:\n$$\\ell(s, b) = -(s+b) + n\\ln(s+b) - \\frac{(x-b)^2}{2\\sigma^2}$$\nThe MLEs, denoted $s^*$ and $b^*$, are found by solving the system of equations where the first partial derivatives of $\\ell(s, b)$ are zero:\n$$\\frac{\\partial\\ell}{\\partial s} = 0 \\quad \\text{and} \\quad \\frac{\\partial\\ell}{\\partial b} = 0$$\nLet's compute the partial derivatives:\n$$\\frac{\\partial\\ell}{\\partial s} = -1 + \\frac{n}{s+b}$$\n$$\\frac{\\partial\\ell}{\\partial b} = -1 + \\frac{n}{s+b} + \\frac{x-b}{\\sigma^2}$$\nSetting the first equation to zero gives:\n$$-1 + \\frac{n}{s^*+b^*} = 0 \\implies s^*+b^* = n$$\nSubstituting this result into the second equation set to zero:\n$$-1 + \\frac{n}{n} + \\frac{x-b^*}{\\sigma^2} = 0 \\implies -1 + 1 + \\frac{x-b^*}{\\sigma^2} = 0 \\implies \\frac{x-b^*}{\\sigma^2} = 0 \\implies b^* = x$$\nUsing this result for $b^*$ in the first condition, we find $s^*$:\n$$s^* + x = n \\implies s^* = n-x$$\nThe MLEs are $s^* = n-x$ and $b^*=x$. We are given $n=35$ and $x=28$.\n$$s^* = 35 - 28 = 7$$\n$$b^* = 28$$\nThe problem states to assume an interior solution ($s > 0$ and $b > 0$), which is consistent with these results.\n\nNext, we derive the asymptotic covariance matrix of the MLEs. This is given by the inverse of the observed information matrix, $I(s^*, b^*)$, which is the negative of the Hessian matrix of the log-likelihood, evaluated at the MLEs. The Hessian matrix $H$ is the matrix of second partial derivatives.\n\nLet's compute the second partial derivatives of $\\ell(s,b)$:\n$$\\frac{\\partial^2\\ell}{\\partial s^2} = \\frac{\\partial}{\\partial s} \\left(-1 + \\frac{n}{s+b}\\right) = -\\frac{n}{(s+b)^2}$$\n$$\\frac{\\partial^2\\ell}{\\partial b^2} = \\frac{\\partial}{\\partial b} \\left(-1 + \\frac{n}{s+b} + \\frac{x-b}{\\sigma^2}\\right) = -\\frac{n}{(s+b)^2} - \\frac{1}{\\sigma^2}$$\n$$\\frac{\\partial^2\\ell}{\\partial s \\partial b} = \\frac{\\partial}{\\partial b} \\left(-1 + \\frac{n}{s+b}\\right) = -\\frac{n}{(s+b)^2}$$\nBy Clairaut's theorem, $\\frac{\\partial^2\\ell}{\\partial b \\partial s} = \\frac{\\partial^2\\ell}{\\partial s \\partial b}$. The Hessian matrix is:\n$$H(s,b) = \\begin{pmatrix} \\frac{\\partial^2\\ell}{\\partial s^2}  \\frac{\\partial^2\\ell}{\\partial s \\partial b} \\\\ \\frac{\\partial^2\\ell}{\\partial b \\partial s}  \\frac{\\partial^2\\ell}{\\partial b^2} \\end{pmatrix} = \\begin{pmatrix} -\\frac{n}{(s+b)^2}  -\\frac{n}{(s+b)^2} \\\\ -\\frac{n}{(s+b)^2}  -\\frac{n}{(s+b)^2} - \\frac{1}{\\sigma^2} \\end{pmatrix}$$\nWe evaluate this matrix at the MLEs, where $s^*+b^* = n$:\n$$H(s^*, b^*) = \\begin{pmatrix} -\\frac{n}{n^2}  -\\frac{n}{n^2} \\\\ -\\frac{n}{n^2}  -\\frac{n}{n^2} - \\frac{1}{\\sigma^2} \\end{pmatrix} = \\begin{pmatrix} -\\frac{1}{n}  -\\frac{1}{n} \\\\ -\\frac{1}{n}  -\\frac{1}{n} - \\frac{1}{\\sigma^2} \\end{pmatrix}$$\nThe observed information matrix at the MLE is $I(s^*, b^*) = -H(s^*, b^*)$:\n$$I(s^*, b^*) = \\begin{pmatrix} \\frac{1}{n}  \\frac{1}{n} \\\\ \\frac{1}{n}  \\frac{1}{n} + \\frac{1}{\\sigma^2} \\end{pmatrix}$$\nThe asymptotic covariance matrix $C$ is the inverse of $I(s^*, b^*)$. For a $2 \\times 2$ matrix $A = \\begin{pmatrix} a  b \\\\ c  d \\end{pmatrix}$, its inverse is $A^{-1} = \\frac{1}{ad-bc}\\begin{pmatrix} d  -b \\\\ -c  a \\end{pmatrix}$.\nThe determinant of $I(s^*, b^*)$ is:\n$$\\det(I) = \\left(\\frac{1}{n}\\right)\\left(\\frac{1}{n} + \\frac{1}{\\sigma^2}\\right) - \\left(\\frac{1}{n}\\right)\\left(\\frac{1}{n}\\right) = \\frac{1}{n^2} + \\frac{1}{n\\sigma^2} - \\frac{1}{n^2} = \\frac{1}{n\\sigma^2}$$\nNow we compute the inverse matrix $C$:\n$$C = [I(s^*, b^*)]^{-1} = \\frac{1}{1/(n\\sigma^2)} \\begin{pmatrix} \\frac{1}{n} + \\frac{1}{\\sigma^2}  -\\frac{1}{n} \\\\ -\\frac{1}{n}  \\frac{1}{n} \\end{pmatrix} = n\\sigma^2 \\begin{pmatrix} \\frac{\\sigma^2+n}{n\\sigma^2}  -\\frac{1}{n} \\\\ -\\frac{1}{n}  \\frac{1}{n} \\end{pmatrix}$$\n$$C = \\begin{pmatrix} n\\sigma^2 \\left(\\frac{\\sigma^2+n}{n\\sigma^2}\\right)  n\\sigma^2 \\left(-\\frac{1}{n}\\right) \\\\ n\\sigma^2 \\left(-\\frac{1}{n}\\right)  n\\sigma^2 \\left(\\frac{1}{n}\\right) \\end{pmatrix} = \\begin{pmatrix} n+\\sigma^2  -\\sigma^2 \\\\ -\\sigma^2  \\sigma^2 \\end{pmatrix}$$\nThe covariance matrix $C$ contains the variances and covariances of the estimators:\n$$C = \\begin{pmatrix} \\operatorname{Var}(s^*)  \\operatorname{Cov}(s^*, b^*) \\\\ \\operatorname{Cov}(b^*, s^*)  \\operatorname{Var}(b^*) \\end{pmatrix}$$\nSo, we have:\n$\\operatorname{Var}(s^*) = n+\\sigma^2$\n$\\operatorname{Cov}(s^*, b^*) = \\operatorname{Cov}(b^*, s^*) = -\\sigma^2$\n$\\operatorname{Var}(b^*) = \\sigma^2$\n\nWe are given $n=35$ and $\\sigma=6$, so $\\sigma^2=36$. Substituting these values:\n$\\operatorname{Var}(s^*) = 35 + 36 = 71$\n$\\operatorname{Cov}(s^*, b^*) = -36$\n$\\operatorname{Var}(b^*) = 36$\n\nThe required six values to be reported in a single row matrix are $s^{\\ast}$, $b^{\\ast}$, $\\operatorname{Var}(s^{\\ast})$, $\\operatorname{Cov}(s^{\\ast}, b^{\\ast})$, $\\operatorname{Cov}(b^{\\ast}, s^{\\ast})$, and $\\operatorname{Var}(b^{\\ast})$.\nThe values are:\n$s^* = 7$\n$b^* = 28$\n$\\operatorname{Var}(s^*) = 71$\n$\\operatorname{Cov}(s^*, b^*) = -36$\n$\\operatorname{Cov}(b^*, s^*) = -36$\n$\\operatorname{Var}(b^*) = 36$\nThese will be presented in the specified format.", "answer": "$$\n\\boxed{\\begin{pmatrix} 7  28  71  -36  -36  36 \\end{pmatrix}}\n$$", "id": "3506272"}, {"introduction": "Building on the practical estimation of uncertainties from the previous exercise, we now explore the theoretical foundations of statistical precision. This problem tasks you with deriving the Fisher Information for a signal strength parameter $\\mu$ in an extended unbinned likelihood model, a very general and powerful setup in particle physics. By distinguishing between the observed and expected Fisher Information, you will connect this fundamental quantity directly to the asymptotic variance of the estimator, revealing the ultimate statistical power of an experiment [@problem_id:3506307].", "problem": "A high-energy physics analysis models the number and features of reconstructed events as the superposition of two independent Poisson point processes: a signal component with expected yield $s$ and a background component with expected yield $b$. The parameter of interest is the signal strength modifier $\\mu \\ge 0$, which scales the expected signal yield to $\\mu s$. Each event carries a real-valued observable $x \\in \\mathbb{R}$ with probability density functions (PDFs) $f_s(x)$ and $f_b(x)$ for signal and background, respectively, where $f_s$ and $f_b$ are each normalized to $1$ on $\\mathbb{R}$. The observed data are $n$ events with values $\\{x_i\\}_{i=1}^{n}$.\n\nStarting from the definition of the extended unbinned likelihood for a superposition of independent Poisson point processes, derive the log-likelihood for $\\mu$, the score function, and the observed Fisher information (FI) $J(\\mu)$ for $\\mu$. Then compute the expected Fisher information $I(\\mu)$ by averaging over the joint distribution of the Poisson counts and the event features induced by the superposed point process. Finally, specialize to the case $f_s(x) = f_b(x)$ for all $x$, and derive a closed-form expression for the asymptotic variance of the maximum likelihood estimator (MLE) $\\hat{\\mu}$, given by the reciprocal of the expected Fisher information evaluated at the true signal strength $\\mu_0$.\n\nAssumptions and definitions:\n- The extended unbinned likelihood for a Poisson point process with total intensity density $\\Lambda_{\\mu}(x) = \\mu s f_s(x) + b f_b(x)$ on $\\mathbb{R}$ is given by $L(\\mu) \\propto \\exp(-\\nu(\\mu)) \\prod_{i=1}^{n} \\Lambda_{\\mu}(x_i)$, where $\\nu(\\mu) = \\int_{\\mathbb{R}} \\Lambda_{\\mu}(x) \\,\\mathrm{d}x = \\mu s + b$.\n- The observed Fisher information is $J(\\mu) = -\\frac{\\partial^2}{\\partial \\mu^2} \\ln L(\\mu)$ evaluated on the observed data.\n- The expected Fisher information is $I(\\mu) = \\mathbb{E}[J(\\mu)]$ with the expectation taken under the true process at $\\mu$.\n- You may invoke the fundamental identity for Poisson point processes that $\\mathbb{E}\\left[\\sum_{i=1}^{n} h(x_i)\\right] = \\int_{\\mathbb{R}} \\Lambda_{\\mu}(x) h(x)\\,\\mathrm{d}x$ for any integrable function $h$.\n- Treat $s0$ and $b0$ as known constants, and take $f_s$ and $f_b$ to be known normalized PDFs. The parameter $\\mu$ is real-valued and dimensionless.\n\nProvide your final answer as a single closed-form expression for the asymptotic variance $\\mathrm{Var}(\\hat{\\mu})$ in the special case $f_s(x) = f_b(x)$, expressed in terms of $\\mu_0$, $s$, and $b$. No units are required. Do not round.", "solution": "The problem requires the derivation of several statistical quantities for an extended unbinned likelihood model and then the computation of the asymptotic variance of the maximum likelihood estimator (MLE) for the signal strength modifier $\\mu$ in a special case.\n\nThe likelihood for a superposition of independent Poisson point processes is given as $L(\\mu) \\propto \\exp(-\\nu(\\mu)) \\prod_{i=1}^{n} \\Lambda_{\\mu}(x_i)$, where $n$ is the observed number of events.\nThe total expected yield is $\\nu(\\mu) = \\int_{\\mathbb{R}} \\Lambda_{\\mu}(x) \\,\\mathrm{d}x = \\mu s + b$.\nThe intensity density is $\\Lambda_{\\mu}(x) = \\mu s f_s(x) + b f_b(x)$, where $s > 0$ and $b > 0$ are known constants, and $f_s(x)$ and $f_b(x)$ are known, normalized probability density functions (PDFs). The observed data consist of $n$ event feature values $\\{x_i\\}_{i=1}^{n}$.\n\nFirst, we derive the log-likelihood function, $\\ell(\\mu) = \\ln L(\\mu)$. Ignoring terms that are constant with respect to $\\mu$, we have:\n$$\n\\ell(\\mu) = -\\nu(\\mu) + \\sum_{i=1}^{n} \\ln(\\Lambda_{\\mu}(x_i))\n$$\n$$\n\\ell(\\mu) = -(\\mu s + b) + \\sum_{i=1}^{n} \\ln(\\mu s f_s(x_i) + b f_b(x_i))\n$$\nSince the term $-b$ is independent of $\\mu$, it does not contribute to the derivatives with respect to $\\mu$ and can be dropped for the purpose of finding the MLE and Fisher information.\n$$\n\\ell(\\mu) = -\\mu s + \\sum_{i=1}^{n} \\ln(\\mu s f_s(x_i) + b f_b(x_i))\n$$\nNext, we derive the score function, which is the first derivative of the log-likelihood with respect to the parameter $\\mu$:\n$$\nS(\\mu) = \\frac{\\partial \\ell(\\mu)}{\\partial \\mu} = \\frac{\\partial}{\\partial \\mu} \\left( -\\mu s + \\sum_{i=1}^{n} \\ln(\\mu s f_s(x_i) + b f_b(x_i)) \\right)\n$$\n$$\nS(\\mu) = -s + \\sum_{i=1}^{n} \\frac{1}{\\mu s f_s(x_i) + b f_b(x_i)} \\cdot \\frac{\\partial}{\\partial \\mu}(\\mu s f_s(x_i) + b f_b(x_i))\n$$\n$$\nS(\\mu) = -s + \\sum_{i=1}^{n} \\frac{s f_s(x_i)}{\\mu s f_s(x_i) + b f_b(x_i)}\n$$\nThe observed Fisher information, $J(\\mu)$, is defined as the negative of the second derivative of the log-likelihood: $J(\\mu) = -\\frac{\\partial^2 \\ell(\\mu)}{\\partial \\mu^2}$. We compute this by differentiating the score function:\n$$\n\\frac{\\partial S(\\mu)}{\\partial \\mu} = \\frac{\\partial}{\\partial \\mu} \\left( -s + \\sum_{i=1}^{n} \\frac{s f_s(x_i)}{\\mu s f_s(x_i) + b f_b(x_i)} \\right)\n$$\n$$\n\\frac{\\partial S(\\mu)}{\\partial \\mu} = \\sum_{i=1}^{n} s f_s(x_i) \\left( -1 \\cdot (\\mu s f_s(x_i) + b f_b(x_i))^{-2} \\cdot \\frac{\\partial}{\\partial \\mu}(\\mu s f_s(x_i) + b f_b(x_i)) \\right)\n$$\n$$\n\\frac{\\partial S(\\mu)}{\\partial \\mu} = \\sum_{i=1}^{n} s f_s(x_i) \\left( - \\frac{s f_s(x_i)}{(\\mu s f_s(x_i) + b f_b(x_i))^2} \\right) = - \\sum_{i=1}^{n} \\frac{s^2 f_s(x_i)^2}{(\\mu s f_s(x_i) + b f_b(x_i))^2}\n$$\nThus, the observed Fisher information is:\n$$\nJ(\\mu) = -\\frac{\\partial^2 \\ell(\\mu)}{\\partial \\mu^2} = \\sum_{i=1}^{n} \\frac{s^2 f_s(x_i)^2}{(\\mu s f_s(x_i) + b f_b(x_i))^2}\n$$\nThe expected Fisher information, $I(\\mu)$, is the expectation of $J(\\mu)$ over the data distribution defined by the model at parameter value $\\mu$. The problem provides the identity $\\mathbb{E}\\left[\\sum_{i=1}^{n} h(x_i)\\right] = \\int_{\\mathbb{R}} \\Lambda_{\\mu}(x) h(x) \\, \\mathrm{d}x$. Let $h(x) = \\frac{s^2 f_s(x)^2}{(\\mu s f_s(x) + b f_b(x))^2}$. Then:\n$$\nI(\\mu) = \\mathbb{E}[J(\\mu)] = \\int_{\\mathbb{R}} \\Lambda_{\\mu}(x) h(x) \\, \\mathrm{d}x = \\int_{\\mathbb{R}} (\\mu s f_s(x) + b f_b(x)) \\frac{s^2 f_s(x)^2}{(\\mu s f_s(x) + b f_b(x))^2} \\, \\mathrm{d}x\n$$\n$$\nI(\\mu) = \\int_{\\mathbb{R}} \\frac{s^2 f_s(x)^2}{\\mu s f_s(x) + b f_b(x)} \\, \\mathrm{d}x\n$$\nThis is the general expression for the expected Fisher information for $\\mu$.\n\nNow, we specialize to the case where the signal and background PDFs are identical, i.e., $f_s(x) = f_b(x)$ for all $x$. Let's call this common PDF $f(x)$. Substituting this into the expression for $I(\\mu)$:\n$$\nI(\\mu) = \\int_{\\mathbb{R}} \\frac{s^2 f(x)^2}{\\mu s f(x) + b f(x)} \\, \\mathrm{d}x\n$$\nFactoring $f(x)$ out of the denominator:\n$$\nI(\\mu) = \\int_{\\mathbb{R}} \\frac{s^2 f(x)^2}{(\\mu s + b)f(x)} \\, \\mathrm{d}x = \\int_{\\mathbb{R}} \\frac{s^2 f(x)}{\\mu s + b} \\, \\mathrm{d}x\n$$\nThe term $\\frac{s^2}{\\mu s + b}$ is constant with respect to the integration variable $x$, so it can be factored out of the integral:\n$$\nI(\\mu) = \\frac{s^2}{\\mu s + b} \\int_{\\mathbb{R}} f(x) \\, \\mathrm{d}x\n$$\nSince $f(x)$ is a normalized PDF, $\\int_{\\mathbb{R}} f(x) \\, \\mathrm{d}x = 1$. This yields a simplified expression for the expected Fisher information in this special case:\n$$\nI(\\mu) = \\frac{s^2}{\\mu s + b}\n$$\nThe asymptotic variance of the MLE $\\hat{\\mu}$ is given by the inverse of the expected Fisher information evaluated at the true value of the parameter, $\\mu_0$. This is the Cramér-Rao Lower Bound.\n$$\n\\mathrm{Var}(\\hat{\\mu}) = [I(\\mu_0)]^{-1}\n$$\nSubstituting the expression for $I(\\mu)$ evaluated at $\\mu_0$:\n$$\nI(\\mu_0) = \\frac{s^2}{\\mu_0 s + b}\n$$\nTherefore, the asymptotic variance is:\n$$\n\\mathrm{Var}(\\hat{\\mu}) = \\left( \\frac{s^2}{\\mu_0 s + b} \\right)^{-1} = \\frac{\\mu_0 s + b}{s^2}\n$$\nThis is the final closed-form expression for the asymptotic variance of $\\hat{\\mu}$ under the specified conditions.", "answer": "$$\\boxed{\\frac{\\mu_0 s + b}{s^2}}$$", "id": "3506307"}, {"introduction": "We now pivot to a direct comparison between the frequentist and Bayesian paradigms for interval estimation, using the fundamental binomial model for an efficiency $\\epsilon$. You will first derive the Jeffreys prior, a cornerstone of objective Bayesian analysis, from the Fisher Information, and then construct the corresponding Bayesian credible interval. This exercise culminates in a critical evaluation of this interval's frequentist coverage properties against the classic Clopper-Pearson confidence interval, providing concrete insight into the different guarantees and philosophies underpinning these two schools of thought [@problem_id:3506258].", "problem": "You are given a binomial model where the number of observed successes `k` in `n` trials is modeled as $k \\sim \\mathrm{Bin}(n,\\epsilon)$ with an unknown efficiency parameter $\\epsilon \\in [0,1]$. Starting from first principles of statistical inference and information geometry, perform the following tasks:\n\n- Derive the Fisher information $I(\\epsilon)$ for the binomial model with parameter $\\epsilon$ and fixed `n`. Use the definition $I(\\epsilon) = \\mathbb{E}\\left[-\\frac{\\partial^2}{\\partial \\epsilon^2} \\log p(k \\mid n,\\epsilon)\\right]$ where $p(k \\mid n,\\epsilon)$ is the binomial likelihood.\n- Using the definition of Jeffreys prior, $\\pi(\\epsilon) \\propto \\sqrt{I(\\epsilon)}$, derive the form of the Jeffreys prior for $\\epsilon$ in the binomial model.\n- Derive the posterior distribution $p(\\epsilon \\mid k,n)$ under the Jeffreys prior and the binomial likelihood by applying Bayes' theorem. Define the equal-tailed Bayesian credible interval at nominal level $1-\\alpha$ as the interval between the $\\alpha/2$ and $1-\\alpha/2$ posterior quantiles.\n- Define the Clopper–Pearson (CP) confidence interval by inverting the binomial cumulative distribution for a two-sided test at level $\\alpha$ and express its endpoints in terms of quantiles of suitable Beta distributions. The CP interval is used as a frequentist benchmark.\n\nImplement a complete program that:\n1. Computes, for each $k \\in \\{0,1,\\dots,n\\}$, the equal-tailed Bayesian credible interval for $\\epsilon$ under the Jeffreys prior at nominal level $1-\\alpha$.\n2. Computes, for each $k \\in \\{0,1,\\dots,n\\}$, the Clopper–Pearson (CP) confidence interval at the same nominal level $1-\\alpha$.\n3. For a specified grid of true efficiencies $\\epsilon$ values, evaluates the exact frequentist coverage of each interval by summing the binomial probabilities of `k` for which the interval contains the true $\\epsilon$. For a fixed true $\\epsilon$, define coverage as $C(\\epsilon) = \\sum_{k=0}^{n} \\mathbb{I}\\{\\epsilon \\in [L(k),U(k)]\\} \\cdot \\mathrm{Bin}(k;n,\\epsilon)$, where $[L(k),U(k)]$ is the interval computed for `k`, and $\\mathbb{I}\\{\\cdot\\}$ is the indicator function.\n4. For each test case, computes two summary metrics for each interval type: \n   - The worst-case deficit relative to nominal coverage, defined as $\\min_{\\epsilon \\text{ in grid}} \\left( C(\\epsilon) - (1-\\alpha) \\right)$.\n   - The mean absolute coverage error over the grid, defined as $\\frac{1}{M}\\sum_{i=1}^{M} \\left| C(\\epsilon_i) - (1-\\alpha) \\right|$ where $M$ is the number of grid points.\n\nUse the following test suite of parameter values that probe typical conditions and edge cases:\n- Case $1$: $n=10$, $\\alpha=0.1$, and a grid of $21$ efficiencies $\\epsilon \\in \\{0.00,0.05,\\dots,1.00\\}$.\n- Case $2$: $n=20$, $\\alpha=0.1$, and a grid of $11$ efficiencies $\\epsilon \\in \\{0.00,0.10,\\dots,1.00\\}$.\n- Case $3$: $n=20$, $\\alpha=0.32$, and a grid of $21$ efficiencies $\\epsilon \\in \\{0.00,0.05,\\dots,1.00\\}$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each test case, output a list of four floats: $[$worst-case deficit for Bayesian intervals, worst-case deficit for Clopper–Pearson intervals, mean absolute coverage error for Bayesian intervals, mean absolute coverage error for Clopper–Pearson intervals$]$. Aggregate the three test-case result lists into one top-level list and print it. For example, $\\left[\\left[a_1,b_1,c_1,d_1\\right],\\left[a_2,b_2,c_2,d_2\\right],\\left[a_3,b_3,c_3,d_3\\right]\\right]$. All coverage quantities and deficits must be expressed as decimal fractions, with no percentage signs.", "solution": "The problem requires a thorough analysis and comparison of Bayesian credible intervals and frequentist confidence intervals for a binomial proportion parameter $\\epsilon$. The validation confirms that the problem is scientifically grounded, well-posed, objective, and contains all necessary information for a unique solution. The definitions and methodologies are standard in the field of statistical inference.\n\n### Theoretical Derivations\n\nFirst, we derive the necessary theoretical components from first principles as requested.\n\n**1. Fisher Information for the Binomial Model**\n\nThe Fisher information $I(\\epsilon)$ quantifies the amount of information that an observable random variable $k$ carries about an unknown parameter $\\epsilon$. For a binomial model, the likelihood of observing $k$ successes in $n$ trials is given by the probability mass function:\n$$p(k \\mid n,\\epsilon) = \\binom{n}{k} \\epsilon^k (1-\\epsilon)^{n-k}$$\nThe log-likelihood, $\\ell(\\epsilon) = \\log p(k \\mid n,\\epsilon)$, is:\n$$\\ell(\\epsilon) = \\log\\binom{n}{k} + k \\log \\epsilon + (n-k) \\log(1-\\epsilon)$$\nTo find the Fisher information, we compute the second partial derivative of the log-likelihood with respect to $\\epsilon$:\n$$\\frac{\\partial \\ell}{\\partial \\epsilon} = \\frac{k}{\\epsilon} - \\frac{n-k}{1-\\epsilon}$$\n$$\\frac{\\partial^2 \\ell}{\\partial \\epsilon^2} = -\\frac{k}{\\epsilon^2} - \\frac{n-k}{(1-\\epsilon)^2}$$\nThe Fisher information is defined as the expectation of the negative of this second derivative:\n$$I(\\epsilon) = \\mathbb{E}\\left[-\\frac{\\partial^2 \\ell}{\\partial \\epsilon^2}\\right] = \\mathbb{E}\\left[\\frac{k}{\\epsilon^2} + \\frac{n-k}{(1-\\epsilon)^2}\\right]$$\nUsing the linearity of expectation and the fact that $\\mathbb{E}[k] = n\\epsilon$ for a binomial distribution, we get:\n$$I(\\epsilon) = \\frac{\\mathbb{E}[k]}{\\epsilon^2} + \\frac{n-\\mathbb{E}[k]}{(1-\\epsilon)^2} = \\frac{n\\epsilon}{\\epsilon^2} + \\frac{n-n\\epsilon}{(1-\\epsilon)^2}$$\n$$I(\\epsilon) = \\frac{n}{\\epsilon} + \\frac{n(1-\\epsilon)}{(1-\\epsilon)^2} = \\frac{n}{\\epsilon} + \\frac{n}{1-\\epsilon} = \\frac{n(1-\\epsilon) + n\\epsilon}{\\epsilon(1-\\epsilon)}$$\n$$I(\\epsilon) = \\frac{n}{\\epsilon(1-\\epsilon)}$$\n\n**2. Jeffreys Prior for the Binomial Parameter**\n\nThe Jeffreys prior is a non-informative prior derived from the Fisher information, defined as $\\pi(\\epsilon) \\propto \\sqrt{I(\\epsilon)}$. It is designed to be invariant under reparameterization.\nUsing the derived Fisher information:\n$$\\pi(\\epsilon) \\propto \\sqrt{\\frac{n}{\\epsilon(1-\\epsilon)}} \\propto \\frac{1}{\\sqrt{\\epsilon(1-\\epsilon)}} = \\epsilon^{-1/2} (1-\\epsilon)^{-1/2}$$\nThis functional form is proportional to the probability density function of a Beta distribution, $\\mathrm{Beta}(\\epsilon; \\alpha, \\beta) \\propto \\epsilon^{\\alpha-1} (1-\\epsilon)^{\\beta-1}$. By comparing the exponents, we find:\n$$\\alpha - 1 = -1/2 \\implies \\alpha = 1/2$$\n$$\\beta - 1 = -1/2 \\implies \\beta = 1/2$$\nThus, the Jeffreys prior for the binomial parameter $\\epsilon$ is a $\\mathrm{Beta}(1/2, 1/2)$ distribution.\n\n**3. Bayesian Posterior and Credible Interval**\n\nThe posterior distribution $p(\\epsilon \\mid k,n)$ is obtained by applying Bayes' theorem, which states that the posterior is proportional to the product of the likelihood and the prior:\n$$p(\\epsilon \\mid k,n) \\propto p(k \\mid n,\\epsilon) \\pi(\\epsilon)$$\nSubstituting the binomial likelihood and the Jeffreys prior:\n$$p(\\epsilon \\mid k,n) \\propto \\left(\\epsilon^k (1-\\epsilon)^{n-k}\\right) \\cdot \\left(\\epsilon^{1/2-1} (1-\\epsilon)^{1/2-1}\\right)$$\n$$p(\\epsilon \\mid k,n) \\propto \\epsilon^{k+1/2-1} (1-\\epsilon)^{n-k+1/2-1}$$\nThis is the kernel of a Beta distribution with updated parameters. The posterior distribution for $\\epsilon$ is:\n$$p(\\epsilon \\mid k,n) = \\mathrm{Beta}(\\epsilon; k+1/2, n-k+1/2)$$\nThe equal-tailed Bayesian credible interval at a nominal level of $1-\\alpha$ is the interval $[\\epsilon_L, \\epsilon_U]$ defined by the $\\alpha/2$ and $1-\\alpha/2$ quantiles of this posterior distribution.\n\n**4. Clopper–Pearson Confidence Interval**\n\nThe Clopper–Pearson (CP) interval is a frequentist method that guarantees the coverage probability is at least $1-\\alpha$ for all possible values of $\\epsilon$. It is constructed by inverting a family of two-sided binomial tests. For an observed number of successes $k$, the interval $[\\epsilon_L, \\epsilon_U]$ is found by solving the following equations for $\\epsilon$:\n$$ \\text{For the lower bound } \\epsilon_L: \\quad P(X \\ge k \\mid n, \\epsilon_L) = \\sum_{j=k}^{n} \\binom{n}{j} \\epsilon_L^j (1-\\epsilon_L)^{n-j} = \\frac{\\alpha}{2} $$\n$$ \\text{For the upper bound } \\epsilon_U: \\quad P(X \\le k \\mid n, \\epsilon_U) = \\sum_{j=0}^{k} \\binom{n}{j} \\epsilon_U^j (1-\\epsilon_U)^{n-j} = \\frac{\\alpha}{2} $$\nThese equations can be expressed in terms of quantiles of the Beta distribution. The lower bound $\\epsilon_L$ is the $\\alpha/2$ quantile of a $\\mathrm{Beta}(k, n-k+1)$ distribution. The upper bound $\\epsilon_U$ is the $1-\\alpha/2$ quantile of a $\\mathrm{Beta}(k+1, n-k)$ distribution. For the edge cases, when $k=0$, $\\epsilon_L = 0$, and when $k=n$, $\\epsilon_U = 1$.\n\n### Computational Strategy\n\nThe implementation will follow these theoretical results to compute and compare the two interval types.\n\n1.  **Interval Calculation**: For a given test case $(n, \\alpha)$, we first iterate through all possible numbers of successes $k \\in \\{0, 1, \\dots, n\\}$. For each $k$, we compute the Bayesian and Clopper-Pearson intervals.\n    *   **Bayesian Interval**: The endpoints are the $\\alpha/2$ and $1-\\alpha/2$ quantiles of the posterior distribution $\\mathrm{Beta}(k+0.5, n-k+0.5)$.\n    *   **Clopper–Pearson Interval**: The endpoints are the $\\alpha/2$ quantile of $\\mathrm{Beta}(k, n-k+1)$ and the $1-\\alpha/2$ quantile of $\\mathrm{Beta}(k+1, n-k)$, with special handling for $k=0$ (lower bound is $0$) and $k=n$ (upper bound is $1$).\n\n2.  **Coverage Evaluation**: For each true efficiency value $\\epsilon$ in the specified grid, we calculate the frequentist coverage of each interval type. The coverage $C(\\epsilon)$ is the probability that the random interval $[L(k), U(k)]$ contains the true $\\epsilon$. This is computed by summing the binomial probabilities $\\mathrm{Bin}(k; n, \\epsilon)$ for all outcomes $k$ where the true $\\epsilon$ falls within the computed interval for that $k$:\n    $$C(\\epsilon) = \\sum_{k=0}^{n} \\mathbb{I}\\{\\epsilon \\in [L(k), U(k)]\\} \\cdot \\mathrm{Bin}(k; n, \\epsilon)$$\n    where $\\mathbb{I}\\{\\cdot\\}$ is the indicator function.\n\n3.  **Metrics Calculation**: After computing the coverage $C(\\epsilon_i)$ for all $M$ points in the grid, we calculate two summary metrics for each interval type:\n    *   **Worst-Case Deficit**: The minimum difference between the actual coverage and the nominal coverage $1-\\alpha$ over the grid: $\\min_{\\epsilon_i} (C(\\epsilon_i) - (1-\\alpha))$. A negative value indicates under-coverage.\n    *   **Mean Absolute Coverage Error**: The average absolute deviation of the coverage from the nominal level: $\\frac{1}{M}\\sum_{i=1}^{M} |C(\\epsilon_i) - (1-\\alpha)|$.\n\nThis procedure is repeated for each of the three test cases defined in the problem. The final program will aggregate and print the four computed metrics for each test case.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import beta, binom\n\ndef compute_metrics(n: int, alpha: float, epsilon_grid: np.ndarray):\n    \"\"\"\n    Computes and evaluates Bayesian and Clopper-Pearson intervals.\n\n    Args:\n        n: The number of trials.\n        alpha: The significance level.\n        epsilon_grid: A grid of true efficiency values to evaluate coverage.\n\n    Returns:\n        A list containing four floats:\n        [worst-case deficit for Bayesian, worst-case deficit for CP,\n         mean absolute error for Bayesian, mean absolute error for CP].\n    \"\"\"\n    ks = np.arange(n + 1)\n    \n    # 1. Compute intervals for all possible outcomes k\n    bayesian_intervals = np.zeros((n + 1, 2))\n    cp_intervals = np.zeros((n + 1, 2))\n\n    for k in ks:\n        # Bayesian interval with Jeffreys prior (Posterior is Beta(k+0.5, n-k+0.5))\n        post_a = k + 0.5\n        post_b = n - k + 0.5\n        bayesian_intervals[k, :] = beta.ppf([alpha / 2, 1 - alpha / 2], post_a, post_b)\n\n        # Clopper-Pearson interval\n        cp_lower = 0.0 if k == 0 else beta.ppf(alpha / 2, k, n - k + 1)\n        cp_upper = 1.0 if k == n else beta.ppf(1 - alpha / 2, k + 1, n - k)\n        cp_intervals[k, :] = [cp_lower, cp_upper]\n\n    # 2. Compute frequentist coverage for each epsilon in the grid\n    nominal_coverage = 1.0 - alpha\n    bayesian_coverages = []\n    cp_coverages = []\n\n    for eps in epsilon_grid:\n        # Binomial probabilities for all k given the true epsilon\n        binom_probs = binom.pmf(ks, n, eps)\n        \n        # Check which k's result in an interval that covers the true epsilon\n        # Add a small tolerance for floating point comparisons at the boundary\n        is_covered_bayes = (bayesian_intervals[:, 0] = eps)  (eps = bayesian_intervals[:, 1])\n        is_covered_cp = (cp_intervals[:, 0] = eps)  (eps = cp_intervals[:, 1])\n        \n        # Sum the probabilities of the k's that produce a covering interval\n        bayesian_coverages.append(np.sum(binom_probs[is_covered_bayes]))\n        cp_coverages.append(np.sum(binom_probs[is_covered_cp]))\n    \n    bayesian_coverages = np.array(bayesian_coverages)\n    cp_coverages = np.array(cp_coverages)\n    \n    # 3. Compute summary metrics\n    # Worst-case deficit relative to nominal coverage\n    bayesian_deficit = np.min(bayesian_coverages - nominal_coverage)\n    cp_deficit = np.min(cp_coverages - nominal_coverage)\n    \n    # Mean absolute coverage error\n    bayesian_mae = np.mean(np.abs(bayesian_coverages - nominal_coverage))\n    cp_mae = np.mean(np.abs(cp_coverages - nominal_coverage))\n\n    return [bayesian_deficit, cp_deficit, bayesian_mae, cp_mae]\n\ndef solve():\n    \"\"\"\n    Main function to run the specified test cases and print results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {'n': 10, 'alpha': 0.1, 'grid_points': 21},\n        {'n': 20, 'alpha': 0.1, 'grid_points': 11},\n        {'n': 20, 'alpha': 0.32, 'grid_points': 21},\n    ]\n\n    all_results = []\n    for case in test_cases:\n        n = case['n']\n        alpha = case['alpha']\n        epsilon_grid = np.linspace(0.0, 1.0, case['grid_points'])\n        \n        result = compute_metrics(n, alpha, epsilon_grid)\n        all_results.append(result)\n\n    # Format the final output string to match the problem specification\n    # e.g., [[a1,b1,c1,d1],[a2,b2,c2,d2],[a3,b3,c3,d3]] with no spaces.\n    case_strings = []\n    for res in all_results:\n        case_strings.append(f\"[{','.join(map(str, res))}]\")\n    \n    final_output = f\"[{','.join(case_strings)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "3506258"}]}