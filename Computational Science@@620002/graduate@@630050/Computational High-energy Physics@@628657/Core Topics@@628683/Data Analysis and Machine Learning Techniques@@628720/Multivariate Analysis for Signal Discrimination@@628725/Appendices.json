{"hands_on_practices": [{"introduction": "Before diving into complex machine learning models, it is crucial to understand the fundamental limits of discrimination. The Receiver Operating Characteristic (ROC) curve quantifies classifier performance, and for simple cases, its ideal shape can be derived analytically. This practice [@problem_id:3524097] guides you through a first-principles derivation of the optimal ROC curve for the canonical problem of separating two Gaussian distributions, grounding the abstract power of the Neyman-Pearson lemma in a concrete formula that reveals how separation and resolution govern distinguishability.", "problem": "In a binary classification scenario for signal discrimination in computational high-energy physics, consider a one-dimensional feature $x$ measured from events that belong to either a signal class $S$ or a background class $B$. The class-conditional densities are Gaussian with equal variances, specifically $p(x \\mid S) = \\mathcal{N}(\\mu_{S}, \\sigma^{2})$ and $p(x \\mid B) = \\mathcal{N}(\\mu_{B}, \\sigma^{2})$, where $\\mu_{S}$, $\\mu_{B}$, and $\\sigma > 0$ are finite constants and $\\mu_{S} > \\mu_{B}$. Using the Neyman–Pearson lemma and the Likelihood-Ratio Test (LRT) as the optimal detector under a false-positive constraint, construct the Receiver Operating Characteristic (ROC) curve by analytically integrating the Gaussian distributions over the decision region induced by a likelihood-ratio threshold. Eliminate the threshold to obtain a closed-form functional relationship that expresses the true positive rate as a function of the false positive rate for all admissible thresholds. Your final answer must be a single closed-form analytical expression in terms of $\\mu_{S}$, $\\mu_{B}$, and $\\sigma$, and may involve the cumulative distribution function of the standard normal distribution. Do not provide any numerical approximation; an exact expression is required.", "solution": "The user wants to derive the Receiver Operating Characteristic (ROC) curve for a binary classification problem with Gaussian class-conditional densities.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n-   **Binary Classification:** Signal class $S$ versus Background class $B$.\n-   **Feature:** A one-dimensional feature $x$.\n-   **Class-Conditional Densities:**\n    -   Signal: $p(x \\mid S) = \\mathcal{N}(\\mu_{S}, \\sigma^{2})$\n    -   Background: $p(x \\mid B) = \\mathcal{N}(\\mu_{B}, \\sigma^{2})$\n-   **Parameters:** $\\mu_{S}$, $\\mu_{B}$, and $\\sigma$ are finite constants.\n-   **Constraints:** $\\sigma > 0$ and $\\mu_{S} > \\mu_{B}$.\n-   **Methodology:** Neyman–Pearson lemma and Likelihood-Ratio Test (LRT).\n-   **Objective:** Construct the ROC curve by finding a closed-form functional relationship between the True Positive Rate (TPR) and the False Positive Rate (FPR). The final expression should be in terms of $\\mu_{S}$, $\\mu_{B}$, and $\\sigma$, and may involve the cumulative distribution function of the standard normal distribution.\n\n**Step 2: Validate Using Extracted Givens**\n-   **Scientific Grounding:** The problem is a classic and fundamental exercise in statistical signal processing and decision theory, which is a core component of data analysis in high-energy physics. The use of Gaussian distributions and the Neyman-Pearson lemma is standard and scientifically sound.\n-   **Well-Posedness:** The problem is well-posed. The distributions are fully specified, and the condition $\\mu_{S} > \\mu_{B}$ ensures a non-trivial discrimination problem. The goal is clear and a unique analytical solution exists.\n-   **Objectivity:** The problem is stated in precise, objective mathematical language.\n-   **Flaw Assessment:** The problem does not violate any of the invalidity criteria. It is scientifically sound, formalizable, complete, feasible, and well-posed.\n\n**Step 3: Verdict and Action**\nThe problem is **valid**. A solution will be provided.\n\n### Solution Derivation\n\nThe problem requires the derivation of the Receiver Operating Characteristic (ROC) curve, which describes the performance of a binary classifier. The ROC curve is a plot of the True Positive Rate (TPR) versus the False Positive Rate (FPR).\n\nAccording to the Neyman–Pearson lemma, the most powerful test for discriminating between two simple hypotheses, $H_B: x \\sim p(x \\mid B)$ (background) and $H_S: x \\sim p(x \\mid S)$ (signal), is the Likelihood-Ratio Test (LRT). The likelihood ratio $\\Lambda(x)$ is defined as:\n$$\n\\Lambda(x) = \\frac{p(x \\mid S)}{p(x \\mid B)}\n$$\nThe decision rule is to classify an observation $x$ as signal if $\\Lambda(x) > \\eta$, where $\\eta$ is a decision threshold. The value of $\\eta$ determines a point on the ROC curve.\n\nGiven the class-conditional densities are Gaussian distributions with equal variance:\n$$\np(x \\mid S) = \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left(-\\frac{(x - \\mu_{S})^2}{2\\sigma^2}\\right)\n$$\n$$\np(x \\mid B) = \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left(-\\frac{(x - \\mu_{B})^2}{2\\sigma^2}\\right)\n$$\nThe likelihood ratio is:\n$$\n\\Lambda(x) = \\frac{\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left(-\\frac{(x - \\mu_{S})^2}{2\\sigma^2}\\right)}{\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left(-\\frac{(x - \\mu_{B})^2}{2\\sigma^2}\\right)} = \\exp\\left( \\frac{(x - \\mu_{B})^2 - (x - \\mu_{S})^2}{2\\sigma^2} \\right)\n$$\nSince the natural logarithm, $\\ln$, is a monotonically increasing function, the inequality $\\Lambda(x) > \\eta$ is equivalent to $\\ln(\\Lambda(x)) > \\ln(\\eta)$. Taking the logarithm of the likelihood ratio simplifies the expression:\n$$\n\\ln(\\Lambda(x)) = \\frac{(x^2 - 2x\\mu_{B} + \\mu_{B}^2) - (x^2 - 2x\\mu_{S} + \\mu_{S}^2)}{2\\sigma^2} = \\frac{2x(\\mu_{S} - \\mu_{B}) - (\\mu_{S}^2 - \\mu_{B}^2)}{2\\sigma^2}\n$$\n$$\n\\ln(\\Lambda(x)) = \\frac{\\mu_{S} - \\mu_{B}}{\\sigma^2}x - \\frac{\\mu_{S}^2 - \\mu_{B}^2}{2\\sigma^2}\n$$\nThe decision rule $\\ln(\\Lambda(x)) > \\ln(\\eta)$ becomes:\n$$\n\\frac{\\mu_{S} - \\mu_{B}}{\\sigma^2}x > \\ln(\\eta) + \\frac{\\mu_{S}^2 - \\mu_{B}^2}{2\\sigma^2}\n$$\nGiven the problem states that $\\mu_{S} > \\mu_{B}$, the term $\\frac{\\mu_{S} - \\mu_{B}}{\\sigma^2}$ is positive. Therefore, the inequality is equivalent to classifying an event as signal if $x$ is greater than some threshold, which we will denote as $x_{th}$:\n$$\nx > x_{th} \\quad \\text{where} \\quad x_{th} = \\frac{\\sigma^2}{\\mu_{S} - \\mu_{B}}\\left(\\ln(\\eta) + \\frac{\\mu_{S}^2 - \\mu_{B}^2}{2\\sigma^2}\\right)\n$$\nThe decision region for signal is $\\Omega_{S} = \\{x \\in \\mathbb{R} \\mid x > x_{th}\\}$. Varying the LRT threshold $\\eta$ from $0$ to $\\infty$ corresponds to varying $x_{th}$ from $-\\infty$ to $\\infty$.\n\nThe TPR and FPR are defined as the probabilities of a signal or background event, respectively, falling into the signal decision region $\\Omega_S$:\n$$\n\\text{TPR}(x_{th}) = P(x \\in \\Omega_S \\mid S) = \\int_{x_{th}}^{\\infty} p(x \\mid S) dx\n$$\n$$\n\\text{FPR}(x_{th}) = P(x \\in \\Omega_S \\mid B) = \\int_{x_{th}}^{\\infty} p(x \\mid B) dx\n$$\nThese integrals can be expressed in terms of the cumulative distribution function (CDF) of the standard normal distribution, $\\mathcal{N}(0, 1)$, which we denote as $\\Phi(z) = \\int_{-\\infty}^{z} \\frac{1}{\\sqrt{2\\pi}} \\exp(-t^2/2) dt$.\nFor a general Gaussian variable $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$, the probability $P(X > a)$ is given by:\n$$\nP(X > a) = P\\left(\\frac{X - \\mu}{\\sigma} > \\frac{a - \\mu}{\\sigma}\\right) = 1 - \\Phi\\left(\\frac{a - \\mu}{\\sigma}\\right)\n$$\nUsing the symmetry property of the standard normal PDF, $1 - \\Phi(z) = \\Phi(-z)$, we have:\n$$\nP(X > a) = \\Phi\\left(-\\frac{a - \\mu}{\\sigma}\\right) = \\Phi\\left(\\frac{\\mu - a}{\\sigma}\\right)\n$$\nApplying this to our TPR and FPR expressions:\n$$\n\\text{TPR}(x_{th}) = \\Phi\\left(\\frac{\\mu_{S} - x_{th}}{\\sigma}\\right)\n$$\n$$\n\\text{FPR}(x_{th}) = \\Phi\\left(\\frac{\\mu_{B} - x_{th}}{\\sigma}\\right)\n$$\nThese two equations provide a parametric representation of the ROC curve, with $x_{th}$ as the parameter. To obtain the desired closed-form relationship expressing TPR as a function of FPR, we must eliminate $x_{th}$.\nFrom the FPR equation, we use the inverse standard normal CDF (the quantile function or probit function), denoted as $\\Phi^{-1}$, to solve for $x_{th}$:\n$$\n\\Phi^{-1}(\\text{FPR}) = \\frac{\\mu_{B} - x_{th}}{\\sigma}\n$$\n$$\n\\sigma \\Phi^{-1}(\\text{FPR}) = \\mu_{B} - x_{th}\n$$\n$$\nx_{th} = \\mu_{B} - \\sigma \\Phi^{-1}(\\text{FPR})\n$$\nNow, substitute this expression for $x_{th}$ into the TPR equation:\n$$\n\\text{TPR} = \\Phi\\left(\\frac{\\mu_{S} - (\\mu_{B} - \\sigma \\Phi^{-1}(\\text{FPR}))}{\\sigma}\\right)\n$$\nSimplifying the argument of $\\Phi$:\n$$\n\\text{TPR} = \\Phi\\left(\\frac{\\mu_{S} - \\mu_{B} + \\sigma \\Phi^{-1}(\\text{FPR})}{\\sigma}\\right)\n$$\n$$\n\\text{TPR} = \\Phi\\left(\\frac{\\mu_{S} - \\mu_{B}}{\\sigma} + \\Phi^{-1}(\\text{FPR})\\right)\n$$\nThis is the final closed-form analytical expression for the ROC curve, relating the true positive rate to the false positive rate. The expression depends on the specified parameters $\\mu_S$, $\\mu_B$, and $\\sigma$, and involves the standard normal CDF $\\Phi$ and its inverse $\\Phi^{-1}$. The term $\\frac{\\mu_S - \\mu_B}{\\sigma}$ is a measure of the separation between the two distributions in units of their standard deviation.", "answer": "$$\n\\boxed{\\Phi\\left(\\frac{\\mu_{S} - \\mu_{B}}{\\sigma} + \\Phi^{-1}(\\text{FPR})\\right)}\n$$", "id": "3524097"}, {"introduction": "Theoretical models provide benchmarks, but real-world high-energy physics data is shaped by detector effects that smear the underlying physical reality. This exercise [@problem_id:3524159] bridges the gap between theory and experiment by asking you to construct a discriminant within a toy model that includes a detector transfer function. You will practice the essential skill of numerically solving the convolution integral that relates a latent physical variable to a measured observable, a core task in modern data analysis.", "problem": "Consider a simplified binary hypothesis test for High-Energy Physics (HEP) multivariate signal discrimination within a toy model of a two-to-two ($2 \\to 2$) scattering process in the center-of-mass (CoM) frame. A single measured scalar observable $x$ represents a calorimetric energy measurement of one final-state particle. The true but latent energy is $E$. The detector response is modeled by a Gaussian transfer function, and the parton-level energy distribution depends on the hypothesis $H \\in \\{S,B\\}$, where $S$ denotes the signal hypothesis and $B$ denotes the background hypothesis. All energies must be treated in $\\mathrm{GeV}$, and probability density functions (PDFs) are to be treated with respect to energy in $\\mathrm{GeV}$.\n\nStarting from the law of total probability and the definition of a transfer function, the conditional likelihood of observing $x$ under hypothesis $H$ is given by the integral\n$$\nP(x \\mid H) \\;=\\; \\int_{0}^{E_{\\max}} W(x \\mid E)\\, f_H(E)\\, dE,\n$$\nwhere $W(x \\mid E)$ is the detector transfer function, and $f_H(E)$ is the normalized parton-level energy PDF under hypothesis $H$.\n\nYou must use the following specific and physically plausible modeling choices:\n- Energy support: $E \\in [0, E_{\\max}]$ with $E_{\\max} = 1000\\,\\mathrm{GeV}$.\n- Detector transfer function (Gaussian): for any real $x$ and $E \\in [0,E_{\\max}]$,\n$$\nW(x \\mid E) \\;=\\; \\frac{1}{\\sqrt{2\\pi}\\,\\sigma_{\\text{meas}}}\\,\\exp\\!\\left(-\\frac{(x - E)^2}{2\\,\\sigma_{\\text{meas}}^2}\\right), \\quad \\sigma_{\\text{meas}} = 20\\,\\mathrm{GeV}.\n$$\n- Signal prior (parton-level) PDF: a truncated Gaussian shape on $[0,E_{\\max}]$ with mean $\\mu_S = 500\\,\\mathrm{GeV}$ and width parameter $\\tau_S = 60\\,\\mathrm{GeV}$,\n$$\nf_S(E) \\;=\\; \\frac{\\exp\\!\\left(-\\frac{(E - \\mu_S)^2}{2\\,\\tau_S^2}\\right)}{Z_S}, \\quad Z_S \\;=\\; \\int_{0}^{E_{\\max}} \\exp\\!\\left(-\\frac{(E - \\mu_S)^2}{2\\,\\tau_S^2}\\right)\\, dE.\n$$\n- Background prior (parton-level) PDF: a falling power-law on $[0,E_{\\max}]$ with scale $E_0 = 200\\,\\mathrm{GeV}$ and index $n = 3.5$,\n$$\nf_B(E) \\;=\\; \\frac{\\left(1 + \\frac{E}{E_0}\\right)^{-n}}{Z_B}, \\quad Z_B \\;=\\; \\int_{0}^{E_{\\max}} \\left(1 + \\frac{E}{E_0}\\right)^{-n} dE.\n$$\n\nDefine the optimal equal-prior discriminant $D(x)$ by the Neyman–Pearson (NP) construction for two simple hypotheses as\n$$\nD(x) \\;=\\; \\frac{P(x \\mid S)}{P(x \\mid S) + P(x \\mid B)}.\n$$\n\nTasks:\n- Set up the integral for $P(x \\mid H)$ explicitly using the provided $W(x \\mid E)$ and $f_H(E)$.\n- Compute numerical approximations for $P(x \\mid S)$, $P(x \\mid B)$, and $D(x)$ at each specified $x$ using one-dimensional numerical integration of the energy $E$ over $[0, E_{\\max}]$. The normalizations $Z_S$ and $Z_B$ must be computed consistently with the same numerical accuracy standard as used for the main likelihood integrals.\n- All intermediate and final computations must be consistent with the units stated; however, the final reported values are pure numbers corresponding to PDF values in $\\mathrm{GeV}^{-1}$ for $P(x \\mid H)$ and dimensionless for $D(x)$.\n- Report each requested numeric value rounded to $6$ decimal places.\n\nTest suite:\n- Use the following $x$ values (in $\\mathrm{GeV}$): $x \\in \\{500.0,\\, 50.0,\\, 950.0,\\, 0.0,\\, 1000.0\\}$.\n\nFinal output format:\n- Your program should produce a single line of output containing a comma-separated flat list enclosed in square brackets, in the order\n$$\n[\\;P(x_1\\mid S),\\;P(x_1\\mid B),\\;D(x_1),\\;P(x_2\\mid S),\\;P(x_2\\mid B),\\;D(x_2),\\;\\ldots\\;],\n$$\nwith each float rounded to $6$ decimal places as specified. No additional text must be printed.\n- The program must be fully self-contained, require no user input, and use only the specified numerical libraries.", "solution": "The problem requires the computation of conditional likelihoods and an optimal discriminant for a simplified signal-versus-background hypothesis test in high-energy physics. The core of the problem lies in relating a true, latent energy distribution to a measured observable via a detector response model. This relationship is mathematically described by a convolution integral.\n\nThe fundamental principle is the law of total probability. The probability density of observing a measured energy $x$ given a hypothesis $H$ (where $H$ is either Signal, $S$, or Background, $B$), is obtained by marginalizing over all possible true energy values $E$. This is expressed through the integral:\n$$\nP(x \\mid H) = \\int P(x, E \\mid H) \\, dE\n$$\nUsing the definition of conditional probability, $P(x, E \\mid H) = P(x \\mid E, H) f_H(E)$, where $f_H(E)$ is the probability density of the true energy $E$ under hypothesis $H$. The detector response, $W(x \\mid E)$, is assumed to be independent of the underlying physics hypothesis, so $P(x \\mid E, H) = W(x \\mid E)$. The integral thus becomes:\n$$\nP(x \\mid H) = \\int_{0}^{E_{\\max}} W(x \\mid E) f_H(E) \\, dE\n$$\nThe integration is performed over the allowed range of true energies, $E \\in [0, E_{\\max}]$.\n\nThe problem provides specific functional forms for all components:\nThe energy range is defined by $E_{\\max} = 1000\\,\\mathrm{GeV}$.\nThe detector transfer function, $W(x \\mid E)$, models the measurement resolution as a Gaussian distribution centered at the true energy $E$:\n$$\nW(x \\mid E) = \\frac{1}{\\sqrt{2\\pi}\\,\\sigma_{\\text{meas}}}\\,\\exp\\!\\left(-\\frac{(x - E)^2}{2\\,\\sigma_{\\text{meas}}^2}\\right), \\quad \\text{with } \\sigma_{\\text{meas}} = 20\\,\\mathrm{GeV}\n$$\nThe parton-level energy PDFs, $f_H(E)$, must be normalized to unity over the domain $[0, E_{\\max}]$.\n\nFor the Signal hypothesis ($H=S$):\nThe PDF $f_S(E)$ is a truncated Gaussian with mean $\\mu_S = 500\\,\\mathrm{GeV}$ and width $\\tau_S = 60\\,\\mathrm{GeV}$.\n$$\nf_S(E) = \\frac{1}{Z_S} \\exp\\!\\left(-\\frac{(E - \\mu_S)^2}{2\\,\\tau_S^2}\\right)\n$$\nwhere the normalization constant $Z_S$ is given by the integral of the unnormalized distribution over the energy support:\n$$\nZ_S = \\int_{0}^{E_{\\max}} \\exp\\!\\left(-\\frac{(E - \\mu_S)^2}{2\\,\\tau_S^2}\\right) dE\n$$\n\nFor the Background hypothesis ($H=B$):\nThe PDF $f_B(E)$ is a falling power-law distribution with scale $E_0 = 200\\,\\mathrm{GeV}$ and index $n = 3.5$.\n$$\nf_B(E) = \\frac{1}{Z_B} \\left(1 + \\frac{E}{E_0}\\right)^{-n}\n$$\nThe corresponding normalization constant $Z_B$ is:\n$$\nZ_B = \\int_{0}^{E_{\\max}} \\left(1 + \\frac{E}{E_0}\\right)^{-n} dE\n$$\n\nSubstituting these definitions into the main convolution integral, we obtain the explicit expressions for the conditional likelihoods:\nFor the Signal:\n$$\nP(x \\mid S) = \\int_{0}^{1000} \\left[ \\frac{1}{\\sqrt{2\\pi}\\,(20)}\\exp\\!\\left(-\\frac{(x - E)^2}{2\\,(20)^2}\\right) \\right] \\left[ \\frac{1}{Z_S}\\exp\\!\\left(-\\frac{(E - 500)^2}{2\\,(60)^2}\\right) \\right] dE\n$$\nFor the Background:\n$$\nP(x \\mid B) = \\int_{0}^{1000} \\left[ \\frac{1}{\\sqrt{2\\pi}\\,(20)}\\exp\\!\\left(-\\frac{(x - E)^2}{2\\,(20)^2}\\right) \\right] \\left[ \\frac{1}{Z_B}\\left(1 + \\frac{E}{200}\\right)^{-3.5} \\right] dE\n$$\nThese integrals, including those for $Z_S$ and $Z_B$, do not all have simple closed-form analytical solutions and must be evaluated numerically. The problem specifies that a consistent numerical approach be used for all integrals. A robust method for this task is numerical quadrature, as provided by the `scipy.integrate.quad` function in Python, which implements a high-precision adaptive integration algorithm.\n\nThe computational procedure is as follows:\n1.  Numerically compute the normalization constants $Z_S$ and $Z_B$ by integrating the unnormalized PDF functions from $E=0$ to $E=E_{\\max}$.\n2.  For each specified value of the observable $x$, numerically compute the integrals for $P(x \\mid S)$ and $P(x \\mid B)$.\n3.  From these likelihoods, calculate the Neyman-Pearson discriminant $D(x)$, which for equal prior probabilities of signal and background, is equivalent to the posterior probability of the signal hypothesis:\n$$\nD(x) = \\frac{P(x \\mid S)}{P(x \\mid S) + P(x \\mid B)}\n$$\nThis quantity provides an optimal metric for discriminating between the two hypotheses, with values approaching $1$ indicating a signal-like event and values approaching $0$ indicating a background-like event. The requested values are computed for each $x$ in the test suite $\\{500.0, 50.0, 950.0, 0.0, 1000.0\\}$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.integrate import quad\n\ndef solve():\n    \"\"\"\n    Computes likelihoods and discriminant for a HEP hypothesis test.\n    \"\"\"\n    # Define the physical and model constants from the problem statement.\n    E_max = 1000.0  # GeV\n    sigma_meas = 20.0  # GeV\n    mu_S = 500.0  # GeV\n    tau_S = 60.0  # GeV\n    E_0 = 200.0  # GeV\n    n = 3.5\n\n    # Define the test cases from the problem statement.\n    test_cases = [500.0, 50.0, 950.0, 0.0, 1000.0]\n\n    # --- Step 1: Define unnormalized parton-level PDFs ---\n\n    def unnormalized_fs(E):\n        \"\"\"Unnormalized signal PDF.\"\"\"\n        return np.exp(-(E - mu_S)**2 / (2 * tau_S**2))\n\n    def unnormalized_fb(E):\n        \"\"\"Unnormalized background PDF.\"\"\"\n        return (1 + E / E_0)**(-n)\n\n    # --- Step 2: Compute normalization constants Z_S and Z_B ---\n    # The integrals are computed numerically for consistency as required.\n    Z_S, _ = quad(unnormalized_fs, 0, E_max)\n    Z_B, _ = quad(unnormalized_fb, 0, E_max)\n\n    # --- Step 3: Define normalized PDFs and the transfer function ---\n\n    def f_S(E):\n        \"\"\"Normalized signal PDF f_S(E).\"\"\"\n        return unnormalized_fs(E) / Z_S\n\n    def f_B(E):\n        \"\"\"Normalized background PDF f_B(E).\"\"\"\n        return unnormalized_fb(E) / Z_B\n\n    def W(x, E):\n        \"\"\"Detector transfer function W(x|E).\"\"\"\n        return (1 / (np.sqrt(2 * np.pi) * sigma_meas)) * np.exp(-(x - E)**2 / (2 * sigma_meas**2))\n\n    # --- Step 4: Compute likelihoods and discriminant for each x ---\n    \n    results = []\n    for x in test_cases:\n        # Define the integrands for the convolution integrals P(x|H).\n        # The integrand is the product of the transfer function and the prior PDF.\n        def integrand_S(E):\n            return W(x, E) * f_S(E)\n\n        def integrand_B(E):\n            return W(x, E) * f_B(E)\n\n        # Perform numerical integration to find the likelihoods P(x|S) and P(x|B).\n        P_x_S, _ = quad(integrand_S, 0, E_max)\n        P_x_B, _ = quad(integrand_B, 0, E_max)\n\n        # Compute the optimal discriminant D(x).\n        # A check for division by zero is prudent, though unlikely in this scenario\n        # as the likelihoods are integrals of non-negative functions.\n        denominator = P_x_S + P_x_B\n        if denominator == 0:\n            # This case corresponds to an observed x that is impossible under both hypotheses.\n            # The discriminant is undefined; a neutral value of 0.5 is a reasonable default.\n            D_x = 0.5\n        else:\n            D_x = P_x_S / denominator\n        \n        results.extend([P_x_S, P_x_B, D_x])\n\n    # --- Step 5: Format and print the final output ---\n\n    # Format each number to 6 decimal places as a string.\n    formatted_results = [f\"{val:.6f}\" for val in results]\n    \n    # Print the final result in the exact required format.\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3524159"}, {"introduction": "A powerful discriminant is only the first step; for many physics applications, a classifier's output must be a reliable probability estimate. This practice [@problem_id:3524128] addresses the crucial task of calibration, transforming a raw classifier score into a trustworthy probability. You will implement isotonic regression using the Pool Adjacent Violators (PAV) algorithm, a robust non-parametric method to enforce monotonicity and improve the quantitative interpretation of your model's predictions.", "problem": "You are given binned validation data from a multivariate classifier used for signal discrimination in computational high-energy physics. Each bin corresponds to a contiguous range of classifier scores and contains a number of signal events and background events. Assume the classifier produces a score interpreted as an uncalibrated probability estimate, and you wish to calibrate it to be a proper probability by solving a monotone regression problem via isotonic regression, then quantify calibration quality using the Brier score.\n\nBegin from the following foundations:\n- The empirical risk minimization principle under squared loss prescribes minimizing the weighted mean squared error of predictions relative to targets. For binned data, with per-bin observed positive fraction, this becomes minimizing a weighted sum of squared deviations.\n- Isotonic regression is the projection of a vector onto the cone of non-decreasing sequences under a norm induced by weights, yielding a non-decreasing function estimate that minimizes the weighted squared loss.\n- The Brier score is the average of squared differences between predicted probabilities and binary outcomes.\n\nFormally, let there be $n$ bins indexed by $i \\in \\{1,\\dots,n\\}$. For each bin $i$, you are given:\n- A representative classifier score $x_i \\in [0,1]$ (assume bins are already ordered by increasing $x_i$).\n- The number of signal events $s_i \\in \\mathbb{N}$ and the number of background events $b_i \\in \\mathbb{N}$, with total $n_i = s_i + b_i$.\nDefine the observed positive rate $y_i = s_i / n_i$. Define weights $w_i = n_i$ for calibration.\n\nYour tasks are:\n1. Implement isotonic regression to solve the monotone regression problem\n$$\n\\min_{f_1,\\dots,f_n} \\sum_{i=1}^{n} w_i \\left(f_i - y_i\\right)^2 \\quad \\text{subject to} \\quad f_1 \\le f_2 \\le \\dots \\le f_n,\n$$\nproducing calibrated probabilities $f_i$ for each bin. Solve this exactly using a correct algorithm for isotonic regression under squared loss with weights (do not approximate).\n2. Compute the Brier score before calibration and after calibration for the binned validation data. For bin-level predictions, the event-level Brier score can be computed exactly using counts:\n$$\n\\text{Brier}(p) = \\frac{1}{\\sum_{i=1}^{n} n_i} \\sum_{i=1}^{n} \\left[ s_i \\left(p_i - 1\\right)^2 + b_i \\left(p_i - 0\\right)^2 \\right],\n$$\nwhere $p_i$ is the predicted probability for all events in bin $i$. Use $p_i = x_i$ for the pre-calibration Brier score, and $p_i = f_i$ for the post-calibration Brier score.\n3. Produce results for a test suite of four cases that probe different facets of the solution, including non-monotonic observed rates, already monotonic cases, extreme probabilities, and tied scores.\n\nUse the following test suite:\n- Case $1$ (non-monotonic observed rates requiring pooling):\n  - $x = [0.05, 0.15, 0.30, 0.45, 0.60]$\n  - $s = [5, 12, 10, 8, 25]$\n  - $b = [95, 88, 90, 92, 75]$\n- Case $2$ (already monotone and near-calibrated):\n  - $x = [0.02, 0.10, 0.25, 0.40, 0.55, 0.70]$\n  - $s = [2, 8, 20, 35, 55, 70]$\n  - $b = [98, 92, 80, 65, 45, 30]$\n- Case $3$ (extremes with bins having $y_i$ near $0$ and $1$):\n  - $x = [0.05, 0.20, 0.50, 0.80, 0.95]$\n  - $s = [0, 4, 25, 80, 95]$\n  - $b = [100, 96, 75, 20, 5]$\n- Case $4$ (tied scores with local non-monotonicity):\n  - $x = [0.10, 0.10, 0.30, 0.30, 0.30, 0.60]$\n  - $s = [12, 8, 25, 18, 20, 40]$\n  - $b = [88, 92, 75, 82, 80, 60]$\n\nAlgorithmic requirements:\n- Implement isotonic regression using the Pool Adjacent Violators (PAV) algorithm, where Pool Adjacent Violators (PAV) is the standard exact algorithm for isotonic regression under squared loss with weights.\n- Do not rely on external data or any machine learning libraries beyond the specified environment.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order $[\\text{pre}_1, \\text{post}_1, \\text{pre}_2, \\text{post}_2, \\text{pre}_3, \\text{post}_3, \\text{pre}_4, \\text{post}_4]$, where $\\text{pre}_k$ and $\\text{post}_k$ are the pre-calibration and post-calibration Brier scores for case $k$. Express each floating-point number rounded to six decimal places, and print no other text.", "solution": "The problem statement has been validated and is determined to be a valid, well-posed, and scientifically grounded problem.\n\n### Problem Validation\n\n#### Step 1: Extracted Givens\n- **Task**: Calibrate an uncalibrated classifier score using isotonic regression and evaluate the calibration quality using the Brier score.\n- **Data per bin $i$**:\n    - Representative classifier score: $x_i \\in [0,1]$, with bins ordered by increasing $x_i$.\n    - Number of signal events: $s_i \\in \\mathbb{N}$.\n    - Number of background events: $b_i \\in \\mathbb{N}$.\n- **Derived quantities**:\n    - Total events per bin: $n_i = s_i + b_i$.\n    - Observed positive rate (empirical probability): $y_i = s_i / n_i$.\n    - Weights for regression: $w_i = n_i$.\n- **Isotonic Regression Problem**: Find the calibrated probabilities $f_i$ by solving the constrained optimization problem:\n$$\n\\min_{f_1,\\dots,f_n} \\sum_{i=1}^{n} w_i \\left(f_i - y_i\\right)^2 \\quad \\text{subject to} \\quad f_1 \\le f_2 \\le \\dots \\le f_n\n$$\n- **Algorithm**: The Pool Adjacent Violators (PAV) algorithm must be used for the exact solution.\n- **Evaluation Metric**: The Brier score for a set of binned predictions $p_i$ is given by:\n$$\n\\text{Brier}(p) = \\frac{1}{\\sum_{i=1}^{n} n_i} \\sum_{i=1}^{n} \\left[ s_i \\left(p_i - 1\\right)^2 + b_i \\left(p_i - 0\\right)^2 \\right]\n$$\n- **Evaluation Scenarios**:\n    - Pre-calibration: Use the uncalibrated scores $p_i = x_i$.\n    - Post-calibration: Use the isotonic regression solution $p_i = f_i$.\n- **Test Suite**: Four specific cases are provided, defining the vectors $x$, $s$, and $b$ for each.\n    - Case $1$: `x = [0.05, 0.15, 0.30, 0.45, 0.60]`, `s = [5, 12, 10, 8, 25]`, `b = [95, 88, 90, 92, 75]`\n    - Case $2$: `x = [0.02, 0.10, 0.25, 0.40, 0.55, 0.70]`, `s = [2, 8, 20, 35, 55, 70]`, `b = [98, 92, 80, 65, 45, 30]`\n    - Case $3$: `x = [0.05, 0.20, 0.50, 0.80, 0.95]`, `s = [0, 4, 25, 80, 95]`, `b = [100, 96, 75, 20, 5]`\n    - Case $4$: `x = [0.10, 0.10, 0.30, 0.30, 0.30, 0.60]`, `s = [12, 8, 25, 18, 20, 40]`, `b = [88, 92, 75, 82, 80, 60]`\n\n#### Step 2: Validation Using Extracted Givens\nThe problem is scientifically grounded, well-posed, and objective.\n- **Scientific Soundness**: The concepts of classifier calibration, isotonic regression, the Pool Adjacent Violators (PAV) algorithm, and the Brier score are all standard, well-established topics in statistics and machine learning. The application to high-energy physics signal discrimination is a common and appropriate use case.\n- **Well-Posedness**: The problem defines a clear objective (calculate Brier scores before and after calibration), provides all necessary data and formulas, and specifies the exact algorithm (PAV) to be used. The PAV algorithm is known to yield a unique solution to the weighted least-squares isotonic regression problem.\n- **Completeness and Consistency**: The problem is fully self-contained. All variables ($x_i, s_i, b_i$), definitions ($y_i, w_i, n_i$), optimization criteria, evaluation metrics, and test data are explicitly provided. There are no internal contradictions.\n- **Realism**: The provided data (event counts and classifier scores) are realistic for binned data from a physics experiment.\n- **Clarity**: The problem is formulated in precise mathematical and algorithmic terms, leaving no room for ambiguity.\n\n#### Step 3: Verdict and Action\nThe problem is **valid**. A solution will be provided.\n\n### Solution Methodology\n\nThe objective is to calibrate the output of a classifier and quantify the improvement. The provided classifier scores $x_i$ for each bin $i$ are treated as initial, uncalibrated probability estimates. The calibration process aims to find a new set of probabilities $f_i$ that are monotonically non-decreasing with respect to the bin ordering and are \"closer\" to the true, observed event fractions in those bins.\n\n#### Step 1: Initial Data Processing\nFor each test case, we are given the representative scores $x_i$, signal counts $s_i$, and background counts $b_i$. From these, we first compute the necessary derived quantities for each bin $i \\in \\{1, \\dots, n\\}$:\n- The total number of events: $n_i = s_i + b_i$.\n- The observed signal fraction, which is the empirical probability: $y_i = s_i / n_i$. This will be $0$ if $n_i = 0$, though all test cases have $n_i > 0$.\n- The weight for each bin, which is the number of events: $w_i = n_i$.\n\n#### Step 2: Pre-Calibration Brier Score\nThe Brier score measures the mean squared error between predicted probabilities and actual outcomes. For binned data, it can be calculated directly from the counts. Before calibration, the predicted probability for each event in bin $i$ is taken as the representative score $x_i$. The pre-calibration Brier score, $\\text{Brier}_{\\text{pre}}$, is calculated as:\n$$\n\\text{Brier}_{\\text{pre}} = \\frac{1}{\\sum_{j=1}^{n} n_j} \\sum_{i=1}^{n} \\left[ s_i \\left(x_i - 1\\right)^2 + b_i \\left(x_i - 0\\right)^2 \\right]\n$$\nHere, the term $s_i(x_i - 1)^2$ represents the summed squared error for the $s_i$ signal events (true outcome $1$), and $b_i(x_i - 0)^2$ is the error for the $b_i$ background events (true outcome $0$).\n\n#### Step 3: Isotonic Regression using the Pool Adjacent Violators (PAV) Algorithm\nThe core of the calibration is to find a set of calibrated probabilities $f = (f_1, f_2, \\dots, f_n)$ that minimizes the weighted sum of squared errors with respect to the observed fractions $y_i$, under the constraint that the probabilities are monotonically non-decreasing. This is the isotonic regression problem:\n$$\n\\min_{f_1,\\dots,f_n} \\sum_{i=1}^{n} n_i \\left(f_i - y_i\\right)^2 \\quad \\text{subject to} \\quad f_1 \\le f_2 \\le \\dots \\le f_n\n$$\nThe PAV algorithm provides an exact solution to this problem. It operates by iteratively identifying and resolving \"violations\" of the monotonicity constraint, where $y_i > y_{i+1}$ in the sequence being considered. The resolution involves \"pooling\" adjacent blocks of bins and replacing their individual values with their weighted average until the entire sequence is non-decreasing.\n\nThe algorithm can be implemented as follows:\n1. Initialize an active set of blocks. Each block represents one or more contiguous original bins. A block can be represented by its total signal count $\\sum s_k$ and total event count $\\sum n_k$.\n2. Iterate through the bins $i = 1, \\dots, n$. For each bin, create a new block with its values $(s_i, n_i)$ and add it to the active set.\n3. After adding a new block, check if its value (fraction of signal events) is less than the value of the preceding block in the active set.\n   - The value of a block $j$ spanning original bins from $a$ to $b$ is $V_j = (\\sum_{k=a}^{b} s_k) / (\\sum_{k=a}^{b} n_k)$.\n   - If a violation $V_{\\text{last}} < V_{\\text{prev}}$ exists, merge the last two blocks. The new merged block has a total signal count equal to the sum of the signal counts of the merged blocks, and a total event count equal to the sum of their event counts.\n   - Repeat this merging process backward until the monotonicity constraint is satisfied for all blocks currently in the active set.\n4. After iterating through all bins, the active set contains the final set of pooled blocks.\n5. Construct the solution vector $f = (f_1, \\dots, f_n)$. For each block in the final active set, all the original bins constituting that block are assigned the same calibrated probability, which is the value of that block.\n\n#### Step 4: Post-Calibration Brier Score\nAfter obtaining the calibrated probabilities $f_i$ from the PAV algorithm, the post-calibration Brier score, $\\text{Brier}_{\\text{post}}$, is calculated using the same formula as before, but with the uncalibrated scores $x_i$ replaced by the calibrated probabilities $f_i$:\n$$\n\\text{Brier}_{\\text{post}} = \\frac{1}{\\sum_{j=1}^{n} n_j} \\sum_{i=1}^{n} \\left[ s_i \\left(f_i - 1\\right)^2 + b_i \\left(f_i - 0\\right)^2 \\right]\n$$\nBy construction, the isotonic regression solution $f$ minimizes the weighted mean squared error relative to the empirical probabilities $y_i$. Since the Brier score is a related measure of performance, we expect $\\text{Brier}_{\\text{post}} \\le \\text{Brier}_{\\text{pre}}$, with a strict inequality if the original scores $x_i$ were not well-calibrated or monotonic.\nThe procedure outlined above is applied to each of the $4$ test cases to generate the required pre- and post-calibration Brier scores.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the calibration and Brier score calculation for all test cases.\n    \"\"\"\n    \n    test_cases = [\n        {\n            \"x\": np.array([0.05, 0.15, 0.30, 0.45, 0.60]),\n            \"s\": np.array([5, 12, 10, 8, 25]),\n            \"b\": np.array([95, 88, 90, 92, 75]),\n        },\n        {\n            \"x\": np.array([0.02, 0.10, 0.25, 0.40, 0.55, 0.70]),\n            \"s\": np.array([2, 8, 20, 35, 55, 70]),\n            \"b\": np.array([98, 92, 80, 65, 45, 30]),\n        },\n        {\n            \"x\": np.array([0.05, 0.20, 0.50, 0.80, 0.95]),\n            \"s\": np.array([0, 4, 25, 80, 95]),\n            \"b\": np.array([100, 96, 75, 20, 5]),\n        },\n        {\n            \"x\": np.array([0.10, 0.10, 0.30, 0.30, 0.30, 0.60]),\n            \"s\": np.array([12, 8, 25, 18, 20, 40]),\n            \"b\": np.array([88, 92, 75, 82, 80, 60]),\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        pre_brier, post_brier = run_calibration(case['x'], case['s'], case['b'])\n        results.append(f\"{pre_brier:.6f}\")\n        results.append(f\"{post_brier:.6f}\")\n\n    print(f\"[{','.join(results)}]\")\n\ndef pav_algorithm(s, n):\n    \"\"\"\n    Implements the Pool Adjacent Violators (PAV) algorithm.\n\n    The problem is to find a non-decreasing sequence f that minimizes\n    sum(n_i * (f_i - y_i)^2), where y_i = s_i / n_i.\n    This is equivalent to minimizing sum(n_i * (f_i - s_i/n_i)^2).\n    The PAV algorithm provides the exact solution.\n\n    Args:\n        s (np.ndarray): Array of signal counts per bin.\n        n (np.ndarray): Array of total events per bin.\n\n    Returns:\n        np.ndarray: The calibrated, isotonic probabilities f_i.\n    \"\"\"\n    num_bins = len(s)\n    if num_bins == 0:\n        return np.array([])\n    \n    # Active set of blocks. Each block is [sum_s, sum_n, start_index].\n    active_set = []\n    \n    for i in range(num_bins):\n        # Add new block for the current bin\n        active_set.append([s[i], n[i], i])\n        \n        # Merge blocks backwards if monotonicity is violated\n        while len(active_set) > 1:\n            last_block = active_set[-1]\n            prev_block = active_set[-2]\n            \n            # Value of a block is its signal fraction\n            val_last = last_block[0] / last_block[1] if last_block[1] > 0 else 0\n            val_prev = prev_block[0] / prev_block[1] if prev_block[1] > 0 else 0\n\n            if val_last  val_prev:\n                # Merge the last two blocks\n                merged_s = prev_block[0] + last_block[0]\n                merged_n = prev_block[1] + last_block[1]\n                merged_start_idx = prev_block[2]\n                \n                # Replace the last two blocks with the merged one\n                active_set.pop()\n                active_set.pop()\n                active_set.append([merged_s, merged_n, merged_start_idx])\n            else:\n                # No violation, stop merging for this step\n                break\n\n    # Reconstruct the solution vector f from the final blocks\n    f = np.zeros(num_bins)\n    for block_idx, block in enumerate(active_set):\n        sum_s, sum_n, start_idx = block\n        value = sum_s / sum_n if sum_n > 0 else 0\n        end_idx = num_bins\n        # Find the end index for this block\n        if block_idx + 1  len(active_set):\n            end_idx = active_set[block_idx + 1][2]\n        \n        for i in range(start_idx, end_idx):\n            f[i] = value\n            \n    return f\n\ndef calculate_brier_score(p, s, b):\n    \"\"\"\n    Calculates the Brier score for binned data.\n\n    Args:\n        p (np.ndarray): Array of predicted probabilities for each bin.\n        s (np.ndarray): Array of signal counts per bin.\n        b (np.ndarray): Array of background counts per bin.\n        \n    Returns:\n        float: The calculated Brier score.\n    \"\"\"\n    n = s + b\n    total_events = np.sum(n)\n    if total_events == 0:\n        return 0.0\n\n    # Loss is sum of s_i*(p_i-1)^2 + b_i*(p_i-0)^2 for each bin i\n    total_loss = np.sum(s * (p - 1)**2 + b * p**2)\n    \n    return total_loss / total_events\n\ndef run_calibration(x, s, b):\n    \"\"\"\n    Performs calibration and calculates pre/post Brier scores.\n    \n    Args:\n        x (np.ndarray): Classifier scores.\n        s (np.ndarray): Signal counts.\n        b (np.ndarray): Background counts.\n\n    Returns:\n        tuple[float, float]: (pre-calibration Brier, post-calibration Brier).\n    \"\"\"\n    # 1. Calculate pre-calibration Brier score\n    pre_brier = calculate_brier_score(x, s, b)\n    \n    # 2. Perform isotonic regression using PAV\n    n = s + b\n    # The problem implies y_i = s_i / n_i, but pav_algorithm works directly with s and n\n    # to avoid floating point issues and correctly calculate weighted averages.\n    f = pav_algorithm(s, n)\n    \n    # 3. Calculate post-calibration Brier score\n    post_brier = calculate_brier_score(f, s, b)\n    \n    return pre_brier, post_brier\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3524128"}]}