{"hands_on_practices": [{"introduction": "A cornerstone of data analysis in physics is estimating model parameters from data and quantifying the uncertainty of that estimation. This first exercise provides a practical implementation of the weighted least-squares method for linear models, which is equivalent to minimizing the $\\chi^2$ statistic under Gaussian noise assumptions. You will not only find the best-fit parameters but also use the powerful result of Wilks' theorem, which relates the change in the $\\chi^2$ value ($\\Delta\\chi^2$) to confidence intervals, allowing you to build a foundational toolkit for parameter estimation and uncertainty reporting [@problem_id:3507420].", "problem": "Consider a sequence of binned measurements with Gaussian noise and known, bin-wise standard deviations. Let $y_i$ denote the observed value in bin $i$, $\\sigma_i$ the known standard deviation in bin $i$, and $f(x_i;\\boldsymbol{\\theta})$ the model prediction that depends linearly on a parameter vector $\\boldsymbol{\\theta} \\in \\mathbb{R}^k$. Under the Gaussian-noise assumption, the log-likelihood differs from the weighted least-squares objective by an additive constant, and minimizing the negative log-likelihood is equivalent to minimizing the weighted sum of squared residuals. Denote the $\\chi^2$ objective as\n$$\n\\chi^2(\\boldsymbol{\\theta}) \\equiv \\sum_{i=1}^{N} \\frac{\\left(y_i - f(x_i;\\boldsymbol{\\theta})\\right)^2}{\\sigma_i^2}.\n$$\nLet $\\hat{\\boldsymbol{\\theta}}$ be the parameter vector that minimizes $\\chi^2(\\boldsymbol{\\theta})$. The minimum value is $\\chi^2_{\\min} = \\chi^2(\\hat{\\boldsymbol{\\theta}})$. The goodness-of-fit of the model can be assessed by comparing $\\chi^2_{\\min}$ to a reference distribution derived from the Gaussian model and the number of degrees of freedom (DoF), defined as $\\nu \\equiv N - k$ for a linear model with full-rank design matrix. Moreover, Wilks’ theorem states that, under standard regularity conditions and in the asymptotic regime, the difference\n$$\n\\Delta \\chi^2(\\boldsymbol{\\theta}) \\equiv \\chi^2(\\boldsymbol{\\theta}) - \\chi^2_{\\min}\n$$\nis distributed as a chi-squared random variable with degrees of freedom given by the number of constrained parameters (the number of parameters of interest), provided nuisance parameters are profiled (minimized) over. This allows construction of confidence regions in parameter space defined by $\\Delta \\chi^2(\\boldsymbol{\\theta}) \\le q_\\alpha$, where $q_\\alpha$ is the upper-tail quantile corresponding to a confidence level $\\alpha$ for the appropriate chi-squared distribution.\n\nTask. Write a complete, self-contained program that:\n- Implements weighted linear least-squares estimation to obtain $\\hat{\\boldsymbol{\\theta}}$, $\\chi^2_{\\min}$, and the goodness-of-fit $p$-value for the null hypothesis that the data are described by the model with some true parameter vector, using $\\nu = N - k$ degrees of freedom and the chi-squared reference distribution.\n- Implements the Wilks’ theorem construction of confidence regions via $\\Delta \\chi^2$ for:\n  - Joint confidence regions in the full parameter space when all parameters are of interest.\n  - Profiled confidence intervals for a single parameter of interest when the remaining parameters are treated as nuisance parameters and minimized over.\n\nUse only the given datasets below. All quantities are dimensionless. Angles are not involved. Express all confidence levels as decimals in $[0,1]$.\n\nDatasets and models:\n- Case $1$ (single parameter template fit): For $i \\in \\{1,\\dots,5\\}$, define the template vector $t_i$ by $t = [1,2,3,4,5]$. The model is $f_i(a) = a\\,t_i$ with parameter $a \\in \\mathbb{R}$. The observed data and uncertainties are $y = [1.1, 2.0, 2.9, 4.1, 4.9]$ and $\\sigma = [1,1,1,1,1]$. Compute:\n  - The best-fit $\\hat{a}$ and the goodness-of-fit $p$-value using $\\nu = N - k = 5 - 1 = 4$.\n  - Using Wilks’ theorem with one parameter of interest, determine if the fixed values $a_0 = 1.3$ and $a_1 = 1.1$ lie in the $\\alpha = 0.95$ and $\\alpha = 0.68$ confidence intervals, respectively, by testing whether $\\Delta \\chi^2(a_0)$ and $\\Delta \\chi^2(a_1)$ are less than or equal to the corresponding chi-squared quantiles for $1$ degree of freedom.\n- Case $2$ (two-parameter weighted linear model): Let $x = [-3,-2,-1,0,1,2,3,4]$, and the model be $f_i(a,b) = a + b\\,x_i$ with parameters $(a,b) \\in \\mathbb{R}^2$. The observed data and uncertainties are $y = [-0.7, 0.1, 0.6, 1.1, 1.6, 2.0, 2.7, 3.1]$ and $\\sigma = [0.2,0.2,0.2,0.2,0.2,0.2,0.2,0.2]$. Compute:\n  - The best-fit $(\\hat{a},\\hat{b})$ and the goodness-of-fit $p$-value using $\\nu = N - k = 8 - 2 = 6$.\n  - Using Wilks’ theorem for a joint region with two parameters of interest, determine if the points $(a,b) = (1.169, 0.527381)$ and $(a,b) = (1.209, 0.527381)$ lie in the $\\alpha = 0.90$ confidence region by testing whether $\\Delta \\chi^2(a,b)$ is less than or equal to the corresponding chi-squared quantile for $2$ degrees of freedom.\n  - Using the profiled version of Wilks’ theorem with one parameter of interest $a$ (treat $b$ as a nuisance parameter to be minimized over), determine if the fixed values $a_2 = 1.169$ and $a_3 = 1.209$ lie in the $\\alpha = 0.95$ confidence interval by testing whether the profiled $\\Delta \\chi^2_{\\mathrm{prof}}(a)$ is less than or equal to the chi-squared quantile for $1$ degree of freedom.\n- Case $3$ (single parameter template fit with heteroscedastic uncertainties): For $i \\in \\{1,\\dots,6\\}$, define the template vector $t = [1.0, 0.5, 2.0, 1.5, 3.0, 2.5]$. The model is $f_i(a) = a\\,t_i$. The observed data and uncertainties are $y = [2.1, 1.0, 4.1, 2.9, 6.2, 5.1]$ and $\\sigma = [0.3, 0.2, 0.5, 0.4, 0.6, 0.5]$. Compute:\n  - The best-fit $\\hat{a}$ and the goodness-of-fit $p$-value using $\\nu = N - k = 6 - 1 = 5$.\n  - Using Wilks’ theorem with one parameter of interest, determine if the fixed values $a_4 = 2.2$ and $a_5 = 2.5$ lie in the $\\alpha = 0.99$ confidence interval by testing whether $\\Delta \\chi^2(a)$ is less than or equal to the chi-squared quantile for $1$ degree of freedom.\n\nAlgorithmic and statistical requirements:\n- Start from the Gaussian likelihood for independent bins and the definition of $\\chi^2(\\boldsymbol{\\theta})$; derive the weighted normal equations for the linear model and solve for $\\hat{\\boldsymbol{\\theta}}$ using the design matrix formalism with a diagonal weight matrix $W = \\mathrm{diag}(1/\\sigma_i^2)$.\n- Compute $\\chi^2_{\\min}$ and the goodness-of-fit $p$-value from the chi-squared survival function at $\\chi^2_{\\min}$ with $\\nu$ degrees of freedom.\n- For any candidate parameter vector $\\boldsymbol{\\theta}_0$, compute $\\Delta \\chi^2(\\boldsymbol{\\theta}_0)$. For profiled one-parameter tests, minimize over nuisance parameters analytically through the linear algebra implied by the weighted normal equations.\n- Determine membership in the confidence region by comparing $\\Delta \\chi^2$ to the chi-squared quantile at the specified confidence level and degrees of freedom.\n\nTest suite and final output specification:\n- Use exactly the three cases above as the test suite.\n- For each case, return a list with the following elements in order:\n  - Case $1$: $[\\hat{a}, \\text{p-value}, \\text{inside_C0.95_a=1.3}, \\text{inside_C0.68_a=1.1}]$.\n  - Case $2$: $[[\\hat{a}, \\hat{b}], \\text{p-value}, \\text{inside_C0.90_2D(1.169,0.527381)}, \\text{inside_C0.90_2D(1.209,0.527381)}, \\text{inside_C0.95_profile_a=1.169}, \\text{inside_C0.95_profile_a=1.209}]$.\n  - Case $3$: $[\\hat{a}, \\text{p-value}, \\text{inside_C0.99_a=2.2}, \\text{inside_C0.99_a=2.5}]$.\n- Round all floating-point outputs to $6$ decimal places. Booleans should be exact logical values.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[result1,result2,result3]$). No additional text or whitespace is permitted in the output line.", "solution": "The user-provided problem is a well-posed exercise in linear statistical modeling, specifically weighted least-squares estimation and the application of Wilks' theorem for constructing confidence regions. The problem statement is scientifically sound, self-contained, and algorithmically tractable. It is therefore deemed valid and a full solution follows.\n\n### Theoretical Framework\n\nThe core of the problem is the minimization of the chi-squared ($\\chi^2$) objective function for a model that is linear in its parameters.\n\nGiven a set of $N$ data points $(x_i, y_i)$ with uncertainties $\\sigma_i$ on the measurements $y_i$, and a model function $f(x_i; \\boldsymbol{\\theta})$ that is linear in the $k$ parameters $\\boldsymbol{\\theta} \\in \\mathbb{R}^k$, we can express the model in matrix form:\n$$\n\\mathbb{E}[\\boldsymbol{y}] = \\boldsymbol{f}(\\boldsymbol{x}; \\boldsymbol{\\theta}) = A \\boldsymbol{\\theta}\n$$\nwhere $\\boldsymbol{y} = (y_1, \\dots, y_N)^T$ is the vector of observed values, $A$ is the $N \\times k$ design matrix, and $\\boldsymbol{\\theta} = (\\theta_1, \\dots, \\theta_k)^T$ is the vector of parameters. The element $A_{ij}$ of the design matrix is the coefficient of the $j$-th parameter in the model for the $i$-th data point.\n\nThe $\\chi^2$ objective function, assuming independent Gaussian uncertainties, is the weighted sum of squared residuals:\n$$\n\\chi^2(\\boldsymbol{\\theta}) = \\sum_{i=1}^{N} \\left(\\frac{y_i - f(x_i; \\boldsymbol{\\theta})}{\\sigma_i}\\right)^2\n$$\nIn matrix notation, this becomes:\n$$\n\\chi^2(\\boldsymbol{\\theta}) = (\\boldsymbol{y} - A\\boldsymbol{\\theta})^T W (\\boldsymbol{y} - A\\boldsymbol{\\theta})\n$$\nwhere $W$ is the diagonal weight matrix with elements $W_{ii} = 1/\\sigma_i^2$.\n\nTo find the parameter vector $\\hat{\\boldsymbol{\\theta}}$ that minimizes $\\chi^2$, we set the gradient of $\\chi^2(\\boldsymbol{\\theta})$ with respect to $\\boldsymbol{\\theta}$ to zero:\n$$\n\\nabla_{\\boldsymbol{\\theta}} \\chi^2(\\boldsymbol{\\theta}) = -2 A^T W \\boldsymbol{y} + 2 A^T W A \\boldsymbol{\\theta} = \\boldsymbol{0}\n$$\nThis yields the **normal equations**:\n$$\n(A^T W A) \\boldsymbol{\\theta} = A^T W \\boldsymbol{y}\n$$\nAssuming the design matrix $A$ has full column rank, the matrix $A^T W A$ is invertible. The best-fit estimator for $\\boldsymbol{\\theta}$ is then:\n$$\n\\hat{\\boldsymbol{\\theta}} = (A^T W A)^{-1} (A^T W \\boldsymbol{y})\n$$\nThe minimum value of the $\\chi^2$ function, denoted $\\chi^2_{\\min}$, is obtained by substituting $\\hat{\\boldsymbol{\\theta}}$ back into the objective function:\n$$\n\\chi^2_{\\min} = \\chi^2(\\hatboldsymbol{\\theta}}) = (\\boldsymbol{y} - A\\hat{\\boldsymbol{\\theta}})^T W (\\boldsymbol{y} - A\\hat{\\boldsymbol{\\theta}})\n$$\nUnder the null hypothesis that the data are generated from the model $f(x_i; \\boldsymbol{\\theta}_{\\text{true}})$ for some true parameter vector $\\boldsymbol{\\theta}_{\\text{true}}$, the statistic $\\chi^2_{\\min}$ follows a chi-squared distribution with $\\nu = N - k$ degrees of freedom. The goodness-of-fit can be quantified by a $p$-value, which is the probability of obtaining a $\\chi^2_{\\min}$ value as large or larger than the one observed. This is computed using the survival function (1 - CDF) of the $\\chi^2(\\nu)$ distribution:\n$$\np\\text{-value} = P(\\chi^2_\\nu \\ge \\chi^2_{\\min}) = \\text{sf}(\\chi^2_{\\min}, \\nu)\n$$\n\n### Confidence Regions via Wilks' Theorem\n\nWilks' theorem states that the statistic $\\Delta\\chi^2(\\boldsymbol{\\theta}) \\equiv \\chi^2(\\boldsymbol{\\theta}) - \\chi^2_{\\min}$ is distributed as a chi-squared variable. The number of degrees of freedom, $d$, for this distribution is equal to the number of parameters of interest being tested.\n\nA confidence region with confidence level $\\alpha$ for a set of $d$ parameters is the set of all parameter points $\\boldsymbol{\\theta}_0$ such that:\n$$\n\\Delta\\chi^2(\\boldsymbol{\\theta}_0) \\le q_\\alpha\n$$\nwhere $q_\\alpha$ is the upper-tail quantile of the $\\chi^2(d)$ distribution, i.e., $P(\\chi^2_d \\le q_\\alpha) = \\alpha$. This critical value is found using the percent-point function (inverse CDF).\n\nFor a linear model, a computationally efficient formula for $\\Delta\\chi^2$ can be derived:\n$$\n\\Delta\\chi^2(\\boldsymbol{\\theta}_0) = (\\boldsymbol{\\theta}_0 - \\hat{\\boldsymbol{\\theta}})^T (A^T W A) (\\boldsymbol{\\theta}_0 - \\hat{\\boldsymbol{\\theta}})\n$$\n\nFor a profiled confidence interval on a single parameter, say $\\theta_j$, we treat the other $k-1$ parameters as nuisance parameters. For each fixed value $\\theta_j = \\theta_{j,0}$, we minimize $\\chi^2$ with respect to the nuisance parameters. The resulting profiled statistic $\\Delta\\chi^2_{\\text{prof}}(\\theta_{j,0})$ is distributed as $\\chi^2(1)$. For linear models, this simplifies to:\n$$\n\\Delta\\chi^2_{\\text{prof}}(\\theta_{j,0}) = \\frac{(\\theta_{j,0} - \\hat{\\theta}_j)^2}{C_{jj}}\n$$\nwhere $C = (A^T W A)^{-1}$ is the covariance matrix of the least-squares estimators, and $C_{jj}$ is its $j$-th diagonal element (the variance of $\\hat{\\theta}_j$).\n\n### Algorithmic Implementation Strategy\n\nA generalized function will be defined to solve the weighted linear least-squares problem. This function takes the design matrix $A$, the data vector $\\boldsymbol{y}$, and the uncertainties vector $\\boldsymbol{\\sigma}$ as input. It computes and returns the best-fit parameters $\\hat{\\boldsymbol{\\theta}}$, the minimum $\\chi^2$, the covariance matrix $C$, and the goodness-of-fit $p$-value. Specific functions for each case will then use these results to perform the required confidence region tests.\n\n#### Case 1: Single Parameter Template Fit\n- Model: $f_i(a) = a\\,t_i$.\n- Parameters: $\\boldsymbol{\\theta} = [a]$.\n- Design matrix: $A$ is an $N \\times 1$ matrix (column vector) given by the template $t$.\n- $N=5$, $k=1$, $\\nu=4$. For CI tests, $d=1$.\n- Uncertainties are homoscedastic ($\\sigma_i=1$), so $W=I$.\n\n#### Case 2: Two-Parameter Linear Model\n- Model: $f_i(a,b) = a + b\\,x_i$.\n- Parameters: $\\boldsymbol{\\theta} = [a, b]^T$.\n- Design matrix: $A$ is an $N \\times 2$ matrix, with the first column being all ones and the second column being the values $x_i$.\n- $N=8$, $k=2$, $\\nu=6$.\n- For the joint confidence region test, we use $\\Delta\\chi^2(\\boldsymbol{\\theta}_0)$ with $d=2$.\n- For the profiled confidence interval on $a$, we use $\\Delta\\chi^2_{\\text{prof}}(a_0)$ with $d=1$.\n\n#### Case 3: Heteroscedastic Template Fit\n- Model: $f_i(a) = a\\,t_i$.\n- Parameters: $\\boldsymbol{\\theta} = [a]$.\n- Design matrix: $A$ is an $N \\times 1$ matrix given by the template $t$.\n- $N=6$, $k=1$, $\\nu=5$. For CI tests, $d=1$.\n- Uncertainties are heteroscedastic, so the weight matrix $W$ is diagonal but not a multiple of the identity. The full weighted least-squares formalism is required.\n\nThe calculations for each case will be performed using `numpy` for matrix operations and `scipy.stats.chi2` for quantiles and survival functions. All floating-point results are rounded to six decimal places as specified.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import chi2\n\ndef solve_weighted_least_squares(A, y, sigma):\n    \"\"\"\n    Solves a weighted linear least-squares problem.\n    \n    Args:\n        A (np.ndarray): Design matrix (N x k).\n        y (np.ndarray): Observed data vector (N,).\n        sigma (np.ndarray): Uncertainties vector (N,).\n\n    Returns:\n        tuple: A tuple containing:\n            - theta_hat (np.ndarray): Best-fit parameters (k,).\n            - chi2_min (float): Minimum chi-squared value.\n            - C (np.ndarray): Covariance matrix of parameters (k x k).\n            - p_value (float): Goodness-of-fit p-value.\n    \"\"\"\n    N, k = A.shape\n    W = np.diag(1 / sigma**2)\n\n    # Normal equations: (A^T W A) theta = A^T W y\n    A_T_W_A = A.T @ W @ A\n    A_T_W_y = A.T @ W @ y\n    \n    # Covariance matrix and best-fit parameters\n    C = np.linalg.inv(A_T_W_A)\n    theta_hat = C @ A_T_W_y\n    \n    # Minimum chi-squared\n    residuals = y - A @ theta_hat\n    chi2_min = residuals.T @ W @ residuals\n    \n    # Goodness-of-fit p-value\n    dof = N - k\n    p_value = chi2.sf(chi2_min, df=dof)\n    \n    return theta_hat, chi2_min, C, p_value, A_T_W_A\n\ndef format_results(data):\n    \"\"\"\n    Recursively formats results, rounding floats and leaving booleans.\n    \"\"\"\n    if isinstance(data, list):\n        return [format_results(item) for item in data]\n    if isinstance(data, (float, np.floating)):\n        return round(data, 6)\n    return data\n\ndef solve_case1():\n    \"\"\"Calculates and returns results for Case 1.\"\"\"\n    y = np.array([1.1, 2.0, 2.9, 4.1, 4.9])\n    sigma = np.array([1.0, 1.0, 1.0, 1.0, 1.0])\n    t = np.array([1.0, 2.0, 3.0, 4.0, 5.0])\n    A = t.reshape(-1, 1)\n\n    a_hat_vec, chi2_min, _, p_value, A_T_W_A = solve_weighted_least_squares(A, y, sigma)\n    a_hat = a_hat_vec[0]\n\n    # Confidence interval tests for a\n    def check_ci(a_test, alpha):\n        theta_test = np.array([a_test])\n        theta_hat = np.array([a_hat])\n        delta_chi2 = (theta_test - theta_hat).T @ A_T_W_A @ (theta_test - theta_hat)\n        q_alpha = chi2.ppf(alpha, df=1)\n        return delta_chi2 <= q_alpha\n\n    inside_c095_a13 = check_ci(1.3, 0.95)\n    inside_c068_a11 = check_ci(1.1, 0.68)\n    \n    return [a_hat, p_value, inside_c095_a13, inside_c068_a11]\n\ndef solve_case2():\n    \"\"\"Calculates and returns results for Case 2.\"\"\"\n    x = np.array([-3.0, -2.0, -1.0, 0.0, 1.0, 2.0, 3.0, 4.0])\n    y = np.array([-0.7, 0.1, 0.6, 1.1, 1.6, 2.0, 2.7, 3.1])\n    sigma = np.array([0.2] * 8)\n    A = np.vstack([np.ones_like(x), x]).T\n\n    theta_hat, chi2_min, C, p_value, A_T_W_A = solve_weighted_least_squares(A, y, sigma)\n\n    # Joint confidence region tests (d=2)\n    def check_joint_cr(theta_test, alpha):\n        delta_chi2 = (theta_test - theta_hat).T @ A_T_W_A @ (theta_test - theta_hat)\n        q_alpha = chi2.ppf(alpha, df=2)\n        return delta_chi2 <= q_alpha\n\n    pt1 = np.array([1.169, 0.527381])\n    pt2 = np.array([1.209, 0.527381])\n    inside_cr90_pt1 = check_joint_cr(pt1, 0.90)\n    inside_cr90_pt2 = check_joint_cr(pt2, 0.90)\n\n    # Profiled confidence interval tests for a (d=1)\n    def check_profiled_ci(a_test, alpha):\n        delta_chi2_prof = (a_test - theta_hat[0])**2 / C[0, 0]\n        q_alpha = chi2.ppf(alpha, df=1)\n        return delta_chi2_prof <= q_alpha\n\n    inside_ci95_a1169 = check_profiled_ci(1.169, 0.95)\n    inside_ci95_a1209 = check_profiled_ci(1.209, 0.95)\n\n    return [list(theta_hat), p_value, inside_cr90_pt1, inside_cr90_pt2, inside_ci95_a1169, inside_ci95_a1209]\n\ndef solve_case3():\n    \"\"\"Calculates and returns results for Case 3.\"\"\"\n    t = np.array([1.0, 0.5, 2.0, 1.5, 3.0, 2.5])\n    y = np.array([2.1, 1.0, 4.1, 2.9, 6.2, 5.1])\n    sigma = np.array([0.3, 0.2, 0.5, 0.4, 0.6, 0.5])\n    A = t.reshape(-1, 1)\n\n    a_hat_vec, chi2_min, _, p_value, A_T_W_A = solve_weighted_least_squares(A, y, sigma)\n    a_hat = a_hat_vec[0]\n\n    # Confidence interval tests for a\n    def check_ci(a_test, alpha):\n        theta_test = np.array([a_test])\n        theta_hat = np.array([a_hat])\n        delta_chi2 = (theta_test - theta_hat).T @ A_T_W_A @ (theta_test - theta_hat)\n        q_alpha = chi2.ppf(alpha, df=1)\n        return delta_chi2 <= q_alpha\n\n    inside_c099_a22 = check_ci(2.2, 0.99)\n    inside_c099_a25 = check_ci(2.5, 0.99)\n    \n    return [a_hat, p_value, inside_c099_a22, inside_c099_a25]\n\ndef solve():\n    \"\"\"\n    Main function to solve all test cases and print the final output.\n    \"\"\"\n    result1 = solve_case1()\n    result2 = solve_case2()\n    result3 = solve_case3()\n\n    all_results = [\n        format_results(result1),\n        format_results(result2),\n        format_results(result3)\n    ]\n    \n    # Use str() to handle nested lists and booleans correctly\n    # and replace spaces for compactness\n    print(str(all_results).replace(\" \", \"\"))\n\nsolve()\n```", "id": "3507420"}, {"introduction": "While the Gaussian approximation is powerful, many high-energy physics experiments are fundamentally counting experiments, where the data in each bin follows a Poisson distribution. In the low-count regime, which is common in searches for rare phenomena, Gaussian approximations to the Poisson likelihood can be inaccurate and lead to biased results. This practice guides you through a Monte Carlo simulation to compare the performance of the statistically rigorous Poisson likelihood deviance with its common approximations, Pearson's $\\chi^2_P$ and Neyman's $\\chi^2_N$, providing crucial insights into which statistical tool to trust when every event counts [@problem_id:3507402].", "problem": "You are given a binned counting experiment typical of Large Hadron Collider analyses. In each bin indexed by $i \\in \\{1,\\dots,n_{\\mathrm{bins}}\\}$, the observed counts $d_i$ are modeled as independent Poisson random variables with mean $m_i(\\mu) = b_i + \\mu\\, s_i$, where $b_i \\ge 0$ is a known background expectation, $s_i \\ge 0$ is a known signal template, and $\\mu \\ge 0$ is an unknown nonnegative signal strength parameter to be estimated by fitting. Your task is to compare three objective functions for fitting and goodness-of-fit in the sparse-count regime and to quantify the differences in $p$-values at low counts.\n\nUse the following fundamental base:\n- The per-bin probability for Poisson data is $P(d_i \\mid m_i) = \\frac{m_i^{d_i} e^{-m_i}}{d_i!}$ with $d_i \\in \\{0,1,2,\\dots\\}$ and $m_i > 0$.\n- The global likelihood is the product over bins, $L(\\mu) = \\prod_{i=1}^{n_{\\mathrm{bins}}} P(d_i \\mid m_i(\\mu))$.\n- The deviance (twice the negative log-likelihood ratio relative to a saturated model) is $D(\\mu) = 2 \\sum_{i=1}^{n_{\\mathrm{bins}}} \\left( m_i(\\mu) - d_i + d_i \\ln \\frac{d_i}{m_i(\\mu)} \\right)$ with the convention that $d_i \\ln \\frac{d_i}{m_i(\\mu)} = 0$ when $d_i = 0$.\n- Two Gaussian-approximation chi-squared functions commonly used in practice are:\n  - Pearson: $\\chi^2_{\\mathrm{P}}(\\mu) = \\sum_{i=1}^{n_{\\mathrm{bins}}} \\frac{\\left(d_i - m_i(\\mu)\\right)^2}{m_i(\\mu)}$.\n  - Neyman: $\\chi^2_{\\mathrm{N}}(\\mu) = \\sum_{i \\in \\mathcal{I}(d)} \\frac{\\left(d_i - m_i(\\mu)\\right)^2}{d_i}$, where $\\mathcal{I}(d) = \\{ i \\mid d_i > 0\\}$ is the set of bins with nonzero observed counts. When computing $\\chi^2_{\\mathrm{N}}$, exclude bins with $d_i = 0$ from the sum.\n\nFitting rule:\n- For each objective $T(\\mu) \\in \\{\\chi^2_{\\mathrm{P}}(\\mu), \\chi^2_{\\mathrm{N}}(\\mu), D(\\mu)\\}$, define the corresponding estimator $\\hat{\\mu}$ as the minimizer of $T(\\mu)$ over the closed interval $\\mu \\in [0, \\mu_{\\max}]$, where $\\mu_{\\max}$ is a fixed finite bound specified in each test case. Impose the nonnegativity constraint $0 \\le \\mu \\le \\mu_{\\max}$.\n\nGoodness-of-fit $p$-value mapping:\n- For each minimized statistic value $T(\\hat{\\mu})$, compute an asymptotic goodness-of-fit $p$-value by comparing to a chi-squared distribution with degrees of freedom $n_{\\mathrm{dof}} = n_{\\mathrm{eff}} - n_{\\mathrm{par}}$, where $n_{\\mathrm{par}} = 1$ is the number of fitted parameters and $n_{\\mathrm{eff}}$ is the effective number of bins in the sum:\n  - For Pearson and Deviance, take $n_{\\mathrm{eff}} = n_{\\mathrm{bins}}$ so that $n_{\\mathrm{dof}} = n_{\\mathrm{bins}} - 1$.\n  - For Neyman, take $n_{\\mathrm{eff}} = |\\mathcal{I}(d)|$ so that $n_{\\mathrm{dof}} = |\\mathcal{I}(d)| - 1$. If $|\\mathcal{I}(d)| - 1 < 1$, set $n_{\\mathrm{dof}} = 1$ to ensure a well-defined $p$-value. Use the chi-squared survival function to compute the $p$-value, i.e., $p = 1 - F_{\\chi^2}(T(\\hat{\\mu}); n_{\\mathrm{dof}})$.\n\nMonte Carlo procedure:\n- For each test case, perform $R$ independent pseudoexperiments. In each pseudoexperiment, generate counts $d_i \\sim \\mathrm{Poisson}(m_i(\\mu_{\\mathrm{true}}))$ independently over bins using the specified $b_i$, $s_i$, and $\\mu_{\\mathrm{true}}$. For each objective $T$, compute $\\hat{\\mu}_T$ and the corresponding minimized statistic $T(\\hat{\\mu}_T)$ and $p$-value. Quantify performance and differences as follows:\n  - Bias: For each method $T$, compute the sample mean of $\\hat{\\mu}_T - \\mu_{\\mathrm{true}}$ across the $R$ pseudoexperiments.\n  - Mean $p$-value: For each method $T$, compute the sample mean of the $p$-values across the $R$ pseudoexperiments.\n  - $p$-value differences relative to the deviance: Compute the sample mean of $|p_{\\mathrm{P}} - p_{D}|$ and of $|p_{\\mathrm{N}} - p_{D}|$ across the $R$ pseudoexperiments.\n\nNumerical conventions and edge-handling:\n- Treat $d_i \\ln \\frac{d_i}{m_i(\\mu)}$ as $0$ when $d_i = 0$.\n- Ensure $m_i(\\mu) = b_i + \\mu s_i$ remains strictly positive in computations; in practice, clip $m_i(\\mu)$ below by a small positive constant if needed to maintain numerical stability in logarithms and denominators.\n- For $\\chi^2_{\\mathrm{N}}$, exclude bins with $d_i = 0$ as defined above.\n\nRandomness and determinism:\n- Use the fixed random seed $20250101$ for all pseudoexperiments to ensure deterministic output.\n\nTest suite:\n- Case A (sparse, nonzero signal): $n_{\\mathrm{bins}} = 8$, $R = 400$, $\\mu_{\\mathrm{true}} = 1.2$, $\\mu_{\\max} = 5.0$, background $b = [0.6, 0.7, 0.3, 0.5, 0.2, 0.9, 0.4, 0.8]$, signal template $s = [0.2, 0.5, 0.1, 0.05, 0.1, 0.3, 0.2, 0.25]$.\n- Case B (very low counts, boundary at zero signal): $n_{\\mathrm{bins}} = 12$, $R = 400$, $\\mu_{\\mathrm{true}} = 0.0$, $\\mu_{\\max} = 3.0$, background $b = [0.8, 0.9, 0.7, 1.1, 0.6, 0.5, 0.4, 0.3, 0.9, 0.7, 0.6, 1.0]$, signal template $s = [0.1, 0.2, 0.0, 0.05, 0.1, 0.05, 0.05, 0.0, 0.2, 0.1, 0.05, 0.1]$.\n- Case C (mixed counts, moderate signal): $n_{\\mathrm{bins}} = 10$, $R = 300$, $\\mu_{\\mathrm{true}} = 0.5$, $\\mu_{\\max} = 3.0$, background $b = [2.0, 3.0, 1.5, 4.0, 2.5, 3.5, 1.0, 2.0, 5.0, 2.5]$, signal template $s = [0.5, 1.0, 0.2, 1.5, 0.8, 1.2, 0.1, 0.4, 2.0, 0.7]$.\n\nOutput specification:\n- For each test case, produce a list of eight real numbers in the following order:\n  $[\\text{bias}_{\\mathrm{P}}, \\text{bias}_{\\mathrm{N}}, \\text{bias}_{D}, \\overline{p}_{\\mathrm{P}}, \\overline{p}_{\\mathrm{N}}, \\overline{p}_{D}, \\overline{|p_{\\mathrm{P}} - p_{D}|}, \\overline{|p_{\\mathrm{N}} - p_{D}|}]$,\n  where bars denote sample means over the $R$ pseudoexperiments. Round each number to six decimal places.\n- Your program should produce a single line of output containing the results as a top-level list of per-case result lists, for example: $[[x_{1,1},\\dots,x_{1,8}],[x_{2,1},\\dots,x_{2,8}],[x_{3,1},\\dots,x_{3,8}]]$. No additional text should be printed.", "solution": "The problem requires a comparative analysis of three different objective functions—Deviance, Pearson's $\\chi^2$, and Neyman's $\\chi^2$—for parameter estimation and goodness-of-fit testing in the context of a binned Poisson counting experiment. This is a canonical problem in high-energy physics data analysis, particularly relevant in sparse-data regimes where asymptotic statistical approximations may falter.\n\nThe methodological foundation is rooted in the principles of statistical inference:\n1.  **Modeling**: The observed count $d_i$ in each bin is modeled as an independent Poisson random variable with a mean $m_i(\\mu) = b_i + \\mu s_i$. This linear model for the mean captures a signal component with strength $\\mu$ superimposed on a background contribution $b_i$.\n2.  **Parameter Estimation**: The signal strength $\\mu$ is an unknown parameter to be estimated from the data. This is achieved by finding the value $\\hat{\\mu}$ that minimizes a chosen objective function $T(\\mu)$, a procedure known as M-estimation. The problem specifies a constrained minimization over the physically motivated interval $\\mu \\in [0, \\mu_{\\max}]$.\n    -   **Deviance ($D(\\mu)$)**: This statistic is derived from the likelihood-ratio principle, specifically $D(\\mu) = -2 \\ln \\frac{L(\\mu)}{L_{\\text{sat}}}$, where $L(\\mu)$ is the Poisson likelihood for the model and $L_{\\text{sat}}$ is the likelihood of a \"saturated\" model that perfectly fits the data. Minimizing the deviance is therefore equivalent to maximizing the likelihood, yielding the Maximum Likelihood Estimator (MLE). The MLE is known for its desirable asymptotic properties, including efficiency and unbiasedness.\n    -   **Pearson's Chi-Squared ($\\chi^2_{\\mathrm{P}}(\\mu)$)**: This is a weighted least-squares statistic, $\\sum (d_i - m_i)^2 / m_i$. The weight for each bin is the inverse of the model-predicted variance, since for a Poisson distribution, the variance equals the mean. This statistic is asymptotically equivalent to the deviance.\n    -   **Neyman's Chi-Squared ($\\chi^2_{\\mathrm{N}}(\\mu)$)**: Similar to Pearson's, this is a least-squares statistic, $\\sum (d_i - m_i)^2 / d_i$, but it uses the observed count $d_i$ as an estimate of the variance for weighting. This can be problematic, especially for low counts; bins with $d_i=0$ contribute no information and must be excluded, potentially biasing the fit.\n3.  **Goodness-of-Fit (GoF) Testing**: After fitting, the minimized value of the statistic, $T(\\hat{\\mu})$, is used to assess how well the model describes the data. According to theorems by Wilks and Pearson, under the null hypothesis that the model is correct, the distribution of these statistics is asymptotically a chi-squared ($\\chi^2$) distribution. The number of degrees of freedom ($n_{\\mathrm{dof}}$) is the effective number of data points (bins) minus the number of fitted parameters ($n_{\\mathrm{par}}=1$). The problem defines $n_{\\mathrm{dof}}$ for each statistic, reflecting standard conventions. A $p$-value is computed as the probability of observing a statistic value as large or larger than $T(\\hat{\\mu})$, given this $\\chi^2$ distribution.\n4.  **Monte Carlo Simulation**: To evaluate the performance of these methods in finite-sample, low-count scenarios, a Monte Carlo study is performed. For a known \"true\" value $\\mu_{\\mathrm{true}}$, multiple pseudo-experiments are generated. For each, the fitting and GoF procedure is carried out. By averaging over many such experiments, we can estimate key frequentist properties:\n    -   **Bias**: The average difference $\\hat{\\mu} - \\mu_{\\mathrm{true}}$ indicates if an estimator systematically over- or underestimates the true value.\n    -   **Mean $p$-value**: For a correctly calibrated GoF test, the $p$-values should be uniformly distributed in $[0, 1]$, with a mean of $0.5$. Deviations from this value signal miscalibration of the asymptotic approximation.\n    -   **$p$-value differences**: Comparing the $p$-values from the approximate $\\chi^2$ methods to those from the deviance-based method quantifies their disagreement.\n\nThe provided solution implements this entire procedure deterministically by using a fixed random seed. It iterates through the test cases, running the prescribed number of pseudoexperiments for each. Within each experiment, it generates Poisson data, defines the three objective functions, performs bounded scalar minimization using `scipy.optimize.minimize_scalar`, calculates the corresponding GoF $p$-values using `scipy.stats.chi2.sf`, and accumulates the specified metrics. Finally, it computes the sample means and formats the output as required. Numerical stability is ensured by clipping model predictions to a small positive floor before their use in denominators or logarithms.", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import minimize_scalar\nfrom scipy.stats import chi2\n\ndef solve():\n    \"\"\"\n    Main function to run the specified test cases and print results.\n    \"\"\"\n    # Set up a global random number generator for reproducibility across all test cases.\n    rng = np.random.default_rng(20250101)\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"name\": \"Case A\",\n            \"n_bins\": 8, \"R\": 400, \"mu_true\": 1.2, \"mu_max\": 5.0,\n            \"b\": np.array([0.6, 0.7, 0.3, 0.5, 0.2, 0.9, 0.4, 0.8]),\n            \"s\": np.array([0.2, 0.5, 0.1, 0.05, 0.1, 0.3, 0.2, 0.25]),\n        },\n        {\n            \"name\": \"Case B\",\n            \"n_bins\": 12, \"R\": 400, \"mu_true\": 0.0, \"mu_max\": 3.0,\n            \"b\": np.array([0.8, 0.9, 0.7, 1.1, 0.6, 0.5, 0.4, 0.3, 0.9, 0.7, 0.6, 1.0]),\n            \"s\": np.array([0.1, 0.2, 0.0, 0.05, 0.1, 0.05, 0.05, 0.0, 0.2, 0.1, 0.05, 0.1]),\n        },\n        {\n            \"name\": \"Case C\",\n            \"n_bins\": 10, \"R\": 300, \"mu_true\": 0.5, \"mu_max\": 3.0,\n            \"b\": np.array([2.0, 3.0, 1.5, 4.0, 2.5, 3.5, 1.0, 2.0, 5.0, 2.5]),\n            \"s\": np.array([0.5, 1.0, 0.2, 1.5, 0.8, 1.2, 0.1, 0.4, 2.0, 0.7]),\n        },\n    ]\n\n    all_results = []\n    for case in test_cases:\n        results = run_monte_carlo_case(case, rng)\n        all_results.append(results)\n\n    # Format the final list of lists into the required string representation.\n    inner_strings = [f\"[{','.join(f'{v:.6f}' for v in res)}]\" for res in all_results]\n    output_str = f\"[{','.join(inner_strings)}]\"\n    print(output_str)\n\ndef run_monte_carlo_case(case_params, rng):\n    \"\"\"\n    Performs a Monte Carlo simulation for a single test case.\n    \"\"\"\n    n_bins = case_params[\"n_bins\"]\n    R = case_params[\"R\"]\n    mu_true = case_params[\"mu_true\"]\n    mu_max = case_params[\"mu_max\"]\n    b = case_params[\"b\"]\n    s = case_params[\"s\"]\n\n    # Small constant for numerical stability\n    epsilon = 1e-12\n\n    # Initialize accumulators for summary statistics\n    bias_p_sum, bias_n_sum, bias_d_sum = 0.0, 0.0, 0.0\n    p_p_sum, p_n_sum, p_d_sum = 0.0, 0.0, 0.0\n    p_diff_pd_sum, p_diff_nd_sum = 0.0, 0.0\n\n    m_true_counts = b + mu_true * s\n\n    for _ in range(R):\n        # Generate pseudo-experimental data\n        d = rng.poisson(m_true_counts)\n\n        # --- Objective Functions ---\n        def pearson_chi2(mu):\n            m = np.clip(b + mu * s, epsilon, None)\n            return np.sum((d - m)**2 / m)\n\n        def neyman_chi2(mu):\n            d_pos_mask = d > 0\n            if not np.any(d_pos_mask):\n                return 0.0\n            d_pos = d[d_pos_mask]\n            m_pos = (b + mu * s)[d_pos_mask]\n            return np.sum((d_pos - m_pos)**2 / d_pos)\n\n        def deviance(mu):\n            m = np.clip(b + mu * s, epsilon, None)\n            # Use np.where to handle the d_i=0 case as per the convention\n            log_term = np.where(d > 0, d * np.log(d / m), 0)\n            return 2 * np.sum(m - d + log_term)\n\n        # --- Minimization ---\n        res_p = minimize_scalar(pearson_chi2, bounds=(0, mu_max), method='bounded')\n        res_n = minimize_scalar(neyman_chi2, bounds=(0, mu_max), method='bounded')\n        res_d = minimize_scalar(deviance, bounds=(0, mu_max), method='bounded')\n\n        mu_hat_p, stat_min_p = res_p.x, res_p.fun\n        mu_hat_n, stat_min_n = res_n.x, res_n.fun\n        mu_hat_d, stat_min_d = res_d.x, res_d.fun\n\n        # --- Goodness-of-Fit p-value Calculation ---\n        p_values = {}\n\n        # Pearson\n        ndof_p = n_bins - 1\n        p_values['p'] = chi2.sf(stat_min_p, ndof_p) if ndof_p > 0 else 1.0\n\n        # Neyman\n        n_eff_n = np.sum(d > 0)\n        ndof_n = max(1, n_eff_n - 1)\n        p_values['n'] = chi2.sf(stat_min_n, ndof_n)\n\n        # Deviance\n        ndof_d = n_bins - 1\n        p_values['d'] = chi2.sf(stat_min_d, ndof_d) if ndof_d > 0 else 1.0\n\n        # --- Accumulate Metrics ---\n        bias_p_sum += mu_hat_p - mu_true\n        bias_n_sum += mu_hat_n - mu_true\n        bias_d_sum += mu_hat_d - mu_true\n\n        p_p_sum += p_values['p']\n        p_n_sum += p_values['n']\n        p_d_sum += p_values['d']\n\n        p_diff_pd_sum += abs(p_values['p'] - p_values['d'])\n        p_diff_nd_sum += abs(p_values['n'] - p_values['d'])\n    \n    # --- Calculate Final Averages ---\n    results = [\n        bias_p_sum / R,\n        bias_n_sum / R,\n        bias_d_sum / R,\n        p_p_sum / R,\n        p_n_sum / R,\n        p_d_sum / R,\n        p_diff_pd_sum / R,\n        p_diff_nd_sum / R,\n    ]\n    return results\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3507402"}, {"introduction": "Statistical theorems are powerful, but their validity rests on specific assumptions that are not always met in practice. This exercise explores a critical failure mode of Wilks' theorem that occurs when a parameter's true value lies on a physical boundary, a common scenario in searches for new phenomena where a signal strength might be zero. You will use toy Monte Carlo simulations to investigate how the distribution of the $\\Delta\\chi^2$ test statistic deviates from the standard $\\chi^2$ prediction and learn how to construct statistically correct confidence intervals by calibrating against this empirical distribution, a vital technique for ensuring robust scientific claims [@problem_id:3507406].", "problem": "Consider a one-parameter Effective Field Theory (EFT) template for a binned cross-section measurement with Gaussian uncertainties. Let the predicted bin means be $m_i(\\theta) = b_i + s_i \\,\\theta$ for bins $i = 1, \\dots, N_{\\mathrm{bins}}$, where $\\theta \\ge 0$ is a dimensionless EFT coupling constrained by a physical boundary at zero, and $b_i, s_i$ are known bin-wise background and signal templates, respectively. The observed data are $x_i$, modeled as independent Gaussian random variables with means $m_i(\\theta_{\\mathrm{true}})$ and known standard deviations $\\sigma_i$. Define the chi-squared function for a given $\\theta$ and observation $x$ as\n$$\n\\chi^2(\\theta; x) \\;=\\; \\sum_{i=1}^{N_{\\mathrm{bins}}} \\frac{\\left(x_i - b_i - s_i \\,\\theta\\right)^2}{\\sigma_i^2}.\n$$\nThe profiled best-fit $\\hat{\\theta}$ minimizes $\\chi^2(\\theta;x)$ subject to the physical boundary $\\theta \\ge 0$. The likelihood ratio test statistic for a null parameter value $\\theta_0$ is the profiled difference\n$$\n\\Delta \\chi^2(\\theta_0; x) \\;=\\; \\chi^2(\\theta_0; x) \\;-\\; \\min_{\\theta \\ge 0} \\chi^2(\\theta; x).\n$$\nIn asymptotic regimes with regularity conditions, Wilks’ theorem states that, for a single parameter, $\\Delta \\chi^2$ is distributed as a chi-square distribution with $1$ degree of freedom. However, in small samples and when parameters lie near physical boundaries, the asymptotic prediction can fail. Your task is to empirically investigate the validity of Wilks’ theorem and to define corrected confidence regions by toy Monte Carlo.\n\nStarting from first principles and core definitions, implement the following steps:\n- Model specification: Use $N_{\\mathrm{bins}} = 3$ with dimensionless arrays $b = [10.0,12.0,8.0]$, $s = [4.0,6.0,3.0]$, and $\\sigma = [3.2,3.5,3.0]$. These are fixed across all tests.\n- Estimation: For any observation $x$, minimize $\\chi^2(\\theta;x)$ over $\\theta \\ge 0$ to obtain $\\hat{\\theta}$ and $\\chi^2_{\\min}(x) = \\chi^2(\\hat{\\theta}; x)$.\n- Test statistic: For a given $\\theta_0$, compute $\\Delta \\chi^2(\\theta_0;x)$.\n- Coverage study (Wilks): For each test case, generate $N_{\\mathrm{toys}}$ independent toys $x^{(t)}$ by drawing each $x_i^{(t)}$ from a Gaussian with mean $m_i(\\theta_{\\mathrm{true}})$ and standard deviation $\\sigma_i$. Compute $\\Delta \\chi^2(\\theta_{\\mathrm{true}}; x^{(t)})$ for each toy. Using the theoretical 95% quantile of the chi-square distribution with one degree of freedom, $q_{0.95}^{(\\chi^2_1)}$, compute the empirical coverage as the fraction of toys satisfying $\\Delta \\chi^2(\\theta_{\\mathrm{true}}; x^{(t)}) \\le q_{0.95}^{(\\chi^2_1)}$. Express the coverage as a decimal fraction.\n- Goodness-of-fit validity check: Using the toy distribution of $\\Delta \\chi^2(\\theta_{\\mathrm{true}}; x)$, perform a Kolmogorov–Smirnov (KS) test against the cumulative distribution function of $\\chi^2$ with one degree of freedom. Report whether the KS test fails to reject the Wilks hypothesis at significance level $0.05$; return a boolean.\n- Corrected confidence threshold: Define a corrected 95% threshold $q_{0.95}^{\\mathrm{corr}}$ as the empirical 95th percentile of the toy distribution of $\\Delta \\chi^2(\\theta_{\\mathrm{true}}; x)$.\n- Confidence interval construction: For a single observed dataset $\\tilde{x}$ in each test case, construct both the Wilks-based and corrected 95% confidence intervals for $\\theta$ as the set of $\\theta \\ge 0$ satisfying $\\chi^2(\\theta; \\tilde{x}) \\le \\chi^2_{\\min}(\\tilde{x}) + q$, for $q \\in \\{q_{0.95}^{(\\chi^2_1)}, q_{0.95}^{\\mathrm{corr}}\\}$. Report the interval endpoints as floats in the unit of the dimensionless coupling.\n\nUse the following test suite (each case uses dimensionless units):\n- Case 1 (boundary): $\\theta_{\\mathrm{true}} = 0.0$, $N_{\\mathrm{toys}} = 10000$, toy random seed $12345$, observed-dataset seed $54321$.\n- Case 2 (near boundary): $\\theta_{\\mathrm{true}} = 0.05$, $N_{\\mathrm{toys}} = 10000$, toy random seed $23456$, observed-dataset seed $65432$.\n- Case 3 (interior): $\\theta_{\\mathrm{true}} = 0.5$, $N_{\\mathrm{toys}} = 10000$, toy random seed $34567$, observed-dataset seed $76543$.\n\nYour program should compute, for each case:\n1. The empirical coverage using $q_{0.95}^{(\\chi^2_1)}$ (decimal fraction).\n2. The corrected threshold $q_{0.95}^{\\mathrm{corr}}$ (float).\n3. The KS test decision as a boolean indicating whether the Wilks hypothesis is not rejected at level $0.05$.\n4. The Wilks-based confidence interval endpoints $(\\theta_{\\mathrm{low}}^{\\mathrm{Wilks}}, \\theta_{\\mathrm{high}}^{\\mathrm{Wilks}})$ for the observed dataset.\n5. The corrected confidence interval endpoints $(\\theta_{\\mathrm{low}}^{\\mathrm{corr}}, \\theta_{\\mathrm{high}}^{\\mathrm{corr}})$ for the observed dataset.\n\nFinal output format:\n- Your program should produce a single line of output containing a list of three inner lists (one per test case). Each inner list must have seven entries in the exact order:\n$$\n[\\mathrm{coverage},\\; q_{0.95}^{\\mathrm{corr}},\\; \\mathrm{KS\\_pass},\\; \\theta_{\\mathrm{low}}^{\\mathrm{Wilks}},\\; \\theta_{\\mathrm{high}}^{\\mathrm{Wilks}},\\; \\theta_{\\mathrm{low}}^{\\mathrm{corr}},\\; \\theta_{\\mathrm{high}}^{\\mathrm{corr}}].\n$$\nFor example, the program should print a string of the form\n$$\n\\left[ [c_1,q_1,p_1,w\\ell_1,wh_1,cl_1,ch_1], [c_2,q_2,p_2,w\\ell_2,wh_2,cl_2,ch_2], [c_3,q_3,p_3,w\\ell_3,wh_3,cl_3,ch_3] \\right],\n$$\nwith numeric entries in dimensionless units and booleans for the KS decisions. No additional text should be printed.", "solution": "The problem requires a thorough statistical analysis of a binned cross-section measurement described by a one-parameter Effective Field Theory (EFT) model. The core tasks involve parameter estimation via chi-squared minimization, assessing the validity of Wilks' theorem using toy Monte Carlo simulations, and constructing corrected confidence intervals.\n\n### 1. Mathematical Model and $\\chi^2$ Function\n\nThe predicted number of events in bin $i$ is given by a linear function of the EFT coupling parameter $\\theta$:\n$$\nm_i(\\theta) = b_i + s_i \\theta\n$$\nwhere $b_i$ is the background prediction, $s_i$ is the signal scaling from the EFT operator, and $\\theta$ is the parameter of interest. The physical constraint $\\theta \\ge 0$ is imposed. The observed data $x_i$ in each bin are assumed to follow independent Gaussian distributions with means $m_i(\\theta_{\\mathrm{true}})$ and known standard deviations $\\sigma_i$.\n\nThe $\\chi^2$ function, which serves as the negative log-likelihood (up to constants) for this model, is defined as:\n$$\n\\chi^2(\\theta; x) = \\sum_{i=1}^{N_{\\mathrm{bins}}} \\frac{\\left(x_i - m_i(\\theta)\\right)^2}{\\sigma_i^2} = \\sum_{i=1}^{N_{\\mathrm{bins}}} \\frac{\\left(x_i - b_i - s_i \\theta\\right)^2}{\\sigma_i^2}\n$$\nThis function is a quadratic in $\\theta$ and can be expanded into the form $\\chi^2(\\theta) = A\\theta^2 - 2B\\theta + C$, where the coefficients $A$, $B$, and $C$ are functions of the data $x$ and the fixed templates:\n$$\nA = \\sum_{i=1}^{N_{\\mathrm{bins}}} \\frac{s_i^2}{\\sigma_i^2}\n$$\n$$\nB = \\sum_{i=1}^{N_{\\mathrm{bins}}} \\frac{s_i(x_i - b_i)}{\\sigma_i^2}\n$$\n$$\nC = \\sum_{i=1}^{N_{\\mathrm{bins}}} \\frac{(x_i - b_i)^2}{\\sigma_i^2}\n$$\nSince $A > 0$, the parabola opens upwards, guaranteeing a unique global minimum.\n\n### 2. Best-Fit Parameter Estimation\n\nTo find the value of $\\theta$ that minimizes $\\chi^2(\\theta; x)$, we first find the unconstrained minimum by setting the derivative to zero:\n$$\n\\frac{d\\chi^2}{d\\theta} = 2A\\theta - 2B = 0\n$$\nThis yields the unconstrained best-fit value, denoted $\\theta_{\\mathrm{uc}}$:\n$$\n\\theta_{\\mathrm{uc}}(x) = \\frac{B}{A} = \\frac{\\sum_i s_i(x_i - b_i)/\\sigma_i^2}{\\sum_i s_i^2/\\sigma_i^2}\n$$\nHowever, the problem specifies a physical boundary $\\theta \\ge 0$. The profiled best-fit estimate, $\\hat{\\theta}$, is the value of $\\theta$ within the allowed region that minimizes the $\\chi^2$. This is found by:\n$$\n\\hat{\\theta}(x) = \\max(0, \\theta_{\\mathrm{uc}}(x))\n$$\nIf the unconstrained minimum $\\theta_{\\mathrm{uc}}$ is physical (i.e., non-negative), it is the solution. If $\\theta_{\\mathrm{uc}}$ is negative, the minimum within the physical region $\\theta \\ge 0$ lies at the boundary, $\\hat{\\theta} = 0$. The minimum value of the $\\chi^2$ for a given dataset $x$ is $\\chi^2_{\\min}(x) = \\chi^2(\\hat{\\theta}(x); x)$.\n\n### 3. Likelihood Ratio Test Statistic\n\nThe test statistic for a null hypothesis $\\theta_0$ is the profiled likelihood ratio, expressed as a difference in $\\chi^2$ values:\n$$\n\\Delta \\chi^2(\\theta_0; x) = \\chi^2(\\theta_0; x) - \\chi^2_{\\min}(x) = \\chi^2(\\theta_0; x) - \\chi^2(\\hat{\\theta}; x)\n$$\nThis can be calculated efficiently. By completing the square, we can write $\\chi^2(\\theta) = A(\\theta - \\theta_{\\mathrm{uc}})^2 + \\chi^2(\\theta_{\\mathrm{uc}})$. Substituting this into the $\\Delta\\chi^2$ definition gives:\n$$\n\\Delta \\chi^2(\\theta_0; x) = \\left[A(\\theta_0 - \\theta_{\\mathrm{uc}})^2 + \\chi^2(\\theta_{\\mathrm{uc}})\\right] - \\left[A(\\hat{\\theta} - \\theta_{\\mathrm{uc}})^2 + \\chi^2(\\theta_{\\mathrm{uc}})\\right] = A \\left[ (\\theta_0 - \\theta_{\\mathrm{uc}})^2 - (\\hat{\\theta} - \\theta_{\\mathrm{uc}})^2 \\right]\n$$\nThis form is computationally efficient as it avoids recomputing the full $\\chi^2$ sums.\n\n### 4. Monte Carlo Simulation and Coverage Study\n\nWilks' theorem states that, under certain regularity conditions, the $\\Delta\\chi^2(\\theta_{\\mathrm{true}}; x)$ statistic is asymptotically distributed as a chi-square distribution with one degree of freedom ($\\chi^2_1$). These conditions are not met when the true parameter value, $\\theta_{\\mathrm{true}}$, lies on a boundary of the parameter space. We investigate this using toy Monte Carlo simulations.\n\nFor each test case defined by a $\\theta_{\\mathrm{true}}$:\n1.  Generate $N_{\\mathrm{toys}}$ pseudo-datasets (toys), $x^{(t)}$, by drawing each bin value $x_i^{(t)}$ from a Gaussian distribution $\\mathcal{N}(m_i(\\theta_{\\mathrm{true}}), \\sigma_i^2)$.\n2.  For each toy $x^{(t)}$, calculate $\\theta_{\\mathrm{uc}}(x^{(t)})$, $\\hat{\\theta}(x^{(t)})$, and the test statistic $\\Delta\\chi^2_t = \\Delta\\chi^2(\\theta_{\\mathrm{true}}; x^{(t)})$.\n3.  The collection of $N_{\\mathrm{toys}}$ values of $\\Delta\\chi^2_t$ forms an empirical probability distribution for the test statistic.\n\nThe empirical coverage is the fraction of toys for which the true parameter value is contained within the nominal 95% confidence interval. This corresponds to the fraction of toys where $\\Delta\\chi^2_t \\le q_{0.95}^{(\\chi^2_1)}$, where $q_{0.95}^{(\\chi^2_1)} \\approx 3.841$ is the 95th percentile of the $\\chi^2_1$ distribution. If Wilks' theorem holds, this coverage should be close to $0.95$.\n\n### 5. Hypothesis Testing and Corrected Threshold\n\nTo formally check the validity of Wilks' theorem, we perform a Kolmogorov-Smirnov (KS) test. The empirical cumulative distribution function (CDF) of the toy $\\Delta\\chi^2_t$ values is compared against the theoretical CDF of the $\\chi^2_1$ distribution. If the p-value of the test is less than the significance level $\\alpha=0.05$, we reject the null hypothesis that the data follows the $\\chi^2_1$ distribution.\n\nWhen Wilks' theorem fails, the nominal threshold $q_{0.95}^{(\\chi^2_1)}$ is incorrect. A corrected threshold, $q_{0.95}^{\\mathrm{corr}}$, can be determined empirically as the 95th percentile of the toy distribution of $\\Delta\\chi^2_t$. This ensures that the resulting confidence intervals will have the correct coverage (by construction).\n\n### 6. Confidence Interval Construction\n\nA confidence interval for $\\theta$ is constructed by inverting the hypothesis test. For an observed dataset $\\tilde{x}$, the 95% confidence interval is the set of $\\theta$ values that are not rejected at the 5% level. This is the set $\\{\\theta \\ge 0 \\mid \\Delta\\chi^2(\\theta; \\tilde{x}) \\le q \\}$, where $q$ is the chosen threshold ($q_{0.95}^{(\\chi^2_1)}$ for the Wilks-based interval, or $q_{0.95}^{\\mathrm{corr}}$ for the corrected interval). The condition can be written as:\n$$\n\\chi^2(\\theta; \\tilde{x}) \\le \\chi^2_{\\min}(\\tilde{x}) + q\n$$\nUsing the expression $\\Delta\\chi^2(\\theta; \\tilde{x}) = A [ (\\theta - \\theta_{\\mathrm{uc}})^2 - (\\hat{\\theta} - \\theta_{\\mathrm{uc}})^2 ]$, the inequality becomes:\n$$\nA [ (\\theta - \\theta_{\\mathrm{uc}})^2 - (\\hat{\\theta} - \\theta_{\\mathrm{uc}})^2 ] \\le q\n$$\n$$\n(\\theta - \\theta_{\\mathrm{uc}})^2 \\le (\\hat{\\theta} - \\theta_{\\mathrm{uc}})^2 + \\frac{q}{A}\n$$\nThis inequality defines a symmetric interval around $\\theta_{\\mathrm{uc}}$:\n$$\n\\theta_{\\mathrm{uc}} - \\delta \\le \\theta \\le \\theta_{\\mathrm{uc}} + \\delta\n$$\nwhere $\\delta = \\sqrt{(\\hat{\\theta} - \\theta_{\\mathrm{uc}})^2 + q/A}$. Taking into account the physical boundary $\\theta \\ge 0$, the final confidence interval is $[\\theta_{\\mathrm{low}}, \\theta_{\\mathrm{high}}]$:\n$$\n\\theta_{\\mathrm{low}} = \\max\\left(0, \\theta_{\\mathrm{uc}} - \\delta\\right)\n$$\n$$\n\\theta_{\\mathrm{high}} = \\theta_{\\mathrm{uc}} + \\delta\n$$\nThis procedure is applied to the generated \"observed\" dataset $\\tilde{x}$ for each test case, using both the Wilks and corrected thresholds.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import chi2, kstest\n\ndef solve():\n    \"\"\"\n    Performs a statistical analysis of an EFT model, including chi-squared minimization,\n    coverage studies via toy MC, and confidence interval construction.\n    \"\"\"\n    # Fixed model parameters (dimensionless)\n    b = np.array([10.0, 12.0, 8.0])\n    s = np.array([4.0, 6.0, 3.0])\n    sigma = np.array([3.2, 3.5, 3.0])\n\n    # Pre-calculate the coefficient 'A' of the quadratic chi-squared function,\n    # as it is independent of the data.\n    # chi^2(theta) = A*theta^2 - 2*B*theta + C\n    A = np.sum((s / sigma)**2)\n\n    # Theoretical 95% quantile of the chi-square distribution with 1 d.o.f.\n    # This is the threshold 'q' used for Wilks-based intervals.\n    q_wilks = chi2.ppf(0.95, 1)\n\n    # Test cases defined in the problem statement\n    test_cases = [\n        # (theta_true, N_toys, toy_seed, observed_data_seed)\n        (0.0, 10000, 12345, 54321),\n        (0.05, 10000, 23456, 65432),\n        (0.5, 10000, 34567, 76543),\n    ]\n\n    all_results = []\n    \n    for theta_true, N_toys, toy_seed, obs_seed in test_cases:\n        # --- Part 1: Toy Monte Carlo Simulation to study Delta chi^2 distribution ---\n        rng_toy = np.random.default_rng(toy_seed)\n        \n        # Calculate true means for toy generation\n        m_true = b + s * theta_true\n        \n        delta_chi2_toys = np.zeros(N_toys)\n        \n        for t in range(N_toys):\n            # Generate a toy dataset x by sampling from Gaussian distributions\n            x_toy = rng_toy.normal(loc=m_true, scale=sigma)\n            \n            # For the toy data, find the best-fit parameter theta_hat\n            # B_toy is the coefficient of the linear term in the chi-squared function\n            B_toy = np.sum(s * (x_toy - b) / sigma**2)\n            theta_uc_toy = B_toy / A  # Unconstrained best-fit\n            theta_hat_toy = np.maximum(0.0, theta_uc_toy) # Constrained best-fit\n\n            # Calculate the test statistic Delta chi^2(theta_true; x_toy)\n            # Delta_chi2 = chi2(theta_true) - chi2(theta_hat)\n            # This is equivalent to A * ((theta_true - theta_uc)^2 - (theta_hat - theta_uc)^2)\n            delta_chi2 = A * ((theta_true - theta_uc_toy)**2 - (theta_hat_toy - theta_uc_toy)**2)\n            delta_chi2_toys[t] = delta_chi2\n\n        # Filter out any small negative values from floating point inaccuracies\n        delta_chi2_toys = np.maximum(0, delta_chi2_toys)\n\n        # --- Part 2: Analyze the toy distribution ---\n\n        # 1. Empirical Coverage using Wilks' threshold\n        coverage = np.sum(delta_chi2_toys <= q_wilks) / N_toys\n        \n        # 2. Corrected 95% Threshold (q_corr)\n        q_corr = np.percentile(delta_chi2_toys, 95)\n        \n        # 3. KS test for Wilks' hypothesis validity\n        # Null hypothesis: the toy delta_chi2 distribution is a chi2 with 1 d.o.f.\n        ks_statistic, p_value = kstest(delta_chi2_toys, 'chi2', args=(1,))\n        ks_pass = p_value > 0.05  # True if we fail to reject the null at alpha=0.05\n\n        # --- Part 3: Confidence Interval for a single \"observed\" dataset ---\n        \n        rng_obs = np.random.default_rng(obs_seed)\n        x_obs = rng_obs.normal(loc=m_true, scale=sigma)\n        \n        # Find best-fit theta for the observed data\n        B_obs = np.sum(s * (x_obs - b) / sigma**2)\n        theta_uc_obs = B_obs / A\n        theta_hat_obs = np.maximum(0.0, theta_uc_obs)\n\n        def get_interval(q_val, theta_uc, theta_hat):\n            \"\"\"Calculates the confidence interval for a given threshold q.\"\"\"\n            # The interval is defined by chi2(theta) <= chi2_min + q, which solves to:\n            # |theta - theta_uc|^2 <= |theta_hat - theta_uc|^2 + q/A\n            delta = np.sqrt((theta_hat - theta_uc)**2 + q_val / A)\n            low = np.maximum(0.0, theta_uc - delta)\n            high = theta_uc + delta\n            return low, high\n\n        # 4. Wilks-based confidence interval\n        wilks_low, wilks_high = get_interval(q_wilks, theta_uc_obs, theta_hat_obs)\n        \n        # 5. Corrected confidence interval\n        corr_low, corr_high = get_interval(q_corr, theta_uc_obs, theta_hat_obs)\n        \n        # Assemble results for this case in the specified order\n        case_results = [\n            coverage, q_corr, ks_pass, \n            wilks_low, wilks_high, \n            corr_low, corr_high\n        ]\n        all_results.append(case_results)\n\n    # Convert boolean to lower case for consistency if needed, but python default is fine.\n    # Format the final output string as a list of lists.\n    # str([1, 2, True]) -> '[1, 2, True]', which is a valid representation.\n    final_output_str = \"[\" + \",\".join([str(res) for res in all_results]) + \"]\"\n    \n    # Print the final result in the exact required format.\n    print(final_output_str)\n\nsolve()\n```", "id": "3507406"}]}