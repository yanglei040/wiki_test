{"hands_on_practices": [{"introduction": "Understanding the Receiver Operating Characteristic (ROC) curve begins with grasping its fundamental connection to the underlying probability distributions of a classifier's score. This exercise provides a first-principles derivation of an ROC curve and its Area Under the Curve (AUC) for a hypothetical scenario where signal and background scores follow Laplace distributions [@problem_id:3529673]. By working through this analytic case, you will build a strong intuition for how the separation between signal and background score distributions mathematically determines the shape and discriminatory power of the resulting classifier.", "problem": "In a binary classification task for protonâ€“proton collision events in computational high-energy physics, a scalar score $S$ is used to separate a signal hypothesis (e.g., Higgs boson production) from a background hypothesis (e.g., Quantum Chromodynamics multijet). Assume that, after a physics-motivated calibration, the conditional distributions of $S$ under the two hypotheses are Laplace with a common scale parameter $b>0$ but different means: the signal score follows $S \\mid \\text{signal} \\sim \\mathrm{Laplace}(\\mu_{s}, b)$ and the background score follows $S \\mid \\text{background} \\sim \\mathrm{Laplace}(\\mu_{b}, b)$, with $\\mu_{s} > \\mu_{b}$. The Laplace probability density function is $f(s \\mid \\mu, b) = \\frac{1}{2 b} \\exp\\!\\left(-\\frac{|s - \\mu|}{b}\\right)$.\n\nA decision rule classifies an event as signal if $S \\geq t$, where $t \\in \\mathbb{R}$ is a threshold. Starting only from the definitions of true positive rate $\\,\\mathrm{TPR}(t) = \\mathbb{P}(S \\geq t \\mid \\text{signal})\\,$ and false positive rate $\\,\\mathrm{FPR}(t) = \\mathbb{P}(S \\geq t \\mid \\text{background})\\,$, and the Laplace distribution specified above:\n\n1. Derive the parametric receiver operating characteristic (ROC) curve given by $\\,(\\mathrm{FPR}(t), \\mathrm{TPR}(t))\\,$ as $t$ varies over $\\mathbb{R}$. Your derivation must account for all regimes of $t$ relative to $\\mu_{b}$ and $\\mu_{s}$ and express the ROC as functions of $\\,\\mathrm{FPR}\\,$ and $\\,\\mathrm{TPR}\\,$ without leaving unevaluated probabilities.\n2. Characterize the concavity of this ROC by computing $\\,\\frac{d\\,\\mathrm{TPR}}{d\\,\\mathrm{FPR}}\\,$ and $\\,\\frac{d^{2}\\,\\mathrm{TPR}}{d\\,\\mathrm{FPR}^{2}}\\,$ wherever they are defined, and determining the sign of the second derivative.\n3. Compute the exact area under the ROC curve (AUC) as a closed-form analytic expression in terms of $\\Delta \\equiv \\mu_{s} - \\mu_{b} > 0$ and $b > 0$.\n\nExpress your final answer as the exact closed-form expression for the AUC in terms of $\\Delta$ and $b$.", "solution": "The problem statement is validated as scientifically grounded, well-posed, and objective. It is a standard problem in statistical decision theory applied to a plausible scenario in high-energy physics. All necessary information is provided, and the problem is free of contradictions or vagueness. We proceed to the solution.\n\nThe problem requires the derivation of the receiver operating characteristic (ROC) curve, an analysis of its concavity, and the calculation of the area under the curve (AUC) for a binary classifier based on a score $S$. The score $S$ follows a Laplace distribution for both signal and background hypotheses, with a common scale parameter $b$ but different means $\\mu_{s}$ and $\\mu_{b}$ (where $\\mu_{s} > \\mu_{b}$).\n\nThe probability density function (PDF) of a Laplace distribution is given by $f(s \\mid \\mu, b) = \\frac{1}{2 b} \\exp\\left(-\\frac{|s - \\mu|}{b}\\right)$. The corresponding cumulative distribution function (CDF), $F(s \\mid \\mu, b) = \\int_{-\\infty}^{s} f(x \\mid \\mu, b) dx$, is:\n$$ F(s \\mid \\mu, b) = \\begin{cases} \\frac{1}{2} \\exp\\left(\\frac{s - \\mu}{b}\\right) & \\text{if } s \\leq \\mu \\\\ 1 - \\frac{1}{2} \\exp\\left(-\\frac{s - \\mu}{b}\\right) & \\text{if } s > \\mu \\end{cases} $$\n\nThe true positive rate ($\\mathrm{TPR}$) and false positive rate ($\\mathrm{FPR}$) are defined as functions of the decision threshold $t$:\n$\\mathrm{TPR}(t) = \\mathbb{P}(S \\geq t \\mid \\text{signal}) = 1 - F(t \\mid \\mu_{s}, b)$\n$\\mathrm{FPR}(t) = \\mathbb{P}(S \\geq t \\mid \\text{background}) = 1 - F(t \\mid \\mu_{b}, b)$\n\nUsing the CDF, we can write the explicit expressions for $\\mathrm{TPR}(t)$ and $\\mathrm{FPR}(t)$:\n$$ \\mathrm{TPR}(t) = \\begin{cases} 1 - \\frac{1}{2} \\exp\\left(\\frac{t - \\mu_{s}}{b}\\right) & \\text{if } t \\leq \\mu_{s} \\\\ \\frac{1}{2} \\exp\\left(-\\frac{t - \\mu_{s}}{b}\\right) & \\text{if } t > \\mu_{s} \\end{cases} $$\n$$ \\mathrm{FPR}(t) = \\begin{cases} 1 - \\frac{1}{2} \\exp\\left(\\frac{t - \\mu_{b}}{b}\\right) & \\text{if } t \\leq \\mu_{b} \\\\ \\frac{1}{2} \\exp\\left(-\\frac{t - \\mu_{b}}{b}\\right) & \\text{if } t > \\mu_{b} \\end{cases} $$\n\n**1. Derivation of the Parametric ROC Curve**\n\nThe ROC curve is the set of points $(\\mathrm{FPR}(t), \\mathrm{TPR}(t))$ for $t \\in (-\\infty, \\infty)$. We denote $\\mathrm{FPR}$ by $x$ and $\\mathrm{TPR}$ by $y$. To find the relationship $y(x)$, we eliminate the parameter $t$. Since $\\mu_{s} > \\mu_{b}$, we consider three regimes for $t$. Let $\\Delta \\equiv \\mu_{s} - \\mu_{b} > 0$.\n\n**Regime 1: $t > \\mu_{s}$** (which implies $t > \\mu_{b}$)\nIn this case:\n$x = \\frac{1}{2} \\exp\\left(-\\frac{t - \\mu_{b}}{b}\\right)$\n$y = \\frac{1}{2} \\exp\\left(-\\frac{t - \\mu_{s}}{b}\\right)$\nFrom the expression for $y$, we have $2y = \\exp(-t/b) \\exp(\\mu_{s}/b)$, which gives $\\exp(-t/b) = 2y \\exp(-\\mu_{s}/b)$.\nSubstituting this into the expression for $x$:\n$x = \\frac{1}{2} \\left[2y \\exp\\left(-\\frac{\\mu_{s}}{b}\\right)\\right] \\exp\\left(\\frac{\\mu_{b}}{b}\\right) = y \\exp\\left(\\frac{\\mu_{b} - \\mu_{s}}{b}\\right) = y \\exp\\left(-\\frac{\\Delta}{b}\\right)$.\nThus, $y = x \\exp\\left(\\frac{\\Delta}{b}\\right)$.\nThe range of $x$ for this regime is found by considering the limits for $t$. As $t \\to \\infty$, $x \\to 0$. When $t=\\mu_s$, $x = \\frac{1}{2} \\exp\\left(-\\frac{\\mu_s - \\mu_b}{b}\\right) = \\frac{1}{2} \\exp\\left(-\\frac{\\Delta}{b}\\right)$.\nSo, this segment applies for $x \\in \\left[0, \\frac{1}{2} \\exp\\left(-\\frac{\\Delta}{b}\\right)\\right]$.\n\n**Regime 2: $\\mu_{b} < t \\leq \\mu_{s}$**\nIn this case:\n$x = \\frac{1}{2} \\exp\\left(-\\frac{t - \\mu_{b}}{b}\\right)$\n$y = 1 - \\frac{1}{2} \\exp\\left(\\frac{t - \\mu_{s}}{b}\\right)$\nFrom the expression for $x$, $2x = \\exp(-t/b) \\exp(\\mu_{b}/b)$, so $\\exp(t/b) = \\frac{1}{2x} \\exp(\\mu_{b}/b)$.\nSubstitute this into the expression for $y$:\n$y = 1 - \\frac{1}{2} \\left[\\frac{1}{2x} \\exp\\left(\\frac{\\mu_{b}}{b}\\right)\\right] \\exp\\left(-\\frac{\\mu_{s}}{b}\\right) = 1 - \\frac{1}{4x} \\exp\\left(\\frac{\\mu_{b} - \\mu_{s}}{b}\\right)$.\nThus, $y = 1 - \\frac{1}{4x} \\exp\\left(-\\frac{\\Delta}{b}\\right)$.\nThe range of $x$ is from $t=\\mu_s$ (lower bound on $x$, as found before) to $t=\\mu_b$. At $t=\\mu_b$, $x = \\frac{1}{2}\\exp(0) = \\frac{1}{2}$.\nSo, this segment applies for $x \\in \\left(\\frac{1}{2} \\exp\\left(-\\frac{\\Delta}{b}\\right), \\frac{1}{2}\\right]$.\n\n**Regime 3: $t \\leq \\mu_{b}$** (which implies $t < \\mu_{s}$)\nIn this case:\n$x = 1 - \\frac{1}{2} \\exp\\left(\\frac{t - \\mu_{b}}{b}\\right)$\n$y = 1 - \\frac{1}{2} \\exp\\left(\\frac{t - \\mu_{s}}{b}\\right)$\nFrom the expression for $x$, $2(1-x) = \\exp(t/b) \\exp(-\\mu_{b}/b)$, so $\\exp(t/b) = 2(1-x) \\exp(\\mu_{b}/b)$.\nSubstitute this into the expression for $y$:\n$y = 1 - \\frac{1}{2} \\left[2(1-x) \\exp\\left(\\frac{\\mu_{b}}{b}\\right)\\right] \\exp\\left(-\\frac{\\mu_{s}}{b}\\right) = 1 - (1-x) \\exp\\left(\\frac{\\mu_{b} - \\mu_{s}}{b}\\right)$.\nThus, $y = 1 - (1-x) \\exp\\left(-\\frac{\\Delta}{b}\\right)$.\nThe range of $x$ is from $t=\\mu_b$ ($x=1/2$) to $t \\to -\\infty$ ($x \\to 1$).\nSo, this segment applies for $x \\in \\left(\\frac{1}{2}, 1\\right]$.\n\nThe complete ROC curve $y(x) = \\mathrm{TPR}(\\mathrm{FPR})$ is:\n$$ y(x) = \\begin{cases} x \\exp\\left(\\frac{\\Delta}{b}\\right) & \\text{if } 0 \\leq x \\leq \\frac{1}{2} \\exp\\left(-\\frac{\\Delta}{b}\\right) \\\\ 1 - \\frac{1}{4x} \\exp\\left(-\\frac{\\Delta}{b}\\right) & \\text{if } \\frac{1}{2} \\exp\\left(-\\frac{\\Delta}{b}\\right) < x \\leq \\frac{1}{2} \\\\ 1 - (1-x) \\exp\\left(-\\frac{\\Delta}{b}\\right) & \\text{if } \\frac{1}{2} < x \\leq 1 \\end{cases} $$\n\n**2. Characterization of Concavity**\n\nWe compute the first and second derivatives of $y(x)$ with respect to $x$ in each regime.\nFor $x \\in \\left(0, \\frac{1}{2} \\exp\\left(-\\frac{\\Delta}{b}\\right)\\right)$:\n$\\frac{dy}{dx} = \\exp\\left(\\frac{\\Delta}{b}\\right)$\n$\\frac{d^2y}{dx^2} = 0$\n\nFor $x \\in \\left(\\frac{1}{2} \\exp\\left(-\\frac{\\Delta}{b}\\right), \\frac{1}{2}\\right)$:\n$\\frac{dy}{dx} = \\frac{d}{dx} \\left(1 - \\frac{1}{4} \\exp\\left(-\\frac{\\Delta}{b}\\right) x^{-1}\\right) = \\frac{1}{4x^2} \\exp\\left(-\\frac{\\Delta}{b}\\right)$\n$\\frac{d^2y}{dx^2} = \\frac{d}{dx} \\left(\\frac{1}{4} \\exp\\left(-\\frac{\\Delta}{b}\\right) x^{-2}\\right) = -\\frac{1}{2x^3} \\exp\\left(-\\frac{\\Delta}{b}\\right)$\n\nFor $x \\in \\left(\\frac{1}{2}, 1\\right)$:\n$\\frac{dy}{dx} = \\frac{d}{dx} \\left(1 - \\exp\\left(-\\frac{\\Delta}{b}\\right) + x \\exp\\left(-\\frac{\\Delta}{b}\\right)\\right) = \\exp\\left(-\\frac{\\Delta}{b}\\right)$\n$\\frac{d^2y}{dx^2} = 0$\n\nThe first derivative $\\frac{dy}{dx}$ is continuous across the transition points. The second derivative $\\frac{d^2y}{dx^2}$ is $0$ on the two linear segments and strictly negative on the central segment, since $x>0$, $b>0$, and $\\Delta>0$. A function with a non-positive second derivative (where defined) is concave. The ROC curve is composed of two line segments connected by a strictly concave curve, making the overall curve concave. This is consistent with the general property of ROC curves derived from a likelihood ratio test, which this classifier represents.\n\n**3. Computation of the Area Under the Curve (AUC)**\n\nThe AUC is the integral of the ROC function $y(x)$ from $x=0$ to $x=1$. We split the integral into the three regimes identified above. Let $x_1 = \\frac{1}{2} \\exp\\left(-\\frac{\\Delta}{b}\\right)$ and $x_2 = \\frac{1}{2}$.\n$\\mathrm{AUC} = \\int_{0}^{1} y(x) dx = \\int_{0}^{x_1} y(x) dx + \\int_{x_1}^{x_2} y(x) dx + \\int_{x_2}^{1} y(x) dx$.\n\nIntegral 1:\n$\\int_{0}^{x_1} x \\exp\\left(\\frac{\\Delta}{b}\\right) dx = \\exp\\left(\\frac{\\Delta}{b}\\right) \\left[\\frac{x^2}{2}\\right]_{0}^{x_1} = \\frac{1}{2} \\exp\\left(\\frac{\\Delta}{b}\\right) \\left(\\frac{1}{2} \\exp\\left(-\\frac{\\Delta}{b}\\right)\\right)^2 = \\frac{1}{8} \\exp\\left(\\frac{\\Delta}{b}\\right) \\exp\\left(-\\frac{2\\Delta}{b}\\right) = \\frac{1}{8} \\exp\\left(-\\frac{\\Delta}{b}\\right)$.\n\nIntegral 2:\n$\\int_{x_1}^{x_2} \\left(1 - \\frac{1}{4x} \\exp\\left(-\\frac{\\Delta}{b}\\right)\\right) dx = \\left[x - \\frac{1}{4}\\exp\\left(-\\frac{\\Delta}{b}\\right) \\ln(x)\\right]_{x_1}^{x_2}$\n$= \\left(\\frac{1}{2} - \\frac{1}{4}\\exp\\left(-\\frac{\\Delta}{b}\\right) \\ln\\left(\\frac{1}{2}\\right)\\right) - \\left(\\frac{1}{2}\\exp\\left(-\\frac{\\Delta}{b}\\right) - \\frac{1}{4}\\exp\\left(-\\frac{\\Delta}{b}\\right) \\ln\\left(\\frac{1}{2}\\exp\\left(-\\frac{\\Delta}{b}\\right)\\right)\\right)$\nUsing $\\ln(ab) = \\ln(a) + \\ln(b)$ and $\\ln(\\exp(-\\Delta/b)) = -\\Delta/b$:\n$= \\frac{1}{2} - \\frac{1}{2}\\exp\\left(-\\frac{\\Delta}{b}\\right) + \\frac{1}{4}\\exp\\left(-\\frac{\\Delta}{b}\\right) \\left( -\\ln\\left(\\frac{1}{2}\\right) + \\ln\\left(\\frac{1}{2}\\right) - \\frac{\\Delta}{b} \\right)$\n$= \\frac{1}{2} - \\frac{1}{2}\\exp\\left(-\\frac{\\Delta}{b}\\right) - \\frac{\\Delta}{4b}\\exp\\left(-\\frac{\\Delta}{b}\\right)$.\n\nIntegral 3:\n$\\int_{x_2}^{1} \\left(1 - (1-x) \\exp\\left(-\\frac{\\Delta}{b}\\right)\\right) dx = \\left[x + \\frac{(1-x)^2}{2} \\exp\\left(-\\frac{\\Delta}{b}\\right)\\right]_{1/2}^{1}$\n$= (1 + 0) - \\left(\\frac{1}{2} + \\frac{(1-1/2)^2}{2} \\exp\\left(-\\frac{\\Delta}{b}\\right)\\right) = 1 - \\frac{1}{2} - \\frac{1}{8} \\exp\\left(-\\frac{\\Delta}{b}\\right) = \\frac{1}{2} - \\frac{1}{8} \\exp\\left(-\\frac{\\Delta}{b}\\right)$.\n\nSumming the three integrals:\n$\\mathrm{AUC} = \\left(\\frac{1}{8} \\exp\\left(-\\frac{\\Delta}{b}\\right)\\right) + \\left(\\frac{1}{2} - \\frac{1}{2}\\exp\\left(-\\frac{\\Delta}{b}\\right) - \\frac{\\Delta}{4b}\\exp\\left(-\\frac{\\Delta}{b}\\right)\\right) + \\left(\\frac{1}{2} - \\frac{1}{8} \\exp\\left(-\\frac{\\Delta}{b}\\right)\\right)$\n$\\mathrm{AUC} = (\\frac{1}{2}+\\frac{1}{2}) + \\left(\\frac{1}{8} - \\frac{1}{2} - \\frac{1}{8}\\right)\\exp\\left(-\\frac{\\Delta}{b}\\right) - \\frac{\\Delta}{4b}\\exp\\left(-\\frac{\\Delta}{b}\\right)$\n$\\mathrm{AUC} = 1 - \\frac{1}{2}\\exp\\left(-\\frac{\\Delta}{b}\\right) - \\frac{\\Delta}{4b}\\exp\\left(-\\frac{\\Delta}{b}\\right)$\n$\\mathrm{AUC} = 1 - \\left(\\frac{1}{2} + \\frac{\\Delta}{4b}\\right) \\exp\\left(-\\frac{\\Delta}{b}\\right)$.\n\nThis expression gives the exact area under the ROC curve in terms of the separation of means $\\Delta = \\mu_s - \\mu_b$ and the common scale parameter $b$.", "answer": "$$\n\\boxed{1 - \\left(\\frac{1}{2} + \\frac{\\Delta}{4b}\\right) \\exp\\left(-\\frac{\\Delta}{b}\\right)}\n$$", "id": "3529673"}, {"introduction": "In many scientific applications, particularly in high-energy physics searches for rare signals, the overall performance of a classifier is less important than its performance in the specific regime of very low background contamination. This practice moves from theory to application by challenging you to implement the partial Area Under the Curve (pAUC), a metric that focuses evaluation on a constrained False Positive Rate (FPR) range [@problem_id:3529666]. Mastering the computation of pAUC from empirical, weighted data is a crucial skill for realistically assessing a classifier's utility in environments where background rejection is critical.", "problem": "You are analyzing binary classifiers used in High-Energy Physics (HEP) searches where the signal yield is extremely small and the background rate must be suppressed to ultra-low leakage. In such settings, Receiver Operating Characteristic (ROC) analysis focuses on the region of very small False Positive Rate (FPR) because triggers and selection pipelines must keep the background acceptance near zero while maximizing the discovery potential. Starting from foundational definitions of binary classification under thresholding, derive a mathematically rigorous definition of the partial Area Under the Curve (partial AUC) restricted to the interval of False Positive Rate within $[0,\\alpha]$ for a given $0 \\le \\alpha \\le 1$, and justify why this quantity captures performance in the ultra-low background regime. Then, construct an algorithm to compute this partial AUC from empirical ROC points generated by finite samples of scored events, including support for per-event nonnegative sample weights representing expected yields or livetime scaling factors. Your derivation must start from the definitions of True Positive Rate (TPR) and False Positive Rate (FPR) under thresholded classification and proceed to an integral expression over the ROC curve restricted to $[0,\\alpha]$ without assuming any pre-given formulas.\n\nDefine the following base quantities explicitly and use them consistently:\n\n- A scored dataset consisting of $n$ events indexed by $i \\in \\{1,\\dots,n\\}$, each with a real-valued score $s_i \\in \\mathbb{R}$, a binary label $y_i \\in \\{0,1\\}$ (where $y_i = 1$ denotes signal and $y_i = 0$ denotes background), and an optional nonnegative sample weight $w_i \\in [0,\\infty)$.\n- For any threshold $\\tau \\in \\mathbb{R}$, a classifier that predicts $\\hat{y}_i(\\tau) = 1$ if $s_i \\ge \\tau$ and $\\hat{y}_i(\\tau) = 0$ otherwise.\n- The weighted counts $P = \\sum_{i=1}^n w_i \\,\\mathbb{1}\\{y_i = 1\\}$ and $N = \\sum_{i=1}^n w_i \\,\\mathbb{1}\\{y_i = 0\\}$, assumed finite and strictly positive for meaningful rates, and the corresponding rates\n$$\nTPR(\\tau) = \\frac{\\sum_{i=1}^n w_i \\,\\mathbb{1}\\{y_i = 1,\\, s_i \\ge \\tau\\}}{P}, \\qquad\nFPR(\\tau) = \\frac{\\sum_{i=1}^n w_i \\,\\mathbb{1}\\{y_i = 0,\\, s_i \\ge \\tau\\}}{N}.\n$$\n\nYour program must implement the following:\n\n1. Construct empirical ROC points by sweeping $\\tau$ from $+\\infty$ down to $-\\infty$ over the set of distinct score values $\\{s_i\\}$, aggregating tied scores at identical thresholds. Use the weighted definitions to form a nondecreasing sequence of $FPR$ values in $[0,1]$ and the corresponding $TPR$ values in $[0,1]$, starting at $(0,0)$ and ending at $(1,1)$.\n2. Define the partial AUC over the interval $FPR \\in [0,\\alpha]$ from first principles as the area under the curve traced by the function $TPR(FPR)$ when restricted to $[0,\\alpha]$, and compute it from the empirical ROC points via a mathematically consistent integration procedure that treats the finite-sample ROC as piecewise linear between consecutive empirical points and clips integrals to the interval $[0,\\alpha]$.\n3. Handle ties in scores (multiple events sharing the same $s_i$) by aggregating their contributions at the same threshold before updating rates, and support optional sample weights $w_i$; if weights are not provided, take $w_i = 1$ for all $i$.\n4. Validate that $0 \\le \\alpha \\le 1$; if $P = 0$ or $N = 0$, the rates are ill-defined, but such cases will not appear in the test suite below.\n\nRelevance requirement: Based on the above first-principles definition, explain why the partial AUC concentrated on $FPR \\in [0,\\alpha]$ is the correct figure of merit for searches constrained to ultra-low background leakage (that is, small $FPR$), and why integrating only over that region faithfully reflects discovery sensitivity in the operational regime of HEP triggers and selections.\n\nTest suite and required output: Your program must compute the partial AUC for each of the following parameter sets. For clarity, each list below is ordered; all numbers are given explicitly as decimals, and the same order must be reproduced in the code.\n\n- Case 1 (balanced, moderate separation, unweighted): scores [5.1, 4.9, 3.8, 3.5, 2.0, 4.7, 3.9, 3.2, 1.9, 1.0, 0.3, -0.5], labels [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0], $\\alpha = 0.1$, no weights.\n- Case 2 (same data, boundary $\\alpha = 0$): scores [5.1, 4.9, 3.8, 3.5, 2.0, 4.7, 3.9, 3.2, 1.9, 1.0, 0.3, -0.5], labels [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0], $\\alpha = 0.0$, no weights.\n- Case 3 (ties present, full AUC with $\\alpha = 1$): scores [0.8, 0.8, 0.2, -0.1, 0.8, 0.5, 0.0, -0.2, -0.2], labels [1, 1, 1, 1, 0, 0, 0, 0, 0], $\\alpha = 1.0$, no weights.\n- Case 4 (weighted, ultra-low leakage focus): scores [3.2, 2.9, 1.7, 3.1, 2.5, 0.7, 0.2], labels [1, 1, 1, 0, 0, 0, 0], weights [1.0, 0.8, 0.5, 1000.0, 200.0, 50.0, 10.0], $\\alpha = 0.001$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[r_1,r_2,r_3,r_4]$), where $r_k$ is the partial AUC for case $k$ expressed as a decimal number. No other text should be printed.", "solution": "The problem requires the derivation of a rigorous definition for the partial Area Under the Curve (pAUC), its justification as a key metric in High-Energy Physics (HEP), and the construction of an algorithm to compute it from empirical data. The derivation and algorithm must be based on first principles, starting from the provided definitions of a threshold-based binary classifier.\n\nLet the dataset consist of $n$ events, each described by a tuple $(s_i, y_i, w_i)$ for $i \\in \\{1,\\dots,n\\}$, where $s_i \\in \\mathbb{R}$ is the classifier score, $y_i \\in \\{0,1\\}$ is the true label ($1$ for signal, $0$ for background), and $w_i \\in [0,\\infty)$ is a nonnegative event weight. If weights are not provided, we assume $w_i=1$ for all $i$. The total weighted sum of signal events is $P = \\sum_{i: y_i=1} w_i$ and the total weighted sum of background events is $N = \\sum_{i: y_i=0} w_i$. Both $P$ and $N$ are assumed to be strictly positive.\n\nA decision threshold $\\tau \\in \\mathbb{R}$ classifies events as positive if their score $s_i \\ge \\tau$. The True Positive Rate (TPR) and False Positive Rate (FPR) at this threshold are defined as:\n$$ TPR(\\tau) = \\frac{\\sum_{i=1}^n w_i \\,\\mathbb{1}\\{y_i = 1,\\, s_i \\ge \\tau\\}}{P} $$\n$$ FPR(\\tau) = \\frac{\\sum_{i=1}^n w_i \\,\\mathbb{1}\\{y_i = 0,\\, s_i \\ge \\tau\\}}{N} $$\nwhere $\\mathbb{1}\\{\\cdot\\}$ is the indicator function.\n\nThe Receiver Operating Characteristic (ROC) curve is the set of points $(FPR(\\tau), TPR(\\tau))$ parameterized by $\\tau$ as it sweeps from $+\\infty$ to $-\\infty$. As $\\tau$ decreases, both $FPR(\\tau)$ and $TPR(\\tau)$ are non-decreasing, tracing a path in the unit square from $(0,0)$ to $(1,1)$. The ROC curve can be viewed as a function $TPR(FPR)$.\n\nThe standard Area Under the Curve (AUC) is the integral of this function over its entire domain:\n$$ AUC = \\int_{0}^{1} TPR(FPR) \\, d(FPR) $$\nThe partial AUC (pAUC), restricted to the False Positive Rate interval $[0, \\alpha]$ for a given $\\alpha \\in [0, 1]$, is the natural extension of this definition to a limited domain:\n$$ \\text{pAUC}(\\alpha) = \\int_{0}^{\\alpha} TPR(FPR) \\, d(FPR) $$\n\nFor a finite dataset, the functions $TPR(\\tau)$ and $FPR(\\tau)$ are step functions that change values only at the scores $s_i$ present in the data. To construct the empirical ROC curve, we consider the set of unique scores $\\{\\tau_1, \\tau_2, \\dots, \\tau_m\\}$ sorted in descending order, $\\tau_1 > \\tau_2 > \\dots > \\tau_m$. We can define a sequence of ROC points $(x_j, y_j)$ for $j \\in \\{0, 1, \\dots, m\\}$. The starting point is $(x_0, y_0) = (0,0)$, corresponding to a threshold $\\tau > \\tau_1$.\n\nFor each unique score $\\tau_j$, we aggregate the weights of all signal and background events with that score. Let $\\Delta P_{w,j} = \\sum_{i: s_i=\\tau_j, y_i=1} w_i$ and $\\Delta N_{w,j} = \\sum_{i: s_i=\\tau_j, y_i=0} w_i$. We define cumulative weighted counts of true positives ($TP_w$) and false positives ($FP_w$) by summing these contributions as the threshold is lowered. The $j$-th ROC point $(x_j, y_j)$ is generated by considering all scores greater than or equal to $\\tau_j$:\n$$ TP_{w,j} = \\sum_{k=1}^{j} \\Delta P_{w,k} \\qquad FP_{w,j} = \\sum_{k=1}^{j} \\Delta N_{w,k} $$\n$$ x_j = \\frac{FP_{w,j}}{N} \\qquad y_j = \\frac{TP_{w,j}}{P} $$\nThis process generates an ordered sequence of ROC points $(x_0, y_0), (x_1, y_1), \\dots, (x_m, y_m)$, where $0=x_0 \\le x_1 \\le \\dots \\le x_m=1$ and $0=y_0 \\le y_1 \\le \\dots \\le y_m=1$.\n\nThe problem specifies that the curve between these empirical points should be treated as piecewise linear. The integral for pAUC can thus be computed using the trapezoidal rule, summing the areas of trapezoids formed by consecutive points $(x_{j-1}, y_{j-1})$ and $(x_j, y_j)$. The area of one such trapezoid is $\\frac{1}{2}(y_j + y_{j-1})(x_j - x_{j-1})$.\n\nThe algorithm to compute $\\text{pAUC}(\\alpha)$ is as follows:\n1. Initialize $\\text{pAUC} = 0$.\n2. Iterate through the ROC points from $j=1$ to $m$. Let the previous point be $(x_{j-1}, y_{j-1})$ and the current point be $(x_j, y_j)$.\n3. If $x_{j-1} \\ge \\alpha$, the integration range has been fully covered. Terminate the summation.\n4. If $x_j \\le \\alpha$, the entire trapezoid lies within the integration range. Add its full area to the total:\n   $$ \\Delta A = \\frac{1}{2}(y_j + y_{j-1})(x_j - x_{j-1}) $$\n5. If $x_{j-1} < \\alpha < x_j$, this is the final segment intersecting the boundary $\\alpha$. We must clip the trapezoid at $FPR=\\alpha$. The TPR value at this boundary, $y(\\alpha)$, is found by linear interpolation:\n   $$ y(\\alpha) = y_{j-1} + (y_j - y_{j-1})\\frac{\\alpha - x_{j-1}}{x_j - x_{j-1}} $$\n   The area of the clipped trapezoid is:\n   $$ \\Delta A = \\frac{1}{2}(y(\\alpha) + y_{j-1})(\\alpha - x_{j-1}) $$\n   Add this area to the total and then terminate the summation.\nThis procedure provides a mathematically consistent computation of the partial AUC for an empirical, piecewise linear ROC curve.\n\n**Relevance to High-Energy Physics:**\nIn High-Energy Physics (HEP) searches for rare phenomena, signal events are vastly outnumbered by background events. For instance, at the Large Hadron Collider (LHC), the rate of uninteresting background collisions can be $10^9$ to $10^{12}$ times higher than the rate of a signal process like Higgs boson production. Consequently, data acquisition systems (triggers) and offline analysis selections must apply extremely stringent criteria to reject background while retaining as much signal as possible.\n\nThis operational constraint means that any useful classifier must perform well in the regime of very small False Positive Rate (FPR), typically $FPR \\ll 10^{-3}$. A classifier's performance at moderate or high FPR (e.g., $FPR > 0.01$) is irrelevant, because such a loose selection would allow an overwhelming number of background events into the final dataset, rendering any signal unobservable. Standard AUC, which integrates over the full range $FPR \\in [0,1]$, averages performance over both relevant and irrelevant regimes. A classifier might achieve a high overall AUC due to excellent performance at high FPR, while being useless for a HEP search due to poor performance at low FPR.\n\nThe partial AUC, $\\text{pAUC}(\\alpha)$ for a small $\\alpha$ (e.g., $\\alpha = 10^{-4}$), resolves this issue. It is a figure of merit that quantifies the classifier's performance exclusively within the operationally accessible, low-FPR region. It measures the average True Positive Rate (signal efficiency) that can be achieved subject to the constraint that the background leakage (FPR) does not exceed $\\alpha$. A higher $\\text{pAUC}(\\alpha)$ in this regime directly indicates a greater ability to separate signal from background under realistic experimental conditions, which translates to enhanced discovery sensitivity or improved precision for measurements. Therefore, it is the correct metric for evaluating and optimizing classifiers for discovery-oriented searches in HEP.", "answer": "```python\nimport numpy as np\n\ndef compute_partial_auc(scores: list[float], labels: list[int], alpha: float, weights: list[float] = None) -> float:\n    \"\"\"\n    Computes the partial Area Under the ROC Curve (pAUC) for FPR in [0, alpha].\n\n    Args:\n        scores: A list of real-valued scores from a classifier.\n        labels: A list of binary true labels (1 for signal, 0 for background).\n        alpha: The upper bound of the False Positive Rate interval [0, alpha].\n        weights: An optional list of non-negative weights for each event. If None,\n                 all events are weighted equally (weight = 1).\n\n    Returns:\n        The partial AUC value.\n    \"\"\"\n    if not (0.0 <= alpha <= 1.0):\n        raise ValueError(\"alpha must be between 0 and 1.\")\n\n    scores = np.array(scores, dtype=np.float64)\n    labels = np.array(labels, dtype=np.int32)\n    if weights is None:\n        weights = np.ones_like(scores, dtype=np.float64)\n    else:\n        weights = np.array(weights, dtype=np.float64)\n\n    # Calculate total weight of positives (P) and negatives (N)\n    pos_mask = (labels == 1)\n    neg_mask = (labels == 0)\n    P = np.sum(weights[pos_mask])\n    N = np.sum(weights[neg_mask])\n\n    if P == 0 or N == 0:\n        # Undefined rates, but problem guarantees this won't occur in test suite.\n        return 0.0\n\n    # Get unique scores and sort them in descending order\n    unique_scores = np.unique(scores)[::-1]\n\n    # Generate ROC points\n    roc_points = []\n    tp_w, fp_w = 0.0, 0.0\n    roc_points.append((0.0, 0.0))  # Start at (0,0)\n\n    for score_thresh in unique_scores:\n        # Find all events at the current score threshold\n        at_thresh_mask = (scores == score_thresh)\n        \n        # Sum weights of TPs and FPs at this threshold\n        d_tp = np.sum(weights[at_thresh_mask & pos_mask])\n        d_fp = np.sum(weights[at_thresh_mask & neg_mask])\n\n        tp_w += d_tp\n        fp_w += d_fp\n        roc_points.append((fp_w / N, tp_w / P))\n\n    # Calculate partial AUC using the trapezoidal rule, clipped at alpha\n    p_auc = 0.0\n    prev_x, prev_y = roc_points[0]\n\n    for curr_x, curr_y in roc_points[1:]:\n        if prev_x >= alpha:\n            break\n        \n        if curr_x <= alpha:\n            # Full trapezoid is within the [0, alpha] range\n            p_auc += 0.5 * (curr_y + prev_y) * (curr_x - prev_x)\n        else:\n            # This trapezoid is clipped by alpha.\n            # Linearly interpolate TPR at FPR = alpha\n            if curr_x > prev_x: # Avoid division by zero for vertical segments\n                interp_y = prev_y + (curr_y - prev_y) * (alpha - prev_x) / (curr_x - prev_x)\n            else:\n                interp_y = curr_y # Vertical line, take upper y\n            \n            p_auc += 0.5 * (interp_y + prev_y) * (alpha - prev_x)\n            break\n        \n        prev_x, prev_y = curr_x, curr_y\n\n    return p_auc\n\ndef solve():\n    \"\"\"\n    Runs the test suite for the partial AUC calculation.\n    \"\"\"\n    test_cases = [\n        {\n            \"scores\": [5.1, 4.9, 3.8, 3.5, 2.0, 4.7, 3.9, 3.2, 1.9, 1.0, 0.3, -0.5],\n            \"labels\": [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n            \"alpha\": 0.1,\n            \"weights\": None\n        },\n        {\n            \"scores\": [5.1, 4.9, 3.8, 3.5, 2.0, 4.7, 3.9, 3.2, 1.9, 1.0, 0.3, -0.5],\n            \"labels\": [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n            \"alpha\": 0.0,\n            \"weights\": None\n        },\n        {\n            \"scores\": [0.8, 0.8, 0.2, -0.1, 0.8, 0.5, 0.0, -0.2, -0.2],\n            \"labels\": [1, 1, 1, 1, 0, 0, 0, 0, 0],\n            \"alpha\": 1.0,\n            \"weights\": None\n        },\n        {\n            \"scores\": [3.2, 2.9, 1.7, 3.1, 2.5, 0.7, 0.2],\n            \"labels\": [1, 1, 1, 0, 0, 0, 0],\n            \"weights\": [1.0, 0.8, 0.5, 1000.0, 200.0, 50.0, 10.0],\n            \"alpha\": 0.001\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        result = compute_partial_auc(\n            scores=case[\"scores\"],\n            labels=case[\"labels\"],\n            alpha=case[\"alpha\"],\n            weights=case[\"weights\"]\n        )\n        results.append(str(result))\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3529666"}, {"introduction": "Modern machine learning models can provide not only a prediction but also an estimate of their own uncertainty, opening the door for more sophisticated, risk-aware decision-making. This exercise explores this advanced concept by asking you to construct a selection rule that considers both the classifier's score and its predictive uncertainty, with the goal of optimizing the final discovery significance [@problem_id:3529707]. By comparing a standard selection to a risk-aware one, you will learn a practical method for improving the robustness and real-world impact of a classifier in a scientific discovery context.", "problem": "In a simulated binary classification task representative of event selection in computational high-energy physics, a classifier outputs for each event a predicted score $s \\in [0,1]$ and an associated predictive uncertainty $\\sigma_s \\in [0,\\infty)$. Events are of two classes: signal and background. A standard Receiver Operating Characteristic (ROC) curve considers acceptance regions defined by a threshold $t$ on the score such that an event is accepted if and only if $s > t$. A risk-aware ROC modifies the acceptance by additionally requiring $\\sigma_s < \\sigma_{\\max}$, i.e., an event is accepted if and only if $s > t$ and $\\sigma_s < \\sigma_{\\max}$.\n\nYou are to compute, for given datasets and thresholds, the following quantities for both the standard selection and the risk-aware selection:\n- The True Positive Rate (TPR), defined as the fraction of signal events accepted: $\\mathrm{TPR} = \\frac{\\text{number of accepted signal events}}{\\text{total number of signal events}}$.\n- The False Positive Rate (FPR), defined as the fraction of background events accepted: $\\mathrm{FPR} = \\frac{\\text{number of accepted background events}}{\\text{total number of background events}}$.\n\nAssume that each event within a given class has equal weight. Let the expected total yields before selection be $S_{\\text{tot}}$ for signal and $B_{\\text{tot}}$ for background (both are strictly positive real numbers). The effective yields after selection are $s_{\\text{eff}} = S_{\\text{tot}} \\cdot \\mathrm{TPR}$ and $b_{\\text{eff}} = B_{\\text{tot}} \\cdot \\mathrm{FPR}$. Define the discovery significance $Z$ as the profile-likelihood Asimov discovery significance appropriate for counting experiments with Poisson statistics and known background, derived from the likelihood-ratio test under the signal-plus-background versus background-only hypotheses. Express $Z$ in a dimensionless form.\n\nUsing the fundamental definitions of $\\mathrm{TPR}$, $\\mathrm{FPR}$, and Poisson likelihood, and the risk-aware acceptance region defined above, implement a program that, for each test case, computes:\n- $\\mathrm{TPR}_{\\text{std}}$, $\\mathrm{FPR}_{\\text{std}}$, $Z_{\\text{std}}$ for the standard selection with acceptance rule $s > t$.\n- $\\mathrm{TPR}_{\\text{risk}}$, $\\mathrm{FPR}_{\\text{risk}}$, $Z_{\\text{risk}}$ for the risk-aware selection with acceptance rule $s > t$ and $\\sigma_s < \\sigma_{\\max}$.\n- The changes $\\Delta \\mathrm{TPR} = \\mathrm{TPR}_{\\text{risk}} - \\mathrm{TPR}_{\\text{std}}$, $\\Delta \\mathrm{FPR} = \\mathrm{FPR}_{\\text{risk}} - \\mathrm{FPR}_{\\text{std}}$, and $\\Delta Z = Z_{\\text{risk}} - Z_{\\text{std}}$.\n\nAll rates must be expressed as decimals in the interval $[0,1]$, not as percentages. The discovery significance $Z$ is dimensionless. No physical units are required for any quantity in this problem.\n\nUse the following datasets (the arrays are ordered lists of real numbers). For each event, the score is $s$ and the predictive uncertainty is $\\sigma_s$. Signal events:\n- Scores $s_{\\text{sig}} = [0.95, 0.90, 0.88, 0.85, 0.80, 0.78, 0.76, 0.70, 0.65, 0.60, 0.55, 0.50]$.\n- Uncertainties $\\sigma_{\\text{sig}} = [0.05, 0.04, 0.10, 0.15, 0.08, 0.20, 0.12, 0.25, 0.18, 0.30, 0.35, 0.40]$.\n\nBackground events:\n- Scores $s_{\\text{bkg}} = [0.80, 0.75, 0.70, 0.68, 0.65, 0.62, 0.60, 0.58, 0.55, 0.52, 0.50, 0.48, 0.45, 0.40, 0.35, 0.30, 0.25, 0.20, 0.15, 0.10]$.\n- Uncertainties $\\sigma_{\\text{bkg}} = [0.30, 0.25, 0.20, 0.18, 0.22, 0.27, 0.35, 0.15, 0.40, 0.38, 0.28, 0.18, 0.12, 0.10, 0.08, 0.05, 0.04, 0.03, 0.02, 0.01]$.\n\nUse the following test suite of parameter sets $(t, \\sigma_{\\max}, S_{\\text{tot}}, B_{\\text{tot}})$:\n- Case 1: $(t = 0.60, \\sigma_{\\max} = 0.20, S_{\\text{tot}} = 50.0, B_{\\text{tot}} = 200.0)$.\n- Case 2: $(t = 0.60, \\sigma_{\\max} = 1.00, S_{\\text{tot}} = 50.0, B_{\\text{tot}} = 200.0)$.\n- Case 3: $(t = 0.20, \\sigma_{\\max} = 0.05, S_{\\text{tot}} = 50.0, B_{\\text{tot}} = 200.0)$.\n- Case 4: $(t = 0.65, \\sigma_{\\max} = 0.20, S_{\\text{tot}} = 5.0, B_{\\text{tot}} = 1000.0)$.\n\nYour program should compute for each case the nine-tuple $[\\mathrm{TPR}_{\\text{std}}, \\mathrm{FPR}_{\\text{std}}, Z_{\\text{std}}, \\mathrm{TPR}_{\\text{risk}}, \\mathrm{FPR}_{\\text{risk}}, Z_{\\text{risk}}, \\Delta \\mathrm{TPR}, \\Delta \\mathrm{FPR}, \\Delta Z]$, with all floats rounded to $6$ decimal places.\n\nFinal output format: Your program should produce a single line of output containing a list of these nine-tuples, one per test case, in order, as a comma-separated list enclosed in square brackets (e.g., `[[\\cdots], [\\cdots], \\ldots]`). No other text should be printed.", "solution": "We start from core definitions appropriate for receiver operating characteristic analysis and Poisson counting significance in high-energy physics.\n\n1. Definitions of classification rates. For a binary classifier with a deterministic acceptance region $\\mathcal{A}$ in feature space, the True Positive Rate (TPR) is defined as $\\mathrm{TPR} = \\frac{N_{\\text{sig,acc}}}{N_{\\text{sig,tot}}}$, where $N_{\\text{sig,acc}}$ is the number of signal events in $\\mathcal{A}$ and $N_{\\text{sig,tot}}$ is the total number of signal events. The False Positive Rate (FPR) is $\\mathrm{FPR} = \\frac{N_{\\text{bkg,acc}}}{N_{\\text{bkg,tot}}}$ analogously for background. When each event has equal weight, these fractions are empirical estimates of the class-conditional acceptance probabilities.\n\n2. Acceptance regions. In this problem, there are two acceptance rules:\n- Standard rule: accept if and only if $s > t$ for a prescribed threshold $t \\in [0,1]$. The acceptance region is $\\mathcal{A}_{\\text{std}}(t) = \\{ (s, \\sigma_s) : s > t \\}$.\n- Risk-aware rule: accept if and only if $s > t$ and $\\sigma_s < \\sigma_{\\max}$ for a prescribed $\\sigma_{\\max} > 0$. The acceptance region is $\\mathcal{A}_{\\text{risk}}(t,\\sigma_{\\max}) = \\{ (s, \\sigma_s) : s > t \\} \\cap \\{ (s, \\sigma_s) : \\sigma_s < \\sigma_{\\max} \\}$. This definition encodes risk aversion by excluding high-uncertainty predictions.\n\nGiven the provided arrays of $s$ and $\\sigma_s$, we implement these regions as indicator functions $I_{\\text{std}}(s,\\sigma_s;t) = 1$ if $s > t$ and $0$ otherwise, and $I_{\\text{risk}}(s,\\sigma_s;t,\\sigma_{\\max}) = 1$ if $s > t$ and $\\sigma_s < \\sigma_{\\max}$ and $0$ otherwise. Then\n$$\n\\mathrm{TPR}_{\\text{std}}(t) = \\frac{1}{N_{\\text{sig,tot}}}\\sum_{i=1}^{N_{\\text{sig,tot}}} I_{\\text{std}}(s_i^{(\\text{sig})}, \\sigma_{s,i}^{(\\text{sig})}; t),\n$$\n$$\n\\mathrm{FPR}_{\\text{std}}(t) = \\frac{1}{N_{\\text{bkg,tot}}}\\sum_{j=1}^{N_{\\text{bkg,tot}}} I_{\\text{std}}(s_j^{(\\text{bkg})}, \\sigma_{s,j}^{(\\text{bkg})}; t),\n$$\nand similarly\n$$\n\\mathrm{TPR}_{\\text{risk}}(t,\\sigma_{\\max}) = \\frac{1}{N_{\\text{sig,tot}}}\\sum_{i=1}^{N_{\\text{sig,tot}}} I_{\\text{risk}}(s_i^{(\\text{sig})}, \\sigma_{s,i}^{(\\text{sig})}; t,\\sigma_{\\max}),\n$$\n$$\n\\mathrm{FPR}_{\\text{risk}}(t,\\sigma_{\\max}) = \\frac{1}{N_{\\text{bkg,tot}}}\\sum_{j=1}^{N_{\\text{bkg,tot}}} I_{\\text{risk}}(s_j^{(\\text{bkg})}, \\sigma_{s,j}^{(\\text{bkg})}; t,\\sigma_{\\max}).\n$$\n\n3. Effective yields. In high-energy physics counting experiments with expected preselection yields $S_{\\text{tot}}$ and $B_{\\text{tot}}$, the effect of a selection is to scale these yields by the acceptance fractions. Thus, the effective yields are\n$$\ns_{\\text{eff,std}} = S_{\\text{tot}} \\cdot \\mathrm{TPR}_{\\text{std}}, \\quad b_{\\text{eff,std}} = B_{\\text{tot}} \\cdot \\mathrm{FPR}_{\\text{std}},\n$$\n$$\ns_{\\text{eff,risk}} = S_{\\text{tot}} \\cdot \\mathrm{TPR}_{\\text{risk}}, \\quad b_{\\text{eff,risk}} = B_{\\text{tot}} \\cdot \\mathrm{FPR}_{\\text{risk}}.\n$$\n\n4. Discovery significance. For a counting experiment with a Poisson-distributed count $n$ and known background expectation $b$, testing the background-only hypothesis against the signal-plus-background alternative can be performed using a likelihood-ratio test. The median discovery significance under the signal-plus-background model is given by the Asimov approximation, a well-tested result in high-energy physics. Starting from the profile likelihood ratio and evaluating on the Asimov dataset (the mean counts), one obtains the closed-form expression\n$$\nZ_A(s,b) = \\sqrt{2 \\left[ (s + b) \\ln \\left( 1 + \\frac{s}{b} \\right) - s \\right]},\n$$\nfor $s > 0$ and $b > 0$, where $s$ and $b$ are the signal and background expectations after selection. This expression is dimensionless. In the limiting case $s = 0$, $Z_A(0,b) = 0$. The case $b \\to 0^+$ yields increasing $Z_A$, but in this problem the test suite ensures $b_{\\text{eff}} > 0$.\n\nThus,\n$$\nZ_{\\text{std}} = Z_A(s_{\\text{eff,std}}, b_{\\text{eff,std}}), \\quad Z_{\\text{risk}} = Z_A(s_{\\text{eff,risk}}, b_{\\text{eff,risk}}).\n$$\n\n5. Differences. The changes due to introducing the uncertainty constraint are computed as\n$$\n\\Delta \\mathrm{TPR} = \\mathrm{TPR}_{\\text{risk}} - \\mathrm{TPR}_{\\text{std}}, \\quad \\Delta \\mathrm{FPR} = \\mathrm{FPR}_{\\text{risk}} - \\mathrm{FPR}_{\\text{std}},\n$$\n$$\n\\Delta Z = Z_{\\text{risk}} - Z_{\\text{std}}.\n$$\n\n6. Algorithmic implementation. For each test case $(t,\\sigma_{\\max}, S_{\\text{tot}}, B_{\\text{tot}})$:\n- Evaluate the indicator functions on the provided arrays to count accepted signal and background events for both standard and risk-aware rules.\n- Compute $\\mathrm{TPR}$ and $\\mathrm{FPR}$ by dividing by the respective class totals.\n- Compute $s_{\\text{eff}}$ and $b_{\\text{eff}}$ by scaling by $S_{\\text{tot}}$ and $B_{\\text{tot}}$.\n- Compute $Z$ using $Z_A$ above.\n- Compute the differences $\\Delta \\mathrm{TPR}$, $\\Delta \\mathrm{FPR}$, $\\Delta Z$.\n- Round all floats to $6$ decimal places.\n- Aggregate the nine outputs in the specified order for this test case.\n\n7. Edge conditions and numerical stability. We enforce strict inequalities exactly as defined: accept if and only if $s > t$ and $\\sigma_s < \\sigma_{\\max}$ for the risk-aware rule. The test suite has been designed to avoid $b_{\\text{eff}} = 0$; if one wished to generalize beyond this, one could guard the logarithm by replacing $b$ with $\\max(b, \\epsilon)$ for a small $\\epsilon > 0$, but this is not required here.\n\nExecuting the program with the specified datasets and cases yields, for each case, the tuple $[\\,\\mathrm{TPR}_{\\text{std}},\\,\\mathrm{FPR}_{\\text{std}},\\,Z_{\\text{std}},\\,\\mathrm{TPR}_{\\text{risk}},\\,\\mathrm{FPR}_{\\text{risk}},\\,Z_{\\text{risk}},\\,\\Delta \\mathrm{TPR},\\,\\Delta \\mathrm{FPR},\\,\\Delta Z\\,]$ as a list, and the final output is a single line containing the list of these lists, meeting the required format.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef asimov_significance(s, b):\n    \"\"\"\n    Compute the Asimov discovery significance for Poisson counts with known background:\n    Z_A = sqrt(2 * [ (s + b) * ln(1 + s/b) - s ])\n    Handles edge cases s <= 0 -> 0. Clips b to small positive for numerical stability.\n    \"\"\"\n    if s <= 0.0:\n        return 0.0\n    # Ensure b is positive to avoid division by zero; test cases ensure b > 0.\n    b_safe = max(b, 1e-15)\n    return float(np.sqrt(2.0 * ((s + b_safe) * np.log1p(s / b_safe) - s)))\n\ndef compute_metrics(sig_scores, sig_uncs, bkg_scores, bkg_uncs, t, sigma_max, S_tot, B_tot):\n    sig_scores = np.asarray(sig_scores, dtype=float)\n    sig_uncs = np.asarray(sig_uncs, dtype=float)\n    bkg_scores = np.asarray(bkg_scores, dtype=float)\n    bkg_uncs = np.asarray(bkg_uncs, dtype=float)\n\n    # Indicators for standard acceptance: s > t\n    acc_sig_std = (sig_scores > t)\n    acc_bkg_std = (bkg_scores > t)\n\n    # Indicators for risk-aware acceptance: s > t and sigma_s < sigma_max\n    acc_sig_risk = (sig_scores > t) & (sig_uncs < sigma_max)\n    acc_bkg_risk = (bkg_scores > t) & (bkg_uncs < sigma_max)\n\n    # Rates\n    Nsig = sig_scores.size\n    Nbkg = bkg_scores.size\n\n    TPR_std = acc_sig_std.sum() / Nsig\n    FPR_std = acc_bkg_std.sum() / Nbkg\n\n    TPR_risk = acc_sig_risk.sum() / Nsig\n    FPR_risk = acc_bkg_risk.sum() / Nbkg\n\n    # Effective yields\n    s_eff_std = S_tot * TPR_std\n    b_eff_std = B_tot * FPR_std\n\n    s_eff_risk = S_tot * TPR_risk\n    b_eff_risk = B_tot * FPR_risk\n\n    # Significances\n    Z_std = asimov_significance(s_eff_std, b_eff_std)\n    Z_risk = asimov_significance(s_eff_risk, b_eff_risk)\n\n    # Deltas\n    dTPR = TPR_risk - TPR_std\n    dFPR = FPR_risk - FPR_std\n    dZ = Z_risk - Z_std\n\n    return [TPR_std, FPR_std, Z_std, TPR_risk, FPR_risk, Z_risk, dTPR, dFPR, dZ]\n\ndef solve():\n    # Define the datasets from the problem statement.\n    s_sig = [0.95, 0.90, 0.88, 0.85, 0.80, 0.78, 0.76, 0.70, 0.65, 0.60, 0.55, 0.50]\n    u_sig = [0.05, 0.04, 0.10, 0.15, 0.08, 0.20, 0.12, 0.25, 0.18, 0.30, 0.35, 0.40]\n\n    s_bkg = [0.80, 0.75, 0.70, 0.68, 0.65, 0.62, 0.60, 0.58, 0.55, 0.52,\n             0.50, 0.48, 0.45, 0.40, 0.35, 0.30, 0.25, 0.20, 0.15, 0.10]\n    u_bkg = [0.30, 0.25, 0.20, 0.18, 0.22, 0.27, 0.35, 0.15, 0.40, 0.38,\n             0.28, 0.18, 0.12, 0.10, 0.08, 0.05, 0.04, 0.03, 0.02, 0.01]\n\n    # Test suite: (t, sigma_max, S_tot, B_tot)\n    test_cases = [\n        (0.60, 0.20, 50.0, 200.0),   # Case 1\n        (0.60, 1.00, 50.0, 200.0),   # Case 2\n        (0.20, 0.05, 50.0, 200.0),   # Case 3\n        (0.65, 0.20, 5.0, 1000.0),   # Case 4\n    ]\n\n    results = []\n    for t, sigma_max, S_tot, B_tot in test_cases:\n        metrics = compute_metrics(s_sig, u_sig, s_bkg, u_bkg, t, sigma_max, S_tot, B_tot)\n        # Round to 6 decimals as required\n        metrics_rounded = [round(x, 6) for x in metrics]\n        results.append(metrics_rounded)\n\n    # Format as a single line list of lists\n    def format_list(lst):\n        return \"[\" + \",\".join(f\"{x:.6f}\" for x in lst) + \"]\"\n\n    formatted = \"[\" + \",\".join(format_list(r) for r in results) + \"]\"\n    print(formatted)\n\nsolve()\n```", "id": "3529707"}]}