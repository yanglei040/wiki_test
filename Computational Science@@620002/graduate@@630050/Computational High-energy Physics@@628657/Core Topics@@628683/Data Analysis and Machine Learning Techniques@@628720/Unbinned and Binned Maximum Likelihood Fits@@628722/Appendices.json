{"hands_on_practices": [{"introduction": "The choice between an unbinned and a binned likelihood represents a fundamental trade-off between statistical precision and computational feasibility. This exercise provides a quantitative understanding of this trade-off by using Fisher information to measure the information loss inherent in binning data. By deriving and comparing the information content for both scenarios [@problem_id:3540423], you will build a foundational intuition for optimizing an analysis to retain maximum sensitivity.", "problem": "You are studying the information-theoretic efficiency of unbinned versus binned maximum likelihood fits for an exponential lifetime model within a finite observation window. Consider independent and identically distributed samples drawn from the exponential probability density function defined on the nonnegative line, $f(x \\mid \\lambda) = \\lambda e^{-\\lambda x}$ for $x \\ge 0$ and parameter $\\lambda  0$, but suppose the data acquisition is limited to a finite window $[0, T]$. Condition on observing exactly $N$ events within this window. For the unbinned case, the likelihood is the product of the truncated density on $[0, T]$. For the binned case, partition the window $[0, T]$ into $m$ equal-width bins and use the multinomial likelihood for the bin counts conditioned on $N$. Using the foundational definitions below, derive analytical expressions and evaluate them numerically.\n\nFundamental definitions:\n- The log-likelihood for independent observations $x_1, \\dots, x_N$ with density $f(x \\mid \\lambda)$ is $\\ell(\\lambda) = \\sum_{i=1}^N \\log f(x_i \\mid \\lambda)$.\n- The Fisher information for parameter $\\lambda$ from $N$ independent samples is $I(\\lambda) = - \\mathbb{E}\\left[\\frac{\\partial^2 \\ell(\\lambda)}{\\partial \\lambda^2}\\right]$, where the expectation is with respect to the true data-generating distribution.\n- For a multinomial model with probabilities $p_k(\\lambda)$ that depend on $\\lambda$ and total count $N$, the Fisher information for $\\lambda$ is $I(\\lambda) = N \\sum_{k=1}^m \\frac{1}{p_k(\\lambda)} \\left(\\frac{\\partial p_k(\\lambda)}{\\partial \\lambda}\\right)^2$, provided $p_k(\\lambda)  0$ for all $k$.\n\nTasks:\n1. Unbinned information under truncation. Given the observation window $[0, T]$, treat the data as drawn from the truncated exponential on $[0, T]$, that is, the conditional density $f_T(x \\mid \\lambda) = \\frac{\\lambda e^{-\\lambda x}}{1 - e^{-\\lambda T}}$ for $0 \\le x \\le T$. Starting from the Fisher information definition, derive the per-event Fisher information for $\\lambda$ under this truncation, and hence obtain the total Fisher information for $N$ observations in $[0, T]$.\n2. Binned information with equal-width bins. Partition $[0, T]$ into $m$ equal-width bins with edges $0, \\Delta, 2\\Delta, \\dots, T$, where $\\Delta = T/m$. Let $p_k(\\lambda)$ denote the bin probability under $f_T(x \\mid \\lambda)$ for bin $k \\in \\{1, \\dots, m\\}$. Starting from the multinomial Fisher information definition above, derive an expression for the total Fisher information for $\\lambda$ in terms of $p_k(\\lambda)$ and $\\frac{\\partial p_k(\\lambda)}{\\partial \\lambda}$.\n3. Optimization of $m$ subject to a minimal expected occupancy constraint. For numerical stability of binned fits in practice, enforce a constraint that the expected count in every bin must be at least a positive threshold $c_{\\min}$. With $N$ total events, this means $N \\cdot p_k(\\lambda) \\ge c_{\\min}$ for all $k \\in \\{1, \\dots, m\\}$. Among all integers $m \\in \\{1, 2, \\dots, N\\}$ satisfying this constraint, choose $m^\\star$ that maximizes the binned Fisher information (equivalently, minimizes information loss relative to the unbinned case). If multiple $m$ achieve the same maximum to within floating-point tolerance, select the largest such $m$.\n4. Implement a program that, for each test case specified below, computes:\n   - The total unbinned Fisher information $I_{\\text{unb}}(\\lambda, T, N)$.\n   - The total binned Fisher information $I_{\\text{bin}}(\\lambda, T, N, m^\\star)$ using the optimized $m^\\star$ under the occupancy constraint.\n   - The ratio $r = I_{\\text{bin}} / I_{\\text{unb}}$.\n   - The optimized bin count $m^\\star$.\n   Report floating-point results rounded to six decimal places.\n\nAssumptions and clarifications:\n- All quantities are dimensionless; no physical units are used.\n- Angles are not involved.\n- All probabilities must sum to one, and all $p_k(\\lambda)$ must be strictly positive for the Fisher information to be finite.\n- Use the conditional-on-$N$ framework consistently for both unbinned and binned cases.\n\nTest suite:\nCompute the outputs for the following parameter sets $(\\lambda, T, N, c_{\\min})$:\n- Case A: $(0.5, 6.0, 1000, 5)$\n- Case B: $(0.2, 20.0, 500, 10)$\n- Case C: $(1.0, 3.0, 30, 5)$\n\nFinal output format:\nYour program should produce a single line containing the results as a comma-separated list of lists, one list per test case, in the order [Case A, Case B, Case C]. Each inner list must be of the form $[m^\\star, I_{\\text{unb}}, I_{\\text{bin}}, r]$, where $m^\\star$ is an integer and the other entries are floats rounded to six decimal places. For example: [[mA, IunbA, IbinA, rA],[mB, IunbB, IbinB, rB],[mC, IunbC, IbinC, rC]].", "solution": "The problem is subjected to validation against the specified criteria.\n\n### Step 1: Extract Givens\n- **Probability Density Function (PDF)**: Exponential distribution on the non-negative line, $f(x \\mid \\lambda) = \\lambda e^{-\\lambda x}$ for $x \\ge 0$ and $\\lambda  0$.\n- **Observation Window**: Data acquisition is limited to the interval $[0, T]$.\n- **Truncated Conditional Density**: For $N$ events observed in $[0, T]$, the density is $f_T(x \\mid \\lambda) = \\frac{\\lambda e^{-\\lambda x}}{1 - e^{-\\lambda T}}$ for $0 \\le x \\le T$.\n- **Log-Likelihood Definition**: For i.i.d. observations $x_1, \\dots, x_N$, $\\ell(\\lambda) = \\sum_{i=1}^N \\log f(x_i \\mid \\lambda)$.\n- **Unbinned Fisher Information Definition**: $I(\\lambda) = - \\mathbb{E}\\left[\\frac{\\partial^2 \\ell(\\lambda)}{\\partial \\lambda^2}\\right]$.\n- **Binned Model**: The window $[0, T]$ is partitioned into $m$ equal-width bins. Bin width is $\\Delta = T/m$.\n- **Multinomial Binned Fisher Information Definition**: For total count $N$ and bin probabilities $p_k(\\lambda)$, $I(\\lambda) = N \\sum_{k=1}^m \\frac{1}{p_k(\\lambda)} \\left(\\frac{\\partial p_k(\\lambda)}{\\partial \\lambda}\\right)^2$, for $p_k(\\lambda)  0$.\n- **Optimization Constraint**: For a chosen number of bins $m$, the expected count in every bin must be at least $c_{\\min}$, i.e., $N \\cdot p_k(\\lambda) \\ge c_{\\min}$ for all $k \\in \\{1, \\dots, m\\}$.\n- **Optimization Objective**: Find $m^\\star \\in \\{1, 2, \\dots, N\\}$ that satisfies the constraint and maximizes the binned Fisher information. If there is a tie, select the largest such $m$.\n- **Test Cases**:\n    - Case A: $(\\lambda, T, N, c_{\\min}) = (0.5, 6.0, 1000, 5)$\n    - Case B: $(\\lambda, T, N, c_{\\min}) = (0.2, 20.0, 500, 10)$\n    - Case C: $(\\lambda, T, N, c_{\\min}) = (1.0, 3.0, 30, 5)$\n- **Output Requirements**: For each test case, compute $[m^\\star, I_{\\text{unb}}, I_{\\text{bin}}, r]$, with $I_{\\text{unb}}$, $I_{\\text{bin}}$, and $r = I_{\\text{bin}} / I_{\\text{unb}}$ rounded to six decimal places.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is assessed for validity:\n- **Scientifically Grounded**: The problem is based on fundamental principles of statistical inference, namely maximum likelihood estimation and Fisher information theory, applied to the exponential distribution. This is a standard and well-understood topic in computational physics and statistics. The setup is scientifically sound.\n- **Well-Posed**: The problem specifies all necessary definitions, constraints, and objectives. The derivation tasks are clearly stated. The optimization task for $m^\\star$ has a clear objective function (maximizing $I_{\\text{bin}}$), a well-defined constraint set, and a tie-breaking rule, ensuring a unique solution exists.\n- **Objective**: The problem is stated using precise mathematical language, free from subjective or ambiguous terminology.\n- **Other Flaws**: The problem does not violate any of the invalidity criteria. It is self-contained, logically consistent, and computationally feasible. The mathematical framework is standard. The provided test cases are reasonable and do not introduce contradictions.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. A full, reasoned solution is provided below.\n\nThe solution proceeds by deriving the analytical expressions for the required quantities and then implementing them numerically.\n\n**1. Unbinned Fisher Information, $I_{\\text{unb}}$**\n\nThe log-likelihood for a single observation $x$ from the truncated density $f_T(x \\mid \\lambda) = \\frac{\\lambda e^{-\\lambda x}}{1 - e^{-\\lambda T}}$ is:\n$$ \\ell_1(\\lambda) = \\log(f_T(x \\mid \\lambda)) = \\log(\\lambda) - \\lambda x - \\log(1 - e^{-\\lambda T}) $$\nThe first derivative with respect to $\\lambda$ (the score function) is:\n$$ \\frac{\\partial \\ell_1(\\lambda)}{\\partial \\lambda} = \\frac{1}{\\lambda} - x - \\frac{1}{1 - e^{-\\lambda T}} \\cdot (-e^{-\\lambda T}) \\cdot (-T) = \\frac{1}{\\lambda} - x - \\frac{T e^{-\\lambda T}}{1 - e^{-\\lambda T}} $$\nThe second derivative is:\n$$ \\frac{\\partial^2 \\ell_1(\\lambda)}{\\partial \\lambda^2} = -\\frac{1}{\\lambda^2} - \\frac{\\partial}{\\partial \\lambda}\\left(\\frac{T e^{-\\lambda T}}{1 - e^{-\\lambda T}}\\right) $$\nUsing the quotient rule, the derivative of the second term is $\\frac{-T^2 e^{-\\lambda T}}{(1 - e^{-\\lambda T})^2}$. Thus:\n$$ \\frac{\\partial^2 \\ell_1(\\lambda)}{\\partial \\lambda^2} = -\\frac{1}{\\lambda^2} + \\frac{T^2 e^{-\\lambda T}}{(1 - e^{-\\lambda T})^2} $$\nThis expression is independent of the random variable $x$. Therefore, the expectation $\\mathbb{E}\\left[\\frac{\\partial^2 \\ell_1(\\lambda)}{\\partial \\lambda^2}\\right]$ is the expression itself. The Fisher information for a single event is:\n$$ I_1(\\lambda) = -\\mathbb{E}\\left[\\frac{\\partial^2 \\ell_1(\\lambda)}{\\partial \\lambda^2}\\right] = - \\left(-\\frac{1}{\\lambda^2} + \\frac{T^2 e^{-\\lambda T}}{(1 - e^{-\\lambda T})^2}\\right) = \\frac{1}{\\lambda^2} - \\frac{T^2 e^{-\\lambda T}}{(1 - e^{-\\lambda T})^2} $$\nFor $N$ independent and identically distributed observations, the total Fisher information is additive:\n$$ I_{\\text{unb}}(\\lambda, T, N) = N \\cdot I_1(\\lambda) = N \\left(\\frac{1}{\\lambda^2} - \\frac{T^2 e^{-\\lambda T}}{(1 - e^{-\\lambda T})^2}\\right) $$\nThis expression is non-negative, as can be shown by the inequality $\\sinh(v) \\ge v$ for $v \\ge 0$.\n\n**2. Binned Fisher Information, $I_{\\text{bin}}$**\n\nFor $m$ bins of equal width $\\Delta = T/m$, the probability of an event falling into bin $k \\in \\{1, \\dots, m\\}$, with edges $[(k-1)\\Delta, k\\Delta]$, is given by the integral of the truncated density:\n$$ p_k(\\lambda) = \\int_{(k-1)\\Delta}^{k\\Delta} f_T(x \\mid \\lambda) dx = \\frac{1}{1 - e^{-\\lambda T}} \\int_{(k-1)\\Delta}^{k\\Delta} \\lambda e^{-\\lambda x} dx $$\n$$ p_k(\\lambda) = \\frac{[-e^{-\\lambda x}]_{(k-1)\\Delta}^{k\\Delta}}{1 - e^{-\\lambda T}} = \\frac{e^{-\\lambda(k-1)\\Delta} - e^{-\\lambda k\\Delta}}{1 - e^{-\\lambda T}} $$\nFor numerical stability, this can be rewritten using the `expm1` function, where $\\text{expm1}(z) = e^z - 1$:\n$$ p_k(\\lambda) = \\frac{e^{-\\lambda k\\Delta}(e^{\\lambda\\Delta} - 1)}{e^{-\\lambda T}(e^{\\lambda T} - 1)} = e^{\\lambda(T-k\\Delta)} \\frac{\\text{expm1}(\\lambda\\Delta)}{\\text{expm1}(\\lambda T)} $$\nThe Fisher information for the binned model is $I_{\\text{bin}}(\\lambda) = N \\sum_{k=1}^m \\frac{1}{p_k(\\lambda)} \\left(\\frac{\\partial p_k(\\lambda)}{\\partial \\lambda}\\right)^2$. This can be rewritten as $N \\sum_{k=1}^m p_k(\\lambda) \\left(\\frac{\\partial \\log p_k(\\lambda)}{\\partial \\lambda}\\right)^2$. Taking the logarithm of the stable expression for $p_k(\\lambda)$:\n$$ \\log p_k(\\lambda) = \\lambda(T-k\\Delta) + \\log(\\text{expm1}(\\lambda\\Delta)) - \\log(\\text{expm1}(\\lambda T)) $$\nDifferentiating with respect to $\\lambda$:\n$$ \\frac{\\partial \\log p_k(\\lambda)}{\\partial \\lambda} = (T - k\\Delta) + \\frac{\\Delta e^{\\lambda\\Delta}}{\\text{expm1}(\\lambda\\Delta)} - \\frac{T e^{\\lambda T}}{\\text{expm1}(\\lambda T)} $$\nThis provides a numerically stable way to compute the terms in the sum for $I_{\\text{bin}}$.\n\n**3. Optimization of Bin Count, $m^\\star$**\n\nThe constraint is $N \\cdot p_k(\\lambda) \\ge c_{\\min}$ for all $k = 1, \\dots, m$. The bin probability $p_k(\\lambda)$ is a monotonically decreasing function of $k$. Thus, the constraint is most restrictive for the last bin, $k=m$. The condition simplifies to:\n$$ N \\cdot p_m(\\lambda) \\ge c_{\\min} $$\nUsing the stable expression for $p_k(\\lambda)$ with $k=m$ and $\\Delta=T/m$:\n$$ p_m(\\lambda) = \\frac{\\text{expm1}(\\lambda T/m)}{\\text{expm1}(\\lambda T)} $$\nThe Fisher information $I_{\\text{bin}}(m)$ is expected to be a monotonically increasing function of $m$ (as finer binning preserves more information), approaching $I_{\\text{unb}}$ as $m \\to \\infty$. Therefore, to maximize $I_{\\text{bin}}(m)$ subject to the constraint, we should choose the largest possible $m$ that satisfies it. The function $p_m(\\lambda)$ is a monotonically decreasing function of $m$. This means we can search for the largest integer $m \\in \\{1, \\dots, N\\}$ that satisfies $N \\cdot \\frac{\\text{expm1}(\\lambda T/m)}{\\text{expm1}(\\lambda T)} \\ge c_{\\min}$. A linear scan from $m=1$ upwards is efficient enough for the given problem constraints. The first value of $m$ to violate the constraint establishes the upper bound, and $m^\\star$ is the value immediately preceding it.\n\nThe implementation will follow these derived formulas.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all specified test cases.\n    It orchestrates the calculation for each case and formats the final output.\n    \"\"\"\n    test_cases = [\n        (0.5, 6.0, 1000, 5),    # Case A\n        (0.2, 20.0, 500, 10),   # Case B\n        (1.0, 3.0, 30, 5),      # Case C\n    ]\n\n    all_results = []\n    for case in test_cases:\n        result = compute_results_for_case(*case)\n        all_results.append(result)\n\n    # Format the final output string as specified\n    formatted_results = []\n    for res in all_results:\n        # res is [m_star, i_unb, i_bin, ratio]\n        s = f'[{res[0]},{res[1]:.6f},{res[2]:.6f},{res[3]:.6f}]'\n        formatted_results.append(s)\n    print(f\"[{','.join(formatted_results)}]\")\n\n\ndef compute_results_for_case(lam, T, N, c_min):\n    \"\"\"\n    Computes all required quantities for a single test case.\n\n    Args:\n        lam (float): The lambda parameter of the exponential distribution.\n        T (float): The upper bound of the observation window.\n        N (int): The total number of events.\n        c_min (int): The minimum required expected occupancy per bin.\n\n    Returns:\n        list: A list containing [m_star, I_unb, I_bin, ratio].\n    \"\"\"\n    # 1. Compute the unbinned Fisher information\n    i_unb = calculate_i_unb(lam, T, N)\n\n    # 2. Find the optimal number of bins m_star\n    m_star = find_m_star(lam, T, N, c_min)\n\n    # 3. Compute the binned Fisher information for m_star\n    i_bin = 0.0\n    if m_star  0:\n        i_bin = calculate_i_bin(lam, T, N, m_star)\n\n    # 4. Compute the ratio of binned to unbinned information\n    ratio = i_bin / i_unb if i_unb  0 else 0.0\n\n    # 5. Return the results\n    return [m_star, i_unb, i_bin, ratio]\n\n\ndef calculate_i_unb(lam, T, N):\n    \"\"\"\n    Calculates the total unbinned Fisher information for N events.\n    Formula: N * (1/lambda^2 - (T^2 * exp(-lambda*T)) / (1 - exp(-lambda*T))^2)\n    \"\"\"\n    lam_T = lam * T\n    # The parameters ensure lam  0, T  0, so lam_T  0.\n    # No risk of division by zero in the denominator for lam_T  0.\n    exp_m_lam_T = np.exp(-lam_T)\n    one_minus_exp = 1.0 - exp_m_lam_T\n    \n    term2 = (T**2 * exp_m_lam_T) / (one_minus_exp**2)\n    i_one_event = (1.0 / lam**2) - term2\n    \n    return N * i_one_event\n\n\ndef find_m_star(lam, T, N, c_min):\n    \"\"\"\n    Finds the optimal number of bins m_star by finding the largest m\n    that satisfies the minimum occupancy constraint.\n    \"\"\"\n    m_star_found = 0\n    lam_T = lam * T\n\n    # Constraint: N * p_m = c_min = expm1(lam*T/m) = (c_min/N_tot) * expm1(lam*T)\n    # Target for comparison to avoid recomputing the RHS in the loop\n    if np.isclose(lam_T, 0): # Should not happen with given parameters\n        return 1 if N = c_min else 0\n        \n    threshold = (c_min / N) * np.expm1(lam_T)\n\n    # Linearly scan for m from 1 to N.\n    # p_m is monotonically decreasing in m, so we can stop when the constraint fails.\n    for m in range(1, N + 1):\n        lam_T_over_m = lam_T / m\n        expm1_val = np.expm1(lam_T_over_m)\n        if expm1_val = threshold:\n            m_star_found = m\n        else:\n            break\n            \n    return m_star_found\n\n\ndef calculate_i_bin(lam, T, N, m):\n    \"\"\"\n    Calculates the total binned Fisher information for a given number of bins m.\n    \"\"\"\n    delta = T / m\n    lam_T = lam * T\n    lam_delta = lam * delta\n\n    def h(x):\n        \"\"\"Helper function for x*exp(x)/(exp(x)-1), stable for x near 0.\"\"\"\n        if np.isclose(x, 0):\n            return 1.0\n        return (x * np.exp(x)) / np.expm1(x)\n\n    h_lam_T = h(lam_T)\n    h_lam_delta = h(lam_delta)\n    \n    if np.isclose(lam_T, 0):\n        return 0.0\n        \n    expm1_lam_T = np.expm1(lam_T)\n    expm1_lam_delta = np.expm1(lam_delta)\n    \n    common_pk_factor = expm1_lam_delta / expm1_lam_T\n    common_dpk_term = (h_lam_delta - h_lam_T) / lam\n\n    info_sum = 0.0\n    for k in range(1, m + 1):\n        T_minus_k_delta = T - k * delta\n        \n        # Calculate p_k = exp(lam*(T-k*delta)) * common_pk_factor\n        pk = np.exp(lam * T_minus_k_delta) * common_pk_factor\n        \n        # Calculate d(log p_k)/d(lam) = (T-k*delta) + common_dpk_term\n        dlogpk_dlam = T_minus_k_delta + common_dpk_term\n        \n        # Term in sum for info is p_k * (d(log p_k)/d(lam))^2\n        if pk  0:\n            info_sum += pk * (dlogpk_dlam**2)\n    \n    return N * info_sum\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3540423"}, {"introduction": "In many real-world analyses, it is impractical or undesirable to use the full dataset, leading to the use of summary statistics. This practice explores the consequences of such data reduction on statistical power, grounded in the principle that not all data points contribute equally to constraining a parameter of interest. By quantifying the sensitivity loss for different summary statistic selection strategies [@problem_id:3540393], you will develop a critical skill for designing efficient and powerful physics analyses.", "problem": "Consider a one-dimensional binned counting experiment typical of computational high-energy physics, with $m$ equal-width bins covering the range $x \\in [0,1]$. Events are modeled as independent Poisson counts per bin. The expected count in bin $i$ (with bin edges $x_{i-1} = (i-1)/m$ and $x_i = i/m$) is given by\n$$\n\\mu_i(\\theta) \\;=\\; \\lambda_b \\,\\Delta \\;+\\; \\lambda_s \\int_{x_{i-1}}^{x_i} \\exp\\!\\left(-\\frac{(x - \\theta)^2}{2\\sigma^2}\\right)\\,dx,\n$$\nwhere $\\Delta = 1/m$ is the bin width, $\\lambda_b$ is a uniform background rate, $\\lambda_s$ is the signal amplitude, $\\sigma$ is the fixed width of the signal, and $\\theta \\in (0,1)$ is the unknown location parameter of the signal peak. All quantities are dimensionless. Assume independent Poisson fluctuations in each bin.\n\nDefine the full binned Poisson likelihood for the expected counts $\\mu_i(\\theta)$ and the observed counts $n_i$ as\n$$\n\\mathcal{L}_{\\text{full}}(\\theta) \\;=\\; \\prod_{i=1}^{m} \\mathrm{Poisson}\\!\\left(n_i \\,\\middle|\\, \\mu_i(\\theta)\\right).\n$$\nWorking in the Asimov regime (replace $n_i$ by $\\mu_i(\\theta)$ pointwise), the Fisher information for $\\theta$ under the full binned Poisson model is\n$$\nI_{\\text{full}}(\\theta) \\;=\\; \\sum_{i=1}^{m} \\frac{\\left(\\frac{\\partial \\mu_i(\\theta)}{\\partial \\theta}\\right)^2}{\\mu_i(\\theta)}.\n$$\n\nNow consider an Approximate Bayesian Computation (ABC)-style summary-statistic fit: instead of the full count vector, we select a summary vector of dimension $p$ consisting of bin counts $n_i$ for bins in an index set $S \\subset \\{1,\\dots,m\\}$ with $|S|=p$. Model these summaries with a Gaussian synthetic likelihood with diagonal covariance equal to the corresponding Poisson variances,\n$$\n\\mathcal{L}_{\\text{sum}}(\\theta) \\;\\propto\\; \\exp\\!\\left(-\\frac{1}{2}\\sum_{i \\in S} \\frac{\\big(n_i - \\mu_i(\\theta)\\big)^2}{\\mu_i(\\theta)}\\right),\n$$\nwhich yields the Fisher information for $\\theta$ under the summary-statistic model\n$$\nI_{\\text{sum}}(\\theta) \\;=\\; \\sum_{i \\in S} \\frac{\\left(\\frac{\\partial \\mu_i(\\theta)}{\\partial \\theta}\\right)^2}{\\mu_i(\\theta)}.\n$$\n\nDefine the sensitivity ratio\n$$\nR(p, S, \\theta) \\;=\\; \\frac{I_{\\text{sum}}(\\theta)}{I_{\\text{full}}(\\theta)},\n$$\nand the corresponding sensitivity loss\n$$\nL(p, S, \\theta) \\;=\\; 1 - R(p, S, \\theta).\n$$\nThe goal is to quantify the sensitivity loss as the summary dimension $p$ varies and as the choice of index set $S$ changes.\n\nYour task is to implement a complete program that:\n- Uses $m = 64$, $\\sigma = 0.05$, $\\lambda_b = 400$, and $\\lambda_s = 1200$ as baseline model parameters.\n- Computes $\\mu_i(\\theta)$ and $\\frac{\\partial \\mu_i(\\theta)}{\\partial \\theta}$ numerically for each bin via mid-point quadrature with at least $200$ uniform samples per bin.\n- Computes $I_{\\text{full}}(\\theta)$ using the Asimov prescription.\n- For a given summary dimension $p$ and selection rule for $S$, computes $I_{\\text{sum}}(\\theta)$ and the sensitivity ratio $R(p, S, \\theta)$.\n\nTwo selection rules for $S$ must be supported:\n- Best-$p$: choose the index set $S$ consisting of the $p$ bins with largest $|\\frac{\\partial \\mu_i(\\theta)}{\\partial \\theta}|$.\n- Naive-first-$p$: choose $S = \\{1,2,\\dots,p\\}$ (the first $p$ bins in index order).\n\nTo test different facets of the solution, use the following test suite of parameter tuples $(p, \\theta, \\text{rule}, \\alpha)$, where $\\text{rule} \\in \\{ \\text{\"best\"}, \\text{\"first\"} \\}$ and $\\alpha$ scales the signal amplitude by replacing $\\lambda_s$ with $\\alpha \\lambda_s$:\n1. $(4, 0.50, \\text{\"best\"}, 1.0)$\n2. $(16, 0.50, \\text{\"best\"}, 1.0)$\n3. $(64, 0.50, \\text{\"best\"}, 1.0)$\n4. $(16, 0.05, \\text{\"best\"}, 1.0)$\n5. $(16, 0.50, \\text{\"first\"}, 1.0)$\n6. $(16, 0.50, \\text{\"best\"}, 0.2)$\n\nFor each test case, output the sensitivity ratio $R(p, S, \\theta)$ as a float. Your program should produce a single line of output containing the six results as a comma-separated list enclosed in square brackets, with each float rounded to six decimal places (e.g., $[0.123456,0.654321,\\dots]$). No physical units are involved; all quantities are dimensionless. Angles do not appear. Percentages must not be used; the outputs are raw decimal fractions.", "solution": "The problem is well-posed, scientifically sound, and provides all necessary information for a unique solution. It is a standard exercise in computational statistics for high-energy physics, involving binned likelihoods, Fisher information, and summary statistics. The problem is therefore deemed valid.\n\nThe core of the problem is to compute the sensitivity ratio $R(p, S, \\theta) = I_{\\text{sum}}(\\theta) / I_{\\text{full}}(\\theta)$, which quantifies the information loss when moving from a full set of bin counts to a smaller subset of summary statistics. This requires computing the Fisher information for two different statistical models of a binned counting experiment.\n\nFirst, we define the model for the expected number of events, $\\mu_i(\\theta)$, in bin $i$. The $m$ bins partition the domain $x \\in [0, 1]$ into intervals $[x_{i-1}, x_i]$, where $x_i = i/m$ for $i=1, \\dots, m$. The bin width is $\\Delta = 1/m$. The expected count is a sum of a uniform background component and a Gaussian signal component:\n$$\n\\mu_i(\\theta) \\;=\\; \\lambda_b \\,\\Delta \\;+\\; \\lambda_s \\int_{x_{i-1}}^{x_i} \\exp\\!\\left(-\\frac{(x - \\theta)^2}{2\\sigma^2}\\right)\\,dx\n$$\nHere, $\\lambda_b$ is the background rate, $\\lambda_s$ is the signal amplitude, $\\sigma$ is the signal width, and $\\theta$ is the signal peak position.\n\nThe Fisher information, in the Asimov regime where observed counts $n_i$ are replaced by their expectations $\\mu_i(\\theta)$, is given by:\n$$\nI(\\theta) \\;=\\; \\sum_{i} \\frac{\\left(\\frac{\\partial \\mu_i(\\theta)}{\\partial \\theta}\\right)^2}{\\mu_i(\\theta)}\n$$\nThe sum is over all bins for the full model ($I_{\\text{full}}$), and over a selected subset of bins $S$ for the summary model ($I_{\\text{sum}}$). To compute this, we need the derivative $\\frac{\\partial \\mu_i(\\theta)}{\\partial \\theta}$. Since the bin limits do not depend on $\\theta$, we can differentiate under the integral sign (Leibniz rule):\n$$\n\\frac{\\partial \\mu_i(\\theta)}{\\partial \\theta} \\;=\\; \\frac{\\partial}{\\partial \\theta} \\left( \\lambda_s \\int_{x_{i-1}}^{x_i} \\exp\\!\\left(-\\frac{(x - \\theta)^2}{2\\sigma^2}\\right)\\,dx \\right) \\;=\\; \\lambda_s \\int_{x_{i-1}}^{x_i} \\frac{\\partial}{\\partial \\theta} \\exp\\!\\left(-\\frac{(x - \\theta)^2}{2\\sigma^2}\\right)\\,dx\n$$\nThe derivative of the integrand is:\n$$\n\\frac{\\partial}{\\partial \\theta} \\exp\\!\\left(-\\frac{(x - \\theta)^2}{2\\sigma^2}\\right) \\;=\\; \\exp\\!\\left(-\\frac{(x - \\theta)^2}{2\\sigma^2}\\right) \\cdot \\left( -\\frac{2(x-\\theta)(-1)}{2\\sigma^2} \\right) \\;=\\; \\exp\\!\\left(-\\frac{(x - \\theta)^2}{2\\sigma^2}\\right) \\frac{x-\\theta}{\\sigma^2}\n$$\nThus, the derivative of the expected count is expressed as another integral:\n$$\n\\frac{\\partial \\mu_i(\\theta)}{\\partial \\theta} \\;=\\; \\lambda_s \\int_{x_{i-1}}^{x_i} \\exp\\!\\left(-\\frac{(x - \\theta)^2}{2\\sigma^2}\\right)\\frac{x-\\theta}{\\sigma^2}\\,dx\n$$\nThe problem specifies that both the integral for $\\mu_i(\\theta)$ and the one for its derivative must be computed numerically using mid-point quadrature with $N_{quad} \\ge 200$ samples per bin. For a generic integral $\\int_a^b f(x)dx$, this method approximates the value as $(b-a) \\cdot \\frac{1}{N_{quad}} \\sum_{k=1}^{N_{quad}} f(x_k)$, where the points $x_k$ are the midpoints of $N_{quad}$ equal sub-intervals partitioning $[a,b]$. For bin $i$ with width $\\Delta$, we use $N_{quad}=200$ points.\n\nThe overall algorithm proceeds as follows for each test case $(p, \\theta, \\text{rule}, \\alpha)$:\n1. Set model parameters: $m=64$, $\\sigma=0.05$, $\\lambda_b=400$, and $\\lambda_s = \\alpha \\cdot 1200$.\n2. For each bin $i=1, \\dots, m$:\n   a. Define bin edges $x_{i-1}=(i-1)/m$ and $x_i=i/m$.\n   b. Generate $N_{quad}=200$ quadrature points within $[x_{i-1}, x_i]$.\n   c. Numerically compute the signal integral part of $\\mu_i(\\theta)$ and the integral for $\\frac{\\partial \\mu_i(\\theta)}{\\partial \\theta}$.\n   d. Store the full values of $\\mu_i(\\theta)$ and $\\frac{\\partial \\mu_i(\\theta)}{\\partial \\theta}$ in arrays.\n3. Compute the per-bin Fisher information terms, $\\left(\\frac{\\partial \\mu_i(\\theta)}{\\partial \\theta}\\right)^2 / \\mu_i(\\theta)$, for all $m$ bins.\n4. Calculate the full Fisher information, $I_{\\text{full}}(\\theta)$, by summing these terms over all $m$ bins.\n5. Determine the index set $S$ of $p$ bins according to the given `rule`:\n   - `\"best\"`: Select the $p$ bins with the largest values of $|\\frac{\\partial \\mu_i(\\theta)}{\\partial \\theta}|$. This corresponds to the bins most sensitive to changes in $\\theta$.\n   - `\"first\"`: Select the first $p$ bins, i.e., $S = \\{1, 2, \\dots, p\\}$.\n6. Calculate the summary-statistic Fisher information, $I_{\\text{sum}}(\\theta)$, by summing the per-bin information terms over the bins in set $S$.\n7. Compute the sensitivity ratio $R(p, S, \\theta) = I_{\\text{sum}}(\\theta) / I_{\\text{full}}(\\theta)$. For the case $p=m$, this ratio should be $1$.\n\nThis procedure is implemented in the following Python program. It iterates through the specified test cases, computes the ratio for each, and formats the results into the required single-line output.", "answer": "```python\nimport numpy as np\n\ndef compute_sensitivity_ratio(p, theta, rule, alpha, m, sigma, lambda_b, lambda_s_base, n_quad):\n    \"\"\"\n    Computes the sensitivity ratio R(p, S, theta) for a given set of parameters.\n    \"\"\"\n    # Set model parameters for the current test case\n    lambda_s = alpha * lambda_s_base\n    delta = 1.0 / m\n    sigma2 = sigma**2\n\n    # Arrays to store bin-wise calculations\n    mu = np.zeros(m)\n    dmu_dtheta = np.zeros(m)\n    \n    # Pre-calculate quadrature offsets for re-use in each bin\n    # These are the midpoints of n_quad sub-intervals of width 1.\n    quad_offsets = (np.arange(n_quad) + 0.5) / n_quad\n\n    # Loop over each bin to calculate mu_i and its derivative\n    for i in range(m):\n        bin_edge_low = i / m\n        \n        # Generate N_quad sample points at the midpoints of sub-intervals within the current bin\n        x_samples = bin_edge_low + quad_offsets * delta\n        \n        # Evaluate the integrands at the sample points\n        signal_integrand_values = np.exp(-(x_samples - theta)**2 / (2 * sigma2))\n        deriv_integrand_values = signal_integrand_values * (x_samples - theta) / sigma2\n        \n        # Approximate the integrals using the mean of the integrand values\n        integral_signal = np.mean(signal_integrand_values) * delta\n        integral_deriv = np.mean(deriv_integrand_values) * delta\n        \n        # Compute the final mu_i and d(mu_i)/d(theta)\n        mu[i] = lambda_b * delta + lambda_s * integral_signal\n        dmu_dtheta[i] = lambda_s * integral_deriv\n\n    # Calculate the per-bin terms for Fisher information\n    # Adding a small epsilon to the denominator for numerical stability, though mu  0 is guaranteed by background.\n    info_terms = (dmu_dtheta**2) / (mu + 1e-15)\n\n    # Calculate the full Fisher Information by summing over all bins\n    I_full = np.sum(info_terms)\n\n    # If I_full is zero, the ratio is ill-defined. This should not happen with the given parameters.\n    # Return a meaningful value based on p.\n    if I_full == 0:\n        return 1.0 if p == m else 0.0\n\n    # Determine the index set S based on the selection rule\n    if rule == \"best\":\n        # Select indices of the p bins with the largest absolute derivative\n        abs_dmu = np.abs(dmu_dtheta)\n        # argsort sorts in ascending order, so we take the last p indices\n        indices_S = np.argsort(abs_dmu)[-p:]\n    elif rule == \"first\":\n        # Select the first p bins\n        indices_S = np.arange(p)\n    else:\n        # This case should not be reached with the given problem statement\n        raise ValueError(f\"Unknown selection rule: {rule}\")\n\n    # Calculate the summary-statistic Fisher Information by summing over the selected bins\n    I_sum = np.sum(info_terms[indices_S])\n\n    # Compute the final sensitivity ratio\n    R = I_sum / I_full\n    \n    return R\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print the results.\n    \"\"\"\n    # Baseline model parameters from the problem statement\n    m = 64\n    sigma = 0.05\n    lambda_b = 400.0\n    lambda_s_base = 1200.0\n    n_quad = 200\n\n    # Test suite of parameter tuples (p, theta, rule, alpha)\n    test_cases = [\n        (4, 0.50, \"best\", 1.0),\n        (16, 0.50, \"best\", 1.0),\n        (64, 0.50, \"best\", 1.0),\n        (16, 0.05, \"best\", 1.0),\n        (16, 0.50, \"first\", 1.0),\n        (16, 0.50, \"best\", 0.2),\n    ]\n\n    results = []\n    for p, theta, rule, alpha in test_cases:\n        ratio = compute_sensitivity_ratio(\n            p, theta, rule, alpha,\n            m, sigma, lambda_b, lambda_s_base, n_quad\n        )\n        # Format the result to six decimal places\n        results.append(f\"{ratio:.6f}\")\n\n    # Print the final output in the required format\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3540393"}, {"introduction": "Beyond parameter estimation, a core task in data analysis is model selection, such as deciding on the number of components in a mixture model. This advanced exercise reveals the subtle but significant impact of data representation on this process, using the Akaike and Bayesian Information Criteria ($AIC$ and $BIC$). By applying these criteria to both unbinned and binned fits [@problem_id:3540403], you will investigate how discretization can alter the penalty for model complexity and potentially change the conclusions of a statistical test.", "problem": "You are given a one-dimensional mixture of Gaussian probability density functions (PDFs) as a generative model for independent event-level observations, and its binned counterpart as a Poisson process for bin counts. Consider two rival model families indexed by the number of Gaussian components $K \\in \\{1,2\\}$, with unknown parameters to be fitted by maximum likelihood. For unbinned data, the independent and identically distributed event-level observations $\\{x_i\\}_{i=1}^N$ are assumed to be drawn from a normalized mixture density $p(x \\mid \\theta, K) = \\sum_{j=1}^{K} w_j \\, \\phi(x; \\mu_j, \\sigma_j)$, where $\\phi(x;\\mu,\\sigma)$ is a Gaussian PDF with mean $\\mu$ and standard deviation $\\sigma$, the mixture weights satisfy $\\sum_{j=1}^{K} w_j = 1$ and $w_j \\ge 0$, and $\\theta$ denotes the full parameter vector. For binned data, the observation is a vector of independent Poisson counts $\\{n_b\\}_{b=1}^{B}$ for $B$ bins with edges $\\{a_b,b_b\\}$, and the expected count in bin $b$ under parameters $\\theta$ is $E_b(\\theta, K) = N \\, P_b(\\theta, K)$, where $P_b(\\theta, K) = \\int_{a_b}^{b_b} p(x \\mid \\theta, K)\\, dx$ and $N = \\sum_{b=1}^B n_b$ is the fixed total number of events.\n\nStarting from the fundamental definitions below, implement and compare the Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC) in the unbinned and binned settings, and investigate how discretization (binning) alters the effectiveness of these penalties in selecting the number of Gaussian components. Your implementation must be a complete, runnable program that performs maximum likelihood fits and computes AIC/BIC to select between $K=1$ and $K=2$.\n\nFundamental laws and definitions to be used:\n- For unbinned independent data $\\{x_i\\}_{i=1}^N$, the log-likelihood is $\\ell_{\\mathrm{unb}}(\\theta, K) = \\sum_{i=1}^{N} \\log p(x_i \\mid \\theta, K)$ for a normalized PDF $p(x \\mid \\theta, K)$.\n- For binned independent Poisson counts $\\{n_b\\}_{b=1}^B$ with expected counts $\\{E_b(\\theta, K)\\}_{b=1}^B$, the extended log-likelihood is $\\ell_{\\mathrm{bin}}(\\theta, K) = \\sum_{b=1}^B \\left[n_b \\log E_b(\\theta, K) - E_b(\\theta, K) - \\log(n_b!)\\right]$. Terms independent of $\\theta$ may be dropped for maximization and model comparison if they cancel between models.\n- The Akaike Information Criterion (AIC) is $\\mathrm{AIC} = 2k - 2 \\, \\ell(\\hat{\\theta}, K)$, with $k$ the number of free parameters and $\\hat{\\theta}$ the maximum likelihood estimator. The Bayesian Information Criterion (BIC) is $\\mathrm{BIC} = k \\log M - 2 \\, \\ell(\\hat{\\theta}, K)$, where $M$ is the sample size: for unbinned fits use $M=N$, and for binned fits use $M=B$ (the number of independent Poisson counts). In both families, use $k=2$ for $K=1$ (one mean and one standard deviation) and $k=5$ for $K=2$ (two means, two standard deviations, and one independent mixture weight).\n- The Gaussian Cumulative Distribution Function (CDF) appears when integrating a Gaussian PDF over a bin, via $P_b(\\theta, K) = \\sum_{j=1}^K w_j \\left[\\Phi\\!\\left(\\frac{b_b - \\mu_j}{\\sigma_j}\\right) - \\Phi\\!\\left(\\frac{a_b - \\mu_j}{\\sigma_j}\\right)\\right]$, where $\\Phi$ is the Gaussian CDF.\n\nYour program must:\n- Simulate unbinned data for each test case from a specified true mixture using a fixed random seed, then construct binned counts by histogramming into a specified number of equal-width bins on a range chosen by the true mixtureâ€™s overall mean and variance.\n- Perform maximum likelihood estimation separately for the unbinned likelihood and for the binned likelihood, for each candidate model $K \\in \\{1,2\\}$. For the unbinned case, you may use the Expectation-Maximization (EM) algorithm for $K=2$ and the closed-form maximum likelihood estimator for $K=1$. For the binned case, perform numerical optimization of the binned log-likelihood (you may drop constant terms that cancel in model selection), with suitable unconstrained reparameterizations to enforce positivity and mixture-weight constraints.\n- Compute $\\mathrm{AIC}$ and $\\mathrm{BIC}$ in both the unbinned and binned settings for $K=1$ and $K=2$, and select the $K$ that minimizes each criterion.\n- Output, for each test case, a list with four integers $[K_{\\mathrm{unb,AIC}}, K_{\\mathrm{unb,BIC}}, K_{\\mathrm{bin,AIC}}, K_{\\mathrm{bin,BIC}}]$.\n\nTest suite to ensure coverage:\n- Case $1$ (single component, coarse bins): seed $= 12345$, $N=1000$, true components $[(1.0, 0.0, 1.0)]$, $B=12$.\n- Case $2$ (single component, fine bins): seed $= 12345$, $N=1000$, true components $[(1.0, 0.0, 1.0)]$, $B=60$.\n- Case $3$ (well-separated two components, coarse bins): seed $= 12346$, $N=1000$, true components $[(0.5, -2.0, 0.7), (0.5, 2.0, 0.7)]$, $B=12$.\n- Case $4$ (well-separated two components, fine bins): seed $= 12346$, $N=1000$, true components $[(0.5, -2.0, 0.7), (0.5, 2.0, 0.7)]$, $B=60$.\n- Case $5$ (overlapping two components, coarse bins): seed $= 12347$, $N=500$, true components $[(0.7, 0.0, 1.0), (0.3, 0.8, 1.0)]$, $B=12$.\n- Case $6$ (overlapping two components, fine bins): seed $= 12347$, $N=500$, true components $[(0.7, 0.0, 1.0), (0.3, 0.8, 1.0)]$, $B=60$.\n- Case $7$ (small sample, two components, coarse bins): seed $= 12348$, $N=80$, true components $[(0.5, -1.0, 1.0), (0.5, 1.0, 1.0)]$, $B=12$.\n- Case $8$ (small sample, two components, fine bins): seed $= 12348$, $N=80$, true components $[(0.5, -1.0, 1.0), (0.5, 1.0, 1.0)]$, $B=60$.\n\nRange and binning instruction:\n- For each case, define the binning range as $[\\mu_{\\mathrm{mix}} - 5 \\sigma_{\\mathrm{mix}}, \\mu_{\\mathrm{mix}} + 5 \\sigma_{\\mathrm{mix}}]$, where $\\mu_{\\mathrm{mix}} = \\sum_j w_j \\mu_j$ and $\\sigma_{\\mathrm{mix}}^2 = \\sum_j w_j (\\sigma_j^2 + \\mu_j^2) - \\mu_{\\mathrm{mix}}^2$. Use $B$ equal-width bins over this range.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list of lists with no spaces, in the order of cases $1$ through $8$, for example: \"[[k11,k12,k13,k14],[k21,k22,k23,k24],...,[k81,k82,k83,k84]]\", where each $k$ is an integer in $\\{1,2\\}$ indicating the selected number of components under the specified criterion and data type.", "solution": "The problem is valid as it is scientifically grounded, well-posed, and contains all necessary information for a unique, verifiable solution. It requires the implementation of standard statistical methods in the context of high-energy physics data analysis. The solution proceeds by first generating data, then performing maximum likelihood fits under two model hypotheses ($K=1$ and $K=2$ Gaussian components) for both unbinned and binned representations of the data, and finally applying information criteria to select the preferred model in each case.\n\n### 1. Data Simulation and Preparation\nFor each test case, a dataset of $N$ independent events $\\{x_i\\}_{i=1}^N$ is simulated. Each event is drawn from the true generative probability density function (PDF), which is a mixture of Gaussian distributions:\n$$p(x \\mid \\theta_{\\mathrm{true}}, K_{\\mathrm{true}}) = \\sum_{j=1}^{K_{\\mathrm{true}}} w_j \\, \\phi(x; \\mu_j, \\sigma_j)$$\nwhere $\\phi(x; \\mu, \\sigma)$ is the Gaussian PDF, and $\\{w_j, \\mu_j, \\sigma_j\\}$ are the true mixture parameters. This is achieved by first sampling a component index $j$ with probability $w_j$, and then drawing a value from the corresponding Gaussian distribution $\\mathcal{N}(\\mu_j, \\sigma_j^2)$.\n\nFor the binned analysis, the data is discretized into a histogram of $B$ bins. The range of the histogram is determined by the overall mean $\\mu_{\\mathrm{mix}}$ and standard deviation $\\sigma_{\\mathrm{mix}}$ of the true mixture distribution, spanning $[\\mu_{\\mathrm{mix}} - 5 \\sigma_{\\mathrm{mix}}, \\mu_{\\mathrm{mix}} + 5 \\sigma_{\\mathrm{mix}}]$. The mixture mean is $\\mu_{\\mathrm{mix}} = \\sum_j w_j \\mu_j$. The mixture variance $\\sigma_{\\mathrm{mix}}^2$ is found using the law of total variance:\n$$\\sigma_{\\mathrm{mix}}^2 = E[\\sigma^2(X|C)] + \\mathrm{Var}(E[X|C]) = \\sum_{j=1}^{K_{\\mathrm{true}}} w_j \\sigma_j^2 + \\left( \\sum_{j=1}^{K_{\\mathrm{true}}} w_j \\mu_j^2 \\right) - \\mu_{\\mathrm{mix}}^2$$\nThe resulting histogram consists of counts $\\{n_b\\}_{b=1}^B$ in each bin, where $\\sum_b n_b = N$.\n\n### 2. Unbinned Maximum Likelihood Estimation\nThe goal is to find the parameter vector $\\hat{\\theta}$ that maximizes the log-likelihood function $\\ell_{\\mathrm{unb}}(\\theta, K) = \\sum_{i=1}^{N} \\log p(x_i \\mid \\theta, K)$.\n\nFor the $K=1$ model, $p(x \\mid \\theta, 1) = \\phi(x; \\mu, \\sigma)$. The maximum likelihood estimators (MLEs) for the parameters have closed-form solutions: the sample mean $\\hat{\\mu} = \\frac{1}{N}\\sum_i x_i$ and the sample variance $\\hat{\\sigma}^2 = \\frac{1}{N}\\sum_i (x_i - \\hat{\\mu})^2$. The maximized log-likelihood is then $\\ell_{\\mathrm{unb}}(\\hat{\\theta}, 1) = \\sum_{i=1}^{N} \\log \\phi(x_i; \\hat{\\mu}, \\hat{\\sigma})$.\n\nFor the $K=2$ model, $p(x \\mid \\theta, 2) = w_1 \\phi(x; \\mu_1, \\sigma_1) + (1-w_1) \\phi(x; \\mu_2, \\sigma_2)$. The likelihood for this mixture model does not have a closed-form maximum. The Expectation-Maximization (EM) algorithm is an iterative procedure well-suited for this problem.\n1.  **Initialization**: Parameters $(\\mu_1, \\sigma_1, \\mu_2, \\sigma_2, w_1)$ are initialized. A deterministic approach is used where the sorted data is split in two, and the moments of each half provide the initial means and standard deviations.\n2.  **E-Step**: The posterior probability, or \"responsibility,\" of component $j$ for generating data point $x_i$ is computed:\n    $$\\gamma_{ij} = \\frac{w_j \\phi(x_i; \\mu_j, \\sigma_j)}{\\sum_{l=1}^{2} w_l \\phi(x_i; \\mu_l, \\sigma_l)}$$\n3.  **M-Step**: The parameters are updated to maximize the expected log-likelihood, given the current responsibilities:\n    $$N_j = \\sum_{i=1}^{N} \\gamma_{ij}, \\quad w_j^{\\mathrm{new}} = \\frac{N_j}{N}, \\quad \\mu_j^{\\mathrm{new}} = \\frac{1}{N_j} \\sum_{i=1}^{N} \\gamma_{ij} x_i, \\quad (\\sigma_j^{\\mathrm{new}})^2 = \\frac{1}{N_j} \\sum_{i=1}^{N} \\gamma_{ij} (x_i - \\mu_j^{\\mathrm{new}})^2$$\nThe E-step and M-step are repeated until the change in the total log-likelihood $\\ell_{\\mathrm{unb}}(\\theta, 2) = \\sum_i \\log p(x_i \\mid \\theta, 2)$ falls below a tolerance.\n\n### 3. Binned Maximum Likelihood Estimation\nFor binned data, the counts $\\{n_b\\}$ are treated as independent Poisson random variables with expectations $\\{E_b(\\theta, K)\\}$. The model selection is based on the binned extended log-likelihood, from which terms independent of parameters $\\theta$ (i.e., $\\sum_b \\log(n_b!)$) are dropped, as they cancel when comparing models:\n$$\\ell^*_{\\mathrm{bin}}(\\theta, K) = \\sum_{b=1}^{B} \\left[ n_b \\log E_b(\\theta, K) - E_b(\\theta, K) \\right]$$\nThe expected count in bin $b$, with edges $[a_b, b_b]$, is given by $E_b(\\theta, K) = N \\cdot P_b(\\theta, K)$, where $P_b(\\theta, K)$ is the probability of an event falling in that bin. This probability is calculated by integrating the PDF over the bin, which involves the Gaussian cumulative distribution function (CDF), $\\Phi(z)$:\n$$P_b(\\theta, K) = \\int_{a_b}^{b_b} p(x \\mid \\theta, K) \\, dx = \\sum_{j=1}^{K} w_j \\left[ \\Phi\\left(\\frac{b_b - \\mu_j}{\\sigma_j}\\right) - \\Phi\\left(\\frac{a_b - \\mu_j}{\\sigma_j}\\right) \\right]$$\nMaximizing $\\ell^*_{\\mathrm{bin}}$ is performed via numerical optimization. To handle parameter constraints, the optimization is done over an unconstrained space. Standard deviations $\\sigma_j  0$ are reparameterized as $\\sigma_j = \\exp(\\log \\sigma_j)$. The mixture weight $w_1 \\in [0, 1]$ for the $K=2$ case is reparameterized using the logit transformation, $w_1 = (1 + \\exp(-\\alpha_1))^{-1}$. The parameters found from the unbinned fits serve as initial values for the numerical minimizer to promote stable and efficient convergence.\n\n### 4. Model Selection\nThe Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC) are used to compare the performance of the $K=1$ and $K=2$ models. The model with the lower criterion value is preferred.\n$$\\mathrm{AIC} = 2k - 2 \\ell_{\\mathrm{max}}$$\n$$\\mathrm{BIC} = k \\log M - 2 \\ell_{\\mathrm{max}}$$\nHere, $\\ell_{\\mathrm{max}}$ is the maximized log-likelihood (either $\\ell_{\\mathrm{unb}}$ or $\\ell^*_{\\mathrm{bin}}$), and $k$ is the number of free parameters in the model. As specified, $k=2$ for the $K=1$ model $(\\mu, \\sigma)$ and $k=5$ for the $K=2$ model $(\\mu_1, \\sigma_1, \\mu_2, \\sigma_2, w_1)$. The sample size $M$ is defined as the number of independent observations: for the unbinned fit, $M=N$ (the number of events), and for the binned fit, $M=B$ (the number of bins). This analysis is performed for both unbinned and binned data, yielding four model selections for each test case.", "answer": "```python\nimport numpy as np\nfrom scipy import stats, optimize, special\n\ndef solve():\n    \"\"\"\n    Implements the full analysis pipeline for comparing AIC/BIC in unbinned and binned\n    Gaussian mixture model fits, as specified in the problem statement.\n    \"\"\"\n\n    test_cases = [\n        {'seed': 12345, 'N': 1000, 'components': [(1.0, 0.0, 1.0)], 'B': 12},\n        {'seed': 12345, 'N': 1000, 'components': [(1.0, 0.0, 1.0)], 'B': 60},\n        {'seed': 12346, 'N': 1000, 'components': [(0.5, -2.0, 0.7), (0.5, 2.0, 0.7)], 'B': 12},\n        {'seed': 12346, 'N': 1000, 'components': [(0.5, -2.0, 0.7), (0.5, 2.0, 0.7)], 'B': 60},\n        {'seed': 12347, 'N': 500, 'components': [(0.7, 0.0, 1.0), (0.3, 0.8, 1.0)], 'B': 12},\n        {'seed': 12347, 'N': 500, 'components': [(0.7, 0.0, 1.0), (0.3, 0.8, 1.0)], 'B': 60},\n        {'seed': 12348, 'N': 80, 'components': [(0.5, -1.0, 1.0), (0.5, 1.0, 1.0)], 'B': 12},\n        {'seed': 12348, 'N': 80, 'components': [(0.5, -1.0, 1.0), (0.5, 1.0, 1.0)], 'B': 60},\n    ]\n\n    results = []\n\n    for case in test_cases:\n        # --- 1. Data Generation and Preparation ---\n        rng = np.random.default_rng(case['seed'])\n        N = case['N']\n        components = case['components']\n        n_comp = len(components)\n        \n        if n_comp == 1:\n            w, mu, sigma = components[0]\n            unbinned_data = rng.normal(mu, sigma, N)\n        else:\n            weights, means, sigmas = zip(*components)\n            component_indices = rng.choice(n_comp, size=N, p=weights)\n            unbinned_data = np.concatenate([\n                rng.normal(means[i], sigmas[i], np.sum(component_indices == i))\n                for i in range(n_comp)\n            ])\n\n        B = case['B']\n        k_params = {1: 2, 2: 5}\n\n        # --- 2. Unbinned Fits ---\n        # K=1\n        mu1_unb, sigma1_unb = np.mean(unbinned_data), np.std(unbinned_data)\n        logL_unb_k1 = np.sum(stats.norm.logpdf(unbinned_data, loc=mu1_unb, scale=sigma1_unb))\n        \n        # K=2 (EM algorithm)\n        x_sorted = np.sort(unbinned_data)\n        split = N // 2\n        mu1_init, mu2_init = np.mean(x_sorted[:split]), np.mean(x_sorted[split:])\n        sigma1_init, sigma2_init = np.std(x_sorted[:split]), np.std(x_sorted[split:])\n        w1_init = 0.5\n        \n        mu1_em, mu2_em = mu1_init, mu2_init\n        sigma1_em, sigma2_em = max(sigma1_init, 1e-6), max(sigma2_init, 1e-6)\n        w1_em = w1_init\n        \n        for _ in range(100):\n            pdf1 = stats.norm.pdf(unbinned_data, mu1_em, sigma1_em)\n            pdf2 = stats.norm.pdf(unbinned_data, mu2_em, sigma2_em)\n            \n            resp_num1 = w1_em * pdf1\n            denominator = resp_num1 + (1 - w1_em) * pdf2\n            denominator[denominator  1e-9] = 1e-9\n            resp1 = resp_num1 / denominator\n            \n            N1 = np.sum(resp1)\n            if N1  1e-6 or N - N1  1e-6: break\n\n            w1_em = N1 / N\n            mu1_em = np.sum(resp1 * unbinned_data) / N1\n            mu2_em = np.sum((1 - resp1) * unbinned_data) / (N - N1)\n            sigma1_em = np.sqrt(np.sum(resp1 * (unbinned_data - mu1_em)**2) / N1 + 1e-9)\n            sigma2_em = np.sqrt(np.sum((1 - resp1) * (unbinned_data - mu2_em)**2) / (N - N1) + 1e-9)\n\n        logL_unb_k2 = np.sum(np.log(w1_em * stats.norm.pdf(unbinned_data, mu1_em, sigma1_em) + \n                                     (1 - w1_em) * stats.norm.pdf(unbinned_data, mu2_em, sigma2_em) + 1e-12))\n        \n        # Unbinned model selection\n        aic_unb_k1, bic_unb_k1 = 2 * k_params[1] - 2 * logL_unb_k1, k_params[1] * np.log(N) - 2 * logL_unb_k1\n        aic_unb_k2, bic_unb_k2 = 2 * k_params[2] - 2 * logL_unb_k2, k_params[2] * np.log(N) - 2 * logL_unb_k2\n        K_unb_aic = 1 if aic_unb_k1  aic_unb_k2 else 2\n        K_unb_bic = 1 if bic_unb_k1  bic_unb_k2 else 2\n\n        # --- 3. Binned Fits ---\n        true_w, true_mu, true_s = zip(*components)\n        mu_mix = np.sum(np.array(true_w) * np.array(true_mu))\n        var_mix = np.sum(np.array(true_w) * (np.array(true_s)**2 + np.array(true_mu)**2)) - mu_mix**2\n        bin_range = (mu_mix - 5 * np.sqrt(var_mix), mu_mix + 5 * np.sqrt(var_mix))\n        binned_counts, bin_edges = np.histogram(unbinned_data, bins=B, range=bin_range)\n        \n        # Binned Negative Log-Likelihood\n        def binned_nll(params, K):\n            if K == 1:\n                mu1, log_sigma1 = params\n                prob = stats.norm.cdf(bin_edges[1:], mu1, np.exp(log_sigma1)) - \\\n                       stats.norm.cdf(bin_edges[:-1], mu1, np.exp(log_sigma1))\n            else: # K=2\n                mu1, log_sigma1, mu2, log_sigma2, alpha1 = params\n                w1 = special.expit(alpha1)\n                p1 = stats.norm.cdf(bin_edges[1:], mu1, np.exp(log_sigma1)) - \\\n                     stats.norm.cdf(bin_edges[:-1], mu1, np.exp(log_sigma1))\n                p2 = stats.norm.cdf(bin_edges[1:], mu2, np.exp(log_sigma2)) - \\\n                     stats.norm.cdf(bin_edges[:-1], mu2, np.exp(log_sigma2))\n                prob = w1 * p1 + (1 - w1) * p2\n            \n            E_b = N * prob\n            nll = -np.sum(binned_counts * np.log(E_b + 1e-12) - E_b)\n            return nll if np.isfinite(nll) else np.inf\n\n        # K=1 binned fit\n        init_bin_k1 = [mu1_unb, np.log(sigma1_unb)]\n        res_bin_k1 = optimize.minimize(binned_nll, init_bin_k1, args=(1,), method='Nelder-Mead')\n        logL_bin_k1 = -res_bin_k1.fun\n\n        # K=2 binned fit\n        w1_init_clip = np.clip(w1_em, 1e-6, 1 - 1e-6)\n        init_bin_k2 = [mu1_em, np.log(sigma1_em), mu2_em, np.log(sigma2_em), special.logit(w1_init_clip)]\n        res_bin_k2 = optimize.minimize(binned_nll, init_bin_k2, args=(2,), method='Nelder-Mead')\n        logL_bin_k2 = -res_bin_k2.fun\n\n        # Binned model selection\n        aic_bin_k1, bic_bin_k1 = 2 * k_params[1] - 2 * logL_bin_k1, k_params[1] * np.log(B) - 2 * logL_bin_k1\n        aic_bin_k2, bic_bin_k2 = 2 * k_params[2] - 2 * logL_bin_k2, k_params[2] * np.log(B) - 2 * logL_bin_k2\n        K_bin_aic = 1 if aic_bin_k1  aic_bin_k2 else 2\n        K_bin_bic = 1 if bic_bin_k1  bic_bin_k2 else 2\n\n        results.append([K_unb_aic, K_unb_bic, K_bin_aic, K_bin_bic])\n\n    # --- 4. Final Output ---\n    formatted_results = [f\"[{','.join(map(str, res))}]\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3540403"}]}