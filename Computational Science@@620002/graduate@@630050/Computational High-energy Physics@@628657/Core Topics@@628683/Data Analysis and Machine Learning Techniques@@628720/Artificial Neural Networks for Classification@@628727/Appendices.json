{"hands_on_practices": [{"introduction": "A powerful classifier is only the first step in a high-energy physics analysis; the ultimate goal is to translate its output into a physical measurement or discovery. This practice bridges the gap between machine learning and statistical inference by tasking you with optimizing a classifier's selection threshold to maximize the expected discovery significance [@problem_id:3505051]. By working through a well-defined statistical model, you will gain hands-on experience connecting the abstract score from an Artificial Neural Network (ANN) to the concrete statistical power of an experiment, a foundational skill for any particle physicist employing machine learning.", "problem": "Consider a binary classification problem in computational high-energy physics where Artificial Neural Networks (ANNs) are trained to discriminate signal events from background events. By the Neymanâ€“Pearson lemma, the optimal classifier score is a monotonic function of the likelihood ratio between the class-conditional probability density functions (PDFs). Let $x$ be a one-dimensional kinematic observable, and consider a classifier score $f(x)$ that is strictly monotonic with the likelihood ratio $p(x|s)/p(x|b)$, where $p(x|s)$ and $p(x|b)$ denote the PDFs for signal and background, respectively.\n\nAssume the following well-defined statistical model:\n- The PDFs are Gaussian with equal variance: $p(x|s) = \\mathcal{N}(\\mu_s, \\sigma)$ and $p(x|b) = \\mathcal{N}(\\mu_b, \\sigma)$ with $\\mu_s \\in \\mathbb{R}$, $\\mu_b \\in \\mathbb{R}$, and $\\sigma > 0$.\n- The pre-selection expected event counts are $S_0 > 0$ for signal and $B_0 > 0$ for background.\n- A threshold $\\tau > 0$ is applied on the classifier score $f(x)$, selecting events satisfying $f(x) > \\tau$.\n- Define the acceptance functionals\n$$\ns(\\tau) = \\int_{\\mathbb{R}} \\mathbb{1}_{f(x) > \\tau} \\, p(x|s) \\, dx,\n\\quad\nb(\\tau) = \\int_{\\mathbb{R}} \\mathbb{1}_{f(x) > \\tau} \\, p(x|b) \\, dx,\n$$\nand the post-selection expected counts\n$$\nS(\\tau) = S_0 \\, s(\\tau), \\quad B(\\tau) = B_0 \\, b(\\tau).\n$$\n\nStarting from the definition of the likelihood ratio test for discovery and the Poisson log-likelihood for a single-bin counting experiment, derive from first principles the discovery test statistic for the background-only hypothesis using the Asimov data set, and express the expected discovery significance $Z(\\tau)$ as a functional of the threshold $\\tau$, the Gaussian parameters $(\\mu_s, \\mu_b, \\sigma)$, and the pre-selection counts $(S_0,B_0)$. The quantity $Z(\\tau)$ is dimensionless.\n\nImplement a program that:\n1. Uses the monotonicity of the likelihood ratio for Gaussian PDFs with equal variance to convert the threshold $\\tau$ on $f(x)$ into an equivalent threshold $t(\\tau)$ on $x$.\n2. Computes $s(\\tau)$ and $b(\\tau)$ exactly for the Gaussian PDFs via the complementary cumulative distribution functions.\n3. Computes $S(\\tau)$ and $B(\\tau)$ for each test case.\n4. Computes $Z(\\tau)$ under the Asimov approximation of the Poisson likelihood-based discovery test statistic for the background-only hypothesis, handling the limiting case $B(\\tau) \\to 0$ rigorously.\n5. Produces a single line of output containing the results as a comma-separated list enclosed in square brackets, with each $Z(\\tau)$ rounded to six decimals.\n\nUse the following test suite of parameter values to evaluate your implementation. Each test case is a tuple $(\\mu_s, \\mu_b, \\sigma, S_0, B_0, \\tau)$:\n- Test $1$: $(\\mu_s, \\mu_b, \\sigma, S_0, B_0, \\tau) = (125, 100, 15, 50, 200, 1)$.\n- Test $2$: $(\\mu_s, \\mu_b, \\sigma, S_0, B_0, \\tau) = (125, 100, 15, 50, 200, 2)$.\n- Test $3$: $(\\mu_s, \\mu_b, \\sigma, S_0, B_0, \\tau) = (125, 100, 15, 50, 200, 0.5)$.\n- Test $4$: $(\\mu_s, \\mu_b, \\sigma, S_0, B_0, \\tau) = (125, 100, 15, 50, 200, 10)$.\n- Test $5$ (degenerate distributions): $(\\mu_s, \\mu_b, \\sigma, S_0, B_0, \\tau) = (100, 100, 15, 50, 200, 0.9)$.\n- Test $6$ (degenerate distributions): $(\\mu_s, \\mu_b, \\sigma, S_0, B_0, \\tau) = (100, 100, 15, 50, 200, 1.1)$.\n- Test $7$ (background-dominated): $(\\mu_s, \\mu_b, \\sigma, S_0, B_0, \\tau) = (125, 100, 15, 5, 1000, 2)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example $[z_1,z_2,\\dots,z_7]$, where each $z_i$ is the float value of $Z(\\tau)$ for Test $i$, rounded to six decimals. No units are required since $Z(\\tau)$ is dimensionless.", "solution": "The problem requires the derivation and implementation of a formula for the expected discovery significance $Z(\\tau)$ for a binary classification problem in high-energy physics. The derivation will proceed from first principles, assuming a statistical model based on Gaussian probability density functions (PDFs) and Poisson counting statistics.\n\nA central premise of the problem is that the classifier score, $f(x)$, is strictly monotonic with the likelihood ratio $L(x) = p(x|s)/p(x|b)$. The Neyman-Pearson lemma states that the most powerful statistical test for a given significance level is based on a threshold on this likelihood ratio. Therefore, the optimal selection criterion is of the form $L(x) > k$ for some constant $k$. We are given a cut $f(x) > \\tau$. For this to be an optimal cut, $f(x)$ must be a strictly increasing function of $L(x)$. The simplest and most direct interpretation, which we adopt for this solution, is that the classifier score $f(x)$ is the likelihood ratio itself, i.e., $f(x) = L(x)$. The threshold on the classifier score, $\\tau$, is therefore a threshold on the likelihood ratio.\n\nThe solution is structured in four principal steps:\n1.  Derivation of the selection threshold on the observable $x$.\n2.  Calculation of the signal and background acceptances, $s(\\tau)$ and $b(\\tau)$.\n3.  Calculation of the post-selection event counts, $S(\\tau)$ and $B(\\tau)$.\n4.  Derivation of the Asimov significance, $Z(\\tau)$.\n\n**Step 1: Derivation of the decision threshold on the observable $x$**\n\nThe signal and background PDFs are given as Gaussian distributions with a common standard deviation $\\sigma$:\n$$\np(x|s) = \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left(-\\frac{(x-\\mu_s)^2}{2\\sigma^2}\\right)\n\\quad \\text{and} \\quad\np(x|b) = \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left(-\\frac{(x-\\mu_b)^2}{2\\sigma^2}\\right)\n$$\nThe likelihood ratio $L(x)$ is:\n$$\nL(x) = \\frac{p(x|s)}{p(x|b)} = \\frac{\\exp\\left(-\\frac{(x-\\mu_s)^2}{2\\sigma^2}\\right)}{\\exp\\left(-\\frac{(x-\\mu_b)^2}{2\\sigma^2}\\right)} = \\exp\\left(\\frac{(x-\\mu_b)^2 - (x-\\mu_s)^2}{2\\sigma^2}\\right)\n$$\nExpanding the squares in the exponent:\n$$\n(x-\\mu_b)^2 - (x-\\mu_s)^2 = (x^2 - 2x\\mu_b + \\mu_b^2) - (x^2 - 2x\\mu_s + \\mu_s^2) = 2x(\\mu_s - \\mu_b) - (\\mu_s^2 - \\mu_b^2)\n$$\nThus, the logarithm of the likelihood ratio is a linear function of $x$:\n$$\n\\ln L(x) = \\frac{x(\\mu_s - \\mu_b)}{\\sigma^2} - \\frac{\\mu_s^2 - \\mu_b^2}{2\\sigma^2}\n$$\nThe selection criterion $f(x) > \\tau$, interpreted as $L(x) > \\tau$, is equivalent to $\\ln L(x) > \\ln \\tau$ since the logarithm is a strictly increasing function.\n$$\n\\frac{x(\\mu_s - \\mu_b)}{\\sigma^2} - \\frac{(\\mu_s - \\mu_b)(\\mu_s + \\mu_b)}{2\\sigma^2} > \\ln \\tau\n$$\nMultiplying by $\\sigma^2$ and isolating the term with $x$:\n$$\nx(\\mu_s - \\mu_b) > \\sigma^2 \\ln \\tau + \\frac{(\\mu_s - \\mu_b)(\\mu_s + \\mu_b)}{2}\n$$\nThe direction of the inequality for $x$ depends on the sign of $(\\mu_s - \\mu_b)$:\n-   If $\\mu_s > \\mu_b$: The term $(\\mu_s - \\mu_b)$ is positive. The selection region is $x > t(\\tau)$, where\n    $$\n    t(\\tau) = \\frac{\\sigma^2 \\ln \\tau}{\\mu_s - \\mu_b} + \\frac{\\mu_s + \\mu_b}{2}\n    $$\n-   If $\\mu_s < \\mu_b$: The term $(\\mu_s - \\mu_b)$ is negative. The inequality flips, and the selection region is $x < t(\\tau)$.\n-   If $\\mu_s = \\mu_b$: The PDFs are identical, so $L(x) = 1$ for all $x$. The condition becomes $1 > \\tau$.\n    -   If $\\tau < 1$, the condition is always true, and all events are selected. The region is $(-\\infty, \\infty)$.\n    -   If $\\tau \\ge 1$, the condition is always false, and no events are selected. The region is the empty set $\\emptyset$.\n\n**Step 2: Calculation of Signal and Background Acceptances**\n\nThe acceptances $s(\\tau)$ and $b(\\tau)$ are the probabilities that a signal or background event, respectively, falls into the selected region. These probabilities are calculated by integrating the corresponding PDFs over this region. We use the complementary cumulative distribution function (CCDF) of the standard normal distribution, known as the Q-function, $Q(z) = \\frac{1}{\\sqrt{2\\pi}} \\int_z^\\infty e^{-u^2/2} du$. For a general normal distribution $\\mathcal{N}(\\mu, \\sigma)$, the probability $P(X>x)$ is $Q((x-\\mu)/\\sigma)$. The Q-function is related to the complementary error function, $\\text{erfc}(z) = \\frac{2}{\\sqrt{\\pi}}\\int_z^\\infty e^{-t^2} dt$, by $Q(z) = \\frac{1}{2}\\text{erfc}(z/\\sqrt{2})$.\n\n-   Case $\\mu_s > \\mu_b$ (region is $x>t(\\tau)$):\n    $$\n    s(\\tau) = \\int_{t(\\tau)}^\\infty p(x|s) dx = Q\\left(\\frac{t(\\tau)-\\mu_s}{\\sigma}\\right)\n    $$\n    $$\n    b(\\tau) = \\int_{t(\\tau)}^\\infty p(x|b) dx = Q\\left(\\frac{t(\\tau)-\\mu_b}{\\sigma}\\right)\n    $$\n-   Case $\\mu_s < \\mu_b$ (region is $x<t(\\tau)$):\n    $$\n    s(\\tau) = \\int_{-\\infty}^{t(\\tau)} p(x|s) dx = 1 - Q\\left(\\frac{t(\\tau)-\\mu_s}{\\sigma}\\right)\n    $$\n    $$\n    b(\\tau) = \\int_{-\\infty}^{t(\\tau)} p(x|b) dx = 1 - Q\\left(\\frac{t(\\tau)-\\mu_b}{\\sigma}\\right)\n    $$\n-   Case $\\mu_s = \\mu_b$:\n    -   If $\\tau < 1$, the region is $(-\\infty, \\infty)$, so $s(\\tau)=1$ and $b(\\tau)=1$.\n    -   If $\\tau \\ge 1$, the region is $\\emptyset$, so $s(\\tau)=0$ and $b(\\tau)=0$.\n\n**Step 3: Calculation of Post-selection Event Counts**\n\nThe expected numbers of signal and background events after applying the selection are:\n$$\nS(\\tau) = S_0 \\, s(\\tau)\n\\quad \\text{and} \\quad\nB(\\tau) = B_0 \\, b(\\tau)\n$$\nwhere $S_0$ and $B_0$ are the pre-selection expected counts.\n\n**Step 4: Derivation of the Asimov Significance $Z(\\tau)$**\n\nThe problem asks for a discovery significance, which tests the background-only hypothesis ($H_0$) against the signal-plus-background hypothesis ($H_1$). For a single-bin counting experiment, the number of observed events $n$ follows a Poisson distribution. The likelihood is $L(\\mu) = P(n|\\mu S+B) = \\frac{(\\mu S+B)^n e^{-(\\mu S+B)}}{n!}$, where $S$ and $B$ are the expected counts $S(\\tau)$ and $B(\\tau)$, and $\\mu$ is the signal strength multiplier ($\\mu=0$ for $H_0$, $\\mu=1$ for $H_1$).\n\nThe test statistic for discovery is the profile likelihood ratio:\n$$\nq_0 = -2 \\ln \\frac{L(0)}{L(\\hat{\\mu})} = -2 \\ln \\frac{\\text{Pois}(n|B)}{\\text{Pois}(n|\\hat{\\mu}S+B)}\n$$\nwhere $\\hat{\\mu} = \\max(0, (n-B)/S)$ is the maximum likelihood estimate of $\\mu$. For $n > B$, this simplifies to:\n$$\nq_0 = 2\\left(n \\ln\\frac{n}{B} - (n-B)\\right)\n$$\nFor $n \\le B$, $q_0=0$.\n\nThe Asimov dataset is a representative dataset where the observation $n$ is set to its expectation value under the $H_1$ hypothesis, i.e., $n_A = S+B$. This provides the median expected significance. Substituting $n = S+B$ into the formula for $q_0$ (noting that $S>0$ implies $S+B>B$):\n$$\nq_{0,A} = 2\\left((S+B) \\ln\\frac{S+B}{B} - (S+B-B)\\right) = 2\\left((S+B) \\ln\\left(1+\\frac{S}{B}\\right) - S\\right)\n$$\nIn the large-sample limit, the significance $Z$ relates to the test statistic by $Z = \\sqrt{q_0}$. Therefore, the expected significance from the Asimov dataset is:\n$$\nZ(\\tau) = \\sqrt{q_{0,A}} = \\sqrt{2\\left((S(\\tau)+B(\\tau)) \\ln\\left(1+\\frac{S(\\tau)}{B(\\tau)}\\right) - S(\\tau)\\right)}\n$$\nThis formula is well-defined for $S(\\tau) \\ge 0$ and $B(\\tau) > 0$. We must consider the limiting cases:\n-   If $B(\\tau) \\to 0$ with $S(\\tau)>0$, the significance $Z(\\tau)$ diverges to infinity. However, for the given Gaussian model, $B(\\tau)$ is an integral of a strictly positive function over a region of non-zero measure, so $B(\\tau)$ will not be exactly zero unless the integration region is empty.\n-   If $S(\\tau) = 0$ (and $B(\\tau) \\ge 0$), then $Z(\\tau)=0$. No signal means no discovery is possible.\n-   If $S(\\tau) = 0$ and $B(\\tau) = 0$, which occurs in our model when $\\mu_s=\\mu_b$ and $\\tau \\ge 1$, then $Z(\\tau)=0$. If no events of any kind are expected, there is no information and thus zero significance.\n\nThe implementation will compute $S(\\tau)$ and $B(\\tau)$ for each test case and apply this formula for $Z(\\tau)$, correctly handling the special cases.", "answer": "```python\nimport numpy as np\nfrom scipy.special import erfc\n\ndef solve():\n    \"\"\"\n    Calculates the expected discovery significance for a set of test cases\n    in a high-energy physics classification problem.\n    \"\"\"\n    test_cases = [\n        # (mu_s, mu_b, sigma, S0, B0, tau)\n        (125.0, 100.0, 15.0, 50.0, 200.0, 1.0),\n        (125.0, 100.0, 15.0, 50.0, 200.0, 2.0),\n        (125.0, 100.0, 15.0, 50.0, 200.0, 0.5),\n        (125.0, 100.0, 15.0, 50.0, 200.0, 10.0),\n        (100.0, 100.0, 15.0, 50.0, 200.0, 0.9),\n        (100.0, 100.0, 15.0, 50.0, 200.0, 1.1),\n        (125.0, 100.0, 15.0, 5.0, 1000.0, 2.0),\n    ]\n\n    results = []\n    \n    # Q-function using scipy's complementary error function for numerical stability\n    # Q(z) = 0.5 * erfc(z / sqrt(2))\n    sqrt2 = np.sqrt(2.0)\n    def q_function(z):\n        return 0.5 * erfc(z / sqrt2)\n\n    for case in test_cases:\n        mu_s, mu_b, sigma, s0, b0, tau = case\n\n        s_acceptance = 0.0\n        b_acceptance = 0.0\n\n        if mu_s == mu_b:\n            # PDFs are identical, L(x) = 1 for all x.\n            # Selection depends only on the threshold tau relative to 1.\n            if tau < 1.0:\n                s_acceptance, b_acceptance = 1.0, 1.0\n            else:\n                s_acceptance, b_acceptance = 0.0, 0.0\n        else:\n            # Calculate the threshold t(tau) on the observable x.\n            # t(tau) = sigma^2 * ln(tau) / (mu_s - mu_b) + (mu_s + mu_b) / 2\n            t_tau = (sigma**2 * np.log(tau)) / (mu_s - mu_b) + (mu_s + mu_b) / 2.0\n\n            # Arguments for the Q-function\n            z_s = (t_tau - mu_s) / sigma\n            z_b = (t_tau - mu_b) / sigma\n\n            if mu_s > mu_b:\n                # Selection region is x > t(tau)\n                s_acceptance = q_function(z_s)\n                b_acceptance = q_function(z_b)\n            else: # mu_s < mu_b\n                # Selection region is x < t(tau)\n                s_acceptance = 1.0 - q_function(z_s)\n                b_acceptance = 1.0 - q_function(z_b)\n\n        # Post-selection expected counts\n        s_final = s0 * s_acceptance\n        b_final = b0 * b_acceptance\n\n        # Calculate Asimov significance Z\n        if b_final <= 0:\n            # If B=0 and S>0, significance is infinite.\n            # If B=0 and S=0, significance is 0.\n            # The Gaussian model implies B>0 unless the acceptance region is empty,\n            # in which case S is also 0.\n            z_value = 0.0\n        elif s_final <= 0:\n            # If S=0, no discovery is possible.\n            z_value = 0.0\n        else:\n            # Asimov formula: sqrt(2 * ((S+B)ln(1+S/B) - S))\n            term1 = (s_final + b_final) * np.log(1.0 + s_final / b_final)\n            term2 = s_final\n            q0_a = 2.0 * (term1 - term2)\n            # Handle potential negative results due to float precision\n            z_value = np.sqrt(max(0.0, q0_a))\n            \n        results.append(f\"{z_value:.6f}\")\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3505051"}, {"introduction": "Beyond predictive accuracy, a trustworthy model must also communicate its own uncertainty. This exercise delves into a modern interpretation of dropout, not merely as a regularization technique, but as a form of approximate Bayesian inference [@problem_id:3505062]. By deriving the predictive mean and variance from first principles, you will uncover how dropout-induced stochasticity at test time can provide a computationally efficient estimate of model (epistemic) uncertainty, offering a deeper insight into the network's confidence in its predictions.", "problem": "In a binary classification task in computational High-Energy Physics (HEP), an Artificial Neural Network (ANN) is trained to discriminate boosted top-quark jets from light Quantum Chromodynamics (QCD) jets using a feature vector $x \\in \\mathbb{R}^{d}$ encoding substructure observables. Consider a single-hidden-layer ANN with $m$ hidden units and a linear output representing the logit. For a particular event $x_{\\star}$, the hidden pre-activations are $a_{j}(x_{\\star}) = w_{j}^{\\top} x_{\\star} + b_{j}$, where $w_{j} \\in \\mathbb{R}^{d}$ and $b_{j} \\in \\mathbb{R}$ for $j \\in \\{1,\\dots,m\\}$. The hidden nonlinearity is a Rectified Linear Unit (ReLU), and for this specific $x_{\\star}$ you may assume $a_{j}(x_{\\star}) > 0$ for all $j$, so the hidden activations equal $h_{j}(x_{\\star}) = a_{j}(x_{\\star})$. The output logit is computed as\n$$\nf(x_{\\star},z) = c + \\sum_{j=1}^{m} v_{j} \\,\\tilde{h}_{j}(x_{\\star},z_{j}),\n$$\nwhere $v_{j} \\in \\mathbb{R}$ and $c \\in \\mathbb{R}$ are the output weights and bias, respectively, and $\\tilde{h}_{j}$ are dropout-masked activations.\n\nInterpret dropout as approximate Bayesian inference by treating the dropout mask as a variational posterior over multiplicative latent variables. Use test-time inverted dropout with independent Bernoulli masks $z_{j} \\sim \\mathrm{Bernoulli}(p)$ for $j \\in \\{1,\\dots,m\\}$, where $p \\in (0,1]$ is the retention probability, and define\n$$\n\\tilde{h}_{j}(x_{\\star},z_{j}) = \\frac{z_{j}}{p}\\,h_{j}(x_{\\star}).\n$$\nAssume the only source of uncertainty is the dropout-induced epistemic uncertainty (ignore any aleatoric noise). Starting from the fundamental definitions of expectation and variance for sums of independent random variables, derive the predictive mean $\\mathbb{E}[f(x_{\\star},z)]$ and predictive variance $\\mathrm{Var}[f(x_{\\star},z)]$ conditioned on $x_{\\star}$ under this Bernoulli variational distribution.\n\nProvide your final answer as a single closed-form analytic expression for the predictive variance $\\mathrm{Var}[f(x_{\\star},z)]$ in terms of $p$, $v_{j}$, and $h_{j}(x_{\\star})$ for $j=1,\\dots,m$. Do not include any units. If you obtain an exact symbolic expression, do not approximate it. The final boxed answer must be a single expression.", "solution": "The output logit for a given input $x_{\\star}$ is defined as a function of the dropout mask $z = (z_1, z_2, \\dots, z_m)$:\n$$\nf(x_{\\star},z) = c + \\sum_{j=1}^{m} v_{j} \\,\\tilde{h}_{j}(x_{\\star},z_{j})\n$$\nwhere $c$ and $v_{j}$ are constants. The dropout-masked activation $\\tilde{h}_{j}$ is given by:\n$$\n\\tilde{h}_{j}(x_{\\star},z_{j}) = \\frac{z_{j}}{p}\\,h_{j}(x_{\\star})\n$$\nThe variables $z_{j}$ are independent and identically distributed random variables drawn from a Bernoulli distribution, $z_{j} \\sim \\mathrm{Bernoulli}(p)$, where $p \\in (0,1]$ is the retention probability. For a given input $x_{\\star}$, the hidden activations $h_{j}(x_{\\star})$ are fixed values. Substituting the expression for $\\tilde{h}_{j}$ into the definition of $f(x_{\\star},z)$, we get:\n$$\nf(x_{\\star},z) = c + \\sum_{j=1}^{m} v_{j} \\left(\\frac{z_{j}}{p}\\,h_{j}(x_{\\star})\\right)\n$$\nThis can be rearranged to isolate the random variables $z_{j}$:\n$$\nf(x_{\\star},z) = c + \\sum_{j=1}^{m} \\left(\\frac{v_{j} h_{j}(x_{\\star})}{p}\\right) z_{j}\n$$\nThe expectation and variance are taken with respect to the distribution of the random vector $z$.\n\nFirst, we derive the predictive mean, $\\mathbb{E}[f(x_{\\star},z)]$. We use the linearity of the expectation operator:\n$$\n\\mathbb{E}[f(x_{\\star},z)] = \\mathbb{E}\\left[c + \\sum_{j=1}^{m} \\frac{v_{j} h_{j}(x_{\\star})}{p} z_{j}\\right]\n$$\n$$\n\\mathbb{E}[f(x_{\\star},z)] = \\mathbb{E}[c] + \\mathbb{E}\\left[\\sum_{j=1}^{m} \\frac{v_{j} h_{j}(x_{\\star})}{p} z_{j}\\right]\n$$\nThe expectation of the constant $c$ is $c$. For the sum, we can move the expectation inside the summation:\n$$\n\\mathbb{E}[f(x_{\\star},z)] = c + \\sum_{j=1}^{m} \\mathbb{E}\\left[\\frac{v_{j} h_{j}(x_{\\star})}{p} z_{j}\\right]\n$$\nThe terms $\\frac{v_{j} h_{j}(x_{\\star})}{p}$ are constants with respect to the expectation over $z_{j}$, so they can be factored out:\n$$\n\\mathbb{E}[f(x_{\\star},z)] = c + \\sum_{j=1}^{m} \\frac{v_{j} h_{j}(x_{\\star})}{p} \\mathbb{E}[z_{j}]\n$$\nFor a Bernoulli random variable $z_{j} \\sim \\mathrm{Bernoulli}(p)$, the expectation is $\\mathbb{E}[z_{j}] = p$. Substituting this into the equation gives:\n$$\n\\mathbb{E}[f(x_{\\star},z)] = c + \\sum_{j=1}^{m} \\frac{v_{j} h_{j}(x_{\\star})}{p} \\cdot p = c + \\sum_{j=1}^{m} v_{j} h_{j}(x_{\\star})\n$$\nThis result is the standard forward pass of the network without dropout, which confirms the property of inverted dropout scaling.\n\nNext, we derive the predictive variance, $\\mathrm{Var}[f(x_{\\star},z)]$. The variance of a random variable shifted by a constant is the variance of the original random variable, i.e., $\\mathrm{Var}(X+k) = \\mathrm{Var}(X)$. Thus, the constant bias term $c$ does not contribute to the variance:\n$$\n\\mathrm{Var}[f(x_{\\star},z)] = \\mathrm{Var}\\left[c + \\sum_{j=1}^{m} \\frac{v_{j} h_{j}(x_{\\star})}{p} z_{j}\\right] = \\mathrm{Var}\\left[\\sum_{j=1}^{m} \\frac{v_{j} h_{j}(x_{\\star})}{p} z_{j}\\right]\n$$\nThe problem states that the dropout masks $z_{j}$ are independent for $j \\in \\{1, \\dots, m\\}$. For a sum of independent random variables, the variance of the sum is the sum of the variances. Specifically, for independent $X_j$ and constants $a_j$, $\\mathrm{Var}(\\sum_j a_j X_j) = \\sum_j \\mathrm{Var}(a_j X_j) = \\sum_j a_j^2 \\mathrm{Var}(X_j)$. In our case, the random variables are $z_{j}$ and the constant coefficients are $\\frac{v_{j} h_{j}(x_{\\star})}{p}$. Therefore:\n$$\n\\mathrm{Var}[f(x_{\\star},z)] = \\sum_{j=1}^{m} \\mathrm{Var}\\left[\\frac{v_{j} h_{j}(x_{\\star})}{p} z_{j}\\right]\n$$\nUsing the property $\\mathrm{Var}(kX) = k^2\\mathrm{Var}(X)$, where $k$ is a constant, we have:\n$$\n\\mathrm{Var}[f(x_{\\star},z)] = \\sum_{j=1}^{m} \\left(\\frac{v_{j} h_{j}(x_{\\star})}{p}\\right)^2 \\mathrm{Var}[z_{j}]\n$$\nFor a Bernoulli random variable $z_{j} \\sim \\mathrm{Bernoulli}(p)$, the variance is $\\mathrm{Var}[z_{j}] = p(1-p)$. Substituting this into the expression for the variance:\n$$\n\\mathrm{Var}[f(x_{\\star},z)] = \\sum_{j=1}^{m} \\frac{v_{j}^{2} h_{j}(x_{\\star})^{2}}{p^2} \\cdot p(1-p)\n$$\nSimplifying the expression by canceling one factor of $p$ yields the final result for the predictive variance:\n$$\n\\mathrm{Var}[f(x_{\\star},z)] = \\sum_{j=1}^{m} v_{j}^{2} h_{j}(x_{\\star})^{2} \\frac{1-p}{p}\n$$\nThis can be written by factoring out the term dependent on $p$:\n$$\n\\mathrm{Var}[f(x_{\\star},z)] = \\frac{1-p}{p} \\sum_{j=1}^{m} v_{j}^{2} h_{j}(x_{\\star})^{2}\n$$\nThis is the closed-form analytic expression for the predictive variance as requested by the problem statement.", "answer": "$$\n\\boxed{\\frac{1-p}{p} \\sum_{j=1}^{m} v_{j}^{2} h_{j}(x_{\\star})^{2}}\n$$", "id": "3505062"}, {"introduction": "To build a complete picture of predictive uncertainty, we must distinguish what the model does not know (epistemic uncertainty) from the inherent randomness of the data itself (aleatoric uncertainty). This practice guides you through the derivation and implementation of the fundamental decomposition of predictive variance using a model ensemble [@problem_id:3505082]. You will not only prove this key identity but also apply it to compute essential metrics of uncertainty and calibration, learning how to critically assess the reliability and trustworthiness of a classification model.", "problem": "You are given a binary classification setting common in computational high-energy physics where artificial neural networks (ANNs) output predictive probabilities for event classes (e.g., distinguishing signal from background). From the perspective of Bayesian modeling, an ensemble of independently trained ANNs can be interpreted as a finite sample from a posterior over model parameters. For a given input feature vector $x$, denote by $p_m(x)$ the predictive probability of class $y=1$ from ensemble member $m$, and by $Y \\in \\{0,1\\}$ the random label. Assume the following fundamentals as the starting point: the conditional distribution $Y \\mid x, w$ is Bernoulli with mean $p_w(x)$; the predictive (Bayesian) expectation is an average over the model posterior; and the law of total variance holds for nested sources of randomness.\n\nStarting only from these fundamentals and the definition of variance, derive a decomposition of the predictive variance of $Y$ at fixed $x$ into two nonnegative components associated with model uncertainty and data uncertainty. Express each component in terms of ensemble-averaged quantities that are computable from $\\{p_m(x)\\}$. Briefly justify each step by invoking the fundamental principles above.\n\nThen, using the decomposition derived, implement a program that estimates the following for specified test suites:\n- The maximum absolute discrepancy per test suite between the left-hand side predictive variance at each sample and the right-hand side sum of the two components you derived, both estimated using the given finite ensemble. Aggregate this discrepancy across samples in the suite using the maximum absolute difference.\n- The Expected Calibration Error (ECE) computed by binning the ensemble-mean predictive probabilities into $M$ equal-width bins on $[0,1]$ and taking the weighted average of the absolute difference between the empirical frequency and the average predicted probability in each nonempty bin.\n- The Brier reliability term from the classical Brier score decomposition using the same binning as ECE.\n- The average Negative Log-Likelihood (NLL) per sample using the ensemble-mean predictive probability with a numerically safe clipping at a small $\\epsilon$ to avoid taking the logarithm of $0$.\n\nAll quantities must be computed with the following precise definitions:\n- For each sample $i$, with ensemble member probabilities $\\{p_{m,i}\\}_{m=1}^{M_e}$, define the ensemble mean $\\bar{p}_i = \\frac{1}{M_e}\\sum_{m=1}^{M_e} p_{m,i}$. Define the per-sample predictive variance estimate as $\\bar{p}_i (1-\\bar{p}_i)$. Define the data (aleatoric) part as the ensemble mean of Bernoulli variances $\\frac{1}{M_e}\\sum_{m=1}^{M_e} p_{m,i}(1-p_{m,i})$. Define the model (epistemic) part as the variance across models of $\\{p_{m,i}\\}$ using the population variance convention (denominator $M_e$).\n- For ECE and Brier reliability at the suite level, let $M$ be the number of bins. For each sample, assign $\\bar{p}_i$ to one of $M$ bins partitioning $[0,1]$ into equal-width intervals. For each nonempty bin $b$, let $n_b$ be the count, $\\hat{p}_b$ be the mean of $\\bar{p}_i$ in the bin, and $\\hat{f}_b$ be the empirical frequency of $y_i=1$ in the bin. Let $N$ be the total number of samples in the suite. Define\n  - $\\mathrm{ECE} = \\sum_{b} \\frac{n_b}{N} \\left| \\hat{p}_b - \\hat{f}_b \\right|$,\n  - $\\mathrm{Reliability} = \\sum_{b} \\frac{n_b}{N} \\left( \\hat{p}_b - \\hat{f}_b \\right)^2$.\n- For NLL, with labels $y_i \\in \\{0,1\\}$ and clipped probabilities $\\tilde{p}_i = \\min(\\max(\\bar{p}_i,\\epsilon),1-\\epsilon)$ with $\\epsilon = 10^{-12}$, define the average NLL as $\\frac{1}{N}\\sum_{i=1}^N \\left( - y_i \\log \\tilde{p}_i - (1-y_i) \\log (1-\\tilde{p}_i) \\right)$.\n- All logarithms are natural logarithms.\n\nYour program must implement the computations above for the following test suites. In each suite, all numbers are dimensionless probabilities. Each suite specifies an ensemble of predictive probabilities of shape $M_e \\times N$ and a vector of $N$ binary labels:\n- Suite A (well-calibrated, moderately confident, alternating labels):\n  - Ensemble size $M_e = 4$, number of samples $N = 8$, number of bins $M = 4$.\n  - Labels $y = [\\,1,0,1,0,1,0,1,0\\,]$.\n  - Ensemble predictions (each row is one model, entries are $p_{m,i}$):\n    - Model $1$: $[\\,0.75, 0.15, 0.70, 0.30, 0.85, 0.25, 0.65, 0.20\\,]$,\n    - Model $2$: $[\\,0.80, 0.10, 0.72, 0.28, 0.88, 0.22, 0.60, 0.18\\,]$,\n    - Model $3$: $[\\,0.78, 0.12, 0.68, 0.32, 0.86, 0.24, 0.62, 0.16\\,]$,\n    - Model $4$: $[\\,0.77, 0.14, 0.69, 0.31, 0.84, 0.26, 0.63, 0.19\\,]$.\n- Suite B (overconfident with boundary probabilities, mixed labels):\n  - Ensemble size $M_e = 3$, number of samples $N = 8$, number of bins $M = 4$.\n  - Labels $y = [\\,1,1,0,0,1,0,1,0\\,]$.\n  - Ensemble predictions:\n    - Model $1$: $[\\,1.00, 0.95, 0.00, 0.02, 0.99, 0.00, 0.90, 0.10\\,]$,\n    - Model $2$: $[\\,0.98, 0.97, 0.02, 0.01, 1.00, 0.00, 0.92, 0.08\\,]$,\n    - Model $3$: $[\\,0.97, 1.00, 0.04, 0.00, 0.98, 0.02, 0.91, 0.09\\,]$.\n- Suite C (underconfident, near-ambiguous probabilities):\n  - Ensemble size $M_e = 3$, number of samples $N = 8$, number of bins $M = 4$.\n  - Labels $y = [\\,1,0,1,0,1,0,1,0\\,]$.\n  - Ensemble predictions:\n    - Model $1$: $[\\,0.55, 0.45, 0.60, 0.40, 0.52, 0.48, 0.58, 0.42\\,]$,\n    - Model $2$: $[\\,0.50, 0.50, 0.55, 0.45, 0.53, 0.47, 0.57, 0.43\\,]$,\n    - Model $3$: $[\\,0.52, 0.48, 0.58, 0.42, 0.51, 0.49, 0.56, 0.44\\,]$.\n\nImplementation requirements:\n- Use the ensemble mean $\\bar{p}_i$ for calibration metrics and NLL.\n- Use exactly $M=4$ equal-width bins on $[0,1]$ for all suites. Assign a sample with $\\bar{p}_i=1$ to the last bin and with $\\bar{p}_i=0$ to the first bin.\n- For each suite, compute and record in order: the maximum absolute discrepancy for the variance decomposition across samples, the ECE, the Brier reliability, and the average NLL.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The final list must concatenate the four results for Suite A, followed by the four for Suite B, followed by the four for Suite C. Express each value rounded to $6$ decimal places.\n\nNo external input is provided. All output values are dimensionless. The expected output format is thus\n$[r_{A,1}, r_{A,2}, r_{A,3}, r_{A,4}, r_{B,1}, r_{B,2}, r_{B,3}, r_{B,4}, r_{C,1}, r_{C,2}, r_{C,3}, r_{C,4}]$.", "solution": "First, we will derive the decomposition of the predictive variance. Let $Y \\in \\{0,1\\}$ be the random variable for the true class label, and let $x$ be a fixed input feature vector. Our model's parameters (weights) are denoted by $w$, which are considered random variables following a posterior distribution $p(w|\\mathcal{D})$ given the training data $\\mathcal{D}$. An ensemble of $M_e$ models provides a set of parameter vectors $\\{w_m\\}_{m=1}^{M_e}$ sampled from this posterior.\n\nThe problem states that for a fixed set of parameters $w$, the conditional distribution of the label is a Bernoulli distribution, $Y \\mid x, w \\sim \\mathrm{Bernoulli}(p_w(x))$, where $p_w(x)$ is the model's output probability for class $Y=1$.\n\nWe aim to decompose the total predictive variance of $Y$ given $x$ and the training data $\\mathcal{D}$, which we denote as $\\mathrm{Var}(Y|x, \\mathcal{D})$. This variance arises from two sources of randomness: the inherent stochasticity of the data, and our uncertainty about the model parameters $w$. We can separate these using the law of total variance, which states that for any two random variables $A$ and $B$, $\\mathrm{Var}(A) = E_B[\\mathrm{Var}_A(A|B)] + \\mathrm{Var}_B(E_A[A|B])$.\n\nApplying this to our problem, we treat $Y$ as the primary random variable and the model parameters $w$ as the conditioning variable. All expectations $E_w[\\cdot]$ and variances $\\mathrm{Var}_w(\\cdot)$ are taken over the posterior distribution $p(w|\\mathcal{D})$.\n\n$$\n\\mathrm{Var}(Y|x, \\mathcal{D}) = E_{w|\\mathcal{D}}[\\mathrm{Var}_{Y}(Y | x, w)] + \\mathrm{Var}_{w|\\mathcal{D}}(E_{Y}[Y | x, w])\n$$\n\nLet's analyze the two terms on the right-hand side.\n\n1.  The inner expectation, $E_{Y}[Y | x, w]$, is the expected value of a Bernoulli random variable with parameter $p_w(x)$. By definition, this is simply the parameter itself:\n    $$\n    E_{Y}[Y | x, w] = p_w(x)\n    $$\n\n2.  The inner variance, $\\mathrm{Var}_{Y}(Y | x, w)$, is the variance of that same Bernoulli random variable. For a Bernoulli distribution with parameter $p$, the variance is $p(1-p)$. Thus:\n    $$\n    \\mathrm{Var}_{Y}(Y | x, w) = p_w(x)(1 - p_w(x))\n    $$\n\nSubstituting these back into the law of total variance formula, we get:\n$$\n\\mathrm{Var}(Y|x, \\mathcal{D}) = E_{w|\\mathcal{D}}[p_w(x)(1 - p_w(x))] + \\mathrm{Var}_{w|\\mathcal{D}}(p_w(x))\n$$\n\nThis equation provides the desired decomposition. The two terms on the right are:\n-   **Data (Aleatoric) Uncertainty**: $E_{w|\\mathcal{D}}[p_w(x)(1 - p_w(x))]$. This term is the expected value of the Bernoulli variance, averaged over the posterior distribution of the model parameters. It represents the inherent, irreducible uncertainty in the data, even if we knew the model parameters perfectly. It is called aleatoric uncertainty.\n-   **Model (Epistemic) Uncertainty**: $\\mathrm{Var}_{w|\\mathcal{D}}(p_w(x))$. This term is the variance of the model's predictive probability itself, taken over the posterior distribution of the parameters. It quantifies how much the model's predictions vary due to our uncertainty about the parameters $w$. This uncertainty is reducible with more data and is called epistemic uncertainty.\n\nNow, we must relate the left-hand side of the equation, $\\mathrm{Var}(Y|x, \\mathcal{D})$, to an interpretable quantity. The left-hand side is the variance of the Bayesian predictive distribution $p(Y|x, \\mathcal{D})$. The mean of this predictive distribution is the Bayesian model average (BMA) prediction, found by marginalizing over the parameters $w$:\n$$\n\\bar{p}(x) = E[Y|x, \\mathcal{D}] = E_{w|\\mathcal{D}}[E_Y[Y|x,w]] = E_{w|\\mathcal{D}}[p_w(x)]\n$$\nThe full predictive distribution $p(Y|x, \\mathcal{D})$ is also a Bernoulli distribution with this mean, $\\bar{p}(x)$. Therefore, its variance is:\n$$\n\\mathrm{Var}(Y|x, \\mathcal{D}) = \\bar{p}(x)(1 - \\bar{p}(x))\n$$\n\nWith this, we can state the full identity:\n$$\n\\underbrace{\\bar{p}(x)(1 - \\bar{p}(x))}_{\\text{Total Predictive Variance}} = \\underbrace{E_{w|\\mathcal{D}}[p_w(x)(1 - p_w(x))]}_{\\text{Aleatoric Uncertainty}} + \\underbrace{\\mathrm{Var}_{w|\\mathcal{D}}(p_w(x))}_{\\text{Epistemic Uncertainty}}\n$$\nThis identity can be confirmed by expanding the right-hand side, since $\\mathrm{Var}(A) = E[A^2] - (E[A])^2$:\n$$\nE_w[p_w(x) - p_w(x)^2] + (E_w[p_w(x)^2] - (E_w[p_w(x)])^2)\n$$\n$$\n= E_w[p_w(x)] - E_w[p_w(x)^2] + E_w[p_w(x)^2] - (E_w[p_w(x)])^2\n$$\n$$\n= E_w[p_w(x)] - (E_w[p_w(x)])^2 = \\bar{p}(x) - (\\bar{p}(x))^2 = \\bar{p}(x)(1 - \\bar{p}(x))\n$$\nThe identity holds. The problem asks to compute these quantities from a finite ensemble $\\{p_m(x)\\}_{m=1}^{M_e}$. We approximate the true posterior expectations and variances with sample estimators from the ensemble. For a given sample $i$:\n-   The ensemble mean is $\\bar{p}_i = \\frac{1}{M_e}\\sum_{m=1}^{M_e} p_{m,i}$. This estimates $\\bar{p}(x_i)$.\n-   The total predictive variance (LHS) is estimated as $\\bar{p}_i(1-\\bar{p}_i)$.\n-   The aleatoric uncertainty is estimated as the mean of the individual Bernoulli variances: $\\frac{1}{M_e}\\sum_{m=1}^{M_e} p_{m,i}(1-p_{m,i})$.\n-   The epistemic uncertainty is estimated as the variance of the predictions across the ensemble: $\\frac{1}{M_e}\\sum_{m=1}^{M_e} (p_{m,i} - \\bar{p}_i)^2$, which is the population variance of $\\{p_{m,i}\\}$.\n\nThis completes the derivation and links the theoretical quantities to the estimators defined in the problem. The subsequent parts of the problem require implementing these formulas, along with standard metrics like ECE, Brier reliability, and NLL, which are also defined precisely. The implementation will follow these definitions directly.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to define test suites, compute metrics, and print results.\n    \"\"\"\n\n    test_suites = [\n        {\n            \"name\": \"Suite A\",\n            \"Me\": 4, \"N\": 8, \"M_bins\": 4,\n            \"labels\": np.array([1, 0, 1, 0, 1, 0, 1, 0]),\n            \"preds\": np.array([\n                [0.75, 0.15, 0.70, 0.30, 0.85, 0.25, 0.65, 0.20],\n                [0.80, 0.10, 0.72, 0.28, 0.88, 0.22, 0.60, 0.18],\n                [0.78, 0.12, 0.68, 0.32, 0.86, 0.24, 0.62, 0.16],\n                [0.77, 0.14, 0.69, 0.31, 0.84, 0.26, 0.63, 0.19],\n            ])\n        },\n        {\n            \"name\": \"Suite B\",\n            \"Me\": 3, \"N\": 8, \"M_bins\": 4,\n            \"labels\": np.array([1, 1, 0, 0, 1, 0, 1, 0]),\n            \"preds\": np.array([\n                [1.00, 0.95, 0.00, 0.02, 0.99, 0.00, 0.90, 0.10],\n                [0.98, 0.97, 0.02, 0.01, 1.00, 0.00, 0.92, 0.08],\n                [0.97, 1.00, 0.04, 0.00, 0.98, 0.02, 0.91, 0.09],\n            ])\n        },\n        {\n            \"name\": \"Suite C\",\n            \"Me\": 3, \"N\": 8, \"M_bins\": 4,\n            \"labels\": np.array([1, 0, 1, 0, 1, 0, 1, 0]),\n            \"preds\": np.array([\n                [0.55, 0.45, 0.60, 0.40, 0.52, 0.48, 0.58, 0.42],\n                [0.50, 0.50, 0.55, 0.45, 0.53, 0.47, 0.57, 0.43],\n                [0.52, 0.48, 0.58, 0.42, 0.51, 0.49, 0.56, 0.44],\n            ])\n        }\n    ]\n\n    all_results = []\n    for suite in test_suites:\n        results = calculate_metrics(\n            suite[\"preds\"], suite[\"labels\"], suite[\"M_bins\"]\n        )\n        all_results.extend(results)\n\n    # Format the final output string\n    print(f\"[{','.join(f'{r:.6f}' for r in all_results)}]\")\n\ndef calculate_metrics(preds, labels, M_bins, epsilon=1e-12):\n    \"\"\"\n    Computes the four required metrics for a given test suite.\n\n    Args:\n        preds (np.ndarray): Ensemble predictions of shape (Me, N).\n        labels (np.ndarray): True labels of shape (N,).\n        M_bins (int): Number of bins for ECE and Reliability.\n        epsilon (float): Clipping value for NLL calculation.\n\n    Returns:\n        tuple: A tuple containing the four computed metrics.\n    \"\"\"\n    N = labels.shape[0]\n\n    # Calculate ensemble mean probabilities\n    p_bar = np.mean(preds, axis=0)\n\n    # 1. Maximum absolute discrepancy for variance decomposition\n    lhs_variance = p_bar * (1 - p_bar)\n    aleatoric_variance = np.mean(preds * (1 - preds), axis=0)\n    epistemic_variance = np.var(preds, axis=0, ddof=0) # ddof=0 for population variance\n    rhs_variance = aleatoric_variance + epistemic_variance\n    max_discrepancy = np.max(np.abs(lhs_variance - rhs_variance))\n    \n    # 2. ECE and 3. Brier Reliability\n    ece = 0.0\n    reliability = 0.0\n    \n    # Assign samples to bins\n    # For p_bar=1.0, floor(1.0*M_bins) = M_bins. Clip to M_bins-1.\n    bin_indices = np.floor(p_bar * M_bins)\n    bin_indices = np.minimum(M_bins - 1, bin_indices).astype(int)\n\n    for b in range(M_bins):\n        in_bin_mask = (bin_indices == b)\n        n_b = np.sum(in_bin_mask)\n        \n        if n_b > 0:\n            p_hat_b = np.mean(p_bar[in_bin_mask])\n            f_hat_b = np.mean(labels[in_bin_mask])\n            \n            ece += (n_b / N) * np.abs(p_hat_b - f_hat_b)\n            reliability += (n_b / N) * (p_hat_b - f_hat_b)**2\n\n    # 4. Average Negative Log-Likelihood (NLL)\n    p_tilde = np.clip(p_bar, epsilon, 1 - epsilon)\n    nll_terms = -labels * np.log(p_tilde) - (1 - labels) * np.log(1 - p_tilde)\n    avg_nll = np.mean(nll_terms)\n\n    return (max_discrepancy, ece, reliability, avg_nll)\n\nsolve()\n```", "id": "3505082"}]}