{"hands_on_practices": [{"introduction": "Before we can correct for detector effects, we must first build a precise mathematical model of them. This exercise guides you through the fundamental process of constructing a detector response matrix from raw Monte Carlo simulation counts ([@problem_id:3540787]). By calculating key quantities like the acceptance matrix, bin-wise efficiencies, and the acceptance-corrected response, you will gain a practical understanding of how the \"forward problem\"—the mapping from true to reconstructed events—is defined.", "problem": "A detector is simulated with Monte Carlo (MC) for a process whose particle-level (truth) phase space is partitioned into three bins indexed by $i \\in \\{1,2,3\\}$, and whose reconstructed (reco) phase space is partitioned into four bins indexed by $j \\in \\{1,2,3,4\\}$. Let $N_{i}^{\\text{truth}}$ denote the total number of MC truth events generated in truth bin $i$, and let $N_{ji}$ denote the number of those truth-bin-$i$ events that are reconstructed in reco bin $j$ by the detector simulation. The MC yields are:\n- Truth counts: $N_{1}^{\\text{truth}}=1000$, $N_{2}^{\\text{truth}}=1200$, $N_{3}^{\\text{truth}}=800$.\n- Migration counts (reco given truth): for $i=1$, $(N_{1,1},N_{2,1},N_{3,1},N_{4,1})=(320,280,100,50)$; for $i=2$, $(N_{1,2},N_{2,2},N_{3,2},N_{4,2})=(150,420,360,90)$; for $i=3$, $(N_{1,3},N_{2,3},N_{3,3},N_{4,3})=(40,160,280,120)$.\n\nStarting from the frequentist definitions of probability and conditional probability, and treating the MC as a large-sample estimator of those probabilities, derive and compute:\n1. The acceptance (also called response with inefficiency) matrix $A_{ji}$, defined conceptually as the conditional probability to be reconstructed in reco bin $j$ given the event was in truth bin $i$, estimated from the MC counts.\n2. The efficiency $\\epsilon_{i}$ for each truth bin $i$, defined conceptually as the total probability that an event in truth bin $i$ is reconstructed into any of the four reco bins.\n3. The acceptance-corrected response $\\tilde{R}_{ji}$, defined conceptually as the conditional probability to be in reco bin $j$ given the event was in truth bin $i$ and that it was reconstructed (i.e., conditioned on acceptance), obtained by normalizing each truth column of $A_{ji}$ by its corresponding $\\epsilon_{i}$.\n\nAssume all estimates are exact ratios of counts without any further regularization or smoothing. Express all probabilities as exact rational numbers, without rounding. Report your final answer as a single row vector using the $\\mathrm{pmatrix}$ environment, listing entries in the following order:\n- First the matrix $A_{ji}$ column-wise by truth bin $i=1,2,3$, and within each column by $j=1,2,3,4$;\n- Then the efficiencies $(\\epsilon_{1},\\epsilon_{2},\\epsilon_{3})$;\n- Then the acceptance-corrected response matrix $\\tilde{R}_{ji}$ in the same column-wise order.", "solution": "The problem statement has been validated and is deemed to be self-contained, scientifically grounded, and well-posed. It presents a standard computational task in high-energy physics related to detector simulation and unfolding. All necessary data and definitions are provided, and there are no internal contradictions. We may therefore proceed with the solution.\n\nThe problem asks for the computation of three related quantities describing detector effects: the acceptance matrix $A_{ji}$, the truth-bin efficiencies $\\epsilon_i$, and the acceptance-corrected response matrix $\\tilde{R}_{ji}$. We will derive and compute each in turn, based on the frequentist interpretation of probability applied to the provided Monte Carlo (MC) event counts.\n\nLet $T_i$ be the event that a particle is generated in the truth bin $i$, and let $R_j$ be the event that it is reconstructed in the reco bin $j$. The given counts are:\n- Total number of events generated in truth bin $i$: $N(\\text{T}_i) = N_{i}^{\\text{truth}}$.\n- Number of events generated in truth bin $i$ and reconstructed in reco bin $j$: $N(\\text{R}_j \\cap \\text{T}_i) = N_{ji}$.\n\nThe provided numerical values are:\n- $N_{1}^{\\text{truth}} = 1000$, $N_{2}^{\\text{truth}} = 1200$, $N_{3}^{\\text{truth}} = 800$.\n- For $i=1$: $(N_{1,1}, N_{2,1}, N_{3,1}, N_{4,1}) = (320, 280, 100, 50)$.\n- For $i=2$: $(N_{1,2}, N_{2,2}, N_{3,2}, N_{4,2}) = (150, 420, 360, 90)$.\n- For $i=3$: $(N_{1,3}, N_{2,3}, N_{3,3}, N_{4,3}) = (40, 160, 280, 120)$.\n\nAll probabilities are estimated as ratios of counts.\n\n**1. The Acceptance Matrix $A_{ji}$**\n\nThe acceptance matrix $A_{ji}$ is defined as the conditional probability of an event being reconstructed in reco bin $j$, given that it originated in truth bin $i$. This is denoted $P(\\text{R}_j | \\text{T}_i)$. Using the frequentist definition of conditional probability, we estimate this as:\n$$\nA_{ji} = P(\\text{R}_j | \\text{T}_i) = \\frac{N(\\text{R}_j \\cap \\text{T}_i)}{N(\\text{T}_i)} = \\frac{N_{ji}}{N_{i}^{\\text{truth}}}\n$$\nWe compute the $4 \\times 3$ matrix elements column by column for each truth bin $i \\in \\{1,2,3\\}$.\n\nFor truth bin $i=1$ ($N_{1}^{\\text{truth}}=1000$):\n$A_{11} = \\frac{N_{11}}{N_{1}^{\\text{truth}}} = \\frac{320}{1000} = \\frac{8}{25}$\n$A_{21} = \\frac{N_{21}}{N_{1}^{\\text{truth}}} = \\frac{280}{1000} = \\frac{7}{25}$\n$A_{31} = \\frac{N_{31}}{N_{1}^{\\text{truth}}} = \\frac{100}{1000} = \\frac{1}{10}$\n$A_{41} = \\frac{N_{41}}{N_{1}^{\\text{truth}}} = \\frac{50}{1000} = \\frac{1}{20}$\n\nFor truth bin $i=2$ ($N_{2}^{\\text{truth}}=1200$):\n$A_{12} = \\frac{N_{12}}{N_{2}^{\\text{truth}}} = \\frac{150}{1200} = \\frac{1}{8}$\n$A_{22} = \\frac{N_{22}}{N_{2}^{\\text{truth}}} = \\frac{420}{1200} = \\frac{7}{20}$\n$A_{32} = \\frac{N_{32}}{N_{2}^{\\text{truth}}} = \\frac{360}{1200} = \\frac{3}{10}$\n$A_{42} = \\frac{N_{42}}{N_{2}^{\\text{truth}}} = \\frac{90}{1200} = \\frac{3}{40}$\n\nFor truth bin $i=3$ ($N_{3}^{\\text{truth}}=800$):\n$A_{13} = \\frac{N_{13}}{N_{3}^{\\text{truth}}} = \\frac{40}{800} = \\frac{1}{20}$\n$A_{23} = \\frac{N_{23}}{N_{3}^{\\text{truth}}} = \\frac{160}{800} = \\frac{1}{5}$\n$A_{33} = \\frac{N_{33}}{N_{3}^{\\text{truth}}} = \\frac{280}{800} = \\frac{7}{20}$\n$A_{43} = \\frac{N_{43}}{N_{3}^{\\text{truth}}} = \\frac{120}{800} = \\frac{3}{20}$\n\n**2. The Efficiency $\\epsilon_i$**\n\nThe efficiency $\\epsilon_i$ for a truth bin $i$ is the total probability that an event from that bin is reconstructed in *any* of the reco bins. This is the sum of the conditional probabilities over all possible reconstruction outcomes $j$.\n$$\n\\epsilon_i = P(\\text{reconstructed} | \\text{T}_i) = \\sum_{j=1}^{4} P(\\text{R}_j | \\text{T}_i) = \\sum_{j=1}^{4} A_{ji}\n$$\nEquivalently, it is the total number of reconstructed events originating from truth bin $i$ divided by the total number of events generated in truth bin $i$.\n$$\n\\epsilon_i = \\frac{\\sum_{j=1}^{4} N_{ji}}{N_{i}^{\\text{truth}}}\n$$\nWe compute $\\epsilon_i$ for each truth bin $i \\in \\{1,2,3\\}$.\n\nFor truth bin $i=1$:\n$\\epsilon_1 = \\frac{320+280+100+50}{1000} = \\frac{750}{1000} = \\frac{3}{4}$\n\nFor truth bin $i=2$:\n$\\epsilon_2 = \\frac{150+420+360+90}{1200} = \\frac{1020}{1200} = \\frac{17}{20}$\n\nFor truth bin $i=3$:\n$\\epsilon_3 = \\frac{40+160+280+120}{800} = \\frac{600}{800} = \\frac{3}{4}$\n\n**3. The Acceptance-Corrected Response $\\tilde{R}_{ji}$**\n\nThe acceptance-corrected response $\\tilde{R}_{ji}$ is the conditional probability that an event is reconstructed in reco bin $j$, given that it originated in truth bin $i$ *and* that it was reconstructed. Let 'Reco'd' be the event that a particle is reconstructed in any of the bins. Then $\\tilde{R}_{ji} = P(\\text{R}_j | \\text{T}_i \\cap \\text{Reco'd})$.\nBy the definition of conditional probability:\n$$\n\\tilde{R}_{ji} = \\frac{P(\\text{R}_j \\cap (\\text{T}_i \\cap \\text{Reco'd}))}{P(\\text{T}_i \\cap \\text{Reco'd})}\n$$\nSince the event $\\text{R}_j$ implies the event 'Reco'd', the intersection $\\text{R}_j \\cap \\text{Reco'd}$ is simply $\\text{R}_j$. Thus, the numerator becomes $P(\\text{R}_j \\cap \\text{T}_i)$. The expression simplifies to:\n$$\n\\tilde{R}_{ji} = \\frac{P(\\text{R}_j \\cap \\text{T}_i)}{P(\\text{Reco'd} \\cap \\text{T}_i)}\n$$\nDividing the numerator and denominator by $P(\\text{T}_i)$ yields:\n$$\n\\tilde{R}_{ji} = \\frac{P(\\text{R}_j \\cap \\text{T}_i) / P(\\text{T}_i)}{P(\\text{Reco'd} \\cap \\text{T}_i) / P(\\text{T}_i)} = \\frac{P(\\text{R}_j | \\text{T}_i)}{P(\\text{Reco'd} | \\text{T}_i)} = \\frac{A_{ji}}{\\epsilon_i}\n$$\nThis confirms the prescription of normalizing each column of $A_{ji}$ by the corresponding efficiency $\\epsilon_i$. For each truth bin $i$, the column $\\tilde{R}_{ji}$ is a properly normalized probability distribution, i.e., $\\sum_{j=1}^{4} \\tilde{R}_{ji} = 1$.\n\nFor truth bin $i=1$ ($\\epsilon_1 = 3/4$):\n$\\tilde{R}_{11} = \\frac{A_{11}}{\\epsilon_1} = \\frac{8/25}{3/4} = \\frac{32}{75}$\n$\\tilde{R}_{21} = \\frac{A_{21}}{\\epsilon_1} = \\frac{7/25}{3/4} = \\frac{28}{75}$\n$\\tilde{R}_{31} = \\frac{A_{31}}{\\epsilon_1} = \\frac{1/10}{3/4} = \\frac{4}{30} = \\frac{2}{15}$\n$\\tilde{R}_{41} = \\frac{A_{41}}{\\epsilon_1} = \\frac{1/20}{3/4} = \\frac{4}{60} = \\frac{1}{15}$\n\nFor truth bin $i=2$ ($\\epsilon_2 = 17/20$):\n$\\tilde{R}_{12} = \\frac{A_{12}}{\\epsilon_2} = \\frac{1/8}{17/20} = \\frac{20}{136} = \\frac{5}{34}$\n$\\tilde{R}_{22} = \\frac{A_{22}}{\\epsilon_2} = \\frac{7/20}{17/20} = \\frac{7}{17}$\n$\\tilde{R}_{32} = \\frac{A_{32}}{\\epsilon_2} = \\frac{3/10}{17/20} = \\frac{60}{170} = \\frac{6}{17}$\n$\\tilde{R}_{42} = \\frac{A_{42}}{\\epsilon_2} = \\frac{3/40}{17/20} = \\frac{60}{680} = \\frac{3}{34}$\n\nFor truth bin $i=3$ ($\\epsilon_3 = 3/4$):\n$\\tilde{R}_{13} = \\frac{A_{13}}{\\epsilon_3} = \\frac{1/20}{3/4} = \\frac{4}{60} = \\frac{1}{15}$\n$\\tilde{R}_{23} = \\frac{A_{23}}{\\epsilon_3} = \\frac{1/5}{3/4} = \\frac{4}{15}$\n$\\tilde{R}_{33} = \\frac{A_{33}}{\\epsilon_3} = \\frac{7/20}{3/4} = \\frac{28}{60} = \\frac{7}{15}$\n$\\tilde{R}_{43} = \\frac{A_{43}}{\\epsilon_3} = \\frac{3/20}{3/4} = \\frac{12}{60} = \\frac{1}{5} = \\frac{3}{15}$\n\nCombining all results into a single row vector as specified completes the task.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{8}{25} & \\frac{7}{25} & \\frac{1}{10} & \\frac{1}{20} & \\frac{1}{8} & \\frac{7}{20} & \\frac{3}{10} & \\frac{3}{40} & \\frac{1}{20} & \\frac{1}{5} & \\frac{7}{20} & \\frac{3}{20} & \\frac{3}{4} & \\frac{17}{20} & \\frac{3}{4} & \\frac{32}{75} & \\frac{28}{75} & \\frac{2}{15} & \\frac{1}{15} & \\frac{5}{34} & \\frac{7}{17} & \\frac{6}{17} & \\frac{3}{34} & \\frac{1}{15} & \\frac{4}{15} & \\frac{7}{15} & \\frac{3}{15}\n\\end{pmatrix}\n}\n$$", "id": "3540787"}, {"introduction": "With a forward model in place, the core challenge of unfolding is solving the ill-posed inverse problem. This coding exercise ([@problem_id:3540846]) allows you to directly compare the performance of unregularized inversion with two powerful regularization techniques: Tikhonov ($\\ell_2$) and Total Variation ($\\ell_1$). By applying these methods to a spectrum with a sharp efficiency drop, you will see firsthand how different regularizers preserve or smooth important features, a critical consideration in real-world data analysis.", "problem": "You are given a one-dimensional unfolding problem that models detector smearing and a sharp efficiency drop at a threshold. The fundamental base is the linear forward model that connects the unknown true spectrum to the measured spectrum. Let $N$ denote the number of bins, let $\\mathbf{t} \\in \\mathbb{R}^{N}$ denote the unknown true bin contents, and let $\\mathbf{m} \\in \\mathbb{R}^{N}$ denote the measured bin contents. The detector response is modeled by a matrix $\\mathbf{A} \\in \\mathbb{R}^{N \\times N}$, such that the forward model is\n$$\n\\mathbf{m} = \\mathbf{A} \\mathbf{t}.\n$$\nThe response matrix incorporates a bin-wise efficiency $\\varepsilon_i$ and resolution smearing. The smearing is modeled by a Gaussian kernel in bin space with width $\\sigma$, and the efficiency applies multiplicatively per true bin. For each true bin index $i \\in \\{0,\\dots,N-1\\}$, the column $i$ of $\\mathbf{A}$ is given by\n$$\nA_{j i} = \\varepsilon_i \\, \\frac{\\exp\\left( -\\frac{(j-i)^2}{2 \\sigma^2} \\right)}{\\sum\\limits_{k=0}^{N-1} \\exp\\left( -\\frac{(k-i)^2}{2 \\sigma^2} \\right)}, \\quad j \\in \\{0,\\dots,N-1\\}.\n$$\nThe efficiency model contains a sharp drop at a threshold bin index $\\theta$. Specifically,\n$$\n\\varepsilon_i = \\begin{cases}\n1, & i < \\theta, \\\\\n\\varepsilon_{\\mathrm{drop}}, & i \\ge \\theta,\n\\end{cases}\n$$\nwhere $0 < \\varepsilon_{\\mathrm{drop}} < 1$.\n\nYour task is to construct the response matrix based on the above definitions, synthesize a measured spectrum from a known true spectrum, and perform unfolding by three methods: unregularized least squares, Tikhonov regularization with a first-difference quadratic penalty, and Total Variation (TV) regularization with a first-difference $\\ell_1$ penalty. The unfolded spectrum estimates are denoted $\\widehat{\\mathbf{t}}_{\\mathrm{LS}}$, $\\widehat{\\mathbf{t}}_{\\mathrm{Tik}}$, and $\\widehat{\\mathbf{t}}_{\\mathrm{TV}}$, respectively.\n\n1. Forward model and true spectrum:\n   - Use $N = 50$ bins.\n   - Use the true spectrum $\\mathbf{t}$ with components $t_i = 1$ for all $i \\in \\{0,\\dots,N-1\\}$.\n   - Construct $\\mathbf{A}$ using the Gaussian smearing width $\\sigma$ and the efficiency model with threshold $\\theta$ and drop $\\varepsilon_{\\mathrm{drop}}$ as specified in the test suite.\n\n2. Unfolding methods:\n   - Unregularized least squares: solve\n     $$\n     \\widehat{\\mathbf{t}}_{\\mathrm{LS}} = \\operatorname*{arg\\,min}_{\\mathbf{x} \\in \\mathbb{R}^N} \\frac{1}{2} \\lVert \\mathbf{A}\\mathbf{x} - \\mathbf{m} \\rVert_2^2.\n     $$\n   - Tikhonov regularization (first-difference quadratic): let $\\mathbf{D} \\in \\mathbb{R}^{(N-1)\\times N}$ be the first-difference operator defined by $(\\mathbf{D}\\mathbf{x})_i = x_{i+1} - x_i$. For a given regularization strength $\\alpha > 0$, solve\n     $$\n     \\widehat{\\mathbf{t}}_{\\mathrm{Tik}} = \\operatorname*{arg\\,min}_{\\mathbf{x} \\in \\mathbb{R}^N} \\frac{1}{2} \\lVert \\mathbf{A}\\mathbf{x} - \\mathbf{m} \\rVert_2^2 + \\frac{\\alpha}{2} \\lVert \\mathbf{D}\\mathbf{x} \\rVert_2^2.\n     $$\n     This yields the normal equations\n     $$\n     \\left( \\mathbf{A}^\\top \\mathbf{A} + \\alpha \\mathbf{D}^\\top \\mathbf{D} \\right) \\widehat{\\mathbf{t}}_{\\mathrm{Tik}} = \\mathbf{A}^\\top \\mathbf{m}.\n     $$\n   - Total Variation (TV) regularization (first-difference $\\ell_1$): for a given TV strength $\\tau > 0$, solve\n     $$\n     \\widehat{\\mathbf{t}}_{\\mathrm{TV}} = \\operatorname*{arg\\,min}_{\\mathbf{x} \\in \\mathbb{R}^N} \\frac{1}{2} \\lVert \\mathbf{A}\\mathbf{x} - \\mathbf{m} \\rVert_2^2 + \\tau \\lVert \\mathbf{D}\\mathbf{x} \\rVert_1.\n     $$\n     Implement this by the Alternating Direction Method of Multipliers (ADMM), introducing an auxiliary variable $\\mathbf{z} = \\mathbf{D}\\mathbf{x}$ and a penalty parameter $\\rho > 0$, with iterations\n     $$\n     \\mathbf{x}^{k+1} = \\left( \\mathbf{A}^\\top \\mathbf{A} + \\rho \\mathbf{D}^\\top \\mathbf{D} \\right)^{-1}\\left( \\mathbf{A}^\\top \\mathbf{m} + \\rho \\mathbf{D}^\\top(\\mathbf{z}^k - \\mathbf{u}^k) \\right),\n     $$\n     $$\n     \\mathbf{z}^{k+1} = \\operatorname{soft}\\left(\\mathbf{D}\\mathbf{x}^{k+1} + \\mathbf{u}^k, \\frac{\\tau}{\\rho}\\right),\n     $$\n     $$\n     \\mathbf{u}^{k+1} = \\mathbf{u}^k + \\mathbf{D}\\mathbf{x}^{k+1} - \\mathbf{z}^{k+1},\n     $$\n     where the soft-thresholding operator is defined componentwise by $\\operatorname{soft}(v, \\kappa) = \\operatorname{sign}(v)\\max(|v| - \\kappa, 0)$.\n\n3. Metrics to quantify how regularizers handle the induced edge:\n   For each unfolded spectrum $\\widehat{\\mathbf{t}}$ and each test case, compute:\n   - Mean squared error (MSE): \n     $$\n     \\mathrm{MSE}(\\widehat{\\mathbf{t}}) = \\frac{1}{N} \\sum_{i=0}^{N-1} \\left( \\widehat{t}_i - t_i \\right)^2.\n     $$\n   - Edge residual magnitude at the threshold:\n     Define the threshold index as $\\theta$. If $\\theta \\in \\{0,\\dots,N-2\\}$, use\n     $$\n     E(\\widehat{\\mathbf{t}}) = \\left| \\widehat{t}_{\\theta+1} - \\widehat{t}_{\\theta} \\right|.\n     $$\n     If $\\theta = N-1$, use\n     $$\n     E(\\widehat{\\mathbf{t}}) = \\left| \\widehat{t}_{\\theta} - \\widehat{t}_{\\theta-1} \\right|.\n     $$\n   - High-region mean bias relative to the true spectrum:\n     $$\n     B(\\widehat{\\mathbf{t}}) = \\frac{1}{N-\\theta} \\sum_{i=\\theta}^{N-1} \\widehat{t}_i - \\frac{1}{N-\\theta} \\sum_{i=\\theta}^{N-1} t_i.\n     $$\n     Since $t_i = 1$, this simplifies to $B(\\widehat{\\mathbf{t}}) = \\frac{1}{N-\\theta} \\sum_{i=\\theta}^{N-1} \\widehat{t}_i - 1$.\n\n4. Test suite:\n   Use the following three parameter sets to form the response matrix, synthesize $\\mathbf{m} = \\mathbf{A}\\mathbf{t}$, and unfold:\n   - Case $1$ (general case): $N = 50$, $\\sigma = 1.5$, $\\theta = 30$, $\\varepsilon_{\\mathrm{drop}} = 0.05$, $\\alpha = 10^{-2}$, $\\tau = 10^{-2}$, $\\rho = 0.5$.\n   - Case $2$ (boundary threshold at the last bin): $N = 50$, $\\sigma = 1.5$, $\\theta = 49$, $\\varepsilon_{\\mathrm{drop}} = 0.05$, $\\alpha = 10^{-2}$, $\\tau = 10^{-2}$, $\\rho = 0.5$.\n   - Case $3$ (minimal smearing, extreme drop): $N = 50$, $\\sigma = 0.5$, $\\theta = 25$, $\\varepsilon_{\\mathrm{drop}} = 0.01$, $\\alpha = 10^{-2}$, $\\tau = 10^{-2}$, $\\rho = 0.5$.\n\n5. Required final output format:\n   Your program should produce a single line of output containing the results as a list of lists, one inner list per test case. For each test case, output a list containing $9$ floating-point numbers in the following order:\n   $$\n   \\left[ \\mathrm{MSE}(\\widehat{\\mathbf{t}}_{\\mathrm{LS}}), \\, E(\\widehat{\\mathbf{t}}_{\\mathrm{LS}}), \\, B(\\widehat{\\mathbf{t}}_{\\mathrm{LS}}), \\, \\mathrm{MSE}(\\widehat{\\mathbf{t}}_{\\mathrm{Tik}}), \\, E(\\widehat{\\mathbf{t}}_{\\mathrm{Tik}}), \\, B(\\widehat{\\mathbf{t}}_{\\mathrm{Tik}}), \\, \\mathrm{MSE}(\\widehat{\\mathbf{t}}_{\\mathrm{TV}}), \\, E(\\widehat{\\mathbf{t}}_{\\mathrm{TV}}), \\, B(\\widehat{\\mathbf{t}}_{\\mathrm{TV}}) \\right].\n   $$\n   Aggregate the three test cases as a single list in the above order, yielding an output of the form\n   $$\n   \\left[ [\\cdots], [\\cdots], [\\cdots] \\right].\n   $$\n   No other text should be printed.", "solution": "The posed problem is a well-defined exercise in computational high-energy physics, specifically addressing the deconvolution or \"unfolding\" of detector effects from a measured one-dimensional spectrum. The problem is scientifically grounded, mathematically consistent, and provides all necessary information to proceed with a solution.\n\nThe core of the problem is the linear forward model, which relates the true, unknown distribution $\\mathbf{t} \\in \\mathbb{R}^N$ to the measured distribution $\\mathbf{m} \\in \\mathbb{R}^N$ via a detector response matrix $\\mathbf{A} \\in \\mathbb{R}^{N \\times N}$. This relationship is given by:\n$$\n\\mathbf{m} = \\mathbf{A} \\mathbf{t}\n$$\nThe goal of unfolding is to estimate the true spectrum $\\mathbf{t}$ given the measured spectrum $\\mathbf{m}$ and the response matrix $\\mathbf{A}$. This is an inverse problem, often ill-posed due to the properties of $\\mathbf{A}$.\n\nThe response matrix $\\mathbf{A}$ models two primary detector effects: resolution smearing and detection efficiency.\nFor each true bin $i$, the corresponding column of $\\mathbf{A}$ describes how events originating in bin $i$ are distributed across the measured bins $j$.\nThe smearing is modeled by a normalized Gaussian kernel with width $\\sigma$, representing the finite resolution of the detector which blurs sharp features. The efficiency $\\varepsilon_i$ is a multiplicative factor for each true bin, representing the probability that an event in bin $i$ is detected at all. The problem specifies a sharp drop in efficiency at a threshold bin $\\theta$. The elements of $\\mathbf{A}$ are thus defined as:\n$$\nA_{j i} = \\varepsilon_i \\, \\frac{\\exp\\left( -\\frac{(j-i)^2}{2 \\sigma^2} \\right)}{\\sum_{k=0}^{N-1} \\exp\\left( -\\frac{(k-i)^2}{2 \\sigma^2} \\right)}\n$$\nwhere the efficiency $\\varepsilon_i$ is a step function:\n$$\n\\varepsilon_i = \\begin{cases}\n1, & \\text{if } i < \\theta \\\\\n\\varepsilon_{\\mathrm{drop}}, & \\text{if } i \\ge \\theta\n\\end{cases}\n$$\nWith the known true spectrum $\\mathbf{t}$ (a flat distribution with $t_i = 1$), the measured spectrum $\\mathbf{m}$ is synthesized as $\\mathbf{m} = \\mathbf{A} \\mathbf{t}$.\n\nThree unfolding methods are employed to estimate $\\mathbf{t}$ from $\\mathbf{m}$ and $\\mathbf{A}$.\n\n1.  **Unregularized Least Squares (LS)**: This is the most direct approach to solving the inverse problem. It seeks to find an estimate $\\widehat{\\mathbf{t}}_{\\mathrm{LS}}$ that minimizes the squared difference between the re-convolved estimate $\\mathbf{A}\\mathbf{x}$ and the measured data $\\mathbf{m}$. The objective function is:\n    $$\n    \\widehat{\\mathbf{t}}_{\\mathrm{LS}} = \\operatorname*{arg\\,min}_{\\mathbf{x} \\in \\mathbb{R}^N} \\frac{1}{2} \\lVert \\mathbf{A}\\mathbf{x} - \\mathbf{m} \\rVert_2^2\n    $$\n    This is a standard linear least squares problem, the solution of which is found by solving the normal equations:\n    $$\n    \\mathbf{A}^\\top \\mathbf{A} \\, \\widehat{\\mathbf{t}}_{\\mathrm{LS}} = \\mathbf{A}^\\top \\mathbf{m}\n    $$\n    However, the matrix $\\mathbf{A}^\\top \\mathbf{A}$ is often ill-conditioned, causing the solution to be highly sensitive to small perturbations, which typically manifest as large, unphysical oscillations.\n\n2.  **Tikhonov Regularization**: This method extends least squares by adding a penalty term that regularizes the solution, suppressing oscillations. A common choice, used here, penalizes the squared $\\ell_2$-norm of the first differences of the solution vector, promoting smoothness. The first-difference operator is a matrix $\\mathbf{D} \\in \\mathbb{R}^{(N-1)\\times N}$. The objective function is:\n    $$\n    \\widehat{\\mathbf{t}}_{\\mathrm{Tik}} = \\operatorname*{arg\\,min}_{\\mathbf{x} \\in \\mathbb{R}^N} \\frac{1}{2} \\lVert \\mathbf{A}\\mathbf{x} - \\mathbf{m} \\rVert_2^2 + \\frac{\\alpha}{2} \\lVert \\mathbf{D}\\mathbf{x} \\rVert_2^2\n    $$\n    Here, $\\alpha > 0$ is the regularization strength, which balances the goodness-of-fit with the smoothness of the solution. This also has a closed-form solution via a modified set of normal equations:\n    $$\n    \\left( \\mathbf{A}^\\top \\mathbf{A} + \\alpha \\mathbf{D}^\\top \\mathbf{D} \\right) \\widehat{\\mathbf{t}}_{\\mathrm{Tik}} = \\mathbf{A}^\\top \\mathbf{m}\n    $$\n    The addition of the term $\\alpha \\mathbf{D}^\\top \\mathbf{D}$ generally improves the condition number of the matrix, leading to a more stable solution. However, this regularization can oversmooth sharp features, like the edge induced by the efficiency drop.\n\n3.  **Total Variation (TV) Regularization**: This method uses an $\\ell_1$-norm penalty on the first differences, which is known for its ability to preserve sharp edges while smoothing flat regions. The objective function is:\n    $$\n    \\widehat{\\mathbf{t}}_{\\mathrm{TV}} = \\operatorname*{arg\\,min}_{\\mathbf{x} \\in \\mathbb{R}^N} \\frac{1}{2} \\lVert \\mathbf{A}\\mathbf{x} - \\mathbf{m} \\rVert_2^2 + \\tau \\lVert \\mathbf{D}\\mathbf{x} \\rVert_1\n    $$\n    The parameter $\\tau > 0$ controls the strength of the TV regularization. Unlike Tikhonov regularization, this problem does not have a simple closed-form solution due to the non-differentiable $\\ell_1$-norm. It is solved here using the Alternating Direction Method of Multipliers (ADMM). This iterative algorithm introduces an auxiliary variable $\\mathbf{z}$ and solves the problem by alternating between updates for the primary variable $\\mathbf{x}$, the auxiliary variable $\\mathbf{z}$, and a dual variable $\\mathbf{u}$. The update steps as specified are implemented:\n    The $\\mathbf{x}$-update involves solving a linear system similar to the Tikhonov case. The $\\mathbf{z}$-update involves a component-wise soft-thresholding operation, which is the key step that promotes sparsity in the differences $\\mathbf{D}\\mathbf{x}$, thereby preserving edges. The $\\mathbf{u}$-update is a simple step to enforce the constraint $\\mathbf{z} = \\mathbf{D}\\mathbf{x}$.\n\nFinally, the performance of each unfolding method is evaluated using three metrics:\n-   **Mean Squared Error (MSE)**: $\\frac{1}{N} \\sum (\\widehat{t}_i - t_i)^2$. This provides a global measure of the accuracy of the unfolded spectrum.\n-   **Edge Residual Magnitude ($E$)**: $|\\widehat{t}_{\\theta+1} - \\widehat{t}_{\\theta}|$ (or similar at the boundary). This specifically quantifies how well the sharp drop in the true spectrum (which is implicitly introduced by the efficiency model) is reconstructed. Since the true spectrum is flat ($t_i=1$), the ideal difference is $0$, but the unfolding must counteract the efficiency step. This tests the regularizers' handling of edges.\n-   **High-Region Mean Bias ($B$)**: $\\frac{1}{N-\\theta} \\sum_{i=\\theta}^{N-1} \\widehat{t}_i - 1$. This measures the systematic deviation of the mean of the unfolded solution from the true mean in the region affected by the efficiency drop, quantifying any bias introduced by the unfolding process.\n\nThe implementation will proceed by systematically constructing the matrices, synthesizing the data, applying each of the three unfolding algorithms, and computing the nine specified metrics for each test case.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_case(N, sigma, theta, eps_drop, alpha, tau, rho):\n    \"\"\"\n    Runs a single test case for the unfolding problem.\n    \"\"\"\n    # 1. Setup: Define the true spectrum\n    t_true = np.ones(N)\n\n    # 2. Construct model matrices\n\n    # Construct efficiency vector\n    eps = np.ones(N)\n    if theta < N:\n        eps[theta:] = eps_drop\n\n    # Construct response matrix A\n    A = np.zeros((N, N))\n    indices = np.arange(N)\n    for i in range(N):\n        # Gaussian smearing kernel for column i\n        kernel_col = np.exp(-(indices - i)**2 / (2 * sigma**2))\n        # The kernel must be normalized to conserve events before efficiency\n        norm = np.sum(kernel_col)\n        if norm > 0:\n            kernel_col /= norm\n        # Apply efficiency\n        A[:, i] = eps[i] * kernel_col\n\n    # Construct first-difference operator D\n    D = np.zeros((N - 1, N))\n    row_indices = np.arange(N - 1)\n    D[row_indices, row_indices] = -1\n    D[row_indices, row_indices + 1] = 1\n\n    # 3. Synthesize measured data\n    m = A @ t_true\n\n    # 4. Perform unfolding with the three methods\n\n    # 4.1. Unregularized Least Squares\n    # Solve (A^T A) x = A^T m\n    try:\n        lhs_ls = A.T @ A\n        rhs_ls = A.T @ m\n        t_hat_ls = np.linalg.solve(lhs_ls, rhs_ls)\n    except np.linalg.LinAlgError:\n        # Use pseudo-inverse for singular matrices\n        t_hat_ls = np.linalg.pinv(A) @ m\n\n    # 4.2. Tikhonov Regularization\n    # Solve (A^T A + alpha D^T D) x = A^T m\n    lhs_tik = A.T @ A + alpha * (D.T @ D)\n    rhs_tik = A.T @ m\n    t_hat_tik = np.linalg.solve(lhs_tik, rhs_tik)\n\n    # 4.3. Total Variation Regularization via ADMM\n    def soft_threshold(v, kappa):\n        \"\"\"Soft-thresholding operator.\"\"\"\n        return np.sign(v) * np.maximum(np.abs(v) - kappa, 0)\n    \n    # Initialize ADMM variables\n    x = np.zeros(N)\n    z = np.zeros(N - 1)\n    u = np.zeros(N - 1)\n    \n    # Pre-compute the inverse matrix for the x-update step for efficiency\n    x_update_matrix = A.T @ A + rho * (D.T @ D)\n    x_update_matrix_inv = np.linalg.inv(x_update_matrix)\n    \n    # ADMM iterations\n    num_iterations = 200\n    for _ in range(num_iterations):\n        # x-update\n        rhs_x = A.T @ m + rho * D.T @ (z - u)\n        x = x_update_matrix_inv @ rhs_x\n        \n        # z-update\n        z_arg = D @ x + u\n        z = soft_threshold(z_arg, tau / rho)\n        \n        # u-update\n        u = u + D @ x - z\n    \n    t_hat_tv = x\n\n    # 5. Calculate and collect metrics\n    \n    all_results = []\n    for t_hat in [t_hat_ls, t_hat_tik, t_hat_tv]:\n        # Metric 1: Mean Squared Error (MSE)\n        mse = np.mean((t_hat - t_true)**2)\n        \n        # Metric 2: Edge Residual Magnitude (E)\n        if theta == N - 1:\n            edge_residual = np.abs(t_hat[theta] - t_hat[theta - 1])\n        elif theta < N - 1:\n            edge_residual = np.abs(t_hat[theta + 1] - t_hat[theta])\n        else: # Case where theta >= N, not in tests, but for completeness.\n            edge_residual = 0.0\n\n        # Metric 3: High-Region Mean Bias (B)\n        # The true mean in the high region is 1.0 since t_true is all ones.\n        if N > theta:\n            bias = np.mean(t_hat[theta:]) - 1.0\n        else: # Case theta >= N\n            bias = 0.0\n\n        all_results.extend([mse, edge_residual, bias])\n        \n    return all_results\n\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1 (general case)\n        {'N': 50, 'sigma': 1.5, 'theta': 30, 'eps_drop': 0.05, \n         'alpha': 1e-2, 'tau': 1e-2, 'rho': 0.5},\n        # Case 2 (boundary threshold)\n        {'N': 50, 'sigma': 1.5, 'theta': 49, 'eps_drop': 0.05, \n         'alpha': 1e-2, 'tau': 1e-2, 'rho': 0.5},\n        # Case 3 (minimal smearing, extreme drop)\n        {'N': 50, 'sigma': 0.5, 'theta': 25, 'eps_drop': 0.01, \n         'alpha': 1e-2, 'tau': 1e-2, 'rho': 0.5},\n    ]\n\n    results = []\n    for params in test_cases:\n        case_result = run_case(**params)\n        results.append(case_result)\n\n    # Final print statement in the exact required format.\n    # We construct the string manually to avoid spaces python's default str() adds.\n    case_strings = []\n    for res_list in results:\n        case_strings.append(f\"[{','.join(map(str, res_list))}]\")\n    \n    print(f\"[{','.join(case_strings)}]\")\n\nsolve()\n\n```", "id": "3540846"}, {"introduction": "Regularization methods introduce a crucial hyperparameter, $\\tau$, that balances fidelity to the data against the smoothness of the solution, but how do we choose its optimal value? This practice introduces Generalized Cross-Validation (GCV), a powerful statistical technique for making this choice objectively ([@problem_id:3540844]). You will implement an efficient algorithm to compute the GCV score and use it to find the best regularization strength, moving from heuristic choices to a principled, data-driven optimization.", "problem": "Consider a linear inverse problem that models detector smearing in high-energy physics as $\\mathbf{y} = \\mathbf{A} \\mathbf{x}_{\\text{true}} + \\boldsymbol{\\varepsilon}$, where $\\mathbf{y} \\in \\mathbb{R}^m$ is the measured spectrum, $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ is the detector response matrix, $\\mathbf{x}_{\\text{true}} \\in \\mathbb{R}^n$ is the unknown true spectrum to be unfolded, and $\\boldsymbol{\\varepsilon} \\in \\mathbb{R}^m$ is an additive noise vector. We assume $\\boldsymbol{\\varepsilon}$ is an independent, identically distributed Gaussian noise vector with zero mean and finite variance. To stabilize the inversion, use Tikhonov regularization with a linear operator $\\mathbf{L} \\in \\mathbb{R}^{p \\times n}$, and define the regularized estimator $\\mathbf{x}_\\tau$ as the minimizer of the cost function\n$$\nJ(\\mathbf{x};\\tau) = \\|\\mathbf{A} \\mathbf{x} - \\mathbf{y}\\|_2^2 + \\tau \\|\\mathbf{L} \\mathbf{x}\\|_2^2,\n$$\nfor a regularization strength $\\tau > 0$. The fitted measurement is $\\mathbf{y}_{\\text{hat}} = \\mathbf{A} \\mathbf{x}_\\tau$, and the hat matrix $\\mathbf{H}$ is defined as the unique linear map $\\mathbf{y} \\mapsto \\mathbf{y}_{\\text{hat}}$ induced by the above Tikhonov estimator, so that $\\mathbf{y}_{\\text{hat}} = \\mathbf{H} \\mathbf{y}$ holds for all $\\mathbf{y}$. The Generalized Cross-Validation (GCV) score for a given $\\tau$ is defined by\n$$\n\\operatorname{GCV}(\\tau) = \\frac{\\|( \\mathbf{I}_m - \\mathbf{H} ) \\mathbf{y} \\|_2^2}{\\left(m - \\operatorname{trace}(\\mathbf{H})\\right)^2},\n$$\nwhere $\\mathbf{I}_m$ is the $m \\times m$ identity matrix and $m$ is the dimension of $\\mathbf{y}$. The goal is to choose $\\tau$ that minimizes $\\operatorname{GCV}(\\tau)$.\n\nStarting only from the standard normal equations for the Tikhonov estimator and the definition of the hat matrix as the linear map from $\\mathbf{y}$ to $\\mathbf{y}_{\\text{hat}}$, derive an algorithm to compute $\\operatorname{GCV}(\\tau)$ for a specified set of $\\tau$ values without forming $\\mathbf{H}$ explicitly as a dense matrix. Use fundamental linear algebra identities and properties of the trace to design a numerically stable procedure.\n\nYour program must implement this algorithm and apply it to the following test suite. In each test case, construct $\\mathbf{A}$ as a column-normalized Gaussian response matrix with entries\n$$\nA_{ij} = \\exp\\!\\left(-\\frac{(i - s j)^2}{2 \\sigma^2}\\right),\n$$\nfor $i \\in \\{0,1,\\dots,m-1\\}$ and $j \\in \\{0,1,\\dots,n-1\\}$, where $s = \\frac{m-1}{n-1}$. After constructing $\\mathbf{A}$, normalize each column so that $\\sum_{i=0}^{m-1} A_{ij} = 1$ for all $j$. The regularization matrix $\\mathbf{L}$ should be chosen per case as specified. The measured vector is $\\mathbf{y} = \\mathbf{A} \\mathbf{x}_{\\text{true}} + \\boldsymbol{\\eta}$, where $\\mathbf{x}_{\\text{true}}$ and the additive term $\\boldsymbol{\\eta}$ are given in the test suite. The set of candidate values of $\\tau$ to search over is the geometric grid\n$$\n\\{\\tau_k\\}_{k=0}^{60} = \\{10^{-8}, 10^{-8 + \\Delta}, \\dots, 10^{2}\\},\n$$\nwith $61$ points equally spaced in $\\log_{10}$, that is, $\\Delta = \\frac{2 - (-8)}{60}$. For numerical robustness, if $m - \\operatorname{trace}(\\mathbf{H})$ is numerically zero for some $\\tau$, treat $\\operatorname{GCV}(\\tau)$ as $+\\infty$.\n\nTest suite specifications:\n- Case 1 (happy path, well-conditioned):\n  - $m = 6$, $n = 5$, $\\sigma = 0.8$, $\\mathbf{L} = \\mathbf{I}_n$.\n  - $\\mathbf{x}_{\\text{true}} = [\\,12.0,\\,18.0,\\,25.0,\\,18.0,\\,12.0\\,]$.\n  - $\\boldsymbol{\\eta} = [\\,0.5,\\,-0.3,\\,0.2,\\,-0.1,\\,0.4,\\,-0.2\\,]$.\n- Case 2 (ill-conditioned response, curvature regularization):\n  - $m = 6$, $n = 6$, $\\sigma = 1.5$, $\\mathbf{L}$ is the second-difference operator with rows implementing $[\\,1,\\,-2,\\,1\\,]$ stencils, so $\\mathbf{L} \\in \\mathbb{R}^{(n-2) \\times n}$ with entries $L_{k,k} = 1$, $L_{k,k+1} = -2$, $L_{k,k+2} = 1$ for $k \\in \\{0,\\dots,n-3\\}$ and zeros elsewhere.\n  - $\\mathbf{x}_{\\text{true}} = [\\,8.0,\\,12.0,\\,20.0,\\,12.0,\\,8.0,\\,5.0\\,]$.\n  - $\\boldsymbol{\\eta} = [\\,0.2,\\,-0.1,\\,0.3,\\,-0.2,\\,0.1,\\,-0.1\\,]$.\n- Case 3 (underdetermined system, identity regularization):\n  - $m = 5$, $n = 7$, $\\sigma = 0.7$, $\\mathbf{L} = \\mathbf{I}_n$.\n  - $\\mathbf{x}_{\\text{true}} = [\\,5.0,\\,7.0,\\,9.0,\\,11.0,\\,13.0,\\,11.0,\\,9.0\\,]$.\n  - $\\boldsymbol{\\eta} = [\\,0.1,\\,-0.1,\\,0.2,\\,-0.2,\\,0.1\\,]$.\n- Case 4 (sharp structure, first-difference smoothing):\n  - $m = 8$, $n = 8$, $\\sigma = 0.6$, $\\mathbf{L}$ is the first-difference operator with $p = n-1$ rows, each row $k$ has entries $L_{k,k} = -1$, $L_{k,k+1} = 1$ for $k \\in \\{0,\\dots,n-2\\}$ and zeros elsewhere.\n  - $\\mathbf{x}_{\\text{true}} = [\\,2.0,\\,3.0,\\,30.0,\\,3.0,\\,2.0,\\,1.5,\\,1.0,\\,0.5\\,]$.\n  - $\\boldsymbol{\\eta} = [\\,0.1,\\,-0.1,\\,0.0,\\,0.05,\\,-0.05,\\,0.0,\\,0.02,\\,-0.02\\,]$.\n\nYour program should compute, for each case, the value of $\\tau$ in the specified grid that minimizes $\\operatorname{GCV}(\\tau)$. The final output format must be a single line containing the four selected values as a comma-separated list enclosed in square brackets, for example, $[\\,\\tau_1,\\,\\tau_2,\\,\\tau_3,\\,\\tau_4\\,]$. No physical units or angle units are involved, and all outputs must be real-valued floating-point numbers.", "solution": "The problem of unfolding a true spectrum $\\mathbf{x}_{\\text{true}} \\in \\mathbb{R}^n$ from a measured spectrum $\\mathbf{y} \\in \\mathbb{R}^m$ subject to detector smearing and noise is modeled by the linear system $\\mathbf{y} = \\mathbf{A} \\mathbf{x}_{\\text{true}} + \\boldsymbol{\\varepsilon}$. Here, $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ is the detector response matrix and $\\boldsymbol{\\varepsilon}$ is a noise vector. We seek an estimate $\\mathbf{x}$ for the unknown $\\mathbf{x}_{\\text{true}}$. Due to the typically ill-conditioned nature of $\\mathbf{A}$, a direct inversion is unstable. Tikhonov regularization is employed to find a stable solution by minimizing a penalized cost function.\n\nThe problem is validated as follows:\n1.  **Extract Givens**:\n    -   Model: $\\mathbf{y} = \\mathbf{A} \\mathbf{x}_{\\text{true}} + \\boldsymbol{\\varepsilon}$, where $\\boldsymbol{\\varepsilon}$ is i.i.d. Gaussian with zero mean.\n    -   Cost Function: $J(\\mathbf{x};\\tau) = \\|\\mathbf{A} \\mathbf{x} - \\mathbf{y}\\|_2^2 + \\tau \\|\\mathbf{L} \\mathbf{x}\\|_2^2$, with regularization parameter $\\tau > 0$ and operator $\\mathbf{L} \\in \\mathbb{R}^{p \\times n}$.\n    -   Estimator $\\mathbf{x}_\\tau$: $\\mathbf{x}_\\tau = \\operatorname{argmin}_{\\mathbf{x}} J(\\mathbf{x};\\tau)$.\n    -   Fitted Measurement: $\\mathbf{y}_{\\text{hat}} = \\mathbf{A} \\mathbf{x}_\\tau$.\n    -   Hat Matrix $\\mathbf{H}$: The linear map such that $\\mathbf{y}_{\\text{hat}} = \\mathbf{H} \\mathbf{y}$.\n    -   Generalized Cross-Validation (GCV) Score: $\\operatorname{GCV}(\\tau) = \\frac{\\|( \\mathbf{I}_m - \\mathbf{H} ) \\mathbf{y} \\|_2^2}{\\left(m - \\operatorname{trace}(\\mathbf{H})\\right)^2}$, where $\\mathbf{I}_m$ is the $m \\times m$ identity.\n    -   Response Matrix $\\mathbf{A}$: $A_{ij} = \\exp\\left(-\\frac{(i - s j)^2}{2 \\sigma^2}\\right)$ for $i \\in \\{0, \\dots, m-1\\}$, $j \\in \\{0, \\dots, n-1\\}$, with $s = \\frac{m-1}{n-1}$. Columns are normalized to sum to $1$.\n    -   Regularization Parameter Grid: $\\{\\tau_k\\}_{k=0}^{60} = \\{10^{-8}, 10^{-8 + \\Delta}, \\dots, 10^{2}\\}$ with $\\Delta = \\frac{10}{60}$.\n    -   Test Cases: Four explicit cases specifying $m, n, \\sigma, \\mathbf{L}, \\mathbf{x}_{\\text{true}}, \\boldsymbol{\\eta}$.\n\n2.  **Validate Using Extracted Givens**:\n    -   **Scientific Grounding**: The problem is firmly located within the standard theoretical framework of inverse problems and regularization theory, which is a core topic in computational physics and data analysis. All concepts (Tikhonov regularization, GCV, response matrix) are standard and correctly stated.\n    -   **Well-Posed**: The problem asks to find a value $\\tau$ from a discrete grid that minimizes a function. A minimum is guaranteed to exist.\n    -   **Objective**: The problem is stated objectively with precise mathematical definitions and numerical data. There are no subjective or ambiguous elements.\n    -   **Completeness**: All necessary parameters, matrices, vectors, and functional forms are specified for each test case, making the problem self-contained.\n    -   **Consistency**: The definitions and constraints are mutually consistent. For example, the definition of the second-difference operator for Case 2 is internally consistent regarding dimensions and indices.\n\n3.  **Verdict and Action**: The problem is valid. A complete, reasoned solution will be provided.\n\n### Derivation of the GCV Computation Algorithm\n\nThe core task is to devise an algorithm to compute $\\operatorname{GCV}(\\tau)$ for a grid of $\\tau$ values without explicitly forming the dense $m \\times m$ hat matrix $\\mathbf{H}$.\n\n**1. The Tikhonov Solution**\nThe estimator $\\mathbf{x}_\\tau$ minimizes the quadratic cost function $J(\\mathbf{x};\\tau)$. We find this minimum by setting the gradient with respect to $\\mathbf{x}$ to zero:\n$$\n\\nabla_\\mathbf{x} J(\\mathbf{x};\\tau) = \\nabla_\\mathbf{x} \\left( ( \\mathbf{A}\\mathbf{x} - \\mathbf{y} )^T ( \\mathbf{A}\\mathbf{x} - \\mathbf{y} ) + \\tau ( \\mathbf{L}\\mathbf{x} )^T ( \\mathbf{L}\\mathbf{x} ) \\right) = 0\n$$\n$$\n\\nabla_\\mathbf{x} \\left( \\mathbf{x}^T \\mathbf{A}^T \\mathbf{A} \\mathbf{x} - 2 \\mathbf{y}^T \\mathbf{A} \\mathbf{x} + \\mathbf{y}^T \\mathbf{y} + \\tau \\mathbf{x}^T \\mathbf{L}^T \\mathbf{L} \\mathbf{x} \\right) = 0\n$$\n$$\n2 \\mathbf{A}^T \\mathbf{A} \\mathbf{x} - 2 \\mathbf{A}^T \\mathbf{y} + 2 \\tau \\mathbf{L}^T \\mathbf{L} \\mathbf{x} = 0\n$$\nThis yields the normal equations for the Tikhonov problem:\n$$\n( \\mathbf{A}^T \\mathbf{A} + \\tau \\mathbf{L}^T \\mathbf{L} ) \\mathbf{x} = \\mathbf{A}^T \\mathbf{y}\n$$\nThe solution $\\mathbf{x}_\\tau$ is therefore:\n$$\n\\mathbf{x}_\\tau = ( \\mathbf{A}^T \\mathbf{A} + \\tau \\mathbf{L}^T \\mathbf{L} )^{-1} \\mathbf{A}^T \\mathbf{y}\n$$\nFor a given $\\tau$, $\\mathbf{x}_\\tau$ can be found by solving this $n \\times n$ linear system. The matrix $\\mathbf{M}_\\tau = \\mathbf{A}^T \\mathbf{A} + \\tau \\mathbf{L}^T \\mathbf{L}$ is symmetric and, for $\\tau > 0$, positive definite, ensuring a unique solution exists.\n\n**2. Expression for the Hat Matrix**\nThe hat matrix $\\mathbf{H}$ maps the measurement vector $\\mathbf{y}$ to the fitted measurement vector $\\mathbf{y}_{\\text{hat}} = \\mathbf{A} \\mathbf{x}_\\tau$. Substituting the expression for $\\mathbf{x}_\\tau$:\n$$\n\\mathbf{y}_{\\text{hat}} = \\mathbf{A} \\mathbf{x}_\\tau = \\mathbf{A} ( \\mathbf{A}^T \\mathbf{A} + \\tau \\mathbf{L}^T \\mathbf{L} )^{-1} \\mathbf{A}^T \\mathbf{y}\n$$\nBy the definition $\\mathbf{y}_{\\text{hat}} = \\mathbf{H} \\mathbf{y}$, we identify the hat matrix as:\n$$\n\\mathbf{H} = \\mathbf{A} ( \\mathbf{A}^T \\mathbf{A} + \\tau \\mathbf{L}^T \\mathbf{L} )^{-1} \\mathbf{A}^T = \\mathbf{A} \\mathbf{M}_\\tau^{-1} \\mathbf{A}^T\n$$\n\n**3. Computing the GCV Numerator**\nThe numerator of the GCV score is the squared norm of the residual vector, $\\|( \\mathbf{I}_m - \\mathbf{H} ) \\mathbf{y} \\|_2^2$.\n$$\n( \\mathbf{I}_m - \\mathbf{H} ) \\mathbf{y} = \\mathbf{y} - \\mathbf{H} \\mathbf{y} = \\mathbf{y} - \\mathbf{y}_{\\text{hat}} = \\mathbf{y} - \\mathbf{A} \\mathbf{x}_\\tau\n$$\nThe numerator is thus simply $\\| \\mathbf{y} - \\mathbf{A} \\mathbf{x}_\\tau \\|_2^2$. This can be computed efficiently:\n1.  Solve the normal equations for $\\mathbf{x}_\\tau$.\n2.  Compute the vector $\\mathbf{r} = \\mathbf{y} - \\mathbf{A} \\mathbf{x}_\\tau$.\n3.  Calculate the squared Euclidean norm $\\mathbf{r}^T \\mathbf{r}$.\n\n**4. Computing the GCV Denominator**\nThe denominator is $(m - \\operatorname{trace}(\\mathbf{H}))^2$. The main challenge is to compute $\\operatorname{trace}(\\mathbf{H})$ without forming the $m \\times m$ matrix $\\mathbf{H}$. The trace is the sum of the diagonal elements, $\\operatorname{trace}(\\mathbf{H}) = \\sum_{i=0}^{m-1} H_{ii}$.\nThe $i$-th diagonal element is given by $H_{ii} = \\mathbf{e}_i^T \\mathbf{H} \\mathbf{e}_i$, where $\\mathbf{e}_i$ is the $i$-th standard basis vector in $\\mathbb{R}^m$.\n$$\nH_{ii} = \\mathbf{e}_i^T \\left( \\mathbf{A} \\mathbf{M}_\\tau^{-1} \\mathbf{A}^T \\right) \\mathbf{e}_i = (\\mathbf{e}_i^T \\mathbf{A}) \\mathbf{M}_\\tau^{-1} (\\mathbf{A}^T \\mathbf{e}_i)\n$$\nLet $\\mathbf{a}_i^T$ denote the $i$-th row of $\\mathbf{A}$. Then $\\mathbf{e}_i^T \\mathbf{A} = \\mathbf{a}_i^T$ and $\\mathbf{A}^T \\mathbf{e}_i = \\mathbf{a}_i$. Thus:\n$$\nH_{ii} = \\mathbf{a}_i^T \\mathbf{M}_\\tau^{-1} \\mathbf{a}_i\n$$\nTo compute this quantity, we define an intermediate vector $\\mathbf{z}_i = \\mathbf{M}_\\tau^{-1} \\mathbf{a}_i$. This vector can be found by solving the linear system $\\mathbf{M}_\\tau \\mathbf{z}_i = \\mathbf{a}_i$. Once $\\mathbf{z}_i$ is found, the diagonal element is obtained by the dot product $H_{ii} = \\mathbf{a}_i^T \\mathbf{z}_i$.\nThe total trace is the sum over all diagonal elements:\n$$\n\\operatorname{trace}(\\mathbf{H}) = \\sum_{i=0}^{m-1} H_{ii} = \\sum_{i=0}^{m-1} \\mathbf{a}_i^T \\left( \\mathbf{M}_\\tau^{-1} \\mathbf{a}_i \\right)\n$$\nThis procedure requires solving $m$ linear systems of size $n \\times n$ for each value of $\\tau$. While computationally more intensive than a single solve, it is feasible for the given problem dimensions and avoids forming the potentially large matrix $\\mathbf{H}$.\n\n**5. Complete Algorithm**\nFor each test case, the following algorithm is executed:\n1.  Construct the matrices $\\mathbf{A}$, $\\mathbf{L}$ and the vector $\\mathbf{y}$ as specified. Pre-compute $\\mathbf{A}^T\\mathbf{A}$, $\\mathbf{L}^T\\mathbf{L}$, and $\\mathbf{A}^T \\mathbf{y}$.\n2.  Generate the logarithmic grid of $61$ candidate values for $\\tau$ from $10^{-8}$ to $10^2$.\n3.  Initialize `min_gcv` to infinity and `best_tau` to `None`.\n4.  For each $\\tau_k$ in the grid:\n    a. Form the matrix $\\mathbf{M}_k = \\mathbf{A}^T\\mathbf{A} + \\tau_k \\mathbf{L}^T\\mathbf{L}$.\n    b. Solve the system $\\mathbf{M}_k \\mathbf{x}_\\tau = \\mathbf{A}^T \\mathbf{y}$ for the regularized solution $\\mathbf{x}_\\tau$.\n    c. Compute the GCV numerator: $N = \\|\\mathbf{y} - \\mathbf{A} \\mathbf{x}_\\tau\\|_2^2$.\n    d. Compute the trace of the hat matrix:\n        i. Initialize $\\operatorname{trace}(\\mathbf{H}) = 0$.\n        ii. For each row index $i$ from $0$ to $m-1$:\n           - Extract the $i$-th row of $\\mathbf{A}$, denoted $\\mathbf{a}_i^T$.\n           - Solve the linear system $\\mathbf{M}_k \\mathbf{z}_i = \\mathbf{a}_i$ for $\\mathbf{z}_i$.\n           - Compute the diagonal element $H_{ii} = \\mathbf{a}_i^T \\mathbf{z}_i$.\n           - Accumulate the trace: $\\operatorname{trace}(\\mathbf{H}) \\leftarrow \\operatorname{trace}(\\mathbf{H}) + H_{ii}$.\n    e. Compute the denominator term $D = m - \\operatorname{trace}(\\mathbf{H})$.\n    f. If $D$ is numerically close to zero, set $\\operatorname{GCV}(\\tau_k) = \\infty$. Otherwise, compute $\\operatorname{GCV}(\\tau_k) = N / D^2$.\n    g. If $\\operatorname{GCV}(\\tau_k) < \\text{min\\_gcv}$, update $\\text{min\\_gcv} = \\operatorname{GCV}(\\tau_k)$ and $\\text{best\\_tau} = \\tau_k$.\n5.  The optimal regularization parameter for the case is `best_tau`.\n\nThis algorithm rigorously follows from the definitions and adheres to the constraint of not forming the dense hat matrix $\\mathbf{H}$. It is implemented to solve the provided test cases.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the GCV optimization for all test cases.\n    \"\"\"\n    test_cases = [\n        {\n            \"m\": 6, \"n\": 5, \"sigma\": 0.8, \"L_type\": \"identity\",\n            \"x_true\": np.array([12.0, 18.0, 25.0, 18.0, 12.0]),\n            \"eta\": np.array([0.5, -0.3, 0.2, -0.1, 0.4, -0.2]),\n        },\n        {\n            \"m\": 6, \"n\": 6, \"sigma\": 1.5, \"L_type\": \"second_diff\",\n            \"x_true\": np.array([8.0, 12.0, 20.0, 12.0, 8.0, 5.0]),\n            \"eta\": np.array([0.2, -0.1, 0.3, -0.2, 0.1, -0.1]),\n        },\n        {\n            \"m\": 5, \"n\": 7, \"sigma\": 0.7, \"L_type\": \"identity\",\n            \"x_true\": np.array([5.0, 7.0, 9.0, 11.0, 13.0, 11.0, 9.0]),\n            \"eta\": np.array([0.1, -0.1, 0.2, -0.2, 0.1]),\n        },\n        {\n            \"m\": 8, \"n\": 8, \"sigma\": 0.6, \"L_type\": \"first_diff\",\n            \"x_true\": np.array([2.0, 3.0, 30.0, 3.0, 2.0, 1.5, 1.0, 0.5]),\n            \"eta\": np.array([0.1, -0.1, 0.0, 0.05, -0.05, 0.0, 0.02, -0.02]),\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        best_tau = find_best_tau(case)\n        results.append(f\"{best_tau:.8e}\")\n\n    print(f\"[{','.join(results)}]\")\n\ndef construct_A(m, n, sigma):\n    \"\"\"Constructs the response matrix A.\"\"\"\n    if n > 1:\n        s = (m - 1) / (n - 1)\n    else:\n        s = 0.0\n    \n    i_coords = np.arange(m).reshape(-1, 1)\n    j_coords = np.arange(n).reshape(1, -1)\n    \n    A = np.exp(-(i_coords - s * j_coords)**2 / (2 * sigma**2))\n    \n    # Normalize each column to sum to 1\n    col_sums = A.sum(axis=0)\n    # Avoid division by zero if a column is all zeros, though unlikely here\n    col_sums[col_sums == 0] = 1.0\n    A = A / col_sums\n    \n    return A\n\ndef construct_L(L_type, n):\n    \"\"\"Constructs the regularization operator L.\"\"\"\n    if L_type == \"identity\":\n        return np.eye(n)\n    elif L_type == \"first_diff\":\n        p = n - 1\n        if p <= 0: return np.zeros((0, n))\n        L = np.zeros((p, n))\n        L[np.arange(p), np.arange(p)] = -1.0\n        L[np.arange(p), np.arange(p) + 1] = 1.0\n        return L\n    elif L_type == \"second_diff\":\n        p = n - 2\n        if p <= 0: return np.zeros((0, n))\n        L = np.zeros((p, n))\n        L[np.arange(p), np.arange(p)] = 1.0\n        L[np.arange(p), np.arange(p) + 1] = -2.0\n        L[np.arange(p), np.arange(p) + 2] = 1.0\n        return L\n    else:\n        raise ValueError(f\"Unknown L_type: {L_type}\")\n\ndef find_best_tau(case_params):\n    \"\"\"\n    Finds the optimal tau for a given case by minimizing the GCV score.\n    \"\"\"\n    m, n, sigma = case_params[\"m\"], case_params[\"n\"], case_params[\"sigma\"]\n    L_type, x_true, eta = case_params[\"L_type\"], case_params[\"x_true\"], case_params[\"eta\"]\n\n    A = construct_A(m, n, sigma)\n    L = construct_L(L_type, n)\n    \n    y = A @ x_true + eta\n\n    AtA = A.T @ A\n    LtL = L.T @ L\n    Aty = A.T @ y\n\n    # Generate the logarithmic grid of tau values\n    # The problem statement has a small typo in Delta, it should be (2 - (-8)) / 60\n    taus = np.logspace(-8, 2, num=61)\n\n    min_gcv = np.inf\n    best_tau = None\n\n    for tau in taus:\n        # Form the matrix for the normal equations\n        M_tau = AtA + tau * LtL\n\n        try:\n            # Solve for the Tikhonov estimator x_tau\n            x_tau = np.linalg.solve(M_tau, Aty)\n            \n            # Compute the numerator of the GCV score\n            numerator = np.linalg.norm(A @ x_tau - y)**2\n\n            # Compute the trace of the hat matrix H without forming H\n            trace_H = 0.0\n            for i in range(m):\n                a_i_row = A[i, :]\n                z_i = np.linalg.solve(M_tau, a_i_row)\n                H_ii = a_i_row.T @ z_i\n                trace_H += H_ii\n\n            # Compute the denominator term\n            denominator_term = m - trace_H\n\n            if np.isclose(denominator_term, 0.0):\n                gcv = np.inf\n            else:\n                gcv = numerator / (denominator_term**2)\n        \n        except np.linalg.LinAlgError:\n            # If the matrix M_tau is singular, GCV is not well-defined.\n            gcv = np.inf\n\n        if gcv < min_gcv:\n            min_gcv = gcv\n            best_tau = tau\n\n    return best_tau\n\nsolve()\n```", "id": "3540844"}]}