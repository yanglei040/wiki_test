{"hands_on_practices": [{"introduction": "In a weighted Monte Carlo sample, events contribute unequally to the final measurement, meaning the statistical precision is not simply determined by the total number of events $N$. This first practice introduces the crucial concept of effective sample size, $N_{\\text{eff}}$, which provides a true measure of the statistical power of a weighted sample. By working through this problem [@problem_id:3513799], you will learn to quantify the impact of weight variation on the variance of an estimator and understand why minimizing this variation is a key objective.", "problem": "In computational high-energy physics, event generators often produce simulated events with positive weights $w_i>0$ that reflect differential cross-section factors or importance sampling adjustments. Consider estimating a scalar observable using $N$ independent event-level contributions $Y_i$ that have a common finite variance $\\sigma^2$ under the target physics distribution. A standard self-normalized weighted estimator is constructed as $\\hat{\\mu}=\\sum_{i=1}^{N}\\alpha_i Y_i$ with normalized coefficients $\\alpha_i=w_i/\\sum_{j=1}^{N}w_j$. Using the laws of variance for linear combinations of independent random variables and the normalization constraint on $\\{\\alpha_i\\}$, define the notion of an effective sample size $N_{\\text{eff}}$ by matching the variance of $\\hat{\\mu}$ to that of an unweighted sample mean. Evaluate $N_{\\text{eff}}$ numerically for the weights $\\{2,1,1,0.5\\}$, and interpret $N_{\\text{eff}}$ in terms of statistical power.\n\nSelect all statements that are correct:\n\nA. For independent event contributions with common variance $\\sigma^2$, the variance of the self-normalized weighted estimator satisfies $\\operatorname{Var}(\\hat{\\mu})=\\sigma^2\\sum_{i=1}^{N}\\alpha_i^2$, and equating $\\operatorname{Var}(\\hat{\\mu})=\\sigma^2/N_{\\text{eff}}$ yields $N_{\\text{eff}}=\\left(\\sum_{i=1}^{N}w_i\\right)^2\\Big/\\left(\\sum_{i=1}^{N}w_i^2\\right)$.\n\nB. For any choice of nonnegative weights, one always has $N_{\\text{eff}}\\geq N$.\n\nC. The quantity $N_{\\text{eff}}$ equals the number of strictly positive weights.\n\nD. One has $N_{\\text{eff}}=N$ if and only if all weights are identical up to a common scaling, that is $w_i=c$ for some constant $c>0$ and all $i$.\n\nE. For the weights $\\{2,1,1,0.5\\}$, $N_{\\text{eff}}$ is $3.24$, so $\\operatorname{Var}(\\hat{\\mu})=\\sigma^2/3.24$, which matches the variance of an unweighted mean computed from $3.24$ independent, equal-weight events in the sense of statistical power.\n\nF. In acceptance-rejection unweighting with acceptance probability $\\min\\{1,w_i/w_{\\max}\\}$ for a normalization constant $w_{\\max}=\\max_i w_i$, the expected number of accepted events equals $N_{\\text{eff}}$.", "solution": "The problem statement poses a well-defined question in the context of statistical data analysis for computational physics. It asks for the derivation and interpretation of the effective sample size, $N_{\\text{eff}}$, for a weighted sample. All provided information is scientifically sound, self-contained, and sufficient for a rigorous solution. The problem is therefore valid.\n\nThe core task is to define the effective sample size $N_{\\text{eff}}$ by comparing the variance of a weighted mean to that of an unweighted mean.\n\nLet the set of $N$ weights be $\\{w_1, w_2, \\ldots, w_N\\}$, with $w_i > 0$. The weighted estimator for a scalar observable $\\mu$ is given by:\n$$ \\hat{\\mu} = \\sum_{i=1}^{N} \\alpha_i Y_i $$\nwhere $Y_i$ are independent contributions with a common variance $\\operatorname{Var}(Y_i) = \\sigma^2$, and the coefficients $\\alpha_i$ are normalized weights:\n$$ \\alpha_i = \\frac{w_i}{\\sum_{j=1}^{N} w_j} $$\nThese coefficients satisfy the normalization condition $\\sum_{i=1}^{N} \\alpha_i = 1$.\n\nThe variance of the estimator $\\hat{\\mu}$ is calculated using the property of variance for a linear combination of independent random variables, $\\operatorname{Var}\\left(\\sum_i c_i X_i\\right) = \\sum_i c_i^2 \\operatorname{Var}(X_i)$. In our case, the coefficients are the $\\alpha_i$, which are treated as constants for this calculation.\n$$ \\operatorname{Var}(\\hat{\\mu}) = \\operatorname{Var}\\left(\\sum_{i=1}^{N} \\alpha_i Y_i\\right) = \\sum_{i=1}^{N} \\alpha_i^2 \\operatorname{Var}(Y_i) = \\sum_{i=1}^{N} \\alpha_i^2 \\sigma^2 = \\sigma^2 \\sum_{i=1}^{N} \\alpha_i^2 $$\n\nAn unweighted mean of $M$ independent events, $\\bar{Y} = \\frac{1}{M}\\sum_{i=1}^{M} Y_i$, has a variance:\n$$ \\operatorname{Var}(\\bar{Y}) = \\operatorname{Var}\\left(\\frac{1}{M}\\sum_{i=1}^{M} Y_i\\right) = \\frac{1}{M^2} \\sum_{i=1}^{M} \\operatorname{Var}(Y_i) = \\frac{1}{M^2} (M \\sigma^2) = \\frac{\\sigma^2}{M} $$\n\nThe effective sample size, $N_{\\text{eff}}$, is defined by equating the variance of the weighted estimator $\\hat{\\mu}$ to the variance of an unweighted mean of $N_{\\text{eff}}$ events:\n$$ \\operatorname{Var}(\\hat{\\mu}) = \\frac{\\sigma^2}{N_{\\text{eff}}} $$\nSubstituting our expression for $\\operatorname{Var}(\\hat{\\mu})$:\n$$ \\sigma^2 \\sum_{i=1}^{N} \\alpha_i^2 = \\frac{\\sigma^2}{N_{\\text{eff}}} $$\n$$ N_{\\text{eff}} = \\frac{1}{\\sum_{i=1}^{N} \\alpha_i^2} $$\nNow, we substitute the definition of $\\alpha_i$ in terms of the original weights $w_i$. Let $W = \\sum_{j=1}^{N} w_j$.\n$$ N_{\\text{eff}} = \\frac{1}{\\sum_{i=1}^{N} \\left(\\frac{w_i}{W}\\right)^2} = \\frac{1}{\\frac{1}{W^2} \\sum_{i=1}^{N} w_i^2} = \\frac{W^2}{\\sum_{i=1}^{N} w_i^2} $$\nThus, the formula for the effective sample size is:\n$$ N_{\\text{eff}} = \\frac{\\left(\\sum_{i=1}^{N} w_i\\right)^2}{\\sum_{i=1}^{N} w_i^2} $$\n\nNow, we evaluate each option.\n\n**A. For independent event contributions with common variance $\\sigma^2$, the variance of the self-normalized weighted estimator satisfies $\\operatorname{Var}(\\hat{\\mu})=\\sigma^2\\sum_{i=1}^{N}\\alpha_i^2$, and equating $\\operatorname{Var}(\\hat{\\mu})=\\sigma^2/N_{\\text{eff}}$ yields $N_{\\text{eff}}=\\left(\\sum_{i=1}^{N}w_i\\right)^2\\Big/\\left(\\sum_{i=1}^{N}w_i^2\\right)$.**\nOur derivation above confirms both parts of this statement. The variance calculation is correct, and the resulting expression for $N_{\\text{eff}}$ is also correct.\nVerdict: **Correct**.\n\n**B. For any choice of nonnegative weights, one always has $N_{\\text{eff}}\\geq N$.**\nTo evaluate this, we compare $N_{\\text{eff}} = \\frac{(\\sum w_i)^2}{\\sum w_i^2}$ with $N$.\nConsider the Cauchy-Schwarz inequality for two vectors $u = (w_1, \\ldots, w_N)$ and $v = (1, \\ldots, 1)$ in $\\mathbb{R}^N$:\n$$ \\left(\\sum_{i=1}^{N} u_i v_i\\right)^2 \\leq \\left(\\sum_{i=1}^{N} u_i^2\\right) \\left(\\sum_{i=1}^{N} v_i^2\\right) $$\n$$ \\left(\\sum_{i=1}^{N} w_i \\cdot 1\\right)^2 \\leq \\left(\\sum_{i=1}^{N} w_i^2\\right) \\left(\\sum_{i=1}^{N} 1^2\\right) $$\n$$ \\left(\\sum_{i=1}^{N} w_i\\right)^2 \\leq \\left(\\sum_{i=1}^{N} w_i^2\\right) \\cdot N $$\nSince weights are positive, $\\sum w_i^2 > 0$, so we can divide by it without changing the inequality's direction:\n$$ \\frac{\\left(\\sum_{i=1}^{N} w_i\\right)^2}{\\sum_{i=1}^{N} w_i^2} \\leq N $$\nThis means $N_{\\text{eff}} \\leq N$. The statement claims $N_{\\text{eff}} \\geq N$, which is the opposite. Therefore, the statement is false. The inequality of weights reduces the effective sample size.\nVerdict: **Incorrect**.\n\n**C. The quantity $N_{\\text{eff}}$ equals the number of strictly positive weights.**\nThe problem provides a specific set of weights to test: $w = \\{2, 1, 1, 0.5\\}$. Here, the number of events is $N=4$, and all weights are strictly positive.\nLet's compute $N_{\\text{eff}}$ for this set:\n$$ \\sum_{i=1}^{4} w_i = 2 + 1 + 1 + 0.5 = 4.5 $$\n$$ \\sum_{i=1}^{4} w_i^2 = 2^2 + 1^2 + 1^2 + 0.5^2 = 4 + 1 + 1 + 0.25 = 6.25 $$\n$$ N_{\\text{eff}} = \\frac{(4.5)^2}{6.25} = \\frac{20.25}{6.25} = 3.24 $$\nThe number of strictly positive weights is $4$, but $N_{\\text{eff}} = 3.24$. Thus, $3.24 \\neq 4$. The statement is false.\nVerdict: **Incorrect**.\n\n**D. One has $N_{\\text{eff}}=N$ if and only if all weights are identical up to a common scaling, that is $w_i=c$ for some constant $c>0$ and all $i$.**\nThe phrase \"identical up to a common scaling\" is clarified by \"that is $w_i=c$ for some constant $c>0$\". This means we must test the condition $w_1 = w_2 = \\ldots = w_N = c > 0$.\nThe equality in the Cauchy-Schwarz inequality used in option B holds if and only if the vectors $u$ and $v$ are linearly dependent, i.e., $u = k v$ for some scalar $k$.\nIn our case, $u = (w_1, \\ldots, w_N)$ and $v=(1, \\ldots, 1)$. Linear dependence means $w_i = k \\cdot 1$ for all $i$. Since weights are positive, $k$ must be a positive constant, let's call it $c$. Thus, $N_{\\text{eff}} = N$ if and only if $w_i = c$ for all $i$.\nThe statement is correct. It precisely describes the condition for maximum effective sample size.\nVerdict: **Correct**.\n\n**E. For the weights $\\{2,1,1,0.5\\}$, $N_{\\text{eff}}$ is $3.24$, so $\\operatorname{Var}(\\hat{\\mu})=\\sigma^2/3.24$, which matches the variance of an unweighted mean computed from $3.24$ independent, equal-weight events in the sense of statistical power.**\nThe calculation in option C shows that $N_{\\text{eff}} = 3.24$ is correct for the given weights.\nBy the definition of $N_{\\text{eff}}$, we have $\\operatorname{Var}(\\hat{\\mu}) = \\sigma^2 / N_{\\text{eff}}$, which is $\\sigma^2 / 3.24$. This is correct.\nThe variance of an unweighted mean of $M$ events is $\\sigma^2/M$. For a hypothetical sample of $M=3.24$ events, the variance would indeed be $\\sigma^2/3.24$.\nStatistical power, for tests concerning the mean, is a function of the standard error of the estimator, $\\sqrt{\\operatorname{Var}(\\hat{\\mu})}$. Since the variance of the weighted mean is mathematically equivalent to the variance of an unweighted mean of $3.24$ events, the statistical power is identical. The interpretation is sound and represents the fundamental purpose of the $N_{\\text{eff}}$ concept.\nVerdict: **Correct**.\n\n**F. In acceptance-rejection unweighting with acceptance probability $\\min\\{1,w_i/w_{\\max}\\}$ for a normalization constant $w_{\\max}=\\max_i w_i$, the expected number of accepted events equals $N_{\\text{eff}}$.**\nThe acceptance probability for the $i$-th event is $p_i = w_i/w_{\\max}$ (since $w_i \\le w_{\\max}$, the $\\min$ is redundant). The process of accepting or rejecting each event is a Bernoulli trial. The expected number of accepted events, $E[N_{\\text{acc}}]$, is the sum of the individual acceptance probabilities over all $N$ initial events:\n$$ E[N_{\\text{acc}}] = \\sum_{i=1}^{N} p_i = \\sum_{i=1}^{N} \\frac{w_i}{w_{\\max}} = \\frac{1}{w_{\\max}} \\sum_{i=1}^{N} w_i $$\nThe statement claims $E[N_{\\text{acc}}] = N_{\\text{eff}}$. Let's test this with the given weights $w = \\{2, 1, 1, 0.5\\}$.\nWe have $w_{\\max} = \\max\\{2, 1, 1, 0.5\\} = 2$ and $\\sum w_i = 4.5$.\n$$ E[N_{\\text{acc}}] = \\frac{4.5}{2} = 2.25 $$\nFrom option C, we know $N_{\\text{eff}} = 3.24$.\nSince $2.25 \\neq 3.24$, the statement is false. These are two distinct quantities that describe different properties of the weighted sample. They are only equal in the trivial case where all weights are identical.\nVerdict: **Incorrect**.", "answer": "$$\\boxed{ADE}$$", "id": "3513799"}, {"introduction": "Standard unweighting can be inefficient if it rejects too many events, especially those with large weights that are physically important. This exercise explores partial unweighting, a sophisticated hybrid strategy that aims to reduce variance by unweighting low-weight events while retaining high-weight ones. To solve this problem [@problem_id:3513734], you will apply the laws of total expectation and variance to construct and analyze a novel estimator, developing skills essential for designing custom variance-reduction techniques.", "problem": "In a Monte Carlo (MC) simulation for a high-energy physics process, the goal is to estimate the total cross section $\\sigma$ defined by the integral $\\sigma = \\int \\sigma(x) \\,\\mathrm{d}x$, where $x$ labels final-state kinematics and $\\sigma(x) \\ge 0$ is the differential cross section. Events are generated by importance sampling from a proposal density $g(x)$, and each event $x$ carries a weight $w(x) = \\sigma(x)/g(x)$. Let $w$ denote the random variable of event weights when $x \\sim g(x)$. Assume the events are independent and identically distributed with weight probability density $\\rho(w)$ supported on $[0,\\infty)$ and with finite first and second moments.\n\nConsider the following partial unweighting scheme with a fixed threshold $w_c > 0$: for an event with weight $w < w_c$, perform unweighting by accepting the event with probability $w/w_c$ and assigning it the constant weight $w_c$ upon acceptance; for an event with weight $w \\ge w_c$, retain the original weighted event without modification. Define a per-event contribution $Y$ to a combined estimator that implements this rule. For a sample of $N$ independent events with weights $(w_1,\\dots,w_N)$ and independent Bernoulli trials $(B_1,\\dots,B_N)$ used for unweighting with $\\mathbb{P}(B_i = 1 \\mid w_i) = \\min(1, w_i/w_c)$, define a combined estimator $\\hat{\\sigma}_{\\mathrm{pu}}$ of $\\sigma$ based on $(Y_1,\\dots,Y_N)$.\n\nStarting only from the definition of importance sampling, the unbiasedness of the standard MC estimator $\\hat{\\sigma} = \\frac{1}{N}\\sum_{i=1}^{N} w_i$, and the law of total expectation and variance, derive the explicit expression for the combined estimator $\\hat{\\sigma}_{\\mathrm{pu}}$ under the partial unweighting scheme and its variance $\\mathrm{Var}(\\hat{\\sigma}_{\\mathrm{pu}})$ in closed form. Express your final results in terms of $N$, $w_c$, and expectations with respect to $\\rho(w)$, such as $\\mathbb{E}[w]$, $\\mathbb{E}[w \\,\\mathbf{1}_{\\{w < w_c\\}}]$, and $\\mathbb{E}[w^{2} \\,\\mathbf{1}_{\\{w \\ge w_c\\}}]$. No numerical approximation is required. State the expressions for both the estimator and its variance as your final answer. Do not include units.", "solution": "The problem statement has been validated and is deemed sound, well-posed, and scientifically grounded. We may proceed with the solution.\n\nThe objective is to derive the explicit expression for a partial unweighting estimator $\\hat{\\sigma}_{\\mathrm{pu}}$ and its variance, $\\mathrm{Var}(\\hat{\\sigma}_{\\mathrm{pu}})$. The estimator is constructed from a set of $N$ independent and identically distributed (i.i.d.) events. For each event $i$, we have an initial weight $w_i$ and a corresponding per-event contribution $Y_i$ to the estimator.\n\nFirst, we formalize the per-event contribution $Y_i$ based on the partial unweighting scheme described. Let $w$ be the weight of a single event. The scheme depends on a fixed positive threshold $w_c$.\n- If $w < w_c$, the event is accepted with probability $p = w/w_c$. Upon acceptance, its contribution is the constant weight $w_c$. If rejected (with probability $1-p$), its contribution is $0$. This outcome can be modeled by the random variable $B w_c$, where $B$ is a Bernoulli random variable with parameter $p=w/w_c$, i.e., $\\mathbb{P}(B=1|w) = w/w_c$.\n- If $w \\ge w_c$, the event is retained with its original weight $w$. Its contribution is deterministically $w$.\n\nWe can express the contribution $Y$ for a single event with weight $w$ using indicator functions. Let $\\mathbf{1}_{\\{w < w_c\\}}$ be the indicator function for the condition $w < w_c$, and $\\mathbf{1}_{\\{w \\ge w_c\\}}$ for $w \\ge w_c$. The contribution $Y$ is a random variable defined as:\n$$Y = (B w_c) \\mathbf{1}_{\\{w < w_c\\}} + w \\mathbf{1}_{\\{w \\ge w_c\\}}$$\nwhere $B$ is a Bernoulli random variable, conditional on $w$, with $\\mathbb{P}(B=1|w) = w/w_c$. For $w \\ge w_c$, the first term is zero, and $Y=w$.\n\nThe combined estimator $\\hat{\\sigma}_{\\mathrm{pu}}$ for a sample of $N$ i.i.d. events is the sample mean of the individual contributions $Y_i$:\n$$\\hat{\\sigma}_{\\mathrm{pu}} = \\frac{1}{N} \\sum_{i=1}^{N} Y_i$$\nwhere each $Y_i$ is constructed from its corresponding weight $w_i$:\n$$Y_i = (B_i w_c) \\mathbf{1}_{\\{w_i < w_c\\}} + w_i \\mathbf{1}_{\\{w_i \\ge w_c\\}}$$\nHere, $B_i$ are independent Bernoulli variables with parameters $w_i/w_c$ for each event $i$ where $w_i < w_c$. This provides the explicit expression for the estimator.\n\nNext, we demonstrate that this estimator is unbiased for $\\sigma$. An estimator is unbiased if its expectation equals the true value. Since the $Y_i$ are i.i.d., the expectation of the estimator is:\n$$\\mathbb{E}[\\hat{\\sigma}_{\\mathrm{pu}}] = \\mathbb{E}\\left[\\frac{1}{N} \\sum_{i=1}^{N} Y_i\\right] = \\frac{1}{N} \\sum_{i=1}^{N} \\mathbb{E}[Y_i] = \\mathbb{E}[Y]$$\nWe compute $\\mathbb{E}[Y]$ using the law of total expectation: $\\mathbb{E}[Y] = \\mathbb{E}_w[\\mathbb{E}[Y|w]]$.\nFirst, we find the conditional expectation of $Y$ given a weight $w$:\n- If $w < w_c$: $Y = B w_c$.\n$$\\mathbb{E}[Y|w] = \\mathbb{E}[B w_c | w] = w_c \\mathbb{E}[B|w] = w_c \\left( \\frac{w}{w_c} \\right) = w$$\n- If $w \\ge w_c$: $Y = w$, which is a constant given $w$.\n$$\\mathbb{E}[Y|w] = \\mathbb{E}[w|w] = w$$\nIn both cases, we find that $\\mathbb{E}[Y|w] = w$. This means the procedure is unbiased for any given event weight.\nNow, we take the expectation over all possible values of $w$:\n$$\\mathbb{E}[Y] = \\mathbb{E}_w[\\mathbb{E}[Y|w]] = \\mathbb{E}_w[w]$$\nThe problem states that the standard Monte Carlo estimator $\\hat{\\sigma} = \\frac{1}{N}\\sum w_i$ is unbiased for $\\sigma = \\int \\sigma(x) dx$. This implies $\\mathbb{E}[w] = \\sigma$.\nTherefore, $\\mathbb{E}[\\hat{\\sigma}_{\\mathrm{pu}}] = \\mathbb{E}[Y] = \\mathbb{E}[w] = \\sigma$. The estimator $\\hat{\\sigma}_{\\mathrm{pu}}$ is unbiased.\n\nFinally, we derive the variance of the estimator, $\\mathrm{Var}(\\hat{\\sigma}_{\\mathrm{pu}})$. Since the $Y_i$ are i.i.d., the variance of their sum is the sum of their variances:\n$$\\mathrm{Var}(\\hat{\\sigma}_{\\mathrm{pu}}) = \\mathrm{Var}\\left(\\frac{1}{N} \\sum_{i=1}^{N} Y_i\\right) = \\frac{1}{N^2} \\sum_{i=1}^{N} \\mathrm{Var}(Y_i) = \\frac{1}{N} \\mathrm{Var}(Y)$$\nTo compute $\\mathrm{Var}(Y)$, we use the law of total variance (also known as the variance decomposition formula):\n$$\\mathrm{Var}(Y) = \\mathbb{E}_w[\\mathrm{Var}(Y|w)] + \\mathrm{Var}_w(\\mathbb{E}[Y|w])$$\nWe have already computed the inner conditional expectation $\\mathbb{E}[Y|w] = w$. The second term is thus:\n$$\\mathrm{Var}_w(\\mathbb{E}[Y|w]) = \\mathrm{Var}_w(w) = \\mathbb{E}[w^2] - (\\mathbb{E}[w])^2 = \\mathbb{E}[w^2] - \\sigma^2$$\nNow we compute the first term, $\\mathbb{E}_w[\\mathrm{Var}(Y|w)]$. First, we find the conditional variance $\\mathrm{Var}(Y|w)$:\n- If $w < w_c$: $Y = B w_c$.\n$$\\mathrm{Var}(Y|w) = \\mathrm{Var}(B w_c | w) = w_c^2 \\mathrm{Var}(B|w)$$\nFor a Bernoulli variable $B$ with parameter $p=w/w_c$, the variance is $p(1-p)$.\n$$\\mathrm{Var}(Y|w) = w_c^2 \\left(\\frac{w}{w_c}\\right) \\left(1 - \\frac{w}{w_c}\\right) = w_c w \\left(1 - \\frac{w}{w_c}\\right) = w_c w - w^2$$\n- If $w \\ge w_c$: $Y=w$, which is a constant given $w$.\n$$\\mathrm{Var}(Y|w) = 0$$\nWe can write this compactly: $\\mathrm{Var}(Y|w) = (w_c w - w^2)\\mathbf{1}_{\\{w < w_c\\}}$.\nNext, we take the expectation of this conditional variance over $w$:\n$$\\mathbb{E}_w[\\mathrm{Var}(Y|w)] = \\mathbb{E}[(w_c w - w^2)\\mathbf{1}_{\\{w < w_c\\}}] = \\mathbb{E}[w_c w \\mathbf{1}_{\\{w < w_c\\}}] - \\mathbb{E}[w^2 \\mathbf{1}_{\\{w < w_c\\}}]$$\n$$= w_c \\mathbb{E}[w \\mathbf{1}_{\\{w < w_c\\}}] - \\mathbb{E}[w^2 \\mathbf{1}_{\\{w < w_c\\}}]$$\nNow, assemble the total variance $\\mathrm{Var}(Y)$:\n$$\\mathrm{Var}(Y) = \\left( w_c \\mathbb{E}[w \\mathbf{1}_{\\{w < w_c\\}}] - \\mathbb{E}[w^2 \\mathbf{1}_{\\{w < w_c\\}}] \\right) + \\left( \\mathbb{E}[w^2] - (\\mathbb{E}[w])^2 \\right)$$\nWe can decompose the term $\\mathbb{E}[w^2]$ as $\\mathbb{E}[w^2] = \\mathbb{E}[w^2 \\mathbf{1}_{\\{w < w_c\\}}] + \\mathbb{E}[w^2 \\mathbf{1}_{\\{w \\ge w_c\\}}]$. Substituting this into the variance expression gives:\n$$\\mathrm{Var}(Y) = w_c \\mathbb{E}[w \\mathbf{1}_{\\{w < w_c\\}}] - \\mathbb{E}[w^2 \\mathbf{1}_{\\{w < w_c\\}}] + \\mathbb{E}[w^2 \\mathbf{1}_{\\{w < w_c\\}}] + \\mathbb{E}[w^2 \\mathbf{1}_{\\{w \\ge w_c\\}}] - (\\mathbb{E}[w])^2$$\nThe terms $\\mathbb{E}[w^2 \\mathbf{1}_{\\{w < w_c\\}}]$ cancel out, yielding:\n$$\\mathrm{Var}(Y) = w_c \\mathbb{E}[w \\mathbf{1}_{\\{w < w_c\\}}] + \\mathbb{E}[w^2 \\mathbf{1}_{\\{w \\ge w_c\\}}] - (\\mathbb{E}[w])^2$$\nThis expression is in the required format. The variance of the estimator $\\hat{\\sigma}_{\\mathrm{pu}}$ is therefore:\n$$\\mathrm{Var}(\\hat{\\sigma}_{\\mathrm{pu}}) = \\frac{1}{N} \\left( w_c \\mathbb{E}[w \\mathbf{1}_{\\{w < w_c\\}}] + \\mathbb{E}[w^2 \\mathbf{1}_{\\{w \\ge w_c\\}}] - (\\mathbb{E}[w])^2 \\right)$$\nThese results for the estimator and its variance are the required final expressions.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\hat{\\sigma}_{\\mathrm{pu}} = \\frac{1}{N} \\sum_{i=1}^{N} \\left( B_i w_c \\mathbf{1}_{\\{w_i < w_c\\}} + w_i \\mathbf{1}_{\\{w_i \\ge w_c\\}} \\right) & \\mathrm{Var}(\\hat{\\sigma}_{\\mathrm{pu}}) = \\frac{1}{N} \\left( w_c \\mathbb{E}[w \\mathbf{1}_{\\{w < w_c\\}}] + \\mathbb{E}[w^2 \\mathbf{1}_{\\{w \\ge w_c\\}}] - (\\mathbb{E}[w])^2 \\right)\n\\end{pmatrix}\n}\n$$", "id": "3513734"}, {"introduction": "Higher-order calculations in quantum field theory often produce events with negative weights, posing a significant challenge for statistical analysis and variance control. This practice introduces signed Poisson thinning, a method for handling such events by reinterpreting them as a random collection of unweighted positive and negative counts. By deriving the theoretical properties and implementing the simulation [@problem_id:3513801], you will gain a hands-on understanding of variance inflation and a practical tool used in modern physics simulations.", "problem": "You are given the task of formalizing and testing a signed Poisson thinning procedure for handling negative event weights in computational high-energy physics. Consider a single weighted event whose weight is decomposed as $w = w_{+} - w_{-}$ with $w_{+} \\ge 0$ and $w_{-} \\ge 0$. The goal is to represent this event by a signed unweighted sample constructed from two independent Poisson random variables and to study unbiasedness and variance properties, including the effect of negative weights on variance inflation.\n\nStarting from the following foundational facts:\n- If $N \\sim \\mathrm{Poisson}(\\lambda)$, then $\\mathbb{E}[N] = \\lambda$ and $\\mathrm{Var}[N] = \\lambda$ for any $\\lambda \\ge 0$.\n- If $X$ and $Y$ are independent random variables, then $\\mathbb{E}[X \\pm Y] = \\mathbb{E}[X] \\pm \\mathbb{E}[Y]$ and $\\mathrm{Var}[X \\pm Y] = \\mathrm{Var}[X] + \\mathrm{Var}[Y]$.\n- For any constant $c$, $\\mathbb{E}[c X] = c\\,\\mathbb{E}[X]$ and $\\mathrm{Var}[c X] = c^{2}\\,\\mathrm{Var}[X]$.\n\nDefine the signed Poisson thinning estimator for a chosen unit scale $a > 0$ as follows. Draw two independent Poisson random variables $N_{+} \\sim \\mathrm{Poisson}(\\lambda_{+})$ and $N_{-} \\sim \\mathrm{Poisson}(\\lambda_{-})$ with rates $\\lambda_{+} = w_{+}/a$ and $\\lambda_{-} = w_{-}/a$. Define the signed unweighted estimator $S = a\\,(N_{+} - N_{-})$.\n\nTasks to be addressed:\n- Derive from first principles that $S$ is an unbiased estimator of $w$, and derive its variance in terms of $w_{+}$, $w_{-}$, and $a$.\n- Define and derive a variance inflation factor that isolates the effect of sign cancellations: for $|w| > 0$, define $F = \\dfrac{w_{+} + w_{-}}{|w|}$ and interpret it as the factor by which the variance $a\\,(w_{+} + w_{-})$ exceeds the variance that would occur if the same absolute net weight $|w|$ were produced using only nonnegative weights, namely $a\\,|w|$. For the special case $|w| = 0$, specify that the inflation factor is undefined and must be reported via a sentinel as described below.\n\nProgramming task:\n- Implement a program that, for a given test suite, simulates the signed thinning estimator $S$ and compares empirical mean and variance with their derived theoretical values. For each test case, use $R$ independent repetitions to generate independent samples of $S$, and compute the empirical mean $\\hat{\\mu}$ and empirical variance $\\hat{v}$ using population normalization. The theoretical mean is $\\mu = w_{+} - w_{-}$ and the theoretical variance is $v = a\\,(w_{+} + w_{-})$.\n- For $|w| > 0$, compute the empirical inflation factor $\\hat{F} = \\hat{v}/(a\\,|w|)$ and the theoretical $F = (w_{+} + w_{-})/|w|$. For $|w| = 0$, do not compute an inflation factor; instead, output the sentinel value $-1.0$ for the inflation-factor error as specified below.\n- For each test case, output the triple of differences\n  $(\\Delta_{\\mu}, \\Delta_{v}, \\Delta_{F}) = \\big(\\hat{\\mu} - \\mu,\\; \\hat{v} - v,\\; \\hat{F} - F\\big)$,\n  where for $|w| = 0$ you must output $\\Delta_{F} = -1.0$ by convention.\n\nSimulation details:\n- For each test case, generate $R$ independent pairs $(N_{+}, N_{-})$ with the specified rates, form $S = a\\,(N_{+} - N_{-})$ for each repeat, and compute $\\hat{\\mu}$ and $\\hat{v}$ using population normalization. Use independent random number generator seeds per test case to ensure reproducibility.\n\nTest suite:\n- The program must use the following test cases, each specified as a quintuple $(w_{+}, w_{-}, a, R, \\text{seed})$:\n  - Case A: $(3.0, 0.0, 1.2, 1000000, 12345)$\n  - Case B: $(0.0, 2.5, 0.8, 1000000, 271828)$\n  - Case C: $(5.0, 5.0, 1.0, 1000000, 314159)$\n  - Case D: $(4.0, 1.5, 0.7, 1000000, 1618033)$\n\nRequired final output format:\n- Your program should produce a single line of output containing the concatenated list of results from all test cases as a comma-separated list enclosed in square brackets. The results must be ordered as $[\\Delta_{\\mu}^{A}, \\Delta_{v}^{A}, \\Delta_{F}^{A}, \\Delta_{\\mu}^{B}, \\Delta_{v}^{B}, \\Delta_{F}^{B}, \\Delta_{\\mu}^{C}, \\Delta_{v}^{C}, \\Delta_{F}^{C}, \\Delta_{\\mu}^{D}, \\Delta_{v}^{D}, \\Delta_{F}^{D}]$ with each value rendered as a floating-point number. For the inflation-factor error in cases with $|w| = 0$, output the sentinel value $-1.0$.", "solution": "The problem statement is valid. It is scientifically grounded in probability theory and statistics, well-posed with all necessary information provided, and objective in its formulation. It represents a formalizable and relevant problem in computational high-energy physics concerning the statistical treatment of weighted Monte Carlo events. We will now proceed with the solution.\n\nThe solution consists of two parts: first, a theoretical derivation of the statistical properties of the proposed estimator, and second, a numerical implementation to verify these derivations.\n\n### Theoretical Derivations\n\nThe problem defines a signed unweighted estimator $S$ for a weighted event with weight $w = w_{+} - w_{-}$, where $w_{+} \\ge 0$ and $w_{-} \\ge 0$. The estimator is constructed using a unit scale $a > 0$ and two independent Poisson random variables, $N_{+} \\sim \\mathrm{Poisson}(\\lambda_{+})$ and $N_{-} \\sim \\mathrm{Poisson}(\\lambda_{-})$, with rates $\\lambda_{+} = w_{+}/a$ and $\\lambda_{-} = w_{-}/a$. The estimator is given by $S = a\\,(N_{+} - N_{-})$.\n\n#### 1. Unbiasedness of the Estimator $S$\nAn estimator is unbiased if its expectation equals the true value of the parameter it estimates. Here, we must show that $\\mathbb{E}[S] = w$. We use the fundamental properties of expectation. By the linearity of expectation, we have:\n$$\n\\mathbb{E}[S] = \\mathbb{E}[a\\,(N_{+} - N_{-})] = a\\,\\mathbb{E}[N_{+} - N_{-}]\n$$\nSince $N_{+}$ and $N_{-}$ are independent, the expectation of their difference is the difference of their expectations:\n$$\n\\mathbb{E}[S] = a\\,(\\mathbb{E}[N_{+}] - \\mathbb{E}[N_{-}])\n$$\nThe expectation of a Poisson random variable $N \\sim \\mathrm{Poisson}(\\lambda)$ is $\\mathbb{E}[N] = \\lambda$. Applying this to $N_{+}$ and $N_{-}$:\n$$\n\\mathbb{E}[S] = a\\,(\\lambda_{+} - \\lambda_{-})\n$$\nSubstituting the given definitions for the rates, $\\lambda_{+} = w_{+}/a$ and $\\lambda_{-} = w_{-}/a$:\n$$\n\\mathbb{E}[S] = a\\,\\left(\\frac{w_{+}}{a} - \\frac{w_{-}}{a}\\right) = a\\,\\left(\\frac{w_{+} - w_{-}}{a}\\right) = w_{+} - w_{-}\n$$\nBy definition, $w = w_{+} - w_{-}$, so we have demonstrated that $\\mathbb{E}[S] = w$. Thus, $S$ is an unbiased estimator of $w$.\n\n#### 2. Variance of the Estimator $S$\nNext, we derive the variance of $S$, denoted as $\\mathrm{Var}(S)$. We use the property $\\mathrm{Var}(cX) = c^2\\,\\mathrm{Var}(X)$ for any constant $c$:\n$$\n\\mathrm{Var}(S) = \\mathrm{Var}(a\\,(N_{+} - N_{-})) = a^2\\,\\mathrm{Var}(N_{+} - N_{-})\n$$\nFor independent random variables $X$ and $Y$, the variance of their sum or difference is the sum of their variances: $\\mathrm{Var}(X \\pm Y) = \\mathrm{Var}(X) + \\mathrm{Var}(Y)$. Since $N_{+}$ and $N_{-}$ are independent:\n$$\n\\mathrm{Var}(S) = a^2\\,(\\mathrm{Var}(N_{+}) + \\mathrm{Var}(N_{-}))\n$$\nThe variance of a Poisson random variable $N \\sim \\mathrm{Poisson}(\\lambda)$ is $\\mathrm{Var}(N) = \\lambda$. Applying this property:\n$$\n\\mathrm{Var}(S) = a^2\\,(\\lambda_{+} + \\lambda_{-})\n$$\nSubstituting the definitions for the rates $\\lambda_{+}$ and $\\lambda_{-}$:\n$$\n\\mathrm{Var}(S) = a^2\\,\\left(\\frac{w_{+}}{a} + \\frac{w_{-}}{a}\\right) = a^2\\,\\left(\\frac{w_{+} + w_{-}}{a}\\right) = a\\,(w_{+} + w_{-})\n$$\nThis is the theoretical variance of the estimator, $v = a\\,(w_{+} + w_{-})$, as stated in the problem.\n\n#### 3. Variance Inflation Factor $F$\nThe variance inflation factor is defined for $|w| > 0$ as $F = \\dfrac{w_{+} + w_{-}}{|w|}$. To interpret this, we compare the actual variance $v = a\\,(w_{+} + w_{-})$ with the variance of an idealized estimator that achieves the same net weight magnitude $|w|$ but without any cancellations from negative weights.\n\nIn such an ideal case, the total weight magnitude $|w|$ would be constructed entirely from positive weights (or entirely from negative weights, which is equivalent in terms of variance). This corresponds to a weight decomposition $w'_{\\text{total}} = w'_+ + w'_- = |w|$, where one component is zero. The variance of the estimator in this ideal scenario, $v_{\\text{ideal}}$, would be:\n$$\nv_{\\text{ideal}} = a\\,(w'_+ + w'_-) = a\\,|w|\n$$\nThis represents the minimal variance achievable for a net weight of magnitude $|w|$ using this estimation scheme.\n\nThe ratio of the actual variance to this ideal minimum variance is:\n$$\n\\frac{v}{v_{\\text{ideal}}} = \\frac{a\\,(w_{+} + w_{-})}{a\\,|w|} = \\frac{w_{+} + w_{-}}{|w|} = F\n$$\nThus, $F$ is precisely the factor by which the variance is inflated due to the simultaneous presence of positive and negative weight components ($w_{+} > 0$ and $w_{-} > 0$), which lead to cancellations in the net weight $w$. By the triangle inequality, $w_{+} + w_{-} \\ge |w_{+} - w_{-}| = |w|$, which implies $F \\ge 1$. The factor $F$ is equal to $1$ if and only if either $w_{+}$ or $w_{-}$ is zero (no cancellation), and $F > 1$ when both are positive, indicating variance inflation.\n\nFor the case $|w| = 0$, which occurs when $w_{+} = w_{-}$, the denominator of $F$ is $0$, making the factor undefined. The problem correctly specifies that this case must be handled with a sentinel value. In this situation, the variance is $v = a\\,(w_{+} + w_{-}) = 2aw_{+}$, which is non-zero, while the net weight is zero, leading to an infinite relative uncertainty.\n\nThe following Python program implements the simulation to numerically verify these theoretical findings.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Simulates a signed Poisson thinning procedure for several test cases,\n    compares empirical and theoretical statistics, and outputs the differences.\n    \"\"\"\n    # Test cases are given as quintuples: (w+, w-, a, R, seed)\n    test_cases = [\n        # Case A\n        (3.0, 0.0, 1.2, 1000000, 12345),\n        # Case B\n        (0.0, 2.5, 0.8, 1000000, 271828),\n        # Case C\n        (5.0, 5.0, 1.0, 1000000, 314159),\n        # Case D\n        (4.0, 1.5, 0.7, 1000000, 1618033),\n    ]\n\n    results = []\n    for w_plus, w_minus, a, R, seed in test_cases:\n        # Create a dedicated random number generator for reproducibility.\n        rng = np.random.default_rng(seed)\n\n        # 1. Theoretical calculations\n        w_net = w_plus - w_minus\n        mu_th = w_net\n        v_th = a * (w_plus + w_minus)\n\n        # 2. Simulation\n        # Define Poisson rates\n        lambda_plus = w_plus / a\n        lambda_minus = w_minus / a\n\n        # Generate R independent Poisson-distributed random numbers\n        N_plus_samples = rng.poisson(lam=lambda_plus, size=R)\n        N_minus_samples = rng.poisson(lam=lambda_minus, size=R)\n\n        # Construct samples of the estimator S\n        S_samples = a * (N_plus_samples - N_minus_samples)\n\n        # 3. Compute empirical statistics\n        # Empirical mean\n        mu_emp = np.mean(S_samples)\n        \n        # Empirical variance using population normalization (ddof=0)\n        v_emp = np.var(S_samples, ddof=0)\n\n        # 4. Calculate differences\n        delta_mu = mu_emp - mu_th\n        delta_v = v_emp - v_th\n\n        # Handle the inflation factor F\n        # Use np.isclose for robust floating-point comparison to zero\n        if not np.isclose(w_net, 0.0):\n            # Theoretical inflation factor\n            F_th = (w_plus + w_minus) / abs(w_net)\n            \n            # Empirical inflation factor\n            F_emp = v_emp / (a * abs(w_net))\n            \n            delta_F = F_emp - F_th\n        else:\n            # As per problem specification, use sentinel value for delta_F when w=0\n            delta_F = -1.0\n        \n        results.extend([delta_mu, delta_v, delta_F])\n    \n    # Format and print the final output in the required single-line format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3513801"}]}