{"hands_on_practices": [{"introduction": "The Kalman filter is the engine of modern track reconstruction. To truly understand how it works, it is essential to move beyond the abstract equations and apply them to a concrete example. This exercise [@problem_id:3539735] guides you through a single prediction-update cycle, the fundamental building block of the filter, allowing you to calculate the updated track state and its uncertainty by hand and build intuition for how new information is incorporated.", "problem": "A charged-particle track is reconstructed across two silicon detector layers in a collider experiment. Within a small step from a previous surface to the current measurement plane, adopt the linearized state-space track model with state vector $\\mathbf{x} = (u, t)^{\\mathsf{T}}$, where $u$ is the local transverse position in $\\mathrm{cm}$ and $t = \\mathrm{d}u/\\mathrm{d}z$ is the local slope (dimensionless). The process model is linear with a transport matrix $F$ and process noise covariance $Q$ representing small multiple-scattering fluctuations. The measurement at the current plane is a single hit coordinate $y$ related linearly to the state through a measurement matrix $H$, with measurement noise covariance $R$. All noises are assumed zero-mean Gaussian, independent, and with the stated covariances. You are to perform one prediction-update cycle from the previous filtered state to incorporate the hit.\n\nUse the following numerical inputs:\n- Previous filtered state and covariance: $\\mathbf{x}_{0} = \\begin{pmatrix} 0.05 \\\\ 0.002 \\end{pmatrix}$, $P_{0} = \\begin{pmatrix} 1.0 \\times 10^{-4} & 0 \\\\ 0 & 1.0 \\times 10^{-6} \\end{pmatrix}$.\n- Transport matrix and process noise covariance: $F = \\begin{pmatrix} 1 & 20 \\\\ 0 & 1 \\end{pmatrix}$, $Q = \\begin{pmatrix} 1.0 \\times 10^{-5} & 0 \\\\ 0 & 1.0 \\times 10^{-6} \\end{pmatrix}$.\n- Measurement model and noise covariance: $H = \\begin{pmatrix} 1 & 0 \\end{pmatrix}$, $R = 4.0 \\times 10^{-4}$.\n- Measured hit coordinate: $y = 0.115$ (in $\\mathrm{cm}$).\n\nStarting from first principles of the Bayesian linear-Gaussian model for state propagation and measurement, derive and compute:\n1. The predicted state $\\mathbf{x}^{-}$ and predicted covariance $P^{-}$.\n2. The measurement residual $r$, the innovation covariance $S$, and the optimal linear gain (Kalman gain) $K$ for the update.\n3. The updated state $\\mathbf{x}^{+}$ and updated covariance $P^{+}$ after incorporating the hit.\n4. The incremental chi-squared $\\Delta \\chi^{2}$ contributed by this hit.\n\nReport the incremental chi-squared $\\Delta \\chi^{2}$ as your final numerical answer. Round your answer to four significant figures. Since $\\Delta \\chi^{2}$ is dimensionless, no unit should be reported for the final answer.", "solution": "The problem requires performing a single prediction-update cycle of a linear Kalman filter to incorporate a new measurement into a charged-particle track's state estimate. The process is grounded in the principles of Bayesian inference for linear-Gaussian systems. We will proceed by first validating the problem statement and then systematically deriving and computing the requested quantities.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- Previous filtered state vector: $\\mathbf{x}_{0} = \\begin{pmatrix} 0.05 \\\\ 0.002 \\end{pmatrix}$\n- Previous filtered covariance matrix: $P_{0} = \\begin{pmatrix} 1.0 \\times 10^{-4} & 0 \\\\ 0 & 1.0 \\times 10^{-6} \\end{pmatrix}$\n- Transport matrix: $F = \\begin{pmatrix} 1 & 20 \\\\ 0 & 1 \\end{pmatrix}$\n- Process noise covariance matrix: $Q = \\begin{pmatrix} 1.0 \\times 10^{-5} & 0 \\\\ 0 & 1.0 \\times 10^{-6} \\end{pmatrix}$\n- Measurement matrix: $H = \\begin{pmatrix} 1 & 0 \\end{pmatrix}$\n- Measurement noise covariance (scalar): $R = 4.0 \\times 10^{-4}$\n- Measured hit coordinate (scalar): $y = 0.115$\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem describes a standard application of the Kalman filter for track reconstruction in high-energy physics. The linear state-space model is a common and valid approximation for small propagation steps. The physical interpretation of the state vector components ($u$ for position, $t$ for slope) and the matrices ($F$ for linear transport, $Q$ for multiple scattering, $H$ for a position measurement, $R$ for sensor resolution) are all standard in the field. The numerical values are physically plausible for a silicon detector system.\n- **Well-Posed:** The problem provides a complete set of initial conditions and model parameters required to execute one cycle of the Kalman filter. The calculations lead to a unique and stable solution.\n- **Objective:** The problem is stated using precise mathematical and physical terminology, free of ambiguity or subjective claims.\n\n**Step 3: Verdict and Action**\nThe problem is valid. It is a well-posed, scientifically grounded exercise in applying the Kalman filter algorithm. We will proceed with the solution.\n\n### Derivation and Computation\n\nThe Kalman filter cycle consists of two main steps: a prediction step (propagating the state to the new measurement surface) and an update step (correcting the state using the new measurement).\n\n**1. Predicted State and Covariance**\n\nThe state is propagated from the previous surface to the current measurement plane using the transport matrix $F$. The process noise $Q$ accounts for uncertainties added during this transport, such as from multiple scattering.\n\nThe predicted (a priori) state $\\mathbf{x}^{-}$ is the expectation of the propagated state:\n$$\n\\mathbf{x}^{-} = F \\mathbf{x}_{0}\n$$\nThe predicted (a priori) covariance $P^{-}$ is the sum of the propagated previous covariance and the process noise covariance:\n$$\nP^{-} = F P_{0} F^{\\mathsf{T}} + Q\n$$\nSubstituting the given values:\n$$\n\\mathbf{x}^{-} = \\begin{pmatrix} 1 & 20 \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} 0.05 \\\\ 0.002 \\end{pmatrix} = \\begin{pmatrix} 1 \\cdot (0.05) + 20 \\cdot (0.002) \\\\ 0 \\cdot (0.05) + 1 \\cdot (0.002) \\end{pmatrix} = \\begin{pmatrix} 0.05 + 0.04 \\\\ 0.002 \\end{pmatrix} = \\begin{pmatrix} 0.09 \\\\ 0.002 \\end{pmatrix}\n$$\nFor the covariance:\n$$\nF P_{0} F^{\\mathsf{T}} = \\begin{pmatrix} 1 & 20 \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} 1.0 \\times 10^{-4} & 0 \\\\ 0 & 1.0 \\times 10^{-6} \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 20 & 1 \\end{pmatrix}\n$$\n$$\n= \\begin{pmatrix} 1.0 \\times 10^{-4} & 2.0 \\times 10^{-5} \\\\ 0 & 1.0 \\times 10^{-6} \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 20 & 1 \\end{pmatrix}\n$$\n$$\n= \\begin{pmatrix} (1.0 \\times 10^{-4}) + (2.0 \\times 10^{-5})(20) & 2.0 \\times 10^{-5} \\\\ 1.0 \\times 10^{-6}(20) & 1.0 \\times 10^{-6} \\end{pmatrix}\n$$\n$$\n= \\begin{pmatrix} 1.0 \\times 10^{-4} + 4.0 \\times 10^{-4} & 2.0 \\times 10^{-5} \\\\ 2.0 \\times 10^{-5} & 1.0 \\times 10^{-6} \\end{pmatrix} = \\begin{pmatrix} 5.0 \\times 10^{-4} & 2.0 \\times 10^{-5} \\\\ 2.0 \\times 10^{-5} & 1.0 \\times 10^{-6} \\end{pmatrix}\n$$\nNow, adding the process noise covariance $Q$:\n$$\nP^{-} = \\begin{pmatrix} 5.0 \\times 10^{-4} & 2.0 \\times 10^{-5} \\\\ 2.0 \\times 10^{-5} & 1.0 \\times 10^{-6} \\end{pmatrix} + \\begin{pmatrix} 1.0 \\times 10^{-5} & 0 \\\\ 0 & 1.0 \\times 10^{-6} \\end{pmatrix}\n$$\n$$\nP^{-} = \\begin{pmatrix} 5.1 \\times 10^{-4} & 2.0 \\times 10^{-5} \\\\ 2.0 \\times 10^{-5} & 2.0 \\times 10^{-6} \\end{pmatrix}\n$$\n\n**2. Measurement Residual, Innovation Covariance, and Kalman Gain**\n\nThe measurement residual $r$ (or innovation) is the difference between the actual measurement $y$ and the predicted measurement $H \\mathbf{x}^{-}$. The innovation covariance $S$ is the variance of this residual. The Kalman gain $K$ is the optimal weight for combining the prediction with the residual.\n\nThe residual $r$ is:\n$$\nr = y - H \\mathbf{x}^{-}\n$$\nThe innovation covariance $S$ is:\n$$\nS = H P^{-} H^{\\mathsf{T}} + R\n$$\nThe Kalman gain $K$ is:\n$$\nK = P^{-} H^{\\mathsf{T}} S^{-1}\n$$\nSubstituting values:\n$$\nr = 0.115 - \\begin{pmatrix} 1 & 0 \\end{pmatrix} \\begin{pmatrix} 0.09 \\\\ 0.002 \\end{pmatrix} = 0.115 - 0.09 = 0.025\n$$\nFor the innovation covariance $S$ (which is a scalar in this case):\n$$\nH P^{-} H^{\\mathsf{T}} = \\begin{pmatrix} 1 & 0 \\end{pmatrix} \\begin{pmatrix} 5.1 \\times 10^{-4} & 2.0 \\times 10^{-5} \\\\ 2.0 \\times 10^{-5} & 2.0 \\times 10^{-6} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 5.1 \\times 10^{-4} & 2.0 \\times 10^{-5} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = 5.1 \\times 10^{-4}\n$$\n$$\nS = H P^{-} H^{\\mathsf{T}} + R = (5.1 \\times 10^{-4}) + (4.0 \\times 10^{-4}) = 9.1 \\times 10^{-4}\n$$\nFor the Kalman gain $K$ (which is a $2 \\times 1$ vector):\n$$\nP^{-} H^{\\mathsf{T}} = \\begin{pmatrix} 5.1 \\times 10^{-4} & 2.0 \\times 10^{-5} \\\\ 2.0 \\times 10^{-5} & 2.0 \\times 10^{-6} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 5.1 \\times 10^{-4} \\\\ 2.0 \\times 10^{-5} \\end{pmatrix}\n$$\n$$\nK = P^{-} H^{\\mathsf{T}} S^{-1} = \\begin{pmatrix} 5.1 \\times 10^{-4} \\\\ 2.0 \\times 10^{-5} \\end{pmatrix} (9.1 \\times 10^{-4})^{-1} = \\frac{1}{9.1 \\times 10^{-4}} \\begin{pmatrix} 5.1 \\times 10^{-4} \\\\ 2.0 \\times 10^{-5} \\end{pmatrix} = \\frac{1}{9.1} \\begin{pmatrix} 5.1 \\\\ 0.2 \\end{pmatrix}\n$$\n\n**3. Updated State and Covariance**\n\nThe updated (a posteriori) state $\\mathbf{x}^{+}$ is the predicted state corrected by the weighted residual. The updated covariance $P^{+}$ is the predicted covariance reduced by the information gained from the measurement.\n\nThe updated state $\\mathbf{x}^{+}$ is:\n$$\n\\mathbf{x}^{+} = \\mathbf{x}^{-} + K r\n$$\nThe updated covariance $P^{+}$ is:\n$$\nP^{+} = (I - K H) P^{-}\n$$\nSubstituting values:\n$$\n\\mathbf{x}^{+} = \\begin{pmatrix} 0.09 \\\\ 0.002 \\end{pmatrix} + \\frac{1}{9.1} \\begin{pmatrix} 5.1 \\\\ 0.2 \\end{pmatrix} (0.025) = \\begin{pmatrix} 0.09 \\\\ 0.002 \\end{pmatrix} + \\begin{pmatrix} \\frac{0.1275}{9.1} \\\\ \\frac{0.005}{9.1} \\end{pmatrix} \\approx \\begin{pmatrix} 0.09 + 0.01401 \\\\ 0.002 + 0.000549 \\end{pmatrix} = \\begin{pmatrix} 0.10401 \\\\ 0.002549 \\end{pmatrix}\n$$\nAnd for the covariance:\n$$\nK H = \\frac{1}{9.1} \\begin{pmatrix} 5.1 \\\\ 0.2 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\end{pmatrix} = \\frac{1}{9.1} \\begin{pmatrix} 5.1 & 0 \\\\ 0.2 & 0 \\end{pmatrix}\n$$\n$$\nI - K H = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} - \\frac{1}{9.1} \\begin{pmatrix} 5.1 & 0 \\\\ 0.2 & 0 \\end{pmatrix} = \\begin{pmatrix} 1 - \\frac{5.1}{9.1} & 0 \\\\ -\\frac{0.2}{9.1} & 1 \\end{pmatrix} = \\begin{pmatrix} \\frac{4.0}{9.1} & 0 \\\\ -\\frac{0.2}{9.1} & 1 \\end{pmatrix}\n$$\n$$\nP^{+} = \\begin{pmatrix} \\frac{4.0}{9.1} & 0 \\\\ -\\frac{0.2}{9.1} & 1 \\end{pmatrix} \\begin{pmatrix} 5.1 \\times 10^{-4} & 2.0 \\times 10^{-5} \\\\ 2.0 \\times 10^{-5} & 2.0 \\times 10^{-6} \\end{pmatrix} \\approx \\begin{pmatrix} 2.24 \\times 10^{-4} & 8.79 \\times 10^{-6} \\\\ 8.79 \\times 10^{-6} & 1.56 \\times 10^{-6} \\end{pmatrix}\n$$\n\n**4. Incremental Chi-Squared**\n\nThe incremental chi-squared, $\\Delta \\chi^2$, quantifies the compatibility of the measurement with the predicted state. It is the squared Mahalanobis distance of the residual, normalized by its covariance. For a scalar measurement, this is:\n$$\n\\Delta \\chi^{2} = r^{\\mathsf{T}} S^{-1} r = \\frac{r^2}{S}\n$$\nUsing the previously computed values:\n$$\nr = 0.025\n$$\n$$\nS = 9.1 \\times 10^{-4}\n$$\n$$\n\\Delta \\chi^{2} = \\frac{(0.025)^2}{9.1 \\times 10^{-4}} = \\frac{6.25 \\times 10^{-4}}{9.1 \\times 10^{-4}} = \\frac{6.25}{9.1} \\approx 0.6868131868...\n$$\nRounding to four significant figures, the incremental chi-squared contributed by this hit is $0.6868$.", "answer": "$$\n\\boxed{0.6868}\n$$", "id": "3539735"}, {"introduction": "A filter is only as good as its underlying model. In realistic scenarios, model parameters like measurement noise are not perfectly known, and predictions can be systematically biased. This practice [@problem_id:3539718] explores the crucial topic of filter robustness, asking you to derive how prior biases propagate and how innovation statistics can be used to create an online \"plug-in\" estimator for the true measurement noise, a key technique in adaptive filtering.", "problem": "Consider a single-parameter track model in a high-energy collider detector where the state at discrete time index $k$ is a scalar $x_{k}$ and the measurement is $z_{k}$. The dynamics are dominated by multiple scattering that is modeled as a mean-zero perturbation, so the one-step prediction for the state has mean $\\hat{x}_{k}^{-}$ and variance $P^{-}$, where $P^{-} > 0$ is assumed stationary over the hit sequence. The measurement model is $z_{k} = x_{k} + v_{k}$ with $v_{k} \\sim \\mathcal{N}(0, \\sigma^{2})$ independent of all past states and predictions. A standard linear-Gaussian Kalman Filter (KF) update is applied with a possibly mis-specified measurement noise variance $\\tau^{2} \\neq \\sigma^{2}$, yielding the scalar Kalman gain $K = \\frac{P^{-}}{P^{-} + \\tau^{2}}$ and posterior mean $\\hat{x}_{k}^{+} = \\hat{x}_{k}^{-} + K\\,(z_{k} - \\hat{x}_{k}^{-})$. In track reconstruction, small systematic modeling errors can render the prior mean biased; let the prior mean bias be $b^{-} = \\mathbb{E}[\\hat{x}_{k}^{-} - x_{k}]$, which may be nonzero due to, for example, imperfect magnetic field maps or unmodeled material.\n\nTask (derivation from first principles):\n\n(1) Starting from the KF update equation and the basic properties of expectations and variances for independent Gaussian random variables, derive the posterior mean bias $\\mathbb{E}[\\hat{x}_{k}^{+} - x_{k}]$ in terms of $b^{-}$ and $K$. Explain how the choice of $\\tau^{2}$ modifies $K$ and thereby the magnitude of the posterior bias. Your derivation must explicitly show the role of $\\tau^{2}$ in the shrinkage toward the prior.\n\n(2) Define the innovation residual $r_{k} = z_{k} - \\hat{x}_{k}^{-}$. Under the true measurement noise variance $\\sigma^{2}$ and the stated independence assumptions, derive $\\operatorname{Var}(r_{k})$ in terms of $P^{-}$ and $\\sigma^{2}$. Using only residual statistics available online during reconstruction, propose a plug-in estimator $\\hat{\\sigma}^{2}$ for $\\sigma^{2}$ based on $N$ innovations $\\{r_{1}, r_{2}, \\dots, r_{N}\\}$ and the known stationary $P^{-}$. Your estimator must correct for any nonzero residual mean and be expressible as a closed-form analytic expression that uses the sample residual variance. Express the final estimator as a single analytic expression in terms of $N$, $\\{r_{k}\\}$, and $P^{-}$.\n\nAnswer specification: The final answer must be the single closed-form analytic expression for $\\hat{\\sigma}^{2}$ requested in part (2). No numerical evaluation is required and no units are to be reported in the final answer box.", "solution": "This problem is valid. It is a well-posed and scientifically grounded exercise in Kalman filter theory, directly applicable to track reconstruction in high-energy physics. It is free of contradictions, ambiguities, or factual unsoundness. All required information is provided.\n\nThe solution is partitioned into the two tasks requested.\n\n### Part (1): Derivation of the Posterior Mean Bias\n\nThe posterior mean bias is defined as the expected value of the difference between the posterior state estimate $\\hat{x}_{k}^{+}$ and the true state $x_{k}$. We begin our derivation from this definition, $b^{+} = \\mathbb{E}[\\hat{x}_{k}^{+} - x_{k}]$.\n\nThe problem provides the Kalman filter (KF) update equation for the posterior mean:\n$$\n\\hat{x}_{k}^{+} = \\hat{x}_{k}^{-} + K(z_{k} - \\hat{x}_{k}^{-})\n$$\nwhere $\\hat{x}_{k}^{-}$ is the prior mean estimate, $z_{k}$ is the measurement, and $K$ is the Kalman gain.\n\nSubstituting this into the definition of the posterior bias:\n$$\nb^{+} = \\mathbb{E}\\left[ \\left( \\hat{x}_{k}^{-} + K(z_{k} - \\hat{x}_{k}^{-}) \\right) - x_{k} \\right]\n$$\nWe can rearrange the terms inside the expectation:\n$$\nb^{+} = \\mathbb{E}\\left[ (\\hat{x}_{k}^{-} - x_{k}) + K(z_{k} - \\hat{x}_{k}^{-}) \\right]\n$$\nBy the linearity of the expectation operator, we can write:\n$$\nb^{+} = \\mathbb{E}[\\hat{x}_{k}^{-} - x_{k}] + K \\cdot \\mathbb{E}[z_{k} - \\hat{x}_{k}^{-}]\n$$\nThe first term is the definition of the prior mean bias, given as $b^{-} = \\mathbb{E}[\\hat{x}_{k}^{-} - x_{k}]$. The second term involves the expected value of the innovation residual, $r_{k} = z_{k} - \\hat{x}_{k}^{-}$. We must evaluate this expectation.\n\nUsing the given measurement model, $z_{k} = x_{k} + v_{k}$, where $v_{k}$ is a zero-mean noise term ($\\mathbb{E}[v_{k}] = 0$):\n$$\n\\mathbb{E}[z_{k} - \\hat{x}_{k}^{-}] = \\mathbb{E}[(x_{k} + v_{k}) - \\hat{x}_{k}^{-}]\n$$\nRearranging and applying the linearity of expectation:\n$$\n\\mathbb{E}[z_{k} - \\hat{x}_{k}^{-}] = \\mathbb{E}[v_{k}] - \\mathbb{E}[\\hat{x}_{k}^{-} - x_{k}]\n$$\nSubstituting the known expectations, $\\mathbb{E}[v_{k}]=0$ and $\\mathbb{E}[\\hat{x}_{k}^{-} - x_{k}] = b^{-}$:\n$$\n\\mathbb{E}[z_{k} - \\hat{x}_{k}^{-}] = 0 - b^{-} = -b^{-}\n$$\nNow, we substitute this result back into the expression for the posterior bias $b^{+}$:\n$$\nb^{+} = b^{-} + K(-b^{-})\n$$\nFactoring out $b^{-}$ yields the final expression for the posterior mean bias:\n$$\nb^{+} = (1 - K) b^{-}\n$$\nThis result shows that the posterior bias $b^{+}$ is the prior bias $b^{-}$ scaled by a factor of $(1-K)$. The term $(1-K)$ acts as a shrinkage factor.\n\nThe role of the mis-specified measurement noise variance, $\\tau^2$, is to control this shrinkage. The Kalman gain $K$ is given by:\n$$\nK = \\frac{P^{-}}{P^{-} + \\tau^{2}}\n$$\nwhere $P^{-}$ is the stationary prior error variance. Since $P^{-} > 0$ and $\\tau^2$ must be non-negative, the gain $K$ is always in the range $0 \\le K \\le 1$.\n\n-   If we choose a very large $\\tau^2 \\to \\infty$, we are telling the filter that the measurement $z_k$ is extremely unreliable. This causes the Kalman gain $K \\to 0$. In this limit, the shrinkage factor $(1-K) \\to 1$, and the posterior bias becomes $b^{+} \\to b^{-}$. The filter effectively ignores the measurement, and the bias from the prior estimate is fully propagated to the posterior estimate.\n\n-   If we choose a very small $\\tau^2 \\to 0$, we are telling the filter that the measurement is extremely precise. This causes the Kalman gain $K \\to 1$. In this limit, the shrinkage factor $(1-K) \\to 0$, and the posterior bias becomes $b^{+} \\to 0$. The filter heavily weights the new measurement, which is itself unbiased ($\\mathbb{E}[z_k - x_k] = \\mathbb{E}[v_k] = 0$), thus effectively correcting the bias of the prior.\n\nTherefore, the choice of $\\tau^2$ directly modulates the Kalman gain $K$, which determines the extent to which the filter trusts the new measurement. This trust level, in turn, dictates how much of the initial prior bias $b^{-}$ is corrected or \"shrunk\" in the posterior estimate.\n\n### Part (2): Derivation of the Estimator for $\\sigma^2$\n\nFirst, we derive the variance of the innovation residual, $\\operatorname{Var}(r_k)$, where $r_{k} = z_{k} - \\hat{x}_{k}^{-}$. We must use the true measurement noise variance $\\sigma^2$.\n\nThe definition of variance is $\\operatorname{Var}(X) = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2$. From Part (1), we know the mean of the residual is $\\mathbb{E}[r_k] = -b^{-}$.\n\nTo find the variance, we first express the residual in terms of fundamental quantities.\n$$\nr_{k} = z_{k} - \\hat{x}_{k}^{-} = (x_{k} + v_{k}) - \\hat{x}_{k}^{-} = v_{k} - (\\hat{x}_{k}^{-} - x_{k})\n$$\nThe problem states that the measurement noise $v_k$ is independent of all past states and predictions. This implies $v_k$ is independent of $x_k$ and $\\hat{x}_k^{-}$, and therefore independent of the prior error term $(\\hat{x}_{k}^{-} - x_{k})$. For two independent random variables $A$ and $B$, $\\operatorname{Var}(A - B) = \\operatorname{Var}(A) + \\operatorname{Var}(B)$. Applying this property:\n$$\n\\operatorname{Var}(r_k) = \\operatorname{Var}(v_k - (\\hat{x}_{k}^{-} - x_{k})) = \\operatorname{Var}(v_k) + \\operatorname{Var}(\\hat{x}_{k}^{-} - x_{k})\n$$\nThe variance of the measurement noise is given as $\\operatorname{Var}(v_k) = \\sigma^2$. The variance of the prior error, $(\\hat{x}_{k}^{-} - x_{k})$, is $P^{-}$ (as it represents the random fluctuations from effects like multiple scattering, separate from the systematic bias $b^{-}$). Adding a constant bias $b^{-}$ to a random variable does not change its variance, so $\\operatorname{Var}(\\hat{x}_{k}^{-} - x_{k}) = P^{-}$.\n\nTherefore, the variance of the innovation is:\n$$\n\\operatorname{Var}(r_k) = \\sigma^2 + P^{-}\n$$\nThis equation provides a direct relationship between the true measurement noise variance $\\sigma^2$, the known prior variance $P^{-}$, and the variance of the innovation residuals, $\\operatorname{Var}(r_k)$. We can rearrange it to express $\\sigma^2$:\n$$\n\\sigma^2 = \\operatorname{Var}(r_k) - P^{-}\n$$\nThis is a method of moments equation. To create a plug-in estimator for $\\sigma^2$, we must estimate $\\operatorname{Var}(r_k)$ from the sample of $N$ innovations, $\\{r_1, r_2, \\dots, r_N\\}$.\n\nSince the residual mean $\\mathbb{E}[r_k] = -b^{-}$ may be non-zero, we must use an estimator for variance that is robust to an unknown mean. The standard unbiased estimator for the variance of a population, given a sample of size $N$, is the sample variance, $S_r^2$:\n$$\nS_r^2 = \\frac{1}{N-1} \\sum_{k=1}^{N} (r_k - \\bar{r})^2\n$$\nwhere $\\bar{r}$ is the sample mean of the residuals, calculated as:\n$$\n\\bar{r} = \\frac{1}{N} \\sum_{j=1}^{N} r_j\n$$\nThis estimator for $\\operatorname{Var}(r_k)$ intrinsically corrects for the non-zero mean of the residuals by subtracting the sample mean $\\bar{r}$ from each observation.\n\nBy plugging our estimator for $\\operatorname{Var}(r_k)$ into the rearranged equation, we obtain the plug-in estimator $\\hat{\\sigma}^2$ for the true measurement variance $\\sigma^2$:\n$$\n\\hat{\\sigma}^2 = S_r^2 - P^{-} = \\frac{1}{N-1} \\sum_{k=1}^{N} (r_k - \\bar{r})^2 - P^{-}\n$$\nSubstituting the expression for the sample mean $\\bar{r}$, we arrive at the final single analytic expression for the estimator in terms of $N$, $\\{r_k\\}$, and $P^{-}$:\n$$\n\\hat{\\sigma}^2 = \\frac{1}{N-1} \\sum_{k=1}^{N} \\left(r_k - \\frac{1}{N}\\sum_{j=1}^{N} r_j\\right)^2 - P^{-}\n$$\nThis estimator uses the sample variance of the observed residuals, which is computable from the data stream, and the known prior variance $P^{-}$ to provide an estimate of the true underlying measurement noise variance $\\sigma^2$.", "answer": "$$\n\\boxed{\\frac{1}{N-1} \\sum_{k=1}^{N} \\left(r_k - \\frac{1}{N}\\sum_{j=1}^{N} r_j\\right)^2 - P^{-}}\n$$", "id": "3539718"}, {"introduction": "Track reconstruction in a high-luminosity collider is not just a filtering problem; it is a monumental combinatorial puzzle. This practice [@problem_id:3539732] addresses the challenge of data association: assigning the correct hits to the correct tracks in a dense environment with spurious signals. You will implement a state-of-the-art probabilistic framework to calculate association likelihoods and use them to find the globally optimal assignment, translating Bayesian reasoning into a solvable computational task.", "problem": "You are given a set of candidate particle tracks extrapolated to a single detector layer and a set of measured hits in the same layer. For each candidate track, you have a predicted two-dimensional position and a covariance that represents the extrapolation uncertainty. The detector provides two-dimensional hit positions, and the measurement noise covariance is known. There is also a uniform clutter model for spurious hits described by a Poisson Point Process (PPP). Your task is to compute association probabilities between tracks and hits based on a principled probabilistic model and then select a one-to-one assignment of hits to tracks under a Maximum a Posteriori (MAP) rule.\n\nFundamental base and physical modeling:\n- Assume the measurement model is a Multivariate Normal (MVN): conditional on a track with predicted position $\\mathbf{x}_i \\in \\mathbb{R}^2$ and extrapolation covariance $\\mathbf{S}_i \\in \\mathbb{R}^{2 \\times 2}$, a true associated hit $\\mathbf{z}_j \\in \\mathbb{R}^2$ is distributed as $\\mathcal{N}(\\mathbf{x}_i, \\mathbf{C}_i)$ where $\\mathbf{C}_i = \\mathbf{S}_i + \\mathbf{R}$ and $\\mathbf{R} \\in \\mathbb{R}^{2 \\times 2}$ is the measurement noise covariance. All positions are expressed in millimeters (mm), and covariances are expressed in square millimeters ($\\mathrm{mm}^2$).\n- Assume a uniform clutter density $\\lambda$ over a known sensor area $A$ (in $\\mathrm{mm}^2$) with an expected number of clutter hits $\\Lambda$ per readout. Then $\\lambda = \\Lambda / A$ has units $\\mathrm{mm}^{-2}$.\n- Each track $i$ has a Probability of Detection (PoD) $P_D^{(i)} \\in (0,1)$ and a prior Probability of Existence $P_T^{(i)} \\in (0,1)$.\n- Use a standard gating rule with a squared Mahalanobis distance $d_{ij}^2 = (\\mathbf{z}_j - \\mathbf{x}_i)^\\top \\mathbf{C}_i^{-1} (\\mathbf{z}_j - \\mathbf{x}_i)$ and a gate threshold $\\gamma > 0$. An association $(i,j)$ is admissible only if $d_{ij}^2 \\le \\gamma$.\n\nAssociation probabilities:\n- For an admissible pair $(i,j)$, define the local likelihood ratio $L_{ij} = \\dfrac{P_D^{(i)} \\cdot \\mathcal{N}(\\mathbf{z}_j; \\mathbf{x}_i, \\mathbf{C}_i)}{\\lambda}$, where $\\mathcal{N}(\\mathbf{z}_j; \\mathbf{x}_i, \\mathbf{C}_i)$ is the MVN probability density function evaluated at $\\mathbf{z}_j$. For inadmissible pairs, set $L_{ij} = 0$.\n- For a missed detection hypothesis of track $i$, define $L_{i0} = 1 - P_D^{(i)}$.\n- The normalized per-track association probabilities are then $p_{ij} = \\dfrac{L_{ij}}{L_{i0} + \\sum_k L_{ik}}$ for each admissible hit $j$, and $p_{i0} = \\dfrac{L_{i0}}{L_{i0} + \\sum_k L_{ik}}$ for the missed detection.\n\nMaximum a Posteriori (MAP) assignment:\n- Consider the joint assignment where each track is assigned either to one hit or to a missed detection, with the constraint that no hit is assigned to more than one track. The MAP assignment maximizes the joint posterior under independence assumptions and the PPP clutter model. Up to additive constants independent of the assignment, this reduces to maximizing $\\sum_i \\left[ \\log P_T^{(i)} + \\log L_{i,a(i)} \\right]$, where $a(i)$ is the chosen hypothesis for track $i$ (either a specific hit $j$ or $0$ for missed detection), subject to one-to-one constraints among hits.\n- Implement this selection by solving a linear assignment problem that minimizes the sum of negative log posterior contributions. Use a cost matrix with entries $c_{ij} = -\\left(\\log P_T^{(i)} + \\log P_D^{(i)} + \\log \\mathcal{N}(\\mathbf{z}_j; \\mathbf{x}_i, \\mathbf{C}_i) - \\log \\lambda\\right)$ for admissible pairs, a very large cost for inadmissible pairs, and $c_{i,\\text{miss}} = -\\left(\\log P_T^{(i)} + \\log(1 - P_D^{(i)})\\right)$ for missed detection. Enforce one-to-one constraints by giving each track a unique missed-detection column.\n\nNumerical and unit requirements:\n- Positions must be treated in millimeters (mm), covariances in square millimeters ($\\mathrm{mm}^2$). Angles are not involved in this problem.\n- The final output consists of assignment indices, which are dimensionless integers in zero-based indexing. Use $-1$ to denote a missed detection for a track.\n\nTest suite:\nProvide four test cases with scientifically plausible parameters. For each case, you must compute the MAP assignment as described.\n\n- Test Case $1$ (two tracks, three hits, broad gate):\n  - Sensor area: $A = 10000\\,\\mathrm{mm}^2$, expected clutter: $\\Lambda = 0.5$, so $\\lambda = 0.00005\\,\\mathrm{mm}^{-2}$.\n  - Measurement covariance: $\\mathbf{R} = \\begin{bmatrix} 1.0 & 0 \\\\ 0 & 1.0 \\end{bmatrix}\\,\\mathrm{mm}^2$.\n  - Gate threshold: $\\gamma = 9.0$.\n  - Tracks:\n    - Track $1$: $\\mathbf{x}_1 = [10.0, 10.0]\\,\\mathrm{mm}$, $\\mathbf{S}_1 = \\begin{bmatrix} 0.5 & 0 \\\\ 0 & 0.5 \\end{bmatrix}\\,\\mathrm{mm}^2$, $P_D^{(1)} = 0.9$, $P_T^{(1)} = 0.95$.\n    - Track $2$: $\\mathbf{x}_2 = [20.0, 20.0]\\,\\mathrm{mm}$, $\\mathbf{S}_2 = \\begin{bmatrix} 0.7 & 0 \\\\ 0 & 0.7 \\end{bmatrix}\\,\\mathrm{mm}^2$, $P_D^{(2)} = 0.85$, $P_T^{(2)} = 0.9$.\n  - Hits: $\\mathbf{z}_1 = [9.6, 10.4]\\,\\mathrm{mm}$, $\\mathbf{z}_2 = [20.2, 19.7]\\,\\mathrm{mm}$, $\\mathbf{z}_3 = [50.0, 50.0]\\,\\mathrm{mm}$.\n\n- Test Case $2$ (three tracks, three hits, competition for a hit):\n  - Sensor area: $A = 400\\,\\mathrm{mm}^2$, expected clutter: $\\Lambda = 0.2$, so $\\lambda = 0.0005\\,\\mathrm{mm}^{-2}$.\n  - Measurement covariance: $\\mathbf{R} = \\begin{bmatrix} 0.2 & 0 \\\\ 0 & 0.2 \\end{bmatrix}\\,\\mathrm{mm}^2$.\n  - Gate threshold: $\\gamma = 4.0$.\n  - Tracks:\n    - Track $1$: $\\mathbf{x}_1 = [0.0, 0.0]\\,\\mathrm{mm}$, $\\mathbf{S}_1 = \\begin{bmatrix} 0.3 & 0 \\\\ 0 & 0.3 \\end{bmatrix}\\,\\mathrm{mm}^2$, $P_D^{(1)} = 0.95$, $P_T^{(1)} = 0.99$.\n    - Track $2$: $\\mathbf{x}_2 = [0.5, 0.0]\\,\\mathrm{mm}$, $\\mathbf{S}_2 = \\begin{bmatrix} 0.3 & 0 \\\\ 0 & 0.3 \\end{bmatrix}\\,\\mathrm{mm}^2$, $P_D^{(2)} = 0.95$, $P_T^{(2)} = 0.99$.\n    - Track $3$: $\\mathbf{x}_3 = [5.0, 5.0]\\,\\mathrm{mm}$, $\\mathbf{S}_3 = \\begin{bmatrix} 0.3 & 0 \\\\ 0 & 0.3 \\end{bmatrix}\\,\\mathrm{mm}^2$, $P_D^{(3)} = 0.9$, $P_T^{(3)} = 0.9$.\n  - Hits: $\\mathbf{z}_1 = [0.2, -0.1]\\,\\mathrm{mm}$, $\\mathbf{z}_2 = [0.6, 0.1]\\,\\mathrm{mm}$, $\\mathbf{z}_3 = [5.1, 4.9]\\,\\mathrm{mm}$.\n\n- Test Case $3$ (one track, hits out of gate, low detection probability):\n  - Sensor area: $A = 10000\\,\\mathrm{mm}^2$, expected clutter: $\\Lambda = 1.0$, so $\\lambda = 0.0001\\,\\mathrm{mm}^{-2}$.\n  - Measurement covariance: $\\mathbf{R} = \\begin{bmatrix} 1.0 & 0 \\\\ 0 & 1.0 \\end{bmatrix}\\,\\mathrm{mm}^2$.\n  - Gate threshold: $\\gamma = 9.0$.\n  - Track $1$: $\\mathbf{x}_1 = [100.0, 100.0]\\,\\mathrm{mm}$, $\\mathbf{S}_1 = \\begin{bmatrix} 1.0 & 0 \\\\ 0 & 1.0 \\end{bmatrix}\\,\\mathrm{mm}^2$, $P_D^{(1)} = 0.2$, $P_T^{(1)} = 0.8$.\n  - Hits: $\\mathbf{z}_1 = [110.0, 110.0]\\,\\mathrm{mm}$, $\\mathbf{z}_2 = [90.0, 90.0]\\,\\mathrm{mm}$.\n\n- Test Case $4$ (boundary gating at threshold):\n  - Sensor area: $A = 100\\,\\mathrm{mm}^2$, expected clutter: $\\Lambda = 0.1$, so $\\lambda = 0.001\\,\\mathrm{mm}^{-2}$.\n  - Measurement covariance: $\\mathbf{R} = \\begin{bmatrix} 10^{-8} & 0 \\\\ 0 & 10^{-8} \\end{bmatrix}\\,\\mathrm{mm}^2$.\n  - Gate threshold: $\\gamma = 4.0$.\n  - Track $1$: $\\mathbf{x}_1 = [0.0, 0.0]\\,\\mathrm{mm}$, $\\mathbf{S}_1 = \\begin{bmatrix} 1.0 & 0 \\\\ 0 & 1.0 \\end{bmatrix}\\,\\mathrm{mm}^2$, $P_D^{(1)} = 0.7$, $P_T^{(1)} = 0.9$.\n  - Hits: $\\mathbf{z}_1 = [2.0, 0.0]\\,\\mathrm{mm}$, $\\mathbf{z}_2 = [10.0, 0.0]\\,\\mathrm{mm}$.\n\nProgram output specification:\n- For each test case, compute the MAP assignment and produce a list of integers of length equal to the number of tracks, using zero-based indexing for hit indices and $-1$ to indicate missed detection.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is the per-test-case assignment list. For example, the output should look like $[\\,[a_{1,1}, a_{1,2}, \\dots], [a_{2,1}, \\dots], \\dots\\,]$ with zero-based indexing and $-1$ representing missed detection.", "solution": "We begin from probability theory and a physically realistic measurement model. A candidate track $i$ extrapolated to a detector layer is represented by a predicted position $\\mathbf{x}_i \\in \\mathbb{R}^2$ and an extrapolation covariance $\\mathbf{S}_i \\in \\mathbb{R}^{2 \\times 2}$. A measured hit $\\mathbf{z}_j \\in \\mathbb{R}^2$ is modeled as the true position corrupted by detector noise with covariance $\\mathbf{R} \\in \\mathbb{R}^{2 \\times 2}$. Under standard assumptions, the conditional distribution of an associated hit is a Multivariate Normal (MVN), $\\mathbf{z}_j \\mid \\text{track } i \\sim \\mathcal{N}(\\mathbf{x}_i, \\mathbf{C}_i)$ where $\\mathbf{C}_i = \\mathbf{S}_i + \\mathbf{R}$. All positions are measured in millimeters (mm), and the covariances are in square millimeters ($\\mathrm{mm}^2$).\n\nTo ensure physical and numerical realism, we apply a gate based on the squared Mahalanobis distance\n$$\nd_{ij}^2 = (\\mathbf{z}_j - \\mathbf{x}_i)^\\top \\mathbf{C}_i^{-1} (\\mathbf{z}_j - \\mathbf{x}_i),\n$$\nand only consider associations $(i,j)$ for which $d_{ij}^2 \\le \\gamma$, where $\\gamma > 0$ is a specified threshold (e.g., corresponding to a confidence region under the MVN).\n\nWe model spurious hits as a uniform Poisson Point Process (PPP) over the sensor area $A$ with expected count $\\Lambda$, yielding a spatial clutter density $\\lambda = \\Lambda / A$ in $\\mathrm{mm}^{-2}$. This density provides a reference for evaluating how likely a hit is to be clutter as opposed to originating from a particular track.\n\nEach track $i$ has a Probability of Detection (PoD) $P_D^{(i)} \\in (0,1)$ and a prior Probability of Existence $P_T^{(i)} \\in (0,1)$. Given a hit $\\mathbf{z}_j$, the local likelihood ratio for an admissible association $(i,j)$ follows from Bayes rule and the PPP clutter model:\n$$\nL_{ij} = \\frac{P_D^{(i)} \\, \\mathcal{N}(\\mathbf{z}_j; \\mathbf{x}_i, \\mathbf{C}_i)}{\\lambda}.\n$$\nFor inadmissible $(i,j)$ (outside the gate), we set $L_{ij} = 0$. The missed detection hypothesis for track $i$ has likelihood ratio\n$$\nL_{i0} = 1 - P_D^{(i)}.\n$$\nThese quantities are dimensionless because $\\mathcal{N}(\\cdot)$ and $\\lambda$ are both densities, and $P_D^{(i)}$ is dimensionless.\n\nFor per-track association probabilities, we normalize using\n$$\np_{ij} = \\frac{L_{ij}}{L_{i0} + \\sum_{k} L_{ik}}, \\quad p_{i0} = \\frac{L_{i0}}{L_{i0} + \\sum_{k} L_{ik}},\n$$\nwhich follows from Bayes rule under mutually exclusive hypotheses for track $i$.\n\nThe global Maximum a Posteriori (MAP) assignment problem considers assigning each track to a single hypothesis $a(i)$ (either a specific hit index $j$ or $0$ for missed detection) while enforcing that no hit is assigned to more than one track. Under independence assumptions and ignoring constants independent of the assignment, the joint posterior is proportional to\n$$\n\\prod_i \\left( P_T^{(i)} \\cdot L_{i,a(i)} \\right),\n$$\nand we therefore seek to maximize\n$$\n\\sum_i \\left[ \\log P_T^{(i)} + \\log L_{i,a(i)} \\right].\n$$\nThis is a linear assignment problem if we construct an appropriate cost matrix. For admissible associations $(i,j)$, define the cost\n$$\nc_{ij} = -\\left(\\log P_T^{(i)} + \\log P_D^{(i)} + \\log \\mathcal{N}(\\mathbf{z}_j; \\mathbf{x}_i, \\mathbf{C}_i) - \\log \\lambda \\right),\n$$\nand for inadmissible associations set $c_{ij}$ to a very large value to effectively disallow the pairing. For missed detection, we define a unique dummy column per track with cost\n$$\nc_{i,\\text{miss}} = -\\left(\\log P_T^{(i)} + \\log(1 - P_D^{(i)})\\right).\n$$\nHaving $N$ tracks and $M$ hits, we build a cost matrix of size $N \\times (M + N)$ by appending $N$ missed-detection columns, one for each track. This ensures the algorithm assigns exactly one hypothesis to each track and enforces that each hit can be assigned at most once. The Hungarian algorithm (also known as the Kuhnâ€“Munkres algorithm) can then be used to find the assignment that minimizes the total cost, which corresponds to the MAP solution.\n\nImplementation details:\n- Compute $\\mathbf{C}_i = \\mathbf{S}_i + \\mathbf{R}$ for each track.\n- Compute $d_{ij}^2$ using a numerically stable method such as solving $\\mathbf{C}_i \\mathbf{y} = \\mathbf{z}_j - \\mathbf{x}_i$ for $\\mathbf{y}$ and then $d_{ij}^2 = (\\mathbf{z}_j - \\mathbf{x}_i)^\\top \\mathbf{y}$.\n- Compute the MVN density\n$$\n\\mathcal{N}(\\mathbf{z}_j; \\mathbf{x}_i, \\mathbf{C}_i) = \\frac{1}{2\\pi \\sqrt{\\det \\mathbf{C}_i}} \\exp\\left( -\\frac{1}{2} d_{ij}^2 \\right).\n$$\n- Use a large finite cost (for example, $10^9$) to represent disallowed assignments. Avoid taking $\\log 0$ by not computing log densities for gated-out pairs.\n- For each test case, construct the cost matrix, solve the assignment, and decode the result: if a track is assigned to a hit column $j < M$, output $j$; if it is assigned to its unique missed-detection column, output $-1$.\n\nEdge cases:\n- If all pairs for a track are outside the gate, it will be assigned to missed detection because the admissible hit costs are effectively infinite while $c_{i,\\text{miss}}$ is finite.\n- If the squared Mahalanobis distance equals the gate threshold ($d_{ij}^2 = \\gamma$), the association is admissible by definition ($\\le \\gamma$), and the density contributes accordingly.\n- With low $P_D^{(i)}$, $c_{i,\\text{miss}}$ may become competitive relative to hit associations even if some hits are within the gate, reflecting the underlying physics of low detection probability.\n\nThe program will compute the assignments for the four provided test cases and print a single line containing a list of assignment lists, each of which is a list of integers using zero-based hit indices and $-1$ for missed detection. This output is unitless as it represents discrete indices.", "answer": "[[0, 1],[0, 1, 2],[-1],[0]]", "id": "3539732"}]}