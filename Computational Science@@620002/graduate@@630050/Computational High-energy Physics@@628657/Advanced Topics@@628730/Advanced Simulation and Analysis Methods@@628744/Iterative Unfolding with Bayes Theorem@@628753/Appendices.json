{"hands_on_practices": [{"introduction": "To begin our hands-on exploration, we first derive the core mechanism of iterative Bayesian unfolding directly from its foundational principles. This exercise [@problem_id:3518211] will guide you through deriving the first update step, revealing its intuitive interpretation as a \"normalized back-projection\" of the observed data. By then comparing this result to a naive matrix inversion, you will gain a direct appreciation for the inherent regularizing power of the Bayesian approach, even after a single iteration.", "problem": "Consider a simplified detector model in computational high-energy physics with three truth bins indexed by $i \\in \\{1,2,3\\}$ and three measured bins indexed by $j \\in \\{1,2,3\\}$. The detector response is described by a response matrix $A_{ji}$, where $A_{ji}$ is the conditional probability $P(j \\mid i)$ that an event originating in truth bin $i$ is reconstructed in measured bin $j$. Let the measured counts be the vector $m = (m_{1}, m_{2}, m_{3})$ and the unknown true counts be $n = (n_{1}, n_{2}, n_{3})$. Assume that acceptance effects are encoded in the columns of $A_{ji}$ (so column sums can be less than one), and ignore any out-of-acceptance bin.\n\nStarting only from Bayes’ theorem and the definition $A_{ji} = P(j \\mid i)$, derive the expression for the first Bayesian iterative unfolding estimate $n_{i}^{(1)}$ under a flat prior $n_{i}^{(0)} = c$ (where $c > 0$ is constant across $i$). Show that this first iteration can be interpreted as a normalized back-projection of the measured counts through $A_{ji}$.\n\nThen, for the specific three-bin case with\n$$\nA \\;=\\;\n\\begin{pmatrix}\n0.70 & 0.10 & 0.05 \\\\\n0.20 & 0.70 & 0.20 \\\\\n0.05 & 0.15 & 0.60\n\\end{pmatrix}\n\\quad\\text{and}\\quad\nm \\;=\\; \\begin{pmatrix} 1200 \\\\ 800 \\\\ 500 \\end{pmatrix},\n$$\ncompute:\n1. The first-iteration estimate $n^{(1)}$ obtained from your Bayes-based derivation with the flat prior $n_{i}^{(0)} = c$.\n2. The matrix-inversion unfolded estimate $n^{\\mathrm{MI}}$ defined by solving $A\\,n^{\\mathrm{MI}} = m$.\n\nFinally, quantify the deviation of the first iteration from the matrix-inversion result using the relative Euclidean norm\n$$\nr \\;=\\; \\frac{\\|\\,n^{(1)} - n^{\\mathrm{MI}}\\,\\|_{2}}{\\|\\,n^{\\mathrm{MI}}\\,\\|_{2}}.\n$$\nExpress your final answer for $r$ as a decimal fraction with no units, and round your answer to four significant figures.", "solution": "**Part 1: Derivation of the First Iteration Formula**\n\nThe goal is to derive the expression for the first iterative unfolding estimate, $n_i^{(1)}$, starting from Bayes' theorem and the given definitions.\n\nBayes' theorem relates the posterior probability of a cause (event in truth bin $i$) given an effect (event in measured bin $j$) to the conditional probability of the effect given the cause and the prior probability of the cause. Formally,\n$$P(i \\mid j) = \\frac{P(j \\mid i) P(i)}{P(j)}$$\nwhere:\n- $P(i \\mid j)$ is the posterior probability that an event measured in bin $j$ originated from truth bin $i$.\n- $P(j \\mid i)$ is the conditional probability of measuring an event in bin $j$ given it originated in truth bin $i$. This is given by the response matrix element, $A_{ji}$.\n- $P(i)$ is the prior probability that an event originates in truth bin $i$.\n- $P(j)$ is the total probability of measuring an event in bin $j$, which acts as a normalization constant. It is obtained by marginalizing over all possible causes: $P(j) = \\sum_k P(j \\mid k) P(k)$.\n\nThe unfolded number of events in truth bin $i$, which we denote $n_i'$, can be estimated by considering the measured counts $m_j$. For each measured bin $j$, the number of counts is $m_j$. We can re-distribute these counts back to the truth bins according to the posterior probability $P(i \\mid j)$. Summing the contributions from all measured bins gives the estimate for $n_i'$:\n$$n_i' = \\sum_j m_j P(i \\mid j)$$\nThis formulation assumes that the total number of unfolded events is equal to the total number of measured events, which is a feature of this specific Bayesian update step when efficiencies are not explicitly corrected for in a separate step.\n\nIn the iterative Bayesian unfolding scheme, the prior $P(i)$ for a given iteration is determined by the result of the previous iteration. For the first iteration, $n_i^{(1)}$, the prior $P_0(i)$ is based on the initial guess, $n_i^{(0)}$. The problem specifies a flat prior, $n_i^{(0)} = c$ for some constant $c>0$, for all $i \\in \\{1, 2, 3\\}$. This implies a uniform prior probability distribution:\n$$P_0(i) = \\frac{n_i^{(0)}}{\\sum_k n_k^{(0)}} = \\frac{c}{3c} = \\frac{1}{3}$$\nThe prior probability $P_0(i)$ is constant for all $i$. Let's denote this constant by $K$.\n\nNow, we can compute the terms in Bayes' theorem for this first iteration:\n$P(j \\mid i) = A_{ji}$\n$P_0(i) = K$\n$P(j) = \\sum_k P(j \\mid k) P_0(k) = \\sum_k A_{jk} K = K \\sum_k A_{jk}$\n\nSubstituting these into the expression for the posterior probability $P(i \\mid j)$:\n$$P(i \\mid j) = \\frac{A_{ji} K}{K \\sum_k A_{jk}} = \\frac{A_{ji}}{\\sum_k A_{jk}}$$\nThe first-iteration estimate $n_i^{(1)}$ is then found by substituting this posterior probability back into the redistribution formula:\n$$n_i^{(1)} = \\sum_j m_j \\frac{A_{ji}}{\\sum_k A_{jk}}$$\nThis expression for $n_i^{(1)}$ is independent of the constant $c$ from the flat prior, as required for a well-defined procedure.\n\n**Interpretation as Normalized Back-Projection**\n\nThe derived formula $n_i^{(1)} = \\sum_j m_j \\frac{A_{ji}}{\\sum_k A_{jk}}$ can be interpreted as a normalized back-projection.\n- The term $A_{ji}$ corresponds to an element of the transposed response matrix, $(A^T)_{ij}$. Multiplying the vector of measured counts $m$ by $A^T$ is known as a \"back-projection\". The term $m_j A_{ji}$ represents the contribution of the measurement in bin $j$ back-projected to the true bin $i$.\n- The denominator, $\\sum_k A_{jk}$, is the sum of the $j$-th row of the matrix $A$. As shown in the derivation, this term serves as the normalization factor required to transform the forward conditional probabilities $P(j \\mid i) = A_{ji}$ into the reverse conditional probabilities $P(i \\mid j)$, under the specific assumption of a flat prior $P_0(i)$.\n- Thus, each term in the sum, $m_j \\frac{A_{ji}}{\\sum_k A_{jk}}$, represents the counts measured in bin $j$ re-assigned to truth bin $i$ based on the normalized probability that an event seen in $j$ originated in $i$. Summing over all measured bins $j$ provides the total estimated count for the true bin $i$.\n\n**Part 2: Numerical Calculations**\n\nThe given data are:\n$$\nA \\;=\\;\n\\begin{pmatrix}\n0.70 & 0.10 & 0.05 \\\\\n0.20 & 0.70 & 0.20 \\\\\n0.05 & 0.15 & 0.60\n\\end{pmatrix}\n\\quad\\text{and}\\quad\nm \\;=\\; \\begin{pmatrix} 1200 \\\\ 800 \\\\ 500 \\end{pmatrix}\n$$\n\n**1. Compute the first-iteration estimate $n^{(1)}$**\n\nFirst, we calculate the row sums of $A$, $R_j = \\sum_k A_{jk}$:\n$R_1 = 0.70 + 0.10 + 0.05 = 0.85$\n$R_2 = 0.20 + 0.70 + 0.20 = 1.10$\n$R_3 = 0.05 + 0.15 + 0.60 = 0.80$\n\nNow, we apply the formula for $n_i^{(1)}$:\n$n_1^{(1)} = \\sum_j m_j \\frac{A_{j1}}{R_j} = m_1 \\frac{A_{11}}{R_1} + m_2 \\frac{A_{21}}{R_2} + m_3 \\frac{A_{31}}{R_3}$\n$n_1^{(1)} = (1200) \\frac{0.70}{0.85} + (800) \\frac{0.20}{1.10} + (500) \\frac{0.05}{0.80} \\approx 988.2353 + 145.4545 + 31.25 = 1164.9398$\n\n$n_2^{(1)} = \\sum_j m_j \\frac{A_{j2}}{R_j} = m_1 \\frac{A_{12}}{R_1} + m_2 \\frac{A_{22}}{R_2} + m_3 \\frac{A_{32}}{R_3}$\n$n_2^{(1)} = (1200) \\frac{0.10}{0.85} + (800) \\frac{0.70}{1.10} + (500) \\frac{0.15}{0.80} \\approx 141.1765 + 509.0909 + 93.75 = 744.0174$\n\n$n_3^{(1)} = \\sum_j m_j \\frac{A_{j3}}{R_j} = m_1 \\frac{A_{13}}{R_1} + m_2 \\frac{A_{23}}{R_2} + m_3 \\frac{A_{33}}{R_3}$\n$n_3^{(1)} = (1200) \\frac{0.05}{0.85} + (800) \\frac{0.20}{1.10} + (500) \\frac{0.60}{0.80} \\approx 70.5882 + 145.4545 + 375 = 591.0427$\n\nSo, the first-iteration estimate is $n^{(1)} \\approx (1164.94, 744.02, 591.04)$.\n\n**2. Compute the matrix-inversion estimate $n^{\\mathrm{MI}}$**\n\nThe matrix-inversion estimate is the solution to the linear system $A n^{\\mathrm{MI}} = m$, which is $n^{\\mathrm{MI}} = A^{-1} m$.\nFirst, we find the determinant of $A$:\n$\\det(A) = 0.70(0.70 \\cdot 0.60 - 0.20 \\cdot 0.15) - 0.10(0.20 \\cdot 0.60 - 0.20 \\cdot 0.05) + 0.05(0.20 \\cdot 0.15 - 0.70 \\cdot 0.05)$\n$\\det(A) = 0.70(0.39) - 0.10(0.11) + 0.05(-0.005) = 0.273 - 0.011 - 0.00025 = 0.26175$.\nSince $\\det(A) \\neq 0$, the matrix is invertible. The inverse is $A^{-1} = \\frac{1}{\\det(A)}\\text{adj}(A)$, where $\\text{adj}(A)$ is the adjugate matrix of $A$.\nThe adjugate of $A$ is the transpose of the cofactor matrix:\n$\\text{adj}(A) = \n\\begin{pmatrix}\n0.39 & -0.0525 & -0.015 \\\\\n-0.11 & 0.4175 & -0.13 \\\\\n-0.005 & -0.10 & 0.47\n\\end{pmatrix}$\n\nNow we compute $n^{\\mathrm{MI}} = A^{-1}m$:\n$n^{\\mathrm{MI}} = \\frac{1}{0.26175}\n\\begin{pmatrix}\n0.39 & -0.0525 & -0.015 \\\\\n-0.11 & 0.4175 & -0.13 \\\\\n-0.005 & -0.10 & 0.47\n\\end{pmatrix}\n\\begin{pmatrix}\n1200 \\\\\n800 \\\\\n500\n\\end{pmatrix}$\n$n^{\\mathrm{MI}} = \\frac{1}{0.26175}\n\\begin{pmatrix}\n0.39(1200) - 0.0525(800) - 0.015(500) \\\\\n-0.11(1200) + 0.4175(800) - 0.13(500) \\\\\n-0.005(1200) - 0.10(800) + 0.47(500)\n\\end{pmatrix}\n= \\frac{1}{0.26175}\n\\begin{pmatrix}\n468 - 42 - 7.5 \\\\\n-132 + 334 - 65 \\\\\n-6 - 80 + 235\n\\end{pmatrix}\n= \\frac{1}{0.26175}\n\\begin{pmatrix}\n418.5 \\\\\n137 \\\\\n149\n\\end{pmatrix}$\n$n^{\\mathrm{MI}} \\approx \n\\begin{pmatrix}\n1598.8548 \\\\\n523.4986 \\\\\n569.2593\n\\end{pmatrix}$\n\n**Part 3: Quantify the Deviation**\n\nFinally, we compute the relative Euclidean norm $r = \\frac{\\|\\,n^{(1)} - n^{\\mathrm{MI}}\\,\\|_{2}}{\\|\\,n^{\\mathrm{MI}}\\,\\|_{2}}$.\nFirst, the difference vector $d = n^{(1)} - n^{\\mathrm{MI}}$:\n$d \\approx (1164.9398 - 1598.8548, 744.0174 - 523.4986, 591.0427 - 569.2593)$\n$d \\approx (-433.9150, 220.5188, 21.7834)$\n\nNext, the Euclidean norms:\n$\\|\\,d\\,\\|_{2} = \\sqrt{(-433.9150)^2 + (220.5188)^2 + (21.7834)^2} \\approx \\sqrt{188282.1 + 48628.5 + 474.5} = \\sqrt{237385.1} \\approx 487.2218$\n$\\|\\,n^{\\mathrm{MI}}\\,\\|_{2} = \\sqrt{(1598.8548)^2 + (523.4986)^2 + (569.2593)^2} \\approx \\sqrt{2556336.8 + 274050.8 + 324056.2} = \\sqrt{3154443.8} \\approx 1776.0754$\n\nThe relative norm $r$ is:\n$r = \\frac{487.2218}{1776.0754} \\approx 0.2743256$\n\nRounding to four significant figures, we get $r \\approx 0.2743$.", "answer": "$$\\boxed{0.2743}$$", "id": "3518211"}, {"introduction": "A crucial element of any Bayesian method is the choice of the prior distribution. While a simple flat prior is a common starting point, a more principled approach can significantly improve performance, especially in low-statistics regimes. This practice [@problem_id:3518183] delves into the derivation of the Jeffreys prior, an objective choice derived from the Fisher information, and demonstrates how it can be interpreted as an initial \"pseudo-count\" to regularize the unfolding problem and prevent the loss of information from bins with zero counts.", "problem": "In computational High Energy Physics (HEP), unfolding aims to infer the distribution of a true quantity discretized into bins, denoted by $T_i$ for $i$ in a finite set of bin indices, from measurements recorded in reconstructed bins, denoted by $R_j$. In iterative Bayesian unfolding (IBU), one updates an initial prior over the true-bin means using Bayes’ theorem and the detector response. Consider a two-bin ($i \\in \\{1,2\\}$) true distribution and a two-bin ($j \\in \\{1,2\\}$) reconstructed distribution. The detector response probabilities are $A_{j i} \\equiv P(R_j \\mid T_i)$, assumed known and given by $A_{11} = 0.8$, $A_{12} = 0.2$, $A_{21} = 0.2$, $A_{22} = 0.8$. The observed reconstructed counts are $m_1 = 0$ and $m_2 = 3$.\n\n(a) Starting from first principles, use the definition of the Jeffreys prior $\\pi(\\theta) \\propto \\sqrt{I(\\theta)}$, where $I(\\theta)$ is the Fisher information, and the Poisson likelihood for an observed count $k$ given a mean $\\mu$, to derive the Jeffreys prior for a Poisson mean $\\mu_i$ in bin $i$, showing that $\\pi(\\mu_i) \\propto \\mu_i^{-1/2}$.\n\n(b) Based on part (a), explain how one may encode this prior information as an initial pseudo-count $n_i^{(0)}$ per true bin when initializing IBU, and analyze the implications of this choice for low-statistics situations (for example, when $m_j = 0$ for some $j$). Your explanation should rely on the connection between Jeffreys’ prior and conjugate Bayesian updating for the Poisson model, and on Bayes’ theorem and the law of total probability, not on heuristic rules.\n\n(c) Using the conclusion of part (b) in the case of two true bins with no additional shape information (so that the initial prior over true-bin membership is symmetric), perform the first IBU update to compute the unfolded estimate $n_1^{(1)}$ for the true bin $i=1$. You may treat the initial prior probabilities over true bins as proportional to the initial pseudo-counts $n_i^{(0)}$ that arise from the Jeffreys prior and normalize them across bins. Report the exact value of $n_1^{(1)}$ with no rounding and no units.", "solution": "**(a) Derivation of the Jeffreys Prior for a Poisson Mean**\n\nWe are tasked with deriving the Jeffreys prior for the mean $\\mu_i$ of a Poisson distribution. The Jeffreys prior for a parameter $\\theta$ is defined as $\\pi(\\theta) \\propto \\sqrt{I(\\theta)}$, where $I(\\theta)$ is the Fisher information.\n\nLet us consider a single bin $i$, and for simplicity, let its mean be denoted by $\\mu$. The probability of observing $k$ counts in this bin, given the mean $\\mu$, follows a Poisson distribution. The likelihood function is:\n$$L(\\mu \\mid k) = P(k \\mid \\mu) = \\frac{\\mu^k \\exp(-\\mu)}{k!}$$\nThe natural logarithm of the likelihood, or log-likelihood, is:\n$$\\ln L(\\mu \\mid k) = k \\ln \\mu - \\mu - \\ln(k!)$$\nThe Fisher information $I(\\mu)$ is defined as the negative of the expectation value of the second derivative of the log-likelihood with respect to the parameter $\\mu$. First, we compute the first and second derivatives:\n$$\\frac{\\partial}{\\partial \\mu} \\ln L(\\mu \\mid k) = \\frac{k}{\\mu} - 1$$\n$$\\frac{\\partial^2}{\\partial \\mu^2} \\ln L(\\mu \\mid k) = -\\frac{k}{\\mu^2}$$\nThe Fisher information is then:\n$$I(\\mu) = -E\\left[\\frac{\\partial^2}{\\partial \\mu^2} \\ln L(\\mu \\mid k)\\right]$$\nThe expectation is taken with respect to the distribution of the data $k$, which is the Poisson distribution with mean $\\mu$. Therefore, $E[k] = \\mu$. Substituting this into the expression for $I(\\mu)$:\n$$I(\\mu) = -E\\left[-\\frac{k}{\\mu^2}\\right] = \\frac{E[k]}{\\mu^2} = \\frac{\\mu}{\\mu^2} = \\frac{1}{\\mu}$$\nThe Jeffreys prior $\\pi(\\mu)$ is proportional to the square root of the Fisher information:\n$$\\pi(\\mu) \\propto \\sqrt{I(\\mu)} = \\sqrt{\\frac{1}{\\mu}} = \\mu^{-1/2}$$\nThis derivation holds for the mean $\\mu_i$ of any bin $i$. Thus, we have shown that $\\pi(\\mu_i) \\propto \\mu_i^{-1/2}$.\n\n**(b) Interpretation of the Jeffreys Prior as an Initial Pseudo-Count**\n\nTo understand how this prior information is encoded, we examine the framework of conjugate Bayesian updating for the Poisson model. The conjugate prior for a Poisson likelihood is the Gamma distribution. A Gamma distribution for the mean $\\mu$ with shape parameter $\\alpha$ and rate parameter $\\beta$ is given by:\n$$\\pi(\\mu \\mid \\alpha, \\beta) \\propto \\mu^{\\alpha-1} \\exp(-\\beta\\mu)$$\nThe Jeffreys prior derived in part (a), $\\pi(\\mu) \\propto \\mu^{-1/2}$, can be identified as a Gamma distribution with parameters $\\alpha = 1/2$ and $\\beta=0$. This is an improper prior since its integral over $\\mu \\in [0, \\infty)$ diverges.\n\nWhen we update this prior with an observation of $k$ counts from a Poisson process, the posterior distribution for $\\mu$ is also a Gamma distribution. The posterior is proportional to the product of the likelihood and the prior:\n$$P(\\mu \\mid k) \\propto L(k \\mid \\mu) \\pi(\\mu) \\propto \\left(\\mu^k \\exp(-\\mu)\\right) \\left(\\mu^{\\alpha-1} \\exp(-\\beta\\mu)\\right) = \\mu^{k+\\alpha-1} \\exp(-(\\beta+1)\\mu)$$\nFor the Jeffreys prior ($\\alpha=1/2, \\beta=0$), the posterior distribution is:\n$$P(\\mu \\mid k) \\propto \\mu^{k+1/2-1} \\exp(-\\mu)$$\nThis is a proper Gamma distribution, specifically $\\text{Gamma}(k+1/2, 1)$. The mean of this posterior distribution is $E[\\mu \\mid k] = \\frac{k+1/2}{1} = k + 1/2$.\n\nThis result provides a clear interpretation: starting with the Jeffreys prior is equivalent to obtaining a posterior estimate for the mean by adding $1/2$ to the observed count $k$. This suggests that the Jeffreys prior can be encoded as an initial \"pseudo-count\" of $n_i^{(0)} = 1/2$ for each true bin $i$. This serves as the starting point for the iterative unfolding procedure before any data are considered.\n\nThe implications for low-statistics situations are significant. The Iterative Bayesian Unfolding (IBU) algorithm updates an estimate for the number of true-bin counts, $n_i^{(k)}$, at each iteration $k$. A common update rule is $n_i^{(k+1)} = \\sum_j m_j P^{(k)}(T_i \\mid R_j)$, where $P^{(k)}(T_i \\mid R_j)$ depends on the prior probabilities from the previous step, $P^{(k)}(T_i) \\propto n_i^{(k)}$. If we were to initialize with $n_i^{(0)} = 0$ for some bin $i$, its prior probability $P^{(0)}(T_i)$ would be zero, and consequently, its unfolded estimate $n_i^{(k)}$ would remain zero for all subsequent iterations, regardless of the observed data. This is an undesirable property, as it prematurely rules out a true bin as a possible cause for the observed effects.\n\nBy choosing $n_i^{(0)} = 1/2$ for all $i$, we ensure that every true bin starts with a non-zero count. This regularizes the problem, preventing the estimates for any bin from being permanently fixed at zero. Even if a reconstructed bin $j$ has zero observed counts ($m_j = 0$), the true bins that could contribute to it are not eliminated from consideration for contributing to other observed bins. This initialization guarantees that the initial priors $P^{(0)}(T_i)$ are well-defined and non-zero, allowing the algorithm to distribute the observed counts $m_j$ among all possible true causes according to the response probabilities.\n\n**(c) First Iteration of Unfolding**\n\nWe are asked to perform the first IBU update to compute the unfolded estimate $n_1^{(1)}$ for the true bin $i=1$.\nBased on part (b), and the condition of no additional shape information (implying symmetry), we initialize the pseudo-counts for the two true bins as:\n$$n_1^{(0)} = \\frac{1}{2}, \\quad n_2^{(0)} = \\frac{1}{2}$$\nThe total initial pseudo-count is $N^{(0)} = n_1^{(0)} + n_2^{(0)} = 1/2 + 1/2 = 1$. The initial prior probabilities over the true bins are normalized:\n$$P^{(0)}(T_1) = \\frac{n_1^{(0)}}{N^{(0)}} = \\frac{1/2}{1} = \\frac{1}{2}$$\n$$P^{(0)}(T_2) = \\frac{n_2^{(0)}}{N^{(0)}} = \\frac{1/2}{1} = \\frac{1}{2}$$\nThe IBU update formula for the first iteration is:\n$$n_i^{(1)} = \\sum_{j=1}^{2} m_j P(T_i \\mid R_j)$$\nwhere the posterior probability $P(T_i \\mid R_j)$ is computed using Bayes' theorem with the initial prior $P^{(0)}(T_i)$:\n$$P(T_i \\mid R_j) = \\frac{P(R_j \\mid T_i) P^{(0)}(T_i)}{\\sum_{l=1}^{2} P(R_j \\mid T_l) P^{(0)}(T_l)} = \\frac{A_{ji} P^{(0)}(T_i)}{\\sum_{l=1}^{2} A_{jl} P^{(0)}(T_l)}$$\nWe need to compute $n_1^{(1)}$:\n$$n_1^{(1)} = m_1 P(T_1 \\mid R_1) + m_2 P(T_1 \\mid R_2)$$\nSubstituting the formula for the posterior probabilities:\n$$n_1^{(1)} = m_1 \\frac{A_{11} P^{(0)}(T_1)}{A_{11} P^{(0)}(T_1) + A_{12} P^{(0)}(T_2)} + m_2 \\frac{A_{21} P^{(0)}(T_1)}{A_{21} P^{(0)}(T_1) + A_{22} P^{(0)}(T_2)}$$\nWe are given the following values:\n\\begin{itemize}\n    \\item Observed counts: $m_1 = 0$, $m_2 = 3$\n    \\item Response matrix elements: $A_{11} = 0.8$, $A_{12} = 0.2$, $A_{21} = 0.2$, $A_{22} = 0.8$\n    \\item Initial priors: $P^{(0)}(T_1) = 1/2$, $P^{(0)}(T_2) = 1/2$\n\\end{itemize}\nThe first term in the expression for $n_1^{(1)}$ is zero because $m_1 = 0$. We only need to compute the second term:\n$$n_1^{(1)} = 0 + 3 \\times \\frac{(0.2) \\times (1/2)}{(0.2) \\times (1/2) + (0.8) \\times (1/2)}$$\nSimplifying the expression within the fraction:\n$$n_1^{(1)} = 3 \\times \\frac{0.1}{0.1 + 0.4} = 3 \\times \\frac{0.1}{0.5}$$\n$$n_1^{(1)} = 3 \\times \\frac{1}{5} = \\frac{3}{5}$$\nConverting to decimal form, this is $0.6$. The question asks for the exact value.", "answer": "$$\\boxed{\\frac{3}{5}}$$", "id": "3518183"}, {"introduction": "Bridging the gap between idealized models and real-world applications requires us to rigorously validate the components of our analysis. The response matrix, typically estimated from Monte Carlo simulations, is not a perfect model but an estimate with its own statistical uncertainties and physical properties, such as detector efficiency. This exercise [@problem_id:3518218] challenges you to think like an experimentalist by designing a statistical test to check the consistency of a given response matrix and to develop a method for renormalizing it to match known efficiencies, a vital skill for handling systematic uncertainties.", "problem": "In iterative unfolding with Bayes' theorem in computational high-energy physics, the detector response is summarized by a response matrix $A_{ji}$ that encodes the probability for a particle produced in truth bin $i$ to be reconstructed in detector bin $j$. When the probability of any reconstruction from truth bin $i$ is less than unity, the detector efficiency is $\\epsilon_i = \\sum_{j} A_{ji} \\leq 1$, and the missing probability is $1 - \\epsilon_i$. Bayesian iterative unfolding updates a prior truth distribution $p_i$ using Bayes' theorem and the response $A_{ji}$, and the forward model predicts a reconstructed distribution $r_j = \\sum_{i} A_{ji} p_i$.\n\nYou are given a $3 \\times 3$ response matrix $A_{ji}$, a target efficiency vector $\\epsilon_i$, Monte Carlo (MC) sample sizes $N_i$ used to estimate each column of $A_{ji}$, and a normalized prior $p_i$:\n- Columns $i=1,2,3$ correspond to truth bins; rows $j=1,2,3$ correspond to reconstructed bins.\n- The matrix entries are\n$$\nA \\equiv \\begin{pmatrix}\n0.56 & 0.096 & 0.05 \\\\\n0.16 & 0.416 & 0.125 \\\\\n0.08 & 0.128 & 0.325\n\\end{pmatrix},\n$$\nso that the column sums are $S_i \\equiv \\sum_{j} A_{ji} = \\{0.80,\\,0.64,\\,0.50\\}$.\n- The target efficiencies are $\\epsilon_i = \\{0.78,\\,0.65,\\,0.52\\}$.\n- The MC sample sizes per truth bin are $N_i = \\{5000,\\,4000,\\,4500\\}$.\n- The prior truth distribution is $p_i = \\{0.5,\\,0.3,\\,0.2\\}$.\n\nStarting only from the fundamental probability definitions of a response matrix and Bayes' theorem, do the following:\n1. Derive why a consistency condition for a physically valid response is $\\sum_{j} A_{ji} = \\epsilon_i$ and why the forward-modeled reconstructed distribution is $r_j = \\sum_{i} A_{ji} p_i$. Explain why the sum $\\sum_{j} r_j$ equals $\\sum_{i} \\epsilon_i p_i$.\n2. Design a Gaussian large-sample test statistic that checks the column-sum consistency against the target efficiencies, using the MC sampling variance appropriate to estimating a binomial efficiency from $N_i$ truth-level trials for each $i$. Explicitly write your test statistic $T$ in terms of $S_i$, $\\epsilon_i$, and $N_i$.\n3. Compute the forward-modeled reconstructed distribution $r_j = \\sum_{i} A_{ji} p_i$ using the provided $A_{ji}$ and $p_i$, and verify its sum against $\\sum_{i} S_i p_i$ and against $\\sum_{i} \\epsilon_i p_i$.\n4. Propose a column-wise renormalization $A'_{ji}$ that enforces $\\sum_{j} A'_{ji} = \\epsilon_i$ exactly while leaving all within-column migration fractions $A_{ji}/\\sum_{j} A_{ji}$ unchanged. Provide the explicit multiplicative factors in terms of $A_{ji}$, $S_i$, and $\\epsilon_i$.\n5. Using your test statistic from part 2, evaluate $T$ numerically with the given data.\n\nReport as your final answer the numerical value of $T$, rounded to four significant figures, without units.", "solution": "**1. Derivations**\n\nLet $T_i$ be the event that a particle is generated in truth bin $i$, and $R_j$ be the event that it is reconstructed in detector bin $j$. The problem defines the response matrix as $A_{ji} = P(R_j | T_i)$ and the prior truth distribution as $p_i = P(T_i)$.\n\nThe detector efficiency for truth bin $i$, $\\epsilon_i$, is the probability that a particle from bin $i$ is reconstructed in *any* detector bin. Since the reconstruction bins $R_j$ are mutually exclusive outcomes, the total probability is the sum of the individual probabilities:\n$$ \\epsilon_i = P(R_1 \\cup R_2 \\cup \\dots | T_i) = \\sum_{j} P(R_j | T_i) = \\sum_{j} A_{ji} $$\nThis confirms the first identity.\n\nThe forward-modeled reconstructed distribution, $r_j$, describes the unconditional probability of reconstruction in detector bin $j$. Using the law of total probability, we sum over all possible true origins $T_i$:\n$$ r_j = P(R_j) = \\sum_{i} P(R_j \\cap T_i) $$\nUsing the definition of conditional probability, $P(R_j \\cap T_i) = P(R_j | T_i) P(T_i) = A_{ji} p_i$. Substituting this gives the forward model:\n$$ r_j = \\sum_{i} A_{ji} p_i $$\nThis confirms the second identity.\n\nTo find the sum of the reconstructed distribution, we sum $r_j$ over all $j$ and substitute its definition:\n$$ \\sum_{j} r_j = \\sum_{j} \\left( \\sum_{i} A_{ji} p_i \\right) $$\nAs the sums are finite, we can interchange the order of summation:\n$$ \\sum_{j} r_j = \\sum_{i} \\sum_{j} A_{ji} p_i = \\sum_{i} p_i \\left( \\sum_{j} A_{ji} \\right) $$\nFrom our first derivation, we know that $\\sum_{j} A_{ji} = \\epsilon_i$. Therefore,\n$$ \\sum_{j} r_j = \\sum_{i} \\epsilon_i p_i $$\nThis shows that the total probability of reconstruction is the efficiency for each truth bin, $\\epsilon_i$, weighted by the probability of that truth bin, $p_i$.\n\n**2. Test Statistic Design**\n\nThe column sums $S_i = \\sum_j A_{ji}$ are estimates of the true efficiencies, which we hypothesize to be $\\epsilon_i$. These estimates are derived from a Monte Carlo simulation of size $N_i$ for each truth bin $i$. For a given truth bin $i$, each of the $N_i$ events is either reconstructed (a success) or not. This is a binomial process. The number of successes, $K_i$, follows a binomial distribution $B(N_i, \\epsilon_i)$ under the null hypothesis. The estimated efficiency is $S_i = K_i / N_i$.\n\nFor a large sample size $N_i$, the binomial distribution is well-approximated by a Gaussian distribution. The mean of the estimator $S_i$ is $E[S_i] = \\epsilon_i$ and its variance is $\\text{Var}(S_i) = \\frac{\\epsilon_i(1-\\epsilon_i)}{N_i}$.\n\nWe can define a standardized variable, or pull, for each truth bin $i$, which will follow a standard normal distribution $N(0,1)$ under the null hypothesis:\n$$ z_i = \\frac{S_i - E[S_i]}{\\sqrt{\\text{Var}(S_i)}} = \\frac{S_i - \\epsilon_i}{\\sqrt{\\epsilon_i(1-\\epsilon_i)/N_i}} $$\nAssuming the Monte Carlo simulations for each truth bin are statistically independent, the pulls $z_i$ are independent. A test statistic $T$ for the overall consistency can be formed by summing the squares of these pulls. The resulting statistic follows a chi-squared ($\\chi^2$) distribution with a number of degrees of freedom equal to the number of terms (here, $3$).\n$$ T = \\sum_{i=1}^{3} z_i^2 = \\sum_{i=1}^{3} \\frac{(S_i - \\epsilon_i)^2}{\\epsilon_i(1-\\epsilon_i)/N_i} = \\sum_{i=1}^{3} \\frac{N_i (S_i - \\epsilon_i)^2}{\\epsilon_i(1-\\epsilon_i)} $$\nThis provides the required Gaussian large-sample test statistic.\n\n**3. Forward-Modeled Distribution Calculation**\n\nWe compute the reconstructed distribution $r_j = \\sum_{i} A_{ji} p_i$ using matrix multiplication $r = Ap$:\n$$\n\\begin{pmatrix} r_1 \\\\ r_2 \\\\ r_3 \\end{pmatrix}\n=\n\\begin{pmatrix}\n0.56 & 0.096 & 0.05 \\\\\n0.16 & 0.416 & 0.125 \\\\\n0.08 & 0.128 & 0.325\n\\end{pmatrix}\n\\begin{pmatrix} 0.5 \\\\ 0.3 \\\\ 0.2 \\end{pmatrix}\n$$\n$r_1 = (0.56)(0.5) + (0.096)(0.3) + (0.05)(0.2) = 0.28 + 0.0288 + 0.01 = 0.3188$\n$r_2 = (0.16)(0.5) + (0.416)(0.3) + (0.125)(0.2) = 0.08 + 0.1248 + 0.025 = 0.2298$\n$r_3 = (0.08)(0.5) + (0.128)(0.3) + (0.325)(0.2) = 0.04 + 0.0384 + 0.065 = 0.1434$\nThe reconstructed distribution is $r_j = \\{0.3188, 0.2298, 0.1434\\}$.\n\nThe sum is $\\sum_{j} r_j = 0.3188 + 0.2298 + 0.1434 = 0.692$.\n\nWe verify this against $\\sum_i S_i p_i$. The sums $S_i$ are the efficiencies of the provided matrix $A$. As derived in part 1, $\\sum_j r_j$ must equal $\\sum_i S_i p_i$.\n$\\sum_i S_i p_i = S_1 p_1 + S_2 p_2 + S_3 p_3 = (0.80)(0.5) + (0.64)(0.3) + (0.50)(0.2) = 0.4 + 0.192 + 0.1 = 0.692$.\nThe verification holds: $\\sum_j r_j = \\sum_i S_i p_i = 0.692$.\n\nWe now check this against $\\sum_i \\epsilon_i p_i$ using the target efficiencies:\n$\\sum_i \\epsilon_i p_i = \\epsilon_1 p_1 + \\epsilon_2 p_2 + \\epsilon_3 p_3 = (0.78)(0.5) + (0.65)(0.3) + (0.52)(0.2) = 0.39 + 0.195 + 0.104 = 0.689$.\nAs expected, $\\sum_j r_j \\neq \\sum_i \\epsilon_i p_i$ because the given matrix $A$ has column sums $S_i$, not $\\epsilon_i$. The discrepancy ($0.692$ vs. $0.689$) reflects the difference between the MC-estimated efficiencies and the target efficiencies.\n\n**4. Column-wise Renormalization**\n\nWe seek a new matrix $A'_{ji}$ such that its column sums are $\\sum_j A'_{ji} = \\epsilon_i$, while preserving the within-column migration fractions. The migration fraction for column $i$ is the relative probability of being reconstructed in bin $j$, given reconstruction occurred, which is $f_{ji} = A_{ji} / (\\sum_k A_{ki}) = A_{ji}/S_i$.\n\nWe enforce this condition on the new matrix:\n$$ \\frac{A'_{ji}}{ \\sum_k A'_{ki} } = \\frac{A_{ji}}{S_i} $$\nSince we require $\\sum_k A'_{ki} = \\epsilon_i$, we have:\n$$ \\frac{A'_{ji}}{\\epsilon_i} = \\frac{A_{ji}}{S_i} $$\nSolving for $A'_{ji}$ gives the renormalized matrix elements:\n$$ A'_{ji} = A_{ji} \\left( \\frac{\\epsilon_i}{S_i} \\right) $$\nThe multiplicative factor for each element in column $i$ is $\\frac{\\epsilon_i}{S_i}$, or written in full, $\\frac{\\epsilon_i}{\\sum_{j} A_{ji}}$.\n\n**5. Numerical Evaluation of Test Statistic $T$**\n\nWe compute the value of the test statistic $T$ using the formula from part 2:\n$$ T = \\sum_{i=1}^{3} \\frac{N_i (S_i - \\epsilon_i)^2}{\\epsilon_i(1-\\epsilon_i)} $$\nGiven data:\n$S = \\{0.80, 0.64, 0.50\\}$\n$\\epsilon = \\{0.78, 0.65, 0.52\\}$\n$N = \\{5000, 4000, 4500\\}$\n\nFor $i=1$:\n$$ T_1 = \\frac{5000 (0.80-0.78)^2}{0.78(1-0.78)} = \\frac{5000 (0.02)^2}{0.78(0.22)} = \\frac{5000(0.0004)}{0.1716} = \\frac{2}{0.1716} \\approx 11.6550 $$\nFor $i=2$:\n$$ T_2 = \\frac{4000 (0.64-0.65)^2}{0.65(1-0.65)} = \\frac{4000 (-0.01)^2}{0.65(0.35)} = \\frac{4000(0.0001)}{0.2275} = \\frac{0.4}{0.2275} \\approx 1.7582 $$\nFor $i=3$:\n$$ T_3 = \\frac{4500 (0.50-0.52)^2}{0.52(1-0.52)} = \\frac{4500 (-0.02)^2}{0.52(0.48)} = \\frac{4500(0.0004)}{0.2496} = \\frac{1.8}{0.2496} \\approx 7.2115 $$\nThe total test statistic is the sum of these terms:\n$$ T = T_1 + T_2 + T_3 \\approx 11.6550 + 1.7582 + 7.2115 = 20.6247 $$\nRounding to four significant figures, $T = 20.62$.", "answer": "$$\\boxed{20.62}$$", "id": "3518218"}]}