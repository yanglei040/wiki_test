{"hands_on_practices": [{"introduction": "Approximate Bayesian Computation (ABC) is one of the foundational methods in simulation-based inference, relying on the intuitive principle of accepting simulated parameters if their corresponding data is \"close\" to observation. This exercise moves beyond the purely algorithmic view of ABC to explore its exact analytical consequences in a fundamental counting experiment scenario [@problem_id:3536607]. By deriving the ABC posterior mean in closed form, you will gain a deeper understanding of how the tolerance parameter $\\epsilon$ implicitly defines an effective likelihood, connecting the simple algorithm to its formal Bayesian interpretation.", "problem": "Consider a toy High-Energy Physics (HEP) cross-section measurement scenario in which the summary statistic $s(x)$ is the number of selected jets in an event sample. Model the data-generating process as follows: the summary $s(x)$ is a nonnegative integer count $k$ drawn from a Poisson distribution with rate parameter $\\theta$, namely $k \\sim \\mathrm{Poisson}(\\theta)$, where $\\theta \\ge 0$ is the unknown signal rate. Assume the improper uniform prior $p(\\theta) \\propto 1$ for $\\theta \\in [0, \\infty)$.\n\nUsing Approximate Bayesian Computation (ABC), define the acceptance rule for simulated data $k_{\\mathrm{sim}}$ relative to the observed count $k_{\\mathrm{obs}}$ as $|k_{\\mathrm{sim}} - k_{\\mathrm{obs}}| \\le \\epsilon$, where $\\epsilon \\ge 0$ is a tolerance. The ABC posterior is then the distribution over $\\theta$ obtained by conditioning on the event that a simulated summary falls within the tolerance of the observed summary.\n\nStarting from the fundamental definitions of the Poisson model and Bayes' theorem, and without invoking any pre-derived target formulas, derive the ABC posterior mean $\\mathbb{E}[\\theta \\mid |K - k_{\\mathrm{obs}}| \\le \\epsilon]$ in closed form as a function of the observed count $k_{\\mathrm{obs}}$ and the tolerance $\\epsilon$. The derivation must be based on first principles and valid manipulations of integrals that follow from the Poisson model and properties of the Gamma function.\n\nYour program must implement this derived expression to compute the ABC posterior mean for each test case below. No simulation is permitted in the program; the result must be computed using the closed-form expression you derive.\n\nTest suite:\n- $(k_{\\mathrm{obs}}, \\epsilon) = (12, 0)$\n- $(k_{\\mathrm{obs}}, \\epsilon) = (0, 0.5)$\n- $(k_{\\mathrm{obs}}, \\epsilon) = (7, 1.0)$\n- $(k_{\\mathrm{obs}}, \\epsilon) = (5, 0.7)$\n- $(k_{\\mathrm{obs}}, \\epsilon) = (2, 3.0)$\n- $(k_{\\mathrm{obs}}, \\epsilon) = (1, 10.0)$\n\nAll quantities are dimensionless counts or rates, so no physical units are required. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[r_1,r_2,r_3]$), where each $r_i$ is the ABC posterior mean for the corresponding test case, expressed as a floating-point number.", "solution": "The problem asks for a closed-form derivation of the Approximate Bayesian Computation (ABC) posterior mean for the rate parameter $\\theta$ of a Poisson process. The derivation must start from first principles.\n\nLet $k_{\\mathrm{obs}}$ be the observed non-negative integer count and $\\epsilon \\ge 0$ be the tolerance. The data-generating process is a Poisson distribution, $K \\sim \\mathrm{Poisson}(\\theta)$, with probability mass function $P(K=k \\mid \\theta) = \\frac{e^{-\\theta}\\theta^k}{k!}$ for $k \\in \\{0, 1, 2, \\dots\\}$. The prior on the unknown parameter $\\theta$ is an improper uniform prior, $p(\\theta) \\propto 1$ for $\\theta \\in [0, \\infty)$.\n\nThe ABC acceptance criterion is defined as $|k_{\\mathrm{sim}} - k_{\\mathrm{obs}}| \\le \\epsilon$, where $k_{\\mathrm{sim}}$ is a simulated data point drawn from the model, $k_{\\mathrm{sim}} \\sim \\mathrm{Poisson}(\\theta)$. Let $A$ denote the acceptance event, $A = \\{ k_{\\mathrm{sim}} : |k_{\\mathrm{sim}} - k_{\\mathrm{obs}}| \\le \\epsilon \\}$. The ABC posterior distribution, $p_{\\mathrm{ABC}}(\\theta \\mid k_{\\mathrm{obs}})$, is the exact posterior distribution of $\\theta$ conditional on the event $A$ having occurred.\n\nBy Bayes' theorem, the posterior is proportional to the likelihood multiplied by the prior:\n$$p_{\\mathrm{ABC}}(\\theta \\mid k_{\\mathrm{obs}}) \\propto P(A \\mid \\theta) p(\\theta)$$\n\nThe prior is given as $p(\\theta) \\propto 1$. The term $P(A \\mid \\theta)$ is the \"ABC likelihood,\" which is the probability of drawing a simulated data point $k_{\\mathrm{sim}}$ that satisfies the acceptance criterion, given a value of $\\theta$. This is calculated by summing the Poisson probabilities for all integer counts $k$ that fall within the acceptance region:\n$$P(A \\mid \\theta) = \\sum_{k \\in \\mathbb{Z}_{\\ge 0} \\text{ s.t. } |k - k_{\\mathrm{obs}}| \\le \\epsilon} P(K=k \\mid \\theta)$$\n\nThe condition $|k - k_{\\mathrm{obs}}| \\le \\epsilon$ is equivalent to $k_{\\mathrm{obs}} - \\epsilon \\le k \\le k_{\\mathrm{obs}} + \\epsilon$. Since $k$ must be a non-negative integer, the set of accepted counts is defined by the integer bounds:\n$$k_{\\min} = \\lceil \\max(0, k_{\\mathrm{obs}} - \\epsilon) \\rceil$$\n$$k_{\\max} = \\lfloor k_{\\mathrm{obs}} + \\epsilon \\rfloor$$\nThe ABC likelihood can now be written as a sum over this range:\n$$P(A \\mid \\theta) = \\sum_{k=k_{\\min}}^{k_{\\max}} \\frac{e^{-\\theta}\\theta^k}{k!}$$\nIf $k_{\\min} > k_{\\max}$, the acceptance set is empty and the sum is zero. We assume for the derivation that $k_{\\min} \\le k_{\\max}$.\n\nThe unnormalized ABC posterior is therefore:\n$$p_{\\mathrm{ABC}}(\\theta \\mid k_{\\mathrm{obs}}) \\propto \\left( \\sum_{k=k_{\\min}}^{k_{\\max}} \\frac{e^{-\\theta}\\theta^k}{k!} \\right) \\cdot 1 = e^{-\\theta} \\sum_{k=k_{\\min}}^{k_{\\max}} \\frac{\\theta^k}{k!}$$\n\nThe ABC posterior mean, $\\mathbb{E}_{\\mathrm{ABC}}[\\theta]$, is the expectation of $\\theta$ with respect to this posterior distribution:\n$$\\mathbb{E}_{\\mathrm{ABC}}[\\theta] = \\mathbb{E}[\\theta \\mid A] = \\frac{\\int_0^\\infty \\theta \\cdot p_{\\mathrm{ABC}}(\\theta \\mid k_{\\mathrm{obs}}) \\,d\\theta}{\\int_0^\\infty p_{\\mathrm{ABC}}(\\theta \\mid k_{\\mathrm{obs}}) \\,d\\theta}$$\n\nLet's evaluate the denominator (the normalization constant, $Z$) first:\n$$Z = \\int_0^\\infty e^{-\\theta} \\sum_{k=k_{\\min}}^{k_{\\max}} \\frac{\\theta^k}{k!} \\,d\\theta$$\nSince the sum is finite, we can interchange the order of summation and integration:\n$$Z = \\sum_{k=k_{\\min}}^{k_{\\max}} \\frac{1}{k!} \\int_0^\\infty \\theta^k e^{-\\theta} \\,d\\theta$$\nThe integral is the definition of the Gamma function, $\\Gamma(z) = \\int_0^\\infty t^{z-1}e^{-t}\\,dt$. Specifically, $\\int_0^\\infty \\theta^k e^{-\\theta} \\,d\\theta = \\Gamma(k+1)$, and for integer $k$, $\\Gamma(k+1) = k!$.\nSubstituting this result back into the expression for $Z$:\n$$Z = \\sum_{k=k_{\\min}}^{k_{\\max}} \\frac{1}{k!} (k!) = \\sum_{k=k_{\\min}}^{k_{\\max}} 1 = k_{\\max} - k_{\\min} + 1$$\n\nNext, we evaluate the numerator, $N$:\n$$N = \\int_0^\\infty \\theta \\left( e^{-\\theta} \\sum_{k=k_{\\min}}^{k_{\\max}} \\frac{\\theta^k}{k!} \\right) \\,d\\theta = \\int_0^\\infty e^{-\\theta} \\sum_{k=k_{\\min}}^{k_{\\max}} \\frac{\\theta^{k+1}}{k!} \\,d\\theta$$\nAgain, we interchange summation and integration:\n$$N = \\sum_{k=k_{\\min}}^{k_{\\max}} \\frac{1}{k!} \\int_0^\\infty \\theta^{k+1} e^{-\\theta} \\,d\\theta$$\nThe integral is $\\Gamma(k+2) = (k+1)!$.\n$$N = \\sum_{k=k_{\\min}}^{k_{\\max}} \\frac{(k+1)!}{k!} = \\sum_{k=k_{\\min}}^{k_{\\max}} (k+1)$$\nThis sum is an arithmetic progression. Its value can be found by summing from $k=k_{\\min}$ to $k_{\\max}$ for both terms:\n$$N = \\left(\\sum_{k=k_{\\min}}^{k_{\\max}} k\\right) + \\left(\\sum_{k=k_{\\min}}^{k_{\\max}} 1\\right)$$\nThe first term is the sum of an arithmetic series, which is the number of terms times the average of the first and last term: $\\frac{(k_{\\max} - k_{\\min} + 1)(k_{\\min} + k_{\\max})}{2}$. The second term is simply $k_{\\max} - k_{\\min} + 1$.\n$$N = \\frac{(k_{\\max} - k_{\\min} + 1)(k_{\\min} + k_{\\max})}{2} + (k_{\\max} - k_{\\min} + 1)$$\n\nFinally, we compute the ABC posterior mean by dividing $N$ by $Z$:\n$$\\mathbb{E}_{\\mathrm{ABC}}[\\theta] = \\frac{N}{Z} = \\frac{\\frac{(k_{\\max} - k_{\\min} + 1)(k_{\\min} + k_{\\max})}{2} + (k_{\\max} - k_{\\min} + 1)}{k_{\\max} - k_{\\min} + 1}$$\n$$\\mathbb{E}_{\\mathrm{ABC}}[\\theta] = \\frac{k_{\\min} + k_{\\max}}{2} + 1$$\n\nThis is the final closed-form expression for the ABC posterior mean. It is the arithmetic mean of the minimum and maximum accepted integer counts, plus one. This expression will be implemented to solve the problem.\nThe required quantities are:\n$$k_{\\min} = \\lceil \\max(0, k_{\\mathrm{obs}} - \\epsilon) \\rceil$$\n$$k_{\\max} = \\lfloor k_{\\mathrm{obs}} + \\epsilon \\rfloor$$\n$$\\mathbb{E}[\\theta \\mid |K - k_{\\mathrm{obs}}| \\le \\epsilon] = \\frac{k_{\\min} + k_{\\max}}{2} + 1$$\nThis derivation is based entirely on first principles as required.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import gamma # Not used in final formula but part of environment.\n\ndef solve():\n    \"\"\"\n    Computes the ABC posterior mean for a series of test cases based on the\n    analytically derived closed-form expression.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (12, 0),\n        (0, 0.5),\n        (7, 1.0),\n        (5, 0.7),\n        (2, 3.0),\n        (1, 10.0),\n    ]\n\n    results = []\n    for k_obs, epsilon in test_cases:\n        # The derivation provides a closed-form solution for the ABC posterior mean.\n        # The formula is E[theta] = (k_min + k_max) / 2 + 1.\n        \n        # First, determine the bounds of the accepted integer counts k.\n        # The acceptance condition is |k - k_obs| <= epsilon, which is equivalent to\n        # k_obs - epsilon <= k <= k_obs + epsilon.\n        # Since k must be a non-negative integer, we have:\n        \n        # k_min is the smallest integer k >= 0 such that k >= k_obs - epsilon.\n        # This is found by taking the maximum of 0 and k_obs - epsilon, and then\n        # taking the ceiling to get the next integer.\n        k_min = int(np.ceil(np.maximum(0, k_obs - epsilon)))\n        \n        # k_max is the largest integer k such that k <= k_obs + epsilon.\n        # This is found by taking the floor.\n        k_max = int(np.floor(k_obs + epsilon))\n        \n        # The derived formula for the ABC posterior mean.\n        # It is assumed that k_min <= k_max, which is true for all test cases.\n        # If k_min > k_max, the acceptance set is empty, and the posterior would be undefined.\n        posterior_mean = (k_min + k_max) / 2.0 + 1.0\n        \n        results.append(posterior_mean)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3536607"}, {"introduction": "Modern simulation-based inference often leverages machine learning to overcome the inefficiencies of simpler methods like ABC. A key insight is that the likelihood ratio, a central quantity for statistical inference, can be learned by training a classifier to distinguish between data simulated under different parameter hypotheses. This foundational exercise provides the theoretical cornerstone for this powerful class of techniques [@problem_id:3536670]. By analytically deriving the precise relationship between an optimal classifier's output and the true likelihood ratio in a tractable Gaussian model, you will uncover exactly why classifiers are such an effective tool for likelihood-free inference.", "problem": "Consider the binary parameter inference setting foundational to simulation-based inference in computational high-energy physics, where a single observation $x \\in \\mathbb{R}$ is generated from a Normal distribution with unit variance and unknown mean parameter $\\theta \\in \\{\\theta_{0}, \\theta_{1}\\}$. Specifically, assume the generative model $x \\sim \\mathcal{N}(\\theta, 1)$, with class-prior probabilities $\\pi_{0} = \\mathbb{P}(\\theta = \\theta_{0})$ and $\\pi_{1} = \\mathbb{P}(\\theta = \\theta_{1})$, satisfying $\\pi_{0} + \\pi_{1} = 1$ and $\\pi_{0}, \\pi_{1} \\in (0,1)$. In the likelihood-free paradigm, the Bayes-optimal logistic classifier trained to distinguish $\\theta_{0}$ from $\\theta_{1}$ under cross-entropy loss converges to the true posterior $s(x) = \\mathbb{P}(\\theta = \\theta_{1} \\mid x)$, whose logit is defined by $\\ell(x) = \\ln\\!\\big( s(x) / \\big(1 - s(x)\\big) \\big)$. Starting from first principles, namely Bayes’ theorem and the definition of the Normal distribution’s probability density function, derive the analytic form of the optimal logistic classifier’s logit $\\ell(x)$ as an explicit linear function of $x$, and use this derivation to establish its mapping to the exact likelihood ratio between the competing hypotheses $\\theta_{1}$ and $\\theta_{0}$. Express your final answer as a single closed-form analytic expression for $\\ell(x)$ in terms of $x$, $\\theta_{0}$, $\\theta_{1}$, $\\pi_{0}$, and $\\pi_{1}$. No numerical approximation is required; do not introduce any units. The final answer must be a single expression only.", "solution": "The scenario is binary parameter inference with a single scalar observation and two competing hypotheses for the mean of a Normal distribution with unit variance. The fundamental base is Bayes’ theorem and the Normal distribution’s probability density function. We first establish the Bayes-optimal classifier and its logit in terms of likelihoods and priors, and then compute the explicit form for the Normal model.\n\nBy Bayes’ theorem, the posterior probability that the parameter equals $\\theta_{1}$ given $x$ is\n$$\n\\mathbb{P}(\\theta = \\theta_{1} \\mid x)\n= \\frac{\\pi_{1} \\, p(x \\mid \\theta_{1})}{\\pi_{1} \\, p(x \\mid \\theta_{1}) + \\pi_{0} \\, p(x \\mid \\theta_{0})} \\, ,\n$$\nwhere $p(x \\mid \\theta)$ denotes the likelihood under the Normal model. Define the logistic classifier’s output $s(x) = \\mathbb{P}(\\theta = \\theta_{1} \\mid x)$ and its logit\n$$\n\\ell(x) = \\ln\\!\\left(\\frac{s(x)}{1 - s(x)}\\right) = \\ln\\!\\left(\\frac{\\mathbb{P}(\\theta = \\theta_{1} \\mid x)}{\\mathbb{P}(\\theta = \\theta_{0} \\mid x)}\\right) \\, .\n$$\nUsing Bayes’ theorem for the odds,\n$$\n\\frac{\\mathbb{P}(\\theta = \\theta_{1} \\mid x)}{\\mathbb{P}(\\theta = \\theta_{0} \\mid x)}\n= \\frac{\\pi_{1} \\, p(x \\mid \\theta_{1})}{\\pi_{0} \\, p(x \\mid \\theta_{0})} \\, ,\n$$\nso the logit is\n$$\n\\ell(x) = \\ln\\!\\left(\\frac{\\pi_{1}}{\\pi_{0}}\\right) + \\ln\\!\\left(\\frac{p(x \\mid \\theta_{1})}{p(x \\mid \\theta_{0})}\\right) \\, .\n$$\nThis exhibits the mapping to the exact likelihood ratio $\\Lambda(x) = \\frac{p(x \\mid \\theta_{1})}{p(x \\mid \\theta_{0})}$ via\n$$\n\\ell(x) = \\ln \\Lambda(x) + \\ln\\!\\left(\\frac{\\pi_{1}}{\\pi_{0}}\\right) \\, .\n$$\n\nWe now compute $\\ln \\Lambda(x)$ under the Normal model $x \\sim \\mathcal{N}(\\theta, 1)$. The density is\n$$\np(x \\mid \\theta) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\!\\left( -\\frac{(x - \\theta)^{2}}{2} \\right) \\, .\n$$\nThus,\n$$\n\\ln \\Lambda(x) = \\ln p(x \\mid \\theta_{1}) - \\ln p(x \\mid \\theta_{0})\n= -\\frac{(x - \\theta_{1})^{2}}{2} + \\frac{(x - \\theta_{0})^{2}}{2} \\, ,\n$$\nsince the $\\ln\\!\\left( \\frac{1}{\\sqrt{2\\pi}} \\right)$ terms cancel. Expand and simplify:\n$$\n-\\frac{(x - \\theta_{1})^{2}}{2} + \\frac{(x - \\theta_{0})^{2}}{2}\n= -\\frac{x^{2} - 2 x \\theta_{1} + \\theta_{1}^{2}}{2} + \\frac{x^{2} - 2 x \\theta_{0} + \\theta_{0}^{2}}{2} \\, ,\n$$\n$$\n= \\left( -\\frac{x^{2}}{2} + x \\theta_{1} - \\frac{\\theta_{1}^{2}}{2} \\right)\n+ \\left( \\frac{x^{2}}{2} - x \\theta_{0} + \\frac{\\theta_{0}^{2}}{2} \\right) \\, ,\n$$\n$$\n= x (\\theta_{1} - \\theta_{0}) - \\frac{\\theta_{1}^{2} - \\theta_{0}^{2}}{2} \\, .\n$$\nTherefore, the optimal logit is\n$$\n\\ell(x) = \\left[ x (\\theta_{1} - \\theta_{0}) - \\frac{\\theta_{1}^{2} - \\theta_{0}^{2}}{2} \\right] + \\ln\\!\\left(\\frac{\\pi_{1}}{\\pi_{0}}\\right) \\, .\n$$\n\nThis expression is linear in $x$ and consists of a slope $(\\theta_{1} - \\theta_{0})$ and an intercept $- \\frac{\\theta_{1}^{2} - \\theta_{0}^{2}}{2} + \\ln\\!\\left(\\frac{\\pi_{1}}{\\pi_{0}}\\right)$. The mapping to the likelihood ratio is explicit:\n$$\n\\ell(x) = \\ln \\Lambda(x) + \\ln\\!\\left(\\frac{\\pi_{1}}{\\pi_{0}}\\right) \\quad \\text{with} \\quad \\ln \\Lambda(x) = x (\\theta_{1} - \\theta_{0}) - \\frac{\\theta_{1}^{2} - \\theta_{0}^{2}}{2} \\, .\n$$\nHence the Bayes-optimal logistic classifier $s(x)$ is $s(x) = \\sigma(\\ell(x))$ with $\\sigma(u) = \\frac{1}{1 + \\exp(-u)}$, and the requested final analytic form of the logit is as above.", "answer": "$$\\boxed{x\\left(\\theta_{1}-\\theta_{0}\\right)-\\frac{\\theta_{1}^{2}-\\theta_{0}^{2}}{2}+\\ln\\!\\left(\\frac{\\pi_{1}}{\\pi_{0}}\\right)}$$", "id": "3536670"}, {"introduction": "While some inference tasks only require likelihood ratios, a complete Bayesian analysis aims to approximate the full posterior distribution, which can be challenging when it has a complex structure. Normalizing flows are powerful deep generative models that learn complex distributions by transforming a simple base density through an invertible, differentiable function. This practice problem addresses the common challenge of multimodal posteriors arising from detector ambiguities by guiding you through the construction of a mixture-of-flows model [@problem_id:3536610]. You will gain hands-on experience implementing the change-of-variables theorem and the robust numerical methods essential for deploying these state-of-the-art techniques.", "problem": "You are given a toy setting inspired by two-fold detector ambiguities in computational high-energy physics, in which an observed summary statistic induces a bimodal posterior over a latent kinematic parameter. In order to approximate this posterior in a likelihood-free manner, you must construct a mixture-of-flows model in one dimension and evaluate its log-likelihood at a set of synthetic test points.\n\nBegin from the following fundamental base:\n\n- Bayesian inference uses Bayes' theorem, stating that $p(\\theta \\mid s) \\propto p(s \\mid \\theta)\\,p(\\theta)$, where $s$ is an observed summary and $\\theta$ is a latent parameter. In ambiguous detector mappings (for example a two-fold sign ambiguity), $p(\\theta \\mid s)$ is typically multimodal.\n- The change-of-variables theorem for invertible transformations states that if $x = f(z)$ for an invertible, differentiable function $f$, and $z$ has density $p_Z(z)$, then the induced density on $x$ is $p_X(x) = p_Z(f^{-1}(x))\\,\\left\\lvert \\dfrac{d f^{-1}(x)}{d x} \\right\\rvert$.\n\nModel the bimodal posterior as a mixture of two one-dimensional normalizing flows, each transforming a base Gaussian $z \\sim \\mathcal{N}(0,\\sigma^2)$ into a component density. The $k$-th flow is defined as $x = f_k(z)$ with\n$$\nf_k(z) = a_k\\,z + b_k + c_k\\,\\tanh(d_k\\,z),\n$$\nwhere all parameters are real numbers and chosen such that $f_k$ is strictly increasing (hence invertible). The mixture-of-flows posterior approximation is\n$$\np(x) = \\sum_{k=1}^{2} w_k\\,p_k(x),\n$$\nwith mixture weights $w_k > 0$ satisfying $\\sum_k w_k = 1$, and $p_k(x)$ the density induced by $f_k$ from the base Gaussian via the change-of-variables theorem. The log-likelihood at a point $x$ is $\\log p(x)$.\n\nFor computational stability, compute $\\log p(x)$ using a numerically stable summation strategy for terms of the form $\\log\\left(\\sum_k w_k \\exp(\\ell_k)\\right)$, where $\\ell_k = \\log p_k(x)$.\n\nUse the following scientifically consistent parameterization that encodes a two-fold ambiguity by placing two symmetric modes:\n- Base Gaussian variance $\\sigma^2$ with $\\sigma = 1.0$.\n- Component $k=1$: $a_1 = 1.25$, $b_1 = +2.0$, $c_1 = 0.8$, $d_1 = 0.7$, weight $w_1 = 0.5$.\n- Component $k=2$: $a_2 = 1.25$, $b_2 = -2.0$, $c_2 = 0.8$, $d_2 = 0.7$, weight $w_2 = 0.5$.\n\nThe strict monotonicity needed for invertibility is guaranteed because the derivative\n$$\n\\frac{d f_k}{d z}(z) = a_k + c_k\\,d_k\\,\\operatorname{sech}^2(d_k\\,z)\n$$\nsatisfies $\\frac{d f_k}{d z}(z) \\ge a_k > 0$ for all $z$, given the above parameter values.\n\nImplement a numerical inversion routine for $f_k$ to obtain $z = f_k^{-1}(x)$ at arbitrary $x$, using a robust bracketing and bisection method suitable for strictly monotonic functions.\n\nTest Suite:\nEvaluate the log-likelihood $\\log p(x)$ at the following synthetic test points that probe different qualitative regimes:\n- Happy-path near modes: $x = -2.0$, $x = +2.0$.\n- Valley between modes: $x = 0.0$.\n- Moderate tails: $x = -4.0$, $x = +4.0$.\n- Far tail (edge case): $x = +9.0$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[result_1,result_2,\\dots]$). Each result must be a floating-point number representing $\\log p(x)$ for one test point, in the order listed above. No physical units apply in this problem; all quantities are dimensionless real numbers.", "solution": "The problem is valid. It is a well-posed, scientifically grounded problem in computational statistics, with direct relevance to simulation-based inference in high-energy physics. All necessary parameters and functional forms are provided, and the task is to implement a numerically robust algorithm to compute a specific quantity.\n\nThe objective is to compute the log-likelihood, $\\log p(x)$, for a series of test points $x$, where $p(x)$ is a bimodal probability density function modeled as a mixture of two one-dimensional normalizing flows.\n\nThe overall approach involves three main stages:\n1.  Deriving the analytical expression for the log-likelihood of a single flow component, $\\ell_k(x) = \\log p_k(x)$.\n2.  Developing a numerical method to compute the inverse of the flow transformation, $z = f_k^{-1}(x)$, which is a necessary input for $\\ell_k(x)$.\n3.  Combining the log-likelihoods of the two components into the final log-likelihood of the mixture model, $\\log p(x)$, using a numerically stable method.\n\n**1. Component Log-Likelihood**\n\nThe density $p_k(x)$ for the $k$-th component is obtained by applying the change-of-variables theorem to the transformation $x = f_k(z)$, where the base variable $z$ follows a Gaussian distribution $p_Z(z) = \\mathcal{N}(z \\mid 0, \\sigma^2)$. The formula is:\n$$\np_k(x) = p_Z(f_k^{-1}(x)) \\left| \\frac{d f_k^{-1}(x)}{d x} \\right|\n$$\nUsing the inverse function theorem, the Jacobian determinant can be expressed in terms of the derivative of the forward transformation $f_k(z)$:\n$$\n\\left| \\frac{d f_k^{-1}(x)}{d x} \\right| = \\left| \\left( \\left. \\frac{d f_k(z)}{dz} \\right|_{z=f_k^{-1}(x)} \\right)^{-1} \\right| = \\left( \\frac{d f_k}{dz}(z) \\right)^{-1}\n$$\nThe absolute value can be dropped because the problem specifies that $f_k(z)$ is strictly increasing, guaranteeing its derivative is positive.\n\nLetting $z = f_k^{-1}(x)$, the density is:\n$$\np_k(x) = p_Z(z) \\left( \\frac{d f_k}{dz}(z) \\right)^{-1}\n$$\nFor numerical stability, we work with the log-likelihood, $\\ell_k(x) = \\log p_k(x)$:\n$$\n\\ell_k(x) = \\log p_Z(z) - \\log\\left(\\frac{d f_k}{dz}(z)\\right)\n$$\nThe base distribution is Gaussian, $p_Z(z) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{z^2}{2\\sigma^2}\\right)$, so its logarithm is:\n$$\n\\log p_Z(z) = -\\frac{z^2}{2\\sigma^2} - \\frac{1}{2}\\log(2\\pi\\sigma^2)\n$$\nThe derivative of the transformation $f_k(z) = a_k z + b_k + c_k \\tanh(d_k z)$ is given as:\n$$\n\\frac{d f_k}{dz}(z) = a_k + c_k d_k \\operatorname{sech}^2(d_k z)\n$$\nSubstituting these expressions gives the full formula for the component log-likelihood:\n$$\n\\ell_k(x) = -\\frac{(f_k^{-1}(x))^2}{2\\sigma^2} - \\frac{1}{2}\\log(2\\pi\\sigma^2) - \\log\\left( a_k + c_k d_k \\operatorname{sech}^2(d_k f_k^{-1}(x)) \\right)\n$$\nThis expression depends on the value of $z = f_k^{-1}(x)$, which must be found numerically.\n\n**2. Numerical Inversion of the Flow Transformation**\n\nTo evaluate $\\ell_k(x)$ for a given $x$, we must first compute $z = f_k^{-1}(x)$ by solving the nonlinear equation $x = f_k(z)$ for $z$. This is equivalent to finding the root of the function $g_k(z; x) = f_k(z) - x = 0$.\n\nSince $f_k(z)$ is strictly monotonic and continuous, and its domain and range are $(-\\infty, \\infty)$, a unique root $z$ exists for any real number $x$. The problem specifies using a bisection method, which is a robust root-finding algorithm for monotonic functions. The algorithm requires an initial interval $[z_{low}, z_{high}]$ that brackets the root, i.e., where $g_k(z_{low}; x)$ and $g_k(z_{high}; x)$ have opposite signs. Due to the linear term $a_k z$ dominating for large $|z|$, a sufficiently large, fixed interval (e.g., $[-100, 100]$) can reliably bracket the root for the range of $x$ values considered. The bisection algorithm iteratively halves the interval while keeping the root bracketed, converging to the root with a predictable error reduction.\n\n**3. Mixture Model Log-Likelihood**\n\nThe total probability density $p(x)$ is a weighted sum of the component densities:\n$$\np(x) = \\sum_{k=1}^{2} w_k p_k(x)\n$$\nThe total log-likelihood is $\\log p(x) = \\log\\left(\\sum_{k=1}^{2} w_k p_k(x)\\right)$. A naive computation of this sum can be numerically unstable if the values of $p_k(x)$ are very large or very small. To avoid floating-point overflow or underflow, we use the log-sum-exp trick. We first rewrite the sum in terms of the component log-likelihoods $\\ell_k(x)$:\n$$\n\\log p(x) = \\log\\left(\\sum_{k=1}^{2} w_k e^{\\ell_k(x)}\\right) = \\log\\left(\\sum_{k=1}^{2} e^{\\log w_k + \\ell_k(x)}\\right)\n$$\nLet $y_k = \\log w_k + \\ell_k(x)$. The expression becomes $\\log(\\sum_k e^{y_k})$. The log-sum-exp trick is to factor out the largest term:\n$$\n\\log\\left(\\sum_{k=1}^{2} e^{y_k}\\right) = y_{\\max} + \\log\\left(\\sum_{k=1}^{2} e^{y_k - y_{\\max}}\\right) = y_{\\max} + \\log\\left(e^{y_1 - y_{\\max}} + e^{y_2 - y_{\\max}}\\right)\n$$\nwhere $y_{\\max} = \\max(y_1, y_2)$. This formulation ensures that the arguments of the exponential function are less than or equal to $0$, preventing overflow and loss of precision.\n\n**Summary of the Algorithm**\n\nFor each test point $x$:\n1.  Initialize an array to hold terms $y_k = \\log w_k + \\ell_k(x)$.\n2.  For each component $k=1, 2$:\n    a. Use the bisection method to numerically solve $f_k(z) - x = 0$ to find $z_k = f_k^{-1}(x)$.\n    b. Calculate the component log-likelihood $\\ell_k(x)$ using the derived formula with $z_k$.\n    c. Compute $y_k = \\log(w_k) + \\ell_k(x)$.\n3.  Compute the final log-likelihood $\\log p(x)$ by applying the log-sum-exp function to the array of $y_k$ values.\n4.  Store the result.\n\nThis procedure will be repeated for all test points provided in the problem statement.", "answer": "```python\nimport numpy as np\nfrom scipy.special import logsumexp\n\ndef solve():\n    \"\"\"\n    Solves the problem of evaluating the log-likelihood of a mixture-of-flows model\n    at a set of test points, as specified in the problem statement.\n    \"\"\"\n\n    # --- Problem Parameters ---\n    # Constant for log(2*pi)\n    LOG_2PI = np.log(2 * np.pi)\n\n    # Base Gaussian standard deviation\n    SIGMA = 1.0\n\n    # Parameters for the two flow components\n    PARAMS = {\n        1: {'a': 1.25, 'b': 2.0, 'c': 0.8, 'd': 0.7, 'w': 0.5},\n        2: {'a': 1.25, 'b': -2.0, 'c': 0.8, 'd': 0.7, 'w': 0.5},\n    }\n\n    # Test points to evaluate\n    test_points = [-2.0, 2.0, 0.0, -4.0, 4.0, 9.0]\n\n    # --- Core Functions ---\n\n    def f_k(z, k):\n        \"\"\"The forward transformation f_k(z).\"\"\"\n        p = PARAMS[k]\n        return p['a'] * z + p['b'] + p['c'] * np.tanh(p['d'] * z)\n\n    def dfk_dz(z, k):\n        \"\"\"The derivative df_k/dz.\"\"\"\n        p = PARAMS[k]\n        # sech^2(x) = 1 / cosh^2(x)\n        sech_sq = 1.0 / np.cosh(p['d'] * z)**2\n        return p['a'] + p['c'] * p['d'] * sech_sq\n\n    def invert_fk(x, k, tol=1e-15, max_iter=100, z_bracket=(-100.0, 100.0)):\n        \"\"\"\n        Finds z = f_k^{-1}(x) by solving f_k(z) - x = 0 using bisection.\n        A wide, fixed bracket is sufficient for this monotonic function.\n        \"\"\"\n        z_low, z_high = z_bracket\n        g_low = f_k(z_low, k) - x\n        \n        # Assumption: The root is bracketed. For this problem's function and\n        # a wide bracket, this is a safe assumption.\n        \n        for _ in range(max_iter):\n            if (z_high - z_low) < tol:\n                break\n            \n            z_mid = (z_low + z_high) / 2.0\n            g_mid = f_k(z_mid, k) - x\n            \n            if np.sign(g_mid) == np.sign(g_low):\n                z_low = z_mid\n                g_low = g_mid\n            else:\n                z_high = z_mid\n                \n        return (z_low + z_high) / 2.0\n\n    def log_p_k(x, k):\n        \"\"\"Computes the log-likelihood log p_k(x) for a single component.\"\"\"\n        # Step 1: Numerically find z = f_k^{-1}(x)\n        z = invert_fk(x, k)\n        \n        # Step 2: Compute log of base density p_Z(z)\n        log_p_base = -0.5 * LOG_2PI - (z**2) / (2 * SIGMA**2)\n        \n        # Step 3: Compute log of the Jacobian determinant of the inverse transform\n        # This is -log(derivative of the forward transform)\n        log_det_J_inv = -np.log(dfk_dz(z, k))\n        \n        # The component log-likelihood is the sum of these two terms\n        return log_p_base + log_det_J_inv\n\n    def log_p(x):\n        \"\"\"\n        Computes the total log-likelihood log p(x) for the mixture model\n        using the log-sum-exp trick for numerical stability.\n        \"\"\"\n        keys = sorted(PARAMS.keys())\n        \n        # Compute log(w_k) + log(p_k(x)) for each component\n        y_k = np.array([\n            np.log(PARAMS[k]['w']) + log_p_k(x, k) for k in keys\n        ])\n        \n        # Use scipy's logsumexp to compute log(sum(exp(y_k)))\n        return logsumexp(y_k)\n\n    # --- Main Execution ---\n    results = [log_p(x) for x in test_points]\n    \n    # Format and print the final output\n    print(f\"[{','.join(f'{r:.8f}' for r in results)}]\")\n\nsolve()\n```", "id": "3536610"}]}