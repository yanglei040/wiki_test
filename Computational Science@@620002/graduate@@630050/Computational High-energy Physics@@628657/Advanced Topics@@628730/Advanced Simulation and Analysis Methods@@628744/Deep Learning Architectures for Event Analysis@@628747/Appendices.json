{"hands_on_practices": [{"introduction": "Applying Convolutional Neural Networks (CNNs) to jet 'images' was an early and influential approach for event analysis in high-energy physics. This practice grounds this concept by having you perform a manual cross-correlation, the core operation of a convolutional layer, on a simplified jet image. By completing this exercise [@problem_id:3510623], you will gain a concrete understanding of how CNNs extract local features and how architectural parameters like receptive field size correspond directly to physical angular scales defined by the pseudorapidity-azimuth ($\\eta$-$\\phi$) plane.", "problem": "You are analyzing a jet substructure classifier built with a Convolutional Neural Network (CNN) on a discretized pseudorapidity–azimuth plane used in high-energy physics event analysis. The jet is centered at pseudorapidity $\\eta \\approx 0$, and you use a local $3 \\times 3$ patch of a jet image with unit-normalized, dimensionless intensities:\n$$\nI \\;=\\; \\begin{pmatrix}\n0.12 & 0.08 & 0.05\\\\\n0.20 & 0.15 & 0.10\\\\\n0.18 & 0.12 & 0.06\n\\end{pmatrix}.\n$$\nA learned $2 \\times 2$ kernel, which approximates an oriented contrast operator, is given by\n$$\nK \\;=\\; \\begin{pmatrix}\n0.5 & -0.25\\\\\n-0.5 & 0.25\n\\end{pmatrix}.\n$$\nIn the forward pass of the CNN, adopt the standard deep learning convention that the layer performs discrete cross-correlation (no kernel flipping), with stride $s=1$ and no padding. The output is the valid $2 \\times 2$ activation map $Y$ with entries\n$$\nY_{i,j} \\;=\\; \\sum_{u=0}^{1}\\sum_{v=0}^{1} K_{u,v}\\, I_{i+u,\\,j+v}, \\quad \\text{for } i,j \\in \\{1,2\\}.\n$$\nAssume a square pixelization of the $\\eta$–$\\phi$ plane with $\\Delta \\eta = 0.1$ and $\\Delta \\phi = 0.1$, where $\\phi$ is measured in radians. In the standard jet analysis metric, the angular distance is defined by $\\Delta R = \\sqrt{(\\Delta \\eta)^{2} + (\\Delta \\phi)^{2}}$. For a single activation of $Y$, the receptive field is the corresponding $2 \\times 2$ input patch; define its angular span $\\Delta R_{\\text{span}}$ as the Euclidean length of the diagonal of this receptive field in the $\\eta$–$\\phi$ plane.\n\nCompute the following in order:\n- The full $2 \\times 2$ activation map $Y$.\n- The scalar $S$ equal to the sum of all entries of $Y$.\n- The angular span $\\Delta R_{\\text{span}}$ of a single receptive field.\n- The final scalar $P \\equiv S \\times \\Delta R_{\\text{span}}$.\n\nExpress your final scalar $P$ as a pure number (dimensionless) rounded to four significant figures. All intermediate quantities may be left exact until the final rounding instruction is applied. Angles are in radians.", "solution": "The problem requires calculating the activation map $Y$, its sum $S$, the receptive field's angular span $\\Delta R_{\\text{span}}$, and their product $P = S \\times \\Delta R_{\\text{span}}$.\n\n**1. Compute the Activation Map $Y$**\n\nThe cross-correlation operation $Y_{i,j} = \\sum_{u=0}^{1}\\sum_{v=0}^{1} K_{u,v}\\, I_{i+u,\\,j+v}$ is applied to the input image $I$ with the kernel $K$.\n$$\nI = \\begin{pmatrix} 0.12 & 0.08 & 0.05\\\\ 0.20 & 0.15 & 0.10\\\\ 0.18 & 0.12 & 0.06 \\end{pmatrix}, \\quad K = \\begin{pmatrix} 0.5 & -0.25\\\\ -0.5 & 0.25 \\end{pmatrix}\n$$\n- $Y_{1,1} = (0.5)(0.12) + (-0.25)(0.08) + (-0.5)(0.20) + (0.25)(0.15) = 0.06 - 0.02 - 0.10 + 0.0375 = -0.0225$\n- $Y_{1,2} = (0.5)(0.08) + (-0.25)(0.05) + (-0.5)(0.15) + (0.25)(0.10) = 0.04 - 0.0125 - 0.075 + 0.025 = -0.0225$\n- $Y_{2,1} = (0.5)(0.20) + (-0.25)(0.15) + (-0.5)(0.18) + (0.25)(0.12) = 0.10 - 0.0375 - 0.09 + 0.03 = 0.0025$\n- $Y_{2,2} = (0.5)(0.15) + (-0.25)(0.10) + (-0.5)(0.12) + (0.25)(0.06) = 0.075 - 0.025 - 0.06 + 0.015 = 0.0050$\n\nSo, the activation map is $Y = \\begin{pmatrix} -0.0225 & -0.0225 \\\\ 0.0025 & 0.0050 \\end{pmatrix}$.\n\n**2. Compute the Sum $S$**\n\nThe sum of all entries in $Y$ is:\n$$S = -0.0225 - 0.0225 + 0.0025 + 0.0050 = -0.0375$$\n\n**3. Compute the Angular Span $\\Delta R_{\\text{span}}$**\n\nThe receptive field is a $2 \\times 2$ patch of pixels. Each pixel spans $\\Delta\\eta = 0.1$ and $\\Delta\\phi = 0.1$. The total area covered is a rectangle in the $\\eta$-$\\phi$ plane with side lengths $L_{\\eta} = 2 \\times \\Delta\\eta = 0.2$ and $L_{\\phi} = 2 \\times \\Delta\\phi = 0.2$. The angular span is the length of this rectangle's diagonal:\n$$\\Delta R_{\\text{span}} = \\sqrt{L_{\\eta}^2 + L_{\\phi}^2} = \\sqrt{(0.2)^2 + (0.2)^2} = \\sqrt{0.04 + 0.04} = \\sqrt{0.08} = 0.2\\sqrt{2}$$\n\n**4. Compute the Final Scalar $P$**\n\nThe final product is $P = S \\times \\Delta R_{\\text{span}}$.\n$$P = -0.0375 \\times 0.2\\sqrt{2} = -0.0075\\sqrt{2} \\approx -0.0106066...$$\nRounding to four significant figures, we get $P \\approx -0.01061$.", "answer": "$$\n\\boxed{-0.01061}\n$$", "id": "3510623"}, {"introduction": "While CNNs are powerful, particle events are more fundamentally represented as sets or graphs rather than regular grids. Graph Neural Networks (GNNs) are designed to operate directly on this irregular data structure through a process called message passing. This exercise [@problem_id:3510657] provides a hands-on calculation of a single message-passing step, illustrating the core computational primitive of a GNN and demonstrating how to explicitly encode physical symmetries into the network design.", "problem": "A graph neural network for event-level analysis in computational high-energy physics operates on a directed complete graph of four reconstructed particles (nodes), each with kinematic features $x_i=(p_{T,i},\\eta_i,\\phi_i)$, where $p_{T,i}$ is transverse momentum, $\\eta_i$ is pseudorapidity, and $\\phi_i$ is azimuthal angle. The initial scalar hidden state for node $i$ is defined by a linear embedding $h_i^{(0)}=w_p \\ln p_{T,i} + w_{\\eta}\\,\\eta_i$, which respects azimuthal rotational invariance by using only $\\ln p_{T,i}$ and $\\eta_i$. For each directed edge $(i \\leftarrow j)$, define the edge feature $e_{ij}=(\\Delta \\eta_{ij}, \\Delta \\phi_{ij}, \\ln p_{T,j})$, with $\\Delta \\eta_{ij}=\\eta_j-\\eta_i$ and $\\Delta \\phi_{ij}$ given by the wrapped difference on the interval $[-\\pi,\\pi]$ so that it is the minimal signed separation in azimuth. The message from node $j$ to node $i$ is computed as\n$$\nm_{ij}=\\tanh\\!\\left(a\\,h_j^{(0)}+b_1\\,\\Delta \\eta_{ij}+b_2\\,\\cos(\\Delta \\phi_{ij})+b_3\\,\\ln p_{T,j}+c\\right),\n$$\nand the node update uses sum aggregation,\n$$\na_i=\\sum_{j\\neq i} m_{ij},\\quad h_i^{(1)}=h_i^{(0)}+\\alpha\\,a_i.\n$$\nYou are given the following constants and node features:\n- Embedding weights: $w_p=0.6$, $w_{\\eta}=0.3$.\n- Message weights: $a=0.8$, $b_1=0.15$, $b_2=0.2$, $b_3=0.05$, $c=-0.02$.\n- Update coefficient: $\\alpha=0.7$.\n- Node $1$: $(p_{T,1},\\eta_1,\\phi_1)=(50, 0.2, 0.10)$.\n- Node $2$: $(p_{T,2},\\eta_2,\\phi_2)=(40, -0.3, 3.05)$.\n- Node $3$: $(p_{T,3},\\eta_3,\\phi_3)=(25, 1.1, -3.00)$.\n- Node $4$: $(p_{T,4},\\eta_4,\\phi_4)=(10, -2.0, 2.90)$.\n\nAll angles are in radians, and $\\ln$ denotes the natural logarithm. Using the above definitions, compute the updated hidden state $h_2^{(1)}$ for node $2$. Express your final result as a real number and round your answer to four significant figures. No physical units are required for the final answer.", "solution": "The problem is to compute the updated hidden state $h_2^{(1)}$ for node $2$ in a graph neural network. The problem statement provides all necessary definitions, constants, and data, and is scientifically and mathematically sound. Therefore, it is deemed valid and a solution can be derived.\n\nThe updated hidden state for a node $i$ after one message passing layer is given by the formula:\n$$h_i^{(1)} = h_i^{(0)} + \\alpha a_i$$\nwhere $h_i^{(0)}$ is the initial hidden state, $\\alpha$ is the update coefficient, and $a_i$ is the aggregated message.\nFor node $i=2$, this becomes:\n$$h_2^{(1)} = h_2^{(0)} + \\alpha a_2$$\nThe aggregated message $a_2$ is the sum of incoming messages from all other nodes $j \\neq 2$:\n$$a_2 = \\sum_{j\\neq 2} m_{2j} = m_{21} + m_{23} + m_{24}$$\nThe process requires two main stages: first, calculating the initial hidden states $h_j^{(0)}$ for all nodes, and second, calculating the messages $m_{2j}$ for $j=1, 3, 4$ to find $a_2$.\n\n**1. Calculation of Initial Hidden States $h_j^{(0)}$**\n\nThe initial hidden state for a node $j$ is defined as $h_j^{(0)} = w_p \\ln p_{T,j} + w_{\\eta} \\eta_j$.\nThe given weights are $w_p = 0.6$ and $w_{\\eta} = 0.3$. The node features are:\n- Node $1$: $(p_{T,1}, \\eta_1, \\phi_1) = (50, 0.2, 0.10)$\n- Node $2$: $(p_{T,2}, \\eta_2, \\phi_2) = (40, -0.3, 3.05)$\n- Node $3$: $(p_{T,3}, \\eta_3, \\phi_3) = (25, 1.1, -3.00)$\n- Node $4$: $(p_{T,4}, \\eta_4, \\phi_4) = (10, -2.0, 2.90)$\n\nFirst, we compute the natural logarithm of the transverse momenta:\n- $\\ln p_{T,1} = \\ln(50) \\approx 3.91202$\n- $\\ln p_{T,2} = \\ln(40) \\approx 3.68888$\n- $\\ln p_{T,3} = \\ln(25) \\approx 3.21888$\n- $\\ln p_{T,4} = \\ln(10) \\approx 2.30259$\n\nNow, we compute the initial hidden states:\n- $h_1^{(0)} = (0.6) \\times (\\ln 50) + (0.3) \\times (0.2) \\approx (0.6)(3.91202) + 0.06 = 2.34721 + 0.06 = 2.40721$\n- $h_2^{(0)} = (0.6) \\times (\\ln 40) + (0.3) \\times (-0.3) \\approx (0.6)(3.68888) - 0.09 = 2.21333 - 0.09 = 2.12333$\n- $h_3^{(0)} = (0.6) \\times (\\ln 25) + (0.3) \\times (1.1) \\approx (0.6)(3.21888) + 0.33 = 1.93133 + 0.33 = 2.26133$\n- $h_4^{(0)} = (0.6) \\times (\\ln 10) + (0.3) \\times (-2.0) \\approx (0.6)(2.30259) - 0.6 = 1.38155 - 0.6 = 0.78155$\n\n**2. Calculation of Messages $m_{2j}$ to Node $2$**\n\nThe message from node $j$ to node $i$ is given by:\n$$m_{ij}=\\tanh\\!\\left(a\\,h_j^{(0)}+b_1\\,\\Delta \\eta_{ij}+b_2\\,\\cos(\\Delta \\phi_{ij})+b_3\\,\\ln p_{T,j}+c\\right)$$\nwith constants $a=0.8$, $b_1=0.15$, $b_2=0.2$, $b_3=0.05$, and $c=-0.02$. We need to compute $m_{21}$, $m_{23}$, and $m_{24}$.\n\n**Message $m_{21}$ (from node $j=1$ to node $i=2$):**\n- $\\Delta \\eta_{21} = \\eta_1 - \\eta_2 = 0.2 - (-0.3) = 0.5$\n- $\\Delta \\phi_{21} = \\phi_1 - \\phi_2 = 0.10 - 3.05 = -2.95$. This value is within $[-\\pi, \\pi]$, as $\\pi \\approx 3.14159$.\n- $\\cos(\\Delta \\phi_{21}) = \\cos(-2.95) \\approx -0.98506$\nLet $Z_{21}$ be the argument of $\\tanh$:\n$Z_{21} = (0.8)h_1^{(0)} + (0.15)\\Delta \\eta_{21} + (0.2)\\cos(\\Delta \\phi_{21}) + (0.05)\\ln p_{T,1} + (-0.02)$\n$Z_{21} \\approx (0.8)(2.40721) + (0.15)(0.5) + (0.2)(-0.98506) + (0.05)(3.91202) - 0.02$\n$Z_{21} \\approx 1.92577 + 0.075 - 0.19701 + 0.19560 - 0.02 = 1.97936$\n$m_{21} = \\tanh(1.97936) \\approx 0.96255$\n\n**Message $m_{23}$ (from node $j=3$ to node $i=2$):**\n- $\\Delta \\eta_{23} = \\eta_3 - \\eta_2 = 1.1 - (-0.3) = 1.4$\n- $\\Delta \\phi_{23} = \\phi_3 - \\phi_2 = -3.00 - 3.05 = -6.05$. This is outside $[-\\pi, \\pi]$. We wrap it by adding $2\\pi$: $\\Delta \\phi_{23, \\text{wrapped}} = -6.05 + 2\\pi \\approx -6.05 + 6.28318 = 0.23318$.\n- $\\cos(\\Delta \\phi_{23}) = \\cos(0.23318) \\approx 0.97290$\nLet $Z_{23}$ be the argument of $\\tanh$:\n$Z_{23} = (0.8)h_3^{(0)} + (0.15)\\Delta \\eta_{23} + (0.2)\\cos(\\Delta \\phi_{23}) + (0.05)\\ln p_{T,3} + (-0.02)$\n$Z_{23} \\approx (0.8)(2.26133) + (0.15)(1.4) + (0.2)(0.97290) + (0.05)(3.21888) - 0.02$\n$Z_{23} \\approx 1.80906 + 0.21 + 0.19458 + 0.16094 - 0.02 = 2.35458$\n$m_{23} = \\tanh(2.35458) \\approx 0.98207$\n\n**Message $m_{24}$ (from node $j=4$ to node $i=2$):**\n- $\\Delta \\eta_{24} = \\eta_4 - \\eta_2 = -2.0 - (-0.3) = -1.7$\n- $\\Delta \\phi_{24} = \\phi_4 - \\phi_2 = 2.90 - 3.05 = -0.15$. This value is within $[-\\pi, \\pi]$.\n- $\\cos(\\Delta \\phi_{24}) = \\cos(-0.15) \\approx 0.98877$\nLet $Z_{24}$ be the argument of $\\tanh$:\n$Z_{24} = (0.8)h_4^{(0)} + (0.15)\\Delta \\eta_{24} + (0.2)\\cos(\\Delta \\phi_{24}) + (0.05)\\ln p_{T,4} + (-0.02)$\n$Z_{24} \\approx (0.8)(0.78155) + (0.15)(-1.7) + (0.2)(0.98877) + (0.05)(2.30259) - 0.02$\n$Z_{24} \\approx 0.62524 - 0.255 + 0.19775 + 0.11513 - 0.02 = 0.66312$\n$m_{24} = \\tanh(0.66312) \\approx 0.58055$\n\n**3. Calculation of the Aggregated Message $a_2$ and Updated State $h_2^{(1)}$**\n\nThe aggregated message for node $2$ is the sum of the incoming messages:\n$a_2 = m_{21} + m_{23} + m_{24} \\approx 0.96255 + 0.98207 + 0.58055 = 2.52517$\n\nFinally, we update the hidden state for node $2$ using $h_2^{(1)} = h_2^{(0)} + \\alpha a_2$ with $\\alpha = 0.7$:\n$h_2^{(1)} \\approx 2.12333 + (0.7)(2.52517)$\n$h_2^{(1)} \\approx 2.12333 + 1.76762 = 3.89095$\n\nThe problem asks for the result to be rounded to four significant figures. The number is $3.89095$. The first four significant figures are $3$, $8$, $9$, and $0$. The next digit is $9$, which is $5$ or greater, so we round up the fourth significant digit.\n$h_2^{(1)} \\approx 3.891$", "answer": "$$\n\\boxed{3.891}\n$$", "id": "3510657"}, {"introduction": "The attention mechanism, central to the Transformer architecture, offers a highly flexible way for particles in an event to exchange information by learning pairwise interaction strengths. However, we can also inject explicit physical priors into this process using masks to guide the model's focus. This practice [@problem_id:3510640] demonstrates this principle by having you compute attention outputs under a 'locality mask', which restricts interactions to a particle's nearest neighbors in angular space, providing insight into how architectural choices can enforce known physics.", "problem": "Consider a simplified event in computational high-energy physics comprising $3$ reconstructed particles with angular coordinates $(\\eta, \\phi)$ given by Particle $1$: $(\\eta_1, \\phi_1) = (0.0, 0.0)$, Particle $2$: $(\\eta_2, \\phi_2) = (0.2, 0.1)$, and Particle $3$: $(\\eta_3, \\phi_3) = (0.8, 0.7)$, where angles are in radians. We study a single-head scaled dot-product attention mechanism applied to these particles, consistent with modern deep learning architectures for event analysis. Use the following foundational definitions:\n\n- The scaled dot-product attention scores for a query index $i$ and key index $j$ are $s_{ij} = \\frac{Q_i \\cdot K_j}{\\sqrt{d_k}} + M_{ij}$, where $Q_i \\in \\mathbb{R}^{d_k}$ is the query of particle $i$, $K_j \\in \\mathbb{R}^{d_k}$ is the key of particle $j$, $d_k$ is the key dimensionality, and $M_{ij}$ is a mask.\n- The attention weights are $a_{ij} = \\frac{\\exp(s_{ij})}{\\sum_{j'=1}^{3} \\exp(s_{ij'})}$.\n- The output for particle $i$ is $O_i = \\sum_{j=1}^{3} a_{ij} V_j$, with $V_j \\in \\mathbb{R}^{d_v}$ the value of particle $j$.\n\nAdopt a locality mask that encodes a nearest-neighbor attention within the event topology: for each query particle $i$, let $M_{ij} = 0$ if $j = i$ or $j$ is the unique nearest neighbor of $i$ in angular distance $\\Delta R_{ij} = \\sqrt{(\\eta_i - \\eta_j)^2 + (\\phi_i - \\phi_j)^2}$; otherwise set $M_{ij} = -\\infty$. Assume ties do not occur for the given coordinates.\n\nLet the single-head parameters be $d_k = 2$, $d_v = 2$, and the query, key, and value matrices (rows correspond to particles $1,2,3$) be\n$$\nQ = \\begin{pmatrix}\n1 & -1 \\\\\n0 & 2 \\\\\n-1 & 0\n\\end{pmatrix}, \\quad\nK = \\begin{pmatrix}\n0 & 0 \\\\\n0 & 0 \\\\\n0 & 0\n\\end{pmatrix}, \\quad\nV = \\begin{pmatrix}\n1 & 2 \\\\\n2 & -1 \\\\\n0 & 3\n\\end{pmatrix}.\n$$\n\nUsing only the above definitions and the provided data, compute the single-head attention outputs $O_1$, $O_2$, and $O_3$ for the three particles under the locality mask. Then, compute the scalar\n$$S = \\|O_1\\|_2^2 + \\|O_2\\|_2^2 + \\|O_3\\|_2^2,$$\nwhere $\\|\\cdot\\|_2$ denotes the Euclidean norm on $\\mathbb{R}^{2}$. Provide the final value of $S$ as an exact real number. No rounding is required.", "solution": "The problem asks for the scalar $S = \\|O_1\\|_2^2 + \\|O_2\\|_2^2 + \\|O_3\\|_2^2$, where $O_i$ are the outputs of a single-head attention mechanism with a locality mask.\n\n**1. Determine Nearest Neighbors and Construct the Mask**\n\nFirst, we calculate the squared angular distances $\\Delta R_{ij}^2 = (\\eta_i - \\eta_j)^2 + (\\phi_i - \\phi_j)^2$ for all pairs of particles.\n- Particles: $P_1(0,0)$, $P_2(0.2, 0.1)$, $P_3(0.8, 0.7)$.\n- $\\Delta R_{12}^2 = (0-0.2)^2 + (0-0.1)^2 = 0.04 + 0.01 = 0.05$.\n- $\\Delta R_{13}^2 = (0-0.8)^2 + (0-0.7)^2 = 0.64 + 0.49 = 1.13$.\n- $\\Delta R_{23}^2 = (0.2-0.8)^2 + (0.1-0.7)^2 = (-0.6)^2 + (-0.6)^2 = 0.36 + 0.36 = 0.72$.\n\nFrom these distances, we find the nearest neighbor for each particle:\n- For $P_1$, $\\Delta R_{12}^2  \\Delta R_{13}^2$, so its nearest neighbor is $P_2$.\n- For $P_2$, $\\Delta R_{21}^2  \\Delta R_{23}^2$, so its nearest neighbor is $P_1$.\n- For $P_3$, $\\Delta R_{32}^2  \\Delta R_{31}^2$, so its nearest neighbor is $P_2$.\n\nThe mask $M_{ij}$ is $0$ if $j=i$ or $j$ is the nearest neighbor of $i$, and $-\\infty$ otherwise. This gives the mask matrix:\n$$ M = \\begin{pmatrix} 0  0  -\\infty \\\\ 0  0  -\\infty \\\\ -\\infty  0  0 \\end{pmatrix} $$\n\n**2. Calculate Attention Scores and Weights**\n\nThe attention scores are $s_{ij} = \\frac{Q_i \\cdot K_j}{\\sqrt{d_k}} + M_{ij}$. Since the key matrix $K$ is the zero matrix, the term $\\frac{Q_i \\cdot K_j}{\\sqrt{d_k}}$ is always zero. Thus, the score matrix $s$ is identical to the mask matrix $M$.\n\nThe attention weights are $a_{ij} = \\text{softmax}(s_i)_j$. We use $\\exp(0)=1$ and $\\exp(-\\infty)=0$.\n- For $i=1$: $[\\exp(0), \\exp(0), \\exp(-\\infty)] = [1, 1, 0]$. Normalizing gives $[1/2, 1/2, 0]$.\n- For $i=2$: $[\\exp(0), \\exp(0), \\exp(-\\infty)] = [1, 1, 0]$. Normalizing gives $[1/2, 1/2, 0]$.\n- For $i=3$: $[\\exp(-\\infty), \\exp(0), \\exp(0)] = [0, 1, 1]$. Normalizing gives $[0, 1/2, 1/2]$.\n\nThe attention weight matrix is:\n$$ A = \\begin{pmatrix} 1/2  1/2  0 \\\\ 1/2  1/2  0 \\\\ 0  1/2  1/2 \\end{pmatrix} $$\n\n**3. Calculate Output Vectors**\n\nThe output is $O_i = \\sum_j a_{ij} V_j$, where $V_1=(1,2)$, $V_2=(2,-1)$, $V_3=(0,3)$.\n- $O_1 = \\frac{1}{2}V_1 + \\frac{1}{2}V_2 = \\frac{1}{2}(1,2) + \\frac{1}{2}(2,-1) = (\\frac{1}{2},1) + (1, -\\frac{1}{2}) = (\\frac{3}{2}, \\frac{1}{2})$.\n- $O_2 = \\frac{1}{2}V_1 + \\frac{1}{2}V_2 = O_1 = (\\frac{3}{2}, \\frac{1}{2})$.\n- $O_3 = \\frac{1}{2}V_2 + \\frac{1}{2}V_3 = \\frac{1}{2}(2,-1) + \\frac{1}{2}(0,3) = (1, -\\frac{1}{2}) + (0, \\frac{3}{2}) = (1, 1)$.\n\n**4. Calculate the Final Sum $S$**\n\nFinally, we compute the sum of the squared Euclidean norms:\n- $\\|O_1\\|_2^2 = (\\frac{3}{2})^2 + (\\frac{1}{2})^2 = \\frac{9}{4} + \\frac{1}{4} = \\frac{10}{4} = \\frac{5}{2}$.\n- $\\|O_2\\|_2^2 = \\|O_1\\|_2^2 = \\frac{5}{2}$.\n- $\\|O_3\\|_2^2 = 1^2 + 1^2 = 2$.\n\n$$S = \\|O_1\\|_2^2 + \\|O_2\\|_2^2 + \\|O_3\\|_2^2 = \\frac{5}{2} + \\frac{5}{2} + 2 = 5 + 2 = 7$$\nThe final result is an exact real number.", "answer": "$$\n\\boxed{7}\n$$", "id": "3510640"}]}