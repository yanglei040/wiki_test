{"hands_on_practices": [{"introduction": "A cornerstone of model-independent searches is scanning for localized excesses over a continuous background. However, the very act of searching in many places introduces the \"look-elsewhere effect,\" which can inflate the significance of random fluctuations. This exercise provides essential practice in correcting for this multiple testing bias, a critical skill for assessing the true statistical significance of any potential discovery. You will start from first principles to apply the Bonferroni and Šidák corrections to a hypothetical search, allowing you to directly compare a conservative bound with an exact correction under independence [@problem_id:3504747].", "problem": "In a model-independent anomaly search over a continuous invariant-mass spectrum, a sliding-window scan is performed to identify local excesses without committing to a specific signal model. Consider a scan that evaluates $K = 100$ disjoint windows, each giving a local one-sided $p$-value that tests the null hypothesis \"background-only\" against the alternative \"background plus localized excess.\" The smallest observed local $p$-value is $p_{\\min} = 10^{-4}$. Assume that the local tests are calibrated so that under the null hypothesis each window’s local $p$-value is uniformly distributed on $[0,1]$, and that the tests across disjoint windows are independent.\n\nStarting from the foundational definitions of a $p$-value and the family-wise error rate (FWER), and from the complement rule for probabilities together with Boole’s inequality (the union bound), derive the global $p$-value of observing at least one fluctuation as extreme as the minimum local $p$-value across the $K$ windows under:\n\n- The Bonferroni correction, which provides a valid FWER control under arbitrary dependence.\n- The Šidák correction, which provides an exact FWER under independence.\n\nThen convert each global $p$-value into a one-sided Gaussian-equivalent significance $Z$ (in units of standard deviations $\\sigma$) using the standard normal cumulative distribution function $\\Phi$, by solving $Z = \\Phi^{-1}(1 - p_{\\mathrm{global}})$ for each correction.\n\nRound your final $Z$ values to four significant figures and report them. Express the final numerical results in units of $\\sigma$ as defined above. Do not include units in the final boxed answer.", "solution": "The user has provided a problem statement that must be validated before a solution is attempted.\n\n### Step 1: Extract Givens\n-   Number of disjoint windows: $K = 100$\n-   Smallest observed local one-sided $p$-value: $p_{\\min} = 10^{-4}$\n-   The null hypothesis is \"background-only\".\n-   Under the null hypothesis, each window's local $p$-value is uniformly distributed on $[0,1]$.\n-   The tests across disjoint windows are independent.\n-   Task 1: Derive the global $p$-value, $p_{\\mathrm{global}}$, for both the Bonferroni and Šidák corrections.\n-   Task 2: Convert each $p_{\\mathrm{global}}$ into a one-sided Gaussian-equivalent significance $Z$ via the relation $Z = \\Phi^{-1}(1 - p_{\\mathrm{global}})$, where $\\Phi$ is the standard normal cumulative distribution function (CDF).\n-   Task 3: Round the final $Z$ values to four significant figures.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientifically Grounded**: The problem is well-grounded in statistics and its application to high-energy physics. The concepts of $p$-values, the \"look-elsewhere effect\", Bonferroni correction, Šidák correction, and Gaussian significance are standard tools in the field for model-independent searches.\n2.  **Well-Posed**: The problem is clearly defined with all necessary information provided. The assumptions of independence and the uniform distribution of local $p$-values under the null hypothesis are explicitly stated, allowing for a unique and meaningful solution.\n3.  **Objective**: The language is precise, quantitative, and free of any subjective or ambiguous terminology.\n4.  **Completeness**: The problem is self-contained. It provides the number of tests $K$, the minimum observed local $p$-value $p_{\\min}$, and the necessary statistical assumptions.\n5.  **Realism**: The scenario described is a simplified but realistic representation of an anomaly search in experimental particle physics. The values provided are plausible.\n6.  **Other Flaws**: The problem is neither trivial nor tautological. It requires a correct derivation and application of established statistical formulas. It is not metaphorical and is directly on-topic.\n\n### Step 3: Verdict and Action\nThe problem is valid. The solution process will now proceed.\n\nThe global $p$-value, $p_{\\mathrm{global}}$, addresses the \"look-elsewhere effect\" or the multiple testing problem. It is defined as the probability, under the global null hypothesis (i.e., no signal is present in any of the $K$ windows), of observing at least one local $p$-value as small as or smaller than the minimum one actually found, $p_{\\min}$. Let $P_1, P_2, \\ldots, P_K$ be the random variables representing the local $p$-values for the $K$ windows. The global $p$-value is the probability of the event that the minimum of these local $p$-values is less than or equal to $p_{\\min}$:\n$$p_{\\mathrm{global}} = P(\\min_{i=1, \\ldots, K} P_i \\le p_{\\min})$$\nThis is equivalent to the probability of the union of events where at least one local $p$-value is less than or equal to $p_{\\min}$. Let $A_i$ be the event $P_i \\le p_{\\min}$. Then:\n$$p_{\\mathrm{global}} = P\\left(\\bigcup_{i=1}^{K} A_i\\right)$$\nUnder the null hypothesis, each $P_i$ is uniformly distributed on $[0,1]$, thus $P(A_i) = P(P_i \\le p_{\\min}) = p_{\\min}$.\n\n**1. Bonferroni Correction**\n\nThe Bonferroni correction is derived from Boole's inequality, also known as the union bound. For any set of events $A_1, \\ldots, A_K$, the inequality states that the probability of their union is no greater than the sum of their individual probabilities:\n$$P\\left(\\bigcup_{i=1}^{K} A_i\\right) \\le \\sum_{i=1}^{K} P(A_i)$$\nThis bound holds regardless of any dependence structure among the events. Applying this to our problem, we obtain an upper bound on the global $p$-value:\n$$p_{\\mathrm{global}} \\le \\sum_{i=1}^{K} P(P_i \\le p_{\\min}) = \\sum_{i=1}^{K} p_{\\min} = K \\cdot p_{\\min}$$\nThe Bonferroni-corrected $p$-value is defined as this upper bound, conventionally capped at $1$:\n$$p_{\\mathrm{global, Bonf}} = \\min(1, K \\cdot p_{\\min})$$\nThis provides a conservative estimate that guarantees control of the family-wise error rate (FWER) at any desired level $\\alpha$.\nFor the given values $K=100$ and $p_{\\min}=10^{-4}$:\n$$p_{\\mathrm{global, Bonf}} = 100 \\times 10^{-4} = 10^{-2} = 0.01$$\nSince $0.01 < 1$, the minimum function is not active.\n\n**2. Šidák Correction**\n\nThe Šidák correction provides an exact global $p$-value under the assumption that the individual tests are independent, an assumption explicitly stated in the problem. The derivation begins with the complement rule for probabilities. The complement of \"at least one $P_i \\le p_{\\min}$\" is \"all $P_i > p_{\\min}$\".\n$$p_{\\mathrm{global}} = P(\\min_{i} P_i \\le p_{\\min}) = 1 - P(\\forall i, P_i > p_{\\min})$$\nBecause the tests are independent, the probability of the joint event is the product of the probabilities of the individual events:\n$$P(\\forall i, P_i > p_{\\min}) = \\prod_{i=1}^{K} P(P_i > p_{\\min})$$\nSince each $P_i$ is uniformly distributed on $[0,1]$ under the null hypothesis, the probability $P(P_i > p_{\\min})$ is:\n$$P(P_i > p_{\\min}) = 1 - P(P_i \\le p_{\\min}) = 1 - p_{\\min}$$\nSubstituting this back, we get the exact global $p$-value under independence, which is the Šidák correction:\n$$p_{\\mathrm{global, Šidák}} = 1 - (1 - p_{\\min})^K$$\nFor the given values $K=100$ and $p_{\\min}=10^{-4}$:\n$$p_{\\mathrm{global, Šidák}} = 1 - (1 - 10^{-4})^{100} = 1 - (0.9999)^{100}$$\n\n**3. Conversion to Significance $Z$**\n\nThe significance $Z$ is found by inverting the standard normal cumulative distribution function $\\Phi$. It is the value on the standard normal distribution for which the integral of the tail from $Z$ to infinity equals the $p$-value. This is given by the relation $p_{\\mathrm{global}} = 1 - \\Phi(Z)$, which rearranges to $Z = \\Phi^{-1}(1 - p_{\\mathrm{global}})$.\n\nFor the Bonferroni correction:\n$$Z_{\\mathrm{Bonf}} = \\Phi^{-1}(1 - p_{\\mathrm{global, Bonf}}) = \\Phi^{-1}(1 - 0.01) = \\Phi^{-1}(0.99)$$\nUsing standard statistical tables or computational software, $\\Phi^{-1}(0.99) \\approx 2.32634787$.\n\nFor the Šidák correction:\n$$Z_{\\mathrm{Šidák}} = \\Phi^{-1}(1 - p_{\\mathrm{global, Šidák}}) = \\Phi^{-1}\\left(1 - \\left[1 - (1 - p_{\\min})^K\\right]\\right) = \\Phi^{-1}\\left((1 - p_{\\min})^K\\right)$$\n$$Z_{\\mathrm{Šidák}} = \\Phi^{-1}\\left((1 - 10^{-4})^{100}\\right) = \\Phi^{-1}\\left((0.9999)^{100}\\right)$$\nNumerically, $(0.9999)^{100} \\approx 0.990049833$.\n$$Z_{\\mathrm{Šidák}} = \\Phi^{-1}(0.990049833) \\approx 2.3283592$$\n\n**4. Final Numerical Results**\n\nThe problem requires rounding the final $Z$ values to four significant figures.\n-   Bonferroni significance: $Z_{\\mathrm{Bonf}} \\approx 2.32634787 \\rightarrow 2.326$.\n-   Šidák significance: $Z_{\\mathrm{Šidák}} \\approx 2.3283592 \\rightarrow 2.328$.\n\nThe Bonferroni correction provides a conservative upper bound on the true $p$-value, resulting in a slightly larger $p$-value and thus a slightly smaller significance compared to the exact Šidák correction, which is applicable here due to the independence of the tests.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n2.326 & 2.328\n\\end{pmatrix}\n}\n$$", "id": "3504747"}, {"introduction": "Moving beyond simple bump hunts, many modern anomaly detection strategies operate in a semi-supervised context where we might have a model for a potential new signal but not know its prevalence. This practice delves into this scenario by tasking you with fitting a mixture model to binned data using the Expectation-Maximization (EM) algorithm. By implementing the EM algorithm from scratch, you will gain a deep, practical understanding of how to estimate the fraction of an anomalous component in a dataset and calculate the uncertainty on that estimate using the Fisher information, a foundational technique in statistical data analysis [@problem_id:3504732].", "problem": "You are given a histogram of event counts across discrete bins aggregated from a high-energy collider search. The working hypothesis is that the observed distribution is a mixture of a known background shape and a possible small contamination (anomaly) with a distinct shape. Let the total number of events be $N$, the bin index be $i \\in \\{1,\\dots,K\\}$, the background template be $\\{b_i\\}_{i=1}^K$ with $\\sum_{i=1}^K b_i = 1$, and the anomaly template be $\\{s_i\\}_{i=1}^K$ with $\\sum_{i=1}^K s_i = 1$. The mixture model assumes that the probability for an event to be in bin $i$ is\n$$\np_i(\\epsilon) = (1-\\epsilon)\\,b_i + \\epsilon\\,s_i,\n$$\nwhere $\\epsilon \\in [0,1]$ is the unknown contamination fraction to be estimated. The observed data are the counts $\\{n_i\\}_{i=1}^K$ with $\\sum_{i=1}^K n_i = N$. Assume the counts are realizations of a multinomial random variable with parameters $N$ and probabilities $\\{p_i(\\epsilon)\\}_{i=1}^K$.\n\nStarting from the definition of the multinomial likelihood,\n$$\n\\mathcal{L}(\\epsilon) \\propto \\prod_{i=1}^K p_i(\\epsilon)^{n_i},\n$$\nand its log-likelihood,\n$$\n\\ell(\\epsilon) = \\sum_{i=1}^K n_i \\log p_i(\\epsilon),\n$$\nderive an Expectation-Maximization (EM) algorithm that estimates $\\epsilon$ using the latent variable formulation in which each event is assigned an unobserved indicator of whether it came from the background or the contamination. Use the principle that EM maximizes the expected complete-data log-likelihood with respect to the current posterior over the latent variables. Show from first principles why the resulting fixed-point update for $\\epsilon$ is consistent with maximizing the observed-data likelihood under the mixture model. Finally, derive the observed Fisher information for $\\epsilon$ at the maximum likelihood estimate $\\hat{\\epsilon}$, defined as\n$$\n\\mathcal{I}_{\\text{obs}}(\\hat{\\epsilon}) \\equiv -\\left.\\frac{\\partial^2 \\ell(\\epsilon)}{\\partial \\epsilon^2}\\right|_{\\epsilon=\\hat{\\epsilon}},\n$$\nand use it to report an uncertainty estimate $\\sigma_\\epsilon = \\sqrt{1 / \\mathcal{I}_{\\text{obs}}(\\hat{\\epsilon})}$. If $\\mathcal{I}_{\\text{obs}}(\\hat{\\epsilon}) = 0$, define $\\sigma_\\epsilon$ to be $+\\infty$.\n\nYour task is to write a complete program that:\n- Implements the EM procedure to estimate $\\epsilon$, using only the binned counts and the templates $\\{b_i\\}$ and $\\{s_i\\}$, with the E-step responsibilities computed consistently for binned data and the M-step that updates $\\epsilon$.\n- Iterates until convergence, using an absolute tolerance $\\tau = 10^{-12}$ on $\\epsilon$ or a maximum of $10{,}000$ iterations, whichever comes first.\n- Computes the observed Fisher information at the converged $\\hat{\\epsilon}$ and reports $\\sigma_\\epsilon$ as defined above.\n- Produces results for each test case as a two-element list $[\\hat{\\epsilon}, \\sigma_\\epsilon]$, where both entries are decimal fractions (no percentage sign) rounded to six decimal places.\n\nUse the following test suite, each specified by $(\\{n_i\\}, \\{b_i\\}, \\{s_i\\}, \\epsilon_0)$ where $\\epsilon_0$ is the initial value for the EM iterations:\n\n- Test case $1$ (happy path, moderate contamination, smooth $b$ and peaked $s$):\n  - $\\{n_i\\} = \\{143, 121, 98, 76, 40, 22\\}$\n  - $\\{b_i\\} = \\{0.30, 0.25, 0.20, 0.15, 0.07, 0.03\\}$\n  - $\\{s_i\\} = \\{0.05, 0.10, 0.15, 0.20, 0.25, 0.25\\}$\n  - $\\epsilon_0 = 0.02$\n\n- Test case $2$ (boundary case, no contamination, background-only counts):\n  - $\\{n_i\\} = \\{100, 200, 300, 250, 150\\}$\n  - $\\{b_i\\} = \\{0.10, 0.20, 0.30, 0.25, 0.15\\}$\n  - $\\{s_i\\} = \\{0.05, 0.05, 0.10, 0.30, 0.50\\}$\n  - $\\epsilon_0 = 0.01$\n\n- Test case $3$ (low-information case, $s$ close to $b$):\n  - $\\{n_i\\} = \\{113, 112, 112, 112, 112, 112, 127\\}$\n  - $\\{b_i\\} = \\{0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.16\\}$\n  - $\\{s_i\\} = \\{0.15, 0.14, 0.14, 0.14, 0.14, 0.14, 0.15\\}$\n  - $\\epsilon_0 = 0.05$\n\n- Test case $4$ (edge support case, background has a zero-probability bin):\n  - $\\{n_i\\} = \\{12, 180, 291, 117\\}$\n  - $\\{b_i\\} = \\{0.00, 0.30, 0.50, 0.20\\}$\n  - $\\{s_i\\} = \\{0.40, 0.30, 0.20, 0.10\\}$\n  - $\\epsilon_0 = 0.02$\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case result is itself a two-element list. For example, the output format must be of the form\n$$\n\\big[ [\\hat{\\epsilon}_1,\\sigma_{\\epsilon,1}], [\\hat{\\epsilon}_2,\\sigma_{\\epsilon,2}], [\\hat{\\epsilon}_3,\\sigma_{\\epsilon,3}], [\\hat{\\epsilon}_4,\\sigma_{\\epsilon,4}] \\big],\n$$\nwith all numerical entries rounded to six decimal places and expressed as decimal fractions.", "solution": "The user has provided a well-posed problem in statistical inference. It is scientifically grounded, self-contained, and objective. All necessary parameters and data for a unique solution are provided. Therefore, the problem is valid and a solution will be provided.\n\n### Theoretical Derivation\n\nThe problem asks for an estimator for the contamination fraction $\\epsilon$ in a mixture model, based on observed binned counts $\\{n_i\\}_{i=1}^K$. The model for the probability of an event falling in bin $i$ is $p_i(\\epsilon) = (1-\\epsilon)b_i + \\epsilon s_i$, where $\\{b_i\\}$ and $\\{s_i\\}$ are the known background and anomaly probability templates, respectively. The data $\\{n_i\\}$ are assumed to follow a multinomial distribution with total count $N = \\sum_{i=1}^K n_i$ and probabilities $\\{p_i(\\epsilon)\\}$.\n\n#### 1. Expectation-Maximization (EM) Algorithm\n\nThe EM algorithm is an iterative method for finding maximum likelihood estimates of parameters in statistical models with latent variables.\n\n**Latent Variable Formulation**: For each of the $N$ individual events, we introduce a latent indicator variable $Z_j \\in \\{0, 1\\}$ for $j=1, \\dots, N$. $Z_j=1$ indicates the event originated from the anomaly component, while $Z_j=0$ indicates it originated from the background. The prior probability is $P(Z_j=1) = \\epsilon$. The observed data is the bin index $X_j \\in \\{1, \\dots, K\\}$ for each event. The binned counts are $n_i = \\sum_{j=1}^N \\mathbb{I}(X_j=i)$, where $\\mathbb{I}(\\cdot)$ is the indicator function.\n\n**Complete-Data Log-Likelihood**: The complete data consists of the pairs $\\{(X_j, Z_j)\\}_{j=1}^N$. The complete-data likelihood is:\n$$ \\mathcal{L}_{\\text{comp}}(\\epsilon) = \\prod_{j=1}^N P(X_j, Z_j | \\epsilon) = \\prod_{j=1}^N \\left[ (1-\\epsilon) b_{X_j} \\right]^{1-Z_j} \\left[ \\epsilon s_{X_j} \\right]^{Z_j} $$\nThe corresponding log-likelihood is:\n$$ \\ell_{\\text{comp}}(\\epsilon) = \\sum_{j=1}^N \\left[ (1-Z_j)\\log(1-\\epsilon) + Z_j\\log(\\epsilon) \\right] + \\sum_{j=1}^N \\left[ (1-Z_j)\\log(b_{X_j}) + Z_j\\log(s_{X_j}) \\right] $$\nIgnoring terms not dependent on $\\epsilon$, we have:\n$$ \\ell_{\\text{comp}}(\\epsilon) \\propto \\left(\\sum_{j=1}^N (1-Z_j)\\right) \\log(1-\\epsilon) + \\left(\\sum_{j=1}^N Z_j\\right) \\log(\\epsilon) $$\nLet $N_s = \\sum_{j=1}^N Z_j$ be the total (unobserved) number of anomaly events. Then $\\sum_{j=1}^N (1-Z_j) = N - N_s$.\n$$ \\ell_{\\text{comp}}(\\epsilon) \\propto (N - N_s) \\log(1-\\epsilon) + N_s \\log(\\epsilon) $$\n\n**E-Step (Expectation)**: In the E-step, we compute the expectation of the complete-data log-likelihood with respect to the posterior distribution of the latent variables $Z$ given the observed data $X$ and the current parameter estimate $\\epsilon^{(t)}$. This is the $Q$ function:\n$$ Q(\\epsilon | \\epsilon^{(t)}) = E_{Z|X, \\epsilon^{(t)}}[\\ell_{\\text{comp}}(\\epsilon)] $$\nThis simplifies to computing the expectation of $N_s$:\n$$ N_s^{(t)} = E[N_s | X, \\epsilon^{(t)}] = E\\left[\\sum_{j=1}^N Z_j \\middle| X, \\epsilon^{(t)}\\right] = \\sum_{j=1}^N E[Z_j | X_j, \\epsilon^{(t)}] $$\nThe expectation $E[Z_j | X_j, \\epsilon^{(t)}]$ is the posterior probability that event $j$ is an anomaly, given it was observed in bin $X_j$. This is the \"responsibility\" $r_{X_j}^{(t)}$:\n$$ r_i^{(t)} \\equiv P(Z_j=1 | X_j=i, \\epsilon^{(t)}) = \\frac{P(X_j=i|Z_j=1)P(Z_j=1|\\epsilon^{(t)})}{P(X_j=i|\\epsilon^{(t)})} = \\frac{s_i \\epsilon^{(t)}}{(1-\\epsilon^{(t)})b_i + \\epsilon^{(t)}s_i} = \\frac{\\epsilon^{(t)} s_i}{p_i(\\epsilon^{(t)})} $$\nThe expected total number of anomaly events is the sum of responsibilities over all observed events, which, grouped by bin, is:\n$$ N_s^{(t)} = \\sum_{i=1}^K n_i r_i^{(t)} $$\nThe $Q$ function is then:\n$$ Q(\\epsilon | \\epsilon^{(t)}) = (N - N_s^{(t)}) \\log(1-\\epsilon) + N_s^{(t)} \\log(\\epsilon) + \\text{constant} $$\n\n**M-Step (Maximization)**: In the M-step, we maximize $Q(\\epsilon | \\epsilon^{(t)})$ with respect to $\\epsilon$ to find the next estimate $\\epsilon^{(t+1)}$.\n$$ \\frac{\\partial Q}{\\partial \\epsilon} = -\\frac{N - N_s^{(t)}}{1-\\epsilon} + \\frac{N_s^{(t)}}{\\epsilon} = 0 $$\nSolving for $\\epsilon$ gives:\n$$ \\epsilon^{(t+1)} = \\frac{N_s^{(t)}}{N} = \\frac{1}{N} \\sum_{i=1}^K n_i r_i^{(t)} $$\nSubstituting the expression for $r_i^{(t)}$ yields the final update rule:\n$$ \\epsilon^{(t+1)} = \\frac{1}{N} \\sum_{i=1}^K n_i \\frac{\\epsilon^{(t)} s_i}{(1-\\epsilon^{(t)})b_i + \\epsilon^{(t)}s_i} $$\nThis iterative procedure is guaranteed to not decrease the observed-data log-likelihood at each step.\n\n#### 2. Consistency with Maximum Likelihood Estimation\n\nThe observed-data log-likelihood (up to a constant) is:\n$$ \\ell(\\epsilon) = \\sum_{i=1}^K n_i \\log p_i(\\epsilon) = \\sum_{i=1}^K n_i \\log((1-\\epsilon)b_i + \\epsilon s_i) $$\nTo find the Maximum Likelihood Estimate (MLE) $\\hat{\\epsilon}$, we set its first derivative (the score function) to zero:\n$$ \\frac{\\partial \\ell}{\\partial \\epsilon} = \\sum_{i=1}^K n_i \\frac{s_i - b_i}{p_i(\\epsilon)} = 0 $$\nWe can rewrite this derivative in another form. Since $p_i(\\epsilon) = (1-\\epsilon)b_i + \\epsilon s_i$, we have $s_i = (p_i(\\epsilon) - (1-\\epsilon)b_i) / \\epsilon$. A more useful identity is $s_i - b_i = (s_i - p_i(\\epsilon)) / (1-\\epsilon)$. Using this:\n$$ \\frac{\\partial \\ell}{\\partial \\epsilon} = \\sum_{i=1}^K \\frac{n_i}{p_i(\\epsilon)} \\frac{s_i - p_i(\\epsilon)}{1-\\epsilon} = \\frac{1}{1-\\epsilon} \\left( \\sum_{i=1}^K \\frac{n_i s_i}{p_i(\\epsilon)} - \\sum_{i=1}^K \\frac{n_i p_i(\\epsilon)}{p_i(\\epsilon)} \\right) = \\frac{1}{1-\\epsilon} \\left( \\sum_{i=1}^K \\frac{n_i s_i}{p_i(\\epsilon)} - N \\right) $$\nSetting the derivative to zero for $\\epsilon \\in (0,1)$ implies $\\sum_{i=1}^K \\frac{n_i s_i}{p_i(\\hat{\\epsilon})} = N$.\n\nNow, consider a fixed point $\\hat{\\epsilon}$ of the EM update rule, where $\\hat{\\epsilon} = \\epsilon^{(t+1)} = \\epsilon^{(t)}$:\n$$ \\hat{\\epsilon} = \\frac{1}{N} \\sum_{i=1}^K n_i \\frac{\\hat{\\epsilon} s_i}{p_i(\\hat{\\epsilon})} $$\nFor $\\hat{\\epsilon} \\in (0,1)$, we can divide by $\\hat{\\epsilon}$:\n$$ 1 = \\frac{1}{N} \\sum_{i=1}^K \\frac{n_i s_i}{p_i(\\hat{\\epsilon})} \\implies \\sum_{i=1}^K \\frac{n_i s_i}{p_i(\\hat{\\epsilon})} = N $$\nThis is precisely the condition for a critical point of the observed-data log-likelihood. Thus, any internal fixed point of the EM algorithm corresponds to a stationary point of the log-likelihood function, demonstrating consistency.\n\n#### 3. Observed Fisher Information and Uncertainty\n\nThe observed Fisher information is defined as the negative of the second derivative of the log-likelihood evaluated at the MLE, $\\hat{\\epsilon}$:\n$$ \\mathcal{I}_{\\text{obs}}(\\hat{\\epsilon}) = -\\left.\\frac{\\partial^2 \\ell(\\epsilon)}{\\partial \\epsilon^2}\\right|_{\\epsilon=\\hat{\\epsilon}} $$\nWe compute the second derivative:\n$$ \\frac{\\partial^2 \\ell}{\\partial \\epsilon^2} = \\frac{\\partial}{\\partial \\epsilon} \\left( \\sum_{i=1}^K n_i \\frac{s_i - b_i}{p_i(\\epsilon)} \\right) = \\sum_{i=1}^K n_i (s_i - b_i) \\left( -\\frac{1}{p_i(\\epsilon)^2} \\right) \\frac{\\partial p_i(\\epsilon)}{\\partial \\epsilon} $$\nSince $\\frac{\\partial p_i(\\epsilon)}{\\partial \\epsilon} = s_i - b_i$, we get:\n$$ \\frac{\\partial^2 \\ell}{\\partial \\epsilon^2} = - \\sum_{i=1}^K n_i \\frac{(s_i - b_i)^2}{p_i(\\epsilon)^2} $$\nThe observed Fisher information is therefore:\n$$ \\mathcal{I}_{\\text{obs}}(\\hat{\\epsilon}) = \\sum_{i=1}^K n_i \\frac{(s_i - b_i)^2}{((1-\\hat{\\epsilon})b_i + \\hat{\\epsilon} s_i)^2} $$\nThe uncertainty on the estimate $\\hat{\\epsilon}$ is given by the Cramér-Rao lower bound, which is approximated by the inverse of the square root of the observed Fisher information:\n$$ \\sigma_\\epsilon = \\sqrt{1 / \\mathcal{I}_{\\text{obs}}(\\hat{\\epsilon})} $$\nIf $\\mathcal{I}_{\\text{obs}}(\\hat{\\epsilon}) = 0$, which occurs if $s_i=b_i$ for all bins with $n_i>0$, the parameter $\\epsilon$ is not identifiable, and the uncertainty $\\sigma_\\epsilon$ is infinite.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves for the contamination fraction epsilon and its uncertainty using an EM algorithm\n    for a set of test cases.\n    \"\"\"\n    test_cases = [\n        # Test case 1\n        (\n            np.array([143, 121, 98, 76, 40, 22]),\n            np.array([0.30, 0.25, 0.20, 0.15, 0.07, 0.03]),\n            np.array([0.05, 0.10, 0.15, 0.20, 0.25, 0.25]),\n            0.02\n        ),\n        # Test case 2\n        (\n            np.array([100, 200, 300, 250, 150]),\n            np.array([0.10, 0.20, 0.30, 0.25, 0.15]),\n            np.array([0.05, 0.05, 0.10, 0.30, 0.50]),\n            0.01\n        ),\n        # Test case 3\n        (\n            np.array([113, 112, 112, 112, 112, 112, 127]),\n            np.array([0.14, 0.14, 0.14, 0.14, 0.14, 0.14, 0.16]),\n            np.array([0.15, 0.14, 0.14, 0.14, 0.14, 0.14, 0.15]),\n            0.05\n        ),\n        # Test case 4\n        (\n            np.array([12, 180, 291, 117]),\n            np.array([0.00, 0.30, 0.50, 0.20]),\n            np.array([0.40, 0.30, 0.20, 0.10]),\n            0.02\n        ),\n    ]\n\n    TOL = 1e-12\n    MAX_ITER = 10000\n    \n    results_str = []\n\n    for n, b, s, eps0 in test_cases:\n        N = np.sum(n)\n        eps = float(eps0)\n\n        for _ in range(MAX_ITER):\n            eps_old = eps\n            \n            # Mixture probability for each bin\n            p = (1.0 - eps) * b + eps * s\n            \n            # The EM update can be written in a numerically stable form that avoids\n            # explicitly calculating the responsibilities, which can be unstable if eps is small.\n            # eps_new = (1/N) * sum(n_i * r_i) = (1/N) * sum(n_i * eps * s_i / p_i)\n            #           = (eps/N) * sum(n_i * s_i / p_i)\n            \n            # To handle cases where p_i might be zero, we use np.divide to return 0 for 0/0.\n            # This is correct as if p_i=0, n_i must also be 0 for a non-infinite log-likelihood,\n            # so the contribution to the sum is 0.\n            sum_term = np.sum(np.divide(n * s, p, out=np.zeros_like(p), where=p!=0))\n            \n            eps = eps * sum_term / N\n            \n            # Constrain epsilon to the valid range [0, 1] to handle potential floating point inaccuracies.\n            # The update rule theoretically preserves this property.\n            eps = max(0.0, min(1.0, eps))\n            \n            if abs(eps - eps_old) < TOL:\n                break\n        \n        eps_hat = eps\n        \n        # Calculate Observed Fisher Information\n        p_hat = (1.0 - eps_hat) * b + eps_hat * s\n        diff_s_b = s - b\n        \n        # Terms for the Fisher information sum: n_i * ((s_i - b_i) / p_hat_i)^2\n        # Use np.divide for numerical stability, similar to the EM step.\n        numer = n * (diff_s_b**2)\n        denom = p_hat**2\n        fisher_terms = np.divide(numer, denom, out=np.zeros_like(numer), where=denom!=0)\n        \n        fisher_info = np.sum(fisher_terms)\n        \n        sigma_eps = 0.0\n        if fisher_info > 1e-30:  # Use a small threshold to avoid division by zero\n            sigma_eps = (1.0 / fisher_info)**0.5\n        else:\n            sigma_eps = float('inf')\n\n        results_str.append(f\"[{eps_hat:.6f},{sigma_eps:.6f}]\")\n\n    print(f\"[{','.join(results_str)}]\")\n\nsolve()\n```", "id": "3504732"}, {"introduction": "Detecting an anomalous event is only the first step; understanding *why* it is anomalous is crucial for physical interpretation. This hands-on practice explores the domain of model interpretability by asking you to implement an attribution method, Integrated Gradients, to pinpoint which constituents of an event are most responsible for its high anomaly score. You will not only calculate these attributions for a simplified, yet powerful, anomaly score but also validate their faithfulness through counterfactual and causal validation tests, providing a robust framework for building trust in anomaly detection models [@problem_id:3504728].", "problem": "You are given a formulation for an event-level anomaly score suitable for model-independent searches in computational high-energy physics. Each event is represented as a list of $N$ constituents (particles), each with a four-vector $(E, p_x, p_y, p_z)$. All quantities in this problem are dimensionless standardized features, and thus no physical unit conversion is needed. For physical plausibility, each constituent is treated as massless, so $E = \\sqrt{p_x^2 + p_y^2 + p_z^2}$ must hold after any operation that modifies its momentum.\n\nLet $x \\in \\mathbb{R}^{D}$ denote the concatenated event vector with $D = 4N$, ordered as $[E_1,p_{x,1},p_{y,1},p_{z,1},\\dots,E_N,p_{x,N},p_{y,N},p_{z,N}]^\\top$. Define a background subspace by the columns of the matrix $U \\in \\mathbb{R}^{D\\times 4}$, whose columns are the indicator vectors $u_E, u_{p_x}, u_{p_y}, u_{p_z}$, where each $u$ has entries equal to $1$ at positions corresponding to its coordinate kind across all particles and $0$ elsewhere. The orthogonal projection onto the column space of $U$ is\n$$\nP \\equiv U (U^\\top U)^{-1} U^\\top.\n$$\nDefine the anomaly score\n$$\ns(x) \\equiv \\|(I - P)x\\|_2^2,\n$$\nthat is, the squared norm of the residual orthogonal to the background subspace.\n\nYou must propose and implement an attribution method that localizes anomalous constituents within events by computing integrated gradients of $s(x)$ with respect to the input $x$. Use the straight-line path from a zero baseline $x_0 = 0$ to the input $x$, and the standard path-integral definition of integrated gradients for each component $i$,\n$$\n\\mathrm{IG}_i(x) \\equiv (x_i - x_{0,i}) \\int_{0}^{1} \\frac{\\partial s\\big(x_0 + \\alpha (x - x_0)\\big)}{\\partial x_i}\\, d\\alpha.\n$$\nAggregate attributions per constituent $i$ by summing its four components,\n$$\na_i \\equiv \\sum_{j \\in \\{E,p_x,p_y,p_z\\}} \\mathrm{IG}_{(i,j)}(x).\n$$\n\nValidate faithfulness using two tests.\n\n1) Counterfactual removals: select the indices of the top-$k$ constituents by $a_i$ (highest values), remove them by setting their four-vectors to the zero vector, and recompute $s(x)$. Similarly, remove the bottom-$k$ constituents and recompute $s(x)$. Define the removal drop fractions\n$$\n\\Delta_{\\mathrm{top}} \\equiv \n\\begin{cases}\n\\dfrac{s(x) - s(x_{\\text{remove-top-}k})}{s(x)},  \\text{if } s(x) > 0 \\\\\n0,  \\text{if } s(x) = 0\n\\end{cases}\n\\quad\n\\Delta_{\\mathrm{bottom}} \\equiv \n\\begin{cases}\n\\dfrac{s(x) - s(x_{\\text{remove-bottom-}k})}{s(x)},  \\text{if } s(x) > 0 \\\\\n0,  \\text{if } s(x) = 0\n\\end{cases}\n$$\nReport the removal advantage $\\Delta_{\\mathrm{adv}} \\equiv \\Delta_{\\mathrm{top}} - \\Delta_{\\mathrm{bottom}}$.\n\n2) Causal perturbations of particle four-vectors: select the most-attributed constituent index $i^\\star \\equiv \\arg\\max_i a_i$ and the least-attributed constituent index $i_\\mathrm{min} \\equiv \\arg\\min_i a_i$. For each of these, apply a small causal perturbation to its three-momentum by scaling $\\vec{p} = (p_x,p_y,p_z)$ to $(1-\\epsilon)\\,\\vec{p}$ with a given $\\epsilon  0$, and then set its energy to $E \\leftarrow \\|\\vec{p}\\|_2$ to preserve the massless condition. Recompute the anomaly scores $s(x_{\\text{perturb-top}})$ and $s(x_{\\text{perturb-bottom}})$ and define the perturbation efficacy ratio\n$$\nR \\equiv \n\\frac{\\max\\big(s(x) - s(x_{\\text{perturb-top}}),\\, 0\\big) + \\delta}\n{\\max\\big(s(x) - s(x_{\\text{perturb-bottom}}),\\, 0\\big) + \\delta},\n$$\nwith a fixed stabilization constant $\\delta = 10^{-12}$ to avoid division by zero. Larger $R$ indicates that reducing the most-attributed constituent’s momentum decreases the anomaly score more effectively than an equivalent perturbation on the least-attributed constituent.\n\nAdditionally, verify the completeness property\n$$\n\\sum_{i=1}^{D} \\mathrm{IG}_i(x) \\stackrel{?}{=} s(x) - s(x_0),\n$$\nand report a boolean indicating whether $|\\sum_i \\mathrm{IG}_i(x) - s(x)| \\leq 10^{-9}$.\n\nYour program must implement the above attribution and validation on the following test suite. For each test case, first construct the event’s four-vectors by computing $E_i = \\sqrt{p_{x,i}^2 + p_{y,i}^2 + p_{z,i}^2}$ from the provided three-momenta, then form $x$ by concatenation in the prescribed order. Use $\\epsilon = 0.1$.\n\nTest suite:\n- Case $1$ (happy path): $N=3$, $k=1$. Three-momenta:\n  - Particle $1$: $(p_x,p_y,p_z) = (2.0, 0.1, 9.75)$,\n  - Particle $2$: $(p_x,p_y,p_z) = (2.1, -0.2, 9.6)$,\n  - Particle $3$: $(p_x,p_y,p_z) = (0.5, 6.0, 1.0)$.\n- Case $2$ (boundary with zero score): $N=2$, $k=1$. Three-momenta:\n  - Particle $1$: $(p_x,p_y,p_z) = (1.0, 2.0, 3.0)$,\n  - Particle $2$: $(p_x,p_y,p_z) = (1.0, 2.0, 3.0)$.\n- Case $3$ (edge with multiple anomalous constituents): $N=4$, $k=2$. Three-momenta:\n  - Particle $1$: $(p_x,p_y,p_z) = (3.0, 0.2, 2.0)$,\n  - Particle $2$: $(p_x,p_y,p_z) = (3.2, -0.1, 1.9)$,\n  - Particle $3$: $(p_x,p_y,p_z) = (0.0, 5.0, 0.5)$,\n  - Particle $4$: $(p_x,p_y,p_z) = (-0.5, -4.5, 0.4)$.\n\nFinal output format:\nYour program must output a single line containing a flat list aggregating the results of the three cases in order. For each case, output the triple\n$[\\text{completeness\\_ok},\\, \\Delta_{\\mathrm{adv}},\\, R]$,\nand concatenate the three triples into one flat list. Concretely, your output must be a single line of the form\n$[\\text{b}_1,\\Delta_{\\mathrm{adv},1},R_1,\\text{b}_2,\\Delta_{\\mathrm{adv},2},R_2,\\text{b}_3,\\Delta_{\\mathrm{adv},3},R_3]$,\nwhere each $\\text{b}_i$ is a boolean and each $\\Delta_{\\mathrm{adv},i}$ and $R_i$ is a floating-point number.", "solution": "The user wants me to solve a computational problem in high-energy physics involving anomaly detection, attribution, and validation.\n\n### Step 1: Problem Validation\n\nI have meticulously reviewed the problem statement. The givens are:\n- The representation of a particle event as a set of four-vectors and a concatenated vector $x$.\n- The massless condition $E = \\|\\vec{p}\\|_2$.\n- The definition of a background subspace via indicator vectors in a matrix $U$.\n- The formula for an orthogonal projection matrix $P = U(U^\\top U)^{-1}U^\\top$.\n- The definition of the anomaly score $s(x) = \\|(I-P)x\\|_2^2$.\n- The formula for Integrated Gradients (IG) attribution with a zero baseline.\n- The aggregation method for constituent-level attributions $a_i$.\n- Two validation tests: counterfactual removals and causal perturbations, with explicit formulas for $\\Delta_{\\mathrm{adv}}$ and $R$.\n- One verification test: the completeness property of IG.\n- All necessary parameters ($\\epsilon=0.1, \\delta=10^{-12}$) and a complete test suite with three cases.\n- A precise output format.\n\nThe problem is scientifically grounded, using established concepts from linear algebra, vector calculus, and machine learning explainability applied to a simplified but plausible physics scenario. It is well-posed, with all definitions and data provided, ensuring a unique, computable solution. All terms are objective and precisely defined. No scientific, formal, or logical flaws were found. The problem is valid.\n\n### Step 2: Derivations and Simplifications\n\nBefore implementation, I will simplify the mathematical expressions to create an efficient algorithm.\n\nThe matrix $U \\in \\mathbb{R}^{4N \\times 4}$ consists of four orthogonal column vectors $u_k$ for $k \\in \\{E, p_x, p_y, p_z\\}$. Each vector has $N$ entries equal to $1$ and the rest are $0$. The inner product is $u_k^\\top u_j = N \\delta_{kj}$, where $\\delta_{kj}$ is the Kronecker delta.\nThus, $U^\\top U = N \\cdot I_4$, where $I_4$ is the $4 \\times 4$ identity matrix.\nThe inverse is $(U^\\top U)^{-1} = \\frac{1}{N} I_4$.\nThe projection matrix simplifies to $P = U (\\frac{1}{N} I_4) U^\\top = \\frac{1}{N} U U^\\top$.\n\nThe action of $P$ on an event vector $x$ can be understood by first computing $U^\\top x$. This product results in a $4 \\times 1$ vector containing the sum of all energies, all $p_x$, all $p_y$, and all $p_z$ components, respectively (i.e., the total four-momentum of the event).\nLet $\\bar{v} = (\\bar{E}, \\bar{p}_x, \\bar{p}_y, \\bar{p}_z)$ be the average four-momentum across the $N$ constituents. Then the vector $Px$ is the event vector where each constituent's four-momentum is replaced by this average four-momentum.\n\nThe vector $(I-P)x = x - Px$ is therefore the event vector of deviations, where each constituent's four-vector is replaced by its deviation from the average four-vector: $(E_i - \\bar{E}, p_{x,i} - \\bar{p}_x, \\dots)$.\nThe anomaly score $s(x) = \\|(I-P)x\\|_2^2$ is the sum of the squared norms of these deviation four-vectors. It measures the internal variance of the four-momenta within the event.\n$s(x) = \\sum_{i=1}^N \\left( (E_i - \\bar{E})^2 + (p_{x,i} - \\bar{p}_x)^2 + (p_{y,i} - \\bar{p}_y)^2 + (p_{z,i} - \\bar{p}_z)^2 \\right)$.\n\nFor the attribution, the Integrated Gradients integral can be solved analytically. The score function is a quadratic form, $s(y) = y^\\top (I-P) y$. The gradient is $\\nabla s(y) = 2(I-P)y$.\nUsing the path $\\gamma(\\alpha) = \\alpha x$ (since baseline $x_0=0$), the IG formula becomes:\n$$\n\\mathrm{IG}_i(x) = x_i \\int_0^1 \\frac{\\partial s(\\alpha x)}{\\partial(\\alpha x_i)} d\\alpha\n$$\nThe term inside the integral is the $i$-th component of the gradient of $s$ evaluated at $\\alpha x$, which is $(2(I-P)(\\alpha x))_i = 2\\alpha((I-P)x)_i$.\nThe integral evaluates to:\n$$\n\\mathrm{IG}_i(x) = x_i \\cdot \\left( ((I-P)x)_i \\int_0^1 2\\alpha d\\alpha \\right) = x_i \\cdot ((I-P)x)_i \\cdot [\\alpha^2]_0^1 = x_i ((I-P)x)_i\n$$\nThe vector of integrated gradients is simply the element-wise product of the input vector $x$ and the residual vector $(I-P)x$.\n\nThe completeness property, $\\sum_{i} \\mathrm{IG}_i(x) = s(x)$, follows directly from this simplified form:\n$$\n\\sum_i \\mathrm{IG}_i(x) = \\sum_i x_i ((I-P)x)_i = x^\\top (I-P)x\n$$\nSince $P$ is a projection matrix, $I-P$ is symmetric and idempotent, so $s(x) = x^\\top (I-P)^\\top (I-P)x = x^\\top (I-P)x$. The property holds analytically and any deviation will be due to floating-point error.\n\nThe constituent attribution $a_j$ is the sum of IG components for that particle, which is the dot product of the particle's four-vector with its deviation from the average four-vector.\n\n### Step 3: Algorithmic Implementation\n\nThe solution will be implemented in Python using the `numpy` library.\n1.  A main loop iterates through the test cases.\n2.  For each case, a `process_case` function is called.\n3.  Inside `process_case`:\n    a. The initial event vector $x$ is constructed from the given three-momenta, calculating energies via $E = \\|\\vec{p}\\|_2$.\n    b. Helper functions `compute_s` and `compute_ig_vector` are defined based on the simplified formulas above. They operate on a flattened event vector and the number of particles $N$.\n    c. The initial score $s(x)$ and constituent attributions $\\{a_i\\}$ are computed.\n    d. The completeness property is checked.\n    e. The indices of the top-$k$ and bottom-$k$ constituents are determined by sorting the attributions.\n    f. For the removal test, two new vectors, $x_{\\text{remove-top-}k}$ and $x_{\\text{remove-bottom-}k}$, are created by zeroing out the corresponding four-vectors. Their scores are computed, and $\\Delta_{\\mathrm{adv}}$ is calculated.\n    g. For the perturbation test, the momenta of the most- and least-attributed constituents are scaled by $(1-\\epsilon)$, their energies are re-calculated to maintain the massless condition, and two new event vectors are created. Their scores are computed, and the ratio $R$ is calculated.\n    h. The function returns the triplet `(completeness_ok, delta_adv, R)`.\n4.  The results from all cases are aggregated into a single list and printed in the specified format.\n\nThis structured approach ensures all requirements of the problem are met precisely.", "answer": "```python\nimport numpy as np\n\ndef compute_s(x_flat: np.ndarray, n_particles: int) - float:\n    \"\"\"Computes the anomaly score s(x) for a flattened event vector.\"\"\"\n    if n_particles == 0:\n        return 0.0\n    \n    four_vectors = x_flat.reshape(n_particles, 4)\n    \n    avg_four_vec = np.mean(four_vectors, axis=0)\n    devs = four_vectors - avg_four_vec\n    s = np.sum(devs**2)\n    return float(s)\n\ndef compute_ig_vector(x_flat: np.ndarray, n_particles: int) - np.ndarray:\n    \"\"\"Computes the vector of Integrated Gradients.\"\"\"\n    if n_particles == 0:\n        return np.zeros_like(x_flat)\n        \n    four_vectors = x_flat.reshape(n_particles, 4)\n    avg_four_vec = np.mean(four_vectors, axis=0)\n    \n    # The vector Px has the average four-vector repeated for each particle.\n    avg_vec_tiled = np.tile(avg_four_vec, n_particles)\n    \n    # Residual vector (I-P)x\n    residual_vec = x_flat - avg_vec_tiled\n    \n    # IG(x) = x element-wise-product (I-P)x\n    ig_vec = x_flat * residual_vec\n    return ig_vec\n\ndef process_case(three_momenta, k, epsilon, delta):\n    \"\"\"Processes a single test case to compute validation metrics.\"\"\"\n    \n    n_particles = len(three_momenta)\n    if n_particles == 0:\n        return True, 0.0, 1.0\n\n    # Step 1: Construct the input vector x from three-momenta\n    four_vectors_list = []\n    for p_vec_tuple in three_momenta:\n        p_vec = np.array(p_vec_tuple, dtype=np.float64)\n        energy = np.linalg.norm(p_vec)\n        four_vectors_list.append(np.concatenate(([energy], p_vec)))\n    \n    four_vectors = np.array(four_vectors_list)\n    x = four_vectors.flatten()\n\n    # Step 2: Calculate original anomaly score\n    s_x = compute_s(x, n_particles)\n\n    # Step 3: Compute Integrated Gradients and constituent attributions\n    ig_vector = compute_ig_vector(x, n_particles)\n    attributions = np.sum(ig_vector.reshape(n_particles, 4), axis=1)\n\n    # Step 4: Verify completeness property\n    completeness_ok = np.abs(np.sum(ig_vector) - s_x) = 1e-9\n\n    # Find ranked indices for attributions\n    sorted_indices = np.argsort(attributions)\n\n    # Step 5: Faithfulness Test 1: Counterfactual removals\n    top_k_indices = sorted_indices[-k:]\n    bottom_k_indices = sorted_indices[:k]\n\n    x_rem_top = x.copy()\n    for idx in top_k_indices:\n        x_rem_top[idx*4:(idx+1)*4] = 0.0\n    s_rem_top = compute_s(x_rem_top, n_particles)\n\n    x_rem_bot = x.copy()\n    for idx in bottom_k_indices:\n        x_rem_bot[idx*4:(idx+1)*4] = 0.0\n    s_rem_bot = compute_s(x_rem_bot, n_particles)\n\n    if s_x  1e-12:\n        delta_top = (s_x - s_rem_top) / s_x\n        delta_bottom = (s_x - s_rem_bot) / s_x\n    else:\n        delta_top = 0.0\n        delta_bottom = 0.0\n    \n    delta_adv = delta_top - delta_bottom\n\n    # Step 6: Faithfulness Test 2: Causal perturbations\n    i_star = sorted_indices[-1]\n    i_min = sorted_indices[0]\n\n    # Perturb top-attributed constituent\n    x_pert_top_4vecs = four_vectors.copy()\n    p_vec_top = x_pert_top_4vecs[i_star, 1:4]\n    p_vec_top_pert = (1.0 - epsilon) * p_vec_top\n    x_pert_top_4vecs[i_star, 1:4] = p_vec_top_pert\n    x_pert_top_4vecs[i_star, 0] = np.linalg.norm(p_vec_top_pert)\n    s_pert_top = compute_s(x_pert_top_4vecs.flatten(), n_particles)\n\n    # Perturb bottom-attributed constituent\n    x_pert_bot_4vecs = four_vectors.copy()\n    p_vec_bot = x_pert_bot_4vecs[i_min, 1:4]\n    p_vec_bot_pert = (1.0 - epsilon) * p_vec_bot\n    x_pert_bot_4vecs[i_min, 1:4] = p_vec_bot_pert\n    x_pert_bot_4vecs[i_min, 0] = np.linalg.norm(p_vec_bot_pert)\n    s_pert_bot = compute_s(x_pert_bot_4vecs.flatten(), n_particles)\n\n    numerator = np.maximum(s_x - s_pert_top, 0) + delta\n    denominator = np.maximum(s_x - s_pert_bot, 0) + delta\n    R = numerator / denominator\n\n    return completeness_ok, delta_adv, R\n\ndef solve():\n    \"\"\"Main function to run all test cases and print results.\"\"\"\n    test_cases = [\n        {\n            \"momenta\": [(2.0, 0.1, 9.75), (2.1, -0.2, 9.6), (0.5, 6.0, 1.0)],\n            \"k\": 1,\n        },\n        {\n            \"momenta\": [(1.0, 2.0, 3.0), (1.0, 2.0, 3.0)],\n            \"k\": 1,\n        },\n        {\n            \"momenta\": [(3.0, 0.2, 2.0), (3.2, -0.1, 1.9), (0.0, 5.0, 0.5), (-0.5, -4.5, 0.4)],\n            \"k\": 2,\n        },\n    ]\n\n    epsilon = 0.1\n    delta = 1e-12\n    \n    results = []\n    for case in test_cases:\n        res_tuple = process_case(case[\"momenta\"], case[\"k\"], epsilon, delta)\n        results.extend(res_tuple)\n    \n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3504728"}]}