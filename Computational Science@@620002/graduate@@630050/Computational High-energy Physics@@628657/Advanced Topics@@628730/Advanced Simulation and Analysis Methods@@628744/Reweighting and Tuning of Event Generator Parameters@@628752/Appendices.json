{"hands_on_practices": [{"introduction": "Reweighting allows us to assess the impact of a different theoretical model without the immense computational cost of regenerating a full Monte Carlo event sample. This practice explores the foundational principle of this technique through the lens of Parton Distribution Function (PDF) reweighting. By starting from the QCD factorization theorem, you will see how the weight of a single simulated event can be analytically rescaled to reflect a change in the underlying PDFs, a common task when evaluating PDF uncertainties or comparing different PDF sets [@problem_id:3532078].", "problem": "An event generator has produced a single proton–proton event whose hard process is initiated by an up quark and an anti-up quark, i.e., the incoming flavors are $(u,\\bar{u})$. The first parton carries longitudinal momentum fraction $x_1=0.1$ from beam $1$ and the second parton carries $x_2=0.02$ from beam $2$. The event was generated at the factorization scale $\\mu_F=100\\ \\mathrm{GeV}$, and the renormalization scale is set equal to the factorization scale, $\\mu_R=\\mu_F$. Assume the leading-order partonic subprocess for $(u,\\bar{u})$ is unchanged under reweighting and that the only modification when moving between two parton distribution function (PDF) sets is in the PDFs themselves. Here, a PDF refers to a Parton Distribution Function (PDF), $f_i(x,\\mu_F)$, which gives the probability density to find a parton of flavor $i$ carrying momentum fraction $x$ at scale $\\mu_F$.\n\nYou are given central-member values from two different PDF sets, denoted $S_A$ (the original set used to generate the event) and $S_B$ (the target set to which the event will be reweighted). The numerical values (dimensionless) are:\n- For $S_A$: $f_u^{A}(x_1=0.1,\\mu_F=100\\ \\mathrm{GeV})=2.70$ and $f_{\\bar{u}}^{A}(x_2=0.02,\\mu_F=100\\ \\mathrm{GeV})=0.25$.\n- For $S_B$: $f_u^{B}(x_1=0.1,\\mu_F=100\\ \\mathrm{GeV})=2.35$ and $f_{\\bar{u}}^{B}(x_2=0.02,\\mu_F=100\\ \\mathrm{GeV})=0.27$.\n\nStarting from the factorization theorem for hadronic cross sections and basic properties of event weights in factorized Monte Carlo event generation, derive the reweighting factor $R$ that rescales the event weight from $S_A$ to $S_B$ for this event and compute its value. Express the final ratio $R$ as a dimensionless number and round your answer to four significant figures.\n\nIn addition, explain, using first principles and without invoking any pre-provided shortcut formulas, how uncertainty eigenvectors in a Hessian PDF set $\\{(k,+),(k,-)\\}_{k=1}^{N}$ can be propagated to an arbitrary observable via event-by-event PDF reweighting. Your explanation should clearly articulate the construction of eigenvector-specific event weights and how they are combined to yield the uncertainty of a weighted observable, but your final numeric answer must be only the reweighting factor $R$.", "solution": "The problem requires the derivation and calculation of a reweighting factor for a single high-energy physics event when changing the underlying Parton Distribution Function (PDF) set. It also asks for a conceptual explanation of how PDF uncertainties from a Hessian set are propagated to an observable. We will address these two parts in sequence.\n\nFirst, we validate the problem statement.\nThe givens are:\n- Incoming parton flavors: $(u,\\bar{u})$\n- Parton momentum fractions: $x_1=0.1$ from beam $1$ and $x_2=0.02$ from beam $2$.\n- Factorization scale: $\\mu_F=100\\ \\mathrm{GeV}$.\n- Renormalization scale: $\\mu_R=\\mu_F$.\n- Original PDF set $S_A$ values: $f_u^{A}(x_1=0.1,\\mu_F=100\\ \\mathrm{GeV})=2.70$ and $f_{\\bar{u}}^{A}(x_2=0.02,\\mu_F=100\\ \\mathrm{GeV})=0.25$.\n- Target PDF set $S_B$ values: $f_u^{B}(x_1=0.1,\\mu_F=100\\ \\mathrm{GeV})=2.35$ and $f_{\\bar{u}}^{B}(x_2=0.02,\\mu_F=100\\ \\mathrm{GeV})=0.27$.\n- Key assumptions: the leading-order partonic subprocess is unchanged, and only the PDFs themselves are modified between sets.\n\nThe problem is scientifically grounded in the theory of perturbative Quantum Chromodynamics (QCD) and its application in Monte Carlo event generation. The framework of PDF reweighting is standard practice in high-energy physics. The provided values are realistic, and the question is well-posed, self-contained, and objective. It does not violate any fundamental principles and is directly relevant to the specified topic. Hence, the problem is deemed valid.\n\nWe now proceed with the solution.\n\nPart 1: Derivation and Calculation of the Reweighting Factor $R$.\n\nThe foundation for calculating cross sections in hadronic collisions is the QCD factorization theorem. It states that the differential cross section $d\\sigma$ for a process $pp \\to X$ can be calculated by summing over all possible initial parton flavors $i$ and $j$, integrating over all possible momentum fractions $x_1$ and $x_2$, and convolving the PDFs with the partonic hard-scattering cross section $d\\hat{\\sigma}_{ij}$:\n$$\nd\\sigma = \\sum_{i,j} \\int dx_1 \\int dx_2 f_i(x_1, \\mu_F) f_j(x_2, \\mu_F) d\\hat{\\sigma}_{ij}(x_1, x_2, \\mu_F, \\mu_R)\n$$\nHere, $f_i(x, \\mu_F)$ is the PDF for parton flavor $i$ carrying momentum fraction $x$ of the proton's momentum at factorization scale $\\mu_F$. The term $d\\hat{\\sigma}_{ij}$ is the differential cross section for the partonic subprocess $i+j \\to F$, where $F$ is the final state of the hard scatter.\n\nA Monte Carlo event generator produces events corresponding to specific points in the available phase space. For a single event generated for a specific hard process, the initial partons and their momentum fractions are fixed. The weight $w$ of such an unweighted event (before showering, hadronization, and detector simulation) is proportional to the value of the integrand of the cross section formula at that specific kinematic point.\n\nFor the given event, the incoming partons are an up quark ($u$) from proton $1$ and an anti-up quark ($\\bar{u}$) from proton $2$. Their respective momentum fractions are $x_1=0.1$ and $x_2=0.02$. The event was generated using PDF set $S_A$. Therefore, its original weight, $w_A$, is proportional to the product of the relevant PDFs from set $S_A$ and the partonic cross section $\\hat{\\sigma}_{u\\bar{u}}$:\n$$\nw_A \\propto f_u^A(x_1, \\mu_F) f_{\\bar{u}}^A(x_2, \\mu_F) \\hat{\\sigma}_{u\\bar{u}}\n$$\nWe wish to determine the weight this exact same event would have if it had been generated using PDF set $S_B$. Let this new weight be $w_B$. According to the problem statement, the partonic cross section $\\hat{\\sigma}_{u\\bar{u}}$ and all other aspects of the event kinematics are unchanged. The only change is the PDF set. Thus, the new weight $w_B$ is:\n$$\nw_B \\propto f_u^B(x_1, \\mu_F) f_{\\bar{u}}^B(x_2, \\mu_F) \\hat{\\sigma}_{u\\bar{u}}\n$$\nThe reweighting factor $R$ is defined as the ratio of the new weight to the old weight, $R = w_B / w_A$. The proportionality constant and the partonic cross section $\\hat{\\sigma}_{u\\bar{u}}$ cancel in the ratio:\n$$\nR = \\frac{f_u^B(x_1, \\mu_F) f_{\\bar{u}}^B(x_2, \\mu_F)}{f_u^A(x_1, \\mu_F) f_{\\bar{u}}^A(x_2, \\mu_F)}\n$$\nWe can now substitute the provided numerical values into this expression.\nGiven:\n- $x_1 = 0.1$, $x_2 = 0.02$, $\\mu_F = 100\\ \\mathrm{GeV}$\n- $f_u^A(x_1, \\mu_F) = 2.70$\n- $f_{\\bar{u}}^A(x_2, \\mu_F) = 0.25$\n- $f_u^B(x_1, \\mu_F) = 2.35$\n- $f_{\\bar{u}}^B(x_2, \\mu_F) = 0.27$\n\nThe reweighting factor $R$ is:\n$$\nR = \\frac{(2.35) \\times (0.27)}{(2.70) \\times (0.25)} = \\frac{0.6345}{0.675}\n$$\nCalculating this value gives:\n$$\nR = 0.94\n$$\nThe problem requires the answer to be rounded to four significant figures. Therefore, $R = 0.9400$.\n\nPart 2: Explanation of Hessian PDF Uncertainty Propagation.\n\nThe second part of the task is to explain, from first principles, the propagation of PDF uncertainties using Hessian eigenvector sets.\n\nA modern PDF set, such as one following the Hessian method, provides not just a central (best-fit) PDF, denoted $S_0$, but also a collection of $N$ pairs of \"eigenvector\" sets, $\\{(S_k^+, S_k^-)\\}_{k=1}^N$. These eigenvector sets represent systematic excursions from the central fit along $N$ orthogonal directions in the multi-dimensional parameter space used for the PDF determination. Each pair $(S_k^+, S_k^-)$ corresponds to a $\\pm 1\\sigma$ deviation along the $k$-th eigenvector.\n\nThe process of propagating these uncertainties to a predicted observable $\\mathcal{O}$ relies on event-by-event reweighting, following the same principle derived in the first part. Let us assume a large sample of $M$ Monte Carlo events has been generated using the central PDF set $S_0$. Each event $e$ (for $e=1, \\dots, M$) has a weight $w_{0,e}$ and a value $\\mathcal{O}_e$ for the observable in question. The prediction for the observable using the central PDF set is the weighted average:\n$$\n\\langle \\mathcal{O} \\rangle_0 = \\frac{\\sum_{e=1}^M w_{0,e} \\mathcal{O}_e}{\\sum_{e=1}^M w_{0,e}}\n$$\nTo determine the impact of the PDF uncertainty, one does not need to generate new event samples for each of the $2N$ eigenvector sets. Instead, one can calculate $2N$ new weights for each event. For a single event $e$ with initial partons $i,j$ at momentum fractions $(x_{1,e}, x_{2,e})$, the reweighting factors from the central set $S_0$ to an eigenvector set $S_k^\\pm$ are:\n$$\nR_{k,e}^\\pm = \\frac{f_i^{(k,\\pm)}(x_{1,e}, \\mu_F) f_j^{(k,\\pm)}(x_{2,e}, \\mu_F)}{f_i^{(0)}(x_{1,e}, \\mu_F) f_j^{(0)}(x_{2,e}, \\mu_F)}\n$$\nThe new weight for event $e$ corresponding to PDF set $S_k^\\pm$ is $w_{k,e}^\\pm = w_{0,e} \\times R_{k,e}^\\pm$. Using these new weights, one can compute the value of the observable $\\mathcal{O}$ as it would be predicted by each eigenvector set:\n$$\n\\langle \\mathcal{O} \\rangle_k^\\pm = \\frac{\\sum_{e=1}^M w_{k,e}^\\pm \\mathcal{O}_e}{\\sum_{e=1}^M w_{k,e}^\\pm}\n$$\nThis procedure yields a central value $\\langle \\mathcal{O} \\rangle_0$ and $2N$ systematic variations, $\\langle \\mathcal{O} \\rangle_k^+$ and $\\langle \\mathcal{O} \\rangle_k^-$, for $k=1, \\dots, N$.\n\nThe final step is to combine these variations into a single total uncertainty. The Hessian formalism provides a master formula for this. Because the eigenvectors are constructed to be orthogonal in the parameter space of the PDF fit, their contributions to the total uncertainty can be added in quadrature. The symmetric formula for the total PDF uncertainty, $\\Delta \\mathcal{O}$, is:\n$$\n(\\Delta\\mathcal{O})^2 = \\sum_{k=1}^N \\left( \\frac{\\langle \\mathcal{O} \\rangle_k^+ - \\langle \\mathcal{O} \\rangle_k^-}{2} \\right)^2\n$$\nThe term $(\\langle \\mathcal{O} \\rangle_k^+ - \\langle \\mathcal{O} \\rangle_k^-)/2$ represents the change in the observable $\\mathcal{O}$ for a $+1\\sigma$ shift along the $k$-th eigenvector. Summing these shifts in quadrature gives the total variance due to the uncertainties in all PDF parameters. Asymmetric formulas also exist to handle cases where the upward and downward variations are not symmetric, but the principle of combining orthogonal contributions remains the same. This reweighting technique is computationally efficient, as it allows the estimation of PDF uncertainties from a single, centrally-generated event sample.", "answer": "$$\n\\boxed{0.9400}\n$$", "id": "3532078"}, {"introduction": "While powerful, event reweighting is not statistically \"free\" and can degrade the precision of a Monte Carlo sample. This practice introduces the concept of the effective sample size, $N_{\\text{eff}}$, a crucial metric for quantifying this loss of statistical power, particularly in the presence of the positive and negative weights characteristic of next-to-leading order (NLO) calculations. Working through this exercise will provide you with the tools to diagnose the statistical health of a weighted sample and reason about practical strategies to mitigate excessive variance in a physics analysis [@problem_id:3532093].", "problem": "You are performing parameter reweighting and tuning of a Next-to-Leading Order (NLO) event generator with a parton shower. A Monte Carlo (MC) sample of $N$ events is produced at a reference parameter value $\\theta_{0}$, with signed base event weights $\\{w_{i}^{(0)}\\}_{i=1}^{N}$ that include negative weights from subtraction terms. You consider a small retune to a nearby value $\\theta$, and approximate the per-event reweight factor by a first-order linear response model $r_{i}(\\theta)=1+a_{i}(\\theta-\\theta_{0})$, where the sensitivities $a_{i}$ are event-dependent and known. The reweighted event weights are $w_{i}=w_{i}^{(0)}\\,r_{i}(\\theta)$, and the estimator for the total cross section at $\\theta$ is the weighted sum $\\hat{\\sigma}=\\sum_{i=1}^{N} w_{i}$. Assume events are independent and that the standard MC variance estimate for a weighted sum applies.\n\nStarting only from the following foundations:\n- The definition of a weighted-sum estimator $\\hat{\\sigma}=\\sum_{i=1}^{N} w_{i}$ for independent events.\n- The well-tested fact that the variance of an independent weighted-sum estimator is $\\mathrm{Var}(\\hat{\\sigma})=\\sum_{i=1}^{N} w_{i}^{2}$ under standard MC sampling assumptions.\n- The definition of the effective sample size $N_{\\text{eff}}$ as the size of an equivalent unweighted sample that would yield the same relative statistical uncertainty, implying $N_{\\text{eff}}=(\\sum_{i=1}^{N} w_{i})^{2}/\\sum_{i=1}^{N} w_{i}^{2}$ and the relative statistical uncertainty $u_{\\text{rel}}=\\sqrt{\\mathrm{Var}(\\hat{\\sigma})}/|\\hat{\\sigma}|$.\n\nConsider the concrete sample with $N=10$, parameter shift $\\theta-\\theta_{0}=0.1$, and the following per-event data:\n- Event $1$: $w_{1}^{(0)}=1.2$, $a_{1}=0.05$.\n- Event $2$: $w_{2}^{(0)}=0.8$, $a_{2}=0.10$.\n- Event $3$: $w_{3}^{(0)}=-0.6$, $a_{3}=0.20$.\n- Event $4$: $w_{4}^{(0)}=0.5$, $a_{4}=-0.15$.\n- Event $5$: $w_{5}^{(0)}=-1.0$, $a_{5}=0.05$.\n- Event $6$: $w_{6}^{(0)}=2.0$, $a_{6}=0.00$.\n- Event $7$: $w_{7}^{(0)}=1.1$, $a_{7}=-0.10$.\n- Event $8$: $w_{8}^{(0)}=-0.9$, $a_{8}=0.15$.\n- Event $9$: $w_{9}^{(0)}=0.7$, $a_{9}=0.05$.\n- Event $10$: $w_{10}^{(0)}=-0.4$, $a_{10}=-0.20$.\n\nTasks:\n1. Compute the reweighted weights $w_{i}=w_{i}^{(0)}\\,[1+a_{i}(\\theta-\\theta_{0})]$ for all $i$, then compute $N_{\\text{eff}}$ and the relative statistical uncertainty $u_{\\text{rel}}=\\sqrt{\\sum_{i=1}^{N} w_{i}^{2}}/\\left|\\sum_{i=1}^{N} w_{i}\\right|$ on $\\hat{\\sigma}$ at $\\theta$. Report $N_{\\text{eff}}$ as a pure number and $u_{\\text{rel}}$ as a decimal fraction.\n2. Briefly justify, from the definitions above, why $N_{\\text{eff}}=(\\sum_{i} w_{i})^{2}/\\sum_{i} w_{i}^{2}$ and why negative weights tend to reduce $N_{\\text{eff}}$ for fixed $\\sum_{i} w_{i}^{2}$.\n3. Identify and explain at least three principled strategies that can mitigate variance blow-up when reweighting and tuning in the presence of signed weights, ensuring that each strategy is compatible with preserving unbiasedness in the target observable.\n\nRound both $N_{\\text{eff}}$ and $u_{\\text{rel}}$ to four significant figures. Express the final answer as a row vector $\\big(N_{\\text{eff}},\\,u_{\\text{rel}}\\big)$ with no units, where $u_{\\text{rel}}$ is a pure number (decimal fraction).", "solution": "The problem asks for calculations and conceptual explanations related to parameter reweighting in a Next-to-Leading Order (NLO) Monte Carlo event generator. The problem is validated as self-contained, scientifically grounded, and well-posed. We will address each of the three tasks in sequence.\n\nFirst, we address Task 1: computing the reweighted weights $w_i$, the effective sample size $N_{\\text{eff}}$, and the relative statistical uncertainty $u_{\\text{rel}}$.\n\nThe parameter shift is given as $\\theta - \\theta_0 = 0.1$. The reweight factor for each event $i$ is calculated using the provided linear response model: $r_i(\\theta) = 1 + a_i(\\theta - \\theta_0) = 1 + a_i \\times 0.1$. The reweighted weight is then $w_i = w_i^{(0)} r_i(\\theta)$.\n\nWe compute the reweighted weight $w_i$ for each of the $N=10$ events:\n- Event $1$: $r_1 = 1 + 0.05 \\times 0.1 = 1.005$. $w_1 = 1.2 \\times 1.005 = 1.206$.\n- Event $2$: $r_2 = 1 + 0.10 \\times 0.1 = 1.01$. $w_2 = 0.8 \\times 1.01 = 0.808$.\n- Event $3$: $r_3 = 1 + 0.20 \\times 0.1 = 1.02$. $w_3 = -0.6 \\times 1.02 = -0.612$.\n- Event $4$: $r_4 = 1 - 0.15 \\times 0.1 = 0.985$. $w_4 = 0.5 \\times 0.985 = 0.4925$.\n- Event $5$: $r_5 = 1 + 0.05 \\times 0.1 = 1.005$. $w_5 = -1.0 \\times 1.005 = -1.005$.\n- Event $6$: $r_6 = 1 + 0.00 \\times 0.1 = 1.00$. $w_6 = 2.0 \\times 1.00 = 2.0$.\n- Event $7$: $r_7 = 1 - 0.10 \\times 0.1 = 0.99$. $w_7 = 1.1 \\times 0.99 = 1.089$.\n- Event $8$: $r_8 = 1 + 0.15 \\times 0.1 = 1.015$. $w_8 = -0.9 \\times 1.015 = -0.9135$.\n- Event $9$: $r_9 = 1 + 0.05 \\times 0.1 = 1.005$. $w_9 = 0.7 \\times 1.005 = 0.7035$.\n- Event $10$: $r_{10} = 1 - 0.20 \\times 0.1 = 0.98$. $w_{10} = -0.4 \\times 0.98 = -0.392$.\n\nNext, we compute the sum of the reweighted weights, $\\sum_{i=1}^{N} w_i$, and the sum of their squares, $\\sum_{i=1}^{N} w_i^2$.\nThe sum of weights is:\n$$ \\hat{\\sigma} = \\sum_{i=1}^{10} w_i = 1.206 + 0.808 - 0.612 + 0.4925 - 1.005 + 2.0 + 1.089 - 0.9135 + 0.7035 - 0.392 = 3.3765 $$\nThe sum of weights squared is:\n$$ \\sum_{i=1}^{10} w_i^2 = (1.206)^2 + (0.808)^2 + (-0.612)^2 + (0.4925)^2 + (-1.005)^2 + (2.0)^2 + (1.089)^2 + (-0.9135)^2 + (0.7035)^2 + (-0.392)^2 $$\n$$ \\sum_{i=1}^{10} w_i^2 = 1.454436 + 0.652864 + 0.374544 + 0.24255625 + 1.010025 + 4.0 + 1.185921 + 0.83448225 + 0.49491225 + 0.153664 \\approx 10.403405 $$\n\nNow we can compute $N_{\\text{eff}}$ and $u_{\\text{rel}}$.\nUsing the provided formula for effective sample size:\n$$ N_{\\text{eff}} = \\frac{(\\sum_{i=1}^{N} w_i)^2}{\\sum_{i=1}^{N} w_i^2} = \\frac{(3.3765)^2}{10.403405} = \\frac{11.40073225}{10.403405} \\approx 1.095866 $$\nRounding to four significant figures, $N_{\\text{eff}} = 1.096$.\n\nThe relative statistical uncertainty is given by:\n$$ u_{\\text{rel}} = \\frac{\\sqrt{\\sum_{i=1}^{N} w_i^2}}{\\left|\\sum_{i=1}^{N} w_i\\right|} = \\frac{\\sqrt{10.403405}}{|3.3765|} \\approx \\frac{3.225431}{3.3765} \\approx 0.955255 $$\nRounding to four significant figures, $u_{\\text{rel}} = 0.9553$.\nAs a consistency check, we can verify that $u_{\\text{rel}} \\approx 1/\\sqrt{N_{\\text{eff}}}$. Indeed, $1/\\sqrt{1.095866} \\approx 0.955255$, which matches our result for $u_{\\text{rel}}$.\n\nSecond, we address Task 2: justifying the formula for $N_{\\text{eff}}$ and the effect of negative weights.\n\nThe justification for $N_{\\text{eff}} = (\\sum w_i)^2 / \\sum w_i^2$ starts from its definition: $N_{\\text{eff}}$ is the size of an equivalent unweighted sample that would yield the same relative statistical uncertainty.\nLet us consider an unweighted sample of size $N_{\\text{eff}}$. An unweighted sample means all events have the same weight. Let the total cross section estimate be $\\hat{\\sigma}$. Then each of the $N_{\\text{eff}}$ events has a weight $w' = \\hat{\\sigma}/N_{\\text{eff}}$. The estimator is $\\sum_{i=1}^{N_{\\text{eff}}} w' = N_{\\text{eff}} \\times (\\hat{\\sigma}/N_{\\text{eff}}) = \\hat{\\sigma}$. The variance of this estimator is $\\sum_{i=1}^{N_{\\text{eff}}} (w')^2 = N_{\\text{eff}} \\times (\\hat{\\sigma}/N_{\\text{eff}})^2 = \\hat{\\sigma}^2/N_{\\text{eff}}$. The relative uncertainty for this unweighted sample is therefore $u'_{\\text{rel}} = \\sqrt{\\text{Var}} / |\\hat{\\sigma}| = \\sqrt{\\hat{\\sigma}^2/N_{\\text{eff}}} / |\\hat{\\sigma}| = 1/\\sqrt{N_{\\text{eff}}}$.\nFor the original weighted sample, the relative uncertainty is given as $u_{\\text{rel}} = \\sqrt{\\sum w_i^2} / |\\sum w_i|$.\nBy equating the relative uncertainties, $u'_{\\text{rel}} = u_{\\text{rel}}$, we get $1/\\sqrt{N_{\\text{eff}}} = \\sqrt{\\sum w_i^2} / |\\sum w_i|$. Squaring both sides gives $1/N_{\\text{eff}} = (\\sum w_i^2) / (\\sum w_i)^2$. Inverting this expression yields the desired formula: $N_{\\text{eff}} = (\\sum w_i)^2 / \\sum w_i^2$.\n\nNegative weights tend to reduce $N_{\\text{eff}}$ for a fixed sum of squares $\\sum w_i^2$. The formula for $N_{\\text{eff}}$ has the sum of squares, $\\sum w_i^2$, in the denominator and the square of the sum, $(\\sum w_i)^2$, in the numerator. The term $\\sum w_i^2$ is always non-negative. For a fixed value of this denominator, $N_{\\text{eff}}$ is maximized by maximizing the numerator, $(\\sum w_i)^2$.\nWhen some weights $w_i$ are positive and others are negative, cancellations occur in the sum $\\sum w_i$. By the triangle inequality, $|\\sum w_i| \\leq \\sum |w_i|$. For a mixed-sign set of weights, this inequality is typically strict, i.e., $|\\sum w_i| < \\sum |w_i|$.\nIf we consider a hypothetical sample with weights $w'_i = |w_i|$, it has the same sum of squares, $\\sum (w'_i)^2 = \\sum |w_i|^2 = \\sum w_i^2$. However, its sum of weights is $\\sum w'_i = \\sum |w_i|$.\nSince $|\\sum w_i| < \\sum |w_i|$, it follows that $(\\sum w_i)^2 < (\\sum |w_i|)^2$.\nTherefore, a sample with negative weights will have a smaller numerator $(\\sum w_i)^2$ compared to a sample where all weights are made positive, while the denominator $\\sum w_i^2$ remains the same. This leads to a smaller $N_{\\text{eff}}$. A smaller $N_{\\text{eff}}$ signifies a larger relative statistical uncertainty $u_{\\text{rel}} \\approx 1/\\sqrt{N_{\\text{eff}}}$.\n\nThird, we address Task 3: identifying three principled strategies to mitigate variance blow-up while preserving unbiasedness.\n\n1.  **Optimization of Matching and Shower Parameters**: In NLO with Parton Shower (NLO+PS) generators, large positive and negative weights often arise from a mismatch between the fixed-order real emission matrix element and the parton shower's approximation of it. By tuning parameters that control the separation between the hard and soft/collinear regimes (e.g., the shower starting scale, often called `hdamp` in POWHEG), one can achieve a smoother transition. This reduces the size of the subtraction terms and, consequently, the magnitudes of the resulting signed weights $|w_i^{(0)}|$ from the start. This directly reduces the sum of squares $\\sum w_i^2$ for a given total cross section $\\sum w_i$, thus increasing $N_{\\text{eff}}$. This procedure is compatible with unbiasedness because these parameters are part of the definition of how the matching is performed; changing them corresponds to a different, but formally equally valid, scheme for combining the NLO calculation with the shower. The integrated cross section remains unbiased.\n\n2.  **Definition of a Fiducial Phase Space**: Large weights are frequently generated in extreme regions of phase space where the theoretical model may be less reliable (e.g., very high transverse momentum) or that are outside the acceptance of an experimental analysis. A standard strategy is to define a fiducial volume by applying cuts on the final-state particles' kinematics (e.g., on momenta and pseudorapidities). Events falling outside this volume are discarded. This physically removes the source of the largest weights from the sample used for analysis and tuning. The variance is thus controlled. This preserves unbiasedness for the *fiducial cross section*, which is the quantity of interest in almost all comparisons between theory and experiment. The estimator for the fiducial cross section remains unbiased.\n\n3.  **Increasing the Size of the Monte Carlo Sample**: The effective sample size $N_{\\text{eff}}$ is proportional to the total number of generated events, $N$. Specifically, $N_{\\text{eff}} = N \\cdot (\\langle w \\rangle^2 / \\langle w^2 \\rangle)$, where $\\langle w \\rangle$ and $\\langle w^2 \\rangle$ are the sample averages of the weights and weights squared. For a given weight distribution, doubling the number of generated events $N$ will, on average, double $N_{\\text{eff}}$. This in turn reduces the relative statistical uncertainty $u_{\\text{rel}} \\approx 1/\\sqrt{N_{\\text{eff}}}$ by a factor of $1/\\sqrt{2}$. While computationally expensive, this \"brute force\" method is the most fundamental way to improve statistical precision. It is inherently unbiased and acts as a final recourse when other methods to improve the weight distribution are insufficient.", "answer": "$$\n\\boxed{\\begin{pmatrix} 1.096 & 0.9553 \\end{pmatrix}}\n$$", "id": "3532093"}, {"introduction": "Having learned to quantify the statistical cost of reweighting, we now turn to an advanced technique for actively mitigating it. This practice guides you through the implementation of a control variate, a powerful variance reduction method, within a toy model of NLO event generation. The key idea is to leverage a simpler, correlated process—in this case, the Born-level kinematics—to subtract a significant portion of the variance from a more complex estimator without introducing bias [@problem_id:3532090]. This exercise bridges the gap between statistical theory and practical, performance-enhancing algorithm design in computational physics.", "problem": "Consider a toy model of Next-to-Leading Order (NLO) event generator reweighting with negative weights, designed to probe stability and variance reduction using a control variate based on Born kinematics. You are given a one-dimensional kinematic variable $x$ defined on the interval $[0,1]$, sampled from a uniform density. The baseline Born-level matrix element is modeled as $B(\\theta, x) = \\theta \\, x^{m}$, where $\\theta$ is a real parameter controlling the normalization and $m$ is a real parameter controlling the shape. The NLO event weight is modeled deterministically as $w(x) = A \\, x^{u} - D \\, x^{v}$, where $A, D > 0$ and $u, v \\geq 0$, allowing for negative weights in regions where $D \\, x^{v} > A \\, x^{u}$. The reweighting factor based on Born kinematics is defined as $r_{B}(\\theta; \\theta_{0}, x) = \\dfrac{B(\\theta, x)}{B(\\theta_{0}, x)}$. You will analyze the estimator for the target integral\n$$\nI(\\theta) = \\int_{0}^{1} w(x) \\, r_{B}(\\theta; \\theta_{0}, x) \\, \\mathrm{d}x,\n$$\nand design a control variate using $g(x) = r_{B}(\\theta; \\theta_{0}, x)$ with known expectation under the uniform density on $[0,1]$.\n\nStarting from first principles of Monte Carlo integration and variance analysis, derive the conditions under which the naive reweighted estimator of $I(\\theta)$ has finite variance when $x \\sim \\mathrm{Uniform}(0,1)$ and event weights $w(x)$ can be negative. Then, propose a control variate estimator of the form\n$$\n\\widehat{I}_{\\mathrm{cv}} = \\frac{1}{N} \\sum_{i=1}^{N} \\left( w(x_{i}) \\, r_{B}(\\theta; \\theta_{0}, x_{i}) - \\alpha \\left( g(x_{i}) - \\mu_{g} \\right) \\right),\n$$\nwhere $x_{i} \\sim \\mathrm{Uniform}(0,1)$, $\\mu_{g} = \\mathbb{E}[g(X)]$, and $\\alpha$ is a constant chosen to minimize the variance of the estimator. Prove that the proposed estimator is unbiased for any constant $\\alpha$ when $\\mu_{g}$ is known exactly, and derive the optimal choice of $\\alpha$ that minimizes the variance of $\\widehat{I}_{\\mathrm{cv}}$ in terms of expectations over the uniform density on $[0,1]$. All derivations must be strictly based on fundamental definitions of expectation, variance, covariance, and the control variate method; do not assume or use shortcut formulas.\n\nImplement a program that evaluates, for each test case, the following quantities exactly (by analytical integration over $x \\in [0,1]$):\n- The finiteness of the naive estimator variance, returned as a boolean indicating whether all necessary second moments exist for $\\widehat{I}$ with $f(x) = w(x) \\, r_{B}(\\theta; \\theta_{0}, x)$.\n- The finiteness of the control variate estimator variance when using the optimal $\\alpha$, returned as a boolean.\n- The ratio of the minimized control variate variance to the naive variance, expressed as a float (dimensionless). When the control variate is degenerate (for example, $g(x)$ is constant so its variance is zero), return the ratio as $1.0$.\n- The optimal control variate coefficient $\\alpha$, expressed as a float. If $\\alpha$ is undefined due to a degenerate control variate, return a not-a-number value.\n\nUse the following parameterization, which ensures $r_{B}(\\theta; \\theta_{0}, x)$ is a monomial ratio:\n- Let $c = \\theta / \\theta_{0}$ and $p = m - m_{0}$, so that $g(x) = r_{B}(\\theta; \\theta_{0}, x) = c \\, x^{p}$.\n- Let $w(x) = A \\, x^{u} - D \\, x^{v}$ with $A, D > 0$ and $u, v \\geq 0$.\n\nYour program must analytically compute all expectations and variances by integrating monomials over $[0,1]$. Recall that for the integral $\\int_{0}^{1} x^{a} \\, \\mathrm{d}x$ to exist, it is necessary that $a > -1$. You must encode and apply these existence conditions to determine finiteness of each relevant moment, specifically the first and second moments of $g(x)$, $f(x)$, and the mixed moment required for covariance. All outputs are dimensionless.\n\nTest Suite:\nImplement your program to evaluate the following five test cases, each specified as an ordered tuple $(A, D, u, v, \\theta_{0}, \\theta, m_{0}, m)$:\n1. $(1.2, 2.0, 0, 1, 1.0, 1.3, 0.5, 0.8)$\n2. $(1.2, 2.0, 0, 1, 1.0, 0.9, 0.5, 0.01)$\n3. $(2.0, 1.0, 0, 1, 1.0, 0.9, 0.3, 0.5)$\n4. $(1.0, 3.0, 0, 2, 1.0, 1.4, 0.6, 1.0)$\n5. $(1.2, 2.0, 0, 1, 1.0, 2.5, 0.5, 0.8)$\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case result must itself be a list in the form $[\\text{finite\\_base}, \\text{finite\\_cv}, \\text{variance\\_ratio}, \\alpha_{\\text{opt}}]$, where $\\text{finite\\_base}$ and $\\text{finite\\_cv}$ are booleans, and $\\text{variance\\_ratio}$ and $\\alpha_{\\text{opt}}$ are floats. For example: \"[[True,True,0.42,1.2345],[...],...]\". No additional text should be printed.", "solution": "The problem requires a validation and, if valid, a full derivation and implementation for analyzing a control variate technique in a toy model of event generator reweighting.\n\nThe problem is deemed valid. It is scientifically grounded in the principles of Monte Carlo integration and variance reduction, well-posed with all necessary information provided, and objective in its formulation. It does not violate any of the invalidity criteria. We may therefore proceed with a full solution.\n\nOur objective is to analyze the statistical properties of two estimators for the integral $I(\\theta) = \\int_{0}^{1} w(x) \\, r_{B}(\\theta; \\theta_{0}, x) \\, \\mathrm{d}x$. The variable $x$ is sampled from a uniform distribution on $[0,1]$, so its probability density function is $p(x)=1$. For any function $h(x)$, its expectation is $\\mathbb{E}[h(X)] = \\int_0^1 h(x) \\mathrm{d}x$.\n\nThe quantity to be integrated is $f(x) = w(x) \\, r_{B}(\\theta; \\theta_{0}, x)$. The problem provides the following forms:\n- The event weight is $w(x) = A \\, x^{u} - D \\, x^{v}$.\n- The Born reweighting factor, which also serves as our control variate $g(x)$, is $g(x) = r_{B}(\\theta; \\theta_{0}, x) = \\frac{\\theta x^m}{\\theta_0 x^{m_0}} = (\\theta/\\theta_0) x^{m-m_0}$.\nUsing the specified parameterization, $c = \\theta / \\theta_{0}$ and $p = m - m_{0}$, we have $g(x) = c \\, x^{p}$.\nThus, the function whose expectation we seek is $f(x) = w(x) g(x) = (A \\, x^{u} - D \\, x^{v})(c \\, x^{p}) = A c \\, x^{u+p} - D c \\, x^{v+p}$.\n\nA key tool for our analysis is the analytical integral of a monomial over the unit interval, which corresponds to the expectation $\\mathbb{E}[X^a]$ for $X \\sim \\mathrm{Uniform}(0,1)$:\n$$\n\\mathbb{E}[X^a] = \\int_{0}^{1} x^{a} \\, \\mathrm{d}x = \\left[ \\frac{x^{a+1}}{a+1} \\right]_{0}^{1} = \\frac{1}{a+1}\n$$\nThis integral, and thus the expectation, is finite if and only if the exponent $a > -1$.\n\n**1. Variance of the Naive Estimator**\nThe naive Monte Carlo estimator for $I(\\theta) = \\mathbb{E}[f(X)]$ is given by the sample mean:\n$$\n\\widehat{I} = \\frac{1}{N} \\sum_{i=1}^{N} f(x_i)\n$$\nThe variance of this estimator is $\\mathrm{Var}(\\widehat{I}) = \\frac{1}{N} \\mathrm{Var}(f(X))$. This variance is finite if and only if the second moment of $f(X)$, $\\mathbb{E}[f(X)^2]$, is finite. The second moment is the integral of $f(x)^2$:\n$$\n\\mathbb{E}[f(X)^2] = \\int_{0}^{1} f(x)^2 \\mathrm{d}x = \\int_{0}^{1} (A c \\, x^{u+p} - D c \\, x^{v+p})^2 \\mathrm{d}x\n$$\nExpanding the integrand gives:\n$$\nf(x)^2 = A^2 c^2 x^{2(u+p)} - 2 A D c^2 x^{u+v+2p} + D^2 c^2 x^{2(v+p)}\n$$\nThe integral of this sum is finite if and only if the integral of each monomial term is finite. Applying the condition that the exponent must be greater than $-1$ to each term, we obtain the three conditions for the finiteness of $\\mathrm{Var}(\\widehat{I})$:\n$1$. $2(u+p) > -1$, which is equivalent to $u+p > -1/2$.\n$2$. $u+v+2p > -1$.\n$3$. $2(v+p) > -1$, which is equivalent to $v+p > -1/2$.\n\n**2. The Control Variate Estimator**\nThe proposed control variate estimator is:\n$$\n\\widehat{I}_{\\mathrm{cv}} = \\frac{1}{N} \\sum_{i=1}^{N} \\left( f(x_{i}) - \\alpha \\left( g(x_{i}) - \\mu_{g} \\right) \\right)\n$$\nwhere $f(x) = w(x) g(x)$, $g(x) = c x^p$, and $\\mu_g = \\mathbb{E}[g(X)]$.\n\n**2.1. Unbiasedness of the Estimator**\nWe prove that $\\widehat{I}_{\\mathrm{cv}}$ is an unbiased estimator of $I(\\theta)$ for any constant $\\alpha$, provided the expectations exist. The expectation of the estimator is:\n$$\n\\mathbb{E}[\\widehat{I}_{\\mathrm{cv}}] = \\mathbb{E}\\left[f(X) - \\alpha(g(X) - \\mu_g)\\right]\n$$\nBy linearity of expectation:\n$$\n\\mathbb{E}[\\widehat{I}_{\\mathrm{cv}}] = \\mathbb{E}[f(X)] - \\alpha \\, \\mathbb{E}[g(X) - \\mu_g]\n$$\nBy definition, $\\mathbb{E}[f(X)] = I(\\theta)$. The second term is:\n$$\n\\mathbb{E}[g(X) - \\mu_g] = \\mathbb{E}[g(X)] - \\mathbb{E}[\\mu_g] = \\mu_g - \\mu_g = 0\n$$\nsince $\\mu_g$ is a constant. Therefore,\n$$\n\\mathbb{E}[\\widehat{I}_{\\mathrm{cv}}] = I(\\theta) - \\alpha \\cdot 0 = I(\\theta)\n$$\nThis proves that the estimator is unbiased. This holds as long as $\\mathbb{E}[f(X)]$ and $\\mathbb{E}[g(X)]$ are finite. The conditions for this are $u+p > -1$, $v+p > -1$, and $p > -1$. These conditions are weaker than those required for finite variance.\n\n**2.2. Optimal Control Variate Coefficient $\\alpha$**\nThe variance of the control variate estimator is $\\frac{1}{N} \\mathrm{Var}(f(X) - \\alpha(g(X)-\\mu_g))$. Let $Y = f(X) - \\alpha(g(X)-\\mu_g)$. Since adding a constant does not change variance, $\\mathrm{Var}(Y) = \\mathrm{Var}(f(X) - \\alpha g(X))$. Using the standard formula for the variance of a difference:\n$$\n\\begin{aligned}\n\\mathrm{Var}(Y) &= \\mathrm{Var}(f(X)) + \\mathrm{Var}(\\alpha g(X)) - 2\\mathrm{Cov}(f(X), \\alpha g(X)) \\\\\n&= \\mathrm{Var}(f(X)) + \\alpha^2 \\mathrm{Var}(g(X)) - 2\\alpha \\mathrm{Cov}(f(X), g(X))\n\\end{aligned}\n$$\nThis is a quadratic function of $\\alpha$. To find the value $\\alpha_{\\mathrm{opt}}$ that minimizes this variance, we take the derivative with respect to $\\alpha$ and set it to zero:\n$$\n\\frac{\\mathrm{d}}{\\mathrm{d}\\alpha} \\mathrm{Var}(Y) = 2\\alpha \\mathrm{Var}(g(X)) - 2\\mathrm{Cov}(f(X), g(X)) = 0\n$$\nSolving for $\\alpha$ yields the optimal coefficient, provided that $\\mathrm{Var}(g(X)) \\neq 0$:\n$$\n\\alpha_{\\mathrm{opt}} = \\frac{\\mathrm{Cov}(f(X), g(X))}{\\mathrm{Var}(g(X))}\n$$\nThe covariance is defined as $\\mathrm{Cov}(f, g) = \\mathbb{E}[fg] - \\mathbb{E}[f]\\mathbb{E}[g]$ and variance as $\\mathrm{Var}(g) = \\mathbb{E}[g^2] - (\\mathbb{E}[g])^2$.\n\n**3. Analytical Calculation of Required Quantities**\nTo implement the program, we must calculate the necessary expectations, variances, and covariances analytically.\n\n-   $\\mathbb{E}[g(X)] = \\mathbb{E}[c X^p] = c \\mathbb{E}[X^p] = \\frac{c}{p+1}$, requires $p > -1$.\n-   $\\mathbb{E}[g(X)^2] = \\mathbb{E}[c^2 X^{2p}] = c^2 \\mathbb{E}[X^{2p}] = \\frac{c^2}{2p+1}$, requires $2p > -1$.\n-   $\\mathrm{Var}(g(X)) = \\mathbb{E}[g(X)^2] - (\\mathbb{E}[g(X)])^2$, requires $2p > -1$.\n\n-   $\\mathbb{E}[f(X)] = \\mathbb{E}[AcX^{u+p} - DcX^{v+p}] = \\frac{Ac}{u+p+1} - \\frac{Dc}{v+p+1}$, requires $u+p > -1$ and $v+p > -1$.\n-   $\\mathbb{E}[f(X)^2] = \\mathbb{E}[(AcX^{u+p} - DcX^{v+p})^2] = \\frac{A^2c^2}{2(u+p)+1} - \\frac{2ADc^2}{u+v+2p+1} + \\frac{D^2c^2}{2(v+p)+1}$, requires $2(u+p) > -1$, $u+v+2p > -1$, and $2(v+p) > -1$.\n-   $\\mathrm{Var}(f(X)) = \\mathbb{E}[f(X)^2] - (\\mathbb{E}[f(X)])^2$, requires the same conditions as $\\mathbb{E}[f(X)^2]$.\n\n-   $\\mathbb{E}[f(X)g(X)] = \\mathbb{E}[(AcX^{u+p} - DcX^{v+p})(cX^p)] = \\mathbb{E}[Ac^2X^{u+2p} - Dc^2X^{v+2p}] = \\frac{Ac^2}{u+2p+1} - \\frac{Dc^2}{v+2p+1}$, requires $u+2p > -1$ and $v+2p > -1$.\n-   $\\mathrm{Cov}(f, g) = \\mathbb{E}[fg] - \\mathbb{E}[f]\\mathbb{E}[g]$, requires all component expectations to be finite.\n\n**Finiteness Conditions Summary:**\n-   **Naive Estimator Variance Finiteness ($\\mathtt{finite\\_base}$):** All second moments of $f(X)$ must exist. This requires $2(u+p) > -1$, $u+v+2p > -1$, and $2(v+p) > -1$.\n-   **Control Variate Estimator Variance Finiteness ($\\mathtt{finite\\_cv}$):** The minimized variance is $\\mathrm{Var}(f) - \\mathrm{Cov}(f,g)^2/\\mathrm{Var}(g)$. Its finiteness requires $\\mathrm{Var}(f)$, $\\mathrm{Var}(g)$, and $\\mathrm{Cov}(f,g)$ to be finite. This leads to a combined set of conditions:\n    1.  Conditions for $\\mathrm{Var}(f)$: (same as for `finite_base`).\n    2.  Conditions for $\\mathrm{Var}(g)$: $2p > -1$.\n    3.  Conditions for $\\mathrm{Cov}(f,g)$: $u+2p > -1$ and $v+2p > -1$ (the other moment conditions are implied by Var conditions).\n    `finite_cv` is true if all these conditions hold.\n\n**Degenerate Control Variate:**\nIf $\\mathrm{Var}(g(X))=0$, the control variate provides no variance reduction and $\\alpha_{\\mathrm{opt}}$ is undefined. This occurs if $g(x)$ is a constant, which for $g(x)=cx^p$ means either $c=0$ or $p=0$. In this case, the variance ratio is $1.0$ and $\\alpha_{\\mathrm{opt}}$ is `not-a-number`.\nThe minimized variance is $\\mathrm{Var}_{\\mathrm{min}} = \\mathrm{Var}(f) \\left(1 - \\rho_{fg}^2\\right)$, where $\\rho_{fg} = \\frac{\\mathrm{Cov}(f,g)}{\\sqrt{\\mathrm{Var}(f)\\mathrm{Var}(g)}}$ is the correlation coefficient. The ratio of variances is $\\frac{\\mathrm{Var}_{\\mathrm{min}}}{\\mathrm{Var}(f)} = 1 - \\rho_{fg}^2$.", "answer": "[[true,true,0.01021481977717462,0.9230769230769231],[true,true,0.009403480036665764,-0.03571936302484646],[true,true,0.06173491410712759,0.3076923076923077],[true,true,0.0076974163914833215,2.1555555555555554],[true,true,0.01021481977717462,2.307692307692308]]", "id": "3532090"}]}