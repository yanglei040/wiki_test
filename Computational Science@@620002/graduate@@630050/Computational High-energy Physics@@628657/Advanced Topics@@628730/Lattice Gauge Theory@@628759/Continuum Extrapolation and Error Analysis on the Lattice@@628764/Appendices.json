{"hands_on_practices": [{"introduction": "This practice establishes the foundational workflow for a robust continuum extrapolation. We begin by generating synthetic but realistic time-series data that includes autocorrelation, a key feature of Monte Carlo simulations. You will then learn to handle these correlations by estimating the integrated autocorrelation time, $\\tau_{\\mathrm{int}}$, applying a blocking procedure, and finally using a blocked bootstrap to correctly propagate statistical uncertainties into the continuum-limit estimate. This end-to-end exercise [@problem_id:3509811] provides the essential toolkit for analyzing raw simulation data.", "problem": "You are given the task of implementing a statistically sound continuum extrapolation for a lattice observable measured at multiple lattice spacings, accounting for autocorrelations in the time-series data. The program you write must estimate the integrated autocorrelation time, choose a blocking size that controls residual correlations, and propagate statistical errors into the continuum limit estimate via a blocked bootstrap procedure.\n\nStart from the following fundamental base:\n- For a stationary time series with mean $m$, variance $\\sigma^{2}$, and normalized autocorrelation function $\\rho(t)$ at integer lag $t$, the variance of the sample mean of $N$ correlated observations is increased by the integrated autocorrelation time $\\tau_{\\mathrm{int}}$ defined as\n$$\n\\tau_{\\mathrm{int}}=\\frac{1}{2}+\\sum_{t=1}^{\\infty}\\rho(t),\n$$\nand a practical estimator replaces $\\infty$ by a finite window $W$ selected by a self-consistency rule.\n- Under Symanzik effective theory assumptions for a suitably improved lattice action and operator, leading cutoff effects for many observables scale as $\\mathcal{O}(a^{2})$, so one may model the $a$-dependence as\n$$\n\\mathcal{O}(a)=\\mathcal{O}_{0}+c_{1}\\,a^{2},\n$$\nand determine the continuum limit $\\mathcal{O}_{0}$ by a linear fit in $a^{2}$.\n\nImplement the following, with all quantities dimensionless (no physical units required):\n\n1) Time-series generation. For each lattice spacing $a$ in a test case, generate a deterministic autoregressive process of order one (AR(1)) with mean given by the Symanzik model and uniformly distributed innovations from a fixed linear congruential generator (LCG). Specifically, for a given $a$, define the target mean\n$$\n\\mu(a)=\\mathcal{O}_{0}^{\\mathrm{true}}+c_{1}^{\\mathrm{true}}\\,a^{2},\n$$\nand let the AR(1) recursion for the series $\\{X_{t}^{(a)}\\}_{t=1}^{N}$ be\n$$\nX_{t+1}^{(a)}=\\mu(a)+\\phi(a)\\left(X_{t}^{(a)}-\\mu(a)\\right)+\\epsilon_{t}^{(a)},\n$$\nwith $\\phi(a)=1-\\frac{1}{L_{\\mathrm{corr}}(a)}$, and innovations $\\epsilon_{t}^{(a)}=\\sigma(a)\\left(2U_{t}^{(a)}-1\\right)$ where $\\{U_{t}^{(a)}\\}$ is a deterministic pseudo-random sequence from the LCG\n$$\ns_{t+1}=(\\alpha s_{t}+\\gamma)\\bmod M,\\quad U_{t}=\\frac{s_{t}}{M},\n$$\nwith $M=2^{32}$, $\\alpha=1664525$, $\\gamma=1013904223$. Use an $a$-specific integer seed $s_{0}$ as provided for each series. For each time series, generate $N_{\\mathrm{burn}}=50+10\\,L_{\\mathrm{corr}}(a)$ burn-in steps that are discarded, then keep the next $N$ points as the dataset for that $a$.\n\n2) Integrated autocorrelation time estimation. For each generated time series, estimate the integrated autocorrelation time $\\tau_{\\mathrm{int}}$ using the standard windowed estimator with self-consistent window selection. Let $\\bar{X}$ be the sample mean and $C(t)$ the unbiased sample autocovariance at lag $t$. Define the normalized autocorrelation $\\rho(t)=C(t)/C(0)$. For a chosen window size $W$, define\n$$\n\\hat{\\tau}_{\\mathrm{int}}(W)=\\frac{1}{2}+\\sum_{t=1}^{W}\\rho(t).\n$$\nChoose $W$ to be the smallest lag such that $W\\geq c\\,\\hat{\\tau}_{\\mathrm{int}}(W)$ with $c=6$, subject to $W\\leq W_{\\max}=\\min\\{N-1,1000\\}$. If no such $W$ exists up to $W_{\\max}$, set $W=W_{\\max}$. Report the estimate $\\hat{\\tau}_{\\mathrm{int}}=\\max\\{\\frac{1}{2},\\hat{\\tau}_{\\mathrm{int}}(W)\\}$.\n\n3) Blocking and blocked bootstrap. For each time series:\n- Choose an integer block size $B_{\\mathrm{blk}}=\\max\\left\\{1,\\left\\lceil 2\\,\\hat{\\tau}_{\\mathrm{int}}\\right\\rceil\\right\\}$.\n- Partition the series into $M=\\left\\lfloor N/B_{\\mathrm{blk}}\\right\\rfloor$ non-overlapping blocks of length $B_{\\mathrm{blk}}$ and compute the $M$ block means. If $M=0$, use a single block equal to the overall sample mean.\n- Implement a block bootstrap with $R$ bootstrap replicates. In each replicate, resample $M$ blocks with replacement and take the average of the resampled block means to obtain one bootstrap estimate of the mean at each lattice spacing. Use the same fixed LCG as above to generate the bootstrap resampling indices, with the replicate and lattice spacing sequences continuing from a specified bootstrap seed.\n\n4) Continuum extrapolation. For each bootstrap replicate, perform an unweighted linear least-squares fit of the bootstrap means $\\{Y^{(a)}\\}$ at the given $a$ values to the model\n$$\nY^{(a)}=\\mathcal{O}_{0}+c_{1}\\,a^{2},\n$$\nand record the bootstrap value of $\\mathcal{O}_{0}$. Use the sample mean of the bootstrap distribution of $\\mathcal{O}_{0}$ as the point estimate and the sample standard deviation (with denominator $R-1$) as the standard error.\n\nNumerical requirements and test suite:\n- You must implement the above workflow for the following three test cases. All numbers below are exact and must be used as given.\n\nTest case A:\n- Lattice spacings: $a\\in\\{0.12,0.09,0.06\\}$.\n- Series length: $N=512$.\n- Correlation lengths: $L_{\\mathrm{corr}}(a)\\in\\{12,24,48\\}$ corresponding respectively to $a=0.12,0.09,0.06$.\n- True parameters: $\\mathcal{O}_{0}^{\\mathrm{true}}=1.2345$, $c_{1}^{\\mathrm{true}}=2.75$.\n- Innovation amplitudes: $\\sigma(a)\\equiv 0.25$ for all $a$.\n- Innovation seeds: $s_{0}\\in\\{123456789,987654321,42424242\\}$ corresponding respectively to $a=0.12,0.09,0.06$.\n- Bootstrap replicates: $R=499$.\n- Bootstrap seed: $s_{0}^{\\mathrm{boot}}=20250601$.\n\nTest case B:\n- Lattice spacings: $a\\in\\{0.10,0.07,0.05,0.04\\}$.\n- Series length: $N=384$.\n- Correlation lengths: $L_{\\mathrm{corr}}(a)\\in\\{8,16,24,32\\}$ corresponding respectively to $a=0.10,0.07,0.05,0.04$.\n- True parameters: $\\mathcal{O}_{0}^{\\mathrm{true}}=0.5$, $c_{1}^{\\mathrm{true}}=-1.2$.\n- Innovation amplitudes: $\\sigma(a)\\equiv 0.20$ for all $a$.\n- Innovation seeds: $s_{0}\\in\\{13579,24680,112233,998877\\}$ corresponding respectively to $a=0.10,0.07,0.05,0.04$.\n- Bootstrap replicates: $R=399$.\n- Bootstrap seed: $s_{0}^{\\mathrm{boot}}=1357911$.\n\nTest case C:\n- Lattice spacings: $a\\in\\{0.16,0.08,0.04\\}$.\n- Series length: $N=256$.\n- Correlation lengths: $L_{\\mathrm{corr}}(a)\\in\\{2,4,6\\}$ corresponding respectively to $a=0.16,0.08,0.04$.\n- True parameters: $\\mathcal{O}_{0}^{\\mathrm{true}}=-0.2$, $c_{1}^{\\mathrm{true}}=0.8$.\n- Innovation amplitudes: $\\sigma(a)\\equiv 0.15$ for all $a$.\n- Innovation seeds: $s_{0}\\in\\{314159,271828,161803\\}$ corresponding respectively to $a=0.16,0.08,0.04$.\n- Bootstrap replicates: $R=299$.\n- Bootstrap seed: $s_{0}^{\\mathrm{boot}}=424242$.\n\nImplementation prescriptions and output specification:\n- For autocovariance estimation, use the unbiased estimator $C(t)=\\frac{1}{N-t}\\sum_{i=1}^{N-t}(X_{i}-\\bar{X})(X_{i+t}-\\bar{X})$ up to $t=W_{\\max}$, and $\\rho(t)=C(t)/C(0)$ when $C(0)>0$. If $C(0)\\leq 0$, set $\\hat{\\tau}_{\\mathrm{int}}=\\frac{1}{2}$.\n- For the self-consistent window selection, use $c=6$ and $W_{\\max}=\\min\\{N-1,1000\\}$.\n- For blocking, use $B_{\\mathrm{blk}}=\\max\\left\\{1,\\left\\lceil 2\\,\\hat{\\tau}_{\\mathrm{int}}\\right\\rceil\\right\\}$ and $M=\\left\\lfloor N/B_{\\mathrm{blk}}\\right\\rfloor$. If $M=0$, use a single block whose mean is the sample mean.\n- For block bootstrap resampling, for each replicate draw $M$ independent indices uniformly from $\\{0,1,\\dots,M-1\\}$ using the specified LCG with state initialized to the given bootstrap seed and advanced sequentially across all draws and lattice spacings within that test case.\n- For the linear fit, perform an ordinary least-squares fit of $Y$ versus $a^{2}$ to estimate $\\mathcal{O}_{0}$ and $c_{1}$ for each bootstrap replicate.\n\nFinal output format:\n- For each test case, report a list consisting of:\n  1) the three or four values of $\\hat{\\tau}_{\\mathrm{int}}$ in the order of the given $a$'s, each rounded to $6$ decimal places,\n  2) the corresponding three or four integer block sizes $B_{\\mathrm{blk}}$,\n  3) the bootstrap mean of $\\mathcal{O}_{0}$ across replicates, rounded to $6$ decimal places,\n  4) the bootstrap standard deviation of $\\mathcal{O}_{0}$ across replicates, rounded to $6$ decimal places.\n- Your program should produce a single line of output containing the results for Test case A, Test case B, and Test case C, in that order, as a comma-separated list enclosed in square brackets, where each element is the per-test list described above. For example, the output structure must be of the form\n$$\n[\\,[\\hat{\\tau}_{\\mathrm{int}}(a_{1}),\\dots,\\hat{\\tau}_{\\mathrm{int}}(a_{K}),B_{\\mathrm{blk}}(a_{1}),\\dots,B_{\\mathrm{blk}}(a_{K}),\\widehat{\\mathcal{O}}_{0},\\mathrm{SE}(\\widehat{\\mathcal{O}}_{0})],\\;[\\dots],\\;[\\dots]\\,].\n$$\nAll rounding operations must be applied before printing, and each numerical entry should be a standard decimal numeral.", "solution": "The user-provided problem is a comprehensive and well-posed exercise in computational physics, specifically focusing on the statistical analysis of lattice simulation data. The task is to perform a continuum extrapolation of a lattice observable, which involves simulating correlated data, analyzing its autocorrelation properties, and using a blocked bootstrap method to propagate statistical uncertainties into the final continuum-limit estimate. The problem is scientifically grounded in the principles of statistical mechanics, time-series analysis, and Symanzik effective theory for lattice discretizations. All parameters, algorithms, and procedures are specified with sufficient precision to permit a unique, deterministic solution. Therefore, the problem is deemed valid and a full solution will be provided.\n\nThe solution is implemented by following a sequence of prescribed steps:\n\n1.  **Time-Series Generation**: For each given lattice spacing $a$, a synthetic time series of an observable is generated. This series is modeled as a first-order autoregressive (AR(1)) process. The process is constructed to fluctuate around a mean value $\\mu(a)$, which itself depends on the lattice spacing according to the Symanzik effective theory model, $\\mu(a)=\\mathcal{O}_{0}^{\\mathrm{true}}+c_{1}^{\\mathrm{true}}\\,a^{2}$. The AR(1) recursion is given by $X_{t+1}^{(a)}=\\mu(a)+\\phi(a)\\left(X_{t}^{(a)}-\\mu(a)\\right)+\\epsilon_{t}^{(a)}$. The parameter $\\phi(a)=1-1/L_{\\mathrm{corr}}(a)$ controls the autocorrelation time of the series, with $L_{\\mathrm{corr}}(a)$ being the given correlation length. The innovations $\\epsilon_{t}^{(a)}$ are random noise terms generated from a deterministic Linear Congruential Generator (LCG) with specified parameters ($M=2^{32}$, $\\alpha=1664525$, $\\gamma=1013904223$) and seeds. An initial portion of each series, the burn-in period $N_{\\mathrm{burn}}=50+10\\,L_{\\mathrm{corr}}(a)$, is discarded to ensure the generated data represents the stationary state of the process.\n\n2.  **Integrated Autocorrelation Time Estimation**: The key to correct error analysis for correlated data is the integrated autocorrelation time, $\\tau_{\\mathrm{int}}$. For each generated time series, we estimate $\\tau_{\\mathrm{int}}$ using a windowed estimator. First, the sample autocovariance function $C(t)$ is computed for lags $t$ from $0$ up to a maximum $W_{\\max}$. Following the problem's prescription, we use the formula $C(t)=\\frac{1}{N-t}\\sum_{i=1}^{N-t}(X_{i}-\\bar{X})(X_{i+t}-\\bar{X})$, where $\\bar{X}$ is the mean of the entire time series of length $N$. The normalized autocorrelation function is then $\\rho(t)=C(t)/C(0)$. The estimated integrated autocorrelation time for a given window size $W$ is $\\hat{\\tau}_{\\mathrm{int}}(W)=\\frac{1}{2}+\\sum_{t=1}^{W}\\rho(t)$. The window size $W$ is chosen via a self-consistency procedure: it is the smallest integer lag $W$ that satisfies the condition $W\\geq c\\,\\hat{\\tau}_{\\mathrm{int}}(W)$, with the specified constant $c=6$. This procedure truncates the sum when the noise in $\\rho(t)$ begins to dominate the signal. If this condition is not met for any $W \\leq W_{\\max}$, then $W=W_{\\max}$ is used. The final reported estimate is $\\hat{\\tau}_{\\mathrm{int}}=\\max\\{\\frac{1}{2},\\hat{\\tau}_{\\mathrm{int}}(W)\\}$.\n\n3.  **Blocking and Blocked Bootstrap**: To handle the autocorrelations and propagate statistical errors, a blocked bootstrap method is employed. First, each time series is partitioned into non-overlapping blocks. The size of these blocks, $B_{\\mathrm{blk}}=\\max\\left\\{1,\\left\\lceil 2\\,\\hat{\\tau}_{\\mathrm{int}}\\right\\rceil\\right\\}$, is chosen to be larger than the autocorrelation time, such that the means of the blocks are approximately statistically independent. The series is divided into $M=\\left\\lfloor N/B_{\\mathrm{blk}}\\right\\rfloor$ blocks, and the mean of each block is computed. This results in a new, much shorter time series of $M$ block means for each lattice spacing $a$.\n    A bootstrap analysis is then performed. For each of $R$ bootstrap replicates, a new set of data is created. For each lattice spacing $a$, $M$ block means are resampled with replacement from the original set of $M$ block means. The average of these resampled block means gives one bootstrap estimate, $Y^{(a)}$, for the observable at that lattice spacing. This process is repeated for all lattice spacings, using a single LCG instance that advances sequentially, to generate a full set of $\\{Y^{(a)}\\}$ for one bootstrap replicate.\n\n4.  **Continuum Extrapolation**: For each of the $R$ bootstrap replicates, the continuum limit $\\mathcal{O}_{0}$ is estimated. This is done by performing an unweighted linear least-squares fit to the model $Y^{(a)}=\\mathcal{O}_{0}+c_{1}\\,a^{2}$, using the bootstrap means $Y^{(a)}$ versus the corresponding values of $a^2$. The intercept of this fit provides one bootstrap estimate of $\\mathcal{O}_{0}$.\n\n5.  **Final Estimates**: After generating $R$ bootstrap estimates for $\\mathcal{O}_{0}$, a bootstrap distribution is obtained. The final point estimate for the continuum value, $\\widehat{\\mathcal{O}}_{0}$, is the sample mean of this distribution. The corresponding statistical uncertainty, $\\mathrm{SE}(\\widehat{\\mathcal{O}}_{0})$, is given by the sample standard deviation of the distribution, using a denominator of $R-1$.\n\nThe entire procedure is deterministic due to the use of specified seeds for the LCG. The final results for each test case are collected, rounded to six decimal places, and formatted into the specified nested list structure.", "answer": "```python\nimport numpy as np\nimport math\n\nclass LCG:\n    \"\"\"A Linear Congruential Generator as specified in the problem.\"\"\"\n    def __init__(self, seed, M=2**32, alpha=1664525, gamma=1013904223):\n        self.state = seed\n        self.M = M\n        self.alpha = alpha\n        self.gamma = gamma\n\n    def next_int(self):\n        \"\"\"Generates the next integer in the sequence.\"\"\"\n        self.state = (self.alpha * self.state + self.gamma) % self.M\n        return self.state\n\n    def next_float(self):\n        \"\"\"Generates the next float in [0, 1) in the sequence.\"\"\"\n        return self.next_int() / self.M\n\ndef generate_ar1_series(a, N, L_corr, O0_true, c1_true, sigma, seed):\n    \"\"\"Generates a single AR(1) time series.\"\"\"\n    rng = LCG(seed)\n    mu_a = O0_true + c1_true * a**2\n    phi_a = 1.0 - 1.0 / L_corr\n    N_burn = 50 + 10 * L_corr\n\n    # Initial value at the mean\n    x_t = mu_a\n\n    # Burn-in phase\n    for _ in range(N_burn):\n        u_t = rng.next_float()\n        epsilon_t = sigma * (2 * u_t - 1)\n        x_t = mu_a + phi_a * (x_t - mu_a) + epsilon_t\n\n    # Data generation phase\n    series = np.zeros(N)\n    for t in range(N):\n        u_t = rng.next_float()\n        epsilon_t = sigma * (2 * u_t - 1)\n        x_t = mu_a + phi_a * (x_t - mu_a) + epsilon_t\n        series[t] = x_t\n        \n    return series\n\ndef estimate_tau_int(series, c, W_max):\n    \"\"\"Estimates the integrated autocorrelation time with a self-consistent window.\"\"\"\n    N = len(series)\n    if N < 2:\n        return 0.5\n        \n    mean = np.mean(series)\n    series_centered = series - mean\n\n    # Calculate C(0)\n    C0 = np.dot(series_centered, series_centered) / N\n    if C0 <= 0:\n        return 0.5\n\n    tau_int_sum = 0.0\n    final_W = W_max\n    \n    for W in range(1, W_max + 1):\n        if W >= N:\n            final_W = W - 1\n            break\n        \n        # Calculate C(W)\n        CW = np.dot(series_centered[:-W], series_centered[W:]) / (N - W)\n        rho_W = CW / C0\n        tau_int_sum += rho_W\n        \n        tau_int_W = 0.5 + tau_int_sum\n        \n        if W >= c * tau_int_W:\n            final_W = W\n            break\n    \n    # Recalculate tau_int up to final_W if the loop finished\n    if W == W_max:\n        tau_int_W_final = 0.5 + tau_int_sum\n    else: # if loop broke early\n        tau_int_W_final = tau_int_W\n\n    return max(0.5, tau_int_W_final)\n\ndef solve_case(case_params):\n    \"\"\"\n    Processes a single test case from data generation to final result.\n    \"\"\"\n    a_s = np.array(case_params['a'])\n    N = case_params['N']\n    L_corrs = case_params['L_corr']\n    O0_true = case_params['O0_true']\n    c1_true = case_params['c1_true']\n    sigmas = case_params['sigma']\n    seeds = case_params['seeds']\n    R = case_params['R']\n    boot_seed = case_params['boot_seed']\n    \n    num_a = len(a_s)\n    \n    all_series = []\n    # Part 1: Generate time series for all lattice spacings\n    for i in range(num_a):\n        series = generate_ar1_series(a_s[i], N, L_corrs[i], O0_true, c1_true, sigmas[i], seeds[i])\n        all_series.append(series)\n\n    tau_ints = []\n    block_sizes = []\n    blocked_data = []\n\n    # Part 2 & 3: Estimate tau_int and create block means\n    for i in range(num_a):\n        series = all_series[i]\n        W_max = min(N - 1, 1000)\n        \n        tau = estimate_tau_int(series, c=6, W_max=W_max)\n        tau_ints.append(tau)\n        \n        B_blk = max(1, math.ceil(2 * tau))\n        block_sizes.append(B_blk)\n        \n        M = N // B_blk\n        if M == 0:\n            block_means = np.array([np.mean(series)])\n        else:\n            num_pts_to_use = M * B_blk\n            block_means = np.mean(series[:num_pts_to_use].reshape(M, B_blk), axis=1)\n        blocked_data.append(block_means)\n\n    # Part 3 & 4: Blocked bootstrap and continuum extrapolation\n    boot_rng = LCG(boot_seed)\n    O0_bootstrap_dist = []\n    a_squared = a_s**2\n\n    for _ in range(R):\n        boot_means_Y = []\n        for i in range(num_a):\n            blocks = blocked_data[i]\n            M_i = len(blocks)\n            indices = [int(boot_rng.next_float() * M_i) for _ in range(M_i)]\n            resampled_blocks = blocks[indices]\n            boot_mean = np.mean(resampled_blocks)\n            boot_means_Y.append(boot_mean)\n        \n        # OLS fit to Y = O0 + c1 * a^2\n        Y = np.array(boot_means_Y)\n        X = a_squared\n        \n        X_bar = np.mean(X)\n        Y_bar = np.mean(Y)\n        \n        # Handle case of vertical line (all a are same, not in this problem)\n        if np.sum((X - X_bar)**2) == 0:\n             c1_hat = 0\n        else:\n            c1_hat = np.sum((X - X_bar) * (Y - Y_bar)) / np.sum((X - X_bar)**2)\n        \n        O0_hat = Y_bar - c1_hat * X_bar\n        O0_bootstrap_dist.append(O0_hat)\n    \n    O0_bootstrap_dist = np.array(O0_bootstrap_dist)\n    final_O0_mean = np.mean(O0_bootstrap_dist)\n    final_O0_std_err = np.std(O0_bootstrap_dist, ddof=1)\n\n    # Assemble results\n    results = []\n    for tau in tau_ints:\n        results.append(f\"{tau:.6f}\")\n    for bs in block_sizes:\n        results.append(str(bs))\n    results.append(f\"{final_O0_mean:.6f}\")\n    results.append(f\"{final_O0_std_err:.6f}\")\n    \n    return results\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final output.\n    \"\"\"\n    test_cases_params = [\n        {\n            'a': [0.12, 0.09, 0.06], 'N': 512, 'L_corr': [12, 24, 48],\n            'O0_true': 1.2345, 'c1_true': 2.75, 'sigma': [0.25, 0.25, 0.25],\n            'seeds': [123456789, 987654321, 42424242], 'R': 499, 'boot_seed': 20250601\n        },\n        {\n            'a': [0.10, 0.07, 0.05, 0.04], 'N': 384, 'L_corr': [8, 16, 24, 32],\n            'O0_true': 0.5, 'c1_true': -1.2, 'sigma': [0.20, 0.20, 0.20, 0.20],\n            'seeds': [13579, 24680, 112233, 998877], 'R': 399, 'boot_seed': 1357911\n        },\n        {\n            'a': [0.16, 0.08, 0.04], 'N': 256, 'L_corr': [2, 4, 6],\n            'O0_true': -0.2, 'c1_true': 0.8, 'sigma': [0.15, 0.15, 0.15],\n            'seeds': [314159, 271828, 161803], 'R': 299, 'boot_seed': 424242\n        }\n    ]\n    \n    all_results = []\n    for params in test_cases_params:\n        case_result = solve_case(params)\n        all_results.append(f\"[{','.join(case_result)}]\")\n\n    print(f\"[{','.join(all_results)}]\")\n\nsolve()\n```", "id": "3509811"}, {"introduction": "Building on the basic extrapolation, we now address the systematic uncertainty arising from the choice of the fitting function itself. This practice introduces Bayesian Model Averaging (BMA), a powerful method for combining results from multiple competing theoretical models, such as those with leading corrections proportional to $a$, $a^2$, or even $a^2 \\ln a$. By implementing BMA [@problem_id:3509855], you will learn to compute a model-averaged continuum value and a credible interval that rigorously incorporates this model uncertainty, moving beyond reliance on a single, arbitrarily chosen ansatz.", "problem": "Implement a complete program that performs Bayesian model averaging (BMA) for continuum extrapolation of lattice data using three competing ansätze and returns the model-averaged continuum value and its equal-tailed credible interval. All quantities in this problem are dimensionless. The three models are defined by basis functions of the lattice spacing $a$ as follows:\n- $M_1$: constant plus linear, with $f(a) = f_0 + c_1\\,a$,\n- $M_2$: constant plus quadratic, with $f(a) = f_0 + c_2\\,a^2$,\n- $M_3$: constant plus quadratic times logarithm, with $f(a) = f_0 + c_3\\,a^2 \\log a$, where the logarithm is the natural logarithm and $a \\in (0,1)$ so that $\\log a$ is well-defined and negative.\n\nYou must treat the problem in a fully Bayesian linear model with Gaussian observational noise and independent Gaussian priors on the coefficients. Specifically:\n- Likelihood: given data $(a_i, y_i, \\sigma_i)$ for $i=1,\\dots,n$, model $M$ predicts $y_i$ through a linear design matrix in the coefficients, and the data likelihood is a product of Gaussians with known standard deviations $\\sigma_i$.\n- Parameter prior for each model $M_j$: the coefficient vector $\\theta = (f_0, c_j)$ is assigned a zero-mean Gaussian prior with diagonal covariance $\\mathrm{diag}(\\tau_0^2, \\tau_c^2)$, where $\\tau_0 = 10$ and $\\tau_c = 1$.\n- Model prior: models $M_1$, $M_2$, and $M_3$ have equal prior probability.\n\nYour program must:\n1. For each model $M_j$, compute the model evidence by integrating the parameters under the Gaussian prior and linear-Gaussian likelihood.\n2. For each model $M_j$, compute the posterior distribution of the continuum parameter $f_0$ (i.e., the coefficient of the constant basis function) given the data under that model.\n3. Compute the posterior probability of each model.\n4. Construct the model-averaged posterior for $f_0$ as a discrete mixture of the three model-specific Gaussian posteriors, with weights equal to the posterior model probabilities.\n5. Compute the model-averaged posterior mean of $f_0$ and its central equal-tailed credible interval with credibility level $0.68$ (i.e., lower quantile at $0.16$ and upper quantile at $0.84$) from the mixture distribution.\n\nFundamental base you must use:\n- Bayes’ theorem for model selection and parameter inference,\n- Properties of multivariate normal distributions for linear-Gaussian models with Gaussian priors,\n- Definitions of central equal-tailed credible intervals.\n\nConstraints and requirements:\n- Treat all inputs as dimensionless numbers. No angles appear in this problem; no unit conversion is needed.\n- Implement numerically stable linear algebra. You may assume the observational error covariance is diagonal with entries $\\sigma_i^2$.\n- Use the natural logarithm for $\\log a$.\n- The credible interval level is to be interpreted as a probability (e.g., $0.68$), not a percentage. Report the interval endpoints, not the half-width.\n\nTest suite:\nYour program must run on the following five datasets. Each dataset provides arrays of lattice spacings $a$, observed values $y$, and observational standard deviations $\\sigma$. Use exactly these values in the stated order.\n\n- Case 1 (typical quadratic cutoff effects):\n  - $a = [\\,0.12,\\, 0.09,\\, 0.06,\\, 0.045\\,]$\n  - $y = [\\,0.52028,\\, 0.50772,\\, 0.50582,\\, 0.50143\\,]$\n  - $\\sigma = [\\,0.005,\\, 0.004,\\, 0.003,\\, 0.003\\,]$\n\n- Case 2 (dominantly linear cutoff effects):\n  - $a = [\\,0.16,\\, 0.12,\\, 0.08,\\, 0.04\\,]$\n  - $y = [\\,0.877,\\, 0.902,\\, 0.939,\\, 0.964\\,]$\n  - $\\sigma = [\\,0.01,\\, 0.01,\\, 0.01,\\, 0.01\\,]$\n\n- Case 3 (log-enhanced quadratic corrections):\n  - $a = [\\,0.10,\\, 0.08,\\, 0.06,\\, 0.05,\\, 0.04\\,]$\n  - $y = [\\,0.189987074535,\\, 0.1909176683392,\\, 0.1954358607098302,\\, 0.1957553346575,\\, 0.19842489934\\,]$\n  - $\\sigma = [\\,0.004,\\, 0.004,\\, 0.004,\\, 0.004,\\, 0.004\\,]$\n\n- Case 4 (sparse data):\n  - $a = [\\,0.10,\\, 0.05\\,]$\n  - $y = [\\,0.311,\\, 0.29875\\,]$\n  - $\\sigma = [\\,0.02,\\, 0.02\\,]$\n\n- Case 5 (high-noise regime):\n  - $a = [\\,0.15,\\, 0.12,\\, 0.09,\\, 0.06,\\, 0.03\\,]$\n  - $y = [\\,0.845,\\, 0.78,\\, 0.805,\\, 0.765,\\, 0.77\\,]$\n  - $\\sigma = [\\,0.05,\\, 0.05,\\, 0.05,\\, 0.05,\\, 0.05\\,]$\n\nOutput specification:\n- For each case, compute the model-averaged posterior mean of $f_0$ and the endpoints of the central equal-tailed credible interval at credibility level $0.68$.\n- Round each reported number to $6$ decimal places.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each case contributes a sub-list in the form $[\\,\\text{mean},\\, \\text{lower},\\, \\text{upper}\\,]$. For example, the overall format must be like $[[m_1,\\ell_1,u_1],[m_2,\\ell_2,u_2],\\dots,[m_5,\\ell_5,u_5]]$ with exactly five triples in order corresponding to the cases above.", "solution": "The user requires the implementation of a Bayesian Model Averaging (BMA) procedure for continuum extrapolation of lattice data. The problem is well-posed, scientifically grounded, and contains all necessary information for its resolution. It falls directly within the domain of computational physics and Bayesian statistics.\n\nWe will proceed by first deriving the necessary mathematical formulas for the Bayesian linear model and then implementing them to solve the problem for the provided test cases.\n\n### Bayesian Linear Regression Formalism\n\nLet a given model $M_j$ be described by the linear relation $\\mathbf{y} = X_j \\boldsymbol{\\theta}_j + \\boldsymbol{\\epsilon}$, where:\n- $\\mathbf{y}$ is the $n \\times 1$ vector of observed values.\n- $X_j$ is the $n \\times k$ design matrix, with $k=2$ parameters for all models.\n- $\\boldsymbol{\\theta}_j$ is the $k \\times 1$ vector of model parameters, $(f_0, c_j)^T$.\n- $\\boldsymbol{\\epsilon}$ is the vector of observation errors, assumed to be drawn from a Gaussian distribution with mean $\\mathbf{0}$ and known diagonal covariance matrix $\\Sigma = \\mathrm{diag}(\\sigma_1^2, \\dots, \\sigma_n^2)$.\n\nThe likelihood of the data given the parameters is:\n$$p(\\mathbf{y} | \\boldsymbol{\\theta}_j, M_j) = \\mathcal{N}(\\mathbf{y} | X_j \\boldsymbol{\\theta}_j, \\Sigma) \\propto \\exp\\left(-\\frac{1}{2}(\\mathbf{y} - X_j \\boldsymbol{\\theta}_j)^T \\Sigma^{-1} (\\mathbf{y} - X_j \\boldsymbol{\\theta}_j)\\right)$$\n\nThe problem specifies a Gaussian prior on the parameters $\\boldsymbol{\\theta}_j$:\n$$p(\\boldsymbol{\\theta}_j | M_j) = \\mathcal{N}(\\boldsymbol{\\theta}_j | \\boldsymbol{\\mu}_0, \\Sigma_0)$$\nwhere the prior mean is $\\boldsymbol{\\mu}_0 = \\mathbf{0}$ and the prior covariance is $\\Sigma_0 = \\mathrm{diag}(\\tau_0^2, \\tau_c^2)$ with $\\tau_0 = 10$ and $\\tau_c = 1$.\n\n#### 1. Parameter Posterior Distribution\nDue to the conjugacy of the Gaussian prior with the linear-Gaussian likelihood, the posterior distribution of the parameters is also a Gaussian:\n$$p(\\boldsymbol{\\theta}_j | \\mathbf{y}, M_j) = \\mathcal{N}(\\boldsymbol{\\theta}_j | \\boldsymbol{\\mu}_{n,j}, \\Sigma_{n,j})$$\nThe posterior covariance $\\Sigma_{n,j}$ and mean $\\boldsymbol{\\mu}_{n,j}$ are given by:\n$$\n\\Sigma_{n,j} = (\\Sigma_0^{-1} + X_j^T \\Sigma^{-1} X_j)^{-1} \\\\\n\\boldsymbol{\\mu}_{n,j} = \\Sigma_{n,j} (X_j^T \\Sigma^{-1} \\mathbf{y})\n$$\nSince $\\boldsymbol{\\mu}_0=\\mathbf{0}$, the term involving the prior mean vanishes. Let $W = \\Sigma^{-1}$. The equations become:\n$$\nA_j = \\Sigma_0^{-1} + X_j^T W X_j \\\\\n\\Sigma_{n,j} = A_j^{-1} \\\\\n\\boldsymbol{\\mu}_{n,j} = \\Sigma_{n,j} (X_j^T W \\mathbf{y})\n$$\nThe parameter of interest is $f_0$, which is the first component of $\\boldsymbol{\\theta}_j$. Its marginal posterior distribution under model $M_j$ is a univariate Gaussian:\n$$p(f_0 | \\mathbf{y}, M_j) = \\mathcal{N}(f_0 | \\mu_{f_0, j}, \\sigma_{f_0, j}^2)$$\nwhere $\\mu_{f_0, j} = (\\boldsymbol{\\mu}_{n,j})_1$ and $\\sigma_{f_0, j}^2 = (\\Sigma_{n,j})_{11}$.\n\n#### 2. Model Evidence (Marginal Likelihood)\nThe evidence for model $M_j$ is the probability of the data integrated over all possible parameter values:\n$$p(\\mathbf{y} | M_j) = \\int p(\\mathbf{y} | \\boldsymbol{\\theta}_j, M_j) p(\\boldsymbol{\\theta}_j | M_j) d\\boldsymbol{\\theta}_j$$\nFor the linear-Gaussian case, this integral can be evaluated analytically. The result is:\n$$p(\\mathbf{y} | M_j) = \\mathcal{N}(\\mathbf{y} | X_j\\boldsymbol{\\mu}_0, \\Sigma + X_j\\Sigma_0 X_j^T)$$\nTo compute this stably, we use the logarithm of the evidence. A convenient and stable expression is:\n$$\n\\log p(\\mathbf{y}|M_j) = -\\frac{n}{2} \\log (2\\pi) -\\frac{1}{2}\\left( \\log\\det\\Sigma + \\log\\det\\Sigma_0 + \\log\\det A_j \\right) -\\frac{1}{2}\\left( \\mathbf{y}^T W \\mathbf{y} - \\boldsymbol{\\mu}_{n,j}^T A_j \\boldsymbol{\\mu}_{n,j} \\right)\n$$\nwhere $\\boldsymbol{\\mu}_{n,j}^T A_j \\boldsymbol{\\mu}_{n,j} = (X_j^T W \\mathbf{y})^T \\Sigma_{n,j} (X_j^T W \\mathbf{y})$.\n\n#### 3. Posterior Model Probability\nUsing Bayes' theorem for models, the posterior probability of model $M_j$ is:\n$$P(M_j | \\mathbf{y}) = \\frac{p(\\mathbf{y} | M_j) P(M_j)}{\\sum_{k=1}^3 p(\\mathbf{y} | M_k) P(M_k)}$$\nSince the prior probabilities $P(M_j)$ are equal ($1/3$), they cancel, and we get:\n$$P(M_j | \\mathbf{y}) = \\frac{p(\\mathbf{y} | M_j)}{\\sum_{k=1}^3 p(\\mathbf{y} | M_k)}$$\nTo handle potential numerical underflow or overflow, we compute these probabilities from the log-evidences $\\ell_j = \\log p(\\mathbf{y}|M_j)$ using the log-sum-exp trick:\nLet $\\ell_{\\max} = \\max(\\ell_1, \\ell_2, \\ell_3)$. Then the posterior weights $w_j = P(M_j|\\mathbf{y})$ are:\n$$w_j = \\frac{\\exp(\\ell_j - \\ell_{\\max})}{\\sum_{k=1}^3 \\exp(\\ell_k - \\ell_{\\max})}$$\n\n#### 4. BMA Posterior, Mean, and Credible Interval\nThe BMA posterior distribution for $f_0$ is a weighted mixture of the individual model posteriors:\n$$p(f_0 | \\mathbf{y}) = \\sum_{j=1}^3 w_j \\, p(f_0 | \\mathbf{y}, M_j) = \\sum_{j=1}^3 w_j \\, \\mathcal{N}(f_0 | \\mu_{f_0, j}, \\sigma_{f_0, j}^2)$$\nThe mean of this mixture distribution is the BMA posterior mean:\n$$E[f_0 | \\mathbf{y}] = \\sum_{j=1}^3 w_j \\mu_{f_0, j}$$\nThe central equal-tailed credible interval is defined by the quantiles of the mixture distribution. For a $68\\%$ credible interval, we need the $16\\%$ and $84\\%$ quantiles. The cumulative distribution function (CDF) of the mixture is:\n$$F(x) = P(f_0 \\le x | \\mathbf{y}) = \\sum_{j=1}^3 w_j \\, \\Phi\\left(\\frac{x - \\mu_{f_0, j}}{\\sigma_{f_0, j}}\\right)$$\nwhere $\\Phi$ is the standard normal CDF. We must find the values $x_{0.16}$ and $x_{0.84}$ such that $F(x_{0.16}) = 0.16$ and $F(x_{0.84}) = 0.84$. These equations are solved numerically using a root-finding algorithm.\n\n### Algorithmic Implementation\n\nFor each test case, the algorithm proceeds as follows:\n1.  Initialize prior parameters: $\\tau_0=10, \\tau_c=1$.\n2.  For each of the three models ($M_1, M_2, M_3$):\n    a. Construct the design matrix $X_j$ from the lattice spacings $a_i$ using the model's basis functions: $a_i$, $a_i^2$, and $a_i^2 \\log a_i$.\n    b. Compute the posterior mean $\\boldsymbol{\\mu}_{n,j}$ and covariance $\\Sigma_{n,j}$.\n    c. Extract the mean $\\mu_{f_0, j}$ and standard deviation $\\sigma_{f_0, j}$ for the parameter $f_0$.\n    d. Compute the log-model-evidence $\\ell_j = \\log p(\\mathbf{y}|M_j)$.\n3.  Calculate the posterior model probabilities $w_j$ from the log-evidences.\n4.  Calculate the BMA posterior mean for $f_0$.\n5.  Define the mixture CDF, $F(x)$.\n6.  Use a numerical root-finder (e.g., `scipy.optimize.brentq`) to find the lower ($x_{0.16}$) and upper ($x_{0.84}$) bounds of the $68\\%$ credible interval by solving $F(x) - q = 0$ for $q=0.16$ and $q=0.84$.\n7.  Collect and format the BMA mean, lower bound, and upper bound, rounded to six decimal places.\n\nThis procedure is systematically applied to all five test cases to produce the final result.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm\nfrom scipy.optimize import brentq\n\ndef bma_extrapolation(a_data, y_data, sigma_data):\n    \"\"\"\n    Performs Bayesian Model Averaging for continuum extrapolation.\n    \"\"\"\n    # Convert inputs to numpy arrays\n    a = np.array(a_data)\n    y = np.array(y_data)\n    sigma = np.array(sigma_data)\n    \n    # Priors\n    tau_0 = 10.0\n    tau_c = 1.0\n    prior_cov_inv = np.diag([1.0 / tau_0**2, 1.0 / tau_c**2])\n    log_det_prior_cov = 2.0 * (np.log(tau_0) + np.log(tau_c))\n\n    # Data-related quantities\n    n = len(y)\n    W = np.diag(1.0 / sigma**2)\n    log_det_obs_cov = np.sum(2.0 * np.log(sigma))\n    yT_W_y = y @ W @ y\n    \n    # Basis functions for the three models\n    basis_functions = [\n        lambda x: x,\n        lambda x: x**2,\n        lambda x: x**2 * np.log(x)\n    ]\n    \n    model_posteriors = []\n    log_evidences = []\n\n    for basis_func in basis_functions:\n        # Build design matrix X\n        phi = basis_func(a)\n        X = np.vstack([np.ones_like(phi), phi]).T\n        \n        # Posterior calculation\n        A = prior_cov_inv + X.T @ W @ X\n        A_inv = np.linalg.inv(A)\n        b = X.T @ W @ y\n        \n        post_mean = A_inv @ b\n        post_cov = A_inv\n        \n        f0_mean = post_mean[0]\n        f0_var = post_cov[0, 0]\n        \n        model_posteriors.append({'mean': f0_mean, 'std': np.sqrt(f0_var)})\n        \n        # Log evidence calculation\n        sign, log_det_A = np.linalg.slogdet(A)\n        energy_term = yT_W_y - b @ post_mean\n        \n        log_evidence = -0.5 * (n * np.log(2 * np.pi) + log_det_prior_cov +\n                               log_det_obs_cov + log_det_A + energy_term)\n        log_evidences.append(log_evidence)\n    \n    # Model probabilities (weights)\n    log_evidences = np.array(log_evidences)\n    max_log_evidence = np.max(log_evidences)\n    evidences = np.exp(log_evidences - max_log_evidence)\n    post_probs = evidences / np.sum(evidences)\n    \n    # BMA mean\n    bma_mean = np.sum([p['mean'] * w for p, w in zip(model_posteriors, post_probs)])\n    \n    # BMA credible interval\n    def mixture_cdf(x):\n        cdf_val = 0.0\n        for i, p in enumerate(model_posteriors):\n            cdf_val += post_probs[i] * norm.cdf(x, loc=p['mean'], scale=p['std'])\n        return cdf_val\n\n    # Find a reasonable search interval for the root finder\n    all_means = [p['mean'] for p in model_posteriors]\n    all_stds = [p['std'] for p in model_posteriors]\n    search_min = min(all_means) - 10 * max(all_stds)\n    search_max = max(all_means) + 10 * max(all_stds)\n\n    # Functions to find roots for quantiles\n    lower_quantile_func = lambda x: mixture_cdf(x) - 0.16\n    upper_quantile_func = lambda x: mixture_cdf(x) - 0.84\n    \n    # Find lower and upper bounds of the 68% CI\n    ci_lower = brentq(lower_quantile_func, search_min, search_max)\n    ci_upper = brentq(upper_quantile_func, search_min, search_max)\n    \n    return [bma_mean, ci_lower, ci_upper]\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1\n        {\n            \"a\": [0.12, 0.09, 0.06, 0.045],\n            \"y\": [0.52028, 0.50772, 0.50582, 0.50143],\n            \"sigma\": [0.005, 0.004, 0.003, 0.003]\n        },\n        # Case 2\n        {\n            \"a\": [0.16, 0.12, 0.08, 0.04],\n            \"y\": [0.877, 0.902, 0.939, 0.964],\n            \"sigma\": [0.01, 0.01, 0.01, 0.01]\n        },\n        # Case 3\n        {\n            \"a\": [0.10, 0.08, 0.06, 0.05, 0.04],\n            \"y\": [0.189987074535, 0.1909176683392, 0.1954358607098302, 0.1957553346575, 0.19842489934],\n            \"sigma\": [0.004, 0.004, 0.004, 0.004, 0.004]\n        },\n        # Case 4\n        {\n            \"a\": [0.10, 0.05],\n            \"y\": [0.311, 0.29875],\n            \"sigma\": [0.02, 0.02]\n        },\n        # Case 5\n        {\n            \"a\": [0.15, 0.12, 0.09, 0.06, 0.03],\n            \"y\": [0.845, 0.78, 0.805, 0.765, 0.77],\n            \"sigma\": [0.05, 0.05, 0.05, 0.05, 0.05]\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        result = bma_extrapolation(case[\"a\"], case[\"y\"], case[\"sigma\"])\n        results.append(result)\n\n    # Format the final output string exactly as specified.\n    result_strings = []\n    for res in results:\n        mean, lower, upper = res\n        result_strings.append(f\"[{mean:.6f},{lower:.6f},{upper:.6f}]\")\n    \n    final_output = f\"[{','.join(result_strings)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "3509855"}, {"introduction": "The final step in a rigorous analysis is to check whether our chosen model and its underlying assumptions are actually consistent with the observed data. This exercise [@problem_id:3509857] introduces Posterior Predictive Checks (PPCs) as a crucial diagnostic tool, specifically for detecting underestimated measurement errors. You will implement an iterative procedure to augment the reported errors with an \"intrinsic scatter\" term, $\\sigma_{\\mathrm{int}}$, until the model's predictions are statistically compatible with the data, ensuring the credibility of your final uncertainty estimates.", "problem": "You are given a synthetic lattice dataset for the mass of the boson in the Schwinger model (one-dimensional Quantum Electrodynamics), constructed so that the continuum value is known from the exactly solvable theory. In the massless Schwinger model, the boson mass is $m = g / \\sqrt{\\pi}$, so the continuum target in units of the coupling $g$ is $m/g = 1/\\sqrt{\\pi}$. On the lattice with spacing $a$, an observable $O(a)$ approximates its continuum limit $O_0$ via a discretization expansion, where the leading artifact for an unimproved local operator is commonly of order $a^2$, so that $O(a) = O_0 + c_2 a^2 + \\text{higher-order terms}$. Measurements at finite lattice spacing have reported statistical errors that may be accurate or underestimated. Your task is to detect and correct underestimated errors through Posterior Predictive Checks (PPCs), and to perform a continuum extrapolation with a rigorous uncertainty estimate.\n\nFundamental base:\n- The lattice spacing $a$ tends to the continuum limit as $a \\to 0$.\n- The measurement model is additive and Gaussian: $y_i = O(a_i) + \\epsilon_i$ with $\\epsilon_i \\sim \\mathcal{N}(0,\\sigma_{i,\\text{rep}}^2)$, where $\\sigma_{i,\\text{rep}}$ is the reported standard deviation for point $i$.\n- The leading discretization term is quadratic in $a$ for an unimproved action, so $O(a) = O_0 + c_2 a^2$ is a minimal parametric model.\n- In Bayesian model checking, Posterior Predictive Checks (PPCs) quantify whether the observed data are typical draws from the posterior predictive distribution. A common PPC is coverage of a central credible band and its associated binomial tail probability under the nominal coverage level.\n- If reported errors are underestimated, a practical correction is to augment each variance with an intrinsic scatter $\\sigma_{\\text{int}}^2$ such that the posterior predictive coverage is statistically consistent with the nominal level.\n\nYour program must implement the following principled procedure on each case in the test suite:\n1. Assume the parametric model $y_i = O_0 + c_2 a_i^2 + \\epsilon_i$ with $\\epsilon_i$ Gaussian. Treat the total variance as $\\sigma_i^2 = \\sigma_{i,\\text{rep}}^2 + \\sigma_{\\text{int}}^2$, where $\\sigma_{\\text{int}} \\ge 0$ is an intrinsic scatter to be determined.\n2. For a given $\\sigma_{\\text{int}}$, perform a weighted least-squares fit to $(O_0,c_2)$ with weights $w_i = 1/\\sigma_i^2$. Use the resulting parameter covariance to approximate a Gaussian posterior for $(O_0,c_2)$ under a non-informative prior.\n3. Construct the $95\\%$ posterior predictive interval for each $y_i$ using the sum of observational variance and parametric uncertainty at $a_i$. Compute the coverage fraction (as a decimal) of the observed $y_i$ falling inside these intervals and the corresponding binomial tail probability for observing at most that many successes under the nominal coverage $0.95$.\n4. Detect underestimated errors by checking the initial PPC with $\\sigma_{\\text{int}} = 0$. If the binomial tail probability is less than $0.05$, declare a failure and increase $\\sigma_{\\text{int}}$ until the PPC passes by finding the smallest $\\sigma_{\\text{int}} \\in [0,0.05]$ on a uniform grid that achieves a binomial tail probability at least $0.05$.\n5. Report the final continuum estimate $O_0$ and its $95\\%$ credible interval from the parameter covariance. All reported quantities must be expressed in the dimensionless ratio $m/g$, i.e., units of $g$. The coverage must be reported as a decimal. The pass/fail indicator must be reported as an integer, with $1$ indicating that the PPC passes after correction and $0$ otherwise.\n\nTest suite:\n- Case $1$ (accurate errors, happy path): lattice spacings $a = [0.30,0.20,0.15,0.10,0.05]$, reported standard deviations $\\sigma_{\\text{rep}} = [0.006,0.005,0.004,0.004,0.003]$, observed values $y = [0.6221895835,0.5851895835,0.5786895835,0.5681895835,0.5666895835]$.\n- Case $2$ (underestimated errors): lattice spacings $a = [0.30,0.20,0.15,0.10,0.05]$, reported standard deviations $\\sigma_{\\text{rep}} = [0.002,0.002,0.002,0.002,0.002]$, observed values $y = [0.6251895835,0.5821895835,0.5816895835,0.5651895835,0.5716895835]$.\n- Case $3$ (coarse lattices dominate): lattice spacings $a = [0.35,0.30,0.25]$, reported standard deviations $\\sigma_{\\text{rep}} = [0.010,0.009,0.008]$, observed values $y = [0.6214395835,0.6121895835,0.5934395835]$.\n- Case $4$ (boundary with minimal identifiability): lattice spacings $a = [0.20,0.10]$, reported standard deviations $\\sigma_{\\text{rep}} = [0.006,0.004]$, observed values $y = [0.5891895835,0.5676895835]$.\n- Case $5$ (underestimated errors with mild model misspecification and an outlier): lattice spacings $a = [0.30,0.20,0.15,0.12,0.08]$, reported standard deviations $\\sigma_{\\text{rep}} = [0.003,0.003,0.003,0.003,0.003]$, observed values $y = [0.6169495835,0.5865495835,0.5789870835,0.5807466395,0.5670131995]$.\n\nFinal output format:\nYour program should produce a single line of output containing the results for all cases as a comma-separated top-level list of per-case results, where each per-case result is a list of five elements in the order $[O_0, \\text{CI}_{\\text{low}}, \\text{CI}_{\\text{high}}, \\text{coverage}, \\text{pass}]$. For example, the printed structure must look like $[[\\dots],[\\dots],\\dots]$ with all numbers and integers shown explicitly. The continuum estimate and its interval must be reported in units of $g$, i.e., dimensionless ratios $m/g$. The coverage must be a decimal and the pass/fail indicator must be an integer.", "solution": "The problem requires performing a continuum extrapolation of lattice data, including a principled method for assessing and correcting underestimated statistical errors. The procedure involves fitting a parametric model, using Posterior Predictive Checks (PPCs) for model validation, and iteratively adjusting an intrinsic error term to satisfy the PPC. I will first detail the theoretical and statistical framework, and then describe its implementation.\n\n### 1. Model Formulation\n\nThe core of the problem is to extrapolate an observable $O(a)$, measured at several finite lattice spacings $a$, to the continuum limit $a \\to 0$. The problem states that for an unimproved local operator, the leading-order discretization artifacts are of order $a^2$. Therefore, we can model the dependence of the observable on the lattice spacing as:\n$$\nO(a) = O_0 + c_2 a^2 + \\mathcal{O}(a^4)\n$$\nwhere $O_0$ is the desired continuum value and $c_2$ is a coefficient parametrizing the leading discretization effects. For our analysis, we truncate this series and adopt the linear model:\n$$\nO(a) = O_0 + c_2 a^2\n$$\nThe observed data points $y_i$ at lattice spacings $a_i$ are subject to statistical fluctuations. We model this with an additive Gaussian noise term, $\\epsilon_i$. The problem postulates that the reported variances, $\\sigma_{i,\\text{rep}}^2$, may be underestimated. To account for this, we introduce a non-negative intrinsic scatter term, $\\sigma_{\\text{int}}$, which is added in quadrature to the reported error. The total variance for the $i$-th measurement is thus:\n$$\n\\sigma_i^2 = \\sigma_{i,\\text{rep}}^2 + \\sigma_{\\text{int}}^2\n$$\nThe full measurement model for the $i$-th data point is:\n$$\ny_i = O_0 + c_2 a_i^2 + \\epsilon_i, \\quad \\text{where} \\quad \\epsilon_i \\sim \\mathcal{N}(0, \\sigma_i^2)\n$$\n\n### 2. Parameter Estimation via Weighted Least Squares (WLS)\n\nGiven a value for $\\sigma_{\\text{int}}$, and thus for each $\\sigma_i^2$, the parameters $\\theta = (O_0, c_2)^T$ can be estimated by minimizing the weighted sum of squared residuals, or $\\chi^2$:\n$$\n\\chi^2(\\theta) = \\sum_{i=1}^{N} \\frac{(y_i - (O_0 + c_2 a_i^2))^2}{\\sigma_i^2}\n$$\nThis is a Weighted Least Squares (WLS) problem. In matrix notation, let $y = (y_1, \\dots, y_N)^T$ be the vector of observations, and let $X$ be the $N \\times 2$ design matrix, where the $i$-th row is $(1, a_i^2)$. Let $W$ be the $N \\times N$ diagonal weight matrix with entries $W_{ii} = 1/\\sigma_i^2$. The WLS estimator for $\\theta$ is:\n$$\n\\hat{\\theta} = (X^T W X)^{-1} X^T W y\n$$\nAssuming a non-informative (flat) prior on $\\theta$, the posterior distribution $p(\\theta|y)$ is approximately a multivariate Gaussian centered at $\\hat{\\theta}$ with a covariance matrix $C$:\n$$\nC = \\text{Cov}(\\hat{\\theta}) = (X^T W X)^{-1}\n$$\nThe continuum extrapolation is $\\hat{O}_0 = \\hat{\\theta}_0$, and its variance is the top-left element of the covariance matrix, $\\text{Var}(\\hat{O}_0) = C_{00}$. The $95\\%$ credible interval for $O_0$ is then calculated as $\\hat{O}_0 \\pm z \\sqrt{C_{00}}$, where $z = \\Phi^{-1}(0.975) \\approx 1.96$ is the $97.5$-th percentile of the standard normal distribution.\n\n### 3. Model Checking with Posterior Predictive Checks (PPC)\n\nA key task is to validate the model, including our choice for $\\sigma_{\\text{int}}$. We use a Posterior Predictive Check. The posterior predictive distribution for a new (or replicated) observation $y_i^{\\text{pred}}$ at a given lattice spacing $a_i$ is the distribution of $y_i^{\\text{pred}}$ averaged over the posterior distribution of the parameters $\\theta$. For a Gaussian posterior, this results in a Gaussian predictive distribution:\n$$\ny_i^{\\text{pred}} \\sim \\mathcal{N}(\\mu_{\\text{pred},i}, \\sigma_{\\text{pred},i}^2)\n$$\nThe mean $\\mu_{\\text{pred},i}$ is a prediction from the best-fit model:\n$$\n\\mu_{\\text{pred},i} = \\hat{O}_0 + \\hat{c}_2 a_i^2 = (X\\hat{\\theta})_i\n$$\nThe predictive variance $\\sigma_{\\text{pred},i}^2$ is the sum of two components: the observational variance $\\sigma_i^2$ and the uncertainty in the mean prediction due to parameter uncertainty. This parametric uncertainty is propagated from the parameter covariance matrix $C$:\n$$\n\\text{Var}(\\mu_{\\text{pred},i}) = x_i^T C x_i, \\quad \\text{where} \\quad x_i = (1, a_i^2)^T\n$$\nThus, the total predictive variance is:\n$$\n\\sigma_{\\text{pred},i}^2 = \\sigma_i^2 + x_i^T C x_i = (\\sigma_{i,\\text{rep}}^2 + \\sigma_{\\text{int}}^2) + x_i^T C x_i\n$$\nA $95\\%$ posterior predictive interval for each observation $y_i$ is then $[\\mu_{\\text{pred},i} - z \\sigma_{\\text{pred},i}, \\mu_{\\text{pred},i} + z \\sigma_{\\text{pred},i}]$.\n\nThe check consists of comparing the observed data $y_i$ to these intervals. We count the number of data points, $k$, that fall within their respective $95\\%$ predictive intervals. If the model is a good description of the data, we expect this to happen for approximately $95\\%$ of the points. The number of \"successes\" $k$ out of $N$ trials should follow a binomial distribution, $k \\sim B(N, p=0.95)$. A significant deviation, specifically too few successes, suggests the model is too confident and its predictive intervals are too narrow. We quantify this deviation by computing the binomial tail probability $P(K \\le k)$, where $K \\sim B(N, p=0.95)$. If this probability is less than a threshold (here, $0.05$), we conclude that the model fails the check.\n\n### 4. Iterative Error Correction\n\nThe overall procedure is as follows:\n1.  Initialize with $\\sigma_{\\text{int}} = 0$.\n2.  Perform the WLS fit and compute the parameter estimates $\\hat{\\theta}$ and covariance $C$.\n3.  Conduct the PPC: Construct the $95\\%$ posterior predictive intervals and calculate the number of covered data points, $k$.\n4.  Compute the binomial tail probability $P(K \\le k)$.\n5.  If this probability is greater than or equal to $0.05$, the model passes. The analysis is complete. The pass indicator is set to $1$.\n6.  If the probability is less than $0.05$, the model fails, likely due to underestimated errors. We then search for the smallest non-negative $\\sigma_{\\text{int}}$ that resolves this failure. This is done by iterating through a uniform grid of $\\sigma_{\\text{int}}$ values in the range $[0, 0.05]$ and repeating steps 2-4. The first value of $\\sigma_{\\text{int}}$ for which the tail probability is at least $0.05$ is chosen. The analysis results from this value are adopted, and the pass indicator is set to $1$. If no value of $\\sigma_{\\text{int}}$ up to $0.05$ is sufficient to pass the check, the procedure fails to find a suitable correction, and the pass indicator is set to $0$.\n\nThe final reported values for $O_0$ and its $95\\%$ credible interval are taken from the analysis corresponding to the final accepted value of $\\sigma_{\\text{int}}$.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import binom, norm\n\ndef solve():\n    \"\"\"\n    Main function to run the analysis on all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        # Case 1: accurate errors, happy path\n        ([0.30, 0.20, 0.15, 0.10, 0.05],\n         [0.006, 0.005, 0.004, 0.004, 0.003],\n         [0.6221895835, 0.5851895835, 0.5786895835, 0.5681895835, 0.5666895835]),\n        # Case 2: underestimated errors\n        ([0.30, 0.20, 0.15, 0.10, 0.05],\n         [0.002, 0.002, 0.002, 0.002, 0.002],\n         [0.6251895835, 0.5821895835, 0.5816895835, 0.5651895835, 0.5716895835]),\n        # Case 3: coarse lattices dominate\n        ([0.35, 0.30, 0.25],\n         [0.010, 0.009, 0.008],\n         [0.6214395835, 0.6121895835, 0.5934395835]),\n        # Case 4: boundary with minimal identifiability\n        ([0.20, 0.10],\n         [0.006, 0.004],\n         [0.5891895835, 0.5676895835]),\n        # Case 5: underestimated errors with mild model misspecification and an outlier\n        ([0.30, 0.20, 0.15, 0.12, 0.08],\n         [0.003, 0.003, 0.003, 0.003, 0.003],\n         [0.6169495835, 0.5865495835, 0.5789870835, 0.5807466395, 0.5670131995])\n    ]\n\n    results = []\n    for case_data in test_cases:\n        a_vals, sigma_rep_vals, y_vals = (np.array(d) for d in case_data)\n        result = process_case(a_vals, sigma_rep_vals, y_vals)\n        results.append(result)\n\n    # Format the final output string as specified in the problem\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef process_case(a, sigma_rep, y):\n    \"\"\"\n    Performs the full analysis for a single case: WLS fit, PPC, and error correction.\n    \"\"\"\n    \n    def perform_analysis(sigma_int_val):\n        \"\"\"\n        Helper function to perform WLS fit and PPC for a given sigma_int.\n        \"\"\"\n        # 1. Total variance calculation\n        total_sigma_sq = sigma_rep**2 + sigma_int_val**2\n        \n        n_pts = len(y)\n        if n_pts < 2:\n            return None\n\n        # 2. Weighted Least Squares (WLS) fit\n        X = np.vstack([np.ones(n_pts), a**2]).T\n        W = np.diag(1.0 / total_sigma_sq)\n        \n        try:\n            XT_W_X = X.T @ W @ X\n            param_cov = np.linalg.inv(XT_W_X)\n            XT_W_y = X.T @ W @ y\n            params = param_cov @ XT_W_y\n        except np.linalg.LinAlgError:\n            return None # Should not happen for N>=2 with distinct a_i\n\n        O0_hat = params[0]\n\n        # 3. Posterior Predictive Check (PPC)\n        mu_pred = X @ params\n        var_parametric = np.sum((X @ param_cov) * X, axis=1)\n        var_pred = total_sigma_sq + var_parametric\n\n        z_95 = norm.ppf(1.0 - 0.05 / 2.0)\n        lower_bound = mu_pred - z_95 * np.sqrt(var_pred)\n        upper_bound = mu_pred + z_95 * np.sqrt(var_pred)\n        \n        is_covered = (y >= lower_bound) & (y <= upper_bound)\n        coverage_count = np.sum(is_covered)\n        coverage_fraction = coverage_count / n_pts\n        \n        # 4. Binomial tail probability for PPC\n        tail_prob = binom.cdf(coverage_count, n=n_pts, p=0.95)\n        \n        return {\n            \"O0\": O0_hat,\n            \"param_cov\": param_cov,\n            \"coverage\": coverage_fraction,\n            \"tail_prob\": tail_prob\n        }\n\n    # Start with initial check for sigma_int = 0\n    initial_analysis = perform_analysis(sigma_int_val=0.0)\n    \n    if initial_analysis is None:\n        return [float('nan'), float('nan'), float('nan'), float('nan'), 0]\n\n    final_analysis = initial_analysis\n    pass_flag = 1 # Assume success unless proven otherwise\n\n    # If PPC fails, search for a corrective sigma_int\n    if initial_analysis[\"tail_prob\"] < 0.05:\n        sigma_int_grid = np.linspace(0, 0.05, 501)\n        correction_found = False\n        \n        last_good_analysis = None\n        for s_int in sigma_int_grid[1:]:\n            current_analysis = perform_analysis(sigma_int_val=s_int)\n            if current_analysis is None:\n                continue\n            last_good_analysis = current_analysis\n\n            if current_analysis[\"tail_prob\"] >= 0.05:\n                final_analysis = current_analysis\n                correction_found = True\n                break\n        \n        if not correction_found:\n            # Correction failed even at max sigma_int, use a valid last result\n            if last_good_analysis:\n                final_analysis = last_good_analysis\n            pass_flag = 0\n\n    # Extract final results from the successful (or final attempted) analysis\n    O0_final = final_analysis[\"O0\"]\n    param_cov_final = final_analysis[\"param_cov\"]\n    coverage_final = final_analysis[\"coverage\"]\n    \n    std_err_O0 = np.sqrt(param_cov_final[0, 0])\n    z_95 = norm.ppf(1.0 - 0.05 / 2.0)\n    CI_low = O0_final - z_95 * std_err_O0\n    CI_high = O0_final + z_95 * std_err_O0\n    \n    # Return formatted list for this case\n    return [O0_final, CI_low, CI_high, coverage_final, pass_flag]\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3509857"}]}