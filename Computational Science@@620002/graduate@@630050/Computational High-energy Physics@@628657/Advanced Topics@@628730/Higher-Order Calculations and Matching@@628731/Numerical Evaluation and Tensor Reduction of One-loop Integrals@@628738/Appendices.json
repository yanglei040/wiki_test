{"hands_on_practices": [{"introduction": "The final value of a loop integral coefficient is often the result of summing many contributions over a multi-dimensional phase space, some of which are large and have opposite signs. This exercise [@problem_id:3525479] addresses the foundational challenge of numerical summation, where naive approaches can lead to a catastrophic loss of precision. By implementing and comparing standard summation algorithms against carefully constructed test cases, you will gain a practical understanding of how to manage rounding errors and preserve the delicate cancellations central to high-energy physics calculations.", "problem": "Consider the numerical aggregation of contributions to tensor coefficients in one-loop integrals over phase-space points. In many situations arising from Passarino–Veltman (PV) tensor reduction of one-loop integrals, near-canceling terms of large magnitude occur, and the final coefficient is the delicate residual of these cancellations. Let the contributions be real numbers modeled as a sequence $\\{x_i\\}_{i=1}^N$ summed to produce a partial tensor coefficient $S = \\sum_{i=1}^N x_i$. The stability of the numerical summation in floating point arithmetic depends heavily on the summation algorithm.\n\nWe will compare and quantify the numerical benefit of two compensated summation strategies relative to a naive summation:\n- Naive summation: straight accumulation $s \\leftarrow s + x_i$ in encounter order.\n- Pairwise summation: a balanced binary-tree summation that repeatedly sums adjacent pairs to reduce dynamic range effects.\n- Kahan compensated summation: a compensation algorithm maintaining an error accumulator to reduce loss of low-order bits.\n\nWork in double-precision (IEEE 754 binary64) arithmetic with a significand of $p = 53$ bits. For accuracy assessment, define the absolute error of an algorithm as $e = |S_{\\text{alg}} - S_{\\text{true}}|$, where $S_{\\text{alg}}$ is the computed double-precision sum and $S_{\\text{true}}$ is the correctly rounded double-precision value of the exact rational sum $\\sum_i x_i$. Define the normalization as follows:\n- If $|S_{\\text{true}}| > 0$, use the relative error $r = e / |S_{\\text{true}}|$.\n- If $|S_{\\text{true}}| = 0$, use the scale $L_1 = \\sum_{i=1}^N |x_i|$ and take $r = e / L_1$.\nFrom $r$, define the number of correct bits as $b = -\\log_2(r)$ for $r > 0$, and the bits lost (relative to the 53-bit significand) as $\\ell = \\max(0,\\, p - b)$. If $r = 0$, set $\\ell = 0$.\n\nImplement the three summation algorithms and report $\\ell$ for each algorithm on the following four scientifically plausible test cases that emulate near-cancellation patterns encountered during numerical evaluation of tensor coefficients after PV reduction and phase-space sampling. In all cases, the $x_i$ are to be constructed as rational numbers exactly representable in binary floating point when possible (i.e., integers and inverse powers of two) to allow an exact rational ground truth. The sequence definitions must be adhered to exactly.\n\nTest suite:\n- Case A (moderate alternating cancellation with a slowly varying offset): Let $N = 20000$ and define\n  $$x_i = (-1)^i\\left(2^{-20} + (i+1)2^{-44}\\right), \\quad i = 0,1,\\dots,N-1.$$\n  This models alternating large contributions with a small, slowly varying term that yields a nonzero residual.\n- Case B (catastrophic local cancellation via interleaving large and tiny terms): Let $M = 8000$ and create a triad pattern repeated $M$ times:\n  $$\\{2^{30},\\, 2^{-40},\\, -2^{30}\\}.$$\n  The exact sum over each triad is $2^{-40}$, but naive encounter order causes severe loss when adding $2^{-40}$ immediately after $2^{30}$.\n- Case C (exact zero-sum with dynamic range and adversarial ordering): Let $M = 7000$ and create a quad pattern repeated $M$ times:\n  $$\\{2^{30},\\, 2^{-45},\\, -2^{30},\\, -2^{-45}\\}.$$\n  The exact sum is $0$, but rounding behavior in naive order can accumulate a nonzero residue.\n- Case D (variable large scales with small residuals): Let $M = 6000$ and for $k = 1,2,\\dots,M$ create the triad\n  $$\\{2^{25} + k,\\, 2^{-50},\\, -(2^{25} + k)\\}.$$\n  The exact sum over each triad is $2^{-50}$, testing stability across varying large magnitudes.\n\nRequirements:\n- Construct the sequences $\\{x_i\\}$ as exact rational values using integer and inverse powers of two. Compute the exact rational sum $S_{\\text{exact}} = \\sum_i x_i$. Then set $S_{\\text{true}}$ to the correctly rounded double-precision value of $S_{\\text{exact}}$.\n- Implement:\n  1. Naive summation: $s \\leftarrow s + x_i$ in sequence order.\n  2. Pairwise summation: repeatedly sum adjacent pairs in a balanced tree manner until one value remains.\n  3. Kahan compensated summation: with compensation variable $c$, update using $y = x_i - c$, $t = s + y$, $c = (t - s) - y$, $s = t$.\n- Compute the bits lost $\\ell$ for each method in each case using the definitions above. If $S_{\\text{true}} = 0$, use the $L_1$ normalization described above. Report $\\ell$ as floating-point numbers.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case contributes a three-element list in the order [naive, pairwise, kahan]. The final output must thus be a single list of four inner lists:\n  $$\\text{Output} = [[\\ell_{\\text{naive,A}},\\ell_{\\text{pairwise,A}},\\ell_{\\text{kahan,A}}], [\\ell_{\\text{naive,B}},\\ell_{\\text{pairwise,B}},\\ell_{\\text{kahan,B}}], [\\ell_{\\text{naive,C}},\\ell_{\\text{pairwise,C}},\\ell_{\\text{kahan,C}}], [\\ell_{\\text{naive,D}},\\ell_{\\text{pairwise,D}},\\ell_{\\text{kahan,D}}]].$$\nNo physical units or angles are applicable; numerical values must be reported as dimensionless floats.", "solution": "The core numerical issue in summing contributions to tensor coefficients from one-loop integrals after Passarino–Veltman (PV) tensor reduction arises from floating point rounding when large, nearly canceling terms are combined. Phase-space integration can produce sequences $\\{x_i\\}$ where $x_i$ are large and alternate in sign, while the desired coefficients are the small residuals. This demands careful numerical summation methods.\n\nPrinciples from floating point arithmetic:\n- IEEE 754 binary64 has a significand with $p = 53$ bits. Let the machine epsilon be $\\varepsilon \\approx 2^{-53}$ in relative terms.\n- Naive summation $s \\leftarrow s + x_i$ can accumulate rounding error. When summands have vastly different magnitudes, the smaller term can be entirely swallowed (its least significant bits vanish), a phenomenon known as catastrophic cancellation when subsequent subtraction reveals the lost information.\n- Pairwise summation reduces error growth by summing numbers of similar magnitude first, mitigating the dynamic range of intermediate values.\n- Kahan compensated summation adds a correction term $c$ to carry forward the rounding residue, capturing lost low-order bits and reinjecting them in subsequent additions.\n\nAccuracy metric design:\nGiven an algorithm’s output $S_{\\text{alg}}$ in double precision and the ground truth $S_{\\text{true}}$ defined as the correctly rounded double-precision value of the exact rational sum $S_{\\text{exact}} = \\sum_i x_i$, define the absolute error $e = |S_{\\text{alg}} - S_{\\text{true}}|$. We normalize the error as follows:\n- If $|S_{\\text{true}}| > 0$, use $r = e / |S_{\\text{true}}|$.\n- If $|S_{\\text{true}}| = 0$, use the scale $L_1 = \\sum_i |x_i|$ and set $r = e / L_1$. This reflects that, for a zero true sum, absolute error must be evaluated against a meaningful scale to avoid division by zero and to reflect the conditioning of the problem.\nFrom $r$, define the number of correct bits as $b = -\\log_2(r)$ for $r > 0$. Interpreting binary64 as providing $p = 53$ bits in the significand, the bits lost are $\\ell = \\max(0, p - b)$. If $r = 0$, set $\\ell = 0$. This metric quantifies how many bits of precision were effectively lost relative to the ideal best-rounded double-precision result.\n\nAlgorithmic details:\n- Naive summation is implemented as a simple loop that accumulates a running total in the input order. It is sensitive to input ordering and dynamic range.\n- Pairwise summation performs a balanced binary-tree reduction. Adjacent pairs are summed to form a new, shorter list, and the process repeats until one value remains. This keeps intermediate sums closer in magnitude to inputs, lowering rounding error growth, especially in adversarial ordering.\n- Kahan compensated summation maintains a compensation term $c$ that captures rounding error on each addition. For each $x_i$, we compute $y = x_i - c$ and $t = s + y$, then update $c = (t - s) - y$ and $s = t$. This strategy recovers many lost low-order bits.\n\nRational ground truth:\nTo avoid contaminating the ground truth with floating point rounding, construct the test sequences using exact rationals: integers and powers of two produce values exactly representable in binary floating point. Using exact rational arithmetic (e.g., fractions), we compute $S_{\\text{exact}}$ exactly. We then convert $S_{\\text{exact}}$ to double precision via correct rounding to obtain $S_{\\text{true}}$. This ensures we measure only summation-induced error, not representation error in the final format.\n\nTest suite rationale:\n- Case A uses $x_i = (-1)^i(2^{-20} + (i+1)2^{-44})$ for $i = 0, \\dots, 19999$. The $2^{-20}$ term alternates and largely cancels; the $(i+1)2^{-44}$ term slowly varies and creates a small residual, modeling a common situation in tensor reduction where a dominant alternating structure coexists with a gentle net drift.\n- Case B uses repeated triads $\\{2^{30}, 2^{-40}, -2^{30}\\}$. The true sum per triad is $2^{-40}$. Naive addition of $2^{-40}$ immediately after $2^{30}$ loses low-order bits because $2^{-40} \\ll \\varepsilon \\cdot 2^{30} \\approx 2^{30}2^{-53} = 2^{-23}$. Pairwise summation effectively groups $2^{30}$ with $-2^{30}$ before adding $2^{-40}$, preserving the small residual. Kahan compensated summation also recovers lost bits through compensation.\n- Case C uses repeated quads $\\{2^{30}, 2^{-45}, -2^{30}, -2^{-45}\\}$, which sum to zero exactly. The ordering induces loss of the $2^{-45}$ contribution when added after $2^{30}$, and the later $-2^{-45}$ addition can create a nonzero residue in naive summation. Since $S_{\\text{true}} = 0$, we normalize by $L_1$.\n- Case D uses variable large scales with triads $\\{2^{25} + k, 2^{-50}, -(2^{25} + k)\\}$ for $k = 1, \\dots, 6000$. This tests the robustness of the algorithms against varying large magnitudes that cancel locally with a small residual.\n\nImplementation plan:\n- Generate each test case as a list of exact rational numbers. Compute their double-precision counterparts for summation.\n- Implement the three summation methods.\n- Compute $S_{\\text{true}}$ by summing rationals exactly and converting to double precision.\n- Compute $L_1$ exactly and convert to double precision.\n- Compute bits lost for each method using the definition above.\n- Produce a single-line output: a list of four inner lists, each containing three floats in the order [naive, pairwise, kahan].\n\nThis procedure cleanly separates algorithmic summation error from representation and yields a rigorous, quantitative comparison of compensated summation methods in scenarios mirroring the near-canceling tensor coefficient contributions encountered in computational high-energy physics.", "answer": "```python\nimport math\nfrom fractions import Fraction\n\ndef naive_sum(arr):\n    s = 0.0\n    for x in arr:\n        s += x\n    return s\n\ndef pairwise_sum(arr):\n    # Balanced binary-tree summation without sorting\n    n = len(arr)\n    if n == 0:\n        return 0.0\n    work = arr[:]\n    while len(work) > 1:\n        new_work = []\n        it = iter(work)\n        for a in it:\n            try:\n                b = next(it)\n                new_work.append(a + b)\n            except StopIteration:\n                new_work.append(a)\n        work = new_work\n    return work[0]\n\ndef kahan_sum(arr):\n    s = 0.0\n    c = 0.0\n    for x in arr:\n        y = x - c\n        t = s + y\n        c = (t - s) - y\n        s = t\n    return s\n\ndef bits_lost(computed, true_value, l1_scale, significand_bits=53):\n    error = abs(computed - true_value)\n    if abs(true_value) > 0.0:\n        ratio = error / abs(true_value)\n    else:\n        ratio = (error / l1_scale) if l1_scale > 0.0 else 0.0\n    if ratio == 0.0:\n        return 0.0\n    bits_correct = -math.log2(ratio)\n    lost = significand_bits - bits_correct\n    if lost < 0.0:\n        lost = 0.0\n    return lost\n\ndef case_A():\n    # N = 20000, x_i = (-1)^i (2^-20 + (i+1) 2^-44), i = 0..N-1\n    N = 20000\n    exact = []\n    for i in range(N):\n        sign = 1 if (i % 2 == 0) else -1\n        term = Fraction(1, 2**20) + Fraction(i + 1, 2**44)\n        exact.append(sign * term)\n    floats = [float(x) for x in exact]\n    true_exact = sum(exact, Fraction(0, 1))\n    true_val = float(true_exact)\n    l1_exact = sum((abs(x) for x in exact), Fraction(0, 1))\n    l1_val = float(l1_exact)\n    return floats, true_val, l1_val\n\ndef case_B():\n    # M = 8000, triads {2^30, 2^-40, -2^30}\n    M = 8000\n    exact = []\n    for _ in range(M):\n        exact.append(Fraction(2**30, 1))\n        exact.append(Fraction(1, 2**40))\n        exact.append(Fraction(-2**30, 1))\n    floats = [float(x) for x in exact]\n    true_exact = sum(exact, Fraction(0, 1))\n    true_val = float(true_exact)\n    l1_exact = sum((abs(x) for x in exact), Fraction(0, 1))\n    l1_val = float(l1_exact)\n    return floats, true_val, l1_val\n\ndef case_C():\n    # M = 7000, quads {2^30, 2^-45, -2^30, -2^-45}; true sum = 0\n    M = 7000\n    exact = []\n    for _ in range(M):\n        exact.append(Fraction(2**30, 1))\n        exact.append(Fraction(1, 2**45))\n        exact.append(Fraction(-2**30, 1))\n        exact.append(Fraction(-1, 2**45))\n    floats = [float(x) for x in exact]\n    true_exact = sum(exact, Fraction(0, 1))\n    true_val = float(true_exact)\n    l1_exact = sum((abs(x) for x in exact), Fraction(0, 1))\n    l1_val = float(l1_exact)\n    return floats, true_val, l1_val\n\ndef case_D():\n    # M = 6000, triads {2^25 + k, 2^-50, -(2^25 + k)}, k=1..M\n    M = 6000\n    exact = []\n    for k in range(1, M + 1):\n        H = Fraction(2**25 + k, 1)\n        exact.append(H)\n        exact.append(Fraction(1, 2**50))\n        exact.append(-H)\n    floats = [float(x) for x in exact]\n    true_exact = sum(exact, Fraction(0, 1))\n    true_val = float(true_exact)\n    l1_exact = sum((abs(x) for x in exact), Fraction(0, 1))\n    l1_val = float(l1_exact)\n    return floats, true_val, l1_val\n\ndef solve():\n    # Generate test cases\n    test_cases = [case_A(), case_B(), case_C(), case_D()]\n    results = []\n    for floats, true_val, l1_val in test_cases:\n        # Compute sums\n        s_naive = naive_sum(floats)\n        s_pair = pairwise_sum(floats)\n        s_kahan = kahan_sum(floats)\n        # Compute bits lost\n        b_naive = bits_lost(s_naive, true_val, l1_val)\n        b_pair = bits_lost(s_pair, true_val, l1_val)\n        b_kahan = bits_lost(s_kahan, true_val, l1_val)\n        results.append([b_naive, b_pair, b_kahan])\n    # Print final output in required format\n    # Single line: list of four inner lists [naive, pairwise, kahan] for each case\n    print(f\"[{','.join(str(lst) for lst in results)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3525479"}, {"introduction": "Beyond individual arithmetic operations, a key challenge in modern one-loop tools is the strategic selection of the entire reduction algorithm based on the specific kinematics of a scattering event. This practice [@problem_id:3525555] puts you in the role of an algorithm designer, tasked with creating a robust criterion to choose between the Passarino-Veltman (PV) and Ossola-Papadopoulos-Pittau (OPP) methods. You will use Gram determinants as quantitative probes of kinematic stability to build a decision engine that avoids numerically unstable regions, a crucial feature for any automated calculation.", "problem": "You are tasked with designing and implementing a runtime decision criterion that, given external four-momenta and internal masses for a one-loop integral topology, selects between the Ossola–Papadopoulos–Pittau (OPP) integrand reduction and the Passarino–Veltman (PV) tensor reduction. The decision must be based on the behavior of Baikov representation (BR) minors, with the goal of minimizing exposure to small Gram determinants that cause numerical instabilities in PV reduction. Your program must be a complete, runnable implementation of the criterion.\n\nBegin from the following foundational base:\n\n- A one-loop $N$-point tensor integral depends on external four-momenta $\\{p_i\\}$ and internal masses $\\{m_j\\}$, where the loop momentum is integrated in four-dimensional spacetime using natural units.\n- In Minkowski space with signature $(+,-,-,-)$, the scalar product is $p \\cdot q = p^0 q^0 - \\vec{p}\\cdot\\vec{q}$.\n- The Gram matrix of $r$ independent external four-momenta $\\{p_1,\\dots,p_r\\}$ is defined as $$G_{ij} = p_i \\cdot p_j, \\quad i,j=1,\\dots,r.$$\n- The Baikov representation expresses one-loop integrals in terms of inverse propagators, and its characteristic polynomial has coefficients proportional to principal minors (determinants of principal submatrices) of the Gram matrix of external momenta. These minors encode kinematic degeneracies such as collinearity and near-singular configurations.\n- Passarino–Veltman (PV) tensor reduction relies on inverses of Gram determinants at various ranks; small Gram determinants lead to large cancellations and loss of precision. Ossola–Papadopoulos–Pittau (OPP) integrand reduction avoids explicit Gram inversion and is typically more robust in nearly singular kinematics.\n\nYour objective is to construct a quantitative, dimensionless stability score from Baikov (Gram) minors and use it to decide between PV and OPP. You must implement the following steps:\n\n1. Accept a set of external four-momenta $\\{p_i\\}_{i=1}^r$ with $r \\leq 4$, expressed in gigaelectronvolts (GeV) for energy and momentum components, and a set of internal masses $\\{m_j\\}$ in GeV. For this problem, assume the provided momenta are already independent; do not perform basis reduction.\n2. Construct the Gram matrix $G \\in \\mathbb{R}^{r \\times r}$ with entries $G_{ij} = p_i \\cdot p_j$ using the Minkowski metric $(+,-,-,-)$.\n3. Define a kinematic scale $$s_0 = \\max\\left(\\max_{1\\le i,j\\le r} |G_{ij}|,\\; 1 \\right)$$ in $\\text{GeV}^2$. This sets a reference magnitude for normalization.\n4. For every non-empty index subset $S \\subseteq \\{1,\\dots,r\\}$ of size $|S|=k$, form the principal submatrix $G_S$ and its determinant $\\Delta_S = \\det(G_S)$. Define the normalized minor $$\\widehat{\\Delta}_S = \\frac{|\\Delta_S|}{s_0^{k}}.$$ Then define the global stability score $$\\mu = \\min_{S\\neq \\emptyset} \\left(\\widehat{\\Delta}_S\\right)^{1/k}.$$ Intuitively, $\\mu$ captures the smallest normalized geometric-mean scale among principal minors, and becomes small in nearly singular kinematics.\n5. Compute the normalized full Gram determinant $$\\widehat{\\Delta}_{\\text{full}} = \\frac{|\\det(G)|}{s_0^{r}}.$$ Also compute the condition number $$\\kappa = \\frac{\\sigma_{\\max}(G)}{\\sigma_{\\min}(G)},$$ where $\\sigma_{\\max}$ and $\\sigma_{\\min}$ are the largest and smallest singular values of $G$ (interpreting $\\kappa = 1$ if $r=1$).\n6. Implement the decision rule: choose PV if simultaneously $\\mu \\ge \\tau$, $\\widehat{\\Delta}_{\\text{full}} \\ge \\epsilon$, and $\\kappa \\le \\kappa_{\\max}$; otherwise choose OPP. Use the following default thresholds:\n   - $\\tau = 10^{-5}$\n   - $\\epsilon = 10^{-8}$\n   - $\\kappa_{\\max} = 10^{12}$\n   These are dimensionless thresholds.\n\nYour program must hard-code the following test suite of four cases, each specified as a tuple of external momenta and internal masses. All four-momenta are in GeV, and masses are in GeV. Angles are not directly provided, and all calculations must use radians implicitly when dealing with directional components.\n\n- Test Case 1 (general well-separated kinematics, expected PV-safe):\n  - External momenta: \n    - $p_1 = (200,\\,0,\\,0,\\,200)$\n    - $p_2 = (250,\\,-120,\\,30,\\,-200)$\n    - $p_3 = (180,\\,50,\\,-80,\\,100)$\n  - Internal masses: $[0.0,\\,91.1876,\\,0.0,\\,173.0]$\n- Test Case 2 (near-collinear configuration, expected to prefer OPP):\n  - External momenta:\n    - $p_1 = (300,\\,0,\\,0,\\,300)$\n    - $p_2 = (303,\\,1.5,\\,0,\\,303)$\n    - $p_3 = (50,\\,-0.2,\\,0,\\,50)$\n  - Internal masses: $[0.0,\\,0.0,\\,0.0,\\,0.0]$\n- Test Case 3 (borderline stability case):\n  - External momenta:\n    - $p_1 = (100,\\,0,\\,0,\\,100)$\n    - $p_2 = (100,\\,0.5,\\,0,\\,99.5)$\n    - $p_3 = (120,\\,-40,\\,0,\\,110)$\n  - Internal masses: $[80.379,\\,0.0,\\,0.0,\\,4.18]$\n- Test Case 4 (two-point-like topology: single independent external momentum):\n  - External momenta:\n    - $p_1 = (500,\\,0,\\,0,\\,0)$\n  - Internal masses: $[0.0,\\,125.1]$\n\nFor each test case, output an integer decision code:\n- Output $1$ if PV reduction is selected.\n- Output $0$ if OPP reduction is selected.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[1,0,1,1]\"). There must be no extra text. All calculations are in GeV and $\\text{GeV}^2$ as appropriate, and the final outputs are unitless integers. Ensure numerical robustness in the presence of small determinants by following the normalization described above.", "solution": "The user-provided problem statement is subjected to validation.\n\n### Step 1: Extract Givens\n- **Task**: Design and implement a runtime decision criterion to select between Ossola–Papadopoulos–Pittau (OPP) and Passarino–Veltman (PV) one-loop integral reduction methods.\n- **Basis for Decision**: The behavior of Baikov representation (BR) minors, which are related to Gram determinants.\n- **Physics Context**: One-loop $N$-point tensor integral in four-dimensional spacetime with Minkowski signature $(+,-,-,-)$. The scalar product is $p \\cdot q = p^0 q^0 - \\vec{p}\\cdot\\vec{q}$.\n- **Input**:\n    - A set of $r$ independent external four-momenta $\\{p_i\\}_{i=1}^r$ with $r \\le 4$, where components are in GeV.\n    - A set of internal masses $\\{m_j\\}$ in GeV.\n- **Algorithm Steps**:\n    1.  **Construct Gram Matrix**: $G \\in \\mathbb{R}^{r \\times r}$ with entries $G_{ij} = p_i \\cdot p_j$.\n    2.  **Define Kinematic Scale**: $s_0 = \\max\\left(\\max_{1\\le i,j\\le r} |G_{ij}|,\\; 1 \\right)$ in $\\text{GeV}^2$.\n    3.  **Define Global Stability Score**:\n        - For each non-empty index subset $S \\subseteq \\{1,\\dots,r\\}$ of size $|S|=k$, form the principal submatrix $G_S$ and its determinant $\\Delta_S = \\det(G_S)$.\n        - Define the normalized minor $\\widehat{\\Delta}_S = \\frac{|\\Delta_S|}{s_0^{k}}$.\n        - The global stability score is $\\mu = \\min_{S\\neq \\emptyset} \\left(\\widehat{\\Delta}_S\\right)^{1/k}$.\n    4.  **Define Additional Metrics**:\n        - Normalized full Gram determinant: $\\widehat{\\Delta}_{\\text{full}} = \\frac{|\\det(G)|}{s_0^{r}}$.\n        - Condition number: $\\kappa = \\frac{\\sigma_{\\max}(G)}{\\sigma_{\\min}(G)}$, where $\\sigma_{\\max}$ and $\\sigma_{\\min}$ are the largest and smallest singular values of $G$. For $r=1$, $\\kappa = 1$.\n    5.  **Implement Decision Rule**: Choose PV if $\\mu \\ge \\tau$ AND $\\widehat{\\Delta}_{\\text{full}} \\ge \\epsilon$ AND $\\kappa \\le \\kappa_{\\max}$. Otherwise, choose OPP.\n_   **Thresholds**:\n        - $\\tau = 10^{-5}$\n        - $\\epsilon = 10^{-8}$\n        - $\\kappa_{\\max} = 10^{12}$\n- **Output**: An integer decision code: $1$ for PV, $0$ for OPP.\n- **Test Cases**:\n    - **Case 1**: $p_1 = (200,\\,0,\\,0,\\,200)$, $p_2 = (250,\\,-120,\\,30,\\,-200)$, $p_3 = (180,\\,50,\\,-80,\\,100)$; masses: $[0.0,\\,91.1876,\\,0.0,\\,173.0]$.\n    - **Case 2**: $p_1 = (300,\\,0,\\,0,\\,300)$, $p_2 = (303,\\,1.5,\\,0,\\,303)$, $p_3 = (50,\\,-0.2,\\,0,\\,50)$; masses: $[0.0,\\,0.0,\\,0.0,\\,0.0]$.\n    - **Case 3**: $p_1 = (100,\\,0,\\,0,\\,100)$, $p_2 = (100,\\,0.5,\\,0,\\,99.5)$, $p_3 = (120,\\,-40,\\,0,\\,110)$; masses: $[80.379,\\,0.0,\\,0.0,\\,4.18]$.\n    - **Case 4**: $p_1 = (500,\\,0,\\,0,\\,0)$; masses: $[0.0,\\,125.1]$.\n- **Final Output Format**: A comma-separated list of integer results in square brackets, e.g., `[1,0,1,1]`.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is evaluated against the validation criteria.\n\n- **Scientifically Grounded**: The problem statement is firmly rooted in the established principles of computational high-energy physics, specifically the theory of one-loop integral reduction. The concepts of Passarino-Veltman reduction, OPP reduction, Gram determinants, and their connection to kinematic singularities are standard and factually correct. The use of a Minkowski metric with signature $(+,-,-,-)$ is a standard convention.\n- **Well-Posed**: The problem is well-posed. It provides a complete, deterministic algorithm with clearly defined inputs (momenta), intermediate quantities ($\\mu$, $\\widehat{\\Delta}_{\\text{full}}$, $\\kappa$), and a precise decision rule based on fixed thresholds. The existence of a unique solution for any given input is guaranteed by the deterministic nature of the algorithm. The instruction to assume the independence of external momenta removes a potential ambiguity. The internal masses are provided but are not used in the decision logic; this is an acceptable simplification, as the criterion is explicitly based on external kinematics.\n- **Objective**: The problem is stated in objective, formal language. The decision criterion is purely quantitative, depending on calculated metrics and pre-defined, dimensionless thresholds. There is no room for subjective interpretation.\n\nBased on this analysis, the problem does not exhibit any of the listed flaws (scientific unsoundness, non-formalizability, incompleteness, contradiction, infeasibility, ill-posedness, triviality, or unverifiability).\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A solution will be provided.\n\n---\n\nThe objective is to formulate and implement a decision criterion for choosing between two methods for reducing one-loop tensor integrals in high-energy physics: the Passarino-Veltman (PV) method and the Ossola–Papadopoulos–Pittau (OPP) method. The choice is governed by the numerical stability of the kinematic configuration, which is assessed using determinants of the Gram matrix constructed from external momenta.\n\nThe fundamental structure of a one-loop integral's complexity is encoded in its dependence on the external four-momenta $\\{p_i\\}$. The PV reduction technique systematically decomposes tensor integrals into a basis of scalar integrals. This procedure involves inverting a Gram matrix $G$ whose elements are scalar products of the external momenta, $G_{ij} = p_i \\cdot p_j$. When the external momenta are nearly linearly dependent (e.g., in collinear or soft configurations), the determinant of the Gram matrix, $\\det(G)$, approaches zero. This leads to numerical instability in the PV procedure due to large cancellations between terms. The OPP method, which operates at the integrand level before integration, is generally more robust in such singular kinematic regions.\n\nThe proposed algorithm quantifies this \"near-singularity\" to make a robust choice. The steps are as follows:\n\n**1. Gram Matrix Construction**\nGiven a set of $r$ external four-momenta $\\{p_1, \\dots, p_r\\}$, where each $p_i = (p_i^0, p_i^1, p_i^2, p_i^3)$, we first construct the $r \\times r$ Gram matrix $G$. Its elements are the Minkowski scalar products:\n$$\nG_{ij} = p_i \\cdot p_j = p_i^0 p_j^0 - p_i^1 p_j^1 - p_i^2 p_j^2 - p_i^3 p_j^3\n$$\nThis matrix contains all the geometric information about the external state.\n\n**2. Normalization via Kinematic Scale**\nNumerical stability is sensitive to the overall energy scale of the process. To create a dimensionless and scale-invariant measure of stability, we define a characteristic kinematic scale $s_0$:\n$$\ns_0 = \\max\\left(\\max_{1\\le i,j\\le r} |G_{ij}|,\\; 1 \\right)\n$$\nThe value $1$ is included to handle cases where all $|G_{ij}|$ are less than $1$, preventing artificial inflation of normalized quantities. All determinants will be normalized by appropriate powers of $s_0$.\n\n**3. Global Stability Score $\\mu$**\nKinematic singularities can arise not only from the full set of momenta becoming linearly dependent but also from any subset. For example, in a four-point function, a three-point sub-topology might be planar, or a two-point sub-topology might involve collinear momenta. The Baikov representation of loop integrals makes it clear that the principal minors of the Gram matrix, $\\det(G_S)$, where $G_S$ is the Gram matrix for a subset of momenta $S$, are the relevant quantities.\n\nTo detect any such potential instability, we examine all non-empty subsets of the external momenta. For each subset $S \\subseteq \\{1, \\dots, r\\}$ of size $k = |S|$, we compute the determinant of the corresponding principal submatrix, $\\Delta_S = \\det(G_S)$. We then form a normalized, dimensionless quantity:\n$$\n\\widehat{\\Delta}_S = \\frac{|\\Delta_S|}{s_0^k}\n$$\nTo combine these measures into a single score, we take a geometric-mean-like root and then find the minimum across all subsets. This minimum, $\\mu$, acts as a \"weakest link\" detector:\n$$\n\\mu = \\min_{S \\neq \\emptyset} \\left( \\widehat{\\Delta}_S \\right)^{1/k}\n$$\nA small value of $\\mu$ indicates that at least one subset of momenta forms a near-singular configuration, signaling a potential problem for PV reduction.\n\n**4. Additional Stability Metrics**\nWhile $\\mu$ is a sensitive probe, two other standard metrics provide complementary information:\n\n-   **Normalized Full Gram Determinant $\\widehat{\\Delta}_{\\text{full}}$**: This is a direct measure of the singularity of the full system of $r$ momenta. It is a special case of $\\widehat{\\Delta}_S$ where $S=\\{1, \\dots, r\\}$.\n    $$\n    \\widehat{\\Delta}_{\\text{full}} = \\frac{|\\det(G)|}{s_0^r}\n    $$\n-   **Condition Number $\\kappa$**: From numerical linear algebra, the condition number of a matrix measures the sensitivity of the solution of a system of linear equations to errors in the data. For the Gram matrix $G$, a large condition number indicates that $G$ is close to being singular (non-invertible). It is formally defined as the ratio of the largest to the smallest singular value of $G$:\n    $$\n    \\kappa = \\frac{\\sigma_{\\max}(G)}{\\sigma_{\\min}(G)}\n    $$\n    In the special case of a single external momentum ($r=1$), $G$ is a $1 \\times 1$ matrix, and we define $\\kappa = 1$.\n\n**5. The Decision Rule**\nThe final decision is made by simultaneously checking all three metrics against predefined dimensionless thresholds ($\\tau$, $\\epsilon$, $\\kappa_{\\max}$). The PV method is deemed safe only if the kinematic configuration is sufficiently regular in all aspects.\n\n-   Choose PV (output $1$) if:\n    $$\n    \\mu \\ge \\tau \\quad \\text{AND} \\quad \\widehat{\\Delta}_{\\text{full}} \\ge \\epsilon \\quad \\text{AND} \\quad \\kappa \\le \\kappa_{\\max}\n    $$\n-   Otherwise, choose the more robust OPP method (output $0$).\n\nThe thresholds are set to $\\tau = 10^{-5}$, $\\epsilon = 10^{-8}$, and $\\kappa_{\\max} = 10^{12}$. The PV method is the \"conditional\" choice, while OPP is the default \"safe\" fallback. This logic ensures that any sign of kinematic degeneracy triggers a switch to the more stable algorithm. The internal masses $\\{m_j\\}$ do not enter this particular kinematic stability criterion, although they are crucial for other aspects of the integral evaluation.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom itertools import combinations\n\n# from scipy is not required as numpy covers all needs.\n\ndef solve():\n    \"\"\"\n    Main function to run the decision criterion on all test cases.\n    \"\"\"\n\n    # --- Problem Constants ---\n    TAU = 1e-5\n    EPSILON = 1e-8\n    KAPPA_MAX = 1e12\n\n    # --- Test Cases ---\n    # Each case is a tuple: (list_of_momenta, list_of_masses)\n    # Momenta are lists/tuples of [p0, p1, p2, p3] in GeV.\n    # Masses are in GeV, but not used by this decision logic.\n    test_cases = [\n        # Test Case 1 (general well-separated kinematics, expected PV-safe)\n        (\n            [\n                (200.0, 0.0, 0.0, 200.0),\n                (250.0, -120.0, 30.0, -200.0),\n                (180.0, 50.0, -80.0, 100.0),\n            ],\n            [0.0, 91.1876, 0.0, 173.0],\n        ),\n        # Test Case 2 (near-collinear configuration, expected to prefer OPP)\n        (\n            [\n                (300.0, 0.0, 0.0, 300.0),\n                (303.0, 1.5, 0.0, 303.0),\n                (50.0, -0.2, 0.0, 50.0),\n            ],\n            [0.0, 0.0, 0.0, 0.0],\n        ),\n        # Test Case 3 (borderline stability case)\n        (\n            [\n                (100.0, 0.0, 0.0, 100.0),\n                (100.0, 0.5, 0.0, 99.5),\n                (120.0, -40.0, 0.0, 110.0)\n            ],\n            [80.379, 0.0, 0.0, 4.18],\n        ),\n        # Test Case 4 (two-point-like topology: single independent external momentum)\n        (\n            [(500.0, 0.0, 0.0, 0.0)],\n            [0.0, 125.1],\n        ),\n    ]\n\n    results = []\n    for momenta, _ in test_cases: # Internal masses are not used\n        decision = select_reduction_method(momenta, TAU, EPSILON, KAPPA_MAX)\n        results.append(decision)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\n\ndef minkowski_dot(p1, p2):\n    \"\"\"\n    Computes the Minkowski dot product with signature (+, -, -, -).\n    \"\"\"\n    return p1[0] * p2[0] - p1[1] * p2[1] - p1[2] * p2[2] - p1[3] * p2[3]\n\ndef select_reduction_method(momenta, tau, epsilon, kappa_max):\n    \"\"\"\n    Implements the decision criterion for a single kinematic configuration.\n    \n    Args:\n        momenta (list of tuples/lists): The set of external four-momenta.\n        tau (float): Threshold for the global stability score.\n        epsilon (float): Threshold for the normalized full Gram determinant.\n        kappa_max (float): Threshold for the condition number.\n\n    Returns:\n        int: 1 for PV reduction, 0 for OPP reduction.\n    \"\"\"\n    momenta_np = [np.array(p, dtype=np.float64) for p in momenta]\n    r = len(momenta_np)\n\n    if r == 0:\n        return 0 # Default to OPP for trivial case\n\n    # 1. Construct the Gram matrix G\n    gram_matrix = np.zeros((r, r), dtype=np.float64)\n    for i in range(r):\n        for j in range(i, r):\n            dot_product = minkowski_dot(momenta_np[i], momenta_np[j])\n            gram_matrix[i, j] = dot_product\n            gram_matrix[j, i] = dot_product\n\n    # 2. Define the kinematic scale s0\n    s0 = np.max(np.abs(gram_matrix))\n    if s0 < 1.0:\n        s0 = 1.0\n\n    # 3. Compute the global stability score mu\n    mu = float('inf')\n    indices = range(r)\n    for k in range(1, r + 1):\n        for subset_indices in combinations(indices, k):\n            sub_matrix = gram_matrix[np.ix_(subset_indices, subset_indices)]\n            det_S = np.linalg.det(sub_matrix)\n            \n            # Avoid division by zero if s0 is extremely small (although prevented by max(..., 1))\n            norm_det_S = np.abs(det_S) / (s0**k)\n            \n            subset_score = norm_det_S**(1.0 / k)\n            if subset_score < mu:\n                mu = subset_score\n\n    # 4. Compute additional metrics: normalized full determinant and condition number\n    det_full = np.linalg.det(gram_matrix)\n    norm_det_full = np.abs(det_full) / (s0**r)\n\n    if r == 1:\n        kappa = 1.0\n    else:\n        # np.linalg.cond handles singular matrices by returning inf\n        kappa = np.linalg.cond(gram_matrix)\n\n    # 5. Apply the decision rule\n    if mu >= tau and norm_det_full >= epsilon and kappa <= kappa_max:\n        return 1 # Choose PV\n    else:\n        return 0 # Choose OPP\n\n# Execute the main function\nsolve()\n```", "id": "3525555"}, {"introduction": "After a tensor reduction is performed, often by solving a large linear system, it is crucial to have a measure of confidence in the resulting coefficients. This final practice [@problem_id:3525516] focuses on *a posteriori* error estimation, a powerful technique for quality control. By deriving and implementing a residual-based error bound, you will learn how to assess the accuracy of your numerical solution without knowing the exact answer, providing a vital check on the stability and reliability of the reduction procedure.", "problem": "Consider a one-loop tensor reduction where the numerator is represented in a basis of scalar master integrals. After projection onto this basis at a finite set of conditions (for example, tensor structures or sampling configurations), the coefficient vector $\\mathbf{c} \\in \\mathbb{R}^{m}$ of scalar master integrals is determined by solving a linear system $\\mathbf{A}\\mathbf{c} = \\mathbf{b}$, where $\\mathbf{A} \\in \\mathbb{R}^{m \\times m}$ is a coefficient matrix assembled from the projection, and $\\mathbf{b} \\in \\mathbb{R}^{m}$ contains the projected numerator data. Suppose a numerical procedure produces an approximate coefficient vector $\\hat{\\mathbf{c}}$.\n\nStarting from the identity $\\mathbf{A}(\\mathbf{c}^{\\star} - \\hat{\\mathbf{c}}) = \\mathbf{b} - \\mathbf{A}\\hat{\\mathbf{c}}$ and from the definition of the induced matrix norm, derive a residual-based estimator that bounds the coefficient error $\\lVert \\mathbf{c}^{\\star} - \\hat{\\mathbf{c}} \\rVert_{2}$ using the operator norm $\\lVert \\mathbf{A}^{-1} \\rVert_{2}$ and the residual norm $\\lVert \\mathbf{b} - \\mathbf{A}\\hat{\\mathbf{c}} \\rVert_{2}$. Then, considering the scalar amplitude reconstructed from scalar master integrals as $a = \\mathbf{s}^{\\top}\\mathbf{c}$ for a given weight vector $\\mathbf{s} \\in \\mathbb{R}^{m}$ (encoding the numerical values of scalar master integrals in specified kinematics), derive a corresponding bound for the amplitude error $\\lvert \\mathbf{s}^{\\top}\\hat{\\mathbf{c}} - \\mathbf{s}^{\\top}\\mathbf{c}^{\\star} \\rvert$ in terms of $\\lVert \\mathbf{s} \\rVert_{2}$ and the residual-based coefficient error estimator.\n\nImplement a complete program that:\n- Uses the spectral norm (induced $2$-norm) for vectors and matrices.\n- Computes $\\lVert \\mathbf{A}^{-1} \\rVert_{2}$ as the reciprocal of the smallest singular value of $\\mathbf{A}$.\n- For each test case, constructs the residual $\\mathbf{r} = \\mathbf{b} - \\mathbf{A}\\hat{\\mathbf{c}}$, computes the coefficient error estimator and the amplitude error estimator, and checks them against the actual errors using a tolerance of $\\tau = 10^{-12}$.\n- Reports, for each test case, two boolean results:\n  1. Whether $\\lVert \\hat{\\mathbf{c}} - \\mathbf{c}^{\\star} \\rVert_{2} \\le \\lVert \\mathbf{A}^{-1} \\rVert_{2}\\,\\lVert \\mathbf{r} \\rVert_{2} + \\tau$.\n  2. Whether $\\lvert \\mathbf{s}^{\\top}(\\hat{\\mathbf{c}} - \\mathbf{c}^{\\star}) \\rvert \\le \\lVert \\mathbf{s} \\rVert_{2}\\,\\lVert \\mathbf{A}^{-1} \\rVert_{2}\\,\\lVert \\mathbf{r} \\rVert_{2} + \\tau$.\n\nUse the following test suite. For each case, compute $\\mathbf{b} = \\mathbf{A}\\mathbf{c}^{\\star}$, then use the specified $\\hat{\\mathbf{c}}$ and $\\mathbf{s}$.\n\n- Test case $1$ (well-conditioned, small perturbation):\n  - $\\mathbf{A}_{1} = \\begin{bmatrix} 3 & 1 & 0 \\\\ 1 & 2 & 1 \\\\ 0 & 1 & 2 \\end{bmatrix}$,\n    $\\mathbf{c}^{\\star}_{1} = \\begin{bmatrix} 1 \\\\ -2 \\\\ 0.5 \\end{bmatrix}$,\n    $\\hat{\\mathbf{c}}_{1} = \\mathbf{c}^{\\star}_{1} + \\begin{bmatrix} 10^{-6} \\\\ -2\\times 10^{-6} \\\\ 10^{-6} \\end{bmatrix}$,\n    $\\mathbf{s}_{1} = \\begin{bmatrix} 0.7 \\\\ -1.3 \\\\ 0.2 \\end{bmatrix}$.\n- Test case $2$ (ill-conditioned diagonal):\n  - $\\mathbf{A}_{2} = \\mathrm{diag}(1, 10^{-6}, 10^{-9})$,\n    $\\mathbf{c}^{\\star}_{2} = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix}$,\n    $\\hat{\\mathbf{c}}_{2} = \\mathbf{c}^{\\star}_{2} + \\begin{bmatrix} 10^{-10} \\\\ -10^{-10} \\\\ 10^{-10} \\end{bmatrix}$,\n    $\\mathbf{s}_{2} = \\begin{bmatrix} 1 \\\\ -2 \\\\ 3 \\end{bmatrix}$.\n- Test case $3$ (exact reconstruction):\n  - $\\mathbf{A}_{3} = \\begin{bmatrix} 2 & -1 \\\\ -1 & 2 \\end{bmatrix}$,\n    $\\mathbf{c}^{\\star}_{3} = \\begin{bmatrix} 0.3 \\\\ -0.4 \\end{bmatrix}$,\n    $\\hat{\\mathbf{c}}_{3} = \\mathbf{c}^{\\star}_{3}$,\n    $\\mathbf{s}_{3} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$.\n- Test case $4$ (nearly singular with small off-diagonal perturbations):\n  - $\\mathbf{A}_{4} = \\begin{bmatrix} 1 & 1 & 1 \\\\ 1 & 1 + 10^{-12} & 1 \\\\ 1 & 1 & 1 + 10^{-12} \\end{bmatrix}$,\n    $\\mathbf{c}^{\\star}_{4} = \\begin{bmatrix} 0.2 \\\\ -0.1 \\\\ 0.05 \\end{bmatrix}$,\n    $\\hat{\\mathbf{c}}_{4} = \\mathbf{c}^{\\star}_{4} + \\begin{bmatrix} 10^{-8} \\\\ -2\\times 10^{-8} \\\\ 3\\times 10^{-8} \\end{bmatrix}$,\n    $\\mathbf{s}_{4} = \\begin{bmatrix} 2 \\\\ -0.5 \\\\ 0.3 \\end{bmatrix}$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain, for each test case in order from $1$ to $4$, first the boolean for the coefficient error bound validity and then the boolean for the amplitude error bound validity. Therefore, the output must have $8$ boolean entries in total, e.g., $[\\mathrm{True},\\mathrm{True},\\dots]$.", "solution": "The problem requires the derivation of two residual-based error estimators and their numerical verification. The first estimator bounds the error in the coefficient vector of scalar master integrals, and the second bounds the corresponding error in the reconstructed scalar amplitude. The context is the solution of a linear system $\\mathbf{A}\\mathbf{c} = \\mathbf{b}$, where $\\mathbf{c}^{\\star}$ is the exact solution and $\\hat{\\mathbf{c}}$ is a numerically obtained approximation. All matrix and vector norms are specified as the $2$-norm (Euclidean norm for vectors, spectral norm for matrices).\n\nFirst, we derive the bound for the coefficient error, $\\lVert \\hat{\\mathbf{c}} - \\mathbf{c}^{\\star} \\rVert_{2}$. We begin with the provided identity, which relates the error in the solution to the residual of the approximate solution. Let the exact solution $\\mathbf{c}^{\\star}$ satisfy $\\mathbf{A}\\mathbf{c}^{\\star} = \\mathbf{b}$. The residual vector $\\mathbf{r}$ for the approximate solution $\\hat{\\mathbf{c}}$ is defined as $\\mathbf{r} = \\mathbf{b} - \\mathbf{A}\\hat{\\mathbf{c}}$. Substituting $\\mathbf{b} = \\mathbf{A}\\mathbf{c}^{\\star}$ into the definition of the residual gives:\n$$ \\mathbf{r} = \\mathbf{A}\\mathbf{c}^{\\star} - \\mathbf{A}\\hat{\\mathbf{c}} = \\mathbf{A}(\\mathbf{c}^{\\star} - \\hat{\\mathbf{c}}) $$\nAssuming the matrix $\\mathbf{A}$ is non-singular, its inverse $\\mathbf{A}^{-1}$ exists. We can thus solve for the coefficient error vector, here denoted $\\mathbf{e}_{\\mathbf{c}} = \\mathbf{c}^{\\star} - \\hat{\\mathbf{c}}$:\n$$ \\mathbf{e}_{\\mathbf{c}} = \\mathbf{A}^{-1}\\mathbf{r} $$\nTo find a bound on the magnitude of the error, we take the $2$-norm of both sides:\n$$ \\lVert \\mathbf{e}_{\\mathbf{c}} \\rVert_{2} = \\lVert \\mathbf{A}^{-1}\\mathbf{r} \\rVert_{2} $$\nBy the definition of an induced matrix norm, we have the property $\\lVert \\mathbf{M}\\mathbf{x} \\rVert \\le \\lVert \\mathbf{M} \\rVert \\lVert \\mathbf{x} \\rVert$. Applying this inequality with the matrix $\\mathbf{A}^{-1}$ and the vector $\\mathbf{r}$ for the $2$-norm yields:\n$$ \\lVert \\mathbf{A}^{-1}\\mathbf{r} \\rVert_{2} \\le \\lVert \\mathbf{A}^{-1} \\rVert_{2} \\lVert \\mathbf{r} \\rVert_{2} $$\nCombining these results, we arrive at the desired error bound. Since $\\lVert \\hat{\\mathbf{c}} - \\mathbf{c}^{\\star} \\rVert_{2} = \\lVert -(\\mathbf{c}^{\\star} - \\hat{\\mathbf{c}}) \\rVert_{2} = \\lVert \\mathbf{c}^{\\star} - \\hat{\\mathbf{c}} \\rVert_{2} = \\lVert \\mathbf{e}_{\\mathbf{c}} \\rVert_{2}$, the inequality is:\n$$ \\lVert \\hat{\\mathbf{c}} - \\mathbf{c}^{\\star} \\rVert_{2} \\le \\lVert \\mathbf{A}^{-1} \\rVert_{2} \\lVert \\mathbf{r} \\rVert_{2} = \\lVert \\mathbf{A}^{-1} \\rVert_{2} \\lVert \\mathbf{b} - \\mathbf{A}\\hat{\\mathbf{c}} \\rVert_{2} $$\nThis inequality establishes a direct relationship between the norm of the residual $\\mathbf{r}$ (a computable quantity) and the norm of the actual error $\\hat{\\mathbf{c}} - \\mathbf{c}^{\\star}$ (which is generally not computable without knowing $\\mathbf{c}^{\\star}$). The factor $\\lVert \\mathbf{A}^{-1} \\rVert_{2}$ acts as an amplification factor; for ill-conditioned matrices, this term can be very large, meaning a small residual does not guarantee a small error.\n\nSecond, we derive the bound for the amplitude error, $\\lvert \\mathbf{s}^{\\top}\\hat{\\mathbf{c}} - \\mathbf{s}^{\\top}\\mathbf{c}^{\\star} \\rvert$. The scalar amplitude $a$ is given by a linear combination of the coefficients, $a = \\mathbf{s}^{\\top}\\mathbf{c}$. The error in the amplitude, $e_{a}$, is:\n$$ e_{a} = \\lvert \\mathbf{s}^{\\top}\\hat{\\mathbf{c}} - \\mathbf{s}^{\\top}\\mathbf{c}^{\\star} \\rvert = \\lvert \\mathbf{s}^{\\top}(\\hat{\\mathbf{c}} - \\mathbf{c}^{\\star}) \\rvert $$\nRecognizing that $\\hat{\\mathbf{c}} - \\mathbf{c}^{\\star} = -\\mathbf{e}_{\\mathbf{c}} = -(\\mathbf{c}^{\\star} - \\hat{\\mathbf{c}})$, we can write:\n$$ e_{a} = \\lvert \\mathbf{s}^{\\top}(-\\mathbf{e}_{\\mathbf{c}}) \\rvert = \\lvert -\\mathbf{s}^{\\top}\\mathbf{e}_{\\mathbf{c}} \\rvert = \\lvert \\mathbf{s}^{\\top}\\mathbf{e}_{\\mathbf{c}} \\rvert $$\nThe expression $\\mathbf{s}^{\\top}\\mathbf{e}_{\\mathbf{c}}$ is the standard dot product between the vectors $\\mathbf{s}$ and $\\mathbf{e}_{\\mathbf{c}}$. We can apply the Cauchy-Schwarz inequality, $\\lvert \\mathbf{u}^{\\top}\\mathbf{v} \\rvert \\le \\lVert \\mathbf{u} \\rVert_{2} \\lVert \\mathbf{v} \\rVert_{2}$, with $\\mathbf{u} = \\mathbf{s}$ and $\\mathbf{v} = \\mathbf{e}_{\\mathbf{c}}$:\n$$ \\lvert \\mathbf{s}^{\\top}\\mathbf{e}_{\\mathbf{c}} \\rvert \\le \\lVert \\mathbf{s} \\rVert_{2} \\lVert \\mathbf{e}_{\\mathbf{c}} \\rVert_{2} $$\nNow, we substitute the bound we previously derived for $\\lVert \\mathbf{e}_{\\mathbf{c}} \\rVert_{2}$:\n$$ e_{a} \\le \\lVert \\mathbf{s} \\rVert_{2} (\\lVert \\mathbf{A}^{-1} \\rVert_{2} \\lVert \\mathbf{r} \\rVert_{2}) $$\nThus, the final bound on the amplitude error is:\n$$ \\lvert \\mathbf{s}^{\\top}(\\hat{\\mathbf{c}} - \\mathbf{c}^{\\star}) \\rvert \\le \\lVert \\mathbf{s} \\rVert_{2} \\lVert \\mathbf{A}^{-1} \\rVert_{2} \\lVert \\mathbf{b} - \\mathbf{A}\\hat{\\mathbf{c}} \\rVert_{2} $$\nThis result shows that the error in the final physical quantity (the amplitude) is bounded by the same residual-based estimator, scaled by the norm of the weight vector $\\mathbf{s}$.\n\nFor implementation, we follow the problem's instructions. The spectral norm of the inverse, $\\lVert \\mathbf{A}^{-1} \\rVert_{2}$, is calculated as the reciprocal of the smallest singular value of $\\mathbf{A}$, i.e., $1/\\sigma_{\\min}(\\mathbf{A})$. The singular values are obtained using singular value decomposition (SVD). All vector norms are computed as the standard Euclidean ($L_2$) norm. For each test case, the program constructs the relevant vectors and matrices, computes the exact right-hand side $\\mathbf{b} = \\mathbf{A}\\mathbf{c}^{\\star}$, the residual $\\mathbf{r} = \\mathbf{b} - \\mathbf{A}\\hat{\\mathbf{c}}$, and the error estimators. It then compares the actual errors against the estimators, including the specified floating-point tolerance $\\tau = 10^{-12}$, to produce the required boolean results.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Derives and verifies residual-based error estimators for a linear system\n    arising in one-loop tensor reduction.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"A\": np.array([[3.0, 1.0, 0.0], [1.0, 2.0, 1.0], [0.0, 1.0, 2.0]]),\n            \"c_star\": np.array([1.0, -2.0, 0.5]),\n            \"c_hat\": np.array([1.0, -2.0, 0.5]) + np.array([1e-6, -2e-6, 1e-6]),\n            \"s\": np.array([0.7, -1.3, 0.2]),\n        },\n        {\n            \"A\": np.diag([1.0, 1e-6, 1e-9]),\n            \"c_star\": np.array([1.0, 1.0, 1.0]),\n            \"c_hat\": np.array([1.0, 1.0, 1.0]) + np.array([1e-10, -1e-10, 1e-10]),\n            \"s\": np.array([1.0, -2.0, 3.0]),\n        },\n        {\n            \"A\": np.array([[2.0, -1.0], [-1.0, 2.0]]),\n            \"c_star\": np.array([0.3, -0.4]),\n            \"c_hat\": np.array([0.3, -0.4]),\n            \"s\": np.array([1.0, 1.0]),\n        },\n        {\n            \"A\": np.array([[1.0, 1.0, 1.0], \n                           [1.0, 1.0 + 1e-12, 1.0], \n                           [1.0, 1.0, 1.0 + 1e-12]]),\n            \"c_star\": np.array([0.2, -0.1, 0.05]),\n            \"c_hat\": np.array([0.2, -0.1, 0.05]) + np.array([1e-8, -2e-8, 3e-8]),\n            \"s\": np.array([2.0, -0.5, 0.3]),\n        },\n    ]\n\n    tau = 1e-12\n    results = []\n    \n    for case in test_cases:\n        A = case[\"A\"]\n        c_star = case[\"c_star\"]\n        c_hat = case[\"c_hat\"]\n        s = case[\"s\"]\n\n        # b is determined from the exact solution c_star\n        b = A @ c_star\n        \n        # Residual vector r = b - A*c_hat\n        r = b - (A @ c_hat)\n        \n        # Spectral norm of A_inv is 1 / smallest singular value of A\n        singular_values = np.linalg.svd(A, compute_uv=False)\n        norm_A_inv = 1.0 / np.min(singular_values)\n        \n        # L2 norm of the residual vector\n        norm_r = np.linalg.norm(r, 2)\n        \n        # --- Coefficient Error Check ---\n        \n        # Estimator for the coefficient error\n        coeff_error_estimator = norm_A_inv * norm_r\n        \n        # Actual coefficient error\n        actual_coeff_error = np.linalg.norm(c_hat - c_star, 2)\n        \n        # Check condition 1: actual error <= estimator + tolerance\n        # This is the test required by the problem:\n        # ||c_hat - c_star||_2 <= ||A_inv||_2 * ||r||_2 + tau\n        is_coeff_bound_valid = actual_coeff_error <= coeff_error_estimator + tau\n        results.append(is_coeff_bound_valid)\n\n        # --- Amplitude Error Check ---\n        \n        # L2 norm of the weight vector s\n        norm_s = np.linalg.norm(s, 2)\n\n        # Estimator for the amplitude error\n        amp_error_estimator = norm_s * coeff_error_estimator\n        \n        # Actual amplitude error\n        actual_amp_error = np.abs(s @ (c_hat - c_star))\n        \n        # Check condition 2: actual error <= estimator + tolerance\n        # This is the test required by the problem:\n        # |s^T * (c_hat - c_star)| <= ||s||_2 * ||A_inv||_2 * ||r||_2 + tau\n        is_amp_bound_valid = actual_amp_error <= amp_error_estimator + tau\n        results.append(is_amp_bound_valid)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3525516"}]}