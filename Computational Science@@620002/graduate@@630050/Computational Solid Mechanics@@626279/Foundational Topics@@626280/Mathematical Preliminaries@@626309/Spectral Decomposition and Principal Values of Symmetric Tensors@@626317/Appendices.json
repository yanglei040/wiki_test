{"hands_on_practices": [{"introduction": "A symmetric tensor can be characterized by quantities that remain unchanged regardless of the coordinate system used to describe it. This exercise connects two sets of such quantities: the principal invariants derived from the characteristic polynomial and the tensor's eigenvalues. By calculating both sets independently and verifying their equivalence, you will solidify your understanding of the deep connection between a tensor's algebraic form and its fundamental spectral properties [@problem_id:3601958].", "problem": "A second-order real symmetric tensor in computational solid mechanics admits an orthogonal spectral decomposition and possesses three principal invariants that can be expressed in terms of the coefficients of its characteristic polynomial. Starting from the characteristic polynomial $p_{\\boldsymbol{A}}(\\lambda)=\\det(\\lambda \\boldsymbol{I}-\\boldsymbol{A})$ and the fundamental relation between its coefficients and the elementary symmetric polynomials in the eigenvalues, derive the definitions of the principal invariants $I_1$, $I_2$, and $I_3$ for a real symmetric tensor. Then, for the specific tensor\n$$\n\\boldsymbol{A}=\\begin{pmatrix}2&-1&0\\\\-1&2&0\\\\0&0&3\\end{pmatrix},\n$$\ncompute the values of $I_1$, $I_2$, and $I_3$ using tensor operations, and independently obtain the eigenvalues of $\\boldsymbol{A}$ to verify that $I_1$ equals the sum of the eigenvalues, $I_2$ equals the sum of the pairwise products of the eigenvalues, and $I_3$ equals the product of the eigenvalues. Provide your final numerical values for $(I_1,I_2,I_3)$ as a single row matrix. No rounding is required.", "solution": "The problem requires the derivation of the principal invariants of a second-order real symmetric tensor, the calculation of these invariants for a specific tensor using tensor operations, and the verification of these values using the tensor's eigenvalues. The entire process must be validated and presented with rigorous mathematical formalism.\n\nThe problem statement is valid. It is scientifically grounded in the principles of linear algebra and continuum mechanics, well-posed with all necessary information provided, and stated with objective, formal language. It is a standard problem in the study of tensor analysis.\n\nLet $\\boldsymbol{A}$ be a second-order real symmetric tensor in a $3$-dimensional Euclidean space. Its components in a given orthonormal basis are represented by a $3 \\times 3$ symmetric matrix. The eigenvalues of $\\boldsymbol{A}$, denoted by $\\lambda_1, \\lambda_2, \\lambda_3$, are the roots of the characteristic polynomial $p_{\\boldsymbol{A}}(\\lambda)$. As per the problem definition, the characteristic polynomial is $p_{\\boldsymbol{A}}(\\lambda) = \\det(\\lambda \\boldsymbol{I} - \\boldsymbol{A})$, where $\\boldsymbol{I}$ is the second-order identity tensor.\n\nLet the matrix representation of $\\boldsymbol{A}$ be\n$$\n\\boldsymbol{A} = \\begin{pmatrix} A_{11} & A_{12} & A_{13} \\\\ A_{21} & A_{22} & A_{23} \\\\ A_{31} & A_{32} & A_{33} \\end{pmatrix}\n$$\nwhere $A_{ij} = A_{ji}$ due to symmetry.\n\nThe characteristic equation is $\\det(\\lambda \\boldsymbol{I} - \\boldsymbol{A}) = 0$.\n$$\n\\det\\begin{pmatrix} \\lambda - A_{11} & -A_{12} & -A_{13} \\\\ -A_{21} & \\lambda - A_{22} & -A_{23} \\\\ -A_{31} & -A_{32} & \\lambda - A_{33} \\end{pmatrix} = 0\n$$\nExpanding the determinant yields a cubic polynomial in $\\lambda$:\n$p_{\\boldsymbol{A}}(\\lambda) = \\lambda^3 - (A_{11} + A_{22} + A_{33})\\lambda^2 + \\left( (A_{11}A_{22} - A_{12}A_{21}) + (A_{22}A_{33} - A_{23}A_{32}) + (A_{33}A_{11} - A_{13}A_{31}) \\right)\\lambda - \\det(\\boldsymbol{A})$.\n\nSince $\\lambda_1, \\lambda_2, \\lambda_3$ are the roots of this polynomial, it can also be written in factored form:\n$p_{\\boldsymbol{A}}(\\lambda) = (\\lambda - \\lambda_1)(\\lambda - \\lambda_2)(\\lambda - \\lambda_3) = \\lambda^3 - (\\lambda_1 + \\lambda_2 + \\lambda_3)\\lambda^2 + (\\lambda_1\\lambda_2 + \\lambda_2\\lambda_3 + \\lambda_3\\lambda_1)\\lambda - \\lambda_1\\lambda_2\\lambda_3$.\n\nBy comparing the coefficients of the powers of $\\lambda$ in both expressions, we can define the three principal invariants ($I_1, I_2, I_3$) of the tensor $\\boldsymbol{A}$. These invariants are independent of the coordinate system chosen.\nThe first principal invariant, $I_1$, is the coefficient of $-\\lambda^2$:\n$I_1 = A_{11} + A_{22} + A_{33} = \\text{tr}(\\boldsymbol{A}) = \\lambda_1 + \\lambda_2 + \\lambda_3$.\n\nThe second principal invariant, $I_2$, is the coefficient of $\\lambda$:\n$I_2 = (A_{11}A_{22} - A_{12}^2) + (A_{22}A_{33} - A_{23}^2) + (A_{33}A_{11} - A_{13}^2) = \\lambda_1\\lambda_2 + \\lambda_2\\lambda_3 + \\lambda_3\\lambda_1$.\nThis is the sum of the principal minors of $\\boldsymbol{A}$. A more convenient expression for $I_2$ can be derived:\n$I_2 = \\frac{1}{2} [(\\text{tr}(\\boldsymbol{A}))^2 - \\text{tr}(\\boldsymbol{A}^2)]$.\n\nThe third principal invariant, $I_3$, is the negative of the constant term:\n$I_3 = \\det(\\boldsymbol{A}) = \\lambda_1\\lambda_2\\lambda_3$.\n\nNow, we apply these definitions to the specific tensor provided:\n$$\n\\boldsymbol{A}=\\begin{pmatrix}2&-1&0\\\\-1&2&0\\\\0&0&3\\end{pmatrix}\n$$\n\nFirst, we compute the invariants using tensor operations.\nThe first invariant $I_1$ is the trace of $\\boldsymbol{A}$:\n$I_1 = \\text{tr}(\\boldsymbol{A}) = 2 + 2 + 3 = 7$.\n\nTo compute the second invariant $I_2$, we first need $\\boldsymbol{A}^2$:\n$$\n\\boldsymbol{A}^2 = \\boldsymbol{A}\\boldsymbol{A} = \\begin{pmatrix}2&-1&0\\\\-1&2&0\\\\0&0&3\\end{pmatrix} \\begin{pmatrix}2&-1&0\\\\-1&2&0\\\\0&0&3\\end{pmatrix} = \\begin{pmatrix} (2)(2)+(-1)(-1) & (2)(-1)+(-1)(2) & 0 \\\\ (-1)(2)+(2)(-1) & (-1)(-1)+(2)(2) & 0 \\\\ 0 & 0 & (3)(3) \\end{pmatrix} = \\begin{pmatrix}5&-4&0\\\\-4&5&0\\\\0&0&9\\end{pmatrix}\n$$\nThe trace of $\\boldsymbol{A}^2$ is:\n$\\text{tr}(\\boldsymbol{A}^2) = 5 + 5 + 9 = 19$.\nNow, we can compute $I_2$:\n$I_2 = \\frac{1}{2}[(\\text{tr}(\\boldsymbol{A}))^2 - \\text{tr}(\\boldsymbol{A}^2)] = \\frac{1}{2}[7^2 - 19] = \\frac{1}{2}[49 - 19] = \\frac{1}{2}[30] = 15$.\n\nThe third invariant $I_3$ is the determinant of $\\boldsymbol{A}$:\n$I_3 = \\det(\\boldsymbol{A}) = \\det\\begin{pmatrix}2&-1&0\\\\-1&2&0\\\\0&0&3\\end{pmatrix}$.\nExpanding along the third row:\n$I_3 = 3 \\times \\det\\begin{pmatrix}2&-1\\\\-1&2\\end{pmatrix} = 3 \\times ((2)(2) - (-1)(-1)) = 3 \\times (4 - 1) = 3 \\times 3 = 9$.\n\nSo, from tensor operations, the invariants are $(I_1, I_2, I_3) = (7, 15, 9)$.\n\nNext, we independently compute the eigenvalues of $\\boldsymbol{A}$ by solving the characteristic equation $\\det(\\boldsymbol{A} - \\lambda\\boldsymbol{I}) = 0$.\n$$\n\\det\\begin{pmatrix}2-\\lambda&-1&0\\\\-1&2-\\lambda&0\\\\0&0&3-\\lambda\\end{pmatrix} = 0\n$$\nExpanding along the third row:\n$(3-\\lambda) \\det\\begin{pmatrix}2-\\lambda&-1\\\\-1&2-\\lambda\\end{pmatrix} = 0$.\nThis gives one eigenvalue immediately: $\\lambda_3 = 3$.\nThe remaining eigenvalues are roots of the $2 \\times 2$ determinant:\n$(2-\\lambda)^2 - (-1)(-1) = 0$\n$(2-\\lambda)^2 - 1 = 0$\n$(2-\\lambda)^2 = 1$\n$2-\\lambda = \\pm 1$.\nThis yields two eigenvalues:\n$2 - \\lambda = 1 \\implies \\lambda_1 = 1$.\n$2 - \\lambda = -1 \\implies \\lambda_2 = 3$.\nThe set of eigenvalues of $\\boldsymbol{A}$ is $\\{\\lambda_1, \\lambda_2, \\lambda_3\\} = \\{1, 3, 3\\}$.\n\nFinally, we verify that these eigenvalues satisfy the invariant relations:\nSum of eigenvalues: $\\lambda_1 + \\lambda_2 + \\lambda_3 = 1 + 3 + 3 = 7$. This equals $I_1$.\nSum of pairwise products of eigenvalues: $\\lambda_1\\lambda_2 + \\lambda_1\\lambda_3 + \\lambda_2\\lambda_3 = (1)(3) + (1)(3) + (3)(3) = 3 + 3 + 9 = 15$. This equals $I_2$.\nProduct of eigenvalues: $\\lambda_1\\lambda_2\\lambda_3 = (1)(3)(3) = 9$. This equals $I_3$.\n\nThe verification is successful. The values of the principal invariants computed via tensor operations are consistent with the values derived from the eigenvalues. The final numerical values are $(I_1, I_2, I_3) = (7, 15, 9)$.", "answer": "$$\n\\boxed{\\begin{pmatrix} 7 & 15 & 9 \\end{pmatrix}}\n$$", "id": "3601958"}, {"introduction": "The spectral decomposition theorem provides more than just a method for finding eigenvalues; it offers a profound insight into the structure of a symmetric tensor as a weighted sum of its principal components. This practice reverses the typical analysis by asking you to reconstruct a tensor from a given set of principal values and directions. Completing this exercise will reinforce the physical and geometric meaning of the decomposition, viewing it as a recipe for building the whole from its fundamental parts [@problem_id:3601998].", "problem": "In a three-dimensional finite element simulation of an elastic solid, the local Cauchy stress tensor at a Gauss point is known to be symmetric. The stored result for the stress tensor is the symmetric second-order tensor $\\boldsymbol{A} \\in \\mathbb{R}^{3 \\times 3}$ with components\n$$\n\\boldsymbol{A} = \\begin{pmatrix}\n\\frac{7}{3} & \\frac{8}{3} & \\frac{5}{3} \\\\\n\\frac{8}{3} & \\frac{7}{3} & -\\frac{5}{3} \\\\\n\\frac{5}{3} & -\\frac{5}{3} & \\frac{4}{3}\n\\end{pmatrix}.\n$$\nIndependently, a principal value extraction tool reported three principal values (eigenvalues) and associated normalized principal directions (eigenvectors) for the same point:\n$$\n\\lambda_{1} = 5, \\quad \\boldsymbol{n}_{1} = \\frac{1}{\\sqrt{2}}\\begin{pmatrix}1 \\\\ 1 \\\\ 0\\end{pmatrix}, \\qquad\n\\lambda_{2} = 3, \\quad \\boldsymbol{n}_{2} = \\frac{1}{\\sqrt{6}}\\begin{pmatrix}1 \\\\ -1 \\\\ 2\\end{pmatrix}, \\qquad\n\\lambda_{3} = -2, \\quad \\boldsymbol{n}_{3} = \\frac{1}{\\sqrt{3}}\\begin{pmatrix}1 \\\\ -1 \\\\ -1\\end{pmatrix}.\n$$\nAll stress values are in megapascals (MPa). The principal directions are mutually orthonormal.\n\nStarting only from the defining properties of real symmetric tensors and orthonormal eigenbases, reconstruct a tensor $\\boldsymbol{B}$ from the given principal values and normalized principal directions, and verify consistency with $\\boldsymbol{A}$ by computing the Frobenius norm of the difference. Report the value\n$$\n\\left\\|\\boldsymbol{A} - \\boldsymbol{B}\\right\\|_{F}.\n$$\nExpress your answer in megapascals (MPa). If the numerical value is exact, do not round; otherwise, follow the instruction to round your answer to a stated number of significant figures if required. No rounding is required here.", "solution": "The problem requires the reconstruction of a symmetric tensor $\\boldsymbol{B}$ from its given principal values (eigenvalues) and principal directions (eigenvectors), and then to verify its consistency with a given tensor $\\boldsymbol{A}$ by computing the Frobenius norm of their difference.\n\nThe spectral theorem for a real symmetric tensor $\\boldsymbol{S} \\in \\mathbb{R}^{3 \\times 3}$ states that it can be diagonalized by an orthogonal transformation. This is equivalent to expressing the tensor as a sum of its eigenvalues $\\lambda_i$ weighted by the outer products of their corresponding normalized and mutually orthogonal eigenvectors $\\boldsymbol{n}_i$. The reconstruction formula is given by the spectral decomposition:\n$$\n\\boldsymbol{S} = \\sum_{i=1}^{3} \\lambda_i (\\boldsymbol{n}_i \\otimes \\boldsymbol{n}_i) = \\sum_{i=1}^{3} \\lambda_i \\boldsymbol{n}_i \\boldsymbol{n}_i^T\n$$\nwhere $\\boldsymbol{n}_i \\boldsymbol{n}_i^T$ is the outer product of the eigenvector $\\boldsymbol{n}_i$ with itself, resulting in a projection matrix.\n\nThe given principal values are $\\lambda_1 = 5$, $\\lambda_2 = 3$, and $\\lambda_3 = -2$. The corresponding normalized principal directions are:\n$$\n\\boldsymbol{n}_{1} = \\frac{1}{\\sqrt{2}}\\begin{pmatrix}1\\\\1\\\\0\\end{pmatrix}, \\quad \n\\boldsymbol{n}_{2} = \\frac{1}{\\sqrt{6}}\\begin{pmatrix}1\\\\-1\\\\2\\end{pmatrix}, \\quad \n\\boldsymbol{n}_{3} = \\frac{1}{\\sqrt{3}}\\begin{pmatrix}1\\\\-1\\\\-1\\end{pmatrix}\n$$\nWe will construct the tensor $\\boldsymbol{B}$ using these values. This involves computing each term $\\lambda_i (\\boldsymbol{n}_i \\otimes \\boldsymbol{n}_i)$ and summing them.\n\nFor the first principal value and direction ($\\lambda_1=5, \\boldsymbol{n}_1$):\n$$\n\\boldsymbol{n}_1 \\otimes \\boldsymbol{n}_1 = \\boldsymbol{n}_1 \\boldsymbol{n}_1^T = \\left(\\frac{1}{\\sqrt{2}}\\begin{pmatrix}1\\\\1\\\\0\\end{pmatrix}\\right) \\left(\\frac{1}{\\sqrt{2}}\\begin{pmatrix}1 & 1 & 0\\end{pmatrix}\\right) = \\frac{1}{2}\\begin{pmatrix} 1 & 1 & 0 \\\\ 1 & 1 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix}\n$$\n$$\n\\lambda_1 (\\boldsymbol{n}_1 \\otimes \\boldsymbol{n}_1) = 5 \\left( \\frac{1}{2}\\begin{pmatrix} 1 & 1 & 0 \\\\ 1 & 1 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix} \\right) = \\begin{pmatrix} \\frac{5}{2} & \\frac{5}{2} & 0 \\\\ \\frac{5}{2} & \\frac{5}{2} & 0 \\\\ 0 & 0 & 0 \\end{pmatrix}\n$$\nFor the second principal value and direction ($\\lambda_2=3, \\boldsymbol{n}_2$):\n$$\n\\boldsymbol{n}_2 \\otimes \\boldsymbol{n}_2 = \\boldsymbol{n}_2 \\boldsymbol{n}_2^T = \\left(\\frac{1}{\\sqrt{6}}\\begin{pmatrix}1\\\\-1\\\\2\\end{pmatrix}\\right) \\left(\\frac{1}{\\sqrt{6}}\\begin{pmatrix}1 & -1 & 2\\end{pmatrix}\\right) = \\frac{1}{6}\\begin{pmatrix} 1 & -1 & 2 \\\\ -1 & 1 & -2 \\\\ 2 & -2 & 4 \\end{pmatrix}\n$$\n$$\n\\lambda_2 (\\boldsymbol{n}_2 \\otimes \\boldsymbol{n}_2) = 3 \\left( \\frac{1}{6}\\begin{pmatrix} 1 & -1 & 2 \\\\ -1 & 1 & -2 \\\\ 2 & -2 & 4 \\end{pmatrix} \\right) = \\frac{1}{2}\\begin{pmatrix} 1 & -1 & 2 \\\\ -1 & 1 & -2 \\\\ 2 & -2 & 4 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2} & -\\frac{1}{2} & 1 \\\\ -\\frac{1}{2} & \\frac{1}{2} & -1 \\\\ 1 & -1 & 2 \\end{pmatrix}\n$$\nFor the third principal value and direction ($\\lambda_3=-2, \\boldsymbol{n}_3$):\n$$\n\\boldsymbol{n}_3 \\otimes \\boldsymbol{n}_3 = \\boldsymbol{n}_3 \\boldsymbol{n}_3^T = \\left(\\frac{1}{\\sqrt{3}}\\begin{pmatrix}1\\\\-1\\\\-1\\end{pmatrix}\\right) \\left(\\frac{1}{\\sqrt{3}}\\begin{pmatrix}1 & -1 & -1\\end{pmatrix}\\right) = \\frac{1}{3}\\begin{pmatrix} 1 & -1 & -1 \\\\ -1 & 1 & 1 \\\\ -1 & 1 & 1 \\end{pmatrix}\n$$\n$$\n\\lambda_3 (\\boldsymbol{n}_3 \\otimes \\boldsymbol{n}_3) = -2 \\left( \\frac{1}{3}\\begin{pmatrix} 1 & -1 & -1 \\\\ -1 & 1 & 1 \\\\ -1 & 1 & 1 \\end{pmatrix} \\right) = \\begin{pmatrix} -\\frac{2}{3} & \\frac{2}{3} & \\frac{2}{3} \\\\ \\frac{2}{3} & -\\frac{2}{3} & -\\frac{2}{3} \\\\ \\frac{2}{3} & -\\frac{2}{3} & -\\frac{2}{3} \\end{pmatrix}\n$$\nNow, we sum these three matrices to reconstruct $\\boldsymbol{B}$:\n$$\n\\boldsymbol{B} = \\lambda_1 (\\boldsymbol{n}_1 \\otimes \\boldsymbol{n}_1) + \\lambda_2 (\\boldsymbol{n}_2 \\otimes \\boldsymbol{n}_2) + \\lambda_3 (\\boldsymbol{n}_3 \\otimes \\boldsymbol{n}_3)\n$$\n$$\n\\boldsymbol{B} = \\begin{pmatrix} \\frac{5}{2} & \\frac{5}{2} & 0 \\\\ \\frac{5}{2} & \\frac{5}{2} & 0 \\\\ 0 & 0 & 0 \\end{pmatrix} + \\begin{pmatrix} \\frac{1}{2} & -\\frac{1}{2} & 1 \\\\ -\\frac{1}{2} & \\frac{1}{2} & -1 \\\\ 1 & -1 & 2 \\end{pmatrix} + \\begin{pmatrix} -\\frac{2}{3} & \\frac{2}{3} & \\frac{2}{3} \\\\ \\frac{2}{3} & -\\frac{2}{3} & -\\frac{2}{3} \\\\ \\frac{2}{3} & -\\frac{2}{3} & -\\frac{2}{3} \\end{pmatrix}\n$$\nSumming the components:\n$B_{11} = \\frac{5}{2} + \\frac{1}{2} - \\frac{2}{3} = 3 - \\frac{2}{3} = \\frac{9-2}{3} = \\frac{7}{3}$\n$B_{12} = \\frac{5}{2} - \\frac{1}{2} + \\frac{2}{3} = 2 + \\frac{2}{3} = \\frac{6+2}{3} = \\frac{8}{3}$\n$B_{13} = 0 + 1 + \\frac{2}{3} = \\frac{3+2}{3} = \\frac{5}{3}$\n$B_{22} = \\frac{5}{2} + \\frac{1}{2} - \\frac{2}{3} = 3 - \\frac{2}{3} = \\frac{7}{3}$\n$B_{23} = 0 - 1 - \\frac{2}{3} = \\frac{-3-2}{3} = -\\frac{5}{3}$\n$B_{33} = 0 + 2 - \\frac{2}{3} = \\frac{6-2}{3} = \\frac{4}{3}$\nSince the tensor must be symmetric, $B_{21}=B_{12}$, $B_{31}=B_{13}$, and $B_{32}=B_{23}$. Indeed, our calculation shows this.\nThe reconstructed tensor $\\boldsymbol{B}$ is:\n$$\n\\boldsymbol{B} = \\begin{pmatrix}\n\\frac{7}{3} & \\frac{8}{3} & \\frac{5}{3} \\\\\n\\frac{8}{3} & \\frac{7}{3} & -\\frac{5}{3} \\\\\n\\frac{5}{3} & -\\frac{5}{3} & \\frac{4}{3}\n\\end{pmatrix}\n$$\nThis is identical to the given stress tensor $\\boldsymbol{A}$:\n$$\n\\boldsymbol{A} = \\begin{pmatrix}\n\\frac{7}{3} & \\frac{8}{3} & \\frac{5}{3} \\\\\n\\frac{8}{3} & \\frac{7}{3} & -\\frac{5}{3} \\\\\n\\frac{5}{3} & -\\frac{5}{3} & \\frac{4}{3}\n\\end{pmatrix}\n$$\nThus, we have $\\boldsymbol{A} = \\boldsymbol{B}$.\n\nThe final step is to compute the Frobenius norm of the difference $\\boldsymbol{A} - \\boldsymbol{B}$.\nThe difference tensor is:\n$$\n\\boldsymbol{A} - \\boldsymbol{B} = \\begin{pmatrix} 0 & 0 & 0 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix} = \\boldsymbol{0}\n$$\nThe Frobenius norm of an $m \\times n$ matrix $\\boldsymbol{M}$ is defined as $\\|\\boldsymbol{M}\\|_F = \\sqrt{\\sum_{i=1}^{m}\\sum_{j=1}^{n} |M_{ij}|^2}$.\nFor the $3 \\times 3$ zero matrix $\\boldsymbol{0}$, the Frobenius norm is:\n$$\n\\left\\|\\boldsymbol{A} - \\boldsymbol{B}\\right\\|_{F} = \\left\\|\\boldsymbol{0}\\right\\|_{F} = \\sqrt{0^2 + 0^2 + \\dots + 0^2} = 0\n$$\nThe result indicates that the provided principal values and directions are perfectly consistent with the given stress tensor components. The value is $0$ MPa.", "answer": "$$\\boxed{0}$$", "id": "3601998"}, {"introduction": "While analytical methods are perfect for small matrices, real-world computational mechanics often involves problems where direct diagonalization is infeasible or inefficient. This practical exercise introduces the power iteration, a foundational numerical algorithm for efficiently finding the dominant eigenvalue and eigenvector, which is often the most critical quantity in an analysis. By implementing and testing this method, you will bridge the gap between abstract theory and computational practice, and gain insight into the convergence properties that underpin many numerical solvers [@problem_id:3601980].", "problem": "Consider a real, symmetric, second-order tensor (represented as a matrix) $\\boldsymbol{A} \\in \\mathbb{R}^{3 \\times 3}$, which admits the spectral decomposition $\\boldsymbol{A} = \\boldsymbol{V} \\boldsymbol{\\Lambda} \\boldsymbol{V}^{\\top}$ with $\\boldsymbol{V}$ orthogonal (columns are orthonormal eigenvectors) and $\\boldsymbol{\\Lambda} = \\operatorname{diag}(\\lambda_1,\\lambda_2,\\lambda_3)$ containing the eigenvalues. In computational solid mechanics, the eigenvalues are the principal values (principal stresses or principal strains as appropriate), and the eigenvectors are the principal directions. The power iteration is a fundamental algorithm to approximate the dominant principal direction and principal value starting from a nonzero initial vector. Starting with a normalized initial vector $\\boldsymbol{x}_0$ with $||\\boldsymbol{x}_0||_2 = 1$ and $\\boldsymbol{x}_0$ not orthogonal to the dominant principal direction, the $k$-th power iterate is defined recursively by $\\boldsymbol{y}_{i+1} = \\boldsymbol{A} \\boldsymbol{x}_i$ and $\\boldsymbol{x}_{i+1} = \\boldsymbol{y}_{i+1} / ||\\boldsymbol{y}_{i+1}||_2$ for $i = 0,1,\\dots,k-1$. The Rayleigh quotient $\\widehat{\\lambda}(\\boldsymbol{x}_k) = \\boldsymbol{x}_k^{\\top} \\boldsymbol{A} \\boldsymbol{x}_k$ approximates the dominant principal value. Suppose the eigenvalues are ordered by magnitude such that $|\\lambda_1| > |\\lambda_2| \\ge |\\lambda_3|$ (a spectral gap). Consider the error in the estimated dominant direction measured as the maximum absolute projection of $\\boldsymbol{x}_k$ onto the non-dominant eigenspaces, namely $e_k = \\max\\left(|\\boldsymbol{v}_2^{\\top} \\boldsymbol{x}_k|, |\\boldsymbol{v}_3^{\\top} \\boldsymbol{x}_k|\\right)$, where $\\boldsymbol{v}_i$ denotes the eigenvector associated with $\\lambda_i$ and the set $\\{\\boldsymbol{v}_1,\\boldsymbol{v}_2,\\boldsymbol{v}_3\\}$ is orthonormal. Under the spectral gap assumption and an initial coefficient dominance condition $|\\boldsymbol{v}_i^{\\top} \\boldsymbol{x}_0| \\le |\\boldsymbol{v}_1^{\\top} \\boldsymbol{x}_0|$ for $i \\in \\{2,3\\}$, the power iteration admits the bound $e_k \\le \\left|\\lambda_2/\\lambda_1\\right|^k$.\n\nYour task is to:\n- Implement the power iteration to estimate the dominant principal value and principal direction of $\\boldsymbol{A}$.\n- Compute the error metric $e_k$ defined above at iteration $k$.\n- Verify the bound $e_k \\le \\left|\\lambda_2/\\lambda_1\\right|^k$ when a spectral gap $|\\lambda_1| > |\\lambda_2|$ holds and the initial coefficient dominance condition is satisfied.\n- For verification of ground truth principal values and directions, use the spectral decomposition of $\\boldsymbol{A}$; your implementation should compute them from $\\boldsymbol{A}$ and must not assume them a priori.\n\nUse the following test suite of parameter values. In all cases, angles are specified in radians and every vector must be normalized to have Euclidean norm $1$ before starting the iteration:\n1. Test case $1$ (general symmetric with clear gap): Let $\\boldsymbol{Q} = \\boldsymbol{R}_z(\\theta)\\boldsymbol{R}_y(\\phi)$ with $\\theta = 0.3$ and $\\phi = 0.4$, where $\\boldsymbol{R}_z(\\theta) = \\begin{bmatrix} \\cos\\theta & -\\sin\\theta & 0 \\\\ \\sin\\theta & \\cos\\theta & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}$ and $\\boldsymbol{R}_y(\\phi) = \\begin{bmatrix} \\cos\\phi & 0 & \\sin\\phi \\\\ 0 & 1 & 0 \\\\ -\\sin\\phi & 0 & \\cos\\phi \\end{bmatrix}$. Let $\\boldsymbol{\\Lambda} = \\operatorname{diag}(3.0, 1.5, 0.5)$ and $\\boldsymbol{A} = \\boldsymbol{Q} \\boldsymbol{\\Lambda} \\boldsymbol{Q}^{\\top}$. Let $\\boldsymbol{x}_0$ be the normalized vector proportional to $\\boldsymbol{v}_1 + 0.6\\,\\boldsymbol{v}_2 + 0.2\\,\\boldsymbol{v}_3$, where $\\boldsymbol{v}_i$ are the columns of $\\boldsymbol{Q}$. Use $k = 8$.\n2. Test case $2$ (diagonal with near second value): Let $\\boldsymbol{A} = \\operatorname{diag}(10.0, 9.9, 1.0)$. Let $\\boldsymbol{x}_0$ be the normalized vector proportional to $\\begin{bmatrix} 1.0 \\\\ 0.8 \\\\ 0.2 \\end{bmatrix}$. Use $k = 6$.\n3. Test case $3$ (indefinite with negative dominant magnitude): Let $\\boldsymbol{Q} = \\boldsymbol{R}_x(\\psi)\\boldsymbol{R}_z(\\theta)$ with $\\psi = 0.25$ and $\\theta = 0.5$, where $\\boldsymbol{R}_x(\\psi) = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & \\cos\\psi & -\\sin\\psi \\\\ 0 & \\sin\\psi & \\cos\\psi \\end{bmatrix}$. Let $\\boldsymbol{\\Lambda} = \\operatorname{diag}(-4.0, 3.9, 1.0)$ and $\\boldsymbol{A} = \\boldsymbol{Q} \\boldsymbol{\\Lambda} \\boldsymbol{Q}^{\\top}$. Let $\\boldsymbol{x}_0$ be the normalized vector proportional to $\\boldsymbol{v}_1 + 0.7\\,\\boldsymbol{v}_2 + 0.5\\,\\boldsymbol{v}_3$, where $\\boldsymbol{v}_i$ are the columns of $\\boldsymbol{Q}$. Use $k = 10$.\n4. Test case $4$ (small spectral gap): Let $\\boldsymbol{Q} = \\boldsymbol{R}_y(\\phi)$ with $\\phi = 0.2$. Let $\\boldsymbol{\\Lambda} = \\operatorname{diag}(1.0, 0.99, 0.5)$ and $\\boldsymbol{A} = \\boldsymbol{Q} \\boldsymbol{\\Lambda} \\boldsymbol{Q}^{\\top}$. Let $\\boldsymbol{x}_0$ be the normalized vector proportional to $\\boldsymbol{v}_1 + 0.4\\,\\boldsymbol{v}_2 + 0.1\\,\\boldsymbol{v}_3$, where $\\boldsymbol{v}_i$ are the columns of $\\boldsymbol{Q}$. Use $k = 12$.\n5. Test case $5$ (boundary case with no spectral gap): Let $\\boldsymbol{A} = \\operatorname{diag}(2.0, 2.0, 0.5)$. Let $\\boldsymbol{x}_0$ be the normalized vector proportional to $\\begin{bmatrix} 1.0 \\\\ 0.8 \\\\ 0.1 \\end{bmatrix}$. Use $k = 8$.\n\nFor each test case:\n- Compute the power iteration estimate $\\boldsymbol{x}_k$ and the Rayleigh quotient $\\widehat{\\lambda}(\\boldsymbol{x}_k)$ as a float.\n- Compute the error $e_k = \\max\\left(|\\boldsymbol{v}_2^{\\top} \\boldsymbol{x}_k|, |\\boldsymbol{v}_3^{\\top} \\boldsymbol{x}_k|\\right)$ as a float.\n- Determine a boolean indicating whether the bound $e_k \\le \\left|\\lambda_2/\\lambda_1\\right|^k$ holds. If the spectral gap condition $|\\lambda_1| > |\\lambda_2|$ fails, the boolean must be $\\text{False}$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case result is itself a list of the form $[\\widehat{\\lambda}(\\boldsymbol{x}_k), e_k, \\text{bound\\_ok}]$. For example, the output must look like $[[\\dots],[\\dots],[\\dots],[\\dots],[\\dots]]$. No physical units or angles need to be reported in the output, and no external input is permitted. Use the Euclidean norm for all vector normalizations.", "solution": "The problem statement is assessed to be valid. It presents a well-defined task in numerical linear algebra, specifically the application and verification of the power iteration method for finding the dominant eigenpair of a real, symmetric $3 \\times 3$ tensor. The problem is scientifically grounded in established principles of computational mechanics and linear algebra. It is self-contained, providing all necessary matrices, initial vectors, and parameters for five distinct test cases. The objectives are clear, objective, and computationally verifiable. The test cases are thoughtfully designed to explore different scenarios, including a clear spectral gap, a small spectral gap, negative eigenvalues, and a degenerate case with a repeated dominant eigenvalue, which serves to test the understanding of the method's convergence conditions.\n\nThe solution proceeds as follows:\nFirst, for each test case, the real symmetric matrix $\\boldsymbol{A} \\in \\mathbb{R}^{3 \\times 3}$ is constructed according to the provided specifications. For cases involving rotation matrices, we first define the standard rotation matrices $\\boldsymbol{R}_x(\\psi)$, $\\boldsymbol{R}_y(\\phi)$, and $\\boldsymbol{R}_z(\\theta)$:\n$$\n\\boldsymbol{R}_x(\\psi) = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & \\cos\\psi & -\\sin\\psi \\\\ 0 & \\sin\\psi & \\cos\\psi \\end{bmatrix}, \\quad\n\\boldsymbol{R}_y(\\phi) = \\begin{bmatrix} \\cos\\phi & 0 & \\sin\\phi \\\\ 0 & 1 & 0 \\\\ -\\sin\\phi & 0 & \\cos\\phi \\end{bmatrix}, \\quad\n\\boldsymbol{R}_z(\\theta) = \\begin{bmatrix} \\cos\\theta & -\\sin\\theta & 0 \\\\ \\sin\\theta & \\cos\\theta & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}\n$$\nThe matrix $\\boldsymbol{A}$ is then formed either directly or through the spectral construction $\\boldsymbol{A} = \\boldsymbol{Q} \\boldsymbol{\\Lambda} \\boldsymbol{Q}^{\\top}$, where $\\boldsymbol{Q}$ is an orthogonal matrix of eigenvectors and $\\boldsymbol{\\Lambda}$ is a diagonal matrix of eigenvalues.\n\nSecond, to establish the ground truth for verification, we perform a spectral decomposition of the constructed matrix $\\boldsymbol{A}$. Since $\\boldsymbol{A}$ is real and symmetric, it is guaranteed to have real eigenvalues and a complete set of orthonormal eigenvectors. We compute the eigenvalues $\\lambda_i$ and corresponding eigenvectors $\\boldsymbol{v}_i$. A critical step is to order these eigenpairs according to the descending magnitude of the eigenvalues, such that $|\\lambda_1| \\ge |\\lambda_2| \\ge |\\lambda_3|$. The problem requires using the convention $|\\lambda_1| > |\\lambda_2| \\ge |\\lambda_3|$ for the convergence theory, so our sorting must reflect this. The eigenvectors must be reordered consistently with the sorted eigenvalues.\n\nThird, the initial vector $\\boldsymbol{x}_0$ for the power iteration is constructed. As specified, $\\boldsymbol{x}_0$ is derived from a given proportional vector, which is then normalized to have a Euclidean norm of $1$, i.e., $||\\boldsymbol{x}_0||_2 = 1$.\n\nFourth, the power iteration algorithm is executed for a prescribed number of iterations, $k$. Starting with $\\boldsymbol{x}_0$, the sequence is generated by the recurrence relation:\n$$\n\\boldsymbol{y}_{i+1} = \\boldsymbol{A} \\boldsymbol{x}_i\n$$\n$$\n\\boldsymbol{x}_{i+1} = \\frac{\\boldsymbol{y}_{i+1}}{||\\boldsymbol{y}_{i+1}||_2} \\quad \\text{for } i = 0, 1, \\dots, k-1.\n$$\nThe final vector from this process is denoted $\\boldsymbol{x}_k$.\n\nFifth, we compute the required output quantities for each test case:\n1.  The Rayleigh quotient, $\\widehat{\\lambda}(\\boldsymbol{x}_k)$, which provides an estimate of the dominant eigenvalue $\\lambda_1$, is calculated as:\n    $$\n    \\widehat{\\lambda}(\\boldsymbol{x}_k) = \\boldsymbol{x}_k^{\\top} \\boldsymbol{A} \\boldsymbol{x}_k\n    $$\n2.  The error metric, $e_k$, which measures the projection of the iterated vector $\\boldsymbol{x}_k$ onto the non-dominant eigenspaces, is calculated using the ground-truth eigenvectors $\\boldsymbol{v}_2$ and $\\boldsymbol{v}_3$:\n    $$\n    e_k = \\max\\left(|\\boldsymbol{v}_2^{\\top} \\boldsymbol{x}_k|, |\\boldsymbol{v}_3^{\\top} \\boldsymbol{x}_k|\\right)\n    $$\n3.  The verification of the theoretical error bound, $e_k \\le \\left|\\frac{\\lambda_2}{\\lambda_1}\\right|^k$, is performed. This check is contingent on the spectral gap condition, $|\\lambda_1| > |\\lambda_2|$.\n    - If $|\\lambda_1| > |\\lambda_2|$ is false (as in Test Case 5), the bound is not applicable, and the verification result is mandated to be `False`.\n    - If $|\\lambda_1| > |\\lambda_2|$ is true, we compute the bound $B_k = \\left|\\frac{\\lambda_2}{\\lambda_1}\\right|^k$ and determine the boolean result of the inequality $e_k \\le B_k$. The initial coefficient dominance condition $|\\boldsymbol{v}_i^{\\top} \\boldsymbol{x}_0| \\le |\\boldsymbol{v}_1^{\\top} \\boldsymbol{x}_0|$ for $i\\in\\{2,3\\}$ is satisfied by all test cases where the spectral gap exists, so the bound is expected to hold.\n\nFinally, the results for all five test cases, each consisting of the triplet $[\\widehat{\\lambda}(\\boldsymbol{x}_k), e_k, \\text{bound\\_ok}]$, are aggregated into a single list and formatted into the precise string representation required by the problem statement.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final result.\n    \"\"\"\n\n    def Rx(psi):\n        c, s = np.cos(psi), np.sin(psi)\n        return np.array([[1, 0, 0], [0, c, -s], [0, s, c]])\n\n    def Ry(phi):\n        c, s = np.cos(phi), np.sin(phi)\n        return np.array([[c, 0, s], [0, 1, 0], [-s, 0, c]])\n\n    def Rz(theta):\n        c, s = np.cos(theta), np.sin(theta)\n        return np.array([[c, -s, 0], [s, c, 0], [0, 0, 1]])\n\n    def process_case(case_params):\n        \"\"\"\n        Processes a single test case.\n        \"\"\"\n        # Step 1: Construct the matrix A\n        if case_params['type'] == 'general':\n            Q_list = []\n            if 'theta' in case_params and 'phi' in case_params:\n                Q_list.append(Rz(case_params['theta']))\n                Q_list.append(Ry(case_params['phi']))\n            elif 'psi' in case_params and 'theta' in case_params:\n                Q_list.append(Rx(case_params['psi']))\n                Q_list.append(Rz(case_params['theta']))\n            elif 'phi' in case_params:\n                Q_list.append(Ry(case_params['phi']))\n            \n            Q = np.eye(3)\n            for R_mat in Q_list:\n                Q = Q @ R_mat\n\n            Lambda = np.diag(case_params['lambda_diag'])\n            A = Q @ Lambda @ Q.T\n        else: # diagonal\n            A = np.diag(case_params['lambda_diag'])\n\n        # Step 2: Determine ground truth eigenvalues/vectors from A\n        evals, evecs = np.linalg.eigh(A)\n        # Sort by descending absolute value of eigenvalues\n        sort_indices = np.argsort(np.abs(evals))[::-1]\n        lambdas = evals[sort_indices]\n        v = evecs[:, sort_indices]\n\n        # Eigenvectors for ground truth\n        v1, v2, v3 = v[:, 0], v[:, 1], v[:, 2]\n\n        # Step 3: Construct the initial vector x0\n        k = case_params['k']\n        if 'x0_coeffs' in case_params:\n            if case_params['type'] == 'general':\n                # Use columns of Q which are the eigenvectors by construction\n                v_from_Q1, v_from_Q2, v_from_Q3 = Q[:,0], Q[:,1], Q[:,2]\n                x0_unnormalized = (case_params['x0_coeffs'][0] * v_from_Q1 +\n                                   case_params['x0_coeffs'][1] * v_from_Q2 +\n                                   case_params['x0_coeffs'][2] * v_from_Q3)\n            else: # diagonal with x0 coeffs implies standard basis\n                x0_unnormalized = np.array(case_params['x0_coeffs'])\n        else:\n            x0_unnormalized = np.array(case_params['x0_vec'])\n\n        # Normalize x0\n        x0 = x0_unnormalized / np.linalg.norm(x0_unnormalized)\n\n        # Step 4: Power Iteration\n        x = x0\n        for _ in range(k):\n            y = A @ x\n            x = y / np.linalg.norm(y)\n        x_k = x\n\n        # Step 5: Compute outputs\n        # Rayleigh quotient\n        lambda_hat = x_k.T @ A @ x_k\n\n        # Error metric e_k\n        e_k = max(abs(v2.T @ x_k), abs(v3.T @ x_k))\n\n        # Bound verification\n        lambda1_mag, lambda2_mag = abs(lambdas[0]), abs(lambdas[1])\n        \n        spectral_gap_exists = lambda1_mag > lambda2_mag\n        bound_ok = False\n        if not spectral_gap_exists:\n            bound_ok = False\n        else:\n            bound_val = (lambda2_mag / lambda1_mag)**k\n            bound_ok = e_k <= bound_val\n        \n        return [lambda_hat, e_k, str(bound_ok).lower()]\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            'type': 'general', 'theta': 0.3, 'phi': 0.4,\n            'lambda_diag': [3.0, 1.5, 0.5],\n            'x0_coeffs': [1.0, 0.6, 0.2], 'k': 8\n        },\n        {\n            'type': 'diagonal', 'lambda_diag': [10.0, 9.9, 1.0],\n            'x0_vec': [1.0, 0.8, 0.2], 'k': 6\n        },\n        {\n            'type': 'general', 'psi': 0.25, 'theta': 0.5,\n            'lambda_diag': [-4.0, 3.9, 1.0],\n            'x0_coeffs': [1.0, 0.7, 0.5], 'k': 10\n        },\n        {\n            'type': 'general', 'phi': 0.2,\n            'lambda_diag': [1.0, 0.99, 0.5],\n            'x0_coeffs': [1.0, 0.4, 0.1], 'k': 12\n        },\n        {\n            'type': 'diagonal', 'lambda_diag': [2.0, 2.0, 0.5],\n            'x0_vec': [1.0, 0.8, 0.1], 'k': 8\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        results.append(process_case(case))\n\n    # Final print statement in the exact required format.\n    str_results = []\n    for r in results:\n        # Format each sublist to a string without spaces\n        s = f\"[{r[0]},{r[1]},{r[2]}]\"\n        str_results.append(s)\n    \n    print(f\"[{','.join(str_results)}]\")\n\nsolve()\n```", "id": "3601980"}]}