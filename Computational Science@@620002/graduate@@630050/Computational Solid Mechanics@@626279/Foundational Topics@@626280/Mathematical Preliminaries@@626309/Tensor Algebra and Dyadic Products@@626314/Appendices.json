{"hands_on_practices": [{"introduction": "The dyadic product is the fundamental building block for constructing second-order tensors. This exercise explores the intrinsic structure of a dyad, revealing that the product of two non-zero vectors forms a rank-one tensor [@problem_id:3604564]. By calculating its matrix form and determining its singular values, you will solidify the connection between abstract tensor algebra and the concrete concepts of matrix rank and singular value decomposition, which are foundational to many computational algorithms.", "problem": "Consider vectors $\\boldsymbol{u}, \\boldsymbol{v}, \\boldsymbol{w} \\in \\mathbb{R}^{3}$ and the second-order tensor (matrix) formed by the dyadic product $\\boldsymbol{u} \\otimes \\boldsymbol{v}$, defined by the components $(\\boldsymbol{u} \\otimes \\boldsymbol{v})_{ij} = u_{i} v_{j}$. The action of a second-order tensor on a vector is defined by standard tensor-vector contraction, $(\\boldsymbol{A}\\boldsymbol{w})_{i} = \\sum_{j=1}^{3} A_{ij} w_{j}$, and the Euclidean inner product of two vectors is $\\boldsymbol{v} \\cdot \\boldsymbol{w} = \\sum_{j=1}^{3} v_{j} w_{j}$. In computational solid mechanics, rank-one updates and dyadic products commonly arise in stress and stiffness formulations; establishing their algebraic properties is essential for stable algorithms.\n\nGiven $\\boldsymbol{u} = (1,0,0)^{\\mathrm{T}}$ and $\\boldsymbol{v} = (0,2,0)^{\\mathrm{T}}$, perform the following:\n\n1. Compute the dyadic product $\\boldsymbol{u} \\otimes \\boldsymbol{v}$ as a $3 \\times 3$ matrix using the componentwise definition.\n2. Verify that $\\boldsymbol{u} \\otimes \\boldsymbol{v}$ is rank-one by demonstrating, for arbitrary $\\boldsymbol{w} \\in \\mathbb{R}^{3}$, that $(\\boldsymbol{u} \\otimes \\boldsymbol{v}) \\boldsymbol{w} = \\boldsymbol{u} \\left( \\boldsymbol{v} \\cdot \\boldsymbol{w} \\right)$.\n3. Determine the singular values of $\\boldsymbol{u} \\otimes \\boldsymbol{v}$, defined as the nonnegative square roots of the eigenvalues of $(\\boldsymbol{u} \\otimes \\boldsymbol{v})^{\\mathrm{T}} (\\boldsymbol{u} \\otimes \\boldsymbol{v})$.\n\nExpress the final answer as a row matrix containing the singular values in non-increasing order. No rounding is required and no units are needed.", "solution": "The problem is validated as scientifically sound, well-posed, objective, and self-contained. It presents a standard exercise in linear and tensor algebra, fundamental to disciplines like computational solid mechanics. All definitions and data provided are standard and consistent. We may, therefore, proceed with the solution.\n\nThe problem is divided into three parts. We will address them sequentially. The given vectors are $\\boldsymbol{u} = (1,0,0)^{\\mathrm{T}}$ and $\\boldsymbol{v} = (0,2,0)^{\\mathrm{T}}$. In column-vector notation, this is:\n$$\n\\boldsymbol{u} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\quad \\boldsymbol{v} = \\begin{pmatrix} 2 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\nIt appears there is a typo in my scratchpad. Let me re-verify the given vectors. The problem states $\\boldsymbol{u} = (1,0,0)^{\\mathrm{T}}$ and $\\boldsymbol{v} = (0,2,0)^{\\mathrm{T}}$. Thus, the correct column vectors are:\n$$\n\\boldsymbol{u} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\quad \\boldsymbol{v} = \\begin{pmatrix} 0 \\\\ 2 \\\\ 0 \\end{pmatrix}\n$$\n\n**Part 1: Compute the dyadic product $\\boldsymbol{u} \\otimes \\boldsymbol{v}$**\n\nThe dyadic product of two vectors, $\\boldsymbol{u}$ and $\\boldsymbol{v}$, results in a second-order tensor (or matrix) $\\boldsymbol{A} = \\boldsymbol{u} \\otimes \\boldsymbol{v}$. Its components are given by the definition $A_{ij} = u_i v_j$. In matrix notation, this is equivalent to the outer product $\\boldsymbol{u} \\boldsymbol{v}^{\\mathrm{T}}$.\n\nThe vectors are $\\boldsymbol{u} = (u_1, u_2, u_3) = (1, 0, 0)$ and $\\boldsymbol{v} = (v_1, v_2, v_3) = (0, 2, 0)$.\nThe transpose of $\\boldsymbol{v}$ is $\\boldsymbol{v}^{\\mathrm{T}} = \\begin{pmatrix} 0 & 2 & 0 \\end{pmatrix}$.\n\nWe compute the matrix product $\\boldsymbol{u} \\boldsymbol{v}^{\\mathrm{T}}$:\n$$\n\\boldsymbol{u} \\otimes \\boldsymbol{v} = \\boldsymbol{u} \\boldsymbol{v}^{\\mathrm{T}} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} \\begin{pmatrix} 0 & 2 & 0 \\end{pmatrix} = \\begin{pmatrix}\n(1)(0) & (1)(2) & (1)(0) \\\\\n(0)(0) & (0)(2) & (0)(0) \\\\\n(0)(0) & (0)(2) & (0)(0)\n\\end{pmatrix}\n$$\n$$\n\\boldsymbol{u} \\otimes \\boldsymbol{v} = \\begin{pmatrix}\n0 & 2 & 0 \\\\\n0 & 0 & 0 \\\\\n0 & 0 & 0\n\\end{pmatrix}\n$$\nThis is the $3 \\times 3$ matrix representation of the dyadic product $\\boldsymbol{u} \\otimes \\boldsymbol{v}$.\n\n**Part 2: Verify that $(\\boldsymbol{u} \\otimes \\boldsymbol{v}) \\boldsymbol{w} = \\boldsymbol{u} ( \\boldsymbol{v} \\cdot \\boldsymbol{w} )$**\n\nThis is a fundamental property of the dyadic product. We will verify it generally using index notation, as requested for an arbitrary vector $\\boldsymbol{w} = (w_1, w_2, w_3)^{\\mathrm{T}} \\in \\mathbb{R}^3$.\n\nLet $\\boldsymbol{A} = \\boldsymbol{u} \\otimes \\boldsymbol{v}$. The $i$-th component of the vector resulting from the action of $\\boldsymbol{A}$ on $\\boldsymbol{w}$ is given by the tensor-vector contraction rule:\n$$\n(\\boldsymbol{A} \\boldsymbol{w})_i = \\sum_{j=1}^{3} A_{ij} w_j\n$$\nSubstituting the component definition of the dyadic product, $A_{ij} = u_i v_j$:\n$$\n(\\boldsymbol{A} \\boldsymbol{w})_i = \\sum_{j=1}^{3} (u_i v_j) w_j\n$$\nThe component $u_i$ is independent of the summation index $j$, so it can be factored out of the summation:\n$$\n(\\boldsymbol{A} \\boldsymbol{w})_i = u_i \\left( \\sum_{j=1}^{3} v_j w_j \\right)\n$$\nThe expression in the parenthesis is the definition of the Euclidean inner product (dot product) of $\\boldsymbol{v}$ and $\\boldsymbol{w}$:\n$$\n\\boldsymbol{v} \\cdot \\boldsymbol{w} = \\sum_{j=1}^{3} v_j w_j\n$$\nSubstituting this back, we get:\n$$\n(\\boldsymbol{A} \\boldsymbol{w})_i = u_i (\\boldsymbol{v} \\cdot \\boldsymbol{w})\n$$\nThe term $\\boldsymbol{v} \\cdot \\boldsymbol{w}$ is a scalar. The expression $u_i (\\boldsymbol{v} \\cdot \\boldsymbol{w})$ represents the $i$-th component of the vector $\\boldsymbol{u}$ scaled by this scalar. Therefore, the resulting vector is $\\boldsymbol{u} (\\boldsymbol{v} \\cdot \\boldsymbol{w})$. This completes the general verification:\n$$\n(\\boldsymbol{u} \\otimes \\boldsymbol{v}) \\boldsymbol{w} = \\boldsymbol{u} (\\boldsymbol{v} \\cdot \\boldsymbol{w})\n$$\nThis identity demonstrates that the image of the linear transformation represented by $\\boldsymbol{u} \\otimes \\boldsymbol{v}$ is the one-dimensional space spanned by the vector $\\boldsymbol{u}$. Consequently, the tensor $\\boldsymbol{u} \\otimes \\boldsymbol{v}$ has a rank of $1$ (assuming $\\boldsymbol{u}$ and $\\boldsymbol{v}$ are non-zero vectors).\n\n**Part 3: Determine the singular values of $\\boldsymbol{u} \\otimes \\boldsymbol{v}$**\n\nThe singular values of a matrix $\\boldsymbol{A}$ are defined as the non-negative square roots of the eigenvalues of the matrix $\\boldsymbol{A}^{\\mathrm{T}} \\boldsymbol{A}$. Let $\\boldsymbol{A} = \\boldsymbol{u} \\otimes \\boldsymbol{v}$. From Part 1, we have:\n$$\n\\boldsymbol{A} = \\begin{pmatrix}\n0 & 2 & 0 \\\\\n0 & 0 & 0 \\\\\n0 & 0 & 0\n\\end{pmatrix}\n$$\nThe transpose of $\\boldsymbol{A}$ is:\n$$\n\\boldsymbol{A}^{\\mathrm{T}} = \\begin{pmatrix}\n0 & 0 & 0 \\\\\n2 & 0 & 0 \\\\\n0 & 0 & 0\n\\end{pmatrix}\n$$\nNow, we compute the product $\\boldsymbol{A}^{\\mathrm{T}} \\boldsymbol{A}$:\n$$\n\\boldsymbol{A}^{\\mathrm{T}} \\boldsymbol{A} = \\begin{pmatrix}\n0 & 0 & 0 \\\\\n2 & 0 & 0 \\\\\n0 & 0 & 0\n\\end{pmatrix} \\begin{pmatrix}\n0 & 2 & 0 \\\\\n0 & 0 & 0 \\\\\n0 & 0 & 0\n\\end{pmatrix} = \\begin{pmatrix}\n0 & 0 & 0 \\\\\n0 & 4 & 0 \\\\\n0 & 0 & 0\n\\end{pmatrix}\n$$\nLet this matrix be denoted by $\\boldsymbol{B} = \\boldsymbol{A}^{\\mathrm{T}} \\boldsymbol{A}$. We need to find the eigenvalues $\\lambda$ of $\\boldsymbol{B}$ by solving the characteristic equation $\\det(\\boldsymbol{B} - \\lambda \\boldsymbol{I}) = 0$, where $\\boldsymbol{I}$ is the $3 \\times 3$ identity matrix.\n$$\n\\det\\left( \\begin{pmatrix}\n0 & 0 & 0 \\\\\n0 & 4 & 0 \\\\\n0 & 0 & 0\n\\end{pmatrix} - \\lambda \\begin{pmatrix}\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1\n\\end{pmatrix} \\right) = 0\n$$\n$$\n\\det\\begin{pmatrix}\n-\\lambda & 0 & 0 \\\\\n0 & 4-\\lambda & 0 \\\\\n0 & 0 & -\\lambda\n\\end{pmatrix} = 0\n$$\nThe determinant of this diagonal matrix is the product of its diagonal entries:\n$$\n(-\\lambda)(4-\\lambda)(-\\lambda) = 0\n$$\n$$\n\\lambda^2 (4-\\lambda) = 0\n$$\nThe eigenvalues $\\lambda$ of $\\boldsymbol{A}^{\\mathrm{T}} \\boldsymbol{A}$ are $\\lambda_1 = 4$, $\\lambda_2 = 0$, and $\\lambda_3 = 0$.\n\nThe singular values $\\sigma$ are the non-negative square roots of these eigenvalues.\n$$\n\\sigma_1 = \\sqrt{4} = 2\n$$\n$$\n\\sigma_2 = \\sqrt{0} = 0\n$$\n$$\n\\sigma_3 = \\sqrt{0} = 0\n$$\nThe singular values of $\\boldsymbol{u} \\otimes \\boldsymbol{v}$ are $\\{2, 0, 0\\}$. Arranged in non-increasing order, they are $2$, $0$, $0$.\n\nAlternatively, using the general properties: $\\boldsymbol{A}^{\\mathrm{T}}\\boldsymbol{A} = (\\boldsymbol{u}\\boldsymbol{v}^{\\mathrm{T}})^{\\mathrm{T}}(\\boldsymbol{u}\\boldsymbol{v}^{\\mathrm{T}}) = (\\boldsymbol{v}\\boldsymbol{u}^{\\mathrm{T}})(\\boldsymbol{u}\\boldsymbol{v}^{\\mathrm{T}}) = \\boldsymbol{v}(\\boldsymbol{u}^{\\mathrm{T}}\\boldsymbol{u})\\boldsymbol{v}^{\\mathrm{T}} = (\\boldsymbol{u} \\cdot \\boldsymbol{u})(\\boldsymbol{v} \\otimes \\boldsymbol{v}) = \\|\\boldsymbol{u}\\|^2(\\boldsymbol{v} \\otimes \\boldsymbol{v})$. The eigenvalues of this matrix are $\\|\\boldsymbol{u}\\|^2\\|\\boldsymbol{v}\\|^2$, $0$, and $0$. The non-zero singular value is thus $\\sigma = \\sqrt{\\|\\boldsymbol{u}\\|^2\\|\\boldsymbol{v}\\|^2} = \\|\\boldsymbol{u}\\|\\|\\boldsymbol{v}\\|$.\nFor $\\boldsymbol{u} = (1,0,0)^{\\mathrm{T}}$, $\\|\\boldsymbol{u}\\| = \\sqrt{1^2+0^2+0^2} = 1$.\nFor $\\boldsymbol{v} = (0,2,0)^{\\mathrm{T}}$, $\\|\\boldsymbol{v}\\| = \\sqrt{0^2+2^2+0^2} = 2$.\nThe non-zero singular value is $\\sigma = (1)(2) = 2$. The other two are $0$. This confirms the result.\n\nThe final answer is the singular values arranged in a row matrix in non-increasing order.", "answer": "$$\n\\boxed{\\begin{pmatrix} 2 & 0 & 0 \\end{pmatrix}}\n$$", "id": "3604564"}, {"introduction": "Building on the properties of a single dyad, this practice investigates how these fundamental tensors interact with general second-order tensors through the double-dot product. You will use index notation to simplify the expression $(u \\otimes v):A$ into the more familiar matrix-vector form $u \\cdot (Av)$, a common operation in evaluating work and energy terms [@problem_id:3604550]. This develops fluency in converting between tensor notations and reinforces the principle of scalar invariance, which is critical for formulating physically consistent models.", "problem": "In computational solid mechanics (CSM), second-order tensors and their contractions arise naturally in the evaluation of work, energy, and fluxes. Let $u, v \\in \\mathbb{R}^{3}$ be column vectors and let $A \\in \\mathbb{R}^{3 \\times 3}$ be a second-order tensor. The dyadic product $u \\otimes v$ is the second-order tensor with components $(u \\otimes v)_{ij} = u_{i} v_{j}$, and the double contraction of second-order tensors $X:A$ is defined by $X:A = X_{ij} A_{ij}$ under the Einstein Summation Convention (ESC). Working in a Cartesian frame with the Euclidean metric defined by the Kronecker delta $\\delta_{ij}$, use index notation and the core definitions just stated to reduce the scalar $(u \\otimes v):A = u_{i} v_{j} A_{ij}$ to a standard Euclidean inner-product form built from $u$, $v$, and $A$. Explain briefly why this scalar is invariant under any orthogonal change of basis. Then, evaluate this scalar for\n$$\nu = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix}, \\quad\nv = \\begin{pmatrix} 0 \\\\ 1 \\\\ 1 \\end{pmatrix}, \\quad\nA = \\begin{pmatrix}\n2 & -1 & 0 \\\\\n-1 & 3 & 1 \\\\\n0 & 1 & 0\n\\end{pmatrix}.\n$$\nProvide the exact scalar value (no rounding). The quantity is dimensionless; do not report units.", "solution": "The problem statement is first validated against the required criteria.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- `u, v \\in \\mathbb{R}^{3}` are column vectors.\n- `A \\in \\mathbb{R}^{3 \\times 3}` is a second-order tensor.\n- The dyadic product is defined by its components `(u \\otimes v)_{ij} = u_{i} v_{j}`.\n- The double contraction is defined as `X:A = X_{ij} A_{ij}` using the Einstein Summation Convention (ESC).\n- The space is a Cartesian frame with the Euclidean metric defined by the Kronecker delta `\\delta_{ij}`.\n- The scalar to be analyzed is `(u \\otimes v):A = u_{i} v_{j} A_{ij}`.\n- The following specific values are provided for evaluation:\n$$\nu = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix}, \\quad\nv = \\begin{pmatrix} 0 \\\\ 1 \\\\ 1 \\end{pmatrix}, \\quad\nA = \\begin{pmatrix}\n2 & -1 & 0 \\\\\n-1 & 3 & 1 \\\\\n0 & 1 & 0\n\\end{pmatrix}\n$$\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem uses standard, correct definitions for the dyadic product and double contraction from tensor algebra and continuum mechanics. These operations are fundamental in computational solid mechanics (CSM).\n- **Well-Posed:** The problem is well-posed. It asks for three distinct tasks: a symbolic reduction, an explanation of a property (invariance), and a numerical evaluation. All necessary information is provided for each task.\n- **Objective:** The problem is stated using precise mathematical language and is free from subjective or ambiguous terms.\n\nThe problem is self-contained, consistent, and scientifically sound. It does not violate any of the invalidity criteria.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A complete solution will be provided.\n\n### Solution\n\nLet the scalar quantity be denoted by $S$. The problem asks for the reduction and evaluation of $S = (u \\otimes v):A$.\n\n**Part 1: Reduction of the Expression**\n\nBy definition, the double contraction `X:A` is the sum of the element-wise products of the components, $X_{ij} A_{ij}$. Here, the tensor $X$ is the dyadic product $u \\otimes v$, whose components are given by $(u \\otimes v)_{ij} = u_{i} v_{j}$.\n\nSubstituting the components of the dyadic product into the definition of the double contraction, we get:\n$$\nS = (u \\otimes v):A = (u \\otimes v)_{ij} A_{ij} = u_{i} v_{j} A_{ij}\n$$\nThe Einstein Summation Convention implies summation over both indices $i$ and $j$ from $1$ to $3$. Since the components $u_i$, $v_j$, and $A_{ij}$ are scalars, their multiplication is commutative. We can rearrange the terms as:\n$$\nS = u_{i} (A_{ij} v_{j})\n$$\nWe recognize the term in the parentheses, $A_{ij} v_{j}$, as the definition of the $i$-th component of the matrix-vector product $Av$. Let us define a vector $w$ such that $w = Av$. The components of $w$ are given by $w_{i} = A_{ij} v_{j}$.\n\nSubstituting this back into the expression for $S$, we have:\n$$\nS = u_{i} w_{i}\n$$\nThis is the definition of the standard Euclidean inner product (or dot product) of the vectors $u$ and $w$. Therefore, we can write:\n$$\nS = u \\cdot w = u \\cdot (Av)\n$$\nIn matrix notation, for column vectors $u$ and $v$ and matrix $A$, this inner product is expressed as:\n$$\nS = u^{T}(Av)\n$$\nThis is the desired reduction of the expression to a standard inner-product form.\n\n**Part 2: Invariance Under Orthogonal Change of Basis**\n\nA scalar quantity is, by definition, invariant under a change of basis. We can demonstrate this explicitly for the expression $S = u^{T} A v$. Consider an orthogonal change of basis from the original (unprimed) frame to a new (primed) frame. This transformation is represented by an orthogonal matrix $Q$, which satisfies $Q^{T}Q = QQ^{T} = I$, where $I$ is the identity matrix.\n\nThe components of the vectors $u$ and $v$, and the second-order tensor $A$ transform as follows:\n$$\nu' = Qu, \\quad v' = Qv, \\quad A' = QAQ^{T}\n$$\nThe scalar $S'$ in the new basis is given by the same form:\n$$\nS' = (u')^{T} A' v'\n$$\nSubstituting the transformation rules into this expression:\n$$\nS' = (Qu)^{T} (QAQ^{T}) (Qv)\n$$\nUsing the property of the transpose of a matrix product, $(XY)^{T} = Y^{T}X^{T}$, we have $(Qu)^{T} = u^{T}Q^{T}$.\n$$\nS' = (u^{T}Q^{T}) (QAQ^{T}) (Qv)\n$$\nBy the associativity of matrix multiplication, we can regroup the terms:\n$$\nS' = u^{T} (Q^{T}Q) A (Q^{T}Q) v\n$$\nSince $Q$ is an orthogonal matrix, $Q^{T}Q = I$. Substituting this into the expression for $S'$:\n$$\nS' = u^{T} I A I v = u^{T} A v\n$$\nThis is precisely the expression for the scalar $S$ in the original basis. Thus, $S' = S$, which demonstrates that the scalar is invariant under any orthogonal change of basis.\n\n**Part 3: Evaluation of the Scalar**\n\nWe are asked to evaluate $S = u^{T}Av$ for the given $u$, $v$, and $A$:\n$$\nu = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix}, \\quad\nv = \\begin{pmatrix} 0 \\\\ 1 \\\\ 1 \\end{pmatrix}, \\quad\nA = \\begin{pmatrix}\n2 & -1 & 0 \\\\\n-1 & 3 & 1 \\\\\n0 & 1 & 0\n\\end{pmatrix}\n$$\nFirst, we compute the matrix-vector product $w = Av$:\n$$\nw = Av = \\begin{pmatrix}\n2 & -1 & 0 \\\\\n-1 & 3 & 1 \\\\\n0 & 1 & 0\n\\end{pmatrix}\n\\begin{pmatrix} 0 \\\\ 1 \\\\ 1 \\end{pmatrix}\n= \\begin{pmatrix}\n(2)(0) + (-1)(1) + (0)(1) \\\\\n(-1)(0) + (3)(1) + (1)(1) \\\\\n(0)(0) + (1)(1) + (0)(1)\n\\end{pmatrix}\n= \\begin{pmatrix}\n-1 \\\\\n4 \\\\\n1\n\\end{pmatrix}\n$$\nNext, we compute the inner product $S = u^{T}w$:\n$$\nS = u^{T}w = \\begin{pmatrix} 1 & 1 & 0 \\end{pmatrix}\n\\begin{pmatrix} -1 \\\\ 4 \\\\ 1 \\end{pmatrix}\n= (1)(-1) + (1)(4) + (0)(1)\n$$\n$$\nS = -1 + 4 + 0 = 3\n$$\nThe value of the scalar is $3$.", "answer": "$$\\boxed{3}$$", "id": "3604550"}, {"introduction": "This final practice bridges the gap between exact analytical theory and the practical realities of numerical computation. While the identity $(a \\otimes b) : (c \\otimes d) = (a \\cdot c)(b \\cdot d)$ holds perfectly in symbolic mathematics, its implementation in floating-point arithmetic can introduce errors [@problem_id:3604592]. By designing a numerical experiment to test this identity under various conditions, you will gain crucial insights into numerical stability, loss of significance, and catastrophic cancellation, essential considerations for developing robust computational mechanics software.", "problem": "Consider the following task in computational solid mechanics focusing on tensor algebra and dyadic products. Work in a $3$-dimensional Euclidean space with standard basis and inner product. Use only the following base definitions: the dyadic (outer) product of vectors $a$ and $b$ with components $a_i$ and $b_j$ is the second-order tensor $(a \\otimes b)_{ij} = a_i b_j$, the double contraction of two second-order tensors $A$ and $B$ is $A:B = \\sum_{i=1}^{3} \\sum_{j=1}^{3} A_{ij} B_{ij}$, and the Euclidean dot product of vectors $a$ and $b$ is $a \\cdot b = \\sum_{i=1}^{3} a_i b_i$. Starting strictly from these definitions, derive symbolically the scalar obtained by the double contraction of two dyads formed from vectors $a,b$ and $c,d$. Then, design and implement a numerical experiment in 64-bit floating-point arithmetic following the Institute of Electrical and Electronics Engineers (IEEE) 754 binary64 standard to assess finite-precision behavior.\n\nYour program must, for each test case, compute two scalars by two routes:\n- Route 1: Construct $a \\otimes b$ and $c \\otimes d$ from the provided vectors, then compute the double contraction $ (a \\otimes b) : (c \\otimes d)$ directly from their components.\n- Route 2: Compute the dot products $a \\cdot c$ and $b \\cdot d$ from the provided vectors, then form their product.\n\nFor each test case, report the relative error between the two Route 1 and Route 2 scalars using the following definition to avoid division by zero in degenerate cases:\n$$\n\\mathrm{rel\\_err} = \\frac{\\left| x - y \\right|}{\\max\\left( \\left| x \\right|, \\left| y \\right|, \\varepsilon \\right)},\n$$\nwhere $x$ is the scalar from Route 1, $y$ is the scalar from Route 2, and $\\varepsilon$ is the machine epsilon for IEEE 754 binary64 arithmetic (unitless). All outputs are unitless real numbers.\n\nTest Suite (each item is a quadruple of $3$-vectors $(a,b,c,d)$ designed to probe typical behavior, ill-conditioning, and edge cases):\n- Case 1 (general moderate magnitudes): \n  $a = [1.0, 2.0, 3.0]$, $b = [-1.0, 0.0, 2.0]$, $c = [0.5, -1.0, 4.0]$, $d = [3.0, -2.0, 1.0]$.\n- Case 2 (nearly collinear and antiparallel with tiny perturbations):\n  $a = [1.0, 0.0, 0.0]$, $b = [1.0, 0.0, 0.0]$, $c = [1.0, 10^{-16}, 0.0]$, $d = [-1.0, 10^{-16}, 0.0]$.\n- Case 3 (extreme scaling with cancellation but avoiding overflow):\n  $a = [10^{150}, -10^{150}, 10^{150}]$, $b = [10^{150}, $2\\times 10^{150}$, -3\\times 10^{150}]$, $c = [10^{-150}, -10^{-150}, 10^{-150}]$, $d = [-10^{-150}, $4\\times 10^{150}$, 2\\times 10^{-150}]$.\n- Case 4 (exact cancellation to near-zero):\n  $a = [1.0, 1.0, 1.0]$, $b = [1.0, -1.0, 0.0]$, $c = [1.0, -1.0, 0.0]$, $d = [1.0, 1.0, 0.0]$.\n- Case 5 (nearly orthogonal with tiny dot products):\n  $a = [1.0, 0.0, 0.0]$, $b = [0.0, 1.0, 0.0]$, $c = [10^{-16}, 1.0, 0.0]$, $d = [1.0, 10^{-16}, 0.0]$.\n- Case 6 (mixed scales and signs to test summation and cancellation):\n  $a = [0.123456789, -98765.4321, 3.1415926535]$, $b = [-2.718281828, 1.0, 10^{-12}]$, $c = [10^{12}, -10^{5}, 2.0]$, $d = [-3.0, 4.0, -5.0]$.\n- Case 7 (boundary with a zero vector):\n  $a = [0.0, 0.0, 0.0]$, $b = [1.0, 2.0, 3.0]$, $c = [4.0, 5.0, 6.0]$, $d = [7.0, 8.0, 9.0]$.\n\nRequired final output format: Your program should produce a single line of output containing the relative errors for the seven test cases, in order, as a comma-separated list enclosed in square brackets (for example, $[r_1,r_2,\\dots,r_7]$), where each $r_i$ is a unitless real number represented as a float.", "solution": "The problem requires a symbolic derivation of a tensor identity and a subsequent numerical experiment to investigate the effects of finite-precision arithmetic. The full process is detailed below.\n\nFirst, we address the symbolic derivation. The task is to find the scalar result of the operation $(a \\otimes b) : (c \\otimes d)$, where $a$, $b$, $c$, and $d$ are vectors in a $3$-dimensional Euclidean space, $\\otimes$ denotes the dyadic (or outer) product, and $:$ denotes the double contraction. We are given the definitions in component form, which we will use directly. Let the vectors be represented by their components in a standard orthonormal basis: $a = (a_1, a_2, a_3)$, $b = (b_1, b_2, b_3)$, and so on.\n\nThe dyadic product $(a \\otimes b)$ results in a second-order tensor, which we will denote as $A$. Its components are given by the definition:\n$$ A_{ij} = (a \\otimes b)_{ij} = a_i b_j $$\nSimilarly, the dyadic product $(c \\otimes d)$ results in a second-order tensor $B$ with components:\n$$ B_{ij} = (c \\otimes d)_{ij} = c_i d_j $$\nThe indices $i$ and $j$ range from $1$ to $3$.\n\nNext, we apply the definition of the double contraction of two second-order tensors $A$ and $B$:\n$$ A:B = \\sum_{i=1}^{3} \\sum_{j=1}^{3} A_{ij} B_{ij} $$\nSubstituting the component expressions for $A$ and $B$ into this definition, we obtain:\n$$ (a \\otimes b) : (c \\otimes d) = \\sum_{i=1}^{3} \\sum_{j=1}^{3} (a_i b_j) (c_i d_j) $$\nSince the components are scalars, we can rearrange the terms within the summation:\n$$ (a \\otimes b) : (c \\otimes d) = \\sum_{i=1}^{3} \\sum_{j=1}^{3} a_i c_i b_j d_j $$\nThe summation over $i$ is independent of the summation over $j$. Therefore, we can separate the double summation into a product of two single summations:\n$$ (a \\otimes b) : (c \\otimes d) = \\left( \\sum_{i=1}^{3} a_i c_i \\right) \\left( \\sum_{j=1}^{3} b_j d_j \\right) $$\nWe recognize these two summations as the definitions of the Euclidean dot product. The first term is the dot product of $a$ and $c$, and the second term is the dot product of $b$ and $d$:\n$$ \\sum_{i=1}^{3} a_i c_i = a \\cdot c $$\n$$ \\sum_{j=1}^{3} b_j d_j = b \\cdot d $$\nSubstituting these back, we arrive at the final symbolic identity:\n$$ (a \\otimes b) : (c \\otimes d) = (a \\cdot c)(b \\cdot d) $$\nThis derivation confirms that, in exact arithmetic, the two computational routes described in the problem statement are equivalent.\n\nThe numerical experiment is designed to assess how this mathematical identity holds under the constraints of finite-precision arithmetic, specifically the IEEE $754$ binary$64$ standard. We compute the same conceptual quantity via two different sequences of operations.\n\nRoute $1$ computes the scalar $x$ by first constructing the full tensor representations and then performing the double contraction:\n$1$. Construct the tensor $A = a \\otimes b$, a $3 \\times 3$ matrix with entries $A_{ij} = a_i b_j$.\n$2$. Construct the tensor $B = c \\otimes d$, a $3 \\times 3$ matrix with entries $B_{ij} = c_i d_j$.\n$3$. Compute the double contraction $x = A:B = \\sum_{i=1}^{3} \\sum_{j=1}^{3} A_{ij} B_{ij}$. This involves a summation over $9$ terms.\n\nRoute $2$ computes the scalar $y$ by first calculating the dot products and then multiplying the results:\n$1$. Compute the dot product $p_1 = a \\cdot c = \\sum_{i=1}^{3} a_i c_i$. This is a sum over $3$ terms.\n$2$. Compute the dot product $p_2 = b \\cdot d = \\sum_{j=1}^{3} b_j d_j$. This is also a sum over $3$ terms.\n$3$. Compute the final product $y = p_1 \\cdot p_2$.\n\nThe key difference lies in the order of operations. Route $1$ computes a sum of products, whereas Route $2$ computes a product of sums. In floating-point arithmetic, these are not guaranteed to be identical due to rounding errors, especially when summing numbers of disparate magnitudes (loss of significance) or when intermediate calculations lead to catastrophic cancellation.\n\nTo quantify the discrepancy, we compute the relative error between the results $x$ and $y$. The specified formula is:\n$$ \\mathrm{rel\\_err} = \\frac{\\left| x - y \\right|}{\\max\\left( \\left| x \\right|, \\left| y \\right|, \\varepsilon \\right)} $$\nHere, $\\varepsilon$ is the machine epsilon for the arithmetic standard being used. For IEEE $754$ binary$64$ (double-precision floats), $\\varepsilon = 2^{-52}$, which is approximately $2.22 \\times 10^{-16}$. The inclusion of $\\varepsilon$ in the denominator serves to regularize the calculation, preventing division by zero when both $x$ and $y$ are zero and providing a meaningful error scale when they are smaller than $\\varepsilon$.\n\nThe implementation will be carried out in Python using the NumPy library, which provides robust and efficient functions for linear algebra operations on arrays that adhere to the specified floating-point standard. The dyadic product $a \\otimes b$ is computed using `np.outer(a, b)`. The double contraction $A:B$ is equivalent to the Frobenius inner product of the matrices, which can be computed by element-wise multiplication followed by summation, i.e., `np.sum(A * B)`. The dot product $a \\cdot c$ is computed using `np.dot(a, c)`. The machine epsilon $\\varepsilon$ is obtained from `np.finfo(np.float64).eps`. The program will process each of the seven test cases, calculate the relative error, and report the results in the specified format.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the computational solid mechanics problem by performing a symbolic\n    derivation and a numerical experiment on tensor products.\n    \"\"\"\n\n    # Get the machine epsilon for IEEE 754 binary64 floating-point arithmetic.\n    epsilon = np.finfo(np.float64).eps\n\n    # Test Suite: Each item is a quadruple of 3-vectors (a, b, c, d).\n    test_cases = [\n        # Case 1: general moderate magnitudes\n        (\n            [1.0, 2.0, 3.0], \n            [-1.0, 0.0, 2.0], \n            [0.5, -1.0, 4.0], \n            [3.0, -2.0, 1.0]\n        ),\n        # Case 2: nearly collinear and antiparallel with tiny perturbations\n        (\n            [1.0, 0.0, 0.0], \n            [1.0, 0.0, 0.0], \n            [1.0, 1e-16, 0.0], \n            [-1.0, 1e-16, 0.0]\n        ),\n        # Case 3: extreme scaling with cancellation but avoiding overflow\n        (\n            [1e150, -1e150, 1e150],\n            [1e150, 2e150, -3e150],\n            [1e-150, -1e-150, 1e-150],\n            [-1e-150, 4e-150, 2e-150]\n        ),\n        # Case 4: exact cancellation to near-zero\n        (\n            [1.0, 1.0, 1.0], \n            [1.0, -1.0, 0.0], \n            [1.0, -1.0, 0.0], \n            [1.0, 1.0, 0.0]\n        ),\n        # Case 5: nearly orthogonal with tiny dot products\n        (\n            [1.0, 0.0, 0.0],\n            [0.0, 1.0, 0.0],\n            [1e-16, 1.0, 0.0],\n            [1.0, 1e-16, 0.0]\n        ),\n        # Case 6: mixed scales and signs to test summation and cancellation\n        (\n            [0.123456789, -98765.4321, 3.1415926535],\n            [-2.718281828, 1.0, 1e-12],\n            [1e12, -1e5, 2.0],\n            [-3.0, 4.0, -5.0]\n        ),\n        # Case 7: boundary with a zero vector\n        (\n            [0.0, 0.0, 0.0], \n            [1.0, 2.0, 3.0], \n            [4.0, 5.0, 6.0], \n            [7.0, 8.0, 9.0]\n        ),\n    ]\n\n    results = []\n    for case in test_cases:\n        a_vec, b_vec, c_vec, d_vec = case\n        \n        # Ensure vectors are numpy arrays with the correct float type\n        a = np.array(a_vec, dtype=np.float64)\n        b = np.array(b_vec, dtype=np.float64)\n        c = np.array(c_vec, dtype=np.float64)\n        d = np.array(d_vec, dtype=np.float64)\n\n        # Route 1: (a ⊗ b) : (c ⊗ d)\n        # Construct the dyadic products (outer products)\n        A = np.outer(a, b)\n        B = np.outer(c, d)\n        # Compute the double contraction (Frobenius inner product)\n        x = np.sum(A * B)\n\n        # Route 2: (a ⋅ c) * (b ⋅ d)\n        # Compute the dot products\n        ac_dot = np.dot(a, c)\n        bd_dot = np.dot(b, d)\n        # Compute the final product\n        y = ac_dot * bd_dot\n\n        # Calculate the relative error using the specified formula\n        numerator = np.abs(x - y)\n        denominator = np.max([np.abs(x), np.abs(y), epsilon])\n        relative_error = numerator / denominator\n        \n        results.append(relative_error)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3604592"}]}