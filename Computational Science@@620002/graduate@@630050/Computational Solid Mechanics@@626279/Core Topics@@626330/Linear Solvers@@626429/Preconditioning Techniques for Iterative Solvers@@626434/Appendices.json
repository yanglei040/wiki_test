{"hands_on_practices": [{"introduction": "The theoretical properties of a stiffness matrix are directly tied to the physics of the underlying mechanical system. This first practice provides a concrete example of this connection by analyzing a simple one-dimensional bar. You will explore how the absence of boundary conditions leads to a singular matrix with a nullspace corresponding to rigid-body motion, and then quantify how different choices for applying constraints dramatically alter the system's spectral properties and condition number [@problem_id:3590165]. This exercise is fundamental to understanding how to formulate well-posed problems and serves as an intuitive entry point into the role of coarse-grid solvers in domain decomposition methods.", "problem": "A straight, homogeneous, prismatic one-dimensional bar of length $L$, cross-sectional area $A$, and Young’s modulus $E$ is modeled under small-strain, linear elasticity with no external loads. Using two equal linear finite elements, the bar is discretized into three nodes with free–free boundary conditions. Starting from Hooke’s law $\\sigma = E \\varepsilon$ and the total potential energy $\\Pi(u) = \\frac{1}{2}\\int_{0}^{L} E A \\left(\\frac{du}{dx}\\right)^{2} dx$, derive the assembled $3 \\times 3$ global stiffness matrix $A$ for this free–free system using the standard two-node linear shape functions and element-by-element assembly. Explain why $A$ is symmetric positive semi-definite with a one-dimensional nullspace corresponding to rigid-body translation.\n\nTo remove the nullspace, impose a single displacement constraint by pinning exactly one node (i.e., enforce a Dirichlet condition $u_{p}=0$ at node $p$). Form the reduced symmetric positive definite stiffness matrix $A_{p}$ by eliminating the row and column associated with node $p$. Consider two cases: pinning an end node ($p=1$) and pinning the middle node ($p=2$). For each case, compute all eigenvalues of $A_{p}$, and from these compute the spectral condition number $\\kappa_{2}(A_{p}) = \\lambda_{\\max}(A_{p}) / \\lambda_{\\min}(A_{p})$.\n\nFinally, quantify the conditioning improvement due to the placement of the single constraint by computing the ratio\n$$\nR \\,=\\, \\frac{\\kappa_{2}(A_{1})}{\\kappa_{2}(A_{2})}.\n$$\nExpress $R$ exactly in simplest radical form. No rounding is required. In your derivation, clearly connect how the constraint removes the nullspace and shifts the smallest eigenvalue from zero to a positive value, and briefly comment on the implications of constraint placement for preconditioner design in iterative solvers for computational solid mechanics (for example, the Conjugate Gradient method (CG) applied to Symmetric Positive Definite (SPD) systems).", "solution": "The problem requires the derivation and analysis of the stiffness matrices for a one-dimensional bar under different boundary conditions. The analysis will proceed in several steps: first, the derivation of the element stiffness matrix; second, the assembly of the global stiffness matrix for the free-free system; third, the analysis of this global matrix's properties; fourth, the imposition of constraints and analysis of the resulting reduced systems; and finally, the calculation of a ratio of condition numbers and a discussion of the implications.\n\nA one-dimensional bar of length $L$ is discretized using two linear finite elements of equal length $h = L/2$. This creates three nodes located at positions $x_1=0$, $x_2=L/2$, and $x_3=L$. The corresponding axial displacements are $u_1$, $u_2$, and $u_3$.\n\nThe strain energy for a single element of length $h$ is given by $\\Pi^{(e)} = \\frac{1}{2}\\int_{0}^{h} E A \\left(\\frac{du}{dx}\\right)^{2} dx$. For a two-node linear element, the displacement field is $u(x) = N_1(x) u_i + N_2(x) u_j$, where $u_i$ and $u_j$ are the nodal displacements. The linear shape functions are $N_1(x) = 1 - x/h$ and $N_2(x) = x/h$. The strain is $\\varepsilon = \\frac{du}{dx} = \\frac{d N_1}{dx} u_i + \\frac{d N_2}{dx} u_j = -\\frac{1}{h} u_i + \\frac{1}{h} u_j$. In matrix form, $\\varepsilon = \\mathbf{B} \\mathbf{u}^{(e)}$, where the strain-displacement matrix is $\\mathbf{B} = \\frac{1}{h} \\begin{pmatrix} -1 & 1 \\end{pmatrix}$ and the nodal displacement vector is $\\mathbf{u}^{(e)} = \\begin{pmatrix} u_i \\\\ u_j \\end{pmatrix}$.\n\nThe element stiffness matrix $\\mathbf{k}^{(e)}$ is defined by the relation $\\Pi^{(e)} = \\frac{1}{2} (\\mathbf{u}^{(e)})^{T} \\mathbf{k}^{(e)} \\mathbf{u}^{(e)}$. Substituting the expression for strain, and assuming constant $E$ and $A$:\n$$\n\\Pi^{(e)} = \\frac{1}{2}\\int_{0}^{h} E A \\left( (\\mathbf{B} \\mathbf{u}^{(e)})^T (\\mathbf{B} \\mathbf{u}^{(e)}) \\right) dx = \\frac{1}{2} (\\mathbf{u}^{(e)})^{T} \\left( \\int_{0}^{h} E A \\mathbf{B}^T \\mathbf{B} dx \\right) \\mathbf{u}^{(e)}\n$$\nSince $\\mathbf{B}$, $E$, and $A$ are constant over the element, the element stiffness matrix is:\n$$\n\\mathbf{k}^{(e)} = E A h \\mathbf{B}^T \\mathbf{B} = E A h \\left( \\frac{1}{h} \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix} \\right) \\left( \\frac{1}{h} \\begin{pmatrix} -1 & 1 \\end{pmatrix} \\right) = \\frac{EA}{h} \\begin{pmatrix} 1 & -1 \\\\ -1 & 1 \\end{pmatrix}\n$$\nFor our problem, the two elements have length $h = L/2$. Let the constant term be $C = \\frac{EA}{h} = \\frac{EA}{L/2} = \\frac{2EA}{L}$.\nElement $1$ connects nodes $1$ and $2$, so its stiffness matrix is $\\mathbf{k}^{(1)} = C \\begin{pmatrix} 1 & -1 \\\\ -1 & 1 \\end{pmatrix}$.\nElement $2$ connects nodes $2$ and $3$, so its stiffness matrix is $\\mathbf{k}^{(2)} = C \\begin{pmatrix} 1 & -1 \\\\ -1 & 1 \\end{pmatrix}$.\n\nThe $3 \\times 3$ global stiffness matrix $A$ is assembled by summing the contributions from each element into the appropriate global degrees of freedom:\n$$\nA = \\begin{pmatrix}\nk^{(1)}_{11} & k^{(1)}_{12} & 0 \\\\\nk^{(1)}_{21} & k^{(1)}_{22} + k^{(2)}_{11} & k^{(2)}_{12} \\\\\n0 & k^{(2)}_{21} & k^{(2)}_{22}\n\\end{pmatrix}\n= C \\begin{pmatrix}\n1 & -1 & 0 \\\\\n-1 & 1+1 & -1 \\\\\n0 & -1 & 1\n\\end{pmatrix}\n= \\frac{2EA}{L} \\begin{pmatrix}\n1 & -1 & 0 \\\\\n-1 & 2 & -1 \\\\\n0 & -1 & 1\n\\end{pmatrix}\n$$\nThis is the assembled global stiffness matrix for the free-free system.\n\nThe matrix $A$ is symmetric positive semi-definite. Symmetry ($A = A^T$) is evident from its construction. It is positive semi-definite because the quadratic form $\\mathbf{u}^T A \\mathbf{u}$ represents twice the total strain energy $\\Pi$, which is $\\int_{0}^{L} E A (\\frac{du}{dx})^2 dx$. Since $E>0$ and $A>0$, this energy is always non-negative. The energy is zero if and only if the strain $\\frac{du}{dx}$ is zero everywhere. This occurs when $u(x)$ is a constant, which corresponds to a rigid-body translation of the entire bar. For the discrete system, this means $u_1=u_2=u_3=c$ for any constant $c \\neq 0$. This non-zero displacement vector $\\mathbf{u} = c \\begin{pmatrix} 1 & 1 & 1 \\end{pmatrix}^T$ results in zero energy, meaning $\\mathbf{u}^T A \\mathbf{u} = 0$. Therefore, $A$ is positive semi-definite, not positive definite. The nullspace of $A$ consists of all such vectors, and it is a one-dimensional space spanned by the vector $\\mathbf{v} = \\begin{pmatrix} 1 & 1 & 1 \\end{pmatrix}^T$. We can verify this:\n$$\nA \\mathbf{v} = \\frac{2EA}{L} \\begin{pmatrix}\n1 & -1 & 0 \\\\\n-1 & 2 & -1 \\\\\n0 & -1 & 1\n\\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\frac{2EA}{L} \\begin{pmatrix} 1-1+0 \\\\ -1+2-1 \\\\ 0-1+1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\nThe existence of this nullspace (a zero eigenvalue) implies that $A$ is singular.\n\nTo obtain a unique solution, we must apply boundary conditions that remove the rigid-body motion. This is done by pinning a node.\n\nCase $1$: Pinning an end node, $p=1$.\nWe enforce the condition $u_1=0$. The reduced system is obtained by removing the first row and first column from $A$. The reduced stiffness matrix $A_1$ governs the remaining degrees of freedom, $u_2$ and $u_3$:\n$$\nA_1 = \\frac{2EA}{L} \\begin{pmatrix} 2 & -1 \\\\ -1 & 1 \\end{pmatrix}\n$$\nTo find the eigenvalues, let's analyze the matrix part $M_1 = \\begin{pmatrix} 2 & -1 \\\\ -1 & 1 \\end{pmatrix}$. The characteristic equation is $\\det(M_1 - \\lambda I) = 0$:\n$$\n(2-\\lambda)(1-\\lambda) - (-1)(-1) = 0 \\implies \\lambda^2 - 3\\lambda + 2 - 1 = 0 \\implies \\lambda^2 - 3\\lambda + 1 = 0\n$$\nThe roots are $\\lambda = \\frac{3 \\pm \\sqrt{9-4}}{2} = \\frac{3 \\pm \\sqrt{5}}{2}$.\nThe eigenvalues of $A_1$ are these values scaled by $C = \\frac{2EA}{L}$:\n$\\lambda_{\\min}(A_1) = \\frac{2EA}{L} \\left(\\frac{3-\\sqrt{5}}{2}\\right) = \\frac{EA}{L}(3-\\sqrt{5})$\n$\\lambda_{\\max}(A_1) = \\frac{2EA}{L} \\left(\\frac{3+\\sqrt{5}}{2}\\right) = \\frac{EA}{L}(3+\\sqrt{5})$\nBoth eigenvalues are positive, so $A_1$ is symmetric positive definite (SPD). The spectral condition number is:\n$$\n\\kappa_2(A_1) = \\frac{\\lambda_{\\max}(A_1)}{\\lambda_{\\min}(A_1)} = \\frac{3+\\sqrt{5}}{3-\\sqrt{5}} = \\frac{(3+\\sqrt{5})(3+\\sqrt{5})}{(3-\\sqrt{5})(3+\\sqrt{5})} = \\frac{9+6\\sqrt{5}+5}{9-5} = \\frac{14+6\\sqrt{5}}{4} = \\frac{7+3\\sqrt{5}}{2}\n$$\n\nCase $2$: Pinning the middle node, $p=2$.\nWe enforce $u_2=0$. The reduced system is obtained by removing the second row and second column from $A$. The reduced matrix $A_2$ governs degrees of freedom $u_1$ and $u_3$:\n$$\nA_2 = \\frac{2EA}{L} \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}\n$$\nThis matrix is diagonal. Its eigenvalues are the diagonal entries.\n$\\lambda_1 = \\lambda_2 = \\frac{2EA}{L}$.\nThus, $\\lambda_{\\min}(A_2) = \\lambda_{\\max}(A_2) = \\frac{2EA}{L}$. Both are positive, so $A_2$ is SPD.\nThe spectral condition number is:\n$$\n\\kappa_2(A_2) = \\frac{\\lambda_{\\max}(A_2)}{\\lambda_{\\min}(A_2)} = \\frac{2EA/L}{2EA/L} = 1\n$$\n\nFinally, we compute the ratio $R$:\n$$\nR = \\frac{\\kappa_2(A_1)}{\\kappa_2(A_2)} = \\frac{\\frac{7+3\\sqrt{5}}{2}}{1} = \\frac{7+3\\sqrt{5}}{2}\n$$\n\nImplications for preconditioning in computational solid mechanics:\nThe unconstrained matrix $A$ is singular ($\\kappa_2(A)=\\infty$) due to its nullspace corresponding to rigid-body motion. Iterative solvers like the Conjugate Gradient (CG) method require a symmetric positive definite (SPD) matrix and would fail on $A$. Imposing a single displacement constraint removes the nullspace, shifting the smallest eigenvalue from $0$ to a positive value, thus making the matrix SPD and the system solvable.\n\nThis analysis demonstrates that the choice of constraint placement has a profound impact on the conditioning of the resulting system matrix. Pinning the central node ($p=2$) results in an ideally conditioned matrix with $\\kappa_2(A_2)=1$. Physically, this decouples the system into two independent bars of length $L/2$, one fixed at $x=L/2$ and free at $x=0$, the other fixed at $x=L/2$ and free at $x=L$, leading to the diagonal matrix $A_2$. An iterative solver would converge in a minimal number of iterations for such a system. Pinning an end node ($p=1$) results in a coupled system with a condition number $\\kappa_2(A_1) \\approx 6.85$, which is well-conditioned but significantly worse than the optimal case.\n\nThis simple example illustrates a crucial principle in advanced preconditioning techniques for large-scale solid mechanics, such as in domain decomposition methods (e.g., FETI, BDD). In these methods, a global \"coarse problem\" is constructed specifically to handle the nullspace modes (rigid body motions) of floating subdomains. The way these subdomains are constrained in the coarse problem is analogous to choosing the pin location in our bar problem. A well-designed coarse problem, which often involves constraining average or central degrees of freedom, leads to a much better conditioned preconditioned system, dramatically accelerating the convergence of iterative solvers. The choice of constraints is not arbitrary but a critical aspect of algorithm design for performant computational mechanics simulations.", "answer": "$$\n\\boxed{\\frac{7+3\\sqrt{5}}{2}}\n$$", "id": "3590165"}, {"introduction": "While simple iterative methods like Jacobi or Gauss-Seidel can be effective for isotropic problems, their performance can degrade catastrophically in the presence of strong material anisotropy, a common feature in geomechanics and composite materials. This practice introduces Local Fourier Analysis (LFA), a powerful tool for predicting the performance of iterative methods, to diagnose the failure of a point-wise smoother. You will then see how a more sophisticated line smoother, which accounts for the directional coupling, can restore robust performance, illustrating a core principle of modern multigrid methods: the smoother must effectively damp all high-frequency error components [@problem_id:3552374].", "problem": "Consider steady anisotropic diffusion in a layered geomaterial with principal permeabilities aligned to the Cartesian axes, modeled by the operator $-\\nabla \\cdot ( \\mathbf{K} \\nabla u )$ with $\\mathbf{K} = \\operatorname{diag}(K_x, K_y)$, on a rectangular domain discretized by a uniform grid with spacing $h$ in both directions. Assume periodic boundary conditions to enable Local Fourier Analysis (LFA). Let $K_x = 1$ and $K_y = \\epsilon$ with $0 < \\epsilon \\ll 1$, representing an anisotropy ratio $\\kappa = K_x/K_y = 1/\\epsilon$. Using the standard five-point finite-difference discretization, the discrete operator acting on grid function $u_{i,j}$ has the form\n$$\n(Au)_{i,j} = \\frac{1}{h^2}\\Big( 2(1+\\epsilon) u_{i,j} - u_{i-1,j} - u_{i+1,j} - \\epsilon u_{i,j-1} - \\epsilon u_{i,j+1} \\Big).\n$$\nYou will analyze smoothing for high-frequency error components and design an effective line smoother.\n\nTasks:\n1) Starting from the definition of the point-Jacobi iteration $u^{(k+1)} = u^{(k)} + D^{-1}(f - Au^{(k)})$, where $D = \\operatorname{diag}(A)$, derive the error-propagation symbol $\\tilde{S}_J(\\theta_x,\\theta_y)$ by applying $I - D^{-1}A$ to a Fourier mode $e_{i,j} = \\exp(i(\\theta_x i + \\theta_y j))$. Then evaluate the amplification factor for the high-frequency mode with wavenumbers $(\\theta_x,\\theta_y) = (0,\\pi)$. Based on your derived expression, explain its behavior as $\\epsilon \\to 0$, and state whether point-Jacobi adequately smooths this mode.\n\n2) Construct an $x$-line symmetric Gauss–Seidel (GS) smoother as follows: for each row $j$, solve exactly the tridiagonal system in the $x$-direction defined by the $x$-couplings, and sweep forward in $y$ (using newly updated row $j-1$) and then backward in $y$ (using newly updated row $j+1$). Starting from the block form of the discrete equations, derive the LFA error-amplification factor for a generic Fourier mode $e_{i,j} = \\exp(i(\\theta_x i + \\theta_y j))$ after one complete symmetric line-GS sweep. Then evaluate the amplification factor magnitude for the same high-frequency mode $(\\theta_x,\\theta_y) = (0,\\pi)$ with anisotropy ratio $\\kappa = 10^3$ (that is, $\\epsilon = 10^{-3}$). \n\nGive your final answer as the exact value of this amplification factor magnitude; no rounding is required and no units are to be included.", "solution": "The problem is well-posed and scientifically grounded, allowing for a complete analysis. We address the two tasks in sequence.\n\nPart 1: Analysis of the Point-Jacobi Smoother\n\nThe point-Jacobi iteration for the linear system $Au = f$ is given by $u^{(k+1)} = u^{(k)} + D^{-1}(f - Au^{(k)})$, where $D$ is the diagonal part of the matrix $A$. The error $e^{(k)} = u - u^{(k)}$ propagates according to the equation $e^{(k+1)} = (I - D^{-1}A) e^{(k)}$. The matrix $S_J = I - D^{-1}A$ is the error propagation matrix for the point-Jacobi method. To perform Local Fourier Analysis (LFA), we analyze the effect of this operator on a single Fourier mode, $e_{i,j} = \\exp(i(\\theta_x i + \\theta_y j))$, where $\\theta_x, \\theta_y \\in [-\\pi, \\pi]$ are the wavenumbers.\n\nThe discrete operator $A$ is given by\n$$\n(Au)_{i,j} = \\frac{1}{h^2}\\Big( 2(1+\\epsilon) u_{i,j} - u_{i-1,j} - u_{i+1,j} - \\epsilon u_{i,j-1} - \\epsilon u_{i,j+1} \\Big).\n$$\nApplying $A$ to the Fourier mode $e_{i,j}$ yields:\n$$\n(Ae)_{i,j} = \\frac{1}{h^2} \\Big( 2(1+\\epsilon) - \\exp(-i\\theta_x) - \\exp(i\\theta_x) - \\epsilon\\exp(-i\\theta_y) - \\epsilon\\exp(i\\theta_y) \\Big) e_{i,j}\n$$\nUsing Euler's formula, $\\exp(i\\phi) + \\exp(-i\\phi) = 2\\cos(\\phi)$, we find the symbol (eigenvalue) $\\tilde{A}(\\theta_x, \\theta_y)$ of the operator $A$:\n$$\n\\tilde{A}(\\theta_x, \\theta_y) = \\frac{1}{h^2} \\Big( 2(1+\\epsilon) - 2\\cos(\\theta_x) - 2\\epsilon\\cos(\\theta_y) \\Big) = \\frac{2}{h^2} \\Big( (1-\\cos(\\theta_x)) + \\epsilon(1-\\cos(\\theta_y)) \\Big).\n$$\nThe diagonal part of $A$, denoted by $D$, is the term that multiplies $u_{i,j}$: $(Du)_{i,j} = \\frac{2(1+\\epsilon)}{h^2} u_{i,j}$. The symbol of $D$ is thus a constant:\n$$\n\\tilde{D} = \\frac{2(1+\\epsilon)}{h^2}.\n$$\nThe error-propagation symbol for point-Jacobi, $\\tilde{S}_J(\\theta_x, \\theta_y)$, is the symbol of $I - D^{-1}A$:\n$$\n\\tilde{S}_J(\\theta_x, \\theta_y) = 1 - \\frac{\\tilde{A}(\\theta_x, \\theta_y)}{\\tilde{D}} = 1 - \\frac{\\frac{2}{h^2} \\Big( (1+\\epsilon) - \\cos(\\theta_x) - \\epsilon\\cos(\\theta_y) \\Big)}{\\frac{2(1+\\epsilon)}{h^2}}.\n$$\nSimplifying the expression, we obtain:\n$$\n\\tilde{S}_J(\\theta_x, \\theta_y) = \\frac{(1+\\epsilon) - \\Big( (1+\\epsilon) - \\cos(\\theta_x) - \\epsilon\\cos(\\theta_y) \\Big)}{1+\\epsilon} = \\frac{\\cos(\\theta_x) + \\epsilon\\cos(\\theta_y)}{1+\\epsilon}.\n$$\nNow, we evaluate this amplification factor for the high-frequency mode with wavenumbers $(\\theta_x, \\theta_y) = (0, \\pi)$. This mode is smooth in the $x$-direction and highly oscillatory in the $y$-direction.\n$$\n\\tilde{S}_J(0, \\pi) = \\frac{\\cos(0) + \\epsilon\\cos(\\pi)}{1+\\epsilon} = \\frac{1 - \\epsilon}{1+\\epsilon}.\n$$\nTo understand the effectiveness of the smoother in the limit of strong anisotropy ($\\epsilon \\to 0$), we examine the behavior of this amplification factor:\n$$\n\\lim_{\\epsilon \\to 0} \\tilde{S}_J(0, \\pi) = \\lim_{\\epsilon \\to 0} \\frac{1 - \\epsilon}{1+\\epsilon} = 1.\n$$\nAn amplification factor of $1$ indicates that the error component corresponding to this mode is not damped at all. Therefore, point-Jacobi is not an adequate smoother for this problem, as its performance degrades catastrophically for modes that are oscillatory in the direction of weak coupling.\n\nPart 2: Analysis of the $x$-line Symmetric Gauss-Seidel Smoother\n\nFor an $x$-line smoother, we group the unknowns by rows (lines). The discrete equations can be written in a block tridiagonal form, $A u = f$. The matrix $A$ can be decomposed as $A = L+D+U$, where $D$ is the block-diagonal matrix whose blocks represent the intra-line couplings, and $L$ and $U$ are the strict block lower and upper triangular matrices representing inter-line couplings.\n$$ A = \\frac{1}{h^2} \\begin{pmatrix} T_x & -\\epsilon I & & \\\\ -\\epsilon I & T_x & -\\epsilon I & \\\\ & \\ddots & \\ddots & \\ddots \\\\ & & -\\epsilon I & T_x \\end{pmatrix} $$\nHere, $T_x$ is the tridiagonal matrix for a single line, with entries $(-1, 2(1+\\epsilon), -1)$. The symbol of the operator $h^2 D$ is the symbol of $T_x$, which we denote $\\tilde{T}_x(\\theta_x)$:\n$$\n\\tilde{T}_x(\\theta_x) = 2(1+\\epsilon) - 2\\cos(\\theta_x) = 4\\sin^2(\\frac{\\theta_x}{2}) + 2\\epsilon.\n$$\nThe symbols for the LFA of the block operators $h^2L$ and $h^2U$ are denoted $\\tilde{L}'(\\theta_y)$ and $\\tilde{U}'(\\theta_y)$:\n$$\n\\tilde{L}'(\\theta_y) = -\\epsilon\\exp(-i\\theta_y), \\quad \\tilde{U}'(\\theta_y) = -\\epsilon\\exp(i\\theta_y).\n$$\nThe symmetric Gauss-Seidel (SGS) smoother consists of a forward sweep followed by a backward sweep. The error propagation operator is $S_{SGS} = (D+U)^{-1}L(D+L)^{-1}U$. The corresponding LFA symbol is:\n$$\n\\tilde{S}_{SGS}(\\theta_x, \\theta_y) = (\\tilde{D}' + \\tilde{U}')^{-1} \\tilde{L}' (\\tilde{D}' + \\tilde{L}')^{-1} \\tilde{U}'.\n$$\nSubstituting the symbols for the block operators (scaled by $h^2$):\n$$\n\\tilde{S}_{SGS}(\\theta_x, \\theta_y) = \\frac{1}{\\tilde{T}_x(\\theta_x) - \\epsilon\\exp(i\\theta_y)} \\cdot \\big(-\\epsilon\\exp(-i\\theta_y)\\big) \\cdot \\frac{1}{\\tilde{T}_x(\\theta_x) - \\epsilon\\exp(-i\\theta_y)} \\cdot \\big(-\\epsilon\\exp(i\\theta_y)\\big).\n$$\n$$\n\\tilde{S}_{SGS}(\\theta_x, \\theta_y) = \\frac{\\epsilon^2}{\\big(\\tilde{T}_x(\\theta_x)-\\epsilon\\exp(i\\theta_y)\\big)\\big(\\tilde{T}_x(\\theta_x)-\\epsilon\\exp(-i\\theta_y)\\big)}.\n$$\nThe denominator is the product of a complex number and its conjugate, which results in its squared magnitude:\n$$\n\\tilde{S}_{SGS}(\\theta_x, \\theta_y) = \\frac{\\epsilon^2}{|\\tilde{T}_x(\\theta_x) - \\epsilon\\exp(i\\theta_y)|^2} = \\frac{\\epsilon^2}{(\\tilde{T}_x(\\theta_x) - \\epsilon\\cos(\\theta_y))^2 + (-\\epsilon\\sin(\\theta_y))^2}.\n$$\n$$\n\\tilde{S}_{SGS}(\\theta_x, \\theta_y) = \\frac{\\epsilon^2}{\\tilde{T}_x(\\theta_x)^2 - 2\\epsilon\\tilde{T}_x(\\theta_x)\\cos(\\theta_y) + \\epsilon^2}.\n$$\nWe are asked to evaluate the magnitude of this factor for the mode $(\\theta_x, \\theta_y) = (0, \\pi)$ with $\\kappa = 10^3$, which means $\\epsilon = 10^{-3}$. First, we evaluate the symbols at the specified wavenumbers.\nFor $\\theta_x = 0$:\n$$\n\\tilde{T}_x(0) = 4\\sin^2(0) + 2\\epsilon = 2\\epsilon.\n$$\nFor $\\theta_y = \\pi$:\n$$\n\\cos(\\pi) = -1.\n$$\nSubstituting these into the expression for $\\tilde{S}_{SGS}$:\n$$\n\\tilde{S}_{SGS}(0, \\pi) = \\frac{\\epsilon^2}{(2\\epsilon)^2 - 2\\epsilon(2\\epsilon)(-1) + \\epsilon^2} = \\frac{\\epsilon^2}{4\\epsilon^2 + 4\\epsilon^2 + \\epsilon^2} = \\frac{\\epsilon^2}{9\\epsilon^2} = \\frac{1}{9}.\n$$\nThis amplification factor is a real, positive constant independent of $\\epsilon$ (and thus $\\kappa$). The magnitude of the amplification factor for the mode $(0, \\pi)$ is therefore $|\\frac{1}{9}| = \\frac{1}{9}$. The specific value of $\\kappa = 10^3$ does not affect this result.", "answer": "$$\\boxed{\\frac{1}{9}}$$", "id": "3552374"}, {"introduction": "Ultimately, the choice of a preconditioner in a real-world simulation is a pragmatic one, balancing setup cost, per-iteration cost, and convergence rate. This final practice moves from theoretical analysis to computational economics by building a performance model for two popular preconditioner families: Incomplete LU (ILU) factorization and Algebraic Multigrid (AMG). By analyzing how their costs scale with problem size for a 3D elasticity problem, you will determine the optimal configuration for an ILU preconditioner and identify the crossover point where the superior scalability of AMG makes it the more efficient choice for large-scale analysis [@problem_id:3590222].", "problem": "Consider quasi-static, small-strain, linear elasticity on the unit cube with homogeneous Dirichlet boundary conditions. The strong form is $\\nabla \\cdot \\sigma = f$ with the constitutive relation $\\sigma = \\mathbb{C} : \\varepsilon(u)$, where $\\varepsilon(u) = (\\nabla u + \\nabla u^{\\top})/2$ and $\\mathbb{C}$ is a uniformly elliptic, bounded fourth-order tensor. A standard conforming finite element discretization yields a sparse, symmetric positive definite linear system $A u = b$ with $A \\in \\mathbb{R}^{N \\times N}$, where $N$ is the number of displacement unknowns. We consider a sequence of uniformly refined meshes indexed by $k \\in \\{0,1,2,\\dots\\}$, with $N_k = N_0 \\cdot 8^k$ for $N_0 = 1.5 \\times 10^4$.\n\nThe linear system is solved using Preconditioned Conjugate Gradient (PCG), applied to two alternative preconditioners:\n- Incomplete Lower-Upper factorization with threshold (ILUT) characterized by a drop tolerance $\\tau \\in (0,\\infty)$.\n- Algebraic Multigrid (AMG) using a single V-cycle per PCG iteration.\n\nAdopt the following modeling assumptions grounded in well-tested facts about computational complexity and convergence for three-dimensional elasticity:\n- The sparse matrix-vector product and vector operations in one PCG iteration scale linearly with $N$; model their cost as $a_0 N$ with $a_0 = 5.0 \\times 10^{-1}$ in normalized cost units.\n- The application of the ILUT preconditioner in one iteration is dominated by two triangular solves whose cost scales linearly with the number of nonzeros, which increases with decreasing $\\tau$. Model this as an additive term $a_1 \\tau^{-q} N$ with $a_1 = 5.0 \\times 10^{-2}$ and $q = 1$.\n- The ILUT setup (factorization) cost scales linearly with $N$ and with the same fill factor dependence; model it as $s_1 \\tau^{-q} N$ with $s_1 = 1.0 \\times 10^2$.\n- For three-dimensional elasticity, the PCG iteration count with ILU-type preconditioning grows with mesh refinement roughly like $N^{1/3}$, and weaker preconditioners (larger $\\tau$) increase the iteration count. Model the ILUT iteration count as $K_{\\mathrm{ILUT}}(N,\\tau) = b N^{1/3} \\tau^{q}$ with $b = 1$ and $q = 1$.\n- For AMG with a V-cycle, the per-iteration application cost is linear in $N$, and the iteration count is mesh-independent when the near-nullspace is correctly treated. Aggregate the AMG setup and solve cost into a single linear model $T_{\\mathrm{AMG}}(N) = g N$ with $g = 2.0 \\times 10^2$ in the same normalized units.\n\nUsing these assumptions, develop a simple cost model that balances per-iteration cost and iteration count for ILUT with drop tolerance $\\tau$, including its setup cost, and for AMG with V-cycles. Then:\n1. Derive the $\\tau$ that minimizes the total ILUT cost for a given $N$.\n2. Determine the smallest refinement level $k$ such that the total AMG cost is strictly less than the minimized total ILUT cost for $N_k = N_0 \\cdot 8^k$.\n\nExpress the final answer as the integer $k$ specified above. No rounding instruction is necessary because $k$ is an integer.", "solution": "The total computational cost for the ILUT-preconditioned solver, denoted $T_{\\mathrm{ILUT}}(N, \\tau)$, is the sum of a one-time setup (factorization) cost and the total cost of the iterative solve. The setup cost is given as $s_1 \\tau^{-q} N$. The total solve cost is the product of the number of iterations, $K_{\\mathrm{ILUT}}(N, \\tau)$, and the cost per iteration. The cost per PCG iteration is the sum of the sparse matrix-vector product and vector operations cost, $a_0 N$, and the cost of applying the preconditioner, $a_1 \\tau^{-q} N$.\n\nThe total cost for ILUT is therefore:\n$$\nT_{\\mathrm{ILUT}}(N, \\tau) = (\\text{Setup Cost}) + (\\text{Number of Iterations}) \\times (\\text{Cost per Iteration})\n$$\n$$\nT_{\\mathrm{ILUT}}(N, \\tau) = (s_1 \\tau^{-q} N) + K_{\\mathrm{ILUT}}(N, \\tau) \\times (a_0 N + a_1 \\tau^{-q} N)\n$$\nSubstituting the provided models for the number of iterations, $K_{\\mathrm{ILUT}}(N, \\tau) = b N^{1/3} \\tau^{q}$, and the given parameter values $b=1$ and $q=1$:\n$$\nT_{\\mathrm{ILUT}}(N, \\tau) = s_1 N \\tau^{-1} + (N^{1/3} \\tau) (a_0 N + a_1 N \\tau^{-1})\n$$\nExpanding the expression, we obtain the total cost as a function of the number of degrees of freedom $N$ and the drop tolerance $\\tau$:\n$$\nT_{\\mathrm{ILUT}}(N, \\tau) = s_1 N \\tau^{-1} + a_0 N^{4/3} \\tau + a_1 N^{4/3}\n$$\nTo find the optimal drop tolerance $\\tau_{\\mathrm{opt}}$ that minimizes this cost for a given $N$, we compute the first derivative of $T_{\\mathrm{ILUT}}$ with respect to $\\tau$ and set it to zero.\n$$\n\\frac{\\partial T_{\\mathrm{ILUT}}}{\\partial \\tau} = -s_1 N \\tau^{-2} + a_0 N^{4/3} = 0\n$$\nSolving for $\\tau^2$:\n$$\na_0 N^{4/3} = s_1 N \\tau^{-2} \\implies \\tau^2 = \\frac{s_1 N}{a_0 N^{4/3}} = \\frac{s_1}{a_0 N^{1/3}}\n$$\nSince $\\tau$ must be positive, we take the positive square root:\n$$\n\\tau_{\\mathrm{opt}}(N) = \\sqrt{\\frac{s_1}{a_0}} N^{-1/6}\n$$\nThe second derivative, $\\frac{\\partial^2 T_{\\mathrm{ILUT}}}{\\partial \\tau^2} = 2 s_1 N \\tau^{-3}$, is positive for all valid parameters ($s_1 > 0$, $N > 0$, $\\tau > 0$), confirming that this value of $\\tau$ corresponds to a local minimum.\n\nNext, we substitute $\\tau_{\\mathrm{opt}}$ back into the cost function $T_{\\mathrm{ILUT}}(N, \\tau)$ to find the minimized ILUT cost, $T_{\\mathrm{ILUT, min}}(N)$.\n$$\nT_{\\mathrm{ILUT, min}}(N) = s_1 N (\\tau_{\\mathrm{opt}})^{-1} + a_0 N^{4/3} \\tau_{\\mathrm{opt}} + a_1 N^{4/3}\n$$\nThe terms are:\n$$\ns_1 N (\\tau_{\\mathrm{opt}})^{-1} = s_1 N \\left(\\sqrt{\\frac{a_0}{s_1}} N^{1/6}\\right) = \\sqrt{s_1 a_0} N^{7/6}\n$$\n$$\na_0 N^{4/3} \\tau_{\\mathrm{opt}} = a_0 N^{4/3} \\left(\\sqrt{\\frac{s_1}{a_0}} N^{-1/6}\\right) = \\sqrt{a_0 s_1} N^{4/3 - 1/6} = \\sqrt{a_0 s_1} N^{7/6}\n$$\nCombining these terms, the minimized total cost for ILUT is:\n$$\nT_{\\mathrm{ILUT, min}}(N) = 2 \\sqrt{a_0 s_1} N^{7/6} + a_1 N^{4/3}\n$$\nThe total cost for AMG is given by the linear model $T_{\\mathrm{AMG}}(N) = g N$. We need to find the smallest integer refinement level $k$ such that the total AMG cost is strictly less than the minimized total ILUT cost for $N_k = N_0 \\cdot 8^k$. The condition is:\n$$\nT_{\\mathrm{AMG}}(N_k) < T_{\\mathrm{ILUT, min}}(N_k)\n$$\n$$\ng N_k < 2 \\sqrt{a_0 s_1} N_k^{7/6} + a_1 N_k^{4/3}\n$$\nSince $N_k > 0$, we can divide the inequality by $N_k$:\n$$\ng < 2 \\sqrt{a_0 s_1} N_k^{1/6} + a_1 N_k^{1/3}\n$$\nLet's substitute the given numerical values: $g=2.0 \\times 10^2$, $a_0 = 5.0 \\times 10^{-1}$, $s_1 = 1.0 \\times 10^2$, and $a_1 = 5.0 \\times 10^{-2}$.\n$$\n200 < 2 \\sqrt{(0.5)(100)} N_k^{1/6} + 0.05 N_k^{1/3}\n$$\n$$\n200 < 2 \\sqrt{50} N_k^{1/6} + 0.05 N_k^{1/3}\n$$\n$$\n200 < 10 \\sqrt{2} N_k^{1/6} + 0.05 N_k^{1/3}\n$$\nNow, substitute $N_k = N_0 \\cdot 8^k = (1.5 \\times 10^4) \\cdot 8^k$.\n$$\nN_k^{1/6} = (N_0 \\cdot 8^k)^{1/6} = N_0^{1/6} \\cdot (8^{1/6})^k = N_0^{1/6} \\cdot (\\sqrt{2})^k = N_0^{1/6} \\cdot 2^{k/2}\n$$\n$$\nN_k^{1/3} = (N_0 \\cdot 8^k)^{1/3} = N_0^{1/3} \\cdot (8^{1/3})^k = N_0^{1/3} \\cdot 2^k\n$$\nThe inequality becomes:\n$$\n200 < 10 \\sqrt{2} \\cdot N_0^{1/6} \\cdot 2^{k/2} + 0.05 \\cdot N_0^{1/3} \\cdot 2^k\n$$\nLet's evaluate the coefficients involving $N_0 = 1.5 \\times 10^4$:\n$$\nN_0^{1/6} = (15000)^{1/6} \\approx 4.9635\n$$\n$$\nN_0^{1/3} = (15000)^{1/3} \\approx 24.6621\n$$\nThe inequality is approximately:\n$$\n200 < 10 \\sqrt{2} (4.9635) \\cdot 2^{k/2} + 0.05 (24.6621) \\cdot 2^k\n$$\n$$\n200 < (70.194) \\cdot 2^{k/2} + (1.2331) \\cdot 2^k\n$$\nLet $x = 2^{k/2}$. Since $k \\ge 0$, we have $x \\ge 1$. The inequality transforms into a quadratic in $x$:\n$$\n1.2331 x^2 + 70.194 x - 200 > 0\n$$\nTo find where this inequality holds, we find the roots of the quadratic equation $1.2331 x^2 + 70.194 x - 200 = 0$.\n$$\nx = \\frac{-70.194 \\pm \\sqrt{70.194^2 - 4(1.2331)(-200)}}{2(1.2331)}\n$$\n$$\nx = \\frac{-70.194 \\pm \\sqrt{4927.2 + 986.48}}{2.4662} = \\frac{-70.194 \\pm \\sqrt{5913.68}}{2.4662} \\approx \\frac{-70.194 \\pm 76.900}{2.4662}\n$$\nSince $x = 2^{k/2}$ must be positive, we take the positive root:\n$$\nx \\approx \\frac{6.706}{2.4662} \\approx 2.7191\n$$\nThe quadratic is positive for $x > 2.7191$. We must find the smallest integer $k$ that satisfies this.\n$$\n2^{k/2} > 2.7191\n$$\nTaking the natural logarithm of both sides:\n$$\n\\frac{k}{2} \\ln(2) > \\ln(2.7191)\n$$\n$$\nk > \\frac{2 \\ln(2.7191)}{\\ln(2)} \\approx \\frac{2 (1.0003)}{0.6931} \\approx 2.8864\n$$\nSince $k$ must be an integer, the smallest integer value for $k$ that satisfies this condition is $3$.", "answer": "$$\\boxed{3}$$", "id": "3590222"}]}