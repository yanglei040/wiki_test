{"hands_on_practices": [{"introduction": "This first practice guides you through the fundamental derivation of the discrete adjoint method for a general linear system. By starting from first principles and applying the method of Lagrange multipliers, you will learn how to formulate the adjoint equation and derive an expression for the gradient that elegantly sidesteps the need to compute state sensitivities. Mastering this foundational derivation is the essential first step toward understanding and applying the adjoint method in any context. [@problem_id:2594547]", "problem": "Consider a linear, parameterized finite element method (FEM) model with discrete state vector $u(p) \\in \\mathbb{R}^{n}$ governed by the equilibrium equations\n$$\nK(p)\\,u(p) \\;=\\; f(p),\n$$\nwhere $K(p) \\in \\mathbb{R}^{n \\times n}$ is a parameter-dependent stiffness matrix that is nonsingular for the parameter values of interest, and $f(p) \\in \\mathbb{R}^{n}$ is the parameter-dependent load vector. Let the output functional be\n$$\nJ(u,p) \\;=\\; \\tfrac{1}{2}\\,u^{T}\\,Q(p)\\,u,\n$$\nwhere $Q(p) \\in \\mathbb{R}^{n \\times n}$ is symmetric and sufficiently smooth with respect to the scalar parameter $p \\in \\mathbb{R}$. Assume all functions are differentiable as needed.\n\nUsing only first principles (the chain rule, linearization of the state equation, and Lagrange multipliers), perform the following:\n\n1) Compute the partial derivative $\\frac{\\partial J}{\\partial u}$.\n\n2) Introduce an adjoint vector $\\lambda(p) \\in \\mathbb{R}^{n}$ to eliminate the dependence on the state sensitivity $\\frac{\\mathrm{d}u}{\\mathrm{d}p}$ from the total derivative $\\frac{\\mathrm{d}J}{\\mathrm{d}p}$. Starting from the residual $R(u,p) = K(p)\\,u - f(p) = 0$, derive:\n- An adjoint equation that defines $\\lambda(p)$, and\n- An explicit expression for $\\frac{\\mathrm{d}J}{\\mathrm{d}p}$ written entirely in terms of $K$, $Q$, $u$, and parameter derivatives with respect to $p$.\n\nDo not assume that $K(p)$ is symmetric. You may use the standard Euclidean inner product to form duality pairings.\n\n3) Briefly explain why your final gradient expression does not require solving for $\\frac{\\mathrm{d}u}{\\mathrm{d}p}$.\n\nProvide as your final answer a single closed-form analytical expression for $\\frac{\\mathrm{d}J}{\\mathrm{d}p}$ in terms of $K$, $Q$, $u$, $\\frac{\\partial K}{\\partial p}$, $\\frac{\\partial f}{\\partial p}$, $\\frac{\\partial Q}{\\partial p}$, and an adjoint vector $\\lambda$ that satisfies your adjoint equation. No numerical substitution is required. Your final answer must be a single symbolic expression and must not include any units.", "solution": "The problem as stated is subjected to validation before a solution is attempted.\n\nFirst, the givens are extracted verbatim.\n- State equation: $K(p)\\,u(p) \\;=\\; f(p)$, where $K(p) \\in \\mathbb{R}^{n \\times n}$ is nonsingular, $u(p) \\in \\mathbb{R}^{n}$, and $f(p) \\in \\mathbb{R}^{n}$.\n- Output functional: $J(u,p) \\;=\\; \\tfrac{1}{2}\\,u^{T}\\,Q(p)\\,u$, where $Q(p) \\in \\mathbb{R}^{n \\times n}$ is symmetric and smooth.\n- Parameter: $p \\in \\mathbb{R}$ is a scalar.\n- Assumptions: All functions are differentiable as needed. The matrix $K(p)$ is not assumed to be symmetric.\n- Tasks: $1$) Compute $\\partial J/\\partial u$. $2$) Derive the adjoint equation and an expression for $\\mathrm{d}J/\\mathrm{d}p$ that eliminates dependence on $\\mathrm{d}u/\\mathrm{d}p$. $3$) Explain the elimination of the $\\mathrm{d}u/\\mathrm{d}p$ term.\n\nSecond, the problem is validated against the required criteria.\n- **Scientifically Grounded**: The problem is a standard exercise in sensitivity analysis using the adjoint method, a fundamental technique in optimization, optimal control, and computational engineering. It is firmly based on established principles of vector calculus and linear algebra.\n- **Well-Posed**: The problem is well-posed. The a priori assumption that $K(p)$ is nonsingular ensures that the state vector $u(p)$ is uniquely defined. The required functional derivatives and the adjoint system are uniquely determinable under the given smoothness assumptions.\n- **Objective**: The problem is stated in precise, unambiguous mathematical language.\n- **Completeness and Consistency**: The problem provides all necessary definitions and constraints. There are no internal contradictions.\n- **Feasibility and Structure**: The problem is a theoretical derivation and is entirely feasible. The structure is logical and guides a step-by-step derivation from first principles.\n\nThe problem is deemed valid as it satisfies all criteria. A solution will now be constructed.\n\n**1) Computation of $\\frac{\\partial J}{\\partial u}$**\n\nThe output functional is given by $J(u,p) = \\frac{1}{2} u^T Q(p) u$. To compute the partial derivative of the scalar $J$ with respect to the vector $u$, we consider the differential $\\mathrm{d}J$ for a perturbation $\\mathrm{d}u$ at a fixed parameter $p$.\n$$\n\\mathrm{d}J = \\frac{1}{2} (\\mathrm{d}u)^T Q u + \\frac{1}{2} u^T Q (\\mathrm{d}u)\n$$\nSince $(\\mathrm{d}u)^T Q u$ is a scalar, it is equal to its own transpose: $(\\mathrm{d}u)^T Q u = (u^T Q^T (\\mathrm{d}u))^T = u^T Q^T (\\mathrm{d}u)$. Substituting this into the first term gives:\n$$\n\\mathrm{d}J = \\frac{1}{2} u^T Q^T (\\mathrm{d}u) + \\frac{1}{2} u^T Q (\\mathrm{d}u) = \\frac{1}{2} u^T (Q^T + Q) \\mathrm{d}u\n$$\nThe problem states that the matrix $Q(p)$ is symmetric, so $Q^T = Q$. The expression simplifies to:\n$$\n\\mathrm{d}J = \\frac{1}{2} u^T (2Q) \\mathrm{d}u = u^T Q \\mathrm{d}u\n$$\nBy definition, the differential $\\mathrm{d}J$ is related to the partial derivative (a row vector, or covector) by $\\mathrm{d}J = \\frac{\\partial J}{\\partial u} \\mathrm{d}u$. By comparison, we identify the partial derivative of $J$ with respect to $u$ as:\n$$\n\\frac{\\partial J}{\\partial u} = u^T Q(p)\n$$\nThe gradient of $J$ with respect to $u$, denoted $\\nabla_u J$, is the transpose of this row vector, which would be the column vector $Q(p)u$.\n\n**2) Derivation of the Adjoint Equation and Sensitivity Expression**\n\nThe goal is to find the total derivative $\\frac{\\mathrm{d}J}{\\mathrm{d}p}$. The functional $J$ depends on $p$ both explicitly through $Q(p)$ and implicitly through the state vector $u(p)$. Applying the multivariable chain rule:\n$$\n\\frac{\\mathrm{d}J}{\\mathrm{d}p} = \\frac{\\partial J}{\\partial p} + \\frac{\\partial J}{\\partial u} \\frac{\\mathrm{d}u}{\\mathrm{d}p}\n$$\nThe first term, the explicit derivative, is computed by differentiating $J$ with respect to $p$ while holding $u$ constant:\n$$\n\\frac{\\partial J}{\\partial p} = \\frac{\\partial}{\\partial p} \\left( \\frac{1}{2} u^T Q(p) u \\right) = \\frac{1}{2} u^T \\frac{\\partial Q}{\\partial p} u\n$$\nSubstituting this and the result from part 1) gives the expression for the total derivative, which is known as the direct sensitivity formula:\n$$\n\\frac{\\mathrm{d}J}{\\mathrm{d}p} = \\frac{1}{2} u^T \\frac{\\partial Q}{\\partial p} u + u^T Q \\frac{\\mathrm{d}u}{\\mathrm{d}p}\n$$\nThis expression depends on the state sensitivity $\\frac{\\mathrm{d}u}{\\mathrm{d}p}$, which is computationally expensive to obtain as it requires solving a linear system for each parameter. The adjoint method circumvents this.\n\nWe use the method of Lagrange multipliers. The constraint is the state equation, written as a residual $R(u,p) = K(p)u - f(p) = 0$. We form the augmented functional $\\mathcal{L}$ by adjoining the constraint to the original functional $J$ using a vector of Lagrange multipliers $\\lambda \\in \\mathbb{R}^n$, which we will call the adjoint vector.\n$$\n\\mathcal{L}(u, p, \\lambda) = J(u, p) + \\lambda^T R(u, p) = \\frac{1}{2} u^T Q u + \\lambda^T (Ku - f)\n$$\nSince the state equation is always satisfied ($R(u(p), p) = 0$), we have $\\mathcal{L} = J$ for any choice of $\\lambda$. Therefore, their total derivatives with respect to $p$ are equal: $\\frac{\\mathrm{d}J}{\\mathrm{d}p} = \\frac{\\mathrm{d}\\mathcal{L}}{\\mathrm{d}p}$. We compute $\\frac{\\mathrm{d}\\mathcal{L}}{\\mathrm{d}p}$ by applying the chain rule to $\\mathcal{L}$ as a function of $u$, $p$, and $\\lambda$:\n$$\n\\frac{\\mathrm{d}\\mathcal{L}}{\\mathrm{d}p} = \\frac{\\partial \\mathcal{L}}{\\partial p} + \\frac{\\partial \\mathcal{L}}{\\partial u} \\frac{\\mathrm{d}u}{\\mathrm{d}p} + \\frac{\\partial \\mathcal{L}}{\\partial \\lambda} \\frac{\\mathrm{d}\\lambda}{\\mathrm{d}p}\n$$\nThe core of the adjoint method is to choose $\\lambda$ to annihilate the term containing the state sensitivity $\\frac{\\mathrm{d}u}{\\mathrm{d}p}$. This is achieved by setting its coefficient to zero:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial u} = 0\n$$\nLet us compute this partial derivative:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial u} = \\frac{\\partial}{\\partial u} \\left( \\frac{1}{2} u^T Q u + \\lambda^T K u - \\lambda^T f \\right) = u^T Q + \\lambda^T K\n$$\nSetting this to zero yields the condition $u^T Q + \\lambda^T K = 0$. Transposing this equation gives the standard form of the **adjoint equation**:\n$$\n(u^T Q + \\lambda^T K)^T = 0^T \\implies Q^T u + K^T \\lambda = 0\n$$\nSince $Q$ is symmetric ($Q^T = Q$), we have:\n$$\nK^T \\lambda = -Q u\n$$\nThis is a linear system of equations that defines the adjoint vector $\\lambda(p)$. Notice it involves the transpose of the stiffness matrix, $K^T$, and its solution requires the state vector $u$.\n\nWith this specific choice of $\\lambda$, the term $\\frac{\\partial \\mathcal{L}}{\\partial u} \\frac{\\mathrm{d}u}{\\mathrm{d}p}$ in the expression for $\\frac{\\mathrm{d}\\mathcal{L}}{\\mathrm{d}p}$ becomes zero. Furthermore, the term $\\frac{\\partial \\mathcal{L}}{\\partial \\lambda}$ is simply $R^T$. As the state equation $R=0$ must hold, this term is also zero, so $\\frac{\\partial \\mathcal{L}}{\\partial \\lambda} \\frac{\\mathrm{d}\\lambda}{\\mathrm{d}p} = 0$ regardless of $\\frac{\\mathrm{d}\\lambda}{\\mathrm{d}p}$.\n\nThe total derivative of the functional thus simplifies to:\n$$\n\\frac{\\mathrm{d}J}{\\mathrm{d}p} = \\frac{\\mathrm{d}\\mathcal{L}}{\\mathrm{d}p} = \\frac{\\partial \\mathcal{L}}{\\partial p}\n$$\nWe now compute the partial derivative of $\\mathcal{L}$ with respect to $p$:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial p} = \\frac{\\partial}{\\partial p} \\left( \\frac{1}{2} u^T Q(p) u + \\lambda^T (K(p)u - f(p)) \\right) = \\frac{1}{2} u^T \\frac{\\partial Q}{\\partial p} u + \\lambda^T \\left( \\frac{\\partial K}{\\partial p} u - \\frac{\\partial f}{\\partial p} \\right)\n$$\nThis gives the final sensitivity expression:\n$$\n\\frac{\\mathrm{d}J}{\\mathrm{d}p} = \\frac{1}{2} u^T \\frac{\\partial Q}{\\partial p} u + \\lambda^T \\left( \\frac{\\partial K}{\\partial p} u - \\frac{\\partial f}{\\partial p} \\right)\n$$\n\n**3) Explanation for the Elimination of $\\frac{\\mathrm{d}u}{\\mathrm{d}p}$**\n\nThe final expression for the gradient $\\frac{\\mathrm{d}J}{\\mathrm{d}p}$ does not require solving for the state sensitivity vector $\\frac{\\mathrm{d}u}{\\mathrm{d}p}$ because of the specific construction of the adjoint problem. By introducing the Lagrange multiplier (adjoint) vector $\\lambda$ and forming the augmented functional $\\mathcal{L}$, we gain an additional degree of freedom. This freedom is used to impose the adjoint equation, $K^T\\lambda = -Qu$. This equation is specifically designed to be the condition that makes the coefficient of the $\\frac{\\mathrm{d}u}{\\mathrm{d}p}$ term in the chain rule expansion of $\\frac{\\mathrm{d}\\mathcal{L}}{\\mathrm{d}p}$ equal to zero. By satisfying this adjoint equation, the dependence of the total derivative $\\frac{\\mathrm{d}J}{\\mathrm{d}p}$ on the state sensitivity $\\frac{\\mathrm{d}u}{\\mathrm{d}p}$ is algebraically eliminated, leaving only terms that depend on the state $u$, the adjoint state $\\lambda$, and the direct partial derivatives of the problem data ($K$, $f$, $Q$) with respect to the parameter $p$. This constitutes the primary advantage of the adjoint method for sensitivity analysis involving a large number of parameters, as one only needs to solve one state system and one adjoint system, rather than a new system for each parameter's sensitivity.", "answer": "$$\n\\boxed{\\frac{1}{2} u^T \\frac{\\partial Q}{\\partial p} u + \\lambda^T \\left( \\frac{\\partial K}{\\partial p} u - \\frac{\\partial f}{\\partial p} \\right)}\n$$", "id": "2594547"}, {"introduction": "Building on the abstract theory, this exercise transitions to a concrete implementation for a 1D elastic bar. You will see how the continuous problem is discretized and how the adjoint-based sensitivity can be computed efficiently by summing local contributions in an element-by-element loop. This practice demonstrates a core technique in finite element programming that avoids the formation and storage of large, dense sensitivity matrices, highlighting the computational power of the adjoint method. [@problem_id:3543020]", "problem": "Consider a one-dimensional small-strain, linear elastic bar of length $L$, cross-sectional area $A$, and Young’s modulus field $E(x,\\theta)=E_0+\\theta\\,\\psi(x)$ where $\\theta$ is a scalar design parameter and $\\psi(x)$ is a prescribed, elementwise constant field over a Finite Element Method (FEM) mesh of $N$ equal-length two-node linear elements. The bar is fixed at the left end and is subject to an externally applied nodal force $F$ at the right end. Let the discrete displacement vector be $u\\in\\mathbb{R}^{N+1}$, and define the discrete residual vector $R(u,\\theta)\\in\\mathbb{R}^{N+1}$ to represent the algebraic balance of internal and external forces arising from the FEM discretization of the governing Partial Differential Equation (PDE). Assuming small-strain linear elasticity and Hooke’s law, the element stiffness matrix for element $e$ with nodes $i$ and $j$ is given by the standard two-node axial form proportional to $\\frac{EA}{h}$, with $h=L/N$ the element length, and the global stiffness matrix $K(\\theta)$ is assembled by summing the elemental contributions.\n\nDefine the scalar objective functional\n$$\nJ(u,\\theta)=f^\\top u+\\alpha\\,\\theta\\int_0^L \\psi(x)\\,A\\,dx,\n$$\nwhere $f\\in\\mathbb{R}^{N+1}$ is the global external force vector (with $f$ nonzero only at the rightmost node), and $\\alpha$ is a given dimensionless constant. The functional $J$ includes the compliance term $f^\\top u$ and an explicit regularization term linear in $\\theta$.\n\nYour tasks are:\n\n1. Starting from the fundamental balance of linear momentum in one dimension and Hooke’s law for small-strain linear elasticity, derive the discrete residual $R(u,\\theta)=K(\\theta)u-f$ and the adjoint equation needed to express the total sensitivity of $J$ with respect to the scalar parameter $\\theta$ without forming any dense sensitivity matrices. Clearly state the adjoint vector in terms of the primal quantities for this symmetric linear problem.\n\n2. From first principles, derive how to compute the scalar sensitivity $\\frac{dJ}{d\\theta}$ using element-level quantities and adjoint-based contraction. Show how to:\n   - Assemble the explicit derivative term $\\partial_\\theta J$ using an element loop.\n   - Assemble the contraction term involving the adjoint vector and the derivative of the residual with respect to $\\theta$, $(\\partial_\\theta R)^\\top \\Lambda$, using an element loop and only local two-by-two elemental matrices and local slices of $u$ and $\\Lambda$.\n\n3. Implement a complete, self-contained program that:\n   - Assembles $K(\\theta)$ by looping over elements and placing $2\\times 2$ elemental contributions into the global matrix.\n   - Imposes a Dirichlet boundary condition $u(0)=0$ at the left end by eliminating the corresponding degree of freedom and solves for the unknown displacements.\n   - Assembles the adjoint vector via solving the appropriate linear system.\n   - Computes $\\partial_\\theta J$ by summing elementwise contributions $\\alpha\\,\\psi_e\\,A\\,h$ where $\\psi_e$ is the elementwise constant value of $\\psi(x)$ on element $e$.\n   - Computes $(\\partial_\\theta R)^\\top \\Lambda$ using an elementwise contraction $\\lambda_e^\\top (k'_e\\,u_e)$ where $k'_e$ is the elementwise derivative of the stiffness matrix with respect to $\\theta$ and $(u_e,\\lambda_e)$ are the local displacement and adjoint subvectors for element $e$.\n   - Returns $\\frac{dJ}{d\\theta}$ for each test case using the adjoint-based expression derived in part 2, without forming any dense global sensitivity matrices.\n\nPhysical units: take $L$ in m, $A$ in $\\text{m}^2$, $E_0$ and $\\psi$ in Pa, $F$ in N, and displacements in m. The sensitivity $\\frac{dJ}{d\\theta}$ must be expressed in N$\\cdot$m. No angles are involved. Discretize the bar into equal-length elements with $h=L/N$.\n\nTest suite. Your program must compute $\\frac{dJ}{d\\theta}$ for the following four parameter sets:\n\n- Case 1 (general case): $N=5$, $L=1.0$ m, $A=0.01$ $\\text{m}^2$, $E_0=200\\times 10^9$ Pa, $\\theta=0.2$, $F=1000$ N, $\\alpha=1.0$, and $\\psi_e$ linearly increasing over elements from $10\\times 10^9$ Pa at $e=0$ to $50\\times 10^9$ Pa at $e=4$.\n\n- Case 2 (base parameter): same as Case 1 but with $\\theta=0.0$.\n\n- Case 3 (zero load edge case): $N=3$, $L=2.0$ m, $A=0.02$ $\\text{m}^2$, $E_0=70\\times 10^9$ Pa, $\\theta=0.5$, $F=0$ N, $\\alpha=0.5$, and $\\psi_e=5\\times 10^9$ Pa for all elements.\n\n- Case 4 (single-element boundary case with negative parameter): $N=1$, $L=1.0$ m, $A=0.01$ $\\text{m}^2$, $E_0=200\\times 10^9$ Pa, $\\theta=-0.1$, $F=500$ N, $\\alpha=2.0$, and $\\psi_e=30\\times 10^9$ Pa for the single element.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each entry should be a float in scientific notation with six digits after the decimal point, in N$\\cdot$m, representing $\\frac{dJ}{d\\theta}$ for the cases listed above in order. For example, the format must be exactly like $[x_1,x_2,x_3,x_4]$ where each $x_i$ is of the form $a.bbbbbbe\\pm cc$.", "solution": "The problem requires the derivation and implementation of an adjoint-based sensitivity analysis for a one-dimensional linear elastic bar. The goal is to compute the total sensitivity of a scalar objective functional, $J$, with respect to a design parameter, $\\theta$, without computing any dense sensitivity matrices.\n\n### Task 1: Derivation of the Discrete Residual and Adjoint Equation\n\nThe mechanical system is governed by the one-dimensional balance of linear momentum, which, for static equilibrium and no body forces, is given by the strong form:\n$$\n-\\frac{d}{dx}\\left(\\sigma(x) A\\right) = 0 \\quad \\text{for } x \\in (0, L)\n$$\nwhere $\\sigma(x)$ is the axial stress and $A$ is the cross-sectional area. The constitutive relation is Hooke's law for small-strain linear elasticity, $\\sigma(x) = E(x, \\theta) \\epsilon(x)$, where $\\epsilon(x) = \\frac{du}{dx}$ is the axial strain and $u(x)$ is the displacement field. The Young's modulus is dependent on a design parameter $\\theta$ as $E(x,\\theta)=E_0+\\theta\\,\\psi(x)$. The governing equation is thus:\n$$\n-\\frac{d}{dx}\\left(A (E_0+\\theta\\,\\psi(x)) \\frac{du}{dx}\\right) = 0\n$$\nThe boundary conditions are an essential (Dirichlet) condition at the left end, $u(0)=0$, and a natural (Neumann) condition at the right end, $A E(L, \\theta) \\frac{du}{dx}|_{x=L} = F$.\n\nTo derive the Finite Element Method (FEM) formulation, we start with the weak form. Multiplying the PDE by a test function $w(x)$ (where $w(0)=0$) and integrating over the domain $[0,L]$ gives:\n$$\n\\int_0^L -\\frac{d}{dx}\\left(A E \\frac{du}{dx}\\right) w(x) \\,dx = 0\n$$\nIntegrating by parts, we get:\n$$\n\\int_0^L \\left(A E \\frac{du}{dx}\\right)\\frac{dw}{dx} \\,dx - \\left[A E \\frac{du}{dx} w(x)\\right]_0^L = 0\n$$\nApplying the boundary conditions $w(0)=0$ and $A E(L, \\theta)\\frac{du}{dx}|_{x=L}=F$, this simplifies to:\n$$\n\\int_0^L \\frac{dw}{dx} A E(x,\\theta) \\frac{du}{dx} \\,dx = w(L)F\n$$\nWe discretize the domain into $N$ linear two-node elements of equal length $h=L/N$. The displacement field is approximated as $u(x) \\approx \\sum_{i=0}^{N} u_i N_i(x)$, where $u_i$ are the nodal displacements and $N_i(x)$ are the piecewise linear shape functions. Substituting this approximation for both the trial function $u$ and the test function $w$ leads to the discrete system of linear algebraic equations:\n$$\nK(\\theta)u = f\n$$\nHere, $u \\in \\mathbb{R}^{N+1}$ is the vector of nodal displacements, $f \\in \\mathbb{R}^{N+1}$ is the global external force vector with a single non-zero entry $F$ at the last node, and $K(\\theta) \\in \\mathbb{R}^{(N+1)\\times(N+1)}$ is the global stiffness matrix assembled from element stiffness matrices $k_e$. For an element $e$ spanning nodes $i$ and $j$, with constant modulus $E_e=E_0+\\theta\\psi_e$, the stiffness matrix is:\n$$\nk_e(\\theta) = \\frac{E_e A}{h} \\begin{pmatrix} 1 & -1 \\\\ -1 & 1 \\end{pmatrix} = \\frac{(E_0+\\theta\\psi_e)A}{h} \\begin{pmatrix} 1 & -1 \\\\ -1 & 1 \\end{pmatrix}\n$$\nThe discrete residual vector, representing the balance of forces at each node, is defined as:\n$$\nR(u, \\theta) = K(\\theta)u - f\n$$\nAt equilibrium, $R(u(\\theta), \\theta) = 0$.\n\nThe objective functional is given as $J(u, \\theta) = f^\\top u + \\alpha\\,\\theta\\int_0^L \\psi(x)\\,A\\,dx$. We seek its total derivative with respect to $\\theta$:\n$$\n\\frac{dJ}{d\\theta} = \\frac{\\partial J}{\\partial u} \\frac{du}{d\\theta} + \\frac{\\partial J}{\\partial \\theta}\n$$\nTo avoid computing the sensitivity matrix $\\frac{du}{d\\theta}$, we use the adjoint method. We differentiate the equilibrium equation $R(u(\\theta), \\theta) = 0$ with respect to $\\theta$:\n$$\n\\frac{dR}{d\\theta} = \\frac{\\partial R}{\\partial u} \\frac{du}{d\\theta} + \\frac{\\partial R}{\\partial \\theta} = 0 \\implies \\frac{du}{d\\theta} = - \\left(\\frac{\\partial R}{\\partial u}\\right)^{-1} \\frac{\\partial R}{\\partial \\theta}\n$$\nSubstituting this into the expression for $\\frac{dJ}{d\\theta}$:\n$$\n\\frac{dJ}{d\\theta} = \\frac{\\partial J}{\\partial \\theta} - \\frac{\\partial J}{\\partial u} \\left(\\frac{\\partial R}{\\partial u}\\right)^{-1} \\frac{\\partial R}{\\partial \\theta}\n$$\nWe define the adjoint vector $\\Lambda$ to eliminate the matrix inversion. The adjoint equation is:\n$$\n\\left(\\frac{\\partial R}{\\partial u}\\right)^\\top \\Lambda = \\left(\\frac{\\partial J}{\\partial u}\\right)^\\top\n$$\nWith this definition, the term $\\frac{\\partial J}{\\partial u} (\\frac{\\partial R}{\\partial u})^{-1}$ can be rewritten as $\\Lambda^\\top$. Let's compute the partial derivatives:\n- $\\frac{\\partial R}{\\partial u} = K(\\theta)$\n- $\\frac{\\partial J}{\\partial u} = f^\\top$\n\nThe adjoint equation becomes $K(\\theta)^\\top \\Lambda = f$. Since the element stiffness matrices $k_e$ are symmetric, the assembled global stiffness matrix $K(\\theta)$ is also symmetric, so $K(\\theta)^\\top = K(\\theta)$. The adjoint equation is therefore:\n$$\nK(\\theta)\\Lambda = f\n$$\nThis is identical to the primal equilibrium equation $K(\\theta)u = f$. Since $K(\\theta)$ (after applying boundary conditions) is invertible, the solution is unique. Thus, for this specific problem, the adjoint vector is equal to the primal displacement vector:\n$$\n\\Lambda = u\n$$\nThe total sensitivity is now expressed as:\n$$\n\\frac{dJ}{d\\theta} = \\frac{\\partial J}{\\partial \\theta} - \\Lambda^\\top \\frac{\\partial R}{\\partial \\theta}\n$$\n\n### Task 2: Element-wise Computation of Sensitivity\n\nWe now formulate the computation of $\\frac{dJ}{d\\theta}$ using element-level quantities. The total sensitivity consists of two parts: the explicit derivative term $\\frac{\\partial J}{\\partial \\theta}$ and the contraction term $\\Lambda^\\top \\frac{\\partial R}{\\partial \\theta}$.\n\nThe explicit derivative term is obtained by differentiating $J$ with respect to $\\theta$ while holding $u$ constant:\n$$\n\\frac{\\partial J}{\\partial \\theta} = \\frac{\\partial}{\\partial \\theta} \\left( f^\\top u + \\alpha\\,\\theta\\int_0^L \\psi(x)\\,A\\,dx \\right) = \\alpha\\int_0^L \\psi(x)\\,A\\,dx\n$$\nSince $\\psi(x)$ is defined as an element-wise constant field, $\\psi_e$, the integral becomes a sum over the $N$ elements:\n$$\n\\frac{\\partial J}{\\partial \\theta} = \\alpha \\sum_{e=0}^{N-1} \\int_{x_e}^{x_{e+1}} \\psi_e A \\,dx = \\sum_{e=0}^{N-1} \\alpha\\,\\psi_e\\,A\\,h\n$$\nThis term can be readily assembled by iterating over the elements.\n\nThe second term involves the partial derivative of the residual with respect to $\\theta$:\n$$\n\\frac{\\partial R}{\\partial \\theta} = \\frac{\\partial}{\\partial \\theta} (K(\\theta)u - f) = \\frac{\\partial K(\\theta)}{\\partial \\theta} u = K'(\\theta)u\n$$\nThe contraction term is therefore $(\\partial_\\theta R)^\\top \\Lambda = (K'(\\theta)u)^\\top \\Lambda = u^\\top K'(\\theta)^\\top \\Lambda$. Since $K'(\\theta)$ is also symmetric, this is $u^\\top K'(\\theta) \\Lambda$. This global contraction can be computed by summing element-level contributions:\n$$\nu^\\top K'(\\theta) \\Lambda = \\sum_{e=0}^{N-1} u_e^\\top k'_e(\\theta) \\lambda_e\n$$\nwhere $u_e$ and $\\lambda_e$ are the local displacement and adjoint vectors for element $e$, and $k'_e(\\theta)$ is the derivative of the element stiffness matrix with respect to $\\theta$:\n$$\nk'_{e}(\\theta) = \\frac{\\partial}{\\partial \\theta} \\left( \\frac{(E_0+\\theta\\psi_e)A}{h} \\begin{pmatrix} 1 & -1 \\\\ -1 & 1 \\end{pmatrix} \\right) = \\frac{\\psi_e A}{h} \\begin{pmatrix} 1 & -1 \\\\ -1 & 1 \\end{pmatrix}\n$$\nThe full expression for the total sensitivity is:\n$$\n\\frac{dJ}{d\\theta} = \\sum_{e=0}^{N-1} \\alpha\\,\\psi_e\\,A\\,h - \\sum_{e=0}^{N-1} \\lambda_e^\\top k'_e u_e\n$$\nSince we found that $\\Lambda = u$, it follows that $\\lambda_e=u_e$ for all elements. The final expression used for implementation is:\n$$\n\\frac{dJ}{d\\theta} = \\sum_{e=0}^{N-1} \\left( \\alpha\\,\\psi_e\\,A\\,h - u_e^\\top k'_e u_e \\right)\n$$\nThis elegant result allows for the computation of the total sensitivity by first solving the primal problem for $u$, and then performing a single loop over elements to sum up the explicit derivative and the contraction contributions, completely avoiding the formation of any large sensitivity matrices.\n\n### Task 3: Implementation Strategy\n\nThe implementation will follow these steps for each test case:\n1.  Define problem parameters: $N, L, A, E_0, \\theta, F, \\alpha, \\psi_e$. Element length is $h=L/N$.\n2.  Assemble the global stiffness matrix $K(\\theta)$ of size $(N+1)\\times(N+1)$ by iterating through elements $e=0, \\dots, N-1$. For each element, compute $k_e(\\theta)$ and add its entries to the corresponding global degrees of freedom.\n3.  Construct the global force vector $f$, which is zero everywhere except for $f_N = F$.\n4.  Apply the Dirichlet boundary condition $u_0=0$ by eliminating the first row and column of the K-matrix and the first entry of the force vector.\n5.  Solve the reduced linear system $K_{reduced} u_{reduced} = f_{reduced}$ for the unknown displacements. Reconstruct the full displacement vector $u$.\n6.  Recognize that the adjoint vector $\\Lambda$ is identical to the primal displacement vector $u$, so set $\\Lambda=u$.\n7.  Initialize the total sensitivity $\\frac{dJ}{d\\theta}$ to zero.\n8.  Loop through the elements $e=0, \\dots, N-1$:\n    a. Calculate the explicit sensitivity contribution for the element: $s_e^{exp} = \\alpha\\,\\psi_e\\,A\\,h$.\n    b. Form the element stiffness derivative matrix $k'_e$.\n    c. Extract the local displacement/adjoint vectors $u_e = \\lambda_e = [u_i, u_{i+1}]$.\n    d. Calculate the element contraction term: $s_e^{contr} = \\lambda_e^\\top k'_e u_e$.\n    e. Update the total sensitivity: $\\frac{dJ}{d\\theta} \\leftarrow \\frac{dJ}{d\\theta} + s_e^{exp} - s_e^{contr}$.\n9.  Return the computed total sensitivity $\\frac{dJ}{d\\theta}$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef compute_sensitivity(N, L, A, E0, theta, F, alpha, psi_values):\n    \"\"\"\n    Computes the adjoint-based sensitivity dJ/dtheta for a 1D elastic bar.\n\n    Args:\n        N (int): Number of elements.\n        L (float): Length of the bar (m).\n        A (float): Cross-sectional area (m^2).\n        E0 (float): Base Young's modulus (Pa).\n        theta (float): Scalar design parameter (dimensionless).\n        F (float): Applied force at the right end (N).\n        alpha (float): Regularization constant (dimensionless).\n        psi_values (np.ndarray): Array of element-wise psi values (Pa).\n\n    Returns:\n        float: The computed sensitivity dJ/dtheta (N*m).\n    \"\"\"\n    num_nodes = N + 1\n    h = L / N\n\n    # 1. Assemble global stiffness matrix K(theta)\n    K = np.zeros((num_nodes, num_nodes))\n    for e in range(N):\n        # Element properties\n        psi_e = psi_values[e]\n        E_e = E0 + theta * psi_e\n        \n        # Element stiffness matrix\n        k_e_scalar = E_e * A / h\n        k_e = k_e_scalar * np.array([[1, -1], [-1, 1]])\n\n        # Assembly into global matrix\n        i, j = e, e + 1\n        K[i:i+2, i:i+2] += k_e\n\n    # 2. Define global force vector f\n    f = np.zeros(num_nodes)\n    f[-1] = F\n\n    # 3. Apply Dirichlet BC u(0) = 0 and solve for primal displacements u\n    # We solve K_reduced * u_reduced = f_reduced\n    K_reduced = K[1:, 1:]\n    f_reduced = f[1:]\n    \n    # Primal solve\n    try:\n        u_reduced = np.linalg.solve(K_reduced, f_reduced)\n    except np.linalg.LinAlgError:\n        # This may happen if K is singular, e.g., if F=0 and the body is unconstrained.\n        # But with one fixed BC, K_reduced should be invertible.\n        # If F=0, u_reduced will be all zeros.\n        u_reduced = np.zeros(N)\n\n    # Reconstruct full displacement vector\n    u = np.zeros(num_nodes)\n    u[1:] = u_reduced\n\n    # 4. Solve for adjoint vector Lambda\n    # For this self-adjoint problem, Lambda = u\n    Lambda = u\n\n    # 5. Compute dJ/dtheta using element-wise contributions\n    dJ_dtheta = 0.0\n    \n    # Element-wise loop for sensitivity calculation\n    for e in range(N):\n        # Nodes of the current element\n        i, j = e, e + 1\n        \n        # a) Explicit derivative of J w.r.t. theta\n        psi_e = psi_values[e]\n        dJ_dtheta_explicit_e = alpha * psi_e * A * h\n        \n        # b) Contraction term: lambda_e^T * k'_e * u_e\n        \n        # Element stiffness derivative matrix k'_e\n        k_prime_e_scalar = psi_e * A / h\n        k_prime_e = k_prime_e_scalar * np.array([[1, -1], [-1, 1]])\n        \n        # Local displacement and adjoint vectors\n        u_e = u[i:i+2]\n        lambda_e = Lambda[i:i+2]\n        \n        # Element contraction\n        contraction_e = lambda_e.T @ k_prime_e @ u_e\n        \n        # c) Accumulate total sensitivity\n        dJ_dtheta += (dJ_dtheta_explicit_e - contraction_e)\n        \n    return dJ_dtheta\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: General case\n        {'N': 5, 'L': 1.0, 'A': 0.01, 'E0': 200e9, 'theta': 0.2, 'F': 1000, \n         'alpha': 1.0, 'psi_values': np.linspace(10e9, 50e9, 5)},\n        \n        # Case 2: Base parameter (theta=0)\n        {'N': 5, 'L': 1.0, 'A': 0.01, 'E0': 200e9, 'theta': 0.0, 'F': 1000, \n         'alpha': 1.0, 'psi_values': np.linspace(10e9, 50e9, 5)},\n        \n        # Case 3: Zero load edge case\n        {'N': 3, 'L': 2.0, 'A': 0.02, 'E0': 70e9, 'theta': 0.5, 'F': 0, \n         'alpha': 0.5, 'psi_values': np.full(3, 5e9)},\n\n        # Case 4: Single-element boundary case with negative parameter\n        {'N': 1, 'L': 1.0, 'A': 0.01, 'E0': 200e9, 'theta': -0.1, 'F': 500, \n         'alpha': 2.0, 'psi_values': np.full(1, 30e9)}\n    ]\n\n    results = []\n    for case in test_cases:\n        result = compute_sensitivity(**case)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    formatted_results = [f\"{r:.6e}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3543020"}, {"introduction": "Real-world engineering systems are often nonlinear, requiring iterative solvers like the Newton-Raphson method. This final practice explores how the adjoint method is applied in a nonlinear setting and, more importantly, investigates a critical pitfall: gradient inconsistency. By intentionally using an approximate tangent from the forward solve to construct the adjoint system, you will quantify the resulting error in the gradient, a crucial lesson for developing and debugging robust sensitivity analyses in advanced computational models. [@problem_id:3543074]", "problem": "Consider a nonlinear static equilibrium in computational solid mechanics where the discrete equilibrium is stated as the root-finding problem $R(u,p)=0$, with $u\\in\\mathbb{R}^n$ the state (nodal displacements) and $p\\in\\mathbb{R}^m$ a parameter vector. Assume that the forward problem is solved by a Newton–Raphson (NR) iteration, which at iteration $k$ linearizes the residual about the current iterate $u^{(k)}$ and computes an update $\\Delta u^{(k)}$ from a linear system. The exact, consistent linearization uses the tangent $K(u,p)=\\frac{\\partial R}{\\partial u}(u,p)$, but in practice an approximate tangent $K^\\mathrm{approx}(u,p)\\neq\\frac{\\partial R}{\\partial u}(u,p)$ might be used for the iterations. Let a scalar objective $J(u,p)$ be defined on the state and parameters, for which the sensitivity with respect to $p$ is needed.\n\nYour tasks are as follows.\n\n- Using only fundamental definitions from nonlinear algebraic system solves and the chain rule, construct the discrete adjoint system associated with the converged NR solution $u^\\star(p)$ of $R(u,p)=0$ when the linearization is exact. From this, derive the expression for the gradient $\\nabla_p J(u^\\star(p),p)$ that depends on the adjoint solution.\n\n- Now suppose that, in the forward NR iterations, the linear solve uses an approximate tangent $K^\\mathrm{approx}(u,p)\\neq\\frac{\\partial R}{\\partial u}(u,p)$. Construct the corresponding adjoint system that one would obtain if the same approximate tangent is reused in the adjoint linear solve at the converged state. Compare this to the exact discrete adjoint system and identify the mathematical source of gradient inconsistency.\n\n- To quantify the impact, employ a manufactured, dimensionless one-degree-of-freedom nonlinear elasticity model that represents a two-node bar with a nonlinear constitutive response. Let the residual be\n$$\nR(u,p) \\;=\\; p\\,u \\;+\\; \\beta\\,u^3 \\;-\\; F,\n$$\nwith $u\\in\\mathbb{R}$ the single displacement unknown at the free node, $p\\in\\mathbb{R}$ a material stiffness-like parameter, $\\beta\\in\\mathbb{R}_{\\ge 0}$ a nonlinearity parameter, and $F\\in\\mathbb{R}$ an applied load. Take the scalar objective to be\n$$\nJ(u,p) \\;=\\; \\tfrac{1}{2}\\,u^2,\n$$\nwhich is a standard energy-like quantity. The manufactured structure is dimensionless; express all numerical outputs in dimensionless units.\n\n- In the forward solve, use a damped NR iteration that updates\n$$\nu^{(k+1)} \\;=\\; u^{(k)} \\;-\\; \\alpha^{(k)}\\left(K^\\mathrm{approx}(u^{(k)},p)\\right)^{-1} R(u^{(k)},p),\n$$\nwith a backtracking line-search factor $\\alpha^{(k)}\\in(0,1]$ chosen to reduce the residual norm. For the approximate tangent, use the parametric family\n$$\nK^\\mathrm{approx}(u,p) \\;=\\; p \\;+\\; c\\,\\beta\\,u^2,\n$$\nwhere $c\\in\\mathbb{R}$ is a user-specified coefficient. The exact tangent is $\\frac{\\partial R}{\\partial u}(u,p)=p+3\\beta u^2$.\n\n- After converging to $u^\\star(p)$, compute two gradients with respect to $p$ at that state: the exact discrete adjoint gradient obtained from solving the adjoint with the exact tangent, and the approximate gradient obtained from solving the adjoint with the approximate tangent. Then report the absolute error and a relative error measure between these two gradients.\n\nDesign a single program that, for each of the following test cases $(p,\\beta,F,c)$, performs the forward solve to obtain $u^\\star$, computes the exact and the approximate adjoint-based gradients with respect to $p$, and then reports $u^\\star$, the exact gradient, the approximate gradient, the absolute error, and the relative error. Use the following test suite, chosen to exercise a typical case, a linear case, a strongly nonlinear case, an over-stiff approximate tangent, and a zero-load edge case:\n\n- Test $1$: $(p,\\beta,F,c)=(2.0,1.0,1.0,0.0)$.\n- Test $2$: $(p,\\beta,F,c)=(1.0,0.0,1.2,0.0)$.\n- Test $3$: $(p,\\beta,F,c)=(0.5,5.0,0.2,1.0)$.\n- Test $4$: $(p,\\beta,F,c)=(0.2,1.0,0.01,6.0)$.\n- Test $5$: $(p,\\beta,F,c)=(3.0,2.0,0.0,0.0)$.\n\nAll quantities are dimensionless; express all outputs as dimensionless floating-point numbers.\n\nYour program should produce a single line of output containing the results as a comma-separated Python-style list of lists, each inner list corresponding to one test case in the given order and containing five floats in the order $[u^\\star,\\;\\text{grad\\_exact},\\;\\text{grad\\_approx},\\;\\text{abs\\_error},\\;\\text{rel\\_error}]$. For example, the output format must be exactly like\n$[\\,[u_1,g^\\text{exact}_1,g^\\text{approx}_1,e^\\text{abs}_1,e^\\text{rel}_1],\\,[u_2,\\dots],\\dots\\,]$.", "solution": "The problem requires the derivation of exact and inconsistent discrete adjoint sensitivity expressions for a general nonlinear system, followed by a numerical study of a specific one-degree-of-freedom model.\n\nThe general problem is to find the sensitivity of a scalar objective function $J(u,p)$ with respect to a parameter vector $p \\in \\mathbb{R}^m$, where the state vector $u \\in \\mathbb{R}^n$ is an implicit function of $p$ defined by the solution to a system of nonlinear algebraic equations, $R(u,p)=0$. The solution $u^\\star(p)$ is found at the converged limit of a Newton-Raphson (NR) iteration.\n\nFirst, we derive the expression for the exact discrete adjoint-based gradient. The total derivative of $J$ with respect to a parameter $p_i$ is given by the chain rule:\n$$\n\\frac{dJ}{dp_i} = \\frac{\\partial J}{\\partial p_i} + \\frac{\\partial J}{\\partial u} \\frac{du}{dp_i}\n$$\nThe sensitivity of the state with respect to the parameter, $\\frac{du}{dp_i}$, is unknown. It can be found by differentiating the governing equilibrium equation, $R(u(p), p) = 0$, with respect to $p_i$:\n$$\n\\frac{dR}{dp_i} = \\frac{\\partial R}{\\partial p_i} + \\frac{\\partial R}{\\partial u} \\frac{du}{dp_i} = 0\n$$\nLet $K = \\frac{\\partial R}{\\partial u}$ be the exact tangent stiffness matrix. The above equation can be rearranged to solve for the state sensitivity:\n$$\nK \\frac{du}{dp_i} = - \\frac{\\partial R}{\\partial p_i} \\implies \\frac{du}{dp_i} = -K^{-1} \\frac{\\partial R}{\\partial p_i}\n$$\nSubstituting this back into the expression for the total derivative of $J$ gives the direct differentiation formula for the gradient:\n$$\n\\frac{dJ}{dp_i} = \\frac{\\partial J}{\\partial p_i} - \\frac{\\partial J}{\\partial u} K^{-1} \\frac{\\partial R}{\\partial p_i}\n$$\nThe adjoint method provides an efficient alternative to this approach, especially when the number of parameters $m$ is large. We introduce a vector of adjoint variables, $\\lambda \\in \\mathbb{R}^n$, defined as the solution to the linear adjoint system:\n$$\nK^T \\lambda = \\left(\\frac{\\partial J}{\\partial u}\\right)^T\n$$\nBy solving for $\\lambda$ and taking the transpose, we find $\\lambda^T = \\frac{\\partial J}{\\partial u} K^{-1}$. Substituting this into the direct differentiation formula, we obtain the adjoint expression for the gradient:\n$$\n\\frac{dJ}{dp_i} = \\frac{\\partial J}{\\partial p_i} - \\lambda^T \\frac{\\partial R}{\\partial p_i}\n$$\nThis is the exact discrete adjoint sensitivity. It requires the solution of one linear system for $\\lambda$ regardless of the number of parameters, making it computationally advantageous. All partial derivatives and the tangent $K$ are evaluated at the converged state $(u^\\star, p)$.\n\nNext, we consider the case where the forward NR iterations use an approximate tangent $K^\\mathrm{approx}(u,p) \\neq K(u,p)$. In practice, to save computational cost, one might reuse $K^\\mathrm{approx}$ from the forward solve to construct an \"approximate\" adjoint system. This system is defined as:\n$$\n(K^\\mathrm{approx})^T \\lambda^\\mathrm{approx} = \\left(\\frac{\\partial J}{\\partial u}\\right)^T\n$$\nwhere $K^\\mathrm{approx}$ is evaluated at the converged state $u^\\star$. The resulting \"approximate\" gradient is then computed using this $\\lambda^\\mathrm{approx}$:\n$$\n\\left(\\frac{dJ}{dp_i}\\right)_\\mathrm{approx} = \\frac{\\partial J}{\\partial p_i} - (\\lambda^\\mathrm{approx})^T \\frac{\\partial R}{\\partial p_i}\n$$\nSubstituting the definition of $\\lambda^\\mathrm{approx}$ reveals the underlying expression:\n$$\n\\left(\\frac{dJ}{dp_i}\\right)_\\mathrm{approx} = \\frac{\\partial J}{\\partial p_i} - \\frac{\\partial J}{\\partial u} (K^\\mathrm{approx})^{-1} \\frac{\\partial R}{\\partial p_i}\n$$\nThe mathematical source of gradient inconsistency is now evident. Comparing the exact gradient $\\frac{dJ}{dp_i}$ with the approximate one, we see that the term $K^{-1}$ has been replaced by $(K^\\mathrm{approx})^{-1}$. The adjoint method is a formal mathematical identity that relies on the operator in the adjoint equation, $K^T$, being the exact transpose of the operator in the linearized state equation, $K = \\frac{\\partial R}{\\partial u}$. When an approximate tangent $K^\\mathrm{approx} \\neq \\frac{\\partial R}{\\partial u}$ is used, this correspondence is broken. The resulting sensitivity $(\\frac{dJ}{dp_i})_\\mathrm{approx}$ is no longer the true total derivative of $J$ with respect to $p_i$. The error in the gradient is given by:\n$$\n\\text{Error} = \\left(\\frac{dJ}{dp_i}\\right)_\\mathrm{approx} - \\frac{dJ}{dp_i} = \\frac{\\partial J}{\\partial u} \\left( K^{-1} - (K^\\mathrm{approx})^{-1} \\right) \\frac{\\partial R}{\\partial p_i}\n$$\nThis error is zero only if $K^\\mathrm{approx} = K$, or if $\\frac{\\partial J}{\\partial u}$ or $\\frac{\\partial R}{\\partial p_i}$ is zero.\n\nWe now apply this framework to the specified one-degree-of-freedom model. The variables $u$ and $p$ are scalars.\nThe residual is $R(u,p) = p\\,u + \\beta\\,u^3 - F$.\nThe objective is $J(u,p) = \\frac{1}{2}\\,u^2$.\n\nThe necessary partial derivatives, evaluated at a converged state $u^\\star$, are:\n- $\\frac{\\partial J}{\\partial u}(u^\\star) = u^\\star$\n- $\\frac{\\partial J}{\\partial p}(u^\\star) = 0$\n- $\\frac{\\partial R}{\\partial p}(u^\\star) = u^\\star$\n\nThe exact and approximate tangents (which are scalars in this 1D case) are:\n- Exact tangent: $K(u^\\star, p) = \\frac{\\partial R}{\\partial u}(u^\\star, p) = p + 3\\beta (u^\\star)^2$\n- Approximate tangent: $K^\\mathrm{approx}(u^\\star, p) = p + c\\,\\beta\\,(u^\\star)^2$\n\nFor the exact gradient, the scalar adjoint variable $\\lambda$ is found from $K \\lambda = \\frac{\\partial J}{\\partial u}$, which gives $\\lambda = \\frac{u^\\star}{p + 3\\beta (u^\\star)^2}$. The exact gradient with respect to $p$ is:\n$$\n\\left(\\frac{dJ}{dp}\\right)_{\\text{exact}} = \\frac{\\partial J}{\\partial p} - \\lambda \\frac{\\partial R}{\\partial p} = 0 - \\left(\\frac{u^\\star}{p + 3\\beta (u^\\star)^2}\\right) u^\\star = -\\frac{(u^\\star)^2}{p + 3\\beta (u^\\star)^2}\n$$\nFor the approximate gradient, the approximate adjoint variable $\\lambda^\\mathrm{approx}$ is found from $K^\\mathrm{approx} \\lambda^\\mathrm{approx} = \\frac{\\partial J}{\\partial u}$, giving $\\lambda^\\mathrm{approx} = \\frac{u^\\star}{p + c\\beta (u^\\star)^2}$. The approximate gradient is:\n$$\n\\left(\\frac{dJ}{dp}\\right)_{\\text{approx}} = \\frac{\\partial J}{\\partial p} - \\lambda^\\mathrm{approx} \\frac{\\partial R}{\\partial p} = 0 - \\left(\\frac{u^\\star}{p + c\\beta (u^\\star)^2}\\right) u^\\star = -\\frac{(u^\\star)^2}{p + c\\beta (u^\\star)^2}\n$$\nThe numerical implementation will first solve $R(u,p)=0$ for $u^\\star$ using the damped Newton-Raphson scheme with tangent $K^\\mathrm{approx}(u^{(k)}, p)$. Upon convergence, $u^\\star$ is used in the above two formulas to compute the gradients. The absolute error is $|\\text{grad\\_approx} - \\text{grad\\_exact}|$ and the relative error is this absolute error divided by $|\\text{grad\\_exact}|$, with care taken for the case where the exact gradient is zero.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test cases.\n    It computes the converged state, exact and approximate adjoint gradients,\n    and the associated errors for a 1D nonlinear model.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (p, beta, F, c)\n        (2.0, 1.0, 1.0, 0.0),\n        (1.0, 0.0, 1.2, 0.0),\n        (0.5, 5.0, 0.2, 1.0),\n        (0.2, 1.0, 0.01, 6.0),\n        (3.0, 2.0, 0.0, 0.0),\n    ]\n\n    results = []\n    \n    # --- Newton-Raphson Solver Parameters ---\n    TOL = 1e-12  # Convergence tolerance for residual\n    MAX_ITER = 100 # Maximum number of iterations\n    MAX_LS_STEPS = 10 # Maximum backtracking steps in line search\n\n    for case in test_cases:\n        p, beta, F, c = case\n\n        # Define model-specific functions\n        def get_residual(u, p_val, beta_val, F_val):\n            return p_val * u + beta_val * u**3 - F_val\n\n        def get_approx_tangent(u, p_val, beta_val, c_val):\n            return p_val + c_val * beta_val * u**2\n\n        # --- Forward Solve: Damped Newton-Raphson ---\n        u_k = 0.0\n        for _ in range(MAX_ITER):\n            res = get_residual(u_k, p, beta, F)\n\n            if abs(res)  TOL:\n                break\n\n            k_approx = get_approx_tangent(u_k, p, beta, c)\n            \n            # Prevent division by zero, although not expected for these cases\n            if abs(k_approx)  1e-15:\n                # If tangent is zero and residual is not, we can't proceed.\n                # This could happen if p=0, beta>0, c>0, u=0.\n                u_k = np.nan # Indicate solver failure\n                break\n\n            delta_u = -res / k_approx\n\n            # --- Backtracking Line Search ---\n            alpha = 1.0\n            for _ in range(MAX_LS_STEPS):\n                u_new = u_k + alpha * delta_u\n                res_new = get_residual(u_new, p, beta, F)\n                if abs(res_new)  abs(res):\n                    break\n                alpha /= 2.0\n            \n            u_k = u_new\n        \n        u_star = u_k\n\n        # --- Adjoint-based Gradient Computations at u_star ---\n        \n        # Exact gradient\n        k_exact = p + 3.0 * beta * u_star**2\n        if abs(k_exact)  1e-15:\n            # This occurs if denominator is zero.\n            grad_exact = np.inf if u_star**2 > 0 else 0.0\n        else:\n            grad_exact = -(u_star**2) / k_exact\n            \n        # Approximate gradient\n        k_approx_conv = p + c * beta * u_star**2\n        if abs(k_approx_conv)  1e-15:\n            grad_approx = np.inf if u_star**2 > 0 else 0.0\n        else:\n            grad_approx = -(u_star**2) / k_approx_conv\n            \n        # --- Error Computations ---\n        abs_error = abs(grad_approx - grad_exact)\n        \n        # Define relative error. If exact gradient is zero, the error is only\n        # non-zero if the approximate gradient is non-zero (infinite relative error).\n        # However, for this problem, if grad_exact is zero (because u_star=0),\n        # grad_approx is also zero. So, if grad_exact is zero, relative error is zero.\n        if abs(grad_exact)  1e-15:\n            rel_error = 0.0 if abs_error  1e-15 else np.inf\n        else:\n            rel_error = abs_error / abs(grad_exact)\n\n        results.append([u_star, grad_exact, grad_approx, abs_error, rel_error])\n\n    # Final print statement in the exact required format.\n    # Format: [[u_1,g_e_1,g_a_1,e_a_1,e_r_1],[u_2,...],...]\n    output_str = \"[\" + \",\".join(\n        f\"[{res[0]},{res[1]},{res[2]},{res[3]},{res[4]}]\" for res in results\n    ) + \"]\"\n    print(output_str)\n\n\nsolve()\n\n```", "id": "3543074"}]}