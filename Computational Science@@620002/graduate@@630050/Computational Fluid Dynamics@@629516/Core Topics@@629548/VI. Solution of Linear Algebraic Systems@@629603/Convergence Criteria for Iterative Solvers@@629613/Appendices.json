{"hands_on_practices": [{"introduction": "The foundation of monitoring any iterative solver is tracking the residual, but a raw, unscaled residual norm provides little insight. Its magnitude depends heavily on the problem's scaling, discretization, and physical parameters. This first hands-on practice [@problem_id:3305239] invites you to implement and compare three of the most common residual normalization strategies. By running a model problem that mimics the behavior of the Navier-Stokes equations under mesh refinement and varying Reynolds numbers, you will gain crucial intuition about which criteria are robust and which can be misleading.", "problem": "You are tasked with examining how different residual normalization choices influence perceived convergence of an iterative linear solver arising in the Newton linearization of the compressible Navier–Stokes equations. Work in a one-dimensional, nondimensional setting that captures the scaling of convective and diffusive transport and the compressibility stabilization. The foundation is the following widely used process: a nonlinear residual $F(U) = 0$ is solved by Newton’s method, producing a linear system $J(U^{(m)}) \\,\\Delta U^{(m)} = -F(U^{(m)})$ at nonlinear iteration index $m$, where $J$ is the Jacobian matrix of $F$. For a fixed nonlinear iterate $U^{(m)}$, the inner linear iterative solver maintains a current iterate $x^{(k)}$ solving $J x = b$ with $b = -F(U^{(m)})$, and defines the linear residual $r^{(k)} = b - J x^{(k)}$. Convergence of the inner solver is often declared by checking whether a normalized residual falls below a tolerance.\n\nStarting from this base, construct a synthetic linearized one-dimensional operator and investigate three residual normalization strategies: scaling by the initial residual norm, by the right-hand side norm, and by the diagonal of the Jacobian. Use the fundamental scalings from one-dimensional convection–diffusion with compressibility stabilization to define $J \\in \\mathbb{R}^{n \\times n}$ as a tridiagonal matrix:\n- The diagonal entries are $J_{ii} = \\alpha + \\dfrac{2 \\nu}{h^2}$.\n- The sub-diagonal entries are $J_{i,i-1} = -\\dfrac{\\nu}{h^2} - \\dfrac{U}{2 h}$ for $i \\geq 2$.\n- The super-diagonal entries are $J_{i,i+1} = -\\dfrac{\\nu}{h^2} + \\dfrac{U}{2 h}$ for $i \\leq n-1$.\nHere, $h = \\dfrac{1}{n}$ is the grid spacing on the unit interval, $U$ is a constant advection speed, and $\\nu = \\dfrac{1}{Re}$ is the kinematic viscosity in nondimensional form, with $Re$ the Reynolds number. The parameter $\\alpha > 0$ represents compressibility or pseudo-time stabilization and may be taken as sufficiently large to ensure diagonal dominance across the test suite.\n\nDefine the right-hand side as $b_i = \\sin(\\pi x_i)$ with $x_i = i h$ for $i = 1, \\dots, n$. Use an initial guess $x^{(0)}_i = x_i$ for $i = 1, \\dots, n$, so that $r^{(0)} = b - J x^{(0)} \\neq b$.\n\nUse the Weighted Jacobi (WJ) iteration to advance the linear solve:\n$$\nx^{(k+1)} = x^{(k)} + \\omega D^{-1} \\left(b - J x^{(k)}\\right),\n$$\nwhere $D = \\mathrm{diag}(J)$ and $\\omega \\in (0, 2)$ is a relaxation parameter. Choose a fixed number of inner iterations $K$ and a fixed relaxation parameter $\\omega$, and assume $D$ has strictly positive entries.\n\nFor each test case, after $K$ iterations, compute the residual $r^{(K)}$ and evaluate the following three normalized residual definitions:\n1. Scaling by the initial residual norm: $R_{r_0} = \\dfrac{\\lVert r^{(K)} \\rVert_2}{\\lVert r^{(0)} \\rVert_2}$.\n2. Scaling by the right-hand side norm: $R_{b} = \\dfrac{\\lVert r^{(K)} \\rVert_2}{\\lVert b \\rVert_2}$.\n3. Scaling by the Jacobian diagonal (row-wise equilibration): $R_{D} = \\dfrac{\\lVert D^{-1} r^{(K)} \\rVert_2}{\\lVert D^{-1} r^{(0)} \\rVert_2}$.\n\nIn addition, for each normalization define a boolean convergence flag by comparing to a fixed tolerance $\\tau$: for normalization $R_{\\star}$, set $C_{\\star} = \\mathrm{True}$ if $R_{\\star} \\le \\tau$ and $C_{\\star} = \\mathrm{False}$ otherwise.\n\nYour program must implement the above using the following fixed parameters:\n- Use $U = 1$.\n- Use $\\alpha = 500$.\n- Use $\\omega = 1$.\n- Use $K = 30$.\n- Use $\\tau = 10^{-6}$.\nAll quantities are dimensionless, and angles are not used.\n\nConstruct and run the following test suite, designed to probe mesh refinement and Reynolds number variation:\n- Case $1$: $n = 50$, $Re = 50$.\n- Case $2$: $n = 200$, $Re = 50$.\n- Case $3$: $n = 50$, $Re = 1000$.\n- Case $4$: $n = 200$, $Re = 1000$.\n- Case $5$: $n = 50$, $Re = 5$.\n\nFor each case, your program must return a list in the order $[R_{r_0}, R_b, R_D, C_{r_0}, C_b, C_D]$ containing the three floats and three booleans. Aggregate the results for all cases into a single list, so the final output is a single line containing a comma-separated list of the five case results, enclosed in square brackets. For example, the printed line must look like $[\\,[\\dots],\\,[\\dots],\\,[\\dots],\\,[\\dots],\\,[\\dots]\\,]$ with each inner list in the specified order. No additional text may be printed.", "solution": "The problem requires a critical examination of three distinct residual normalization strategies used to assess the convergence of an iterative linear solver. The context is a synthetic one-dimensional problem representative of those encountered in computational fluid dynamics (CFD), specifically stemming from the Newton linearization of the compressible Navier-Stokes equations. The analysis will be performed by implementing the specified numerical scheme and applying it to a suite of test cases.\n\nFirst, we formalize the problem setup. The core task is to iteratively solve the linear system of equations:\n$$\nJ x = b\n$$\nHere, $J \\in \\mathbb{R}^{n \\times n}$ is a sparse, tridiagonal matrix representing the discretized one-dimensional convection-diffusion operator with a stabilization term. Its entries are defined based on a uniform grid with spacing $h = \\dfrac{1}{n}$ on the unit interval. The parameters include a constant advection speed $U$, a kinematic viscosity $\\nu = \\dfrac{1}{Re}$ (where $Re$ is the Reynolds number), and a stabilization parameter $\\alpha$. The matrix entries are given as:\n- Diagonal: $J_{ii} = \\alpha + \\dfrac{2 \\nu}{h^2}$\n- Sub-diagonal: $J_{i,i-1} = -\\dfrac{\\nu}{h^2} - \\dfrac{U}{2 h}$ for $i = 2, \\dots, n$\n- Super-diagonal: $J_{i,i+1} = -\\dfrac{\\nu}{h^2} + \\dfrac{U}{2 h}$ for $i = 1, \\dots, n-1$\n\nThe right-hand side (RHS) vector $b \\in \\mathbb{R}^n$ is defined by evaluating a smooth function on the grid points $x_i = i h$ for $i = 1, \\dots, n$:\n$$\nb_i = \\sin(\\pi x_i)\n$$\nThe iterative solution process begins with an initial guess $x^{(0)} \\in \\mathbb{R}^n$, which is specified as the vector of grid coordinates:\n$$\nx^{(0)}_i = x_i\n$$\n\nThe iterative solver chosen is the Weighted Jacobi (WJ) method. The solution is advanced from iteration $k$ to $k+1$ using the formula:\n$$\nx^{(k+1)} = x^{(k)} + \\omega D^{-1} \\left(b - J x^{(k)}\\right)\n$$\nwhere $D = \\mathrm{diag}(J)$ is the diagonal part of $J$, $\\omega$ is a relaxation parameter, and the term in the parentheses is the linear residual at iteration $k$, defined as $r^{(k)} = b - J x^{(k)}$. For this problem, the parameters are fixed: $\\omega = 1$ (reducing the method to the standard Jacobi iteration) and the number of iterations is fixed at $K = 30$. The problem assumes that $D$ has strictly positive entries, a condition satisfied here since $J_{ii} = \\alpha + \\frac{2\\nu}{h^2}$ with $\\alpha > 0$ and $\\nu > 0$.\n\nAfter performing $K=30$ iterations, we obtain the final state $x^{(K)}$ and the final residual $r^{(K)} = b - J x^{(K)}$. The central task is to evaluate three different normalized residuals:\n1.  **Scaling by the initial residual norm**: This metric, $R_{r_0}$, measures the reduction in the residual's magnitude relative to its starting value. The initial residual is $r^{(0)} = b - J x^{(0)}$.\n    $$\n    R_{r_0} = \\dfrac{\\lVert r^{(K)} \\rVert_2}{\\lVert r^{(0)} \\rVert_2}\n    $$\n2.  **Scaling by the right-hand side norm**: This metric, $R_{b}$, measures the final residual's magnitude relative to the forcing term $b$.\n    $$\n    R_{b} = \\dfrac{\\lVert r^{(K)} \\rVert_2}{\\lVert b \\rVert_2}\n    $$\n3.  **Scaling by the Jacobian diagonal (row-wise equilibration)**: This metric, $R_{D}$, measures the reduction in the norm of the preconditioned residual, where the preconditioner is the diagonal of the Jacobian. This often provides a scale-invariant measure of convergence.\n    $$\n    R_{D} = \\dfrac{\\lVert D^{-1} r^{(K)} \\rVert_2}{\\lVert D^{-1} r^{(0)} \\rVert_2}\n    $$\n\nFor each of these normalized residuals $R_{\\star}$, a boolean convergence flag $C_{\\star}$ is determined by comparing it against a fixed tolerance $\\tau = 10^{-6}$. The flag is set to $\\mathrm{True}$ if $R_{\\star} \\le \\tau$ and $\\mathrm{False}$ otherwise.\n\nThe computational procedure for each test case $(n, Re)$ is as follows:\n1.  Set the fixed parameters: $U = 1$, $\\alpha = 500$, $\\omega = 1$, $K = 30$, $\\tau = 10^{-6}$.\n2.  Calculate the derived parameters: $h = 1/n$ and $\\nu = 1/Re$.\n3.  Construct the $n \\times n$ tridiagonal matrix $J$.\n4.  Construct the grid vector $x_{grid}$ with entries $x_i = i h$.\n5.  Construct the RHS vector $b$ with entries $b_i = \\sin(\\pi x_i)$.\n6.  Set the initial solution vector $x^{(0)} = x_{grid}$.\n7.  Calculate the initial residual $r^{(0)} = b - Jx^{(0)}$.\n8.  Extract the diagonal matrix $D$ from $J$.\n9.  Compute and store the denominator norms: $\\lVert r^{(0)} \\rVert_2$, $\\lVert b \\rVert_2$, and $\\lVert D^{-1} r^{(0)} \\rVert_2$.\n10. Execute the Jacobi iteration loop for $k$ from $0$ to $K-1=29$:\n    $x^{(k+1)} = x^{(k)} + \\omega D^{-1} (b - J x^{(k)})$.\n11. Upon completion, compute the final residual $r^{(K)} = b - J x^{(K)}$.\n12. Compute the numerator norms: $\\lVert r^{(K)} \\rVert_2$ and $\\lVert D^{-1} r^{(K)} \\rVert_2$.\n13. Calculate the three ratios $R_{r_0}$, $R_{b}$, and $R_{D}$.\n14. Determine the three boolean flags $C_{r_0}$, $C_{b}$, and $C_{D}$.\n15. Collate the results into a list of the form $[R_{r_0}, R_b, R_D, C_{r_0}, C_b, C_D]$.\n\nThis procedure is systematically applied to all specified test cases, and the final results are aggregated into a single list.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_case(n, Re, U, alpha, omega, K, tau):\n    \"\"\"\n    Solves the 1D convection-diffusion problem for one test case.\n\n    Args:\n        n (int): Number of grid points.\n        Re (float): Reynolds number.\n        U (float): Advection speed.\n        alpha (float): Stabilization parameter.\n        omega (float): Weighted Jacobi relaxation parameter.\n        K (int): Number of iterations.\n        tau (float): Convergence tolerance.\n\n    Returns:\n        list: A list containing [R_r0, R_b, R_D, C_r0, C_b, C_D].\n    \"\"\"\n    # 1. Calculate derived parameters\n    h = 1.0 / n\n    nu = 1.0 / Re\n\n    # 2. Construct the tridiagonal Jacobian matrix J\n    J = np.zeros((n, n))\n    \n    # Diagonal term\n    diag_val = alpha + (2 * nu / h**2)\n    # Sub-diagonal term\n    sub_diag_val = -nu / h**2 - U / (2 * h)\n    # Super-diagonal term\n    sup_diag_val = -nu / h**2 + U / (2 * h)\n\n    # Fill the matrix J using np.diag\n    J += np.diag(np.full(n, diag_val))\n    J += np.diag(np.full(n - 1, sub_diag_val), k=-1)\n    J += np.diag(np.full(n - 1, sup_diag_val), k=1)\n    \n    # 3. Construct grid, RHS vector b, and initial guess x_k\n    # Grid points x_i = i*h for i=1,...,n, corresponding to array indices 0,...,n-1\n    x_grid = h * (np.arange(n) + 1)\n    \n    b = np.sin(np.pi * x_grid)\n    x_k = x_grid.copy() # Initial guess x^(0)\n\n    # 4. Extract diagonal D and compute its inverse\n    D = np.diag(J)\n    D_inv = 1.0 / D\n\n    # 5. Calculate initial residual r^(0) and required norms\n    r0 = b - J @ x_k\n    norm_r0 = np.linalg.norm(r0, 2)\n    norm_b = np.linalg.norm(b, 2)\n    \n    # Preconditioned initial residual and its norm\n    # D_inv is a 1D array, so we use element-wise multiplication\n    D_inv_r0 = D_inv * r0\n    norm_D_inv_r0 = np.linalg.norm(D_inv_r0, 2)\n    \n    # Edge case: If initial residual is zero, convergence is perfect.\n    if np.isclose(norm_r0, 0.0):\n        return [0.0, 0.0, 0.0, True, True, True]\n\n    # 6. Perform K iterations of the Weighted Jacobi method\n    for _ in range(K):\n        r_k = b - J @ x_k\n        x_k += omega * D_inv * r_k\n        \n    # 7. Compute final residual r^(K) and its norms\n    r_K = b - J @ x_k\n    norm_rK = np.linalg.norm(r_K, 2)\n    \n    D_inv_r_K = D_inv * r_K\n    norm_D_inv_r_K = np.linalg.norm(D_inv_r_K, 2)\n\n    # 8. Calculate the three normalized residual metrics\n    R_r0 = norm_rK / norm_r0\n    R_b = norm_rK / norm_b if norm_b > 0 else np.inf\n    # Handle division by zero for R_D, although norm_D_inv_r0 is unlikely to be zero\n    # if norm_r0 wasn't.\n    R_D = norm_D_inv_r_K / norm_D_inv_r0 if norm_D_inv_r0 > 0 else np.inf\n\n    # 9. Determine the convergence flags\n    C_r0 = R_r0 <= tau\n    C_b = R_b <= tau\n    C_D = R_D <= tau\n\n    return [R_r0, R_b, R_D, C_r0, C_b, C_D]\n\ndef solve():\n    # Define the fixed parameters from the problem statement.\n    U = 1.0\n    alpha = 500.0\n    omega = 1.0\n    K = 30\n    tau = 1.0e-6\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (50, 50.0),    # Case 1\n        (200, 50.0),   # Case 2\n        (50, 1000.0),  # Case 3\n        (200, 1000.0), # Case 4\n        (50, 5.0),     # Case 5\n    ]\n\n    all_results = []\n    for n, Re in test_cases:\n        result = run_case(n, Re, U, alpha, omega, K, tau)\n        all_results.append(result)\n\n    # Final print statement in the exact required format.\n    # The string representation of a list of lists matches the required format.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n\n```", "id": "3305239"}, {"introduction": "After establishing the importance of normalization, a new question arises: what is the appropriate tolerance? Driving the residual to machine precision is rarely necessary and often computationally wasteful. This is because the total error is a combination of algebraic error from the iterative solver and discretization error from the underlying mesh. This analytical exercise [@problem_id:3305198] guides you through the process of balancing these two error sources to derive an optimal stopping criterion. You will discover how to establish a rational tolerance that prevents the solver from expending effort to reduce the algebraic error far below the inherent discretization error floor.", "problem": "Consider steady one-dimensional convection–diffusion in Computational Fluid Dynamics (CFD) for the scalar field $u(x)$ governed on the interval $[0,1]$ by the boundary value problem\n$$\n-\\nu\\,u''(x) + \\beta\\,u'(x) = f(x), \\qquad u(0)=0,\\quad u(1)=0,\n$$\nwith constant physical parameters $\\nu>0$ (diffusivity) and $\\beta>0$ (convection speed). Take a manufactured right-hand side defined by $f(x)=1$ so that the continuous solution is smooth and well-posed. Discretize the spatial derivatives on a uniform grid of spacing $h$ using a first-order upwind approximation for the convective term $\\beta\\,u'(x)$ and a second-order central-difference approximation for the diffusive term $-\\nu\\,u''(x)$. The discrete system can be written abstractly as the linear system $A\\,u_h=b$ where $A$ is the finite-difference operator on the grid and $b$ is the assembled source term.\n\nDefine the algebraic solver error $e_{\\mathrm{alg}}$ by $e_{\\mathrm{alg}}=u_h^{\\star}-u_h^{(k)}$, where $u_h^{\\star}$ is the exact solution of the discrete system and $u_h^{(k)}$ is the current iterate of an iterative linear solver, and define the algebraic residual $r^{(k)}=b-A\\,u_h^{(k)}$. Let $\\|\\cdot\\|$ denote any subordinate matrix-vector norm. Use the basic inequality for linear systems,\n$$\n\\|e_{\\mathrm{alg}}\\| \\le \\|A^{-1}\\|\\,\\|r^{(k)}\\|,\n$$\nand the leading-order consistency of the employed finite-difference scheme to characterize the discretization error floor. Specifically, use the leading-order local truncation error of the first-order upwind formula for $\\beta\\,u'(x)$, which is proportional to $\\frac{\\beta h}{2}\\,u''(x)$, to bound the global discretization error of the discrete solution $u_h^{\\star}$.\n\nStarting from these principles and without invoking any shortcut formulas, derive a closed-form analytic expression for the smallest absolute residual tolerance $\\tau^{\\star}$ (in the same norm used for the residual) such that further tightening of the solver tolerance below $\\tau^{\\star}$ cannot reduce the dominant leading-order total error beyond the asymptotic discretization error floor. Your expression must be in terms of the physical parameters $\\beta$, $\\nu$, and the mesh spacing $h$ only, and it should be obtained by balancing the leading-order algebraic solver error with the leading-order discretization error implied by the truncation error analysis of the chosen scheme. Express the final answer as a single analytic expression. No units are required.", "solution": "The user wants me to find the optimal absolute residual tolerance $\\tau^{\\star}$ for an iterative solver applied to a discretized convection-diffusion problem. The criterion for optimality is that the algebraic error should be balanced with the discretization error.\n\nFirst, I acknowledge the problem statement. The governing equation is a steady one-dimensional convection-diffusion boundary value problem on the interval $x \\in [0, 1]$:\n$$\n-\\nu\\,u''(x) + \\beta\\,u'(x) = f(x), \\qquad u(0)=0,\\quad u(1)=0\n$$\nwhere the physical parameters $\\nu>0$ (diffusivity) and $\\beta>0$ (convection speed) are constants, and the source term is $f(x)=1$. Although the parameters are given physical names, the use of $f(x)=1$ implies that the problem is cast in a dimensionless framework, where $\\nu$, $\\beta$, $u$, and $x$ are dimensionless quantities. I proceed under this interpretation.\n\nThe total error in the numerical solution at the $k$-th iteration, $u_h^{(k)}$, is the difference between the exact continuous solution $u(x_i)$ at grid nodes $x_i$ and the numerical solution vector, $u_h^{(k)}$. This error can be decomposed into two components:\n$1$. The discretization error, $e_{\\mathrm{disc}} = u - u_h^{\\star}$, which is the difference between the exact continuous solution (evaluated at the nodes) and the exact solution of the discrete system $A u_h = b$.\n$2$. The algebraic error, $e_{\\mathrm{alg}} = u_h^{\\star} - u_h^{(k)}$, which is the difference between the exact discrete solution and the current iterate from the solver.\n\nThe total error is the sum of these two: $u - u_h^{(k)} = e_{\\mathrm{disc}} + e_{\\mathrm{alg}}$. By the triangle inequality for any vector norm $\\|\\cdot\\|$, the magnitude of the total error is bounded by:\n$$\n\\|u - u_h^{(k)}\\| \\le \\|e_{\\mathrm{disc}}\\| + \\|e_{\\mathrm{alg}}\\|\n$$\n\nThe algebraic error is related to the residual $r^{(k)} = b - A u_h^{(k)}$. Since $b = A u_h^{\\star}$, we have $r^{(k)} = A u_h^{\\star} - A u_h^{(k)} = A(u_h^{\\star} - u_h^{(k)}) = A e_{\\mathrm{alg}}$. This implies $e_{\\mathrm{alg}} = A^{-1} r^{(k)}$. Taking the norm, we obtain the inequality provided in the problem statement:\n$$\n\\|e_{\\mathrm{alg}}\\| \\le \\|A^{-1}\\| \\|r^{(k)}\\|\n$$\nThe discretization error is related to the local truncation error, $\\tau_{\\mathrm{LTE}}$. The truncation error is defined as the residual obtained when the exact continuous solution $u$ is substituted into the finite difference operator $A_h$. Let's denote the vector of $u(x_i)$ values as $u$. Then $A u = b + \\tau_{\\mathrm{LTE}}$. Since $A u_h^{\\star} = b$, we have $A(u - u_h^{\\star}) = \\tau_{\\mathrm{LTE}}$, or $A e_{\\mathrm{disc}} = \\tau_{\\mathrm{LTE}}$. This implies $e_{\\mathrm{disc}} = A^{-1} \\tau_{\\mathrm{LTE}}$, and taking the norm gives the bound:\n$$\n\\|e_{\\mathrm{disc}}\\| \\le \\|A^{-1}\\| \\|\\tau_{\\mathrm{LTE}}\\|\n$$\nThe problem specifies a first-order upwind scheme for the convective term and a second-order central difference for the diffusive term. The overall accuracy is limited by the lower-order scheme, so the method is first-order. The leading-order term of the local truncation error is given as originating from the upwind approximation:\n$$\n\\tau_{\\mathrm{LTE}}(x) \\approx \\frac{\\beta h}{2} u''(x)\n$$\nwhere $h$ is the grid spacing.\n\nA rational stopping criterion for the iterative solver is to terminate when the algebraic error is on the same order as the discretization error. Tightening the solver tolerance further would expend computational effort for a negligible reduction in the total error, which would be dominated by the discretization error floor. This balancing condition can be expressed as $\\|e_{\\mathrm{alg}}\\| \\sim \\|e_{\\mathrm{disc}}\\|$. Using the bounds derived above:\n$$\n\\|A^{-1}\\| \\|r^{(k)}\\| \\sim \\|A^{-1}\\| \\|\\tau_{\\mathrm{LTE}}\\|\n$$\nThe term $\\|A^{-1}\\|$ cancels, leading to the elegant criterion that the final residual norm should be on the order of the norm of the local truncation error. Let $\\tau^{\\star}$ be the desired residual tolerance. Then:\n$$\n\\tau^{\\star} \\sim \\|\\tau_{\\mathrm{LTE}}\\| \\approx \\left\\| \\frac{\\beta h}{2} u''(x) \\right\\|\n$$\nTo obtain a specific value for $\\tau^{\\star}$, we need a single scalar measure for the norm. The infinity norm is a natural choice as it represents the maximum error over the domain.\n$$\n\\tau^{\\star} = \\frac{\\beta h}{2} \\|u''(x)\\|_{\\infty} = \\frac{\\beta h}{2} \\sup_{x \\in [0,1]} |u''(x)|\n$$\nTo evaluate this, we must find the second derivative of the exact solution $u(x)$. The BVP is $-\\nu u'' + \\beta u' = 1$ with $u(0)=u(1)=0$. This is a linear second-order ODE with constant coefficients.\nThe homogeneous solution is $u_h(x) = C_1 + C_2 \\exp((\\beta/\\nu)x)$.\nA particular solution is $u_p(x) = x/\\beta$.\nThe general solution is $u(x) = C_1 + C_2 \\exp((\\beta/\\nu)x) + x/\\beta$.\nApplying the boundary conditions:\n$u(0)=0 \\implies C_1 + C_2 = 0 \\implies C_1 = -C_2$.\n$u(1)=0 \\implies C_1 + C_2 \\exp(\\beta/\\nu) + 1/\\beta = 0$.\nSubstituting $C_1=-C_2$ into the second equation gives $-C_2 + C_2 \\exp(\\beta/\\nu) + 1/\\beta = 0$, which yields $C_2( \\exp(\\beta/\\nu) - 1 ) = -1/\\beta$.\nThus, $C_2 = -\\frac{1}{\\beta(\\exp(\\beta/\\nu)-1)}$.\nThe full solution is:\n$$\nu(x) = \\frac{1}{\\beta(\\exp(\\beta/\\nu)-1)} - \\frac{\\exp((\\beta/\\nu)x)}{\\beta(\\exp(\\beta/\\nu)-1)} + \\frac{x}{\\beta}\n$$\nNow, we compute the first and second derivatives:\n$$\nu'(x) = -\\frac{(\\beta/\\nu)\\exp((\\beta/\\nu)x)}{\\beta(\\exp(\\beta/\\nu)-1)} + \\frac{1}{\\beta} = -\\frac{\\exp((\\beta/\\nu)x)}{\\nu(\\exp(\\beta/\\nu)-1)} + \\frac{1}{\\beta}\n$$\n$$\nu''(x) = -\\frac{(\\beta/\\nu)\\exp((\\beta/\\nu)x)}{\\nu(\\exp(\\beta/\\nu)-1)} = -\\frac{\\beta}{\\nu^2} \\frac{\\exp((\\beta/\\nu)x)}{\\exp(\\beta/\\nu)-1}\n$$\nTo find the supremum of $|u''(x)|$ on $[0,1]$, we examine its magnitude. Since $\\beta>0$ and $\\nu>0$, the term $\\frac{\\beta}{\\nu^2} \\frac{1}{\\exp(\\beta/\\nu)-1}$ is a positive constant. The function $\\exp((\\beta/\\nu)x)$ is positive and monotonically increasing for $x \\in [0,1]$. Therefore, the maximum magnitude of $u''(x)$ occurs at $x=1$:\n$$\n\\|u''(x)\\|_{\\infty} = |u''(1)| = \\left| -\\frac{\\beta}{\\nu^2} \\frac{\\exp(\\beta/\\nu)}{\\exp(\\beta/\\nu)-1} \\right| = \\frac{\\beta}{\\nu^2} \\frac{\\exp(\\beta/\\nu)}{\\exp(\\beta/\\nu)-1}\n$$\nFinally, we substitute this expression back into our formula for $\\tau^{\\star}$:\n$$\n\\tau^{\\star} = \\frac{\\beta h}{2} \\|u''(x)\\|_{\\infty} = \\frac{\\beta h}{2} \\left( \\frac{\\beta}{\\nu^2} \\frac{\\exp(\\beta/\\nu)}{\\exp(\\beta/\\nu)-1} \\right)\n$$\nThis simplifies to the final closed-form expression for the optimal residual tolerance:\n$$\n\\tau^{\\star} = \\frac{\\beta^2 h}{2\\nu^2} \\frac{\\exp(\\beta/\\nu)}{\\exp(\\beta/\\nu)-1}\n$$", "answer": "$$\n\\boxed{\\frac{\\beta^{2} h}{2\\nu^{2}} \\frac{\\exp(\\frac{\\beta}{\\nu})}{\\exp(\\frac{\\beta}{\\nu}) - 1}}\n$$", "id": "3305198"}, {"introduction": "Ultimately, CFD simulations are run to predict specific physical quantities, such as lift, drag, or heat transfer rates, not to minimize an abstract vector norm. The most sophisticated convergence criteria are therefore goal-oriented, focusing directly on the error in a specified Quantity of Interest (QoI). In this final practice [@problem_id:3305215], you will use the powerful discrete adjoint method to derive a stopping criterion that guarantees a desired level of accuracy for a drag coefficient. This technique represents a paradigm shift from controlling a generic residual to ensuring the fidelity of the engineering result you actually care about.", "problem": "A steady, incompressible flow is discretized and solved in Computational Fluid Dynamics (CFD). The discrete steady equations take the linear algebraic form $A u = b$, where $A \\in \\mathbb{R}^{n \\times n}$ is the Jacobian of the discretized steady Navier–Stokes operator about a fixed state, $u \\in \\mathbb{R}^{n}$ is the discrete state vector of nondimensionalized variables, and $b \\in \\mathbb{R}^{n}$ is the corresponding right-hand side. An iterative linear solver produces iterates $u_k$ with residual $r_k = b - A u_k$. The quantity of interest is the nondimensional drag coefficient functional $J(u) \\in \\mathbb{R}$. The discrete adjoint variable $\\psi \\in \\mathbb{R}^{n}$ is defined to satisfy the discrete adjoint equation $A^{\\top} \\psi = \\nabla J(u)$, where $\\nabla J(u)$ is the discrete gradient of $J$ with respect to $u$. Assume the following are known and fixed for the linearized system at the operating point: \n- the operator norm bound $\\|A^{-1}\\|_{2} \\leq 4.0 \\times 10^{3}$, \n- the gradient norm $\\|\\nabla J(u)\\|_{2} = 2.5$, \n- all norms are the Euclidean norm $\\|\\cdot\\|_{2}$, \n- all variables have been nondimensionalized, and $J(u)$ is a drag coefficient.\nThe application requires that the absolute error in the drag coefficient prediction at termination satisfy $|J(u_k) - J(u_{\\star})| \\leq \\Delta_{J}$ with $\\Delta_{J} = 1.0 \\times 10^{-4}$, where $u_{\\star}$ denotes the exact steady solution of $A u = b$. Starting from the definitions of residual, error, and the discrete adjoint, and without assuming any particular form of $J$ beyond differentiability, derive a sufficient condition on the residual norm $\\|r_k\\|_{2}$ that guarantees $|J(u_k) - J(u_{\\star})| \\leq \\Delta_{J}$, expressed in terms of $\\|A^{-1}\\|_{2}$ and $\\|\\nabla J(u)\\|_{2}$. Then, compute the numerical tolerance on $\\|r_k\\|_{2}$ implied by the provided data. Express your final tolerance as a dimensionless number and round your answer to three significant figures.", "solution": "The problem is validated as self-contained, scientifically grounded, and well-posed. The objective is to derive a sufficient condition on the norm of the residual, $\\|r_k\\|_{2}$, that guarantees a desired accuracy for a quantity of interest, $J(u)$. This is a standard problem in goal-oriented a posteriori error estimation for numerical solutions of partial differential equations.\n\nLet $u_{\\star}$ be the exact solution to the discrete linear system $A u = b$, so that $A u_{\\star} = b$. Let $u_k$ be the approximate solution at iteration $k$ of an iterative solver. The state error vector is defined as $e_k = u_k - u_{\\star}$. The residual vector is defined as $r_k = b - A u_k$.\n\nWe can relate the error vector $e_k$ to the residual vector $r_k$.\nSubtracting the equation for the approximate solution, $A u_k = b - r_k$, from the equation for the exact solution, $A u_{\\star} = b$, we get:\n$$A u_{\\star} - A u_k = b - (b - r_k)$$\n$$A (u_{\\star} - u_k) = r_k$$\n$$-A e_k = r_k$$\nAssuming the matrix $A$ is invertible, we can express the error vector in terms of the residual:\n$$e_k = -A^{-1} r_k$$\n\nThe quantity of interest is the functional $J(u)$. We are interested in the absolute error in this functional, $|J(u_k) - J(u_{\\star})|$.\nSince the problem is posed in the context of a linearized system around an operating point, we can approximate the change in the functional $J$ using a first-order Taylor series expansion. The error in the functional is thus approximated by the dot product of the gradient of the functional and the error in the state vector:\n$$J(u_k) - J(u_{\\star}) \\approx \\nabla J(u)^{\\top} (u_k - u_{\\star})$$\nwhere $\\nabla J(u)$ is the gradient of the functional evaluated at the linearization point $u$. This is the standard formulation for dual-weighted residual error estimation. Let $\\delta J = J(u_k) - J(u_{\\star})$.\n$$\\delta J \\approx \\nabla J(u)^{\\top} e_k$$\n\nSubstituting the expression for $e_k$:\n$$\\delta J \\approx \\nabla J(u)^{\\top} (-A^{-1} r_k) = - \\nabla J(u)^{\\top} A^{-1} r_k$$\n\nTo simplify this expression, we introduce the discrete adjoint variable $\\psi$, which is defined by the discrete adjoint equation:\n$$A^{\\top} \\psi = \\nabla J(u)$$\nSolving for $\\psi$, we get $\\psi = (A^{\\top})^{-1} \\nabla J(u)$.\nWe can rewrite the expression for the error $\\delta J$ using the properties of the transpose. Recall that for any compatible matrices $X$ and $Y$, $(XY)^{\\top} = Y^{\\top}X^{\\top}$. Let $X = \\nabla J(u)^{\\top}$ and $Y=A^{-1}$. This isn't quite right.\nLet's rewrite the term $\\nabla J(u)^{\\top} A^{-1}$. Let's transpose the adjoint equation:\n$$(A^{\\top} \\psi)^{\\top} = (\\nabla J(u))^{\\top}$$\n$$\\psi^{\\top} A = (\\nabla J(u))^{\\top}$$\nRight-multiplying by $A^{-1}$:\n$$\\psi^{\\top} A A^{-1} = (\\nabla J(u))^{\\top} A^{-1}$$\n$$\\psi^{\\top} = (\\nabla J(u))^{\\top} A^{-1}$$\n\nNow, we substitute this into the expression for $\\delta J$:\n$$\\delta J \\approx - \\psi^{\\top} r_k$$\nThis shows that the error in the functional of interest is approximately equal to the negative of the inner product of the adjoint solution vector and the residual vector.\n\nTo find a sufficient condition for convergence, we take the absolute value and apply the Cauchy-Schwarz inequality for the Euclidean inner product and norm (${\\|\\cdot\\|}_2$):\n$$|\\delta J| = |J(u_k) - J(u_{\\star})| \\approx |-\\psi^{\\top} r_k| = |\\psi^{\\top} r_k| \\leq \\|\\psi\\|_{2} \\|r_k\\|_{2}$$\n\nTo make this bound useful, we need to bound $\\|\\psi\\|_{2}$. From the definition of the adjoint, $\\psi = (A^{\\top})^{-1} \\nabla J(u)$. Taking the norm:\n$$\\|\\psi\\|_{2} = \\|(A^{\\top})^{-1} \\nabla J(u)\\|_{2}$$\nUsing the property of an induced matrix norm, $\\|Mx\\|_{2} \\leq \\|M\\|_{2} \\|x\\|_{2}$:\n$$\\|\\psi\\|_{2} \\leq \\|(A^{\\top})^{-1}\\|_{2} \\|\\nabla J(u)\\|_{2}$$\nFor the Euclidean $2$-norm, the norm of a matrix and its transpose are equal. Also, the norm of the inverse of a transpose is the norm of the transpose of the inverse: $\\|(A^{\\top})^{-1}\\|_{2} = \\|(A^{-1})^{\\top}\\|_{2}$. Thus:\n$$\\|(A^{\\top})^{-1}\\|_{2} = \\|A^{-1}\\|_{2}$$\nSo, the bound on the norm of the adjoint vector is:\n$$\\|\\psi\\|_{2} \\leq \\|A^{-1}\\|_{2} \\|\\nabla J(u)\\|_{2}$$\n\nCombining these inequalities, we get a bound on the error in the functional:\n$$|J(u_k) - J(u_{\\star})| \\lesssim \\|\\psi\\|_{2} \\|r_k\\|_{2} \\leq \\|A^{-1}\\|_{2} \\|\\nabla J(u)\\|_{2} \\|r_k\\|_{2}$$\n\nThe problem requires that the absolute error be no more than $\\Delta_{J} = 1.0 \\times 10^{-4}$. To guarantee this, we enforce the condition that our upper bound is less than or equal to $\\Delta_{J}$:\n$$\\|A^{-1}\\|_{2} \\|\\nabla J(u)\\|_{2} \\|r_k\\|_{2} \\leq \\Delta_{J}$$\n\nThis yields the sufficient condition on the residual norm $\\|r_k\\|_{2}$:\n$$\\|r_k\\|_{2} \\leq \\frac{\\Delta_{J}}{\\|A^{-1}\\|_{2} \\|\\nabla J(u)\\|_{2}}$$\n\nNow, we substitute the given numerical values to compute the required tolerance. We are given:\n- $\\Delta_{J} = 1.0 \\times 10^{-4}$\n- $\\|A^{-1}\\|_{2} \\leq 4.0 \\times 10^{3}$\n- $\\|\\nabla J(u)\\|_{2} = 2.5$\n\nTo ensure the inequality holds, we must use the largest possible value for the denominator, which means using the upper bound for $\\|A^{-1}\\|_{2}$.\n$$\\|r_k\\|_{2} \\leq \\frac{1.0 \\times 10^{-4}}{(4.0 \\times 10^{3}) \\times 2.5}$$\nFirst, compute the product in the denominator:\n$$ (4.0 \\times 10^{3}) \\times 2.5 = (4.0 \\times 2.5) \\times 10^{3} = 10.0 \\times 10^{3} = 1.0 \\times 10^{4} $$\nNow, perform the division:\n$$\\|r_k\\|_{2} \\leq \\frac{1.0 \\times 10^{-4}}{1.0 \\times 10^{4}} = 1.0 \\times 10^{-8}$$\n\nThe problem asks for the numerical tolerance to be rounded to three significant figures.\nThe calculated tolerance is $1.0 \\times 10^{-8}$. Expressed with three significant figures, this is $1.00 \\times 10^{-8}$.\nThis is the required numerical tolerance on the Euclidean norm of the residual.", "answer": "$$\n\\boxed{1.00 \\times 10^{-8}}\n$$", "id": "3305215"}]}