{"hands_on_practices": [{"introduction": "The Conjugate Gradient method is defined by a set of elegant recurrence relations. To truly grasp how these relations work together to find the solution, there is no substitute for performing an iteration by hand. This first exercise [@problem_id:3371621] guides you through the explicit calculation of all key quantities in the first step of the CG algorithm for a simple $2 \\times 2$ system, turning abstract formulas into concrete numerical values.", "problem": "Consider a linear system arising from a symmetric positive definite discretization of an elliptic operator in computational fluid dynamics, such as the pressure Poisson equation on a minimal control-volume stencil. Let the matrix be $A=\\begin{pmatrix}4&1\\\\1&3\\end{pmatrix}$, the right-hand side be $b=\\begin{pmatrix}1\\\\2\\end{pmatrix}$, and the initial iterate be $x_{0}=\\begin{pmatrix}0\\\\0\\end{pmatrix}$. The Conjugate Gradient method (CG) is derived by minimizing the quadratic functional $\\phi(x)=\\frac{1}{2}x^{\\top}Ax-b^{\\top}x$ over Krylov subspaces, with residuals $r_{k}=b-Ax_{k}$, mutually $A$-conjugate search directions, and step sizes chosen so that the new residual is orthogonal to the current search direction in the Euclidean inner product. Starting from these principles, and without assuming any shortcut formulas, compute the first-step quantities $\\alpha_{0}$, $x_{1}$, $r_{1}$, and $\\beta_{0}$ explicitly for the given $A$, $b$, and $x_{0}$. Provide exact values with no rounding. For reporting, express the final answer as the row $\\left(\\alpha_{0}, x_{1,1}, x_{1,2}, r_{1,1}, r_{1,2}, \\beta_{0}\\right)$, where $x_{1,i}$ and $r_{1,i}$ denote the components of $x_{1}$ and $r_{1}$, respectively.", "solution": "The problem is well-posed and scientifically sound. It requires the computation of the first iteration of the Conjugate Gradient (CG) method for a given linear system, starting from the fundamental principles of the algorithm rather than relying on a pre-packaged algorithm summary.\n\nThe system to be solved is $Ax=b$, where the matrix $A$ is symmetric and positive definite (SPD). The CG method iteratively constructs a solution by minimizing the quadratic functional $\\phi(x) = \\frac{1}{2}x^{\\top}Ax - b^{\\top}x$. The gradient of this functional is $\\nabla\\phi(x) = Ax - b$, which is the negative of the residual, $r(x) = b - Ax$. Thus, minimizing $\\phi(x)$ is equivalent to finding $x$ such that $\\nabla\\phi(x) = 0$, which is the solution to $Ax=b$.\n\nThe givens are:\nThe matrix $A = \\begin{pmatrix} 4 & 1 \\\\ 1 & 3 \\end{pmatrix}$.\nThe right-hand side vector $b = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$.\nThe initial guess for the solution $x_{0} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$.\n\nThe CG algorithm proceeds as follows for iteration $k=0, 1, 2, ...$:\n1. Update the solution: $x_{k+1} = x_{k} + \\alpha_{k} p_{k}$\n2. Update the residual: $r_{k+1} = r_{k} - \\alpha_{k} A p_{k}$\n3. Update the search direction: $p_{k+1} = r_{k+1} + \\beta_{k} p_{k}$\n\nThe parameters $\\alpha_k$ and $\\beta_k$ are derived from core principles.\n\n**Step 0: Initialization**\n\nFirst, we compute the initial residual $r_0$ based on the initial guess $x_0$.\n$$r_{0} = b - Ax_{0}$$\nWith $x_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$, the initial residual is simply $b$:\n$$r_{0} = b = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$$\nThe first search direction $p_0$ is chosen to be the direction of steepest descent, which is the initial residual:\n$$p_{0} = r_{0} = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$$\n\n**Step 1: First Iteration ($k=0$)**\n\nWe need to compute $\\alpha_{0}$, $x_{1}$, $r_{1}$, and $\\beta_{0}$.\n\n**Computing the step size $\\alpha_{0}$**\nThe step size $\\alpha_{0}$ is chosen to minimize $\\phi(x_{1}) = \\phi(x_{0} + \\alpha_{0} p_{0})$ along the search direction $p_{0}$. This minimum is achieved when the new residual $r_{1}$ is orthogonal to the current search direction $p_{0}$, i.e., $p_{0}^{\\top}r_{1} = 0$.\nThe new residual is given by $r_{1} = b - Ax_{1} = b - A(x_{0} + \\alpha_{0} p_{0}) = (b - Ax_{0}) - \\alpha_{0}Ap_{0} = r_0 - \\alpha_0 A p_0$.\nSubstituting this into the orthogonality condition:\n$$p_{0}^{\\top}(r_{0} - \\alpha_{0} A p_{0}) = 0$$\n$$p_{0}^{\\top}r_{0} - \\alpha_{0} p_{0}^{\\top}A p_{0} = 0$$\nSolving for $\\alpha_0$ yields:\n$$\\alpha_{0} = \\frac{p_{0}^{\\top}r_{0}}{p_{0}^{\\top}A p_{0}}$$\nSince $p_0 = r_0$, this becomes:\n$$\\alpha_{0} = \\frac{r_{0}^{\\top}r_{0}}{r_{0}^{\\top}A r_{0}}$$\nWe calculate the necessary quantities:\n$r_{0}^{\\top}r_{0} = \\begin{pmatrix} 1 & 2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} = (1)(1) + (2)(2) = 1 + 4 = 5$.\n$A p_{0} = \\begin{pmatrix} 4 & 1 \\\\ 1 & 3 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 4(1) + 1(2) \\\\ 1(1) + 3(2) \\end{pmatrix} = \\begin{pmatrix} 6 \\\\ 7 \\end{pmatrix}$.\n$p_{0}^{\\top}A p_{0} = r_{0}^{\\top}A p_{0} = \\begin{pmatrix} 1 & 2 \\end{pmatrix} \\begin{pmatrix} 6 \\\\ 7 \\end{pmatrix} = (1)(6) + (2)(7) = 6 + 14 = 20$.\nSubstituting these values:\n$$\\alpha_{0} = \\frac{5}{20} = \\frac{1}{4}$$\n\n**Computing the new iterate $x_{1}$**\nThe new solution estimate $x_1$ is found by moving from $x_0$ along the direction $p_0$ by the step size $\\alpha_0$:\n$$x_{1} = x_{0} + \\alpha_{0} p_{0} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} + \\frac{1}{4} \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{4} \\\\ \\frac{2}{4} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{4} \\\\ \\frac{1}{2} \\end{pmatrix}$$\nSo, $x_{1,1} = \\frac{1}{4}$ and $x_{1,2} = \\frac{1}{2}$.\n\n**Computing the new residual $r_{1}$**\nThe new residual $r_1$ can be computed using the update formula:\n$$r_{1} = r_{0} - \\alpha_{0} A p_{0} = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} - \\frac{1}{4} \\begin{pmatrix} 6 \\\\ 7 \\end{pmatrix} = \\begin{pmatrix} 1 - \\frac{6}{4} \\\\ 2 - \\frac{7}{4} \\end{pmatrix} = \\begin{pmatrix} \\frac{4}{4} - \\frac{6}{4} \\\\ \\frac{8}{4} - \\frac{7}{4} \\end{pmatrix} = \\begin{pmatrix} -\\frac{2}{4} \\\\ \\frac{1}{4} \\end{pmatrix} = \\begin{pmatrix} -\\frac{1}{2} \\\\ \\frac{1}{4} \\end{pmatrix}$$\nSo, $r_{1,1} = -\\frac{1}{2}$ and $r_{1,2} = \\frac{1}{4}$.\n\n**Computing the coefficient $\\beta_{0}$**\nThe coefficient $\\beta_0$ is used to construct the next search direction, $p_1 = r_1 + \\beta_0 p_0$. The fundamental principle is that the new search direction $p_1$ must be $A$-conjugate to the previous direction $p_0$, meaning $p_{1}^{\\top}A p_{0} = 0$.\n$$(r_{1} + \\beta_{0} p_{0})^{\\top}A p_{0} = 0$$\n$$r_{1}^{\\top}A p_{0} + \\beta_{0} p_{0}^{\\top}A p_{0} = 0$$\nSolving for $\\beta_0$:\n$$\\beta_{0} = -\\frac{r_{1}^{\\top}A p_{0}}{p_{0}^{\\top}A p_{0}}$$\nWe have the terms from the previous calculations: $A p_0 = \\begin{pmatrix} 6 \\\\ 7 \\end{pmatrix}$ and $p_{0}^{\\top}A p_{0} = 20$.\nWe need to calculate the numerator:\n$r_{1}^{\\top}A p_{0} = \\begin{pmatrix} -\\frac{1}{2} & \\frac{1}{4} \\end{pmatrix} \\begin{pmatrix} 6 \\\\ 7 \\end{pmatrix} = (-\\frac{1}{2})(6) + (\\frac{1}{4})(7) = -3 + \\frac{7}{4} = -\\frac{12}{4} + \\frac{7}{4} = -\\frac{5}{4}$.\nNow we can compute $\\beta_0$:\n$$\\beta_{0} = - \\frac{-\\frac{5}{4}}{20} = \\frac{5}{4 \\cdot 20} = \\frac{5}{80} = \\frac{1}{16}$$\n\nThe requested quantities are $\\alpha_{0} = \\frac{1}{4}$, $x_{1} = \\begin{pmatrix} \\frac{1}{4} \\\\ \\frac{1}{2} \\end{pmatrix}$, $r_{1} = \\begin{pmatrix} -\\frac{1}{2} \\\\ \\frac{1}{4} \\end{pmatrix}$, and $\\beta_{0} = \\frac{1}{16}$.\nThe final answer is assembled into the specified row vector format $(\\alpha_{0}, x_{1,1}, x_{1,2}, r_{1,1}, r_{1,2}, \\beta_{0})$.\nThis gives the row vector $(\\frac{1}{4}, \\frac{1}{4}, \\frac{1}{2}, -\\frac{1}{2}, \\frac{1}{4}, \\frac{1}{16})$.", "answer": "$$\\boxed{\\begin{pmatrix} \\frac{1}{4} & \\frac{1}{4} & \\frac{1}{2} & -\\frac{1}{2} & \\frac{1}{4} & \\frac{1}{16} \\end{pmatrix}}$$", "id": "3371621"}, {"introduction": "While the first exercise focused on the mechanics of a single iteration, the power of the CG method lies in its rapid convergence over many iterations. This convergence speed, however, is not universal; it is intrinsically linked to the properties of the matrix $A$, specifically its spectral condition number $\\kappa(A)$. This practice [@problem_id:3371645] explores this fundamental relationship, asking you to apply the standard CG convergence bound to estimate the number of iterations required to achieve a desired accuracy, providing a tangible sense of how problem conditioning impacts solver performance.", "problem": "Consider the symmetric positive definite linear system $A x = b$ that arises from a cell-centered finite volume discretization of the steady pressure Poisson equation in incompressible flow on a uniform grid, where $A \\in \\mathbb{R}^{n \\times n}$ is symmetric positive definite and its spectral condition number $\\kappa(A)$ is defined as $\\kappa(A) = \\lambda_{\\max}(A)/\\lambda_{\\min}(A)$. The Conjugate Gradient method (CG) is applied to this system with exact arithmetic and no preconditioning. Assume the spectral condition number is known to be $\\kappa(A) = 10^{4}$. Using the standard worst-case error bound for the Conjugate Gradient method in the $A$-norm in terms of $\\kappa(A)$, determine the smallest integer iteration count $k$ such that the bound guarantees an $A$-norm error reduction factor of at most $10^{-6}$ from an arbitrary initial error. Express your final iteration count as an integer with no units.", "solution": "The problem requires the determination of the smallest integer iteration count, denoted by $k$, for the Conjugate Gradient (CG) method to guarantee a specified error reduction for a linear system $A x = b$. The matrix $A \\in \\mathbb{R}^{n \\times n}$ is given to be symmetric positive definite. The analysis relies on the standard worst-case convergence bound of the CG method.\n\nThe error at iteration $k$ is defined as $e_k = x_k - x$, where $x_k$ is the approximate solution at iteration $k$ and $x$ is the exact solution. The error is measured in the $A$-norm, which is defined as $\\|v\\|_A = \\sqrt{v^T A v}$ for any vector $v \\in \\mathbb{R}^n$. The classical upper bound for the relative error reduction in the $A$-norm after $k$ iterations of the CG method is given by the inequality:\n$$ \\frac{\\|e_k\\|_A}{\\|e_0\\|_A} \\leq 2 \\left( \\frac{\\sqrt{\\kappa(A)} - 1}{\\sqrt{\\kappa(A)} + 1} \\right)^k $$\nwhere $e_0$ is the initial error and $\\kappa(A)$ is the spectral condition number of the matrix $A$, defined as the ratio of its largest to smallest eigenvalue, $\\kappa(A) = \\lambda_{\\max}(A) / \\lambda_{\\min}(A)$.\n\nThe problem provides the following data:\n1. The spectral condition number is $\\kappa(A) = 10^4$.\n2. The desired error reduction factor is at most $10^{-6}$. This implies we seek the smallest $k$ such that the bound on the error ratio is less than or equal to this value: $\\frac{\\|e_k\\|_A}{\\|e_0\\|_A} \\leq 10^{-6}$.\n\nWe must find the smallest integer $k$ that satisfies the inequality when the right-hand side of the theoretical bound is constrained by the required reduction factor:\n$$ 2 \\left( \\frac{\\sqrt{\\kappa(A)} - 1}{\\sqrt{\\kappa(A)} + 1} \\right)^k \\leq 10^{-6} $$\nWe substitute the given value of $\\kappa(A) = 10^4$. First, we compute the square root of the condition number:\n$$ \\sqrt{\\kappa(A)} = \\sqrt{10^4} = 10^2 = 100 $$\nSubstituting this into the inequality gives:\n$$ 2 \\left( \\frac{100 - 1}{100 + 1} \\right)^k \\leq 10^{-6} $$\n$$ 2 \\left( \\frac{99}{101} \\right)^k \\leq 10^{-6} $$\nTo solve for the integer $k$, we first isolate the term raised to the power of $k$:\n$$ \\left( \\frac{99}{101} \\right)^k \\leq \\frac{1}{2} \\times 10^{-6} $$\nNext, we apply the natural logarithm to both sides of the inequality. As the natural logarithm, $\\ln(x)$, is a monotonically increasing function for $x > 0$, the direction of the inequality is preserved:\n$$ \\ln\\left[ \\left( \\frac{99}{101} \\right)^k \\right] \\leq \\ln\\left( \\frac{1}{2} \\times 10^{-6} \\right) $$\nUsing the logarithmic property $\\ln(a^b) = b \\ln(a)$, we get:\n$$ k \\ln\\left( \\frac{99}{101} \\right) \\leq \\ln\\left( \\frac{1}{2} \\right) + \\ln(10^{-6}) $$\n$$ k \\ln\\left( \\frac{99}{101} \\right) \\leq -\\ln(2) - 6 \\ln(10) $$\nThe term $\\ln(99/101)$ is negative because its argument, $99/101$, is less than $1$. Therefore, when we divide by $\\ln(99/101)$ to isolate $k$, we must reverse the direction of the inequality:\n$$ k \\geq \\frac{-\\ln(2) - 6 \\ln(10)}{\\ln(99/101)} $$\nThis expression can be made more convenient for calculation by multiplying the numerator and denominator by $-1$ and using the property $-\\ln(x/y) = \\ln(y/x)$:\n$$ k \\geq \\frac{\\ln(2) + 6 \\ln(10)}{\\ln(101/99)} $$\nTo find the numerical value for this lower bound on $k$, we use the standard values for the natural logarithms: $\\ln(2) \\approx 0.693147$ and $\\ln(10) \\approx 2.302585$.\nThe numerator is:\n$$ \\ln(2) + 6 \\ln(10) \\approx 0.693147 + 6 \\times (2.302585) = 0.693147 + 13.81551 = 14.508657 $$\nThe denominator is:\n$$ \\ln\\left(\\frac{101}{99}\\right) = \\ln(101) - \\ln(99) \\approx 4.6151205 - 4.5951198 = 0.0200007 $$\nNow, we compute the ratio:\n$$ k \\geq \\frac{14.508657}{0.0200007} \\approx 725.409 $$\nThe number of iterations $k$ must be an integer. The inequality $k \\geq 725.409$ requires that we find the smallest integer satisfying this condition. This is obtained by taking the ceiling of the numerical value:\n$$ k = \\lceil 725.409 \\rceil = 726 $$\nTherefore, a minimum of $726$ iterations are required for the worst-case bound to guarantee an error reduction of at most $10^{-6}$.", "answer": "$$\\boxed{726}$$", "id": "3371645"}, {"introduction": "In practical CFD applications, the 'vanilla' CG method is rarely sufficient; performance is dramatically enhanced using preconditioning. The Preconditioned Conjugate Gradient (PCG) method effectively transforms the linear system to improve its condition number, but this introduces new subtleties. This exercise [@problem_id:3371613] presents a critical thinking challenge, asking you to analyze how a seemingly good preconditioner can make standard convergence metrics misleading, a crucial lesson in the robust application of advanced numerical solvers.", "problem": "A Conjugate Gradient (CG) solver for a symmetric positive definite (SPD) linear system arises in computational fluid dynamics when discretizing elliptic operators such as the pressure Poisson equation. Consider the canonical model problem for incompressible flow pressure correction, the Poisson equation for pressure, given by $-\\nabla^{2} p = f$ on a rectangular domain with homogeneous Dirichlet boundary conditions. A standard second-order central finite difference discretization on a uniform Cartesian grid with spacings $h_x$ and $h_y$ yields an SPD linear system $A p = b$ with the five-point stencil; for an interior node, the diagonal entry satisfies $A_{ii} = 2\\left(h_x^{-2} + h_y^{-2}\\right)$ and the off-diagonal entries in the coordinate directions are $-h_x^{-2}$ and $-h_y^{-2}$. The preconditioned Conjugate Gradient (PCG) method for $A p = b$ with a left preconditioner $M$ (SPD) generates iterates $p_k$ and residuals $r_k = b - A p_k$. A commonly available diagnostic is the Euclidean norm of the preconditioned residual, $\\|M^{-1} r_k\\|_2$.\n\nStarting only from these foundational facts and the definitions of residual and preconditioning, construct and assess scenarios in which the preconditioned residual norm $\\|M^{-1} r_k\\|_2$ can be deceptively small while the true residual norm $\\|r_k\\|_2$ remains unacceptably large. In particular, reason from first principles using the scaling of the discretized operator, the definition of the residual, and operator norm inequalities such as $\\|M^{-1} r\\|_2 \\le \\|M^{-1}\\|_2 \\|r\\|_2$ and $r = M (M^{-1} r)$ implying $\\|r\\|_2 \\le \\|M\\|_2 \\|M^{-1} r\\|_2$. Do not assume any specific stopping criteria beyond what is stated; instead, examine how absolute thresholds on $\\|M^{-1} r_k\\|_2$ can misrepresent the state of convergence.\n\nWhich of the following options correctly construct such a scenario and correctly explain why naive stopping based on a small absolute value of $\\|M^{-1} r_k\\|_2$ can be misleading? Select all that apply.\n\nA. Anisotropically stretched mesh in the $x$-direction with $h_x = 10^{-3}$ and $h_y = 1$ for the five-point Laplacian, using the Jacobi preconditioner $M = \\mathrm{diag}(A)$. Suppose at some iteration $k$ the residual $r_k$ is concentrated at a single interior node with magnitude $\\|r_k\\|_2 \\approx 10^{3}$. Then $M_{ii} \\approx 2\\left(h_x^{-2} + h_y^{-2}\\right) \\approx 2 \\cdot 10^{6}$, so the corresponding component of $M^{-1} r_k$ is scaled down by approximately $1/M_{ii}$, yielding $\\|M^{-1} r_k\\|_2 \\approx 5 \\cdot 10^{-4}$ despite $\\|r_k\\|_2 \\approx 10^{3}$. Stopping on an absolute threshold for $\\|M^{-1} r_k\\|_2$ would prematurely declare convergence although the true residual is large.\n\nB. For any SPD preconditioner $M$, the quantity $\\|M^{-1} r_k\\|_2$ is a monotonically decreasing and reliable surrogate for $\\|r_k\\|_2$, so stopping when $\\|M^{-1} r_k\\|_2$ is small cannot be misleading.\n\nC. Under left preconditioning of an SPD system, the true error in the $A$-energy norm, $\\|p_k - p_\\star\\|_{A} = \\sqrt{(p_k - p_\\star)^{\\top} A (p_k - p_\\star)}$, always satisfies a bound of the form $\\|p_k - p_\\star\\|_{A} \\le C \\|M^{-1} r_k\\|_2$ with a constant $C$ independent of $M$, hence small $\\|M^{-1} r_k\\|_2$ is always a trustworthy indicator of convergence.\n\nD. Consider the extreme but SPD-consistent choice $A = I$ (arising, for example, after nondimensionalization and exact diagonal dominance in a localized decoupled subproblem) with a diagonal SPD preconditioner $M = \\mathrm{diag}(10^{6}, 1, \\dots, 1)$. Starting from $p_0 = 0$ and taking $b = r_0 = 10^{3} e_1$ yields $\\|r_0\\|_2 = 10^{3}$, while $\\|M^{-1} r_0\\|_2 = 10^{-3}$. The preconditioner rescales the first component so strongly that an absolute stopping test on $\\|M^{-1} r_k\\|_2$ would accept a solution with a large true residual.", "solution": "The user wants a critical analysis of the relationship between the true residual norm and the preconditioned residual norm in the context of the Preconditioned Conjugate Gradient (PCG) method.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\n-   **Governing Equation:** The pressure Poisson equation, $-\\nabla^{2} p = f$, on a rectangular domain with homogeneous Dirichlet boundary conditions.\n-   **Discretization Method:** Standard second-order central finite difference on a uniform Cartesian grid with spacings $h_x$ and $h_y$.\n-   **Linear System:** An SPD linear system $A p = b$.\n-   **Matrix Properties:** For an interior node $i$, the diagonal entry is $A_{ii} = 2\\left(h_x^{-2} + h_y^{-2}\\right)$, and off-diagonal entries are $-h_x^{-2}$ and $-h_y^{-2}$ (the five-point stencil).\n-   **Numerical Method:** Preconditioned Conjugate Gradient (PCG) with a left preconditioner $M$, which is SPD.\n-   **Definitions:** Iterates are $p_k$. Residuals are $r_k = b - A p_k$.\n-   **Convergence Diagnostic:** The Euclidean norm of the preconditioned residual, $\\|M^{-1} r_k\\|_2$.\n-   **Core Task:** Construct and assess scenarios where $\\|M^{-1} r_k\\|_2$ is deceptively small while the true residual norm $\\|r_k\\|_2$ remains large.\n-   **Analytical Tools:** Use scaling of the operator, definition of the residual, and the operator norm inequalities $\\|M^{-1} r\\|_2 \\le \\|M^{-1}\\|_2 \\|r\\|_2$ and $\\|r\\|_2 \\le \\|M\\|_2 \\|M^{-1} r\\|_2$.\n-   **Constraint:** Examine how an absolute threshold on $\\|M^{-1} r_k\\|_2$ can be misleading.\n\n**Step 2: Validate Using Extracted Givens**\n\n-   **Scientifically Grounded:** The problem statement is firmly rooted in the standard theory and practice of numerical linear algebra for solving partial differential equations. The pressure Poisson equation, finite-difference discretization, the resulting SPD matrix structure, and the PCG method are all canonical subjects in computational science and engineering.\n-   **Well-Posed:** The problem is well-posed as a conceptual analysis task. It asks to evaluate specific scenarios based on a clearly defined theoretical framework and mathematical relationships. A definite conclusion can be reached for each option based on these principles.\n-   **Objective:** The problem is stated in precise, objective, and technical language, free of ambiguity or subjective claims.\n\nThe problem statement is internally consistent, scientifically sound, and well-posed. No flaws are detected.\n\n**Step 3: Verdict and Action**\n\nThe problem is valid. The solution process will proceed.\n\n### Derivation and Option Analysis\n\nThe core of the problem lies in the relationship between the norm of the true residual, $\\|r_k\\|_2$, and the norm of the preconditioned residual, $\\|M^{-1} r_k\\|_2$. These two quantities are related via the SPD preconditioner $M$. By definition, $r_k = M(M^{-1} r_k)$. Applying the properties of induced matrix norms, we can establish bounds:\n$$\n\\|r_k\\|_2 = \\|M(M^{-1} r_k)\\|_2 \\le \\|M\\|_2 \\|M^{-1} r_k\\|_2\n$$\n$$\n\\|M^{-1} r_k\\|_2 \\le \\|M^{-1}\\|_2 \\|r_k\\|_2\n$$\nFrom the first inequality, it is evident that $\\|r_k\\|_2$ can be significantly larger than $\\|M^{-1} r_k\\|_2$ if the spectral norm of the preconditioner, $\\|M\\|_2 = \\lambda_{\\max}(M)$, is large. A \"deceptively small\" preconditioned residual norm occurs when $\\|M\\|_2 \\gg 1$. In such a case, a small value for $\\|M^{-1} r_k\\|_2$ does not guarantee a small value for $\\|r_k\\|_2$. The stopping criterion based on an absolute tolerance for $\\|M^{-1} r_k\\|_2$ becomes unreliable because it ignores the scaling effect of $M$. A much more robust stopping criterion would involve a relative norm, such as $\\|M^{-1} r_k\\|_2 \\le \\text{tol} \\cdot \\|M^{-1} r_0\\|_2$, or would monitor the true residual norm directly (at extra computational cost).\n\nWe now evaluate each option based on this principle.\n\n**A. Anisotropically stretched mesh in the $x$-direction with $h_x = 10^{-3}$ and $h_y = 1$ for the five-point Laplacian, using the Jacobi preconditioner $M = \\mathrm{diag}(A)$. Suppose at some iteration $k$ the residual $r_k$ is concentrated at a single interior node with magnitude $\\|r_k\\|_2 \\approx 10^{3}$. Then $M_{ii} \\approx 2\\left(h_x^{-2} + h_y^{-2}\\right) \\approx 2 \\cdot 10^{6}$, so the corresponding component of $M^{-1} r_k$ is scaled down by approximately $1/M_{ii}$, yielding $\\|M^{-1} r_k\\|_2 \\approx 5 \\cdot 10^{-4}$ despite $\\|r_k\\|_2 \\approx 10^{3}$. Stopping on an absolute threshold for $\\|M^{-1} r_k\\|_2$ would prematurely declare convergence although the true residual is large.**\n\nThe Jacobi preconditioner is defined as $M = \\mathrm{diag}(A)$. For an interior node $i$, the diagonal entry of $A$ is given by $A_{ii} = 2(h_x^{-2} + h_y^{-2})$.\nWith the given mesh spacings $h_x = 10^{-3}$ and $h_y = 1$, we have:\n$$\nA_{ii} = 2 \\left( (10^{-3})^{-2} + 1^{-2} \\right) = 2(10^6 + 1) \\approx 2 \\cdot 10^6\n$$\nThe preconditioner $M$ is a diagonal matrix with these large values on its diagonal. Consequently, its inverse, $M^{-1}$, is a diagonal matrix with entries $M^{-1}_{ii} = 1/A_{ii} \\approx 1/(2 \\cdot 10^6) = 5 \\cdot 10^{-7}$.\nThe scenario assumes the residual $r_k$ is concentrated at a single node, say node $j$, such that $\\|r_k\\|_2 \\approx 10^3$. This means $r_k \\approx 10^3 e_j$, where $e_j$ is the standard basis vector.\nThe preconditioned residual is $M^{-1} r_k$. Since both $M^{-1}$ and $r_k$ are localized, we have:\n$$\nM^{-1} r_k \\approx M^{-1} (10^3 e_j) = 10^3 (M^{-1} e_j) = 10^3 (M^{-1}_{jj}) e_j \\approx 10^3 (5 \\cdot 10^{-7}) e_j = 5 \\cdot 10^{-4} e_j\n$$\nThe norm of the preconditioned residual is therefore:\n$$\n\\|M^{-1} r_k\\|_2 \\approx \\|5 \\cdot 10^{-4} e_j\\|_2 = 5 \\cdot 10^{-4}\n$$\nThe calculations are correct. We have a situation where the true residual norm is large ($\\approx 10^3$) while the preconditioned residual norm is small ($\\approx 5 \\cdot 10^{-4}$). This discrepancy is caused by the large entries in the preconditioner $M$, i.e., a large $\\|M\\|_2 \\approx 2 \\cdot 10^6$. An absolute stopping criterion like $\\|M^{-1} r_k\\|_2 < 10^{-3}$ would be satisfied, leading to premature termination. This option correctly constructs a physically relevant scenario and accurately explains the mechanism.\n\nVerdict: **Correct**.\n\n**B. For any SPD preconditioner $M$, the quantity $\\|M^{-1} r_k\\|_2$ is a monotonically decreasing and reliable surrogate for $\\|r_k\\|_2$, so stopping when $\\|M^{-1} r_k\\|_2$ is small cannot be misleading.**\n\nThis statement makes two strong claims. First, that $\\|M^{-1} r_k\\|_2$ is monotonically decreasing. While the PCG algorithm ensures the $A$-norm of the error, $\\|p_k - p_\\star\\|_A$, decreases monotonically, and consequently the $A^{-1}$-norm of the residual $\\|r_k\\|_{A^{-1}}$ also decreases monotonically, the same is not generally guaranteed for $\\|M^{-1} r_k\\|_2$. It may exhibit non-monotonic behavior, though it trends downwards.\nSecond, and more importantly, it claims $\\|M^{-1} r_k\\|_2$ is a \"reliable surrogate\" for $\\|r_k\\|_2$. As established in our initial derivation, the relationship is $\\|r_k\\|_2 \\le \\|M\\|_2 \\|M^{-1} r_k\\|_2$. If $\\|M\\|_2$ is large, $\\|M^{-1} r_k\\|_2$ is not a reliable surrogate for $\\|r_k\\|_2$ in an absolute sense. A small value of the former does not imply a small value of the latter. Option A provides a direct counterexample to this claim.\n\nVerdict: **Incorrect**.\n\n**C. Under left preconditioning of an SPD system, the true error in the $A$-energy norm, $\\|p_k - p_\\star\\|_{A} = \\sqrt{(p_k - p_\\star)^{\\top} A (p_k - p_\\star)}$, always satisfies a bound of the form $\\|p_k - p_\\star\\|_{A} \\le C \\|M^{-1} r_k\\|_2$ with a constant $C$ independent of $M$, hence small $\\|M^{-1} r_k\\|_2$ is always a trustworthy indicator of convergence.**\n\nLet's analyze the relationship between the error norm and the preconditioned residual norm. The error is $e_k = p_k - p_\\star$, and the residual is $r_k = b - Ap_k = A p_\\star - A p_k = -A e_k$.\nThe $A$-energy norm of the error is $\\|e_k\\|_A$. Its square is:\n$$\n\\|e_k\\|_A^2 = e_k^\\top A e_k = (-A^{-1} r_k)^\\top A (-A^{-1} r_k) = r_k^\\top A^{-1} A A^{-1} r_k = r_k^\\top A^{-1} r_k = \\|r_k\\|_{A^{-1}}^2\n$$\nThis is a standard identity. Now we relate this to $\\|M^{-1} r_k\\|_2$. Let $z_k = M^{-1} r_k$, which implies $r_k = M z_k$.\n$$\n\\|e_k\\|_A^2 = (M z_k)^\\top A^{-1} (M z_k) = z_k^\\top M A^{-1} M z_k\n$$\nFor this to be bounded by $C^2 \\|z_k\\|_2^2$, where $C$ is a constant, we must have:\n$$\nz_k^\\top M A^{-1} M z_k \\le C^2 (z_k^\\top z_k)\n$$\nBy the definition of the induced $2$-norm (or Rayleigh quotient), the smallest possible value for $C^2$ that satisfies this for all $z_k$ is the largest eigenvalue of the matrix $M A^{-1} M$.\n$$\nC^2 = \\lambda_{\\max}(M A^{-1} M) = \\|M A^{-1} M\\|_2\n$$\nThis constant $C = \\sqrt{\\|M A^{-1} M\\|_2}$ explicitly depends on the preconditioner $M$. The statement's claim that $C$ is independent of $M$ is false. For example, if we scale $M$ by a factor $\\alpha > 0$, the constant becomes $C' = \\sqrt{\\|\\alpha M A^{-1} \\alpha M\\|_2} = \\alpha \\sqrt{\\|M A^{-1} M\\|_2} = \\alpha C$.\n\nVerdict: **Incorrect**.\n\n**D. Consider the extreme but SPD-consistent choice $A = I$ (arising, for example, after nondimensionalization and exact diagonal dominance in a localized decoupled subproblem) with a diagonal SPD preconditioner $M = \\mathrm{diag}(10^{6}, 1, \\dots, 1)$. Starting from $p_0 = 0$ and taking $b = r_0 = 10^{3} e_1$ yields $\\|r_0\\|_2 = 10^{3}$, while $\\|M^{-1} r_0\\|_2 = 10^{-3}$. The preconditioner rescales the first component so strongly that an absolute stopping test on $\\|M^{-1} r_k\\|_2$ would accept a solution with a large true residual.**\n\nThis option provides a clear, abstract test case.\n-   System matrix: $A = I$ (Identity matrix), which is SPD.\n-   Preconditioner: $M = \\mathrm{diag}(10^6, 1, \\dots, 1)$, which is also SPD.\n-   Initial state: $p_0 = 0$. The initial residual is $r_0 = b - A p_0 = b$.\n-   Right-hand side: $b = 10^3 e_1$, where $e_1$ is the first standard basis vector.\nThe true residual at iteration $k=0$ is $r_0 = 10^3 e_1$. Its Euclidean norm is:\n$$\n\\|r_0\\|_2 = \\|10^3 e_1\\|_2 = 10^3\n$$\nThis is a large residual.\nThe inverse preconditioner is $M^{-1} = \\mathrm{diag}(10^{-6}, 1, \\dots, 1)$.\nThe preconditioned residual at $k=0$ is $M^{-1} r_0$:\n$$\nM^{-1} r_0 = M^{-1} (10^3 e_1) = 10^3 (M^{-1} e_1) = 10^3 (10^{-6} e_1) = 10^{-3} e_1\n$$\nIts Euclidean norm is:\n$$\n\\|M^{-1} r_0\\|_2 = \\|10^{-3} e_1\\|_2 = 10^{-3}\n$$\nThis is a small value. The scenario is constructed correctly. The initial state would satisfy a stopping criterion like $\\|M^{-1} r_k\\|_2 < 10^{-2}$, causing the algorithm to terminate immediately and accept the initial guess $p_0 = 0$ as the solution, despite the true residual norm being $10^3$. The explanation provided is accurate: the huge first entry of $M$ (i.e., large $\\|M\\|_2 = 10^6$) causes the strong rescaling and the discrepancy between the norms.\n\nVerdict: **Correct**.", "answer": "$$\\boxed{AD}$$", "id": "3371613"}]}