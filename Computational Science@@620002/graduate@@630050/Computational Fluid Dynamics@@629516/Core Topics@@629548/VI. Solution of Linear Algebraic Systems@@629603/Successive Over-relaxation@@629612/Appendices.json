{"hands_on_practices": [{"introduction": "To truly grasp the Successive Over-Relaxation (SOR) method, it is essential to move beyond the abstract formula and engage with its mechanics directly. This first exercise provides a foundational, hands-on calculation for a small-scale linear system [@problem_id:3581656]. By manually constructing the SOR iteration matrix $T_{\\omega}$ and calculating its spectral radius, you will solidify your understanding of how the relaxation parameter $\\omega$ influences the properties of the iteration.", "problem": "Consider the linear system $A x = b$ with \n$$\nA=\\begin{pmatrix}2  -1 \\\\ -1  2\\end{pmatrix}, \n\\qquad \nb=\\begin{pmatrix}1 \\\\ 0\\end{pmatrix},\n\\qquad \n\\omega=\\frac{3}{2}.\n$$\nStarting from the decomposition of $A$ into its diagonal part, strictly lower triangular part, and strictly upper triangular part, and from the definition of the Successive Over-Relaxation (SOR) method as a stationary iteration obtained by applying a relaxation parameter to the Gauss–Seidel update, derive the iteration in the affine form $x^{k+1}=T_{\\omega}\\,x^{k}+c_{\\omega}$, identify the SOR iteration matrix $T_{\\omega}$ for the given $A$ and $\\omega$, and determine its spectral radius (the maximum absolute value of its eigenvalues). Your final reported answer must be the spectral radius as a single exact number. Do not round.", "solution": "The problem statement is first subjected to validation.\n\n**Step 1: Extract Givens**\n- The linear system is $A x = b$.\n- The matrix $A$ is given as $A=\\begin{pmatrix}2  -1 \\\\ -1  2\\end{pmatrix}$.\n- The vector $b$ is given as $b=\\begin{pmatrix}1 \\\\ 0\\end{pmatrix}$.\n- The relaxation parameter $\\omega$ is given as $\\omega=\\frac{3}{2}$.\n- The task is to derive the Successive Over-Relaxation (SOR) iteration matrix $T_{\\omega}$ and compute its spectral radius, $\\rho(T_{\\omega})$.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded**: The problem is a standard exercise in numerical linear algebra concerning stationary iterative methods, specifically the SOR method. It is based on well-established mathematical principles.\n- **Well-Posed**: The problem is well-posed. The matrix $A$ and the parameter $\\omega$ are explicitly defined. The task of finding the SOR iteration matrix and its spectral radius is a uniquely defined mathematical procedure. The matrix $A$ is symmetric and positive definite (its eigenvalues are $1$ and $3$), which guarantees that the SOR method converges for any $\\omega \\in (0, 2)$. The given value $\\omega = \\frac{3}{2}$ lies within this interval.\n- **Objective**: The problem is stated using precise, objective mathematical language.\n- **Completeness**: All necessary information to determine the iteration matrix $T_{\\omega}$ and its spectral radius is provided. The vector $b$ is not required for this specific task, but its presence does not create a contradiction.\n\n**Step 3: Verdict and Action**\nThe problem is deemed **valid** as it is scientifically grounded, well-posed, and complete. A full solution will be provided.\n\nThe Successive Over-Relaxation (SOR) method is a stationary iterative method for solving a linear system $A x = b$. The matrix $A$ is first decomposed into its diagonal component $D$, its strictly lower triangular component $-L$, and its strictly upper triangular component $-U$, such that $A = D - L - U$.\n\nFor the given matrix $A = \\begin{pmatrix}2  -1 \\\\ -1  2\\end{pmatrix}$, the decomposition is:\n$$D = \\begin{pmatrix}2  0 \\\\ 0  2\\end{pmatrix}, \\quad L = \\begin{pmatrix}0  0 \\\\ 1  0\\end{pmatrix}, \\quad U = \\begin{pmatrix}0  1 \\\\ 0  0\\end{pmatrix}$$\n\nThe SOR iteration is defined by the equation:\n$$(D - \\omega L) x^{k+1} = \\big((1-\\omega)D + \\omega U\\big) x^{k} + \\omega b$$\nThis can be written in the affine form $x^{k+1} = T_{\\omega} x^{k} + c_{\\omega}$, where the iteration matrix $T_{\\omega}$ is given by:\n$$T_{\\omega} = (D - \\omega L)^{-1} \\big((1-\\omega)D + \\omega U\\big)$$\nand the constant vector $c_{\\omega}$ is given by $c_{\\omega} = \\omega (D - \\omega L)^{-1} b$. Our goal is to find the spectral radius of $T_{\\omega}$.\n\nWe are given $\\omega = \\frac{3}{2}$. First, we compute the matrix $(D - \\omega L)$:\n$$D - \\omega L = \\begin{pmatrix}2  0 \\\\ 0  2\\end{pmatrix} - \\frac{3}{2}\\begin{pmatrix}0  0 \\\\ 1  0\\end{pmatrix} = \\begin{pmatrix}2  0 \\\\ -\\frac{3}{2}  2\\end{pmatrix}$$\n\nNext, we find the inverse of this matrix, $(D - \\omega L)^{-1}$:\n$$(D - \\omega L)^{-1} = \\frac{1}{(2)(2) - (0)(-\\frac{3}{2})} \\begin{pmatrix}2  0 \\\\ \\frac{3}{2}  2\\end{pmatrix} = \\frac{1}{4} \\begin{pmatrix}2  0 \\\\ \\frac{3}{2}  2\\end{pmatrix} = \\begin{pmatrix}\\frac{1}{2}  0 \\\\ \\frac{3}{8}  \\frac{1}{2}\\end{pmatrix}$$\n\nNow, we compute the matrix $\\big((1-\\omega)D + \\omega U\\big)$. Since $\\omega = \\frac{3}{2}$, the term $(1-\\omega)$ is $1 - \\frac{3}{2} = -\\frac{1}{2}$.\n$$(1-\\omega)D + \\omega U = -\\frac{1}{2}\\begin{pmatrix}2  0 \\\\ 0  2\\end{pmatrix} + \\frac{3}{2}\\begin{pmatrix}0  1 \\\\ 0  0\\end{pmatrix} = \\begin{pmatrix}-1  0 \\\\ 0  -1\\end{pmatrix} + \\begin{pmatrix}0  \\frac{3}{2} \\\\ 0  0\\end{pmatrix} = \\begin{pmatrix}-1  \\frac{3}{2} \\\\ 0  -1\\end{pmatrix}$$\n\nFinally, we compute the SOR iteration matrix $T_{\\omega}$ by multiplying the two resulting matrices:\n$$T_{\\omega} = (D - \\omega L)^{-1} \\big((1-\\omega)D + \\omega U\\big) = \\begin{pmatrix}\\frac{1}{2}  0 \\\\ \\frac{3}{8}  \\frac{1}{2}\\end{pmatrix} \\begin{pmatrix}-1  \\frac{3}{2} \\\\ 0  -1\\end{pmatrix}$$\n$$T_{\\omega} = \\begin{pmatrix} (\\frac{1}{2})(-1) + (0)(0)  (\\frac{1}{2})(\\frac{3}{2}) + (0)(-1) \\\\ (\\frac{3}{8})(-1) + (\\frac{1}{2})(0)  (\\frac{3}{8})(\\frac{3}{2}) + (\\frac{1}{2})(-1) \\end{pmatrix} = \\begin{pmatrix} -\\frac{1}{2}  \\frac{3}{4} \\\\ -\\frac{3}{8}  \\frac{9}{16} - \\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} -\\frac{1}{2}  \\frac{3}{4} \\\\ -\\frac{3}{8}  \\frac{1}{16} \\end{pmatrix}$$\n\nTo find the spectral radius $\\rho(T_{\\omega})$, we need to find the eigenvalues of $T_{\\omega}$ by solving the characteristic equation $\\det(T_{\\omega} - \\lambda I) = 0$.\n$$\\det\\begin{pmatrix} -\\frac{1}{2} - \\lambda  \\frac{3}{4} \\\\ -\\frac{3}{8}  \\frac{1}{16} - \\lambda \\end{pmatrix} = 0$$\n$$\\left(-\\frac{1}{2} - \\lambda\\right)\\left(\\frac{1}{16} - \\lambda\\right) - \\left(\\frac{3}{4}\\right)\\left(-\\frac{3}{8}\\right) = 0$$\n$$-\\frac{1}{32} + \\frac{1}{2}\\lambda - \\frac{1}{16}\\lambda + \\lambda^2 + \\frac{9}{32} = 0$$\n$$\\lambda^2 + \\left(\\frac{8}{16} - \\frac{1}{16}\\right)\\lambda + \\left(\\frac{9}{32} - \\frac{1}{32}\\right) = 0$$\n$$\\lambda^2 + \\frac{7}{16}\\lambda + \\frac{8}{32} = 0$$\n$$\\lambda^2 + \\frac{7}{16}\\lambda + \\frac{1}{4} = 0$$\n\nWe solve this quadratic equation for $\\lambda$ using the quadratic formula, $\\lambda = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$:\n$$\\lambda = \\frac{-\\frac{7}{16} \\pm \\sqrt{\\left(\\frac{7}{16}\\right)^2 - 4(1)\\left(\\frac{1}{4}\\right)}}{2(1)} = \\frac{-\\frac{7}{16} \\pm \\sqrt{\\frac{49}{256} - 1}}{2}$$\n$$\\lambda = \\frac{-\\frac{7}{16} \\pm \\sqrt{\\frac{49 - 256}{256}}}{2} = \\frac{-\\frac{7}{16} \\pm i\\frac{\\sqrt{207}}{16}}{2}$$\nThe two eigenvalues are a complex conjugate pair:\n$$\\lambda_{1,2} = -\\frac{7}{32} \\pm i\\frac{\\sqrt{207}}{32}$$\n\nThe spectral radius $\\rho(T_{\\omega})$ is the maximum absolute value (modulus) of the eigenvalues. Since the eigenvalues are a complex conjugate pair, their moduli are equal. We calculate the modulus squared of one of the eigenvalues:\n$$|\\lambda|^2 = \\left(-\\frac{7}{32}\\right)^2 + \\left(\\frac{\\sqrt{207}}{32}\\right)^2 = \\frac{49}{1024} + \\frac{207}{1024} = \\frac{49 + 207}{1024} = \\frac{256}{1024} = \\frac{1}{4}$$\nThe modulus is the square root of this value:\n$$|\\lambda| = \\sqrt{\\frac{1}{4}} = \\frac{1}{2}$$\nBoth eigenvalues have a modulus of $\\frac{1}{2}$. Therefore, the spectral radius of $T_{\\omega}$ is:\n$$\\rho(T_{\\omega}) = \\max(|\\lambda_1|, |\\lambda_2|) = \\frac{1}{2}$$", "answer": "$$\\boxed{\\frac{1}{2}}$$", "id": "3581656"}, {"introduction": "Having mastered the mechanics of constructing the SOR matrix, we now turn to the theory that makes it so powerful. This practice explores the celebrated results of D.M. Young for the 1D Poisson model problem, a cornerstone of numerical analysis [@problem_id:3367845]. You will derive the analytical relationship between the convergence rates of the Jacobi, Gauss-Seidel, and SOR methods, culminating in the formula for the optimal relaxation parameter, $\\omega_{\\mathrm{opt}}$, that yields the fastest convergence.", "problem": "Consider the one-dimensional Poisson equation $-u''(x)=f(x)$ on the interval $[0,1]$ with homogeneous Dirichlet boundary conditions $u(0)=u(1)=0$. Discretize the problem using a uniform grid with $n$ interior points and grid spacing $h=\\frac{1}{n+1}$, leading to the linear system $A \\mathbf{u}=\\mathbf{b}$ with the tridiagonal Toeplitz matrix $A=\\operatorname{tridiag}(-1,2,-1) \\in \\mathbb{R}^{n \\times n}$.\n\nLet $\\mathcal{J}$ denote the Jacobi iteration with splitting $A=D-L-U$, where $D=\\operatorname{diag}(2)$, $L$ and $U$ are the strictly lower and strictly upper parts with entries $1$ on their first sub- and super-diagonals, respectively, so that the Jacobi iteration matrix is $T_{J}=D^{-1}(L+U)$. It is well known that $T_{J}$ is diagonalizable by the discrete sine transform with eigenpairs $\\{(\\mu_{k},\\mathbf{v}^{(k)})\\}_{k=1}^{n}$, where $\\mathbf{v}^{(k)}$ has components $v^{(k)}_{i}=\\sin\\!\\left(\\frac{i k \\pi}{n+1}\\right)$ for $i=1,\\dots,n$, and $\\mu_{k}=\\cos\\!\\left(\\frac{k \\pi}{n+1}\\right)$.\n\nLet $\\mathcal{GS}$ denote the Gauss–Seidel (GS) method with iteration matrix $T_{GS}=(D-L)^{-1}U$, and let $\\mathcal{SOR}$ denote Successive Over-Relaxation (SOR) with relaxation parameter $\\omega \\in (0,2)$ and iteration matrix $T_{\\omega}=(D-\\omega L)^{-1}\\!\\left((1-\\omega)D+\\omega U\\right)$.\n\nUsing only fundamental properties of stationary iterations, the structure of $A$ from the finite-difference discretization, and well-tested results on consistently ordered splittings of $A$, do the following:\n\n1. Starting from the eigenpairs of $T_{J}$, derive an expression for the spectral radius of Gauss–Seidel, $\\rho_{GS}$, in terms of $\\{\\mu_{k}\\}_{k=1}^{n}$. Justify all steps from first principles of error-propagation for linear stationary iterations and the structural properties of $A$.\n\n2. Using the classical theorem of Young relating the optimal SOR relaxation parameter to the Jacobi spectral radius for consistently ordered splittings, obtain a closed-form expression for the optimal $\\omega$ that minimizes the spectral radius $\\rho(T_{\\omega})$ in terms of $n$.\n\nProvide your final answer as a single analytic expression in terms of $n$, with no units. No numerical rounding is required.", "solution": "The problem as stated is valid. It is scientifically grounded in the well-established theory of numerical linear algebra and the analysis of iterative methods for solving linear systems derived from partial differential equations. The problem is well-posed, objective, self-contained, and all provided information, including the matrix structure and the eigenvalues of the Jacobi iteration matrix, is factually correct.\n\nThe problem asks for two results: first, an expression for the spectral radius of the Gauss-Seidel (GS) iteration, $\\rho_{GS}$, in terms of the eigenvalues of the Jacobi iteration matrix, $\\{\\mu_{k}\\}$; second, a closed-form expression for the optimal Successive Over-Relaxation (SOR) parameter, $\\omega_{opt}$, as a function of the number of interior grid points, $n$.\n\nThe system matrix $A = \\operatorname{tridiag}(-1, 2, -1)$ is a symmetric tridiagonal matrix. Such matrices are known to be consistently ordered. This property is central, as it establishes a precise analytical relationship between the eigenvalues of the Jacobi ($T_J$), Gauss-Seidel ($T_{GS}$), and SOR ($T_{\\omega}$) iteration matrices.\n\n**Part 1: Spectral Radius of Gauss-Seidel Iteration**\n\nFor a consistently ordered matrix splitting, it is a classical result (due to D.M. Young) that if $\\mu$ is an eigenvalue of the Jacobi matrix $T_J$, then $\\lambda = \\mu^2$ is an eigenvalue of the Gauss-Seidel matrix $T_{GS}$. The problem provides the eigenvalues of $T_J$ as $\\mu_{k} = \\cos\\left(\\frac{k \\pi}{n+1}\\right)$ for $k=1, \\dots, n$.\nTherefore, the eigenvalues of the GS iteration matrix, $T_{GS}$, are given by the set $\\{\\lambda_k\\}_{k=1}^n$ where $\\lambda_k = \\mu_k^2 = \\cos^2\\left(\\frac{k \\pi}{n+1}\\right)$.\n\nThe spectral radius of an iteration matrix is defined as the maximum of the absolute values of its eigenvalues. For the Gauss-Seidel method, this is $\\rho_{GS} = \\rho(T_{GS}) = \\max_{k} |\\lambda_k|$.\nSubstituting the expression for $\\lambda_k$ in terms of $\\mu_k$:\n$$ \\rho_{GS} = \\max_{k=1,\\dots,n} |\\mu_k^2| $$\nSince the Jacobi eigenvalues $\\mu_k$ are real numbers, $\\mu_k^2$ is always non-negative. Thus, the absolute value is redundant. The expression for the spectral radius of the Gauss-Seidel iteration in terms of the Jacobi eigenvalues is:\n$$ \\rho_{GS} = \\max_{k=1,\\dots,n} (\\mu_k^2) $$\nThis can also be written in terms of the spectral radius of the Jacobi matrix, $\\rho_J = \\rho(T_J) = \\max_k |\\mu_k|$, as $\\rho_{GS} = (\\rho_J)^2$, because the squaring function is monotonic for non-negative arguments.\n\n**Part 2: Optimal SOR Relaxation Parameter**\n\nTo find the optimal SOR relaxation parameter, $\\omega_{opt}$, we first need to determine the spectral radius of the Jacobi matrix, $\\rho_J$.\n$$ \\rho_J = \\max_{k=1,\\dots,n} |\\mu_k| = \\max_{k=1,\\dots,n} \\left|\\cos\\left(\\frac{k \\pi}{n+1}\\right)\\right| $$\nThe arguments of the cosine function, $\\frac{k \\pi}{n+1}$, lie in the interval $(0, \\pi)$ for $k \\in \\{1, 2, \\dots, n\\}$. In this interval, the cosine function is strictly decreasing from $1$ to $-1$. The maximum absolute value will occur at the arguments closest to $0$ or $\\pi$.\nFor $k=1$, the argument is $\\frac{\\pi}{n+1}$.\nFor $k=n$, the argument is $\\frac{n \\pi}{n+1} = \\pi - \\frac{\\pi}{n+1}$.\nThe corresponding eigenvalues are $\\mu_1 = \\cos\\left(\\frac{\\pi}{n+1}\\right)$ and $\\mu_n = \\cos\\left(\\frac{n\\pi}{n+1}\\right) = -\\cos\\left(\\frac{\\pi}{n+1}\\right)$.\nThe maximum absolute value is therefore:\n$$ \\rho_J = \\left|\\cos\\left(\\frac{\\pi}{n+1}\\right)\\right| = \\cos\\left(\\frac{\\pi}{n+1}\\right) $$\nsince for $n \\ge 1$, the argument $\\frac{\\pi}{n+1}$ is in $(0, \\pi/2]$, where cosine is non-negative. Note that for any integer $n \\ge 1$, we have $\\rho_J  1$, which ensures the convergence of the Jacobi method.\n\nThe problem allows the use of the classical theorem of Young, which relates the optimal SOR parameter $\\omega_{opt}$ to the Jacobi spectral radius $\\rho_J$ for consistently ordered matrices with real Jacobi eigenvalues. The formula is:\n$$ \\omega_{opt} = \\frac{2}{1 + \\sqrt{1 - \\rho_J^2}} $$\nSubstituting the expression for $\\rho_J$:\n$$ \\omega_{opt} = \\frac{2}{1 + \\sqrt{1 - \\cos^2\\left(\\frac{\\pi}{n+1}\\right)}} $$\nUsing the fundamental trigonometric identity $\\sin^2(x) + \\cos^2(x) = 1$, we can simplify the term inside the square root:\n$$ \\omega_{opt} = \\frac{2}{1 + \\sqrt{\\sin^2\\left(\\frac{\\pi}{n+1}\\right)}} $$\nSince $n \\ge 1$, the argument $\\frac{\\pi}{n+1}$ lies in the interval $(0, \\pi/2]$, where the sine function is non-negative. Therefore, the square root simplifies to:\n$$ \\sqrt{\\sin^2\\left(\\frac{\\pi}{n+1}\\right)} = \\sin\\left(\\frac{\\pi}{n+1}\\right) $$\nSubstituting this back into the expression for $\\omega_{opt}$ yields the final closed-form expression:\n$$ \\omega_{opt} = \\frac{2}{1 + \\sin\\left(\\frac{\\pi}{n+1}\\right)} $$\nThis expression gives the optimal relaxation parameter that minimizes the spectral radius of the SOR iteration matrix for the given discretized Poisson problem, as a function of the number of interior grid points $n$.", "answer": "$$ \\boxed{\\frac{2}{1 + \\sin\\left(\\frac{\\pi}{n+1}\\right)}} $$", "id": "3367845"}, {"introduction": "Our final practice moves from idealized model problems to the conceptual challenges encountered in applied computational fluid dynamics (CFD). This exercise examines the pressure Poisson equation and contrasts the behavior of the SOR method under different physical boundary conditions [@problem_id:3367883]. You will analyze how the properties of the system matrix—specifically, whether it is positive definite or singular—dictate the convergence of the iteration and necessitate practical strategies like gauge fixing for problems with nullspaces.", "problem": "Consider the steady incompressible pressure Poisson equation in Computational Fluid Dynamics (CFD),\n$$-\\nabla^2 p = f \\quad \\text{in } \\Omega,$$\nand its standard second-order finite-difference discretization on a uniform Cartesian grid of spacing $h$ covering $\\Omega = (0,1)^2$. Use lexicographic ordering for the unknowns and the standard $5$-point Laplacian. Let $A \\in \\mathbb{R}^{n \\times n}$ be the resulting coefficient matrix and $b \\in \\mathbb{R}^n$ the discrete right-hand side. Consider two boundary-condition scenarios:\n- Pure Dirichlet boundary conditions, $p=0$ on $\\partial \\Omega$, yielding matrix $A_D$.\n- Pure Neumann boundary conditions, $\\partial p/\\partial n=0$ on $\\partial \\Omega$, yielding matrix $A_N$.\n\nWith the splitting $A = D - L - U$, where $D$ is the diagonal of $A$ and $L$ and $U$ are the strict lower and upper triangular parts, respectively, define the Successive Over-Relaxation (SOR) method with relaxation parameter $\\omega \\in \\mathbb{R}$ by\n$$p^{(k+1)} = (D - \\omega L)^{-1}\\Big(\\big(1-\\omega\\big)D + \\omega U\\Big)p^{(k)} + \\omega (D - \\omega L)^{-1} b.$$\nThe corresponding error-propagation matrix is\n$$G_{\\text{SOR}}(\\omega) = (D - \\omega L)^{-1}\\Big(\\big(1-\\omega\\big)D + \\omega U\\Big),$$\nso that the error $e^{(k)} = p^{(k)} - p^\\star$ evolves as $e^{(k+1)} = G_{\\text{SOR}}(\\omega) e^{(k)}$, where $p^\\star$ is a fixed point of the iteration. Convergence of the iteration for a given $\\omega$ is characterized by the spectral radius $\\rho\\big(G_{\\text{SOR}}(\\omega)\\big)$.\n\nStarting from the fundamental discrete properties of the Laplacian, namely symmetry, definiteness under Dirichlet conditions, and constant-field nullspace under Neumann conditions, and from the definition of the SOR iteration above, reason about how pure Dirichlet versus pure Neumann boundary conditions affect the spectrum of $G_{\\text{SOR}}(\\omega)$ and the choice of $\\omega$.\n\nSelect all correct statements:\n\nA. Under pure Dirichlet conditions, $A_D$ is Symmetric Positive Definite (SPD). For any relaxation parameter $0  \\omega  2$, the SOR iteration is convergent with $\\rho\\big(G_{\\text{SOR}}(\\omega)\\big)1$, and there exists an optimal $\\omega^\\star \\in (1,2)$ that minimizes $\\rho\\big(G_{\\text{SOR}}(\\omega)\\big)$, yielding strict contraction across all error modes.\n\nB. Under pure Neumann conditions, even when the discrete compatibility condition $\\sum_{i=1}^n b_i = 0$ holds, one can choose $\\omega \\in (0,2)$ so that $\\rho\\big(G_{\\text{SOR}}(\\omega)\\big)1$ because over-relaxation suppresses the constant nullspace mode of $A_N$.\n\nC. Under pure Neumann conditions, $A_N$ is singular with a one-dimensional nullspace spanned by the discrete constant vector $\\mathbf{1}$. Then $G_{\\text{SOR}}(\\omega)$ has an eigenvalue $\\lambda=1$ associated with $\\mathbf{1}$, hence $\\rho\\big(G_{\\text{SOR}}(\\omega)\\big)\\ge 1$. Nevertheless, for any $0  \\omega  2$, SOR contracts error components orthogonal to the nullspace. To obtain strict contraction of the full error, one may fix the gauge (e.g., enforce $\\sum_{i=1}^n p_i = 0$ or pin a single degree of freedom), which removes the nullspace; after such gauge fixing, one can choose $\\omega^\\star \\in (1,2)$ analogously to the Dirichlet case.\n\nD. In nearly singular situations (for example, weak enforcement of a pressure reference or geometries that make the smallest eigenvalue of $A$ very close to $0$), selecting $\\omega$ closer to $1$ reduces sensitivity to the near-nullspace and improves robustness, at the expense of slower convergence in well-resolved modes; additionally, projecting the iterate to enforce $\\sum_{i=1}^n p_i = 0$ at each step prevents drift along the near-nullspace direction.", "solution": "The problem statement is a valid exercise in numerical linear algebra, specifically concerning the application of the Successive Over-Relaxation (SOR) method to the linear systems arising from the finite-difference discretization of the Poisson equation with different boundary conditions. The definitions, equations, and scenarios described are standard in the fields of computational fluid dynamics and numerical analysis. The problem is scientifically grounded, well-posed, and objective. We may proceed with the solution.\n\nThe core of the problem is to analyze the convergence properties of the SOR method, which depend on the spectral radius $\\rho\\big(G_{\\text{SOR}}(\\omega)\\big)$ of the iteration matrix $G_{\\text{SOR}}(\\omega)$. The properties of $G_{\\text{SOR}}(\\omega)$ are, in turn, determined by the properties of the system matrix $A$, which are dictated by the underlying physics and boundary conditions (Dirichlet vs. Neumann).\n\nLet us analyze the two scenarios.\n\n**Scenario 1: Pure Dirichlet Boundary Conditions**\n\nThe standard $5$-point finite-difference discretization of the negative Laplacian, $-\\nabla^2$, on a square domain with pure Dirichlet boundary conditions ($p=0$ on $\\partial\\Omega$) results in a matrix $A_D$. This matrix has the following established properties:\n1.  **Symmetry**: $A_D$ is symmetric. This anises because the coupling between any two grid nodes $i$ and $j$ is reciprocal.\n2.  **Positive Definiteness**: $A_D$ is positive definite. Its eigenvalues are all strictly positive. For an $M \\times M$ grid of interior nodes, the eigenvalues of the discrete Laplacian operator are known to be real and negative, which means the eigenvalues of $A_D$ (corresponding to $-\\nabla^2$) are all real and positive. Thus, for any non-zero vector $x \\in \\mathbb{R}^n$, $x^T A_D x > 0$. A matrix that is Symmetric and Positive Definite is termed SPD.\n3.  **Diagonal Entries**: The diagonal entries of $A_D$ are positive. For the $5$-point stencil scaled by $1$, the diagonal entry for each node is $4$.\n4.  **Consistent Ordering**: The lexicographic ordering of the grid points gives $A_D$ a block-tridiagonal structure, and it can be shown that $A_D$ is a consistently ordered matrix.\n\nFor a matrix $A$ that is real, symmetric, and has positive diagonal entries, the Ostrowski-Reich theorem states that the SOR iteration converges if and only if $A$ is positive definite and the relaxation parameter $\\omega$ is in the range $0  \\omega  2$. Since $A_D$ is SPD, we are guaranteed that $\\rho\\big(G_{\\text{SOR}}(\\omega)\\big)  1$ for any $\\omega \\in (0,2)$.\n\nFurthermore, for SPD and consistently ordered matrices, the theory of Young provides a precise relationship for the optimal relaxation parameter $\\omega^\\star$ that minimizes $\\rho\\big(G_{\\text{SOR}}(\\omega)\\big)$. This optimal value is given by $\\omega^\\star = \\frac{2}{1 + \\sqrt{1 - \\rho(G_J)^2}}$, where $G_J = D^{-1}(L+U)$ is the Jacobi iteration matrix. For the given problem, $\\rho(G_J)  1$, which implies that $1  \\omega^\\star  2$. At this optimal value, the convergence is significantly accelerated compared to the Gauss-Seidel method ($\\omega=1$).\n\n**Scenario 2: Pure Neumann Boundary Conditions**\n\nThe discretization of the Poisson equation with pure Neumann boundary conditions ($\\partial p/\\partial n=0$ on $\\partial\\Omega$) results in a matrix $A_N$. This problem is fundamentally different because the solution is only defined up to an arbitrary constant.\n1.  **Symmetry**: With a suitable finite-difference scheme (e.g., using ghost nodes and centered differences), the resulting matrix $A_N$ is symmetric.\n2.  **Singularity**: $A_N$ is singular. The vector $\\mathbf{1} = (1, 1, \\dots, 1)^T$ represents a constant field. The condition $\\partial p/\\partial n = 0$ is satisfied by any constant $p$. Thus, the discrete operator $A_N$ annihilates the constant vector, i.e., $A_N \\mathbf{1} = \\mathbf{0}$. This means $\\mathbf{1}$ is in the nullspace of $A_N$, and $0$ is an eigenvalue. For the connected domain $\\Omega$, this nullspace is one-dimensional.\n3.  **Positive Semi-Definiteness**: Since all other eigenvalues of the Neumann Laplacian are positive, $A_N$ is Symmetric Positive Semi-definite (SPSD). For any vector $x \\in \\mathbb{R}^n$, $x^T A_N x \\ge 0$.\n\nLet's analyze the SOR iteration for a singular matrix $A_N$ where $A_N \\mathbf{1} = \\mathbf{0}$. This condition implies $(D-L-U)\\mathbf{1} = \\mathbf{0}$, or $D\\mathbf{1} = (L+U)\\mathbf{1}$. Let's investigate if $\\mathbf{1}$ is an eigenvector of $G_{\\text{SOR}}(\\omega)$. We test if $G_{\\text{SOR}}(\\omega)\\mathbf{1} = \\lambda \\mathbf{1}$.\n$$G_{\\text{SOR}}(\\omega)\\mathbf{1} = (D - \\omega L)^{-1}\\Big(\\big(1-\\omega\\big)D + \\omega U\\Big)\\mathbf{1}$$\nFor $\\lambda=1$ to be an eigenvalue, we must have $\\big(1-\\omega\\big)D\\mathbf{1} + \\omega U\\mathbf{1} = (D - \\omega L)\\mathbf{1}$.\nThis simplifies to $-\\omega D\\mathbf{1} + \\omega U\\mathbf{1} = -\\omega L\\mathbf{1}$, or $\\omega(L+U)\\mathbf{1} = \\omega D\\mathbf{1}$. Assuming $\\omega \\ne 0$, this is equivalent to $(L+U)\\mathbf{1} = D\\mathbf{1}$, which is true because $A_N\\mathbf{1}=0$.\nThus, for any $\\omega \\ne 0$, $\\mathbf{1}$ is an eigenvector of $G_{\\text{SOR}}(\\omega)$ with an eigenvalue of $1$. Consequently, the spectral radius $\\rho\\big(G_{\\text{SOR}}(\\omega)\\big) \\ge 1$, and the SOR method cannot converge to a unique solution. The component of the error in the direction of the nullspace, $\\mathbf{1}$, is not damped.\n\nHowever, on the subspace of vectors orthogonal to $\\mathbf{1}$, the matrix $A_N$ acts as a positive definite operator. It can be shown that for $0  \\omega  2$, the SOR iteration contracts all error components that are orthogonal to the nullspace.\n\nTo achieve convergence to a unique solution, the singularity must be removed. This process is known as gauge fixing. Methods include:\n- Pinning the pressure at one node (e.g., $p_j = 0$), which removes a row and column from $A_N$, yielding a smaller SPD matrix.\n- Enforcing a global constraint, such as $\\sum_{i=1}^n p_i = 0$, often implemented by projecting the solution vector onto the subspace of vectors with zero mean at each iteration.\nOnce the system is regularized in this way, the resulting matrix is SPD, and the analysis from the Dirichlet case applies, including the existence of an optimal $\\omega^\\star \\in (1,2)$.\n\nNow we evaluate each option.\n\n**A. Under pure Dirichlet conditions, $A_D$ is Symmetric Positive Definite (SPD). For any relaxation parameter $0  \\omega  2$, the SOR iteration is convergent with $\\rho\\big(G_{\\text{SOR}}(\\omega)\\big)1$, and there exists an optimal $\\omega^\\star \\in (1,2)$ that minimizes $\\rho\\big(G_{\\text{SOR}}(\\omega)\\big)$, yielding strict contraction across all error modes.**\nThis statement is a direct consequence of the properties of the discrete Laplacian with Dirichlet boundary conditions and the Ostrowski-Reich and Young theorems for SOR. As derived above, $A_D$ is SPD, and for such matrices, SOR converges for all $\\omega \\in (0,2)$. Since $A_D$ is also consistently ordered, an optimal over-relaxation parameter $\\omega^\\star \\in (1,2)$ exists.\n**Verdict: Correct.**\n\n**B. Under pure Neumann conditions, even when the discrete compatibility condition $\\sum_{i=1}^n b_i = 0$ holds, one can choose $\\omega \\in (0,2)$ so that $\\rho\\big(G_{\\text{SOR}}(\\omega)\\big)1$ because over-relaxation suppresses the constant nullspace mode of $A_N$.**\nThis statement is incorrect. As shown in our analysis, the nullspace vector $\\mathbf{1}$ is an eigenvector of the SOR iteration matrix $G_{\\text{SOR}}(\\omega)$ with an eigenvalue of exactly $1$ for any $\\omega \\neq 0$. Therefore, $\\rho\\big(G_{\\text{SOR}}(\\omega)\\big) \\ge 1$. Over-relaxation does not suppress the nullspace mode; it preserves it. The compatibility condition $\\sum b_i = 0$ ensures a solution exists, but does not guarantee convergence of the SOR iteration to a unique solution.\n**Verdict: Incorrect.**\n\n**C. Under pure Neumann conditions, $A_N$ is singular with a one-dimensional nullspace spanned by the discrete constant vector $\\mathbf{1}$. Then $G_{\\text{SOR}}(\\omega)$ has an eigenvalue $\\lambda=1$ associated with $\\mathbf{1}$, hence $\\rho\\big(G_{\\text{SOR}}(\\omega)\\big)\\ge 1$. Nevertheless, for any $0  \\omega  2$, SOR contracts error components orthogonal to the nullspace. To obtain strict contraction of the full error, one may fix the gauge (e.g., enforce $\\sum_{i=1}^n p_i = 0$ or pin a single degree of freedom), which removes the nullspace; after such gauge fixing, one can choose $\\omega^\\star \\in (1,2)$ analogously to the Dirichlet case.**\nThis statement accurately summarizes the situation for the pure Neumann problem. It correctly identifies the singularity of $A_N$, the resulting eigenvalue of $1$ for $G_{\\text{SOR}}(\\omega)$, the convergence of error components outside the nullspace, and the standard methods of gauge fixing to restore well-posedness and allow for optimal over-relaxation. Every part of this statement is consistent with our derivation.\n**Verdict: Correct.**\n\n**D. In nearly singular situations (for example, weak enforcement of a pressure reference or geometries that make the smallest eigenvalue of $A$ very close to $0$), selecting $\\omega$ closer to $1$ reduces sensitivity to the near-nullspace and improves robustness, at the expense of slower convergence in well-resolved modes; additionally, projecting the iterate to enforce $\\sum_{i=1}^n p_i = 0$ at each step prevents drift along the near-nullspace direction.**\nThis statement addresses the practical issue of ill-conditioned systems, which behave similarly to singular ones. The theoretical optimal $\\omega^\\star$ approaches $2$ as a system becomes more ill-conditioned ($\\rho(G_J) \\to 1$). However, using such an aggressive $\\omega$ can excite slowly converging error modes associated with the near-nullspace, leading to poor practical performance or instability. Choosing a more conservative $\\omega$ (closer to $1$, the Gauss-Seidel value) is a common heuristic to improve robustness for these modes, sacrificing the theoretically optimal asymptotic convergence rate. Furthermore, projecting the iterate to remove any component in the near-nullspace direction (e.g., by enforcing $\\sum p_i = 0$) is a standard and effective technique to prevent the solution from drifting, which is a significant problem in nearly singular systems. The statement is a correct description of practical wisdom in numerical methods.\n**Verdict: Correct.**", "answer": "$$\\boxed{ACD}$$", "id": "3367883"}]}