{"hands_on_practices": [{"introduction": "Understanding the theoretical basis for stability is the first step toward effectively using under-relaxation. This practice guides you through a foundational stability analysis for the steady convection-diffusion equation, a canonical problem in CFD. By deriving a criterion that links the maximum stable under-relaxation factor $\\omega$ to the mesh Peclet number, you will gain crucial insight into how numerical stability is dictated by the interplay of physical transport phenomena and grid resolution [@problem_id:3386083].", "problem": "Consider a segregated fixed-point iteration used in Computational Fluid Dynamics (CFD) to solve the steady one-dimensional convection–diffusion equation discretized by the Finite Volume Method (FVM) on a uniform grid with cell width $h$. The governing continuous equation for a passive scalar field $\\,\\phi(x)\\,$ is $-\\alpha\\,\\frac{d^2 \\phi}{dx^2} + u\\,\\frac{d\\phi}{dx} = s(x)$, where $\\alpha > 0$ is the diffusion coefficient and $u$ is the constant convection speed. In the segregated iteration, the diffusion operator is treated implicitly while the convection operator is treated explicitly from the previous iterate. Define the face-diffusion coefficient $D_f = \\alpha/h$ and the face-convection magnitude $F_f = |u|$, and the mesh Peclet number by $Pe_h = \\dfrac{F_f}{2 D_f} = \\dfrac{|u| h}{2 \\alpha}$.\n\nStarting from first principles of linear stationary iterations and operator splitting, do the following:\n\n1) Model the segregated iteration as a linear fixed-point update for the discrete cell-centered unknowns $\\,\\phi^k\\,$ of the form $\\phi^{k+1} = G(\\omega)\\,\\phi^k + c$, where $\\,G(\\omega)\\,$ is the iteration matrix modified by Under-Relaxation (UR) with factor $\\,\\omega \\in (0,1]$, and $\\,c\\,$ is a constant vector arising from sources and boundary conditions. Derive the structure of $\\,G(\\omega)\\,$ by splitting the discretized operator into an implicit diffusion part and an explicit convection part. Then, using the central-difference stencil for convection, show that the spectrum of the preconditioned convection-to-diffusion operator $\\,D^{-1} C\\,$ is purely imaginary and obtain a bound on its spectral radius in terms of the mesh Peclet number $\\,Pe_h\\,$. Use only well-tested matrix-analysis tools, such as induced norms or Gershgorin-type bounds, and fundamental properties of skew-symmetric matrices.\n\n2) From the derived bound, obtain a sufficient stability criterion for the under-relaxation factor $\\,\\omega\\,$ such that the segregated iteration is contractive, i.e., the spectral radius of $\\,G(\\omega)\\,$ is strictly less than $\\,1\\,$. Express the maximum stable $\\,\\omega_{\\max}\\,$ as a function of the mesh Peclet number $\\,Pe_h\\,$ and rigorously justify each step from the properties of the iteration matrix. Your derivation must start from the fixed-point iteration structure and must not use shortcut formulas.\n\n3) Propose an adaptive under-relaxation scheme that uses the local (cellwise) mesh Peclet number $\\,Pe_i\\,$ to reduce $\\,\\omega\\,$ in high-Peclet regions to avoid spurious oscillations. The scheme must satisfy both of the following: \n   - Respect the sufficient stability bound you derived, i.e., no cell’s under-relaxation factor exceeds the sufficient stability value implied by its $\\,Pe_i\\,$.\n   - Enforce a global cap $\\,\\omega \\le 1\\,$. \n   For definiteness, use the formula $\\,\\omega_i = \\min\\!\\big( \\omega_{\\text{base}},\\, \\min\\!\\big(1,\\, \\omega_{\\max}(Pe_i)\\big) \\big)\\,$ for per-cell factors, where $\\,\\omega_{\\text{base}} \\in (0,1]$ is a user-selected baseline value. Also define a global recommendation $\\,\\omega_{\\text{global}} = \\min\\limits_i \\omega_{\\max}(Pe_i)\\,$, which guarantees the sufficient stability bound holds in all cells when a single global $\\,\\omega\\,$ is used.\n\n4) Implement a complete program that computes, for a given set of test inputs, the adaptive per-cell under-relaxation factors and global recommendation according to the derived sufficient stability bound. All quantities in this problem are dimensionless, and no physical units are required. For numerical reporting, round floating-point results to $8$ decimal places.\n\nTest Suite:\n- Test Case 1 (Global Recommendation, “happy path”): $Pe = [0.5,\\, 1.0,\\, 2.0]$, and baseline $\\omega_{\\text{base}} = 0.95$. Report $\\,\\omega_{\\text{global}}\\,$ as a single float.\n- Test Case 2 (Per-Cell Adaptive, mixed Peclet): $Pe = [0.1,\\, 1.0,\\, 5.0,\\, 10.0]$, and baseline $\\omega_{\\text{base}} = 0.8$. Report the list $[\\omega_1,\\,\\omega_2,\\,\\omega_3,\\,\\omega_4]$.\n- Test Case 3 (Global Recommendation, high Peclet edge case): $Pe = [0.0,\\, 100.0,\\, 50.0]$, and baseline $\\omega_{\\text{base}} = 1.0$. Report $\\,\\omega_{\\text{global}}\\,$ as a single float.\n- Test Case 4 (Per-Cell Adaptive, including zero Peclet): $Pe = [0.0,\\, 0.5,\\, 2.0,\\, 20.0,\\, 100.0]$, and baseline $\\omega_{\\text{base}} = 0.9$. Report the list $[\\omega_1,\\,\\omega_2,\\,\\omega_3,\\,\\omega_4,\\,\\omega_5]$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the four test cases described above. Specifically, the output should be of the form \n$[\\omega_{\\text{global,TC1}}, [\\omega_i\\ \\text{for TC2}], \\omega_{\\text{global,TC3}}, [\\omega_i\\ \\text{for TC4}]]$ \nwith each floating-point value rounded to $8$ decimal places.", "solution": "The problem requires the analysis and implementation of an under-relaxation scheme for a segregated iterative solver applied to the one-dimensional steady convection-diffusion equation. The analysis proceeds in four parts as stipulated.\n\n### 1) Derivation of the Iteration Matrix and Spectral Bound\n\nFirst, we discretize the governing equation, $-\\alpha\\,\\frac{d^2 \\phi}{dx^2} + u\\,\\frac{d\\phi}{dx} = s(x)$, using the Finite Volume Method (FVM) on a uniform grid with cell width $h$. Integrating over a control volume around cell $i$ and using central differences for both diffusion and convection fluxes yields the discrete equation for the cell-centered unknown $\\phi_i$:\n$$\n\\left(-\\frac{\\alpha}{h}(\\phi_{i+1} - \\phi_i)\\right) - \\left(-\\frac{\\alpha}{h}(\\phi_i - \\phi_{i-1})\\right) + \\left(u \\frac{\\phi_i + \\phi_{i+1}}{2}\\right) - \\left(u \\frac{\\phi_{i-1} + \\phi_i}{2}\\right) = S_i\n$$\nwhere $S_i$ is the integrated source term for cell $i$. Grouping terms, we can separate the contributions from the diffusion and convection operators. The discrete system for the vector of unknowns $\\phi$ can be written as $(D_{mat} + C_{mat})\\phi = b$, where $b$ is the vector of source and boundary condition terms. The matrices $D_{mat}$ and $C_{mat}$ represent the discretized diffusion and convection operators, respectively. For an interior cell $i$, their non-zero entries are:\n- **Diffusion Matrix ($D_{mat}$)**: $(D_{mat}\\phi)_i = \\frac{\\alpha}{h}(- \\phi_{i-1} + 2\\phi_i - \\phi_{i+1})$. This matrix is symmetric and, with appropriate boundary conditions, positive definite.\n- **Convection Matrix ($C_{mat}$)**: $(C_{mat}\\phi)_i = \\frac{u}{2}(- \\phi_{i-1} + \\phi_{i+1})$. This matrix is real and skew-symmetric, i.e., $C_{mat}^T = -C_{mat}$.\n\nThe segregated iteration treats the diffusion part implicitly and the convection part explicitly. This corresponds to a matrix splitting where the iteration for an intermediate field $\\phi^*$ is given by:\n$$\nD_{mat} \\phi^* = -C_{mat} \\phi^k + b\n$$\nThe under-relaxation step with factor $\\omega \\in (0,1]$ updates the solution from the previous iterate $\\phi^k$ using $\\phi^*$:\n$$\n\\phi^{k+1} = (1-\\omega)\\phi^k + \\omega \\phi^* = (1-\\omega)\\phi^k + \\omega D_{mat}^{-1}(-C_{mat} \\phi^k + b)\n$$\nRearranging this gives the standard linear fixed-point form $\\phi^{k+1} = G(\\omega)\\phi^k + c$:\n$$\n\\phi^{k+1} = \\left((1-\\omega)I - \\omega D_{mat}^{-1} C_{mat}\\right) \\phi^k + \\omega D_{mat}^{-1} b\n$$\nThe iteration matrix is thus $G(\\omega) = (1-\\omega)I - \\omega G_{PC}$, where $G_{PC} = D_{mat}^{-1} C_{mat}$ is the preconditioned convection-to-diffusion operator. The constant vector is $c = \\omega D_{mat}^{-1} b$.\n\nNext, we show that the spectrum of $G_{PC}$ is purely imaginary. Let $\\lambda$ be an eigenvalue of $G_{PC}$ with corresponding eigenvector $v$:\n$$\nD_{mat}^{-1} C_{mat} v = \\lambda v \\implies C_{mat} v = \\lambda D_{mat} v\n$$\nTaking the conjugate transpose of this equation gives $v^* C_{mat}^* = \\bar{\\lambda} v^* D_{mat}^*$. Since $D_{mat}$ is real and symmetric ($D_{mat}^* = D_{mat}$) and $C_{mat}$ is real and skew-symmetric ($C_{mat}^* = C_{mat}^T = -C_{mat}$), this becomes $-v^* C_{mat} = \\bar{\\lambda} v^* D_{mat}$.\nLeft-multiplying the original eigenvalue equation by $v^*$ yields $v^* C_{mat} v = \\lambda v^* D_{mat} v$. Adding this to the result from the conjugate transpose, $-v^* C_{mat} v = \\bar{\\lambda} v^* D_{mat} v$, gives:\n$$\n0 = (\\lambda + \\bar{\\lambda})(v^* D_{mat} v) = 2 \\text{Re}(\\lambda) (v^* D_{mat} v)\n$$\nSince $D_{mat}$ is positive definite, $v^* D_{mat} v > 0$ for any non-zero eigenvector $v$. Therefore, we must have $\\text{Re}(\\lambda) = 0$, which proves that the eigenvalues of $G_{PC}$ are purely imaginary, i.e., of the form $\\lambda = i\\beta$ for some real $\\beta$.\n\nTo obtain a bound on the spectral radius $\\rho(G_{PC}) = \\max |\\lambda|$, we perform a local analysis. Let $v_k$ be the component of an eigenvector $v$ with the maximum magnitude, $|v_k| \\ge |v_j|$ for all $j$. The $k$-th row of the eigenvalue equation $C_{mat} v = \\lambda D_{mat} v$ is:\n$$\n\\frac{u}{2}(v_{k+1} - v_{k-1}) = \\lambda \\frac{\\alpha}{h}(-v_{k-1} + 2v_k - v_{k+1})\n$$\nAssuming $v_k \\ne 0$, we divide by it and rearrange to solve for $\\lambda$:\n$$\n\\lambda = \\frac{\\frac{u}{2}(\\frac{v_{k+1}}{v_k} - \\frac{v_{k-1}}{v_k})}{\\frac{\\alpha}{h}(2 - \\frac{v_{k-1}}{v_k} - \\frac{v_{k+1}}{v_k})} = \\frac{|u|h}{2\\alpha} \\frac{\\frac{u}{|u|}(\\frac{v_{k+1}}{v_k} - \\frac{v_{k-1}}{v_k})}{2 - (\\frac{v_{k-1}}{v_k} + \\frac{v_{k+1}}{v_k})}\n$$\nLet $z_1 = \\frac{v_{k+1}}{v_k}$ and $z_2 = \\frac{v_{k-1}}{v_k}$. By definition of $v_k$, $|z_1| \\le 1$ and $|z_2| \\le 1$. The mesh Peclet number is $Pe_h = \\frac{|u|h}{2\\alpha}$. Taking the magnitude:\n$$\n|\\lambda| = Pe_h \\left| \\frac{z_1 - z_2}{2 - (z_1+z_2)} \\right| \\le Pe_h \\sup_{|z_1|\\le 1, |z_2|\\le 1} \\frac{|z_1 - z_2|}{|2 - (z_1+z_2)|}\n$$\nThe maximizing choice for the expression on the right within the defined constraints occurs for $z_1 = i$ and $z_2 = -i$, which yields a value of $\\frac{|i - (-i)|}{|2 - (i-i)|} = \\frac{|2i|}{|2|} = 1$. Thus, we obtain the bound on the spectral radius:\n$$\n\\rho(G_{PC}) = \\max |\\lambda| \\le Pe_h\n$$\n\n### 2) Derivation of the Stability Criterion\n\nFor the iteration to be contractive, the spectral radius of the iteration matrix $G(\\omega)$ must be strictly less than $1$, i.e., $\\rho(G(\\omega)) < 1$. The eigenvalues $\\mu$ of $G(\\omega)$ are related to the eigenvalues $\\lambda$ of $G_{PC}$ by $\\mu = (1-\\omega) - \\omega \\lambda$.\nUsing our findings from Part 1, we have $\\lambda = i\\beta$ with $|\\beta| \\le Pe_h$. The magnitude of $\\mu$ is:\n$$\n|\\mu|^2 = |(1-\\omega) - i\\omega\\beta|^2 = (1-\\omega)^2 + (\\omega\\beta)^2\n$$\nTo ensure stability, we require $|\\mu|^2 < 1$ for all possible values of $\\beta$. This condition is most restrictive for the largest value of $|\\beta|$, so a sufficient condition for stability is found by setting $|\\beta| = \\rho(G_{PC})$ and using our bound, $|\\beta| \\le Pe_h$:\n$$\n(1-\\omega)^2 + \\omega^2 Pe_h^2 < 1\n$$\nExpanding and simplifying the inequality, for $\\omega \\in (0,1]$:\n$$\n1 - 2\\omega + \\omega^2 + \\omega^2 Pe_h^2 < 1\n$$\n$$\n-2\\omega + \\omega^2(1+Pe_h^2) < 0\n$$\nSince $\\omega > 0$, we can divide by it:\n$$\n-2 + \\omega(1+Pe_h^2) < 0\n$$\n$$\n\\omega (1+Pe_h^2) < 2 \\implies \\omega < \\frac{2}{1+Pe_h^2}\n$$\nThis is the sufficient stability criterion derived from our spectral bound. Since the problem specifies that the under-relaxation factor must be in the range $\\omega \\in (0,1]$, the maximum allowed value for $\\omega$ must satisfy both constraints. Therefore, the maximum stable under-relaxation factor, $\\omega_{\\max}$, is given by:\n$$\n\\omega_{\\max}(Pe_h) = \\min\\left(1, \\frac{2}{1+Pe_h^2}\\right)\n$$\n\n### 3) Adaptive Under-Relaxation Scheme\n\nBased on the derived stability bound, we can define an adaptive under-relaxation scheme that adjusts the factor $\\omega_i$ based on the local mesh Peclet number $Pe_i$.\n\nThe maximum stable under-relaxation factor for a cell with Peclet number $Pe_i$ is $\\omega_{\\max}(Pe_i) = \\min\\left(1, \\frac{2}{1+Pe_i^2}\\right)$. This function is $1$ for $Pe_i \\le 1$ and decreases towards $0$ as $Pe_i$ increases.\n\nThe scheme for the adaptive per-cell under-relaxation factor $\\omega_i$ is given by the formula:\n$$\n\\omega_i = \\min\\!\\big( \\omega_{\\text{base}},\\, \\min\\!\\big(1,\\, \\omega_{\\max}(Pe_i)\\big) \\big)\n$$\nAs reasoned in Part 2, the term $\\min(1, \\omega_{\\max}(Pe_i))$ is simply $\\omega_{\\max}(Pe_i)$ because $\\omega_{\\max}(Pe_i)$ is already capped at $1$. Therefore, the formula simplifies to:\n$$\n\\omega_i = \\min(\\omega_{\\text{base}}, \\omega_{\\max}(Pe_i)) = \\min\\left(\\omega_{\\text{base}}, \\frac{2}{1+Pe_i^2}\\right)\n$$\nNote that since $\\omega_{\\text{base}} \\le 1$, the combined expression is also capped at $1$.\n\nFor a single global under-relaxation factor $\\omega_{\\text{global}}$ that guarantees stability in all cells, we must choose the most restrictive (smallest) value among the stability limits of all cells:\n$$\n\\omega_{\\text{global}} = \\min_{i} \\omega_{\\max}(Pe_i)\n$$\nSince $\\omega_{\\max}(Pe)$ is a monotonically decreasing function of $Pe$ for $Pe \\ge 0$, this minimum corresponds to the cell with the maximum Peclet number, $Pe_{\\max} = \\max_i Pe_i$:\n$$\n\\omega_{\\text{global}} = \\omega_{\\max}(Pe_{\\max}) = \\min\\left(1, \\frac{2}{1+Pe_{\\max}^2}\\right)\n$$\n\n### 4) Program Implementation\n\nThe following Python program implements the derived formulas to compute the adaptive and global under-relaxation factors for the provided test suite.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by calculating under-relaxation factors for given test cases.\n    \"\"\"\n\n    test_cases = [\n        {'type': 'global', 'Pe': [0.5, 1.0, 2.0], 'w_base': 0.95},\n        {'type': 'per-cell', 'Pe': [0.1, 1.0, 5.0, 10.0], 'w_base': 0.8},\n        {'type': 'global', 'Pe': [0.0, 100.0, 50.0], 'w_base': 1.0},\n        {'type': 'per-cell', 'Pe': [0.0, 0.5, 2.0, 20.0, 100.0], 'w_base': 0.9},\n    ]\n\n    results = []\n    \n    def get_w_max(pe):\n        \"\"\"\n        Calculates the maximum stable under-relaxation factor for a given Peclet number.\n        omega_max(Pe) = min(1, 2 / (1 + Pe^2))\n        \"\"\"\n        if pe < 0:\n            # Peclet number is defined based on |u|, so it should be non-negative.\n            raise ValueError(\"Peclet number cannot be negative.\")\n        \n        # The stability bound derived from first principles.\n        bound = 2.0 / (1.0 + pe**2)\n        \n        # The final value is capped at 1.0 as per omega in (0, 1].\n        return min(1.0, bound)\n\n    for case in test_cases:\n        pe_list = case['Pe']\n        \n        if case['type'] == 'global':\n            # For the global recommendation, find the maximum Peclet number in the list.\n            if not pe_list:\n                # Handle empty list case to avoid error with max()\n                max_pe = 0.0\n            else:\n                max_pe = max(pe_list)\n            \n            # The global omega is determined by the most restrictive (highest Pe) case.\n            w_global = get_w_max(max_pe)\n            results.append(w_global)\n            \n        elif case['type'] == 'per-cell':\n            w_base = case['w_base']\n            w_adaptive_list = []\n            \n            # For the per-cell adaptive scheme, calculate omega for each cell.\n            for pe in pe_list:\n                # The local stability limit for the cell's Peclet number.\n                w_limit = get_w_max(pe)\n                \n                # The actual omega used is the minimum of the base value and the local limit.\n                # omega_i = min(omega_base, omega_max(Pe_i))\n                w_i = min(w_base, w_limit)\n                w_adaptive_list.append(w_i)\n            results.append(w_adaptive_list)\n\n    # Helper function to format results exactly as required.\n    def format_result(item):\n        if isinstance(item, list):\n            # Format each float in the list to 8 decimal places and join them.\n            formatted_list = [f\"{x:.8f}\" for x in item]\n            return f\"[{','.join(formatted_list)}]\"\n        elif isinstance(item, float):\n            # Format a single float to 8 decimal places.\n            return f\"{item:.8f}\"\n        else:\n            return str(item)\n\n    # Convert all results to the specified string format.\n    final_output_strings = [format_result(r) for r in results]\n    \n    # Print the final comma-separated list enclosed in square brackets.\n    print(f\"[{','.join(final_output_strings)}]\")\n\nsolve()\n```", "id": "3386083"}, {"introduction": "While theoretical analysis provides stability bounds, practical CFD problems are often highly nonlinear and require adaptive strategies to ensure robust convergence. This exercise moves from theory to implementation by tasking you with developing a monotonicity-safeguarded solver for a stiff reactive flow model. By implementing a backtracking line search on the relaxation factor, you will learn how to enforce a monotonic decrease in the residual, creating a robust iteration that can handle the strong nonlinearities where fixed relaxation often fails [@problem_id:3386114].", "problem": "You must design and implement a monotonicity-safeguarded under-relaxation scheme for iterative coupling that enforces a residual decrease at every accepted update and quantify its computational overhead versus iteration reduction on a representative reduced model for reactive compressible flows. The target audience is at the advanced graduate level in Computational Fluid Dynamics (CFD). The program must be complete and runnable.\n\nConsider the following dimensionless reduced steady model of a reactive compressible mixture in a single control volume. The unknown state vector is $x = [T, Y]^{\\top}$, where $T$ is a dimensionless temperature and $Y$ is a dimensionless reaction progress variable. The governing steady residual is defined by\n$$\nr(x) =\n\\begin{bmatrix}\nr_T(T, Y) \\\\\nr_Y(T, Y)\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n-\\alpha \\,(T - T_a) + q\\,S(T, Y) \\\\\n-\\beta\\,Y + S(T, Y)\n\\end{bmatrix},\n$$\nwhere the reaction source term is\n$$\nS(T, Y) = k_0 \\,\\exp\\!\\left(-\\frac{E}{T}\\right) \\,(1 - Y) - k_1 \\, Y,\n$$\nwith strictly positive parameters $\\alpha$, $\\beta$, $k_0$, $E$, $k_1$, $q$ and ambient temperature $T_a$. This model captures the essential stiffness and coupling features: the temperature $T$ enters $S(T, Y)$ through the Arrhenius factor $\\exp\\!\\left(-E/T\\right)$ and heat release $q\\,S(T,Y)$ couples back to $T$ via the energy residual $r_T$; $Y$ relaxes under destruction $\\beta Y$ and consumption $k_1 Y$ balanced by production $k_0 \\exp(-E/T)(1-Y)$.\n\nYou will use an iterative coupling map $G(x)$ defined by sequential sub-solves:\n- Freeze the temperature at the current iterate $T^k$ and compute the species update by solving $r_Y(T^k, Y) = 0$ for $Y$ exactly, yielding\n$$\nY^{\\star}(T^k) = \\frac{k_0 \\exp\\!\\left(-\\frac{E}{T^k}\\right)}{\\beta + k_1 + k_0 \\exp\\!\\left(-\\frac{E}{T^k}\\right)}.\n$$\n- Freeze the species at $Y^{\\star}(T^k)$ and compute the temperature update by solving $r_T(T, Y^{\\star}(T^k)) = 0$ for $T$ along a physically reasonable bracket $T \\in [T_{\\min}, T_{\\max}]$ with $T_{\\min} > 0$ and $T_{\\max}$ sufficiently large to encompass the expected solution. The solve must be robust to stiffness induced by large $E/T$.\n\nDefine the candidate coupling update direction $s^k = G(x^k) - x^k$. The monotonicity-safeguarded under-relaxation selects a step length $\\omega_k \\in (0, 1]$ by backtracking such that the accepted update $x^{k+1} = x^k + \\omega_k s^k$ satisfies the residual norm decrease\n$$\n\\left\\| r\\!\\left(x^{k+1}\\right) \\right\\|_2 \\le \\left\\| r\\!\\left(x^k\\right) \\right\\|_2,\n$$\nwhere $\\|\\cdot\\|_2$ denotes the Euclidean norm. If the condition is not met at a trial $\\omega$, reduce $\\omega$ multiplicatively by a constant factor in $(0,1)$ and retry until success or a minimum $\\omega_{\\min}$ is reached, in which case declare failure to find a monotonically decreasing step.\n\nYou must:\n- Derive the monotonicity safeguard from first principles, starting from fixed-point coupling, and show its relation to the residual norm decrease condition.\n- Implement two solvers:\n  1. A baseline iterative coupling with a fixed under-relaxation factor $\\omega_{\\text{const}} \\in (0,1]$ without monotonicity safeguard.\n  2. The monotonicity-safeguarded backtracking line search on $\\omega_k$ that enforces $\\left\\| r\\!\\left(x^{k+1}\\right) \\right\\|_2 \\le \\left\\| r\\!\\left(x^{k}\\right) \\right\\|_2$.\n- Use the same coupling map $G(x)$ for both solvers.\n- Use a convergence criterion $\\left\\| r\\!\\left(x^{k}\\right) \\right\\|_2 \\le \\varepsilon$ with tolerance $\\varepsilon = 10^{-6}$ and a maximum iteration count $k_{\\max} = 200$.\n- Quantify overhead versus iteration reduction by reporting, for each test case:\n  - The number of iterations to converge for the safeguarded method, as an integer.\n  - The number of residual norm evaluations performed by the safeguarded method, as an integer. Count only calls to compute $\\left\\| r(x) \\right\\|_2$ used for convergence checks and backtracking acceptance tests, not evaluations inside sub-solvers for $G(x)$.\n  - The total number of backtracking reductions of $\\omega$ over the entire run, as an integer.\n  - The number of iterations to converge for the baseline method, as an integer or $-1$ if it fails to converge within $k_{\\max}$.\n  - The number of residual norm evaluations for the baseline method, as an integer.\n  - The overhead, defined as the difference in residual norm evaluations between the safeguarded and baseline methods, as an integer.\n  - The iteration reduction, defined as the difference in iteration counts between baseline and safeguarded, as an integer (positive means fewer iterations with safeguard).\n\nAll variables and numbers in this problem are dimensionless; no physical units are required. Angles are not used. Percentages must not be used.\n\nTest suite and parameters. For each case, use $T_a = 1$ and the initial guess $x^0 = [T^0, Y^0]^{\\top}$ provided below:\n- Case A (mild coupling, happy path):\n  - $\\alpha = 1.0$, $\\beta = 0.5$, $k_0 = 10.0$, $E = 5.0$, $k_1 = 1.0$, $q = 1.0$, $T^0 = 0.8$, $Y^0 = 0.1$.\n- Case B (stiff reaction, strong Arrhenius nonlinearity):\n  - $\\alpha = 3.0$, $\\beta = 2.0$, $k_0 = 50.0$, $E = 20.0$, $k_1 = 1.0$, $q = 1.5$, $T^0 = 0.5$, $Y^0 = 0.0$.\n- Case C (near equilibrium initial condition):\n  - $\\alpha = 1.5$, $\\beta = 1.0$, $k_0 = 30.0$, $E = 10.0$, $k_1 = 0.5$, $q = 1.5$, $T^0 = 1.0$, $Y^0 = Y^{\\star}(T^0)$, where\n    $$\n    Y^{\\star}(T^0) = \\frac{k_0 \\exp\\!\\left(-\\frac{E}{T^0}\\right)}{\\beta + k_1 + k_0 \\exp\\!\\left(-\\frac{E}{T^0}\\right)}.\n    $$\n\nAlgorithmic settings to be used by your program:\n- Fixed baseline under-relaxation factor $\\omega_{\\text{const}} = 0.5$.\n- Backtracking reduction factor for the safeguarded method $\\theta = 0.5$ (i.e., $\\omega \\leftarrow \\theta \\,\\omega$ on each backtrack).\n- Minimum step length $\\omega_{\\min} = 10^{-8}$.\n- Bracketing interval for the temperature sub-solve: start with $T_{\\min} = 10^{-3}$ and $T_{\\max} = 3.0$; if no sign change is found for $r_T(T, Y^{\\star}(T^k))$, increase $T_{\\max}$ geometrically through the sequence $10.0$, $50.0$, $100.0$ until a sign change is found or declare failure to bracket.\n\nYour program must produce a single line of output containing the results for Case A, Case B, and Case C in order, as a comma-separated list of lists enclosed in square brackets. Each inner list must have the form\n$[\\text{iters\\_safe}, \\text{evals\\_safe}, \\text{backtracks\\_safe}, \\text{iters\\_base}, \\text{evals\\_base}, \\text{overhead}, \\text{iteration\\_delta}]$,\nwith all entries integers. For example, the printed output must look like\n$[[a_1,b_1,c_1,d_1,e_1,f_1,g_1],[a_2,b_2,c_2,d_2,e_2,f_2,g_2],[a_3,b_3,c_3,d_3,e_3,f_3,g_3]]$.", "solution": "The user has specified a problem that requires the design, implementation, and analysis of a monotonicity-safeguarded under-relaxation scheme for a coupled nonlinear system representative of reactive compressible flows. The problem is valid, scientifically grounded, and well-posed. We will proceed with the derivation and solution.\n\n### Theoretical Foundation\n\nThe core task is to find the steady-state solution $x$ for a system of nonlinear equations defined by the residual function $r(x) = 0$. The state vector is $x = [T, Y]^{\\top}$, representing dimensionless temperature and reaction progress. The system is described by:\n$$\nr(x) =\n\\begin{bmatrix}\nr_T(T, Y) \\\\\nr_Y(T, Y)\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n-\\alpha \\,(T - T_a) + q\\,S(T, Y) \\\\\n-\\beta\\,Y + S(T, Y)\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n0 \\\\\n0\n\\end{bmatrix}\n$$\nwhere the reactive source term $S(T, Y)$ is given by the Arrhenius-type expression:\n$$\nS(T, Y) = k_0 \\,\\exp\\!\\left(-\\frac{E}{T}\\right) \\,(1 - Y) - k_1 \\, Y\n$$\n\nA common strategy for such multiphysics problems is an iterative coupling or block Gauss-Seidel approach. The problem defines a specific coupling map, $G(x^k)$, which generates a candidate for the next iterate, $x_{\\text{cand}} = G(x^k)$, by solving the component equations sequentially. A fixed point of this map, $x = G(x)$, is a solution to the full system $r(x) = 0$.\n\nThe simplest iterative method is the direct fixed-point iteration, $x^{k+1} = G(x^k)$. However, this can be unstable, especially for stiff systems where the eigenvalues of the iteration's Jacobian can have large magnitudes. To improve robustness, we introduce an under-relaxation factor, $\\omega \\in (0, 1]$. The update is then a relaxed step in the direction of the candidate solution:\n$$\nx^{k+1} = x^k + \\omega (G(x^k) - x^k)\n$$\nHere, $s^k = G(x^k) - x^k$ is the update direction provided by the coupling map.\n\nThe problem requires a comparison of two strategies for selecting $\\omega$:\n$1$. **Baseline Method**: A constant under-relaxation factor $\\omega = \\omega_{\\text{const}}$ is used for all iterations. This is simple but may converge slowly or fail if the chosen factor is not suitable for the problem's specific stiffness.\n$2$. **Monotonicity-Safeguarded Method**: The step length $\\omega_k$ is chosen adaptively at each iteration $k$ to ensure progress towards the solution. Progress is measured by the Euclidean norm of the residual, $\\|r(x)\\|_2$. The safeguard enforces a simple monotonic decrease in this norm:\n$$\n\\left\\| r(x^{k+1}) \\right\\|_2 \\le \\left\\| r(x^k) \\right\\|_2\n$$\nThis condition is enforced using a backtracking line search. We start with an optimistic full step ($\\omega_k = 1$) and compute a trial state $x_{\\text{trial}} = x^k + \\omega_k s^k$. If the trial state satisfies the monotonicity condition, the step is accepted ($x^{k+1} = x_{\\text{trial}}$). If not, the step is deemed too aggressive. We then \"backtrack\" by reducing the step length multiplicatively ($\\omega_k \\leftarrow \\theta \\omega_k$, for a reduction factor $\\theta \\in (0, 1)$) and repeat the trial until an acceptable step is found or the step length becomes smaller than a prescribed minimum $\\omega_{\\min}$.\n\nThis safeguard turns the simple fixed-point iteration into a more robust nonlinear solver. While the direction $s^k$ is not necessarily a formal descent direction for the function $f(x) = \\|r(x)\\|_2$, the line search ensures that we do not accept steps that move to regions of higher residual, thus preventing many forms of iterative divergence common in stiff, strongly coupled problems. The primary trade-off is the increased computational cost per iteration due to the potential for multiple residual evaluations within the backtracking loop, versus the potential for a significant reduction in the total number of iterations required for convergence, especially in difficult cases.\n\n### Implementation Strategy\n\nThe implementation will consist of the following components:\n- Functions to compute the source term $S(T, Y)$ and the residual vector $r(x)$.\n- A function to compute the coupling map $G(x)$. This involves two steps: an analytical solution for the species update $Y^{\\star}$ and a numerical 1D root-find for the temperature update $T^{\\star}$. The `scipy.optimize.brentq` algorithm is ideal for the root-finding step, as it is robust and guaranteed to find a root within a given bracket where the function values have opposite signs. The implementation will include the specified logic for expanding the search bracket for $T^{\\star}$ if a root is not initially bracketed.\n- Two solver functions: `solver_baseline` implementing the fixed $\\omega_{\\text{const}}$ method, and `solver_safeguarded` implementing the backtracking line search on $\\omega_k$.\n- Each solver will track the number of iterations, the number of residual norm evaluations, and, for the safeguarded method, the total number of backtracking events. These counters are implemented strictly according to the problem's definitions.\n- A main driver loop will execute both solvers for each of the three test cases (A, B, and C), calculate the requested performance metrics (overhead and iteration reduction), and format the results for printing. The initial condition for Case C, $Y^0 = Y^{\\star}(T^0)$, is calculated prior to starting the solver. The handling of non-converged cases (indicated by an iteration count of $-1$) in the calculation of iteration reduction is managed by treating the iteration count as the maximum allowed, $k_{\\max}$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import brentq\n\ndef solve():\n    \"\"\"\n    Main function to design, run, and compare the two under-relaxation schemes.\n    \"\"\"\n\n    # --- Algorithmic and Model Settings ---\n    K_MAX = 200\n    EPSILON = 1e-6\n    OMEGA_CONST = 0.5\n    THETA = 0.5\n    OMEGA_MIN = 1e-8\n    T_A = 1.0\n\n    # --- Helper Functions for the Physics Model ---\n\n    def S_term(T, Y, params):\n        \"\"\"Computes the dimensionless reaction source term S(T, Y).\"\"\"\n        k0, E, k1 = params['k0'], params['E'], params['k1']\n        if T <= 0:\n            return -k1 * Y\n        return k0 * np.exp(-E / T) * (1 - Y) - k1 * Y\n\n    def residual(x, params):\n        \"\"\"Computes the residual vector r(x) = [r_T, r_Y]^T.\"\"\"\n        T, Y = x\n        alpha, beta, q = params['alpha'], params['beta'], params['q']\n        s_val = S_term(T, Y, params)\n        r_T = -alpha * (T - T_A) + q * s_val\n        r_Y = -beta * Y + s_val\n        return np.array([r_T, r_Y])\n\n    def coupling_map_G(x, params):\n        \"\"\"Computes the coupling map G(x) via sequential sub-solves.\"\"\"\n        T_k, _ = x\n        alpha, beta, k0, E, k1, q = params['alpha'], params['beta'], params['k0'], params['E'], params['k1'], params['q']\n\n        # 1. Compute species update Y_star by solving r_Y(T_k, Y) = 0\n        if T_k <= 0:\n            exp_term = 0.0\n        else:\n            exp_term = k0 * np.exp(-E / T_k)\n        Y_star = exp_term / (beta + k1 + exp_term) if (beta + k1 + exp_term) != 0 else 0.0\n\n        # 2. Compute temperature update T_star by solving r_T(T, Y_star) = 0\n        def r_T_for_solve(T):\n            s_val = S_term(T, Y_star, params)\n            return -alpha * (T - T_A) + q * s_val\n\n        T_max_search_list = [3.0, 10.0, 50.0, 100.0]\n        T_min_br = 1e-3\n        T_star = None\n\n        for T_max_br in T_max_search_list:\n            try:\n                T_star = brentq(r_T_for_solve, T_min_br, T_max_br, xtol=1e-12, rtol=1e-12)\n                break\n            except ValueError:\n                continue\n        \n        if T_star is None:\n            raise RuntimeError(f\"Failed to bracket root for Temperature sub-problem with x_k={x}\")\n            \n        return np.array([T_star, Y_star])\n\n    # --- Iterative Solver Implementations ---\n\n    def solver_baseline(x0, params):\n        \"\"\"Baseline solver with fixed under-relaxation factor.\"\"\"\n        x = np.copy(x0)\n        evals = 0\n        \n        for k in range(K_MAX):\n            norm_r = np.linalg.norm(residual(x, params))\n            evals += 1\n            if norm_r <= EPSILON:\n                return k, evals\n\n            x_cand = coupling_map_G(x, params)\n            x = x + OMEGA_CONST * (x_cand - x)\n\n        norm_r = np.linalg.norm(residual(x, params))\n        evals += 1\n        return (K_MAX, evals) if norm_r <= EPSILON else (-1, evals)\n\n    def solver_safeguarded(x0, params):\n        \"\"\"Solver with monotonicity-safeguarded backtracking line search.\"\"\"\n        x = np.copy(x0)\n        evals = 0\n        backtracks = 0\n        \n        for k in range(K_MAX):\n            norm_r_k = np.linalg.norm(residual(x, params))\n            evals += 1\n            if norm_r_k <= EPSILON:\n                return k, evals, backtracks\n            \n            s_k = coupling_map_G(x, params) - x\n            omega = 1.0\n            \n            while True:\n                x_trial = x + omega * s_k\n                norm_r_trial = np.linalg.norm(residual(x_trial, params))\n                evals += 1\n                \n                if norm_r_trial <= norm_r_k:\n                    x = x_trial\n                    break\n                \n                omega *= THETA\n                backtracks += 1\n                \n                if omega < OMEGA_MIN:\n                    return -1, evals, backtracks\n\n        norm_r = np.linalg.norm(residual(x, params))\n        evals += 1\n        return (K_MAX, evals, backtracks) if norm_r <= EPSILON else (-1, evals, backtracks)\n\n    # --- Main Execution ---\n\n    # Define the test cases from the problem statement.\n    case_a_params = {'alpha': 1.0, 'beta': 0.5, 'k0': 10.0, 'E': 5.0, 'k1': 1.0, 'q': 1.0}\n    case_b_params = {'alpha': 3.0, 'beta': 2.0, 'k0': 50.0, 'E': 20.0, 'k1': 1.0, 'q': 1.5}\n    case_c_params = {'alpha': 1.5, 'beta': 1.0, 'k0': 30.0, 'E': 10.0, 'k1': 0.5, 'q': 1.5}\n\n    # Calculate special initial condition for Case C\n    T0_c = 1.0\n    exp_term_c = case_c_params['k0'] * np.exp(-case_c_params['E'] / T0_c)\n    Y0_c = exp_term_c / (case_c_params['beta'] + case_c_params['k1'] + exp_term_c)\n    \n    test_cases = [\n        (case_a_params, np.array([0.8, 0.1])),\n        (case_b_params, np.array([0.5, 0.0])),\n        (case_c_params, np.array([T0_c, Y0_c])),\n    ]\n\n    results = []\n    for params, x0 in test_cases:\n        # Run safeguarded solver\n        iters_safe, evals_safe, backtracks_safe = solver_safeguarded(x0, params)\n        \n        # Run baseline solver\n        iters_base, evals_base = solver_baseline(x0, params)\n        \n        # Calculate derived metrics\n        overhead = evals_safe - evals_base\n        \n        # Handle -1 in iteration counts for a consistent delta\n        iters_safe_for_delta = iters_safe if iters_safe != -1 else K_MAX\n        iters_base_for_delta = iters_base if iters_base != -1 else K_MAX\n        iteration_delta = iters_base_for_delta - iters_safe_for_delta\n        \n        results.append([\n            iters_safe, evals_safe, backtracks_safe,\n            iters_base, evals_base,\n            overhead, iteration_delta\n        ])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3386114"}, {"introduction": "For the most challenging coupled systems, simple relaxation schemes can converge very slowly or not at all. This advanced practice introduces you to the realm of Interface Quasi-Newton (IQN) methods, which dramatically accelerate convergence by building an approximation of the system's Jacobian. You will implement and compare a sophisticated IQN solver, complete with a line-search safeguard and Broyden updates, against a simpler Aitken-accelerated scheme, providing a clear demonstration of the power of higher-order methods for complex multiphysics problems like fluid-structure interaction with contact [@problem_id:3386130].", "problem": "You are asked to design and compare two iterative interface coupling methods for a fixed-point surrogate of a contact-dominated Fluid-Structure Interaction (FSI) problem. The goal is to construct an Interface Quasi-Newton (IQN) method with secant updates for the inverse Jacobian and an inner under-relaxation safeguard with line search parameter $\\omega_k \\in (0,1]$, and to quantify its performance against a pure Aitken Delta-Squared (Δ²) acceleration applied to a fixed-point iteration. The comparison metric is the ratio of the number of outer iterations required by Aitken to those required by the IQN method to reach a specified tolerance.\n\nStart from the following fundamental base: the coupled interface equilibrium can be modeled by a residual map $R(u)$ derived from balance of interface tractions between a structure and a fluid, where $u \\in \\mathbb{R}^m$ denotes the interface displacement vector. Let $K_s \\in \\mathbb{R}^{m \\times m}$ be a symmetric positive-definite structural stiffness matrix, $K_f \\in \\mathbb{R}^{m \\times m}$ be a symmetric positive semidefinite fluid stiffness-like linearization, $K_c \\in \\mathbb{R}^{m \\times m}$ be a diagonal matrix of nonnegative contact penalties, $u_c \\in \\mathbb{R}^m$ be the non-penetration thresholds (gaps), and $b \\in \\mathbb{R}^m$ be the external load vector. Define the piecewise residual\n$$\nR(u) \\;=\\; K_s u \\;-\\; b \\;-\\; K_f u \\;-\\; K_c \\,\\max\\!\\big(0,\\; u - u_c\\big),\n$$\nwhere the maximum is understood componentwise. A one-sided penalty enforces contact if and only if the component $u_i$ exceeds the corresponding threshold $u_{c,i}$.\n\nDefine a fixed-point map $G : \\mathbb{R}^m \\to \\mathbb{R}^m$ by a one-step preconditioned Picard iteration with structural preconditioner,\n$$\nG(u) \\;=\\; u \\;-\\; M^{-1} R(u), \\quad \\text{with } M \\equiv K_s.\n$$\nThe fixed-point problem is to find $u^\\star$ such that $u^\\star = G(u^\\star)$, equivalently $R(u^\\star) = 0$. For the purposes of this assignment, all computations are nondimensional and unitless.\n\nImplement the following two iterative methods to solve $R(u) = 0$:\n\n- Pure Aitken Delta-Squared (Δ²) accelerated fixed-point iteration with a scalar relaxation parameter $\\omega_k \\in \\mathbb{R}$ applied to a vector update, without any safeguard. Let $d_k = G(u_k) - u_k$. Initialize $\\omega_0$ to a prescribed constant and iterate\n$$\nu_{k+1} \\;=\\; u_k \\;+\\; \\omega_k\\, d_k.\n$$\nFor $k \\ge 1$, update $\\omega_k$ by the vector Aitken formula\n$$\n\\omega_k \\;=\\; -\\, \\omega_{k-1} \\, \\frac{d_{k-1}^\\top \\big(d_k - d_{k-1}\\big)}{\\|d_k - d_{k-1}\\|_2^2},\n$$\nwith the convention that if the denominator is $0$ then $\\omega_k$ remains unchanged. No line search or clipping is allowed; this is the pure Aitken variant.\n\n- Interface Quasi-Newton (IQN) method on the residual $f(u) \\equiv R(u)$ with inverse secant updates and an inner under-relaxation safeguard. Let $H_k \\approx \\big(\\nabla f(u_k)\\big)^{-1}$ be an approximation to the inverse Jacobian. Start with $H_0 = K_s^{-1}$. Given $u_k$ and $r_k = f(u_k)$, compute the trial step $s_k = -\\, H_k\\, r_k$. Select a step length $\\omega_k \\in (0,1]$ by backtracking so that\n$$\n\\|f(u_k + \\omega_k s_k)\\|_2 \\;\\le\\; \\big(1 - c\\, \\omega_k\\big)\\, \\|r_k\\|_2,\n$$\nfor a fixed $c \\in (0,1)$, decreasing $\\omega_k \\leftarrow \\tau \\omega_k$ with $\\tau \\in (0,1)$ until the inequality holds. Accept $u_{k+1} = u_k + \\omega_k s_k$, $r_{k+1} = f(u_{k+1})$. Perform the inverse Broyden secant update that enforces $H_{k+1}\\,\\Delta r_k = \\Delta u_k$ with $\\Delta u_k = u_{k+1} - u_k$ and $\\Delta r_k = r_{k+1} - r_k$,\n$$\nH_{k+1} \\;=\\; H_k \\;+\\; \\frac{\\big(\\Delta u_k - H_k \\Delta r_k\\big)\\, \\Delta r_k^\\top}{\\Delta r_k^\\top \\Delta r_k},\n$$\nwhenever the denominator is nonzero; otherwise keep $H_{k+1} = H_k$.\n\nFor both methods, terminate when the two-norm of the residual is below a tolerance, that is when $\\|R(u_k)\\|_2 \\le \\varepsilon$, or when a maximum number of iterations is reached. Use the same absolute tolerance and iteration cap for both methods.\n\nTest Suite. Use the following parameter sets, which cover a normal regime, a contact-dominated scalar regime with near-unit contraction, and a two-dimensional coupled contact regime. In each case, use the specified initial guess, tolerance, and algorithmic constants.\n\n- Case $\\mathsf{A}$ (scalar, no contact):\n  - Dimension $m = 1$.\n  - $K_s = [\\,1.0\\,]$, $K_f = [\\,0.6\\,]$, $K_c = [\\,0.0\\,]$, $u_c = [\\,1.0\\,]$, $b = [\\,0.2\\,]$.\n  - Initial guess $u_0 = [\\,0.0\\,]$.\n\n- Case $\\mathsf{B}$ (scalar, contact-dominated):\n  - Dimension $m = 1$.\n  - $K_s = [\\,1.0\\,]$, $K_f = [\\,0.6\\,]$, $K_c = [\\,0.35\\,]$, $u_c = [\\,0.2\\,]$, $b = [\\,0.3\\,]$.\n  - Initial guess $u_0 = [\\,0.0\\,]$.\n\n- Case $\\mathsf{C}$ (two-dimensional, coupled contact):\n  - Dimension $m = 2$.\n  - $K_s = \\begin{bmatrix} 1.0 & 0.0 \\\\ 0.0 & 1.0 \\end{bmatrix}$,\n    $K_f = \\begin{bmatrix} 0.55 & 0.10 \\\\ 0.10 & 0.55 \\end{bmatrix}$,\n    $K_c = \\operatorname{diag}(0.35,\\,0.25)$,\n    $u_c = \\begin{bmatrix} 0.1 \\\\ 0.3 \\end{bmatrix}$,\n    $b = \\begin{bmatrix} 0.22 \\\\ 0.25 \\end{bmatrix}$.\n  - Initial guess $u_0 = \\begin{bmatrix} 0.0 \\\\ 0.0 \\end{bmatrix}$.\n\nFor all cases, use the same solver parameters:\n- Absolute tolerance $\\varepsilon = 10^{-10}$ for the residual norm $\\|R(u_k)\\|_2$.\n- Maximum number of outer iterations $N_{\\max} = 1000$.\n- Aitken initial relaxation $\\omega_0 = 0.5$.\n- IQN safeguard constants $c = 10^{-4}$ and $\\tau = 0.5$.\n\nYour program should, for each case, compute the number of outer iterations $N_{\\mathsf{Aitken}}$ and $N_{\\mathsf{IQN}}$ required to satisfy the stopping criterion (or reach the cap), and then produce the ratio $\\rho = N_{\\mathsf{Aitken}} / N_{\\mathsf{IQN}}$. If a method hits the cap without convergence, treat its iteration count as $N_{\\max}$ for the ratio computation.\n\nFinal Output Format. Your program should produce a single line of output containing the three ratios for Cases $\\mathsf{A}$, $\\mathsf{B}$, and $\\mathsf{C}$, in this order, as a comma-separated list enclosed in square brackets, each rounded to three decimal places, for example\n$$\n[\\,\\rho_{\\mathsf{A}},\\rho_{\\mathsf{B}},\\rho_{\\mathsf{C}}\\,].\n$$\nNo additional text should be printed.", "solution": "The user has provided a problem that requires the implementation and comparison of two numerical methods for solving a system of nonlinear equations modeling a fluid-structure interaction (FSI) problem with contact. The problem is well-defined, scientifically sound, and all necessary parameters and equations are provided.\n\n### Problem Formulation\n\nThe core of the problem is to find the root $u^\\star$ of a nonlinear residual function $R(u) : \\mathbb{R}^m \\to \\mathbb{R}^m$, such that $R(u^\\star) = 0$. The residual represents the force imbalance at the fluid-structure interface and is defined as:\n$$\nR(u) = K_s u - b - K_f u - K_c \\max(0, u - u_c)\n$$\nHere, $u$ is the interface displacement vector, $K_s$ and $K_f$ are structural and fluid stiffness matrices, $K_c$ is a contact penalty matrix, $u_c$ represents the initial gap before contact, and $b$ is an external load vector. The term $\\max(0, u - u_c)$ is applied component-wise and models a one-sided, non-penetration contact condition. This `max` function makes the system nonlinear and non-smooth.\n\nTwo iterative methods are to be implemented and compared for solving $R(u)=0$.\n\n### Method 1: Aitken Delta-Squared ($\\Delta^2$) Accelerated Fixed-Point Iteration\n\nThis method begins by reformulating the root-finding problem $R(u)=0$ into an equivalent fixed-point problem $u = G(u)$. A suitable fixed-point map $G(u)$ is constructed using a preconditioned Picard iteration:\n$$\nG(u) = u - M^{-1} R(u)\n$$\nThe problem specifies the structural stiffness matrix $K_s$ as the preconditioner, $M=K_s$. Since $K_s$ is given as symmetric positive-definite, its inverse $K_s^{-1}$ exists. The basic fixed-point iteration would be $u_{k+1} = G(u_k)$.\n\nTo accelerate convergence, a relaxation parameter $\\omega_k$ is introduced:\n$$\nu_{k+1} = u_k + \\omega_k d_k, \\quad \\text{where } d_k = G(u_k) - u_k = -K_s^{-1} R(u_k)\n$$\nThe Aitken $\\Delta^2$ method provides a dynamic update for $\\omega_k$ that attempts to approximate the optimal relaxation factor based on the recent behavior of the iteration. For $k \\ge 1$, $\\omega_k$ is computed using the previous relaxation factor $\\omega_{k-1}$ and the change in the iterative step vector, $\\Delta d_k = d_k - d_{k-1}$:\n$$\n\\omega_k = - \\omega_{k-1} \\frac{d_{k-1}^\\top (d_k - d_{k-1})}{\\|d_k - d_{k-1}\\|_2^2}\n$$\nThis formula is derived from an analysis of linearly-converging sequences and effectively provides an estimate for accelerating the convergence of the dominant error mode. The problem specifies using a constant initial value $\\omega_0$ and applying this update rule without any safeguards (e.g., clipping $\\omega_k$ to a \"safe\" range), which can lead to instability if the sequence does not behave as expected.\n\nThe algorithm proceeds as follows:\n1. Initialize $u_0$ and $\\omega_0$.\n2. For $k = 0, 1, 2, \\dots, N_{\\max}-1$:\n   a. Compute the residual $r_k = R(u_k)$.\n   b. Check for convergence: if $\\|r_k\\|_2 \\le \\varepsilon$, stop a-nd return $k$.\n   c. Compute the update direction $d_k = -K_s^{-1} r_k$.\n   d. If $k=0$, set $\\omega_k = \\omega_0$. If $k>0$, compute $\\omega_k$ using the Aitken formula. If the denominator is zero, maintain the previous value, $\\omega_k = \\omega_{k-1}$.\n   e. Update the solution: $u_{k+1} = u_k + \\omega_k d_k$.\n3. If convergence is not reached, the iteration count is $N_{\\max}$.\n\n### Method 2: Interface Quasi-Newton (IQN) with Line Search\n\nThis method treats the problem directly as a root-finding task for $f(u) \\equiv R(u) = 0$. It is a variant of Newton's method, which iteratively solves $u_{k+1} = u_k - (\\nabla f(u_k))^{-1} f(u_k)$. Since the Jacobian $\\nabla f(u)$ is non-trivial to compute (and is discontinuous due to the contact term), a quasi-Newton approach is used. We maintain an approximation $H_k$ to the inverse Jacobian $(\\nabla f(u_k))^{-1}$.\n\nThe algorithm proceeds as follows:\n1. Initialize $u_0$ and the inverse Jacobian approximation $H_0 = K_s^{-1}$.\n2. For $k = 0, 1, 2, \\dots, N_{\\max}-1$:\n   a. Compute the residual $r_k = f(u_k)$.\n   b. Check for convergence: if $\\|r_k\\|_2 \\le \\varepsilon$, stop and return $k$.\n   c. Compute the trial Newton step: $s_k = -H_k r_k$.\n   d. **Line Search (Safeguard):** Find a step length $\\omega_k \\in (0, 1]$ that ensures sufficient decrease in the residual. Starting with $\\omega_k=1$, decrease it by a factor $\\tau$ (e.g., $\\tau=0.5$) until the Armijo-Goldstein condition is satisfied:\n      $$\n      \\|f(u_k + \\omega_k s_k)\\|_2 \\le (1 - c \\omega_k) \\|r_k\\|_2\n      $$\n      for a small constant $c$ (e.g., $c=10^{-4}$). This prevents the iteration from taking overly aggressive steps that could lead to divergence.\n   e. Update the solution: $u_{k+1} = u_k + \\omega_k s_k$.\n   f. **Broyden Update:** Improve the inverse Jacobian approximation. Let $\\Delta u_k = u_{k+1} - u_k$ and $\\Delta r_k = f(u_{k+1}) - f(u_k)$. Update $H_k$ to $H_{k+1}$ using the inverse Broyden formula, which enforces the secant condition $H_{k+1} \\Delta r_k = \\Delta u_k$:\n      $$\n      H_{k+1} = H_k + \\frac{(\\Delta u_k - H_k \\Delta r_k) \\Delta r_k^\\top}{\\Delta r_k^\\top \\Delta r_k}\n      $$\n      If the denominator is zero, the approximation is not updated, $H_{k+1} = H_k$.\n3. If convergence is not reached, the iteration count is $N_{\\max}$.\n\nThe IQN method is generally more robust and powerful than the Aitken-accelerated Picard iteration, as it builds a more sophisticated model of the (inverse) Jacobian, and the line search provides crucial stabilization. The comparison will quantify this performance difference for the given test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the test cases and print the results.\n    \"\"\"\n\n    def compute_residual(u, Ks, Kf, Kc, uc, b):\n        \"\"\"\n        Computes the residual R(u) for the FSI problem.\n        R(u) = Ks*u - b - Kf*u - Kc*max(0, u - uc)\n        \"\"\"\n        contact_term = np.maximum(0, u - uc)\n        # Using @ for matrix-vector multiplication\n        contact_force = Kc @ contact_term\n        residual = Ks @ u - b - Kf @ u - contact_force\n        return residual\n\n    def aitken_solver(u0, Ks, Kf, Kc, uc, b, omega0, tol, max_iter):\n        \"\"\"\n        Implements the Aitken Delta-Squared accelerated fixed-point iteration.\n        \"\"\"\n        u = np.array(u0, dtype=float)\n        omega = float(omega0)\n        \n        # Precompute the inverse of Ks, as it is constant\n        try:\n            Ks_inv = np.linalg.inv(Ks)\n        except np.linalg.LinAlgError:\n            # Fallback for singular matrix, although Ks is positive-definite\n            return max_iter\n\n        # Iteration k=0\n        r = compute_residual(u, Ks, Kf, Kc, uc, b)\n        if np.linalg.norm(r) <= tol:\n            return 0\n        \n        d_prev = -Ks_inv @ r\n        u_next = u + omega * d_prev\n        u = u_next\n        \n        for k in range(1, max_iter):\n            r = compute_residual(u, Ks, Kf, Kc, uc, b)\n            if np.linalg.norm(r) <= tol:\n                return k\n\n            d_current = -Ks_inv @ r\n            delta_d = d_current - d_prev\n            denom = np.dot(delta_d, delta_d)\n            \n            # Update omega using Aitken's formula\n            if denom > 1e-16: # Safeguard against division by zero\n                omega = -omega * np.dot(d_prev, delta_d) / denom\n            \n            # Update solution\n            u_next = u + omega * d_current\n            \n            u = u_next\n            d_prev = d_current\n\n        return max_iter\n\n    def iqn_solver(u0, Ks, Kf, Kc, uc, b, c_armijo, tau, tol, max_iter):\n        \"\"\"\n        Implements the Interface Quasi-Newton (IQN) method with line search.\n        \"\"\"\n        u = np.array(u0, dtype=float)\n        \n        try:\n            H = np.linalg.inv(Ks)\n        except np.linalg.LinAlgError:\n            return max_iter\n\n        r = compute_residual(u, Ks, Kf, Kc, uc, b)\n        norm_r = np.linalg.norm(r)\n\n        for k in range(max_iter):\n            if norm_r <= tol:\n                return k\n\n            # Compute trial step\n            s = -H @ r\n\n            # Backtracking line search (Armijo condition)\n            omega = 1.0\n            u_next = u + omega * s\n            r_next = compute_residual(u_next, Ks, Kf, Kc, uc, b)\n            norm_r_next = np.linalg.norm(r_next)\n            \n            armijo_cond = (1.0 - c_armijo * omega) * norm_r\n            \n            # Limit the number of backtracking steps to avoid infinite loops\n            backtrack_count = 0 \n            while norm_r_next > armijo_cond and backtrack_count < 20: \n                omega *= tau\n                u_next = u + omega * s\n                r_next = compute_residual(u_next, Ks, Kf, Kc, uc, b)\n                norm_r_next = np.linalg.norm(r_next)\n                armijo_cond = (1.0 - c_armijo * omega) * norm_r\n                backtrack_count += 1\n                if omega < 1e-10: # Step size too small\n                     break\n\n            # Updates for Broyden\n            delta_u = u_next - u\n            delta_r = r_next - r\n\n            # Update H using Broyden's method\n            denom = np.dot(delta_r, delta_r)\n            if denom > 1e-16:\n                term1 = delta_u - (H @ delta_r)\n                H = H + np.outer(term1, delta_r) / denom\n\n            # Update solution and residual\n            u = u_next\n            r = r_next\n            norm_r = norm_r_next\n\n        return max_iter\n\n    # Define test cases\n    test_cases = [\n        # Case A: scalar, no contact\n        {\n            \"m\": 1,\n            \"Ks\": np.array([[1.0]]), \"Kf\": np.array([[0.6]]), \"Kc\": np.array([[0.0]]),\n            \"uc\": np.array([1.0]), \"b\": np.array([0.2]), \"u0\": np.array([0.0])\n        },\n        # Case B: scalar, contact-dominated\n        {\n            \"m\": 1,\n            \"Ks\": np.array([[1.0]]), \"Kf\": np.array([[0.6]]), \"Kc\": np.array([[0.35]]),\n            \"uc\": np.array([0.2]), \"b\": np.array([0.3]), \"u0\": np.array([0.0])\n        },\n        # Case C: 2D, coupled contact\n        {\n            \"m\": 2,\n            \"Ks\": np.array([[1.0, 0.0], [0.0, 1.0]]),\n            \"Kf\": np.array([[0.55, 0.10], [0.10, 0.55]]),\n            \"Kc\": np.diag([0.35, 0.25]),\n            \"uc\": np.array([0.1, 0.3]), \"b\": np.array([0.22, 0.25]),\n            \"u0\": np.array([0.0, 0.0])\n        }\n    ]\n\n    # Solver parameters\n    tol = 1e-10\n    max_iter = 1000\n    omega0 = 0.5\n    c_armijo = 1e-4\n    tau = 0.5\n\n    results = []\n    for case in test_cases:\n        N_aitken = aitken_solver(\n            case[\"u0\"], case[\"Ks\"], case[\"Kf\"], case[\"Kc\"], case[\"uc\"], case[\"b\"],\n            omega0, tol, max_iter\n        )\n        N_iqn = iqn_solver(\n            case[\"u0\"], case[\"Ks\"], case[\"Kf\"], case[\"Kc\"], case[\"uc\"], case[\"b\"],\n            c_armijo, tau, tol, max_iter\n        )\n        \n        # Avoid division by zero if IQN fails to converge in 0 iterations (theoretically possible if u0 is the solution)\n        if N_iqn == 0:\n            ratio = float('inf') if N_aitken > 0 else 1.0\n        else:\n            ratio = N_aitken / N_iqn\n        \n        results.append(f\"{ratio:.3f}\")\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3386130"}]}