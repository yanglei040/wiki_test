{"hands_on_practices": [{"introduction": "To begin our hands-on exploration, we first tackle the fundamental mechanics of weighted least-squares (WLS) gradient reconstruction. WLS allows us to assign varying importance to neighbor data, a common practice where closer points are considered more influential than distant ones. This exercise will solidify your understanding by guiding you through the derivation of the normal equations and demonstrating how different weighting strategies quantitatively affect both the reconstructed gradient and its statistical uncertainty [@problem_id:3339283].", "problem": "A scalar field $u(\\boldsymbol{x})$ is sampled on an unstructured two-dimensional stencil around a central control volume centroid located at $\\boldsymbol{x}_{0}=(0,0)$. The neighbor offsets are $\\boldsymbol{r}_{1}=(1,0)$, $\\boldsymbol{r}_{2}=(0,1)$, $\\boldsymbol{r}_{3}=(-1,0)$, $\\boldsymbol{r}_{4}=(0,-1)$, and $\\boldsymbol{r}_{5}=(1,1)$. The measured increments relative to the center are $\\Delta u_{1}=1.25$, $\\Delta u_{2}=-0.72$, $\\Delta u_{3}=-1.17$, $\\Delta u_{4}=0.69$, and $\\Delta u_{5}=0.54$, all dimensionless. Use the linearized reconstruction model $\\Delta u_{i}\\approx \\boldsymbol{r}_{i}\\cdot \\boldsymbol{g}$, where $\\boldsymbol{g}=(g_{x},g_{y})^{\\mathsf{T}}$ is the gradient at the centroid, and form the weighted least-squares objective $J(\\boldsymbol{g})=\\sum_{i=1}^{5}w_{i}\\left(\\boldsymbol{r}_{i}\\cdot \\boldsymbol{g}-\\Delta u_{i}\\right)^{2}$.\n\nStarting from this model and the definition of weighted least squares, derive the normal equations from first principles and solve them to obtain the reconstructed gradient $\\boldsymbol{g}$ under two choices of weights:\n- Uniform weights $W_{1}=\\mathrm{diag}(1,1,1,1,1)$.\n- Distance-based weights $W_{2}=\\mathrm{diag}(1,1,1,1,1/2)$.\n\nAssume the measurement errors are independent with a common variance, so that $\\Delta u_{i}= \\boldsymbol{r}_{i}\\cdot \\boldsymbol{g}_{\\mathrm{true}}+\\varepsilon_{i}$ with $\\mathbb{E}[\\varepsilon_{i}]=0$ and $\\mathrm{Cov}(\\varepsilon)=\\sigma^{2}I$. Under this assumption, use the general expression for the covariance of a weighted least-squares estimator derived from the objective above to compute the ratio of determinants of the covariance matrices of $\\boldsymbol{g}$ obtained with $W_{2}$ versus $W_{1}$.\n\nExpress your final answer as a single row matrix containing five entries in the following order: $g_{x}$ with $W_{1}$, $g_{y}$ with $W_{1}$, $g_{x}$ with $W_{2}$, $g_{y}$ with $W_{2}$, and the ratio of determinants $\\det\\left(\\mathrm{Cov}_{W_{2}}(\\boldsymbol{g})\\right)/\\det\\left(\\mathrm{Cov}_{W_{1}}(\\boldsymbol{g})\\right)$. Round your numerical entries to four significant figures. No physical units are required.", "solution": "The user wants to solve a weighted least-squares problem to estimate a gradient vector.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- A scalar field is denoted by $u(\\boldsymbol{x})$.\n- The central point is at $\\boldsymbol{x}_{0}=(0,0)$.\n- Neighbor offsets are $\\boldsymbol{r}_{1}=(1,0)$, $\\boldsymbol{r}_{2}=(0,1)$, $\\boldsymbol{r}_{3}=(-1,0)$, $\\boldsymbol{r}_{4}=(0,-1)$, and $\\boldsymbol{r}_{5}=(1,1)$.\n- Measured increments are $\\Delta u_{1}=1.25$, $\\Delta u_{2}=-0.72$, $\\Delta u_{3}=-1.17$, $\\Delta u_{4}=0.69$, and $\\Delta u_{5}=0.54$.\n- The linearized model is $\\Delta u_{i}\\approx \\boldsymbol{r}_{i}\\cdot \\boldsymbol{g}$, where $\\boldsymbol{g}=(g_{x},g_{y})^{\\mathsf{T}}$ is the gradient.\n- The weighted least-squares objective function is $J(\\boldsymbol{g})=\\sum_{i=1}^{5}w_{i}\\left(\\boldsymbol{r}_{i}\\cdot \\boldsymbol{g}-\\Delta u_{i}\\right)^{2}$.\n- Two weight matrices are specified:\n    1. Uniform weights: $W_{1}=\\mathrm{diag}(1,1,1,1,1)$.\n    2. Distance-based weights: $W_{2}=\\mathrm{diag}(1,1,1,1,1/2)$.\n- The error model is $\\Delta u_{i}= \\boldsymbol{r}_{i}\\cdot \\boldsymbol{g}_{\\mathrm{true}}+\\varepsilon_{i}$ with $\\mathbb{E}[\\varepsilon_{i}]=0$ and $\\mathrm{Cov}(\\varepsilon)=\\sigma^{2}I$.\n- The task is to derive the normal equations, solve for $\\boldsymbol{g}$ for both weight sets, and compute the ratio of determinants of the covariance matrices of the estimators, $\\det\\left(\\mathrm{Cov}_{W_{2}}(\\boldsymbol{g})\\right)/\\det\\left(\\mathrm{Cov}_{W_{1}}(\\boldsymbol{g})\\right)$.\n- Numerical results should be rounded to four significant figures.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem describes the weighted least-squares method for gradient reconstruction from scattered data. This is a standard and widely used technique in computational fluid dynamics, finite element analysis, and other areas of computational science. The underlying principles are based on Taylor series expansions and statistical estimation, which are well-established.\n- **Well-Posed:** The problem is structured as a linear least-squares problem. The design matrix $\\boldsymbol{A}$ (whose rows are $\\boldsymbol{r}_i^{\\mathsf{T}}$) has full column rank, and the weight matrices $\\boldsymbol{W}_1$ and $\\boldsymbol{W}_2$ are positive definite. This ensures that the matrix $\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}\\boldsymbol{A}$ is invertible, guaranteeing a unique solution for the gradient $\\boldsymbol{g}$.\n- **Objective:** The problem is stated using precise mathematical language and definitions. All quantities are unambiguously defined. No subjective or opinion-based statements are present.\n- The problem is self-contained, with all necessary data and models provided. It is not contradictory, unrealistic, ill-posed, or trivial.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A complete solution will be provided.\n\n### Derivation and Solution\n\nThe problem is to find the gradient vector $\\boldsymbol{g} = (g_x, g_y)^{\\mathsf{T}}$ that minimizes the weighted least-squares objective function:\n$$ J(\\boldsymbol{g}) = \\sum_{i=1}^{5} w_i (\\boldsymbol{r}_i \\cdot \\boldsymbol{g} - \\Delta u_i)^2 $$\nThis can be expressed in matrix form. Let the design matrix $\\boldsymbol{A}$ be a $5 \\times 2$ matrix whose rows are the vectors $\\boldsymbol{r}_i^{\\mathsf{T}}$, and let $\\boldsymbol{b}$ be a $5 \\times 1$ vector of the measurements $\\Delta u_i$.\n$$\n\\boldsymbol{A} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ -1 & 0 \\\\ 0 & -1 \\\\ 1 & 1 \\end{pmatrix}, \\quad \\boldsymbol{b} = \\begin{pmatrix} 1.25 \\\\ -0.72 \\\\ -1.17 \\\\ 0.69 \\\\ 0.54 \\end{pmatrix}\n$$\nThe system of equations is $\\boldsymbol{A}\\boldsymbol{g} \\approx \\boldsymbol{b}$. The objective function in matrix form is:\n$$ J(\\boldsymbol{g}) = (\\boldsymbol{A}\\boldsymbol{g} - \\boldsymbol{b})^{\\mathsf{T}} \\boldsymbol{W} (\\boldsymbol{A}\\boldsymbol{g} - \\boldsymbol{b}) $$\nwhere $\\boldsymbol{W}$ is the diagonal matrix of weights $w_i$. To minimize $J(\\boldsymbol{g})$, we set its gradient with respect to $\\boldsymbol{g}$ to zero. First, expand the objective function:\n$$ J(\\boldsymbol{g}) = (\\boldsymbol{g}^{\\mathsf{T}}\\boldsymbol{A}^{\\mathsf{T}} - \\boldsymbol{b}^{\\mathsf{T}}) \\boldsymbol{W} (\\boldsymbol{A}\\boldsymbol{g} - \\boldsymbol{b}) = \\boldsymbol{g}^{\\mathsf{T}}\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}\\boldsymbol{A}\\boldsymbol{g} - \\boldsymbol{g}^{\\mathsf{T}}\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}\\boldsymbol{b} - \\boldsymbol{b}^{\\mathsf{T}}\\boldsymbol{W}\\boldsymbol{A}\\boldsymbol{g} + \\boldsymbol{b}^{\\mathsf{T}}\\boldsymbol{W}\\boldsymbol{b} $$\nSince $\\boldsymbol{b}^{\\mathsf{T}}\\boldsymbol{W}\\boldsymbol{A}\\boldsymbol{g}$ is a scalar, it equals its transpose $(\\boldsymbol{b}^{\\mathsf{T}}\\boldsymbol{W}\\boldsymbol{A}\\boldsymbol{g})^{\\mathsf{T}} = \\boldsymbol{g}^{\\mathsf{T}}\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}^{\\mathsf{T}}\\boldsymbol{b}$. Since $\\boldsymbol{W}$ is diagonal, $\\boldsymbol{W}^{\\mathsf{T}} = \\boldsymbol{W}$. Thus, the two middle terms are identical.\n$$ J(\\boldsymbol{g}) = \\boldsymbol{g}^{\\mathsf{T}}(\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}\\boldsymbol{A})\\boldsymbol{g} - 2\\boldsymbol{g}^{\\mathsf{T}}(\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}\\boldsymbol{b}) + \\boldsymbol{b}^{\\mathsf{T}}\\boldsymbol{W}\\boldsymbol{b} $$\nDifferentiating with respect to $\\boldsymbol{g}$ and setting the result to zero yields:\n$$ \\frac{\\partial J}{\\partial \\boldsymbol{g}} = 2(\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}\\boldsymbol{A})\\boldsymbol{g} - 2(\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}\\boldsymbol{b}) = \\boldsymbol{0} $$\nThis gives the **normal equations**:\n$$ (\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}\\boldsymbol{A})\\boldsymbol{g} = \\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}\\boldsymbol{b} $$\nThe solution for the gradient estimate $\\boldsymbol{g}$ is:\n$$ \\boldsymbol{g} = (\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}\\boldsymbol{A})^{-1} (\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}\\boldsymbol{b}) $$\n\n**Case 1: Uniform Weights** $\\boldsymbol{W}_1 = \\mathrm{diag}(1,1,1,1,1) = \\boldsymbol{I}$\nThe normal equations simplify to $(\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{A})\\boldsymbol{g}_1 = \\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{b}$.\nFirst, we compute $\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{A}$:\n$$ \\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{A} = \\begin{pmatrix} 1 & 0 & -1 & 0 & 1 \\\\ 0 & 1 & 0 & -1 & 1 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ -1 & 0 \\\\ 0 & -1 \\\\ 1 & 1 \\end{pmatrix} = \\begin{pmatrix} 1^2+(-1)^2+1^2 & 1(1) \\\\ 1(1) & 1^2+(-1)^2+1^2 \\end{pmatrix} = \\begin{pmatrix} 3 & 1 \\\\ 1 & 3 \\end{pmatrix} $$\nNext, we compute $\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{b}$:\n$$ \\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{b} = \\begin{pmatrix} 1 & 0 & -1 & 0 & 1 \\\\ 0 & 1 & 0 & -1 & 1 \\end{pmatrix} \\begin{pmatrix} 1.25 \\\\ -0.72 \\\\ -1.17 \\\\ 0.69 \\\\ 0.54 \\end{pmatrix} = \\begin{pmatrix} 1.25 - (-1.17) + 0.54 \\\\ -0.72 - 0.69 + 0.54 \\end{pmatrix} = \\begin{pmatrix} 2.96 \\\\ -0.87 \\end{pmatrix} $$\nThe inverse of $\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{A}$ is:\n$$ (\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{A})^{-1} = \\frac{1}{3(3)-1(1)} \\begin{pmatrix} 3 & -1 \\\\ -1 & 3 \\end{pmatrix} = \\frac{1}{8} \\begin{pmatrix} 3 & -1 \\\\ -1 & 3 \\end{pmatrix} $$\nNow we solve for $\\boldsymbol{g}_1$:\n$$ \\boldsymbol{g}_1 = \\frac{1}{8} \\begin{pmatrix} 3 & -1 \\\\ -1 & 3 \\end{pmatrix} \\begin{pmatrix} 2.96 \\\\ -0.87 \\end{pmatrix} = \\frac{1}{8} \\begin{pmatrix} 3(2.96) - 1(-0.87) \\\\ -1(2.96) + 3(-0.87) \\end{pmatrix} = \\frac{1}{8} \\begin{pmatrix} 8.88 + 0.87 \\\\ -2.96 - 2.61 \\end{pmatrix} = \\frac{1}{8} \\begin{pmatrix} 9.75 \\\\ -5.57 \\end{pmatrix} = \\begin{pmatrix} 1.21875 \\\\ -0.69625 \\end{pmatrix} $$\nRounding to four significant figures, $g_{x,1} = 1.219$ and $g_{y,1} = -0.6963$.\n\n**Case 2: Distance-based Weights** $\\boldsymbol{W}_2 = \\mathrm{diag}(1,1,1,1,1/2)$\nWe compute $\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}_2\\boldsymbol{A}$:\n$$ \\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}_2\\boldsymbol{A} = \\sum_{i=1}^5 w_i \\boldsymbol{r}_i \\boldsymbol{r}_i^{\\mathsf{T}} = 1\\begin{pmatrix}1\\\\0\\end{pmatrix}\\begin{pmatrix}1&0\\end{pmatrix} + 1\\begin{pmatrix}0\\\\1\\end{pmatrix}\\begin{pmatrix}0&1\\end{pmatrix} + 1\\begin{pmatrix}-1\\\\0\\end{pmatrix}\\begin{pmatrix}-1&0\\end{pmatrix} + 1\\begin{pmatrix}0\\\\-1\\end{pmatrix}\\begin{pmatrix}0&-1\\end{pmatrix} + \\frac{1}{2}\\begin{pmatrix}1\\\\1\\end{pmatrix}\\begin{pmatrix}1&1\\end{pmatrix} $$\n$$ = \\begin{pmatrix}1&0\\\\0&0\\end{pmatrix} + \\begin{pmatrix}0&0\\\\0&1\\end{pmatrix} + \\begin{pmatrix}1&0\\\\0&0\\end{pmatrix} + \\begin{pmatrix}0&0\\\\0&1\\end{pmatrix} + \\begin{pmatrix}0.5&0.5\\\\0.5&0.5\\end{pmatrix} = \\begin{pmatrix} 2.5 & 0.5 \\\\ 0.5 & 2.5 \\end{pmatrix} $$\nNext, we compute $\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}_2\\boldsymbol{b}$:\n$$ \\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}_2\\boldsymbol{b} = \\sum_{i=1}^5 w_i \\boldsymbol{r}_i \\Delta u_i = 1\\begin{pmatrix}1\\\\0\\end{pmatrix}(1.25) + 1\\begin{pmatrix}0\\\\1\\end{pmatrix}(-0.72) + 1\\begin{pmatrix}-1\\\\0\\end{pmatrix}(-1.17) + 1\\begin{pmatrix}0\\\\-1\\end{pmatrix}(0.69) + \\frac{1}{2}\\begin{pmatrix}1\\\\1\\end{pmatrix}(0.54) $$\n$$ = \\begin{pmatrix}1.25\\\\-0.72\\end{pmatrix} + \\begin{pmatrix}1.17\\\\0\\end{pmatrix} + \\begin{pmatrix}0\\\\-0.69\\end{pmatrix} + \\begin{pmatrix}0.27\\\\0.27\\end{pmatrix} = \\begin{pmatrix} 1.25+1.17+0.27 \\\\-0.72-0.69+0.27 \\end{pmatrix} = \\begin{pmatrix} 2.69 \\\\ -1.14 \\end{pmatrix} $$\nThe inverse of $\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}_2\\boldsymbol{A}$ is:\n$$ (\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}_2\\boldsymbol{A})^{-1} = \\frac{1}{2.5^2-0.5^2} \\begin{pmatrix} 2.5 & -0.5 \\\\ -0.5 & 2.5 \\end{pmatrix} = \\frac{1}{6} \\begin{pmatrix} 2.5 & -0.5 \\\\ -0.5 & 2.5 \\end{pmatrix} $$\nNow we solve for $\\boldsymbol{g}_2$:\n$$ \\boldsymbol{g}_2 = \\frac{1}{6} \\begin{pmatrix} 2.5 & -0.5 \\\\ -0.5 & 2.5 \\end{pmatrix} \\begin{pmatrix} 2.69 \\\\ -1.14 \\end{pmatrix} = \\frac{1}{6} \\begin{pmatrix} 2.5(2.69) - 0.5(-1.14) \\\\ -0.5(2.69) + 2.5(-1.14) \\end{pmatrix} = \\frac{1}{6} \\begin{pmatrix} 6.725 + 0.57 \\\\ -1.345 - 2.85 \\end{pmatrix} = \\frac{1}{6} \\begin{pmatrix} 7.295 \\\\ -4.195 \\end{pmatrix} = \\begin{pmatrix} 1.2158\\bar{3} \\\\ -0.6991\\bar{6} \\end{pmatrix} $$\nRounding to four significant figures, $g_{x,2} = 1.216$ and $g_{y,2} = -0.6992$.\n\n**Covariance Matrix Ratio**\nThe estimated gradient is $\\hat{\\boldsymbol{g}} = (\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}\\boldsymbol{A})^{-1} \\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}\\boldsymbol{b}$. The measurement vector $\\boldsymbol{b}$ is related to the true gradient $\\boldsymbol{g}_{\\mathrm{true}}$ by $\\boldsymbol{b} = \\boldsymbol{A}\\boldsymbol{g}_{\\mathrm{true}} + \\boldsymbol{\\varepsilon}$, where $\\boldsymbol{\\varepsilon}$ is the error vector with $\\mathbb{E}[\\boldsymbol{\\varepsilon}] = \\boldsymbol{0}$ and $\\mathrm{Cov}(\\boldsymbol{\\varepsilon}) = \\mathbb{E}[\\boldsymbol{\\varepsilon}\\boldsymbol{\\varepsilon}^{\\mathsf{T}}] = \\sigma^2 \\boldsymbol{I}$.\nThe estimation error is $\\hat{\\boldsymbol{g}} - \\boldsymbol{g}_{\\mathrm{true}} = (\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}\\boldsymbol{A})^{-1} \\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}\\boldsymbol{\\varepsilon}$.\nThe covariance matrix of the estimator $\\hat{\\boldsymbol{g}}$ is:\n$$ \\mathrm{Cov}(\\hat{\\boldsymbol{g}}) = \\mathbb{E}[(\\hat{\\boldsymbol{g}} - \\boldsymbol{g}_{\\mathrm{true}})(\\hat{\\boldsymbol{g}} - \\boldsymbol{g}_{\\mathrm{true}})^{\\mathsf{T}}] = \\mathbb{E}[((\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}\\boldsymbol{A})^{-1} \\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}\\boldsymbol{\\varepsilon}) ((\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}\\boldsymbol{A})^{-1} \\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}\\boldsymbol{\\varepsilon})^{\\mathsf{T}}] $$\n$$ \\mathrm{Cov}(\\hat{\\boldsymbol{g}}) = (\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}\\boldsymbol{A})^{-1} \\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W} \\mathbb{E}[\\boldsymbol{\\varepsilon}\\boldsymbol{\\varepsilon}^{\\mathsf{T}}] \\boldsymbol{W}^{\\mathsf{T}}\\boldsymbol{A} ((\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}\\boldsymbol{A})^{-1})^{\\mathsf{T}} $$\nSince $\\mathbb{E}[\\boldsymbol{\\varepsilon}\\boldsymbol{\\varepsilon}^{\\mathsf{T}}] = \\sigma^2\\boldsymbol{I}$, $\\boldsymbol{W}$ is diagonal, and $\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}\\boldsymbol{A}$ is symmetric:\n$$ \\mathrm{Cov}_W(\\boldsymbol{g}) = \\sigma^2 (\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}\\boldsymbol{A})^{-1} (\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}^2\\boldsymbol{A}) (\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}\\boldsymbol{A})^{-1} $$\nFor $\\boldsymbol{W}_1 = \\boldsymbol{I}$, we have $\\boldsymbol{W}_1^2 = \\boldsymbol{I}$. The formula simplifies to the OLS case:\n$$ \\mathrm{Cov}_{W_1}(\\boldsymbol{g}) = \\sigma^2 (\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{A})^{-1} (\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{A}) (\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{A})^{-1} = \\sigma^2 (\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{A})^{-1} $$\nThe determinant is:\n$$ \\det(\\mathrm{Cov}_{W_1}(\\boldsymbol{g})) = \\det(\\sigma^2 (\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{A})^{-1}) = (\\sigma^2)^2 \\det((\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{A})^{-1}) = \\frac{\\sigma^4}{\\det(\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{A})} = \\frac{\\sigma^4}{8} $$\nFor $\\boldsymbol{W}_2 = \\mathrm{diag}(1,1,1,1,1/2)$, we have $\\boldsymbol{W}_2^2 = \\mathrm{diag}(1,1,1,1,1/4)$.\n$$ \\det(\\mathrm{Cov}_{W_2}(\\boldsymbol{g})) = \\det\\left( \\sigma^2 (\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}_2\\boldsymbol{A})^{-1} (\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}_2^2\\boldsymbol{A}) (\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}_2\\boldsymbol{A})^{-1} \\right) $$\n$$ = (\\sigma^2)^2 \\frac{\\det(\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}_2^2\\boldsymbol{A})}{(\\det(\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}_2\\boldsymbol{A}))^2} $$\nWe already found $\\det(\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}_2\\boldsymbol{A}) = \\det \\begin{pmatrix} 2.5 & 0.5 \\\\ 0.5 & 2.5 \\end{pmatrix} = 6$.\nWe need to compute $\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}_2^2\\boldsymbol{A}$:\n$$ \\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}_2^2\\boldsymbol{A} = \\sum_{i=1}^5 w_i^2 \\boldsymbol{r}_i \\boldsymbol{r}_i^{\\mathsf{T}} = 1^2\\begin{pmatrix}1&0\\\\0&1\\end{pmatrix} + 1^2\\begin{pmatrix}1&0\\\\0&1\\end{pmatrix} + (\\frac{1}{2})^2\\begin{pmatrix}1&1\\\\1&1\\end{pmatrix} = \\begin{pmatrix}2&0\\\\0&2\\end{pmatrix} + \\frac{1}{4}\\begin{pmatrix}1&1\\\\1&1\\end{pmatrix} = \\begin{pmatrix} 2.25 & 0.25 \\\\ 0.25 & 2.25 \\end{pmatrix} $$\nThe determinant is $\\det(\\boldsymbol{A}^{\\mathsf{T}}\\boldsymbol{W}_2^2\\boldsymbol{A}) = 2.25^2 - 0.25^2 = 5.0625 - 0.0625 = 5$.\nSo, $\\det(\\mathrm{Cov}_{W_2}(\\boldsymbol{g})) = \\sigma^4 \\frac{5}{6^2} = \\frac{5\\sigma^4}{36}$.\nThe ratio is:\n$$ \\frac{\\det(\\mathrm{Cov}_{W_2}(\\boldsymbol{g}))}{\\det(\\mathrm{Cov}_{W_1}(\\boldsymbol{g}))} = \\frac{5\\sigma^4/36}{\\sigma^4/8} = \\frac{5}{36} \\times 8 = \\frac{40}{36} = \\frac{10}{9} = 1.111\\bar{1} $$\nRounding to four significant figures, the ratio is $1.111$.\n\nThe final results, rounded to four significant figures, are:\n- $g_{x}$ with $W_1$: $1.219$\n- $g_{y}$ with $W_1$: $-0.6963$\n- $g_{x}$ with $W_2$: $1.216$\n- $g_{y}$ with $W_2$: $-0.6992$\n- Ratio of determinants: $1.111$", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1.219 & -0.6963 & 1.216 & -0.6992 & 1.111\n\\end{pmatrix}\n}\n$$", "id": "3339283"}, {"introduction": "Moving from analytical derivation to computational practice reveals the critical importance of numerical stability. The geometric arrangement of neighbor points directly impacts the conditioning of the least-squares system matrix, and a poor-quality stencil with nearly coplanar neighbors can render a naive implementation useless. This coding-based practice will guide you through diagnosing and solving this common problem, contrasting the fragile normal equations approach with robust, rank-revealing factorization methods and demonstrating stencil expansion as a practical remedy [@problem_id:3339293].", "problem": "You are asked to implement and analyze least-squares gradient reconstruction for a single control volume in three-dimensional space under increasingly pathological geometric stencils. The goal is to show how near-coplanar neighbor configurations drive the linear least-squares system toward singularity and to design rank-revealing remedies based on pivoting and stencil expansion. The task must be solved by writing a complete, runnable program that constructs the specified stencils, computes gradients with multiple methods, and reports quantitative diagnostics.\n\nConsider a scalar field $\\,\\phi(\\mathbf{x})\\,$ that is linear in space, $\\;\\phi(\\mathbf{x}) = a\\,x + b\\,y + c\\,z\\,$, where $\\,\\mathbf{x} = (x,y,z)\\,$ and the constants $\\,a,b,c\\,$ are given. For a central point $\\,\\mathbf{x}_0\\,$ with neighboring points $\\,\\mathbf{x}_i\\,$, define the least-squares gradient reconstruction of $\\,\\nabla \\phi\\,$ at $\\,\\mathbf{x}_0\\,$ by minimizing\n$$\nJ(\\mathbf{g}) \\,=\\, \\sum_{i=1}^{N} w_i \\,\\Big(\\phi(\\mathbf{x}_i) - \\phi(\\mathbf{x}_0) - \\mathbf{g}\\cdot(\\mathbf{x}_i - \\mathbf{x}_0)\\Big)^2,\n$$\nwith weights $\\,w_i = 1\\,$ for all neighbors. This leads to the linear system\n$$\n\\mathbf{A}\\,\\mathbf{g} \\,\\approx\\, \\mathbf{b},\n$$\nwhere the $\\,i$-th row of $\\,\\mathbf{A}\\in\\mathbb{R}^{N\\times 3}\\,$ is $\\,(\\mathbf{x}_i - \\mathbf{x}_0)^\\top\\,$, and $\\,\\mathbf{b}\\in\\mathbb{R}^{N}\\,$ has entries $\\,\\phi(\\mathbf{x}_i) - \\phi(\\mathbf{x}_0)\\,$. In the absence of noise and with full column rank of $\\,\\mathbf{A}\\,$, the exact solution $\\,\\mathbf{g}^\\star = \\nabla\\phi(\\mathbf{x}_0)\\,$ is recovered. However, if the neighbor vectors are nearly coplanar, $\\,\\mathbf{A}\\,$ becomes ill-conditioned or rank-deficient.\n\nYour program must implement and compare the following three methods:\n- A naive normal-equations solver using the matrix $\\,\\mathbf{N} = \\mathbf{A}^\\top \\mathbf{A}\\,$ and right-hand side $\\,\\mathbf{c} = \\mathbf{A}^\\top \\mathbf{b}\\,$ solved for $\\,\\mathbf{g}\\,$ by direct linear solve. If $\\,\\mathbf{N}\\,$ is singular or numerically too ill-conditioned, this method may fail or produce unstable results.\n- A rank-revealing solver based on a column-pivoted $\\,\\mathbf{Q}\\mathbf{R}\\,$ factorization of $\\,\\mathbf{A}\\,$, with a numerical rank determined from the diagonal of $\\,\\mathbf{R}\\,$ using a threshold $\\,\\tau = \\mathrm{rcond}\\cdot |R_{11}|\\,$ with $\\,\\mathrm{rcond} = 10^{-12}\\,$. The solution should set components associated with negligible columns to zero to obtain a minimal-norm least-squares solution consistent with the estimated numerical rank.\n- A stencil expansion remedy: augment the initial neighbor set with additional candidate points (provided below) until the estimated numerical rank reaches $\\,3\\,$, and then recompute the pivoted $\\,\\mathbf{Q}\\mathbf{R}\\,$ solution on the expanded stencil.\n\nTo quantify near-singularity, use the singular value decomposition to estimate the $\\,2$-norm condition number of $\\,\\mathbf{A}\\,$ as $\\,\\kappa_2(\\mathbf{A}) = \\sigma_{\\max}/\\sigma_{\\min}\\,$. If $\\,\\sigma_{\\min} = 0\\,$, report $\\,\\kappa_2(\\mathbf{A})\\,$ as the sentinel value $\\,10^{18}\\,$. For the naive normal-equations method, if the linear solve fails due to singularity or if the condition number of $\\,\\mathbf{N}\\,$ exceeds $\\,10^{15}\\,$, report the error for this method as the sentinel value $\\,10^{9}\\,$.\n\nImplement the following test suite with three cases. In all cases, take the central point $\\,\\mathbf{x}_0 = (0,0,0)\\,$ and unit weights $\\,w_i = 1\\,$. Round all floating-point outputs to six decimal places.\n\n- Case 1 (well-conditioned): use coefficients $\\,a=2.3\\,$, $\\,b=-1.7\\,$, $\\,c=0.9\\,$ and neighbors\n  $$\n  (1,0,0),\\;(-1,0,0),\\;(0,1,0),\\;(0,-1,0),\\;(0,0,1),\\;(0,0,-1).\n  $$\n- Case 2 (nearly coplanar): use coefficients $\\,a=2.3\\,$, $\\,b=-1.7\\,$, $\\,c=0.9\\,$. Let $\\,\\epsilon = 10^{-8}\\,$ and neighbors\n  $$\n  (1,0,\\epsilon),\\;(-1,0,\\epsilon),\\;(0,1,\\epsilon),\\;(0,-1,\\epsilon),\\;(1,1,\\epsilon),\\;(-1,-1,\\epsilon).\n  $$\n  Provide candidate expansion points in the following order and add them one at a time until the estimated numerical rank reaches $\\,3\\,$:\n  $$\n  (0,0,0.2),\\;(0.2,0,0.2),\\;(0,0.2,0.2).\n  $$\n- Case 3 (exactly coplanar with compatible field): use coefficients $\\,a=2.3\\,$, $\\,b=-1.7\\,$, $\\,c=0.0\\,$, and neighbors\n  $$\n  (1,0,0),\\;(-1,0,0),\\;(0,1,0),\\;(0,-1,0),\\;(1,1,0),\\;(-1,-1,0).\n  $$\n  Use the same candidate expansion points as in Case 2, in the same order.\n\nFor each case, construct $\\,\\mathbf{A}\\,$ and $\\,\\mathbf{b}\\,$ exactly from the definitions above. Denote the true gradient by $\\,\\mathbf{g}_{\\mathrm{true}} = (a,b,c)^\\top\\,$. For each method, compute the reconstructed gradient $\\,\\widehat{\\mathbf{g}}\\,$ and the Euclidean error $\\,\\|\\widehat{\\mathbf{g}}-\\mathbf{g}_{\\mathrm{true}}\\|_2\\,$. Also estimate the numerical rank before and after expansion using the pivoted $\\,\\mathbf{Q}\\mathbf{R}\\,$ with the threshold described above.\n\nYour program must produce a single line of output containing the results for the three cases as a comma-separated list enclosed in square brackets. Each case must be a list of the form\n$$\n[\\kappa_2(\\mathbf{A}),\\; r_{\\mathrm{init}},\\; e_{\\mathrm{naive}},\\; e_{\\mathrm{pivot}},\\; e_{\\mathrm{expand}},\\; r_{\\mathrm{expand}}],\n$$\nwhere $\\,\\kappa_2(\\mathbf{A})\\,$ is the capped condition number of $\\,\\mathbf{A}\\,$ (with $\\,10^{18}\\,$ sentinel if singular), $\\,r_{\\mathrm{init}}\\,$ is the estimated numerical rank for the initial stencil, $\\,e_{\\mathrm{naive}}\\,$ is the naive normal-equations error (with $\\,10^{9}\\,$ sentinel if the solve fails or is too ill-conditioned), $\\,e_{\\mathrm{pivot}}\\,$ is the error from the rank-revealing pivoted solution on the initial stencil, $\\,e_{\\mathrm{expand}}\\,$ is the error from the pivoted solution on the expanded stencil, and $\\,r_{\\mathrm{expand}}\\,$ is the estimated numerical rank after expansion. Round all floating-point entries to six decimal places. The final printed line must look like\n$$\n[[\\cdots],[\\cdots],[\\cdots]],\n$$\nwith no additional text.\n\nAll angles, if any, are not applicable here. No physical units are involved in this problem. All requested numerical outputs are pure real numbers without units and must be rounded as specified.", "solution": "The present task requires the implementation and comparative analysis of three numerical methods for least-squares gradient reconstruction of a linear scalar field in three dimensions. The analysis focuses on the numerical stability and accuracy of these methods when applied to geometric stencils of varying quality, ranging from well-conditioned to rank-deficient.\n\n### Problem Formulation\n\nWe are given a linear scalar field $\\phi(\\mathbf{x}) = a\\,x + b\\,y + c\\,z$. The gradient of this field is constant everywhere, given by the vector of coefficients $\\mathbf{g}_{\\text{true}} = \\nabla\\phi = (a, b, c)^\\top$.\n\nThe objective is to reconstruct this gradient at a central point $\\mathbf{x}_0$ using a set of $N$ neighboring points $\\{\\mathbf{x}_i\\}_{i=1}^N$. The least-squares formulation seeks a gradient vector $\\mathbf{g}$ that minimizes the sum of squared errors:\n$$\nJ(\\mathbf{g}) = \\sum_{i=1}^{N} w_i \\left( (\\phi(\\mathbf{x}_i) - \\phi(\\mathbf{x}_0)) - \\mathbf{g} \\cdot (\\mathbf{x}_i - \\mathbf{x}_0) \\right)^2\n$$\nGiven that weights are unity ($w_i=1$) and the central point is the origin ($\\mathbf{x}_0 = (0,0,0)$), we have $\\Delta\\mathbf{x}_i = \\mathbf{x}_i$ and $\\phi(\\mathbf{x}_0) = 0$. The functional simplifies to:\n$$\nJ(\\mathbf{g}) = \\sum_{i=1}^{N} \\left( \\phi(\\mathbf{x}_i) - \\mathbf{g} \\cdot \\mathbf{x}_i \\right)^2\n$$\nThis is a standard linear least-squares problem, which can be expressed in matrix form as finding $\\mathbf{g}$ that minimizes $\\|\\mathbf{A}\\mathbf{g} - \\mathbf{b}\\|_2^2$. The system matrix $\\mathbf{A} \\in \\mathbb{R}^{N \\times 3}$ and the right-hand-side vector $\\mathbf{b} \\in \\mathbb{R}^{N}$ are constructed as follows:\n- The $i$-th row of $\\mathbf{A}$ is the transposed displacement vector, $(\\mathbf{x}_i - \\mathbf{x}_0)^\\top = \\mathbf{x}_i^\\top$.\n- The $i$-th entry of $\\mathbf{b}$ is the scalar difference, $\\phi(\\mathbf{x}_i) - \\phi(\\mathbf{x}_0) = \\phi(\\mathbf{x}_i)$.\n\nSince $\\phi(\\mathbf{x}_i) = a x_i + b y_i + c z_i = \\mathbf{x}_i^\\top \\mathbf{g}_{\\text{true}}$, it follows that $\\mathbf{b} = \\mathbf{A}\\mathbf{g}_{\\text{true}}$. This means the linear system is consistent. If the matrix $\\mathbf{A}$ has full column rank (i.e., rank $3$), the unique least-squares solution is $\\mathbf{g} = \\mathbf{g}_{\\text{true}}$. However, if the neighbor points $\\{\\mathbf{x}_i\\}$ are coplanar or nearly coplanar, the rows of $\\mathbf{A}$ become linearly dependent or nearly so, causing $\\mathbf{A}$ to be rank-deficient or ill-conditioned.\n\n### Numerical Methods and Diagnostics\n\nWe will implement and compare three methods to solve for $\\mathbf{g}$:\n\n1.  **Naive Normal Equations**: This method transforms the overdetermined system into a square $3 \\times 3$ system by pre-multiplying by $\\mathbf{A}^\\top$:\n    $$\n    (\\mathbf{A}^\\top \\mathbf{A}) \\mathbf{g} = \\mathbf{A}^\\top \\mathbf{b}\n    $$\n    The solution is $\\widehat{\\mathbf{g}}_{\\text{naive}} = (\\mathbf{A}^\\top \\mathbf{A})^{-1} \\mathbf{A}^\\top \\mathbf{b}$. While simple, this approach is numerically unstable if $\\mathbf{A}$ is ill-conditioned, because the condition number of the normal matrix $\\mathbf{N} = \\mathbf{A}^\\top \\mathbf{A}$ is the square of the condition number of $\\mathbf{A}$, i.e., $\\kappa_2(\\mathbf{N}) = \\kappa_2(\\mathbf{A})^2$. A large condition number for $\\mathbf{A}$ can lead to a prohibitively large one for $\\mathbf{N}$, resulting in significant numerical error or failure of the linear solver. We will monitor $\\kappa_2(\\mathbf{N})$ and use a sentinel error value of $10^9$ if it exceeds $10^{15}$ or if the solver fails.\n\n2.  **Rank-Revealing Pivoted QR Factorization**: This method provides a more robust solution by directly addressing the rank of $\\mathbf{A}$. We compute the column-pivoted QR factorization $\\mathbf{A}\\mathbf{P} = \\mathbf{Q}\\mathbf{R}$, where $\\mathbf{P}$ is a permutation matrix, $\\mathbf{Q}$ has orthonormal columns, and $\\mathbf{R}$ is upper triangular with diagonal entries of decreasing magnitude. The numerical rank $r$ is estimated by counting the number of diagonal elements $|R_{kk}|$ that are larger than a threshold $\\tau = \\text{rcond} \\cdot |R_{11}|$, with $\\text{rcond} = 10^{-12}$. The least-squares problem is solved by finding the minimal-norm solution. This is achieved by solving the well-conditioned part of the system corresponding to the estimated rank $r$ and setting the remaining $3-r$ components of the permuted solution vector to zero. This procedure effectively regularizes the problem by projecting it onto the numerically stable subspace spanned by the \"strong\" columns of $\\mathbf{A}$.\n\n3.  **Stencil Expansion**: When the initial stencil of neighbors is geometrically deficient (rank $< 3$), this method remedies the issue by augmenting the stencil. Candidate points are added one by one to the set of neighbors until the numerical rank, estimated via pivoted QR, reaches the full rank of $3$. Once a full-rank stencil is achieved, the gradient $\\widehat{\\mathbf{g}}_{\\text{expand}}$ is computed using the robust pivoted QR method on this expanded system. This approach aims to restore the well-posedness of the problem by incorporating additional geometric information.\n\nThe quality of the initial stencil is quantified by the $2$-norm condition number of $\\mathbf{A}$, $\\kappa_2(\\mathbf{A}) = \\sigma_{\\max}/\\sigma_{\\min}$, computed from its singular values. If $\\mathbf{A}$ is singular ($\\sigma_{\\min}=0$), a sentinel value of $10^{18}$ is reported. The performance of each method is measured by the Euclidean error $\\|\\widehat{\\mathbf{g}} - \\mathbf{g}_{\\text{true}}\\|_2$.\n\n### Analysis of Test Cases\n\n-   **Case 1 (Well-conditioned)**: The neighbors form an orthogonal basis. The matrix $\\mathbf{A}$ is perfectly well-conditioned with $\\kappa_2(\\mathbf{A}) = 1$. The numerical rank is robustly identified as $3$. All three methods are expected to work flawlessly, yielding errors close to machine precision ($0$).\n\n-   **Case 2 (Nearly Coplanar)**: The neighbors lie on the plane $z = \\epsilon = 10^{-8}$. The displacement vectors are nearly coplanar, making the matrix $\\mathbf{A}$ severely ill-conditioned with a very large $\\kappa_2(\\mathbf{A})$. The numerical rank will be estimated as $2$. The normal equations method is expected to produce a large error due to the squared condition number. The pivoted QR method will correctly identify the rank deficiency and find a solution within the dominant $2$D subspace, resulting in a significantly smaller error. Stencil expansion will add a point with a non-trivial $z$-component, restoring the rank to $3$ and enabling an accurate reconstruction with very low error.\n\n-   **Case 3 (Exactly Coplanar)**: The neighbors lie on the plane $z=0$. The matrix $\\mathbf{A}$ is exactly rank-deficient (rank $2$), and its smallest singular value is zero, yielding an infinite condition number. The matrix $\\mathbf{A}^\\top \\mathbf{A}$ is singular, so the normal equations method will fail, triggering the sentinel error. The true gradient's $z$-component is $c=0$, meaning $\\mathbf{g}_{\\text{true}}$ lies within the subspace spanned by the columns of $\\mathbf{A}$. The pivoted QR method, by finding the minimal-norm solution, will correctly identify $\\mathbf{g}_{\\text{true}}$ and yield zero error. Stencil expansion will restore rank to $3$ and also find the exact solution.", "answer": "```python\nimport numpy as np\nfrom scipy.linalg import qr, solve_triangular\n\ndef solve_pivoted_qr(A, b, rcond):\n    \"\"\"\n    Solves the least-squares problem Ax = b using rank-revealing QR\n    factorization with column pivoting.\n    \"\"\"\n    n_cols = A.shape[1]\n    if A.shape[0] == 0:\n        return np.zeros(n_cols), 0\n        \n    Q, R, P = qr(A, pivoting=True)\n    \n    # Estimate numerical rank\n    if np.abs(R[0, 0]) < np.finfo(float).eps:\n        rank = 0\n    else:\n        tau = rcond * np.abs(R[0, 0])\n        rank = np.sum(np.abs(np.diag(R)) > tau)\n\n    # Solve for the permuted gradient vector y = P^T * g\n    y = np.zeros(n_cols)\n    if rank > 0:\n        d = Q.T @ b\n        try:\n            y[:rank] = solve_triangular(R[:rank, :rank], d[:rank], check_finite=False)\n        except np.linalg.LinAlgError:\n            # This should not happen with a proper rank check, but as a safeguard.\n            pass\n\n    # Un-permute the solution to get g\n    g_hat = np.zeros(n_cols)\n    g_hat[P] = y\n    \n    return g_hat, rank\n\ndef calc_error(g_hat, g_true):\n    \"\"\"Computes the Euclidean norm of the error vector.\"\"\"\n    return np.linalg.norm(g_hat - g_true)\n\ndef process_case(coeffs, neighbors, expansion_pts):\n    \"\"\"\n    Processes a single test case, computes gradients with three methods,\n    and returns all specified diagnostic values.\n    \"\"\"\n    # Constants and Sentinels\n    RCOND_QR = 1e-12\n    COND_N_THRESH = 1e15\n    ERROR_SENTINEL = 1e9\n    KAPPA_SENTINEL = 1e18\n\n    # --- Initial Stencil Setup ---\n    g_true = np.array(coeffs, dtype=float)\n    x0 = np.array([0.0, 0.0, 0.0])\n    \n    A = neighbors - x0\n    b = A @ g_true\n\n    # --- Diagnostics for Initial Stencil ---\n    # Condition number of A using SVD\n    try:\n        singular_values = np.linalg.svd(A, compute_uv=False)\n        if singular_values[-1] < np.finfo(float).eps * singular_values[0]:\n            kappa_A = KAPPA_SENTINEL\n        else:\n            kappa_A = singular_values[0] / singular_values[-1]\n    except np.linalg.LinAlgError:\n        kappa_A = KAPPA_SENTINEL\n\n    # --- Method 2: Rank-Revealing Pivoted QR (Initial Stencil) ---\n    g_pivot, r_init = solve_pivoted_qr(A, b, RCOND_QR)\n    e_pivot = calc_error(g_pivot, g_true)\n\n    # --- Method 1: Naive Normal Equations ---\n    e_naive = ERROR_SENTINEL\n    N = A.T @ A\n    c = A.T @ b\n    if np.linalg.cond(N) < COND_N_THRESH:\n        try:\n            g_naive = np.linalg.solve(N, c)\n            e_naive = calc_error(g_naive, g_true)\n        except np.linalg.LinAlgError:\n            e_naive = ERROR_SENTINEL\n    \n    # --- Method 3: Stencil Expansion ---\n    if r_init == 3:\n        # If rank is already full, expansion is not needed.\n        A_exp, b_exp = A, b\n        r_expand = r_init\n        g_expand = g_pivot\n        e_expand = e_pivot\n    else:\n        # Augment stencil until rank is 3\n        current_neighbors = neighbors.copy()\n        current_rank = r_init\n        \n        for pt in expansion_pts:\n            if current_rank == 3:\n                break\n            \n            # Add point and rebuild system\n            current_neighbors = np.vstack([current_neighbors, pt])\n            A_exp_loop = current_neighbors - x0\n            \n            # Re-evaluate rank\n            _, R_loop, _ = qr(A_exp_loop, pivoting=True)\n            tau_loop = RCOND_QR * np.abs(R_loop[0, 0])\n            current_rank = np.sum(np.abs(np.diag(R_loop)) > tau_loop)\n\n        # Final expanded system\n        A_exp = current_neighbors - x0\n        b_exp = A_exp @ g_true\n        r_expand = current_rank\n        \n        # Solve with expanded stencil\n        g_expand, _ = solve_pivoted_qr(A_exp, b_exp, RCOND_QR)\n        e_expand = calc_error(g_expand, g_true)\n        \n    return [kappa_A, float(r_init), e_naive, e_pivot, e_expand, float(r_expand)]\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    # Case 1: Well-conditioned\n    case1 = {\n        \"coeffs\": (2.3, -1.7, 0.9),\n        \"neighbors\": np.array([\n            [1, 0, 0], [-1, 0, 0], [0, 1, 0],\n            [0, -1, 0], [0, 0, 1], [0, 0, -1]\n        ]),\n        \"expansion_pts\": []\n    }\n\n    # Case 2: Nearly coplanar\n    epsilon = 1e-8\n    case2 = {\n        \"coeffs\": (2.3, -1.7, 0.9),\n        \"neighbors\": np.array([\n            [1, 0, epsilon], [-1, 0, epsilon], [0, 1, epsilon],\n            [0, -1, epsilon], [1, 1, epsilon], [-1, -1, epsilon]\n        ]),\n        \"expansion_pts\": np.array([\n            [0, 0, 0.2], [0.2, 0, 0.2], [0, 0.2, 0.2]\n        ])\n    }\n\n    # Case 3: Exactly coplanar\n    case3 = {\n        \"coeffs\": (2.3, -1.7, 0.0),\n        \"neighbors\": np.array([\n            [1, 0, 0], [-1, 0, 0], [0, 1, 0],\n            [0, -1, 0], [1, 1, 0], [-1, -1, 0]\n        ]),\n        \"expansion_pts\": np.array([\n            [0, 0, 0.2], [0.2, 0, 0.2], [0, 0.2, 0.2]\n        ])\n    }\n    \n    test_cases = [case1, case2, case3]\n    all_results = [process_case(**params) for params in test_cases]\n\n    # Format output string\n    output_parts = []\n    for result in all_results:\n        formatted_result = [f\"{val:.6f}\" for val in result]\n        output_parts.append(f\"[{','.join(formatted_result)}]\")\n    \n    print(f\"[{','.join(output_parts)}]\")\n\nsolve()\n```", "id": "3339293"}, {"introduction": "After mastering the robust computation of a gradient, the final step is to integrate it effectively within a larger numerical framework, such as a finite-volume scheme. A raw, unlimited linear reconstruction can introduce non-physical oscillations near sharp solution features, violating the discrete maximum principle. This practice explores the design of limiters, which are essential for ensuring that the high-order reconstruction remains bounded and the overall scheme is stable and non-oscillatory, bridging the gap between numerical analysis and physical realism [@problem_id:3339339].", "problem": "Consider a finite-volume method for a scalar field $f$ on an unstructured grid in computational fluid dynamics (CFD), where a piecewise-linear reconstruction is applied inside each cell $i$ using a least-squares gradient $g$. The reconstruction at a sampling location $x_i + \\Delta \\boldsymbol{x}_s$ associated with cell $i$ is $f_i + \\phi\\, g \\cdot \\Delta \\boldsymbol{x}_s$, where $f_i$ is the cell-average in cell $i$, $\\Delta \\boldsymbol{x}_s$ is the vector from the cell centroid $x_i$ to the sampling point $s \\in S(i)$, and $\\phi \\in [0,1]$ is a scalar limiter to be designed. The least-squares gradient $g$ is understood to be consistent for linear data, i.e., it exactly reproduces gradients when $f$ varies linearly.\n\nTo avoid introducing spurious oscillations and ensure a local discrete maximum principle (DMP), the reconstruction at all sampling points in $S(i)$ must remain bounded by the local extrema built from cell averages in a near-neighborhood. Let $\\mathcal{N}(i)$ denote the set of neighbor cells of $i$. Define\n$$\nf_{\\min,i} = \\min\\left(\\{f_i\\} \\cup \\{f_j : j \\in \\mathcal{N}(i)\\}\\right), \\qquad\nf_{\\max,i} = \\max\\left(\\{f_i\\} \\cup \\{f_j : j \\in \\mathcal{N}(i)\\}\\right).\n$$\nA limiter $\\phi$ is sought that enforces, for all $s \\in S(i)$,\n$$\nf_{\\min,i} \\le f_i + \\phi \\, g \\cdot \\Delta \\boldsymbol{x}_s \\le f_{\\max,i},\n$$\nwith $\\phi \\in [0,1]$, and such that $\\phi = 1$ whenever the unconstrained reconstruction $f_i + g \\cdot \\Delta \\boldsymbol{x}_s$ already satisfies these bounds for all $s \\in S(i)$. The design must be justified from first principles: use the definitions above, inequality manipulation, and properties of the reconstruction and least-squares gradient, without appealing to shortcut formulas.\n\nWhich of the following candidate definitions for $\\phi$ satisfies these requirements?\n\nA. \n$$\n\\phi = \\min_{s \\in S(i)}\n\\begin{cases}\n\\min\\!\\left(1, \\dfrac{f_{\\max,i} - f_i}{g \\cdot \\Delta \\boldsymbol{x}_s}\\right), & g \\cdot \\Delta \\boldsymbol{x}_s > 0 \\\\\n\\min\\!\\left(1, \\dfrac{f_{\\min,i} - f_i}{g \\cdot \\Delta \\boldsymbol{x}_s}\\right), & g \\cdot \\Delta \\boldsymbol{x}_s < 0 \\\\\n1, & g \\cdot \\Delta \\boldsymbol{x}_s = 0\n\\end{cases}\n$$\n\nB.\n$$\n\\phi = \\max_{s \\in S(i)}\n\\begin{cases}\n\\min\\!\\left(1, \\dfrac{f_{\\max,i} - f_i}{g \\cdot \\Delta \\boldsymbol{x}_s}\\right), & g \\cdot \\Delta \\boldsymbol{x}_s > 0 \\\\\n\\min\\!\\left(1, \\dfrac{f_{\\min,i} - f_i}{g \\cdot \\Delta \\boldsymbol{x}_s}\\right), & g \\cdot \\Delta \\boldsymbol{x}_s < 0 \\\\\n1, & g \\cdot \\Delta \\boldsymbol{x}_s = 0\n\\end{cases}\n$$\n\nC.\n$$\n\\phi = \\min\\!\\left(1, \\min_{s \\in S(i)} \\dfrac{f_{\\max,i} - f_{\\min,i}}{\\left|g \\cdot \\Delta \\boldsymbol{x}_s\\right|}\\right)\n$$\n\nD.\n$$\n\\phi = \\min\\!\\left(1, \\max_{s \\in S(i)} \\dfrac{\\left|f_i\\right|}{\\left|g \\cdot \\Delta \\boldsymbol{x}_s\\right|}\\right)\n$$\n\nSelect the option that yields a limiter $\\phi$ consistent with the stated first-principles requirements, including enforcement of the local bounds for all $s \\in S(i)$, $\\phi \\in [0,1]$, and preservation of linear exactness in smooth regions by recovering $\\phi = 1$ when unconstrained reconstructions are already bounded.", "solution": "## Problem Validation\n\n### Step 1: Extract Givens\nThe problem statement provides the following definitions and conditions:\n- A scalar field $f$ on an unstructured grid.\n- A piecewise-linear reconstruction inside each cell $i$: $f_i + \\phi\\, g \\cdot \\Delta \\boldsymbol{x}_s$.\n- $f_i$: the cell-average in cell $i$.\n- $g$: a least-squares gradient, consistent for linear data.\n- $x_i$: the cell centroid of cell $i$.\n- $S(i)$: a set of sampling locations associated with cell $i$.\n- $\\Delta \\boldsymbol{x}_s$: vector from centroid $x_i$ to sampling point $s \\in S(i)$.\n- $\\phi$: a scalar limiter, with the constraint $\\phi \\in [0,1]$.\n- $\\mathcal{N}(i)$: the set of neighbor cells of $i$.\n- Local extrema definitions:\n  $$f_{\\min,i} = \\min\\left(\\{f_i\\} \\cup \\{f_j : j \\in \\mathcal{N}(i)\\}\\right)$$\n  $$f_{\\max,i} = \\max\\left(\\{f_i\\} \\cup \\{f_j : j \\in \\mathcal{N}(i)\\}\\right)$$\n- The limiter $\\phi$ must enforce the following bounds for all $s \\in S(i)$:\n  $$f_{\\min,i} \\le f_i + \\phi \\, g \\cdot \\Delta \\boldsymbol{x}_s \\le f_{\\max,i}$$\n- An additional requirement: $\\phi = 1$ whenever the unconstrained reconstruction $f_i + g \\cdot \\Delta \\boldsymbol{x}_s$ already satisfies the bounds for all $s \\in S(i)$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is well-defined and scientifically grounded within the field of computational fluid dynamics (CFD), specifically concerning high-resolution finite-volume methods.\n- **Scientifically Grounded**: The concepts presented—finite-volume methods, cell-averages, gradient reconstruction, limiters, and the discrete maximum principle (DMP)—are standard and fundamental components of numerical schemes for conservation laws. The formulation is a direct representation of the Barth-Jespersen limiter, a well-established technique.\n- **Well-Posed**: The task is to derive a formula for a scalar $\\phi$ that satisfies a set of well-defined mathematical inequalities. The existence of such a limiter is known, and the problem provides all necessary information to derive its form.\n- **Objective**: The problem is stated using precise mathematical language and definitions ($f_i, g, \\Delta \\boldsymbol{x}_s, f_{\\min,i}, f_{\\max,i}$). There are no subjective or ambiguous terms.\n\nThe problem does not violate any of the invalidity criteria. It is a standard, verifiable problem in numerical analysis for partial differential equations.\n\n### Step 3: Verdict and Action\nThe problem statement is **valid**. Proceeding to the solution.\n\n## Derivation of the Limiter $\\phi$\n\nThe objective is to find a single value for the limiter $\\phi$ for cell $i$ that ensures the reconstructed values at all sampling points $s \\in S(i)$ remain within the local bounds $[f_{\\min,i}, f_{\\max,i}]$. The core constraint is a pair of inequalities that must hold for all $s \\in S(i)$:\n$$\nf_{\\min,i} \\le f_i + \\phi \\, g \\cdot \\Delta \\boldsymbol{x}_s \\le f_{\\max,i}\n$$\nWe also have the constraint that $\\phi \\in [0,1]$. By the definitions of $f_{\\min,i}$ and $f_{\\max,i}$, we know that $f_{\\min,i} \\le f_i \\le f_{\\max,i}$. This implies $f_{\\max,i} - f_i \\ge 0$ and $f_{\\min,i} - f_i \\le 0$.\n\nWe can split the main inequality into two parts:\n1. $f_i + \\phi \\, g \\cdot \\Delta \\boldsymbol{x}_s \\le f_{\\max,i} \\implies \\phi \\, g \\cdot \\Delta \\boldsymbol{x}_s \\le f_{\\max,i} - f_i$\n2. $f_i + \\phi \\, g \\cdot \\Delta \\boldsymbol{x}_s \\ge f_{\\min,i} \\implies \\phi \\, g \\cdot \\Delta \\boldsymbol{x}_s \\ge f_{\\min,i} - f_i$\n\nTo isolate $\\phi$, we must consider the sign of the quantity $\\delta_s = g \\cdot \\Delta \\boldsymbol{x}_s$.\n\n**Case 1: $\\delta_s = g \\cdot \\Delta \\boldsymbol{x}_s > 0$**\nInequality (1) becomes: $\\phi \\le \\dfrac{f_{\\max,i} - f_i}{\\delta_s}$.\nInequality (2) becomes: $\\phi \\ge \\dfrac{f_{\\min,i} - f_i}{\\delta_s}$.\nSince $f_{\\min,i} - f_i \\le 0$ and $\\delta_s > 0$, the right-hand side of the second inequality is non-positive. The constraint $\\phi \\ge 0$ is stronger. Thus, for $\\phi \\in [0,1]$, the second inequality is automatically satisfied. The only active constraint on $\\phi$ is from the upper bound:\n$$\n\\phi \\le \\dfrac{f_{\\max,i} - f_i}{g \\cdot \\Delta \\boldsymbol{x}_s}\n$$\n\n**Case 2: $\\delta_s = g \\cdot \\Delta \\boldsymbol{x}_s < 0$**\nWhen we divide by a negative number, the inequality sign flips.\nInequality (1) becomes: $\\phi \\ge \\dfrac{f_{\\max,i} - f_i}{\\delta_s}$.\nInequality (2) becomes: $\\phi \\le \\dfrac{f_{\\min,i} - f_i}{\\delta_s}$.\nSince $f_{\\max,i} - f_i \\ge 0$ and $\\delta_s < 0$, the right-hand side of the first inequality is non-positive. The constraint $\\phi \\ge 0$ is stronger, so the first inequality is automatically satisfied for $\\phi \\in [0,1]$. The only active constraint on $\\phi$ is from the lower bound:\n$$\n\\phi \\le \\dfrac{f_{\\min,i} - f_i}{g \\cdot \\Delta \\boldsymbol{x}_s}\n$$\nNote that since both numerator ($f_{\\min,i} - f_i \\le 0$) and denominator ($g \\cdot \\Delta \\boldsymbol{x}_s < 0$) are non-positive/negative, the ratio is non-negative, which is consistent.\n\n**Case 3: $\\delta_s = g \\cdot \\Delta \\boldsymbol{x}_s = 0$**\nThe inequalities become $f_{\\min,i} \\le f_i \\le f_{\\max,i}$. This is always true by definition. Thus, there is no constraint on $\\phi$ in this case. To be as non-dissipative as possible, we can allow $\\phi$ to be its maximum value, i.e., $\\phi=1$.\n\n**Combining Constraints**\nFor each sampling point $s \\in S(i)$, we have an upper bound on $\\phi$, let's call it $\\phi_s^{\\max}$.\n$$\n\\phi_s^{\\max} =\n\\begin{cases}\n\\dfrac{f_{\\max,i} - f_i}{g \\cdot \\Delta \\boldsymbol{x}_s}, & g \\cdot \\Delta \\boldsymbol{x}_s > 0 \\\n$$2ex]\n\\dfrac{f_{\\min,i} - f_i}{g \\cdot \\Delta \\boldsymbol{x}_s}, & g \\cdot \\Delta \\boldsymbol{x}_s < 0 \\\n$$2ex]\n\\infty, & g \\cdot \\Delta \\boldsymbol{x}_s = 0\n\\end{cases}\n$$\nThe single limiter value $\\phi$ for cell $i$ must satisfy $\\phi \\le \\phi_s^{\\max}$ for **all** $s \\in S(i)$. To find the largest possible $\\phi$ that satisfies all these constraints, we must take the minimum of all the upper bounds:\n$$\n\\phi_{\\text{candidate}} = \\min_{s \\in S(i)} \\left( \\phi_s^{\\max} \\right)\n$$\nAdditionally, the problem requires $\\phi \\in [0,1]$. As shown, $\\phi_s^{\\max}$ is always non-negative, so their minimum is also non-negative. We only need to enforce $\\phi \\le 1$. The final, most permissive (largest) $\\phi$ that satisfies all constraints is:\n$$\n\\phi = \\min\\left(1, \\min_{s \\in S(i)} \\left( \\phi_s^{\\max} \\right)\\right)\n$$\nThis can be rewritten by moving the $\\min(1, \\cdot)$ inside the minimum over $s$:\n$$\n\\phi = \\min_{s \\in S(i)} \\left( \\min(1, \\phi_s^{\\max}) \\right)\n$$\nSubstituting the piecewise definition of $\\phi_s^{\\max}$:\n$$\n\\phi = \\min_{s \\in S(i)}\n\\begin{cases}\n\\min\\!\\left(1, \\dfrac{f_{\\max,i} - f_i}{g \\cdot \\Delta \\boldsymbol{x}_s}\\right), & g \\cdot \\Delta \\boldsymbol{x}_s > 0 \\\n$$2ex]\n\\min\\!\\left(1, \\dfrac{f_{\\min,i} - f_i}{g \\cdot \\Delta \\boldsymbol{x}_s}\\right), & g \\cdot \\Delta \\boldsymbol{x}_s < 0 \\\n$$2ex]\n\\min(1, \\infty) = 1, & g \\cdot \\Delta \\boldsymbol{x}_s = 0\n\\end{cases}\n$$\nThis expression matches Option A.\n\nFinally, we verify the condition that if the unconstrained reconstruction ($f_i + g \\cdot \\Delta \\boldsymbol{x}_s$) is already bounded for all $s$, then $\\phi = 1$.\nThe condition $f_{\\min,i} \\le f_i + g \\cdot \\Delta \\boldsymbol{x}_s \\le f_{\\max,i}$ is equivalent to:\n- If $g \\cdot \\Delta \\boldsymbol{x}_s > 0$: $g \\cdot \\Delta \\boldsymbol{x}_s \\le f_{\\max,i} - f_i \\implies 1 \\le \\dfrac{f_{\\max,i} - f_i}{g \\cdot \\Delta \\boldsymbol{x}_s}$.\n- If $g \\cdot \\Delta \\boldsymbol{x}_s < 0$: $g \\cdot \\Delta \\boldsymbol{x}_s \\ge f_{\\min,i} - f_i \\implies 1 \\le \\dfrac{f_{\\min,i} - f_i}{g \\cdot \\Delta \\boldsymbol{x}_s}$.\n- If $g \\cdot \\Delta \\boldsymbol{x}_s = 0$, the condition is trivially satisfied.\nIn all cases where limiting is potentially needed, the ratio that defines $\\phi_s^{\\max}$ is greater than or equal to $1$. Thus, for each $s$, the term inside the $\\min_{s \\in S(i)}$ is $\\min(1, \\text{a value} \\ge 1) = 1$. The overall result is $\\phi = \\min(1, 1, \\dots, 1) = 1$. The condition is satisfied.\n\n## Option-by-Option Analysis\n\n**A.**\n$$\n\\phi = \\min_{s \\in S(i)}\n\\begin{cases}\n\\min\\!\\left(1, \\dfrac{f_{\\max,i} - f_i}{g \\cdot \\Delta \\boldsymbol{x}_s}\\right), & g \\cdot \\Delta \\boldsymbol{x}_s > 0, \\\\\n\\min\\!\\left(1, \\dfrac{f_{\\min,i} - f_i}{g \\cdot \\Delta \\boldsymbol{x}_s}\\right), & g \\cdot \\Delta \\boldsymbol{x}_s < 0, \\\\\n1, & g \\cdot \\Delta \\boldsymbol{x}_s = 0,\n\\end{cases}\n$$\nThis option perfectly matches our step-by-step derivation from first principles. It correctly identifies the active bound for each sign of $g \\cdot \\Delta \\boldsymbol{x}_s$, takes the minimum permissible factor over all sampling points $s \\in S(i)$ to ensure the bounds are respected everywhere, and incorporates the $\\phi \\in [0,1]$ constraint.\n**Verdict: Correct.**\n\n**B.**\n$$\n\\phi = \\max_{s \\in S(i)}\n\\begin{cases}\n...\n\\end{cases}\n$$\nThis option uses $\\max_{s \\in S(i)}$ instead of $\\min_{s \\in S(i)}$. If different sampling points require different levels of limiting (e.g., for point $s_1$, $\\phi \\le 0.5$ is required, and for point $s_2$, $\\phi \\le 0.8$ is required), taking the maximum ($\\phi = 0.8$) would satisfy the condition for $s_2$ but violate it for $s_1$. To satisfy the bounds for all points, the most restrictive (smallest) upper bound must be chosen. Therefore, a minimum over $s \\in S(i)$ is required.\n**Verdict: Incorrect.**\n\n**C.**\n$$\n\\phi = \\min\\!\\left(1, \\min_{s \\in S(i)} \\dfrac{f_{\\max,i} - f_{\\min,i}}{\\left|g \\cdot \\Delta \\boldsymbol{x}_s\\right|}\\right)\n$$\nThis option uses the total range of neighboring values, $f_{\\max,i} - f_{\\min,i}$, as the numerator. This does not correctly represent the \"room\" available for the reconstruction, which is either $f_{\\max,i} - f_i$ or $f_i - f_{\\min,i}$. Consider $f_{\\min,i}=0$, $f_i=0.9$, $f_{\\max,i}=1$, and $g \\cdot \\Delta \\boldsymbol{x}_s = 0.2 > 0$. The reconstruction must satisfy $0.9 + \\phi(0.2) \\le 1$, which imposes $\\phi \\le 0.5$. Option C calculates a limit based on $\\dfrac{1-0}{|0.2|} = 5$, giving $\\phi = \\min(1,5)=1$. Using $\\phi=1$ gives a reconstructed value of $0.9 + 1(0.2) = 1.1$, which violates the bound $f_{\\max,i}=1$.\n**Verdict: Incorrect.**\n\n**D.**\n$$\n\\phi = \\min\\!\\left(1, \\max_{s \\in S(i)} \\dfrac{\\left|f_i\\right|}{\\left|g \\cdot \\Delta \\boldsymbol{x}_s\\right|}\\right)\n$$\nThis option has multiple flaws. First, it uses $|f_i|$ as the numerator, which has no connection to the bounds $f_{\\min,i}$ and $f_{\\max,i}$ or the available correction range. Second, similar to option B, it incorrectly uses $\\max_{s \\in S(i)}$, which would fail to enforce the condition for the most restrictive sampling point. Using the same counterexample as for C ($f_i=0.9$, $g \\cdot \\Delta \\boldsymbol{x}_s = 0.2$, requiring $\\phi \\le 0.5$), this formula gives (for a single point) $\\phi = \\min(1, \\frac{|0.9|}{|0.2|}) = \\min(1, 4.5) = 1$. This would lead to an overshoot.\n**Verdict: Incorrect.**", "answer": "$$\\boxed{A}$$", "id": "3339339"}]}