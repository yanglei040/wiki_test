{"hands_on_practices": [{"introduction": "The fundamental process of hyperbolic grid generation involves marching points from a boundary along normal vectors. This exercise [@problem_id:3332127] focuses on a critical practical challenge that arises when these normals are derived from a level-set function, which can fail at sharp geometric corners. By implementing and testing a robust fallback mechanism, you will directly address this common failure mode and learn to build more resilient grid generation tools.", "problem": "Consider hyperbolic grid generation in two spatial dimensions where a grid is advanced into the interior of a domain by marching along a family of normals from a closed boundary curve. The fundamental base for this formulation is that the mapping from a computational marching coordinate $s$ and a boundary parameter $\\xi$ to physical space $\\mathbf{x}(s,\\xi)\\in\\mathbb{R}^2$ satisfies a first-order Partial Differential Equation (PDE) of hyperbolic type, namely $\\,\\partial_s \\mathbf{x} = a(\\mathbf{x})\\,\\mathbf{n}(\\mathbf{x})\\,$, where $a(\\mathbf{x})$ is a positive speed function and $\\mathbf{n}(\\mathbf{x})$ is a unit normal field. A widely used construction in Computational Fluid Dynamics (CFD) is to define the unit normal via a level set $\\,\\phi(\\mathbf{x})\\,$, taking $\\,\\mathbf{n}(\\mathbf{x}) = \\nabla \\phi(\\mathbf{x})/\\lVert \\nabla \\phi(\\mathbf{x})\\rVert\\,$ where $\\nabla$ denotes the spatial gradient. It is also well-known that when the level set $\\,\\phi\\,$ has kinks or non-differentiable features near corners of the boundary $\\,\\Gamma\\,$, the gradient magnitude $\\,\\lVert \\nabla \\phi\\rVert\\,$ can be near zero, which may compromise robustness of the marching scheme. In this problem you will discretize this hyperbolic marching process and study robustness in the presence of such kinks.\n\nYou must implement the following, starting only from the preceding fundamental base and standard numerical discretization principles.\n\n- Discretize the marching PDE $\\,\\partial_s \\mathbf{x} = \\mathbf{n}(\\mathbf{x})\\,$ with constant speed $\\,a(\\mathbf{x})\\equiv 1\\,$ using an explicit forward Euler update with marching step $\\,\\Delta s > 0\\,$. Let $\\,\\Gamma(\\xi)\\,$ be a given counterclockwise parameterization of the boundary with parameter $\\,\\xi\\in[0,1)\\,$ sampled at $\\,N\\,$ uniformly spaced points $\\,\\{\\xi_j\\}_{j=0}^{N-1}\\,$ with periodic indexing. Let $\\,\\mathbf{x}^{(0)}_j = \\Gamma(\\xi_j)\\,$ and advance $\\,M\\,$ layers by\n$$\n\\mathbf{x}^{(k+1)}_j \\;=\\; \\mathbf{x}^{(k)}_j \\;+\\; \\Delta s \\,\\mathbf{n}_{\\mathrm{reg}}\\!\\left(\\mathbf{x}^{(k)}_j;\\,\\xi_j\\right), \\quad \\text{for } j\\in\\{0,\\dots,N-1\\},\\; k\\in\\{0,\\dots,M-1\\}.\n$$\nThe field $\\,\\mathbf{n}_{\\mathrm{reg}}\\,$ is a robust unit inward normal defined as follows: compute $\\,\\nabla \\phi(\\mathbf{x})\\,$ and its magnitude $\\,g=\\lVert \\nabla \\phi(\\mathbf{x})\\rVert\\,$. If $\\,g> \\tau\\,$ for a specified threshold $\\,\\tau\\ge 0\\,$, set $\\,\\mathbf{n}_{\\mathrm{reg}}(\\mathbf{x};\\xi) = \\nabla \\phi(\\mathbf{x})/g\\,$. Otherwise, replace the level-set-derived direction by the unit inward geometric normal $\\,\\mathbf{n}_{\\mathrm{geo}}(\\xi)\\,$ associated with the boundary parameter $\\,\\xi\\,$, defined by rotating the unit tangent $\\,\\mathbf{t}(\\xi)$ of $\\,\\Gamma(\\xi)\\,$ by $\\,{+}\\,\\pi/2\\,$ radians to point inward. For tests where the fallback is disabled, interpret $\\,\\tau<0\\,$ to mean that no fallback is allowed and, if $\\,g=0\\,$ occurs, you must take $\\,\\mathbf{n}_{\\mathrm{reg}}(\\mathbf{x};\\xi)=\\mathbf{0}\\,$ at that point.\n\n- To assess robustness, approximate the local area scaling (the magnitude of the discrete Jacobian determinant) between marching layers and along-boundary neighbors. For each layer $\\,k\\in\\{0,\\dots,M-1\\}\\,$ and each $\\,j\\in\\{0,\\dots,N-1\\}\\,$ define the vectors\n$$\n\\Delta_s \\mathbf{x}^{(k)}_j = \\mathbf{x}^{(k+1)}_j - \\mathbf{x}^{(k)}_j,\\qquad\n\\Delta_\\xi \\mathbf{x}^{(k)}_j = \\mathbf{x}^{(k)}_{j+1\\,\\mathrm{mod}\\,N} - \\mathbf{x}^{(k)}_j,\n$$\nand the oriented cell area (scalar in two dimensions) by the $z$-component of the cross product\n$$\nA^{(k)}_j \\;=\\; \\big(\\Delta_s \\mathbf{x}^{(k)}_j\\big)_x \\,\\big(\\Delta_\\xi \\mathbf{x}^{(k)}_j\\big)_y \\;-\\; \\big(\\Delta_s \\mathbf{x}^{(k)}_j\\big)_y \\,\\big(\\Delta_\\xi \\mathbf{x}^{(k)}_j\\big)_x,\n$$\nand use its magnitude $\\,|A^{(k)}_j|\\,$ as a discrete surrogate for the absolute Jacobian. Define the minimum absolute area over the whole grid by\n$$\nA_{\\min} \\;=\\; \\min_{\\,0\\le k\\le M-1}\\;\\min_{\\,0\\le j\\le N-1} \\, |A^{(k)}_j|.\n$$\n\n- Implement two level sets $\\,\\phi(\\mathbf{x})\\,$ and their gradients $\\,\\nabla\\phi(\\mathbf{x})\\,$:\n  1. A smooth circle: $\\,\\phi_{\\mathrm{circ}}(x,y) = 1 - \\sqrt{x^2 + y^2}\\,$ so that $\\,\\nabla \\phi_{\\mathrm{circ}}(x,y) = -\\big(x,y\\big)/\\sqrt{x^2+y^2}\\,$ points inward.\n  2. A product form that has a kink at the square corners: $\\,\\phi_{\\square}(x,y) = \\big(|x|-1\\big)\\,\\big(|y|-1\\big)\\,$, with gradient $\\,\\nabla \\phi_{\\square}(x,y) = \\big(\\mathrm{sgn}(x)\\,(|y|-1),\\;\\mathrm{sgn}(y)\\,(|x|-1)\\big)\\,$, where $\\,\\mathrm{sgn}(\\cdot)\\,$ is the sign function with $\\,\\mathrm{sgn}(0)=0\\,$. Note that $\\,\\nabla \\phi_{\\square}\\,$ vanishes at the four corners of the square, modeling a level-set kink.\n\n- Parameterize the boundary $\\,\\Gamma(\\xi)\\,$ and its unit tangent $\\,\\mathbf{t}(\\xi)\\,$ as follows:\n  1. For the circle of radius $\\,1\\,$: $\\,\\Gamma_{\\mathrm{circ}}(\\xi) = \\big(\\cos(2\\pi \\xi),\\;\\sin(2\\pi \\xi)\\big)\\,$ for $\\,\\xi\\in[0,1)\\,$ sampled at $\\,N\\,$ equispaced values, with $\\,N$ specified per test.\n  2. For the square $\\,[-1,1]\\times[-1,1]\\,$: sample the boundary counterclockwise including the four corners exactly once by taking $\\,N_{\\mathrm{edge}}\\,$ points on the right edge including both endpoints, then $\\,N_{\\mathrm{edge}}-1\\,$ points on the top edge excluding the shared corner, then $\\,N_{\\mathrm{edge}}-1\\,$ points on the left edge excluding the shared corner, and finally $\\,N_{\\mathrm{edge}}-2\\,$ points on the bottom edge excluding both shared corners, giving a total $\\,N = 4\\,N_{\\mathrm{edge}} - 4\\,$ points. Use periodic central differences along this closed polygonal chain to approximate $\\,\\mathbf{t}(\\xi)\\,$, and rotate by $\\,{+}\\,\\pi/2\\,$ radians to obtain the unit inward geometric normal $\\,\\mathbf{n}_{\\mathrm{geo}}(\\xi)\\,$.\n\n- For each test case below, compute $\\,A_{\\min}\\,$ and also verify that all simulated positions remain finite (no Not-a-Number or infinity values). A test case is declared robust if and only if $\\,A_{\\min} \\ge A_{\\mathrm{thr}}\\,$ and all values are finite. Your program must evaluate the following test suite and output a boolean per case:\n  1. Test A (smooth baseline): circle with $\\,N=256\\,$, $\\,\\Delta s = 0.01\\,$, $\\,M=20\\,$, fallback disabled (set $\\,\\tau=-1\\,$), and threshold $\\,A_{\\mathrm{thr}} = 10^{-4}\\,$.\n  2. Test B (corner kink, naive normals): square with $\\,N_{\\mathrm{edge}}=65\\,$, hence $\\,N=256\\,$, $\\,\\Delta s = 0.01\\,$, $\\,M=20\\,$, fallback disabled (set $\\,\\tau=-1\\,$), and threshold $\\,A_{\\mathrm{thr}} = 10^{-4}\\,$.\n  3. Test C (corner kink, robust normals with fallback): same square and discretization as Test B but with fallback enabled using $\\,\\tau = 0.05\\,$, and threshold $\\,A_{\\mathrm{thr}} = 10^{-4}\\,$.\n\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[True,False,True]\"), in the order [Test A, Test B, Test C]. No additional text should be printed.\n\nAll angles are in radians. No physical units are involved. All outputs must be booleans as specified.", "solution": "The problem requires the implementation and validation of a two-dimensional hyperbolic grid generation algorithm. The core of this method is the iterative advancement of grid points from an initial boundary curve into the domain's interior. This process is governed by a first-order hyperbolic partial differential equation (PDE), which we will discretize and solve numerically. The primary objective is to analyze the robustness of this method, particularly how it handles geometric singularities like sharp corners, by comparing a naive implementation with one that includes a fallback mechanism for computing normal vectors.\n\nThe governing PDE for the grid point positions $\\mathbf{x}$ as a function of a marching coordinate $s$ is given as $\\partial_s \\mathbf{x} = a(\\mathbf{x})\\,\\mathbf{n}(\\mathbf{x})$. For this problem, the speed function $a(\\mathbf{x})$ is taken to be constant, $a(\\mathbf{x}) \\equiv 1$, simplifying the equation to:\n$$\n\\partial_s \\mathbf{x} = \\mathbf{n}(\\mathbf{x})\n$$\nThe unit normal field $\\mathbf{n}(\\mathbf{x})$ is derived from the gradient of a level-set function $\\phi(\\mathbf{x})$, such that $\\mathbf{n}(\\mathbf{x}) = \\nabla \\phi / \\lVert \\nabla \\phi \\rVert$.\n\nWe will discretize this PDE in both space and the marching direction. The boundary curve $\\Gamma(\\xi)$ is sampled at $N$ discrete points $\\{\\xi_j\\}_{j=0}^{N-1}$, forming the initial grid layer $\\mathbf{x}^{(0)}_j = \\Gamma(\\xi_j)$. Subsequent grid layers are generated by marching forward in $s$ using an explicit forward Euler scheme with a step size $\\Delta s > 0$. The update rule for a grid point $\\mathbf{x}^{(k)}_j$ at layer $k$ and boundary index $j$ is:\n$$\n\\mathbf{x}^{(k+1)}_j = \\mathbf{x}^{(k)}_j + \\Delta s \\,\\mathbf{n}_{\\mathrm{reg}}\\!\\left(\\mathbf{x}^{(k)}_j;\\,\\xi_j\\right)\n$$\nThis process is repeated for $M$ steps, generating $M$ new grid layers.\n\nA crucial component is the definition of the regularized normal vector, $\\mathbf{n}_{\\mathrm{reg}}$. The naive approach of using the level-set gradient can fail where the gradient vanishes, such as at sharp corners represented by kinks in the level-set function. To address this, a robust method is introduced. Let $g = \\lVert \\nabla \\phi(\\mathbf{x}) \\rVert$ be the magnitude of the level-set gradient.\n- If $g$ is greater than a specified threshold $\\tau \\ge 0$, the normal is computed from the level set: $\\mathbf{n}_{\\mathrm{reg}} = \\nabla \\phi / g$.\n- If $g \\le \\tau$, the algorithm falls back to a pre-computed geometric normal, $\\mathbf{n}_{\\mathrm{geo}}(\\xi_j)$, which is derived directly from the geometry of the initial boundary curve $\\Gamma$. This geometric normal is obtained by rotating the boundary's unit tangent vector $\\mathbf{t}(\\xi_j)$ by $+\\pi/2$ radians, ensuring it points inward for a counterclockwise-parameterized boundary.\n- A special case is defined for when the fallback is disabled, indicated by $\\tau < 0$. In this scenario, no fallback is permitted. If the gradient magnitude $g$ becomes zero, the normal vector must be set to zero, $\\mathbf{n}_{\\mathrm{reg}} = \\mathbf{0}$, which will halt the motion of that specific grid point.\n\nTo evaluate the quality and robustness of the generated grid, we compute a discrete measure of the local cell area. For each cell defined by the points $(\\mathbf{x}^{(k)}_j, \\mathbf{x}^{(k)}_{j+1}, \\mathbf{x}^{(k+1)}_{j+1}, \\mathbf{x}^{(k+1)}_j)$, we define two vectors corresponding to the sides of the quadrilateral:\n$$\n\\Delta_s \\mathbf{x}^{(k)}_j = \\mathbf{x}^{(k+1)}_j - \\mathbf{x}^{(k)}_j \\quad (\\text{march direction})\n$$\n$$\n\\Delta_\\xi \\mathbf{x}^{(k)}_j = \\mathbf{x}^{(k)}_{j+1\\,\\mathrm{mod}\\,N} - \\mathbf{x}^{(k)}_j \\quad (\\text{along-boundary direction})\n$$\nThe oriented area of this cell, $A^{(k)}_j$, is given by the $z$-component of the cross product of these two vectors in the $xy$-plane:\n$$\nA^{(k)}_j = \\big(\\Delta_s \\mathbf{x}^{(k)}_j\\big)_x \\,\\big(\\Delta_\\xi \\mathbf{x}^{(k)}_j\\big)_y - \\big(\\Delta_s \\mathbf{x}^{(k)}_j\\big)_y \\,\\big(\\Delta_\\xi \\mathbf{x}^{(k)}_j\\big)_x\n$$\nA grid is considered to be of poor quality if its cells collapse or invert, which corresponds to this area magnitude approaching zero. The metric for overall grid robustness is the minimum absolute area over all cells in the entire grid:\n$$\nA_{\\min} = \\min_{k,j} |A^{(k)}_j|\n$$\nA test case is deemed robust if and only if $A_{\\min}$ is greater than or equal to a given threshold $A_{\\mathrm{thr}}$, and all computed grid point coordinates are finite numbers.\n\nThe implementation will cover two geometries:\n1.  **Circle**: The boundary is a unit circle $\\Gamma_{\\mathrm{circ}}(\\xi) = (\\cos(2\\pi\\xi), \\sin(2\\pi\\xi))$ and the level set is $\\phi_{\\mathrm{circ}}(x,y) = 1 - \\sqrt{x^2+y^2}$. Its gradient, $\\nabla\\phi_{\\mathrm{circ}} = \\frac{-1}{\\sqrt{x^2+y^2}}(x,y)$, is well-behaved (non-zero and smooth) away from the origin.\n2.  **Square**: The boundary is the perimeter of the square $[-1,1]\\times[-1,1]$. The level set is $\\phi_{\\square}(x,y) = (|x|-1)(|y|-1)$, whose gradient is $\\nabla\\phi_{\\square} = (\\mathrm{sgn}(x)(|y|-1), \\mathrm{sgn}(y)(|x|-1))$. This gradient vanishes at the four corners $(\\pm 1, \\pm 1)$, modeling the pathological case. For the square's geometric normals, the boundary tangent vector is approximated using periodic central differences on the discretized boundary points.\n\nWe will execute three test cases:\n- **Test A**: A smooth baseline using the circle boundary, with the fallback mechanism disabled ($\\tau = -1$). We expect this to be robust as there are no inherent geometric or level-set singularities.\n- **Test B**: The square boundary with its singular level set, also with the fallback disabled ($\\tau = -1$). We anticipate failure, as the vanishing gradient at the corners will cause $\\mathbf{n}_{\\mathrm{reg}}=\\mathbf{0}$, leading to grid cell collapse and $A_{\\min} < A_{\\mathrm{thr}}$.\n- **Test C**: The same square configuration as Test B, but with the robust fallback mechanism enabled ($\\tau = 0.05$). The algorithm should now use the well-defined geometric normals near the corners, preventing cell collapse and resulting in a robust grid with $A_{\\min} \\ge A_{\\mathrm{thr}}$.", "answer": "```python\nimport numpy as np\n\ndef grad_phi_circ(x: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Computes the gradient of the circle level-set function.\n    phi(x,y) = 1 - sqrt(x^2 + y^2)\n    grad_phi = -(x,y) / sqrt(x^2 + y^2)\n    \"\"\"\n    norm = np.linalg.norm(x, axis=1, keepdims=True)\n    # Avoid division by zero at the origin, though it's not on the boundary.\n    norm[norm == 0] = 1e-16\n    return -x / norm\n\ndef grad_phi_square(x: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Computes the gradient of the square level-set function.\n    phi(x,y) = (|x|-1)(|y|-1)\n    grad_phi = (sgn(x)(|y|-1), sgn(y)(|x|-1))\n    \"\"\"\n    sx = np.sign(x[:, 0])\n    sy = np.sign(x[:, 1])\n    abs_x = np.abs(x[:, 0])\n    abs_y = np.abs(x[:, 1])\n    return np.column_stack([sx * (abs_y - 1), sy * (abs_x - 1)])\n\ndef run_test(shape: str, N_param: int, ds: float, M: int, tau: float, A_thr: float) -> bool:\n    \"\"\"\n    Runs a single hyperbolic grid generation test case.\n    \"\"\"\n    if shape == 'circle':\n        N = N_param\n        xi = np.linspace(0, 1, N, endpoint=False)\n        x0 = np.column_stack([np.cos(2 * np.pi * xi), np.sin(2 * np.pi * xi)])\n        grad_phi_func = grad_phi_circ\n        n_geo = None\n    elif shape == 'square':\n        N_edge = N_param\n        N = 4 * N_edge - 4\n        \n        y_right = np.linspace(1, -1, N_edge)\n        x_top = np.linspace(1, -1, N_edge)[1:]\n        y_left = np.linspace(-1, 1, N_edge)[1:]\n        x_bottom = np.linspace(-1, 1, N_edge)[1:-1]\n        \n        pts_right = np.column_stack([np.ones_like(y_right), y_right])\n        pts_top = np.column_stack([x_top, np.ones_like(x_top)])\n        pts_left = np.column_stack([-np.ones_like(y_left), y_left])\n        pts_bottom = np.column_stack([x_bottom, -np.ones_like(x_bottom)])\n        \n        x0 = np.concatenate([pts_right, pts_top, pts_left, pts_bottom], axis=0)\n        grad_phi_func = grad_phi_square\n        \n        # Pre-compute geometric normals for fallback\n        t = np.roll(x0, -1, axis=0) - np.roll(x0, 1, axis=0)\n        norm_t = np.linalg.norm(t, axis=1, keepdims=True)\n        # Avoid division by zero, though unlikely for a non-degenerate polygon\n        norm_t[norm_t == 0] = 1.0\n        unit_t = t / norm_t\n        # Rotate tangent by +pi/2 to get inward normal: (tx, ty) -> (-ty, tx)\n        n_geo = np.column_stack([-unit_t[:, 1], unit_t[:, 0]])\n    else:\n        raise ValueError(\"Invalid shape specified\")\n\n    x_k = x0.copy()\n    all_points = [x0]\n    all_areas = []\n\n    for _ in range(M):\n        grad = grad_phi_func(x_k)\n        g = np.linalg.norm(grad, axis=1)\n        \n        n_reg = np.zeros_like(x_k)\n\n        # Fallback disabled case (tau < 0)\n        if tau < 0:\n            mask_g_pos = g > 0\n            # Use level-set normal where g > 0, otherwise remains 0\n            n_reg[mask_g_pos] = grad[mask_g_pos] / g[mask_g_pos, np.newaxis]\n        # Fallback enabled case (tau >= 0)\n        else:\n            mask_phi = g > tau\n            mask_geo = ~mask_phi\n            \n            # Use level-set normal where g > tau\n            if np.any(mask_phi):\n                n_reg[mask_phi] = grad[mask_phi] / g[mask_phi, np.newaxis]\n            \n            # Use geometric normal where g <= tau (fallback)\n            if np.any(mask_geo):\n                n_reg[mask_geo] = n_geo[mask_geo]\n\n        # Forward Euler step\n        x_kplus1 = x_k + ds * n_reg\n        \n        # Calculate cell areas using current layer and next\n        delta_s_x = x_kplus1 - x_k\n        delta_xi_x = np.roll(x_k, -1, axis=0) - x_k\n        \n        # 2D cross product: a_x * b_y - a_y * b_x\n        areas_k = delta_s_x[:, 0] * delta_xi_x[:, 1] - delta_s_x[:, 1] * delta_xi_x[:, 0]\n        all_areas.append(np.abs(areas_k))\n        \n        x_k = x_kplus1\n        all_points.append(x_k)\n        \n    all_points = np.array(all_points)\n    all_areas = np.concatenate(all_areas)\n    \n    A_min = np.min(all_areas)\n    all_finite = np.all(np.isfinite(all_points))\n    \n    is_robust = all_finite and (A_min >= A_thr)\n    return is_robust\n\ndef solve():\n    \"\"\"\n    Defines and runs the test suite.\n    \"\"\"\n    test_cases = [\n        # Test A: Circle, smooth, no fallback\n        {'shape': 'circle', 'N_param': 256, 'ds': 0.01, 'M': 20, 'tau': -1.0, 'A_thr': 1e-4},\n        # Test B: Square, kinks, no fallback\n        {'shape': 'square', 'N_param': 65, 'ds': 0.01, 'M': 20, 'tau': -1.0, 'A_thr': 1e-4},\n        # Test C: Square, kinks, fallback enabled\n        {'shape': 'square', 'N_param': 65, 'ds': 0.01, 'M': 20, 'tau': 0.05, 'A_thr': 1e-4},\n    ]\n\n    results = []\n    for case in test_cases:\n        result = run_test(**case)\n        results.append(result)\n\n    # Format output exactly as specified\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3332127"}, {"introduction": "Beyond simple orthogonal extrusion, advanced hyperbolic methods offer powerful control over the grid's internal structure. This practice [@problem_id:3332135] introduces a sophisticated technique that uses the Hessian of a background field to steer grid lines, enabling precise control over cell anisotropy and uniformity. Implementing this method will demonstrate how to move beyond default marching and actively engineer a grid for superior quality, a crucial skill for advanced CFD analysis.", "problem": "Consider a two-dimensional hyperbolic grid generation process in Computational Fluid Dynamics (CFD), where a boundary-fitted grid patch is constructed by marching interior grid points from an initial boundary curve along a prescribed direction field. Let the mapping be $\\mathbf{x}(u,s) \\in \\mathbb{R}^2$ with parameters $u \\in [0,1]$ (boundary curve parameter) and pseudo-time $s \\ge 0$ (marching parameter). The marching is governed by the first-order Partial Differential Equation (PDE) $\\partial_s \\mathbf{x}(u,s) = \\alpha\\,\\mathbf{d}(\\mathbf{x}(u,s),u)$, where $\\alpha > 0$ is a constant step length (units of length per unit pseudo-time). Two marching direction models are to be compared: (i) Hessian-based anisotropy and (ii) boundary normal-only marching.\n\nFundamental definitions for this problem:\n- The boundary curve is given by $\\boldsymbol{\\Gamma}(u) = (x(u),y(u))$, with $x(u) = u$ and $y(u) = y_0 + A\\sin(2\\pi u)$. The unit tangent is $\\mathbf{t}(u) = \\frac{\\partial \\boldsymbol{\\Gamma}/\\partial u}{\\|\\partial \\boldsymbol{\\Gamma}/\\partial u\\|}$, and the chosen outward unit normal is $\\mathbf{n}(u) = (-t_y(u),\\,t_x(u))$, flipped if necessary so that $n_y(u) \\ge 0$.\n- Given a twice differentiable scalar field $\\phi(x,y)$, the Hessian is $H = \\nabla\\nabla\\phi \\in \\mathbb{R}^{2\\times 2}$. For Hessian-based anisotropy, the marching direction is the principal eigenvector corresponding to the largest eigenvalue: $\\mathbf{d}_\\text{aniso}(\\mathbf{x},u) = \\mathbf{v}_{\\max}(H(\\mathbf{x}))$, oriented to satisfy $\\mathbf{d}_\\text{aniso}\\cdot\\mathbf{n}(u) \\ge 0$. In the isotropic case where the eigenvalues of $H$ are equal within a small tolerance, set $\\mathbf{d}_\\text{aniso}(\\mathbf{x},u) = \\mathbf{n}(u)$.\n- For boundary normal-only marching, $\\mathbf{d}_\\text{normal}(\\mathbf{x},u) = \\mathbf{n}(u)$ is used for all $\\mathbf{x}$ along the streamline associated with parameter $u$.\n\nLet the discrete grid be defined on $(u_i,s_j)$ with $u_i = \\frac{i}{M-1}$ for $i=0,1,\\ldots,M-1$, and $s_j = j\\,\\Delta s$ with constant step $\\Delta s = \\alpha$ for $j=0,1,\\ldots,N$. The initial condition is $\\mathbf{x}(u_i,0) = \\boldsymbol{\\Gamma}(u_i)$. The marching update is the explicit scheme $\\mathbf{x}(u_i,s_{j+1}) = \\mathbf{x}(u_i,s_j) + \\alpha\\,\\mathbf{d}(\\mathbf{x}(u_i,s_j),u_i)$, with $\\mathbf{d} = \\mathbf{d}_\\text{aniso}$ or $\\mathbf{d}_\\text{normal}$, respectively.\n\nGrid quality is evaluated using the Jacobian determinant (J) of the mapping, approximated discretely by the cell area of the parallelogram spanned by forward differences $\\Delta_u \\mathbf{x}$ and $\\Delta_s \\mathbf{x}$:\n$$\nJ_{i,j} \\approx \\left| \\det\\begin{pmatrix}\nx_{i+1,j}-x_{i,j} & x_{i,j+1}-x_{i,j} \\\\\ny_{i+1,j}-y_{i,j} & y_{i,j+1}-y_{i,j}\n\\end{pmatrix} \\right| = \\left| (x_{i+1,j}-x_{i,j})(y_{i,j+1}-y_{i,j}) - (y_{i+1,j}-y_{i,j})(x_{i,j+1}-x_{i,j}) \\right|.\n$$\nUniformity is quantified by the Coefficient of Variation (CoV), defined as $\\mathrm{CoV} = \\frac{\\sigma(J)}{\\mu(J)}$, where $\\sigma(J)$ and $\\mu(J)$ are the standard deviation and mean of the set $\\{J_{i,j}\\}$ over all valid $(i,j)$ cells.\n\nYour task is to implement this grid generation and quality evaluation for the following test suite, and to compare the uniformity of the Jacobian between Hessian-based anisotropy and normal-only marching by reporting the ratio $R = \\frac{\\mathrm{CoV}_\\text{normal}}{\\mathrm{CoV}_\\text{aniso}}$ for each case.\n\nAll cases share:\n- Domain bounds are implicitly enforced by construction without explicit boundary reflection; parameters are chosen to keep trajectories within the unit square.\n- Parameters: $M=51$, $N=30$, $\\alpha=0.015$, $y_0=0.2$, $A=0.05$.\n\nTest suite (three cases):\n1. Case 1 (constant, anisotropic Hessian): $\\phi(x,y) = a\\,x^2 + b\\,y^2 + c\\,x\\,y$ with $a=0.5$, $b=4.0$, $c=0.2$, so $H = \\begin{pmatrix}2a & c\\\\ c & 2b\\end{pmatrix}$ is constant and positive definite.\n2. Case 2 (spatially varying anisotropic Hessian): $\\phi(x,y) = x^2 + y^2 + \\frac{1}{2} x\\,y^2$, so $H(x,y) = \\begin{pmatrix}2 & y\\\\ y & 2 + x\\end{pmatrix}$, which is symmetric and positive definite for $(x,y) \\in [0,1]^2$.\n3. Case 3 (isotropic Hessian): $\\phi(x,y) = x^2 + y^2$, so $H = \\begin{pmatrix}2 & 0\\\\ 0 & 2\\end{pmatrix}$ everywhere, implying equal eigenvalues and the anisotropy rule falls back to $\\mathbf{n}(u)$.\n\nFor each case, compute the grids for both marching strategies, assemble the set $\\{J_{i,j}\\}$ over $i=0,\\ldots,M-2$ and $j=0,\\ldots,N-1$, compute $\\mathrm{CoV}_\\text{aniso}$ and $\\mathrm{CoV}_\\text{normal}$, and return $R$ as a float.\n\nFinal output format:\n- Your program should produce a single line of output containing the results for the three cases as a comma-separated list enclosed in square brackets (e.g., \"[r1,r2,r3]\"), where each $r_k$ is the computed float $R$ for Case $k$, in arbitrary floating-point representation. No units are required and no angles appear.", "solution": "The problem is well-defined, scientifically sound, and provides all necessary information to proceed with a solution. The task is to implement and compare two hyperbolic grid generation methods in a two-dimensional space. The comparison is based on the uniformity of the resulting grid cells, quantified by the Coefficient of Variation (CoV) of the cell Jacobians.\n\nThe algorithmic steps to solve this problem are as follows:\n\n1.  **Discretization of the Initial Boundary**\n    The process begins with the initial boundary curve, $\\boldsymbol{\\Gamma}(u) \\in \\mathbb{R}^2$, defined for a parameter $u \\in [0,1]$. The curve is given by $\\boldsymbol{\\Gamma}(u) = (x(u), y(u))$ where $x(u) = u$ and $y(u) = y_0 + A\\sin(2\\pi u)$. This continuous curve is discretized into $M$ points, $\\mathbf{x}_{i,0}$, corresponding to $u_i = i/(M-1)$ for $i=0, 1, \\ldots, M-1$.\n    $$\n    \\mathbf{x}_{i,0} = \\boldsymbol{\\Gamma}(u_i) = \\left( u_i, y_0 + A\\sin(2\\pi u_i) \\right)\n    $$\n    For each point on this initial curve, we must compute the outward unit normal vector, $\\mathbf{n}(u_i)$. This is essential for both marching strategies. First, the tangent vector to the curve is found by differentiation:\n    $$\n    \\frac{\\partial\\boldsymbol{\\Gamma}}{\\partial u} = \\left( \\frac{dx}{du}, \\frac{dy}{du} \\right) = (1, 2\\pi A \\cos(2\\pi u))\n    $$\n    The unit tangent vector, $\\mathbf{t}(u)$, is this vector normalized to unit length:\n    $$\n    \\mathbf{t}(u) = \\frac{\\partial\\boldsymbol{\\Gamma}/\\partial u}{\\|\\partial\\boldsymbol{\\Gamma}/\\partial u\\|} = \\frac{(1, 2\\pi A \\cos(2\\pi u))}{\\sqrt{1^2 + (2\\pi A \\cos(2\\pi u))^2}} = (t_x(u), t_y(u))\n    $$\n    The unit normal vector, $\\mathbf{n}(u)$, is obtained by rotating the unit tangent vector by $90^\\circ$. The problem specifies the choice $\\mathbf{n}(u) = (-t_y(u), t_x(u))$, which must be oriented such that its $y$-component is non-negative. The $y$-component of this normal is $t_x(u)$, which is $1 / \\|\\partial\\boldsymbol{\\Gamma}/\\partial u\\|$. Since the norm is always positive, $t_x(u) > 0$, so this choice of normal inherently satisfies the orientation constraint. These $M$ normal vectors, $\\mathbf{n}_i = \\mathbf{n}(u_i)$, are computed and stored.\n\n2.  **Grid Marching via Explicit Euler Scheme**\n    The grid is generated by marching away from the initial boundary. The grid points $\\mathbf{x}(u,s)$ are a function of the boundary parameter $u$ and the marching parameter $s$. The evolution of the grid is governed by the hyperbolic Partial Differential Equation (PDE) $\\partial_s \\mathbf{x} = \\alpha \\mathbf{d}$, where $\\mathbf{d}$ is the local marching direction.\n    We discretize this PDE in $s$ using a simple forward Euler scheme with a constant step size $\\Delta s = \\alpha$. The grid at step $j+1$ is calculated from the grid at step $j$ as:\n    $$\n    \\mathbf{x}(u_i, s_{j+1}) = \\mathbf{x}(u_i, s_j) + \\alpha\\,\\mathbf{d}(\\mathbf{x}(u_i,s_j), u_i)\n    $$\n    This update is performed for $j=0, \\ldots, N-1$, generating a grid of size $M \\times (N+1)$ points.\n\n3.  **Marching Direction Models**\n    Two models for the direction field $\\mathbf{d}$ are implemented and compared.\n\n    -   **Boundary Normal-Only Marching ($\\mathbf{d}_\\text{normal}$)**: This is the simpler model. The marching direction for any point on a streamline originating from $\\boldsymbol{\\Gamma}(u_i)$ is constant and equal to the initial normal vector at that boundary point.\n        $$\n        \\mathbf{d}_\\text{normal}(\\mathbf{x}, u_i) = \\mathbf{n}(u_i)\n        $$\n        Since a streamline is defined by a constant $u_i$, the direction $\\mathbf{d}$ is constant for all marching steps $j$ along that streamline.\n\n    -   **Hessian-Based Anisotropic Marching ($\\mathbf{d}_\\text{aniso}$)**: This model adapts the marching direction to a background scalar field, $\\phi(x,y)$, to control grid properties like cell stretching and orientation. The direction is determined by the Hessian matrix of this field, $H = \\nabla\\nabla\\phi$.\n        $$\n        H(\\mathbf{x}) = \\begin{pmatrix} \\partial_{xx}\\phi & \\partial_{xy}\\phi \\\\ \\partial_{yx}\\phi & \\partial_{yy}\\phi \\end{pmatrix}\n        $$\n        For each point $\\mathbf{x}_{i,j}$ at each marching step $j$, the marching direction $\\mathbf{d}_\\text{aniso}$ is chosen as the principal eigenvector $\\mathbf{v}_{\\max}$ of the Hessian matrix $H(\\mathbf{x}_{i,j})$, corresponding to its largest eigenvalue $\\lambda_{\\max}$. This eigenvector indicates the direction of maximum curvature of the field $\\phi$. The computed eigenvector is then oriented to point \"outward\" by ensuring its dot product with the initial normal $\\mathbf{n}(u_i)$ is non-negative: if $\\mathbf{v}_{\\max} \\cdot \\mathbf{n}(u_i) < 0$, the direction is flipped to $-\\mathbf{v}_{\\max}$.\n        A special condition handles isotropic cases. If the eigenvalues of $H$ are equal within a small numerical tolerance (i.e., $|\\lambda_{\\max} - \\lambda_{\\min}| < \\epsilon$), the field is considered isotropic at that point, and the marching direction defaults to the boundary normal, $\\mathbf{d}_\\text{aniso} = \\mathbf{n}(u_i)$.\n\n4.  **Grid Quality Evaluation**\n    Once a grid is generated, its quality is assessed. The uniformity of the grid cells is a key metric. This is evaluated using the Jacobian determinant, $J$, of the coordinate transformation $(u,s) \\to (x,y)$. We use a finite difference approximation for $J$ at each cell $(i,j)$, which corresponds to the area of the parallelogram formed by the vectors $\\Delta_u \\mathbf{x} = \\mathbf{x}_{i+1,j} - \\mathbf{x}_{i,j}$ and $\\Delta_s \\mathbf{x} = \\mathbf{x}_{i,j+1} - \\mathbf{x}_{i,j}$.\n    $$\n    J_{i,j} \\approx \\left| \\det(\\Delta_u \\mathbf{x}, \\Delta_s \\mathbf{x}) \\right| = \\left| (x_{i+1,j}-x_{i,j})(y_{i,j+1}-y_{i,j}) - (y_{i+1,j}-y_{i,j})(x_{i,j+1}-x_{i,j}) \\right|\n    $$\n    The set of all such cell areas $\\{J_{i,j}\\}$ for $i \\in [0, M-2]$ and $j \\in [0, N-1]$ is assembled.\n    To quantify the uniformity of these areas, the Coefficient of Variation (CoV) is calculated. It is the ratio of the standard deviation $\\sigma(J)$ to the mean $\\mu(J)$ of the set of Jacobians.\n    $$\n    \\mathrm{CoV} = \\frac{\\sigma(J)}{\\mu(J)}\n    $$\n    A lower CoV indicates a more uniform grid, where cell areas are more consistent.\n\n5.  **Comparison and Final Output**\n    For each of the three test cases, we generate two grids: one using $\\mathbf{d}_\\text{normal}$ and one using $\\mathbf{d}_\\text{aniso}$. We then compute their respective uniformity metrics, $\\mathrm{CoV}_\\text{normal}$ and $\\mathrm{CoV}_\\text{aniso}$. The final result for each case is the ratio:\n    $$\n    R = \\frac{\\mathrm{CoV}_\\text{normal}}{\\mathrm{CoV}_\\text{aniso}}\n    $$\n    This ratio quantifies the relative improvement in grid uniformity offered by the Hessian-based method. A value $R > 1.0$ indicates that the anisotropic method produced a grid with lower CoV (i.e., better uniformity) than the simple normal-marching method. For Case 3, the Hessian is isotropic everywhere, so the anisotropic method is designed to default to the normal-only method. Therefore, we expect $\\mathrm{CoV}_\\text{normal} = \\mathrm{CoV}_\\text{aniso}$, leading to $R=1.0$, which serves as a validation of the implementation.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the hyperbolic grid generation problem for three test cases.\n    \"\"\"\n    # Shared parameters\n    M = 51\n    N = 30\n    alpha = 0.015\n    y0 = 0.2\n    A = 0.05\n    \n    # Isotropic check tolerance\n    EIG_TOL = 1e-9\n\n    def get_hessian_case1(x, y):\n        a, b, c = 0.5, 4.0, 0.2\n        return np.array([[2.0 * a, c], [c, 2.0 * b]])\n\n    def get_hessian_case2(x, y):\n        return np.array([[2.0, y], [y, 2.0 + x]])\n\n    def get_hessian_case3(x, y):\n        return np.array([[2.0, 0.0], [0.0, 2.0]])\n\n    def compute_cov(grid):\n        \"\"\"\n        Computes the Coefficient of Variation (CoV) of the Jacobian for a given grid.\n        \"\"\"\n        # Vectorized computation of Jacobians (cell areas)\n        # delta_u_x: x_{i+1,j} - x_{i,j}\n        # delta_u_y: y_{i+1,j} - y_{i,j}\n        delta_u_x = grid[1:, :-1, 0] - grid[:-1, :-1, 0]\n        delta_u_y = grid[1:, :-1, 1] - grid[:-1, :-1, 1]\n        \n        # delta_s_x: x_{i,j+1} - x_{i,j}\n        # delta_s_y: y_{i,j+1} - y_{i,j}\n        delta_s_x = grid[:-1, 1:, 0] - grid[:-1, :-1, 0]\n        delta_s_y = grid[:-1, 1:, 1] - grid[:-1, :-1, 1]\n\n        jacobians = np.abs(delta_u_x * delta_s_y - delta_u_y * delta_s_x)\n        \n        if jacobians.size == 0:\n            return 0.0\n            \n        mean_j = np.mean(jacobians)\n        std_j = np.std(jacobians)\n\n        if mean_j == 0:\n            return 0.0 \n\n        return std_j / mean_j\n\n    def generate_grid(u_points, initial_normals, hessian_func=None):\n        \"\"\"\n        Generates a grid using either normal-only or Hessian-based marching.\n        \"\"\"\n        grid = np.zeros((M, N + 1, 2))\n        \n        # Initial condition: set grid at s=0 to the boundary curve\n        grid[:, 0, 0] = u_points\n        grid[:, 0, 1] = y0 + A * np.sin(2 * np.pi * u_points)\n\n        # Marching loop\n        for j in range(N):\n            if hessian_func is None:  # Normal-only marching\n                directions = initial_normals\n                grid[:, j + 1, :] = grid[:, j, :] + alpha * directions\n            else:  # Hessian-based anisotropic marching\n                for i in range(M):\n                    pt = grid[i, j, :]\n                    x, y = pt[0], pt[1]\n                    \n                    H = hessian_func(x, y)\n                    eigvals, eigvecs = np.linalg.eigh(H)\n                    \n                    # Check for isotropy\n                    if np.abs(eigvals[1] - eigvals[0]) < EIG_TOL:\n                        direction = initial_normals[i]\n                    else:\n                        # Principal eigenvector (for largest eigenvalue)\n                        direction = eigvecs[:, 1]\n                        \n                        # Orient the direction vector\n                        if np.dot(direction, initial_normals[i]) < 0:\n                            direction = -direction\n                    \n                    grid[i, j + 1, :] = grid[i, j, :] + alpha * direction\n        \n        return grid\n\n    # Initial boundary discretization\n    u_i = np.linspace(0, 1, M)\n    \n    # Calculate initial tangent vectors\n    dx_du = np.ones_like(u_i)\n    dy_du = 2 * np.pi * A * np.cos(2 * np.pi * u_i)\n    \n    # Calculate initial unit normal vectors\n    tangent_norms = np.sqrt(dx_du**2 + dy_du**2)\n    t_x = dx_du / tangent_norms\n    t_y = dy_du / tangent_norms\n    \n    # n = (-t_y, t_x). n_y = t_x = 1/norm > 0, so orientation is correct.\n    normals = np.vstack((-t_y, t_x)).T\n\n    test_cases = [get_hessian_case1, get_hessian_case2, get_hessian_case3]\n    results = []\n\n    for hessian_func in test_cases:\n        # Generate grid with normal-only marching\n        grid_normal = generate_grid(u_i, normals, hessian_func=None)\n        cov_normal = compute_cov(grid_normal)\n\n        # Generate grid with Hessian-based marching\n        grid_aniso = generate_grid(u_i, normals, hessian_func=hessian_func)\n        cov_aniso = compute_cov(grid_aniso)\n\n        if cov_aniso == 0:\n             # Should not happen in these test cases, but robust for division\n            ratio = 1.0 if cov_normal == 0 else float('inf')\n        else:\n            ratio = cov_normal / cov_aniso\n        \n        results.append(ratio)\n\n    # Format the final output string\n    print(f\"[{','.join(f'{r:.15f}'.rstrip('0').rstrip('.') for r in results)}]\")\n\nsolve()\n```", "id": "3332135"}, {"introduction": "A grid's quality is fundamentally tied to the boundary from which it is generated, but real-world boundaries always have imperfections. This theoretical practice [@problem_id:3332114] delves into the stability of the hyperbolic marching process by using perturbation analysis to track how small, random errors on the initial surface propagate into the volume mesh. By deriving the variance of the grid Jacobian, you will gain a deeper, analytical understanding of the method's sensitivity to input uncertainty.", "problem": "Consider a hyperbolic grid generation approach for volume meshes in Computational Fluid Dynamics (CFD), defined by a mapping $\\mathbf{r}(\\xi,\\eta,s) \\in \\mathbb{R}^{3}$ from computational coordinates $(\\xi,\\eta,s)$ to physical space. The grid is advanced hyperbolically from an initial surface $\\Gamma(\\xi,\\eta)$ located at $s=0$ by marching along the local normal according to the first-order system\n$$\n\\frac{\\partial \\mathbf{r}}{\\partial s}(\\xi,\\eta,s) \\;=\\; \\mathbf{n}(\\xi,\\eta,s), \n\\qquad \n\\mathbf{n} \\;=\\; \\frac{\\mathbf{r}_{\\xi} \\times \\mathbf{r}_{\\eta}}{\\|\\mathbf{r}_{\\xi} \\times \\mathbf{r}_{\\eta}\\|},\n$$\nwith initial condition $\\mathbf{r}(\\xi,\\eta,0) = \\Gamma(\\xi,\\eta)$. Take as the base initial surface the plane $\\Gamma(\\xi,\\eta) = (\\xi,\\eta,0)^{\\top}$ so that the base solution is $\\mathbf{r}_{0}(\\xi,\\eta,s) = (\\xi,\\eta,s)^{\\top}$.\n\nNow introduce an uncertain boundary perturbation of amplitude $\\varepsilon$ so that the marched grid solves the same hyperbolic system with the perturbed initial condition\n$$\n\\Gamma_{\\varepsilon}(\\xi,\\eta) \\;=\\; \\Gamma(\\xi,\\eta) \\;+\\; \\varepsilon\\,\\boldsymbol{\\zeta}(\\xi,\\eta),\n$$\nwhere $\\boldsymbol{\\zeta}=(\\zeta_{x},\\zeta_{y},\\zeta_{z})^{\\top}$ is a mean-zero, stationary Gaussian random vector field on the parameter plane $(\\xi,\\eta)$ with independent components. Each component $\\zeta_{i}$ has isotropic Gaussian covariance\n$$\n\\mathbb{E}\\!\\left[\\zeta_{i}(\\xi,\\eta)\\,\\zeta_{i}(\\xi',\\eta')\\right] \\;=\\; \\sigma_{i}^{2}\\,\\exp\\!\\left(-\\frac{(\\xi-\\xi')^{2}+(\\eta-\\eta')^{2}}{2\\,\\ell_{i}^{2}}\\right),\n\\qquad i \\in \\{x,y,z\\},\n$$\nwith $\\sigma_{i}>0$ and correlation length $\\ell_{i}>0$. Define the volume Jacobian determinant\n$$\nJ(\\xi,\\eta,s) \\;=\\; \\det\\!\\big[\\mathbf{r}_{\\xi}(\\xi,\\eta,s),\\,\\mathbf{r}_{\\eta}(\\xi,\\eta,s),\\,\\mathbf{r}_{s}(\\xi,\\eta,s)\\big] \\;=\\; \\mathbf{r}_{\\xi}\\cdot\\big(\\mathbf{r}_{\\eta}\\times \\mathbf{r}_{s}\\big).\n$$\n\nBy linearizing the hyperbolic system about $\\varepsilon=0$ (i.e., about the base solution $\\mathbf{r}_{0}$) and keeping the leading nontrivial order in $\\varepsilon$, derive an explicit, closed-form analytic expression for the variance $\\mathrm{Var}\\!\\left[J(\\xi,\\eta,s)\\right]$ at a fixed $(\\xi,\\eta,s)$, expressed in terms of $\\varepsilon$, $s$, $\\sigma_{x}$, $\\sigma_{y}$, $\\sigma_{z}$, $\\ell_{x}$, $\\ell_{y}$, and $\\ell_{z}$. Your final answer must be a single closed-form analytical expression. No numerical evaluation is required.", "solution": "The problem is assessed to be valid as it is scientifically grounded in the principles of computational fluid dynamics and stochastic analysis, is well-posed, and is stated in objective, formal language. It is complete, consistent, and solvable using standard mathematical techniques.\n\nWe seek the variance of the volume Jacobian determinant, $\\mathrm{Var}[J]$, for a hyperbolic grid generation system under a stochastic initial condition. The solution is obtained by linearizing the governing equations around the unperturbed base solution.\n\nThe base solution corresponds to $\\varepsilon=0$. The initial surface is $\\mathbf{r}(\\xi,\\eta,0) = \\Gamma(\\xi,\\eta) = (\\xi, \\eta, 0)^\\top$. The base solution is given as $\\mathbf{r}_0(\\xi,\\eta,s) = (\\xi,\\eta,s)^\\top$. Let's verify this and establish the base state. The partial derivatives of $\\mathbf{r}_0$ are:\n$$\n\\mathbf{r}_{0,\\xi} = (1,0,0)^\\top, \\quad \\mathbf{r}_{0,\\eta} = (0,1,0)^\\top, \\quad \\mathbf{r}_{0,s} = (0,0,1)^\\top.\n$$\nThe normal vector to a surface of constant $s$ is:\n$$\n\\mathbf{n}_0 = \\frac{\\mathbf{r}_{0,\\xi} \\times \\mathbf{r}_{0,\\eta}}{\\|\\mathbf{r}_{0,\\xi} \\times \\mathbf{r}_{0,\\eta}\\|} = \\frac{(1,0,0)^\\top \\times (0,1,0)^\\top}{\\|(0,0,1)^\\top\\|} = (0,0,1)^\\top.\n$$\nThe governing equation $\\frac{\\partial \\mathbf{r}_0}{\\partial s} = \\mathbf{n}_0$ becomes $\\frac{\\partial \\mathbf{r}_0}{\\partial s} = (0,0,1)^\\top$. Integrating with respect to $s$ from the initial condition $\\mathbf{r}_0(\\xi,\\eta,0) = (\\xi,\\eta,0)^\\top$ yields $\\mathbf{r}_0(\\xi,\\eta,s) = (\\xi,\\eta,s)^\\top$, confirming the given base solution. The Jacobian of the base solution is $J_0 = \\mathbf{r}_{0,\\xi} \\cdot (\\mathbf{r}_{0,\\eta} \\times \\mathbf{r}_{0,s}) = (1,0,0)^\\top \\cdot ((0,1,0)^\\top \\times (0,0,1)^\\top) = 1$.\n\nNext, we introduce a first-order perturbation expansion in $\\varepsilon$:\n$$\n\\mathbf{r}(\\xi,\\eta,s) = \\mathbf{r}_0(\\xi,\\eta,s) + \\varepsilon \\mathbf{r}_1(\\xi,\\eta,s) + O(\\varepsilon^2).\n$$\nThe perturbed initial condition is $\\mathbf{r}(\\xi,\\eta,0) = \\Gamma(\\xi,\\eta) + \\varepsilon \\boldsymbol{\\zeta}(\\xi,\\eta)$. Substituting the expansion and equating terms of order $\\varepsilon^1$ gives the initial condition for the perturbation:\n$$\n\\mathbf{r}_1(\\xi,\\eta,0) = \\boldsymbol{\\zeta}(\\xi,\\eta) = (\\zeta_x, \\zeta_y, \\zeta_z)^\\top.\n$$\nWe now linearize the governing equation $\\frac{\\partial \\mathbf{r}}{\\partial s} = \\mathbf{n}$. The partial derivatives are $\\mathbf{r}_{\\xi} = \\mathbf{r}_{0,\\xi} + \\varepsilon \\mathbf{r}_{1,\\xi}$ and $\\mathbf{r}_{\\eta} = \\mathbf{r}_{0,\\eta} + \\varepsilon \\mathbf{r}_{1,\\eta}$. The normal vector $\\mathbf{n}$ is expanded to first order in $\\varepsilon$:\n$$\n\\mathbf{n} = \\mathbf{n}_0 + \\varepsilon \\mathbf{n}_1 + O(\\varepsilon^2).\n$$\nThe first-order perturbation to the normal, $\\mathbf{n}_1$, can be shown to be $\\mathbf{n}_1 = \\mathbf{C}_1 - \\mathbf{n}_0(\\mathbf{n}_0 \\cdot \\mathbf{C}_1)$, where $\\mathbf{C}_1 = \\mathbf{r}_{0,\\xi} \\times \\mathbf{r}_{1,\\eta} + \\mathbf{r}_{1,\\xi} \\times \\mathbf{r}_{0,\\eta}$.\nSubstituting the base vectors $\\mathbf{r}_{0,\\xi}=(1,0,0)^\\top$ and $\\mathbf{r}_{0,\\eta}=(0,1,0)^\\top$, and letting $\\mathbf{r}_1=(x_1, y_1, z_1)^\\top$:\n$$\n\\mathbf{C}_1 = (1,0,0)^\\top \\times (\\partial_\\eta x_1, \\partial_\\eta y_1, \\partial_\\eta z_1)^\\top + (\\partial_\\xi x_1, \\partial_\\xi y_1, \\partial_\\xi z_1)^\\top \\times (0,1,0)^\\top \\\\\n= (0, -\\partial_\\eta z_1, \\partial_\\eta y_1)^\\top + (-\\partial_\\xi z_1, 0, \\partial_\\xi x_1)^\\top = (-\\partial_\\xi z_1, -\\partial_\\eta z_1, \\partial_\\xi x_1 + \\partial_\\eta y_1)^\\top.\n$$\nWith $\\mathbf{n}_0 = (0,0,1)^\\top$, the dot product is $\\mathbf{n}_0 \\cdot \\mathbf{C}_1 = \\partial_\\xi x_1 + \\partial_\\eta y_1$.\nThe governing equation $\\frac{\\partial}{\\partial s}(\\mathbf{r}_0 + \\varepsilon \\mathbf{r}_1) = \\mathbf{n}_0 + \\varepsilon \\mathbf{n}_1 + O(\\varepsilon^2)$ gives, at order $\\varepsilon^1$:\n$$\n\\frac{\\partial \\mathbf{r}_1}{\\partial s} = \\mathbf{n}_1 = \\mathbf{C}_1 - \\mathbf{n}_0(\\mathbf{n}_0 \\cdot \\mathbf{C}_1) = (-\\partial_\\xi z_1, -\\partial_\\eta z_1, \\partial_\\xi x_1 + \\partial_\\eta y_1)^\\top - (0,0,1)^\\top(\\partial_\\xi x_1 + \\partial_\\eta y_1)\n$$\n$$\n\\frac{\\partial \\mathbf{r}_1}{\\partial s} = (-\\partial_\\xi z_1, -\\partial_\\eta z_1, 0)^\\top.\n$$\nThis yields a system of PDEs for the components of $\\mathbf{r}_1$:\n$$\n\\frac{\\partial x_1}{\\partial s} = -\\frac{\\partial z_1}{\\partial \\xi}, \\quad \\frac{\\partial y_1}{\\partial s} = -\\frac{\\partial z_1}{\\partial \\eta}, \\quad \\frac{\\partial z_1}{\\partial s} = 0.\n$$\nWith initial conditions $x_1(s=0)=\\zeta_x$, $y_1(s=0)=\\zeta_y$, $z_1(s=0)=\\zeta_z$, we integrate with respect to $s$. The third equation gives $z_1(\\xi,\\eta,s) = \\zeta_z(\\xi,\\eta)$. Substituting this into the first two equations and integrating gives:\n$$\nx_1(\\xi,\\eta,s) = \\zeta_x(\\xi,\\eta) - s \\frac{\\partial \\zeta_z}{\\partial \\xi}(\\xi,\\eta) \\\\\ny_1(\\xi,\\eta,s) = \\zeta_y(\\xi,\\eta) - s \\frac{\\partial \\zeta_z}{\\partial \\eta}(\\xi,\\eta)\n$$\nSo, the solution for the first-order perturbation is $\\mathbf{r}_1 = (\\zeta_x - s \\partial_\\xi \\zeta_z, \\zeta_y - s \\partial_\\eta \\zeta_z, \\zeta_z)^\\top$.\n\nNext, we linearize the Jacobian $J = \\mathbf{r}_{\\xi}\\cdot(\\mathbf{r}_{\\eta}\\times \\mathbf{r}_{s})$.\n$$\nJ \\approx J_0 + \\varepsilon J_1 = \\mathbf{r}_{0,\\xi}\\cdot(\\mathbf{r}_{0,\\eta}\\times \\mathbf{r}_{0,s}) + \\varepsilon \\left[ \\mathbf{r}_{1,\\xi}\\cdot(\\mathbf{r}_{0,\\eta}\\times \\mathbf{r}_{0,s}) + \\mathbf{r}_{0,\\xi}\\cdot(\\mathbf{r}_{1,\\eta}\\times \\mathbf{r}_{0,s}) + \\mathbf{r}_{0,\\xi}\\cdot(\\mathbf{r}_{0,\\eta}\\times \\mathbf{r}_{1,s}) \\right].\n$$\nUsing the identity $\\det[\\mathbf{a},\\mathbf{b},\\mathbf{c}]=\\mathbf{a}\\cdot(\\mathbf{b}\\times\\mathbf{c})$ and the fact that $(\\mathbf{r}_{0,\\xi}, \\mathbf{r}_{0,\\eta}, \\mathbf{r}_{0,s})$ is the standard Cartesian basis, we find $J_1 = \\partial_\\xi x_1 + \\partial_\\eta y_1 + \\partial_s z_1$.\nSubstituting the solution for $\\mathbf{r}_1$:\n$$\n\\partial_\\xi x_1 = \\partial_\\xi \\zeta_x - s \\partial_{\\xi\\xi} \\zeta_z \\\\\n\\partial_\\eta y_1 = \\partial_\\eta \\zeta_y - s \\partial_{\\eta\\eta} \\zeta_z \\\\\n\\partial_s z_1 = \\partial_s \\zeta_z = 0\n$$\nSo, $J_1 = \\partial_\\xi \\zeta_x + \\partial_\\eta \\zeta_y - s (\\partial_{\\xi\\xi} \\zeta_z + \\partial_{\\eta\\eta} \\zeta_z) = \\partial_\\xi \\zeta_x + \\partial_\\eta \\zeta_y - s \\nabla^2 \\zeta_z$, where $\\nabla^2 = \\partial_{\\xi\\xi} + \\partial_{\\eta\\eta}$.\nThe Jacobian is $J \\approx 1 + \\varepsilon (\\partial_\\xi\\zeta_x + \\partial_\\eta\\zeta_y - s \\nabla^2\\zeta_z)$.\n\nWe now compute the variance, $\\mathrm{Var}[J] = \\mathbb{E}[(J - \\mathbb{E}[J])^2]$. The field $\\boldsymbol{\\zeta}$ is mean-zero, so $\\mathbb{E}[\\zeta_i]=0$ and $\\mathbb{E}[\\partial_{\\dots}\\zeta_i]=0$. Thus, $\\mathbb{E}[J_1]=0$ and $\\mathbb{E}[J] \\approx 1$.\nThe variance to leading order is:\n$$\n\\mathrm{Var}[J] \\approx \\mathbb{E}[(1 + \\varepsilon J_1 - 1)^2] = \\varepsilon^2 \\mathbb{E}[J_1^2] = \\varepsilon^2 \\mathbb{E}[(\\partial_\\xi\\zeta_x + \\partial_\\eta\\zeta_y - s \\nabla^2\\zeta_z)^2].\n$$\nSince the components $\\zeta_x, \\zeta_y, \\zeta_z$ are statistically independent, the expectation of cross-terms involving different components is zero. For example, $\\mathbb{E}[(\\partial_\\xi\\zeta_x)(\\partial_\\eta\\zeta_y)] = \\mathbb{E}[\\partial_\\xi\\zeta_x]\\mathbb{E}[\\partial_\\eta\\zeta_y] = 0$.\nThe expression for the variance simplifies to:\n$$\n\\mathrm{Var}[J] \\approx \\varepsilon^2 \\left( \\mathbb{E}[(\\partial_\\xi \\zeta_x)^2] + \\mathbb{E}[(\\partial_\\eta \\zeta_y)^2] + s^2 \\mathbb{E}[(\\nabla^2 \\zeta_z)^2] \\right).\n$$\nWe need to compute the variances of the derivatives of the stationary Gaussian random fields. For a stationary field $\\zeta_i$ with covariance $C_i(\\xi, \\eta; \\xi', \\eta') = C_i(\\Delta\\xi, \\Delta\\eta)$, we have:\n$$\n\\mathbb{E}[(\\partial_\\xi \\zeta_i)^2] = \\left. \\frac{\\partial^2 C_i(\\xi, \\eta; \\xi', \\eta')}{\\partial\\xi \\partial\\xi'} \\right|_{\\xi'=\\xi, \\eta'=\\eta} = -\\left. \\frac{\\partial^2 C_i(\\Delta\\xi, \\Delta\\eta)}{\\partial(\\Delta\\xi)^2} \\right|_{\\Delta\\xi=0, \\Delta\\eta=0}.\n$$\nWith $C_i = \\sigma_i^2 \\exp(-\\frac{\\Delta\\xi^2+\\Delta\\eta^2}{2\\ell_i^2})$, we find $\\mathbb{E}[(\\partial_\\xi \\zeta_i)^2] = \\frac{\\sigma_i^2}{\\ell_i^2}$. By isotropy, $\\mathbb{E}[(\\partial_\\eta \\zeta_i)^2] = \\frac{\\sigma_i^2}{\\ell_i^2}$. Thus:\n$$\n\\mathbb{E}[(\\partial_\\xi \\zeta_x)^2] = \\frac{\\sigma_x^2}{\\ell_x^2} \\quad \\text{and} \\quad \\mathbb{E}[(\\partial_\\eta \\zeta_y)^2] = \\frac{\\sigma_y^2}{\\ell_y^2}.\n$$\nFor the last term, $\\mathbb{E}[(\\nabla^2 \\zeta_z)^2]$, we use spectral methods. The power spectral density $S_z(\\mathbf{k})$ is the Fourier transform of the covariance $C_z(\\Delta\\boldsymbol{\\xi})$. For the given Gaussian covariance, $S_z(|\\mathbf{k}|) = 2\\pi \\sigma_z^2 \\ell_z^2 \\exp(-|\\mathbf{k}|^2 \\ell_z^2 / 2)$. The variance of a filtered process $\\mathcal{L}\\zeta_z$ is given by Parseval's theorem:\n$$\n\\mathbb{E}[(\\mathcal{L}\\zeta_z)^2] = \\frac{1}{(2\\pi)^2} \\int |\\widehat{\\mathcal{L}}(\\mathbf{k})|^2 S_z(\\mathbf{k}) d\\mathbf{k}.\n$$\nFor the Laplacian operator $\\mathcal{L}=\\nabla^2$, its Fourier transform is $\\widehat{\\nabla^2}(\\mathbf{k}) = -|\\mathbf{k}|^2 = -(k_\\xi^2+k_\\eta^2)$.\n$$\n\\mathbb{E}[(\\nabla^2 \\zeta_z)^2] = \\frac{1}{(2\\pi)^2} \\int (|\\mathbf{k}|^2)^2 S_z(\\mathbf{k}) d\\mathbf{k} = \\frac{1}{(2\\pi)^2} \\int |\\mathbf{k}|^4 (2\\pi \\sigma_z^2 \\ell_z^2 \\exp(-|\\mathbf{k}|^2 \\ell_z^2 / 2)) d\\mathbf{k}.\n$$\nConverting to polar coordinates $(k,\\theta)$ where $k=|\\mathbf{k}|$ and $d\\mathbf{k} = k dk d\\theta$:\n$$\n\\mathbb{E}[(\\nabla^2 \\zeta_z)^2] = \\frac{2\\pi \\sigma_z^2 \\ell_z^2}{(2\\pi)^2} \\int_0^{2\\pi} d\\theta \\int_0^\\infty k^4 \\exp(-k^2 \\ell_z^2/2) k dk \\\\\n= \\frac{\\sigma_z^2 \\ell_z^2}{2\\pi} (2\\pi) \\int_0^\\infty k^5 \\exp(-k^2 \\ell_z^2/2) dk.\n$$\nLet $u=k^2\\ell_z^2/2$, so $k^2=2u/\\ell_z^2$ and $kdk=du/\\ell_z^2$. The integral becomes:\n$$\n\\sigma_z^2 \\ell_z^2 \\int_0^\\infty \\left(\\frac{2u}{\\ell_z^2}\\right)^2 e^{-u} \\frac{du}{\\ell_z^2} = \\sigma_z^2 \\ell_z^2 \\frac{4}{\\ell_z^4 \\ell_z^2} \\int_0^\\infty u^2 e^{-u} du = \\frac{4\\sigma_z^2}{\\ell_z^4} \\Gamma(3).\n$$\nSince $\\Gamma(3) = 2! = 2$, we get:\n$$\n\\mathbb{E}[(\\nabla^2 \\zeta_z)^2] = \\frac{8\\sigma_z^2}{\\ell_z^4}.\n$$\nSubstituting all the variance terms back into the expression for $\\mathrm{Var}[J]$:\n$$\n\\mathrm{Var}[J] = \\varepsilon^2 \\left( \\frac{\\sigma_x^2}{\\ell_x^2} + \\frac{\\sigma_y^2}{\\ell_y^2} + s^2 \\frac{8\\sigma_z^2}{\\ell_z^4} \\right).\n$$", "answer": "$$\n\\boxed{\\varepsilon^{2} \\left( \\frac{\\sigma_{x}^{2}}{\\ell_{x}^{2}} + \\frac{\\sigma_{y}^{2}}{\\ell_{y}^{2}} + \\frac{8 s^{2} \\sigma_{z}^{2}}{\\ell_{z}^{4}} \\right)}\n$$", "id": "3332114"}]}