{"hands_on_practices": [{"introduction": "The cornerstone of developing a reliable adjoint solver is rigorous verification. The Taylor test, also known as an adjoint consistency check, provides a powerful method for this purpose by confirming that the adjoint-computed gradient correctly represents the first-order term in the function's Taylor expansion. This practice [@problem_id:3289227] guides you through the implementation of this fundamental test for a functional that includes a non-differentiable component, mimicking features like flux limiters or turbulence model clips commonly found in CFD. By analyzing the convergence behavior, you will gain firsthand experience in diagnosing how the presence of such \"kinks\" can degrade the gradient approximation from second-order to first-order accuracy, a critical insight for robust optimization.", "problem": "You are tasked with implementing and validating a first-order adjoint linearization check for a nondifferentiable post-processing functional that mimics shock-sensor behavior in computational fluid dynamics. The goal is to test the consistency of a reverse-mode discrete adjoint (automatic differentiation) gradient against finite-difference directional perturbations in the state, and to quantify how the check breaks down near nondifferentiable kinks such as discontinuities.\n\nStart from the following fundamental base:\n- The directional derivative of a scalar functional $J(\\mathbf{w})$ at a state vector $\\mathbf{w} \\in \\mathbb{R}^N$ along a direction $\\delta \\mathbf{w} \\in \\mathbb{R}^N$ is defined through the first-order Taylor expansion,\n$$\nJ(\\mathbf{w}+\\epsilon \\,\\delta \\mathbf{w}) = J(\\mathbf{w}) + \\epsilon \\,\\nabla J(\\mathbf{w})^\\top \\delta \\mathbf{w} + r(\\epsilon;\\mathbf{w},\\delta \\mathbf{w}),\n$$\nwhere the remainder $r(\\epsilon;\\mathbf{w},\\delta \\mathbf{w})$ satisfies $r(\\epsilon;\\mathbf{w},\\delta \\mathbf{w}) = o(\\epsilon)$ as $\\epsilon \\to 0$ when $J$ is differentiable at $\\mathbf{w}$ in the direction $\\delta \\mathbf{w}$, and $r(\\epsilon;\\mathbf{w},\\delta \\mathbf{w}) = \\mathcal{O}(\\epsilon^2)$ when $J$ has a Lipschitz continuous gradient in a neighborhood of $\\mathbf{w}$ in the direction $\\delta \\mathbf{w}$.\n- The chain rule provides the basis for adjoint (reverse-mode automatic differentiation) accumulation of derivatives in composite functions.\n\nYou will consider the following discrete functional that represents a convex combination of a shock-sensor term and a smoothing term:\n$$\nJ(\\mathbf{w}) \\;=\\; \\sum_{i=0}^{N-2} \\phi\\!\\left(d_i\\right), \\quad d_i \\;=\\; w_{i+1}-w_i, \\quad \\phi(x) \\;=\\; |x| \\;+\\; \\tfrac{1}{2}\\kappa x^2,\n$$\nwith a given constant $\\kappa > 0$. The absolute value models a nondifferentiable component typical of discontinuity sensors; the quadratic term regularizes the functional and ensures a smooth curvature contribution where the absolute value is locally linear. Use the following rule for the derivative of the absolute value: for $x \\neq 0$, $\\frac{d}{dx}|x|=\\operatorname{sign}(x)$, and at $x=0$ use the specific subgradient selection $\\frac{d}{dx}|x|\\big|_{x=0}=0$.\n\nYour program must:\n1. Implement a reverse-mode adjoint (automatic differentiation) evaluation of the gradient $\\nabla J(\\mathbf{w})$ based on the chain rule applied to the computational graph that maps $\\mathbf{w} \\mapsto \\{d_i\\} \\mapsto \\{\\phi(d_i)\\} \\mapsto J$. The gradient accumulation must be constructed explicitly from first principles, not using any external automatic differentiation tool.\n2. Generate random perturbations $\\delta \\mathbf{w}$ with a specified seed, and verify the adjoint linearization\n$$\nJ(\\mathbf{w}+\\epsilon \\,\\delta \\mathbf{w}) - J(\\mathbf{w}) \\;\\approx\\; \\epsilon \\,\\nabla J(\\mathbf{w})^\\top \\delta \\mathbf{w}\n$$\nfor a decreasing sequence of $\\epsilon$ values. For each case, quantify the empirical convergence order $p$ obtained from the remainder magnitude $E(\\epsilon) = \\big|J(\\mathbf{w}+\\epsilon \\,\\delta \\mathbf{w}) - J(\\mathbf{w}) - \\epsilon \\,\\nabla J(\\mathbf{w})^\\top \\delta \\mathbf{w}\\big|$ via a least-squares fit of $\\log_{10}E(\\epsilon)$ versus $\\log_{10}\\epsilon$ across the test $\\epsilon$ values. A value $p \\approx 2$ indicates locally twice-differentiable behavior and a successful adjoint consistency check, while $p \\approx 1$ indicates a breakdown due to nondifferentiability near kinks or discontinuities.\n3. Track how the empirical order $p$ changes across smooth and nonsmooth state fields $\\mathbf{w}$.\n\nAngles are not involved. No physical units are required.\n\nImplement the test suite below. Use $N=200$ grid points and $\\kappa = 0.1$. Use the following three state fields $\\mathbf{w}$ and perturbation seeds, and use the same set of perturbation magnitudes for all cases:\n- Case 1 (smooth field, “happy path”): $w_i = \\sin(2\\pi x_i) + 0.2\\cos(3\\pi x_i) + 0.3$, with $x_i = i/(N-1)$ for $i=0,\\dots,N-1$. Random direction seed is $13$.\n- Case 2 (near-kink field, boundary of differentiability): $w_i = 0$ for all $i$. Random direction seed is $17$.\n- Case 3 (discontinuous field, shock-like step): $w_i = 1$ for $x_i  0.5$ and $w_i = -1$ otherwise. Random direction seed is $19$.\n\nFor each case, generate a random perturbation $\\delta \\mathbf{w}$ with independent standard normal entries, then normalize it to unit Euclidean norm. Use the perturbation magnitudes\n$$\n\\epsilon \\in \\{10^{-1}, 10^{-2}, 10^{-3}, 10^{-4}\\}.\n$$\n\nFor each case, compute the empirical convergence order $p$ as the slope of the best-fit line of $\\log_{10}E(\\epsilon)$ versus $\\log_{10}\\epsilon$ using linear least squares over the provided $\\epsilon$ values. Report the three $p$ values (one per case) as floating-point numbers rounded to three decimal places.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[p1,p2,p3]\").", "solution": "The problem is first validated against the specified criteria.\n\n### Step 1: Extract Givens\n- **Functional:** $J(\\mathbf{w}) = \\sum_{i=0}^{N-2} \\phi(d_i)$, where $d_i = w_{i+1}-w_i$.\n- **Component Function:** $\\phi(x) = |x| + \\frac{1}{2}\\kappa x^2$, with a constant $\\kappa > 0$.\n- **Subgradient Rule:** At $x=0$, the derivative of the absolute value is taken as $\\frac{d}{dx}|x|\\big|_{x=0}=0$. For $x \\neq 0$, the standard derivative $\\frac{d}{dx}|x|=\\operatorname{sign}(x)$ is used.\n- **Adjoint Linearization and Remainder:** The check is based on $J(\\mathbf{w}+\\epsilon \\,\\delta \\mathbf{w}) - J(\\mathbf{w}) \\approx \\epsilon \\,\\nabla J(\\mathbf{w})^\\top \\delta \\mathbf{w}$. The remainder is $E(\\epsilon) = \\big|J(\\mathbf{w}+\\epsilon \\,\\delta \\mathbf{w}) - J(\\mathbf{w}) - \\epsilon \\,\\nabla J(\\mathbf{w})^\\top \\delta \\mathbf{w}\\big|$.\n- **Convergence Order:** The empirical order $p$ is obtained from a least-squares fit of $\\log_{10}E(\\epsilon)$ versus $\\log_{10}\\epsilon$.\n- **Constants:** Grid points $N=200$, regularization parameter $\\kappa = 0.1$.\n- **Perturbation Magnitudes:** $\\epsilon \\in \\{10^{-1}, 10^{-2}, 10^{-3}, 10^{-4}\\}$.\n- **Test Cases:**\n  1. **Smooth Field:** $w_i = \\sin(2\\pi x_i) + 0.2\\cos(3\\pi x_i) + 0.3$, with $x_i = i/(N-1)$. Random seed for $\\delta\\mathbf{w}$ is $13$.\n  2. **Near-Kink Field:** $w_i = 0$ for all $i$. Random seed for $\\delta\\mathbf{w}$ is $17$.\n  3. **Discontinuous Field:** $w_i = 1$ for $x_i  0.5$ and $w_i = -1$ otherwise. Random seed for $\\delta\\mathbf{w}$ is $19$.\n- **Perturbation Vector:** $\\delta\\mathbf{w}$ is generated from independent standard normal entries and normalized to unit Euclidean norm.\n- **Output Format:** Three $p$ values, one for each case, rounded to three decimal places, in the format `[p1,p2,p3]`.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is assessed based on the validation criteria:\n- **Scientifically Grounded:** The problem is a standard numerical verification technique known as a Taylor test or adjoint consistency check, which is fundamental in the development of adjoint-based optimization and sensitivity analysis codes, particularly in computational fluid dynamics (CFD). The functional is a canonical example of a nonsmooth penalty term. All mathematical concepts (Taylor series, chain rule, subgradients, least-squares fitting) are well-established.\n- **Well-Posed:** The problem is well-posed. All parameters, state vectors, functions, and numerical procedures are precisely defined. The specification of the subgradient of $|x|$ at $x=0$ removes ambiguity, ensuring a unique gradient vector can be computed for any state $\\mathbf{w}$. The process described leads to a unique set of three computable values.\n- **Objective:** The problem is stated in precise, objective mathematical language. There are no subjective or opinion-based elements.\n\nThe problem exhibits none of the specified invalidity flaws. It is scientifically sound, fully specified, computationally feasible, and directly relevant to the stated topic of adjoint methods.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\n### Principle-Based Design of the Solution\n\n#### 1. The Functional and its Adjoint Gradient\nThe scalar functional $J$ is a function of the state vector $\\mathbf{w} \\in \\mathbb{R}^N$, defined as:\n$$\nJ(\\mathbf{w}) = \\sum_{i=0}^{N-2} \\phi(d_i), \\quad \\text{where } d_i = w_{i+1} - w_i \\text{ and } \\phi(x) = |x| + \\frac{1}{2}\\kappa x^2.\n$$\nTo compute the gradient $\\nabla J(\\mathbf{w})$, whose components are $\\frac{\\partial J}{\\partial w_j}$, we apply the chain rule:\n$$\n\\frac{\\partial J}{\\partial w_j} = \\sum_{i=0}^{N-2} \\frac{\\partial \\phi(d_i)}{\\partial w_j} = \\sum_{i=0}^{N-2} \\frac{d\\phi}{dx}(d_i) \\frac{\\partial d_i}{\\partial w_j}.\n$$\nThe derivative of $\\phi(x)$ with respect to its argument $x$ is:\n$$\n\\frac{d\\phi}{dx}(x) = \\frac{d|x|}{dx} + \\kappa x.\n$$\nUsing the problem's specified subgradient choice for the absolute value function, where $\\frac{d|x|}{dx}\\big|_{x=0}=0$, we have:\n$$\n\\frac{d\\phi}{dx}(x) = \\operatorname{sign}(x) + \\kappa x,\n$$\nwhere it is understood that $\\operatorname{sign}(0) = 0$.\n\nThe term $\\frac{\\partial d_i}{\\partial w_j}$ is non-zero only when $j$ is $i$ or $i+1$. Specifically, $\\frac{\\partial d_i}{\\partial w_{i+1}} = 1$ and $\\frac{\\partial d_i}{\\partial w_i} = -1$. This sparse dependency structure allows us to assemble the gradient. For any component $w_j$, it influences only $d_j$ (if $j  N-1$) and $d_{j-1}$ (if $j > 0$).\nBy accumulating the contributions, the gradient components are:\n- For $j=0$: $\\frac{\\partial J}{\\partial w_0} = \\frac{d\\phi}{dx}(d_0) \\frac{\\partial d_0}{\\partial w_0} = -\\frac{d\\phi}{dx}(d_0)$.\n- For $0  j  N-1$: $\\frac{\\partial J}{\\partial w_j} = \\frac{d\\phi}{dx}(d_{j-1})\\frac{\\partial d_{j-1}}{\\partial w_j} + \\frac{d\\phi}{dx}(d_j)\\frac{\\partial d_j}{\\partial w_j} = \\frac{d\\phi}{dx}(d_{j-1}) - \\frac{d\\phi}{dx}(d_j)$.\n- For $j=N-1$: $\\frac{\\partial J}{\\partial w_{N-1}} = \\frac{d\\phi}{dx}(d_{N-2})\\frac{\\partial d_{N-2}}{\\partial w_{N-1}} = \\frac{d\\phi}{dx}(d_{N-2})$.\n\nThis constitutes the reverse-mode automatic differentiation (adjoint) calculation for $\\nabla J(\\mathbf{w})$.\n\n#### 2. Adjoint Linearization Check\nThe first-order Taylor expansion of $J(\\mathbf{w})$ around a point $\\mathbf{w}$ in an arbitrary direction $\\delta\\mathbf{w}$ is:\n$$\nJ(\\mathbf{w}+\\epsilon \\,\\delta \\mathbf{w}) = J(\\mathbf{w}) + \\epsilon \\,\\nabla J(\\mathbf{w})^\\top \\delta \\mathbf{w} + \\mathcal{O}(\\epsilon^2).\n$$\nThis holds if $J$ is twice continuously differentiable in a neighborhood of $\\mathbf{w}$. The term $\\nabla J(\\mathbf{w})^\\top \\delta \\mathbf{w}$ is the directional derivative, which can be computed efficiently using the adjoint-derived gradient $\\nabla J(\\mathbf{w})$. The adjoint linearization check verifies that the change in the functional, $J(\\mathbf{w}+\\epsilon \\,\\delta \\mathbf{w}) - J(\\mathbf{w})$, is accurately approximated by the linear term $\\epsilon \\,\\nabla J(\\mathbf{w})^\\top \\delta \\mathbf{w}$.\n\nThe absolute error, or remainder, of this approximation is:\n$$\nE(\\epsilon) = \\big|J(\\mathbf{w}+\\epsilon \\,\\delta \\mathbf{w}) - J(\\mathbf{w}) - \\epsilon \\,\\nabla J(\\mathbf{w})^\\top \\delta \\mathbf{w}\\big|.\n$$\nIf $J$ is locally smooth, $E(\\epsilon)$ should scale as $\\mathcal{O}(\\epsilon^2)$. However, if the perturbation $\\epsilon\\,\\delta\\mathbf{w}$ causes the argument of the absolute value function to cross $x=0$, a \"kink\" is activated where the derivative is discontinuous. In this case, the first-order Taylor expansion is no longer a complete description of the local behavior, and the remainder $E(\\epsilon)$ will be dominated by terms of order $\\mathcal{O}(\\epsilon)$, reflecting the nondifferentiability.\n\n#### 3. Empirical Order of Convergence\nTo quantify this behavior, we model the error as $E(\\epsilon) \\approx C \\epsilon^p$ for some constant $C$ and convergence order $p$. Taking the base-10 logarithm yields a linear relationship:\n$$\n\\log_{10} E(\\epsilon) \\approx \\log_{10} C + p \\log_{10} \\epsilon.\n$$\nBy computing $E(\\epsilon)$ for a sequence of decreasing $\\epsilon$ values, we obtain a set of data points $(\\log_{10}\\epsilon, \\log_{10}E(\\epsilon))$. The empirical convergence order $p$ is then determined as the slope of the best-fit line through these points, calculated using linear least squares.\n\n#### 4. Implementation and Case Analysis\nThe implementation will follow these principles. Vectorized `numpy` operations will be used for efficiency. For each test case, we will:\n1. Construct the state vector $\\mathbf{w}$.\n2. Generate and normalize the random perturbation vector $\\delta\\mathbf{w}$.\n3. Compute the baseline functional value $J(\\mathbf{w})$ and the adjoint gradient $\\nabla J(\\mathbf{w})$.\n4. For each $\\epsilon \\in \\{10^{-1}, 10^{-2}, 10^{-3}, 10^{-4}\\}$, compute the remainder $E(\\epsilon)$.\n5. Perform a linear regression on $(\\log_{10}\\epsilon, \\log_{10}E(\\epsilon))$ to find the slope $p$.\n\n- **Case 1 (Smooth Field):** The state vector $\\mathbf{w}$ is a smooth function. The differences $d_i = w_{i+1}-w_i$ will be small but generally non-zero. The functional is evaluated away from the nondifferentiable kinks. The behavior is expected to be smooth, yielding a convergence order $p \\approx 2$.\n- **Case 2 (Near-Kink Field):** The state vector is $\\mathbf{w}=\\mathbf{0}$, meaning all differences are $d_i=0$. The functional is evaluated entirely at the kinks. The chosen subgradient is $\\nabla J(\\mathbf{0}) = \\mathbf{0}$. Any perturbation $\\delta\\mathbf{w}$ will move the arguments $d_i$ away from $0$, activating the nondifferentiability. The error will be dominated by the linear term, thus we expect $p \\approx 1$.\n- **Case 3 (Discontinuous Field):** The state vector represents a discrete step. Most differences are $d_i=0$, with one large non-zero difference at the step location. As in Case 2, the perturbation will activate the kinks at the many locations where $d_i=0$. This nondifferentiable behavior is expected to dominate, leading to a convergence order $p \\approx 1$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and validates a first-order adjoint linearization check\n    for a nondifferentiable functional.\n    \"\"\"\n\n    # Define problem constants\n    N = 200\n    KAPPA = 0.1\n    EPSILONS = np.array([1e-1, 1e-2, 1e-3, 1e-4])\n\n    def compute_J(w, kappa):\n        \"\"\"Computes the functional J(w).\"\"\"\n        d = w[1:] - w[:-1]\n        phi_d = np.abs(d) + 0.5 * kappa * d**2\n        return np.sum(phi_d)\n\n    def compute_grad_J(w, kappa):\n        \"\"\"Computes the adjoint gradient of J(w).\"\"\"\n        d = w[1:] - w[:-1]\n        \n        # Derivative of phi(x) = |x| + 0.5*kappa*x^2\n        # d(phi)/dx = sign(x) + kappa*x, with sign(0)=0 as per problem spec.\n        # np.sign(0) is 0, so this works directly.\n        phi_prime_d = np.sign(d) + kappa * d\n        \n        grad_w = np.zeros_like(w)\n        \n        # Reverse-mode accumulation\n        grad_w[1:] += phi_prime_d\n        grad_w[:-1] -= phi_prime_d\n        \n        return grad_w\n\n    def calculate_convergence_order(w, seed, n, kappa, epsilons):\n        \"\"\"\n        Calculates the empirical convergence order p for a given state w.\n        \"\"\"\n        # Generate and normalize the random perturbation\n        rng = np.random.default_rng(seed)\n        dw_raw = rng.standard_normal(n)\n        dw = dw_raw / np.linalg.norm(dw_raw)\n        \n        # Compute baseline values\n        J0 = compute_J(w, kappa)\n        grad_J = compute_grad_J(w, kappa)\n        adj_dot_dw = np.dot(grad_J, dw)\n        \n        errors = []\n        for eps in epsilons:\n            w_eps = w + eps * dw\n            J_eps = compute_J(w_eps, kappa)\n            error = np.abs(J_eps - J0 - eps * adj_dot_dw)\n            errors.append(error)\n            \n        log_eps = np.log10(epsilons)\n        log_E = np.log10(np.array(errors))\n        \n        # Fit a line log(E) = p * log(eps) + c to find the slope p\n        # using numpy's polyfit for linear regression.\n        p = np.polyfit(log_eps, log_E, 1)[0]\n        \n        return p\n\n    # Define test cases\n    x_grid = np.linspace(0, 1, N)\n    \n    # Case 1: Smooth field\n    w1 = np.sin(2 * np.pi * x_grid) + 0.2 * np.cos(3 * np.pi * x_grid) + 0.3\n    seed1 = 13\n    \n    # Case 2: Near-kink field\n    w2 = np.zeros(N)\n    seed2 = 17\n    \n    # Case 3: Discontinuous field\n    w3 = np.ones(N)\n    w3[x_grid >= 0.5] = -1.0\n    seed3 = 19\n    \n    test_cases = [\n        (w1, seed1),\n        (w2, seed2),\n        (w3, seed3),\n    ]\n\n    results = []\n    for w, seed in test_cases:\n        p = calculate_convergence_order(w, seed, N, KAPPA, EPSILONS)\n        results.append(f\"{p:.3f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3289227"}, {"introduction": "From verifying basic gradients, we now advance to a core application of adjoint methods in engineering: PDE-constrained shape optimization. The goal is to efficiently compute how an objective function, like aerodynamic drag, changes in response to modifications of a boundary shape. This practice [@problem_id:3289239] simplifies this complex problem to a one-dimensional diffusion equation, allowing you to focus on the essential mechanics of shape sensitivity analysis. You will implement an adjoint solver to compute the shape gradient predicted by the Hadamard form and validate its accuracy against a finite-difference approximation obtained by directly perturbing the computational mesh, thus connecting abstract theory to a concrete numerical workflow.", "problem": "Consider a one-dimensional model problem that captures the essence of shape sensitivity for diffusion-type Partial Differential Equations (PDEs), a common surrogate in computational fluid dynamics. Let the domain be the interval $\\Omega = (0,L)$ with outward unit normal at $x=L$ equal to $+1$ and at $x=0$ equal to $-1$. The state variable $u(x)$ solves the boundary value problem\n$$\n-\\,u''(x) = f(x) \\quad \\text{for } x \\in (0,L), \\qquad u(0)=0, \\quad u(L)=0,\n$$\nwith a constant source $f(x) \\equiv 1$. Define the objective functional\n$$\nJ(\\Omega) = \\int_0^L u(x)\\,dx.\n$$\nA small normal boundary perturbation at $x=L$ is given by $L \\mapsto L_\\epsilon = L + \\epsilon\\,V_n(L)$, where $\\epsilon$ is a small scalar and $V_n(L)$ is the normal component of the boundary velocity at $x=L$. The Hadamard form states that the shape derivative can be expressed as a boundary integral\n$$\n\\mathrm{d}J(\\Omega; \\boldsymbol{V}) \\;=\\; \\int_{\\partial \\Omega} g \\, \\boldsymbol{V} \\cdot \\boldsymbol{n} \\, ds,\n$$\nwhere $g$ is the shape gradient density and $\\boldsymbol{n}$ is the outward unit normal on $\\partial \\Omega$. In one dimension, this reduces to a signed sum over the endpoints.\n\nUsing the adjoint method, the adjoint variable $p(x)$ is defined to enforce stationarity of a Lagrangian and satisfies the adjoint PDE\n$$\np''(x) = w(x) \\quad \\text{for } x \\in (0,L), \\qquad p(0)=0, \\quad p(L)=0,\n$$\nwith $w(x) \\equiv 1$ corresponding to the linear sensitivity of $J(\\Omega)$ with respect to $u$. The Hadamard form for this problem reduces to a boundary expression involving the state and adjoint normal derivatives. In particular, for the chosen Lagrangian and constraints, the shape gradient at $x=L$ is\n$$\ng(L) \\;=\\; -\\,u'(L)\\,p'(L).\n$$\nYour task is to implement a mesh-perturbation test to validate the Hadamard form numerically by comparing:\n1. The adjoint shape gradient prediction $-\\,u'(L)\\,p'(L)\\,V_n(L)$, and\n2. A finite-difference evaluation of $\\displaystyle \\frac{J(\\Omega_\\epsilon)-J(\\Omega)}{\\epsilon}$ computed by solving the perturbed problem on $(0,L_\\epsilon)$.\n\nImplement the following steps for each test case:\n- Discretize the forward PDE for $u(x)$ on a uniform mesh with $N$ intervals over $(0,L)$ using second-order central differences for interior points and enforce Dirichlet boundary conditions $u(0)=u(L)=0$.\n- Compute $J(\\Omega)$ using the composite trapezoidal rule on the same mesh.\n- Discretize and solve the adjoint PDE for $p(x)$ using the same mesh and boundary conditions. Use $w(x)\\equiv 1$.\n- Approximate $u'(L)$ and $p'(L)$ using a second-order one-sided finite difference formula at the right boundary:\n$$\n\\phi'(L) \\approx \\frac{3\\,\\phi(L) - 4\\,\\phi(L-h) + \\phi(L-2h)}{2h},\n$$\nwith $\\phi(L)=0$ and $h=L/N$.\n- Form the adjoint prediction $G_{\\text{adj}} = -\\,u'(L)\\,p'(L)\\,V_n(L)$ with $V_n(L)=1$.\n- Form the finite-difference estimate $G_{\\text{fd}} = \\dfrac{J(\\Omega_\\epsilon)-J(\\Omega)}{\\epsilon}$ by re-solving the forward problem on $(0,L_\\epsilon)$ with the same number of intervals $N$ and computing $J(\\Omega_\\epsilon)$ by trapezoidal rule.\n- Report the relative error $E = \\dfrac{\\left|G_{\\text{adj}} - G_{\\text{fd}}\\right|}{\\max\\left(10^{-12}, \\left|G_{\\text{fd}}\\right|\\right)}$ as a float for each test case.\n\nNo physical units are required for this problem, and angles do not appear. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, \"[result1,result2,...]\".\n\nUse the following test suite of parameter values $(L,N,\\epsilon)$ with $V_n(L)=1$ for all cases:\n- Test 1 (happy path): $(1.0, 200, 1.0\\times 10^{-4})$.\n- Test 2 (small domain edge case): $(0.2, 200, 1.0\\times 10^{-5})$.\n- Test 3 (larger domain): $(3.0, 300, 1.0\\times 10^{-4})$.\n- Test 4 (coarser mesh stress test): $(1.0, 50, 1.0\\times 10^{-4})$.\n\nYour program must implement the numerical scheme described above and output the four relative errors $E$ as floats in a single list on one line in the exact specified format.", "solution": "The problem statement is critically evaluated for validity before a solution is attempted.\n\n### Step 1: Extract Givens\n\nThe provided information is as follows:\n- **State Equation (Forward Problem)**: A one-dimensional boundary value problem for the state variable $u(x)$ on the domain $\\Omega = (0,L)$.\n$$-\\,u''(x) = f(x) \\quad \\text{for } x \\in (0,L)$$\nwith a constant source term $f(x) \\equiv 1$ and Dirichlet boundary conditions $u(0)=0, u(L)=0$.\n- **Objective Functional**:\n$$J(\\Omega) = \\int_0^L u(x)\\,dx$$\n- **Boundary Perturbation**: The domain length $L$ is perturbed to $L_\\epsilon = L + \\epsilon\\,V_n(L)$, where $V_n(L)$ is the normal velocity at $x=L$.\n- **Adjoint Equation**: The adjoint variable $p(x)$ satisfies the boundary value problem:\n$$p''(x) = w(x) \\quad \\text{for } x \\in (0,L)$$\nwith a constant source term $w(x) \\equiv 1$ and Dirichlet boundary conditions $p(0)=0, p(L)=0$.\n- **Shape Gradient**: The shape gradient density at $x=L$ is given by the formula:\n$$g(L) = -\\,u'(L)\\,p'(L)$$\n- **Numerical Methods**:\n    - **PDE Discretization**: Second-order central differences on a uniform mesh with $N$ intervals ($h=L/N$).\n    - **Integral Approximation**: Composite trapezoidal rule.\n    - **Boundary Derivative Approximation**: Second-order one-sided finite difference formula: $\\phi'(L) \\approx \\frac{3\\,\\phi(L) - 4\\,\\phi(L-h) + \\phi(L-2h)}{2h}$.\n- **Tasks**:\n    1.  Calculate the adjoint-based gradient prediction: $G_{\\text{adj}} = g(L)\\,V_n(L) = -\\,u'(L)\\,p'(L)\\,V_n(L)$.\n    2.  Calculate the finite-difference gradient estimate: $G_{\\text{fd}} = \\dfrac{J(\\Omega_\\epsilon)-J(\\Omega)}{\\epsilon}$.\n    3.  Compute the relative error: $E = \\dfrac{\\left|G_{\\text{adj}} - G_{\\text{fd}}\\right|}{\\max\\left(10^{-12}, \\left|G_{\\text{fd}}\\right|\\right)}$.\n- **Constants and Parameters**: $V_n(L)=1$ for all cases.\n- **Test Cases**: A list of four tuples $(L, N, \\epsilon)$:\n    1.  $(1.0, 200, 1.0\\times 10^{-4})$\n    2.  $(0.2, 200, 1.0\\times 10^{-5})$\n    3.  $(3.0, 300, 1.0\\times 10^{-4})$\n    4.  $(1.0, 50, 1.0\\times 10^{-4})$\n\n### Step 2: Validate Using Extracted Givens\n\nThe problem is evaluated against the validation criteria:\n- **Scientifically Grounded**: The problem is an application of standard techniques in numerical analysis and optimization theory to a canonical differential equation. The state and adjoint equations are forms of the one-dimensional Poisson equation, a fundamental model in physics and engineering. The concept of a shape derivative, the Hadamard form, and the use of the adjoint method for its computation are well-established, core principles in sensitivity analysis and shape optimization for PDEs. All provided formulas for numerical approximation (central differences, trapezoidal rule, one-sided derivative) are standard and correct.\n- **Well-Posed**: The boundary value problems for both the state $u(x)$ and adjoint $p(x)$ are well-posed, admitting unique solutions. The overall task is a numerical verification procedure, which is well-defined and leads to a unique numerical result for each test case.\n- **Objective**: The problem is stated in precise, formal mathematical language. It is free from ambiguity, subjectivity, or non-scientific claims.\n- **Completeness and Consistency**: The problem statement is self-contained. It provides all necessary equations, boundary conditions, numerical schemes, and parameter values required to perform the computation. There are no internal contradictions. For instance, the adjoint equation $p''=w$ with $w=1$ is consistent with deriving the sensitivity of the functional $J=\\int u\\,dx$.\n\n### Step 3: Verdict and Action\n\nThe problem is **valid**. It is a well-posed, scientifically sound, and clearly articulated numerical task based on established principles of computational mathematics. A complete solution will be provided.\n\n### Solution\n\nThe task is to perform a mesh-perturbation test to numerically validate the adjoint-based formula for the shape gradient of an objective functional constrained by a differential equation. We will compare the gradient computed via the adjoint method, $G_{\\text{adj}}$, with an estimate obtained by a finite difference approximation, $G_{\\text{fd}}$.\n\n**1. Analytical Framework**\n\nFor context and verification, we first establish the analytical solution.\nThe state equation is $-\\,u''(x) = 1$ with $u(0)=u(L)=0$. Integrating twice and applying the boundary conditions yields the exact solution for the state variable:\n$$u(x) = \\frac{1}{2} x(L-x)$$\nThe adjoint equation is $p''(x) = 1$ with $p(0)=p(L)=0$. This can be written as $-\\,p''(x) = -1$. Its solution is:\n$$p(x) = -\\frac{1}{2} x(L-x) = -u(x)$$\nThe objective functional $J$ can be computed analytically:\n$$J(\\Omega) = \\int_0^L \\frac{1}{2} x(L-x) \\,dx = \\frac{1}{2} \\left[ \\frac{Lx^2}{2} - \\frac{x^3}{3} \\right]_0^L = \\frac{L^3}{12}$$\nThe derivatives of the state and adjoint variables at $x=L$ are:\n$$u'(x) = \\frac{L}{2} - x \\implies u'(L) = -\\frac{L}{2}$$\n$$p'(x) = x - \\frac{L}{2} \\implies p'(L) = \\frac{L}{2}$$\nThe exact shape gradient density at $x=L$ is:\n$$g(L) = -u'(L)p'(L) = -\\left(-\\frac{L}{2}\\right)\\left(\\frac{L}{2}\\right) = \\frac{L^2}{4}$$\nFor $V_n(L)=1$, the exact shape derivative is $\\frac{L^2}{4}$. This provides a benchmark for our numerical results.\n\n**2. Numerical Implementation Strategy**\n\nThe core of the implementation is a solver for the generic 1D Poisson problem $-y''(x) = f(x)$ on $(0, \\ell)$ with $y(0)=y(\\ell)=0$.\n- **PDE Solver**: We discretize the domain $(0, \\ell)$ into $N$ uniform intervals of width $h = \\ell/N$. The grid points are $x_i = i h$ for $i=0, \\dots, N$. The second derivative at an interior point $x_i$ ($i=1, \\dots, N-1$) is approximated by a second-order central difference:\n$$ -y''(x_i) \\approx \\frac{-y_{i-1} + 2y_i - y_{i+1}}{h^2} = f_i $$\nThis leads to a system of $N-1$ linear equations for the interior nodal values $\\mathbf{y}_{int} = [y_1, \\dots, y_{N-1}]^T$. The system is of the form $A \\mathbf{y}_{int} = \\mathbf{b}$, where $A$ is a symmetric positive-definite tridiagonal matrix with $2$ on the main diagonal and $-1$ on the super- and sub-diagonals, scaled by $1/h^2$. The right-hand side is $\\mathbf{b} = [f_1, \\dots, f_{N-1}]^T$. We will employ `scipy.linalg.solve_banded` for an efficient solution. The full solution vector is then $\\mathbf{y} = [0, y_1, \\dots, y_{N-1}, 0]^T$.\n\nWith this solver, the procedure for each test case $(L, N, \\epsilon)$ is as follows:\n\n- **Step A: Baseline and Adjoint Computations (on domain $\\Omega = (0,L)$)**\n    1.  Solve for the state $u(x)$ by calling the Poisson solver with $\\ell=L$, $N$, and a source function $f(x)=1$. Let the solution be $\\mathbf{u}$.\n    2.  Compute the baseline objective functional $J(\\Omega)$ using the composite trapezoidal rule on $\\mathbf{u}$ and step size $h=L/N$: $J(\\Omega) \\approx h \\sum_{i=1}^{N-1} u_i$.\n    3.  Solve for the adjoint $p(x)$ by calling the Poisson solver for $-p''=-1$ with $\\ell=L$, $N$, and a source function equivalent to $f(x)=-1$. Let the solution be $\\mathbf{p}$.\n    4.  Approximate the derivatives $u'(L)$ and $p'(L)$ using the provided second-order one-sided formula on the numerical solutions $\\mathbf{u}$ and $\\mathbf{p}$, respectively. For a generic solution $\\phi$ with $\\phi_N=0$, the formula is $\\phi'(L) \\approx \\frac{-4\\phi_{N-1} + \\phi_{N-2}}{2h}$.\n    5.  Calculate the adjoint gradient prediction: $G_{\\text{adj}} = -u'(L)_{num} \\, p'(L)_{num} \\, V_n(L)$, with $V_n(L) = 1$.\n\n- **Step B: Finite Difference Computation (on perturbed domain $\\Omega_\\epsilon = (0,L_\\epsilon)$)**\n    1.  Define the perturbed domain length $L_\\epsilon = L + \\epsilon V_n(L)$.\n    2.  Solve for the perturbed state $u_\\epsilon(x)$ by calling the Poisson solver with $\\ell=L_\\epsilon$, $N$, and source $f(x)=1$. Let the solution be $\\mathbf{u}_\\epsilon$.\n    3.  Compute the perturbed objective functional $J(\\Omega_\\epsilon)$ using the trapezoidal rule on $\\mathbf{u}_\\epsilon$ and the new step size $h_\\epsilon = L_\\epsilon/N$: $J(\\Omega_\\epsilon) \\approx h_\\epsilon \\sum_{i=1}^{N-1} u_{\\epsilon,i}$.\n    4.  Calculate the finite difference gradient estimate: $G_{\\text{fd}} = \\frac{J(\\Omega_\\epsilon) - J(\\Omega)}{\\epsilon}$.\n\n- **Step C: Error Calculation**\n    1.  Compute the relative error $E$ between the two gradient approximations: $E = \\dfrac{\\left|G_{\\text{adj}} - G_{\\text{fd}}\\right|}{\\max\\left(10^{-12}, \\left|G_{\\text{fd}}\\right|\\right)}$.\n\nThis procedure will be executed for each test case specified in the problem statement.", "answer": "```python\nimport numpy as np\nfrom scipy.linalg import solve_banded\n\ndef solve_poisson_1d(length, n_intervals, source_val):\n    \"\"\"\n    Solves the 1D Poisson equation -y'' = f on (0, length) with y(0)=y(length)=0.\n\n    Args:\n        length (float): The length of the domain.\n        n_intervals (int): The number of intervals for the uniform mesh.\n        source_val (float): The constant value of the source term f.\n\n    Returns:\n        numpy.ndarray: The numerical solution vector y, including boundary points.\n    \"\"\"\n    h = length / n_intervals\n    num_interior_pts = n_intervals - 1\n\n    if num_interior_pts  1:\n        return np.zeros(n_intervals + 1)\n\n    # Set up the tridiagonal system Ay_int = b for interior points\n    # The equation is (-y_{i-1} + 2y_i - y_{i+1})/h^2 = f_i\n    # This leads to a matrix with main diagonal 2, and sub/super diagonals -1.\n    diagonals = np.zeros((3, num_interior_pts))\n    diagonals[0, 1:] = -1.0  # Super-diagonal\n    diagonals[1, :] = 2.0    # Main diagonal\n    diagonals[2, :-1] = -1.0 # Sub-diagonal\n\n    # Right-hand side vector\n    b = np.full(num_interior_pts, h**2 * source_val)\n\n    # Solve the banded system\n    y_interior = solve_banded((1, 1), diagonals, b)\n\n    # Construct the full solution vector including boundaries\n    y = np.zeros(n_intervals + 1)\n    y[1:-1] = y_interior\n    return y\n\ndef compute_integral_J(y, h):\n    \"\"\"\n    Computes the integral of y using the composite trapezoidal rule.\n    Assumes boundary values y[0] and y[-1] are zero.\n\n    Args:\n        y (numpy.ndarray): The function values on the grid.\n        h (float): The grid spacing.\n\n    Returns:\n        float: The approximate value of the integral.\n    \"\"\"\n    return h * np.sum(y[1:-1])\n\ndef compute_boundary_derivative(phi, h):\n    \"\"\"\n    Computes the derivative at the right boundary phi'(L) using a \n    second-order one-sided finite difference formula.\n    phi'(L) approx (3*phi(L) - 4*phi(L-h) + phi(L-2h))/(2h).\n    Given phi(L) = 0.\n    \n    Args:\n        phi (numpy.ndarray): The function values, where phi[-1] is phi(L).\n        h (float): The grid spacing.\n\n    Returns:\n        float: The approximate derivative at the boundary.\n    \"\"\"\n    # phi[-1] is phi(L), phi[-2] is phi(L-h), phi[-3] is phi(L-2h)\n    # The formula given is (3*phi[-1] - 4*phi[-2] + phi[-3]) / (2*h).\n    # Since boundary condition enforces phi[-1] = 0.\n    return (-4.0 * phi[-2] + phi[-3]) / (2.0 * h)\n\ndef solve():\n    \"\"\"\n    Main function to run the mesh-perturbation test for the given cases.\n    \"\"\"\n    test_cases = [\n        (1.0, 200, 1.0e-4),\n        (0.2, 200, 1.0e-5),\n        (3.0, 300, 1.0e-4),\n        (1.0, 50, 1.0e-4),\n    ]\n\n    results = []\n\n    for L, N, epsilon in test_cases:\n        Vn_L = 1.0\n\n        # --- Base problem and Adjoint Method ---\n        h = L / N\n        \n        # Solve forward problem for u(x) on (0,L)\n        # -u'' = 1\n        u_sol = solve_poisson_1d(L, N, source_val=1.0)\n        \n        # Compute baseline objective functional J(Omega)\n        J_base = compute_integral_J(u_sol, h)\n        \n        # Solve adjoint problem for p(x) on (0,L)\n        # p'' = 1, so -p'' = -1\n        p_sol = solve_poisson_1d(L, N, source_val=-1.0)\n        \n        # Compute derivatives at the right boundary\n        u_prime_L = compute_boundary_derivative(u_sol, h)\n        p_prime_L = compute_boundary_derivative(p_sol, h)\n        \n        # Calculate the adjoint gradient prediction\n        G_adj = -u_prime_L * p_prime_L * Vn_L\n\n        # --- Perturbed problem and Finite Difference Method ---\n        L_eps = L + epsilon * Vn_L\n        h_eps = L_eps / N\n\n        # Solve forward problem for u_eps(x) on (0, L_eps)\n        u_eps_sol = solve_poisson_1d(L_eps, N, source_val=1.0)\n        \n        # Compute objective functional on perturbed domain J(Omega_eps)\n        J_eps = compute_integral_J(u_eps_sol, h_eps)\n\n        # Calculate the finite difference gradient estimate\n        G_fd = (J_eps - J_base) / epsilon\n        \n        # --- Compute Relative Error ---\n        denominator = max(1.0e-12, abs(G_fd))\n        error = abs(G_adj - G_fd) / denominator\n        results.append(error)\n\n    # Print results in the specified format\n    print(f\"[{','.join(f'{r:.15f}' for r in results)}]\")\n\nsolve()\n```", "id": "3289239"}, {"introduction": "Extending adjoint methods from steady-state to time-dependent problems unlocks powerful capabilities for design and control, but also introduces new numerical challenges, chief among them being the potential for instability. For systems with non-normal dynamics, which are prevalent in fluid mechanics, the backward-in-time integration of the adjoint equations can lead to exponential error growth, or \"blow-up,\" rendering the computed sensitivities useless. This exercise [@problem_id:3289215] provides a hands-on exploration of this phenomenon using a simple linear dynamical system. You will implement a reverse-time low-pass filter to stabilize the adjoint integration and quantitatively evaluate the trade-off between numerical stability and gradient accuracy, a crucial consideration for any unsteady adjoint implementation.", "problem": "Consider a linear time-varying, semi-discrete dynamical system that models the evolution of a two-component state $u \\in \\mathbb{R}^{2}$ under a non-normal operator. The system is defined on a uniform time grid $t_{n} = n \\Delta t$ for $n \\in \\{0,1,\\dots,N\\}$ with constant time step $\\Delta t$. The forward dynamics are given by the explicit Euler scheme applied to the linear ordinary differential equation\n$$\n\\frac{d u}{d t} = A(t)\\, u + \\theta\\, b(t)\\, s,\n$$\nwhere $A(t) \\in \\mathbb{R}^{2 \\times 2}$ is a time-dependent, upper-triangular matrix \n$$\nA(t) = \\begin{bmatrix} -\\alpha  \\beta(t) \\\\ 0  -\\alpha \\end{bmatrix},\n$$\n$\\theta \\in \\mathbb{R}$ is a scalar parameter, $b(t) \\in \\mathbb{R}$ is a known scalar function of time, and $s \\in \\mathbb{R}^{2}$ is a fixed source vector. The discrete forward update is\n$$\nu_{n+1} = u_{n} + \\Delta t \\left(A_{n}\\, u_{n} + \\theta\\, b_{n}\\, s\\right),\n$$\nwhere $A_{n} \\equiv A(t_{n})$ and $b_{n} \\equiv b(t_{n})$. The initial condition is $u_{0} = 0$. Define the performance measure\n$$\nJ(\\theta) = \\frac{1}{2}\\,\\|u_{N}\\|_{2}^{2},\n$$\nwhere $\\|\\cdot\\|_{2}$ denotes the Euclidean norm.\n\nYou will implement a discrete adjoint method to compute the sensitivity $\\frac{dJ}{d\\theta}$ and investigate an adjoint stabilization technique based on reverse-time low-pass filtering of the adjoint variables. The discrete adjoint for the explicit Euler scheme is defined by the terminal condition\n$$\np_{N} = \\frac{\\partial J}{\\partial u_{N}} = u_{N},\n$$\nand the backward recurrence\n$$\np_{n} = \\left(I + \\Delta t\\, A_{n}\\right)^{\\top} p_{n+1}, \\quad n = N-1, N-2, \\dots, 0.\n$$\nThe corresponding discrete-adjoint gradient is\n$$\ng_{\\text{adj}} = \\sum_{n=0}^{N-1} \\Delta t\\, b_{n}\\, s^{\\top} p_{n+1}.\n$$\nTo stabilize potential blow-up in the adjoint due to non-normal amplification when integrating backward in time, define a reverse-time, first-order low-pass filter applied to the adjoint sequence $\\{p_{n}\\}$:\n$$\nq_{N} = p_{N}, \\quad q_{n} = r\\, p_{n} + (1-r)\\, q_{n+1}, \\quad r \\in (0,1], \\quad n = N-1, \\dots, 0.\n$$\nThe filtered-adjoint gradient uses the filtered sequence $\\{q_{n}\\}$ in place of $\\{p_{n}\\}$:\n$$\ng_{\\text{filt}}(r) = \\sum_{n=0}^{N-1} \\Delta t\\, b_{n}\\, s^{\\top} q_{n+1}.\n$$\n\nYour task is to write a program that, for a prescribed test suite of filter strengths and non-normality levels, evaluates whether reverse-time low-pass filtering mitigates adjoint blow-up while preserving the correctness of the gradient with respect to a finite-difference benchmark. Specifically:\n\n- Use the following fixed numerical specifications:\n  - Time horizon $T = 20.0$ with uniform step size $\\Delta t = 0.005$ and number of steps $N = 4000$.\n  - Damping parameter $\\alpha = 0.01$.\n  - Time-varying non-normal coupling \n    $$\n    \\beta(t) = \\text{scale} \\cdot \\beta_{0}\\, \\left(1 + 0.5 \\sin(\\omega t)\\right),\n    $$\n    with base amplitude $\\beta_{0} = 120.0$ and frequency $\\omega = 0.7$.\n  - Source $s = \\begin{bmatrix} 1.0 \\\\ 0.0 \\end{bmatrix}$ and input modulation $b(t) = \\cos\\!\\left(2\\pi t / T\\right)$.\n  - Parameter value $\\theta = 0.3$.\n  - Finite-difference step $\\varepsilon = 10^{-6}$ for a central-difference benchmark \n    $$\n    g_{\\text{fd}} = \\frac{J(\\theta + \\varepsilon) - J(\\theta - \\varepsilon)}{2\\varepsilon}.\n    $$\n  - Blow-up threshold $M = 10^{8}$ assessed on the sequence actually used to compute the gradient (unfiltered if $r = 1$, filtered if $r \\in (0,1)$) by the criterion \n    $$\n    \\max_{0 \\le n \\le N} \\|z_{n}\\|_{2} > M \\quad \\Rightarrow \\quad \\text{blow-up} = 1,\n    $$\n    where $z_{n} = p_{n}$ if $r = 1$ and $z_{n} = q_{n}$ otherwise.\n\n- Implement the forward simulation for given $\\theta$ and $\\text{scale}$ to compute $J(\\theta)$ via the explicit Euler scheme. Then implement the discrete adjoint and the reverse-time filter with coefficient $r$ to compute $g_{\\text{filt}}(r)$.\n\n- For each test case, compute:\n  - The blow-up indicator as an integer: $1$ if blow-up is detected according to the above criterion, and $0$ otherwise.\n  - The relative gradient error\n    $$\n    E = \\frac{|g_{\\text{filt}}(r) - g_{\\text{fd}}|}{\\max\\{1, |g_{\\text{fd}}|\\}}.\n    $$\n\nTest Suite:\nEvaluate the program for the following four parameter pairs $(r,\\text{scale})$:\n- Case $1$: $(r, \\text{scale}) = (1.0, 1.5)$.\n- Case $2$: $(r, \\text{scale}) = (0.8, 1.0)$.\n- Case $3$: $(r, \\text{scale}) = (0.2, 1.5)$.\n- Case $4$: $(r, \\text{scale}) = (1.0, 0.5)$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with entries ordered as\n$$\n[\\text{blow}_{1}, E_{1}, \\text{blow}_{2}, E_{2}, \\text{blow}_{3}, E_{3}, \\text{blow}_{4}, E_{4}],\n$$\nwhere $\\text{blow}_{k} \\in \\{0,1\\}$ and $E_{k}$ is a floating-point number for the corresponding test case $k \\in \\{1,2,3,4\\}$. No additional text should be printed.", "solution": "The problem statement has been analyzed and is determined to be valid. It presents a well-posed, scientifically grounded numerical task within the domain of computational science, specifically focusing on discrete adjoint methods for sensitivity analysis of a non-normal dynamical system. All parameters, equations, and evaluation criteria are explicitly and consistently defined, allowing for a unique and verifiable solution.\n\nThe objective is to compute the sensitivity of a performance measure $J(\\theta)$ with respect to a parameter $\\theta$ for a linear time-varying system. The sensitivity, or gradient, $\\frac{dJ}{d\\theta}$, is computed using a discrete adjoint method. The problem investigates the numerical instability (blow-up) that can arise during the backward-in-time integration of the adjoint equations due to the non-normal nature of the system operator. A reverse-time low-pass filter is introduced as a potential stabilization technique, and its effectiveness is evaluated by comparing the resulting filtered-adjoint gradient $g_{\\text{filt}}(r)$ against a benchmark gradient $g_{\\text{fd}}$ computed via finite differences.\n\nThe core of the problem involves implementing a sequence of numerical algorithms for each specified test case. The methodology is broken down as follows:\n\n1.  **Forward Simulation and Performance Evaluation**\n\n    The state of the system, $u \\in \\mathbb{R}^{2}$, evolves according to the discrete forward update rule derived from an explicit Euler discretization:\n    $$\n    u_{n+1} = u_{n} + \\Delta t \\left(A_{n}\\, u_{n} + \\theta\\, b_{n}\\, s\\right)\n    $$\n    where $u_{0} = 0$. The time-dependent matrix is $A_{n} = A(t_{n}) = \\begin{bmatrix} -\\alpha  \\beta(t_{n}) \\\\ 0  -\\alpha \\end{bmatrix}$. This equation is stepped forward in time from $n=0$ to $n=N-1$ to find the final state $u_{N}$. The performance measure is then calculated as:\n    $$\n    J(\\theta) = \\frac{1}{2}\\,\\|u_{N}\\|_{2}^{2} = \\frac{1}{2} u_{N}^{\\top} u_{N}\n    $$\n    This forward simulation must be performed for three values of the parameter $\\theta$: the nominal value $\\theta_{\\text{nom}}$, $\\theta_{\\text{nom}} + \\varepsilon$, and $\\theta_{\\text{nom}} - \\varepsilon$.\n\n2.  **Finite-Difference Benchmark Gradient**\n\n    A central-difference approximation provides a benchmark value for the gradient. Using the performance measures computed in the previous step, the finite-difference gradient is:\n    $$\n    g_{\\text{fd}} = \\frac{J(\\theta_{\\text{nom}} + \\varepsilon) - J(\\theta_{\\text{nom}} - \\varepsilon)}{2\\varepsilon}\n    $$\n    where $\\varepsilon$ is a small perturbation. This value serves as the \"ground truth\" against which the adjoint-based gradients are compared.\n\n3.  **Discrete Adjoint Simulation**\n\n    The discrete adjoint method provides an efficient way to compute the gradient. The adjoint variables, $p \\in \\mathbb{R}^{2}$, are governed by a backward recurrence relation. The simulation starts with a terminal condition derived from the gradient of the performance measure $J$ with respect to the final state $u_N$:\n    $$\n    p_{N} = \\frac{\\partial J}{\\partial u_{N}} = u_{N}\n    $$\n    where $u_N$ is the final state from the forward simulation using the nominal parameter $\\theta_{\\text{nom}}$. The adjoint variables are then integrated backward in time from $n=N-1$ down to $n=0$:\n    $$\n    p_{n} = \\left(I + \\Delta t\\, A_{n}\\right)^{\\top} p_{n+1}\n    $$\n    The large off-diagonal terms in $A_n$ can cause non-normal amplification, leading to a rapid growth in the magnitude $\\|p_n\\|$ as the backward integration proceeds, a phenomenon referred to as \"adjoint blow-up\".\n\n4.  **Reverse-Time Low-Pass Filtering**\n\n    To mitigate potential blow-up, a recursive, first-order, low-pass filter is applied to the adjoint sequence $\\{p_n\\}$ during the backward integration. This generates a filtered adjoint sequence $\\{q_n\\}$. The filter is defined by the recurrence:\n    $$\n    q_{N} = p_{N}\n    $$\n    $$\n    q_{n} = r\\, p_{n} + (1-r)\\, q_{n+1}, \\quad \\text{for } n = N-1, \\dots, 0\n    $$\n    The filter strength is controlled by the parameter $r \\in (0,1]$. A value of $r=1$ corresponds to no filtering, in which case $q_n = p_n$ for all $n$. Smaller values of $r$ correspond to stronger filtering, which averages the current adjoint state $p_n$ with the previously computed filtered state $q_{n+1}$.\n\n5.  **Adjoint Gradient and Error Calculation**\n\n    The gradient is computed by summing the contributions from each time step, weighted by the adjoint variables. The filtered-adjoint gradient $g_{\\text{filt}}(r)$ is given by:\n    $$\n    g_{\\text{filt}}(r) = \\sum_{n=0}^{N-1} \\Delta t\\, b_{n}\\, s^{\\top} q_{n+1}\n    $$\n    This summation is performed concurrently with the backward integration of the adjoint and filter equations.\n\n6.  **Evaluation Metrics**\n\n    For each test case defined by a pair $(r, \\text{scale})$, two metrics are computed:\n    - **Blow-up Indicator:** The stability of the backward integration is assessed. Let the sequence used for the gradient be denoted by $\\{z_n\\}$, where $z_n = p_n$ if $r=1$ and $z_n = q_n$ if $r \\in (0,1)$. Blow-up is detected if the maximum norm of this sequence exceeds a threshold $M$:\n      $$\n      \\text{blow-up} = \\begin{cases} 1  \\text{if } \\max_{0 \\le n \\le N} \\|z_{n}\\|_{2} > M \\\\ 0  \\text{otherwise} \\end{cases}\n      $$\n    - **Relative Gradient Error:** The accuracy of the filtered-adjoint gradient is measured by its relative error with respect to the finite-difference benchmark:\n      $$\n      E = \\frac{|g_{\\text{filt}}(r) - g_{\\text{fd}}|}{\\max\\{1, |g_{\\text{fd}}|\\}}\n      $$\n    The denominator $\\max\\{1, |g_{\\text{fd}}|\\}$ ensures the error is well-defined even if the benchmark gradient is close to zero.\n\nThe computational procedure for each test case is to first run the necessary forward simulations to compute $g_{\\text{fd}}$, then run the backward adjoint simulation incorporating the specified filter strength $r$ to compute $g_{\\text{filt}}(r)$ and check for blow-up, and finally compute the relative error $E$. This process is repeated for all four test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main solver function that orchestrates the computation for all test cases.\n    \"\"\"\n    \n    # Fixed numerical specifications from the problem statement\n    T = 20.0\n    dt = 0.005\n    N = int(T / dt)  # 4000 steps\n    alpha = 0.01\n    beta0 = 120.0\n    omega = 0.7\n    s = np.array([1.0, 0.0])\n    theta_nom = 0.3\n    eps = 1e-6\n    M = 1e8\n\n    # Test suite: pairs of (r, scale)\n    test_cases = [\n        (1.0, 1.5),  # Case 1\n        (0.8, 1.0),  # Case 2\n        (0.2, 1.5),  # Case 3\n        (1.0, 0.5),  # Case 4\n    ]\n\n    # Pre-compute time-dependent arrays\n    time_pts = np.linspace(0, T, N + 1)\n    b_vals = np.cos(2 * np.pi * time_pts / T)\n\n    def forward_sim(theta_val, scale_val, beta_vals_scaled):\n        \"\"\"\n        Runs the forward simulation to compute the objective J and final state u_N.\n        \"\"\"\n        u = np.zeros(2)\n        for n in range(N):\n            A_n = np.array([[-alpha, beta_vals_scaled[n]], [0, -alpha]])\n            u = u + dt * (A_n @ u + theta_val * b_vals[n] * s)\n        \n        J = 0.5 * (u @ u)\n        return J, u\n\n    def compute_metrics(r, scale):\n        \"\"\"\n        Computes the blow-up indicator and relative error for a given (r, scale) case.\n        \"\"\"\n        # --- Step 1: Compute time-dependent coupling for this scale ---\n        beta_vals_scaled = scale * beta0 * (1 + 0.5 * np.sin(omega * time_pts))\n\n        # --- Step 2: Compute finite-difference gradient benchmark ---\n        J_p, _ = forward_sim(theta_nom + eps, scale, beta_vals_scaled)\n        J_m, _ = forward_sim(theta_nom - eps, scale, beta_vals_scaled)\n        g_fd = (J_p - J_m) / (2 * eps)\n\n        # --- Step 3: Run forward simulation for nominal theta to get u_N ---\n        _, u_N_nom = forward_sim(theta_nom, scale, beta_vals_scaled)\n\n        # --- Step 4: Run backward adjoint simulation with filtering ---\n        p = u_N_nom.copy()\n        q = u_N_nom.copy()\n        \n        # Initialize gradient and max norm for blow-up check\n        g_filt = 0.0\n        \n        z_n = q if r  1.0 else p\n        max_norm_sq = z_n @ z_n\n        \n        for n in range(N - 1, -1, -1):\n            # p_next and q_next correspond to p_{n+1} and q_{n+1}\n            p_next = p.copy()\n            q_next = q.copy()\n\n            # Add contribution to gradient sum\n            # Note: s is a 1D array, so s @ q_next is s^T q_{n+1}\n            g_filt += dt * b_vals[n] * (s @ q_next)\n\n            # Compute p_n\n            A_n = np.array([[-alpha, beta_vals_scaled[n]], [0, -alpha]])\n            prop_T = (np.eye(2) + dt * A_n).T\n            p = prop_T @ p_next\n            \n            # Compute q_n\n            q = r * p + (1 - r) * q_next\n\n            # Update max norm for blow-up check\n            z_n = q if r  1.0 else p\n            current_norm_sq = z_n @ z_n\n            if current_norm_sq > max_norm_sq:\n                max_norm_sq = current_norm_sq\n        \n        # --- Step 5: Finalize metrics ---\n        blowup = 1 if np.sqrt(max_norm_sq) > M else 0\n        \n        error = np.abs(g_filt - g_fd) / max(1.0, np.abs(g_fd))\n        \n        return blowup, error\n\n    results = []\n    for r_case, scale_case in test_cases:\n        blowup_val, error_val = compute_metrics(r_case, scale_case)\n        results.append(blowup_val)\n        results.append(error_val)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3289215"}]}