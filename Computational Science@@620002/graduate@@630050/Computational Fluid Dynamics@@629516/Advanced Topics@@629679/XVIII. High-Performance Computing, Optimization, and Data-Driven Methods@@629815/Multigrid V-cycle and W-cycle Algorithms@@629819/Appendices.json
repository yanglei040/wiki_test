{"hands_on_practices": [{"introduction": "The power of multigrid methods comes from their ability to efficiently handle error components at different length scales using a hierarchy of grids. The essential operations that move information between these grids are restriction (fine-to-coarse) and prolongation (coarse-to-fine). In this first exercise [@problem_id:3347230], you will analyze the fundamental properties of these grid-transfer operators by applying them to a simple quadratic function, allowing you to quantify the error introduced by this process and build intuition for how a coarse grid represents fine-grid data.", "problem": "Consider a prototypical grid-transfer step used in multigrid V-cycle and W-cycle iterations within Computational Fluid Dynamics (CFD) for a smooth scalar field representative of a Partial Differential Equation (PDE) solution. Let the computational domain be $\\Omega = [0,1] \\times [0,1]$, and let the fine grid be uniform with $9 \\times 9$ nodal points (i.e., $8$ intervals per direction), so the fine-grid spacing is $h = \\frac{1}{8}$. The coarse grid is defined by selecting every other fine-grid node, yielding $5 \\times 5$ coarse-grid nodes with spacing $2h$.\n\nLet the continuous field be $u(x,y) = x^{2} + y^{2}$, and suppose the fine-grid samples of $u$ are given exactly at the fine-grid nodes. Define the full-weighting restriction operator from fine to coarse at a coarse node $(x_{0},y_{0})$ by the nine-point stencil\n$$\n\\mathcal{R}_{\\text{FW}}[u] (x_{0},y_{0}) \\;=\\; \\frac{1}{16}\\Big(u(x_{0}-h,y_{0}-h) + 2\\,u(x_{0},y_{0}-h) + u(x_{0}+h,y_{0}-h) \\\\\n+ 2\\,u(x_{0}-h,y_{0}) + 4\\,u(x_{0},y_{0}) + 2\\,u(x_{0}+h,y_{0}) \\\\\n+ u(x_{0}-h,y_{0}+h) + 2\\,u(x_{0},y_{0}+h) + u(x_{0}+h,y_{0}+h)\\Big).\n$$\nThen define the prolongation from coarse to fine by cellwise bilinear interpolation: on a coarse cell with corners $(X_{0},Y_{0})$, $(X_{0}+2h,Y_{0})$, $(X_{0},Y_{0}+2h)$, $(X_{0}+2h,Y_{0}+2h)$, for any point $(x,y)$ in that cell with local coordinates $s,t \\in [0,1]$ such that $x = X_{0} + 2hs$ and $y = Y_{0} + 2ht$, the interpolant equals\n$$\n\\mathcal{P}_{\\text{bilinear}}[U_{c}](x,y) \\;=\\; (1-s)(1-t)\\,U_{c}(X_{0},Y_{0}) \\;+\\; s(1-t)\\,U_{c}(X_{0}+2h,Y_{0}) \\\\\n+\\; (1-s)t\\,U_{c}(X_{0},Y_{0}+2h) \\;+\\; s t\\,U_{c}(X_{0}+2h,Y_{0}+2h),\n$$\nwhere $U_{c}$ denotes the coarse-grid values obtained by restriction. Using these definitions, apply full-weighting to the given fine-grid samples of $u$ to obtain coarse-grid values, then apply bilinear interpolation to reconstruct approximate fine-grid values at the fine nodes. Compute the maximum pointwise interpolation error magnitude over all fine-grid nodes,\n$$\nE_{\\max} \\;=\\; \\max_{\\text{fine nodes }(x,y)} \\left|\\mathcal{P}_{\\text{bilinear}}\\big[\\mathcal{R}_{\\text{FW}}[u]\\big](x,y) \\;-\\; u(x,y)\\right|,\n$$\nand provide your answer in exact form as a single real number. No rounding is required, and no units are to be included in the final answer.", "solution": "The user wants me to solve a problem.\n\nHere's my plan:\n1.  **Validate the problem statement.**\n    *   **Extract Givens**:\n        *   Computational domain: $\\Omega = [0,1] \\times [0,1]$.\n        *   Fine grid: $9 \\times 9$ nodes, spacing $h = \\frac{1}{8}$.\n        *   Coarse grid: $5 \\times 5$ nodes, spacing $2h = \\frac{1}{4}$.\n        *   Continuous field: $u(x,y) = x^2 + y^2$.\n        *   Fine-grid values are exact samples of $u(x,y)$.\n        *   Restriction operator: $\\mathcal{R}_{\\text{FW}}$ is the full-weighting 9-point stencil.\n        *   Prolongation operator: $\\mathcal{P}_{\\text{bilinear}}$ is bilinear interpolation.\n        *   Objective: Compute $E_{\\max} = \\max_{\\text{fine nodes }(x,y)} |\\mathcal{P}_{\\text{bilinear}}[\\mathcal{R}_{\\text{FW}}[u]](x,y) - u(x,y)|$.\n    *   **Validate**:\n        *   **Scientifically Grounded**: The problem is based on standard, fundamental concepts of multigrid methods (restriction, prolongation), which are a cornerstone of numerical analysis for solving PDEs. The specific operators (full-weighting, bilinear interpolation) and the test function ($u(x,y)=x^2+y^2$) are canonical choices for analyzing the properties of such methods.\n        *   **Well-Posed**: The problem is clearly stated with all necessary definitions and values. The objective is a specific calculation leading to a unique numerical answer.\n        *   **Objective**: The problem is stated using precise mathematical language, free of ambiguity or subjective content.\n        *   **Conclusion**: The problem is valid. The mention of a continuous field $u(x,y)$ addresses the potential ambiguity of applying the restriction stencil at the boundaries of the domain by implying that the function's definition extends beyond the grid, a standard practice in truncation error analysis.\n\n2.  **Solve the problem.**\n    The solution process involves three main steps:\n    1.  Apply the restriction operator $\\mathcal{R}_{\\text{FW}}$ to the fine-grid representation of $u(x,y)$ to obtain the coarse-grid values $U_c$.\n    2.  Apply the prolongation operator $\\mathcal{P}_{\\text{bilinear}}$ to the coarse-grid values $U_c$ to obtain the interpolated fine-grid values $u_I$.\n    3.  Compute the error $E(x,y) = u_I(x,y) - u(x,y)$ and find its maximum magnitude over all fine-grid nodes.\n\n    **Step 1: Restriction**\n    Let $(x_0, y_0)$ be a coarse-grid node. The full-weighting restriction operator $\\mathcal{R}_{\\text{FW}}$ computes the coarse-grid value $U_c(x_0, y_0)$ as a weighted average of the nine fine-grid values centered at $(x_0, y_0)$. The fine-grid values are given by the continuous function $u(x,y) = x^2 + y^2$.\n    The operator is defined as:\n    $$\n    U_c(x_0, y_0) = \\frac{1}{16} \\sum_{i=-1}^{1} \\sum_{j=-1}^{1} w_{ij} u(x_0+ih, y_0+jh)\n    $$\n    where the weights are $w_{00}=4$, $w_{\\pm 1, 0}=w_{0, \\pm 1}=2$, and $w_{\\pm 1, \\pm 1}=1$.\n\n    We can analyze the action of this operator on $u(x,y)$ by expanding $u(x_0+ih, y_0+jh)$ in a Taylor series around $(x_0, y_0)$. Since $u(x,y)$ is a quadratic polynomial, the Taylor expansion is exact and finite:\n    $$\n    u(x_0+ih, y_0+jh) = u(x_0, y_0) + (ih) \\frac{\\partial u}{\\partial x} + (jh) \\frac{\\partial u}{\\partial y} + \\frac{1}{2} \\left( (ih)^2 \\frac{\\partial^2 u}{\\partial x^2} + (jh)^2 \\frac{\\partial^2 u}{\\partial y^2} + 2(ih)(jh) \\frac{\\partial^2 u}{\\partial x \\partial y} \\right)\n    $$\n    For $u(x,y) = x^2 + y^2$, the partial derivatives at $(x_0, y_0)$ are: $\\frac{\\partial u}{\\partial x} = 2x_0$, $\\frac{\\partial u}{\\partial y} = 2y_0$, $\\frac{\\partial^2 u}{\\partial x^2} = 2$, $\\frac{\\partial^2 u}{\\partial y^2} = 2$, and $\\frac{\\partial^2 u}{\\partial x \\partial y} = 0$.\n    Substituting these into the expansion gives:\n    $$\n    u(x_0+ih, y_0+jh) = (x_0^2+y_0^2) + 2x_0 i h + 2y_0 j h + i^2 h^2 + j^2 h^2\n    $$\n    Now, we compute the weighted sum:\n    $$\n    U_c(x_0, y_0) = \\frac{1}{16} \\sum_{i,j} w_{ij} \\left( u(x_0, y_0) + 2x_0 i h + 2y_0 j h + i^2 h^2 + j^2 h^2 \\right)\n    $$\n    By linearity of summation:\n    $$\n    U_c(x_0, y_0) = \\frac{1}{16} \\left( u(x_0, y_0)\\sum_{i,j} w_{ij} + 2x_0 h \\sum_{i,j} w_{ij}i + 2y_0 h \\sum_{i,j} w_{ij}j + h^2 \\sum_{i,j} w_{ij}i^2 + h^2 \\sum_{i,j} w_{ij}j^2 \\right)\n    $$\n    We evaluate the sums of weighted indices:\n    $\\sum_{i,j} w_{ij} = 1+2+1+2+4+2+1+2+1 = 16$.\n    Due to the symmetry of the weights ($w_{ij} = w_{-i,j}$ and $w_{ij}=w_{i,-j}$):\n    $\\sum_{i,j} w_{ij} i = 0$ and $\\sum_{i,j} w_{ij} j = 0$.\n    The sum for the $i^2$ term is:\n    $\\sum_{i,j} w_{ij} i^2 = \\sum_{j} (w_{-1,j}(-1)^2 + w_{0,j}(0)^2 + w_{1,j}(1)^2) = \\sum_{j} (w_{-1,j} + w_{1,j})$.\n    For $j \\in \\{-1, 1\\}$, $w_{-1,j}+w_{1,j} = 1+1=2$. For $j=0$, $w_{-1,0}+w_{1,0} = 2+2=4$.\n    So, $\\sum_{i,j} w_{ij} i^2 = 2 + 4 + 2 = 8$. By symmetry, $\\sum_{i,j} w_{ij} j^2 = 8$.\n\n    Substituting these sums back into the expression for $U_c$:\n    $$\n    U_c(x_0, y_0) = \\frac{1}{16} \\left( 16 u(x_0, y_0) + 0 + 0 + 8h^2 + 8h^2 \\right) = u(x_0, y_0) + \\frac{16h^2}{16} = u(x_0, y_0) + h^2\n    $$\n    So, the coarse-grid values are $U_c(x,y) = x^2+y^2+h^2$ at each coarse-grid node $(x,y)$.\n\n    **Step 2: Prolongation**\n    Next, we apply bilinear interpolation $\\mathcal{P}_{\\text{bilinear}}$ to the coarse-grid data $U_c$ to get values $u_I$ on the fine grid. The operator is linear, so we can write:\n    $$\n    u_I(x,y) = \\mathcal{P}_{\\text{bilinear}}[U_c](x,y) = \\mathcal{P}_{\\text{bilinear}}[u+h^2](x,y) = \\mathcal{P}_{\\text{bilinear}}[u](x,y) + \\mathcal{P}_{\\text{bilinear}}[h^2](x,y)\n    $$\n    Bilinear interpolation of a constant value ($h^2$) results in that same constant value everywhere. Thus, $\\mathcal{P}_{\\text{bilinear}}[h^2](x,y) = h^2$.\n    The equation simplifies to:\n    $$\n    u_I(x,y) = \\mathcal{P}_{\\text{bilinear}}[u](x,y) + h^2\n    $$\n\n    **Step 3: Error Calculation**\n    The error at a fine-grid point $(x,y)$ is:\n    $$\n    E(x,y) = u_I(x,y) - u(x,y) = \\left( \\mathcal{P}_{\\text{bilinear}}[u](x,y) - u(x,y) \\right) + h^2\n    $$\n    The term in the parenthesis is the standard error of bilinear interpolation for the function $u(x,y)=x^2+y^2$. Let's compute this error.\n    Consider a coarse cell with corners at $(X_0, Y_0), (X_0+2h, Y_0), (X_0, Y_0+2h), (X_0+2h, Y_0+2h)$. A point $(x,y)$ in this cell is parameterized by $x = X_0 + s(2h)$ and $y = Y_0 + t(2h)$ with $s,t \\in [0,1]$.\n    The bilinear interpolation operator is also linear with respect to the function being interpolated, so $\\mathcal{P}_{\\text{bilinear}}[x^2+y^2] = \\mathcal{P}_{\\text{bilinear}}[x^2] + \\mathcal{P}_{\\text{bilinear}}[y^2]$.\n    Let's find the interpolation error for $f(z)=z^2$ using 1D linear interpolation on $[Z_0, Z_0+L]$. The interpolant is $f_I(z) = (1-\\sigma)Z_0^2 + \\sigma(Z_0+L)^2$ where $z=Z_0+\\sigma L$.\n    The error is $f_I(z) - z^2 = ((1-\\sigma)Z_0^2 + \\sigma(Z_0+L)^2) - (Z_0+\\sigma L)^2 = L^2\\sigma(1-\\sigma)$.\n    For bilinear interpolation of $u(x,y)=x^2+y^2$, we have:\n    $\\mathcal{P}_{\\text{bilinear}}[x^2](x,y) = x^2 + (2h)^2s(1-s) = x^2+4h^2s(1-s)$.\n    $\\mathcal{P}_{\\text{bilinear}}[y^2](x,y) = y^2 + (2h)^2t(1-t) = y^2+4h^2t(1-t)$.\n    The interpolation error for $u$ is:\n    $$\n    \\mathcal{P}_{\\text{bilinear}}[u](x,y) - u(x,y) = (x^2+4h^2s(1-s) + y^2+4h^2t(1-t)) - (x^2+y^2) = 4h^2s(1-s) + 4h^2t(1-t)\n    $$\n    The total error is therefore:\n    $$\n    E(x,y) = \\left( 4h^2s(1-s) + 4h^2t(1-t) \\right) + h^2 = h^2(1 + 4s(1-s) + 4t(1-t))\n    $$\n\n    **Step 4: Maximization**\n    We need to find the maximum of $|E(x,y)|$ over all fine-grid nodes. Since $h>0$ and $s,t\\in[0,1]$, the terms $s(1-s)$ and $t(1-t)$ are non-negative, so $E(x,y)$ is always positive. Thus, $|E(x,y)|=E(x,y)$.\n    The fine-grid nodes are located at coordinates $(ih, jh)$ for $i,j \\in \\{0, 1, \\dots, 8\\}$. A fine node $(ih, jh)$ has local coordinates $(s,t)$ within a coarse cell given by $s = (i \\pmod 2)/2$ and $t = (j \\pmod 2)/2$.\n    The possible values for $s$ and $t$ at fine-grid nodes are $0$ and $1/2$.\n    We need to maximize $E(x,y)$ for $(s,t) \\in \\{0, 1/2\\} \\times \\{0, 1/2\\}$.\n    The function $f(\\sigma) = \\sigma(1-\\sigma)$ is $0$ for $\\sigma=0$ and $1/4$ for $\\sigma=1/2$.\n    The error function $E$ is maximized when $s(1-s)$ and $t(1-t)$ are maximized, which occurs at $s=1/2$ and $t=1/2$.\n    This corresponds to fine-grid nodes with odd indices $(i,j)$, which are the centers of the coarse grid cells.\n    The maximum error is:\n    $$\n    E_{\\max} = h^2\\left(1 + 4\\left(\\frac{1}{2}\\right)\\left(1-\\frac{1}{2}\\right) + 4\\left(\\frac{1}{2}\\right)\\left(1-\\frac{1}{2}\\right)\\right) = h^2\\left(1 + 4\\left(\\frac{1}{4}\\right) + 4\\left(\\frac{1}{4}\\right)\\right)\n    $$\n    $$\n    E_{\\max} = h^2(1 + 1 + 1) = 3h^2\n    $$\n    The problem provides $h = \\frac{1}{8}$. Substituting this value:\n    $$\n    E_{\\max} = 3 \\left(\\frac{1}{8}\\right)^2 = 3 \\left(\\frac{1}{64}\\right) = \\frac{3}{64}\n    $$", "answer": "$$\\boxed{\\frac{3}{64}}$$", "id": "3347230"}, {"introduction": "While grid transfers handle the communication between scales, the actual error reduction on any given grid is performed by a \"smoother\". An effective smoother does not need to solve the problem efficiently, but it must be excellent at damping high-frequency, oscillatory error components, leaving the smooth, low-frequency error to be resolved on the coarser grid. This hands-on coding practice [@problem_id:3347195] guides you through implementing a Red-Black Gauss-Seidel smoother and quantifying its damping effect on different error modes, providing a clear demonstration of this critical principle.", "problem": "Consider the Poisson equation $-\\Delta u = f$ on the unit square $\\Omega = (0,1)\\times(0,1)$ with homogeneous Dirichlet boundary conditions $u=0$ on $\\partial \\Omega$. Use a uniform grid of $N \\times N$ interior points with $N=32$, grid spacing $h = 1/(N+1)$, and the standard $5$-point finite difference discretization for the negative Laplacian. The resulting linear system is $A u = f_h$, where $A$ is the sparse matrix corresponding to the $5$-point stencil and $f_h$ is the right-hand side sampled at grid points. The pre-smoother in a multigrid method applies one sweep of Red–Black Gauss–Seidel (RBGS), which partitions interior grid points into two colors by the parity of the sum of their integer indices: \"red\" if $(i+j)$ is even and \"black\" if $(i+j)$ is odd, and updates red points first and black points second. Each update replaces $u_{i,j}$ by the average of its four nearest neighbors plus a forcing term, consistent with the $5$-point discretization of $-\\Delta$.\n\nYour tasks are:\n- Implement one RBGS pre-smoothing sweep for the discrete system $A u = f_h$ using the $5$-point stencil for $-\\Delta$. Use homogeneous Dirichlet boundary conditions at the outer ghost layers, meaning values outside the interior grid are fixed to zero and not updated. The right-hand side is $f(x,y) = \\sin(\\pi x)\\sin(\\pi y)$ sampled at the interior grid points $(x_i,y_j) = (i h, j h)$ for $i,j \\in \\{1,\\ldots,N\\}$.\n- Quantify the smoothing (damping) of an error Fourier mode by the RBGS pre-smoother. To do so, consider the linear stationary iteration induced by RBGS. For such an iteration, the error $e^k = u^k - u^\\star$ evolves as $e^{k+1} = S e^k$ with an error-propagation operator $S$ that does not depend on the right-hand side. Therefore, the damping of a specific error mode can be evaluated by setting $f_h \\equiv 0$, initializing the interior unknowns to a prescribed error mode $e^0$, applying one RBGS sweep, and measuring the ratio of the norms $\\|e^1\\|_2 / \\|e^0\\|_2$. Define the error mode by\n$$\ne^0_{i,j} = \\cos(\\theta_x i)\\cos(\\theta_y j), \\quad \\text{for } i,j \\in \\{1,\\ldots,N\\},\n$$\nwith the angle unit in radians. After one RBGS sweep with $f_h \\equiv 0$, let $e^1$ denote the updated interior field, and compute the damping factor\n$$\n\\mu(\\theta_x,\\theta_y) = \\frac{\\|e^1\\|_2}{\\|e^0\\|_2},\n$$\nwhere $\\|\\cdot\\|_2$ is the Euclidean norm over the $N \\times N$ interior grid.\n\nAssumptions and definitions to be used:\n- The $5$-point discretization of $-\\Delta$ on the uniform grid yields, for each interior node $(i,j)$,\n$$\n\\frac{4 u_{i,j} - u_{i-1,j} - u_{i+1,j} - u_{i,j-1} - u_{i,j+1}}{h^2} = f_{i,j}.\n$$\n- The RBGS update at interior node $(i,j)$ replaces $u_{i,j}$ by\n$$\nu_{i,j} \\leftarrow \\frac{1}{4}\\left(u_{i-1,j} + u_{i+1,j} + u_{i,j-1} + u_{i,j+1} + h^2 f_{i,j}\\right),\n$$\napplied first to all red nodes using current black neighbors, then to all black nodes using updated red neighbors. Values outside the interior grid are treated as zero during neighbor access due to the homogeneous Dirichlet boundary condition.\n\nTest suite and required outputs:\n- Use $N=32$ and $h = 1/(N+1)$. Construct $f_{i,j} = \\sin(\\pi x_i)\\sin(\\pi y_j)$ with $(x_i,y_j) = (i h, j h)$ for the pre-smoothing operator. For quantifying damping, set $f_{i,j} \\equiv 0$ and use the error mode initialization described above.\n- Evaluate the damping factor $\\mu(\\theta_x,\\theta_y)$ for the following three cases (angles in radians):\n    - Case $1$: $(\\theta_x,\\theta_y) = (\\pi/2,\\pi/2)$.\n    - Case $2$: $(\\theta_x,\\theta_y) = (\\pi,\\pi)$.\n    - Case $3$: $(\\theta_x,\\theta_y) = (\\pi/16,\\pi/16)$.\n- Your program must compute the three damping factors in the order above and produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[r_1,r_2,r_3]$). The entries must be floating-point numbers. The angle unit is radians, and there are no physical units to report.\n\nConstraints and expectations:\n- Begin from the fundamental definitions: the Poisson equation $-\\Delta u = f$, the standard $5$-point discretization, and the iterative RBGS smoother. Do not invoke any specialized multigrid formulas beyond these definitions.\n- Ensure scientific realism by using the stated grid, boundary conditions, and forcing function. The damping factors are to be computed numerically as specified, not by external formulas.", "solution": "The user-provided problem is assessed to be **valid**. It is scientifically sound, well-posed, and objective, presenting a standard problem in the field of numerical analysis for partial differential equations. All necessary parameters and definitions are provided, and the task is a direct numerical implementation of a well-defined algorithm.\n\nThe problem asks for an analysis of the smoothing properties of a single Red-Black Gauss-Seidel (RBGS) sweep, a common component in multigrid solvers. The analysis is to be performed by quantifying the damping of specific error modes for the 2D Poisson equation on a unit square.\n\nLet the continuous problem be the Poisson equation with homogeneous Dirichlet boundary conditions:\n$$\n-\\Delta u = f \\quad \\text{in } \\Omega = (0,1) \\times (0,1)\n$$\n$$\nu = 0 \\quad \\text{on } \\partial\\Omega\n$$\nWe discretize this problem on a uniform grid with $N \\times N$ interior points, where $N=32$. The grid points are $(x_i, y_j) = (ih, jh)$ for $i,j \\in \\{0, 1, \\dots, N+1\\}$, with grid spacing $h = 1/(N+1)$. The interior points correspond to indices $i,j \\in \\{1, \\dots, N\\}$. The values on the boundary, where $i$ or $j$ are $0$ or $N+1$, are fixed at $u_{i,j}=0$.\n\nUsing the standard $5$-point finite difference stencil for the negative Laplacian, $-\\Delta$, at an interior grid point $(i,j)$, we obtain the discrete equation:\n$$\n\\frac{4 u_{i,j} - u_{i-1,j} - u_{i+1,j} - u_{i,j-1} - u_{i,j+1}}{h^2} = f_{i,j}\n$$\nwhere $f_{i,j} = f(x_i, y_j)$. This forms a large, sparse linear system of equations, $A \\mathbf{u} = \\mathbf{f}_h$.\n\nThe Red-Black Gauss-Seidel (RBGS) method is an iterative technique to solve this system. It reorders the grid points into two sets: \"red\" points, where the sum of indices $(i+j)$ is even, and \"black\" points, where $(i+j)$ is odd. A single RBGS sweep consists of two stages:\n1.  First, update the value $u_{i,j}$ for all red points simultaneously. The update rule is derived by solving the stencil equation for $u_{i,j}$:\n    $$\n    u_{i,j} \\leftarrow \\frac{1}{4} \\left( u_{i-1,j} + u_{i+1,j} + u_{i,j-1} + u_{i,j+1} + h^2 f_{i,j} \\right)\n    $$\n    Critically, for a red point, all its four neighbors $(i\\pm 1, j)$ and $(i, j\\pm 1)$ are black. Therefore, all red points can be updated in parallel using the current values of their black neighbors.\n2.  Second, update the value $u_{i,j}$ for all black points simultaneously. This update uses the same formula but now relies on the **newly computed** values at the neighboring red points from the first stage.\n\nThe purpose of a smoother in a multigrid context is to damp the high-frequency components of the error. The error, $\\mathbf{e}^k = \\mathbf{u}^k - \\mathbf{u}^*$, where $\\mathbf{u}^*$ is the exact discrete solution, evolves according to an error propagation operator $S$ such that $\\mathbf{e}^{k+1} = S \\mathbf{e}^k$. This operator is independent of the right-hand side vector $\\mathbf{f}_h$. Consequently, we can analyze its properties by setting $\\mathbf{f}_h = \\mathbf{0}$ and observing the smoother's action on an initial error field, $\\mathbf{e}^0$.\n\nWe are tasked to compute the damping factor for specific initial error modes of the form:\n$$\ne^0_{i,j} = \\cos(\\theta_x i) \\cos(\\theta_y j), \\quad \\text{for } i,j \\in \\{1, \\dots, N\\}\n$$\nThe parameters $(\\theta_x, \\theta_y)$ determine the frequency of the error mode. Small values correspond to low-frequency (smooth) error, while values close to $\\pi$ correspond to high-frequency (oscillatory) error.\n\nAfter applying one RBGS sweep to an initial error field $\\mathbf{e}^0$ (with $\\mathbf{f}_h = \\mathbf{0}$), we obtain a new error field $\\mathbf{e}^1$. The damping factor $\\mu$ is defined as the ratio of the Euclidean norms of the resulting and initial error fields over the interior grid:\n$$\n\\mu(\\theta_x, \\theta_y) = \\frac{\\|\\mathbf{e}^1\\|_2}{\\|\\mathbf{e}^0\\|_2} = \\frac{\\sqrt{\\sum_{i=1}^N \\sum_{j=1}^N (e^1_{i,j})^2}}{\\sqrt{\\sum_{i=1}^N \\sum_{j=1}^N (e^0_{i,j})^2}}\n$$\nA value of $\\mu < 1$ indicates that the error component is damped, with smaller values signifying more effective smoothing. A value of $\\mu \\approx 1$ indicates poor damping for that mode.\n\nTo implement this numerically, we will use a $(N+2) \\times (N+2)$ grid to naturally incorporate the homogeneous Dirichlet boundary conditions. The procedure for each test case $(\\theta_x, \\theta_y)$ is:\n1.  Initialize a grid for the error $\\mathbf{e}^0$ of size $(N+2) \\times (N+2)$ with zeros.\n2.  Populate the interior points $e^0_{i,j}$ for $i,j \\in \\{1,\\dots,N\\}$ using the specified cosine formula.\n3.  Calculate the initial norm $\\|\\mathbf{e}^0\\|_2$ over the interior grid.\n4.  Perform one RBGS sweep on $\\mathbf{e}^0$ to compute $\\mathbf{e}^1$. This involves a red pass followed by a black pass, correctly handling the data dependencies as described above. The $h^2 f_{i,j}$ term is zero for this error analysis.\n5.  Calculate the final norm $\\|\\mathbf{e}^1\\|_2$ over the interior grid.\n6.  Compute the ratio $\\mu = \\|\\mathbf{e}^1\\|_2 / \\|\\mathbf{e}^0\\|_2$.\n\nThis procedure is repeated for the three specified pairs of $(\\theta_x, \\theta_y)$, and the resulting damping factors are reported.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef calculate_damping_factor(N, theta_x, theta_y):\n    \"\"\"\n    Calculates the damping factor of a single Red-Black Gauss-Seidel sweep\n    for a given error mode.\n\n    Args:\n        N (int): The number of interior grid points in one dimension.\n        theta_x (float): The frequency parameter for the x-direction of the error mode.\n        theta_y (float): The frequency parameter for the y-direction of the error mode.\n\n    Returns:\n        float: The damping factor mu = ||e^1||_2 / ||e^0||_2.\n    \"\"\"\n    # Grid setup including boundary layers\n    # Grid is (N+2)x(N+2). Interior is [1:N+1, 1:N+1].\n    e0 = np.zeros((N + 2, N + 2))\n\n    # Create grid indices for the interior\n    i_indices, j_indices = np.mgrid[1:N + 1, 1:N + 1]\n\n    # Initialize the interior of the grid with the specified error mode\n    e0[1:N + 1, 1:N + 1] = np.cos(theta_x * i_indices) * np.cos(theta_y * j_indices)\n\n    # Calculate the L2 norm of the initial error field over the interior\n    norm_e0 = np.linalg.norm(e0[1:N + 1, 1:N + 1])\n    \n    # If the initial norm is zero, the concept of a damping ratio is ill-defined.\n    # For the given test cases this will not happen.\n    if norm_e0 == 0:\n        return 1.0\n\n    # The grid that will be updated by the RBGS sweep\n    e1 = e0.copy()\n\n    # Create masks for red and black points in the interior\n    # Red points: (i+j) is even\n    # Black points: (i+j) is odd\n    red_mask = ((i_indices + j_indices) % 2) == 0\n    black_mask = ((i_indices + j_indices) % 2) == 1\n\n    # Get a view of the interior of the grid to be updated\n    e1_interior = e1[1:N + 1, 1:N + 1]\n\n    # --- RBGS Sweep (with f_h = 0) ---\n\n    # 1. Red Pass: Update all red points.\n    # The update for each red point depends only on its black neighbors.\n    # These updates can be computed simultaneously using the state before the sweep.\n    # We use slicing to get neighbor values.\n    # e1[i-1, j] -> e1[0:N, 1:N+1]\n    # e1[i+1, j] -> e1[2:N+2, 1:N+1]\n    # e1[i, j-1] -> e1[1:N+1, 0:N]\n    # e1[i, j+1] -> e1[1:N+1, 2:N+2]\n    neighbors_sum = (e1[0:N, 1:N+1] + e1[2:N+2, 1:N+1] +\n                     e1[1:N+1, 0:N] + e1[1:N+1, 2:N+2])\n    \n    # The update uses the average of the neighbors. Since f_h=0, there is no source term.\n    # Masked assignment updates only the red points.\n    e1_interior[red_mask] = 0.25 * neighbors_sum[red_mask]\n\n    # 2. Black Pass: Update all black points.\n    # The update for each black point depends on its red neighbors.\n    # We must use the *updated* values of the red neighbors from the previous pass.\n    # By re-calculating the sum of neighbors now, we are implicitly using the\n    # updated values for red points stored in e1.\n    neighbors_sum = (e1[0:N, 1:N+1] + e1[2:N+2, 1:N+1] +\n                     e1[1:N+1, 0:N] + e1[1:N+1, 2:N+2])\n                     \n    # Masked assignment updates only the black points.\n    e1_interior[black_mask] = 0.25 * neighbors_sum[black_mask]\n\n    # --- End of RBGS Sweep ---\n\n    # Calculate the L2 norm of the final error field over the interior\n    norm_e1 = np.linalg.norm(e1_interior)\n\n    # Return the damping factor\n    return norm_e1 / norm_e0\n\ndef solve():\n    \"\"\"\n    Main function to run the test cases and print the results.\n    \"\"\"\n    # Define the problem parameters and test cases.\n    N = 32\n    test_cases = [\n        # Case 1: High-frequency mode\n        (np.pi / 2, np.pi / 2),\n        # Case 2: Highest frequency (checkerboard) mode\n        (np.pi, np.pi),\n        # Case 3: Low-frequency mode\n        (np.pi / 16, np.pi / 16),\n    ]\n\n    results = []\n    for theta_x, theta_y in test_cases:\n        damping_factor = calculate_damping_factor(N, theta_x, theta_y)\n        results.append(damping_factor)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3347195"}, {"introduction": "Having explored grid transfers and smoothers, we can now assemble a full V-cycle to solve a problem. This final practice moves into a more advanced and realistic scenario often seen in CFD: solving a singular system arising from Neumann boundary conditions. When an operator has a nullspace, this property is inherited by the coarse-grid operators and requires special handling to prevent catastrophic convergence failure. This exercise [@problem_id:3347220] will have you implement a V-cycle and contrast a \"naive\" coarse-grid solver with one that correctly manages the nullspace, demonstrating a critical lesson for robust multigrid design.", "problem": "Consider the one-dimensional pressure Poisson subproblem with homogeneous Neumann boundary conditions, a canonical subproblem in computational fluid dynamics. The continuous model is the Laplace equation on the closed interval $[0,1]$ with $u(x)$ satisfying\n$$\n\\frac{d^2 u}{dx^2} = f(x), \\quad \\frac{du}{dx}(0) = 0, \\quad \\frac{du}{dx}(1) = 0,\n$$\nfor a source term $f(x)$ whose integral over $[0,1]$ is zero. The integral constraint is necessary and sufficient for solvability and expresses the presence of a constant nullspace of the Neumann Laplacian.\n\nDiscretize the interval $[0,1]$ using $n$ equally spaced grid points $\\{x_i\\}_{i=0}^{n-1}$ with $x_i = i/(n-1)$ and standard second-order finite differences that enforce the homogeneous Neumann boundary conditions with one-sided differences at the endpoints. The resulting discrete operator $\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$ is the symmetric positive semidefinite one-dimensional Neumann Laplacian defined componentwise by\n$$\n\\mathbf{A}_{0,0} = 1, \\quad \\mathbf{A}_{0,1} = -1,\n$$\n$$\n\\mathbf{A}_{i,i-1} = -1, \\quad \\mathbf{A}_{i,i} = 2, \\quad \\mathbf{A}_{i,i+1} = -1 \\quad \\text{for} \\quad i = 1,2,\\dots,n-2,\n$$\n$$\n\\mathbf{A}_{n-1,n-2} = -1, \\quad \\mathbf{A}_{n-1,n-1} = 1,\n$$\nand all other entries are zero. The constant vector $\\mathbf{1} = [1,1,\\dots,1]^\\top$ satisfies $\\mathbf{A}\\mathbf{1} = \\mathbf{0}$ and spans the nullspace. A compatible right-hand side vector $\\mathbf{b} \\in \\mathbb{R}^n$ must satisfy $\\sum_{i=0}^{n-1} b_i = 0$, otherwise the system is inconsistent.\n\nDefine a standard geometric two-level multigrid V-cycle to approximately solve $\\mathbf{A}\\mathbf{u} = \\mathbf{b}$ from the initial guess $\\mathbf{u}^{(0)}=\\mathbf{0}$ using:\n- The weighted Jacobi smoother with weight $\\omega \\in (0,1)$ and $\\nu_{\\mathrm{pre}}$ pre-smoothing sweeps and $\\nu_{\\mathrm{post}}$ post-smoothing sweeps:\n$$\n\\mathbf{u} \\leftarrow \\mathbf{u} + \\omega \\mathbf{D}^{-1}(\\mathbf{b}-\\mathbf{A}\\mathbf{u}),\n$$\nwhere $\\mathbf{D} = \\mathrm{diag}(\\mathbf{A})$.\n- Linear interpolation prolongation $\\mathbf{P} \\in \\mathbb{R}^{n_f \\times n_c}$ from the coarse grid of size $n_c$ to the fine grid of size $n_f$, with $n_f = 2 n_c - 1$, defined by:\n  - For coarse index $j=0,1,\\dots,n_c-1$, let the corresponding fine even index be $i=2j$. Then set\n  $$\n  P_{i,j} = 1.\n  $$\n  - For fine odd indices $i=1,3,\\dots,n_f-2$, define\n  $$\n  P_{i,\\lfloor i/2 \\rfloor} = \\tfrac{1}{2}, \\quad P_{i,\\lfloor i/2 \\rfloor + 1} = \\tfrac{1}{2}.\n  $$\n- Full-weighting restriction $\\mathbf{R} \\in \\mathbb{R}^{n_c \\times n_f}$ from fine to coarse defined by, for $j=0,1,\\dots,n_c-1$ with the corresponding fine even index $i=2j$,\n  - At the left boundary $j=0$:\n  $$\n  R_{0,0} = \\tfrac{1}{2}, \\quad R_{0,1} = \\tfrac{1}{2}.\n  $$\n  - At interior coarse indices $j=1,2,\\dots,n_c-2$:\n  $$\n  R_{j,i-1} = \\tfrac{1}{4}, \\quad R_{j,i} = \\tfrac{1}{2}, \\quad R_{j,i+1} = \\tfrac{1}{4}.\n  $$\n  - At the right boundary $j=n_c-1$:\n  $$\n  R_{n_c-1,n_f-2} = \\tfrac{1}{2}, \\quad R_{n_c-1,n_f-1} = \\tfrac{1}{2}.\n  $$\n- The Galerkin coarse-grid operator:\n$$\n\\mathbf{A}_c = \\mathbf{R}\\mathbf{A}_f\\mathbf{P},\n$$\nwhere $\\mathbf{A}_f$ is the fine-grid operator. Note $\\mathbf{A}_c$ is singular with the same constant nullspace.\n\nDefine two coarse-grid solvers for the residual equation $\\mathbf{A}_c \\mathbf{e}_c = \\mathbf{r}_c$:\n- A nullspace-respecting coarse solver that enforces the zero-mean constraint by first projecting the coarse residual into the range of $\\mathbf{A}_c$ and then returning a coarse correction orthogonal to the nullspace:\n$$\n\\mathbf{r}_c \\leftarrow \\mathbf{r}_c - \\left(\\frac{1}{n_c}\\sum_{j=0}^{n_c-1} (\\mathbf{r}_c)_j \\right)\\mathbf{1}, \\quad \\mathbf{e}_c = \\mathbf{A}_c^{\\dagger}\\mathbf{r}_c, \\quad \\mathbf{e}_c \\leftarrow \\mathbf{e}_c - \\left(\\frac{1}{n_c}\\sum_{j=0}^{n_c-1} (\\mathbf{e}_c)_j \\right)\\mathbf{1},\n$$\nwhere $\\mathbf{A}_c^{\\dagger}$ denotes the Moore–Penrose pseudoinverse.\n- A failing coarse solver that uses the Moore–Penrose pseudoinverse without compensating for the nullspace, i.e.,\n$$\n\\mathbf{e}_c = \\mathbf{A}_c^{\\dagger}\\mathbf{r}_c,\n$$\nwith no projection of $\\mathbf{r}_c$ or $\\mathbf{e}_c$ onto the zero-mean subspace.\n\nConsider a single V-cycle update on the fine grid of size $n_f$ with initial iterate $\\mathbf{u}^{(0)}=\\mathbf{0}$, pre-smoothing, residual restriction and coarse solve, prolongation and correction addition, and post-smoothing. Let $\\mathbf{u}^{\\star}$ denote the exact discrete zero-mean solution of $\\mathbf{A}\\mathbf{u}=\\mathbf{b}$ chosen to satisfy $\\sum_i u_i^{\\star} = 0$. Define the error reduction factor after one V-cycle as\n$$\n\\rho = \\frac{\\|\\mathbf{u}^{\\star}-\\mathbf{u}^{(1)}\\|_2}{\\|\\mathbf{u}^{\\star}-\\mathbf{u}^{(0)}\\|_2} = \\frac{\\|\\mathbf{u}^{\\star}-\\mathbf{u}^{(1)}\\|_2}{\\|\\mathbf{u}^{\\star}\\|_2},\n$$\nwhere $\\|\\cdot\\|_2$ denotes the Euclidean norm.\n\nImplement a program that constructs the above operators and performs one V-cycle for both coarse solvers (nullspace-respecting and failing) and computes the corresponding error reduction factors $\\rho_{\\mathrm{respect}}$ and $\\rho_{\\mathrm{fail}}$.\n\nUse the following test suite of parameter values and right-hand sides:\n- Test case $1$ (happy path): $n_f=9$, $\\omega=2/3$, $\\nu_{\\mathrm{pre}}=2$, $\\nu_{\\mathrm{post}}=2$, and $b_i = \\sin(2\\pi x_i)$ for $i=0,1,\\dots,n_f-1$.\n- Test case $2$ (boundary-size coarse grid): $n_f=5$, $\\omega=2/3$, $\\nu_{\\mathrm{pre}}=1$, $\\nu_{\\mathrm{post}}=1$, and $b_i = \\sin(2\\pi x_i)$ for $i=0,1,\\dots,n_f-1$.\n- Test case $3$ (low-frequency source): $n_f=7$, $\\omega=0.7$, $\\nu_{\\mathrm{pre}}=3$, $\\nu_{\\mathrm{post}}=3$, and $b_i = \\cos(\\pi x_i) - \\frac{1}{n_f}\\sum_{k=0}^{n_f-1}\\cos(\\pi x_k)$ for $i=0,1,\\dots,n_f-1$.\n\nFor each test case, compute the pair of floats $[\\rho_{\\mathrm{respect}}, \\rho_{\\mathrm{fail}}]$ after one V-cycle starting from $\\mathbf{u}^{(0)}=\\mathbf{0}$. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets with nested lists for the three test cases, in the exact format\n$$\n[[\\rho_{\\mathrm{respect}}^{(1)},\\rho_{\\mathrm{fail}}^{(1)}],[\\rho_{\\mathrm{respect}}^{(2)},\\rho_{\\mathrm{fail}}^{(2)}],[\\rho_{\\mathrm{respect}}^{(3)},\\rho_{\\mathrm{fail}}^{(3)}]],\n$$\nwhere superscripts indicate the test case index. No physical units, angles, or percentages are involved; all outputs are dimensionless floats.", "solution": "We begin from the fundamental properties of the Laplace operator with homogeneous Neumann boundary conditions, which naturally arises in computational fluid dynamics pressure correction steps. The continuous operator has a constant nullspace due to the boundary flux being zero and the domain having no Dirichlet anchoring; physically, adding a constant to the pressure does not change the velocity field. Discretization using second-order finite differences that implement homogeneous Neumann conditions at the endpoints yields a symmetric positive semidefinite matrix $\\mathbf{A}$ with row sums equal to zero and the constant vector $\\mathbf{1}$ in its nullspace. Consequently, for solvability of $\\mathbf{A}\\mathbf{u}=\\mathbf{b}$, the discrete source must satisfy the compatibility condition $\\sum_i b_i = 0$.\n\nMultigrid methods address the error smoothing and coarse-grid correction across scales. The V-cycle consists of smoothing steps on the fine grid, restricting the remaining error (residual) to the coarse grid, solving or approximately solving the coarse problem, prolongating the correction to the fine level, and post-smoothing. For weighted Jacobi with diagonal matrix $\\mathbf{D}$, one sweep updates the iterate via $\\mathbf{u} \\leftarrow \\mathbf{u} + \\omega \\mathbf{D}^{-1}(\\mathbf{b}-\\mathbf{A}\\mathbf{u})$. Weighted Jacobi damps high-frequency error components effectively.\n\nThe intergrid transfer operators must be selected to respect the geometry and the boundary conditions. Linear interpolation prolongation $\\mathbf{P}$ maps coarse entries to the fine grid by exact injection at even indices and averaging to infer odd indices. Full-weighting restriction $\\mathbf{R}$ maps a fine vector to a coarse vector by local averaging consistent with the stencil; at boundaries, the weights adjust to account for missing neighbors. The Galerkin coarse-grid operator is $\\mathbf{A}_c = \\mathbf{R}\\mathbf{A}_f\\mathbf{P}$ and inherits symmetry and the nullspace. Specifically, the constant vector on the coarse grid is in the nullspace, and thus $\\mathbf{A}_c$ is singular.\n\nA subtle but critical point for Neumann problems is the treatment of the nullspace on the coarse level. The residual equation $\\mathbf{A}_c \\mathbf{e}_c = \\mathbf{r}_c$ has a solution if and only if $\\sum_j (\\mathbf{r}_c)_j = 0$ (because the range of $\\mathbf{A}_c$ consists of zero-sum vectors). Even if the fine residual $\\mathbf{r}_f$ has zero sum—which it does for any fine iterate because $\\sum_i (\\mathbf{A}_f \\mathbf{u})_i = \\sum_i \\sum_k A_{ik} u_k = \\sum_k \\left(\\sum_i A_{ik}\\right) u_k = 0$ by zero row sum and hence $\\sum_i r_i = \\sum_i b_i = 0$—the choice of restriction can produce a coarse residual $\\mathbf{r}_c$ that does not maintain exact zero sum. Full weighting is conservative locally, but globally the summed weights vary, and unless the restriction is specially designed to preserve the mean, $\\sum_j (\\mathbf{r}_c)_j$ need not equal zero.\n\nIf the coarse-grid solve ignores the nullspace constraint and attempts a raw pseudoinverse solution, it will compute a least-squares correction that includes arbitrary components along the nullspace. On the coarse grid, the solution is not unique: adding any multiple of the constant vector to $\\mathbf{e}_c$ yields another solution because $\\mathbf{A}_c \\mathbf{1} = \\mathbf{0}$. Prolongating such a correction introduces a spurious constant mode on the fine grid. Weighted Jacobi and the fine-grid operator cannot remove constant-mode error because it lies in the nullspace of $\\mathbf{A}_f$; thus, the V-cycle will fail to reduce this component, increasing the error reduction factor.\n\nTo quantify this effect, we compute the error reduction factor $\\rho = \\|\\mathbf{u}^{\\star}-\\mathbf{u}^{(1)}\\|_2 / \\|\\mathbf{u}^{\\star}\\|_2$ after a single V-cycle from $\\mathbf{u}^{(0)}=\\mathbf{0}$. Here $\\mathbf{u}^{\\star}$ is the exact discrete zero-mean solution chosen by fixing the gauge $\\sum_i u_i^{\\star}=0$, which can be constructed numerically via the Moore–Penrose pseudoinverse followed by subtracting the mean:\n$$\n\\mathbf{u}^{\\star} = \\mathbf{A}^{\\dagger}\\mathbf{b} - \\left(\\frac{1}{n}\\sum_{i=0}^{n-1} (\\mathbf{A}^{\\dagger}\\mathbf{b})_i\\right)\\mathbf{1}.\n$$\nThis produces a representative solution in the equivalence class of solutions that differ by constants.\n\nThe V-cycle algorithm is:\n1. Pre-smoothing: apply $\\nu_{\\mathrm{pre}}$ weighted Jacobi sweeps to $\\mathbf{u}$.\n2. Compute fine residual $\\mathbf{r}_f = \\mathbf{b} - \\mathbf{A}_f \\mathbf{u}$.\n3. Restrict residual to coarse grid $\\mathbf{r}_c = \\mathbf{R}\\mathbf{r}_f$.\n4. Coarse solve: either\n   - Nullspace-respecting: enforce zero-mean residual by $\\mathbf{r}_c \\leftarrow \\mathbf{r}_c - \\left(\\frac{1}{n_c}\\sum_j (\\mathbf{r}_c)_j\\right)\\mathbf{1}$, solve $\\mathbf{e}_c = \\mathbf{A}_c^{\\dagger}\\mathbf{r}_c$, and enforce zero-mean correction $\\mathbf{e}_c \\leftarrow \\mathbf{e}_c - \\left(\\frac{1}{n_c}\\sum_j (\\mathbf{e}_c)_j\\right)\\mathbf{1}$.\n   - Failing: compute $\\mathbf{e}_c = \\mathbf{A}_c^{\\dagger}\\mathbf{r}_c$ with no projections.\n5. Prolongate correction: $\\mathbf{e}_f = \\mathbf{P}\\mathbf{e}_c$.\n6. Update fine solution: $\\mathbf{u} \\leftarrow \\mathbf{u} + \\mathbf{e}_f$.\n7. Post-smoothing: apply $\\nu_{\\mathrm{post}}$ weighted Jacobi sweeps to $\\mathbf{u}$.\n\nThe coarse-grid operator is singular in both cases, but the nullspace-respecting solver explicitly removes the problematic constant component from both the coarse residual and correction, ensuring that the correction lies in the proper subspace where $\\mathbf{A}_c$ is invertible. The failing solver leaves the constant component, and the prolongation introduces an error component that cannot be reduced by subsequent smoothing. Therefore, for compatible sources and the given transfer operators, we expect $\\rho_{\\mathrm{respect}} < \\rho_{\\mathrm{fail}}$.\n\nWe test three scenarios:\n- Test case $1$ with $n_f=9$ and $b_i=\\sin(2\\pi x_i)$. This is a typical smooth mean-zero source and exercises interior behavior.\n- Test case $2$ with $n_f=5$ to emphasize boundary-coarse-grid effects where the coarse grid is very small.\n- Test case $3$ with $n_f=7$ and a low-frequency source $b_i = \\cos(\\pi x_i) - \\text{mean}$, which creates a source that projects strongly onto the lowest-frequency error component; the failing solver is particularly detrimental here.\n\nFor each case, we compute the exact zero-mean solution via the pseudoinverse and mean subtraction, perform one V-cycle for both coarse solver variants, and compute\n$$\n\\rho_{\\mathrm{respect}} = \\frac{\\|\\mathbf{u}^{\\star}-\\mathbf{u}^{(1)}_{\\mathrm{respect}}\\|_2}{\\|\\mathbf{u}^{\\star}\\|_2}, \\quad \\rho_{\\mathrm{fail}} = \\frac{\\|\\mathbf{u}^{\\star}-\\mathbf{u}^{(1)}_{\\mathrm{fail}}\\|_2}{\\|\\mathbf{u}^{\\star}\\|_2}.\n$$\nWe then aggregate the outputs as\n$$\n[[\\rho_{\\mathrm{respect}}^{(1)},\\rho_{\\mathrm{fail}}^{(1)}],[\\rho_{\\mathrm{respect}}^{(2)},\\rho_{\\mathrm{fail}}^{(2)}],[\\rho_{\\mathrm{respect}}^{(3)},\\rho_{\\mathrm{fail}}^{(3)}]].\n$$\nThis directly exhibits the impact of improperly handling the coarse-grid nullspace on the convergence of a multigrid V-cycle for discrete Neumann problems in computational fluid dynamics. The program constructs all operators explicitly and uses only the stated operations, ensuring reproducibility and adherence to the specified output format.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef neumann_laplacian_1d(n: int) -> np.ndarray:\n    \"\"\"\n    Construct the 1D Neumann Laplacian (positive semidefinite) with n grid points.\n    A[0,0]=1, A[0,1]=-1; interior i: A[i,i]=2, A[i,i-1]=-1, A[i,i+1]=-1; A[n-1,n-2]=-1, A[n-1,n-1]=1\n    \"\"\"\n    A = np.zeros((n, n), dtype=float)\n    if n == 1:\n        # Degenerate single-point case: purely nullspace; not used in tests.\n        A[0, 0] = 0.0\n        return A\n    # Left boundary\n    A[0, 0] = 1.0\n    A[0, 1] = -1.0\n    # Interior\n    for i in range(1, n - 1):\n        A[i, i] = 2.0\n        A[i, i - 1] = -1.0\n        A[i, i + 1] = -1.0\n    # Right boundary\n    A[n - 1, n - 1] = 1.0\n    A[n - 1, n - 2] = -1.0\n    return A\n\ndef prolongation_linear(n_coarse: int, n_fine: int) -> np.ndarray:\n    \"\"\"\n    Linear interpolation prolongation from coarse to fine for 1D grids with n_fine = 2*n_coarse - 1.\n    \"\"\"\n    assert n_fine == 2 * n_coarse - 1, \"Fine size must be 2*n_coarse - 1\"\n    P = np.zeros((n_fine, n_coarse), dtype=float)\n    # Injection at even indices\n    for j in range(n_coarse):\n        i_even = 2 * j\n        P[i_even, j] = 1.0\n    # Averaging for odd indices\n    for i in range(1, n_fine, 2):\n        j_left = (i - 1) // 2\n        j_right = j_left + 1\n        P[i, j_left] = 0.5\n        if j_right < n_coarse:\n            P[i, j_right] = 0.5\n    return P\n\ndef restriction_full_weighting(n_fine: int, n_coarse: int) -> np.ndarray:\n    \"\"\"\n    Full-weighting restriction from fine to coarse for 1D grids with n_fine = 2*n_coarse - 1.\n    Boundary rows use 0.5 weights, interior rows use [0.25, 0.5, 0.25] centered on even fine indices.\n    \"\"\"\n    assert n_fine == 2 * n_coarse - 1, \"Fine size must be 2*n_coarse - 1\"\n    R = np.zeros((n_coarse, n_fine), dtype=float)\n    for j in range(n_coarse):\n        i_even = 2 * j\n        if i_even == 0:\n            R[j, 0] = 0.5\n            if n_fine > 1:\n                R[j, 1] = 0.5\n        elif i_even == n_fine - 1:\n            R[j, n_fine - 2] = 0.5\n            R[j, n_fine - 1] = 0.5\n        else:\n            R[j, i_even - 1] = 0.25\n            R[j, i_even] = 0.5\n            R[j, i_even + 1] = 0.25\n    return R\n\ndef weighted_jacobi(A: np.ndarray, b: np.ndarray, u: np.ndarray, omega: float, sweeps: int) -> np.ndarray:\n    \"\"\"\n    Perform 'sweeps' iterations of weighted Jacobi on Au = b with weight omega.\n    \"\"\"\n    D = np.diag(A)\n    # Avoid division by zero: for Neumann Laplacian, diagonals are positive (1 at boundaries, 2 interior)\n    invD = 1.0 / D\n    for _ in range(sweeps):\n        r = b - A @ u\n        u = u + omega * (invD * r)\n    return u\n\ndef zero_mean(vec: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Project a vector onto the zero-mean subspace.\n    \"\"\"\n    return vec - np.mean(vec)\n\ndef exact_zero_mean_solution(A: np.ndarray, b: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute a representative exact solution with zero mean using pseudoinverse and mean subtraction.\n    Assumes b is compatible with A (sum(b) ~ 0).\n    \"\"\"\n    u = np.linalg.pinv(A) @ b\n    u = zero_mean(u)\n    return u\n\ndef v_cycle_one(A_f: np.ndarray, R: np.ndarray, P: np.ndarray,\n                b_f: np.ndarray, omega: float, nu_pre: int, nu_post: int,\n                respect_nullspace: bool) -> np.ndarray:\n    \"\"\"\n    Perform a single two-level V-cycle on fine grid.\n    \"\"\"\n    n_f = A_f.shape[0]\n    n_c = R.shape[0]\n    # Pre-smoothing\n    u = np.zeros(n_f, dtype=float)\n    u = weighted_jacobi(A_f, b_f, u, omega, nu_pre)\n    # Fine residual\n    r_f = b_f - A_f @ u\n    # Restrict to coarse\n    r_c = R @ r_f\n    # Build coarse operator\n    A_c = R @ A_f @ P\n    # Coarse solve\n    if respect_nullspace:\n        # Enforce zero-mean on coarse residual\n        r_c = zero_mean(r_c)\n        e_c = np.linalg.pinv(A_c) @ r_c\n        # Remove coarse nullspace component from correction\n        e_c = zero_mean(e_c)\n    else:\n        e_c = np.linalg.pinv(A_c) @ r_c\n        # No nullspace handling: potentially inject constant modes\n    # Prolongate and correct\n    e_f = P @ e_c\n    u = u + e_f\n    # Post-smoothing\n    u = weighted_jacobi(A_f, b_f, u, omega, nu_post)\n    return u\n\ndef build_rhs_sine(n: int) -> np.ndarray:\n    \"\"\"\n    Build b_i = sin(2*pi*x_i) with x_i = i/(n-1). This RHS has mean zero.\n    \"\"\"\n    x = np.linspace(0.0, 1.0, n)\n    b = np.sin(2.0 * np.pi * x)\n    # Numerical rounding might cause tiny residual mean; enforce exact zero mean.\n    b = zero_mean(b)\n    return b\n\ndef build_rhs_cos_pi_minus_mean(n: int) -> np.ndarray:\n    \"\"\"\n    Build b_i = cos(pi*x_i) - mean(cos(pi*x_i)), enforcing zero mean.\n    \"\"\"\n    x = np.linspace(0.0, 1.0, n)\n    c = np.cos(np.pi * x)\n    b = c - np.mean(c)\n    return b\n\ndef error_reduction_factors(n_f: int, omega: float, nu_pre: int, nu_post: int, b: np.ndarray) -> list:\n    \"\"\"\n    Compute [rho_respect, rho_fail] for one V-cycle with the given parameters and RHS.\n    \"\"\"\n    # Fine operator\n    A_f = neumann_laplacian_1d(n_f)\n    # Coarse grid size: n_c = (n_f + 1)//2\n    n_c = (n_f + 1) // 2\n    # Transfers\n    P = prolongation_linear(n_c, n_f)\n    R = restriction_full_weighting(n_f, n_c)\n    # Exact zero-mean solution\n    u_star = exact_zero_mean_solution(A_f, b)\n    norm0 = np.linalg.norm(u_star)\n    # Respecting nullspace\n    u1_respect = v_cycle_one(A_f, R, P, b, omega, nu_pre, nu_post, respect_nullspace=True)\n    rho_respect = np.linalg.norm(u_star - u1_respect) / (norm0 if norm0 != 0.0 else 1.0)\n    # Failing nullspace handling\n    u1_fail = v_cycle_one(A_f, R, P, b, omega, nu_pre, nu_post, respect_nullspace=False)\n    rho_fail = np.linalg.norm(u_star - u1_fail) / (norm0 if norm0 != 0.0 else 1.0)\n    return [rho_respect, rho_fail]\n\ndef format_nested_list(lst):\n    \"\"\"\n    Format nested list of floats without spaces, with reasonable precision.\n    \"\"\"\n    if isinstance(lst, list):\n        return '[' + ','.join(format_nested_list(x) for x in lst) + ']'\n    elif isinstance(lst, float):\n        # Use 12 significant digits to be concise yet precise\n        return f\"{lst:.12g}\"\n    else:\n        return str(lst)\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each test case: (n_f, omega, nu_pre, nu_post, rhs_builder)\n    test_cases = [\n        (9, 2.0/3.0, 2, 2, build_rhs_sine),          # Test case 1 (happy path)\n        (5, 2.0/3.0, 1, 1, build_rhs_sine),          # Test case 2 (small coarse grid)\n        (7, 0.7,      3, 3, build_rhs_cos_pi_minus_mean),  # Test case 3 (low-frequency source)\n    ]\n\n    results = []\n    for n_f, omega, nu_pre, nu_post, rhs_builder in test_cases:\n        b = rhs_builder(n_f)\n        pair = error_reduction_factors(n_f, omega, nu_pre, nu_post, b)\n        results.append(pair)\n\n    # Final print statement in the exact required format.\n    print(format_nested_list(results))\n\nsolve()\n```", "id": "3347220"}]}