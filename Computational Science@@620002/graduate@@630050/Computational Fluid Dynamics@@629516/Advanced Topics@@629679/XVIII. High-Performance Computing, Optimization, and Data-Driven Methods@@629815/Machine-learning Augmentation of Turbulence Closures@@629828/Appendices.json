{"hands_on_practices": [{"introduction": "Before building complex machine learning models, it is crucial to understand how their corrections interact with the underlying governing equations of fluid dynamics. This exercise provides a foundational, first-principles look at this interaction. By working through the turbulent kinetic energy ($k$) budget in a simplified control volume, you will directly calculate how a model-predicted augmentation to the eddy viscosity, $\\nu_{t}$, alters the balance between energy production, dissipation, and transport, providing a tangible link between the ML model and the flow physics [@problem_id:3342944].", "problem": "Consider an incompressible turbulent flow governed by the Reynolds-Averaged Navier–Stokes (RANS) equations. Let the turbulent kinetic energy (TKE) be defined as $k = \\frac{1}{2} \\langle u_{i}' u_{i}' \\rangle$, where $\\langle \\cdot \\rangle$ denotes Reynolds averaging and $u_{i}'$ is the fluctuating velocity. Starting from the exact Reynolds-averaged transport equation for $k$, derive the budget decomposition into production, dissipation, and transport. Use the Boussinesq eddy-viscosity approximation $- \\langle u_{i}' u_{j}' \\rangle = 2 \\nu_{t} S_{ij} - \\frac{2}{3} k \\delta_{ij}$, where $\\nu_{t}$ is the eddy viscosity and $S_{ij} = \\frac{1}{2} \\left( \\frac{\\partial U_{i}}{\\partial x_{j}} + \\frac{\\partial U_{j}}{\\partial x_{i}} \\right)$ is the mean strain-rate tensor, to express the modeled production term $P_{k}$ in terms of $\\nu_{t}$ and $S_{ij}$. Model the transport as gradient diffusion with an effective diffusivity using $T_{j} = - \\left( \\nu + \\frac{\\nu_{t}}{\\sigma_{k}} \\right) \\frac{\\partial k}{\\partial x_{j}}$, where $\\nu$ is the kinematic viscosity and $\\sigma_{k}$ is a positive constant.\n\nThen, integrate the $k$-equation over a rectangular control volume with periodic boundaries in the streamwise and spanwise directions and impermeable walls located at $y = 0$ and $y = H$. The width, length, and height of the control volume are $L_{z}$, $L_{x}$, and $H$, respectively, with $L_{x} = 1\\,\\mathrm{m}$, $L_{z} = 0.5\\,\\mathrm{m}$, and $H = 0.1\\,\\mathrm{m}$. The mean flow has a spatially uniform shear $\\frac{\\partial U_{x}}{\\partial y} = S$ with $S = 100\\,\\mathrm{s^{-1}}$ and other mean gradients vanish. Assume the dissipative term $\\varepsilon$ is uniform with $\\varepsilon = 4\\,\\mathrm{m^{2}\\,s^{-3}}$ and the gradient of $k$ is uniform with $\\frac{\\partial k}{\\partial y} = -10\\,\\mathrm{m^{2}\\,s^{-2}\\,m^{-1}}$. The molecular viscosity is $\\nu = 1.0 \\times 10^{-6}\\,\\mathrm{m^{2}\\,s^{-1}}$, and the baseline eddy viscosity (without augmentation) is spatially uniform with $\\nu_{t,0} = 5.0 \\times 10^{-4}\\,\\mathrm{m^{2}\\,s^{-1}}$. Let $\\sigma_{k} = 1.0$.\n\nA Machine Learning (ML) correction augments the eddy viscosity as $\\nu_{t}^{\\mathrm{ML}}(y) = \\nu_{t,0} \\left[ 1 + \\alpha \\, \\phi(y) \\right]$, where $\\alpha = 0.2$ and the feature is $\\phi(y) = \\frac{y}{H}$. Using the derived global (control-volume integrated) $k$-budget, compute the instantaneous rate of change of the control-volume integrated turbulent kinetic energy $\\frac{\\mathrm{d}}{\\mathrm{d}t} \\int_{V} k \\,\\mathrm{d}V$ under the ML correction. Express your final answer in $\\mathrm{m^{5}\\,s^{-3}}$ and round your answer to four significant figures.", "solution": "The problem requires the calculation of the instantaneous rate of change of the volume-integrated turbulent kinetic energy (TKE) in a specific flow configuration. This involves deriving the integrated TKE budget and then evaluating each term using the given models and parameters.\n\nThe exact Reynolds-averaged transport equation for the turbulent kinetic energy, $k = \\frac{1}{2} \\langle u_{i}' u_{i}' \\rangle$, is:\n$$\n\\frac{\\partial k}{\\partial t} + U_j \\frac{\\partial k}{\\partial x_j} = P_k - \\varepsilon + \\mathcal{D}_k\n$$\nwhere $U_j$ is the mean velocity, $P_k = - \\langle u_{i}' u_{j}' \\rangle \\frac{\\partial U_{i}}{\\partial x_{j}}$ is the production rate of TKE, $\\varepsilon = \\nu \\langle \\frac{\\partial u_{i}'}{\\partial x_{j}} \\frac{\\partial u_{i}'}{\\partial x_{j}} \\rangle$ is the dissipation rate, and $\\mathcal{D}_k$ is the transport term, which includes turbulent, pressure, and viscous diffusion effects. The transport term can be written in divergence form, $\\mathcal{D}_k = \\frac{\\partial T_{j}^{\\text{total}}}{\\partial x_j}$. The problem provides a model for this total transport flux, $T_j$.\n\nUsing the incompressibility condition $\\frac{\\partial U_j}{\\partial x_j} = 0$, the advection term can be written as $U_j \\frac{\\partial k}{\\partial x_j} = \\frac{\\partial (U_j k)}{\\partial x_j}$. The modeled TKE equation is thus:\n$$\n\\frac{\\partial k}{\\partial t} + \\frac{\\partial (U_j k)}{\\partial x_j} = P_k - \\varepsilon + \\frac{\\partial T_j}{\\partial x_j}\n$$\nwhere $T_j = - \\left( \\nu + \\frac{\\nu_{t}}{\\sigma_{k}} \\right) \\frac{\\partial k}{\\partial x_{j}}$.\n\nFirst, we express the production term $P_k$ using the Boussinesq eddy-viscosity approximation, $- \\langle u_{i}' u_{j}' \\rangle = 2 \\nu_{t} S_{ij} - \\frac{2}{3} k \\delta_{ij}$. The production term can be written as $P_k = - \\langle u_i' u_j' \\rangle S_{ij}$, as the product of a symmetric tensor ($\\langle u_i' u_j' \\rangle$) with an anti-symmetric tensor (the mean rotation rate tensor) is zero.\nSubstituting the Boussinesq model:\n$$\nP_k = \\left( 2 \\nu_t S_{ij} - \\frac{2}{3} k \\delta_{ij} \\right) S_{ij} = 2 \\nu_t S_{ij} S_{ij} - \\frac{2}{3} k \\delta_{ij} S_{ij}\n$$\nFor an incompressible flow, the trace of the mean strain-rate tensor is zero: $S_{ii} = \\frac{\\partial U_i}{\\partial x_i} = 0$. Since $\\delta_{ij} S_{ij} = S_{ii}$, the second term vanishes. This yields the modeled production term:\n$$\nP_k = 2 \\nu_t S_{ij} S_{ij}\n$$\n\nNext, we integrate the modeled TKE equation over the control volume $V$:\n$$\n\\int_V \\frac{\\partial k}{\\partial t} \\, \\mathrm{d}V + \\int_V \\frac{\\partial (U_j k)}{\\partial x_j} \\, \\mathrm{d}V = \\int_V P_k \\, \\mathrm{d}V - \\int_V \\varepsilon \\, \\mathrm{d}V + \\int_V \\frac{\\partial T_j}{\\partial x_j} \\, \\mathrm{d}V\n$$\nLet $\\mathcal{K} = \\int_V k \\, \\mathrm{d}V$. The term we must compute is $\\frac{\\mathrm{d}\\mathcal{K}}{\\mathrm{d}t} = \\int_V \\frac{\\partial k}{\\partial t} \\, \\mathrm{d}V$.\nApplying the divergence theorem to the advection and transport terms:\n$$\n\\frac{\\mathrm{d}\\mathcal{K}}{\\mathrm{d}t} + \\oint_A (U_j k) n_j \\, \\mathrm{d}A = \\int_V P_k \\, \\mathrm{d}V - \\int_V \\varepsilon \\, \\mathrm{d}V + \\oint_A T_j n_j \\, \\mathrm{d}A\n$$\nwhere $A$ is the surface of the control volume. The surface integrals over the periodic boundaries in the streamwise ($x$) and spanwise ($z$) directions cancel out. At the impermeable walls ($y=0$ and $y=H$), the normal velocity $U_y$ is zero. Therefore, the surface integral of the advection term $\\oint (U_j k) n_j \\, \\mathrm{d}A$ is zero. The remaining surface integral for the transport term is:\n$$\n\\oint_A T_j n_j \\, \\mathrm{d}A = \\int_{0}^{L_z}\\int_{0}^{L_x} [T_y|_{y=H} - T_y|_{y=0}] \\, \\mathrm{d}x\\mathrm{d}z = L_x L_z [T_y(H) - T_y(0)]\n$$\nThe integrated TKE budget is:\n$$\n\\frac{\\mathrm{d}\\mathcal{K}}{\\mathrm{d}t} = \\underbrace{\\int_V P_k \\, \\mathrm{d}V}_{\\mathcal{P}} - \\underbrace{\\int_V \\varepsilon \\, \\mathrm{d}V}_{\\mathcal{E}} + \\underbrace{L_x L_z [T_y(H) - T_y(0)]}_{\\mathcal{T}}\n$$\nWe now evaluate each term on the right-hand side.\nThe control volume is $V = L_x L_z H = (1)(0.5)(0.1) = 0.05\\,\\mathrm{m^3}$.\n\n1.  **Integrated Production, $\\mathcal{P}$**:\n    The mean flow is a simple shear flow with $\\frac{\\partial U_x}{\\partial y} = S = 100\\,\\mathrm{s^{-1}}$ and all other mean velocity gradients are zero. The strain-rate tensor components are $S_{12} = S_{21} = \\frac{1}{2}S$, and all others are zero.\n    Thus, $S_{ij} S_{ij} = S_{12}^2 + S_{21}^2 = (\\frac{S}{2})^2 + (\\frac{S}{2})^2 = \\frac{S^2}{2}$.\n    The production rate is $P_k(y) = 2 \\nu_t(y) (\\frac{S^2}{2}) = \\nu_t(y) S^2$.\n    The eddy viscosity is given by the ML-augmented model $\\nu_t(y) = \\nu_{t,0} \\left( 1 + \\alpha \\frac{y}{H} \\right)$.\n    We integrate $P_k$ over the volume:\n    $$\n    \\mathcal{P} = \\int_V \\nu_t(y) S^2 \\, \\mathrm{d}V = S^2 L_x L_z \\int_0^H \\nu_{t,0} \\left( 1 + \\alpha \\frac{y}{H} \\right) \\mathrm{d}y\n    $$\n    $$\n    \\mathcal{P} = S^2 L_x L_z \\nu_{t,0} \\left[ y + \\frac{\\alpha y^2}{2H} \\right]_0^H = S^2 L_x L_z \\nu_{t,0} \\left( H + \\frac{\\alpha H^2}{2H} \\right) = S^2 (L_x L_z H) \\nu_{t,0} \\left( 1 + \\frac{\\alpha}{2} \\right)\n    $$\n    Substituting the given values: $S=100\\,\\mathrm{s^{-1}}$, $V=0.05\\,\\mathrm{m^3}$, $\\nu_{t,0} = 5.0 \\times 10^{-4}\\,\\mathrm{m^{2}\\,s^{-1}}$, and $\\alpha=0.2$.\n    $$\n    \\mathcal{P} = (100^2)(0.05)(5.0 \\times 10^{-4}) \\left( 1 + \\frac{0.2}{2} \\right) = (10000)(0.05)(5.0 \\times 10^{-4})(1.1) = (500)(5.0 \\times 10^{-4})(1.1) = 0.25 \\times 1.1 = 0.275\\,\\mathrm{m^{5}\\,s^{-3}}\n    $$\n2.  **Integrated Dissipation, $\\mathcal{E}$**:\n    The dissipation rate $\\varepsilon$ is uniform with $\\varepsilon = 4\\,\\mathrm{m^{2}\\,s^{-3}}$.\n    $$\n    \\mathcal{E} = \\int_V \\varepsilon \\, \\mathrm{d}V = \\varepsilon V = (4)(0.05) = 0.2\\,\\mathrm{m^{5}\\,s^{-3}}\n    $$\n3.  **Net Transport, $\\mathcal{T}$**:\n    The transport contribution is from the net flux through the walls. We need $T_y(H) - T_y(0)$.\n    The flux is $T_y(y) = - \\left( \\nu + \\frac{\\nu_t(y)}{\\sigma_k} \\right) \\frac{\\partial k}{\\partial y}$. The TKE gradient $\\frac{\\partial k}{\\partial y}$ is uniform at $-10\\,\\mathrm{m\\,s^{-2}}$.\n    $$\n    T_y(y) = - \\left( \\nu + \\frac{\\nu_{t,0}}{\\sigma_k} \\left(1 + \\alpha \\frac{y}{H}\\right) \\right) \\frac{\\partial k}{\\partial y}\n    $$\n    The difference is:\n    $$\n    T_y(H) - T_y(0) = - \\left( \\nu + \\frac{\\nu_{t,0}(1+\\alpha)}{\\sigma_k} \\right)\\frac{\\partial k}{\\partial y} - \\left[ - \\left( \\nu + \\frac{\\nu_{t,0}}{\\sigma_k} \\right)\\frac{\\partial k}{\\partial y} \\right]\n    $$\n    $$\n    T_y(H) - T_y(0) = \\left( -\\nu - \\frac{\\nu_{t,0}}{\\sigma_k} - \\frac{\\alpha \\nu_{t,0}}{\\sigma_k} + \\nu + \\frac{\\nu_{t,0}}{\\sigma_k} \\right) \\frac{\\partial k}{\\partial y} = - \\frac{\\alpha \\nu_{t,0}}{\\sigma_k} \\frac{\\partial k}{\\partial y}\n    $$\n    The total transport term is $\\mathcal{T} = L_x L_z [T_y(H) - T_y(0)] = - L_x L_z \\frac{\\alpha \\nu_{t,0}}{\\sigma_k} \\frac{\\partial k}{\\partial y}$.\n    Substituting values: $L_x=1\\,\\mathrm{m}$, $L_z=0.5\\,\\mathrm{m}$, $\\alpha=0.2$, $\\nu_{t,0}=5.0 \\times 10^{-4}\\,\\mathrm{m^{2}\\,s^{-1}}$, $\\sigma_k=1.0$, and $\\frac{\\partial k}{\\partial y}=-10\\,\\mathrm{m\\,s^{-2}}$.\n    $$\n    \\mathcal{T} = -(1)(0.5) \\frac{(0.2)(5.0 \\times 10^{-4})}{1.0} (-10) = -(0.5)(1.0 \\times 10^{-4})(-10) = (0.5)(1.0 \\times 10^{-3}) = 0.0005\\,\\mathrm{m^{5}\\,s^{-3}}\n    $$\nFinally, we compute the total rate of change of integrated TKE:\n$$\n\\frac{\\mathrm{d}\\mathcal{K}}{\\mathrm{d}t} = \\mathcal{P} - \\mathcal{E} + \\mathcal{T} = 0.275 - 0.2 + 0.0005 = 0.0755\\,\\mathrm{m^{5}\\,s^{-3}}\n$$\nThe problem asks for the answer to be rounded to four significant figures.\n$$\n\\frac{\\mathrm{d}\\mathcal{K}}{\\mathrm{d}t} = 0.07550\\,\\mathrm{m^{5}\\,s^{-3}}\n$$", "answer": "$$\n\\boxed{0.07550}\n$$", "id": "3342944"}, {"introduction": "A primary challenge in developing data-driven turbulence models is ensuring that their predictions are physically plausible and do not lead to numerical instabilities. This practice explores a core technique for achieving this by embedding physical constraints directly into the training process. You will construct a differentiable loss function that penalizes violations of the realizability constraints on the Reynolds stress anisotropy tensor, $b_{ij}$, a critical step for training robust and trustworthy models using gradient-based optimization [@problem_id:3342982].", "problem": "You are asked to formalize and implement a batch loss for machine-learning augmentation of turbulence closures that enforces eigenvalue-based realizability of the Reynolds-stress anisotropy. The setting is as follows. In Reynolds-Averaged Navier–Stokes (RANS) modeling, one seeks to learn a mapping from the mean strain-rate tensor $S_{ij}$ and mean rotation-rate tensor $R_{ij}$ to the anisotropy tensor $b_{ij}$ that matches a target obtained from Direct Numerical Simulation (DNS) data. The anisotropy tensor $b_{ij}$ is symmetric by definition, and physical realizability imposes eigenvalue bounds along with the traceless constraint. You are to derive a differentiable penalty that enforces these realizability constraints and combine it with a data misfit to produce a batch loss that can be minimized during training.\n\nUse only well-accepted foundations: the definition of the anisotropy tensor as a symmetric second-order tensor with zero trace, the requirement that its eigenvalues lie in the closed interval $\\left[-\\frac{1}{3}, \\frac{2}{3}\\right]$ for realizability of Reynolds stresses, and standard batch-averaged mean-squared-error losses. Your derivation must proceed from these constraints to a differentiable penalty that vanishes when all constraints are satisfied and increases smoothly when they are violated, and to a batch-averaged total loss that adds the misfit and the realizability penalty. No other shortcut formulas may be assumed.\n\nFor the purpose of numerical evaluation, use the following representation (a minimal tensor-basis model) for the model-predicted anisotropy in each sample $s$:\n$$\nb_{ij}^{\\text{pred},(s)} \\;=\\; a_0 I_{ij} \\;+\\; a_1 \\, T^{(1)}_{ij}(S^{(s)}) \\;+\\; a_2 \\, T^{(2)}_{ij}(S^{(s)}) \\;+\\; a_3 \\, T^{(3)}_{ij}(R^{(s)}),\n$$\nwith the tensor basis defined by\n$$\nT^{(1)}(S) \\;=\\; S, \\qquad\nT^{(2)}(S) \\;=\\; S^2 \\;-\\; \\frac{\\operatorname{tr}(S^2)}{3} I, \\qquad\nT^{(3)}(R) \\;=\\; R^2 \\;-\\; \\frac{\\operatorname{tr}(R^2)}{3} I,\n$$\nand $I$ is the identity tensor. Here $a_0,a_1,a_2,a_3$ are fixed scalar coefficients for each batch (constant across samples in that batch). The mean rotation-rate tensor $R$ is antisymmetric, hence $R^2$ is symmetric, so each $T^{(n)}$ is symmetric and traceless except possibly $a_0 I$. The target tensor in each sample is $b_{ij}^{\\text{DNS},(s)}$.\n\nYour program must:\n- Derive and implement a differentiable realizability penalty based on the eigenvalues of $b^{\\text{pred},(s)}$, enforcing that all eigenvalues lie in $\\left[-\\frac{1}{3}, \\frac{2}{3}\\right]$, and additionally penalize nonzero trace. Use a smooth approximation of the positive-part operator to retain differentiability with a tunable sharpness parameter $\\beta > 0$.\n- Use a batch-averaged mean-squared error (Frobenius norm squared) between $b^{\\text{pred},(s)}$ and $b^{\\text{DNS},(s)}$ across all samples in the batch.\n- Combine the mean-squared error and the mean realizability penalty with a nonnegative weight $\\gamma$ and a nonnegative trace-penalty weight $\\mu$.\n\nAll quantities are non-dimensional and unitless.\n\nImplement the loss for the following test suite. Each test case $t$ specifies a batch consisting of one or more samples $s$, an anisotropy model coefficient vector $\\mathbf{a} = [a_0,a_1,a_2,a_3]$, and penalty hyperparameters $(\\gamma,\\mu,\\beta)$. For each test case, compute the scalar batch loss $L^{(t)}$.\n\nDefinitions common to all test cases:\n- The per-sample data misfit is\n$$\n\\mathcal{E}^{(s)} \\;=\\; \\left\\| b^{\\text{pred},(s)} - b^{\\text{DNS},(s)} \\right\\|_F^2,\n$$\nwhere $\\|\\cdot\\|_F$ denotes the Frobenius norm.\n- The per-sample realizability penalty must be differentiable, must be constructed solely from the eigenvalues of $b^{\\text{pred},(s)}$ and its trace, and must penalize violations of the eigenvalue bounds $\\left[-\\frac{1}{3}, \\frac{2}{3}\\right]$ and the traceless condition. The penalty must depend on a smooth approximation parameter $\\beta$ and a trace weight $\\mu$.\n- The batch loss over $N$ samples is\n$$\nL \\;=\\; \\frac{1}{N}\\sum_{s=1}^{N} \\mathcal{E}^{(s)} \\;+\\; \\gamma \\,\\frac{1}{N}\\sum_{s=1}^{N} \\mathcal{P}^{(s)},\n$$\nwhere $\\mathcal{P}^{(s)}$ is your differentiable realizability penalty for sample $s$.\n\nTest suite:\n\n- Test case $1$ (happy path, small misfit, no severe violations): batch has $N=2$ samples. Use $\\gamma = 10$, $\\mu = 5$, $\\beta = 50$. Model coefficients $\\mathbf{a} = [a_0,a_1,a_2,a_3] = [0, 0.3, 0.2, -0.1]$.\n  - Sample $s=1$:\n    - $S^{(1)} = \\begin{bmatrix} 0 & 0.1 & 0 \\\\ 0.1 & 0 & 0.05 \\\\ 0 & 0.05 & 0 \\end{bmatrix}$,\n      $R^{(1)} = \\begin{bmatrix} 0 & 0.02 & 0 \\\\ -0.02 & 0 & 0.01 \\\\ 0 & -0.01 & 0 \\end{bmatrix}$.\n    - Construct $b^{\\text{DNS},(1)}$ from the same basis using coefficients $[0, 0.25, 0.15, -0.05]$.\n  - Sample $s=2$:\n    - $S^{(2)} = \\begin{bmatrix} 0 & -0.08 & 0 \\\\ -0.08 & 0 & 0.03 \\\\ 0 & 0.03 & 0 \\end{bmatrix}$,\n      $R^{(2)} = \\begin{bmatrix} 0 & -0.01 & 0 \\\\ 0.01 & 0 & -0.02 \\\\ 0 & 0.02 & 0 \\end{bmatrix}$.\n    - Construct $b^{\\text{DNS},(2)}$ from the same basis using coefficients $[0, 0.22, 0.10, -0.04]$.\n\n- Test case $2$ (eigenvalue bound violations in both directions): batch has $N=1$ sample. Use $\\gamma = 10$, $\\mu = 5$, $\\beta = 50$. Model coefficients $\\mathbf{a} = [0, 0.8, 0.5, 0]$. The sample has\n  $S^{(1)} = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & -1 & 0 \\\\ 0 & 0 & 0 \\end{bmatrix}$,\n  $R^{(1)} = \\begin{bmatrix} 0 & 0 & 0 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 0 \\end{bmatrix}$,\n  and $b^{\\text{DNS},(1)} = \\begin{bmatrix} 0 & 0 & 0 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 0 \\end{bmatrix}$.\n\n- Test case $3$ (trace violation only, eigenvalues within bounds): batch has $N=1$ sample. Use $\\gamma = 10$, $\\mu = 5$, $\\beta = 50$. Model coefficients $\\mathbf{a} = [0.1, 0, 0, 0]$. The sample has\n  $S^{(1)} = \\begin{bmatrix} 0 & 0 & 0 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 0 \\end{bmatrix}$,\n  $R^{(1)} = \\begin{bmatrix} 0 & 0 & 0 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 0 \\end{bmatrix}$,\n  and $b^{\\text{DNS},(1)} = \\begin{bmatrix} 0 & 0 & 0 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 0 \\end{bmatrix}$.\n\n- Test case $4$ (boundary case, eigenvalues exactly at bounds): batch has $N=1$ sample. Use $\\gamma = 10$, $\\mu = 5$, $\\beta = 50$. Model coefficients $\\mathbf{a} = [0, 0.5, 0.5, 0]$. The sample has\n  $S^{(1)} = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & -1 & 0 \\\\ 0 & 0 & 0 \\end{bmatrix}$,\n  $R^{(1)} = \\begin{bmatrix} 0 & 0 & 0 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 0 \\end{bmatrix}$,\n  and $b^{\\text{DNS},(1)}$ equal to the model prediction for these inputs and coefficients.\n\nYour program must produce a single line of output containing the scalar batch losses for test cases $1$ through $4$, as a comma-separated list enclosed in square brackets, exactly in the form:\n\"[loss1,loss2,loss3,loss4]\".", "solution": "The central task is to formalize and implement a batch loss function for a machine-learning-augmented turbulence closure model. This loss function must ensure that the predicted Reynolds-stress anisotropy tensor, $b_{ij}$, adheres to physical realizability constraints. The total loss, $L$, is a weighted sum of a data misfit term and a penalty term for violations of these constraints. The derivation proceeds from the fundamental principles of turbulence modeling and constrained optimization.\n\nFirst, we define the structure of the model's prediction. For each data sample $s$ in a batch, the model predicts an anisotropy tensor $b_{ij}^{\\text{pred},(s)}$ as a linear combination of a tensor basis. The basis elements are functions of the mean strain-rate tensor $S_{ij}^{(s)}$ and the mean rotation-rate tensor $R_{ij}^{(s)}$. The model is given by\n$$\nb_{ij}^{\\text{pred},(s)} \\;=\\; a_0 I_{ij} \\;+\\; a_1 \\, T^{(1)}_{ij}(S^{(s)}) \\;+\\; a_2 \\, T^{(2)}_{ij}(S^{(s)}) \\;+\\; a_3 \\, T^{(3)}_{ij}(R^{(s)})\n$$\nwhere $\\mathbf{a} = [a_0, a_1, a_2, a_3]$ are scalar coefficients to be learned, and $I_{ij}$ is the $3 \\times 3$ identity tensor. The tensor basis elements are defined as:\n$$\nT^{(1)}(S) \\;=\\; S\n$$\n$$\nT^{(2)}(S) \\;=\\; S^2 \\;-\\; \\frac{\\operatorname{tr}(S^2)}{3} I\n$$\n$$\nT^{(3)}(R) \\;=\\; R^2 \\;-\\; \\frac{\\operatorname{tr}(R^2)}{3} I\n$$\nHere, $S$ is symmetric and $R$ is antisymmetric. By construction, $S^2$ and $R^2$ are symmetric, and the basis tensors $T^{(2)}$ and $T^{(3)}$ are symmetric and traceless. The tensor $T^{(1)}=S$ is also traceless for incompressible flows, which is the standard context.\n\nThe total batch loss function $L$ for a batch of $N$ samples is defined as the sum of the mean data misfit and the mean realizability penalty:\n$$\nL \\;=\\; \\frac{1}{N}\\sum_{s=1}^{N} \\mathcal{E}^{(s)} \\;+\\; \\gamma \\,\\frac{1}{N}\\sum_{s=1}^{N} \\mathcal{P}^{(s)}\n$$\nwhere $\\mathcal{E}^{(s)}$ is the per-sample data misfit, $\\mathcal{P}^{(s)}$ is the per-sample realizability penalty, and $\\gamma \\ge 0$ is a user-defined weight.\n\nThe data misfit term for a single sample, $\\mathcal{E}^{(s)}$, measures the discrepancy between the predicted anisotropy tensor $b^{\\text{pred},(s)}$ and the target tensor $b^{\\text{DNS},(s)}$ obtained from high-fidelity Direct Numerical Simulation (DNS) data. We use the squared Frobenius norm for this purpose:\n$$\n\\mathcal{E}^{(s)} \\;=\\; \\left\\| b^{\\text{pred},(s)} - b^{\\text{DNS},(s)} \\right\\|_F^2 \\;=\\; \\sum_{i=1}^3 \\sum_{j=1}^3 \\left( b_{ij}^{\\text{pred},(s)} - b_{ij}^{\\text{DNS},(s)} \\right)^2\n$$\n\nThe realizability penalty, $\\mathcal{P}^{(s)}$, enforces two physical constraints on $b^{\\text{pred},(s)}$: it must be traceless, and its eigenvalues must lie within a specific range. We construct a differentiable penalty function that is zero when constraints are satisfied and positive otherwise.\n\nThe first constraint is that the anisotropy tensor must be traceless: $\\operatorname{tr}(b^{\\text{pred},(s)}) = 0$. A violation of this constraint is penalized using a simple quadratic term, weighted by the hyperparameter $\\mu \\ge 0$:\n$$\n\\mathcal{P}_{\\text{trace}}^{(s)} \\;=\\; \\mu \\left( \\operatorname{tr}(b^{\\text{pred},(s)}) \\right)^2\n$$\n\nThe second constraint, stemming from the requirement that the Reynolds stresses themselves be positive semi-definite, imposes bounds on the eigenvalues $\\lambda_k^{(s)}$ of $b^{\\text{pred},(s)}$. For $k \\in \\{1, 2, 3\\}$, the eigenvalues must satisfy:\n$$\n-\\frac{1}{3} \\le \\lambda_k^{(s)} \\le \\frac{2}{3}\n$$\nTo enforce this in a differentiable manner, we penalize violations of these bounds. A violation occurs if $\\lambda_k^{(s)} < -1/3$ or $\\lambda_k^{(s)} > 2/3$. We can express the magnitude of these violations using the positive-part function $[x]_+ = \\max(0, x)$. The violations are $[-\\frac{1}{3} - \\lambda_k^{(s)}]_+$ and $[\\lambda_k^{(s)} - \\frac{2}{3}]_+$. To create a smooth penalty landscape, we use the square of these quantities.\n\nThe problem specifies using a smooth approximation of the positive-part operator to ensure differentiability. We employ a softplus-like function, $sp(x, \\beta) = \\frac{1}{\\beta}\\log(1+e^{\\beta x})$, which converges to $[x]_+$ as the sharpness parameter $\\beta \\to \\infty$. The eigenvalue penalty for sample $s$ is the sum of squared penalties over all three eigenvalues:\n$$\n\\mathcal{P}_{\\text{eig}}^{(s)} \\;=\\; \\sum_{k=1}^3 \\left[ \\left(sp\\left(-\\frac{1}{3} - \\lambda_k^{(s)}, \\beta\\right)\\right)^2 + \\left(sp\\left(\\lambda_k^{(s)} - \\frac{2}{3}, \\beta\\right)\\right)^2 \\right]\n$$\nwhere $sp(x, \\beta) = \\frac{1}{\\beta} \\log(1+e^{\\beta x})$. This function is smoothly differentiable with respect to its arguments, and consequently, with respect to the eigenvalues $\\lambda_k^{(s)}$ and the model coefficients $\\mathbf{a}$.\n\nThe total per-sample realizability penalty $\\mathcal{P}^{(s)}$ is the sum of the trace penalty and the eigenvalue penalty:\n$$\n\\mathcal{P}^{(s)} \\;=\\; \\mathcal{P}_{\\text{eig}}^{(s)} + \\mathcal{P}_{\\text{trace}}^{(s)} \\;=\\; \\sum_{k=1}^3 \\left[ \\left(sp\\left(-\\frac{1}{3} - \\lambda_k^{(s)}, \\beta\\right)\\right)^2 + \\left(sp\\left(\\lambda_k^{(s)} - \\frac{2}{3}, \\beta\\right)\\right)^2 \\right] + \\mu \\left( \\operatorname{tr}(b^{\\text{pred},(s)}) \\right)^2\n$$\n\nCombining all components, the final batch loss $L$ to be minimized during training is:\n$$\nL = \\frac{1}{N}\\sum_{s=1}^{N} \\left\\| b^{\\text{pred},(s)} - b^{\\text{DNS},(s)} \\right\\|_F^2 + \\frac{\\gamma}{N}\\sum_{s=1}^{N} \\left( \\sum_{k=1}^3 \\left[ \\left(sp\\left(-\\frac{1}{3} - \\lambda_k^{(s)}, \\beta\\right)\\right)^2 + \\left(sp\\left(\\lambda_k^{(s)} - \\frac{2}{3}, \\beta\\right)\\right)^2 \\right] + \\mu \\left( \\operatorname{tr}(b^{\\text{pred},(s)}) \\right)^2 \\right)\n$$\nThis formulation correctly translates the physical and mathematical requirements into a computable, differentiable loss function suitable for gradient-based optimization of the model coefficients $\\mathbf{a}$. The implementation will compute this quantity for each of the specified test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy import ...\n\ndef solve():\n    \"\"\"\n    Solves the problem of computing the batch loss for machine-learning\n    augmentation of turbulence closures, including a realizability penalty.\n    \"\"\"\n\n    test_cases = [\n        # Test case 1\n        {\n            \"N\": 2,\n            \"gamma\": 10.0,\n            \"mu\": 5.0,\n            \"beta\": 50.0,\n            \"a\": np.array([0.0, 0.3, 0.2, -0.1]),\n            \"samples\": [\n                {\n                    \"S\": np.array([[0.0, 0.1, 0.0], [0.1, 0.0, 0.05], [0.0, 0.05, 0.0]]),\n                    \"R\": np.array([[0.0, 0.02, 0.0], [-0.02, 0.0, 0.01], [0.0, -0.01, 0.0]]),\n                    \"a_dns\": np.array([0.0, 0.25, 0.15, -0.05])\n                },\n                {\n                    \"S\": np.array([[0.0, -0.08, 0.0], [-0.08, 0.0, 0.03], [0.0, 0.03, 0.0]]),\n                    \"R\": np.array([[0.0, -0.01, 0.0], [0.01, 0.0, -0.02], [0.0, 0.02, 0.0]]),\n                    \"a_dns\": np.array([0.0, 0.22, 0.10, -0.04])\n                }\n            ]\n        },\n        # Test case 2\n        {\n            \"N\": 1,\n            \"gamma\": 10.0,\n            \"mu\": 5.0,\n            \"beta\": 50.0,\n            \"a\": np.array([0.0, 0.8, 0.5, 0.0]),\n            \"samples\": [\n                {\n                    \"S\": np.array([[1.0, 0.0, 0.0], [0.0, -1.0, 0.0], [0.0, 0.0, 0.0]]),\n                    \"R\": np.zeros((3, 3)),\n                    \"b_dns\": np.zeros((3, 3))\n                }\n            ]\n        },\n        # Test case 3\n        {\n            \"N\": 1,\n            \"gamma\": 10.0,\n            \"mu\": 5.0,\n            \"beta\": 50.0,\n            \"a\": np.array([0.1, 0.0, 0.0, 0.0]),\n            \"samples\": [\n                {\n                    \"S\": np.zeros((3, 3)),\n                    \"R\": np.zeros((3, 3)),\n                    \"b_dns\": np.zeros((3, 3))\n                }\n            ]\n        },\n        # Test case 4\n        {\n            \"N\": 1,\n            \"gamma\": 10.0,\n            \"mu\": 5.0,\n            \"beta\": 50.0,\n            \"a\": np.array([0.0, 0.5, 0.5, 0.0]),\n            \"samples\": [\n                {\n                    \"S\": np.array([[1.0, 0.0, 0.0], [0.0, -1.0, 0.0], [0.0, 0.0, 0.0]]),\n                    \"R\": np.zeros((3, 3)),\n                    # b_dns will be set to b_pred\n                }\n            ]\n        }\n    ]\n\n    def compute_b(S, R, a):\n        \"\"\"Computes the anisotropy tensor b from S, R, and coefficients a.\"\"\"\n        I = np.identity(3)\n        S2 = S @ S\n        R2 = R @ R\n        T1 = S\n        T2 = S2 - np.trace(S2) / 3.0 * I\n        T3 = R2 - np.trace(R2) / 3.0 * I\n        return a[0] * I + a[1] * T1 + a[2] * T2 + a[3] * T3\n\n    def softplus(x, beta):\n        \"\"\"Numerically stable softplus function.\"\"\"\n        # sp(x, beta) = (1/beta) * log(1 + exp(beta*x))\n        # This is computed via np.logaddexp(0, beta * x) to avoid overflow\n        return (1.0 / beta) * np.logaddexp(0.0, beta * x)\n\n    results = []\n    for case in test_cases:\n        N = case[\"N\"]\n        gamma = case[\"gamma\"]\n        mu = case[\"mu\"]\n        beta = case[\"beta\"]\n        a_pred_coeffs = case[\"a\"]\n        \n        total_misfit = 0.0\n        total_penalty = 0.0\n\n        for s_data in case[\"samples\"]:\n            S = s_data[\"S\"]\n            R = s_data[\"R\"]\n            \n            # 1. Compute predicted anisotropy tensor b_pred\n            b_pred = compute_b(S, R, a_pred_coeffs)\n\n            # 2. Compute target anisotropy tensor b_dns\n            if \"b_dns\" in s_data:\n                b_dns = s_data[\"b_dns\"]\n            elif \"a_dns\" in s_data:\n                b_dns = compute_b(S, R, s_data[\"a_dns\"])\n            else: # Test Case 4: b_dns = b_pred\n                b_dns = b_pred\n\n            # 3. Compute data misfit term\n            misfit = np.sum((b_pred - b_dns)**2)\n            total_misfit += misfit\n\n            # 4. Compute realizability penalty term\n            # 4a. Trace penalty\n            trace_penalty = mu * (np.trace(b_pred))**2\n\n            # 4b. Eigenvalue penalty\n            # Since b_pred is real and symmetric, use eigvalsh\n            eigenvalues = np.linalg.eigvalsh(b_pred)\n            \n            # Violations of lower bound: -1/3 <= lambda\n            pen_lower = softplus(-1.0/3.0 - eigenvalues, beta)\n            \n            # Violations of upper bound: lambda <= 2/3\n            pen_upper = softplus(eigenvalues - 2.0/3.0, beta)\n            \n            eigenvalue_penalty = np.sum(pen_lower**2 + pen_upper**2)\n            \n            sample_penalty = eigenvalue_penalty + trace_penalty\n            total_penalty += sample_penalty\n\n        # 5. Compute final batch loss\n        mean_misfit = total_misfit / N\n        mean_penalty = total_penalty / N\n        batch_loss = mean_misfit + gamma * mean_penalty\n        results.append(batch_loss)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3342982"}, {"introduction": "The ultimate goal of an ML-augmented closure is often to improve predictions of system-level engineering quantities, such as lift or drag. This capstone exercise contrasts two advanced, end-to-end training paradigms for achieving this goal. You will implement and compare a 'local' training approach that targets fundamental flow quantities against a 'global' approach that uses the powerful adjoint method to directly minimize error in an engineering observable, illuminating the trade-offs between different training philosophies in a practical, differentiable programming context [@problem_id:3343024].", "problem": "Consider a one-dimensional steady scalar convection–diffusion surrogate for Reynolds-averaged Navier–Stokes closures on a uniform grid over the interval $\\left[0,1\\right]$ with Dirichlet boundary conditions. Let the unknown discrete state be the vector $\\mathbf{u}\\in\\mathbb{R}^{n}$ at the interior grid points, and let the effective diffusivity at cell faces be modeled as $k\\left(\\theta,x\\right)=\\nu\\left(1+\\theta f\\left(x\\right)\\right)$, where $\\nu&gt;0$ is a given molecular viscosity, $\\theta\\in\\mathbb{R}$ is a scalar machine-learning closure parameter, and $f\\left(x\\right)$ is a prescribed feature field. The discrete linear system takes the form\n$$\n\\mathbf{A}\\left(\\theta\\right)\\mathbf{u}=\\mathbf{b}\\left(\\theta\\right),\n$$\nobtained by second-order central differences for the diffusion term and first-order upwind for positive convection speed. A positive constant convection speed $a&gt;0$ is assumed. The left and right boundary values are $u_{L}$ and $u_{R}$, respectively. The observable of interest is a drag proxy defined as the left-wall shear,\n$$\nJ\\left(\\theta,\\mathbf{u}\\right)=k_{1/2}\\left(\\theta\\right)\\,\\frac{u_{1}-u_{L}}{\\Delta x},\n$$\nwhere $u_{1}$ is the first interior value and $k_{1/2}\\left(\\theta\\right)$ is the diffusivity at the leftmost face. The target value for this observable, $J^{\\star}$, is generated by the same model evaluated at a hidden ground-truth parameter $\\theta^{\\star}$ using the same discretization and parameters.\n\nYour task is to implement and compare two training strategies to estimate $\\theta$:\n- Strategy A (global, adjoint-based): minimize the scalar loss\n$$\n\\mathcal{L}_{J}\\left(\\theta\\right)=\\tfrac{1}{2}\\left(J\\left(\\theta,\\mathbf{u}\\left(\\theta\\right)\\right)-J^{\\star}\\right)^{2},\n$$\nand compute its gradient with respect to $\\theta$ using a discrete adjoint of the linearized system. For a differentiable linear system $\\mathbf{A}\\left(\\theta\\right)\\mathbf{u}\\left(\\theta\\right)=\\mathbf{b}\\left(\\theta\\right)$ and observable $J\\left(\\theta,\\mathbf{u}\\right)$, the total derivative satisfies\n$$\n\\frac{\\mathrm{d}J}{\\mathrm{d}\\theta}=\\frac{\\partial J}{\\partial \\theta}+\\boldsymbol{\\lambda}^{\\top}\\left(\\frac{\\partial \\mathbf{b}}{\\partial \\theta}-\\frac{\\partial \\mathbf{A}}{\\partial \\theta}\\,\\mathbf{u}\\right),\n$$\nwhere the adjoint variable $\\boldsymbol{\\lambda}\\in\\mathbb{R}^{n}$ solves the adjoint linear system\n$$\n\\mathbf{A}\\left(\\theta\\right)^{\\top}\\boldsymbol{\\lambda}=\\frac{\\partial J}{\\partial \\mathbf{u}}.\n$$\nUse gradient descent updates $\\theta\\leftarrow\\theta-\\eta_{J}\\,\\frac{\\mathrm{d}\\mathcal{L}_{J}}{\\mathrm{d}\\theta}$ for a specified learning rate $\\eta_{J}$.\n\n- Strategy B (local, stress-target): define the facewise true shear stresses from the ground-truth state as $\\tau_{f}^{\\star}=k_{f}\\left(\\theta^{\\star}\\right)\\,g_{f}^{\\star}$, where $g_{f}^{\\star}$ is the true face gradient computed from $\\mathbf{u}\\left(\\theta^{\\star}\\right)$ and the boundary values. For a candidate $\\theta$, define the predicted stresses as $\\tau_{f}\\left(\\theta\\right)=k_{f}\\left(\\theta\\right)\\,g_{f}^{\\star}$, and minimize the local quadratic loss\n$$\n\\mathcal{L}_{S}\\left(\\theta\\right)=\\tfrac{1}{2}\\sum_{f} \\left(\\tau_{f}\\left(\\theta\\right)-\\tau_{f}^{\\star}\\right)^{2}.\n$$\nUse the exact local gradient $\\frac{\\mathrm{d}\\mathcal{L}_{S}}{\\mathrm{d}\\theta}$ and gradient descent updates $\\theta\\leftarrow\\theta-\\eta_{S}\\,\\frac{\\mathrm{d}\\mathcal{L}_{S}}{\\mathrm{d}\\theta}$ for a specified learning rate $\\eta_{S}$.\n\nYour program must:\n- Assemble $\\mathbf{A}\\left(\\theta\\right)$ and $\\mathbf{b}\\left(\\theta\\right)$ on a uniform grid with $N_{x}$ total nodes, where the interior dimension is $n=N_{x}-2$. Let the grid spacing be $\\Delta x=\\frac{1}{N_{x}-1}$, and enforce $u\\left(0\\right)=u_{L}$ and $u\\left(1\\right)=u_{R}$. Use $a&gt;0$ upwind for the convection term. The diffusion operator is built with face diffusivities $k_{i+1/2}\\left(\\theta\\right)=\\nu\\left(1+\\theta f_{i+1/2}\\right)$, and the convection operator uses an upwind stencil $(a/\\Delta x)\\left(u_{i}-u_{i-1}\\right)$.\n- Define $f\\left(x\\right)=\\alpha\\sin\\left(2\\pi x\\right)+\\beta\\cos\\left(4\\pi x\\right)$ at faces with $x$ evaluated at face centers and constants $\\alpha$ and $\\beta$ chosen so that $1+\\theta f\\left(x\\right)&gt;0$ for all faces and all iterations in the test suite; use $\\alpha=0.5$ and $\\beta=0.25$.\n- Use $u_{L}=1$ and $u_{R}=0$.\n- Implement both training strategies starting from $\\theta_{0}=0$ for a fixed number of gradient descent iterations $K$.\n- Report convergence and stability metrics defined as follows:\n  - Convergence: a boolean that is true if $\\left|\\theta_{K}-\\theta^{\\star}\\right|&lt;\\varepsilon$, with tolerance $\\varepsilon=10^{-3}$.\n  - Stability: a boolean that is true if the corresponding loss sequence is monotonically non-increasing up to a numerical tolerance $\\delta=10^{-12}$, that is, $\\mathcal{L}_{t}-\\mathcal{L}_{t-1}\\leq \\delta$ for all iterations $t\\in\\{1,\\dots,K\\}$.\n\nDesign the algorithm from first principles, starting from the discrete conservation law and the chain rule of calculus for the adjoint derivation. Ensure scientific plausibility by keeping $1+\\theta f\\left(x\\right)&gt;0$ throughout all runs.\n\nTest suite:\nUse the following four parameter sets, each defining one test case. For all, use $\\theta^{\\star}=0.8$, $\\nu=0.05$, $u_{L}=1$, $u_{R}=0$, $\\alpha=0.5$, $\\beta=0.25$, and $K=60$.\n- Case $1$: $N_{x}=33$, $a=1.0$, $\\eta_{J}=0.1$, $\\eta_{S}=0.5$.\n- Case $2$: $N_{x}=65$, $a=5.0$, $\\eta_{J}=0.05$, $\\eta_{S}=0.3$.\n- Case $3$: $N_{x}=33$, $a=1.0$, $\\eta_{J}=1.2$, $\\eta_{S}=0.5$.\n- Case $4$: $N_{x}=33$, $a=1.0$, $\\eta_{J}=0.005$, $\\eta_{S}=1.0$.\n\nFinal output format:\nYour program should produce a single line of output containing a list of results, one per test case, where each test case result is itself a list\n$$\n\\left[\\theta_{J,\\mathrm{final}},\\ \\theta_{S,\\mathrm{final}},\\ \\mathcal{L}_{J,\\mathrm{final}},\\ \\mathcal{L}_{S,\\mathrm{final}},\\ c_{J},\\ s_{J},\\ c_{S},\\ s_{S}\\right],\n$$\nwith $\\theta_{J,\\mathrm{final}}$ and $\\theta_{S,\\mathrm{final}}$ the final parameters for the global and local strategies, $\\mathcal{L}_{J,\\mathrm{final}}$ and $\\mathcal{L}_{S,\\mathrm{final}}$ the final losses, and $c_{\\cdot}$ and $s_{\\cdot}$ the booleans for convergence and stability, respectively. The final printed line must be a single list containing the four per-case lists, formatted as a comma-separated list enclosed in square brackets (for example, $\\left[\\left[\\cdot\\right],\\left[\\cdot\\right],\\left[\\cdot\\right],\\left[\\cdot\\right]\\right]$). All quantities are dimensionless, so no physical units are required. Angles, if any, must be in radians, but no angle appears in this problem.", "solution": "The user wants me to act as a STEM professor and solve a problem related to machine learning augmentation of turbulence closures.\n\nThe problem asks me to implement and compare two different training strategies for a scalar parameter `theta` in a 1D steady convection-diffusion equation.\n\nFirst, I must meticulously validate the problem statement.\n\n## Problem Validation\n\n### Step 1: Extract Givens\n\n- **Equation**: 1D steady scalar convection-diffusion on a uniform grid over $[0, 1]$.\n- **Unknown**: Discrete state vector $\\mathbf{u} \\in \\mathbb{R}^n$ at interior grid points.\n- **Model for effective diffusivity**: $k(\\theta, x) = \\nu(1 + \\theta f(x))$, where:\n    - $\\nu > 0$ is molecular viscosity.\n    - $\\theta \\in \\mathbb{R}$ is a scalar ML closure parameter.\n    - $f(x)$ is a prescribed feature field.\n- **Discrete linear system**: $\\mathbf{A}(\\theta)\\mathbf{u} = \\mathbf{b}(\\theta)$.\n- **Discretization**:\n    - Second-order central differences for diffusion.\n    - First-order upwind for positive convection speed $a > 0$.\n- **Boundary conditions**: Dirichlet, $u(0) = u_L$, $u(1) = u_R$.\n- **Observable**: Drag proxy (left-wall shear), $J(\\theta, \\mathbf{u}) = k_{1/2}(\\theta) \\frac{u_1 - u_L}{\\Delta x}$.\n    - $u_1$ is the first interior value.\n    - $k_{1/2}(\\theta)$ is diffusivity at the leftmost face.\n- **Target value**: $J^\\star$ generated by the same model at a hidden ground-truth parameter $\\theta^\\star$.\n- **Task**: Implement and compare two training strategies to estimate $\\theta$.\n\n**Strategy A (global, adjoint-based)**:\n- **Loss function**: $\\mathcal{L}_J(\\theta) = \\frac{1}{2}(J(\\theta, \\mathbf{u}(\\theta)) - J^\\star)^2$.\n- **Gradient computation**: Use discrete adjoint method.\n    - Total derivative: $\\frac{\\mathrm{d}J}{\\mathrm{d}\\theta} = \\frac{\\partial J}{\\partial \\theta} + \\boldsymbol{\\lambda}^{\\top} (\\frac{\\partial \\mathbf{b}}{\\partial \\theta} - \\frac{\\partial \\mathbf{A}}{\\partial \\theta} \\mathbf{u})$.\n    - Adjoint system: $\\mathbf{A}(\\theta)^{\\top}\\boldsymbol{\\lambda} = \\frac{\\partial J}{\\partial \\mathbf{u}}$.\n- **Update rule**: Gradient descent, $\\theta \\leftarrow \\theta - \\eta_J \\frac{\\mathrm{d}\\mathcal{L}_J}{\\mathrm{d}\\theta}$.\n\n**Strategy B (local, stress-target)**:\n- **True shear stresses**: $\\tau_{f}^{\\star} = k_{f}(\\theta^{\\star}) g_{f}^{\\star}$, where $g_f^\\star$ is the true face gradient from $\\mathbf{u}(\\theta^\\star)$.\n- **Predicted stresses**: $\\tau_f(\\theta) = k_f(\\theta) g_f^\\star$.\n- **Loss function**: $\\mathcal{L}_S(\\theta) = \\frac{1}{2} \\sum_{f} (\\tau_f(\\theta) - \\tau_f^\\star)^2$.\n- **Gradient computation**: Exact local gradient $\\frac{\\mathrm{d}\\mathcal{L}_S}{\\mathrm{d}\\theta}$.\n- **Update rule**: Gradient descent, $\\theta \\leftarrow \\theta - \\eta_S \\frac{\\mathrm{d}\\mathcal{L}_S}{\\mathrm{d}\\theta}$.\n\n**Implementation Details**:\n- **Grid**: Uniform grid with $N_x$ total nodes.\n- **Interior dimension**: $n = N_x - 2$.\n- **Grid spacing**: $\\Delta x = \\frac{1}{N_x - 1}$.\n- **Boundary conditions**: $u(0)=u_L$, $u(1)=u_R$.\n- **Convection operator stencil**: $(a/\\Delta x)(u_i - u_{i-1})$.\n- **Diffusion operator**: Built with face diffusivities $k_{i+1/2}(\\theta) = \\nu(1 + \\theta f_{i+1/2})$.\n- **Feature field**: $f(x) = \\alpha \\sin(2\\pi x) + \\beta \\cos(4\\pi x)$ at face centers.\n- **Constants**: $\\alpha=0.5, \\beta=0.25$.\n- **Positivity constraint**: Ensure $1 + \\theta f(x) > 0$.\n- **Boundary values**: $u_L=1, u_R=0$.\n- **Initial parameter**: $\\theta_0 = 0$.\n- **Iterations**: $K$.\n\n**Metrics**:\n- **Convergence**: $|\\theta_K - \\theta^\\star| < \\varepsilon$, with $\\varepsilon = 10^{-3}$.\n- **Stability**: $\\mathcal{L}_t - \\mathcal{L}_{t-1} \\leq \\delta$, with $\\delta = 10^{-12}$ for $t \\in \\{1, ..., K\\}$.\n\n**Test Suite**:\n- Common parameters: $\\theta^\\star = 0.8$, $\\nu = 0.05$, $u_L=1, u_R=0$, $\\alpha=0.5, \\beta=0.25, K=60$.\n- Case 1: $N_x=33, a=1.0, \\eta_J=0.1, \\eta_S=0.5$.\n- Case 2: $N_x=65, a=5.0, \\eta_J=0.05, \\eta_S=0.3$.\n- Case 3: $N_x=33, a=1.0, \\eta_J=1.2, \\eta_S=0.5$.\n- Case 4: $N_x=33, a=1.0, \\eta_J=0.005, \\eta_S=1.0$.\n\n**Output Format**:\n- A single line `[[...], [...], [...], [...]]`.\n- Each inner list: `[\\theta_{J,final}, \\theta_{S,final}, \\mathcal{L}_{J,final}, \\mathcal{L}_{S,final}, c_J, s_J, c_S, s_S]`.\n\n### Step 2: Validate Using Extracted Givens\n\n1.  **Scientifically Grounded**: The problem is well-grounded in computational fluid dynamics, specifically in the area of data-driven turbulence modeling. It uses a standard 1D convection-diffusion equation as a surrogate, a common practice for developing and testing new methods. The techniques involved, such as finite difference discretization, adjoint-based optimization, and physics-informed loss functions, are all established and scientifically valid.\n2.  **Well-Posed**: The problem is well-posed. The use of a first-order upwind scheme for the convective term and central differences for diffusion on a steady problem generally leads to a diagonally dominant and thus non-singular system matrix $\\mathbf{A}$, ensuring a unique solution for the state vector $\\mathbf{u}$. The optimization tasks are clearly defined with specific loss functions. The problem specifies all necessary parameters and conditions.\n3.  **Objective**: All aspects of the problem are defined with mathematical precision and objective numerical values. There is no subjective or ambiguous language.\n4.  **Scientific or Factual Unsoundness**: The problem is factually sound. The physical constraint that diffusivity must be positive ($k(\\theta, x) > 0$) is noted. For the given feature field $f(x) = 0.5 \\sin(2\\pi x) + 0.25 \\cos(4\\pi x)$, the minimum value on $[0,1]$ is $-0.75$. The positivity constraint $1 + \\theta f(x) > 0$ requires $1 + \\theta(-0.75) > 0$ for positive $\\theta$, which implies $\\theta < 4/3 \\approx 1.333$. The test cases use $\\theta^\\star=0.8$ and start training from $\\theta=0$, which is well within the valid range. The learning rates are chosen to test a range of behaviors, including potential instability (e.g., $\\eta_J=1.2$), which is a valid part of a numerical methods investigation.\n5.  **Non-Formalizable or Irrelevant**: The problem is highly formalizable and directly relevant to its stated topic.\n6.  **Incomplete or Contradictory Setup**: The setup is complete and self-consistent. All required constants, boundary conditions, and discretization schemes are provided.\n7.  **Unrealistic or Infeasible**: The problem is a standard numerical experiment and is entirely feasible.\n8.  **Ill-Posed or Poorly Structured**: The problem is clearly structured and poses a solvable numerical task.\n9.  **Pseudo-Profound, Trivial, or Tautological**: The problem is non-trivial, requiring a careful implementation of numerical methods for PDEs, the adjoint method for sensitivity analysis, and optimization loops. It represents a conceptually sound and educational task in computational science.\n10. **Outside Scientific Verifiability**: The results are computationally verifiable.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\n### Principle-Based Design\n\nThe solution will be developed by systematically discretizing the governing partial differential equation and then implementing the two specified optimization strategies.\n\n**1. Discretization of the Governing Equation**\n\nThe one-dimensional steady-state convection-diffusion equation is:\n$$\n\\frac{\\mathrm{d}}{\\mathrm{d}x} \\left( a u(x) - k(\\theta,x) \\frac{\\mathrm{d}u}{\\mathrm{d}x} \\right) = 0\n$$\nWe discretize this equation on a uniform grid with $N_x$ nodes $x_i = i \\Delta x$ for $i=0, \\dots, N_x-1$, where $\\Delta x = 1/(N_x-1)$. The unknowns are the values $u_i$ at the $n=N_x-2$ interior nodes, $i=1, \\dots, n$. The values at the boundaries are fixed: $u_0=u_L$ and $u_{N_x-1}=u_R$.\n\nThe problem specifies a finite volume-like discretization. The discrete equation at an interior node $i$ is formulated based on the convective and diffusive fluxes at the cell faces $x_{i-1/2}$ and $x_{i+1/2}$. The residual at node $i$ is:\n$$\nR_i = \\left( a u_i - a u_{i-1} \\right) \\frac{1}{\\Delta x} - \\frac{1}{\\Delta x}\\left( k_{i+1/2} \\frac{u_{i+1}-u_i}{\\Delta x} - k_{i-1/2} \\frac{u_i-u_{i-1}}{\\Delta x} \\right) = 0\n$$\nwhere $k_{i+1/2} = k(\\theta, x_{i+1/2})$. Multiplying by $\\Delta x^2$ and rearranging terms for $u_{i-1}$, $u_i$, and $u_{i+1}$ yields the linear equation for row $i$:\n$$\n(-a\\Delta x - k_{i-1/2}) u_{i-1} + (a\\Delta x + k_{i+1/2} + k_{i-1/2}) u_i - k_{i+1/2} u_{i+1} = 0\n$$\nThis equation holds for interior nodes $i=1, \\dots, n$. For $i=1$, $u_0=u_L$ is known and moved to the right-hand side. For $i=n$, $u_{n+1}=u_{N_x-1}=u_R$ is known and moved to the right-hand side. This defines the tridiagonal system $\\mathbf{A}(\\theta)\\mathbf{u} = \\mathbf{b}(\\theta)$.\n\n**2. Strategy A: Global Adjoint-Based Optimization**\n\nThis strategy minimizes the loss $\\mathcal{L}_{J}(\\theta) = \\frac{1}{2}(J(\\theta) - J^{\\star})^2$, where $J(\\theta) = J(\\theta, \\mathbf{u}(\\theta))$ is the predicted wall shear. The gradient of the loss is $\\frac{\\mathrm{d}\\mathcal{L}_{J}}{\\mathrm{d}\\theta} = (J-J^\\star)\\frac{\\mathrm{d}J}{\\mathrm{d}\\theta}$.\n\nThe core of this method is the computation of the total derivative $\\frac{\\mathrm{d}J}{\\mathrm{d}\\theta}$, which accounts for the dependence of the state $\\mathbf{u}$ on the parameter $\\theta$. Using the chain rule on the implicit function defined by the linear system $\\mathbf{A}\\mathbf{u}=\\mathbf{b}$ gives:\n$$\n\\frac{\\mathrm{d}J}{\\mathrm{d}\\theta} = \\frac{\\partial J}{\\partial \\theta} + \\left(\\frac{\\partial J}{\\partial \\mathbf{u}}\\right)^{\\top} \\frac{\\mathrm{d}\\mathbf{u}}{\\mathrm{d}\\theta}\n$$\nFrom $\\mathbf{A}\\mathbf{u}=\\mathbf{b}$, we differentiate with respect to $\\theta$ to find $\\frac{\\mathrm{d}\\mathbf{u}}{\\mathrm{d}\\theta}$:\n$$\n\\frac{\\partial \\mathbf{A}}{\\partial \\theta}\\mathbf{u} + \\mathbf{A}\\frac{\\mathrm{d}\\mathbf{u}}{\\mathrm{d}\\theta} = \\frac{\\partial \\mathbf{b}}{\\partial \\theta} \\implies \\frac{\\mathrm{d}\\mathbf{u}}{\\mathrm{d}\\theta} = \\mathbf{A}^{-1}\\left( \\frac{\\partial \\mathbf{b}}{\\partial \\theta} - \\frac{\\partial \\mathbf{A}}{\\partial \\theta}\\mathbf{u} \\right)\n$$\nSubstituting this into the expression for $\\frac{\\mathrm{d}J}{\\mathrm{d}\\theta}$ gives:\n$$\n\\frac{\\mathrm{d}J}{\\mathrm{d}\\theta} = \\frac{\\partial J}{\\partial \\theta} + \\left(\\frac{\\partial J}{\\partial \\mathbf{u}}\\right)^{\\top} \\mathbf{A}^{-1}\\left( \\frac{\\partial \\mathbf{b}}{\\partial \\theta} - \\frac{\\partial \\mathbf{A}}{\\partial \\theta}\\mathbf{u} \\right)\n$$\nTo avoid the computationally expensive matrix inversion $\\mathbf{A}^{-1}$, we introduce the adjoint vector $\\boldsymbol{\\lambda}$, defined as the solution to $\\mathbf{A}^{\\top}\\boldsymbol{\\lambda} = (\\frac{\\partial J}{\\partial \\mathbf{u}})$. This allows us to write:\n$$\n\\left(\\frac{\\partial J}{\\partial \\mathbf{u}}\\right)^{\\top} \\mathbf{A}^{-1} = (\\mathbf{A}^{-\\top} \\frac{\\partial J}{\\partial \\mathbf{u}})^{\\top} = \\boldsymbol{\\lambda}^{\\top}\n$$\nThis leads to the efficient formula for the total derivative provided in the problem statement:\n$$\n\\frac{\\mathrm{d}J}{\\mathrm{d}\\theta}=\\frac{\\partial J}{\\partial \\theta}+\\boldsymbol{\\lambda}^{\\top}\\left(\\frac{\\partial \\mathbf{b}}{\\partial \\theta}-\\frac{\\partial \\mathbf{A}}{\\partial \\theta}\\,\\mathbf{u}\\right)\n$$\nThe training algorithm involves: (i) solving the primal system for $\\mathbf{u}$, (ii) solving the adjoint system for $\\boldsymbol{\\lambda}$, (iii) computing all necessary partial derivatives, (iv) assembling the total gradient, and (v) updating $\\theta$ via gradient descent.\n\n**3. Strategy B: Local Stress-Target Optimization**\n\nThis strategy minimizes a loss function based on local quantities, the shear stresses $\\tau_f = k_f g_f$ at each cell face $f$. The \"true\" gradients $g_f^\\star$ are computed from the ground-truth state $\\mathbf{u}(\\theta^\\star)$ and are then held fixed. The model predicts stresses as $\\tau_f(\\theta) = k_f(\\theta) g_f^\\star$. The loss is:\n$$\n\\mathcal{L}_{S}(\\theta) = \\frac{1}{2} \\sum_{f} (\\tau_f(\\theta) - \\tau_f^\\star)^2\n$$\nThe gradient with respect to $\\theta$ is straightforwardly computed using the chain rule:\n$$\n\\frac{\\mathrm{d}\\mathcal{L}_{S}}{\\mathrm{d}\\theta} = \\sum_{f} (\\tau_f(\\theta) - \\tau_f^\\star) \\frac{\\partial \\tau_f(\\theta)}{\\partial \\theta}\n$$\nSince $\\tau_f(\\theta) = \\nu(1+\\theta f_f) g_f^\\star$, its derivative is $\\frac{\\partial \\tau_f(\\theta)}{\\partial \\theta} = \\nu f_f g_f^\\star$. This training strategy is \"local\" because the gradient calculation does not require solving a global linear system (neither primal nor adjoint) at each iteration. It only requires the ground-truth gradients $g_f^\\star$ as input data. Consequently, each training step is computationally much cheaper than in Strategy A.\n\nThe implementation will follow these principles, with careful attention to matrix construction and index management.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and compares two training strategies for a machine-learning closure\n    parameter in a 1D convection-diffusion problem, as specified in the problem statement.\n    \"\"\"\n    \n    # Test suite definition from the problem statement\n    test_cases = [\n        # (Nx, a, eta_J, eta_S)\n        (33, 1.0, 0.1, 0.5),\n        (65, 5.0, 0.05, 0.3),\n        (33, 1.0, 1.2, 0.5),\n        (33, 1.0, 0.005, 1.0),\n    ]\n    \n    # Shared physical and numerical parameters\n    theta_star = 0.8\n    nu = 0.05\n    uL = 1.0\n    uR = 0.0\n    alpha = 0.5\n    beta = 0.25\n    K = 60\n    epsilon_conv = 1e-3\n    delta_stab = 1e-12\n\n    def feature_field(x, alpha, beta):\n        \"\"\"Computes the prescribed feature field f(x).\"\"\"\n        return alpha * np.sin(2 * np.pi * x) + beta * np.cos(4 * np.pi * x)\n\n    def run_case(Nx, a, eta_J, eta_S):\n        \"\"\"\n        Executes a single test case, including ground truth generation,\n        training with both strategies, and metric calculation.\n        \"\"\"\n        # Grid setup\n        n = Nx - 2  # Number of interior points\n        dx = 1.0 / (Nx - 1)\n        \n        # Face coordinates and feature field `f` evaluated at faces\n        x_faces = np.linspace(dx / 2, 1 - dx / 2, Nx - 1)\n        f_faces = feature_field(x_faces, alpha, beta)\n        \n        def get_k_faces(theta):\n            \"\"\"Computes effective diffusivity k(theta) at all cell faces.\"\"\"\n            return nu * (1 + theta * f_faces)\n        \n        def assemble_system(theta):\n            \"\"\"Assembles the discrete linear system A(theta)*u = b(theta).\"\"\"\n            k_f = get_k_faces(theta)\n            A = np.zeros((n, n))\n            b = np.zeros(n)\n\n            # Assemble the tridiagonal matrix A based on the discrete stencil.\n            # Main diagonal\n            np.fill_diagonal(A, a * dx + k_f[0:n] + k_f[1:n+1])\n            # Super-diagonal\n            np.fill_diagonal(A[0:, 1:], -k_f[1:n])\n            # Sub-diagonal\n            np.fill_diagonal(A[1:, 0:], -a * dx - k_f[1:n])\n            \n            # Assemble the RHS vector b from boundary conditions.\n            b[0] = uL * (a * dx + k_f[0])\n            b[n-1] += uR * k_f[n] # Correction for when n=1\n            \n            return A, b\n        \n        # --- Ground Truth Generation ---\n        A_star, b_star = assemble_system(theta_star)\n        u_star = np.linalg.solve(A_star, b_star)\n        u_star_full = np.concatenate(([uL], u_star, [uR]))\n        \n        k_faces_star = get_k_faces(theta_star)\n        J_star = k_faces_star[0] * (u_star[0] - uL) / dx\n        \n        g_faces_star = (u_star_full[1:] - u_star_full[:-1]) / dx\n        tau_faces_star = k_faces_star * g_faces_star\n        \n        # --- Strategy A: Global, Adjoint-based Training ---\n        theta_J = 0.0\n        loss_J_history = []\n        for _ in range(K):\n            A_J, b_J = assemble_system(theta_J)\n            u_J = np.linalg.solve(A_J, b_J)\n            k_faces_J = get_k_faces(theta_J)\n            \n            J = k_faces_J[0] * (u_J[0] - uL) / dx\n            loss_J = 0.5 * (J - J_star)**2\n            loss_J_history.append(loss_J)\n            \n            # Solve adjoint system A^T * lambda = dJ/du\n            dJ_du = np.zeros(n)\n            dJ_du[0] = k_faces_J[0] / dx\n            lambda_vec = np.linalg.solve(A_J.T, dJ_du)\n            \n            # Compute partial derivatives w.r.t. theta\n            dk_dtheta_faces = nu * f_faces\n            dJ_dtheta = dk_dtheta_faces[0] * (u_J[0] - uL) / dx\n            \n            db_dtheta = np.zeros(n)\n            db_dtheta[0] = uL * dk_dtheta_faces[0]\n            if n > 0:\n                db_dtheta[n-1] += uR * dk_dtheta_faces[n]\n\n            dA_dtheta = np.zeros((n, n))\n            np.fill_diagonal(dA_dtheta, dk_dtheta_faces[0:n] + dk_dtheta_faces[1:n+1])\n            np.fill_diagonal(dA_dtheta[0:, 1:], -dk_dtheta_faces[1:n])\n            np.fill_diagonal(dA_dtheta[1:, 0:], -dk_dtheta_faces[1:n])\n\n            # Assemble total gradient and update theta\n            rhs_term = db_dtheta - dA_dtheta @ u_J\n            dJ_dtheta_total = dJ_dtheta + lambda_vec.T @ rhs_term\n            dLJ_dtheta = (J - J_star) * dJ_dtheta_total\n            theta_J -= eta_J * dLJ_dtheta\n            \n        final_theta_J = theta_J\n        final_loss_J = loss_J_history[-1]\n        \n        # --- Strategy B: Local, Stress-target Training ---\n        theta_S = 0.0\n        loss_S_history = []\n        for _ in range(K):\n            k_faces_S = get_k_faces(theta_S)\n            tau_faces_S = k_faces_S * g_faces_star\n            loss_S = 0.5 * np.sum((tau_faces_S - tau_faces_star)**2)\n            loss_S_history.append(loss_S)\n            \n            dtau_dtheta = (nu * f_faces) * g_faces_star\n            dLS_dtheta = np.sum((tau_faces_S - tau_faces_star) * dtau_dtheta)\n            theta_S -= eta_S * dLS_dtheta\n            \n        final_theta_S = theta_S\n        final_loss_S = loss_S_history[-1]\n        \n        # --- Metrics Calculation ---\n        c_J = abs(final_theta_J - theta_star) < epsilon_conv\n        c_S = abs(final_theta_S - theta_star) < epsilon_conv\n        s_J = all(loss_J_history[i] - loss_J_history[i-1] <= delta_stab for i in range(1, K))\n        s_S = all(loss_S_history[i] - loss_S_history[i-1] <= delta_stab for i in range(1, K)) if len(loss_S_history) > 1 else True\n        \n        return [final_theta_J, final_theta_S, final_loss_J, final_loss_S, c_J, s_J, c_S, s_S]\n    \n    # Run all test cases and collect results\n    all_results = [run_case(*params) for params in test_cases]\n    \n    # Format the final output string as specified\n    print(f\"[{','.join(map(str, all_results))}]\")\n\n# Execute the main function\nsolve()\n```", "id": "3343024"}]}