{"hands_on_practices": [{"introduction": "Proper Orthogonal Decomposition (POD) is fundamentally an application of Singular Value Decomposition (SVD) to a matrix of data snapshots. However, the standard SVD finds a basis that is optimal for the Euclidean norm. In many computational fluid dynamics applications, particularly those using finite element methods, the physically relevant inner product is a weighted one defined by a mass matrix, $M$. This practice [@problem_id:3356822] guides you through the crucial technique of computing $M$-orthonormal POD modes by transforming the problem into a standard Euclidean setting, performing SVD, and then transforming the modes back, ensuring your reduced-order basis correctly represents the system's energy.", "problem": "Consider a weighted inner product space associated with a diagonal mass matrix $M \\in \\mathbb{R}^{n \\times n}$ with strictly positive diagonal entries, representing a symmetric positive definite discretization of an $L^2$ inner product on a computational fluid dynamics state vector. Let $X \\in \\mathbb{R}^{n \\times k}$ be a snapshot matrix whose columns are state vectors sampled at distinct times. A set of $r$ Proper Orthogonal Decomposition (POD) modes is defined as the columns of a matrix $\\Phi \\in \\mathbb{R}^{n \\times r}$ that satisfy the $M$-orthonormality condition $\\Phi^\\top M \\Phi = I_r$ and span the subspace that minimizes the mean-square projection error of $X$ in the norm induced by $M$, namely the Frobenius norm $\\|X - \\Phi \\Phi^\\top M X\\|_{M,F}$, where $\\|A\\|_{M,F}^2 = \\mathrm{trace}(A^\\top M A)$.\n\nStarting from these definitions and the fundamental properties of orthogonal projections and Singular Value Decomposition (SVD) in Euclidean spaces, design an algorithm that computes $M$-orthonormal POD modes by transforming the snapshots to a Euclidean setting, performing a standard SVD, and recovering modes in the original coordinates so that the result satisfies the $M$-orthonormality condition. Your algorithm must also quantify the captured energy of the truncated POD subspace using the singular values obtained in the Euclidean setting, expressed as a decimal between $0$ and $1$.\n\nImplement the algorithm in a single self-contained program. For each test case below, compute:\n1. A boolean indicating whether the computed modes are $M$-orthonormal, i.e., whether $\\|\\Phi^\\top M \\Phi - I_r\\|_F \\leq \\varepsilon$ for a chosen numerical tolerance $\\varepsilon$.\n2. A float giving the fraction of energy captured by the first $r$ modes, defined as the sum of squares of the leading $r$ singular values divided by the sum of squares of all singular values obtained in the Euclidean-transformed snapshot matrix. Express this fraction as a decimal rounded to eight decimal places.\n\nUse $\\varepsilon = 10^{-10}$.\n\nTest suite:\n- Case 1: $n=5$, $k=4$, $M = \\mathrm{diag}([1.0, 2.0, 3.0, 0.5, 4.0])$, and\n$$\nX = \\begin{bmatrix}\n1 & 0 & 2 & -1 \\\\\n0 & 1 & 1 & 0 \\\\\n2 & 1 & 0 & 1 \\\\\n-1 & 0 & 1 & 2 \\\\\n0 & 2 & -1 & 1\n\\end{bmatrix}, \\quad r = 2.\n$$\n- Case 2: $n=6$, $k=5$, $M = \\mathrm{diag}([1,1,1,1,1,1])$, and\n$$\nX = \\begin{bmatrix}\n1 & 0 & 1 & 2 & 1 \\\\\n0 & 1 & 1 & 2 & 0 \\\\\n0 & 0 & 0 & 0 & 0 \\\\\n1 & 2 & 3 & 6 & 1 \\\\\n2 & 1 & 3 & 6 & 2 \\\\\n3 & 0 & 3 & 6 & 3\n\\end{bmatrix}, \\quad r = 2.\n$$\n- Case 3: $n=7$, $k=3$, $M = \\mathrm{diag}([10^{-8}, 10^{-4}, 10^{-2}, 1, 10^{2}, 10^{4}, 10^{8}])$, and\n$$\nX = \\begin{bmatrix}\n1 & 2 & 3 \\\\\n0 & 1 & 0 \\\\\n3 & 0 & 1 \\\\\n4 & 1 & -1 \\\\\n0 & 1 & 2 \\\\\n-2 & 0 & 0 \\\\\n1 & -1 & 1\n\\end{bmatrix}, \\quad r = 2.\n$$\n- Case 4: $n=4$, $k=2$, $M = \\mathrm{diag}([0.75, 1.25, 2.5, 0.5])$, and\n$$\nX = \\begin{bmatrix}\n1 & 0 \\\\\n0 & 1 \\\\\n1 & 1 \\\\\n-1 & 2\n\\end{bmatrix}, \\quad r = 2.\n$$\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets in the order\n$[b_1, f_1, b_2, f_2, b_3, f_3, b_4, f_4]$,\nwhere $b_i$ is the boolean orthonormality check for case $i$, and $f_i$ is the energy fraction for case $i$ rounded to eight decimal places.", "solution": "The problem is well-grounded in the theory of model order reduction, specifically Proper Orthogonal Decomposition (POD). The use of a weighted inner product ($M$-inner product) is a standard extension of POD, commonly encountered in finite element analysis where $M$ represents a mass matrix. The connection between the weighted norm optimization and a standard SVD via a change of coordinates is a fundamental and correct technique. The problem is well-posed. The existence and uniqueness (up to signs) of the optimal subspace are guaranteed by the Eckart-Young-Mirsky theorem applied in the transformed space. The givens for each test case are complete and sufficient to execute the required algorithm. The problem is stated with mathematical precision. All terms, norms, and objectives are formally defined. There is no ambiguity or subjectivity.\n\nThe objective is to find an $M$-orthonormal basis $\\Phi \\in \\mathbb{R}^{n \\times r}$ that minimizes the projection error $\\|X - \\Phi\\Phi^\\top M X\\|_{M,F}$. The core of the method is to transform this problem into an equivalent one in a standard Euclidean space.\n\nThe weighted inner product is defined as $\\langle u, v \\rangle_M = u^\\top M v$. Since $M$ is a diagonal matrix with strictly positive entries, it is symmetric positive definite. We can define a matrix $M^{1/2}$ as the diagonal matrix whose entries are the square roots of the diagonal entries of $M$. Thus, $M = M^{1/2} M^{1/2}$.\n\nConsider a change of coordinates for any vector $v \\in \\mathbb{R}^n$ to a new vector $\\tilde{v} \\in \\mathbb{R}^n$ defined by $\\tilde{v} = M^{1/2} v$. The standard Euclidean inner product in this transformed space relates to the weighted inner product in the original space as follows:\n$$\n\\langle \\tilde{u}, \\tilde{v} \\rangle = \\tilde{u}^\\top \\tilde{v} = (M^{1/2} u)^\\top (M^{1/2} v) = u^\\top (M^{1/2})^\\top M^{1/2} v = u^\\top M v = \\langle u, v \\rangle_M\n$$\nThis demonstrates that the $M$-inner product of original vectors is equivalent to the Euclidean inner product of the transformed vectors. Consequently, minimizing an error in the norm induced by $M$ is equivalent to minimizing the error in the standard Euclidean norm in the transformed space.\n\nWe apply this transformation to the snapshot matrix $X$:\n$$\n\\tilde{X} = M^{1/2} X\n$$\nThe problem is now to find an optimal rank-$r$ approximation of the data in $\\tilde{X}$. This is the standard POD problem in a Euclidean space, which is solved by the Singular Value Decomposition (SVD) of $\\tilde{X}$. Let the SVD of $\\tilde{X}$ be:\n$$\n\\tilde{X} = U \\Sigma V^\\top\n$$\nHere, the columns of $U \\in \\mathbb{R}^{n \\times p}$ (with $p = \\min(n, k)$) are the left singular vectors, which form an orthonormal basis. The optimal $r$-dimensional subspace for approximating the columns of $\\tilde{X}$ is spanned by the first $r$ left singular vectors. We denote the matrix of these vectors as $U_r \\in \\mathbb{R}^{n \\times r}$. These are the POD modes in the transformed space, let's call them $\\tilde{\\Phi} = U_r$. They satisfy the standard orthonormality condition $\\tilde{\\Phi}^\\top \\tilde{\\Phi} = U_r^\\top U_r = I_r$.\n\nTo obtain the POD modes $\\Phi$ in the original space, we must reverse the transformation:\n$$\n\\tilde{\\Phi} = M^{1/2} \\Phi \\implies \\Phi = M^{-1/2} \\tilde{\\Phi} = M^{-1/2} U_r\n$$\nHere, $M^{-1/2}$ is the diagonal matrix with diagonal entries $1/\\sqrt{M_{ii}}$.\n\nWe must verify that these modes satisfy the $M$-orthonormality condition $\\Phi^\\top M \\Phi = I_r$:\n$$\n\\Phi^\\top M \\Phi = (M^{-1/2} U_r)^\\top M (M^{-1/2} U_r) = U_r^\\top (M^{-1/2})^\\top M M^{-1/2} U_r\n$$\nSince $M^{-1/2}$ is diagonal, it is symmetric, so $(M^{-1/2})^\\top = M^{-1/2}$.\n$$\n\\Phi^\\top M \\Phi = U_r^\\top M^{-1/2} M M^{-1/2} U_r = U_r^\\top (M^{-1/2} M^{1/2})(M^{1/2} M^{-1/2}) U_r = U_r^\\top I I U_r = U_r^\\top U_r = I_r\n$$\nThe condition is satisfied. This confirms that $\\Phi = M^{-1/2} U_r$ are the correct $M$-orthonormal POD modes.\n\nThe fraction of energy captured by the first $r$ modes is calculated using the singular values, $\\sigma_i$, obtained from the SVD of the transformed matrix $\\tilde{X}$. The total energy in the snapshots (in the weighted norm) is proportional to $\\mathrm{trace}(\\tilde{X}^\\top \\tilde{X}) = \\sum_j \\sigma_j^2$. The energy captured by the first $r$ modes is $\\sum_{i=1}^r \\sigma_i^2$. The fraction is thus:\n$$\nf = \\frac{\\sum_{i=1}^r \\sigma_i^2}{\\sum_{j=1}^{\\min(n,k)} \\sigma_j^2}\n$$\n\nThe algorithm is as follows:\n1.  Given $M$ (as a vector of diagonal entries), $X$, and $r$, construct the transformation matrices $M^{1/2}$ and $M^{-1/2}$ as diagonal matrices. In implementation, operations can use the vectors of diagonal entries directly for efficiency.\n2.  Compute the transformed snapshot matrix $\\tilde{X} = M^{1/2} X$.\n3.  Compute the SVD of $\\tilde{X}$ to obtain $U$ and the singular values $\\sigma_j$.\n4.  Extract the first $r$ columns of $U$ to form $U_r$.\n5.  Compute the POD modes $\\Phi = M^{-1/2} U_r$.\n6.  For verification, compute the Frobenius norm of the error matrix $\\|\\Phi^\\top M \\Phi - I_r\\|_F$ and check if it is less than or equal to the tolerance $\\varepsilon=10^{-10}$.\n7.  Calculate the energy fraction $f$ using the formula above.\nThis procedure will be applied to each test case.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes M-orthonormal POD modes and captured energy for a series of test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"n\": 5, \"k\": 4, \"r\": 2,\n            \"M_diag\": np.array([1.0, 2.0, 3.0, 0.5, 4.0]),\n            \"X\": np.array([\n                [1, 0, 2, -1],\n                [0, 1, 1, 0],\n                [2, 1, 0, 1],\n                [-1, 0, 1, 2],\n                [0, 2, -1, 1]\n            ])\n        },\n        {\n            \"n\": 6, \"k\": 5, \"r\": 2,\n            \"M_diag\": np.array([1.0, 1.0, 1.0, 1.0, 1.0, 1.0]),\n            \"X\": np.array([\n                [1, 0, 1, 2, 1],\n                [0, 1, 1, 2, 0],\n                [0, 0, 0, 0, 0],\n                [1, 2, 3, 6, 1],\n                [2, 1, 3, 6, 2],\n                [3, 0, 3, 6, 3]\n            ])\n        },\n        {\n            \"n\": 7, \"k\": 3, \"r\": 2,\n            \"M_diag\": np.array([1e-8, 1e-4, 1e-2, 1, 1e2, 1e4, 1e8]),\n            \"X\": np.array([\n                [1, 2, 3],\n                [0, 1, 0],\n                [3, 0, 1],\n                [4, 1, -1],\n                [0, 1, 2],\n                [-2, 0, 0],\n                [1, -1, 1]\n            ])\n        },\n        {\n            \"n\": 4, \"k\": 2, \"r\": 2,\n            \"M_diag\": np.array([0.75, 1.25, 2.5, 0.5]),\n            \"X\": np.array([\n                [1, 0],\n                [0, 1],\n                [1, 1],\n                [-1, 2]\n            ])\n        }\n    ]\n\n    epsilon = 1e-10\n    results = []\n\n    for case in test_cases:\n        M_diag = case[\"M_diag\"]\n        X = case[\"X\"]\n        r = case[\"r\"]\n\n        # 1. Compute diagonal transformation operators from M.\n        M_sqrt_diag = np.sqrt(M_diag)\n        M_inv_sqrt_diag = 1.0 / M_sqrt_diag\n\n        # 2. Transform the snapshot matrix to the Euclidean setting.\n        # Broadcasting (n,1) * (n,k) is more efficient than full matrix multiplication.\n        X_tilde = M_sqrt_diag[:, np.newaxis] * X\n\n        # 3. Perform SVD on the transformed snapshot matrix.\n        # `full_matrices=False` is used for efficiency.\n        U, s, Vt = np.linalg.svd(X_tilde, full_matrices=False)\n\n        # 4. Extract the first r transformed modes (leading left singular vectors).\n        Ur = U[:, :r]\n\n        # 5. Transform the modes back to the original weighted inner product space.\n        Phi = M_inv_sqrt_diag[:, np.newaxis] * Ur\n\n        # 6. Verify the M-orthonormality of the computed modes.\n        M_mat = np.diag(M_diag)\n        identity_r = np.eye(r)\n        check_matrix = Phi.T @ M_mat @ Phi\n        ortho_error = np.linalg.norm(check_matrix - identity_r, 'fro')\n        is_orthonormal = ortho_error <= epsilon\n\n        # 7. Calculate the fraction of energy captured by the r modes.\n        s_squared = s**2\n        captured_energy = np.sum(s_squared[:r])\n        total_energy = np.sum(s_squared)\n        \n        if total_energy > 0:\n            energy_fraction = captured_energy / total_energy\n        else:\n            # This case happens if X is a zero matrix. Assume non-zero data.\n            # If captured and total are both 0, fraction is ill-defined.\n            # Depending on definition, can be 0 or 1. If r>0 and cap=0, tot=0, then 1.\n            # If r=0, cap=0, tot=0, then 0.\n            # For this problem's context with non-zero X, this branch is not taken.\n            energy_fraction = 1.0 if r > 0 and captured_energy == 0 else 0.0\n\n        results.append(str(is_orthonormal).lower())\n        results.append(f\"{energy_fraction:.8f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3356822"}, {"introduction": "Once a set of POD modes is computed, the next critical step in building a reduced-order model is deciding how many modes to retain. Truncating the basis at an optimal rank $r$ is essential for balancing model fidelity and computational cost. This decision becomes challenging when measurement noise corrupts the snapshot data, as the singular values associated with noise can obscure those of the underlying dynamics. This exercise [@problem_id:3356836] provides hands-on experience with a powerful, theoretically-grounded method for this task: applying the Gavish-Donoho optimal hard threshold, derived from random matrix theory, to systematically separate the signal from the noise and choose a robust truncation rank.", "problem": "A sequence of equal-time snapshots of a nondimensionalized planar velocity field from a computational fluid dynamics simulation of a vortex-dominated wake is collected into a snapshot matrix $\\mathbf{X} \\in \\mathbb{R}^{n \\times n}$ via standard stacking of state vectors at $n$ consecutive time instants. The measurements are contaminated by additive, zero-mean, independent and identically distributed Gaussian noise with standard deviation $\\sigma$. The singular value decomposition (SVD) of $\\mathbf{X}$ yields singular values $\\{\\sigma_{i}\\}_{i=1}^{n}$ in descending order.\n\nUse the following scientifically plausible data:\n- The matrix size is $n = 400$ (square aspect).\n- The estimated nondimensional noise level is $\\sigma = 0.05$.\n- The leading measured singular values are\n$$\n\\{\\sigma_{i}\\}_{i=1}^{20} = \\{9.0,\\; 7.5,\\; 6.0,\\; 5.0,\\; 3.0,\\; 2.5,\\; 2.0,\\; 1.7,\\; 1.4,\\; 1.2,\\; 1.0,\\; 0.8,\\; 0.6,\\; 0.5,\\; 0.4,\\; 0.35,\\; 0.3,\\; 0.25,\\; 0.2,\\; 0.15\\}.\n$$\n\nStarting from the fundamental definitions of the singular value decomposition and the standard noisy snapshot model, and invoking asymptotic random matrix behavior for noise-only matrices, derive the Gavish–Donoho universal hard threshold specialized to the square case and apply it to select the reduced rank $r$ for a proper orthogonal decomposition (POD) truncation. Then, explain qualitatively, from first principles of dynamic mode decomposition (DMD), how this rank selection influences the computed DMD spectra (eigenvalues and amplitudes), focusing on the suppression of spurious noise-dominated content.\n\nReport only the selected rank $r$ as your final answer. No rounding instruction is needed, and the answer is unitless.", "solution": "The problem is scientifically grounded, drawing upon established principles in computational fluid dynamics, reduced-order modeling (POD, DMD), and random matrix theory. The premise of using a hard threshold derived from random matrix theory to denoise a singular value spectrum is a standard and widely accepted technique in modern data analysis. The problem is well-posed, with all necessary data provided to calculate a unique rank $r$. The language is objective and the data is physically plausible for a nondimensionalized fluid dynamics problem. The problem is not trivial and requires a specific derivation and its application. The problem is valid.\n\nThe problem models the measured snapshot matrix $\\mathbf{X}$ as the sum of a true, low-rank signal matrix $\\mathbf{X}_{\\text{true}}$ and a noise matrix $\\mathbf{N}$:\n$$\n\\mathbf{X} = \\mathbf{X}_{\\text{true}} + \\mathbf{N}\n$$\nwhere $\\mathbf{X}, \\mathbf{X}_{\\text{true}}, \\mathbf{N} \\in \\mathbb{R}^{n \\times n}$. The entries of the noise matrix $\\mathbf{N}$ are independent and identically distributed (i.i.d.) random variables drawn from a Gaussian distribution with mean $0$ and variance $\\sigma^2$.\n\nThe core principle behind the hard threshold is to identify the range of singular values that would be produced by the noise matrix $\\mathbf{N}$ alone. Singular values of the measured matrix $\\mathbf{X}$ that are larger than the maximum singular value expected from pure noise are considered to belong to the signal $\\mathbf{X}_{\\text{true}}$.\n\nWe invoke a fundamental result from Random Matrix Theory (RMT), the Marchenko-Pastur law, which describes the distribution of eigenvalues of sample covariance matrices. Let us consider the matrix $\\mathbf{C} = \\frac{1}{n}\\mathbf{N}^T\\mathbf{N}$. In the limit of large matrix dimensions ($n \\to \\infty$), the distribution of the eigenvalues $\\lambda$ of $\\mathbf{C}$ converges to a specific probability density function. The support of this distribution (i.e., the interval containing the eigenvalues) is given by:\n$$\n\\lambda \\in [\\sigma^2(1 - \\sqrt{\\beta})^2, \\sigma^2(1 + \\sqrt{\\beta})^2]\n$$\nwhere $\\beta = m/n$ is the aspect ratio of the matrix. In this problem, the matrix is square, so $m=n=400$, and the aspect ratio is $\\beta = 1$.\n\nSubstituting $\\beta=1$ into the support interval, we get:\n$$\n[\\sigma^2(1 - \\sqrt{1})^2, \\sigma^2(1 + \\sqrt{1})^2] = [\\sigma^2(0)^2, \\sigma^2(2)^2] = [0, 4\\sigma^2]\n$$\nThis result implies that for a large square noise matrix, all eigenvalues of the normalized covariance matrix $\\frac{1}{n}\\mathbf{N}^T\\mathbf{N}$ are contained within the interval $[0, 4\\sigma^2]$. The largest eigenvalue asymptotically approaches the upper bound:\n$$\n\\lambda_{\\max}\\left(\\frac{1}{n}\\mathbf{N}^T\\mathbf{N}\\right) \\to 4\\sigma^2\n$$\nThe singular values $s_i$ of the noise matrix $\\mathbf{N}$ are related to the eigenvalues of $\\mathbf{N}^T\\mathbf{N}$ by $s_i^2 = \\lambda_i(\\mathbf{N}^T\\mathbf{N})$. We can rewrite this in terms of the eigenvalues of $\\mathbf{C}$:\n$$\ns_i^2 = \\lambda_i(n\\mathbf{C}) = n \\lambda_i(\\mathbf{C}) \\implies s_i = \\sqrt{n \\lambda_i(\\mathbf{C})}\n$$\nTherefore, the maximum singular value of the noise matrix $\\mathbf{N}$, $s_{\\max}$, asymptotically approaches:\n$$\ns_{\\max} \\to \\sqrt{n \\cdot \\lambda_{\\max}(\\mathbf{C})} = \\sqrt{n \\cdot 4\\sigma^2} = 2\\sigma\\sqrt{n}\n$$\nThis value serves as the universal hard threshold, often attributed to Gavish and Donoho for this square case. We denote this threshold by $\\tau$. Any singular value of the measured matrix $\\mathbf{X}$ above this threshold is deemed to be part of the signal, while any singular value at or below it is considered indistinguishable from noise.\n$$\n\\tau = 2\\sigma\\sqrt{n}\n$$\n\nWe are given the numerical values $n = 400$ and $\\sigma = 0.05$. We can now compute the threshold $\\tau$:\n$$\n\\tau = 2 \\times 0.05 \\times \\sqrt{400} = 0.1 \\times 20 = 2.0\n$$\nThe rank $r$ is selected by counting the number of singular values $\\{\\sigma_i\\}$ of the data matrix $\\mathbf{X}$ that are strictly greater than this threshold.\n$$\nr = \\text{count}\\{i \\mid \\sigma_i > \\tau\\}\n$$\nUsing the provided list of singular values and the calculated threshold $\\tau=2.0$:\n- $\\sigma_1 = 9.0 > 2.0$\n- $\\sigma_2 = 7.5 > 2.0$\n- $\\sigma_3 = 6.0 > 2.0$\n- $\\sigma_4 = 5.0 > 2.0$\n- $\\sigma_5 = 3.0 > 2.0$\n- $\\sigma_6 = 2.5 > 2.0$\n- $\\sigma_7 = 2.0 \\ngtr 2.0$\n- $\\sigma_8 = 1.7 < 2.0$\n... and all subsequent singular values are also less than $2.0$.\n\nThe first $6$ singular values are strictly greater than the threshold. Therefore, the optimal rank for the POD truncation is $r=6$.\n\nThe POD truncation at rank $r$ acts as a crucial denoising step prior to the application of DMD.\nDynamic Mode Decomposition (DMD) seeks to find a best-fit linear operator $\\mathbf{A}$ that advances the system state in time, $\\mathbf{x}_{k+1} \\approx \\mathbf{A}\\mathbf{x}_k$. The algorithm computes this operator and its spectral C (eigenvalues and eigenvectors) from the snapshot data. A common and robust variant of the DMD algorithm first projects the data onto a low-rank POD basis. The SVD of the snapshot matrix $\\mathbf{X}$ (or a sub-matrix thereof) yields the POD basis $\\mathbf{U}$. Truncating this basis to its first $r$ columns, $\\mathbf{U}_r$, projects the data onto a subspace that captures the most energetic structures of the flow.\n\nThe choice of rank $r$ is critical. The singular value spectrum ($\\{\\sigma_i\\}$) reveals the energy distribution among the POD modes. The first few modes with large singular values typically represent large-scale, coherent, and physically meaningful structures (e.g., vortices, shedding patterns). The trailing modes with small singular values are associated with fine-scale structures, and, crucially, are dominated by measurement noise. Noise is typically a high-dimensional, stochastic process that does not adhere to the low-dimensional linear dynamics that DMD is designed to identify.\n\nIf a rank $r$ much larger than the optimal one is chosen, noisy POD modes are included in the subspace onto which the dynamics are projected. When the DMD algorithm attempts to fit a linear model to this noise-contaminated data, it will generate a large number of spurious DMD modes and eigenvalues. The resulting DMD spectrum (the set of eigenvalues in the complex plane) becomes polluted with many eigenvalues that have no physical meaning, often scattered inside the unit circle. This obscures the true dynamics of the system and degrades the quality and interpretability of the DMD model.\n\nBy selecting the rank $r=6$ based on the Gavish-Donoho hard threshold, we are systematically filtering out the subspace dimensions that are dominated by noise. The DMD algorithm is then applied to a \"cleaned\" representation of the system, which contains only the coherent, high-energy dynamics. This leads to a much cleaner DMD spectrum, populated by a few dominant eigenvalues that accurately represent the frequencies and growth/decay rates of the underlying physical phenomena. The suppression of spurious, noise-driven content results in a more robust, accurate, and physically interpretable low-order model of the fluid flow.", "answer": "$$\n\\boxed{6}\n$$", "id": "3356836"}, {"introduction": "After selecting a low-dimensional basis, the governing equations are projected onto this subspace to create the reduced-order model (ROM). While the standard Galerkin projection uses the same basis for trial and test functions, more advanced Petrov-Galerkin methods use different bases, which can be advantageous for stability and accuracy. This practice [@problem_id:3356825] delves into a key consequence of such projections: the potential for non-normal transient energy growth, where a stable system can experience short-term amplification of disturbances. By constructing a ROM and systematically varying the obliqueness between trial and test spaces, you will investigate how this geometric property directly influences the non-normal behavior of the reduced dynamics, a crucial consideration for robust model design.", "problem": "Consider a linearized computational fluid dynamics model obtained by linearizing the Navier–Stokes equations about a steady base flow, which is modeled here abstractly as a finite-dimensional linear time-invariant system governed by the ordinary differential equation $ \\dot{x}(t) = A x(t) $ with $ x(t) \\in \\mathbb{R}^n $ and $ A \\in \\mathbb{R}^{n \\times n} $. For a reduced-order model based on Proper Orthogonal Decomposition (POD), one uses a trial subspace spanned by the columns of a matrix $ V \\in \\mathbb{R}^{n \\times r} $ and a test subspace spanned by the columns of a matrix $ W \\in \\mathbb{R}^{n \\times r} $, where both $ V $ and $ W $ have orthonormal columns, but need not coincide. The Petrov–Galerkin reduced operator is then defined by $ A_r = W^\\top A V \\in \\mathbb{R}^{r \\times r} $, and the reduced dynamics $ \\dot{y}(t) = A_r y(t) $ approximate the full dynamics in the subspace, with reconstructed state $ \\hat{x}(t) = V y(t) $. \n\nNon-normal transient growth refers to the phenomenon whereby the norm $ \\| e^{A t} \\|_2 $ or $ \\| e^{A_r t} \\|_2 $ can exhibit initial growth for some time $ t > 0 $ even when all eigenvalues of $ A $ (or $ A_r $) have strictly negative real parts. One source of such growth in reduced-order models is the obliqueness between the trial and test spaces. When $ V $ and $ W $ are different orthonormal bases, the matrix $ G = V^\\top W \\in \\mathbb{R}^{r \\times r} $ has singular values equal to the cosines of the principal angles between the subspaces $ \\text{span}(V) $ and $ \\text{span}(W) $. An obliqueness measure can thus be based on the condition number $ \\kappa_2(G) = \\sigma_{\\max}(G)/\\sigma_{\\min}(G) $, which equals $ 1 $ when $ V = W $ and grows as the spaces become more oblique.\n\nYour task is to construct a concrete linear operator $ A $ of advection–diffusion type, generate a POD trial basis $ V $ from snapshots of the full system, parametrize a family of test spaces $ W(\\varepsilon) $ that tilt away from $ V $ by a tunable amount $ \\varepsilon \\ge 0 $, and then quantify the relationship between the obliqueness measure $ \\kappa_2(V^\\top W) $ and the maximum short-time transient growth of the reduced evolution operator measured by $ g_{\\max} = \\max_{t \\in \\mathcal{T}} \\| e^{A_r t} \\|_2 $ on a prescribed time grid $ \\mathcal{T} $. Specifically:\n\n1. Construct a one-dimensional advection–diffusion semi-discrete operator with Dirichlet boundary conditions. Let $ n = 50 $, grid spacing $ h = 1/(n+1) $, advection speed $ c = 1.0 $, and kinematic viscosity $ \\nu = 0.02 $. Define the backward-difference first-derivative matrix $ D_1 \\in \\mathbb{R}^{n \\times n} $ by $ (D_1)_{ii} = 1/h $ for all $ i $, $ (D_1)_{i,i-1} = -1/h $ for $ i = 2, \\dots, n $, and zero otherwise, and the standard second-derivative matrix $ D_2 \\in \\mathbb{R}^{n \\times n} $ by $ (D_2)_{ii} = -2/h^2 $, $ (D_2)_{i,i-1} = (D_2)_{i,i+1} = 1/h^2 $ for interior indices in $ \\{2,\\dots,n-1\\} $ with Dirichlet boundary enforcement at the ends. Set $ A = -c D_1 + \\nu D_2 $. This $ A $ is stable for the chosen parameters and is generally non-normal because of the upwind-like advection term with boundaries.\n\n2. Generate a snapshot matrix $ X \\in \\mathbb{R}^{n \\times m} $ using two deterministic initial conditions $ x_0^{(1)} $ and $ x_0^{(2)} $ defined on the interior grid points $ x_j = j h $ for $ j = 1,\\dots,n $ as $ x_0^{(1)}[j] = \\sin(\\pi x_j) $ and $ x_0^{(2)}[j] = \\sin(2 \\pi x_j) $. Evolve each initial condition under the linear dynamics $ \\dot{x} = A x $ to times $ t_k $ uniformly spaced in $ [0,1] $ with $ N_t = 51 $ samples. Form $ X $ by concatenating the states for both initial conditions across all $ t_k $.\n\n3. Compute a reduced POD trial basis $ V \\in \\mathbb{R}^{n \\times r} $ with $ r = 3 $ as the leading left singular vectors of $ X $.\n\n4. Construct a deterministic auxiliary matrix $ U_{\\text{aux}} \\in \\mathbb{R}^{n \\times r} $ with columns $ u_1[j] = \\sin(2 \\pi x_j) $, $ u_2[j] = \\sin(4 \\pi x_j) $, and $ u_3[j] = \\cos(2 \\pi x_j) $ for the same grid. Project $ U_{\\text{aux}} $ onto the orthogonal complement of $ \\text{span}(V) $ to obtain $ N = (I - V V^\\top) U_{\\text{aux}} $. For each parameter $ \\varepsilon \\in \\{0.0, 0.2, 0.5, 0.99\\} $, define a test basis candidate $ \\widetilde{W}(\\varepsilon) = V + \\varepsilon N $ and then orthonormalize its columns to obtain an orthonormal test basis $ W(\\varepsilon) $ via a thin QR factorization. This ensures $ W(\\varepsilon) $ spans a subspace that is oblique to $ \\text{span}(V) $ by an amount controlled by $ \\varepsilon $.\n\n5. For each $ \\varepsilon $, form the reduced operator $ A_r(\\varepsilon) = W(\\varepsilon)^\\top A V $ and compute:\n   - The obliqueness measure $ \\kappa_2(V^\\top W(\\varepsilon)) $.\n   - The maximum short-time transient growth $ g_{\\max}(\\varepsilon) = \\max_{t \\in \\mathcal{T}} \\| e^{A_r(\\varepsilon) t} \\|_2 $ over the uniform grid $ \\mathcal{T} = \\{0, 0.01, 0.02, \\dots, 1.00\\} $.\n\n6. Your program must output a single line containing the eight floating-point numbers $ [\\kappa_2(V^\\top W(0.0)), g_{\\max}(0.0), \\kappa_2(V^\\top W(0.2)), g_{\\max}(0.2), \\kappa_2(V^\\top W(0.5)), g_{\\max}(0.5), \\kappa_2(V^\\top W(0.99)), g_{\\max}(0.99)] $ rounded to five decimal places, in that order, as a comma-separated list enclosed in square brackets.\n\nThe fundamental base you must use for your derivations and algorithmic design includes: the linear time-invariant system $ \\dot{x} = A x $ and its solution $ x(t) = e^{A t} x(0) $, the definition of a Proper Orthogonal Decomposition basis via the singular value decomposition of the snapshot matrix, the Petrov–Galerkin projection $ A_r = W^\\top A V $, and the definition of non-normal transient growth as the operator norm $ \\| e^{A_r t} \\|_2 $. The test suite is the set of $ \\varepsilon $ values specified above. The final answer must be a runnable program that produces exactly the single-line output in the specified format. No physical units or angles are used in this problem, and all numeric outputs are pure dimensionless real numbers reported as decimal floats rounded to five decimal places.", "solution": "The problem presented is a well-posed and scientifically sound exercise in the field of computational fluid dynamics, specifically concerning reduced-order modeling via projection methods. It investigates the impact of non-orthogonality between trial and test bases in a Petrov-Galerkin framework on the transient growth characteristics of the resulting reduced-order model. The problem is self-contained, with all necessary parameters and procedures clearly defined. We shall proceed with a step-by-step derivation and construction of the solution.\n\nThe foundation of the problem is a linear time-invariant (LTI) system describing the dynamics of a state vector $x(t) \\in \\mathbb{R}^n$:\n$$\n\\dot{x}(t) = A x(t)\n$$\nwhere $A \\in \\mathbb{R}^{n \\times n}$ is the system matrix.\n\nThe system matrix $A$ represents a semi-discretization of a one-dimensional advection-diffusion operator on a uniform grid with $n=50$ interior points. The domain is taken to be of unit length, so the grid spacing is $h = 1/(n+1) = 1/51$. The governing equation is\n$$\nA = -c D_1 + \\nu D_2\n$$\nwith advection speed $c=1.0$ and kinematic viscosity $\\nu=0.02$.\n\nThe first-derivative operator $D_1 \\in \\mathbb{R}^{n \\times n}$ is a backward-difference approximation. Its matrix elements are given by:\n$$\n(D_1)_{ij} = \n\\begin{cases} \n1/h & \\text{if } i=j \\\\\n-1/h & \\text{if } j=i-1 \\\\\n0 & \\text{otherwise}\n\\end{cases}\n$$\nThis corresponds to the approximation $(\\partial u/\\partial x)_i \\approx (u_i - u_{i-1})/h$. With the implicit Dirichlet boundary condition $u_0=0$, the first row correctly represents the boundary interaction.\n\nThe second-derivative operator $D_2 \\in \\mathbb{R}^{n \\times n}$ is the standard second-order central difference approximation for the Laplacian with homogeneous Dirichlet boundary conditions ($u_0=0$, $u_{n+1}=0$):\n$$\n(D_2)_{ij} = \n\\begin{cases} \n-2/h^2 & \\text{if } i=j \\\\\n1/h^2 & \\text{if } |i-j|=1 \\\\\n0 & \\text{otherwise}\n\\end{cases}\n$$\n\nSnapshots of the system's evolution are collected to form a data matrix $X$, from which the dominant dynamical modes are extracted. We use two initial conditions, defined on the grid points $x_j = j h$ for $j=1, \\dots, n$:\n$$\nx_0^{(1)}[j] = \\sin(\\pi x_j)\n$$\n$$\nx_0^{(2)}[j] = \\sin(2 \\pi x_j)\n$$\nFor each initial condition $x_0$, the state at time $t$ is given by $x(t) = e^{A t} x_0$. We sample the evolution at $N_t = 51$ uniformly spaced time instances $t_k$ in the interval $[0, 1]$. The snapshot matrix $X \\in \\mathbb{R}^{n \\times m}$ is formed by concatenating the resulting state vectors:\n$$\nX = [x^{(1)}(t_1), \\dots, x^{(1)}(t_{N_t}), x^{(2)}(t_1), \\dots, x^{(2)}(t_{N_t})]\n$$\nThis results in a snapshot matrix with $m = 2 \\times N_t = 102$ columns.\n\nThe Proper Orthogonal Decomposition (POD) provides an optimal low-dimensional basis for the snapshot data in the least-squares sense. The POD basis vectors are the left singular vectors of the snapshot matrix $X$. We perform a singular value decomposition (SVD) of $X$:\n$$\nX = U \\Sigma \\mathcal{V}^\\top\n$$\nwhere $U \\in \\mathbb{R}^{n \\times n}$ and $\\mathcal{V} \\in \\mathbb{R}^{m \\times m}$ have orthonormal columns, and $\\Sigma \\in \\mathbb{R}^{n \\times m}$ is a diagonal matrix of singular values. The trial basis $V \\in \\mathbb{R}^{n \\times r}$ for a reduced-order model of dimension $r=3$ is constructed by taking the first $r$ columns of $U$, which correspond to the $r$ largest singular values.\n$$\nV = [u_1, u_2, u_3]\n$$\n\nTo study the effect of obliqueness, we construct a family of test bases $W(\\varepsilon)$ that deviate from the trial basis $V$ by an amount controlled by a parameter $\\varepsilon$. The construction proceeds as follows:\nAn auxiliary matrix $U_{\\text{aux}} \\in \\mathbb{R}^{n \\times r}$ is defined with columns given by sinusoidal functions on the grid $x_j$:\n$$\nu_{\\text{aux},1}[j] = \\sin(2 \\pi x_j), \\quad u_{\\text{aux},2}[j] = \\sin(4 \\pi x_j), \\quad u_{\\text{aux},3}[j] = \\cos(2 \\pi x_j)\n$$\nTo ensure the perturbation of the test basis is in a direction not already contained in the trial subspace $\\text{span}(V)$, we project $U_{\\text{aux}}$ onto the orthogonal complement of $\\text{span}(V)$. The projection operator onto $\\text{span}(V)$ is $P_V = V V^\\top$, and onto its complement is $P_{V^\\perp} = I - V V^\\top$. The non-parallel component is:\n$$\nN = (I - V V^\\top) U_{\\text{aux}}\n$$\nA candidate test basis, $\\widetilde{W}(\\varepsilon)$, is formed by linearly combining $V$ and $N$:\n$$\n\\widetilde{W}(\\varepsilon) = V + \\varepsilon N\n$$\nFor this to serve as a basis, its columns must be orthonormal. We achieve this through a thin QR factorization of $\\widetilde{W}(\\varepsilon)$:\n$$\n\\widetilde{W}(\\varepsilon) = W(\\varepsilon) R\n$$\nwhere $W(\\varepsilon) \\in \\mathbb{R}^{n \\times r}$ has orthonormal columns and $R \\in \\mathbb{R}^{r \\times r}$ is upper triangular. The matrix $W(\\varepsilon)$ is the desired orthonormal test basis. For $\\varepsilon=0$, $\\widetilde{W}(0) = V$, and since $V$ is already orthonormal, the QR factorization yields $W(0)=V$ (up to column signs, which we can ignore). As $\\varepsilon$ increases, the subspace $\\text{span}(W(\\varepsilon))$ tilts away from $\\text{span}(V)$.\n\nFor each specified value of $\\varepsilon \\in \\{0.0, 0.2, 0.5, 0.99\\}$, we perform the following analysis:\n\nFirst, we form the Petrov-Galerkin reduced-order operator $A_r(\\varepsilon) \\in \\mathbb{R}^{r \\times r}$:\n$$\nA_r(\\varepsilon) = W(\\varepsilon)^\\top A V\n$$\nNote that for $\\varepsilon=0$, we have $W(0)=V$, and $A_r(0) = V^\\top A V$ is a standard Galerkin projection.\n\nSecond, we quantify the obliqueness between the trial and test subspaces using the condition number of their inner product matrix $G(\\varepsilon) = V^\\top W(\\varepsilon)$. The singular values of $G(\\varepsilon)$ are the cosines of the principal angles between the two subspaces. The obliqueness measure is:\n$$\n\\kappa_2(\\varepsilon) = \\kappa_2(G(\\varepsilon)) = \\frac{\\sigma_{\\max}(G(\\varepsilon))}{\\sigma_{\\min}(G(\\varepsilon))}\n$$\nA value of $\\kappa_2 = 1$ indicates the subspaces are identical, while larger values imply greater obliqueness.\n\nThird, we quantify the non-normal transient growth of the reduced-order system $\\dot{y}(t) = A_r(\\varepsilon) y(t)$. The growth is measured by the maximum value of the matrix norm of the propagator, $\\|e^{A_r(\\varepsilon)t}\\|_2$, over a specified time grid $\\mathcal{T} = \\{0, 0.01, \\dots, 1.00\\}$.\n$$\ng_{\\max}(\\varepsilon) = \\max_{t \\in \\mathcal{T}} \\|e^{A_r(\\varepsilon)t}\\|_2\n$$\nThe 2-norm of the matrix exponential, $\\|e^{A_r(\\varepsilon)t}\\|_2$, is its largest singular value. Its value can exceed $1$ for $t>0$ even if all eigenvalues of $A_r(\\varepsilon)$ have negative real parts, a hallmark of non-normal systems. This calculation is performed for each $\\varepsilon$ to establish the relationship between basis obliqueness and transient energy growth. The final results are then collected and formatted as requested.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import expm\n\ndef solve():\n    \"\"\"\n    Solves the reduced-order modeling problem as specified.\n    \"\"\"\n    # 1. Construct the full-order operator A\n    n = 50\n    h = 1.0 / (n + 1)\n    c = 1.0\n    nu = 0.02\n    r = 3\n\n    # Construct D1 (backward difference)\n    diag_main = np.ones(n) / h\n    diag_sub = -np.ones(n - 1) / h\n    D1 = np.diag(diag_main) + np.diag(diag_sub, k=-1)\n\n    # Construct D2 (central difference)\n    diag_main = -2.0 * np.ones(n) / (h**2)\n    diag_off = np.ones(n - 1) / (h**2)\n    D2 = np.diag(diag_main) + np.diag(diag_off, k=1) + np.diag(diag_off, k=-1)\n\n    A = -c * D1 + nu * D2\n\n    # 2. Generate the snapshot matrix X\n    grid_x = np.arange(1, n + 1) * h\n    \n    # Initial conditions\n    x0_1 = np.sin(np.pi * grid_x)\n    x0_2 = np.sin(2 * np.pi * grid_x)\n    \n    # Time points for snapshots\n    Nt = 51\n    t_snapshots = np.linspace(0, 1, Nt)\n    \n    snapshots = []\n    # Evolve first initial condition\n    for t in t_snapshots:\n        x_t = expm(A * t) @ x0_1\n        snapshots.append(x_t)\n        \n    # Evolve second initial condition\n    for t in t_snapshots:\n        x_t = expm(A * t) @ x0_2\n        snapshots.append(x_t)\n        \n    X = np.array(snapshots).T # each column is a snapshot\n\n    # 3. Compute the POD trial basis V\n    U, _, _ = np.linalg.svd(X, full_matrices=False)\n    V = U[:, :r]\n\n    # 4. Construct the auxiliary matrix and prepare for the loop\n    u_aux_1 = np.sin(2 * np.pi * grid_x)\n    u_aux_2 = np.sin(4 * np.pi * grid_x)\n    u_aux_3 = np.cos(2 * np.pi * grid_x)\n    U_aux = np.vstack([u_aux_1, u_aux_2, u_aux_3]).T\n\n    # Project U_aux onto the orthogonal complement of span(V)\n    N = U_aux - V @ (V.T @ U_aux)\n    \n    eps_values = [0.0, 0.2, 0.5, 0.99]\n    time_grid_growth = np.arange(0, 1.001, 0.01) # 0, 0.01, ..., 1.00\n    \n    final_results = []\n\n    # 5. Loop over epsilon values, compute metrics\n    for eps in eps_values:\n        # Construct the test basis W(epsilon)\n        W_tilde = V + eps * N\n        W, _ = np.linalg.qr(W_tilde)\n        \n        # Form the reduced operator A_r\n        Ar = W.T @ A @ V\n        \n        # Compute obliqueness measure kappa\n        G = V.T @ W\n        kappa = np.linalg.cond(G)\n        \n        # Compute maximum transient growth g_max\n        g_max = 0.0\n        for t in time_grid_growth:\n            if t == 0:\n                # Norm is 1 at t=0\n                norm_val = 1.0\n            else:\n                exp_Art = expm(Ar * t)\n                norm_val = np.linalg.norm(exp_Art, 2)\n            \n            if norm_val > g_max:\n                g_max = norm_val\n        \n        final_results.extend([kappa, g_max])\n\n    # 6. Format and print the final output\n    formatted_results = [f\"{val:.5f}\" for val in final_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3356825"}]}