{"hands_on_practices": [{"introduction": "Extracting resonance parameters from experimental data is a foundational task in nuclear physics that goes beyond simple curve-fitting. This first exercise grounds the fitting process in rigorous statistical theory, starting from the Poisson nature of particle counting experiments. By deriving the maximum-likelihood equations and the Fisher information matrix for the Breit-Wigner parameters $(E_R, \\Gamma)$, you will build a direct connection between the physical model and the principles of optimal parameter estimation. [@problem_id:3596467]", "problem": "A fixed-target experiment measures the reaction yield for a single, isolated nuclear resonance by scanning the incident beam energy across the resonance. At each scan point, indexed by $i=1,\\dots,N$, the beam energy is centered at $E_i$ with a bin half-width $\\Delta E_i/2$. The integrated luminosity is $\\mathcal{L}_i$ and the detection efficiency is $\\varepsilon_i$, both known. Let $k_i$ be the observed number of detected events in bin $i$, which are independent counts.\n\nAssume the mean number of detected events in bin $i$ is given by a Poisson expectation\n$$\n\\mu_i(E_R,\\Gamma) \\equiv \\mathcal{L}_i\\,\\varepsilon_i \\int_{E_i-\\Delta E_i/2}^{E_i+\\Delta E_i/2} \\sigma(E;E_R,\\Gamma)\\,dE,\n$$\nwhere the total cross section near the resonance is modeled by a known flat background plus a single-level Breitâ€“Wigner resonance,\n$$\n\\sigma(E;E_R,\\Gamma) = \\sigma_{\\mathrm{b}} + K \\frac{\\Gamma^2/4}{\\left(E-E_R\\right)^2 + \\Gamma^2/4},\n$$\nwith $\\sigma_{\\mathrm{b}}>0$ a known background cross section and $K>0$ a known scale factor that collects kinematic and channel factors. The unknown parameters to be estimated are the resonance energy $E_R$ and the total width $\\Gamma$.\n\nWork in the narrow-bin approximation $\\Delta E_i \\ll \\Gamma$, under which the expected counts simplify to\n$$\n\\mu_i(E_R,\\Gamma) \\approx C_i\\,\\sigma(E_i;E_R,\\Gamma), \\quad C_i \\equiv \\mathcal{L}_i \\varepsilon_i \\Delta E_i.\n$$\n\nTasks:\n- Starting from the independent Poisson model for $\\{k_i\\}_{i=1}^N$ and the definition of the likelihood, write the log-likelihood for $(E_R,\\Gamma)$ and derive the maximum-likelihood stationarity equations that implicitly define the estimators $\\widehat{E}_R$ and $\\widehat{\\Gamma}$ in terms of the data $\\{k_i\\}$ and the model $\\{\\mu_i\\}$.\n- Using the definition of the Fisher information for independent Poisson observations, derive the $2\\times 2$ Fisher information matrix for $(E_R,\\Gamma)$ in the narrow-bin approximation, expressed explicitly in terms of $\\{E_i,\\Delta E_i,\\mathcal{L}_i,\\varepsilon_i\\}$, the known constants $\\sigma_{\\mathrm{b}}$ and $K$, and the parameters $(E_R,\\Gamma)$. Define any intermediate symbols that you introduce.\n\nAnswer specification:\n- Provide the final answer as a single analytical expression for the $2\\times 2$ Fisher information matrix $I(E_R,\\Gamma)$ with entries written as explicit sums over bins.\n- No numerical evaluation or rounding is required.\n- Do not include units in the final answer.", "solution": "The problem requires the derivation of the maximum-likelihood stationarity equations and the Fisher information matrix for the parameters of a single-level Breit-Wigner resonance, $(E_R, \\Gamma)$, based on a set of independent Poisson-distributed event counts $\\{k_i\\}_{i=1}^N$.\n\nFirst, let us establish the model. The number of events $k_i$ in bin $i$ follows a Poisson distribution with mean $\\mu_i$, denoted $k_i \\sim \\text{Pois}(\\mu_i)$. The observations are independent. The total likelihood function for the parameter set $\\theta = (E_R, \\Gamma)$ given the data $\\{k_i\\}$ is the product of the individual Poisson probabilities:\n$$\nL(\\theta; \\{k_i\\}) = \\prod_{i=1}^{N} \\frac{\\mu_i(\\theta)^{k_i} \\exp(-\\mu_i(\\theta))}{k_i!}\n$$\nThe log-likelihood function, $\\ell(\\theta) = \\ln L(\\theta)$, is therefore:\n$$\n\\ell(\\theta) = \\sum_{i=1}^{N} \\left( k_i \\ln \\mu_i(\\theta) - \\mu_i(\\theta) - \\ln(k_i!) \\right)\n$$\nThe problem specifies the use of the narrow-bin approximation, $\\Delta E_i \\ll \\Gamma$, where the mean number of counts is\n$$\n\\mu_i(E_R, \\Gamma) \\approx C_i \\sigma(E_i; E_R, \\Gamma)\n$$\nwith $C_i \\equiv \\mathcal{L}_i \\varepsilon_i \\Delta E_i$. The cross section $\\sigma(E_i; E_R, \\Gamma)$ is given by\n$$\n\\sigma(E_i; E_R, \\Gamma) = \\sigma_{\\mathrm{b}} + K \\frac{\\Gamma^2/4}{(E_i - E_R)^2 + \\Gamma^2/4}\n$$\nwhere $\\sigma_{\\mathrm{b}}$ and $K$ are known positive constants.\n\n**Part 1: Maximum Likelihood Stationarity Equations**\n\nThe maximum likelihood estimators $(\\widehat{E}_R, \\widehat{\\Gamma})$ are the values of $(E_R, \\Gamma)$ that maximize $\\ell(E_R, \\Gamma)$. They are found by solving the stationarity equations, which set the partial derivatives of the log-likelihood with respect to the parameters to zero:\n$$\n\\frac{\\partial \\ell}{\\partial E_R} = 0 \\quad \\text{and} \\quad \\frac{\\partial \\ell}{\\partial \\Gamma} = 0\n$$\nLet's compute these derivatives. For a generic parameter $\\theta_j \\in \\{E_R, \\Gamma\\}$:\n$$\n\\frac{\\partial \\ell}{\\partial \\theta_j} = \\sum_{i=1}^{N} \\left( \\frac{k_i}{\\mu_i} \\frac{\\partial \\mu_i}{\\partial \\theta_j} - \\frac{\\partial \\mu_i}{\\partial \\theta_j} \\right) = \\sum_{i=1}^{N} \\left( \\frac{k_i}{\\mu_i} - 1 \\right) \\frac{\\partial \\mu_i}{\\partial \\theta_j}\n$$\nTo proceed, we need the partial derivatives of $\\mu_i$ with respect to $E_R$ and $\\Gamma$. Let's define the denominator of the Breit-Wigner term as $D_i(E_R, \\Gamma) = (E_i - E_R)^2 + \\Gamma^2/4$. Then,\n$$\n\\mu_i(E_R, \\Gamma) = C_i \\left( \\sigma_{\\mathrm{b}} + K \\frac{\\Gamma^2/4}{D_i} \\right)\n$$\nThe derivatives are computed using the chain rule:\n$$\n\\frac{\\partial \\mu_i}{\\partial E_R} = C_i \\frac{\\partial}{\\partial E_R} \\left( K \\frac{\\Gamma^2/4}{D_i} \\right) = C_i K \\frac{\\Gamma^2}{4} \\left( - \\frac{1}{D_i^2} \\right) \\frac{\\partial D_i}{\\partial E_R} = -C_i K \\frac{\\Gamma^2}{4D_i^2} \\left[ 2(E_i-E_R)(-1) \\right] = C_i K \\frac{\\Gamma^2 (E_i - E_R)}{2 D_i^2}\n$$\n$$\n\\frac{\\partial \\mu_i}{\\partial \\Gamma} = C_i \\frac{\\partial}{\\partial \\Gamma} \\left( K \\frac{\\Gamma^2/4}{D_i} \\right) = C_i K \\frac{(\\Gamma/2) D_i - (\\Gamma^2/4)(\\Gamma/2)}{D_i^2} = C_i K \\frac{\\Gamma}{2D_i^2} \\left( D_i - \\frac{\\Gamma^2}{4} \\right) = C_i K \\frac{\\Gamma (E_i - E_R)^2}{2 D_i^2}\n$$\nSubstituting these derivatives into the general form of the stationarity equations gives the expressions that implicitly define the estimators $\\widehat{E}_R$ and $\\widehat{\\Gamma}$:\n$$\n\\sum_{i=1}^{N} \\left( \\frac{k_i}{\\mu_i(\\widehat{E}_R, \\widehat{\\Gamma})} - 1 \\right) C_i K \\frac{\\widehat{\\Gamma}^2 (E_i - \\widehat{E}_R)}{2 [(E_i - \\widehat{E}_R)^2 + \\widehat{\\Gamma}^2/4]^2} = 0\n$$\n$$\n\\sum_{i=1}^{N} \\left( \\frac{k_i}{\\mu_i(\\widehat{E}_R, \\widehat{\\Gamma})} - 1 \\right) C_i K \\frac{\\widehat{\\Gamma} (E_i - \\widehat{E}_R)^2}{2 [(E_i - \\widehat{E}_R)^2 + \\widehat{\\Gamma}^2/4]^2} = 0\n$$\nThese are the required stationarity equations.\n\n**Part 2: Fisher Information Matrix**\n\nThe Fisher information matrix $I(\\theta)$ for a set of parameters $\\theta = (\\theta_1, \\dots, \\theta_m)$ is defined by its elements $I_{jk} = -E\\left[\\frac{\\partial^2 \\ell}{\\partial \\theta_j \\partial \\theta_k}\\right]$. For independent Poisson observations $k_i \\sim \\text{Pois}(\\mu_i(\\theta))$, this simplifies to:\n$$\nI_{jk}(\\theta) = \\sum_{i=1}^{N} \\frac{1}{\\mu_i(\\theta)} \\frac{\\partial \\mu_i(\\theta)}{\\partial \\theta_j} \\frac{\\partial \\mu_i(\\theta)}{\\partial \\theta_k}\n$$\nOur parameters are $\\theta_1 = E_R$ and $\\theta_2 = \\Gamma$. We need to compute the $2 \\times 2$ matrix $I(E_R, \\Gamma)$ with elements $I_{E_R E_R}$, $I_{\\Gamma \\Gamma}$, and $I_{E_R \\Gamma} = I_{\\Gamma E_R}$. We use the derivatives of $\\mu_i$ derived previously.\n\nThe elements are:\n\n$I_{E_R E_R} = \\sum_{i=1}^{N} \\frac{1}{\\mu_i} \\left( \\frac{\\partial \\mu_i}{\\partial E_R} \\right)^2$\n\n$I_{\\Gamma \\Gamma} = \\sum_{i=1}^{N} \\frac{1}{\\mu_i} \\left( \\frac{\\partial \\mu_i}{\\partial \\Gamma} \\right)^2$\n\n$I_{E_R \\Gamma} = \\sum_{i=1}^{N} \\frac{1}{\\mu_i} \\frac{\\partial \\mu_i}{\\partial E_R} \\frac{\\partial \\mu_i}{\\partial \\Gamma}$\n\nLet's substitute the expressions for $\\mu_i$ and its derivatives. Recall $C_i = \\mathcal{L}_i \\varepsilon_i \\Delta E_i$ and $D_i = (E_i - E_R)^2 + \\Gamma^2/4$.\n$$\n\\mu_i = C_i \\left( \\sigma_{\\mathrm{b}} + K \\frac{\\Gamma^2/4}{D_i} \\right) = \\frac{C_i}{D_i} \\left( \\sigma_{\\mathrm{b}} D_i + K \\frac{\\Gamma^2}{4} \\right)\n$$\nSubstituting the derivative for $E_R$:\n$$\nI_{E_R E_R} = \\sum_{i=1}^N \\frac{1}{\\mu_i} \\left( C_i K \\frac{\\Gamma^2 (E_i - E_R)}{2 D_i^2} \\right)^2 = \\sum_{i=1}^N \\frac{C_i^2 K^2 \\Gamma^4 (E_i-E_R)^2}{4 \\mu_i D_i^4}\n$$\nSubstituting $\\mu_i$:\n$$\nI_{E_R E_R} = \\sum_{i=1}^N \\frac{C_i^2 K^2 \\Gamma^4 (E_i-E_R)^2}{4 D_i^4 \\frac{C_i}{D_i} \\left( \\sigma_{\\mathrm{b}} D_i + K \\frac{\\Gamma^2}{4} \\right)} = \\sum_{i=1}^N \\frac{C_i K^2 \\Gamma^4 (E_i-E_R)^2}{4 D_i^3 \\left( \\sigma_{\\mathrm{b}} D_i + K \\frac{\\Gamma^2}{4} \\right)}\n$$\nSubstituting the derivative for $\\Gamma$:\n$$\nI_{\\Gamma \\Gamma} = \\sum_{i=1}^N \\frac{1}{\\mu_i} \\left( C_i K \\frac{\\Gamma (E_i - E_R)^2}{2 D_i^2} \\right)^2 = \\sum_{i=1}^N \\frac{C_i^2 K^2 \\Gamma^2 (E_i-E_R)^4}{4 \\mu_i D_i^4} = \\sum_{i=1}^N \\frac{C_i K^2 \\Gamma^2 (E_i-E_R)^4}{4 D_i^3 \\left( \\sigma_{\\mathrm{b}} D_i + K \\frac{\\Gamma^2}{4} \\right)}\n$$\nFor the off-diagonal element:\n$$\nI_{E_R \\Gamma} = \\sum_{i=1}^N \\frac{1}{\\mu_i} \\left( C_i K \\frac{\\Gamma^2 (E_i - E_R)}{2 D_i^2} \\right) \\left( C_i K \\frac{\\Gamma (E_i - E_R)^2}{2 D_i^2} \\right) = \\sum_{i=1}^N \\frac{C_i K^2 \\Gamma^3 (E_i-E_R)^3}{4 D_i^3 \\left( \\sigma_{\\mathrm{b}} D_i + K \\frac{\\Gamma^2}{4} \\right)}\n$$\nTo write the final expressions explicitly, we substitute $C_i = \\mathcal{L}_i \\varepsilon_i \\Delta E_i$ and $D_i = (E_i - E_R)^2 + \\Gamma^2/4$.\n\nThe elements of the Fisher information matrix $I(E_R, \\Gamma)$ are:\n$$\nI_{E_R E_R} = \\sum_{i=1}^N \\frac{(\\mathcal{L}_i \\varepsilon_i \\Delta E_i) K^2 \\Gamma^4 (E_i-E_R)^2}{4 \\left(\\sigma_{\\mathrm{b}}\\left((E_i-E_R)^2 + \\frac{\\Gamma^2}{4}\\right) + K \\frac{\\Gamma^2}{4}\\right)\\left((E_i-E_R)^2 + \\frac{\\Gamma^2}{4}\\right)^3}\n$$\n$$\nI_{\\Gamma \\Gamma} = \\sum_{i=1}^N \\frac{(\\mathcal{L}_i \\varepsilon_i \\Delta E_i) K^2 \\Gamma^2 (E_i-E_R)^4}{4 \\left(\\sigma_{\\mathrm{b}}\\left((E_i-E_R)^2 + \\frac{\\Gamma^2}{4}\\right) + K \\frac{\\Gamma^2}{4}\\right)\\left((E_i-E_R)^2 + \\frac{\\Gamma^2}{4}\\right)^3}\n$$\n$$\nI_{E_R \\Gamma} = \\sum_{i=1}^N \\frac{(\\mathcal{L}_i \\varepsilon_i \\Delta E_i) K^2 \\Gamma^3 (E_i-E_R)^3}{4 \\left(\\sigma_{\\mathrm{b}}\\left((E_i-E_R)^2 + \\frac{\\Gamma^2}{4}\\right) + K \\frac{\\Gamma^2}{4}\\right)\\left((E_i-E_R)^2 + \\frac{\\Gamma^2}{4}\\right)^3}\n$$\nThese expressions constitute the entries of the $2 \\times 2$ Fisher information matrix.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\sum_{i=1}^{N} \\frac{(\\mathcal{L}_i \\varepsilon_i \\Delta E_i) K^2 \\Gamma^4 (E_i-E_R)^2}{4 \\left(\\sigma_{\\mathrm{b}}\\left((E_i-E_R)^2 + \\frac{\\Gamma^2}{4}\\right) + K \\frac{\\Gamma^2}{4}\\right)\\left((E_i-E_R)^2 + \\frac{\\Gamma^2}{4}\\right)^3} & \\sum_{i=1}^{N} \\frac{(\\mathcal{L}_i \\varepsilon_i \\Delta E_i) K^2 \\Gamma^3 (E_i-E_R)^3}{4 \\left(\\sigma_{\\mathrm{b}}\\left((E_i-E_R)^2 + \\frac{\\Gamma^2}{4}\\right) + K \\frac{\\Gamma^2}{4}\\right)\\left((E_i-E_R)^2 + \\frac{\\Gamma^2}{4}\\right)^3} \\\\\n\\sum_{i=1}^{N} \\frac{(\\mathcal{L}_i \\varepsilon_i \\Delta E_i) K^2 \\Gamma^3 (E_i-E_R)^3}{4 \\left(\\sigma_{\\mathrm{b}}\\left((E_i-E_R)^2 + \\frac{\\Gamma^2}{4}\\right) + K \\frac{\\Gamma^2}{4}\\right)\\left((E_i-E_R)^2 + \\frac{\\Gamma^2}{4}\\right)^3} & \\sum_{i=1}^{N} \\frac{(\\mathcal{L}_i \\varepsilon_i \\Delta E_i) K^2 \\Gamma^2 (E_i-E_R)^4}{4 \\left(\\sigma_{\\mathrm{b}}\\left((E_i-E_R)^2 + \\frac{\\Gamma^2}{4}\\right) + K \\frac{\\Gamma^2}{4}\\right)\\left((E_i-E_R)^2 + \\frac{\\Gamma^2}{4}\\right)^3}\n\\end{pmatrix}\n}\n$$", "id": "3596467"}, {"introduction": "While theoretical frameworks like maximum likelihood provide a target, the practical task of finding the optimal parameters relies on numerical optimization algorithms. These algorithms can be surprisingly fragile, and their success often hinges on the numerical properties of the problem. This practice dives into the mechanics of nonlinear least-squares fitting, asking you to compute the model's Jacobian matrix and analyze its conditioning, which is a key indicator of numerical stability. [@problem_id:3596455] Through this exercise, you will gain hands-on experience in diagnosing ill-conditioned problems and learn how standard techniques like parameter scaling and regularization are essential for achieving stable fits.", "problem": "You are given a nonlinear least squares model for a single nuclear resonance with a polynomial background, where the dependent variable is a cross section as a function of energy. The model assumes the observed data points $\\{(E_i, y_i, \\sigma_i)\\}_{i=1}^N$ with energy $E_i$ in $\\mathrm{MeV}$, cross section $y_i$ in $\\mathrm{barn}$, and measurement uncertainty $\\sigma_i$ in $\\mathrm{barn}$. The parametric model is a sum of a Breit-Wigner resonance and a polynomial background,\n$$\n\\hat{y}(E;\\boldsymbol{\\theta}) = A\\,S(E;E_0,\\Gamma) + \\sum_{k=0}^{m} c_k\\,x(E)^k,\n$$\nwhere $A$ is the resonance amplitude in $\\mathrm{barn}$, $E_0$ is the resonance centroid in $\\mathrm{MeV}$, $\\Gamma$ is the total width in $\\mathrm{MeV}$, and the background coefficients $c_k$ are in $\\mathrm{barn}$. The normalized background basis uses\n$$\nx(E) = \\frac{E - E_c}{E_s},\n$$\nwith $E_c$ the center of the energy window (in $\\mathrm{MeV}$) and $E_s$ its half-width (in $\\mathrm{MeV}$) so that $x(E) \\in [-1,1]$. The nonrelativistic Breit-Wigner shape is\n$$\nS(E;E_0,\\Gamma) = \\frac{(\\Gamma/2)^2}{(E - E_0)^2 + (\\Gamma/2)^2}.\n$$\n\nConsider the residual vector of the unweighted least squares,\n$$\n\\mathbf{r}(\\boldsymbol{\\theta}) = \\mathbf{y} - \\hat{\\mathbf{y}}(\\boldsymbol{\\theta}),\n$$\nand its Jacobian matrix $J(\\boldsymbol{\\theta}) = \\partial \\mathbf{r}/\\partial \\boldsymbol{\\theta} \\in \\mathbb{R}^{N \\times P}$ where $P = m+4$ is the number of parameters. The conditioning of $J$ directly affects Gauss-Newton convergence. Your tasks are to compute the Jacobian, evaluate its conditioning, and investigate how scaling and simple Gaussian priors improve the conditioning and the positive definiteness of the approximate Hessian. Use the following fundamental bases:\n\n- The least squares residual definition $\\mathbf{r}(\\boldsymbol{\\theta}) = \\mathbf{y} - \\hat{\\mathbf{y}}(\\boldsymbol{\\theta})$.\n- The Jacobian definition $J(\\boldsymbol{\\theta}) = \\partial \\mathbf{r}/\\partial \\boldsymbol{\\theta}$.\n- The singular value decomposition condition number $\\kappa(J) = \\sigma_{\\max}(J)/\\sigma_{\\min}(J)$ with $\\sigma_{\\max}$, $\\sigma_{\\min}$ the largest and smallest singular values.\n- The Gauss-Newton approximate Hessian $H \\approx J^\\top J$ for unweighted least squares, and $H \\approx J_w^\\top J_w$ for weighted least squares with $J_w = WJ$ and $W = \\mathrm{diag}(1/\\sigma_i)$.\n\nProceed from first principles and compute the Jacobian entries by direct differentiation of $\\hat{y}(E;\\boldsymbol{\\theta})$. Do not assume any provided shortcuts. Then:\n\n1. Compute the unweighted Jacobian $J(\\boldsymbol{\\theta})$ at a specified initial guess $\\boldsymbol{\\theta}_0$ and the corresponding condition number $\\kappa_{\\mathrm{raw}}$.\n2. Define a scaled Jacobian $J_{\\mathrm{scaled}} = W J C^{-1}$ where $W = \\mathrm{diag}(1/\\sigma_i)$ whitens the residuals, and $C$ is a diagonal matrix of column norms of $WJ$ such that each column of $J_{\\mathrm{scaled}}$ has unit Euclidean norm. Compute $\\kappa_{\\mathrm{scaled}}$.\n3. Construct a Gaussian prior precision matrix $\\Lambda = \\mathrm{diag}(\\lambda_j)$ for the parameters with independent priors:\n   - $A \\sim \\mathcal{N}(0, \\sigma_A^2)$ with $\\sigma_A = 10\\,\\mathrm{barn}$, so $\\lambda_A = 1/\\sigma_A^2$.\n   - $E_0 \\sim \\mathcal{N}(0, \\sigma_{E_0}^2)$ with $\\sigma_{E_0} = 0.5\\,\\mathrm{MeV}$, so $\\lambda_{E_0} = 1/\\sigma_{E_0}^2$.\n   - $\\Gamma \\sim \\mathcal{N}(0, \\sigma_{\\Gamma}^2)$ with $\\sigma_{\\Gamma} = 0.2\\,\\mathrm{MeV}$, so $\\lambda_{\\Gamma} = 1/\\sigma_{\\Gamma}^2$.\n   - $c_k \\sim \\mathcal{N}(0, \\sigma_{c,k}^2)$ with $\\sigma_{c,k} = \\sigma_c/(k+1)$, $\\sigma_c = 0.5\\,\\mathrm{barn}$, so $\\lambda_{c,k} = 1/\\sigma_{c,k}^2$.\n   Evaluate the smallest eigenvalue of the unregularized approximate Hessian $H_0 = J^\\top J$ and report whether it is strictly positive (boolean), and the smallest eigenvalue of the regularized approximate Hessian $H_{\\mathrm{prior}} = J_w^\\top J_w + \\Lambda$ and report whether it is strictly positive (boolean). Use a strict positivity threshold of $\\varepsilon = 10^{-12}$ in the corresponding parameter units.\n\nDesign a program that performs the above computations for the following test suite of cases. Each case specifies the energy window and sampling, the true parameters used to generate synthetic data (for realism, although the Jacobian only depends on the current parameter values), and the initial guess $\\boldsymbol{\\theta}_0$ where the Jacobian is evaluated. All energies must be in $\\mathrm{MeV}$, widths in $\\mathrm{MeV}$, cross sections and background coefficients in $\\mathrm{barn}$.\n\n- Case $1$ (happy path, well-conditioned):\n  - Energy window: $E \\in [3,7]\\,\\mathrm{MeV}$ sampled at $N=200$ equally spaced points.\n  - True parameters: $E_0 = 5\\,\\mathrm{MeV}$, $\\Gamma = 0.5\\,\\mathrm{MeV}$, $A = 2.0\\,\\mathrm{barn}$, polynomial degree $m=2$, $c_0 = 0.5\\,\\mathrm{barn}$, $c_1 = 0.1\\,\\mathrm{barn}$, $c_2 = -0.02\\,\\mathrm{barn}$.\n  - Measurement uncertainty: $\\sigma_i = 0.02\\,\\mathrm{barn}$ for all $i$.\n  - Initial guess: $E_0^{(0)} = 5.1\\,\\mathrm{MeV}$, $\\Gamma^{(0)} = 0.55\\,\\mathrm{MeV}$, $A^{(0)} = 1.8\\,\\mathrm{barn}$, $c_k^{(0)} = c_k$ for all $k$.\n\n- Case $2$ (boundary case, very narrow width, potential undersampling):\n  - Energy window: $E \\in [4.8,5.2]\\,\\mathrm{MeV}$ sampled at $N=100$ equally spaced points.\n  - True parameters: $E_0 = 5.0\\,\\mathrm{MeV}$, $\\Gamma = 0.05\\,\\mathrm{MeV}$, $A = 2.0\\,\\mathrm{barn}$, polynomial degree $m=2$, $c_0 = 0.5\\,\\mathrm{barn}$, $c_1 = 0.1\\,\\mathrm{barn}$, $c_2 = -0.02\\,\\mathrm{barn}$.\n  - Measurement uncertainty: $\\sigma_i = 0.02\\,\\mathrm{barn}$ for all $i$.\n  - Initial guess: $E_0^{(0)} = 5.02\\,\\mathrm{MeV}$, $\\Gamma^{(0)} = 0.06\\,\\mathrm{MeV}$, $A^{(0)} = 1.9\\,\\mathrm{barn}$, $c_k^{(0)} = c_k$ for all $k$.\n\n- Case $3$ (edge case, higher-degree background with potential collinearity):\n  - Energy window: $E \\in [0,10]\\,\\mathrm{MeV}$ sampled at $N=300$ equally spaced points.\n  - True parameters: $E_0 = 5.0\\,\\mathrm{MeV}$, $\\Gamma = 0.3\\,\\mathrm{MeV}$, $A = 1.5\\,\\mathrm{barn}$, polynomial degree $m=5$, $c_0 = 0.3\\,\\mathrm{barn}$, $c_1 = -0.15\\,\\mathrm{barn}$, $c_2 = 0.05\\,\\mathrm{barn}$, $c_3 = -0.02\\,\\mathrm{barn}$, $c_4 = 0.01\\,\\mathrm{barn}$, $c_5 = -0.005\\,\\mathrm{barn}$.\n  - Measurement uncertainty: $\\sigma_i = 0.03\\,\\mathrm{barn}$ for all $i$.\n  - Initial guess: $E_0^{(0)} = 5.2\\,\\mathrm{MeV}$, $\\Gamma^{(0)} = 0.27\\,\\mathrm{MeV}$, $A^{(0)} = 1.35\\,\\mathrm{barn}$, $c_k^{(0)} = c_k$ for all $k$.\n\n- Case $4$ (edge case, low-amplitude resonance overshadowed by background):\n  - Energy window: $E \\in [3,7]\\,\\mathrm{MeV}$ sampled at $N=200$ equally spaced points.\n  - True parameters: $E_0 = 5.0\\,\\mathrm{MeV}$, $\\Gamma = 0.5\\,\\mathrm{MeV}$, $A = 0.1\\,\\mathrm{barn}$, polynomial degree $m=2$, $c_0 = 0.5\\,\\mathrm{barn}$, $c_1 = 0.1\\,\\mathrm{barn}$, $c_2 = -0.02\\,\\mathrm{barn}$.\n  - Measurement uncertainty: $\\sigma_i = 0.02\\,\\mathrm{barn}$ for all $i$.\n  - Initial guess: $E_0^{(0)} = 5.1\\,\\mathrm{MeV}$, $\\Gamma^{(0)} = 0.55\\,\\mathrm{MeV}$, $A^{(0)} = 0.12\\,\\mathrm{barn}$, $c_k^{(0)} = c_k$ for all $k$.\n\nFor each case, your program must compute and return a list containing:\n- $\\kappa_{\\mathrm{raw}}$ (float),\n- $\\kappa_{\\mathrm{scaled}}$ (float),\n- the improvement ratio $\\rho = \\kappa_{\\mathrm{raw}}/\\kappa_{\\mathrm{scaled}}$ (float),\n- a boolean indicating whether $\\min\\operatorname{eig}(H_0) > \\varepsilon$,\n- a boolean indicating whether $\\min\\operatorname{eig}(H_{\\mathrm{prior}}) > \\varepsilon$.\n\nFinal output format: Your program should produce a single line of output containing the results for the four cases as a comma-separated list of four sublists, each sublist formatted as specified above and enclosed in square brackets, with no spaces (for example, $\\big[\\,[1.0,0.5,2.0,\\mathrm{True},\\mathrm{True}],\\ldots\\,\\big]$). The values of $\\kappa_{\\mathrm{raw}}$, $\\kappa_{\\mathrm{scaled}}$, and $\\rho$ are dimensionless floats. The positive definiteness booleans must be exact logical values. Express all physical quantities internally in the units provided above; the output is unitless or boolean as specified.", "solution": "The problem requires an analysis of the conditioning of the Jacobian matrix for a nonlinear least squares model of a nuclear resonance. The analysis involves computing the Jacobian at a given parameter guess, evaluating its condition number with and without scaling, and assessing the positive definiteness of the corresponding approximate Hessian matrix, both with and without Tikhonov regularization derived from Gaussian priors.\n\nThe parametric model for the cross section $\\hat{y}$ as a function of energy $E$ is given by the sum of a Breit-Wigner resonance term and a polynomial background term:\n$$\n\\hat{y}(E;\\boldsymbol{\\theta}) = A\\,S(E;E_0,\\Gamma) + \\sum_{k=0}^{m} c_k\\,x(E)^k\n$$\nThe parameter vector $\\boldsymbol{\\theta}$ of size $P=m+4$ is composed of the resonance amplitude $A$, the resonance energy $E_0$, the resonance width $\\Gamma$, and the $m+1$ background coefficients $\\{c_k\\}_{k=0}^m$. We order the parameter vector as $\\boldsymbol{\\theta} = [A, E_0, \\Gamma, c_0, c_1, \\dots, c_m]^\\top$.\n\nThe Breit-Wigner lineshape function $S(E;E_0,\\Gamma)$ is defined as:\n$$\nS(E;E_0,\\Gamma) = \\frac{(\\Gamma/2)^2}{(E - E_0)^2 + (\\Gamma/2)^2}\n$$\nThe background polynomial is expressed in a normalized basis $x(E)$:\n$$\nx(E) = \\frac{E - E_c}{E_s}\n$$\nwhere $E_c$ is the center of the energy interval and $E_s$ is its half-width, ensuring $x(E) \\in [-1,1]$ over the fitting window.\n\nFor a set of $N$ energy points $\\{E_i\\}_{i=1}^N$, the least squares problem aims to minimize the sum of squared residuals. The residual vector is $\\mathbf{r}(\\boldsymbol{\\theta}) = \\mathbf{y} - \\hat{\\mathbf{y}}(\\boldsymbol{\\theta})$, where $\\mathbf{y}$ is the vector of observed cross sections and $\\hat{\\mathbf{y}}(\\boldsymbol{\\theta})$ is the vector of model predictions. The Jacobian matrix of the residual vector is a key component in iterative solvers like the Gauss-Newton algorithm. Its definition is:\n$$\nJ(\\boldsymbol{\\theta}) = \\frac{\\partial \\mathbf{r}(\\boldsymbol{\\theta})}{\\partial \\boldsymbol{\\theta}} = - \\frac{\\partial \\hat{\\mathbf{y}}(\\boldsymbol{\\theta})}{\\partial \\boldsymbol{\\theta}}\n$$\nThe entry $J_{ij}$ corresponds to the partial derivative of the $i$-th residual with respect to the $j$-th parameter, evaluated at $E_i$. We compute these derivatives from first principles.\n\nThe partial derivatives of $\\hat{y}(E; \\boldsymbol{\\theta})$ with respect to each parameter are:\n1.  Derivative with respect to amplitude $A$:\n    $$ \\frac{\\partial \\hat{y}}{\\partial A} = S(E;E_0,\\Gamma) $$\n2.  Derivative with respect to resonance energy $E_0$:\n    $$ \\frac{\\partial \\hat{y}}{\\partial E_0} = A \\frac{\\partial S}{\\partial E_0} = A \\left( (\\Gamma/2)^2 \\frac{-1}{((E-E_0)^2+(\\Gamma/2)^2)^2} (2(E-E_0)(-1)) \\right) = A \\frac{2(E-E_0)(\\Gamma/2)^2}{((E-E_0)^2+(\\Gamma/2)^2)^2} $$\n3.  Derivative with respect to resonance width $\\Gamma$:\n    $$ \\frac{\\partial \\hat{y}}{\\partial \\Gamma} = A \\frac{\\partial S}{\\partial \\Gamma} = A \\frac{ \\frac{\\partial}{\\partial\\Gamma}((\\Gamma/2)^2) \\cdot ((E-E_0)^2+(\\Gamma/2)^2) - (\\Gamma/2)^2 \\cdot \\frac{\\partial}{\\partial\\Gamma}((E-E_0)^2+(\\Gamma/2)^2) }{ ((E-E_0)^2+(\\Gamma/2)^2)^2 } $$\n    $$ \\frac{\\partial \\hat{y}}{\\partial \\Gamma} = A \\frac{ (\\Gamma/2) \\cdot ((E-E_0)^2+(\\Gamma/2)^2) - (\\Gamma/2)^2 \\cdot (\\Gamma/2) }{ ((E-E_0)^2+(\\Gamma/2)^2)^2 } = A \\frac{(\\Gamma/2)(E-E_0)^2}{((E-E_0)^2+(\\Gamma/2)^2)^2} $$\n4.  Derivative with respect to background coefficient $c_k$:\n    $$ \\frac{\\partial \\hat{y}}{\\partial c_k} = \\frac{\\partial}{\\partial c_k} \\sum_{j=0}^{m} c_j\\,x(E)^j = x(E)^k $$\n\nThe columns of the Jacobian matrix $J$ at a specific parameter guess $\\boldsymbol{\\theta}_0$ are constructed using the negative of these expressions evaluated at each energy $E_i$ and for each parameter in $\\boldsymbol{\\theta}_0$.\n\nThe first task is to compute the condition number $\\kappa_{\\mathrm{raw}}$ of this unweighted Jacobian $J$. This is given by the ratio of its largest to its smallest singular value: $\\kappa_{\\mathrm{raw}}(J) = \\sigma_{\\max}(J) / \\sigma_{\\min}(J)$. A large condition number indicates that the least squares problem is ill-conditioned.\n\nThe second task is to improve a related condition number through scaling. We define a weighted Jacobian $J_w = WJ$, where $W = \\mathrm{diag}(1/\\sigma_i)$ is a diagonal matrix of inverse measurement uncertainties. This transformation is equivalent to whitening the residuals. A further scaling is applied to the columns of $J_w$. We define a diagonal matrix $C$ whose diagonal elements $C_{jj}$ are the Euclidean norms of the columns of $J_w$, i.e., $C_{jj} = \\|(J_w)_{:,j}\\|_2$. The fully scaled Jacobian is $J_{\\mathrm{scaled}} = J_w C^{-1}$. By construction, each column of $J_{\\mathrm{scaled}}$ has a unit Euclidean norm. We then compute its condition number $\\kappa_{\\mathrm{scaled}}$, which is typically much improved over $\\kappa_{\\mathrm{raw}}$. The improvement is quantified by the ratio $\\rho = \\kappa_{\\mathrm{raw}} / \\kappa_{\\mathrm{scaled}}$.\n\nThe third task concerns the Gauss-Newton approximation of the Hessian matrix. For unweighted least squares, this is $H_0 \\approx J^\\top J$. For the algorithm to be stable, $H_0$ must be positive definite. We check this by computing its eigenvalues and verifying if the minimum eigenvalue is strictly positive, i.e., $\\min\\operatorname{eig}(H_0) > \\varepsilon$ for a small threshold $\\varepsilon = 10^{-12}$.\nWhen $H_0$ is ill-conditioned or singular, regularization can be introduced. Using Gaussian priors for the parameters leads to a regularized or maximum a posteriori (MAP) estimation problem. The Hessian of the corresponding objective function is approximated by $H_{\\mathrm{prior}} = J_w^\\top J_w + \\Lambda$, where $\\Lambda$ is the prior precision matrix (inverse of the prior covariance matrix). For the specified independent Gaussian priors, $\\Lambda = \\mathrm{diag}(\\lambda_j)$ with $\\lambda_j = 1/\\sigma_j^2$, where $\\sigma_j$ is the standard deviation of the prior on parameter $\\theta_j$. The specific values are $\\lambda_A = 1/\\sigma_A^2 = 1/10^2 = 0.01$, $\\lambda_{E_0} = 1/\\sigma_{E_0}^2 = 1/0.5^2 = 4$, $\\lambda_{\\Gamma} = 1/\\sigma_{\\Gamma}^2 = 1/0.2^2 = 25$, and $\\lambda_{c,k} = 1/\\sigma_{c,k}^2 = ((k+1)/\\sigma_c)^2 = (2(k+1))^2$ with $\\sigma_c=0.5$. Adding the diagonal, positive definite matrix $\\Lambda$ to the positive semi-definite $J_w^\\top J_w$ typically ensures that the resulting regularized Hessian $H_{\\mathrm{prior}}$ is strictly positive definite. We verify this by checking if $\\min\\operatorname{eig}(H_{\\mathrm{prior}}) > \\varepsilon$.\n\nThe computational procedure for each test case is as follows:\n1.  Set up the energy grid $\\{E_i\\}$ and the initial parameter guess $\\boldsymbol{\\theta}_0$.\n2.  Construct the $N \\times P$ unweighted Jacobian matrix $J$ using the derived partial derivatives evaluated at $\\boldsymbol{\\theta}_0$.\n3.  Compute $\\kappa_{\\mathrm{raw}} = \\operatorname{cond}(J)$.\n4.  Construct the weighting matrix $W$ and the weighted Jacobian $J_w = WJ$.\n5.  Compute the column norms of $J_w$ to form the diagonal matrix $C$.\n6.  Construct the scaled Jacobian $J_{\\mathrm{scaled}} = J_w C^{-1}$ and compute $\\kappa_{\\mathrm{scaled}} = \\operatorname{cond}(J_{\\mathrm{scaled}})$.\n7.  Calculate the improvement ratio $\\rho = \\kappa_{\\mathrm{raw}} / \\kappa_{\\mathrm{scaled}}$.\n8.  Compute the unregularized Hessian $H_0 = J^\\top J$ and determine if its minimum eigenvalue is greater than $\\varepsilon = 10^{-12}$.\n9.  Construct the prior precision matrix $\\Lambda$.\n10. Compute the regularized Hessian $H_{\\mathrm{prior}} = J_w^\\top J_w + \\Lambda$ and determine if its minimum eigenvalue is greater than $\\varepsilon = 10^{-12}$.\n11. Collect these five results: $\\kappa_{\\mathrm{raw}}$, $\\kappa_{\\mathrm{scaled}}$, $\\rho$, and the two booleans for positive definiteness.\nThis procedure is repeated for all four specified test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem for all test cases and prints the final result.\n    \"\"\"\n    \n    # Test cases defined as a list of dictionaries.\n    test_cases = [\n        {\n            \"E_range\": (3.0, 7.0), \"N\": 200, \"m\": 2,\n            \"sigma\": 0.02,\n            \"theta0\": np.array([1.8, 5.1, 0.55, 0.5, 0.1, -0.02])\n        },\n        {\n            \"E_range\": (4.8, 5.2), \"N\": 100, \"m\": 2,\n            \"sigma\": 0.02,\n            \"theta0\": np.array([1.9, 5.02, 0.06, 0.5, 0.1, -0.02])\n        },\n        {\n            \"E_range\": (0.0, 10.0), \"N\": 300, \"m\": 5,\n            \"sigma\": 0.03,\n            \"theta0\": np.array([1.35, 5.2, 0.27, 0.3, -0.15, 0.05, -0.02, 0.01, -0.005])\n        },\n        {\n            \"E_range\": (3.0, 7.0), \"N\": 200, \"m\": 2,\n            \"sigma\": 0.02,\n            \"theta0\": np.array([0.12, 5.1, 0.55, 0.5, 0.1, -0.02])\n        }\n    ]\n\n    # Prior definitions\n    prior_params = {\n        \"sigma_A\": 10.0,\n        \"sigma_E0\": 0.5,\n        \"sigma_Gamma\": 0.2,\n        \"sigma_c\": 0.5\n    }\n\n    epsilon = 1e-12\n    \n    results = []\n    for case in test_cases:\n        result = process_case(case, prior_params, epsilon)\n        results.append(result)\n\n    # Format output to be a list of lists string with no spaces.\n    formatted_results = [f\"[{','.join(map(str, res))}]\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\ndef get_jacobian(E, theta0, m, E_c, E_s):\n    \"\"\"\n    Computes the Jacobian matrix for the given model.\n    J = -d(y_hat)/d(theta)\n    \"\"\"\n    N = len(E)\n    P = m + 4\n    J = np.zeros((N, P))\n    \n    A, E0, Gamma = theta0[0], theta0[1], theta0[2]\n    c_coeffs = theta0[3:]\n    \n    # Breit-Wigner common terms\n    G2 = Gamma / 2.0\n    G2_sq = G2**2\n    E_minus_E0 = E - E0\n    E_minus_E0_sq = E_minus_E0**2\n    den = E_minus_E0_sq + G2_sq\n    den_sq = den**2\n    \n    # Column for A\n    # d(y_hat)/dA = S(E)\n    S = G2_sq / den\n    J[:, 0] = -S\n    \n    # Column for E0\n    # d(y_hat)/dE0 = A * 2 * (E - E0) * (Gamma/2)^2 / ((E-E0)^2 + (Gamma/2)^2)^2\n    J[:, 1] = -A * 2 * E_minus_E0 * G2_sq / den_sq\n    \n    # Column for Gamma\n    # d(y_hat)/dGamma = A * (Gamma/2) * (E-E0)^2 / ((E-E0)^2 + (Gamma/2)^2)^2\n    J[:, 2] = -A * G2 * E_minus_E0_sq / den_sq\n    \n    # Columns for c_k\n    # d(y_hat)/dc_k = x(E)^k\n    x = (E - E_c) / E_s\n    for k in range(m + 1):\n        J[:, 3 + k] = -np.power(x, k)\n        \n    return J\n\ndef process_case(case_params, prior_params, epsilon):\n    \"\"\"\n    Processes a single test case to compute all required values.\n    \"\"\"\n    E_min, E_max = case_params[\"E_range\"]\n    N = case_params[\"N\"]\n    m = case_params[\"m\"]\n    sigma = case_params[\"sigma\"]\n    theta0 = case_params[\"theta0\"]\n    \n    E = np.linspace(E_min, E_max, N)\n    E_c = (E_max + E_min) / 2.0\n    E_s = (E_max - E_min) / 2.0\n    \n    # 1. Unweighted Jacobian and condition number\n    J = get_jacobian(E, theta0, m, E_c, E_s)\n    kappa_raw = np.linalg.cond(J)\n    \n    # 2. Scaled Jacobian and condition number\n    W = np.diag(np.full(N, 1.0/sigma))\n    J_w = W @ J\n    \n    col_norms = np.linalg.norm(J_w, axis=0)\n    # Handle potential zero columns to avoid division by zero\n    col_norms[col_norms == 0] = 1.0\n    C_inv = np.diag(1.0 / col_norms)\n    \n    J_scaled = J_w @ C_inv\n    kappa_scaled = np.linalg.cond(J_scaled)\n    \n    improvement_ratio = kappa_raw / kappa_scaled if kappa_scaled > 0 else float('inf')\n\n    # 3. Hessian analysis\n    # Unregularized Hessian H0\n    H0 = J.T @ J\n    eigvals_H0 = np.linalg.eigvalsh(H0)\n    is_H0_pos_def = bool(np.min(eigvals_H0) > epsilon)\n    \n    # Regularized Hessian H_prior\n    sigma_A = prior_params[\"sigma_A\"]\n    sigma_E0 = prior_params[\"sigma_E0\"]\n    sigma_Gamma = prior_params[\"sigma_Gamma\"]\n    sigma_c = prior_params[\"sigma_c\"]\n    \n    lambda_A = 1.0 / sigma_A**2\n    lambda_E0 = 1.0 / sigma_E0**2\n    lambda_Gamma = 1.0 / sigma_Gamma**2\n    lambda_c = [((k + 1) / sigma_c)**2 for k in range(m + 1)]\n    \n    Lambda_diag = np.concatenate(([lambda_A, lambda_E0, lambda_Gamma], lambda_c))\n    Lambda = np.diag(Lambda_diag)\n    \n    H_prior = J_w.T @ J_w + Lambda\n    eigvals_H_prior = np.linalg.eigvalsh(H_prior)\n    is_H_prior_pos_def = bool(np.min(eigvals_H_prior) > epsilon)\n    \n    return [kappa_raw, kappa_scaled, improvement_ratio, is_H0_pos_def, is_H_prior_pos_def]\n    \n\n# The problem defines a specific program structure. Calling solve() will execute it.\nif __name__ == \"__main__\":\n    solve()\n\n```", "id": "3596455"}, {"introduction": "A successful numerical fit yields a set of best-fit parameters and their corresponding covariance matrix, but the analysis does not end there. The final and most crucial step is to critically interpret these results to understand what the data truly reveals about the underlying physics. This exercise focuses on post-fit analysis, guiding you to compute and interpret the covariance matrix to uncover correlations between the resonance energy $E_R$ and width $\\Gamma$. [@problem_id:3596468] By exploring scenarios where these parameters are difficult to distinguish, you will develop an intuition for parameter identifiability and how it is influenced by experimental conditions like energy resolution and background levels.", "problem": "You are asked to write a complete and runnable program that performs a weighted nonlinear least-squares fit to a physically motivated resonance model in a nuclear scattering cross section and then analyzes the parameter correlations and identifiability of the resonance energy and width. The model consists of a single isolated resonance described by a Lorentzian line shape consistent with probability conservation and unitarity, plus a smooth linear background. Your program must begin from the definitions of the scattering matrix and the basic structure of resonant scattering and justify the model used for the fit within the code comments. The fitting should be conducted for synthetic datasets with known measurement uncertainties, treated as independent and Gaussian, and the parameter covariance must be estimated from the Fisher information associated with the weighted least-squares objective. The program must compute the covariance matrix for the resonance energy and width, the corresponding correlation coefficient, and an identifiability flag based on a numerically specified criterion.\n\nDefinitions and requirements:\n\n- Let the independent variable be the center-of-mass energy $E$ in $\\mathrm{MeV}$ and the observable be the cross section $y(E)$ in barns. The resonant structure is characterized by a pole in the scattering matrix close to the real energy axis with pole position $E_R - i \\Gamma/2$, where $E_R$ is the resonance energy in $\\mathrm{MeV}$ and $\\Gamma$ is the total width in $\\mathrm{MeV}$. The background is assumed to be slowly varying over the energy window.\n- Define the parameter vector as $\\theta = (E_R, \\Gamma, A, B_0, B_1)$, where $A$ is a positive amplitude parameter in barns, $B_0$ is a background intercept in barns, and $B_1$ is a background slope in barns per $\\mathrm{MeV}$.\n- Given data points $\\{(E_i, y_i, \\sigma_i)\\}_{i=1}^N$ with $N$ points, where $\\sigma_i$ are known standard deviations in barns, the weighted nonlinear least-squares objective is\n$$\n\\chi^2(\\theta) = \\sum_{i=1}^N \\frac{\\left[y_i - y(E_i;\\theta)\\right]^2}{\\sigma_i^2}.\n$$\n- At the best-fit $\\hat{\\theta}$, the parameter covariance matrix is approximated by the inverse of the Fisher information for independent Gaussian errors,\n$$\n\\mathrm{Cov}(\\hat{\\theta}) \\approx \\left(J^\\top W J\\right)^{-1},\n$$\nwhere $J$ is the Jacobian matrix with $J_{ij} = \\partial y(E_i;\\theta)/\\partial \\theta_j$ evaluated at $\\hat{\\theta}$, and $W$ is diagonal with $W_{ii} = 1/\\sigma_i^2$.\n- Extract the $2\\times 2$ covariance submatrix corresponding to $(E_R, \\Gamma)$, compute its correlation coefficient\n$$\n\\rho_{E_R,\\Gamma} = \\frac{\\mathrm{Cov}(E_R,\\Gamma)}{\\sqrt{\\mathrm{Var}(E_R)\\,\\mathrm{Var}(\\Gamma)}},\n$$\nand compute the $2$-norm condition number of this $2\\times 2$ covariance submatrix,\n$$\n\\kappa = \\|C\\|_2 \\,\\|C^{-1}\\|_2,\n$$\nwhere $C$ is the covariance submatrix for $(E_R,\\Gamma)$.\n- Define an identifiability flag $I$ for $(E_R,\\Gamma)$ as follows: set $I$ to $\\mathrm{False}$ (not identifiable) if either $|\\rho_{E_R,\\Gamma}| > 0.95$ or $\\kappa > 10^8$, and set $I$ to $\\mathrm{True}$ otherwise.\n\nYour program must generate synthetic datasets using a resonance line shape consistent with single-level resonant scattering and a linear background. Energies must be in $\\mathrm{MeV}$, and cross sections must be in barns. Use evenly spaced energy grids symmetric about $E_R$, with the half-width of the window proportional to $\\Gamma$. Do not add random noise; use the model to generate $y_i$ exactly and assign a constant $\\sigma_i$ per test case.\n\nImplement a weighted nonlinear least-squares fit for each test case with the absolute weighting of the uncertainties enforced, and then compute:\n- The correlation coefficient $\\rho_{E_R,\\Gamma}$ (dimensionless).\n- The condition number $\\kappa$ (dimensionless).\n- The identifiability flag $I$ (boolean).\n\nTest suite:\n\nProvide and solve the following three test cases. For each case, energies must be expressed in $\\mathrm{MeV}$, widths in $\\mathrm{MeV}$, cross sections in barns, and uncertainties in barns. Each energy grid must run from $E_{\\min} = E_R - m\\,\\Gamma$ to $E_{\\max} = E_R + m\\,\\Gamma$ with $N$ evenly spaced points, where $m$ is a dimensionless multiplier:\n\n- Case $1$ (well-sampled resonance, moderate background):\n  - $E_R = 5.0\\,\\mathrm{MeV}$, $\\Gamma = 0.2\\,\\mathrm{MeV}$, $A = 10.0\\,\\mathrm{barns}$, $B_0 = 1.0\\,\\mathrm{barns}$, $B_1 = 0.2\\,\\mathrm{barns}/\\mathrm{MeV}$,\n  - $m = 8$, $N = 121$, $\\sigma_i \\equiv 0.05\\,\\mathrm{barns}$ for all $i$.\n\n- Case $2$ (narrow resonance with coarse sampling):\n  - $E_R = 5.0\\,\\mathrm{MeV}$, $\\Gamma = 0.01\\,\\mathrm{MeV}$, $A = 10.0\\,\\mathrm{barns}$, $B_0 = 1.0\\,\\mathrm{barns}$, $B_1 = 0.2\\,\\mathrm{barns}/\\mathrm{MeV}$,\n  - $m = 6$, $N = 21$, $\\sigma_i \\equiv 0.05\\,\\mathrm{barns}$ for all $i$.\n\n- Case $3$ (limited window with strong background slope):\n  - $E_R = 5.0\\,\\mathrm{MeV}$, $\\Gamma = 0.5\\,\\mathrm{MeV}$, $A = 5.0\\,\\mathrm{barns}$, $B_0 = 1.0\\,\\mathrm{barns}$, $B_1 = 5.0\\,\\mathrm{barns}/\\mathrm{MeV}$,\n  - $m = 2$, $N = 41$, $\\sigma_i \\equiv 0.05\\,\\mathrm{barns}$ for all $i$.\n\nAlgorithmic expectations:\n\n- Your code must justify the resonance line shape from fundamental principles of resonant scattering and implement the weighted nonlinear least-squares fit using a modern numerical routine.\n- Compute the parameter covariance using the appropriate weighting with $\\sigma_i$ treated as absolute uncertainties.\n- Extract the $(E_R,\\Gamma)$ covariance submatrix, compute $\\rho_{E_R,\\Gamma}$ and $\\kappa$, and set the identifiability flag $I$ as per the criterion above.\n\nFinal output format:\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case and is itself a list of the form $[\\rho_{E_R,\\Gamma}, \\kappa, I]$. For example, the printed line must look like\n$[[r_1, c_1, b_1],[r_2, c_2, b_2],[r_3, c_3, b_3]]$,\nwhere $r_k$ and $c_k$ are floats and $b_k$ is a boolean for case $k$.", "solution": "The problem requires the implementation of a weighted nonlinear least-squares fit to a physically motivated resonance model and a subsequent analysis of parameter correlations and identifiability. The solution is structured around three core components: the definition of the physical model, the numerical fitting procedure, and the statistical analysis of the parameter uncertainties.\n\n**1. The Physical Model: Breit-Wigner Resonance**\n\nThe model for the nuclear scattering cross section is based on the fundamental principles of resonant scattering in quantum mechanics. A resonance corresponds to the formation of a quasi-stable intermediate state with a characteristic energy $E_R$ and a finite lifetime, which is inversely proportional to its decay width $\\Gamma$. The presence of such a state leads to a sharp peak in the energy-dependent cross section.\n\nThis behavior is formally described by the scattering matrix ($S$-matrix). To conserve probability, the $S$-matrix must be unitary, i.e., $S^\\dagger S = I$. For a single, isolated resonance in a single scattering channel, the $S$-matrix element can be parameterized in the vicinity of the resonance by a pole in the complex energy plane at $E = E_R - i\\Gamma/2$. This leads to the Breit-Wigner formula for the $S$-matrix element:\n$$\nS(E) \\approx e^{2i\\delta_{bg}} \\frac{E - E_R - i\\Gamma/2}{E - E_R + i\\Gamma/2}\n$$\nwhere $\\delta_{bg}$ represents a slowly varying background phase shift. For our phenomenological model, we can simplify by setting $\\delta_{bg}=0$. The resonant part of the elastic scattering cross section, $\\sigma_{\\text{res}}(E)$, is proportional to $|1 - S(E)|^2$. A direct calculation yields:\n$$\n\\sigma_{\\text{res}}(E) \\propto \\left|1 - \\frac{E - E_R - i\\Gamma/2}{E - E_R + i\\Gamma/2}\\right|^2 = \\left|\\frac{(E - E_R + i\\Gamma/2) - (E - E_R - i\\Gamma/2)}{E - E_R + i\\Gamma/2}\\right|^2 = \\left|\\frac{i\\Gamma}{E - E_R + i\\Gamma/2}\\right|^2 = \\frac{\\Gamma^2}{(E - E_R)^2 + (\\Gamma/2)^2}\n$$\nThis function has a Lorentzian shape. The problem specifies a model combining this resonant structure with a simple linear background. The complete model for the cross section $y(E)$ is therefore defined as:\n$$\ny(E; \\theta) = A \\frac{(\\Gamma/2)^2}{(E - E_R)^2 + (\\Gamma/2)^2} + B_0 + B_1 E\n$$\nThe parameter vector is $\\theta = (E_R, \\Gamma, A, B_0, B_1)$, where $E_R$ is the resonance energy, $\\Gamma$ is its full width at half maximum (FWHM), $A$ is the peak amplitude of the resonant part, and $B_0$ and $B_1$ are the intercept and slope of the linear background, respectively.\n\n**2. Weighted Nonlinear Least-Squares Fitting**\n\nThe task is to find the best-fit parameters $\\hat{\\theta}$ for a given set of synthetic data points $\\{(E_i, y_i, \\sigma_i)\\}_{i=1}^N$. This is achieved by minimizing the weighted sum of squared residuals, the chi-squared statistic $\\chi^2$:\n$$\n\\chi^2(\\theta) = \\sum_{i=1}^N \\left(\\frac{y_i - y(E_i;\\theta)}{\\sigma_i}\\right)^2\n$$\nMinimizing this value is a nonlinear least-squares problem. The algorithm will utilize `scipy.optimize.curve_fit`, a standard routine based on the Levenberg-Marquardt algorithm. The synthetic data for each test case is generated from the model itself without adding random noise, meaning the true parameters $\\theta_{\\text{true}}$ correspond to the global minimum where $\\chi^2(\\theta_{\\text{true}}) = 0$. The fit is initialized with $\\theta_{\\text{true}}$ to ensure immediate and exact convergence. The `absolute_sigma=True` argument is passed to `curve_fit` to ensure the uncertainties $\\sigma_i$ are treated as absolute values, which is critical for the correct scaling of the output covariance matrix.\n\n**3. Parameter Covariance and Identifiability Analysis**\n\nThe final step is to assess the identifiability of the resonance energy $E_R$ and width $\\Gamma$. This is done by analyzing the parameter covariance matrix, $\\mathrm{Cov}(\\hat{\\theta})$, which quantifies the uncertainties and correlations of the best-fit parameters. In nonlinear least-squares, this matrix is approximated by the inverse of the Fisher information matrix. For independent Gaussian errors, this is:\n$$\n\\mathrm{Cov}(\\hat{\\theta}) \\approx (J^\\top W J)^{-1}\n$$\nwhere $J$ is the Jacobian matrix of the model with respect to the parameters, $J_{ij} = \\frac{\\partial y(E_i;\\theta)}{\\partial \\theta_j}$, evaluated at the best-fit solution $\\hat{\\theta}$, and $W$ is the diagonal weight matrix with elements $W_{ii} = 1/\\sigma_i^2$. The `curve_fit` function with `absolute_sigma=True` returns exactly this matrix.\n\nThe analysis focuses on the $2 \\times 2$ submatrix $C$ of $\\mathrm{Cov}(\\hat{\\theta})$ corresponding to the parameters $(E_R, \\Gamma)$:\n$$\nC = \\begin{pmatrix} \\mathrm{Var}(E_R) & \\mathrm{Cov}(E_R, \\Gamma) \\\\ \\mathrm{Cov}(\\Gamma, E_R) & \\mathrm{Var}(\\Gamma) \\end{pmatrix}\n$$\nFrom this submatrix, two metrics are computed:\na) The **correlation coefficient** $\\rho_{E_R,\\Gamma}$, which measures the linear interdependence between the estimators for $E_R$ and $\\Gamma$:\n$$\n\\rho_{E_R,\\Gamma} = \\frac{C_{12}}{\\sqrt{C_{11}C_{22}}}\n$$\nA value of $|\\rho_{E_R,\\Gamma}|$ approaching $1$ indicates a strong correlation, which makes it difficult to disentangle the effects of the two parameters.\n\nb) The **$2$-norm condition number** $\\kappa$ of the covariance submatrix $C$, defined as $\\kappa = \\|C\\|_2 \\|C^{-1}\\|_2$. For a symmetric positive definite matrix like $C$, this is the ratio of its largest to its smallest eigenvalue, $\\kappa = \\lambda_{\\max}/\\lambda_{\\min}$. A large condition number signifies that the problem of determining $E_R$ and $\\Gamma$ is ill-conditioned, meaning a small perturbation in the data could cause a large change in the parameter estimates.\n\nFinally, an **identifiability flag** $I$ is determined based on the specified criteria: $I$ is set to $\\mathrm{False}$ if $|\\rho_{E_R,\\Gamma}| > 0.95$ or if $\\kappa > 10^8$, and to $\\mathrm{True}$ otherwise. The program implements this full workflow for the three provided test cases, each representing a different experimental scenario, and outputs the resulting tuple $[\\rho_{E_R,\\Gamma}, \\kappa, I]$ for each.", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import curve_fit\nfrom numpy.linalg import cond\n\ndef solve():\n    \"\"\"\n    Performs a weighted nonlinear least-squares fit of a Breit-Wigner resonance model\n    to synthetic data and analyzes the parameter identifiability.\n    \"\"\"\n\n    # --- Model Definition and Justification ---\n    # In nuclear scattering, an isolated resonance can be described by the Breit-Wigner formula.\n    # This arises from parameterizing the S-matrix near a pole E_R - i*Gamma/2 on the complex energy plane,\n    # a consequence of a quasi-bound state with finite lifetime. For single-channel scattering,\n    # the cross-section exhibits a Lorentzian energy dependence. The phenomenological model used here\n    # combines a Lorentzian peak with a linear background, which is a common approach for analyzing\n    # experimental data. The model is:\n    # y(E) = A * (Gamma/2)^2 / ((E - E_R)^2 + (Gamma/2)^2) + B_0 + B_1 * E\n    # The parameters are theta = (E_R, Gamma, A, B_0, B_1).\n    def resonance_model(E, E_R, Gamma, A, B0, B1):\n        \"\"\"\n        Defines the resonance model: a Breit-Wigner-like Lorentzian plus a linear background.\n\n        Args:\n            E (float or array): Center-of-mass energy in MeV.\n            E_R (float): Resonance energy in MeV.\n            Gamma (float): Resonance total width in MeV.\n            A (float): Resonance amplitude in barns.\n            B0 (float): Background intercept in barns.\n            B1 (float): Background slope in barns/MeV.\n\n        Returns:\n            float or array: Cross section in barns.\n        \"\"\"\n        lorentzian = A * (Gamma / 2.0)**2 / ((E - E_R)**2 + (Gamma / 2.0)**2)\n        background = B0 + B1 * E\n        return lorentzian + background\n\n    test_cases = [\n        # Case 1: Well-sampled resonance, moderate background.\n        {\"params\": (5.0, 0.2, 10.0, 1.0, 0.2), \"m\": 8.0, \"N\": 121, \"sigma\": 0.05},\n        # Case 2: Narrow resonance with coarse sampling.\n        {\"params\": (5.0, 0.01, 10.0, 1.0, 0.2), \"m\": 6.0, \"N\": 21, \"sigma\": 0.05},\n        # Case 3: Limited window with strong background slope.\n        {\"params\": (5.0, 0.5, 5.0, 1.0, 5.0), \"m\": 2.0, \"N\": 41, \"sigma\": 0.05},\n    ]\n\n    all_results = []\n\n    for case in test_cases:\n        p_true = case[\"params\"]\n        E_R_true, Gamma_true, _, _, _ = p_true\n        m, N, sigma_val = case[\"m\"], case[\"N\"], case[\"sigma\"]\n\n        # --- Step 1: Generate Synthetic Data ---\n        # The data is generated without noise, so y_i = y(E_i; theta_true).\n        E_min = E_R_true - m * Gamma_true\n        E_max = E_R_true + m * Gamma_true\n        E_data = np.linspace(E_min, E_max, N)\n        y_data = resonance_model(E_data, *p_true)\n        sigma_data = np.full_like(y_data, sigma_val)\n\n        # --- Step 2: Perform Weighted Nonlinear Least-Squares Fit ---\n        # We use the true parameters as the initial guess to ensure convergence\n        # to the exact solution, as the data is noise-free.\n        # 'absolute_sigma=True' is crucial. It ensures the covariance matrix pcov\n        # is calculated as (J^T W J)^-1, where W_ii = 1/sigma_i^2, as required.\n        try:\n            _, pcov = curve_fit(\n                resonance_model,\n                E_data,\n                y_data,\n                p0=p_true,\n                sigma=sigma_data,\n                absolute_sigma=True,\n                check_finite=True\n            )\n        except RuntimeError:\n            # In ill-conditioned cases, the fit might fail.\n            # Represent this scenario with NaN values.\n            all_results.append([np.nan, np.nan, False])\n            continue\n        \n        # --- Step 3: Analyze Parameter Covariance and Identifiability ---\n        # Extract the 2x2 covariance submatrix for (E_R, Gamma).\n        # The order of parameters is (E_R, Gamma, A, B0, B1).\n        C_ER_Gamma = pcov[0:2, 0:2]\n\n        # Compute the correlation coefficient rho_{E_R, Gamma}.\n        var_ER = C_ER_Gamma[0, 0]\n        var_Gamma = C_ER_Gamma[1, 1]\n        cov_ER_Gamma = C_ER_Gamma[0, 1]\n        \n        # Avoid division by zero if variances are zero or negative (numerical instability)\n        if var_ER <= 0 or var_Gamma <= 0:\n            rho = np.nan\n        else:\n            rho = cov_ER_Gamma / np.sqrt(var_ER * var_Gamma)\n\n        # Compute the 2-norm condition number kappa of the submatrix.\n        kappa = cond(C_ER_Gamma, 2)\n\n        # Determine the identifiability flag I based on the given criteria.\n        is_identifiable = not (np.abs(rho) > 0.95 or kappa > 1e8)\n\n        # Append results for this case. Ensure boolean is a native Python bool.\n        all_results.append([rho, kappa, bool(is_identifiable)])\n\n    # --- Final Output Formatting ---\n    # The output must be a single line: [[r1, c1, b1],[r2, c2, b2],...].\n    # The `str` representation of a list of lists matches this format.\n    formatted_results = \",\".join(map(str, all_results))\n    print(f\"[{formatted_results}]\")\n\nsolve()\n```", "id": "3596468"}]}