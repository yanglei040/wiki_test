{"hands_on_practices": [{"introduction": "The Metropolis-Hastings algorithm is powerful because of its generality. This exercise [@problem_id:3604490] provides hands-on practice with its core mechanics in a common and practical setting: enforcing a physical constraint on a parameter space. You will work with an augmented target distribution that uses a Lagrange multiplier to enforce the constraint, and derive the acceptance probability for a proposal that is not symmetric, requiring careful application of the full Hastings ratio.", "problem": "Consider symmetric nuclear matter modeled at saturation density $\\rho_{0} = 0.16$ $\\mathrm{fm}^{-3}$, where the energy per nucleon is represented by a Skyrme-like mean-field surrogate $E/A(\\theta)$ with parameter vector $\\theta = (t_{0}, t_{3}, \\sigma)$. Define\n$$\n\\frac{E(\\theta)}{A} \\equiv \\alpha + \\beta\\, t_{0} + \\zeta\\, t_{3}\\, \\rho_{0}^{\\sigma}\n$$\nwith known constants $\\alpha = 22$, $\\beta = 0.02$, and $\\zeta = -0.001$. To enforce the bulk property $\\frac{E(\\theta)}{A} = E_{0}$ with $E_{0} = -16$, introduce a positive Lagrange multiplier $\\lambda$ and augment the posterior for $(\\theta, \\lambda)$ according to\n$$\n\\pi(\\theta,\\lambda) \\propto p(D \\mid \\theta)\\, \\exp\\!\\big[-\\lambda\\big(\\tfrac{E(\\theta)}{A} - E_{0}\\big)\\big]\\, p(\\theta)\\, p(\\lambda)\n$$\nwhere $p(D \\mid \\theta)$ is the data likelihood, $p(\\theta)$ is the prior over $\\theta$, and $p(\\lambda)$ is the prior over $\\lambda$. The observational surrogate data are modeled by a Gaussian likelihood\n$$\np(D \\mid \\theta) \\propto \\exp\\!\\Big(-\\frac{(y(\\theta) - y_{D})^{2}}{2 s^{2}}\\Big)\n$$\nwith $y(\\theta) \\equiv d_{0} + d_{1}\\, t_{0} + d_{3}\\, t_{3}$, $d_{0} = 0$, $d_{1} = 0.001$, $d_{3} = -0.00005$, $y_{D} = 5$, and $s = 2$. Assume independent Gaussian priors\n$$\nt_{0} \\sim \\mathcal{N}(\\mu_{0}, \\sigma_{0}^{2}), \\quad t_{3} \\sim \\mathcal{N}(\\mu_{3}, \\sigma_{3}^{2}), \\quad \\sigma \\sim \\mathcal{N}(\\mu_{\\sigma}, \\sigma_{\\sigma}^{2})\n$$\nwith $(\\mu_{0}, \\sigma_{0}) = (-1900, 300)$, $(\\mu_{3}, \\sigma_{3}) = (13000, 2000)$, $(\\mu_{\\sigma}, \\sigma_{\\sigma}) = (0.5, 0.05)$, and a Gamma prior for $\\lambda$,\n$$\n\\lambda \\sim \\mathrm{Gamma}(a,b), \\quad a = 2, \\quad b = 1\n$$\nwith density $p(\\lambda) \\propto \\lambda^{a-1} \\exp(-b \\lambda)$. Consider a Metropolis-Hastings (MH) sampler on the augmented space $(\\theta,\\lambda)$ with the following proposals:\n- A symmetric Gaussian random-walk proposal on $\\theta$: $\\theta' = \\theta + \\xi$, with $\\xi \\sim \\mathcal{N}(0, \\Sigma_{q})$ for a fixed positive-definite covariance matrix $\\Sigma_{q}$,\n- A random-walk in $\\ln \\lambda$: $\\ln \\lambda' = \\ln \\lambda + \\eta$, with $\\eta \\sim \\mathcal{N}(0, s_{\\ell}^{2})$, which implies the marginal proposal density $q(\\lambda' \\mid \\lambda)$ is log-normal.\n\nTasks:\n1. Starting from Bayes’ theorem and the Metropolis-Hastings (MH) acceptance probability definition, derive the general MH acceptance probability $\\alpha\\big((\\theta,\\lambda) \\to (\\theta',\\lambda')\\big)$ for this augmented target $\\pi(\\theta,\\lambda)$ and the specified proposals. Your derivation must explicitly account for the proposal asymmetry induced by the log-normal proposal in $\\lambda$.\n2. Evaluate the acceptance probability for a single proposed update from the current state $(\\theta,\\lambda)$ to $(\\theta',\\lambda')$ with the following numerical values:\n   - Current state: $t_{0} = -1800$, $t_{3} = 14000$, $\\sigma = 0.5$, $\\lambda = 0.8$.\n   - Proposed state: $t_{0}' = -1750$, $t_{3}' = 15000$, $\\sigma' = 0.45$, $\\lambda' = 1.1$.\nUse the quantities and models given above without any additional approximations. Express your final numerical answer for the acceptance probability as a pure decimal, rounded to four significant figures.", "solution": "The problem asks for two tasks: first, to derive the general Metropolis-Hastings (MH) acceptance probability for a given augmented posterior and proposal mechanism; and second, to evaluate this probability for a specific state transition.\n\n## Part 1: Derivation of the Acceptance Probability\n\nThe state of the Markov chain is given by the vector $X = (\\theta, \\lambda)$, where $\\theta = (t_0, t_3, \\sigma)$. The target distribution is the posterior $\\pi(\\theta, \\lambda)$. The MH acceptance probability $\\alpha(X \\to X')$ for a move from a current state $X$ to a proposed state $X'$ is defined as:\n$$\n\\alpha(X \\to X') = \\min\\left(1, R\\right)\n$$\nwhere $R$ is the Hastings ratio, given by:\n$$\nR = \\frac{\\pi(X') q(X|X')}{\\pi(X) q(X'|X)}\n$$\nHere, $\\pi(X) = \\pi(\\theta, \\lambda)$ is the target posterior density and $q(X'|X)$ is the proposal density.\n\nThe proposal distribution is factored into independent proposals for $\\theta$ and $\\lambda$: $q(X'|X) = q(\\theta'|\\theta)q(\\lambda'|\\lambda)$.\n\n1.  **Proposal Ratio for $\\theta$**: The proposal for $\\theta$ is a symmetric Gaussian random walk, $\\theta' = \\theta + \\xi$ with $\\xi \\sim \\mathcal{N}(0, \\Sigma_q)$. The proposal density is $q(\\theta'|\\theta) = \\mathcal{N}(\\theta'; \\theta, \\Sigma_q)$. A key property of the Gaussian density is its symmetry in its arguments, meaning $\\mathcal{N}(\\theta'; \\theta, \\Sigma_q) = \\mathcal{N}(\\theta; \\theta', \\Sigma_q)$. Therefore, $q(\\theta'|\\theta) = q(\\theta|\\theta')$, and the proposal ratio for $\\theta$ is unity:\n    $$\n    \\frac{q(\\theta|\\theta')}{q(\\theta'|\\theta)} = 1\n    $$\n\n2.  **Proposal Ratio for $\\lambda$**: The proposal for $\\lambda$ is a random walk in the logarithm of $\\lambda$, i.e., $\\ln \\lambda' = \\ln \\lambda + \\eta$, where $\\eta \\sim \\mathcal{N}(0, s_\\ell^2)$. Let $g(\\eta)$ be the density of $\\eta$, which is symmetric: $g(\\eta) = g(-\\eta)$. To find the proposal density $q(\\lambda'|\\lambda)$, we use the change of variables formula. Let $u = \\ln \\lambda$, so the proposal is $u' = u + \\eta$. The proposal density in terms of $u$ is $q_u(u'|u) = g(u'-u)$. The proposal density for $\\lambda$ is then:\n    $$\n    q(\\lambda'|\\lambda) = q_u(\\ln\\lambda'|\\ln\\lambda) \\left|\\frac{d(\\ln \\lambda')}{d\\lambda'}\\right| = g(\\ln\\lambda' - \\ln\\lambda) \\frac{1}{\\lambda'}\n    $$\n    Similarly, the reverse proposal density is:\n    $$\n    q(\\lambda|\\lambda') = g(\\ln\\lambda - \\ln\\lambda') \\frac{1}{\\lambda}\n    $$\n    The proposal ratio for $\\lambda$ is therefore:\n    $$\n    \\frac{q(\\lambda|\\lambda')}{q(\\lambda'|\\lambda)} = \\frac{g(\\ln\\lambda - \\ln\\lambda') / \\lambda}{g(\\ln\\lambda' - \\ln\\lambda) / \\lambda'} = \\frac{g(-(\\ln\\lambda' - \\ln\\lambda)) / \\lambda}{g(\\ln\\lambda' - \\ln\\lambda) / \\lambda'}\n    $$\n    Using the symmetry of $g$, this simplifies to the well-known Jacobian correction factor:\n    $$\n    \\frac{q(\\lambda|\\lambda')}{q(\\lambda'|\\lambda)} = \\frac{1/\\lambda}{1/\\lambda'} = \\frac{\\lambda'}{\\lambda}\n    $$\n\nCombining these results, the full proposal ratio is $\\frac{q(X|X')}{q(X'|X)} = \\frac{\\lambda'}{\\lambda}$.\n\nThe Hastings ratio $R$ becomes:\n$$\nR = \\frac{\\pi(\\theta', \\lambda')}{\\pi(\\theta, \\lambda)} \\frac{\\lambda'}{\\lambda}\n$$\n\nNow we express the ratio of the posterior densities. The posterior is given as:\n$$\n\\pi(\\theta,\\lambda) \\propto p(D \\mid \\theta)\\, \\exp\\!\\big[-\\lambda\\big(\\tfrac{E(\\theta)}{A} - E_{0}\\big)\\big]\\, p(\\theta)\\, p(\\lambda)\n$$\nThe ratio is:\n$$\n\\frac{\\pi(\\theta', \\lambda')}{\\pi(\\theta, \\lambda)} = \\frac{p(D \\mid \\theta')}{p(D \\mid \\theta)} \\frac{\\exp[-\\lambda'(\\frac{E(\\theta')}{A}-E_0)]}{\\exp[-\\lambda(\\frac{E(\\theta)}{A}-E_0)]} \\frac{p(\\theta')}{p(\\theta)} \\frac{p(\\lambda')}{p(\\lambda)}\n$$\nLet's analyze each term in the ratio:\n-   **Likelihood ratio**: $p(D \\mid \\theta) \\propto \\exp(-\\frac{(y(\\theta) - y_{D})^{2}}{2 s^{2}})$\n    $$\n    \\frac{p(D \\mid \\theta')}{p(D \\mid \\theta)} = \\exp\\left( \\frac{(y(\\theta) - y_{D})^{2} - (y(\\theta') - y_{D})^{2}}{2 s^{2}} \\right)\n    $$\n-   **Constraint term ratio**:\n    $$\n    \\frac{\\exp[-\\lambda'(\\frac{E(\\theta')}{A}-E_0)]}{\\exp[-\\lambda(\\frac{E(\\theta)}{A}-E_0)]} = \\exp\\left( \\lambda\\left(\\tfrac{E(\\theta)}{A} - E_{0}\\right) - \\lambda'\\left(\\tfrac{E(\\theta')}{A} - E_{0}\\right) \\right)\n    $$\n-   **Parameter prior ratio**: $p(\\theta) = p(t_0)p(t_3)p(\\sigma)$ due to independence.\n    $$\n    \\frac{p(\\theta')}{p(\\theta)} = \\frac{p(t_0')}{p(t_0)} \\frac{p(t_3')}{p(t_3)} \\frac{p(\\sigma')}{p(\\sigma)}\n    $$\n    For a generic Gaussian prior $x \\sim \\mathcal{N}(\\mu_x, \\sigma_x^2)$, the ratio is $\\frac{p(x')}{p(x)} = \\exp\\left( \\frac{(x - \\mu_x)^2 - (x' - \\mu_x)^2}{2\\sigma_x^2} \\right)$.\n-   **Lagrange multiplier prior ratio**: $\\lambda \\sim \\mathrm{Gamma}(a,b)$, so $p(\\lambda) \\propto \\lambda^{a-1} \\exp(-b\\lambda)$.\n    $$\n    \\frac{p(\\lambda')}{p(\\lambda)} = \\frac{(\\lambda')^{a-1}\\exp(-b\\lambda')}{\\lambda^{a-1}\\exp(-b\\lambda)} = \\left(\\frac{\\lambda'}{\\lambda}\\right)^{a-1} \\exp(-b(\\lambda' - \\lambda))\n    $$\nCombining the $\\lambda$ prior ratio with the proposal ratio $\\lambda'/\\lambda$:\n$$\n\\frac{p(\\lambda')}{p(\\lambda)} \\frac{\\lambda'}{\\lambda} = \\left(\\frac{\\lambda'}{\\lambda}\\right)^{a-1} \\exp(-b(\\lambda' - \\lambda)) \\frac{\\lambda'}{\\lambda} = \\left(\\frac{\\lambda'}{\\lambda}\\right)^{a} \\exp(-b(\\lambda' - \\lambda))\n$$\nFinally, assembling all parts, the Hastings ratio $R$ is:\n$$\nR = \\left(\\frac{\\lambda'}{\\lambda}\\right)^{a} \\exp\\left(-b(\\lambda' - \\lambda)\\right) \\times \\exp\\left( \\frac{(y(\\theta) - y_{D})^{2} - (y(\\theta') - y_{D})^{2}}{2 s^{2}} \\right) \\times \\exp\\left( \\lambda\\left(\\tfrac{E(\\theta)}{A} - E_{0}\\right) - \\lambda'\\left(\\tfrac{E(\\theta')}{A} - E_{0}\\right) \\right) \\times \\prod_{i \\in \\{0,3,\\sigma\\}} \\exp\\left( \\frac{(x_i - \\mu_i)^2 - (x_i' - \\mu_i)^2}{2\\sigma_i^2} \\right)\n$$\nwhere the product is over the parameters $t_0, t_3, \\sigma$. The acceptance probability is $\\alpha = \\min(1,R)$.\n\n## Part 2: Numerical Evaluation\n\nWe are given the current state $(\\theta, \\lambda)$ and proposed state $(\\theta', \\lambda')$, along with all necessary constants and model parameters. We proceed to calculate each term of the Hastings ratio $R$.\n\n**Given values:**\n-   Current state: $t_0 = -1800$, $t_3 = 14000$, $\\sigma = 0.5$, $\\lambda = 0.8$.\n-   Proposed state: $t'_0 = -1750$, $t'_3 = 15000$, $\\sigma' = 0.45$, $\\lambda' = 1.1$.\n-   Constants: $\\rho_0=0.16$, $\\alpha=22$, $\\beta=0.02$, $\\zeta=-0.001$, $E_0=-16$.\n-   Likelihood: $d_0=0$, $d_1=0.001$, $d_3=-0.00005$, $y_D=5$, $s=2$.\n-   Priors: $(\\mu_0, \\sigma_0) = (-1900, 300)$, $(\\mu_3, \\sigma_3) = (13000, 2000)$, $(\\mu_\\sigma, \\sigma_\\sigma) = (0.5, 0.05)$.\n-   Gamma prior: $a=2, b=1$.\n\n**1. Likelihood Ratio:**\n$y(\\theta) = d_1 t_0 + d_3 t_3 = 0.001(-1800) - 0.00005(14000) = -1.8 - 0.7 = -2.5$.\n$y(\\theta') = d_1 t'_0 + d_3 t'_3 = 0.001(-1750) - 0.00005(15000) = -1.75 - 0.75 = -2.5$.\nSince $y(\\theta) = y(\\theta')$, the likelihood ratio is $\\exp(0) = 1$.\n\n**2. Constraint Term Ratio:**\n$E/A(\\theta) = \\alpha + \\beta t_0 + \\zeta t_3 \\rho_0^\\sigma = 22 + 0.02(-1800) - 0.001(14000)(0.16)^{0.5} = 22 - 36 - 14(0.4) = -19.6$.\n$E/A(\\theta') = \\alpha + \\beta t'_0 + \\zeta t'_3 \\rho_0^{\\sigma'} = 22 + 0.02(-1750) - 0.001(15000)(0.16)^{0.45}$.\nWe calculate $(0.16)^{0.45} \\approx 0.4383845$.\n$E/A(\\theta') \\approx 22 - 35 - 15(0.4383845) = -13 - 6.5757675 = -19.5757675$.\nThe exponent for the ratio is $\\lambda(\\tfrac{E(\\theta)}{A} - E_0) - \\lambda'(\\tfrac{E(\\theta')}{A} - E_0)$.\n$0.8(-19.6 - (-16)) - 1.1(-19.5757675 - (-16)) = 0.8(-3.6) - 1.1(-3.5757675) = -2.88 + 3.93334425 = 1.05334425$.\nThe ratio is $\\exp(1.05334425) \\approx 2.86725$.\n\n**3. Parameter Priors Ratios:**\n-   **$t_0$ prior:** $\\exp\\left( \\frac{(-1800 - (-1900))^2 - (-1750 - (-1900))^2}{2(300)^2} \\right) = \\exp\\left( \\frac{100^2 - 150^2}{180000} \\right) = \\exp\\left( \\frac{-12500}{180000} \\right) = \\exp(-5/72) \\approx 0.93291$.\n-   **$t_3$ prior:** $\\exp\\left( \\frac{(14000 - 13000)^2 - (15000 - 13000)^2}{2(2000)^2} \\right) = \\exp\\left( \\frac{1000^2 - 2000^2}{8000000} \\right) = \\exp\\left( \\frac{-3000000}{8000000} \\right) = \\exp(-3/8) \\approx 0.68729$.\n-   **$\\sigma$ prior:** $\\exp\\left( \\frac{(0.5 - 0.5)^2 - (0.45 - 0.5)^2}{2(0.05)^2} \\right) = \\exp\\left( \\frac{0 - (-0.05)^2}{0.005} \\right) = \\exp\\left( \\frac{-0.0025}{0.005} \\right) = \\exp(-0.5) \\approx 0.60653$.\n\n**4. $\\lambda$ Prior and Proposal Ratio (Combined):**\nWith $a=2, b=1, \\lambda=0.8, \\lambda'=1.1$:\nRatio is $\\left(\\frac{1.1}{0.8}\\right)^{2} \\exp(-1(1.1 - 0.8)) = (1.375)^2 \\exp(-0.3) = 1.890625 \\times 0.740818 \\approx 1.40061$.\n\n**5. Total Hastings Ratio R:**\n$R$ is the product of all these terms:\n$R \\approx 1 \\times 2.86725 \\times 0.93291 \\times 0.68729 \\times 0.60653 \\times 1.40061$.\n(The last term is the combined $\\lambda$ term, the others are the likelihood, constraint, and $\\theta$ prior ratios).\n$R \\approx (2.86725) \\times (0.93291) \\times (0.68729) \\times (0.60653) \\times (1.40061) \\approx 1.56150$.\n\n**6. Acceptance Probability $\\alpha$:**\nThe acceptance probability is $\\alpha = \\min(1, R)$.\nSince $R \\approx 1.5615 > 1$, the acceptance probability is $\\alpha = 1$.\nRounding to four significant figures, the result is $1.000$.", "answer": "$$\\boxed{1.000}$$", "id": "3604490"}, {"introduction": "To truly master an algorithm, it is essential to understand not just how it works, but *why* each component is necessary. This practice [@problem_id:3604520] explores the Metropolis-Adjusted Langevin Algorithm (MALA) by analyzing the consequences of omitting its 'Metropolis-adjusted' step. By deriving the exact stationary distribution of this simplified, uncorrected algorithm, you will uncover the systematic biases it introduces and gain a deeper appreciation for the role of the Metropolis-Hastings acceptance test in ensuring statistical correctness.", "problem": "A single-parameter nuclear data calibration problem considers the logarithm of a thermal neutron capture cross section, denoted by $\\theta$, whose Bayesian posterior density (after assimilating count data in a linear-Gaussian forward model) is Gaussian with density\n$$\n\\pi(\\theta)\\;\\propto\\;\\exp\\!\\Big(-\\frac{(\\theta-\\mu)^{2}}{2\\sigma^{2}}\\Big),\n$$\nwhere $\\mu$ and $\\sigma^{2}$ are known hyperparameters obtained from the prior and the likelihood. To sample $\\pi(\\theta)$, you employ the Metropolis-adjusted Langevin algorithm (MALA) with proposal step size $h>0$, but the gradient used in the Langevin drift is computed from a fast surrogate model. Concretely, at each iteration you use an unbiased noisy surrogate gradient\n$$\n\\widetilde{\\nabla}\\log\\pi(\\theta)\\;=\\;\\nabla\\log\\pi(\\theta)\\;+\\;\\varepsilon,\\qquad \\varepsilon\\sim\\mathcal{N}(0,\\tau^{2}),\n$$\nwith $\\varepsilon$ independent across iterations and independent of the Gaussian proposal noise. In the small-step regime where the Metropolis-Hastings (MH) acceptance is near $1$, the MALA dynamics approach the unadjusted noisy Langevin discretization\n$$\n\\theta_{n+1}\\;=\\;\\theta_{n}\\;+\\;\\frac{h}{2}\\,\\widetilde{\\nabla}\\log\\pi(\\theta_{n})\\;+\\;\\sqrt{h}\\,\\xi_{n},\\qquad \\xi_{n}\\sim\\mathcal{N}(0,1),\n$$\nwith $\\xi_{n}$ independent of $\\varepsilon_{n}$ and of the past. For the Gaussian target above, $\\nabla\\log\\pi(\\theta) = -(\\theta-\\mu)/\\sigma^{2}$, so the update becomes a linear autoregression with two independent Gaussian noises.\n\nStarting only from the definitions above and standard properties of linear Gaussian recursions, do the following:\n\n1) Derive the exact stationary variance $V(h,\\sigma,\\tau)$ of the Markov chain $\\{\\theta_{n}\\}$ driven by the unadjusted noisy Langevin recursion. State clearly any stability condition you require on $h$ and $\\sigma^{2}$ for the variance to exist.\n\n2) Using your exact result, extract the leading-order small-$h$ bias in the stationary variance relative to the target variance $\\sigma^{2}$, that is, expand $V(h,\\sigma,\\tau)-\\sigma^{2}$ to first order in $h$.\n\n3) Briefly justify, without explicit computation, how one could modify the acceptance step to remove this bias when returning to a true Metropolis-adjusted algorithm: for example, by augmenting the state with the gradient-noise draw and using a pseudo-marginal acceptance, or by employing a delayed-acceptance MH scheme based on the surrogate followed by the exact posterior.\n\nProvide, as your final answer, the exact closed-form expression for $V(h,\\sigma,\\tau)$ simplified as much as possible. No numerical evaluation is required. Express your final answer as a single analytic expression. Do not include units.", "solution": "The problem asks for the analysis of a noisy, unadjusted Langevin algorithm for sampling a one-dimensional Gaussian target distribution. The tasks are to derive the stationary variance of the resulting Markov chain, analyze its bias for small step sizes, and conceptually explain how a Metropolis-Hastings acceptance step would correct this bias.\n\nFirst, we establish the explicit recursion for the Markov chain $\\{\\theta_n\\}$. The target posterior density is given by $\\pi(\\theta) \\propto \\exp\\left(-\\frac{(\\theta-\\mu)^{2}}{2\\sigma^{2}}\\right)$, which is a Gaussian distribution $\\mathcal{N}(\\mu, \\sigma^2)$ with mean $\\mu$ and variance $\\sigma^2$. The gradient of its logarithm is $\\nabla\\log\\pi(\\theta) = -\\frac{\\theta-\\mu}{\\sigma^2}$. The algorithm uses a noisy surrogate for this gradient, $\\widetilde{\\nabla}\\log\\pi(\\theta_n) = \\nabla\\log\\pi(\\theta_n) + \\varepsilon_n = -\\frac{\\theta_n-\\mu}{\\sigma^2} + \\varepsilon_n$, where $\\varepsilon_n$ is a random noise term with distribution $\\mathcal{N}(0, \\tau^2)$.\n\nSubstituting this into the unadjusted noisy Langevin recursion $\\theta_{n+1} = \\theta_{n} + \\frac{h}{2}\\widetilde{\\nabla}\\log\\pi(\\theta_{n}) + \\sqrt{h}\\xi_{n}$, where $\\xi_n \\sim \\mathcal{N}(0, 1)$, we obtain:\n$$\n\\theta_{n+1} = \\theta_n + \\frac{h}{2}\\left(-\\frac{\\theta_n-\\mu}{\\sigma^2} + \\varepsilon_n\\right) + \\sqrt{h}\\xi_n\n$$\nRearranging the terms, we can write this as a linear autoregression of order $1$, or an AR($1$) process:\n$$\n\\theta_{n+1} = \\left(1 - \\frac{h}{2\\sigma^2}\\right)\\theta_n + \\frac{h\\mu}{2\\sigma^2} + \\frac{h}{2}\\varepsilon_n + \\sqrt{h}\\xi_n\n$$\n\n1) To derive the stationary variance $V(h,\\sigma,\\tau) = \\text{Var}(\\theta_n)$ in the stationary state, we first establish the condition for the existence of a stationary distribution. For an AR($1$) process of the form $X_{n+1} = A X_n + C$, a stationary distribution exists if the magnitude of the autoregressive coefficient $A$ is less than $1$. In our case, $A = 1 - \\frac{h}{2\\sigma^2}$. Since $h>0$ and $\\sigma^2>0$, the condition $A  1$ is always satisfied. The stability condition is thus $|A|  1$, which requires $A  -1$:\n$$\n1 - \\frac{h}{2\\sigma^2}  -1 \\implies 2  \\frac{h}{2\\sigma^2} \\implies h  4\\sigma^2\n$$\nThis is the stability condition on $h$ and $\\sigma^2$ for the variance to exist.\n\nAssuming the chain is stationary, the moments are constant, i.e., $E[\\theta_{n+1}] = E[\\theta_n] = E_{st}$ and $\\text{Var}(\\theta_{n+1}) = \\text{Var}(\\theta_n) = V$. Taking the expectation of the recursion and noting that $E[\\varepsilon_n] = 0$ and $E[\\xi_n] = 0$:\n$$\nE_{st} = \\left(1 - \\frac{h}{2\\sigma^2}\\right)E_{st} + \\frac{h\\mu}{2\\sigma^2} \\implies \\frac{h}{2\\sigma^2}E_{st} = \\frac{h\\mu}{2\\sigma^2} \\implies E_{st} = \\mu\n$$\nThe stationary mean is $\\mu$, which matches the mean of the target distribution. The process is unbiased in its mean.\n\nNow we compute the variance. Since the noise terms $\\varepsilon_n$ and $\\xi_n$ are independent of each other and of the past state $\\theta_n$, the variance of the sum is the sum of the variances:\n$$\n\\text{Var}(\\theta_{n+1}) = \\text{Var}\\left[\\left(1 - \\frac{h}{2\\sigma^2}\\right)\\theta_n\\right] + \\text{Var}\\left[\\frac{h}{2}\\varepsilon_n\\right] + \\text{Var}\\left[\\sqrt{h}\\xi_n\\right]\n$$\nSubstituting $V$ for $\\text{Var}(\\theta_n)$, $\\tau^2$ for $\\text{Var}(\\varepsilon_n)$, and $1$ for $\\text{Var}(\\xi_n)$:\n$$\nV = \\left(1 - \\frac{h}{2\\sigma^2}\\right)^2 V + \\left(\\frac{h}{2}\\right)^2 \\tau^2 + (\\sqrt{h})^2 (1)\n$$\n$$\nV = \\left(1 - \\frac{h}{\\sigma^2} + \\frac{h^2}{4\\sigma^4}\\right) V + \\frac{h^2\\tau^2}{4} + h\n$$\nWe solve for $V$:\n$$\nV \\left[1 - \\left(1 - \\frac{h}{\\sigma^2} + \\frac{h^2}{4\\sigma^4}\\right)\\right] = h + \\frac{h^2\\tau^2}{4}\n$$\n$$\nV \\left[\\frac{h}{\\sigma^2} - \\frac{h^2}{4\\sigma^4}\\right] = h\\left(1 + \\frac{h\\tau^2}{4}\\right)\n$$\n$$\nV \\left[\\frac{h(4\\sigma^2 - h)}{4\\sigma^4}\\right] = h\\left(\\frac{4+h\\tau^2}{4}\\right)\n$$\nCanceling a factor of $h$ (since $h>0$) and rearranging gives the exact stationary variance:\n$$\nV(h,\\sigma,\\tau) = \\frac{4\\sigma^4}{4\\sigma^2 - h} \\cdot \\frac{4+h\\tau^2}{4} = \\frac{\\sigma^4(4+h\\tau^2)}{4\\sigma^2 - h}\n$$\n\n2) To find the leading-order small-$h$ bias in the variance, we expand $V(h,\\sigma,\\tau) - \\sigma^2$ to first order in $h$.\n$$\nV(h,\\sigma,\\tau) - \\sigma^2 = \\frac{4\\sigma^4 + h\\sigma^4\\tau^2}{4\\sigma^2 - h} - \\sigma^2 = \\frac{4\\sigma^4 + h\\sigma^4\\tau^2 - \\sigma^2(4\\sigma^2 - h)}{4\\sigma^2 - h}\n$$\n$$\nV(h,\\sigma,\\tau) - \\sigma^2 = \\frac{4\\sigma^4 + h\\sigma^4\\tau^2 - 4\\sigma^4 + h\\sigma^2}{4\\sigma^2 - h} = \\frac{h\\sigma^2 + h\\sigma^4\\tau^2}{4\\sigma^2 - h} = \\frac{h\\sigma^2(1+\\sigma^2\\tau^2)}{4\\sigma^2 - h}\n$$\nFor small $h$, the denominator $4\\sigma^2 - h$ can be approximated by $4\\sigma^2$. The expression is of order $h$, so we can make this substitution to get the leading-order term:\n$$\nV(h,\\sigma,\\tau) - \\sigma^2 \\approx \\frac{h\\sigma^2(1+\\sigma^2\\tau^2)}{4\\sigma^2} = \\frac{h}{4}(1+\\sigma^2\\tau^2)\n$$\nThus, the leading-order bias is $\\frac{h}{4}(1+\\sigma^2\\tau^2)$. This bias comprises two parts: a term $\\frac{h}{4}$ from the Euler-Maruyama discretization of the Langevin SDE, and a term $\\frac{h\\sigma^2\\tau^2}{4}$ arising from the noisy gradient.\n\n3) The unadjusted Langevin algorithm is an approximation that does not, in general, sample from the exact target distribution $\\pi(\\theta)$. This leads to the systematic bias in the stationary variance derived above. The Metropolis-Hastings (MH) algorithm provides a general framework to construct a Markov chain that samples from any desired target distribution $\\pi(\\theta)$, provided one can evaluate $\\pi(\\theta)$ up to a constant. The core of the method is the acceptance step, which corrects for any inaccuracies in the proposal mechanism. The MH acceptance probability ensures that the resulting Markov chain satisfies the detailed balance condition with respect to $\\pi(\\theta)$. This condition, $\\pi(x)P(y|x) = \\pi(y)P(x|y)$ where $P$ is the full transition kernel, is a sufficient condition to guarantee that $\\pi(\\theta)$ is the unique stationary distribution of the chain.\n\nBy reintroducing the acceptance step to form a proper MALA algorithm, the dynamics are forced to satisfy detailed balance with respect to the true posterior $\\pi(\\theta)$. Consequently, the stationary distribution of the chain becomes exactly $\\pi(\\theta)$, and all its moments, including the variance, will match those of $\\pi(\\theta)$. The stationary variance will thus be exactly $\\sigma^2$, and the bias is removed. Implementing this with noisy gradients requires care. A pseudo-marginal approach augments the state space to $(\\theta, \\varepsilon)$ and defines an acceptance probability that correctly accounts for the randomess in the proposal kernel, ensuring the marginal distribution for $\\theta$ is the exact $\\pi(\\theta)$. A delayed-acceptance scheme uses the fast surrogate-based proposal but filters it with an acceptance probability based on the true posterior, which also ensures the chain targets the correct distribution, often with computational savings. In either case, the use of an MH acceptance test based on the true target $\\pi(\\theta)$ eliminates the bias inherent in the unadjusted dynamics.", "answer": "$$\n\\boxed{\\frac{\\sigma^{4}(4 + h\\tau^{2})}{4\\sigma^{2} - h}}\n$$", "id": "3604520"}, {"introduction": "A central task in science is comparing competing theories, which often correspond to statistical models of different complexity and dimension. This advanced practice [@problem_id:3604527] introduces the Reversible Jump Metropolis-Hastings (RJMCMC) algorithm, a powerful extension that allows a Markov chain to explore both the parameters within a model and the model space itself. By deriving the acceptance probability for a 'birth' move that adds a new parameter, you will engage with the key ingredients of trans-dimensional MCMC, including dimension-matching maps, proposal ratios, and Jacobians.", "problem": "In ab initio nuclear theory, one often compares a two-nucleon (2N) interaction model $M_{1}$ against an augmented model $M_{2}$ that includes a leading three-nucleon (3N) force in addition to the 2N force. Consider a reversible jump Metropolis-Hastings (RJ-MH) scheme to sample from the joint posterior over model index $M \\in \\{M_{1},M_{2}\\}$ and model parameters conditioned on data $\\mathcal{D}$. The target distribution is the joint posterior implied by Bayes’ theorem, namely $p(\\theta_{k},M_{k}\\mid \\mathcal{D}) \\propto p(\\mathcal{D}\\mid \\theta_{k},M_{k})\\,p(\\theta_{k}\\mid M_{k})\\,p(M_{k})$, where $k\\in\\{1,2\\}$.\n\nAssume the following scientifically motivated setting tied to low-energy nuclear physics calibration:\n- The 2N model $M_{1}$ has parameter vector $\\theta_{1}=(C_{S},C_{T})\\in \\mathbb{R}^{2}$. The 2N+3N model $M_{2}$ has parameter vector $\\theta_{2}=(C_{S},C_{T},c_{E})\\in \\mathbb{R}^{3}$, where $c_{E}\\in \\mathbb{R}$ is a 3N low-energy constant.\n- The 2N parameters share the same prior under both models: $p(\\theta_{1}\\mid M_{1})=p(\\theta_{1}\\mid M_{2})$ (hence they cancel in ratios when held fixed).\n- The prior for the 3N coupling is Gaussian: $p(c_{E}\\mid M_{2})=\\mathcal{N}(0,\\tau^{2})$ with $\\tau=1$.\n- The model priors are $p(M_{1})=0.6$ and $p(M_{2})=0.4$.\n- A scalar discrepancy statistic $y$ summarizing the triton binding-energy residual (experiment minus theory) is observed with value $y=0$, and the data model is Gaussian with known error standard deviation $\\sigma=0.3$:\n  - Under $M_{1}$, $y\\mid \\theta_{1},M_{1}\\sim \\mathcal{N}(\\mu_{1},\\sigma^{2})$ with $\\mu_{1}=b$ and $b=0.5$.\n  - Under $M_{2}$, $y\\mid \\theta_{2},M_{2}\\sim \\mathcal{N}(\\mu_{2},\\sigma^{2})$ with $\\mu_{2}=b+a\\,c_{E}$, where $a=-2.0$ represents the linearized sensitivity of the residual to $c_{E}$ near the current calibration point.\n\nConsider a birth move from $(M_{1},\\theta_{1})$ to $(M_{2},\\theta_{2})$ defined by the deterministic dimension-matching map $\\mathcal{T}:(\\theta_{1},u)\\mapsto \\theta_{2}=(\\theta_{1},c_{E})$ with $c_{E}=u$. The auxiliary variable is proposed as $u\\sim q_{12}(u\\mid \\theta_{1})=\\mathcal{N}(m,s^{2})$ with $m=-0.1$ and $s=0.5$. The reverse death move from $(M_{2},\\theta_{2})$ to $(M_{1},\\theta_{1})$ drops $c_{E}$ deterministically and has no reverse auxiliary draw, so $q_{21}(\\cdot\\mid \\theta_{2})$ is the constant $1$ on the null set of reverse random variables. The Jacobian of the transformation is $J=\\left|\\partial \\theta_{2}/\\partial(\\theta_{1},u)\\right|=1$. Suppose the move-type selection probabilities are $r_{12}=r_{21}=0.5$.\n\nYou are currently at model $M_{1}$ with some fixed $\\theta_{1}$ and a particular forward proposal draw realized as $u^{\\star}=-0.3$, hence $c_{E}^{\\star}=-0.3$ and $\\theta_{2}^{\\star}=(\\theta_{1},c_{E}^{\\star})$. Using the definitions of detailed balance for Markov chains and Bayes’ theorem as the foundational principles, derive the RJ-MH acceptance probability $\\alpha_{\\text{birth}}$ for this specific birth move and evaluate it numerically. Express your final answer as a decimal number rounded to four significant figures. Do not include units.", "solution": "The problem requires the calculation of the acceptance probability for a \"birth\" move in a reversible jump Metropolis-Hastings (RJ-MH) framework. This move attempts a transition from a state in model $M_1$ to a state in a higher-dimensional model $M_2$.\n\nThe state in model $M_k$ is specified by the model index $k$ and the parameter vector $\\theta_k$. The target distribution is the joint posterior $p(\\theta_k, M_k \\mid \\mathcal{D})$, where the data $\\mathcal{D}$ consists of a single observation $y=0$. The current state is $x = (M_1, \\theta_1)$. The proposed state is $x' = (M_2, \\theta_2^\\star)$.\n\nThe general formula for the RJ-MH acceptance probability $\\alpha(x \\to x')$ is:\n$$ \\alpha(x \\to x') = \\min(1, R) $$\nwhere $R$ is the acceptance ratio. For the specific birth move from state $(M_1, \\theta_1)$ to $(M_2, \\theta_2^\\star)$, the ratio $R$ is given by:\n$$ R = \\underbrace{\\frac{p(M_2, \\theta_2^\\star \\mid y)}{p(M_1, \\theta_1 \\mid y)}}_{\\text{Posterior Ratio}} \\times \\underbrace{\\frac{r_{21} \\, q_{21}(\\cdot \\mid \\theta_2^\\star)}{r_{12} \\, q_{12}(u^\\star \\mid \\theta_1)}}_{\\text{Proposal Ratio}} \\times \\underbrace{|J|}_{\\text{Jacobian}} $$\nHere, $u^\\star$ is the specific draw from the auxiliary variable distribution $q_{12}$ used to generate the new parameters in $M_2$. In this problem, the proposal maps $(\\theta_1, u)$ to $\\theta_2 = (\\theta_1, c_E)$ via $c_E = u$, so the proposed parameter is $c_E^\\star = u^\\star=-0.3$ and $\\theta_2^\\star = (\\theta_1, c_E^\\star)$. The reverse move is deterministic, so there are no reverse auxiliary variables, and its proposal density $q_{21}$ is $1$. The Jacobian $|J|$ of the dimension-matching map is given as $1$.\n\nLet's analyze each term of the ratio $R$.\n\nFirst, the Posterior Ratio, using Bayes' theorem $p(\\theta_k, M_k \\mid y) \\propto p(y \\mid \\theta_k, M_k) p(\\theta_k \\mid M_k) p(M_k)$:\n$$ \\frac{p(M_2, \\theta_2^\\star \\mid y)}{p(M_1, \\theta_1 \\mid y)} = \\frac{p(y \\mid \\theta_2^\\star, M_2) \\, p(\\theta_2^\\star \\mid M_2) \\, p(M_2)}{p(y \\mid \\theta_1, M_1) \\, p(\\theta_1 \\mid M_1) \\, p(M_1)} $$\nThe parameter vector $\\theta_2^\\star$ is $(\\theta_1, c_E^\\star)$. We can factor the prior $p(\\theta_2^\\star \\mid M_2)$ as $p(c_E^\\star \\mid \\theta_1, M_2) p(\\theta_1 \\mid M_2)$. Assuming the prior for $c_E$ is independent of $\\theta_1$, this simplifies to $p(c_E^\\star \\mid M_2) p(\\theta_1 \\mid M_2)$. The problem states that the prior for the shared parameters $\\theta_1$ is the same under both models, i.e., $p(\\theta_1 \\mid M_1) = p(\\theta_1 \\mid M_2)$. These terms thus cancel. The posterior ratio simplifies to:\n$$ \\frac{p(M_2, \\theta_2^\\star \\mid y)}{p(M_1, \\theta_1 \\mid y)} = \\frac{p(y \\mid \\theta_2^\\star, M_2) \\, p(c_E^\\star \\mid M_2) \\, p(M_2)}{p(y \\mid \\theta_1, M_1) \\, p(M_1)} $$\n\nNow, let's substitute the given distributions:\nThe likelihood ratio is:\n$$ \\frac{p(y \\mid \\theta_2^\\star, M_2)}{p(y \\mid \\theta_1, M_1)} = \\frac{\\mathcal{N}(y \\mid b+ac_E^\\star, \\sigma^2)}{\\mathcal{N}(y \\mid b, \\sigma^2)} = \\frac{\\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y - (b+ac_E^\\star))^2}{2\\sigma^2}\\right)}{\\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y-b)^2}{2\\sigma^2}\\right)} $$\nWith $y=0$, this becomes:\n$$ \\exp\\left(\\frac{b^2 - (b+ac_E^\\star)^2}{2\\sigma^2}\\right) $$\nThe parameter prior for $c_E$ is $p(c_E^\\star \\mid M_2) = \\mathcal{N}(c_E^\\star \\mid 0, \\tau^2) = \\frac{1}{\\sqrt{2\\pi\\tau^2}} \\exp\\left(-\\frac{(c_E^\\star)^2}{2\\tau^2}\\right)$.\nThe model prior ratio is $\\frac{p(M_2)}{p(M_1)}$.\n\nSecond, the Proposal Ratio:\n$$ \\frac{r_{21} \\, q_{21}(\\cdot \\mid \\theta_2^\\star)}{r_{12} \\, q_{12}(u^\\star \\mid \\theta_1)} $$\nGiven $r_{12}=r_{21}=0.5$, this ratio is $1$. The reverse proposal $q_{21}=1$. The forward proposal for the auxiliary variable is $q_{12}(u^\\star \\mid \\theta_1) = \\mathcal{N}(u^\\star \\mid m, s^2) = \\frac{1}{\\sqrt{2\\pi s^2}} \\exp\\left(-\\frac{(u^\\star - m)^2}{2s^2}\\right)$.\nSo the proposal ratio term evaluates to $\\frac{1}{q_{12}(u^\\star \\mid \\theta_1)}$.\n\nThird, the Jacobian is given as $|J|=1$.\n\nCombining all terms, and recalling $c_E^\\star=u^\\star$, the full acceptance ratio $R$ is:\n$$ R = \\frac{p(y \\mid \\theta_2^\\star, M_2)}{p(y \\mid \\theta_1, M_1)} \\times \\frac{p(M_2)}{p(M_1)} \\times p(c_E^\\star \\mid M_2) \\times \\frac{1}{q_{12}(u^\\star \\mid \\theta_1)} \\times 1 $$\nSubstituting the probability density functions:\n$$ R = \\exp\\left(\\frac{b^2 - (b+au^\\star)^2}{2\\sigma^2}\\right) \\times \\frac{p(M_2)}{p(M_1)} \\times \\frac{\\frac{1}{\\sqrt{2\\pi\\tau^2}} \\exp\\left(-\\frac{(u^\\star)^2}{2\\tau^2}\\right)}{\\frac{1}{\\sqrt{2\\pi s^2}} \\exp\\left(-\\frac{(u^\\star-m)^2}{2s^2}\\right)} $$\n$$ R = \\exp\\left(\\frac{b^2 - (b+au^\\star)^2}{2\\sigma^2}\\right) \\times \\frac{p(M_2)}{p(M_1)} \\times \\frac{s}{\\tau} \\exp\\left(\\frac{(u^\\star-m)^2}{2s^2} - \\frac{(u^\\star)^2}{2\\tau^2}\\right) $$\nNow, we substitute the numerical values provided:\n$b=0.5$, $a=-2.0$, $\\sigma=0.3$, $u^\\star=-0.3$, $p(M_1)=0.6$, $p(M_2)=0.4$, $\\tau=1$, $m=-0.1$, $s=0.5$.\n\n1.  Likelihood-related term exponent:\n    The mean for $M_2$ is $\\mu_2 = b + a u^\\star = 0.5 + (-2.0)(-0.3) = 0.5 + 0.6 = 1.1$.\n    The exponent is $\\frac{b^2 - \\mu_2^2}{2\\sigma^2} = \\frac{(0.5)^2 - (1.1)^2}{2(0.3)^2} = \\frac{0.25 - 1.21}{2(0.09)} = \\frac{-0.96}{0.18} = -\\frac{16}{3}$.\n\n2.  Model prior ratio:\n    $\\frac{p(M_2)}{p(M_1)} = \\frac{0.4}{0.6} = \\frac{2}{3}$.\n\n3.  Prior/Proposal term:\n    The prefactor is $\\frac{s}{\\tau} = \\frac{0.5}{1} = 0.5$.\n    The exponent is $\\frac{(u^\\star-m)^2}{2s^2} - \\frac{(u^\\star)^2}{2\\tau^2} = \\frac{(-0.3 - (-0.1))^2}{2(0.5)^2} - \\frac{(-0.3)^2}{2(1)^2} = \\frac{(-0.2)^2}{0.5} - \\frac{0.09}{2} = \\frac{0.04}{0.5} - 0.045 = 0.08 - 0.045 = 0.035$.\n\nCombining these pieces into the expression for $R$:\n$$ R = \\exp\\left(-\\frac{16}{3}\\right) \\times \\frac{2}{3} \\times 0.5 \\times \\exp(0.035) $$\n$$ R = \\exp\\left(-\\frac{16}{3} + 0.035\\right) \\times \\left(\\frac{2}{3} \\times \\frac{1}{2}\\right) $$\n$$ R = \\exp\\left(-5.3333... + 0.035\\right) \\times \\frac{1}{3} $$\n$$ R = \\exp\\left(-5.298333...\\right) \\times \\frac{1}{3} $$\n$$ R \\approx (0.005000406) \\times \\frac{1}{3} \\approx 0.001666802 $$\nThe acceptance probability $\\alpha_{\\text{birth}} = \\min(1, R)$. Since $R  1$, we have $\\alpha_{\\text{birth}} = R$.\nRounding the result to four significant figures gives $0.001667$.", "answer": "$$\\boxed{0.001667}$$", "id": "3604527"}]}