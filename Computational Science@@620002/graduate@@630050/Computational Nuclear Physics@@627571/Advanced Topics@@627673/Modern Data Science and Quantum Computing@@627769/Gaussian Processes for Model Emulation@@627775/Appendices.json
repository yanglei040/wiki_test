{"hands_on_practices": [{"introduction": "In many areas of physics, computational models can efficiently provide not only a primary output but also its gradients with respect to input parameters, often through adjoint sensitivity methods. Because differentiation is a linear operation, a Gaussian process (GP) can naturally assimilate this derivative information. This practice demonstrates how to construct the joint covariance structure for function values and their gradients, enabling you to build a more data-efficient emulator by conditioning on both types of observations from a nuclear transport code [@problem_id:3561163].", "problem": "In computational nuclear physics, adjoint-based sensitivities from deterministic neutron transport provide gradient information of a reactor response with respect to input parameters. Consider emulating a scalar nuclear response by a Gaussian process (GP), specifically the infinite-medium effective multiplication factor as a function of a single scalar input. Let the scalar response be modeled as a zero-mean Gaussian process (GP) with covariance kernel $k(x,x')$, where $x$ is a single scalar input parameter defined as $x = \\ln \\Sigma_{a}$, with $\\Sigma_{a}$ the macroscopic absorption cross section of a homogeneous mixture near a nominal state. Assume $k(x,x')$ is twice continuously differentiable.\n\n1) Starting from the definition of a Gaussian process and the fact that linear functionals applied to a GP yield another GP, derive the joint covariance structure between function values and gradients. Specifically, derive expressions for $\\mathrm{Cov}(f(x), f(x'))$, $\\mathrm{Cov}(f(x), \\partial f/\\partial x'(x'))$, and $\\mathrm{Cov}(\\partial f/\\partial x(x), \\partial f/\\partial x'(x'))$ in terms of partial derivatives of the kernel $k(x,x')$. Clearly justify each step from first principles.\n\n2) In adjoint sensitivity analysis for reactor physics, one obtains observations of $\\partial f/\\partial x$ at selected $x$ values. Explain how to incorporate such gradient observations into the GP by constructing the joint Gaussian prior over the vector of observations comprised of both function values and gradients. Include independent observation noises for function and gradient channels with variances $\\sigma_{n,f}^{2}$ and $\\sigma_{n,g}^{2}$, respectively.\n\n3) Consider the squared-exponential kernel\n$$\nk(x,x') \\;=\\; \\sigma_{f}^{2}\\,\\exp\\!\\left(-\\frac{(x-x')^{2}}{2\\ell^{2}}\\right),\n$$\nwith hyperparameters $\\sigma_{f} = 0.2$ and $\\ell = 0.6$. You are given one function observation from a neutron transport calculation,\n$$\nf(x_{1}) \\text{ at } x_{1} = 0 \\text{ with observed value } y_{f} = 1.05,\n$$\nand one gradient observation from an adjoint sensitivity calculation,\n$$\n\\left.\\frac{\\partial f}{\\partial x}\\right|_{x_{2}} \\text{ at } x_{2} = 0.5 \\text{ with observed value } y_{g} = -0.10.\n$$\nAssume independent Gaussian observation noises with $\\sigma_{n,f} = 0.01$ for the function observation and $\\sigma_{n,g} = 0.02$ for the gradient observation. Using your results from parts (1)–(2), form the $2\\times 2$ joint covariance matrix of the observations, the $2\\times 1$ cross-covariance vector between $f(x_{*})$ at the test input $x_{*} = 0.2$ and the observation vector, and then derive the posterior mean expression for $f(x_{*})$ given the mixed observations.\n\n4) Evaluate the posterior predictive mean at $x_{*} = 0.2$ numerically for the given hyperparameters and observations. Express the final predicted multiplication factor as a dimensionless number, and round your answer to four significant figures.", "solution": "The problem is assessed to be valid. It is scientifically grounded in the established theory of Gaussian processes and its application to surrogate modeling, a standard technique in computational science. The problem is well-posed, objective, self-contained, and all data and conditions provided are consistent and sufficient for deriving a unique solution.\n\n### Part 1: Derivation of the Joint Covariance Structure\n\nA scalar function $f(x)$ is a Gaussian process (GP) if, for any collection of points $x_1, \\dots, x_n$, the random vector $(f(x_1), \\dots, f(x_n))$ has a multivariate Gaussian distribution. A GP is fully specified by its mean function $m(x) = \\mathbb{E}[f(x)]$ and its covariance function (or kernel) $k(x,x') = \\mathrm{Cov}(f(x), f(x'))$. The problem states that the response is modeled as a zero-mean GP, so $m(x) = 0$ for all $x$.\n\nA crucial property of GPs is that they are closed under linear operations. The derivative operator $\\frac{\\partial}{\\partial x}$ is a linear operator. Therefore, if $f(x)$ is a GP, then its derivative $\\frac{\\partial f}{\\partial x}(x)$ is also a GP. The joint distribution of $f(x)$ and its derivatives is also Gaussian. We can derive the covariance structure by utilizing the properties of the covariance and the assumption that differentiation and expectation operators can be interchanged. This is permissible because the kernel $k(x,x')$ is assumed to be twice continuously differentiable, which implies that the sample paths of the GP are mean-square differentiable.\n\nGiven the zero-mean assumption, $\\mathbb{E}[f(x)] = 0$ and, by linearity of expectation, $\\mathbb{E}[\\frac{\\partial f}{\\partial x}(x)] = \\frac{\\partial}{\\partial x}\\mathbb{E}[f(x)] = 0$.\n\n1.  **Covariance between function values, $\\mathrm{Cov}(f(x), f(x'))$**:\n    By the definition of the covariance function for a GP, this is simply the kernel itself.\n    $$ \\mathrm{Cov}(f(x), f(x')) = \\mathbb{E}[(f(x) - \\mathbb{E}[f(x)])(f(x') - \\mathbb{E}[f(x')])] = \\mathbb{E}[f(x)f(x')] = k(x,x') $$\n\n2.  **Covariance between a function value and a gradient, $\\mathrm{Cov}(f(x), \\frac{\\partial f}{\\partial x'}(x'))$**:\n    We use the definition of covariance and interchange differentiation and expectation.\n    $$ \\mathrm{Cov}\\left(f(x), \\frac{\\partial f}{\\partial x'}(x')\\right) = \\mathbb{E}\\left[f(x) \\frac{\\partial f}{\\partial x'}(x')\\right] = \\frac{\\partial}{\\partial x'} \\mathbb{E}[f(x)f(x')] $$\n    Since $\\mathbb{E}[f(x)f(x')] = k(x,x')$, we have:\n    $$ \\mathrm{Cov}\\left(f(x), \\frac{\\partial f}{\\partial x'}(x')\\right) = \\frac{\\partial k(x,x')}{\\partial x'} $$\n    By symmetry of the covariance operator, $\\mathrm{Cov}(\\frac{\\partial f}{\\partial x}(x), f(x')) = \\mathrm{Cov}(f(x'), \\frac{\\partial f}{\\partial x}(x))$. Based on the previous result, this is $\\frac{\\partial k(x',x)}{\\partial x}$. Since for most common kernels $k(x,x')$ is a symmetric function of its arguments, its partial derivatives will also have specific symmetries. For a kernel that is a function of $(x-x')$, we have $\\frac{\\partial k(x',x)}{\\partial x} = \\frac{\\partial k(x,x')}{\\partial x}$.\n\n3.  **Covariance between gradients, $\\mathrm{Cov}(\\frac{\\partial f}{\\partial x}(x), \\frac{\\partial f}{\\partial x'}(x'))$**:\n    We apply the same procedure, differentiating with respect to both arguments.\n    $$ \\mathrm{Cov}\\left(\\frac{\\partial f}{\\partial x}(x), \\frac{\\partial f}{\\partial x'}(x')\\right) = \\mathbb{E}\\left[\\frac{\\partial f}{\\partial x}(x) \\frac{\\partial f}{\\partial x'}(x')\\right] = \\frac{\\partial}{\\partial x} \\mathbb{E}\\left[f(x) \\frac{\\partial f}{\\partial x'}(x')\\right] $$\n    Using the result from the previous step:\n    $$ = \\frac{\\partial}{\\partial x} \\left( \\frac{\\partial k(x,x')}{\\partial x'} \\right) = \\frac{\\partial^2 k(x,x')}{\\partial x \\partial x'} $$\n\n### Part 2: Incorporating Gradient Observations and Noise\n\nWe are given observations of both function values and function gradients. Let there be $N_f$ function observations $\\{y_{f,i}\\}_{i=1}^{N_f}$ at inputs $\\{x_{f,i}\\}_{i=1}^{N_f}$ and $N_g$ gradient observations $\\{y_{g,j}\\}_{j=1}^{N_g}$ at inputs $\\{x_{g,j}\\}_{j=1}^{N_g}$. These observations are modeled as the true latent values corrupted by independent Gaussian noise.\n$$ y_{f,i} = f(x_{f,i}) + \\epsilon_{f,i}, \\quad \\epsilon_{f,i} \\sim \\mathcal{N}(0, \\sigma_{n,f}^2) $$\n$$ y_{g,j} = \\left.\\frac{\\partial f}{\\partial x}\\right|_{x=x_{g,j}} + \\epsilon_{g,j}, \\quad \\epsilon_{g,j} \\sim \\mathcal{N}(0, \\sigma_{n,g}^2) $$\nLet us define an augmented observation vector $\\mathbf{y}$ and a corresponding augmented latent function vector $\\mathbf{f}$. For the problem's specific case of one function and one gradient observation at inputs $x_1$ and $x_2$ respectively, we have:\n$$ \\mathbf{y} = \\begin{pmatrix} y_f \\\\ y_g \\end{pmatrix}, \\quad \\mathbf{f} = \\begin{pmatrix} f(x_1) \\\\ \\left.\\frac{\\partial f}{\\partial x}\\right|_{x=x_2} \\end{pmatrix} $$\nThe vector of latent values $\\mathbf{f}$ is drawn from a zero-mean multivariate Gaussian distribution with a covariance matrix $K$ constructed using the results from Part 1.\n$$ \\mathbf{f} \\sim \\mathcal{N}(\\mathbf{0}, K) $$\nwhere\n$$ K = \\begin{pmatrix}\n\\mathrm{Cov}(f(x_1), f(x_1)) & \\mathrm{Cov}(f(x_1), \\frac{\\partial f}{\\partial x}(x_2)) \\\\\n\\mathrm{Cov}(\\frac{\\partial f}{\\partial x}(x_2), f(x_1)) & \\mathrm{Cov}(\\frac{\\partial f}{\\partial x}(x_2), \\frac{\\partial f}{\\partial x}(x_2))\n\\end{pmatrix} = \\begin{pmatrix}\nk(x_1, x_1) & \\frac{\\partial k(x_1, x')}{\\partial x'}\\big|_{x'=x_2} \\\\\n\\frac{\\partial k(x, x_1)}{\\partial x}\\big|_{x=x_2} & \\frac{\\partial^2 k(x, x')}{\\partial x \\partial x'}\\big|_{x=x_2, x'=x_2}\n\\end{pmatrix} $$\nThe observation vector $\\mathbf{y}$ is the sum of the latent vector $\\mathbf{f}$ and a noise vector $\\boldsymbol{\\epsilon} = (\\epsilon_f, \\epsilon_g)^T$. The noise is assumed independent of the process $f$ and has a covariance matrix $\\Sigma_{noise}$. Due to the independence of the noise terms, this matrix is diagonal:\n$$ \\Sigma_{noise} = \\begin{pmatrix} \\sigma_{n,f}^2 & 0 \\\\ 0 & \\sigma_{n,g}^2 \\end{pmatrix} $$\nThe sum of two independent Gaussian vectors is Gaussian. Thus, the joint Gaussian prior over the vector of observations $\\mathbf{y}$ is:\n$$ \\mathbf{y} \\sim \\mathcal{N}(\\mathbf{0}, K_y), \\quad \\text{where} \\quad K_y = K + \\Sigma_{noise} $$\n\n### Part 3: Formulation for the Specific Problem\n\nGiven the squared-exponential kernel:\n$$ k(x,x') = \\sigma_{f}^{2}\\,\\exp\\left(-\\frac{(x-x')^{2}}{2\\ell^{2}}\\right) $$\nThe required partial derivatives are:\n$$ \\frac{\\partial k(x,x')}{\\partial x'} = \\sigma_{f}^{2}\\,\\exp\\left(-\\frac{(x-x')^{2}}{2\\ell^{2}}\\right) \\left(\\frac{x-x'}{\\ell^2}\\right) $$\n$$ \\frac{\\partial k(x,x')}{\\partial x} = \\sigma_{f}^{2}\\,\\exp\\left(-\\frac{(x-x')^{2}}{2\\ell^{2}}\\right) \\left(-\\frac{x-x'}{\\ell^2}\\right) $$\n$$ \\frac{\\partial^2 k(x,x')}{\\partial x \\partial x'} = \\frac{\\sigma_f^2}{\\ell^2} \\exp\\left(-\\frac{(x-x')^{2}}{2\\ell^{2}}\\right) \\left(1 - \\frac{(x-x')^2}{\\ell^2}\\right) $$\nThe observation points are $x_1 = 0$ (function) and $x_2=0.5$ (gradient). The test point is $x_*=0.2$.\n\nThe $2\\times 2$ joint covariance matrix of the observations, $K_y = K + \\Sigma_{noise}$, is constructed as follows:\n$K_{11} = k(x_1, x_1) = k(0,0) = \\sigma_f^2$\n$K_{12} = \\frac{\\partial k(x_1, x')}{\\partial x'}\\big|_{x'=x_2} = \\frac{\\partial k(0, x')}{\\partial x'}\\big|_{x'=0.5} = \\frac{\\sigma_f^2}{\\ell^2}(0-0.5) \\exp\\left(-\\frac{(0-0.5)^2}{2\\ell^2}\\right)$\n$K_{21} = \\frac{\\partial k(x, x_1)}{\\partial x}\\big|_{x=x_2} = \\frac{\\partial k(x, 0)}{\\partial x}\\big|_{x=0.5} = \\frac{\\sigma_f^2}{\\ell^2}(-(0.5-0)) \\exp\\left(-\\frac{(0.5-0)^2}{2\\ell^2}\\right) = K_{12}$\n$K_{22} = \\frac{\\partial^2 k(x, x')}{\\partial x \\partial x'}\\big|_{x=x_2, x'=x_2} = \\frac{\\partial^2 k(x, x')}{\\partial x \\partial x'}\\big|_{x=0.5, x'=0.5} = \\frac{\\sigma_f^2}{\\ell^2} (1-0) = \\frac{\\sigma_f^2}{\\ell^2}$\nSo, the full observation covariance matrix is:\n$$ K_y = \\begin{pmatrix} \\sigma_f^2 + \\sigma_{n,f}^2 & \\frac{-\\sigma_f^2}{2\\ell^2}\\exp\\left(\\frac{-1}{8\\ell^2}\\right) \\\\ \\frac{-\\sigma_f^2}{2\\ell^2}\\exp\\left(\\frac{-1}{8\\ell^2}\\right) & \\frac{\\sigma_f^2}{\\ell^2} + \\sigma_{n,g}^2 \\end{pmatrix} $$\n\nThe $2\\times 1$ cross-covariance vector between $f(x_*)$ and the observation vector $\\mathbf{f} = (f(x_1), \\partial f/\\partial x(x_2))^T$ is denoted by $k_*$.\n$$ k_* = \\begin{pmatrix} \\mathrm{Cov}(f(x_*), f(x_1)) \\\\ \\mathrm{Cov}(f(x_*), \\frac{\\partial f}{\\partial x}(x_2)) \\end{pmatrix} = \\begin{pmatrix} k(x_*, x_1) \\\\ \\frac{\\partial k(x_*, x')}{\\partial x'}\\big|_{x'=x_2} \\end{pmatrix} $$\n$$ k_* = \\begin{pmatrix} \\sigma_f^2 \\exp\\left(-\\frac{(x_*-x_1)^2}{2\\ell^2}\\right) \\\\ \\frac{\\sigma_f^2}{\\ell^2}(x_*-x_2)\\exp\\left(-\\frac{(x_*-x_2)^2}{2\\ell^2}\\right) \\end{pmatrix} = \\begin{pmatrix} \\sigma_f^2 \\exp\\left(-\\frac{(0.2-0)^2}{2\\ell^2}\\right) \\\\ \\frac{\\sigma_f^2}{\\ell^2}(0.2-0.5)\\exp\\left(-\\frac{(0.2-0.5)^2}{2\\ell^2}\\right) \\end{pmatrix} $$\n\nThe posterior mean for $f(x_*)$ given the observation vector $\\mathbf{y}_{obs} = (y_f, y_g)^T$ is given by the standard formula for conditional Gaussian distributions. With a zero prior mean, this is:\n$$ \\bar{f}(x_*) = \\mathbb{E}[f(x_*)|\\mathbf{y}=\\mathbf{y}_{obs}] = k_*^T K_y^{-1} \\mathbf{y}_{obs} $$\nSubstituting the matrices derived:\n$$ \\bar{f}(x_*) = k_*^T (K+\\Sigma_{noise})^{-1} \\mathbf{y}_{obs} $$\n\n### Part 4: Numerical Evaluation\n\nWe are given the following values: $\\sigma_f=0.2$, $\\ell=0.6$, $y_f=1.05$ at $x_1=0$, $y_g=-0.10$ at $x_2=0.5$, $\\sigma_{n,f}=0.01$, $\\sigma_{n,g}=0.02$, and the test point is $x_*=0.2$.\nThe hyperparameters are $\\sigma_f^2 = 0.04$ and $\\ell^2 = 0.36$. The noise variances are $\\sigma_{n,f}^2 = 0.0001$ and $\\sigma_{n,g}^2 = 0.0004$.\n\nFirst, we compute the matrix $K$:\n$K_{11} = 0.04$\n$K_{12} = K_{21} = \\frac{0.04}{0.36}(0-0.5)\\exp\\left(-\\frac{0.25}{2(0.36)}\\right) = -\\frac{1}{18} \\exp\\left(-\\frac{25}{72}\\right) \\approx -0.03925836$\n$K_{22} = \\frac{0.04}{0.36} = \\frac{1}{9} \\approx 0.11111111$\nSo, $K \\approx \\begin{pmatrix} 0.04 & -0.03925836 \\\\ -0.03925836 & 0.11111111 \\end{pmatrix}$.\n\nNext, we form $K_y = K + \\Sigma_{noise}$:\n$K_y = \\begin{pmatrix} 0.04+0.0001 & -0.03925836 \\\\ -0.03925836 & 0.11111111+0.0004 \\end{pmatrix} = \\begin{pmatrix} 0.0401 & -0.03925836 \\\\ -0.03925836 & 0.11151111 \\end{pmatrix}$.\n\nNow, we compute the vector $k_*$:\n$(k_*)_1 = k(0.2, 0) = 0.04\\exp\\left(-\\frac{0.2^2}{2(0.36)}\\right) = 0.04\\exp\\left(-\\frac{0.04}{0.72}\\right) = 0.04\\exp\\left(-\\frac{1}{18}\\right) \\approx 0.03783835$\n$(k_*)_2 = \\frac{0.04}{0.36}(0.2-0.5)\\exp\\left(-\\frac{(0.2-0.5)^2}{2(0.36)}\\right) = -\\frac{0.3}{9}\\exp\\left(-\\frac{0.09}{0.72}\\right) = -\\frac{1}{30}\\exp\\left(-\\frac{1}{8}\\right) \\approx -0.02941656$\nSo, $k_* \\approx \\begin{pmatrix} 0.03783835 \\\\ -0.02941656 \\end{pmatrix}$.\n\nWe need to compute $K_y^{-1}$. The determinant is:\n$\\det(K_y) = (0.0401)(0.11151111) - (-0.03925836)^2 \\approx 0.00447160 - 0.00154122 \\approx 0.00293038$.\nThe inverse is:\n$K_y^{-1} \\approx \\frac{1}{0.00293038} \\begin{pmatrix} 0.11151111 & 0.03925836 \\\\ 0.03925836 & 0.0401 \\end{pmatrix} \\approx \\begin{pmatrix} 38.0537 & 13.3970 \\\\ 13.3970 & 13.6842 \\end{pmatrix}$.\n\nNow we compute the posterior mean $\\bar{f}(x_*) = k_*^T K_y^{-1} \\mathbf{y}_{obs}$, where $\\mathbf{y}_{obs} = (1.05, -0.10)^T$.\nLet's first find the vector $v^T = k_*^T K_y^{-1}$:\n$v_1 = (0.03783835)(38.0537) + (-0.02941656)(13.3970) \\approx 1.44009 - 0.39411 \\approx 1.04598$\n$v_2 = (0.03783835)(13.3970) + (-0.02941656)(13.6842) \\approx 0.50692 - 0.40248 \\approx 0.10444$\n$v^T \\approx \\begin{pmatrix} 1.04598 & 0.10444 \\end{pmatrix}$.\n\nFinally, the posterior mean is:\n$\\bar{f}(0.2) = v^T \\mathbf{y}_{obs} = (1.04598)(1.05) + (0.10444)(-0.10) \\approx 1.09828 - 0.01044 = 1.08784$.\n\nRounding to four significant figures, the posterior predictive mean is $1.088$.", "answer": "$$\n\\boxed{1.088}\n$$", "id": "3561163"}, {"introduction": "A hallmark of sophisticated modeling is the incorporation of known physical principles, such as symmetries. An emulator that respects the underlying symmetries of the system it models is not only more elegant but also generalizes more effectively from limited data. This exercise provides a hands-on guide to \"kernel engineering,\" where you will implement a GP emulator that is explicitly invariant under rotations by defining its kernel on a rotationally invariant scalar quantity, demonstrating a core technique in building physics-informed models [@problem_id:3561098].", "problem": "You are asked to implement a Gaussian Process (GP) emulator for a nuclear observable that depends on neutron and proton pairing strengths, denoted by $G_n$ and $G_p$ (measured in megaelectronvolts (MeV)). The emulator must be invariant under planar rotations of the parameter pair $(G_n,G_p)$, which is the natural action of the special orthogonal group $\\mathrm{SO}(2)$ on $\\mathbb{R}^2$. The problem concerns Gaussian processes for model emulation in computational nuclear physics and requires using a covariance kernel that enforces rotational invariance by making predictions depend only on the scalar quantity $s = G_n^2 + G_p^2$.\n\nThe objective is to design and test such an emulator in purely mathematical terms with a program that produces numerical outputs as specified below, based only on the logic and definitions of Gaussian processes. The solution should be based on the following fundamental base:\n- The definition of a Gaussian process prior over functions.\n- The concept that a covariance function (kernel) $k(\\cdot,\\cdot)$ defines a Gaussian process when it is symmetric and positive semidefinite.\n- The fact that rotation matrices $R(\\theta)$ in two dimensions satisfy $R(\\theta)^\\top R(\\theta) = I$, which implies $\\lVert R(\\theta) \\mathbf{z}\\rVert^2 = \\lVert \\mathbf{z}\\rVert^2$ for any vector $\\mathbf{z}$.\n- The standard Gaussian process regression formulas for the posterior predictive mean and variance in the presence of homoscedastic Gaussian noise.\n\nYour emulator must use a one-dimensional Gaussian process defined on the scalar input $s = G_n^2 + G_p^2 \\in \\mathbb{R}_{\\ge 0}$ with a squared-exponential covariance kernel in the scalar domain:\n- Inputs to the GP: $s \\in \\mathbb{R}_{\\ge 0}$.\n- Kernel: $k(s,s') = \\sigma_f^2 \\exp\\!\\left(-\\dfrac{(s-s')^2}{2\\ell^2}\\right)$, with hyperparameters $\\sigma_f$ and $\\ell$ given below.\n- Observation noise: $\\sigma_n^2$ added to the diagonal of the training covariance matrix.\n\nYou are given a synthetic nuclear observable defined by a deterministic function of $s$:\n- $f_{\\text{true}}(s) = A \\exp(-B s) + C s$,\nwith $A = 1.2$ MeV, $B = 0.8$ (per $\\text{MeV}^2$), and $C = 0.3$ (MeV per $\\text{MeV}^2$). Treat these as exact and do not add random noise to the training targets, but include the observation noise $\\sigma_n^2$ in the GP as prescribed.\n\nUse the following fixed hyperparameters for the Gaussian process:\n- $\\sigma_f = 1.0$ MeV,\n- $\\ell = 0.5$ (in units of $s$, i.e., $\\text{MeV}^2$),\n- $\\sigma_n = 10^{-3}$ MeV.\n\nLet the training set consist of the following pairing-parameter pairs $(G_n,G_p)$, with $G_n$ and $G_p$ in MeV:\n- $(0.0, 0.0)$,\n- $(0.2, 0.0)$,\n- $(0.0, 0.3)$,\n- $(0.2, 0.25)$,\n- $(0.45, 0.1)$,\n- $(0.5, 0.35)$,\n- $(0.7, 0.0)$,\n- $(0.0, 0.8)$,\n- $(0.6, 0.6)$,\n- $(0.9, 0.3)$.\nFor each pair, compute $s_i = G_{n,i}^2 + G_{p,i}^2$ and $y_i = f_{\\text{true}}(s_i)$.\n\nUse the following test set of parameter pairs $(G_n,G_p)$, in MeV:\n- $(0.1, 0.2)$,\n- $(0.4, 0.4)$,\n- $(0.8, 0.1)$,\n- $(0.0, 0.5)$.\nFor each pair, compute $s^\\star = G_n^2 + G_p^2$ and the GP posterior predictive mean and standard deviation. Define the rotation matrix\n$$\nR(\\theta) =\n\\begin{pmatrix}\n\\cos\\theta & -\\sin\\theta \\\\\n\\sin\\theta & \\cos\\theta\n\\end{pmatrix},\n$$\nand generate rotated test inputs $(G_n',G_p') = R(\\theta) (G_n,G_p)$ for the specified angles, then recompute $s' = {G_n'}^2 + {G_p'}^2$ and the corresponding GP posterior predictions. Because the GP depends only on $s$, predictions should be invariant under these rotations up to numerical precision.\n\nTest suite and required outputs:\n- Use the angle set $\\{\\theta_1,\\theta_2,\\theta_3\\} = \\{\\pi/7,\\ \\pi/3,\\ 5\\pi/8\\}$ (radians).\n- Define a single-angle test with $\\theta = \\pi/3$, and a multi-angle test over the above set.\n- Compute the following quantities, each expressed in MeV and returned as floating-point numbers:\n  1. The maximum absolute difference in posterior predictive means across the test set between original and rotated inputs for $\\theta = \\pi/3$ (in MeV).\n  2. The maximum absolute difference in posterior predictive standard deviations across the test set between original and rotated inputs for $\\theta = \\pi/3$ (in MeV).\n  3. The maximum absolute difference in posterior predictive means across the test set, maximized over all three angles $\\{\\pi/7,\\ \\pi/3,\\ 5\\pi/8\\}$ (in MeV).\n  4. The maximum absolute difference in posterior predictive standard deviations across the test set, maximized over all three angles (in MeV).\n  5. The posterior predictive mean at $s = 0$ (in MeV).\n  6. The posterior predictive standard deviation at $s = 100$ (in MeV).\n  7. The maximum absolute difference in posterior predictive means across the test set when the entire training set is rotated by $\\theta = 1.11$ radians and the emulator is retrained on the rotated training set, compared to the unrotated emulator (in MeV).\n- Angles must be in radians.\n- All pairing strengths $G_n$ and $G_p$ are in MeV, and all reported outputs must be in MeV.\n\nFinal output format:\n- Your program should produce a single line of output containing the results for the seven quantities above as a comma-separated list enclosed in square brackets (e.g., $[r_1,r_2,r_3,r_4,r_5,r_6,r_7]$), where each $r_i$ is a floating-point number in MeV.\n\nScientific realism and constraints:\n- The observable $f_{\\text{true}}(s)$ is synthetic but chosen to be smooth and monotonic, consistent with typical behavior of pairing contributions to nuclear observables as functions of coupling strengths.\n- The GP prior and noise level are specified to avoid numerical singularities while keeping the analysis deterministic.\n- Your implementation must compute the Gaussian process posterior using numerically stable linear algebra.\n\nYour program must be complete and runnable as is, without any external input, files, or network access. The numerical results should be printed exactly in the specified format on a single line.", "solution": "The problem requires the design and implementation of a Gaussian Process (GP) emulator for a synthetic nuclear observable. The emulator's inputs are the neutron and proton pairing strengths, $(G_n, G_p)$, and a critical constraint is that the emulator must be invariant under planar rotations of these parameters, which corresponds to the action of the group $\\mathrm{SO}(2)$ on the input space $\\mathbb{R}^2$.\n\n### Problem Validation\nThe problem statement has been validated and is deemed sound. It is scientifically grounded in the principles of Gaussian process regression and linear algebra, well-posed with all necessary data and constraints provided, and objective in its formulation. The core concept, enforcing rotational invariance by using the norm-squared of the input vector, $s = G_n^2 + G_p^2$, as the sole feature for the GP, is mathematically correct. This is because for any rotation matrix $R \\in \\mathrm{SO}(2)$ and input vector $\\mathbf{z} = (G_n, G_p)^\\top$, the squared norm is invariant: $\\lVert R\\mathbf{z} \\rVert^2 = (R\\mathbf{z})^\\top(R\\mathbf{z}) = \\mathbf{z}^\\top R^\\top R \\mathbf{z} = \\mathbf{z}^\\top I \\mathbf{z} = \\lVert \\mathbf{z} \\rVert^2$. The problem is computationally feasible and provides a clear, verifiable set of tasks.\n\n### Theoretical Framework: Gaussian Process Regression\n\nA Gaussian process defines a prior distribution over functions. Let the function to be emulated be $f(s)$. The GP prior is specified by a mean function, assumed to be zero without loss of generality, and a covariance function or kernel, $k(s, s')$.\n$$f(s) \\sim \\mathcal{GP}(0, k(s, s'))$$\nThe specified kernel is the squared-exponential kernel:\n$$k(s, s') = \\sigma_f^2 \\exp\\left(-\\frac{(s-s')^2}{2\\ell^2}\\right)$$\nwhere $\\sigma_f$ is the signal standard deviation and $\\ell$ is the characteristic length-scale.\n\nWe are given a set of $N$ training inputs $\\mathbf{s} = (s_1, \\dots, s_N)^\\top$ and corresponding observations $\\mathbf{y} = (y_1, \\dots, y_N)^\\top$. The model assumes that observations are corrupted by independent, identically distributed Gaussian noise, $y_i = f(s_i) + \\epsilon_i$, where $\\epsilon_i \\sim \\mathcal{N}(0, \\sigma_n^2)$. The covariance matrix of the noisy observations is given by:\n$$K_y = K + \\sigma_n^2 I$$\nHere, $K$ is the $N \\times N$ kernel matrix with entries $K_{ij} = k(s_i, s_j)$, and $I$ is the identity matrix.\n\nFor a new set of test inputs $\\mathbf{s}_*$, the GP framework provides a posterior distribution over the corresponding function values $f(\\mathbf{s}_*)$, which is also a Gaussian. The posterior predictive mean and variance for a new observation $y_*$ at input $s_*$ are given by:\n$$ \\mu(s_*) = \\mathbf{k}_*^\\top K_y^{-1} \\mathbf{y} $$\n$$ \\sigma^2(s_*) = k(s_*, s_*) - \\mathbf{k}_*^\\top K_y^{-1} \\mathbf{k}_* + \\sigma_n^2 $$\nwhere $\\mathbf{k}_* = (k(s_1, s_*), \\dots, k(s_N, s_*))^\\top$ is the vector of covariances between the training inputs and the test input, and $k(s_*, s_*) = k(s_*, s_*)$ is the prior variance at the test input. The requested \"posterior predictive standard deviation\" is $\\sigma(s_*) = \\sqrt{\\sigma^2(s_*)}$.\n\nTo ensure numerical stability, the matrix inversion $K_y^{-1}$ is not performed directly. Instead, we use the Cholesky decomposition of the symmetric positive-definite matrix $K_y$. Let $K_y = L L^\\top$, where $L$ is a lower-triangular matrix. The required quantities are then computed as:\n1.  Solve $L \\boldsymbol{\\alpha}' = \\mathbf{y}$ for $\\boldsymbol{\\alpha}'$ using forward substitution.\n2.  Solve $L^\\top \\boldsymbol{\\alpha} = \\boldsymbol{\\alpha}'$ for $\\boldsymbol{\\alpha} = K_y^{-1} \\mathbf{y}$ using backward substitution. The mean is then $\\mu(s_*) = \\mathbf{k}_*^\\top \\boldsymbol{\\alpha}$.\n3.  Solve $L \\mathbf{v} = \\mathbf{k}_*$ for $\\mathbf{v}$ using forward substitution. The term $\\mathbf{k}_*^\\top K_y^{-1} \\mathbf{k}_*$ becomes $\\mathbf{v}^\\top \\mathbf{v}$. The variance is then $\\sigma^2(s_*) = k(s_*, s_*) - \\mathbf{v}^\\top \\mathbf{v} + \\sigma_n^2$.\n\n### Solution Implementation\n\nThe solution proceeds as follows:\n1.  **Data Preparation**: The training parameter pairs $(G_{n,i}, G_{p,i})$ are transformed into scalar inputs $s_i = G_{n,i}^2 + G_{p,i}^2$. The corresponding training targets $y_i$ are generated using the provided deterministic function $f_{\\text{true}}(s) = A \\exp(-B s) + C s$ with $A = 1.2$, $B = 0.8$, and $C = 0.3$.\n2.  **Emulator Training**: An emulator is constructed using the given hyperparameters $\\sigma_f = 1.0$, $\\ell = 0.5$, and $\\sigma_n = 10^{-3}$. The training step involves computing the Cholesky decomposition of $K_y$ and the weight vector $\\boldsymbol{\\alpha} = K_y^{-1}\\mathbf{y}$.\n3.  **Prediction and Invariance Test (Quantities 1-4)**: For each test parameter pair $(G_n, G_p)$ and each test angle $\\theta$, the original pair and its rotated counterpart $(G_n', G_p') = R(\\theta)(G_n, G_p)$ are prepared. As established, the corresponding scalar inputs $s$ and $s'$ are mathematically identical. A single GP prediction is made for each unique $s$ value, and the resulting means and standard deviations for original and rotated pairs are compared. Any non-zero difference is attributable to finite floating-point precision. The maximum absolute differences are computed as required.\n4.  **Specific Predictions (Quantities 5-6)**: The trained emulator is used to predict the posterior mean at $s=0$ and the posterior standard deviation at $s=100$. The latter point is far from the training data, so its standard deviation should approach the prior standard deviation, which is $\\sqrt{\\sigma_f^2 + \\sigma_n^2} \\approx \\sigma_f=1.0$.\n5.  **Retraining Invariance Test (Quantity 7)**: The entire training set is rotated by $\\theta = 1.11$ radians. Since the scalar inputs $s_i$ and targets $y_i$ are invariant under this transformation, the new training dataset is identical to the original. A new emulator is trained on this data. Its predictions on the test set are compared to the original emulator's predictions. Again, any difference should be due to numerical precision errors, and the maximum absolute difference is reported.\n\nThis comprehensive procedure correctly implements the GP model and rigorously verifies the rotational invariance, which is the central theme of the problem.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import cholesky, cho_solve, solve_triangular\n\ndef solve():\n    \"\"\"\n    Implements a rotationally invariant Gaussian Process emulator and computes specified test quantities.\n    \"\"\"\n\n    # --- Problem Definition ---\n\n    # Synthetic true function parameters\n    A = 1.2  # MeV\n    B = 0.8  # 1/MeV^2\n    C = 0.3  # MeV/MeV^2\n\n    # GP hyperparameters\n    SIGMA_F = 1.0  # MeV\n    LENGTH_SCALE = 0.5  # MeV^2\n    SIGMA_N = 1e-3  # MeV\n    \n    # Training data (Gn, Gp) in MeV\n    TRAIN_GN_GP = np.array([\n        [0.0, 0.0], [0.2, 0.0], [0.0, 0.3], [0.2, 0.25], [0.45, 0.1],\n        [0.5, 0.35], [0.7, 0.0], [0.0, 0.8], [0.6, 0.6], [0.9, 0.3]\n    ])\n\n    # Test data (Gn, Gp) in MeV\n    TEST_GN_GP = np.array([\n        [0.1, 0.2], [0.4, 0.4], [0.8, 0.1], [0.0, 0.5]\n    ])\n\n    # Test angles in radians\n    TEST_ANGLES = np.array([np.pi/7, np.pi/3, 5*np.pi/8])\n    THETA_RETRAIN = 1.11\n\n    # --- Helper Functions and Classes ---\n\n    def f_true(s):\n        \"\"\"Synthetic nuclear observable.\"\"\"\n        return A * np.exp(-B * s) + C * s\n\n    class GPEmulator:\n        \"\"\"A Gaussian Process Emulator.\"\"\"\n        def __init__(self, sigma_f, length_scale, sigma_n):\n            self.sigma_f = sigma_f\n            self.length_scale_sq = length_scale**2\n            self.sigma_n_sq = sigma_n**2\n            self.train_s = None\n            self.alpha = None\n            self.L = None\n\n        def _kernel(self, s1, s2):\n            \"\"\"Squared-exponential kernel for 1D arrays s1 and s2.\"\"\"\n            s1_col = s1[:, np.newaxis]\n            s2_row = s2[np.newaxis, :]\n            sq_dist = (s1_col - s2_row)**2\n            return self.sigma_f**2 * np.exp(-0.5 * sq_dist / self.length_scale_sq)\n\n        def train(self, train_s, train_y):\n            \"\"\"Trains the GP emulator.\"\"\"\n            self.train_s = np.asarray(train_s)\n            train_y = np.asarray(train_y)\n            N = len(self.train_s)\n            \n            K = self._kernel(self.train_s, self.train_s)\n            Ky = K + np.eye(N) * self.sigma_n_sq\n            \n            # Use Cholesky decomposition for stability\n            self.L = cholesky(Ky, lower=True)\n            self.alpha = cho_solve((self.L, True), train_y)\n\n        def predict(self, test_s):\n            \"\"\"Predicts posterior mean and standard deviation.\"\"\"\n            test_s = np.atleast_1d(test_s)\n\n            k_star = self._kernel(self.train_s, test_s)\n            \n            # Predictive mean\n            pred_mean = k_star.T @ self.alpha\n            \n            # Predictive variance\n            v = solve_triangular(self.L, k_star, lower=True)\n            k_star_star = self.sigma_f**2  # k(s_*, s_*) is constant for SE kernel\n            \n            var_f = k_star_star - np.sum(v**2, axis=0)\n            var_y = var_f + self.sigma_n_sq\n            \n            pred_std = np.sqrt(np.maximum(0, var_y))\n            \n            return pred_mean, pred_std\n\n    # --- Main Calculation Logic ---\n\n    # 1. Prepare training data and train the main emulator\n    train_s = np.sum(TRAIN_GN_GP**2, axis=1)\n    train_y = f_true(train_s)\n    \n    emulator = GPEmulator(SIGMA_F, LENGTH_SCALE, SIGMA_N)\n    emulator.train(train_s, train_y)\n\n    # 2. Prepare test data and get original predictions\n    test_s = np.sum(TEST_GN_GP**2, axis=1)\n    mean_orig, std_orig = emulator.predict(test_s)\n\n    # 3. Calculate quantities 1-4 (invariance under test set rotation)\n    max_diff_mean_all_angles = 0.0\n    max_diff_std_all_angles = 0.0\n    r1, r2 = 0.0, 0.0\n\n    for theta in TEST_ANGLES:\n        R = np.array([[np.cos(theta), -np.sin(theta)],\n                      [np.sin(theta),  np.cos(theta)]])\n        \n        test_gn_gp_rot = (R @ TEST_GN_GP.T).T\n        test_s_rot = np.sum(test_gn_gp_rot**2, axis=1)\n        \n        # Predictions for rotated data\n        mean_rot, std_rot = emulator.predict(test_s_rot)\n        \n        diff_mean = np.abs(mean_orig - mean_rot)\n        diff_std = np.abs(std_orig - std_rot)\n        \n        if np.isclose(theta, np.pi/3):\n            r1 = np.max(diff_mean)\n            r2 = np.max(diff_std)\n            \n        max_diff_mean_all_angles = max(max_diff_mean_all_angles, np.max(diff_mean))\n        max_diff_std_all_angles = max(max_diff_std_all_angles, np.max(diff_std))\n\n    r3 = max_diff_mean_all_angles\n    r4 = max_diff_std_all_angles\n\n    # 4. Calculate quantity 5 (mean at s=0)\n    mean_at_0, _ = emulator.predict(0.0)\n    r5 = mean_at_0[0]\n\n    # 5. Calculate quantity 6 (std dev at s=100)\n    _, std_at_100 = emulator.predict(100.0)\n    r6 = std_at_100[0]\n\n    # 6. Calculate quantity 7 (invariance under training set rotation)\n    R_retrain = np.array([[np.cos(THETA_RETRAIN), -np.sin(THETA_RETRAIN)],\n                          [np.sin(THETA_RETRAIN),  np.cos(THETA_RETRAIN)]])\n    \n    train_gn_gp_rot = (R_retrain @ TRAIN_GN_GP.T).T\n    train_s_rot = np.sum(train_gn_gp_rot**2, axis=1)\n    train_y_rot = f_true(train_s_rot)\n\n    emu_retrained = GPEmulator(SIGMA_F, LENGTH_SCALE, SIGMA_N)\n    emu_retrained.train(train_s_rot, train_y_rot)\n    \n    mean_retrained, _ = emu_retrained.predict(test_s)\n    \n    diff_mean_retrain = np.abs(mean_orig - mean_retrained)\n    r7 = np.max(diff_mean_retrain)\n    \n    # --- Final Output ---\n    results = [r1, r2, r3, r4, r5, r6, r7]\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3561098"}, {"introduction": "While powerful, Gaussian processes face computational challenges, scaling as $\\mathcal{O}(N^3)$ with the number of training points $N$. To apply GPs to large datasets, approximation methods are essential. This advanced problem explores one such technique, Random Fourier Features, which linearizes the model for stationary kernels and drastically reduces computational cost. By deriving the variance of the kernel approximation error from first principles, you will gain a deep, theoretical understanding of how this method works and how its accuracy relates to the number of features used [@problem_id:3561155].", "problem": "A nuclear equation-of-state emulator is modeled by a Gaussian process (GP) prior with a stationary covariance kernel $k(x,x^{\\prime}) = k(\\tau)$, where $\\tau = x - x^{\\prime}$ and $x \\in \\mathbb{R}^{d}$ represents a $d$-dimensional vector of nuclear-physics input conditions (for example, density, temperature, and isospin asymmetry). Assume $k$ is continuous, positive definite, and real-valued. By Bochner’s theorem, there exists a nonnegative spectral density $p(\\omega)$ such that\n$$\nk(\\tau) = \\int_{\\mathbb{R}^{d}} p(\\omega) \\exp\\!\\left(i\\,\\omega^{\\top} \\tau\\right)\\, \\mathrm{d}\\omega,\n$$\nwith $k(0) = \\int_{\\mathbb{R}^{d}} p(\\omega)\\, \\mathrm{d}\\omega \\equiv k_{0} > 0$. Define the probability density $q(\\omega) = p(\\omega)/k_{0}$ so that\n$$\n\\mathbb{E}_{\\omega \\sim q}\\!\\left[\\cos\\!\\left(\\omega^{\\top} \\tau\\right)\\right] = \\frac{k(\\tau)}{k_{0}}.\n$$\nConsider the random Fourier feature map with random phases,\n$$\nz_{m}(x) = \\sqrt{\\frac{2k_{0}}{M}}\\, \\cos\\!\\left(\\omega_{m}^{\\top} x + b_{m}\\right), \\quad m = 1,\\dots,M,\n$$\nwhere $\\omega_{m} \\overset{\\text{i.i.d.}}{\\sim} q(\\omega)$ and $b_{m} \\overset{\\text{i.i.d.}}{\\sim} \\mathrm{Unif}[0,2\\pi]$, independent of $\\omega_{m}$. Define the Monte Carlo kernel estimator\n$$\n\\widehat{k}_{M}(x,x^{\\prime}) \\equiv z(x)^{\\top} z(x^{\\prime}) = \\sum_{m=1}^{M} z_{m}(x)\\, z_{m}(x^{\\prime}),\n$$\nand the approximation error\n$$\n\\varepsilon_{M}(\\tau) \\equiv \\widehat{k}_{M}(x,x^{\\prime}) - k(\\tau).\n$$\nWorking from the definitions above, the independence of the random features, and standard trigonometric identities, derive a closed-form expression for the variance $\\mathrm{Var}\\!\\left[\\varepsilon_{M}(\\tau)\\right]$ as a function of $M$, $k(0)$, $k(\\tau)$, and $k(2\\tau)$. Express your final answer as a single analytic expression. No numerical evaluation is required, and no rounding is needed. The answer must be a single closed-form expression with no units.", "solution": "The objective is to derive a closed-form expression for the variance of the approximation error, $\\mathrm{Var}[\\varepsilon_{M}(\\tau)]$, where $\\varepsilon_{M}(\\tau) = \\widehat{k}_{M}(x,x^{\\prime}) - k(\\tau)$. The variance of a random variable $Y$ is given by $\\mathrm{Var}[Y] = \\mathbb{E}[Y^2] - (\\mathbb{E}[Y])^2$. We will apply this definition to $\\varepsilon_{M}(\\tau)$.\n\nFirst, we compute the expectation of the error, $\\mathbb{E}[\\varepsilon_{M}(\\tau)]$. By the linearity of expectation,\n$$\n\\mathbb{E}[\\varepsilon_{M}(\\tau)] = \\mathbb{E}[\\widehat{k}_{M}(x,x^{\\prime}) - k(\\tau)] = \\mathbb{E}[\\widehat{k}_{M}(x,x^{\\prime})] - k(\\tau),\n$$\nsince $k(\\tau)$ is a deterministic quantity. The expectation is taken with respect to the random variables $\\{\\omega_m, b_m\\}_{m=1}^{M}$.\n\nLet's compute the expectation of the kernel estimator $\\widehat{k}_{M}(x,x^{\\prime})$.\n$$\n\\widehat{k}_{M}(x,x^{\\prime}) = \\sum_{m=1}^{M} z_{m}(x) z_{m}(x^{\\prime}).\n$$\nBy linearity of expectation,\n$$\n\\mathbb{E}[\\widehat{k}_{M}(x,x^{\\prime})] = \\sum_{m=1}^{M} \\mathbb{E}[z_{m}(x) z_{m}(x^{\\prime})].\n$$\nThe random variables $(\\omega_m, b_m)$ are independent and identically distributed (i.i.d.) for $m=1, \\dots, M$. Therefore, the expectation $\\mathbb{E}[z_{m}(x) z_{m}(x^{\\prime})]$ is the same for all $m$. We can compute it for $m=1$:\n$$\nz_{1}(x) z_{1}(x^{\\prime}) = \\left(\\sqrt{\\frac{2k_{0}}{M}}\\, \\cos(\\omega_{1}^{\\top} x + b_{1})\\right) \\left(\\sqrt{\\frac{2k_{0}}{M}}\\, \\cos(\\omega_{1}^{\\top} x^{\\prime} + b_{1})\\right) = \\frac{2k_{0}}{M} \\cos(\\omega_{1}^{\\top} x + b_{1}) \\cos(\\omega_{1}^{\\top} x^{\\prime} + b_{1}).\n$$\nUsing the trigonometric product-to-sum identity $\\cos(A)\\cos(B) = \\frac{1}{2}[\\cos(A-B) + \\cos(A+B)]$, we get\n$$\nz_{1}(x) z_{1}(x^{\\prime}) = \\frac{k_{0}}{M} \\left[\\cos(\\omega_{1}^{\\top}(x-x^{\\prime})) + \\cos(\\omega_{1}^{\\top}(x+x^{\\prime}) + 2b_{1})\\right].\n$$\nLet $\\tau = x-x^{\\prime}$. The expression becomes\n$$\nz_{1}(x) z_{1}(x^{\\prime}) = \\frac{k_{0}}{M} \\left[\\cos(\\omega_{1}^{\\top}\\tau) + \\cos(\\omega_{1}^{\\top}(x+x^{\\prime}) + 2b_{1})\\right].\n$$\nWe take the expectation over $\\omega_{1} \\sim q(\\omega)$ and $b_{1} \\sim \\mathrm{Unif}[0, 2\\pi]$. The expectation over $b_{1}$ is computed first:\n$$\n\\mathbb{E}_{b_{1}}[\\cos(\\omega_{1}^{\\top}(x+x^{\\prime}) + 2b_{1})] = \\frac{1}{2\\pi} \\int_{0}^{2\\pi} \\cos(\\omega_{1}^{\\top}(x+x^{\\prime}) + 2b) \\, \\mathrm{d}b = 0,\n$$\nas the integral of a cosine function over two full periods is zero.\nThus, the expectation over $b_1$ is\n$$\n\\mathbb{E}_{b_{1}}[z_{1}(x) z_{1}(x^{\\prime})] = \\frac{k_{0}}{M} \\cos(\\omega_{1}^{\\top}\\tau).\n$$\nNext, we take the expectation over $\\omega_{1}$:\n$$\n\\mathbb{E}[z_{1}(x) z_{1}(x^{\\prime})] = \\mathbb{E}_{\\omega_{1}, b_{1}}[z_{1}(x) z_{1}(x^{\\prime})] = \\frac{k_{0}}{M} \\mathbb{E}_{\\omega_{1}}[\\cos(\\omega_{1}^{\\top}\\tau)].\n$$\nUsing the given identity $\\mathbb{E}_{\\omega \\sim q}[\\cos(\\omega^{\\top} \\tau)] = k(\\tau)/k_{0}$, we have\n$$\n\\mathbb{E}[z_{1}(x) z_{1}(x^{\\prime})] = \\frac{k_{0}}{M} \\frac{k(\\tau)}{k_{0}} = \\frac{k(\\tau)}{M}.\n$$\nNow we can compute the expectation of the estimator:\n$$\n\\mathbb{E}[\\widehat{k}_{M}(x,x^{\\prime})] = \\sum_{m=1}^{M} \\mathbb{E}[z_{m}(x) z_{m}(x^{\\prime})] = \\sum_{m=1}^{M} \\frac{k(\\tau)}{M} = M \\frac{k(\\tau)}{M} = k(\\tau).\n$$\nThe estimator $\\widehat{k}_{M}(x,x^{\\prime})$ is unbiased. Substituting this into the expression for the expectation of the error:\n$$\n\\mathbb{E}[\\varepsilon_{M}(\\tau)] = k(\\tau) - k(\\tau) = 0.\n$$\nSince the error has zero mean, its variance is equal to its second moment:\n$$\n\\mathrm{Var}[\\varepsilon_{M}(\\tau)] = \\mathbb{E}[\\varepsilon_{M}(\\tau)^2] = \\mathbb{E}[(\\widehat{k}_{M}(x,x^{\\prime}) - k(\\tau))^2].\n$$\nThis is precisely the variance of the estimator $\\widehat{k}_{M}(x,x^{\\prime})$ itself:\n$$\n\\mathrm{Var}[\\varepsilon_{M}(\\tau)] = \\mathrm{Var}[\\widehat{k}_{M}(x,x^{\\prime})].\n$$\nLet $Y_m = z_m(x)z_m(x')$. Then $\\widehat{k}_{M}(x,x^{\\prime}) = \\sum_{m=1}^{M} Y_m$. Since the pairs $(\\omega_m, b_m)$ are i.i.d., the random variables $Y_m$ are also i.i.d. The variance of a sum of i.i.d. random variables is the sum of their variances:\n$$\n\\mathrm{Var}[\\widehat{k}_{M}(x,x^{\\prime})] = \\mathrm{Var}\\left[\\sum_{m=1}^{M} Y_m\\right] = \\sum_{m=1}^{M} \\mathrm{Var}[Y_m] = M \\cdot \\mathrm{Var}[Y_1].\n$$\nWe need to compute $\\mathrm{Var}[Y_1] = \\mathbb{E}[Y_1^2] - (\\mathbb{E}[Y_1])^2$. We already found $\\mathbb{E}[Y_1] = k(\\tau)/M$. Next, we compute the second moment $\\mathbb{E}[Y_1^2]$.\n$$\nY_1^2 = \\left( \\frac{k_{0}}{M} \\left[\\cos(\\omega_{1}^{\\top}\\tau) + \\cos(\\omega_{1}^{\\top}(x+x^{\\prime}) + 2b_{1})\\right] \\right)^2.\n$$\nExpanding the square:\n$$\nY_1^2 = \\frac{k_0^2}{M^2} \\left[ \\cos^2(\\omega_{1}^{\\top}\\tau) + 2\\cos(\\omega_{1}^{\\top}\\tau)\\cos(\\omega_{1}^{\\top}(x+x^{\\prime}) + 2b_{1}) + \\cos^2(\\omega_{1}^{\\top}(x+x^{\\prime}) + 2b_{1}) \\right].\n$$\nTaking the expectation over $b_1$:\n$$\n\\mathbb{E}_{b_1}[Y_1^2] = \\frac{k_0^2}{M^2} \\left[ \\cos^2(\\omega_{1}^{\\top}\\tau) + 2\\cos(\\omega_{1}^{\\top}\\tau)\\mathbb{E}_{b_1}[\\cos(\\dots)] + \\mathbb{E}_{b_1}[\\cos^2(\\dots)] \\right].\n$$\nAs before, $\\mathbb{E}_{b_1}[\\cos(\\omega_{1}^{\\top}(x+x^{\\prime}) + 2b_{1})] = 0$. For the last term, we use $\\cos^2(A) = \\frac{1}{2}(1+\\cos(2A))$:\n$$\n\\mathbb{E}_{b_1}[\\cos^2(\\omega_{1}^{\\top}(x+x^{\\prime}) + 2b_{1})] = \\mathbb{E}_{b_1}\\left[\\frac{1}{2}(1 + \\cos(2\\omega_{1}^{\\top}(x+x^{\\prime}) + 4b_{1}))\\right] = \\frac{1}{2},\n$$\nsince the expectation of the cosine term over four periods is zero. So, the expectation over $b_1$ gives:\n$$\n\\mathbb{E}_{b_1}[Y_1^2] = \\frac{k_0^2}{M^2} \\left[ \\cos^2(\\omega_{1}^{\\top}\\tau) + \\frac{1}{2} \\right].\n$$\nNow, we take the expectation over $\\omega_1$:\n$$\n\\mathbb{E}[Y_1^2] = \\mathbb{E}_{\\omega_1, b_1}[Y_1^2] = \\frac{k_0^2}{M^2} \\left[ \\mathbb{E}_{\\omega_1}[\\cos^2(\\omega_{1}^{\\top}\\tau)] + \\frac{1}{2} \\right].\n$$\nAgain, using $\\cos^2(A) = \\frac{1}{2}(1+\\cos(2A))$:\n$$\n\\mathbb{E}_{\\omega_1}[\\cos^2(\\omega_{1}^{\\top}\\tau)] = \\mathbb{E}_{\\omega_1}\\left[\\frac{1}{2}(1 + \\cos(2\\omega_{1}^{\\top}\\tau))\\right] = \\frac{1}{2} (1 + \\mathbb{E}_{\\omega_1}[\\cos(\\omega_{1}^{\\top}(2\\tau))]).\n$$\nUsing the problem's identity with argument $2\\tau$, we have $\\mathbb{E}_{\\omega_1}[\\cos(\\omega_{1}^{\\top}(2\\tau))] = k(2\\tau)/k_0$.\n$$\n\\mathbb{E}_{\\omega_1}[\\cos^2(\\omega_{1}^{\\top}\\tau)] = \\frac{1}{2}\\left(1 + \\frac{k(2\\tau)}{k_0}\\right).\n$$\nSubstituting this back into the expression for $\\mathbb{E}[Y_1^2]$:\n$$\n\\mathbb{E}[Y_1^2] = \\frac{k_0^2}{M^2} \\left[ \\frac{1}{2}\\left(1 + \\frac{k(2\\tau)}{k_0}\\right) + \\frac{1}{2} \\right] = \\frac{k_0^2}{M^2} \\left[ 1 + \\frac{k(2\\tau)}{2k_0} \\right] = \\frac{k_0^2}{M^2} + \\frac{k_0 k(2\\tau)}{2M^2}.\n$$\nNow we compute $\\mathrm{Var}[Y_1]$:\n$$\n\\mathrm{Var}[Y_1] = \\mathbb{E}[Y_1^2] - (\\mathbb{E}[Y_1])^2 = \\left( \\frac{k_0^2}{M^2} + \\frac{k_0 k(2\\tau)}{2M^2} \\right) - \\left(\\frac{k(\\tau)}{M}\\right)^2 = \\frac{1}{M^2} \\left( k_0^2 + \\frac{1}{2}k_0 k(2\\tau) - (k(\\tau))^2 \\right).\n$$\nFinally, we obtain the variance of the approximation error, recalling that $k_0 = k(0)$:\n$$\n\\mathrm{Var}[\\varepsilon_{M}(\\tau)] = M \\cdot \\mathrm{Var}[Y_1] = M \\cdot \\frac{1}{M^2} \\left( (k(0))^2 + \\frac{1}{2}k(0) k(2\\tau) - (k(\\tau))^2 \\right).\n$$\n$$\n\\mathrm{Var}[\\varepsilon_{M}(\\tau)] = \\frac{1}{M} \\left( (k(0))^2 + \\frac{1}{2}k(0) k(2\\tau) - (k(\\tau))^2 \\right).\n$$", "answer": "$$\n\\boxed{\\frac{1}{M} \\left( (k(0))^{2} + \\frac{1}{2} k(0) k(2\\tau) - (k(\\tau))^{2} \\right)}\n$$", "id": "3561155"}]}