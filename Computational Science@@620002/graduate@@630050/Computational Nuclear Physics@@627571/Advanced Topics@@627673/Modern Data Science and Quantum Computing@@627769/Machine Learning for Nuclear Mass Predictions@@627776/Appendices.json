{"hands_on_practices": [{"introduction": "Before building complex models, it's crucial to understand how to handle the foundational data itself. This first exercise grounds your work in sound statistical principles by addressing a common scenario: combining multiple experimental measurements of the same quantity. By deriving the optimal way to aggregate data from first principles, you will build a solid intuition for why inverse-variance weighting is the standard approach for training machine learning models on physical data with reported uncertainties [@problem_id:3568202].", "problem": "Consider a single nuclide whose mass excess is measured independently by two experiments. Let the latent true mass excess be $m \\in \\mathbb{R}$. Each reported value $y_{i}$ is modeled by the Gaussian measurement equation $y_{i} = m + \\epsilon_{i}$, where $\\epsilon_{i} \\sim \\mathcal{N}(0, \\sigma_{i}^{2})$ and $(\\epsilon_{1}, \\epsilon_{2})$ are independent. You are given the following two reports for the same nucleus: $y_{1} = -72382.6\\,\\mathrm{keV}$ with standard uncertainty $\\sigma_{1} = 4.0\\,\\mathrm{keV}$, and $y_{2} = -72385.1\\,\\mathrm{keV}$ with standard uncertainty $\\sigma_{2} = 2.5\\,\\mathrm{keV}$. Starting from the Gaussian likelihood and without assuming any shortcut formulas, derive the maximum likelihood estimator $\\hat{m}$ for $m$ and its standard uncertainty $\\sigma_{\\hat{m}}$. Then, in the context of Machine Learning (ML) with a weighted least squares training objective, where the training loss takes the form $L = \\sum_{i} w_{i} \\left(y_{i} - f(x_{i})\\right)^{2}$, derive the choice of sample weights $w_{i}$ that recovers the Gaussian maximum likelihood principle, and determine the single equivalent loss weight $w_{\\mathrm{agg}}$ that should be assigned if the two measurements are first aggregated into one label at $y = \\hat{m}$ and used as a single training sample. Evaluate $\\hat{m}$, $\\sigma_{\\hat{m}}$, and $w_{\\mathrm{agg}}$ numerically for the given data. Express $\\hat{m}$ in $\\mathrm{keV}$ rounded to five significant figures, express $\\sigma_{\\hat{m}}$ in $\\mathrm{keV}$ rounded to four significant figures, and express $w_{\\mathrm{agg}}$ in $\\mathrm{keV}^{-2}$ rounded to four significant figures.", "solution": "The problem is well-posed and scientifically grounded. We proceed with the derivation and calculation.\n\nThe problem states that each measurement $y_i$ of the true mass excess $m$ is modeled by a Gaussian distribution. The probability density function (PDF) for observing a value $y_i$ given the true value $m$ and standard uncertainty $\\sigma_i$ is:\n$$p(y_i|m, \\sigma_i) = \\frac{1}{\\sqrt{2\\pi\\sigma_i^2}} \\exp\\left(-\\frac{(y_i - m)^2}{2\\sigma_i^2}\\right)$$\nWe have two independent measurements, $(y_1, \\sigma_1)$ and $(y_2, \\sigma_2)$. Due to independence, the joint probability of observing both $y_1$ and $y_2$, known as the likelihood function $L(m)$, is the product of the individual PDFs:\n$$L(m; y_1, y_2) = p(y_1|m, \\sigma_1) \\cdot p(y_2|m, \\sigma_2) = \\frac{1}{2\\pi\\sigma_1\\sigma_2} \\exp\\left(-\\frac{(y_1 - m)^2}{2\\sigma_1^2} - \\frac{(y_2 - m)^2}{2\\sigma_2^2}\\right)$$\nThe maximum likelihood estimator (MLE) $\\hat{m}$ is the value of $m$ that maximizes $L(m)$. It is equivalent and computationally simpler to maximize the log-likelihood function, $\\ell(m) = \\ln(L(m))$:\n$$\\ell(m) = \\ln\\left(\\frac{1}{2\\pi\\sigma_1\\sigma_2}\\right) - \\frac{(y_1 - m)^2}{2\\sigma_1^2} - \\frac{(y_2 - m)^2}{2\\sigma_2^2}$$\nTo find the maximum, we take the derivative of $\\ell(m)$ with respect to $m$ and set it to zero. The constant term vanishes upon differentiation.\n$$\\frac{d\\ell(m)}{dm} = -\\frac{2(y_1 - m)(-1)}{2\\sigma_1^2} - \\frac{2(y_2 - m)(-1)}{2\\sigma_2^2} = \\frac{y_1 - m}{\\sigma_1^2} + \\frac{y_2 - m}{\\sigma_2^2}$$\nSetting the derivative to zero to find the estimator $\\hat{m}$:\n$$\\frac{y_1 - \\hat{m}}{\\sigma_1^2} + \\frac{y_2 - \\hat{m}}{\\sigma_2^2} = 0$$\n$$\\frac{y_1}{\\sigma_1^2} + \\frac{y_2}{\\sigma_2^2} = \\hat{m}\\left(\\frac{1}{\\sigma_1^2} + \\frac{1}{\\sigma_2^2}\\right)$$\nSolving for $\\hat{m}$:\n$$\\hat{m} = \\frac{\\frac{y_1}{\\sigma_1^2} + \\frac{y_2}{\\sigma_2^2}}{\\frac{1}{\\sigma_1^2} + \\frac{1}{\\sigma_2^2}} = \\frac{\\sum_{i=1}^{2} \\frac{y_i}{\\sigma_i^2}}{\\sum_{i=1}^{2} \\frac{1}{\\sigma_i^2}}$$\nThis is the inverse-variance weighted mean of the measurements.\n\nNext, we derive the standard uncertainty of the estimator, $\\sigma_{\\hat{m}}$. The variance of the MLE, $\\sigma_{\\hat{m}}^2$, is given by the CramÃ©r-Rao lower bound, which for an unbiased estimator is the inverse of the Fisher information $I(m)$. The Fisher information is $I(m) = -E\\left[\\frac{d^2\\ell(m)}{dm^2}\\right]$. We compute the second derivative of the log-likelihood:\n$$\\frac{d^2\\ell(m)}{dm^2} = \\frac{d}{dm}\\left(\\frac{y_1 - m}{\\sigma_1^2} + \\frac{y_2 - m}{\\sigma_2^2}\\right) = -\\frac{1}{\\sigma_1^2} - \\frac{1}{\\sigma_2^2} = -\\sum_{i=1}^{2}\\frac{1}{\\sigma_i^2}$$\nSince the second derivative is a constant and does not depend on the data $y_i$, its expectation is simply its value.\n$$I(m) = -E\\left[-\\sum_{i=1}^{2}\\frac{1}{\\sigma_i^2}\\right] = \\sum_{i=1}^{2}\\frac{1}{\\sigma_i^2}$$\nThe variance of $\\hat{m}$ is the inverse of the Fisher information:\n$$\\sigma_{\\hat{m}}^2 = [I(m)]^{-1} = \\left(\\sum_{i=1}^{2}\\frac{1}{\\sigma_i^2}\\right)^{-1} = \\frac{1}{\\frac{1}{\\sigma_1^2} + \\frac{1}{\\sigma_2^2}}$$\nThe standard uncertainty $\\sigma_{\\hat{m}}$ is the square root of the variance:\n$$\\sigma_{\\hat{m}} = \\sqrt{\\sigma_{\\hat{m}}^2} = \\left(\\sum_{i=1}^{2}\\frac{1}{\\sigma_i^2}\\right)^{-1/2}$$\n\nNow, we connect this to the weighted least squares training objective in Machine Learning (ML). The loss function is given as $L = \\sum_{i} w_{i} \\left(y_{i} - f(x_{i})\\right)^{2}$. In our case, we are estimating a single parameter $m$ for one nuclide, so the model prediction is constant, $f(x_i) = m$. The loss function simplifies to:\n$$L(m) = \\sum_{i=1}^{2} w_i (y_i - m)^2$$\nMinimizing this loss function should be equivalent to maximizing the Gaussian likelihood. Maximizing the likelihood is equivalent to minimizing the negative log-likelihood. Ignoring constant terms, the objective to minimize is:\n$$-\\ell(m) \\propto \\sum_{i=1}^{2} \\frac{(y_i - m)^2}{2\\sigma_i^2}$$\nFor the ML loss $L(m)$ to be equivalent to this objective, their functional forms with respect to $m$ must match. Comparing the two expressions:\n$$\\sum_{i=1}^{2} w_i (y_i - m)^2 \\quad \\text{vs.} \\quad \\sum_{i=1}^{2} \\frac{1}{2\\sigma_i^2} (y_i - m)^2$$\nThis equivalence holds if the weights $w_i$ are proportional to $1/\\sigma_i^2$. The conventional choice in statistical weighting is to set $w_i = 1/\\sigma_i^2$. This makes the loss function equal to the chi-squared statistic, $\\chi^2$.\n\nFinally, we consider aggregating the two measurements into a single data point $(y, \\sigma_y)$, where $y = \\hat{m}$ and $\\sigma_y = \\sigma_{\\hat{m}}$. The training loss for this single sample would be $L_{\\mathrm{agg}} = w_{\\mathrm{agg}}(y - m)^2 = w_{\\mathrm{agg}}(\\hat{m} - m)^2$. Following the principle just derived, the appropriate weight $w_{\\mathrm{agg}}$ is the inverse of the variance of this aggregated data point. The variance of our aggregated point $\\hat{m}$ is $\\sigma_{\\hat{m}}^2$. Therefore:\n$$w_{\\mathrm{agg}} = \\frac{1}{\\sigma_{\\hat{m}}^2}$$\nSubstituting the expression for $\\sigma_{\\hat{m}}^2$:\n$$w_{\\mathrm{agg}} = \\frac{1}{\\left(\\frac{1}{\\frac{1}{\\sigma_1^2} + \\frac{1}{\\sigma_2^2}}\\right)} = \\frac{1}{\\sigma_1^2} + \\frac{1}{\\sigma_2^2}$$\nSince $w_i = 1/\\sigma_i^2$, we see that $w_{\\mathrm{agg}} = w_1 + w_2$. The aggregated weight is the sum of the individual weights.\n\nNumerical evaluation:\nGiven data: $y_{1} = -72382.6\\,\\mathrm{keV}$ with $\\sigma_{1} = 4.0\\,\\mathrm{keV}$, and $y_{2} = -72385.1\\,\\mathrm{keV}$ with $\\sigma_{2} = 2.5\\,\\mathrm{keV}$.\nFirst, calculate the variances and weights:\n$\\sigma_1^2 = (4.0)^2 = 16.0\\,\\mathrm{keV}^2 \\implies w_1 = 1/16.0 = 0.0625\\,\\mathrm{keV}^{-2}$\n$\\sigma_2^2 = (2.5)^2 = 6.25\\,\\mathrm{keV}^2 \\implies w_2 = 1/6.25 = 0.16\\,\\mathrm{keV}^{-2}$\n\nCalculate $\\hat{m}$:\n$$\\hat{m} = \\frac{w_1 y_1 + w_2 y_2}{w_1 + w_2} = \\frac{(0.0625)(-72382.6) + (0.16)(-72385.1)}{0.0625 + 0.16} = \\frac{-4523.9125 - 11581.616}{0.2225} = \\frac{-16105.5285}{0.2225} \\approx -72384.4000\\,\\mathrm{keV}$$\nRounding to five significant figures, $\\hat{m} = -72384\\,\\mathrm{keV}$.\n\nCalculate $\\sigma_{\\hat{m}}$:\n$$\\sigma_{\\hat{m}} = (w_1 + w_2)^{-1/2} = (0.2225)^{-1/2} \\approx \\sqrt{4.494382...} \\approx 2.119995...\\,\\mathrm{keV}$$\nRounding to four significant figures, $\\sigma_{\\hat{m}} = 2.120\\,\\mathrm{keV}$.\n\nCalculate $w_{\\mathrm{agg}}$:\n$$w_{\\mathrm{agg}} = w_1 + w_2 = 0.0625 + 0.16 = 0.2225\\,\\mathrm{keV}^{-2}$$\nRounding to four significant figures, $w_{\\mathrm{agg}} = 0.2225\\,\\mathrm{keV}^{-2}$.", "answer": "$$\\boxed{\\begin{pmatrix} -72384 & 2.120 & 0.2225 \\end{pmatrix}}$$", "id": "3568202"}, {"introduction": "Once a model is trained, how do we fairly assess its performance against experimental data of varying quality? This practice extends the concept of weighting from model training to model evaluation, a critical step in a rigorous scientific workflow. You will implement and compare both unweighted and weighted error metrics, gaining a practical understanding of how to prevent less precise data points from disproportionately influencing your assessment of a model's accuracy [@problem_id:3568178].", "problem": "In computational nuclear physics, machine learning models that predict nuclear masses are assessed by comparing predictions to evaluated reference values. When reference masses are accompanied by reported one standard deviation uncertainties, it is statistically appropriate to account for heteroscedastic measurement noise. Assume an independent Gaussian noise model with known standard deviations. Specifically, let the evaluated mass excess values be denoted by $\\{y_i\\}_{i=1}^N$ (in megaelectronvolts), the model predictions by $\\{\\hat{y}_i\\}_{i=1}^N$, and the reported standard deviations by $\\{\\sigma_i\\}_{i=1}^N$, with $\\sigma_i > 0$.\n\nTask. From first principles, starting from the Gaussian likelihood for independent observations with possibly different variances, derive a principled weighting scheme for residuals that reflects measurement precision. Then, implement two metrics:\n- the unweighted Root Mean Squared Error (RMSE), and\n- a weighted Root Mean Squared Error (WRMSE) consistent with the Gaussian heteroscedastic noise model.\n\nBoth metrics must be expressed in megaelectronvolts. The final numerical answers must be rounded to $6$ decimal places.\n\nTest Suite. For each test case below, compute the unweighted RMSE and the weighted RMSE. Use mass excess values in megaelectronvolts (MeV). For all arrays, index alignment corresponds to nuclide alignment.\n\n- Test case A (varied uncertainties, typical residuals):\n  - $y = [-60.234, -45.876, -30.112, -15.995, -8.321]$\n  - $\\hat{y} = [-60.100, -45.700, -30.500, -16.100, -8.400]$\n  - $\\sigma = [0.05, 0.10, 0.20, 0.15, 0.08]$\n- Test case B (equal uncertainties, sanity check):\n  - $y = [-12.000, -25.000, -37.000, -49.000]$\n  - $\\hat{y} = [-12.100, -25.100, -36.900, -49.300]$\n  - $\\sigma = [0.10, 0.10, 0.10, 0.10]$\n- Test case C (large residual outliers paired with large uncertainties):\n  - $y = [-80.000, -70.000, -65.000, -60.000, -55.000]$\n  - $\\hat{y} = [-79.900, -69.800, -66.500, -58.500, -53.000]$\n  - $\\sigma = [0.10, 0.10, 1.50, 1.20, 2.50]$\n- Test case D (a very precise point dominates weighting):\n  - $y = [-10.000, -20.000, -30.000, -40.000]$\n  - $\\hat{y} = [-10.200, -20.150, -29.900, -39.850]$\n  - $\\sigma = [0.01, 0.20, 0.20, 0.20]$\n\nOutput requirements.\n- For each test case, produce two floating-point results in megaelectronvolts: first the unweighted RMSE, then the weighted RMSE, each rounded to $6$ decimal places.\n- Aggregate the results for all test cases into a single list in the following order: A-unweighted, A-weighted, B-unweighted, B-weighted, C-unweighted, C-weighted, D-unweighted, D-weighted.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[r_1,r_2,\\dots,r_8]$).\n\nAngle units are not applicable. No percentages are involved. All required quantities must be expressed in megaelectronvolts (MeV), rounded to $6$ decimal places as specified. The code must be self-contained and must not read any input.", "solution": "The problem is valid. It is scientifically grounded in standard statistical practices for model evaluation in the physical sciences, is well-posed with all necessary information provided, and is expressed objectively.\n\nThe task is to derive a principled weighting scheme for evaluating model predictions against experimental data with known, heteroscedastic uncertainties, and then to implement two metrics: the unweighted Root Mean Squared Error (RMSE) and a corresponding weighted version (WRMSE).\n\n### Derivation from First Principles\n\nThe foundation of our derivation is the assumption that each experimental measurement $y_i$ is a random variable drawn from a Gaussian (Normal) distribution. The mean of this distribution is the true, unknown physical value, which the model aims to predict with $\\hat{y}_i$. The standard deviation of this distribution is the reported experimental uncertainty, $\\sigma_i$. Under this assumption, the probability density of observing the value $y_i$, given the model's prediction $\\hat{y}_i$ and the uncertainty $\\sigma_i$, is:\n$$\nP(y_i | \\hat{y}_i, \\sigma_i) = \\frac{1}{\\sigma_i \\sqrt{2\\pi}} \\exp\\left( -\\frac{(y_i - \\hat{y}_i)^2}{2\\sigma_i^2} \\right)\n$$\nThe problem states that the observations are independent. Therefore, the total likelihood $\\mathcal{L}$ of observing the entire dataset $\\{y_i\\}_{i=1}^N$ is the product of the individual probabilities:\n$$\n\\mathcal{L}(\\{y_i\\} | \\{\\hat{y}_i, \\sigma_i\\}) = \\prod_{i=1}^{N} P(y_i | \\hat{y}_i, \\sigma_i) = \\prod_{i=1}^{N} \\frac{1}{\\sigma_i \\sqrt{2\\pi}} \\exp\\left( -\\frac{(y_i - \\hat{y}_i)^2}{2\\sigma_i^2} \\right)\n$$\nIn model fitting and evaluation, it is computationally and analytically more convenient to work with the logarithm of the likelihood, the log-likelihood $\\ln(\\mathcal{L})$. Maximizing the likelihood is equivalent to maximizing the log-likelihood.\n$$\n\\ln(\\mathcal{L}) = \\ln\\left( \\prod_{i=1}^{N} \\frac{1}{\\sigma_i \\sqrt{2\\pi}} \\exp\\left( -\\frac{(y_i - \\hat{y}_i)^2}{2\\sigma_i^2} \\right) \\right)\n$$\nUsing the properties of logarithms, $\\ln(ab) = \\ln(a) + \\ln(b)$ and $\\ln(e^x) = x$, we get:\n$$\n\\ln(\\mathcal{L}) = \\sum_{i=1}^{N} \\ln\\left( \\frac{1}{\\sigma_i \\sqrt{2\\pi}} \\right) + \\sum_{i=1}^{N} \\left( -\\frac{(y_i - \\hat{y}_i)^2}{2\\sigma_i^2} \\right)\n$$\n$$\n\\ln(\\mathcal{L}) = -\\frac{N}{2}\\ln(2\\pi) - \\sum_{i=1}^{N}\\ln(\\sigma_i) - \\frac{1}{2} \\sum_{i=1}^{N} \\frac{(y_i - \\hat{y}_i)^2}{\\sigma_i^2}\n$$\nWhen assessing a model, we seek to find model parameters that maximize this likelihood. The terms $-\\frac{N}{2}\\ln(2\\pi)$ and $-\\sum_{i=1}^{N}\\ln(\\sigma_i)$ are constant with respect to the model's predictions $\\hat{y}_i$. Therefore, maximizing $\\ln(\\mathcal{L})$ is equivalent to minimizing the remaining term, which is proportional to the chi-squared statistic, $\\chi^2$:\n$$\n\\chi^2 = \\sum_{i=1}^{N} \\frac{(y_i - \\hat{y}_i)^2}{\\sigma_i^2} = \\sum_{i=1}^{N} \\left(\\frac{y_i - \\hat{y}_i}{\\sigma_i}\\right)^2\n$$\nThis expression is a sum of squared residuals, where each squared residual $(y_i - \\hat{y}_i)^2$ is multiplied by a factor of $1/\\sigma_i^2$. This provides the principled weighting scheme: the weight $w_i$ for each squared residual should be the inverse of the variance of the corresponding measurement.\n$$\nw_i = \\frac{1}{\\sigma_i^2}\n$$\nThis is intuitively correct: measurements with higher precision (smaller $\\sigma_i$) have larger variance $\\sigma_i^2$, hence a larger weight $w_i$, and thus contribute more significantly to the overall metric.\n\n### Metric Definitions\n\nWith the weighting scheme established, we can define the required metrics. Let $N$ be the number of data points.\n\n1.  **Unweighted Root Mean Squared Error (RMSE)**: This metric treats all residuals equally. It is the square root of the mean of the squared residuals.\n    $$\n    \\text{RMSE} = \\sqrt{\\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2}\n    $$\n    The units of RMSE are the same as $y_i$ and $\\hat{y}_i$, which are megaelectronvolts (MeV).\n\n2.  **Weighted Root Mean Squared Error (WRMSE)**: This metric incorporates the derived weights $w_i = 1/\\sigma_i^2$. It is the square root of the weighted mean of the squared residuals. The weighted mean is calculated by summing the weighted values and dividing by the sum of the weights.\n    $$\n    \\text{WRMSE} = \\sqrt{\\frac{\\sum_{i=1}^{N} w_i (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{N} w_i}} = \\sqrt{\\frac{\\sum_{i=1}^{N} \\frac{1}{\\sigma_i^2} (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{N} \\frac{1}{\\sigma_i^2}}}\n    $$\n    The units of WRMSE are also in MeV. To verify this, examine the argument of the square root. The numerator term is $\\frac{(y_i - \\hat{y}_i)^2}{\\sigma_i^2}$, which has units of $\\frac{(\\text{MeV})^2}{(\\text{MeV})^2}$ and is thus dimensionless. The sum is also dimensionless. The denominator term is $1/\\sigma_i^2$, which has units of $1/(\\text{MeV})^2$. The sum has units of $1/(\\text{MeV})^2$. The fraction thus has units of $\\frac{\\text{dimensionless}}{1/(\\text{MeV})^2} = (\\text{MeV})^2$. The square root correctly yields MeV.\n\nAs a consistency check, if all uncertainties are equal, i.e., $\\sigma_i = \\sigma_0$ for all $i$, then $w_i = 1/\\sigma_0^2$ is a constant. The WRMSE formula becomes:\n$$\n\\text{WRMSE} = \\sqrt{\\frac{\\sum_{i=1}^{N} \\frac{1}{\\sigma_0^2} (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{N} \\frac{1}{\\sigma_0^2}}} = \\sqrt{\\frac{\\frac{1}{\\sigma_0^2} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2}{\\frac{N}{\\sigma_0^2}}} = \\sqrt{\\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2} = \\text{RMSE}\n$$\nThis confirms that for the homoscedastic case (constant variance), the WRMSE correctly reduces to the unweighted RMSE.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Calculates unweighted and weighted RMSE for nuclear mass predictions\n    based on a Gaussian heteroscedastic noise model.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test case A (varied uncertainties, typical residuals)\n        (\n            np.array([-60.234, -45.876, -30.112, -15.995, -8.321]),\n            np.array([-60.100, -45.700, -30.500, -16.100, -8.400]),\n            np.array([0.05, 0.10, 0.20, 0.15, 0.08])\n        ),\n        # Test case B (equal uncertainties, sanity check)\n        (\n            np.array([-12.000, -25.000, -37.000, -49.000]),\n            np.array([-12.100, -25.100, -36.900, -49.300]),\n            np.array([0.10, 0.10, 0.10, 0.10])\n        ),\n        # Test case C (large residual outliers paired with large uncertainties)\n        (\n            np.array([-80.000, -70.000, -65.000, -60.000, -55.000]),\n            np.array([-79.900, -69.800, -66.500, -58.500, -53.000]),\n            np.array([0.10, 0.10, 1.50, 1.20, 2.50])\n        ),\n        # Test case D (a very precise point dominates weighting)\n        (\n            np.array([-10.000, -20.000, -30.000, -40.000]),\n            np.array([-10.200, -20.150, -29.900, -39.850]),\n            np.array([0.01, 0.20, 0.20, 0.20])\n        )\n    ]\n\n    results = []\n    for y_true, y_pred, sigma in test_cases:\n        \n        # Calculate the residuals (differences between true and predicted values)\n        residuals = y_true - y_pred\n        \n        # 1. Unweighted Root Mean Squared Error (RMSE)\n        # RMSE = sqrt(mean(residuals^2))\n        mse = np.mean(np.square(residuals))\n        rmse = np.sqrt(mse)\n        results.append(rmse)\n        \n        # 2. Weighted Root Mean Squared Error (WRMSE)\n        # Weights are the inverse of the variance (sigma^2)\n        weights = 1.0 / np.square(sigma)\n        \n        # Weighted Mean Squared Error (WMSE) = sum(weights * residuals^2) / sum(weights)\n        weighted_squared_errors = weights * np.square(residuals)\n        wmse = np.sum(weighted_squared_errors) / np.sum(weights)\n        \n        # WRMSE = sqrt(WMSE)\n        wrmse = np.sqrt(wmse)\n        results.append(wrmse)\n\n    # Format the final list of results to 6 decimal places and print.\n    # The required output is a single line, comma-separated list in brackets.\n    output_str = f\"[{','.join(f'{r:.6f}' for r in results)}]\"\n    print(output_str)\n\nsolve()\n```", "id": "3568178"}, {"introduction": "This final practice takes you to the frontier of machine learning in science, where models not only predict but also guide the discovery process. You will design an \"acquisition function\" that uses a model's own uncertainty to strategically recommend the most informative future experiment for locating the neutron dripline. This exercise demonstrates how active learning transforms machine learning from a passive analysis tool into an active participant in scientific exploration, helping to prioritize and accelerate new discoveries [@problem_id:3568168].", "problem": "You are asked to design and implement a one-step, decision-theoretic acquisition function for proposing a single new measurement of the two-neutron separation energy to maximally reduce uncertainty in the two-neutron dripline location for a fixed proton number. Your program must compute and rank this acquisition score across a specified candidate set for multiple test cases.\n\nFor a fixed proton number $Z$, define the two-neutron separation energy $S_{2n}(Z,N)$ as the difference of nuclear binding energies, namely $S_{2n}(Z,N) = B(Z,N) - B(Z,N-2)$. The two-neutron dripline location for the element of proton number $Z$ is the largest neutron number $N_{\\mathrm{d}}$ such that $S_{2n}(Z,N_{\\mathrm{d}}) > 0$ and $S_{2n}(Z,N_{\\mathrm{d}}+2) \\le 0$. In this problem, you will work on a discrete neutron grid and consider $S_{2n}(Z,N)$ as a latent function $g(N)$ defined on a set of integer $N$ values. Assume a Gaussian process prior (equivalently, a joint Gaussian prior on the discrete grid) for $g(N)$ with mean vector $\\boldsymbol{\\mu}$ and covariance matrix $\\mathbf{K}$, together with additive independent Gaussian observation noise of variance $\\tau^2$.\n\nYou must derive, from first principles, a single-step acquisition score that quantifies the expected reduction in the posterior variance of the dripline location $N_{\\mathrm{d}}$ if one were to measure $g(N)$ at a candidate neutron number $N_{\\mathrm{c}}$, and then compute and rank that score for each candidate in the provided candidate set. Use the following foundational facts as your starting point:\n\n- If $\\begin{bmatrix} g(N_*) \\\\ g(N_{\\mathrm{c}}) \\end{bmatrix}$ is jointly Gaussian with covariance matrix entries $k_{**} = \\mathrm{Var}\\!\\left[g(N_*)\\right]$, $k_{\\mathrm{cc}} = \\mathrm{Var}\\!\\left[g(N_{\\mathrm{c}})\\right]$, and $k_{*\\mathrm{c}} = \\mathrm{Cov}\\!\\left[g(N_*), g(N_{\\mathrm{c}})\\right]$, and if a single noisy observation $y_{\\mathrm{c}} = g(N_{\\mathrm{c}}) + \\epsilon$ with $\\epsilon \\sim \\mathcal{N}(0,\\tau^2)$ is acquired, then the posterior variance at $N_*$ is reduced from $k_{**}$ to $k_{**} - \\dfrac{k_{*\\mathrm{c}}^2}{k_{\\mathrm{cc}} + \\tau^2}$.\n- If a scalar parameter $\\theta$ is implicitly defined by a root of a smooth function $h$ via $h(\\theta) = 0$ and one has a Gaussian uncertainty on $h$ at a nominal point, then the delta method gives the variance propagation approximation $\\mathrm{Var}(\\theta) \\approx \\dfrac{\\mathrm{Var}\\!\\left[h(\\theta_0)\\right]}{\\left(h'(\\theta_0)\\right)^2}$ for a suitable linearization point $\\theta_0$ where $h'(\\theta_0) \\neq 0$.\n\nOperationalize the above as follows. For a given test case:\n\n1. Fix a discrete grid of neutron numbers $N \\in \\{N_{\\min}, N_{\\min}+1, \\dots, N_{\\max}\\}$. Treat $g(N)$ on this grid as jointly Gaussian with mean $\\mu(N)$ and covariance $K(N,N')$ given by the squared-exponential kernel on the integer grid:\n$$\nK(N,N') = \\sigma_f^2 \\exp\\!\\left(-\\frac{(N - N')^2}{2 \\ell^2}\\right),\n$$\nwhere $\\sigma_f^2$ is the signal variance and $\\ell$ is the correlation length in neutron-number units. Measurement noise is $\\tau^2$.\n\n2. Define the nominal linearization index $N_*$ as the grid point that minimizes $|\\mu(N)|$ over the grid (i.e., the point of highest probability of being near the $S_{2n} = 0$ level set under the current mean). Approximate the slope $g'(N_*)$ by a finite difference on the grid:\n- If $N_*$ is strictly between $N_{\\min}$ and $N_{\\max}$, use the centered difference $g'(N_*) \\approx \\dfrac{\\mu(N_*+1) - \\mu(N_*-1)}{2}$.\n- If $N_* = N_{\\min}$, use the forward difference $g'(N_*) \\approx \\mu(N_{\\min}+1) - \\mu(N_{\\min})$.\n- If $N_* = N_{\\max}$, use the backward difference $g'(N_*) \\approx \\mu(N_{\\max}) - \\mu(N_{\\max}-1)$.\n\n3. For each candidate $N_{\\mathrm{c}}$ in the supplied candidate set $\\mathcal{C}$ (a subset of the grid), compute an acquisition score proportional to the expected reduction in $\\mathrm{Var}(N_{\\mathrm{d}})$ assuming a single observation at $N_{\\mathrm{c}}$ is taken and assimilated. Use the Gaussian conditioning fact to quantify the reduction in $\\mathrm{Var}\\!\\left[g(N_*)\\right]$ and the delta method to propagate this reduction to $\\mathrm{Var}(N_{\\mathrm{d}})$. To ensure numerical stability when $|g'(N_*)|$ is very small, regularize the denominator by adding a small $\\varepsilon > 0$ to $g'(N_*)^2$ with $\\varepsilon = 10^{-12}$.\n\n4. Rank the candidates in $\\mathcal{C}$ by descending acquisition score, breaking any ties by ascending $N_{\\mathrm{c}}$.\n\nYou must implement the above and run it on the following three test cases. In each case, all energies are in mega-electron-volts, but your code should work purely numerically; no unit conversion is needed. The final program output should be a single line containing a list of three lists, where each inner list is the candidate set sorted in descending order of acquisition score for that test case.\n\nTest Case A (happy path crossing):\n- Proton number $Z = 50$.\n- Neutron grid $N \\in \\{60, 61, \\dots, 90\\}$.\n- Mean function $\\mu(N) = 8.0 - 0.3 \\times (N - 60)$.\n- Kernel hyperparameters $\\sigma_f = 2.0$, $\\ell = 2.5$.\n- Observation noise variance $\\tau^2 = 0.16$.\n- Candidate set $\\mathcal{C} = \\{65, 80, 85, 86, 87, 90\\}$.\n\nTest Case B (all means positive on grid):\n- Proton number $Z = 20$.\n- Neutron grid $N \\in \\{20, 21, \\dots, 40\\}$.\n- Mean function $\\mu(N) = 12.0 - 0.1 \\times (N - 20)$.\n- Kernel hyperparameters $\\sigma_f = 1.5$, $\\ell = 3.0$.\n- Observation noise variance $\\tau^2 = 0.09$.\n- Candidate set $\\mathcal{C} = \\{20, 30, 40\\}$.\n\nTest Case C (all means negative on grid):\n- Proton number $Z = 28$.\n- Neutron grid $N \\in \\{52, 53, \\dots, 62\\}$.\n- Mean function $\\mu(N) = -2.0 - 0.1 \\times (N - 52)$.\n- Kernel hyperparameters $\\sigma_f = 1.0$, $\\ell = 2.0$.\n- Observation noise variance $\\tau^2 = 0.04$.\n- Candidate set $\\mathcal{C} = \\{52, 56, 60, 62\\}$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case and is itself a bracketed, comma-separated list of the candidate neutron numbers sorted by descending acquisition score. For example, a valid output format is $[[n_{1,1},n_{1,2}], [n_{2,1},n_{2,2},n_{2,3}], [n_{3,1}]]$, but with no spaces anywhere in the printed string. Concretely, the output must look like a single string of the form $[[\\cdots],[\\cdots],[\\cdots]]$ without spaces.", "solution": "The problem requires the design and implementation of a single-step decision-theoretic acquisition function. The goal is to identify a future measurement point, from a set of candidates, that is expected to maximally reduce the uncertainty in the location of the two-neutron dripline. The foundation of this problem lies in Bayesian modeling, specifically using a Gaussian Process (GP) to represent our knowledge about the two-neutron separation energy, $S_{2n}$.\n\nFirst, we formalize the problem. The two-neutron separation energy for a fixed proton number $Z$ is modeled as a latent function $g(N)$ over a discrete grid of neutron numbers $N$. This function is assumed to follow a Gaussian Process prior, which means that for any finite set of points $\\{N_1, N_2, \\dots, N_m\\}$, the vector of function values $[g(N_1), g(N_2), \\dots, g(N_m)]^T$ follows a multivariate Gaussian distribution. This distribution is fully specified by a mean function $\\mu(N)$ and a covariance function, or kernel, $K(N,N')$. The problem provides these as:\n- Mean function: $\\mu(N)$, specified for each test case.\n- Covariance function: A squared-exponential kernel, $K(N,N') = \\sigma_f^2 \\exp\\left(-\\frac{(N - N')^2}{2 \\ell^2}\\right)$, where $\\sigma_f^2$ is the signal variance and $\\ell$ is the correlation length.\n\nThe two-neutron dripline, $N_{\\mathrm{d}}$, is defined as the largest neutron number where $S_{2n}(Z,N_{\\mathrm{d}}) > 0$ and $S_{2n}(Z,N_{\\mathrm{d}}+2) \\le 0$. For a smoothly varying $S_{2n}$ function, this is approximated by the root-finding condition $g(N_{\\mathrm{d}}) = S_{2n}(Z,N_{\\mathrm{d}}) \\approx 0$.\n\nOur objective is to quantify the utility of making a new, noisy measurement $y_{\\mathrm{c}} = g(N_{\\mathrm{c}}) + \\epsilon$ at a candidate location $N_{\\mathrm{c}}$, where the noise $\\epsilon$ is Gaussian with mean $0$ and variance $\\tau^2$. The utility, or acquisition score, is defined as the expected reduction in the variance of the dripline location, $\\mathrm{Var}(N_{\\mathrm{d}})$.\n\nTo connect the variance of our function, $\\mathrm{Var}[g(N)]$, to the variance of the dripline location, $\\mathrm{Var}(N_{\\mathrm{d}})$, we employ the delta method. The dripline is implicitly defined by $g(N_{\\mathrm{d}}) = 0$. Linearizing $g(N)$ around a suitable point $N_*$, we have $g(N) \\approx g(N_*) + g'(N_*)(N - N_*)$. Setting $g(N_{\\mathrm{d}}) = 0$ yields $0 \\approx g(N_*) + g'(N_*)(N_{\\mathrm{d}} - N_*)$, which can be rearranged to give an estimate for the dripline location: $N_{\\mathrm{d}} \\approx N_* - \\frac{g(N_*)}{g'(N_*)}$. Propagating variance through this linearized expression gives the delta method approximation:\n$$\n\\mathrm{Var}(N_{\\mathrm{d}}) \\approx \\mathrm{Var}\\left(N_* - \\frac{g(N_*)}{g'(N_*)}\\right) = \\frac{\\mathrm{Var}[g(N_*)]}{(g'(N_*))^2}\n$$\nThe problem specifies that the linearization point $N_*$ should be the grid point where the prior mean $|\\mu(N)|$ is minimized. This is our current best guess for the location of the root. The slope $g'(N_*)$ is approximated by a finite difference of the mean function $\\mu(N)$ at $N_*$ using the provided rules.\n\nThe variance of $g(N_*)$ before any measurement is the prior variance, given by the kernel: $\\mathrm{Var}_{prior}[g(N_*)] = K(N_*, N_*) = k_{**}$.\nThe problem provides the standard formula for the posterior variance of $g(N_*)$ after a noisy measurement at $N_{\\mathrm{c}}$:\n$$\n\\mathrm{Var}_{post}[g(N_*)] = k_{**} - \\frac{k_{*\\mathrm{c}}^2}{k_{\\mathrm{cc}} + \\tau^2}\n$$\nwhere $k_{**} = K(N_*,N_*)$, $k_{\\mathrm{cc}} = K(N_{\\mathrm{c}},N_{\\mathrm{c}})$, and $k_{*\\mathrm{c}} = K(N_*,N_{\\mathrm{c}})$.\n\nThe reduction in the variance of $g(N_*)$ is therefore:\n$$\n\\Delta \\mathrm{Var}[g(N_*)] = \\mathrm{Var}_{prior}[g(N_*)] - \\mathrm{Var}_{post}[g(N_*)] = \\frac{k_{*\\mathrm{c}}^2}{k_{\\mathrm{cc}} + \\tau^2}\n$$\nThe acquisition score, $A(N_{\\mathrm{c}})$, is the expected reduction in the variance of the dripline location. Using the delta method, we propagate the reduction in $\\mathrm{Var}[g(N_*)]$ to a reduction in $\\mathrm{Var}(N_{\\mathrm{d}})$:\n$$\nA(N_{\\mathrm{c}}) = \\Delta \\mathrm{Var}(N_{\\mathrm{d}}) \\approx \\frac{\\Delta \\mathrm{Var}[g(N_*)]}{(g'(N_*))^2} = \\frac{1}{(g'(N_*))^2} \\left(\\frac{k_{*\\mathrm{c}}^2}{k_{\\mathrm{cc}} + \\tau^2}\\right)\n$$\nTo handle cases where the slope might be very close to zero, a small regularization term $\\varepsilon = 10^{-12}$ is added to the squared slope in the denominator. The final expression for the acquisition score is:\n$$\nA(N_{\\mathrm{c}}) = \\frac{k_{*\\mathrm{c}}^2}{((g'(N_*))^2 + \\varepsilon)(k_{\\mathrm{cc}} + \\tau^2)}\n$$\nFor the squared-exponential kernel, $k_{\\mathrm{cc}} = K(N_{\\mathrm{c}}, N_{\\mathrm{c}}) = \\sigma_f^2$ and $k_{*\\mathrm{c}} = K(N_*, N_{\\mathrm{c}}) = \\sigma_f^2 \\exp\\left(-\\frac{(N_* - N_{\\mathrm{c}})^2}{2 \\ell^2}\\right)$.\n\nThe overall algorithm for each test case is as follows:\n1.  Construct the full neutron grid $\\mathcal{N}$ from $N_{\\min}$ to $N_{\\max}$.\n2.  Compute the mean function $\\mu(N)$ for all $N \\in \\mathcal{N}$.\n3.  Identify the linearization point $N_* = \\arg\\min_{N \\in \\mathcal{N}} |\\mu(N)|$.\n4.  Calculate the slope $g'(N_*)$ by applying the specified finite difference rule (forward, backward, or centered) to $\\mu(N)$ at $N_*$.\n5.  For each candidate neutron number $N_{\\mathrm{c}}$ in the candidate set $\\mathcal{C}$:\n    a. Calculate the kernel terms $k_{\\mathrm{cc}} = \\sigma_f^2$ and $k_{*\\mathrm{c}} = \\sigma_f^2 \\exp\\left(-\\frac{(N_* - N_{\\mathrm{c}})^2}{2 \\ell^2}\\right)$.\n    b. Compute the acquisition score $A(N_{\\mathrm{c}})$ using the derived formula.\n6.  Rank the candidates $N_{\\mathrm{c}} \\in \\mathcal{C}$ in descending order of their scores $A(N_{\\mathrm{c}})$, breaking ties by choosing the smaller $N_{\\mathrm{c}}$. This produces the final sorted list for the test case.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all test cases.\n    \"\"\"\n\n    test_cases = [\n        # Test Case A\n        {\n            \"N_min\": 60, \"N_max\": 90,\n            \"mu_params\": {\"c\": 8.0, \"m\": -0.3, \"N0\": 60},\n            \"sigma_f\": 2.0, \"ell\": 2.5, \"tau_sq\": 0.16,\n            \"candidates\": [65, 80, 85, 86, 87, 90]\n        },\n        # Test Case B\n        {\n            \"N_min\": 20, \"N_max\": 40,\n            \"mu_params\": {\"c\": 12.0, \"m\": -0.1, \"N0\": 20},\n            \"sigma_f\": 1.5, \"ell\": 3.0, \"tau_sq\": 0.09,\n            \"candidates\": [20, 30, 40]\n        },\n        # Test Case C\n        {\n            \"N_min\": 52, \"N_max\": 62,\n            \"mu_params\": {\"c\": -2.0, \"m\": -0.1, \"N0\": 52},\n            \"sigma_f\": 1.0, \"ell\": 2.0, \"tau_sq\": 0.04,\n            \"candidates\": [52, 56, 60, 62]\n        }\n    ]\n    \n    epsilon = 1e-12\n    all_results = []\n\n    for case in test_cases:\n        N_min, N_max = case[\"N_min\"], case[\"N_max\"]\n        mu_params = case[\"mu_params\"]\n        sigma_f = case[\"sigma_f\"]\n        ell = case[\"ell\"]\n        tau_sq = case[\"tau_sq\"]\n        candidates = case[\"candidates\"]\n\n        # 1. Define the grid and mean function\n        N_grid = np.arange(N_min, N_max + 1)\n        mu_func = lambda N: mu_params[\"c\"] + mu_params[\"m\"] * (N - mu_params[\"N0\"])\n        mu_vec = mu_func(N_grid)\n\n        # 2. Find the linearization point N_star\n        idx_star = np.argmin(np.abs(mu_vec))\n        N_star = N_grid[idx_star]\n\n        # 3. Compute the slope g'(N_star)\n        if N_star == N_min:\n            # Forward difference\n            g_prime_N_star = mu_vec[1] - mu_vec[0]\n        elif N_star == N_max:\n            # Backward difference\n            g_prime_N_star = mu_vec[-1] - mu_vec[-2]\n        else:\n            # Centered difference\n            g_prime_N_star = (mu_vec[idx_star + 1] - mu_vec[idx_star - 1]) / 2.0\n            \n        g_prime_N_star_sq_reg = g_prime_N_star**2 + epsilon\n\n        # 4. Compute acquisition score for each candidate\n        scores = []\n        k_cc = sigma_f**2\n        \n        # Denominator is constant across candidates for a given test case\n        denominator = g_prime_N_star_sq_reg * (k_cc + tau_sq)\n\n        for Nc in candidates:\n            # Calculate k_star_c\n            k_star_c = sigma_f**2 * np.exp(-((N_star - Nc)**2) / (2 * ell**2))\n            \n            # Numerator\n            numerator = k_star_c**2\n            \n            # Acquisition score\n            score = numerator / denominator\n            scores.append((score, Nc))\n\n        # 5. Rank candidates\n        # Sort by score descending, then by Nc ascending for ties\n        scores.sort(key=lambda x: (-x[0], x[1]))\n        \n        ranked_candidates = [Nc for score, Nc in scores]\n        all_results.append(ranked_candidates)\n\n    # Final print statement in the exact required format\n    inner_results_str = [f\"[{','.join(map(str, r))}]\" for r in all_results]\n    final_str = f\"[{','.join(inner_results_str)}]\"\n    print(final_str)\n\nsolve()\n\n```", "id": "3568168"}]}