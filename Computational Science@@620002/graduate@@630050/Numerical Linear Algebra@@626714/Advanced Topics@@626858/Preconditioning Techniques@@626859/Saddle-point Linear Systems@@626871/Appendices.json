{"hands_on_practices": [{"introduction": "The defining characteristic of a saddle-point matrix is its indefiniteness. This exercise solidifies this fundamental concept by moving beyond abstract definitions to a concrete construction [@problem_id:3575826]. By building a specific saddle-point matrix and finding a vector that makes the associated quadratic form $z^T K z$ negative, you will gain a tangible understanding of the \"saddle\" nature of these systems.", "problem": "Consider the saddle-point block matrix $K \\in \\mathbb{R}^{3 \\times 3}$ of the form\n$$\nK \\;=\\; \\begin{bmatrix} A  B^{T} \\\\[4pt] B  -C \\end{bmatrix},\n$$\nwhere $A \\in \\mathbb{R}^{2 \\times 2}$ is symmetric, $B \\in \\mathbb{R}^{1 \\times 2}$, and $C \\in \\mathbb{R}^{1 \\times 1}$. Work within the discrete sets\n$$\n\\mathcal{A} \\;=\\; \\left\\{ \\begin{bmatrix} a  b \\\\ b  d \\end{bmatrix} \\,\\middle|\\, a,b,d \\in \\{0,1\\} \\right\\}, \n\\quad\n\\mathcal{B} \\;=\\; \\left\\{ \\begin{bmatrix} b_{1}  b_{2} \\end{bmatrix} \\,\\middle|\\, b_{1}, b_{2} \\in \\{0,1\\} \\right\\},\n\\quad\n\\mathcal{C} \\;=\\; \\{1\\},\n$$\nand impose the constraints that $A$ is symmetric positive definite (SPD) and $B \\neq \\mathbf{0}$. Use lexicographic order on the parameter tuples to remove ambiguity: for $A$ compare $(a,b,d)$; for $B$ compare $(b_{1},b_{2})$; for $C$ there is only $1$. Select the lexicographically minimal triple $(A,B,C) \\in \\mathcal{A} \\times \\mathcal{B} \\times \\mathcal{C}$ satisfying these constraints.\n\nStarting from the definition of definiteness via the quadratic form, and using only fundamental linear algebra facts (such as properties of quadratic forms, completion of the square, and the characterization of minimizers for strictly convex quadratic functions), construct a nonzero vector $z \\in \\mathbb{R}^{3}$ such that $z^{T} K z  0$ and argue that $K$ is symmetric indefinite by also identifying a vector that yields a positive quadratic form.\n\nFinally, for your constructed negative-certifying vector $z$, compute the exact value of the scalar $z^{T} K z$. Express your final answer as an exact integer. No rounding is required.", "solution": "The problem is found to be valid as it is mathematically well-defined, self-contained, and consistent.\n\nFirst, we must determine the specific matrix $K$ by selecting the lexicographically minimal triple $(A, B, C)$ from the given sets $\\mathcal{A} \\times \\mathcal{B} \\times \\mathcal{C}$ that satisfies the specified constraints.\n\nThe matrix $A \\in \\mathcal{A}$ is of the form $A = \\begin{bmatrix} a  b \\\\ b  d \\end{bmatrix}$ where $a, b, d \\in \\{0, 1\\}$. The constraint is that $A$ must be symmetric positive definite (SPD). A symmetric matrix is positive definite if and only if all its leading principal minors are positive.\nThe first leading principal minor is $\\det(A_1) = a$. For $A$ to be SPD, we must have $a  0$. Since $a \\in \\{0, 1\\}$, this implies $a=1$.\nThe second leading principal minor is $\\det(A_2) = \\det(A) = ad - b^2$. For $A$ to be SPD, we must have $ad - b^2  0$. Substituting $a=1$, this becomes $d - b^2  0$.\nWe need to find the lexicographically minimal tuple $(a,b,d)$ that satisfies these conditions. We already know $a=1$.\nWe test values for $b \\in \\{0, 1\\}$.\n- If $b=0$, the condition becomes $d - 0^2  0$, so $d  0$. Since $d \\in \\{0, 1\\}$, we must have $d=1$. This gives the tuple $(1,0,1)$.\n- If $b=1$, the condition becomes $d - 1^2  0$, so $d  1$. There is no solution for $d \\in \\{0, 1\\}$.\nTherefore, the only possible tuple is $(1,0,1)$, which is necessarily the lexicographically minimal one.\nThe resulting matrix is $A = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix}$, the $2 \\times 2$ identity matrix $I_2$.\n\nThe matrix $B \\in \\mathcal{B}$ is of the form $B = \\begin{bmatrix} b_1  b_2 \\end{bmatrix}$ where $b_1, b_2 \\in \\{0, 1\\}$. The constraints are $B \\neq \\mathbf{0}$ and that the tuple $(b_1, b_2)$ must be lexicographically minimal. The set $\\mathcal{B}$ consists of the tuples $(0,0), (0,1), (1,0), (1,1)$. Excluding the zero vector gives the set of possible tuples as $\\{(0,1), (1,0), (1,1)\\}$. The lexicographically minimal of these is $(0,1)$.\nThus, $B = \\begin{bmatrix} 0  1 \\end{bmatrix}$.\n\nThe matrix $C \\in \\mathcal{C}$ is drawn from the singleton set $\\mathcal{C} = \\{1\\}$. Thus, $C = [1]$.\n\nNow we assemble the saddle-point matrix $K$:\n$A = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix}$, $B = \\begin{bmatrix} 0  1 \\end{bmatrix}$, $C = [1]$.\nWe have $B^T = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$ and $-C = [-1]$.\n$$\nK = \\begin{bmatrix} A  B^T \\\\ B  -C \\end{bmatrix} = \\begin{bmatrix} 1  0  0 \\\\ 0  1  1 \\\\ 0  1  -1 \\end{bmatrix}.\n$$\nThe problem requires us to show that $K$ is symmetric indefinite. $K$ is symmetric by construction. To show it is indefinite, we must find vectors $z_1, z_2 \\in \\mathbb{R}^3$ such that $z_1^T K z_1  0$ and $z_2^T K z_2  0$.\n\nLet a generic vector $z \\in \\mathbb{R}^3$ be partitioned as $z = \\begin{bmatrix} x \\\\ y \\end{bmatrix}$, where $x = \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} \\in \\mathbb{R}^2$ and $y \\in \\mathbb{R}$. The quadratic form associated with $K$ is:\n$$\nz^T K z = \\begin{bmatrix} x^T  y \\end{bmatrix} \\begin{bmatrix} A  B^T \\\\ B  -C \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\end{bmatrix} = x^T A x + x^T B^T y + y B x - y C y = x^T A x + 2 y B x - y^2 C.\n$$\nSubstituting the specific matrices $A$, $B$, and $C$:\n$$\nz^T K z = x^T I_2 x + 2y \\begin{bmatrix} 0  1 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} - y^2(1) = (x_1^2 + x_2^2) + 2yx_2 - y^2.\n$$\nTo find a vector that yields a positive quadratic form, consider vectors of the form $z = \\begin{bmatrix} x \\\\ 0 \\end{bmatrix}$ with $x \\neq \\mathbf{0}$. For such vectors, $y=0$, and the quadratic form simplifies to $z^T K z = x^T A x$. Since $A=I_2$ is SPD, $x^T A x  0$ for all $x \\neq \\mathbf{0}$. For instance, let us choose $x = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$. This gives the vector $z_p = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}$.\nThe quadratic form value is $z_p^T K z_p = 1^2 + 0^2 + 2(0)(0) - 0^2 = 1  0$.\n\nNext, we must construct a nonzero vector $z$ such that $z^T K z  0$. We analyze the quadratic form $x_1^2 + x_2^2 + 2yx_2 - y^2$ by completing the square with respect to the $x_2$ variable:\n$$\nz^T K z = x_1^2 + (x_2^2 + 2yx_2 + y^2) - y^2 - y^2 = x_1^2 + (x_2 + y)^2 - 2y^2.\n$$\nTo make this expression negative, we can choose $x_1, x_2, y$ to make the positive terms vanish. We set:\n$x_1 = 0$\n$x_2 + y = 0 \\implies x_2 = -y$\nThis choice gives $z^T K z = 0^2 + 0^2 - 2y^2 = -2y^2$. To ensure $z$ is a nonzero vector, we can choose any nonzero value for $y$. Let us choose $y=1$. This implies $x_1=0$ and $x_2=-1$. The resulting vector is:\n$$\nz = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ y \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ -1 \\\\ 1 \\end{bmatrix}.\n$$\nThis vector is nonzero. With this choice of $z$, we have established that $z^T K z  0$. The existence of vectors yielding both positive and negative values confirms that the symmetric matrix $K$ is indefinite.\n\nFinally, we compute the exact value of the scalar $z^T K z$ for this constructed vector $z = \\begin{bmatrix} 0 \\\\ -1 \\\\ 1 \\end{bmatrix}$.\nUsing the completed-square expression:\n$$\nz^T K z = x_1^2 + (x_2 + y)^2 - 2y^2 = 0^2 + (-1 + 1)^2 - 2(1)^2 = 0 + 0 - 2 = -2.\n$$\nAlternatively, by direct matrix multiplication:\n$$\nz^T K z = \\begin{bmatrix} 0  -1  1 \\end{bmatrix} \\begin{bmatrix} 1  0  0 \\\\ 0  1  1 \\\\ 0  1  -1 \\end{bmatrix} \\begin{bmatrix} 0 \\\\ -1 \\\\ 1 \\end{bmatrix}\n$$\n$$\n= \\begin{bmatrix} (0)(1) + (-1)(0) + (1)(0)  (0)(0) + (-1)(1) + (1)(1)  (0)(0) + (-1)(1) + (1)(-1) \\end{bmatrix} \\begin{bmatrix} 0 \\\\ -1 \\\\ 1 \\end{bmatrix}\n$$\n$$\n= \\begin{bmatrix} 0  0  -2 \\end{bmatrix} \\begin{bmatrix} 0 \\\\ -1 \\\\ 1 \\end{bmatrix} = (0)(0) + (0)(-1) + (-2)(1) = -2.\n$$\nBoth methods yield the same result. The value is an exact integer as requested.", "answer": "$$\\boxed{-2}$$", "id": "3575826"}, {"introduction": "Iterative methods are central to solving large-scale saddle-point systems, but their performance is not guaranteed. This practice explores the classic Uzawa method, demonstrating how its convergence is critically linked to the spectral properties of the Schur complement matrix $S = B A^{-1} B^T$ [@problem_id:3575856]. By explicitly calculating the convergence interval for the step-size parameter $\\tau$ and verifying divergence outside of it, you will see firsthand why the theoretical analysis of iterative solvers is essential for practical success.", "problem": "Consider the saddle-point linear system with block matrix $K=\\begin{bmatrix}A  B^{T}\\\\ B  0\\end{bmatrix}$, where $A\\in\\mathbb{R}^{n\\times n}$ is symmetric positive definite and $B\\in\\mathbb{R}^{m\\times n}$. The Uzawa method with constant step size $\\tau0$ applied to this system induces a linear stationary iteration for the Lagrange multiplier error whose iteration matrix is $I-\\tau S$, where $S:=B A^{-1} B^{T}$ is the Schur complement. It is a well-tested fact of linear stationary iterations that convergence occurs if and only if the spectral radius of the iteration matrix is strictly less than $1$.\n\nUsing only these definitions and facts, verify divergence of Uzawa by working with the following explicit and scientifically consistent choice:\n- Take $A=\\mathrm{diag}(2,1)\\in\\mathbb{R}^{2\\times 2}$ and $B=\\begin{bmatrix}1  1\\end{bmatrix}\\in\\mathbb{R}^{1\\times 2}$.\n- Compute the Schur complement $S=B A^{-1} B^{T}$ and its largest eigenvalue $\\lambda_{\\max}(S)$.\n- Identify the open interval for $\\tau$ that ensures convergence of the Uzawa iteration.\n- Then choose the step size $\\tau=\\dfrac{3}{2}$ and demonstrate divergence by computing the spectral radius $\\rho(I-\\tau S)$ and verifying that it exceeds $1$.\n\nReport as your final answer the exact value of $\\rho(I-\\tau S)$. No rounding is required, and no units are involved.", "solution": "The problem is validated as self-contained, scientifically grounded, and well-posed. All provided data and definitions are standard in numerical linear algebra and are internally consistent. I will now proceed with the solution.\n\nThe problem asks us to analyze the convergence of the Uzawa method for a specific saddle-point linear system. The system matrix is given by $K=\\begin{bmatrix}A  B^{T}\\\\ B  0\\end{bmatrix}$, with $A=\\mathrm{diag}(2,1)\\in\\mathbb{R}^{2\\times 2}$ and $B=\\begin{bmatrix}1  1\\end{bmatrix}\\in\\mathbb{R}^{1\\times 2}$. The Uzawa iteration for the Lagrange multiplier error has an iteration matrix $I-\\tau S$, where $S = B A^{-1} B^{T}$ is the Schur complement and $\\tau  0$ is the step size. Convergence occurs if and only if the spectral radius $\\rho(I-\\tau S)  1$.\n\nFirst, we must compute the Schur complement $S$. The matrix $A$ is a diagonal matrix, so its inverse $A^{-1}$ is found by taking the reciprocal of its diagonal elements.\n$$A = \\begin{bmatrix} 2  0 \\\\ 0  1 \\end{bmatrix} \\implies A^{-1} = \\begin{bmatrix} \\frac{1}{2}  0 \\\\ 0  1 \\end{bmatrix}$$\nThe matrix $B$ is given as $B = \\begin{bmatrix} 1  1 \\end{bmatrix}$, and its transpose is $B^{T} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$.\n\nNow, we compute $S = B A^{-1} B^{T}$:\n$$S = \\begin{bmatrix} 1  1 \\end{bmatrix} \\begin{bmatrix} \\frac{1}{2}  0 \\\\ 0  1 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$$\nPerforming the matrix multiplications from left to right:\n$$B A^{-1} = \\begin{bmatrix} 1  1 \\end{bmatrix} \\begin{bmatrix} \\frac{1}{2}  0 \\\\ 0  1 \\end{bmatrix} = \\begin{bmatrix} (1)(\\frac{1}{2}) + (1)(0)  (1)(0) + (1)(1) \\end{bmatrix} = \\begin{bmatrix} \\frac{1}{2}  1 \\end{bmatrix}$$\nThen, we complete the computation of $S$:\n$$S = (B A^{-1}) B^{T} = \\begin{bmatrix} \\frac{1}{2}  1 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} = \\left[ (\\frac{1}{2})(1) + (1)(1) \\right] = \\left[ \\frac{1}{2} + 1 \\right] = \\left[ \\frac{3}{2} \\right]$$\nSo, the Schur complement $S$ is a $1 \\times 1$ matrix, which can be treated as the scalar $\\frac{3}{2}$.\n\nThe eigenvalue(s) of a scalar matrix is just the scalar itself. Therefore, the only eigenvalue of $S$ is $\\lambda(S) = \\frac{3}{2}$. The largest eigenvalue is $\\lambda_{\\max}(S) = \\frac{3}{2}$.\n\nNext, we identify the open interval for $\\tau$ that ensures convergence. The iteration matrix is $M = I - \\tau S$. Since $S$ is the scalar $\\frac{3}{2}$, the identity matrix $I$ is the scalar $1$, and the iteration matrix is the scalar $M = 1 - \\tau(\\frac{3}{2})$.\nThe spectral radius of a scalar matrix is its absolute value. Convergence requires $\\rho(M)  1$:\n$$ \\rho(I-\\tau S) = \\left| 1 - \\frac{3\\tau}{2} \\right|  1 $$\nThis inequality is equivalent to:\n$$ -1  1 - \\frac{3\\tau}{2}  1 $$\nWe solve this compound inequality for $\\tau$. First, for the left side:\n$$ -1  1 - \\frac{3\\tau}{2} \\implies \\frac{3\\tau}{2}  1 + 1 \\implies \\frac{3\\tau}{2}  2 \\implies \\tau  \\frac{4}{3} $$\nNext, for the right side:\n$$ 1 - \\frac{3\\tau}{2}  1 \\implies -\\frac{3\\tau}{2}  0 \\implies \\frac{3\\tau}{2}  0 \\implies \\tau  0 $$\nCombining these results, the Uzawa iteration converges for $\\tau \\in (0, \\frac{4}{3})$.\n\nThe problem asks us to choose the step size $\\tau = \\frac{3}{2}$. Since $\\frac{3}{2} = \\frac{9}{6}$ and $\\frac{4}{3} = \\frac{8}{6}$, we have $\\tau  \\frac{4}{3}$, so the chosen $\\tau$ is outside the convergence interval. We therefore expect divergence.\n\nTo demonstrate this, we compute the spectral radius $\\rho(I - \\tau S)$ for $\\tau = \\frac{3}{2}$.\nThe iteration matrix is $M = I - \\tau S$. With $S = \\frac{3}{2}$ and $\\tau = \\frac{3}{2}$, we have:\n$$ M = 1 - \\left(\\frac{3}{2}\\right) \\left(\\frac{3}{2}\\right) = 1 - \\frac{9}{4} = \\frac{4}{4} - \\frac{9}{4} = -\\frac{5}{4} $$\nThe spectral radius is the absolute value of this scalar:\n$$ \\rho(I - \\tau S) = \\left| -\\frac{5}{4} \\right| = \\frac{5}{4} $$\nSince $\\rho(I - \\tau S) = \\frac{5}{4}  1$, the convergence condition is violated, and the iteration diverges as predicted. The value to be reported is this spectral radius.", "answer": "$$\\boxed{\\frac{5}{4}}$$", "id": "3575856"}, {"introduction": "The performance of sophisticated solvers like MINRES is not magic; it is deeply rooted in the mathematical structure of the problem. This exercise connects the abstract inf-sup stability condition, often denoted by $\\beta$, to the concrete spectral properties of the saddle-point matrix and the resulting robustness of the MINRES algorithm [@problem_id:3575837]. Through a combination of analytical derivation and computational experiment, you will discover how a single theoretical constant can predict whether a numerical method will succeed or fail.", "problem": "Consider the saddle-point linear system matrix family $K(\\epsilon) \\in \\mathbb{R}^{(n+m)\\times(n+m)}$ defined by\n$$\nK(\\epsilon) = \\begin{bmatrix} A  B^{\\top} \\\\ B  -\\epsilon I_m \\end{bmatrix},\n$$\nwhere $A \\in \\mathbb{R}^{n\\times n}$ is symmetric positive definite, $B \\in \\mathbb{R}^{m\\times n}$, and $\\epsilon \\ge 0$. In a finite-dimensional setting, the stability of the pair $(A,B)$ is quantified by the infimum-supremum constant, which for the choice of the $A$-induced norm on the primal space and the Euclidean norm on the dual space is\n$$\n\\beta = \\inf_{q \\neq 0} \\sup_{v \\neq 0} \\frac{q^{\\top} B v}{\\|v\\|_{A}\\,\\|q\\|_2}, \\quad \\text{with } \\|v\\|_{A} = \\sqrt{v^{\\top} A v}.\n$$\nThe goal is to construct a family of examples for which the system can be analyzed in detail and to explore how the constant $\\beta$ controls the extremal eigenvalues of $K(\\epsilon)$ as $\\epsilon \\to 0^{+}$, and to identify parameter regimes under which the Minimum Residual method (MINRES) behaves robustly or deteriorates, based on spectral considerations alone.\n\nStart from the foundational facts of spectral theory for symmetric matrices, singular values for rectangular matrices, and the Schur complement. Avoid using any shortcut formulas that directly give the extremal eigenvalues or ready-made spectral bounds; instead, derive all conclusions from the definitions of $\\beta$ and the structure of $K(\\epsilon)$ as specified.\n\nWork in the special case $m=n$, and let $A=\\mathrm{diag}(a_1,\\dots,a_n)$ with $a_i0$ and $B=\\mathrm{diag}(d_1,\\dots,d_n)$ with $d_i \\ge 0$. In this setting:\n- Derive the relationship between $\\beta$ and the spectral properties of the symmetric positive semidefinite matrix $S = B A^{-1} B^{\\top}$, and explain how $\\beta$ is determined by $\\{a_i\\}$ and $\\{d_i\\}$.\n- Derive how the eigenvalues of $K(\\epsilon)$ decompose into $n$ independent $2\\times 2$ blocks and determine the extremal eigenvalues in terms of $\\epsilon$, $\\{a_i\\}$, and $\\{d_i\\}$.\n- Using these derivations, explain the limiting behavior of the smallest magnitude eigenvalue of $K(\\epsilon)$ as $\\epsilon \\to 0^{+}$, and express this behavior in terms of the constant $\\beta$.\n- Based on spectral theory of optimal Krylov subspace methods for symmetric matrices, justify a simple, quantifiable robustness criterion for MINRES that uses only the spectrum of $K(\\epsilon)$: declare MINRES to be robust if the ratio of the smallest to largest magnitude eigenvalue of $K(\\epsilon)$ exceeds a fixed threshold $\\tau0$, and deteriorating otherwise. For the purposes of this exercise, take $\\tau = 10^{-3}$.\n\nImplement a program that, for given parameters $\\{a_i\\}$, $\\{d_i\\}$, and $\\epsilon$, constructs $K(\\epsilon)$, computes the infimum-supremum constant $\\beta$, computes the smallest and largest magnitude eigenvalues of $K(\\epsilon)$, and classifies the MINRES robustness via the above criterion. The final output for each test case must be a list holding three quantities in the order: $[\\beta, \\lambda_{\\min}^{\\mathrm{abs}}(K(\\epsilon)), \\mathrm{robust}]$, where $\\lambda_{\\min}^{\\mathrm{abs}}(K(\\epsilon))$ denotes the smallest magnitude eigenvalue of $K(\\epsilon)$ and $\\mathrm{robust}$ is a boolean logical value defined by the threshold rule with $\\tau = 10^{-3}$.\n\nNo physical units are involved in this problem. Angles and percentages are not applicable. All matrix entries and results are real numbers.\n\nUse the following test suite of parameter values to exercise different regimes:\n- Test case $1$ (happy path, uniformly stable, small $\\epsilon$): $n=4$, $a=[1,2,3,4]$, $d=[1,1,1,1]$, $\\epsilon=10^{-2}$.\n- Test case $2$ (small infimum-supremum constant, very small $\\epsilon$): $n=4$, $a=[1,1,1,1]$, $d=[10^{-3},1,1,1]$, $\\epsilon=10^{-8}$.\n- Test case $3$ (zero infimum-supremum constant, small but nonzero $\\epsilon$): $n=4$, $a=[1,1,1,1]$, $d=[0,1,1,1]$, $\\epsilon=10^{-2}$.\n- Test case $4$ (zero infimum-supremum constant, $\\epsilon=0$, singular system): $n=4$, $a=[1,1,1,1]$, $d=[0,1,1,1]$, $\\epsilon=0$.\n- Test case $5$ (anisotropic but stable, extremely small $\\epsilon$): $n=4$, $a=[1,4,9,16]$, $d=[1,2,3,4]$, $\\epsilon=10^{-9}$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[result1,result2,result3]$). Each $resultj$ should be the list $[\\beta, \\lambda_{\\min}^{\\mathrm{abs}}(K(\\epsilon)), \\mathrm{robust}]$ for test case $j$, in the order listed above. All computations must be performed using real arithmetic with standard linear algebra routines.", "solution": "The problem statement has been validated and is deemed valid. It is scientifically grounded in the field of numerical linear algebra, well-posed for the specified special case, objective, and provides a complete and consistent setup for a rigorous mathematical derivation and subsequent implementation.\n\nThe solution proceeds in four stages as requested: deriving the relationship between the inf-sup constant $\\beta$ and the matrix properties, deriving the eigenvalues of $K(\\epsilon)$, analyzing the limiting spectral behavior as $\\epsilon \\to 0^{+}$, and applying the spectral robustness criterion for the MINRES method.\n\n**1. Relationship between $\\beta$ and the Schur Complement $S$**\n\nThe infimum-supremum constant is defined as\n$$\n\\beta = \\inf_{q \\neq 0} \\sup_{v \\neq 0} \\frac{q^{\\top} B v}{\\|v\\|_{A}\\,\\|q\\|_2}\n$$\nwhere $\\|v\\|_{A} = \\sqrt{v^{\\top} A v}$. Since $A \\in \\mathbb{R}^{n\\times n}$ is symmetric positive definite (SPD), its square root $A^{1/2}$ and inverse $A^{-1/2}$ exist and are also SPD. Let us analyze the inner supremum term for a fixed non-zero vector $q \\in \\mathbb{R}^m$. By performing a change of variables $w = A^{1/2}v$, we have $v = A^{-1/2}w$ and $\\|v\\|_A = \\sqrt{(A^{-1/2}w)^{\\top} A (A^{-1/2}w)} = \\sqrt{w^{\\top} A^{-1/2} A A^{-1/2} w} = \\sqrt{w^{\\top}w} = \\|w\\|_2$. The expression becomes:\n$$\n\\sup_{v \\neq 0} \\frac{q^{\\top} B v}{\\|v\\|_{A}} = \\sup_{w \\neq 0} \\frac{q^{\\top} B A^{-1/2} w}{\\|w\\|_2}\n$$\nThis is the definition of the Euclidean norm of the vector $(q^{\\top} B A^{-1/2})^{\\top} = A^{-1/2} B^{\\top} q$. Therefore,\n$$\n\\sup_{v \\neq 0} \\frac{q^{\\top} B v}{\\|v\\|_{A}} = \\|A^{-1/2} B^{\\top} q\\|_2 = \\sqrt{(A^{-1/2} B^{\\top} q)^{\\top}(A^{-1/2} B^{\\top} q)} = \\sqrt{q^{\\top} B A^{-1} B^{\\top} q}\n$$\nWe define the Schur complement of $A$ in $K(0)$ as $S = B A^{-1} B^{\\top}$. Since $A^{-1}$ is SPD, $S$ is symmetric positive semidefinite (SPSD). The expression simplifies to $\\sqrt{q^{\\top} S q}$. Substituting this back into the definition of $\\beta$:\n$$\n\\beta = \\inf_{q \\neq 0} \\frac{\\sqrt{q^{\\top} S q}}{\\|q\\|_2} = \\inf_{\\|q\\|_2=1} \\sqrt{q^{\\top} S q}\n$$\nThe square of this expression, $\\beta^2 = \\inf_{\\|q\\|_2=1} q^{\\top} S q$, is the Rayleigh quotient of $S$ minimized over the unit sphere, which is precisely the smallest eigenvalue of the SPSD matrix $S$, denoted $\\lambda_{\\min}(S)$. Thus, we have the fundamental relationship $\\beta = \\sqrt{\\lambda_{\\min}(S)}$.\n\nFor the special case where $m=n$, $A=\\mathrm{diag}(a_1,\\dots,a_n)$ with $a_i0$, and $B=\\mathrm{diag}(d_1,\\dots,d_n)$ with $d_i \\ge 0$, the matrices are diagonal.\n$A^{-1} = \\mathrm{diag}(1/a_1, \\dots, 1/a_n)$.\nThe Schur complement $S$ is also diagonal:\n$$\nS = B A^{-1} B^{\\top} = \\mathrm{diag}(d_i) \\mathrm{diag}(1/a_i) \\mathrm{diag}(d_i) = \\mathrm{diag}\\left(\\frac{d_1^2}{a_1}, \\dots, \\frac{d_n^2}{a_n}\\right)\n$$\nThe eigenvalues of a diagonal matrix are its diagonal entries. Therefore, $\\lambda_{\\min}(S) = \\min_{i \\in \\{1,\\dots,n\\}} \\left\\{\\frac{d_i^2}{a_i}\\right\\}$.\nThe inf-sup constant is then given by:\n$$\n\\beta = \\sqrt{\\min_{i} \\frac{d_i^2}{a_i}} = \\min_{i} \\frac{|d_i|}{\\sqrt{a_i}} = \\min_{i} \\frac{d_i}{\\sqrt{a_i}}\n$$\nsince $d_i \\ge 0$ and $a_i  0$.\n\n**2. Eigenvalue Decomposition of $K(\\epsilon)$**\n\nGiven that $A$ and $B$ are diagonal matrices, the system matrix $K(\\epsilon)$ has a special block structure:\n$$\nK(\\epsilon) = \\begin{bmatrix} \\mathrm{diag}(a_1, \\dots, a_n)  \\mathrm{diag}(d_1, \\dots, d_n) \\\\ \\mathrm{diag}(d_1, \\dots, d_n)  -\\epsilon I_n \\end{bmatrix}\n$$\nThe rows and columns of this matrix can be permuted to group the $i$-th components together. A suitable permutation matrix $P$ transforms $K(\\epsilon)$ into a block-diagonal matrix:\n$$\nP K(\\epsilon) P^{\\top} = \\mathrm{diag}(K_1(\\epsilon), K_2(\\epsilon), \\dots, K_n(\\epsilon))\n$$\nwhere each block $K_i(\\epsilon)$ is a $2\\times 2$ matrix:\n$$\nK_i(\\epsilon) = \\begin{bmatrix} a_i  d_i \\\\ d_i  -\\epsilon \\end{bmatrix}\n$$\nThe spectrum of $K(\\epsilon)$ is the union of the spectra of these smaller matrices $K_i(\\epsilon)$. The eigenvalues of $K_i(\\epsilon)$ are found from its characteristic polynomial $\\det(K_i(\\epsilon) - \\lambda I) = 0$:\n$$\n(a_i - \\lambda)(-\\epsilon - \\lambda) - d_i^2 = 0 \\\\\n\\lambda^2 + (\\epsilon - a_i)\\lambda - (a_i\\epsilon + d_i^2) = 0\n$$\nUsing the quadratic formula, the two eigenvalues for each block $i$ are:\n$$\n\\lambda_{i, \\pm} = \\frac{-( \\epsilon - a_i) \\pm \\sqrt{(\\epsilon - a_i)^2 - 4(1)(-(a_i\\epsilon + d_i^2))}}{2} = \\frac{a_i - \\epsilon \\pm \\sqrt{(a_i + \\epsilon)^2 + 4d_i^2}}{2}\n$$\nThese $2n$ values, $\\{\\lambda_{i, \\pm}\\}_{i=1}^n$, constitute the entire spectrum of $K(\\epsilon)$. Since $a_i  0$, $\\epsilon \\ge 0$, the term under the square root $\\sqrt{(a_i + \\epsilon)^2 + 4d_i^2} \\ge a_i + \\epsilon$. This implies that for each $i$, $\\lambda_{i, +}  0$ and $\\lambda_{i, -} \\le 0$, confirming that $K(\\epsilon)$ is an indefinite matrix. The extremal eigenvalues are $\\lambda_{\\max}(K(\\epsilon)) = \\max_i \\lambda_{i,+}$ and $\\lambda_{\\min}(K(\\epsilon)) = \\min_i \\lambda_{i,-}$.\n\n**3. Limiting Behavior of the Smallest Magnitude Eigenvalue**\n\nWe analyze the smallest magnitude eigenvalue, $\\lambda_{\\min}^{\\mathrm{abs}}(K(\\epsilon)) = \\min_{i, \\pm} |\\lambda_{i, \\pm}|$, as $\\epsilon \\to 0^{+}$. The behavior depends critically on whether $\\beta$ is zero or positive.\n\nCase 1: $\\beta = 0$. This implies from our derivation that there exists at least one index $k$ for which $d_k=0$. For this block, the matrix is $K_k(\\epsilon) = \\mathrm{diag}(a_k, -\\epsilon)$, and its eigenvalues are simply $\\lambda_{k,+} = a_k$ and $\\lambda_{k,-} = -\\epsilon$. Their magnitudes are $a_k  0$ and $\\epsilon \\ge 0$. For any other block $j$ with $d_j  0$, as $\\epsilon \\to 0^{+}$, the eigenvalues $\\lambda_{j,\\pm}$ approach non-zero limits $\\frac{1}{2}(a_j \\pm \\sqrt{a_j^2+4d_j^2})$. For sufficiently small $\\epsilon  0$, the smallest magnitude eigenvalue across all blocks will be $|\\lambda_{k,-}| = \\epsilon$. Thus, if $\\beta=0$, the smallest magnitude eigenvalue approaches $0$ linearly with $\\epsilon$: $\\lambda_{\\min}^{\\mathrm{abs}}(K(\\epsilon)) = \\epsilon$ for small $\\epsilon$.\n\nCase 2: $\\beta  0$. This implies that $d_i  0$ for all $i \\in \\{1, \\dots, n\\}$. We examine the magnitudes $|\\lambda_{i,\\pm}|$ as $\\epsilon \\to 0^{+}$.\n$$\n|\\lambda_{i,+}| = \\lambda_{i,+} = \\frac{1}{2} \\left( a_i - \\epsilon + \\sqrt{(a_i + \\epsilon)^2 + 4d_i^2} \\right) \\xrightarrow{\\epsilon \\to 0^+} \\frac{1}{2}\\left(a_i + \\sqrt{a_i^2+4d_i^2}\\right)  0\n$$\n$$\n|\\lambda_{i,-}| = -\\lambda_{i,-} = \\frac{1}{2} \\left( -(a_i - \\epsilon) + \\sqrt{(a_i + \\epsilon)^2 + 4d_i^2} \\right) \\xrightarrow{\\epsilon \\to 0^+} \\frac{1}{2}\\left(-a_i + \\sqrt{a_i^2+4d_i^2}\\right)  0\n$$\nSince both limits are positive constants, the overall smallest magnitude eigenvalue $\\lambda_{\\min}^{\\mathrm{abs}}(K(\\epsilon))$ converges to a positive constant:\n$$\n\\lim_{\\epsilon \\to 0^+} \\lambda_{\\min}^{\\mathrm{abs}}(K(\\epsilon)) = \\min_{i} \\left\\{ \\frac{1}{2}\\left(-a_i + \\sqrt{a_i^2+4d_i^2}\\right) \\right\\}  0\n$$\nThe value of this constant depends on all $\\{a_i\\}$ and $\\{d_i\\}$, not just on $\\beta$. However, the *behavior* is directly tied to $\\beta$: if $\\beta  0$, the system's spectrum is bounded away from zero uniformly in $\\epsilon$ as $\\epsilon \\to 0^+$, whereas if $\\beta=0$, an eigenvalue approaches zero.\n\n**4. MINRES Robustness Criterion**\n\nThe convergence rate of MINRES depends on the distribution of eigenvalues, and particularly on the condition number of the system matrix. The problem defines a spectral robustness criterion based on the ratio of the smallest to largest magnitude eigenvalues. Let $\\rho = \\lambda_{\\min}^{\\mathrm{abs}}(K(\\epsilon)) / \\lambda_{\\max}^{\\mathrm{abs}}(K(\\epsilon))$. MINRES is declared \"robust\" if $\\rho  \\tau = 10^{-3}$.\n\nFrom the eigenvalue formulas, we can see that for any $i$, $|\\lambda_{i,+}|  |\\lambda_{i,-}|$. Therefore, the absolute extremal eigenvalues are:\n$$\n\\lambda_{\\min}^{\\mathrm{abs}}(K(\\epsilon)) = \\min_{i} |\\lambda_{i,-}| = \\min_{i} \\frac{1}{2} \\left( -(a_i - \\epsilon) + \\sqrt{(a_i + \\epsilon)^2 + 4d_i^2} \\right)\n$$\n$$\n\\lambda_{\\max}^{\\mathrm{abs}}(K(\\epsilon)) = \\max_{i} |\\lambda_{i,+}| = \\max_{i} \\frac{1}{2} \\left( a_i - \\epsilon + \\sqrt{(a_i + \\epsilon)^2 + 4d_i^2} \\right)\n$$\nFor a given test case, we compute these quantities, calculate their ratio $\\rho$, and compare it to the threshold $\\tau=10^{-3}$ to classify the robustness. If $\\lambda_{\\max}^{\\mathrm{abs}}(K(\\epsilon)) = 0$, which can only happen for a zero matrix, the ratio is taken to be $0$. If $\\lambda_{\\min}^{\\mathrm{abs}}(K(\\epsilon)) = 0$ (i.e., the matrix is singular), the ratio is $0$ and the method is deteriorating.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite for the saddle-point system problem.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test case 1: happy path, uniformly stable, small epsilon\n        {'n': 4, 'a': [1, 2, 3, 4], 'd': [1, 1, 1, 1], 'epsilon': 1e-2},\n        # Test case 2: small inf-sup constant, very small epsilon\n        {'n': 4, 'a': [1, 1, 1, 1], 'd': [1e-3, 1, 1, 1], 'epsilon': 1e-8},\n        # Test case 3: zero inf-sup constant, small but nonzero epsilon\n        {'n': 4, 'a': [1, 1, 1, 1], 'd': [0, 1, 1, 1], 'epsilon': 1e-2},\n        # Test case 4: zero inf-sup constant, epsilon=0, singular system\n        {'n': 4, 'a': [1, 1, 1, 1], 'd': [0, 1, 1, 1], 'epsilon': 0},\n        # Test case 5: anisotropic but stable, extremely small epsilon\n        {'n': 4, 'a': [1, 4, 9, 16], 'd': [1, 2, 3, 4], 'epsilon': 1e-9}\n    ]\n\n    results = []\n    tau = 1e-3\n\n    for case in test_cases:\n        a_vec = np.array(case['a'], dtype=float)\n        d_vec = np.array(case['d'], dtype=float)\n        epsilon = float(case['epsilon'])\n        n = case['n']\n\n        # 1. Compute the inf-sup constant beta\n        # beta = min_i(d_i / sqrt(a_i))\n        beta = np.min(d_vec / np.sqrt(a_vec))\n\n        # 2. Compute the eigenvalues of K(epsilon)\n        all_eigenvalues = []\n        for i in range(n):\n            ai = a_vec[i]\n            di = d_vec[i]\n            \n            # lambda_{i, pm} = (a_i - epsilon +/- sqrt((a_i + epsilon)^2 + 4*d_i^2)) / 2\n            sqrt_term = np.sqrt((ai + epsilon)**2 + 4 * di**2)\n            lambda_plus = 0.5 * (ai - epsilon + sqrt_term)\n            lambda_minus = 0.5 * (ai - epsilon - sqrt_term)\n            \n            all_eigenvalues.extend([lambda_plus, lambda_minus])\n        \n        all_eigenvalues = np.array(all_eigenvalues)\n        abs_eigenvalues = np.abs(all_eigenvalues)\n        \n        # 3. Compute smallest and largest magnitude eigenvalues\n        lambda_min_abs = np.min(abs_eigenvalues)\n        lambda_max_abs = np.max(abs_eigenvalues)\n        \n        # 4. Classify MINRES robustness\n        # Handle division by zero for singular/zero matrices\n        if lambda_max_abs > 0:\n            ratio = lambda_min_abs / lambda_max_abs\n        else:\n            ratio = 0.0 # Define ratio for zero matrix\n        \n        robust = ratio > tau\n\n        # 5. Store the results for this case\n        results.append([beta, lambda_min_abs, robust])\n\n    # Final print statement in the exact required format.\n    # The map(str, ...) is used to correctly format the list-of-lists.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3575837"}]}