{"hands_on_practices": [{"introduction": "To truly grasp the Preconditioned Conjugate Gradient (PCG) method, it is essential to move beyond the abstract algorithm and perform the calculations by hand. This exercise provides a concrete opportunity to do just that by applying a single iteration of PCG to a small, manageable linear system. By working through the steps with a simple Jacobi (diagonal) preconditioner, you will gain a tangible understanding of how the initial residual is transformed into a new search direction, laying the groundwork for the method's iterative refinement. [@problem_id:1029864]", "problem": "Consider the linear system $ A \\mathbf{x} = \\mathbf{b} $, where  \n$$ A = \\begin{bmatrix} 4 & -2 & 0 \\\\ -2 & 3 & -1 \\\\ 0 & -1 & 2 \\end{bmatrix} \\quad \\text{and} \\quad \\mathbf{b} = \\begin{bmatrix} 4 \\\\ -3 \\\\ 2 \\end{bmatrix}. $$  \nApply the preconditioned conjugate gradient (PCG) method with diagonal scaling (Jacobi preconditioner), starting from the initial guess $ \\mathbf{x}_0 = \\mathbf{0} $. After one iteration, compute the 2-norm of the residual vector. The Jacobi preconditioner $ M $ is the diagonal part of $ A $, so $ M = \\operatorname{diag}(4, 3, 2) $.", "solution": "1. Initial residual  \n$$r_0 = b - A x_0 = \\begin{bmatrix}4\\\\-3\\\\2\\end{bmatrix}.$$\n2. Preconditioned residual  \n$$M^{-1} = \\operatorname{diag}\\bigl(\\tfrac14,\\tfrac13,\\tfrac12\\bigr),\\qquad \nz_0 = M^{-1}r_0 = \\begin{bmatrix}1\\\\-1\\\\1\\end{bmatrix}.$$\n3. Search direction  \n$$p_0 = z_0 = \\begin{bmatrix}1\\\\-1\\\\1\\end{bmatrix}.$$\n4. Matrix–vector product  \n$$A p_0 = \\begin{bmatrix}4&-2&0\\\\-2&3&-1\\\\0&-1&2\\end{bmatrix}\n\\begin{bmatrix}1\\\\-1\\\\1\\end{bmatrix}\n=\\begin{bmatrix}6\\\\-6\\\\3\\end{bmatrix}.$$\n5. Step length  \n$$\\alpha_0 = \\frac{r_0^T z_0}{p_0^T A p_0}\n=\\frac{9}{15}=\\frac35.$$\n6. Updated residual  \n$$r_1 = r_0 - \\alpha_0 A p_0\n=\\begin{bmatrix}4\\\\-3\\\\2\\end{bmatrix}\n-\\frac35\\begin{bmatrix}6\\\\-6\\\\3\\end{bmatrix}\n=\\begin{bmatrix}0.4\\\\0.6\\\\0.2\\end{bmatrix}.$$\n7. 2-norm of the residual  \n$$\\|r_1\\|_2 = \\sqrt{0.4^2 + 0.6^2 + 0.2^2}\n= \\frac{\\sqrt{14}}{5}.$$", "answer": "$$\\boxed{\\frac{\\sqrt{14}}{5}}$$", "id": "1029864"}, {"introduction": "While a single manual iteration is instructive, the true advantage of preconditioning emerges when solving larger, more challenging systems. This hands-on coding practice transitions from manual calculation to implementation, allowing you to directly compare the convergence of the standard Conjugate Gradient (CG) method against its preconditioned version (PCG). By testing these algorithms on systems with varying spectral properties, such as those arising from discretizing differential equations, you will empirically verify and quantify the dramatic acceleration that a simple preconditioner can provide. [@problem_id:2382390]", "problem": "Consider a family of linear systems defined by a real, symmetric, positive definite matrix $A \\in \\mathbb{R}^{n \\times n}$ and a right-hand side vector $b \\in \\mathbb{R}^{n}$. Let $x_{0} = 0$ be the initial guess. For a given tolerance $\\varepsilon > 0$ and maximum iteration count $k_{\\max} \\in \\mathbb{N}$, define the stopping criterion for an approximate solution $x_{k}$ at iteration $k$ by the relative residual condition $\\lVert b - A x_{k} \\rVert_{2} / \\lVert b \\rVert_{2} \\le \\varepsilon$. Let search directions be mutually $A$-conjugate, and at each iteration $k$ let the new approximation $x_{k}$ be the unique element of the affine subspace $x_{0} + \\mathcal{K}_{k}(A, r_{0})$ that minimizes the energy functional $\\tfrac{1}{2} x^{\\mathsf{T}} A x - b^{\\mathsf{T}} x$, where $r_{0} = b - A x_{0}$ and $\\mathcal{K}_{k}(A, r_{0}) = \\operatorname{span}\\{r_{0}, A r_{0}, \\dots, A^{k-1} r_{0}\\}$. Consider two choices for an auxiliary symmetric positive definite matrix $M$: the identity $M = I$ and the Jacobi choice $M = \\operatorname{diag}(A)$, interpreted as a left preconditioning in the sense that one equivalently considers $M^{-1} A x = M^{-1} b$ while preserving the same iterate generation principle.\n\nFor each test case below, starting from $x_{0} = 0$, determine the smallest iteration counts $k_{I}$ and $k_{J}$ that satisfy the stopping criterion under the choices $M = I$ and $M = \\operatorname{diag}(A)$, respectively, with the same tolerance $\\varepsilon$ and iteration cap $k_{\\max}$. Report, for each test case, the integer difference $k_{I} - k_{J}$.\n\nTest Suite:\n- Case $1$ (one-dimensional Laplacian): Let $n = 50$. Define $A \\in \\mathbb{R}^{n \\times n}$ by $A_{ii} = 2$ for $i = 1,\\dots,n$, $A_{i,i+1} = A_{i+1,i} = -1$ for $i = 1,\\dots,n-1$, and all other entries zero. Let $b \\in \\mathbb{R}^{n}$ be given by $b_{i} = 1$ for all $i$. Let $\\varepsilon = 10^{-8}$ and $k_{\\max} = n$.\n- Case $2$ (two-dimensional Laplacian on a square grid): Let $N = 20$ and $n = N^{2}$. Use lexicographic ordering of interior grid points. Define $A \\in \\mathbb{R}^{n \\times n}$ by the five-point stencil with $A_{ii} = 4$ and $A_{ij} = -1$ if nodes $i$ and $j$ are unit-distance neighbors in the grid (north, south, east, west), and $A_{ij} = 0$ otherwise. Let $b \\in \\mathbb{R}^{n}$ be given by $b_{i} = 1$ for all $i$. Let $\\varepsilon = 10^{-8}$ and $k_{\\max} = n$.\n- Case $3$ (diagonal system with widely spread spectrum): Let $n = 50$ and $A = \\operatorname{diag}(d_{1},\\dots,d_{n})$ with $d_{i} = 10^{6} - (10^{6} - 1)\\,\\frac{i-1}{n-1}$ so that $d_{1} = 10^{6}$ and $d_{n} = 1$. Let $b \\in \\mathbb{R}^{n}$ be given by $b_{i} = 1$ for all $i$. Let $\\varepsilon = 10^{-10}$ and $k_{\\max} = n$.\n- Case $4$ (scalar system): Let $n = 1$, $A = [3]$, $b = [1]$, $\\varepsilon = 10^{-12}$, and $k_{\\max} = 1$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of Cases $1$ through $4$. For example, an output for four cases must have the form $[r_{1},r_{2},r_{3},r_{4}]$, where each $r_{j}$ is the integer value of $k_{I} - k_{J}$ for Case $j$.", "solution": "The problem requires the implementation and comparison of the standard Conjugate Gradient (CG) method and the Jacobi preconditioned Conjugate Gradient (PCG) method for solving several systems of linear equations $A x = b$. For each case, the matrix $A$ is specified to be real, symmetric, and positive definite (SPD), which is the fundamental requirement for the convergence of the CG method. We begin from a null initial guess, $x_0 = 0$.\n\nThe Conjugate Gradient method is an iterative algorithm designed to solve such systems. It operates by generating a sequence of approximate solutions $x_k$ that minimize the energy functional $\\phi(x) = \\frac{1}{2} x^{\\mathsf{T}} A x - b^{\\mathsf{T}} x$ over an expanding sequence of subspaces. The minimum of this functional coincides with the solution to $A x = b$. Specifically, at iteration $k$, the solution $x_k$ is found in the affine subspace $x_0 + \\mathcal{K}_k(A, r_0)$, where $\\mathcal{K}_k(A, r_0) = \\operatorname{span}\\{r_0, A r_0, \\dots, A^{k-1} r_0\\}$ is the $k$-th Krylov subspace generated by $A$ and the initial residual $r_0 = b - A x_0$. Since $x_0=0$, we have $r_0=b$.\n\nThe standard CG algorithm (corresponding to the choice of preconditioner $M=I$) proceeds as follows, starting with $x_0=0$, $r_0=b$, and $p_0=r_0$:\nFor $k = 0, 1, 2, \\dots$ until convergence:\n$$ \\alpha_k = \\frac{r_k^{\\mathsf{T}} r_k}{p_k^{\\mathsf{T}} A p_k} $$\n$$ x_{k+1} = x_k + \\alpha_k p_k $$\n$$ r_{k+1} = r_k - \\alpha_k A p_k $$\n$$ \\beta_k = \\frac{r_{k+1}^{\\mathsf{T}} r_{k+1}}{r_k^{\\mathsf{T}} r_k} $$\n$$ p_{k+1} = r_{k+1} + \\beta_k p_k $$\nThe process terminates when the relative residual $\\lVert r_{k+1} \\rVert_2 / \\lVert b \\rVert_2$ is less than or equal to a given tolerance $\\varepsilon$. The number of iterations is then $k+1$.\n\nPreconditioning is a technique used to accelerate the convergence of iterative methods. It transforms the original system into an equivalent one that is better conditioned. For a symmetric positive definite preconditioner $M$, the preconditioned system is often viewed as $M^{-1} A x = M^{-1} b$. While the matrix $M^{-1}A$ is generally not symmetric, the PCG algorithm is formulated in a way that preserves the necessary symmetries implicitly. The algorithm involves an auxiliary step of solving a system with the matrix $M$ at each iteration. With $M$ being diagonal (Jacobi preconditioning), this step is computationally trivial.\n\nThe PCG algorithm, starting with $x_0=0$ and $r_0=b$, is:\nSolve $M z_0 = r_0$ for $z_0$.\nSet $p_0 = z_0$.\nFor $k = 0, 1, 2, \\dots$ until convergence:\n$$ \\alpha_k = \\frac{r_k^{\\mathsf{T}} z_k}{p_k^{\\mathsf{T}} A p_k} $$\n$$ x_{k+1} = x_k + \\alpha_k p_k $$\n$$ r_{k+1} = r_k - \\alpha_k A p_k $$\nSolve $M z_{k+1} = r_{k+1}$ for $z_{k+1}$.\n$$ \\beta_k = \\frac{r_{k+1}^{\\mathsf{T}} z_{k+1}}{r_k^{\\mathsf{T}} z_k} $$\n$$ p_{k+1} = z_{k+1} + \\beta_k p_k $$\nThe stopping criterion is the same as for the standard CG method. We are tasked to find the smallest iteration counts, $k_I$ for $M=I$ (standard CG) and $k_J$ for $M=\\operatorname{diag}(A)$ (Jacobi PCG), and report the difference $k_I - k_J$.\n\nThe specific test cases are:\n1.  **Case 1 (1D Laplacian)**: $A \\in \\mathbb{R}^{50 \\times 50}$ is a tridiagonal matrix with $2$ on the diagonal and $-1$ on the adjacent off-diagonals. Here, the preconditioner is $M = \\operatorname{diag}(A) = 2I$. This is a simple scaling of the identity matrix.\n2.  **Case 2 (2D Laplacian)**: $A \\in \\mathbb{R}^{400 \\times 400}$ represents the five-point stencil on a $20 \\times 20$ grid. The diagonal entries are $4$. The preconditioner is $M = \\operatorname{diag}(A) = 4I$. Similar to Case $1$, this is a scaling of the identity.\n3.  **Case 3 (Diagonal system)**: $A \\in \\mathbb{R}^{50 \\times 50}$ is a diagonal matrix with eigenvalues spaced linearly from $10^6$ to $1$. This matrix is ill-conditioned. The preconditioner is $M = \\operatorname{diag}(A) = A$.\n4.  **Case 4 (Scalar system)**: A $1 \\times 1$ system with $A = [3]$. The preconditioner is $M = \\operatorname{diag}(A) = A = [3]$.\n\nFor each case, we will implement both algorithms and run them with the specified parameters to find $k_I$ and $k_J$, and then compute their difference. In cases where the preconditioner $M$ is a multiple of the identity matrix, $M=cI$, the PCG algorithm yields the same sequence of iterates $x_k$ as the standard CG applied to the scaled system $(A/c)x = (b/c)$, which in turn produces the same iterates as the standard CG on the original system. Thus, we expect $k_I = k_J$ for Cases $1$ and $2$. For Case $3$, since $M=A$, the PCG algorithm should converge in a single iteration, $k_J=1$. For Case $4$, the problem is scalar and both methods will find the exact solution in one step, thus $k_I = k_J = 1$. The provided Python code implements this logic to determine the required values.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import sparse\n\ndef conjugate_gradient(A, b, epsilon, k_max):\n    \"\"\"\n    Solves Ax=b using the Conjugate Gradient method.\n    Starts with x_0 = 0.\n    \"\"\"\n    n = A.shape[0]\n    x = np.zeros(n, dtype=np.float64)\n    r = b.copy()\n    p = r.copy()\n\n    b_norm_sq = np.dot(b, b)\n    if b_norm_sq == 0:\n        return 0\n\n    # Initial residual check (for x_0 = 0, r_0 = b)\n    if np.dot(r, r) / b_norm_sq <= epsilon**2:\n        return 0\n        \n    rs_old = np.dot(r, r)\n    \n    for k in range(k_max):\n        Ap = A @ p\n        alpha = rs_old / np.dot(p, Ap)\n        x += alpha * p\n        r -= alpha * Ap\n        \n        rs_new = np.dot(r, r)\n        if np.sqrt(rs_new / b_norm_sq) <= epsilon:\n            return k + 1\n            \n        p = r + (rs_new / rs_old) * p\n        rs_old = rs_new\n        \n    return k_max\n\ndef preconditioned_conjugate_gradient(A, b, M_inv_op, epsilon, k_max):\n    \"\"\"\n    Solves Ax=b using the Preconditioned Conjugate Gradient method.\n    Starts with x_0 = 0.\n    M_inv_op is a function that computes M^{-1}v.\n    \"\"\"\n    n = A.shape[0]\n    x = np.zeros(n, dtype=np.float64)\n    r = b.copy()\n\n    b_norm_sq = np.dot(b, b)\n    if b_norm_sq == 0:\n        return 0\n\n    if np.sqrt(np.dot(r, r) / b_norm_sq) <= epsilon:\n        return 0\n\n    z = M_inv_op(r)\n    p = z.copy()\n    rz_old = np.dot(r, z)\n\n    for k in range(k_max):\n        Ap = A @ p\n        alpha = rz_old / np.dot(p, Ap)\n        x += alpha * p\n        r -= alpha * Ap\n\n        if np.sqrt(np.dot(r, r) / b_norm_sq) <= epsilon:\n            return k + 1\n            \n        z = M_inv_op(r)\n        rz_new = np.dot(r, z)\n        \n        beta = rz_new / rz_old\n        p = z + beta * p\n        rz_old = rz_new\n        \n    return k_max\n\ndef solve():\n    \"\"\"\n    Defines and solves the test cases as per the problem description.\n    \"\"\"\n    test_cases = [\n        {\n            \"name\": \"Case 1: 1D Laplacian\",\n            \"n\": 50,\n            \"epsilon\": 1e-8,\n            \"k_max_factor\": 1.0,\n        },\n        {\n            \"name\": \"Case 2: 2D Laplacian\",\n            \"N\": 20,\n            \"epsilon\": 1e-8,\n            \"k_max_factor\": 1.0,\n        },\n        {\n            \"name\": \"Case 3: Diagonal system\",\n            \"n\": 50,\n            \"epsilon\": 1e-10,\n            \"k_max_factor\": 1.0,\n        },\n        {\n            \"name\": \"Case 4: Scalar system\",\n            \"n\": 1,\n            \"epsilon\": 1e-12,\n            \"k_max_factor\": 1.0,\n        },\n    ]\n\n    results = []\n    \n    # Case 1\n    case = test_cases[0]\n    n = case[\"n\"]\n    k_max = int(n * case[\"k_max_factor\"])\n    A1 = sparse.diags([-1, 2, -1], [-1, 0, 1], shape=(n, n), format=\"csr\")\n    b1 = np.ones(n, dtype=np.float64)\n    M1_inv_op = lambda r: r / 2.0\n    k_I = conjugate_gradient(A1, b1, case[\"epsilon\"], k_max)\n    k_J = preconditioned_conjugate_gradient(A1, b1, M1_inv_op, case[\"epsilon\"], k_max)\n    results.append(k_I - k_J)\n    \n    # Case 2\n    case = test_cases[1]\n    N = case[\"N\"]\n    n = N * N\n    k_max = int(n * case[\"k_max_factor\"])\n    T = sparse.diags([-1, 4, -1], [-1, 0, 1], shape=(N, N), format=\"csr\")\n    A2_block_diag = sparse.block_diag([T] * N, format=\"csr\")\n    off_diag_vals = np.full(n - N, -1.0)\n    A2_off_diag = sparse.diags([off_diag_vals, off_diag_vals], [-N, N], shape=(n, n), format=\"csr\")\n    A2 = A2_block_diag + A2_off_diag\n    b2 = np.ones(n, dtype=np.float64)\n    M2_inv_op = lambda r: r / 4.0\n    k_I = conjugate_gradient(A2, b2, case[\"epsilon\"], k_max)\n    k_J = preconditioned_conjugate_gradient(A2, b2, M2_inv_op, case[\"epsilon\"], k_max)\n    results.append(k_I - k_J)\n\n    # Case 3\n    case = test_cases[2]\n    n = case[\"n\"]\n    k_max = int(n * case[\"k_max_factor\"])\n    d = 1e6 - (1e6 - 1) * np.arange(n, dtype=np.float64) / (n - 1)\n    A3 = sparse.diags(d, 0, shape=(n, n), format=\"csr\")\n    b3 = np.ones(n, dtype=np.float64)\n    A3_diag = d\n    M3_inv_op = lambda r: r / A3_diag\n    k_I = conjugate_gradient(A3, b3, case[\"epsilon\"], k_max)\n    k_J = preconditioned_conjugate_gradient(A3, b3, M3_inv_op, case[\"epsilon\"], k_max)\n    results.append(k_I - k_J)\n\n    # Case 4\n    case = test_cases[3]\n    n = case[\"n\"]\n    k_max = int(n * case[\"k_max_factor\"])\n    A4 = np.array([[3.0]], dtype=np.float64)\n    b4 = np.array([1.0], dtype=np.float64)\n    M4_inv_op = lambda r: r / 3.0\n    k_I = conjugate_gradient(A4, b4, case[\"epsilon\"], k_max)\n    k_J = preconditioned_conjugate_gradient(A4, b4, M4_inv_op, case[\"epsilon\"], k_max)\n    results.append(k_I - k_J)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2382390"}, {"introduction": "The remarkable speed-up you observed in the previous exercise is not a coincidence; it is a direct consequence of how the preconditioner alters the eigenvalue spectrum of the system matrix. This problem delves into the theoretical heart of PCG's efficiency, challenging you to construct a system where the preconditioned operator $M^{-1}A$ has a specific, limited number of distinct eigenvalues. By doing so, you can rigorously prove from first principles that the PCG algorithm is guaranteed to find the exact solution in a correspondingly small number of steps, revealing the deep connection between spectral properties and convergence rates. [@problem_id:2427437]", "problem": "Consider solving a linear system with the Conjugate Gradient (CG) method and its preconditioned variant. Construct explicit matrices $A \\in \\mathbb{R}^{5 \\times 5}$ and $M \\in \\mathbb{R}^{5 \\times 5}$ that are both symmetric positive definite, such that the preconditioned operator $M^{-1}A$ has exactly $2$ distinct eigenvalues. Verify the eigenvalue property by direct reasoning from your construction. Then consider applying the Preconditioned Conjugate Gradient (PCG) method, with preconditioner $M$, to the system $A x = b$ for arbitrary $b \\in \\mathbb{R}^{5}$ and arbitrary initial guess $x_0 \\in \\mathbb{R}^{5}$. Provide a rigorous justification, based on first principles of linear algebra and the algorithm’s defining properties, that PCG terminates with the exact solution in a bounded number of iterations that does not depend on $b$ or $x_0$. Determine the minimal integer $k^\\star$ with this property. \n\nYour final answer must be the value of $k^\\star$ only. No rounding is required.", "solution": "The problem requires us to construct specific symmetric positive definite (SPD) matrices $A$ and $M$ of size $5 \\times 5$ such that the preconditioned operator $M^{-1}A$ possesses exactly $2$ distinct eigenvalues. Subsequently, we must provide a rigorous justification for the fact that the Preconditioned Conjugate Gradient (PCG) method for the system $A x = b$ converges to the exact solution in a finite number of iterations, $k^\\star$, which is independent of the choice of the right-hand side $b$ and the initial guess $x_0$. Finally, we must determine this minimal bound $k^\\star$.\n\nFirst, we construct the required matrices $A$ and $M$. A simple and effective construction involves diagonal matrices. Let $M$ be the $5 \\times 5$ identity matrix, $M=I_5$. The identity matrix is symmetric, and all its eigenvalues are $1$, so it is positive definite.\nNext, let us construct the matrix $A$. To ensure $M^{-1}A$ has two distinct eigenvalues, and given our choice of $M=I_5$, the matrix $A$ itself must have two distinct eigenvalues. We also need $A$ to be symmetric and positive definite. A diagonal matrix with positive entries on the diagonal satisfies these requirements. We can choose:\n$$\nA = \\text{diag}(1, 1, 1, 2, 2)\n$$\nThis matrix $A$ is symmetric by construction. Its eigenvalues are its diagonal entries, which are $1$ and $2$. Since all eigenvalues are positive, $A$ is positive definite.\nThe preconditioned operator is $M^{-1}A = I_5^{-1}A = A$. The eigenvalues of $M^{-1}A$ are therefore $\\{1, 1, 1, 2, 2\\}$. The set of distinct eigenvalues is $\\{\\lambda_1, \\lambda_2\\} = \\{1, 2\\}$. Thus, there are exactly $2$ distinct eigenvalues, as required by the problem statement. Our construction of $A$ and $M$ is valid.\n\nNow, we must analyze the convergence of the PCG method. The PCG algorithm for solving the system $A x = b$ with an SPD preconditioner $M$ is mathematically equivalent to applying the standard Conjugate Gradient (CG) algorithm to a transformed linear system. Since $M$ is SPD, it has a unique Cholesky factorization $M = L L^T$, where $L$ is a nonsingular lower triangular matrix.\n\nWe can transform the original system $A x = b$ as follows:\n$$\nA x = b \\implies (L^{-1} A L^{-T}) (L^T x) = L^{-1} b\n$$\nLet us define $\\hat{A} = L^{-1} A L^{-T}$, $\\hat{x} = L^T x$, and $\\hat{b} = L^{-1}b$. The system becomes $\\hat{A} \\hat{x} = \\hat{b}$.\nThe matrix $\\hat{A}$ is SPD. It is symmetric because $A$ is symmetric:\n$$\n\\hat{A}^T = (L^{-1} A L^{-T})^T = (L^{-T})^T A^T (L^{-1})^T = L^{-1} A L^{-T} = \\hat{A}\n$$\nIt is positive definite because $A$ is SPD and $L^{-T}$ is nonsingular. For any non-zero vector $y \\in \\mathbb{R}^5$, let $z = L^{-T}y$. Since $L^{-T}$ is nonsingular, $z \\neq 0$. Then:\n$$\ny^T \\hat{A} y = y^T (L^{-1} A L^{-T}) y = (L^{-T}y)^T A (L^{-T}y) = z^T A z > 0\n$$\nThe PCG algorithm applied to $A x = b$ is designed such that the sequence of iterates $x_k$ it generates corresponds to the sequence of iterates $\\hat{x}_k = L^T x_k$ generated by the standard CG algorithm applied to $\\hat{A} \\hat{x} = \\hat{b}$.\n\nA fundamental theorem of the CG method states that the algorithm terminates with the exact solution in at most $m$ iterations, where $m$ is the number of distinct eigenvalues of the system matrix. This property arises because the error $e_k = x - x_k$ can be expressed as $e_k = P_k(A) e_0$ for some polynomial $P_k$ of degree $k$ with $P_k(0)=1$. CG finds the polynomial that minimizes the $A$-norm of the error. If the matrix has $m$ distinct eigenvalues $\\{\\mu_1, \\dots, \\mu_m\\}$, one can construct a polynomial $Q(t) = \\prod_{i=1}^m (1 - t/\\mu_i)$ of degree $m$ that vanishes at all eigenvalues and satisfies $Q(0)=1$. The CG algorithm finds this polynomial by step $m$, resulting in a zero error. This holds for any initial guess $x_0$ and right-hand side $b$.\n\nFor our preconditioned system, the relevant system matrix is $\\hat{A}$. We need to determine the number of distinct eigenvalues of $\\hat{A}$. The matrices $\\hat{A}$ and $M^{-1}A$ are similar, which means they share the same eigenvalues. We can show this similarity transformation explicitly:\n$$\n\\hat{A} = L^{-1} A L^{-T} = L^{-1} (M M^{-1}) A L^{-T} = L^{-1} (L L^T) (M^{-1}A) (L^T)^{-1} = (L^{-1}L) L^T (M^{-1}A) (L^T)^{-1} = L^T (M^{-1}A) (L^T)^{-1}\n$$\nSince $\\hat{A}$ is a similarity transformation of $M^{-1}A$, they have the same characteristic polynomial and thus the same eigenvalues.\n\nFrom our construction, the preconditioned matrix $M^{-1}A$ has exactly $2$ distinct eigenvalues. Consequently, the transformed matrix $\\hat{A}$ also has exactly $2$ distinct eigenvalues.\n\nTherefore, applying the standard CG convergence theorem to the system $\\hat{A} \\hat{x} = \\hat{b}$, the algorithm is guaranteed to find the exact solution $\\hat{x}$ in at most $2$ iterations. As $\\hat{x}_k = L^T x_k$ and $L$ is nonsingular, if $\\hat{x}_k = \\hat{x}$, then $x_k = x$. This convergence in a maximum of $2$ steps is guaranteed for any initial guess $\\hat{x}_0 = L^T x_0$ and any right-hand side $\\hat{b} = L^{-1} b$, which is equivalent to any $x_0$ and $b$ since $L$ is invertible. While for specific initial conditions, convergence may occur in $1$ iteration (if the initial residual is an eigenvector of $\\hat{A}$), the bound must hold for arbitrary inputs. The worst-case scenario, which dictates the bound, requires that the initial residual has components in the eigenspaces of all distinct eigenvalues.\n\nThe minimal integer $k^\\star$ that bounds the number of iterations for any $b$ and $x_0$ is therefore the number of distinct eigenvalues of the preconditioned operator $M^{-1}A$. In this problem, this number is $2$.\nSo, $k^\\star = 2$.", "answer": "$$\n\\boxed{2}\n$$", "id": "2427437"}]}