{"hands_on_practices": [{"introduction": "To truly understand Tikhonov regularization, it is invaluable to compare it with other fundamental techniques for ill-posed problems. This first practice [@problem_id:3599470] guides you through a numerical comparison with Truncated Singular Value Decomposition (TSVD). By implementing both methods in a simplified setting, you will directly observe how their different \"filtering\" strategies on the singular values lead to different solution characteristics, and you will construct a scenario where TSVD's sharp cutoff can outperform Tikhonov's smooth attenuation.", "problem": "Consider solving a linear inverse problem in the sense of numerical linear algebra: given a matrix $A \\in \\mathbb{R}^{n \\times n}$, an unknown vector $x_{\\mathrm{true}} \\in \\mathbb{R}^{n}$, deterministic data noise $\\eta \\in \\mathbb{R}^{n}$, and observed data $b = A x_{\\mathrm{true}} + \\eta$, compare two regularization strategies: Tikhonov regularization and truncated singular value decomposition. The goal is to compute the relative solution errors for both strategies over a set of test cases and to highlight a design where truncated singular value decomposition can outperform Tikhonov regularization due to spectral gaps.\n\nBase definitions and requirements:\n- For Tikhonov regularization with regularization parameter $\\lambda > 0$, the solution is the minimizer of the strictly convex objective $\\|A x - b\\|_{2}^{2} + \\lambda \\|x\\|_{2}^{2}$, which is the unique solution to the normal equations $(A^{\\top} A + \\lambda I) x_{\\mathrm{tik}} = A^{\\top} b$.\n- For truncated singular value decomposition (TSVD), define the singular value decomposition $A = U \\Sigma V^{\\top}$ with singular values $\\sigma_{1} \\ge \\sigma_{2} \\ge \\dots \\ge \\sigma_{n} \\ge 0$. For a truncation index $k \\in \\{0,1,\\dots,n\\}$, the TSVD solution is $x_{\\mathrm{tsvd}}^{(k)} = \\sum_{i=1}^{k} \\frac{u_{i}^{\\top} b}{\\sigma_{i}} v_{i}$. In this problem, the truncation index $k$ is determined by the rule $k = \\max\\{ i \\in \\{1,\\dots,n\\} : \\sigma_{i}^{2} \\ge \\lambda \\}$, with the convention that if the set is empty, then $k = 0$.\n- The relative error of a candidate solution $\\widehat{x}$ with respect to $x_{\\mathrm{true}}$ is $\\|\\widehat{x} - x_{\\mathrm{true}}\\|_{2} / \\|x_{\\mathrm{true}}\\|_{2}$.\n\nImplementation constraints and specialization for testability:\n- In all test cases, take $n = 10$ and choose $A$ to be diagonal with descending positive diagonal entries, so that $U = I$, $V = I$, and the singular values are the diagonal entries of $A$. This makes $A = \\mathrm{diag}(\\sigma_{1},\\dots,\\sigma_{n})$ and $A^{\\top} A = \\mathrm{diag}(\\sigma_{1}^{2},\\dots,\\sigma_{n}^{2})$.\n- For each test case, define the noise vector deterministically by $\\eta_{i} = \\mathrm{noise\\_level} \\cdot (-1)^{i}$ for $i = 1,2,\\dots,n$.\n- For Tikhonov, the explicit componentwise expression derived from the normal equations in this diagonal setting is $x_{\\mathrm{tik},i} = \\frac{\\sigma_{i}}{\\sigma_{i}^{2} + \\lambda} b_{i}$ for $i = 1,2,\\dots,n$.\n- For TSVD, given $k$ by the above selection rule, the explicit componentwise expression in this diagonal setting is $x_{\\mathrm{tsvd},i}^{(k)} = \\begin{cases} b_{i}/\\sigma_{i}, & i \\le k, \\\\ 0, & i > k. \\end{cases}$\n\nTest suite:\n- Test case $\\#1$ (spectral gap; designed to favor truncated singular value decomposition): \n  - $n = 10$.\n  - Singular values $\\sigma = [1.0, 0.6, 0.36, 0.216, 0.1296, 10^{-3}, 5 \\cdot 10^{-4}, 2 \\cdot 10^{-4}, 10^{-4}, 5 \\cdot 10^{-5}]$.\n  - True solution $x_{\\mathrm{true}} = [1, -\\tfrac{1}{2}, \\tfrac{1}{4}, -\\tfrac{1}{8}, \\tfrac{1}{16}, 0, 0, 0, 0, 0]$.\n  - Regularization parameter $\\lambda = 10^{-5}$.\n  - Noise level $\\mathrm{noise\\_level} = 10^{-6}$.\n- Test case $\\#2$ (smooth spectrum; no pronounced gap):\n  - $n = 10$.\n  - Singular values $\\sigma_{i} = 10^{-\\frac{i-1}{3}}$ for $i = 1,2,\\dots,10$.\n  - True solution $x_{\\mathrm{true},i} = 0.8^{i-1}$ for $i = 1,2,\\dots,10$.\n  - Regularization parameter $\\lambda = 10^{-4}$.\n  - Noise level $\\mathrm{noise\\_level} = 10^{-6}$.\n- Test case $\\#3$ (boundary condition with very small regularization):\n  - $n = 10$.\n  - Singular values $\\sigma_{i} = 10^{-\\frac{i-1}{3}}$ for $i = 1,2,\\dots,10$.\n  - True solution $x_{\\mathrm{true},i} = 0.8^{i-1}$ for $i = 1,2,\\dots,10$.\n  - Regularization parameter $\\lambda = 10^{-12}$.\n  - Noise level $\\mathrm{noise\\_level} = 10^{-6}$.\n- Test case $\\#4$ (boundary condition with very large regularization):\n  - $n = 10$.\n  - Singular values $\\sigma_{i} = 10^{-\\frac{i-1}{3}}$ for $i = 1,2,\\dots,10$.\n  - True solution $x_{\\mathrm{true},i} = 0.8^{i-1}$ for $i = 1,2,\\dots,10$.\n  - Regularization parameter $\\lambda = 10^{1}$.\n  - Noise level $\\mathrm{noise\\_level} = 10^{-6}$.\n\nTasks to implement for each test case:\n- Construct $A = \\mathrm{diag}(\\sigma_{1},\\dots,\\sigma_{n})$, $x_{\\mathrm{true}}$, $\\eta$ with $\\eta_{i} = \\mathrm{noise\\_level} \\cdot (-1)^{i}$, and $b = A x_{\\mathrm{true}} + \\eta$.\n- Compute the Tikhonov solution $x_{\\mathrm{tik}}$ using $x_{\\mathrm{tik},i} = \\frac{\\sigma_{i}}{\\sigma_{i}^{2} + \\lambda} b_{i}$.\n- Compute the truncation index $k = \\max\\{ i : \\sigma_{i}^{2} \\ge \\lambda \\}$ with the convention $k = 0$ if the set is empty.\n- Compute the truncated singular value decomposition solution $x_{\\mathrm{tsvd}}^{(k)}$ using $x_{\\mathrm{tsvd},i}^{(k)} = b_{i}/\\sigma_{i}$ for $i \\le k$ and $x_{\\mathrm{tsvd},i}^{(k)} = 0$ for $i > k$.\n- Compute the relative $2$-norm errors $e_{\\mathrm{tsvd}} = \\|x_{\\mathrm{tsvd}}^{(k)} - x_{\\mathrm{true}}\\|_{2} / \\|x_{\\mathrm{true}}\\|_{2}$ and $e_{\\mathrm{tik}} = \\|x_{\\mathrm{tik}} - x_{\\mathrm{true}}\\|_{2} / \\|x_{\\mathrm{true}}\\|_{2}$.\n- Also compute the boolean $b_{\\mathrm{adv}} = (e_{\\mathrm{tsvd}}  e_{\\mathrm{tik}})$ to indicate whether truncated singular value decomposition strictly outperforms Tikhonov in that test.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case contributes a list of the form $[k, e_{\\mathrm{tsvd}}, e_{\\mathrm{tik}}, b_{\\mathrm{adv}}]$. The final output should therefore be a list with four inner lists, one per test case, in order of test cases $\\#1$ through $\\#4$, for example, $[[k_{1}, e_{\\mathrm{tsvd},1}, e_{\\mathrm{tik},1}, b_{\\mathrm{adv},1}], [k_{2}, e_{\\mathrm{tsvd},2}, e_{\\mathrm{tik},2}, b_{\\mathrm{adv},2}], [k_{3}, e_{\\mathrm{tsvd},3}, e_{\\mathrm{tik},3}, b_{\\mathrm{adv},3}], [k_{4}, e_{\\mathrm{tsvd},4}, e_{\\mathrm{tik},4}, b_{\\mathrm{adv},4}]]$.\n\nNotes:\n- There are no physical units; all computations are in dimensionless floating-point arithmetic.\n- Angles are not used.\n- Express all booleans as language-native boolean values and all errors as floating-point numbers.", "solution": "The problem presented requires the comparison of two standard regularization techniques for solving ill-posed linear inverse problems: Tikhonov regularization and Truncated Singular Value Decomposition (TSVD). The validation confirms that the problem is scientifically sound, well-posed, and provides a clear, self-contained set of instructions and data for a numerical experiment. All definitions and formulas are consistent with the established literature in numerical linear algebra. We may therefore proceed with a solution.\n\nThe core of the problem lies in solving the linear system $A x = b$ where the matrix $A$ is ill-conditioned and the data vector $b$ is corrupted by noise $\\eta$. The model is given by $b = A x_{\\mathrm{true}} + \\eta$, where $x_{\\mathrm{true}}$ is the ground truth solution we seek to approximate. A naive attempt to solve for $x$ via $x = A^{-1} b$ would result in $x = x_{\\mathrm{true}} + A^{-1} \\eta$. Since $A$ is ill-conditioned, its inverse $A^{-1}$ has a very large norm, leading to extreme amplification of the noise term $\\eta$. Regularization methods are designed to counteract this by introducing a controlled bias to the solution in exchange for a dramatic reduction in variance due to noise.\n\nThe problem simplifies the analysis by considering a diagonal matrix $A = \\mathrm{diag}(\\sigma_{1}, \\dots, \\sigma_{n})$, where the diagonal entries $\\sigma_i > 0$ are the singular values of $A$. In the general case, any matrix $A$ has a Singular Value Decomposition (SVD) $A = U \\Sigma V^{\\top}$, where $U$ and $V$ are orthogonal matrices and $\\Sigma = \\mathrm{diag}(\\sigma_1, \\dots, \\sigma_n)$. The choice of a diagonal $A$ is equivalent to working in a basis where the SVD is trivial ($U=V=I$), allowing us to focus on how the singular values themselves are handled by each regularization method.\n\n**Tikhonov Regularization**\n\nTikhonov regularization recasts the problem as an optimization problem, seeking a solution $x$ that minimizes a combination of the data fidelity term and a penalty on the solution norm:\n$$ x_{\\mathrm{tik}} = \\arg\\min_{x} \\left( \\|A x - b\\|_{2}^{2} + \\lambda \\|x\\|_{2}^{2} \\right) $$\nThe parameter $\\lambda > 0$ controls the trade-off. The unique minimizer $x_{\\mathrm{tik}}$ is found by solving the associated normal equations: $(A^{\\top} A + \\lambda I) x_{\\mathrm{tik}} = A^{\\top} b$. For our diagonal matrix $A=\\mathrm{diag}(\\sigma_i)$, this system decouples into $n$ scalar equations:\n$$ (\\sigma_{i}^{2} + \\lambda) x_{\\mathrm{tik},i} = \\sigma_{i} b_{i} \\quad \\text{for } i \\in \\{1, \\dots, n\\} $$\nThis gives the explicit component-wise formula for the Tikhonov solution:\n$$ x_{\\mathrm{tik},i} = \\frac{\\sigma_{i}}{\\sigma_{i}^{2} + \\lambda} b_{i} $$\nThe term $f_{i}^{\\mathrm{tik}} = \\frac{\\sigma_{i}^{2}}{\\sigma_{i}^{2} + \\lambda}$ can be viewed as a \"filter factor\". It smoothly attenuates components associated with small singular values. If $\\sigma_i^2 \\gg \\lambda$, then $f_{i}^{\\mathrm{tik}} \\approx 1$, and the component is largely unchanged. If $\\sigma_i^2 \\ll \\lambda$, then $f_{i}^{\\mathrm{tik}} \\approx 0$, and the component is suppressed.\n\n**Truncated Singular Value Decomposition (TSVD)**\n\nTSVD takes a more direct approach by constructing the solution using only the \"significant\" singular components. The general TSVD solution is given by a truncated sum:\n$$ x_{\\mathrm{tsvd}}^{(k)} = \\sum_{i=1}^{k} \\frac{u_{i}^{\\top} b}{\\sigma_{i}} v_{i} $$\nwhere $k$ is the truncation index, determining how many components are included. In our diagonal case ($U=I, V=I$), this simplifies to:\n$$ x_{\\mathrm{tsvd},i}^{(k)} = \\begin{cases} b_{i}/\\sigma_{i}  \\text{if } i \\le k \\\\ 0  \\text{if } i > k \\end{cases} $$\nThis corresponds to filter factors $f_{i}^{\\mathrm{tsvd}}$ that form a step function: $f_{i}^{\\mathrm{tsvd}} = 1$ for $i \\le k$ and $f_{i}^{\\mathrm{tsvd}} = 0$ for $i > k$. The problem links the choice of $k$ to the Tikhonov parameter $\\lambda$ via the rule $k = \\max\\{ i \\in \\{1,\\dots,n\\} : \\sigma_{i}^{2} \\ge \\lambda \\}$, with $k=0$ if the set is empty. This rule essentially keeps all components for which the Tikhonov filter factor would be at least $1/2$.\n\n**Comparison and the Role of Spectral Gaps**\n\nThe fundamental difference lies in their filter functions: Tikhonov's is smooth, while TSVD's is sharp. Test case $\\#1$ is specifically designed to highlight a scenario where TSVD's sharp cutoff is advantageous. It features a \"spectral gap,\" where the singular values show a large drop between $\\sigma_5$ and $\\sigma_6$. The regularization parameter $\\lambda=10^{-5}$ is chosen to lie within this gap (i.e., $\\sigma_5^2 \\gg \\lambda \\gg \\sigma_6^2$). Furthermore, the true solution $x_{\\mathrm{true}}$ has its information content restricted to the first $5$ components.\n\nUnder these conditions, the truncation rule for TSVD yields $k=5$. TSVD therefore retains the first $5$ components (where the signal lies) and completely discards the remaining components (which contain only noise, as $x_{\\mathrm{true},i}=0$ for $i>5$). This acts as a perfect filter for this specific problem structure. In contrast, Tikhonov regularization applies its smooth filter to all components. While it heavily suppresses components $i > 5$, it still allows a small, filtered amount of noise to pass through. More importantly, it also slightly damps components $i \\le 5$, introducing a regularization error that TSVD does not have forthese components. This leads to TSVD outperforming Tikhonov.\n\nFor the other test cases with smoother spectral decay, the sharp cutoff of TSVD can be detrimental. If $x_{\\mathrm{true}}$ contains significant energy in components that TSVD truncates (because their $\\sigma_i$ are small), it will incur a large regularization error. Tikhonov's gentle damping of these components can result in a better overall approximation.\n\n**Computational Steps**\n\nFor each of the four test cases, the following procedure is implemented:\n1.  Initialize the parameters: $n=10$, singular values $\\sigma$, true solution $x_{\\mathrm{true}}$, regularization parameter $\\lambda$, and noise level.\n2.  Construct the noise vector $\\eta$ where $\\eta_{j} = \\mathrm{noise\\_level} \\cdot (-1)^{j+1}$ for the $0$-based index $j \\in \\{0, \\dots, 9\\}$.\n3.  Compute the data vector $b = \\sigma \\odot x_{\\mathrm{true}} + \\eta$, where $\\odot$ denotes element-wise multiplication.\n4.  Calculate the Tikhonov solution vector $x_{\\mathrm{tik}}$ using its component-wise formula.\n5.  Determine the TSVD truncation index $k$ based on the provided rule.\n6.  Calculate the TSVD solution vector $x_{\\mathrm{tsvd}}^{(k)}$.\n7.  Compute the relative $2$-norm errors for both solutions: $e_{\\mathrm{tik}} = \\|x_{\\mathrm{tik}} - x_{\\mathrm{true}}\\|_{2} / \\|x_{\\mathrm{true}}\\|_{2}$ and $e_{\\mathrm{tsvd}} = \\|x_{\\mathrm{tsvd}}^{(k)} - x_{\\mathrm{true}}\\|_{2} / \\|x_{\\mathrm{true}}\\|_{2}$.\n8.  Evaluate the boolean condition $b_{\\mathrm{adv}} = (e_{\\mathrm{tsvd}}  e_{\\mathrm{tik}})$.\nThe collected results $[k, e_{\\mathrm{tsvd}}, e_{\\mathrm{tik}}, b_{\\mathrm{adv}}]$ for each case are then reported.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_test_case(sigma_vals, xtrue_vals, lam, noise_level):\n    \"\"\"\n    Runs a single test case for comparing Tikhonov and TSVD regularization.\n    \"\"\"\n    n = 10\n    sigma = np.array(sigma_vals, dtype=float)\n    xtrue = np.array(xtrue_vals, dtype=float)\n\n    # Construct the noise vector eta and observed data b\n    # The problem uses 1-based indexing i=1,...,n. Python uses 0-based j=0,...,n-1.\n    # eta_i = noise_level * (-1)^i translates to eta[j] = noise_level * (-1)**(j+1)\n    indices_1_based = np.arange(1, n + 1)\n    eta = noise_level * ((-1) ** indices_1_based)\n    b = sigma * xtrue + eta\n\n    # Compute the Tikhonov solution\n    xtik = (sigma / (sigma**2 + lam)) * b\n\n    # Determine the TSVD truncation index k\n    # k = max{ i in {1..n} : sigma_i^2 = lam }\n    # np.where returns 0-based indices. k needs to be a 1-based count.\n    valid_indices = np.where(sigma**2 = lam)[0]\n    if len(valid_indices) == 0:\n        k = 0\n    else:\n        k = int(np.max(valid_indices) + 1)\n\n    # Compute the TSVD solution\n    xtsvd = np.zeros(n)\n    if k  0:\n        # Slicing with :k works correctly for 0-based index up to k-1.\n        xtsvd[:k] = b[:k] / sigma[:k]\n\n    # Compute the relative 2-norm errors\n    norm_xtrue = np.linalg.norm(xtrue)\n    \n    # This problem guarantees norm_xtrue  0, so no division-by-zero check is needed.\n    e_tsvd = np.linalg.norm(xtsvd - xtrue) / norm_xtrue\n    e_tik = np.linalg.norm(xtik - xtrue) / norm_xtrue\n\n    # Determine if TSVD has a strictly smaller error\n    b_adv = bool(e_tsvd  e_tik)\n    \n    return [k, e_tsvd, e_tik, b_adv]\n\ndef solve():\n    \"\"\"\n    Defines and runs the four test cases, then prints the results.\n    \"\"\"\n    # Test case #1: Spectral gap\n    case1 = {\n        \"sigma_vals\": [1.0, 0.6, 0.36, 0.216, 0.1296, 1e-3, 5e-4, 2e-4, 1e-4, 5e-5],\n        \"xtrue_vals\": [1.0, -0.5, 0.25, -0.125, 0.0625, 0, 0, 0, 0, 0],\n        \"lam\": 1e-5,\n        \"noise_level\": 1e-6\n    }\n    \n    # Test case #2: Smooth spectrum\n    n = 10\n    j_indices = np.arange(n)\n    sigma_smooth = 10**(-j_indices / 3.0)\n    xtrue_smooth = 0.8**j_indices\n    case2 = {\n        \"sigma_vals\": sigma_smooth,\n        \"xtrue_vals\": xtrue_smooth,\n        \"lam\": 1e-4,\n        \"noise_level\": 1e-6\n    }\n    \n    # Test case #3: Small regularization parameter\n    case3 = {\n        \"sigma_vals\": sigma_smooth,\n        \"xtrue_vals\": xtrue_smooth,\n        \"lam\": 1e-12,\n        \"noise_level\": 1e-6\n    }\n\n    # Test case #4: Large regularization parameter\n    case4 = {\n        \"sigma_vals\": sigma_smooth,\n        \"xtrue_vals\": xtrue_smooth,\n        \"lam\": 1e1,\n        \"noise_level\": 1e-6\n    }\n\n    test_cases = [case1, case2, case3, case4]\n    \n    results = []\n    for case in test_cases:\n        result = run_test_case(\n            case[\"sigma_vals\"],\n            case[\"xtrue_vals\"],\n            case[\"lam\"],\n            case[\"noise_level\"]\n        )\n        results.append(result)\n\n    # Print in the specified format: [[k1, e_tsvd1, e_tik1, b_adv1], [k2, ...]]\n    # Python's default string representation for a list of lists matches the required format.\n    print(results)\n\nsolve()\n```", "id": "3599470"}, {"introduction": "The power of Tikhonov regularization extends far beyond the standard case where the penalty is on the solution's norm (i.e., $L=I$). In this exercise [@problem_id:3599497], we explore the crucial role of a general regularization operator $L$. You will investigate a scenario where $L$ is low-rank, meaning it only penalizes components of the solution within a specific subspace, leaving the complementary nullspace of $L$ unregularized. This practice reveals how the forward operator $A$ can create coupling effects that influence the entire solution, even the unregularized parts.", "problem": "Consider the Tikhonov regularization problem in finite-dimensional real Euclidean space. For a given matrix $A \\in \\mathbb{R}^{n \\times n}$, a regularization operator $L \\in \\mathbb{R}^{p \\times n}$, a data vector $b \\in \\mathbb{R}^{n}$, and a regularization parameter $\\lambda \\ge 0$, the Tikhonov-regularized solution $x_{\\lambda} \\in \\mathbb{R}^{n}$ is defined as the unique minimizer of the functional $J_{\\lambda}(x) = \\lVert A x - b \\rVert_{2}^{2} + \\lambda^{2} \\lVert L x \\rVert_{2}^{2}$. In this problem, you will examine how a low-rank operator $L$ induces partial regularization only on a subspace, and you will characterize the behavior of $x_{\\lambda}$ on the complementary subspace.\n\nYou must use only the following foundational starting points:\n- The definition of the Tikhonov objective $J_{\\lambda}(x) = \\lVert A x - b \\rVert_{2}^{2} + \\lambda^{2} \\lVert L x \\rVert_{2}^{2}$ for $\\lambda \\ge 0$.\n- Basic properties of Euclidean inner products, orthogonal projections, and least-squares minimizers in finite-dimensional real vector spaces.\n- The Moore–Penrose pseudoinverse and orthogonal projectors arising from it.\n\nTask:\n1. For a given test case $(A,L,b,\\lambda_{1},\\lambda_{2})$, compute the two Tikhonov-regularized solutions $x_{\\lambda_{1}}$ and $x_{\\lambda_{2}}$ (each is the unique minimizer of $J_{\\lambda}(x)$ for the corresponding $\\lambda$).\n2. Compute the orthogonal projector $P_{\\mathcal{N}}$ onto the nullspace $\\mathcal{N}(L)$ using only linear-algebraic constructions. Specifically, you must compute $P_{\\mathcal{N}} = I - L^{+} L$, where $L^{+}$ denotes the Moore–Penrose pseudoinverse of $L$ and $I$ is the identity matrix of size $n \\times n$.\n3. For each test case, evaluate the Euclidean norm of the change in the $\\mathcal{N}(L)$-component of the solution between $\\lambda_{1}$ and $\\lambda_{2}$, namely the quantity $\\lVert P_{\\mathcal{N}} (x_{\\lambda_{2}} - x_{\\lambda_{1}}) \\rVert_{2}$.\n\nConstruction and interpretation requirements:\n- You must construct examples where $L$ is low-rank so that regularization is applied only on a subspace. You must explicitly examine the behavior of $x_{\\lambda}$ in the complementary subspace $\\mathcal{N}(L)$ by measuring $\\lVert P_{\\mathcal{N}} (x_{\\lambda_{2}} - x_{\\lambda_{1}}) \\rVert_{2}$.\n- You must include one case where $A$ and $L$ are simultaneously block-diagonal with respect to the decomposition $\\mathbb{R}^{n} = \\mathcal{N}(L)^{\\perp} \\oplus \\mathcal{N}(L)$, so that the influence of $\\lambda$ is confined to $\\mathcal{N}(L)^{\\perp}$.\n- You must include another case where $A$ couples $\\mathcal{N}(L)^{\\perp}$ and $\\mathcal{N}(L)$ so that the behavior of $x_{\\lambda}$ in $\\mathcal{N}(L)$ depends on $\\lambda$.\n\nFor reproducibility and comprehensive coverage, use the following fixed test suite of small cases with $n = 4$:\n- Test case $1$ (decoupled, moderate change in $\\lambda$): \n  - $A = \\operatorname{diag}(1,2,3,4)$,\n  - $L = \\operatorname{diag}(1,1,0,0)$,\n  - $b = [1,-2,3,-4]^{\\top}$,\n  - $(\\lambda_{1},\\lambda_{2}) = (0, 0.1)$.\n- Test case $2$ (decoupled, extreme change in $\\lambda$):\n  - $A = \\operatorname{diag}(1,2,3,4)$,\n  - $L = \\operatorname{diag}(1,1,0,0)$,\n  - $b = [1,-2,3,-4]^{\\top}$,\n  - $(\\lambda_{1},\\lambda_{2}) = (0, 1000)$.\n- Test case $3$ (coupled, demonstrating leakage into $\\mathcal{N}(L)$):\n  - $A = \\begin{bmatrix} 1  0  0.5  0 \\\\ 0  2  0  0 \\\\ 0  0  3  0 \\\\ 0  0  0  4 \\end{bmatrix}$,\n  - $L = \\operatorname{diag}(1,1,0,0)$,\n  - $b = [1,-2,3,-4]^{\\top}$,\n  - $(\\lambda_{1},\\lambda_{2}) = (0, 10)$.\n- Test case $4$ (degenerate low-rank, no regularization at all): \n  - $A = \\operatorname{diag}(1,2,3,4)$,\n  - $L = 0_{4 \\times 4}$ (the zero matrix),\n  - $b = [1,-2,3,-4]^{\\top}$,\n  - $(\\lambda_{1},\\lambda_{2}) = (0, 5)$.\n\nNumerical output specification:\n- For each test case, compute the single float value $r = \\lVert P_{\\mathcal{N}} (x_{\\lambda_{2}} - x_{\\lambda_{1}}) \\rVert_{2}$.\n- Your program should produce a single line of output containing the results for the $4$ test cases as a comma-separated list enclosed in square brackets (e.g., $[r_{1},r_{2},r_{3},r_{4}]$).\n- No physical units are involved in this task. Angles are not involved. Do not express answers as percentages.\n\nYour implementation must be a single, self-contained program that constructs the specified test suite internally, performs the computations as described, and prints exactly one line in the specified format. No external input is permitted.", "solution": "The problem requires the analysis of Tikhonov-regularized solutions and their behavior on the nullspace of the regularization operator. The solution proceeds in three main theoretical steps for each test case: first, finding the Tikhonov-regularized solution $x_{\\lambda}$; second, constructing the orthogonal projector onto the nullspace of the regularization operator $L$; and third, computing the specified norm.\n\nStep 1: Deriving the Tikhonov-Regularized Solution $x_{\\lambda}$\n\nThe Tikhonov-regularized solution $x_{\\lambda}$ is defined as the unique minimizer of the functional:\n$$J_{\\lambda}(x) = \\lVert A x - b \\rVert_{2}^{2} + \\lambda^{2} \\lVert L x \\rVert_{2}^{2}$$\nwhere $A \\in \\mathbb{R}^{n \\times n}$, $L \\in \\mathbb{R}^{p \\times n}$, $b \\in \\mathbb{R}^{n}$, and $\\lambda \\ge 0$. The squared Euclidean norms can be expressed using vector transposes:\n$$J_{\\lambda}(x) = (A x - b)^{\\top}(A x - b) + \\lambda^{2} (L x)^{\\top}(L x)$$\nExpanding this expression gives a quadratic function of $x$:\n$$J_{\\lambda}(x) = (x^{\\top}A^{\\top} - b^{\\top})(A x - b) + \\lambda^{2} x^{\\top}L^{\\top}L x$$\n$$J_{\\lambda}(x) = x^{\\top}A^{\\top}A x - x^{\\top}A^{\\top}b - b^{\\top}A x + b^{\\top}b + \\lambda^{2} x^{\\top}L^{\\top}L x$$\nSince $b^{\\top}A x$ is a scalar, it is equal to its transpose $x^{\\top}A^{\\top}b$. Thus, the functional simplifies to:\n$$J_{\\lambda}(x) = x^{\\top}(A^{\\top}A + \\lambda^{2} L^{\\top}L)x - 2 b^{\\top}A x + b^{\\top}b$$\nTo find the minimizer, we compute the gradient of $J_{\\lambda}(x)$ with respect to $x$ and set it to the zero vector. The gradient is:\n$$\\nabla_x J_{\\lambda}(x) = 2(A^{\\top}A + \\lambda^{2} L^{\\top}L)x - 2 A^{\\top}b$$\nSetting $\\nabla_x J_{\\lambda}(x) = 0$ yields the Tikhonov normal equations:\n$$(A^{\\top}A + \\lambda^{2} L^{\\top}L) x_{\\lambda} = A^{\\top}b$$\nThe matrix $H_{\\lambda} = A^{\\top}A + \\lambda^{2} L^{\\top}L$ is the Hessian of the functional. Since $A^{\\top}A$ and $L^{\\top}L$ are positive semi-definite, $H_{\\lambda}$ is also positive semi-definite. For all test cases provided, the matrix $A$ is invertible, which implies $A^{\\top}A$ is positive definite. Therefore, $H_{\\lambda}$ is the sum of a positive definite matrix and a positive semi-definite matrix, making $H_{\\lambda}$ positive definite for all $\\lambda \\ge 0$. This guarantees the existence of a unique solution $x_{\\lambda}$, which can be found by solving the linear system:\n$$x_{\\lambda} = (A^{\\top}A + \\lambda^{2} L^{\\top}L)^{-1} A^{\\top}b$$\nFor each test case, we compute the solutions $x_{\\lambda_{1}}$ and $x_{\\lambda_{2}}$ by applying this formula for $\\lambda = \\lambda_1$ and $\\lambda = \\lambda_2$, respectively.\n\nStep 2: Constructing the Projector $P_{\\mathcal{N}}$\n\nThe problem requires the construction of the orthogonal projector $P_{\\mathcal{N}}$ onto the nullspace of $L$, denoted $\\mathcal{N}(L)$. The specified formula is:\n$$P_{\\mathcal{N}} = I - L^{+}L$$\nwhere $L^{+}$ is the Moore-Penrose pseudoinverse of $L$ and $I$ is the identity matrix. This formula follows from the properties of the pseudoinverse. The matrix product $L^{+}L$ is the orthogonal projector onto the row space of $L$, which is denoted $\\mathcal{R}(L^{\\top})$. In a finite-dimensional real vector space, the row space and the nullspace are orthogonal complements: $\\mathcal{R}(L^{\\top}) = \\mathcal{N}(L)^{\\perp}$. Therefore, $P_{\\mathcal{R}(L^{\\top})} = L^{+}L$. The projector onto the complementary subspace $\\mathcal{N}(L)$ is given by $I - P_{\\mathcal{R}(L^{\\top})}$, leading directly to the provided formula.\n\nStep 3: Evaluating the Change in the Nullspace Component\n\nThe final step is to compute the quantity $r = \\lVert P_{\\mathcal{N}} (x_{\\lambda_{2}} - x_{\\lambda_{1}}) \\rVert_{2}$. This value measures the magnitude of the change in the component of the solution that lies within the nullspace of $L$ as the regularization parameter changes from $\\lambda_1$ to $\\lambda_2$.\n\nThe behavior of this quantity depends on the coupling between the subspaces $\\mathcal{N}(L)$ and its orthogonal complement $\\mathcal{N}(L)^{\\perp}$ induced by the operator $A$.\n- If $A$ is block-diagonal with respect to the orthogonal decomposition $\\mathbb{R}^{n} = \\mathcal{N}(L)^{\\perp} \\oplus \\mathcal{N}(L)$, the minimization problem decouples. The regularization parameter $\\lambda$ only affects the components of $x$ in $\\mathcal{N}(L)^{\\perp}$. The components of $x$ in $\\mathcal{N}(L)$ are determined independently of $\\lambda$. Consequently, $P_{\\mathcal{N}}x_{\\lambda_1} = P_{\\mathcal{N}}x_{\\lambda_2}$, and the result is $r=0$. Test cases 1 and 2 demonstrate this.\n- If $A$ is not block-diagonal (i.e., it couples the two subspaces), then changing $\\lambda$ alters the optimal components in $\\mathcal{N}(L)^{\\perp}$, and this change propagates to the components in $\\mathcal{N}(L)$ to maintain the overall minimum of $J_{\\lambda}(x)$. In this scenario, $P_{\\mathcal{N}}x_{\\lambda}$ will depend on $\\lambda$, leading to a non-zero result $r > 0$. Test case 3 is designed to illustrate this coupling.\n- For the degenerate case where $L=0$, $\\mathcal{N}(L)=\\mathbb{R}^n$, and the regularization term $\\lambda^2 \\lVert Lx \\rVert^2$ is always zero. The solution $x_\\lambda$ is independent of $\\lambda$, resulting in $r=0$. Test case 4 confirms this.\n\nThe algorithm for each test case is as follows:\n1.  Construct matrices $A$ and $L$, and vector $b$.\n2.  Compute $A^\\top A$, $L^\\top L$, and $A^\\top b$.\n3.  For $\\lambda_1$ and $\\lambda_2$, solve the respective normal equations to find $x_{\\lambda_1}$ and $x_{\\lambda_2}$.\n4.  Compute the projector $P_{\\mathcal{N}} = I - L^{+}L$ using the Moore-Penrose pseudoinverse of $L$.\n5.  Calculate the difference vector $\\Delta x = x_{\\lambda_2} - x_{\\lambda_1}$.\n6.  Project the difference vector onto the nullspace: $v = P_{\\mathcal{N}} \\Delta x$.\n7.  Compute and record the Euclidean norm $r = \\lVert v \\rVert_2$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the Tikhonov regularization problem for a suite of test cases.\n\n    For each case (A, L, b, lambda1, lambda2), it computes:\n    1. The Tikhonov solutions x_lambda1 and x_lambda2.\n    2. The orthogonal projector P_N onto the nullspace of L.\n    3. The norm ||P_N * (x_lambda2 - x_lambda1)||_2.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    # Test case 1 (decoupled, moderate change in lambda)\n    A1 = np.diag([1.0, 2.0, 3.0, 4.0])\n    L1 = np.diag([1.0, 1.0, 0.0, 0.0])\n    b1 = np.array([1.0, -2.0, 3.0, -4.0])\n    lambda1_1, lambda1_2 = 0.0, 0.1\n\n    # Test case 2 (decoupled, extreme change in lambda)\n    A2 = np.diag([1.0, 2.0, 3.0, 4.0])\n    L2 = np.diag([1.0, 1.0, 0.0, 0.0])\n    b2 = np.array([1.0, -2.0, 3.0, -4.0])\n    lambda2_1, lambda2_2 = 0.0, 1000.0\n\n    # Test case 3 (coupled, demonstrating leakage into N(L))\n    A3 = np.array([[1.0, 0.0, 0.5, 0.0],\n                   [0.0, 2.0, 0.0, 0.0],\n                   [0.0, 0.0, 3.0, 0.0],\n                   [0.0, 0.0, 0.0, 4.0]])\n    L3 = np.diag([1.0, 1.0, 0.0, 0.0])\n    b3 = np.array([1.0, -2.0, 3.0, -4.0])\n    lambda3_1, lambda3_2 = 0.0, 10.0\n\n    # Test case 4 (degenerate low-rank, no regularization at all)\n    A4 = np.diag([1.0, 2.0, 3.0, 4.0])\n    L4 = np.zeros((4, 4))\n    b4 = np.array([1.0, -2.0, 3.0, -4.0])\n    lambda4_1, lambda4_2 = 0.0, 5.0\n\n    test_cases = [\n        (A1, L1, b1, lambda1_1, lambda1_2),\n        (A2, L2, b2, lambda2_1, lambda2_2),\n        (A3, L3, b3, lambda3_1, lambda3_2),\n        (A4, L4, b4, lambda4_1, lambda4_2)\n    ]\n\n    results = []\n    for A, L, b, lam1, lam2 in test_cases:\n        # Pre-compute matrix products for efficiency\n        AT = A.T\n        LT = L.T\n        ATA = AT @ A\n        LTL = LT @ L\n        ATb = AT @ b\n        \n        # --- Solve for x_lambda1 ---\n        # Form the matrix for the normal equations: (A^T A + lambda^2 L^T L)\n        M1 = ATA + (lam1**2) * LTL\n        # Solve the system M1 * x = ATb\n        x_lam1 = np.linalg.solve(M1, ATb)\n\n        # --- Solve for x_lambda2 ---\n        M2 = ATA + (lam2**2) * LTL\n        x_lam2 = np.linalg.solve(M2, ATb)\n        \n        # --- Compute projector onto the nullspace of L ---\n        # P_N = I - L^+ L\n        n = A.shape[0]\n        I = np.identity(n)\n        # np.linalg.pinv computes the Moore-Penrose pseudoinverse L^+\n        L_pinv = np.linalg.pinv(L)\n        P_N = I - L_pinv @ L\n        \n        # --- Final calculation ---\n        # Compute the change in the solution vector\n        delta_x = x_lam2 - x_lam1\n        # Project the change onto the nullspace of L\n        proj_delta_x = P_N @ delta_x\n        # Compute the Euclidean norm of the projected change\n        result = np.linalg.norm(proj_delta_x)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    # Using a format specifier for consistent floating point output.\n    print(f\"[{','.join(f'{r:.12f}' for r in results)}]\")\n\nsolve()\n```", "id": "3599497"}, {"introduction": "Real-world inverse problems often involve not only ill-conditioning but also known physical or logical constraints on the solution. This advanced practice [@problem_id:3599473] demonstrates how to incorporate such linear equality constraints into the Tikhonov framework. You will formulate a constrained optimization problem with block-separable regularization and solve it by constructing and solving the corresponding Karush-Kuhn-Tucker (KKT) system. This exercise is a key step towards applying regularization theory to complex, practical engineering and scientific models.", "problem": "Consider the following partitioned linear inverse problem with equality constraints and block-separable Tikhonov regularization. Let the unknown vector be partitioned as $x = \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix}$ with $x_1 \\in \\mathbb{R}^{n_1}$ and $x_2 \\in \\mathbb{R}^{n_2}$. Take $n_1 = 3$, $n_2 = 2$, so the total dimension is $n = 5$. The data matrix $A \\in \\mathbb{R}^{m \\times n}$ is given with $m = 6$ and explicit entries\n$$\nA =\n\\begin{bmatrix}\n1  0  0.5  0  0.2 \\\\\n0  1  0.3  0.4  0 \\\\\n0.2  0.1  1  0.5  0.3 \\\\\n0.5  0.2  0.1  1  0.6 \\\\\n0  0.3  0.2  0.1  1 \\\\\n0.4  0  0.5  0.2  0.1\n\\end{bmatrix}.\n$$\nDefine a ground-truth vector that satisfies the constraints by\n$$\nx_{\\mathrm{true}} =\n\\begin{bmatrix}\n0.3 \\\\ 0.4 \\\\ 0.3 \\\\ 0.1 \\\\ -0.1\n\\end{bmatrix},\n$$\nand form the noiseless data vector $b = A x_{\\mathrm{true}}$. Impose two equality constraints collected as $C x = d$, where $C \\in \\mathbb{R}^{q \\times n}$ with $q = 2$ and\n$$\nC = \\begin{bmatrix}\n1  1  1  0  0 \\\\\n0  0  0  1  1\n\\end{bmatrix}, \\qquad\nd = \\begin{bmatrix}\n1 \\\\ 0\n\\end{bmatrix}.\n$$\nThe regularization is block-separable and acts on each partition using first-difference operators. Specifically, let $L_1 \\in \\mathbb{R}^{2 \\times 3}$ and $L_2 \\in \\mathbb{R}^{1 \\times 2}$ be\n$$\nL_1 = \\begin{bmatrix}\n-1  1  0 \\\\\n0  -1  1\n\\end{bmatrix}, \\qquad\nL_2 = \\begin{bmatrix}\n-1  1\n\\end{bmatrix}.\n$$\nFor nonnegative regularization parameters $\\lambda_1$ and $\\lambda_2$, consider the constrained Tikhonov-regularized objective\n$$\n\\Phi(x;\\lambda_1,\\lambda_2) = \\|A x - b\\|_2^2 + \\lambda_1 \\|L_1 x_1\\|_2^2 + \\lambda_2 \\|L_2 x_2\\|_2^2\n$$\nsubject to the equality constraints $C x = d$. The matrix $A$ couples the blocks $x_1$ and $x_2$ through its off-diagonal sub-blocks, while the penalties only act intra-block.\n\nYour task is to compute, for each specified pair $(\\lambda_1,\\lambda_2)$, the unique minimizer $x^\\star(\\lambda_1,\\lambda_2)$ of the constrained objective, and then quantify the trade-off between data fit and blockwise smoothness. For each test case, output the following three floats in order:\n- the squared residual $\\|A x^\\star - b\\|_2^2$,\n- the weighted block-$1$ smoothness contribution $\\lambda_1 \\|L_1 x_1^\\star\\|_2^2$,\n- the weighted block-$2$ smoothness contribution $\\lambda_2 \\|L_2 x_2^\\star\\|_2^2$.\n\nUse the following test suite of regularization parameter pairs, chosen to probe different regimes:\n1. $(\\lambda_1,\\lambda_2) = (0, 0)$, representing the equality-constrained least squares solution without regularization.\n2. $(\\lambda_1,\\lambda_2) = (1000, 0.01)$, representing strong regularization on block $x_1$ and weak regularization on block $x_2$.\n3. $(\\lambda_1,\\lambda_2) = (0.01, 1000)$, representing weak regularization on block $x_1$ and strong regularization on block $x_2$.\n4. $(\\lambda_1,\\lambda_2) = (1, 1)$, representing balanced regularization across both blocks.\n\nYour program must produce a single line of output containing the results aggregated across all test cases as a comma-separated list enclosed in square brackets. The ordering must be the concatenation in the sequence above, i.e., $[\\text{residual}_1, \\text{block1}_1, \\text{block2}_1, \\text{residual}_2, \\text{block1}_2, \\text{block2}_2, \\text{residual}_3, \\text{block1}_3, \\text{block2}_3, \\text{residual}_4, \\text{block1}_4, \\text{block2}_4]$. No physical units are involved, and results should be printed as raw floating-point numbers. The program must be self-contained, use deterministic computations, and require no user input.", "solution": "An initial validation of the problem statement confirms that it is a well-posed problem in numerical linear algebra, specifically an equality-constrained quadratic program (ECQP). All matrices, vectors, and parameters are fully specified, and their dimensions are mutually consistent. The problem is scientifically grounded, objective, and contains sufficient information to yield a unique solution for each test case.\n\nThe core task is to find the vector $x \\in \\mathbb{R}^5$ that minimizes the Tikhonov-regularized objective function\n$$\n\\Phi(x;\\lambda_1,\\lambda_2) = \\|A x - b\\|_2^2 + \\lambda_1 \\|L_1 x_1\\|_2^2 + \\lambda_2 \\|L_2 x_2\\|_2^2\n$$\nsubject to the linear equality constraints $C x = d$. Here, the vector $x$ is partitioned as $x = \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix}$, where $x_1 \\in \\mathbb{R}^{n_1}$ with $n_1=3$ and $x_2 \\in \\mathbb{R}^{n_2}$ with $n_2=2$. The method of Lagrange multipliers provides a direct and rigorous path to the solution.\n\nFirst, we express the objective function in a standard quadratic form. The squared Euclidean norm $\\|v\\|_2^2$ is equivalent to the dot product $v^T v$.\n$$\n\\Phi(x) = (Ax - b)^T(Ax - b) + \\lambda_1 (L_1 x_1)^T (L_1 x_1) + \\lambda_2 (L_2 x_2)^T (L_2 x_2)\n$$\nExpanding the terms, we get:\n$$\n\\Phi(x) = (x^T A^T - b^T)(Ax - b) + \\lambda_1 x_1^T L_1^T L_1 x_1 + \\lambda_2 x_2^T L_2^T L_2 x_2\n$$\n$$\n\\Phi(x) = x^T A^T A x - x^T A^T b - b^T A x + b^T b + \\lambda_1 x_1^T L_1^T L_1 x_1 + \\lambda_2 x_2^T L_2^T L_2 x_2\n$$\nSince the scalar term $b^T A x$ is equal to its transpose $x^T A^T b$, the expression simplifies to:\n$$\n\\Phi(x) = x^T A^T A x - 2b^T A x + b^T b + \\lambda_1 x_1^T L_1^T L_1 x_1 + \\lambda_2 x_2^T L_2^T L_2 x_2\n$$\nTo handle the block-separable regularization terms compactly, we introduce a block-diagonal matrix $\\Gamma \\in \\mathbb{R}^{n \\times n}$, where $n = n_1 + n_2 = 5$. This matrix is defined as:\n$$\n\\Gamma = \\begin{bmatrix} \\lambda_1 L_1^T L_1  0_{n_1 \\times n_2} \\\\ 0_{n_2 \\times n_1}  \\lambda_2 L_2^T L_2 \\end{bmatrix}\n$$\nwhere $0_{i \\times j}$ is a zero matrix of size $i \\times j$. With this definition, the sum of the regularization penalties can be written as $x^T \\Gamma x$. The complete objective function is then:\n$$\n\\Phi(x) = x^T(A^T A + \\Gamma)x - 2b^T A x + b^T b\n$$\nThe problem is now to minimize this quadratic function of $x$ subject to the linear constraint $Cx=d$.\n\nWe construct the Lagrangian function $\\mathcal{L}(x, \\mu)$, where $\\mu \\in \\mathbb{R}^q$ is the vector of Lagrange multipliers corresponding to the $q=2$ constraints:\n$$\n\\mathcal{L}(x, \\mu) = \\Phi(x) + \\mu^T(Cx - d)\n$$\nThe Karush-Kuhn-Tucker (KKT) conditions for optimality require that the gradients of the Lagrangian with respect to both $x$ and $\\mu$ are zero.\n\nThe gradient with respect to $x$ is:\n$$\n\\nabla_x \\mathcal{L}(x, \\mu) = \\nabla_x \\left( x^T(A^T A + \\Gamma)x - 2b^T A x + b^T b + \\mu^T C x - \\mu^T d \\right) = 0\n$$\nUsing standard matrix calculus identities, noting that $A^T A + \\Gamma$ is a symmetric matrix, this yields:\n$$\n2(A^T A + \\Gamma)x - 2A^T b + C^T \\mu = 0\n$$\nThe gradient with respect to $\\mu$ is:\n$$\n\\nabla_\\mu \\mathcal{L}(x, \\mu) = \\nabla_\\mu \\left( ... + \\mu^T(Cx - d) \\right) = 0\n$$\n$$\nCx - d = 0\n$$\nThis second condition simply recovers the original equality constraint.\n\nThese two linear matrix equations can be combined into a single block matrix system, known as the KKT system, for the conjoined vector of unknowns $\\begin{bmatrix} x \\\\ \\mu \\end{bmatrix}$:\n$$\n\\begin{bmatrix}\n2(A^T A + \\Gamma)  C^T \\\\\nC  0\n\\end{bmatrix}\n\\begin{bmatrix}\nx \\\\\n\\mu\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n2 A^T b \\\\\nd\n\\end{bmatrix}\n$$\nThis is a square system of $n+q = 7$ linear equations in $7$ unknowns. The analysis of the problem's givens ensures that the KKT matrix on the left-hand side is non-singular for all specified test cases, guaranteeing a unique solution $x^\\star$ (the minimizer) and $\\mu^\\star$ (the corresponding Lagrange multipliers).\n\nThe computational algorithm for each pair of regularization parameters $(\\lambda_1, \\lambda_2)$ is:\n$1$. Construct the matrix $\\Gamma = \\text{blockdiag}(\\lambda_1 L_1^T L_1, \\lambda_2 L_2^T L_2)$.\n$2$. Form the KKT matrix $K = \\begin{bmatrix} 2(A^T A + \\Gamma)  C^T \\\\ C  0 \\end{bmatrix}$ and the right-hand-side vector $RHS = \\begin{bmatrix} 2A^T b \\\\ d \\end{bmatrix}$.\n$3$. Solve the linear system $Kz = RHS$ for $z = \\begin{bmatrix} x^\\star \\\\ \\mu^\\star \\end{bmatrix}$. The desired solution $x^\\star$ is the vector composed of the first $n=5$ components of $z$.\n$4$. Partition the solution $x^\\star$ into $x_1^\\star$ (the first $n_1=3$ components) and $x_2^\\star$ (the final $n_2=2$ components).\n$5$. Compute the three required metrics: the squared data residual $\\|A x^\\star - b\\|_2^2$, the weighted block-$1$ penalty $\\lambda_1 \\|L_1 x_1^\\star\\|_2^2$, and the weighted block-$2$ penalty $\\lambda_2 \\|L_2 x_2^\\star\\|_2^2$.\n\nThis procedure is implemented numerically to obtain the results for each test case.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the constrained Tikhonov regularization problem for a suite of test cases.\n    \"\"\"\n    # Define problem dimensions and constants as per the statement.\n    n1 = 3\n    n2 = 2\n    n = n1 + n2\n    q = 2\n\n    # Define the data matrix A.\n    A = np.array([\n        [1.0, 0.0, 0.5, 0.0, 0.2],\n        [0.0, 1.0, 0.3, 0.4, 0.0],\n        [0.2, 0.1, 1.0, 0.5, 0.3],\n        [0.5, 0.2, 0.1, 1.0, 0.6],\n        [0.0, 0.3, 0.2, 0.1, 1.0],\n        [0.4, 0.0, 0.5, 0.2, 0.1]\n    ])\n\n    # Define the ground-truth vector x_true.\n    x_true = np.array([0.3, 0.4, 0.3, 0.1, -0.1])\n\n    # Define the constraint matrix C and vector d.\n    C = np.array([\n        [1.0, 1.0, 1.0, 0.0, 0.0],\n        [0.0, 0.0, 0.0, 1.0, 1.0]\n    ])\n    d = np.array([1.0, 0.0])\n\n    # Define the regularization operators L1 and L2.\n    L1 = np.array([\n        [-1.0, 1.0, 0.0],\n        [0.0, -1.0, 1.0]\n    ])\n    L2 = np.array([[-1.0, 1.0]])\n\n    # Define the test suite of regularization parameter pairs (lambda1, lambda2).\n    test_cases = [\n        (0.0, 0.0),\n        (1000.0, 0.01),\n        (0.01, 1000.0),\n        (1.0, 1.0)\n    ]\n\n    # Pre-computation of constant terms\n    b = A @ x_true\n    AtA = A.T @ A\n    Atb = A.T @ b\n    L1tL1 = L1.T @ L1\n    L2tL2 = L2.T @ L2\n\n    results = []\n    for lambda1, lambda2 in test_cases:\n        # 1. Construct the ingredients for the KKT system.\n        \n        # Build the block-diagonal regularization matrix Gamma.\n        Gamma = np.zeros((n, n))\n        Gamma[:n1, :n1] = lambda1 * L1tL1\n        Gamma[n1:, n1:] = lambda2 * L2tL2\n\n        # Build the Hessian of the objective function.\n        H = 2 * (AtA + Gamma)\n\n        # Assemble the full KKT matrix.\n        K = np.zeros((n + q, n + q))\n        K[:n, :n] = H\n        K[:n, n:] = C.T\n        K[n:, :n] = C\n        \n        # Assemble the right-hand-side vector.\n        rhs = np.zeros(n + q)\n        rhs[:n] = 2 * Atb\n        rhs[n:] = d\n\n        # 2. Solve the KKT system K*z = rhs for z = [x_star, mu_star].\n        z = np.linalg.solve(K, rhs)\n        x_star = z[:n]\n\n        # 3. Partition the solution and compute the required output quantities.\n        x1_star = x_star[:n1]\n        x2_star = x_star[n1:]\n        \n        # Calculate squared residual, and weighted smoothness contributions.\n        # np.linalg.norm(v)**2 is a numerically stable way to compute v.T @ v.\n        residual_sq = np.linalg.norm(A @ x_star - b)**2\n        smoothness1 = lambda1 * np.linalg.norm(L1 @ x1_star)**2\n        smoothness2 = lambda2 * np.linalg.norm(L2 @ x2_star)**2\n        \n        results.extend([residual_sq, smoothness1, smoothness2])\n\n    # Final print statement must match the specified format exactly.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3599473"}]}