{"hands_on_practices": [{"introduction": "Computing a function of a matrix is most transparent when we go back to first principles. This exercise [@problem_id:3559842] guides you through computing $f(A)$ for a simple $2 \\times 2$ Jordan block by directly applying the Taylor series definition. You will see how decomposing the matrix into its scalar and nilpotent parts elegantly truncates the infinite series, providing an exact, closed-form result.", "problem": "Let $A \\in \\mathbb{R}^{2 \\times 2}$ be given by $A=\\begin{bmatrix}2  1 \\\\ 0  2\\end{bmatrix}$ and let $f:\\mathbb{C} \\to \\mathbb{C}$ be the analytic function $f(z)=\\sin z$. Using only the definition of an analytic function of a matrix via power series and the decomposition of a matrix into a scalar multiple of the identity plus a nilpotent part, derive an explicit closed-form expression for $f(A)$ without appealing to any pre-memorized formula for functions of Jordan blocks. Your derivation should start from the Taylor series of $f$ about a scalar point, justify any truncation of the series that occurs for this $A$, and indicate exactly which derivatives of $f$ are required and why.\n\nState $f(2)$ and $f^{\\prime}(2)$ explicitly as part of your derivation, and present the final $2 \\times 2$ matrix $f(A)$ in exact symbolic form. Do not approximate trigonometric values. The final answer should be the explicit matrix $f(A)$, given exactly (no rounding).", "solution": "The problem requires the computation of $f(A) = \\sin(A)$ for the matrix $A = \\begin{bmatrix}2  1 \\\\ 0  2\\end{bmatrix}$ by using a Taylor series expansion. The specified method involves decomposing $A$ into a scalar multiple of the identity matrix and a nilpotent matrix.\n\nFirst, we decompose the matrix $A$. The diagonal entries of $A$ are both $2$, which is the sole eigenvalue, $\\lambda = 2$. We can write $A$ as the sum of a scalar matrix $\\lambda I$ and a remainder matrix $N$.\n$$\nA = \\begin{bmatrix}2  1 \\\\ 0  2\\end{bmatrix} = \\begin{bmatrix}2  0 \\\\ 0  2\\end{bmatrix} + \\begin{bmatrix}0  1 \\\\ 0  0\\end{bmatrix}\n$$\nThis decomposition can be written as $A = \\lambda I + N$, where $\\lambda = 2$, $I$ is the $2 \\times 2$ identity matrix, and $N = \\begin{bmatrix}0  1 \\\\ 0  0\\end{bmatrix}$.\n\nNext, we examine the properties of the matrix $N$. A matrix is nilpotent if some power of it is the zero matrix. We compute the powers of $N$:\n$$\nN^1 = \\begin{bmatrix}0  1 \\\\ 0  0\\end{bmatrix} \\neq \\mathbf{0}\n$$\n$$\nN^2 = N \\cdot N = \\begin{bmatrix}0  1 \\\\ 0  0\\end{bmatrix} \\begin{bmatrix}0  1 \\\\ 0  0\\end{bmatrix} = \\begin{bmatrix}0  0 \\\\ 0  0\\end{bmatrix} = \\mathbf{0}\n$$\nSince $N^2 = \\mathbf{0}$, the matrix $N$ is nilpotent with an index of nilpotency of $2$. This implies that all higher powers $N^k$ for $k \\ge 2$ are also the zero matrix.\n\nThe function $f(z) = \\sin z$ is analytic on the entire complex plane. The Taylor series expansion of an analytic function $f$ about a scalar point $\\lambda$ is given by:\n$$\nf(z) = \\sum_{k=0}^{\\infty} \\frac{f^{(k)}(\\lambda)}{k!} (z-\\lambda)^k\n$$\nwhere $f^{(k)}(\\lambda)$ is the $k$-th derivative of $f$ evaluated at $\\lambda$.\n\nTo define the function of a matrix $f(A)$, we can formally substitute the matrix $A$ for the scalar variable $z$ in the Taylor series expansion around $\\lambda$. This is a valid definition because the matrices $\\lambda I$ and $N$ commute ($\\lambda I N = N \\lambda I = \\lambda N$).\n$$\nf(A) = f(\\lambda I + N) = \\sum_{k=0}^{\\infty} \\frac{f^{(k)}(\\lambda)}{k!} (A - \\lambda I)^k\n$$\nSubstituting $A - \\lambda I = N$ into this series gives:\n$$\nf(A) = \\sum_{k=0}^{\\infty} \\frac{f^{(k)}(\\lambda)}{k!} N^k\n$$\nThis is an infinite series in powers of the matrix $N$. We expand the first few terms:\n$$\nf(A) = \\frac{f^{(0)}(\\lambda)}{0!} N^0 + \\frac{f^{(1)}(\\lambda)}{1!} N^1 + \\frac{f^{(2)}(\\lambda)}{2!} N^2 + \\frac{f^{(3)}(\\lambda)}{3!} N^3 + \\dots\n$$\nBy convention, $N^0 = I$ and $0! = 1$. The series can be written as:\n$$\nf(A) = f(\\lambda) I + f'(\\lambda) N + \\frac{f''(\\lambda)}{2} N^2 + \\frac{f'''(\\lambda)}{6} N^3 + \\dots\n$$\nThe crucial step is to use the nilpotency of $N$. As we established that $N^k = \\mathbf{0}$ for all $k \\ge 2$, every term in the series from the $k=2$ term onwards becomes the zero matrix. This fact justifies the truncation of the infinite series to a finite sum. The series for $f(A)$ collapses to:\n$$\nf(A) = f(\\lambda) I + f'(\\lambda) N\n$$\nThis expression shows that to compute $f(A)$, we only need the function $f$ itself and its first derivative, $f'$, evaluated at the eigenvalue $\\lambda$. The derivatives required are $f^{(0)}(z) = f(z)$ and $f^{(1)}(z) = f'(z)$. Higher-order derivatives are not necessary because their corresponding terms in the series are multiplied by zero matrices.\n\nNow, we apply this to the specific function $f(z) = \\sin z$ and the eigenvalue $\\lambda = 2$.\nThe required function and its derivative are:\n-   $f(z) = \\sin z$\n-   $f'(z) = \\cos z$\n\nAs required, we evaluate these at $\\lambda = 2$:\n-   $f(2) = \\sin(2)$\n-   $f'(2) = \\cos(2)$\n\nSubstituting these values and the matrices $I$ and $N$ into the truncated series expression for $f(A)$:\n$$\nf(A) = f(2) \\cdot I + f'(2) \\cdot N\n$$\n$$\nf(A) = \\sin(2) \\begin{bmatrix}1  0 \\\\ 0  1\\end{bmatrix} + \\cos(2) \\begin{bmatrix}0  1 \\\\ 0  0\\end{bmatrix}\n$$\nPerforming the scalar multiplication and matrix addition:\n$$\nf(A) = \\begin{bmatrix}\\sin(2)  0 \\\\ 0  \\sin(2)\\end{bmatrix} + \\begin{bmatrix}0  \\cos(2) \\\\ 0  0\\end{bmatrix}\n$$\n$$\nf(A) = \\begin{bmatrix}\\sin(2)  \\cos(2) \\\\ 0  \\sin(2)\\end{bmatrix}\n$$\nThis is the final explicit closed-form expression for $f(A)$, derived strictly from the power series definition as requested, without recourse to pre-established formulas for functions of Jordan blocks. The trigonometric values are kept in their exact symbolic form.", "answer": "$$\n\\boxed{\\begin{pmatrix} \\sin(2)  \\cos(2) \\\\ 0  \\sin(2) \\end{pmatrix}}\n$$", "id": "3559842"}, {"introduction": "Many practical algorithms for matrix functions exploit matrix structure, a principle you will explore in this problem [@problem_id:3559901]. This practice extends the ideas from a single block to a block triangular matrix, which is a cornerstone of methods like the Schur-Parlett algorithm. By leveraging the fundamental commutation property $Tf(T) = f(T)T$, you will derive and solve the Sylvester equation that governs the off-diagonal blocks of $f(T)$, a key step in the powerful block Parlett recurrence.", "problem": "Consider the analytic matrix function $f$ defined by the power series $f(X)=\\sum_{k=0}^{\\infty} \\alpha_{k} X^{k}$ on a neighborhood of the spectrum of a block upper triangular matrix $T \\in \\mathbb{R}^{4 \\times 4}$ partitioned into $2 \\times 2$ blocks as\n$$\nT=\\begin{pmatrix}\nT_{11}  T_{12} \\\\\n0  T_{22}\n\\end{pmatrix},\n\\quad\nT_{11}=\\begin{pmatrix}\n\\ln(2)  1 \\\\\n0  \\ln(2)\n\\end{pmatrix},\n\\quad\nT_{22}=\\begin{pmatrix}\n\\ln(3)  1 \\\\\n0  \\ln(3)\n\\end{pmatrix},\n\\quad\nT_{12}=\\begin{pmatrix}\n1  0 \\\\\n0  1\n\\end{pmatrix}.\n$$\nLet $f$ be the matrix exponential $f(X)=\\exp(X)$. Using only the foundational definition of analytic matrix functions via power series and the block upper triangular structure of $T$, carry out one step of the block Parlett recurrence to determine the off-diagonal block $F_{12}$ in the block decomposition of $f(T)=\\exp(T)$:\n$$\n\\exp(T)=\\begin{pmatrix}\nF_{11}  F_{12} \\\\\n0  F_{22}\n\\end{pmatrix}.\n$$\nYou must derive the governing linear matrix equation for $F_{12}$ from first principles and solve it using the given data. Report the $(2,2)$ entry of $F_{12}$ as a single closed-form expression. Do not round; provide the exact expression in simplest form.", "solution": "The problem requires the computation of the off-diagonal block $F_{12}$ of the matrix function $F = f(T) = \\exp(T)$, where $T$ is a $2 \\times 2$ block upper triangular matrix. The method specified is to derive and solve the governing linear matrix equation that arises in the block Parlett recurrence.\n\nLet the block matrix $T$ and the function $F=f(T)$ be partitioned as\n$$\nT = \\begin{pmatrix} T_{11}  T_{12} \\\\ 0  T_{22} \\end{pmatrix}, \\quad F = f(T) = \\begin{pmatrix} F_{11}  F_{12} \\\\ 0  F_{22} \\end{pmatrix}.\n$$\nThe function $f$ is analytic and defined by a power series $f(X) = \\sum_{k=0}^{\\infty} \\alpha_k X^k$. A fundamental property derived from this definition is that a matrix commutes with any of its analytic functions, i.e., $Tf(T) = f(T)T$. Expressing this commutation relation in block form provides the equations for the blocks of $F$.\n$$\n\\begin{pmatrix} T_{11}  T_{12} \\\\ 0  T_{22} \\end{pmatrix} \\begin{pmatrix} F_{11}  F_{12} \\\\ 0  F_{22} \\end{pmatrix} = \\begin{pmatrix} F_{11}  F_{12} \\\\ 0  F_{22} \\end{pmatrix} \\begin{pmatrix} T_{11}  T_{12} \\\\ 0  T_{22} \\end{pmatrix}\n$$\nPerforming the block matrix multiplication yields:\n$$\n\\begin{pmatrix} T_{11}F_{11}  T_{11}F_{12} + T_{12}F_{22} \\\\ 0  T_{22}F_{22} \\end{pmatrix} = \\begin{pmatrix} F_{11}T_{11}  F_{11}T_{12} + F_{12}T_{22} \\\\ 0  F_{22}T_{22} \\end{pmatrix}\n$$\nEquating the $(1,2)$ blocks gives the governing linear matrix equation for $F_{12}$:\n$$\nT_{11}F_{12} + T_{12}F_{22} = F_{11}T_{12} + F_{12}T_{22}\n$$\nThis can be rearranged into a Sylvester equation for $F_{12}$:\n$$\nT_{11}F_{12} - F_{12}T_{22} = F_{11}T_{12} - T_{12}F_{22}\n$$\nThis equation has a unique solution for $F_{12}$ if and only if the spectra of $T_{11}$ and $T_{22}$ are disjoint. The eigenvalues of a triangular matrix are its diagonal entries. The spectra are $\\lambda(T_{11}) = \\{\\ln(2)\\}$ and $\\lambda(T_{22}) = \\{\\ln(3)\\}$. Since $\\ln(2) \\neq \\ln(3)$, the spectra are disjoint and a unique solution exists.\n\nTo solve for $F_{12}$, we must first compute the diagonal blocks $F_{11}$ and $F_{22}$. For a block triangular matrix, these are given by $F_{11} = f(T_{11})$ and $F_{22} = f(T_{22})$.\nThe function is $f(X) = \\exp(X)$. The matrices $T_{11}$ and $T_{22}$ are $2 \\times 2$ Jordan blocks. For a matrix of the form $J = \\begin{pmatrix} \\lambda  1 \\\\ 0  \\lambda \\end{pmatrix}$, an analytic function $f$ is computed as $f(J) = \\begin{pmatrix} f(\\lambda)  f'(\\lambda) \\\\ 0  f(\\lambda) \\end{pmatrix}$.\nHere, $f(x) = \\exp(x)$, so $f'(x) = \\exp(x)$.\n\nFor $T_{11} = \\begin{pmatrix} \\ln(2)  1 \\\\ 0  \\ln(2) \\end{pmatrix}$, we have $\\lambda_1 = \\ln(2)$.\n$f(\\lambda_1) = \\exp(\\ln(2)) = 2$ and $f'(\\lambda_1) = \\exp(\\ln(2)) = 2$.\nThus,\n$$\nF_{11} = \\exp(T_{11}) = \\begin{pmatrix} 2  2 \\\\ 0  2 \\end{pmatrix}\n$$\n\nFor $T_{22} = \\begin{pmatrix} \\ln(3)  1 \\\\ 0  \\ln(3) \\end{pmatrix}$, we have $\\lambda_2 = \\ln(3)$.\n$f(\\lambda_2) = \\exp(\\ln(3)) = 3$ and $f'(\\lambda_2) = \\exp(\\ln(3)) = 3$.\nThus,\n$$\nF_{22} = \\exp(T_{22}) = \\begin{pmatrix} 3  3 \\\\ 0  3 \\end{pmatrix}\n$$\nWe are given $T_{12} = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} = I_2$, the $2 \\times 2$ identity matrix.\nNow we compute the right-hand side of the Sylvester equation, $C = F_{11}T_{12} - T_{12}F_{22} = F_{11}I_2 - I_2F_{22} = F_{11} - F_{22}$.\n$$\nC = F_{11} - F_{22} = \\begin{pmatrix} 2  2 \\\\ 0  2 \\end{pmatrix} - \\begin{pmatrix} 3  3 \\\\ 0  3 \\end{pmatrix} = \\begin{pmatrix} -1  -1 \\\\ 0  -1 \\end{pmatrix}\n$$\nLet the unknown matrix be $F_{12} = \\begin{pmatrix} a  b \\\\ c  d \\end{pmatrix}$. The Sylvester equation $T_{11}F_{12} - F_{12}T_{22} = C$ becomes:\n$$\n\\begin{pmatrix} \\ln(2)  1 \\\\ 0  \\ln(2) \\end{pmatrix} \\begin{pmatrix} a  b \\\\ c  d \\end{pmatrix} - \\begin{pmatrix} a  b \\\\ c  d \\end{pmatrix} \\begin{pmatrix} \\ln(3)  1 \\\\ 0  \\ln(3) \\end{pmatrix} = \\begin{pmatrix} -1  -1 \\\\ 0  -1 \\end{pmatrix}\n$$\nPerforming the matrix multiplications on the left-hand side:\n$$\n\\begin{pmatrix} a\\ln(2)+c  b\\ln(2)+d \\\\ c\\ln(2)  d\\ln(2) \\end{pmatrix} - \\begin{pmatrix} a\\ln(3)  a+b\\ln(3) \\\\ c\\ln(3)  c+d\\ln(3) \\end{pmatrix} = \\begin{pmatrix} -1  -1 \\\\ 0  -1 \\end{pmatrix}\n$$\nCombining terms gives:\n$$\n\\begin{pmatrix} a(\\ln(2)-\\ln(3))+c  b(\\ln(2)-\\ln(3))-a+d \\\\ c(\\ln(2)-\\ln(3))  d(\\ln(2)-\\ln(3))-c \\end{pmatrix} = \\begin{pmatrix} -1  -1 \\\\ 0  -1 \\end{pmatrix}\n$$\nThis matrix equality yields a system of four linear equations for the variables $a, b, c, d$. Let $\\Delta = \\ln(2) - \\ln(3) = -\\ln(3/2)$.\n1. From the $(2,1)$ entry: $c(\\ln(2)-\\ln(3)) = 0$. Since $\\ln(2) \\neq \\ln(3)$, we must have $c=0$.\n2. From the $(2,2)$ entry: $d(\\ln(2)-\\ln(3)) - c = -1$. Substituting $c=0$, we get $d(\\ln(2)-\\ln(3)) = -1$, so $d = \\frac{-1}{\\ln(2)-\\ln(3)} = \\frac{1}{\\ln(3)-\\ln(2)}$.\n3. From the $(1,1)$ entry: $a(\\ln(2)-\\ln(3)) + c = -1$. Substituting $c=0$, we get $a(\\ln(2)-\\ln(3)) = -1$, so $a = \\frac{-1}{\\ln(2)-\\ln(3)} = \\frac{1}{\\ln(3)-\\ln(2)}$.\n4. From the $(1,2)$ entry: $b(\\ln(2)-\\ln(3)) - a + d = -1$. Substituting the values for $a$ and $d$: $b(\\ln(2)-\\ln(3)) - \\frac{1}{\\ln(3)-\\ln(2)} + \\frac{1}{\\ln(3)-\\ln(2)} = -1$, which simplifies to $b(\\ln(2)-\\ln(3)) = -1$. So, $b=\\frac{-1}{\\ln(2)-\\ln(3)} = \\frac{1}{\\ln(3)-\\ln(2)}$.\n\nThe resulting off-diagonal block is:\n$$\nF_{12} = \\begin{pmatrix}\n\\frac{1}{\\ln(3)-\\ln(2)}  \\frac{1}{\\ln(3)-\\ln(2)} \\\\\n0  \\frac{1}{\\ln(3)-\\ln(2)}\n\\end{pmatrix}\n$$\nThe problem asks for the $(2,2)$ entry of $F_{12}$, which is $d$.\nThe value is $d = \\frac{1}{\\ln(3)-\\ln(2)}$. This can also be written as $\\frac{1}{\\ln(3/2)}$.", "answer": "$$\n\\boxed{\\frac{1}{\\ln(3) - \\ln(2)}}\n$$", "id": "3559901"}, {"introduction": "While eigenvalues determine the long-term behavior of $\\exp(tA)$, they can be misleading about short-term dynamics, especially for non-normal matrices. This exercise [@problem_id:3559880] has you construct a matrix whose exponential exhibits significant transient growth, a phenomenon not predicted by its eigenvalues alone. By analyzing the matrix's field of values, you will gain a deeper, more nuanced understanding of the crucial role of non-normality in numerical analysis and dynamical systems.", "problem": "Consider the matrix exponential $\\exp(A)$ in the context of linear dynamical systems and its sensitivity to non-normality. Recall the field of values (also called numerical range) $W(A) := \\{ x^{*} A x : \\|x\\|_{2} = 1 \\}$ and the induced operator $2$-norm $\\|M\\|_{2} := \\max_{\\|x\\|_{2}=1} \\|M x\\|_{2}$. Construct a concrete, highly non-normal matrix $A \\in \\mathbb{C}^{2 \\times 2}$ of the form $A = -\\alpha I + N$ where $I$ is the identity matrix, $N$ is strictly upper triangular, and $\\alpha  0$, such that all eigenvalues $\\lambda$ of $A$ satisfy $\\operatorname{Re}(\\lambda) \\le 0$, yet $\\|\\exp(A)\\|_{2}$ is large. Using only the foundational definitions of $\\exp(A)$ via its power series, the field of values $W(A)$, and the operator $2$-norm, analyze how $W(A)$ explains the observed large value of $\\|\\exp(A)\\|_{2}$ despite $\\operatorname{Re}(\\lambda) \\le 0$. Specifically:\n\n- Take $A = \\begin{pmatrix} -\\alpha  s \\\\ 0  -\\alpha \\end{pmatrix}$ with $\\alpha = 1$ and $s = 50$.\n- Verify that $\\operatorname{Re}(\\lambda) \\le 0$ for the eigenvalues $\\lambda$ of $A$.\n- Determine $W(A)$ exactly and explain whether and how it intersects the open right half-plane.\n- Derive an exact closed-form expression for $\\|\\exp(A)\\|_{2}$ from first principles.\n- Evaluate $\\|\\exp(A)\\|_{2}$ for the given $\\alpha$ and $s$ and round your final numerical answer to four significant figures.\n\nYour final answer must be the single real number $\\|\\exp(A)\\|_{2}$, rounded to four significant figures, with no units.", "solution": "The problem requires a detailed analysis of a specific $2 \\times 2$ non-normal matrix, $A$, focusing on the relationship between its eigenvalues, its field of values $W(A)$, and the norm of its exponential, $\\|\\exp(A)\\|_2$. We will proceed by addressing each of the specified tasks in order.\n\nThe given matrix is $A = \\begin{pmatrix} -\\alpha  s \\\\ 0  -\\alpha \\end{pmatrix}$ with parameters $\\alpha = 1$ and $s = 50$.\nThus, the matrix under consideration is $A = \\begin{pmatrix} -1  50 \\\\ 0  -1 \\end{pmatrix}$.\n\nFirst, we verify that the real parts of the eigenvalues of $A$ are non-positive.\nSince $A$ is an upper triangular matrix, its eigenvalues are its diagonal entries. Therefore, the eigenvalues are $\\lambda_1 = -1$ and $\\lambda_2 = -1$.\nThe real part of each eigenvalue is $\\operatorname{Re}(\\lambda) = -1$.\nAs $-1 \\le 0$, the condition that $\\operatorname{Re}(\\lambda) \\le 0$ for all eigenvalues $\\lambda$ of $A$ is satisfied. This property suggests that solutions to the linear system of differential equations $\\dot{x} = Ax$ should decay to zero as time $t \\to \\infty$.\n\nSecond, we determine the field of values (or numerical range) of $A$, defined as $W(A) = \\{ x^{*} A x : x \\in \\mathbb{C}^{2}, \\|x\\|_{2} = 1 \\}$.\nLet $x = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}$ be a vector in $\\mathbb{C}^2$ with $\\|x\\|_2 = 1$, which means $|x_1|^2 + |x_2|^2 = 1$.\nWe compute the quadratic form $x^{*} A x$:\n$$ x^{*} A x = \\begin{pmatrix} \\bar{x_1}  \\bar{x_2} \\end{pmatrix} \\begin{pmatrix} -1  50 \\\\ 0  -1 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} $$\n$$ x^{*} A x = \\begin{pmatrix} \\bar{x_1}  \\bar{x_2} \\end{pmatrix} \\begin{pmatrix} -x_1 + 50x_2 \\\\ -x_2 \\end{pmatrix} $$\n$$ x^{*} A x = \\bar{x_1}(-x_1 + 50x_2) - \\bar{x_2}x_2 = -|x_1|^2 + 50\\bar{x_1}x_2 - |x_2|^2 $$\nUsing the normalization condition $|x_1|^2 + |x_2|^2 = 1$, we can simplify this expression:\n$$ x^{*} A x = -(|x_1|^2 + |x_2|^2) + 50\\bar{x_1}x_2 = -1 + 50\\bar{x_1}x_2 $$\nTo determine the set of all possible values for this expression, we must find the range of the term $\\bar{x_1}x_2$. By the Cauchy-Schwarz inequality, $|\\bar{x_1}x_2| = |x_1||x_2| \\le \\|x\\|_2^2 = 1$. More precisely, under the constraint $|x_1|^2+|x_2|^2=1$, the maximum value of $|x_1||x_2|$ occurs when $|x_1|=|x_2|=1/\\sqrt{2}$, which gives $|x_1||x_2| = 1/2$. The phase of $\\bar{x_1}x_2$ can be arbitrary. Thus, the set of values for $\\bar{x_1}x_2$ is a closed disk in the complex plane centered at the origin with radius $1/2$.\nConsequently, the term $50\\bar{x_1}x_2$ can take any value in the closed disk centered at $0$ with radius $50 \\times (1/2) = 25$.\nThe field of values $W(A)$ is this disk shifted by $-1$. Therefore, $W(A)$ is a closed disk of radius $25$ centered at $-1+0i$:\n$$ W(A) = \\{ z \\in \\mathbb{C} : |z - (-1)| \\le 25 \\} $$\nThis disk extends from a minimum real value of $-1 - 25 = -26$ to a maximum real value of $-1 + 25 = 24$.\nSince the maximum real part is $24  0$, the field of values $W(A)$ has a significant intersection with the open right half-plane $\\{ z \\in \\mathbb{C} : \\operatorname{Re}(z)  0 \\}$. This is crucial. The quantity $\\max\\{\\operatorname{Re}(z) : z \\in W(A)\\}$, known as the numerical abscissa, determines the initial growth rate of $\\|\\exp(tA)\\|_2$. A positive numerical abscissa indicates that $\\|\\exp(tA)\\|_2$ will initially grow for $t0$, despite the fact that all eigenvalues lie in the left half-plane. This transient growth is a hallmark of non-normal matrices and explains how $\\|\\exp(A)\\|_2$ can be large.\n\nThird, we derive an exact closed-form expression for $\\|\\exp(A)\\|_2$.\nThe matrix $A$ can be decomposed as $A = -\\alpha I + N$, where $I$ is the $2 \\times 2$ identity matrix and $N = \\begin{pmatrix} 0  s \\\\ 0  0 \\end{pmatrix}$. Since the identity matrix $I$ commutes with any matrix, $-\\alpha I$ and $N$ commute. This property allows us to write $\\exp(A) = \\exp(-\\alpha I + N) = \\exp(-\\alpha I) \\exp(N)$.\nThe exponential of the scalar matrix $-\\alpha I$ is $\\exp(-\\alpha I) = e^{-\\alpha}I$.\nThe matrix $N$ is nilpotent, specifically $N^2 = \\begin{pmatrix} 0  s \\\\ 0  0 \\end{pmatrix} \\begin{pmatrix} 0  s \\\\ 0  0 \\end{pmatrix} = \\begin{pmatrix} 0  0 \\\\ 0  0 \\end{pmatrix}$.\nThe power series for $\\exp(N)$ thus truncates:\n$$ \\exp(N) = I + N + \\frac{N^2}{2!} + \\frac{N^3}{3!} + \\dots = I + N = \\begin{pmatrix} 1  s \\\\ 0  1 \\end{pmatrix} $$\nCombining these results, we find the matrix exponential of $A$:\n$$ \\exp(A) = (e^{-\\alpha}I)(I+N) = e^{-\\alpha}(I+N) = e^{-\\alpha}\\begin{pmatrix} 1  s \\\\ 0  1 \\end{pmatrix} = \\begin{pmatrix} e^{-\\alpha}  s e^{-\\alpha} \\\\ 0  e^{-\\alpha} \\end{pmatrix} $$\nThe induced $2$-norm of a matrix $M$ is given by $\\|M\\|_2 = \\sqrt{\\rho(M^*M)}$, where $\\rho$ denotes the spectral radius (the maximum modulus of the eigenvalues). Let $M = \\exp(A)$.\nWe compute $M^*M$:\n$$ M^*M = \\left( e^{-\\alpha}\\begin{pmatrix} 1  0 \\\\ s  1 \\end{pmatrix} \\right) \\left( e^{-\\alpha}\\begin{pmatrix} 1  s \\\\ 0  1 \\end{pmatrix} \\right) = e^{-2\\alpha}\\begin{pmatrix} 1  s \\\\ s  s^2+1 \\end{pmatrix} $$\nWe need the eigenvalues of the matrix $B = \\begin{pmatrix} 1  s \\\\ s  s^2+1 \\end{pmatrix}$. The characteristic equation is $\\det(B - \\lambda I) = 0$:\n$$ (1-\\lambda)(s^2+1-\\lambda) - s^2 = 0 $$\n$$ \\lambda^2 - (s^2+2)\\lambda + (s^2+1) - s^2 = 0 $$\n$$ \\lambda^2 - (s^2+2)\\lambda + 1 = 0 $$\nUsing the quadratic formula, the eigenvalues of $B$ are:\n$$ \\lambda = \\frac{(s^2+2) \\pm \\sqrt{(s^2+2)^2 - 4}}{2} = \\frac{s^2+2 \\pm \\sqrt{s^4+4s^2}}{2} = \\frac{s^2+2 \\pm |s|\\sqrt{s^2+4}}{2} $$\nThe largest eigenvalue is $\\lambda_{\\max} = \\frac{s^2+2 + |s|\\sqrt{s^2+4}}{2}$.\nThe spectral radius of $M^*M$ is $\\rho(M^*M) = e^{-2\\alpha} \\lambda_{\\max}$.\nThe $2$-norm of $\\exp(A)$ is the square root of this value.\n$$ \\|\\exp(A)\\|_2 = \\sqrt{e^{-2\\alpha} \\frac{s^2+2 + |s|\\sqrt{s^2+4}}{2}} $$\nThere is a known simplification for this expression. The term under the square root is a perfect square:\n$$ \\frac{s^2+2 + |s|\\sqrt{s^2+4}}{2} = \\left( \\frac{|s|+\\sqrt{s^2+4}}{2} \\right)^2 $$\nTherefore, the $2$-norm of $\\exp(A)$ is:\n$$ \\|\\exp(A)\\|_2 = \\sqrt{e^{-2\\alpha} \\left( \\frac{|s|+\\sqrt{s^2+4}}{2} \\right)^2} = e^{-\\alpha} \\frac{|s|+\\sqrt{s^2+4}}{2} $$\n\nFourth, we evaluate this expression for the given values $\\alpha = 1$ and $s = 50$. Since $s=50  0$, $|s|=s$.\n$$ \\|\\exp(A)\\|_2 = e^{-1} \\frac{50+\\sqrt{50^2+4}}{2} = e^{-1} \\frac{50+\\sqrt{2504}}{2} $$\nWe can simplify $\\sqrt{2504} = \\sqrt{4 \\times 626} = 2\\sqrt{626}$.\n$$ \\|\\exp(A)\\|_2 = e^{-1} \\frac{50+2\\sqrt{626}}{2} = e^{-1} (25+\\sqrt{626}) $$\nNow we compute the numerical value:\n$$ \\sqrt{626} \\approx 25.019992006 $$\n$$ 25 + \\sqrt{626} \\approx 50.019992006 $$\n$$ e^{-1} \\approx 0.36787944117 $$\n$$ \\|\\exp(A)\\|_2 \\approx 0.36787944117 \\times 50.019992006 \\approx 18.39999338 $$\nRounding the result to four significant figures gives $18.40$. This large value, significantly greater than $1$, confirms the transient growth predicted by the analysis of the field of values.", "answer": "$$\\boxed{18.40}$$", "id": "3559880"}]}