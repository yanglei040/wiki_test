{"hands_on_practices": [{"introduction": "The journey to mastering the matrix exponential begins with understanding its behavior on non-diagonalizable matrices. This exercise [@problem_id:3591573] takes you back to first principles, asking you to compute the exponential of a fundamental Jordan block directly from its power series definition. By decomposing the matrix into its scalar and nilpotent parts, you will uncover the elegant structure of the resulting exponential matrix, revealing how polynomial terms emerge on the superdiagonals.", "problem": "Let $J \\in \\mathbb{C}^{3 \\times 3}$ be the $3 \\times 3$ Jordan block associated with the eigenvalue $\\lambda \\in \\mathbb{C}$, defined by $J = \\lambda I + N$, where $I$ is the identity matrix and $N$ is nilpotent of index $3$ with ones on the first superdiagonal, that is,\n$$\nN = \\begin{pmatrix}\n0  1  0 \\\\\n0  0  1 \\\\\n0  0  0\n\\end{pmatrix}, \\quad N^{3} = 0.\n$$\nUsing only the power series definition of the matrix exponential and fundamental properties of polynomials in matrices (including the nilpotency of $N$ and commutation with $I$), derive a closed-form expression for $\\exp(J)$ as a $3 \\times 3$ matrix in terms of $\\exp(\\lambda)$ by writing the nilpotent series explicitly. Then, justify from first principles why the nonzero superdiagonal entries of $\\exp(J)$ have polynomial prefactors multiplying $\\exp(\\lambda)$, and identify these prefactors for this case. Your final answer must be a single analytic matrix expression, and no rounding is required. Express the final exponential using $\\exp(\\cdot)$, not $e^{(\\cdot)}$.", "solution": "The problem is to derive a closed-form expression for the matrix exponential $\\exp(J)$, where $J$ is a $3 \\times 3$ Jordan block with eigenvalue $\\lambda$.\n\nFirst, we validate the problem statement.\nThe givens are:\n- $J \\in \\mathbb{C}^{3 \\times 3}$ is a Jordan block.\n- The associated eigenvalue is $\\lambda \\in \\mathbb{C}$.\n- $J$ is defined as $J = \\lambda I + N$, where $I$ is the $3 \\times 3$ identity matrix.\n- $N$ is the nilpotent matrix $N = \\begin{pmatrix} 0  1  0 \\\\ 0  0  1 \\\\ 0  0  0 \\end{pmatrix}$.\n- The nilpotency condition is $N^3=0$.\n\nThe task is to:\n1.  Derive the closed-form expression for $\\exp(J)$ using the power series definition.\n2.  Use the properties that $N$ is nilpotent and commutes with $I$.\n3.  Justify the polynomial prefactors on the superdiagonal entries.\n4.  Identify these prefactors.\n\nThe problem is scientifically grounded in the theory of matrix functions, specifically the matrix exponential. It is well-posed, with all necessary information provided and no contradictions. The language is objective and precise. Therefore, the problem is deemed valid.\n\nWe begin with the power series definition of the matrix exponential:\n$$ \\exp(J) = \\sum_{k=0}^{\\infty} \\frac{J^k}{k!} $$\nSubstituting the definition of $J = \\lambda I + N$:\n$$ \\exp(J) = \\sum_{k=0}^{\\infty} \\frac{(\\lambda I + N)^k}{k!} $$\nThe matrix $\\lambda I$ is a scalar multiple of the identity matrix, and it commutes with any matrix, including $N$. That is, $(\\lambda I)N = \\lambda(IN) = \\lambda N$ and $N(\\lambda I) = \\lambda(NI) = \\lambda N$. Since $\\lambda I$ and $N$ commute, we can apply the property $\\exp(A+B) = \\exp(A)\\exp(B)$ for commuting matrices $A$ and $B$. Let $A = \\lambda I$ and $B = N$.\n$$ \\exp(J) = \\exp(\\lambda I) \\exp(N) $$\nWe evaluate each exponential term separately.\n\nFor the term $\\exp(\\lambda I)$:\n$$ \\exp(\\lambda I) = \\sum_{k=0}^{\\infty} \\frac{(\\lambda I)^k}{k!} = \\sum_{k=0}^{\\infty} \\frac{\\lambda^k I^k}{k!} $$\nSince $I^k = I$ for all integers $k \\geq 0$ (with $I^0=I$), we can factor out $I$:\n$$ \\exp(\\lambda I) = \\left( \\sum_{k=0}^{\\infty} \\frac{\\lambda^k}{k!} \\right) I = \\exp(\\lambda) I $$\nThis is a scalar matrix with $\\exp(\\lambda)$ on its diagonal.\n\nFor the term $\\exp(N)$:\n$$ \\exp(N) = \\sum_{k=0}^{\\infty} \\frac{N^k}{k!} = \\frac{N^0}{0!} + \\frac{N^1}{1!} + \\frac{N^2}{2!} + \\frac{N^3}{3!} + \\dots $$\nBy convention, $N^0 = I$ and $0! = 1$. The problem states that $N$ is nilpotent of index $3$, meaning $N^3 = 0$. This implies that all higher powers are also zero, i.e., $N^k = 0$ for all $k \\geq 3$. The infinite series for $\\exp(N)$ thus truncates to a finite sum:\n$$ \\exp(N) = I + N + \\frac{1}{2!}N^2 $$\nWe calculate the powers of $N$:\n$N^0 = I = \\begin{pmatrix} 1  0  0 \\\\ 0  1  0 \\\\ 0  0  1 \\end{pmatrix}$\n$N^1 = N = \\begin{pmatrix} 0  1  0 \\\\ 0  0  1 \\\\ 0  0  0 \\end{pmatrix}$\n$N^2 = N \\cdot N = \\begin{pmatrix} 0  1  0 \\\\ 0  0  1 \\\\ 0  0  0 \\end{pmatrix} \\begin{pmatrix} 0  1  0 \\\\ 0  0  1 \\\\ 0  0  0 \\end{pmatrix} = \\begin{pmatrix} 0  0  1 \\\\ 0  0  0 \\\\ 0  0  0 \\end{pmatrix}$\n$N^3 = N^2 \\cdot N = \\begin{pmatrix} 0  0  1 \\\\ 0  0  0 \\\\ 0  0  0 \\end{pmatrix} \\begin{pmatrix} 0  1  0 \\\\ 0  0  1 \\\\ 0  0  0 \\end{pmatrix} = \\begin{pmatrix} 0  0  0 \\\\ 0  0  0 \\\\ 0  0  0 \\end{pmatrix}$, which is the zero matrix.\n\nSubstituting these matrices into the expression for $\\exp(N)$:\n$$ \\exp(N) = \\begin{pmatrix} 1  0  0 \\\\ 0  1  0 \\\\ 0  0  1 \\end{pmatrix} + \\begin{pmatrix} 0  1  0 \\\\ 0  0  1 \\\\ 0  0  0 \\end{pmatrix} + \\frac{1}{2} \\begin{pmatrix} 0  0  1 \\\\ 0  0  0 \\\\ 0  0  0 \\end{pmatrix} $$\n$$ \\exp(N) = \\begin{pmatrix} 1  1  \\frac{1}{2} \\\\ 0  1  1 \\\\ 0  0  1 \\end{pmatrix} $$\nNow, we combine the results to find $\\exp(J)$:\n$$ \\exp(J) = \\exp(\\lambda I) \\exp(N) = (\\exp(\\lambda) I) \\exp(N) = \\exp(\\lambda) \\exp(N) $$\n$$ \\exp(J) = \\exp(\\lambda) \\begin{pmatrix} 1  1  \\frac{1}{2} \\\\ 0  1  1 \\\\ 0  0  1 \\end{pmatrix} = \\begin{pmatrix} \\exp(\\lambda)  \\exp(\\lambda)  \\frac{1}{2}\\exp(\\lambda) \\\\ 0  \\exp(\\lambda)  \\exp(\\lambda) \\\\ 0  0  \\exp(\\lambda) \\end{pmatrix} $$\nThis is the required closed-form expression.\n\nNext, we justify from first principles why the nonzero superdiagonal entries of $\\exp(J)$ have polynomial prefactors multiplying $\\exp(\\lambda)$.\nStarting from the power series and using the binomial theorem for commuting matrices $\\lambda I$ and $N$:\n$$ J^k = (\\lambda I + N)^k = \\sum_{j=0}^{k} \\binom{k}{j} (\\lambda I)^{k-j} N^j = \\sum_{j=0}^{k} \\binom{k}{j} \\lambda^{k-j} N^j $$\nDue to nilpotency, $N^j = 0$ for $j \\ge 3$, so for any $k$, the sum truncates at $j=2$:\n$$ J^k = \\binom{k}{0}\\lambda^k I + \\binom{k}{1}\\lambda^{k-1} N + \\binom{k}{2}\\lambda^{k-2} N^2 $$\n(where $\\binom{k}{j}=0$ if $kj$).\nSubstituting this into the series for $\\exp(J)$:\n$$ \\exp(J) = \\sum_{k=0}^{\\infty} \\frac{1}{k!} \\left( \\lambda^k I + k\\lambda^{k-1} N + \\frac{k(k-1)}{2}\\lambda^{k-2} N^2 \\right) $$\nBy rearranging the summation, we can group terms by powers of $N$:\n$$ \\exp(J) = \\left( \\sum_{k=0}^{\\infty} \\frac{\\lambda^k}{k!} \\right) I + \\left( \\sum_{k=1}^{\\infty} \\frac{k\\lambda^{k-1}}{k!} \\right) N + \\left( \\sum_{k=2}^{\\infty} \\frac{k(k-1)\\lambda^{k-2}}{2k!} \\right) N^2 $$\nWe recognize each sum:\n- The coefficient of $I$ is $\\sum_{k=0}^{\\infty} \\frac{\\lambda^k}{k!} = \\exp(\\lambda)$.\n- The coefficient of $N$ is $\\sum_{k=1}^{\\infty} \\frac{\\lambda^{k-1}}{(k-1)!} = \\exp(\\lambda)$.\n- The coefficient of $N^2$ is $\\frac{1}{2} \\sum_{k=2}^{\\infty} \\frac{\\lambda^{k-2}}{(k-2)!} = \\frac{1}{2}\\exp(\\lambda)$.\nIn general, the coefficient of $N^j$ is $\\frac{1}{j!} \\exp(\\lambda)$.\nSo, $\\exp(J) = \\exp(\\lambda)I + \\exp(\\lambda)N + \\frac{1}{2}\\exp(\\lambda)N^2$.\n\nThe \"prefactors\" multiplying $\\exp(\\lambda)$ come from the coefficients $1, 1, \\frac{1}{2}$. These arise from the $\\frac{1}{j!}$ terms multiplying each power of $N$. The matrix $N^j$ has ones on the $j$-th superdiagonal and zeros elsewhere. Thus, the entry on the $j$-th superdiagonal of $\\exp(J)$ is given by $\\frac{1}{j!}\\exp(\\lambda)$. The prefactor is $\\frac{1}{j!}$, which is a constant and thus a polynomial of degree $0$.\n\nFor this specific case, we identify these prefactors:\n- The first superdiagonal corresponds to $j=1$. The entries are $(\\exp(J))_{1,2}$ and $(\\exp(J))_{2,3}$. The prefactor is $\\frac{1}{1!} = 1$.\n- The second superdiagonal corresponds to $j=2$. The entry is $(\\exp(J))_{1,3}$. The prefactor is $\\frac{1}{2!} = \\frac{1}{2}$.\nThis explains the structure of the resulting matrix.", "answer": "$$\n\\boxed{\\begin{pmatrix} \\exp(\\lambda)  \\exp(\\lambda)  \\frac{1}{2}\\exp(\\lambda) \\\\ 0  \\exp(\\lambda)  \\exp(\\lambda) \\\\ 0  0  \\exp(\\lambda) \\end{pmatrix}}\n$$", "id": "3591573"}, {"introduction": "Many large-scale problems in science and engineering feature matrices with inherent block structures, and exploiting this structure is key to efficient computation. This practice [@problem_id:3591556] challenges you to move from theoretical calculation to algorithmic design by focusing on block upper triangular matrices. You will derive and implement two distinct methods for computing the off-diagonal block of the matrix exponential, gaining direct experience with the crucial role of spectral separation in numerical stability.", "problem": "You are given a block upper triangular matrix of the form\n$$\nA \\;=\\; \\begin{bmatrix} B  E \\\\ 0  C \\end{bmatrix},\n$$\nwhere $B \\in \\mathbb{C}^{m \\times m}$, $C \\in \\mathbb{C}^{n \\times n}$, and $E \\in \\mathbb{C}^{m \\times n}$, with $m, n \\in \\mathbb{N}$. Your tasks are to derive structure-preserving formulas for the matrix exponential, analyze stability in terms of spectral separation, and implement algorithms that exploit the triangular structure to compute the off-diagonal block efficiently.\n\nFundamental base you may use includes the following core definitions and facts:\n- The matrix exponential is defined by the absolutely convergent series\n$$\n\\exp(A) \\;=\\; \\sum_{k=0}^{\\infty} \\frac{A^{k}}{k!}.\n$$\n- For block upper triangular $A$, the product $A^{k}$ is also block upper triangular for each $k \\in \\mathbb{N}$.\n- If $L(X) = BX - XC$ and $\\operatorname{vec}(X)$ stacks the columns of $X$, then $\\operatorname{vec}(L(X)) = \\left(I_{n} \\otimes B - C^{\\top} \\otimes I_{m}\\right)\\operatorname{vec}(X)$, where $\\otimes$ denotes the Kronecker product.\n- For a square matrix $M$, the (matrix) $1$-norm is $\\|M\\|_{1} = \\max_{j} \\sum_{i} |M_{ij}|$.\n\nDo not assume any additional formulas beyond these foundational facts. In particular, do not assume any prepackaged formula for the off-diagonal block of $\\exp(A)$; you must derive it from first principles as specified below.\n\nYour tasks:\n\n1. Starting only from the series definition of the matrix exponential, show that $\\exp(A)$ is block upper triangular, and derive an explicit block series expression for the off-diagonal block $X$ of $\\exp(A)$ in terms of $B$, $E$, and $C$. Then derive an equivalent integral representation for $X$ expressed as an integral over a scalar parameter. Finally, use differentiation under the integral sign to derive a Sylvester-type linear matrix equation that $X$ satisfies. State conditions on $B$ and $C$ under which this Sylvester equation has a unique solution and discuss how these conditions relate to numerical stability via the spectral separation\n$$\n\\operatorname{sep}(B,C) \\;=\\; \\sigma_{\\min}\\!\\left(I_{n} \\otimes B \\;-\\; C^{\\top} \\otimes I_{m}\\right),\n$$\nwhere $\\sigma_{\\min}$ denotes the smallest singular value. Your analysis should be entirely in terms of $B$, $E$, and $C$ and should not assume commutativity unless you explicitly justify it.\n\n2. Propose two algorithms to compute the off-diagonal block $X$ exploiting the block upper triangular structure:\n   - A quadrature-based algorithm that evaluates the integral representation using Gaussian quadrature on $[0,1]$.\n   - A Sylvester-solve-based algorithm that first computes $\\exp(B)$ and $\\exp(C)$ and then solves the Sylvester-type linear matrix equation for $X$.\n   For each algorithm, discuss computational costs in big-$\\mathcal{O}$ notation as a function of $m$, $n$, and the chosen quadrature size, and comment on expected numerical stability in relation to $\\operatorname{sep}(B,C)$.\n\n3. Implement both algorithms and validate them numerically against a dense reference computation of $\\exp(A)$ on the full block matrix. For each test case specified below, construct $A$ from $(B,E,C)$, compute the reference $\\exp(A)$, compute $X$ via both algorithms, assemble the corresponding structured approximations to $\\exp(A)$, and report:\n   - The relative $1$-norm error of the Sylvester-solve-based assembly with respect to the dense reference,\n     $$\n     \\mathrm{err}_{\\mathrm{syl}} \\;=\\; \\frac{\\left\\|\\exp(A)_{\\mathrm{ref}} - \\begin{bmatrix}\\exp(B)  X_{\\mathrm{syl}} \\\\ 0  \\exp(C)\\end{bmatrix}\\right\\|_{1}}{\\left\\|\\exp(A)_{\\mathrm{ref}}\\right\\|_{1}}.\n     $$\n   - The relative $1$-norm error of the integral-quadrature-based assembly with respect to the dense reference,\n     $$\n     \\mathrm{err}_{\\mathrm{int}} \\;=\\; \\frac{\\left\\|\\exp(A)_{\\mathrm{ref}} - \\begin{bmatrix}\\exp(B)  X_{\\mathrm{int}} \\\\ 0  \\exp(C)\\end{bmatrix}\\right\\|_{1}}{\\left\\|\\exp(A)_{\\mathrm{ref}}\\right\\|_{1}}.\n     $$\n   - The spectral separation $\\operatorname{sep}(B,C)$.\n\nUse Gaussian quadrature of order $q = 64$ on $[0,1]$ for the integral-based computation. You may use the Singular Value Decomposition (SVD) to evaluate $\\operatorname{sep}(B,C)$.\n\nThe test suite consists of the following four cases. All matrices are real; you may treat $\\mathbb{R}$ as a subfield of $\\mathbb{C}$:\n- Case $1$ ($m = 2$, $n = 2$): \n  $$\n  B = \\begin{bmatrix} -1  2 \\\\ 0  -2 \\end{bmatrix},\\quad\n  C = \\begin{bmatrix} -\\tfrac{1}{2}  \\tfrac{1}{2} \\\\ 0  -3 \\end{bmatrix},\\quad\n  E = \\begin{bmatrix} 0.3  -0.2 \\\\ 0.1  0.4 \\end{bmatrix}.\n  $$\n- Case $2$ ($m = 2$, $n = 2$): let $\\delta = 10^{-6}$ and\n  $$\n  B = \\begin{bmatrix} 0  1 \\\\ 0  0 \\end{bmatrix},\\quad\n  C = \\begin{bmatrix} \\delta  1 \\\\ 0  \\delta \\end{bmatrix},\\quad\n  E = \\begin{bmatrix} 1  0 \\\\ 0  -1 \\end{bmatrix}.\n  $$\n- Case $3$ ($m = 3$, $n = 1$): \n  $$\n  B = \\begin{bmatrix} 0.1  1  0 \\\\ 0  0.2  1 \\\\ 0  0  0.3 \\end{bmatrix},\\quad\n  C = \\begin{bmatrix} -0.25 \\end{bmatrix},\\quad\n  E = \\begin{bmatrix} 1 \\\\ 0.5 \\\\ -0.25 \\end{bmatrix}.\n  $$\n- Case $4$ ($m = 2$, $n = 2$): \n  $$\n  B = \\begin{bmatrix} 5  0 \\\\ 1  4 \\end{bmatrix},\\quad\n  C = \\begin{bmatrix} -3  1 \\\\ 0  -2 \\end{bmatrix},\\quad\n  E = \\begin{bmatrix} 0.2  -0.1 \\\\ 0.05  0.05 \\end{bmatrix}.\n  $$\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list of four entries, one per test case, enclosed in square brackets. Each entry must itself be a list of three floating-point numbers in scientific notation representing $[\\mathrm{err}_{\\mathrm{syl}}, \\mathrm{err}_{\\mathrm{int}}, \\operatorname{sep}(B,C)]$. For example:\n$$\n\\big[ [x_{1}, y_{1}, z_{1}], [x_{2}, y_{2}, z_{2}], [x_{3}, y_{3}, z_{3}], [x_{4}, y_{4}, z_{4}] \\big].\n$$\n\nNo physical units are involved. Angles do not appear. Percentages must not be used; all reported quantities are dimensionless floating-point numbers. The program must be completely self-contained and perform no input or output other than printing this single line. The numerical libraries you may use are fixed by the execution environment specification.", "solution": "The problem requires a comprehensive theoretical derivation, algorithmic design, and numerical implementation for computing the exponential of a block upper triangular matrix. We shall proceed by first validating the problem statement and then addressing each of the three tasks in sequence.\n\nThe problem is determined to be valid as it is scientifically grounded in established principles of numerical linear algebra, is well-posed with a clear and consistent set of givens and objectives, and is free of any subjective or non-formalizable content. All definitions and tasks are standard within the specified field.\n\n### Task 1: Derivations and Analysis\n\nLet the block upper triangular matrix be $A = \\begin{bmatrix} B  E \\\\ 0  C \\end{bmatrix}$, where $B \\in \\mathbb{C}^{m \\times m}$, $C \\in \\mathbb{C}^{n \\times n}$, and $E \\in \\mathbb{C}^{m \\times n}$.\n\n**1. Block Structure of $\\exp(A)$ and Series for Off-Diagonal Block**\n\nWe begin by examining the powers of $A$. It is given that $A^k$ is block upper triangular. Let us establish the form of $A^k$ by induction. Let $A^k = \\begin{bmatrix} B^k  X_k \\\\ 0  C^k \\end{bmatrix}$. The base case $k=0$ gives $A^0 = I = \\begin{bmatrix} I_m  0 \\\\ 0  I_n \\end{bmatrix}$, so $X_0 = 0$. For $k=1$, $A^1 = A = \\begin{bmatrix} B  E \\\\ 0  C \\end{bmatrix}$, so $X_1 = E$.\nAssume the hypothesis holds for an integer $k \\ge 1$. Then,\n$$\nA^{k+1} = A^k A = \\begin{bmatrix} B^k  X_k \\\\ 0  C^k \\end{bmatrix} \\begin{bmatrix} B  E \\\\ 0  C \\end{bmatrix} = \\begin{bmatrix} B^{k+1}  B^k E + X_k C \\\\ 0  C^{k+1} \\end{bmatrix}.\n$$\nThis confirms that $A^{k+1}$ is also block upper triangular, with $X_{k+1} = B^k E + X_k C$. Unrolling this recurrence relation for $X_k$, we find:\n$X_1 = E$\n$X_2 = BE + X_1C = BE + EC$\n$X_3 = B^2E + X_2C = B^2E + (BE+EC)C = B^2E + BEC + EC^2$\nThe general form is a sum of all possible products of $B$'s and $C$'s sandwiching $E$, with a total of $k-1$ factors of $B$ and $C$. This can be written as:\n$$\nX_k = \\sum_{j=0}^{k-1} B^{k-1-j} E C^j.\n$$\nThe matrix exponential is defined by the series $\\exp(A) = \\sum_{k=0}^{\\infty} \\frac{A^k}{k!}$. Substituting the block form of $A^k$:\n$$\n\\exp(A) = \\sum_{k=0}^{\\infty} \\frac{1}{k!} \\begin{bmatrix} B^k  X_k \\\\ 0  C^k \\end{bmatrix} = \\begin{bmatrix} \\sum_{k=0}^{\\infty} \\frac{B^k}{k!}  \\sum_{k=0}^{\\infty} \\frac{X_k}{k!} \\\\ 0  \\sum_{k=0}^{\\infty} \\frac{C^k}{k!} \\end{bmatrix}.\n$$\nRecognizing the series for the matrix exponential, we obtain the block structure:\n$$\n\\exp(A) = \\begin{bmatrix} \\exp(B)  X \\\\ 0  \\exp(C) \\end{bmatrix},\n$$\nwhere the off-diagonal block $X$ is given by the series:\n$$\nX = \\sum_{k=0}^{\\infty} \\frac{X_k}{k!} = \\sum_{k=1}^{\\infty} \\frac{1}{k!} \\left( \\sum_{j=0}^{k-1} B^{k-1-j} E C^j \\right).\n$$\nThis is the explicit block series expression for $X$. Note that the sum for $X$ starts at $k=1$ since $X_0=0$.\n\n**2. Integral Representation for $X$**\n\nA more compact representation for $X$ can be derived by considering the matrix-valued function $F(s) = \\exp(As)$ for $s \\in \\mathbb{R}$. We know $\\frac{dF}{ds} = A F(s)$ and $F(0) = I$. Let the block structure of $F(s)$ be $F(s) = \\begin{bmatrix} \\exp(Bs)  Y(s) \\\\ 0  \\exp(Cs) \\end{bmatrix}$. Differentiating with respect to $s$ gives:\n$$\n\\frac{dF}{ds} = \\begin{bmatrix} B\\exp(Bs)  Y'(s) \\\\ 0  C\\exp(Cs) \\end{bmatrix}.\n$$\nThe product $AF(s)$ is:\n$$\nA F(s) = \\begin{bmatrix} B  E \\\\ 0  C \\end{bmatrix} \\begin{bmatrix} \\exp(Bs)  Y(s) \\\\ 0  \\exp(Cs) \\end{bmatrix} = \\begin{bmatrix} B\\exp(Bs)  B Y(s) + E \\exp(Cs) \\\\ 0  C\\exp(Cs) \\end{bmatrix}.\n$$\nEquating the $(1,2)$ blocks of $\\frac{dF}{ds}$ and $AF(s)$ yields a linear ordinary differential equation for $Y(s)$:\n$$\nY'(s) = B Y(s) + E \\exp(Cs),\n$$\nwith initial condition $Y(0)=0$ (from $F(0)=I$). We solve this using an integrating factor, $\\exp(-Bs)$. Pre-multiplying by $\\exp(-Bs)$ gives:\n$$\n\\exp(-Bs)Y'(s) - \\exp(-Bs)B Y(s) = \\exp(-Bs) E \\exp(Cs).\n$$\nThe left side is the derivative of a product: $\\frac{d}{ds}(\\exp(-Bs)Y(s))$. Integrating from $0$ to a parameter $t$:\n$$\n\\int_0^t \\frac{d}{ds}(\\exp(-Bs)Y(s)) ds = \\int_0^t \\exp(-Bs) E \\exp(Cs) ds.\n$$\nApplying the fundamental theorem of calculus to the left side:\n$$\n\\exp(-Bt)Y(t) - \\exp(-B\\cdot 0)Y(0) = \\exp(-Bt)Y(t).\n$$\nThus, $Y(t) = \\exp(Bt) \\int_0^t \\exp(-Bs) E \\exp(Cs) ds = \\int_0^t \\exp(B(t-s)) E \\exp(Cs) ds$.\nThe off-diagonal block $X$ of $\\exp(A)$ corresponds to $Y(1)$.\n$$\nX = Y(1) = \\int_0^1 \\exp(B(1-s)) E \\exp(Cs) ds.\n$$\nThis is the required integral representation.\n\n**3. Sylvester Equation for $X$**\n\nTo derive the Sylvester equation for $X$, we use differentiation under the integral sign as suggested. Consider the function $G(t) = \\exp(B(1-t)) E \\exp(Ct)$. The integral representation is $X = \\int_0^1 G(t) dt$.\nLet us compute the derivative of $G(t)$ with respect to $t$:\n\\begin{align*}\n\\frac{dG}{dt} = \\frac{d}{dt} \\left( \\exp(B(1-t)) \\right) E \\exp(Ct) + \\exp(B(1-t)) E \\frac{d}{dt} \\left( \\exp(Ct) \\right) \\\\\n= \\exp(B(1-t))(-B) E \\exp(Ct) + \\exp(B(1-t)) E \\exp(Ct)C \\\\\n= -B G(t) + G(t) C.\n\\end{align*}\nNow, we integrate this identity from $t=0$ to $t=1$:\n$$\n\\int_0^1 \\frac{dG}{dt} dt = \\int_0^1 (-B G(t) + G(t) C) dt.\n$$\nThe left side evaluates to $G(1) - G(0)$:\n$$\nG(1) - G(0) = \\exp(B(0)) E \\exp(C) - \\exp(B(1)) E \\exp(0) = E\\exp(C) - \\exp(B)E.\n$$\nThe right side can be split due to linearity of integration:\n$$\n\\int_0^1 (-B G(t) + G(t) C) dt = -B \\left(\\int_0^1 G(t) dt\\right) + \\left(\\int_0^1 G(t) dt\\right) C = -B X + X C.\n$$\nEquating the two results gives $E\\exp(C) - \\exp(B)E = -B X + X C$. Rearranging this yields the Sylvester equation:\n$$\nBX - XC = \\exp(B)E - E\\exp(C).\n$$\nThis equation is of the form $L(X) = F$, where $L(X) = BX-XC$ is the Sylvester operator and $F = \\exp(B)E - E\\exp(C)$.\n\n**4. Uniqueness of Solution and Numerical Stability**\n\nUsing the provided identity, the Sylvester equation $BX-XC=F$ can be written in vectorized form as a linear system:\n$$\n(I_n \\otimes B - C^T \\otimes I_m) \\operatorname{vec}(X) = \\operatorname{vec}(F).\n$$\nThis system has a unique solution for any right-hand side $F$ if and only if the coefficient matrix $K = I_n \\otimes B - C^T \\otimes I_m$ is invertible. A matrix is invertible if and only if all its eigenvalues are non-zero. The eigenvalues of $K$ are given by $\\lambda_i(B) - \\lambda_j(C)$ for all pairs of eigenvalues $\\lambda_i(B) \\in \\Lambda(B)$ and $\\lambda_j(C) \\in \\Lambda(C)$. Therefore, a unique solution for $X$ exists if and only if $\\Lambda(B) \\cap \\Lambda(C) = \\emptyset$, i.e., the spectra of $B$ and $C$ are disjoint.\n\nNumerical stability concerns the sensitivity of the solution $X$ to perturbations in the input data ($B$, $C$, $E$, and consequently $F$). For the linear system $K \\operatorname{vec}(X) = \\operatorname{vec}(F)$, the condition number of $K$ governs this sensitivity. The condition number is proportional to the inverse of the smallest singular value of $K$, $\\sigma_{\\min}(K)$. The spectral separation, $\\operatorname{sep}(B, C)$, is defined as precisely this value:\n$$\n\\operatorname{sep}(B, C) = \\sigma_{\\min}(I_n \\otimes B - C^T \\otimes I_m).\n$$\nA small value of $\\operatorname{sep}(B, C)$ implies that $K$ is ill-conditioned (close to singular). This occurs when an eigenvalue of $B$ is close to an eigenvalue of $C$. In this scenario, solving the Sylvester equation is numerically unstable: small errors in the computation of $\\exp(B)$, $\\exp(C)$, or $E$, or even representation errors in the matrices themselves, can be amplified by a factor proportional to $1/\\operatorname{sep}(B, C)$, leading to a large error in the computed solution $X$.\n\n### Task 2: Algorithms\n\n**1. Quadrature-Based Algorithm**\nThis algorithm approximates the integral $X = \\int_0^1 \\exp(B(1-t)) E \\exp(Ct) dt$ using numerical quadrature. Specifically, we use $q$-point Gaussian quadrature on $[0,1]$.\nThe integral is approximated by a weighted sum: $X \\approx \\sum_{i=1}^q w_i \\exp(B(1-t_i)) E \\exp(Ct_i)$, where $\\{t_i\\}$ are the quadrature nodes in $[0,1]$ and $\\{w_i\\}$ are the corresponding weights.\n- **Algorithm**:\n    1. Obtain the $q$ nodes $\\{s_i\\}$ and weights $\\{\\hat{w}_i\\}$ for standard Gauss-Legendre quadrature on $[-1,1]$.\n    2. Transform them to the interval $[0,1]$: $t_i = (s_i+1)/2$, $w_i = \\hat{w}_i/2$.\n    3. Initialize $X_{\\mathrm{int}} = 0_{m \\times n}$.\n    4. For $i = 1, \\dots, q$:\n        a. Evaluate $M_i = \\exp(B(1-t_i))$ and $N_i = \\exp(Ct_i)$.\n        b. Compute the term $T_i = w_i M_i E N_i$.\n        c. Accumulate the result: $X_{\\mathrm{int}} = X_{\\mathrm{int}} + T_i$.\n- **Computational Cost**: The primary cost lies within the loop, which executes $q$ times. Inside the loop, we compute two matrix exponentials, $\\exp(B(1-t_i))$ and $\\exp(Ct_i)$, at costs of $\\mathcal{O}(m^3)$ and $\\mathcal{O}(n^3)$ respectively (using standard methods like scaling and squaring). The matrix products cost $\\mathcal{O}(m^2n + mn^2)$. The total cost is therefore $\\mathcal{O}(q(m^3+n^3))$.\n- **Numerical Stability**: This method is generally stable. Its accuracy depends on how well the polynomial approximation inherent in Gaussian quadrature captures the behavior of the integrand. The method's stability is not directly and catastrophically dependent on $\\operatorname{sep}(B,C)$. Even if $\\Lambda(B) \\cap \\Lambda(C) \\neq \\emptyset$, the integral is well-defined, and the method will converge as $q \\to \\infty$. It is expected to be more robust than the Sylvester method when $\\operatorname{sep}(B,C)$ is small.\n\n**2. Sylvester-Solve-Based Algorithm**\nThis algorithm directly solves the linear matrix equation $BX - XC = \\exp(B)E - E\\exp(C)$.\n- **Algorithm**:\n    1. Compute $\\exp(B)$ and $\\exp(C)$.\n    2. Form the right-hand side matrix $F = \\exp(B)E - E\\exp(C)$.\n    3. Solve the Sylvester equation $BX - XC = F$ for $X_{\\mathrm{syl}}$. Standard solvers like `scipy.linalg.solve_sylvester` use the Bartels-Stewart algorithm.\n- **Computational Cost**: Step 1 costs $\\mathcal{O}(m^3+n^3)$. Step 2 costs $\\mathcal{O}(m^2n+mn^2)$. Step 3, using the Bartels-Stewart algorithm, involves Schur decompositions of $B$ and $C$ ($\\mathcal{O}(m^3+n^3)$) followed by a substitution process ($\\mathcal{O}(mn(m+n))$). The overall cost is dominated by the matrix exponentials and Schur decompositions, resulting in a total complexity of $\\mathcal{O}(m^3+n^3)$.\n- **Numerical Stability**: As analyzed in Task 1, the stability of this method is critically dependent on $\\operatorname{sep}(B,C)$. The method is expected to be highly inaccurate and numerically unstable if $\\operatorname{sep}(B,C)$ is small, as the underlying linear system becomes severely ill-conditioned.\n\n### Task 3: Numerical Implementation and Validation\n\nThe implementation will follow the algorithms described in Task 2. We will use `numpy` for matrix algebra, `scipy.linalg.expm` for the reference exponential and for algorithm components, `scipy.linalg.solve_sylvester` for the Sylvester solver, `numpy.polynomial.legendre.leggauss` for quadrature points, and `scipy.linalg.svd` to compute the singular values needed for $\\operatorname{sep}(B,C)$. The results for the four specified test cases are computed and formatted as requested. The analysis of the test cases suggests that Case 2, with $\\delta=10^{-6}$, will highlight the stability differences between the two algorithms due to its very small spectral separation. The other cases have well-separated spectra and should yield accurate results from both methods.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import expm, solve_sylvester, svd\n\ndef solve():\n    \"\"\"\n    Solves the problem for the given test cases.\n    It derives and implements two algorithms to compute the off-diagonal block\n    of the exponential of a block upper triangular matrix.\n    \"\"\"\n    \n    # Define the quadrature order as specified in the problem statement.\n    q = 64\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (\n            np.array([[-1.0, 2.0], [0.0, -2.0]]),\n            np.array([[-0.5, 0.5], [0.0, -3.0]]),\n            np.array([[0.3, -0.2], [0.1, 0.4]])\n        ),\n        (\n            np.array([[0.0, 1.0], [0.0, 0.0]]),\n            np.array([[1e-6, 1.0], [0.0, 1e-6]]),\n            np.array([[1.0, 0.0], [0.0, -1.0]])\n        ),\n        (\n            np.array([[0.1, 1.0, 0.0], [0.0, 0.2, 1.0], [0.0, 0.0, 0.3]]),\n            np.array([[-0.25]]),\n            np.array([[1.0], [0.5], [-0.25]])\n        ),\n        (\n            np.array([[5.0, 0.0], [1.0, 4.0]]),\n            np.array([[-3.0, 1.0], [0.0, -2.0]]),\n            np.array([[0.2, -0.1], [0.05, 0.05]])\n        )\n    ]\n\n    results = []\n    \n    # Get Gauss-Legendre quadrature nodes and weights for interval [-1, 1]\n    # np.polynomial.legendre.leggauss is part of numpy and its use is permitted.\n    s, w_hat = np.polynomial.legendre.leggauss(q)\n    # Transform nodes and weights to interval [0, 1]\n    t_quad = (s + 1.0) / 2.0\n    w_quad = w_hat / 2.0\n\n    for B, C, E in test_cases:\n        m, _ = B.shape\n        n, _ = C.shape\n\n        # 1. Construct the full matrix A and compute the reference solution.\n        A = np.block([\n            [B, E],\n            [np.zeros((n, m)), C]\n        ])\n        exp_A_ref = expm(A)\n        norm_ref = np.linalg.norm(exp_A_ref, ord=1)\n        \n        # Pre-compute diagonal blocks of the exponential.\n        exp_B = expm(B)\n        exp_C = expm(C)\n\n        # 2. Algorithm 1: Sylvester-solve-based method.\n        F = exp_B @ E - E @ exp_C\n        try:\n            # The Sylvester equation is BX - XC = F, so we solve for X with C - -C\n            X_syl = solve_sylvester(B, -C, F)\n        except Exception:\n            # In case of failure, though solve_sylvester is robust\n            X_syl = np.full((m, n), np.nan)\n            \n        exp_A_syl = np.block([\n            [exp_B, X_syl],\n            [np.zeros((n, m)), exp_C]\n        ])\n        err_syl = np.linalg.norm(exp_A_ref - exp_A_syl, ord=1) / norm_ref\n\n        # 3. Algorithm 2: Integral-quadrature-based method.\n        X_int = np.zeros((m, n))\n        for i in range(q):\n            ti = t_quad[i]\n            wi = w_quad[i]\n            # Integrand is exp(B(1-t)) * E * exp(Ct)\n            term = expm(B * (1.0 - ti)) @ E @ expm(C * ti)\n            X_int += wi * term\n\n        exp_A_int = np.block([\n            [exp_B, X_int],\n            [np.zeros((n, m)), exp_C]\n        ])\n        err_int = np.linalg.norm(exp_A_ref - exp_A_int, ord=1) / norm_ref\n\n        # 4. Compute spectral separation sep(B, C).\n        # sep(B,C) = sigma_min(I_n kron B - C^T kron I_m)\n        K = np.kron(np.eye(n), B) - np.kron(C.T, np.eye(m))\n        # scipy.linalg.svd returns singular values in descending order.\n        singular_values = svd(K, compute_uv=False)\n        sep_BC = singular_values[-1]\n\n        results.append([err_syl, err_int, sep_BC])\n    \n    # Format the final output string.\n    output_str = \"[\"\n    for i, res in enumerate(results):\n        output_str += f\"[{res[0]:.6e}, {res[1]:.6e}, {res[2]:.6e}]\"\n        if i  len(results) - 1:\n            output_str += \", \"\n    output_str += \"]\"\n    \n    print(output_str)\n\nsolve()\n```", "id": "3591556"}, {"introduction": "While the eigenvalues of a matrix determine the long-term fate of a linear dynamical system, they do not tell the whole story. This hands-on coding exercise [@problem_id:3591545] delves into the fascinating phenomenon of transient growth, where stable systems can exhibit large, temporary amplification due to matrix nonnormality. By constructing and analyzing specific nonnormal matrices, you will numerically connect this behavior to the geometry of the pseudospectrum, a powerful tool for understanding system dynamics that eigenvalues alone cannot fully describe.", "problem": "You must write a complete, runnable program that constructs explicit stable but highly nonnormal matrices and quantitatively demonstrates large transient growth in the matrix exponential. The task must be grounded in the core definition of the matrix exponential and the $\\varepsilon$-pseudospectrum, and must produce numerical evidence connecting the observed transient growth to the $\\varepsilon$-pseudospectral abscissa and a computable pseudospectral bound on the matrix exponential via a Laplace-transform contour. All quantities are purely mathematical and dimensionless, so no physical units or angles are involved.\n\nUse the following foundational definitions and facts as the base of your derivations and computations:\n\n- The matrix exponential is defined by the power series $e^{tA} = \\sum_{k=0}^{\\infty} \\frac{t^k}{k!} A^k$ for any complex square matrix $A$ and real scalar $t \\ge 0$.\n- The spectral abscissa is $\\alpha(A) := \\max\\{\\operatorname{Re}(\\lambda) : \\lambda \\in \\sigma(A)\\}$, where $\\sigma(A)$ denotes the spectrum of $A$ (the set of eigenvalues).\n- The operator $2$-norm of a matrix $M$ is $\\|M\\|_2 := \\sigma_{\\max}(M)$, the largest singular value. In particular, for any invertible matrix $B$, $\\|B^{-1}\\|_2 = 1 / \\sigma_{\\min}(B)$.\n- For $\\varepsilon  0$, the $\\varepsilon$-pseudospectrum is $\\Lambda_{\\varepsilon}(A) := \\{ z \\in \\mathbb{C} : \\|(zI - A)^{-1}\\|_2 \\ge 1/\\varepsilon \\}$, and the $\\varepsilon$-pseudospectral abscissa is $\\alpha_{\\varepsilon}(A) := \\sup\\{\\operatorname{Re}(z) : z \\in \\Lambda_{\\varepsilon}(A)\\}$.\n- The Laplace transform representation of the matrix exponential for any fixed $\\gamma \\in \\mathbb{R}$ such that $\\gamma  \\alpha(A)$ is\n$$\ne^{tA} = \\frac{1}{2\\pi i} \\int_{\\gamma - i\\infty}^{\\gamma + i\\infty} e^{tz} (zI - A)^{-1} \\, dz,\n$$\nand consequently\n$$\n\\|e^{tA}\\|_2 \\le \\frac{e^{\\gamma t}}{2\\pi} \\int_{-\\infty}^{\\infty} \\|( \\gamma + i\\omega)I - A \\|_2^{-1} \\, d\\omega.\n$$\nIf $\\gamma  \\alpha_{\\varepsilon}(A)$ then for all $\\omega \\in \\mathbb{R}$, $\\|( \\gamma + i\\omega)I - A \\|_2^{-1}  1/\\varepsilon$. Truncating the integral to $|\\omega| \\le \\Omega$ yields a computable bound \n$$\n\\|e^{tA}\\|_2 \\le \\frac{e^{\\gamma t}}{2\\pi} \\int_{-\\Omega}^{\\Omega} \\|( \\gamma + i\\omega)I - A \\|_2^{-1} \\, d\\omega,\n$$\nwhich we will evaluate numerically as an upper bound on the truncated contour.\n\nYour program must implement the following steps for the specified test suite:\n\n1. Construct three test matrices $A$ at the stated dimensions and parameters, each with $\\sigma(A)$ strictly in the left half-plane:\n   - Case $1$: $A_1 = \\operatorname{diag}(-1, -2, -3)$.\n   - Case $2$: $A_2 = \\begin{bmatrix} -1  K_2 \\\\ 0  -10 \\end{bmatrix}$ with $K_2 = 300$. This is upper triangular with a large superdiagonal creating nonnormality.\n   - Case $3$: $A_3 = -I + K_3 N$ where $I$ is the $3 \\times 3$ identity, $N$ is the $3 \\times 3$ nilpotent with ones on its first superdiagonal and zeros elsewhere, and $K_3 = 25$. Explicitly, $N = \\begin{bmatrix} 0  1  0 \\\\ 0  0  1 \\\\ 0  0  0 \\end{bmatrix}$.\n\n2. For each matrix $A$, compute the observed transient growth by evaluating $\\| e^{tA} \\|_2$ on a uniform time grid $t \\in [0, T]$ with $T = 8$ and $N_t = 601$ points (including endpoints). Record the maximum amplification\n   $$\n   M_{\\mathrm{obs}} := \\max_{t \\in \\{t_j\\}_{j=0}^{N_t-1}} \\| e^{tA} \\|_2\n   $$\n   and the maximizing grid time $t^\\star$ at which the maximum is attained (if multiple, choose the smallest such $t$ on the grid). Also compute the spectral abscissa $\\alpha(A)$.\n\n3. For each matrix $A$ and each $\\varepsilon \\in \\{ 10^{-1}, 5 \\cdot 10^{-2} \\}$, approximate $\\alpha_{\\varepsilon}(A)$ by sampling the resolvent norm over a rectangular grid in the complex plane:\n   - Real axis range: $\\operatorname{Re}(z) \\in [ -5 , 1.5 ]$ with step $\\Delta x = 0.05$.\n   - Imaginary axis range: $\\operatorname{Im}(z) \\in [ -20 , 20 ]$ with step $\\Delta y = 0.1$.\n   At each grid point $z = x + i y$, compute $\\|(zI - A)^{-1}\\|_2 = 1 / \\sigma_{\\min}(zI - A)$, and mark it if the value is at least $1/\\varepsilon$. Estimate $\\alpha_{\\varepsilon}(A)$ as the maximum real part $x$ among the marked points (if none are marked, report the minimal real grid value $-5$, which indicates the chosen grid failed to reach the threshold, but this should not occur for the specified matrices and $\\varepsilon$ values).\n\n4. For each matrix $A$, form a truncated pseudospectral bound at time $t = t^\\star$ for $\\varepsilon = 10^{-1}$ as follows:\n   - Set $\\gamma = \\alpha_{\\varepsilon}(A) + \\delta$ with $\\delta = 2 \\cdot 10^{-2}$. This ensures $\\gamma  \\alpha_{\\varepsilon}(A)$.\n   - Numerically approximate\n     $$\n     B_{\\varepsilon}(t^\\star) := \\frac{e^{\\gamma t^\\star}}{2\\pi} \\int_{-\\Omega}^{\\Omega} \\|(\\gamma + i\\omega)I - A \\|_2^{-1} \\, d\\omega\n     $$\n     using the trapezoidal rule with $\\Omega = 50$ and a uniform grid of $N_\\omega = 1001$ nodes on $[-\\Omega, \\Omega]$. This $B_{\\varepsilon}(t^\\star)$ is a computable upper bound on the truncated Bromwich contour and quantifies the growth predicted by the pseudospectral abscissa.\n\n5. For each test case, produce a result list containing exactly six entries:\n   - The observed maximum amplification $M_{\\mathrm{obs}}$ (a float).\n   - The maximizing grid time $t^\\star$ (a float).\n   - The spectral abscissa $\\alpha(A)$ (a float).\n   - The $\\varepsilon$-pseudospectral abscissa $\\alpha_{\\varepsilon}(A)$ for $\\varepsilon = 10^{-1}$ (a float).\n   - The $\\varepsilon$-pseudospectral abscissa $\\alpha_{\\varepsilon}(A)$ for $\\varepsilon = 5 \\cdot 10^{-2}$ (a float).\n   - The truncated pseudospectral bound $B_{\\varepsilon}(t^\\star)$ for $\\varepsilon = 10^{-1}$ (a float).\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with one inner list per test case and each inner list containing the six floats in the order specified above. For example, the output must look like\n$[ [ r_{1,1}, r_{1,2}, r_{1,3}, r_{1,4}, r_{1,5}, r_{1,6} ], [ r_{2,1}, r_{2,2}, r_{2,3}, r_{2,4}, r_{2,5}, r_{2,6} ], [ r_{3,1}, r_{3,2}, r_{3,3}, r_{3,4}, r_{3,5}, r_{3,6} ] ]$\nwith each $r_{i,j}$ printed as a Python float.\n\nThis test suite probes:\n- A normal, diagonal matrix (baseline, no transient growth).\n- A $2 \\times 2$ upper-triangular, strongly nonnormal matrix (significant transient growth).\n- A $3 \\times 3$ Jordan-type, strongly nonnormal matrix (polynomially weighted transient response).\nThe results demonstrate how nonnormality allows $\\|e^{tA}\\|_2$ to far exceed $e^{t \\alpha(A)}$, connect this to $\\alpha_{\\varepsilon}(A)$ becoming nonnegative for small $\\varepsilon$, and quantify the growth using a truncated pseudospectral bound built from $\\Lambda_{\\varepsilon}(A)$.", "solution": "The problem requires a numerical demonstration of the transient growth phenomenon in linear dynamical systems governed by $\\dot{\\boldsymbol{x}} = A \\boldsymbol{x}$, whose solution is $\\boldsymbol{x}(t) = e^{tA} \\boldsymbol{x}(0)$. The analysis hinges on the properties of the matrix exponential $e^{tA}$, specifically its operator $2$-norm, $\\|e^{tA}\\|_2$. For a stable matrix $A$, defined by a spectral abscissa $\\alpha(A)  0$, the long-term behavior is decay, i.e., $\\lim_{t \\to \\infty} \\|e^{tA}\\|_2 = 0$. However, if $A$ is nonnormal (i.e., $A^*A \\ne AA^*$), $\\|e^{tA}\\|_2$ can exhibit substantial transient growth, achieving values much larger than $1$ before decaying. This behavior cannot be predicted by the eigenvalues alone. The $\\varepsilon$-pseudospectrum, $\\Lambda_{\\varepsilon}(A)$, provides a more complete picture by characterizing the sensitivity of eigenvalues to perturbations. Significant transient growth is linked to the $\\varepsilon$-pseudospectral abscissa, $\\alpha_{\\varepsilon}(A)$, being positive for small values of $\\varepsilon$, even when $\\alpha(A)$ is negative. This indicates that a small perturbation to $A$ can shift eigenvalues into the right half-plane, a hallmark of nonnormal systems prone to transient effects. We will verify these principles by constructing and analyzing three distinct matrices.\n\nFirst, the three test matrices are constructed as per the problem specification.\nCase $1$: A normal (diagonal) matrix, which is not expected to exhibit transient growth.\n$$\nA_1 = \\operatorname{diag}(-1, -2, -3) = \\begin{bmatrix} -1  0  0 \\\\ 0  -2  0 \\\\ 0  0  -3 \\end{bmatrix}\n$$\nCase $2$: A nonnormal upper-triangular matrix with a large superdiagonal element $K_2=300$, designed to produce significant transient growth.\n$$\nA_2 = \\begin{bmatrix} -1  300 \\\\ 0  -10 \\end{bmatrix}\n$$\nCase $3$: A nonnormal matrix constructed from a nilpotent Jordan block. This structure is known to cause polynomially-weighted growth. Here, $K_3=25$.\n$$\nA_3 = -I + 25 N = \\begin{bmatrix} -1  25  0 \\\\ 0  -1  25 \\\\ 0  0  -1 \\end{bmatrix}\n$$\nAll three matrices have eigenvalues entirely in the strict left half-plane, meaning they are asymptotically stable.\n\nSecond, we analyze the observed transient growth. For each matrix $A$, we compute $\\|e^{tA}\\|_2$ over a discrete time grid $t_j \\in [0, T]$ with $T=8$ and $N_t = 601$ points. The maximum amplification is $M_{\\mathrm{obs}} = \\max_{j} \\|e^{t_j A}\\|_2$, and $t^\\star$ is the earliest time at which this maximum occurs. The matrix exponential $e^{tA}$ is computed using the `scipy.linalg.expm` function, and the operator $2$-norm is found via `numpy.linalg.norm`. The spectral abscissa $\\alpha(A) = \\max\\{\\operatorname{Re}(\\lambda) : \\lambda \\in \\sigma(A)\\}$ is also computed from the eigenvalues of $A$. For a normal matrix like $A_1$, we expect $M_{\\mathrm{obs}}=1$ at $t^\\star=0$. For the nonnormal matrices $A_2$ and $A_3$, we anticipate $M_{\\mathrm{obs}} \\gg 1$ for some $t^\\star  0$.\n\nThird, we approximate the $\\varepsilon$-pseudospectral abscissa, $\\alpha_{\\varepsilon}(A)$, for $\\varepsilon \\in \\{10^{-1}, 5 \\cdot 10^{-2}\\}$. By definition, $\\Lambda_{\\varepsilon}(A) = \\{ z \\in \\mathbb{C} : \\|(zI - A)^{-1}\\|_2 \\ge 1/\\varepsilon \\}$. The norm of the resolvent, $\\|(zI - A)^{-1}\\|_2$, is equal to $1/\\sigma_{\\min}(zI - A)$, where $\\sigma_{\\min}$ is the smallest singular value. Thus, the condition is equivalent to $\\sigma_{\\min}(zI - A) \\le \\varepsilon$. We discretize a rectangular region of the complex plane with grid points $z_{jk} = x_j + i y_k$, where $x_j \\in [-5, 1.5]$ and $y_k \\in [-20, 20]$. At each point, we compute $\\sigma_{\\min}(z_{jk}I - A)$ and check if it is less than or equal to $\\varepsilon$. The value of $\\alpha_{\\varepsilon}(A)$ is then estimated as the maximum real part $x_j$ among all grid points $z_{jk}$ that satisfy the condition. A positive $\\alpha_{\\varepsilon}(A)$ for a stable matrix $A$ is a strong indicator of potential transient growth.\n\nFourth, we compute a quantitative bound on the transient growth using the pseudospectrum. The Laplace transform representation of the matrix exponential leads to the inequality $\\|e^{tA}\\|_2 \\le \\frac{e^{\\gamma t}}{2\\pi} \\int_{-\\infty}^{\\infty} \\|((\\gamma + i\\omega)I - A)^{-1}\\|_2 \\, d\\omega$ for any $\\gamma  \\alpha(A)$. By choosing $\\gamma$ to be slightly larger than $\\alpha_{\\varepsilon}(A)$, specifically $\\gamma = \\alpha_{\\varepsilon}(A) + \\delta$ with $\\delta = 2 \\cdot 10^{-2}$ and $\\varepsilon = 10^{-1}$, we place the integration contour just to the right of the $\\varepsilon$-pseudospectrum. The bound can be approximated by truncating the integral and evaluating it numerically. We compute the truncated pseudospectral bound at the observed peak time $t=t^\\star$:\n$$\nB_{\\varepsilon}(t^\\star) = \\frac{e^{\\gamma t^\\star}}{2\\pi} \\int_{-\\Omega}^{\\Omega} \\|(\\gamma + i\\omega)I - A \\|_2^{-1} \\, d\\omega\n$$\nThe integral is evaluated using the trapezoidal rule over a grid of $N_{\\omega}=1001$ points in the interval $[-\\Omega, \\Omega]$ with $\\Omega=50$. The integrand $\\|(\\gamma + i\\omega)I - A\\|_2^{-1}$ is once again computed as $1/\\sigma_{\\min}((\\gamma + i\\omega)I-A)$. This bound, $B_{\\varepsilon}(t^\\star)$, provides a theoretical estimate for the maximum observed amplification, $M_{\\mathrm{obs}}$, derived directly from the geometry of the pseudospectrum.\n\nFinally, for each of the three test cases, we compile a list of six numerical results in the specified order: the observed maximum amplification $M_{\\mathrm{obs}}$, the time $t^\\star$ of maximum amplification, the spectral abscissa $\\alpha(A)$, the two estimated $\\varepsilon$-pseudospectral abscissas for $\\varepsilon = 10^{-1}$ and $\\varepsilon = 5 \\cdot 10^{-2}$, and the computed truncated pseudospectral bound $B_{\\varepsilon}(t^\\star)$ for $\\varepsilon = 10^{-1}$. This provides a comprehensive picture, connecting the abstract theory of nonnormality and pseudospectra to concrete numerical observations of transient dynamics.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import expm\n\ndef solve():\n    \"\"\"\n    Constructs stable but nonnormal matrices and demonstrates transient growth in the matrix exponential,\n    connecting it to the epsilon-pseudospectrum.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    K2 = 300.0\n    K3 = 25.0\n    A1 = np.diag([-1.0, -2.0, -3.0])\n    A2 = np.array([[-1.0, K2], [0.0, -10.0]])\n    A3 = np.array([[-1.0, K3, 0.0], [0.0, -1.0, K3], [0.0, 0.0, -1.0]])\n\n    test_cases = [A1, A2, A3]\n\n    # Shared parameters for all cases\n    T = 8.0\n    Nt = 601\n    t_grid = np.linspace(0, T, Nt)\n\n    eps_vals = [1e-1, 5e-2]\n\n    x_range = np.arange(-5.0, 1.5 + 0.05, 0.05)\n    y_range = np.arange(-20.0, 20.0 + 0.1, 0.1)\n\n    delta_bound = 2e-2\n    Omega_bound = 50.0\n    N_omega_bound = 1001\n\n    all_results = []\n\n    for A in test_cases:\n        # Step 2: Compute observed transient growth\n        n = A.shape[0]\n        identity = np.eye(n)\n        \n        etA_norms = [np.linalg.norm(expm(A * t), ord=2) for t in t_grid]\n        \n        M_obs = np.max(etA_norms)\n        # Find the smallest t at which the maximum occurs\n        t_star_idx = np.argmax(etA_norms)\n        t_star = t_grid[t_star_idx]\n\n        alpha_A = np.max(np.real(np.linalg.eigvals(A)))\n\n        # Step 3: Approximate epsilon-pseudospectral abscissa\n        alpha_eps_results = []\n        for eps in eps_vals:\n            max_x = -5.0  # Default value if no point is found\n            \n            # Using np.linalg.svd is more numerically stable than inv\n            # We check for sigma_min(zI - A) = eps\n            for x in x_range[::-1]: # Search from right to left for efficiency\n                found_in_col = False\n                for y in y_range:\n                    z = x + 1j * y\n                    # Smallest singular value of (zI - A)\n                    s_min = np.linalg.svd(z * identity - A, compute_uv=False)[-1]\n                    if s_min = eps:\n                        max_x = x\n                        found_in_col = True\n                        break # Found for this x, go to the next (larger) x\n                if found_in_col:\n                    break # exit x-loop\n            alpha_eps_results.append(max_x)\n        \n        alpha_eps_1 = alpha_eps_results[0]\n        alpha_eps_2 = alpha_eps_results[1]\n\n        # Step 4: Compute truncated pseudospectral bound\n        eps_bound = 1e-1\n        alpha_eps_A_for_bound = alpha_eps_1\n        \n        gamma = alpha_eps_A_for_bound + delta_bound\n        \n        omega_grid = np.linspace(-Omega_bound, Omega_bound, N_omega_bound)\n        \n        integrand_values = []\n        for omega in omega_grid:\n            z = gamma + 1j * omega\n            # resolvent norm is 1 / sigma_min\n            s_min = np.linalg.svd(z * identity - A, compute_uv=False)[-1]\n            integrand_values.append(1.0 / s_min)\n            \n        integral = np.trapz(integrand_values, omega_grid)\n        \n        B_eps = (np.exp(gamma * t_star) / (2 * np.pi)) * integral\n\n        # Step 5: Collate results\n        case_results = [\n            M_obs,\n            t_star,\n            alpha_A,\n            alpha_eps_1,\n            alpha_eps_2,\n            B_eps\n        ]\n        all_results.append(case_results)\n\n    # Final print statement in the exact required format.\n    # Convert nested list to string with desired formatting\n    result_str = \"[\" + \", \".join([str(res) for res in all_results]) + \"]\"\n    print(result_str)\n\nsolve()\n```", "id": "3591545"}]}