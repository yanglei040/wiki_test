{"hands_on_practices": [{"introduction": "The Bartels-Stewart algorithm is the cornerstone direct method for solving the Sylvester equation $AX + XB = C$. This practice delves into the core of the algorithm, which operates on the real Schur forms of $A$ and $B$. By deriving the block back-substitution procedure, you will gain a concrete understanding of how the full-sized Sylvester equation is systematically broken down into a sequence of smaller, easily solvable linear systems [@problem_id:3578476].", "problem": "Let $T \\in \\mathbb{R}^{n \\times n}$ and $S \\in \\mathbb{R}^{m \\times m}$ be in real Schur form, that is, both are block upper quasi-triangular with diagonal blocks of sizes $1 \\times 1$ or $2 \\times 2$. Consider the Sylvester equation $T Y + Y S = \\tilde{C}$ for an unknown matrix $Y \\in \\mathbb{R}^{n \\times m}$ and a given right-hand side $\\tilde{C} \\in \\mathbb{R}^{n \\times m}$. Partition $T$ and $S$ along their Schur blocks and conformally partition $Y$ and $\\tilde{C}$ into block matrices $Y_{ij}$ and $\\tilde{C}_{ij}$, where each $Y_{ij}$ has size equal to the size of the $i$-th diagonal block of $T$ by the size of the $j$-th diagonal block of $S$.\n\nStarting from the definitions of real Schur form, the Sylvester equation, and the upper triangular block structure, derive the block back-substitution equations that isolate $Y_{ij}$ in terms of previously computed blocks. Explicitly write the coefficient operator for $Y_{ij}$ in the four cases determined by the diagonal block sizes:\n\n- $1 \\times 1$ in $T$ and $1 \\times 1$ in $S$,\n- $2 \\times 2$ in $T$ and $1 \\times 1$ in $S$,\n- $1 \\times 1$ in $T$ and $2 \\times 2$ in $S$,\n- $2 \\times 2$ in $T$ and $2 \\times 2$ in $S$.\n\nFor each case, express the resulting linear system to be solved for $Y_{ij}$, identifying the explicit coefficient matrix in terms of the corresponding diagonal blocks of $T$ and $S$.\n\nFinally, specialize to the $2 \\times 2$–$2 \\times 2$ case. Assume the $2 \\times 2$ diagonal block of $T$ has complex eigenvalues $\\lambda$ and $\\overline{\\lambda}$, and the $2 \\times 2$ diagonal block of $S$ has complex eigenvalues $\\mu$ and $\\overline{\\mu}$, with each block in real Schur form. Let $M$ be the $4 \\times 4$ coefficient matrix of the vectorized linear system governing the $2 \\times 2$–$2 \\times 2$ block $Y_{ij}$. Compute $\\det(M)$ and provide a closed-form analytic expression for it solely in terms of $\\lambda$, $\\overline{\\lambda}$, $\\mu$, and $\\overline{\\mu}$. No rounding is required; provide the exact expression.", "solution": "The problem asks for the derivation of the block back-substitution method for the Sylvester equation $T Y + Y S = \\tilde{C}$, where $T$ and $S$ are in real Schur form, and for an analysis of the local linear systems to be solved, including a determinant calculation for a specific case.\n\n### Part 1: Derivation of the Block Back-Substitution Equations\nLet the matrices $T \\in \\mathbb{R}^{n \\times n}$ and $S \\in \\mathbb{R}^{m \\times m}$ be partitioned into blocks according to their real Schur form. This means they are block upper quasi-triangular. Let $T$ have $p$ diagonal blocks and $S$ have $q$ diagonal blocks.\n$$\nT = \\begin{pmatrix} T_{11} & T_{12} & \\cdots & T_{1p} \\\\ & T_{22} & \\cdots & T_{2p} \\\\ & & \\ddots & \\vdots \\\\ & & & T_{pp} \\end{pmatrix}, \\quad\nS = \\begin{pmatrix} S_{11} & S_{12} & \\cdots & S_{1q} \\\\ & S_{22} & \\cdots & S_{2q} \\\\ & & \\ddots & \\vdots \\\\ & & & S_{qq} \\end{pmatrix}\n$$\nHere, $T_{ik} = 0$ for $i>k$ and $S_{lj} = 0$ for $l>j$. The diagonal blocks $T_{ii}$ and $S_{jj}$ are of size $1 \\times 1$ or $2 \\times 2$. The matrices $Y \\in \\mathbb{R}^{n \\times m}$ and $\\tilde{C} \\in \\mathbb{R}^{n \\times m}$ are partitioned conformally, with blocks $Y_{ij}$ and $\\tilde{C}_{ij}$ of size $n_i \\times m_j$, where $n_i$ is the size of $T_{ii}$ and $m_j$ is the size of $S_{jj}$.\n\nThe Sylvester equation $T Y + Y S = \\tilde{C}$ can be written in block form. The $(i,j)$-th block of the equation is given by:\n$$\n(T Y)_{ij} + (Y S)_{ij} = \\tilde{C}_{ij}\n$$\nThe $(i,j)$-th block of the matrix product $TY$ is $\\sum_{k=1}^{p} T_{ik} Y_{kj}$. Since $T$ is block upper triangular ($T_{ik}=0$ for $i>k$), this sum reduces to $\\sum_{k=i}^{p} T_{ik} Y_{kj}$.\nThe $(i,j)$-th block of $YS$ is $\\sum_{l=1}^{q} Y_{il} S_{lj}$. Since $S$ is block upper triangular ($S_{lj}=0$ for $l>j$), this sum reduces to $\\sum_{l=1}^{j} Y_{il} S_{lj}$.\n\nSubstituting these into the block equation gives:\n$$\n\\sum_{k=i}^{p} T_{ik} Y_{kj} + \\sum_{l=1}^{j} Y_{il} S_{lj} = \\tilde{C}_{ij}\n$$\nWe can isolate the terms containing $Y_{ij}$ (which correspond to $k=i$ in the first sum and $l=j$ in the second sum):\n$$\n\\left( T_{ii} Y_{ij} + \\sum_{k=i+1}^{p} T_{ik} Y_{kj} \\right) + \\left( Y_{ij} S_{jj} + \\sum_{l=1}^{j-1} Y_{il} S_{lj} \\right) = \\tilde{C}_{ij}\n$$\nRearranging to solve for the terms with $Y_{ij}$:\n$$\nT_{ii} Y_{ij} + Y_{ij} S_{jj} = \\tilde{C}_{ij} - \\sum_{k=i+1}^{p} T_{ik} Y_{kj} - \\sum_{l=1}^{j-1} Y_{il} S_{lj}\n$$\nThis equation defines a small Sylvester equation for the block $Y_{ij}$. The right-hand side, let's call it $C'_{ij}$, depends only on blocks $Y_{kj}$ with $k > i$ (i.e., blocks in rows below the $i$-th block-row) and blocks $Y_{il}$ with $l < j$ (i.e., blocks in columns to the left of the $j$-th block-column).\n\nThis structure suggests a specific order of computation for the blocks of $Y$. We can compute the blocks $Y_{ij}$ by iterating $i$ from $p$ down to $1$, and for each $i$, iterating $j$ from $1$ up to $q$. At step $(i,j)$, all required blocks $Y_{kj}$ ($k>i$) and $Y_{il}$ ($l<j$) have already been computed in previous steps.\n\n### Part 2 & 3: The Four Cases for the Local Sylvester Equation\nAt each step $(i,j)$ of the back-substitution, we must solve a small Sylvester equation of the form $A X + X B = C$, where $A=T_{ii}$, $B=S_{jj}$, $C=C'_{ij}$, and $X=Y_{ij}$. This equation can be converted into a standard linear system $M \\cdot \\text{vec}(X) = \\text{vec}(C)$ using the vectorization operator $\\text{vec}(\\cdot)$ and the Kronecker product $\\otimes$. The coefficient matrix $M$ is given by $M = I_{m_j} \\otimes T_{ii} + S_{jj}^{\\top} \\otimes I_{n_i}$, where $n_i, m_j$ are the dimensions of $T_{ii}$ and $S_{jj}$ respectively.\n\nThe operator for $Y_{ij}$ is $\\mathcal{L}_{ij}(X) = T_{ii}X + X S_{jj}$. We now analyze the four cases based on the block sizes.\n\n\\textbf{Case 1: $T_{ii}$ is $1 \\times 1$, $S_{jj}$ is $1 \\times 1$.}\nLet $T_{ii} = [t_{ii}]$ and $S_{jj} = [s_{jj}]$. Then $Y_{ij}=[y_{ij}]$ is a scalar. The equation is:\n$t_{ii} y_{ij} + y_{ij} s_{jj} = c'_{ij}$, which simplifies to $(t_{ii} + s_{jj}) y_{ij} = c'_{ij}$.\nThis is a scalar linear equation. The coefficient \"matrix\" is the $1 \\times 1$ matrix $[t_{ii} + s_{jj}]$.\n\n\\textbf{Case 2: $T_{ii}$ is $2 \\times 2$, $S_{jj}$ is $1 \\times 1$.}\nLet $S_{jj} = [s_{jj}]$. $Y_{ij}$ is a $2 \\times 1$ column vector. The equation is:\n$T_{ii} Y_{ij} + Y_{ij} s_{jj} = C'_{ij}$.\nThis can be rewritten as $(T_{ii} + s_{jj} I_2) Y_{ij} = C'_{ij}$.\nThis is a $2 \\times 2$ linear system for the vector $Y_{ij}$. The coefficient matrix is $M = T_{ii} + s_{jj} I_2$.\n\n\\textbf{Case 3: $T_{ii}$ is $1 \\times 1$, $S_{jj}$ is $2 \\times 2$.}\nLet $T_{ii} = [t_{ii}]$. $Y_{ij}$ is a $1 \\times 2$ row vector. The equation is:\n$t_{ii} Y_{ij} + Y_{ij} S_{jj} = C'_{ij}$.\nTaking the transpose of the entire equation gives $Y_{ij}^{\\top} t_{ii} + S_{jj}^{\\top} Y_{ij}^{\\top} = (C'_{ij})^{\\top}$.\nThis can be rewritten as $(S_{jj}^{\\top} + t_{ii} I_2) Y_{ij}^{\\top} = (C'_{ij})^{\\top}$.\nThis is a $2 \\times 2$ linear system for the vector $Y_{ij}^{\\top}$. The coefficient matrix is $M = S_{jj}^{\\top} + t_{ii} I_2$.\n\n\\textbf{Case 4: $T_{ii}$ is $2 \\times 2$, $S_{jj}$ is $2 \\times 2$.}\n$Y_{ij}$ is a $2 \\times 2$ matrix. The equation is $T_{ii} Y_{ij} + Y_{ij} S_{jj} = C'_{ij}$.\nWe vectorize this equation. Let $y = \\text{vec}(Y_{ij})$ and $c' = \\text{vec}(C'_{ij})$. The system becomes $M y = c'$. The coefficient matrix $M$ is a $4 \\times 4$ matrix given by:\n$M = I_2 \\otimes T_{ii} + S_{jj}^{\\top} \\otimes I_2$.\nLet $T_{ii} = \\begin{pmatrix} t_{11} & t_{12} \\\\ t_{21} & t_{22} \\end{pmatrix}$ and $S_{jj} = \\begin{pmatrix} s_{11} & s_{12} \\\\ s_{21} & s_{22} \\end{pmatrix}$.\nThen $M = \\begin{pmatrix} T_{ii} & 0 \\\\ 0 & T_{ii} \\end{pmatrix} + \\begin{pmatrix} s_{11}I_2 & s_{21}I_2 \\\\ s_{12}I_2 & s_{22}I_2 \\end{pmatrix} = \\begin{pmatrix} T_{ii} + s_{11}I_2 & s_{21}I_2 \\\\ s_{12}I_2 & T_{ii} + s_{22}I_2 \\end{pmatrix}$.\nExplicitly:\n$$\nM = \\begin{pmatrix}\nt_{11}+s_{11} & t_{12} & s_{21} & 0 \\\\\nt_{21} & t_{22}+s_{11} & 0 & s_{21} \\\\\ns_{12} & 0 & t_{11}+s_{22} & t_{12} \\\\\n0 & s_{12} & t_{21} & t_{22}+s_{22}\n\\end{pmatrix}\n$$\nSolving this $4 \\times 4$ system gives the entries of $Y_{ij}$.\n\n### Part 4: Determinant of the $4 \\times 4$ Coefficient Matrix\nWe are given that the $2 \\times 2$ block $T_{ii}$ has complex eigenvalues $\\lambda$ and $\\overline{\\lambda}$, and the $2 \\times 2$ block $S_{jj}$ has complex eigenvalues $\\mu$ and $\\overline{\\mu}$. We need to compute the determinant of the $4 \\times 4$ coefficient matrix $M = I_2 \\otimes T_{ii} + S_{jj}^{\\top} \\otimes I_2$.\n\nA fundamental theorem of Kronecker products states that the eigenvalues of a Kronecker sum of the form $I \\otimes A + B \\otimes I$ are all possible sums of the form $\\alpha_k + \\beta_l$, where $\\alpha_k$ is an eigenvalue of $A$ and $\\beta_l$ is an eigenvalue of $B$. In our case, the matrix is $M = I_2 \\otimes T_{ii} + S_{jj}^{\\top} \\otimes I_2$. The eigenvalues of $T_{ii}$ are $\\{\\lambda, \\overline{\\lambda}\\}$ and the eigenvalues of $S_{jj}^{\\top}$ are the same as those of $S_{jj}$, which are $\\{\\mu, \\overline{\\mu}\\}$.\n\nThus, the four eigenvalues of the $4 \\times 4$ matrix $M$ are the four possible sums of an eigenvalue from $T_{ii}$ and an eigenvalue from $S_{jj}$:\n$1$. $\\lambda + \\mu$\n$2$. $\\lambda + \\overline{\\mu}$\n$3$. $\\overline{\\lambda} + \\mu$\n$4$. $\\overline{\\lambda} + \\overline{\\mu}$\n\nThe determinant of a matrix is the product of its eigenvalues. Therefore,\n$$\n\\det(M) = (\\lambda + \\mu)(\\lambda + \\overline{\\mu})(\\overline{\\lambda} + \\mu)(\\overline{\\lambda} + \\overline{\\mu})\n$$\nThis expression is a product of complex numbers. Since the original matrices are real, the eigenvalues of $M$ must appear in conjugate pairs. Indeed, $\\overline{(\\lambda+\\mu)} = \\overline{\\lambda}+\\overline{\\mu}$ and $\\overline{(\\lambda+\\overline{\\mu})} = \\overline{\\lambda}+\\mu$. We can group the terms into conjugate pairs to make it evident that the determinant is real:\n$$\n\\det(M) = [(\\lambda + \\mu)(\\overline{\\lambda} + \\overline{\\mu})] \\cdot [(\\lambda + \\overline{\\mu})(\\overline{\\lambda} + \\mu)]\n$$\nUsing the property $z \\overline{z} = |z|^2$:\n$$\n\\det(M) = |\\lambda + \\mu|^2 |\\lambda + \\overline{\\mu}|^2\n$$\nThe problem asks for the answer solely in terms of $\\lambda, \\overline{\\lambda}, \\mu, \\overline{\\mu}$. The expanded product form is the most direct representation that satisfies this.", "answer": "$$\n\\boxed{(\\lambda+\\mu)(\\overline{\\lambda}+\\overline{\\mu})(\\lambda+\\overline{\\mu})(\\overline{\\lambda}+\\mu)}\n$$", "id": "3578476"}, {"introduction": "Sylvester equations are not just an abstract algebraic curiosity; they are a fundamental tool in matrix perturbation theory. This exercise demonstrates their power by showing how the first-order perturbation of an invariant subspace is governed by the solution to a specific Sylvester equation. Completing this practice will connect the mechanics of solving Sylvester equations to the vital task of understanding how stable spectral decompositions are under changes to the matrix [@problem_id:3578469].", "problem": "Consider a real square matrix $A \\in \\mathbb{R}^{n \\times n}$ that admits a real Schur decomposition $A = Q T Q^{\\top}$, where $Q \\in \\mathbb{R}^{n \\times n}$ is orthogonal and $T \\in \\mathbb{R}^{n \\times n}$ is upper quasi-triangular. Let $A(t) = A + t E$ be a smooth perturbation with $E \\in \\mathbb{R}^{n \\times n}$ fixed and $t \\in \\mathbb{R}$ a small scalar, and suppose $A(t)$ is represented in real Schur form as $A(t) = Q(t) T(t) Q(t)^{\\top}$ with $Q(0) = Q$ and $T(0) = T$. Partition the Schur form with respect to a selected invariant subspace of dimension $k$ by writing\n$$\nT = \\begin{bmatrix}\nT_{11} & T_{12} \\\\\n0 & T_{22}\n\\end{bmatrix}, \\quad Q = \\begin{bmatrix} Q_1 & Q_2 \\end{bmatrix},\n$$\nwhere $T_{11} \\in \\mathbb{R}^{k \\times k}$ corresponds to the selected cluster of eigenvalues, $T_{22} \\in \\mathbb{R}^{(n-k) \\times (n-k)}$ corresponds to its complement, and $T_{12} \\in \\mathbb{R}^{k \\times (n-k)}$.\n\nStarting from first principles in numerical linear algebra, namely the differentiability of orthogonal factorizations and the real Schur form, derive the first-order relation connecting $E$, the derivative of the Schur factors, and the commutator induced by the skew-symmetric generator of $Q(t)$. Specifically, prove that if one enforces that the lower-left block of the derivative of $T$ vanishes at $t = 0$ to preserve the block upper-triangular structure, then the off-diagonal block of the generator of $Q(t)$ must solve a Sylvester equation. Your derivation must begin from the identities $Q(t)^{\\top} Q(t) = I$ and $A(t) = Q(t) T(t) Q(t)^{\\top}$ and proceed by differentiating with respect to $t$ at $t = 0$, using only these fundamental properties and the definition of the matrix commutator.\n\nImplement an algorithm that, given $(Q, T)$, a perturbation $E$, and the block size $k$, performs the following steps:\n- Computes $F = Q^{\\top} E Q$ and partitions it conformably with $T$.\n- Solves for the off-diagonal block of the skew-symmetric generator $\\Omega \\in \\mathbb{R}^{n \\times n}$ in the relation $Q^{\\top} \\dot{Q} = \\Omega$ by formulating and solving the appropriate Sylvester equation.\n- Constructs a skew-symmetric $\\Omega$ with the solved off-diagonal blocks and zeros on the diagonal blocks, and computes the first-order perturbation $dT$ that is consistent with the derived relation.\n- Forms an updated orthogonal factor $Q' = Q + \\varepsilon Q \\Omega$ with a small step $\\varepsilon$ and implements an orthogonality control step to re-orthonormalize $Q'$ using a numerically stable method that preserves proximity to $Q'$ in the Frobenius norm.\n\nYour program must produce quantitative diagnostics that validate the correctness of the Sylvester equation solution and the effectiveness of the orthogonality control. For each test case, compute:\n1. The Frobenius norm of the Sylvester residual $\\| T_{22} \\Omega_{21} - \\Omega_{21} T_{11} + F_{21} \\|_F$.\n2. The Frobenius norm of the lower-left block of $dT$, namely $\\| (dT)_{21} \\|_F$.\n3. The Frobenius norm of the orthogonality defect of $Q'$ defined as $\\| Q'^{\\top} Q' - I \\|_F$.\n4. The same orthogonality defect after re-orthonormalization, defined as $\\| \\widehat{Q}^{\\top} \\widehat{Q} - I \\|_F$, where $\\widehat{Q}$ is the re-orthonormalized version of $Q'$.\n5. A boolean indicating whether the orthogonality control reduced the defect, i.e., whether the post-control defect is strictly smaller than the pre-control defect.\n\nDesign a test suite of four cases with reproducible data generated from fixed seeds, covering a well-separated eigenvalue cluster, a near-resonant cluster, a minimal subspace dimension, and a symmetric case:\n- Case 1 (well-separated cluster): $n = 6$, $k = 3$. Set $T_{11}$ with diagonal entries $[1.0, 1.5, 2.0]$, $T_{22}$ with diagonal entries $[4.0, 5.0, 6.0]$, and $T_{12}$ with entries drawn uniformly from $[-0.1, 0.1]$ using pseudorandom generation with seed $10$. Construct $Q$ from the $\\mathrm{QR}$ factorization of a standard normal matrix with seed $11$. Set $E$ as a standard normal matrix scaled by $10^{-3}$ with seed $12$.\n- Case 2 (near-resonant cluster): $n = 6$, $k = 3$. Set $T_{11}$ with diagonal entries $[2.00, 2.10, 2.20]$, $T_{22}$ with diagonal entries $[2.05, 2.15, 2.25]$, and $T_{12}$ with entries drawn uniformly from $[-0.5, 0.5]$ using seed $20$. Construct $Q$ from the $\\mathrm{QR}$ factorization of a standard normal matrix with seed $21$. Set $E$ as a standard normal matrix scaled by $10^{-3}$ with seed $22$.\n- Case 3 (minimal subspace dimension): $n = 3$, $k = 1$. Set $T_{11}$ with diagonal entry $[1.0]$, $T_{22}$ with diagonal entries $[3.0, 4.0]$, and $T_{12} = 0$. Use $Q = I$. Set $E$ as a standard normal matrix scaled by $10^{-2}$ with seed $30$.\n- Case 4 (symmetric case): $n = 5$, $k = 2$. Construct $A$ as $A = Q D Q^{\\top}$ with $D = \\mathrm{diag}(1.0, 2.0, 3.0, 4.0, 5.0)$ and $Q$ from the $\\mathrm{QR}$ factorization of a standard normal matrix with seed $41$, so that $T = D$ is diagonal. Set $E$ as a symmetric matrix obtained by symmetrizing a standard normal matrix with seed $42$ and scaling by $10^{-3}$.\n\nUse $\\varepsilon = 10^{-3}$ for the $Q'$ update in all cases. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case contributes a list of five entries in the order specified above. For example, the output format must be\n$$\n[ [r_1, d_1, o^{\\mathrm{pre}}_1, o^{\\mathrm{post}}_1, b_1], [r_2, d_2, o^{\\mathrm{pre}}_2, o^{\\mathrm{post}}_2, b_2], [r_3, d_3, o^{\\mathrm{pre}}_3, o^{\\mathrm{post}}_3, b_3], [r_4, d_4, o^{\\mathrm{pre}}_4, o^{\\mathrm{post}}_4, b_4] ].\n$$\nAll numerical quantities are to be reported as floating-point numbers without any physical units. Angles do not appear in this task. The booleans must be printed as either True or False. The program must be self-contained, require no input, and must adhere to the specified execution environment.", "solution": "The problem requires the derivation of a Sylvester equation that governs the first-order perturbation of an invariant subspace of a matrix $A$ under a smooth perturbation $A(t) = A+tE$. This derivation is followed by a numerical implementation to solve this equation and verify the related theoretical results. The derivation begins from first principles, considering the defining properties of the real Schur decomposition.\n\nLet $A(t) = A + tE$ be a smooth matrix-valued function, where $A, E \\in \\mathbb{R}^{n \\times n}$ and $t \\in \\mathbb{R}$ is a small scalar. The real Schur decomposition of $A(t)$ is given by $A(t) = Q(t) T(t) Q(t)^{\\top}$, where $Q(t)$ is an orthogonal matrix and $T(t)$ is an upper quasi-triangular matrix. At $t=0$, we have $A(0)=A$, $Q(0)=Q$, and $T(0)=T$.\n\nOur derivation starts from two fundamental identities that hold for all $t$ in a neighborhood of $0$:\n1. The orthogonality condition: $Q(t)^{\\top} Q(t) = I$, where $I$ is the identity matrix.\n2. The Schur decomposition identity: $A(t) = Q(t) T(t) Q(t)^{\\top}$.\n\nWe differentiate these identities with respect to $t$ and evaluate the result at $t=0$. Let $\\dot{Q} = \\frac{d Q(t)}{dt}|_{t=0}$ and $\\dot{T} = \\frac{d T(t)}{dt}|_{t=0}$.\n\nFirst, differentiating the orthogonality condition using the product rule gives:\n$$\n\\frac{d}{dt} (Q(t)^{\\top} Q(t)) = \\dot{Q}(t)^{\\top} Q(t) + Q(t)^{\\top} \\dot{Q}(t) = \\frac{d}{dt}(I) = 0\n$$\nEvaluating at $t=0$:\n$$\n\\dot{Q}^{\\top} Q + Q^{\\top} \\dot{Q} = 0\n$$\nLet us define the generator of the rotation $\\Omega \\in \\mathbb{R}^{n \\times n}$ as $\\Omega = Q^{\\top} \\dot{Q}$. Substituting this into the equation, we get $(\\dot{Q}^{\\top} Q) + \\Omega = (Q^{\\top} \\dot{Q})^{\\top} + \\Omega = \\Omega^{\\top} + \\Omega = 0$. This proves that $\\Omega$ is a skew-symmetric matrix, i.e., $\\Omega^{\\top} = -\\Omega$. From $\\Omega = Q^{\\top} \\dot{Q}$, we can express the derivative of the orthogonal factor as $\\dot{Q} = Q\\Omega$.\n\nSecond, we differentiate the Schur decomposition identity $A(t) = Q(t) T(t) Q(t)^{\\top}$. At $t=0$, the derivative of the left-hand side is $\\dot{A}(0) = E$. Applying the product rule to the right-hand side gives:\n$$\nE = \\dot{Q} T Q^{\\top} + Q \\dot{T} Q^{\\top} + Q T \\dot{Q}^{\\top}\n$$\nWe can substitute the expressions for the derivatives of the orthogonal factors, $\\dot{Q} = Q\\Omega$ and $\\dot{Q}^{\\top} = \\Omega^{\\top}Q^{\\top} = -\\Omega Q^{\\top}$:\n$$\nE = (Q\\Omega) T Q^{\\top} + Q \\dot{T} Q^{\\top} + Q T (-\\Omega Q^{\\top})\n$$\nWe can factor out $Q$ on the left and $Q^{\\top}$ on the right:\n$$\nE = Q ( \\Omega T + \\dot{T} - T \\Omega ) Q^{\\top}\n$$\nPre-multiplying by $Q^{\\top}$ and post-multiplying by $Q$ yields:\n$$\nQ^{\\top} E Q = \\Omega T - T \\Omega + \\dot{T}\n$$\nLet $F = Q^{\\top} E Q$. The equation can be written in terms of the matrix commutator $[\\Omega, T] = \\Omega T - T \\Omega$:\n$$\n\\dot{T} = F - [\\Omega, T]\n$$\nNow, we partition the matrices $T$, $F$, $\\Omega$, and $\\dot{T}$ conformably with the invariant subspace of dimension $k$:\n$$\nT = \\begin{bmatrix} T_{11} & T_{12} \\\\ 0 & T_{22} \\end{bmatrix}, \\quad F = \\begin{bmatrix} F_{11} & F_{12} \\\\ F_{21} & F_{22} \\end{bmatrix}, \\quad \\Omega = \\begin{bmatrix} \\Omega_{11} & \\Omega_{12} \\\\ \\Omega_{21} & \\Omega_{22} \\end{bmatrix}, \\quad \\dot{T} = \\begin{bmatrix} \\dot{T}_{11} & \\dot{T}_{12} \\\\ \\dot{T}_{21} & \\dot{T}_{22} \\end{bmatrix}\n$$\nwhere $T_{11} \\in \\mathbb{R}^{k \\times k}$, $F_{11} \\in \\mathbb{R}^{k \\times k}$, etc. As $\\Omega$ is skew-symmetric, $\\Omega_{11}$ and $\\Omega_{22}$ must be skew-symmetric, and $\\Omega_{12} = -\\Omega_{21}^{\\top}$.\n\nThe partitioned form of the commutator is:\n$$\n[\\Omega, T] = \\begin{bmatrix} [\\Omega_{11}, T_{11}] - T_{12}\\Omega_{21} & \\Omega_{11}T_{12} + \\Omega_{12}T_{22} - T_{11}\\Omega_{12} - T_{12}\\Omega_{22} \\\\ \\Omega_{21}T_{11} - T_{22}\\Omega_{21} & [\\Omega_{22}, T_{22}] + \\Omega_{21}T_{12} \\end{bmatrix}\n$$\nSubstituting this into $\\dot{T} = F - [\\Omega, T]$ and examining the $(2,1)$ block gives:\n$$\n\\dot{T}_{21} = F_{21} - (\\Omega_{21}T_{11} - T_{22}\\Omega_{21}) = F_{21} + T_{22}\\Omega_{21} - \\Omega_{21}T_{11}\n$$\nThe problem states that we must enforce that the block upper-triangular structure of $T(t)$ is preserved for small $t$. This means that the $(2,1)$ block of its derivative $\\dot{T}$ must be zero: $\\dot{T}_{21}=0$. This condition yields:\n$$\n0 = F_{21} + T_{22}\\Omega_{21} - \\Omega_{21}T_{11}\n$$\nRearranging this gives the Sylvester equation for the off-diagonal block $\\Omega_{21}$:\n$$\nT_{22} \\Omega_{21} - \\Omega_{21} T_{11} = -F_{21}\n$$\nThis fundamental equation determines the rotation of the invariant subspace under the perturbation $E$. The existence and uniqueness of the solution $\\Omega_{21}$ depend on the spectra of $T_{11}$ and $T_{22}$ being disjoint.\n\nThe subsequent implementation will compute the quantities specified in the problem. It will solve this Sylvester equation for $\\Omega_{21}$. To uniquely determine $\\Omega$, we impose the gauge condition that its diagonal blocks are zero, i.e., $\\Omega_{11}=0$ and $\\Omega_{22}=0$. This yields $\\Omega = \\begin{bsmallmatrix} 0 & -\\Omega_{21}^{\\top} \\\\ \\Omega_{21} & 0 \\end{bsmallmatrix}$. With $\\Omega$ fully determined, $\\dot{T}$ is also determined. We will then compute a first-order update to $Q$ as $Q' = Q + \\varepsilon\\dot{Q} = Q(I + \\varepsilon\\Omega)$ and assess its deviation from orthogonality. Finally, we will apply a re-orthonormalization procedure to $Q'$ using the polar decomposition, which finds the closest orthogonal matrix in the Frobenius norm, and quantify the improvement in orthogonality. The requested diagnostics will validate each step of this process.", "answer": "```python\nimport numpy as np\nfrom scipy.linalg import solve_sylvester, polar, qr\n\ndef solve():\n    \"\"\"\n    Solves the numerical linear algebra problem for four test cases.\n    \"\"\"\n\n    def run_case(n, k, T, Q, E, epsilon):\n        \"\"\"\n        Executes the logic for a single test case.\n        \"\"\"\n        # Step 1: Compute F and partition it\n        F = Q.T @ E @ Q\n        F11 = F[:k, :k]\n        F12 = F[:k, k:]\n        F21 = F[k:, :k]\n        F22 = F[k:, k:]\n        \n        T11 = T[:k, :k]\n        T22 = T[k:, k:]\n\n        # Step 2: Solve the Sylvester equation for Omega_21\n        # T22 * Omega_21 - Omega_21 * T11 = -F21\n        # Scipy solves AX + XB = Q, so A=T22, X=Omega_21, B=-T11, Q=-F21\n        Omega_21 = solve_sylvester(T22, -T11, -F21)\n        \n        # Diagnostic 1: Sylvester residual norm\n        sylvester_residual = np.linalg.norm(T22 @ Omega_21 - Omega_21 @ T11 + F21, 'fro')\n\n        # Step 3: Construct Omega and compute dT\n        Omega_11 = np.zeros((k, k))\n        Omega_22 = np.zeros((n - k, n - k))\n        Omega_12 = -Omega_21.T\n        Omega = np.block([\n            [Omega_11, Omega_12],\n            [Omega_21, Omega_22]\n        ])\n        \n        # dT = F - [Omega, T] = F - (Omega @ T - T @ Omega)\n        dT = F - (Omega @ T - T @ Omega)\n        \n        # Diagnostic 2: Norm of the (2,1) block of dT\n        dT_21_norm = np.linalg.norm(dT[k:, :k], 'fro')\n        \n        # Step 4: Form updated Q' and perform orthogonality control\n        I_n = np.identity(n)\n        # Q' = Q + eps * dQ = Q + eps * Q @ Omega = Q @ (I + eps * Omega)\n        Q_prime = Q @ (I_n + epsilon * Omega)\n        \n        # Diagnostic 3: Pre-control orthogonality defect\n        pre_ortho_defect = np.linalg.norm(Q_prime.T @ Q_prime - I_n, 'fro')\n\n        # Re-orthonormalize Q' using polar decomposition to find the closest orthogonal matrix.\n        Q_hat, _ = polar(Q_prime)\n        \n        # Diagnostic 4: Post-control orthogonality defect\n        post_ortho_defect = np.linalg.norm(Q_hat.T @ Q_hat - I_n, 'fro')\n        \n        # Diagnostic 5: Boolean for defect reduction\n        defect_reduced = post_ortho_defect  pre_ortho_defect\n        \n        return [sylvester_residual, dT_21_norm, pre_ortho_defect, post_ortho_defect, defect_reduced]\n\n    test_cases = []\n    epsilon = 1e-3\n\n    # Case 1: Well-separated cluster\n    n, k = 6, 3\n    rng_T12 = np.random.default_rng(10)\n    rng_Q = np.random.default_rng(11)\n    rng_E = np.random.default_rng(12)\n    \n    T11 = np.diag([1.0, 1.5, 2.0])\n    T22 = np.diag([4.0, 5.0, 6.0])\n    T12 = rng_T12.uniform(-0.1, 0.1, size=(k, n - k))\n    T = np.block([[T11, T12], [np.zeros((n - k, k)), T22]])\n    Q, _ = qr(rng_Q.standard_normal(size=(n, n)))\n    E = rng_E.standard_normal(size=(n, n)) * 1e-3\n    test_cases.append(('Case 1', n, k, T, Q, E, epsilon))\n\n    # Case 2: Near-resonant cluster\n    n, k = 6, 3\n    rng_T12 = np.random.default_rng(20)\n    rng_Q = np.random.default_rng(21)\n    rng_E = np.random.default_rng(22)\n    \n    T11 = np.diag([2.00, 2.10, 2.20])\n    T22 = np.diag([2.05, 2.15, 2.25])\n    T12 = rng_T12.uniform(-0.5, 0.5, size=(k, n - k))\n    T = np.block([[T11, T12], [np.zeros((n - k, k)), T22]])\n    Q, _ = qr(rng_Q.standard_normal(size=(n, n)))\n    E = rng_E.standard_normal(size=(n, n)) * 1e-3\n    test_cases.append(('Case 2', n, k, T, Q, E, epsilon))\n    \n    # Case 3: Minimal subspace dimension\n    n, k = 3, 1\n    rng_E = np.random.default_rng(30)\n    \n    T11 = np.array([[1.0]])\n    T22 = np.diag([3.0, 4.0])\n    T12 = np.zeros((k, n-k))\n    T = np.block([[T11, T12], [np.zeros((n - k, k)), T22]])\n    Q = np.identity(n)\n    E = rng_E.standard_normal(size=(n, n)) * 1e-2\n    test_cases.append(('Case 3', n, k, T, Q, E, epsilon))\n    \n    # Case 4: Symmetric case\n    n, k = 5, 2\n    rng_Q = np.random.default_rng(41)\n    rng_E = np.random.default_rng(42)\n    \n    T = np.diag([1.0, 2.0, 3.0, 4.0, 5.0])\n    Q, _ = qr(rng_Q.standard_normal(size=(n, n)))\n    E_rand = rng_E.standard_normal(size=(n, n))\n    E = (E_rand + E_rand.T) / 2 * 1e-3\n    test_cases.append(('Case 4', n, k, T, Q, E, epsilon))\n\n    results = []\n    for _, n_case, k_case, T_case, Q_case, E_case, eps_case in test_cases:\n        result = run_case(n_case, k_case, T_case, Q_case, E_case, eps_case)\n        results.append(result)\n\n    # Format the final output string\n    # E.g., [[1.23, ...], [4.56, ...]]\n    output_str = \"[\" + \", \".join([str(res) for res in results]) + \"]\"\n    print(output_str)\n\nsolve()\n```", "id": "3578469"}, {"introduction": "After learning how to solve a Sylvester equation, a crucial next step is to understand when the solution is numerically reliable. This hands-on coding practice explores the conditioning of the Sylvester operator, linking it to the spectral separation between the involved matrices. Through numerical experiments, you will contrast theoretical measures of sensitivity with empirical estimates and discover the critical role of pseudospectra in predicting behavior for non-normal matrices [@problem_id:3578480].", "problem": "Construct a complete, runnable program that investigates the sensitivity of the Sylvester equation $AX+XB=C$ to perturbations when the spectra of $A$ and $-B$ nearly collide. Your tasks must begin from fundamental, widely accepted definitions in numerical linear algebra and proceed by first-principles reasoning. In particular, start from the linear operator perspective for the Sylvester map and from the definition of the $\\varepsilon$-pseudospectrum, and do not assume any special-case formulae beyond these foundational facts. The central goal is to quantify how the spectral separation $\\mathrm{sep}(A,-B)$ governs the conditioning of the inverse Sylvester map via pseudospectral considerations.\n\nYou must carry out the following, using only the base facts that (i) the Sylvester map $L:\\mathbb{C}^{n\\times m}\\to\\mathbb{C}^{n\\times m}$ defined by $L[X]=AX+XB$ is linear, (ii) the vectorization $\\operatorname{vec}(\\cdot)$ is a linear isometry between $\\left(\\mathbb{C}^{n\\times m},\\|\\cdot\\|_F\\right)$ and $\\left(\\mathbb{C}^{nm},\\|\\cdot\\|_2\\right)$, and (iii) the $\\varepsilon$-pseudospectrum of a square matrix $M$ is $\\Lambda_{\\varepsilon}(M)=\\{z\\in\\mathbb{C}:\\sigma_{\\min}(zI-M)\\le \\varepsilon\\}$, where $\\sigma_{\\min}(\\cdot)$ denotes the smallest singular value.\n\n1) Derive, from the stated base facts and without presupposing any specialized formula, an exact and implementable computation of the spectral separation quantity $\\mathrm{sep}(A,-B)$ as the smallest singular value of a concrete, finite-dimensional matrix representation of the Sylvester operator $L$. Then implement this computation to obtain $\\mathrm{sep}(A,-B)$ numerically.\n\n2) Using only the linearity of $L$ and norm definitions, devise and implement a Monte Carlo procedure to estimate the operator norm of $L^{-1}$ induced by the Frobenius norm, namely $\\|L^{-1}\\|=\\sup_{\\|E\\|_F=1}\\|L^{-1}[E]\\|_F$, by repeatedly solving $AX+XB=E$ for randomly drawn right-hand sides $E$ with $\\|E\\|_F=1$ and taking the maximum observed $\\|X\\|_F$. Ensure numerical reproducibility by explicitly setting pseudorandom seeds where needed.\n\n3) Starting from the definition of the $\\varepsilon$-pseudospectrum, construct and implement a pseudospectral proxy for the separation between $A$ and $-B$ as follows. For a bounded rectangular search region in the complex plane and a uniform grid of candidate points $z$, compute $s_A(z)=\\sigma_{\\min}(zI-A)$ and $s_{-B}(z)=\\sigma_{\\min}(zI+ B)$, and then compute\n$$\n\\mu=\\inf_{z\\ \\text{in the grid}} \\max\\{s_A(z), s_{-B}(z)\\}.\n$$\nUse this $\\mu$ as a pseudospectral separation surrogate and report the reciprocal $1/\\mu$ as a heuristic upper predictor for $\\|L^{-1}\\|$. The design must clearly explain how the grid is chosen to capture the relevant interaction between the spectra of $A$ and $-B$.\n\n4) For a test suite of three cases listed below, each with dimensions $n=m=3$, report, for each case, the triple of real numbers\n$$\n\\big(\\ \\mathrm{sep}(A,-B),\\ \\widehat{\\|L^{-1}\\|},\\ 1/\\mu\\ \\big),\n$$\nwhere $\\widehat{\\|L^{-1}\\|}$ is your Monte Carlo estimate and $1/\\mu$ is the pseudospectral proxy. All norms must be Frobenius norms with the standard Euclidean-induced singular value computations.\n\nTest suite specifications:\n\n- Case 1 (normal, moderately separated): Let $A=\\operatorname{diag}(1,2,3)$ and $B=\\operatorname{diag}(-1-\\delta,-2-\\delta,-3-\\delta)$ with $\\delta=10^{-1}$. Use a search grid formed by taking the real part from the minimum to maximum of $\\{\\operatorname{Re}\\lambda:\\lambda\\in\\Lambda(A)\\cup\\Lambda(-B)\\}$, padded symmetrically by a margin of $0.5$, and the imaginary part from $-0.5$ to $0.5$, both sampled on a $101\\times 101$ uniform lattice.\n\n- Case 2 (normal, nearly colliding): Let $A=\\operatorname{diag}(1,2,3)$ and $B=\\operatorname{diag}(-1-\\delta,-2-\\delta,-3-\\delta)$ with $\\delta=10^{-4}$, and use the same grid construction and resolution as in Case $1$.\n\n- Case 3 (highly nonnormal $A$): Let $A$ be the upper-triangular matrix with ones on the diagonal and superdiagonal entries equal to $\\alpha=20$, i.e.,\n$$\nA=\\begin{bmatrix}\n1  \\alpha  0\\\\\n0  1  \\alpha\\\\\n0  0  1\n\\end{bmatrix},\n$$\nand let $B=-\\left(1+\\delta\\right)I$ with $\\delta=10^{-3}$. Use the same grid construction and resolution as in Case $1$.\n\nFor the Monte Carlo estimation in each case, draw $M=200$ independent matrices $E$ with entries sampled from the standard normal distribution and rescaled to satisfy $\\|E\\|_F=1$, using the pseudorandom seed $1$. Solve $AX+XB=E$ to obtain $X$ and record the maximum observed $\\|X\\|_F$.\n\nFinal output format:\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Concatenate the triples from the three cases in order, so the output has the form\n$$\n[\\ \\mathrm{sep}_1,\\ \\widehat{\\|L^{-1}\\|}_1,\\ (1/\\mu)_1,\\ \\mathrm{sep}_2,\\ \\widehat{\\|L^{-1}\\|}_2,\\ (1/\\mu)_2,\\ \\mathrm{sep}_3,\\ \\widehat{\\|L^{-1}\\|}_3,\\ (1/\\mu)_3\\ ].\n$$\nAll quantities must be real numbers in standard floating-point decimal form with no units. No physical units are involved in this problem, and no angles are required. The program must be self-contained, require no user input, and use only the specified numerical libraries.\n\nDesign constraints and coverage:\n\n- The three cases collectively cover a general well-separated normal situation, a near-collision situation, and a strongly nonnormal situation to probe pseudospectral effects.\n\n- Your derivations must rely only on the definitions of the Sylvester operator, vectorization mapping, singular values, and the $\\varepsilon$-pseudospectrum; from these, derive the finite-dimensional singular value problem for $\\mathrm{sep}(A,-B)$ and justify the Monte Carlo estimation strategy for $\\|L^{-1}\\|$.\n\n- The program must compute all requested values and print the final result line in the exact required format.", "solution": "The analysis of the Sylvester equation $AX+XB=C$, where $A \\in \\mathbb{C}^{n\\times n}$, $B \\in \\mathbb{C}^{m\\times m}$, $C, X \\in \\mathbb{C}^{n\\times m}$, begins by re-casting it in terms of a linear operator. We define the Sylvester operator $L: \\mathbb{C}^{n\\times m} \\to \\mathbb{C}^{n\\times m}$ by its action on a matrix $X$ as $L[X] = AX+XB$. The problem is thus equivalent to solving the linear system $L[X]=C$. The sensitivity of the solution $X$ to perturbations in $C$ is governed by the norm of the inverse operator, $\\|L^{-1}\\|$. A large norm indicates that small changes in $C$ can lead to large changes in $X$, a hallmark of an ill-conditioned problem. This solution will derive and compute three quantities that characterize this conditioning: the spectral separation $\\mathrm{sep}(A,-B)$, a Monte Carlo estimate of $\\|L^{-1}\\|$, and a pseudospectral proxy for $\\|L^{-1}\\|$.\n\n**1. Derivation and Computation of $\\mathrm{sep}(A,-B)$**\n\nThe problem requires a derivation of the spectral separation $\\mathrm{sep}(A,-B)$ from first principles. The separation between two matrices $M_1$ and $M_2$ is formally defined as $\\mathrm{sep}(M_1, M_2) = \\inf_{\\|X\\|_F=1} \\|M_1 X - X M_2\\|_F$. For our Sylvester equation, the relevant quantity is $\\mathrm{sep}(A,-B) = \\inf_{\\|X\\|_F=1} \\|A X - X(-B)\\|_F = \\inf_{\\|X\\|_F=1} \\|AX+XB\\|_F$. This is exactly the infimum of the norm of the image of the unit sphere under the operator $L$, which corresponds to the smallest singular value of the operator $L$.\n\nTo compute this quantity, we must find a finite-dimensional matrix representation of $L$. This is achieved using the vectorization operator, $\\operatorname{vec}(\\cdot)$, which stacks the columns of a matrix into a single column vector. We are given that $\\operatorname{vec}$ is a linear isometry from the space of matrices equipped with the Frobenius norm, $(\\mathbb{C}^{n\\times m}, \\|\\cdot\\|_F)$, to the space of vectors with the Euclidean $2$-norm, $(\\mathbb{C}^{nm}, \\|\\cdot\\|_2)$. This means $\\|X\\|_F = \\|\\operatorname{vec}(X)\\|_2$.\n\nApplying the $\\operatorname{vec}$ operator to the Sylvester equation $AX+XB=C$ yields:\n$$\n\\operatorname{vec}(AX+XB) = \\operatorname{vec}(C)\n$$\nUsing the linearity of $\\operatorname{vec}$ and a fundamental property of the Kronecker product ($\\otimes$), we can write:\n$$\n\\operatorname{vec}(AXB) = (B^{\\top} \\otimes A)\\operatorname{vec}(X)\n$$\nApplying this property to each term in the Sylvester expression:\n$$\n\\operatorname{vec}(AX) = \\operatorname{vec}(AXI_m) = (I_m^{\\top} \\otimes A)\\operatorname{vec}(X) = (I_m \\otimes A)\\operatorname{vec}(X)\n$$\n$$\n\\operatorname{vec}(XB) = \\operatorname{vec}(I_nXB) = (B^{\\top} \\otimes I_n)\\operatorname{vec}(X)\n$$\nHere, $I_k$ is the $k \\times k$ identity matrix. For this problem, $n=m=3$, so the identity matrices are of size $3 \\times 3$.\nCombining these, the vectorized Sylvester equation becomes a standard matrix-vector equation:\n$$\n\\big( (I_m \\otimes A) + (B^{\\top} \\otimes I_n) \\big) \\operatorname{vec}(X) = \\operatorname{vec}(C)\n$$\nLet the matrix representation of the Sylvester operator be $K = (I_m \\otimes A) + (B^{\\top} \\otimes I_n)$. Here, $K$ is a matrix in $\\mathbb{C}^{nm \\times nm}$. The equation is now $K\\mathbf{x} = \\mathbf{c}$, where $\\mathbf{x}=\\operatorname{vec}(X)$ and $\\mathbf{c}=\\operatorname{vec}(C)$.\n\nThe quantity $\\mathrm{sep}(A,-B)$ can now be expressed using this matrix representation.\n$$\n\\mathrm{sep}(A,-B) = \\inf_{\\|X\\|_F=1} \\|L[X]\\|_F = \\inf_{\\|\\operatorname{vec}(X)\\|_2=1} \\|K \\operatorname{vec}(X)\\|_2\n$$\nThe right-hand side is precisely the definition of the smallest singular value of the matrix $K$, denoted $\\sigma_{\\min}(K)$. Therefore, we have derived the exact computational formula:\n$$\n\\mathrm{sep}(A,-B) = \\sigma_{\\min}(I_m \\otimes A + B^{\\top} \\otimes I_n)\n$$\nFor each test case, we will construct the $9 \\times 9$ matrix $K$ and compute its smallest singular value. This value is also theoretically equal to $1/\\|L^{-1}\\|$.\n\n**2. Monte Carlo Estimation of $\\|L^{-1}\\|**\n\nThe norm of the inverse Sylvester operator, induced by the Frobenius norm, is defined as:\n$$\n\\|L^{-1}\\| = \\sup_{C \\neq 0} \\frac{\\|L^{-1}[C]\\|_F}{\\|C\\|_F} = \\sup_{\\|C\\|_F=1} \\|L^{-1}[C]\\|_F\n$$\nLetting $X = L^{-1}[C]$, this is equivalent to finding $\\sup_{\\|L[X]\\|_F=1} \\|X\\|_F$. The Monte Carlo procedure provides a numerical estimate, $\\widehat{\\|L^{-1}\\|}$, for this quantity by sampling. The method involves generating a set of random matrices $E$ on the right-hand side, normalizing them to have unit Frobenius norm, solving the Sylvester equation $AX+XB=E$ for each $E$, and finding the largest resulting norm $\\|X\\|_F$.\n\nThe procedure is as follows:\n1. Initialize a pseudorandom number generator with a fixed seed ($1$) for reproducibility.\n2. Initialize a variable `max_norm` to $0$.\n3. For $M=200$ iterations:\n    a. Generate an $n \\times m$ matrix $E_{raw}$ whose entries are independent samples from the standard normal distribution $\\mathcal{N}(0,1)$.\n    b. Normalize this matrix: $E = E_{raw} / \\|E_{raw}\\|_F$. Now, $\\|E\\|_F=1$.\n    c. Solve the Sylvester equation $AX+XB=E$ for the matrix $X$. This can be done efficiently using specialized solvers such as `scipy.linalg.solve_sylvester`.\n    d. Compute the Frobenius norm of the solution, $\\|X\\|_F$.\n    e. Update `max_norm = max(max_norm, \\|X\\|_F)`.\n4. The final value, `max_norm`, is the estimate $\\widehat{\\|L^{-1}\\|}$. This estimate approaches the true norm as the number of samples $M$ increases, as it becomes more likely that one of the random right-hand sides $E$ aligns with the direction of maximum amplification by $L^{-1}$.\n\n**3. Pseudospectral Proxy for Sensitivity**\n\nThe pseudospectrum of a matrix $M$ provides information about its sensitivity to perturbations. The $\\varepsilon$-pseudospectrum, $\\Lambda_{\\varepsilon}(M)$, is the set of complex numbers $z$ that are eigenvalues of a perturbed matrix $M+E$ with $\\|E\\| \\le \\varepsilon$. An equivalent definition, given in the problem, is $\\Lambda_{\\varepsilon}(M) = \\{ z \\in \\mathbb{C} : \\sigma_{\\min}(zI-M) \\le \\varepsilon \\}$. A small value of $\\sigma_{\\min}(zI-M)$ indicates that $z$ is \"close\" to being an eigenvalue of $M$.\n\nThe sensitivity of the Sylvester equation is high when the spectra of $A$ and $-B$ are close. Pseudospectra generalize this notion: sensitivity is high when the pseudospectra of $A$ and $-B$ are close or overlap. We can quantify this proximity by finding the smallest \"height\" $\\varepsilon$ at which the level sets of the functions $z \\mapsto \\sigma_{\\min}(zI-A)$ and $z \\mapsto \\sigma_{\\min}(zI+B)$ intersect.\n\nThe proxy $\\mu$ is defined as the minimum value of the maximum of these two functions over a search grid:\n$$\n\\mu = \\inf_{z\\ \\text{in grid}} \\max \\{ \\sigma_{\\min}(zI-A), \\sigma_{\\min}(zI+B) \\}\n$$\nA small value of $\\mu$ implies that there exists a point $z$ in the complex plane where both $\\sigma_{\\min}(zI-A)$ and $\\sigma_{\\min}(zI+B)$ are small. This point $z$ is in both $\\Lambda_{\\mu}(A)$ and $\\Lambda_{\\mu}(-B)$, indicating an overlap of the $\\mu$-pseudospectra of $A$ and $-B$. Such an overlap is a strong indicator of ill-conditioning. Consequently, the reciprocal $1/\\mu$ is expected to be a heuristic predictor, typically an upper bound, for $\\|L^{-1}\\|$.\n\nThe grid is constructed to cover the region where the spectra of $A$ and $-B$ interact. For each case, we compute the eigenvalues of $A$ and $-B$, find the extremal real parts, and define a rectangular grid around them, padded by $0.5$. The imaginary axis range is set to $[-0.5, 0.5]$. This is a reasonable choice for the given test cases, as the eigenvalues are all real or close to the real axis. We then iterate over this $101 \\times 101$ grid of complex numbers $z$, compute $\\sigma_{\\min}(zI-A)$ and $\\sigma_{\\min}(zI+B)$ at each point, and find the minimum of their maximum, yielding $\\mu$.\n\n**4. Numerical Implementation and Results**\n\nThe three methodologies are implemented for each of the three test cases.\n- **Case 1:** $A$ and $B$ are normal (diagonal) matrices. The spectra of $A$ and $-B$ are $\\Lambda(A)=\\{1,2,3\\}$ and $\\Lambda(-B)=\\{1.1, 2.1, 3.1\\}$, respectively. The minimal spectral distance is $\\delta=0.1$. As the matrices are normal, $\\|L^{-1}\\| = 1/\\mathrm{sep}(A,-B)$, and we expect the three computed quantities to be close.\n- **Case 2:** A and B are normal, but the spectra are nearly colliding, with a minimal distance of $\\delta=10^{-4}$. We expect all three quantities to be large, on the order of $1/\\delta = 10000$.\n- **Case 3:** $A$ is a highly non-normal upper-triangular matrix, while $B$ is a scalar multiple of the identity. The eigenvalues of $A$ are all $1$, while the eigenvalues of $-B$ are all $1+\\delta = 1.001$. The spectral separation is small, $\\delta=10^{-3}$. However, due to the high non-normality of $A$, its pseudospectra are much larger than the spectral distance $\\delta$ would suggest. We thus anticipate that $\\widehat{\\|L^{-1}\\|}$ and $1/\\mu$ will be significantly larger than $1/\\mathrm{sep}(A,-B)$, demonstrating the failure of simple spectral separation to predict conditioning for non-normal matrices.\n\nThe following program implements these computations and reports the nine specified values in the required format.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import solve_sylvester\n\ndef solve():\n    \"\"\"\n    Performs the required derivations and computations for the Sylvester equation\n    sensitivity analysis across three test cases.\n    \"\"\"\n    \n    # Test suite specifications\n    n = 3\n    m = 3\n    alpha = 20.0\n    \n    test_cases = [\n        {\n            \"name\": \"Case 1: Normal, moderately separated\",\n            \"A\": np.diag([1.0, 2.0, 3.0]),\n            \"B\": np.diag([-1.0 - 1e-1, -2.0 - 1e-1, -3.0 - 1e-1]),\n        },\n        {\n            \"name\": \"Case 2: Normal, nearly colliding\",\n            \"A\": np.diag([1.0, 2.0, 3.0]),\n            \"B\": np.diag([-1.0 - 1e-4, -2.0 - 1e-4, -3.0 - 1e-4]),\n        },\n        {\n            \"name\": \"Case 3: Highly nonnormal A\",\n            \"A\": np.array([[1.0, alpha, 0.0], [0.0, 1.0, alpha], [0.0, 0.0, 1.0]]),\n            \"B\": -(1.0 + 1e-3) * np.eye(m),\n        },\n    ]\n\n    results = []\n\n    for case in test_cases:\n        A = case[\"A\"]\n        B = case[\"B\"]\n\n        # 1. Compute sep(A, -B)\n        # K = I_m kron A + B^T kron I_n\n        K = np.kron(np.eye(m), A) + np.kron(B.T, np.eye(n))\n        # sep(A, -B) = sigma_min(K)\n        # Use svdvals to avoid computing U and Vh, which is more efficient.\n        singular_values_K = np.linalg.svd(K, compute_uv=False)\n        sep_A_minus_B = np.min(singular_values_K)\n        results.append(sep_A_minus_B)\n\n        # 2. Monte Carlo estimation of ||L^-1||\n        M = 200\n        rng = np.random.default_rng(seed=1)\n        max_norm_X = 0.0\n        for _ in range(M):\n            E_raw = rng.standard_normal(size=(n, m))\n            E = E_raw / np.linalg.norm(E_raw, 'fro')\n            \n            # Solve AX + XB = E\n            X = solve_sylvester(A, B, E)\n            \n            norm_X = np.linalg.norm(X, 'fro')\n            if norm_X > max_norm_X:\n                max_norm_X = norm_X\n        \n        estimated_norm_L_inv = max_norm_X\n        results.append(estimated_norm_L_inv)\n\n        # 3. Compute pseudospectral proxy 1/mu\n        # Define the grid\n        eigs_A = np.linalg.eigvals(A)\n        eigs_minus_B = np.linalg.eigvals(-B)\n        all_eigs = np.concatenate((eigs_A, eigs_minus_B))\n        \n        re_min = np.min(np.real(all_eigs))\n        re_max = np.max(np.real(all_eigs))\n        \n        grid_re = np.linspace(re_min - 0.5, re_max + 0.5, 101)\n        grid_im = np.linspace(-0.5, 0.5, 101)\n        \n        mu = np.inf\n        I_n = np.eye(n)\n\n        for re_z in grid_re:\n            for im_z in grid_im:\n                z = re_z + 1j * im_z\n                \n                # s_A(z) = sigma_min(zI - A)\n                s_A_z = np.min(np.linalg.svd(z * I_n - A, compute_uv=False))\n                \n                # s_-B(z) = sigma_min(zI + B)\n                s_minusB_z = np.min(np.linalg.svd(z * I_n + B, compute_uv=False))\n                \n                max_s = max(s_A_z, s_minusB_z)\n                \n                if max_s  mu:\n                    mu = max_s\n        \n        proxy_1_over_mu = 1.0 / mu\n        results.append(proxy_1_over_mu)\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3578480"}]}