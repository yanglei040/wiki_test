{"hands_on_practices": [{"introduction": "To build a strong foundation in solving nonlinear eigenvalue problems, it's essential to understand the mechanics of the iterative algorithms used. This first practice provides a concrete, step-by-step application of one of the fundamental techniques: Residual Inverse Iteration (RII). By performing a single iteration for an eigenpair $(\\lambda, x)$ on a small, analytic NEP $T(\\lambda)x = 0$, you will trace the flow from computing a residual to solving a linear system and updating the approximation [@problem_id:3561644].", "problem": "Consider an analytic Nonlinear Eigenvalue Problem (NEP) defined by the matrix-valued function $T(\\lambda) \\in \\mathbb{C}^{2 \\times 2}$, where an eigenpair $(\\lambda, x)$ with $x \\neq 0$ satisfies $T(\\lambda)x = 0$. Let $T(\\lambda)$ be given by\n$$\nT(\\lambda) = A + (\\lambda + \\lambda^{2})\\,B,\n$$\nwith\n$$\nA = \\begin{pmatrix} -3 & 1 \\\\ 1 & 1 \\end{pmatrix}, \\qquad B = \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix}.\n$$\nDefine the residual for an iterate $(\\lambda_{k}, x_{k})$ as $r_{k} = T(\\lambda_{k})x_{k}$. Consider performing one step of Residual Inverse Iteration (RII) with shift $\\sigma$, where the step is defined as follows: solve the linear system\n$$\nT(\\sigma) s_{k} = -r_{k},\n$$\nform the update $\\hat{x}_{k+1} = x_{k} + s_{k}$, normalize $x_{k+1} = \\hat{x}_{k+1} / \\| \\hat{x}_{k+1} \\|_{2}$, and then choose $\\lambda_{k+1}$ as a real solution of the scalar nonlinear Rayleigh functional equation\n$$\nx_{k+1}^{*} T(\\lambda) x_{k+1} = 0,\n$$\nwith the convention that if multiple real solutions exist, the one closest to the shift $\\sigma$ is selected.\n\nUsing the initial data\n$$\nx_{0} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}, \\qquad \\lambda_{0} = 0, \\qquad \\sigma = 1,\n$$\napply one step of Residual Inverse Iteration to obtain $(x_{1}, \\lambda_{1})$. Compute the updated vector $x_{1}$ and scalar $\\lambda_{1}$ exactly. Express your final answer as a single row containing the two components of $x_{1}$ followed by $\\lambda_{1}$.", "solution": "The problem is validated as scientifically grounded, well-posed, objective, and self-contained. It is a standard exercise in numerical linear algebra concerning the application of the Residual Inverse Iteration (RII) method to a Nonlinear Eigenvalue Problem (NEP). All necessary data and definitions are provided, and no contradictions are present. The matrix $T(\\sigma)$ is invertible, ensuring the main step of the iteration is well-defined.\n\nThe problem asks to perform one step of Residual Inverse Iteration (RII) for the given Nonlinear Eigenvalue Problem $T(\\lambda)x=0$. The matrix-valued function is $T(\\lambda) = A + (\\lambda + \\lambda^{2})B$, where\n$$\nA = \\begin{pmatrix} -3 & 1 \\\\ 1 & 1 \\end{pmatrix}, \\qquad B = \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix}.\n$$\nSubstituting $A$ and $B$, we have\n$$\nT(\\lambda) = \\begin{pmatrix} -3 & 1 \\\\ 1 & 1 \\end{pmatrix} + (\\lambda + \\lambda^{2})\\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix} = \\begin{pmatrix} -3 + \\lambda + \\lambda^{2} & 1 \\\\ 1 & 1 \\end{pmatrix}.\n$$\nThe initial data for the iteration ($k=0$) is given as\n$$\nx_{0} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}, \\qquad \\lambda_{0} = 0, \\qquad \\sigma = 1.\n$$\nWe follow the steps defined for one iteration of RII to find $(\\lambda_{1}, x_{1})$.\n\n**Step 1: Compute the residual $r_{0}$.**\nThe residual $r_{k}$ is defined as $r_{k} = T(\\lambda_{k})x_{k}$. For $k=0$, we have\n$$\nr_{0} = T(\\lambda_{0})x_{0} = T(0)x_{0}.\n$$\nAt $\\lambda=0$, $T(0) = A = \\begin{pmatrix} -3 & 1 \\\\ 1 & 1 \\end{pmatrix}$.\n$$\nr_{0} = \\begin{pmatrix} -3 & 1 \\\\ 1 & 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} -3(1) + 1(1) \\\\ 1(1) + 1(1) \\end{pmatrix} = \\begin{pmatrix} -2 \\\\ 2 \\end{pmatrix}.\n$$\n\n**Step 2: Solve the linear system $T(\\sigma)s_{0} = -r_{0}$.**\nThe shift is $\\sigma=1$. We first evaluate $T(\\sigma)$.\n$$\nT(1) = \\begin{pmatrix} -3 + 1 + 1^{2} & 1 \\\\ 1 & 1 \\end{pmatrix} = \\begin{pmatrix} -1 & 1 \\\\ 1 & 1 \\end{pmatrix}.\n$$\nThe linear system to solve for the update vector $s_{0} = \\begin{pmatrix} s_{0,1} \\\\ s_{0,2} \\end{pmatrix}$ is $T(1)s_{0} = -r_{0}$.\n$$\n\\begin{pmatrix} -1 & 1 \\\\ 1 & 1 \\end{pmatrix} \\begin{pmatrix} s_{0,1} \\\\ s_{0,2} \\end{pmatrix} = -\\begin{pmatrix} -2 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ -2 \\end{pmatrix}.\n$$\nThis corresponds to the system of linear equations:\n$$\n\\begin{cases} -s_{0,1} + s_{0,2} = 2 \\\\ s_{0,1} + s_{0,2} = -2 \\end{cases}\n$$\nAdding the two equations yields $2s_{0,2} = 0$, so $s_{0,2} = 0$.\nSubstituting $s_{0,2}=0$ into the second equation gives $s_{0,1} + 0 = -2$, so $s_{0,1} = -2$.\nThus, the solution is $s_{0} = \\begin{pmatrix} -2 \\\\ 0 \\end{pmatrix}$.\n\n**Step 3: Form the unnormalized updated vector $\\hat{x}_{1}$.**\nThe updated vector is $\\hat{x}_{1} = x_{0} + s_{0}$.\n$$\n\\hat{x}_{1} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} -2 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix}.\n$$\n\n**Step 4: Normalize to obtain $x_{1}$.**\nThe new eigenvector approximation $x_{1}$ is the normalization of $\\hat{x}_{1}$.\n$$\nx_{1} = \\frac{\\hat{x}_{1}}{\\| \\hat{x}_{1} \\|_{2}} = \\frac{1}{\\sqrt{(-1)^{2} + 1^{2}}} \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} -1/\\sqrt{2} \\\\ 1/\\sqrt{2} \\end{pmatrix}.\n$$\nRationalizing the denominator, we get $x_{1} = \\begin{pmatrix} -\\frac{\\sqrt{2}}{2} \\\\ \\frac{\\sqrt{2}}{2} \\end{pmatrix}$.\n\n**Step 5: Solve the scalar nonlinear Rayleigh functional equation for $\\lambda_1$.**\nThe new eigenvalue approximation $\\lambda_{1}$ is a real solution to the equation $x_{1}^{*} T(\\lambda) x_{1} = 0$. Note that using the unnormalized vector $\\hat{x}_{1}$ simplifies the calculation, as $x_{1}^{*} T(\\lambda) x_{1} = \\frac{1}{\\| \\hat{x}_{1} \\|_{2}^{2}} \\hat{x}_{1}^{*} T(\\lambda) \\hat{x}_{1} = 0$ is equivalent to $\\hat{x}_{1}^{*} T(\\lambda) \\hat{x}_{1} = 0$.\nWe have $\\hat{x}_{1} = \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix}$, so its conjugate transpose is $\\hat{x}_{1}^{*} = \\begin{pmatrix} -1 & 1 \\end{pmatrix}$.\nThe equation becomes:\n$$\n\\begin{pmatrix} -1 & 1 \\end{pmatrix} \\begin{pmatrix} -3 + \\lambda + \\lambda^{2} & 1 \\\\ 1 & 1 \\end{pmatrix} \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix} = 0.\n$$\nFirst, we compute the product $T(\\lambda)\\hat{x}_{1}$:\n$$\nT(\\lambda)\\hat{x}_{1} = \\begin{pmatrix} -3 + \\lambda + \\lambda^{2} & 1 \\\\ 1 & 1 \\end{pmatrix} \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} -(-3 + \\lambda + \\lambda^{2}) + 1 \\\\ -1 + 1 \\end{pmatrix} = \\begin{pmatrix} 4 - \\lambda - \\lambda^{2} \\\\ 0 \\end{pmatrix}.\n$$\nNow, we pre-multiply by $\\hat{x}_{1}^{*}$:\n$$\n\\hat{x}_{1}^{*} (T(\\lambda)\\hat{x}_{1}) = \\begin{pmatrix} -1 & 1 \\end{pmatrix} \\begin{pmatrix} 4 - \\lambda - \\lambda^{2} \\\\ 0 \\end{pmatrix} = (-1)(4 - \\lambda - \\lambda^{2}) + (1)(0) = -4 + \\lambda + \\lambda^{2}.\n$$\nSetting this expression to zero gives the quadratic equation for $\\lambda$:\n$$\n\\lambda^{2} + \\lambda - 4 = 0.\n$$\nWe find the roots using the quadratic formula $\\lambda = \\frac{-b \\pm \\sqrt{b^{2}-4ac}}{2a}$:\n$$\n\\lambda = \\frac{-1 \\pm \\sqrt{1^{2} - 4(1)(-4)}}{2(1)} = \\frac{-1 \\pm \\sqrt{1 + 16}}{2} = \\frac{-1 \\pm \\sqrt{17}}{2}.\n$$\nThe two real solutions are $\\lambda_{A} = \\frac{-1 - \\sqrt{17}}{2}$ and $\\lambda_{B} = \\frac{-1 + \\sqrt{17}}{2}$.\n\n**Step 6: Select $\\lambda_{1}$ based on proximity to the shift $\\sigma$.**\nThe problem specifies that if multiple real solutions exist, we must choose the one closest to the shift $\\sigma=1$.\nWe compare the distances $|\\lambda_{A} - 1|$ and $|\\lambda_{B} - 1|$.\n$$\n\\lambda_{A} - 1 = \\frac{-1 - \\sqrt{17}}{2} - 1 = \\frac{-1 - \\sqrt{17} - 2}{2} = \\frac{-3 - \\sqrt{17}}{2}.\n$$\n$$\n\\lambda_{B} - 1 = \\frac{-1 + \\sqrt{17}}{2} - 1 = \\frac{-1 + \\sqrt{17} - 2}{2} = \\frac{-3 + \\sqrt{17}}{2}.\n$$\nSince $4 < \\sqrt{17} < 5$, we have $-3 + \\sqrt{17} > 0$.\nThe squared distances are:\n$$\n|\\lambda_{A} - 1|^{2} = \\left(\\frac{-3 - \\sqrt{17}}{2}\\right)^{2} = \\frac{9 + 6\\sqrt{17} + 17}{4} = \\frac{26 + 6\\sqrt{17}}{4}.\n$$\n$$\n|\\lambda_{B} - 1|^{2} = \\left(\\frac{-3 + \\sqrt{17}}{2}\\right)^{2} = \\frac{9 - 6\\sqrt{17} + 17}{4} = \\frac{26 - 6\\sqrt{17}}{4}.\n$$\nSince $6\\sqrt{17} > 0$, it is clear that $\\frac{26 - 6\\sqrt{17}}{4} < \\frac{26 + 6\\sqrt{17}}{4}$, which implies $|\\lambda_{B}-1| < |\\lambda_{A}-1|$.\nTherefore, the solution closer to $\\sigma=1$ is $\\lambda_{B}$. We set $\\lambda_{1} = \\lambda_{B} = \\frac{-1 + \\sqrt{17}}{2}$.\n\nThe result of one step of RII is the pair $(x_{1}, \\lambda_{1})$ where $x_{1} = \\begin{pmatrix} -\\frac{\\sqrt{2}}{2} \\\\ \\frac{\\sqrt{2}}{2} \\end{pmatrix}$ and $\\lambda_{1} = \\frac{-1 + \\sqrt{17}}{2}$. The final answer is a row matrix containing the components of $x_{1}$ followed by $\\lambda_{1}$.", "answer": "$$\n\\boxed{\\begin{pmatrix} -\\frac{\\sqrt{2}}{2} & \\frac{\\sqrt{2}}{2} & \\frac{-1 + \\sqrt{17}}{2} \\end{pmatrix}}\n$$", "id": "3561644"}, {"introduction": "Standard numerical methods often rely on problem smoothness. This exercise challenges you to confront a scenario where a naive application of Newton's method would struggle due to singularities in a transcendental eigenvalue problem involving $\\tan(\\lambda)$. By analyzing the issue and reformulating the equation to eliminate these poles, you will engage in the crucial practice of algorithmic design, transforming an unreliable method into a robust and quadratically convergent one [@problem_id:3561653].", "problem": "Consider the scalar nonlinear eigenvalue problem (NEP) given by the transcendental equation $t(\\lambda)=a+b\\tan(\\lambda)=0$, where $a\\in\\mathbb{R}$, $b\\in\\mathbb{R}\\setminus\\{0\\}$, and $\\lambda\\in\\mathbb{R}$. The function $t(\\lambda)$ has simple poles at $\\lambda=\\frac{\\pi}{2}+k\\pi$ for $k\\in\\mathbb{Z}$. Your goals are to design a root-finding method that remains robust in the vicinity of these poles and to extend the concepts to a matrix NEP with a $\\tan(\\lambda)$ term.\n\nDerive all results from foundational definitions and first principles, such as first-order Taylor expansion for Newton-type methods, basic derivatives of elementary functions, and standard linear algebra identities. Do not assume or quote problem-specific shortcut formulas.\n\nTasks:\n- Using a first-order Taylor expansion of $t(\\lambda)$ about an iterate, derive a Newton-type iteration for solving $t(\\lambda)=0$ and analyze the behavior of the update as $\\lambda$ approaches the nearest pole $\\lambda_{0}=\\frac{\\pi}{2}+k\\pi$. Specifically, obtain the leading-order asymptotic expression for the update magnitude in terms of the signed distance $d=\\lambda-\\lambda_{0}$ to the pole.\n- Propose a transformation that removes the poles of $\\tan(\\lambda)$ from the scalar equation and yields a locally quadratically convergent scheme away from singularities. Justify its robustness and state the resulting iteration map in closed form, including an explicit rule that preserves branch continuity across iterations.\n- Extend the approach to the matrix NEP $T(\\lambda)=A_{0}+A_{1}\\tan(\\lambda)$ with $A_{0},A_{1}\\in\\mathbb{C}^{n\\times n}$. Introduce a scalar Rayleigh functional $\\varphi(\\lambda)$ constructed from approximate left and right eigenvectors and derive a Newton correction for $\\lambda$ based on first principles. State an explicit algebraic expression for the scalar correction and explain how to enforce branch tracking to avoid crossing poles.\n- Finally, specialize to the scalar problem with $a=-\\frac{1}{3}$ and $b=2$. On the interval $\\left(0,\\frac{\\pi}{2}\\right)$, determine the unique solution $\\lambda^{\\star}$ in exact analytic form. Express the answer in radians and do not round.\n\nYour final reported answer must be a single exact analytic expression for $\\lambda^{\\star}$ (no units in the final box).", "solution": "The problem is evaluated to be valid as it is scientifically grounded in numerical linear algebra, well-posed, objective, and internally consistent. It presents a standard, albeit non-trivial, sequence of tasks related to solving nonlinear eigenvalue problems.\n\nThe problem is addressed in four parts as specified in the prompt.\n\nThe scalar nonlinear eigenvalue problem (NEP) is defined by the equation $t(\\lambda)=0$, where the function $t(\\lambda)$ is given by\n$$t(\\lambda) = a + b\\tan(\\lambda)$$\nwith $a \\in \\mathbb{R}$ and $b \\in \\mathbb{R}\\setminus\\{0\\}$.\n\nTask 1: Derivation and analysis of a Newton-type iteration for $t(\\lambda)=0$.\nA Newton-type iteration is derived from a first-order Taylor series expansion of the function $t(\\lambda)$ around the current iterate $\\lambda_k$. The expansion is\n$$t(\\lambda) \\approx t(\\lambda_k) + t'(\\lambda_k)(\\lambda - \\lambda_k)$$\nTo find the next iterate $\\lambda_{k+1}$, we set $t(\\lambda_{k+1}) = 0$ in the approximation, yielding\n$$0 \\approx t(\\lambda_k) + t'(\\lambda_k)(\\lambda_{k+1} - \\lambda_k)$$\nSolving for the update step $\\Delta\\lambda_k = \\lambda_{k+1} - \\lambda_k$, we get $\\Delta\\lambda_k = -\\frac{t(\\lambda_k)}{t'(\\lambda_k)}$. The iteration is thus\n$$\\lambda_{k+1} = \\lambda_k - \\frac{t(\\lambda_k)}{t'(\\lambda_k)}$$\nTo apply this, we must compute the derivative of $t(\\lambda)$:\n$$t'(\\lambda) = \\frac{d}{d\\lambda}(a + b\\tan(\\lambda)) = b\\sec^2(\\lambda)$$\nThe Newton iteration for $t(\\lambda)=0$ is therefore\n$$\\lambda_{k+1} = \\lambda_k - \\frac{a + b\\tan(\\lambda_k)}{b\\sec^2(\\lambda_k)}$$\nWe analyze the behavior of this iteration near a pole. The poles of $t(\\lambda)$ occur where $\\tan(\\lambda)$ is singular, which are at $\\lambda_0 = \\frac{\\pi}{2} + k\\pi$ for any integer $k \\in \\mathbb{Z}$. Let $\\lambda$ be an iterate close to a pole $\\lambda_0$, and let $d = \\lambda - \\lambda_0$ be the signed distance, with $|d| \\ll 1$.\nThe update is $\\Delta\\lambda = -\\frac{t(\\lambda)}{t'(\\lambda)}$. A more convenient form for analysis near a pole is obtained by multiplying the numerator and denominator by $\\cos^2(\\lambda)$:\n$$\\Delta\\lambda = -\\frac{(a + b\\tan(\\lambda))\\cos^2(\\lambda)}{b} = -\\frac{a\\cos^2(\\lambda) + b\\sin(\\lambda)\\cos(\\lambda)}{b}$$\nNow, we substitute $\\lambda = \\lambda_0 + d = \\frac{\\pi}{2} + k\\pi + d$. We use the identities:\n$$\\cos(\\lambda) = \\cos\\left(\\frac{\\pi}{2} + k\\pi + d\\right) = (-1)^{k+1}\\sin(d)$$\n$$\\sin(\\lambda) = \\sin\\left(\\frac{\\pi}{2} + k\\pi + d\\right) = (-1)^{k}\\cos(d)$$\nSubstituting these into the expression for $\\Delta\\lambda$:\n$$\\cos^2(\\lambda) = \\sin^2(d)$$\n$$\\sin(\\lambda)\\cos(\\lambda) = (-1)^{k}\\cos(d) \\cdot (-1)^{k+1}\\sin(d) = -\\sin(d)\\cos(d)$$\nSo, the update becomes\n$$\\Delta\\lambda = -\\frac{a\\sin^2(d) - b\\sin(d)\\cos(d)}{b} = \\sin(d)\\cos(d) - \\frac{a}{b}\\sin^2(d)$$\nFor small $d$, we use the Taylor expansions $\\sin(d) = d - \\frac{d^3}{6} + O(d^5)$ and $\\cos(d) = 1 - \\frac{d^2}{2} + O(d^4)$.\n$$\\Delta\\lambda = \\left(d - \\frac{d^3}{6} + \\dots\\right)\\left(1 - \\frac{d^2}{2} + \\dots\\right) - \\frac{a}{b}\\left(d - \\frac{d^3}{6} + \\dots\\right)^2$$\n$$\\Delta\\lambda = \\left(d - \\frac{d^3}{2} - \\frac{d^3}{6} + \\dots\\right) - \\frac{a}{b}\\left(d^2 - \\frac{d^4}{3} + \\dots\\right)$$\n$$\\Delta\\lambda = d - \\frac{a}{b}d^2 - \\frac{2}{3}d^3 + O(d^4)$$\nThe leading-order term in the update is $d$. Therefore, the leading-order asymptotic expression for the update magnitude is\n$$|\\Delta\\lambda| \\approx |d|$$\nThis indicates that near a pole, the magnitude of the Newton step is approximately equal to the distance to the pole. The next iterate will be approximately $\\lambda_{k+1} = \\lambda_k + \\Delta\\lambda_k \\approx (\\lambda_0 + d) + d = \\lambda_0 + 2d$, pushing the iterate away from the pole, which is undesirable behavior.\n\nTask 2: Transformation to a robust scheme.\nTo remove the poles, we reformulate the problem. The equation $a + b\\tan(\\lambda) = 0$ is equivalent to $a + b\\frac{\\sin(\\lambda)}{\\cos(\\lambda)} = 0$. Assuming we are not at a pole (where $\\cos(\\lambda)=0$), we can multiply by $\\cos(\\lambda)$ to obtain an equivalent equation without singularities:\n$$g(\\lambda) = a\\cos(\\lambda) + b\\sin(\\lambda) = 0$$\nThis function $g(\\lambda)$ is analytic for all $\\lambda \\in \\mathbb{R}$. We can apply Newton's method to $g(\\lambda) = 0$. The derivative is\n$$g'(\\lambda) = -a\\sin(\\lambda) + b\\cos(\\lambda)$$\nThe resulting iteration map is\n$$\\lambda_{k+1} = \\lambda_k - \\frac{g(\\lambda_k)}{g'(\\lambda_k)} = \\lambda_k - \\frac{a\\cos(\\lambda_k) + b\\sin(\\lambda_k)}{-a\\sin(\\lambda_k) + b\\cos(\\lambda_k)}$$\nThis scheme is robust because both $g(\\lambda)$ and $g'(\\lambda)$ are bounded, continuous, and differentiable for all real $\\lambda$. The denominators in the iteration do not approach zero at the locations of the original poles. Specifically, at a pole $\\lambda_0 = \\frac{\\pi}{2}+k\\pi$, we have $\\cos(\\lambda_0)=0$ and $|\\sin(\\lambda_0)|=1$, so $g(\\lambda_0) = \\pm b \\neq 0$ and $g'(\\lambda_0) = \\mp a$. The iteration is well-defined. Standard analysis of Newton's method shows it is locally quadratically convergent to simple roots. A root is simple if $g'(\\lambda) \\neq 0$ at the root. If $g(\\lambda)=0$ and $g'(\\lambda)=0$ for the same $\\lambda$, then $a\\cos\\lambda+b\\sin\\lambda=0$ and $-a\\sin\\lambda+b\\cos\\lambda=0$. This implies $\\tan\\lambda=-a/b$ and $\\tan\\lambda=b/a$. This would require $-a/b = b/a$, or $a^2+b^2=0$, which is impossible for $a,b \\in \\mathbb{R}$ unless $a=b=0$, but $b\\neq 0$ is given. Thus, all roots are simple, and the convergence is locally quadratic.\n\nThe solutions to $\\tan(\\lambda)=-a/b$ are periodic with period $\\pi$. To preserve branch continuity, we must ensure the iteration does not spuriously jump between solutions separated by multiples of $\\pi$. Let $\\Delta_k = -g(\\lambda_k)/g'(\\lambda_k)$ be the computed Newton update. If $|\\Delta_k|$ is large, the new iterate $\\lambda_k+\\Delta_k$ may be closer to a different root. The branches are indexed by an integer $m$ in the solution form $\\lambda_m = \\arctan(-a/b) + m\\pi$. To stay on a branch, the update step should be the smallest possible. An explicit rule to enforce this is to adjust the update by multiples of $\\pi$ so that its magnitude is less than or equal to $\\pi/2$. The corrected update $\\Delta_k'$ is:\n$$\\Delta_k' = \\Delta_k - \\pi \\cdot \\text{round}\\left(\\frac{\\Delta_k}{\\pi}\\right)$$\nwhere $\\text{round}(x)$ gives the nearest integer to $x$. The final iteration map is:\n$$\\lambda_{k+1} = \\lambda_k + \\Delta_k'$$\n\nTask 3: Extension to the matrix NEP.\nThe matrix NEP is $T(\\lambda) = A_0 + A_1\\tan(\\lambda) = 0$, where this denotes finding a scalar $\\lambda$ such that $T(\\lambda)$ is a singular matrix. Given approximate left and right eigenvectors, $y$ and $x$ respectively, for an approximate eigenvalue $\\lambda$, a scalar Rayleigh functional is constructed as $\\varphi(\\lambda) = y^H T(\\lambda) x$. A Newton correction for $\\lambda$ is derived by applying Newton's method to the scalar equation $\\varphi(\\lambda)=0$.\nThe update $\\delta = \\lambda_{new}-\\lambda$ is given by\n$$\\delta = -\\frac{\\varphi(\\lambda)}{\\varphi'(\\lambda)} = -\\frac{y^H T(\\lambda) x}{y^H T'(\\lambda) x}$$\nwhere we assume $x$ and $y$ are constant with respect to the differentiation of $\\lambda$.\nTo extend the pole removal approach, we define a regularized matrix function $G(\\lambda) = T(\\lambda)\\cos(\\lambda)$:\n$$G(\\lambda) = (A_0 + A_1\\tan(\\lambda))\\cos(\\lambda) = A_0\\cos(\\lambda) + A_1\\sin(\\lambda)$$\nThe eigenvalues of this problem are the same as the original one, as $\\det(G(\\lambda)) = \\cos^n(\\lambda)\\det(T(\\lambda))$, so $\\det(G(\\lambda))=0$ if and only if $\\det(T(\\lambda))=0$ for $\\cos(\\lambda) \\neq 0$.\nThe derivative of $G(\\lambda)$ is\n$$G'(\\lambda) = -A_0\\sin(\\lambda) + A_1\\cos(\\lambda)$$\nThe Newton correction for $\\lambda$ is based on the Rayleigh functional for $G(\\lambda)$, which is $\\psi(\\lambda) = y^H G(\\lambda) x$. The correction $\\delta$ is:\n$$\\delta = -\\frac{\\psi(\\lambda)}{\\psi'(\\lambda)} = -\\frac{y^H G(\\lambda) x}{y^H G'(\\lambda) x}$$\nSubstituting the expressions for $G(\\lambda)$ and $G'(\\lambda)$, we obtain the explicit algebraic expression for the scalar correction:\n$$\\delta = - \\frac{y^H(A_0\\cos(\\lambda) + A_1\\sin(\\lambda))x}{y^H(-A_0\\sin(\\lambda) + A_1\\cos(\\lambda))x}$$\nThe eigenvalues of the matrix NEP $T(\\lambda)$ are also periodic with period $\\pi$. This arises because the singularity condition $\\det(A_0 + A_1\\tan\\lambda)=0$ leads to a polynomial in $\\tau = \\tan\\lambda$. For each root $\\tau_j$ of this polynomial, the eigenvalues for $\\lambda$ are $\\lambda_{j,m} = \\arctan(\\tau_j) + m\\pi$.\nTo enforce branch tracking and avoid jumping between different integers $m$ for a given mode $j$, the same rule as in the scalar case is applied. The computed correction $\\delta$ is adjusted:\n$$\\delta' = \\delta - \\pi \\cdot \\text{round}\\left(\\frac{\\delta}{\\pi}\\right)$$\nThe next iterate for the eigenvalue is $\\lambda_{new} = \\lambda + \\delta'$.\n\nTask 4: Specific scalar problem.\nWe are given $a = -1/3$ and $b = 2$. The equation is\n$$-\\frac{1}{3} + 2\\tan(\\lambda) = 0$$\nWe need to find the unique solution $\\lambda^{\\star}$ in the interval $\\left(0, \\frac{\\pi}{2}\\right)$.\nRearranging the equation to solve for $\\tan(\\lambda)$:\n$$2\\tan(\\lambda) = \\frac{1}{3}$$\n$$\\tan(\\lambda) = \\frac{1}{6}$$\nThe solution for $\\lambda$ is obtained by taking the arctangent of both sides:\n$$\\lambda = \\arctan\\left(\\frac{1}{6}\\right)$$\nThe principal value of the $\\arctan$ function has a range of $\\left(-\\frac{\\pi}{2}, \\frac{\\pi}{2}\\right)$. Since the argument $\\frac{1}{6}$ is positive, the resulting angle is in the first quadrant, i.e., $\\lambda \\in \\left(0, \\frac{\\pi}{2}\\right)$. The tangent function is strictly monotonic on this interval, so this solution is unique.\nThus, the unique solution in the specified interval is $\\lambda^{\\star} = \\arctan\\left(\\frac{1}{6}\\right)$.", "answer": "$$\\boxed{\\arctan\\left(\\frac{1}{6}\\right)}$$", "id": "3561653"}, {"introduction": "In numerical practice, an algorithm's performance is deeply connected to the problem's conditioning. This final exercise shifts focus from the iterative solver itself to the critical pre-processing step of scaling for quadratic eigenvalue problems (QEPs) of the form $(\\lambda^2 M + \\lambda C + K)x=0$. You will analyze a strategy to balance the norms of the matrices $M$, $C$, and $K$, thereby improving the numerical stability of the standard companion linearization, a key step toward obtaining accurate solutions [@problem_id:3561669].", "problem": "Consider the quadratic eigenvalue problem (QEP) in numerical linear algebra,\n$$\nQ(\\lambda) x \\;=\\; \\big(\\lambda^{2} M + \\lambda C + K\\big) x \\;=\\; 0,\n$$\nwith coefficient matrices $M, C, K \\in \\mathbb{C}^{n\\times n}$. Use the matrix $2$-norm throughout. A standard first companion linearization of $Q(\\lambda)$ is the generalized eigenvalue problem\n$$\n\\big(\\lambda A - B\\big) \\begin{bmatrix} x \\\\ y \\end{bmatrix} \\;=\\; 0,\n\\quad\\text{where}\\quad\nA \\;=\\; \\begin{bmatrix} I & 0 \\\\ 0 & M \\end{bmatrix},\n\\quad\nB \\;=\\; \\begin{bmatrix} 0 & I \\\\ -K & -C \\end{bmatrix},\n$$\nand $y = \\lambda x$ enforces equivalence with $Q(\\lambda)$. To mitigate ill-conditioning, one may apply a variable scaling $\\lambda = \\tau \\mu$ with $\\tau > 0$ and a scalar normalization $Q \\mapsto \\widehat{Q}$ defined by\n$$\n\\widehat{Q}(\\mu) \\;=\\; \\beta\\, Q(\\tau \\mu) \\;=\\; \\mu^{2} \\widehat{M} + \\mu \\widehat{C} + \\widehat{K},\n\\quad \\text{where}\\quad\n\\widehat{M} \\;=\\; \\beta \\tau^{2} M,\\;\\; \\widehat{C} \\;=\\; \\beta \\tau C,\\;\\; \\widehat{K} \\;=\\; \\beta K.\n$$\nLet the coefficient norms be\n$$\n\\|M\\| \\;=\\; 2\\times 10^{-4}, \\qquad \\|C\\| \\;=\\; 30, \\qquad \\|K\\| \\;=\\; 8\\times 10^{4}.\n$$\nAnalyze, from first principles, how the scaling $(\\tau,\\beta)$ affects:\n- the normwise backward error proxy for a residual $\\|\\widehat{Q}(\\mu)x\\|$ in terms of $\\|\\widehat{M}\\|$, $\\|\\widehat{C}\\|$, $\\|\\widehat{K}\\|$, and $|\\mu|$, and\n- the norms of the companion linearization blocks $\\widehat{A} = \\begin{bmatrix} I & 0 \\\\ 0 & \\widehat{M} \\end{bmatrix}$ and $\\widehat{B} = \\begin{bmatrix} 0 & I \\\\ -\\widehat{K} & -\\widehat{C} \\end{bmatrix}$.\n\nThen, based on this analysis, select the scaling that implements the following balancing strategy: choose $\\tau$ to minimize \n$$\ng(\\tau) \\;=\\; \\max\\big\\{\\, \\tau^{2}\\|M\\|,\\; \\tau \\|C\\|,\\; \\|K\\| \\,\\big\\},\n$$\nand set $\\beta = 1/g(\\tau)$ so that $\\max\\{\\|\\widehat{M}\\|,\\|\\widehat{C}\\|,\\|\\widehat{K}\\|\\} = 1$. Which option achieves this and is thus most appropriate for balancing $\\|\\widehat{M}\\|$, $\\|\\widehat{C}\\|$, and $\\|\\widehat{K}\\|$ while improving the conditioning proxy of the linearization?\n\nA. Choose $\\tau = \\sqrt{\\|K\\|/\\|M\\|}$ and $\\beta = 1/\\|K\\|$.\n\nB. Choose $\\tau = \\|K\\|/\\|C\\|$ and $\\beta = 1/\\|K\\|$.\n\nC. Choose $\\tau = \\|C\\|/\\|M\\|$ and $\\beta = 1/(\\tau \\|C\\|)$.\n\nD. Choose $\\tau = 1$ and $\\beta = 1/\\|K\\|$.\n\nAnswer by selecting the single best option.", "solution": "The user has requested a detailed analysis of a quadratic eigenvalue problem scaling strategy.\n\n**Problem Validation**\n\nFirst, I shall validate the problem statement.\n\n**Step 1: Extract Givens**\n- The quadratic eigenvalue problem (QEP) is defined as $Q(\\lambda)x = (\\lambda^{2} M + \\lambda C + K) x = 0$, with $M, C, K \\in \\mathbb{C}^{n\\times n}$.\n- Scaling transformation: $\\lambda = \\tau \\mu$ with $\\tau > 0$, and normalization $\\widehat{Q}(\\mu) = \\beta\\, Q(\\tau \\mu)$.\n- Scaled coefficient matrices: $\\widehat{M} = \\beta \\tau^{2} M$, $\\widehat{C} = \\beta \\tau C$, $\\widehat{K} = \\beta K$.\n- Given coefficient norms (using the matrix $2$-norm): $\\|M\\| = 2\\times 10^{-4}$, $\\|C\\| = 30$, $\\|K\\| = 8\\times 10^{4}$.\n- The specified balancing strategy is:\n    1. Choose $\\tau > 0$ to minimize the function $g(\\tau) = \\max\\big\\{\\, \\tau^{2}\\|M\\|,\\; \\tau \\|C\\|,\\; \\|K\\| \\,\\big\\}$.\n    2. Set the normalization factor $\\beta = 1/g(\\tau)$, where $\\tau$ is the value found in the first step.\n- The companion linearization for the scaled problem involves matrices $\\widehat{A} = \\begin{bmatrix} I & 0 \\\\ 0 & \\widehat{M} \\end{bmatrix}$ and $\\widehat{B} = \\begin{bmatrix} 0 & I \\\\ -\\widehat{K} & -\\widehat{C} \\end{bmatrix}$.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientific Groundedness:** The problem describes eigenvalue scaling and balancing for QEPs, along with the first companion linearization. These are standard and well-established techniques in numerical linear algebra for solving polynomial eigenvalue problems. The use of norms to guide the scaling is a fundamental concept in numerical analysis. The problem is scientifically sound.\n- **Well-Posedness:** The problem asks to find parameters $(\\tau, \\beta)$ by solving a well-defined minimization problem. The function $g(\\tau)$ is a maximum of three continuous, non-negative functions, and as such, it is continuous and guaranteed to have a minimum on $\\tau > 0$. The question is mathematically well-posed.\n- **Objectivity:** The problem is stated in precise mathematical language, using standard definitions from linear algebra and numerical analysis. It is objective and free from ambiguity once the tie-breaking condition (\"most appropriate for balancing\") is considered.\n\n**Step 3: Verdict and Action**\nThe problem is valid. I will proceed with the derivation and analysis.\n\n**Derivation of the Optimal Scaling Parameters**\n\nThe core of the problem is to implement the specified balancing strategy. This involves minimizing the function $g(\\tau) = \\max\\{f_1(\\tau), f_2(\\tau), f_3(\\tau)\\}$ for $\\tau > 0$, where:\n- $f_1(\\tau) = \\tau^2 \\|M\\| = (2\\times 10^{-4})\\tau^2$\n- $f_2(\\tau) = \\tau \\|C\\| = 30\\tau$\n- $f_3(\\tau) = \\|K\\| = 8\\times 10^{4}$\n\nThe minimum of the upper envelope function $g(\\tau)$ must occur at a point where its derivative is zero or at a non-differentiable point (a \"kink\"). The kinks occur where two or more of the functions $f_1, f_2, f_3$ intersect. Let us find these intersection points for $\\tau$:\n- $f_1(\\tau) = f_3(\\tau) \\implies \\tau^2 \\|M\\| = \\|K\\| \\implies \\tau = \\sqrt{\\frac{\\|K\\|}{\\|M\\|}}$. Let this be $\\tau_{13}$.\n  $\\tau_{13} = \\sqrt{\\frac{8\\times 10^{4}}{2\\times 10^{-4}}} = \\sqrt{4\\times 10^{8}} = 2\\times 10^{4}$.\n- $f_2(\\tau) = f_3(\\tau) \\implies \\tau \\|C\\| = \\|K\\| \\implies \\tau = \\frac{\\|K\\|}{\\|C\\|}$. Let this be $\\tau_{23}$.\n  $\\tau_{23} = \\frac{8\\times 10^{4}}{30} = \\frac{8}{3}\\times 10^{3} \\approx 2666.67$.\n- $f_1(\\tau) = f_2(\\tau) \\implies \\tau^2 \\|M\\| = \\tau \\|C\\| \\implies \\tau = \\frac{\\|C\\|}{\\|M\\|}$. Let this be $\\tau_{12}$.\n  $\\tau_{12} = \\frac{30}{2\\times 10^{-4}} = 15\\times 10^{4} = 1.5\\times 10^{5}$.\n\nWe observe the ordering $\\tau_{23} < \\tau_{13} < \\tau_{12}$. Now we analyze the behavior of $g(\\tau)$ across intervals defined by these points:\n1.  For $0 < \\tau \\le \\tau_{23}$:\n    - $\\tau \\|C\\| \\le \\tau_{23} \\|C\\| = \\|K\\| = f_3(\\tau)$.\n    - $\\tau^2 \\|M\\| \\le \\tau_{23}^2 \\|M\\| < \\tau_{13}^2 \\|M\\| = \\|K\\| = f_3(\\tau)$. A direct calculation shows $(\\|K\\|/\\|C\\|)^2\\|M\\| = \\frac{\\|K\\|^2\\|M\\|}{\\|C\\|^2} = \\frac{(8\\times 10^4)^2(2\\times 10^{-4})}{30^2} = \\frac{64 \\times 10^8 \\times 2 \\times 10^{-4}}{900} = \\frac{128 \\times 10^4}{900} \\approx 1422$, which is much less than $\\|K\\|=80000$.\n    - Thus, in this interval, $g(\\tau) = \\max\\{f_1(\\tau), f_2(\\tau), f_3(\\tau)\\} = f_3(\\tau) = \\|K\\|$. The function $g(\\tau)$ is constant.\n\n2.  For $\\tau > \\tau_{23}$:\n    - $\\tau \\|C\\| > \\tau_{23} \\|C\\| = \\|K\\|$.\n    - Therefore, $g(\\tau) = \\max\\{\\tau^2\\|M\\|, \\tau\\|C\\|\\}$. Both $f_1(\\tau)$ and $f_2(\\tau)$ are strictly increasing functions for $\\tau>0$. The maximum of two strictly increasing functions is also strictly increasing.\n    - So, for $\\tau > \\tau_{23}$, $g(\\tau)$ is strictly increasing.\n\nCombining these observations, the function $g(\\tau)$ is constant for $\\tau \\in (0, \\tau_{23}]$ and strictly increases for $\\tau > \\tau_{23}$. Therefore, the minimum value of $g(\\tau)$ is $\\|K\\|$ and it is achieved for any $\\tau$ in the interval $(0, \\tau_{23}]$.\n\nThe problem asks to select the single \"most appropriate\" scaling. This implies a secondary criterion must be used to select a unique $\\tau$ from the interval of minimizers $(0, \\tau_{23}]$. The goal of scaling is to make the norms of the scaled matrices, $\\|\\widehat{M}\\|$, $\\|\\widehat{C}\\|$, and $\\|\\widehat{K}\\|$, as close to each other as possible, and ideally of order $1$.\n\nFor any $\\tau \\in (0, \\tau_{23}]$, the minimization gives $g(\\tau) = \\|K\\|$. The strategy then dictates $\\beta = 1/g(\\tau) = 1/\\|K\\|$. Let's examine the norms of the scaled matrices with this $\\beta$:\n- $\\|\\widehat{K}\\| = \\beta \\|K\\| = \\frac{1}{\\|K\\|} \\|K\\| = 1$.\n- $\\|\\widehat{C}\\| = \\beta \\tau \\|C\\| = \\frac{\\tau \\|C\\|}{\\|K\\|}$. Since $\\tau \\le \\tau_{23} = \\|K\\|/\\|C\\|$, we have $\\|\\widehat{C}\\| \\le 1$.\n- $\\|\\widehat{M}\\| = \\beta \\tau^2 \\|M\\| = \\frac{\\tau^2 \\|M\\|}{\\|K\\|}$. Since $\\tau \\le \\tau_{23} < \\tau_{13} = \\sqrt{\\|K\\|/\\|M\\|}$, we have $\\tau^2 < \\|K\\|/\\|M\\|$, which implies $\\|\\widehat{M}\\| < 1$.\n\nTo achieve the best balance among the norms $\\{ \\|\\widehat{M}\\|, \\|\\widehat{C}\\|, \\|\\widehat{K}\\| \\}$, we want them to be as close to each other as possible. We already have $\\|\\widehat{K}\\|=1$, while $\\|\\widehat{M}\\|<1$ and $\\|\\widehat{C}\\|\\le 1$. To make $\\|\\widehat{C}\\|$ and $\\|\\widehat{M}\\|$ as large as possible (and thus closer to $1$), we should choose the largest possible value for $\\tau$ in the interval of minimizers, which is $\\tau = \\tau_{23} = \\|K\\|/\\|C\\|$.\n\nWith this choice, $\\tau = \\|K\\|/\\|C\\|$, we have:\n- Optimal $\\tau = \\|K\\|/\\|C\\|$.\n- $g(\\tau) = \\|K\\|$.\n- Optimal $\\beta = 1/\\|K\\|$.\n\nThis specific choice results in the following scaled norms:\n- $\\|\\widehat{K}\\| = \\beta\\|K\\| = 1$.\n- $\\|\\widehat{C}\\| = \\beta\\tau\\|C\\| = \\frac{1}{\\|K\\|} \\left(\\frac{\\|K\\|}{\\|C\\|}\\right) \\|C\\| = 1$.\n- $\\|\\widehat{M}\\| = \\beta\\tau^2\\|M\\| = \\frac{1}{\\|K\\|} \\left(\\frac{\\|K\\|}{\\|C\\|}\\right)^2 \\|M\\| = \\frac{\\|K\\|\\|M\\|}{\\|C\\|^2} = \\frac{(8\\times 10^4)(2\\times 10^{-4})}{30^2} = \\frac{16}{900} \\approx 0.0178$.\n\nThe resulting norms are $\\{\\approx 0.0178, 1, 1\\}$, which is a very reasonable balance, with two of the three norms being exactly $1$. This choice of $\\tau$ is therefore the \"most appropriate\".\n\n**Option-by-Option Analysis**\n\nA. **Choose $\\tau = \\sqrt{\\|K\\|/\\|M\\|}$ and $\\beta = 1/\\|K\\|$.**\nThis choice corresponds to $\\tau = \\tau_{13} = 2\\times 10^4$. As shown earlier, $\\tau_{13} > \\tau_{23}$, so this value of $\\tau$ does not minimize $g(\\tau)$. At this $\\tau$, $g(\\tau_{13}) = \\max\\{\\|K\\|, \\tau_{13}\\|C\\|, \\|K\\|\\} = \\tau_{13}\\|C\\| = (2\\times 10^4)(30) = 6\\times 10^5$, which is greater than the minimum value of $\\|K\\|=8\\times 10^4$. Since this $\\tau$ does not implement the primary step of the strategy, this option is **Incorrect**.\n\nB. **Choose $\\tau = \\|K\\|/\\|C\\|$ and $\\beta = 1/\\|K\\|$.**\nThis choice corresponds to $\\tau = \\tau_{23} \\approx 2666.67$. This value lies in the interval $(0, \\tau_{23}]$ and is thus a minimizer of $g(\\tau)$. The minimum value is $g(\\tau_{23}) = \\|K\\|$. The strategy then dictates $\\beta = 1/g(\\tau_{23}) = 1/\\|K\\|$. This option perfectly matches our derived optimal choice, which uniquely resolves the ambiguity in the problem statement by providing the best balance among the coefficient norms. This option is **Correct**.\n\nC. **Choose $\\tau = \\|C\\|/\\|M\\|$ and $\\beta = 1/(\\tau \\|C\\|)$.**\nThis choice corresponds to $\\tau = \\tau_{12} = 1.5\\times 10^5$. This value is greater than $\\tau_{23}$, so it does not minimize $g(\\tau)$. At this $\\tau$, $g(\\tau_{12}) = \\max\\{\\tau_{12}^2\\|M\\|, \\tau_{12}\\|C\\|, \\|K\\|\\} = \\max\\{\\tau_{12}\\|C\\|, \\tau_{12}\\|C\\|, \\|K\\|\\} = \\tau_{12}\\|C\\| = (1.5\\times 10^5)(30) = 4.5\\times 10^6$, which is much larger than the minimum value. Since this $\\tau$ does not implement the strategy, this option is **Incorrect**.\n\nD. **Choose $\\tau = 1$ and $\\beta = 1/\\|K\\|$.**\nSince $1 < \\tau_{23} \\approx 2666.67$, the choice $\\tau=1$ is a valid minimizer of $g(\\tau)$. For $\\tau=1$, $g(1) = \\max\\{\\|M\\|, \\|C\\|, \\|K\\|\\} = \\|K\\|$. The strategy then gives $\\beta = 1/g(1) = 1/\\|K\\|$. So, this option does implement the specified minimization strategy. However, we must assess if it is the \"most appropriate\". The scaled norms would be: $\\|\\widehat{K}\\|=1$, $\\|\\widehat{C}\\|=\\|C\\|/\\|K\\| = 30/(8\\times 10^4) = 3.75\\times 10^{-4}$, and $\\|\\widehat{M}\\|=\\|M\\|/\\|K\\| = (2\\times 10^{-4})/(8\\times 10^4) = 2.5\\times 10^{-9}$. These norms are $\\{2.5\\times 10^{-9}, 3.75\\times 10^{-4}, 1\\}$, which are extremely disparate. Compared to the balancing achieved by option B, this is far inferior. Therefore, it is not the \"most appropriate\" choice. This option is **Incorrect**.", "answer": "$$\\boxed{B}$$", "id": "3561669"}]}