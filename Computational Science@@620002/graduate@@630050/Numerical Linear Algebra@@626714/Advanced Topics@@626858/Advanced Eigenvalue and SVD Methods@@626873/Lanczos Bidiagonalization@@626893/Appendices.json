{"hands_on_practices": [{"introduction": "A critical component of any iterative algorithm is a reliable and efficient stopping criterion. For the Lanczos bidiagonalization, convergence of a Ritz triplet is measured by its residual norm. This exercise [@problem_id:3539929] guides you through a short calculation to reveal one of the most elegant features of the Golub-Kahan process: a remarkably inexpensive way to compute this residual norm, using only quantities that are already available from the bidiagonalization process.", "problem": "Consider a real matrix $A \\in \\mathbb{R}^{m \\times n}$ and the Golub–Kahan (GK) bidiagonalization after $k$ steps, which produces orthonormal matrices $U_k \\in \\mathbb{R}^{m \\times k}$ and $V_k \\in \\mathbb{R}^{n \\times k}$, scalars $\\alpha_1,\\dots,\\alpha_k$ and $\\beta_1,\\dots,\\beta_k$, and a bidiagonal matrix $B_k \\in \\mathbb{R}^{k \\times k}$ with diagonal entries $\\alpha_i$ and superdiagonal entries $\\beta_i$. By construction, the GK relations are\n$$\nA V_k = U_k B_k, \\qquad A^{\\top} U_k = V_k B_k^{\\top} + \\beta_k v_{k+1} e_k^{\\top},\n$$\nwhere $v_{k+1} \\in \\mathbb{R}^{n}$ is a unit vector orthogonal to the columns of $V_k$, and $e_k \\in \\mathbb{R}^{k}$ is the $k$-th canonical basis vector.\n\nLet $B_k$ have the Singular Value Decomposition (SVD) $B_k \\widehat{q}_i = \\widehat{\\sigma}_i \\widehat{p}_i$ and $B_k^{\\top} \\widehat{p}_i = \\widehat{\\sigma}_i \\widehat{q}_i$, with $\\widehat{\\sigma}_i > 0$, $\\widehat{p}_i \\in \\mathbb{R}^{k}$, and $\\widehat{q}_i \\in \\mathbb{R}^{k}$ orthonormal. The corresponding Ritz approximations to the singular triplets of $A$ are $u_i = U_k \\widehat{p}_i$, $v_i = V_k \\widehat{q}_i$, and $\\sigma_i \\approx \\widehat{\\sigma}_i$.\n\nA widely used stopping criterion for computing the Singular Value Decomposition (SVD) via GK requires the relative residual of the pair $A^{\\top} u_i \\approx \\sigma_i v_i$ to be smaller than a prescribed tolerance. Specifically, the relative residual for the $i$-th Ritz triplet is\n$$\n\\rho_i \\equiv \\frac{\\|A^{\\top} u_i - \\widehat{\\sigma}_i v_i\\|_2}{\\widehat{\\sigma}_i}.\n$$\n\nSuppose that after $k$ steps, for the dominant triplet $i=1$, you are given the following quantities: the next GK subdiagonal coefficient $\\beta_k = 1.7 \\times 10^{-9}$, the last component of the left singular vector of $B_k$, $e_k^{\\top} \\widehat{p}_1 = 0.45$, and the Ritz singular value $\\widehat{\\sigma}_1 = 3.2$. Using only the GK relations above and the definition of $\\rho_1$, compute the value of the relative residual $\\rho_1$. Round your final answer to four significant figures.", "solution": "The user wants to compute the relative residual $\\rho_1$ for the dominant Ritz triplet after $k$ steps of the Golub-Kahan (GK) bidiagonalization. The problem is first validated to ensure its scientific and mathematical integrity.\n\n### Step 1: Extract Givens\n- A real matrix $A \\in \\mathbb{R}^{m \\times n}$.\n- Golub-Kahan (GK) bidiagonalization relations after $k$ steps:\n  $$A V_k = U_k B_k$$\n  $$A^{\\top} U_k = V_k B_k^{\\top} + \\beta_k v_{k+1} e_k^{\\top}$$\n- Orthonormal matrices $U_k \\in \\mathbb{R}^{m \\times k}$ and $V_k \\in \\mathbb{R}^{n \\times k}$.\n- $v_{k+1} \\in \\mathbb{R}^{n}$ is a unit vector orthogonal to the columns of $V_k$.\n- $e_k \\in \\mathbb{R}^k$ is the $k$-th canonical basis vector.\n- Bidiagonal matrix $B_k \\in \\mathbb{R}^{k \\times k}$ with diagonal entries $\\alpha_i$ and superdiagonal entries $\\beta_i$.\n- SVD of $B_k$ for the $i$-th singular triplet: $B_k \\widehat{q}_i = \\widehat{\\sigma}_i \\widehat{p}_i$ and $B_k^{\\top} \\widehat{p}_i = \\widehat{\\sigma}_i \\widehat{q}_i$, with $\\widehat{\\sigma}_i > 0$.\n- Ritz approximations for the $i$-th triplet of $A$: $u_i = U_k \\widehat{p}_i$, $v_i = V_k \\widehat{q}_i$, and singular value $\\sigma_i \\approx \\widehat{\\sigma}_i$.\n- Definition of the relative residual for the $i$-th Ritz triplet:\n  $$\\rho_i \\equiv \\frac{\\|A^{\\top} u_i - \\widehat{\\sigma}_i v_i\\|_2}{\\widehat{\\sigma}_i}$$\n- Given values for the dominant triplet ($i=1$):\n  - $\\beta_k = 1.7 \\times 10^{-9}$\n  - $e_k^{\\top} \\widehat{p}_1 = 0.45$\n  - $\\widehat{\\sigma}_1 = 3.2$\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded in the well-established theory of numerical linear algebra, specifically the Lanczos bidiagonalization method (equivalent to Golub-Kahan) for computing the SVD of a matrix. The provided GK relations are standard. The definitions of Ritz vectors, Ritz values, and residuals are correct. The problem is well-posed, as it provides all necessary data and relations to uniquely determine the requested value. The language is objective and precise. There are no contradictions, missing information, or pseudoscientific claims. The provided numerical values are physically and mathematically plausible within the context of a converging iterative algorithm. Therefore, the problem is valid.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\nThe objective is to compute the relative residual $\\rho_1$. We begin with its definition:\n$$\n\\rho_1 = \\frac{\\|A^{\\top} u_1 - \\widehat{\\sigma}_1 v_1\\|_2}{\\widehat{\\sigma}_1}\n$$\nLet's analyze the residual vector in the numerator, which we denote as $r_1$:\n$$\nr_1 = A^{\\top} u_1 - \\widehat{\\sigma}_1 v_1\n$$\nThe Ritz vector $u_1$ is defined as $u_1 = U_k \\widehat{p}_1$. Substituting this into the expression for $r_1$:\n$$\nr_1 = A^{\\top} (U_k \\widehat{p}_1) - \\widehat{\\sigma}_1 v_1 = (A^{\\top} U_k) \\widehat{p}_1 - \\widehat{\\sigma}_1 v_1\n$$\nNow, we use the second of the given GK relations, $A^{\\top} U_k = V_k B_k^{\\top} + \\beta_k v_{k+1} e_k^{\\top}$, to substitute for the term $(A^{\\top} U_k)$:\n$$\nr_1 = (V_k B_k^{\\top} + \\beta_k v_{k+1} e_k^{\\top}) \\widehat{p}_1 - \\widehat{\\sigma}_1 v_1\n$$\nApplying the distributive property:\n$$\nr_1 = V_k B_k^{\\top} \\widehat{p}_1 + (\\beta_k v_{k+1} e_k^{\\top}) \\widehat{p}_1 - \\widehat{\\sigma}_1 v_1\n$$\nThe term $(e_k^{\\top} \\widehat{p}_1)$ is a scalar, representing the $k$-th component of the vector $\\widehat{p}_1$. So, the expression becomes:\n$$\nr_1 = V_k (B_k^{\\top} \\widehat{p}_1) + \\beta_k (e_k^{\\top} \\widehat{p}_1) v_{k+1} - \\widehat{\\sigma}_1 v_1\n$$\nNext, we use the property of the SVD of $B_k$, specifically $B_k^{\\top} \\widehat{p}_1 = \\widehat{\\sigma}_1 \\widehat{q}_1$. Substituting this into the equation:\n$$\nr_1 = V_k (\\widehat{\\sigma}_1 \\widehat{q}_1) + \\beta_k (e_k^{\\top} \\widehat{p}_1) v_{k+1} - \\widehat{\\sigma}_1 v_1\n$$\nThe first term can be rewritten as $\\widehat{\\sigma}_1 (V_k \\widehat{q}_1)$. From the definition of the Ritz vector $v_1 = V_k \\widehat{q}_1$, we have:\n$$\nr_1 = \\widehat{\\sigma}_1 v_1 + \\beta_k (e_k^{\\top} \\widehat{p}_1) v_{k+1} - \\widehat{\\sigma}_1 v_1\n$$\nThe terms $\\widehat{\\sigma}_1 v_1$ and $-\\widehat{\\sigma}_1 v_1$ cancel each other out, leaving a remarkably simple expression for the residual vector:\n$$\nr_1 = \\beta_k (e_k^{\\top} \\widehat{p}_1) v_{k+1}\n$$\nNow, we compute the Euclidean norm of this vector:\n$$\n\\|r_1\\|_2 = \\| \\beta_k (e_k^{\\top} \\widehat{p}_1) v_{k+1} \\|_2\n$$\nSince $\\beta_k$ and $(e_k^{\\top} \\widehat{p}_1)$ are scalars, we can factor them out of the norm. The off-diagonal entries $\\beta_i$ in the GK process are non-negative by construction, so $|\\beta_k| = \\beta_k$.\n$$\n\\|r_1\\|_2 = \\beta_k |e_k^{\\top} \\widehat{p}_1| \\|v_{k+1}\\|_2\n$$\nThe problem states that $v_{k+1}$ is a unit vector, which means $\\|v_{k+1}\\|_2 = 1$. Therefore, the norm of the residual is:\n$$\n\\|r_1\\|_2 = \\beta_k |e_k^{\\top} \\widehat{p}_1|\n$$\nFinally, we substitute this result back into the formula for the relative residual $\\rho_1$:\n$$\n\\rho_1 = \\frac{\\beta_k |e_k^{\\top} \\widehat{p}_1|}{\\widehat{\\sigma}_1}\n$$\nNow we can insert the given numerical values: $\\beta_k = 1.7 \\times 10^{-9}$, $e_k^{\\top} \\widehat{p}_1 = 0.45$, and $\\widehat{\\sigma}_1 = 3.2$.\n$$\n\\rho_1 = \\frac{(1.7 \\times 10^{-9}) |0.45|}{3.2}\n$$\n$$\n\\rho_1 = \\frac{1.7 \\times 0.45 \\times 10^{-9}}{3.2} = \\frac{0.765 \\times 10^{-9}}{3.2}\n$$\n$$\n\\rho_1 = 0.2390625 \\times 10^{-9} = 2.390625 \\times 10^{-10}\n$$\nThe problem requires the answer to be rounded to four significant figures. The number is $2.390625 \\times 10^{-10}$. The fifth significant digit is $6$, so we round up the fourth digit.\n$$\n\\rho_1 \\approx 2.391 \\times 10^{-10}\n$$\nThis is the value of the relative residual for the dominant Ritz triplet.", "answer": "$$\n\\boxed{2.391 \\times 10^{-10}}\n$$", "id": "3539929"}, {"introduction": "The basic Lanczos bidiagonalization excels at approximating the extreme singular values of a matrix. To compute a larger portion of the singular spectrum, we need a more sophisticated approach involving restarts and deflation. This practice [@problem_id:3539889] challenges you to implement a robust \"hard-locking\" deflation scheme, where converged singular triplets are identified, stored, and explicitly removed from the search space. This is a crucial technique for transforming the fundamental Lanczos iteration into a practical and powerful SVD solver capable of finding multiple singular values.", "problem": "You are asked to design and implement a robust numerical procedure for computing a subset of dominant singular values of a real rectangular matrix using the Golub–Kahan Lanczos bidiagonalization (GKLB) with a deflation strategy that locks converged singular triplets and continues targeting the remaining ones without losing numerical stability. The target is to produce a complete, runnable program that performs this task on a fixed test suite and reports quantitative results.\n\nStart from the following principles and core definitions:\n\n- The Singular Value Decomposition (SVD) of a real matrix $A \\in \\mathbb{R}^{m \\times n}$ is $A = U \\Sigma V^\\top$, where $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{n \\times n}$ are orthogonal, and $\\Sigma \\in \\mathbb{R}^{m \\times n}$ has nonnegative diagonal entries (the singular values) in nonincreasing order. The leading $r = \\operatorname{rank}(A)$ singular values are positive.\n\n- The Golub–Kahan Lanczos bidiagonalization (GKLB) constructs, given a unit-norm starting vector $v_1 \\in \\mathbb{R}^n$, two matrices $U_k \\in \\mathbb{R}^{m \\times k}$ and $V_k \\in \\mathbb{R}^{n \\times k}$ with orthonormal columns and a bidiagonal matrix $B_k \\in \\mathbb{R}^{k \\times k}$ with diagonal entries $\\alpha_j$ and superdiagonal entries $\\beta_j$ such that\n  $$\n  A V_k = U_k B_k \\quad \\text{and} \\quad A^\\top U_k = V_k B_k^\\top + \\beta_k v_{k+1} e_k^\\top,\n  $$\n  where $e_k \\in \\mathbb{R}^k$ is the $k$-th standard basis vector and $v_{k+1} \\in \\mathbb{R}^n$ is a unit-norm vector. In exact arithmetic, the columns of $U_k$ and $V_k$ are orthonormal, but in floating-point arithmetic explicit reorthogonalization is needed to maintain this property.\n\n- The singular values of $B_k$ (the Ritz singular values) approximate singular values of $A$. Let $B_k = \\widehat{U} \\Theta \\widehat{V}^\\top$ be the Singular Value Decomposition of $B_k$, with $\\Theta = \\operatorname{diag}(\\theta_1,\\dots,\\theta_k)$ and $\\theta_1 \\ge \\dots \\ge \\theta_k \\ge 0$. The corresponding Ritz singular vectors of $A$ are $\\widetilde{u}_i = U_k \\widehat{U} e_i$ and $\\widetilde{v}_i = V_k \\widehat{V} e_i$. A standard bound for the residual norm of the $i$-th Ritz triplet $(\\theta_i,\\widetilde{u}_i,\\widetilde{v}_i)$ is given by the norm of the residual for the transposed problem:\n  $$\n  \\| A^\\top \\widetilde{u}_i - \\theta_i \\widetilde{v}_i \\|_2 = \\beta_k \\left| e_k^\\top \\widehat{U} e_i \\right|.\n  $$\n\n- A deflation strategy known as hard locking removes converged Ritz triplets from the active subspace so that subsequent GKLB steps build orthogonally to the locked singular vectors. Stability is maintained by explicit full reorthogonalization of newly generated basis vectors against both the current GKLB basis and all previously locked singular vectors.\n\nTask:\n\n- Implement a program that, given a matrix $A \\in \\mathbb{R}^{m \\times n}$, an integer $k \\ge 2$, a requested number of singular values $\\mathrm{nev}$, a tolerance $\\tau > 0$, and a maximum number of restart cycles, performs the following:\n\n  1. Builds $k$ steps of GKLB with full reorthogonalization to obtain $U_k$, $V_k$, $B_k$, and the coupling scalar $\\beta_k$ using a unit-norm starting vector orthogonalized against any locked right singular vectors. Reorthogonalize each newly formed candidate vector against all existing basis vectors and all locked vectors using two passes of modified Gram–Schmidt to mitigate loss of orthogonality.\n\n  2. Forms the Singular Value Decomposition $B_k = \\widehat{U}\\,\\Theta\\,\\widehat{V}^\\top$, computes Ritz singular triplets $\\{(\\theta_i,\\widetilde{u}_i,\\widetilde{v}_i)\\}$ with $\\widetilde{u}_i = U_k \\widehat{U} e_i$ and $\\widetilde{v}_i = V_k \\widehat{V} e_i$.\n\n  3. Identifies converged Ritz triplets via the bound\n     $$\n     r_i = \\beta_k \\left| e_k^\\top \\widehat{U} e_i \\right| \\le \\tau \\cdot \\max(1, \\theta_i).\n     $$\n     Any triplet satisfying this criterion is added to the locked sets, provided the newly added vectors remain numerically independent after reorthogonalization against previously locked vectors.\n\n  4. Continues with new GKLB cycles using starting vectors that are orthogonalized against all locked right singular vectors until at least $\\mathrm{nev}$ singular values are locked or the maximum number of cycles is reached.\n\n  5. Returns the set of locked singular values (sorted in nonincreasing order) and the corresponding locked left and right singular vector sets.\n\n- Use the above algorithm to run the following test suite. In all tests, assume real matrices with entries in the International System of Units, but no physical units are required in the final answer. All angles, if any, must be in radians. All numerical outputs must be real numbers. For each test, the answer should be a float or a boolean as specified. For each test case, set $k = 50$, tolerance $\\tau = 10^{-8}$, and cap the maximum number of restart cycles at $50$:\n\n  Test A (generic decaying spectrum):\n  - Dimensions: $m = 120$, $n = 80$.\n  - Construct orthonormal factors $U \\in \\mathbb{R}^{m \\times r}$ and $V \\in \\mathbb{R}^{n \\times r}$ with $r = \\min(m,n)$ by computing the economy-size $QR$ factorizations of dense Gaussian random matrices of sizes $m \\times r$ and $n \\times r$.\n  - Define singular values $s_j = 50 \\cdot \\rho^{j-1}$ for $j = 1,\\dots,r$ with $\\rho = 0.93$.\n  - Form $A = U \\operatorname{diag}(s) V^\\top$.\n  - Target $\\mathrm{nev} = 8$ dominant singular values.\n\n  Test B (repeated leading singular values):\n  - Dimensions: $m = 80$, $n = 80$, $r = 80$.\n  - Construct $U \\in \\mathbb{R}^{m \\times r}$ and $V \\in \\mathbb{R}^{n \\times r}$ as in Test A.\n  - Define singular values $s_1 = s_2 = s_3 = 20$ and $s_j$ linearly decreasing from $12$ to $1$ for $j = 4,\\dots,r$.\n  - Form $A = U \\operatorname{diag}(s) V^\\top$.\n  - Target $\\mathrm{nev} = 3$ dominant singular values.\n\n  Test C (rank-deficient matrix):\n  - Dimensions: $m = 60$, $n = 100$, numerical rank $r = 20$.\n  - Construct $U \\in \\mathbb{R}^{m \\times r}$ and $V \\in \\mathbb{R}^{n \\times r}$ as in Test A, but only of size $r$ columns.\n  - Define singular values $s_j$ linearly decreasing from $5$ to $1$ for $j = 1,\\dots,r$ and set the remaining singular values to $0$.\n  - Form $A = U \\operatorname{diag}(s) V^\\top$.\n  - Target $\\mathrm{nev} = 10$ dominant singular values.\n\nFor each test, compute the ground-truth leading singular values using a dense Singular Value Decomposition of $A$ and compare them to the computed locked singular values. The required outputs for each test are:\n- The maximum absolute error between the computed locked singular values and the ground-truth top $\\mathrm{nev}$ singular values (a float).\n- A boolean indicating whether the locked left singular vectors are numerically orthonormal, i.e., whether $\\| U_{\\mathrm{lock}}^\\top U_{\\mathrm{lock}} - I \\|_2 \\le 10^{-8}$.\n- A boolean indicating whether the locked right singular vectors are numerically orthonormal, i.e., whether $\\| V_{\\mathrm{lock}}^\\top V_{\\mathrm{lock}} - I \\|_2 \\le 10^{-8}$.\n\nFinal output format:\n\n- Your program should produce a single line of output containing a comma-separated list enclosed in square brackets:\n  $$\n  [e_A, oU_A, oV_A, e_B, oU_B, oV_B, e_C, oU_C, oV_C],\n  $$\n  where $e_\\cdot$ are floats and $oU_\\cdot, oV_\\cdot$ are booleans as defined above.", "solution": "The user-provided problem statement has been analyzed and is deemed valid. It is a well-posed problem in numerical linear algebra, describing the implementation of the Golub-Kahan-Lanczos Bidiagonalization (GKLB) algorithm with a hard-locking deflation mechanism for computing dominant singular values and vectors of a real matrix. The description is scientifically sound and provides a clear set of requirements and test cases.\n\nThe solution presented herein implements the specified algorithm. The core of the procedure is an iterative process that builds a bidiagonal matrix whose singular values, known as Ritz values, approximate the singular values of the original matrix. Converged triplets are \"locked\" and removed from the active subspace, allowing the algorithm to efficiently target the remaining unconverged singular values in subsequent iterations.\n\n**1. The Golub-Kahan Lanczos Bidiagonalization (GKLB) Process**\n\nGiven a real matrix $A \\in \\mathbb{R}^{m \\times n}$ and a starting vector $\\boldsymbol{v}_1 \\in \\mathbb{R}^n$ with $\\|\\boldsymbol{v}_1\\|_2 = 1$, the GKLB algorithm iteratively generates two sequences of orthonormal vectors, $\\{\\boldsymbol{u}_j\\}_{j=1}^k \\subset \\mathbb{R}^m$ and $\\{\\boldsymbol{v}_j\\}_{j=1}^{k+1} \\subset \\mathbb{R}^n$, and two sequences of positive scalars, $\\{\\alpha_j\\}_{j=1}^k$ and $\\{\\beta_j\\}_{j=1}^k$. The process is governed by the following recurrences:\n$$\n\\alpha_j \\boldsymbol{u}_j = A \\boldsymbol{v}_j - \\beta_{j-1} \\boldsymbol{u}_{j-1} \\quad (\\text{with } \\beta_0\\boldsymbol{u}_0 \\equiv \\boldsymbol{0})\n$$\n$$\n\\beta_j \\boldsymbol{v}_{j+1} = A^\\top \\boldsymbol{u}_j - \\alpha_j \\boldsymbol{v}_j\n$$\nwhere $\\alpha_j = \\|A \\boldsymbol{v}_j - \\beta_{j-1} \\boldsymbol{u}_{j-1}\\|_2$ and $\\beta_j = \\|A^\\top \\boldsymbol{u}_j - \\alpha_j \\boldsymbol{v}_j\\|_2$. After $k$ steps, these relations can be expressed in matrix form:\n$$\nA V_k = U_k B_k\n$$\n$$\nA^\\top U_k = V_k B_k^\\top + \\beta_k \\boldsymbol{v}_{k+1} \\boldsymbol{e}_k^\\top\n$$\nHere, $U_k = [\\boldsymbol{u}_1, \\dots, \\boldsymbol{u}_k] \\in \\mathbb{R}^{m \\times k}$ and $V_k = [\\boldsymbol{v}_1, \\dots, \\boldsymbol{v}_k] \\in \\mathbb{R}^{n \\times k}$ are matrices with orthonormal columns. $B_k \\in \\mathbb{R}^{k \\times k}$ is an upper bidiagonal matrix with diagonal entries $\\alpha_j$ and superdiagonal entries $\\beta_j$.\n\n**2. Maintaining Orthogonality with Reorthogonalization**\n\nIn finite-precision arithmetic, the vectors generated by the raw Lanczos recurrences quickly lose their mutual orthogonality due to rounding errors. This compromises the accuracy of the method. To ensure stability, we must perform explicit reorthogonalization. As specified, this implementation employs a two-pass modified Gram-Schmidt (MGS) procedure. For each new candidate vector (e.g., $\\boldsymbol{w} = A\\boldsymbol{v}_j$), it is orthogonalized against all previously computed basis vectors (e.g., the columns of $U_{j-1}$) by sequentially removing its projection onto each basis vector. Applying this procedure twice significantly mitigates the loss of orthogonality.\n\n**3. Ritz Pairs, Convergence, and Restarting**\n\nThe singular triplets of the small bidiagonal matrix $B_k$ serve as approximations to the singular triplets of $A$. Let the SVD of $B_k$ be $B_k = \\widehat{U} \\Theta \\widehat{V}^\\top$, where $\\Theta = \\operatorname{diag}(\\theta_1, \\dots, \\theta_k)$. The approximate singular values of $A$, called Ritz values, are the scalars $\\theta_i$. The corresponding approximate singular vectors, or Ritz vectors, are given by $\\widetilde{\\boldsymbol{u}}_i = U_k \\widehat{\\boldsymbol{u}}_i$ and $\\widetilde{\\boldsymbol{v}}_i = V_k \\widehat{\\boldsymbol{v}}_i$.\n\nThe convergence of a Ritz triplet $(\\theta_i, \\widetilde{\\boldsymbol{u}}_i, \\widetilde{\\boldsymbol{v}}_i)$ is assessed using the residual norm, which can be efficiently estimated by the bound $r_i = \\beta_k | \\boldsymbol{e}_k^\\top \\widehat{\\boldsymbol{u}}_i |$. A Ritz triplet is considered converged if this residual bound is smaller than a specified tolerance:\n$$\nr_i \\le \\tau \\cdot \\max(1, \\theta_i)\n$$\nThis hybrid criterion uses an absolute tolerance for small singular values ($\\theta_i < 1$) and a relative tolerance for large ones.\n\nThe overall algorithm operates in cycles. In each cycle, $k$ steps of GKLB are performed. Then, converged Ritz triplets are identified and \"locked\". The process is then restarted, continuing until the desired number of singular values, $\\mathrm{nev}$, have been found.\n\n**4. Deflation via Hard Locking**\n\nDeflation is the process of removing converged eigen- or singular-components from the problem to prevent their repeated discovery and to allow the algorithm to focus on the unconverged parts of the spectrum. The \"hard locking\" strategy implemented here explicitly stores the converged singular vectors in matrices $U_{\\mathrm{lock}}$ and $V_{\\mathrm{lock}}$.\n\nIn all subsequent GKLB cycles, the reorthogonalization step is augmented: each newly generated vector is made orthogonal not only to the current Lanczos basis vectors but also to all vectors in $U_{\\mathrm{lock}}$ or $V_{\\mathrm{lock}}$. The starting vector for each new cycle is also orthogonalized against $V_{\\mathrm{lock}}$. This ensures that the algorithm explores a subspace that is orthogonal to the already-found singular subspaces.\n\nA crucial aspect of this strategy, particularly for clustered or multiple singular values, is ensuring that newly locked vectors are orthonormal. When a Ritz vector converges, it is orthogonalized against the set of previously locked vectors (including those locked within the same cycle) before being added to the locked set. This procedure allows the algorithm to correctly identify an orthonormal basis for the invariant subspace associated with a multiple singular value.", "answer": "```python\nimport numpy as np\nfrom scipy.linalg import qr, svd, norm\n\n# Use a fixed seed for the random number generator to ensure reproducibility.\nRNG = np.random.default_rng(42)\n\ndef mgs_double_pass(w, basis_matrices):\n    \"\"\"\n    Orthogonalizes a vector w against the columns of matrices in basis_matrices\n    using two passes of the modified Gram-Schmidt algorithm.\n\n    Args:\n        w (np.ndarray): The vector to be orthogonalized.\n        basis_matrices (list of np.ndarray): A list of matrices whose columns form the basis.\n\n    Returns:\n        tuple[np.ndarray, float]: A tuple containing the orthogonalized vector and its norm.\n    \"\"\"\n    w_ortho = w.copy()\n    # First pass of MGS\n    for B in basis_matrices:\n        if B is not None and B.shape[1] > 0:\n            for i in range(B.shape[1]):\n                h = np.dot(B[:, i], w_ortho)\n                w_ortho -= h * B[:, i]\n    # Second pass of MGS\n    for B in basis_matrices:\n        if B is not None and B.shape[1] > 0:\n            for i in range(B.shape[1]):\n                h = np.dot(B[:, i], w_ortho)\n                w_ortho -= h * B[:, i]\n    \n    norm_w_ortho = np.linalg.norm(w_ortho)\n    return w_ortho, norm_w_ortho\n\ndef gklb_deflation_svd(A, k, nev, tol, max_cycles):\n    \"\"\"\n    Computes a subset of the largest singular values and corresponding singular vectors\n    of a real matrix A using Golub-Kahan-Lanczos Bidiagonalization (GKLB) with\n    a hard-locking deflation strategy.\n\n    Args:\n        A (np.ndarray): The matrix for which to compute singular values.\n        k (int): The number of Lanczos steps per cycle (basis size).\n        nev (int): The number of singular values to compute.\n        tol (float): The convergence tolerance.\n        max_cycles (int): The maximum number of restart cycles.\n\n    Returns:\n        tuple[np.ndarray, np.ndarray, np.ndarray]: A tuple containing the locked singular\n        values, left singular vectors, and right singular vectors.\n    \"\"\"\n    m, n = A.shape\n    \n    U_locked = np.zeros((m, 0))\n    V_locked = np.zeros((n, 0))\n    S_locked = []\n\n    num_cycles = 0\n    v_start = RNG.random(size=n)\n    \n    while len(S_locked) < nev and num_cycles < max_cycles:\n        num_cycles += 1\n        \n        v1_ortho, v1_norm = mgs_double_pass(v_start, [V_locked])\n        if v1_norm < tol:\n            v_start = RNG.random(size=n)\n            v1_ortho, v1_norm = mgs_double_pass(v_start, [V_locked])\n            if v1_norm < tol:\n                break\n        \n        v1 = v1_ortho / v1_norm\n        \n        V_k = np.zeros((n, k + 1))\n        U_k = np.zeros((m, k))\n        alphas = np.zeros(k)\n        betas = np.zeros(k)\n        \n        V_k[:, 0] = v1\n        k_eff = k\n\n        for j in range(k):\n            u_raw = A @ V_k[:, j]\n            u_ortho, alpha_j = mgs_double_pass(u_raw, [U_locked, U_k[:, :j]])\n            \n            if alpha_j < 1e-12:\n                k_eff = j\n                break\n            \n            alphas[j] = alpha_j\n            U_k[:, j] = u_ortho / alpha_j\n            \n            v_raw = A.T @ U_k[:, j] - alphas[j] * V_k[:, j]\n            v_ortho, beta_j = mgs_double_pass(v_raw, [V_locked, V_k[:, :j+1]])\n\n            betas[j] = beta_j\n            if beta_j < 1e-12:\n                k_eff = j + 1\n                break\n            \n            V_k[:, j+1] = v_ortho / beta_j\n        \n        if k_eff == 0: continue\n\n        B_k = np.diag(alphas[:k_eff]) + np.diag(betas[:k_eff-1], 1)\n        \n        try:\n            U_hat, Theta, Vt_hat = svd(B_k)\n        except np.linalg.LinAlgError:\n            v_start = V_k[:, k_eff]\n            continue\n            \n        V_hat = Vt_hat.T\n        \n        residuals = betas[k_eff-1] * np.abs(U_hat[-1, :])\n        sorted_indices = np.argsort(Theta)[::-1]\n        \n        newly_locked_u_cycle = np.zeros((m, 0))\n        newly_locked_v_cycle = np.zeros((n, 0))\n\n        for i in sorted_indices:\n            theta_i = Theta[i]\n            if residuals[i] <= tol * max(1.0, theta_i):\n                u_ritz = U_k[:, :k_eff] @ U_hat[:, i]\n                v_ritz = V_k[:, :k_eff] @ V_hat[:, i]\n                \n                u_ortho, u_norm = mgs_double_pass(u_ritz, [U_locked, newly_locked_u_cycle])\n                v_ortho, v_norm = mgs_double_pass(v_ritz, [V_locked, newly_locked_v_cycle])\n                \n                if u_norm > tol and v_norm > tol:\n                    u_new = u_ortho / u_norm\n                    v_new = v_ortho / v_norm\n                    \n                    S_locked.append(theta_i)\n                    if newly_locked_u_cycle.shape[1] == 0:\n                        newly_locked_u_cycle = u_new.reshape(-1, 1)\n                        newly_locked_v_cycle = v_new.reshape(-1, 1)\n                    else:\n                        newly_locked_u_cycle = np.hstack([newly_locked_u_cycle, u_new.reshape(-1, 1)])\n                        newly_locked_v_cycle = np.hstack([newly_locked_v_cycle, v_new.reshape(-1, 1)])\n\n                    if len(S_locked) >= nev:\n                        break\n        \n        if newly_locked_u_cycle.shape[1] > 0:\n            U_locked = np.hstack([U_locked, newly_locked_u_cycle])\n            V_locked = np.hstack([V_locked, newly_locked_v_cycle])\n\n        v_start = V_k[:, k_eff]\n    \n    if len(S_locked) > 0:\n        sorted_indices = np.argsort(S_locked)[::-1]\n        S_locked = np.array(S_locked)[sorted_indices]\n        U_locked = U_locked[:, sorted_indices]\n        V_locked = V_locked[:, sorted_indices]\n\n    return S_locked, U_locked, V_locked\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    k = 50\n    tau = 1e-8\n    max_cycles = 50\n    ortho_tol = 1e-8\n    \n    results = []\n\n    # Test Case A\n    m_a, n_a, nev_a = 120, 80, 8\n    r_a = min(m_a, n_a)\n    U_true_a = qr(RNG.standard_normal(size=(m_a, r_a)), mode='economic')[0]\n    V_true_a = qr(RNG.standard_normal(size=(n_a, r_a)), mode='economic')[0]\n    s_true_a = 50.0 * (0.93 ** np.arange(r_a))\n    A_a = U_true_a @ np.diag(s_true_a) @ V_true_a.T\n    S_locked_a, U_locked_a, V_locked_a = gklb_deflation_svd(A_a, k, nev_a, tau, max_cycles)\n    num_found_a = len(S_locked_a)\n    if num_found_a >= nev_a:\n        err_a = np.max(np.abs(S_locked_a[:nev_a] - s_true_a[:nev_a]))\n        oU_a = norm(U_locked_a[:,:nev_a].T @ U_locked_a[:,:nev_a] - np.eye(nev_a), 2) <= ortho_tol\n        oV_a = norm(V_locked_a[:,:nev_a].T @ V_locked_a[:,:nev_a] - np.eye(nev_a), 2) <= ortho_tol\n    else: # Should not happen with these parameters, but as a fallback\n        err_a, oU_a, oV_a = -1.0, False, False\n\n    results.extend([err_a, oU_a, oV_a])\n    \n    # Test Case B\n    m_b, n_b, nev_b = 80, 80, 3\n    r_b = min(m_b, n_b)\n    U_true_b = qr(RNG.standard_normal(size=(m_b, r_b)), mode='economic')[0]\n    V_true_b = qr(RNG.standard_normal(size=(n_b, r_b)), mode='economic')[0]\n    s_true_b = np.concatenate(([20.0, 20.0, 20.0], np.linspace(12, 1, r_b - 3)))\n    A_b = U_true_b @ np.diag(s_true_b) @ V_true_b.T\n    S_locked_b, U_locked_b, V_locked_b = gklb_deflation_svd(A_b, k, nev_b, tau, max_cycles)\n    num_found_b = len(S_locked_b)\n    if num_found_b >= nev_b:\n        err_b = np.max(np.abs(S_locked_b[:nev_b] - s_true_b[:nev_b]))\n        oU_b = norm(U_locked_b[:,:nev_b].T @ U_locked_b[:,:nev_b] - np.eye(nev_b), 2) <= ortho_tol\n        oV_b = norm(V_locked_b[:,:nev_b].T @ V_locked_b[:,:nev_b] - np.eye(nev_b), 2) <= ortho_tol\n    else:\n        err_b, oU_b, oV_b = -1.0, False, False\n\n    results.extend([err_b, oU_b, oV_b])\n    \n    # Test Case C\n    m_c, n_c, nev_c, rank_c = 60, 100, 10, 20\n    U_true_c_rank = qr(RNG.standard_normal(size=(m_c, rank_c)), mode='economic')[0]\n    V_true_c_rank = qr(RNG.standard_normal(size=(n_c, rank_c)), mode='economic')[0]\n    s_true_c = np.linspace(5, 1, rank_c)\n    A_c = U_true_c_rank @ np.diag(s_true_c) @ V_true_c_rank.T\n    S_locked_c, U_locked_c, V_locked_c = gklb_deflation_svd(A_c, k, nev_c, tau, max_cycles)\n    num_found_c = len(S_locked_c)\n    if num_found_c >= nev_c:\n        err_c = np.max(np.abs(S_locked_c[:nev_c] - s_true_c[:nev_c]))\n        oU_c = norm(U_locked_c[:,:nev_c].T @ U_locked_c[:,:nev_c] - np.eye(nev_c), 2) <= ortho_tol\n        oV_c = norm(V_locked_c[:,:nev_c].T @ V_locked_c[:,:nev_c] - np.eye(nev_c), 2) <= ortho_tol\n    else:\n        err_c, oU_c, oV_c = -1.0, False, False\n\n    results.extend([err_c, oU_c, oV_c])\n    \n    # Format output as specified\n    formatted_results = []\n    for r in results:\n        if isinstance(r, bool):\n            formatted_results.append(str(r).lower())\n        else:\n            formatted_results.append(f\"{r:.6e}\") # Using scientific notation for consistency\n\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3539889"}, {"introduction": "While elegant in theory, Lanczos-type methods face a significant practical challenge in finite-precision arithmetic: the gradual loss of orthogonality among the basis vectors. This degradation can lead to the appearance of spurious, or \"ghost,\" Ritz values, which are unwanted copies of already converged singular values. This advanced exercise [@problem_id:3554980] tackles this real-world numerical issue head-on, guiding you to design and implement an algorithm to detect, confirm, and filter these numerical artifacts, thereby ensuring the integrity of the computed singular spectrum.", "problem": "You are given a rectangular real matrix $A \\in \\mathbb{R}^{m \\times n}$ with $m \\geq n$ and asked to study the behavior of the Golub–Kahan Lanczos bidiagonalization process when orthogonality is not enforced among the right Lanczos vectors. You will develop a principled, programmatic failure-detection mechanism for spurious, or \"ghost,\" Ritz singular values that can arise due to loss of orthogonality in the columns of $V_k$. The task is to design an algorithm that (i) detects such failures, (ii) confirms them via an independent duplicate run, and (iii) proposes a filter that removes the detected spurious values, keeping at most one candidate per near-duplicate cluster.\n\nFundamental base to use:\n\n- The singular value decomposition (SVD): for any matrix $A \\in \\mathbb{R}^{m \\times n}$, there exist orthonormal matrices $U \\in \\mathbb{R}^{m \\times m}$, $V \\in \\mathbb{R}^{n \\times n}$, and a nonnegative diagonal matrix $\\Sigma \\in \\mathbb{R}^{m \\times n}$ such that $A = U \\Sigma V^{\\top}$. The singular values of $A$ are the diagonal entries of $\\Sigma$.\n- The Golub–Kahan Lanczos bidiagonalization process: given a nonzero starting vector $b \\in \\mathbb{R}^{m}$ and a target iteration count $k \\in \\mathbb{N}$, it constructs matrices $U_{k+1} \\in \\mathbb{R}^{m \\times (k+1)}$ and $V_k \\in \\mathbb{R}^{n \\times k}$ with orthonormal columns in exact arithmetic, and a lower bidiagonal matrix $B_{\\mathrm{tilde}} \\in \\mathbb{R}^{(k+1) \\times k}$ with diagonal entries $\\alpha_j$ and subdiagonal entries $\\beta_{j+1}$, such that\n$$\nA V_k = U_{k+1} B_{\\mathrm{tilde}}.\n$$\n- Ritz singular values: the singular values of $B_{\\mathrm{tilde}}$ are the projections of the singular values of $A$ onto the constructed subspaces. Their associated approximate singular vectors are $\\widehat{u}_i = U_{k+1} \\widehat{U}_k(:,i)$ and $\\widehat{v}_i = V_k \\widehat{V}_k(:,i)$, where $B_{\\mathrm{tilde}} = \\widehat{U}_k \\widehat{\\Sigma}_k \\widehat{V}_k^{\\top}$ is a singular value decomposition of $B_{\\mathrm{tilde}}$.\n- Predicted residual identity: for an approximate singular triplet $(\\widehat{\\sigma}_i, \\widehat{u}_i, \\widehat{v}_i)$ induced from the SVD of $B_{\\mathrm{tilde}}$, the residual norm for the transposed problem satisfies\n$$\n\\|A^\\top \\widehat{u}_i - \\widehat{\\sigma}_i \\widehat{v}_i\\|_2 = \\beta_{k+1} |[\\widehat{U}_k]_{k+1,i}|,\n$$\nin exact arithmetic.\n\nProblem:\n\n1) Implement the Golub–Kahan Lanczos bidiagonalization for $k$ steps without reorthogonalization. Accept a nonnegative parameter $\\eta_{\\mathrm{noise}}$ to simulate loss of orthogonality by injecting small additive noise into each recurrence before normalization, thereby modeling finite-precision effects. Use the standard three-term recurrences with normalization at each step to produce $U_{k+1}$, $V_k$, and $B_{\\mathrm{tilde}}$.\n\n2) Compute the Ritz singular values $\\{\\widehat{\\sigma}_i\\}_{i=1}^{\\ell}$ from the singular value decomposition of $B_{\\mathrm{tilde}} \\in \\mathbb{R}^{(k+1)\\times k}$, where $\\ell = \\min\\{k+1,k\\} = k$. Form the corresponding approximate singular vectors $\\widehat{u}_i = U_{k+1}\\widehat{U}_k(:,i)$ and $\\widehat{v}_i = V_k \\widehat{V}_k(:,i)$.\n\n3) For each $i \\in \\{1,\\dots,\\ell\\}$, compute both the \"projected-model\" residual predictor\n$$\n\\rho_i = \\beta_{k+1} |[\\widehat{U}_k]_{k+1,i}|\n$$\nand the \"true\" residual norm\n$$\nr_i = \\|A \\widehat{v}_i - \\widehat{\\sigma}_i \\widehat{u}_i\\|_2.\n$$\n\n4) Quantify loss of $V_k$-orthogonality by the Frobenius norm\n$$\n\\delta_V = \\| V_k^{\\top} V_k - I_k \\|_F.\n$$\nWhen $\\delta_V$ is small, $V_k$ is close to orthonormal; when $\\delta_V$ is large, orthogonality is significantly degraded.\n\n5) Develop a ghost-detection mechanism using the following principles:\n- Cluster near-duplicate Ritz singular values by sorting $\\{\\widehat{\\sigma}_i\\}$ and grouping values whose successive gaps are at most a relative threshold $\\tau_{\\mathrm{cluster}} \\cdot \\max\\{1,\\widehat{\\sigma}_i\\}$.\n- For each cluster of size strictly greater than one, mark its extra members (beyond the single best one) as \"suspect\" if both\n  - the $V_k$-orthogonality loss satisfies $\\delta_V > \\tau_{\\mathrm{orth}}$,\n  - and the true residual is inconsistent with the projected-model predictor, for example, if $r_i \\geq \\gamma \\cdot \\rho_i$ and $r_i \\geq \\Gamma \\cdot \\min_{j\\ \\text{in cluster}} r_j$, where $\\gamma$ and $\\Gamma$ are factors greater than $1$.\n- Confirm suspected ghosts by performing a duplicate run with a slightly perturbed starting vector $\\widetilde{b} = \\frac{b + \\varepsilon_{\\mathrm{pert}} \\cdot \\xi}{\\|b + \\varepsilon_{\\mathrm{pert}} \\cdot \\xi\\|_2}$, where $\\xi$ is a random vector with independent and identically distributed standard normal entries, and verifying that near-duplicate clusters of Ritz singular values persist around the same values to within the clustering threshold.\n\n6) Propose an algorithmic filter that removes the detected ghosts by keeping only the member with the smallest true residual within each confirmed cluster and discarding the rest in the output list of Ritz singular values.\n\nYour program must implement the above and return, for each specified test case, the integer count of the number of Ritz singular values removed by your filter from the first run (before perturbation), using the duplicate-run confirmation only as a detection aid but not altering the first-run values except for removal of confirmed ghosts.\n\nTest suite to implement:\n\n- Construct $A_1 \\in \\mathbb{R}^{80 \\times 60}$ as $A_1 = U \\,\\mathrm{diag}(s)\\, V^{\\top}$ where $U \\in \\mathbb{R}^{80 \\times 80}$ and $V \\in \\mathbb{R}^{60 \\times 60}$ are orthonormal matrices drawn from the $Q$ in a $QR$ decomposition of Gaussian random matrices, and $s \\in \\mathbb{R}^{60}$ satisfies $s_j = 10 \\cdot 0.9^{j-1}$ for $j=1,\\dots,60$.\n- Construct $A_2 \\in \\mathbb{R}^{80 \\times 60}$ similarly, but with $s_1 = 5$, $s_2 = 5$, and $s_j = 5 \\cdot 0.85^{j-2}$ for $j=3,\\dots,60$.\n- Use a fixed random seed to make the test deterministic. Let $b \\in \\mathbb{R}^{80}$ be a random vector with independent and identically distributed standard normal entries, normalized.\n\nDefine the following test cases, each identified by a tuple $(A\\_id, k, \\eta\\_{\\mathrm{noise}}, \\varepsilon\\_{\\mathrm{pert}}, \\tau\\_{\\mathrm{cluster}}, \\tau\\_{\\mathrm{orth}})$:\n\n- Case $1$: $(A\\_1, 20, 0, 10^{-8}, 10^{-4}, 10^{-8})$.\n- Case $2$: $(A\\_1, 35, 10^{-2}, 10^{-3}, 10^{-3}, 10^{-3})$.\n- Case $3$: $(A\\_1, 5, 0, 10^{-8}, 10^{-4}, 10^{-8})$.\n- Case $4$: $(A\\_2, 25, 0, 10^{-8}, 10^{-4}, 10^{-8})$.\n\nFor all cases, use the same $b$ and the same random seed for generating all random quantities, and for duplicate runs use the specified $\\varepsilon\\_{\\mathrm{pert}}$. For ghost confirmation, use two residual inconsistency factors $\\gamma = 5$ and $\\Gamma = 10$, and define an absolute residual floor $\\tau_{\\mathrm{abs}} = 100 \\cdot \\epsilon_{\\mathrm{mach}} \\cdot \\|A\\|_2$, where $\\epsilon_{\\mathrm{mach}}$ is double-precision machine epsilon and $\\|A\\|_2$ is the operator two-norm of $A$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each entry is the integer number of removed ghost Ritz singular values for a test case, in the order of the four cases above (for example, \"[0,3,0,0]\"). No extra whitespace is required or expected in the output, and no other text should be printed. No physical units are involved in this problem, and all numeric quantities are dimensionless. Angles are not involved. Percentages are not used; any ratio must be expressed as a decimal value. The program must be a complete, runnable Python script that performs all computations without user input, using only the allowed libraries.", "solution": "The problem requires the development of a principled algorithm to detect and filter spurious \"ghost\" Ritz singular values that arise in the Golub-Kahan-Lanczos bidiagonalization (GKLB) process due to the loss of orthogonality among the computed Lanczos vectors. The solution involves implementing the GKLB algorithm with a simulated loss of orthogonality, computing the Ritz values and their associated residuals, and then applying a multi-stage detection and filtering logic.\n\n**1. The Golub-Kahan Lanczos Bidiagonalization (GKLB) Process and Loss of Orthogonality**\n\nThe GKLB process is an iterative method that, for a given matrix $A \\in \\mathbb{R}^{m \\times n}$ and a starting vector $b \\in \\mathbb{R}^m$, constructs a sequence of orthonormal vectors spanning Krylov subspaces. In exact arithmetic, for $k$ iterations, it produces matrices $U_{k+1} = [u_1, u_2, \\dots, u_{k+1}] \\in \\mathbb{R}^{m \\times (k+1)}$ and $V_k = [v_1, v_2, \\dots, v_k] \\in \\mathbb{R}^{n \\times k}$ with orthonormal columns, and a lower bidiagonal matrix $B_{\\mathrm{tilde}} \\in \\mathbb{R}^{(k+1) \\times k}$ such that the following relations hold:\n$$\nA V_k = U_{k+1} B_{\\mathrm{tilde}}\n$$\n$$\nA^{\\top} U_k = V_k B_{\\mathrm{tilde}}(1:k, :) + \\alpha_{k+1} v_{k+1} e_k^{\\top} \\quad \\text{(if run for k+1 steps for } v)\n$$\nIn finite-precision arithmetic, the orthogonality of the columns of $U_{k+1}$ and, more critically, $V_k$ is progressively lost as the number of iterations $k$ increases. This loss of orthogonality is the root cause of numerical artifacts, such as the appearance of multiple copies of Ritz values that have already converged to an actual singular value of $A$. To model this effect, a noise term, scaled by a parameter $\\eta_{\\mathrm{noise}}$, is introduced into the recurrence relations before the normalization step. Specifically, the unnormalized vectors for the next Lanczos vectors $v_j$ and $u_{j+1}$ are perturbed by additive random noise.\n\nThe algorithm proceeds as follows, starting with $u_1 = b / \\|b\\|_2$ and $\\beta_1=0$, $v_0=0$:\nFor $j = 1, \\dots, k$:\n1. Compute the unnormalized right Lanczos vector: $r_j = A^{\\top} u_j - \\beta_j v_{j-1}$.\n2. Inject noise (if $\\eta_{\\mathrm{noise}} > 0$): $\\tilde{r}_j = r_j + \\text{noise}$.\n3. Normalize to get the diagonal element of $B_{\\mathrm{tilde}}$ and the new right vector: $\\alpha_j = \\|\\tilde{r}_j\\|_2$, $v_j = \\tilde{r}_j / \\alpha_j$.\n4. Compute the unnormalized left Lanczos vector: $p_j = A v_j - \\alpha_j u_j$.\n5. Inject noise (if $\\eta_{\\mathrm{noise}} > 0$): $\\tilde{p}_j = p_j + \\text{noise}$.\n6. Normalize to get the subdiagonal element of $B_{\\mathrm{tilde}}$ and the new left vector: $\\beta_{j+1} = \\|\\tilde{p}_j\\|_2$, $u_{j+1} = \\tilde{p}_j / \\beta_{j+1}$.\n\nThe degree of orthogonality loss in the computed $V_k$ matrix is quantified by the Frobenius norm of the deviation from identity: $\\delta_V = \\| V_k^{\\top} V_k - I_k \\|_F$. A large $\\delta_V$ indicates significant orthogonality loss.\n\n**2. Ritz Singular Values and Residual Analysis**\n\nThe singular values of the bidiagonal matrix $B_{\\mathrm{tilde}}$, denoted $\\{\\widehat{\\sigma}_i\\}_{i=1}^k$, are called Ritz singular values. They serve as approximations to the singular values of the original matrix $A$. Let the SVD of $B_{\\mathrm{tilde}}$ be $B_{\\mathrm{tilde}} = \\widehat{U}_k \\widehat{\\Sigma}_k \\widehat{V}_k^{\\top}$. The corresponding approximate singular vectors for $A$ are $\\widehat{u}_i = U_{k+1} \\widehat{U}_k(:,i)$ and $\\widehat{v}_i = V_k \\widehat{V}_k(:,i)$.\n\nA key tool for assessing the quality of a Ritz pair $(\\widehat{\\sigma}_i, \\widehat{u}_i, \\widehat{v}_i)$ is its residual norm. In the GKLB context, we distinguish between two residuals:\n- The **\"projected-model\" residual predictor**, $\\rho_i = \\beta_{k+1} |[\\widehat{U}_k]_{k+1,i}|$, which is derived from the structure of the bidiagonalization and holds true in exact arithmetic.\n- The **\"true\" residual norm**, $r_i = \\|A \\widehat{v}_i - \\widehat{\\sigma}_i \\widehat{u}_i\\|_2$, which measures the actual error of the approximation.\n\nIn ideal circumstances (exact arithmetic, perfect orthogonality), $\\rho_i = r_i$. A significant discrepancy between them, particularly when $r_i \\gg \\rho_i$, is a strong indicator of numerical instability caused by the loss of orthogonality.\n\n**3. Ghost Detection and Filtering Mechanism**\n\nThe core of the problem is to devise a robust mechanism to identify and filter these spurious Ritz values. The proposed approach is a multi-stage process based on clustering, suspicion, confirmation, and filtering.\n\n- **Step 1: Clustering.** The first sign of ghost values is the emergence of multiple, tightly grouped Ritz values where a single singular value of $A$ should be. We therefore begin by clustering the sorted Ritz values $\\{\\widehat{\\sigma}_i\\}$. Two adjacent Ritz values $\\widehat{\\sigma}_i$ and $\\widehat{\\sigma}_{i+1}$ belong to the same cluster if their relative gap is smaller than a given threshold $\\tau_{\\mathrm{cluster}}$, i.e., $\\widehat{\\sigma}_{i+1} - \\widehat{\\sigma}_i \\leq \\tau_{\\mathrm{cluster}} \\cdot \\max\\{1, \\widehat{\\sigma}_i\\}$.\n\n- **Step 2: Identifying \"Ghost Clusters\".** A cluster of size greater than one does not automatically imply spuriousness; it could represent a true multiple or clustered singular value of $A$. Ghosts are a consequence of orthogonality loss. Therefore, a cluster is flagged as a \"potential ghost cluster\" only if the overall orthogonality loss is significant, i.e., $\\delta_V > \\tau_{\\mathrm{orth}}$.\n\n- **Step 3: Confirmation via a Duplicate Run.** A potential ghost cluster might still be an artifact of a specific starting vector $b$. To confirm that the cluster represents a systematic breakdown of the algorithm, we perform a second, duplicate GKLB run. This run uses a slightly perturbed starting vector $\\widetilde{b} = (b + \\varepsilon_{\\mathrm{pert}} \\cdot \\xi) / \\|b + \\varepsilon_{\\mathrm{pert}} \\cdot \\xi\\|_2$, where $\\xi$ is a random vector. If a cluster of Ritz values reappears at nearly the same location in this second run, it confirms that the phenomenon is a robust feature of the algorithm's failure mode for the given matrix $A$ and iteration count $k$. A cluster from the first run is considered \"confirmed\" if a cluster from the second run has a mean value that is close, within the clustering tolerance.\n\n- **Step 4: Filtering.** Within a confirmed ghost cluster from the first run, we must distinguish the \"best\" approximation from the actual ghosts. The best Ritz pair is the one that minimizes the true residual norm $r_i$. This member is retained. The other members of the cluster are candidates for removal. A member is marked as a ghost and removed if its approximation quality is poor, which is quantified by two conditions:\n    1. Its true residual is significantly larger than its predicted residual: $r_i \\geq \\gamma \\cdot \\rho_i$.\n    2. Its true residual is significantly larger than the best residual in its cluster: $r_i \\geq \\Gamma \\cdot \\min_{j \\text{ in cluster}} r_j$.\nA floor $\\tau_{\\mathrm{abs}} = 100 \\cdot \\epsilon_{\\mathrm{mach}} \\cdot \\|A\\|_2$ is also used for the true residual to avoid flagging minuscule residuals resulting from floating-point noise.\n\nThe final output is the total count of Ritz values from the first run that are identified as ghosts and removed by this procedure.", "answer": "```python\nimport numpy as np\n\ndef gklb(A, b, k, eta_noise, rng):\n    \"\"\"\n    Performs Golub-Kahan-Lanczos bidiagonalization for k steps.\n    \"\"\"\n    m, n = A.shape\n    U = np.zeros((m, k + 1))\n    V = np.zeros((n, k))\n    alphas = np.zeros(k)\n    betas = np.zeros(k + 1)\n\n    u_curr = b / np.linalg.norm(b)\n    U[:, 0] = u_curr\n    v_prev = np.zeros(n)\n\n    for j in range(k):\n        # --- Right-vector recurrence ---\n        r_unnorm = A.T @ u_curr - betas[j] * v_prev\n\n        # Inject noise to simulate finite precision\n        if eta_noise > 0:\n            noise_r = eta_noise * rng.standard_normal(size=r_unnorm.shape)\n            r_unnorm += noise_r\n\n        alpha_j = np.linalg.norm(r_unnorm)\n        alphas[j] = alpha_j\n        \n        if alpha_j == 0:\n            # Breakdown\n            k = j\n            U = U[:, :(k+1)]\n            V = V[:, :k]\n            alphas = alphas[:k]\n            betas = betas[:(k+1)]\n            break\n\n        v_curr = r_unnorm / alpha_j\n        V[:, j] = v_curr\n\n        # --- Left-vector recurrence ---\n        p_unnorm = A @ v_curr - alpha_j * u_curr\n\n        # Inject noise\n        if eta_noise > 0:\n            noise_p = eta_noise * rng.standard_normal(size=p_unnorm.shape)\n            p_unnorm += noise_p\n\n        beta_jp1 = np.linalg.norm(p_unnorm)\n        betas[j + 1] = beta_jp1\n\n        if beta_jp1 == 0:\n            # Breakdown\n            k = j + 1\n            U = U[:, :k]\n            V = V[:, :(k-1)] # This case is subtle\n            alphas = alphas[:(k-1)]\n            betas = betas[:k]\n            break\n\n        u_next = p_unnorm / beta_jp1\n        U[:, j + 1] = u_next\n\n        # Update for next iteration\n        u_curr = u_next\n        v_prev = v_curr\n\n    return U, V, alphas, betas, k\n\ndef analyze_gklb_run(A, b, k, eta_noise, rng):\n    \"\"\"\n    Runs GKLB and computes Ritz values, residuals, and orthogonality loss.\n    \"\"\"\n    m, n = A.shape\n    U, V, alphas, betas, final_k = gklb(A, b, k, eta_noise, rng)\n    \n    if final_k == 0:\n        return [], [], [], 0, []\n\n    k = final_k\n    B_tilde = np.zeros((k + 1, k))\n    np.fill_diagonal(B_tilde, alphas)\n    np.fill_diagonal(B_tilde[1:], betas[1:k+1])\n\n    # SVD of B_tilde to get Ritz values\n    try:\n        U_hat, sigma_hat_vals, Vt_hat = np.linalg.svd(B_tilde, full_matrices=True)\n        V_hat = Vt_hat.T\n    except np.linalg.LinAlgError:\n        return [], [], [], 0, []\n\n    # Adjust if k > rank of B_tilde\n    if len(sigma_hat_vals) < k:\n        k_ritz = len(sigma_hat_vals)\n        U_hat = U_hat[:, :k_ritz]\n        V_hat = V_hat[:, :k_ritz]\n    else:\n        k_ritz = k\n    \n    if k_ritz == 0:\n        return [], [], [], 0, []\n\n    # Compute residuals\n    residuals_true = np.zeros(k_ritz)\n    residuals_pred = np.zeros(k_ritz)\n\n    for i in range(k_ritz):\n        v_ritz = V @ V_hat[:, i]\n        u_ritz = U @ U_hat[:, i]\n        residuals_true[i] = np.linalg.norm(A @ v_ritz - sigma_hat_vals[i] * u_ritz)\n        residuals_pred[i] = betas[k] * np.abs(U_hat[k, i])\n\n    # Orthogonality loss\n    delta_V = np.linalg.norm(V.T @ V - np.eye(k), 'fro')\n\n    return sigma_hat_vals, residuals_true, residuals_pred, delta_V, k\n\n\ndef find_clusters(sigmas, tau_cluster):\n    \"\"\"\n    Groups sigma values into clusters based on a relative gap tolerance.\n    Returns a list of clusters, where each cluster is a list of (value, original_index) tuples.\n    \"\"\"\n    if len(sigmas) == 0:\n        return []\n    \n    sorted_indices = np.argsort(sigmas)\n    sorted_sigmas = sigmas[sorted_indices]\n\n    clusters = []\n    if len(sorted_sigmas) > 0:\n        current_cluster = [(sorted_sigmas[0], sorted_indices[0])]\n        for i in range(1, len(sorted_sigmas)):\n            gap = sorted_sigmas[i] - sorted_sigmas[i-1]\n            threshold = tau_cluster * max(1.0, sorted_sigmas[i-1])\n            if gap <= threshold:\n                current_cluster.append((sorted_sigmas[i], sorted_indices[i]))\n            else:\n                clusters.append(current_cluster)\n                current_cluster = [(sorted_sigmas[i], sorted_indices[i])]\n        clusters.append(current_cluster)\n    return clusters\n\ndef solve():\n    \"\"\"\n    Main solver function.\n    \"\"\"\n    seed = 42\n    rng = np.random.default_rng(seed)\n    \n    # --- Matrix and vector generation ---\n    m, n = 80, 60\n    \n    # A1\n    U1_rand = rng.standard_normal(size=(m, m))\n    U1, _ = np.linalg.qr(U1_rand)\n    V1_rand = rng.standard_normal(size=(n, n))\n    V1, _ = np.linalg.qr(V1_rand)\n    s1 = 10.0 * (0.9 ** np.arange(n))\n    A1 = U1[:, :n] @ np.diag(s1) @ V1.T\n    \n    # A2\n    U2_rand = rng.standard_normal(size=(m, m))\n    U2, _ = np.linalg.qr(U2_rand)\n    V2_rand = rng.standard_normal(size=(n, n))\n    V2, _ = np.linalg.qr(V2_rand)\n    s2 = np.zeros(n)\n    s2[0], s2[1] = 5.0, 5.0\n    s2[2:] = 5.0 * (0.85 ** np.arange(n-2))\n    A2 = U2[:, :n] @ np.diag(s2) @ V2.T\n\n    matrices = {'A_1': A1, 'A_2': A2}\n    \n    b_base = rng.standard_normal(size=m)\n    b_base /= np.linalg.norm(b_base)\n    \n    # --- Test Cases ---\n    test_cases = [\n        ('A_1', 20, 0, 1e-8, 1e-4, 1e-8),\n        ('A_1', 35, 1e-2, 1e-3, 1e-3, 1e-3),\n        ('A_1', 5, 0, 1e-8, 1e-4, 1e-8),\n        ('A_2', 25, 0, 1e-8, 1e-4, 1e-8),\n    ]\n\n    gamma = 5.0\n    Gamma = 10.0\n    eps_mach = np.finfo(float).eps\n\n    results = []\n    \n    for case in test_cases:\n        A_id, k, eta_noise, eps_pert, tau_cluster, tau_orth = case\n        A = matrices[A_id]\n        \n        tau_abs = 100 * eps_mach * np.linalg.norm(A, 2)\n\n        # Run 1 (Main)\n        sigmas1, r1, rho1, delta_V1, k1 = analyze_gklb_run(A, b_base, k, eta_noise, rng)\n        \n        # Run 2 (Perturbed)\n        xi = rng.standard_normal(size=m)\n        b_pert = (b_base + eps_pert * xi) / np.linalg.norm(b_base + eps_pert * xi)\n        sigmas2, _, _, _, _ = analyze_gklb_run(A, b_pert, k, eta_noise, rng)\n\n        if k1 == 0:\n            results.append(0)\n            continue\n\n        # Cluster results\n        clusters1 = find_clusters(sigmas1, tau_cluster)\n        clusters2 = find_clusters(sigmas2, tau_cluster)\n        \n        removed_ghost_count = 0\n        if delta_V1 > tau_orth:\n            # Orthogonality is lost, check for ghost clusters\n            \n            # Identify persistent clusters\n            confirmed_ghost_clusters = []\n            mean_c2s = [np.mean([s for s, _ in c]) for c in clusters2]\n\n            for c1 in clusters1:\n                if len(c1) > 1:\n                    mean_c1 = np.mean([s for s, _ in c1])\n                    is_persistent = False\n                    for mean_c2 in mean_c2s:\n                        if abs(mean_c1 - mean_c2) <= tau_cluster * max(1.0, mean_c1):\n                            is_persistent = True\n                            break\n                    if is_persistent:\n                        confirmed_ghost_clusters.append(c1)\n            \n            # Filter ghosts within confirmed clusters\n            for cluster in confirmed_ghost_clusters:\n                # Find best member (min true residual)\n                min_r_in_cluster = float('inf')\n                best_idx_in_cluster = -1\n                for _, idx in cluster:\n                    if r1[idx] < min_r_in_cluster:\n                        min_r_in_cluster = r1[idx]\n                        best_idx_in_cluster = idx\n\n                # Mark others as ghosts if they meet criteria\n                for _, idx in cluster:\n                    if idx != best_idx_in_cluster:\n                        is_inconsistent = (r1[idx] >= gamma * rho1[idx]) and \\\n                                          (r1[idx] >= Gamma * min_r_in_cluster) and \\\n                                          (r1[idx] > tau_abs)\n                        if is_inconsistent:\n                            removed_ghost_count += 1\n                            \n        results.append(removed_ghost_count)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3554980"}]}