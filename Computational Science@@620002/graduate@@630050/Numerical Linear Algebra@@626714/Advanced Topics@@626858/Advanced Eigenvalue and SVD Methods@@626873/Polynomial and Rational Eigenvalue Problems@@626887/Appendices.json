{"hands_on_practices": [{"introduction": "Rational eigenvalue problems, where the eigenvalue $\\lambda$ appears in the denominator of matrix entries, often seem daunting. However, a powerful technique called realization allows us to convert such problems into an equivalent, and much more familiar, linear generalized eigenvalue problem of the form $Ax = \\lambda Ex$. This exercise provides direct, hands-on practice in this fundamental transformation, starting from the partial fraction expansion of a rational function. By constructing a minimal realization from first principles, you will solidify your understanding of how the poles of the rational matrix function become the eigenvalues of its linear representation [@problem_id:3565395].", "problem": "Let $r(\\lambda) = 1 + \\dfrac{2}{\\lambda - 1} - \\dfrac{1}{\\lambda + 1}$ and let $R(\\lambda) = r(\\lambda) I_{2}$, where $I_{2}$ denotes the $2 \\times 2$ identity matrix. Using only the foundational definition of a rational matrix function realization, namely that a descriptor realization $(A,E,B,C,D)$ with real matrices satisfies\n$$\nR(\\lambda) \\;=\\; D \\;+\\; C\\,(\\lambda E - A)^{-1}\\,B,\n$$\nconstruct an explicit minimal realization $(A,E,B,C,D)$ with $E$ nonsingular such that the finite poles of $R(\\lambda)$ are captured by $(A,E)$ and the direct term equals $I_{2}$. Justify your construction from first principles, including how the residues at the poles determine block sizes and structure.\n\nThen, compute all finite generalized eigenvalues (including algebraic multiplicities) by solving the generalized eigenvalue problem $\\det(A - \\lambda E) = 0$ restricted to the invariant subspace associated with the strictly proper part of $R(\\lambda)$. Present the eigenvalues in a single row matrix in increasing order, repeating values according to their algebraic multiplicity.\n\nYour final answer must be a single row matrix of numbers. No rounding is required.", "solution": "The problem requires the construction of a minimal descriptor realization for a given rational matrix function $R(\\lambda)$ and the subsequent computation of the finite generalized eigenvalues of the resulting system pencil $(A,E)$.\n\n**Step 1: Problem Validation**\n\nThe givens are:\n- $r(\\lambda) = 1 + \\dfrac{2}{\\lambda - 1} - \\dfrac{1}{\\lambda + 1}$\n- $R(\\lambda) = r(\\lambda) I_{2}$, where $I_{2}$ is the $2 \\times 2$ identity matrix.\n- The realization form is a descriptor system: $R(\\lambda) = D + C(\\lambda E - A)^{-1}B$.\n- The required properties of the realization $(A,E,B,C,D)$ are: it must be minimal, $E$ must be nonsingular, and the direct term $D$ must be $I_2$.\n- The task is to construct this realization and then solve the generalized eigenvalue problem $\\det(A - \\lambda E) = 0$ for the finite eigenvalues.\n\nThe problem is scientifically grounded in the fields of linear systems theory and numerical linear algebra. The concept of a minimal state-space or descriptor realization of a rational transfer function is a cornerstone of these fields. The problem is well-posed, objective, and self-contained, with no contradictions or ambiguities. It represents a standard exercise in applying foundational principles. Therefore, the problem is deemed valid.\n\n**Step 2: Decomposition of the Rational Matrix Function**\n\nThe given rational matrix function is $R(\\lambda) = r(\\lambda) I_2$. We substitute the expression for $r(\\lambda)$:\n$$\nR(\\lambda) = \\left(1 + \\dfrac{2}{\\lambda - 1} - \\dfrac{1}{\\lambda + 1}\\right) I_2\n$$\nWe can separate this into a constant term (the value at $\\lambda \\to \\infty$) and a strictly proper part (which vanishes at $\\lambda \\to \\infty$).\n$$\nR(\\lambda) = 1 \\cdot I_2 + \\left(\\dfrac{2}{\\lambda - 1} - \\dfrac{1}{\\lambda + 1}\\right) I_2\n$$\nThe problem specifies that the direct term $D$ must be $I_2$. This aligns with the decomposition, so we identify:\n- The direct term: $D = I_2$.\n- The strictly proper part: $R_{sp}(\\lambda) = \\left(\\dfrac{2}{\\lambda - 1} - \\dfrac{1}{\\lambda + 1}\\right) I_2$.\n\nOur task now is to find a minimal realization $(A,E,B,C)$ with $E$ nonsingular such that $C(\\lambda E - A)^{-1}B = R_{sp}(\\lambda)$.\n\n**Step 3: Construction of the Minimal Realization from First Principles**\n\nThe construction of a minimal realization for $R_{sp}(\\lambda)$ is based on its partial fraction expansion. The function has two simple poles, $p_1 = 1$ and $p_2 = -1$. We can write $R_{sp}(\\lambda)$ as:\n$$\nR_{sp}(\\lambda) = \\frac{1}{\\lambda-1}K_1 + \\frac{1}{\\lambda-p_2}K_2 = \\frac{1}{\\lambda - 1}(2I_2) + \\frac{1}{\\lambda - (-1)}(-I_2)\n$$\nHere, the residue matrices are $K_1 = 2I_2$ and $K_2 = -I_2$.\n\nAccording to realization theory, for a rational function expressed as a sum of terms corresponding to distinct simple poles, a minimal realization can be constructed by assembling the minimal realizations of each individual term. The dimension of the state space of a minimal realization, known as the McMillan degree $\\delta(R)$, is the sum of the local degrees at each pole. For simple poles, the local degree at $p_i$ is given by the rank of the corresponding residue matrix $K_i$.\n\nLet's compute the ranks of the residue matrices:\n- For the pole $p_1 = 1$, the residue is $K_1 = 2I_2 = \\begin{pmatrix} 2 & 0 \\\\ 0 & 2 \\end{pmatrix}$. The rank is $\\text{rank}(K_1) = 2$.\n- For the pole $p_2 = -1$, the residue is $K_2 = -I_2 = \\begin{pmatrix} -1 & 0 \\\\ 0 & -1 \\end{pmatrix}$. The rank is $\\text{rank}(K_2) = 2$.\n\nThe McMillan degree of $R(\\lambda)$ is $\\delta(R) = \\text{rank}(K_1) + \\text{rank}(K_2) = 2 + 2 = 4$. This means a minimal realization will have a state-space dimension of $4$. The matrices $A$ and $E$ will be $4 \\times 4$.\n\nFor each term $\\frac{K_i}{\\lambda-p_i}$, a minimal realization $(A_i, E_i, B_i, C_i)$ with $E_i$ nonsingular can be constructed. We require $C_i(\\lambda E_i - A_i)^{-1}B_i = \\frac{K_i}{\\lambda-p_i}$. A standard choice with $E_i=I_{r_i}$ (where $r_i = \\text{rank}(K_i)$) is to use a full-rank factorization of the residue, $K_i = C_i B_i$, where $C_i$ is $m \\times r_i$ and $B_i$ is $r_i \\times m$. The realization is then given by $A_i=p_i I_{r_i}$, $E_i=I_{r_i}$, and the factors $B_i, C_i$.\n\nFor $p_1 = 1$: $r_1=2$. We factor $K_1 = 2I_2$. A simple factorization is $C_1 = 2I_2$ and $B_1 = I_2$.\n- $A_1 = p_1 I_{r_1} = 1 \\cdot I_2 = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}$\n- $E_1 = I_{r_1} = I_2$\n- $B_1 = I_2 = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}$\n- $C_1 = 2I_2 = \\begin{pmatrix} 2 & 0 \\\\ 0 & 2 \\end{pmatrix}$\n\nFor $p_2 = -1$: $r_2=2$. We factor $K_2 = -I_2$. A simple factorization is $C_2 = -I_2$ and $B_2 = I_2$.\n- $A_2 = p_2 I_{r_2} = -1 \\cdot I_2 = \\begin{pmatrix} -1 & 0 \\\\ 0 & -1 \\end{pmatrix}$\n- $E_2 = I_{r_2} = I_2$\n- $B_2 = I_2 = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}$\n- $C_2 = -I_2 = \\begin{pmatrix} -1 & 0 \\\\ 0 & -1 \\end{pmatrix}$\n\nThe overall realization for the sum $R_{sp}(\\lambda)$ is obtained by assembling these blocks:\n$A = \\begin{pmatrix} A_1 & 0 \\\\ 0 & A_2 \\end{pmatrix} = \\begin{pmatrix} 1 \\cdot I_2 & 0 \\\\ 0 & -1 \\cdot I_2 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & -1 & 0 \\\\ 0 & 0 & 0 & -1 \\end{pmatrix}$\n$E = \\begin{pmatrix} E_1 & 0 \\\\ 0 & E_2 \\end{pmatrix} = \\begin{pmatrix} I_2 & 0 \\\\ 0 & I_2 \\end{pmatrix} = I_4$\n$B = \\begin{pmatrix} B_1 \\\\ B_2 \\end{pmatrix} = \\begin{pmatrix} I_2 \\\\ I_2 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 0 \\\\ 0 & 1 \\end{pmatrix}$\n$C = \\begin{pmatrix} C_1 & C_2 \\end{pmatrix} = \\begin{pmatrix} 2I_2 & -I_2 \\end{pmatrix} = \\begin{pmatrix} 2 & 0 & -1 & 0 \\\\ 0 & 2 & 0 & -1 \\end{pmatrix}$\n\nThis realization is minimal as its dimension ($4$) equals the McMillan degree. It satisfies the condition that $E$ is nonsingular ($E=I_4$). The direct term is $D=I_2$. Thus, we have constructed the required realization $(A,E,B,C,D)$.\n\n**Step 4: Computation of Finite Generalized Eigenvalues**\n\nThe finite generalized eigenvalues of the realization are the solutions $\\lambda$ to the generalized eigenvalue problem $\\det(A - \\lambda E) = 0$. Using the matrices $A$ and $E$ derived above:\n$$\nA - \\lambda E = \\begin{pmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & -1 & 0 \\\\ 0 & 0 & 0 & -1 \\end{pmatrix} - \\lambda \\begin{pmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 1-\\lambda & 0 & 0 & 0 \\\\ 0 & 1-\\lambda & 0 & 0 \\\\ 0 & 0 & -1-\\lambda & 0 \\\\ 0 & 0 & 0 & -1-\\lambda \\end{pmatrix}\n$$\nThe determinant of this diagonal matrix is the product of its diagonal entries:\n$$\n\\det(A - \\lambda E) = (1-\\lambda)(1-\\lambda)(-1-\\lambda)(-1-\\lambda) = (1-\\lambda)^2 (- (1+\\lambda))^2 = (1-\\lambda)^2 (1+\\lambda)^2\n$$\nSetting the determinant to zero gives the characteristic equation $(1-\\lambda)^2 (1+\\lambda)^2 = 0$. The roots of this equation are the eigenvalues:\n- $\\lambda = 1$, with algebraic multiplicity $2$.\n- $\\lambda = -1$, with algebraic multiplicity $2$.\n\nAs expected, the finite eigenvalues of the minimal realization are precisely the poles of the rational function $R(\\lambda)$, and their algebraic multiplicities match the ranks of the corresponding residue matrices.\n\nThe problem asks for the eigenvalues to be presented in a single row matrix in increasing order, repeating values according to their multiplicity. The ordered list of eigenvalues is $-1, -1, 1, 1$.", "answer": "$$\n\\boxed{\\begin{pmatrix} -1 & -1 & 1 & 1 \\end{pmatrix}}\n$$", "id": "3565395"}, {"introduction": "A common challenge in polynomial eigenvalue problems is singularity, which occurs when the leading matrix coefficient (e.g., the $M$ in a QEP) is not invertible. This signals the presence of infinite eigenvalues and can complicate both theoretical analysis and numerical computation. This practice introduces a crucial technique to manage this situation by \"deflating\" the infinite eigenvalues. You will use a rank-revealing decomposition to systematically separate the original problem into a smaller, regular part that governs the finite eigenvalues and a part that accounts for the infinite ones [@problem_id:3565397].", "problem": "Consider the Quadratic Eigenvalue Problem (QEP) defined by the matrix polynomial $P(\\lambda) = \\lambda^{2} M + \\lambda C + K$ with\n$$\nM = \\begin{pmatrix}\n1 & 0 & 0\\\\\n0 & 1 & 0\\\\\n0 & 0 & 0\n\\end{pmatrix}, \\quad\nC = \\begin{pmatrix}\n2 & -1 & 0\\\\\n3 & 4 & 0\\\\\n0 & 0 & 6\n\\end{pmatrix}, \\quad\nK = \\begin{pmatrix}\n3 & 1 & 0\\\\\n0 & 5 & 0\\\\\n0 & 0 & -9\n\\end{pmatrix}.\n$$\nStart from the foundational definitions: an eigenvalue $\\lambda \\in \\mathbb{C}$ (finite or infinite) satisfies $\\det\\!\\big(P(\\lambda)\\big) = 0$, and eigenvalues at infinity are defined via the reversal polynomial $R(\\mu) = \\mu^{2} P(1/\\mu)$. Using only these foundations and well-tested linear algebra facts about orthogonal transformations and rank-revealing decompositions, complete the following tasks.\n\n1. Using a rank-revealing decomposition such as the Singular Value Decomposition (SVD) or a column-pivoted factorization, construct orthogonal matrices $U$ and $V$ and a partition of the transformed coefficients\n$$\n\\widehat{M} = U^{\\top} M V, \\quad \\widehat{C} = U^{\\top} C V, \\quad \\widehat{K} = U^{\\top} K V,\n$$\nthat isolates the zero singular subspace of $M$ so that\n$$\n\\widehat{M} = \\begin{pmatrix} I_{r} & 0\\\\ 0 & 0 \\end{pmatrix}\n$$\nwith $r$ equal to the rank of $M$. Explicitly report $U$, $V$, $r$, and the corresponding block partitions of $\\widehat{C}$ and $\\widehat{K}$.\n\n2. From this decomposition, construct the reduced regular QEP on the finite part,\n$$\nP_{r}(\\lambda) = \\lambda^{2} I_{r} + \\lambda \\widehat{C}_{11} + \\widehat{K}_{11},\n$$\nand explain, from first principles, why the finite eigenvalues of $P(\\lambda)$ coincide with the union of the finite eigenvalues of $P_{r}(\\lambda)$ and those arising from the lower-right reduced scalar or block factor (if any) of strictly lower degree.\n\n3. Confirm explicitly for the given data that exactly one eigenvalue is at infinity and identify the finite factor arising from the lower-right block.\n\n4. Compute the product of all finite eigenvalues of the original QEP $P(\\lambda)$. Express your final answer as an exact simplified fraction. No rounding is required, and no units are involved. Your final answer must be a single real number.", "solution": "The quadratic eigenvalue problem (QEP) is given by $P(\\lambda)v = 0$, where $P(\\lambda) = \\lambda^{2} M + \\lambda C + K$ and $v$ is a nonzero eigenvector. The eigenvalues $\\lambda$ are the roots of the characteristic equation $\\det(P(\\lambda)) = 0$. The matrix $M$ is singular, which indicates the presence of infinite eigenvalues.\n\n**1. Rank-Revealing Decomposition**\n\nThe leading coefficient matrix is\n$$\nM = \\begin{pmatrix}\n1 & 0 & 0\\\\\n0 & 1 & 0\\\\\n0 & 0 & 0\n\\end{pmatrix}.\n$$\nThe rank of $M$ is $r=2$. The matrix $M$ is already in the required diagonal form of a rank-revealing decomposition. We can therefore choose the orthogonal matrices $U$ and $V$ to be the $3 \\times 3$ identity matrix, $I_{3}$.\n$$\nU = V = I_{3} = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}.\n$$\nThe transformed coefficients are:\n$$\n\\widehat{M} = U^{\\top} M V = I_{3} M I_{3} = M = \\begin{pmatrix}\n1 & 0 & 0\\\\\n0 & 1 & 0\\\\\n0 & 0 & 0\n\\end{pmatrix} = \\begin{pmatrix} I_{2} & 0 \\\\ 0 & 0 \\end{pmatrix}.\n$$\n$$\n\\widehat{C} = U^{\\top} C V = I_{3} C I_{3} = C = \\begin{pmatrix}\n2 & -1 & 0\\\\\n3 & 4 & 0\\\\\n0 & 0 & 6\n\\end{pmatrix}.\n$$\n$$\n\\widehat{K} = U^{\\top} K V = I_{3} K I_{3} = K = \\begin{pmatrix}\n3 & 1 & 0\\\\\n0 & 5 & 0\\\\\n0 & 0 & -9\n\\end{pmatrix}.\n$$\nThe rank is $r=2$. We partition $\\widehat{C}$ and $\\widehat{K}$ according to the block structure of $\\widehat{M}$:\n$$\n\\widehat{C} = \\begin{pmatrix} \\widehat{C}_{11} & \\widehat{C}_{12} \\\\ \\widehat{C}_{21} & \\widehat{C}_{22} \\end{pmatrix} = \\begin{pmatrix} \\begin{pmatrix} 2 & -1 \\\\ 3 & 4 \\end{pmatrix} & \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} \\\\ \\begin{pmatrix} 0 & 0 \\end{pmatrix} & 6 \\end{pmatrix}.\n$$\n$$\n\\widehat{K} = \\begin{pmatrix} \\widehat{K}_{11} & \\widehat{K}_{12} \\\\ \\widehat{K}_{21} & \\widehat{K}_{22} \\end{pmatrix} = \\begin{pmatrix} \\begin{pmatrix} 3 & 1 \\\\ 0 & 5 \\end{pmatrix} & \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} \\\\ \\begin{pmatrix} 0 & 0 \\end{pmatrix} & -9 \\end{pmatrix}.\n$$\nSo we have the partitions:\n$\\widehat{C}_{11} = \\begin{pmatrix} 2 & -1 \\\\ 3 & 4 \\end{pmatrix}$, $\\widehat{C}_{12} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$, $\\widehat{C}_{21} = \\begin{pmatrix} 0 & 0 \\end{pmatrix}$, $\\widehat{C}_{22} = (6)$.\n$\\widehat{K}_{11} = \\begin{pmatrix} 3 & 1 \\\\ 0 & 5 \\end{pmatrix}$, $\\widehat{K}_{12} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$, $\\widehat{K}_{21} = \\begin{pmatrix} 0 & 0 \\end{pmatrix}$, $\\widehat{K}_{22} = (-9)$.\n\n**2. Reduced QEP and First-Principles Explanation**\n\nThe reduced regular QEP is formed from the top-left blocks:\n$$\nP_{r}(\\lambda) = \\lambda^{2} I_{r} + \\lambda \\widehat{C}_{11} + \\widehat{K}_{11} = \\lambda^{2} I_{2} + \\lambda \\begin{pmatrix} 2 & -1 \\\\ 3 & 4 \\end{pmatrix} + \\begin{pmatrix} 3 & 1 \\\\ 0 & 5 \\end{pmatrix}.\n$$\nTo explain the origin of the finite eigenvalues, we consider the transformed polynomial $\\widehat{P}(\\lambda) = U^{\\top} P(\\lambda) V = \\lambda^2 \\widehat{M} + \\lambda \\widehat{C} + \\widehat{K}$. Since $U$ and $V$ are orthogonal, $\\det(U^{\\top})\\det(V) = (\\pm 1)(\\pm 1) \\neq 0$. Thus, $\\det(P(\\lambda))=0$ if and only if $\\det(\\widehat{P}(\\lambda))=0$. The eigenvalues are preserved under this transformation.\nSubstituting the block-partitioned matrices into $\\widehat{P}(\\lambda)$:\n$$\n\\widehat{P}(\\lambda) = \\lambda^2 \\begin{pmatrix} I_{r} & 0 \\\\ 0 & 0 \\end{pmatrix} + \\lambda \\begin{pmatrix} \\widehat{C}_{11} & \\widehat{C}_{12} \\\\ \\widehat{C}_{21} & \\widehat{C}_{22} \\end{pmatrix} + \\begin{pmatrix} \\widehat{K}_{11} & \\widehat{K}_{12} \\\\ \\widehat{K}_{21} & \\widehat{K}_{22} \\end{pmatrix} = \\begin{pmatrix} \\lambda^2 I_{r} + \\lambda \\widehat{C}_{11} + \\widehat{K}_{11} & \\lambda \\widehat{C}_{12} + \\widehat{K}_{12} \\\\ \\lambda \\widehat{C}_{21} + \\widehat{K}_{21} & \\lambda \\widehat{C}_{22} + \\widehat{K}_{22} \\end{pmatrix}.\n$$\nIn our specific case, the off-diagonal blocks $\\widehat{C}_{12}, \\widehat{C}_{21}, \\widehat{K}_{12}, \\widehat{K}_{21}$ are all zero. This makes $\\widehat{P}(\\lambda)$ block-diagonal:\n$$\n\\widehat{P}(\\lambda) = \\begin{pmatrix} P_{r}(\\lambda) & 0 \\\\ 0 & \\lambda \\widehat{C}_{22} + \\widehat{K}_{22} \\end{pmatrix}.\n$$\nThe determinant of a block-diagonal matrix is the product of the determinants of its diagonal blocks:\n$$\n\\det(\\widehat{P}(\\lambda)) = \\det(P_{r}(\\lambda)) \\cdot \\det(\\lambda \\widehat{C}_{22} + \\widehat{K}_{22}).\n$$\nTherefore, a finite $\\lambda$ is an eigenvalue of $P(\\lambda)$ if and only if it is a root of $\\det(P_r(\\lambda))=0$ or a root of $\\det(\\lambda \\widehat{C}_{22} + \\widehat{K}_{22})=0$. This shows that the set of finite eigenvalues of $P(\\lambda)$ is the union of the eigenvalues of the reduced QEP $P_{r}(\\lambda)$ and the eigenvalues of the linear pencil $L(\\lambda) = \\lambda \\widehat{C}_{22} + \\widehat{K}_{22}$. $L(\\lambda)$ is the \"lower-right reduced factor\" of strictly lower degree (degree $1$ in this case).\n\n**3. Eigenvalue at Infinity and Finite Factor Identification**\n\nAn eigenvalue is at infinity if $\\mu=0$ is an eigenvalue of the reversal polynomial $R(\\mu) = \\mu^{2} P(1/\\mu) = M + \\mu C + \\mu^{2} K$. This occurs if $\\det(R(0))=0$.\n$$\nR(0) = M + 0 \\cdot C + 0 \\cdot K = M.\n$$\nWe compute $\\det(M) = \\det\\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix} = 0$. This confirms the existence of at least one eigenvalue at infinity.\n\nThe total number of eigenvalues (finite and infinite) of an $n \\times n$ QEP is $2n$. Here $n=3$, so there are $2 \\times 3 = 6$ eigenvalues. The number of finite eigenvalues is equal to the degree of the polynomial $\\det(P(\\lambda))$.\nFrom the factorization in part 2, we have:\n$$\n\\det(P(\\lambda)) = \\det(\\widehat{P}(\\lambda)) = \\det(P_{r}(\\lambda)) \\det(L(\\lambda)).\n$$\nThe degree of $\\det(P_r(\\lambda))$ is $2r = 2(2) = 4$, since $I_r$ is invertible. The degree of $\\det(L(\\lambda))$ is $n-r=3-2=1$, since $\\widehat{C}_{22}=(6)$ is invertible.\nThe total degree of $\\det(P(\\lambda))$ is $4+1 = 5$.\nSince there are $6$ total eigenvalues and $5$ finite eigenvalues, there must be exactly $6 - 5 = 1$ eigenvalue at infinity.\n\nThe finite factor arising from the lower-right block is the linear polynomial $L(\\lambda) = \\lambda \\widehat{C}_{22} + \\widehat{K}_{22}$. Substituting the scalar blocks, we get:\n$$\nL(\\lambda) = \\lambda (6) + (-9) = 6\\lambda - 9.\n$$\n\n**4. Product of Finite Eigenvalues**\n\nThe finite eigenvalues of $P(\\lambda)$ are the roots of $\\det(P(\\lambda))=0$. The product of these roots can be found by taking the product of the roots of each factor from the determinant factorization.\nThe product of the roots of $\\det(L(\\lambda))=6\\lambda - 9 = 0$ is the single root $\\lambda_1 = 9/6 = 3/2$.\n\nThe product of the roots of $\\det(P_r(\\lambda))=0$ can be found from a known result for regular polynomial eigenvalue problems. For a regular QEP $Q(\\lambda) = \\lambda^2 A + \\lambda B + C$, the product of its $2r$ eigenvalues is given by $\\det(C)/\\det(A)$. For $P_r(\\lambda)$, the leading coefficient is $I_r=I_2$ and the constant term is $\\widehat{K}_{11}$. The product of its $4$ eigenvalues is:\n$$\n\\prod_{i=2}^{5} \\lambda_i = \\frac{\\det(\\widehat{K}_{11})}{\\det(I_2)} = \\det(\\widehat{K}_{11}).\n$$\nWe have $\\widehat{K}_{11} = \\begin{pmatrix} 3 & 1 \\\\ 0 & 5 \\end{pmatrix}$, so $\\det(\\widehat{K}_{11}) = (3)(5) - (1)(0) = 15$.\n\nThe product of all finite eigenvalues of the original QEP is the product of the eigenvalues from both factors:\n$$\n\\text{Product} = \\left(\\frac{3}{2}\\right) \\times (15) = \\frac{45}{2}.\n$$\nAs a check, we can fully write out $\\det(P(\\lambda))$:\n$$\n\\det(P_r(\\lambda)) = \\det\\begin{pmatrix} \\lambda^2+2\\lambda+3 & -\\lambda+1 \\\\ 3\\lambda & \\lambda^2+4\\lambda+5 \\end{pmatrix} = (\\lambda^2+2\\lambda+3)(\\lambda^2+4\\lambda+5) - (3\\lambda)(-\\lambda+1) = \\lambda^4 + 6\\lambda^3 + 19\\lambda^2 + 19\\lambda + 15.\n$$\nThe product of roots is $15/1=15$.\n$$\n\\det(P(\\lambda)) = (6\\lambda - 9)(\\lambda^4 + 6\\lambda^3 + 19\\lambda^2 + 19\\lambda + 15) = 6\\lambda^5 + 27\\lambda^4 + \\dots - 135.\n$$\nThe product of the roots of this polynomial is $(-1)^5 \\frac{-135}{6} = \\frac{135}{6} = \\frac{45}{2}$. This confirms the result.\nThe final answer is an exact simplified fraction.", "answer": "$$\n\\boxed{\\frac{45}{2}}\n$$", "id": "3565397"}, {"introduction": "Modern numerical algorithms for nonlinear eigenvalue problems often focus on finding only the eigenvalues located within a specific region of the complex plane. This hands-on coding exercise guides you through the implementation of a sophisticated and powerful method based on contour integration. By numerically approximating integrals of the matrix resolvent along a contour, you can \"count\" the number of eigenvalues inside and then solve a small, projected eigenproblem to find their values. This practice connects deep theoretical concepts from complex analysis with practical, state-of-the-art numerical linear algebra techniques [@problem_id:3565422].", "problem": "Implement a complete program that approximates selected eigenvalues of a quadratic eigenvalue problem (QEP) using contour integral moments computed by trapezoidal quadrature on a circle, followed by solving a small projected generalized eigenproblem. The quadratic eigenvalue problem is defined by a matrix polynomial $P(\\lambda) \\in \\mathbb{C}^{n \\times n}$ of the form $P(\\lambda) = \\lambda^2 M_2 + \\lambda M_1 + M_0$, where $M_2$, $M_1$, and $M_0$ are constant matrices. The target QEP in this task is a diagonal case with $n=3$ constructed so that its exact eigenvalues are known a priori.\n\nYou must start from the following fundamental base:\n- The quadratic eigenvalue problem seeks scalars $\\lambda \\in \\mathbb{C}$ and nonzero vectors $x \\in \\mathbb{C}^n$ such that $P(\\lambda)x = 0$.\n- The spectral projector associated with eigenvalues of $P(\\lambda)$ inside a closed contour $\\Gamma$ is given by the contour integral $S_0 = \\frac{1}{2\\pi i} \\oint_{\\Gamma} P(\\lambda)^{-1} \\, d\\lambda$, provided $P(\\lambda)$ is invertible for all $\\lambda$ on $\\Gamma$.\n- The first moment is $S_1 = \\frac{1}{2\\pi i} \\oint_{\\Gamma} \\lambda P(\\lambda)^{-1} \\, d\\lambda$.\n- If the contour encloses a set of eigenvalues $\\{\\lambda_j\\}_{j=1}^m$ and corresponding eigenprojectors, then $S_0$ has rank $m$ and, on the range of $S_0$, the operator induced by $S_1$ behaves like multiplication by the enclosed eigenvalues. Consequently, approximations to the enclosed eigenvalues can be obtained by projecting $S_1$ onto the range of $S_0$.\n\nYour program must implement the following numerical method:\n1. Build the $3\\times 3$ QEP $P(\\lambda) = \\lambda^2 M_2 + \\lambda M_1 + M_0$ with $M_2 = I_3$ the identity matrix, and diagonal $M_1$ and $M_0$ defined so that $P(\\lambda)$ has exact eigenvalues at the six real scalars $\\{-1, 2, 3, 4, -2, 5\\}$. Concretely, let $(\\alpha_1,\\beta_1)=(-1,2)$, $(\\alpha_2,\\beta_2)=(3,4)$, and $(\\alpha_3,\\beta_3)=(-2,5)$, and define diagonal entries $p_i(\\lambda) = \\lambda^2 - (\\alpha_i+\\beta_i)\\lambda + \\alpha_i \\beta_i$ so that $M_1 = -\\mathrm{diag}(\\alpha_1+\\beta_1,\\alpha_2+\\beta_2,\\alpha_3+\\beta_3)$ and $M_0 = \\mathrm{diag}(\\alpha_1\\beta_1,\\alpha_2\\beta_2,\\alpha_3\\beta_3)$.\n2. For a given circle $\\Gamma$ parameterized by $\\lambda(\\theta) = c + r e^{i\\theta}$ with center $c \\in \\mathbb{C}$ and radius $r>0$, assemble the moment matrices\n   $$C_0 \\approx \\frac{1}{2\\pi i} \\oint_{\\Gamma} P(\\lambda)^{-1} \\, d\\lambda, \\quad C_1 \\approx \\frac{1}{2\\pi i} \\oint_{\\Gamma} \\lambda P(\\lambda)^{-1} \\, d\\lambda,$$\n   using the trapezoidal rule with $N$ equally spaced nodes $\\theta_j = \\frac{2\\pi j}{N}$ for $j=0,1,\\dots,N-1$:\n   $$C_k \\approx \\frac{r}{N} \\sum_{j=0}^{N-1} e^{i\\theta_j} \\lambda(\\theta_j)^k P(\\lambda(\\theta_j))^{-1}, \\quad k \\in \\{0,1\\}.$$\n   This formula follows from $\\frac{1}{2\\pi i} \\oint_{\\Gamma} g(\\lambda)\\, d\\lambda = \\frac{1}{2\\pi} \\int_{0}^{2\\pi} g(\\lambda(\\theta)) i r e^{i\\theta}\\, d\\theta$ and the trapezoidal rule with step $\\Delta\\theta = \\frac{2\\pi}{N}$.\n3. Compute the singular value decomposition $C_0 = U \\Sigma V^*$ and determine the numerical rank $m$ by counting singular values larger than a tolerance $\\tau$ relative to $\\|\\Sigma\\|_{\\infty}$. Form the $m \\times m$ small matrix\n   $$T = U_m^* C_1 V_m \\Sigma_m^{-1},$$\n   where $U_m$, $V_m$, and $\\Sigma_m$ are the leading singular vectors and singular values corresponding to the rank $m$. The eigenvalues of $T$ approximate the eigenvalues of $P(\\lambda)$ inside $\\Gamma$.\n4. Filter the computed eigenvalues by retaining only those within the disk $\\{ \\lambda \\in \\mathbb{C} : |\\lambda - c| \\le r \\}$ to remove potential spurious values. Then compare with the exact eigenvalues enclosed by the same disk. For comparison, compute the symmetric Hausdorff distance between the sets of computed and exact eigenvalues,\n   $$d_H(A,B) = \\max\\left(\\max_{a\\in A}\\min_{b\\in B}|a-b|, \\max_{b\\in B}\\min_{a\\in A}|b-a|\\right),$$\n   where $A$ is the set of computed eigenvalues retained after filtering and $B$ is the set of exact eigenvalues inside the disk.\n5. The final result for each test case must consist of the integer $m_{\\mathrm{comp}}$ equal to the number of retained computed eigenvalues and the floating-point Hausdorff distance $d_H$ between the computed and exact sets.\n\nUse the following test suite, which probes different scenarios:\n- Test case 1 (happy path): $c=2.5$, $r=1.0$, $N=128$.\n- Test case 2 (two close eigenvalues): $c=4.5$, $r=0.8$, $N=128$.\n- Test case 3 (single enclosed eigenvalue): $c=0.0$, $r=1.5$, $N=128$.\n- Test case 4 (no enclosed eigenvalues): $c=10.0$, $r=1.0$, $N=128$.\n\nYour program should produce a single line of output containing the results for the four test cases as a comma-separated list enclosed in square brackets, where each test case result is itself a comma-separated list $[m_{\\mathrm{comp}}, d_H]$. For example, the output format must be exactly\n\"[[m1,d1],[m2,d2],[m3,d3],[m4,d4]]\"\nwith $m_k$ as integers and $d_k$ as floating-point numbers. No physical units are involved, and angles are implicitly in radians through the parameterization $\\theta \\in [0,2\\pi)$.", "solution": "The solution implements the specified contour integral method to find eigenvalues of a quadratic matrix polynomial $P(\\lambda)$ that lie inside a given circular contour $\\Gamma$ in the complex plane.\n\n**1. QEP Definition**\nThe quadratic eigenvalue problem is given by $P(\\lambda)x = (\\lambda^2 M_2 + \\lambda M_1 + M_0)x = 0$. For this specific task, we construct a diagonal QEP of size $n=3$ with known eigenvalues. The eigenvalues are given as three pairs: $(\\alpha_1, \\beta_1) = (-1, 2)$, $(\\alpha_2, \\beta_2) = (3, 4)$, and $(\\alpha_3, \\beta_3) = (-2, 5)$. The diagonal entries of the polynomial $P(\\lambda)$ are defined as $p_i(\\lambda) = (\\lambda - \\alpha_i)(\\lambda - \\beta_i) = \\lambda^2 - (\\alpha_i + \\beta_i)\\lambda + \\alpha_i\\beta_i$.\nComparing this to the general form for a diagonal QEP, $p_i(\\lambda) = \\lambda^2 [M_2]_{ii} + \\lambda [M_1]_{ii} + [M_0]_{ii}$, we can identify the matrices.\n- With $M_2 = I_3$, we have $[M_2]_{ii}=1$.\n- The linear term matrix is $M_1 = \\mathrm{diag}(-(\\alpha_1+\\beta_1), -(\\alpha_2+\\beta_2), -(\\alpha_3+\\beta_3)) = \\mathrm{diag}(-1, -7, -3)$.\n- The constant term matrix is $M_0 = \\mathrm{diag}(\\alpha_1\\beta_1, \\alpha_2\\beta_2, \\alpha_3\\beta_3) = \\mathrm{diag}(-2, 12, -10)$.\nThe complete set of eigenvalues for $P(\\lambda)$ is the union of the roots of its diagonal entries: $\\{-1, 2, 3, 4, -2, 5\\}$.\n\n**2. Contour Integral Method**\nThe core of the method relies on properties of contour integrals of the resolvent $P(\\lambda)^{-1}$. The integral over a closed contour $\\Gamma$ that encloses a set of $m$ eigenvalues (counting multiplicity) but no others,\n$$S_0 = \\frac{1}{2\\pi i} \\oint_{\\Gamma} P(\\lambda)^{-1} \\, d\\lambda,$$\nis a projector onto the invariant subspace associated with the enclosed eigenvalues. The rank of $S_0$ is $m$. The first moment matrix,\n$$S_1 = \\frac{1}{2\\pi i} \\oint_{\\Gamma} \\lambda P(\\lambda)^{-1} \\, d\\lambda,$$\nacts on this invariant subspace. The eigenvalues of the operator pair $(S_1, S_0)$ restricted to this subspace are precisely the eigenvalues enclosed by $\\Gamma$.\n\n**3. Numerical Implementation**\nThe analytical integrals are approximated numerically.\n- **Discretization:** The circular contour $\\lambda(\\theta) = c + r e^{i\\theta}$ is discretized using $N$ equally spaced points $\\theta_j = 2\\pi j / N$ for $j=0, \\dots, N-1$. The integral is approximated using the trapezoidal rule, which exhibits high accuracy for periodic analytic functions. The expression for the approximate moments $C_k$ is derived from the integral definition:\n$$C_k = \\frac{1}{2\\pi i} \\oint_{\\Gamma} \\lambda^k P(\\lambda)^{-1} d\\lambda = \\frac{1}{2\\pi i} \\int_{0}^{2\\pi} \\lambda(\\theta)^k P(\\lambda(\\theta))^{-1} (i r e^{i\\theta}) d\\theta \\approx \\frac{r}{N} \\sum_{j=0}^{N-1} e^{i\\theta_j} \\lambda(\\theta_j)^k P(\\lambda(\\theta_j))^{-1}.$$\nSince $P(\\lambda)$ is diagonal for our problem, its inverse $P(\\lambda)^{-1}$ is also diagonal, with entries $1/p_i(\\lambda)$, simplifying the computation.\n\n- **Subspace and Projection:** The numerical rank $m$ of $S_0$ is estimated from the singular value decomposition (SVD) of its approximation $C_0 = U\\Sigma V^*$. We count the number of singular values $\\sigma_j$ that are significant relative to the largest singular value $\\sigma_1$, i.e., $\\sigma_j > \\tau \\sigma_1$ for a small tolerance $\\tau$ (e.g., $10^{-12}$). This gives the number of eigenvalues inside $\\Gamma$. The leading $m$ singular vectors $U_m$ and $V_m$ form orthonormal bases for the range and co-range of $C_0$.\nThe generalized eigenvalue problem $S_1 y = \\lambda S_0 y$ is projected onto these bases, leading to the small $m \\times m$ generalized eigenproblem $(U_m^* C_1 V_m) z = \\lambda (U_m^* C_0 V_m) z$. Since $U_m^* C_0 V_m \\approx U_m^* (U_m \\Sigma_m V_m^*) V_m = \\Sigma_m$, this simplifies to finding the eigenvalues of the matrix $\\Sigma_m^{-1} (U_m^* C_1 V_m)$. The eigenvalues of $AB$ are identical to the eigenvalues of $BA$, so we can compute the eigenvalues of the matrix specified in the problem, $T = (U_m^* C_1 V_m) \\Sigma_m^{-1}$.\n\n- **Filtering and Comparison:** The eigenvalues of $T$ provide approximations to the eigenvalues of $P(\\lambda)$ inside $\\Gamma$. Due to numerical error, some computed eigenvalues might lie slightly outside the contour disk. These are filtered out. The final set of computed eigenvalues $A$ is compared to the set of exact eigenvalues $B$ known to be inside the disk. The symmetric Hausdorff distance $d_H(A,B) = \\max(\\sup_{a \\in A} \\inf_{b \\in B} |a-b|, \\sup_{b \\in B} \\inf_{a \\in A} |b-a|)$ quantifies the maximum error between the two sets. If both sets are empty, the distance is $0$.\n\nThe program systematically applies these steps to each test case, calculating the number of computed eigenvalues and the Hausdorff distance to the exact set.", "answer": "```python\nimport numpy as np\nfrom scipy.linalg import svd, eigvals\n\ndef solve():\n    \"\"\"\n    Implements the contour integral method for a QEP and runs specified test cases.\n    \"\"\"\n\n    def solve_qep_contour(c, r, N):\n        \"\"\"\n        Solves the QEP for a single contour configuration.\n\n        Args:\n            c (float or complex): Center of the circular contour.\n            r (float): Radius of the circular contour.\n            N (int): Number of quadrature points.\n\n        Returns:\n            list: A list containing [m_comp, d_H], where m_comp is the number\n                  of computed eigenvalues inside the contour and d_H is the\n                  Hausdorff distance to the exact eigenvalues.\n        \"\"\"\n        n = 3  # Size of the matrices\n        exact_eigs = np.array([-1., 2., 3., 4., -2., 5.])\n\n        # 1. Build the diagonal QEP matrices M2, M1, M0\n        alpha_beta_pairs = [(-1, 2), (3, 4), (-2, 5)]\n        m1_diag = -np.array([a + b for a, b in alpha_beta_pairs], dtype=np.float64)\n        m0_diag = np.array([a * b for a, b in alpha_beta_pairs], dtype=np.float64)\n\n        M2 = np.identity(n, dtype=np.float64)\n        M1 = np.diag(m1_diag)\n        M0 = np.diag(m0_diag)\n\n        def P_inv(lam):\n            # Since matrices are diagonal, inversion is entry-wise.\n            # P_ii(lam) = lam^2 * M2_ii + lam * M1_ii + M0_ii\n            p_diag = lam**2 * np.diag(M2) + lam * np.diag(M1) + np.diag(M0)\n            return np.diag(1.0 / p_diag)\n\n        # 2. Assemble moment matrices C0 and C1 using trapezoidal quadrature\n        theta = (2 * np.pi / N) * np.arange(N)\n        lambda_vals = c + r * np.exp(1j * theta)\n\n        C0 = np.zeros((n, n), dtype=np.complex128)\n        C1 = np.zeros((n, n), dtype=np.complex128)\n\n        for j in range(N):\n            lam = lambda_vals[j]\n            P_inv_lam = P_inv(lam)\n            common_term = (r / N) * np.exp(1j * theta[j]) * P_inv_lam\n            C0 += common_term\n            C1 += lam * common_term\n\n        # 3. Use SVD to find subspace and project\n        U, s, Vh = svd(C0)\n\n        # Determine numerical rank m\n        svd_tol = 1e-12\n        if s.size == 0:\n            m = 0\n        else:\n            m = np.sum(s > svd_tol * s[0])\n\n        if m == 0:\n            computed_eigs_filtered = []\n        else:\n            # Truncate SVD components\n            U_m = U[:, :m]\n            s_m = s[:m]\n            Vh_m = Vh[:m, :]\n            V_m = Vh_m.conj().T\n            Sigma_m_inv = np.diag(1 / s_m)\n\n            # Form the small matrix T and find its eigenvalues\n            T = U_m.conj().T @ C1 @ V_m @ Sigma_m_inv\n            computed_eigs = eigvals(T)\n\n            # 4. Filter computed eigenvalues to keep only those inside the disk\n            computed_eigs_filtered = [eig for eig in computed_eigs if np.abs(eig - c) <= r]\n\n        m_comp = len(computed_eigs_filtered)\n\n        # Identify exact eigenvalues inside the disk\n        exact_eigs_inside = [eig for eig in exact_eigs if np.abs(eig - c) <= r]\n\n        # 5. Compute the symmetric Hausdorff distance\n        set_A = np.array(computed_eigs_filtered, dtype=np.complex128)\n        set_B = np.array(exact_eigs_inside, dtype=np.complex128)\n\n        if set_A.size == 0 and set_B.size == 0:\n            d_H = 0.0\n        elif set_A.size == 0 or set_B.size == 0:\n            # If one set is empty and the other isn't, the distance is infinite.\n            # This case is not expected for the given test suite.\n            d_H = np.inf\n        else:\n            dist_A_to_B = np.max([np.min(np.abs(a - set_B)) for a in set_A])\n            dist_B_to_A = np.max([np.min(np.abs(b - set_A)) for b in set_A])\n            d_H = np.max([dist_A_to_B, dist_B_to_A])\n            \n        return [m_comp, d_H]\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (2.5, 1.0, 128),\n        (4.5, 0.8, 128),\n        (0.0, 1.5, 128),\n        (10.0, 1.0, 128),\n    ]\n\n    results = []\n    for case in test_cases:\n        result = solve_qep_contour(case[0], case[1], case[2])\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    formatted_results = [f\"[{r[0]},{r[1]}]\" for r in results]\n    print(f\"[[{','.join(formatted_results)}]]\")\n\nsolve()\n```", "id": "3565422"}]}