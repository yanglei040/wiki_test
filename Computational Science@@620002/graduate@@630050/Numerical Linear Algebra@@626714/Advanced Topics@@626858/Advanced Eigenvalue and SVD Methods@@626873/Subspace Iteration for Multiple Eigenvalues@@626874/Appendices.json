{"hands_on_practices": [{"introduction": "Theoretical convergence rates are essential, but to truly understand subspace iteration, we must be able to quantify its progress. This practice moves from the abstract to the concrete by introducing principal angles as the rigorous way to measure the \"distance\" between the computed subspace and the true invariant subspace. Through a direct calculation based on the Singular Value Decomposition (SVD), you will see firsthand how the angle of misalignment contracts from one iteration to the next, making the concept of subspace convergence tangible [@problem_id:3582669].", "problem": "Consider a real symmetric matrix $A \\in \\mathbb{R}^{4 \\times 4}$ with an invariant subspace of dimension $2$ associated with its two largest eigenvalues. Let $U \\in \\mathbb{R}^{4 \\times 2}$ have orthonormal columns that span this exact invariant subspace, and let $Q_k \\in \\mathbb{R}^{4 \\times 2}$ be the orthonormal subspace iterates produced by a block subspace iteration with a polynomial filter applied once between $Q_0$ and $Q_1$. The columns of $U$, $Q_0$, and $Q_1$ are given in the standard basis of $\\mathbb{R}^{4}$ by\n$$\nU \\;=\\;\n\\begin{pmatrix}\n1 & 0 \\\\\n0 & 1 \\\\\n0 & 0 \\\\\n0 & 0\n\\end{pmatrix},\\qquad\nQ_0 \\;=\\;\n\\begin{pmatrix}\n\\frac{4}{5} & 0 \\\\\n0 & \\frac{12}{13} \\\\\n\\frac{3}{5} & 0 \\\\\n0 & \\frac{5}{13}\n\\end{pmatrix},\\qquad\nQ_1 \\;=\\;\n\\begin{pmatrix}\n\\frac{15}{17} & 0 \\\\\n0 & \\frac{35}{37} \\\\\n\\frac{8}{17} & 0 \\\\\n0 & \\frac{12}{37}\n\\end{pmatrix}.\n$$\nPrincipal angles between two subspaces spanned by the columns of matrices with orthonormal columns are defined via the Singular Value Decomposition (SVD): if $Q, U \\in \\mathbb{R}^{n \\times r}$ have orthonormal columns, then the singular values $\\sigma_i$ of $Q^{\\ast} U$ satisfy $\\sigma_i = \\cos(\\theta_i)$, where $\\theta_i \\in [0,\\frac{\\pi}{2}]$ are the principal angles. Assume all angles are to be interpreted in radians.\n\nUsing only this definition and fundamental properties of the SVD, do the following:\n1. For $k \\in \\{0,1\\}$, compute the largest principal angle $\\theta_{\\max}^{(k)}$ between the subspaces spanned by the columns of $Q_k$ and $U$.\n2. Define the subspace-iteration contraction factor\n$$\n\\rho \\;=\\; \\frac{\\tan\\big(\\theta_{\\max}^{(1)}\\big)}{\\tan\\big(\\theta_{\\max}^{(0)}\\big)}.\n$$\nReport the exact value of $\\rho$ as a simplified expression. No numerical rounding is required. The final answer must be a single real-valued number or a single closed-form analytic expression without units.", "solution": "We begin from the core definition of principal angles between two subspaces. If $Q, U \\in \\mathbb{R}^{n \\times r}$ have orthonormal columns, then the singular values $\\sigma_i$ of $Q^{\\ast} U \\in \\mathbb{R}^{r \\times r}$ equal $\\cos(\\theta_i)$, where $\\theta_i \\in [0,\\frac{\\pi}{2}]$ are the principal angles between $\\mathrm{range}(Q)$ and $\\mathrm{range}(U)$. The largest principal angle corresponds to the smallest singular value of $Q^{\\ast} U$.\n\nWe apply this to $Q_0$ and $U$. Compute $Q_0^{\\ast} U$:\n$$\nQ_0^{\\ast} U \\;=\\;\n\\begin{pmatrix}\n\\frac{4}{5} & 0 & \\frac{3}{5} & 0 \\\\\n0 & \\frac{12}{13} & 0 & \\frac{5}{13}\n\\end{pmatrix}\n\\begin{pmatrix}\n1 & 0 \\\\\n0 & 1 \\\\\n0 & 0 \\\\\n0 & 0\n\\end{pmatrix}\n\\;=\\;\n\\begin{pmatrix}\n\\frac{4}{5} & 0 \\\\\n0 & \\frac{12}{13}\n\\end{pmatrix}.\n$$\nThis is already diagonal with nonnegative diagonal entries, so its singular values are\n$$\n\\sigma_1^{(0)} \\;=\\; \\max\\!\\left\\{\\frac{4}{5}, \\frac{12}{13}\\right\\} \\;=\\; \\frac{12}{13},\\qquad\n\\sigma_2^{(0)} \\;=\\; \\min\\!\\left\\{\\frac{4}{5}, \\frac{12}{13}\\right\\} \\;=\\; \\frac{4}{5}.\n$$\nTherefore, the principal angles satisfy\n$$\n\\cos\\!\\big(\\theta_1^{(0)}\\big) \\;=\\; \\frac{12}{13},\\qquad\n\\cos\\!\\big(\\theta_2^{(0)}\\big) \\;=\\; \\frac{4}{5}.\n$$\nThe largest principal angle is $\\theta_{\\max}^{(0)} = \\max\\{\\theta_1^{(0)}, \\theta_2^{(0)}\\}$. Since $\\cos$ is decreasing on $[0,\\frac{\\pi}{2}]$, the largest angle corresponds to the smallest cosine, namely $\\frac{4}{5}$. Hence\n$$\n\\theta_{\\max}^{(0)} \\;=\\; \\arccos\\!\\left(\\frac{4}{5}\\right).\n$$\nTo prepare for the contraction factor, we compute $\\tan\\!\\big(\\theta_{\\max}^{(0)}\\big)$. Using the Pythagorean triple, $\\sin\\!\\big(\\arccos(\\frac{4}{5})\\big) = \\frac{3}{5}$, thus\n$$\n\\tan\\!\\big(\\theta_{\\max}^{(0)}\\big) \\;=\\; \\frac{\\sin\\!\\big(\\theta_{\\max}^{(0)}\\big)}{\\cos\\!\\big(\\theta_{\\max}^{(0)}\\big)} \\;=\\; \\frac{\\frac{3}{5}}{\\frac{4}{5}} \\;=\\; \\frac{3}{4}.\n$$\n\nNext, we perform the same steps for $Q_1$:\n$$\nQ_1^{\\ast} U \\;=\\;\n\\begin{pmatrix}\n\\frac{15}{17} & 0 & \\frac{8}{17} & 0 \\\\\n0 & \\frac{35}{37} & 0 & \\frac{12}{37}\n\\end{pmatrix}\n\\begin{pmatrix}\n1 & 0 \\\\\n0 & 1 \\\\\n0 & 0 \\\\\n0 & 0\n\\end{pmatrix}\n\\;=\\;\n\\begin{pmatrix}\n\\frac{15}{17} & 0 \\\\\n0 & \\frac{35}{37}\n\\end{pmatrix}.\n$$\nAgain, this is diagonal with singular values\n$$\n\\sigma_1^{(1)} \\;=\\; \\max\\!\\left\\{\\frac{15}{17}, \\frac{35}{37}\\right\\} \\;=\\; \\frac{35}{37},\\qquad\n\\sigma_2^{(1)} \\;=\\; \\min\\!\\left\\{\\frac{15}{17}, \\frac{35}{37}\\right\\} \\;=\\; \\frac{15}{17}.\n$$\nThus\n$$\n\\cos\\!\\big(\\theta_1^{(1)}\\big) \\;=\\; \\frac{35}{37},\\qquad\n\\cos\\!\\big(\\theta_2^{(1)}\\big) \\;=\\; \\frac{15}{17},\\qquad\n\\theta_{\\max}^{(1)} \\;=\\; \\arccos\\!\\left(\\frac{15}{17}\\right).\n$$\nWe compute $\\tan\\!\\big(\\theta_{\\max}^{(1)}\\big)$. Using the Pythagorean triple, $\\sin\\!\\big(\\arccos(\\frac{15}{17})\\big) = \\frac{8}{17}$, so\n$$\n\\tan\\!\\big(\\theta_{\\max}^{(1)}\\big) \\;=\\; \\frac{\\frac{8}{17}}{\\frac{15}{17}} \\;=\\; \\frac{8}{15}.\n$$\n\nThe contraction factor is defined as\n$$\n\\rho \\;=\\; \\frac{\\tan\\!\\big(\\theta_{\\max}^{(1)}\\big)}{\\tan\\!\\big(\\theta_{\\max}^{(0)}\\big)} \\;=\\; \\frac{\\frac{8}{15}}{\\frac{3}{4}} \\;=\\; \\frac{8}{15} \\cdot \\frac{4}{3} \\;=\\; \\frac{32}{45}.\n$$\n\nInterpretation: In subspace iteration for multiple eigenvalues, the principal angles to the target invariant subspace contract from iteration to iteration, with a rate governed asymptotically by spectral gap information. The computed value $\\rho = \\frac{32}{45}$ provides a quantitative measure of this contraction for the worst (largest) principal angle between the first two iterates, indicating convergence since $\\rho \\in (0,1)$.", "answer": "$$\\boxed{\\frac{32}{45}}$$", "id": "3582669"}, {"introduction": "The simple subspace iteration is a powerful tool, but its effectiveness hinges on a crucial condition: a clear separation in the magnitudes of the eigenvalues. This exercise presents a thought experiment on the method's primary failure mode, where the iteration stalls because this separation vanishes. By analyzing this worst-case scenario, you will understand why advanced methods are not just optional improvements but necessary solutions, motivating the use of spectral transformations like polynomial or rational filters to restore convergence [@problem_id:3582701].", "problem": "Consider the block subspace iteration for a diagonalizable matrix $A \\in \\mathbb{C}^{n \\times n}$ that seeks an $s$-dimensional invariant subspace using a block of size $m$, with $m = s$. Let the iteration be defined by $Y_{k+1} = \\mathrm{orth}(A Y_k)$, where $\\mathrm{orth}(\\cdot)$ denotes orthonormalization via the $QR$ (Orthogonal-Triangular) factorization, and $Y_k \\in \\mathbb{C}^{n \\times m}$ has orthonormal columns spanning the current iterate subspace. Assume $Y_0$ has a nonzero component in the desired invariant subspace. Let the eigen-decomposition be $A = X \\Lambda X^{-1}$ with $\\Lambda = \\mathrm{diag}(\\lambda_1,\\dots,\\lambda_n)$ and $X$ the eigenvector matrix. For a normal $A$ with $X$ unitary, the principal angles between the iterate subspace and any fixed invariant subspace evolve under the action of $A$ according to how the magnitudes $\\{|\\lambda_i|\\}$ scale the corresponding components, followed by orthonormalization.\n\nYou are asked to construct a worst-case spectrum where $(i)$ the multiplicity $s$ equals the block size $m$ and $(ii)$ the relevant eigenvalues have equal magnitude, and to explain why, in this setting, convergence is slow or completely stalled and driven only by mixing due to orthonormalization. Then, identify methods that fundamentally change the spectrum seen by the iteration to restore separation and accelerate convergence.\n\nSelect all statements below that are correct in this sense (each statement may include a construction, an explanation of the slow convergence mechanism, and/or a remedy):\n\nA. Take $A$ unitary with $A = Q \\Lambda Q^{*}$ and $\\Lambda = \\mathrm{diag}(\\lambda_1,\\dots,\\lambda_n)$ satisfying $|\\lambda_i| = 1$ for all $i \\in \\{1,\\dots,n\\}$. With $m = s$, targeting any $s$-dimensional invariant subspace by magnitude is ill-posed because $|\\lambda_s| = |\\lambda_{s+1}|$, yielding no spectral contraction. The subspace iteration $Y_{k+1} = \\mathrm{orth}(A Y_k)$ exhibits only rotation and mixing due to orthonormalization, without decay of the components outside the target subspace. A suitable remedy is to transform the problem by applying a rational or polynomial filter, for example replacing $A$ by $(A - \\sigma I)^{-1}$ for an appropriate shift $\\sigma \\in \\mathbb{C}$ so that eigenvalues near $\\sigma$ become dominant in magnitude, or by using a polynomial $p(A)$ for which $|p(\\lambda)|$ is strictly larger on the desired cluster than elsewhere.\n\nB. Take $A = \\mathrm{diag}(1,\\dots,1, 1 - \\varepsilon, \\dots)$ with $s$ copies of $1$ and the remaining eigenvalues equal to $1 - \\varepsilon$ for a small $\\varepsilon > 0$, and run subspace iteration with $m = s$. Convergence cannot occur because orthonormalization removes the dominant components; to remedy this, one should disable orthonormalization entirely.\n\nC. In the equal-magnitude case $|\\lambda_1| = \\dots = |\\lambda_n|$, simply increasing the block size from $m = s$ to $m = s + 1$ guarantees linear convergence at a rate strictly less than $1$, because the block size then exceeds the multiplicity.\n\nD. For Hermitian $A$ with spectrum $\\{-1,1\\}$ and $m = s$ targeting the $+1$ invariant subspace, the block power iteration cannot separate $\\{-1\\}$ from $\\{+1\\}$ by magnitude, because $|-1| = |+1|$. Applying a simple polynomial filter $p(t) = t + 1$ yields $p(A)$ with eigenvalues $\\{0,2\\}$, thereby introducing a strict spectral gap and restoring fast convergence of subspace iteration to the $+1$ eigenspace.\n\nE. When $|\\lambda_s| = |\\lambda_{s+1}|$, Ritz extraction (projection onto the iterate subspace followed by solving the reduced eigenproblem) will still converge to the desired invariant subspace at a linear rate strictly less than $1$ provided full reorthogonalization is used at each step, even without modifying the spectrum.", "solution": "The problem statement asks for an analysis of a worst-case scenario for the block subspace iteration method, specifically when the magnitudes of the target and non-target eigenvalues are equal, leading to a stall in convergence. It also asks for the identification of methods that can remedy this situation.\n\nThe problem statement is first validated.\n\n### Step 1: Extract Givens\n- **Method**: Block subspace iteration for a diagonalizable matrix $A \\in \\mathbb{C}^{n \\times n}$.\n- **Goal**: Find an $s$-dimensional invariant subspace.\n- **Block Size**: $m = s$.\n- **Iteration Formula**: $Y_{k+1} = \\mathrm{orth}(A Y_k)$, where $Y_k \\in \\mathbb{C}^{n \\times m}$ has orthonormal columns, and $\\mathrm{orth}(\\cdot)$ denotes orthonormalization via QR factorization.\n- **Initial Condition**: $Y_0$ has a nonzero component in the desired invariant subspace.\n- **Eigen-decomposition**: $A = X \\Lambda X^{-1}$, with $\\Lambda = \\mathrm{diag}(\\lambda_1, \\dots, \\lambda_n)$.\n- **Convergence Principle**: For a normal matrix $A$, convergence is driven by the scaling of components by eigenvalue magnitudes $|\\lambda_i|$.\n- **Task**: Construct a worst-case spectrum where (i) multiplicity $s$ equals block size $m$ and (ii) relevant eigenvalues have equal magnitude. Explain the resulting slow/stalled convergence and identify remedies.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientific Grounding**: The problem is firmly rooted in numerical linear algebra. The description of subspace iteration, its reliance on the power method, its convergence properties based on spectral separation, and the concept of spectral transformation are all standard and well-established principles.\n- **Well-Posedness**: The problem is well-posed. It describes a specific, known failure mode of a standard algorithm and asks for an explanation and discussion of corrective measures. This is a standard conceptual question in the field.\n- **Objectivity**: The language is technical, precise, and free of ambiguity or subjective claims.\n\nThe problem statement is scientifically sound, well-posed, and objective. There are no contradictions, missing information, or other flaws.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A solution will be derived.\n\n### Principle-Based Derivation\nThe convergence of the simple subspace iteration, $Y_{k+1} = \\mathrm{orth}(A Y_k)$, is an extension of the simple power method. The method seeks to find the invariant subspace corresponding to the $m$ eigenvalues of $A$ with the largest magnitudes. Let the eigenvalues be ordered such that $|\\lambda_1| \\ge |\\lambda_2| \\ge \\dots \\ge |\\lambda_s| \\ge |\\lambda_{s+1}| \\ge \\dots \\ge |\\lambda_n|$. The subspace spanned by the columns of $Y_k$, denoted $\\mathcal{S}_k = \\mathrm{span}(Y_k)$, converges to the invariant subspace spanned by the eigenvectors corresponding to $\\{\\lambda_1, \\dots, \\lambda_s\\}$ (assuming $m=s$).\n\nThe rate of convergence is determined by the separation of the magnitudes of the eigenvalues inside the desired set from those outside. Specifically, the convergence of the angle between $\\mathcal{S}_k$ and the target invariant subspace is governed by the ratio $|\\lambda_{s+1}|/|\\lambda_s|$. If $|\\lambda_s| > |\\lambda_{s+1}|$, the components of the basis vectors in the direction of the \"unwanted\" eigenvectors (corresponding to $\\lambda_{s+1}, \\dots, \\lambda_n$) are damped out at each step relative to the \"wanted\" components.\n\nThe worst-case scenario for convergence occurs when this ratio is close or equal to $1$, i.e., $|\\lambda_s| \\approx |\\lambda_{s+1}|$ or $|\\lambda_s| = |\\lambda_{s+1}|$. In the latter case, the power method step $Z_{k+1} = AY_k$ fails to preferentially amplify the desired components of the subspace. The iteration does not systematically reduce the angle to the target invariant subspace. The orthonormalization step, $Y_{k+1} = \\mathrm{orth}(Z_{k+1})$, is merely a stabilization procedure to prevent the basis vectors from becoming collinear and losing numerical rank; it does not in itself drive convergence toward the target subspace in this stalled scenario.\n\nTo restore convergence, one must apply a spectral transformation. This involves replacing a slow-converging problem for $A$ with a fast-converging problem for a related matrix $f(A)$, where $f$ is a function chosen such that the eigenvalues of $f(A)$, which are $f(\\lambda_i)$, have a favorable separation. The goal is to choose $f$ such that $|f(\\lambda_{s+1})| / |f(\\lambda_s)| \\ll 1$. Common choices for $f$ are rational functions (as in shift-and-invert methods) or polynomials (as in Chebyshev iteration).\n\n### Option-by-Option Analysis\n\n**A. Take $A$ unitary with $A = Q \\Lambda Q^{*}$ and $\\Lambda = \\mathrm{diag}(\\lambda_1,\\dots,\\lambda_n)$ satisfying $|\\lambda_i| = 1$ for all $i \\in \\{1,\\dots,n\\}$. With $m = s$, targeting any $s$-dimensional invariant subspace by magnitude is ill-posed because $|\\lambda_s| = |\\lambda_{s+1}|$, yielding no spectral contraction. The subspace iteration $Y_{k+1} = \\mathrm{orth}(A Y_k)$ exhibits only rotation and mixing due to orthonormalization, without decay of the components outside the target subspace. A suitable remedy is to transform the problem by applying a rational or polynomial filter, for example replacing $A$ by $(A - \\sigma I)^{-1}$ for an appropriate shift $\\sigma \\in \\mathbb{C}$ so that eigenvalues near $\\sigma$ become dominant in magnitude, or by using a polynomial $p(A)$ for which $|p(\\lambda)|$ is strictly larger on the desired cluster than elsewhere.**\n\n- **Analysis**: A unitary matrix is a perfect example of the worst-case scenario, as all its eigenvalues lie on the unit circle in the complex plane, so $|\\lambda_i|=1$ for all $i$. Consequently, for any $s < n$, we have $|\\lambda_s| = |\\lambda_{s+1}| = 1$. The ratio governing convergence is $|\\lambda_{s+1}|/|\\lambda_s| = 1$, so the simple subspace iteration stalls. The explanation that the iteration devolves into rotation (by the unitary operator $A$) and re-orthogonalization is physically and mathematically correct. The proposed remedies, shift-and-invert with $(A - \\sigma I)^{-1}$ and polynomial filtering with $p(A)$, are the standard, effective techniques to create a spectral gap and accelerate convergence. If $\\sigma$ is chosen close to a desired eigenvalue $\\lambda_j$, then $|(\\lambda_j - \\sigma)^{-1}|$ becomes very large, separating it from other transformed eigenvalues. Similarly, a polynomial can be designed to amplify a specific region of the spectrum.\n- **Verdict**: **Correct**.\n\n**B. Take $A = \\mathrm{diag}(1,\\dots,1, 1 - \\varepsilon, \\dots)$ with $s$ copies of $1$ and the remaining eigenvalues equal to $1 - \\varepsilon$ for a small $\\varepsilon > 0$, and run subspace iteration with $m = s$. Convergence cannot occur because orthonormalization removes the dominant components; to remedy this, one should disable orthonormalization entirely.**\n\n- **Analysis**: In this case, $\\lambda_s = 1$ and $\\lambda_{s+1} = 1 - \\varepsilon$. The convergence ratio is $|\\lambda_{s+1}|/|\\lambda_s| = 1-\\varepsilon$. Since $\\varepsilon > 0$, this ratio is strictly less than $1$. Convergence will occur, albeit slowly if $\\varepsilon$ is small. The claim that \"convergence cannot occur\" is false. The claim that \"orthonormalization removes the dominant components\" is nonsensical; orthonormalization is what maintains the linear independence of the basis vectors, which is essential for representing an $s$-dimensional subspace. The proposed remedy to \"disable orthonormalization entirely\" is fatally flawed. Without orthonormalization, the iteration becomes the simultaneous power method, and all column vectors would quickly converge to the direction of the dominant eigenvector (of eigenvalue $1$), causing the basis to become rank-deficient and failing to capture the full $s$-dimensional subspace.\n- **Verdict**: **Incorrect**.\n\n**C. In the equal-magnitude case $|\\lambda_1| = \\dots = |\\lambda_n|$, simply increasing the block size from $m = s$ to $m = s + 1$ guarantees linear convergence at a rate strictly less than $1$, because the block size then exceeds the multiplicity.**\n\n- **Analysis**: The statement proposes to target an $s$-dimensional subspace by using a block of size $m = s+1$. The convergence of this $(s+1)$-dimensional subspace iteration is governed by the ratio $|\\lambda_{m+1}|/|\\lambda_m| = |\\lambda_{s+2}|/|\\lambda_{s+1}|$. In the \"equal-magnitude case\" where $|\\lambda_i|$ is constant for all $i$, this ratio is also $1$. The iteration for the $(s+1)$-dimensional subspace will stall for the exact same reason the iteration for the $s$-dimensional subspace did. Increasing the block size is a strategy used to handle eigenvalue clusters that are close, but not identical, in magnitude. It does not resolve the fundamental problem when magnitudes are exactly equal. The claim that this *guarantees* convergence is false.\n- **Verdict**: **Incorrect**.\n\n**D. For Hermitian $A$ with spectrum $\\{-1,1\\}$ and $m = s$ targeting the $+1$ invariant subspace, the block power iteration cannot separate $\\{-1\\}$ from $\\{+1\\}$ by magnitude, because $|-1| = |+1|$. Applying a simple polynomial filter $p(t) = t + 1$ yields $p(A)$ with eigenvalues $\\{0,2\\}$, thereby introducing a strict spectral gap and restoring fast convergence of subspace iteration to the $+1$ eigenspace.**\n\n- **Analysis**: This is a specific, well-constructed example. The matrix $A$ has eigenvalues $\\pm 1$. The target eigenvalue is $\\lambda_s = 1$, and the next-largest in magnitude is $\\lambda_{s+1} = -1$. The ratio of magnitudes is $|-1|/|1| = 1$, so simple subspace iteration stalls, as correctly stated. The proposed remedy is a polynomial filter, $p(t) = t+1$. The new matrix is $p(A) = A+I$. Its eigenvalues are $\\{ p(-1), p(1) \\} = \\{-1+1, 1+1\\} = \\{0, 2\\}$. The target $+1$ eigenspace of $A$ is now the $2$ eigenspace of $A+I$. The unwanted $-1$ eigenspace of $A$ is now the $0$ eigenspace of $A+I$. The new ratio governing convergence to the target subspace is $|0|/|2| = 0$. A convergence ratio of $0$ implies convergence in a single iteration in exact arithmetic. This is the definition of \"fast convergence\". The statement is entirely correct.\n- **Verdict**: **Correct**.\n\n**E. When $|\\lambda_s| = |\\lambda_{s+1}|$, Ritz extraction (projection onto the iterate subspace followed by solving the reduced eigenproblem) will still converge to the desired invariant subspace at a linear rate strictly less than $1$ provided full reorthogonalization is used at each step, even without modifying the spectrum.**\n\n- **Analysis**: Ritz extraction is the process of finding the best possible approximations to eigenpairs from a given subspace. In subspace iteration, we compute Ritz pairs from the current subspace $\\mathcal{S}_k = \\mathrm{span}(Y_k)$. The convergence of these Ritz pairs to the true eigenpairs of $A$ is dependent on the convergence of the subspace $\\mathcal{S}_k$ to the true invariant subspace. If the underlying subspace iteration $Y_{k+1} = \\mathrm{orth}(AY_k)$ is stalled because $|\\lambda_s| = |\\lambda_{s+1}|$, the subspace $\\mathcal{S}_k$ does not improve. Ritz extraction can only provide the best answer *within the current, non-improving subspace*. It is not a mechanism for accelerating the convergence of the subspace itself. The claim that it will converge at a linear rate is false; if the subspace is not converging, the Ritz pairs will not converge either. Full reorthogonalization is for numerical stability and does not alter this fundamental convergence theory.\n- **Verdict**: **Incorrect**.", "answer": "$$\\boxed{AD}$$", "id": "3582701"}, {"introduction": "In practical applications, we often need to find more eigenpairs after an initial set has converged. The process of removing known solutions to find new ones is called deflation, but it is fraught with numerical peril. This hands-on coding challenge tasks you with implementing and comparing a naive deflation strategy against a numerically robust one, revealing how improper orthogonalization can create spurious \"ghost\" Ritz values that contaminate the results. This exercise highlights the critical difference between a theoretically sound algorithm and its stable, reliable implementation in finite-precision arithmetic [@problem_id:3582676].", "problem": "Consider a real symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$ with disjoint invariant subspaces corresponding to clustered eigenvalues, and a block subspace iteration that has converged to a subset of $k$ eigenvectors. Let the converged subspace be represented by a matrix $Q_{\\mathrm{lock}} \\in \\mathbb{R}^{n \\times k}$ whose columns are numerically computed approximations of the true eigenvectors. The goals are to precisely separate the remaining invariant subspaces and to eliminate “ghost” Ritz values that numerically appear near the already converged eigenvalues due to contamination in $Q_{\\mathrm{lock}}$.\n\nYou must start from the following base principles of numerical linear algebra:\n- The definition of an invariant subspace: a subspace $\\mathcal{S} \\subset \\mathbb{R}^n$ is invariant under $A$ if $A \\mathcal{S} \\subseteq \\mathcal{S}$.\n- The Rayleigh–Ritz method: given a subspace spanned by columns of $Q \\in \\mathbb{R}^{n \\times m}$ with orthonormal columns, the Ritz values are the eigenvalues of $Q^\\top A Q$ and the Ritz vectors in $\\mathbb{R}^n$ are $Q z$, where $z$ is an eigenvector of $Q^\\top A Q$.\n- The orthogonal projector onto the orthogonal complement of a subspace spanned by orthonormal columns $Q \\in \\mathbb{R}^{n \\times k}$ is $P = I - Q Q^\\top$.\n- Subspace iteration principle: repeatedly applying $A$ to a subspace and reorthonormalizing drives the subspace toward the invariant subspace associated with extremal eigenvalues, under appropriate conditions.\n\nDesign two deflation strategies for continuing the computation after $k$ eigenvectors have converged:\n1. A naive deflation that attempts to remove the locked subspace by a single-pass projection against $Q_{\\mathrm{lock}}$ that is not orthonormal. This approach is susceptible to numerical contamination and may admit ghost Ritz values.\n2. A robust deflation that constructs an orthonormal basis $\\widehat{Q}_{\\mathrm{lock}}$ for the span of $Q_{\\mathrm{lock}}$ (for example, via a numerically stable orthonormalization) and uses the orthogonal projector $P = I - \\widehat{Q}_{\\mathrm{lock}} \\widehat{Q}_{\\mathrm{lock}}^\\top$ to form and restrict the deflated operator $P A P$ and to purge locked components at every step.\n\nDefine a ghost Ritz value as any Ritz value obtained in the deflated stage that lies within a given absolute tolerance $\\delta$ of the set of already converged eigenvalues. Formally, if $\\{\\lambda_i\\}_{i=1}^k$ are the converged eigenvalues, a deflated-stage Ritz value $\\mu$ is a ghost if $\\min_{i} |\\mu - \\lambda_i| \\le \\delta$.\n\nImplement the following quantifiable experiment entirely in software:\n- Construct $A$ as $A = Q \\,\\mathrm{diag}(d)\\, Q^\\top$, where $Q \\in \\mathbb{R}^{n \\times n}$ is orthogonal and $d \\in \\mathbb{R}^n$ is a specified list of eigenvalues with prescribed clusters and multiplicities; this ensures known invariant subspaces and known eigenpairs.\n- Let $Q_{\\mathrm{lock}}$ be the exact eigenvectors for a chosen subset of indices (the “converged subset”), then contaminate them numerically by adding a small dense perturbation $\\epsilon R$ to obtain $Q_{\\mathrm{lock}}^{\\mathrm{bad}} = Q_{\\mathrm{lock}} + \\epsilon R$, where $R \\in \\mathbb{R}^{n \\times k}$ is random and $\\epsilon > 0$.\n- For the naive deflation, obtain a trial subspace $Q_{\\mathrm{trial}}$ by a single-pass projection against $Q_{\\mathrm{lock}}^{\\mathrm{bad}}$, $Y = X - Q_{\\mathrm{lock}}^{\\mathrm{bad}} (Q_{\\mathrm{lock}}^{\\mathrm{bad}})^\\top X$, followed by orthonormalization of $Y$; then compute Ritz values of $Q_{\\mathrm{trial}}^\\top A Q_{\\mathrm{trial}}$.\n- For the robust deflation, compute $\\widehat{Q}_{\\mathrm{lock}}$ by orthonormalizing $Q_{\\mathrm{lock}}^{\\mathrm{bad}}$ using a numerically stable method (for example, a two-pass orthogonalization or a singular value decomposition), form $P = I - \\widehat{Q}_{\\mathrm{lock}} \\widehat{Q}_{\\mathrm{lock}}^\\top$, obtain $Q_{\\mathrm{trial}}$ by orthonormalizing $P X$ with a double purge against $\\widehat{Q}_{\\mathrm{lock}}$, and compute Ritz values of $Q_{\\mathrm{trial}}^\\top (P A P) Q_{\\mathrm{trial}}$.\n\nThe tolerance for ghost detection is fixed as $\\delta = 10^{-3}$.\n\nProvide a test suite with three scientifically meaningful cases that exercise different facets of the problem:\n- Case 1 (happy path): $n = 80$, $k = 5$, eigenvalues $d$ consist of a cluster at $1$ of multiplicity $5$, a cluster at $5$ of multiplicity $5$, and the remaining $70$ eigenvalues uniformly distributed in $[10, 20]$. Contamination level $\\epsilon = 5 \\times 10^{-2}$. Trial subspace dimension $m = 8$. Random seed for reproducibility is $12345$.\n- Case 2 (boundary condition with no converged subset): $n = 60$, $k = 0$, eigenvalues $d$ uniformly distributed in $[1, 20]$. Contamination level $\\epsilon = 0$. Trial subspace dimension $m = 6$. Random seed $= 54321$.\n- Case 3 (multiple eigenvalues): $n = 90$, $k = 3$, eigenvalues $d$ consist of a cluster at $3$ of multiplicity $3$, a cluster at $4$ of multiplicity $3$, and the remaining $84$ eigenvalues uniformly distributed in $[6, 12]$. Contamination level $\\epsilon = 10^{-1}$. Trial subspace dimension $m = 6$. Random seed $= 31415$.\n\nFor each case, compute two integers:\n- $g_{\\mathrm{naive}}$: the number of ghost Ritz values for the naive deflation strategy.\n- $g_{\\mathrm{robust}}$: the number of ghost Ritz values for the robust deflation strategy.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each list element is a boolean for the corresponding case:\n- For Case 1: output $\\mathrm{True}$ if $g_{\\mathrm{robust}}  g_{\\mathrm{naive}}$, otherwise $\\mathrm{False}$.\n- For Case 2: output $\\mathrm{True}$ if $g_{\\mathrm{robust}} = g_{\\mathrm{naive}} = 0$, otherwise $\\mathrm{False}$.\n- For Case 3: output $\\mathrm{True}$ if $g_{\\mathrm{robust}}  g_{\\mathrm{naive}}$, otherwise $\\mathrm{False}$.\n\nNo physical units or angle units are involved. All outputs are unitless integers or booleans. The final output format must be exactly a single line like $[\\mathrm{True},\\mathrm{False},\\mathrm{True}]$ with no additional text.", "solution": "The problem requires the design and comparison of two deflation strategies for a block subspace iteration method applied to a real symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$. The primary goal is to demonstrate how a numerically robust deflation strategy can prevent the emergence of spurious \"ghost\" Ritz values, which can occur with a naive approach when the basis for the converged (locked) subspace is not perfectly orthogonal.\n\n### Foundational Principles\n\nLet $A \\in \\mathbb{R}^{n \\times n}$ be a real symmetric matrix. Its spectral decomposition is $A = V \\Lambda V^\\top$, where $V$ is an orthogonal matrix whose columns are the eigenvectors of $A$, and $\\Lambda$ is a diagonal matrix containing the corresponding real eigenvalues.\n\nAn invariant subspace $\\mathcal{S} \\subseteq \\mathbb{R}^n$ under the linear operator $A$ is a subspace for which $A\\mathcal{S} \\subseteq \\mathcal{S}$. For a symmetric matrix $A$, any subspace spanned by a set of its eigenvectors is an invariant subspace.\n\nThe Rayleigh-Ritz method is used to extract approximate eigenpairs from a trial subspace $\\mathcal{Q}$. Given an orthonormal basis for $\\mathcal{Q}$, represented by the columns of a matrix $Q \\in \\mathbb{R}^{n \\times m}$, the method forms the projected matrix $H = Q^\\top A Q \\in \\mathbb{R}^{m \\times m}$. The eigenvalues of $H$ are called Ritz values, and they serve as approximations to the eigenvalues of $A$.\n\nDeflation is a technique used in iterative eigenvalue algorithms to prevent the reconvergence to already found eigenpairs. Once a set of $k$ eigenpairs associated with an invariant subspace $\\mathcal{S}_{\\mathrm{lock}}$ has been determined, deflation modifies the operator or the trial subspace to restrict the search to the orthogonal complement of $\\mathcal{S}_{\\mathrm{lock}}$, denoted $\\mathcal{S}_{\\mathrm{lock}}^\\perp$.\n\n### Deflation Strategy 1: Naive Deflation\n\nThis strategy attempts to remove components of the locked subspace from a new set of trial vectors using a computationally simple, but numerically flawed, procedure. Let the converged eigenvectors be approximated by the columns of a matrix $Q_{\\mathrm{lock}} \\in \\mathbb{R}^{n \\times k}$. In a realistic scenario, due to finite precision arithmetic and approximation errors, this matrix is contaminated, resulting in $Q_{\\mathrm{lock}}^{\\mathrm{bad}}$. The columns of $Q_{\\mathrm{lock}}^{\\mathrm{bad}}$ are generally not orthogonal, i.e., $(Q_{\\mathrm{lock}}^{\\mathrm{bad}})^\\top Q_{\\mathrm{lock}}^{\\mathrm{bad}} \\neq I$.\n\nThe naive deflation, as specified, applies the following operation to a block of random trial vectors $X \\in \\mathbb{R}^{n \\times m}$:\n$$\nY_{\\mathrm{naive}} = X - Q_{\\mathrm{lock}}^{\\mathrm{bad}} (Q_{\\mathrm{lock}}^{\\mathrm{bad}})^\\top X\n$$\nThis operation corresponds to applying the matrix $P_{\\mathrm{improper}} = I - Q_{\\mathrm{lock}}^{\\mathrm{bad}} (Q_{\\mathrm{lock}}^{\\mathrm{bad}})^\\top$. This is not an orthogonal projector because $P_{\\mathrm{improper}}^2 \\neq P_{\\mathrm{improper}}$ when the columns of $Q_{\\mathrm{lock}}^{\\mathrm{bad}}$ are not orthonormal. Consequently, this operation fails to completely remove components from the subspace $\\mathrm{span}(Q_{\\mathrm{lock}}^{\\mathrm{bad}})$.\n\nAn orthonormal basis $Q_{\\mathrm{trial, naive}}$ is then computed from the columns of $Y_{\\mathrm{naive}}$. The Ritz values are found by computing the eigenvalues of the Rayleigh quotient $H_{\\mathrm{naive}} = Q_{\\mathrm{trial, naive}}^\\top A Q_{\\mathrm{trial, naive}}$. Because components of the locked subspace can \"leak\" into $Q_{\\mathrm{trial, naive}}$, the resulting Ritz values may include spurious approximations of the locked eigenvalues. These are the \"ghost\" Ritz values.\n\n### Deflation Strategy 2: Robust Deflation\n\nThis strategy follows a numerically sound procedure to ensure the complete removal of the locked subspace.\n\n1.  **Orthonormalize the Locked Subspace Basis:** The first step is to construct a proper orthonormal basis for the subspace spanned by the contaminated vectors. This is achieved by applying a numerically stable orthonormalization procedure (like a QR decomposition) to $Q_{\\mathrm{lock}}^{\\mathrm{bad}}$ to obtain a matrix $\\widehat{Q}_{\\mathrm{lock}} \\in \\mathbb{R}^{n \\times k}$ whose columns form an orthonormal basis for $\\mathrm{span}(Q_{\\mathrm{lock}}^{\\mathrm{bad}})$.\n\n2.  **Construct Orthogonal Projector:** Using this orthonormal basis, a true orthogonal projector $P$ onto the orthogonal complement of the locked subspace is formed:\n    $$\n    P = I - \\widehat{Q}_{\\mathrm{lock}} \\widehat{Q}_{\\mathrm{lock}}^\\top\n    $$\n    This projector is idempotent ($P^2 = P$) and symmetric ($P^\\top = P$).\n\n3.  **Project Trial Vectors:** The random trial vectors $X$ are projected onto this orthogonal complement:\n    $$\n    Y_{\\mathrm{robust}} = P X = X - \\widehat{Q}_{\\mathrm{lock}} (\\widehat{Q}_{\\mathrm{lock}}^\\top X)\n    $$\n    The columns of $Y_{\\mathrm{robust}}$ are, by construction, orthogonal to the locked subspace $\\mathrm{span}(\\widehat{Q}_{\\mathrm{lock}})$.\n\n4.  **Compute Ritz Values:** An orthonormal basis $Q_{\\mathrm{trial, robust}}$ is computed from $Y_{\\mathrm{robust}}$. The Ritz values are the eigenvalues of the Rayleigh quotient $H_{\\mathrm{robust}} = Q_{\\mathrm{trial, robust}}^\\top A Q_{\\mathrm{trial, robust}}$. Since the columns of $Q_{\\mathrm{trial, robust}}$ are in the range of $P$, we have $P Q_{\\mathrm{trial, robust}} = Q_{\\mathrm{trial, robust}}$. This means the Ritz values are approximations of the eigenvalues of the deflated operator $P A P$ restricted to this trial subspace. This procedure effectively prevents leakage from the locked subspace, thus eliminating ghost Ritz values.\n\n### Experimental Design and Quantification\n\nThe experiment systematically compares these two strategies.\n\n-   A symmetric matrix $A$ is constructed as $A = Q D Q^\\top$, where $Q$ is an orthogonal matrix of true eigenvectors and $D$ is a diagonal matrix of prescribed eigenvalues $d_i$.\n-   The \"locked\" eigenvectors are chosen as the first $k$ columns of $Q$, denoted $Q_{:k}$. These are contaminated by adding random noise: $Q_{\\mathrm{lock}}^{\\mathrm{bad}} = Q_{:k} + \\epsilon R$, where $R$ is a random matrix and $\\epsilon$ controls the contamination level.\n-   The \"locked\" eigenvalues $\\Lambda_{\\mathrm{lock}} = \\{\\lambda_1, \\dots, \\lambda_k\\}$ are the first $k$ eigenvalues from $D$.\n-   A Ritz value $\\mu$ computed during a deflated stage is classified as a \"ghost\" if it is close to any of the locked eigenvalues, i.e., if $\\min_{i \\in \\{1,\\dots,k\\}} |\\mu - \\lambda_i| \\le \\delta$, where $\\delta = 10^{-3}$.\n-   The number of ghost Ritz values is counted for both the naive method ($g_{\\mathrm{naive}}$) and the robust method ($g_{\\mathrm{robust}}$). The test cases are designed to show that $g_{\\mathrm{robust}}  g_{\\mathrm{naive}}$, particularly when $\\epsilon > 0$. The case with $k=0$ serves as a control, where no deflation is performed, and both methods should yield $g_{\\mathrm{naive}} = g_{\\mathrm{robust}} = 0$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import qr, eigh\nfrom scipy.stats import ortho_group\n\ndef solve():\n    \"\"\"\n    Implements and compares naive vs. robust deflation strategies for\n    block subspace iteration to demonstrate the emergence of 'ghost' Ritz values.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases_spec = [\n        # Case 1: Happy path\n        {'n': 80, 'k': 5, 'd_params': {'clusters': [(1.0, 5), (5.0, 5)], 'uniform_range': (10.0, 20.0)}, 'epsilon': 5e-2, 'm': 8, 'seed': 12345, 'condition': 'robust_less_than_naive'},\n        # Case 2: Boundary condition with no converged subset\n        {'n': 60, 'k': 0, 'd_params': {'clusters': [], 'uniform_range': (1.0, 20.0)}, 'epsilon': 0.0, 'm': 6, 'seed': 54321, 'condition': 'robust_equals_naive_zero'},\n        # Case 3: Multiple eigenvalues\n        {'n': 90, 'k': 3, 'd_params': {'clusters': [(3.0, 3), (4.0, 3)], 'uniform_range': (6.0, 12.0)}, 'epsilon': 1e-1, 'm': 6, 'seed': 31415, 'condition': 'robust_less_than_naive'}\n    ]\n\n    delta = 1e-3\n    results = []\n\n    for case in test_cases_spec:\n        n, k, d_params, epsilon, m, seed = case['n'], case['k'], case['d_params'], case['epsilon'], case['m'], case['seed']\n        rng = np.random.default_rng(seed)\n\n        # 1. Construct the test matrix A = Q D Q.T\n        d_list = []\n        num_clustered = 0\n        for val, mult in d_params['clusters']:\n            d_list.extend([val] * mult)\n            num_clustered += mult\n        \n        num_uniform = n - num_clustered\n        if num_uniform > 0:\n            d_list.extend(rng.uniform(d_params['uniform_range'][0], d_params['uniform_range'][1], num_uniform))\n        \n        d = np.array(d_list)\n        d.sort()  # Ensure eigenvalues are sorted for consistent selection\n\n        # Generate a random orthogonal matrix Q\n        Q = ortho_group.rvs(dim=n, random_state=rng)\n        A = Q @ np.diag(d) @ Q.T\n\n        # 2. Handle the k=0 case (no deflation)\n        if k == 0:\n            g_naive = 0\n            g_robust = 0\n        else:\n            # 3. Setup common variables for deflation\n            locked_eigs = d[:k]\n            Q_lock = Q[:, :k]  # True eigenvectors for the locked subspace\n            R = rng.standard_normal((n, k))\n            Q_lock_bad = Q_lock + epsilon * R  # Contaminated basis\n            X = rng.standard_normal((n, m))  # Random trial subspace\n\n            # 4. Naive Deflation\n            Y_naive = X - Q_lock_bad @ (Q_lock_bad.T @ X)\n            Q_trial_naive, _ = qr(Y_naive, mode='economic')\n\n            H_naive = Q_trial_naive.T @ A @ Q_trial_naive\n            ritz_values_naive = eigh(H_naive, eigvals_only=True)\n\n            g_naive = sum(1 for mu in ritz_values_naive if np.any(np.abs(mu - locked_eigs) = delta))\n\n            # 5. Robust Deflation\n            Q_hat_lock, _ = qr(Q_lock_bad, mode='economic') # Orthonormalize the bad basis\n            \n            # Project X onto the orthogonal complement of the locked subspace\n            Y_robust = X - Q_hat_lock @ (Q_hat_lock.T @ X)\n            Q_trial_robust, _ = qr(Y_robust, mode='economic')\n            \n            H_robust = Q_trial_robust.T @ A @ Q_trial_robust\n            ritz_values_robust = eigh(H_robust, eigvals_only=True)\n\n            g_robust = sum(1 for mu in ritz_values_robust if np.any(np.abs(mu - locked_eigs) = delta))\n        \n        # 6. Evaluate the specified condition for the case\n        condition_type = case['condition']\n        if condition_type == 'robust_less_than_naive':\n            results.append(g_robust  g_naive)\n        elif condition_type == 'robust_equals_naive_zero':\n            results.append(g_robust == 0 and g_naive == 0)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3582676"}]}