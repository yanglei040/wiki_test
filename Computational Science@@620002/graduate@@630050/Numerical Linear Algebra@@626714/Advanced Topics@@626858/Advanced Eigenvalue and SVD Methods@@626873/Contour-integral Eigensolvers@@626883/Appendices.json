{"hands_on_practices": [{"introduction": "The effectiveness of a contour-integral eigensolver hinges on its ability to create a rational filter that sharply distinguishes between eigenvalues inside and outside the chosen contour. This exercise provides a foundational analysis of this filter selectivity. By deriving the explicit rational filter for a canonical case and analyzing its performance, you will gain a quantitative understanding of how the separation between eigenvalues and the contour dictates the algorithm's convergence rate. [@problem_id:3541067]", "problem": "Consider a real symmetric (Hermitian) matrix $A \\in \\mathbb{R}^{n \\times n}$ with real spectrum, and suppose the goal is to compute all eigenpairs whose eigenvalues lie in the real interval $\\left[-r, r\\right]$ with $r \\in (0,1)$. A standard contour-integral eigensolver based on the spectral projector approximates\n$$\nP = \\frac{1}{2\\pi i} \\oint_{\\Gamma} (zI - A)^{-1} \\, dz,\n$$\nwhere $\\Gamma$ is the unit circle centered at the origin in the complex plane, oriented counterclockwise, and the integral is approximated by the $M$-point trapezoidal rule over the parametrization $z(\\theta)=\\exp(i\\theta)$ for $\\theta \\in [0,2\\pi)$. The resulting rational filter $R_M(A)$ is applied as a subspace iteration that maps a subspace $V$ to $R_M(A)V$. In this setting, the asymptotic contraction factor of components associated with unwanted eigenvalues (those outside $\\Gamma$) is bounded above by the ratio of the maximal scalar filter magnitude attained on unwanted eigenvalues to the minimal scalar filter magnitude attained on desired eigenvalues.\n\nAssume all desired eigenvalues lie within $\\left[-r, r\\right]$ and all unwanted eigenvalues lie outside the unit circle, with the closest unwanted eigenvalue $\\lambda_{\\star} = 1 + s$ for some $s>0$. Working from the definitions above, the unit-circle parametrization, and the $M$-point trapezoidal rule, derive the explicit scalar rational filter induced by the quadrature, use it to bound the contraction factor as a function of $s$, $M$, and $r$, and then determine the minimal separation $s_{\\min}$ that guarantees a prescribed contraction factor $q \\in (0,1)$ per iteration, i.e., ensures that the ratio of the maximal magnitude of the filter on unwanted eigenvalues to the minimal magnitude on desired eigenvalues is at most $q$. Give your final answer as a single closed-form analytic expression for $s_{\\min}$ in terms of $M$, $r$, and $q$. No rounding is required.", "solution": "The problem asks for the minimal separation $s_{\\min}$ of unwanted eigenvalues from the unit circle to guarantee a specific contraction rate for a contour-integral eigensolver. The derivation proceeds in three main steps: first, we determine the explicit form of the scalar rational filter $R_M(\\lambda)$ induced by the numerical quadrature; second, we establish bounds on the magnitude of this filter for desired and unwanted eigenvalues; and third, we use these bounds to solve for the required separation $s$.\n\n**Step 1: Derivation of the Scalar Rational Filter**\n\nThe spectral projector $P$ for the eigenvalue problem of a matrix $A$ is given by the contour integral\n$$\nP = \\frac{1}{2\\pi i} \\oint_{\\Gamma} (zI - A)^{-1} \\, dz\n$$\nwhere $\\Gamma$ is a contour enclosing the desired eigenvalues. The action of this projector on an eigenvector corresponding to an eigenvalue $\\lambda$ is governed by the scalar function\n$$\np(\\lambda) = \\frac{1}{2\\pi i} \\oint_{\\Gamma} (z - \\lambda)^{-1} \\, dz\n$$\nThe problem specifies that $\\Gamma$ is the unit circle, parametrized by $z(\\theta) = \\exp(i\\theta)$ for $\\theta \\in [0, 2\\pi)$. The differential is $dz = i\\exp(i\\theta)d\\theta$. Substituting this into the integral for $p(\\lambda)$ gives\n$$\np(\\lambda) = \\frac{1}{2\\pi i} \\int_{0}^{2\\pi} \\frac{1}{\\exp(i\\theta) - \\lambda} i\\exp(i\\theta) d\\theta = \\frac{1}{2\\pi} \\int_{0}^{2\\pi} \\frac{\\exp(i\\theta)}{\\exp(i\\theta) - \\lambda} d\\theta\n$$\nThe problem states that this integral is approximated using the $M$-point trapezoidal rule. The interval is $[0, 2\\pi)$ and the step size is $h = \\frac{2\\pi}{M}$. The quadrature points are $\\theta_k = \\frac{2\\pi k}{M}$ for $k \\in \\{0, 1, \\dots, M-1\\}$. The corresponding points on the unit circle are $z_k = \\exp(i\\theta_k) = \\exp(i \\frac{2\\pi k}{M})$, which are the $M$-th roots of unity.\n\nThe trapezoidal rule approximation, which defines the scalar rational filter $R_M(\\lambda)$, is given by\n$$\nR_M(\\lambda) = \\frac{h}{2\\pi} \\sum_{k=0}^{M-1} \\frac{z_k}{z_k - \\lambda} = \\frac{1}{M} \\sum_{k=0}^{M-1} \\frac{z_k}{z_k - \\lambda}\n$$\nWe can rewrite the term in the sum as $\\frac{z_k}{z_k - \\lambda} = 1 + \\frac{\\lambda}{z_k - \\lambda}$. This gives\n$$\nR_M(\\lambda) = \\frac{1}{M} \\sum_{k=0}^{M-1} \\left(1 + \\frac{\\lambda}{z_k - \\lambda}\\right) = 1 + \\frac{\\lambda}{M} \\sum_{k=0}^{M-1} \\frac{1}{z_k - \\lambda}\n$$\nThe sum can be evaluated using the partial fraction expansion of a related rational function. Consider the polynomial $p(z) = z^M - 1$, whose roots are the points $z_k$. Its logarithmic derivative is\n$$\n\\frac{p'(z)}{p(z)} = \\frac{Mz^{M-1}}{z^M - 1} = \\sum_{k=0}^{M-1} \\frac{1}{z - z_k}\n$$\nEvaluating this at $z=\\lambda$ and rearranging signs yields\n$$\n\\sum_{k=0}^{M-1} \\frac{1}{z_k - \\lambda} = - \\sum_{k=0}^{M-1} \\frac{1}{\\lambda - z_k} = - \\frac{M\\lambda^{M-1}}{\\lambda^M - 1}\n$$\nSubstituting this back into the expression for $R_M(\\lambda)$:\n$$\nR_M(\\lambda) = 1 + \\frac{\\lambda}{M} \\left( - \\frac{M\\lambda^{M-1}}{\\lambda^M - 1} \\right) = 1 - \\frac{\\lambda^M}{\\lambda^M - 1} = \\frac{(\\lambda^M - 1) - \\lambda^M}{\\lambda^M - 1} = \\frac{-1}{\\lambda^M - 1}\n$$\nThus, the explicit scalar rational filter is\n$$\nR_M(\\lambda) = \\frac{1}{1 - \\lambda^M}\n$$\n\n**Step 2: Bounding the Contraction Factor**\n\nThe asymptotic contraction factor is bounded by the ratio of the maximal filter magnitude on unwanted eigenvalues to the minimal filter magnitude on desired eigenvalues. Let this bound be $\\eta$.\n$$\n\\eta \\le \\frac{\\sup_{\\lambda_{\\text{out}}} |R_M(\\lambda_{\\text{out}})|}{\\inf_{\\lambda_{\\text{in}}} |R_M(\\lambda_{\\text{in}})|}\n$$\nThe eigenvalues $\\lambda$ are real. Desired eigenvalues $\\lambda_{\\text{in}}$ are in $[-r, r]$ where $r \\in (0,1)$. Unwanted eigenvalues $\\lambda_{\\text{out}}$ are outside the unit circle, with the closest one being at a distance $s>0$ from it, i.e., $|\\lambda_{\\text{out}}| \\ge 1+s$.\n\nFirst, we analyze the denominator, which is the minimal filter magnitude on the desired eigenvalues:\n$$\n\\inf_{\\lambda_{\\text{in}} \\in [-r, r]} |R_M(\\lambda_{\\text{in}})| = \\inf_{\\lambda \\in [-r, r]} \\left| \\frac{1}{1 - \\lambda^M} \\right| = \\frac{1}{\\sup_{\\lambda \\in [-r, r]} |1 - \\lambda^M|}\n$$\nFor any real $\\lambda$ with $|\\lambda| \\le r$, we have $|\\lambda^M| \\le r^M$. By the triangle inequality, $|1 - \\lambda^M| \\le 1 + |\\lambda^M| \\le 1 + r^M$. This upper bound is achieved if $M$ is odd at $\\lambda=-r$. If $M$ is even, the maximum is $1$. To have a bound that is valid for any $M$, we take the worst-case (largest) value for the denominator of the filter, which is $1+r^M$. Thus, we establish a lower bound for the filter magnitude on desired eigenvalues:\n$$\n\\inf_{\\lambda_{\\text{in}}} |R_M(\\lambda_{\\text{in}})| \\ge \\frac{1}{1 + r^M}\n$$\nNext, we analyze the numerator, which is the maximal filter magnitude on the unwanted eigenvalues:\n$$\n\\sup_{\\lambda_{\\text{out}}} |R_M(\\lambda_{\\text{out}})| = \\sup_{|\\lambda| \\ge 1+s, \\lambda \\in \\mathbb{R}} \\left| \\frac{1}{1 - \\lambda^M} \\right| = \\frac{1}{\\inf_{|\\lambda| \\ge 1+s, \\lambda \\in \\mathbb{R}} |1 - \\lambda^M|}\n$$\nWe need to find the minimum of $|1 - \\lambda^M|$ for real $\\lambda$ such that $\\lambda \\ge 1+s$ or $\\lambda \\le -(1+s)$.\n- If $\\lambda \\ge 1+s$, then $\\lambda^M \\ge (1+s)^M > 1$. The function $|1 - \\lambda^M| = \\lambda^M - 1$ is monotonically increasing. Its minimum on this domain is at $\\lambda = 1+s$, with value $(1+s)^M - 1$.\n- If $\\lambda \\le -(1+s)$, let $\\lambda = -x$ where $x \\ge 1+s$. The function is $|1 - (-x)^M|$.\n  - If $M$ is even: $|1 - x^M| = x^M - 1$. Minimum at $x=1+s$, value is $(1+s)^M - 1$.\n  - If $M$ is odd: $|1 + x^M| = 1 + x^M$. Minimum at $x=1+s$, value is $1 + (1+s)^M$.\nThe overall minimum of $|1 - \\lambda^M|$ over the entire domain of unwanted eigenvalues is $\\min((1+s)^M - 1, 1 + (1+s)^M) = (1+s)^M - 1$.\nTherefore, the maximal filter magnitude for unwanted eigenvalues is\n$$\n\\sup_{\\lambda_{\\text{out}}} |R_M(\\lambda_{\\text{out}})| = \\frac{1}{(1+s)^M - 1}\n$$\nCombining these results, we get the bound on the contraction factor:\n$$\n\\eta \\le \\frac{1/((1+s)^M - 1)}{1/(1+r^M)} = \\frac{1+r^M}{(1+s)^M - 1}\n$$\n\n**Step 3: Determining the Minimal Separation $s_{\\min}$**\n\nWe are given that the desired contraction factor is at most $q$, where $q \\in (0,1)$. We set our bound to be less than or equal to $q$ and solve for $s$.\n$$\n\\frac{1+r^M}{(1+s)^M - 1} \\le q\n$$\nSince both $q$ and $(1+s)^M - 1$ are positive, we can rearrange the inequality:\n$$\n(1+s)^M - 1 \\ge \\frac{1+r^M}{q}\n$$\n$$\n(1+s)^M \\ge 1 + \\frac{1+r^M}{q}\n$$\nTaking the $M$-th root of both sides (which is a monotonic operation for positive bases):\n$$\n1+s \\ge \\left(1 + \\frac{1+r^M}{q}\\right)^{1/M}\n$$\n$$\ns \\ge \\left(1 + \\frac{1+r^M}{q}\\right)^{1/M} - 1\n$$\nThis inequality gives the condition on $s$ to ensure the desired contraction rate. The minimal separation, $s_{\\min}$, is the smallest value of $s$ that satisfies this condition.\n$$\ns_{\\min} = \\left(1 + \\frac{1+r^M}{q}\\right)^{1/M} - 1\n$$\nThis is the required closed-form analytic expression for $s_{\\min}$ in terms of $M$, $r$, and $q$.", "answer": "$$\n\\boxed{\\left(1 + \\frac{1+r^M}{q}\\right)^{1/M} - 1}\n$$", "id": "3541067"}, {"introduction": "While many theoretical analyses assume a diagonalizable matrix, real-world problems often involve matrices that are nearly defective, with clustered eigenvalues that challenge standard eigensolvers. The behavior of contour-integral methods in this regime is a critical test of their robustness. This practice delves into this challenging scenario, revealing how the specific choice of quadrature rule can dramatically affect the algorithm's ability to handle the complex structure associated with near-degeneracy. [@problem_id:3541135]", "problem": "Consider the family of $2\\times 2$ upper-triangular matrices $\\{A_{\\epsilon}\\}_{\\epsilon\\ge 0}$ defined by\n$$\nA_{\\epsilon} = \\begin{bmatrix} \\lambda+\\epsilon & 1 \\\\ 0 & \\lambda-\\epsilon \\end{bmatrix}\n= \\lambda I + N + \\epsilon \\begin{bmatrix} 1 & 0 \\\\ 0 & -1 \\end{bmatrix},\n\\quad\nN = \\begin{bmatrix} 0 & 1 \\\\ 0 & 0 \\end{bmatrix},\\quad N^2=0,\n$$\nso that $A_0 = \\lambda I + N$ is a single $2\\times 2$ Jordan block at eigenvalue $\\lambda$. Let the spectral projector $\\Pi$ onto the invariant subspace associated with eigenvalues enclosed by a positively oriented simple closed contour $\\Gamma$ be given by the Riesz integral\n$$\n\\Pi = \\frac{1}{2\\pi i} \\oint_{\\Gamma} (zI - A_{\\epsilon})^{-1}\\,dz.\n$$\nThe Filtered Eigensolver by Contour Techniques (FEAST) constructs a rational filter by approximating the contour integral with a quadrature rule. Using a circular contour $\\Gamma$ parameterized by $z(\\theta) = c + r e^{i\\theta}$ with center $c\\in\\mathbb{C}$ and radius $r>0$, the trapezoidal rule with $K$ equispaced nodes $\\theta_k = \\tfrac{2\\pi k}{K}$ for $k=0,1,\\dots,K-1$ yields the discrete FEAST filter\n$$\nF_K(A_{\\epsilon};c,r) \\equiv \\sum_{k=0}^{K-1} \\omega_k \\left(z_k I - A_{\\epsilon}\\right)^{-1}, \n\\quad z_k = c + r e^{i\\theta_k}, \n\\quad \\omega_k = \\frac{\\Delta\\theta}{2\\pi i} z'(\\theta_k) = \\frac{r e^{i\\theta_k}}{K},\n$$\nwhere $\\Delta\\theta = \\tfrac{2\\pi}{K}$ and $z'(\\theta) = i r e^{i\\theta}$. For the Jordan limit $\\epsilon=0$ and $c=\\lambda$, the resolvent admits the nilpotent expansion\n$$\n(zI - A_0)^{-1} = (z - \\lambda)^{-1} I + (z - \\lambda)^{-2} N,\n$$\nso that one FEAST application corresponds to the discrete moment sums\n$$\nF_K(A_0;\\lambda,r)\n= \\left(\\sum_{k=0}^{K-1} \\omega_k (z_k-\\lambda)^{-1}\\right) I \n+ \\left(\\sum_{k=0}^{K-1} \\omega_k (z_k-\\lambda)^{-2}\\right) N.\n$$\nFrom first principles, analyze how the discrete moments behave as $\\epsilon\\to 0$ and how FEAST’s subspace iteration filters the nilpotent contribution. In particular:\n- Derive the cancellation pattern for the nilpotent coefficient at $\\epsilon=0$ using the above quadrature. Show that for any $K\\ge 2$ the nilpotent term vanishes exactly, while for $K=1$ it persists with explicit magnitude.\n- For $\\epsilon>0$, compute the off-diagonal coefficient generated by the discrete filter,\n$$\n\\alpha_{K}(\\epsilon) \\equiv \\left[F_K(A_{\\epsilon};\\lambda,r)\\right]_{12} = \\sum_{k=0}^{K-1} \\omega_k \\left((z_k-\\lambda-\\epsilon)^{-1}(z_k-\\lambda+\\epsilon)^{-1}\\right),\n$$\nand develop a small-$\\epsilon$ asymptotic prediction for its decay, distinguishing the parity of $K$.\n- Explain the stagnation pattern in FEAST’s subspace iteration: interpret $\\alpha_K(\\epsilon)$ as the residual nilpotent contribution retained by a single filter application and relate its parity-dependent behavior to whether repeated FEAST steps can immediately eliminate the nilpotent part or only reduce it at a rate controlled by $\\epsilon$ and $K$.\n\nImplementation requirements:\n- Fix $\\lambda=0$ and $c=\\lambda=0$.\n- Fix $r=0.5$.\n- Implement the discrete FEAST filter $F_K(A_{\\epsilon};0,0.5)$ exactly as defined above, using complex arithmetic.\n- For each test case, compute the scalar quantity $\\alpha_K(\\epsilon)$ as the magnitude of the $(1,2)$ entry of $F_K(A_{\\epsilon};0,0.5)$, i.e., compute $\\left|\\left[F_K(A_{\\epsilon};0,0.5)\\right]_{12}\\right|$ as a real-valued float.\n\nTest suite:\n- Case 1 (boundary, single-node): $\\epsilon = 0.0$, $K = 1$.\n- Case 2 (minimum exact cancellation): $\\epsilon = 0.0$, $K = 2$.\n- Case 3 (near-defective, even parity exactness): $\\epsilon = 10^{-6}$, $K = 2$.\n- Case 4 (near-defective, odd parity leading-order nonzero): $\\epsilon = 10^{-6}$, $K = 3$.\n- Case 5 (moderate near-defective, odd parity scaling): $\\epsilon = 10^{-3}$, $K = 3$.\n- Case 6 (higher odd parity, stronger suppression): $\\epsilon = 10^{-3}$, $K = 5$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3,result4,result5,result6]\"), where each result is the real-valued float $\\left|\\left[F_K(A_{\\epsilon};0,0.5)\\right]_{12}\\right|$ for the corresponding test case.\n\nAll mathematical symbols, variables, functions, operators, and numbers must be written in LaTeX in this problem description.", "solution": "The problem requires an analysis of the discrete FEAST filter, $F_K(A_{\\epsilon};c,r)$, when applied to a family of nearly-defective matrices $A_{\\epsilon}$. The analysis focuses on the behavior of the $(1,2)$ entry of the filtered matrix, denoted $\\alpha_K(\\epsilon)$, particularly its dependence on the number of quadrature points, $K$, and the perturbation parameter, $\\epsilon$.\n\nFirst, we establish the necessary matrix inverse. The resolvent $(zI - A_{\\epsilon})^{-1}$ is central to the analysis. Given $A_{\\epsilon} = \\begin{bmatrix} \\lambda+\\epsilon & 1 \\\\ 0 & \\lambda-\\epsilon \\end{bmatrix}$, its inverse is:\n$$\n(zI - A_{\\epsilon})^{-1} = \\begin{bmatrix} z - (\\lambda+\\epsilon) & -1 \\\\ 0 & z - (\\lambda-\\epsilon) \\end{bmatrix}^{-1}\n= \\frac{1}{(z - \\lambda - \\epsilon)(z - \\lambda + \\epsilon)}\n\\begin{bmatrix} z - (\\lambda-\\epsilon) & 1 \\\\ 0 & z - (\\lambda-\\epsilon) \\end{bmatrix}\n$$\nThe $(1,2)$ entry of the resolvent is $\\frac{1}{(z - \\lambda - \\epsilon)(z - \\lambda + \\epsilon)}$.\nThe quantity to be computed, $\\alpha_K(\\epsilon)$, is the $(1,2)$ entry of the discrete filter $F_K(A_{\\epsilon};\\lambda,r)$. Using the definition of the filter, we have:\n$$\n\\alpha_{K}(\\epsilon) \\equiv \\left[F_K(A_{\\epsilon};\\lambda,r)\\right]_{12} = \\sum_{k=0}^{K-1} \\omega_k \\left[(z_k I - A_{\\epsilon})^{-1}\\right]_{12} = \\sum_{k=0}^{K-1} \\omega_k \\frac{1}{(z_k - \\lambda - \\epsilon)(z_k - \\lambda + \\epsilon)}\n$$\nWith the specified circular contour centered at $c=\\lambda$, the quadrature points are $z_k = \\lambda + r e^{i\\theta_k}$ and the weights are $\\omega_k = \\frac{r e^{i\\theta_k}}{K}$. Substituting these into the expression for $\\alpha_K(\\epsilon)$ yields:\n$$\n\\alpha_{K}(\\epsilon) = \\sum_{k=0}^{K-1} \\frac{r e^{i\\theta_k}}{K} \\frac{1}{(r e^{i\\theta_k} - \\epsilon)(r e^{i\\theta_k} + \\epsilon)} = \\sum_{k=0}^{K-1} \\frac{r e^{i\\theta_k}}{K} \\frac{1}{r^2 e^{i2\\theta_k} - \\epsilon^2}\n$$\n\nLet us now address the specific analytical tasks.\n\n**1. Analysis at the Jordan Limit ($\\epsilon=0$)**\n\nAt $\\epsilon=0$, the matrix becomes a Jordan block $A_0 = \\lambda I + N$. The expression for $\\alpha_K(0)$ simplifies to:\n$$\n\\alpha_K(0) = \\sum_{k=0}^{K-1} \\frac{r e^{i\\theta_k}}{K} \\frac{1}{r^2 e^{i2\\theta_k}} = \\frac{1}{Kr} \\sum_{k=0}^{K-1} e^{-i\\theta_k}\n$$\nThe sum is a geometric series of the $K$-th roots of unity. Specifically, with $\\theta_k = \\frac{2\\pi k}{K}$, the sum is $\\sum_{k=0}^{K-1} (e^{-i2\\pi/K})^k$.\nThe sum of a geometric series $\\sum_{k=0}^{K-1} q^k$ equals $\\frac{1-q^K}{1-q}$ if $q\\neq 1$, and $K$ if $q=1$.\n\n-   For $K=1$, the common ratio is $e^{-i2\\pi} = 1$. The sum is $1$. Thus,\n    $$\n    \\alpha_1(0) = \\frac{1}{(1)r} \\cdot 1 = \\frac{1}{r}\n    $$\n    The nilpotent contribution persists, with a magnitude inversely proportional to the contour radius $r$.\n\n-   For $K \\ge 2$, the common ratio is $q = e^{-i2\\pi/K} \\neq 1$. The sum is:\n    $$\n    \\sum_{k=0}^{K-1} (e^{-i2\\pi/K})^k = \\frac{1 - (e^{-i2\\pi/K})^K}{1 - e^{-i2\\pi/K}} = \\frac{1 - e^{-i2\\pi}}{1 - e^{-i2\\pi/K}} = \\frac{1-1}{1 - e^{-i2\\pi/K}} = 0\n    $$\n    Therefore, for any $K \\ge 2$:\n    $$\n    \\alpha_K(0) = 0\n    $$\n    This demonstrates an exact cancellation of the nilpotent coefficient when the trapezoidal rule with $K \\ge 2$ nodes is used to approximate the contour integral of the $(z-\\lambda)^{-2}N$ term in the resolvent expansion for the Jordan block.\n\n**2. Asymptotic Analysis for $\\epsilon > 0$**\n\nFor small $\\epsilon$ such that $|\\epsilon| < r$, we analyze $\\alpha_K(\\epsilon)$ by performing a Taylor series expansion of the denominator term:\n$$\n\\frac{1}{r^2 e^{i2\\theta_k} - \\epsilon^2} = \\frac{1}{r^2 e^{i2\\theta_k}} \\left( \\frac{1}{1 - (\\epsilon/r)^2 e^{-i2\\theta_k}} \\right) = \\frac{e^{-i2\\theta_k}}{r^2} \\sum_{j=0}^{\\infty} \\left(\\frac{\\epsilon}{r}\\right)^{2j} e^{-i2j\\theta_k}\n$$\nSubstituting this into the expression for $\\alpha_K(\\epsilon)$:\n$$\n\\alpha_K(\\epsilon) = \\sum_{k=0}^{K-1} \\frac{r e^{i\\theta_k}}{K} \\left( \\frac{e^{-i2\\theta_k}}{r^2} \\sum_{j=0}^{\\infty} \\left(\\frac{\\epsilon}{r}\\right)^{2j} e^{-i2j\\theta_k} \\right) = \\frac{1}{Kr} \\sum_{j=0}^{\\infty} \\left(\\frac{\\epsilon}{r}\\right)^{2j} \\sum_{k=0}^{K-1} e^{-i(2j+1)\\theta_k}\n$$\nThe inner sum, $\\sum_{k=0}^{K-1} e^{-i(2j+1)2\\pi k/K}$, is another geometric sum of roots of unity. It is equal to $K$ if $K$ divides $2j+1$, and $0$ otherwise. We consider the parity of $K$.\n\n-   **Even $K$**: Let $K=2p$ for some integer $p \\ge 1$. An integer multiple of an even number is always even, $mK=m(2p)$. However, $2j+1$ is always odd for any integer $j \\ge 0$. Thus, $2j+1$ can never be a multiple of an even $K$. This means the inner sum is zero for all $j \\ge 0$. Consequently, for any even $K \\ge 2$:\n    $$\n    \\alpha_K(\\epsilon) = 0\n    $$\n    This is a remarkable result: the discrete filter with an even number of nodes perfectly eliminates the off-diagonal coupling not only for the defective case $\\epsilon=0$ but also for the non-defective, nearly-degenerate case $\\epsilon>0$.\n\n-   **Odd $K$**: Let $K=2p+1$ for some integer $p \\ge 1$. Now it is possible for $K$ to divide $2j+1$. The series expansion for $\\alpha_K(\\epsilon)$ will contain non-zero terms. The leading-order term corresponds to the smallest non-negative integer $j$ for which $K$ divides $2j+1$. Since $K$ is odd, we can choose the multiplier $m=1$, giving $2j+1=K$, which yields $j = (K-1)/2$. This is an integer since $K$ is odd. The next non-zero term would be when $2j+1=3K$, giving $j=(3K-1)/2$. For a small-$\\epsilon$ asymptotic prediction, we only need the leading term, where $j=(K-1)/2$:\n    $$\n    \\alpha_K(\\epsilon) = \\frac{1}{Kr} \\left( \\left(\\frac{\\epsilon}{r}\\right)^{2j} \\cdot K \\right) + O(\\epsilon^{2j+2}) \\quad \\text{with } j=\\frac{K-1}{2}\n    $$\n    $$\n    \\alpha_K(\\epsilon) = \\frac{1}{r} \\left(\\frac{\\epsilon}{r}\\right)^{K-1} + O(\\epsilon^{K+1}) = \\frac{\\epsilon^{K-1}}{r^K} + O(\\epsilon^{K+1})\n    $$\n    For odd $K$, the off-diagonal coefficient is non-zero, decaying as $\\epsilon^{K-1}$.\n\n**3. Interpretation of the FEAST Stagnation Pattern**\n\nThe quantity $\\alpha_K(\\epsilon)$ represents the residual nilpotent contribution, i.e., the persistent off-diagonal coupling, after one application of the FEAST filter. The subspace iteration in FEAST aims to project out components outside the desired invariant subspace. For nearly-defective problems, this corresponds to isolating the eigenvectors associated with clustered eigenvalues.\n\n-   When an **even** number of quadrature points ($K \\ge 2$) is used, $\\alpha_K(\\epsilon)=0$. The filtered matrix $F_K(A_{\\epsilon};\\lambda,r)$ is diagonal (its $(1,2)$ entry is zero, and its $(2,1)$ entry is zero by the structure of $A_\\epsilon$). This means the filter perfectly decouples the basis vectors spanning the invariant subspace in a single step. FEAST's subspace iteration will converge immediately to the correct subspace without any stagnation related to the near-degeneracy.\n\n-   When an **odd** number of quadrature points ($K$) is used, $\\alpha_K(\\epsilon) \\approx \\epsilon^{K-1}/r^K \\neq 0$. The filtered matrix remains upper triangular with a small but non-zero off-diagonal element. This means a single filter application does not fully decouple the eigenvectors. Instead, it reduces the coupling from $O(1)$ in $A_\\epsilon$ to $O(\\epsilon^{K-1})$. Repeated applications of FEAST will continue to reduce this coupling, but the convergence rate will be limited by this factor. For very small $\\epsilon$, the convergence will appear to be rapid initially, but will then \"stagnate\" or slow down dramatically once the error in the subspace is on the order of $|\\alpha_K(\\epsilon)|$. The rate of improvement becomes algebraic in $\\epsilon$, rather than geometric as for well-separated eigenvalues. Increasing the (odd) number of nodes $K$ improves the suppression of the nilpotent part, as the residual decays faster with $\\epsilon$ (as $\\epsilon^{K-1}$).\n\nIn conclusion, the parity of the number of quadrature nodes $K$ in the FEAST algorithm is critical for its performance on nearly-defective problems. Even $K$ provides an exact cancellation of the off-diagonal coupling for this $2 \\times 2$ model, avoiding stagnation, while odd $K$ leads to a predictable residual coupling that explains the characteristic stagnation behavior observed in practice.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the magnitude of the (1,2) entry of the discrete FEAST filter \n    for a family of nearly-defective matrices.\n    \"\"\"\n    \n    # Fixed parameters from the problem statement\n    r = 0.5  # Contour radius\n    # lambda and c are zero\n\n    test_cases = [\n        # (epsilon, K)\n        (0.0, 1),\n        (0.0, 2),\n        (1e-6, 2),\n        (1e-6, 3),\n        (1e-3, 3),\n        (1e-3, 5),\n    ]\n\n    results = []\n\n    def compute_alpha(epsilon: float, K: int, r_val: float) -> float:\n        \"\"\"\n        Calculates the scalar quantity alpha_K(epsilon) as the magnitude \n        of the (1,2) entry of the discrete FEAST filter F_K(A_epsilon; 0, r).\n\n        alpha_K(epsilon) = sum_{k=0}^{K-1} omega_k * 1/((z_k - epsilon)*(z_k + epsilon))\n                         = sum_{k=0}^{K-1} omega_k / (z_k^2 - epsilon^2)\n        \n        where z_k = r * exp(i*theta_k) and omega_k = r * exp(i*theta_k) / K = z_k / K.\n        \"\"\"\n        \n        alpha = 0.0 + 0.0j\n        \n        # This handles the K=0 case, though not in test suite.\n        if K == 0:\n            return 0.0\n\n        for k in range(K):\n            # Quadrature node\n            theta_k = 2 * np.pi * k / K\n            z_k = r_val * np.exp(1j * theta_k)\n            \n            # Quadrature weight\n            # The definition is omega_k = (r * exp(i*theta_k)) / K\n            # This is simply z_k / K\n            omega_k = z_k / K\n            \n            # Denominator term\n            # In the degenerate case epsilon=0, z_k**2 could be zero if r=0, but r=0.5\n            denominator = z_k**2 - epsilon**2\n            \n            # Add the contribution from the k-th node\n            # The formula for the (1,2) entry is derived in the text\n            if np.abs(denominator) < 1e-18: # Avoid division by zero for exotic cases\n                # This branch is not expected to be hit with the given parameters,\n                # as |z_k|=r=0.5 and epsilon is small.\n                # A more robust implementation might need careful handling here.\n                # For this problem, it is safe.\n                pass\n            \n            alpha += omega_k / denominator\n\n        return np.abs(alpha)\n\n    for epsilon, K in test_cases:\n        result = compute_alpha(epsilon, K, r)\n        results.append(result)\n\n    # Format the final output as a comma-separated list in brackets\n    print(f\"[{','.join(f'{res:.16e}' for res in results)}]\")\n\nsolve()\n```", "id": "3541135"}, {"introduction": "The practical implementation of contour-integral eigensolvers involves a nested iteration, where \"inner\" iterative linear solvers approximate the action of the resolvent $(z_k I - A)^{-1}$ at each quadrature point. The accuracy of these inner solves directly impacts the convergence of the \"outer\" subspace iteration. This exercise explores the crucial interplay between inner solver accuracy and outer loop convergence, allowing you to derive and test a condition that links inner solver tolerance to the guaranteed contraction of the overall algorithm. [@problem_id:3541107]", "problem": "Consider a Hermitian matrix eigenvalue problem for a real symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$, and the Filtered Eigensolver by Approximate Spectral Transformation (FEAST) algorithm based on contour integrals. The exact spectral projector onto the invariant subspace associated with eigenvalues inside a closed contour $\\Gamma$ in the complex plane is\n$$\nP \\;=\\; \\frac{1}{2\\pi i}\\int_{\\Gamma} (zI - A)^{-1}\\,dz.\n$$\nIn practice, a quadrature rule with nodes $z_k \\in \\mathbb{C}$ and weights $w_k \\in \\mathbb{C}$ approximates the projector by the rational filter\n$$\nR(A) \\;=\\; \\sum_{k=1}^{m} w_k (z_k I - A)^{-1}.\n$$\nOne FEAST outer iteration applies $R(A)$ to a current subspace basis and then performs Rayleigh–Ritz to update the approximate invariant subspace. When the inner linear systems $(z_k I - A) X_k = Y$ are solved inexactly, the computed operator becomes\n$$\n\\widetilde{R}(A) \\;=\\; \\sum_{k=1}^{m} w_k \\left((z_k I - A)^{-1} + E_k\\right),\n$$\nwhere $E_k$ are linear solve error operators induced by nonzero residuals $R_k = Y - (z_k I - A)\\widetilde{X}_k$.\n\nStarting from the fundamental definitions above, and the standard subspace iteration framework, derive a sufficient condition that links the inner solve residual levels to a contraction of the error in the computed FEAST subspace. Your derivation must:\n- Begin from the basic properties of the spectral projector, the quadrature-based rational filter, and the subspace iteration view of FEAST.\n- Introduce the scalar filter $R(\\lambda)$ induced by $R(A)$ on an eigenvalue $\\lambda$ of $A$, and define quantities\n$$\n\\alpha_{\\mathrm{in}} \\;=\\; \\min_{\\lambda \\in \\Lambda_{\\mathrm{in}}} |R(\\lambda)|,\\quad\n\\beta_{\\mathrm{out}} \\;=\\; \\max_{\\lambda \\in \\Lambda_{\\mathrm{out}}} |R(\\lambda)|,\n$$\nwhere $\\Lambda_{\\mathrm{in}}$ and $\\Lambda_{\\mathrm{out}}$ are the sets of eigenvalues of $A$ strictly inside and strictly outside $\\Gamma$, respectively.\n- Bound the operator norm of the perturbation $\\Delta R \\equiv \\widetilde{R}(A)-R(A)$ in terms of the linear system residual levels, showing how a uniform tolerance schedule $0 \\le \\tau_k \\le \\tau$ for the inner residuals can be chosen so that the outer iteration remains a contraction in the subspace error.\n\nThen, design and implement a program that evaluates both your sufficient condition prediction and the actually observed contraction in a concrete instance, as follows.\n\nSet up the following data:\n- Matrix size $n = 40$. Let $A$ be diagonal with entries linearly spaced in the interval $[-1.5, 1.5]$.\n- Target region $\\Gamma$ is a circle centered at $c = 0$ with radius $r = 0.45$. Parameterize $\\Gamma$ by $z(\\theta) = c + r e^{i\\theta}$ for $\\theta \\in [0, 2\\pi]$ and approximate the contour integral with an $m$-point trapezoidal rule. Use quadrature nodes $z_k = c + r e^{i \\theta_k}$ with $\\theta_k = 2\\pi(k-1)/m$ and weights $w_k = \\frac{r}{m} e^{i \\theta_k}$.\n- Define the scalar filter on a real argument $\\lambda$ by $R(\\lambda) = \\sum_{k=1}^{m} \\frac{w_k}{z_k - \\lambda}$.\n- Let $\\Lambda_{\\mathrm{in}} = \\{\\lambda: |\\lambda - c| &lt; r\\}$ and $\\Lambda_{\\mathrm{out}} = \\{\\lambda: |\\lambda - c| \\ge r\\}$, where the $\\lambda$ are the eigenvalues of $A$.\n- Define the bound\n$$\nS \\;=\\; \\sum_{k=1}^m |w_k| \\,\\bigl\\|(z_k I - A)^{-1}\\bigr\\|_2,\n$$\nand evaluate $\\bigl\\|(z_k I - A)^{-1}\\bigr\\|_2 = 1/\\min_{\\lambda \\in \\mathrm{spec}(A)} |z_k - \\lambda|$ for the Hermitian case.\n- Define the sufficient-condition predicted contraction factor for a uniform residual tolerance $\\tau$ as a quantity of the form\n$$\n\\rho_{\\mathrm{pred}} \\;=\\; \\frac{\\beta_{\\mathrm{out}}}{\\alpha_{\\mathrm{in}}} \\;+\\; \\frac{S\\,\\tau}{\\alpha_{\\mathrm{in}}}.\n$$\n- Implement one FEAST outer step with inexact inner solves enforced as follows: for each quadrature node $z_k$, instead of the exact solution $X_k^\\ast = (z_k I - A)^{-1}Y$, use the approximation $\\widetilde{X}_k = (1-\\tau) X_k^\\ast$, which yields a residual $R_k = \\tau Y$ with columnwise relative $2$-norm equal to $\\tau$; assemble $\\widetilde{R}(A)Y = \\sum_{k=1}^{m} w_k \\widetilde{X}_k$; orthonormalize the result to get the updated subspace.\n- Measure the largest principal angle between the target invariant subspace (spanned by the eigenvectors of $A$ with eigenvalues in $\\Lambda_{\\mathrm{in}}$) and the iterate subspace before and after one outer step; report the observed contraction ratio\n$$\n\\rho_{\\mathrm{obs}} \\;=\\; \\frac{\\sin(\\theta_{\\max}^{\\mathrm{new}})}{\\sin(\\theta_{\\max}^{\\mathrm{old}})}.\n$$\n\nYour program must compute, for each test case, the pair $[\\rho_{\\mathrm{pred}}, \\rho_{\\mathrm{obs}}]$ as floats.\n\nTest suite:\n- Use $m = 16$ quadrature nodes.\n- Use an initial block size $p = s + 4$, where $s$ is the number of eigenvalues in $\\Lambda_{\\mathrm{in}}$, and initialize the starting subspace with a fixed-seed Gaussian random matrix orthonormalized by a $QR$ factorization.\n- For the residual tolerance schedule, let $\\tau_{\\max}$ be the largest uniform tolerance such that your sufficient-condition inequality strictly guarantees contraction. Define three test cases with scaling factors $\\sigma \\in \\{0.5, 1.0, 1.5\\}$ and use $\\tau = \\sigma \\tau_{\\max}$ in each case.\n\nYour program should produce a single line of output containing the results as a comma-separated list of the three pairs, each rounded to six decimal places and enclosed in nested square brackets, for example, \"[[x1,y1],[x2,y2],[x3,y3]]\", where each $xj$ is $\\rho_{\\mathrm{pred}}$ and each $yj$ is $\\rho_{\\mathrm{obs}}$ for the $j$-th test case. No physical units are involved. Angles are handled internally in radians. All outputs must be pure numbers without percentage signs.", "solution": "The problem requires the derivation of a sufficient condition for the convergence of the FEAST algorithm with inexact inner linear system solves, followed by a numerical implementation to validate the derived theoretical model.\n\n### Derivation of the Sufficient Condition for Contraction\n\nThe FEAST algorithm can be understood as a form of subspace iteration. A single outer step updates a subspace basis $Q_{\\mathrm{old}}$ to a new basis $Q_{\\mathrm{new}}$ by applying a rational filter operator $\\widetilde{R}(A)$ and re-orthogonalizing. The goal is to make the subspace spanned by $Q_{\\mathrm{new}}$ a better approximation of the target invariant subspace $V_{\\mathrm{in}}$, which is spanned by the eigenvectors of $A$ corresponding to eigenvalues $\\Lambda_{\\mathrm{in}}$ inside a contour $\\Gamma$.\n\nThe quality of the approximation of a subspace $Q$ to the target subspace $V_{\\mathrm{in}}$ is measured by the sine of the largest principal angle, $\\sin(\\theta_{\\max})$, between them. Let $P_{\\mathrm{in}}$ be the orthogonal projector onto $V_{\\mathrm{in}}$. For an orthonormal basis $Q$, $\\sin(\\theta_{\\max}(Q, V_{\\mathrm{in}})) = \\|(I - P_{\\mathrm{in}})Q\\|_2$. Contraction of the error means that after one step, the new angle $\\theta_{\\max}^{\\mathrm{new}}$ is smaller than the old angle $\\theta_{\\max}^{\\mathrm{old}}$.\n\nIn the exact version of FEAST, the new (unnormalized) basis is $Y_{\\mathrm{new}} = R(A)Q_{\\mathrm{old}}$, where $R(A) = \\sum_{k=1}^{m} w_k (z_k I - A)^{-1}$ is the rational filter approximating the spectral projector. The operator $R(A)$ approximately maps vectors in $V_{\\mathrm{in}}$ to themselves (scaled by a factor close to $1$) and vectors in the orthogonal complement $V_{\\mathrm{out}}$ to zero. The convergence rate is dictated by the ratio of the filter's magnitude on the unwanted spectrum to its magnitude on the desired spectrum. Specifically, the contraction factor for the exact algorithm is $\\rho_{\\mathrm{exact}} = \\beta_{\\mathrm{out}} / \\alpha_{\\mathrm{in}}$, where $\\alpha_{\\mathrm{in}} = \\min_{\\lambda \\in \\Lambda_{\\mathrm{in}}} |R(\\lambda)|$ and $\\beta_{\\mathrm{out}} = \\max_{\\lambda \\in \\Lambda_{\\mathrm{out}}} |R(\\lambda)|$.\n\nWhen inner linear systems $(z_k I - A)X_k = Y$ are solved inexactly, the computed operator is $\\widetilde{R}(A) = R(A) + \\Delta R$, where $\\Delta R = \\sum_{k=1}^m w_k E_k$ is the perturbation due to the sum of individual linear solve error operators $E_k$. The new unnormalized basis is $\\widetilde{Y}_{\\mathrm{new}} = \\widetilde{R}(A) Q_{\\mathrm{old}}$. The error in this new subspace basis can be analyzed by projecting it onto $V_{\\mathrm{out}}$:\n$$\n(I-P_{\\mathrm{in}}) \\widetilde{Y}_{\\mathrm{new}} = (I-P_{\\mathrm{in}}) (R(A) + \\Delta R) Q_{\\mathrm{old}}\n$$\nSince $R(A)$ commutes with $P_{\\mathrm{in}}$ (as $A$ is Hermitian), we have $(I-P_{\\mathrm{in}})R(A) = R(A)(I-P_{\\mathrm{in}})$. Thus,\n$$\n(I-P_{\\mathrm{in}}) \\widetilde{Y}_{\\mathrm{new}} = R(A)(I-P_{\\mathrm{in}})Q_{\\mathrm{old}} + (I-P_{\\mathrm{in}})\\Delta R Q_{\\mathrm{old}}\n$$\nTaking the operator $2$-norm gives a bound on the magnitude of the error component:\n$$\n\\|(I-P_{\\mathrm{in}}) \\widetilde{Y}_{\\mathrm{new}}\\|_2 \\le \\|R(A)(I-P_{\\mathrm{in}})Q_{\\mathrm{old}}\\|_2 + \\|(I-P_{\\mathrm{in}})\\Delta R Q_{\\mathrm{old}}\\|_2\n$$\nThe first term is bounded by $\\|R(A)|_{V_{\\mathrm{out}}}\\|_2 \\|(I-P_{\\mathrm{in}})Q_{\\mathrm{old}}\\|_2 = \\beta_{\\mathrm{out}} \\sin(\\theta_{\\max}^{\\mathrm{old}})$. The second term is bounded by $\\|\\Delta R\\|_2 \\|Q_{\\mathrm{old}}\\|_2 = \\|\\Delta R\\|_2$. Therefore,\n$$\n\\|(I-P_{\\mathrm{in}}) \\widetilde{Y}_{\\mathrm{new}}\\|_2 \\le \\beta_{\\mathrm{out}} \\sin(\\theta_{\\max}^{\\mathrm{old}}) + \\|\\Delta R\\|_2\n$$\nSimilarly, the signal component is $P_{\\mathrm{in}}\\widetilde{Y}_{\\mathrm{new}}$. Its norm can be bounded below by $\\|P_{\\mathrm{in}}\\widetilde{Y}_{\\mathrm{new}}\\|_2 \\ge \\sigma_{\\min}(P_{\\mathrm{in}}\\widetilde{R}(A)Q_{\\mathrm{old}})$. A first-order approximation assumes this norm is dominated by the action of the exact filter $R(A)$, so $\\|P_{\\mathrm{in}}\\widetilde{Y}_{\\mathrm{new}}\\|_2 \\approx \\|R(A)|_{V_{\\mathrm{in}}}\\|_2 \\|P_{\\mathrm{in}}Q_{\\mathrm{old}}\\|_2 \\ge \\alpha_{\\mathrm{in}} \\cos(\\theta_{\\max}^{\\mathrm{old}})$. For a reasonably good initial subspace, $\\cos(\\theta_{\\max}^{\\mathrm{old}}) \\approx 1$.\n\nThe new error angle $\\sin(\\theta_{\\max}^{\\mathrm{new}})$ is the norm of the error component of the normalized basis $Q_{\\mathrm{new}}$. This is approximately the ratio of the norms of the error and signal components of $\\widetilde{Y}_{\\mathrm{new}}$:\n$$\n\\sin(\\theta_{\\max}^{\\mathrm{new}}) \\approx \\frac{\\|(I-P_{\\mathrm{in}}) \\widetilde{Y}_{\\mathrm{new}}\\|_2}{\\|P_{\\mathrm{in}}\\widetilde{Y}_{\\mathrm{new}}\\|_2} \\lesssim \\frac{\\beta_{\\mathrm{out}}\\sin(\\theta_{\\max}^{\\mathrm{old}}) + \\|\\Delta R\\|_2}{\\alpha_{\\mathrm{in}}}\n$$\nFrom this linearized model, the ratio of successive errors, which defines the observed contraction factor $\\rho_{\\mathrm{obs}}$, can be expressed as:\n$$\n\\rho_{\\mathrm{obs}} = \\frac{\\sin(\\theta_{\\max}^{\\mathrm{new}})}{\\sin(\\theta_{\\max}^{\\mathrm{old}})} \\approx \\frac{\\beta_{\\mathrm{out}}}{\\alpha_{\\mathrm{in}}} + \\frac{\\|\\Delta R\\|_2}{\\alpha_{\\mathrm{in}}\\sin(\\theta_{\\max}^{\\mathrm{old}})}\n$$\nThe perturbation norm $\\|\\Delta R\\|_2$ is bounded by $\\|\\Delta R\\|_2 \\le \\sum_{k=1}^m |w_k| \\|E_k\\|_2$. The problem specifies an inexact solve model where the relative residual norm is $\\tau$. Although the exact form of $\\|E_k\\|_2$ depends on the solver, a general bound can be related to $\\tau$. The problem introduces the quantity $S = \\sum_{k=1}^m |w_k| \\|(z_k I - A)^{-1}\\|_2$, and a uniform residual tolerance $\\tau$, which allows us to posit the bound $\\|\\Delta R\\|_2 \\le S\\tau$. Substituting this into our expression for $\\rho_{\\mathrm{obs}}$ yields:\n$$\n\\rho_{\\mathrm{obs}} \\approx \\frac{\\beta_{\\mathrm{out}}}{\\alpha_{\\mathrm{in}}} + \\frac{S\\tau}{\\alpha_{\\mathrm{in}}\\sin(\\theta_{\\max}^{\\mathrm{old}})}\n$$\nThis expression shows that the observed contraction depends on the quality of the current subspace through the term $1/\\sin(\\theta_{\\max}^{\\mathrm{old}})$. The formula for the predicted contraction factor provided in the problem, $\\rho_{\\mathrm{pred}} = \\frac{\\beta_{\\mathrm{out}}}{\\alpha_{\\mathrm{in}}} + \\frac{S\\tau}{\\alpha_{\\mathrm{in}}}$, represents a simplified model that omits this dependency. This can be interpreted as a worst-case bound where the error introduced by the inexactness has its largest relative effect, or simply a first-order model that separates the effects of the ideal filter and the solver errors.\n\nA sufficient condition for contraction ($\\rho_{\\mathrm{obs}} < 1$) can be derived from $\\rho_{\\mathrm{pred}} < 1$. This leads to:\n$$\n\\frac{\\beta_{\\mathrm{out}}}{\\alpha_{\\mathrm{in}}} + \\frac{S\\tau}{\\alpha_{\\mathrm{in}}} < 1 \\implies S\\tau < \\alpha_{\\mathrm{in}} - \\beta_{\\mathrm{out}} \\implies \\tau < \\frac{\\alpha_{\\mathrm{in}} - \\beta_{\\mathrm{out}}}{S}\n$$\nThis gives a threshold $\\tau_{\\max} = (\\alpha_{\\mathrm{in}} - \\beta_{\\mathrm{out}})/S$ for the inner solver tolerance that is sufficient to guarantee contraction under this simplified model. The numerical experiment will evaluate the accuracy of this prediction.", "answer": "```python\nimport numpy as np\nfrom scipy import linalg\n\ndef solve():\n    \"\"\"\n    Performs the FEAST algorithm simulation and comparison as described.\n    \"\"\"\n    # 1. Setup\n    n = 40\n    # A is a diagonal matrix with eigenvalues linearly spaced in [-1.5, 1.5]\n    eigenvalues = np.linspace(-1.5, 1.5, n)\n    # A = np.diag(eigenvalues) # Not explicitly needed as we work with eigenvalues\n\n    # Contour parameters\n    c = 0.0\n    r = 0.45\n    m = 16\n\n    # 2. Eigenvalue partitioning\n    is_inside = np.abs(eigenvalues - c) < r\n    lambda_in = eigenvalues[is_inside]\n    lambda_out = eigenvalues[~is_inside]\n    s = len(lambda_in)\n    p = s + 4\n\n    # Target invariant subspace V_in (spanned by columns of identity matrix)\n    in_indices = np.where(is_inside)[0]\n    V_in = np.zeros((n, s))\n    for i, idx in enumerate(in_indices):\n        V_in[idx, i] = 1.0\n\n    # 3. Quadrature rule\n    thetas = 2 * np.pi * np.arange(m) / m\n    z_k = c + r * np.exp(1j * thetas)\n    # The weights per problem statement are w_k = (r/m) * exp(i*theta_k)\n    # This corresponds to the complex velocity dz/dtheta * dtheta / (2*pi*i) summed\n    # My derivation: (1/(2*pi*i)) * sum(f(z_k) * i*r*exp(i*theta_k)*(2*pi/m))\n    # = sum f(z_k) * (r/m)*exp(i*theta_k). Correct.\n    w_k = (r / m) * np.exp(1j * thetas)\n    \n    # 4. Filter analysis\n    def R_scalar(lam, z_nodes, w_nodes):\n        return np.sum(w_nodes / (z_nodes - lam))\n\n    R_in = np.array([R_scalar(lam, z_k, w_k) for lam in lambda_in])\n    R_out = np.array([R_scalar(lam, z_k, w_k) for lam in lambda_out])\n\n    alpha_in = np.min(np.abs(R_in))\n    beta_out = np.max(np.abs(R_out))\n    \n    # 5. Compute S\n    # ||(z_k*I - A)^-1||_2 = 1 / min_i |z_k - lambda_i|\n    norm_inv_z_minus_A = np.array([1.0 / np.min(np.abs(zk - eigenvalues)) for zk in z_k])\n    S = np.sum(np.abs(w_k) * norm_inv_z_minus_A)\n\n    # 6. Determine tau_max\n    # Sufficient condition for contraction from the model: rho_pred < 1\n    # beta_out/alpha_in + S*tau/alpha_in < 1  => S*tau < alpha_in - beta_out\n    tau_max = (alpha_in - beta_out) / S\n    \n    # 7. Test cases\n    test_cases_sigma = [0.5, 1.0, 1.5]\n    results = []\n\n    # Initial subspace (fixed for all tests)\n    rng = np.random.default_rng(0)\n    Q_old, _ = np.linalg.qr(rng.standard_normal((n, p)))\n\n    # Calculate sin(theta_max_old)\n    # Using scipy.linalg.subspace_angles\n    angles_old = linalg.subspace_angles(V_in, Q_old)\n    sin_theta_max_old = np.sin(angles_old[-1])\n\n    for sigma in test_cases_sigma:\n        tau = sigma * tau_max\n        \n        # Calculate rho_pred\n        rho_pred = beta_out / alpha_in + S * tau / alpha_in\n\n        # Perform one inexact FEAST step\n        Y_new = np.zeros((n, p), dtype=np.complex128)\n        for i in range(m):\n            zk_i = z_k[i]\n            wk_i = w_k[i]\n            \n            # Since A is diagonal, (zI-A)^-1 is diagonal\n            inv_diags = 1.0 / (zk_i - eigenvalues)\n            # Action of (zI-A)^-1 on Q_old\n            Xk_star = inv_diags[:, np.newaxis] * Q_old\n            \n            # Inexact solve\n            Xk_tilde = (1.0 - tau) * Xk_star\n\n            Y_new += wk_i * Xk_tilde\n        \n        # Orthonormalize the new subspace basis. Note: Y_new can be rank deficient,\n        # but qr handles it returning a basis for the column space.\n        Q_new, _ = np.linalg.qr(Y_new)\n\n        # Calculate sin(theta_max_new)\n        angles_new = linalg.subspace_angles(V_in, Q_new)\n        sin_theta_max_new = np.sin(angles_new[-1])\n        \n        # Calculate rho_obs\n        # Avoid division by zero if starting subspace is perfect\n        if sin_theta_max_old > 1e-15:\n            rho_obs = sin_theta_max_new / sin_theta_max_old\n        else:\n            rho_obs = 0.0 if sin_theta_max_new < 1e-15 else np.inf\n\n        results.append([round(rho_pred, 6), round(rho_obs, 6)])\n\n    # Final print statement\n    print(f\"{results}\")\n\nsolve()\n```", "id": "3541107"}]}