{"hands_on_practices": [{"introduction": "The primary motivation for linearization is to convert a polynomial eigenvalue problem into a linear one, which can be solved with standard numerical methods. A crucial question is how the eigensystem of the original polynomial relates to that of its linearization. This first exercise provides a direct, hands-on verification of the fundamental eigenvector recovery property for the first companion form, a cornerstone of linearization theory. [@problem_id:3556322]", "problem": "Consider the monic cubic matrix polynomial $P(\\lambda) \\in \\mathbb{R}^{2 \\times 2}$ defined by\n$$\nP(\\lambda) \\;=\\; \\lambda^{3} I_{2} \\;+\\; \\lambda^{2} A_{2} \\;+\\; \\lambda A_{1} \\;+\\; A_{0},\n$$\nwhere $I_{2}$ is the $2 \\times 2$ identity matrix and\n$$\nA_{2} \\;=\\; \\begin{pmatrix} 0 & 1 \\\\ 0 & 0 \\end{pmatrix}, \\qquad\nA_{1} \\;=\\; \\begin{pmatrix} 0 & 0 \\\\ 1 & 0 \\end{pmatrix}, \\qquad\nA_{0} \\;=\\; -2 I_{2}.\n$$\nLet $\\lambda_{0} \\in \\mathbb{R}$ and $x \\in \\mathbb{R}^{2} \\setminus \\{0\\}$ be such that $P(\\lambda_{0}) x = 0$. Define the first companion pencil $L(\\lambda) = \\lambda \\mathcal{M} - \\mathcal{C} \\in \\mathbb{R}^{6 \\times 6}$ associated with $P(\\lambda)$ using the block matrices\n$$\n\\mathcal{M} \\;=\\; \\begin{pmatrix}\nI_{2} & 0 & 0 \\\\\n0 & I_{2} & 0 \\\\\n0 & 0 & I_{2}\n\\end{pmatrix}, \\qquad\n\\mathcal{C} \\;=\\; \\begin{pmatrix}\n0 & I_{2} & 0 \\\\\n0 & 0 & I_{2} \\\\\n-A_{0} & -A_{1} & -A_{2}\n\\end{pmatrix}.\n$$\nStarting from the core definitions of an eigenpair for a matrix polynomial and the construction of the first companion pencil, carry out the following steps:\n\n1. Determine a specific real eigenvalue $\\lambda_{0}$ and a nonzero vector $x \\in \\mathbb{R}^{2}$ such that $P(\\lambda_{0}) x = 0$.\n\n2. Using only the foundational definitions, compute an eigenvector $v \\in \\mathbb{R}^{6}$ of the companion pencil $L(\\lambda)$ at $\\lambda = \\lambda_{0}$ and explicitly identify its $2 \\times 1$ first block, denoted $v_{1}$.\n\n3. Verify from first principles that the first block $v_{1}$ equals the eigenvector $x$ of $P(\\lambda)$ obtained in step 1, and define the scalar quantity\n$$\nr \\;=\\; \\| v_{1} - x \\|_{2}.\n$$\n\nCompute $r$. Express your final answer as an exact number without rounding.", "solution": "The problem is validated as being scientifically grounded, well-posed, and objective. It is a standard problem in numerical linear algebra concerning the linearization of a matrix polynomial. All provided definitions and matrices are standard and self-consistent.\n\nThe solution proceeds in three steps as requested by the problem statement.\n\n**Step 1: Determine a real eigenvalue $\\lambda_{0}$ and eigenvector $x$ of $P(\\lambda)$.**\n\nThe given matrix polynomial is $P(\\lambda) = \\lambda^{3} I_{2} + \\lambda^{2} A_{2} + \\lambda A_{1} + A_{0}$.\nSubstituting the given matrices $A_{2} = \\begin{pmatrix} 0 & 1 \\\\ 0 & 0 \\end{pmatrix}$, $A_{1} = \\begin{pmatrix} 0 & 0 \\\\ 1 & 0 \\end{pmatrix}$, and $A_{0} = -2 I_{2} = \\begin{pmatrix} -2 & 0 \\\\ 0 & -2 \\end{pmatrix}$, we can write $P(\\lambda)$ explicitly:\n$$\nP(\\lambda) = \\lambda^{3} \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} + \\lambda^{2} \\begin{pmatrix} 0 & 1 \\\\ 0 & 0 \\end{pmatrix} + \\lambda \\begin{pmatrix} 0 & 0 \\\\ 1 & 0 \\end{pmatrix} + \\begin{pmatrix} -2 & 0 \\\\ 0 & -2 \\end{pmatrix}\n$$\n$$\nP(\\lambda) = \\begin{pmatrix} \\lambda^{3} - 2 & \\lambda^{2} \\\\ \\lambda & \\lambda^{3} - 2 \\end{pmatrix}\n$$\nThe eigenvalues $\\lambda$ are the roots of the characteristic equation $\\det(P(\\lambda)) = 0$.\n$$\n\\det(P(\\lambda)) = (\\lambda^{3} - 2)(\\lambda^{3} - 2) - (\\lambda^{2})(\\lambda) = (\\lambda^{3} - 2)^{2} - \\lambda^{3} = 0\n$$\nLet $y = \\lambda^{3}$. The equation becomes a quadratic in $y$:\n$$\n(y - 2)^{2} - y = 0\n$$\n$$\ny^{2} - 4y + 4 - y = 0\n$$\n$$\ny^{2} - 5y + 4 = 0\n$$\nFactoring the quadratic yields:\n$$\n(y - 1)(y - 4) = 0\n$$\nThe solutions for $y$ are $y = 1$ and $y = 4$. Substituting back $y = \\lambda^{3}$, we have two conditions: $\\lambda^{3} = 1$ or $\\lambda^{3} = 4$.\nThe problem requires a real eigenvalue $\\lambda_{0} \\in \\mathbb{R}$.\nFrom $\\lambda^{3} = 1$, the real root is $\\lambda_{0} = 1$.\nFrom $\\lambda^{3} = 4$, the real root is $\\lambda_{0} = \\sqrt[3]{4}$.\nWe select the simpler eigenvalue, $\\lambda_{0} = 1$.\n\nNow, we find a corresponding non-zero eigenvector $x = \\begin{pmatrix} x_{1} \\\\ x_{2} \\end{pmatrix} \\in \\mathbb{R}^{2}$ by solving the linear system $P(\\lambda_{0}) x = 0$.\nFor $\\lambda_{0} = 1$, the matrix is:\n$$\nP(1) = \\begin{pmatrix} 1^{3} - 2 & 1^{2} \\\\ 1 & 1^{3} - 2 \\end{pmatrix} = \\begin{pmatrix} -1 & 1 \\\\ 1 & -1 \\end{pmatrix}\n$$\nThe system $P(1)x = 0$ is:\n$$\n\\begin{pmatrix} -1 & 1 \\\\ 1 & -1 \\end{pmatrix} \\begin{pmatrix} x_{1} \\\\ x_{2} \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n$$\nThis gives the single independent equation $-x_{1} + x_{2} = 0$, which implies $x_{1} = x_{2}$. To obtain a specific non-zero eigenvector, we can choose $x_{1} = 1$, which gives $x_{2} = 1$.\nThus, an eigenpair of $P(\\lambda)$ is $\\lambda_{0} = 1$ and $x = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$.\n\n**Step 2: Compute an eigenvector $v$ of the companion pencil $L(\\lambda)$ at $\\lambda = \\lambda_{0}$.**\n\nThe companion pencil is $L(\\lambda) = \\lambda \\mathcal{M} - \\mathcal{C}$. Given $\\mathcal{M} = I_{6}$, this simplifies to $L(\\lambda) = \\lambda I_{6} - \\mathcal{C}$.\nAn eigenvector $v$ of the pencil for the eigenvalue $\\lambda_{0}$ satisfies $L(\\lambda_{0})v = 0$, which is equivalent to the standard eigenvalue problem $\\mathcal{C}v = \\lambda_{0}v$.\nThe companion matrix $\\mathcal{C}$ is given by:\n$$\n\\mathcal{C} = \\begin{pmatrix} 0 & I_{2} & 0 \\\\ 0 & 0 & I_{2} \\\\ -A_{0} & -A_{1} & -A_{2} \\end{pmatrix}\n$$\nSubstituting the matrices for $A_{0}, A_{1}, A_{2}$:\n$-A_{0} = 2 I_{2} = \\begin{pmatrix} 2 & 0 \\\\ 0 & 2 \\end{pmatrix}$, $-A_{1} = \\begin{pmatrix} 0 & 0 \\\\ -1 & 0 \\end{pmatrix}$, $-A_{2} = \\begin{pmatrix} 0 & -1 \\\\ 0 & 0 \\end{pmatrix}$.\n$$\n\\mathcal{C} = \\left(\\begin{array}{cc|cc|cc}\n0 & 0 & 1 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 1 & 0 & 0 \\\\ \\hline\n0 & 0 & 0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & 1 \\\\ \\hline\n2 & 0 & 0 & 0 & 0 & -1 \\\\\n0 & 2 & -1 & 0 & 0 & 0\n\\end{array}\\right)\n$$\nWe need to find $v \\in \\mathbb{R}^{6}$ such that $\\mathcal{C}v = \\lambda_{0}v$ for $\\lambda_{0} = 1$. This is the system $(\\mathcal{C} - I_{6})v = 0$.\nLet $v = (v_{1}, v_{2}, v_{3}, v_{4}, v_{5}, v_{6})^{T}$. The system of equations is:\n$$\n\\left(\\begin{array}{cccccc}\n-1 & 0 & 1 & 0 & 0 & 0 \\\\\n0 & -1 & 0 & 1 & 0 & 0 \\\\\n0 & 0 & -1 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & -1 & 0 & 1 \\\\\n2 & 0 & 0 & 0 & -1 & -1 \\\\\n0 & 2 & -1 & 0 & 0 & -1\n\\end{array}\\right)\n\\begin{pmatrix} v_{1} \\\\ v_{2} \\\\ v_{3} \\\\ v_{4} \\\\ v_{5} \\\\ v_{6} \\end{pmatrix}\n= \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\nFrom the first four rows, we obtain the relations:\n1.  $-v_{1} + v_{3} = 0 \\implies v_{3} = v_{1}$\n2.  $-v_{2} + v_{4} = 0 \\implies v_{4} = v_{2}$\n3.  $-v_{3} + v_{5} = 0 \\implies v_{5} = v_{3}$\n4.  $-v_{4} + v_{6} = 0 \\implies v_{6} = v_{4}$\n\nCombining these, we get $v_{5} = v_{3} = v_{1}$ and $v_{6} = v_{4} = v_{2}$.\nNow we use the last two rows:\n5.  $2v_{1} - v_{5} - v_{6} = 0 \\implies 2v_{1} - v_{1} - v_{2} = 0 \\implies v_{1} - v_{2} = 0 \\implies v_{1} = v_{2}$\n6.  $2v_{2} - v_{3} - v_{6} = 0 \\implies 2v_{2} - v_{1} - v_{2} = 0 \\implies v_{2} - v_{1} = 0 \\implies v_{1} = v_{2}$\n\nBoth equations give the same condition, $v_{1} = v_{2}$. This implies that all components of $v$ are equal: $v_{1} = v_{2} = v_{3} = v_{4} = v_{5} = v_{6}$.\nTo find a specific non-zero eigenvector, we can set $v_{1} = 1$. This gives $v = (1, 1, 1, 1, 1, 1)^{T}$.\nThe problem asks for the first $2 \\times 1$ block of $v$, denoted $v_{1}$ (this notation for the block is distinct from the component $v_1$). Let's partition $v$ into three $2 \\times 1$ blocks:\n$$\nv = \\begin{pmatrix} v_{1} \\\\ v_{2} \\\\ v_{3} \\\\ v_{4} \\\\ v_{5} \\\\ v_{6} \\end{pmatrix} = \\begin{pmatrix} \\begin{pmatrix} v_{1} \\\\ v_{2} \\end{pmatrix} \\\\ \\begin{pmatrix} v_{3} \\\\ v_{4} \\end{pmatrix} \\\\ \\begin{pmatrix} v_{5} \\\\ v_{6} \\end{pmatrix} \\end{pmatrix}\n$$\nThe first block, which the problem denotes as $v_1$, is $\\begin{pmatrix} v_{1} \\\\ v_{2} \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$.\n\n**Step 3: Verify $v_1 = x$ and compute $r$.**\n\nIn step 1, we found an eigenvector $x = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$ for $P(\\lambda)$ corresponding to $\\lambda_{0} = 1$.\nIn step 2, we found that the first block of the pencil's eigenvector $v$ is $v_{1} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$.\n\nWe are asked to verify that $v_{1} = x$.\nOur specific choices for $x$ and $v$ yield $x = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$ and $v_{1} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$, which are indeed equal. This is consistent with the general theory of linearization, which states that an eigenvector $v$ of the first companion pencil has the form $v = (x^{T}, (\\lambda_{0}x)^{T}, \\dots, (\\lambda_{0}^{k-1}x)^{T})^{T}$, where $x$ is the eigenvector of the polynomial $P(\\lambda)$, $\\lambda_0$ is the eigenvalue, and $k$ is the degree. For our case with $k=3$, the first block of $v$ is precisely $x$.\n\nFinally, we compute the scalar quantity $r = \\| v_{1} - x \\|_{2}$:\n$$\nv_{1} - x = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n$$\n$$\nr = \\left\\| \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} \\right\\|_{2} = \\sqrt{0^{2} + 0^{2}} = 0\n$$\nThe value of $r$ is $0$.", "answer": "$$\n\\boxed{0}\n$$", "id": "3556322"}, {"introduction": "While companion forms are a standard tool for linearization, they are not unique; different block orderings or structures can yield different, yet equally valid, linear pencils. This practice explores the concept of strict equivalence, which provides the theoretical framework for showing that two different linearizations indeed solve the same underlying problem. By explicitly constructing the transformation matrices, you will demonstrate that the first and second companion pencils are fundamentally the same, differing only by a change of basis. [@problem_id:3556327]", "problem": "Let $P(\\lambda) = \\lambda^{3} A_{3} + \\lambda^{2} A_{2} + \\lambda A_{1} + A_{0}$ be a $2 \\times 2$ matrix polynomial with coefficient matrices\n$$\nA_{3} = \\begin{bmatrix} 3 & 1 \\\\ 2 & 1 \\end{bmatrix}, \\quad\nA_{2} = \\begin{bmatrix} -1 & 2 \\\\ 0 & 3 \\end{bmatrix}, \\quad\nA_{1} = \\begin{bmatrix} 4 & -2 \\\\ 1 & 0 \\end{bmatrix}, \\quad\nA_{0} = \\begin{bmatrix} 0 & 5 \\\\ -3 & 1 \\end{bmatrix}.\n$$\nConsider two different companion pencils (linearizations) for $P(\\lambda)$. The first pencil $L_{1}(\\lambda)$ is defined as\n$$\nL_{1}(\\lambda) = \\lambda \\begin{bmatrix}\nI_{2} & 0 & 0 \\\\\n0 & I_{2} & 0 \\\\\n0 & 0 & A_{3}\n\\end{bmatrix}\n-\n\\begin{bmatrix}\n0 & -I_{2} & 0 \\\\\n0 & 0 & -I_{2} \\\\\nA_{0} & A_{1} & A_{2}\n\\end{bmatrix},\n$$\nand the second pencil $L_{2}(\\lambda)$ is defined by reversing the block-row and block-column order of $L_{1}(\\lambda)$:\n$$\nL_{2}(\\lambda) = \\lambda \\begin{bmatrix}\nA_{3} & 0 & 0 \\\\\n0 & I_{2} & 0 \\\\\n0 & 0 & I_{2}\n\\end{bmatrix}\n-\n\\begin{bmatrix}\nA_{2} & A_{1} & A_{0} \\\\\n-I_{2} & 0 & 0 \\\\\n0 & -I_{2} & 0\n\\end{bmatrix}.\n$$\nStart from the foundational definitions:\n- A matrix polynomial $P(\\lambda)$ is a matrix whose entries are polynomials in $\\lambda$.\n- A pencil $L(\\lambda)$ is a matrix linear in $\\lambda$, of the form $L(\\lambda) = \\lambda X - Y$.\n- A polynomial matrix $U(\\lambda)$ is called unimodular if $\\det(U(\\lambda))$ is a nonzero constant (i.e., independent of $\\lambda$).\n- Two pencils $L_{1}(\\lambda)$ and $L_{2}(\\lambda)$ are strictly equivalent if there exist unimodular polynomial matrices $U(\\lambda)$ and $V(\\lambda)$ such that $U(\\lambda) L_{1}(\\lambda) V(\\lambda) = L_{2}(\\lambda)$.\n\nConstruct explicit unimodular matrices $U(\\lambda)$ and $V(\\lambda)$ that transform $L_{1}(\\lambda)$ into $L_{2}(\\lambda)$ and verify their strict equivalence via determinant identities. Conclude by computing the product $\\det(U(\\lambda)) \\det(V(\\lambda))$.\n\nYour final answer must be a single real number or a single closed-form analytic expression. No rounding is required.", "solution": "We begin by recalling the fundamental definitions and facts used throughout numerical linear algebra in the study of linearizations of matrix polynomials. A matrix polynomial $P(\\lambda)$ of grade $d$ is $P(\\lambda) = \\sum_{j=0}^{d} \\lambda^{j} A_{j}$ with $A_{j} \\in \\mathbb{R}^{n \\times n}$. A pencil $L(\\lambda)$ is any matrix of the form $L(\\lambda) = \\lambda X - Y$ with constant matrices $X, Y$. A polynomial matrix $U(\\lambda)$ is unimodular if $\\det(U(\\lambda))$ is a nonzero constant independent of $\\lambda$. Strict equivalence of pencils $L_{1}(\\lambda)$ and $L_{2}(\\lambda)$ means the existence of unimodular $U(\\lambda)$ and $V(\\lambda)$ with $U(\\lambda) L_{1}(\\lambda) V(\\lambda) = L_{2}(\\lambda)$. For unimodular $U(\\lambda)$ and $V(\\lambda)$, we have the determinant identity\n$$\n\\det\\big(U(\\lambda)\\big) \\, \\det\\big(L_{1}(\\lambda)\\big) \\, \\det\\big(V(\\lambda)\\big) \\;=\\; \\det\\big(L_{2}(\\lambda)\\big),\n$$\nwhich expresses that strict equivalence preserves the determinant up to a nonzero constant factor.\n\nWe will construct specific unimodular $U(\\lambda)$ and $V(\\lambda)$ that implement the block reversal from $L_{1}(\\lambda)$ to $L_{2}(\\lambda)$. Let $P_{\\mathrm{rev}} \\in \\mathbb{R}^{3 \\times 3}$ be the permutation matrix that reverses the order of the standard basis vectors, namely\n$$\nP_{\\mathrm{rev}} = \\begin{bmatrix}\n0 & 0 & 1 \\\\\n0 & 1 & 0 \\\\\n1 & 0 & 0\n\\end{bmatrix}.\n$$\nDefine the block-permutation matrix\n$$\n\\Pi := P_{\\mathrm{rev}} \\otimes I_{2},\n$$\nwhere $\\otimes$ denotes the Kronecker product. Then $\\Pi \\in \\mathbb{R}^{6 \\times 6}$ acts by reversing the order of $2 \\times 2$ block rows and columns. Consider\n$$\nU(\\lambda) := \\Pi, \\qquad V(\\lambda) := \\Pi.\n$$\nThese matrices are constant (i.e., independent of $\\lambda$), hence are polynomial matrices. They are unimodular because their determinants are nonzero constants, as we now compute.\n\nWe recall the well-tested formula for determinants of Kronecker products: if $A \\in \\mathbb{R}^{m \\times m}$ and $B \\in \\mathbb{R}^{n \\times n}$, then\n$$\n\\det(A \\otimes B) = \\det(A)^{n} \\, \\det(B)^{m}.\n$$\nApplying this to $A = P_{\\mathrm{rev}}$ and $B = I_{2}$, we obtain\n$$\n\\det(\\Pi) = \\det(P_{\\mathrm{rev}})^{2} \\, \\det(I_{2})^{3} = \\det(P_{\\mathrm{rev}})^{2} \\cdot 1 = \\det(P_{\\mathrm{rev}})^{2}.\n$$\nThe permutation $P_{\\mathrm{rev}}$ on three elements is of odd parity (it is a single transposition $(1\\,3)$), so $\\det(P_{\\mathrm{rev}}) = -1$. Therefore\n$$\n\\det(\\Pi) = (-1)^{2} = 1.\n$$\nThus $U(\\lambda) = \\Pi$ and $V(\\lambda) = \\Pi$ are unimodular with determinants equal to $1$.\n\nNext, we verify that $U(\\lambda) L_{1}(\\lambda) V(\\lambda) = L_{2}(\\lambda)$ explicitly. By construction, $L_{1}(\\lambda)$ has the block structure\n$$\nL_{1}(\\lambda) = \\begin{bmatrix}\n\\lambda I_{2} & I_{2} & 0 \\\\\n0 & \\lambda I_{2} & I_{2} \\\\\n- A_{0} & - A_{1} & \\lambda A_{3} - A_{2}\n\\end{bmatrix}.\n$$\nLeft multiplication by $\\Pi$ reverses the order of the block rows, and right multiplication by $\\Pi$ reverses the order of the block columns. Therefore\n$$\n\\Pi \\, L_{1}(\\lambda) \\, \\Pi\n= \\begin{bmatrix}\n\\lambda A_{3} - A_{2} & - A_{1} & - A_{0} \\\\\nI_{2} & \\lambda I_{2} & 0 \\\\\n0 & I_{2} & \\lambda I_{2}\n\\end{bmatrix}.\n$$\nThis is not quite $L_{2}(\\lambda)$ as stated in the problem. The pencil $L_2(\\lambda)$ is:\n$$\nL_{2}(\\lambda) = \\lambda \\begin{bmatrix}\nA_{3} & 0 & 0 \\\\\n0 & I_{2} & 0 \\\\\n0 & 0 & I_{2}\n\\end{bmatrix}\n- \\begin{bmatrix}\nA_{2} & A_{1} & A_{0} \\\\\n- I_{2} & 0 & 0 \\\\\n0 & - I_{2} & 0\n\\end{bmatrix}\n= \\begin{bmatrix}\n\\lambda A_{3} - A_{2} & - A_{1} & - A_{0} \\\\\nI_{2} & \\lambda I_{2} & 0 \\\\\n0 & I_{2} & \\lambda I_{2}\n\\end{bmatrix}.\n$$\nHence we have the exact equality $U(\\lambda) L_{1}(\\lambda) V(\\lambda) = L_{2}(\\lambda)$ with $U(\\lambda) = \\Pi$ and $V(\\lambda) = \\Pi$.\n\nFinally, we verify the determinant identity associated with strict equivalence and compute the requested product. Since $U(\\lambda)$ and $V(\\lambda)$ are unimodular with $\\det(U(\\lambda)) = \\det(V(\\lambda)) = 1$, it follows that\n$$\n\\det\\big(L_{2}(\\lambda)\\big) = \\det\\big(U(\\lambda)\\big) \\, \\det\\big(L_{1}(\\lambda)\\big) \\, \\det\\big(V(\\lambda)\\big) = 1 \\cdot \\det\\big(L_{1}(\\lambda)\\big) \\cdot 1 = \\det\\big(L_{1}(\\lambda)\\big),\n$$\nwhich proves that the pencils are strictly equivalent and have identical determinants as functions of $\\lambda$. Therefore, the requested product is\n$$\n\\det\\big(U(\\lambda)\\big) \\det\\big(V(\\lambda)\\big) = 1 \\cdot 1 = 1.\n$$", "answer": "$$\\boxed{1}$$", "id": "3556327"}, {"introduction": "The previous practices focused on regular matrix polynomials with finite eigenvalues. This final practice extends the analysis to singular matrix polynomials, which possess infinite eigenvalues and are common in applications. By using the reversal polynomial, you will learn the standard technique for finding the algebraic multiplicity of infinite eigenvalues, thus completing the spectral analysis of the polynomial eigenvalue problem for $P(\\lambda)$. [@problem_id:3556363]", "problem": "Let $P(\\lambda) = A_{3}\\lambda^{3} + A_{2}\\lambda^{2} + A_{1}\\lambda + A_{0}$ be a degree-$3$ matrix polynomial of size $n\\times n$ with $n = 2$, where\n$$\nA_{3} = \\begin{pmatrix} 0 & 0 \\\\ 0 & 1 \\end{pmatrix},\\quad\nA_{2} = \\begin{pmatrix} 1 & 1 \\\\ 0 & 1 \\end{pmatrix},\\quad\nA_{1} = \\begin{pmatrix} 0 & 1 \\\\ -1 & 0 \\end{pmatrix},\\quad\nA_{0} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 2 \\end{pmatrix}.\n$$\nAssume $P$ is regular, meaning $\\det P(\\lambda)$ is not the zero polynomial. The reversal of $P$ of grade $3$ is defined by $\\operatorname{rev}_{3}P(\\lambda) := A_3 + \\lambda A_2 + \\lambda^2 A_1 + \\lambda^3 A_0$.\n\nUsing only the foundational definitions of a matrix polynomial, its eigenvalues (finite and at infinity) as roots of $\\det P(\\lambda)$ and of the reversed polynomial, and the definition of reversal $\\operatorname{rev}_{3}P(\\lambda)$, do the following:\n\n- Construct $\\operatorname{rev}_{3}P(\\lambda)$ explicitly.\n- Starting from the definitions, justify the correspondence between finite eigenvalues of $\\operatorname{rev}_{3}P$ and the eigenvalue at infinity of $P$, and explain how the algebraic multiplicity of the eigenvalue at infinity of $P$ can be obtained from the order of vanishing of $\\det(\\operatorname{rev}_{3}P(\\lambda))$ at $\\lambda = 0$.\n- Compute $\\det(\\operatorname{rev}_{3}P(\\lambda))$ for the data above and use it to determine the algebraic multiplicity $m_{\\infty}$ of the eigenvalue at infinity of $P$.\n\nProvide your final answer as the single integer $m_{\\infty}$. No rounding is required.", "solution": "The problem statement is a well-posed exercise in the field of numerical linear algebra, specifically concerning the theory of matrix polynomials. All terms, such as the matrix polynomial $P(\\lambda)$, its coefficient matrices $A_i$, the notion of a regular polynomial, the definition of the reversal polynomial $\\operatorname{rev}_{3}P(\\lambda)$, and the concept of eigenvalues at infinity and their algebraic multiplicity, are standard and rigorously defined. The problem provides all necessary data and definitions for a unique solution to be determined through mathematical reasoning and computation. It is scientifically grounded, objective, and internally consistent. Therefore, the problem is deemed valid.\n\nThe solution is structured as follows: first, the reversal polynomial $\\operatorname{rev}_{3}P(\\lambda)$ is explicitly constructed. Second, the theoretical justification for relating the eigenvalues at infinity of $P(\\lambda)$ to the zero eigenvalues of $\\operatorname{rev}_{3}P(\\lambda)$ is provided. Finally, the determinant of $\\operatorname{rev}_{3}P(\\lambda)$ is computed to find the algebraic multiplicity $m_{\\infty}$ of the eigenvalue at infinity.\n\nThe given matrix polynomial is $P(\\lambda) = A_{3}\\lambda^{3} + A_{2}\\lambda^{2} + A_{1}\\lambda + A_{0}$, with coefficient matrices:\n$$\nA_{3} = \\begin{pmatrix} 0 & 0 \\\\ 0 & 1 \\end{pmatrix},\\quad\nA_{2} = \\begin{pmatrix} 1 & 1 \\\\ 0 & 1 \\end{pmatrix},\\quad\nA_{1} = \\begin{pmatrix} 0 & 1 \\\\ -1 & 0 \\end{pmatrix},\\quad\nA_{0} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 2 \\end{pmatrix}.\n$$\nThe reversal of $P$ of grade $3$ is defined as $\\operatorname{rev}_{3}P(\\lambda) = A_3 + \\lambda A_2 + \\lambda^2 A_1 + \\lambda^3 A_0$.\n\nFirst, we construct $\\operatorname{rev}_{3}P(\\lambda)$ by substituting the given matrices:\n$$\n\\operatorname{rev}_{3}P(\\lambda) = \\begin{pmatrix} 0 & 0 \\\\ 0 & 1 \\end{pmatrix} + \\lambda\\begin{pmatrix} 1 & 1 \\\\ 0 & 1 \\end{pmatrix} + \\lambda^{2}\\begin{pmatrix} 0 & 1 \\\\ -1 & 0 \\end{pmatrix} + \\lambda^{3}\\begin{pmatrix} 1 & 0 \\\\ 0 & 2 \\end{pmatrix}\n$$\n$$\n\\operatorname{rev}_{3}P(\\lambda) = \\begin{pmatrix} \\lambda + \\lambda^3 & \\lambda + \\lambda^2 \\\\ -\\lambda^2 & 1 + \\lambda + 2\\lambda^3 \\end{pmatrix}\n$$\n\nNext, we justify the connection. By definition, eigenvalues at infinity for $P(\\lambda)$ are investigated via the substitution $\\lambda = 1/\\mu$, analyzing the behavior as $\\mu \\to 0$. The reversal polynomial is formally defined as $\\operatorname{rev}_d P(\\mu) = \\mu^d P(1/\\mu)$. For our grade-3 polynomial, this gives:\n$$\n\\operatorname{rev}_{3}P(\\mu) = \\mu^3 P(1/\\mu) = \\mu^3 \\left(A_3 (1/\\mu)^3 + A_2 (1/\\mu)^2 + A_1 (1/\\mu) + A_0\\right) = A_3 + \\mu A_2 + \\mu^2 A_1 + \\mu^3 A_0\n$$\nThis matches the definition provided in the problem. The eigenvalues at infinity of $P(\\lambda)$ are, by definition, the eigenvalues at $\\mu=0$ of the reversal polynomial $\\operatorname{rev}_{3}P(\\mu)$. A value $\\mu_0=0$ is an eigenvalue of $\\operatorname{rev}_{3}P(\\mu)$ if $\\det(\\operatorname{rev}_{3}P(0))=0$. The algebraic multiplicity of an eigenvalue is its multiplicity as a root of the characteristic polynomial. Therefore, the algebraic multiplicity of the eigenvalue at infinity of $P(\\lambda)$, denoted $m_{\\infty}$, is the algebraic multiplicity of the eigenvalue $0$ of $\\operatorname{rev}_{3}P(\\lambda)$. This multiplicity is the order of the root $\\lambda=0$ of $\\det(\\operatorname{rev}_{3}P(\\lambda)) = 0$, which is equivalent to the exponent of the lowest power of $\\lambda$ in the polynomial expansion of $\\det(\\operatorname{rev}_{3}P(\\lambda))$.\n\nNow, we compute $\\det(\\operatorname{rev}_{3}P(\\lambda))$ using the matrix derived earlier:\n$$\n\\det(\\operatorname{rev}_{3}P(\\lambda)) = \\det \\begin{pmatrix} \\lambda + \\lambda^3 & \\lambda + \\lambda^2 \\\\ -\\lambda^2 & 1 + \\lambda + 2\\lambda^3 \\end{pmatrix}\n$$\n$$\n\\det(\\operatorname{rev}_{3}P(\\lambda)) = (\\lambda + \\lambda^3)(1 + \\lambda + 2\\lambda^3) - (\\lambda + \\lambda^2)(-\\lambda^2)\n$$\nWe expand the first term:\n$$\n(\\lambda + \\lambda^3)(1 + \\lambda + 2\\lambda^3) = \\lambda(1 + \\lambda + 2\\lambda^3) + \\lambda^3(1 + \\lambda + 2\\lambda^3)\n$$\n$$\n= (\\lambda + \\lambda^2 + 2\\lambda^4) + (\\lambda^3 + \\lambda^4 + 2\\lambda^6) = \\lambda + \\lambda^2 + \\lambda^3 + 3\\lambda^4 + 2\\lambda^6\n$$\nWe expand the second term:\n$$\n-(\\lambda + \\lambda^2)(-\\lambda^2) = (\\lambda + \\lambda^2)(\\lambda^2) = \\lambda^3 + \\lambda^4\n$$\nCombining the terms:\n$$\n\\det(\\operatorname{rev}_{3}P(\\lambda)) = (\\lambda + \\lambda^2 + \\lambda^3 + 3\\lambda^4 + 2\\lambda^6) + (\\lambda^3 + \\lambda^4)\n$$\n$$\n\\det(\\operatorname{rev}_{3}P(\\lambda)) = 2\\lambda^6 + 4\\lambda^4 + 2\\lambda^3 + \\lambda^2 + \\lambda\n$$\nWe can factor out $\\lambda$ from this polynomial:\n$$\n\\det(\\operatorname{rev}_{3}P(\\lambda)) = \\lambda(2\\lambda^{5} + 4\\lambda^{3} + 2\\lambda^{2} + \\lambda + 1)\n$$\nThe polynomial inside the parentheses, $q(\\lambda) = 2\\lambda^{5} + 4\\lambda^{3} + 2\\lambda^{2} + \\lambda + 1$, has a constant term of $1$. Therefore, $q(0) = 1 \\neq 0$. This shows that $\\lambda=0$ is a simple root of the polynomial $\\det(\\operatorname{rev}_{3}P(\\lambda))$. The order of vanishing at $\\lambda=0$ is $1$.\n\nBased on the reasoning established, the algebraic multiplicity $m_{\\infty}$ of the eigenvalue at infinity of $P(\\lambda)$ is equal to this order of vanishing. Therefore, $m_{\\infty} = 1$.", "answer": "$$\n\\boxed{1}\n$$", "id": "3556363"}]}