{"hands_on_practices": [{"introduction": "The Lanczos bidiagonalization process elegantly reduces a large, often dense, matrix to a much smaller and highly structured bidiagonal form. This practice focuses on the crucial next step: computing the singular values of this resultant bidiagonal matrix [@problem_id:3539923]. By deriving a closed-form solution for a canonical bidiagonal matrix, you will gain fundamental insight into the direct relationship between a matrix's structure and its singular spectrum, a cornerstone of spectral analysis and numerical algorithms.", "problem": "Consider a lower bidiagonal matrix $B \\in \\mathbb{R}^{(n+1)\\times n}$ with $n \\geq 2$ whose nonzero entries are constant along the diagonal and the first subdiagonal: specifically, $B_{i,i} = a$ for $i = 1, 2, \\dots, n$ and $B_{i+1,i} = b$ for $i = 1, 2, \\dots, n$, with $a  0$ and $b  0$. This structure is the canonical bidiagonal form one obtains when applying the Golub–Kahan bidiagonalization (Lanczos bidiagonalization) process to certain shift-invariant operators; computing the singular value decomposition (SVD) of such a bidiagonal is a critical step in practical algorithms for approximating the SVD of a general rectangular matrix.\n\nStarting from first principles — namely, the definition that the singular values of a matrix are the square roots of the eigenvalues of its Gram matrix $B^{\\top}B$ — derive a closed-form analytic expression for the largest singular value of $B$ as a function of $a$, $b$, and $n$. Your final answer must be a single closed-form expression. No numerical evaluation and no rounding are required, and no units should be included.", "solution": "The problem requires the derivation of a closed-form analytic expression for the largest singular value of a specific bidiagonal matrix $B$.\n\nThe matrix $B \\in \\mathbb{R}^{(n+1)\\times n}$, with $n \\geq 2$, is a lower bidiagonal matrix whose nonzero entries are given by $B_{i,i} = a$ for $i = 1, 2, \\dots, n$ and $B_{i+1,i} = b$ for $i = 1, 2, \\dots, n$. The parameters $a$ and $b$ are positive real numbers, $a  0$ and $b  0$. All other entries of $B$ are zero. The explicit structure of $B$ is:\n$$\nB = \\begin{pmatrix}\na  0  \\cdots  \\cdots  0 \\\\\nb  a  0  \\cdots  0 \\\\\n0  b  a  \\ddots  \\vdots \\\\\n\\vdots  \\ddots  \\ddots  \\ddots  0 \\\\\n0  \\cdots  0  b  a \\\\\n0  \\cdots  \\cdots  0  b\n\\end{pmatrix}\n$$\nThe problem states that the singular values, $\\sigma$, of $B$ are the square roots of the eigenvalues, $\\lambda$, of its Gram matrix, $T = B^{\\top}B$. Our first step is to compute this matrix. The matrix $T$ is of size $n \\times n$. The columns of $B$, denoted by $c_j$ for $j=1, \\dots, n$, are vectors in $\\mathbb{R}^{n+1}$. The $j$-th column is $c_j = a e_j + b e_{j+1}$, where $\\{e_k\\}_{k=1}^{n+1}$ are the standard basis vectors in $\\mathbb{R}^{n+1}$.\n\nThe entries of $T$ are given by the inner products $T_{ij} = c_i^{\\top}c_j$.\nFor the diagonal entries ($i=j$):\n$$\nT_{ii} = c_i^{\\top}c_i = (a e_i + b e_{i+1})^{\\top}(a e_i + b e_{i+1}) = a^2 e_i^{\\top}e_i + 2ab e_i^{\\top}e_{i+1} + b^2 e_{i+1}^{\\top}e_{i+1} = a^2(1) + 2ab(0) + b^2(1) = a^2+b^2\n$$\nFor the first off-diagonal entries ($j = i+1$):\n$$\nT_{i,i+1} = c_i^{\\top}c_{i+1} = (a e_i + b e_{i+1})^{\\top}(a e_{i+1} + b e_{i+2}) = a^2 e_i^{\\top}e_{i+1} + ab e_i^{\\top}e_{i+2} + ab e_{i+1}^{\\top}e_{i+1} + b^2 e_{i+1}^{\\top}e_{i+2} = ab\n$$\nSince $T$ is symmetric, we have $T_{i+1,i} = T_{i,i+1} = ab$. For any pair of indices $(i, j)$ such that $|i-j|  1$, the vectors $c_i$ and $c_j$ have disjoint support, so their inner product $c_i^{\\top}c_j$ is $0$.\n\nThus, $T = B^{\\top}B$ is an $n \\times n$ symmetric tridiagonal Toeplitz matrix:\n$$\nT = \\begin{pmatrix}\na^2+b^2  ab  0  \\cdots  0 \\\\\nab  a^2+b^2  ab  \\ddots  \\vdots \\\\\n0  ab  a^2+b^2  \\ddots  0 \\\\\n\\vdots  \\ddots  \\ddots  \\ddots  ab \\\\\n0  \\cdots  0  ab  a^2+b^2\n\\end{pmatrix}\n$$\nThe eigenvalues of a general $n \\times n$ symmetric tridiagonal Toeplitz matrix with diagonal entries $\\alpha$ and off-diagonal entries $\\beta$ are given by the well-established formula:\n$$\n\\lambda_k = \\alpha + 2\\beta \\cos\\left(\\frac{k\\pi}{n+1}\\right), \\quad \\text{for } k = 1, 2, \\dots, n\n$$\nFor the matrix $T$, we have $\\alpha = a^2+b^2$ and $\\beta = ab$. Substituting these into the general formula, we get the eigenvalues of $T = B^{\\top}B$:\n$$\n\\lambda_k = (a^2+b^2) + 2ab \\cos\\left(\\frac{k\\pi}{n+1}\\right), \\quad \\text{for } k = 1, 2, \\dots, n\n$$\nWe seek the largest singular value of $B$, which is $\\sigma_{\\text{max}} = \\sqrt{\\lambda_{\\text{max}}}$, where $\\lambda_{\\text{max}}$ is the largest eigenvalue of $T$. To find $\\lambda_{\\text{max}}$, we must maximize $\\lambda_k$ with respect to the index $k$:\n$$\n\\lambda_{\\text{max}} = \\max_{k \\in \\{1, 2, \\dots, n\\}} \\left[ (a^2+b^2) + 2ab \\cos\\left(\\frac{k\\pi}{n+1}\\right) \\right]\n$$\nGiven that $a  0$ and $b  0$, the coefficient $2ab$ is positive. Consequently, maximizing $\\lambda_k$ is equivalent to maximizing the term $\\cos\\left(\\frac{k\\pi}{n+1}\\right)$.\nFor $k=1, 2, \\dots, n$, the argument of the cosine function, $\\theta_k = \\frac{k\\pi}{n+1}$, lies in the interval $(0, \\pi)$, more precisely, $\\frac{\\pi}{n+1} \\leq \\theta_k \\leq \\frac{n\\pi}{n+1}$.\nThe cosine function is strictly decreasing on the interval $[0, \\pi]$. Therefore, its maximum value for the set of arguments $\\{\\theta_1, \\theta_2, \\dots, \\theta_n\\}$ is achieved at the smallest argument, which corresponds to $k=1$. The maximum value of the cosine term is thus $\\cos\\left(\\frac{\\pi}{n+1}\\right)$.\n\nSubstituting $k=1$ into the eigenvalue formula yields the largest eigenvalue, $\\lambda_{\\text{max}}$:\n$$\n\\lambda_{\\text{max}} = (a^2+b^2) + 2ab \\cos\\left(\\frac{\\pi}{n+1}\\right)\n$$\nThe singular values of $B$ are non-negative, being the square roots of the eigenvalues of the positive semi-definite matrix $B^\\top B$. The largest singular value is the square root of the largest eigenvalue:\n$$\n\\sigma_{\\text{max}} = \\sqrt{\\lambda_{\\text{max}}} = \\sqrt{a^2+b^2 + 2ab \\cos\\left(\\frac{\\pi}{n+1}\\right)}\n$$\nThis expression constitutes the required closed-form analytic expression for the largest singular value of $B$ as a function of $a$, $b$, and $n$.", "answer": "$$ \\boxed{ \\sqrt{a^2+b^2 + 2ab \\cos\\left(\\frac{\\pi}{n+1}\\right)} } $$", "id": "3539923"}, {"introduction": "In practice, Lanczos bidiagonalization is an iterative method, and its efficiency hinges on knowing when to stop. This exercise guides you through the derivation of a simple and computationally inexpensive formula for the residual error of an approximate singular triplet, known as a Ritz triplet [@problem_id:3539929]. Understanding this calculation is essential for implementing robust stopping criteria, which allow iterative SVD solvers to terminate as soon as a desired accuracy is reached, thus saving valuable computational effort.", "problem": "Consider a real matrix $A \\in \\mathbb{R}^{m \\times n}$ and the Golub–Kahan (GK) bidiagonalization after $k$ steps, which produces orthonormal matrices $U_k \\in \\mathbb{R}^{m \\times k}$ and $V_k \\in \\mathbb{R}^{n \\times k}$, scalars $\\alpha_1,\\dots,\\alpha_k$ and $\\beta_1,\\dots,\\beta_k$, and a bidiagonal matrix $B_k \\in \\mathbb{R}^{k \\times k}$ with diagonal entries $\\alpha_i$ and subdiagonal entries $\\beta_i$. By construction, the GK relations are\n$$\nA V_k = U_k B_k, \\qquad A^{\\top} U_k = V_k B_k^{\\top} + \\beta_k v_{k+1} e_k^{\\top},\n$$\nwhere $v_{k+1} \\in \\mathbb{R}^{n}$ is a unit vector orthogonal to the columns of $V_k$, and $e_k \\in \\mathbb{R}^{k}$ is the $k$-th canonical basis vector.\n\nLet $B_k$ have the Singular Value Decomposition (SVD) $B_k \\widehat{q}_i = \\widehat{\\sigma}_i \\widehat{p}_i$ and $B_k^{\\top} \\widehat{p}_i = \\widehat{\\sigma}_i \\widehat{q}_i$, with $\\widehat{\\sigma}_i  0$, $\\widehat{p}_i \\in \\mathbb{R}^{k}$, and $\\widehat{q}_i \\in \\mathbb{R}^{k}$ orthonormal. The corresponding Ritz approximations to the singular triplets of $A$ are $u_i = U_k \\widehat{p}_i$, $v_i = V_k \\widehat{q}_i$, and $\\sigma_i \\approx \\widehat{\\sigma}_i$.\n\nA widely used stopping criterion for computing the Singular Value Decomposition (SVD) via GK requires the relative residual of the pair $A^{\\top} u_i \\approx \\sigma_i v_i$ to be smaller than a prescribed tolerance. Specifically, the relative residual for the $i$-th Ritz triplet is\n$$\n\\rho_i \\equiv \\frac{\\|A^{\\top} u_i - \\widehat{\\sigma}_i v_i\\|_2}{\\widehat{\\sigma}_i}.\n$$\n\nSuppose that after $k$ steps, for the dominant triplet $i=1$, you are given the following quantities: the next GK subdiagonal coefficient $\\beta_k = 1.7 \\times 10^{-9}$, the last component of the left singular vector of $B_k$, $e_k^{\\top} \\widehat{p}_1 = 0.45$, and the Ritz singular value $\\widehat{\\sigma}_1 = 3.2$. Using only the GK relations above and the definition of $\\rho_1$, compute the value of the relative residual $\\rho_1$. Round your final answer to four significant figures.", "solution": "The user wants to compute the relative residual $\\rho_1$ for the dominant Ritz triplet after $k$ steps of the Golub-Kahan (GK) bidiagonalization. The problem is first validated to ensure its scientific and mathematical integrity.\n\n### Step 1: Extract Givens\n- A real matrix $A \\in \\mathbb{R}^{m \\times n}$.\n- Golub-Kahan (GK) bidiagonalization relations after $k$ steps:\n  $$A V_k = U_k B_k$$\n  $$A^{\\top} U_k = V_k B_k^{\\top} + \\beta_k v_{k+1} e_k^{\\top}$$\n- Orthonormal matrices $U_k \\in \\mathbb{R}^{m \\times k}$ and $V_k \\in \\mathbb{R}^{n \\times k}$.\n- $v_{k+1} \\in \\mathbb{R}^{n}$ is a unit vector orthogonal to the columns of $V_k$.\n- $e_k \\in \\mathbb{R}^k$ is the $k$-th canonical basis vector.\n- Bidiagonal matrix $B_k \\in \\mathbb{R}^{k \\times k}$.\n- SVD of $B_k$ for the $i$-th singular triplet: $B_k \\widehat{q}_i = \\widehat{\\sigma}_i \\widehat{p}_i$ and $B_k^{\\top} \\widehat{p}_i = \\widehat{\\sigma}_i \\widehat{q}_i$, with $\\widehat{\\sigma}_i  0$.\n- Ritz approximations for the $i$-th triplet of $A$: $u_i = U_k \\widehat{p}_i$, $v_i = V_k \\widehat{q}_i$, and singular value $\\sigma_i \\approx \\widehat{\\sigma}_i$.\n- Definition of the relative residual for the $i$-th Ritz triplet:\n  $$\\rho_i \\equiv \\frac{\\|A^{\\top} u_i - \\widehat{\\sigma}_i v_i\\|_2}{\\widehat{\\sigma}_i}$$\n- Given values for the dominant triplet ($i=1$):\n  - $\\beta_k = 1.7 \\times 10^{-9}$\n  - $e_k^{\\top} \\widehat{p}_1 = 0.45$\n  - $\\widehat{\\sigma}_1 = 3.2$\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded in the well-established theory of numerical linear algebra, specifically the Lanczos bidiagonalization method (equivalent to Golub-Kahan) for computing the SVD of a matrix. The provided GK relations are standard. The definitions of Ritz vectors, Ritz values, and residuals are correct. The problem is well-posed, as it provides all necessary data and relations to uniquely determine the requested value. The language is objective and precise. There are no contradictions, missing information, or pseudoscientific claims. The provided numerical values are physically and mathematically plausible within the context of a converging iterative algorithm. Therefore, the problem is valid.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\nThe objective is to compute the relative residual $\\rho_1$. We begin with its definition:\n$$\n\\rho_1 = \\frac{\\|A^{\\top} u_1 - \\widehat{\\sigma}_1 v_1\\|_2}{\\widehat{\\sigma}_1}\n$$\nLet's analyze the residual vector in the numerator, which we denote as $r_1$:\n$$\nr_1 = A^{\\top} u_1 - \\widehat{\\sigma}_1 v_1\n$$\nThe Ritz vector $u_1$ is defined as $u_1 = U_k \\widehat{p}_1$. Substituting this into the expression for $r_1$:\n$$\nr_1 = A^{\\top} (U_k \\widehat{p}_1) - \\widehat{\\sigma}_1 v_1 = (A^{\\top} U_k) \\widehat{p}_1 - \\widehat{\\sigma}_1 v_1\n$$\nNow, we use the second of the given GK relations, $A^{\\top} U_k = V_k B_k^{\\top} + \\beta_k v_{k+1} e_k^{\\top}$, to substitute for the term $(A^{\\top} U_k)$:\n$$\nr_1 = (V_k B_k^{\\top} + \\beta_k v_{k+1} e_k^{\\top}) \\widehat{p}_1 - \\widehat{\\sigma}_1 v_1\n$$\nApplying the distributive property:\n$$\nr_1 = V_k B_k^{\\top} \\widehat{p}_1 + (\\beta_k v_{k+1} e_k^{\\top}) \\widehat{p}_1 - \\widehat{\\sigma}_1 v_1\n$$\nThe term $(e_k^{\\top} \\widehat{p}_1)$ is a scalar, representing the $k$-th component of the vector $\\widehat{p}_1$. So, the expression becomes:\n$$\nr_1 = V_k (B_k^{\\top} \\widehat{p}_1) + \\beta_k (e_k^{\\top} \\widehat{p}_1) v_{k+1} - \\widehat{\\sigma}_1 v_1\n$$\nNext, we use the property of the SVD of $B_k$, specifically $B_k^{\\top} \\widehat{p}_1 = \\widehat{\\sigma}_1 \\widehat{q}_1$. Substituting this into the equation:\n$$\nr_1 = V_k (\\widehat{\\sigma}_1 \\widehat{q}_1) + \\beta_k (e_k^{\\top} \\widehat{p}_1) v_{k+1} - \\widehat{\\sigma}_1 v_1\n$$\nThe first term can be rewritten as $\\widehat{\\sigma}_1 (V_k \\widehat{q}_1)$. From the definition of the Ritz vector $v_1 = V_k \\widehat{q}_1$, we have:\n$$\nr_1 = \\widehat{\\sigma}_1 v_1 + \\beta_k (e_k^{\\top} \\widehat{p}_1) v_{k+1} - \\widehat{\\sigma}_1 v_1\n$$\nThe terms $\\widehat{\\sigma}_1 v_1$ and $-\\widehat{\\sigma}_1 v_1$ cancel each other out, leaving a remarkably simple expression for the residual vector:\n$$\nr_1 = \\beta_k (e_k^{\\top} \\widehat{p}_1) v_{k+1}\n$$\nNow, we compute the Euclidean norm of this vector:\n$$\n\\|r_1\\|_2 = \\| \\beta_k (e_k^{\\top} \\widehat{p}_1) v_{k+1} \\|_2\n$$\nSince $\\beta_k$ and $(e_k^{\\top} \\widehat{p}_1)$ are scalars, we can factor them out of the norm. The off-diagonal entries $\\beta_i$ in the GK process are non-negative by construction, so $|\\beta_k| = \\beta_k$.\n$$\n\\|r_1\\|_2 = \\beta_k |e_k^{\\top} \\widehat{p}_1| \\|v_{k+1}\\|_2\n$$\nThe problem states that $v_{k+1}$ is a unit vector, which means $\\|v_{k+1}\\|_2 = 1$. Therefore, the norm of the residual is:\n$$\n\\|r_1\\|_2 = \\beta_k |e_k^{\\top} \\widehat{p}_1|\n$$\nFinally, we substitute this result back into the formula for the relative residual $\\rho_1$:\n$$\n\\rho_1 = \\frac{\\beta_k |e_k^{\\top} \\widehat{p}_1|}{\\widehat{\\sigma}_1}\n$$\nNow we can insert the given numerical values: $\\beta_k = 1.7 \\times 10^{-9}$, $e_k^{\\top} \\widehat{p}_1 = 0.45$, and $\\widehat{\\sigma}_1 = 3.2$.\n$$\n\\rho_1 = \\frac{(1.7 \\times 10^{-9}) |0.45|}{3.2}\n$$\n$$\n\\rho_1 = \\frac{1.7 \\times 0.45 \\times 10^{-9}}{3.2} = \\frac{0.765 \\times 10^{-9}}{3.2}\n$$\n$$\n\\rho_1 = 0.2390625 \\times 10^{-9} = 2.390625 \\times 10^{-10}\n$$\nThe problem requires the answer to be rounded to four significant figures. The number is $2.390625 \\times 10^{-10}$. The fifth significant digit is $6$, so we round up the fourth digit.\n$$\n\\rho_1 \\approx 2.391 \\times 10^{-10}\n$$\nThis is the value of the relative residual for the dominant Ritz triplet.", "answer": "$$\n\\boxed{2.391 \\times 10^{-10}}\n$$", "id": "3539929"}, {"introduction": "Beyond just computing singular values, a primary application of Lanczos bidiagonalization is to find high-quality low-rank approximations of a matrix. This practice challenges you to establish a rigorous a posteriori error bound, which quantifies the accuracy of the computed approximation [@problem_id:3539884]. By connecting the total approximation error to quantities that are readily available from the bidiagonalization process, you will develop a deeper appreciation for the theoretical guarantees that make this method a reliable tool in scientific computing and data analysis.", "problem": "Let $A \\in \\mathbb{R}^{m \\times n}$ and suppose that $k$ steps of the Golub–Kahan Lanczos bidiagonalization applied to $A$ produce orthonormal bases $U_k \\in \\mathbb{R}^{m \\times k}$ and $V_k \\in \\mathbb{R}^{n \\times k}$ and a real upper-bidiagonal matrix $B_k \\in \\mathbb{R}^{k \\times k}$ such that $U_k^{\\top} A V_k = B_k$. Let the Singular Value Decomposition (SVD) of $B_k$ be $B_k = \\widetilde{U} \\Sigma \\widetilde{V}^{\\top}$, where $\\Sigma = \\operatorname{diag}(\\sigma_1(B_k),\\dots,\\sigma_k(B_k))$ with $\\sigma_1(B_k) \\ge \\cdots \\ge \\sigma_k(B_k) \\ge 0$, and define the rank-$r$ approximation within the subspaces by $\\widehat{U}_r := U_k \\widetilde{U}_r$, $\\widehat{V}_r := V_k \\widetilde{V}_r$, and $\\Sigma_r := \\operatorname{diag}(\\sigma_1(B_k),\\dots,\\sigma_r(B_k))$, where $\\widetilde{U}_r$ and $\\widetilde{V}_r$ collect the first $r$ columns of $\\widetilde{U}$ and $\\widetilde{V}$, respectively. Denote the orthogonal projectors by $P_U := U_k U_k^{\\top}$ and $P_V := V_k V_k^{\\top}$.\n\nStarting only from the definitions of orthogonal projectors, the variational characterization of singular values, and the Eckart–Young–Mirsky optimality of truncated Singular Value Decomposition (SVD), derive an a posteriori upper bound for the spectral norm error $\\|A - \\widehat{U}_r \\Sigma_r \\widehat{V}_r^{\\top}\\|_2$ in terms of the projection-residual operators $R_U := (I - P_U) A$ and $R_V := A (I - P_V)$ and the neglected singular values of $B_k$. Specifically, prove a bound of the form\n$$\n\\|A - \\widehat{U}_r \\Sigma_r \\widehat{V}_r^{\\top}\\|_2 \\le \\|R_U\\|_2 + \\|R_V\\|_2 + \\sigma_{r+1}(B_k),\n$$\nwhere by convention $\\sigma_{k+1}(B_k) := 0$ if $r = k$.\n\nThen evaluate this bound numerically under the following certified a posteriori quantities obtained from the bidiagonalization process:\n- $\\|R_U\\|_2 = \\frac{3}{50}$,\n- $\\|R_V\\|_2 = \\frac{1}{25}$,\n- $\\sigma_{r+1}(B_k) = \\frac{1}{20}$.\n\nYour final answer must be the single real number given by this bound. Do not round; provide the exact value as a reduced fraction.", "solution": "The problem requires the derivation of an a posteriori error bound for a rank-$r$ approximation of a matrix $A$ obtained from $k$ steps of the Golub-Kahan Lanczos bidiagonalization, and then the numerical evaluation of this bound.\n\nFirst, we prove the inequality $\\|A - \\widehat{U}_r \\Sigma_r \\widehat{V}_r^{\\top}\\|_2 \\le \\|R_U\\|_2 + \\|R_V\\|_2 + \\sigma_{r+1}(B_k)$. Let $E_r = A - \\widehat{U}_r \\Sigma_r \\widehat{V}_r^{\\top}$ denote the error matrix. Our goal is to derive an upper bound for the spectral norm $\\|E_r\\|_2$.\n\nWe introduce an intermediate matrix, $A_k$, which is the projection of $A$ onto the subspaces spanned by the columns of $U_k$ and $V_k$. The projectors onto the column space of $U_k$ (range($U_k$)) and the column space of $V_k$ (range($V_k$)) are given by $P_U = U_k U_k^{\\top}$ and $P_V = V_k V_k^{\\top}$, respectively. We define $A_k := P_U A P_V$.\n\nUsing the triangle inequality for the spectral norm, we can decompose the error as follows:\n$$\n\\|A - \\widehat{U}_r \\Sigma_r \\widehat{V}_r^{\\top}\\|_2 = \\|(A - A_k) + (A_k - \\widehat{U}_r \\Sigma_r \\widehat{V}_r^{\\top})\\|_2 \\le \\|A - A_k\\|_2 + \\|A_k - \\widehat{U}_r \\Sigma_r \\widehat{V}_r^{\\top}\\|_2\n$$\nWe will now analyze each of the two terms on the right-hand side separately.\n\nAnalysis of the first term, $\\|A - A_k\\|_2$:\nBy definition, $A_k = P_U A P_V$. We can rewrite the difference $A - A_k$ by adding and subtracting the term $P_U A$:\n$$\nA - A_k = A - P_U A + P_U A - P_U A P_V = (I - P_U)A + P_U A(I - P_V)\n$$\nwhere $I$ is the identity matrix of appropriate dimension. Applying the triangle inequality again:\n$$\n\\|A - A_k\\|_2 \\le \\|(I - P_U)A\\|_2 + \\|P_U A(I - P_V)\\|_2\n$$\nThe first term $\\|(I - P_U)A\\|_2$ is precisely the definition of the norm of the projection-residual operator, $\\|R_U\\|_2$. For the second term, we use the submultiplicative property of the spectral norm, $\\|XY\\|_2 \\le \\|X\\|_2 \\|Y\\|_2$. Since $P_U$ is an orthogonal projector, its operator norm is $\\|P_U\\|_2 = 1$.\n$$\n\\|P_U A(I - P_V)\\|_2 \\le \\|P_U\\|_2 \\|A(I - P_V)\\|_2 = 1 \\cdot \\|A(I - P_V)\\|_2 = \\|R_V\\|_2\n$$\nwhere the last term is the definition of $\\|R_V\\|_2$. Combining these results gives the bound for the first term:\n$$\n\\|A - A_k\\|_2 \\le \\|R_U\\|_2 + \\|R_V\\|_2\n$$\n\nAnalysis of the second term, $\\|A_k - \\widehat{U}_r \\Sigma_r \\widehat{V}_r^{\\top}\\|_2$:\nFrom the definition of $A_k$ and the property $U_k^{\\top} A V_k = B_k$ from Lanczos bidiagonalization, we can express $A_k$ in terms of $B_k$:\n$$\nA_k = P_U A P_V = (U_k U_k^{\\top}) A (V_k V_k^{\\top}) = U_k (U_k^{\\top} A V_k) V_k^{\\top} = U_k B_k V_k^{\\top}\n$$\nThe rank-$r$ approximation is given by $\\widehat{U}_r \\Sigma_r \\widehat{V}_r^{\\top}$. Using the definitions $\\widehat{U}_r = U_k \\widetilde{U}_r$ and $\\widehat{V}_r = V_k \\widetilde{V}_r$, we have:\n$$\n\\widehat{U}_r \\Sigma_r \\widehat{V}_r^{\\top} = (U_k \\widetilde{U}_r) \\Sigma_r (V_k \\widetilde{V}_r)^{\\top} = U_k (\\widetilde{U}_r \\Sigma_r \\widetilde{V}_r^{\\top}) V_k^{\\top}\n$$\nThe term $\\widetilde{U}_r \\Sigma_r \\widetilde{V}_r^{\\top}$ is the truncated SVD of $B_k$, which we denote by $B_k^{(r)}$. By the Eckart–Young–Mirsky theorem, $B_k^{(r)}$ is the best rank-$r$ approximation of $B_k$ in the spectral norm.\nThe second term of our error bound becomes:\n$$\n\\|A_k - \\widehat{U}_r \\Sigma_r \\widehat{V}_r^{\\top}\\|_2 = \\|U_k B_k V_k^{\\top} - U_k B_k^{(r)} V_k^{\\top}\\|_2 = \\|U_k (B_k - B_k^{(r)}) V_k^{\\top}\\|_2\n$$\nSince $U_k \\in \\mathbb{R}^{m \\times k}$ and $V_k \\in \\mathbb{R}^{n \\times k}$ are matrices with orthonormal columns ($U_k^{\\top}U_k = I_k$ and $V_k^{\\top}V_k = I_k$), left-multiplication by $U_k$ and right-multiplication by $V_k^{\\top}$ preserve the spectral norm of a $k \\times k$ matrix. That is, for any $M \\in \\mathbb{R}^{k \\times k}$, $\\|U_k M V_k^{\\top}\\|_2 = \\|M\\|_2$. Thus:\n$$\n\\|U_k (B_k - B_k^{(r)}) V_k^{\\top}\\|_2 = \\|B_k - B_k^{(r)}\\|_2\n$$\nAgain, by the Eckart–Young–Mirsky theorem, the error of the best rank-$r$ approximation is the magnitude of the largest neglected singular value:\n$$\n\\|B_k - B_k^{(r)}\\|_2 = \\sigma_{r+1}(B_k)\n$$\nThe convention $\\sigma_{k+1}(B_k) := 0$ for $r=k$ is consistent, as in that case $B_k^{(k)} = B_k$ and the error is $0$.\n\nCombining the bounds for both terms:\nWe substitute the derived bounds back into the original inequality:\n$$\n\\|A - \\widehat{U}_r \\Sigma_r \\widehat{V}_r^{\\top}\\|_2 \\le \\|A - A_k\\|_2 + \\|A_k - \\widehat{U}_r \\Sigma_r \\widehat{V}_r^{\\top}\\|_2 \\le (\\|R_U\\|_2 + \\|R_V\\|_2) + \\sigma_{r+1}(B_k)\n$$\nThis completes the proof of the desired inequality.\n\nFinally, we evaluate this bound numerically using the provided values:\n$\\|R_U\\|_2 = \\frac{3}{50}$\n$\\|R_V\\|_2 = \\frac{1}{25}$\n$\\sigma_{r+1}(B_k) = \\frac{1}{20}$\n\nThe upper bound is calculated as:\n$$\n\\text{Bound} = \\|R_U\\|_2 + \\|R_V\\|_2 + \\sigma_{r+1}(B_k) = \\frac{3}{50} + \\frac{1}{25} + \\frac{1}{20}\n$$\nTo sum these fractions, we find a common denominator, which is the least common multiple of $50$, $25$, and $20$.\n$50 = 2 \\cdot 5^2$\n$25 = 5^2$\n$20 = 2^2 \\cdot 5$\nThe least common multiple is $2^2 \\cdot 5^2 = 100$.\n$$\n\\text{Bound} = \\frac{3 \\cdot 2}{50 \\cdot 2} + \\frac{1 \\cdot 4}{25 \\cdot 4} + \\frac{1 \\cdot 5}{20 \\cdot 5} = \\frac{6}{100} + \\frac{4}{100} + \\frac{5}{100} = \\frac{6+4+5}{100} = \\frac{15}{100}\n$$\nThis fraction can be reduced by dividing the numerator and denominator by their greatest common divisor, which is $5$:\n$$\n\\text{Bound} = \\frac{15 \\div 5}{100 \\div 5} = \\frac{3}{20}\n$$", "answer": "$$\n\\boxed{\\frac{3}{20}}\n$$", "id": "3539884"}]}