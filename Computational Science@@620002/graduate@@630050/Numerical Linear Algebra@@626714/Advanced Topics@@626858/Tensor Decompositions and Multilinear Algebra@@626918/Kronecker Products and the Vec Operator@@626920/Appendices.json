{"hands_on_practices": [{"introduction": "The $\\operatorname{vec}$ operator is a powerful tool for converting complex matrix equations into the familiar linear system form, $M\\mathbf{x} = \\mathbf{b}$. This practice provides a direct application of this technique to a variant of the discrete-time Lyapunov equation, allowing you to walk through the process of vectorization and solving for the unknown matrix's properties. Mastering this transformation is a key step towards tackling a wide range of problems in systems and control theory [@problem_id:1073003].", "problem": "An important class of linear matrix equations is the Sylvester equation, which for given matrices $A, B, C$ seeks a matrix $X$ satisfying $AX + XB = C$. A related equation is the discrete-time Lyapunov equation $X - A^T X A = Q$. This problem concerns a variant of these equations.\n\nTo solve such equations, the Kronecker product and the `vec` operator are powerful tools. For an $m \\times n$ matrix $M$ and a $p \\times q$ matrix $N$, the Kronecker product $M \\otimes N$ is the $mp \\times nq$ block matrix:\n$$\nM \\otimes N = \\begin{pmatrix}\nM_{11}N & \\cdots & M_{1n}N \\\\\n\\vdots & \\ddots & \\vdots \\\\\nM_{m1}N & \\cdots & M_{mn}N\n\\end{pmatrix}\n$$\nThe `vec` operator, $\\text{vec}(A)$, transforms an $m \\times n$ matrix $A$ into an $mn \\times 1$ column vector by stacking its columns:\n$$\n\\text{vec}(A) = \\begin{pmatrix} A_{\\cdot,1} \\\\ \\vdots \\\\ A_{\\cdot,n} \\end{pmatrix}\n$$\nA fundamental identity connecting these concepts is $\\text{vec}(AXB) = (B^T \\otimes A) \\text{vec}(X)$.\n\nConsider the linear matrix equation for an unknown $2 \\times 2$ real matrix $X$:\n$$\nX - A^T X B = I_2\n$$\nwhere $I_2$ is the $2 \\times 2$ identity matrix, and the matrices $A$ and $B$ are defined in terms of a real parameter $\\alpha$ as:\n$$\nA = \\begin{pmatrix} \\alpha & 1 \\\\ 0 & \\alpha \\end{pmatrix}, \\quad B = \\begin{pmatrix} \\alpha & 0 \\\\ 1 & \\alpha \\end{pmatrix}\n$$\nAssume that the parameter $\\alpha$ is such that $|\\alpha| \\neq 1$, which guarantees the existence of a unique solution for $X$.\n\nUsing the Kronecker product formalism, find a closed-form expression for the trace of the solution matrix, $\\text{Tr}(X)$, as a function of $\\alpha$.", "solution": "The given matrix equation is $X - A^T X B = I_2$. We are asked to solve for the trace of $X$ using the Kronecker product formalism.\n\nFirst, we apply the `vec` operator to the entire equation. Using the linearity of the `vec` operator, we get:\n$$\n\\text{vec}(X - A^T X B) = \\text{vec}(I_2)\n$$\n$$\n\\text{vec}(X) - \\text{vec}(A^T X B) = \\text{vec}(I_2)\n$$\nNow, we use the identity $\\text{vec}(AXB) = (B^T \\otimes A) \\text{vec}(X)$. In our case, the matrices in the triple product are $A^T$, $X$, and $B$. Thus we have $A \\to A^T$ and $B \\to B$. Applying the identity:\n$$\n\\text{vec}(A^T X B) = (B^T \\otimes A^T) \\text{vec}(X)\n$$\nSubstituting this back into the vectorized equation gives:\n$$\n\\text{vec}(X) - (B^T \\otimes A^T) \\text{vec}(X) = \\text{vec}(I_2)\n$$\nFactoring out $\\text{vec}(X)$, which is identified as $I \\text{vec}(X)$, where $I$ is the identity matrix of appropriate size ($4 \\times 4$ in this case), we obtain a standard linear system:\n$$\n(I_4 - B^T \\otimes A^T) \\text{vec}(X) = \\text{vec}(I_2)\n$$\nLet's compute the matrices involved. The matrices $A$ and $B$ are given by:\n$$\nA = \\begin{pmatrix} \\alpha & 1 \\\\ 0 & \\alpha \\end{pmatrix}, \\quad B = \\begin{pmatrix} \\alpha & 0 \\\\ 1 & \\alpha \\end{pmatrix}\n$$\nTheir transposes are:\n$$\nA^T = \\begin{pmatrix} \\alpha & 0 \\\\ 1 & \\alpha \\end{pmatrix}, \\quad B^T = \\begin{pmatrix} \\alpha & 1 \\\\ 0 & \\alpha \\end{pmatrix}\n$$\nNow, we compute the Kronecker product $B^T \\otimes A^T$:\n$$\nB^T \\otimes A^T = \\begin{pmatrix} \\alpha & 1 \\\\ 0 & \\alpha \\end{pmatrix} \\otimes \\begin{pmatrix} \\alpha & 0 \\\\ 1 & \\alpha \\end{pmatrix} = \\begin{pmatrix} \\alpha A^T & 1 A^T \\\\ 0 \\cdot A^T & \\alpha A^T \\end{pmatrix}\n$$\n$$\nB^T \\otimes A^T = \\begin{pmatrix} \\alpha \\begin{pmatrix} \\alpha & 0 \\\\ 1 & \\alpha \\end{pmatrix} & \\begin{pmatrix} \\alpha & 0 \\\\ 1 & \\alpha \\end{pmatrix} \\\\ \\begin{pmatrix} 0 & 0 \\\\ 0 & 0 \\end{pmatrix} & \\alpha \\begin{pmatrix} \\alpha & 0 \\\\ 1 & \\alpha \\end{pmatrix} \\end{pmatrix} = \\begin{pmatrix} \\alpha^2 & 0 & \\alpha & 0 \\\\ \\alpha & \\alpha^2 & 1 & \\alpha \\\\ 0 & 0 & \\alpha^2 & 0 \\\\ 0 & 0 & \\alpha & \\alpha^2 \\end{pmatrix}\n$$\nThe coefficient matrix of our linear system is $M = I_4 - B^T \\otimes A^T$:\n$$\nM = \\begin{pmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 1 \\end{pmatrix} - \\begin{pmatrix} \\alpha^2 & 0 & \\alpha & 0 \\\\ \\alpha & \\alpha^2 & 1 & \\alpha \\\\ 0 & 0 & \\alpha^2 & 0 \\\\ 0 & 0 & \\alpha & \\alpha^2 \\end{pmatrix} = \\begin{pmatrix} 1-\\alpha^2 & 0 & -\\alpha & 0 \\\\ -\\alpha & 1-\\alpha^2 & -1 & -\\alpha \\\\ 0 & 0 & 1-\\alpha^2 & 0 \\\\ 0 & 0 & -\\alpha & 1-\\alpha^2 \\end{pmatrix}\n$$\nLet the unknown matrix $X$ be $X = \\begin{pmatrix} x_{11} & x_{12} \\\\ x_{21} & x_{22} \\end{pmatrix}$. Its vectorization is $\\text{vec}(X) = (x_{11}, x_{21}, x_{12}, x_{22})^T$. The vectorization of the identity matrix $I_2$ is $\\text{vec}(I_2) = (1, 0, 0, 1)^T$.\n\nThe linear system $M \\text{vec}(X) = \\text{vec}(I_2)$ is:\n$$\n\\begin{pmatrix} 1-\\alpha^2 & 0 & -\\alpha & 0 \\\\ -\\alpha & 1-\\alpha^2 & -1 & -\\alpha \\\\ 0 & 0 & 1-\\alpha^2 & 0 \\\\ 0 & 0 & -\\alpha & 1-\\alpha^2 \\end{pmatrix} \\begin{pmatrix} x_{11} \\\\ x_{21} \\\\ x_{12} \\\\ x_{22} \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 1 \\end{pmatrix}\n$$\nThis represents a system of four linear equations:\n1. $(1-\\alpha^2)x_{11} - \\alpha x_{12} = 1$\n2. $-\\alpha x_{11} + (1-\\alpha^2)x_{21} - x_{12} - \\alpha x_{22} = 0$\n3. $(1-\\alpha^2)x_{12} = 0$\n4. $-\\alpha x_{12} + (1-\\alpha^2)x_{22} = 1$\n\nWe solve this system for the elements of $X$. From equation (3), since $|\\alpha| \\neq 1$, we have $1-\\alpha^2 \\neq 0$, which implies:\n$$\nx_{12} = 0\n$$\nSubstitute $x_{12}=0$ into equation (4):\n$$\n(1-\\alpha^2)x_{22} = 1 \\implies x_{22} = \\frac{1}{1-\\alpha^2}\n$$\nSubstitute $x_{12}=0$ into equation (1):\n$$\n(1-\\alpha^2)x_{11} = 1 \\implies x_{11} = \\frac{1}{1-\\alpha^2}\n$$\nWe do not need to solve for $x_{21}$ to find the trace. The trace of $X$ is $\\text{Tr}(X) = x_{11} + x_{22}$.\n$$\n\\text{Tr}(X) = \\frac{1}{1-\\alpha^2} + \\frac{1}{1-\\alpha^2} = \\frac{2}{1-\\alpha^2}\n$$\nThis gives the trace of the solution matrix $X$ as a function of $\\alpha$.", "answer": "$$ \\boxed{\\frac{2}{1-\\alpha^2}} $$", "id": "1073003"}, {"introduction": "Building on the basic vectorization technique, we now tackle a generalized Sylvester equation where the resulting system matrix is singular. This exercise highlights how to find a meaningful solution—the one with the minimum Frobenius norm—by employing the Moore-Penrose pseudoinverse. This scenario is common in control theory and estimation problems where systems can be underdetermined, and understanding how to find the minimum-norm solution is of great practical importance [@problem_id:1092345].", "problem": "Let $\\mathbb{R}^{m \\times n}$ be the space of $m \\times n$ real matrices. We introduce the following definitions and properties:\n\n1.  **Kronecker Product:** For $A \\in \\mathbb{R}^{m \\times n}$ and $B \\in \\mathbb{R}^{p \\times q}$, the Kronecker product $A \\otimes B$ is the $mp \\times nq$ block matrix:\n    $$\n    A \\otimes B = \\begin{pmatrix} a_{11}B & \\cdots & a_{1n}B \\\\ \\vdots & \\ddots & \\vdots \\\\ a_{m1}B & \\cdots & a_{mn}B \\end{pmatrix}\n    $$\n2.  **Vectorization Operator:** For a matrix $X = \\begin{pmatrix} x_1 & x_2 & \\cdots & x_n \\end{pmatrix} \\in \\mathbb{R}^{m \\times n}$, where $x_i$ are the columns of $X$, the $\\text{vec}$ operator stacks the columns into a single $mn \\times 1$ column vector:\n    $$\n    \\text{vec}(X) = \\begin{pmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{pmatrix}\n    $$\n3.  **Key Property:** A crucial link between the Kronecker product and the vec operator is the identity $\\text{vec}(AXB) = (B^T \\otimes A)\\text{vec}(X)$ for matrices $A, X, B$ of compatible dimensions.\n4.  **Frobenius Norm:** The Frobenius norm of a matrix $X \\in \\mathbb{R}^{m \\times n}$ is defined as $\\|X\\|_F = \\sqrt{\\sum_{i=1}^m \\sum_{j=1}^n |x_{ij}|^2} = \\sqrt{\\text{Tr}(X^T X)}$. This is equivalent to the standard Euclidean norm of its vectorized form, i.e., $\\|X\\|_F = \\|\\text{vec}(X)\\|_2$.\n\nConsider the generalized Sylvester equation $AXB + CXD = E$. It is known that if this system of linear equations for the entries of $X$ is consistent (i.e., has at least one solution), there exists a unique solution $X_0$ that has the minimum Frobenius norm.\n\n**Problem:**\n\nLet $e_1$ and $e_2$ be the standard basis vectors in $\\mathbb{R}^2$. Let two other vectors be defined as $u = e_1 + e_2$ and $v = e_1 - e_2$. The matrices in the Sylvester equation are defined as follows:\n- $A = uu^T$\n- $B = e_1e_1^T$\n- $C = e_2e_2^T$\n- $D = vv^T$\n- $E$ is the $2 \\times 2$ identity matrix $I_2$.\n\nGiven that the equation $AXB + CXD = E$ is consistent for an unknown matrix $X \\in \\mathbb{R}^{2 \\times 2}$, determine the squared Frobenius norm, $\\|X_0\\|_F^2$, of the unique minimum-norm solution $X_0$.\n\n**Find:** $\\|X_0\\|_F^2$.", "solution": "**1. Formulating the Linear System**\n\nThe given generalized Sylvester equation is $AXB + CXD = E$. We can transform this matrix equation into a standard vector-form linear system using the vec operator. Applying the vec operator to both sides, we get:\n$$\n\\text{vec}(AXB + CXD) = \\text{vec}(E)\n$$\nBy linearity of the vec operator, this becomes:\n$$\n\\text{vec}(AXB) + \\text{vec}(CXD) = \\text{vec}(E)\n$$\nUsing the identity $\\text{vec}(PQR) = (R^T \\otimes P)\\text{vec}(Q)$, we can rewrite the equation as:\n$$\n(B^T \\otimes A)\\text{vec}(X) + (D^T \\otimes C)\\text{vec}(X) = \\text{vec}(E)\n$$\nThis can be expressed as a single linear system $M\\mathbf{x} = \\mathbf{e}$, where:\n- $\\mathbf{x} = \\text{vec}(X)$\n- $\\mathbf{e} = \\text{vec}(E)$\n- $M = (B^T \\otimes A) + (D^T \\otimes C)$\n\n**2. Constructing the Matrices**\n\nFirst, let's write out the vectors and matrices explicitly. The standard basis vectors in $\\mathbb{R}^2$ are $e_1 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$ and $e_2 = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$.\nThe other vectors are $u = e_1 + e_2 = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$ and $v = e_1 - e_2 = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$.\n\nThe matrices are constructed as outer products:\n$A = uu^T = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} \\begin{pmatrix} 1 & 1 \\end{pmatrix} = \\begin{pmatrix} 1 & 1 \\\\ 1 & 1 \\end{pmatrix}$\n$B = e_1e_1^T = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix}$\n$C = e_2e_2^T = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} \\begin{pmatrix} 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 0 & 0 \\\\ 0 & 1 \\end{pmatrix}$\n$D = vv^T = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} \\begin{pmatrix} 1 & -1 \\end{pmatrix} = \\begin{pmatrix} 1 & -1 \\\\ -1 & 1 \\end{pmatrix}$\n$E = I_2 = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}$\n\nAll matrices $A, B, C, D$ are symmetric, so $B^T = B$ and $D^T = D$.\n\nThe vectorized form of $E$ is $\\mathbf{e} = \\text{vec}(I_2) = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 1 \\end{pmatrix}$.\n\n**3. Constructing the System Matrix M**\n\nThe system matrix is $M = (B \\otimes A) + (D \\otimes C)$.\nFirst term:\n$B \\otimes A = \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix} \\otimes \\begin{pmatrix} 1 & 1 \\\\ 1 & 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\cdot A & 0 \\cdot A \\\\ 0 \\cdot A & 0 \\cdot A \\end{pmatrix} = \\begin{pmatrix} 1 & 1 & 0 & 0 \\\\ 1 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\end{pmatrix}$\nSecond term:\n$D \\otimes C = \\begin{pmatrix} 1 & -1 \\\\ -1 & 1 \\end{pmatrix} \\otimes \\begin{pmatrix} 0 & 0 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\cdot C & -1 \\cdot C \\\\ -1 \\cdot C & 1 \\cdot C \\end{pmatrix} = \\begin{pmatrix} \\begin{pmatrix} 0 & 0 \\\\ 0 & 1 \\end{pmatrix} & \\begin{pmatrix} 0 & 0 \\\\ 0 & -1 \\end{pmatrix} \\\\ \\begin{pmatrix} 0 & 0 \\\\ 0 & -1 \\end{pmatrix} & \\begin{pmatrix} 0 & 0 \\\\ 0 & 1 \\end{pmatrix} \\end{pmatrix} = \\begin{pmatrix} 0 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & -1 \\\\ 0 & 0 & 0 & 0 \\\\ 0 & -1 & 0 & 1 \\end{pmatrix}$\nAdding them together:\n$M = \\begin{pmatrix} 1 & 1 & 0 & 0 \\\\ 1 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\end{pmatrix} + \\begin{pmatrix} 0 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & -1 \\\\ 0 & 0 & 0 & 0 \\\\ 0 & -1 & 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 1 & 1 & 0 & 0 \\\\ 1 & 2 & 0 & -1 \\\\ 0 & 0 & 0 & 0 \\\\ 0 & -1 & 0 & 1 \\end{pmatrix}$\n\nThe third row of $M$ is zero, so $\\det(M)=0$. The matrix is singular, as expected from the problem statement. The problem states the system is consistent, meaning $\\mathbf{e}$ is in the column space of $M$, a fact we don't need to re-verify but could.\n\n**4. Finding the Minimum Norm Solution**\n\nThe minimum norm solution to a consistent linear system $M\\mathbf{x} = \\mathbf{e}$ is given by $\\mathbf{x}_0 = M^\\dagger \\mathbf{e}$, where $M^\\dagger$ is the Moore-Penrose pseudoinverse of $M$. We can compute $M^\\dagger$ using a full-rank decomposition.\n\nLet $M=CR$ be a full-rank decomposition, where the columns of $C$ form a basis for the column space of $M$, and $R$ consists of the coefficients to express the columns of $M$ in that basis.\nThe columns of $M$ are $m_1=(1,1,0,0)^T$, $m_2=(1,2,0,-1)^T$, $m_3=(0,0,0,0)^T$, $m_4=(0,-1,0,1)^T$.\nThe column space $C(M)$ is spanned by $m_1$ and $m_4$. We note that $m_2 = m_1 - m_4$ and $m_3 = 0$.\nSo we can choose the basis for $C(M)$ to be $\\{m_1, m_4\\}$. Let $C = \\begin{pmatrix} m_1 & m_4 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 1 & -1 \\\\ 0 & 0 \\\\ 0 & 1 \\end{pmatrix}$.\nThe coefficient matrix $R$ is $R=\\begin{pmatrix} 1 & 1 & 0 & 0 \\\\ 0 & -1 & 0 & 1 \\end{pmatrix}$.\nThe pseudoinverse is then $M^\\dagger = R^\\dagger C^\\dagger = R^T(RR^T)^{-1}(C^TC)^{-1}C^T$.\n\nCalculate the components:\n$C^TC = \\begin{pmatrix} 1 & 1 & 0 & 0 \\\\ 0 & -1 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 1 & -1 \\\\ 0 & 0 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 2 & -1 \\\\ -1 & 2 \\end{pmatrix}$\n$(C^TC)^{-1} = \\frac{1}{2(2)-(-1)^2}\\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix} = \\frac{1}{3}\\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix}$\n\n$RR^T = \\begin{pmatrix} 1 & 1 & 0 & 0 \\\\ 0 & -1 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 1 & -1 \\\\ 0 & 0 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 2 & -1 \\\\ -1 & 2 \\end{pmatrix}$\n$(RR^T)^{-1} = \\frac{1}{3}\\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix}$\n\nNow we assemble $M^\\dagger$:\n$C^\\dagger = (C^TC)^{-1}C^T = \\frac{1}{3}\\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix} \\begin{pmatrix} 1 & 1 & 0 & 0 \\\\ 0 & -1 & 0 & 1 \\end{pmatrix} = \\frac{1}{3}\\begin{pmatrix} 2 & 1 & 0 & 1 \\\\ 1 & -1 & 0 & 2 \\end{pmatrix}$\n$R^\\dagger = R^T(RR^T)^{-1} = \\begin{pmatrix} 1 & 0 \\\\ 1 & -1 \\\\ 0 & 0 \\\\ 0 & 1 \\end{pmatrix} \\frac{1}{3}\\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix} = \\frac{1}{3}\\begin{pmatrix} 2 & 1 \\\\ 1 & -1 \\\\ 0 & 0 \\\\ 1 & 2 \\end{pmatrix}$\n\n$M^\\dagger = R^\\dagger C^\\dagger = \\frac{1}{3}\\begin{pmatrix} 2 & 1 \\\\ 1 & -1 \\\\ 0 & 0 \\\\ 1 & 2 \\end{pmatrix} \\frac{1}{3}\\begin{pmatrix} 2 & 1 & 0 & 1 \\\\ 1 & -1 & 0 & 2 \\end{pmatrix} = \\frac{1}{9}\\begin{pmatrix} 5 & 1 & 0 & 4 \\\\ 1 & 2 & 0 & -1 \\\\ 0 & 0 & 0 & 0 \\\\ 4 & -1 & 0 & 5 \\end{pmatrix}$\n\nNow we compute the minimum norm solution vector $\\mathbf{x}_0 = \\text{vec}(X_0)$:\n$\\mathbf{x}_0 = M^\\dagger \\mathbf{e} = \\frac{1}{9}\\begin{pmatrix} 5 & 1 & 0 & 4 \\\\ 1 & 2 & 0 & -1 \\\\ 0 & 0 & 0 & 0 \\\\ 4 & -1 & 0 & 5 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 1 \\end{pmatrix} = \\frac{1}{9}\\begin{pmatrix} 5 \\cdot 1 + 4 \\cdot 1 \\\\ 1 \\cdot 1 - 1 \\cdot 1 \\\\ 0 \\\\ 4 \\cdot 1 + 5 \\cdot 1 \\end{pmatrix} = \\frac{1}{9}\\begin{pmatrix} 9 \\\\ 0 \\\\ 0 \\\\ 9 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 1 \\end{pmatrix}$\n\n**5. Reconstructing X_0 and Calculating its Norm**\n\nThe vector $\\mathbf{x}_0 = \\text{vec}(X_0)$ is $\\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 1 \\end{pmatrix}$. We un-vectorize it to find the matrix $X_0$:\n$\\text{vec}(X_0) = \\begin{pmatrix} x_{11} \\\\ x_{21} \\\\ x_{12} \\\\ x_{22} \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 1 \\end{pmatrix} \\implies X_0 = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} = I_2$\n\nThe problem asks for the squared Frobenius norm of $X_0$.\n$\\|X_0\\|_F^2 = \\text{Tr}(X_0^T X_0) = \\text{Tr}(I_2^T I_2) = \\text{Tr}(I_2) = 1+1=2$.\nAlternatively, using the vectorized form:\n$\\|X_0\\|_F^2 = \\|\\mathbf{x}_0\\|_2^2 = \\left\\|\\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 1 \\end{pmatrix}\\right\\|_2^2 = 1^2 + 0^2 + 0^2 + 1^2 = 2$.", "answer": "$$ \\boxed{2} $$", "id": "1092345"}, {"introduction": "This advanced practice connects the algebraic structure of the Kronecker sum to the analytical concept of the matrix exponential, a cornerstone of differential equation theory and scientific computing. You will derive a fundamental identity for the action of the matrix exponential and implement a numerically stable algorithm to compute it, a task central to solving linear ODEs. The exercise also delves into the critical issue of nonnormality and its impact on computational accuracy, a key consideration in modern numerical linear algebra [@problem_id:3553560].", "problem": "Let $A \\in \\mathbb{R}^{m \\times m}$, $B \\in \\mathbb{R}^{n \\times n}$, and $X \\in \\mathbb{R}^{m \\times n}$. Consider the task of evaluating the vectorized product $\\operatorname{vec}(\\exp(A)\\,X\\,\\exp(B^{T}))$ without directly forming $\\exp(A)$ and $\\exp(B^{T})$. Your goal is to derive from first principles a method that evaluates this quantity by working instead with a linear operator constructed from $A$ and $B$, and then to implement a numerically stable algorithm based on scaling and squaring for the matrix exponential of that operator.\n\nFundamental definitions you may use as a starting point include:\n- The matrix exponential defined by the power series $\\exp(M) = \\sum_{k=0}^{\\infty} \\frac{M^{k}}{k!}$ for any square matrix $M$.\n- The vectorization operator $\\operatorname{vec}(\\cdot)$ that stacks the columns of a matrix into a single column vector.\n- The Kronecker product $U \\otimes V$ for matrices $U$ and $V$, and basic bilinearity properties.\n- The identity $\\operatorname{vec}(U\\,X\\,V) = (V^{T} \\otimes U)\\operatorname{vec}(X)$ for conforming matrices $U$, $V$, and $X$, which follows from the definitions of the Kronecker product and the vectorization operator.\n\nTasks:\n1. Starting only from the fundamental definitions above, derive a representation of $\\operatorname{vec}(\\exp(A)\\,X\\,\\exp(B^{T}))$ in terms of the action of $\\exp(M_{A,B})$ on $\\operatorname{vec}(X)$ for a structured matrix $M_{A,B}$ that you must construct from $A$ and $B$. Your derivation must identify $M_{A,B}$ explicitly and must rely only on the properties stated in the fundamental base. Do not assume any additional identities.\n2. Design a scaling-and-squaring algorithm to evaluate $\\exp(M_{A,B})$ numerically. Your design must:\n   - Choose a scaling parameter $s \\in \\mathbb{N}$ based on a matrix norm so that $\\lVert M_{A,B}/2^{s}\\rVert$ is sufficiently small for a truncated Taylor polynomial of degree $K$ to approximate $\\exp(M_{A,B}/2^{s})$ to high accuracy.\n   - Form a truncated Taylor approximation $T_{K}(Z) = \\sum_{j=0}^{K} \\frac{Z^{j}}{j!}$ to approximate $\\exp(Z)$ at $Z = M_{A,B}/2^{s}$.\n   - Reconstruct $\\exp(M_{A,B})$ by repeated squaring, using the identity $\\exp(M_{A,B}) = \\left(\\exp(M_{A,B}/2^{s})\\right)^{2^{s}}$.\n   - Apply the resulting $\\exp(M_{A,B})$ to $\\operatorname{vec}(X)$ to obtain an approximation to $\\operatorname{vec}(\\exp(A)\\,X\\,\\exp(B^{T}))$.\n3. Analyze numerical stability and sensitivity when $A$ and $B$ are nonnormal (that is, $A A^{T} \\neq A^{T} A$ and/or $B B^{T} \\neq B^{T} B$). Use an a priori growth proxy $g(A,B) := \\lVert \\exp(A) \\rVert \\,\\lVert \\exp(B) \\rVert$ as a simple indicator of sensitivity of the mapping $X \\mapsto \\exp(A) X \\exp(B^{T})$. Discuss how nonnormality can lead to transient growth and potentially amplify forward error, even when a scaling-and-squaring procedure is backward stable for the exponential.\n4. Implement the algorithm and evaluate it on the following test suite. For each case, compute:\n   - The relative forward error $e = \\frac{\\lVert y_{\\text{approx}} - y_{\\text{ref}} \\rVert_{2}}{\\lVert y_{\\text{ref}} \\rVert_{2}}$, where $y_{\\text{ref}} = \\operatorname{vec}(\\exp(A)\\,X\\,\\exp(B^{T}))$ computed by directly forming $\\exp(A)$ and $\\exp(B)$, and $y_{\\text{approx}}$ is obtained by your scaling-and-squaring algorithm on the structured operator constructed from $A$ and $B$.\n   - The sensitivity proxy $g(A,B) = \\lVert \\exp(A) \\rVert_{2}\\,\\lVert \\exp(B) \\rVert_{2}$.\n   - A boolean pass/fail indicator comparing the relative error against a tolerance of $10^{-9}$; that is, report $\\text{True}$ if $e < 10^{-9}$ and $\\text{False}$ otherwise.\n\nUse the following test suite (all matrices are real, and all numbers are to be interpreted in standard double-precision floating point; no physical units apply):\n- Case 1 (well-conditioned, normal):\n  - $A_{1} = \\begin{bmatrix} -1 & 0 \\\\ 0 & -2 \\end{bmatrix}$,\n    $B_{1} = \\begin{bmatrix} -\\tfrac{1}{2} & 0 \\\\ 0 & -\\tfrac{3}{2} \\end{bmatrix}$,\n    $X_{1} = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix}$.\n- Case 2 (highly nonnormal, moderate norm):\n  - $A_{2} = \\begin{bmatrix} -1 & 10 & 0 \\\\ 0 & -1 & 10 \\\\ 0 & 0 & -1 \\end{bmatrix}$,\n    $B_{2} = \\begin{bmatrix} 0 & 10 & 0 \\\\ 0 & 0 & 10 \\\\ 0 & 0 & 0 \\end{bmatrix}$,\n    $X_{2} = \\begin{bmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 0 \\\\ 1 & 0 & 1 \\end{bmatrix}$.\n- Case 3 (boundary scaling, mixed normality):\n  - $A_{3} \\in \\mathbb{R}^{4 \\times 4}$ with $A_{3}(i,i+1) = 5$ for $i \\in \\{1,2,3\\}$ and all other entries $0$,\n    $B_{3} = -4 I_{4}$,\n    $X_{3} = \\begin{bmatrix}\n    1 & 5 & 9 & 13 \\\\\n    2 & 6 & 10 & 14 \\\\\n    3 & 7 & 11 & 15 \\\\\n    4 & 8 & 12 & 16\n    \\end{bmatrix}$.\n\nImplementation requirements:\n- Your program must implement the scaling-and-squaring algorithm described in Task $2$ on the operator constructed from $A$ and $B$ in Task $1$, using a truncated Taylor polynomial of degree $K$ and a scaling parameter $s$ chosen from a matrix norm criterion. You may use direct matrix-matrix operations for small sizes; do not rely on a black-box matrix exponential for the operator arising from $A$ and $B$.\n- For the reference results and the sensitivity proxy $g(A,B)$, you may compute $\\exp(A)$ and $\\exp(B)$ by any reliable method available to you.\n\nFinal output specification:\n- For each case in the order $1,2,3$, report the triple $\\left(e, \\text{pass}, g(A,B)\\right)$, where $e$ is the relative error as a floating-point number, $\\text{pass}$ is a boolean as defined above, and $g(A,B)$ is the floating-point sensitivity proxy.\n- Your program should produce a single line of output containing the nine results in the order\n  $[e_{1}, \\text{pass}_{1}, g_{1}, e_{2}, \\text{pass}_{2}, g_{2}, e_{3}, \\text{pass}_{3}, g_{3}]$\n  as a comma-separated list enclosed in square brackets, for example,\n  $[0.0,\\text{True},1.0, \\dots]$.", "solution": "### Task 1: Derivation of the Structured Operator $M_{A,B}$\n\nWe are tasked with finding a matrix $M_{A,B}$ such that $\\operatorname{vec}(\\exp(A)\\,X\\,\\exp(B^{T})) = \\exp(M_{A,B})\\operatorname{vec}(X)$. We will derive this identity by considering a matrix-valued ordinary differential equation (ODE).\n\nLet the matrix function $Y(t)$ be defined as $Y(t) = \\exp(At)X\\exp(B^T t)$ for a scalar variable $t$. The quantity of interest is $Y(1) = \\exp(A)X\\exp(B^T)$. We seek an ODE that $Y(t)$ satisfies. Differentiating $Y(t)$ with respect to $t$ using the product rule for matrix functions yields:\n$$\n\\dot{Y}(t) = \\frac{d}{dt} Y(t) = \\left(\\frac{d}{dt}\\exp(At)\\right) X\\exp(B^T t) + \\exp(At)X\\left(\\frac{d}{dt}\\exp(B^T t)\\right)\n$$\nFrom the power series definition of the matrix exponential, a fundamental property is $\\frac{d}{dt}\\exp(Ct) = C\\exp(Ct) = \\exp(Ct)C$. Applying this, we get:\n$$\n\\dot{Y}(t) = (A\\exp(At))X\\exp(B^T t) + \\exp(At)X(\\exp(B^T t)B^T)\n$$\nBy substituting the definition of $Y(t)$, this simplifies to:\n$$\n\\dot{Y}(t) = AY(t) + Y(t)B^T\n$$\nThis is a linear matrix differential equation for $Y(t) \\in \\mathbb{R}^{m \\times n}$, with the initial condition $Y(0) = \\exp(A \\cdot 0)X\\exp(B^T \\cdot 0) = I_m X I_n = X$.\n\nTo transform this into a vector-valued ODE, we apply the vectorization operator $\\operatorname{vec}(\\cdot)$. Let $y(t) = \\operatorname{vec}(Y(t))$. Then $\\dot{y}(t) = \\operatorname{vec}(\\dot{Y}(t))$. Applying $\\operatorname{vec}$ to the ODE gives:\n$$\n\\dot{y}(t) = \\operatorname{vec}(AY(t) + Y(t)B^T)\n$$\nBy the linearity of the $\\operatorname{vec}$ operator, this is:\n$$\n\\dot{y}(t) = \\operatorname{vec}(AY(t)) + \\operatorname{vec}(Y(t)B^T)\n$$\nWe now use the provided identity $\\operatorname{vec}(UXV) = (V^T \\otimes U)\\operatorname{vec}(X)$.\nFor the first term, $\\operatorname{vec}(AY(t))$, we can write it as $\\operatorname{vec}(AY(t)I_n)$. We set $U=A$, $X=Y(t)$ (in the identity), and $V=I_n$. The identity gives:\n$$\n\\operatorname{vec}(AY(t)) = (I_n^T \\otimes A)\\operatorname{vec}(Y(t)) = (I_n \\otimes A)y(t)\n$$\nFor the second term, $\\operatorname{vec}(Y(t)B^T)$, we can write it as $\\operatorname{vec}(I_mY(t)B^T)$. We set $U=I_m$, $X=Y(t)$ (in the identity), and $V=B^T$. The identity gives:\n$$\n\\operatorname{vec}(Y(t)B^T) = ((B^T)^T \\otimes I_m)\\operatorname{vec}(Y(t)) = (B \\otimes I_m)y(t)\n$$\nSubstituting these back into the vectorized ODE, we obtain:\n$$\n\\dot{y}(t) = (I_n \\otimes A)y(t) + (B \\otimes I_m)y(t) = (I_n \\otimes A + B \\otimes I_m)y(t)\n$$\nThis is a standard first-order linear vector ODE of the form $\\dot{y}(t) = M y(t)$, where the matrix $M$ is constant. The solution is $y(t) = \\exp(Mt)y(0)$.\nIn our case, the matrix is $M_{A,B} = I_n \\otimes A + B \\otimes I_m$, and the initial condition is $y(0) = \\operatorname{vec}(Y(0)) = \\operatorname{vec}(X)$.\nThe solution at $t=1$ is precisely the vectorized quantity we seek:\n$$\ny(1) = \\operatorname{vec}(Y(1)) = \\operatorname{vec}(\\exp(A)X\\exp(B^T))\n$$\nTherefore, by setting $t=1$ in the ODE solution, we arrive at the final result:\n$$\n\\operatorname{vec}(\\exp(A)X\\exp(B^T)) = \\exp(I_n \\otimes A + B \\otimes I_m) \\operatorname{vec}(X)\n$$\nThe structured matrix is explicitly identified as $M_{A,B} = I_n \\otimes A + B \\otimes I_m$. This matrix is known as the Kronecker sum of $B$ and $A$, denoted $B \\oplus A$.\n\n### Task 2: Scaling-and-Squaring Algorithm Design\n\nThe task is to compute $y = \\exp(M_{A,B})x$ where $x = \\operatorname{vec}(X)$ and $M_{A,B} = I_n \\otimes A + B \\otimes I_m$. The algorithm must not compute $\\exp(A)$ and $\\exp(B)$ directly but rather work with $\\exp(M_{A,B})$. The matrix $M_{A,B}$ is of size $(mn) \\times (mn)$ and is formed explicitly for this problem. The algorithm proceeds as follows:\n\n1.  **Construct Operator**: Given $A \\in \\mathbb{R}^{m \\times m}$ and $B \\in \\mathbb{R}^{n \\times n}$, construct $M_{A,B} = I_n \\otimes A + B \\otimes I_m$.\n\n2.  **Scaling**: The accuracy of a truncated Taylor series approximation for $\\exp(Z)$ depends on $\\lVert Z \\rVert$. Scaling reduces the norm to a domain where the approximation is accurate. We use the identity $\\exp(M) = (\\exp(M/2^s))^{2^s}$.\n    - Compute the infinity-norm of the operator: $\\lVert M_{A,B} \\rVert_{\\infty}$.\n    - Choose a scaling parameter $s \\in \\mathbb{N}$ such that the scaled matrix $Z = M_{A,B}/2^s$ has a sufficiently small norm. A common target is $\\lVert Z \\rVert_{\\infty} \\leq 0.5$. The number of squarings $s$ is chosen as $s = \\max(0, \\lceil \\log_2(\\lVert M_{A,B} \\rVert_{\\infty} / \\theta) \\rceil)$ for a target norm $\\theta$. We select $\\theta=0.5$.\n\n3.  **Taylor Series Approximation**: Approximate $\\exp(Z) = \\exp(M_{A,B}/2^s)$ using a truncated Taylor series of degree $K$.\n    $$\n    \\exp(Z) \\approx T_K(Z) = \\sum_{j=0}^{K} \\frac{Z^j}{j!} = I + Z + \\frac{Z^2}{2!} + \\dots + \\frac{Z^K}{K!}\n    $$\n    For double-precision accuracy with $\\lVert Z \\rVert_{\\infty} \\leq 0.5$, a degree $K=16$ provides high accuracy. The sum is computed efficiently using an iterative scheme to avoid recomputing matrix powers:\n    Let $E_{scaled} = I$ and $Term = I$.\n    For $j = 1, \\dots, K$:\n    $Term \\leftarrow Term \\cdot Z / j$\n    $E_{scaled} \\leftarrow E_{scaled} + Term$\n\n4.  **Squaring**: Reconstruct the approximation for $\\exp(M_{A,B})$ from the scaled approximation $E_{scaled} \\approx \\exp(M_{A,B}/2^s)$ by repeated squaring.\n    Let $E_{final} = E_{scaled}$.\n    For $i = 1, \\dots, s$:\n    $E_{final} \\leftarrow E_{final} \\cdot E_{final}$\n    The resulting matrix $E_{final}$ is our approximation of $\\exp(M_{A,B})$.\n\n5.  **Final Application**: Apply the computed operator to the vectorized input matrix $x = \\operatorname{vec}(X)$ to get the final result.\n    $$\n    y_{\\text{approx}} = E_{final} \\cdot x\n    $$\n\n### Task 3: Numerical Stability Analysis\n\nThe numerical stability of computing $\\exp(A)X\\exp(B^T)$ via the Kronecker sum formulation depends on the properties of the matrices $A$, $B$, and the derived operator $M_{A,B}$.\n\nWhen $A$ and/or $B$ are nonnormal (i.e., $A A^T \\neq A^T A$), the behavior of the matrix exponential can be sensitive. Nonnormality is associated with transient growth, where $\\lVert \\exp(At) \\rVert$ can increase substantially for small $t>0$ before eventual decay if the eigenvalues of $A$ have negative real parts.\n\nThe forward map is $L: X \\mapsto \\exp(A)X\\exp(B^T)$. In vectorized form, this is $l: \\operatorname{vec}(X) \\mapsto \\exp(M_{A,B})\\operatorname{vec}(X)$, where the operator is $\\exp(M_{A,B}) = \\exp(B) \\otimes \\exp(A)$. The operator norm for this map (with Frobenius norm on matrices and $L_2$ norm on vectors) is:\n$$\n\\lVert \\exp(M_{A,B}) \\rVert_2 = \\lVert \\exp(B) \\otimes \\exp(A) \\rVert_2 = \\lVert \\exp(B) \\rVert_2 \\lVert \\exp(A) \\rVert_2\n$$\nThis is precisely the sensitivity proxy $g(A,B)$ given in the problem. Thus, $g(A,B)$ is not a proxy but the exact operator norm of the forward map, quantifying its amplification potential. A large $g(A,B)$ indicates that the problem is ill-conditioned: small perturbations in the input $X$ can lead to large relative changes in the output.\n\nThe scaling-and-squaring algorithm for the matrix exponential is known to be backward stable. This means the computed approximation, $E_{final}$, is the exact exponential of a slightly perturbed matrix: $E_{final} = \\exp(M_{A,B} + \\Delta M)$, where $\\lVert \\Delta M \\rVert / \\lVert M_{A,B} \\rVert$ is on the order of machine precision.\n\nThe forward error is $\\lVert y_{\\text{approx}} - y_{\\text{ref}} \\rVert = \\lVert (\\exp(M_{A,B}+\\Delta M) - \\exp(M_{A,B}))x \\rVert$. Even with a backward stable algorithm (small $\\Delta M$), the forward error can be large if the matrix exponential function itself is ill-conditioned at $M_{A,B}$. The conditioning of the `expm` function, $\\kappa_{\\exp}(M)$, is large for nonnormal matrices $M$. Nonnormality in $A$ or $B$ induces nonnormality in $M_{A,B} = I_n \\otimes A + B \\otimes I_m$.\n\nIn summary, for nonnormal $A$ and $B$, as in Case 2, we can expect:\n1.  Large transient growth in $\\exp(At)$ or $\\exp(Bt)$, leading to large norms $\\lVert \\exp(A)\\rVert_2$ and $\\lVert \\exp(B)\\rVert_2$.\n2.  A large sensitivity measure $g(A,B)$, indicating an ill-conditioned problem.\n3.  A large condition number $\\kappa_{\\exp}(M_{A,B})$ for the matrix exponential calculation.\n4.  The combination of backward stability and ill-conditioning means that the computed relative error $e$ can be significantly larger than machine precision. The error is roughly bounded by $e \\lesssim g(A,B) \\cdot \\kappa_{\\exp}(M_{A,B}) \\cdot u$, where $u$ is machine epsilon. The presence of nonnormality inflates both $g(A,B)$ and $\\kappa_{\\exp}$ and can lead to a loss of accuracy. Case 2 is designed to demonstrate this effect.", "answer": "```python\nimport numpy as np\nimport scipy.linalg\n\ndef solve():\n    \"\"\"\n    Solves the problem of evaluating vec(exp(A) X exp(B.T)) using a \n    scaling-and-squaring algorithm on the Kronecker sum operator.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    A1 = np.array([[-1., 0.], [0., -2.]])\n    B1 = np.array([[-0.5, 0.], [0., -1.5]])\n    X1 = np.array([[1., 2.], [3., 4.]])\n\n    A2 = np.array([[-1., 10., 0.], [0., -1., 10.], [0., 0., -1.]])\n    B2 = np.array([[0., 10., 0.], [0., 0., 10.], [0., 0., 0.]])\n    X2 = np.array([[1., 0., 1.], [0., 1., 0.], [1., 0., 1.]])\n\n    A3 = np.diag(np.full(3, 5.), k=1)\n    B3 = -4. * np.identity(4)\n    # The problem specifies reshape such that vec(X) = [1, 2, ..., 16]^T\n    X3 = np.arange(1., 17.).reshape((4, 4), order='F')\n\n    test_cases = [\n        (A1, B1, X1),\n        (A2, B2, X2),\n        (A3, B3, X3),\n    ]\n\n    results = []\n    \n    # Degree of Taylor Polynomial\n    K = 16\n    # Tolerance for pass/fail\n    TOL = 1e-9\n\n    for A, B, X in test_cases:\n        m, n = A.shape[0], B.shape[0]\n\n        # --- Reference Calculation ---\n        expA_ref = scipy.linalg.expm(A)\n        expBT_ref = scipy.linalg.expm(B.T)\n        Y_ref = expA_ref @ X @ expBT_ref\n        y_ref = Y_ref.flatten('F')\n\n        # --- Scaling-and-Squaring Algorithm on Kronecker Sum ---\n        \n        # 1. Construct Operator\n        I_m, I_n = np.identity(m), np.identity(n)\n        M_AB = np.kron(I_n, A) + np.kron(B, I_m)\n\n        # 2. Scaling\n        m_norm = np.linalg.norm(M_AB, ord=np.inf)\n        target_norm = 0.5\n        s = 0\n        if m_norm > target_norm:\n            s = int(np.ceil(np.log2(m_norm / target_norm)))\n            \n        Z = M_AB / (2**s)\n\n        # 3. Taylor Series Approximation\n        E_scaled = np.identity(m * n)\n        term = np.identity(m * n)\n        for j in range(1, K + 1):\n            term = term @ Z / j\n            E_scaled += term\n\n        # 4. Squaring\n        E_final = E_scaled\n        for _ in range(s):\n            E_final = E_final @ E_final\n\n        # 5. Final Application\n        x_vec = X.flatten('F')\n        y_approx = E_final @ x_vec\n\n        # --- Compute Output Metrics ---\n        y_ref_norm = np.linalg.norm(y_ref, 2)\n        error_norm = np.linalg.norm(y_approx - y_ref, 2)\n        \n        e = error_norm / y_ref_norm if y_ref_norm > 0 else error_norm\n        \n        passed = e  TOL\n        \n        # Note: norm(exp(B.T), 2) == norm(exp(B), 2)\n        g = np.linalg.norm(expA_ref, 2) * np.linalg.norm(scipy.linalg.expm(B), 2)\n\n        results.extend([e, passed, g])\n\n    # Final print statement in the exact required format.\n    formatted_results = []\n    for item in results:\n        if isinstance(item, bool):\n            formatted_results.append(str(item))\n        else:\n            formatted_results.append(f\"{item:.15e}\")\n            \n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3553560"}]}