{"hands_on_practices": [{"introduction": "This first exercise provides a foundational, hands-on understanding of the CANDECOMP/PARAFAC (CP) model. Before we can develop algorithms to decompose a tensor, it is essential to grasp the forward process: how the factor matrices combine to construct the full tensor. By manually computing the entries of a small, hypothetical tensor from its given factors, you will solidify your understanding of the multilinear structure that the Alternating Least Squares algorithm aims to uncover [@problem_id:3533244].", "problem": "Consider the CANonical DECOMPosition/PARAFAC (CP) model for a three-way tensor, and its use within Alternating Least Squares (ALS), where one fixes two factor matrices and solves a least-squares subproblem for the remaining factor. Work with a concrete rank-$R$ CP representation for a tensor of size $I \\times J \\times K$ given by the sum of $R$ rank-one outer products. Let $I=J=K=2$ and $R=2$. Specify the factor matrices\n$$\nA=\\begin{pmatrix}\n1  2\\\\\n3  1\n\\end{pmatrix},\\quad\nB=\\begin{pmatrix}\n0  1\\\\\n2  -1\n\\end{pmatrix},\\quad\nC=\\begin{pmatrix}\n1  0\\\\\n1  2\n\\end{pmatrix},\n$$\nwhere each column corresponds to a rank-one component. Using the CP definition, the reconstructed tensor $\\mathcal{X}\\in\\mathbb{R}^{2\\times 2\\times 2}$ has entries $x_{i j k}$ obtained from the factors.\n\nTasks:\n- Starting from the rank-one outer-product structure that defines CP, derive the formula for the entries $x_{i j k}$ in terms of $A$, $B$, and $C$, and compute all eight entries of the reconstructed tensor $\\mathcal{X}$ for the specified $A$, $B$, and $C$.\n- Express each frontal slice $\\mathcal{X}(:,:,k)$ as a matrix in terms of $A$, $B$, and the diagonal matrix formed from the $k$-th row of $C$, and verify consistency with the computed entries.\n- Compute the Frobenius norm $\\|\\mathcal{X}\\|_{F}$ of the reconstructed tensor. If you decide to approximate, round your answer to four significant figures; otherwise, provide the exact value.\n\nYour final reported quantity must be the Frobenius norm $\\|\\mathcal{X}\\|_{F}$ as a single real number or a single closed-form analytic expression.", "solution": "The problem is valid as it is scientifically grounded in the principles of multilinear algebra, specifically the CANDECOMP/PARAFAC (CP) decomposition, and is well-posed with all necessary information provided for a unique solution.\n\nThe CANDECOMP/PARAFAC (CP) decomposition of a three-way tensor $\\mathcal{X} \\in \\mathbb{R}^{I \\times J \\times K}$ of rank $R$ is given by a sum of $R$ rank-one tensors. Each rank-one tensor is the outer product of three vectors. Let the factor matrices be $A \\in \\mathbb{R}^{I \\times R}$, $B \\in \\mathbb{R}^{J \\times R}$, and $C \\in \\mathbb{R}^{K \\times R}$. Let the columns of these matrices be $\\mathbf{a}_r$, $\\mathbf{b}_r$, and $\\mathbf{c}_r$ for $r=1, \\dots, R$. The tensor $\\mathcal{X}$ is then expressed as:\n$$\n\\mathcal{X} = \\sum_{r=1}^{R} \\mathbf{a}_r \\circ \\mathbf{b}_r \\circ \\mathbf{c}_r\n$$\nwhere $\\circ$ denotes the outer product.\n\nThe first task is to derive the formula for the entries $x_{ijk}$ of the tensor $\\mathcal{X}$. From the definition of the outer product, an element $(i,j,k)$ of the tensor $\\mathbf{a}_r \\circ \\mathbf{b}_r \\circ \\mathbf{c}_r$ is given by the product of the corresponding vector elements, $(a_r)_i (b_r)_j (c_r)_k$. In matrix notation, these are $a_{ir}$, $b_{jr}$, and $c_{kr}$. Thus, the entry $x_{ijk}$ of the tensor $\\mathcal{X}$ is obtained by summing over the $R$ components:\n$$\nx_{ijk} = \\sum_{r=1}^{R} a_{ir} b_{jr} c_{kr}\n$$\nIn this problem, the dimensions are $I=J=K=2$ and the rank is $R=2$. The given factor matrices are:\n$$\nA=\\begin{pmatrix} 1  2\\\\ 3  1 \\end{pmatrix},\\quad\nB=\\begin{pmatrix} 0  1\\\\ 2  -1 \\end{pmatrix},\\quad\nC=\\begin{pmatrix} 1  0\\\\ 1  2 \\end{pmatrix}\n$$\nThe entries of the reconstructed tensor $\\mathcal{X} \\in \\mathbb{R}^{2 \\times 2 \\times 2}$ are calculated using the formula for $R=2$:\n$$\nx_{ijk} = a_{i1}b_{j1}c_{k1} + a_{i2}b_{j2}c_{k2}\n$$\nWe compute all $8$ entries, organized into the two frontal slices, $\\mathcal{X}(:,:,1)$ and $\\mathcal{X}(:,:,2)$.\n\nFor the first frontal slice ($k=1$):\nThe first row of $C$ is $(c_{11}, c_{12}) = (1, 0)$.\n$x_{111} = a_{11}b_{11}c_{11} + a_{12}b_{12}c_{12} = (1)(0)(1) + (2)(1)(0) = 0$\n$x_{121} = a_{11}b_{21}c_{11} + a_{12}b_{22}c_{12} = (1)(2)(1) + (2)(-1)(0) = 2$\n$x_{211} = a_{21}b_{11}c_{11} + a_{22}b_{12}c_{12} = (3)(0)(1) + (1)(1)(0) = 0$\n$x_{221} = a_{21}b_{21}c_{11} + a_{22}b_{22}c_{12} = (3)(2)(1) + (1)(-1)(0) = 6$\nSo, the first frontal slice is the matrix $\\mathcal{X}(:,:,1) = \\begin{pmatrix} 0  2 \\\\ 0  6 \\end{pmatrix}$.\n\nFor the second frontal slice ($k=2$):\nThe second row of $C$ is $(c_{21}, c_{22}) = (1, 2)$.\n$x_{112} = a_{11}b_{11}c_{21} + a_{12}b_{12}c_{22} = (1)(0)(1) + (2)(1)(2) = 4$\n$x_{122} = a_{11}b_{21}c_{21} + a_{12}b_{22}c_{22} = (1)(2)(1) + (2)(-1)(2) = 2 - 4 = -2$\n$x_{212} = a_{21}b_{11}c_{21} + a_{22}b_{12}c_{22} = (3)(0)(1) + (1)(1)(2) = 2$\n$x_{222} = a_{21}b_{21}c_{21} + a_{22}b_{22}c_{22} = (3)(2)(1) + (1)(-1)(2) = 6 - 2 = 4$\nSo, the second frontal slice is the matrix $\\mathcal{X}(:,:,2) = \\begin{pmatrix} 4  -2 \\\\ 2  4 \\end{pmatrix}$.\n\nThe second task is to express each frontal slice $\\mathcal{X}(:,:,k)$, which we denote by the matrix $X_{(k)}$, in terms of $A$, $B$, and a diagonal matrix formed from the $k$-th row of $C$. The formula for the $k$-th frontal slice is $X_{(k)} = A D_k(C) B^T$, where $D_k(C)$ is a diagonal matrix whose diagonal entries are the elements of the $k$-th row of $C$, i.e., $(D_k(C))_{rr} = c_{kr}$.\nLet's verify this formula by computing the $(i,j)$-th entry of $A D_k(C) B^T$:\n$$\n(A D_k(C) B^T)_{ij} = \\sum_{r=1}^{R} (A D_k(C))_{ir} (B^T)_{rj} = \\sum_{r=1}^{R} \\left( \\sum_{s=1}^{R} a_{is} (D_k(C))_{sr} \\right) b_{jr}\n$$\nSince $D_k(C)$ is diagonal, $(D_k(C))_{sr} = c_{kr}\\delta_{sr}$, where $\\delta_{sr}$ is the Kronecker delta.\n$$\n(A D_k(C) B^T)_{ij} = \\sum_{r=1}^{R} \\left( \\sum_{s=1}^{R} a_{is} c_{ks}\\delta_{sr} \\right) b_{jr} = \\sum_{r=1}^{R} a_{ir} c_{kr} b_{jr} = \\sum_{r=1}^{R} a_{ir}b_{jr}c_{kr} = x_{ijk}\n$$\nThis confirms the slice-wise formula. Now we verify it for the given matrices.\n\nFor $k=1$, the first row of $C$ is $(1,0)$, so $D_1(C) = \\begin{pmatrix} 1  0 \\\\ 0  0 \\end{pmatrix}$.\n$$\nX_{(1)} = A D_1(C) B^T = \\begin{pmatrix} 1  2 \\\\ 3  1 \\end{pmatrix} \\begin{pmatrix} 1  0 \\\\ 0  0 \\end{pmatrix} \\begin{pmatrix} 0  1 \\\\ 2  -1 \\end{pmatrix}^T\n= \\begin{pmatrix} 1  0 \\\\ 3  0 \\end{pmatrix} \\begin{pmatrix} 0  2 \\\\ 1  -1 \\end{pmatrix}\n= \\begin{pmatrix} 0  2 \\\\ 0  6 \\end{pmatrix}\n$$\nThis matches our computed $\\mathcal{X}(:,:,1)$.\n\nFor $k=2$, the second row of $C$ is $(1,2)$, so $D_2(C) = \\begin{pmatrix} 1  0 \\\\ 0  2 \\end{pmatrix}$.\n$$\nX_{(2)} = A D_2(C) B^T = \\begin{pmatrix} 1  2 \\\\ 3  1 \\end{pmatrix} \\begin{pmatrix} 1  0 \\\\ 0  2 \\end{pmatrix} \\begin{pmatrix} 0  1 \\\\ 2  -1 \\end{pmatrix}^T\n= \\begin{pmatrix} 1  4 \\\\ 3  2 \\end{pmatrix} \\begin{pmatrix} 0  2 \\\\ 1  -1 \\end{pmatrix}\n= \\begin{pmatrix} 4  2-4 \\\\ 2  6-2 \\end{pmatrix}\n= \\begin{pmatrix} 4  -2 \\\\ 2  4 \\end{pmatrix}\n$$\nThis matches our computed $\\mathcal{X}(:,:,2)$.\n\nThe third task is to compute the Frobenius norm $\\|\\mathcal{X}\\|_{F}$ of the reconstructed tensor. The squared Frobenius norm is the sum of the squares of all its elements:\n$$\n\\|\\mathcal{X}\\|_{F}^2 = \\sum_{i=1}^{I} \\sum_{j=1}^{J} \\sum_{k=1}^{K} x_{ijk}^2\n$$\nUsing the entries we have calculated:\n$$\n\\|\\mathcal{X}\\|_{F}^2 = \\underbrace{(0^2 + 2^2 + 0^2 + 6^2)}_{\\text{Slice 1}} + \\underbrace{(4^2 + (-2)^2 + 2^2 + 4^2)}_{\\text{Slice 2}}\n$$\n$$\n\\|\\mathcal{X}\\|_{F}^2 = (0 + 4 + 0 + 36) + (16 + 4 + 4 + 16)\n$$\n$$\n\\|\\mathcal{X}\\|_{F}^2 = 40 + 40 = 80\n$$\nThe Frobenius norm is the square root of this value:\n$$\n\\|\\mathcal{X}\\|_{F} = \\sqrt{80} = \\sqrt{16 \\times 5} = 4\\sqrt{5}\n$$\nThe problem specifies to provide the exact value or an approximation to four significant figures. The exact value is $4\\sqrt{5}$.", "answer": "$$\n\\boxed{4\\sqrt{5}}\n$$", "id": "3533244"}, {"introduction": "The Alternating Least Squares (ALS) algorithm iteratively solves a sequence of linear least-squares subproblems, which form its computational core. While mathematically equivalent in exact arithmetic, different numerical methods for solving these subproblems can yield vastly different levels of accuracy in practice. This problem explores the critical trade-off between the computationally efficient Normal Equations approach and the more robust QR factorization method, quantifying the potential loss of precision that arises from squaring the condition number of the system matrix [@problem_id:3533190].", "problem": "Consider a single Alternating Least Squares (ALS) update in the Canonical Polyadic (CP) decomposition. To update the factor matrix $A \\in \\mathbb{R}^{I_{A} \\times R}$, one solves, row-wise, the overdetermined least-squares problems with common design matrix $Z \\in \\mathbb{R}^{(I_{B} I_{C}) \\times R}$ given by the Khatri–Rao product $Z = C \\odot B$, where $B \\in \\mathbb{R}^{I_{B} \\times R}$ and $C \\in \\mathbb{R}^{I_{C} \\times R}$. The right-hand side is obtained by the Matricized-Tensor Times Khatri–Rao Product (MTTKRP). Assume $Z$ has full column rank and computations are performed in floating-point arithmetic with unit roundoff $u$. Two classical approaches are used:\n\n- Normal equations with Cholesky factorization: form $G = Z^{\\top} Z$ and solve $G x = Z^{\\top} b$ via Cholesky.\n- Orthogonal–Triangular (QR) factorization: compute a thin QR factorization $Z = Q R$ with $Q^{\\top} Q = I$ and solve $R x = Q^{\\top} b$.\n\nStarting from the singular value decomposition definition of the $2$-norm condition number $\\kappa_{2}(Z) = \\sigma_{\\max}(Z)/\\sigma_{\\min}(Z)$, the relationship between the singular values of $Z$ and those of $Z^{\\top} Z$, and standard backward stability properties of Cholesky and QR factorizations, compare the forward accuracy of the two approaches for solving the overdetermined least-squares problems with design matrix $Z = C \\odot B$. In your comparison, use only the following base facts: (i) the spectral norm is submultiplicative, (ii) the singular values of $Z^{\\top} Z$ are the squares of the singular values of $Z$, (iii) QR factorization of a full-rank matrix is backward stable, and (iv) Cholesky factorization of a symmetric positive definite matrix is backward stable. You may also use that the Khatri–Rao product satisfies $Z = C \\odot B$ and is a column-submatrix of the Kronecker product $C \\otimes B$ with known norm identities for the Kronecker product.\n\nQuantify the accuracy gap in terms of $u$ and $\\kappa_{2}(Z)$, and then evaluate it in the following concrete scenario: suppose $\\kappa_{2}(B) = 10^{4}$, $\\kappa_{2}(C) = 10^{3}$, and IEEE double precision with $u \\approx 10^{-16}$ is used. Select all statements that are correct.\n\nA. In floating-point arithmetic, the normal-equations-plus-Cholesky route yields a forward relative error on the order of $u \\,\\kappa_{2}(Z)^{2}$ per right-hand side, while the Orthogonal–Triangular (QR) route yields a forward relative error on the order of $u \\,\\kappa_{2}(Z)$. Consequently, relative to QR, normal equations lose about an additional $\\log_{10}\\!\\big(\\kappa_{2}(Z)\\big)$ correct decimal digits. For the given numerical values, one may bound $\\kappa_{2}(Z) \\le \\kappa_{2}(C)\\,\\kappa_{2}(B) = 10^{7}$, so the error magnitudes are approximately $10^{-2}$ (normal equations) versus $10^{-9}$ (QR), i.e., about $7$ extra digits lost by normal equations.\n\nB. Because $(C \\odot B)^{\\top} (C \\odot B) = (C^{\\top} C) \\circ (B^{\\top} B)$ is a Hadamard product, the condition number of the normal equations is at most $\\max\\{\\kappa_{2}(B)^{2}, \\kappa_{2}(C)^{2}\\}$, implying no asymptotic accuracy loss relative to QR.\n\nC. If the columns of $B$ and $C$ are scaled to unit norm, then $\\kappa_{2}(C \\odot B) = 1$ regardless of column correlations, so both approaches have comparable accuracy up to constants.\n\nD. When $Z$ is very tall and skinny, with $(I_{B} I_{C}) \\gg R$, both approaches achieve forward relative error on the order of $u$ independent of $\\kappa_{2}(Z)$, so there is no meaningful accuracy gap between them in that regime.", "solution": "The user is asking for a comparison of the forward accuracy of two methods for solving the overdetermined least-squares problems that arise in the Alternating Least Squares (ALS) algorithm for Canonical Polyadic (CP) decomposition.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- **Problem Context**: A single ALS update for a factor matrix $A \\in \\mathbb{R}^{I_{A} \\times R}$ in a CP decomposition.\n- **Task**: Solve, row-wise, overdetermined least-squares problems $\\min_{x} \\|Zx-b\\|_{2}$.\n- **Design Matrix**: $Z \\in \\mathbb{R}^{(I_{B} I_{C}) \\times R}$ where $Z = C \\odot B$ (Khatri-Rao product).\n- **Factor Matrices**: $B \\in \\mathbb{R}^{I_{B} \\times R}$ and $C \\in \\mathbb{R}^{I_{C} \\times R}$.\n- **Right-hand side**: $b$ is from the Matricized-Tensor Times Khatri–Rao Product (MTTKRP).\n- **Assumptions**: $Z$ has full column rank. Computations are in floating-point arithmetic with unit roundoff $u$.\n- **Method 1 (Normal Equations)**: Form $G = Z^{\\top} Z$ and solve $Gx = Z^{\\top} b$ using Cholesky factorization.\n- **Method 2 (QR Factorization)**: Compute a thin QR factorization $Z = QR$ and solve $Rx = Q^{\\top}b$.\n- **Base Facts**:\n    (i) Spectral norm is submultiplicative.\n    (ii) Singular values of $Z^{\\top} Z$ are the squares of the singular values of $Z$.\n    (iii) QR factorization of a full-rank matrix is backward stable.\n    (iv) Cholesky factorization of a symmetric positive definite matrix is backward stable.\n- **Additional Information**: One may use that $Z = C \\odot B$ is a column-submatrix of $C \\otimes B$ and use known norm identities for the Kronecker product.\n- **Numerical Scenario**: $\\kappa_{2}(B) = 10^{4}$, $\\kappa_{2}(C) = 10^{3}$, $u \\approx 10^{-16}$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem statement is scientifically sound, well-posed, and objective. It poses a standard question in numerical linear algebra concerning the stability of algorithms for solving least-squares problems. The context of CP-ALS is appropriate, as these subproblems are a core component of that algorithm. The given facts are standard results from matrix analysis and numerical analysis. The assumptions are clear and sufficient to perform the requested comparison. The problem does not violate any of the invalidity criteria.\n\n**Step 3: Verdict and Action**\nThe problem is valid. I will proceed with the detailed derivation and evaluation.\n\n### Derivation and Solution\n\nThe core of the problem is to compare the forward error of the least-squares solution computed via the normal equations versus the QR factorization. Let the exact solution to $\\min_{x} \\|Zx - b\\|_{2}$ be $x_{LS} = (Z^{\\top}Z)^{-1}Z^{\\top}b$. The relative forward error for a computed solution $\\hat{x}$ is given by $\\frac{\\|\\hat{x} - x_{LS}\\|_{2}}{\\|x_{LS}\\|_{2}}$.\n\n**1. Analysis of the Normal Equations (NE) Approach**\n\nThis approach consists of two main steps:\n1.  Forming the normal equations matrix $G = Z^{\\top}Z$ and the vector $Z^{\\top}b$.\n2.  Solving the linear system $Gx = Z^{\\top}b$ using Cholesky factorization.\n\nThe primary source of numerical instability is the formation of $G = Z^{\\top}Z$, which explicitly squares the condition number of the problem.\n\n-   Let the singular values of $Z$ be $\\sigma_{1} \\ge \\sigma_{2} \\ge \\dots \\ge \\sigma_{R}  0$.\n-   The $2$-norm condition number of $Z$ is $\\kappa_{2}(Z) = \\frac{\\sigma_{\\max}(Z)}{\\sigma_{\\min}(Z)} = \\frac{\\sigma_{1}}{\\sigma_{R}}$.\n-   From the given fact (ii), the singular values of $G = Z^{\\top}Z$ are the squares of the singular values of $Z$. The eigenvalues of the symmetric positive definite matrix $G$ are its singular values, so $\\lambda_{i}(G) = \\sigma_{i}(Z)^{2}$.\n-   The condition number of $G$ is therefore:\n    $$ \\kappa_{2}(G) = \\frac{\\lambda_{\\max}(G)}{\\lambda_{\\min}(G)} = \\frac{\\sigma_{\\max}(Z)^{2}}{\\sigma_{\\min}(Z)^{2}} = \\left(\\frac{\\sigma_{\\max}(Z)}{\\sigma_{\\min}(Z)}\\right)^{2} = \\kappa_{2}(Z)^{2} $$\n-   The Cholesky factorization used to solve $Gx = Z^{\\top}b$ is backward stable (fact (iv)). This means the computed solution $\\hat{x}_{NE}$ is the exact solution to a nearby system. Standard forward error analysis for solving a linear system $Ax=b$ shows that the relative forward error is bounded by a quantity proportional to the condition number of the matrix, i.e., $\\frac{\\|\\delta x\\|_{2}}{\\|x\\|_{2}} \\lesssim u \\kappa_{2}(A)$.\n-   Applying this to the system $Gx=Z^{\\top}b$, the error incurred just from solving the system is on the order of $u \\kappa_{2}(G) = u \\kappa_{2}(Z)^{2}$. This does not even account for the error in forming $G$ and $Z^{\\top}b$. The full analysis confirms that the forward error for the NE method is dominated by this term.\n-   Thus, the relative forward error for the normal equations approach is approximately $\\mathcal{O}(u \\, \\kappa_{2}(Z)^{2})$.\n\n**2. Analysis of the QR Factorization Approach**\n\nThis approach avoids forming $Z^{\\top}Z$.\n1.  A thin QR factorization $Z = QR$ is computed, where $Q \\in \\mathbb{R}^{(I_{B} I_{C}) \\times R}$ has orthonormal columns ($Q^{\\top}Q=I_{R}$) and $R \\in \\mathbb{R}^{R \\times R}$ is upper triangular.\n2.  The least-squares problem $\\min \\|Zx-b\\|_{2}$ is transformed into $\\min \\|QRx-b\\|_{2}$. Since $Q$ is an orthogonal transformation (preserving the $2$-norm), this is equivalent to $\\min \\|Rx - Q^{\\top}b\\|_{2}$.\n3.  As $Z$ has full column rank, $R$ is invertible, and the solution is found by solving the triangular system $Rx = Q^{\\top}b$.\n\n-   The QR factorization of a full-rank matrix is backward stable (fact (iii)). Standard analysis shows that the computed solution $\\hat{x}_{QR}$ is the exact least-squares solution for a slightly perturbed problem involving $(Z+\\delta Z)$ and $(b+\\delta b)$.\n-   The resulting forward error bound for the QR method is approximately $\\mathcal{O}(u \\, \\kappa_{2}(Z))$, ignoring terms that depend on the residual norm which can also be significant. For a general comparison of sensitivity to matrix conditioning, this is the key term.\n\n**3. Comparison and Quantification**\n-   **Normal Equations Error**: $\\approx C_{NE} \\, u \\, \\kappa_{2}(Z)^{2}$\n-   **QR Factorization Error**: $\\approx C_{QR} \\, u \\, \\kappa_{2}(Z)$\n-   The factor $\\kappa_{2}(Z)$ represents the accuracy gap. If $\\kappa_{2}(Z)$ is large, the normal equations method can be significantly less accurate.\n-   The number of correct decimal digits in a result with relative error $\\epsilon$ is roughly $-\\log_{10}(\\epsilon)$.\n-   Additional digits lost by NE compared to QR = $(-\\log_{10}(\\text{Error}_{QR})) - (-\\log_{10}(\\text{Error}_{NE})) \\approx (-\\log_{10}(u \\, \\kappa_{2}(Z))) - (-\\log_{10}(u \\, \\kappa_{2}(Z)^{2})) = \\log_{10}(u \\, \\kappa_{2}(Z)^{2}) - \\log_{10}(u \\, \\kappa_{2}(Z)) = \\log_{10}\\left(\\frac{u \\, \\kappa_{2}(Z)^{2}}{u \\, \\kappa_{2}(Z)}\\right) = \\log_{10}(\\kappa_{2}(Z))$.\n\n**4. Evaluation of the Concrete Scenario**\n-   Given: $\\kappa_{2}(B) = 10^{4}$, $\\kappa_{2}(C) = 10^{3}$, $u \\approx 10^{-16}$.\n-   The problem allows using the property that $Z = C \\odot B$ is related to the Kronecker product $C \\otimes B$. A standard result for the Khatri-Rao product is the inequality $\\kappa_{2}(C \\odot B) \\le \\kappa_{2}(C) \\kappa_{2}(B)$. This provides an upper bound on the condition number of $Z$.\n-   Using this bound: $\\kappa_{2}(Z) \\le \\kappa_{2}(C)\\kappa_{2}(B) = 10^{3} \\times 10^{4} = 10^{7}$.\n-   Let's use this value for the estimate: $\\kappa_{2}(Z) \\approx 10^{7}$.\n-   **NE Error Estimate**: $u \\, \\kappa_{2}(Z)^{2} \\approx 10^{-16} \\times (10^{7})^{2} = 10^{-16} \\times 10^{14} = 10^{-2}$.\n-   **QR Error Estimate**: $u \\, \\kappa_{2}(Z) \\approx 10^{-16} \\times 10^{7} = 10^{-9}$.\n-   **Extra digits lost by NE**: $\\log_{10}(\\kappa_{2}(Z)) \\approx \\log_{10}(10^{7}) = 7$.\n\n### Option-by-Option Analysis\n\n**A. In floating-point arithmetic, the normal-equations-plus-Cholesky route yields a forward relative error on the order of $u \\,\\kappa_{2}(Z)^{2}$ per right-hand side, while the Orthogonal–Triangular (QR) route yields a forward relative error on the order of $u \\,\\kappa_{2}(Z)$. Consequently, relative to QR, normal equations lose about an additional $\\log_{10}\\!\\big(\\kappa_{2}(Z)\\big)$ correct decimal digits. For the given numerical values, one may bound $\\kappa_{2}(Z) \\le \\kappa_{2}(C)\\,\\kappa_{2}(B) = 10^{7}$, so the error magnitudes are approximately $10^{-2}$ (normal equations) versus $10^{-9}$ (QR), i.e., about $7$ extra digits lost by normal equations.**\nThis statement perfectly matches the derivation above. It correctly identifies the orders of magnitude for the forward errors of both methods, correctly quantifies the loss of precision in terms of digits, correctly applies the standard inequality for the condition number of a Khatri-Rao product, and correctly calculates the estimated errors and lost digits for the given scenario.\n**Verdict: Correct**\n\n**B. Because $(C \\odot B)^{\\top} (C \\odot B) = (C^{\\top} C) \\circ (B^{\\top} B)$ is a Hadamard product, the condition number of the normal equations is at most $\\max\\{\\kappa_{2}(B)^{2}, \\kappa_{2}(C)^{2}\\}$, implying no asymptotic accuracy loss relative to QR.**\nThe identity $(C \\odot B)^{\\top} (C \\odot B) = (C^{\\top} C) \\circ (B^{\\top} B)$ is correct. However, the subsequent claim about the condition number, $\\kappa_{2}((C^{\\top} C) \\circ (B^{\\top} B)) \\le \\max\\{\\kappa_{2}(C^{\\top} C), \\kappa_{2}(B^{\\top} B)\\}$, is incorrect. There is no general theorem in matrix analysis that provides such a tight bound for the condition number of a Hadamard product of two positive semidefinite matrices. The condition number of a Hadamard product is generally not bounded by the maximum of the individual condition numbers. In fact, standard bounds relate it to the product of the condition numbers, not the maximum. The conclusion of \"no asymptotic accuracy loss\" is therefore based on a false premise.\n**Verdict: Incorrect**\n\n**C. If the columns of $B$ and $C$ are scaled to unit norm, then $\\kappa_{2}(C \\odot B) = 1$ regardless of column correlations, so both approaches have comparable accuracy up to constants.**\nIf the columns $b_r$ and $c_r$ have unit norm, then the columns of $Z = C \\odot B$, which are $z_r = c_r \\otimes b_r$, also have unit norm since $\\|z_r\\|_2 = \\|c_r \\otimes b_r\\|_2 = \\|c_r\\|_2 \\|b_r\\|_2 = 1 \\times 1 = 1$. However, for $\\kappa_2(Z)$ to be $1$, the matrix $Z$ must have orthonormal columns, meaning $Z^{\\top}Z = I$. The $(i,j)$-th entry of $Z^{\\top}Z$ is $(c_i^{\\top}c_j)(b_i^{\\top}b_j)$. This product must be zero for $i \\neq j$. This is not guaranteed; if the columns within $C$ and $B$ are correlated (i.e., not orthogonal), this product will be non-zero. The phrase \"regardless of column correlations\" makes the statement definitively false. A high correlation between columns can make $\\kappa_2(Z)$ arbitrarily large, even if all columns are normalized.\n**Verdict: Incorrect**\n\n**D. When $Z$ is very tall and skinny, with $(I_{B} I_{C}) \\gg R$, both approaches achieve forward relative error on the order of $u$ independent of $\\kappa_{2}(Z)$, so there is no meaningful accuracy gap between them in that regime.**\nThis is a false claim. Standard forward error bounds for least-squares solvers, for both the NE and QR methods, are fundamentally dependent on the condition number $\\kappa_{2}(Z)$. The sensitivity of the solution $x_{LS}$ to perturbations in $Z$ and $b$ is governed by $\\kappa_{2}(Z)$, regardless of the matrix dimensions. A \"tall and skinny\" geometry does not eliminate this intrinsic sensitivity. While for certain random matrix ensembles, the condition number might behave well as dimensions grow, the error bounds for a *given* matrix $Z$ always depend on *its* condition number. There is no general principle that makes the error $\\mathcal{O}(u)$.\n**Verdict: Incorrect**", "answer": "$$\\boxed{A}$$", "id": "3533190"}, {"introduction": "The performance of CP-ALS is highly sensitive to its starting point, a direct consequence of the non-convex nature of the tensor decomposition problem. A good initialization can place the algorithm in a favorable basin of attraction, leading to rapid convergence, whereas a poor one can result in slow progress or convergence to a suboptimal local minimum. This practice contrasts a simple random initialization with a spectrally-informed strategy using the Higher-Order Singular Value Decomposition (HOSVD), providing deep insight into why intelligent initialization is a cornerstone of modern, effective CP-ALS implementations [@problem_id:3533196].", "problem": "Consider a third-order data tensor $\\mathcal{X} \\in \\mathbb{R}^{I \\times J \\times K}$ that admits a Canonical Polyadic (CP) decomposition of rank $R$, namely $\\mathcal{X} = \\sum_{r=1}^{R} \\lambda_r \\, a_r \\circ b_r \\circ c_r$, with factor matrices $A_\\star = [a_1,\\dots,a_R] \\in \\mathbb{R}^{I \\times R}$, $B_\\star \\in \\mathbb{R}^{J \\times R}$, $C_\\star \\in \\mathbb{R}^{K \\times R}$, and strictly positive weights $\\lambda_r  0$. Assume the representation is unique up to the usual permutation and scaling indeterminacies, and the factors are well-conditioned in the sense that their condition numbers are bounded by a moderate constant, and the mode-$n$ unfoldings of $\\mathcal{X}$ have clear spectral gaps between the $R$-th and $(R+1)$-st singular values. Suppose we fit a CP model to $\\mathcal{X}$ using Alternating Least Squares (ALS), which alternates between solving linear least-squares subproblems for $A \\in \\mathbb{R}^{I \\times R}$, $B \\in \\mathbb{R}^{J \\times R}$, and $C \\in \\mathbb{R}^{K \\times R}$ while keeping the other two factors fixed.\n\nTwo initializations for $(A,B,C)$ are considered:\n- Random Gaussian initialization: $A^{(0)},B^{(0)},C^{(0)}$ have independent and identically distributed entries with zero mean and variances chosen so that columns are in expectation of unit norm.\n- Higher-Order Singular Value Decomposition (HOSVD)-based initialization: for each mode-$n$ unfolding $X_{(n)}$ of $\\mathcal{X}$, compute its leading $R$ left singular vectors and use these orthonormal bases as the initial column spaces for the corresponding factor matrices.\n\nAssume either the noiseless setting $\\mathcal{X}$ is exactly rank-$R$, or the mildly noisy setting $\\mathcal{X} = \\mathcal{X}_\\star + \\mathcal{E}$ with small additive perturbation $\\mathcal{E}$ whose mode-$n$ operator norms are significantly smaller than the corresponding spectral gaps. Based on first principles from spectral analysis and subspace perturbation theory, select all statements that are most justified regarding which initialization is expected to yield faster convergence of ALS for a well-conditioned tensor.\n\nA. In the noiseless, well-conditioned case, the HOSVD-based initialization recovers the exact factor subspaces $\\mathrm{span}(A_\\star)$, $\\mathrm{span}(B_\\star)$, and $\\mathrm{span}(C_\\star)$ from the mode unfoldings, thereby placing ALS within a favorable local basin of attraction of the true CP solution and typically reducing the number of iterations relative to random Gaussian initialization.\n\nB. With small additive noise and nontrivial spectral gaps $\\gamma_n = \\sigma_R(X_{(n)}) - \\sigma_{R+1}(X_{(n)})$ in each mode, subspace perturbation bounds imply that the principal angles between the HOSVD-initialized subspaces and the true factor subspaces scale on the order of $\\|\\mathcal{E}\\|/\\gamma_n$, whereas randomly initialized subspaces have typical angles bounded away from $0$; consequently, the HOSVD-based initialization is expected to converge faster.\n\nC. Because each ALS step solves a linear least-squares problem whose conditioning depends on the Gram matrices of the current factors, better initial alignment of these factors with the true subspaces yields better conditioned subproblems and larger objective decrease per sweep; thus, on well-conditioned tensors, HOSVD-based initialization tends to accelerate convergence relative to random Gaussian initialization.\n\nD. When $\\mathcal{X}$ is exactly rank-$R$, the ALS objective is jointly convex in $(A,B,C)$, so the convergence speed is independent of initialization; therefore, random Gaussian and HOSVD-based initializations converge at essentially the same rate.\n\nE. HOSVD-based initialization is unsuitable for non-orthogonal CP factors and typically slows ALS relative to random Gaussian for well-conditioned tensors, because it biases the factors toward orthogonality and away from the true CP solution.", "solution": "The user wants a critical validation of the problem statement, followed by a detailed solution and evaluation of the provided options.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- A third-order data tensor $\\mathcal{X} \\in \\mathbb{R}^{I \\times J \\times K}$.\n- $\\mathcal{X}$ admits a Canonical Polyadic (CP) decomposition of rank $R$: $\\mathcal{X} = \\sum_{r=1}^{R} \\lambda_r \\, a_r \\circ b_r \\circ c_r$.\n- Factor matrices are $A_\\star = [a_1,\\dots,a_R] \\in \\mathbb{R}^{I \\times R}$, $B_\\star \\in \\mathbb{R}^{J \\times R}$, $C_\\star \\in \\mathbb{R}^{K \\times R}$.\n- Weights are strictly positive: $\\lambda_r  0$.\n- The representation is unique up to permutation and scaling.\n- Factors are well-conditioned (bounded condition numbers).\n- Mode-$n$ unfoldings $X_{(n)}$ of $\\mathcal{X}$ have clear spectral gaps between the $R$-th and $(R+1)$-st singular values.\n- The fitting algorithm is Alternating Least Squares (ALS) for factors $A, B, C$.\n- Two initializations are considered:\n    1. Random Gaussian initialization: $A^{(0)},B^{(0)},C^{(0)}$ with i.i.d. zero-mean entries.\n    2. Higher-Order Singular Value Decomposition (HOSVD)-based initialization: using the leading $R$ left singular vectors of each mode-$n$ unfolding $X_{(n)}$.\n- Two scenarios are considered:\n    1. Noiseless: $\\mathcal{X}$ is exactly rank-$R$.\n    2. Mildly noisy: $\\mathcal{X} = \\mathcal{X}_\\star + \\mathcal{E}$ with small perturbation $\\mathcal{E}$.\n- The objective is to select all justified statements regarding which initialization is expected to yield faster convergence of ALS.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded**: The problem is set within the established field of numerical multilinear algebra. Concepts like CP decomposition, ALS, HOSVD, mode unfoldings, singular values, and spectral gaps are standard and well-defined. The premises align with foundational principles of tensor analysis.\n- **Well-Posed**: The problem is clearly stated. It asks for a qualitative comparison of the convergence speed of two initialization methods for a specified algorithm under well-defined conditions. This is a standard question in the analysis of iterative algorithms.\n- **Objective**: The language is precise and technical, devoid of subjectivity or ambiguity. The assumptions (well-conditioned factors, spectral gaps) are standard in theoretical analyses of tensor algorithms.\n\n**Step 3: Verdict and Action**\nThe problem statement is valid. It is scientifically sound, well-posed, objective, and contains all necessary information to proceed with a principled analysis. I will proceed to the solution.\n\n### Solution Derivation\n\nThe problem concerns the convergence speed of the Alternating Least Squares (ALS) algorithm for computing the Canonical Polyadic (CP) decomposition of a tensor $\\mathcal{X}$. The convergence of ALS, an iterative non-convex optimization method, is highly sensitive to the initial guess for the factor matrices $(A, B, C)$. We are asked to compare a random initialization with an HOSVD-based initialization.\n\nThe ALS algorithm iteratively minimizes the objective function $\\| \\mathcal{X} - \\sum_{r=1}^R a_r \\circ b_r \\circ c_r \\|_F^2$ by updating one factor matrix at a time while keeping the others fixed. For example, updating $A$ involves solving the linear least-squares problem:\n$$ \\min_{A} \\| X_{(1)} - A(C \\odot B)^T \\|_F^2 $$\nwhere $X_{(1)}$ is the mode-$1$ unfolding of $\\mathcal{X}$ and $\\odot$ denotes the Khatri-Rao product. A good initialization places the iterates $(A^{(k)}, B^{(k)}, C^{(k)})$ in or near the basin of attraction of the true solution $(A_\\star, B_\\star, C_\\star)$, leading to rapid convergence.\n\nThe HOSVD-based initialization uses the leading $R$ left singular vectors of the mode-$n$ unfolding $X_{(n)}$ as the columns for the initial factor matrix of mode $n$. This is a spectrally-informed initialization. Random initialization is uninformed.\n\nWe now evaluate each statement.\n\n**A. In the noiseless, well-conditioned case, the HOSVD-based initialization recovers the exact factor subspaces $\\mathrm{span}(A_\\star)$, $\\mathrm{span}(B_\\star)$, and $\\mathrm{span}(C_\\star)$ from the mode unfoldings, thereby placing ALS within a favorable local basin of attraction of the true CP solution and typically reducing the number of iterations relative to random Gaussian initialization.**\n\nThis statement addresses the ideal noiseless scenario. The mode-$1$ unfolding of the CP model $\\mathcal{X}$ is given by $X_{(1)} = A_\\star (C_\\star \\odot B_\\star)^T$. The column space of $X_{(1)}$ is $\\mathrm{range}(X_{(1)}) = \\mathrm{range}(A_\\star (C_\\star \\odot B_\\star)^T)$. Since the CP decomposition is stated to be unique, the matrix $(C_\\star \\odot B_\\star)$ must have full column rank, which is $R$. Under this condition, the column space of $X_{(1)}$ is identical to the column space of $A_\\star$: $\\mathrm{range}(X_{(1)}) = \\mathrm{range}(A_\\star) = \\mathrm{span}(A_\\star)$. The same logic applies to other modes: $\\mathrm{range}(X_{(2)}) = \\mathrm{span}(B_\\star)$ and $\\mathrm{range}(X_{(3)}) = \\mathrm{span}(C_\\star)$.\n\nThe HOSVD-based initialization computes the leading $R$ left singular vectors of $X_{(n)}$. For a rank-$R$ matrix like $X_{(n)}$, its leading $R$ left singular vectors form an orthonormal basis for its column space. Therefore, the HOSVD initialization finds the *exact* subspaces spanned by the columns of the true factor matrices $A_\\star, B_\\star, B_\\star$.\n\nStarting the ALS algorithm with factors whose columns already span the correct subspaces is a massive advantage. The algorithm only needs to find the correct non-orthogonal representation within these already-identified subspaces, a much simpler task than finding the subspaces themselves from a random orientation. A random initialization, by contrast, will with high probability start in subspaces that are far from the true ones, requiring many iterations to perform the necessary rotations. Thus, the HOSVD initialization places the algorithm in a very favorable position, leading to faster convergence.\n\nVerdict: **Correct**.\n\n**B. With small additive noise and nontrivial spectral gaps $\\gamma_n = \\sigma_R(X_{(n)}) - \\sigma_{R+1}(X_{(n)})$ in each mode, subspace perturbation bounds imply that the principal angles between the HOSVD-initialized subspaces and the true factor subspaces scale on the order of $\\|\\mathcal{E}\\|/\\gamma_n$, whereas randomly initialized subspaces have typical angles bounded away from $0$; consequently, the HOSVD-based initialization is expected to converge faster.**\n\nThis statement generalizes the reasoning from A to the more practical noisy case, $\\mathcal{X} = \\mathcal{X}_\\star + \\mathcal{E}$. The mode-$n$ unfolding is $X_{(n)} = X_{\\star,(n)} + E_{(n)}$, where $X_{\\star,(n)}$ is the unfolding of the true rank-$R$ tensor $\\mathcal{X}_\\star$ and $E_{(n)}$ is the unfolding of the noise tensor $\\mathcal{E}$.\n\nThe HOSVD initialization identifies the leading $R$-dimensional left singular subspace of the perturbed matrix $X_{(n)}$. The true factor subspace is the left singular subspace of the unperturbed matrix $X_{\\star,(n)}$. Subspace perturbation theory, specifically the Davis-Kahan $\\sin\\Theta$ theorem, bounds the sine of the principal angles $\\Theta$ between these two subspaces. The bound is proportional to the norm of the perturbation divided by the spectral gap: $\\|\\sin\\Theta\\|_F \\leq \\frac{\\|E_{(n)}\\|_F}{\\sigma_R(X_{\\star,(n)}) - \\sigma_{R+1}(X_{(n)})}$. Given $\\sigma_{R+1}(X_{\\star,(n)}) = 0$ and the assumption of a small perturbation $\\mathcal{E}$, the gap $\\gamma_n$ in the problem is a good proxy for the theoretical gap. The statement that the angle scales with $\\|\\mathcal{E}\\|/\\gamma_n$ (where $\\|\\mathcal{E}\\|$ is a placeholder for the relevant norm of the perturbation) is a direct consequence of this established theory.\n\nFor small noise and a large gap, the angle is small, meaning the HOSVD-initialized subspaces are very close to the true subspaces. In contrast, a random $R$-dimensional subspace in a high-dimensional space will almost surely have a significant angle with any fixed target subspace. Starting \"close\" to the solution in terms of subspace alignment ensures that ALS begins in a region of the error landscape that is favorable for rapid convergence, often within the basin of quadratic convergence.\n\nVerdict: **Correct**.\n\n**C. Because each ALS step solves a linear least-squares problem whose conditioning depends on the Gram matrices of the current factors, better initial alignment of these factors with the true subspaces yields better conditioned subproblems and larger objective decrease per sweep; thus, on well-conditioned tensors, HOSVD-based initialization tends to accelerate convergence relative to random Gaussian initialization.**\n\nThis statement provides a mechanistic reason for faster convergence related to the numerical properties of the ALS subproblems. The solution to the least-squares problem for updating $A$ from $B$ and $C$ involves the matrix $(C \\odot B)^T(C \\odot B)$. This is a Gram matrix whose $(i,j)$-th entry is $(c_i^T c_j)(b_i^T b_j)$. The condition number of this matrix determines the stability and effectiveness of the update step. An ill-conditioned Gram matrix can lead to numerical errors and extremely slow progress, a phenomenon known as an ALS \"swamp\".\n\nThe HOSVD-based initialization provides initial factors (say, $B^{(0)}$ and $C^{(0)}$) with orthonormal columns. In this case, $(c_i^{(0)})^T c_j^{(0)} = \\delta_{ij}$ and $(b_i^{(0)})^T b_j^{(0)} = \\delta_{ij}$, making the Gram matrix the identity matrix, which has a condition number of $1$ (the best possible). Even in the noisy case, the HOSVD-initialized factors are close to orthonormal and will result in a well-conditioned Gram matrix. A random initialization has no such guarantee; it can easily produce nearly collinear vectors, leading to a severely ill-conditioned or singular Gram matrix, stalling the algorithm from the very first sweep. By ensuring the initial subproblems are well-conditioned, the HOSVD initialization facilitates more substantial and numerically stable updates, accelerating convergence.\n\nVerdict: **Correct**.\n\n**D. When $\\mathcal{X}$ is exactly rank-$R$, the ALS objective is jointly convex in $(A,B,C)$, so the convergence speed is independent of initialization; therefore, random Gaussian and HOSVD-based initializations converge at essentially the same rate.**\n\nThe premise of this statement is fundamentally flawed. The CP decomposition objective function $\\| \\mathcal{X} - \\sum_{r=1}^R a_r \\circ b_r \\circ c_r \\|_F^2$ is multilinear in the entries of $A$, $B$, and $C$. While it is convex in each factor matrix when the others are held fixed (which is why ALS subproblems are convex least-squares problems), it is **not** jointly convex in $(A,B,C)$. The optimization landscape is non-convex and known to contain multiple local minima and saddle points. Consequently, the performance of local search algorithms like ALS is highly dependent on the starting point (initialization). The claim that convergence speed is independent of initialization is incorrect.\n\nVerdict: **Incorrect**.\n\n**E. HOSVD-based initialization is unsuitable for non-orthogonal CP factors and typically slows ALS relative to random Gaussian for well-conditioned tensors, because it biases the factors toward orthogonality and away from the true CP solution.**\n\nThis statement misinterprets the role and benefit of HOSVD initialization. While it is true that HOSVD produces initial factors with orthonormal columns and that the true CP factors are generally not orthogonal, this \"bias\" is not a disadvantage. The primary benefit of HOSVD is identifying the correct *subspaces* spanned by the true factors (exactly in the noiseless case, approximately in the noisy case). The ALS algorithm can then easily find the correct (non-orthogonal) basis within this correct subspace. A random initialization starts in a random subspace, and rotating the entire subspace into the correct orientation is the most difficult part of the optimization process, requiring many iterations. By starting in the correct subspace, HOSVD initialization circumvents this difficult initial phase. The claim that it slows down ALS relative to a random start is contrary to both theoretical understanding and extensive practical evidence.\n\nVerdict: **Incorrect**.", "answer": "$$\\boxed{ABC}$$", "id": "3533196"}]}