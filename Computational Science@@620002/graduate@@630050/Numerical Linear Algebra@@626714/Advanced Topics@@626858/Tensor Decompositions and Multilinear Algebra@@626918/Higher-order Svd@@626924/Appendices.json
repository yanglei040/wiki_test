{"hands_on_practices": [{"introduction": "This first exercise is a fundamental workout, designed to solidify your grasp of the core computational steps of the Higher-Order Singular Value Decomposition. By working with a small, manageable tensor, you will manually perform the key operations: creating mode-$n$ unfoldings, finding the factor matrices from their singular vectors, and finally computing the core tensor. Completing this practice [@problem_id:1071392] will provide a concrete, step-by-step understanding of the HOSVD algorithm in action.", "problem": "The Higher-Order Singular Value Decomposition (HOSVD), also known as the Tucker decomposition, is a generalization of the matrix SVD to higher-order tensors. A real-valued tensor of order $N$, $\\mathcal{A} \\in \\mathbb{R}^{I_1 \\times I_2 \\times \\dots \\times I_N}$, can be decomposed into a core tensor $\\mathcal{G} \\in \\mathbb{R}^{R_1 \\times R_2 \\times \\dots \\times R_N}$ and a set of orthogonal factor matrices $U^{(n)} \\in \\mathbb{R}^{I_n \\times R_n}$ for $n=1, \\dots, N$. The multilinear rank of the decomposition is $(R_1, \\dots, R_N)$, where $R_n \\le I_n$.\n\nThe decomposition is expressed using the n-mode product:\n$$ \\mathcal{A} \\approx \\mathcal{G} \\times_1 U^{(1)} \\times_2 U^{(2)} \\dots \\times_N U^{(N)} $$\nThe n-mode product of a tensor $\\mathcal{X} \\in \\mathbb{R}^{I_1 \\times \\dots \\times I_N}$ with a matrix $M \\in \\mathbb{R}^{J_n \\times I_n}$ along the $n$-th mode is a new tensor $\\mathcal{Y} = \\mathcal{X} \\times_n M$ of size $I_1 \\times \\dots \\times J_n \\times \\dots \\times I_N$, whose elements are given by:\n$$ (\\mathcal{Y})_{i_1, \\dots, j_n, \\dots, i_N} = \\sum_{k=1}^{I_n} (\\mathcal{X})_{i_1, \\dots, k, \\dots, i_N} (M)_{j_n, k} $$\n\nThe factor matrices $U^{(n)}$ for the HOSVD are obtained from the mode-n unfoldings of $\\mathcal{A}$. The mode-n unfolding, or matricization, of $\\mathcal{A}$, denoted $A_{(n)}$, is a matrix of size $I_n \\times (I_1 \\dots I_{n-1} I_{n+1} \\dots I_N)$ whose columns are the mode-n fibers of $\\mathcal{A}$. The factor matrix $U^{(n)}$ is formed by the first $R_n$ left singular vectors of $A_{(n)}$, ordered according to their corresponding singular values in descending order.\n\nThe core tensor $\\mathcal{G}$ is then computed by projecting $\\mathcal{A}$ onto the space spanned by the factor matrices:\n$$ \\mathcal{G} = \\mathcal{A} \\times_1 (U^{(1)})^T \\times_2 (U^{(2)})^T \\dots \\times_N (U^{(N)})^T $$\n\nThe Frobenius norm of a tensor $\\mathcal{A}$ is given by $\\|\\mathcal{A}\\|_F = \\sqrt{\\sum_{i_1, \\dots, i_N} |a_{i_1 \\dots i_N}|^2}$.\n\nConsider a third-order tensor $\\mathcal{A} \\in \\mathbb{R}^{2 \\times 3 \\times 2}$, defined by its two frontal slices (matrices obtained by fixing the third index):\n$$ A_1 = \\mathcal{A}(:,:,1) = \\begin{pmatrix} 1 & 0 & 1 \\\\ 0 & 2 & 0 \\end{pmatrix} $$\n$$ A_2 = \\mathcal{A}(:,:,2) = \\begin{pmatrix} 0 & 1 & 0 \\\\ 1 & 0 & 1 \\end{pmatrix} $$\n\nCalculate the Frobenius norm of the core tensor $\\mathcal{G}$ obtained from the Tucker decomposition of $\\mathcal{A}$ with a specified multilinear rank of $(R_1, R_2, R_3) = (1, 2, 1)$.", "solution": "1. Mode-1 unfolding $A_{(1)}\\in\\mathbb R^{2\\times6}$.  Using the ordering $(i_2,i_3)$ lexicographically,\n\n$$\nA_{(1)}=\\begin{pmatrix}\n1&0&1&0&1&0\\\\\n0&2&0&1&0&1\n\\end{pmatrix}.\n$$\n\nThen\n\n$$\nA_{(1)}A_{(1)}^T\n=\\begin{pmatrix}3&0\\\\0&6\\end{pmatrix},\n$$\n\nso the largest eigenvalue is $6$ with unit eigenvector $\\begin{pmatrix}0\\\\1\\end{pmatrix}$.  Hence \n\n$$\nU^{(1)}=\\begin{pmatrix}0\\\\1\\end{pmatrix},\\;R_1=1.\n$$\n\n\n2. Mode-2 unfolding $A_{(2)}\\in\\mathbb R^{3\\times4}$, ordering $(i_1,i_3)$ lexicographically,\n\n$$\nA_{(2)}=\\begin{pmatrix}1&0&0&1\\\\0&2&1&0\\\\1&0&0&1\\end{pmatrix},\n$$\n\nso\n\n$$\nA_{(2)}A_{(2)}^T=\\begin{pmatrix}2&0&2\\\\0&5&0\\\\2&0&2\\end{pmatrix}.\n$$\n\nIts top two eigenvalues are $5$ and $4$ with orthonormal eigenvectors \n$\\,(0,1,0)^T$ and $\\tfrac1{\\sqrt2}(1,0,1)^T$.  Thus\n\n$$\nU^{(2)}=\\begin{pmatrix}0&\\tfrac1{\\sqrt2}\\\\1&0\\\\0&\\tfrac1{\\sqrt2}\\end{pmatrix},\\;R_2=2.\n$$\n\n\n3. Mode-3 unfolding $A_{(3)}\\in\\mathbb R^{2\\times6}$, ordering $(i_1,i_2)$,\n\n$$\nA_{(3)}=\\begin{pmatrix}1&0&1&0&2&0\\\\0&1&0&1&0&1\\end{pmatrix},\n$$\n\nand\n\n$$\nA_{(3)}A_{(3)}^T=\\begin{pmatrix}6&0\\\\0&3\\end{pmatrix}.\n$$\n\nThe top eigenvalue is $6$ with eigenvector $(1,0)^T$, so\n\n$$\nU^{(3)}=\\begin{pmatrix}1\\\\0\\end{pmatrix},\\;R_3=1.\n$$\n\n\n4. Core tensor\n$\\displaystyle \\mathcal G\n=\\mathcal A\\times_1U^{(1)T}\\times_2U^{(2)T}\\times_3U^{(3)T}$\nhas entries\n\n$$\nG_{1,1,1}=2,\\quad G_{1,2,1}=0,\n$$\n\nso $\\mathcal G\\in\\mathbb R^{1\\times2\\times1}$.\n\n5. Frobenius norm\n\n$$\n\\|\\mathcal G\\|_F=\\sqrt{2^2+0^2} =2.\n$$", "answer": "$$\\boxed{2}$$", "id": "1071392"}, {"introduction": "A powerful way to understand a new concept is to connect it to one that is already familiar. This practice explores the important special case of a tensor where one dimension is of size one, which is effectively a matrix. By analyzing how the HOSVD framework simplifies in this degenerate case [@problem_id:3549431], you will see precisely how it reduces to the standard matrix Singular Value Decomposition (SVD), reinforcing your intuition that HOSVD is a true and consistent generalization.", "problem": "Consider a real $d$-way tensor $\\mathcal{X} \\in \\mathbb{R}^{n_1 \\times \\cdots \\times n_d}$. The Higher-Order Singular Value Decomposition (HOSVD) of $\\mathcal{X}$ is defined by computing, for each mode $n \\in \\{1,\\ldots,d\\}$, the mode-$n$ matricization $X_{(n)}$ and the left singular vectors of $X_{(n)}$, which form an orthonormal factor matrix $U^{(n)}$. The core tensor $\\mathcal{S}$ is then obtained by successive mode-$n$ products with the transposes $U^{(n)\\top}$. The truncated HOSVD of target multilinear rank $(r_1,\\ldots,r_d)$ projects $\\mathcal{X}$ onto the leading $r_n$-dimensional subspaces in each mode and reconstructs a Tucker tensor with factor matrices $U^{(n)}_{r_n}$ and a reduced core.\n\nIn this problem, analyze the degenerate case in which one mode has size $1$, namely a $3$-way tensor $\\mathcal{X} \\in \\mathbb{R}^{n_1 \\times n_2 \\times n_3}$ with $n_3=1$, so that $\\mathcal{X}$ can be identified with a matrix $X \\in \\mathbb{R}^{n_1 \\times n_2}$. Starting from the definitions above and standard properties of the Singular Value Decomposition (SVD) of a matrix, determine which of the following statements are true. Select all that apply.\n\nA. With $n_3=1$ and $r_1=r_2=r$, the truncated Higher-Order Singular Value Decomposition reconstruction coincides with the rank-$r$ truncated Singular Value Decomposition of the matrix $X$.\n\nB. In the degenerate case $n_3=1$, the Higher-Order Singular Value Decomposition core $\\mathcal{S}$, identified with its unique frontal slice, is the diagonal matrix of singular values of $X$.\n\nC. For $n_3=1$, the standard truncated Higher-Order Singular Value Decomposition Frobenius-norm error bound $\\|\\mathcal{X}-\\tilde{\\mathcal{X}}\\|_F \\le \\sqrt{\\sum_{n=1}^3 \\sum_{i>r_n} \\sigma_i(X_{(n)})^2}$ specializes to the exact Eckart–Young–Mirsky error when $r_1=r_2$.\n\nD. If $n_3=1$ and arbitrary $r_1, r_2$, the truncated Higher-Order Singular Value Decomposition reconstruction equals the rank-$\\min\\{r_1,r_2\\}$ truncated Singular Value Decomposition of $X$.\n\nE. In the matrix case ($n_3=1$), the general quasi-optimality factor $\\sqrt{d}$ in the inequality $\\|\\mathcal{X}-\\tilde{\\mathcal{X}}\\|_F \\le \\sqrt{d}\\,\\|\\mathcal{X}-\\mathcal{X}^\\star\\|_F$ remains necessary; one cannot improve it to $1$.", "solution": "The user wants a detailed analysis of the relationship between the Higher-Order Singular Value Decomposition (HOSVD) and the standard matrix Singular Value Decomposition (SVD) in a degenerate case.\n\n### Problem Statement Validation\n\nThe problem provides a standard definition of HOSVD for a $d$-way tensor $\\mathcal{X} \\in \\mathbb{R}^{n_1 \\times \\cdots \\times n_d}$. It then considers the specific case of a $3$-way tensor where one dimension is trivial, i.e., $\\mathcal{X} \\in \\mathbb{R}^{n_1 \\times n_2 \\times n_3}$ with $n_3=1$. This tensor can be naturally identified with a matrix $X \\in \\mathbb{R}^{n_1 \\times n_2}$. The task is to analyze five statements about HOSVD in this specific context, using its definition and the properties of the matrix SVD.\n\n1.  **Givens Extraction**:\n    -   Tensor $\\mathcal{X} \\in \\mathbb{R}^{n_1 \\times \\cdots \\times n_d}$.\n    -   HOSVD involves factor matrices $U^{(n)}$ (left singular vectors of the mode-$n$ matricization $X_{(n)}$) and a core tensor $\\mathcal{S} = \\mathcal{X} \\times_1 U^{(1)\\top} \\cdots \\times_d U^{(d)\\top}$.\n    -   Truncated HOSVD for multilinear rank $(r_1, \\ldots, r_d)$ involves truncated factor matrices $U^{(n)}_{r_n}$ and a reduced core.\n    -   The specific case is a $3$-way tensor with $n_3=1$, identified with a matrix $X \\in \\mathbb{R}^{n_1 \\times n_2}$.\n\n2.  **Validation**:\n    -   **Scientific Grounding**: The problem is rooted in numerical linear algebra, specifically tensor decompositions, a well-established mathematical field. The definitions of HOSVD and SVD are standard. The problem is scientifically sound.\n    -   **Well-Posedness**: The problem is well-posed. It asks to verify specific mathematical statements based on given definitions in a clearly defined scenario. A definite answer exists for each statement.\n    -   **Objectivity**: The problem is stated in precise, objective mathematical language.\n    -   **Completeness**: The provided definitions are sufficient to perform the required analysis.\n\n3.  **Verdict**: The problem is valid. We proceed to the solution.\n\n### Derivation of HOSVD for the Matrix Case\n\nLet $\\mathcal{X} \\in \\mathbb{R}^{n_1 \\times n_2 \\times 1}$ be the tensor, which we identify with the matrix $X \\in \\mathbb{R}^{n_1 \\times n_2}$ where $X_{ij} = \\mathcal{X}_{ij1}$. Let the Singular Value Decomposition (SVD) of $X$ be $X = U \\Sigma V^\\top$, where $U \\in \\mathbb{R}^{n_1 \\times n_1}$ and $V \\in \\mathbb{R}^{n_2 \\times n_2}$ are orthogonal matrices, and $\\Sigma \\in \\mathbb{R}^{n_1 \\times n_2}$ is the rectangular diagonal matrix of singular values $\\sigma_i$.\n\nThe HOSVD requires computing the mode-$n$ matricizations $X_{(n)}$ and their left singular vectors.\n\n-   **Mode 1**: The mode-$1$ matricization $X_{(1)}$ arranges the tensor elements such that the first index $i_1$ determines the row. For $\\mathcal{X} \\in \\mathbb{R}^{n_1 \\times n_2 \\times 1}$, the unfolding results in $X_{(1)} = X \\in \\mathbb{R}^{n_1 \\times n_2}$. The left singular vectors of $X_{(1)}=X$ are the columns of $U$. Thus, the mode-$1$ factor matrix is $U^{(1)} = U$.\n\n-   **Mode 2**: The mode-$2$ matricization $X_{(2)}$ arranges the tensor elements such that the second index $i_2$ determines the row. This results in the matrix transpose: $X_{(2)} = X^\\top \\in \\mathbb{R}^{n_2 \\times n_1}$. The SVD of $X^\\top$ is $(U \\Sigma V^\\top)^\\top = V \\Sigma^\\top U^\\top$. The left singular vectors of $X_{(2)}=X^\\top$ are the columns of $V$. Thus, the mode-$2$ factor matrix is $U^{(2)} = V$.\n\n-   **Mode 3**: The mode-$3$ matricization $X_{(3)}$ arranges the tensor elements such that the third index $i_3$ determines the row. Since $n_3=1$, $X_{(3)}$ is a row vector in $\\mathbb{R}^{1 \\times (n_1 n_2)}$, specifically $X_{(3)} = \\text{vec}(X)^\\top$. The factor matrix $U^{(3)}$ must be orthogonal, and being of size $1 \\times 1$, it must be $U^{(3)} = [1]$ or $U^{(3)} = [-1]$. We can choose $U^{(3)} = [1]$.\n\nThe factor matrices for the HOSVD of $\\mathcal{X}$ are $U^{(1)}=U$, $U^{(2)}=V$, and $U^{(3)}=[1]$.\n\n### Evaluation of Options\n\n**A. With $n_3=1$ and $r_1=r_2=r$, the truncated Higher-Order Singular Value Decomposition reconstruction coincides with the rank-$r$ truncated Singular Value Decomposition of the matrix $X$.**\n\nThe truncated HOSVD reconstruction with multilinear rank $(r_1, r_2, r_3)$ is given by $\\tilde{\\mathcal{X}} = \\tilde{\\mathcal{S}} \\times_1 U^{(1)}_{r_1} \\times_2 U^{(2)}_{r_2} \\times_3 U^{(3)}_{r_3}$, where $\\tilde{\\mathcal{S}} = \\mathcal{X} \\times_1 U^{(1)\\top}_{r_1} \\times_2 U^{(2)\\top}_{r_2} \\times_3 U^{(3)\\top}_{r_3}$ is the reduced core. Due to $n_3=1$, we must have $r_3=1$.\nThe reconstructed matrix $\\tilde{X}$ is given by $\\tilde{X} = U^{(1)}_{r_1} \\tilde{S} (U^{(2)}_{r_2})^\\top$, where $\\tilde{S}$ is the matrix slice of the core $\\tilde{\\mathcal{S}}$.\n$\\tilde{S} = (U^{(1)}_{r_1})^\\top X U^{(2)}_{r_2} = U_{r_1}^\\top X V_{r_2}$.\nSo $\\tilde{X} = U_{r_1} (U_{r_1}^\\top X V_{r_2}) V_{r_2}^\\top = (U_{r_1} U_{r_1}^\\top) X (V_{r_2} V_{r_2}^\\top)$.\nSubstituting $X = \\sum_{i} \\sigma_i u_i v_i^\\top$ and using the orthogonality of $\\{u_i\\}$ and $\\{v_i\\}$, we get:\n$\\tilde{X} = (\\sum_{j=1}^{r_1} u_j u_j^\\top) (\\sum_{i} \\sigma_i u_i v_i^\\top) (\\sum_{k=1}^{r_2} v_k v_k^\\top) = \\sum_{i=1}^{\\min(r_1, r_2)} \\sigma_i u_i v_i^\\top$.\nIf $r_1=r_2=r$, then $\\min(r_1, r_2)=r$. The reconstruction is $\\tilde{X} = \\sum_{i=1}^{r} \\sigma_i u_i v_i^\\top$, which is precisely the rank-$r$ truncated SVD of $X$.\n**Verdict: Correct.**\n\n**B. In the degenerate case $n_3=1$, the Higher-Order Singular Value Decomposition core $\\mathcal{S}$, identified with its unique frontal slice, is the diagonal matrix of singular values of $X$.**\n\nThe full core tensor is $\\mathcal{S} = \\mathcal{X} \\times_1 U^{(1)\\top} \\times_2 U^{(2)\\top} \\times_3 U^{(3)\\top}$.\nWith $U^{(1)}=U$, $U^{(2)}=V$, and $U^{(3)}=[1]$, the operation $\\times_3 U^{(3)\\top}$ is multiplication by $1$ and has no effect.\nThe matrix slice $S$ of $\\mathcal{S}$ is therefore $S = U^\\top X V$.\nSubstituting $X = U \\Sigma V^\\top$, we get:\n$S = U^\\top (U \\Sigma V^\\top) V = (U^\\top U) \\Sigma (V^\\top V) = I \\Sigma I = \\Sigma$.\nThe matrix $\\Sigma \\in \\mathbb{R}^{n_1 \\times n_2}$ has the singular values of $X$ on its main diagonal and zeros elsewhere. This is conventionally called the \"diagonal matrix of singular values\".\n**Verdict: Correct.**\n\n**C. For $n_3=1$, the standard truncated Higher-Order Singular Value Decomposition Frobenius-norm error bound $\\|\\mathcal{X}-\\tilde{\\mathcal{X}}\\|_F \\le \\sqrt{\\sum_{n=1}^3 \\sum_{i>r_n} \\sigma_i(X_{(n)})^2}$ specializes to the exact Eckart–Young–Mirsky error when $r_1=r_2$.**\n\nThe error bound on the squared Frobenius norm is $\\|\\mathcal{X}-\\tilde{\\mathcal{X}}\\|_F^2 \\le \\sum_{n=1}^3 \\sum_{i=r_n+1}^{k_n} \\sigma_i(X_{(n)})^2$, where $k_n$ is the number of singular values of $X_{(n)}$.\nLet $k = \\min(n_1, n_2)$.\n-   For $n=1$: $X_{(1)}=X$, singular values are $\\sigma_i(X)$. Term is $\\sum_{i=r_1+1}^k \\sigma_i(X)^2$.\n-   For $n=2$: $X_{(2)}=X^\\top$, singular values are $\\sigma_i(X)$. Term is $\\sum_{i=r_2+1}^k \\sigma_i(X)^2$.\n-   For $n=3$: $X_{(3)}$ is a rank-$1$ matrix (a row vector). It has only one non-zero singular value. Since $n_3=1$, we must choose $r_3=1$. The sum $\\sum_{i>1} \\sigma_i(X_{(3)})^2$ is an empty sum, which is $0$.\nIf we set $r_1=r_2=r$, the bound becomes $\\|\\mathcal{X}-\\tilde{\\mathcal{X}}\\|_F^2 \\le \\sum_{i=r+1}^k \\sigma_i(X)^2 + \\sum_{i=r+1}^k \\sigma_i(X)^2 = 2 \\sum_{i=r+1}^k \\sigma_i(X)^2$.\nFrom option A, for $r_1=r_2=r$, the HOSVD reconstruction is the rank-$r$ truncated SVD. The Eckart-Young-Mirsky theorem states the exact squared error for this best rank-$r$ approximation is $\\|X - \\tilde{X}\\|_F^2 = \\sum_{i=r+1}^k \\sigma_i(X)^2$.\nThe HOSVD error bound is $2 \\sum_{i=r+1}^k \\sigma_i(X)^2$, which is twice the actual squared error. Therefore, the bound does not specialize to the exact error.\n**Verdict: Incorrect.**\n\n**D. If $n_3=1$ and arbitrary $r_1, r_2$, the truncated Higher-Order Singular Value Decomposition reconstruction equals the rank-$\\min\\{r_1,r_2\\}$ truncated Singular Value Decomposition of $X$.**\n\nAs derived in the analysis for option A, the HOSVD reconstruction for multilinear rank $(r_1, r_2, 1)$ yields the matrix $\\tilde{X} = \\sum_{i=1}^{\\min(r_1, r_2)} \\sigma_i u_i v_i^\\top$. This is precisely the definition of the truncated SVD of $X$ to rank $r = \\min\\{r_1, r_2\\}$. This statement is more general than A, and is correct.\n**Verdict: Correct.**\n\n**E. In the matrix case ($n_3=1$), the general quasi-optimality factor $\\sqrt{d}$ in the inequality $\\|\\mathcal{X}-\\tilde{\\mathcal{X}}\\|_F \\le \\sqrt{d}\\,\\|\\mathcal{X}-\\mathcal{X}^\\star\\|_F$ remains necessary; one cannot improve it to $1$.**\n\nThis inequality relates the error of the HOSVD approximation $\\tilde{\\mathcal{X}}$ to the error of the best rank-$(r_1, \\dots, r_d)$ approximation $\\mathcal{X}^\\star$. For a $3$-way tensor, $d=3$.\nIn our case ($n_3=1$), we have established in the analysis of A and D that the truncated HOSVD with multilinear rank $(r_1, r_2, 1)$ produces the rank-$r$ truncated SVD, where $r = \\min\\{r_1, r_2\\}$.\nThe best approximation $\\mathcal{X}^\\star$ of multilinear rank $(r_1, r_2, 1)$ corresponds to the best matrix approximation $X^\\star$ whose rank is at most $\\min(r_1, r_2)$. By the Eckart-Young-Mirsky theorem, this best approximation is precisely the rank-$r$ truncated SVD.\nThus, in the matrix case, the HOSVD approximation is identical to the best approximation: $\\tilde{\\mathcal{X}} = \\mathcal{X}^\\star$.\nThis implies $\\|\\mathcal{X}-\\tilde{\\mathcal{X}}\\|_F = \\|\\mathcal{X}-\\mathcal{X}^\\star\\|_F$. The inequality holds with a factor of $1$. The factor $\\sqrt{d}$ is not necessary and can be improved to $1$. The statement claims it cannot be improved to $1$.\n**Verdict: Incorrect.**", "answer": "$$\\boxed{ABD}$$", "id": "3549431"}, {"introduction": "Moving from theory to application, a critical task when using HOSVD for data compression or feature extraction is choosing the multilinear rank. This hands-on coding exercise [@problem_id:3549407] delves into this subtle art by having you compare a simple per-mode rank selection heuristic against a more robust global strategy. You will construct a specific tensor that reveals a common pitfall, demonstrating why interactions between modes are crucial for building effective low-rank tensor approximations.", "problem": "Consider a real order-$3$ tensor $X \\in \\mathbb{R}^{I \\times J \\times K}$. Let $X_{(n)}$ denote the mode-$n$ unfolding of $X$, where $n \\in \\{1,2,3\\}$, and let the Singular Value Decomposition (SVD) of $X_{(n)}$ be $X_{(n)} = U_{(n)} \\Sigma_{(n)} V_{(n)}^{\\top}$ with singular values $\\{\\sigma_{n,i}\\}_{i \\ge 1}$ sorted in nonincreasing order. The Higher-Order Singular Value Decomposition (HOSVD) of $X$ is defined by orthonormal factor matrices $U_1 \\in \\mathbb{R}^{I \\times I}$, $U_2 \\in \\mathbb{R}^{J \\times J}$, $U_3 \\in \\mathbb{R}^{K \\times K}$ obtained as the left singular vectors of $X_{(1)}$, $X_{(2)}$, and $X_{(3)}$, respectively, and a core tensor $G \\in \\mathbb{R}^{I \\times J \\times K}$ given by $G = X \\times_1 U_1^{\\top} \\times_2 U_2^{\\top} \\times_3 U_3^{\\top}$, where $\\times_n$ is the mode-$n$ tensor-matrix product. A truncated HOSVD with multilinear ranks $(r_1,r_2,r_3)$ uses the first $r_n$ columns of $U_n$ and the corresponding subtensor of $G$ to form an approximation $\\widehat{X}^{(r_1,r_2,r_3)}$.\n\nYou are asked to implement and compare two rank selection strategies:\n\n- Per-mode scree selection (independent truncations): for a given threshold $\\alpha \\in (0,1)$, choose the smallest $r_n$ such that $\\sum_{i=1}^{r_n} \\sigma_{n,i}^2 \\ge \\alpha \\sum_{i \\ge 1} \\sigma_{n,i}^2$, independently for each mode $n \\in \\{1,2,3\\}$.\n- Joint energy targeting (global selection): for a given target $\\beta \\in (0,1)$ and an upper bound tuple $(r_{1,\\max}, r_{2,\\max}, r_{3,\\max})$ with $1 \\le r_n \\le r_{n,\\max}$, search over all feasible triples $(r_1,r_2,r_3)$ in lexicographic order to find the lexicographically smallest triple such that the relative Frobenius energy captured satisfies $\\| \\widehat{X}^{(r_1,r_2,r_3)} \\|_F^2 / \\| X \\|_F^2 \\ge \\beta$. If no such triple exists within the bounds, choose the triple that maximizes the captured energy.\n\nConstruct tensors that exhibit jointly appearing low-energy modes as follows. For each test case, build $X$ as a sum of rank-$1$ components with orthonormal factors aligned to canonical basis vectors:\n$$\nX \\;=\\; \\sum_{t=1}^{R} \\lambda_t \\, u_t \\otimes v_t \\otimes w_t,\n$$\nwhere $\\{u_t\\}_{t=1}^R \\subset \\mathbb{R}^I$, $\\{v_t\\}_{t=1}^R \\subset \\mathbb{R}^J$, and $\\{w_t\\}_{t=1}^R \\subset \\mathbb{R}^K$ are sets of orthonormal vectors that, for each $t$, are chosen as the corresponding standard basis vectors $e_t$ in their respective spaces (for example, $u_t = e_t \\in \\mathbb{R}^I$). Assume all $\\lambda_t \\ge 0$. This construction ensures that each unfolding $X_{(n)}$ has singular values equal to the set $\\{\\lambda_t\\}_{t=1}^R$ (possibly padded with zeros), while any component indexed by $t \\ge 2$ can only be represented if all three mode ranks satisfy $r_1 \\ge t$, $r_2 \\ge t$, and $r_3 \\ge t$. This tests the pitfall of independent per-mode truncations that may drop a jointly necessary direction in any single mode, thereby removing an entire component even if its total energy is significant.\n\nImplement the following tasks:\n\n1. Given $X$, compute the HOSVD factors $U_1$, $U_2$, $U_3$ by SVDs of $X_{(1)}$, $X_{(2)}$, $X_{(3)}$ and form truncated HOSVD reconstructions $\\widehat{X}^{(r_1,r_2,r_3)}$ for specified $(r_1,r_2,r_3)$.\n2. Implement per-mode scree selection given $\\alpha$ to produce $(r_1^{\\text{per}}, r_2^{\\text{per}}, r_3^{\\text{per}})$, then compute the relative Frobenius error $e_{\\text{per}} = \\| X - \\widehat{X}^{(r_1^{\\text{per}}, r_2^{\\text{per}}, r_3^{\\text{per}})} \\|_F / \\| X \\|_F$.\n3. Implement joint energy targeting given $\\beta$ and bounds $(r_{1,\\max}, r_{2,\\max}, r_{3,\\max})$ to produce $(r_1^{\\text{joint}}, r_2^{\\text{joint}}, r_3^{\\text{joint}})$ and the relative Frobenius error $e_{\\text{joint}} = \\| X - \\widehat{X}^{(r_1^{\\text{joint}}, r_2^{\\text{joint}}, r_3^{\\text{joint}})} \\|_F / \\| X \\|_F$.\n4. For each test case, also report the products $d_{\\text{per}} = r_1^{\\text{per}} r_2^{\\text{per}} r_3^{\\text{per}}$ and $d_{\\text{joint}} = r_1^{\\text{joint}} r_2^{\\text{joint}} r_3^{\\text{joint}}$ to quantify model size.\n\nYour program must implement the above and run the following test suite, constructing $X$ exactly as specified with canonical basis vectors:\n\n- Test case $1$ (jointly necessary second mode, independent truncations fail): $I = 8$, $J = 8$, $K = 8$, $R = 2$, $(\\lambda_1,\\lambda_2) = (1.0, 0.6)$, $\\alpha = 0.85$, $\\beta = 0.95$, $(r_{1,\\max}, r_{2,\\max}, r_{3,\\max}) = (2,2,2)$.\n- Test case $2$ (happy path where independent truncations suffice): $I = 8$, $J = 8$, $K = 8$, $R = 2$, $(\\lambda_1,\\lambda_2) = (1.0, 0.4)$, $\\alpha = 0.6$, $\\beta = 0.8$, $(r_{1,\\max}, r_{2,\\max}, r_{3,\\max}) = (2,2,2)$.\n- Test case $3$ (boundary case with no hidden mode): $I = 8$, $J = 8$, $K = 8$, $R = 2$, $(\\lambda_1,\\lambda_2) = (1.0, 0.0)$, $\\alpha = 0.99$, $\\beta = 0.99$, $(r_{1,\\max}, r_{2,\\max}, r_{3,\\max}) = (2,2,2)$.\n- Test case $4$ (multiple jointly necessary low-energy modes): $I = 6$, $J = 6$, $K = 6$, $R = 3$, $(\\lambda_1,\\lambda_2,\\lambda_3) = (1.0, 0.5, 0.5)$, $\\alpha = 0.66$, $\\beta = 0.9$, $(r_{1,\\max}, r_{2,\\max}, r_{3,\\max}) = (3,3,3)$.\n\nFinal output format. Your program must produce a single line containing a list of numbers that aggregates the results of the four test cases, in the fixed order\n$$\n\\big[ e_{\\text{per}}^{(1)},\\; e_{\\text{joint}}^{(1)},\\; d_{\\text{per}}^{(1)},\\; d_{\\text{joint}}^{(1)},\\; e_{\\text{per}}^{(2)},\\; e_{\\text{joint}}^{(2)},\\; d_{\\text{per}}^{(2)},\\; d_{\\text{joint}}^{(2)},\\; e_{\\text{per}}^{(3)},\\; e_{\\text{joint}}^{(3)},\\; d_{\\text{per}}^{(3)},\\; d_{\\text{joint}}^{(3)},\\; e_{\\text{per}}^{(4)},\\; e_{\\text{joint}}^{(4)},\\; d_{\\text{per}}^{(4)},\\; d_{\\text{joint}}^{(4)} \\big],\n$$\nwhere each error $e_{\\text{per}}^{(i)}$ and $e_{\\text{joint}}^{(i)}$ is rounded to $6$ decimal places and each $d_{\\text{per}}^{(i)}$, $d_{\\text{joint}}^{(i)}$ is an integer.\n\nYour implementation must be self-contained and not require any user input. No physical units, angle units, or percentages are needed beyond the values specified, and all numerical answers are unitless real numbers. The program must rely only on standard linear algebra operations defined above without any plotting.", "solution": "The problem requires the implementation and comparison of two distinct strategies for selecting the multilinear rank in a Higher-Order Singular Value Decomposition (HOSVD) of a third-order tensor. The goal is to highlight a scenario where a common heuristic, independent per-mode rank selection, fails to preserve significant structural information that is otherwise captured by a more holistic, joint-rank selection method.\n\nWe begin by formalizing the necessary concepts from tensor algebra. An order-$3$ tensor is an element of a tensor product of vector spaces, which we represent as a three-dimensional array $X \\in \\mathbb{R}^{I \\times J \\times K}$.\n\nThe mode-$n$ unfolding, or matricization, of $X$, denoted $X_{(n)}$, is the process of re-arranging the tensor elements into a matrix. For $n=1, 2, 3$, the unfoldings are:\n-   $X_{(1)} \\in \\mathbb{R}^{I \\times JK}$, where the element at $(i, (j-1)K+k)$ is $X_{ijk}$.\n-   $X_{(2)} \\in \\mathbb{R}^{J \\times IK}$, where the element at $(j, (k-1)I+i)$ is $X_{ijk}$.\n-   $X_{(3)} \\in \\mathbb{R}^{K \\times IJ}$, where the element at $(k, (i-1)J+j)$ is $X_{ijk}$.\n\nThe mode-$n$ product of a tensor $X \\in \\mathbb{R}^{I_1 \\times \\dots \\times I_N}$ with a matrix $A \\in \\mathbb{R}^{J_n \\times I_n}$, denoted $Y = X \\times_n A$, results in a tensor $Y \\in \\mathbb{R}^{I_1 \\times \\dots \\times J_n \\times \\dots \\times I_N}$. Its elements are given by $Y_{i_1 \\dots j_n \\dots i_N} = \\sum_{k=1}^{I_n} X_{i_1 \\dots k \\dots i_N} A_{j_n k}$.\n\nThe Higher-Order Singular Value Decomposition (HOSVD) of $X$ is a decomposition of the form $X = G \\times_1 U_1 \\times_2 U_2 \\times_3 U_3$, where:\n1.  $U_1 \\in \\mathbb{R}^{I \\times I}$, $U_2 \\in \\mathbb{R}^{J \\times J}$, and $U_3 \\in \\mathbb{R}^{K \\times K}$ are orthonormal factor matrices. The columns of $U_n$ are the left singular vectors obtained from the Singular Value Decomposition (SVD) of the mode-$n$ unfolding: $X_{(n)} = U_n \\Sigma_{(n)} V_n^\\top$.\n2.  $G \\in \\mathbb{R}^{I \\times J \\times K}$ is the core tensor, which is computed as $G = X \\times_1 U_1^\\top \\times_2 U_2^\\top \\times_3 U_3^\\top$. The core tensor $G$ captures the interaction between the factor matrices' components. The Frobenius norms of $X$ and $G$ are equal, i.e., $\\|X\\|_F = \\|G\\|_F$, due to the orthonormality of the factor matrices.\n\nA truncated HOSVD provides a low-rank approximation of $X$. Given a multilinear rank $(r_1, r_2, r_3)$ where $1 \\le r_n \\le \\text{dim}_n(X)$, we truncate the factor matrices to their first $r_n$ columns, yielding $\\tilde{U}_n \\in \\mathbb{R}^{\\text{dim}_n(X) \\times r_n}$, and truncate the core tensor to the principal subtensor $\\tilde{G} = G(1:r_1, 1:r_2, 1:r_3)$. The approximation is then $\\widehat{X}^{(r_1,r_2,r_3)} = \\tilde{G} \\times_1 \\tilde{U}_1 \\times_2 \\tilde{U}_2 \\times_3 \\tilde{U}_3$.\nA key property is that the squared Frobenius norm of the approximation is simply the squared Frobenius norm of the truncated core tensor, i.e., $\\|\\widehat{X}^{(r_1,r_2,r_3)}\\|_F^2 = \\|\\tilde{G}\\|_F^2$. The relative approximation error is given by $e = \\|X - \\widehat{X}\\|_F / \\|X\\|_F$. Using the norm preservation property, this can be efficiently calculated as $e = \\sqrt{1 - \\|\\widehat{X}\\|_F^2 / \\|X\\|_F^2}$.\n\nThe problem specifies a particular tensor construction: $X = \\sum_{t=1}^{R} \\lambda_t \\, u_t \\otimes v_t \\otimes w_t$, where $u_t=e_t$, $v_t=e_t$, and $w_t=e_t$ are standard basis vectors and $\\lambda_t \\ge 0$. This results in a diagonal tensor where the only non-zero entries are $X_{t,t,t} = \\lambda_t$ for $t=1, \\dots, R$. For such a tensor, the unfoldings $X_{(n)}$ have orthogonal rows. The SVD of $X_{(n)}$ yields factor matrices $U_n$ that are identity matrices (or permutation matrices if $\\lambda_t$ are not sorted) and singular values $\\{\\lambda_t\\}_{t=1}^R$. Consequently, the core tensor is $G=X$. The squared Frobenius norm of the approximation $\\widehat{X}^{(r_1,r_2,r_3)}$ simplifies to $\\|\\widehat{X}^{(r_1,r_2,r_3)}\\|_F^2 = \\sum_{t=1}^{\\min(r_1,r_2,r_3)} \\lambda_t^2$, assuming the $\\lambda_t$ are sorted non-increasingly. This special structure makes the energy of the approximation depend on the minimum of the three ranks, which is the basis for the test cases.\n\nThe two rank selection strategies are:\n1.  **Per-mode scree selection**: This method treats each mode independently. For a threshold $\\alpha$, it finds the smallest rank $r_n$ for each mode $n$ that captures at least a fraction $\\alpha$ of the energy in that mode's singular values: $\\sum_{i=1}^{r_n} \\sigma_{n,i}^2 \\ge \\alpha \\sum_{i \\ge 1} \\sigma_{n,i}^2$. For the constructed tensor, the squared singular values $\\{\\sigma_{n,i}^2\\}$ are identical for all modes and equal to $\\{\\lambda_t^2\\}$.\n2.  **Joint energy targeting**: This method considers the total energy of the reconstructed tensor. For a target energy fraction $\\beta$, it searches for the lexicographically smallest rank tuple $(r_1, r_2, r_3)$ within given bounds $(r_{1,\\max}, r_{2,\\max}, r_{3,\\max})$ such that $\\|\\widehat{X}^{(r_1,r_2,r_3)}\\|_F^2 / \\|X\\|_F^2 \\ge \\beta$. The search proceeds by iterating through $r_1, r_2, r_3$ in nested loops. For each tuple, the captured energy is efficiently computed from the core tensor and checked against the threshold. If no tuple satisfies the condition, the one maximizing the captured energy is selected.\n\nWe will illustrate the process with Test Case $4$:\n-   Given: $I=J=K=6$, $R=3$, $(\\lambda_1, \\lambda_2, \\lambda_3) = (1.0, 0.5, 0.5)$, $\\alpha=0.66$, $\\beta=0.9$, and $r_{\\max}=(3,3,3)$.\n-   Tensor construction: $X_{111}=1.0, X_{222}=0.5, X_{333}=0.5$ (using $1$-based indexing).\n-   Total energy: $\\|X\\|_F^2 = 1.0^2 + 0.5^2 + 0.5^2 = 1.5$.\n-   The squared singular values for any mode are $(1.0, 0.25, 0.25)$. The total sum of squares is $1.5$.\n\n-   **Per-mode selection ($r^{\\text{per}}$)**: Target energy is $\\alpha \\sum \\sigma^2 = 0.66 \\times 1.5 = 0.99$.\n    -   For $r=1$, cumulative energy is $1.0^2=1.0$. Since $1.0 \\ge 0.99$, the smallest rank for each mode is $r_n=1$.\n    -   This yields $r^{\\text{per}} = (1,1,1)$ and model size $d_{\\text{per}}=1 \\times 1 \\times 1=1$.\n    -   The approximation $\\widehat{X}^{(1,1,1)}$ captures only the first component. Its energy is $\\lambda_1^2=1.0$.\n    -   The relative error is $e_{\\text{per}} = \\sqrt{1 - 1.0/1.5} = \\sqrt{1/3} \\approx 0.577350$.\n\n-   **Joint energy targeting ($r^{\\text{joint}}$)**: Target relative energy is $\\beta=0.9$.\n    -   The search for $(r_1,r_2,r_3)$ proceeds lexicographically from $(1,1,1)$ to $(3,3,3)$.\n    -   The relative captured energy for a tuple is $(\\sum_{t=1}^{\\min(r_1,r_2,r_3)} \\lambda_t^2) / 1.5$.\n    -   For any tuple with $\\min(r_1,r_2,r_3)=1$, rel. energy is $1.0/1.5 \\approx 0.667 < 0.9$.\n    -   For any tuple with $\\min(r_1,r_2,r_3)=2$, rel. energy is $(1.0+0.25)/1.5 \\approx 0.833 < 0.9$.\n    -   For any tuple with $\\min(r_1,r_2,r_3)=3$, rel. energy is $(1.0+0.25+0.25)/1.5 = 1.0 \\ge 0.9$.\n    -   The lexicographically first tuple where this condition holds is $(3,3,3)$.\n    -   This yields $r^{\\text{joint}} = (3,3,3)$ and model size $d_{\\text{joint}}=3 \\times 3 \\times 3=27$.\n    -   The approximation $\\widehat{X}^{(3,3,3)}$ captures all components. Its energy is $1.5$.\n    -   The relative error is $e_{\\text{joint}} = \\sqrt{1 - 1.5/1.5} = 0$.\n\nThis case demonstrates the pitfall: per-mode selection, misled by the dominance of the first singular value in each mode, chooses a rank of $1$, discarding two components and incurring a large error. The joint selection correctly identifies that all three ranks must be at least $3$ to meet the desired overall approximation quality, even though it leads to a much larger model. The program will execute this logic for all specified test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import svd\n\ndef construct_tensor(I, J, K, lambdas):\n    \"\"\"Constructs the special diagonal tensor X.\"\"\"\n    X = np.zeros((I, J, K))\n    R = len(lambdas)\n    for t in range(R):\n        if t  I and t  J and t  K:\n            X[t, t, t] = lambdas[t]\n    return X\n\ndef mode_n_unfolding(X, n):\n    \"\"\"Computes the mode-n unfolding of a tensor X.\"\"\"\n    if n == 1:\n        return np.reshape(X, (X.shape[0], -1))\n    elif n == 2:\n        return np.reshape(np.transpose(X, (1, 0, 2)), (X.shape[1], -1))\n    elif n == 3:\n        return np.reshape(np.transpose(X, (2, 0, 1)), (X.shape[2], -1))\n    else:\n        raise ValueError(\"Mode must be 1, 2, or 3 for an order-3 tensor.\")\n\ndef mode_n_product(X, A, n):\n    \"\"\"Computes the mode-n product of a tensor X with a matrix A.\"\"\"\n    # This implementation is for order-3 tensors\n    if n not in [1, 2, 3]:\n        raise ValueError(\"Mode must be 1, 2, or 3.\")\n    \n    # np.tensordot contracts over specified axes. \n    # For X_abc and A_ij, we want to sum over the n-th mode of X and the second dim of A.\n    # X mode-1 (a) is index 0. X mode-2 (b) is index 1. X mode-3 (c) is index 2.\n    res_tensor = np.tensordot(X, A, axes=([n-1], [1]))\n    \n    # The new dimension from A is at the end. We move it to the correct (n-th) position.\n    # The original axes shift. e.g. for n=1, original axes are (1,2) of X.\n    # result shape is (J, K, I_new), need to move I_new to front.\n    return np.moveaxis(res_tensor, -1, n-1)\n\ndef compute_hosvd(X):\n    \"\"\"Computes the HOSVD of a tensor X.\"\"\"\n    dims = X.shape\n    U_factors = []\n    singular_values = []\n\n    for n in range(1, 4):\n        X_n = mode_n_unfolding(X, n)\n        # We need the full U matrix to compute the core tensor G correctly\n        U, s, _ = svd(X_n, full_matrices=True)\n        # Make sure U has the correct dimensions if X_n is skinny\n        if U.shape[1]  dims[n-1]:\n             U_full = np.zeros((dims[n-1], dims[n-1]))\n             U_full[:, :U.shape[1]] = U\n             U = U_full\n        U_factors.append(U)\n\n        s_full = np.zeros(min(X_n.shape))\n        s_full[:len(s)] = s\n        singular_values.append(s_full)\n    \n    U1, U2, U3 = U_factors\n    G = mode_n_product(X, U1.T, 1)\n    G = mode_n_product(G, U2.T, 2)\n    G = mode_n_product(G, U3.T, 3)\n    \n    return U_factors, singular_values, G\n\ndef per_mode_scree_selection(singular_values, alpha):\n    \"\"\"Performs per-mode scree plot based rank selection.\"\"\"\n    ranks = []\n    for s_n in singular_values:\n        s_n_sq = s_n**2\n        total_energy = np.sum(s_n_sq)\n        if total_energy == 0:\n            ranks.append(1)\n            continue\n        \n        cumulative_energy = np.cumsum(s_n_sq)\n        target_energy = alpha * total_energy\n        \n        # Find first rank r where cumulative energy exceeds target\n        # np.where returns a tuple of arrays, we need the first element of the first array\n        r_n_candidates = np.where(cumulative_energy >= target_energy)[0]\n        \n        if len(r_n_candidates) > 0:\n            r_n = r_n_candidates[0] + 1\n        else: # Should not happen if alpha = 1, but for safety\n             r_n = len(s_n)\n        ranks.append(r_n)\n    return tuple(ranks)\n\ndef joint_energy_targeting(G, total_energy_sq, beta, r_max):\n    \"\"\"Performs joint energy targeting rank selection.\"\"\"\n    r1_max, r2_max, r3_max = r_max\n    \n    best_ranks = (1, 1, 1)\n    max_captured_energy = -1.0\n\n    for r1 in range(1, r1_max + 1):\n        for r2 in range(1, r2_max + 1):\n            for r3 in range(1, r3_max + 1):\n                G_trunc = G[:r1, :r2, :r3]\n                captured_energy_sq = np.sum(G_trunc**2)\n\n                if total_energy_sq > 0 and captured_energy_sq / total_energy_sq >= beta:\n                    return (r1, r2, r3)\n                \n                if captured_energy_sq > max_captured_energy:\n                    max_captured_energy = captured_energy_sq\n                    best_ranks = (r1, r2, r3)\n    \n    # This fallback is executed if no rank combination meets the beta threshold\n    return best_ranks\n\ndef calculate_error_and_size(ranks, G, total_energy_sq):\n    \"\"\"Calculates relative Frobenius error and model size for given ranks.\"\"\"\n    r1, r2, r3 = ranks\n    d = r1 * r2 * r3\n    \n    G_trunc = G[:r1, :r2, :r3]\n    captured_energy_sq = np.sum(G_trunc**2)\n    \n    if total_energy_sq == 0:\n        error = 0.0\n    else:\n        # Avoid numerical issues with captured_energy_sq > total_energy_sq\n        ratio = min(1.0, captured_energy_sq / total_energy_sq)\n        error = np.sqrt(1.0 - ratio)\n\n    return error, d\n\ndef solve():\n    test_cases = [\n        # (I, J, K, lambdas, alpha, beta, (r1_max, r2_max, r3_max))\n        (8, 8, 8, (1.0, 0.6), 0.85, 0.95, (2, 2, 2)),\n        (8, 8, 8, (1.0, 0.4), 0.6, 0.8, (2, 2, 2)),\n        (8, 8, 8, (1.0, 0.0), 0.99, 0.99, (2, 2, 2)),\n        (6, 6, 6, (1.0, 0.5, 0.5), 0.66, 0.9, (3, 3, 3)),\n    ]\n\n    all_results = []\n\n    for I, J, K, lambdas, alpha, beta, r_max in test_cases:\n        # 1. Construct tensor and compute total energy\n        X = construct_tensor(I, J, K, lambdas)\n        total_energy_sq = np.sum(X**2)\n\n        # 2. Compute HOSVD\n        U_factors, singular_values, G = compute_hosvd(X)\n\n        # 3. Per-mode scree selection\n        r_per = per_mode_scree_selection(singular_values, alpha)\n        e_per, d_per = calculate_error_and_size(r_per, G, total_energy_sq)\n\n        # 4. Joint energy targeting\n        r_joint = joint_energy_targeting(G, total_energy_sq, beta, r_max)\n        e_joint, d_joint = calculate_error_and_size(r_joint, G, total_energy_sq)\n        \n        # 5. Append results\n        all_results.append(round(e_per, 6))\n        all_results.append(round(e_joint, 6))\n        all_results.append(d_per)\n        all_results.append(d_joint)\n\n    # Final print statement\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```", "id": "3549407"}]}