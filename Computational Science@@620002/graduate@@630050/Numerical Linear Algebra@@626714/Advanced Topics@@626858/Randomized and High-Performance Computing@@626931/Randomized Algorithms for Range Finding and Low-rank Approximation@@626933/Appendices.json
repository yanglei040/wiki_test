{"hands_on_practices": [{"introduction": "The primary motivation for using randomized algorithms in linear algebra is their remarkable computational efficiency, especially for large-scale matrices. This practice provides a first-principles look at this advantage by guiding you through a detailed floating-point operation (flop) count for a standard randomized SVD pipeline. By deriving the computational cost and comparing it asymptotically to a deterministic algorithm, you will quantify exactly why these methods are indispensable in modern data analysis and scientific computing [@problem_id:3569802].", "problem": "Consider a dense real matrix $A \\in \\mathbb{R}^{m \\times n}$ with $m \\geq n$, a target rank $k$ with $1 \\leq k < n$, oversampling parameter $p \\geq 0$, and power iteration parameter $q \\geq 0$. Define $\\ell = k + p$. A standard randomized singular value decomposition (SVD) pipeline proceeds as follows: draw a Gaussian test matrix $\\Omega \\in \\mathbb{R}^{n \\times \\ell}$, form the sample matrix $Y = (A A^{\\top})^{q} A \\Omega$, compute an orthonormal basis $Q \\in \\mathbb{R}^{m \\times \\ell}$ for the columns of $Y$ via a thin QR factorization, form the compressed matrix $B = Q^{\\top} A \\in \\mathbb{R}^{\\ell \\times n}$, compute a deterministic SVD $B = \\widetilde{U} \\Sigma V^{\\top}$, and set $U = Q \\widetilde{U}$. Assume the following operation cost model for floating-point operation counts:\n- Dense matrix-matrix multiply of sizes $(m \\times n)$ by $(n \\times r)$ costs $2 m n r$ flops.\n- Householder thin QR of an $(m \\times r)$ matrix with $m \\geq r$ costs $2 m r^{2} - \\tfrac{2}{3} r^{3}$ flops.\n- Deterministic SVD of an $(n \\times r)$ matrix with $n \\geq r$ (via bidiagonalization and diagonal SVD) costs $4 n r^{2} + \\tfrac{8}{3} r^{3}$ flops.\n- Dense matrix-vector multiply of sizes $(m \\times n)$ by $(n \\times 1)$ costs $2 m n$ flops.\n\nFor a deterministic truncated SVD that computes the leading $k$ singular triplets by $k$ steps of the Golub–Kahan bidiagonalization with full reorthogonalization, use the standard model\n$$\nF_{\\mathrm{det}}(m,n,k) \\;=\\; 4 m n k \\;+\\; 2 (m + n) k^{2} \\;+\\; \\tfrac{8}{3} k^{3}.\n$$\n\n(a) Using only the foregoing cost model and the algorithmic steps, derive the total operation count $F_{\\mathrm{rand}}(m,n,k,p,q)$ for the randomized SVD pipeline described above, expressed in terms of $m$, $n$, $k$, $p$, and $q$, including the costs for forming $A \\Omega$, the $q$ power iterations, the QR of $Y$, forming $B = Q^{\\top} A$, the small SVD of $B$, and forming $U = Q \\widetilde{U}$.\n\n(b) Consider the asymptotic regime where $m \\to \\infty$ and $n \\to \\infty$ with $m / n \\to \\gamma \\in (1, \\infty)$, while $k$, $p$, and $q$ are fixed and independent of $m$ and $n$. In this regime, compute the limit of the ratio\n$$\nR \\;=\\; \\lim_{\\substack{m,n \\to \\infty \\\\ m/n \\to \\gamma}} \\frac{F_{\\mathrm{rand}}(m,n,k,p,q)}{F_{\\mathrm{det}}(m,n,k)}.\n$$\nProvide your final answer for $R$ as a single simplified analytic expression in terms of $k$, $p$, and $q$. No numerical approximation is required, and no units are involved. The final answer must be a single closed-form expression.", "solution": "The user has provided a valid problem statement from the field of numerical linear algebra. The problem is scientifically grounded, well-posed, objective, and contains all necessary information to derive a unique solution. The cost models and algorithmic description are standard for the analysis of randomized low-rank approximation methods.\n\n### Part (a): Derivation of the Total Operation Count $F_{\\mathrm{rand}}$\n\nThe total operation count, or flop count, $F_{\\mathrm{rand}}(m,n,k,p,q)$, is the sum of the costs of each step in the described randomized SVD pipeline. We will analyze each step using the provided cost models and the parameter $\\ell = k+p$.\n\n1.  **Forming the sample matrix $Y = (A A^{\\top})^{q} A \\Omega$**:\n    This computation is performed iteratively. The test matrix is $\\Omega \\in \\mathbb{R}^{n \\times \\ell}$. The matrix $A$ is in $\\mathbb{R}^{m \\times n}$.\n    -   First, we form an initial sample matrix $Y_0 = A \\Omega$. This is a multiplication of an $(m \\times n)$ matrix by an $(n \\times \\ell)$ matrix. The cost is $2mn\\ell$ flops. The resulting matrix $Y_0$ is of size $(m \\times \\ell)$.\n    -   Next, we apply the power iteration operator $(A A^{\\top})$ a total of $q$ times. Each application on a matrix $Y_{i-1} \\in \\mathbb{R}^{m \\times \\ell}$ to produce $Y_i = A A^{\\top} Y_{i-1}$ is done in two steps:\n        a. Compute the intermediate matrix $Z_{i-1} = A^{\\top} Y_{i-1}$. This is a multiplication of $A^{\\top} \\in \\mathbb{R}^{n \\times m}$ by $Y_{i-1} \\in \\mathbb{R}^{m \\times \\ell}$. The cost is $2nm\\ell = 2mn\\ell$ flops.\n        b. Compute $Y_i = A Z_{i-1}$. This is a multiplication of $A \\in \\mathbb{R}^{m \\times n}$ by $Z_{i-1} \\in \\mathbb{R}^{n \\times \\ell}$. The cost is $2mn\\ell$ flops.\n    -   The cost of one power iteration step is thus $2mn\\ell + 2mn\\ell = 4mn\\ell$ flops. For $q$ iterations, the cost is $q \\times (4mn\\ell) = 4qmn\\ell$ flops.\n    -   The total cost for forming the final sample matrix $Y$ is the sum of the initial step and the $q$ power iterations:\n        $$\n        \\text{Cost}_Y = 2mn\\ell + 4qmn\\ell = (2+4q)mn\\ell\n        $$\n\n2.  **Compute an orthonormal basis $Q$ for $Y$**:\n    The matrix $Y$ is of size $(m \\times \\ell)$. We perform a thin QR factorization of $Y$ to obtain $Q \\in \\mathbb{R}^{m \\times \\ell}$. The cost model for a Householder thin QR of an $(m \\times r)$ matrix (with $m \\geq r$) is $2mr^2 - \\frac{2}{3}r^3$. Here, $r=\\ell$.\n    $$\n    \\text{Cost}_{QR} = 2m\\ell^2 - \\frac{2}{3}\\ell^3\n    $$\n\n3.  **Form the compressed matrix $B = Q^{\\top} A$**:\n    This step involves multiplying the matrix $Q^{\\top} \\in \\mathbb{R}^{\\ell \\times m}$ by the matrix $A \\in \\mathbb{R}^{m \\times n}$.\n    $$\n    \\text{Cost}_B = 2\\ell mn = 2mn\\ell\n    $$\n\n4.  **Compute the SVD of $B = \\widetilde{U} \\Sigma V^{\\top}$**:\n    The matrix $B$ is of size $(\\ell \\times n)$. The cost model provided is for an $(N \\times R)$ matrix where $N \\geq R$. The asymptotic regime specified in part (b) has $m, n \\to \\infty$ while $k, p$ (and thus $\\ell=k+p$) are fixed. In this limit, we will have $n > \\ell$, so $B$ is a \"short-and-fat\" matrix. The SVD of $B$ is typically computed via the SVD of $B^{\\top} \\in \\mathbb{R}^{n \\times \\ell}$, which is a \"tall-and-skinny\" matrix. For $B^{\\top}$, the dimensions match the cost model with $N=n$ and $R=\\ell$.\n    $$\n    \\text{Cost}_{SVD} = 4n\\ell^2 + \\frac{8}{3}\\ell^3\n    $$\n\n5.  **Form the final left singular vectors $U = Q \\widetilde{U}$**:\n    The matrix $Q$ is of size $(m \\times \\ell)$. The matrix of left singular vectors of $B$, $\\widetilde{U}$, is of size $(\\ell \\times \\ell)$. The product $U=Q\\widetilde{U}$ is an $(m \\times \\ell)$ matrix. The cost for this matrix-matrix multiplication is:\n    $$\n    \\text{Cost}_U = 2m\\ell\\ell = 2m\\ell^2\n    $$\n\nThe total operation count $F_{\\mathrm{rand}}$ is the sum of these costs:\n$$\nF_{\\mathrm{rand}} = \\text{Cost}_Y + \\text{Cost}_{QR} + \\text{Cost}_B + \\text{Cost}_{SVD} + \\text{Cost}_U\n$$\n$$\nF_{\\mathrm{rand}} = (2+4q)mn\\ell + \\left(2m\\ell^2 - \\frac{2}{3}\\ell^3\\right) + 2mn\\ell + \\left(4n\\ell^2 + \\frac{8}{3}\\ell^3\\right) + 2m\\ell^2\n$$\nWe group the terms by their dependence on $m$ and $n$:\n$$\nF_{\\mathrm{rand}} = ((2+4q)mn\\ell + 2mn\\ell) + (2m\\ell^2 + 2m\\ell^2) + (4n\\ell^2) + \\left(-\\frac{2}{3}\\ell^3 + \\frac{8}{3}\\ell^3\\right)\n$$\n$$\nF_{\\mathrm{rand}} = (4+4q)mn\\ell + 4m\\ell^2 + 4n\\ell^2 + \\frac{6}{3}\\ell^3\n$$\n$$\nF_{\\mathrm{rand}} = (4+4q)mn\\ell + 4m\\ell^2 + 4n\\ell^2 + 2\\ell^3\n$$\nThis can be written more compactly as:\n$$\nF_{\\mathrm{rand}}(m,n,k,p,q) = (4+4q)mn(k+p) + 4(m+n)(k+p)^2 + 2(k+p)^3\n$$\n\n### Part (b): Asymptotic Ratio of Operation Counts\n\nWe are asked to compute the limit of the ratio $R = F_{\\mathrm{rand}} / F_{\\mathrm{det}}$ in the asymptotic regime where $m, n \\to \\infty$ with $m/n \\to \\gamma \\in (1, \\infty)$, while $k$, $p$, and $q$ are fixed constants.\n\nThe expressions for the two costs are:\n$$\nF_{\\mathrm{rand}}(m,n,k,p,q) = (4+4q)mn(k+p) + 4(m+n)(k+p)^2 + 2(k+p)^3\n$$\n$$\nF_{\\mathrm{det}}(m,n,k) = 4mnk + 2(m+n)k^2 + \\frac{8}{3}k^3\n$$\nTo evaluate the limit of the ratio, we identify the dominant terms in the numerator and the denominator as $m, n \\to \\infty$. Since $k, p, q$ are fixed, the parameters $\\ell=k+p$ and the factors involving them are constant with respect to $m$ and $n$.\n\nThe term with the fastest growth in both expressions is the one proportional to $mn$. Other terms grow as $m$ or $n$, which is asymptotically smaller than $mn$.\n- The leading term of $F_{\\mathrm{rand}}$ is $(4+4q)mn(k+p)$.\n- The leading term of $F_{\\mathrm{det}}$ is $4mnk$.\n\nThe limit of the ratio is the ratio of the coefficients of the leading terms.\nA more formal way is to divide both the numerator and the denominator by $mn$:\n$$\nR = \\lim_{\\substack{m,n \\to \\infty \\\\ m/n \\to \\gamma}} \\frac{(4+4q)mn(k+p) + 4(m+n)(k+p)^2 + 2(k+p)^3}{4mnk + 2(m+n)k^2 + \\frac{8}{3}k^3}\n$$\n$$\nR = \\lim_{m,n \\to \\infty} \\frac{(4+4q)(k+p) + 4\\left(\\frac{1}{n}+\\frac{1}{m}\\right)(k+p)^2 + \\frac{2(k+p)^3}{mn}}{4k + 2\\left(\\frac{1}{n}+\\frac{1}{m}\\right)k^2 + \\frac{8k^3}{3mn}}\n$$\nAs $m \\to \\infty$ and $n \\to \\infty$, all terms containing $1/m$, $1/n$, or $1/mn$ approach zero:\n$$\n\\lim_{m,n \\to \\infty} \\left(\\frac{1}{n}+\\frac{1}{m}\\right) = 0 \\quad \\text{and} \\quad \\lim_{m,n \\to \\infty} \\frac{1}{mn} = 0\n$$\nThus, the limit simplifies to:\n$$\nR = \\frac{(4+4q)(k+p) + 0 + 0}{4k + 0 + 0} = \\frac{(4+4q)(k+p)}{4k}\n$$\nSimplifying the expression, we get:\n$$\nR = \\frac{4(1+q)(k+p)}{4k} = \\frac{(1+q)(k+p)}{k}\n$$\nThis can also be written as $R=(1+q)(1+p/k)$. The result is independent of the aspect ratio limit $\\gamma$, as expected from the problem framing.", "answer": "$$\\boxed{\\frac{(1+q)(k+p)}{k}}$$", "id": "3569802"}, {"introduction": "Understanding how sketching works requires building strong intuition about its core mechanics. This exercise uses a carefully constructed thought experiment to illuminate the fundamental difference between finding a matrix's range (column space) and its co-range (row space). By analyzing a case where an improperly chosen sketch fails completely, you will see precisely why post-multiplication (a \"right sketch\") is used for range finding, while pre-multiplication (a \"left sketch\") is necessary for co-range finding [@problem_id:3569859].", "problem": "Consider a wide matrix $A \\in \\mathbb{R}^{m \\times n}$ with $m=3$, $n=5$, and exact rank $k=2$. Let $A$ admit a singular value decomposition $A = U_k \\Sigma_k V_k^{\\top}$ where $U_k \\in \\mathbb{R}^{3 \\times 2}$ has orthonormal columns, $\\Sigma_k = \\operatorname{diag}(\\sigma_1,\\sigma_2)$ with $\\sigma_1 > \\sigma_2 > 0$, and $V_k \\in \\mathbb{R}^{5 \\times 2}$ has columns equal to the standard basis vectors $e_4$ and $e_5$ in $\\mathbb{R}^5$; that is, $V_k = [e_4 \\;\\; e_5]$. Hence, $A$ has nonzero columns only in positions $4$ and $5$.\n\nTwo projection-based low-rank approximations are constructed using sketching:\n\n- Right sketch for range finding: Let $\\Omega_R \\in \\mathbb{R}^{5 \\times 2}$ be the matrix of the first two standard basis vectors, $\\Omega_R = [e_1 \\;\\; e_2]$. Form $Y_R = A \\Omega_R$, compute an orthonormal basis $Q_R$ for $\\operatorname{range}(Y_R)$, and define the approximation $A_R = Q_R Q_R^{\\top} A$.\n\n- Left sketch for co-range (row space) finding: Let $\\Phi_L \\in \\mathbb{R}^{3 \\times 2}$ be $\\Phi_L = U_k$. Form $W_L = A^{\\top} \\Phi_L$, compute an orthonormal basis $P_L$ for $\\operatorname{range}(W_L)$, and define the approximation $A_L = A P_L P_L^{\\top}$.\n\nUsing only fundamental definitions from numerical linear algebra (singular value decomposition, orthonormal bases, and orthogonal projectors), compute:\n\n1. The spectral-norm error $E_{\\text{right}} = \\|A - A_R\\|_2$.\n2. The spectral-norm error $E_{\\text{left}} = \\|A - A_L\\|_2$.\n\nThen, relying on these computations and the definitions of range and co-range, justify which side of $A$ should be sketched to find a basis for the range versus the co-range.\n\nProvide your final answer as the single simplified expression for $E_{\\text{right}} - E_{\\text{left}}$. No numerical rounding is required, and no physical units are involved. Express the final answer in closed form.", "solution": "The user has provided a problem statement that requires validation before a solution is attempted.\n\n### Step 1: Extract Givens\n- A matrix $A \\in \\mathbb{R}^{m \\times n}$ with $m=3$ and $n=5$.\n- The exact rank of $A$ is $k=2$.\n- The singular value decomposition (SVD) of $A$ is given by $A = U_k \\Sigma_k V_k^{\\top}$.\n- $U_k \\in \\mathbb{R}^{3 \\times 2}$ has orthonormal columns.\n- $\\Sigma_k = \\operatorname{diag}(\\sigma_1, \\sigma_2)$ with singular values $\\sigma_1 > \\sigma_2 > 0$.\n- $V_k \\in \\mathbb{R}^{5 \\times 2}$ has columns equal to the standard basis vectors $e_4$ and $e_5$ in $\\mathbb{R}^5$; thus, $V_k = [e_4 \\;\\; e_5]$.\n- This implies the first three columns of $A$ are zero vectors.\n\n- The first approximation method is a **right sketch for range finding**:\n  - The sketching matrix is $\\Omega_R = [e_1 \\;\\; e_2] \\in \\mathbb{R}^{5 \\times 2}$.\n  - The sketched matrix is $Y_R = A \\Omega_R$.\n  - $Q_R$ is an orthonormal basis for the range of $Y_R$, $\\operatorname{range}(Y_R)$.\n  - The approximation is $A_R = Q_R Q_R^{\\top} A$.\n  - The associated error is $E_{\\text{right}} = \\|A - A_R\\|_2$.\n\n- The second approximation method is a **left sketch for co-range (row space) finding**:\n  - The sketching matrix is $\\Phi_L = U_k \\in \\mathbb{R}^{3 \\times 2}$.\n  - The sketched matrix is $W_L = A^{\\top} \\Phi_L$.\n  - $P_L$ is an orthonormal basis for the range of $W_L$, $\\operatorname{range}(W_L)$.\n  - The approximation is $A_L = A P_L P_L^{\\top}$.\n  - The associated error is $E_{\\text{left}} = \\|A - A_L\\|_2$.\n\nThe task is to compute $E_{\\text{right}}$ and $E_{\\text{left}}$, justify the appropriate use of left and right sketching, and provide the final value for $E_{\\text{right}} - E_{\\text{left}}$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is set in the context of numerical linear algebra, specifically concerning randomized algorithms for low-rank approximation. All definitions and concepts, such as singular value decomposition, orthonormal bases, orthogonal projectors, and spectral norm, are standard and mathematically rigorous.\n\n- **Scientific Grounding**: The problem is scientifically and mathematically sound. It explores a fundamental concept in randomized numerical linear algebra: the relationship between sketching, range finding, and co-range finding.\n- **Well-Posedness**: The problem is well-posed. The given information is sufficient and self-consistent. The structure of $V_k$ implies that the row space of $A$ is spanned by $\\{e_4, e_5\\}$, meaning columns $1$, $2$, and $3$ of $A$ are zero. This is consistent with the problem statement. The choices for $\\Omega_R$ and $\\Phi_L$ are specific, allowing for a deterministic and unique calculation of the errors.\n- **Objectivity**: The problem is stated in precise, objective mathematical language, free of ambiguity or subjective claims.\n\nThe problem does not violate any of the invalidity criteria. It is a well-defined exercise designed to test the understanding of projection-based approximation methods.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A solution will be constructed.\n\n### Solution Derivation\n\n**1. Calculation of the Right-Sketch Error, $E_{\\text{right}}$**\n\nThe first step is to compute the sketched matrix $Y_R = A \\Omega_R$. We substitute the SVD of $A$ and the definition of $\\Omega_R$:\n$$Y_R = (U_k \\Sigma_k V_k^{\\top}) \\Omega_R = U_k \\Sigma_k V_k^{\\top} [e_1 \\;\\; e_2]$$\nWe are given $V_k = [e_4 \\;\\; e_5]$, so its transpose is $V_k^{\\top} = \\begin{pmatrix} e_4^{\\top} \\\\ e_5^{\\top} \\end{pmatrix}$. We compute the product $V_k^{\\top} \\Omega_R$:\n$$V_k^{\\top} \\Omega_R = \\begin{pmatrix} e_4^{\\top} \\\\ e_5^{\\top} \\end{pmatrix} [e_1 \\;\\; e_2] = \\begin{pmatrix} e_4^{\\top} e_1 & e_4^{\\top} e_2 \\\\ e_5^{\\top} e_1 & e_5^{\\top} e_2 \\end{pmatrix}$$\nSince the standard basis vectors are orthogonal, $e_i^{\\top} e_j = 0$ for $i \\neq j$. All entries in the resulting matrix are zero:\n$$V_k^{\\top} \\Omega_R = \\begin{pmatrix} 0 & 0 \\\\ 0 & 0 \\end{pmatrix} = \\mathbf{0}_{2 \\times 2}$$\nTherefore, the sketched matrix $Y_R$ is the zero matrix:\n$$Y_R = U_k \\Sigma_k \\mathbf{0}_{2 \\times 2} = \\mathbf{0}_{3 \\times 2}$$\nThe range of the zero matrix is the trivial vector space $\\{\\mathbf{0}\\}$. The orthogonal projector onto this space is the zero matrix, so $Q_R Q_R^{\\top} = \\mathbf{0}_{3 \\times 3}$. The resulting approximation $A_R$ is also the zero matrix:\n$$A_R = (\\mathbf{0}_{3 \\times 3}) A = \\mathbf{0}_{3 \\times 5}$$\nThe approximation error is the spectral norm of the difference between $A$ and $A_R$:\n$$E_{\\text{right}} = \\|A - A_R\\|_2 = \\|A - \\mathbf{0}\\|_2 = \\|A\\|_2$$\nThe spectral norm of a matrix is its largest singular value. For matrix $A$, this is $\\sigma_1$.\n$$E_{\\text{right}} = \\sigma_1$$\n\n**2. Calculation of the Left-Sketch Error, $E_{\\text{left}}$**\n\nThe second step is to compute the sketched matrix $W_L = A^{\\top} \\Phi_L$. We substitute the transpose of the SVD of $A$ and the definition of $\\Phi_L$:\n$$A^{\\top} = (U_k \\Sigma_k V_k^{\\top})^{\\top} = V_k \\Sigma_k^{\\top} U_k^{\\top} = V_k \\Sigma_k U_k^{\\top}$$\nThe sketching matrix is $\\Phi_L = U_k$. Thus, $W_L$ is:\n$$W_L = (V_k \\Sigma_k U_k^{\\top}) U_k = V_k \\Sigma_k (U_k^{\\top} U_k)$$\nSince the columns of $U_k$ are orthonormal, $U_k^{\\top} U_k = I_2$, the $2 \\times 2$ identity matrix.\n$$W_L = V_k \\Sigma_k I_2 = V_k \\Sigma_k$$\nGiven $V_k = [e_4 \\;\\; e_5]$ and $\\Sigma_k = \\operatorname{diag}(\\sigma_1, \\sigma_2)$:\n$$W_L = [e_4 \\;\\; e_5] \\begin{pmatrix} \\sigma_1 & 0 \\\\ 0 & \\sigma_2 \\end{pmatrix} = [\\sigma_1 e_4 \\;\\; \\sigma_2 e_5]$$\nThe range of $W_L$ is $\\operatorname{span}\\{\\sigma_1 e_4, \\sigma_2 e_5\\}$. Since $\\sigma_1, \\sigma_2 > 0$, this is equivalent to $\\operatorname{span}\\{e_4, e_5\\}$. The vectors $e_4$ and $e_5$ form an orthonormal set. Thus, an orthonormal basis $P_L$ for $\\operatorname{range}(W_L)$ is simply $P_L = [e_4 \\;\\; e_5] = V_k$.\n\nThe orthogonal projector onto the range of $W_L$ is $P_L P_L^{\\top} = V_k V_k^{\\top}$. The approximation $A_L$ is:\n$$A_L = A P_L P_L^{\\top} = A V_k V_k^{\\top}$$\nThe approximation error is the spectral norm of $A - A_L$:\n$$E_{\\text{left}} = \\|A - A_L\\|_2 = \\|A - A V_k V_k^{\\top}\\|_2 = \\|A(I_5 - V_k V_k^{\\top})\\|_2$$\nSubstitute $A = U_k \\Sigma_k V_k^{\\top}$:\n$$A(I_5 - V_k V_k^{\\top}) = U_k \\Sigma_k V_k^{\\top} (I_5 - V_k V_k^{\\top}) = U_k \\Sigma_k (V_k^{\\top} - V_k^{\\top} V_k V_k^{\\top})$$\nSince the columns of $V_k$ are orthonormal, $V_k^{\\top} V_k = I_2$. The expression simplifies to:\n$$U_k \\Sigma_k (V_k^{\\top} - I_2 V_k^{\\top}) = U_k \\Sigma_k (V_k^{\\top} - V_k^{\\top}) = U_k \\Sigma_k (\\mathbf{0}_{2 \\times 5}) = \\mathbf{0}_{3 \\times 5}$$\nTherefore, the error matrix is the zero matrix, and its norm is zero.\n$$E_{\\text{left}} = \\|\\mathbf{0}_{3 \\times 5}\\|_2 = 0$$\n\n**3. Justification of Sketching Sides**\n\nThe **range** of a matrix $A$ is its column space, which is spanned by the columns of $U_k$ from its SVD. To find this space, one must effectively \"excite\" or sample the columns of $A$. This is done by post-multiplication (sketching from the **right**) with a set of test vectors $\\Omega$, forming $Y = A\\Omega$. The columns of $Y$ are linear combinations of the columns of $A$, so $\\operatorname{range}(Y) \\subseteq \\operatorname{range}(A)$. If $\\Omega$ is chosen well (e.g., randomly), $\\operatorname{range}(Y)$ will be a good approximation to $\\operatorname{range}(A)$. In this problem, the \"right sketch for range finding\" failed because the chosen test matrix $\\Omega_R = [e_1 \\;\\; e_2]$ was orthogonal to the row space of $A$ (which is $\\operatorname{span}\\{e_4, e_5\\}$), resulting in $A\\Omega_R = \\mathbf{0}$.\n\nThe **co-range** (or row space) of $A$ is the range of $A^{\\top}$, which is spanned by the columns of $V_k$. To find this space, one must sample the rows of $A$, which is equivalent to sampling the columns of $A^{\\top}$. This is done by sketching $A^{\\top}$ from the right, creating $W = A^{\\top}\\Phi$. This is equivalent to sketching $A$ from the **left** (since $W^{\\top} = \\Phi^{\\top} A$). If $\\Phi$ is chosen well, $\\operatorname{range}(W)$ will be a good approximation of $\\operatorname{range}(A^{\\top})$. In this problem, the \"left sketch for co-range finding\" used $\\Phi_L=U_k$, which is a perfect basis for the range of $A$. This allowed it to perfectly recover the co-range of $A$, since $\\operatorname{range}(A^\\top \\Phi_L) = \\operatorname{range}(A^\\top U_k) = \\operatorname{range}(V_k)$, yielding zero error.\n\nIn summary, one sketches from the **right to find the range** and from the **left to find the co-range**.\n\n**4. Final Calculation**\n\nThe problem asks for the value of $E_{\\text{right}} - E_{\\text{left}}$.\n$$E_{\\text{right}} - E_{\\text{left}} = \\sigma_1 - 0 = \\sigma_1$$\nThis is the final simplified expression.", "answer": "$$\\boxed{\\sigma_1}$$", "id": "3569859"}, {"introduction": "While randomized methods are fast, their utility depends on their accuracy. This practice moves from computational analysis to error analysis, exploring the theoretical guarantees that underpin these algorithms. By deriving an upper bound for the expected approximation error under a model of decaying singular values, you will uncover the crucial trade-off between algorithm parameters—such as the amount of oversampling—and the quality of the resulting low-rank approximation [@problem_id:3569805].", "problem": "Consider a real matrix $A \\in \\mathbb{R}^{m \\times n}$ with singular value decomposition (SVD) $A = U \\Sigma V^{\\top}$, where $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{n \\times n}$ are orthonormal matrices, and $\\Sigma \\in \\mathbb{R}^{m \\times n}$ is diagonal with nonnegative entries $\\sigma_{1} \\geq \\sigma_{2} \\geq \\cdots \\geq 0$. Suppose the singular values of $A$ decay geometrically, i.e., there exist constants $s > 0$ and $\\rho \\in (0,1)$ such that $\\sigma_{j} = s \\rho^{j-1}$ for all $j \\geq 1$. Let $k \\geq 1$ be a target rank and $p \\geq 2$ be an oversampling parameter. Define $\\ell = k + p$, and draw a test matrix $\\Omega \\in \\mathbb{R}^{n \\times \\ell}$ whose entries are independent standard normal random variables (mean $0$, variance $1$). Form the sample matrix $Y = A \\Omega$, compute an orthonormal basis $Q \\in \\mathbb{R}^{m \\times \\ell}$ for the range of $Y$ via a thin orthonormal-QR factorization (where QR denotes orthonormal-QR), and consider the orthogonal projector $QQ^{\\top}$ onto the column space of $Q$. \n\nStarting from the singular value decomposition, rotational invariance of the standard normal distribution, and fundamental properties of orthogonal projectors and the Frobenius norm, derive an analytic expression that quantifies the effect of choosing $\\ell = k + p$ with oversampling $p$ on the expected approximation error $\\mathbb{E}\\|A - QQ^{\\top}A\\|_{F}$, where $\\|\\cdot\\|_{F}$ denotes the Frobenius norm. Assume only the geometric decay model for the singular values and standard facts about Gaussian matrices and orthogonal projectors; do not assume any specific formula for the expected error a priori. Your final result must be a single closed-form expression in terms of $s$, $\\rho$, $k$, and $p$ that upper bounds $\\mathbb{E}\\|A - QQ^{\\top}A\\|_{F}$ under the stated model. No numerical rounding is required.", "solution": "The problem asks for an analytic expression that upper bounds the expected approximation error $\\mathbb{E}\\|A - QQ^{\\top}A\\|_{F}$ for a randomized range finding algorithm, under a specific model for the singular values of the matrix $A$. The derivation must be based on first principles.\n\nLet $P_Y = QQ^{\\top}$ be the orthogonal projector onto the range of the sample matrix $Y = A\\Omega$. The approximation error is given by the matrix $E = A - P_Y A = (I - P_Y)A$. We are interested in an upper bound for the expected Frobenius norm of this error, $\\mathbb{E}\\|E\\|_{F}$.\n\nBy Jensen's inequality, we have $\\mathbb{E}\\|E\\|_{F} \\leq \\left(\\mathbb{E}\\|E\\|_{F}^2\\right)^{1/2}$. We will therefore proceed by deriving a bound on the expected squared Frobenius norm, $\\mathbb{E}\\|A - P_Y A\\|_{F}^2$.\n\nLet the singular value decomposition (SVD) of $A \\in \\mathbb{R}^{m \\times n}$ be $A = U\\Sigma V^{\\top}$. We partition $A$ into its best rank-$k$ approximation and a residual part: $A = A_k + A_{k\\perp}$, where $A_k = U_k \\Sigma_k V_k^{\\top}$ and $A_{k\\perp} = U_{k\\perp} \\Sigma_{k\\perp} V_{k\\perp}^{\\top}$. Here, $U_k$ and $V_k$ contain the first $k$ columns of $U$ and $V$ respectively, and $\\Sigma_k$ is the diagonal matrix of the first $k$ singular values, $\\sigma_1, \\dots, \\sigma_k$. The matrices $U_{k\\perp}, V_{k\\perp}, \\Sigma_{k\\perp}$ are defined analogously for the singular values from $k+1$ onwards. The column spaces of $A_k$ and $A_{k\\perp}$ are orthogonal, which implies $A_k^{\\top}A_{k\\perp} = 0$ and $A_{k\\perp}^{\\top}A_k = 0$.\n\nThe squared error is $\\|(I - P_Y)A\\|_{F}^2 = \\|(I - P_Y)(A_k + A_{k\\perp})\\|_{F}^2$. A standard result in the analysis of randomized algorithms, based on the orthogonality of the subspaces and the symmetry of the Gaussian distribution, states that the expectation of the cross-term vanishes:\n$$ \\mathbb{E}\\|(I - P_Y)A\\|_{F}^2 = \\mathbb{E}\\|(I - P_Y)A_k\\|_{F}^2 + \\mathbb{E}\\|(I - P_Y)A_{k\\perp}\\|_{F}^2 $$\nWe will bound each of these two terms separately.\n\nFor the second term, since $I - P_Y$ is an orthogonal projector, its operator norm is at most $1$. Thus, $\\|(I - P_Y)A_{k\\perp}\\|_{F} \\leq \\|I - P_Y\\|_{2} \\|A_{k\\perp}\\|_{F} \\leq \\|A_{k\\perp}\\|_{F}$. Therefore,\n$$ \\mathbb{E}\\|(I - P_Y)A_{k\\perp}\\|_{F}^2 \\leq \\mathbb{E}[\\|A_{k\\perp}\\|_{F}^2] = \\|A_{k\\perp}\\|_{F}^2 = \\sum_{j=k+1}^{\\min(m,n)} \\sigma_j^2 $$\nHere, we used the fact that $A_{k\\perp}$ is a fixed matrix.\n\nThe first term, $\\mathbb{E}\\|(I - P_Y)A_k\\|_{F}^2$, quantifies how much of the dominant part of $A$ is lost due to the projection. To bound this term, we construct an auxiliary matrix $Z$ in the range of $Y$ that approximates $A_k$. Let $\\Omega_k = V_k^{\\top}\\Omega \\in \\mathbb{R}^{k \\times \\ell}$ and $\\Omega_{k\\perp} = V_{k\\perp}^{\\top}\\Omega \\in \\mathbb{R}^{(n-k) \\times \\ell}$. Since $\\Omega$ is a standard Gaussian matrix and $V$ is orthogonal, $\\Omega_k$ and $\\Omega_{k\\perp}$ are independent standard Gaussian matrices. The sample matrix $Y$ can be written as:\n$$ Y = A\\Omega = (A_k + A_{k\\perp})\\Omega = A_k V V^{\\top} \\Omega + A_{k\\perp}V V^{\\top} \\Omega = U_k\\Sigma_k\\Omega_k + U_{k\\perp}\\Sigma_{k\\perp}\\Omega_{k\\perp} $$\nSince $\\ell = k+p > k$, the $k \\times \\ell$ matrix $\\Omega_k$ has rank $k$ with probability $1$. Thus, its Moore-Penrose pseudoinverse is $\\Omega_k^{\\dagger} = \\Omega_k^{\\top}(\\Omega_k \\Omega_k^{\\top})^{-1}$, and $\\Omega_k \\Omega_k^{\\dagger} = I_k$.\nConsider the matrix $Z = Y\\Omega_k^{\\dagger}V_k^{\\top}$. The columns of $Z$ are linear combinations of the columns of $Y$, so $\\text{range}(Z) \\subseteq \\text{range}(Y)$. The projection $P_Y A_k$ is the best approximation to $A_k$ from $\\text{range}(Y)$, so $\\|A_k - P_Y A_k\\|_{F} \\leq \\|A_k - Z\\|_{F}$.\nLet us analyze $A_k - Z$:\n\\begin{align*}\nA_k - Z &= A_k - (U_k\\Sigma_k\\Omega_k + U_{k\\perp}\\Sigma_{k\\perp}\\Omega_{k\\perp})\\Omega_k^{\\dagger}V_k^{\\top} \\\\\n&= A_k - U_k\\Sigma_k(\\Omega_k\\Omega_k^{\\dagger})V_k^{\\top} - U_{k\\perp}\\Sigma_{k\\perp}\\Omega_{k\\perp}\\Omega_k^{\\dagger}V_k^{\\top} \\\\\n&= A_k - U_k\\Sigma_k I_k V_k^{\\top} - U_{k\\perp}\\Sigma_{k\\perp}\\Omega_{k\\perp}\\Omega_k^{\\dagger}V_k^{\\top} \\\\\n&= A_k - A_k - U_{k\\perp}\\Sigma_{k\\perp}\\Omega_{k\\perp}\\Omega_k^{\\dagger}V_k^{\\top} \\\\\n&= -U_{k\\perp}\\Sigma_{k\\perp}\\Omega_{k\\perp}\\Omega_k^{\\dagger}V_k^{\\top}\n\\end{align*}\nUsing the unitary invariance of the Frobenius norm ($U_{k\\perp}$ has orthonormal columns), we get:\n$$ \\|(I - P_Y)A_k\\|_{F}^2 \\leq \\|A_k - Z\\|_{F}^2 = \\|-U_{k\\perp}\\Sigma_{k\\perp}\\Omega_{k\\perp}\\Omega_k^{\\dagger}V_k^{\\top}\\|_{F}^2 = \\|\\Sigma_{k\\perp}\\Omega_{k\\perp}\\Omega_k^{\\dagger}V_k^{\\top}\\|_{F}^2 $$\nNow we take the expectation. Since $\\Omega_k$ and $\\Omega_{k\\perp}$ are independent, we can condition on $\\Omega_k$:\n$$ \\mathbb{E}_{\\Omega_{k\\perp}}[\\|\\Sigma_{k\\perp}\\Omega_{k\\perp}(\\Omega_k^{\\dagger}V_k^{\\top})\\|_{F}^2 | \\Omega_k] $$\nFor a fixed matrix $B$ and a standard Gaussian matrix $G$, $\\mathbb{E}\\|G B\\|_F^2 = \\|G\\|_F^2 \\|B\\|_F^2$ is not right. Instead, let $B = \\Omega_k^{\\dagger}V_k^{\\top}$. $\\mathbb{E}_{\\Omega_{k\\perp}}[\\|\\Sigma_{k\\perp}\\Omega_{k\\perp}B\\|_{F}^2] = \\text{Tr}(\\mathbb{E}[B^{\\top}\\Omega_{k\\perp}^{\\top}\\Sigma_{k\\perp}^2\\Omega_{k\\perp}B])$. A standard property of Gaussian matrices states that $\\mathbb{E}[\\Omega_{k\\perp}^{\\top}\\Sigma_{k\\perp}^2\\Omega_{k\\perp}] = \\text{Tr}(\\Sigma_{k\\perp}^2) I_{\\ell} = (\\sum_{j=k+1}^n \\sigma_j^2) I_{\\ell}$.\nThus, the conditional expectation is $(\\sum_{j=k+1}^n \\sigma_j^2) \\text{Tr}(B^{\\top}B) = (\\sum_{j=k+1}^n \\sigma_j^2) \\|B\\|_{F}^2$.\nHere, $\\|B\\|_{F}^2 = \\|\\Omega_k^{\\dagger}V_k^{\\top}\\|_{F}^2 = \\|\\Omega_k^{\\dagger}\\|_{F}^2$, since $V_k^{\\top}$ has orthonormal rows.\nTaking the outer expectation over $\\Omega_k$:\n$$ \\mathbb{E}\\|(I - P_Y)A_k\\|_{F}^2 \\leq \\left(\\sum_{j=k+1}^n \\sigma_j^2\\right) \\mathbb{E}[\\|\\Omega_k^{\\dagger}\\|_{F}^2] $$\nThe term $\\mathbb{E}[\\|\\Omega_k^{\\dagger}\\|_{F}^2]$ is the expected trace of an inverse Wishart matrix. For a $k \\times \\ell$ standard Gaussian matrix $\\Omega_k$ with $\\ell = k+p$ and $p > 1$, this expectation is known:\n$$ \\mathbb{E}[\\|\\Omega_k^{\\dagger}\\|_{F}^2] = \\mathbb{E}[\\text{Tr}((\\Omega_k \\Omega_k^{\\top})^{-1})] = \\frac{k}{p-1} $$\nThis requires $p \\ge 2$, as given in the problem statement.\nSo, we have the bound:\n$$ \\mathbb{E}\\|(I - P_Y)A_k\\|_{F}^2 \\leq \\frac{k}{p-1} \\sum_{j=k+1}^n \\sigma_j^2 $$\nCombining the bounds for the two terms:\n$$ \\mathbb{E}\\|A - P_Y A\\|_{F}^2 \\leq \\frac{k}{p-1} \\sum_{j=k+1}^n \\sigma_j^2 + \\sum_{j=k+1}^n \\sigma_j^2 = \\left(1 + \\frac{k}{p-1}\\right) \\sum_{j=k+1}^n \\sigma_j^2 $$\nFinally, we substitute the given model for singular values, $\\sigma_j = s \\rho^{j-1}$. The sum of the squared tail singular values becomes a geometric series. We consider the sum to infinity, which provides an upper bound for any finite $n$:\n$$ \\sum_{j=k+1}^{\\infty} \\sigma_j^2 = \\sum_{j=k+1}^{\\infty} (s \\rho^{j-1})^2 = s^2 \\sum_{j=k+1}^{\\infty} (\\rho^2)^{j-1} $$\nLet $i=j-1$. The sum is $s^2 \\sum_{i=k}^{\\infty} (\\rho^2)^i$. This is a geometric series with first term $a = s^2(\\rho^2)^k = s^2\\rho^{2k}$ and ratio $r=\\rho^2$. Since $\\rho \\in (0,1)$, the sum is:\n$$ \\sum_{j=k+1}^{\\infty} \\sigma_j^2 = \\frac{s^2\\rho^{2k}}{1-\\rho^2} $$\nSubstituting this into our bound for the expected squared error:\n$$ \\mathbb{E}\\|A - P_Y A\\|_{F}^2 \\leq \\left(1 + \\frac{k}{p-1}\\right) \\frac{s^2\\rho^{2k}}{1-\\rho^2} $$\nBy applying Jensen's inequality to the original error, we take the square root of this upper bound:\n$$ \\mathbb{E}\\|A - P_Y A\\|_{F} \\leq \\left(\\mathbb{E}\\|A - P_Y A\\|_{F}^2\\right)^{1/2} \\leq \\left( \\left(1 + \\frac{k}{p-1}\\right) \\frac{s^2\\rho^{2k}}{1-\\rho^2} \\right)^{1/2} $$\n$$ \\mathbb{E}\\|A - P_Y A\\|_{F} \\leq \\left(1 + \\frac{k}{p-1}\\right)^{1/2} \\frac{s\\rho^k}{\\sqrt{1-\\rho^2}} $$\nThis is the desired closed-form expression.", "answer": "$$\\boxed{s \\left(1 + \\frac{k}{p-1}\\right)^{\\frac{1}{2}} \\frac{\\rho^k}{\\sqrt{1-\\rho^2}}}$$", "id": "3569805"}]}