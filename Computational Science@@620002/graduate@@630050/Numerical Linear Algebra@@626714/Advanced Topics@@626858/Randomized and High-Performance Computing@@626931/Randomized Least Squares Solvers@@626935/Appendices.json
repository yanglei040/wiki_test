{"hands_on_practices": [{"introduction": "The primary motivation for using randomized algorithms in numerical linear algebra is often a dramatic reduction in computational cost. This exercise provides a foundational, quantitative analysis of this benefit by directly comparing the arithmetic complexity of a modern sketch-and-solve pipeline against a classical deterministic method [@problem_id:3570172]. By deriving and contrasting the exact floating-point operation counts, you will gain a concrete understanding of the performance trade-offs and identify the regimes where randomized least squares solvers offer a decisive advantage.", "problem": "Consider a tall-and-skinny least squares problem with matrix $A \\in \\mathbb{R}^{n \\times d}$, $n \\gg d$, and vector $b \\in \\mathbb{R}^{n}$. Let $\\operatorname{nnz}(A)$ denote the number of nonzero entries of $A$. You employ a sketch-and-solve pipeline using an oblivious sparse subspace embedding $S \\in \\mathbb{R}^{m \\times n}$ that has exactly $s$ nonzeros per column, with signs in $\\{-1,+1\\}$ and uniformly random row locations. You form the sketched design matrix $SA \\in \\mathbb{R}^{m \\times d}$ and solve the reduced problem $\\min_{x \\in \\mathbb{R}^{d}} \\|SAx - Sb\\|_{2}$ by Householder orthogonal-triangular (QR) factorization on $SA$. Assume the following well-tested performance models and facts from numerical linear algebra:\n- Each nonzero of $A$ contributes $s$ updates to the corresponding entries of $SA$ when computing $SA = SA(A)$ via the sparse embedding application, so the arithmetic cost to form $SA$ is proportional to $s \\cdot \\operatorname{nnz}(A)$.\n- The Householder orthogonal-triangular (QR) factorization of a dense matrix in $\\mathbb{R}^{m \\times d}$ with $m \\ge d$ costs $2md^{2} - \\tfrac{2}{3}d^{3}$ floating-point operations.\n- A classical Householder orthogonal-triangular (QR) factorization on the original (generally dense after fill-in) $A \\in \\mathbb{R}^{n \\times d}$ costs $2nd^{2} - \\tfrac{2}{3}d^{3}$ floating-point operations.\n\nUsing these bases, derive the exact arithmetic operation counts, as symbolic expressions in terms of $n$, $d$, $m$, $s$, and $\\operatorname{nnz}(A)$, for:\n1. The total number of floating-point operations to form $SA$ and solve the reduced problem by Householder orthogonal-triangular (QR) on $SA$.\n2. The number of floating-point operations for the classical Householder orthogonal-triangular (QR) on $A$.\n\nExpress your final answer as a single row matrix containing two entries corresponding to items 1 and 2 above. Your answer should be a closed-form analytic expression in terms of $n$, $d$, $m$, $s$, and $\\operatorname{nnz}(A)$, in units of floating-point operations. No rounding is required.", "solution": "The problem statement is evaluated for validity before proceeding to a solution.\n\n### Step 1: Extract Givens\n- A tall-and-skinny least squares problem with matrix $A \\in \\mathbb{R}^{n \\times d}$ and vector $b \\in \\mathbb{R}^{n}$.\n- Condition: $n \\gg d$.\n- Notation: $\\operatorname{nnz}(A)$ is the number of nonzero entries of $A$.\n- Method: A sketch-and-solve pipeline using an oblivious sparse subspace embedding $S \\in \\mathbb{R}^{m \\times n}$.\n- Properties of $S$: Exactly $s$ nonzeros per column, with values in $\\{-1,+1\\}$ and uniformly random row locations.\n- Sketched problem: $\\min_{x \\in \\mathbb{R}^{d}} \\|SAx - Sb\\|_{2}$.\n- Solution method for the sketched problem: Householder orthogonal-triangular (QR) factorization on the matrix $SA \\in \\mathbb{R}^{m \\times d}$.\n- Cost Model 1: The arithmetic cost to form $SA$ is proportional to $s \\cdot \\operatorname{nnz}(A)$, based on the fact that each nonzero of $A$ contributes $s$ updates.\n- Cost Model 2: The Householder QR factorization of a dense matrix in $\\mathbb{R}^{m \\times d}$ with $m \\ge d$ costs $2md^{2} - \\frac{2}{3}d^{3}$ floating-point operations.\n- Cost Model 3: The classical Householder QR factorization on $A \\in \\mathbb{R}^{n \\times d}$ costs $2nd^{2} - \\frac{2}{3}d^{3}$ floating-point operations.\n- Required: Derive exact arithmetic operation counts for:\n  1. Forming $SA$ and solving the reduced problem by Householder QR on $SA$.\n  2. The classical Householder QR on $A$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is well-defined and scientifically sound. It is grounded in the established principles of randomized numerical linear algebra, specifically the \"sketch-and-solve\" method for overdetermined least squares problems. The provided cost models for Householder QR factorization are standard, representing the dominant term in the operation count. The description of the sparse embedding and the cost model for its application are consistent with state-of-the-art literature (e.g., Clarkson-Woodruff sketch). All necessary variables ($n, d, m, s, \\operatorname{nnz}(A)$) are defined. The problem is objective and formalizable.\n\nThe phrase \"solve the reduced problem\" requires careful interpretation. The full solution process involves forming $Sb$, performing the QR factorization of $SA$, applying the orthogonal transformation to $Sb$, and solving the resulting triangular system. The problem provides a specific cost model only for the factorization step ($2md^2 - \\frac{2}{3}d^3$). The parallel structure of the questions, where the cost for the classical method is also given just for the factorization, strongly implies that \"solve by QR\" refers to this dominant factorization stage. However, setting up the reduced problem also requires forming $Sb$. The cost for forming $Sb$ is not explicitly given but can be directly derived from the properties of $S$ in a manner analogous to the formation of $SA$. A competent student is expected to deduce this cost. The problem is therefore deemed complete and well-posed.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A solution will be derived based on the provided models and logical deductions.\n\n### Derivation of Operation Counts\n\nThe problem requires the calculation of two distinct operation counts.\n\n**1. Total operations for the sketch-and-solve method**\n\nThe total number of floating-point operations for this method is the sum of three components:\n(a) The cost of forming the sketched matrix $SA$.\n(b) The cost of forming the sketched vector $Sb$.\n(c) The cost of solving the reduced least squares problem using Householder QR factorization on $SA$.\n\n(a) Cost to form $SA$:\nThe sketching matrix $S \\in \\mathbb{R}^{m \\times n}$ has $s$ non-zero entries per column, which are either $+1$ or $-1$. The formation of the product $SA$ can be viewed from the perspective of the non-zero entries of $A$. For each non-zero entry $A_{ij}$, it participates in the computation of the $j$-th column of $SA$. Specifically, the value $A_{ij}$ is multiplied by the $i$-th column of $S$. Since the $i$-th column of $S$ has $s$ non-zero entries, $A_{ij}$ contributes to $s$ entries in the $j$-th column of $SA$. Each such contribution is an update of the form $(SA)_{k,j} \\leftarrow (SA)_{k,j} + S_{k,i}A_{ij}$. Since $S_{k,i} \\in \\{-1, +1\\}$, this is an addition or a subtraction, which counts as one floating-point operation. As there are $\\operatorname{nnz}(A)$ non-zero entries in $A$, the total cost is:\n$$ \\text{Cost}(SA) = s \\cdot \\operatorname{nnz}(A) $$\n\n(b) Cost to form $Sb$:\nThis cost must be included as part of setting up the reduced problem. The vector $b \\in \\mathbb{R}^n$ is typically dense. The product $Sb$ is computed by multiplying the sparse matrix $S$ by the vector $b$. For each element $b_i$ of the vector $b$ (for $i=1, \\dots, n$), it is multiplied by the $i$-th column of $S$. This column has $s$ non-zero entries. Thus, $b_i$ is added or subtracted to $s$ entries in the resulting vector $Sb$. This process is repeated for all $n$ elements of $b$. The total number of floating-point operations is:\n$$ \\text{Cost}(Sb) = s \\cdot n $$\n\n(c) Cost to solve by QR on $SA$:\nThe matrix $SA$ is in $\\mathbb{R}^{m \\times d}$. The problem explicitly provides the cost for the Householder QR factorization of such a matrix as $2md^{2} - \\frac{2}{3}d^{3}$. As reasoned during validation, we use this as the cost to \"solve\" the system via QR.\n$$ \\text{Cost}(\\text{QR on } SA) = 2md^{2} - \\frac{2}{3}d^{3} $$\n\nThe total cost for the first part is the sum of these three components:\n$$ \\text{Total Cost}_1 = \\text{Cost}(SA) + \\text{Cost}(Sb) + \\text{Cost}(\\text{QR on } SA) = s \\cdot \\operatorname{nnz}(A) + sn + 2md^{2} - \\frac{2}{3}d^{3} $$\n\n**2. Operations for the classical Householder QR method**\n\nThe problem asks for the number of floating-point operations for the classical Householder QR factorization on the original matrix $A \\in \\mathbb{R}^{n \\times d}$. This cost is given directly in the problem statement as:\n$$ \\text{Total Cost}_2 = 2nd^{2} - \\frac{2}{3}d^{3} $$\n\nThese two expressions represent the final answers for the two parts of the problem. They will be presented as a row matrix as requested.", "answer": "$$ \\boxed{\\begin{pmatrix} s \\operatorname{nnz}(A) + sn + 2md^{2} - \\frac{2}{3}d^{3} & 2nd^{2} - \\frac{2}{3}d^{3} \\end{pmatrix}} $$", "id": "3570172"}, {"introduction": "A key insight in randomized methods is that not all data points contribute equally to the solution of a least squares problem; some rows have disproportionately high \"leverage.\" This practice moves from abstract theory to a concrete demonstration by having you construct a design matrix $A$ with highly non-uniform leverage scores and then compute its coherence $\\mu(A)$ [@problem_id:3570188]. By working through this worst-case scenario, you will see firsthand why intelligent, data-dependent sampling strategies like leverage-score sampling can vastly outperform simple uniform sampling, making the abstract concept of coherence tangible.", "problem": "Consider the overdetermined least squares problem with design matrix $A \\in \\mathbb{R}^{n \\times k}$ and response vector $b \\in \\mathbb{R}^{n}$, where the goal is to compute $x^{\\star} \\in \\arg\\min_{x \\in \\mathbb{R}^{k}} \\|A x - b\\|_{2}$. Assume a sketch-and-solve approach in which a sampling-and-reweighting matrix $S \\in \\mathbb{R}^{m \\times n}$ is used to form a sketched problem $\\min_{x \\in \\mathbb{R}^{k}} \\|S A x - S b\\|_{2}$. The sampling probabilities are denoted by $p_{i}$ for $i \\in \\{1,2,\\dots,n\\}$, and each selected row is reweighted by the factor $(m p_{i})^{-1/2}$.\n\nDefine the row leverage scores $\\ell_{i}$ as the diagonal entries of the orthogonal projector onto the column space of $A$, equivalently $\\ell_{i} = \\|U_{i,:}\\|_{2}^{2}$ where $A = U R$ is the thin factorization with $U \\in \\mathbb{R}^{n \\times k}$ having orthonormal columns and $R \\in \\mathbb{R}^{k \\times k}$ upper triangular. The coherence of $A$ is defined as $\\mu(A) = \\frac{n}{k} \\max_{i} \\ell_{i}$.\n\nConstruct a worst-case coherent design using polynomial features: take $k = 2$ and let $A$ be formed from the degree-$1$ monomial features, that is, $A$ has two columns corresponding to the functions $x \\mapsto 1$ and $x \\mapsto x$. Let the $n$ input locations be $x_{1}, \\dots, x_{n} \\in \\{-1, +1\\}$ with exactly one location equal to $-1$ and the remaining $n-1$ locations equal to $+1$. Using the fundamental definitions of least squares, leverage scores, and coherence, determine the coherence $\\mu(A)$ for this design.\n\nUsing the notion of subspace embedding for $(1 \\pm \\varepsilon)$-distortion with failure probability at most $\\delta$, and standard sampling-and-reweighting constructions, argue from first principles that the smallest sufficient sample size for uniform sampling (i.e., $p_{i} = 1/n$ for all $i$) scales as $m_{\\mathrm{unif}} \\propto k \\ln(k/\\delta) \\mu(A) \\varepsilon^{-2}$, while the smallest sufficient sample size for leverage-score sampling (i.e., $p_{i} = \\ell_{i}/k$) scales as $m_{\\mathrm{lev}} \\propto k \\ln(k/\\delta) \\varepsilon^{-2}$. Conclude the exact asymptotic gain factor, defined as the ratio $m_{\\mathrm{unif}}/m_{\\mathrm{lev}}$ with identical $(\\varepsilon, \\delta)$, for the above polynomial design, and express your final answer as a single closed-form analytic expression in terms of $n$ only. No physical units are involved. No rounding is required.", "solution": "The user wants to find the asymptotic gain factor of leverage-score sampling over uniform sampling for a specific least squares problem.\n\n### Step 1: Extract Givens\n- Problem type: Overdetermined least squares, $\\min_{x \\in \\mathbb{R}^{k}} \\|A x - b\\|_{2}$.\n- Design matrix: $A \\in \\mathbb{R}^{n \\times k}$.\n- Response vector: $b \\in \\mathbb{R}^{n}$.\n- Sketching matrix: $S \\in \\mathbb{R}^{m \\times n}$, a sampling-and-reweighting matrix.\n- Sketched problem: $\\min_{x \\in \\mathbb{R}^{k}} \\|S A x - S b\\|_{2}$.\n- Sampling probabilities: $p_{i}$ for $i \\in \\{1, 2, \\dots, n\\}$.\n- Reweighting factor: $(m p_{i})^{-1/2}$.\n- Leverage scores: $\\ell_{i} = \\|U_{i,:}\\|_{2}^{2}$, where $A = U R$ is the thin QR factorization of $A$, with $U \\in \\mathbb{R}^{n \\times k}$ having orthonormal columns.\n- Coherence: $\\mu(A) = \\frac{n}{k} \\max_{i} \\ell_{i}$.\n- Specific design:\n    - $k=2$.\n    - Columns of $A$ are basis functions $x \\mapsto 1$ and $x \\mapsto x$.\n    - Input locations $x_{1}, \\dots, x_{n} \\in \\{-1, +1\\}$.\n    - Exactly one location is $-1$, the other $n-1$ are $+1$. Let $x_{1}=-1$ and $x_{i}=+1$ for $i=2, \\dots, n$.\n- Sample size scalings for $(1 \\pm \\varepsilon)$-distortion with failure probability $\\delta$:\n    - Uniform sampling ($p_{i} = 1/n$): $m_{\\mathrm{unif}} \\propto k \\ln(k/\\delta) \\mu(A) \\varepsilon^{-2}$.\n    - Leverage-score sampling ($p_{i} = \\ell_{i}/k$): $m_{\\mathrm{lev}} \\propto k \\ln(k/\\delta) \\varepsilon^{-2}$.\n- Goal: Determine the exact asymptotic gain factor, defined as the ratio $m_{\\mathrm{unif}}/m_{\\mathrm{lev}}$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically and mathematically sound. It is based on standard, well-established concepts in numerical linear algebra and randomized algorithms, including least squares, QR factorization, statistical leverage scores, coherence, and sketch-and-solve methods (specifically sampling and reweighting). The definitions provided are standard in the field. The setup for the design matrix $A$ is explicit and allows for direct computation. The question is well-posed, asking for a specific, computable quantity (the coherence $\\mu(A)$) and a subsequent ratio based on established theoretical results. The problem is self-contained and free of contradictions or ambiguities. Therefore, it is a valid problem.\n\n### Step 3: Proceed to Solution\n\nThe solution involves three main parts:\n1.  Construct the matrix $A$ and compute its thin QR factorization to find the matrix $U$.\n2.  Use the rows of $U$ to compute the leverage scores $\\ell_{i}$ and then the coherence $\\mu(A)$.\n3.  Use the provided scaling laws for sample sizes to determine the asymptotic gain factor $m_{\\mathrm{unif}}/m_{\\mathrm{lev}}$.\n\n**Part 1: Constructing $A$ and finding $U$**\n\nThe matrix $A$ has $n$ rows and $k=2$ columns. The first column is a vector of all ones, corresponding to the basis function $x \\mapsto 1$. The second column corresponds to the basis function $x \\mapsto x$ evaluated at the points $x_1 = -1, x_2 = 1, \\dots, x_n = 1$.\n$$\nA = \\begin{pmatrix}\n1 & x_1 \\\\\n1 & x_2 \\\\\n\\vdots & \\vdots \\\\\n1 & x_n\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n1 & -1 \\\\\n1 & 1 \\\\\n1 & 1 \\\\\n\\vdots & \\vdots \\\\\n1 & 1\n\\end{pmatrix}\n$$\nLet the columns of $A$ be $a_1$ and $a_2$. We find the orthonormal basis for the column space of $A$ using the Gram-Schmidt process to obtain the columns $u_1, u_2$ of the matrix $U$.\n\nThe first column of $A$ is $a_1 = [1, 1, \\dots, 1]^T$. Its squared norm is $\\|a_1\\|_2^2 = \\sum_{i=1}^n 1^2 = n$.\nThe first orthonormal vector $u_1$ is:\n$$\nu_1 = \\frac{a_1}{\\|a_1\\|_2} = \\frac{1}{\\sqrt{n}} \\begin{pmatrix} 1 \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{pmatrix}\n$$\nThe second column of $A$ is $a_2 = [-1, 1, \\dots, 1]^T$. We first find the component of $a_2$ orthogonal to $u_1$, denoted $w_2$.\n$$\nw_2 = a_2 - \\langle a_2, u_1 \\rangle u_1\n$$\nThe inner product is:\n$$\n\\langle a_2, u_1 \\rangle = a_2^T u_1 = \\frac{1}{\\sqrt{n}} \\left( (-1)(1) + (n-1)(1)(1) \\right) = \\frac{n-2}{\\sqrt{n}}\n$$\nSo, $w_2$ is:\n$$\nw_2 = a_2 - \\frac{n-2}{\\sqrt{n}} u_1 = a_2 - \\frac{n-2}{n} a_1 = \\begin{pmatrix} -1 \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{pmatrix} - \\frac{n-2}{n} \\begin{pmatrix} 1 \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} -1 - \\frac{n-2}{n} \\\\ 1 - \\frac{n-2}{n} \\\\ \\vdots \\\\ 1 - \\frac{n-2}{n} \\end{pmatrix} = \\begin{pmatrix} \\frac{-n-n+2}{n} \\\\ \\frac{n-n+2}{n} \\\\ \\vdots \\\\ \\frac{2}{n} \\end{pmatrix} = \\frac{2}{n} \\begin{pmatrix} 1-n \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{pmatrix}\n$$\nNext, we normalize $w_2$ to get $u_2$. The squared norm of $w_2$ is:\n$$\n\\|w_2\\|_2^2 = \\left(\\frac{2}{n}\\right)^2 \\left( (1-n)^2 + \\sum_{i=2}^n 1^2 \\right) = \\frac{4}{n^2} \\left( (n-1)^2 + n-1 \\right) = \\frac{4}{n^2} (n-1)(n-1+1) = \\frac{4(n-1)n}{n^2} = \\frac{4(n-1)}{n}\n$$\nThe norm is $\\|w_2\\|_2 = \\sqrt{\\frac{4(n-1)}{n}} = \\frac{2\\sqrt{n-1}}{\\sqrt{n}}$.\nThe second orthonormal vector $u_2$ is:\n$$\nu_2 = \\frac{w_2}{\\|w_2\\|_2} = \\frac{\\frac{2}{n} \\begin{pmatrix} 1-n \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{pmatrix}}{\\frac{2\\sqrt{n-1}}{\\sqrt{n}}} = \\frac{\\sqrt{n}}{n\\sqrt{n-1}} \\begin{pmatrix} 1-n \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{pmatrix} = \\frac{1}{\\sqrt{n(n-1)}} \\begin{pmatrix} 1-n \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{pmatrix}\n$$\nThe matrix $U$ with orthonormal columns is $U = [u_1 | u_2]$.\n\n**Part 2: Computing Leverage Scores and Coherence**\n\nThe leverage score $\\ell_i$ is the squared Euclidean norm of the $i$-th row of $U$, denoted $U_{i,:}$.\nThe first row of $U$, corresponding to $x_1 = -1$, is:\n$$\nU_{1,:} = \\left( \\frac{1}{\\sqrt{n}}, \\frac{1-n}{\\sqrt{n(n-1)}} \\right)\n$$\nIts leverage score $\\ell_1$ is:\n$$\n\\ell_1 = \\|U_{1,:}\\|_2^2 = \\left(\\frac{1}{\\sqrt{n}}\\right)^2 + \\left(\\frac{1-n}{\\sqrt{n(n-1)}}\\right)^2 = \\frac{1}{n} + \\frac{(1-n)^2}{n(n-1)} = \\frac{1}{n} + \\frac{(n-1)^2}{n(n-1)} = \\frac{1}{n} + \\frac{n-1}{n} = \\frac{n}{n} = 1\n$$\nFor any other row $i \\in \\{2, \\dots, n\\}$, corresponding to $x_i = 1$, the row of $U$ is:\n$$\nU_{i,:} = \\left( \\frac{1}{\\sqrt{n}}, \\frac{1}{\\sqrt{n(n-1)}} \\right)\n$$\nThe leverage score $\\ell_i$ for $i > 1$ is:\n$$\n\\ell_i = \\|U_{i,:}\\|_2^2 = \\left(\\frac{1}{\\sqrt{n}}\\right)^2 + \\left(\\frac{1}{\\sqrt{n(n-1)}}\\right)^2 = \\frac{1}{n} + \\frac{1}{n(n-1)} = \\frac{n-1+1}{n(n-1)} = \\frac{n}{n(n-1)} = \\frac{1}{n-1}\n$$\nAs a sanity check, the sum of leverage scores must equal the rank $k=2$: $\\sum_{i=1}^n \\ell_i = \\ell_1 + \\sum_{i=2}^n \\ell_i = 1 + (n-1) \\left(\\frac{1}{n-1}\\right) = 1+1=2$. This is correct.\n\nNow, we compute the coherence $\\mu(A)$.\nFor $n \\ge 3$, we have $n-1 \\ge 2$, so $\\frac{1}{n-1} \\le \\frac{1}{2} < 1$. For $n=2$, $\\frac{1}{n-1}=1$. Thus, for any $n \\ge 2$, $\\max_i \\ell_i = \\ell_1 = 1$.\n$$\n\\mu(A) = \\frac{n}{k} \\max_{i} \\ell_{i} = \\frac{n}{2} \\cdot 1 = \\frac{n}{2}\n$$\n\n**Part 3: Determining the Asymptotic Gain Factor**\n\nThe problem provides the scaling for sufficient sample sizes for uniform and leverage-score sampling. We are asked to argue for them from first principles. The core of the argument rests on matrix concentration inequalities. For a sampling-and-reweighting scheme, the sample complexity $m$ required to achieve a subspace embedding is dominated by the term $m \\propto (\\max_i \\frac{\\ell_i}{p_i}) \\frac{\\log(k/\\delta)}{\\varepsilon^2}$.\n\nFor uniform sampling, $p_i = 1/n$ for all $i$. The complexity factor becomes:\n$$\n\\max_i \\frac{\\ell_i}{p_i} = \\max_i \\frac{\\ell_i}{1/n} = n \\max_i \\ell_i\n$$\nUsing the definition of coherence $\\mu(A) = \\frac{n}{k} \\max_i \\ell_i$, we can write $n \\max_i \\ell_i = k \\, \\mu(A)$. Thus, $m_{\\mathrm{unif}} \\propto k \\, \\mu(A) \\frac{\\log(k/\\delta)}{\\varepsilon^2}$, which matches the problem statement.\n\nFor leverage-score sampling, $p_i = \\ell_i / \\sum_j \\ell_j = \\ell_i/k$. The complexity factor becomes:\n$$\n\\max_i \\frac{\\ell_i}{p_i} = \\max_i \\frac{\\ell_i}{\\ell_i/k} = \\max_i k = k\n$$\nThus, $m_{\\mathrm{lev}} \\propto k \\frac{\\log(k/\\delta)}{\\varepsilon^2}$, which also matches the problem statement.\n\nThe asymptotic gain factor is the ratio $m_{\\mathrm{unif}}/m_{\\mathrm{lev}}$. Assuming the constants of proportionality are identical (which is standard in this type of analysis), we have:\n$$\n\\frac{m_{\\mathrm{unif}}}{m_{\\mathrm{lev}}} = \\frac{k \\ln(k/\\delta) \\mu(A) \\varepsilon^{-2}}{k \\ln(k/\\delta) \\varepsilon^{-2}} = \\mu(A)\n$$\nSubstituting the value of $\\mu(A)$ we calculated for the specified polynomial design:\n$$\n\\frac{m_{\\mathrm{unif}}}{m_{\\mathrm{lev}}} = \\mu(A) = \\frac{n}{2}\n$$\nThis ratio represents the factor by which uniform sampling requires more samples than the optimal leverage-score sampling to achieve the same approximation guarantee, for this worst-case coherent design.", "answer": "$$\n\\boxed{\\frac{n}{2}}\n$$", "id": "3570188"}, {"introduction": "While randomized solvers are powerful, they are not infallible, and a naive application can lead to \"overfitting\" the sketched data, yielding a poor solution to the original problem. This exercise explores a carefully constructed pathological case where the sketching process annihilates the most important part of the residual vector, leading to a misleadingly perfect fit for the sketched problem [@problem_id:3570209]. Analyzing this failure mode is crucial for developing robust algorithms and building intuition about the importance of theoretical safeguards like proper reweighting and using a holdout set for validation.", "problem": "Consider the overdetermined least squares problem with matrix $A \\in \\mathbb{R}^{m \\times n}$ and vector $b \\in \\mathbb{R}^{m}$, where the goal is to compute $x^\\star \\in \\mathbb{R}^{n}$ that minimizes $\\|A x - b\\|_{2}$. A standard randomized sketch-and-solve approach forms a sketching matrix $S \\in \\mathbb{R}^{s \\times m}$ (for some $s \\ll m$) and solves the sketched problem $\\min_{x \\in \\mathbb{R}^{n}} \\|S A x - S b\\|_{2}$. By definition, the column space $\\mathrm{col}(A)$ is the set $\\{A x : x \\in \\mathbb{R}^{n}\\}$, and the least squares solution solves the normal equations $A^{\\top} A x = A^{\\top} b$ when $A^{\\top} A$ is invertible. \n\nConstruct the following explicit example to reason from first principles. Let $m = 3$ and $n = 2$, and define\n$$\nA = \\begin{bmatrix}\n1 & 0 \\\\\n0 & 1 \\\\\n1 & 1\n\\end{bmatrix},\n\\quad\nb = \\begin{bmatrix}\n1 \\\\\n1 \\\\\n0\n\\end{bmatrix}.\n$$\nObserve that $b \\notin \\mathrm{col}(A)$ because there is no $x \\in \\mathbb{R}^{2}$ satisfying $A x = b$. Let the sketching matrix $S \\in \\mathbb{R}^{2 \\times 3}$ sample the first two rows of $A$ and $b$:\n$$\nS = \\begin{bmatrix}\n1 & 0 & 0 \\\\\n0 & 1 & 0\n\\end{bmatrix}.\n$$\nThen $S A = I_{2}$ and $S b = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$, so $S b \\in \\mathrm{col}(S A)$ exactly. The sketched solution $x_{S}$ minimizes $\\|S A x - S b\\|_{2}$, while the full least squares solution $x^{\\star}$ minimizes $\\|A x - b\\|_{2}$. This setup exhibits the pathological situation in which $S b$ lies in $\\mathrm{col}(S A)$ (so the sketched residual can be zero) even though $b \\notin \\mathrm{col}(A)$ (so the full residual cannot be zero).\n\nAnswer the following multiple-choice question about this construction and the broader implications for sketch-and-solve least squares. Select all statements that are correct.\n\nA. In this construction, the sketched solution achieves zero sketched residual, yet the full residual at $x_{S}$ is nonzero and equals $\\|A x_{S} - b\\|_{2}^{2} = 4$, illustrating overfitting to the sketch due to the residual direction being annihilated by $S$.\n\nB. If one reweights sketched rows by inverse sampling probabilities and uses leverage-score sampling, it eliminates the possibility of overfitting to the sketch in all cases.\n\nC. Using an independent holdout sketch $T \\in \\mathbb{R}^{t \\times m}$ to validate by checking $\\|T(A x_{S} - b)\\|_{2}$ provides a probabilistic safeguard against overfitting: if $T$ is a subspace embedding for $\\mathrm{col}([A \\;\\; b])$, then with high probability $\\|T(A x - b)\\|_{2}$ approximates $\\|A x - b\\|_{2}$ uniformly over $x$.\n\nD. Increasing the sketch size $s$ cannot help in this example because the residual direction lives entirely in the nullspace of $A$.\n\nE. Using ridge regularization in the sketched solve, that is, minimizing $\\|S A x - S b\\|_{2}^{2} + \\lambda \\|x\\|_{2}^{2}$ for some $\\lambda > 0$, necessarily yields a smaller full residual than the exact full least squares solution $x^{\\star}$.\n\nF. If the sketch $S$ is formed by sampling row $i$ with probability $p_{i}$ and rescaling that row by $1/\\sqrt{s p_{i}}$ over $s$ independent samples with replacement, then for any $y \\in \\mathbb{R}^{m}$ one has $\\mathbb{E}[\\|S y\\|_{2}^{2}] = \\|y\\|_{2}^{2}$. Applied to $y = A x - b$, such reweighting removes bias in the sketched residual in expectation, though a single realization can still overfit.", "solution": "### Step 1: Extract Givens\nThe problem statement provides the following information:\n- The overdetermined least squares problem is to find $x^\\star \\in \\mathbb{R}^{n}$ that minimizes $\\|A x - b\\|_{2}$ for $A \\in \\mathbb{R}^{m \\times n}$ and $b \\in \\mathbb{R}^{m}$.\n- A randomized sketch-and-solve approach solves the sketched problem $\\min_{x \\in \\mathbb{R}^{n}} \\|S A x - S b\\|_{2}$ using a sketching matrix $S \\in \\mathbb{R}^{s \\times m}$ with $s \\ll m$.\n- The column space of $A$ is $\\mathrm{col}(A) = \\{A x : x \\in \\mathbb{R}^{n}\\}$.\n- The least squares solution solves the normal equations $A^{\\top} A x = A^{\\top} b$ when $A^{\\top} A$ is invertible.\n- A specific example is constructed with $m = 3$ and $n = 2$:\n$$\nA = \\begin{bmatrix}\n1 & 0 \\\\\n0 & 1 \\\\\n1 & 1\n\\end{bmatrix},\n\\quad\nb = \\begin{bmatrix}\n1 \\\\\n1 \\\\\n0\n\\end{bmatrix}.\n$$\n- It is stated that $b \\notin \\mathrm{col}(A)$.\n- The sketching matrix is given as $S \\in \\mathbb{R}^{2 \\times 3}$:\n$$\nS = \\begin{bmatrix}\n1 & 0 & 0 \\\\\n0 & 1 & 0\n\\end{bmatrix}.\n$$\n- It is stated that $S A = I_{2}$ and $S b = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$, and consequently $S b \\in \\mathrm{col}(S A)$.\n- The sketched solution is denoted $x_{S}$, and the full least squares solution is denoted $x^{\\star}$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is scientifically grounded in the field of numerical linear algebra, specifically randomized algorithms for least squares. The definitions and setup are standard. I will verify the specific claims made in the construction.\n\n1.  **Check if $b \\notin \\mathrm{col}(A)$:** We seek $x = \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix}$ such that $Ax = b$. This yields the system of equations:\n    $1 \\cdot x_1 + 0 \\cdot x_2 = 1 \\implies x_1 = 1$\n    $0 \\cdot x_1 + 1 \\cdot x_2 = 1 \\implies x_2 = 1$\n    $1 \\cdot x_1 + 1 \\cdot x_2 = 0 \\implies x_1 + x_2 = 0$\n    Substituting the first two equations into the third gives $1 + 1 = 0$, which is $2=0$, a contradiction. Thus, the system is inconsistent, and $b \\notin \\mathrm{col}(A)$. The claim is correct.\n\n2.  **Check $SA$, $Sb$, and $Sb \\in \\mathrm{col}(SA)$:**\n    $$\n    SA = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\end{bmatrix} \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{bmatrix} = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} = I_{2}.\n    $$\n    $$\n    Sb = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 1 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}.\n    $$\n    The condition $Sb \\in \\mathrm{col}(SA)$ requires solving $SAx = Sb$ for $x$. This is $I_2 x = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$, which has a unique solution $x = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$. Thus, $Sb \\in \\mathrm{col}(SA)$ is correct.\n\nThe problem statement is internally consistent, mathematically sound, and well-posed. It presents a valid and standard example used to illustrate a potential pitfall of naive sketching.\n\n### Step 3: Verdict and Action\nThe problem is valid. I will now proceed to derive the solutions and evaluate each option.\n\n**Calculations for the specific example:**\n\n**1. Full Least Squares Solution, $x^{\\star}$:**\nThe solution $x^{\\star}$ minimizes $\\|Ax-b\\|_2$ and solves the normal equations $A^{\\top}Ax = A^{\\top}b$.\n$$\nA^{\\top}A = \\begin{bmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\end{bmatrix} \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{bmatrix} = \\begin{bmatrix} 2 & 1 \\\\ 1 & 2 \\end{bmatrix}.\n$$\n$$\nA^{\\top}b = \\begin{bmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 1 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}.\n$$\nThe system to solve is $\\begin{bmatrix} 2 & 1 \\\\ 1 & 2 \\end{bmatrix} x^{\\star} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$.\nThe inverse of $A^{\\top}A$ is $(A^{\\top}A)^{-1} = \\frac{1}{(2)(2) - (1)(1)} \\begin{bmatrix} 2 & -1 \\\\ -1 & 2 \\end{bmatrix} = \\frac{1}{3} \\begin{bmatrix} 2 & -1 \\\\ -1 & 2 \\end{bmatrix}$.\n$$\nx^{\\star} = \\frac{1}{3} \\begin{bmatrix} 2 & -1 \\\\ -1 & 2 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} = \\frac{1}{3} \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 1/3 \\\\ 1/3 \\end{bmatrix}.\n$$\n\n**2. Sketched Solution, $x_{S}$:**\nThe solution $x_S$ minimizes $\\|SAx - Sb\\|_2$.\nAs shown before, this is equivalent to minimizing $\\|I_2 x - \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\\|_2$.\nThis objective function is zero if and only if $x = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$. Thus, the sketched solution is\n$$\nx_S = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}.\n$$\n\n**Detailed Analysis of Options:**\n\n**A.** In this construction, the sketched solution achieves zero sketched residual, yet the full residual at $x_{S}$ is nonzero and equals $\\|A x_{S} - b\\|_{2}^{2} = 4$, illustrating overfitting to the sketch due to the residual direction being annihilated by $S$.\n\n- **Sketched Residual:** The sketched residual is $\\|SAx_S - Sb\\|_2 = \\|I_2 \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} - \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\\|_2 = \\|\\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}\\|_2 = 0$. The first part of the statement is correct.\n- **Full Residual at $x_S$:** We compute the residual vector $r_S = Ax_S - b$.\n$$\nr_S = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} - \\begin{bmatrix} 1 \\\\ 1 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 1 \\\\ 2 \\end{bmatrix} - \\begin{bmatrix} 1 \\\\ 1 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 2 \\end{bmatrix}.\n$$\nThe squared norm of this residual is $\\|r_S\\|_2^2 = 0^2 + 0^2 + 2^2 = 4$. This is nonzero and equals $4$. This part is also correct.\n- **Interpretation:** The sketched solution $x_S$ perfectly fits the sketched data, but performs poorly on the full data. This is a clear case of overfitting. The reason given is that the residual direction is annihilated by $S$. Let's check:\n$$\nS r_S = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\end{bmatrix} \\begin{bmatrix} 0 \\\\ 0 \\\\ 2 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}.\n$$\nThe residual vector $r_S$ is indeed in the nullspace of the sketching matrix $S$. The sketch is \"blind\" to this error component. The entire statement is a correct and insightful analysis of the constructed example.\n- **Verdict:** **Correct**.\n\n**B.** If one reweights sketched rows by inverse sampling probabilities and uses leverage-score sampling, it eliminates the possibility of overfitting to the sketch in all cases.\n\n- Leverage-score sampling is a sophisticated randomized method designed to give more importance to \"influential\" rows, making the kind of pathological error seen in the example much less likely. The sampling probabilities are chosen to be proportional to statistical leverage scores of the rows of $A$.\n- However, the claim is that this method \"eliminates the possibility of overfitting... in all cases.\" Randomized algorithms provide probabilistic guarantees, not deterministic ones. For any given sketch size $s < m$, there is always a non-zero probability (however small) of selecting a \"bad\" set of rows that leads to a poor-quality solution. The guarantees are of the form \"with high probability, the solution quality is bounded.\" The phrase \"in all cases\" makes the statement absolute and therefore false.\n- **Verdict:** **Incorrect**.\n\n**C.** Using an independent holdout sketch $T \\in \\mathbb{R}^{t \\times m}$ to validate by checking $\\|T(A x_{S} - b)\\|_{2}$ provides a probabilistic safeguard against overfitting: if $T$ is a subspace embedding for $\\mathrm{col}([A \\;\\; b])$, then with high probability $\\|T(A x - b)\\|_{2}$ approximates $\\|A x - b\\|_{2}$ uniformly over $x$.\n\n- This statement describes a validation strategy. After finding a candidate solution $x_S$ with a sketch $S$, its quality (full residual norm) is estimated using a second, independent sketch $T$.\n- The theoretical justification rests on the concept of a subspace embedding. A matrix $T$ is a subspace embedding for a subspace $W$ if it approximately preserves the norms of all vectors in $W$. Formally, with high probability, $(1-\\epsilon)\\|y\\|_2 \\le \\|Ty\\|_2 \\le (1+\\epsilon)\\|y\\|_2$ for all $y \\in W$.\n- The set of all possible residual vectors $\\{A x - b : x \\in \\mathbb{R}^{n}\\}$ is a subset of the affine subspace $b + \\mathrm{col}(A)$. All such vectors lie in the span of the columns of $A$ and the vector $b$, which is $\\mathrm{col}([A \\;\\; b])$. Let $W = \\mathrm{col}([A \\;\\; b])$.\n- If $T$ is a subspace embedding for $W$, then with high probability, it will approximately preserve the norms of all vectors in $W$. This includes all residual vectors $A x - b$. The guarantee is \"uniform\" because the same constants $1 \\pm \\epsilon$ apply to all vectors in the subspace simultaneously.\n- Therefore, checking $\\|T(A x_S - b)\\|_2$ would provide a reliable estimate of the true residual norm $\\|A x_S - b\\|_2$, exposing the overfitting if $S$ was a pathological sketch. This is a standard and sound technique.\n- **Verdict:** **Correct**.\n\n**D.** Increasing the sketch size $s$ cannot help in this example because the residual direction lives entirely in the nullspace of $A$.\n\n- Let's analyze the premise: \"the residual direction lives entirely in the nullspace of $A$.\" The residual vector at the sketched solution is $r_S = \\begin{bmatrix} 0 & 0 & 2 \\end{bmatrix}^{\\top}$. The nullspace of $A$, $N(A)$, contains vectors $z$ such that $Az=0$. Since the columns of $A$ are linearly independent, $N(A) = \\{0\\}$. Clearly, $r_S \\neq 0$, so $r_S \\notin N(A)$. The premise is false. Perhaps the nullspace of $A^\\top$ was intended. The residual of the *optimal* solution, $r^\\star = A x^\\star - b$, must be orthogonal to $\\mathrm{col}(A)$, i.e., $A^\\top r^\\star = 0$. However, the problematic residual here is $r_S$, and $A^\\top r_S = \\begin{bmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\end{bmatrix} \\begin{bmatrix} 0 \\\\ 0 \\\\ 2 \\end{bmatrix} = \\begin{bmatrix} 2 \\\\ 2 \\end{bmatrix} \\neq 0$. So the premise is false in either interpretation.\n- Let's analyze the conclusion: \"Increasing the sketch size $s$ cannot help in this example.\" In the example, $m=3$ and a sketch of size $s=2$ is used. If we increase the sketch size to $s=3$, any full-rank $3 \\times 3$ sketch $S$ would make the sketched problem equivalent to the original problem (up to row scaling), yielding the true solution $x^\\star$. For example, if $S=I_3$, the sketched problem is identical to the full problem. Therefore, increasing the sketch size can certainly help. The conclusion is also false.\n- **Verdict:** **Incorrect**.\n\n**E.** Using ridge regularization in the sketched solve, that is, minimizing $\\|S A x - S b\\|_{2}^{2} + \\lambda \\|x\\|_{2}^{2}$ for some $\\lambda > 0$, necessarily yields a smaller full residual than the exact full least squares solution $x^{\\star}$.\n\n- The exact full least squares solution $x^\\star$ is, by definition, the vector that achieves the minimum possible value of $\\|Ax - b\\|_2$. No other vector $x$ can produce a strictly smaller residual norm $\\|Ax - b\\|_2$.\n- Let $x_\\lambda$ be the solution to the regularized sketched problem. The statement is that $\\|A x_\\lambda - b\\|_2 < \\|A x^\\star - b\\|_2$ for some $\\lambda > 0$. This is a mathematical impossibility by the definition of $x^\\star$. At best, we could have $\\|A x_\\lambda - b\\|_2 = \\|A x^\\star - b\\|_2$, which would happen if and only if $x_\\lambda$ is also a least squares solution. In our specific example where $A$ has full column rank, this means we would need $x_\\lambda = x^\\star$.\n- In this specific case, for $\\lambda=2$, the regularized sketched solution is $x_2 = [1/3, 1/3]^\\top = x^\\star$, achieving equality. For any other $\\lambda > 0$, $x_\\lambda \\neq x^\\star$, and therefore $\\|A x_\\lambda - b\\|_2 > \\|A x^\\star - b\\|_2$.\n- The word \"smaller\" implies a strict inequality, which is impossible.\n- **Verdict:** **Incorrect**.\n\n**F.** If the sketch $S$ is formed by sampling row $i$ with probability $p_{i}$ and rescaling that row by $1/\\sqrt{s p_{i}}$ over $s$ independent samples with replacement, then for any $y \\in \\mathbb{R}^{m}$ one has $\\mathbb{E}[\\|S y\\|_{2}^{2}] = \\|y\\|_{2}^{2}$. Applied to $y = A x - b$, such reweighting removes bias in the sketched residual in expectation, though a single realization can still overfit.\n\n- This describes a common randomized sketching method. Let's verify the expectation. Let $\\|Sy\\|_2^2 = \\sum_{j=1}^s (S_j y)^2$, where $S_j$ are the rows of $S$. Each $S_j$ is formed by choosing row $k$ of the identity matrix with probability $p_k$ and scaling it by $1/\\sqrt{sp_k}$. So $S_j = \\frac{1}{\\sqrt{sp_k}} e_k^\\top$ with probability $p_k$.\n- The expected value of $(S_j y)^2$ is:\n$$\n\\mathbb{E}[(S_j y)^2] = \\sum_{k=1}^m p_k \\left( \\frac{1}{\\sqrt{sp_k}} e_k^\\top y \\right)^2 = \\sum_{k=1}^m p_k \\frac{y_k^2}{sp_k} = \\frac{1}{s} \\sum_{k=1}^m y_k^2 = \\frac{1}{s}\\|y\\|_2^2.\n$$\n- Since the $s$ samples are independent and identically distributed, by linearity of expectation:\n$$\n\\mathbb{E}[\\|Sy\\|_2^2] = \\sum_{j=1}^s \\mathbb{E}[(S_j y)^2] = \\sum_{j=1}^s \\frac{1}{s}\\|y\\|_2^2 = s \\left( \\frac{1}{s}\\|y\\|_2^2 \\right) = \\|y\\|_2^2.\n$$\n- The first part of the statement is correct. This property is known as having an unbiased estimator for the squared norm, as $\\mathbb{E}[y^\\top S^\\top S y] = y^\\top \\mathbb{E}[S^\\top S] y = y^\\top I y = \\|y\\|_2^2$.\n- Applying this to $y = Ax - b$, we get $\\mathbb{E}[\\|S(Ax-b)\\|_2^2] = \\|Ax-b\\|_2^2$. This means the sketched objective function is an unbiased estimator of the true objective function. This \"removes bias... in expectation.\"\n- The final clause, \"though a single realization can still overfit,\" is a crucial caveat. An expectation is an average over all possible outcomes of the random sampling. Any single outcome (realization) could be unlucky and result in a poor sketch, just like the deterministic one in the problem setup.\n- The entire statement is a correct description of the properties of this sketching method.\n- **Verdict:** **Correct**.", "answer": "$$\\boxed{ACF}$$", "id": "3570209"}]}