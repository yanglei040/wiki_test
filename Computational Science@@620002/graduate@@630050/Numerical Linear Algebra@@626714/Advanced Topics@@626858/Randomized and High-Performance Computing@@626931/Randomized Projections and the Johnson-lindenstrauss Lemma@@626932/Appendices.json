{"hands_on_practices": [{"introduction": "The Johnson-Lindenstrauss lemma often relies on projecting data with a random orthogonal matrix. This exercise explores the profound connection between the multivariate normal distribution and the Haar measure, which represents the uniform distribution over the group of orthogonal matrices. By demonstrating that the orthogonal factor from the QR decomposition of a Gaussian matrix is Haar-distributed, you will solidify your understanding of the theoretical underpinnings that provide a practical and perfect method for generating ideal random rotations [@problem_id:3570509].", "problem": "Consider a Johnson–Lindenstrauss embedding defined by the mapping $F_{W} : \\mathbb{R}^{d} \\to \\mathbb{R}^{k}$ given by $F_{W}(x) = \\sqrt{\\frac{d}{k}} S W x$, where $S \\in \\mathbb{R}^{k \\times d}$ is the coordinate projector selecting the first $k$ coordinates, and $W \\in \\mathrm{O}(d)$ is an orthogonal matrix. Two constructions of $W$ are considered:\n- A matrix $U$ drawn from the Haar measure on $\\mathrm{O}(d)$, where the Haar measure is the unique left-invariant probability measure on the compact topological group $\\mathrm{O}(d)$.\n- A matrix $Q$ obtained by the $\\mathrm{QR}$ factorization of a random Gaussian matrix $G \\in \\mathbb{R}^{d \\times d}$ with independent entries $G_{ij} \\sim \\mathcal{N}(0,1)$, i.e., $G = QR$, with $R$ upper triangular and with strictly positive diagonal entries by convention.\n\nLet $v \\in \\mathbb{R}^{d}$ be a fixed nonzero vector and define the squared distortion ratio $D_{W}$ of the mapping $F_{W}$ for the difference $v$ as $D_{W} = \\frac{\\|F_{W}(v)\\|_{2}^{2}}{\\|v\\|_{2}^{2}}$. Let $\\mathcal{L}(Q)$ denote the distribution of $Q$, and let $\\mu_{\\mathrm{Haar}}$ denote the Haar probability measure on $\\mathrm{O}(d)$. Define the total variation deviation from Haar as $\\Delta = \\sup_{B \\subset \\mathrm{O}(d)} \\left| \\mathbb{P}(Q \\in B) - \\mu_{\\mathrm{Haar}}(B) \\right|$, where the supremum is taken over Borel subsets $B$ of $\\mathrm{O}(d)$. Define the expected distortion bias between the two constructions as $\\beta = \\mathbb{E}[D_{Q}] - \\mathbb{E}[D_{U}]$.\n\nStarting from the rotational invariance of the multivariate normal distribution and the defining properties of the Haar measure, derive expressions for $\\Delta$ and $\\beta$ in closed form. Provide your final answer as a row matrix containing $\\Delta$ and $\\beta$ in that order. No rounding is required. No physical units are involved in this problem.", "solution": "The user wants me to validate the problem statement and, if it is valid, provide a solution.\n\n### Step 1: Extract Givens\n- **Mapping:** $F_{W} : \\mathbb{R}^{d} \\to \\mathbb{R}^{k}$ defined by $F_{W}(x) = \\sqrt{\\frac{d}{k}} S W x$.\n- **$S$:** A $k \\times d$ matrix representing the coordinate projector that selects the first $k$ coordinates.\n- **$W$:** An orthogonal matrix, $W \\in \\mathrm{O}(d)$.\n- **Construction U:** A matrix $U$ drawn from the Haar measure, $\\mu_{\\mathrm{Haar}}$, on the orthogonal group $\\mathrm{O}(d)$.\n- **Construction Q:** A matrix $Q$ obtained from the $\\mathrm{QR}$ factorization of a random Gaussian matrix $G \\in \\mathbb{R}^{d \\times d}$ with independent and identically distributed entries $G_{ij} \\sim \\mathcal{N}(0,1)$. The factorization is $G = QR$, where $R$ is upper triangular with strictly positive diagonal entries.\n- **Vector $v$:** A fixed nonzero vector in $\\mathbb{R}^{d}$.\n- **Squared Distortion Ratio:** $D_{W} = \\frac{\\|F_{W}(v)\\|_{2}^{2}}{\\|v\\|_{2}^{2}}$.\n- **Total Variation Deviation:** $\\Delta = \\sup_{B \\subset \\mathrm{O}(d)} \\left| \\mathbb{P}(Q \\in B) - \\mu_{\\mathrm{Haar}}(B) \\right|$, where the supremum is over all Borel subsets $B$ of $\\mathrm{O}(d)$.\n- **Expected Distortion Bias:** $\\beta = \\mathbb{E}[D_{Q}] - \\mathbb{E}[D_{U}]$.\n- **Task:** Derive expressions for $\\Delta$ and $\\beta$ in closed form, starting from the rotational invariance of the multivariate normal distribution and the properties of the Haar measure.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is well-defined within the mathematical field of random matrix theory and numerical linear algebra.\n- **Scientifically Grounded:** The concepts presented—Haar measure on $\\mathrm{O}(d)$, QR factorization of Gaussian matrices, total variation distance, and Johnson-Lindenstrauss embeddings—are all standard and rigorously defined mathematical objects and theories. The premise that the distribution of $Q$ is related to the Haar measure is a cornerstone result in random matrix theory.\n- **Well-Posed:** The problem asks for the calculation of two well-defined quantities, $\\Delta$ and $\\beta$. The instructions to start from specific principles (rotational invariance) provide a clear path for the derivation.\n- **Objective:** The problem statement is composed of formal mathematical definitions and is free of ambiguity or subjective language.\n- **Completeness and Consistency:** All necessary definitions are provided. The specification that $R$ has strictly positive diagonal entries ensures the uniqueness of the QR decomposition, which is a crucial detail for the argument.\n\nThe problem is not flawed. It presents a scenario that, while having a simple final answer, requires a rigorous derivation based on fundamental principles, testing the understanding of the relationship between Gaussian random matrices and the Haar measure. The setup is not trivial or pseudo-profound but rather a standard way to frame a proof-based question in this domain.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A full derivation shall be provided.\n\n### Derivation\n\nThe solution requires deriving the values of $\\Delta$ and $\\beta$. This hinges on determining the probability distribution of the matrix $Q$.\n\n**Part 1: Derivation of $\\Delta$**\n\nThe quantity $\\Delta$ is the total variation distance between the distribution of $Q$ and the Haar measure on $\\mathrm{O}(d)$. We will show that these two distributions are identical, which implies $\\Delta=0$.\n\nLet $G$ be a $d \\times d$ matrix with entries $G_{ij}$ being independent and identically distributed standard normal random variables, $G_{ij} \\sim \\mathcal{N}(0,1)$. The joint probability density function of the elements of $G$ is given by\n$$p(G) = \\prod_{i,j=1}^{d} \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{G_{ij}^2}{2}\\right) = (2\\pi)^{-d^2/2} \\exp\\left(-\\frac{1}{2} \\sum_{i,j=1}^{d} G_{ij}^2\\right)$$\nThis can be expressed using the Frobenius norm or trace as:\n$$p(G) = (2\\pi)^{-d^2/2} \\exp\\left(-\\frac{1}{2} \\|G\\|_{\\mathrm{F}}^2\\right) = (2\\pi)^{-d^2/2} \\exp\\left(-\\frac{1}{2} \\mathrm{Tr}(G^T G)\\right)$$\nA key property of this distribution is its rotational invariance. Let $H \\in \\mathrm{O}(d)$ be any fixed orthogonal matrix. Consider the random matrix $G' = HG$. The trace term in the exponent transforms as:\n$$\\mathrm{Tr}((HG)^T(HG)) = \\mathrm{Tr}(G^T H^T H G) = \\mathrm{Tr}(G^T I G) = \\mathrm{Tr}(G^T G)$$\nwhere we have used the property $H^T H = I$ for an orthogonal matrix. Since the transformation $G \\to HG$ is a linear transformation, the change of variables has a constant Jacobian, $| \\det(H) |^d = (\\pm 1)^d = 1$. Thus, the probability density of $G'$ is the same as that of $G$. This means the distribution of $G$ is left-invariant under orthogonal transformations: for any fixed $H \\in \\mathrm{O}(d)$, the random matrix $HG$ has the same distribution as $G$. We denote this as $HG \\sim G$.\n\nNow, let $G = QR$ be the unique QR factorization of $G$, where $Q \\in \\mathrm{O}(d)$ and $R$ is an upper-triangular matrix with strictly positive diagonal entries. This decomposition is unique for almost all matrices $G$ (specifically, for any invertible $G$).\nSince $HG \\sim G$, the unique QR factors of $HG$ must have the same joint distribution as the QR factors of $G$. Let the QR factorization of $HG$ be $HG = Q'R'$. Then we have $(Q', R') \\sim (Q, R)$.\n\nWe can also find the relationship between $(Q', R')$ and $(Q, R)$ algebraically. We have $HG = (HQ)R$. Since $H$ and $Q$ are both in $\\mathrm{O}(d)$, their product $HQ$ is also in $\\mathrm{O}(d)$. The matrix $R$ is upper-triangular with strictly positive diagonal entries. Thus, $(HQ)R$ is a valid QR factorization of the matrix $HG$. By the uniqueness of this decomposition, we must have:\n$$Q' = HQ \\quad \\text{and} \\quad R' = R$$\nCombining the distributional equality $(Q', R') \\sim (Q, R)$ with the algebraic identity $Q' = HQ$, we deduce that the distribution of $Q$ must satisfy:\n$$Q \\sim HQ \\quad \\text{for all fixed } H \\in \\mathrm{O}(d)$$\nThis property is the definition of left-invariance for the probability distribution of the random matrix $Q$. The Haar measure, $\\mu_{\\mathrm{Haar}}$, on a compact topological group such as $\\mathrm{O}(d)$ is the unique left-invariant probability measure. Therefore, the distribution of $Q$, which we may denote $\\mathcal{L}(Q)$, must be the Haar measure:\n$$\\mathcal{L}(Q) = \\mu_{\\mathrm{Haar}}$$\nNow we can compute $\\Delta$. By definition,\n$$\\Delta = \\sup_{B \\subset \\mathrm{O}(d)} \\left| \\mathbb{P}(Q \\in B) - \\mu_{\\mathrm{Haar}}(B) \\right|$$\nSince the probability measure governing $Q$ is precisely $\\mu_{\\mathrm{Haar}}$, we have $\\mathbb{P}(Q \\in B) = \\mu_{\\mathrm{Haar}}(B)$ for any Borel set $B$.\n$$\\Delta = \\sup_{B \\subset \\mathrm{O}(d)} \\left| \\mu_{\\mathrm{Haar}}(B) - \\mu_{\\mathrm{Haar}}(B) \\right| = \\sup_{B \\subset \\mathrm{O}(d)} |0| = 0$$\n\n**Part 2: Derivation of $\\beta$**\n\nThe expected distortion bias is defined as $\\beta = \\mathbb{E}[D_{Q}] - \\mathbb{E}[D_{U}]$.\nThe random matrix $U$ is, by definition, drawn from the Haar measure $\\mu_{\\mathrm{Haar}}$. In Part 1, we proved that the random matrix $Q$ also follows the Haar measure, $\\mathcal{L}(Q) = \\mu_{\\mathrm{Haar}}$.\nThis means that $Q$ and $U$ are identically distributed random matrices.\n\nThe squared distortion ratio $D_W$ is a function of the matrix $W$:\n$$D_{W} = \\frac{\\|F_{W}(v)\\|_{2}^{2}}{\\|v\\|_{2}^{2}}$$\nIf two random variables $X$ and $Y$ are identically distributed, then for any well-defined function $g$, the expected values of $g(X)$ and $g(Y)$ are equal: $\\mathbb{E}[g(X)] = \\mathbb{E}[g(Y)]$.\nIn our case, the random variables are the matrices $Q$ and $U$, and the function is $D_W$. Since $Q \\sim U$, it follows that:\n$$\\mathbb{E}[D_{Q}] = \\mathbb{E}[D_{U}]$$\nTherefore, the expected distortion bias $\\beta$ is:\n$$\\beta = \\mathbb{E}[D_{Q}] - \\mathbb{E}[D_{U}] = 0$$\n\nFor completeness, we can also calculate the value of these expectations. Let $W$ be a Haar-distributed orthogonal matrix.\n$$\\mathbb{E}[D_W] = \\mathbb{E}\\left[\\frac{\\|\\sqrt{\\frac{d}{k}} S W v\\|_{2}^{2}}{\\|v\\|_{2}^{2}}\\right] = \\frac{d}{k \\|v\\|_{2}^{2}} \\mathbb{E}[\\|S W v\\|_{2}^{2}]$$\nLet the vector $y = Wv$. Since $W$ is Haar-distributed and orthogonal, and $v$ is a fixed vector, the vector $y$ is uniformly distributed on the sphere of radius $\\|v\\|_2$ in $\\mathbb{R}^d$. This is a consequence of rotational symmetry. Let $y = (y_1, y_2, \\dots, y_d)^T$.\nWe have $\\|y\\|_2^2 = \\|Wv\\|_2^2 = v^T W^T W v = v^T v = \\|v\\|_2^2$.\nThe matrix $S$ selects the first $k$ coordinates, so $\\|Sy\\|_2^2 = \\sum_{i=1}^k y_i^2$.\nBy the symmetry of the uniform distribution on the sphere, the expected value of the square of each component is the same: $\\mathbb{E}[y_i^2] = c$ for all $i=1, \\dots, d$.\nWe have $\\sum_{i=1}^d \\mathbb{E}[y_i^2] = \\mathbb{E}\\left[\\sum_{i=1}^d y_i^2\\right] = \\mathbb{E}[\\|y\\|_2^2] = \\mathbb{E}[\\|v\\|_2^2] = \\|v\\|_2^2$.\nSo, $d \\cdot c = \\|v\\|_2^2$, which gives $c = \\frac{\\|v\\|_2^2}{d}$.\nThen, the expectation we need is:\n$$\\mathbb{E}[\\|S W v\\|_{2}^{2}] = \\mathbb{E}[\\|Sy\\|_2^2] = \\mathbb{E}\\left[\\sum_{i=1}^k y_i^2\\right] = \\sum_{i=1}^k \\mathbb{E}[y_i^2] = k \\cdot c = k \\frac{\\|v\\|_2^2}{d}$$\nSubstituting this back into the expression for $\\mathbb{E}[D_W]$:\n$$\\mathbb{E}[D_W] = \\frac{d}{k \\|v\\|_{2}^{2}} \\left( k \\frac{\\|v\\|_2^2}{d} \\right) = 1$$\nThus, $\\mathbb{E}[D_Q] = \\mathbb{E}[D_U] = 1$, which confirms that their difference $\\beta$ is $0$.\n\n**Conclusion:**\nThe total variation deviation $\\Delta$ is $0$ because the distribution of the orthogonal factor $Q$ from the QR factorization of a standard Gaussian matrix is precisely the Haar measure on $\\mathrm{O}(d)$. The expected distortion bias $\\beta$ is $0$ because the random matrices $Q$ and $U$ are identically distributed, leading to identical expectations for any function of these matrices, including the distortion ratio.\n\nFinal answer will be the row matrix containing $\\Delta$ and $\\beta$.", "answer": "$$\\boxed{\\begin{pmatrix} 0 & 0 \\end{pmatrix}}$$", "id": "3570509"}, {"introduction": "While generating a Haar-distributed matrix via QR factorization is theoretically sound, it can be computationally intensive for very large dimensions. This practice delves into constructing random rotations by composing a chain of simple, efficient Householder reflections. You will be challenged to connect the length of this chain—a proxy for computational effort—to the quality of the resulting Johnson-Lindenstrauss embedding, deriving how convergence to a truly random rotation impacts the distortion guarantee [@problem_id:3570507].", "problem": "Let $d \\in \\mathbb{N}$ and $1 \\leq k \\leq d-1$. For a unit vector $u \\in \\mathbb{R}^{d}$, define the Householder reflection $H(u) \\in \\mathbb{R}^{d \\times d}$ by $H(u) = I_{d} - 2 u u^{\\top}$. Consider a random orthogonal transformation obtained as a product of $L \\in \\mathbb{N}$ independent Householder reflections with independent unit normals, $Q_{L} = H(u_{L}) H(u_{L-1}) \\cdots H(u_{1})$, where each $u_{i}$ is drawn independently and uniformly from the unit sphere $\\mathbb{S}^{d-1}$. Define the orthogonal Johnson–Lindenstrauss (JL) map $T : \\mathbb{R}^{d} \\to \\mathbb{R}^{k}$ by\n$T = \\sqrt{\\frac{d}{k}} \\, P_{k} \\, Q_{L}$,\nwhere $P_{k} \\in \\mathbb{R}^{k \\times d}$ is the projection onto the first $k$ coordinates.\n\nLet $S \\subset \\mathbb{R}^{d}$ be a finite set with $|S| = m \\geq 2$, and let $\\Delta(T; S)$ be the distortion of $T$ on $S$ defined by\n$\\Delta(T; S) = \\sup_{\\substack{x,y \\in S \\\\ x \\neq y}} \\frac{\\left| \\|T(x-y)\\|^{2} - \\|x-y\\|^{2} \\right|}{\\|x-y\\|^{2}}$.\nWe say that $T$ achieves $(\\varepsilon,\\delta)$-distortion for $S$ if $\\mathbb{P}\\big( \\Delta(T; S) \\leq \\varepsilon \\big) \\geq 1 - \\delta$, with $\\varepsilon \\in (0,1)$ and $\\delta \\in (0,1/2)$.\n\nRandomness quality is measured via the following moment conditions for the random vector $y = Q_{L} x$ for any fixed unit vector $x \\in \\mathbb{R}^{d}$ and any coordinate index $j \\in \\{1,\\dots,d\\}$:\n$$\n\\left| \\mathbb{E}[y_{j}^{2}] - \\frac{1}{d} \\right| \\leq \\alpha \\exp(-\\lambda L), \\qquad \\left| \\mathbb{E}[y_{j}^{4}] - \\frac{3}{d(d+2)} \\right| \\leq \\beta \\exp(-\\lambda L),\n$$\nfor given nonnegative constants $\\alpha, \\beta$ and a mixing rate parameter $\\lambda > 0$. In addition, assume there exists a universal constant $C > 0$ (independent of $d,k$) such that the fourth centered moment of the projection energy\n$W = \\|P_{k} y\\|^{2} - \\frac{k}{d}$\nsatisfies\n$\\mathbb{E}\\big[ W^{4} \\big] \\leq \\mathbb{E}_{\\mathrm{Haar}}\\big[ W^{4} \\big] + C \\exp(-\\lambda L)$,\nwhere $\\mathbb{E}_{\\mathrm{Haar}}[\\cdot]$ denotes expectation when $Q_{L}$ is Haar-distributed on the orthogonal group (i.e., as $L \\to \\infty$), and moreover there is a known bound\n$\\mathbb{E}_{\\mathrm{Haar}}\\big[ W^{4} \\big] \\leq c_{H} \\frac{k^{2}}{d^{4}}$\nfor some universal constant $c_{H} > 0$.\n\nLet $N = \\binom{m}{2}$. Suppose $k$ has been chosen to satisfy\n$\\frac{N \\, c_{H}}{\\varepsilon^{4} \\, k^{2}} \\leq \\frac{\\delta}{2}$.\nUsing only the above moment conditions, standard inequalities, and union bounding over the $N$ pairwise differences in $S$, derive an explicit closed-form analytic expression for the minimal chain length $L_{\\min}$ (as a function of $d$, $k$, $m$, $\\varepsilon$, $\\delta$, $\\lambda$, $c_{H}$, and $C$) sufficient to guarantee that $T$ achieves $(\\varepsilon,\\delta)$-distortion for $S$.\n\nYour final answer must be a single analytic expression for $L_{\\min}$. No numerical evaluation or rounding is required. Define Johnson–Lindenstrauss (JL) on its first appearance, and write all mathematical entities in LaTeX.", "solution": "The problem requires the derivation of the minimal number of Householder reflections, denoted $L_{\\min}$, for a randomly constructed orthogonal Johnson-Lindenstrauss (JL) map to satisfy a prescribed distortion guarantee with high probability. A JL map is a low-dimensional embedding that approximately preserves pairwise distances for a finite set of points, a cornerstone result in high-dimensional data analysis.\n\nThe condition for the map $T$ to achieve $(\\varepsilon, \\delta)$-distortion for the set $S$ is given by $\\mathbb{P}\\big( \\Delta(T; S) \\leq \\varepsilon \\big) \\geq 1 - \\delta$. This is equivalent to bounding the probability of the failure event, $\\mathbb{P}\\big( \\Delta(T; S) > \\varepsilon \\big) \\leq \\delta$.\n\nThe distortion $\\Delta(T; S)$ is defined as\n$$\n\\Delta(T; S) = \\sup_{\\substack{x,y \\in S \\\\ x \\neq y}} \\frac{\\left| \\|T(x-y)\\|^{2} - \\|x-y\\|^{2} \\right|}{\\|x-y\\|^{2}}.\n$$\nLet $z = x-y$ for any distinct pair $x, y \\in S$. Let $v = \\frac{z}{\\|z\\|}$ be the corresponding normalized vector. The term inside the supremum can be written as $\\left| \\|T v\\|^{2} - 1 \\right|$.\nThe map $T$ is given by $T = \\sqrt{\\frac{d}{k}} P_{k} Q_{L}$. For a unit vector $v$, we have:\n$$\n\\|T v\\|^{2} = \\left\\| \\sqrt{\\frac{d}{k}} P_{k} Q_{L} v \\right\\|^{2} = \\frac{d}{k} \\|P_{k} (Q_{L} v)\\|^{2}.\n$$\nLet $y = Q_{L} v$. Since $Q_{L}$ is an orthogonal matrix and $v$ is a unit vector, $y$ is also a unit vector. The problem defines the quantity $W = \\|P_{k} y\\|^{2} - \\frac{k}{d}$. Using this definition, we can express $\\|T v\\|^{2}$ as:\n$$\n\\|T v\\|^{2} = \\frac{d}{k} \\left( W + \\frac{k}{d} \\right) = \\frac{d}{k} W + 1.\n$$\nThe distortion condition $\\left| \\|T v\\|^{2} - 1 \\right| \\leq \\varepsilon$ thus becomes $\\left| \\frac{d}{k} W \\right| \\leq \\varepsilon$, which is equivalent to $|W| \\leq \\frac{k \\varepsilon}{d}$.\n\nThe failure event $\\Delta(T; S) > \\varepsilon$ occurs if there exists at least one pair $(x, y)$ such that the corresponding $|W_v|$ exceeds this bound, where $v=(x-y)/\\|x-y\\|$. Let $V = \\{ (x-y)/\\|x-y\\| : x, y \\in S, x \\neq y \\}$ be the set of normalized difference vectors. The number of such distinct pairs is $N = \\binom{m}{2}$. The failure event can be written as $\\sup_{v \\in V} |W_v| > \\frac{k \\varepsilon}{d}$.\n\nWe can bound the probability of this event using the union bound:\n$$\n\\mathbb{P}\\left( \\Delta(T; S) > \\varepsilon \\right) = \\mathbb{P}\\left( \\bigcup_{v \\in V} \\left\\{ |W_v| > \\frac{k \\varepsilon}{d} \\right\\} \\right) \\leq \\sum_{v \\in V} \\mathbb{P}\\left( |W_v| > \\frac{k \\varepsilon}{d} \\right).\n$$\nThe distribution of the random matrix $Q_L$ is constructed from independent, uniformly-drawn vectors $u_i$. This construction results in a distribution for $Q_L$ that is invariant under orthogonal conjugation. Consequently, the distribution of $y = Q_L v$ is the same for any fixed unit vector $v$. Therefore, the probability $\\mathbb{P}\\left( |W_v| > \\frac{k \\varepsilon}{d} \\right)$ is identical for all $v \\in V$. We denote this common probability by $\\mathbb{P}\\left( |W| > \\frac{k \\varepsilon}{d} \\right)$. Since $|V| \\leq N$, we have:\n$$\n\\mathbb{P}\\left( \\Delta(T; S) > \\varepsilon \\right) \\leq N \\cdot \\mathbb{P}\\left( |W| > \\frac{k \\varepsilon}{d} \\right).\n$$\nTo bound the single-pair probability, we use Markov's inequality. For a non-negative random variable $X$ and $a > 0$, $\\mathbb{P}(X \\geq a) \\leq \\mathbb{E}[X]/a$. Applying this to $X = W^4$ and $a = \\left(\\frac{k \\varepsilon}{d}\\right)^4$:\n$$\n\\mathbb{P}\\left( |W| > \\frac{k \\varepsilon}{d} \\right) = \\mathbb{P}\\left( W^4 > \\left(\\frac{k \\varepsilon}{d}\\right)^4 \\right) \\leq \\frac{\\mathbb{E}[W^4]}{\\left(\\frac{k \\varepsilon}{d}\\right)^4} = \\frac{d^4 \\mathbb{E}[W^4]}{k^4 \\varepsilon^4}.\n$$\nThe problem provides a bound on the fourth moment of $W$:\n$$\n\\mathbb{E}[W^4] \\leq \\mathbb{E}_{\\mathrm{Haar}}[W^4] + C \\exp(-\\lambda L),\n$$\nand a bound on the Haar-averaged moment:\n$$\n\\mathbb{E}_{\\mathrm{Haar}}[W^4] \\leq c_{H} \\frac{k^2}{d^4}.\n$$\nSubstituting these into our inequality for $\\mathbb{E}[W^4]$ yields:\n$$\n\\mathbb{E}[W^4] \\leq c_{H} \\frac{k^2}{d^4} + C \\exp(-\\lambda L).\n$$\nPlugging this into the bound for the single-pair probability:\n$$\n\\mathbb{P}\\left( |W| > \\frac{k \\varepsilon}{d} \\right) \\leq \\frac{d^4}{k^4 \\varepsilon^4} \\left( c_{H} \\frac{k^2}{d^4} + C \\exp(-\\lambda L) \\right) = \\frac{c_{H}}{k^2 \\varepsilon^4} + \\frac{C d^4 \\exp(-\\lambda L)}{k^4 \\varepsilon^4}.\n$$\nNow, we return to the union bound, substituting this expression:\n$$\n\\mathbb{P}\\left( \\Delta(T; S) > \\varepsilon \\right) \\leq N \\left( \\frac{c_{H}}{k^2 \\varepsilon^4} + \\frac{C d^4 \\exp(-\\lambda L)}{k^4 \\varepsilon^4} \\right) = \\frac{N c_{H}}{k^2 \\varepsilon^4} + \\frac{N C d^4 \\exp(-\\lambda L)}{k^4 \\varepsilon^4}.\n$$\nWe require this total failure probability to be at most $\\delta$. The problem provides that $k$ is chosen such that the first term is bounded by $\\frac{\\delta}{2}$:\n$$\n\\frac{N c_{H}}{\\varepsilon^4 k^2} \\leq \\frac{\\delta}{2}.\n$$\nTo satisfy the overall constraint $\\mathbb{P}(\\Delta(T; S) > \\varepsilon) \\leq \\delta$, we must ensure the second term is also bounded by $\\frac{\\delta}{2}$:\n$$\n\\frac{N C d^4 \\exp(-\\lambda L)}{k^4 \\varepsilon^4} \\leq \\frac{\\delta}{2}.\n$$\nWe now solve this inequality for the chain length $L$.\n$$\n\\exp(-\\lambda L) \\leq \\frac{\\delta k^4 \\varepsilon^4}{2 N C d^4}.\n$$\nTaking the natural logarithm of both sides:\n$$\n-\\lambda L \\leq \\ln\\left( \\frac{\\delta k^4 \\varepsilon^4}{2 N C d^4} \\right).\n$$\nSince $\\lambda > 0$, we divide by $-\\lambda$ and reverse the inequality sign:\n$$\nL \\geq -\\frac{1}{\\lambda} \\ln\\left( \\frac{\\delta k^4 \\varepsilon^4}{2 N C d^4} \\right) = \\frac{1}{\\lambda} \\ln\\left( \\left(\\frac{\\delta k^4 \\varepsilon^4}{2 N C d^4}\\right)^{-1} \\right) = \\frac{1}{\\lambda} \\ln\\left( \\frac{2 N C d^4}{\\delta k^4 \\varepsilon^4} \\right).\n$$\nThe minimal integer $L$ must be at least this value. The problem asks for the analytic expression for this lower bound, which we denote $L_{\\min}$. Substituting $N = \\binom{m}{2}$:\n$$\nL_{\\min} = \\frac{1}{\\lambda} \\ln\\left( \\frac{2 C d^4 \\binom{m}{2}}{\\delta k^4 \\varepsilon^4} \\right).\n$$\nThis expression gives the sufficient chain length as a function of the problem parameters, ensuring the desired $(\\varepsilon, \\delta)$-distortion guarantee. The extraneous moment conditions on $\\mathbb{E}[y_j^2]$ and $\\mathbb{E}[y_j^4]$ and the constant $c_H$ are subsumed by the problem's direct assumptions on $\\mathbb{E}[W^4]$ and the choice of $k$, and thus do not appear in the final expression for $L_{\\min}$.", "answer": "$$\\boxed{\\frac{1}{\\lambda} \\ln\\left(\\frac{2 C d^{4} \\binom{m}{2}}{\\delta k^{4} \\varepsilon^{4}}\\right)}$$", "id": "3570507"}, {"introduction": "Dense random rotations are not the only tool for dimensionality reduction; extremely fast, sparse projections are often used in practice, but their properties differ significantly. This problem investigates the behavior of sparse, CountSketch-style projections by having you construct an adversarial dataset designed to challenge the map's structure. By analyzing this worst-case scenario, you will determine the minimum sparsity required for a low-distortion embedding, revealing the inherent trade-offs between computational efficiency and robustness against structured data [@problem_id:3570496].", "problem": "Let $d \\in \\mathbb{N}$, $m \\in \\mathbb{N}$, and $2 \\leq n \\leq d$. Consider two families of linear maps from $\\mathbb{R}^{d}$ to $\\mathbb{R}^{m}$:\n\n- A sparse CountSketch-style map $R \\in \\mathbb{R}^{m \\times d}$ in which each column has exactly $s \\in \\mathbb{N}$ nonzero entries. For each column $j \\in \\{1,\\dots,d\\}$, the $s$ nonzero row indices are chosen uniformly without replacement from $\\{1,\\dots,m\\}$, independently across columns, and the corresponding values are independent Rademacher signs divided by $\\sqrt{s}$, i.e., each nonzero equals $\\pm 1/\\sqrt{s}$ with probability $1/2$ each.\n\n- A dense Gaussian map $G \\in \\mathbb{R}^{m \\times d}$ with entries $G_{ij} \\sim \\mathcal{N}(0,1/m)$, independent and identically distributed.\n\nDefine the adversarial point set $V \\subset \\mathbb{R}^{d}$ by\n$V \\;=\\; \\{\\, v_{i}^{+}, v_{i}^{-} \\,:\\, i \\in \\{2,\\dots,n\\}\\,\\}$, where $v_{i}^{\\pm} \\;=\\; e_{1} \\pm e_{i}$,\nand $e_{k}$ denotes the $k$-th standard basis vector in $\\mathbb{R}^{d}$.\n\nFor a distortion parameter $\\varepsilon \\in (0,1/2)$, say that a linear map $A:\\mathbb{R}^{d}\\to\\mathbb{R}^{m}$ preserves the squared norms of $V$ within distortion $\\varepsilon$ if for all $v \\in V$,\n$\\left|\\, \\|A v\\|_{2}^{2} - \\|v\\|_{2}^{2} \\,\\right| \\;\\leq\\; \\varepsilon \\,\\|v\\|_{2}^{2}.$\n\nWorking from first principles in probability and linear algebra, and using only well-tested concentration tools (e.g., Chernoff bounds for hypergeometric/binomial variables, Berry–Esseen or Hoeffding for Rademacher sums, and Paley–Zygmund), do the following:\n\n1. Construct and justify an explicit mechanism by which $R$ can incur a larger distortion on $V$ than $G$ at the same embedding dimension $m$, by analyzing the inner products $\\langle R e_{1}, R e_{i} \\rangle$ and their impact on $\\|R v_{i}^{\\pm}\\|_{2}^{2}$ versus the corresponding Gaussian behavior.\n\n2. By quantifying the overlap $K$ between the supports of the $s$ nonzeros in column $1$ and column $i$ of $R$, and the fluctuation of the signed overlap conditional on $K$, derive an asymptotic expression for the minimal column sparsity $s^{\\star} = s^{\\star}(n,m,\\varepsilon)$ such that, for the sparse map $R$, the probability that $R$ preserves the squared norms of all vectors in $V$ within distortion $\\varepsilon$ is at least one half. Your derivation should track exponential-rate terms in $n$, $m$, and $\\varepsilon$, and you may ignore absolute constant factors inside logarithms as long as the final rate is correctly captured.\n\n3. Conclude with a closed-form analytic expression for $s^{\\star}(n,m,\\varepsilon)$ that cleanly separates its dependence on $n$, $m$, and $\\varepsilon$. Your answer must be a single analytic expression in terms of $n$, $m$, and $\\varepsilon$. No numerical evaluation is required.\n\nAnswer format: Provide only the expression for $s^{\\star}(n,m,\\varepsilon)$, fully simplified. No rounding is needed. Do not include units.", "solution": "The problem requires an analysis of a sparse CountSketch-style random projection, $R \\in \\mathbb{R}^{m \\times d}$, and its ability to preserve the norms of a specific set of vectors, $V = \\{e_1 \\pm e_i : i=2,\\dots,n\\}$, compared to a dense Gaussian projection, $G \\in \\mathbb{R}^{m \\times d}$. We are asked to derive the minimal column sparsity $s^{\\star}$ for $R$ to satisfy a given distortion criterion.\n\nFirst, let's establish the norm of the vectors in the set $V$. For any $v_i^{\\pm} = e_1 \\pm e_i$, where $e_k$ is the $k$-th standard basis vector, the squared Euclidean norm is:\n$$\n\\|v_i^{\\pm}\\|_2^2 = \\langle e_1 \\pm e_i, e_1 \\pm e_i \\rangle = \\|e_1\\|_2^2 \\pm 2\\langle e_1, e_i \\rangle + \\|e_i\\|_2^2\n$$\nSince the standard basis vectors are orthonormal, $\\|e_k\\|_2^2 = 1$ for all $k$ and $\\langle e_j, e_k \\rangle = 0$ for $j \\neq k$. Thus,\n$$\n\\|v_i^{\\pm}\\|_2^2 = 1 \\pm 0 + 1 = 2\n$$\nSo, for any vector $v \\in V$, its squared norm is $\\|v\\|_2^2=2$.\n\nThe distortion condition for a linear map $A: \\mathbb{R}^{d} \\to \\mathbb{R}^{m}$ is given as:\n$$\n\\left|\\, \\|A v\\|_{2}^{2} - \\|v\\|_{2}^{2} \\,\\right| \\leq \\varepsilon \\,\\|v\\|_{2}^{2}\n$$\nSubstituting $\\|v\\|_2^2 = 2$, this becomes:\n$$\n\\left|\\, \\|A v\\|_{2}^{2} - 2 \\,\\right| \\leq 2\\varepsilon\n$$\nLet's analyze the term $\\|Av\\|_2^2$ for $v = v_i^{\\pm} = e_1 \\pm e_i$. Let $c_j = Ae_j$ denote the $j$-th column of the matrix $A$. Then $Av = A(e_1 \\pm e_i) = Ae_1 \\pm Ae_i = c_1 \\pm c_i$.\n$$\n\\|Av\\|_2^2 = \\|c_1 \\pm c_i\\|_2^2 = \\|c_1\\|_2^2 + \\|c_i\\|_2^2 \\pm 2\\langle c_1, c_i \\rangle\n$$\nThe distortion condition is therefore:\n$$\n\\left|\\, (\\|c_1\\|_2^2 + \\|c_i\\|_2^2 \\pm 2\\langle c_1, c_i \\rangle) - 2 \\,\\right| \\leq 2\\varepsilon\n$$\n\n**1. Mechanism for Larger Distortion in Sparse Maps**\n\nWe now compare the behavior of this distortion term for the sparse map $R$ and the dense map $G$.\n\nFor the sparse map $R \\in \\mathbb{R}^{m \\times d}$, each column $c_j = Re_j$ has exactly $s$ non-zero entries, each with a value of $\\pm 1/\\sqrt{s}$. The squared norm of any column is therefore fixed:\n$$\n\\|c_j\\|_2^2 = \\sum_{k=1}^{s} \\left(\\pm \\frac{1}{\\sqrt{s}}\\right)^2 = s \\cdot \\frac{1}{s} = 1\n$$\nFor the map $R$, the distortion condition simplifies significantly:\n$$\n\\left|\\, (1 + 1 \\pm 2\\langle Re_1, Re_i \\rangle) - 2 \\,\\right| \\leq 2\\varepsilon\n$$\n$$\n\\left|\\, \\pm 2\\langle Re_1, Re_i \\rangle \\,\\right| \\leq 2\\varepsilon\n$$\n$$\n|\\langle Re_1, Re_i \\rangle| \\leq \\varepsilon\n$$\nFor the sparse map $R$, the preservation of norms for the set $V$ depends exclusively on controlling the magnitude of the inner products between column $1$ and other columns $i \\in \\{2,\\dots,n\\}$.\n\nFor the dense Gaussian map $G \\in \\mathbb{R}^{m \\times d}$, the entries are $G_{kj} \\sim \\mathcal{N}(0, 1/m)$. Let $c_j = Ge_j$. The squared norm $\\|c_j\\|_2^2 = \\sum_{k=1}^m G_{kj}^2$ is a random variable. Its expectation is $E[\\|c_j\\|_2^2] = \\sum_{k=1}^m E[G_{kj}^2] = \\sum_{k=1}^m (1/m) = 1$. The variable $m\\|c_j\\|_2^2$ follows a chi-squared distribution $\\chi_m^2$, which is tightly concentrated around its mean $m$. Similarly, the inner product $\\langle c_1, c_i \\rangle = \\sum_{k=1}^m G_{k1}G_{ki}$ is a sum of i.i.d. random variables with mean $0$ and variance $1/m^2$, so $E[\\langle c_1, c_i \\rangle]=0$ and $\\text{Var}(\\langle c_1, c_i \\rangle) = m \\cdot (1/m^2) = 1/m$. For $G$, the distortion arises from three fluctuating terms: $(\\|c_1\\|_2^2-1)$, $(\\|c_i\\|_2^2-1)$, and $\\langle c_1, c_i \\rangle$. These are all well-behaved, near-Gaussian variables with variances of order $1/m$.\n\nThe explicit mechanism by which $R$ can incur larger distortion lies in the structure of the inner product $\\langle Re_1, Re_i \\rangle$. Let $S_j \\subset \\{1, \\dots, m\\}$ be the set of row indices where column $j$ of $R$ has non-zero entries, so $|S_j|=s$. The inner product is non-zero only on the intersection of these supports:\n$$\n\\langle Re_1, Re_i \\rangle = \\sum_{k \\in S_1 \\cap S_i} R_{k1} R_{ki}\n$$\nLet $K = |S_1 \\cap S_i|$ be the size of this random overlap. For each $k \\in S_1 \\cap S_i$, the term $R_{k1}R_{ki} = (\\pm 1/\\sqrt{s})(\\pm 1/\\sqrt{s}) = (1/s) \\cdot \\rho_k$, where $\\rho_k$ is a product of two independent Rademacher random variables, and is itself a Rademacher variable. So,\n$$\n\\langle Re_1, Re_i \\rangle = \\frac{1}{s}\\sum_{k=1}^K \\rho_k = \\frac{Z_K}{s}\n$$\nwhere $Z_K$ is a sum of $K$ i.i.d. Rademacher variables. The crucial point is that $K$ is a random variable. It follows a hypergeometric distribution, as it represents the number of common successes when drawing $s$ items (rows for column 1) and then another $s$ items (rows for column $i$) without replacement from a population of $m$. The distribution of $\\langle Re_1, Re_i \\rangle$ is therefore a mixture, conditional on $K$. A chance large fluctuation in $K$ can lead to a sum over many more terms than average, producing a heavy-tailed distribution for the inner product that is not present in the Gaussian case. This makes larger values of the inner product, and thus larger distortions, more probable for $R$ than for $G$ at the same embedding dimension $m$.\n\n**2. Derivation of Minimal Sparsity $s^{\\star}$**\n\nWe need to find the minimal sparsity $s=s^{\\star}$ such that $R$ preserves the norms of all $v \\in V$ with probability at least $1/2$. This requires $|\\langle Re_1, Re_i \\rangle| \\leq \\varepsilon$ to hold for all $i \\in \\{2, \\dots, n\\}$. Let $F_i$ be the failure event $|\\langle Re_1, Re_i \\rangle| > \\varepsilon$. By the union bound, the total failure probability is $P(\\cup_{i=2}^n F_i) \\leq \\sum_{i=2}^n P(F_i)$. By symmetry, all $P(F_i)$ are equal. So we need $(n-1) P(F_2) \\le 1/2$, or $P(F_2) \\le \\frac{1}{2(n-1)}$.\n\nThe failure condition is $|\\frac{Z_K}{s}| > \\varepsilon$, or $|Z_K| > s\\varepsilon$.\nLet's analyze what causes this failure. Given the overlap $K$, $Z_K$ is a sum of $K$ Rademacher variables, so $E[Z_K|K]=0$ and $\\text{Var}(Z_K|K)=K$. The \"typical\" magnitude of $Z_K$ is $\\sqrt{K}$. The failure condition $|Z_K| > s\\varepsilon$ becomes likely when the typical magnitude is comparable to the threshold, i.e., $\\sqrt{K} \\approx s\\varepsilon$. This defines a critical overlap size:\n$$\nK_{crit} = (s\\varepsilon)^2\n$$\nIf the random overlap $K$ happens to be greater than or equal to $K_{crit}$, the conditional probability of failure $P(|Z_K| > s\\varepsilon | K \\ge K_{crit})$ becomes significant (a constant not depending on the problem parameters). Therefore, the overall failure probability is dominated by the probability of this rare event where the overlap is critically large:\n$$\nP(F_2) \\approx P(K \\ge K_{crit}) = P(K \\ge s^2\\varepsilon^2)\n$$\nTo ensure the embedding succeeds with high probability, we must make this event sufficiently rare. We need to bound the tail probability of $K$. The distribution of $K$ is Hypergeometric($N=m, K_{pop}=s, n_{draw}=s$). For $m \\gg s$, this is well-approximated by a Binomial distribution $B(s, p=s/m)$, as the probability of picking a specific row for column $i$ is $s/m$.\n\nWe use the Chernoff bound for the upper tail of a binomial distribution. For a random variable $X \\sim B(N, p)$, the probability of observing at least $k$ successes is bounded by $P(X \\ge k) \\le \\exp(-N \\cdot D_{KL}(k/N || p))$, where $D_{KL}(q||p) = q\\ln(q/p) + (1-q)\\ln\\left(\\frac{1-q}{1-p}\\right)$ is the Kullback-Leibler divergence.\nHere, $N=s$, $k=K_{crit}=s^2\\varepsilon^2$, and $p=s/m$. The fraction of successes is $q=k/N = s^2\\varepsilon^2/s = s\\varepsilon^2$. We must satisfy the condition $q \\le 1$, meaning $s\\varepsilon^2 \\le 1$.\nWe require $P(K \\ge s^2\\varepsilon^2) \\le \\frac{1}{2(n-1)}$.\nThis leads to the inequality:\n$$\n\\exp(-s \\cdot D_{KL}(s\\varepsilon^2 || s/m)) \\le \\frac{1}{2(n-1)}\n$$\nTaking the natural logarithm of both sides:\n$$\n-s \\cdot D_{KL}(s\\varepsilon^2 || s/m) \\le -\\ln(2(n-1))\n$$\n$$\ns \\cdot D_{KL}(s\\varepsilon^2 || s/m) \\ge \\ln(2(n-1))\n$$\nIn the regime of interest, sparsity is effective when $s$ is small, so $p=s/m$ is small. The distortion parameter $\\varepsilon$ is also small. We are interested in the case where the required number of overlaps $q=s\\varepsilon^2$ is much larger than the expected number of overlaps $p=s/m$. This corresponds to the condition $m\\varepsilon^2 \\gg 1$, which is typical for Johnson-Lindenstrauss results.\nIn this case ($q \\gg p$, and both are small), the KL divergence can be approximated by its dominant term:\n$$\nD_{KL}(q||p) \\approx q\\ln(q/p)\n$$\nSubstituting this approximation into our inequality:\n$$\ns \\cdot \\left( (s\\varepsilon^2) \\ln\\left(\\frac{s\\varepsilon^2}{s/m}\\right) \\right) \\ge \\ln(2(n-1))\n$$\n$$\ns^2 \\varepsilon^2 \\ln(m\\varepsilon^2) \\ge \\ln(2(n-1))\n$$\nAs per the problem, we track exponential-rate terms and can ignore constant factors inside logarithms for large $n$. Thus, $\\ln(2(n-1)) \\approx \\ln(n)$.\n$$\ns^2 \\varepsilon^2 \\ln(m\\varepsilon^2) \\gtrsim \\ln(n)\n$$\nSolving for the minimal sparsity $s = s^{\\star}$:\n$$\n(s^{\\star})^2 \\gtrsim \\frac{\\ln(n)}{\\varepsilon^2 \\ln(m\\varepsilon^2)}\n$$\n$$\ns^{\\star} \\approx \\frac{\\sqrt{\\ln(n)}}{\\varepsilon\\sqrt{\\ln(m\\varepsilon^2)}}\n$$\n\n**3. Closed-Form Analytic Expression for $s^{\\star}(n, m, \\varepsilon)$**\n\nBased on the asymptotic derivation above, the minimal column sparsity $s^{\\star}$ that separates its dependence on $n$, $m$, and $\\varepsilon$ is:\n$$\ns^{\\star}(n, m, \\varepsilon) = \\frac{1}{\\varepsilon} \\sqrt{\\frac{\\ln(n)}{\\ln(m\\varepsilon^2)}}\n$$\nThis expression captures the required scaling of sparsity with the number of points $n$, the embedding dimension $m$, and the distortion $\\varepsilon$.", "answer": "$$\\boxed{\\frac{1}{\\varepsilon} \\sqrt{\\frac{\\ln(n)}{\\ln(m\\varepsilon^2)}}}$$", "id": "3570496"}]}