{"hands_on_practices": [{"introduction": "Understanding algorithm complexity begins with the basics. This first practice focuses on one of the most fundamental operations in numerical linear algebra: dense matrix-vector multiplication. By meticulously counting the floating-point operations (flops) and applying the formal definition of Big-O notation, you will build a solid foundation for analyzing more complex algorithms and confirm the well-known $O(n^2)$ complexity of this core routine [@problem_id:3534517].", "problem": "Consider dense matrix-vector multiplication in the sense of the Basic Linear Algebra Subprograms (BLAS), specifically the operation $y \\leftarrow A x$ with $A \\in \\mathbb{R}^{n \\times n}$ and $x, y \\in \\mathbb{R}^{n}$. Work within the standard floating-point operation (flop) cost model used in numerical linear algebra: each scalar floating-point addition counts as $1$ flop and each scalar floating-point multiplication counts as $1$ flop; ignore data movement, branching, and loop overhead. Assume no fused multiply-add is used.\n\nStarting from fundamental definitions of flop counting and the formal definition of Big-O (Landau) notation, derive the exact flop count $F(n)$ for a straightforward, row-oriented implementation of the dense matrix-vector multiplication that computes each component $y_{i}$ as the dot product of the $i$-th row of $A$ with $x$ using a single accumulator. Then, using the formal definition of Big-O notation, demonstrate that $F(n)$ is in the set $O(n^{2})$.\n\nReport the exact flop count as a single closed-form expression in $n$. Do not provide an inequality, equation, or asymptotic notation in the final answer. No rounding is required.", "solution": "The problem is validated as self-contained, consistent, and scientifically sound. It is a standard problem in introductory numerical linear algebra and computational complexity analysis.\n\nThe task is to determine the exact floating-point operation (flop) count for a dense matrix-vector multiplication $y \\leftarrow A x$, where $A \\in \\mathbb{R}^{n \\times n}$ and $x, y \\in \\mathbb{R}^{n}$, and then to formally prove that this count is in $O(n^2)$.\n\nFirst, we will derive the exact flop count, denoted by $F(n)$. The operation is defined component-wise for each element $y_i$ of the vector $y$, where $i$ ranges from $1$ to $n$. The formula for the $i$-th component is:\n$$\ny_i = \\sum_{j=1}^{n} A_{ij} x_j\n$$\nThis can be expanded as:\n$$\ny_i = A_{i1}x_1 + A_{i2}x_2 + \\dots + A_{in}x_n\n$$\nWe analyze the cost of computing a single component $y_i$ based on the specified flop model, where one scalar addition and one scalar multiplication each count as $1$ flop.\n\nFor a single $y_i$, the computation involves the following operations:\n$1.$ A sequence of $n$ scalar multiplications: $A_{i1}x_1, A_{i2}x_2, \\dots, A_{in}x_n$. This contributes $n$ multiplication flops.\n$2.$ The summation of the $n$ products resulting from the multiplications. Adding $n$ terms together requires $n-1$ scalar additions. For example, $(A_{i1}x_1 + A_{i2}x_2)$ is one addition, adding $A_{i3}x_3$ to this sum is a second addition, and so on, until the $n$-th term is added, which constitutes the $(n-1)$-th addition. The use of a single accumulator for the dot product corresponds to this sequential summation. This contributes $n-1$ addition flops.\n\nThe total number of flops to compute a single component $y_i$ is the sum of the multiplication and addition flops:\n$$\n\\text{Flops per component} = n + (n-1) = 2n-1\n$$\nSince the vector $y$ has $n$ components ($y_1, y_2, \\dots, y_n$), and the computation of each component is independent and has the same cost, the total flop count $F(n)$ for the entire matrix-vector multiplication is the number of components multiplied by the flops per component:\n$$\nF(n) = n \\times (2n-1)\n$$\nExpanding this expression gives the final closed-form for the exact flop count:\n$$\nF(n) = 2n^2 - n\n$$\nNext, we must use the formal definition of Big-O notation to show that $F(n)$ is in the set $O(n^2)$. A function $f(n)$ is in $O(g(n))$ if there exist a positive constant $C$ and a natural number $n_0$ such that for all $n \\geq n_0$, the inequality $|f(n)| \\leq C |g(n)|$ holds.\n\nIn this case, our function is $f(n) = F(n) = 2n^2 - n$, and we want to show it belongs to $O(n^2)$, so $g(n) = n^2$.\nWe need to find constants $C  0$ and $n_0 \\in \\mathbb{N}$ such that for all $n \\geq n_0$:\n$$\n|2n^2 - n| \\leq C |n^2|\n$$\nSince $n$ represents the dimension of the matrix, $n$ must be a positive integer, so $n \\geq 1$.\nFor $n \\geq 1$, the term $2n^2 - n = n(2n - 1)$ is non-negative, so $|2n^2 - n| = 2n^2 - n$. Also, for $n \\geq 1$, $|n^2| = n^2$.\nThe inequality becomes:\n$$\n2n^2 - n \\leq C n^2\n$$\nTo find a suitable value for $C$, we can manipulate the inequality. Since $n^2  0$ for $n \\geq 1$, we can divide both sides by $n^2$:\n$$\n2 - \\frac{1}{n} \\leq C\n$$\nWe need to find a constant $C$ that satisfies this for all $n$ from some $n_0$ onwards. Let us choose $n_0 = 1$. For all $n \\geq 1$, we know that $0  \\frac{1}{n} \\leq 1$.\nThis implies $2 - \\frac{1}{n}  2$.\nTherefore, we can choose any constant $C$ such that $C \\geq 2$. A simple and valid choice is $C = 2$.\n\nLet's verify our choice. We choose $C = 2$ and $n_0 = 1$. We must show that for all $n \\geq 1$, the inequality $2n^2 - n \\leq 2n^2$ holds.\nSubtracting $2n^2$ from both sides gives:\n$$\n-n \\leq 0\n$$\nThis inequality is true for all positive integers $n$, and thus certainly true for all $n \\geq 1$.\nSince we have found a constant $C = 2$ and an integer $n_0 = 1$ that satisfy the definition, we have formally demonstrated that $F(n) = 2n^2 - n$ is in $O(n^2)$.\n\nThe final answer requested is the exact flop count as a single closed-form expression in $n$. This is the function $F(n)$ we derived.", "answer": "$$\n\\boxed{2n^{2} - n}\n$$", "id": "3534517"}, {"introduction": "While many classical algorithms are analyzed by summing operations in loops, some of the most groundbreaking performance improvements come from recursive, divide-and-conquer strategies. This exercise explores Strassen's revolutionary algorithm for matrix multiplication, which demonstrates how to break the intuitive $O(n^3)$ barrier. You will derive its unique complexity of $O(n^{\\log_{2}(7)})$ by setting up and analyzing a recurrence relation, a crucial technique for understanding recursive algorithms [@problem_id:3534539].", "problem": "Consider multiplying two dense square matrices of dimension $n \\times n$ over the real numbers using Strassen’s method of block recursion. Assume the Random Access Machine (RAM) model in which each scalar addition and each scalar multiplication has unit cost. At one level of the recursion, Strassen’s construction partitions each input matrix into $2 \\times 2$ blocks of dimension $(n/2) \\times (n/2)$, performs exactly $7$ recursive multiplications of $(n/2) \\times (n/2)$ submatrices, and combines these results using a fixed finite number of additions and subtractions of $(n/2) \\times (n/2)$ matrices. Let $T(n)$ denote the total number of scalar arithmetic operations (additions and multiplications) required to multiply two $n \\times n$ matrices by Strassen’s algorithm, and suppose $n$ is a power of $2$ so that no floors or ceilings arise. Use the facts that adding or subtracting two $(m \\times m)$ matrices costs $\\Theta(m^{2})$ scalar operations and that the number of block additions and subtractions performed at each recursion level is a fixed constant independent of $n$.\n\nStarting from these principles, derive and analyze the divide-and-conquer recurrence satisfied by $T(n)$, explicitly outline the recursion tree it induces, and determine the dominant scaling of $T(n)$ as $n \\to \\infty$. Provide your final answer as the closed-form analytic expression representing the leading-order asymptotic arithmetic complexity of Strassen’s algorithm in $n$, ignoring constant factors and lower-order terms. Your final answer must be a single expression in $n$ (no inequalities or Big-O symbols inside the final box).", "solution": "The problem asks for the asymptotic complexity of Strassen's matrix multiplication algorithm for two dense square matrices of dimension $n \\times n$. We are given that $n$ is a power of $2$, say $n = 2^k$ for some integer $k \\ge 0$. The analysis is based on a divide-and-conquer recurrence relation.\n\nLet $T(n)$ be the total number of scalar arithmetic operations (additions, subtractions, and multiplications) required to multiply two $n \\times n$ matrices using this algorithm. The method involves partitioning each $n \\times n$ matrix into four $(n/2) \\times (n/2)$ submatrices. It then recursively computes $7$ matrix products of size $(n/2) \\times (n/2)$. The cost of these recursive calls is $7 \\cdot T(n/2)$.\n\nAfter the recursive multiplications, the results are combined using a fixed number of matrix additions and subtractions. Let this constant number be $c_{ops}$. The problem states this number is finite and independent of $n$. The matrices being added or subtracted are of size $(n/2) \\times (n/2)$. Adding or subtracting two $m \\times m$ matrices requires $m^2$ scalar operations. Thus, one addition/subtraction of $(n/2) \\times (n/2)$ matrices costs $(n/2)^2 = n^2/4$ scalar operations. The total cost for this combination step at one level of recursion is therefore $c_{ops} \\cdot (n^2/4)$. This cost is proportional to $n^2$, so we can write it as $C n^2$ for some positive constant $C$.\n\nCombining the costs of the recursive calls and the combination step, we arrive at the recurrence relation for $T(n)$:\n$$T(n) = 7 T(n/2) + C n^2$$\nThe base case for the recursion is the multiplication of two $1 \\times 1$ matrices, which consists of a single scalar multiplication. Hence, $T(1) = 1$.\n\nTo determine the dominant scaling of $T(n)$, we can analyze the recursion tree induced by this recurrence.\nThe tree has a depth of $k = \\log_2(n)$, since at each level $j$ (from $j=0$ at the root to $j=k$ at the leaves), the problem size is reduced from $n/2^j$ to $n/2^{j+1}$. The recursion stops when the size is $1$, i.e., $n/2^k = 1$.\n\nLet's sum the costs at each level of the tree:\nLevel $0$ (root): The work done is the combination cost for the initial problem of size $n$, which is $C n^2$.\nLevel $1$: There are $7$ subproblems, each of size $n/2$. The total work done at this level is $7 \\times C(n/2)^2 = 7 C (n^2/4) = C n^2 (7/4)^1$.\nLevel $2$: There are $7^2$ subproblems, each of size $n/4$. The total work done at this level is $7^2 \\times C(n/4)^2 = 49 C (n^2/16) = C n^2 (7/4)^2$.\nLevel $j$: In general, at level $j$, there are $7^j$ subproblems, each of size $n/2^j$. The total combination work done at this level is $7^j \\times C(n/2^j)^2 = 7^j C (n^2/4^j) = C n^2 (7/4)^j$.\n\nThe total work done at all internal nodes of the recursion tree (i.e., the total cost of all additions and subtractions) is the sum of the work at levels $j=0$ to $j=k-1$:\n$$W_{internal} = \\sum_{j=0}^{k-1} C n^2 \\left(\\frac{7}{4}\\right)^j$$\nThis is a geometric series with ratio $r = 7/4$. The sum is:\n$$W_{internal} = C n^2 \\frac{(7/4)^k - 1}{(7/4) - 1} = C n^2 \\frac{(7/4)^{\\log_2(n)} - 1}{3/4} = \\frac{4C}{3} n^2 \\left(\\left(\\frac{7}{4}\\right)^{\\log_2(n)} - 1\\right)$$\nWe can simplify the term $(7/4)^{\\log_2(n)}$:\n$$\\left(\\frac{7}{4}\\right)^{\\log_2(n)} = \\frac{7^{\\log_2(n)}}{4^{\\log_2(n)}} = \\frac{n^{\\log_2(7)}}{(2^2)^{\\log_2(n)}} = \\frac{n^{\\log_2(7)}}{2^{2\\log_2(n)}} = \\frac{n^{\\log_2(7)}}{2^{\\log_2(n^2)}} = \\frac{n^{\\log_2(7)}}{n^2}$$\nSubstituting this back into the expression for $W_{internal}$:\n$$W_{internal} = \\frac{4C}{3} n^2 \\left(\\frac{n^{\\log_2(7)}}{n^2} - 1\\right) = \\frac{4C}{3} n^{\\log_2(7)} - \\frac{4C}{3} n^2$$\n\nThe total cost $T(n)$ is the sum of the internal work and the work done at the leaves.\nThe leaves correspond to the base cases of the recursion. The number of leaves at depth $k = \\log_2(n)$ is $7^k = 7^{\\log_2(n)} = n^{\\log_2(7)}$. Each leaf represents a $1 \\times 1$ matrix multiplication, which has a cost of $T(1)=1$.\nThe total work at the leaves is:\n$$W_{leaves} = 7^k \\cdot T(1) = n^{\\log_2(7)} \\cdot 1 = n^{\\log_2(7)}$$\n\nThe total cost is $T(n) = W_{internal} + W_{leaves}$:\n$$T(n) = \\left(\\frac{4C}{3} n^{\\log_2(7)} - \\frac{4C}{3} n^2\\right) + n^{\\log_2(7)} = \\left(1 + \\frac{4C}{3}\\right) n^{\\log_2(7)} - \\frac{4C}{3} n^2$$\nTo determine the dominant scaling as $n \\to \\infty$, we compare the exponents of $n$. The exponents are $\\log_2(7)$ and $2$.\nWe know that $2^2 = 4$ and $2^3 = 8$. Since $4  7  8$, we have $\\log_2(4)  \\log_2(7)  \\log_2(8)$, which implies $2  \\log_2(7)  3$. Numerically, $\\log_2(7) \\approx 2.807$.\nBecause $\\log_2(7)  2$, the term $n^{\\log_2(7)}$ grows faster than $n^2$. Therefore, the term $\\left(1 + \\frac{4C}{3}\\right) n^{\\log_2(7)}$ is the dominant term in the expression for $T(n)$.\n\nThe leading-order asymptotic arithmetic complexity is the part of the expression that grows fastest with $n$. Ignoring the constant factor $\\left(1 + \\frac{4C}{3}\\right)$ and the lower-order term $-\\frac{4C}{3} n^2$, the complexity is determined by the scaling of $n^{\\log_2(7)}$.\n\nThis result can also be confirmed using the Master Theorem for recurrences of the form $T(n) = a T(n/b) + f(n)$. In our case, $a=7$, $b=2$, and $f(n) = C n^2$. We compare $f(n)$ with $n^{\\log_b(a)} = n^{\\log_2(7)}$. Since $f(n) = C n^2$ and $2  \\log_2(7)$, we have $f(n) = O(n^{\\log_2(7) - \\epsilon})$ for some $\\epsilon  0$ (e.g., $\\epsilon = \\log_2(7) - 2$). This corresponds to Case 1 of the Master Theorem, which gives the solution $T(n) = \\Theta(n^{\\log_b(a)}) = \\Theta(n^{\\log_2(7)})$.\n\nThe question asks for the closed-form analytic expression for the leading-order asymptotic complexity, which is the functional form $n^{\\log_2(7)}$.", "answer": "$$\\boxed{n^{\\log_{2}(7)}}$$", "id": "3534539"}, {"introduction": "Asymptotic notation is a powerful tool, but it describes behavior as $n \\to \\infty$ and can sometimes be misleading for practical problem sizes by hiding large constant factors or lower-order terms. This final practice moves from pure theory to a computational investigation, comparing two different strategies for polynomial root-finding. By analyzing a hypothetical model that includes explicit cost constants, you will discover the \"crossover point\" where the asymptotically faster algorithm actually becomes the more efficient choice, illustrating a critical lesson in practical algorithm design [@problem_id:3534505].", "problem": "Consider a monic polynomial of degree $n$, denoted by $p(x) = x^n + a_{n-1} x^{n-1} + \\cdots + a_0$. Two algorithmic strategies to compute approximations to its roots in numerical linear algebra are compared from the standpoint of algorithmic complexity using Big O notation, but with explicit tracking of hidden constants under a floating-point operations (flops) cost model.\n\nStrategy A (companion matrix via eigenvalues): Form the $n \\times n$ companion matrix $C$ associated to $p(x)$, and compute its eigenvalues using the Francis implicit double-shift $QR$ algorithm specialized to an upper Hessenberg matrix. It is a standard, well-tested fact that the $QR$ algorithm applied to an $n \\times n$ Hessenberg matrix requires $\\Theta(n^3)$ floating-point operations in the worst case to compute all eigenvalues, and it is also well-known that the cost of each implicit double-shift bulge-chasing sweep is $\\Theta(n^2)$ flops. Under the flops model where each floating-point addition or multiplication counts as one unit of work, take the following scientifically plausible constant model: one sweep costs approximately $12 n^2$ flops, and a constant number $\\Theta(n)$ of sweeps are needed to complete the process. Therefore, Strategy A is modeled by the total cost\n$$\nT_{\\mathrm{A}}(n) = c_{\\mathrm{A}} n^3,\n$$\nwith $c_{\\mathrm{A}} = 12$.\n\nStrategy B (multipoint evaluation and Newton refinement): Choose $n$ initial guesses for the roots, for instance on a Cauchy radius circle; then apply $k$ steps of Newton's method to refine these guesses. Each Newton iteration requires evaluating $p(x)$ and $p'(x)$ at all $n$ points. The multipoint evaluation can be performed using a subproduct/remainder tree with Fast Fourier Transform (FFT)-based polynomial arithmetic. A well-tested fact is that polynomial multiplication of length $L$ using FFT costs $\\Theta(L \\log_2 L)$ operations. Under the flops model, assume a single real FFT of length $L$ costs $c_{\\mathrm{FFT}} L \\log_2 L$ flops with $c_{\\mathrm{FFT}} = 5$, and that an FFT-based polynomial multiplication uses two forward FFTs, one inverse FFT, and a linear-time pointwise multiplication. This gives an approximate multiplication cost $20 L \\log_2 L + 2L$. The subproduct/remainder tree has $O(\\log_2 n)$ levels, with $O(n)$ total polynomial length per level; to model the hidden constants, use a padding to the next power of two to reduce FFT constant factor variability by defining $n' = 2^{\\lceil \\log_2 n \\rceil}$ and taking $L \\approx n'$ uniformly. The total multipoint evaluation cost for $p(x)$ at $n$ points is modeled as\n$$\nT_{\\mathrm{eval}}(n) \\approx c_{\\mathrm{eval}}\\, n' \\left( \\log_2 n' \\right)^2,\n$$\nwith $c_{\\mathrm{eval}} = 40$. Evaluating both $p$ and $p'$ at $n$ points per Newton iteration multiplies the cost by $2$, and $k$ Newton iterations multiply by $k$, yielding the Strategy B total cost\n$$\nT_{\\mathrm{B}}(n,k) = 2 k \\, T_{\\mathrm{eval}}(n) \\approx 80 k \\, n' \\left( \\log_2 n' \\right)^2.\n$$\n\nYour task is to implement a program that, for a provided test suite of $(n,k)$ pairs, computes the ratio\n$$\nR(n,k) = \\frac{T_{\\mathrm{A}}(n)}{T_{\\mathrm{B}}(n,k)} = \\frac{c_{\\mathrm{A}} n^3}{80 k \\, n' \\left( \\log_2 n' \\right)^2},\n$$\nwith $c_{\\mathrm{A}} = 12$ and $n' = 2^{\\lceil \\log_2 n \\rceil}$. This ratio quantifies how misleading the Big O asymptotics can be at moderate $n$: values $R(n,k)  1$ indicate that, under this explicit constant model, Strategy A (companion matrix eigenvalue) is predicted to be more expensive than Strategy B (multipoint evaluation plus Newton) for the given $(n,k)$, while $R(n,k)  1$ indicates the opposite.\n\nFundamental bases to use:\n- The definition of Big O notation: For functions $f(n)$ and $g(n)$, $f(n) = O(g(n))$ means there exist positive constants $C$ and $n_0$ such that $f(n) \\le C g(n)$ for all $n \\ge n_0$.\n- Flops cost model in numerical linear algebra: Count each floating-point addition or multiplication as $1$ unit of cost; transformations and FFTs are accounted for by their standard operation counts noted above.\n- The subproduct/remainder tree multipoint evaluation algorithm and FFT-based polynomial multiplication are widely accepted algorithmic building blocks with the noted $\\Theta(L \\log_2 L)$ behavior.\n\nTest suite:\n- Case 1: $(n,k) = (8,5)$, small degree with more refinement steps.\n- Case 2: $(n,k) = (32,3)$, moderate degree and typical refinement.\n- Case 3: $(n,k) = (100,4)$, non-power-of-two degree to demonstrate padding effects.\n- Case 4: $(n,k) = (128,2)$, larger moderate degree and fewer refinement steps.\n- Case 5: $(n,k) = (512,2)$, large degree where asymptotics should favor Strategy B.\n\nFor each test case, the program must return the single floating-point value $R(n,k)$ as defined above. No physical units apply since the outputs are in flop counts. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[r_1,r_2,r_3,r_4,r_5]$), where each $r_i$ is the ratio for the corresponding test case in the specified order.\n\nConstraints and requirements:\n- Implement the padding to the next power of two via $n' = 2^{\\lceil \\log_2 n \\rceil}$ inside the computation of $T_{\\mathrm{B}}(n,k)$.\n- Use the constants $c_{\\mathrm{A}} = 12$ and $c_{\\mathrm{eval}} = 40$ and $c_{\\mathrm{FFT}} = 5$ as specified.\n- Ensure numeric stability for logarithms by using $\\log_2$ on integer $n' \\ge 2$ in all provided test cases.", "solution": "The problem has been validated and is determined to be a valid, well-posed scientific problem. All required constants, formulas, and test cases are provided, and the underlying principles are grounded in established concepts of numerical linear algebra and algorithmic complexity analysis.\n\nThe objective is to compute the ratio $R(n,k)$ of the operational costs of two root-finding strategies for a polynomial of degree $n$, where $k$ is the number of refinement iterations for the second strategy. This ratio serves to compare the practical performance of an asymptotically slower algorithm (Strategy A) with an asymptotically faster one (Strategy B) for finite, moderate values of $n$, highlighting the role of hidden constants in Big O notation.\n\nThe cost for Strategy A, based on the Francis $QR$ algorithm for a companion matrix, is given by\n$$T_{\\mathrm{A}}(n) = c_{\\mathrm{A}} n^3$$\nwith the constant $c_{\\mathrm{A}} = 12$.\n\nThe cost for Strategy B, based on multipoint evaluation using FFTs and Newton's method, is given by\n$$T_{\\mathrm{B}}(n,k) = 80 k \\, n' \\left( \\log_2 n' \\right)^2$$\nwhere $n'$ is the smallest power of two greater than or equal to $n$, formally defined as\n$$n' = 2^{\\lceil \\log_2 n \\rceil}$$\n\nThe ratio to be computed is\n$$R(n,k) = \\frac{T_{\\mathrm{A}}(n)}{T_{\\mathrm{B}}(n,k)} = \\frac{12 n^3}{80 k \\, n' \\left( \\log_2 n' \\right)^2}$$\n\nWe will now compute this ratio for each of the five test cases provided.\n\n**Case 1: $(n,k) = (8,5)$**\nFirst, we determine $n'$. Since $n=8$ is a power of two, $\\log_2 8 = 3$, and $\\lceil \\log_2 8 \\rceil = 3$.\n$$n' = 2^3 = 8$$\nNow, we compute the costs $T_{\\mathrm{A}}(8)$ and $T_{\\mathrm{B}}(8,5)$.\n$$T_{\\mathrm{A}}(8) = 12 \\times 8^3 = 12 \\times 512 = 6144$$\n$$T_{\\mathrm{B}}(8,5) = 80 \\times 5 \\times 8 \\times (\\log_2 8)^2 = 400 \\times 8 \\times 3^2 = 3200 \\times 9 = 28800$$\nThe ratio is:\n$$R(8,5) = \\frac{6144}{28800} = 0.21333...$$\nSince $R  1$, for this small degree, Strategy B is more expensive.\n\n**Case 2: $(n,k) = (32,3)$**\nFirst, we determine $n'$. Since $n=32$ is a power of two, $\\log_2 32 = 5$, and $\\lceil \\log_2 32 \\rceil = 5$.\n$$n' = 2^5 = 32$$\nNow, we compute the costs $T_{\\mathrm{A}}(32)$ and $T_{\\mathrm{B}}(32,3)$.\n$$T_{\\mathrmA}}(32) = 12 \\times 32^3 = 12 \\times 32768 = 393216$$\n$$T_{\\mathrm{B}}(32,3) = 80 \\times 3 \\times 32 \\times (\\log_2 32)^2 = 240 \\times 32 \\times 5^2 = 7680 \\times 25 = 192000$$\nThe ratio is:\n$$R(32,3) = \\frac{393216}{192000} = 2.048$$\nSince $R  1$, Strategy A is now more expensive than Strategy B.\n\n**Case 3: $(n,k) = (100,4)$**\nFirst, we determine $n'$. For $n=100$, we have $\\log_2 100 \\approx 6.6438$. Therefore, $\\lceil \\log_2 100 \\rceil = 7$.\n$$n' = 2^7 = 128$$\nNow, we compute the costs $T_{\\mathrm{A}}(100)$ and $T_{\\mathrm{B}}(100,4)$.\n$$T_{\\mathrm{A}}(100) = 12 \\times 100^3 = 12 \\times 1000000 = 12000000$$\n$$T_{\\mathrm{B}}(100,4) = 80 \\times 4 \\times 128 \\times (\\log_2 128)^2 = 320 \\times 128 \\times 7^2 = 40960 \\times 49 = 2007040$$\nThe ratio is:\n$$R(100,4) = \\frac{12000000}{2007040} \\approx 5.97895408...$$\nThe cost of Strategy A is almost $6$ times that of Strategy B. The padding from $n=100$ to $n'=128$ for the FFTs is a significant factor, but the $n^3$ growth of Strategy A dominates.\n\n**Case 4: $(n,k) = (128,2)$**\nFirst, we determine $n'$. Since $n=128$ is a power of two, $\\log_2 128 = 7$, and $\\lceil \\log_2 128 \\rceil = 7$.\n$$n' = 2^7 = 128$$\nNow, we compute the costs $T_{\\mathrm{A}}(128)$ and $T_{\\mathrm{B}}(128,2)$.\n$$T_{\\mathrm{A}}(128) = 12 \\times 128^3 = 12 \\times 2097152 = 25165824$$\n$$T_{\\mathrm{B}}(128,2) = 80 \\times 2 \\times 128 \\times (\\log_2 128)^2 = 160 \\times 128 \\times 7^2 = 20480 \\times 49 = 1003520$$\nThe ratio is:\n$$R(128,2) = \\frac{25165824}{1003520} = 25.0775$$\n\n**Case 5: $(n,k) = (512,2)$**\nFirst, we determine $n'$. Since $n=512$ is a power of two, $\\log_2 512 = 9$, and $\\lceil \\log_2 512 \\rceil = 9$.\n$$n' = 2^9 = 512$$\nNow, we compute the costs $T_{\\mathrm{A}}(512)$ and $T_{\\mathrm{B}}(512,2)$.\n$$T_{\\mathrm{A}}(512) = 12 \\times 512^3 = 12 \\times 134217728 = 1610612736$$\n$$T_{\\mathrm{B}}(512,2) = 80 \\times 2 \\times 512 \\times (\\log_2 512)^2 = 160 \\times 512 \\times 9^2 = 81920 \\times 81 = 6635520$$\nThe ratio is:\n$$R(512,2) = \\frac{1610612736}{6635520} \\approx 242.730303...$$\nAs expected, for large $n$, the advantage of the asymptotically superior Strategy B becomes very pronounced.\n\nThe following program will implement this calculation for the given test suite.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy import ...\n\ndef solve():\n    \"\"\"\n    Computes the cost ratio R(n,k) for two polynomial root-finding algorithms\n    based on the provided cost models and test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (8, 5),    # Case 1: Small degree\n        (32, 3),   # Case 2: Moderate degree, power-of-two\n        (100, 4),  # Case 3: Non-power-of-two degree\n        (128, 2),  # Case 4: Larger moderate degree\n        (512, 2),  # Case 5: Large degree\n    ]\n\n    results = []\n    \n    # Constant for Strategy A\n    c_A = 12.0\n\n    for n, k in test_cases:\n        # Calculate cost for Strategy A: T_A(n) = c_A * n^3\n        t_a = c_A * (n ** 3)\n\n        # Calculate n' for Strategy B: n' = 2^ceil(log2(n))\n        # This computes the next power of two = n.\n        if n == 0:\n            # Although not in test cases, handle the edge case for log2.\n            n_prime = 0\n        else:\n            ceil_log2_n = np.ceil(np.log2(n))\n            n_prime = 2**ceil_log2_n\n        \n        # Log2 of n_prime will now be an integer.\n        log2_n_prime = np.log2(n_prime)\n\n        # Calculate cost for Strategy B: T_B(n,k) = 80 * k * n' * (log2(n'))^2\n        # Check for n_prime  2 to avoid log2 of 1 or 0, though test cases prevent this.\n        if n_prime  2:\n            t_b = float('inf') # Define as infinitely costly to avoid division by zero\n        else:\n            t_b = 80.0 * k * n_prime * (log2_n_prime ** 2)\n\n        # Compute the ratio R(n,k) = T_A / T_B\n        if t_b == 0:\n            # Handle division by zero, though not expected from problem constraints.\n            ratio = float('inf')\n        else:\n            ratio = t_a / t_b\n\n        results.append(ratio)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3534505"}]}