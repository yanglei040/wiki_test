{"hands_on_practices": [{"introduction": "Our journey into computational cost begins with a foundational task: evaluating a polynomial. While seemingly simple, this exercise reveals a core principle of numerical analysis—that the way an algorithm is structured can dramatically alter its efficiency. In this practice [@problem_id:3538828], you will compare a naive, brute-force approach with the elegant and efficient Horner's method, providing a clear, quantitative measure of how much a better algorithm can save.", "problem": "Consider the polynomial $p(x) = a_{0} + a_{1} x + a_{2} x^{2} + \\cdots + a_{n} x^{n}$ of degree $n$, where all coefficients $a_{k}$ for $k \\in \\{0,1,\\dots,n\\}$ are real and generically nonzero, and $x \\in \\mathbb{R}$ is a given scalar. You will evaluate $p(x)$ using two different methods and count the Floating-Point Operations (FLOPs), where by definition one floating-point addition costs $1$ FLOP and one floating-point multiplication costs $1$ FLOP; no other operation types are permitted, and Fused Multiply-Add (FMA) is not allowed.\n\nMethod A (Horner’s rule): Evaluate $p(x)$ using the nested recurrence starting from the highest-degree coefficient, i.e., initialize with $y := a_{n}$ and then iteratively update $y := y \\cdot x + a_{k}$ for $k = n-1, n-2, \\dots, 0$.\n\nMethod B (naive monomial evaluation): Evaluate $p(x)$ by computing each monomial $a_{k} x^{k}$ separately and then summing. For each $k \\geq 1$, compute $x^{k}$ by repeated multiplication by $x$ starting from the value $x$ itself (so that the construction of $x^{k}$ uses exactly $k-1$ multiplications), then multiply the result by $a_{k}$ once, and add it into an accumulator initialized to $a_{0}$. Do not reuse any intermediate powers of $x$ across different monomials.\n\nStarting from the foundational definitions of a polynomial and the arithmetic structure of FLOPs, derive the exact FLOP counts for Method A and Method B as functions of $n$. Then, express the ratio of Method B’s FLOP count to Method A’s FLOP count as a single closed-form expression in $n$. Provide this ratio as your final answer. No rounding is required.", "solution": "The problem requires a comparative analysis of the computational cost, measured in Floating-Point Operations (FLOPs), for evaluating a polynomial of degree $n$ using two distinct algorithms: Horner's rule and naive monomial evaluation. We are asked to derive the FLOP count for each method and then determine the ratio of the naive method's cost to Horner's method's cost.\n\nThe polynomial is given by $p(x) = \\sum_{k=0}^{n} a_k x^k$, where $n$ is the degree, $a_k \\in \\mathbb{R}$ are the coefficients, and $x \\in \\mathbb{R}$ is the point of evaluation. By definition, one floating-point addition or one floating-point multiplication contributes $1$ FLOP.\n\nFirst, we analyze Method A, which is Horner's rule. The algorithm is defined by an initialization followed by a recurrence:\n1.  Initialize an accumulator $y$ with the highest-degree coefficient: $y := a_{n}$. This is an assignment and costs $0$ FLOPs.\n2.  Iteratively update the accumulator for $k = n-1, n-2, \\dots, 0$: $y := y \\cdot x + a_{k}$.\n\nLet us examine the computational cost of the iterative step $y := y \\cdot x + a_{k}$. This operation consists of one multiplication ($y \\cdot x$) and one addition (the result plus $a_k$). Therefore, each step of the iteration costs exactly $1$ multiplication FLOP and $1$ addition FLOP, for a total of $2$ FLOPs per iteration.\n\nThe loop runs for the index $k$ from $n-1$ down to $0$. The number of iterations is the number of integer values in this range, which is $(n-1) - 0 + 1 = n$.\nThe total FLOP count for Method A, denoted $C_A(n)$, is the product of the number of iterations and the cost per iteration.\n$$C_A(n) = n \\times (\\text{1 multiplication} + \\text{1 addition}) = n \\times (1 + 1) \\text{ FLOPs} = 2n \\text{ FLOPs}$$\nThus, the total cost for Horner's method is $C_A(n) = 2n$.\n\nNext, we analyze Method B, the naive monomial evaluation. The algorithm is defined as follows:\n1.  Initialize an accumulator for the sum, $S := a_0$. This costs $0$ FLOPs.\n2.  For each index $k$ from $1$ to $n$, compute the term $a_k x^k$ and add it to the accumulator: $S := S + a_k x^k$.\n\nThe problem specifies that for each $k \\geq 1$, the power $x^k$ is computed by $k-1$ successive multiplications (e.g., $x^2 = x \\cdot x$, $x^3 = x^2 \\cdot x$, etc.), and that no intermediate powers of $x$ are reused between the computation of different monomials.\n\nLet's break down the FLOPs for computing a single term $a_k x^k$ and adding it to the sum, for a given $k \\in \\{1, 2, \\dots, n\\}$:\n-   Computing the power $x^k$: This requires $k-1$ multiplications.\n-   Multiplying by the coefficient $a_k$: This requires $1$ multiplication.\n-   Adding the result $a_k x^k$ to the accumulator $S$: This requires $1$ addition.\n\nThe total number of multiplications for the $k$-th term is $(k-1) + 1 = k$.\nThe total number of additions for the $k$-th term is $1$.\nThe total FLOPs for processing the $k$-th term is therefore $k+1$.\n\nTo find the total FLOP count for Method B, $C_B(n)$, we must sum the costs for all terms from $k=1$ to $n$.\nThe total number of multiplications is the sum of multiplications for each term:\n$$M_B(n) = \\sum_{k=1}^{n} k = \\frac{n(n+1)}{2}$$\nThe total number of additions is the sum of additions for each term:\n$$A_B(n) = \\sum_{k=1}^{n} 1 = n$$\nThe total FLOP count for Method B is the sum of all multiplications and additions:\n$$C_B(n) = M_B(n) + A_B(n) = \\frac{n(n+1)}{2} + n = \\frac{n^2+n}{2} + \\frac{2n}{2} = \\frac{n^2+3n}{2}$$\n\nFinally, we are asked to find the ratio of Method B's FLOP count to Method A's FLOP count. Let this ratio be $R(n)$.\n$$R(n) = \\frac{C_B(n)}{C_A(n)} = \\frac{\\frac{n^2+3n}{2}}{2n}$$\nFor a polynomial of degree $n \\ge 1$, both $C_A(n)$ and $C_B(n)$ are non-zero, allowing for simplification. For the edge case $n=0$, $p(x)=a_0$, and both methods require $0$ FLOPs, making the ratio indeterminate ($0/0$). However, the request for a single closed-form expression suggests we should find a formula valid for $n \\ge 1$ where the calculation is non-trivial. For $n \\ge 1$, we can simplify the expression for $R(n)$:\n$$R(n) = \\frac{n^2+3n}{4n} = \\frac{n(n+3)}{4n}$$\nSince $n \\neq 0$, we can cancel the factor of $n$ from the numerator and denominator:\n$$R(n) = \\frac{n+3}{4}$$\nThis closed-form expression gives the ratio of the computational costs for any polynomial of degree $n \\ge 1$.", "answer": "$$\\boxed{\\frac{n+3}{4}}$$", "id": "3538828"}, {"introduction": "Having established the basics of counting floating-point operations (flops) in real arithmetic, we now extend our analysis to the realm of complex numbers, which are indispensable in fields like signal processing and quantum mechanics. This problem [@problem_id:3538838] requires you to determine the cost of a complex dot product by breaking it down into its constituent real operations. This exercise builds your ability to analyze the cost of higher-level operations from first principles.", "problem": "Consider the computation of the complex dot product of two length-$n$ vectors $\\boldsymbol{z}, \\boldsymbol{w} \\in \\mathbb{C}^{n}$ defined as\n$$\ns \\;=\\; \\sum_{k=1}^{n} z_{k}\\,w_{k}.\n$$\nAdopt the standard real-floating-point-operation (flop) model in which each real addition or real multiplication costs $1$ flop. Under this model, assume that a complex multiplication costs $6$ flops and a complex addition costs $2$ flops. Ignore memory traffic, loop overhead, and any instruction-level fusion. Compute the total flop count required to evaluate $s$ using an accumulation strategy that minimizes the number of complex additions by first computing a single complex product to initialize the accumulator and then accumulating the remaining terms. Express your final answer as a closed-form expression in $n$.", "solution": "The problem requires the calculation of the total number of real floating-point operations (flops) needed to compute the complex dot product $s = \\sum_{k=1}^{n} z_{k} w_{k}$ for two vectors $\\boldsymbol{z}, \\boldsymbol{w} \\in \\mathbb{C}^{n}$.\n\nFirst, we must validate the problem statement.\n\n**Step 1: Extract Givens**\n-   The operation to be computed is the complex dot product: $s = \\sum_{k=1}^{n} z_{k}\\,w_{k}$.\n-   The input vectors are $\\boldsymbol{z}, \\boldsymbol{w} \\in \\mathbb{C}^{n}$, where $n$ is the length.\n-   The computational model is the real-floating-point-operation (flop) model.\n-   The cost of one real addition is $1$ flop.\n-   The cost of one real multiplication is $1$ flop.\n-   The cost of one complex multiplication is specified as $6$ flops. This is consistent with the standard implementation $(a+ib)(c+id) = (ac-bd)+i(ad+bc)$, which requires $4$ real multiplications and $2$ real additions/subtractions, totaling $6$ flops.\n-   The cost of one complex addition is specified as $2$ flops. This is consistent with $(a+ib)+(c+id) = (a+c)+i(b+d)$, which requires $2$ real additions.\n-   Factors to be ignored: memory traffic, loop overhead, and instruction-level fusion.\n-   The algorithm to be used is an accumulation strategy: initialize an accumulator with a single complex product, then accumulate the remaining terms.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, as it deals with a standard operation in numerical linear algebra and uses a standard model for computational cost analysis (flop counting). The provided costs for complex arithmetic are correct and standard. The problem is well-posed, with all necessary information provided to derive a unique solution. The language is objective and precise. The problem is not trivial, as it requires a careful breakdown of the specified algorithm into its constituent operations. Therefore, the problem is deemed valid.\n\n**Step 3: Verdict and Action**\nThe problem is valid. We proceed with the solution.\n\nThe computation of $s$ is given by the sum:\n$$\ns = z_1 w_1 + z_2 w_2 + \\dots + z_n w_n\n$$\nThe problem specifies a particular accumulation strategy. Let's analyze this algorithm for $n \\ge 1$:\n\n1.  **Initialization**: The accumulator is initialized with the first term of the sum.\n    -   Compute the product $p_1 = z_1 w_1$. This is one complex multiplication.\n    -   The accumulator, let's call it $s_{acc}$, is set to $s_{acc} \\leftarrow p_1$.\n\n2.  **Accumulation Loop**: The remaining $n-1$ terms are accumulated. This is done by iterating from $k=2$ to $k=n$.\n    -   For each $k$ in $\\{2, 3, \\dots, n\\}$:\n        a. Compute the product $p_k = z_k w_k$. This is one complex multiplication.\n        b. Add this product to the accumulator: $s_{acc} \\leftarrow s_{acc} + p_k$. This is one complex addition.\n\nWe can now count the total number of complex operations for a vector of length $n$.\n-   **Complex Multiplications**: There is one complex multiplication for the initialization ($k=1$) and one complex multiplication in each of the $n-1$ iterations of the loop (for $k=2, \\dots, n$). The total number of complex multiplications is $1 + (n-1) = n$.\n-   **Complex Additions**: There are no complex additions during initialization. There is one complex addition in each of the $n-1$ iterations of the loop. The total number of complex additions is $n-1$.\n\nNote that for the base case $n=1$, the sum is just $s = z_1 w_1$. The algorithm performs the initialization step (one complex multiplication) and the loop (from $k=2$ to $1$) does not execute. This results in $1$ complex multiplication and $0$ complex additions, which is consistent with our formulas $n$ and $n-1$ for $n=1$. The problem implies $n \\ge 1$ as the summation index starts at $k=1$.\n\nNow, we convert the counts of complex operations into a total flop count, which we denote as $F(n)$. Let $C_{mult}$ be the cost of a complex multiplication and $C_{add}$ be the cost of a complex addition.\n-   $C_{mult} = 6$ flops\n-   $C_{add} = 2$ flops\n\nThe total flop count is the sum of the costs of all operations:\n$$\nF(n) = (\\text{Number of complex multiplications}) \\times C_{mult} + (\\text{Number of complex additions}) \\times C_{add}\n$$\nSubstituting the counts and costs:\n$$\nF(n) = n \\cdot C_{mult} + (n-1) \\cdot C_{add}\n$$\n$$\nF(n) = n \\cdot 6 + (n-1) \\cdot 2\n$$\nNow, we simplify this expression to obtain a closed form in terms of $n$:\n$$\nF(n) = 6n + 2n - 2\n$$\n$$\nF(n) = 8n - 2\n$$\nThis formula gives the total flop count for computing the complex dot product for vectors of length $n \\ge 1$ using the specified algorithm and cost model.", "answer": "$$\n\\boxed{8n - 2}\n$$", "id": "3538838"}, {"introduction": "We now apply our cost-analysis skills to a large-scale, practical scenario common in computational science: solving a system of linear equations for many different conditions. This problem [@problem_id:2160772] demonstrates the crucial trade-off between the one-time, expensive cost of matrix factorization and the cheap cost of using that factorization for subsequent solves. Understanding this balance is key to designing efficient computational workflows for real-world modeling and simulation.", "problem": "A team of computational scientists is modeling a complex physical system where the system's state, represented by a vector $x$, is determined by a system of linear equations $Ax=b$. The matrix $A$, which is of size $600 \\times 600$, describes the fixed internal interactions of the system and does not change. The team needs to analyze the system's response to 100 different external conditions, each represented by a unique right-hand side vector $b$.\n\nTo solve this problem efficiently, they first compute the Lower-Upper (LU) factorization of the matrix $A$. The approximate number of floating-point operations (flops) required for the LU factorization of an $N \\times N$ matrix is given by the formula $\\frac{2}{3}N^3$.\n\nOnce the factorization $A=LU$ is obtained, finding the solution $x$ for any given $b$ is a two-step process: first solving the lower triangular system $Ly=b$ (forward substitution), and then solving the upper triangular system $Ux=y$ (backward substitution). The approximate number of flops required to solve a single $N \\times N$ triangular system (either lower or upper) is $N^2$.\n\nCalculate the total number of floating-point operations required to find the solutions for all 100 different external conditions. This calculation must include the one-time cost of performing the LU factorization.", "solution": "Let $N=600$ denote the matrix dimension and let $R=100$ denote the number of distinct right-hand sides.\n\nThe LU factorization cost for an $N \\times N$ matrix is given by\n$$\n\\frac{2}{3}N^{3}.\n$$\nThe cost to solve one lower triangular system and one upper triangular system is $N^{2}+N^{2}=2N^{2}$ per right-hand side. For $R$ right-hand sides, this totals\n$$\n2RN^{2}.\n$$\nTherefore, the total flop count is\n$$\n\\frac{2}{3}N^{3}+2RN^{2}.\n$$\nSubstituting $N=600$ and $R=100$,\n$$\n\\frac{2}{3}\\cdot 600^{3}+2\\cdot 100 \\cdot 600^{2}.\n$$\nCompute each term exactly:\n$$\n600^{3}=(6\\cdot 10^{2})^{3}=216\\cdot 10^{6}=216{,}000{,}000,\n$$\nso\n$$\n\\frac{2}{3}\\cdot 600^{3}=\\frac{2}{3}\\cdot 216{,}000{,}000=144{,}000{,}000.\n$$\nAlso,\n$$\n600^{2}=360{,}000,\\quad 2\\cdot 100 \\cdot 600^{2}=200\\cdot 360{,}000=72{,}000{,}000.\n$$\nAdding both contributions,\n$$\n144{,}000{,}000+72{,}000{,}000=216{,}000{,}000.\n$$\nThus, the total number of floating-point operations is $216{,}000{,}000$.", "answer": "$$\\boxed{216000000}$$", "id": "2160772"}]}