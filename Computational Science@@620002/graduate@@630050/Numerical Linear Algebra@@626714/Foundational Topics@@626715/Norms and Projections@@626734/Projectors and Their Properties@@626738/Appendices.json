{"hands_on_practices": [{"introduction": "Projectors are fundamentally defined by their geometric action, mapping vectors onto a target subspace along a specified direction. For computation and analysis, however, we need an explicit matrix representation. This first practice challenges you to bridge this gap by deriving the well-known formula for an oblique projector directly from its core geometric properties: its range and its null space [@problem_id:3567668].", "problem": "Let $A \\in \\mathbb{R}^{m \\times n}$ have full column rank $n$ and let $C \\in \\mathbb{R}^{n \\times m}$ have full row rank $n$. Assume that $C A \\in \\mathbb{R}^{n \\times n}$ is invertible. Define an operator $P \\in \\mathbb{R}^{m \\times m}$ that is a projector in the sense that $P^{2} = P$, with the additional obliqueness conditions $\\operatorname{range}(P) = \\operatorname{range}(A)$ and $\\operatorname{null}(P) = \\operatorname{null}(C)$. Starting only from the core definitions of range, null space, projector, and bijectivity on subspaces, derive an explicit closed-form expression for $P$ in terms of $A$ and $C$.\n\nThen, starting from the definition of a generalized inverse $G$ of a matrix $M$ as any matrix satisfying $M G M = M$, and a reflexive generalized inverse as one additionally satisfying $G M G = G$, relate the projector $P$ to generalized inverses of $C$ by identifying a specific reflexive generalized inverse $G$ of $C$ such that $P = G C$. Justify the reflexivity of your identified $G$ using only these defining properties.\n\nProvide, as your final answer, the single analytic expression for the projector $P$ you have derived. No numerical approximation is required.", "solution": "The problem is well-posed. The conditions provided are sufficient to uniquely determine the projector $P$. Specifically, the space $\\mathbb{R}^m$ can be decomposed as the direct sum of the specified range and null space, $\\mathbb{R}^m = \\operatorname{range}(A) \\oplus \\operatorname{null}(C)$. This is because for any vector $v \\in \\operatorname{range}(A) \\cap \\operatorname{null}(C)$, we have $v = Ax$ for some $x \\in \\mathbb{R}^n$ and $Cv = 0$. Substituting the first into the second gives $C(Ax) = (CA)x = 0$. Since $CA$ is invertible, it must be that $x=0$, which implies $v = A0 = 0$. Thus, the intersection is trivial, and the direct sum is valid.\n\nWe now derive the explicit form of the projector $P$. By definition, a projector $P$ maps any vector $y \\in \\mathbb{R}^m$ to its component in the range of $P$, along the null space of $P$. Any vector $y \\in \\mathbb{R}^m$ has a unique decomposition $y = u + v$, where $u \\in \\operatorname{range}(P)$ and $v \\in \\operatorname{null}(P)$. The action of the projector is defined as $Py = u$.\n\nAccording to the problem statement, we have $\\operatorname{range}(P) = \\operatorname{range}(A)$ and $\\operatorname{null}(P) = \\operatorname{null}(C)$. Therefore, for any $y \\in \\mathbb{R}^m$, we can write $y = u + v$ where $u \\in \\operatorname{range}(A)$ and $v \\in \\operatorname{null}(C)$.\nThe projected vector is $Py = u$.\n\nSince $u \\in \\operatorname{range}(A)$, there exists a vector $x \\in \\mathbb{R}^n$ such that $u = Ax$. Because $A$ has full column rank $n$, the columns of $A$ are linearly independent, and thus for any given $u \\in \\operatorname{range}(A)$, the vector $x$ is unique. Our goal is to find an expression for $x$ in terms of the known vector $y$.\n\nWe start with the decomposition $y = Ax + v$. We can apply the matrix $C$ to both sides of this equation:\n$$Cy = C(Ax + v)$$\nBy the linearity of matrix multiplication, this becomes:\n$$Cy = C(Ax) + Cv$$\nFrom the problem definition, $v \\in \\operatorname{null}(C)$, which means $Cv = 0$. The equation simplifies to:\n$$Cy = (CA)x$$\nWe are given that the square matrix $CA \\in \\mathbb{R}^{n \\times n}$ is invertible. Therefore, we can solve for $x$ by pre-multiplying by the inverse, $(CA)^{-1}$:\n$$x = (CA)^{-1}Cy$$\nNow we can substitute this expression for $x$ back into the equation for the projected vector, $Py = Ax$:\n$$Py = A \\left((CA)^{-1}Cy\\right)$$\n$$Py = [A(CA)^{-1}C]y$$\nSince this equation holds for any arbitrary vector $y \\in \\mathbb{R}^m$, the expression in the brackets must be the matrix representation of the projector $P$.\n$$P = A(CA)^{-1}C$$\nTo be thorough, we can verify that this $P$ satisfies the defining property of a projector, $P^2 = P$:\n$$P^2 = \\left(A(CA)^{-1}C\\right) \\left(A(CA)^{-1}C\\right) = A(CA)^{-1}(CA)(CA)^{-1}C = A(CA)^{-1}I_n(CA)^{-1}C = A(CA)^{-1}C = P$$\nwhere $I_n$ is the $n \\times n$ identity matrix.\n\nNext, we relate this projector to a generalized inverse of $C$. A matrix $G$ is a generalized inverse of a matrix $M$ if $MGM=M$. It is a reflexive generalized inverse if, in addition, $GMG=G$. We are looking for a specific reflexive generalized inverse $G$ of $C$ such that $P = GC$.\n\nComparing our derived expression $P = A(CA)^{-1}C$ with the form $P=GC$, we can identify $G$ as:\n$$G = A(CA)^{-1}$$\nNow we must verify that this $G$ is a reflexive generalized inverse of $C$.\n\nFirst, we check the generalized inverse condition, $CGC = C$:\n$$CGC = C \\left( A(CA)^{-1} \\right) C$$\nUsing the associativity of matrix multiplication, we can group the terms as:\n$$CGC = (CA)(CA)^{-1}C$$\nSince $(CA)(CA)^{-1} = I_n$, the expression becomes:\n$$CGC = I_n C = C$$\nThis condition is satisfied.\n\nSecond, we check the reflexive condition, $GCG = G$:\n$$GCG = \\left( A(CA)^{-1} \\right) C \\left( A(CA)^{-1} \\right)$$\nAgain, using associativity, we group the middle terms:\n$$GCG = A(CA)^{-1} \\left( C A(CA)^{-1} \\right)$$\nWe can regroup the terms within the parenthesis as $C A (CA)^{-1} = (CA)(CA)^{-1} = I_n$. However, a more direct path is to recognize the sub-expression $CG$:\n$$CG = C \\left(A(CA)^{-1}\\right) = (CA)(CA)^{-1} = I_n$$\nSo $G$ is a right inverse of $C$. Substituting $CG=I_n$ back into the reflexive condition check:\n$$GCG = G(CG) = G I_n = G$$\nThis condition is also satisfied.\n\nThus, we have shown that $G = A(CA)^{-1}$ is a reflexive generalized inverse of $C$, and the projector $P$ can be expressed as $P = GC$. The explicit closed-form expression for the projector $P$ remains $P = A(CA)^{-1}C$.", "answer": "$$\n\\boxed{A(CA)^{-1}C}\n$$", "id": "3567668"}, {"introduction": "A powerful application of projection theory is to characterize and compute with multiple subspaces simultaneously. This practice guides you through the construction of the orthogonal projector onto the intersection of two subspaces, $\\mathcal{U} \\cap \\mathcal{V}$. You will explore a direct algebraic method based on orthogonal complements and contrast it with the celebrated Dykstra's algorithm, a purely geometric iterative approach [@problem_id:3567687].", "problem": "You are given two linear subspaces $\\mathcal{U} \\subset \\mathbb{R}^n$ and $\\mathcal{V} \\subset \\mathbb{R}^n$, each specified by a matrix of basis vectors as columns. Your task is to design a program that, for each test case below, constructs the orthogonal projector onto the intersection $\\mathcal{U} \\cap \\mathcal{V}$ by reasoning from first principles, and compares it to a projector obtained via an iterative scheme based on Dykstra’s algorithm for alternating projections.\n\nFundamental definitions and facts that you may use as the starting point:\n- A matrix with orthonormal columns $Q \\in \\mathbb{R}^{n \\times k}$ spans a subspace $\\mathcal{S} = \\operatorname{range}(Q) \\subset \\mathbb{R}^n$. The orthogonal projector onto $\\mathcal{S}$ is the linear map $P_{\\mathcal{S}} : \\mathbb{R}^n \\to \\mathbb{R}^n$ satisfying $P_{\\mathcal{S}}^2 = P_{\\mathcal{S}}$ and $P_{\\mathcal{S}}^\\top = P_{\\mathcal{S}}$, and $P_{\\mathcal{S}} x$ is the unique closest point to $x$ in $\\mathcal{S}$ with respect to the Euclidean norm. \n- For a subspace $\\mathcal{S} \\subset \\mathbb{R}^n$, the orthogonal complement is $\\mathcal{S}^\\perp = \\{ x \\in \\mathbb{R}^n : x^\\top y = 0 \\ \\forall y \\in \\mathcal{S} \\}$. The direct-sum relation $(\\mathcal{U} \\cap \\mathcal{V})^\\perp = \\mathcal{U}^\\perp + \\mathcal{V}^\\perp$ holds for all subspaces $\\mathcal{U}, \\mathcal{V} \\subset \\mathbb{R}^n$. \n- The Moore–Penrose pseudoinverse is a linear-algebraic operator that generalizes inversion to rank-deficient matrices and can be defined via Singular Value Decomposition (SVD). It yields the orthogonal projector onto a column space when multiplied as $A A^{+}$ for any matrix $A$.\n\nYour program must:\n1. From the provided basis matrices for $\\mathcal{U}$ and $\\mathcal{V}$, construct numerically stable orthonormal bases for $\\mathcal{U}$ and $\\mathcal{V}$ using a robust procedure grounded in the Singular Value Decomposition (SVD), discarding directions with singular values below a user-chosen tolerance.\n2. Construct orthonormal bases for $\\mathcal{U}^\\perp$ and $\\mathcal{V}^\\perp$ (the orthogonal complements), for example by computing the null spaces of $U^\\top$ and $V^\\top$ using a method based on the Singular Value Decomposition.\n3. Using only the fundamental facts above (do not assume any closed-form formula not derivable from these), derive and implement a numerically stable expression for the orthogonal projector onto $\\mathcal{U} \\cap \\mathcal{V}$ in terms of the orthogonal projector onto $\\mathcal{U}^\\perp + \\mathcal{V}^\\perp$ and the Moore–Penrose pseudoinverse.\n4. Implement the two-set Dykstra alternating projections procedure to project an arbitrary vector $x \\in \\mathbb{R}^n$ onto $\\mathcal{U} \\cap \\mathcal{V}$ using only the orthogonal projectors onto $\\mathcal{U}$ and onto $\\mathcal{V}$. To compare a linear operator against the closed-form projector, apply the iterative projection to each standard basis vector to assemble an approximate projector matrix column-by-column. Use a stopping tolerance of $10^{-12}$ in Euclidean norm with a maximum of $2000$ iterations.\n5. For each test case, compute:\n   - The Frobenius norm of the difference between the closed-form projector and the Dykstra-assembled projector, which should be a nonnegative real number.\n   - The dimension of $\\mathcal{U} \\cap \\mathcal{V}$, computed as an integer using an appropriate numerically stable method grounded in projector properties.\n   - The Frobenius-norm idempotence residual of the closed-form projector, namely $\\lVert P^2 - P \\rVert_F$, which should be a nonnegative real number.\n\nTest suite:\nProvide results for the following five test cases. In each, the columns of the matrices are basis vectors for the subspaces; convert them to orthonormal bases before forming projectors.\n\n- Case $1$ ($n = 5$): \n  - $\\mathcal{U}$ is spanned by the columns of $U_1 = [u_{1,1}\\ u_{1,2}]$ with $u_{1,1} = [1,1,0,0,0]^\\top$ and $u_{1,2} = [0,0,1,0,0]^\\top$.\n  - $\\mathcal{V}$ is spanned by the columns of $V_1 = [v_{1,1}\\ v_{1,2}\\ v_{1,3}]$ with $v_{1,1} = [1,0,0,0,0]^\\top$, $v_{1,2} = [0,1,1,0,0]^\\top$, and $v_{1,3} = [0,0,0,1,0]^\\top$.\n\n- Case $2$ ($n = 4$):\n  - $\\mathcal{U}$ is spanned by the columns of $U_2 = [e_1\\ e_2]$ where $e_i$ denotes the $i$-th column of the identity in $\\mathbb{R}^4$.\n  - $\\mathcal{V}$ is spanned by the columns of $V_2 = [e_1\\ e_2\\ e_3]$ in $\\mathbb{R}^4$.\n\n- Case $3$ ($n = 3$):\n  - $\\mathcal{U}$ is spanned by the columns of $U_3 = [e_1]$ in $\\mathbb{R}^3$.\n  - $\\mathcal{V}$ is spanned by the columns of $V_3 = [e_2]$ in $\\mathbb{R}^3$.\n\n- Case $4$ ($n = 3$):\n  - $\\mathcal{U}$ is spanned by the columns of $U_4 = [e_1\\ e_2\\ e_3]$ in $\\mathbb{R}^3$.\n  - $\\mathcal{V}$ is spanned by the columns of $V_4 = [e_1\\ e_2\\ e_3]$ in $\\mathbb{R}^3$.\n\n- Case $5$ ($n = 3$):\n  - $\\mathcal{U}$ is spanned by the columns of $U_5 = [e_1\\ e_2]$ in $\\mathbb{R}^3$.\n  - $\\mathcal{V}$ is spanned by the columns of $V_5 = [w_1\\ w_2]$ with $w_1 = [1,\\varepsilon,0]^\\top$ and $w_2 = [0,1,0]^\\top$ for $\\varepsilon = 10^{-8}$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list of sublists, one per test case, in the order of cases $1$ through $5$. For each case, output the sublist $[e, d, i]$ where $e$ is the Frobenius norm difference between the two projector constructions (a float), $d$ is the computed dimension of $\\mathcal{U} \\cap \\mathcal{V}$ (an integer), and $i$ is the Frobenius-norm idempotence residual of the closed-form projector (a float). For example, a valid output shape is $[[e_1,d_1,i_1],[e_2,d_2,i_2],\\dots,[e_5,d_5,i_5]]$ with numeric values in place of symbols. No additional text should be printed.", "solution": "The objective is to construct and compare two representations of the orthogonal projector onto the intersection of two linear subspaces, $\\mathcal{U} \\cap \\mathcal{V} \\subset \\mathbb{R}^n$. The first method is a closed-form expression derived from fundamental principles of linear algebra. The second is an iterative operator constructed using Dykstra's alternating projection algorithm.\n\nLet the two subspaces be $\\mathcal{U}$ and $\\mathcal{V}$, specified by matrices $U_{basis} \\in \\mathbb{R}^{n \\times k_U}$ and $V_{basis} \\in \\mathbb{R}^{n \\times k_V}$ whose columns form bases for $\\mathcal{U}$ and $\\mathcal{V}$ respectively. The solution procedure is as follows:\n\n**1. Numerically Stable Orthonormal Basis Construction**\n\nA robust method for finding an orthonormal basis for a subspace $\\mathcal{S} = \\operatorname{range}(A)$ from a matrix of spanning vectors $A \\in \\mathbb{R}^{n \\times k}$ is the Singular Value Decomposition (SVD). The SVD of $A$ is given by $A = S \\Sigma V^\\top$, where $S \\in \\mathbb{R}^{n \\times n}$ and $V \\in \\mathbb{R}^{k \\times k}$ are orthogonal matrices, and $\\Sigma \\in \\mathbb{R}^{n \\times k}$ is a rectangular diagonal matrix with non-negative real numbers on the diagonal, known as singular values.\n\nThe rank $r$ of the matrix $A$ is the number of its singular values that are greater than a small tolerance $\\tau > 0$. The first $r$ columns of $S$ form an orthonormal basis for the column space of $A$, i.e., for $\\mathcal{S}$. Let this basis matrix be $Q_\\mathcal{S} = S[:, 1:r]$. The remaining $n-r$ columns of $S$, denoted $Q_{\\mathcal{S}^\\perp} = S[:, (r+1):n]$, form an orthonormal basis for the orthogonal complement of the column space, $\\mathcal{S}^\\perp$, which is equivalent to the null space of $A^\\top$.\n\nThis procedure is applied to both $U_{basis}$ and $V_{basis}$ to obtain orthonormal bases $Q_\\mathcal{U}$ for $\\mathcal{U}$ and $Q_\\mathcal{V}$ for $\\mathcal{V}$, as well as bases $Q_{\\mathcal{U}^\\perp}$ for $\\mathcal{U}^\\perp$ and $Q_{\\mathcal{V}^\\perp}$ for $\\mathcal{V}^\\perp$. For numerical stability and reproducibility, a tolerance $\\tau = 10^{-12}$ is used.\n\n**2. Closed-Form Projector for the Intersection $\\mathcal{U} \\cap \\mathcal{V}$**\n\nThe derivation of the orthogonal projector onto $\\mathcal{U} \\cap \\mathcal{V}$, denoted $P_{\\mathcal{U} \\cap \\mathcal{V}}$, relies on the following fundamental properties:\n- For any subspace $\\mathcal{S}$, the projector onto its orthogonal complement is $P_{\\mathcal{S}^\\perp} = I - P_\\mathcal{S}$, where $I$ is the identity matrix.\n- De Morgan's law for subspaces states that $(\\mathcal{U} \\cap \\mathcal{V})^\\perp = \\mathcal{U}^\\perp + \\mathcal{V}^\\perp$.\n\nFrom the first property, the projector onto the intersection $\\mathcal{W} = \\mathcal{U} \\cap \\mathcal{V}$ can be expressed in terms of the projector onto its complement $\\mathcal{W}^\\perp$:\n$$P_{\\mathcal{U} \\cap \\mathcal{V}} = I - P_{(\\mathcal{U} \\cap \\mathcal{V})^\\perp}$$\nUsing the second property, we can substitute the expression for the complement:\n$$P_{\\mathcal{U} \\cap \\mathcal{V}} = I - P_{\\mathcal{U}^\\perp + \\mathcal{V}^\\perp}$$\nThe sum of two subspaces $\\mathcal{S}_1 + \\mathcal{S}_2$ is the space spanned by the union of their bases. If $Q_1$ and $Q_2$ are matrices whose columns form orthonormal bases for $\\mathcal{S}_1$ and $\\mathcal{S}_2$ respectively, then $\\mathcal{S}_1 + \\mathcal{S}_2 = \\operatorname{range}([Q_1, Q_2])$. Let $C = [Q_{\\mathcal{U}^\\perp}, Q_{\\mathcal{V}^\\perp}]$ be the matrix formed by concatenating the basis vectors for $\\mathcal{U}^\\perp$ and $\\mathcal{V}^\\perp$.\nThe orthogonal projector onto the range of any matrix $A$ is given by $A A^{+}$, where $A^{+}$ is the Moore-Penrose pseudoinverse of $A$. Therefore, the projector onto $\\mathcal{U}^\\perp + \\mathcal{V}^\\perp$ is:\n$$P_{\\mathcal{U}^\\perp + \\mathcal{V}^\\perp} = C C^{+}$$\nCombining these results yields the final closed-form expression for the projector onto the intersection:\n$$P_{\\mathcal{U} \\cap \\mathcal{V}} = I - [Q_{\\mathcal{U}^\\perp}, Q_{\\mathcal{V}^\\perp}] [Q_{\\mathcal{U}^\\perp}, Q_{\\mathcal{V}^\\perp}]^{+}$$\nThis constitutes the primary, or \"closed-form\", projector, which we denote $P_{CF}$.\n\n**3. Iterative Projector via Dykstra's Algorithm**\n\nDykstra's algorithm is an iterative method for finding the projection of a vector onto the intersection of two or more convex sets. Since subspaces are convex, it can be applied to find $P_{\\mathcal{U} \\cap \\mathcal{V}}x$ for any vector $x \\in \\mathbb{R}^n$. For two subspaces $\\mathcal{U}$ and $\\mathcal{V}$ with projectors $P_\\mathcal{U}$ and $P_\\mathcal{V}$, the algorithm to project a point $x$ is:\nInitialize: $y_0 = x$, and correction terms $p_0 = \\mathbf{0}$, $q_0 = \\mathbf{0}$.\nFor $k=1, 2, \\dots$:\n1. $x_k = P_\\mathcal{U}(y_{k-1} + p_{k-1})$\n2. $p_k = (y_{k-1} + p_{k-1}) - x_k$\n3. $y_k = P_\\mathcal{V}(x_k + q_{k-1})$\n4. $q_k = (x_k + q_{k-1}) - y_k$\nThe sequence $y_k$ converges to $P_{\\mathcal{U} \\cap \\mathcal{V}}x$. Iteration proceeds until the change $\\lVert y_k - y_{k-1} \\rVert_2$ falls below a tolerance of $10^{-12}$, or a maximum of $2000$ iterations is reached.\n\nTo construct the full projector matrix, $P_{Dykstra}$, this procedure is applied to each of the $n$ standard basis vectors $e_i \\in \\mathbb{R}^n$. The resulting projected vector forms the $i$-th column of $P_{Dykstra}$:\n$$P_{Dykstra} = [ \\operatorname{Dykstra}(e_1), \\operatorname{Dykstra}(e_2), \\dots, \\operatorname{Dykstra}(e_n) ]$$\nwhere $\\operatorname{Dykstra}(x)$ denotes the output of the algorithm for input $x$.\n\n**4. Evaluation Metrics**\n\nFor each test case, three quantities are computed to evaluate and compare the projectors:\n1.  **Frobenius Norm Difference ($e$)**: This measures the discrepancy between the two constructed projectors, $e = \\lVert P_{CF} - P_{Dykstra} \\rVert_F$. This value should be close to zero, reflecting the convergence of Dykstra's algorithm.\n2.  **Dimension of Intersection ($d$)**: For an orthogonal projector $P$, its rank equals its trace, $\\operatorname{rank}(P) = \\operatorname{tr}(P)$, which gives the dimension of the subspace it projects onto. Thus, the dimension of the intersection is calculated as $d = \\operatorname{dim}(\\mathcal{U} \\cap \\mathcal{V}) = \\operatorname{tr}(P_{CF})$. Since numerical computations are involved, the result is rounded to the nearest integer.\n3.  **Idempotence Residual ($i$)**: A matrix $P$ is a projector if and only if it is idempotent, i.e., $P^2 = P$. The Frobenius norm of the residual, $i = \\lVert P_{CF}^2 - P_{CF} \\rVert_F$, measures how closely the computed closed-form projector satisfies this fundamental property. This value should be near machine precision.", "answer": "```python\nimport numpy as np\n\ndef get_orthonormal_bases(A, tol):\n    \"\"\"\n    Computes orthonormal bases for the column space of A and its orthogonal complement.\n    Uses SVD for numerical stability.\n    \"\"\"\n    if A.size == 0 or A.shape[1] == 0:\n        return np.empty((A.shape[0], 0)), np.identity(A.shape[0])\n\n    U, s, _ = np.linalg.svd(A, full_matrices=True)\n    rank = np.sum(s > tol)\n    \n    Q = U[:, :rank]\n    Q_perp = U[:, rank:]\n    return Q, Q_perp\n\ndef get_intersection_projector(Q_U_perp, Q_V_perp, n, tol):\n    \"\"\"\n    Constructs the orthogonal projector onto the intersection U_cap_V\n    using the formula P = I - P_{U_perp + V_perp}.\n    \"\"\"\n    # Concatenate the bases for the orthogonal complements\n    if Q_U_perp.shape[1] == 0 and Q_V_perp.shape[1] == 0:\n        C = np.empty((n, 0))\n    elif Q_U_perp.shape[1] == 0:\n        C = Q_V_perp\n    elif Q_V_perp.shape[1] == 0:\n        C = Q_U_perp\n    else:\n        C = np.hstack((Q_U_perp, Q_V_perp))\n\n    # Projector onto the sum of the complements\n    if C.shape[1] == 0:\n        # Sum of complements is the {0} subspace, projector is zero matrix\n        P_sum_perp = np.zeros((n, n))\n    else:\n        # P = A A^+, numerically stable via pinv\n        P_sum_perp = C @ np.linalg.pinv(C, rcond=tol)\n    \n    # P_intersection = I - P_sum_perp\n    P_CF = np.identity(n) - P_sum_perp\n    return P_CF\n\ndef dykstra_projection(x, P_U, P_V, tol, max_iter):\n    \"\"\"\n    Projects a vector x onto the intersection of two subspaces U and V\n    using Dykstra's alternating projection algorithm.\n    \"\"\"\n    y = x.copy()\n    p = np.zeros_like(x)\n    q = np.zeros_like(x)\n\n    for _ in range(max_iter):\n        y_prev = y\n        \n        x_k = P_U @ (y_prev + p)\n        p = (y_prev + p) - x_k\n\n        y = P_V @ (x_k + q)\n        q = (x_k + q) - y\n\n        if np.linalg.norm(y - y_prev) < tol:\n            break\n            \n    return y\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and produce the final output.\n    \"\"\"\n    epsilon = 1e-8\n    e1_3, e2_3, e3_3 = np.identity(3).T\n    e1_4, e2_4, e3_4, e4_4 = np.identity(4).T\n\n    test_cases = [\n        # Case 1 (n=5)\n        (np.array([[1, 0], [1, 0], [0, 1], [0, 0], [0, 0]]), \n         np.array([[1, 0, 0], [0, 1, 0], [0, 1, 0], [0, 0, 1], [0, 0, 0]])),\n        # Case 2 (n=4)\n        (np.vstack([e1_4, e2_4]).T, np.vstack([e1_4, e2_4, e3_4]).T),\n        # Case 3 (n=3)\n        (e1_3.reshape(-1, 1), e2_3.reshape(-1, 1)),\n        # Case 4 (n=3)\n        (np.identity(3), np.identity(3)),\n        # Case 5 (n=3), with epsilon\n        (np.vstack([e1_3, e2_3]).T, np.array([[1, 0], [epsilon, 1], [0, 0]]))\n    ]\n\n    SVD_TOL = 1e-12\n    DYKSTRA_TOL = 1e-12\n    DYKSTRA_MAX_ITER = 2000\n\n    results = []\n\n    for U_basis, V_basis in test_cases:\n        n = U_basis.shape[0]\n\n        # 1 & 2. Get orthonormal bases for subspaces and their complements\n        Q_U, Q_U_perp = get_orthonormal_bases(U_basis, SVD_TOL)\n        Q_V, Q_V_perp = get_orthonormal_bases(V_basis, SVD_TOL)\n        \n        # 3. Construct the closed-form projector onto the intersection\n        P_CF = get_intersection_projector(Q_U_perp, Q_V_perp, n, SVD_TOL)\n\n        # 4. Construct projector using Dykstra's algorithm\n        P_U = Q_U @ Q_U.T\n        P_V = Q_V @ Q_V.T\n        P_Dykstra = np.zeros((n, n))\n        for i in range(n):\n            e_i = np.zeros(n)\n            e_i[i] = 1.0\n            col = dykstra_projection(e_i, P_U, P_V, DYKSTRA_TOL, DYKSTRA_MAX_ITER)\n            P_Dykstra[:, i] = col\n\n        # 5. Compute the required metrics\n        # Frobenius norm difference\n        e = np.linalg.norm(P_CF - P_Dykstra, 'fro')\n        \n        # Dimension of intersection (rank of projector = trace of projector)\n        d = int(np.round(np.trace(P_CF)))\n        \n        # Idempotence residual of the closed-form projector\n        i = np.linalg.norm(P_CF @ P_CF - P_CF, 'fro')\n\n        results.append([e, d, i])\n    \n    # Format and print the final output\n    print(str(results).replace(\" \", \"\"))\n\nsolve()\n```", "id": "3567687"}, {"introduction": "In practice, a matrix that is theoretically an orthogonal projector can lose its defining properties of symmetry and idempotence ($P^2 = P$) due to numerical errors. This final exercise addresses the crucial 'denoising' problem of finding the nearest valid orthogonal projector to such a corrupted matrix. By applying the spectral theorem, you will derive an elegant solution that demonstrates a powerful connection between optimization and matrix decomposition [@problem_id:3567643].", "problem": "Consider a real matrix $P \\in \\mathbb{R}^{n \\times n}$ that arises as a numerically computed approximation to a projector. In finite precision arithmetic, such a $P$ may be slightly non-symmetric and slightly non-idempotent. Let the symmetric part of $P$ be $P_s = \\frac{1}{2}(P + P^{\\top})$. A projector is a matrix $X$ satisfying $X^2 = X$. An orthogonal projector is a projector that is also symmetric, i.e., $X = X^{\\top}$ and $X^2 = X$. The Frobenius norm of a matrix $A$ is $\\|A\\|_F = \\sqrt{\\sum_{i,j} A_{ij}^2}$.\n\nYour task is to design a program that, for each provided test matrix $P$, evaluates the quality of two repair strategies:\n- the simple symmetrization $P_s$, and\n- the optimized repair by the nearest orthogonal projector in Frobenius norm.\n\nThe program must be derived from and implement the following fundamental and well-tested principles without relying on prepackaged shortcut formulas:\n- The spectral theorem for real symmetric matrices: any real symmetric matrix $S$ admits an eigendecomposition $S = Q \\Lambda Q^{\\top}$ with $Q$ orthogonal and $\\Lambda$ real diagonal.\n- The Frobenius norm is unitarily invariant and induced by the inner product $\\langle A, B \\rangle = \\mathrm{trace}(A^{\\top} B)$.\n- If $A = S + K$ with $S = S^{\\top}$ and $K = -K^{\\top}$, then $\\|A\\|_F^2 = \\|S\\|_F^2 + \\|K\\|_F^2$ and $\\langle S, K \\rangle = 0$.\n\nYou must, from these principles, deduce an algorithm to compute the nearest orthogonal projector to a given real matrix $P$ in Frobenius norm, and then compute the following three quantities for each test case:\n- the idempotency defect of the simple symmetrization: $\\|P_s^2 - P_s\\|_F$,\n- the distance from the original matrix to its symmetrization: $\\|P - P_s\\|_F$,\n- the distance from the original matrix to the nearest orthogonal projector $P_{\\star}$ in Frobenius norm: $\\|P - P_{\\star}\\|_F$.\n\nYour program must implement your derived method for computing $P_{\\star}$ using only the listed principles and standard numerical linear algebra operations.\n\nTest Suite:\nCompute the specified three quantities for each of the following explicitly given matrices. All entries are in plain real numbers. Ensure to use the exact numerical values as written.\n\n- Test case $1$ ($4 \\times 4$):\n$$\nP^{(1)} =\n\\begin{bmatrix}\n1 & 0 & 0 & 0.001 \\\\\n0 & 1 & 0 & 0 \\\\\n0 & 0 & 0 & 0 \\\\\n-0.001 & 0 & 0 & 0\n\\end{bmatrix}.\n$$\n\n- Test case $2$ ($4 \\times 4$):\n$$\nP^{(2)} =\n\\begin{bmatrix}\n0.5 & 0 & 0.5 & 0.001 \\\\\n0 & 0.5 & 0.0005 & 0.5 \\\\\n0.5 & -0.0005 & 0.5 & 0 \\\\\n0.001 & 0.5 & 0 & 0.5\n\\end{bmatrix}.\n$$\n\n- Test case $3$ ($4 \\times 4$):\n$$\nP^{(3)} =\n\\begin{bmatrix}\n0.49 & 0.001 & 0 & 0 \\\\\n-0.001 & 0.51 & 0 & 0 \\\\\n0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 1\n\\end{bmatrix}.\n$$\n\n- Test case $4$ ($5 \\times 5$):\n$$\nP^{(4)} =\n\\begin{bmatrix}\n0.5 & 0 & 0 & 0 & 0.0002 \\\\\n0 & 1 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0.5 & 0 \\\\\n-0.0002 & 0 & 0 & 0 & 1\n\\end{bmatrix}.\n$$\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case must yield a list of three floating-point numbers in the order\n$[\\|P_s^2 - P_s\\|_F,\\ \\|P - P_s\\|_F,\\ \\|P - P_{\\star}\\|_F]$,\nand the final output must be a list of these per-test lists in the same order as above. For example, the output must have the form\n$[[a_1,b_1,c_1],[a_2,b_2,c_2],[a_3,b_3,c_3],[a_4,b_4,c_4]]$\nwith no additional text. Angles and physical units are not applicable; report all real numbers as standard decimal floating-point values. The program must not read any input and must be fully deterministic.", "solution": "The problem requires the design of a program to evaluate two methods for repairing a numerically computed matrix $P \\in \\mathbb{R}^{n \\times n}$ that is an approximation of an orthogonal projector. An orthogonal projector $X$ is a matrix that is both symmetric ($X = X^{\\top}$) and idempotent ($X^2 = X$). The two repair strategies are:\n1.  A simple symmetrization, resulting in $P_s = \\frac{1}{2}(P + P^{\\top})$.\n2.  A more sophisticated repair by finding the nearest orthogonal projector $P_{\\star}$ in the Frobenius norm.\n\nThe core of the task is to derive, from first principles, an algorithm to find $P_{\\star}$. This involves solving the optimization problem:\n$$\n\\min_{X} \\|P - X\\|_F \\quad \\text{subject to} \\quad X = X^{\\top} \\text{ and } X^2 = X.\n$$\nThe derivation must utilize the provided principles: the spectral theorem for real symmetric matrices, the properties of the Frobenius norm, and the orthogonality of symmetric and skew-symmetric matrices.\n\nFirst, let us formalize the derivation of the nearest orthogonal projector $P_{\\star}$. We seek to minimize the objective function $f(X) = \\|P - X\\|_F^2$. Every real square matrix $P$ can be uniquely decomposed into its symmetric part $P_s = \\frac{1}{2}(P + P^{\\top})$ and its skew-symmetric part $P_k = \\frac{1}{2}(P - P^{\\top})$, such that $P = P_s + P_k$. The candidate matrix $X$, being an orthogonal projector, must be symmetric. Therefore, the difference $P_s - X$ is also symmetric. We can rewrite the objective function as:\n$$\n\\|P - X\\|_F^2 = \\|(P_s + P_k) - X\\|_F^2 = \\|(P_s - X) + P_k\\|_F^2.\n$$\nThe problem statement provides the principle that for any matrix decomposed into its symmetric part $S$ and skew-symmetric part $K$, the squared Frobenius norm is $\\|S+K\\|_F^2 = \\|S\\|_F^2 + \\|K\\|_F^2$, which follows from the fact that symmetric and skew-symmetric matrices are orthogonal under the Frobenius inner product, i.e., $\\langle S, K \\rangle = \\mathrm{trace}(S^{\\top}K) = 0$. Applying this principle to the expression $(P_s - X) + P_k$, where $(P_s - X)$ is symmetric and $P_k$ is skew-symmetric, we get:\n$$\n\\|(P_s - X) + P_k\\|_F^2 = \\|P_s - X\\|_F^2 + \\|P_k\\|_F^2.\n$$\nThe term $\\|P_k\\|_F^2$ is determined solely by the input matrix $P$ and is constant with respect to the choice of $X$. Therefore, minimizing $\\|P-X\\|_F^2$ is equivalent to minimizing $\\|P_s - X\\|_F^2$. This insight simplifies the problem: the nearest orthogonal projector to $P$ is the same as the nearest orthogonal projector to its symmetric part, $P_s$. Our optimization problem is now:\n$$\n\\min_{X} \\|P_s - X\\|_F \\quad \\text{subject to} \\quad X = X^{\\top} \\text{ and } X^2 = X.\n$$\nThe next step employs the spectral theorem for real symmetric matrices, which states that any such matrix $S$ can be diagonalized by an orthogonal matrix. For $P_s$, we have the eigendecomposition $P_s = Q \\Lambda Q^{\\top}$, where $Q$ is an orthogonal matrix ($Q^{\\top}Q = I$) whose columns are the eigenvectors of $P_s$, and $\\Lambda$ is a real diagonal matrix whose diagonal entries $\\lambda_1, \\ldots, \\lambda_n$ are the corresponding eigenvalues.\nWe use a further principle provided: the Frobenius norm is unitarily invariant. This means $\\|A\\|_F = \\|UAV\\|_F$ for any orthogonal matrices $U$ and $V$. Applying this to our objective function with $U=Q^{\\top}$ and $V=Q$:\n$$\n\\|P_s - X\\|_F^2 = \\|Q \\Lambda Q^{\\top} - X\\|_F^2 = \\|Q^{\\top}(Q \\Lambda Q^{\\top} - X)Q\\|_F^2 = \\|\\Lambda - Q^{\\top}XQ\\|_F^2.\n$$\nLet us define a new variable $Y = Q^{\\top}XQ$. Since $X$ is an orthogonal projector and $Q$ is orthogonal, $Y$ is also an orthogonal projector:\n-   Symmetry: $Y^{\\top} = (Q^{\\top}XQ)^{\\top} = Q^{\\top}X^{\\top}Q = Q^{\\top}XQ = Y$.\n-   Idempotence: $Y^2 = (Q^{\\top}XQ)(Q^{\\top}XQ) = Q^{\\top}X(QQ^{\\top})XQ = Q^{\\top}X^2Q = Q^{\\top}XQ = Y$.\nThe problem is transformed into finding an orthogonal projector $Y$ that minimizes $\\|\\Lambda - Y\\|_F^2$.\n$$\n\\|\\Lambda - Y\\|_F^2 = \\sum_{i,j=1}^{n} (\\Lambda_{ij} - Y_{ij})^2.\n$$\nSince $\\Lambda$ is diagonal, this is $\\sum_{i=1}^{n} (\\lambda_i - Y_{ii})^2 + \\sum_{i \\neq j} Y_{ij}^2$. To minimize this sum, any non-zero off-diagonal elements $Y_{ij}$ would strictly increase the value. Thus, the optimal $Y$ must be a diagonal matrix. A diagonal matrix $Y$ that is also a projector ($Y^2=Y$) must have diagonal entries $Y_{ii}$ that satisfy $Y_{ii}^2 = Y_{ii}$, meaning $Y_{ii} \\in \\{0, 1\\}$.\nThe problem thus reduces to choosing, for each $i$, a value $Y_{ii} \\in \\{0, 1\\}$ that minimizes $(\\lambda_i - Y_{ii})^2$. This is achieved by simple thresholding:\n-   If $\\lambda_i \\ge 0.5$, choosing $Y_{ii}=1$ minimizes the term $(\\lambda_i-1)^2$.\n-   If $\\lambda_i < 0.5$, choosing $Y_{ii}=0$ minimizes the term $(\\lambda_i-0)^2$.\nLet's call this optimal diagonal projector $\\Lambda_{\\star}$. Its diagonal entries are $(\\Lambda_{\\star})_{ii} = 1$ if $\\lambda_i \\ge 0.5$ and $0$ otherwise.\nHaving found the optimal $\\Lambda_{\\star}$, we transform back to find the nearest orthogonal projector $P_{\\star}$ in the original basis:\n$$\nP_{\\star} = Q \\Lambda_{\\star} Q^{\\top}.\n$$\nThe algorithm for computing $P_{\\star}$ from a given matrix $P$ is:\n1.  Compute the symmetric part $P_s = \\frac{1}{2}(P + P^{\\top})$.\n2.  Perform an eigendecomposition of $P_s$ to find its eigenvalues $\\Lambda$ and eigenvectors $Q$, such that $P_s = Q \\Lambda Q^{\\top}$.\n3.  Construct a diagonal matrix $\\Lambda_{\\star}$ by thresholding the eigenvalues: $(\\Lambda_{\\star})_{ii} = 1$ if $\\lambda_i \\ge 0.5$, and $0$ otherwise.\n4.  Compute the nearest orthogonal projector $P_{\\star} = Q \\Lambda_{\\star} Q^{\\top}$.\n\nFor each test matrix $P$, we will compute the following three quantities:\n1.  The idempotency defect of the simple symmetrization: $\\|P_s^2 - P_s\\|_F$. This measures how far $P_s$ is from being idempotent.\n2.  The distance from the original matrix to its symmetrization: $\\|P - P_s\\|_F$. This is simply the norm of the skew-symmetric part, $\\|P_k\\|_F$.\n3.  The distance from the original matrix to the nearest orthogonal projector: $\\|P - P_{\\star}\\|_F$. This is the minimal possible distance from $P$ to any orthogonal projector in the Frobenius norm.\n\nThe following Python code implements this derived procedure for each of the provided test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the projector repair problem for a series of test matrices.\n    For each matrix P, it computes:\n    1. Idempotency defect of the symmetric part Ps: ||Ps^2 - Ps||_F\n    2. Distance from P to its symmetric part: ||P - Ps||_F\n    3. Distance from P to the nearest orthogonal projector P_star: ||P - P_star||_F\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test case 1\n        np.array([\n            [1.0, 0.0, 0.0, 0.001],\n            [0.0, 1.0, 0.0, 0.0],\n            [0.0, 0.0, 0.0, 0.0],\n            [-0.001, 0.0, 0.0, 0.0]\n        ]),\n        # Test case 2\n        np.array([\n            [0.5, 0.0, 0.5, 0.001],\n            [0.0, 0.5, 0.0005, 0.5],\n            [0.5, -0.0005, 0.5, 0.0],\n            [0.001, 0.5, 0.0, 0.5]\n        ]),\n        # Test case 3\n        np.array([\n            [0.49, 0.001, 0.0, 0.0],\n            [-0.001, 0.51, 0.0, 0.0],\n            [0.0, 0.0, 0.0, 0.0],\n            [0.0, 0.0, 0.0, 1.0]\n        ]),\n        # Test case 4\n        np.array([\n            [0.5, 0.0, 0.0, 0.0, 0.0002],\n            [0.0, 1.0, 0.0, 0.0, 0.0],\n            [0.0, 0.0, 0.0, 0.0, 0.0],\n            [0.0, 0.0, 0.0, 0.5, 0.0],\n            [-0.0002, 0.0, 0.0, 0.0, 1.0]\n        ])\n    ]\n\n    results = []\n    for P in test_cases:\n        # Compute the symmetric part Ps\n        Ps = 0.5 * (P + P.T)\n\n        # 1. Idempotency defect of Ps\n        idempotency_defect_ps = np.linalg.norm(Ps @ Ps - Ps, 'fro')\n\n        # 2. Distance from P to Ps\n        dist_p_ps = np.linalg.norm(P - Ps, 'fro')\n\n        # 3. Distance from P to the nearest orthogonal projector P_star\n        # The derivation shows we need to find the eigendecomposition of Ps\n        eigenvalues, Q = np.linalg.eigh(Ps)\n\n        # Threshold the eigenvalues to get the eigenvalues of the nearest projector\n        # in the diagonalized basis\n        lambda_star_diag = np.where(eigenvalues >= 0.5, 1.0, 0.0)\n        Lambda_star = np.diag(lambda_star_diag)\n\n        # Construct the nearest orthogonal projector P_star\n        P_star = Q @ Lambda_star @ Q.T\n        \n        # Compute the distance from P to P_star\n        dist_p_pstar = np.linalg.norm(P - P_star, 'fro')\n        \n        results.append([idempotency_defect_ps, dist_p_ps, dist_p_pstar])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(str(res) for res in results)}]\")\n\nsolve()\n```", "id": "3567643"}]}