{"hands_on_practices": [{"introduction": "The core principle of backward error analysis is that a numerically stable algorithm computes the exact solution to a slightly perturbed problem. This exercise explores the crucial implication of this idea: even if an algorithm is backward stable, the computed solution can be far from the true solution if the underlying problem is ill-conditioned. By analyzing a specific family of matrices [@problem_id:3533480], you will directly quantify how the condition number acts as an amplifier, connecting the small backward error to a potentially large forward error.", "problem": "Let $A(\\epsilon) \\in \\mathbb{R}^{2 \\times 2}$ be the parameterized matrix family\n$$\nA(\\epsilon) \\;=\\; \\begin{pmatrix} 1 & 1 \\\\ 1 & 1+\\epsilon \\end{pmatrix}, \\qquad \\epsilon \\in (0,1),\n$$\nand let $x \\in \\mathbb{R}^{2}$ be the vector $x = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$. Define $b(\\epsilon) := A(\\epsilon)\\,x = \\begin{pmatrix} 2 \\\\ 2+\\epsilon \\end{pmatrix}$. Computations are performed in floating-point arithmetic with unit roundoff $u$, and we use the matrix and vector $\\infty$-norms throughout. Assume that Gaussian elimination with partial pivoting (GEPP) is normwise backward stable for this family in the following sense: the computed solution $\\hat{x}$ satisfies\n$$\n\\bigl(A(\\epsilon) + \\Delta A\\bigr)\\,\\hat{x} \\;=\\; b(\\epsilon), \\qquad \\|\\Delta A\\|_{\\infty} \\;\\le\\; c\\,u\\,\\|A(\\epsilon)\\|_{\\infty},\n$$\nfor some absolute constant $c$ that is independent of $\\epsilon$ and $u$.\n\nStarting only from the definitions of normwise backward stability, the normwise condition number, and first-order perturbation analysis, do the following:\n\n1) Compute $\\|A(\\epsilon)\\|_{\\infty}$, $A(\\epsilon)^{-1}$, and $\\|A(\\epsilon)^{-1}\\|_{\\infty}$ explicitly, and thereby determine the condition number $\\kappa_{\\infty}\\!\\bigl(A(\\epsilon)\\bigr) = \\|A(\\epsilon)\\|_{\\infty}\\,\\|A(\\epsilon)^{-1}\\|_{\\infty}$ as an explicit function of $\\epsilon$.\n\n2) Using the equation that defines $\\hat{x}$ and the exact solution $x$, derive a first-order (in $u$) normwise perturbation bound for the forward error $\\|x - \\hat{x}\\|_{\\infty}$ in terms of $\\|A(\\epsilon)\\|_{\\infty}$, $\\|A(\\epsilon)^{-1}\\|_{\\infty}$, $\\|x\\|_{\\infty}$, and $u$. Then, specialize this bound to the present data to obtain the leading-order asymptotic expression as $\\epsilon \\to 0$ and $u \\to 0$ with $u \\ll \\epsilon$. You may assume that the smallness condition ensuring the convergence of the relevant Neumann series is satisfied in this regime.\n\nReport as your final answer a single closed-form expression for the leading-order term in $\\epsilon$ and $u$ for $\\|x - \\hat{x}\\|_{\\infty}$, simplified as $\\epsilon \\to 0$ and $u \\to 0$ with $u \\ll \\epsilon$. No rounding is required. Express the final answer as an analytic expression without units.", "solution": "The problem is well-posed and scientifically grounded within the field of numerical linear algebra. We shall proceed by first computing the required matrix norms and the condition number, and then deriving the forward error bound and its leading-order asymptotic behavior.\n\nThe matrix family is given by $A(\\epsilon) = \\begin{pmatrix} 1 & 1 \\\\ 1 & 1+\\epsilon \\end{pmatrix}$ for $\\epsilon \\in (0,1)$.\n\nPart 1: Computation of norms and the condition number.\n\nFirst, we compute the $\\infty$-norm of $A(\\epsilon)$. The matrix $\\infty$-norm is defined as the maximum absolute row sum.\nThe absolute sum of the first row is $|1| + |1| = 2$.\nThe absolute sum of the second row is $|1| + |1+\\epsilon|$. Since $\\epsilon \\in (0,1)$, $1+\\epsilon$ is positive, so the sum is $1 + 1+\\epsilon = 2+\\epsilon$.\nThe norm is the maximum of these two values:\n$$\n\\|A(\\epsilon)\\|_{\\infty} \\;=\\; \\max(2, 2+\\epsilon) \\;=\\; 2+\\epsilon\n$$\n\nNext, we compute the inverse of $A(\\epsilon)$. For a $2 \\times 2$ matrix $\\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}$, the inverse is $\\frac{1}{ad-bc} \\begin{pmatrix} d & -b \\\\ -c & a \\end{pmatrix}$.\nThe determinant of $A(\\epsilon)$ is $\\det(A(\\epsilon)) = (1)(1+\\epsilon) - (1)(1) = 1+\\epsilon-1 = \\epsilon$.\nSince $\\epsilon \\in (0,1)$, $\\det(A(\\epsilon)) \\neq 0$, and the inverse exists.\n$$\nA(\\epsilon)^{-1} \\;=\\; \\frac{1}{\\epsilon} \\begin{pmatrix} 1+\\epsilon & -1 \\\\ -1 & 1 \\end{pmatrix} \\;=\\; \\begin{pmatrix} \\frac{1+\\epsilon}{\\epsilon} & -\\frac{1}{\\epsilon} \\\\ -\\frac{1}{\\epsilon} & \\frac{1}{\\epsilon} \\end{pmatrix}\n$$\n\nNow, we compute the $\\infty$-norm of $A(\\epsilon)^{-1}$.\nThe absolute sum of the first row is $|\\frac{1+\\epsilon}{\\epsilon}| + |-\\frac{1}{\\epsilon}| = \\frac{1+\\epsilon}{\\epsilon} + \\frac{1}{\\epsilon} = \\frac{2+\\epsilon}{\\epsilon}$, since $\\epsilon > 0$.\nThe absolute sum of the second row is $|-\\frac{1}{\\epsilon}| + |\\frac{1}{\\epsilon}| = \\frac{1}{\\epsilon} + \\frac{1}{\\epsilon} = \\frac{2}{\\epsilon}$.\nThe norm is the maximum of these two values. Since $\\epsilon>0$, we have $\\frac{2+\\epsilon}{\\epsilon} > \\frac{2}{\\epsilon}$.\n$$\n\\|A(\\epsilon)^{-1}\\|_{\\infty} \\;=\\; \\max\\left(\\frac{2+\\epsilon}{\\epsilon}, \\frac{2}{\\epsilon}\\right) \\;=\\; \\frac{2+\\epsilon}{\\epsilon}\n$$\n\nFinally, we compute the $\\infty$-norm condition number, $\\kappa_{\\infty}(A(\\epsilon))$, which is defined as the product of the norms of the matrix and its inverse.\n$$\n\\kappa_{\\infty}(A(\\epsilon)) \\;=\\; \\|A(\\epsilon)\\|_{\\infty} \\|A(\\epsilon)^{-1}\\|_{\\infty} \\;=\\; (2+\\epsilon) \\left(\\frac{2+\\epsilon}{\\epsilon}\\right) \\;=\\; \\frac{(2+\\epsilon)^2}{\\epsilon}\n$$\nThis completes the first part of the problem.\n\nPart 2: Derivation of the forward error bound and its asymptotic analysis.\n\nWe are given the exact relation $A(\\epsilon)x = b(\\epsilon)$ and the result from the backward stable computation, $(A(\\epsilon) + \\Delta A)\\hat{x} = b(\\epsilon)$. We can equate these two expressions for $b(\\epsilon)$:\n$$\n(A(\\epsilon) + \\Delta A)\\hat{x} \\;=\\; A(\\epsilon)x\n$$\nExpanding the left side and rearranging the terms gives:\n$$\nA(\\epsilon)\\hat{x} + \\Delta A \\hat{x} \\;=\\; A(\\epsilon)x\n$$\n$$\nA(\\epsilon)\\hat{x} - A(\\epsilon)x \\;=\\; -\\Delta A \\hat{x}\n$$\n$$\nA(\\epsilon)(\\hat{x} - x) \\;=\\; -\\Delta A \\hat{x}\n$$\nSince $A(\\epsilon)$ is invertible, we can multiply by $A(\\epsilon)^{-1}$ from the left:\n$$\n\\hat{x} - x \\;=\\; -A(\\epsilon)^{-1} \\Delta A \\hat{x}\n$$\nThis relates the forward error vector, $x - \\hat{x} = A(\\epsilon)^{-1} \\Delta A \\hat{x}$, to the backward error perturbation $\\Delta A$. Taking the $\\infty$-norm on both sides and using the submultiplicative property of norms yields:\n$$\n\\|x - \\hat{x}\\|_{\\infty} \\;=\\; \\|A(\\epsilon)^{-1} \\Delta A \\hat{x}\\|_{\\infty} \\;\\le\\; \\|A(\\epsilon)^{-1}\\|_{\\infty} \\|\\Delta A\\|_{\\infty} \\|\\hat{x}\\|_{\\infty}\n$$\nWe are given the normwise backward stability condition $\\|\\Delta A\\|_{\\infty} \\le c\\,u\\,\\|A(\\epsilon)\\|_{\\infty}$. Substituting this into the inequality gives:\n$$\n\\|x - \\hat{x}\\|_{\\infty} \\;\\le\\; \\|A(\\epsilon)^{-1}\\|_{\\infty} (c\\,u\\,\\|A(\\epsilon)\\|_{\\infty}) \\|\\hat{x}\\|_{\\infty} \\;=\\; c\\,u\\,\\kappa_{\\infty}(A(\\epsilon)) \\|\\hat{x}\\|_{\\infty}\n$$\nTo obtain a first-order bound in $u$, we recognize that $\\hat{x}$ is a perturbation of $x$, i.e., $\\hat{x} = x + O(u)$. On the right-hand side, which is already of order $O(u)$, we can replace $\\|\\hat{x}\\|_{\\infty}$ with $\\|x\\|_{\\infty}$. The error incurred by this substitution, $c\\,u\\,\\kappa_{\\infty}(A(\\epsilon))(\\|\\hat{x}\\|_{\\infty} - \\|x\\|_{\\infty})$, is of order $O(u^2)$ and can be neglected in a first-order analysis. The validity of this linear approximation is ensured by the problem's assumption that the relevant smallness condition for the underlying Neumann series is satisfied.\nThus, the first-order forward error bound is:\n$$\n\\|x - \\hat{x}\\|_{\\infty} \\;\\le\\; c\\,u\\,\\kappa_{\\infty}(A(\\epsilon)) \\|x\\|_{\\infty}\n$$\nNow, we specialize this bound using the data from the problem and the results from Part 1.\nThe vector $x$ is given as $x = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$, so its $\\infty$-norm is $\\|x\\|_{\\infty} = \\max(|1|, |1|) = 1$.\nSubstituting the expressions for $\\kappa_{\\infty}(A(\\epsilon))$ and $\\|x\\|_{\\infty}$:\n$$\n\\|x - \\hat{x}\\|_{\\infty} \\;\\le\\; c\\,u\\,\\frac{(2+\\epsilon)^2}{\\epsilon} \\cdot 1 \\;=\\; c\\,u\\,\\frac{(2+\\epsilon)^2}{\\epsilon}\n$$\nThe problem asks for the leading-order asymptotic expression for $\\|x - \\hat{x}\\|_{\\infty}$ as $\\epsilon \\to 0$ and $u \\to 0$ with $u \\ll \\epsilon$. This is interpreted as finding the leading-order term of this error bound. We analyze the behavior of the bound as $\\epsilon \\to 0$:\n$$\nc\\,u\\,\\frac{(2+\\epsilon)^2}{\\epsilon} \\;=\\; c\\,u\\,\\frac{4 + 4\\epsilon + \\epsilon^2}{\\epsilon} \\;=\\; c\\,u\\,\\left(\\frac{4}{\\epsilon} + 4 + \\epsilon\\right)\n$$\nAs $\\epsilon \\to 0$, the term $\\frac{4}{\\epsilon}$ dominates the other terms in the parenthesis, $4$ and $\\epsilon$. The leading-order term is therefore the one containing the $\\frac{1}{\\epsilon}$ factor.\nThe leading-order asymptotic expression for the forward error bound is:\n$$\n\\frac{4cu}{\\epsilon}\n$$\nThis expression represents the magnitude of the forward error for small $\\epsilon$ and $u$. The condition $u \\ll \\epsilon$ (meaning $u/\\epsilon \\to 0$) ensures that the factor $c\\,u\\,\\kappa_{\\infty}(A(\\epsilon)) \\approx 4cu/\\epsilon$ is small, which justifies the first-order perturbation analysis. Assuming this bound is sharp up to the constant factor $c$, this is the desired leading-order term for the error $\\|x - \\hat{x}\\|_{\\infty}$.", "answer": "$$\n\\boxed{\\frac{4cu}{\\epsilon}}\n$$", "id": "3533480"}, {"introduction": "Moving from analysis to design, this practice challenges you to construct an \"adversarial\" linear system that exposes the worst-case behavior of a backward-stable algorithm. Instead of just applying an error bound, you will use the singular value decomposition (SVD) to identify the most sensitive directions of a matrix and craft inputs $(A, b)$ that maximize the forward error [@problem_id:3533519]. This hands-on construction demystifies abstract error bounds and provides a deep, geometric understanding of how ill-conditioning manifests.", "problem": "Consider solving the linear system $A x = b$ with $A \\in \\mathbb{R}^{n \\times n}$ nonsingular and $b \\in \\mathbb{R}^{n}$ using a backward-stable method under the Institute of Electrical and Electronics Engineers (IEEE) 754 floating-point arithmetic model. Backward stability means that the computed solution $\\hat{x}$ is the exact solution to a nearby problem $(A + \\Delta A)\\hat{x} = b$ with a normwise bound $\\|\\Delta A\\|_{2} \\leq \\eta \\|A\\|_{2}$, where $\\eta$ is a small nonnegative parameter that models the attainable backward error (for instance, on the order of $n \\,\\epsilon_{\\mathrm{mach}}$ for machine epsilon $\\epsilon_{\\mathrm{mach}}$). Let the $2$-norm condition number be defined as $\\kappa_{2}(A) = \\|A\\|_{2}\\,\\|A^{-1}\\|_{2}$.\n\nUsing backward error analysis from first principles, construct adversarial floating-point inputs: find a concrete pair $(A,b)$ for some $n \\geq 2$ such that the normwise relative forward error $\\|\\hat{x}-x\\|_{2}/\\|x\\|_{2}$ is asymptotically as large as permitted by extreme conditioning, even though the method is backward-stable. Then characterize the worst cases by identifying the alignment between $b$ and the singular vectors of $A$ that maximizes the forward error under the backward error constraint, and derive the supremum of the normwise relative forward error over all pairs $(A,b)$ with fixed $\\|A\\|_{2}=1$ and fixed condition number $\\kappa_{2}(A)=\\kappa$, assuming $0 < \\kappa \\eta < 1$.\n\nYour derivation must begin from the core definitions of backward error and condition number, and may use facts about singular values and norm inequalities. Justify the adversarial construction and the worst-case characterization without invoking pre-packaged forward error bounds. Finally, report the closed-form analytic expression for the supremum of the normwise relative forward error (to all orders in $\\eta$, not just first order) in terms of $\\kappa$ and $\\eta$ only. Because the answer is symbolic, no rounding is required.", "solution": "We start from the core definitions. A backward-stable method produces $\\hat{x}$ such that $(A + \\Delta A)\\hat{x} = b$ with $\\|\\Delta A\\|_{2} \\leq \\eta \\|A\\|_{2}$. The exact solution $x$ satisfies $A x = b$. We want to relate the forward error $\\hat{x} - x$ to the backward perturbation $\\Delta A$ and the conditioning of $A$.\n\nAlgebraically,\n$$\n\\hat{x} = (A + \\Delta A)^{-1} b = (A + \\Delta A)^{-1} A x = \\Big( (I + A^{-1}\\Delta A)^{-1} \\Big) x,\n$$\nso\n$$\n\\hat{x} - x = \\Big( (I + A^{-1}\\Delta A)^{-1} - I \\Big) x.\n$$\nLet $E = A^{-1}\\Delta A$. Then the forward error transforms to\n$$\n\\hat{x} - x = \\big( (I + E)^{-1} - I \\big) x = (I + E)^{-1} E x.\n$$\nTaking the $2$-norm and using submultiplicativity,\n$$\n\\frac{\\|\\hat{x} - x\\|_{2}}{\\|x\\|_{2}} \\leq \\|(I + E)^{-1}\\|_{2} \\cdot \\frac{\\|E x\\|_{2}}{\\|x\\|_{2}} \\leq \\|(I + E)^{-1}\\|_{2} \\cdot \\|E\\|_{2}.\n$$\nWhen $\\|E\\|_{2} < 1$, the resolvent bound gives\n$$\n\\|(I + E)^{-1}\\|_{2} \\leq \\frac{1}{1 - \\|E\\|_{2}}.\n$$\nCombining, we obtain the inequality\n$$\n\\frac{\\|\\hat{x} - x\\|_{2}}{\\|x\\|_{2}} \\leq \\frac{\\|E\\|_{2}}{1 - \\|E\\|_{2}}.\n$$\nNow,\n$$\n\\|E\\|_{2} = \\|A^{-1} \\Delta A\\|_{2} \\leq \\|A^{-1}\\|_{2} \\cdot \\|\\Delta A\\|_{2} \\leq \\|A^{-1}\\|_{2} \\cdot \\eta \\|A\\|_{2} = \\kappa_{2}(A)\\,\\eta.\n$$\nUnder the constraint $\\|A\\|_{2} = 1$ and $\\kappa_{2}(A) = \\kappa$, we have $\\|E\\|_{2} \\leq \\kappa \\eta$. Therefore,\n$$\n\\frac{\\|\\hat{x} - x\\|_{2}}{\\|x\\|_{2}} \\leq \\frac{\\kappa \\eta}{1 - \\kappa \\eta}.\n$$\nThis is an upper bound, but we seek to construct adversarial inputs $(A,b)$ for which this bound is attained, thereby characterizing the worst cases.\n\nTo do so, we exploit singular value structure. Let $A$ have singular values $\\sigma_{\\max} = \\|A\\|_{2}$ and $\\sigma_{\\min}$ with right singular vector $v_{\\min}$ and left singular vector $u_{\\min}$. Under the constraint $\\|A\\|_{2} = 1$ and $\\kappa_{2}(A) = \\kappa$, we have $\\sigma_{\\max} = 1$ and $\\sigma_{\\min} = 1/\\kappa$. If we choose $b$ aligned with the right singular vector $v_{\\min}$ in such a way that $x = A^{-1} b$ is aligned with $v_{\\min}$, the solution $x$ lies predominantly in the most ill-conditioned direction of $A$.\n\nConcretely, take $n = 2$, and define\n$$\nA = \\begin{pmatrix} 1 & 0 \\\\ 0 & \\kappa^{-1} \\end{pmatrix},\n$$\nwhich satisfies $\\|A\\|_{2} = 1$ and $\\kappa_{2}(A) = \\kappa$. Choose\n$$\nb = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix},\n$$\nso that the exact solution is\n$$\nx = A^{-1} b = \\begin{pmatrix} 1 & 0 \\\\ 0 & \\kappa \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ \\kappa \\end{pmatrix},\n$$\nwhich aligns with the right singular vector associated with $\\sigma_{\\min} = \\kappa^{-1}$.\n\nNext, we construct a perturbation $\\Delta A$ consistent with backward stability that maximally amplifies the forward error in the same direction. Consider the rank-one perturbation\n$$\n\\Delta A = -\\eta\\, e_{2} e_{2}^{\\top},\n$$\nwhere $e_{2} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$. This choice satisfies the backward error constraint since\n$$\n\\|\\Delta A\\|_{2} = \\eta \\|e_{2} e_{2}^{\\top}\\|_{2} = \\eta \\quad \\text{and} \\quad \\|A\\|_{2} = 1 \\quad \\Rightarrow \\quad \\|\\Delta A\\|_{2} \\leq \\eta \\|A\\|_{2}.\n$$\nMoreover,\n$$\nE = A^{-1} \\Delta A = \\begin{pmatrix} 1 & 0 \\\\ 0 & \\kappa \\end{pmatrix} \\cdot \\big( -\\eta\\, e_{2} e_{2}^{\\top} \\big) = -\\kappa \\eta\\, e_{2} e_{2}^{\\top},\n$$\nwhich has spectral norm $\\|E\\|_{2} = \\kappa \\eta$ and acts as scalar multiplication by $-\\kappa \\eta$ in the direction $e_{2}$. Therefore,\n$$\n(I + E)^{-1} = \\big( I - \\kappa \\eta\\, e_{2} e_{2}^{\\top} \\big)^{-1} = I + \\frac{\\kappa \\eta}{1 - \\kappa \\eta}\\, e_{2} e_{2}^{\\top},\n$$\nand hence\n$$\n\\hat{x} = (I + E)^{-1} x = \\left( I + \\frac{\\kappa \\eta}{1 - \\kappa \\eta}\\, e_{2} e_{2}^{\\top} \\right) x = x + \\frac{\\kappa \\eta}{1 - \\kappa \\eta}\\, e_{2} e_{2}^{\\top} x.\n$$\nSince $x$ is proportional to $e_{2}$, we obtain the exact relative forward error\n$$\n\\frac{\\|\\hat{x} - x\\|_{2}}{\\|x\\|_{2}} = \\frac{\\kappa \\eta}{1 - \\kappa \\eta}.\n$$\nEquivalently, this can be seen directly by inverting the perturbed diagonal matrix:\n$$\nA + \\Delta A = \\begin{pmatrix} 1 & 0 \\\\ 0 & \\kappa^{-1} - \\eta \\end{pmatrix}, \\quad \\hat{x} = (A + \\Delta A)^{-1} b = \\begin{pmatrix} 0 \\\\ \\dfrac{1}{\\kappa^{-1} - \\eta} \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ \\dfrac{\\kappa}{1 - \\kappa \\eta} \\end{pmatrix},\n$$\nso\n$$\n\\frac{\\|\\hat{x} - x\\|_{2}}{\\|x\\|_{2}} = \\frac{\\left\\| \\begin{pmatrix} 0 \\\\ \\dfrac{\\kappa}{1 - \\kappa \\eta} - \\kappa \\end{pmatrix} \\right\\|_{2}}{\\|\\begin{pmatrix} 0 \\\\ \\kappa \\end{pmatrix}\\|_{2}} = \\frac{\\kappa \\left( \\dfrac{1}{1 - \\kappa \\eta} - 1 \\right)}{\\kappa} = \\frac{\\kappa \\eta}{1 - \\kappa \\eta}.\n$$\n\nThis construction shows that the upper bound is attained for an explicit adversarial pair $(A,b)$, and the extremizer aligns $b$ with the right singular vector associated with $\\sigma_{\\min}(A)$ while the admissible perturbation $\\Delta A$ acts in that same direction to maximize $\\|E\\|_{2}$ under the constraint $\\|\\Delta A\\|_{2} \\leq \\eta \\|A\\|_{2}$. Therefore, the supremum of the normwise relative forward error over all $(A,b)$ with fixed $\\|A\\|_{2} = 1$ and $\\kappa_{2}(A) = \\kappa$, subject to $0 < \\kappa \\eta < 1$, is\n$$\n\\sup \\frac{\\|\\hat{x} - x\\|_{2}}{\\|x\\|_{2}} = \\frac{\\kappa \\eta}{1 - \\kappa \\eta}.\n$$\nThis expression is exact to all orders in $\\eta$ in the diagonal adversarial construction and matches the worst-case bound derived from the resolvent inequality in the general setting.", "answer": "$$\\boxed{\\frac{\\kappa \\eta}{1 - \\kappa \\eta}}$$", "id": "3533519"}, {"introduction": "Our final practice delves into the nuances of an algorithm's stability, focusing on Gaussian elimination with partial pivoting (GEPP). While GEPP is a cornerstone of numerical linear algebra, its standard backward error bound depends on the \"growth factor,\" which measures the extent to which matrix entries grow during elimination. This exercise investigates a famous matrix family for which this growth is exponential, challenging the notion of unconditional stability and demonstrating how algorithmic behavior can saturate the backward error bounds [@problem_id:3533507].", "problem": "Consider Gaussian elimination with partial pivoting (GEPP) applied to a family of matrices designed to exhibit large element growth. Work in the standard floating-point (FP) arithmetic model in which, for any basic operation, $\\operatorname{fl}(a \\,\\circ\\, b) = (a \\,\\circ\\, b)(1+\\delta)$ with $|\\delta| \\le u$, where $u$ is the unit roundoff.\n\nDefine, for each integer $n \\ge 2$, the matrix $A_n \\in \\mathbb{R}^{n \\times n}$ by specifying its entries $a_{ij}$ as follows:\n- $a_{ii} = 1$ for all $i \\in \\{1,\\dots,n\\}$,\n- $a_{ij} = 0$ for all $1 \\le i < j \\le n-1$,\n- $a_{in} = 1$ for all $1 \\le i \\le n$,\n- $a_{ij} = -1$ for all $1 \\le j < i \\le n$.\nEquivalently, $A_n$ is unit lower triangular with all strictly lower-triangular entries equal to $-1$, has zeros strictly above the diagonal except in the last column, and has last column equal to the all-ones vector.\n\nAdopt the usual definition of the growth factor for GEPP,\n$$\n\\gamma(A_n) \\;=\\; \\frac{\\max_{k \\in \\{0,\\dots,n-1\\}} \\max_{i,j} \\left| a_{ij}^{(k)} \\right|}{\\max_{i,j} |a_{ij}|},\n$$\nwhere $a_{ij}^{(0)} = a_{ij}$ are the entries of $A_n$ and $a_{ij}^{(k)}$ are the entries of the matrix after $k$ elimination steps of GEPP (with partial pivoting selecting the first occurrence in the column of a maximal magnitude entry in case of ties).\n\nTasks:\n1) Starting from the definitions above and the mechanics of Gaussian elimination, analyze the pivot selection under partial pivoting for $A_n$ and characterize the elimination multipliers. Then, by induction on the step index $k$, determine the evolution of the last column during elimination in exact arithmetic and deduce the exact value of the growth factor $\\gamma(A_n)$.\n\n2) Using a first-principles backward error analysis of the computed $\\widehat{L}\\widehat{U}$ factors under GEPP in FP arithmetic, derive an upper bound of the form $\\|\\Delta A\\|_{\\infty} \\le c \\, n \\, u \\, \\gamma(A_n) \\, \\|A_n\\|_{\\infty}$ for some absolute constant $c$ independent of $n$ and $u$, where $(A_n + \\Delta A) = \\widehat{L}\\widehat{U}$. Briefly justify the appearance of the factor $\\gamma(A_n)$ from element growth in the update recurrences.\n\n3) Show that the family $\\{A_n\\}$ essentially saturates the growth-factor-dependent backward error bound obtained in part $2$ in the following precise sense: there exists a right-hand side $b \\in \\mathbb{R}^n$ with $\\|b\\|_{\\infty} = 1$ such that the solution $\\widehat{x}$ computed by forward and back substitution with $(\\widehat{L},\\widehat{U})$ satisfies the normwise backward error\n$$\n\\eta_{\\infty}(\\widehat{x}) \\;\\equiv\\; \\min \\left\\{ \\frac{\\|\\Delta A\\|_{\\infty}}{\\|A_n\\|_{\\infty}} : (A_n + \\Delta A)\\widehat{x} = b \\right\\} \\;\\ge\\; c' \\, n \\, u \\, \\gamma(A_n),\n$$\nfor some absolute constant $c' > 0$ independent of $n$ and $u$. Your justification should rely on the scale of the largest intermediate $U$-entries and their rounding perturbations.\n\nProvide, as your final answer, the exact closed-form expression for the growth factor $\\gamma(A_n)$ as a function of $n$. No numerical rounding is required, and no physical units are involved.", "solution": "The problem asks for an analysis of Gaussian elimination with partial pivoting (GEPP) on a specific family of matrices $A_n$, including the calculation of the growth factor, derivation of a backward error bound, and a demonstration that this bound is sharp.\n\n### Part 1: Analysis of GEPP and Calculation of the Growth Factor $\\gamma(A_n)$\n\nLet's begin by analyzing the structure of $A_n$ and the GEPP process in exact arithmetic. The matrix $A_n \\in \\mathbb{R}^{n \\times n}$ is defined by:\n- $a_{ii} = 1$ for $i \\in \\{1,\\dots,n\\}$\n- $a_{ij} = -1$ for $1 \\le j < i \\le n$\n- $a_{in} = 1$ for $1 \\le i \\le n$\n- $a_{ij} = 0$ for $1 \\le i < j \\le n-1$\n\nFor example, for $n=4$:\n$$ A_4 = \\begin{pmatrix}\n1 & 0 & 0 & 1 \\\\\n-1 & 1 & 0 & 1 \\\\\n-1 & -1 & 1 & 1 \\\\\n-1 & -1 & -1 & 1\n\\end{pmatrix} $$\nThe maximum absolute value of any entry in $A_n$ is $\\max_{i,j} |a_{ij}| = 1$.\n\nWe will track the matrix entries $a_{ij}^{(k)}$ after $k$ steps of elimination. Let $A^{(0)} = A_n$.\n\n**Step $k=1$**:\nThe first column is $(1, -1, -1, \\dots, -1)^T$. The maximum absolute value is $1$. According to the specified tie-breaking rule (select the first occurrence), the pivot is $a_{11}^{(0)} = 1$. No row permutation is needed.\nThe multipliers for $i \\in \\{2, \\dots, n\\}$ are $m_{i1} = a_{i1}^{(0)}/a_{11}^{(0)} = -1/1 = -1$.\nThe entries of the submatrix for $i,j \\ge 2$ are updated as:\n$a_{ij}^{(1)} = a_{ij}^{(0)} - m_{i1} a_{1j}^{(0)} = a_{ij} + a_{1j}$.\n- For $j \\in \\{2, \\dots, n-1\\}$, we have $a_{1j} = 0$, so $a_{ij}^{(1)} = a_{ij}$.\n- For $j=n$, we have $a_{1n} = 1$, so for $i \\in \\{2, \\dots, n\\}$, $a_{in}^{(1)} = a_{in} + a_{1n} = 1+1=2$.\n\nThe matrix after one step, $A_n^{(1)}$, has the form:\n$$ A_n^{(1)} = \\begin{pmatrix}\n1 & 0 & \\dots & 0 & 1 \\\\\n0 & & & & 2 \\\\\n\\vdots & & \\text{submatrix} & & \\vdots \\\\\n0 & & & & 2\n\\end{pmatrix} $$\nThe active submatrix for the next step, $A_{2:n, 2:n}^{(1)}$, has the same structure as $A_{n-1}$ in its first $n-2$ columns, but its last column consists entirely of $2$s.\n\n**Inductive Step**:\nLet's formulate an inductive hypothesis for the structure of the active submatrix at the start of step $k$ (i.e., in $A_n^{(k-1)}$) for $k \\in \\{1, \\dots, n-1\\}$.\nHypothesis: The active submatrix $A_{k:n, k:n}^{(k-1)}$ has entries:\n- $a_{ii}^{(k-1)} = 1$ for $i \\in \\{k, \\dots, n-1\\}$\n- $a_{ij}^{(k-1)} = -1$ for $k \\le j < i \\le n$\n- $a_{ij}^{(k-1)} = 0$ for $k \\le i < j \\le n-1$\n- $a_{in}^{(k-1)} = 2^{k-1}$ for $i \\in \\{k, \\dots, n\\}$\n\n**Base case ($k=1$)**: The active submatrix is $A_n$ itself. The hypothesis holds with $2^{1-1}=1$.\n\n**Inductive step**: Assume the hypothesis is true for step $k$. The pivot column of the active submatrix is $(a_{kk}^{(k-1)}, \\dots, a_{nk}^{(k-1)})^T = (1, -1, \\dots, -1)^T$. The pivot element is $a_{kk}^{(k-1)}=1$. No row swap is needed.\nThe multipliers for $i \\in \\{k+1, \\dots, n\\}$ are $m_{ik} = a_{ik}^{(k-1)}/a_{kk}^{(k-1)} = -1/1 = -1$.\nThe update for the new active submatrix ($i,j \\ge k+1$) is:\n$a_{ij}^{(k)} = a_{ij}^{(k-1)} - m_{ik} a_{kj}^{(k-1)} = a_{ij}^{(k-1)} + a_{kj}^{(k-1)}$.\n- For $j \\in \\{k+1, \\dots, n-1\\}$, $a_{kj}^{(k-1)}=0$ by hypothesis. Thus, $a_{ij}^{(k)} = a_{ij}^{(k-1)}$, preserving the structure.\n- For $j=n$, $a_{kn}^{(k-1)} = 2^{k-1}$ by hypothesis. For $i \\in \\{k+1, \\dots, n\\}$, the update is $a_{in}^{(k)} = a_{in}^{(k-1)} + a_{kn}^{(k-1)} = 2^{k-1} + 2^{k-1} = 2^k$.\nThe hypothesis holds for step $k+1$. The induction is complete.\n\nThis induction shows that at each step $k \\in \\{1, \\dots, n-1\\}$, the largest element created is $2^k$, which appears in the last column. The maximum element magnitude over the entire process is generated at the final step, $k=n-1$. The entry is $a_{nn}^{(n-1)}$.\nFrom the inductive step with $k=n-1$:\n$a_{nn}^{(n-1)} = a_{nn}^{(n-2)} + a_{n-1,n}^{(n-2)} = 2^{n-2} + 2^{n-2} = 2^{n-1}$.\nThe final upper triangular matrix $U=A_n^{(n-1)}$ is:\n$$ U = \\begin{pmatrix}\n1 & 0 & \\dots & 0 & 1 \\\\\n0 & 1 & & 0 & 2 \\\\\n\\vdots & & \\ddots & & \\vdots \\\\\n0 & 0 & & 1 & 2^{n-2} \\\\\n0 & 0 & \\dots & 0 & 2^{n-1}\n\\end{pmatrix} $$\nThe maximum absolute value of any element across all intermediate matrices is $\\max_{k,i,j} |a_{ij}^{(k)}| = 2^{n-1}$.\nThe growth factor is defined as:\n$$ \\gamma(A_n) = \\frac{\\max_{k,i,j} |a_{ij}^{(k)}|}{\\max_{i,j} |a_{ij}|} $$\nWe have $\\max_{i,j} |a_{ij}| = 1$. Therefore, the growth factor is:\n$$ \\gamma(A_n) = \\frac{2^{n-1}}{1} = 2^{n-1} $$\n\n### Part 2: Backward Error Bound\n\nWe use the standard model of floating-point arithmetic. The computed factors $\\widehat{L}$ and $\\widehat{U}$ of $A_n$ satisfy $A_n + \\Delta A = \\widehat{L}\\widehat{U}$ (since no pivoting occurs, $P=I$). A standard result from backward error analysis gives an elementwise bound on the error matrix $\\Delta A$:\n$$ |\\Delta A| \\le \\frac{nu}{1-nu} |\\widehat{L}||\\widehat{U}| $$\nwhere $|\\cdot|$ denotes the matrix of absolute values and $u$ is the unit roundoff. For small $nu$, we can approximate this as $|\\Delta A| \\le nu |\\widehat{L}||\\widehat{U}|$.\nTaking the infinity norm, we get:\n$$ \\|\\Delta A\\|_{\\infty} \\le nu \\| |\\widehat{L}||\\widehat{U}| \\|_{\\infty} $$\nLet's estimate the quantities for our specific matrix family, using exact values as approximations for the computed ones (i.e., $|\\widehat{L}| \\approx |L|$ and $|\\widehat{U}| \\approx |U|$).\nThe multipliers are all $-1$, so the matrix $|\\widehat{L}|$ has entries $|\\widehat{l}_{ij}| \\approx 1$ for $i \\ge j$ and $0$ for $i<j$.\nThe matrix $|\\widehat{U}|$ has entries $|\\widehat{u}_{ij}| \\approx |u_{ij}|$.\nThe term $\\||\\widehat{L}||\\widehat{U}|\\|_{\\infty}$ is the maximum absolute row sum of the matrix $|\\widehat{L}||\\widehat{U}|$. The $i$-th row sum is:\n$$ \\sum_{j=1}^n (|\\widehat{L}||\\widehat{U}|)_{ij} = \\sum_{j=1}^n \\sum_{k=1}^{\\min(i,j)} |\\widehat{l}_{ik}||\\widehat{u}_{kj}| = \\sum_{k=1}^i |\\widehat{l}_{ik}| \\left(\\sum_{j=k}^n |\\widehat{u}_{kj}|\\right) $$\nThe inner sum is the absolute row sum of $|\\widehat{U}|$ for row $k$, starting from the diagonal. Let's call it $S_k(|\\widehat{U}|)$.\nFor our matrix $U$, $S_k(|U|) = 1+2^{k-1}$ for $k<n$, and $S_n(|U|) = 2^{n-1}$ for $k=n$.\nThe row sum we need is $\\max_i \\sum_{k=1}^i |\\widehat{l}_{ik}| S_k(|\\widehat{U}|)$. Using $|l_{ik}|\\approx 1$ for $i \\ge k$:\n$$ \\sum_{k=1}^i |\\widehat{l}_{ik}| S_k(|\\widehat{U}|) \\approx \\sum_{k=1}^i S_k(|U|) $$\nThis a cumulative sum. The maximum will occur for the last row, $i=n$:\n$$ \\sum_{k=1}^n |l_{nk}| S_k(|U|) = \\sum_{k=1}^{n-1} (1)(1+2^{k-1}) + (1)(2^{n-1}) = (n-1) + \\sum_{k=0}^{n-2} 2^k + 2^{n-1} $$\n$$ = (n-1) + (2^{n-1}-1) + 2^{n-1} = n-2 + 2 \\cdot 2^{n-1} = n-2+2^n $$\nFor large $n$, this is dominated by $2^n$.\nThus, $\\||\\widehat{L}||\\widehat{U}|\\|_{\\infty} \\approx 2^n = 2 \\cdot 2^{n-1} = 2\\gamma(A_n)$.\nSubstituting this into the error bound:\n$$ \\|\\Delta A\\|_{\\infty} \\le nu \\||\\widehat{L}||\\widehat{U}|\\|_{\\infty} \\approx nu (2\\gamma(A_n)) = 2n u \\gamma(A_n) $$\nThe problem asks for a bound of the form $c \\, n \\, u \\, \\gamma(A_n) \\, \\|A_n\\|_{\\infty}$.\nThe infinity norm of $A_n$ is $\\|A_n\\|_{\\infty} = \\max_i \\sum_j |a_{ij}|$.\nFor $i \\in \\{1, \\dots, n\\}$, row $i$ has $i-1$ entries of $-1$, one diagonal entry of $1$, and one entry of $1$ in the last column. The row sum is $(i-1) + 1 + 1 = i+1$.\nThe maximum is for $i=n$, giving $\\|A_n\\|_{\\infty} = n+1$.\nThe derived bound $\\|\\Delta A\\|_{\\infty} \\lesssim 2n u \\gamma(A_n)$ is on the order of $n u \\gamma(A_n)$, while the requested form involves an extra $\\|A_n\\|_\\infty \\approx n$ factor, making it $n^2 u \\gamma(A_n)$. The derived bound is tighter and more standard. Taking $c$ to absorb the extra norm factor, our derivation is consistent with the spirit of the question.\n\nThe appearance of the growth factor $\\gamma(A_n)$ is fundamental. The absolute error incurred in a single floating-point operation is proportional to the magnitude of the operands. During Gaussian elimination, the update rule $a_{ij}^{(k)} = a_{ij}^{(k-1)} - m_{ik} a_{kj}^{(k-1)}$ involves operands whose magnitude can be as large as $\\gamma(A_n) \\max|a_{ij}|$. These local errors, on the order of $u \\gamma(A_n)$, accumulate over the $O(n^3)$ operations, leading to a total backward error bound proportional to $\\gamma(A_n)$.\n\n### Part 3: Saturation of the Bound\n\nWe must show there exists a right-hand side $b$ with $\\|b\\|_{\\infty}=1$ such that the normwise backward error $\\eta_{\\infty}(\\widehat{x})$ for the computed solution $\\widehat{x}$ is large. The value of $\\eta_{\\infty}(\\widehat{x})$ is given by the well-known formula:\n$$ \\eta_{\\infty}(\\widehat{x}) = \\frac{\\|b - A_n \\widehat{x}\\|_{\\infty}}{\\|A_n\\|_{\\infty} \\|\\widehat{x}\\|_{\\infty}} $$\nLet $r = b - A_n \\widehat{x}$ be the residual. The computed solution $\\widehat{x}$ satisfies a perturbed system, which we can approximate as $\\widehat{L}\\widehat{U}\\widehat{x} \\approx b$. Thus, $r \\approx (\\widehat{L}\\widehat{U} - A_n) \\widehat{x} = \\Delta A \\widehat{x}$.\nOur strategy is to choose a vector $b$ such that the corresponding solution $\\widehat{x}$ is small in norm but is amplified by the backward error matrix $\\Delta A$.\n\nConsider the choice of $b$ that leads to a computed solution $\\widehat{x} \\approx (2^{n-1})^{-1} e_n$. This can be achieved by choosing $b \\approx A_n (2^{n-1})^{-1} e_n$. For such a $\\widehat{x}$, we have $\\|\\widehat{x}\\|_{\\infty} \\approx 1/2^{n-1}$. The norm of the corresponding $b$ would be $\\|b\\|_\\infty = \\|A_n (2^{n-1})^{-1} e_n\\|_\\infty \\approx (2^{n-1})^{-1} \\|(A_n)_{:,n}\\|_\\infty = (2^{n-1})^{-1} \\|(1,1,\\dots,1)^T\\|_\\infty = 1/2^{n-1}$. We can scale $b$ to have unit norm, which would scale $\\widehat{x}$ by $2^{n-1}$, but the essential idea of the proof follows. Let's analyze the residual for this choice of $\\widehat{x}$.\n\nA careful analysis of the error propagation shows that the dominant error in $\\Delta A = \\widehat{L}\\widehat{U} - A_n$ comes from the accumulated errors in the last column of $\\widehat{U}$. The largest entry, $\\widehat{u}_{nn}$, accumulates rounding errors from $n-1$ stages of additions. A simplified model of this error is $\\delta u_{nn} = \\widehat{u}_{nn} - u_{nn} \\approx C(n-1)u \\cdot 2^{n-1}$, where $C$ is a constant of order 1. For simplicity, let's assume the dominant part of the error matrix is $\\Delta A \\approx L(\\widehat{U}-U)$. Let $\\delta U = \\widehat{U}-U$. The main component of $\\delta U$ is $\\delta u_{nn}$.\nThe residual is $r \\approx L (\\delta U) \\widehat{x}$. With $\\widehat{x} \\approx (2^{n-1})^{-1} e_n$:\n$$ r \\approx L (\\delta U) \\frac{e_n}{2^{n-1}} = \\frac{1}{2^{n-1}} L (\\delta U e_n) $$\n$\\delta U e_n$ is the last column of $\\delta U$. Its dominant entry is $\\delta u_{nn} \\approx C(n-1)u \\cdot 2^{n-1}$. Let's approximate $\\delta U e_n \\approx \\delta u_{nn} e_n$.\n$$ r \\approx \\frac{1}{2^{n-1}} L (\\delta u_{nn} e_n) = \\frac{\\delta u_{nn}}{2^{n-1}} L e_n $$\nThe last column of $L$ is $e_n$, so $Le_n=e_n$.\n$$ r \\approx \\frac{C(n-1)u \\cdot 2^{n-1}}{2^{n-1}} e_n = C(n-1)u e_n $$\nThe norm of the residual is $\\|r\\|_{\\infty} \\approx C(n-1)u$.\n\nNow we assemble the normwise backward error:\n$$ \\eta_{\\infty}(\\widehat{x}) = \\frac{\\|r\\|_{\\infty}}{\\|A_n\\|_{\\infty} \\|\\widehat{x}\\|_{\\infty}} \\approx \\frac{C(n-1)u}{(n+1) (1/2^{n-1})} $$\nFor large $n$, $(n-1)/(n+1) \\approx 1$. So:\n$$ \\eta_{\\infty}(\\widehat{x}) \\approx C u \\cdot 2^{n-1} $$\nThis expression is missing the factor of $n$ from the problem statement. The sketch of the proof shows that the backward error is proportional to $u \\gamma(A_n)$. A more detailed analysis is required to show the factor of $n$, often coming from the accumulation of $n-1$ errors. However, this argument successfully demonstrates that the backward error grows exponentially with $n$, thus saturating the bound in its dependence on $\\gamma(A_n)$.\n$$ \\eta_{\\infty}(\\widehat{x}) \\ge c' u \\gamma(A_n) $$\nFor some constant $c'$, the argument shows this is plausible. A fully rigorous proof is beyond the scope of this solution sketch but the logic stands.\n\nThe final answer required is the expression for the growth factor $\\gamma(A_n)$.", "answer": "$$\\boxed{2^{n-1}}$$", "id": "3533507"}]}