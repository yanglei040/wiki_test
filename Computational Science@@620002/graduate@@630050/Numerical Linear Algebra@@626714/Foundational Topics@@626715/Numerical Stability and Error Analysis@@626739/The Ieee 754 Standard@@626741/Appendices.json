{"hands_on_practices": [{"introduction": "To build a solid understanding of numerical stability and error analysis, we must first quantify the fundamental precision of the tools we use. This exercise grounds our exploration in the bedrock of the IEEE 754 standard by connecting the bit-level representation of the common binary formats to the concept of unit roundoff, $u$. By calculating the precision and unit roundoff for each format, you will gain a quantitative feel for the trade-offs between storage and accuracy that govern all floating-point computations.", "problem": "Consider the Institute of Electrical and Electronics Engineers (IEEE) 754 Standard for Floating-Point Arithmetic and its binary interchange formats: binary16 (half precision), binary32 (single precision), binary64 (double precision), and binary128 (quadruple precision). Each normalized floating-point number in these formats can be written as $x = m \\times 2^{E}$, where $m \\in [1, 2)$ is the significand with an implicit leading $1$ followed by a fixed number of fraction bits, and $E$ is an integer exponent within the normal range determined by the format’s exponent field and bias. For the four formats, the number of fraction bits is respectively $10$ (binary16), $23$ (binary32), $52$ (binary64), and $112$ (binary128).\n\nUnder the round-to-nearest, ties-to-even rule, the rounding error for a single rounding step on a normalized value is bounded by half of the spacing between adjacent representable numbers at that magnitude. In the standard rounding model for normalized arithmetic in numerical linear algebra, one writes $\\mathrm{fl}(x) = x(1 + \\delta)$ for a single rounding step, with $|\\delta| \\leq u$, where $u$ is the unit roundoff. The number of correct base-$2$ digits is defined as the count of bits in the significand that are guaranteed to match the exact value after rounding, and the unit roundoff $u$ quantifies the maximal relative rounding error bound under round-to-nearest, ties-to-even.\n\nStarting from the format definitions above and the round-to-nearest, ties-to-even rule, derive for each format the number of correct base-$2$ digits (significand precision) and the corresponding exact unit roundoff $u$ for normalized numbers. Present your final answer as the single row matrix\n$$\n\\left(p_{16},\\, u_{16},\\, p_{32},\\, u_{32},\\, p_{64},\\, u_{64},\\, p_{128},\\, u_{128}\\right),\n$$\nwhere $p_{f}$ is the number of correct base-$2$ digits (significand bits, including the implicit leading $1$) and $u_{f}$ is the unit roundoff for the indicated format $f \\in \\{16,32,64,128\\}$. No rounding is required; provide exact values.", "solution": "The problem requires the derivation of the significand precision $p$ and the unit roundoff $u$ for four binary interchange formats specified by the IEEE 754 standard: binary16, binary32, binary64, and binary128. The solution proceeds by first defining these quantities based on the provided information and then calculating their values for each format.\n\nA normalized floating-point number $x$ is represented as $x = s \\times m \\times 2^{E}$, where $s$ is the sign ($\\pm 1$), $m$ is the significand, and $E$ is the exponent. For the formats in question, the significand $m$ is normalized to be in the interval $[1, 2)$ and has the binary form $m = (1.f_1f_2...f_n)_2$, where the leading $1$ is implicit and $f_1f_2...f_n$ are the $n$ fraction bits stored in memory.\n\nThe problem defines the \"number of correct base-2 digits\" as the significand precision, denoted by $p$. This is the total number of bits in the significand, including the implicit leading $1$. If a format has $n$ fraction bits, the significand precision is given by:\n$$\np = n + 1\n$$\nThis value $p$ determines the number of significant bits that can be represented for a given number.\n\nThe unit roundoff, denoted by $u$, is defined as the maximum relative error bound for a single rounding operation under the round-to-nearest rule. The standard rounding model is given as $\\mathrm{fl}(x) = x(1 + \\delta)$, where $|\\delta| \\leq u$.\n\nLet a real number $x$ have a representation that requires more than $p$ significand bits. The process of rounding, $\\mathrm{fl}(x)$, maps $x$ to the nearest representable floating-point number. Let a representable number be $y = m \\times 2^E$, where $m$ is a $p$-bit significand. The next larger representable number is $y^{+} = (m + 2^{-(p-1)}) \\times 2^E$. The distance between these two numbers, known as the \"unit in the last place\" (ulp) relative to the exponent $E$, is $\\mathrm{ulp}(y) = 2^{-(p-1)} \\times 2^E$.\n\nUnder the round-to-nearest rule, the absolute error is bounded by half of this distance:\n$$\n|\\mathrm{fl}(x) - x| \\leq \\frac{1}{2} \\mathrm{ulp}(\\mathrm{fl}(x)) = \\frac{1}{2} \\times 2^{-(p-1)} \\times 2^E = 2^{-p} \\times 2^E\n$$\nThe relative error is given by $\\frac{|\\mathrm{fl}(x) - x|}{|x|}$. To find the upper bound $u$, we must find the supremum of this quantity over all possible non-zero real numbers $x$.\n$$\nu = \\sup_{x} \\frac{|\\mathrm{fl}(x) - x|}{|x|}\n$$\nThe absolute rounding error is maximized for numbers lying exactly halfway between two representable floating-point numbers. Let's consider such a number $x = (m + 2^{-p}) \\times 2^E$, where $m$ is the significand of the lower representable number. The absolute error for rounding this number is $|\\mathrm{fl}(x) - x| = 2^{-p} \\times 2^E$. The corresponding relative error is:\n$$\n\\frac{2^{-p} \\times 2^E}{|x|} = \\frac{2^{-p} \\times 2^E}{|(m + 2^{-p}) \\times 2^E|} = \\frac{2^{-p}}{m + 2^{-p}}\n$$\nThis expression is maximized when the denominator is minimized. The smallest significand $m$ for a normalized number is $1$. Therefore, the maximum relative error occurs for numbers close to the powers of $2$ and is bounded by a value approaching $\\frac{2^{-p}}{1} = 2^{-p}$.\nThe standard definition for unit roundoff in numerical analysis for round-to-nearest arithmetic is precisely this bound:\n$$\nu = 2^{-p}\n$$\nThis value ensures that the relation $\\mathrm{fl}(x) = x(1+\\delta)$ holds with $|\\delta| \\leq u$.\n\nWe can now compute $p$ and $u$ for each specified format.\n\n1.  **binary16 (half precision):**\n    The number of fraction bits is given as $n_{16} = 10$.\n    The significand precision is $p_{16} = n_{16} + 1 = 10 + 1 = 11$.\n    The unit roundoff is $u_{16} = 2^{-p_{16}} = 2^{-11}$.\n\n2.  **binary32 (single precision):**\n    The number of fraction bits is given as $n_{32} = 23$.\n    The significand precision is $p_{32} = n_{32} + 1 = 23 + 1 = 24$.\n    The unit roundoff is $u_{32} = 2^{-p_{32}} = 2^{-24}$.\n\n3.  **binary64 (double precision):**\n    The number of fraction bits is given as $n_{64} = 52$.\n    The significand precision is $p_{64} = n_{64} + 1 = 52 + 1 = 53$.\n    The unit roundoff is $u_{64} = 2^{-p_{64}} = 2^{-53}$.\n\n4.  **binary128 (quadruple precision):**\n    The number of fraction bits is given as $n_{128} = 112$.\n    The significand precision is $p_{128} = n_{128} + 1 = 112 + 1 = 113$.\n    The unit roundoff is $u_{128} = 2^{-p_{128}} = 2^{-113}$.\n\nCombining these results into the required row matrix $(p_{16}, u_{16}, p_{32}, u_{32}, p_{64}, u_{64}, p_{128}, u_{128})$ yields the final answer.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n11 & 2^{-11} & 24 & 2^{-24} & 53 & 2^{-53} & 113 & 2^{-113}\n\\end{pmatrix}\n}\n$$", "id": "3589114"}, {"introduction": "Moving from the static properties of floating-point numbers to the dynamics of arithmetic, we encounter the crucial role of rounding modes. An exact real number that falls between two representable machine numbers must be rounded, and the IEEE 754 standard provides several rules for how to do this. This practice uses a carefully constructed hypothetical example to demonstrate how different rounding modes—specifically, rounding toward $+\\infty$ and $-\\infty$—can produce different results for the same operation, illustrating a concept vital for interval arithmetic and the construction of provably correct numerical bounds.", "problem": "Consider arithmetic conforming to the Institute of Electrical and Electronics Engineers (IEEE) 754 binary64 format (double precision), which represents a normalized floating-point number $x$ as $x = m \\cdot 2^{e}$ with $1 \\leq m < 2$, $e \\in \\mathbb{Z}$, and a $53$-bit significand (including the implicit leading $1$). The unit in the last place $\\mathrm{ulp}(x)$ for a normalized $x$ is defined as the gap between adjacent representable floating-point numbers at the magnitude of $x$, which equals $2^{e-52}$ when $x$ has unbiased exponent $e$. Directed rounding toward $+\\infty$ returns the smallest representable floating-point number $r$ such that $r \\geq s$, while directed rounding toward $-\\infty$ returns the largest representable floating-point number $r$ such that $r \\leq s$, where $s$ is the exact real result.\n\nYou are to construct and analyze a concrete binary64 example relevant to numerical linear algebra, where $a$ is a large magnitude quantity and $b$ is a much smaller correction. Let $a = 2^{100}$ and $b = 3 \\cdot 2^{46}$, and consider the floating-point summation of $a + b$ performed with exact real addition followed by rounding in either of the two directed modes (toward $+\\infty$ and toward $-\\infty$). Using only the IEEE 754 core definitions given above, determine the two rounded sums $r_{+} = \\operatorname{round}_{+\\infty}(a+b)$ and $r_{-} = \\operatorname{round}_{-\\infty}(a+b)$ and then quantify the magnitude of their difference, $|r_{+} - r_{-}|$, as a multiple of $\\mathrm{ulp}(a+b)$.\n\nExpress your final answer as a single real number equal to the multiple $k$ in $|r_{+} - r_{-}| = k \\cdot \\mathrm{ulp}(a+b)$. No rounding is required.", "solution": "The problem requires us to analyze the floating-point summation of two numbers, $a$ and $b$, under two different directed rounding modes.\n\nFirst, we represent the given numbers in the standard floating-point form.\nThe number $a = 2^{100}$ is already in the normalized form $m \\cdot 2^{e}$, with a significand $m_a = 1$ and an exponent $e_a = 100$. It is exactly representable in the binary64 format since its fractional part is zero.\n\nThe number $b = 3 \\cdot 2^{46}$ can be written as $b = (2+1) \\cdot 2^{46} = (1.5) \\cdot 2 \\cdot 2^{46} = 1.5 \\cdot 2^{47}$. It is also a normalized, exactly representable number with significand $m_b = 1.5 = 1.1_2$ and exponent $e_b = 47$.\n\nNext, we calculate the exact real sum $s = a+b$.\n$s = 2^{100} + 3 \\cdot 2^{46}$\nTo analyze the sum in the floating-point system, we factor out the larger power of two, $2^{100}$, to determine the exponent and significand of the result.\n$s = 2^{100} \\cdot \\left(1 + \\frac{3 \\cdot 2^{46}}{2^{100}}\\right) = 2^{100} \\cdot (1 + 3 \\cdot 2^{-54})$\nThe exact sum $s$ has an exponent $e_s = 100$ and a significand $m_s = 1 + 3 \\cdot 2^{-54}$.\n\nNow we examine the significand $m_s$ to see if it is representable in the binary64 format. A representable significand has the form $1.f_1 f_2 \\dots f_{52}$, where $f_i$ are the $52$ fractional bits. The value of such a significand is $1 + \\sum_{i=1}^{52} f_i 2^{-i}$.\nLet's express the fractional part of $m_s$ in binary.\n$3 \\cdot 2^{-54} = (2+1) \\cdot 2^{-54} = 2 \\cdot 2^{-54} + 1 \\cdot 2^{-54} = 2^{-53} + 2^{-54}$.\nSo, the significand is $m_s = 1 + 2^{-53} + 2^{-54}$.\nIn binary, this is represented as $m_s = 1.\\underbrace{00\\dots0}_{52 \\text{ zeros}}11_2$.\nThe significand $m_s$ has non-zero bits at the $53^{rd}$ and $54^{th}$ positions after the binary point. Since the binary64 format only stores $52$ fractional bits, $m_s$ is not exactly representable. The sum $s = a+b$ must be rounded.\n\nTo perform directed rounding, we must identify the two representable floating-point numbers that bracket the exact sum $s$. These numbers will have the same exponent, $e=100$, as $s$.\nLet $r_{-}$ be the largest representable number less than or equal to $s$, and $r_{+}$ be the smallest representable number greater than or equal to $s$.\nThe significand of $r_{-}$ is obtained by truncating the binary representation of $m_s$ after the $52^{nd}$ fractional bit.\n$m_{-} = 1.\\underbrace{00\\dots0}_{52 \\text{ zeros}} = 1$.\nThus, $r_{-} = 1 \\cdot 2^{100} = 2^{100}$.\n\nThe number $r_{+}$ is the next representable floating-point number after $r_{-}$. The gap between consecutive representable numbers with exponent $e$ is $2^{e-52}$. This value is also the definition of $\\mathrm{ulp}$ for numbers in this range. For $e=100$, the gap is $2^{100-52} = 2^{48}$.\nSo, $r_{+} = r_{-} + 2^{100-52} = 2^{100} + 2^{48}$.\nThe significand of $r_{+}$ is $m_{+} = 1 + 2^{-52}$, which is $1.\\underbrace{00\\dots01}_{52 \\text{ bits}}$.\n\nWe have the inequality $r_{-} < s < r_{+}$. Let's verify this:\n$r_{-} = 2^{100}$\n$s = 2^{100} + 3 \\cdot 2^{46}$\n$r_{+} = 2^{100} + 2^{48} = 2^{100} + 4 \\cdot 2^{46}$\nIndeed, $2^{100} < 2^{100} + 3 \\cdot 2^{46} < 2^{100} + 4 \\cdot 2^{46}$.\n\nNow we apply the specified rounding modes:\n- Rounding toward $-\\infty$ (floor): $\\operatorname{round}_{-\\infty}(s)$ is the largest representable number $r \\leq s$. This is $r_{-}$.\n  $\\operatorname{round}_{-\\infty}(a+b) = r_{-} = 2^{100}$.\n- Rounding toward $+\\infty$ (ceiling): $\\operatorname{round}_{+\\infty}(s)$ is the smallest representable number $r \\geq s$. This is $r_{+}$.\n  $\\operatorname{round}_{+\\infty}(a+b) = r_{+} = 2^{100} + 2^{48}$.\n\nThe next step is to compute the magnitude of the difference between these two rounded values.\n$|r_{+} - r_{-}| = |(2^{100} + 2^{48}) - 2^{100}| = 2^{48}$.\n\nFinally, we need to express this difference as a multiple of $\\mathrm{ulp}(a+b)$.\nThe exact sum $s = a+b$ is a normalized number with exponent $e=100$. According to the problem's definition, $\\mathrm{ulp}(s)$ is given by:\n$\\mathrm{ulp}(a+b) = 2^{e-52} = 2^{100-52} = 2^{48}$.\n\nWe are looking for the constant $k$ such that $|r_{+} - r_{-}| = k \\cdot \\mathrm{ulp}(a+b)$.\nSubstituting the values we found:\n$2^{48} = k \\cdot 2^{48}$.\nSolving for $k$ gives $k = 1$.\n\nThe magnitude of the difference between the two directed rounding results is exactly one unit in the last place of the sum.", "answer": "$$\\boxed{1}$$", "id": "3589160"}, {"introduction": "Mastery of floating-point arithmetic lies not just in understanding its limitations, but in using its features to design superior algorithms. This advanced practice delves into the critical task of accurate summation, a cornerstone of many numerical linear algebra algorithms like dot products and matrix-vector multiplication. You will analyze and compare the revolutionary Fused Multiply-Add (FMA) operation against classical techniques like Kahan compensated summation, revealing how modern hardware features can be leveraged to mitigate rounding error accumulation and build more robust numerical software.", "problem": "Consider the Institute of Electrical and Electronics Engineers Standard for Floating-Point Arithmetic (IEEE 754) binary64 format under round-to-nearest ties-to-even, with subnormals supported and no overflow for the operations described. Let the unit roundoff be denoted by $u$ (for binary64, $u = 2^{-53}$). The Fused Multiply-Add (FMA) operation computes $a \\times b + c$ as the exact real $a b + c$, then rounds once to the destination format. Assume $n \\geq 1$ and $x_i, a_i, b_i \\in \\mathbb{R}$ are bounded so that no overflow occurs.\n\nYou are asked to analyze the effect of FMA on compensated summation in numerical linear algebra. For a plain summation $S = \\sum_{i=1}^{n} x_i$, the Kahan-style compensated summation uses a compensation variable $c$ and performs the update\n$$\ny \\leftarrow x_i - c, \\quad t \\leftarrow s + y, \\quad c \\leftarrow (t - s) - y, \\quad s \\leftarrow t,\n$$\nwhich algebraically ensures $s + c$ is closer to the exact running sum than $s$ alone. For a dot product $S = \\sum_{i=1}^{n} a_i b_i$, one may use FMA in the accumulation step $s \\leftarrow \\mathrm{fma}(a_i, b_i, s)$.\n\nStarting from the IEEE 754 definitions of rounding to nearest, single-rounding semantics of FMA, and the standard floating-point error model of arithmetic operations, analyze which of the following statements are true or false, and justify when FMA can replace explicit compensation to achieve similar accuracy. Select all correct options.\n\nA. In a pure summation $S = \\sum_{i=1}^{n} x_i$, replacing $s \\leftarrow s + x_i$ by $s \\leftarrow \\mathrm{fma}(x_i, 1, s)$ eliminates the need for the Kahan-style compensation and achieves a forward error bound comparable to Kahan’s compensated summation (up to a small constant factor), provided rounding mode is round-to-nearest ties-to-even.\n\nB. In a dot product $S = \\sum_{i=1}^{n} a_i b_i$, using $s \\leftarrow \\mathrm{fma}(a_i, b_i, s)$ reduces the number of roundings per term from two to one. The resulting forward error remains of order $O(n u)$ in worst-case adversarial input orders, whereas Kahan-style compensation can reduce the growth to $O(u)$; hence FMA alone cannot match Kahan’s worst-case accuracy for long, ill-conditioned accumulations.\n\nC. For $p_i \\leftarrow \\mathrm{fl}(a_i b_i)$ and $e_i \\leftarrow \\mathrm{fma}(a_i, b_i, -p_i)$, one has $p_i + e_i = a_i b_i$ exactly in real arithmetic, provided no overflow occurs and $e_i$ does not underflow to zero. Consequently, accumulating $p_i$ in one running sum and $e_i$ in a secondary running sum yields a compensation mechanism derived from FMA that can match the accuracy benefits of Kahan-style compensation in dot products under these conditions.\n\nD. Under directed rounding (e.g., round-toward-zero), the Kahan update $c \\leftarrow (t - s) - y$ ceases to be a valid numerical identity, so FMA is strictly superior and can replace explicit compensation in any rounding mode.\n\nE. If flush-to-zero (FTZ) or denormals-are-zero (DAZ) is enabled, then computing $e_i \\leftarrow \\mathrm{fma}(a_i, b_i, -p_i)$ in statement C can spuriously yield $e_i = 0$ for cancellation-dominated terms, undermining compensation; therefore, to rely on FMA in place of explicit Kahan-style compensation, one must ensure full subnormal support and avoid underflow in $e_i$.", "solution": "The problem requires an analysis of several statements concerning the use of the Fused Multiply-Add (FMA) operation in the context of numerical summation and dot products, specifically in comparison to Kahan-style compensated summation. The operational context is the IEEE 754 binary64 standard, with round-to-nearest ties-to-even rounding, support for subnormal numbers, and no overflow. The unit roundoff is $u = 2^{-53}$.\n\nA standard floating-point operation $\\circ \\in \\{+, -, \\times, \\div\\}$ is modeled as $\\mathrm{fl}(x \\circ y) = (x \\circ y)(1+\\delta)$, where $|\\delta| \\leq u$.\nThe Fused Multiply-Add operation is modeled as $\\mathrm{fma}(a, b, c) = \\mathrm{fl}(ab+c) = (ab+c)(1+\\eta)$, where $|\\eta| \\leq u$. The key feature is that the exact product $a \\times b$ is computed, added to $c$, and only then is the final result rounded once to the destination format.\n\nKahan's summation algorithm for $S = \\sum_{i=1}^{n} x_i$ maintains a running sum $s$ and a compensation term $c$. An iteration is:\n$y \\leftarrow \\mathrm{fl}(x_i - c)$\n$t \\leftarrow \\mathrm{fl}(s + y)$\n$c \\leftarrow \\mathrm{fl}(\\mathrm{fl}(t - s) - y)$\n$s \\leftarrow t$\nUnder standard assumptions, the error in the final sum $s_n$ is bounded by $(2u + O(nu^2))\\sum_{i=1}^{n} |x_i|$. The crucial feature is that the error bound is independent of $n$ to the first order in $u$. In contrast, simple recursive summation $s_k \\leftarrow \\mathrm{fl}(s_{k-1} + x_k)$ has an error bound that grows linearly with $n$, approximately $(n-1)u \\sum_{i=1}^{n} |x_i|$.\n\nWe will now evaluate each statement.\n\n### Option A Analysis\nThe statement proposes replacing a standard addition $s \\leftarrow \\mathrm{fl}(s + x_i)$ with an FMA operation $s \\leftarrow \\mathrm{fma}(x_i, 1, s)$ for a pure summation $S = \\sum_{i=1}^{n} x_i$. The claim is that this replacement obviates the need for Kahan compensation and achieves a comparable error bound.\n\nThe operation $\\mathrm{fma}(x_i, 1, s)$ computes $\\mathrm{fl}(x_i \\cdot 1 + s)$. Let $x_i$ and $s$ be floating-point numbers. The number $1$ is exactly representable. The product $x_i \\cdot 1$ is simply $x_i$, an exact operation. Therefore, $\\mathrm{fma}(x_i, 1, s) = \\mathrm{fl}(x_i + s)$. This is precisely the same operation as a standard floating-point addition.\n\nThe use of FMA in this manner offers no new computational path or reduction in rounding errors compared to a simple addition. The summation remains a recursive summation, where the update rule for the sum $s_k$ is $s_k \\leftarrow \\mathrm{fl}(s_{k-1} + x_k)$. As noted, the worst-case forward error for this method is of order $O(nu)$. Kahan's compensated summation has a worst-case forward error bound of order $O(u)$ (ignoring higher-order terms in $u$). The error bounds are not comparable for large $n$. Therefore, this use of FMA does not eliminate the need for Kahan-style compensation to achieve high accuracy.\n\nVerdict: **Incorrect**.\n\n### Option B Analysis\nThe statement concerns a dot product $S = \\sum_{i=1}^{n} a_i b_i$.\nPart 1: Using $s \\leftarrow \\mathrm{fma}(a_i, b_i, s)$ reduces the number of roundings per term from two to one.\nA standard implementation computes $p_i \\leftarrow \\mathrm{fl}(a_i b_i)$ (one rounding) and then updates the sum $s \\leftarrow \\mathrm{fl}(s + p_i)$ (a second rounding). In total, there are two rounding errors per term. The FMA-based update is $s \\leftarrow \\mathrm{fma}(a_i, b_i, s) = \\mathrm{fl}(a_i b_i + s)$. This involves computing the exact product $a_i b_i$, adding the previous sum $s$, and then performing a single rounding. This reduces the number of rounding operations per term from two to one. This part of the statement is correct.\n\nPart 2: The resulting forward error remains of order $O(nu)$ in worst-case, and FMA alone cannot match Kahan's worst-case accuracy for long, ill-conditioned accumulations.\nThe FMA-based dot product is a recursive summation of the terms $a_i b_i$. Let $s_k$ be the sum after $k$ terms. The update is $s_k = \\mathrm{fl}(a_k b_k + s_{k-1}) = (a_k b_k + s_{k-1})(1+\\eta_k)$, where $|\\eta_k| \\le u$. The error propagation analysis for this is essentially identical to that of standard recursive summation. The final error is bounded by a quantity proportional to $nu\\sum_{i=1}^n|a_ib_i|$. The growth of the error bound is linear in $n$. An ill-conditioned accumulation, such as summing many small terms into a large running sum, will exhibit this poor accuracy.\nKahan-style compensation, when applied to the sequence of products $p_i = \\mathrm{fl}(a_i b_i)$, yields a final error whose bound is independent of $n$ to first order in $u$. For large $n$, this is a significant improvement. Therefore, FMA when used for simple accumulation does not match the accuracy of compensated summation for long or ill-conditioned sums.\n\nVerdict: **Correct**.\n\n### Option C Analysis\nThe statement describes an FMA-based method for creating an error-free transformation for a product, a technique known as `TwoProduct`. The method is:\n1.  $p_i \\leftarrow \\mathrm{fl}(a_i b_i)$\n2.  $e_i \\leftarrow \\mathrm{fma}(a_i, b_i, -p_i)$\n\nPart 1: \"one has $p_i + e_i = a_i b_i$ exactly in real arithmetic\".\nLet $a_i$ and $b_i$ be binary floating-point numbers with $P$ significand bits (for binary64, $P=53$). Their exact product $a_i b_i$ can require up to $2P$ bits to represent. The quantity $p_i = \\mathrm{fl}(a_i b_i)$ is this exact product rounded to $P$ bits. The rounding error is $E = a_i b_i - p_i$. A fundamental theorem of floating-point arithmetic (originally due to Dekker) states that this error $E$ is itself exactly representable as a floating-point number, provided there is no underflow or overflow. The FMA instruction computes $e_i = \\mathrm{fl}(a_i b_i - p_i)$. Since the exact value $a_i b_i - p_i$ is a floating-point number, rounding it has no effect, i.e., $\\mathrm{fl}(a_i b_i - p_i) = a_i b_i - p_i$. Thus, $e_i = a_i b_i - p_i$, which rearranges to $p_i + e_i = a_i b_i$. This part of the statement, including its provisos, is correct. This is a standard Error-Free Transformation (EFT).\n\nPart 2: \"accumulating $p_i$ in one running sum and $e_i$ in a secondary running sum yields a compensation mechanism... that can match the accuracy benefits of Kahan-style compensation\".\nThis describes an accurate summation algorithm. We have decomposed the exact dot product $\\sum a_i b_i$ into $\\sum (p_i + e_i) = \\sum p_i + \\sum e_i$. We can now compute the sum of the high-order parts, $S_p = \\sum p_i$, and the sum of the low-order error parts, $S_e = \\sum e_i$. The final result is then $S_p + S_e$. This is a form of double-precision accumulation (related to double-double arithmetic). Algorithms like `Dot2` from Ogita, Rump, and Oishi use exactly this principle. The error analysis for such methods shows they are highly accurate, yielding error bounds comparable to or better than applying Kahan summation to the sequence of rounded products $\\mathrm{fl}(a_i b_i)$.\n\nVerdict: **Correct**.\n\n### Option D Analysis\nThe statement claims that under directed rounding (e.g., round-toward-zero), the Kahan update $c \\leftarrow \\mathrm{fl}(\\mathrm{fl}(t-s)-y)$ becomes invalid, and therefore FMA is \"strictly superior\".\n\nPart 1: \"Under directed rounding... the Kahan update... ceases to be a valid numerical identity\".\nThe correctness of the error calculation in Kahan summation, which relies on an EFT known as `TwoSum`, is critically dependent on the rounding mode being round-to-nearest. Specifically, the theorem that allows recovery of the rounding error in $t = \\mathrm{fl}(s+y)$ using subtraction, often leveraging properties like Sterbenz's Lemma ($\\mathrm{fl}(a-b)=a-b$ if $a/2 \\le b \\le 2a$), breaks down with directed rounding. Directed rounding introduces a bias, and the rounding error is no longer guaranteed to be a floating-point number that can be recovered exactly by simple subtractions. So, Kahan compensation is less effective and the error term is not recovered exactly. This part of the statement is correct.\n\nPart 2: \"so FMA is strictly superior and can replace explicit compensation in any rounding mode\".\nThis conclusion is a non-sequitur. Even if Kahan summation is degraded, it does not mean that a simple FMA-based summation, $s \\leftarrow \\mathrm{fma}(a_i, b_i, s)$, is \"strictly superior\". As established in B, this simple FMA accumulation has an error bound that grows with $O(nu)$. A degraded Kahan summation, while not as effective as in round-to-nearest mode, typically still cancels the high-order parts of the error and often performs much better than naive summation, especially for large $n$. To declare FMA \"strictly superior\" is incorrect. Furthermore, the statement seems to ignore the possibility of FMA-based compensation schemes (like in C), which would be far superior to the simple FMA accumulation. The claim is an overgeneralization and logically flawed.\n\nVerdict: **Incorrect**.\n\n### Option E Analysis\nThe statement considers the FMA-based compensation from option C, $e_i \\leftarrow \\mathrm{fma}(a_i, b_i, -p_i)$, under FTZ (flush-to-zero) or DAZ (denormals-are-zero) modes.\n\nThe compensation mechanism relies on the precise calculation of the error term $e_i = a_i b_i - p_i$. This error term can be very small. Specifically, its magnitude is bounded: $|e_i| \\le u |p_i|$. If $p_i$ is itself small, or due to cancellation, $e_i$ can fall into the subnormal range (numbers with magnitude smaller than the smallest normalized number, $2^{E_{min}}$).\n\nFTZ (Flush-To-Zero) is a mode where any operation whose result would be a subnormal number instead produces a (signed) zero. If the true error $e_i$ is a non-zero value in the subnormal range, computing it with FTZ enabled will yield a result of $0$.\nDAZ (Denormals-Are-Zero) treats subnormal numbers as zero when they appear as inputs to operations. This could also affect the computation of $e_i$.\n\nIf $e_i$ is spuriously flushed to zero, the identity $p_i + e_i = a_i b_i$ breaks down. The error term for that step is lost, and the compensation fails. The running sum of the error terms, $\\sum e_i$, becomes inaccurate, and the accuracy benefit of the compensation scheme is undermined. Therefore, for the `TwoProduct` EFT and similar compensation schemes to be reliable, it is essential that the hardware and software environment support subnormal numbers correctly, as per the full IEEE 754 standard. The problem statement correctly assumes this support (\"subnormals supported\"), and this option correctly identifies what would happen if that support were absent.\n\nVerdict: **Correct**.", "answer": "$$\\boxed{BCE}$$", "id": "3589159"}]}