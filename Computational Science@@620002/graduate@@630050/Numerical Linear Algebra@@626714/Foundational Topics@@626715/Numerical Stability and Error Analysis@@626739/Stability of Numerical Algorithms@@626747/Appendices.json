{"hands_on_practices": [{"introduction": "A cornerstone of numerical analysis is understanding the relationship between the residual, which is computable, and the true error, which is not. This exercise provides a concrete, hands-on calculation to demonstrate how the condition number of a matrix, $\\kappa(A)$, governs the amplification from a small residual to a potentially much larger forward error. By working through this fundamental example [@problem_id:3581515], you will solidify your understanding of these critical concepts.", "problem": "Consider the linear system $A x = b$ with \n$$A = \\mathrm{diag}(1, 10, 100), \\quad b = \\begin{pmatrix} 6 \\\\ 8 \\\\ 0 \\end{pmatrix}.$$\nA numerical algorithm produces an approximate solution $\\hat{x}$ whose residual $r$ is measured as \n$$r = b - A \\hat{x} = \\begin{pmatrix} 1 \\times 10^{-6} \\\\ -2 \\times 10^{-6} \\\\ 2 \\times 10^{-6} \\end{pmatrix}.$$\nUsing the Euclidean vector norm $\\|\\cdot\\|_{2}$ and the associated induced matrix norm, and taking the matrix condition number in the spectral norm $\\kappa_{2}(A)$ as known for this $A$, derive from first principles an a priori upper bound (dependent only on $\\kappa_{2}(A)$, $\\|r\\|_{2}$, and $\\|b\\|_{2}$) for the relative forward error $\\dfrac{\\|x - \\hat{x}\\|_{2}}{\\|x\\|_{2}}$, where $x$ is the exact solution to $A x = b$. Then evaluate this bound numerically for the given data. Round your final numerical bound to four significant figures.", "solution": "The user wants to find an a priori upper bound for the relative forward error of an approximate solution to a linear system and then evaluate this bound numerically.\n\n### Step 1: Problem Validation\n\n**1.1. Extract Givens:**\n- Linear system: $A x = b$\n- Matrix $A$: $A = \\mathrm{diag}(1, 10, 100)$\n- Vector $b$: $b = \\begin{pmatrix} 6 \\\\ 8 \\\\ 0 \\end{pmatrix}$\n- Exact solution: $x$\n- Approximate solution: $\\hat{x}$\n- Residual vector $r$: $r = b - A \\hat{x} = \\begin{pmatrix} 1 \\times 10^{-6} \\\\ -2 \\times 10^{-6} \\\\ 2 \\times 10^{-6} \\end{pmatrix}$\n- Norms: Euclidean vector norm $\\|\\cdot\\|_{2}$ and the associated induced matrix norm (spectral norm).\n- Known constant: Condition number $\\kappa_{2}(A)$.\n- Goal: Derive an upper bound for the relative forward error $\\dfrac{\\|x - \\hat{x}\\|_{2}}{\\|x\\|_{2}}$ in terms of $\\kappa_{2}(A)$, $\\|r\\|_{2}$, and $\\|b\\|_{2}$, and then compute its numerical value.\n\n**1.2. Validate Using Extracted Givens:**\n- **Scientifically Grounded:** The problem is a standard exercise in numerical linear algebra, dealing with the relationship between the residual and the forward error. This is a fundamental concept in the analysis of numerical algorithms for solving linear systems. The principles involved are well-established.\n- **Well-Posed:** The matrix $A$ is diagonal with non-zero diagonal entries, so it is invertible, guaranteeing a unique solution $x$ exists. The problem asks for a standard upper bound, which is a well-defined derivation. All necessary data is provided.\n- **Objective:** The problem is stated using precise mathematical language and definitions, free from ambiguity or subjective claims.\n- **Other criteria:** The problem is self-contained, consistent, and scientifically feasible. It does not violate any of the specified invalidity criteria.\n\n**1.3. Verdict and Action:**\nThe problem is valid. A complete solution will be provided.\n\n### Step 2: Derivation of the Upper Bound\n\nThe relative forward error is defined as $\\dfrac{\\|x - \\hat{x}\\|_{2}}{\\|x\\|_{2}}$. We need to find an upper bound for this quantity.\n\nLet $e = x - \\hat{x}$ be the forward error vector.\nThe residual is given as $r = b - A \\hat{x}$.\nThe exact solution $x$ satisfies the equation $A x = b$.\nSubstituting $b = A x$ into the expression for the residual, we get:\n$$r = A x - A \\hat{x} = A(x - \\hat{x}) = A e$$\nSince the matrix $A$ is invertible, we can express the error $e$ in terms of the residual $r$:\n$$e = A^{-1} r$$\nTaking the Euclidean norm of both sides and applying the property of induced matrix norms, $\\|Mv\\|_{2} \\le \\|M\\|_{2}\\|v\\|_{2}$, we obtain a bound on the absolute forward error:\n$$\\|e\\|_{2} = \\|x - \\hat{x}\\|_{2} = \\|A^{-1} r\\|_{2} \\le \\|A^{-1}\\|_{2} \\|r\\|_{2}$$\nThis gives an upper bound on the norm of the error vector. To find a bound for the relative error, we need to find a lower bound for $\\|x\\|_{2}$.\nFrom the original equation $A x = b$, we take the norm of both sides:\n$$\\|b\\|_{2} = \\|A x\\|_{2}$$\nUsing the same property of induced matrix norms, we have:\n$$\\|b\\|_{2} \\le \\|A\\|_{2} \\|x\\|_{2}$$\nSince $A$ is invertible and $b \\neq 0$, the solution $x$ is non-zero, so $\\|x\\|_{2} > 0$. We can rearrange the inequality to obtain a lower bound on $\\|x\\|_{2}$:\n$$\\|x\\|_{2} \\ge \\frac{\\|b\\|_{2}}{\\|A\\|_{2}}$$\nTaking the reciprocal, we get an upper bound for $\\dfrac{1}{\\|x\\|_{2}}$:\n$$\\frac{1}{\\|x\\|_{2}} \\le \\frac{\\|A\\|_{2}}{\\|b\\|_{2}}$$\nNow, we can combine the bounds for $\\|x - \\hat{x}\\|_{2}$ and $\\dfrac{1}{\\|x\\|_{2}}$ to bound the relative forward error:\n$$\\frac{\\|x - \\hat{x}\\|_{2}}{\\|x\\|_{2}} \\le (\\|A^{-1}\\|_{2} \\|r\\|_{2}) \\left( \\frac{\\|A\\|_{2}}{\\|b\\|_{2}} \\right)$$\nRearranging the terms, we get:\n$$\\frac{\\|x - \\hat{x}\\|_{2}}{\\|x\\|_{2}} \\le \\|A\\|_{2} \\|A^{-1}\\|_{2} \\frac{\\|r\\|_{2}}{\\|b\\|_{2}}$$\nBy definition, the condition number of $A$ in the spectral norm is $\\kappa_{2}(A) = \\|A\\|_{2} \\|A^{-1}\\|_{2}$. Substituting this into the inequality gives the desired a priori upper bound:\n$$\\frac{\\|x - \\hat{x}\\|_{2}}{\\|x\\|_{2}} \\le \\kappa_{2}(A) \\frac{\\|r\\|_{2}}{\\|b\\|_{2}}$$\n\n### Step 3: Numerical Evaluation of the Bound\n\nTo evaluate this bound, we must compute the values of $\\kappa_{2}(A)$, $\\|b\\|_{2}$, and $\\|r\\|_{2}$.\n\n**1. Calculation of $\\kappa_{2}(A)$:**\nThe matrix $A$ is a diagonal matrix: $A = \\mathrm{diag}(1, 10, 100)$. For a symmetric matrix (which includes diagonal matrices), the spectral norm $\\|A\\|_{2}$ is equal to its spectral radius, which is the maximum absolute value of its eigenvalues. The eigenvalues of a diagonal matrix are its diagonal entries.\nThe eigenvalues of $A$ are $\\lambda_1 = 1$, $\\lambda_2 = 10$, and $\\lambda_3 = 100$.\nTherefore, the spectral norm of $A$ is:\n$$\\|A\\|_{2} = \\max\\{|1|, |10|, |100|\\} = 100$$\nThe inverse of $A$ is $A^{-1} = \\mathrm{diag}(1^{-1}, 10^{-1}, 100^{-1}) = \\mathrm{diag}(1, 0.1, 0.01)$.\nThe eigenvalues of $A^{-1}$ are $1$, $0.1$, and $0.01$.\nThe spectral norm of $A^{-1}$ is:\n$$\\|A^{-1}\\|_{2} = \\max\\{|1|, |0.1|, |0.01|\\} = 1$$\nThe condition number $\\kappa_{2}(A)$ is the product of these norms:\n$$\\kappa_{2}(A) = \\|A\\|_{2} \\|A^{-1}\\|_{2} = 100 \\times 1 = 100$$\nAlternatively, for a symmetric positive definite matrix, $\\kappa_{2}(A) = \\frac{\\lambda_{\\max}}{\\lambda_{\\min}} = \\frac{100}{1} = 100$.\n\n**2. Calculation of $\\|b\\|_{2}$:**\nThe vector $b$ is given as $b = \\begin{pmatrix} 6 \\\\ 8 \\\\ 0 \\end{pmatrix}$. Its Euclidean norm is:\n$$\\|b\\|_{2} = \\sqrt{6^2 + 8^2 + 0^2} = \\sqrt{36 + 64} = \\sqrt{100} = 10$$\n\n**3. Calculation of $\\|r\\|_{2}$:**\nThe residual vector $r$ is given as $r = \\begin{pmatrix} 1 \\times 10^{-6} \\\\ -2 \\times 10^{-6} \\\\ 2 \\times 10^{-6} \\end{pmatrix}$. Its Euclidean norm is:\n$$\\|r\\|_{2} = \\sqrt{(1 \\times 10^{-6})^2 + (-2 \\times 10^{-6})^2 + (2 \\times 10^{-6})^2}$$\n$$\\|r\\|_{2} = \\sqrt{1 \\times 10^{-12} + 4 \\times 10^{-12} + 4 \\times 10^{-12}} = \\sqrt{9 \\times 10^{-12}} = 3 \\times 10^{-6}$$\n\n**4. Final Calculation of the Bound:**\nSubstituting the computed values into the derived inequality:\n$$\\frac{\\|x - \\hat{x}\\|_{2}}{\\|x\\|_{2}} \\le \\kappa_{2}(A) \\frac{\\|r\\|_{2}}{\\|b\\|_{2}} = 100 \\times \\frac{3 \\times 10^{-6}}{10}$$\n$$\\frac{\\|x - \\hat{x}\\|_{2}}{\\|x\\|_{2}} \\le 10 \\times (3 \\times 10^{-6}) = 3 \\times 10^{-5}$$\nThe problem requires the numerical bound to be rounded to four significant figures.\n$$3 \\times 10^{-5} = 3.000 \\times 10^{-5}$$\nThus, the upper bound for the relative forward error is $3.000 \\times 10^{-5}$.", "answer": "$$\n\\boxed{3.000 \\times 10^{-5}}\n$$", "id": "3581515"}, {"introduction": "While the condition number is a vital tool, it does not tell the whole story, especially for non-symmetric matrices. This practice [@problem_id:3581533] delves into the intriguing behavior of nonnormal matrices, where a small residual can mask a large forward error, causing even corrective procedures like iterative refinement to fail. Through a direct analytical derivation, you will uncover the precise mechanism by which nonnormality can undermine numerical stability, providing a deeper and more nuanced perspective on error analysis.", "problem": "Consider the family of upper-triangular, nonnormal matrices $A_{M} \\in \\mathbb{R}^{2 \\times 2}$ defined by\n$$\nA_{M} = \\begin{pmatrix} 1 & M \\\\ 0 & 1 \\end{pmatrix},\n$$\nwith a fixed right-hand side vector $b = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$ and the exact solution $x$ of $A_{M} x = b$. A computed approximation $\\hat{x}$ is obtained by solving the perturbed system $(A_{M} + E)\\hat{x} = b$, where the backward error matrix $E$ is\n$$\nE = \\begin{pmatrix} 0 & 0 \\\\ \\varepsilon & 0 \\end{pmatrix},\n$$\nwith $0 < \\varepsilon \\ll 1$ representing the unit roundoff of the working-precision floating-point (FP) arithmetic. An iterative refinement step is performed as follows: the residual $r = b - A_{M}\\hat{x}$ is computed in sufficiently high precision so that $r$ is exact for the given $\\hat{x}$ and $A_{M}$, and the correction $d$ is obtained by solving $(A_{M} + E)d = r$ using the same working-precision factors that produced $\\hat{x}$. The new approximation is $\\hat{x}^{(1)} = \\hat{x} + d$.\n\nStarting from the core definitions of backward error, forward error, and the iterative refinement update, and employing only linear algebraic identities, derive the exact linear error-propagation operator $T(M,\\varepsilon)$ in the $2$-norm that maps the current forward error $e = x - \\hat{x}$ to the next forward error $e^{(1)} = x - \\hat{x}^{(1)}$. Using this operator, determine explicitly the largest value of $M \\geq 0$ (as a function of $\\varepsilon$) for which the first iterative refinement step strictly decreases the forward error in the $2$-norm, i.e., for which $\\|e^{(1)}\\|_2 < \\|e\\|_2$ for all nonzero $e$.\n\nYour derivation must explain why, despite the residual $r$ being computed in higher precision (hence exact for $A_{M}$ and $\\hat{x}$), the nonnormality of $A_{M}$ can make the residual misleading, and how this ties to the sensitivity of $A_{M}^{-1}$ to rounding via the resolvent $(A_{M} + E)^{-1}$. Express the final answer $M_{\\text{crit}}(\\varepsilon)$ as a closed-form analytic expression. No rounding is required, and there are no physical units in this problem. The final answer must be a single analytical expression.", "solution": "The problem is first validated to ensure it is scientifically sound, well-posed, objective, and self-contained.\n\n### Step 1: Extract Givens\n-   The matrix family is $A_{M} = \\begin{pmatrix} 1 & M \\\\ 0 & 1 \\end{pmatrix} \\in \\mathbb{R}^{2 \\times 2}$.\n-   The right-hand side vector is $b = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$.\n-   The exact solution $x$ satisfies $A_{M} x = b$.\n-   The computed approximation $\\hat{x}$ satisfies $(A_{M} + E)\\hat{x} = b$.\n-   The backward error matrix is $E = \\begin{pmatrix} 0 & 0 \\\\ \\varepsilon & 0 \\end{pmatrix}$, with $0 < \\varepsilon \\ll 1$.\n-   The iterative refinement update is defined by:\n    1.  Residual computation: $r = b - A_{M}\\hat{x}$ (exact).\n    2.  Correction computation: $(A_{M} + E)d = r$.\n    3.  New approximation: $\\hat{x}^{(1)} = \\hat{x} + d$.\n-   The forward errors are $e = x - \\hat{x}$ and $e^{(1)} = x - \\hat{x}^{(1)}$.\n-   The task is to find the linear operator $T(M, \\varepsilon)$ such that $e^{(1)} = T(M, \\varepsilon)e$ and to find the largest $M \\geq 0$ for which $\\|e^{(1)}\\|_2 < \\|e\\|_2$ for all nonzero $e$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is well-defined within the field of numerical linear algebra. It uses standard concepts such as backward error, forward error, iterative refinement, matrix norms, and nonnormality. The matrix $A_M$ is a classic example of a nonnormal matrix (a shear matrix, related to a Jordan block), often used to study phenomena where spectral information can be misleading. The setup for iterative refinement, where the same perturbed solver is used to find the correction, is standard. The problem is mathematically and scientifically sound, self-contained, and objective. It is not trivial and addresses a core concept in the stability analysis of numerical algorithms.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\n### Solution Derivation\nThe derivation proceeds as follows:\n1.  Determine the exact solution $x$.\n2.  Derive the error propagation operator $T(M, \\varepsilon)$ that maps the initial forward error $e$ to the new forward error $e^{(1)}$.\n3.  Analyze the condition for strict error reduction, which is that the spectral norm (2-norm) of the operator $T$ must be less than $1$.\n4.  Solve the resulting inequality for $M$ to find the critical value $M_{\\text{crit}}$.\n\nFirst, we find the exact solution $x$ from $A_{M}x = b$:\n$$\n\\begin{pmatrix} 1 & M \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}\n$$\nSolving this system by back substitution gives $x_2 = 0$ and $x_1 + M(0) = 1$, so $x_1 = 1$. Thus, the exact solution is $x = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$.\n\nNext, we derive the error propagation operator. The new approximation is $\\hat{x}^{(1)} = \\hat{x} + d$. The new forward error is:\n$$\ne^{(1)} = x - \\hat{x}^{(1)} = x - (\\hat{x} + d) = (x - \\hat{x}) - d = e - d\n$$\nThe correction $d$ is the solution to $(A_{M} + E)d = r$. The residual $r$ is computed exactly as $r = b - A_{M}\\hat{x}$. Since $b=A_Mx$, we can write:\n$$\nr = A_{M}x - A_{M}\\hat{x} = A_{M}(x - \\hat{x}) = A_{M}e\n$$\nThis relationship is exact. Now we express $d$ in terms of $e$:\n$$\nd = (A_{M} + E)^{-1}r = (A_{M} + E)^{-1}A_{M}e\n$$\nSubstituting this into the expression for $e^{(1)}$:\n$$\ne^{(1)} = e - (A_{M} + E)^{-1}A_{M}e = \\left(I - (A_{M} + E)^{-1}A_{M}\\right)e\n$$\nFrom this, we identify the error propagation operator $T(M, \\varepsilon)$:\n$$\nT(M, \\varepsilon) = I - (A_{M} + E)^{-1}A_{M}\n$$\nTo compute $T$ explicitly, we first find the inverse of $A_{M} + E$:\n$$\nA_{M} + E = \\begin{pmatrix} 1 & M \\\\ \\varepsilon & 1 \\end{pmatrix}\n$$\nThe determinant is $\\det(A_{M} + E) = 1 \\cdot 1 - M \\cdot \\varepsilon = 1 - M\\varepsilon$. For the inverse to exist, we must have $M\\varepsilon \\neq 1$. Given $0 < \\varepsilon \\ll 1$, this is a reasonable assumption.\n$$\n(A_{M} + E)^{-1} = \\frac{1}{1 - M\\varepsilon} \\begin{pmatrix} 1 & -M \\\\ -\\varepsilon & 1 \\end{pmatrix}\n$$\nNow, we compute the product $(A_{M} + E)^{-1}A_{M}$:\n$$\n(A_{M} + E)^{-1}A_{M} = \\frac{1}{1 - M\\varepsilon} \\begin{pmatrix} 1 & -M \\\\ -\\varepsilon & 1 \\end{pmatrix} \\begin{pmatrix} 1 & M \\\\ 0 & 1 \\end{pmatrix} = \\frac{1}{1 - M\\varepsilon} \\begin{pmatrix} 1 & 0 \\\\ -\\varepsilon & 1-M\\varepsilon \\end{pmatrix}\n$$\nFinally, we compute $T(M, \\varepsilon)$:\n$$\nT(M, \\varepsilon) = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} - \\frac{1}{1 - M\\varepsilon} \\begin{pmatrix} 1 & 0 \\\\ -\\varepsilon & 1-M\\varepsilon \\end{pmatrix}\n$$\n$$\nT(M, \\varepsilon) = \\begin{pmatrix} 1 - \\frac{1}{1-M\\varepsilon} & 0 \\\\ 0 - \\frac{-\\varepsilon}{1-M\\varepsilon} & 1 - \\frac{1-M\\varepsilon}{1-M\\varepsilon} \\end{pmatrix} = \\begin{pmatrix} \\frac{1-M\\varepsilon-1}{1-M\\varepsilon} & 0 \\\\ \\frac{\\varepsilon}{1-M\\varepsilon} & 0 \\end{pmatrix} = \\frac{1}{1-M\\varepsilon} \\begin{pmatrix} -M\\varepsilon & 0 \\\\ \\varepsilon & 0 \\end{pmatrix}\n$$\nThe nonnormality of $A_M$ for $M \\neq 0$ creates a situation where the norm of the residual $\\|r\\|_2$ can be small, while the norm of the forward error $\\|e\\|_2$ is large. Here, $r = A_M e$, so $e=A_M^{-1}r$. The norm of the inverse, $\\|A_M^{-1}\\|_2 = \\|\\begin{pmatrix} 1 & -M \\\\ 0 & 1 \\end{pmatrix}\\|_2 = \\frac{1}{2}(\\sqrt{M^2+4}+M)$, grows with $M$. This amplification of the residual to the error is a hallmark of nonnormal systems. The iterative refinement process tries to correct the solution using a solver based on the perturbed matrix $A_M+E$. The operator $T$ quantifies the failure of this correction, which fails precisely because the perturbed inverse $(A_M+E)^{-1}$ is a poor approximation to $A_M^{-1}$ when $M$ is large.\n\nThe condition for the first iterative refinement step to strictly decrease the forward error for any nonzero error vector $e$ is that the induced 2-norm (spectral norm) of the operator $T$ must be strictly less than $1$:\n$$\n\\|T(M, \\varepsilon)\\|_2 < 1\n$$\nThe spectral norm $\\|T\\|_2$ is the square root of the largest eigenvalue of $T^T T$.\n$$\nT^T = \\frac{1}{1-M\\varepsilon} \\begin{pmatrix} -M\\varepsilon & \\varepsilon \\\\ 0 & 0 \\end{pmatrix}\n$$\n$$\nT^T T = \\frac{1}{(1-M\\varepsilon)^2} \\begin{pmatrix} -M\\varepsilon & \\varepsilon \\\\ 0 & 0 \\end{pmatrix} \\begin{pmatrix} -M\\varepsilon & 0 \\\\ \\varepsilon & 0 \\end{pmatrix} = \\frac{1}{(1-M\\varepsilon)^2} \\begin{pmatrix} (-M\\varepsilon)^2 + \\varepsilon^2 & 0 \\\\ 0 & 0 \\end{pmatrix}\n$$\n$$\nT^T T = \\frac{\\varepsilon^2(M^2+1)}{(1-M\\varepsilon)^2} \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix}\n$$\nThe eigenvalues of $T^T T$ are $\\lambda_1 = \\frac{\\varepsilon^2(M^2+1)}{(1-M\\varepsilon)^2}$ and $\\lambda_2 = 0$. The largest eigenvalue is $\\lambda_1$.\nThe spectral norm is therefore:\n$$\n\\|T\\|_2 = \\sqrt{\\lambda_1} = \\sqrt{\\frac{\\varepsilon^2(M^2+1)}{(1-M\\varepsilon)^2}} = \\frac{|\\varepsilon|\\sqrt{M^2+1}}{|1-M\\varepsilon|}\n$$\nGiven that $M \\ge 0$ and $0 < \\varepsilon \\ll 1$, we can assume $1 - M\\varepsilon > 0$. The inequality becomes:\n$$\n\\frac{\\varepsilon\\sqrt{M^2+1}}{1-M\\varepsilon} < 1\n$$\nSince both sides are positive, we can square the inequality without changing its direction:\n$$\n\\frac{\\varepsilon^2(M^2+1)}{(1-M\\varepsilon)^2} < 1 \\implies \\varepsilon^2(M^2+1) < (1-M\\varepsilon)^2\n$$\n$$\n\\varepsilon^2 M^2 + \\varepsilon^2 < 1 - 2M\\varepsilon + M^2\\varepsilon^2\n$$\nCanceling the $\\varepsilon^2 M^2$ term from both sides gives:\n$$\n\\varepsilon^2 < 1 - 2M\\varepsilon\n$$\nNow, we solve for $M$:\n$$\n2M\\varepsilon < 1 - \\varepsilon^2\n$$\n$$\nM < \\frac{1 - \\varepsilon^2}{2\\varepsilon}\n$$\nThis inequality defines the range of $M \\ge 0$ for which the iterative refinement step is guaranteed to be a contraction in the 2-norm. The question asks for the largest value of $M$ for which this strict inequality holds. This corresponds to the supremum of the interval $[0, \\frac{1 - \\varepsilon^2}{2\\varepsilon})$, which is the boundary value.\n$$\nM_{\\text{crit}}(\\varepsilon) = \\frac{1 - \\varepsilon^2}{2\\varepsilon}\n$$\nFor any $M < M_{\\text{crit}}$, refinement converges; for $M > M_{\\text{crit}}$, it can diverge. At $M=M_{\\text{crit}}$, $\\|T\\|_2 = 1$, and there exists at least one error direction (the first left singular vector of $T$) for which the error norm does not strictly decrease.", "answer": "$$\\boxed{\\frac{1 - \\varepsilon^{2}}{2\\varepsilon}}$$", "id": "3581533"}, {"introduction": "We now transition from analytical exercises to a practical computational problem involving a workhorse of numerical linear algebra: the Lanczos algorithm. While elegant in theory, the standard Lanczos process is notoriously unstable in finite precision, as the basis vectors quickly lose their orthogonality. This hands-on coding challenge [@problem_id:3581514] guides you through implementing and comparing different reorthogonalization strategies to witness firsthand how these stability issues manifest and how they can be effectively controlled in practice.", "problem": "Design and implement a program that investigates the stability of the symmetric Lanczos process under finite precision arithmetic by applying a selective reorthogonalization strategy to maintain orthogonality among Krylov basis vectors and quantifying its effect on the accuracy of Ritz values. Your program must implement three variants of the Lanczos iteration for real symmetric matrices: no reorthogonalization, selective reorthogonalization, and full reorthogonalization. The empirical comparison must adhere to the following specifications.\n\nFundamental base and assumptions:\n- Use the standard floating-point model of rounding for binary64 arithmetic: for any basic operation, $\\mathrm{fl}(x \\circ y) = (x \\circ y) (1 + \\delta)$ with $|\\delta| \\leq \\epsilon_{\\mathrm{mach}}$, where $\\epsilon_{\\mathrm{mach}}$ denotes machine epsilon for double precision.\n- The Krylov subspace of order $k$ for a real symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$ and a starting vector $v_{1}$ with $\\|v_{1}\\|_{2} = 1$ is $\\mathcal{K}_{k}(A, v_{1}) = \\mathrm{span}\\{v_{1}, A v_{1}, \\dots, A^{k-1} v_{1} \\}$. The Lanczos process attempts to generate an orthonormal basis $\\{v_{1}, \\dots, v_{k}\\}$ for $\\mathcal{K}_{k}(A, v_{1})$, producing a real symmetric tridiagonal matrix $T_{k} \\in \\mathbb{R}^{k \\times k}$ whose eigenvalues are the Ritz values with respect to $(A, \\mathcal{K}_{k})$.\n- In exact arithmetic, the Lanczos vectors are orthonormal and a $3$-term recurrence suffices. In finite precision arithmetic, loss of orthogonality may occur; reorthogonalization can mitigate this instability.\n\nSelective reorthogonalization directive:\n- Implement a selective reorthogonalization criterion based on measured loss of orthogonality. Let $\\eta = c \\sqrt{\\epsilon_{\\mathrm{mach}}}$ with $c = 10$. At Lanczos step $j$, after forming the candidate vector $w$, compute inner products $h_{i} = v_{i}^{\\top} w$ for $i = 1, \\dots, j$. If $|h_{i}| > \\eta$ for any $i$, perform modified Gram–Schmidt reorthogonalization against those $v_{i}$ for which the inequality holds, and repeat this check at most $2$ passes to reduce $|h_{i}|$ back near $\\mathcal{O}(\\epsilon_{\\mathrm{mach}})$.\n- For the full reorthogonalization variant, at each step $j$ perform modified Gram–Schmidt reorthogonalization against all previously computed Lanczos vectors $\\{v_{1}, \\dots, v_{j}\\}$, performing at most $2$ passes.\n- For the no reorthogonalization variant, perform only the $3$-term recurrence without any Gram–Schmidt steps.\n\nAccuracy metric:\n- For a given symmetric matrix $A$, a target subspace dimension $k$, and a target count $r \\leq k$, compute the $r$ largest Ritz values from $T_{k}$ (in algebraic order) and compare them against the $r$ largest true eigenvalues of $A$ (in algebraic order). Define the error as the maximum absolute deviation after sorting both sets in descending order:\n$$\nE = \\max_{1 \\leq i \\leq r} \\left| \\lambda^{(A)}_{(i)} - \\theta^{(T)}_{(i)} \\right|,\n$$\nwhere $\\lambda^{(A)}_{(i)}$ are the $r$ largest eigenvalues of $A$ and $\\theta^{(T)}_{(i)}$ are the $r$ largest Ritz values.\n- Define the improvement factor due to selective reorthogonalization as\n$$\n\\mathcal{I} = \\frac{E_{\\mathrm{none}}}{E_{\\mathrm{sel}}},\n$$\nwhere $E_{\\mathrm{none}}$ is the error under no reorthogonalization and $E_{\\mathrm{sel}}$ is the error under selective reorthogonalization. If $E_{\\mathrm{sel}} = 0$, report $\\mathcal{I} = 10^{16}$.\n\nImplementation requirements:\n- Implement the symmetric Lanczos process that returns the tridiagonal $T_{k}$ for each of the three variants described above.\n- Use a fixed, reproducible random starting vector $v_{1}$ with $\\|v_{1}\\|_{2} = 1$ for each test case.\n- Compute true eigenvalues of $A$ using a numerically stable symmetric eigensolver.\n\nTest suite:\n- Your program must run the following three test cases and report the improvement factor $\\mathcal{I}$ for each case as defined above. In all cases, use double precision arithmetic, no physical units are involved, and no angles are used.\n    1. Happy path with well-separated spectrum:\n        - $n = 80$, $k = 30$, $r = 5$.\n        - Construct $A = Q \\Lambda Q^{\\top}$, where $\\Lambda = \\mathrm{diag}(\\ell_{1}, \\dots, \\ell_{n})$ with $\\ell_{i}$ linearly spaced from $1$ to $100$, and $Q$ is the orthogonal factor from the $\\mathrm{QR}$ factorization of a random Gaussian matrix with seed $0$.\n        - Use a random Gaussian starting vector with seed $1$, normalized to unit $2$-norm.\n    2. Challenging cluster near the top of the spectrum:\n        - $n = 120$, $k = 60$, $r = 8$.\n        - Construct $A = Q \\Lambda Q^{\\top}$, where $\\Lambda$ has $10$ eigenvalues near $10$ given by $10 + \\delta_{i}$ with $\\delta_{i}$ i.i.d. uniformly sampled in $[-10^{-8}, 10^{-8}]$, and the remaining $110$ eigenvalues linearly spaced in $[0.1, 9.9]$. Use seed $2$ for generating $Q$ and for the uniform samples.\n        - Use a random Gaussian starting vector with seed $3$, normalized to unit $2$-norm.\n    3. Boundary case with long run and structured operator:\n        - $n = 90$, $k = 90$, $r = 10$.\n        - Construct $A$ as the tridiagonal Toeplitz matrix with zeros on the diagonal and ones on the first sub- and super-diagonals, i.e., $A_{i,i} = 0$ and $A_{i,i+1} = A_{i+1,i} = 1$ for $i = 1, \\dots, n-1$.\n        - Use a random Gaussian starting vector with seed $4$, normalized to unit $2$-norm.\n\nRequired final output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain exactly $3$ floating-point numbers corresponding to the improvement factors $\\mathcal{I}$ for the test cases $1$, $2$, and $3$ in this order. For example, the format must be exactly like $[x_{1},x_{2},x_{3}]$ with no extra spaces, where $x_{i}$ are decimal strings.\n\nScoring and acceptance criteria:\n- The program must be self-contained and runnable without user input.\n- The selective reorthogonalization must use the threshold $\\eta = 10 \\sqrt{\\epsilon_{\\mathrm{mach}}}$ and at most $2$ Gram–Schmidt passes per step.\n- Each $\\mathcal{I}$ must be a finite nonnegative float, using the rule $\\mathcal{I} = 10^{16}$ if $E_{\\mathrm{sel}} = 0$.", "solution": "The user requests an implementation and analysis of the symmetric Lanczos process, focusing on numerical stability. Specifically, the task is to compare the accuracy of Ritz values computed with no reorthogonalization against those computed using a selective reorthogonalization strategy. The comparison is quantified by an improvement factor, $\\mathcal{I}$. The problem is well-defined, scientifically sound, and provides all necessary parameters, including matrix constructions, algorithm specifications, and evaluation metrics, making it a valid and verifiable numerical experiment.\n\nI will structure the solution by first implementing the core Lanczos iteration, which can operate in two modes: `none` (no reorthogonalization) and `selective`. Then, I will create helper functions to construct the specific matrices and starting vectors for each of the three test cases. A main function will orchestrate the execution for each test case, compute the required errors and improvement factors, and format the output as specified.\n\n### The Symmetric Lanczos Algorithm\n\nThe Lanczos process generates an orthonormal basis $\\{v_1, v_2, \\dots, v_k\\}$ for the Krylov subspace $\\mathcal{K}_k(A, v_1)$ and a symmetric tridiagonal matrix $T_k$. The recurrence relation is:\n$$ \\beta_j v_{j+1} = A v_j - \\alpha_j v_j - \\beta_{j-1} v_{j-1} $$\nwhere $\\alpha_j = v_j^\\top A v_j$ and $\\beta_j = \\|A v_j - \\alpha_j v_j - \\beta_{j-1} v_{j-1}\\|_2$. In finite precision arithmetic, the orthogonality of the vectors $\\{v_j\\}$ is quickly lost, leading to inaccuracies in the Ritz values (eigenvalues of $T_k$).\n\n### Reorthogonalization Strategies\n\nTo counteract this instability, reorthogonalization is employed.\n1.  **No Reorthogonalization (`none`)**: The basic recurrence is used without any correction. This is cheap but numerically unstable for long iterations.\n2.  **Selective Reorthogonalization (`selective`)**: Orthogonality is selectively enforced. At each step $j$, the new vector $w_j = A v_j - \\alpha_j v_j - \\beta_{j-1} v_{j-1}$ (which should be orthogonal to $v_1, \\dots, v_j$) is checked. If its projection onto any previous vector $v_i$ is larger than a threshold $\\eta = c \\sqrt{\\epsilon_{\\mathrm{mach}}}$ (with $c=10$), we reorthogonalize $w_j$ against those specific vectors. The problem specifies using Modified Gram-Schmidt (MGS) for this correction, repeated for at most two passes to ensure orthogonality is restored. This provides a balance between the cost of full reorthogonalization and the instability of no reorthogonalization.\n\n### Implementation Details\n\nA function `lanczos_iteration(A, v1, k, mode)` will be implemented. It will take the matrix $A$, starting vector $v_1$, number of iterations $k$, and a `mode` string (`'none'` or `'selective'`).\n\n-   The loop will iterate $j$ from $0$ to $k-1$.\n-   Inside the loop, for each new vector, the three-term recurrence will be applied.\n-   If `mode` is `'selective'`, the reorthogonalization logic will be triggered. This involves computing inner products $h_i = v_i^\\top w$ and, if $|h_i| > \\eta$, applying MGS steps: $w \\leftarrow w - (v_i^\\top w)v_i$. This check-and-correct procedure is done in up to two passes.\n-   The function returns the $k \\times k$ tridiagonal matrix $T_k$.\n\n### Evaluation\n\nFor each test case:\n1.  The specified $n \\times n$ matrix $A$ and starting vector $v_1$ are constructed.\n2.  The true $r$ largest eigenvalues of $A$, denoted $\\lambda^{(A)}_{(i)}$, are computed using a stable eigensolver (`numpy.linalg.eigh`).\n3.  The Lanczos iteration is run for both `mode='none'` and `mode='selective'` to obtain $T_{k, \\text{none}}$ and $T_{k, \\text{sel}}$.\n4.  The $r$ largest eigenvalues (Ritz values) of each $T_k$ matrix, denoted $\\theta^{(T)}_{(i)}$, are computed using an efficient tridiagonal eigensolver (`scipy.linalg.eigh_tridiagonal`).\n5.  The errors $E_{\\mathrm{none}}$ and $E_{\\mathrm{sel}}$ are calculated as the maximum absolute difference between the corresponding sorted lists of true eigenvalues and Ritz values:\n    $$ E = \\max_{1 \\leq i \\leq r} \\left| \\lambda^{(A)}_{(i)} - \\theta^{(T)}_{(i)} \\right| $$\n6.  The improvement factor $\\mathcal{I} = E_{\\mathrm{none}} / E_{\\mathrm{sel}}$ is computed. The special case where $E_{\\mathrm{sel}}=0$ is handled by setting $\\mathcal{I}=10^{16}$.\n\nThis process is repeated for the three defined test cases, and the resulting improvement factors are collected and printed.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import eigh_tridiagonal\n\ndef lanczos_iteration(A, v1, k, mode='none'):\n    \"\"\"\n    Performs the symmetric Lanczos iteration for a matrix A and starting vector v1.\n\n    Args:\n        A (np.ndarray): The symmetric matrix.\n        v1 (np.ndarray): The starting vector of unit norm.\n        k (int): The number of Lanczos steps (dimension of Krylov subspace).\n        mode (str): Reorthogonalization mode: 'none' or 'selective'.\n\n    Returns:\n        np.ndarray: The symmetric tridiagonal matrix T_k of size k x k.\n    \"\"\"\n    n = A.shape[0]\n    V = np.zeros((n, k + 1))\n    alphas = np.zeros(k)\n    betas = np.zeros(k)\n\n    eps_mach = np.finfo(float).eps\n    eta = 10.0 * np.sqrt(eps_mach)\n    breakdown_tol = 1e-14\n\n    V[:, 0] = v1\n\n    for j in range(k):\n        v_curr = V[:, j]\n        w = A @ v_curr\n\n        if j > 0:\n            w -= betas[j - 1] * V[:, j - 1]\n\n        alphas[j] = v_curr.T @ w\n        w -= alphas[j] * v_curr\n\n        if mode == 'selective':\n            # Perform selective reorthogonalization with at most 2 MGS passes\n            for _ in range(2):\n                # Projections of w onto the basis V\n                h = V[:, :j + 1].T @ w\n                indices_to_reortho = np.where(np.abs(h) > eta)[0]\n                \n                if len(indices_to_reortho) == 0:\n                    break  # Orthogonality is sufficient\n                \n                # Apply MGS steps for the vectors that lost orthogonality\n                for i in indices_to_reortho:\n                    w -= (V[:, i].T @ w) * V[:, i]\n\n        betas[j] = np.linalg.norm(w)\n\n        if betas[j]  breakdown_tol:\n            # Breakdown: Krylov subspace is invariant or exhausted.\n            k_actual = j + 1\n            T_k = np.zeros((k, k))\n            sub_T_alphas = alphas[:k_actual]\n            sub_T_betas = betas[:k_actual - 1]\n            T_k_sub = np.diag(sub_T_alphas) + np.diag(sub_T_betas, 1) + np.diag(sub_T_betas, -1)\n            T_k[:k_actual, :k_actual] = T_k_sub\n            return T_k\n            \n        V[:, j + 1] = w / betas[j]\n\n    T_k = np.diag(alphas) + np.diag(betas[:k - 1], 1) + np.diag(betas[:k - 1], -1)\n    return T_k\n\ndef run_test_case(case_id):\n    \"\"\"\n    Sets up and runs a single test case, returning the improvement factor.\n    \"\"\"\n    if case_id == 1:\n        n, k, r = 80, 30, 5\n        rng_Q = np.random.default_rng(0)\n        G = rng_Q.standard_normal((n, n))\n        Q, _ = np.linalg.qr(G)\n        lambda_vals = np.linspace(1, 100, n)\n        A = Q @ np.diag(lambda_vals) @ Q.T\n        rng_v1 = np.random.default_rng(1)\n        v1 = rng_v1.standard_normal(n)\n    elif case_id == 2:\n        n, k, r = 120, 60, 8\n        rng = np.random.default_rng(2)\n        cluster_vals = 10.0 + rng.uniform(-1e-8, 1e-8, size=10)\n        other_vals = np.linspace(0.1, 9.9, n - 10)\n        lambda_vals = np.concatenate((other_vals, cluster_vals))\n        G = rng.standard_normal((n, n))\n        Q, _ = np.linalg.qr(G)\n        A = Q @ np.diag(lambda_vals) @ Q.T\n        rng_v1 = np.random.default_rng(3)\n        v1 = rng_v1.standard_normal(n)\n    elif case_id == 3:\n        n, k, r = 90, 90, 10\n        A = np.diag(np.ones(n - 1), 1) + np.diag(np.ones(n - 1), -1)\n        rng_v1 = np.random.default_rng(4)\n        v1 = rng_v1.standard_normal(n)\n    else:\n        raise ValueError(\"Invalid case ID\")\n        \n    v1 /= np.linalg.norm(v1)\n\n    # Compute true eigenvalues of A\n    true_eigvals = np.linalg.eigh(A)[0]\n    largest_true_eigvals = np.flip(true_eigvals[-r:]) # descending order\n\n    # Run Lanczos with no reorthogonalization\n    T_none = lanczos_iteration(A, v1, k, mode='none')\n    ritz_vals_none = eigh_tridiagonal(np.diag(T_none), np.diag(T_none, 1))[0]\n    largest_ritz_none = np.flip(ritz_vals_none[-r:])\n\n    # Run Lanczos with selective reorthogonalization\n    T_sel = lanczos_iteration(A, v1, k, mode='selective')\n    ritz_vals_sel = eigh_tridiagonal(np.diag(T_sel), np.diag(T_sel, 1))[0]\n    largest_ritz_sel = np.flip(ritz_vals_sel[-r:])\n\n    # Compute errors\n    E_none = np.max(np.abs(largest_true_eigvals - largest_ritz_none))\n    E_sel = np.max(np.abs(largest_true_eigvals - largest_ritz_sel))\n    \n    # Compute improvement factor\n    if E_sel == 0.0:\n        if E_none == 0.0:\n            return 1.0\n        return 1e16\n    \n    return E_none / E_sel\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    # The problem statement defines the test cases.\n    # We simply iterate through them.\n    test_cases = [1, 2, 3]\n\n    results = []\n    for case_id in test_cases:\n        improvement_factor = run_test_case(case_id)\n        results.append(improvement_factor)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3581514"}]}