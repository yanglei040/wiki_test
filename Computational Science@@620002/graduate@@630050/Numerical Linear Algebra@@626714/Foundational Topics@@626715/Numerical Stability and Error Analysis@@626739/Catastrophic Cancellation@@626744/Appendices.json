{"hands_on_practices": [{"introduction": "While summation appears to be one of the most elementary operations in computing, it serves as a powerful case study for the subtleties of floating-point arithmetic. This practice explores how catastrophic cancellation can undermine the accuracy of a sum, particularly when the sequence contains alternating signs and is ill-conditioned. By implementing and comparing three distinct algorithms—naive summation, recursive pairwise summation, and Kahan's compensated summation—you will empirically observe how algorithmic design choices lead to vastly different error characteristics, ranging from linear to logarithmic to constant error growth [@problem_id:3536136].", "problem": "Let $u$ denote the unit roundoff of the standard double-precision binary format in Institute of Electrical and Electronics Engineers (IEEE) $754$ arithmetic, specifically $u = 2^{-53}$. Consider the floating-point model for addition: for real numbers $x$ and $y$, the computed floating-point sum is $\\operatorname{fl}(x + y) = (x + y)(1 + \\delta)$ with $|\\delta| \\le u$, assuming rounding to nearest with ties to even and absence of overflow, underflow, or subnormal complications. The phenomenon of catastrophic cancellation arises when adding numbers of opposite signs whose magnitudes are close, resulting in loss of leading digits of significance and amplification of rounding errors in the result. In numerical linear algebra, summation strategies influence how cancellation affects the accumulated error.\n\nDesign a test sequence exhibiting severe cancellation that allows empirical comparison of three summation strategies as the sequence length $n$ grows while $u$ is fixed:\n- Naive sequential summation: compute $S_{\\text{naive}}(x_1,\\dots,x_n)$ by iteratively accumulating from $i=1$ to $n$ using $s \\leftarrow \\operatorname{fl}(s + x_i)$ with $s$ initialized to $0$.\n- Pairwise (binary tree) summation: compute $S_{\\text{pairwise}}(x_1,\\dots,x_n)$ by recursively splitting the index set into halves, summing each half, and then adding the two partial sums.\n- Kahan compensated summation: compute $S_{\\text{Kahan}}(x_1,\\dots,x_n)$ with a compensation variable $c$ to carry low-order parts, using the iteration $y \\leftarrow \\operatorname{fl}(x_i - c)$, $t \\leftarrow \\operatorname{fl}(s + y)$, $c \\leftarrow \\operatorname{fl}(\\operatorname{fl}(t - s) - y)$, $s \\leftarrow t$, with $s$ and $c$ initialized to $0$.\n\nFor a fixed scalar $s = 1$, define for each $n \\in \\mathbb{N}$ the length-$n$ vector $x \\in \\mathbb{R}^n$ by\n$$\nx_i = (-1)^{i-1} + \\frac{s}{n}, \\quad i = 1,2,\\dots,n.\n$$\nThis sequence has alternating signs enforcing cancellation between the $\\pm 1$ components while adding a uniform bias $\\frac{s}{n}$ so that the exact sum is\n$$\nS_{\\text{exact}}(n) = \\sum_{i=1}^{n} x_i = \n\\begin{cases}\ns, & \\text{if } n \\text{ is even}, \\\\\n1 + s, & \\text{if } n \\text{ is odd}.\n\\end{cases}\n$$\nGiven the floating-point computations of the three summation strategies, define the absolute error of each strategy for a given $n$ as\n$$\nE_{\\text{naive}}(n) = \\left| S_{\\text{naive}}(x_1,\\dots,x_n) - S_{\\text{exact}}(n) \\right|,\n$$\n$$\nE_{\\text{pairwise}}(n) = \\left| S_{\\text{pairwise}}(x_1,\\dots,x_n) - S_{\\text{exact}}(n) \\right|,\n$$\n$$\nE_{\\text{Kahan}}(n) = \\left| S_{\\text{Kahan}}(x_1,\\dots,x_n) - S_{\\text{exact}}(n) \\right|.\n$$\n\nImplement a complete program that:\n- Uses the specified floating-point model implicitly by relying on the host language’s double-precision arithmetic.\n- Constructs the sequence $x$ above for each $n$ in a provided test suite.\n- Computes $E_{\\text{naive}}(n)$, $E_{\\text{pairwise}}(n)$, and $E_{\\text{Kahan}}(n)$ for each $n$.\n\nTest suite and coverage:\n- Use the test suite $n \\in \\{1, 2, 8, 64, 1024, 16384, 65536\\}$ to cover:\n  - A boundary minimal case $n = 1$ with no cancellation.\n  - Small even $n$ values $n = 2, 8$ illustrating initial cancellation effects.\n  - Moderate $n$ value $n = 64$ showing increased cancellation depth.\n  - Large $n$ values $n = 1024, 16384, 65536$ to assess error growth scaling with $n$.\n- For each $n$, compute and record the three absolute errors as floating-point numbers.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets and ordered as\n$$\n[\\;E_{\\text{naive}}(1),E_{\\text{pairwise}}(1),E_{\\text{Kahan}}(1),E_{\\text{naive}}(2),E_{\\text{pairwise}}(2),E_{\\text{Kahan}}(2),\\dots,E_{\\text{naive}}(65536),E_{\\text{pairwise}}(65536),E_{\\text{Kahan}}(65536)\\;].\n$$\nNo additional text should be printed. All numerical values must be in pure floating-point units without any physical units attached.", "solution": "The problem statement is analyzed and found to be valid. It describes a well-posed, scientifically grounded computational experiment in numerical linear algebra to demonstrate the effects of catastrophic cancellation and compare the stability of three fundamental summation algorithms. All necessary data, definitions, and constraints are provided, and there are no contradictions or ambiguities.\n\n### Theoretical Framework\n\nThe core of this problem is to observe catastrophic cancellation, a phenomenon in floating-point arithmetic where the subtraction of two nearly equal numbers results in a dramatic loss of relative precision. The computed result is dominated by rounding errors from previous computations, rather than the true mathematical difference.\n\nThe severity of this issue for a summation problem can be quantified by its condition number. For a sum $S = \\sum_{i=1}^{n} x_i$, the condition number is given by $\\kappa = \\frac{\\sum_{i=1}^{n} |x_i|}{|S|}$. A large condition number indicates that small relative errors in the input values $x_i$ can be magnified into large relative errors in the final sum $S$.\n\nFor the test sequence provided, $x_i = (-1)^{i-1} + \\frac{s}{n}$ with $s=1$, the individual terms $|x_i|$ are all close to $1$. Thus, $\\sum_{i=1}^{n} |x_i| \\approx n$. The exact sum, $S_{\\text{exact}}(n)$, is either $s=1$ (for even $n$) or $1+s=2$ (for odd $n$). The condition number is therefore $\\kappa \\approx n$, which is large for large $n$. This signifies that the summation is ill-conditioned and highly susceptible to rounding errors. The goal is to see how different algorithms cope with this inherent instability.\n\n### Algorithm Analysis and Implementation Strategy\n\nWe will implement and compare three algorithms, each with distinct error propagation characteristics. The implementations will rely on standard double-precision floating-point arithmetic ($u = 2^{-53}$), as implicitly provided by the Python runtime environment.\n\n1.  **Naive Sequential Summation ($S_{\\text{naive}}$)**\n    *   **Principle:** This is the most basic method, defined by the simple iterative accumulation $s_{new} \\leftarrow \\operatorname{fl}(s_{old} + x_i)$. The sum is computed in a fixed order, typically from $i=1$ to $n$.\n    *   **Error Analysis:** For an ill-conditioned sum like this one, naive summation performs poorly. The running sum $s_k = \\sum_{i=1}^{k} x_i$ alternates between values close to $1$ (for odd $k$) and values close to $0$ (for even $k$). When an intermediate sum $s_{k-1} \\approx 1$ is added to the next term $x_k \\approx -1$, catastrophic cancellation occurs. The rounding error from each step is carried into the next, and the total absolute error is theoretically bounded by, and empirically observed to grow in proportion to, $n \\cdot u \\cdot \\max_k |s_k|$. For this problem, the error scales as $O(n \\cdot u)$.\n    *   **Implementation:** A standard `for` loop that iterates through the elements of the input sequence and accumulates the sum in a single floating-point variable initialized to $0.0$.\n\n2.  **Pairwise Summation ($S_{\\text{pairwise}}$)**\n    *   **Principle:** This is a recursive, divide-and-conquer algorithm. The sequence is split into two halves, each half is summed recursively, and the two resulting partial sums are finally added together: $S(x_1, \\dots, x_n) = \\operatorname{fl}(S(x_1, \\dots, x_{\\lfloor n/2 \\rfloor}) + S(x_{\\lfloor n/2 \\rfloor+1}, \\dots, x_n))$.\n    *   **Error Analysis:** Pairwise summation mitigates error accumulation by structuring the additions in a binary tree. This reduces the maximum number of rounding errors any single term $x_i$ can accumulate from $\\approx n$ to $\\log_2 n$. The resulting error bound scales as $O(u \\cdot \\log n)$. This logarithmic growth is a substantial improvement over the linear growth of the naive method.\n    *   **Implementation:** A recursive function is a natural way to implement this. To avoid the performance penalty of creating array slices at each level of recursion, the implementation will pass start and end indices to operate on a single, shared array. The base case of the recursion handles sequences of length $1$ or $0$.\n\n3.  **Kahan Compensated Summation ($S_{\\text{Kahan}}$)**\n    *   **Principle:** This sophisticated algorithm explicitly tracks and corrects for the rounding error in each addition. It uses a compensation variable, $c$, to store the low-order part of the result that is lost to rounding. The core of the iteration is $y \\leftarrow \\operatorname{fl}(x_i - c)$, $t \\leftarrow \\operatorname{fl}(s + y)$, $c \\leftarrow \\operatorname{fl}(\\operatorname{fl}(t - s) - y)$, and $s \\leftarrow t$. The term $\\operatorname{fl}(t-s) - y$ is a clever way to recover the negative of the rounding error from the addition $s+y$. This error is then subtracted from the next term $x_{i+1}$ (via the $y \\leftarrow x_{i+1} - c$ step), effectively re-injecting the lost precision into the sum.\n    *   **Error Analysis:** The genius of Kahan's algorithm is that the accumulated error is bounded by a small constant multiple of the unit roundoff, independent of the number of terms $n$. The error bound is $O(u)$. This makes it exceptionally accurate, even for very long and ill-conditioned sums.\n    *   **Implementation:** A `for` loop that initializes both the sum $s$ and the compensator $c$ to $0.0$. Inside the loop, the four-step Kahan update is performed for each element in the sequence.\n\n### Computational Experiment\nThe program will execute the following steps for each $n$ in the test suite $\\{1, 2, 8, 64, 1024, 16384, 65536\\}$:\n-   **Generate Sequence:** Construct the length-$n$ vector $x$ where $x_i = (-1)^{i-1} + \\frac{s}{n}$ for $i=1,\\dots,n$ and $s=1$. This is done using `numpy` for efficient vector operations.\n-   **Compute Exact Sum:** Calculate $S_{\\text{exact}}(n)$, which is $s$ if $n$ is even and $1+s$ if $n$ is odd.\n-   **Compute Numerical Sums:** Apply the three implemented summation functions ($S_{\\text{naive}}$, $S_{\\text{pairwise}}$, $S_{\\text{Kahan}}$) to the vector $x$.\n-   **Calculate Errors:** Compute the absolute errors $E(n) = |S_{\\text{computed}}(n) - S_{\\text{exact}}(n)|$ for each of the three methods.\n-   **Aggregate Results:** The computed errors are collected into a single list, ordered as specified, and printed in the required format. The results are expected to show $E_{\\text{Kahan}}(n) \\ll E_{\\text{pairwise}}(n) \\ll E_{\\text{naive}}(n)$ for large $n$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef naive_sum(x: np.ndarray) -> float:\n    \"\"\"\n    Computes the sum of a sequence using naive iterative accumulation.\n    s <- fl(s + x_i)\n    \"\"\"\n    s = 0.0\n    for val in x:\n        s += val\n    return s\n\ndef _pairwise_sum_recursive(arr: np.ndarray, start: int, end: int) -> float:\n    \"\"\"\n    Recursive helper for pairwise summation using indices to avoid slicing.\n    \"\"\"\n    n = end - start\n    if n <= 0:\n        return 0.0\n    if n == 1:\n        # Return a Python float to ensure standard float arithmetic.\n        return float(arr[start])\n    \n    mid = start + n // 2\n    # Recursively sum each half and then add the results.\n    sum1 = _pairwise_sum_recursive(arr, start, mid)\n    sum2 = _pairwise_sum_recursive(arr, mid, end)\n    return sum1 + sum2\n\ndef pairwise_sum(x: np.ndarray) -> float:\n    \"\"\"\n    Computes the sum of a sequence using a recursive pairwise (binary tree) strategy.\n    \"\"\"\n    return _pairwise_sum_recursive(x, 0, len(x))\n\ndef kahan_sum(x: np.ndarray) -> float:\n    \"\"\"\n    Computes the sum of a sequence using Kahan's compensated summation algorithm.\n    \"\"\"\n    s = 0.0  # The running sum\n    c = 0.0  # The compensation for lost low-order bits\n    for val in x:\n        # y incorporates the previous compensation.\n        y = float(val) - c\n        # s is updated. t is the new sum, but a low-order part of y might be lost.\n        t = s + y\n        # (t - s) is the high-order part of y that was successfully added to s.\n        # (t - s) - y retrieves the negative of the low-order part (the error).\n        c = (t - s) - y\n        s = t\n    return s\n\ndef solve():\n    \"\"\"\n    Main function to run the summation experiment and print results.\n    \"\"\"\n    \n    # Test suite covering various scales of n as specified in the problem.\n    test_cases = [1, 2, 8, 64, 1024, 16384, 65536]\n    \n    # The fixed scalar s for the sequence definition.\n    s_scalar = 1.0\n    \n    results = []\n    \n    for n in test_cases:\n        # Step 1: Construct the sequence x for the current n.\n        # x_i = (-1)^(i-1) + s/n, for i=1,...,n\n        # Using 0-based indexing j=i-1: x_j = (-1)^j + s/n\n        indices = np.arange(n, dtype=np.float64)\n        signs = (-1.0)**indices\n        bias = s_scalar / n\n        x = (signs + bias).astype(np.float64)\n\n        # Step 2: Calculate the exact sum S_exact(n).\n        # S_exact is s if n is even, and 1+s if n is odd.\n        if n % 2 == 0:\n            s_exact = s_scalar\n        else:\n            s_exact = 1.0 + s_scalar\n\n        # Step 3: Compute sums using the three different strategies.\n        s_naive = naive_sum(x)\n        s_pairwise = pairwise_sum(x)\n        s_kahan = kahan_sum(x)\n        \n        # Step 4: Calculate absolute errors.\n        e_naive = abs(s_naive - s_exact)\n        e_pairwise = abs(s_pairwise - s_exact)\n        e_kahan = abs(s_kahan - s_exact)\n        \n        # Step 5: Append errors to the results list.\n        results.extend([e_naive, e_pairwise, e_kahan])\n\n    # Final Step: Format the final output as a single comma-separated string in brackets.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3536136"}, {"introduction": "Moving from scalar sums to matrix properties, we encounter catastrophic cancellation in one of the most fundamental checks in numerical linear algebra: verifying the orthogonality of a matrix $Q$. The naive test, which computes the residual $\\|Q^\\top Q - I\\|$, fails precisely when $Q$ is closest to being truly orthogonal, as it involves subtracting nearly identical matrices. This exercise challenges you to move beyond direct computation and instead develop more robust, diagnostic tests based on the intrinsic properties of orthogonal matrices, such as their singular values and norm-preserving nature [@problem_id:3536100].", "problem": "Consider an $n \\times n$ real matrix $Q$ intended to be orthogonal, meaning $Q^\\top Q = I$. In practical floating-point computations adhering to the Institute of Electrical and Electronics Engineers (IEEE) $754$ standard, testing orthogonality via the residual norm $\\|Q^\\top Q - I\\|$ can suffer from catastrophic cancellation when $Q$ is very close to orthogonal. The goal is to analyze this phenomenon from first principles and design alternative tests that are numerically more reliable when $Q$ is close to orthogonal. The derivation must begin from foundational definitions and models appropriate to numerical linear algebra.\n\nUse as fundamental base: the floating-point arithmetic model $fl(a \\,\\text{op}\\, b) = (a \\,\\text{op}\\, b)(1+\\delta)$ with $|\\delta| \\le u$ for a single operation, where $u$ is the unit roundoff (machine epsilon), and for inner products and matrix products, error accumulation is bounded by well-known growth factors; the definitions of matrix norms and singular values; and the notion of a condition number of a matrix with respect to the matrix $2$-norm. Do not assume or reference any particular algorithmic \"shortcut\" formula in the problem statement.\n\nTasks:\n1. Starting from the floating-point model and the definition of the induced $2$-norm, explain why the naive orthogonality test based on computing $\\|Q^\\top Q - I\\|_2$ exhibits catastrophic cancellation when $Q$ is close to orthogonal.\n2. Design and implement in code at least three alternative tests that avoid catastrophic cancellation by operating on quantities that do not require subtracting nearly equal matrices:\n   - A singular-value-based test using the Singular Value Decomposition (SVD) to evaluate how far the singular values of $Q$ are from $1$.\n   - A condition-number-based test using the spectral condition number $\\kappa_2(Q)$ with appropriate transformation to avoid subtracting $1$ directly.\n   - A transformed norm-based test that assesses norm preservation via a logarithmic transformation of ratios of norms of $Qx$ and $x$ over a set of random unit vectors.\n3. For numerical experiments, construct the following $4$ test matrices of dimension $n = 50$:\n   - Case A (happy path): $Q_A$ obtained via a stable Householder-based $QR$ factorization (using a standard library routine) of a random Gaussian matrix; extract the $Q$ factor to serve as an approximately orthogonal matrix.\n   - Case B (near-orthogonal with tiny diagonal perturbations): $Q_B = Q_A D$, where $D = \\operatorname{diag}(1+\\varepsilon_i)$ and each $\\varepsilon_i$ is a small random number of magnitude about $10^{-12}$ with random signs, so that $Q_B$ is very close to orthogonal but not exactly.\n   - Case C (loss of orthogonality from Classical Gram-Schmidt): Construct a sequence of nearly dependent columns $v_1, \\dots, v_n$ with $v_j = v_{j-1} + \\alpha w_j$ for $j \\ge 2$, where $w_j$ are independent standard Gaussian vectors and $\\alpha$ is small (e.g., $\\alpha = 10^{-8}$); apply Classical Gram-Schmidt to form $Q_C$ and observe orthogonality loss due to numerical effects.\n   - Case D (scaled orthogonal boundary): $Q_D = \\alpha Q_A$ with a scale $\\alpha = 1 + 8u$, where $u$ is machine epsilon. This creates a carefully chosen near-scaling that is representable in floating point and probes cancellation in $Q_D^\\top Q_D - I$.\n4. For each test case, compute and return the following metrics as floating-point numbers:\n   - The naive spectral norm residual $\\|fl(Q^\\top Q - I)\\|_2$, computed by explicitly forming $Q^\\top Q - I$ in floating point and then taking the spectral norm.\n   - A singular-value gap indicator $\\max_i | \\sigma_i(Q)^2 - 1 |$, where $\\sigma_i(Q)$ are the singular values of $Q$.\n   - A condition-number indicator $\\log(\\kappa_2(Q))$, where $\\kappa_2(Q)$ is the ratio of the largest to the smallest singular value.\n   - A transformed norm-preservation indicator $\\max_{x \\in \\mathcal{S}} \\left| \\log\\left( \\|Qx\\|_2 / \\|x\\|_2 \\right) \\right|$, where $\\mathcal{S}$ is a set of randomly sampled unit vectors (use at least $200$ samples).\n   - A polar factor gap indicator $\\max_i |\\sigma_i(Q) - 1|$, which corresponds to the distance from $Q$ to its unitary polar factor in the matrix $2$-norm.\n5. Your program must:\n   - Use a fixed random seed for reproducibility.\n   - Produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case contributes a list of $5$ floats in the order described above.\n   - For example, the output format is $[[r_{A1},r_{A2},r_{A3},r_{A4},r_{A5}],[r_{B1},r_{B2},r_{B3},r_{B4},r_{B5}],\\dots]$.\n   - No physical units or angle units are involved. Express all quantities as raw floats.\n\nThis problem is universally applicable in numerical linear algebra and requires reasoning from the floating-point model and basic definitions to derive robust tests that avoid cancellation, followed by implementing and exercising these tests on carefully chosen cases that probe different facets: stable orthogonal construction, near-orthogonality, algorithm-induced loss of orthogonality, and cancellation-prone scaling.", "solution": "The problem statement is evaluated as valid. It is scientifically grounded in the principles of numerical linear algebra and floating-point arithmetic, is well-posed with clear definitions and objectives, and presents a non-trivial, verifiable challenge. The problem directly addresses the core concepts of numerical stability and catastrophic cancellation in a practical context.\n\nWe proceed with the solution.\n\n### Task 1: Analysis of Catastrophic Cancellation in the Naive Test\n\nThe naive test for orthogonality of an $n \\times n$ matrix $Q$ involves computing the residual matrix $R = Q^\\top Q - I$ and its norm, typically the spectral norm $\\|R\\|_2$. We will demonstrate why this procedure is numerically unreliable when $Q$ is very close to being orthogonal.\n\nOur analysis is based on the standard model of floating-point arithmetic, where for a binary operation $\\text{op} \\in \\{+, -, \\times, /\\}$, the computed result is $fl(a \\,\\text{op}\\, b) = (a \\,\\text{op}\\, b)(1+\\delta)$ with $|\\delta| \\le u$, where $u$ is the unit roundoff, or machine epsilon.\n\nLet $Q$ be a matrix that is nearly orthogonal. This means it can be expressed as a small perturbation of an exactly orthogonal matrix $U$ (where $U^\\top U = I$). Let $Q = U + E$, where $\\|E\\|_2 = \\epsilon$ for some small $\\epsilon > 0$.\n\nThe true value of the residual matrix is:\n$$ R_{true} = Q^\\top Q - I = (U+E)^\\top(U+E) - I = (U^\\top + E^\\top)(U+E) - I $$\n$$ R_{true} = U^\\top U + U^\\top E + E^\\top U + E^\\top E - I $$\nSince $U^\\top U = I$, this simplifies to:\n$$ R_{true} = U^\\top E + E^\\top U + E^\\top E $$\nThe norm of the true residual is $\\|R_{true}\\|_2 = \\|U^\\top E + E^\\top U + E^\\top E\\|_2$. Assuming $\\epsilon$ is small, the second-order term $E^\\top E$ is negligible. Using the triangle inequality and properties of the spectral norm ($\\|U^\\top\\|_2 = 1$), we have:\n$$ \\|R_{true}\\|_2 \\le \\|U^\\top E\\|_2 + \\|E^\\top U\\|_2 + \\|E^\\top E\\|_2 \\approx 2\\|E\\|_2 = 2\\epsilon $$\nThus, the quantity we wish to measure, $\\|R_{true}\\|_2$, is on the order of $\\epsilon$, the deviation of $Q$ from an orthogonal matrix.\n\nNow, consider the floating-point computation. Let $\\hat{C} = fl(Q^\\top Q)$ be the computed matrix product. The standard error analysis of matrix multiplication shows that $\\hat{C} = Q^\\top Q + F$, where $F$ is a matrix of errors. The norm of this error is bounded, to a first order, by $\\|F\\|_2 \\lesssim \\gamma_n \\|Q^\\top\\|_2 \\|Q\\|_2$, where $\\gamma_n = \\frac{nu}{1-nu}$. Since $Q$ is nearly orthogonal, $\\|Q\\|_2 \\approx 1$, and thus the error incurred is $\\|F\\|_2 \\approx O(nu)$.\n\nThe naive test computes $\\hat{R} = fl(\\hat{C} - I)$. For off-diagonal elements, $\\hat{R}_{ij} = \\hat{C}_{ij}$ for $i \\ne j$. The critical step is the subtraction on the diagonal:\n$$ \\hat{R}_{ii} = fl(\\hat{C}_{ii} - 1) $$\nLet's analyze the value of $\\hat{C}_{ii}$. It is the computed inner product of the $i$-th column of $Q$ with itself: $\\hat{C}_{ii} = fl(q_i^\\top q_i)$. The true value is $c_{ii} = q_i^\\top q_i = ( (U+E)e_i )^\\top ( (U+E)e_i ) = \\| (U+E)e_i \\|_2^2 \\approx 1 + 2(Ue_i)^\\top (Ee_i)$, so $c_{ii}$ is very close to $1$. Let $c_{ii} = 1 + \\delta_{true}$, where $|\\delta_{true}| \\approx O(\\epsilon)$. The computed value is $\\hat{C}_{ii} \\approx c_{ii} + \\delta_{comp}$, where $|\\delta_{comp}| \\approx O(n u)$ is the error from computing the inner product.\n\nWhen we compute the subtraction $fl(\\hat{C}_{ii} - 1)$, we are computing:\n$$ fl((1 + \\delta_{true} + \\delta_{comp}) - 1) = fl(\\delta_{true} + \\delta_{comp}) \\approx \\delta_{true} + \\delta_{comp} $$\nThe quantity we want is $\\delta_{true}$, but it is contaminated by an unavoidable error $\\delta_{comp}$ of size $O(n u)$. If the true deviation from orthogonality is very small, such that $\\epsilon \\lesssim O(nu)$, then $|\\delta_{true}| \\lesssim |\\delta_{comp}|$. The floating-point rounding error from the matrix multiplication swamps the true signal. The relative error in the computed diagonal entry of the residual is:\n$$ \\frac{(\\text{computed value}) - (\\text{true value})}{(\\text{true value})} = \\frac{(\\delta_{true} + \\delta_{comp}) - \\delta_{true}}{\\delta_{true}} = \\frac{\\delta_{comp}}{\\delta_{true}} $$\nThis ratio can be very large if $|\\delta_{true}|$ is small, which is characteristic of catastrophic cancellation. Consequently, the computed norm $\\|fl(Q^\\top Q - I)\\|_2$ will be approximately $\\|F\\|_2 \\approx O(nu)$, regardless of whether the true residual norm $\\|R_{true}\\|_2$ is much smaller. The test yields a value on the order of machine precision limits, providing no information about the actual orthogonality of $Q$ below this noise floor.\n\n### Tasks 2, 4, and 5: Alternative Numerically Stable Tests\n\nTo avoid this cancellation, we must design tests that do not involve subtracting nearly equal matrices or numbers. The following tests achieve this by reformulating the orthogonality condition.\n\n**1. Singular-Value-Based Tests**\nA matrix $Q$ is orthogonal if and only if all its singular values, $\\sigma_i(Q)$, are equal to $1$. The singular values can be computed to high relative accuracy by robust algorithms like the SVD. We can then inspect their deviation from $1$.\n\n- **Polar Factor Gap Indicator**: A direct measure of non-orthogonality is $\\max_i |\\sigma_i(Q) - 1|$. This quantity is also equal to $\\|Q - U\\|_2$, where $U$ is the unique orthogonal polar factor of $Q$ (the closest orthogonal matrix to $Q$ in the $2$-norm). This test computes the singular values $\\sigma_i$ and then subtracts $1$. This scalar subtraction does not suffer from catastrophic cancellation in the same way, as the $\\sigma_i$ are the primary computed quantities.\n- **Singular-Value-Squared Gap Indicator**: Another robust metric is $\\max_i |\\sigma_i(Q)^2 - 1|$. The values $\\sigma_i(Q)^2$ are the eigenvalues of the matrix $Q^\\top Q$. This metric is equivalent to measuring the deviation of the eigenvalues of $Q^\\top Q$ from $1$. However, we compute it by first finding the singular values $\\sigma_i(Q)$, squaring them, and then subtracting $1$. This route avoids the explicit, full-rank matrix subtraction $Q^\\top Q - I$ and its associated forward error.\n\n**2. Condition-Number-Based Test**\nThe spectral condition number of a matrix is $\\kappa_2(Q) = \\sigma_{\\max}(Q) / \\sigma_{\\min}(Q)$. For an orthogonal matrix, all singular values are $1$, so $\\sigma_{\\max} = \\sigma_{\\min} = 1$ and $\\kappa_2(Q) = 1$. A condition number close to $1$ implies that all singular values are close to each other, and if the matrix has norm close to $1$, they must all be close to $1$. A large condition number indicates a departure from orthogonality (or from a scaled orthogonal matrix).\n\n- **Condition Number Log Indicator**: To measure the deviation of $\\kappa_2(Q)$ from $1$, we use a logarithmic transformation: $\\log(\\kappa_2(Q))$. For values of $\\kappa_2(Q)$ near $1$, $\\log(\\kappa_2(Q)) \\approx \\kappa_2(Q) - 1$. This transformation is numerically stable and converts a multiplicative property into an additive one near $0$. A value near $0$ indicates nearness to orthogonality.\n\n**3. Transformed Norm-Based Test**\nAn orthogonal matrix preserves the Euclidean norm, i.e., $\\|Qx\\|_2 = \\|x\\|_2$ for all vectors $x$. This is equivalent to the ratio $\\|Qx\\|_2 / \\|x\\|_2$ being $1$. We can test this property statistically.\n\n- **Transformed Norm-Preservation Indicator**: We sample a set $\\mathcal{S}$ of random unit vectors (where $\\|x\\|_2=1$) and calculate $\\max_{x \\in \\mathcal{S}} \\left| \\log\\left( \\|Qx\\|_2 \\right) \\right|$. For any unit vector $x$, the value of $\\|Qx\\|_2$ is bounded by the extremal singular values: $\\sigma_{\\min}(Q) \\le \\|Qx\\|_2 \\le \\sigma_{\\max}(Q)$. Sampling over many vectors effectively probes this range. The logarithmic transformation, as before, stably maps values near $1$ to values near $0$, avoiding a direct subtraction. A maximum value close to $0$ indicates that $Q$ preserves the norm of many vectors, which is strong evidence for its near-orthogonality.\n\nThe implementation of these tests and their application to the specified test matrices is provided in the final answer code block. The test cases are designed to expose the failure of the naive method and the robustness of the alternatives under various conditions: stable construction ($Q_A$), small explicit perturbation ($Q_B$), algorithmic degradation ($Q_C$), and a carefully scaled matrix designed to induce cancellation ($Q_D$).", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef classical_gram_schmidt(V):\n    \"\"\"\n    Performs Classical Gram-Schmidt orthogonalization on the columns of V.\n    \"\"\"\n    n, m = V.shape\n    Q = np.zeros_like(V, dtype=float)\n    for j in range(m):\n        v_j = V[:, j].copy()\n        for i in range(j):\n            q_i = Q[:, i]\n            proj = np.dot(q_i.T, v_j)\n            v_j -= proj * q_i\n        \n        norm_vj = np.linalg.norm(v_j)\n        if norm_vj < 1e-12: # Avoid division by zero for linearly dependent vectors\n            # In a real scenario, one might raise an error or handle this case.\n            # Here we just produce a zero vector which signifies loss of rank.\n            Q[:, j] = 0.0\n        else:\n            Q[:, j] = v_j / norm_vj\n    return Q\n\ndef calculate_metrics(Q, rng, n_samples=200):\n    \"\"\"\n    Calculates the 5 specified orthogonality metrics for a given matrix Q.\n    \"\"\"\n    n = Q.shape[0]\n\n    # Metric 1: Naive spectral norm residual\n    # Compute Q.T @ Q - I in floating point and take its spectral norm.\n    try:\n        # Use high precision for matrix product if available, but default is float64\n        residual_matrix = Q.T @ Q - np.eye(n)\n        naive_residual = np.linalg.norm(residual_matrix, 2)\n    except np.linalg.LinAlgError:\n        naive_residual = np.inf\n\n    # Compute SVD for the other metrics\n    try:\n        s = np.linalg.svd(Q, compute_uv=False)\n        s_max = s[0]\n        s_min = s[-1]\n    except np.linalg.LinAlgError:\n        # If SVD fails, metrics are undefined\n        return [naive_residual, np.inf, np.inf, np.inf, np.inf]\n\n    # Metric 2: Singular-value-squared gap indicator\n    svd_squared_gap = np.max(np.abs(s**2 - 1))\n\n    # Metric 3: Condition-number indicator\n    if s_min < 1e-14: # Avoid division by zero or huge numbers\n        cond_log = np.inf\n    else:\n        kappa = s_max / s_min\n        cond_log = np.log(kappa)\n\n    # Metric 4: Transformed norm-preservation indicator\n    # Generate a set of random unit vectors\n    rand_vectors = rng.standard_normal(size=(n, n_samples))\n    rand_vectors /= np.linalg.norm(rand_vectors, axis=0)\n\n    # Apply Q and compute norms\n    q_times_vectors = Q @ rand_vectors\n    norms_q_times_vectors = np.linalg.norm(q_times_vectors, axis=0)\n    \n    # The norm of original vectors is 1, so log(ratio) is log(new_norm)\n    norm_preservation_log = np.max(np.abs(np.log(norms_q_times_vectors)))\n\n    # Metric 5: Polar factor gap indicator\n    polar_factor_gap = np.max(np.abs(s - 1))\n    \n    return [\n        naive_residual,\n        svd_squared_gap,\n        cond_log,\n        norm_preservation_log,\n        polar_factor_gap\n    ]\n\ndef solve():\n    \"\"\"\n    Main function to construct test matrices and compute metrics.\n    \"\"\"\n    # Define problem parameters\n    n = 50\n    seed = 42\n    rng = np.random.default_rng(seed)\n    u = np.finfo(float).eps\n\n    # --- Construct Test Matrices\n    \n    # Case A: Happy path (QR factorization of a random matrix)\n    A_rand = rng.standard_normal(size=(n, n))\n    Q_A, _ = np.linalg.qr(A_rand)\n\n    # Case B: Near-orthogonal with small diagonal perturbations\n    epsilons = 1e-12 * (2 * rng.random(n) - 1)\n    D = np.diag(1 + epsilons)\n    Q_B = Q_A @ D\n\n    # Case C: Loss of orthogonality from Classical Gram-Schmidt\n    alpha_cgs = 1e-8\n    V = np.zeros((n, n))\n    v_prev = rng.standard_normal(size=n)\n    V[:, 0] = v_prev\n    for j in range(1, n):\n        w_j = rng.standard_normal(size=n)\n        v_curr = v_prev + alpha_cgs * w_j\n        V[:, j] = v_curr\n        v_prev = v_curr\n    Q_C = classical_gram_schmidt(V)\n\n    # Case D: Scaled orthogonal boundary\n    alpha_d = 1 + 8 * u\n    Q_D = alpha_d * Q_A\n    \n    test_matrices = [Q_A, Q_B, Q_C, Q_D]\n\n    results = []\n    for Q in test_matrices:\n        metrics = calculate_metrics(Q, rng)\n        results.append(metrics)\n\n    # Final print statement in the exact required format.\n    # The map(str, ...) converts each inner list to its string representation.\n    # The join then combines these strings with commas.\n    print(f\"[{','.join(map(str, results))}]\")\n\n# Execute the solution\nsolve()\n\n```", "id": "3536100"}, {"introduction": "This advanced practice delves into the heart of matrix perturbation theory, analyzing the sensitivity of the matrix inverse to small changes in its input. Directly computing the difference $(A+E)^{-1} - A^{-1}$ is a textbook example of an operation prone to catastrophic cancellation, especially for ill-conditioned matrices $A$. You will use the powerful framework of Fréchet derivatives to derive a stable first-order approximation and, through algebraic manipulation, an exact formula that avoids this numerical pitfall, reinforcing the principle that reformulating an expression is often the key to a stable algorithm [@problem_id:3536151].", "problem": "You are given a square, nonsingular matrix $A \\in \\mathbb{R}^{n \\times n}$ and a perturbation $E \\in \\mathbb{R}^{n \\times n}$ with small norm. Directly computing the difference $(A+E)^{-1} - A^{-1}$ by forming each inverse and subtracting can incur catastrophic cancellation when $E$ is small and $A$ is ill-conditioned. Your tasks are to derive a stable first-order approximation using the Fréchet derivative, obtain an algebraically stable exact representation that avoids the subtraction of nearly equal matrices, and implement and test these computations numerically.\n\nStart from the following fundamental base:\n- The definition of the matrix inverse: $A^{-1}$ satisfies $A A^{-1} = I$ for nonsingular $A$.\n- The definition of the Fréchet derivative of a matrix function $f$: the linear operator $L_f(A,\\cdot)$ that satisfies $f(A+E) - f(A) = L_f(A,E) + o(\\|E\\|)$ as $\\|E\\| \\to 0$, with $L_f(A,\\cdot)$ linear in $E$.\n- The product rule for Fréchet derivatives: for matrix functions $g$ and $h$, the derivative of $g(A)h(A)$ satisfies $L_{g h}(A,E) = L_g(A,E) h(A) + g(A) L_h(A,E)$.\n- Standard norm properties for matrices, including the Frobenius norm $\\|X\\|_F = \\sqrt{\\sum_{i,j} x_{ij}^2}$.\n\nProceed as follows:\n1) Derive from first principles the Fréchet derivative $L_f(A,E)$ of the function $f(A) = A^{-1}$, starting only from the identity $A A^{-1} = I$ and the product rule for Fréchet derivatives, without using any pre-memorized formula for the derivative of the matrix inverse.\n2) Using only algebraic manipulations grounded in the identity $(A+E)(A+E)^{-1} = I$ and $A A^{-1} = I$, derive an exact expression for $(A+E)^{-1} - A^{-1}$ that avoids the direct subtraction of two nearly equal matrices and that can be evaluated using solutions of linear systems with coefficient matrices $A$ and $A+E$, without explicitly inverting any matrix.\n3) Explain qualitatively why subtracting $(A+E)^{-1}$ and $A^{-1}$ is susceptible to catastrophic cancellation when $\\|E\\|$ is small compared to $\\|A\\|$ and $A$ is ill-conditioned, and relate this to a subtraction condition number $\\kappa_{\\mathrm{sub}} = \\dfrac{\\|X\\|_F + \\|Y\\|_F}{\\|X-Y\\|_F}$ with $X = (A+E)^{-1}$ and $Y = A^{-1}$.\n\nThen implement a program that does the following:\n- For each test case, compute three quantities:\n  a) The reference “exact” difference $D_{\\mathrm{ref}}$ using your algebraically stable identity from step 2), evaluated via solving linear systems; do not form any explicit inverse.\n  b) The first-order Fréchet approximation $D_{\\mathrm{lin}}$ obtained from step 1), evaluated via solving linear systems; do not form any explicit inverse.\n  c) The naive difference $D_{\\mathrm{naive}} = (A+E)^{-1} - A^{-1}$ computed by explicitly forming both inverses and subtracting.\n- For each test case, report the following four floats:\n  i) $r_{\\mathrm{naive}} = \\dfrac{\\|D_{\\mathrm{naive}} - D_{\\mathrm{ref}}\\|_F}{\\|D_{\\mathrm{ref}}\\|_F}$,\n  ii) $r_{\\mathrm{lin}} = \\dfrac{\\|D_{\\mathrm{lin}} - D_{\\mathrm{ref}}\\|_F}{\\|D_{\\mathrm{ref}}\\|_F}$,\n  iii) $\\kappa_{\\mathrm{sub}} = \\dfrac{\\|(A+E)^{-1}\\|_F + \\|A^{-1}\\|_F}{\\|(A+E)^{-1} - A^{-1}\\|_F}$,\n  iv) $\\kappa_2(A)$, the $2$-norm condition number of $A$.\n- Use the Frobenius norm for all norms as explicitly indicated above.\n- The final output format must be a single line containing a list of the per-test-case results, each itself being a list of four floats as described above, printed as a comma-separated list enclosed in square brackets.\n\nTest suite:\n- Test case 1 (Hilbert matrix): $n=8$, $A \\in \\mathbb{R}^{8 \\times 8}$ with entries $a_{ij} = \\dfrac{1}{i+j+1}$ for $i,j = 0,1,\\dots,7$. Let $E = \\tau \\, e_1 e_1^\\top$ where $e_1 = [1,0,\\dots,0]^\\top \\in \\mathbb{R}^8$ and $\\tau = 10^{-10}$.\n- Test case 2 (Toeplitz-like symmetric positive definite matrix): $n=12$, $A \\in \\mathbb{R}^{12 \\times 12}$ with entries $a_{ij} = \\rho^{|i-j|}$ for $i,j = 0,1,\\dots,11$ and $\\rho = 0.999$. Let $E = \\tau \\, (e_1 e_n^\\top + e_n e_1^\\top)$ with $n=12$, $e_1, e_n$ the first and last standard basis vectors, and $\\tau = 10^{-8}$.\n- Test case 3 (highly scaled diagonal): $n=8$, $A = \\operatorname{diag}(10^0,10^{-2},10^{-4},10^{-6},10^{-8},10^{-10},10^{-12},10^{-14})$. Let $E = \\tau \\, \\operatorname{diag}(1,-1,1,-1,1,-1,1,-1)$ with $\\tau = 10^{-16}$.\n\nAngle units and physical units are not applicable in this problem.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is a list of the four floats $[r_{\\mathrm{naive}}, r_{\\mathrm{lin}}, \\kappa_{\\mathrm{sub}}, \\kappa_2(A)]$ for a test case, in the same order as the test suite above. For example, a valid output format for three test cases is:\n- [[0.1,0.2,3.0,4.0],[...],[...]]\nNo other text should be printed.", "solution": "The problem is assessed as valid. It is scientifically grounded in the principles of numerical linear algebra, is well-posed with all necessary information provided, and is stated objectively.\n\n**1. Derivation of the Fréchet Derivative of the Matrix Inverse**\n\nLet the matrix function be $f(A) = A^{-1}$. The defining identity for the matrix inverse is $A f(A) = I$, where $I$ is the identity matrix. We wish to find the Fréchet derivative of $f(A)$, which we denote by $L_f(A, E)$.\n\nWe take the Fréchet derivative of both sides of the identity $A A^{-1} = I$. The derivative of the constant function $C(A) = I$ is the zero operator, so its evaluation for a perturbation $E$ is the zero matrix, i.e., $L_C(A, E) = \\mathbf{0}$.\n\nFor the left side, $A A^{-1}$, we apply the product rule for Fréchet derivatives: $L_{gh}(A,E) = L_g(A,E) h(A) + g(A) L_h(A,E)$. Let $g(A) = A$ and $h(A) = A^{-1}$.\n\nThe Fréchet derivative of the identity function $g(A)=A$ is simply $L_g(A,E) = E$. The derivative of $h(A)=A^{-1}$ is the quantity we want to find, $L_{A^{-1}}(A,E)$.\n\nApplying the product rule to $A A^{-1}$ yields:\n$$\nL_{A A^{-1}}(A,E) = (L_A(A,E)) A^{-1} + A (L_{A^{-1}}(A,E)) = E A^{-1} + A L_{A^{-1}}(A,E)\n$$\nEquating the derivatives of both sides of the original identity $A A^{-1} = I$, we get:\n$$\nE A^{-1} + A L_{A^{-1}}(A,E) = \\mathbf{0}\n$$\nTo solve for $L_{A^{-1}}(A,E)$, we rearrange the equation:\n$$\nA L_{A^{-1}}(A,E) = -E A^{-1}\n$$\nSince $A$ is nonsingular, we can multiply from the left by $A^{-1}$:\n$$\nA^{-1} (A L_{A^{-1}}(A,E)) = A^{-1}(-E A^{-1})\n$$\n$$\nI L_{A^{-1}}(A,E) = -A^{-1} E A^{-1}\n$$\nThus, the Fréchet derivative of the matrix inverse function is:\n$$\nL_{A^{-1}}(A,E) = -A^{-1} E A^{-1}\n$$\nThe first-order approximation for the difference $(A+E)^{-1} - A^{-1}$ is therefore given by this derivative:\n$$\nD_{\\mathrm{lin}} = (A+E)^{-1} - A^{-1} \\approx -A^{-1} E A^{-1}\n$$\n\n**2. Derivation of the Algebraically Stable Exact Expression**\n\nWe aim to find an exact expression for the difference $\\Delta = (A+E)^{-1} - A^{-1}$ that avoids the direct subtraction of nearly equal matrices. We start with the identity for the inverse of the perturbed matrix:\n$$\n(A+E)(A+E)^{-1} = I\n$$\nWe express $(A+E)^{-1}$ as the sum of the original inverse and the difference: $(A+E)^{-1} = A^{-1} + \\Delta$. Substituting this into the identity gives:\n$$\n(A+E)(A^{-1} + \\Delta) = I\n$$\nExpanding the left-hand side using distributivity:\n$$\nA(A^{-1} + \\Delta) + E(A^{-1} + \\Delta) = I\n$$\n$$\nA A^{-1} + A\\Delta + E A^{-1} + E\\Delta = I\n$$\nUsing the fact that $A A^{-1} = I$:\n$$\nI + A\\Delta + E A^{-1} + E\\Delta = I\n$$\nSubtracting $I$ from both sides leaves:\n$$\nA\\Delta + E A^{-1} + E\\Delta = \\mathbf{0}\n$$\nWe now solve for $\\Delta$. Factoring out $\\Delta$ from the terms where it appears:\n$$\n(A+E)\\Delta + E A^{-1} = \\mathbf{0}\n$$\n$$\n(A+E)\\Delta = -E A^{-1}\n$$\nSince $E$ has a small norm, $A+E$ is nonsingular along with $A$. We can thus multiply from the left by $(A+E)^{-1}$:\n$$\n\\Delta = -(A+E)^{-1} E A^{-1}\n$$\nThis gives the exact difference $D_{\\mathrm{ref}} = (A+E)^{-1} - A^{-1} = -(A+E)^{-1} E A^{-1}$. This form is numerically stable because the small result $\\Delta$ is obtained via multiplications involving the small matrix $E$, rather than by cancellation of large quantities.\n\nTo evaluate this expression without explicitly forming any matrix inverses, we can use linear system solvers. The computation is a two-step process:\n1.  Compute the intermediate matrix $M = E A^{-1}$. Instead of inverting $A$, we solve the matrix equation $M A = E$. This is equivalent to solving $A^T M^T = E^T$ for the matrix $M^T$. This requires solving $n$ linear systems with the coefficient matrix $A^T$.\n2.  Compute the final result $D_{\\mathrm{ref}} = -(A+E)^{-1} M$. This is equivalent to solving the matrix equation $(A+E) D_{\\mathrm{ref}} = -M$, which also requires solving $n$ linear systems, this time with the coefficient matrix $A+E$.\n\n**3. Qualitative Explanation of Catastrophic Cancellation**\n\nCatastrophic cancellation is a phenomenon in floating-point arithmetic where the subtraction of two nearly equal numbers results in a value with a large relative error. The leading, most significant bits of the numbers cancel, and the result is determined by the trailing, less significant bits, which are often affected by rounding errors from prior computations.\n\nIn this problem, we consider the naive computation $D_{\\mathrm{naive}} = (A+E)^{-1} - A^{-1}$. When the perturbation $E$ is small in norm compared to $A$, the matrix $A+E$ is close to $A$. Consequently, their inverses are also close: $(A+E)^{-1} \\approx A^{-1}$. Performing the subtraction $(A+E)^{-1} - A^{-1}$ is thus an operation on two nearly equal matrices, making it susceptible to catastrophic cancellation.\n\nThe severity of this issue is amplified when $A$ is ill-conditioned. The sensitivity of the subtraction is quantified by the subtraction condition number, $\\kappa_{\\mathrm{sub}}$. Let $X=(A+E)^{-1}$ and $Y=A^{-1}$.\n$$\n\\kappa_{\\mathrm{sub}} = \\frac{\\|X\\|_F + \\|Y\\|_F}{\\|X-Y\\|_F}\n$$\nFor small $\\|E\\|_F$, we have the approximation $\\|X-Y\\|_F = \\|(A+E)^{-1} - A^{-1}\\|_F \\approx \\|-A^{-1} E A^{-1}\\|_F \\le \\|A^{-1}\\|_F^2 \\|E\\|_F$. The numerator is approximately $2\\|A^{-1}\\|_F$. Therefore,\n$$\n\\kappa_{\\mathrm{sub}} \\approx \\frac{2\\|A^{-1}\\|_F}{\\|-A^{-1} E A^{-1}\\|_F}\n$$\nAn ill-conditioned matrix $A$ has a large condition number $\\kappa(A) = \\|A\\|\\|A^{-1}\\|$, which implies that $\\|A^{-1}\\|$ is large relative to $1/\\|A\\|$. This makes the numerator of $\\kappa_{\\mathrm{sub}}$ large. The denominator is small due to the $\\|E\\|_F$ factor. The combination results in a very large $\\kappa_{\\mathrm{sub}}$, indicating that the subtraction is extremely ill-conditioned. Any small relative errors in the computed values of $(A+E)^{-1}$ and $A^{-1}$ are magnified by this large factor, leading to a potentially complete loss of relative accuracy in the final result $D_{\\mathrm{naive}}$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import toeplitz, solve as scipy_solve\n\ndef solve():\n    \"\"\"\n    Solves the problem for the given test suite and prints the results\n    in the specified format.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test case 1 (Hilbert matrix)\n        {'n': 8, 'type': 'hilbert', 'tau': 1e-10},\n        # Test case 2 (Toeplitz-like SPD matrix)\n        {'n': 12, 'type': 'toeplitz', 'rho': 0.999, 'tau': 1e-8},\n        # Test case 3 (Highly scaled diagonal)\n        {'n': 8, 'type': 'diag', 'tau': 1e-16},\n    ]\n\n    results = []\n    for params in test_cases:\n        n = params['n']\n        \n        # --- Construct matrices A and E for the current test case ---\n        if params['type'] == 'hilbert':\n            # A_ij = 1/(i+j+1) for i,j = 0..n-1\n            A = np.fromfunction(lambda i, j: 1.0 / (i + j + 1), (n, n), dtype=float)\n            E = np.zeros((n, n), dtype=float)\n            E[0, 0] = params['tau']\n        elif params['type'] == 'toeplitz':\n            # A_ij = rho^|i-j| for i,j = 0..n-1\n            rho = params['rho']\n            A = toeplitz(rho**np.arange(n))\n            E = np.zeros((n, n), dtype=float)\n            tau = params['tau']\n            E[0, n - 1] = tau\n            E[n - 1, 0] = tau\n        elif params['type'] == 'diag':\n            # A = diag(1, 1e-2, ..., 10^(-2*(n-1)))\n            A = np.diag(10.0**(-2 * np.arange(n)))\n            E_diag = np.ones(n)\n            E_diag[1::2] = -1\n            E = params['tau'] * np.diag(E_diag)\n\n        A_plus_E = A + E\n        \n        # --- a) Compute D_ref (stable exact formula) ---\n        # D_ref = -(A+E)^{-1} E A^{-1}\n        # Step 1: Compute M = E A^{-1} by solving A^T M^T = E^T\n        M_T = scipy_solve(A.T, E.T, assume_a='gen')\n        M = M_T.T\n        # Step 2: Compute D_ref by solving (A+E) D_ref = -M\n        D_ref = scipy_solve(A_plus_E, -M, assume_a='gen')\n\n        # --- b) Compute D_lin (Fréchet approximation) ---\n        # D_lin = -A^{-1} E A^{-1} = -A^{-1} M\n        # We reuse M = E A^{-1} from the previous step.\n        # Solve A D_lin = -M\n        D_lin = scipy_solve(A, -M, assume_a='gen')\n\n        # --- c) Compute D_naive (direct subtraction) ---\n        A_inv = np.linalg.inv(A)\n        A_plus_E_inv = np.linalg.inv(A_plus_E)\n        D_naive = A_plus_E_inv - A_inv\n\n        # --- d) Compute the four required metrics ---\n        norm_fro = lambda x: np.linalg.norm(x, 'fro')\n\n        # i) r_naive: Relative error of the naive computation\n        D_ref_norm = norm_fro(D_ref)\n        if D_ref_norm > 0:\n            r_naive = norm_fro(D_naive - D_ref) / D_ref_norm\n        else:\n            r_naive = norm_fro(D_naive - D_ref)\n\n        # ii) r_lin: Relative error of the linear approximation\n        if D_ref_norm > 0:\n            r_lin = norm_fro(D_lin - D_ref) / D_ref_norm\n        else:\n            r_lin = norm_fro(D_lin - D_ref)\n\n        # iii) kappa_sub: Subtraction condition number\n        A_plus_E_inv_norm = norm_fro(A_plus_E_inv)\n        A_inv_norm = norm_fro(A_inv)\n        D_naive_norm = norm_fro(D_naive)\n        \n        if D_naive_norm > 0:\n            kappa_sub = (A_plus_E_inv_norm + A_inv_norm) / D_naive_norm\n        else:\n            # If the computed naive difference is zero, the cancellation is \"total\",\n            # indicating an infinitely ill-conditioned operation.\n            kappa_sub = np.inf\n        \n        # iv) kappa_2(A): 2-norm condition number of A\n        kappa_A = np.linalg.cond(A, 2)\n\n        results.append([r_naive, r_lin, kappa_sub, kappa_A])\n\n    # Final print statement in the exact required format.\n    # Format: [[r1_1,r1_2,k1_sub,k1_A],[r2_1,r2_2,k2_sub,k2_A],...]\n    case_strings = []\n    for res in results:\n        # Convert each float to a string. Using `repr` for full precision.\n        res_str = \",\".join(map(repr, res))\n        case_strings.append(f\"[{res_str}]\")\n    \n    print(f\"[{','.join(case_strings)}]\")\n\nsolve()\n```", "id": "3536151"}]}