{"hands_on_practices": [{"introduction": "Mastering a concept often begins with a return to its foundations. This exercise solidifies your understanding of matrix norms and condition numbers by having you compute them from their formal supremum definitions, rather than using memorized formulas for the $1$-norm and $\\infty$-norm. By working through a concrete $2 \\times 2$ example, you will practice the core mechanics and discover an elegant general property that holds for all such matrices.", "problem": "Let $A \\in \\mathbb{R}^{2 \\times 2}$ be the matrix\n$$\nA = \\begin{pmatrix} 7 & -3 \\\\ 2 & 5 \\end{pmatrix}.\n$$\nStarting from the definitions of induced matrix norms and the condition number, compute the induced $1$-norm $\\|A\\|_{1}$, the induced infinity-norm $\\|A\\|_{\\infty}$, and the corresponding condition numbers $\\kappa_{1}(A)$ and $\\kappa_{\\infty}(A)$. Use only the definition that for a vector norm $\\|\\cdot\\|_{p}$, the induced matrix norm is $\\|A\\|_{p} \\,=\\, \\displaystyle \\sup_{x \\neq 0} \\frac{\\|Ax\\|_{p}}{\\|x\\|_{p}}$, and the definition of the condition number $\\kappa_{p}(A) \\,=\\, \\|A\\|_{p}\\,\\|A^{-1}\\|_{p}$. After carrying out these computations for the given matrix, analyze and explain any observed differences between $\\kappa_{1}(A)$ and $\\kappa_{\\infty}(A)$ by deriving, from first principles, a general identity for arbitrary invertible $2 \\times 2$ matrices that accounts for your observation.\n\nAs your final reported value, provide the exact value of the ratio $\\kappa_{1}(A) / \\kappa_{\\infty}(A)$ in simplest form. No rounding is required.", "solution": "The problem will be validated before a solution is attempted.\n\n### Step 1: Extract Givens\n- The matrix $A \\in \\mathbb{R}^{2 \\times 2}$ is given as $A = \\begin{pmatrix} 7 & -3 \\\\ 2 & 5 \\end{pmatrix}$.\n- The definition of the induced matrix norm is $\\|A\\|_{p} = \\displaystyle \\sup_{x \\neq 0} \\frac{\\|Ax\\|_{p}}{\\|x\\|_{p}}$.\n- The definition of the condition number is $\\kappa_{p}(A) = \\|A\\|_{p}\\,\\|A^{-1}\\|_{p}$.\n- The task is to compute $\\|A\\|_{1}$, $\\|A\\|_{\\infty}$, $\\kappa_{1}(A)$, and $\\kappa_{\\infty}(A)$ starting from these definitions.\n- The task includes analyzing the observed relationship between $\\kappa_{1}(A)$ and $\\kappa_{\\infty}(A)$ by deriving a general identity for arbitrary invertible $2 \\times 2$ matrices.\n- The final reported value is the exact value of the ratio $\\kappa_{1}(A) / \\kappa_{\\infty}(A)$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem is based on fundamental and standard definitions in numerical linear algebra, specifically induced matrix norms and condition numbers. The concepts are mathematically rigorous.\n- **Well-Posed**: The problem is well-defined. The matrix $A$ is specified, and the quantities to be computed are based on standard, non-ambiguous definitions. The matrix $A$ is invertible as its determinant is non-zero, so $A^{-1}$ and the condition numbers exist. A unique solution is expected.\n- **Objective**: The language is precise and mathematical, free of any subjectivity or opinion.\n- **Self-Contained and Consistent**: The problem provides all necessary information (the matrix and the base definitions) to proceed with the solution. There are no contradictions.\n- **Relevance**: The problem is directly related to the topic of problem conditioning and condition numbers in numerical linear algebra.\n\n### Step 3: Verdict and Action\nThe problem is deemed valid. A full solution will be provided.\n\n### Solution\nThe problem requires the computation of matrix norms and condition numbers for a given matrix $A$, and an explanation for the observed results based on a general derivation.\n\nFirst, we must establish the computational formulas for the induced $1$-norm and $\\infty$-norm from their supremum definition, as stipulated. Let $M$ be a general $m \\times n$ matrix with entries $m_{ij}$.\n\nFor the $1$-norm, $\\|M\\|_1$:\nThe $1$-norm of a vector $v \\in \\mathbb{R}^k$ is $\\|v\\|_1 = \\sum_{i=1}^k |v_i|$. For $x \\in \\mathbb{R}^n$, we have:\n$$ \\|Mx\\|_1 = \\sum_{i=1}^m \\left| \\sum_{j=1}^n m_{ij} x_j \\right| \\le \\sum_{i=1}^m \\sum_{j=1}^n |m_{ij}| |x_j| $$\nRearranging the summation order:\n$$ \\|Mx\\|_1 \\le \\sum_{j=1}^n |x_j| \\left( \\sum_{i=1}^m |m_{ij}| \\right) \\le \\left( \\max_{1 \\le k \\le n} \\sum_{i=1}^m |m_{ik}| \\right) \\sum_{j=1}^n |x_j| = \\left( \\max_{k} \\sum_{i} |m_{ik}| \\right) \\|x\\|_1 $$\nThis shows that $\\|M\\|_1 \\le \\max_{j} \\sum_{i} |m_{ij}|$. To show equality, we must find a vector $x$ for which the bound is attained. Let $k$ be the column index for which the maximum absolute column sum is achieved. Let $x$ be the standard basis vector $e_k$, where $(e_k)_j = \\delta_{jk}$. Then $\\|x\\|_1 = 1$. For this $x$, $Mx$ is the $k$-th column of $M$.\n$$ \\|Me_k\\|_1 = \\sum_{i=1}^m |m_{ik}| = \\max_{j} \\sum_{i} |m_{ij}| $$\nThus, the supremum is the maximum absolute column sum: $\\|M\\|_1 = \\max_{j} \\sum_{i} |m_{ij}|$.\n\nFor the $\\infty$-norm, $\\|M\\|_\\infty$:\nThe $\\infty$-norm of a vector $v \\in \\mathbb{R}^k$ is $\\|v\\|_\\infty = \\max_{1 \\le i \\le k} |v_i|$. For $x \\in \\mathbb{R}^n$ with $\\|x\\|_\\infty \\le 1$:\n$$ \\|Mx\\|_\\infty = \\max_{1 \\le i \\le m} \\left| \\sum_{j=1}^n m_{ij} x_j \\right| \\le \\max_{1 \\le i \\le m} \\sum_{j=1}^n |m_{ij}| |x_j| \\le \\max_{1 \\le i \\le m} \\sum_{j=1}^n |m_{ij}| \\|x\\|_\\infty $$\nThis shows that $\\|M\\|_\\infty \\le \\left( \\max_{i} \\sum_{j} |m_{ij}| \\right) \\|x\\|_\\infty$. To show equality, let $k$ be the row index where the maximum absolute row sum occurs. Define a vector $x$ with components $x_j = \\text{sgn}(m_{kj})$. Then $\\|x\\|_\\infty=1$ (if the $k$-th row is not all zeros, which would be a trivial case). The $k$-th component of $Mx$ is:\n$$ (Mx)_k = \\sum_{j=1}^n m_{kj} x_j = \\sum_{j=1}^n m_{kj} \\text{sgn}(m_{kj}) = \\sum_{j=1}^n |m_{kj}| = \\max_{i} \\sum_{j} |m_{ij}| $$\nSince $\\|Mx\\|_\\infty \\ge |(Mx)_k|$, we have shown the bound is attained. Thus, the supremum is the maximum absolute row sum: $\\|M\\|_\\infty = \\max_{i} \\sum_{j} |m_{ij}|$.\n\nNow, we apply these formulas to the given matrix $A = \\begin{pmatrix} 7 & -3 \\\\ 2 & 5 \\end{pmatrix}$.\n\n**Computation of Norms of $A$**\nThe absolute column sums for $A$ are:\n- Column 1: $|7| + |2| = 7 + 2 = 9$.\n- Column 2: $|-3| + |5| = 3 + 5 = 8$.\nSo, $\\|A\\|_1 = \\max(9, 8) = 9$.\n\nThe absolute row sums for $A$ are:\n- Row 1: $|7| + |-3| = 7 + 3 = 10$.\n- Row 2: $|2| + |5| = 2 + 5 = 7$.\nSo, $\\|A\\|_\\infty = \\max(10, 7) = 10$.\n\n**Computation of Inverse and its Norms**\nTo find the condition numbers, we need $A^{-1}$. For a $2 \\times 2$ matrix $\\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}$, the inverse is $\\frac{1}{ad-bc} \\begin{pmatrix} d & -b \\\\ -c & a \\end{pmatrix}$.\nThe determinant of $A$ is $\\det(A) = (7)(5) - (-3)(2) = 35 + 6 = 41$. Since $\\det(A) \\neq 0$, $A$ is invertible.\n$$ A^{-1} = \\frac{1}{41} \\begin{pmatrix} 5 & 3 \\\\ -2 & 7 \\end{pmatrix} = \\begin{pmatrix} 5/41 & 3/41 \\\\ -2/41 & 7/41 \\end{pmatrix} $$\nNow we compute the norms of $A^{-1}$:\n- Column 1 sum: $|5/41| + |-2/41| = (5+2)/41 = 7/41$.\n- Column 2 sum: $|3/41| + |7/41| = (3+7)/41 = 10/41$.\nSo, $\\|A^{-1}\\|_1 = \\max(7/41, 10/41) = 10/41$.\n\n- Row 1 sum: $|5/41| + |3/41| = (5+3)/41 = 8/41$.\n- Row 2 sum: $|-2/41| + |7/41| = (2+7)/41 = 9/41$.\nSo, $\\|A^{-1}\\|_\\infty = \\max(8/41, 9/41) = 9/41$.\n\n**Computation of Condition Numbers**\nUsing the definition $\\kappa_p(A) = \\|A\\|_p \\|A^{-1}\\|_p$:\n$$ \\kappa_{1}(A) = \\|A\\|_{1} \\|A^{-1}\\|_{1} = 9 \\cdot \\frac{10}{41} = \\frac{90}{41} $$\n$$ \\kappa_{\\infty}(A) = \\|A\\|_{\\infty} \\|A^{-1}\\|_{\\infty} = 10 \\cdot \\frac{9}{41} = \\frac{90}{41} $$\nWe observe that $\\kappa_{1}(A) = \\kappa_{\\infty}(A)$.\n\n**Analysis of the Observation**\nThe problem requests a derivation of a general identity for $2 \\times 2$ matrices to explain this observation.\nFirst, we establish a general identity relating the $1$-norm and $\\infty$-norm for any square matrix $M$ and its transpose $M^T$.\n$\\|M\\|_1 = \\max_j \\sum_i |m_{ij}|$.\n$\\|M^T\\|_\\infty = \\max_i \\sum_j |(M^T)_{ij}| = \\max_i \\sum_j |m_{ji}|$. By relabeling indices, this is clearly the same as the maximum absolute column sum of $M$. So, $\\|M\\|_1 = \\|M^T\\|_\\infty$.\nSimilarly, $\\|M\\|_\\infty = \\max_i \\sum_j |m_{ij}| = \\|M^T\\|_1$.\n\nUsing this, we can relate $\\kappa_1(A)$ to $\\kappa_\\infty(A^T)$:\n$\\kappa_1(A) = \\|A\\|_1 \\|A^{-1}\\|_1$.\nUsing the identities above and the property $(A^{-1})^T=(A^T)^{-1}$:\n$\\kappa_1(A) = \\|A^T\\|_\\infty \\|(A^{-1})^T\\|_\\infty = \\|A^T\\|_\\infty \\|(A^T)^{-1}\\|_\\infty = \\kappa_\\infty(A^T)$.\nThis identity, $\\kappa_1(A) = \\kappa_\\infty(A^T)$, holds for any invertible square matrix $A$.\n\nThe observation $\\kappa_1(A) = \\kappa_\\infty(A)$ for our specific matrix would be explained if, for this matrix $A$, $\\kappa_\\infty(A) = \\kappa_\\infty(A^T)$. We will now show this holds for *any* invertible $2 \\times 2$ matrix.\nLet $B = \\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}$ be an arbitrary invertible $2 \\times 2$ matrix, so $\\det(B) = ad-bc \\neq 0$.\n$\\|B\\|_\\infty = \\max(|a|+|b|, |c|+|d|)$.\n$B^{-1} = \\frac{1}{\\det(B)} \\begin{pmatrix} d & -b \\\\ -c & a \\end{pmatrix}$.\n$\\|B^{-1}\\|_\\infty = \\frac{1}{|\\det(B)|} \\max(|d|+|-b|, |-c|+|a|) = \\frac{1}{|\\det(B)|} \\max(|b|+|d|, |a|+|c|)$.\nSo, $\\kappa_\\infty(B) = \\|B\\|_\\infty \\|B^{-1}\\|_\\infty = \\frac{\\max(|a|+|b|, |c|+|d|) \\cdot \\max(|a|+|c|, |b|+|d|)}{|\\det(B)|}$.\n\nNow consider the transpose $B^T = \\begin{pmatrix} a & c \\\\ b & d \\end{pmatrix}$.\n$\\|B^T\\|_\\infty = \\max(|a|+|c|, |b|+|d|)$.\n$(B^T)^{-1} = (B^{-1})^T = \\frac{1}{\\det(B^T)} \\begin{pmatrix} d & -c \\\\ -b & a \\end{pmatrix}$. Note $\\det(B^T) = \\det(B)$.\n$\\|(B^T)^{-1}\\|_\\infty = \\frac{1}{|\\det(B)|} \\max(|d|+|-c|, |-b|+|a|) = \\frac{1}{|\\det(B)|} \\max(|c|+|d|, |a|+|b|)$.\nSo, $\\kappa_\\infty(B^T) = \\|B^T\\|_\\infty \\|(B^T)^{-1}\\|_\\infty = \\frac{\\max(|a|+|c|, |b|+|d|) \\cdot \\max(|a|+|b|, |c|+|d|)}{|\\det(B)|}$.\nBy the commutativity of multiplication, it is evident that $\\kappa_\\infty(B) = \\kappa_\\infty(B^T)$.\n\nSince this identity $\\kappa_\\infty(B) = \\kappa_\\infty(B^T)$ holds for any invertible $2 \\times 2$ matrix $B$, and we have the general identity $\\kappa_1(A) = \\kappa_\\infty(A^T)$, it follows that for any invertible $2 \\times 2$ matrix $A$:\n$$ \\kappa_1(A) = \\kappa_\\infty(A^T) = \\kappa_\\infty(A) $$\nThis general identity for all $2 \\times 2$ matrices explains why we observed $\\kappa_1(A) = \\kappa_\\infty(A)$ for the specific matrix given.\n\n**Final Calculation**\nThe problem asks for the ratio $\\kappa_{1}(A) / \\kappa_{\\infty}(A)$. Based on our specific calculations and the general identity we just proved:\n$$ \\frac{\\kappa_{1}(A)}{\\kappa_{\\infty}(A)} = \\frac{90/41}{90/41} = 1 $$", "answer": "$$\n\\boxed{1}\n$$", "id": "3567330"}, {"introduction": "While it is useful to compute the condition number for a fixed matrix, a deeper intuition comes from seeing how it behaves as a matrix changes. This problem provides a classic example of an ill-conditioned system, where a single small parameter $\\epsilon$ controls its proximity to singularity [@problem_id:3567278]. By deriving an exact expression for the condition number $\\kappa_2(A)$ and analyzing its behavior as $\\epsilon \\to 0^{+}$, you will gain a clear, analytical understanding of how ill-conditioning is driven by a matrix becoming nearly rank-deficient.", "problem": "Let $A \\in \\mathbb{R}^{2 \\times 2}$ be the symmetric matrix $A=\\begin{pmatrix}1 & 1 \\\\ 1 & 1+\\epsilon\\end{pmatrix}$ with a small parameter $\\epsilon>0$. Using only the core definitions of the matrix two-norm, singular values, and the two-norm condition number, derive an exact closed-form expression for the two-norm condition number $\\kappa_{2}(A)$ as a function of $\\epsilon$. Then, starting from the same foundational principles, determine the leading-order asymptotic behavior of $\\kappa_{2}(A)$ as $\\epsilon \\to 0^{+}$ and provide a principled explanation of how the proximity of $A$ to a rank-deficient matrix drives poor conditioning. Your final reported value must be the exact closed-form expression for $\\kappa_{2}(A)$ in terms of $\\epsilon$. No rounding is required and no units are involved.", "solution": "The problem statement is evaluated as valid. It is a well-posed, self-contained, and scientifically grounded problem in the field of numerical linear algebra. All data and definitions are clear and mathematically consistent, permitting a rigorous and unique solution.\n\nThe two-norm condition number of an invertible matrix $A \\in \\mathbb{R}^{n \\times n}$ is defined as $\\kappa_{2}(A) = \\|A\\|_{2} \\|A^{-1}\\|_{2}$. The two-norm of a matrix, $\\|A\\|_{2}$, is its largest singular value, $\\sigma_{\\max}(A)$. The singular values $\\sigma_i$ of $A$ are the square roots of the eigenvalues of the matrix $A^{T}A$. Consequently, if $A$ is invertible, its condition number can be expressed as the ratio of its largest to its smallest singular value:\n$$\n\\kappa_{2}(A) = \\frac{\\sigma_{\\max}(A)}{\\sigma_{\\min}(A)}\n$$\nThe given matrix is $A=\\begin{pmatrix}1 & 1 \\\\ 1 & 1+\\epsilon\\end{pmatrix}$. Since $A$ is a symmetric matrix, i.e., $A^{T} = A$, its singular values are the absolute values of its eigenvalues. We begin by finding the eigenvalues, $\\lambda$, of $A$ by solving the characteristic equation $\\det(A - \\lambda I) = 0$, where $I$ is the $2 \\times 2$ identity matrix.\n$$\n\\det\\left(\\begin{pmatrix}1 & 1 \\\\ 1 & 1+\\epsilon\\end{pmatrix} - \\lambda\\begin{pmatrix}1 & 0 \\\\ 0 & 1\\end{pmatrix}\\right) = 0\n$$\n$$\n\\det\\begin{pmatrix}1-\\lambda & 1 \\\\ 1 & 1+\\epsilon-\\lambda\\end{pmatrix} = 0\n$$\nExpanding the determinant gives the characteristic polynomial:\n$$\n(1-\\lambda)(1+\\epsilon-\\lambda) - (1)(1) = 0\n$$\n$$\n1 + \\epsilon - \\lambda - \\lambda - \\epsilon\\lambda + \\lambda^2 - 1 = 0\n$$\n$$\n\\lambda^2 - (2+\\epsilon)\\lambda + \\epsilon = 0\n$$\nWe solve this quadratic equation for $\\lambda$ using the quadratic formula, $\\lambda = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$:\n$$\n\\lambda = \\frac{(2+\\epsilon) \\pm \\sqrt{(-(2+\\epsilon))^2 - 4(1)(\\epsilon)}}{2(1)}\n$$\n$$\n\\lambda = \\frac{2+\\epsilon \\pm \\sqrt{4 + 4\\epsilon + \\epsilon^2 - 4\\epsilon}}{2}\n$$\n$$\n\\lambda = \\frac{2+\\epsilon \\pm \\sqrt{4 + \\epsilon^2}}{2}\n$$\nThis gives the two eigenvalues of $A$:\n$$\n\\lambda_1 = \\frac{2+\\epsilon + \\sqrt{4 + \\epsilon^2}}{2}\n$$\n$$\n\\lambda_2 = \\frac{2+\\epsilon - \\sqrt{4 + \\epsilon^2}}{2}\n$$\nSince the problem states $\\epsilon > 0$, we can determine the sign of these eigenvalues. Clearly, $\\lambda_1 > 0$. For $\\lambda_2$, we must compare $2+\\epsilon$ with $\\sqrt{4+\\epsilon^2}$. Since both quantities are positive, we can compare their squares: $(2+\\epsilon)^2 = 4 + 4\\epsilon + \\epsilon^2$. As $\\epsilon > 0$, we have $4+4\\epsilon+\\epsilon^2 > 4+\\epsilon^2$, which implies $(2+\\epsilon)^2 > (\\sqrt{4+\\epsilon^2})^2$. Therefore, $2+\\epsilon > \\sqrt{4+\\epsilon^2}$, which proves that $\\lambda_2 > 0$.\n\nSince both eigenvalues are positive, the matrix $A$ is symmetric positive definite. For a symmetric positive definite matrix, the singular values are identical to the eigenvalues. The largest singular value is $\\sigma_{\\max}(A) = \\lambda_1$ and the smallest is $\\sigma_{\\min}(A) = \\lambda_2$.\nThe two-norm condition number is therefore the ratio of these two eigenvalues:\n$$\n\\kappa_{2}(A) = \\frac{\\lambda_1}{\\lambda_2} = \\frac{\\frac{2+\\epsilon + \\sqrt{4 + \\epsilon^2}}{2}}{\\frac{2+\\epsilon - \\sqrt{4 + \\epsilon^2}}{2}}\n$$\n$$\n\\kappa_{2}(A) = \\frac{2+\\epsilon + \\sqrt{4 + \\epsilon^2}}{2+\\epsilon - \\sqrt{4 + \\epsilon^2}}\n$$\nThis is the exact closed-form expression for the condition number.\n\nNext, we determine the leading-order asymptotic behavior as $\\epsilon \\to 0^{+}$. The numerator approaches $2+0+\\sqrt{4+0} = 4$. For the denominator, we use the Taylor expansion for $\\sqrt{1+x} \\approx 1 + \\frac{1}{2}x$ for small $x$.\n$$\n\\sqrt{4+\\epsilon^2} = 2\\sqrt{1+\\frac{\\epsilon^2}{4}} \\approx 2\\left(1 + \\frac{1}{2}\\frac{\\epsilon^2}{4}\\right) = 2 + \\frac{\\epsilon^2}{4}\n$$\nSubstituting this into the denominator of $\\kappa_2(A)$:\n$$\n\\text{Denominator} = 2 + \\epsilon - \\left(2 + \\frac{\\epsilon^2}{4} + O(\\epsilon^4)\\right) = \\epsilon - \\frac{\\epsilon^2}{4} + O(\\epsilon^4)\n$$\nThus, for $\\epsilon \\to 0^{+}$:\n$$\n\\kappa_{2}(A) \\approx \\frac{4}{\\epsilon - \\frac{\\epsilon^2}{4}} = \\frac{4}{\\epsilon(1 - \\frac{\\epsilon}{4})} \\approx \\frac{4}{\\epsilon}\n$$\nThe leading-order asymptotic behavior is $\\kappa_{2}(A) \\sim \\frac{4}{\\epsilon}$.\n\nFinally, the poor conditioning is explained by the proximity of $A$ to a singular matrix. As $\\epsilon \\to 0^{+}$, the matrix $A$ approaches the matrix $A_0 = \\begin{pmatrix}1 & 1 \\\\ 1 & 1\\end{pmatrix}$. The matrix $A_0$ is singular (rank-deficient), as its columns are linearly dependent and its determinant is $1-1=0$. A singular matrix has at least one singular value equal to zero, and thus an infinite condition number.\nFor our matrix $A$, as $\\epsilon \\to 0^{+}$, the largest singular value $\\sigma_{\\max}(A) = \\lambda_1$ approaches $\\frac{2+\\sqrt{4}}{2} = 2$.\nThe smallest singular value $\\sigma_{\\min}(A) = \\lambda_2$, based on our asymptotic analysis of the denominator, approaches $\\frac{1}{2}(\\epsilon - \\frac{\\epsilon^2}{4}) \\approx \\frac{\\epsilon}{2}$.\nThe condition number, $\\kappa_2(A) = \\frac{\\sigma_{\\max}(A)}{\\sigma_{\\min}(A)}$, therefore behaves as $\\frac{2}{\\epsilon/2} = \\frac{4}{\\epsilon}$. The small parameter $\\epsilon$ directly controls the magnitude of the smallest singular value. As $A$ becomes closer to the singular matrix $A_0$ (i.e., as $\\epsilon \\to 0^{+}$), its smallest singular value approaches zero, causing the ratio $\\sigma_{\\max}/\\sigma_{\\min}$ to diverge. This demonstrates the principle that matrices that are \"nearly singular\" are ill-conditioned.", "answer": "$$\n\\boxed{\\frac{2+\\epsilon + \\sqrt{4 + \\epsilon^2}}{2+\\epsilon - \\sqrt{4 + \\epsilon^2}}}\n$$", "id": "3567278"}, {"introduction": "The condition number provides a powerful, worst-case bound on error amplification, but reality is often more nuanced. This hands-on coding exercise demonstrates that the actual error in solving a linear system depends critically on the *direction* of the data and perturbations relative to the matrix's singular vectors [@problem_id:3567283]. By constructing a system with a large condition number and observing the outcome for different right-hand sides, you will see firsthand that ill-conditioning sensitizes a problem only in specific directions, a crucial insight for practical numerical work.", "problem": "Consider the task of constructing a square matrix $A \\in \\mathbb{R}^{n \\times n}$ with prescribed singular values and then studying the conditioning of the linear system $A x = b$ with respect to forward error in the solution when the coefficient matrix is perturbed. Use the following fundamental base: the singular value decomposition (SVD), the induced two-norm (spectral norm), and the definition of the two-norm condition number. The singular value decomposition (SVD) states that any real matrix $A$ can be written as $A = U \\Sigma V^{\\top}$ with $U$ and $V$ orthogonal and $\\Sigma$ diagonal with nonnegative entries $\\sigma_1 \\ge \\sigma_2 \\ge \\cdots \\ge \\sigma_n \\ge 0$ that are the singular values. The induced two-norm satisfies $\\|A\\|_2 = \\sigma_{\\max}$, where $\\sigma_{\\max}$ is the largest singular value. The two-norm condition number is defined by $\\kappa_2(A) = \\|A\\|_2 \\|A^{-1}\\|_2 = \\sigma_{\\max} / \\sigma_{\\min}$ for a nonsingular $A$, where $\\sigma_{\\min}$ denotes the smallest singular value.\n\nYou must:\n- Construct $A$ from a prescribed set of singular values by taking $U = I_n$ and $V = I_n$ so that $A = \\Sigma$ is diagonal, with the diagonal entries equal to the prescribed singular values in nonincreasing order. This construction ensures that the left singular vectors $u_i$ and right singular vectors $v_i$ coincide with the standard basis vectors $e_i$, and that $\\kappa_2(A)$ equals the ratio of the largest to the smallest prescribed singular value.\n- For a given right-hand side $b$, solve $A x = b$ exactly (in exact arithmetic modeled by floating-point linear algebra routines).\n- Introduce a structured rank-one perturbation $\\Delta A$ with controlled two-norm, defined by $\\Delta A = \\eta \\, \\|A\\|_2 \\, u_{\\max} v_{\\min}^{\\top}$, where $u_{\\max}$ is the left singular vector associated with $\\sigma_{\\max}$, $v_{\\min}$ is the right singular vector associated with $\\sigma_{\\min}$, and $0 < \\eta \\ll 1$ is a prescribed small scalar. Consider the perturbed system $(A + \\Delta A) \\hat{x} = b$ and compute the forward error $e_{\\mathrm{f}} = \\|\\hat{x} - x\\|_2 / \\|x\\|_2$.\n- Explore how the direction of $b$ relative to the smallest right singular vector $v_{\\min}$ controls the forward error by setting $b$ to specific directions in the left singular vector basis (which, under $U = I_n$, coincide with the standard basis). Specifically use $b = u_{\\min}$, $b = u_{\\max}$, $b = u_2$, and $b = (u_{\\max} + u_{\\min}) / \\|u_{\\max} + u_{\\min}\\|_2$, and compare the resulting $e_{\\mathrm{f}}$ values.\n\nYour derivation must begin from the SVD and the definitions above, and must not assume any shortcut formulas beyond these core definitions. In particular, you should reason from the first-order perturbation model for $(A + \\Delta A) \\hat{x} = b$ to connect the structure of $\\Delta A$ and the direction of $b$ to the forward error. Your program should implement these computations exactly as specified.\n\nTest Suite:\nLet $n = 5$ and consider the following five test cases, each providing a prescribed list of singular values, a direction for $b$, and a perturbation level $\\eta$. In each case, you must construct $A = \\operatorname{diag}(\\sigma_1, \\dots, \\sigma_5)$ with the given singular values, verify (internally) that $\\kappa_2(A) = \\sigma_{\\max}/\\sigma_{\\min}$ matches the target obtained from the list, and then compute the forward error as described.\n\n- Case 1 (happy path, moderately ill-conditioned): singular values $[10, 5, 2, 1, 0.1]$, $b = u_{\\min}$, $\\eta = 10^{-8}$. The target condition number is $\\kappa_2(A) = 100$.\n- Case 2 (directional edge case with negligible coupling): singular values $[10, 5, 2, 1, 0.1]$, $b = u_{\\max}$, $\\eta = 10^{-8}$. The target condition number is $\\kappa_2(A) = 100$.\n- Case 3 (orthogonal to $u_{\\min}$): singular values $[10, 5, 2, 1, 0.1]$, $b = u_2$, $\\eta = 10^{-8}$. The target condition number is $\\kappa_2(A) = 100$.\n- Case 4 (well-conditioned boundary): singular values $[1, 1, 1, 1, 1]$, $b = u_{\\min}$, $\\eta = 10^{-8}$. The target condition number is $\\kappa_2(A) = 1$.\n- Case 5 (extremely ill-conditioned boundary): singular values $[1, 1, 1, 1, 10^{-8}]$, $b = u_{\\min}$, $\\eta = 10^{-8}$. The target condition number is $\\kappa_2(A) = 10^{8}$.\n\nFor each case, compute a single float equal to the forward error $e_{\\mathrm{f}} = \\|\\hat{x} - x\\|_2 / \\|x\\|_2$. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[r_1,r_2,r_3,r_4,r_5]$) in the order of the cases listed above. There are no physical units involved; all quantities are dimensionless real numbers. Angles are not used.", "solution": "The user-provided problem is subjected to a rigorous validation procedure.\n\n### Step 1: Extract Givens\n- **Task**: Construct a square matrix $A \\in \\mathbb{R}^{n \\times n}$ with prescribed singular values, solve $A x = b$, and study the forward error in the solution to a perturbed system $(A + \\Delta A) \\hat{x} = b$.\n- **Fundamental Base**:\n    - Singular Value Decomposition (SVD): $A = U \\Sigma V^{\\top}$, where $U, V$ are orthogonal, $\\Sigma$ is diagonal with non-negative entries $\\sigma_1 \\ge \\sigma_2 \\ge \\cdots \\ge \\sigma_n \\ge 0$.\n    - Induced two-norm: $\\|A\\|_2 = \\sigma_{\\max}$.\n    - Two-norm condition number: $\\kappa_2(A) = \\|A\\|_2 \\|A^{-1}\\|_2 = \\sigma_{\\max} / \\sigma_{\\min}$ for non-singular $A$.\n- **Construction of $A$**:\n    - $U = I_n$ and $V = I_n$.\n    - $A = \\Sigma = \\operatorname{diag}(\\sigma_1, \\dots, \\sigma_n)$ with prescribed singular values.\n    - Left and right singular vectors $u_i$ and $v_i$ are the standard basis vectors $e_i$.\n- **Perturbation $\\Delta A$**:\n    - $\\Delta A = \\eta \\, \\|A\\|_2 \\, u_{\\max} v_{\\min}^{\\top}$, where $0 < \\eta \\ll 1$.\n    - $u_{\\max}$ is the left singular vector for $\\sigma_{\\max}$.\n    - $v_{\\min}$ is the right singular vector for $\\sigma_{\\min}$.\n- **Forward Error**:\n    - $e_{\\mathrm{f}} = \\|\\hat{x} - x\\|_2 / \\|x\\|_2$.\n- **Directions for $b$**:\n    - $b = u_{\\min}$\n    - $b = u_{\\max}$\n    - $b = u_2$\n    - $b = (u_{\\max} + u_{\\min}) / \\|u_{\\max} + u_{\\min}\\|_2$\n- **Test Suite**:\n    - $n = 5$.\n    - Case 1: singular values $[10, 5, 2, 1, 0.1]$, $b = u_{\\min}$, $\\eta = 10^{-8}$.\n    - Case 2: singular values $[10, 5, 2, 1, 0.1]$, $b = u_{\\max}$, $\\eta = 10^{-8}$.\n    - Case 3: singular values $[10, 5, 2, 1, 0.1]$, $b = u_2$, $\\eta = 10^{-8}$.\n    - Case 4: singular values $[1, 1, 1, 1, 1]$, $b = u_{\\min}$, $\\eta = 10^{-8}$.\n    - Case 5: singular values $[1, 1, 1, 1, 10^{-8}]$, $b = u_{\\min}$, $\\eta = 10^{-8}$.\n- **Required Output**: A single float for $e_{\\mathrm{f}}$ for each case, presented as a comma-separated list in brackets.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is evaluated against the specified criteria:\n- **Scientifically Grounded**: The problem is firmly rooted in the fundamental principles of numerical linear algebra, specifically perturbation theory, singular value decomposition, and matrix conditioning. All definitions and concepts are standard and mathematically sound.\n- **Well-Posed**: The problem is well-posed. The construction of the matrix $A$, the perturbation $\\Delta A$, and the right-hand side vector $b$ are all precisely specified. For each test case, all inputs are provided, leading to a unique and computable solution for the forward error $e_{\\mathrm{f}}$.\n- **Objective**: The problem is stated using objective and precise mathematical language. There are no subjective or opinion-based statements.\n- **Flaw Checklist**:\n    1.  **Scientific/Factual Unsoundness**: None. The premises are factually correct.\n    2.  **Non-Formalizable/Irrelevant**: None. The problem is a classic, formalizable numerical experiment directly related to the topic of problem conditioning.\n    3.  **Incomplete/Contradictory Setup**: None. All necessary data ($n$, singular values, $\\eta$, choices for $b$) are provided for each test case.\n    4.  **Unrealistic/Infeasible**: None. The computations are standard and well within the capabilities of numerical libraries.\n    5.  **Ill-Posed/Poorly Structured**: None. The structure is clear and leads to a unique result.\n    6.  **Pseudo-Profound/Trivial**: None. While the setup is simplified, it is designed to reveal a non-trivial insight: that the worst-case error bound involving $\\kappa_2(A)$ is not always tight and that the error's sensitivity depends on the alignment of the perturbation and the right-hand side vector with the singular vectors.\n    7.  **Outside Scientific Verifiability**: None. The problem is a mathematical calculation and is perfectly verifiable.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A reasoned, step-by-step solution will be provided.\n\n### Solution\nThe problem requires us to analyze the forward error in a linear system under a specific, structured perturbation. We begin from the foundational principles provided.\n\nLet $A \\in \\mathbb{R}^{n \\times n}$. The problem specifies the construction of $A$ from its singular value decomposition $A = U \\Sigma V^{\\top}$ by setting the orthogonal matrices $U$ and $V$ to the identity matrix, $U=I_n$ and $V=I_n$. The matrix $\\Sigma$ is a diagonal matrix whose entries are the prescribed singular values, $\\sigma_1 \\ge \\sigma_2 \\ge \\cdots \\ge \\sigma_n > 0$. Thus, the matrix $A$ is itself a diagonal matrix:\n$$\nA = \\Sigma = \\operatorname{diag}(\\sigma_1, \\sigma_2, \\ldots, \\sigma_n)\n$$\nFor this matrix $A$, the singular values are $\\{\\sigma_i\\}_{i=1}^n$. The left singular vectors $u_i$ and right singular vectors $v_i$ are the standard basis vectors $e_i \\in \\mathbb{R}^n$, where $e_i$ is a column vector with a $1$ in the $i$-th position and zeros elsewhere. Specifically, $u_i = v_i = e_i$ for all $i=1, \\ldots, n$.\n\nThe key vectors for the perturbation and analysis are:\n- $u_{\\max} = u_1 = e_1$ (left singular vector for $\\sigma_{\\max} = \\sigma_1$)\n- $u_{\\min} = u_n = e_n$ (left singular vector for $\\sigma_{\\min} = \\sigma_n$)\n- $v_{\\min} = v_n = e_n$ (right singular vector for $\\sigma_{\\min} = \\sigma_n$)\n- $u_2 = e_2$\n\nThe perturbation matrix $\\Delta A$ is defined as $\\Delta A = \\eta \\, \\|A\\|_2 \\, u_{\\max} v_{\\min}^{\\top}$.\nSince $\\|A\\|_2 = \\sigma_{\\max} = \\sigma_1$, and with our specific singular vectors, the perturbation is:\n$$\n\\Delta A = \\eta \\, \\sigma_1 \\, e_1 e_n^{\\top}\n$$\nThis is a rank-one matrix with only one non-zero element: a $\\eta \\sigma_1$ at position $(1, n)$.\n\nWe are comparing the solution $x$ of the unperturbed system $Ax=b$ with the solution $\\hat{x}$ of the perturbed system $(A + \\Delta A) \\hat{x} = b$.\nThe unperturbed solution is readily found:\n$$\nx = A^{-1}b = \\Sigma^{-1}b = \\sum_{i=1}^{n} \\frac{b_i}{\\sigma_i} e_i\n$$\nwhere $b_i = e_i^\\top b$ is the $i$-th component of $b$.\n\nTo find the perturbed solution $\\hat{x}$, we must compute the inverse of $A + \\Delta A = \\Sigma + \\eta \\sigma_1 e_1 e_n^{\\top}$. This can be done exactly using the Sherman-Morrison formula for the inverse of a rank-$1$ update: $(M + uv^\\top)^{-1} = M^{-1} - \\frac{M^{-1}uv^\\top M^{-1}}{1 + v^\\top M^{-1}u}$.\nHere, $M = \\Sigma$, $u = \\eta \\sigma_1 e_1$, and $v = e_n$. The denominator is:\n$$\n1 + v^\\top M^{-1} u = 1 + e_n^\\top \\Sigma^{-1} (\\eta \\sigma_1 e_1) = 1 + \\eta \\sigma_1 (e_n^\\top \\Sigma^{-1} e_1)\n$$\nSince $\\Sigma^{-1}$ is diagonal, $e_n^\\top \\Sigma^{-1} e_1 = (\\Sigma^{-1})_{n,1} = 0$. Thus, the denominator is $1$.\nThe inverse is:\n$$\n(A + \\Delta A)^{-1} = \\Sigma^{-1} - \\Sigma^{-1}(\\eta \\sigma_1 e_1 e_n^\\top)\\Sigma^{-1} = \\Sigma^{-1} - \\eta \\sigma_1 (\\Sigma^{-1}e_1)(e_n^\\top\\Sigma^{-1})\n$$\nWe have $\\Sigma^{-1}e_1 = (1/\\sigma_1)e_1$ and $e_n^\\top\\Sigma^{-1} = (1/\\sigma_n)e_n^\\top$. Substituting these in:\n$$\n(A + \\Delta A)^{-1} = \\Sigma^{-1} - \\eta \\sigma_1 \\left(\\frac{1}{\\sigma_1}e_1\\right) \\left(\\frac{1}{\\sigma_n}e_n^\\top\\right) = \\Sigma^{-1} - \\frac{\\eta}{\\sigma_n} e_1 e_n^\\top\n$$\nNow we find the perturbed solution $\\hat{x}$:\n$$\n\\hat{x} = (A + \\Delta A)^{-1}b = \\left(\\Sigma^{-1} - \\frac{\\eta}{\\sigma_n} e_1 e_n^\\top\\right)b = \\Sigma^{-1}b - \\frac{\\eta}{\\sigma_n} e_1 (e_n^\\top b)\n$$\nRecognizing that $x = \\Sigma^{-1}b$ and $e_n^\\top b = b_n$, we find the error vector $\\delta x = \\hat{x} - x$:\n$$\n\\delta x = \\hat{x} - x = - \\frac{\\eta b_n}{\\sigma_n} e_1\n$$\nThis is an exact result. The forward error is an error in the solution vector, which is proportional to $e_1 = u_{\\max}$. Its magnitude depends on $b_n$, the component of $b$ along $u_n = u_{\\min}$.\n\nThe relative forward error $e_{\\mathrm{f}}$ is the ratio of the norms of $\\delta x$ and $x$:\n$$\ne_{\\mathrm{f}} = \\frac{\\|\\delta x\\|_2}{\\|x\\|_2} = \\frac{\\| -(\\eta b_n/\\sigma_n) e_1 \\|_2}{\\| \\Sigma^{-1}b \\|_2} = \\frac{(\\eta |b_n|/\\sigma_n) \\|e_1\\|_2}{\\| \\Sigma^{-1}b \\|_2} = \\frac{\\eta |b_n|/\\sigma_n}{\\left(\\sum_{i=1}^{n} (b_i/\\sigma_i)^2\\right)^{1/2}}\n$$\n\nWe now apply this formula to the specified test cases.\n\n- **For any case where $b$ is orthogonal to $u_{\\min} = e_n$ (i.e., $b_n = 0$)**:\nThe numerator $\\eta |b_n|/\\sigma_n$ is zero, so $e_{\\mathrm{f}} = 0$. This applies to Case 2 ($b=u_{\\max}=e_1$) and Case 3 ($b=u_2=e_2$), as both $e_1$ and $e_2$ are orthogonal to $e_n=e_5$ (since $n=5$).\n\n- **For any case where $b = u_{\\min} = e_n$**:\nHere, $b_n=1$ and $b_i=0$ for $i \\neq n$.\nThe numerator is $\\eta \\cdot 1 / \\sigma_n$.\nThe denominator is $\\|\\Sigma^{-1}e_n\\|_2 = \\|(1/\\sigma_n)e_n\\|_2 = (1/\\sigma_n)\\|e_n\\|_2 = 1/\\sigma_n$.\nThe forward error is:\n$$\ne_{\\mathrm{f}} = \\frac{\\eta/\\sigma_n}{1/\\sigma_n} = \\eta\n$$\nThis result is independent of the condition number $\\kappa_2(A)$ of the matrix. This applies to Cases 1, 4, and 5.\n\nLet us compute the expected results for each test case:\n- **Case 1**: $b = u_{\\min}$, $\\eta = 10^{-8}$. The forward error $e_{\\mathrm{f}}$ is $\\eta = 10^{-8}$.\n- **Case 2**: $b = u_{\\max}$. Since $u_{\\max}=e_1$ is orthogonal to $u_{\\min}=e_5$, $b_5=0$ and the forward error $e_{\\mathrm{f}}$ is $0$.\n- **Case 3**: $b = u_2$. Since $u_2=e_2$ is orthogonal to $u_{\\min}=e_5$, $b_5=0$ and the forward error $e_{\\mathrm{f}}$ is $0$.\n- **Case 4**: $b = u_{\\min}$, $\\eta = 10^{-8}$. The forward error $e_{\\mathrm{f}}$ is $\\eta = 10^{-8}$.\n- **Case 5**: $b = u_{\\min}$, $\\eta = 10^{-8}$. The forward error $e_{\\mathrm{f}}$ is $\\eta = 10^{-8}$.\n\nThe numerical implementation will perform these calculations using floating-point arithmetic. For the cases where the analytical result is $0$, the numerical result will be a very small number, close to machine epsilon. For the other cases, the result will be very close to $\\eta$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the numerical linear algebra problem as specified.\n    Constructs a matrix A from prescribed singular values, introduces a\n    structured perturbation, and computes the forward error for different\n    right-hand side vectors b.\n    \"\"\"\n    \n    # Test cases: (list of singular_values, string for b_choice, float for eta)\n    test_cases = [\n        # Case 1: Moderately ill-conditioned, b aligned with u_min\n        ([10.0, 5.0, 2.0, 1.0, 0.1], 'umin', 1e-8),\n        # Case 2: Moderately ill-conditioned, b aligned with u_max\n        ([10.0, 5.0, 2.0, 1.0, 0.1], 'umax', 1e-8),\n        # Case 3: Moderately ill-conditioned, b aligned with u_2\n        ([10.0, 5.0, 2.0, 1.0, 0.1], 'u2', 1e-8),\n        # Case 4: Well-conditioned, b aligned with u_min\n        ([1.0, 1.0, 1.0, 1.0, 1.0], 'umin', 1e-8),\n        # Case 5: Extremely ill-conditioned, b aligned with u_min\n        ([1.0, 1.0, 1.0, 1.0, 1e-8], 'umin', 1e-8)\n    ]\n\n    results = []\n    n = 5\n\n    for sv, b_choice, eta in test_cases:\n        # Step 1: Construct the matrix A from singular values.\n        # As per the problem, A = Sigma, since U=I and V=I.\n        A = np.diag(sv)\n        \n        # Step 2: Define key parameters and vectors based on the problem statement.\n        # The singular values are given in non-increasing order.\n        sigma_max = sv[0]\n        # sigma_min is not explicitly needed for the delta_A construction, but for context.\n        # sigma_min = sv[-1]\n        \n        # The singular vectors u_i and v_i are the standard basis vectors e_i.\n        u_max = np.zeros(n)\n        u_max[0] = 1.0\n        \n        v_min = np.zeros(n)\n        v_min[n-1] = 1.0\n        \n        # Step 3: Construct the perturbation matrix Delta_A.\n        # Delta_A = eta * ||A||_2 * u_max * v_min^T\n        # ||A||_2 = sigma_max\n        # u_max = e_1, v_min = e_n\n        delta_A = eta * sigma_max * np.outer(u_max, v_min)\n        \n        # Step 4: Define the right-hand side vector b.\n        if b_choice == 'umin':\n            # b = u_min = e_n\n            b = np.zeros(n)\n            b[n-1] = 1.0\n        elif b_choice == 'umax':\n            # b = u_max = e_1\n            b = np.zeros(n)\n            b[0] = 1.0\n        elif b_choice == 'u2':\n            # b = u_2 = e_2\n            b = np.zeros(n)\n            b[1] = 1.0\n        else:\n            # This case is included for robustness, though not in the test suite.\n            raise ValueError(f\"Unknown b_choice: {b_choice}\")\n\n        # Step 5: Solve the unperturbed and perturbed linear systems.\n        # Unperturbed: Ax = b\n        x = np.linalg.solve(A, b)\n        \n        # Perturbed: (A + Delta_A) * x_hat = b\n        A_perturbed = A + delta_A\n        x_hat = np.linalg.solve(A_perturbed, b)\n        \n        # Step 6: Compute the relative forward error.\n        # e_f = ||x_hat - x||_2 / ||x||_2\n        norm_x_diff = np.linalg.norm(x_hat - x)\n        norm_x = np.linalg.norm(x)\n        \n        # Handle the case where norm_x is zero, though not expected here.\n        if norm_x == 0:\n            forward_error = norm_x_diff\n        else:\n            forward_error = norm_x_diff / norm_x\n            \n        results.append(forward_error)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3567283"}]}