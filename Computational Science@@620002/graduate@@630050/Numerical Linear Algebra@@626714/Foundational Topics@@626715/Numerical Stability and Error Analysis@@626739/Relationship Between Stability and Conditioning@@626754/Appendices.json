{"hands_on_practices": [{"introduction": "The conditioning of a problem quantifies its sensitivity to perturbations, but sometimes this sensitivity is an artifact of poor representation. This practice demonstrates how a simple pre-processing step, known as diagonal scaling, can dramatically improve a matrix's condition number. By working through this example [@problem_id:3573473], you will see how re-scaling the rows and columns of an ill-conditioned matrix can lead to a much better-conditioned problem, thereby reducing the theoretical upper bound on the forward error for stable solution methods like LU factorization with partial pivoting.", "problem": "Consider solving the linear system $A x = b$ by Lowerâ€“Upper (LU) factorization with partial pivoting. Let\n$$\nA \\;=\\; \\begin{pmatrix} 10^{-8} & 1 \\\\ 1 & 10^{7} \\end{pmatrix},\n$$\nwhich is ill-scaled. Define diagonal scalings\n$$\nD_{r} \\;=\\; \\begin{pmatrix} 1 & 0 \\\\ 0 & 10^{-7} \\end{pmatrix}, \\qquad\nD_{c} \\;=\\; \\begin{pmatrix} 10^{8} & 0 \\\\ 0 & 1 \\end{pmatrix},\n$$\nand the scaled coefficient matrix\n$$\nS \\;=\\; D_{r} \\, A \\, D_{c}.\n$$\nYou will analyze how diagonal scaling affects the infinity-norm condition number and the forward error bound under a backward stable algorithm.\n\nStarting only from the following fundamental definitions and facts:\n- The matrix infinity norm is $\\|A\\|_{\\infty} \\;=\\; \\max_{1 \\leq i \\leq n} \\sum_{j=1}^{n} |a_{ij}|$.\n- The (infinity-norm) condition number is $\\kappa_{\\infty}(A) \\;=\\; \\|A\\|_{\\infty} \\, \\|A^{-1}\\|_{\\infty}$.\n- Backward stability of Gaussian elimination with partial pivoting: the computed solution $\\hat{x}$ satisfies $(A + \\Delta A)\\hat{x} = b$ with a backward error bounded by $\\|\\Delta A\\|_{\\infty} \\leq C \\, n \\, \\rho \\, \\epsilon \\, \\|A\\|_{\\infty}$, where $C$ is a modest constant independent of $A$, $n$ is the dimension, $\\rho$ is the growth factor, and $\\epsilon$ is machine precision.\n- The forward error and conditioning are related by first principles: if $(A + \\Delta A)\\hat{x} = b$ and $A x = b$, then $A(\\hat{x} - x) = -\\Delta A \\hat{x}$, which can be combined with the above inequalities to produce a forward error bound whose leading term is proportional to $\\kappa_{\\infty}(A)$ times the relative backward error.\n\nTasks:\n1. Compute $S$ explicitly from $A$, $D_{r}$, and $D_{c}$, and evaluate $\\kappa_{\\infty}(A)$ and $\\kappa_{\\infty}(S)$ exactly using the above norm and condition number definitions.\n2. For each matrix, perform $2 \\times 2$ LU factorization with partial pivoting and determine the growth factor $\\rho$ by comparing the largest magnitude entry in the resulting upper-triangular factor to the largest magnitude entry in the original matrix.\n3. Using only the stated definitions and facts, derive an upper bound on the relative forward error $\\|\\hat{x} - x\\|_{\\infty}/\\|x\\|_{\\infty}$ whose leading-order term (in $\\epsilon$) isolates the dependence on $\\kappa_{\\infty}(\\cdot)$, and explain how diagonal scaling modifies this bound.\n4. Finally, compute the exact ratio\n$$\nR \\;=\\; \\frac{\\text{leading-order upper bound for }A}{\\text{leading-order upper bound for }S},\n$$\nunder the simplifying assumptions that $C$, $n$, $\\rho$, and $\\epsilon$ are common to both systems and that the leading-order term dominates (i.e., higher-order terms in $\\epsilon$ can be neglected for a meaningful bound).\n\nExpress your final answer $R$ as a single exact rational number in simplest terms. No rounding is required.", "solution": "This problem is a valid exercise in numerical linear algebra, assessing the concepts of matrix scaling, conditioning, and error analysis for LU factorization. All provided definitions are standard and the tasks are well-posed. We will proceed with the solution by addressing each of the four designated tasks in sequence.\n\n### Task 1: Scaled Matrix and Condition Numbers\n\nFirst, we compute the scaled matrix $S = D_{r} A D_{c}$.\nThe given matrices are:\n$$ A = \\begin{pmatrix} 10^{-8} & 1 \\\\ 1 & 10^{7} \\end{pmatrix}, \\quad D_{r} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 10^{-7} \\end{pmatrix}, \\quad D_{c} = \\begin{pmatrix} 10^{8} & 0 \\\\ 0 & 1 \\end{pmatrix} $$\nThe product is calculated as:\n$$ S = D_{r} A D_{c} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 10^{-7} \\end{pmatrix} \\begin{pmatrix} 10^{-8} & 1 \\\\ 1 & 10^{7} \\end{pmatrix} \\begin{pmatrix} 10^{8} & 0 \\\\ 0 & 1 \\end{pmatrix} $$\n$$ S = \\begin{pmatrix} 1 \\cdot 10^{-8} + 0 \\cdot 1 & 1 \\cdot 1 + 0 \\cdot 10^{7} \\\\ 0 \\cdot 10^{-8} + 10^{-7} \\cdot 1 & 0 \\cdot 1 + 10^{-7} \\cdot 10^{7} \\end{pmatrix} \\begin{pmatrix} 10^{8} & 0 \\\\ 0 & 1 \\end{pmatrix} $$\n$$ S = \\begin{pmatrix} 10^{-8} & 1 \\\\ 10^{-7} & 1 \\end{pmatrix} \\begin{pmatrix} 10^{8} & 0 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 10^{-8} \\cdot 10^{8} + 1 \\cdot 0 & 10^{-8} \\cdot 0 + 1 \\cdot 1 \\\\ 10^{-7} \\cdot 10^{8} + 1 \\cdot 0 & 10^{-7} \\cdot 0 + 1 \\cdot 1 \\end{pmatrix} $$\n$$ S = \\begin{pmatrix} 1 & 1 \\\\ 10 & 1 \\end{pmatrix} $$\nNext, we compute the condition numbers $\\kappa_{\\infty}(A)$ and $\\kappa_{\\infty}(S)$. The condition number is defined as $\\kappa_{\\infty}(M) = \\|M\\|_{\\infty} \\|M^{-1}\\|_{\\infty}$. For a general $2 \\times 2$ matrix $M = \\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}$, its inverse is $M^{-1} = \\frac{1}{ad-bc} \\begin{pmatrix} d & -b \\\\ -c & a \\end{pmatrix}$.\n\nFor matrix $A$:\nThe infinity norm is $\\|A\\|_{\\infty} = \\max(|10^{-8}| + |1|, |1| + |10^{7}|) = 1 + 10^{7}$.\nThe determinant is $\\det(A) = (10^{-8})(10^{7}) - (1)(1) = 10^{-1} - 1 = 0.1 - 1 = -0.9 = -\\frac{9}{10}$.\nThe inverse is $A^{-1} = \\frac{1}{-9/10} \\begin{pmatrix} 10^{7} & -1 \\\\ -1 & 10^{-8} \\end{pmatrix} = -\\frac{10}{9} \\begin{pmatrix} 10^{7} & -1 \\\\ -1 & 10^{-8} \\end{pmatrix}$.\nThe norm of the inverse is $\\|A^{-1}\\|_{\\infty} = |-\\frac{10}{9}| \\max(|10^{7}| + |-1|, |-1| + |10^{-8}|) = \\frac{10}{9} (10^{7} + 1)$.\nThe condition number of $A$ is:\n$$ \\kappa_{\\infty}(A) = \\|A\\|_{\\infty} \\|A^{-1}\\|_{\\infty} = (10^{7} + 1) \\cdot \\frac{10}{9} (10^{7} + 1) = \\frac{10}{9} (10^{7} + 1)^{2} $$\n\nFor matrix $S$:\nThe infinity norm is $\\|S\\|_{\\infty} = \\max(|1| + |1|, |10| + |1|) = \\max(2, 11) = 11$.\nThe determinant is $\\det(S) = (1)(1) - (1)(10) = -9$.\nThe inverse is $S^{-1} = \\frac{1}{-9} \\begin{pmatrix} 1 & -1 \\\\ -10 & 1 \\end{pmatrix} = \\frac{1}{9} \\begin{pmatrix} -1 & 1 \\\\ 10 & -1 \\end{pmatrix}$.\nThe norm of the inverse is $\\|S^{-1}\\|_{\\infty} = \\frac{1}{9} \\max(|-1| + |1|, |10| + |-1|) = \\frac{1}{9} \\max(2, 11) = \\frac{11}{9}$.\nThe condition number of $S$ is:\n$$ \\kappa_{\\infty}(S) = \\|S\\|_{\\infty} \\|S^{-1}\\|_{\\infty} = 11 \\cdot \\frac{11}{9} = \\frac{121}{9} $$\n\n### Task 2: LU Factorization and Growth Factors\n\nWe perform LU factorization with partial pivoting. For a matrix $M$, the growth factor $\\rho$ is the ratio of the largest magnitude entry in the computed upper-triangular factor $U$ to the largest magnitude entry in $M$.\n\nFor matrix $A = \\begin{pmatrix} 10^{-8} & 1 \\\\ 1 & 10^{7} \\end{pmatrix}$:\nThe largest magnitude entry in the first column is $|a_{21}| = 1$. We pivot by swapping rows $1$ and $2$.\nLet $P = \\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix}$.\n$PA = \\begin{pmatrix} 1 & 10^{7} \\\\ 10^{-8} & 1 \\end{pmatrix}$.\nThe elimination multiplier is $m_{21} = \\frac{10^{-8}}{1} = 10^{-8}$.\nThe $L$ and $U$ factors are:\n$L_{A} = \\begin{pmatrix} 1 & 0 \\\\ 10^{-8} & 1 \\end{pmatrix}$, $U_{A} = \\begin{pmatrix} 1 & 10^{7} \\\\ 0 & 1 - (10^{-8})(10^{7}) \\end{pmatrix} = \\begin{pmatrix} 1 & 10^{7} \\\\ 0 & 1 - 0.1 \\end{pmatrix} = \\begin{pmatrix} 1 & 10^{7} \\\\ 0 & 0.9 \\end{pmatrix}$.\nThe largest magnitude entry in $A$ is $\\|A\\|_{\\max} = 10^{7}$. The largest magnitude entry in $U_A$ is $\\|U_A\\|_{\\max} = 10^{7}$.\nThe growth factor for $A$ is $\\rho_{A} = \\frac{\\|U_A\\|_{\\max}}{\\|A\\|_{\\max}} = \\frac{10^{7}}{10^{7}} = 1$.\n\nFor matrix $S = \\begin{pmatrix} 1 & 1 \\\\ 10 & 1 \\end{pmatrix}$:\nThe largest magnitude entry in the first column is $|s_{21}| = 10$. We pivot by swapping rows $1$ and $2$.\nLet $P = \\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix}$.\n$PS = \\begin{pmatrix} 10 & 1 \\\\ 1 & 1 \\end{pmatrix}$.\nThe elimination multiplier is $m_{21} = \\frac{1}{10} = 0.1$.\nThe $L$ and $U$ factors are:\n$L_{S} = \\begin{pmatrix} 1 & 0 \\\\ 0.1 & 1 \\end{pmatrix}$, $U_{S} = \\begin{pmatrix} 10 & 1 \\\\ 0 & 1 - (0.1)(1) \\end{pmatrix} = \\begin{pmatrix} 10 & 1 \\\\ 0 & 0.9 \\end{pmatrix}$.\nThe largest magnitude entry in $S$ is $\\|S\\|_{\\max} = 10$. The largest magnitude entry in $U_S$ is $\\|U_S\\|_{\\max} = 10$.\nThe growth factor for $S$ is $\\rho_{S} = \\frac{\\|U_S\\|_{\\max}}{\\|S\\|_{\\max}} = \\frac{10}{10} = 1$.\n\n### Task 3: Forward Error Bound Derivation\n\nWe are given that the computed solution $\\hat{x}$ to $Ax = b$ satisfies $(A + \\Delta A)\\hat{x} = b$ for a backward error matrix $\\Delta A$. The true solution satisfies $Ax=b$.\nSubtracting the two equations gives $A\\hat{x} + \\Delta A \\hat{x} - Ax = 0$, which rearranges to $A(\\hat{x}-x) = -\\Delta A \\hat{x}$.\nTo find the error in $x$, we can write $\\hat{x} = x + (\\hat{x}-x)$. Substituting this into the right-hand side gives:\n$A(\\hat{x}-x) = -\\Delta A (x + (\\hat{x}-x))$.\nRearranging terms to solve for the error $\\hat{x}-x$:\n$(A+\\Delta A)(\\hat{x}-x) = -\\Delta A x$.\n$\\hat{x}-x = -(A+\\Delta A)^{-1} \\Delta A x$.\nTaking the infinity norm on both sides:\n$\\|\\hat{x}-x\\|_{\\infty} = \\|-(A+\\Delta A)^{-1} \\Delta A x\\|_{\\infty} \\le \\|(A+\\Delta A)^{-1}\\|_{\\infty} \\|\\Delta A\\|_{\\infty} \\|x\\|_{\\infty}$.\nWe can write $(A+\\Delta A)^{-1} = (A(I+A^{-1}\\Delta A))^{-1} = (I+A^{-1}\\Delta A)^{-1} A^{-1}$.\nUsing the submultiplicative property and the series expansion for $(I-B)^{-1}$, assuming $\\|A^{-1}\\Delta A\\|_{\\infty} < 1$:\n$\\|(A+\\Delta A)^{-1}\\|_{\\infty} \\le \\|(I+A^{-1}\\Delta A)^{-1}\\|_{\\infty} \\|A^{-1}\\|_{\\infty} \\le \\frac{\\|A^{-1}\\|_{\\infty}}{1-\\|A^{-1}\\Delta A\\|_{\\infty}}$.\nSubstituting this into the error inequality provides the bound for the relative forward error:\n$$ \\frac{\\|\\hat{x}-x\\|_{\\infty}}{\\|x\\|_{\\infty}} \\le \\frac{\\|A^{-1}\\|_{\\infty} \\|\\Delta A\\|_{\\infty}}{1 - \\|A^{-1}\\|_{\\infty} \\|\\Delta A\\|_{\\infty}} $$\nWe are given $\\|\\Delta A\\|_{\\infty} \\le C n \\rho \\epsilon \\|A\\|_{\\infty}$. Let $\\eta = C n \\rho \\epsilon$.\nThen $\\|A^{-1}\\|_{\\infty} \\|\\Delta A\\|_{\\infty} \\le \\|A^{-1}\\|_{\\infty} (\\eta \\|A\\|_{\\infty}) = \\eta \\kappa_{\\infty}(A)$.\nSubstituting this, we get the bound:\n$$ \\frac{\\|\\hat{x}-x\\|_{\\infty}}{\\|x\\|_{\\infty}} \\le \\frac{\\eta \\kappa_{\\infty}(A)}{1 - \\eta \\kappa_{\\infty}(A)} $$\nFor small $\\epsilon$, the term $\\eta \\kappa_{\\infty}(A)$ is small, and we can expand the denominator as a geometric series: $\\eta \\kappa_{\\infty}(A) (1 + \\eta \\kappa_{\\infty}(A) + (\\eta \\kappa_{\\infty}(A))^2 + \\dots)$.\nThe leading-order term (in $\\epsilon$) of this upper bound is $\\eta \\kappa_{\\infty}(A) = C n \\rho \\epsilon \\kappa_{\\infty}(A)$.\n\nDiagonal scaling transforms the problem $Ax=b$ into an equivalent system $Sy=d$ where $S=D_r A D_c$, $y=D_c^{-1}x$, and $d=D_r b$. The forward error bound for solving this new system for $y$ has a leading-order term of $C n \\rho_S \\epsilon \\kappa_{\\infty}(S)$. Scaling modifies the bound by replacing the product $\\rho_A \\kappa_{\\infty}(A)$ for the original system with $\\rho_S \\kappa_{\\infty}(S)$ for the scaled system. In this problem, $\\rho_A=\\rho_S=1$, so the change in the error bound is governed by the change in the condition number. Our calculations show $\\kappa_{\\infty}(A) \\approx 1.1 \\times 10^{14}$ while $\\kappa_{\\infty}(S) \\approx 13.4$, so the scaling dramatically reduces the condition number and thus the theoretical upper bound on the forward error.\n\n### Task 4: Ratio of Error Bounds\n\nThe ratio $R$ is defined as the leading-order upper bound for system $A$ divided by the leading-order upper bound for system $S$. Using the result from Task 3 and the assumption that $C$, $n$, $\\rho$, and $\\epsilon$ are common to both systems:\n$$ R = \\frac{\\text{leading-order upper bound for }A}{\\text{leading-order upper bound for }S} = \\frac{C n \\rho \\epsilon \\, \\kappa_{\\infty}(A)}{C n \\rho \\epsilon \\, \\kappa_{\\infty}(S)} = \\frac{\\kappa_{\\infty}(A)}{\\kappa_{\\infty}(S)} $$\nSubstituting the values calculated in Task 1:\n$$ \\kappa_{\\infty}(A) = \\frac{10}{9} (10^{7} + 1)^{2} $$\n$$ \\kappa_{\\infty}(S) = \\frac{121}{9} $$\nThe ratio is:\n$$ R = \\frac{\\frac{10}{9} (10^{7} + 1)^{2}}{\\frac{121}{9}} = \\frac{10(10^{7} + 1)^{2}}{121} $$\nTo simplify, we note that $10^{7}+1 = 10000001$. The sum of alternating digits is $1-0+0-0+0-0+0-1=0$, which means $10000001$ is divisible by $11$.\nIndeed, $10000001 = 11 \\times 909091$.\nSubstituting this into the expression for $R$:\n$$ R = \\frac{10(11 \\times 909091)^{2}}{121} = \\frac{10 \\times 11^{2} \\times 909091^{2}}{121} = 10 \\times 909091^{2} $$\nWe compute the square $909091^{2}$:\n$$ 909091^{2} = 826446446281 $$\nFinally, we multiply by $10$:\n$$ R = 10 \\times 826446446281 = 8264464462810 $$\nThis is the exact rational number for the ratio $R$.", "answer": "$$\n\\boxed{8264464462810}\n$$", "id": "3573473"}, {"introduction": "For a given problem, the choice of algorithm can be the difference between an accurate solution and numerical catastrophe. This exercise [@problem_id:3573481] directly contrasts two common methods for solving least squares problems: the stable QR factorization and the often-unstable Normal Equations. You will discover how the Normal Equations method inherently squares the condition number $\\kappa_{2}(A)$ of the problem, making it highly susceptible to error for even moderately ill-conditioned matrices, while the QR method's error scales with $\\kappa_{2}(A)$ itself.", "problem": "Consider the overdetermined least squares (LS) problem with matrix and right-hand side\n$$\nA_{\\delta} \\in \\mathbb{R}^{3 \\times 2}, \\quad A_{\\delta} = \\begin{pmatrix} 1 & 0 \\\\ 0 & \\delta \\\\ 0 & 0 \\end{pmatrix}, \n\\qquad \nb_{\\delta} = \\begin{pmatrix} 1 \\\\ \\delta \\\\ 0 \\end{pmatrix},\n$$\nwhere $\\delta \\in (0,1)$ is a parameter. The exact LS solution $x^{\\star}$ satisfies $A_{\\delta} x^{\\star} = b_{\\delta}$.\n\nAssume floating-point arithmetic with unit roundoff $u = 2^{-53}$. Consider two solvers:\n1. An orthogonal-triangular (QR) factorization based LS solver.\n2. A normal equations solver that first forms $A_{\\delta}^{\\top} A_{\\delta}$ and $A_{\\delta}^{\\top} b_{\\delta}$ in floating point and then solves the resulting symmetric positive definite (SPD) linear system by a numerically stable method.\n\nStarting only from the following foundations:\n- The spectral (two-norm) condition number $\\kappa_{2}(M) = \\sigma_{\\max}(M)/\\sigma_{\\min}(M)$ for a full-column-rank matrix $M$, where $\\sigma_{\\max}(M)$ and $\\sigma_{\\min}(M)$ are the largest and smallest singular values.\n- The definition of backward stability: a backward-stable algorithm returns the exact solution to a nearby problem with data perturbed by a quantity of order $u$ in relative norm.\n- The first-order relation between forward error and conditioning: to leading order, the relative forward error is bounded by the condition number times the relative backward error of the data.\n- The identity $\\kappa_{2}(A^{\\top}A) = \\kappa_{2}(A)^{2}$ for any full-column-rank matrix $A$.\n\nDerive explicit first-order upper bounds, in terms of $\\kappa_{2}(A_{\\delta})$ and $u$, for the relative forward error $\\|x_{\\mathrm{comp}} - x^{\\star}\\|_{2}/\\|x^{\\star}\\|_{2}$ of each solver. Using these bounds, compute the ratio\n$$\nR(\\delta) = \\frac{\\text{(normal equations forward error bound)}}{\\text{(QR forward error bound)}}\n$$\nfor the specific instance $\\delta = 10^{-8}$. Report $R(10^{-8})$ as your final answer. No rounding is necessary and no units are required.", "solution": "The user wants me to critically validate and solve a numerical linear algebra problem.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\n*   Least squares problem with matrix $A_{\\delta} \\in \\mathbb{R}^{3 \\times 2}$ and right-hand side $b_{\\delta}$, where\n    $$\n    A_{\\delta} = \\begin{pmatrix} 1 & 0 \\\\ 0 & \\delta \\\\ 0 & 0 \\end{pmatrix}, \n    \\qquad \n    b_{\\delta} = \\begin{pmatrix} 1 \\\\ \\delta \\\\ 0 \\end{pmatrix}.\n    $$\n*   The parameter $\\delta$ is in the range $\\delta \\in (0,1)$.\n*   The exact least squares solution $x^{\\star}$ satisfies the equation $A_{\\delta} x^{\\star} = b_{\\delta}$.\n*   Floating-point arithmetic is used, with unit roundoff $u = 2^{-53}$.\n*   Two solvers are considered: a QR-based solver and a normal equations solver.\n*   The analysis must be based on four foundational principles:\n    1.  The spectral condition number is $\\kappa_{2}(M) = \\sigma_{\\max}(M)/\\sigma_{\\min}(M)$.\n    2.  A backward-stable algorithm gives the exact solution to a nearby problem with data perturbed by a quantity of order $u$ in relative norm.\n    3.  The first-order relation: relative forward error $\\le$ (condition number) $\\times$ (relative backward error).\n    4.  The identity: $\\kappa_{2}(A^{\\top}A) = \\kappa_{2}(A)^{2}$ for a full-column-rank matrix $A$.\n\n**Step 2: Validate Using Extracted Givens**\n\nThe problem is scientifically grounded in established principles of numerical linear algebra. It is a canonical example used to illustrate the concepts of conditioning and stability. The problem statement is objective, self-contained, and mathematically consistent. The matrix $A_\\delta$ has full column rank for $\\delta \\in (0,1)$. The condition $A_{\\delta} x^{\\star} = b_{\\delta}$ implies that the least squares problem has a zero residual, which is a crucial detail.\nLet $x^{\\star} = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}$. Then $A_{\\delta} x^{\\star} = \\begin{pmatrix} 1 & 0 \\\\ 0 & \\delta \\\\ 0 & 0 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = \\begin{pmatrix} x_1 \\\\ \\delta x_2 \\\\ 0 \\end{pmatrix}$. Setting this equal to $b_{\\delta} = \\begin{pmatrix} 1 \\\\ \\delta \\\\ 0 \\end{pmatrix}$ yields $x_1=1$ and $\\delta x_2 = \\delta$. Since $\\delta \\neq 0$, we have $x_2=1$. Thus, the exact solution is $x^{\\star} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$, confirming the consistency of the problem statement. The problem is well-posed and has a unique, meaningful solution path based on the provided foundations.\n\n**Step 3: Verdict and Action**\n\nThe problem is valid. A complete solution will be provided.\n\n### Solution\n\nThe solution requires deriving first-order upper bounds for the relative forward error of two different least squares solvers and then computing their ratio. The forward error is defined as $\\|x_{\\mathrm{comp}} - x^{\\star}\\|_{2}/\\|x^{\\star}\\|_{2}$.\n\nFirst, we determine the condition number of the matrix $A_{\\delta}$. The singular values of $A_{\\delta}$ are the square roots of the eigenvalues of $A_{\\delta}^{\\top}A_{\\delta}$.\n$$\nA_{\\delta}^{\\top}A_{\\delta} = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & \\delta & 0 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 0 & \\delta \\\\ 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 0 & \\delta^2 \\end{pmatrix}.\n$$\nThe eigenvalues of this diagonal matrix are $\\lambda_1 = 1$ and $\\lambda_2 = \\delta^2$. The singular values of $A_{\\delta}$ are $\\sigma_{\\max}(A_{\\delta}) = \\sqrt{1} = 1$ and $\\sigma_{\\min}(A_{\\delta}) = \\sqrt{\\delta^2} = \\delta$, since $\\delta > 0$.\nThe spectral condition number of $A_{\\delta}$ is:\n$$\n\\kappa_2(A_{\\delta}) = \\frac{\\sigma_{\\max}(A_{\\delta})}{\\sigma_{\\min}(A_{\\delta})} = \\frac{1}{\\delta}.\n$$\n\nNow, we analyze each solver using the provided foundations. The guiding principle is the first-order error relation:\n$$\n\\frac{\\|x_{\\mathrm{comp}} - x^{\\star}\\|_{2}}{\\|x^{\\star}\\|_{2}} \\le (\\text{Condition Number}) \\times (\\text{Relative Backward Error}).\n$$\n\n**1. Orthogonal-Triangular (QR) Factorization Solver**\n\nA QR-based solver for the least squares problem is known to be backward stable. According to the problem's foundations, this means the computed solution $x_{\\mathrm{QR}}$ is the exact solution to a nearby problem where the data $(A_{\\delta}, b_{\\delta})$ is perturbed by a relative amount of order $u$. So, the relative backward error is of order $u$.\n\nThe appropriate condition number for a least squares problem depends on the residual. For a problem with zero residual, as is the case here since $A_{\\delta}x^{\\star}=b_{\\delta}$, the sensitivity of the solution is governed by $\\kappa_2(A_{\\delta})$.\nApplying the given error relation, the first-order upper bound for the relative forward error of the QR solver is:\n$$\nE_{\\mathrm{QR}} = \\kappa_2(A_{\\delta}) \\cdot u = \\frac{1}{\\delta} u.\n$$\nHere, we neglect any $O(1)$ constants, as the focus is on the dependence on $\\kappa_2(A_{\\delta})$ and $u$.\n\n**2. Normal Equations Solver**\n\nThis method consists of two parts:\na. Form the normal-equations matrix $C = A_{\\delta}^{\\top}A_{\\delta}$ and the right-hand side vector $d = A_{\\delta}^{\\top}b_{\\delta}$ in floating-point arithmetic.\nb. Solve the resulting symmetric positive definite (SPD) linear system $Cx=d$ using a numerically stable method.\n\nThis entire process can be viewed as an algorithm for solving the linear system $Cx=d$. The floating-point computations in step (a) introduce perturbations into the data $(C, d)$ of this linear system. The numerically stable solver in step (b) also contributes a backward error. The cumulative effect is that the computed solution, $x_{\\mathrm{NE}}$, is the exact solution to a perturbed linear system, where the relative perturbations to $C$ and $d$ are of order $u$. Thus, the relative backward error for the problem of solving $Cx=d$ is of order $u$.\n\nThe condition number relevant to this linear system problem is $\\kappa_2(C) = \\kappa_2(A_{\\delta}^{\\top}A_{\\delta})$. Using the identity provided in the problem statement, we can relate this to the condition number of $A_{\\delta}$:\n$$\n\\kappa_2(A_{\\delta}^{\\top}A_{\\delta}) = \\kappa_2(A_{\\delta})^2 = \\left(\\frac{1}{\\delta}\\right)^2 = \\frac{1}{\\delta^2}.\n$$\nApplying the error relation, the first-order upper bound for the relative forward error of the normal equations solver is:\n$$\nE_{\\mathrm{NE}} = \\kappa_2(A_{\\delta}^{\\top}A_{\\delta}) \\cdot u = \\kappa_2(A_{\\delta})^2 \\cdot u = \\frac{1}{\\delta^2} u.\n$$\n\n**Ratio of Error Bounds**\n\nWe are asked to compute the ratio $R(\\delta) = \\frac{\\text{(normal equations forward error bound)}}{\\text{(QR forward error bound)}}$.\n$$\nR(\\delta) = \\frac{E_{\\mathrm{NE}}}{E_{\\mathrm{QR}}} = \\frac{\\frac{1}{\\delta^2} u}{\\frac{1}{\\delta} u} = \\frac{1}{\\delta}.\n$$\n\nFinally, we evaluate this ratio for the specific instance $\\delta = 10^{-8}$.\n$$\nR(10^{-8}) = \\frac{1}{10^{-8}} = 10^8.\n$$\nThis result demonstrates that for an ill-conditioned problem (small $\\delta$), the error bound for the normal equations method is significantly larger than that for the QR-based method, highlighting the superior numerical stability of the latter.", "answer": "$$\n\\boxed{10^8}\n$$", "id": "3573481"}, {"introduction": "The relationship between forward error, backward error, and conditioning is often summarized as: Forward Error $\\approx$ Conditioning $\\times$ Backward Error. This practice [@problem_id:3573488] provides a fascinating and counter-intuitive look at this rule, presenting an algorithm that is technically \"unstable\" due to a large backward error. You will calculate how, despite this instability, the algorithm produces a highly accurate result because the problem itself is extremely well-conditioned, demonstrating that both factors must be considered in concert to predict the final accuracy.", "problem": "Consider the linear system $A x = b$ in $\\mathbb{R}^{3}$ with\n$$\nA = \\mathrm{diag}(1, 2, 3), \\qquad b = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}.\n$$\nAn implementation error in a solver produces the iterate $y$ by first replacing $A$ with $A + E$ where $E = 0.05\\,A$, and then computing\n$$\ny = (A + E)^{-1} b.\n$$\nThis practice is known to be algorithmically unstable because the backward error $\\|E\\|_{2} / \\|A\\|_{2}$ is not close to the roundoff level of the underlying arithmetic. Nevertheless, the problem instance $(A, b)$ may be sufficiently well-conditioned to mitigate the instability and still yield an accurate $y$.\n\nUsing only the core definitions from numerical linear algebra (the two-norm condition number $\\kappa_{2}(A) = \\|A\\|_{2}\\,\\|A^{-1}\\|_{2}$, backward error characterized by the perturbation $E$, and forward error $\\|y - x\\|_{2}$ where $x = A^{-1} b$), perform the following:\n\n1. Compute the two-norm condition number $\\kappa_{2}(A)$.\n2. Compute the relative backward error $\\|E\\|_{2} / \\|A\\|_{2}$.\n3. Derive $y$ in terms of $x$ and $A^{-1}E$ starting from $(A + E) y = b$ and obtain the exact value of the relative forward error $\\|y - x\\|_{2} / \\|x\\|_{2}$.\n4. Provide the exact value of the relative forward error as a single number. Do not round; express the answer in simplest exact form.\n\nYour final response must be a single real-valued number as specified above.", "solution": "The problem as stated is formally sound. It is self-contained, scientifically grounded in numerical linear algebra, and well-posed, providing all necessary information to compute a unique, verifiable solution. All terms are defined according to standard conventions in the field. Therefore, a full solution is warranted.\n\nThe problem asks for four specific items: the condition number of the matrix $A$, the relative backward error, a derivation of the relative forward error, and its exact value. We will address each in turn.\n\nThe given linear system is $A x = b$, with the matrix $A$ and vector $b$ defined as:\n$$\nA = \\mathrm{diag}(1, 2, 3) = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 2 & 0 \\\\ 0 & 0 & 3 \\end{pmatrix}, \\qquad b = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}\n$$\nThe exact solution is $x = A^{-1} b$.\nA perturbed solution $y$ is computed from $(A+E)y=b$, where the perturbation is $E = 0.05 A$.\n\n**1. Computation of the condition number $\\kappa_{2}(A)$**\n\nThe two-norm condition number is defined as $\\kappa_{2}(A) = \\|A\\|_{2} \\|A^{-1}\\|_{2}$. For a diagonal matrix, the matrix $2$-norm is the maximum absolute value of its diagonal entries.\nThe matrix $A$ has diagonal entries $1$, $2$, and $3$. Thus, its $2$-norm is:\n$$\n\\|A\\|_{2} = \\max(|1|, |2|, |3|) = 3\n$$\nThe inverse of $A$, being a diagonal matrix, is found by taking the reciprocal of each diagonal entry:\n$$\nA^{-1} = \\mathrm{diag}(1^{-1}, 2^{-1}, 3^{-1}) = \\mathrm{diag}\\left(1, \\frac{1}{2}, \\frac{1}{3}\\right)\n$$\nThe $2$-norm of $A^{-1}$ is similarly the maximum of the absolute values of its diagonal entries:\n$$\n\\|A^{-1}\\|_{2} = \\max\\left(|1|, \\left|\\frac{1}{2}\\right|, \\left|\\frac{1}{3}\\right|\\right) = 1\n$$\nTherefore, the condition number $\\kappa_{2}(A)$ is:\n$$\n\\kappa_{2}(A) = \\|A\\|_{2} \\|A^{-1}\\|_{2} = 3 \\times 1 = 3\n$$\n\n**2. Computation of the relative backward error**\n\nThe relative backward error is given by the expression $\\|E\\|_{2} / \\|A\\|_{2}$.\nThe perturbation matrix $E$ is defined as $E = 0.05 A$.\n$$\nE = 0.05 \\times \\mathrm{diag}(1, 2, 3) = \\mathrm{diag}(0.05, 0.10, 0.15)\n$$\nThe $2$-norm of $E$ is the maximum absolute value of its diagonal entries:\n$$\n\\|E\\|_{2} = \\max(|0.05|, |0.10|, |0.15|) = 0.15\n$$\nThe relative backward error is then:\n$$\n\\frac{\\|E\\|_{2}}{\\|A\\|_{2}} = \\frac{0.15}{3} = 0.05\n$$\nAlternatively, since norms are homogeneous, for $E=cA$ with a scalar $c$, we have $\\|E\\|_2 = \\|cA\\|_2 = |c|\\|A\\|_2$. The relative backward error is therefore $\\frac{|c|\\|A\\|_2}{\\|A\\|_2} = |c|$. In this problem, $c = 0.05$, so the relative backward error must be $0.05$.\n\n**3. Derivation and computation of the relative forward error**\n\nThe computed solution $y$ satisfies the equation $(A + E) y = b$. The exact solution $x$ satisfies $A x = b$. We can substitute $b=Ax$ into the first equation:\n$$\n(A + E) y = A x\n$$\nSince $A = \\mathrm{diag}(1, 2, 3)$, its determinant is $1 \\times 2 \\times 3 = 6 \\neq 0$, so $A$ is invertible. We can multiply by $A^{-1}$ from the left:\n$$\nA^{-1}(A + E) y = A^{-1}(A x)\n$$\n$$\n(A^{-1}A + A^{-1}E) y = (A^{-1}A) x\n$$\n$$\n(I + A^{-1}E) y = I x = x\n$$\nHere, $I$ is the $3 \\times 3$ identity matrix.\nThe problem provides the specific structure of the perturbation, $E = 0.05 A$. We can substitute this into the term $A^{-1}E$:\n$$\nA^{-1}E = A^{-1}(0.05 A) = 0.05 (A^{-1} A) = 0.05 I\n$$\nThis simplifies the equation for $y$ substantially:\n$$\n(I + 0.05 I) y = x\n$$\n$$\n(1.05 I) y = x\n$$\nSolving for $y$ yields a simple relationship between $y$ and $x$:\n$$\ny = (1.05 I)^{-1} x = \\frac{1}{1.05} I x = \\frac{1}{1.05} x\n$$\nNow we can determine the forward error, $y - x$:\n$$\ny - x = \\frac{1}{1.05} x - x = \\left(\\frac{1}{1.05} - 1\\right) x = \\left(\\frac{1 - 1.05}{1.05}\\right) x = -\\frac{0.05}{1.05} x\n$$\nThe relative forward error is defined as $\\frac{\\|y - x\\|_{2}}{\\|x\\|_{2}}$.\nTaking the $2$-norm of the forward error vector:\n$$\n\\|y - x\\|_{2} = \\left\\|-\\frac{0.05}{1.05} x\\right\\|_{2} = \\left|-\\frac{0.05}{1.05}\\right| \\|x\\|_{2} = \\frac{0.05}{1.05} \\|x\\|_{2}\n$$\nThe vector $x = A^{-1} b = \\mathrm{diag}(1, 1/2, 1/3) \\begin{pmatrix} 1, 1, 1 \\end{pmatrix}^T = \\begin{pmatrix} 1, 1/2, 1/3 \\end{pmatrix}^T$ is not the zero vector, so its norm $\\|x\\|_{2}$ is non-zero, and we can divide by it.\nThe relative forward error is:\n$$\n\\frac{\\|y - x\\|_{2}}{\\|x\\|_{2}} = \\frac{\\frac{0.05}{1.05} \\|x\\|_{2}}{\\|x\\|_{2}} = \\frac{0.05}{1.05}\n$$\n\n**4. Exact value of the relative forward error**\n\nThe final step is to express this value in its simplest exact form.\n$$\n\\frac{0.05}{1.05} = \\frac{5/100}{105/100} = \\frac{5}{105}\n$$\nDividing the numerator and the denominator by their greatest common divisor, which is $5$:\n$$\n\\frac{5}{105} = \\frac{5 \\div 5}{105 \\div 5} = \\frac{1}{21}\n$$\nThus, the exact value of the relative forward error is $\\frac{1}{21}$.", "answer": "$$\\boxed{\\frac{1}{21}}$$", "id": "3573488"}]}