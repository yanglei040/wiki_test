{"hands_on_practices": [{"introduction": "We begin with a foundational exercise to solidify the connection between a matrix's condition number and its singular values. This practice involves a direct calculation for a simple symmetric matrix, providing a concrete example of how the ratio of the largest to the smallest singular value quantifies the matrix's sensitivity. Mastering this basic computation is the first step toward understanding the broader implications of conditioning in numerical linear algebra. [@problem_id:1049315]", "problem": "Consider the symmetric $2 \\times 2$ matrix\n$$\nA = \\begin{bmatrix} 2 & 1 \\\\ 1 & 2 \\end{bmatrix}\n$$\nThe condition number for matrix inversion with respect to the spectral norm (2-norm) is defined as $\\kappa(A) = \\|A\\|_2 \\cdot \\|A^{-1}\\|_2$, where $\\| \\cdot \\|_2$ denotes the spectral norm. This can be equivalently expressed using the singular values $\\sigma_{\\max}$ and $\\sigma_{\\min}$ of $A$. Compute $\\kappa(A)$ by determining the singular values of $A$.", "solution": "1. The spectral norm of a symmetric positive-definite matrix equals its largest eigenvalue, and its inverseâ€™s spectral norm equals the reciprocal of its smallest eigenvalue. Since the eigenvalues of $A$ will be positive, it is positive-definite, and its singular values are its eigenvalues.\n2. Compute the eigenvalues of $A$ from its characteristic equation:\n$$\n\\det(A-\\lambda I)=(2-\\lambda)^2-1=\\lambda^2-4\\lambda+3=0\n$$\n3. Solve the quadratic equation to get the eigenvalues:\n$$\n\\lambda=\\frac{4\\pm\\sqrt{16-12}}{2}=\\{3,1\\}\n$$\n4. Hence the singular values are $\\sigma_{\\max}=3$ and $\\sigma_{\\min}=1$, giving the condition number:\n$$\n\\kappa(A)=\\frac{\\sigma_{\\max}}{\\sigma_{\\min}}=\\frac{3}{1}=3\n$$", "answer": "$$\\boxed{3}$$", "id": "1049315"}, {"introduction": "Building upon the basic definition, this next problem explores how the condition number behaves under small perturbations. By analyzing a matrix that includes a variable imperfection term, $\\epsilon$, you will derive an expression for the condition number as a function of this perturbation. This exercise provides a powerful illustration of why the condition number is a key measure of a system's numerical stability and robustness. [@problem_id:2210773]", "problem": "Consider a system of linear equations $A \\mathbf{x} = \\mathbf{b}$ that models a two-dimensional physical process. Ideally, the system's behavior is described by the $2 \\times 2$ identity matrix, $I_2$. However, due to a small manufacturing imperfection, a weak cross-coupling term is introduced. The system matrix is now given by $A(\\epsilon) = I_2 + \\epsilon E_{12}$, where $\\epsilon$ is a non-negative real number representing the magnitude of the imperfection, and $E_{12}$ is the matrix with a 1 in the entry at the first row and second column, and zeros elsewhere.\n\nThe sensitivity of the solution $\\mathbf{x}$ to perturbations in the vector $\\mathbf{b}$ is characterized by the condition number of the matrix $A(\\epsilon)$. The condition number with respect to the matrix 2-norm is defined as $\\kappa_2(A) = \\|A\\|_2 \\|A^{-1}\\|_2$.\n\nDetermine the 2-norm condition number, $\\kappa_2(A(\\epsilon))$, of this perturbed matrix. Express your answer as a closed-form analytic expression in terms of $\\epsilon$.", "solution": "We are given $A(\\epsilon) = I_{2} + \\epsilon E_{12} = \\begin{pmatrix} 1 & \\epsilon \\\\ 0 & 1 \\end{pmatrix}$ with $\\epsilon \\geq 0$. The 2-norm condition number is defined by $\\kappa_{2}(A) = \\|A\\|_{2}\\,\\|A^{-1}\\|_{2}$. Using the singular value characterization of the matrix 2-norm, we have $\\|A\\|_{2} = \\sigma_{\\max}(A)$ and $\\|A^{-1}\\|_{2} = 1/\\sigma_{\\min}(A)$, so\n$$\n\\kappa_{2}(A) = \\frac{\\sigma_{\\max}(A)}{\\sigma_{\\min}(A)} = \\sqrt{\\frac{\\lambda_{\\max}(A^{T}A)}{\\lambda_{\\min}(A^{T}A)}},\n$$\nwhere $\\lambda_{\\max}$ and $\\lambda_{\\min}$ are the largest and smallest eigenvalues of $A^{T}A$.\n\nCompute $A^{T}A$:\n$$\nA^{T} = \\begin{pmatrix} 1 & 0 \\\\ \\epsilon & 1 \\end{pmatrix}, \\quad\nA^{T}A = \\begin{pmatrix} 1 & \\epsilon \\\\ \\epsilon & 1 + \\epsilon^{2} \\end{pmatrix}.\n$$\nThe eigenvalues of $A^{T}A$ solve the characteristic equation:\n$$\n\\det\\!\\left(A^{T}A - \\lambda I\\right) = (1 - \\lambda)(1 + \\epsilon^{2} - \\lambda) - \\epsilon^{2} = 0.\n$$\nExpanding and rearranging into standard form gives:\n$$\n\\lambda^2 - (2 + \\epsilon^2)\\lambda + 1 = 0.\n$$\nNote that the product of the eigenvalues is $\\lambda_{\\max}\\lambda_{\\min} = 1$. This allows for a significant simplification of the condition number formula:\n$$\n\\kappa_{2}(A(\\epsilon)) = \\sqrt{\\frac{\\lambda_{\\max}}{\\lambda_{\\min}}} = \\sqrt{\\frac{\\lambda_{\\max}}{1/\\lambda_{\\max}}} = \\sqrt{\\lambda_{\\max}^2} = \\lambda_{\\max}.\n$$\nThus, the condition number is simply the largest eigenvalue of $A^T A$. We find this eigenvalue using the quadratic formula:\n$$\n\\lambda_{\\max} = \\frac{(2 + \\epsilon^{2}) + \\sqrt{(2 + \\epsilon^{2})^{2} - 4}}{2} = \\frac{2 + \\epsilon^{2} + \\sqrt{4\\epsilon^2 + \\epsilon^4}}{2} = \\frac{2 + \\epsilon^{2} + \\epsilon\\sqrt{4 + \\epsilon^{2}}}{2}.\n$$\nSimplifying this expression gives the final answer for the condition number:\n$$\n\\kappa_{2}(A(\\epsilon)) = 1 + \\frac{\\epsilon^{2}}{2} + \\frac{\\epsilon}{2}\\sqrt{\\epsilon^{2} + 4}.\n$$", "answer": "$$\\boxed{1 + \\frac{\\epsilon^{2}}{2} + \\frac{\\epsilon}{2}\\sqrt{\\epsilon^{2} + 4}}$$", "id": "2210773"}, {"introduction": "Our final practice bridges theory and application by situating the condition number within the context of optimal experimental design. You will investigate how different statistical optimality criteria (D-optimality and A-optimality) lead to different design choices and, consequently, different condition numbers for the design matrix. This advanced problem highlights the practical trade-offs faced by scientists and engineers, where optimizing for one performance metric can impact the numerical conditioning of the underlying model. [@problem_id:3540105]", "problem": "Consider linear least squares with two regressors, modeled as $y = X \\beta + \\varepsilon$, where $X \\in \\mathbb{R}^{2 \\times 2}$ is the design matrix and the information matrix is $M = X^{\\mathsf{T}} X$. Assume the two columns of $X$ are orthogonal and let the column costs be $c_1 > c_2 > 0$. You have a fixed design budget $B > 0$ that constrains the squared Euclidean norms of the columns $x_1$ and $x_2$ of $X$ by $c_1 \\|x_1\\|_2^2 + c_2 \\|x_2\\|_2^2 = B$. Adopt the standard definitions from optimal experimental design: determinant-optimality (D-optimality) is the choice of $X$ that maximizes $\\det(M)$ under the constraint, while trace-optimality (A-optimality) is the choice of $X$ that minimizes $\\operatorname{tr}(M^{-1})$ under the constraint. The $2$-norm condition number of $X$ is $\\kappa_2(X) = \\sigma_{\\max}(X) / \\sigma_{\\min}(X)$, where $\\sigma_{\\max}(X)$ and $\\sigma_{\\min}(X)$ are the largest and smallest singular values of $X$.\n\n(a) Construct explicit designs $X_{\\mathrm{D}}$ and $X_{\\mathrm{A}}$ satisfying the orthogonal-column structure and the budget constraint that achieve D-optimality and A-optimality, respectively.\n\n(b) Compute $\\kappa_2(X_{\\mathrm{D}})$ and $\\kappa_2(X_{\\mathrm{A}})$, and explain the trade-offs between determinant-optimality and trace-optimality in terms of how they affect the spectrum of $M$ and the conditioning of $X$.\n\nReport, as your final answer, the closed-form expression for the ratio $\\kappa_2(X_{\\mathrm{D}}) / \\kappa_2(X_{\\mathrm{A}})$ in terms of $c_1$ and $c_2$. No rounding is required.", "solution": "Let the two orthogonal columns of the matrix $X \\in \\mathbb{R}^{2 \\times 2}$ be denoted by $x_1$ and $x_2$. The orthogonality condition is $x_1^T x_2 = 0$. The information matrix $M$ is given by:\n$$\nM = X^T X = \\begin{pmatrix} x_1^T \\\\ x_2^T \\end{pmatrix} [x_1, x_2] = \\begin{pmatrix} x_1^T x_1 & x_1^T x_2 \\\\ x_2^T x_1 & x_2^T x_2 \\end{pmatrix} = \\begin{pmatrix} \\|x_1\\|_2^2 & 0 \\\\ 0 & \\|x_2\\|_2^2 \\end{pmatrix}\n$$\nLet us define $\\lambda_1 = \\|x_1\\|_2^2$ and $\\lambda_2 = \\|x_2\\|_2^2$. These quantities represent the eigenvalues of the information matrix $M$. The problem requires $\\lambda_1 > 0$ and $\\lambda_2 > 0$ for $M$ to be invertible. The budget constraint is given in terms of these eigenvalues:\n$$\nc_1 \\lambda_1 + c_2 \\lambda_2 = B\n$$\nwhere $c_1 > c_2 > 0$ and $B > 0$. Our task is to find the values of $\\lambda_1$ and $\\lambda_2$ that satisfy the specified optimality criteria and then construct the corresponding matrices $X$.\n\n(a) Construction of $X_{\\mathrm{D}}$ and $X_{\\mathrm{A}}$\n\n**D-Optimality**\n\nThe D-optimality criterion requires maximizing the determinant of the information matrix, $\\det(M)$, subject to the budget constraint.\n$$\n\\det(M) = \\det \\begin{pmatrix} \\lambda_1 & 0 \\\\ 0 & \\lambda_2 \\end{pmatrix} = \\lambda_1 \\lambda_2\n$$\nWe need to maximize the function $f(\\lambda_1, \\lambda_2) = \\lambda_1 \\lambda_2$ subject to $c_1 \\lambda_1 + c_2 \\lambda_2 = B$. We can express $\\lambda_2$ in terms of $\\lambda_1$ from the constraint: $\\lambda_2 = (B - c_1 \\lambda_1) / c_2$. Substituting this into the objective function gives a function of a single variable, $\\lambda_1$:\n$$\nf(\\lambda_1) = \\lambda_1 \\left( \\frac{B - c_1 \\lambda_1}{c_2} \\right) = \\frac{1}{c_2} (B \\lambda_1 - c_1 \\lambda_1^2)\n$$\nThis is a quadratic function of $\\lambda_1$ representing a downward-opening parabola. The maximum occurs at its vertex, which can be found by setting the first derivative to zero:\n$$\n\\frac{df}{d\\lambda_1} = \\frac{1}{c_2} (B - 2c_1 \\lambda_1) = 0\n$$\nThis yields $B - 2c_1 \\lambda_1 = 0$, so the D-optimal value for $\\lambda_1$ is:\n$$\n\\lambda_{1, \\mathrm{D}} = \\frac{B}{2c_1}\n$$\nSubstituting this back into the budget constraint to find $\\lambda_2$:\n$$\nc_1 \\left( \\frac{B}{2c_1} \\right) + c_2 \\lambda_{2, \\mathrm{D}} = B \\implies \\frac{B}{2} + c_2 \\lambda_{2, \\mathrm{D}} = B \\implies c_2 \\lambda_{2, \\mathrm{D}} = \\frac{B}{2}\n$$\nSo, the D-optimal value for $\\lambda_2$ is:\n$$\n\\lambda_{2, \\mathrm{D}} = \\frac{B}{2c_2}\n$$\nTo construct an explicit design matrix $X_{\\mathrm{D}}$, we can choose the columns to be scaled standard basis vectors, which are orthogonal.\n$$\nx_{1, \\mathrm{D}} = \\sqrt{\\lambda_{1, \\mathrm{D}}} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\sqrt{\\frac{B}{2c_1}} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\quad \\text{and} \\quad x_{2, \\mathrm{D}} = \\sqrt{\\lambda_{2, \\mathrm{D}}} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = \\sqrt{\\frac{B}{2c_2}} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}\n$$\nThus, the D-optimal design matrix is:\n$$\nX_{\\mathrm{D}} = \\begin{pmatrix} \\sqrt{\\frac{B}{2c_1}} & 0 \\\\ 0 & \\sqrt{\\frac{B}{2c_2}} \\end{pmatrix}\n$$\n\n**A-Optimality**\n\nThe A-optimality criterion requires minimizing the trace of the inverse of the information matrix, $\\operatorname{tr}(M^{-1})$, subject to the budget constraint.\n$$\nM^{-1} = \\begin{pmatrix} 1/\\lambda_1 & 0 \\\\ 0 & 1/\\lambda_2 \\end{pmatrix} \\implies \\operatorname{tr}(M^{-1}) = \\frac{1}{\\lambda_1} + \\frac{1}{\\lambda_2}\n$$\nWe use the method of Lagrange multipliers to minimize $g(\\lambda_1, \\lambda_2) = \\frac{1}{\\lambda_1} + \\frac{1}{\\lambda_2}$ subject to $c_1 \\lambda_1 + c_2 \\lambda_2 = B$. The Lagrangian is:\n$$\n\\mathcal{L}(\\lambda_1, \\lambda_2, \\mu) = \\frac{1}{\\lambda_1} + \\frac{1}{\\lambda_2} + \\mu(c_1 \\lambda_1 + c_2 \\lambda_2 - B)\n$$\nTaking partial derivatives with respect to $\\lambda_1$ and $\\lambda_2$ and setting them to zero:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\lambda_1} = -\\frac{1}{\\lambda_1^2} + \\mu c_1 = 0 \\implies \\lambda_1^2 = \\frac{1}{\\mu c_1} \\implies \\lambda_1 = \\frac{1}{\\sqrt{\\mu c_1}}\n$$\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\lambda_2} = -\\frac{1}{\\lambda_2^2} + \\mu c_2 = 0 \\implies \\lambda_2^2 = \\frac{1}{\\mu c_2} \\implies \\lambda_2 = \\frac{1}{\\sqrt{\\mu c_2}}\n$$\nFrom these two equations, we find the ratio:\n$$\n\\frac{\\lambda_1}{\\lambda_2} = \\frac{1/\\sqrt{\\mu c_1}}{1/\\sqrt{\\mu c_2}} = \\sqrt{\\frac{c_2}{c_1}} \\implies \\lambda_1 \\sqrt{c_1} = \\lambda_2 \\sqrt{c_2}\n$$\nNow, we use this relationship along with the budget constraint. Let $\\lambda_1 = k/\\sqrt{c_1}$ and $\\lambda_2 = k/\\sqrt{c_2}$ for some constant $k$. Substituting into the budget constraint:\n$$\nc_1 \\left( \\frac{k}{\\sqrt{c_1}} \\right) + c_2 \\left( \\frac{k}{\\sqrt{c_2}} \\right) = B \\implies k\\sqrt{c_1} + k\\sqrt{c_2} = B \\implies k(\\sqrt{c_1} + \\sqrt{c_2}) = B\n$$\nSo, $k = B / (\\sqrt{c_1} + \\sqrt{c_2})$. The A-optimal values for $\\lambda_1$ and $\\lambda_2$ are:\n$$\n\\lambda_{1, \\mathrm{A}} = \\frac{B}{\\sqrt{c_1}(\\sqrt{c_1} + \\sqrt{c_2})}\n$$\n$$\n\\lambda_{2, \\mathrm{A}} = \\frac{B}{\\sqrt{c_2}(\\sqrt{c_1} + \\sqrt{c_2})}\n$$\nAn explicit A-optimal design matrix $X_{\\mathrm{A}}$ can be constructed similarly:\n$$\nX_{\\mathrm{A}} = \\begin{pmatrix} \\sqrt{\\frac{B}{\\sqrt{c_1}(\\sqrt{c_1} + \\sqrt{c_2})}} & 0 \\\\ 0 & \\sqrt{\\frac{B}{\\sqrt{c_2}(\\sqrt{c_1} + \\sqrt{c_2})}} \\end{pmatrix}\n$$\n\n(b) Condition Numbers and Trade-offs\n\nThe singular values of $X$, $\\sigma_i(X)$, are related to the eigenvalues of $M = X^T X$ by $\\sigma_i(X) = \\sqrt{\\lambda_i(M)}$. In our case, the singular values of $X$ are $\\sqrt{\\lambda_1}$ and $\\sqrt{\\lambda_2}$. The $2$-norm condition number of $X$ is\n$$\n\\kappa_2(X) = \\frac{\\sigma_{\\max}(X)}{\\sigma_{\\min}(X)} = \\sqrt{\\frac{\\lambda_{\\max}}{\\lambda_{\\min}}}\n$$\nGiven $c_1 > c_2 > 0$.\n\nFor the D-optimal design:\n$\\lambda_{1, \\mathrm{D}} = \\frac{B}{2c_1}$ and $\\lambda_{2, \\mathrm{D}} = \\frac{B}{2c_2}$. Since $c_1 > c_2$, we have $\\lambda_{1, \\mathrm{D}} < \\lambda_{2, \\mathrm{D}}$.\nSo, $\\lambda_{\\min} = \\lambda_{1, \\mathrm{D}}$ and $\\lambda_{\\max} = \\lambda_{2, \\mathrm{D}}$.\n$$\n\\kappa_2(X_{\\mathrm{D}}) = \\sqrt{\\frac{\\lambda_{2, \\mathrm{D}}}{\\lambda_{1, \\mathrm{D}}}} = \\sqrt{\\frac{B/(2c_2)}{B/(2c_1)}} = \\sqrt{\\frac{c_1}{c_2}}\n$$\n\nFor the A-optimal design:\n$\\lambda_{1, \\mathrm{A}} = \\frac{B}{\\sqrt{c_1}(\\sqrt{c_1} + \\sqrt{c_2})}$ and $\\lambda_{2, \\mathrm{A}} = \\frac{B}{\\sqrt{c_2}(\\sqrt{c_1} + \\sqrt{c_2})}$.\nSince $c_1 > c_2$, we have $\\sqrt{c_1} > \\sqrt{c_2}$, which implies $\\sqrt{c_1}(\\sqrt{c_1} + \\sqrt{c_2}) > \\sqrt{c_2}(\\sqrt{c_1} + \\sqrt{c_2})$. Taking the reciprocal reverses the inequality, so $\\lambda_{1, \\mathrm{A}} < \\lambda_{2, \\mathrm{A}}$.\nSo, $\\lambda_{\\min} = \\lambda_{1, \\mathrm{A}}$ and $\\lambda_{\\max} = \\lambda_{2, \\mathrm{A}}$.\n$$\n\\kappa_2(X_{\\mathrm{A}}) = \\sqrt{\\frac{\\lambda_{2, \\mathrm{A}}}{\\lambda_{1, \\mathrm{A}}}} = \\sqrt{\\frac{B/(\\sqrt{c_2}(\\sqrt{c_1}+\\sqrt{c_2}))}{B/(\\sqrt{c_1}(\\sqrt{c_1}+\\sqrt{c_2}))}} = \\sqrt{\\frac{\\sqrt{c_1}}{\\sqrt{c_2}}} = \\left(\\frac{c_1}{c_2}\\right)^{1/4}\n$$\n\n**Trade-offs:**\nThe spectrum of the information matrix $M$ consists of its eigenvalues $\\{\\lambda_1, \\lambda_2\\}$. The conditioning of $X$ is determined by the ratio of these eigenvalues.\n- **Spectrum:** For D-optimality, the ratio of eigenvalues is $\\frac{\\lambda_{\\max}}{\\lambda_{\\min}} = \\frac{c_1}{c_2}$. For A-optimality, this ratio is $\\frac{\\lambda_{\\max}}{\\lambda_{\\min}} = \\sqrt{\\frac{c_1}{c_2}}$. Since $c_1 > c_2$, we have $\\frac{c_1}{c_2} > \\sqrt{\\frac{c_1}{c_2}} > 1$. This shows that the A-optimal design produces eigenvalues for $M$ that are closer together (more balanced) than the D-optimal design. D-optimality allocates the budget such that the cost-weighted norms are equal ($c_1\\lambda_1 = c_2\\lambda_2 = B/2$), which puts more \"energy\" (squared norm) into the column with the lower cost ($c_2$). This spreads the eigenvalues apart. A-optimality seeks to avoid very small eigenvalues (since $1/\\lambda_i$ would blow up), which forces a more even distribution of energy and pulls the eigenvalues closer.\n- **Conditioning:** The condition number reflects the sensitivity of the least squares solution to perturbations. Since $\\left(\\frac{c_1}{c_2}\\right)^{1/2} > \\left(\\frac{c_1}{c_2}\\right)^{1/4}$, we have $\\kappa_2(X_{\\mathrm{D}}) > \\kappa_2(X_{\\mathrm{A}})$. The A-optimal design results in a better-conditioned matrix $X$ than the D-optimal design.\n- **The Trade-off:** D-optimality maximizes $\\det(M)$, which corresponds to minimizing the volume of the confidence ellipsoid for the parameter estimates $\\beta$. It focuses on overall \"informational volume\". In contrast, A-optimality minimizes $\\operatorname{tr}(M^{-1})$, which is proportional to the sum of the variances of the parameter estimates. This criterion is more sensitive to small eigenvalues and thus leads to a more robust design with better conditioning, at the cost of a smaller determinant (informational volume) compared to the D-optimal design.\n\nFinally, we compute the required ratio of the condition numbers.\n$$\n\\frac{\\kappa_2(X_{\\mathrm{D}})}{\\kappa_2(X_{\\mathrm{A}})} = \\frac{\\sqrt{c_1/c_2}}{(c_1/c_2)^{1/4}} = \\frac{(c_1/c_2)^{1/2}}{(c_1/c_2)^{1/4}} = \\left(\\frac{c_1}{c_2}\\right)^{1/2 - 1/4} = \\left(\\frac{c_1}{c_2}\\right)^{1/4}\n$$", "answer": "$$\n\\boxed{\\left(\\frac{c_1}{c_2}\\right)^{1/4}}\n$$", "id": "3540105"}]}