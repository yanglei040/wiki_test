{"hands_on_practices": [{"introduction": "In numerical computing, even familiar formulas like the quadratic equation can produce highly inaccurate results due to the limitations of floating-point arithmetic. This exercise confronts the issue of catastrophic cancellation, a phenomenon where subtracting two nearly equal numbers leads to a disastrous loss of relative precision. By implementing both a naive and a stabilized formula, you will gain practical skills in identifying numerical instabilities and reformulating expressions to preserve accuracy [@problem_id:2370392].", "problem": "A monic quadratic equation with real coefficients has the form $a x^2 + b x + c = 0$ with $a \\neq 0$. The exact real roots exist when the discriminant $D = b^2 - 4 a c \\ge 0$. In floating-point (FP) arithmetic, direct evaluation of the quadratic formula can suffer catastrophic cancellation when $b^2 \\gg 4 a c$, because one root is obtained by subtracting two nearly equal numbers. This problem asks you to quantify absolute and relative error for that cancellation-prone root and to mitigate the error by an algebraically equivalent reformulation derived from first principles.\n\nFundamental bases you may use:\n- Vieta’s formulas for quadratic equations: if $r_1$ and $r_2$ are the exact roots, then $r_1 + r_2 = -\\dfrac{b}{a}$ and $r_1 r_2 = \\dfrac{c}{a}$.\n- The quadratic formula expressing the exact roots in real arithmetic when $D \\ge 0$.\n- The definitions of absolute error and relative error. For an approximation $\\tilde{x}$ to a nonzero exact value $x$, the absolute error is $|\\tilde{x} - x|$ and the relative error is $\\dfrac{|\\tilde{x} - x|}{|x|}$. If $x = 0$, define the relative error to be the absolute error.\n\nYour tasks:\n1. For each test case below, compute both floating-point approximations to the two roots by directly applying the quadratic formula in standard double precision (that is, evaluate $\\dfrac{-b \\pm \\sqrt{b^2 - 4 a c}}{2 a}$ in the most straightforward way). Identify the cancellation-prone root as follows: if $b > 0$, then the cancellation-prone root is computed using the $+$ sign; if $b < 0$, it is computed using the $-$ sign. This identification reflects which formula subtracts two nearly equal quantities when $b^2 \\gg 4 a c$.\n2. Starting strictly from Vieta’s formulas and basic algebra, derive a numerically stable reformulation that avoids subtracting nearly equal numbers for the cancellation-prone root, and implement it. Your implementation must not rely on any “given” stabilized formula; it must follow from your derivation. You may, however, compute the other root in any stable manner implied by your derivation and then recover the cancellation-prone root using $r_1 r_2 = \\dfrac{c}{a}$.\n3. For ground truth, compute high-precision approximations to the exact roots using real arithmetic with at least $80$ decimal digits of precision, then cast to double precision for comparison. For each test case, choose the exact root that corresponds to the cancellation-prone one (the one of smaller magnitude when $b^2 \\gg 4 a c$) and compute both its absolute error and its relative error for the naive evaluation (Task $1$) and for your stable reformulation (Task $2$).\n4. Your program must aggregate the results for all test cases into a single flat list of floating-point numbers in the order, per test case: $[\\text{abs\\_err\\_naive}, \\text{rel\\_err\\_naive}, \\text{abs\\_err\\_stable}, \\text{rel\\_err\\_stable}]$, concatenated across test cases in the same order as listed below.\n\nTest suite (each triplet is $(a,b,c)$):\n- Case $1$: $(a,b,c) = (\\,1\\,,\\,10^8\\,,\\,1\\,)$.\n- Case $2$: $(a,b,c) = (\\,1\\,,\\,-10^8\\,,\\,1\\,)$.\n- Case $3$: $(a,b,c) = (\\,1\\,,\\,3\\,,\\,10^{-3}\\,)$.\n- Case $4$: $(a,b,c) = (\\,10^{-3}\\,,\\,10^5\\,,\\,10^{-3}\\,)$.\n\nNumerical and output requirements:\n- Use radians for any angular computations if they arise, although none are expected here.\n- No physical units are involved.\n- All outputs must be raw numbers; do not use a percentage sign. Relative error must be a pure number (for example, output $0.001$ for one-tenth of one percent).\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[r_1,r_2,\\dots]$). The final list must contain exactly $\\;16\\;$ floating-point numbers corresponding to the $\\;4\\;$ values per test case over $\\;4\\;$ cases, in the same order as the test suite.", "solution": "The problem presented is a classical exercise in numerical analysis, concerning the loss of precision when solving a quadratic equation $a x^2 + b x + c = 0$ using floating-point arithmetic. The analysis of the problem's validity confirms it is scientifically grounded, well-posed, and objective. We shall proceed with a rigorous solution.\n\nThe standard quadratic formula provides the two exact roots, $r_1$ and $r_2$, as:\n$$\nr_{1,2} = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}\n$$\nThe problem focuses on the condition where $b^2 \\gg 4ac$. In this regime, the discriminant $D = b^2 - 4ac$ is dominated by the $b^2$ term. Consequently, the square root $\\sqrt{b^2 - 4ac}$ can be approximated by a Taylor series expansion:\n$$\n\\sqrt{b^2 - 4ac} = |b| \\sqrt{1 - \\frac{4ac}{b^2}} \\approx |b| \\left(1 - \\frac{2ac}{b^2}\\right) = |b| - \\frac{2ac}{|b|}\n$$\nThis approximation reveals that $\\sqrt{b^2 - 4ac}$ is a quantity very close to $|b|$.\n\nThis proximity is the source of numerical instability. The phenomenon is known as catastrophic cancellation.\n\\begin{itemize}\n    \\item If $b > 0$, the numerator for one of the roots is $-b + \\sqrt{b^2-4ac}$. Since $\\sqrt{b^2-4ac} \\approx b$, this expression involves the subtraction of two nearly equal numbers. This operation results in a significant loss of relative precision in the computed result. The root affected is $r_1 = \\frac{-b + \\sqrt{b^2-4ac}}{2a}$.\n    \\item If $b < 0$, then $-b$ is a positive quantity. The term $\\sqrt{b^2-4ac} \\approx \\sqrt{b^2} = |b| = -b$. The numerator for one root becomes $-b - \\sqrt{b^2-4ac}$, which is again a subtraction of two nearly equal positive numbers (e.g., if $b=-10^8$, $-b=10^8$ and $\\sqrt{D} \\approx 10^8$). This affects the root $r_2 = \\frac{-b - \\sqrt{b^2-4ac}}{2a}$.\n\\end{itemize}\nIn both cases, the root with the smaller magnitude is the one susceptible to this catastrophic cancellation. The other root, which involves an addition of terms of the same sign ($-b$ and $-\\sqrt{D}$ if $b>0$, or $-b$ and $+\\sqrt{D}$ if $b<0$), is computed stably.\n\nTo mitigate this numerical error, we must derive an alternative formulation. The problem requires this derivation to start from first principles, specifically Vieta's formulas. The strategy is to first compute the \"stable\" root accurately and then use it to find the \"unstable\" (cancellation-prone) root.\n\nLet $r_{\\text{stable}}$ be the root that is computed without subtractive cancellation. It can be expressed in a unified manner using the sign function, $\\text{sgn}(b)$:\n$$\nr_{\\text{stable}} = \\frac{-b - \\text{sgn}(b)\\sqrt{b^2-4ac}}{2a}\n$$\nThis formula ensures that the two terms in the numerator, $-b$ and $-\\text{sgn}(b)\\sqrt{D}$, have the same sign, and their sum is computed robustly. For the non-trivial cases where $b^2 \\gg 4ac$, $b$ is non-zero, so $\\text{sgn}(b)$ is either $1$ or $-1$.\n\nHaving obtained an accurate value for $r_{\\text{stable}}$, we employ Vieta's formula for the product of roots, $r_1 r_2 = c/a$. If $r_{\\text{prone}}$ is the other root (the one susceptible to cancellation), then:\n$$\nr_{\\text{prone}} \\cdot r_{\\text{stable}} = \\frac{c}{a}\n$$\nFrom this, we can solve for $r_{\\text{prone}}$:\n$$\nr_{\\text{prone}} = \\frac{c/a}{r_{\\text{stable}}} = \\frac{c}{a \\cdot r_{\\text{stable}}}\n$$\nThis expression for $r_{\\text{prone}}$ involves only division and multiplication, which are numerically stable operations in this context, thus avoiding the catastrophic cancellation inherent in the naive formula.\n\nThe computational procedure to solve the problem is as follows:\n\\begin{enumerate}\n    \\item For each test case $(a, b, c)$, we first establish a ground truth value for the cancellation-prone root. This is achieved by computing the roots using the standard quadratic formula but with high-precision arithmetic (at least $80$ decimal digits, using Python's `decimal` module). The root corresponding to the subtraction of nearly equal terms is identified as the exact value, $x_{\\text{exact}}$, and cast to a standard double-precision float.\n\n    \\item We compute the cancellation-prone root, $\\tilde{x}_{\\text{naive}}$, using the direct quadratic formula in standard double-precision floating-point arithmetic.\n\n    \\item We compute the cancellation-prone root, $\\tilde{x}_{\\text{stable}}$, using the derived stable method. First, the stable root $\\tilde{r}_{\\text{stable}}$ is computed. Then, the prone root is found via $\\tilde{x}_{\\text{stable}} = (c/a) / \\tilde{r}_{\\text{stable}}$.\n\n    \\item For both the naive and stable methods, we calculate the absolute error, $|\\tilde{x} - x_{\\text{exact}}|$, and the relative error, $|\\tilde{x} - x_{\\text{exact}}| / |x_{\\text{exact}}|$. If $x_{\\text{exact}}=0$, the relative error is taken to be the absolute error.\n\\end{enumerate}\nThese four error metrics are then collected for each test case and aggregated into a single list for the final output.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom decimal import Decimal, getcontext\n\ndef solve():\n    \"\"\"\n    Solves for the roots of a quadratic equation using naive and stable methods,\n    and calculates the absolute and relative errors for the cancellation-prone root.\n    \"\"\"\n    # Set the precision for the decimal module to 85, which is more than the required 80 digits,\n    # to ensure accuracy of the ground truth calculations.\n    getcontext().prec = 85\n\n    # Define the test cases from the problem statement. Each tuple is (a, b, c).\n    test_cases = [\n        (1.0, 1e8, 1.0),\n        (1.0, -1e8, 1.0),\n        (1.0, 3.0, 1e-3),\n        (1e-3, 1e5, 1e-3),\n    ]\n\n    all_results = []\n    for a, b, c in test_cases:\n        # Use standard double-precision floats for coefficients in FP calculations\n        a_fp, b_fp, c_fp = float(a), float(b), float(c)\n\n        # Task 3: Ground truth using high-precision arithmetic\n        a_d, b_d, c_d = Decimal(a), Decimal(b), Decimal(c)\n        discriminant_d = (b_d**2 - 4 * a_d * c_d).sqrt()\n\n        # Identify the exact cancellation-prone root based on the sign of b.\n        # This is the root with the smaller magnitude.\n        if b_fp > 0:\n            # The root from (-b + sqrt(D)) suffers cancellation\n            x_exact_d = (-b_d + discriminant_d) / (2 * a_d)\n        else: # b = 0\n            # The root from (-b - sqrt(D)) suffers cancellation\n            x_exact_d = (-b_d - discriminant_d) / (2 * a_d)\n        \n        # Cast the high-precision ground truth to a standard float for comparison\n        x_exact = float(x_exact_d)\n\n        # Task 1: Naive evaluation using the standard quadratic formula in double precision\n        discriminant_fp = np.sqrt(b_fp**2 - 4 * a_fp * c_fp)\n        \n        # Identify the cancellation-prone root from the naive calculation\n        if b_fp > 0:\n            x_naive_prone = (-b_fp + discriminant_fp) / (2 * a_fp)\n        else:\n            x_naive_prone = (-b_fp - discriminant_fp) / (2 * a_fp)\n\n        # Task 2: Stable reformulation derived from first principles\n        # First, compute the stable root (the one without subtractive cancellation)\n        # np.copysign is robust and correctly applies the sign of b.\n        # It's equivalent to sgn(b) for b!=0.\n        stable_root = (-b_fp - np.copysign(1.0, b_fp) * discriminant_fp) / (2 * a_fp)\n        \n        # Then, use Vieta's formula (r1 * r2 = c/a) to find the cancellation-prone root.\n        # This avoids the subtraction of nearly equal numbers.\n        if stable_root == 0:\n            # Handle the unlikely case of the stable root being exactly zero\n            # This would imply c=0, in which case the prone root is also 0.\n            x_stable_prone = 0.0\n        else:\n            x_stable_prone = (c_fp / a_fp) / stable_root\n\n        # Task 4: Compute absolute and relative errors for both methods\n        # Naive method errors\n        abs_err_naive = abs(x_naive_prone - x_exact)\n        # Per problem spec, if x_exact is 0, relative error is absolute error\n        if x_exact != 0:\n            rel_err_naive = abs_err_naive / abs(x_exact)\n        else:\n            rel_err_naive = abs_err_naive\n        \n        # Stable method errors\n        abs_err_stable = abs(x_stable_prone - x_exact)\n        if x_exact != 0:\n            rel_err_stable = abs_err_stable / abs(x_exact)\n        else:\n            rel_err_stable = abs_err_stable\n\n        # Aggregate the results for this test case\n        all_results.extend([abs_err_naive, rel_err_naive, abs_err_stable, rel_err_stable])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.17e}' for r in all_results)}]\")\n\nsolve()\n```", "id": "2370392"}, {"introduction": "Beyond algorithmic flaws, the intrinsic properties of a problem can dictate the reliability of its solution. This exercise explores ill-conditioned linear systems, where the coefficient matrix $A$ is nearly singular. You will investigate a scenario where a computed solution $\\hat{x}$ yields a very small relative residual $\\frac{\\|b - A\\hat{x}\\|_{2}}{\\|b\\|_{2}}$, yet is far from the true solution $x^{\\star}$, revealing the deceptive nature of residuals in the face of ill-conditioning [@problem_id:3574233].", "problem": "Consider a linear system in the setting of numerical linear algebra with a nearly rank-deficient coefficient matrix. Let\n$$\nA \\;=\\; \\begin{pmatrix}\n1  1 \\\\\n1  1 + 10^{-8}\n\\end{pmatrix}, \\qquad x^{\\star} \\;=\\; \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}, \\qquad b \\;=\\; A x^{\\star}.\n$$\nSuppose that a computed solution is produced by moving along a near-null direction of $A$, specifically\n$$\n\\hat{x} \\;=\\; x^{\\star} + s\\,v, \\qquad v \\;=\\; \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}, \\qquad s \\;=\\; 10^{3}.\n$$\nUsing only the core definitions of residual and error, define the relative residual\n$$\n\\mathrm{rr} \\;=\\; \\frac{\\|b - A\\hat{x}\\|_{2}}{\\|b\\|_{2}},\n$$\nand the relative solution error\n$$\n\\mathrm{re} \\;=\\; \\frac{\\|\\hat{x} - x^{\\star}\\|_{2}}{\\|x^{\\star}\\|_{2}}.\n$$\nCompute both quantities and then compute the amplification factor\n$$\n\\alpha \\;=\\; \\frac{\\mathrm{re}}{\\mathrm{rr}}.\n$$\nFinally, briefly explain the geometric reason, in terms of near-null directions of $A$, why $\\mathrm{rr}$ can be small while $\\mathrm{re}$ is large in this construction. Report the single numerical value of $\\alpha$ and round your answer to four significant figures.", "solution": "The problem statement is evaluated to be valid. It is scientifically grounded in numerical linear algebra, well-posed with all necessary information provided, and expressed in objective, formal language. We may proceed with the solution.\n\nThe task is to compute the relative residual $\\mathrm{rr}$, the relative solution error $\\mathrm{re}$, and the amplification factor $\\alpha = \\mathrm{re}/\\mathrm{rr}$ for a given linear system, and to provide a geometric explanation for the results.\n\nFirst, we list the given quantities:\nThe matrix $A$ is\n$$ A = \\begin{pmatrix} 1  1 \\\\ 1  1 + 10^{-8} \\end{pmatrix} $$\nThe exact solution $x^{\\star}$ is\n$$ x^{\\star} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} $$\nThe right-hand side vector $b$ is defined as $b = A x^{\\star}$. Let us compute $b$:\n$$ b = \\begin{pmatrix} 1  1 \\\\ 1  1 + 10^{-8} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\cdot 1 + 1 \\cdot 1 \\\\ 1 \\cdot 1 + (1 + 10^{-8}) \\cdot 1 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 2 + 10^{-8} \\end{pmatrix} $$\nThe computed solution $\\hat{x}$ is given by\n$$ \\hat{x} = x^{\\star} + s\\,v $$\nwhere $s = 10^3$ and $v = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$.\n$$ \\hat{x} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} + 10^3 \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 1 + 1000 \\\\ 1 - 1000 \\end{pmatrix} = \\begin{pmatrix} 1001 \\\\ -999 \\end{pmatrix} $$\n\nNext, we compute the relative solution error, $\\mathrm{re} = \\frac{\\|\\hat{x} - x^{\\star}\\|_{2}}{\\|x^{\\star}\\|_{2}}$.\nThe error vector is $\\hat{x} - x^{\\star} = s\\,v = 10^3 \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 1000 \\\\ -1000 \\end{pmatrix}$.\nThe Euclidean norm of the error vector is:\n$$ \\|\\hat{x} - x^{\\star}\\|_{2} = \\|s\\,v\\|_{2} = |s| \\cdot \\|v\\|_{2} = 10^3 \\sqrt{1^2 + (-1)^2} = 10^3 \\sqrt{2} $$\nThe Euclidean norm of the true solution is:\n$$ \\|x^{\\star}\\|_{2} = \\sqrt{1^2 + 1^2} = \\sqrt{2} $$\nThe relative solution error is therefore:\n$$ \\mathrm{re} = \\frac{10^3 \\sqrt{2}}{\\sqrt{2}} = 10^3 $$\n\nNow, we compute the relative residual, $\\mathrm{rr} = \\frac{\\|b - A\\hat{x}\\|_{2}}{\\|b\\|_{2}}$.\nFirst, we find the residual vector, $r = b - A\\hat{x}$. Using the definitions $b = A x^{\\star}$ and $\\hat{x} = x^{\\star} + s\\,v$:\n$$ r = A x^{\\star} - A(x^{\\star} + s\\,v) = A x^{\\star} - A x^{\\star} - s(Av) = -s(Av) $$\nLet's compute the vector $Av$:\n$$ Av = \\begin{pmatrix} 1  1 \\\\ 1  1 + 10^{-8} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 1 \\cdot 1 + 1 \\cdot (-1) \\\\ 1 \\cdot 1 + (1 + 10^{-8}) \\cdot (-1) \\end{pmatrix} = \\begin{pmatrix} 1 - 1 \\\\ 1 - 1 - 10^{-8} \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ -10^{-8} \\end{pmatrix} $$\nNow, we can find the residual vector $r$:\n$$ r = -s(Av) = -10^3 \\begin{pmatrix} 0 \\\\ -10^{-8} \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 10^{-5} \\end{pmatrix} $$\nThe Euclidean norm of the residual vector is:\n$$ \\|b - A\\hat{x}\\|_{2} = \\|r\\|_{2} = \\sqrt{0^2 + (10^{-5})^2} = 10^{-5} $$\nNext, we need the Euclidean norm of $b$:\n$$ \\|b\\|_{2} = \\left\\| \\begin{pmatrix} 2 \\\\ 2 + 10^{-8} \\end{pmatrix} \\right\\|_{2} = \\sqrt{2^2 + (2 + 10^{-8})^2} = \\sqrt{4 + 4 + 4 \\cdot 10^{-8} + 10^{-16}} = \\sqrt{8 + 4 \\cdot 10^{-8} + 10^{-16}} $$\nThe relative residual is:\n$$ \\mathrm{rr} = \\frac{10^{-5}}{\\sqrt{8 + 4 \\cdot 10^{-8} + 10^{-16}}} $$\n\nFinally, we compute the amplification factor $\\alpha$:\n$$ \\alpha = \\frac{\\mathrm{re}}{\\mathrm{rr}} = \\frac{10^3}{\\frac{10^{-5}}{\\sqrt{8 + 4 \\cdot 10^{-8} + 10^{-16}}}} = 10^3 \\cdot 10^5 \\sqrt{8 + 4 \\cdot 10^{-8} + 10^{-16}} $$\n$$ \\alpha = 10^8 \\sqrt{8 + 4 \\cdot 10^{-8} + 10^{-16}} $$\nThe terms $4 \\cdot 10^{-8}$ and $10^{-16}$ are very small compared to $8$. For the purpose of numerical evaluation, we have:\n$$ \\alpha = 10^8 \\sqrt{8.000000040000001} \\approx 10^8 \\cdot 2.8284271284 $$\n$$ \\alpha \\approx 2.8284271284 \\times 10^8 $$\nRounding to four significant figures, we get:\n$$ \\alpha \\approx 2.828 \\times 10^8 $$\n\nGeometric explanation:\nThe reason for the large amplification factor $\\alpha$ lies in the properties of the matrix $A$. $A$ is nearly singular (or ill-conditioned), which means it has a \"near-null direction.\" This is a direction in the domain space that is mapped to a vector of very small magnitude in the codomain. In this problem, the vector $v = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$ represents this near-null direction. As we calculated, $Av = \\begin{pmatrix} 0 \\\\ -10^{-8} \\end{pmatrix}$, which has a very small norm, $\\|Av\\|_2 = 10^{-8}$, compared to the norm of $v$, which is $\\|v\\|_2 = \\sqrt{2}$.\n\nThe error in the solution, $\\hat{x} - x^{\\star} = s\\,v$, is a large vector since it is a large multiple ($s=10^3$) of $v$. This leads to a large relative error, $\\mathrm{re} = 10^3$.\n\nHowever, the residual vector, $r = b - A\\hat{x}$, is equal to $-A(\\hat{x} - x^{\\star}) = -s(Av)$. Because the matrix-vector product $Av$ is very small, the residual vector $r$ has a small norm, $\\|r\\|_2 = |s| \\|Av\\|_2 = 10^3 \\cdot 10^{-8} = 10^{-5}$. The right-hand side vector $b$ has a norm of approximately $\\|b\\|_2 \\approx \\sqrt{8} \\approx 2.828$. This results in a very small relative residual $\\mathrm{rr} \\approx 10^{-5}/2.828$, which is on the order of $10^{-6}$.\n\nGeometrically, the linear transformation defined by $A$ severely \"squashes\" vectors that lie in the direction of its near-null space. The error vector $\\hat{x} - x^{\\star}$ was specifically constructed to lie in this direction. Consequently, a very large error in the input space (the solution space) is mapped to a very small vector in the output space. This small output vector is the difference between $A\\hat{x}$ and $b$, which is the residual. Therefore, a small residual does not guarantee a small solution error when the matrix is ill-conditioned. The amplification factor $\\alpha$ quantifies this phenomenon; in this case, the solution error is over $10^8$ times larger than the residual error. This factor is related to the condition number of the matrix $A$.", "answer": "$$\n\\boxed{2.828 \\times 10^8}\n$$", "id": "3574233"}, {"introduction": "To build a robust understanding of numerical error, we must formalize the connection between an algorithm's stability and a problem's sensitivity. This practice guides you through a theoretical construction that demonstrates the sharpness of the fundamental error bound in linear algebra. By engineering a specific case, you will prove how a backward-stable method can still produce a large forward error, with the amplification factor being precisely the matrix's condition number [@problem_id:3574243].", "problem": "Let $u \\in (0,10^{-1})$ denote the unit roundoff of a hypothetical floating-point machine. For each $u$, define a nonsingular, parameterized $2 \\times 2$ matrix\n$$\nA(\\varepsilon) \\;=\\; \\begin{pmatrix} 1  1 \\\\ 1  1+\\varepsilon \\end{pmatrix}, \\qquad \\varepsilon  0.\n$$\nWork throughout with the spectral (two) norm. Recall the following fundamental definitions:\n- The spectral condition number is $\\kappa_{2}(A) = \\|A\\|_{2}\\,\\|A^{-1}\\|_{2} = \\sigma_{\\max}(A)/\\sigma_{\\min}(A)$, where $\\sigma_{\\max}(A)$ and $\\sigma_{\\min}(A)$ are the largest and smallest singular values of $A$.\n- Given the exact solution $x$ to $A x = b$ and an approximate solution $\\hat{x}$, the forward relative error is $\\|\\hat{x}-x\\|_{2}/\\|x\\|_{2}$. The residual is $r = b - A \\hat{x}$, with relative residual $\\|r\\|_{2}/\\|b\\|_{2}$.\n- A backward-stable solve with respect to the right-hand side is modeled as producing $\\hat{x}$ that solves $A \\hat{x} = b + \\Delta b$ with $\\|\\Delta b\\|_{2}/\\|b\\|_{2} \\le c\\,u$ for a modest constant $c$ independent of problem conditioning.\n\nConsider the following construction that depends on $u$:\n- Set the parameter to $\\varepsilon(u) = 4u$ and the matrix to $A(u) := A(\\varepsilon(u))$.\n- Let $v_{\\max}(u)$ and $u_{\\min}(u)$ be unit right and left singular vectors corresponding to $\\sigma_{\\max}(A(u))$ and $\\sigma_{\\min}(A(u))$, respectively.\n- Define the exact solution $x(u) := v_{\\max}(u)$ and the right-hand side $b(u) := A(u)\\,x(u)$.\n- Let the computed solution $\\hat{x}(u)$ be the exact solution of the perturbed system $A(u)\\,\\hat{x}(u) = b(u) + \\Delta b(u)$ with a backward perturbation chosen as $\\Delta b(u) = u\\,\\|b(u)\\|_{2}\\,u_{\\min}(u)$.\n\nStarting only from these definitions and standard singular value properties, carry out the following tasks:\n- Show that $A(\\varepsilon)$ is nonsingular for all $\\varepsilon0$ and that $\\kappa_{2}(A(\\varepsilon)) \\to \\infty$ as $\\varepsilon \\to 0^{+}$.\n- Prove that the relative residual satisfies $\\|r(u)\\|_{2}/\\|b(u)\\|_{2} = u$, where $r(u) = b(u) - A(u)\\hat{x}(u)$.\n- Derive an explicit expression for the forward relative error $\\|\\hat{x}(u) - x(u)\\|_{2}/\\|x(u)\\|_{2}$ in terms of $\\kappa_{2}(A(u))$ and $u$, and then specialize it using $\\varepsilon(u)=4u$.\n- Compute the limit, as $u \\to 0^{+}$, of the forward relative error $\\|\\hat{x}(u) - x(u)\\|_{2}/\\|x(u)\\|_{2}$.\n\nYour final answer must be a single real number. No rounding is required.", "solution": "The problem is well-posed and scientifically grounded in numerical linear algebra. We proceed to solve it by addressing the four tasks in order.\n\nFirst, we analyze the properties of the matrix $A(\\varepsilon) = \\begin{pmatrix} 1  1 \\\\ 1  1+\\varepsilon \\end{pmatrix}$ for $\\varepsilon  0$.\nThe determinant of $A(\\varepsilon)$ is given by $\\det(A(\\varepsilon)) = (1)(1+\\varepsilon) - (1)(1) = \\varepsilon$. Since it is given that $\\varepsilon  0$, the determinant is non-zero, which proves that $A(\\varepsilon)$ is nonsingular for all $\\varepsilon  0$.\n\nNext, we investigate the behavior of the spectral condition number $\\kappa_{2}(A(\\varepsilon))$ as $\\varepsilon \\to 0^{+}$. The condition number is defined as the ratio of the largest to the smallest singular values, $\\kappa_{2}(A) = \\sigma_{\\max}(A)/\\sigma_{\\min}(A)$. The singular values are the square roots of the eigenvalues of the matrix $A(\\varepsilon)^T A(\\varepsilon)$.\n$$\nA(\\varepsilon)^T A(\\varepsilon) = \\begin{pmatrix} 1  1 \\\\ 1  1+\\varepsilon \\end{pmatrix} \\begin{pmatrix} 1  1 \\\\ 1  1+\\varepsilon \\end{pmatrix} = \\begin{pmatrix} 1^2+1^2  1 \\cdot 1 + 1 \\cdot (1+\\varepsilon) \\\\ 1 \\cdot 1 + (1+\\varepsilon) \\cdot 1  1^2 + (1+\\varepsilon)^2 \\end{pmatrix} = \\begin{pmatrix} 2  2+\\varepsilon \\\\ 2+\\varepsilon  2+2\\varepsilon+\\varepsilon^2 \\end{pmatrix}.\n$$\nThe eigenvalues $\\lambda$ of $A^T A$ are the roots of the characteristic polynomial $\\det(A^T A - \\lambda I) = 0$.\n$$\n(2-\\lambda)(2+2\\varepsilon+\\varepsilon^2-\\lambda) - (2+\\varepsilon)^2 = 0\n$$\n$$\n\\lambda^2 - (2+2+2\\varepsilon+\\varepsilon^2)\\lambda + 2(2+2\\varepsilon+\\varepsilon^2) - (4+4\\varepsilon+\\varepsilon^2) = 0\n$$\n$$\n\\lambda^2 - (4+2\\varepsilon+\\varepsilon^2)\\lambda + (4+4\\varepsilon+2\\varepsilon^2) - (4+4\\varepsilon+\\varepsilon^2) = 0\n$$\n$$\n\\lambda^2 - (4+2\\varepsilon+\\varepsilon^2)\\lambda + \\varepsilon^2 = 0\n$$\nThe two eigenvalues, $\\lambda_{\\max}$ and $\\lambda_{\\min}$, are the roots of this quadratic equation. The singular values are $\\sigma_{\\max} = \\sqrt{\\lambda_{\\max}}$ and $\\sigma_{\\min} = \\sqrt{\\lambda_{\\min}}$. The square of the condition number is $\\kappa_2(A(\\varepsilon))^2 = \\sigma_{\\max}^2 / \\sigma_{\\min}^2 = \\lambda_{\\max} / \\lambda_{\\min}$.\nFrom Vieta's formulas, $\\lambda_{\\max} \\lambda_{\\min} = \\varepsilon^2$ and $\\lambda_{\\max} + \\lambda_{\\min} = 4+2\\varepsilon+\\varepsilon^2$.\nAs $\\varepsilon \\to 0^{+}$, the sum of the roots approaches $4$ and their product approaches $0$. This implies one root approaches $4$ and the other approaches $0$. Let $\\lambda_{\\max} \\to 4$ and $\\lambda_{\\min} \\to 0$.\nMore precisely, $\\lambda_{\\min} = \\frac{\\varepsilon^2}{\\lambda_{\\max}}$. As $\\varepsilon \\to 0^{+}$, $\\lambda_{\\max} \\approx 4$, so $\\lambda_{\\min} \\approx \\varepsilon^2/4$.\nThe condition number $\\kappa_2(A(\\varepsilon)) = \\sqrt{\\lambda_{\\max}/\\lambda_{\\min}} \\approx \\sqrt{4/(\\varepsilon^2/4)} = \\sqrt{16/\\varepsilon^2} = 4/\\varepsilon$.\nTherefore, $\\lim_{\\varepsilon \\to 0^+} \\kappa_2(A(\\varepsilon)) = \\lim_{\\varepsilon \\to 0^+} \\frac{4}{\\varepsilon} = \\infty$. This completes the first task.\n\nSecond, we prove that the relative residual $\\|r(u)\\|_{2}/\\|b(u)\\|_{2} = u$.\nThe residual is defined as $r(u) = b(u) - A(u)\\hat{x}(u)$. The computed solution $\\hat{x}(u)$ is defined to be the exact solution to the perturbed system $A(u)\\hat{x}(u) = b(u) + \\Delta b(u)$.\nSubstituting this into the definition of the residual, we get:\n$$\nr(u) = b(u) - (b(u) + \\Delta b(u)) = -\\Delta b(u).\n$$\nThe problem specifies the perturbation as $\\Delta b(u) = u\\,\\|b(u)\\|_{2}\\,u_{\\min}(u)$.\nTaking the spectral norm of the residual vector:\n$$\n\\|r(u)\\|_{2} = \\|-\\Delta b(u)\\|_{2} = \\|\\Delta b(u)\\|_{2} = \\|u\\,\\|b(u)\\|_{2}\\,u_{\\min}(u)\\|_{2}.\n$$\nSince $u  0$ and $\\|b(u)\\|_2$ is a scalar, we can factor them out of the norm:\n$$\n\\|r(u)\\|_{2} = u\\,\\|b(u)\\|_{2}\\,\\|u_{\\min}(u)\\|_{2}.\n$$\nThe vector $u_{\\min}(u)$ is defined as a unit left singular vector, so $\\|u_{\\min}(u)\\|_{2} = 1$.\nThis simplifies the expression to $\\|r(u)\\|_{2} = u\\,\\|b(u)\\|_{2}$.\nThe relative residual is then:\n$$\n\\frac{\\|r(u)\\|_{2}}{\\|b(u)\\|_{2}} = \\frac{u\\,\\|b(u)\\|_{2}}{\\|b(u)\\|_{2}} = u.\n$$\nThis completes the second task.\n\nThird, we derive an expression for the forward relative error $\\|\\hat{x}(u) - x(u)\\|_{2}/\\|x(u)\\|_{2}$.\nWe start with the defining equations for the exact and computed solutions:\n1. $A(u)x(u) = b(u)$\n2. $A(u)\\hat{x}(u) = b(u) + \\Delta b(u)$\nSubtracting the first equation from the second gives:\n$A(u)(\\hat{x}(u) - x(u)) = \\Delta b(u)$.\nSince $A(u)$ is nonsingular, the error vector is $\\hat{x}(u) - x(u) = A(u)^{-1} \\Delta b(u)$.\nWe substitute the given form of $\\Delta b(u) = u\\,\\|b(u)\\|_{2}\\,u_{\\min}(u)$:\n$$\n\\hat{x}(u) - x(u) = u\\,\\|b(u)\\|_{2}\\,A(u)^{-1}u_{\\min}(u).\n$$\nThe definitions from the Singular Value Decomposition (SVD) state that $A(u)v_i(u) = \\sigma_i(u)u_i(u)$ and $A(u)^{-1}u_i(u) = \\frac{1}{\\sigma_i(u)}v_i(u)$, where $v_i(u)$ and $u_i(u)$ are the right and left singular vectors corresponding to the singular value $\\sigma_i(u)$. Thus, $A(u)^{-1}u_{\\min}(u) = \\frac{1}{\\sigma_{\\min}(A(u))}v_{\\min}(u)$.\nAlso, $b(u) = A(u)x(u) = A(u)v_{\\max}(u) = \\sigma_{\\max}(A(u))u_{\\max}(u)$. Taking the norm, $\\|b(u)\\|_2 = \\sigma_{\\max}(A(u))\\|u_{\\max}(u)\\|_2 = \\sigma_{\\max}(A(u))$.\nSubstituting these into the error vector expression:\n$$\n\\hat{x}(u) - x(u) = u\\,\\sigma_{\\max}(A(u))\\,\\left(\\frac{1}{\\sigma_{\\min}(A(u))}v_{\\min}(u)\\right) = u\\,\\frac{\\sigma_{\\max}(A(u))}{\\sigma_{\\min}(A(u))}\\,v_{\\min}(u) = u\\,\\kappa_2(A(u))\\,v_{\\min}(u).\n$$\nTaking the norm of the error vector:\n$$\n\\|\\hat{x}(u) - x(u)\\|_{2} = \\|u\\,\\kappa_2(A(u))\\,v_{\\min}(u)\\|_{2} = u\\,\\kappa_2(A(u))\\,\\|v_{\\min}(u)\\|_{2}.\n$$\nSince $v_{\\min}(u)$ is a unit right singular vector, $\\|v_{\\min}(u)\\|_2=1$, so $\\|\\hat{x}(u) - x(u)\\|_{2} = u\\,\\kappa_2(A(u))$.\nThe forward relative error is obtained by dividing by $\\|x(u)\\|_2$. The problem defines $x(u) = v_{\\max}(u)$, which is a unit vector, so $\\|x(u)\\|_2 = 1$.\nThe forward relative error is $\\frac{\\|\\hat{x}(u) - x(u)\\|_{2}}{\\|x(u)\\|_{2}} = u\\,\\kappa_{2}(A(u))$.\n\nNow we specialize this using $\\varepsilon(u)=4u$. From the analysis for the first task, the roots of the characteristic polynomial $\\lambda^2 - (4+2\\varepsilon+\\varepsilon^2)\\lambda + \\varepsilon^2 = 0$ are $\\lambda = \\frac{(4+2\\varepsilon+\\varepsilon^2) \\pm \\sqrt{(4+2\\varepsilon+\\varepsilon^2)^2 - 4\\varepsilon^2}}{2}$.\nThe condition number is $\\kappa_2(A(\\varepsilon)) = \\sqrt{\\lambda_{\\max}/\\lambda_{\\min}}$.\n$$\n\\kappa_2(A(\\varepsilon)) = \\sqrt{\\frac{4+2\\varepsilon+\\varepsilon^2 + \\sqrt{D}}{4+2\\varepsilon+\\varepsilon^2 - \\sqrt{D}}} = \\frac{4+2\\varepsilon+\\varepsilon^2 + \\sqrt{D}}{2\\varepsilon}\n$$\nwhere $D=(4+2\\varepsilon+\\varepsilon^2)^2-4\\varepsilon^2$.\nThe forward relative error is $u\\,\\kappa_2(A(u)) = u\\,\\kappa_2(A(4u))$.\n$$\n\\text{Fwd. Rel. Err.} = u \\cdot \\frac{4+2(4u)+(4u)^2 + \\sqrt{(4+2(4u)+(4u)^2)^2-4(4u)^2}}{2(4u)}\n$$\n$$\n= u \\cdot \\frac{4+8u+16u^2 + \\sqrt{(4+8u+16u^2)^2-64u^2}}{8u}\n$$\n$$\n= \\frac{4+8u+16u^2 + \\sqrt{(4+8u+16u^2)^2-64u^2}}{8}.\n$$\nThis completes the third task.\n\nFourth, we compute the limit of the forward relative error as $u \\to 0^{+}$.\n$$\nL = \\lim_{u \\to 0^+} \\frac{4+8u+16u^2 + \\sqrt{(4+8u+16u^2)^2-64u^2}}{8}.\n$$\nThe expression is a continuous function of $u$ at $u=0$. We can evaluate the limit by direct substitution of $u=0$:\n$$\nL = \\frac{4+8(0)+16(0)^2 + \\sqrt{(4+8(0)+16(0)^2)^2-64(0)^2}}{8}\n$$\n$$\nL = \\frac{4 + \\sqrt{4^2 - 0}}{8} = \\frac{4 + \\sqrt{16}}{8} = \\frac{4+4}{8} = \\frac{8}{8} = 1.\n$$\nThe limit of the forward relative error is $1$.\nThis result demonstrates a key concept in numerical analysis: a small relative residual (of size $u$) does not guarantee a small forward error. Here, the forward error is amplified by the condition number $\\kappa_2(A(u))$ which, by construction, is approximately $1/u$, leading to a constant forward error of order $1$.", "answer": "$$\\boxed{1}$$", "id": "3574243"}]}