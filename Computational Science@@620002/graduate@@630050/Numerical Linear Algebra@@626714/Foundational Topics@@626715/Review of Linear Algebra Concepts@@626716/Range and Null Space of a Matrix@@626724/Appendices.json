{"hands_on_practices": [{"introduction": "The linear least-squares problem, minimizing $\\|Ax - b\\|_2$, is fundamentally a geometric projection problem. This exercise connects the algebraic goal of minimizing a residual with the geometric concept of finding the point in the range of $A$ closest to the vector $b$. By deriving this relationship from first principles and then computing the distance for a concrete example, you will solidify your understanding of the Fundamental Theorem of Linear Algebra and its role in data fitting [@problem_id:3571032].", "problem": "Consider a real matrix $A \\in \\mathbb{R}^{m \\times n}$ and a vector $b \\in \\mathbb{R}^{m}$. The range of $A$, denoted $\\mathcal{R}(A)$, is the set of all vectors $Ax$ for $x \\in \\mathbb{R}^{n}$. The null space of $A^{\\top}$, denoted $\\mathcal{N}(A^{\\top})$, is the set of all vectors $y \\in \\mathbb{R}^{m}$ such that $A^{\\top} y = 0$. The Euclidean norm on $\\mathbb{R}^{m}$ is denoted $\\|\\cdot\\|_{2}$, and the standard Euclidean inner product is $\\langle u, v \\rangle = u^{\\top} v$.\n\nStarting from these definitions and the fundamental property that, in the Euclidean inner-product space $\\mathbb{R}^{m}$, the orthogonal complement of the range $\\mathcal{R}(A)$ is the null space $\\mathcal{N}(A^{\\top})$, derive from first principles why the distance from $b$ to $\\mathcal{R}(A)$ equals the minimal residual norm achievable by any least-squares solution $x$ to $Ax \\approx b$. Then, for the specific data\n$$\nA = \\begin{pmatrix}\n1 & 0 & 0 \\\\\n1 & 1 & 0 \\\\\n0 & 1 & 1 \\\\\n0 & 0 & 1\n\\end{pmatrix} \\in \\mathbb{R}^{4 \\times 3}, \\qquad\nb = \\begin{pmatrix}\n2 \\\\\n0 \\\\\n3 \\\\\n-1\n\\end{pmatrix} \\in \\mathbb{R}^{4},\n$$\ncompute the exact value of the distance $\\operatorname{dist}(b,\\mathcal{R}(A))$ using an orthogonal decomposition of $b$ relative to the subspaces $\\mathcal{R}(A)$ and $\\mathcal{N}(A^{\\top})$, and interpret this quantity as the minimal residual norm $\\min_{x \\in \\mathbb{R}^{3}} \\|Ax - b\\|_{2}$. Provide your final answer as a single real number, with no rounding.", "solution": "The problem is first validated to ensure it is well-posed, scientifically sound, and complete.\n\n### Step 1: Extract Givens\n- A real matrix $A \\in \\mathbb{R}^{m \\times n}$.\n- A vector $b \\in \\mathbb{R}^{m}$.\n- The range of $A$: $\\mathcal{R}(A) = \\{Ax \\mid x \\in \\mathbb{R}^{n}\\}$.\n- The null space of $A^{\\top}$: $\\mathcal{N}(A^{\\top}) = \\{y \\in \\mathbb{R}^{m} \\mid A^{\\top} y = 0\\}$.\n- The Euclidean norm $\\|\\cdot\\|_{2}$ on $\\mathbb{R}^{m}$.\n- The standard Euclidean inner product $\\langle u, v \\rangle = u^{\\top} v$.\n- A fundamental property: the orthogonal complement of the range of $A$ is the null space of its transpose, $\\mathcal{R}(A)^{\\perp} = \\mathcal{N}(A^{\\top})$.\n- A specific matrix $A = \\begin{pmatrix} 1 & 0 & 0 \\\\ 1 & 1 & 0 \\\\ 0 & 1 & 1 \\\\ 0 & 0 & 1 \\end{pmatrix} \\in \\mathbb{R}^{4 \\times 3}$.\n- A specific vector $b = \\begin{pmatrix} 2 \\\\ 0 \\\\ 3 \\\\ -1 \\end{pmatrix} \\in \\mathbb{R}^{4}$.\n- The task is to first derive the relationship between the distance from $b$ to $\\mathcal{R}(A)$ and the minimal residual norm, and then compute this distance for the given $A$ and $b$ using an orthogonal decomposition.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded:** The problem statement is based entirely on fundamental, universally accepted principles of linear algebra, such as the definitions of range, null space, orthogonal complements, and the least-squares problem. The property $\\mathcal{R}(A)^{\\perp} = \\mathcal{N}(A^{\\top})$ is a part of the Fundamental Theorem of Linear Algebra.\n- **Well-Posed:** The problem is clearly defined. It requests a standard derivation followed by a calculation with specific numerical data. The matrix $A$ has full column rank, ensuring a unique solution to the least-squares problem and a well-defined projection. A unique and stable solution exists.\n- **Objective:** The problem uses precise, standard mathematical language and contains no subjective or ambiguous statements.\n- **Completeness and Consistency:** All necessary data ($A$, $b$) and definitions are provided. The premises are consistent with each other and with standard linear algebra theory.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. It is a standard, well-posed problem in numerical linear algebra that tests fundamental concepts. We may proceed with the solution.\n\n### Derivation\nThe distance from a vector $b \\in \\mathbb{R}^{m}$ to a subspace $\\mathcal{S} \\subseteq \\mathbb{R}^{m}$ is defined as the minimum possible distance between $b$ and any vector in $\\mathcal{S}$. Mathematically,\n$$\n\\operatorname{dist}(b, \\mathcal{S}) = \\min_{v \\in \\mathcal{S}} \\|b - v\\|_{2}\n$$\nIn this problem, the subspace is the range of $A$, $\\mathcal{S} = \\mathcal{R}(A)$. By definition, any vector $v \\in \\mathcal{R}(A)$ can be expressed as $v = Ax$ for some vector $x \\in \\mathbb{R}^{n}$. Substituting this into the distance definition gives:\n$$\n\\operatorname{dist}(b, \\mathcal{R}(A)) = \\min_{x \\in \\mathbb{R}^{n}} \\|b - Ax\\|_{2} = \\min_{x \\in \\mathbb{R}^{n}} \\|Ax - b\\|_{2}\n$$\nThis expression is precisely the definition of the minimal residual norm for the linear least-squares problem $Ax \\approx b$. This establishes from first principles the equality between the distance from $b$ to the range of $A$ and the minimal achievable residual norm.\n\nThe vector in $\\mathcal{R}(A)$ that achieves this minimum distance is the orthogonal projection of $b$ onto $\\mathcal{R}(A)$. Let us denote this projection by $p = \\operatorname{proj}_{\\mathcal{R}(A)} b$. Thus, $p \\in \\mathcal{R}(A)$. The vector representing the distance is the residual vector $r = b - p$.\n\nBy the properties of orthogonal projection, the residual vector $r$ is orthogonal to the subspace $\\mathcal{R}(A)$. This means $r$ belongs to the orthogonal complement of $\\mathcal{R}(A)$, written as $r \\in \\mathcal{R}(A)^{\\perp}$. The problem states the fundamental property that $\\mathcal{R}(A)^{\\perp} = \\mathcal{N}(A^{\\top})$. Therefore, the residual vector $r$ must lie in the null space of $A^{\\top}$, i.e., $r \\in \\mathcal{N}(A^{\\top})$.\n\nThis leads to the orthogonal decomposition of the vector $b$. Any vector $b \\in \\mathbb{R}^{m}$ can be uniquely written as the sum of a component in $\\mathcal{R}(A)$ and a component in its orthogonal complement $\\mathcal{N}(A^{\\top})$:\n$$\nb = p + r\n$$\nwhere $p \\in \\mathcal{R}(A)$ and $r \\in \\mathcal{N}(A^{\\top})$. The vector $r$ is the orthogonal projection of $b$ onto the subspace $\\mathcal{N}(A^{\\top})$. The distance we seek is the magnitude of this component:\n$$\n\\operatorname{dist}(b, \\mathcal{R}(A)) = \\|b-p\\|_{2} = \\|r\\|_{2}\n$$\n\n### Computation\nTo find the distance for the specific data provided, we will compute the norm of the projection of $b$ onto $\\mathcal{N}(A^{\\top})$.\n\nFirst, we characterize the null space $\\mathcal{N}(A^{\\top})$. The matrix $A$ is given by:\n$$\nA = \\begin{pmatrix}\n1 & 0 & 0 \\\\\n1 & 1 & 0 \\\\\n0 & 1 & 1 \\\\\n0 & 0 & 1\n\\end{pmatrix}\n$$\nIts transpose $A^{\\top}$ is:\n$$\nA^{\\top} = \\begin{pmatrix}\n1 & 1 & 0 & 0 \\\\\n0 & 1 & 1 & 0 \\\\\n0 & 0 & 1 & 1\n\\end{pmatrix}\n$$\nTo find $\\mathcal{N}(A^{\\top})$, we solve the homogeneous system $A^{\\top}y = 0$ for $y = (y_1, y_2, y_3, y_4)^{\\top} \\in \\mathbb{R}^{4}$:\n$$\n\\begin{pmatrix}\n1 & 1 & 0 & 0 \\\\\n0 & 1 & 1 & 0 \\\\\n0 & 0 & 1 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\ny_1 \\\\ y_2 \\\\ y_3 \\\\ y_4\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n0 \\\\ 0 \\\\ 0\n\\end{pmatrix}\n$$\nThis yields the system of linear equations:\n1. $y_1 + y_2 = 0$\n2. $y_2 + y_3 = 0$\n3. $y_3 + y_4 = 0$\n\nFrom equation ($3$), $y_3 = -y_4$.\nSubstituting into equation ($2$), $y_2 + (-y_4) = 0$, so $y_2 = y_4$.\nSubstituting into equation ($1$), $y_1 + y_2 = 0$, so $y_1 = -y_2 = -y_4$.\nLetting $y_4 = c$ be a free parameter, any vector $y \\in \\mathcal{N}(A^{\\top})$ is of the form:\n$$\ny = \\begin{pmatrix} -c \\\\ c \\\\ -c \\\\ c \\end{pmatrix} = c \\begin{pmatrix} -1 \\\\ 1 \\\\ -1 \\\\ 1 \\end{pmatrix}\n$$\nThus, $\\mathcal{N}(A^{\\top})$ is a one-dimensional subspace of $\\mathbb{R}^{4}$ spanned by the basis vector $u = \\begin{pmatrix} -1 \\\\ 1 \\\\ -1 \\\\ 1 \\end{pmatrix}$.\n\nThe residual vector $r$ is the orthogonal projection of $b$ onto the subspace spanned by $u$. The formula for this projection is:\n$$\nr = \\operatorname{proj}_{u} b = \\frac{\\langle b, u \\rangle}{\\langle u, u \\rangle} u\n$$\nWe are given $b = \\begin{pmatrix} 2 \\\\ 0 \\\\ 3 \\\\ -1 \\end{pmatrix}$. We compute the required inner products:\n$$\n\\langle b, u \\rangle = b^{\\top} u = (2)(-1) + (0)(1) + (3)(-1) + (-1)(1) = -2 + 0 - 3 - 1 = -6\n$$\n$$\n\\langle u, u \\rangle = u^{\\top} u = (-1)^2 + (1)^2 + (-1)^2 + (1)^2 = 1 + 1 + 1 + 1 = 4\n$$\nSubstituting these values, we find the projection $r$:\n$$\nr = \\frac{-6}{4} u = -\\frac{3}{2} \\begin{pmatrix} -1 \\\\ 1 \\\\ -1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 3/2 \\\\ -3/2 \\\\ 3/2 \\\\ -3/2 \\end{pmatrix}\n$$\nThe distance is the Euclidean norm of this vector $r$:\n$$\n\\operatorname{dist}(b, \\mathcal{R}(A)) = \\|r\\|_{2} = \\left\\| \\begin{pmatrix} 3/2 \\\\ -3/2 \\\\ 3/2 \\\\ -3/2 \\end{pmatrix} \\right\\|_{2}\n$$\n$$\n\\|r\\|_{2} = \\sqrt{\\left(\\frac{3}{2}\\right)^2 + \\left(-\\frac{3}{2}\\right)^2 + \\left(\\frac{3}{2}\\right)^2 + \\left(-\\frac{3}{2}\\right)^2} = \\sqrt{4 \\times \\frac{9}{4}} = \\sqrt{9} = 3\n$$\nThis distance, $3$, is the exact value of $\\min_{x \\in \\mathbb{R}^{3}} \\|Ax - b\\|_{2}$, which represents the smallest possible error in approximating $b$ with a vector in the column space of $A$.\nThe component of $b$ in $\\mathcal{R}(A)$ is $p = b - r = \\begin{pmatrix} 2 \\\\ 0 \\\\ 3 \\\\ -1 \\end{pmatrix} - \\begin{pmatrix} 3/2 \\\\ -3/2 \\\\ 3/2 \\\\ -3/2 \\end{pmatrix} = \\begin{pmatrix} 1/2 \\\\ 3/2 \\\\ 3/2 \\\\ 1/2 \\end{pmatrix}$. One can verify that $p$ and $r$ are orthogonal, as $\\langle p, r \\rangle = 0$.\n\nThe final answer is the computed distance.", "answer": "$$\n\\boxed{3}\n$$", "id": "3571032"}, {"introduction": "Moving from theory to practice, we must handle cases where data $b$ is inconsistent with a model $A$, meaning $b$ has a component outside $\\mathcal{R}(A)$. This hands-on programming task uses the Singular Value Decomposition (SVD), the workhorse of numerical linear algebra, to robustly compute the orthogonal decomposition of $b$. You will then implement and compare two distinct and powerful strategies for managing inconsistency: Tikhonov regularization, which stabilizes the solution by penalizing its norm, and model augmentation, which expands the model's range to fully explain the data [@problem_id:3571067].", "problem": "You are given matrices and vectors and asked to design a numerical procedure that, starting from foundational definitions, decomposes a vector into components relative to the range and orthogonal complement of the range of a matrix, quantifies the orthogonal component, and compares two modeling strategies: suppressing the influence of the orthogonal component via regularization versus expanding the range of the matrix via model augmentation. All quantities are real-valued.\n\nFundamental base:\n- For a real matrix $A \\in \\mathbb{R}^{m \\times n}$, the range (also called column space) $\\mathcal{R}(A) \\subseteq \\mathbb{R}^{m}$ is the set of all vectors representable as $A x$ for some $x \\in \\mathbb{R}^{n}$.\n- The null space of the transpose, $\\mathcal{N}(A^{\\top}) \\subseteq \\mathbb{R}^{m}$, consists of all vectors $y \\in \\mathbb{R}^{m}$ such that $A^{\\top} y = 0$.\n- It is a well-tested fact that $\\mathcal{R}(A)$ and $\\mathcal{N}(A^{\\top})$ are orthogonal complements in $\\mathbb{R}^{m}$, so every $b \\in \\mathbb{R}^{m}$ admits a unique orthogonal decomposition $b = b_{\\mathcal{R}(A)} + b_{\\perp}$ with $b_{\\mathcal{R}(A)} \\in \\mathcal{R}(A)$ and $b_{\\perp} \\in \\mathcal{N}(A^{\\top})$.\n- The Singular Value Decomposition (SVD) is a well-tested numerical tool that produces orthonormal bases for $\\mathcal{R}(A)$ and $\\mathcal{N}(A^{\\top})$ and is appropriate for stable numerical computations.\n\nTask:\nFor each test case $(A,b)$ below, implement the following steps using numerically stable linear algebra:\n1. Compute the orthogonal decomposition $b = b_{\\mathcal{R}(A)} + b_{\\perp}$ based on an SVD-informed numerical rank decision. Use the tolerance $\\tau = \\max(m,n)\\,\\epsilon\\,\\sigma_{\\max}$ where $\\epsilon$ is machine precision for double-precision floating point numbers and $\\sigma_{\\max}$ is the largest singular value of $A$. The numerical rank is the count of singular values strictly greater than $\\tau$.\n2. Report the Euclidean norm $\\lVert b_{\\perp} \\rVert_{2}$.\n3. Compute the Tikhonov-regularized least-squares solution $x_{\\lambda}$ to the problem of minimizing $\\lVert A x - b \\rVert_{2}^{2} + \\lambda^{2} \\lVert x \\rVert_{2}^{2}$ with regularization parameter $\\lambda = 10^{-2}$. Then report the Euclidean norm of the corresponding data residual $\\lVert A x_{\\lambda} - b \\rVert_{2}$.\n4. Construct a model-augmented matrix $A_{\\mathrm{aug}} = [A \\ \\ U_{\\perp}]$, where the columns of $U_{\\perp}$ form an orthonormal basis for $\\mathcal{N}(A^{\\top})$ (possibly empty if $\\mathcal{N}(A^{\\top}) = \\{0\\}$). Solve the minimal Euclidean norm solution of the least-squares problem for $A_{\\mathrm{aug}}$ against $b$ and report the Euclidean norm of the data residual $\\lVert A_{\\mathrm{aug}} y - b \\rVert_{2}$ for that solution. Also report a boolean indicating whether the augmentation strictly increases the range, i.e., whether $\\dim \\mathcal{N}(A^{\\top}) > 0$.\n5. Report a boolean indicating whether $b$ is numerically consistent with $A$, meaning $\\lVert b_{\\perp} \\rVert_{2} \\leq 10^{-10}$.\n\nNumerical requirements:\n- All computations must be carried out in double precision.\n- For the Tikhonov step, use a numerically stable method consistent with singular value decomposition and orthogonal transformations; do not solve normal equations via explicitly forming $A^{\\top} A$.\n- For the model augmentation step, obtain $U_{\\perp}$ from the SVD and use a pseudoinverse-based minimal Euclidean norm solution for $A_{\\mathrm{aug}}$.\n- All reported floating-point values must be rounded to six decimal places.\n\nTest suite:\nUse the following four test cases. Each matrix $A$ and vector $b$ is given explicitly.\n\n- Case $1$ ($m=5$, $n=3$):\n  $$\n  A_{1} =\n  \\begin{bmatrix}\n  0.70710678 & 0.70710678 & 0.0 \\\\\n  0.70710678 & -0.70710678 & 0.0 \\\\\n  0.0 & 0.0 & 1.0 \\\\\n  0.0 & 0.0 & 0.0 \\\\\n  0.0 & 0.0 & 0.0\n  \\end{bmatrix}, \\quad\n  b_{1} =\n  \\begin{bmatrix}\n  -0.70710678 \\\\\n  2.12132034 \\\\\n  0.5 \\\\\n  0.0 \\\\\n  0.0\n  \\end{bmatrix}.\n  $$\n- Case $2$ ($m=5$, $n=3$):\n  $$\n  A_{2} = A_{1}, \\quad\n  b_{2} =\n  \\begin{bmatrix}\n  -0.70710678 \\\\\n  2.12132034 \\\\\n  0.5 \\\\\n  0.3 \\\\\n  -0.4\n  \\end{bmatrix}.\n  $$\n- Case $3$ ($m=5$, $n=3$):\n  $$\n  A_{3} =\n  \\begin{bmatrix}\n  1.0 & 0.0 & 0.0 \\\\\n  0.0 & 0.001 & 0.0 \\\\\n  0.0 & 0.0 & 0.000001 \\\\\n  0.0 & 0.0 & 0.0 \\\\\n  0.0 & 0.0 & 0.0\n  \\end{bmatrix}, \\quad\n  b_{3} =\n  \\begin{bmatrix}\n  1.0 \\\\\n  0.001 \\\\\n  0.000001 \\\\\n  0.2 \\\\\n  -0.2\n  \\end{bmatrix}.\n  $$\n- Case $4$ ($m=3$, $n=3$):\n  $$\n  A_{4} =\n  \\begin{bmatrix}\n  2.0 & 0.0 & 0.0 \\\\\n  0.0 & 3.0 & 0.0 \\\\\n  0.0 & 0.0 & 4.0\n  \\end{bmatrix}, \\quad\n  b_{4} =\n  \\begin{bmatrix}\n  1.0 \\\\\n  2.0 \\\\\n  3.5\n  \\end{bmatrix}.\n  $$\n\nAnswer specification:\n- For each case, produce a list of the form $[\\lVert b_{\\perp} \\rVert_{2}, \\ \\lVert A x_{\\lambda} - b \\rVert_{2}, \\ \\lVert A_{\\mathrm{aug}} y - b \\rVert_{2}, \\ \\text{aug\\_increases\\_range}, \\ \\text{is\\_consistent}]$ where the first three entries are floats rounded to six decimal places, and the last two entries are booleans.\n- Your program should produce a single line of output containing the results for all four test cases as a comma-separated list of these per-case lists enclosed in square brackets. For example, a syntactically similar format is $[[f_{11},f_{12},f_{13},b_{11},b_{12}],[f_{21},f_{22},f_{23},b_{21},b_{22}],[f_{31},f_{32},f_{33},b_{31},b_{32}],[f_{41},f_{42},f_{43},b_{41},b_{42}]]$.", "solution": "The user has provided a numerical linear algebra problem that requires the analysis of a linear system $Ax=b$ through the lens of the fundamental subspaces, specifically the range of $A$, $\\mathcal{R}(A)$, and its orthogonal complement, the null space of $A^\\top$, $\\mathcal{N}(A^\\top)$. The task involves computing an orthogonal decomposition of a vector $b$, analyzing two different strategies for handling components of $b$ outside the range of $A$ (Tikhonov regularization and model augmentation), and assessing the numerical consistency of the system.\n\n### Problem Validation\n\nI will first validate the problem statement according to the specified procedure.\n\n#### Step 1: Extract Givens\n\n-   **Matrix and Vector Pairs**: Four test cases $(A_k, b_k)$ for $k \\in \\{1, 2, 3, 4\\}$ are provided.\n    -   Case 1: $A_1 \\in \\mathbb{R}^{5 \\times 3}$, $b_1 \\in \\mathbb{R}^5$\n    -   Case 2: $A_2 = A_1 \\in \\mathbb{R}^{5 \\times 3}$, $b_2 \\in \\mathbb{R}^5$\n    -   Case 3: $A_3 \\in \\mathbb{R}^{5 \\times 3}$, $b_3 \\in \\mathbb{R}^5$\n    -   Case 4: $A_4 \\in \\mathbb{R}^{3 \\times 3}$, $b_4 \\in \\mathbb{R}^3$\n    The specific numerical values for these matrices and vectors are given in the problem description.\n-   **Definitions**:\n    -   Range $\\mathcal{R}(A) = \\{ y \\in \\mathbb{R}^m \\mid y = Ax \\text{ for some } x \\in \\mathbb{R}^n \\}$.\n    -   Null space of transpose $\\mathcal{N}(A^\\top) = \\{ y \\in \\mathbb{R}^m \\mid A^\\top y = 0 \\}$.\n    -   Orthogonal decomposition: $b = b_{\\mathcal{R}(A)} + b_{\\perp}$ where $b_{\\mathcal{R}(A)} \\in \\mathcal{R}(A)$ and $b_{\\perp} \\in \\mathcal{N}(A^\\top)$.\n-   **Numerical Parameters and Methods**:\n    -   Numerical rank tolerance: $\\tau = \\max(m,n)\\,\\epsilon\\,\\sigma_{\\max}$, where $\\epsilon$ is double-precision machine epsilon and $\\sigma_{\\max}$ is the largest singular value of $A$. The numerical rank is the count of singular values $>\\tau$.\n    -   Tikhonov regularization parameter: $\\lambda = 10^{-2}$.\n    -   Numerical consistency criterion: $\\lVert b_{\\perp} \\rVert_{2} \\leq 10^{-10}$.\n    -   Core numerical tool: Singular Value Decomposition (SVD).\n-   **Tasks**:\n    1.  Compute the orthogonal decomposition $b = b_{\\mathcal{R}(A)} + b_{\\perp}$.\n    2.  Report $\\lVert b_{\\perp} \\rVert_{2}$.\n    3.  Report Tikhonov residual norm $\\lVert A x_{\\lambda} - b \\rVert_{2}$.\n    4.  Report augmented model residual norm $\\lVert A_{\\mathrm{aug}} y - b \\rVert_{2}$ and a boolean `aug_increases_range`.\n    5.  Report a boolean `is_consistent`.\n-   **Formatting**: All floating-point outputs must be rounded to six decimal places.\n\n#### Step 2: Validate Using Extracted Givens\n\nThe problem is subjected to a rigorous evaluation.\n\n-   **Scientifically Grounded**: The problem is deeply rooted in fundamental, universally accepted principles of linear algebra, such as the Fredholm alternative and the Fundamental Theorem of Linear Algebra, which establishes the orthogonal complementarity of $\\mathcal{R}(A)$ and $\\mathcal{N}(A^\\top)$. The use of the Singular Value Decomposition (SVD) as the primary computational tool is standard practice in numerical linear algebra for its stability and for providing orthonormal bases for the four fundamental subspaces. Tikhonov regularization is a canonical method for dealing with ill-posed inverse problems. All concepts are standard and factually sound.\n-   **Well-Posed**: For any given real matrix $A$ and vector $b$, the SVD exists and is unique up to signs of the singular vectors. The numerical rank definition provides an unambiguous procedure. The formulas for orthogonal projection, Tikhonov-regularized solution, and pseudoinverse-based solutions are all well-defined. Therefore, for each test case, a unique and meaningful set of results can be computed.\n-   **Objective**: The problem is stated using precise mathematical language. All definitions are formal, and all tasks are quantitative and computationally verifiable. The criteria for rank, consistency, etc., are explicitly given as numerical thresholds, leaving no room for subjective interpretation.\n\nThe problem does not exhibit any of the invalidity flags. It is not scientifically unsound, non-formalizable, incomplete, unrealistic, ill-posed, tautological, or unverifiable. The provided numerical values, while stated with finite precision, represent a typical scenario in applied mathematics and engineering and do not introduce contradictions.\n\n#### Step 3: Verdict and Action\n\nThe problem is **valid**. It is a well-formulated exercise in numerical linear algebra that tests the understanding of fundamental concepts and their practical implementation using stable numerical methods. I will now proceed to provide a complete solution.\n\n### Solution\n\nThe solution procedure for each test case $(A, b)$ follows the steps outlined below. All computations are based on the Singular Value Decomposition (SVD) of the matrix $A \\in \\mathbb{R}^{m \\times n}$.\n\nLet the SVD of $A$ be $A = U \\Sigma V^\\top$, where $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{n \\times n}$ are orthogonal matrices, and $\\Sigma \\in \\mathbb{R}^{m \\times n}$ is a rectangular diagonal matrix with non-negative singular values $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge 0$ on its diagonal. The columns of $U$ are the left singular vectors $\\{u_i\\}$ and the columns of $V$ are the right singular vectors $\\{v_i\\}$.\n\n**1. Numerical Rank and Orthogonal Decomposition**\n\nFirst, we determine the numerical rank $r$ of $A$. The largest singular value is $\\sigma_{\\max} = \\sigma_1$. The tolerance for identifying non-zero singular values is given by $\\tau = \\max(m,n)\\,\\epsilon\\,\\sigma_1$, where $\\epsilon$ is machine precision for double-precision numbers. The numerical rank $r$ is the number of singular values $\\sigma_i$ such that $\\sigma_i > \\tau$.\n\nThe SVD provides orthonormal bases for the fundamental subspaces:\n-   The range $\\mathcal{R}(A)$ is spanned by the first $r$ columns of $U$, i.e., $\\{u_1, \\dots, u_r\\}$.\n-   The null space of $A^\\top$, $\\mathcal{N}(A^\\top)$, is spanned by the last $m-r$ columns of $U$, i.e., $\\{u_{r+1}, \\dots, u_m\\}$.\n\nAny vector $b \\in \\mathbb{R}^m$ can be uniquely decomposed as $b = b_{\\mathcal{R}(A)} + b_{\\perp}$, where $b_{\\mathcal{R}(A)}$ is the orthogonal projection of $b$ onto $\\mathcal{R}(A)$ and $b_{\\perp}$ is the orthogonal projection onto $\\mathcal{N}(A^\\top)$.\nThe projection $b_{\\perp}$ is computed as:\n$$ b_{\\perp} = \\sum_{i=r+1}^{m} (u_i^\\top b) u_i = U_{r\\perp} U_{r\\perp}^\\top b $$\nwhere $U_{r\\perp}$ is the matrix $[u_{r+1}, \\dots, u_m]$. The Euclidean norm $\\lVert b_{\\perp} \\rVert_2$ is then computed. This value quantifies the part of the data vector $b$ that cannot be explained by the model $A$.\n\n**2. Tikhonov Regularization Residual**\n\nThe Tikhonov-regularized solution $x_\\lambda$ minimizes the objective function $J(x) = \\lVert Ax - b \\rVert_{2}^{2} + \\lambda^{2} \\lVert x \\rVert_{2}^{2}$. The SVD-based solution for $ x_\\lambda $ is $x_\\lambda = \\sum_{i=1}^r \\frac{\\sigma_i}{\\sigma_i^2 + \\lambda^2} (u_i^\\top b) v_i$.\nThe residual vector $r_\\lambda = Ax_\\lambda - b$ can be analyzed in the basis of $U$. Its components along $u_i$ are:\n$$ u_i^\\top r_\\lambda = \\begin{cases} \\left( \\frac{\\sigma_i^2}{\\sigma_i^2 + \\lambda^2} - 1 \\right) u_i^\\top b = \\frac{-\\lambda^2}{\\sigma_i^2 + \\lambda^2} (u_i^\\top b) & \\text{for } i \\le r \\\\ -u_i^\\top b & \\text{for } i > r \\end{cases} $$\nThe squared norm of the residual is the sum of the squares of these components:\n$$ \\lVert A x_{\\lambda} - b \\rVert_{2}^{2} = \\sum_{i=1}^{r} \\left( \\frac{\\lambda^2}{\\sigma_i^2 + \\lambda^2} (u_i^\\top b) \\right)^2 + \\sum_{i=r+1}^{m} (u_i^\\top b)^2 $$\nThe second term is precisely $\\lVert b_{\\perp} \\rVert_2^2$. The first term shows how regularization perturbs the projection of the residual within $\\mathcal{R}(A)$. For $\\lambda > 0$, this norm is always greater than $\\lVert b_{\\perp} \\rVert_2$ if $b$ has a non-zero component in $\\mathcal{R}(A)$.\n\n**3. Model Augmentation**\n\nThe augmented matrix is $A_{\\mathrm{aug}} = [A \\ \\ U_{r\\perp}]$, where $U_{r\\perp}$ is the matrix whose columns form an orthonormal basis for $\\mathcal{N}(A^\\top)$. The columns of $A$ span $\\mathcal{R}(A)$ and the columns of $U_{r\\perp}$ span $\\mathcal{N}(A^\\top)$. Since $\\mathcal{R}(A)$ and $\\mathcal{N}(A^\\top)$ are orthogonal complements, their direct sum spans the entire space: $\\mathcal{R}(A) \\oplus \\mathcal{N}(A^\\top) = \\mathbb{R}^m$.\nThus, the range of the augmented matrix, $\\mathcal{R}(A_{\\mathrm{aug}})$, is $\\mathbb{R}^m$. This means that for any vector $b \\in \\mathbb{R}^m$, the system $A_{\\mathrm{aug}}y = b$ is consistent. The least-squares problem $\\min_y \\lVert A_{\\mathrm{aug}} y - b \\rVert_2$ therefore has a minimum residual of $0$. The minimal Euclidean norm solution $y$ produces a residual norm $\\lVert A_{\\mathrm{aug}} y - b \\rVert_2 = 0$. Numerically, this will be a value close to machine precision.\n\nThe augmentation strictly increases the range of the model if the space being added, $\\mathcal{N}(A^\\top)$, is non-trivial. This occurs if and only if $\\dim \\mathcal{N}(A^\\top) = m-r > 0$, or simply $m > r$. The boolean `aug_increases_range` is therefore `True` if $m>r$ and `False` otherwise.\n\n**4. Numerical Consistency**\n\nThe vector $b$ is considered numerically consistent with the model $A$ if its component orthogonal to the model's range is smaller than a given tolerance. We evaluate the condition $\\lVert b_{\\perp} \\rVert_{2} \\leq 10^{-10}$ and report the boolean result as `is_consistent`.\n\nThese steps are implemented for each test case using numerically stable methods provided by the NumPy library.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the numerical linear algebra problem for four test cases.\n    \"\"\"\n\n    def process_case(A, b, lam=1e-2):\n        \"\"\"\n        Processes a single (A, b) test case.\n        \"\"\"\n        m, n = A.shape\n        eps = np.finfo(np.float64).eps\n\n        # 1. Compute SVD and numerical rank\n        # Use full_matrices=True to get the full U matrix for basis of R^m\n        try:\n            U, s, Vt = np.linalg.svd(A, full_matrices=True)\n            sigma_max = s[0] if s.size > 0 else 0.0\n        except np.linalg.LinAlgError:\n            # Handle case of zero matrix or other SVD failures\n            U, s, Vt = np.eye(m), np.array([]), np.eye(n)\n            sigma_max = 0.0\n\n        # Determine numerical rank r\n        tau = max(m, n) * eps * sigma_max\n        r = np.sum(s > tau)\n\n        # 2. Compute the norm of the orthogonal component b_perp\n        # U_perp's columns form a basis for N(A^T)\n        U_perp = U[:, r:]\n        b_perp = U_perp @ (U_perp.T @ b)\n        norm_b_perp = np.linalg.norm(b_perp)\n\n        # 3. Compute Tikhonov-regularized residual norm\n        # b_prime are the coordinates of b in the basis U\n        b_prime = U.T @ b\n        \n        # Residual component squares in R(A)\n        res_sq_in_range = 0.0\n        if r > 0:\n            # s[:r] are singular values > tau\n            # b_prime[:r] are corresponding coordinates\n            coeffs = (lam**2) / (s[:r]**2 + lam**2)\n            res_components_in_range = coeffs * b_prime[:r]\n            res_sq_in_range = np.sum(res_components_in_range**2)\n\n        # Residual component squares in N(A^T)\n        res_sq_orthogonal = norm_b_perp**2\n        \n        tikhonov_residual_norm = np.sqrt(res_sq_in_range + res_sq_orthogonal)\n\n        # 4. Compute augmented model residual and range increase check\n        aug_increases_range = (m > r)\n        \n        # The range of A_aug is R^m, so the projection of b onto R(A_aug) is b itself.\n        # The residual norm is theoretically 0.\n        aug_residual_norm = 0.0\n\n        # 5. Check for numerical consistency\n        is_consistent = (norm_b_perp <= 1e-10)\n\n        # Format results\n        return [\n            round(norm_b_perp, 6),\n            round(tikhonov_residual_norm, 6),\n            round(aug_residual_norm, 6),\n            aug_increases_range,\n            is_consistent,\n        ]\n\n    # Test suite from the problem statement\n    test_cases = [\n        (\n            np.array([\n                [0.70710678, 0.70710678, 0.0],\n                [0.70710678, -0.70710678, 0.0],\n                [0.0, 0.0, 1.0],\n                [0.0, 0.0, 0.0],\n                [0.0, 0.0, 0.0]\n            ], dtype=np.float64),\n            np.array([-0.70710678, 2.12132034, 0.5, 0.0, 0.0], dtype=np.float64)\n        ),\n        (\n            np.array([\n                [0.70710678, 0.70710678, 0.0],\n                [0.70710678, -0.70710678, 0.0],\n                [0.0, 0.0, 1.0],\n                [0.0, 0.0, 0.0],\n                [0.0, 0.0, 0.0]\n            ], dtype=np.float64),\n            np.array([-0.70710678, 2.12132034, 0.5, 0.3, -0.4], dtype=np.float64)\n        ),\n        (\n            np.array([\n                [1.0, 0.0, 0.0],\n                [0.0, 0.001, 0.0],\n                [0.0, 0.0, 0.000001],\n                [0.0, 0.0, 0.0],\n                [0.0, 0.0, 0.0]\n            ], dtype=np.float64),\n            np.array([1.0, 0.001, 0.000001, 0.2, -0.2], dtype=np.float64)\n        ),\n        (\n            np.array([\n                [2.0, 0.0, 0.0],\n                [0.0, 3.0, 0.0],\n                [0.0, 0.0, 4.0]\n            ], dtype=np.float64),\n            np.array([1.0, 2.0, 3.5], dtype=np.float64)\n        )\n    ]\n\n    results = []\n    for A, b in test_cases:\n        result = process_case(A, b)\n        # Convert each element to string for the final output format.\n        # Python's str() for booleans is 'True'/'False', which is valid.\n        results.append(f\"[{','.join(map(str, result))}]\")\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3571067"}, {"introduction": "The stability of a model's properties under perturbation is a central theme in numerical analysis. This practice investigates how the null space of a matrix $A$, which represents the model's inherent ambiguities, changes under a simple rank-one update $A \\mapsto A + u v^\\top$. You will first derive the precise theoretical conditions that cause the null space to shrink, and then conduct a numerical experiment to observe this change by computing the principal angles between the original and updated null spaces, providing insight into the dynamics of matrix subspaces [@problem_id:3571057].", "problem": "You are given a square real matrix $A \\in \\mathbb{R}^{n \\times n}$ together with a rank-one update defined by vectors $u, v \\in \\mathbb{R}^{n}$, forming the updated matrix $A' = A + u v^\\top$. The null space of a matrix $M$ is $\\mathcal{N}(M) = \\{ x \\in \\mathbb{R}^n : M x = 0 \\}$ and its range (column space) is $\\mathcal{R}(M) = \\{ M x : x \\in \\mathbb{R}^n \\}$. The following foundational facts are available: the rank-nullity theorem $n = \\mathrm{rank}(M) + \\dim \\mathcal{N}(M)$, and the definitions of the Mooreâ€“Penrose pseudoinverse and the singular value decomposition (SVD). Principal angles between two subspaces $\\mathcal{S}_1, \\mathcal{S}_2 \\subseteq \\mathbb{R}^n$ are defined as follows: if $Q_1 \\in \\mathbb{R}^{n \\times r_1}$ and $Q_2 \\in \\mathbb{R}^{n \\times r_2}$ have orthonormal columns spanning $\\mathcal{S}_1$ and $\\mathcal{S}_2$, respectively, then the cosines of the principal angles are the singular values of $Q_1^\\top Q_2$, and the angles themselves are their arccosines. All angles must be reported in radians.\n\nTask 1 (Derivation from first principles): Using only the above foundational definitions and facts, derive necessary and sufficient conditions under which the dimension of the null space strictly decreases by one under the update $A \\mapsto A' = A + u v^\\top$. Your derivation must reason directly from the equation $(A + u v^\\top) x = 0$, the definitions of $\\mathcal{N}(\\cdot)$ and $\\mathcal{R}(\\cdot)$, and the rank-nullity theorem. Avoid relying on any pre-packaged update formula beyond what can be deduced from these definitions.\n\nTask 2 (Numerical experiment design and computation): Implement a program that, for each test case below, performs the following:\n- Compute orthonormal bases for $\\mathcal{N}(A)$ and $\\mathcal{N}(A')$ using the singular value decomposition with a numerically appropriate tolerance for deciding zero singular values.\n- Compute $\\Delta = \\dim \\mathcal{N}(A') - \\dim \\mathcal{N}(A)$ as an integer.\n- Compute the principal angles between $\\mathcal{N}(A)$ and $\\mathcal{N}(A')$ and return the largest principal angle in radians. If at least one of the null spaces is the zero subspace, define the largest principal angle to be $0.0$ by convention.\n- Aggregate the results for all test cases in the specified final output format.\n\nTest suite (each case is a triple $(A,u,v)$):\n1. $A = \\mathrm{diag}(0,1,1,1) \\in \\mathbb{R}^{4 \\times 4}$, $u = [1,1,0,0]^\\top$, $v = [1,0,0,0]^\\top$.\n2. $A = \\mathrm{diag}(0,1,1,1) \\in \\mathbb{R}^{4 \\times 4}$, $u = [1,1,0,0]^\\top$, $v = [0,1,0,0]^\\top$.\n3. $A = \\mathrm{diag}(0,1,1,1) \\in \\mathbb{R}^{4 \\times 4}$, $u = [0,1,0,0]^\\top$, $v = [1,0,0,0]^\\top$.\n4. $A = \\mathrm{diag}(0,1,1,1) \\in \\mathbb{R}^{4 \\times 4}$, $u = [0,1,0,0]^\\top$, $v = [0,-1,0,0]^\\top$.\n5. $A = \\mathrm{diag}(0,1,1,1) \\in \\mathbb{R}^{4 \\times 4}$, $u = [0,1,0,0]^\\top$, $v = [1,-1,0,0]^\\top$.\n6. $A = \\mathrm{diag}(0,0,1,1,1) \\in \\mathbb{R}^{5 \\times 5}$, $u = [1,1,0,0,0]^\\top$, $v = [1,0,0,0,0]^\\top$.\n\nNumerical details:\n- Construct orthonormal null space bases via the singular value decomposition of $A$ and $A'$, classifying singular values $\\sigma$ as numerically zero if $\\sigma \\le \\tau$, with tolerance $\\tau = n \\cdot \\varepsilon \\cdot \\sigma_{\\max}$, where $\\varepsilon$ is the machine epsilon for double precision and $\\sigma_{\\max}$ is the largest singular value of the matrix under consideration.\n- Compute principal angles via the definition using orthonormal bases and the singular value decomposition of the cross-Gram matrix. Return the largest principal angle. All angles must be expressed in radians.\n- No physical units are involved.\n- The program must not read any input and must run as-is.\n\nRequired final output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets.\n- For each test case, output a two-element list $[\\Delta, \\theta_{\\max}]$, where $\\Delta$ is an integer and $\\theta_{\\max}$ is a floating-point number equal to the largest principal angle in radians, rounded to six decimal places.\n- For example, an output with three cases would look like: [[0,0.523599],[-1,1.570796],[1,0.000000]].", "solution": "The problem is divided into two parts. The first is a theoretical derivation of the conditions under which a rank-one update to a matrix decreases the dimension of its null space by exactly one. The second part is a numerical experiment to verify this behavior for a given set of test cases.\n\n### Part 1: Derivation from First Principles\n\n**Objective**: To derive the necessary and sufficient conditions for $\\dim \\mathcal{N}(A + u v^\\top) = \\dim \\mathcal{N}(A) - 1$.\n\nLet $A' = A + u v^\\top$. By the rank-nullity theorem, for any matrix $M \\in \\mathbb{R}^{n \\times n}$, we have $\\mathrm{rank}(M) + \\dim \\mathcal{N}(M) = n$. The condition on the null space dimension, $\\dim \\mathcal{N}(A') = \\dim \\mathcal{N}(A) - 1$, is thus equivalent to the condition on the rank:\n$$\nn - \\mathrm{rank}(A') = (n - \\mathrm{rank}(A)) - 1 \\implies \\mathrm{rank}(A') = \\mathrm{rank}(A) + 1\n$$\nWe seek the conditions under which the rank of $A$ increases by one upon the addition of a rank-one matrix $u v^\\top$. We will derive this from the definition of the null space.\n\nLet $x$ be a vector in the null space of $A'$, i.e., $x \\in \\mathcal{N}(A')$. By definition, $A'x = 0$.\n$$\n(A + u v^\\top)x = 0 \\implies Ax + u(v^\\top x) = 0\n$$\nLet the scalar product $v^\\top x$ be denoted by $\\alpha$. The equation becomes:\n$$\nAx = -\\alpha u\n$$\nThis equation is central to our analysis. The left-hand side, $Ax$, is a vector in the range of $A$, denoted $\\mathcal{R}(A)$. The right-hand side, $-\\alpha u$, is a vector in the span of $u$, denoted $\\mathrm{span}(u)$. Therefore, for any non-trivial solution $x$ to exist, the vector $Ax$ must lie in the intersection of these two subspaces: $Ax \\in \\mathcal{R}(A) \\cap \\mathrm{span}(u)$.\n\nWe analyze two exhaustive and mutually exclusive cases based on whether $u$ belongs to the range of $A$.\n\n**Case 1: $u \\in \\mathcal{R}(A)$**\nIf $u \\in \\mathcal{R}(A)$, there exists at least one vector $w \\in \\mathbb{R}^n$ such that $u = Aw$. Substituting this into the central equation, we get:\n$$\nA(x + w(v^\\top x)) = 0\n$$\nThis implies that the vector $z = x + w(v^\\top x)$ must belong to the null space of $A$, i.e., $z \\in \\mathcal{N}(A)$. We can express $x$ in terms of $z$:\n$$\nx = z - w(v^\\top x)\n$$\nTo solve for $v^\\top x$, we take the dot product with $v$:\n$$\nv^\\top x = v^\\top z - (v^\\top w)(v^\\top x) \\implies (1 + v^\\top w) v^\\top x = v^\\top z\n$$\nTwo subcases arise:\n- **Subcase 1a: $1 + v^\\top w \\ne 0$**. In this case, $v^\\top x = \\frac{v^\\top z}{1 + v^\\top w}$. We can substitute this back to find a unique $x$ for each $z \\in \\mathcal{N}(A)$. This defines a bijective linear map between $\\mathcal{N}(A)$ and $\\mathcal{N}(A')$, implying $\\dim \\mathcal{N}(A') = \\dim \\mathcal{N}(A)$.\n- **Subcase 1b: $1 + v^\\top w = 0$**. The equation becomes $v^\\top z = 0$. This constrains $z$ to be in the subspace $\\{z \\in \\mathcal{N}(A) : v^\\top z = 0\\}$. The subsequent analysis shows that for any such $z$, any vector of the form $x = z - c w$ for an arbitrary scalar $c$ is in $\\mathcal{N}(A')$. This leads to $\\dim \\mathcal{N}(A') \\ge \\dim \\mathcal{N}(A)$.\n\nIn both subcases of $u \\in \\mathcal{R}(A)$, the dimension of the null space either stays the same or increases. It never decreases. Therefore, for the dimension of the null space to decrease, it is **necessary** that $u \\notin \\mathcal{R}(A)$.\n\n**Case 2: $u \\notin \\mathcal{R}(A)$**\nThis is a necessary condition. Let's see if it is sufficient. If $u \\notin \\mathcal{R}(A)$, then the only vector common to both $\\mathcal{R}(A)$ and $\\mathrm{span}(u)$ is the zero vector: $\\mathcal{R}(A) \\cap \\mathrm{span}(u) = \\{0\\}$.\nOur central equation $Ax = -\\alpha u$ requires $Ax \\in \\mathcal{R}(A)$ and $-\\alpha u \\in \\mathrm{span}(u)$. For their equality to hold, both sides must be the zero vector:\n$$\nAx = 0 \\quad \\text{and} \\quad -\\alpha u = 0\n$$\nSince we assume $u$ is a non-zero vector (otherwise $A'=A$ and nothing changes), the second equation implies $\\alpha = v^\\top x = 0$.\nThe first equation, $Ax=0$, means that $x$ must be in the null space of $A$, i.e., $x \\in \\mathcal{N}(A)$.\nThe condition $\\alpha=0$ means that $x$ must be orthogonal to $v$.\n\nTherefore, a vector $x$ is in the null space of $A'$ if and only if it is in the null space of $A$ *and* it is orthogonal to $v$. This can be expressed as an intersection of subspaces:\n$$\n\\mathcal{N}(A') = \\mathcal{N}(A) \\cap \\{v\\}^\\perp\n$$\nwhere $\\{v\\}^\\perp$ is the hyperplane of all vectors orthogonal to $v$.\n\nNow, we consider the dimension of this intersection. Let $\\dim \\mathcal{N}(A) = k$. The intersection of a $k$-dimensional subspace with a hyperplane results in a subspace of dimension either $k$ or $k-1$.\n- The dimension is $k$ if $\\mathcal{N}(A)$ is a subspace of $\\{v\\}^\\perp$. This occurs if and only if every vector in $\\mathcal{N}(A)$ is orthogonal to $v$, which means $v \\in (\\mathcal{N}(A))^\\perp$.\n- The dimension is $k-1$ if $\\mathcal{N}(A)$ is not a subspace of $\\{v\\}^\\perp$. This occurs if and only if there is at least one vector in $\\mathcal{N}(A)$ that is not orthogonal to $v$, which means $v \\notin (\\mathcal{N}(A))^\\perp$.\n\nBy the fundamental theorem of linear algebra, the orthogonal complement of the null space of a matrix is the range of its transpose: $(\\mathcal{N}(A))^\\perp = \\mathcal{R}(A^\\top)$.\n\nSo, the condition $\\dim \\mathcal{N}(A') = \\dim \\mathcal{N}(A) - 1$ holds if and only if $v \\notin \\mathcal{R}(A^\\top)$.\n\n**Conclusion**:\nCombining the findings from both cases, the necessary and sufficient conditions for the dimension of the null space to strictly decrease by one, i.e., $\\dim \\mathcal{N}(A + u v^\\top) = \\dim \\mathcal{N}(A) - 1$, are:\n1.  The vector $u$ must not be in the range (column space) of $A$: $u \\notin \\mathcal{R}(A)$.\n2.  The vector $v$ must not be in the range of the transpose of $A$ (row space of $A$): $v \\notin \\mathcal{R}(A^\\top)$.\n\n### Part 2: Numerical Computation\n\nThe implementation computes the null spaces and their dimensions for $A$ and $A' = A + uv^\\top$ for each test case.\n\n**Null Space Computation**: The null space of a matrix $M \\in \\mathbb{R}^{n \\times n}$ is computed via its Singular Value Decomposition (SVD), $M = U S V^\\top$. The right singular vectors (columns of $V$, or rows of $V^\\top$) corresponding to singular values that are numerically zero form an orthonormal basis for the null space $\\mathcal{N}(M)$. A singular value $\\sigma_i$ is considered numerically zero if it is smaller than a tolerance $\\tau = n \\cdot \\varepsilon \\cdot \\sigma_{\\max}$, where $\\varepsilon$ is the machine epsilon and $\\sigma_{\\max}$ is the largest singular value of $M$.\n\n**Principal Angle Computation**: Given orthonormal bases $Q_1$ and $Q_2$ for two subspaces $\\mathcal{S}_1$ and $\\mathcal{S}_2$, the cosines of the principal angles between them are the singular values of the matrix $Q_1^\\top Q_2$. The largest principal angle $\\theta_{\\max}$ corresponds to the smallest cosine. Thus, $\\theta_{\\max} = \\arccos(\\min(\\mathrm{svd}(Q_1^\\top Q_2)))$. If either subspace is trivial (dimension zero), the angle is taken to be $0.0$ by convention.\n\n**Program Logic**:\n1. For each test case $(A, u, v)$:\n2. Construct $A' = A + u v^\\top$.\n3. Define a function to compute the null space dimension and an orthonormal basis matrix for a given matrix using SVD and the specified tolerance.\n4. Apply this function to both $A$ and $A'$ to get $(\\dim \\mathcal{N}(A), Q_A)$ and $(\\dim \\mathcal{N}(A'), Q_{A'})$.\n5. Calculate the change in dimension $\\Delta = \\dim \\mathcal{N}(A') - \\dim \\mathcal{N}(A)$.\n6. If either null space has dimension $0$, set $\\theta_{\\max}=0.0$.\n7. Otherwise, compute the SVD of $Q_A^\\top Q_{A'}$ to find the singular values (cosines).\n8. The largest principal angle is the arccosine of the smallest singular value.\n9. Store the pair $[\\Delta, \\theta_{\\max}]$ for the current test case.\n10. After processing all cases, format the results into the required string format `[[d1,t1],[d2,t2],...]` with $\\theta_{\\max}$ rounded to six decimal places.", "answer": "```python\nimport numpy as np\nimport scipy.linalg\n\ndef solve():\n    \"\"\"\n    Solves the two-part problem: derivation (in text) and numerical experiment (in code).\n    This function implements the numerical experiment for the given test suite.\n    \"\"\"\n\n    def get_null_space_info(matrix):\n        \"\"\"\n        Computes the dimension and an orthonormal basis for the null space of a matrix\n        using Singular Value Decomposition (SVD).\n\n        Args:\n            matrix (np.ndarray): The input matrix.\n\n        Returns:\n            tuple: A tuple containing:\n                - int: The dimension of the null space.\n                - np.ndarray: A matrix whose columns form an orthonormal basis for the\n                              null space. Shape is (n, dim). If dim is 0, shape is (n, 0).\n        \"\"\"\n        m, n = matrix.shape\n        if n == 0:\n            return 0, np.empty((m, 0))\n\n        # full_matrices=True ensures Vh is always n x n\n        try:\n            _, s, vh = scipy.linalg.svd(matrix, full_matrices=True)\n        except np.linalg.LinAlgError:\n            # Should not occur with the test cases, but good practice\n            return 0, np.empty((n, 0))\n            \n        # Determine the tolerance for identifying zero singular values\n        sigma_max = s[0] if s.size > 0 else 1.0\n        # If the matrix is numerically zero, sigma_max will be close to zero.\n        # Use 1.0 as a floor to prevent a vanishingly small tolerance.\n        if sigma_max < np.finfo(float).eps:\n            sigma_max = 1.0\n        \n        # Per problem specification\n        tol = n * np.finfo(float).eps * sigma_max\n\n        # The number of singular values less than or equal to the tolerance\n        # The SVD in SciPy returns min(m, n) singular values. In this problem, m=n.\n        num_zero_s = np.sum(s <= tol)\n\n        if num_zero_s == 0:\n            return 0, np.empty((n, 0))\n        else:\n            # The basis vectors for the null space are the last 'num_zero_s' columns of V,\n            # which correspond to the last 'num_zero_s' rows of Vh.\n            null_space_basis_rows = vh[-num_zero_s:]\n            # Return as a matrix with orthonormal columns\n            return num_zero_s, null_space_basis_rows.T\n\n    def analyze_case(A, u, v):\n        \"\"\"\n        Performs the numerical analysis for a single test case.\n        \"\"\"\n        # Form the updated matrix A' = A + u v^T\n        A_prime = A + np.outer(u, v)\n\n        # Compute null space info for A\n        dim_A, Q_A = get_null_space_info(A)\n        \n        # Compute null space info for A'\n        dim_A_prime, Q_A_prime = get_null_space_info(A_prime)\n        \n        # Task: Compute Delta = dim N(A') - dim N(A)\n        delta = dim_A_prime - dim_A\n        \n        # Task: Compute the largest principal angle between N(A) and N(A')\n        if dim_A == 0 or dim_A_prime == 0:\n            # By convention, if one space is trivial, the angle is 0.\n            theta_max = 0.0\n        else:\n            # Cosines of principal angles are the singular values of Q_A^T * Q_A_prime\n            M = Q_A.T @ Q_A_prime\n            \n            # We only need the singular values, not U or Vh\n            s_M = scipy.linalg.svd(M, compute_uv=False)\n            \n            if s_M.size == 0:\n                # This case implies one of the spaces was trivial, handled above.\n                # However, it's a safe fallback.\n                theta_max = 0.0\n            else:\n                # The largest angle corresponds to the smallest cosine.\n                cos_theta_min = np.min(s_M)\n                # Clip to handle potential floating-point inaccuracies > 1.0\n                theta_max = np.arccos(np.clip(cos_theta_min, -1.0, 1.0))\n\n        return [delta, theta_max]\n\n    test_cases = [\n        (np.diag([0., 1., 1., 1.]), np.array([1., 1., 0., 0.]), np.array([1., 0., 0., 0.])),\n        (np.diag([0., 1., 1., 1.]), np.array([1., 1., 0., 0.]), np.array([0., 1., 0., 0.])),\n        (np.diag([0., 1., 1., 1.]), np.array([0., 1., 0., 0.]), np.array([1., 0., 0., 0.])),\n        (np.diag([0., 1., 1., 1.]), np.array([0., 1., 0., 0.]), np.array([0., -1., 0., 0.])),\n        (np.diag([0., 1., 1., 1.]), np.array([0., 1., 0., 0.]), np.array([1., -1., 0., 0.])),\n        (np.diag([0., 0., 1., 1., 1.]), np.array([1., 1., 0., 0., 0.]), np.array([1., 0., 0., 0., 0.])),\n    ]\n\n    results = []\n    for A, u, v in test_cases:\n        result = analyze_case(A, u, v)\n        results.append(result)\n\n    # Format the final output string as specified\n    formatted_results = [f\"[{d},{t:.6f}]\" for d, t in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3571057"}]}