{"hands_on_practices": [{"introduction": "A cornerstone of numerical linear algebra is understanding how matrix factorizations reveal fundamental properties of the vector spaces associated with a matrix. This first practice connects the full QR factorization directly to the geometry of the column space $\\mathcal{R}(A)$ and its orthogonal complement $\\mathcal{R}(A)^\\perp$. By working through this derivation, you will solidify your understanding of how an orthonormal basis for these two fundamental subspaces is explicitly constructed by the factors of $A = QR$ [@problem_id:3555901].", "problem": "Let $A \\in \\mathbb{R}^{m \\times n}$ be a tall-skinny matrix with $m \\gg n$, and assume the columns of $A$ are linearly independent so that $\\operatorname{rank}(A) = n$. Consider the full orthogonal-triangular (QR) factorization (QR) of $A$, written as $A = Q R$, where $Q \\in \\mathbb{R}^{m \\times m}$ is orthogonal and $R \\in \\mathbb{R}^{m \\times n}$ is upper triangular in its leading $n \\times n$ block and has zeros in its bottom $(m-n) \\times n$ block. Partition $Q = [\\,Q_{1}\\ \\ Q_{2}\\,]$ with $Q_{1} \\in \\mathbb{R}^{m \\times n}$ and $Q_{2} \\in \\mathbb{R}^{m \\times (m-n)}$. Using only foundational definitions of linear independence, span, basis, dimension, orthogonal matrices, and orthogonal complements, derive the dimension of the orthogonal complement of the range (column space) of $A$, denoted $\\dim \\mathcal{R}(A)^{\\perp}$, and explain how $Q$ from $A = Q R$ provides a basis for $\\mathcal{R}(A)^{\\perp}$. Your final reported answer must be a single closed-form expression in terms of $m$ and $n$ for $\\dim \\mathcal{R}(A)^{\\perp}$. No numerical rounding is required.", "solution": "The problem is subjected to validation.\n\n### Step 1: Extract Givens\n-   $A \\in \\mathbb{R}^{m \\times n}$ is a tall-skinny matrix, meaning $m \\gg n$.\n-   The columns of $A$ are linearly independent.\n-   $\\operatorname{rank}(A) = n$.\n-   The full orthogonal-triangular (QR) factorization of $A$ is $A = Q R$.\n-   $Q \\in \\mathbb{R}^{m \\times m}$ is an orthogonal matrix.\n-   $R \\in \\mathbb{R}^{m \\times n}$ is upper triangular in its leading $n \\times n$ block and has zeros in its bottom $(m-n) \\times n$ block.\n-   $Q$ is partitioned as $Q = [\\,Q_{1}\\ \\ Q_{2}\\,]$, where $Q_{1} \\in \\mathbb{R}^{m \\times n}$ and $Q_{2} \\in \\mathbb{R}^{m \\times (m-n)}$.\n-   The task is to derive $\\dim \\mathcal{R}(A)^{\\perp}$ and explain how $Q$ provides a basis for $\\mathcal{R}(A)^{\\perp}$ using only foundational definitions.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is well-defined within the mathematical framework of linear algebra. All terms, such as range ($\\mathcal{R}(A)$), rank, QR factorization, orthogonal complement ($\\perp$), dimension ($\\dim$), and linear independence, are standard and precisely defined. The given conditions, including that $A$ is a full-rank tall-skinny matrix, are consistent and form a complete basis for a solvable problem. The problem does not violate any scientific principles, is not based on false premises, and requires a rigorous derivation from first principles, making it a valid and substantive exercise.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\n### Derivation\n\nThe range, or column space, of the matrix $A \\in \\mathbb{R}^{m \\times n}$ is denoted by $\\mathcal{R}(A)$ and is defined as the span of its column vectors. Let the columns of $A$ be $\\mathbf{a}_1, \\mathbf{a}_2, \\ldots, \\mathbf{a}_n$, where each $\\mathbf{a}_j \\in \\mathbb{R}^m$. Then,\n$$\n\\mathcal{R}(A) = \\operatorname{span}\\{\\mathbf{a}_1, \\mathbf{a}_2, \\ldots, \\mathbf{a}_n\\}\n$$\nWe are given that the columns of $A$ are linearly independent. By definition, a basis for a vector space is a set of linearly independent vectors that spans the space. Therefore, the set of column vectors $\\{\\mathbf{a}_1, \\mathbf{a}_2, \\ldots, \\mathbf{a}_n\\}$ forms a basis for $\\mathcal{R}(A)$.\n\nThe dimension of a vector space is the number of vectors in any of its bases. Since the basis for $\\mathcal{R}(A)$ contains $n$ vectors, its dimension is $n$.\n$$\n\\dim \\mathcal{R}(A) = n\n$$\nThis is consistent with the given information that $\\operatorname{rank}(A) = n$, as the rank of a matrix is equal to the dimension of its column space.\n\nThe orthogonal complement of a subspace $W$ of $\\mathbb{R}^m$, denoted $W^{\\perp}$, is the set of all vectors in $\\mathbb{R}^m$ that are orthogonal to every vector in $W$. For the subspace $\\mathcal{R}(A)$, we have:\n$$\n\\mathcal{R}(A)^{\\perp} = \\{ \\mathbf{v} \\in \\mathbb{R}^m \\mid \\mathbf{v}^T \\mathbf{w} = 0 \\text{ for all } \\mathbf{w} \\in \\mathcal{R}(A) \\}\n$$\nA fundamental theorem of linear algebra, often called the Orthogonal Decomposition Theorem, states that for any subspace $W$ of $\\mathbb{R}^m$, the entire space can be expressed as the direct sum of the subspace and its orthogonal complement: $\\mathbb{R}^m = W \\oplus W^{\\perp}$. This implies a relationship between their dimensions:\n$$\n\\dim(\\mathbb{R}^m) = \\dim(W) + \\dim(W^{\\perp})\n$$\nApplying this to our subspace $\\mathcal{R}(A)$, we have:\n$$\n\\dim(\\mathbb{R}^m) = \\dim(\\mathcal{R}(A)) + \\dim(\\mathcal{R}(A)^{\\perp})\n$$\nThe dimension of the space $\\mathbb{R}^m$ is $m$. Substituting the known dimensions, we get:\n$$\nm = n + \\dim(\\mathcal{R}(A)^{\\perp})\n$$\nSolving for the dimension of the orthogonal complement gives:\n$$\n\\dim \\mathcal{R}(A)^{\\perp} = m - n\n$$\nThis is the dimension of the orthogonal complement of the range of $A$.\n\nNext, we must explain how the full QR factorization $A = QR$ provides a basis for $\\mathcal{R}(A)^{\\perp}$. The factorization is given by:\n$$\nA = Q R = [\\,Q_1\\ \\ Q_2\\,] \\begin{pmatrix} R_1 \\\\ 0 \\end{pmatrix}\n$$\nwhere $R_1$ is an $n \\times n$ upper triangular matrix and $0$ is the $(m-n) \\times n$ block of zeros. Performing the block matrix multiplication yields:\n$$\nA = Q_1 R_1 + Q_2 \\cdot 0 = Q_1 R_1\n$$\nThis equation shows that each column of $A$ is a linear combination of the columns of $Q_1$. Therefore, the column space of $A$ is a subspace of the column space of $Q_1$, i.e., $\\mathcal{R}(A) \\subseteq \\mathcal{R}(Q_1)$. Since $\\operatorname{rank}(A) = n$, the matrix $R_1$ is invertible. This allows us to write $Q_1 = A R_1^{-1}$, which shows that each column of $Q_1$ is a linear combination of the columns of $A$, implying $\\mathcal{R}(Q_1) \\subseteq \\mathcal{R}(A)$. Together, these two inclusions prove that the column spaces are identical:\n$$\n\\mathcal{R}(A) = \\mathcal{R}(Q_1)\n$$\nThe matrix $Q \\in \\mathbb{R}^{m \\times m}$ is orthogonal. By definition, its columns form an orthonormal basis for $\\mathbb{R}^m$. The orthogonality condition is $Q^T Q = I_m$, where $I_m$ is the $m \\times m$ identity matrix. Using the partitioned form of $Q$:\n$$\nQ^T Q = \\begin{pmatrix} Q_1^T \\\\ Q_2^T \\end{pmatrix} [\\,Q_1\\ \\ Q_2\\,] = \\begin{pmatrix} Q_1^T Q_1 & Q_1^T Q_2 \\\\ Q_2^T Q_1 & Q_2^T Q_2 \\end{pmatrix} = \\begin{pmatrix} I_n & 0 \\\\ 0 & I_{m-n} \\end{pmatrix}\n$$\nFrom this block matrix identity, we extract the sub-matrix equation $Q_2^T Q_1 = 0$. This equation signifies that every column of $Q_2$ is orthogonal to every column of $Q_1$.\n\nLet $\\mathbf{v}$ be an arbitrary vector in the span of the columns of $Q_2$, so $\\mathbf{v} \\in \\mathcal{R}(Q_2)$. Let $\\mathbf{w}$ be an arbitrary vector in the span of the columns of $Q_1$, so $\\mathbf{w} \\in \\mathcal{R}(Q_1)$. Then $\\mathbf{v}$ can be written as $\\mathbf{v} = Q_2 \\mathbf{c}$ for some coefficient vector $\\mathbf{c} \\in \\mathbb{R}^{m-n}$, and $\\mathbf{w}$ can be written as $\\mathbf{w} = Q_1 \\mathbf{d}$ for some $\\mathbf{d} \\in \\mathbb{R}^n$. Their inner product is:\n$$\n\\mathbf{v}^T \\mathbf{w} = (Q_2 \\mathbf{c})^T (Q_1 \\mathbf{d}) = \\mathbf{c}^T Q_2^T Q_1 \\mathbf{d} = \\mathbf{c}^T (0) \\mathbf{d} = 0\n$$\nThis shows that every vector in $\\mathcal{R}(Q_2)$ is orthogonal to every vector in $\\mathcal{R}(Q_1)$. Since we established $\\mathcal{R}(A) = \\mathcal{R}(Q_1)$, it follows that every vector in $\\mathcal{R}(Q_2)$ is orthogonal to every vector in $\\mathcal{R}(A)$. By the definition of the orthogonal complement, this means $\\mathcal{R}(Q_2) \\subseteq \\mathcal{R}(A)^{\\perp}$.\n\nThe columns of $Q_2$ are a subset of the columns of the orthogonal matrix $Q$. Therefore, the columns of $Q_2$ are themselves orthonormal and, consequently, linearly independent. The number of columns in $Q_2 \\in \\mathbb{R}^{m \\times (m-n)}$ is $m - n$.\nWe have thus found a set of $m - n$ linearly independent vectors (the columns of $Q_2$) that all belong to the subspace $\\mathcal{R}(A)^{\\perp}$. Since we previously derived that the dimension of this subspace is $\\dim \\mathcal{R}(A)^{\\perp} = m - n$, this set of vectors is not just a linearly independent set within the subspace, but must form a basis for it.\n\nIn conclusion, the columns of the matrix $Q_2$ from the full QR factorization of $A$ form an orthonormal basis for the orthogonal complement of the range of $A$, $\\mathcal{R}(A)^{\\perp}$.", "answer": "$$\n\\boxed{m-n}\n$$", "id": "3555901"}, {"introduction": "In theory, determining if a set of vectors is linearly independent is a binary question, often answered by checking if a determinant is non-zero. In finite-precision arithmetic, this concept becomes fuzzy, leading to the more practical notion of *numerical rank*. This exercise demonstrates through code why naive tests based on determinants are unreliable and introduces the industry-standard, robust methods for diagnosing numerical rank using column-pivoted QR factorization and the Singular Value Decomposition (SVD) [@problem_id:3555833].", "problem": "You are given that numerical linear algebra is carried out in finite-precision floating-point arithmetic with machine epsilon denoted by $\\epsilon_{\\mathrm{mach}}$. The foundational definitions to use are as follows. A set of columns of a matrix $A \\in \\mathbb{R}^{m \\times n}$ is linearly independent if the only solution to $A x = 0$ is $x = 0$. The column space (span) of $A$ is the set of all linear combinations of its columns. The dimension of the column space is the rank of $A$. The determinant of a square matrix $A \\in \\mathbb{R}^{n \\times n}$ equals the product of its singular values and scales multiplicatively with column scalings. A robust diagnostic for the numerical rank uses orthogonal factorizations or the singular value decomposition (SVD). In a column-pivoted QR decomposition $A P = Q R$, where $P$ is a permutation matrix, $Q$ is orthogonal, and $R$ is upper trapezoidal, the diagonal entries of $R$ typically decay and reveal rank. In the singular value decomposition $A = U \\Sigma V^\\top$, the diagonal of $\\Sigma$ contains the singular values $\\sigma_i$ sorted in nonincreasing order. The recommended numerical rank threshold is $ \\tau_{\\mathrm{rel}} = \\max(m,n)\\,\\epsilon_{\\mathrm{mach}}\\,\\|A\\|_2$, where $\\|A\\|_2 = \\sigma_{\\max}(A)$ is the spectral norm. This threshold compares magnitudes relative to scale, unlike a fixed absolute tolerance.\n\nImplement a program that, for a given test suite of matrices $A$, performs three diagnostics:\n\n- A naive determinant-based independence test:\n  - If $A$ is square, compute $d = \\det(A)$ and declare the columns independent if $|d| > \\theta$, where $\\theta = 10^{-15}$ is a fixed absolute tolerance.\n  - If $A$ is rectangular with $m \\ge n$, compute $g = \\det(A^\\top A)$ and declare the columns independent if $|g| > \\theta$, using the same fixed absolute tolerance $\\theta = 10^{-15}$.\n- A robust numerical rank via column-pivoted QR: compute a column-pivoted QR factorization $A P = Q R$. Define $\\tau_{\\mathrm{QR}} = \\max(m,n)\\,\\epsilon_{\\mathrm{mach}}\\,|R_{11}|$. Let $r_{\\mathrm{QR}}$ be the count of diagonal entries $|R_{ii}|$ strictly greater than $\\tau_{\\mathrm{QR}}$.\n- A robust numerical rank via SVD: compute the singular values $\\{\\sigma_i\\}$ of $A$. Define $\\tau_{\\mathrm{SVD}} = \\max(m,n)\\,\\epsilon_{\\mathrm{mach}}\\,\\sigma_{\\max}$. Let $r_{\\mathrm{SVD}}$ be the count of singular values $\\sigma_i$ strictly greater than $\\tau_{\\mathrm{SVD}}$.\n\nThe goal is to construct and evaluate scenarios in which naive determinant-based tests are misleading due to floating-point scaling, while the robust diagnostics based on pivoted QR and SVD provide scale-aware assessments of numerical independence.\n\nTest Suite. Use the following matrices (all entries are in base-$10$ scientific notation):\n\n- Case $\\mathbf{A_1}$ (square, full algebraic rank but tiny determinant): \n  $$A_1 = \\begin{bmatrix}\n  1 & 1 & 1 \\\\\n  0 & 10^{-12} & 0 \\\\\n  0 & 0 & 10^{-12}\n  \\end{bmatrix} \\in \\mathbb{R}^{3 \\times 3}.$$\n  The algebraic rank is $3$, and $\\det(A_1) = 10^{-24}$.\n\n- Case $\\mathbf{A_2}$ (square, extreme column scaling, large absolute determinant but numerically rank-deficient under relative thresholds):\n  $$A_2 = \\mathrm{diag}\\!\\left(10^{16}, 1, 10^{-16}\\right) \\in \\mathbb{R}^{3 \\times 3}.$$\n  The algebraic rank is $3$, $\\det(A_2) = 1$, and the singular values are exactly $\\{10^{16}, 1, 10^{-16}\\}$.\n\n- Case $\\mathbf{A_3}$ (rectangular, uniform scaling of a well-conditioned tall matrix; naive Gram determinant is tiny purely by absolute scaling, but the numerical rank is full relative to scale):\n  $$A_3 = 10^{-10} \\cdot \\begin{bmatrix}\n  1 & 0 & 1 \\\\\n  0 & 1 & 1 \\\\\n  0 & 0 & 1 \\\\\n  0 & 0 & 1 \\\\\n  0 & 0 & 1\n  \\end{bmatrix} \\in \\mathbb{R}^{5 \\times 3}.$$\n  The algebraic rank is $3$. Note that $A_3^\\top A_3$ scales by $10^{-20}$ and its determinant scales by $10^{-60}$.\n\nFor each case, your program should produce a list with three components:\n- The boolean result of the naive determinant-based independence test described above.\n- The integer numerical rank $r_{\\mathrm{QR}}$ from column-pivoted QR with the threshold $\\tau_{\\mathrm{QR}}$.\n- The integer numerical rank $r_{\\mathrm{SVD}}$ from SVD with the threshold $\\tau_{\\mathrm{SVD}}$.\n\nFinal Output Format. Aggregate the results for all test cases into a single Python-style list of lists on one line, in the order $[A_1, A_2, A_3]$. Each inner list must be of the form $[\\text{naive}, r_{\\mathrm{QR}}, r_{\\mathrm{SVD}}]$. Your program should produce a single line of output containing this aggregated list, for example, a format analogous to $[ [\\text{b}, k, \\ell], [\\text{b}, k, \\ell], [\\text{b}, k, \\ell] ]$ where $\\text{b}$ is a boolean and $k, \\ell$ are integers. No physical units are required, and there are no angles or percentages in this task.", "solution": "The problem is valid as it is scientifically grounded in the principles of numerical linear algebra, is mathematically well-posed with all necessary information provided, and is stated objectively. We proceed with the solution.\n\nThe core of this problem is to contrast naive, scale-dependent tests for linear independence with robust, scale-aware numerical methods. We are asked to evaluate three matrices using three different diagnostics: a determinant-based test with a fixed absolute threshold, a rank test based on column-pivoted QR factorization, and a rank test based on the Singular Value Decomposition (SVD). The machine epsilon for standard double-precision floating-point arithmetic, $\\epsilon_{\\mathrm{mach}}$, is approximately $2.22 \\times 10^{-16}$.\n\n**Diagnostic Methodologies**\n\n1.  **Naive Determinant Test**: This test leverages the property that a set of $n$ vectors in $\\mathbb{R}^n$ is linearly independent if and only if the determinant of the matrix formed by these vectors is non-zero. For a rectangular $m \\times n$ matrix $A$ (with $m \\ge n$), the columns are linearly independent if and only if the Gram matrix $A^\\top A$ is nonsingular, i.e., $\\det(A^\\top A) \\ne 0$. In finite precision, we test if $|\\det| > \\theta$ for a fixed, absolute tolerance $\\theta = 10^{-15}$. This method is notoriously unreliable because the determinant is highly sensitive to the scaling of the matrix columns. For a matrix $A \\in \\mathbb{R}^{n \\times n}$, if one column is scaled by a factor $c$, the determinant is also scaled by $c$. For the Gram matrix determinant, if $A \\in \\mathbb{R}^{m \\times n}$ is scaled to $c A$, then $\\det((cA)^\\top(cA)) = \\det(c^2 A^\\top A) = (c^2)^n \\det(A^\\top A) = c^{2n} \\det(A^\\top A)$. This extreme sensitivity to scaling can yield values that are vanishingly small or astronomically large, even for well-conditioned matrices, leading to incorrect conclusions when compared against a fixed threshold.\n\n2.  **Robust QR Rank Test**: The column-pivoted QR factorization, $AP = QR$, decomposes a matrix $A \\in \\mathbb{R}^{m \\times n}$ into an orthogonal matrix $Q$, an upper-trapezoidal matrix $R$, and a permutation matrix $P$ that reorders the columns of $A$. At each step of the factorization, the column with the largest remaining norm is chosen as the next pivot. This heuristic ensures that the magnitudes of the diagonal entries of $R$ are non-increasing: $|R_{11}| \\ge |R_{22}| \\ge \\dots$. The numerical rank is estimated by counting the number of diagonal entries $|R_{ii}|$ that are significantly larger than zero, relative to the scale of the matrix. The specified threshold is $\\tau_{\\mathrm{QR}} = \\max(m,n)\\,\\epsilon_{\\mathrm{mach}}\\,|R_{11}|$. Since column pivoting makes $|R_{11}|$ a good estimate for the norm of $A$, this threshold is adaptive to the matrix's scale. The rank $r_{\\mathrm{QR}}$ is the number of indices $i$ for which $|R_{ii}| > \\tau_{\\mathrm{QR}}$.\n\n3.  **Robust SVD Rank Test**: The Singular Value Decomposition, $A = U \\Sigma V^\\top$, is the most reliable tool for determining numerical rank. It decomposes $A \\in \\mathbb{R}^{m \\times n}$ into orthogonal matrices $U$ and $V$, and a diagonal matrix $\\Sigma$ whose entries $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge \\sigma_{\\min(m,n)} \\ge 0$ are the singular values of $A$. The singular values are the lengths of the semi-axes of the hyper-ellipsoid obtained by transforming the unit sphere by $A$. They provide a stable and fundamental measure of the matrix's action on vector spaces. The numerical rank is the number of singular values that are significant relative to the largest singular value, $\\sigma_{\\max} = \\|A\\|_2$. The specified threshold is $\\tau_{\\mathrm{SVD}} = \\max(m,n)\\,\\epsilon_{\\mathrm{mach}}\\,\\sigma_{\\max}$. The rank $r_{\\mathrm{SVD}}$ is the count of singular values $\\sigma_i$ such that $\\sigma_i > \\tau_{\\mathrm{SVD}}$.\n\nWe now apply these three diagnostics to each test case.\n\n**Case $\\mathbf{A_1}$**\nThe matrix is $A_1 = \\begin{bmatrix} 1 & 1 & 1 \\\\ 0 & 10^{-12} & 0 \\\\ 0 & 0 & 10^{-12} \\end{bmatrix}$. Here, $m=3$, $n=3$.\n- **Naive Test**: $A_1$ is square, so we compute its determinant. $\\det(A_1) = 1 \\cdot (10^{-12} \\cdot 10^{-12} - 0) = 10^{-24}$. Comparing this to the fixed threshold $\\theta = 10^{-15}$, we find $|\\det(A_1)| = 10^{-24} \\le 10^{-15}$. The naive test incorrectly concludes the columns are linearly dependent. Result: `False`.\n- **QR Rank Test**: Even though the matrix is ill-conditioned, the columns are numerically independent. Column-pivoted QR factorization yields an upper-triangular matrix $R$. The largest diagonal element is $|R_{11}| \\approx \\sqrt{1^2+1^2+1^2} \\approx 1.732$. The threshold is $\\tau_{\\mathrm{QR}} = 3 \\cdot \\epsilon_{\\mathrm{mach}} \\cdot |R_{11}| \\approx 3 \\cdot (2.22 \\times 10^{-16}) \\cdot 1.732 \\approx 1.15 \\times 10^{-15}$. The absolute values of the diagonal entries of $R$ are approximately $\\{1.732, 1.22 \\times 10^{-12}, 1.15 \\times 10^{-12}\\}$.\n  - $|R_{11}| \\approx 1.732 > 1.15 \\times 10^{-15}$ (True)\n  - $|R_{22}| \\approx 1.22 \\times 10^{-12} > 1.15 \\times 10^{-15}$ (True)\n  - $|R_{33}| \\approx 1.15 \\times 10^{-12} > 1.15 \\times 10^{-15}$ (True)\n  All three diagonal entries exceed the relative threshold. Thus, the numerical rank is correctly identified as $3$. The result is $r_{\\mathrm{QR}} = 3$.\n- **SVD Rank Test**: The singular values of $A_1$ are approximately $\\sigma = \\{1.732, 10^{-12}, 10^{-12}\\}$. The largest is $\\sigma_{\\max} \\approx 1.732$. The threshold is $\\tau_{\\mathrm{SVD}} = 3 \\cdot \\epsilon_{\\mathrm{mach}} \\cdot \\sigma_{\\max} \\approx 1.15 \\times 10^{-15}$.\n  - $\\sigma_1 \\approx 1.732 > 1.15 \\times 10^{-15}$ (True)\n  - $\\sigma_2 = 10^{-12} > 1.15 \\times 10^{-15}$ (True)\n  - $\\sigma_3 = 10^{-12} > 1.15 \\times 10^{-15}$ (True)\n  All three singular values are above the threshold. SVD correctly identifies the numerical rank as $3$. Thus, $r_{\\mathrm{SVD}} = 3$.\n- **Result for $A_1$**: `[False, 3, 3]`.\n\n**Case $\\mathbf{A_2}$**\nThe matrix is $A_2 = \\mathrm{diag}(10^{16}, 1, 10^{-16})$. Here, $m=3$, $n=3$.\n- **Naive Test**: $A_2$ is square. $\\det(A_2) = 10^{16} \\cdot 1 \\cdot 10^{-16} = 1$. Since $|\\det(A_2)| = 1 > 10^{-15}$, the naive test concludes the columns are independent. Result: `True`. This is algebraically correct but numerically misleading given the extreme conditioning.\n- **QR Rank Test**: Since $A_2$ is diagonal with entries sorted by magnitude, the column-pivoted QR results in $P=I, Q=I, R=A_2$. The diagonal entries of $R$ are $\\{10^{16}, 1, 10^{-16}\\}$. The largest is $|R_{11}| = 10^{16}$. The threshold is $\\tau_{\\mathrm{QR}} = 3 \\cdot \\epsilon_{\\mathrm{mach}} \\cdot |R_{11}| = 3 \\cdot (2.22 \\times 10^{-16}) \\cdot 10^{16} \\approx 6.66$.\n  - $|R_{11}| = 10^{16} > 6.66$ (True)\n  - $|R_{22}| = 1 > 6.66$ (False)\n  - $|R_{33}| = 10^{-16} > 6.66$ (False)\n  Only one diagonal element exceeds the relative threshold. Thus, $r_{\\mathrm{QR}} = 1$.\n- **SVD Rank Test**: For a positive diagonal matrix, the singular values are the diagonal entries. $\\sigma = \\{10^{16}, 1, 10^{-16}\\}$. The largest is $\\sigma_{\\max} = 10^{16}$. The threshold is $\\tau_{\\mathrm{SVD}} = 3 \\cdot \\epsilon_{\\mathrm{mach}} \\cdot \\sigma_{\\max} \\approx 6.66$.\n  - $\\sigma_1 = 10^{16} > 6.66$ (True)\n  - $\\sigma_2 = 1 > 6.66$ (False)\n  - $\\sigma_3 = 10^{-16} > 6.66$ (False)\n  Only one singular value exceeds the threshold. The numerical rank is correctly identified as $1$. The condition number $\\sigma_{\\max}/\\sigma_{\\min}$ is $10^{32}$, indicating extreme ill-conditioning. Thus, $r_{\\mathrm{SVD}} = 1$.\n- **Result for $A_2$**: `[True, 1, 1]`.\n\n**Case $\\mathbf{A_3}$**\nThe matrix is $A_3 = 10^{-10} \\cdot B$, where $B = \\begin{bmatrix} 1&0&1 \\\\ 0&1&1 \\\\ 0&0&1 \\\\ 0&0&1 \\\\ 0&0&1 \\end{bmatrix}$. Here, $m=5$, $n=3$.\n- **Naive Test**: $A_3$ is rectangular. We compute $g = \\det(A_3^\\top A_3)$.\n  $A_3^\\top A_3 = (10^{-10})^2 B^\\top B = 10^{-20} B^\\top B$. The matrix $B^\\top B = \\begin{bmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\\\ 1 & 1 & 5 \\end{bmatrix}$ has $\\det(B^\\top B) = 3$.\n  Thus, $\\det(A_3^\\top A_3) = (10^{-20})^3 \\det(B^\\top B) = 3 \\times 10^{-60}$.\n  Since $|g| = 3 \\times 10^{-60} \\le 10^{-15}$, the naive test concludes the columns are linearly dependent. Result: `False`. This is incorrect and is caused by the overall small scaling of the matrix, not by any linear dependence.\n- **QR Rank Test**: The diagonal elements of $R$ from the QR factorization of $A_3$ will be $10^{-10}$ times the diagonal elements from the QR of $B$. For $B$, column pivoting yields diagonal entries for its $R$ factor of approximately $\\{-2.236, 1.183, 0.942\\}$. For $A_3$, the diagonal of its $R$ factor is approximately $\\{ -2.236 \\times 10^{-10}, 1.183 \\times 10^{-10}, 0.942 \\times 10^{-10} \\}$. The largest is $|R_{11}| \\approx 2.236 \\times 10^{-10}$. The threshold is $\\tau_{\\mathrm{QR}} = \\max(5,3) \\cdot \\epsilon_{\\mathrm{mach}} \\cdot |R_{11}| = 5 \\cdot (2.22 \\times 10^{-16}) \\cdot (2.236 \\times 10^{-10}) \\approx 2.48 \\times 10^{-25}$.\n  All three diagonal entries have magnitudes on the order of $10^{-10}$, which are far greater than $\\tau_{\\mathrm{QR}}$. Thus, $r_{\\mathrm{QR}} = 3$.\n- **SVD Rank Test**: The singular values scale linearly: $\\sigma_i(A_3) = 10^{-10} \\sigma_i(B)$. The singular values of $B$ are approximately $\\{2.418, 1.239, 0.811\\}$. So, the singular values of $A_3$ are $\\{2.418 \\times 10^{-10}, 1.239 \\times 10^{-10}, 0.811 \\times 10^{-10}\\}$. The largest is $\\sigma_{\\max} \\approx 2.418 \\times 10^{-10}$. The threshold is $\\tau_{\\mathrm{SVD}} = \\max(5,3) \\cdot \\epsilon_{\\mathrm{mach}} \\cdot \\sigma_{\\max} = 5 \\cdot (2.22 \\times 10^{-16}) \\cdot (2.418 \\times 10^{-10}) \\approx 2.68 \\times 10^{-25}$.\n  All three singular values of $A_3$ have magnitudes on the order of $10^{-10}$, well above the threshold. Thus, $r_{\\mathrm{SVD}} = 3$.\n- **Result for $A_3$**: `[False, 3, 3]`.\n\nThe final aggregated results demonstrate that the naive determinant test is unreliable, while the SVD and column-pivoted QR provide robust, scale-aware assessments of numerical rank.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import qr\n\ndef solve():\n    \"\"\"\n    Implements and compares three methods for diagnosing linear independence\n    and numerical rank for a suite of test matrices.\n    \"\"\"\n    \n    # Define machine epsilon for double precision floating point numbers.\n    epsilon_mach = np.finfo(float).eps\n    \n    # Define the fixed absolute tolerance for the naive determinant test.\n    theta = 1e-15\n\n    # Define the test suite of matrices.\n    A1 = np.array([\n        [1.0, 1.0, 1.0],\n        [0.0, 1e-12, 0.0],\n        [0.0, 0.0, 1e-12]\n    ])\n\n    A2 = np.diag([1e16, 1.0, 1e-16])\n\n    A3 = 1e-10 * np.array([\n        [1.0, 0.0, 1.0],\n        [0.0, 1.0, 1.0],\n        [0.0, 0.0, 1.0],\n        [0.0, 0.0, 1.0],\n        [0.0, 0.0, 1.0]\n    ])\n\n    test_cases = [A1, A2, A3]\n    \n    results = []\n    \n    for A in test_cases:\n        m, n = A.shape\n        \n        # 1. Naive determinant-based independence test\n        naive_result = False\n        if m == n:\n            # Square matrix case\n            det_val = np.linalg.det(A)\n            if np.abs(det_val) > theta:\n                naive_result = True\n        elif m > n:\n            # Rectangular matrix case (tall)\n            gram_det = np.linalg.det(A.T @ A)\n            if np.abs(gram_det) > theta:\n                naive_result = True\n        \n        # 2. Robust numerical rank via column-pivoted QR\n        _, R, _ = qr(A, pivoting=True)\n        # The diagonal of R from scipy.linalg.qr (m,n) is the first min(m,n) elements\n        R_diag_abs = np.abs(np.diag(R))\n        tau_qr = max(m, n) * epsilon_mach * R_diag_abs[0] if R_diag_abs.size > 0 else 0\n        # Count diagonal entries strictly greater than the threshold.\n        # Ensure we only check up to the number of columns.\n        r_qr = np.sum(R_diag_abs[:n] > tau_qr)\n        \n        # 3. Robust numerical rank via SVD\n        s = np.linalg.svd(A, compute_uv=False)\n        sigma_max = s[0] if s.size > 0 else 0\n        tau_svd = max(m, n) * epsilon_mach * sigma_max\n        # Count singular values strictly greater than the threshold.\n        r_svd = np.sum(s > tau_svd)\n        \n        results.append([naive_result, r_qr, r_svd])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3555833"}, {"introduction": "Beyond just determining the dimension of a subspace, a crucial task is to construct a stable orthonormal basis for it. This practice focuses on the Modified Gram-Schmidt (MGS) algorithm, a numerically superior alternative to its classical counterpart for building such a basis. You will implement MGS and use the deviation of the resulting basis from perfect orthonormality, quantified by the norm $\\|Q^\\top Q - I\\|_2$, as a powerful diagnostic tool to assess the conditioning and numerical independence of the original set of vectors [@problem_id:3555869].", "problem": "You are given a real matrix $A \\in \\mathbb{R}^{m \\times n}$ whose columns may be nearly linearly dependent. The task is to implement a numerically stable orthonormalization procedure based on the modified Gram–Schmidt algorithm to construct a matrix $Q \\in \\mathbb{R}^{m \\times n}$ whose columns span the same subspace as the columns of $A$ but are orthonormal whenever numerical independence allows. Use this construction to diagnose numerical independence by estimating the spectral norm $\\|Q^\\top Q - I\\|_2$, where $I \\in \\mathbb{R}^{n \\times n}$ is the identity matrix. The value $\\|Q^\\top Q - I\\|_2$ quantifies the deviation from exact orthonormality, and serves as a diagnostic: small values indicate numerically independent columns of $A$, while values near one or larger indicate loss of numerical independence (for example, exactly dependent or zero columns).\n\nFundamental base and definitions to be used:\n- Linear independence: columns $\\{a_1,\\dots,a_n\\}$ of $A$ are linearly independent if the only solution of $\\sum_{j=1}^n \\alpha_j a_j = 0$ is $\\alpha_j = 0$ for all $j$.\n- Span: the span of columns of $A$ is the set of all linear combinations of the columns of $A$.\n- Basis: a set of linearly independent vectors that spans a subspace is a basis of that subspace.\n- Dimension: the number of vectors in a basis of a subspace is its dimension.\n- Inner product: for $u,v \\in \\mathbb{R}^m$, $(u,v) = u^\\top v$.\n- Orthogonal projection: given an orthonormal set $\\{q_1,\\dots,q_k\\}$, the projection of $v$ onto $\\mathrm{span}\\{q_1,\\dots,q_k\\}$ is $\\sum_{i=1}^k (q_i^\\top v) q_i$.\n- Modified Gram–Schmidt (MGS): iteratively orthogonalize each column of $A$ against previously formed orthonormal vectors using inner products; when the residual norm is below a tolerance, treat the column as numerically dependent and set the corresponding column of $Q$ to the zero vector.\n- Spectral norm: for any matrix $M$, $\\|M\\|_2$ equals its largest singular value, a well-tested fact in numerical linear algebra.\n\nImplementation requirements:\n1. Implement the modified Gram–Schmidt procedure to produce $Q$ and $R$ from $A$, following the iterative orthogonalization based on inner products and residual normalization. If the residual norm $r_{jj} = \\|v_j\\|_2$ after orthogonalization of column $j$ is below a tolerance threshold, declare the column numerically dependent and set the corresponding column of $Q$ to the zero vector. Otherwise, set $q_j = v_j / r_{jj}$.\n2. The tolerance may be based on double-precision machine epsilon and the scale of $A$, e.g., depend on $m$, $n$, and $\\|A\\|_2$; ensure numerical robustness without relying on any untested shortcuts.\n3. Compute the diagnostic value $\\|Q^\\top Q - I\\|_2$, where $I$ is the identity matrix of size $n \\times n$. Use a stable method; noting that $\\|M\\|_2$ equals the largest singular value of $M$ is acceptable as it is a well-tested fact.\n4. For each test case, return the single float $\\|Q^\\top Q - I\\|_2$.\n\nTest suite:\nProvide the following matrices $A$, each encoded explicitly within your program:\n- Case $1$ (well-conditioned, numerically independent): $m = 5$, $n = 3$, with\n  $$\n  A_1 = \\begin{bmatrix}\n  1 & 0 & 0 \\\\\n  0 & 1 & 0 \\\\\n  0 & 0 & 1 \\\\\n  1 & 1 & 0 \\\\\n  0 & 1 & 1\n  \\end{bmatrix}.\n  $$\n- Case $2$ (nearly dependent columns): $m = 5$, $n = 3$, with columns\n  $$\n  a_1 = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 4 \\\\ 5 \\end{bmatrix},\\quad\n  a_2 = \\begin{bmatrix} 1.0000000001 \\\\ 2 \\\\ 2.9999999999 \\\\ 4 \\\\ 5.0000000001 \\end{bmatrix},\\quad\n  a_3 = \\begin{bmatrix} 5 \\\\ 4 \\\\ 3 \\\\ 2 \\\\ 1 \\end{bmatrix},\n  $$\n  and $A_2 = [a_1\\ a_2\\ a_3]$.\n- Case $3$ (exact dependence): $m = 5$, $n = 3$, with columns\n  $$\n  a_1 = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 4 \\\\ 5 \\end{bmatrix},\\quad\n  a_2 = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 4 \\\\ 5 \\end{bmatrix},\\quad\n  a_3 = \\begin{bmatrix} 2 \\\\ 1 \\\\ 0 \\\\ 1 \\\\ 2 \\end{bmatrix},\n  $$\n  and $A_3 = [a_1\\ a_2\\ a_3]$.\n- Case $4$ (ill-conditioned but full rank): $m = 4$, $n = 4$, the Hilbert matrix $H$ with entries\n  $$\n  (A_4)_{ij} = \\frac{1}{i + j - 1},\\quad 1 \\le i \\le 4,\\ 1 \\le j \\le 4.\n  $$\n- Case $5$ (zero column as an edge case): $m = 4$, $n = 3$, with\n  $$\n  A_5 = \\begin{bmatrix}\n  1 & 0 & 0 \\\\\n  0 & 1 & 0 \\\\\n  0 & 0 & 0 \\\\\n  0 & 0 & 0\n  \\end{bmatrix}.\n  $$\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, ordered as $[\\|Q_1^\\top Q_1 - I\\|_2,\\ \\|Q_2^\\top Q_2 - I\\|_2,\\ \\|Q_3^\\top Q_3 - I\\|_2,\\ \\|Q_4^\\top Q_4 - I\\|_2,\\ \\|Q_5^\\top Q_5 - I\\|_2]$, where $Q_k$ is the matrix computed from $A_k$ by your modified Gram–Schmidt implementation. Each entry must be a single floating-point number. No physical units or angle units are involved in this problem.", "solution": "The problem requires the implementation of the Modified Gram–Schmidt (MGS) algorithm to construct an orthonormal basis for the column space of a given matrix $A \\in \\mathbb{R}^{m \\times n}$. This process will be used to diagnose the numerical linear independence of the columns of $A$. The diagnostic metric is the spectral norm of the deviation from orthonormality, $\\|Q^\\top Q - I\\|_2$, where $Q$ is the computed matrix with orthonormal columns and $I$ is the $n \\times n$ identity matrix.\n\nThe core of the solution is the MGS algorithm, chosen for its superior numerical stability over the classical version. The algorithm iteratively generates a set of orthonormal vectors $\\{q_1, q_2, \\dots, q_n\\}$ that span the same space as the columns of $A$, $\\{a_1, a_2, \\dots, a_n\\}$.\n\nLet $A = [a_1 | a_2 | \\dots | a_n]$. We initialize a working matrix $V = A$. The MGS algorithm proceeds as follows for $j = 1, \\dots, n$:\n\n1.  **Normalization**: The current vector $v_j$ is normalized to produce the next orthonormal vector $q_j$. First, its Euclidean norm, $r_{jj} = \\|v_j\\|_2$, is computed.\n    -   If $r_{jj}$ is smaller than a pre-defined tolerance $\\tau$, the vector $a_j$ is considered numerically linearly dependent on the preceding vectors $\\{a_1, \\dots, a_{j-1}\\}$. In this case, the corresponding basis vector is set to zero, $q_j = 0$, to signify that this column does not increase the dimension of the span.\n    -   Otherwise, the vector is normalized: $q_j = v_j / r_{jj}$.\n\n2.  **Orthogonalization**: The remaining vectors in the working set, $\\{v_{j+1}, \\dots, v_n\\}$, are made orthogonal to the newly formed vector $q_j$. For each $k = j+1, \\dots, n$, we update $v_k$ by subtracting its projection onto $q_j$:\n    $$\n    r_{jk} = q_j^\\top v_k\n    $$\n    $$\n    v_k \\leftarrow v_k - r_{jk} q_j\n    $$\n    This immediate update of the remaining vectors is the key difference from Classical Gram-Schmidt and is the source of MGS's enhanced numerical stability. By orthogonalizing against $q_j$, we ensure that subsequent calculations for vector $v_k$ minimize the influence of components that are parallel to $q_j$.\n\nThe selection of the tolerance $\\tau$ is critical for robustly identifying numerical dependence. The problem statement specifies that it should be based on the matrix dimensions $m, n$, the machine epsilon $\\varepsilon$, and the scale of the matrix, such as $\\|A\\|_2$. A common and well-justified choice in numerical literature (e.g., Golub & Van Loan, \"Matrix Computations\") is to use a tolerance proportional to the initial norm of the matrix. We will use the Frobenius norm $\\|A\\|_F$ as a computationally efficient proxy for the spectral norm $\\|A\\|_2$. The tolerance is thus defined as:\n$$\n\\tau = \\sqrt{n} \\cdot \\varepsilon \\cdot \\|A\\|_F\n$$\nwhere $\\varepsilon$ is the double-precision machine epsilon. This formulation scales the tolerance with the size and magnitude of the input matrix, providing a robust threshold for determining if a residual norm is numerically zero.\n\nAfter computing the matrix $Q = [q_1 | q_2 | \\dots | q_n]$, we evaluate its deviation from perfect orthonormality. An ideal orthonormal matrix $Q$ satisfies $Q^\\top Q = I$. Numerical errors, particularly those amplified by near-linear dependence in $A$, cause the computed $Q$ to deviate from this property. The diagnostic metric $\\|Q^\\top Q - I\\|_2$ quantifies this deviation. The spectral norm, $\\|M\\|_2$, of a matrix $M$ is its largest singular value. For the symmetric matrix $Q^\\top Q - I$, this is equal to the maximum absolute eigenvalue, i.e., $\\|Q^\\top Q - I\\|_2 = \\max_i |\\lambda_i(Q^\\top Q - I)|$.\n\nIf the columns of $A$ are numerically independent and well-conditioned, the MGS algorithm produces a matrix $Q$ that is very close to orthonormal, and thus $\\|Q^\\top Q - I\\|_2$ will be a small value, on the order of $\\varepsilon$. If a column $a_j$ is detected as numerically dependent, the corresponding $q_j$ is set to the zero vector. This makes the $j^{th}$ row and column of $Q^\\top Q$ zero. Consequently, the $j^{th}$ diagonal element of $Q^\\top Q - I$ is $-1$, which implies that one of its eigenvalues is $-1$. Therefore, the spectral norm $\\|Q^\\top Q - I\\|_2$ will be at least $1$, correctly flagging the loss of full numerical rank. For nearly dependent columns that are not filtered by the tolerance, MGS will still produce a matrix $Q$ with a significant loss of orthogonality, resulting in a large value for the diagnostic norm.\n\nThe implementation will follow these steps:\n1.  Define a function to perform the Modified Gram–Schmidt orthonormalization with the specified tolerance logic.\n2.  For each test-case matrix $A_k$, compute the corresponding $Q_k$.\n3.  Compute the matrix $M_k = Q_k^\\top Q_k - I$.\n4.  Calculate the spectral norm $\\|M_k\\|_2$ using standard numerical library functions, which find the largest singular value.\n5.  Collect these diagnostic values for all test cases and format them as required.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by implementing the Modified Gram-Schmidt algorithm,\n    applying it to a suite of test cases, and computing a diagnostic\n    for numerical independence.\n    \"\"\"\n\n    def modified_gram_schmidt(A: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Computes an orthonormal basis Q for the column space of A using the\n        Modified Gram-Schmidt algorithm. Handles numerical dependence by\n        setting columns of Q to zero if the residual norm is below a tolerance.\n\n        Args:\n            A: The input matrix of shape (m, n).\n\n        Returns:\n            A matrix Q of shape (m, n) whose columns form an orthonormal basis\n            for the column space of A.\n        \"\"\"\n        m, n = A.shape\n        if n == 0:\n            return np.zeros((m, 0), dtype=float)\n\n        A_float = A.astype(float)\n        V = A_float.copy()\n        Q = np.zeros_like(A_float)\n\n        # Use the norm of the original matrix A_float for a stable tolerance.\n        norm_A_fro = np.linalg.norm(A_float, 'fro')\n        \n        # Tolerance based on numerical analysis literature (e.g., Golub & Van Loan).\n        # A small norm_A_fro should not lead to a zero tolerance.\n        # np.finfo(float).eps is approx 2.22e-16.\n        # Add a small value to norm_A_fro to handle the case of a zero matrix A.\n        tol = np.sqrt(n) * np.finfo(float).eps * (norm_A_fro + np.finfo(float).tiny)\n\n        for j in range(n):\n            v_j = V[:, j]\n            norm_v_j = np.linalg.norm(v_j)\n\n            if norm_v_j < tol:\n                # The column is numerically dependent on the previous columns.\n                # The corresponding column of Q remains the zero vector.\n                pass  # Q[:, j] is already zero\n            else:\n                q_j = v_j / norm_v_j\n                Q[:, j] = q_j\n                \n                # Orthogonalize the remaining vectors against the new q_j.\n                # This is the \"modified\" part of the algorithm.\n                for k in range(j + 1, n):\n                    # Compute projection coefficient\n                    r_jk = np.dot(q_j, V[:, k])\n                    # Subtract projection\n                    V[:, k] -= r_jk * q_j\n        return Q\n\n    def calculate_diagnostic(Q: np.ndarray) -> float:\n        \"\"\"\n        Calculates the diagnostic value ||Q^T Q - I||_2.\n\n        Args:\n            Q: The matrix with orthonormalized columns.\n\n        Returns:\n            The spectral norm of (Q^T Q - I).\n        \"\"\"\n        m, n = Q.shape\n        if n == 0:\n            return 0.0\n\n        I = np.identity(n)\n        M = Q.T @ Q - I\n        \n        # The spectral norm (ord=2) is the largest singular value.\n        spectral_norm = np.linalg.norm(M, ord=2)\n        return float(spectral_norm)\n\n    # Test Case 1: Well-conditioned, numerically independent\n    A1 = np.array([\n        [1, 0, 0],\n        [0, 1, 0],\n        [0, 0, 1],\n        [1, 1, 0],\n        [0, 1, 1]\n    ], dtype=float)\n\n    # Test Case 2: Nearly dependent columns\n    a2_1 = np.array([1, 2, 3, 4, 5], dtype=float)\n    a2_2 = np.array([1.0000000001, 2, 2.9999999999, 4, 5.0000000001], dtype=float)\n    a2_3 = np.array([5, 4, 3, 2, 1], dtype=float)\n    A2 = np.stack([a2_1, a2_2, a2_3], axis=1)\n\n    # Test Case 3: Exact dependence\n    a3_1 = np.array([1, 2, 3, 4, 5], dtype=float)\n    a3_2 = np.array([1, 2, 3, 4, 5], dtype=float)\n    a3_3 = np.array([2, 1, 0, 1, 2], dtype=float)\n    A3 = np.stack([a3_1, a3_2, a3_3], axis=1)\n\n    # Test Case 4: Ill-conditioned but full rank (Hilbert matrix)\n    n_hilbert = 4\n    A4 = np.array([[1.0 / (i + j - 1) for j in range(1, n_hilbert + 1)] for i in range(1, n_hilbert + 1)], dtype=float)\n    \n    # Test Case 5: Zero column as an edge case\n    A5 = np.array([\n        [1, 0, 0],\n        [0, 1, 0],\n        [0, 0, 0],\n        [0, 0, 0]\n    ], dtype=float)\n    \n    test_cases = [A1, A2, A3, A4, A5]\n    results = []\n\n    for A in test_cases:\n        Q = modified_gram_schmidt(A)\n        diagnostic = calculate_diagnostic(Q)\n        results.append(diagnostic)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3555869"}]}