{"hands_on_practices": [{"introduction": "Our intuition about geometry is deeply rooted in the standard Euclidean inner product, which treats all coordinate directions equally. However, in many scientific and engineering applications, some features or variables are inherently more significant than others. This exercise [@problem_id:3551127] invites you to explore how a weighted inner product, defined by a symmetric positive definite matrix $W$, creates a new geometry by altering our fundamental notions of length and angle. By calculating and contrasting the angle between two vectors in both the Euclidean and a $W$-weighted space, you will gain a concrete understanding of how this mathematical \"warping\" of space is realized through the Cholesky factorization of the weighting matrix.", "problem": "Let $W \\in \\mathbb{R}^{3 \\times 3}$ be the symmetric positive definite (SPD) matrix\n$$\nW \\;=\\; \\begin{pmatrix}\n3 & 1 & 0 \\\\\n1 & 2 & 1 \\\\\n0 & 1 & 4\n\\end{pmatrix},\n$$\nand let $x, y \\in \\mathbb{R}^{3}$ be the vectors $x = \\begin{pmatrix} 1 \\\\ 2 \\\\ -1 \\end{pmatrix}$ and $y = \\begin{pmatrix} 2 \\\\ -1 \\\\ 1 \\end{pmatrix}$. Using only the core definitions of an inner product, the norm induced by an inner product, and the angle induced by an inner product on a finite-dimensional real vector space, compute the angle between $x$ and $y$ induced by $W$. Also compute the Euclidean angle between $x$ and $y$, and then explain, using first principles and without invoking any pre-packaged formulas beyond the definitions, how the metric defined by $W$ reweights and mixes the coordinates and thereby changes the angle relative to the Euclidean case. In your explanation, interpret $W$ through its unique lower-triangular Cholesky factor with positive diagonal entries.\n\nExpress the final angle induced by $W$ as an exact closed-form analytic expression in radians. Do not approximate. Provide, as your final answer, only the $W$-angle in radians in exact form.", "solution": "The problem is first validated to ensure it is scientifically grounded, well-posed, and objective.\nThe givens are:\nA symmetric matrix $W \\in \\mathbb{R}^{3 \\times 3}$:\n$$\nW \\;=\\; \\begin{pmatrix}\n3 & 1 & 0 \\\\\n1 & 2 & 1 \\\\\n0 & 1 & 4\n\\end{pmatrix}\n$$\nTwo vectors $x, y \\in \\mathbb{R}^{3}$:\n$$\nx = \\begin{pmatrix} 1 \\\\ 2 \\\\ -1 \\end{pmatrix}, \\quad y = \\begin{pmatrix} 2 \\\\ -1 \\\\ 1 \\end{pmatrix}\n$$\nThe problem states that $W$ is symmetric positive definite (SPD). A matrix is symmetric if $W = W^T$, which is true by inspection. To verify positive definiteness, we use Sylvester's criterion, checking the determinants of the leading principal minors:\n$D_1 = \\det(3) = 3 > 0$.\n$D_2 = \\det\\begin{pmatrix} 3 & 1 \\\\ 1 & 2 \\end{pmatrix} = 3 \\cdot 2 - 1 \\cdot 1 = 5 > 0$.\n$D_3 = \\det(W) = 3(2 \\cdot 4 - 1 \\cdot 1) - 1(1 \\cdot 4 - 0 \\cdot 1) = 3(7) - 4 = 17 > 0$.\nSince all leading principal minors are positive, $W$ is indeed SPD.\nThe problem is well-defined, self-contained, and scientifically sound. It is a standard exercise in linear algebra involving inner product spaces. Thus, we proceed with the solution.\n\nThe core definition of an inner product on a real vector space $V$ is a function $\\langle \\cdot, \\cdot \\rangle: V \\times V \\to \\mathbb{R}$ that is positive definite, symmetric, and bilinear. For any SPD matrix $W \\in \\mathbb{R}^{n \\times n}$, the function $\\langle u, v \\rangle_W = u^T W v$ defines a valid inner product on $\\mathbb{R}^n$.\n\nThe norm of a vector $u$ induced by this inner product is $\\|u\\|_W = \\sqrt{\\langle u, u \\rangle_W} = \\sqrt{u^T W u}$.\n\nThe angle $\\theta_W$ between two non-zero vectors $x$ and $y$ is defined by the relation:\n$$\n\\cos(\\theta_W) = \\frac{\\langle x, y \\rangle_W}{\\|x\\|_W \\|y\\|_W}\n$$\nwhich implies $\\theta_W = \\arccos\\left(\\frac{\\langle x, y \\rangle_W}{\\|x\\|_W \\|y\\|_W}\\right)$.\n\nWe first compute the quantities required for the angle induced by $W$.\n1. The inner product $\\langle x, y \\rangle_W$:\n$$\n\\langle x, y \\rangle_W = x^T W y = \\begin{pmatrix} 1 & 2 & -1 \\end{pmatrix} \\begin{pmatrix} 3 & 1 & 0 \\\\ 1 & 2 & 1 \\\\ 0 & 1 & 4 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ -1 \\\\ 1 \\end{pmatrix}\n$$\nFirst, we compute the product $W y$:\n$$\nW y = \\begin{pmatrix} 3(2) + 1(-1) + 0(1) \\\\ 1(2) + 2(-1) + 1(1) \\\\ 0(2) + 1(-1) + 4(1) \\end{pmatrix} = \\begin{pmatrix} 5 \\\\ 1 \\\\ 3 \\end{pmatrix}\n$$\nThen, we compute $x^T (W y)$:\n$$\n\\langle x, y \\rangle_W = \\begin{pmatrix} 1 & 2 & -1 \\end{pmatrix} \\begin{pmatrix} 5 \\\\ 1 \\\\ 3 \\end{pmatrix} = 1(5) + 2(1) + (-1)(3) = 5 + 2 - 3 = 4\n$$\n\n2. The induced norm of $x$, $\\|x\\|_W$:\n$$\n\\|x\\|_W^2 = \\langle x, x \\rangle_W = x^T W x = \\begin{pmatrix} 1 & 2 & -1 \\end{pmatrix} \\begin{pmatrix} 3 & 1 & 0 \\\\ 1 & 2 & 1 \\\\ 0 & 1 & 4 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\\\ -1 \\end{pmatrix}\n$$\nFirst, $W x$:\n$$\nW x = \\begin{pmatrix} 3(1) + 1(2) + 0(-1) \\\\ 1(1) + 2(2) + 1(-1) \\\\ 0(1) + 1(2) + 4(-1) \\end{pmatrix} = \\begin{pmatrix} 5 \\\\ 4 \\\\ -2 \\end{pmatrix}\n$$\nThen, $x^T (W x)$:\n$$\n\\|x\\|_W^2 = \\begin{pmatrix} 1 & 2 & -1 \\end{pmatrix} \\begin{pmatrix} 5 \\\\ 4 \\\\ -2 \\end{pmatrix} = 1(5) + 2(4) + (-1)(-2) = 5 + 8 + 2 = 15\n$$\nSo, $\\|x\\|_W = \\sqrt{15}$.\n\n3. The induced norm of $y$, $\\|y\\|_W$:\n$$\n\\|y\\|_W^2 = \\langle y, y \\rangle_W = y^T W y = \\begin{pmatrix} 2 & -1 & 1 \\end{pmatrix} \\begin{pmatrix} 3 & 1 & 0 \\\\ 1 & 2 & 1 \\\\ 0 & 1 & 4 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ -1 \\\\ 1 \\end{pmatrix}\n$$\nWe already have $W y = \\begin{pmatrix} 5 \\\\ 1 \\\\ 3 \\end{pmatrix}$.\n$$\n\\|y\\|_W^2 = \\begin{pmatrix} 2 & -1 & 1 \\end{pmatrix} \\begin{pmatrix} 5 \\\\ 1 \\\\ 3 \\end{pmatrix} = 2(5) + (-1)(1) + 1(3) = 10 - 1 + 3 = 12\n$$\nSo, $\\|y\\|_W = \\sqrt{12} = 2\\sqrt{3}$.\n\nNow, we compute the cosine of the angle $\\theta_W$:\n$$\n\\cos(\\theta_W) = \\frac{4}{\\sqrt{15} \\sqrt{12}} = \\frac{4}{\\sqrt{180}} = \\frac{4}{\\sqrt{36 \\cdot 5}} = \\frac{4}{6\\sqrt{5}} = \\frac{2}{3\\sqrt{5}} = \\frac{2\\sqrt{5}}{15}\n$$\nThe angle $\\theta_W$ is therefore:\n$$\n\\theta_W = \\arccos\\left(\\frac{2\\sqrt{5}}{15}\\right)\n$$\nSince $\\frac{2\\sqrt{5}}{15} > 0$, the angle $\\theta_W$ is acute.\n\nNext, we compute the standard Euclidean angle, $\\theta_E$. The Euclidean inner product is $\\langle x, y \\rangle_E = x^T y$ (which corresponds to $W=I$, the identity matrix).\n$$\n\\langle x, y \\rangle_E = \\begin{pmatrix} 1 & 2 & -1 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ -1 \\\\ 1 \\end{pmatrix} = 1(2) + 2(-1) + (-1)(1) = 2 - 2 - 1 = -1\n$$\nThe Euclidean norms are:\n$$\n\\|x\\|_E^2 = x^T x = 1^2 + 2^2 + (-1)^2 = 1 + 4 + 1 = 6 \\implies \\|x\\|_E = \\sqrt{6}\n$$\n$$\n\\|y\\|_E^2 = y^T y = 2^2 + (-1)^2 + 1^2 = 4 + 1 + 1 = 6 \\implies \\|y\\|_E = \\sqrt{6}\n$$\nThe cosine of the Euclidean angle $\\theta_E$ is:\n$$\n\\cos(\\theta_E) = \\frac{\\langle x, y \\rangle_E}{\\|x\\|_E \\|y\\|_E} = \\frac{-1}{\\sqrt{6} \\sqrt{6}} = -\\frac{1}{6}\n$$\nThe Euclidean angle is $\\theta_E = \\arccos\\left(-\\frac{1}{6}\\right)$. Since $\\cos(\\theta_E) < 0$, the angle $\\theta_E$ is obtuse.\n\nFinally, we explain how the metric $W$ changes the angle. The key is to use the Cholesky factorization of $W$. Since $W$ is SPD, it has a unique decomposition $W = LL^T$ where $L$ is a lower-triangular matrix with positive diagonal entries. Let $L = \\begin{pmatrix} l_{11} & 0 & 0 \\\\ l_{21} & l_{22} & 0 \\\\ l_{31} & l_{32} & l_{33} \\end{pmatrix}$.\n$$\nLL^T = \\begin{pmatrix} l_{11}^2 & l_{11}l_{21} & l_{11}l_{31} \\\\ l_{21}l_{11} & l_{21}^2+l_{22}^2 & l_{21}l_{31}+l_{22}l_{32} \\\\ l_{31}l_{11} & l_{31}l_{21}+l_{32}l_{22} & l_{31}^2+l_{32}^2+l_{33}^2 \\end{pmatrix} = \\begin{pmatrix} 3 & 1 & 0 \\\\ 1 & 2 & 1 \\\\ 0 & 1 & 4 \\end{pmatrix}\n$$\nSolving for the entries of $L$:\n$l_{11} = \\sqrt{3}$\n$l_{21} = 1/l_{11} = 1/\\sqrt{3}$\n$l_{31} = 0/l_{11} = 0$\n$l_{22} = \\sqrt{2 - l_{21}^2} = \\sqrt{2 - 1/3} = \\sqrt{5/3}$\n$l_{32} = (1 - l_{21}l_{31})/l_{22} = 1/\\sqrt{5/3} = \\sqrt{3/5}$\n$l_{33} = \\sqrt{4 - l_{31}^2 - l_{32}^2} = \\sqrt{4 - 0 - 3/5} = \\sqrt{17/5}$\nThus, the Cholesky factor is:\n$$\nL = \\begin{pmatrix} \\sqrt{3} & 0 & 0 \\\\ 1/\\sqrt{3} & \\sqrt{5/3} & 0 \\\\ 0 & \\sqrt{3/5} & \\sqrt{17/5} \\end{pmatrix}\n$$\nThe inner product induced by $W$ can be rewritten using $L$:\n$$\n\\langle x, y \\rangle_W = x^T W y = x^T (LL^T) y = (L^T x)^T (L^T y)\n$$\nLet's define a change of coordinates $z' = L^T z$. Then $\\langle x, y \\rangle_W = (x')^T y' = \\langle x', y' \\rangle_E$. This equality is the core of the explanation. It shows that the $W$-inner product of $x$ and $y$ is equivalent to the standard Euclidean inner product of the transformed vectors $x' = L^T x$ and $y' = L^T y$. Consequently, the angle $\\theta_W$ between $x$ and $y$ is precisely the Euclidean angle between $x'$ and $y'$.\n\nThe transformation matrix is $L^T = \\begin{pmatrix} \\sqrt{3} & 1/\\sqrt{3} & 0 \\\\ 0 & \\sqrt{5/3} & \\sqrt{3/5} \\\\ 0 & 0 & \\sqrt{17/5} \\end{pmatrix}$. This transformation reweights and mixes the original coordinates of a vector $z = (z_1, z_2, z_3)^T$ to produce a new vector $z'=(z'_1, z'_2, z'_3)^T$:\n$z'_1 = \\sqrt{3} z_1 + (1/\\sqrt{3}) z_2$\n$z'_2 = \\sqrt{5/3} z_2 + \\sqrt{3/5} z_3$\n$z'_3 = \\sqrt{17/5} z_3$\n\nUnlike the Euclidean metric (where $W=I$), which treats each coordinate axis with equal weight and independently, the $W$ metric applies weights and creates \"crosstalk\" between coordinates. The diagonal entries of $W$ ($3, 2, 4$) give more weight to squares of coordinates ($x_1^2, x_2^2, x_3^2$) than the Euclidean norm, while the off-diagonal entries ($W_{12}=W_{21}=1, W_{23}=W_{32}=1$) introduce coupling terms ($x_1x_2, x_2x_3$). The Cholesky decomposition provides a systematic way to see this as a coordinate transformation.\n\nApplying this transformation $L^T$ to our vectors $x$ and $y$:\n$$\nx' = L^T x = \\begin{pmatrix} \\sqrt{3} & 1/\\sqrt{3} & 0 \\\\ 0 & \\sqrt{5/3} & \\sqrt{3/5} \\\\ 0 & 0 & \\sqrt{17/5} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} \\sqrt{3} + 2/\\sqrt{3} \\\\ 2\\sqrt{5/3} - \\sqrt{3/5} \\\\ -\\sqrt{17/5} \\end{pmatrix} = \\begin{pmatrix} 5/\\sqrt{3} \\\\ 7/\\sqrt{15} \\\\ -\\sqrt{17/5} \\end{pmatrix}\n$$\n$$\ny' = L^T y = \\begin{pmatrix} \\sqrt{3} & 1/\\sqrt{3} & 0 \\\\ 0 & \\sqrt{5/3} & \\sqrt{3/5} \\\\ 0 & 0 & \\sqrt{17/5} \\end{pmatrix} \\begin{pmatrix} 2 \\\\ -1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 2\\sqrt{3} - 1/\\sqrt{3} \\\\ -\\sqrt{5/3} + \\sqrt{3/5} \\\\ \\sqrt{17/5} \\end{pmatrix} = \\begin{pmatrix} 5/\\sqrt{3} \\\\ -2/\\sqrt{15} \\\\ \\sqrt{17/5} \\end{pmatrix}\n$$\nThe Euclidean dot product of these transformed vectors is:\n$$\n\\langle x', y' \\rangle_E = \\left(\\frac{5}{\\sqrt{3}}\\right)\\left(\\frac{5}{\\sqrt{3}}\\right) + \\left(\\frac{7}{\\sqrt{15}}\\right)\\left(\\frac{-2}{\\sqrt{15}}\\right) + \\left(\\frac{-\\sqrt{17}}{\\sqrt{5}}\\right)\\left(\\frac{\\sqrt{17}}{\\sqrt{5}}\\right) = \\frac{25}{3} - \\frac{14}{15} - \\frac{17}{5} = \\frac{125 - 14 - 51}{15} = \\frac{60}{15} = 4\n$$\nThis matches our previous calculation of $\\langle x, y \\rangle_W$. The transformation $L^T$ has warped the space. The most significant effect is on the first coordinate: both $x'_1$ and $y'_1$ are equal to $5/\\sqrt{3}$. This strong alignment in the dominant first component of the transformed vectors creates a large positive contribution ($25/3 \\approx 8.33$) to their dot product, which overcomes the negative contributions from the other components. This is why the angle changes from obtuse ($\\cos(\\theta_E) < 0$) to acute ($\\cos(\\theta_W) > 0$). The geometry defined by $W$ \"pulls\" the vectors $x$ and $y$ closer together than they are in the standard Euclidean geometry.", "answer": "$$\n\\boxed{\\arccos\\left(\\frac{2\\sqrt{5}}{15}\\right)}\n$$", "id": "3551127"}, {"introduction": "Building on the concept of non-Euclidean geometries, we now turn to one of the most fundamental operations in linear algebra: projection. Finding the \"best approximation\" of a vector within a subspace is the cornerstone of solving least-squares problems. This practice [@problem_id:3551176] challenges you to compute and compare two different \"best\" approximations of a vectorâ€”one defined by the standard Euclidean inner product and another by a weighted inner product. By working through the mechanics of both orthogonal decompositions, you will see firsthand how the choice of norm directly influences the solution, a key principle in designing and interpreting results from weighted least-squares and other optimization problems.", "problem": "Consider the weighted inner product on $\\mathbb{R}^{3}$ defined by $\\langle x, y \\rangle_{W} = x^{\\mathsf{T}} W y$, where $W$ is a symmetric positive definite (SPD) matrix. Let $W = \\mathrm{diag}(2,3,5)$, let $A \\in \\mathbb{R}^{3 \\times 2}$ have columns $a_{1} = (1,0,1)^{\\mathsf{T}}$ and $a_{2} = (0,1,1)^{\\mathsf{T}}$, and let $b = (2,3,4)^{\\mathsf{T}}$. Using only the fundamental definitions of inner products, norms, and orthogonality, form the $W$-orthogonal decomposition $b = p_{W} + q_{W}$ with $p_{W} \\in \\mathrm{range}(A)$ and $q_{W} \\in \\mathrm{range}(A)^{\\perp_{W}}$, and the Euclidean decomposition $b = p_{2} + q_{2}$ with $p_{2} \\in \\mathrm{range}(A)$ and $q_{2} \\in \\mathrm{range}(A)^{\\perp_{2}}$ under the standard inner product. Compute the ratio $R$ of the squared norms of the projection components,\n$$\nR \\;=\\; \\frac{\\|p_{W}\\|_{W}^{2}}{\\|p_{2}\\|_{2}^{2}} \\;=\\; \\frac{p_{W}^{\\mathsf{T}} W p_{W}}{p_{2}^{\\mathsf{T}} p_{2}},\n$$\nand express your final answer as an exact value. No rounding is required.", "solution": "The problem requires the computation of a ratio of squared norms of two different orthogonal projections of a vector $b$ onto a subspace spanned by the columns of a matrix $A$. The first projection is with respect to the standard Euclidean inner product, and the second is with respect to a weighted inner product defined by a matrix $W$.\n\nThe given data are:\nThe vector space is $\\mathbb{R}^{3}$.\nThe weighted inner product is $\\langle x, y \\rangle_{W} = x^{\\mathsf{T}} W y$.\nThe weight matrix is $W = \\mathrm{diag}(2,3,5) = \\begin{pmatrix} 2 & 0 & 0 \\\\ 0 & 3 & 0 \\\\ 0 & 0 & 5 \\end{pmatrix}$. This matrix is symmetric and has positive diagonal entries, so it is positive definite.\nThe matrix $A \\in \\mathbb{R}^{3 \\times 2}$ is given by its columns $a_{1} = (1,0,1)^{\\mathsf{T}}$ and $a_{2} = (0,1,1)^{\\mathsf{T}}$. Thus, $A = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{pmatrix}$.\nThe vector to be projected is $b = (2,3,4)^{\\mathsf{T}}$.\n\nThe subspace of interest is the range of $A$, denoted $\\mathrm{range}(A)$, which is the span of its columns $\\{a_{1}, a_{2}\\}$.\n\nFirst, we find the Euclidean projection $p_{2}$ of $b$ onto $\\mathrm{range}(A)$.\nThe vector $p_{2}$ is the unique vector in $\\mathrm{range}(A)$ such that the residual vector $q_{2} = b - p_{2}$ is orthogonal to $\\mathrm{range}(A)$ with respect to the standard Euclidean inner product $\\langle u, v \\rangle_{2} = u^{\\mathsf{T}}v$.\nSince $p_{2} \\in \\mathrm{range}(A)$, it can be written as a linear combination of the columns of $A$, i.e., $p_{2} = Ax$ for some vector $x = (x_{1}, x_{2})^{\\mathsf{T}} \\in \\mathbb{R}^{2}$.\nThe orthogonality condition is that $\\langle b - Ax, v \\rangle_{2} = 0$ for all $v \\in \\mathrm{range}(A)$. It is sufficient to enforce this for the basis vectors $a_{1}$ and $a_{2}$:\n$$\n\\langle b - Ax, a_{1} \\rangle_{2} = a_{1}^{\\mathsf{T}}(b - Ax) = 0\n$$\n$$\n\\langle b - Ax, a_{2} \\rangle_{2} = a_{2}^{\\mathsf{T}}(b - Ax) = 0\n$$\nThese two equations can be written in matrix form as $A^{\\mathsf{T}}(b - Ax) = 0$, which leads to the normal equations:\n$$\nA^{\\mathsf{T}}Ax = A^{\\mathsf{T}}b\n$$\nWe compute the matrices involved:\n$$\nA^{\\mathsf{T}}A = \\begin{pmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{pmatrix} = \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix}\n$$\n$$\nA^{\\mathsf{T}}b = \\begin{pmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 3 \\\\ 4 \\end{pmatrix} = \\begin{pmatrix} 6 \\\\ 7 \\end{pmatrix}\n$$\nWe solve the system $\\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix} \\begin{pmatrix} x_{1} \\\\ x_{2} \\end{pmatrix} = \\begin{pmatrix} 6 \\\\ 7 \\end{pmatrix}$.\nThe inverse of $A^{\\mathsf{T}}A$ is $(A^{\\mathsf{T}}A)^{-1} = \\frac{1}{(2)(2) - (1)(1)} \\begin{pmatrix} 2 & -1 \\\\ -1 & 2 \\end{pmatrix} = \\frac{1}{3} \\begin{pmatrix} 2 & -1 \\\\ -1 & 2 \\end{pmatrix}$.\nSo, $x = \\frac{1}{3} \\begin{pmatrix} 2 & -1 \\\\ -1 & 2 \\end{pmatrix} \\begin{pmatrix} 6 \\\\ 7 \\end{pmatrix} = \\frac{1}{3} \\begin{pmatrix} (2)(6) - (1)(7) \\\\ (-1)(6) + (2)(7) \\end{pmatrix} = \\frac{1}{3} \\begin{pmatrix} 5 \\\\ 8 \\end{pmatrix}$.\nThe projection is $p_{2} = Ax = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{pmatrix} \\frac{1}{3} \\begin{pmatrix} 5 \\\\ 8 \\end{pmatrix} = \\frac{1}{3} \\begin{pmatrix} 5 \\\\ 8 \\\\ 13 \\end{pmatrix}$.\nThe squared Euclidean norm of $p_{2}$ is $\\|p_{2}\\|_{2}^{2} = p_{2}^{\\mathsf{T}}p_{2}$. A more direct calculation uses the orthogonality property: $\\langle p_{2}, b - p_{2} \\rangle_{2} = 0$, which implies $\\|p_{2}\\|_{2}^{2} = \\langle p_{2}, p_{2} \\rangle_{2} = \\langle p_{2}, b \\rangle_{2} = p_{2}^{\\mathsf{T}}b$.\n$$\n\\|p_{2}\\|_{2}^{2} = \\left(\\frac{1}{3} \\begin{pmatrix} 5 \\\\ 8 \\\\ 13 \\end{pmatrix}\\right)^{\\mathsf{T}} \\begin{pmatrix} 2 \\\\ 3 \\\\ 4 \\end{pmatrix} = \\frac{1}{3} \\begin{pmatrix} 5 & 8 & 13 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 3 \\\\ 4 \\end{pmatrix} = \\frac{1}{3}((5)(2) + (8)(3) + (13)(4)) = \\frac{1}{3}(10 + 24 + 52) = \\frac{86}{3}.\n$$\n\nSecond, we find the $W$-orthogonal projection $p_{W}$ of $b$ onto $\\mathrm{range}(A)$.\nThe vector $p_{W}$ is in $\\mathrm{range}(A)$, so $p_{W} = Ay$ for some $y = (y_{1}, y_{2})^{\\mathsf{T}} \\in \\mathbb{R}^{2}$. The residual $q_{W} = b - p_{W}$ must be orthogonal to $\\mathrm{range}(A)$ with respect to the $W$-inner product.\nThis means $\\langle b - Ay, v \\rangle_{W} = 0$ for all $v \\in \\mathrm{range}(A)$. We test this against the basis vectors $a_{1}$ and $a_{2}$:\n$$\n\\langle b - Ay, a_{i} \\rangle_{W} = a_{i}^{\\mathsf{T}}W(b - Ay) = 0 \\quad \\text{for } i=1,2.\n$$\nThis leads to the weighted normal equations $A^{\\mathsf{T}}W(b - Ay) = 0$, or:\n$$\nA^{\\mathsf{T}}WAy = A^{\\mathsf{T}}Wb\n$$\nWe compute the matrices for this system:\n$$\nA^{\\mathsf{T}}W = \\begin{pmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\end{pmatrix} \\begin{pmatrix} 2 & 0 & 0 \\\\ 0 & 3 & 0 \\\\ 0 & 0 & 5 \\end{pmatrix} = \\begin{pmatrix} 2 & 0 & 5 \\\\ 0 & 3 & 5 \\end{pmatrix}\n$$\n$$\nA^{\\mathsf{T}}WA = (A^{\\mathsf{T}}W)A = \\begin{pmatrix} 2 & 0 & 5 \\\\ 0 & 3 & 5 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{pmatrix} = \\begin{pmatrix} 7 & 5 \\\\ 5 & 8 \\end{pmatrix}\n$$\n$$\nA^{\\mathsf{T}}Wb = (A^{\\mathsf{T}}W)b = \\begin{pmatrix} 2 & 0 & 5 \\\\ 0 & 3 & 5 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 3 \\\\ 4 \\end{pmatrix} = \\begin{pmatrix} (2)(2) + (0)(3) + (5)(4) \\\\ (0)(2) + (3)(3) + (5)(4) \\end{pmatrix} = \\begin{pmatrix} 24 \\\\ 29 \\end{pmatrix}\n$$\nWe solve the system $\\begin{pmatrix} 7 & 5 \\\\ 5 & 8 \\end{pmatrix} \\begin{pmatrix} y_{1} \\\\ y_{2} \\end{pmatrix} = \\begin{pmatrix} 24 \\\\ 29 \\end{pmatrix}$.\nThe inverse of $A^{\\mathsf{T}}WA$ is $(A^{\\mathsf{T}}WA)^{-1} = \\frac{1}{(7)(8) - (5)(5)} \\begin{pmatrix} 8 & -5 \\\\ -5 & 7 \\end{pmatrix} = \\frac{1}{31} \\begin{pmatrix} 8 & -5 \\\\ -5 & 7 \\end{pmatrix}$.\nSo, $y = \\frac{1}{31} \\begin{pmatrix} 8 & -5 \\\\ -5 & 7 \\end{pmatrix} \\begin{pmatrix} 24 \\\\ 29 \\end{pmatrix} = \\frac{1}{31} \\begin{pmatrix} (8)(24) - (5)(29) \\\\ (-5)(24) + (7)(29) \\end{pmatrix} = \\frac{1}{31} \\begin{pmatrix} 192 - 145 \\\\ -120 + 203 \\end{pmatrix} = \\frac{1}{31} \\begin{pmatrix} 47 \\\\ 83 \\end{pmatrix}$.\nThe squared $W$-norm of $p_{W}$ is $\\|p_{W}\\|_{W}^{2} = p_{W}^{\\mathsf{T}}Wp_{W}$. Using the $W$-orthogonality condition $\\langle p_{W}, b - p_{W}\\rangle_{W} = 0$, we have $\\|p_{W}\\|_{W}^{2} = \\langle p_{W}, b \\rangle_{W} = p_{W}^{\\mathsf{T}}Wb$.\nSince $p_{W} = Ay$, this becomes $\\|p_{W}\\|_{W}^{2} = (Ay)^{\\mathsf{T}}Wb = y^{\\mathsf{T}}(A^{\\mathsf{T}}Wb)$.\n$$\n\\|p_{W}\\|_{W}^{2} = \\left(\\frac{1}{31} \\begin{pmatrix} 47 \\\\ 83 \\end{pmatrix}\\right)^{\\mathsf{T}} \\begin{pmatrix} 24 \\\\ 29 \\end{pmatrix} = \\frac{1}{31} \\begin{pmatrix} 47 & 83 \\end{pmatrix} \\begin{pmatrix} 24 \\\\ 29 \\end{pmatrix} = \\frac{1}{31}((47)(24) + (83)(29)).\n$$\nWe calculate the products: $47 \\times 24 = 1128$ and $83 \\times 29 = 2407$.\n$$\n\\|p_{W}\\|_{W}^{2} = \\frac{1}{31}(1128 + 2407) = \\frac{3535}{31}.\n$$\n\nFinally, we compute the ratio $R$:\n$$\nR = \\frac{\\|p_{W}\\|_{W}^{2}}{\\|p_{2}\\|_{2}^{2}} = \\frac{3535/31}{86/3} = \\frac{3535 \\times 3}{31 \\times 86} = \\frac{10605}{2666}.\n$$\nThe numbers in the numerator and denominator can be factored.\n$10605 = 3 \\times 3535 = 3 \\times 5 \\times 707 = 3 \\times 5 \\times 7 \\times 101$.\n$2666 = 2 \\times 1333 = 2 \\times 31 \\times 43$.\nThere are no common factors between the numerator and denominator, so the fraction is in its simplest form.", "answer": "$$\\boxed{\\frac{10605}{2666}}$$", "id": "3551176"}, {"introduction": "Orthogonal projections are not only geometrically intuitive but also numerically robust, with an operator norm of one. This is not true for all projectors. This practice [@problem_id:3551195] delves into the world of oblique projections, where the range and null space are not orthogonal. You will first derive the general form of an oblique projector and then investigate a case where the angle between its range and null space becomes small. Through a combination of theoretical analysis and a short computational experiment, you will demonstrate a crucial principle of numerical stability: a geometrically ill-conditioned projection (with a large operator norm) can catastrophically amplify small rounding errors, turning a seemingly benign perturbation into a large error in the result.", "problem": "Let $\\mathbb{R}^n$ be equipped with the standard Euclidean inner product, defined for vectors $x,y \\in \\mathbb{R}^n$ by $\\langle x,y\\rangle = x^\\top y$, and the induced $2$-norm $\\|x\\|_2 = \\sqrt{\\langle x,x\\rangle}$. A linear operator $P \\in \\mathbb{R}^{n \\times n}$ is called a projector if $P^2 = P$. An orthogonal projector is a projector whose range is orthogonal to its null space. An oblique projector is any projector that is not orthogonal, typically characterized by having range and null spaces that are not orthogonal. The spectral norm of a matrix $P$, denoted $\\|P\\|_2$, is the operator norm induced by the vector $2$-norm, defined by $\\|P\\|_2 = \\max_{\\|x\\|_2=1} \\|Px\\|_2$, and equals the largest singular value of $P$.\n\nStarting from the fundamental definitions above, derive how to construct a projector $P$ that projects onto a given subspace along a complementary subspace, without assuming any pre-given formula for $P$. Then, in the special case $n=2$ with the target subspace spanned by the vector $a = [1,0]^\\top$ and the complementary direction defined via the linear functional induced by $b = [\\cos \\alpha, \\sin \\alpha]^\\top$ (angles must be interpreted in radians), construct the oblique projector onto $\\operatorname{span}\\{a\\}$ along $\\operatorname{null}(b^\\top)$. Explain why the spectral norm $\\|P\\|_2$ can be large when the angle $\\alpha$ is close to $\\pi/2$, and why this leads to amplification of rounding errors in projection-based computations.\n\nYour program must implement the following steps for each test case:\n- Construct $P \\in \\mathbb{R}^{2 \\times 2}$ that projects onto $\\operatorname{span}\\{a\\}$ along $\\operatorname{null}(b^\\top)$ for the specified angle $\\alpha$.\n- Compute the spectral norm $\\|P\\|_2$ using the singular value decomposition of $P$.\n- Emulate a rounding error by adding a perturbation $\\delta = \\varepsilon v$ to an input vector $x$, where $x = [0,0]^\\top$, $\\varepsilon = 10^{-12}$, and $v$ is the right singular vector corresponding to the largest singular value of $P$ (this choice represents a worst-case direction for amplification in the $2$-norm).\n- Compute the amplification ratio $r = \\frac{\\|P(x+\\delta)-P x\\|_2}{\\|\\delta\\|_2}$, which equals $\\frac{\\|P\\delta\\|_2}{\\|\\delta\\|_2}$; report this ratio as a float.\n\nUse the following test suite of angle values (all angles are in radians):\n- $\\alpha_1 = 0.0$ (baseline orthogonal-like case),\n- $\\alpha_2 = 1.2$ (moderate obliqueness),\n- $\\alpha_3 = 1.5706$ (near-degenerate obliqueness with very large $\\|P\\|_2$).\n\nFor each test case, output two floats: first the spectral norm $\\|P\\|_2$, then the amplification ratio $r$. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order $[\\|P\\|_2(\\alpha_1), r(\\alpha_1), \\|P\\|_2(\\alpha_2), r(\\alpha_2), \\|P\\|_2(\\alpha_3), r(\\alpha_3)]$.", "solution": "The problem is valid as it is scientifically grounded in linear algebra and numerical analysis, well-posed, and objective. All necessary definitions and data are provided for a unique solution.\n\nWe begin by addressing the theoretical derivation.\n\nA projection operator $P$ onto a subspace $\\mathcal{R}$ along a complementary subspace $\\mathcal{N}$ is defined by the properties that for any vector $x \\in \\mathbb{R}^n$, its image $Px$ lies in $\\mathcal{R}$, and the residual $x - Px$ lies in $\\mathcal{N}$. The space $\\mathbb{R}^n$ is the direct sum of these subspaces, $\\mathbb{R}^n = \\mathcal{R} \\oplus \\mathcal{N}$. These conditions uniquely define the projection. The range of the projector is $\\operatorname{range}(P) = \\mathcal{R}$ and its null space is $\\operatorname{null}(P) = \\mathcal{N}$. A projector is idempotent, meaning $P^2 = P$.\n\nLet the target subspace $\\mathcal{R}$ be the column space (range) of a matrix $A \\in \\mathbb{R}^{n \\times k}$ with linearly independent columns. Let the complementary subspace $\\mathcal{N}$ be the null space of a matrix $B^\\top \\in \\mathbb{R}^{k \\times n}$, i.e., $\\mathcal{N} = \\operatorname{null}(B^\\top)$, where $B \\in \\mathbb{R}^{n \\times k}$ also has linearly independent columns. The condition $\\mathbb{R}^n = \\mathcal{R} \\oplus \\mathcal{N}$ implies that the matrix $B^\\top A \\in \\mathbb{R}^{k \\times k}$ is invertible.\n\nTo construct the matrix $P$, we formalize its defining properties for an arbitrary vector $x \\in \\mathbb{R}^n$:\n1.  The projection $Px$ must lie in $\\mathcal{R} = \\operatorname{range}(A)$. This means there exists a coefficient vector $c \\in \\mathbb{R}^k$ such that $Px = Ac$.\n2.  The residual $x - Px$ must lie in $\\mathcal{N} = \\operatorname{null}(B^\\top)$. This means $B^\\top (x - Px) = 0$.\n\nSubstituting the expression from the first property into the second gives:\n$$\nB^\\top (x - Ac) = 0\n$$\n$$\nB^\\top x - B^\\top Ac = 0\n$$\n$$\nB^\\top Ac = B^\\top x\n$$\nSince the matrix $B^\\top A$ is invertible, we can solve for the coefficient vector $c$:\n$$\nc = (B^\\top A)^{-1} B^\\top x\n$$\nSubstituting this expression for $c$ back into the equation $Px = Ac$, we obtain:\n$$\nPx = A \\left((B^\\top A)^{-1} B^\\top x\\right)\n$$\nSince this equation must hold for any vector $x$, the matrix representation of the projector $P$ is:\n$$\nP = A(B^\\top A)^{-1} B^\\top\n$$\nThis is the general formula for an oblique projector onto $\\operatorname{range}(A)$ along $\\operatorname{null}(B^\\top)$.\n\nNow, we apply this to the specific case given in the problem, where $n=2$.\nThe target subspace is $\\mathcal{R} = \\operatorname{span}\\{a\\}$ with $a = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$. This corresponds to setting $A=a$.\nThe complementary subspace is $\\mathcal{N} = \\operatorname{null}(b^\\top)$ with $b = \\begin{pmatrix} \\cos \\alpha \\\\ \\sin \\alpha \\end{pmatrix}$. This corresponds to setting $B=b$.\n\nIn this case, $A$ and $B$ are column vectors, so the product $b^\\top a$ is a $1 \\times 1$ matrix (a scalar):\n$$\nb^\\top a = \\begin{pmatrix} \\cos \\alpha & \\sin \\alpha \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\cos \\alpha\n$$\nThe inverse is $(b^\\top a)^{-1} = \\frac{1}{\\cos \\alpha}$. This inverse exists if and only if $\\cos \\alpha \\neq 0$, which means $\\alpha \\neq \\frac{\\pi}{2} + k\\pi$ for any integer $k$. This is the condition for $\\mathcal{R}$ and $\\mathcal{N}$ to be complementary subspaces.\n\nSubstituting into the general formula for $P$:\n$$\nP = a(b^\\top a)^{-1} b^\\top = a \\left(\\frac{1}{\\cos \\alpha}\\right) b^\\top = \\frac{1}{\\cos \\alpha} a b^\\top\n$$\nWe compute the outer product $ab^\\top$:\n$$\nab^\\top = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\begin{pmatrix} \\cos \\alpha & \\sin \\alpha \\end{pmatrix} = \\begin{pmatrix} 1 \\cdot \\cos \\alpha & 1 \\cdot \\sin \\alpha \\\\ 0 \\cdot \\cos \\alpha & 0 \\cdot \\sin \\alpha \\end{pmatrix} = \\begin{pmatrix} \\cos \\alpha & \\sin \\alpha \\\\ 0 & 0 \\end{pmatrix}\n$$\nFinally, we obtain the matrix for the projector $P$:\n$$\nP = \\frac{1}{\\cos \\alpha} \\begin{pmatrix} \\cos \\alpha & \\sin \\alpha \\\\ 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 1 & \\frac{\\sin \\alpha}{\\cos \\alpha} \\\\ 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 1 & \\tan \\alpha \\\\ 0 & 0 \\end{pmatrix}\n$$\nThe spectral norm $\\|P\\|_2$ is defined as the largest singular value of $P$, which is the square root of the largest eigenvalue of the matrix $P^\\top P$.\n$$\nP^\\top P = \\begin{pmatrix} 1 & 0 \\\\ \\tan \\alpha & 0 \\end{pmatrix} \\begin{pmatrix} 1 & \\tan \\alpha \\\\ 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 1 & \\tan \\alpha \\\\ \\tan \\alpha & \\tan^2 \\alpha \\end{pmatrix}\n$$\nThe eigenvalues $\\lambda$ of $P^\\top P$ are the roots of the characteristic equation $\\det(P^\\top P - \\lambda I) = 0$:\n$$\n\\det \\begin{pmatrix} 1-\\lambda & \\tan \\alpha \\\\ \\tan \\alpha & \\tan^2 \\alpha - \\lambda \\end{pmatrix} = (1-\\lambda)(\\tan^2 \\alpha - \\lambda) - \\tan^2 \\alpha = 0\n$$\n$$\n\\lambda^2 - \\lambda(1 + \\tan^2 \\alpha) = 0 \\implies \\lambda(\\lambda - (1 + \\tan^2 \\alpha)) = 0\n$$\nThe eigenvalues are $\\lambda_1 = 0$ and $\\lambda_2 = 1 + \\tan^2 \\alpha = \\sec^2 \\alpha$.\nThe singular values of $P$ are the square roots of these eigenvalues: $\\sigma_1 = \\sqrt{\\sec^2 \\alpha} = |\\sec \\alpha|$ and $\\sigma_2 = 0$. The largest singular value is $\\sigma_{\\max} = |\\sec \\alpha|$.\nTherefore, the spectral norm of the projector is:\n$$\n\\|P\\|_2 = |\\sec \\alpha| = \\frac{1}{|\\cos \\alpha|}\n$$\nAs the angle $\\alpha$ approaches $\\frac{\\pi}{2}$, $\\cos \\alpha$ approaches $0$, causing $\\|P\\|_2$ to approach infinity. Geometrically, when $\\alpha \\to \\frac{\\pi}{2}$, the vector $b$ becomes orthogonal to $a$. The space $\\mathcal{N} = \\operatorname{null}(b^\\top) = (\\operatorname{span}\\{b\\})^\\perp$ becomes nearly identical to the space $\\mathcal{R} = \\operatorname{span}\\{a\\}$, since if $b \\perp a$, then $a \\in (\\operatorname{span}\\{b\\})^\\perp$. The two subspaces are no longer complementary, and the projection is ill-conditioned, which is reflected in the unbounded norm.\n\nThe amplification of rounding errors is a direct consequence of the operator norm's definition. Let $x$ be an input vector and $\\delta$ be a small perturbation (e.g., a rounding error). The projected output for the perturbed input is $P(x+\\delta)$. The error in the output is $P(x+\\delta) - Px = P\\delta$ by linearity. The magnitude of this output error is $\\|P\\delta\\|_2$.\nThe definition of the spectral norm is $\\|P\\|_2 = \\max_{\\delta \\neq 0} \\frac{\\|P\\delta\\|_2}{\\|\\delta\\|_2}$. This implies the inequality $\\|P\\delta\\|_2 \\le \\|P\\|_2 \\|\\delta\\|_2$. The spectral norm $\\|P\\|_2$ is the worst-case amplification factor for the norm of the input error.\nWhen $\\|P\\|_2$ is large (i.e., when $\\alpha \\approx \\pi/2$), a small input error $\\|\\delta\\|_2$ can be magnified into a large output error $\\|P\\delta\\|_2$. This maximum amplification occurs when the error vector $\\delta$ is aligned with the right singular vector $v$ corresponding to the largest singular value $\\sigma_{\\max} = \\|P\\|_2$. For such a $\\delta = \\varepsilon v$, the amplification ratio is exactly:\n$$\nr = \\frac{\\|P\\delta\\|_2}{\\|\\delta\\|_2} = \\frac{\\|P(\\varepsilon v)\\|_2}{\\|\\varepsilon v\\|_2} = \\frac{\\varepsilon \\|Pv\\|_2}{\\varepsilon \\|v\\|_2} = \\frac{\\sigma_{\\max}\\|v\\|_2}{\\|v\\|_2} = \\sigma_{\\max} = \\|P\\|_2\n$$\nThe problem asks to compute this ratio for a perturbation $\\delta$ aligned with this worst-case direction. Given that the initial vector is $x = [0,0]^\\top$, $Px=0$, so the ratio simplifies to $\\frac{\\|P(x+\\delta)-P x\\|_2}{\\|\\delta\\|_2} = \\frac{\\|P\\delta\\|_2}{\\|\\delta\\|_2}$, which, as shown, will be equal to $\\|P\\|_2$. The numerical implementation will verify this relationship.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Constructs oblique projectors for given angles, computes their spectral norms,\n    and calculates the amplification ratio for a worst-case perturbation.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        0.0,      # baseline orthogonal-like case\n        1.2,      # moderate obliqueness\n        1.5706,   # near-degenerate obliqueness\n    ]\n\n    results = []\n    # Epsilon for perturbation, as specified.\n    epsilon = 1e-12\n    # Input vector x, as specified.\n    x = np.array([0.0, 0.0])\n\n    for alpha in test_cases:\n        # Construct the oblique projector P as derived in the solution.\n        # P = [[1, tan(alpha)], [0, 0]]\n        tan_alpha = np.tan(alpha)\n        P = np.array([[1.0, tan_alpha],\n                      [0.0, 0.0]])\n\n        # Compute the singular value decomposition of P.\n        # U: left singular vectors, s: singular values, Vh: right singular vectors (Hermitian).\n        # The spectral norm is the largest singular value.\n        try:\n            U, s, Vh = np.linalg.svd(P)\n        except np.linalg.LinAlgError:\n            # Handle potential numerical errors for extreme cases, though unlikely here.\n            results.extend([np.inf, np.inf])\n            continue\n            \n        spectral_norm = s[0]\n\n        # Emulate a rounding error by perturbing the input vector x.\n        # The worst-case direction for amplification is the right singular vector 'v'\n        # corresponding to the largest singular value. 'v' is the first row of Vh.\n        v = Vh[0, :]\n        \n        # Create the perturbation delta.\n        delta = epsilon * v\n        \n        # The problem asks for the amplification ratio r.\n        # r = ||P(x+delta) - Px||_2 / ||delta||_2\n        # Since x is the zero vector, Px is also the zero vector.\n        # So, r = ||P(delta)||_2 / ||delta||_2\n        \n        # Project the perturbation\n        P_delta = P @ delta\n        \n        # Calculate the norms of the projected perturbation and the original perturbation.\n        norm_P_delta = np.linalg.norm(P_delta)\n        norm_delta = np.linalg.norm(delta)\n        \n        # Compute the amplification ratio.\n        # This should theoretically be equal to the spectral norm.\n        if norm_delta > 0:\n            amplification_ratio = norm_P_delta / norm_delta\n        else:\n            # This case should not be reached with epsilon > 0 and v being a unit vector.\n            amplification_ratio = 0.0\n        \n        results.extend([spectral_norm, amplification_ratio])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3551195"}]}