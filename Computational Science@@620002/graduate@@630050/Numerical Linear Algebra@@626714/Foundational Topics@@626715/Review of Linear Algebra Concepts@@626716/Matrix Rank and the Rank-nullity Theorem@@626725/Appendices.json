{"hands_on_practices": [{"introduction": "This first practice lays the groundwork by tackling a fundamental task in numerical linear algebra: computing a basis for the null space. You will implement a robust method based on the column-pivoted QR factorization and discover how numerical rank is not an absolute property but an estimate dependent on both a tolerance and the algorithmic choices made, such as pivoting strategy. This exercise is crucial for understanding the practical mechanics behind the rank-nullity theorem in finite precision [@problem_id:3558928].", "problem": "You are to implement a complete, runnable program that, for given real matrices, estimates a basis for the null space $\\ker(A)$ using only the upper-triangular factor $R$ obtained from a Column-Pivoted Orthogonal-Triangular (QR) factorization, and then analyzes the sensitivity of the estimation to the column pivot order. The investigation must be grounded in the core definitions of rank, null space, and the rank-nullity theorem, and must proceed from these principles without relying on any shortcut formulas in the problem statement.\n\nFundamental base for the derivation:\n- For a matrix $A \\in \\mathbb{R}^{m \\times n}$, the rank-nullity theorem states that $\\operatorname{rank}(A) + \\dim \\ker(A) = n$.\n- The null space $\\ker(A)$ is the set of all vectors $x \\in \\mathbb{R}^{n}$ such that $A x = 0$.\n- A Column-Pivoted Orthogonal-Triangular (QR) factorization produces a permutation of the columns and orthogonal-triangular factors, where the factorization satisfies $A P = Q R$, with $P$ a permutation matrix, $Q$ orthogonal, and $R$ upper-triangular (or upper-trapezoidal).\n\nTasks to perform:\n1. For each test matrix $A$, compute a Column-Pivoted QR factorization of $A$ with a prescribed column pre-permutation. Use the diagonal of the resulting $R$ to estimate the numerical rank $\\hat{r}$ via a threshold rule: treat $R_{ii}$ values with magnitude below a threshold as numerically zero. Let the threshold be specified by a dimensionless parameter $\\tau$ scaled relative to the largest diagonal magnitude of $R$.\n2. Using only $R$ and the pivot information, construct an estimate of a basis for $\\ker(A)$ of dimension $n - \\hat{r}$, and map it back to the original column ordering. Do not use Singular Value Decomposition (SVD) in this construction; use only $R$ and the pivot information.\n3. Quantify the quality of the estimated basis by computing the Frobenius norm of the residual $A N$, where $N$ is the matrix whose columns form the estimated basis for the null space. Report this norm as a floating-point number.\n4. To analyze sensitivity to pivot order, repeat the construction of the null space basis under a second column pre-permutation of $A$. Compute the principal angles between the two estimated null space subspaces, and report the maximum principal angle. The angle unit must be radians.\n5. Additionally, report whether the two runs (with different pre-permutations) agree on the estimated nullity $\\widehat{\\nu} = n - \\hat{r}$, as a boolean.\n\nOutput specification for each test case:\n- Produce a list containing five items:\n  - The estimated rank $\\hat{r}$ as an integer.\n  - The estimated nullity $\\widehat{\\nu} = n - \\hat{r}$ as an integer.\n  - The Frobenius norm $\\|A N\\|_F$ as a float.\n  - The maximum principal angle between the two estimated null spaces (in radians) as a float.\n  - A boolean indicating whether the two runs agree on $\\widehat{\\nu}$.\n- Aggregate the results for all test cases into a single line of output containing the lists as a comma-separated list enclosed in square brackets, for example, $[\\text{case1},\\text{case2},\\ldots]$.\n\nTest suite:\nImplement the following four scientifically sound and diverse test cases. All random draws must be reproducible by using the specified seeds.\n\n- Test case 1 (happy path, full row rank, rectangular):\n  - $A \\in \\mathbb{R}^{4 \\times 7}$ with entries drawn independently from a standard normal distribution using seed $12345$.\n  - Threshold parameter $\\tau = 10^{-12}$.\n  - First pre-permutation: identity order $\\{0,1,2,3,4,5,6\\}$.\n  - Second pre-permutation: reversed order $\\{6,5,4,3,2,1,0\\}$.\n\n- Test case 2 (near rank-deficient with controlled singular spectrum):\n  - $A \\in \\mathbb{R}^{6 \\times 8}$ constructed as $A = U \\Sigma V^{\\top}$, where $U \\in \\mathbb{R}^{6 \\times 6}$ and $V \\in \\mathbb{R}^{8 \\times 8}$ are orthogonal matrices obtained by QR factorizations of standard normal matrices using seed $2024$, and $\\Sigma \\in \\mathbb{R}^{6 \\times 8}$ has diagonal singular values $\\{1, 10^{-1}, 10^{-2}, 10^{-3}, 10^{-12}, 10^{-12}\\}$ on its first $6$ diagonal entries (zeros elsewhere).\n  - Threshold parameter $\\tau = 10^{-8}$.\n  - First pre-permutation: identity order $\\{0,1,2,3,4,5,6,7\\}$.\n  - Second pre-permutation: random permutation derived from seed $4242$ applied to $\\{0,\\ldots,7\\}$.\n\n- Test case 3 (equal-norm columns to induce pivot-order sensitivity):\n  - $A \\in \\mathbb{R}^{5 \\times 9}$ with columns normalized to unit $2$-norm. Construct $G \\in \\mathbb{R}^{5 \\times 9}$ with standard normal entries using seed $99$, and define $A$ by $A[:,j] = G[:,j] / \\|G[:,j]\\|_2$ for each column index $j$.\n  - Threshold parameter $\\tau = 10^{-10}$.\n  - First pre-permutation: identity order $\\{0,1,2,3,4,5,6,7,8\\}$.\n  - Second pre-permutation: reversed order $\\{8,7,6,5,4,3,2,1,0\\}$.\n\n- Test case 4 (structured deficiency via duplicate and dependent columns):\n  - $A \\in \\mathbb{R}^{5 \\times 5}$ constructed as follows: draw $B \\in \\mathbb{R}^{5 \\times 3}$ with standard normal entries using seed $555$, and set $A = [b_1, b_2, b_3, b_1 + b_2, b_2]$, where $b_1$, $b_2$, $b_3$ are the columns of $B$.\n  - Threshold parameter $\\tau = 10^{-12}$.\n  - First pre-permutation: identity order $\\{0,1,2,3,4\\}$.\n  - Second pre-permutation: permutation $\\{2,1,0,4,3\\}$.\n\nAngle unit requirement:\n- All reported angles must be expressed in radians.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each test case result itself being a list of the five items described above, for example, $[[\\cdot,\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot,\\cdot],\\ldots]$.", "solution": "The problem requires the implementation of an algorithm to estimate the null space of a real matrix $A \\in \\mathbb{R}^{m \\times n}$ using its Column-Pivoted QR factorization. The sensitivity of this estimation to the column pivot order must also be analyzed. The entire procedure is to be derived from first principles, namely the definitions of matrix rank, null space, the rank-nullity theorem, and QR factorization.\n\n### Theoretical Foundation\n\nThe rank-nullity theorem states that for any linear map represented by a matrix $A \\in \\mathbb{R}^{m \\times n}$, the dimension of its domain ($n$) is the sum of its rank and the dimension of its null space (kernel). Formally:\n$$ \\operatorname{rank}(A) + \\dim(\\ker(A)) = n $$\nThe null space, $\\ker(A)$, is the set of all vectors $x \\in \\mathbb{R}^{n}$ that are mapped to the zero vector: $\\ker(A) = \\{x \\in \\mathbb{R}^{n} \\mid Ax = 0\\}$. Our goal is to find a basis for this vector subspace.\n\nA Column-Pivoted QR factorization of a matrix $A_{p} \\in \\mathbb{R}^{m \\times n}$ provides a decomposition $A_{p}P = QR$, where $P$ is a permutation matrix, $Q \\in \\mathbb{R}^{m \\times m}$ is an orthogonal matrix ($Q^T Q = I$), and $R \\in \\mathbb{R}^{m \\times n}$ is an upper-trapezoidal matrix. The problem involves a prescribed pre-permutation of columns of $A$ before this factorization. Let this pre-permutation be represented by a permutation matrix $P_{pre}$. We first form the matrix $A' = AP_{pre}$. Then, we compute the column-pivoted QR factorization of $A'$, which yields $A'P_{qr} = QR$. Combining these, we have:\n$$ (AP_{pre})P_{qr} = QR \\implies A(P_{pre}P_{qr}) = QR $$\nLet $P_{total} = P_{pre}P_{qr}$ be the total permutation matrix. The original problem of finding $x$ such that $Ax=0$ can be transformed. Let $x = P_{total}y$ for some vector $y \\in \\mathbb{R}^n$. Substituting this into the null space equation gives:\n$$ A(P_{total}y) = 0 \\implies (AP_{total})y = 0 \\implies QRy = 0 $$\nSince $Q$ is orthogonal, it is invertible, and we can multiply by $Q^T$ from the left:\n$$ Q^TQRy = Q^T0 \\implies Ry = 0 $$\nThus, finding the null space of $A$ is equivalent to finding the null space of $R$ and then mapping the basis vectors back using the total permutation $P_{total}$.\n\n### Null Space Construction from $R$\n\nThe matrix $R$ is upper-trapezoidal. Due to column pivoting, the diagonal elements $|R_{ii}|$ are sorted in a non-increasing order, which facilitates rank estimation. The numerical rank $\\hat{r}$ can be estimated by counting the number of diagonal elements whose magnitude is above a certain threshold. This threshold is defined as $\\delta = \\tau \\cdot \\max_i(|R_{ii}|)$, where $\\tau$ is a given tolerance parameter. All $|R_{ii}| < \\delta$ are considered numerically zero. The estimated rank $\\hat{r}$ is the count of diagonal entries satisfying $|R_{ii}| \\ge \\delta$. Consequently, the estimated nullity is $\\widehat{\\nu} = n - \\hat{r}$.\n\nTo find a basis for $\\ker(R)$, we solve $Ry=0$. We partition the matrix $R$ and the vector $y$ according to the estimated rank $\\hat{r}$:\n$$ R = \\begin{pmatrix} R_{11} & R_{12} \\\\ 0 & R_{22} \\end{pmatrix}, \\quad y = \\begin{pmatrix} y_1 \\\\ y_2 \\end{pmatrix} $$\nHere, $R_{11} \\in \\mathbb{R}^{\\hat{r} \\times \\hat{r}}$ is an upper-triangular matrix with non-zero diagonal entries (by definition of $\\hat{r}$), making it invertible. $R_{12} \\in \\mathbb{R}^{\\hat{r} \\times \\widehat{\\nu}}$, and $R_{22} \\in \\mathbb{R}^{(m-\\hat{r}) \\times \\widehat{\\nu}}$ is a matrix whose entries are all numerically small. The vector $y$ is partitioned into $y_1 \\in \\mathbb{R}^{\\hat{r}}$ and $y_2 \\in \\mathbb{R}^{\\widehat{\\nu}}$.\n\nThe equation $Ry=0$ expands to:\n$$ R_{11}y_1 + R_{12}y_2 = 0 $$\n$$ R_{22}y_2 = 0 $$\nApproximating $R_{22}$ as a zero matrix, the second equation is satisfied for any choice of $y_2$. This confirms that the dimension of the null space is indeed $\\widehat{\\nu}$. To construct a basis, we can choose $\\widehat{\\nu}$ linearly independent vectors for $y_2$. The standard choice is the set of columns of the identity matrix $I_{\\widehat{\\nu}} \\in \\mathbb{R}^{\\widehat{\\nu} \\times \\widehat{\\nu}}$.\n\nFor each basis vector $e_k$ selected for $y_2$, we solve for the corresponding $y_1$ from the first equation:\n$$ R_{11}y_1 = -R_{12}y_2 $$\nSince $R_{11}$ is invertible, we have a unique solution for $y_1$:\n$$ y_1 = -R_{11}^{-1}R_{12}y_2 $$\nThis system can be solved efficiently for all $\\widehat{\\nu}$ basis vectors of $y_2$ at once by solving the matrix equation $R_{11}X = -R_{12}$, where the columns of $X \\in \\mathbb{R}^{\\hat{r} \\times \\widehat{\\nu}}$ will correspond to the $y_1$ parts of the null space basis vectors. This is a triangular system that can be solved via back substitution.\n\nThe resulting basis for $\\ker(R)$ is represented by the columns of the matrix $Z \\in \\mathbb{R}^{n \\times \\widehat{\\nu}}$:\n$$ Z = \\begin{pmatrix} X \\\\ I_{\\widehat{\\nu}} \\end{pmatrix} = \\begin{pmatrix} -R_{11}^{-1} R_{12} \\\\ I_{\\widehat{\\nu}} \\end{pmatrix} $$\nThe columns of $Z$ are the vectors $y^{(k)}$. To obtain the basis for $\\ker(A)$, we must apply the permutation $P_{total}$. The basis vectors for $\\ker(A)$ are the columns of $N = P_{total}Z$. If $p_{total}$ is the integer vector representing the permutation $P_{total}$, this operation rearranges the rows of $Z$: $N[p_{total}, :] = Z$.\n\n### Basis Orthonormalization and Comparison\n\nThe constructed basis $N$ is not guaranteed to be orthonormal. For robust numerical comparisons, it is beneficial to convert it to an orthonormal basis spanning the same subspace. This is achieved via a QR factorization of $N$ itself, $N = \\tilde{N}R_N$. The columns of $\\tilde{N}$ form the desired orthonormal basis.\n\nTo analyze the sensitivity to pivot order, we generate two different null space bases, $\\tilde{N}_1$ and $\\tilde{N}_2$, using two different initial column pre-permutations. We compare them by:\n1.  **Nullity Agreement**: A boolean check if the estimated nullities $\\widehat{\\nu}_1$ and $\\widehat{\\nu}_2$ are equal.\n2.  **Maximum Principal Angle**: The principal angles $\\{\\theta_k\\}$ between the subspaces spanned by $\\tilde{N}_1$ and $\\tilde{N}_2$ quantify their geometric alignment. The cosines of these angles are the singular values of the matrix $\\tilde{N}_1^T \\tilde{N}_2$. The maximum principal angle, $\\max_k \\theta_k$, indicates the largest deviation between the two subspaces. This is computed efficiently using `scipy.linalg.subspace_angles`.\n\n### Algorithmic Implementation\n\nThe overall algorithm for each test case is as follows:\n1.  For each of the two prescribed pre-permutations:\n    a. Permute the columns of the input matrix $A$.\n    b. Perform column-pivoted QR factorization on the permuted matrix to obtain $Q$, $R$, and an additional pivot order.\n    c. Combine the pre-permutation and the QR pivot order to get the total permutation.\n    d. Estimate the rank $\\hat{r}$ and nullity $\\widehat{\\nu}$ from the diagonal of $R$ using the specified tolerance $\\tau$.\n    e. If $\\widehat{\\nu} > 0$, construct the basis $N$ for the null space of $A$ as derived above. This involves solving a triangular system with $R_{11}$ and permuting the basis vectors back to the original ordering.\n    f. Orthonormalize the basis $N$ to get $\\tilde{N}$.\n2.  Report the rank $\\hat{r}_1$ and nullity $\\widehat{\\nu}_1$ from the first run.\n3.  Calculate the Frobenius norm of the residual, $\\|A\\tilde{N}_1\\|_F$, to assess the quality of the first estimated basis.\n4.  Calculate the maximum principal angle between the subspaces spanned by $\\tilde{N}_1$ and $\\tilde{N}_2$.\n5.  Report a boolean indicating if $\\widehat{\\nu}_1 = \\widehat{\\nu}_2$.\nThese five results are collected for each test case as specified.", "answer": "```python\nimport numpy as np\nfrom scipy import linalg\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final results.\n    \"\"\"\n\n    def analyze_null_space(A, tau, pre_permutation):\n        \"\"\"\n        Analyzes a matrix A to find its null space basis using QR factorization.\n\n        Args:\n            A (np.ndarray): The input matrix.\n            tau (float): The threshold parameter for rank estimation.\n            pre_permutation (np.ndarray): The initial column permutation indices.\n\n        Returns:\n            tuple: A tuple containing:\n                - est_rank (int): Estimated numerical rank.\n                - est_nullity (int): Estimated numerical nullity.\n                - ortho_null_basis (np.ndarray): Orthonormal basis for the null space.\n                - total_perm_indices (np.ndarray): Total permutation indices.\n        \"\"\"\n        m, n = A.shape\n        if n == 0:\n            return 0, 0, np.empty((0, 0)), np.array([], dtype=int)\n\n        A_perm = A[:, pre_permutation]\n        Q, R, qr_pivot_indices = linalg.qr(A_perm, pivoting=True)\n\n        k = min(m, n)\n        diag_R = np.abs(np.diag(R[:k, :k]))\n        \n        # Handle case where diag_R is empty\n        if diag_R.size == 0:\n            max_r_diag = 0.0\n        else:\n            max_r_diag = np.max(diag_R)\n\n        # To avoid threshold being 0 if max_r_diag is 0\n        if max_r_diag > 0:\n            threshold = tau * max_r_diag\n        else:\n            threshold = tau\n            \n        est_rank = np.sum(diag_R >= threshold)\n        est_nullity = n - est_rank\n\n        total_perm_indices = pre_permutation[qr_pivot_indices]\n\n        if est_nullity == 0:\n            return est_rank, est_nullity, np.empty((n, 0)), total_perm_indices\n\n        # Partition R\n        # R is (m,n), we need its upper-left (r x n) part for computation\n        R_rank_part = R[:est_rank, :]\n        R11 = R_rank_part[:, :est_rank]\n        R12 = R_rank_part[:, est_rank:]\n\n        # Solve R11 * X = -R12 using back-substitution\n        X = linalg.solve_triangular(R11, -R12, lower=False)\n\n        # Construct the basis for ker(R) in the permuted space\n        Z = np.vstack((X, np.eye(est_nullity)))\n        \n        # Un-permute the rows of the basis to get the basis for ker(A)\n        # To get the original basis N from Z (where N's columns are permuted into Z), we use:\n        # N[total_perm_indices, :] = Z\n        # This is equivalent to creating an empty N and filling it using the permutation indices.\n        null_basis = np.zeros_like(Z)\n        null_basis[total_perm_indices] = Z\n        \n        # Orthonormalize the basis for stability in angle calculations\n        ortho_null_basis, _ = np.linalg.qr(null_basis)\n\n        return est_rank, est_nullity, ortho_null_basis, total_perm_indices\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: Happy path\n        {\n            \"name\": \"Full row rank, rectangular\",\n            \"seed\": 12345,\n            \"shape\": (4, 7),\n            \"tau\": 1e-12,\n            \"perm1\": np.arange(7),\n            \"perm2\": np.arange(7)[::-1],\n            \"builder\": lambda shape, seed: np.random.default_rng(seed).standard_normal(shape)\n        },\n        # Case 2: Near rank-deficient\n        {\n            \"name\": \"Near rank-deficient\",\n            \"seed\": 2024,\n            \"shape\": (6, 8),\n            \"tau\": 1e-8,\n            \"perm1\": np.arange(8),\n            \"perm2\": np.random.default_rng(4242).permutation(8),\n            \"builder\": lambda shape, seed: (\n                lambda U, S, V: U @ S @ V.T\n            )(\n                linalg.qr(np.random.default_rng(seed).standard_normal((6, 6)))[0],\n                np._core.function_base.diag_indices(6, 8), # placeholder for S creation\n                linalg.qr(np.random.default_rng(seed).standard_normal((8, 8)))[0]\n            )\n        },\n        # Case 3: Equal-norm columns\n        {\n            \"name\": \"Equal-norm columns\",\n            \"seed\": 99,\n            \"shape\": (5, 9),\n            \"tau\": 1e-10,\n            \"perm1\": np.arange(9),\n            \"perm2\": np.arange(9)[::-1],\n            \"builder\": lambda shape, seed: (\n                lambda G: G / np.linalg.norm(G, axis=0)\n            )(np.random.default_rng(seed).standard_normal(shape))\n        },\n        # Case 4: Structured deficiency\n        {\n            \"name\": \"Structured deficiency\",\n            \"seed\": 555,\n            \"shape\": (5, 5),\n            \"tau\": 1e-12,\n            \"perm1\": np.arange(5),\n            \"perm2\": np.array([2, 1, 0, 4, 3]),\n            \"builder\": lambda shape, seed: (\n                lambda B: np.column_stack((B[:, 0], B[:, 1], B[:, 2], B[:, 0] + B[:, 1], B[:, 1]))\n            )(np.random.default_rng(seed).standard_normal((5, 3)))\n        }\n    ]\n\n    # Special handling for test case 2 builder\n    rng_u_v = np.random.default_rng(2024)\n    U_tc2, _ = linalg.qr(rng_u_v.standard_normal((6, 6)))\n    V_tc2, _ = linalg.qr(rng_u_v.standard_normal((8, 8)))\n    S_tc2 = np.zeros((6, 8))\n    s_vals = [1, 1e-1, 1e-2, 1e-3, 1e-12, 1e-12]\n    np.fill_diagonal(S_tc2, s_vals)\n    A_tc2 = U_tc2 @ S_tc2 @ V_tc2.T\n    test_cases[1]['builder'] = lambda shape, seed: A_tc2\n\n\n    results = []\n    for case in test_cases:\n        A = case[\"builder\"](case[\"shape\"], case[\"seed\"])\n        tau = case[\"tau\"]\n        perm1 = case[\"perm1\"]\n        perm2 = case[\"perm2\"]\n        m, n = A.shape\n\n        r1, nu1, N1, _ = analyze_null_space(A, tau, perm1)\n        r2, nu2, N2, _ = analyze_null_space(A, tau, perm2)\n\n        # Calculate Frobenius norm of the residual\n        if nu1 > 0:\n            fro_norm = np.linalg.norm(A @ N1, 'fro')\n        else:\n            fro_norm = 0.0\n\n        # Calculate max principal angle\n        # Only meaningful if both subspaces are non-trivial\n        if nu1 > 0 and nu2 > 0:\n            # subspace_angles returns angles sorted, max is the last one.\n            angles = linalg.subspace_angles(N1, N2)\n            max_angle = angles[-1] if angles.size > 0 else 0.0\n        else:\n            max_angle = 0.0\n\n        # Check for nullity agreement\n        nullity_agrees = (nu1 == nu2)\n\n        results.append([r1, nu1, fro_norm, max_angle, nullity_agrees])\n    \n    # Custom formatting to match the sample output exactly.\n    # Python's default float formatting may differ.\n    final_output_str = \"[\"\n    for i, res_list in enumerate(results):\n        final_output_str += \"[\"\n        final_output_str += f\"{res_list[0]},\"\n        final_output_str += f\"{res_list[1]},\"\n        final_output_str += f\"{res_list[2]:.15e},\"\n        final_output_str += f\"{res_list[3]:.15e},\"\n        final_output_str += f\"{str(res_list[4]).lower()}\"\n        final_output_str += \"]\"\n        if i  len(results) - 1:\n            final_output_str += \",\"\n    final_output_str += \"]\"\n    \n    print(final_output_str)\n\n\nsolve()\n```", "id": "3558928"}, {"introduction": "Building on the theme of numerical rank determination, this exercise explores the power of orthogonal transformations as a preconditioning tool. After proving that such transformations preserve the exact rank and nullity of a matrix, you will design a computational experiment to demonstrate how a random orthogonal premultiplication can dramatically improve the performance of a naive rank-estimation algorithm [@problem_id:3558929]. This highlights a key principle: while a matrix's theoretical rank is fixed, its numerical properties can be favorably altered to make that rank easier to compute.", "problem": "Consider a real matrix $A \\in \\mathbb{R}^{m \\times n}$ and an orthogonal matrix $Q \\in \\mathbb{R}^{m \\times m}$, where an orthogonal matrix is defined by $Q^{\\top} Q = I_m$. The null space $\\mathcal{N}(A)$ is defined as $\\{ x \\in \\mathbb{R}^n : A x = 0 \\}$, and the rank of $A$ is the dimension of the column space of $A$. The rank-nullity theorem states that for any linear map $T : \\mathbb{R}^n \\to \\mathbb{R}^m$ represented by a matrix $A$, one has $\\mathrm{rank}(A) + \\mathrm{nullity}(A) = n$.\n\nYour tasks are:\n\n- Prove from first principles (using only core definitions and widely accepted facts) that for any orthogonal $Q$, the null space is invariant under left multiplication by $Q$, i.e., $\\mathcal{N}(A) = \\mathcal{N}(Q A)$. Consequently, deduce that the nullity and rank of $A$ are preserved under left multiplication by $Q$.\n\n- Design a computational experiment to test whether random orthogonal premultiplication (left multiplication by a random orthogonal $Q$) can improve practical numerical rank detection when a deliberately pivot-averse algorithm is used. Use a pivot-free Gaussian elimination heuristic to estimate numerical rank: at step $k$ (starting from $k=0$), treat the diagonal entry $A_{k,k}$ as a pivot if $|A_{k,k}|  \\tau$, and perform elimination below the pivot without any row or column pivoting; count the number of accepted pivots as the estimated rank. The tolerance $\\tau$ is a fixed positive threshold.\n\n- Implement the following test suite of matrices with controlled low-rank structure and structured row-bursty noise. For each test case, construct $A$ as the sum $A_0 + E$, where $A_0$ has exact rank $r$ and is formed by placing an identity block at the bottom $r$ rows and the left $r$ columns:\n  $$\n  (A_0)_{m-r+i,\\, i} = 1 \\quad \\text{for} \\quad i=0,1,\\dots,r-1,\n  $$\n  and zero elsewhere. The noise $E$ is added only to $K$ randomly selected rows; on each selected row, add independent and identically distributed Gaussian noise of variance $\\sigma^2$ per entry.\n\n- For each test case, generate a random orthogonal matrix $Q$ (for example, by computing the orthogonal factor in the QR decomposition of a dense Gaussian matrix). Estimate the rank using the specified pivot-free elimination on $A$ and on $Q A$, both with the same tolerance $\\tau$. Define the improvement metric as an integer per test case:\n  $$\n  \\mathrm{improve} =\n  \\begin{cases}\n    1  \\text{if the post-premultiplication estimate equals the ground-truth rank } r \\text{ and the pre-premultiplication estimate does not,} \\\\\n    -1  \\text{if the pre-premultiplication estimate equals } r \\text{ and the post-premultiplication estimate does not,} \\\\\n    0  \\text{otherwise.}\n  \\end{cases}\n  $$\n  The ground-truth nullity is $n - r$. The test suite is:\n\n  - Test case $1$: $m=50$, $n=40$, $r=30$, $K=3$, $\\sigma=10^{-6}$, $\\tau=10^{-3}$, random seed $123$.\n  - Test case $2$: $m=40$, $n=50$, $r=30$, $K=3$, $\\sigma=10^{-6}$, $\\tau=10^{-3}$, random seed $234$.\n  - Test case $3$: $m=60$, $n=60$, $r=60$, $K=4$, $\\sigma=10^{-6}$, $\\tau=10^{-3}$, random seed $345$.\n  - Test case $4$: $m=80$, $n=20$, $r=18$, $K=2$, $\\sigma=10^{-6}$, $\\tau=10^{-3}$, random seed $456$.\n\nYour program should produce a single line of output containing the improvement values for the four test cases as a comma-separated list enclosed in square brackets, for example, $[x_1,x_2,x_3,x_4]$, where each $x_i$ is an integer in $\\{-1,0,1\\}$ as defined above. The program must be self-contained, use the specified libraries, and take no input.", "solution": "The problem presents a theoretical proof followed by a computational experiment concerning the rank of a matrix and its behavior under orthogonal transformations. The validation and solution will be addressed in two parts as requested.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- A real matrix $A \\in \\mathbb{R}^{m \\times n}$.\n- An orthogonal matrix $Q \\in \\mathbb{R}^{m \\times m}$, defined by $Q^{\\top} Q = I_m$.\n- Null space definition: $\\mathcal{N}(A) = \\{ x \\in \\mathbb{R}^n : A x = 0 \\}$.\n- Rank definition: The dimension of the column space of $A$.\n- Rank-Nullity Theorem: $\\mathrm{rank}(A) + \\mathrm{nullity}(A) = n$.\n- Theoretical Task: Prove $\\mathcal{N}(A) = \\mathcal{N}(Q A)$ and deduce that $\\mathrm{nullity}(A) = \\mathrm{nullity}(QA)$ and $\\mathrm{rank}(A) = \\mathrm{rank}(QA)$.\n- Computational Task: Design and implement an experiment to test the effect of random orthogonal premultiplication on a specific numerical rank estimation algorithm.\n- Rank Estimation Algorithm: A pivot-free Gaussian elimination heuristic. A pivot is counted at step $k$ if the diagonal element $|A_{k,k}|  \\tau$. Elimination proceeds below the pivot. The estimated rank is the total count of such pivots. No row or column swapping is performed.\n- Matrix Construction for Experiment: $A = A_0 + E$.\n  - $A_0$ is a matrix of exact rank $r$ with $(A_0)_{m-r+i, i} = 1$ for $i=0, 1, \\dots, r-1$, and zero elsewhere.\n  - $E$ is a noise matrix where $K$ randomly selected rows contain i.i.d. Gaussian noise with mean $0$ and variance $\\sigma^2$.\n- Improvement Metric: An integer defined as $1$ if the estimate for $QA$ is correct (equals $r$) while the estimate for $A$ is not; $-1$ if the estimate for $A$ is correct while the estimate for $QA$ is not; and $0$ otherwise.\n- Test Cases:\n  1. $m=50, n=40, r=30, K=3, \\sigma=10^{-6}, \\tau=10^{-3}$, random seed $123$.\n  2. $m=40, n=50, r=30, K=3, \\sigma=10^{-6}, \\tau=10^{-3}$, random seed $234$.\n  3. $m=60, n=60, r=60, K=4, \\sigma=10^{-6}, \\tau=10^{-3}$, random seed $345$.\n  4. $m=80, n=20, r=18, K=2, \\sigma=10^{-6}, \\tau=10^{-3}$, random seed $456$.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientific Grounding**: The problem is firmly rooted in fundamental principles of linear algebra (rank, null space, orthogonal matrices) and numerical linear algebra (rank determination, numerical stability, preconditioning). The concepts are standard and scientifically sound.\n- **Well-Posedness**: The theoretical proof required is a standard result. The computational experiment is meticulously defined, with all parameters, matrix structures, algorithms, and evaluation metrics specified. The use of random seeds ensures that the computational experiment has a unique, reproducible outcome.\n- **Objectivity**: The language is precise and mathematical, free of any subjective or opinion-based assertions.\n\nThe problem does not exhibit any flaws such as scientific unsoundness, ambiguity, missing information, or contradictions. The construction of the test matrices $A_0$ is a deliberate and sophisticated choice, creating a scenario where the naive rank estimation algorithm is designed to fail, thus making the test for improvement meaningful. It is a well-posed and substantive problem.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A complete solution will be provided.\n\n### Solution\n\n**Part 1: Theoretical Proof**\n\nWe are tasked to prove that for a real matrix $A \\in \\mathbb{R}^{m \\times n}$ and an orthogonal matrix $Q \\in \\mathbb{R}^{m \\times m}$, the null space is invariant under left multiplication by $Q$, that is $\\mathcal{N}(A) = \\mathcal{N}(Q A)$. We will prove this by demonstrating mutual inclusion.\n\n1.  **Prove $\\mathcal{N}(A) \\subseteq \\mathcal{N}(Q A)$**:\n    Let $x$ be an arbitrary vector in the null space of $A$, i.e., $x \\in \\mathcal{N}(A)$. By definition, this means:\n    $$A x = 0$$\n    where $0$ is the zero vector in $\\mathbb{R}^m$. Left-multiplying this equation by the matrix $Q$, we get:\n    $$Q (A x) = Q 0$$\n    Using the associativity of matrix multiplication, we have:\n    $$(Q A) x = 0$$\n    This equation shows that the vector $x$ is in the null space of the matrix $QA$. Therefore, $x \\in \\mathcal{N}(Q A)$. Since $x$ was an arbitrary element of $\\mathcal{N}(A)$, we conclude that $\\mathcal{N}(A) \\subseteq \\mathcal{N}(Q A)$.\n\n2.  **Prove $\\mathcal{N}(Q A) \\subseteq \\mathcal{N}(A)$**:\n    Let $y$ be an arbitrary vector in the null space of $QA$, i.e., $y \\in \\mathcal{N}(Q A)$. By definition:\n    $$(Q A) y = 0$$\n    The matrix $Q$ is given as orthogonal, which means $Q^{\\top} Q = I_m$, where $I_m$ is the $m \\times m$ identity matrix. This property implies that $Q$ is invertible, and its inverse is its transpose, $Q^{-1} = Q^{\\top}$. Left-multiplying the equation by $Q^{\\top}$, we obtain:\n    $$Q^{\\top} (Q A) y = Q^{\\top} 0$$\n    Using associativity of matrix multiplication again:\n    $$(Q^{\\top} Q) A y = 0$$\n    Substituting $Q^{\\top} Q = I_m$:\n    $$I_m A y = 0$$\n    $$A y = 0$$\n    This equation shows that the vector $y$ is in the null space of $A$. Therefore, $y \\in \\mathcal{N}(A)$. Since $y$ was an arbitrary element of $\\mathcal{N}(Q A)$, we conclude that $\\mathcal{N}(Q A) \\subseteq \\mathcal{N}(A)$.\n\nCombining both inclusions, we have established that the sets are identical:\n$$\\mathcal{N}(A) = \\mathcal{N}(Q A)$$\n\n**Consequences for Nullity and Rank**\n\n-   **Nullity**: The nullity of a matrix is defined as the dimension of its null space, $\\mathrm{nullity}(M) = \\dim(\\mathcal{N}(M))$. Since we have proven that $\\mathcal{N}(A) = \\mathcal{N}(Q A)$, their dimensions must be equal. Thus:\n    $$\\mathrm{nullity}(A) = \\dim(\\mathcal{N}(A)) = \\dim(\\mathcal{N}(Q A)) = \\mathrm{nullity}(Q A)$$\n    The nullity is preserved under left multiplication by an orthogonal matrix.\n\n-   **Rank**: The rank-nullity theorem states that for any matrix $M \\in \\mathbb{R}^{m \\times n}$, the sum of its rank and nullity equals the number of columns, $n$:\n    $$\\mathrm{rank}(M) + \\mathrm{nullity}(M) = n$$\n    Applying this theorem to matrix $A$:\n    $$\\mathrm{rank}(A) = n - \\mathrm{nullity}(A)$$\n    Applying it to matrix $QA$:\n    $$\\mathrm{rank}(Q A) = n - \\mathrm{nullity}(Q A)$$\n    Since we have just shown that $\\mathrm{nullity}(A) = \\mathrm{nullity}(Q A)$, it follows directly that:\n    $$\\mathrm{rank}(A) = \\mathrm{rank}(Q A)$$\n    The rank is also preserved under left multiplication by an orthogonal matrix. This completes the theoretical proof.\n\n**Part 2: Computational Experiment Design and Implementation**\n\nThe experiment aims to demonstrate a practical aspect of this theoretical result. While $\\mathrm{rank}(A)$ and $\\mathrm{rank}(QA)$ are theoretically identical, their numerical estimation can differ significantly depending on the algorithm and the matrix structure.\n\nThe chosen rank estimation algorithm, a pivot-free Gaussian elimination, is intentionally naive. Its success depends entirely on finding sufficiently large elements on the main diagonal. The matrix $A$ is constructed as $A_0 + E$. The base matrix $A_0$ has its only non-zero entries (all equal to $1$) located in the bottom-left corner, specifically at positions $(m-r+i, i)$ for $i \\in \\{0, \\dots, r-1\\}$. This ensures that the first $\\min(m, n, r)$ diagonal entries of $A_0$ are all zero. The added noise $E$ is of small magnitude ($\\sigma=10^{-6}$), so it is unlikely to create diagonal entries greater than the tolerance $\\tau=10^{-3}$ on its own. Consequently, the naive algorithm is expected to fail on matrix $A$, estimating a rank far below the true rank $r$.\n\nLeft-multiplying $A$ by a random orthogonal matrix $Q$ effectively performs a random rotation/reflection of the rows of $A$. This scrambling operation mixes the rows, distributing the large values from the bottom-left of $A_0$ across many rows of $QA$. It is highly probable that this will create large-magnitude entries on the diagonal of $QA$, allowing the naive algorithm to correctly identify the pivots and estimate the rank as $r$.\n\nThe implementation will follow this logic, executing the four specified test cases and calculating the improvement metric for each. The metric will be $1$ if random premultiplication \"fixes\" the rank estimate, $-1$ if it \"breaks\" it, and $0$ otherwise. Given the setup, we expect the improvement to be $1$ in most, if not all, cases where the initial structure foils the algorithm.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import qr\n\ndef solve():\n    \"\"\"\n    Executes the computational experiment to test the effect of orthogonal\n    premultiplication on a naive rank estimation algorithm.\n    \"\"\"\n    \n    # Test cases as specified in the problem statement.\n    # Format: (m, n, r, K, sigma, tau, random_seed)\n    test_cases = [\n        (50, 40, 30, 3, 1e-6, 1e-3, 123),\n        (40, 50, 30, 3, 1e-6, 1e-3, 234),\n        (60, 60, 60, 4, 1e-6, 1e-3, 345),\n        (80, 20, 18, 2, 1e-6, 1e-3, 456),\n    ]\n\n    results = []\n\n    def estimate_rank_naive(matrix, tolerance):\n        \"\"\"\n        Estimates the rank of a matrix using a pivot-free Gaussian elimination\n        heuristic. The rank is the number of diagonal pivots with magnitude\n        greater than the tolerance. No pivoting is performed.\n        \"\"\"\n        M = matrix.copy()\n        m, n = M.shape\n        num_pivots = 0\n        max_pivots = min(m, n)\n        \n        for k in range(max_pivots):\n            # Check if the diagonal element can be accepted as a pivot\n            if np.abs(M[k, k]) > tolerance:\n                num_pivots += 1\n                pivot_val = M[k, k]\n                # Perform elimination on the rows below the pivot\n                if k  m - 1: # Ensure there are rows below\n                    for i in range(k + 1, m):\n                        factor = M[i, k] / pivot_val\n                        M[i, k:] -= factor * M[k, k:]\n        return num_pivots\n\n    for m, n, r, K, sigma, tau, seed in test_cases:\n        # Initialize the random number generator for reproducibility of the case\n        rng = np.random.default_rng(seed)\n\n        # 1. Construct the matrix A = A0 + E\n        # A0 has rank r with an identity block in the bottom-left\n        A0 = np.zeros((m, n))\n        if r > 0 and r = m and r = n:\n            row_indices = np.arange(m - r, m)\n            col_indices = np.arange(r)\n            A0[row_indices[:, np.newaxis], col_indices] = np.eye(r)\n\n        # E is zero except for K randomly chosen rows with Gaussian noise\n        E = np.zeros((m, n))\n        if K > 0:\n            noise_row_indices = rng.choice(m, size=K, replace=False)\n            for row_idx in noise_row_indices:\n                E[row_idx, :] = rng.normal(loc=0.0, scale=sigma, size=n)\n\n        A = A0 + E\n\n        # 2. Generate a random orthogonal matrix Q of size m x m\n        # A standard method is to take the Q factor of a random Gaussian matrix's QR decomposition.\n        Z = rng.standard_normal((m, m))\n        Q, _ = qr(Z)\n\n        # 3. Estimate rank of A and QA\n        rank_A_est = estimate_rank_naive(A, tau)\n        \n        QA = Q @ A\n        rank_QA_est = estimate_rank_naive(QA, tau)\n\n        # 4. Calculate the improvement metric\n        est_A_correct = (rank_A_est == r)\n        est_QA_correct = (rank_QA_est == r)\n        \n        improvement = 0\n        if est_QA_correct and not est_A_correct:\n            improvement = 1\n        elif est_A_correct and not est_QA_correct:\n            improvement = -1\n        \n        results.append(improvement)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3558929"}, {"introduction": "Our final practice ventures into the advanced topic of Krylov subspaces, which are foundational to modern iterative methods. You will first establish the direct link between the rank of a Krylov matrix and the dimension of the corresponding subspace, and then implement a sophisticated diagnostic to distinguish a true, structurally stable rank deficiency from severe ill-conditioning that merely mimics it [@problem_id:3558881]. This exercise confronts the subtle but critical challenge of interpreting numerical rank in the context of nearly-dependent vector sequences.", "problem": "Let $A \\in \\mathbb{R}^{n \\times n}$ and $b \\in \\mathbb{R}^n$. Define the $k$-step Krylov subspace generated by $A$ and $b$ as $\\mathcal{K}_k(A,b) = \\mathrm{span}\\{b, Ab, A^2 b, \\dots, A^{k-1} b\\}$. Consider the square $n \\times n$ Krylov matrix $K_n(A,b) = [b, Ab, \\dots, A^{n-1} b]$ formed by concatenating the Krylov vectors as columns.\n\nStarting from core definitions and well-tested facts, derive a precise relationship between the dimension of the Krylov subspace and the rank of the Krylov matrix. Then, design and implement a numerical diagnostic that: (i) estimates the dimension of the Krylov subspace by computing the numerical rank of $K_n(A,b)$ via the Singular Value Decomposition (SVD; Singular Value Decomposition), and (ii) distinguishes a true rank deficiency (exact linear dependence among the Krylov columns) from slow Krylov growth due to near-dependence caused by defective or nearly defective $A$.\n\nUse as fundamental base:\n- The definition of rank, nullspace, and column space of a matrix.\n- The rank-nullity theorem for linear maps $T : \\mathbb{R}^n \\to \\mathbb{R}^n$, stating that $\\mathrm{rank}(T) + \\mathrm{nullity}(T) = n$.\n- The Singular Value Decomposition (SVD) characterizes the Euclidean operator norm and the distance of a matrix to the set of rank-deficient matrices via its smallest singular value.\n\nYour program must:\n1. Construct $K_n(A,b)$.\n2. Estimate the numerical rank $r$ of $K_n(A,b)$ using the criterion $r = \\#\\{ \\sigma_i : \\sigma_i  \\tau \\}$, where $\\sigma_i$ are the singular values of $K_n(A,b)$ and $\\tau = n \\cdot \\epsilon \\cdot \\|K_n(A,b)\\|_2$, with $\\epsilon$ the machine precision for double precision floating point.\n3. Compute the smallest singular value $\\sigma_{\\min}(K_n(A,b))$.\n4. Implement a stability-based diagnostic to classify the observed low rank as either a true deficiency or slow growth. The diagnostic must perform the following:\n   - Compute the baseline rank $r_0$ with threshold $\\tau_0 = n \\cdot \\epsilon \\cdot \\|K_n(A,b)\\|_2$.\n   - Vary the threshold across the set $\\{\\tau_0/10, \\tau_0, 10\\tau_0\\}$ and recompute the rank.\n   - Apply small, norm-scaled perturbations of magnitude $\\delta_A = 10^{-12} \\cdot \\max\\{1, \\|A\\|_2\\}$ to $A$ and $\\delta_b = 10^{-12} \\cdot \\max\\{1, \\|b\\|_2\\}$ to $b$, and recompute the rank under the same three thresholds for each perturbation case: $(A,b)$, $(A,b+\\Delta b)$, $(A+\\Delta A, b)$, and $(A+\\Delta A, b+\\Delta b)$ with randomly generated perturbations $\\Delta A$ and $\\Delta b$ using a fixed random seed for reproducibility.\n   - Define the stability score $s$ as the fraction of all recomputed ranks equal to $r_0$. Classify as \"true deficiency\" if $r_0  n$ and $s \\geq 0.9$, and as \"slow growth\" otherwise.\n\nProve the relationship between $\\dim \\mathcal{K}_n(A,b)$ and $\\mathrm{rank}(K_n(A,b))$ from the definitions and rank-nullity, and justify the numerical diagnostic design based on SVD properties and perturbation stability.\n\nYou must implement the above as a complete, runnable program. No user input is permitted.\n\nTest suite:\n- Case $1$ (structural deficiency in diagonal $A$): $n=6$, $A = \\mathrm{diag}(1,2,3,4,5,6)$, $b = [1, 1, 0, 0, 0, 0]^\\top$.\n- Case $2$ (defective $A$ with a single Jordan block, slow growth): $n=6$, $A = J_6(1)$, where $J_6(1)$ has ones on the diagonal and ones on the superdiagonal, zeros elsewhere; $b = e_1$.\n- Case $3$ (nearly defective, clustered spectrum, full rank but ill-conditioned): $n=6$, $A = \\mathrm{diag}(1, 1+10^{-12}, 1+2\\cdot 10^{-12}, 1+3\\cdot 10^{-12}, 1+4\\cdot 10^{-12}, 1+5\\cdot 10^{-12}) + U$, where $U$ is strictly upper bidiagonal with $10^{-12}$ on the superdiagonal; $b$ is a fixed random vector with entries drawn from the standard normal distribution and then scaled to unit norm using a fixed seed.\n- Case $4$ (boundary case): $n=6$, $A = \\mathrm{diag}(1,2,3,4,5,6)$, $b = 0$.\n\nAnswer specification:\n- For each case, output a list $[r, \\sigma_{\\min}, f]$ where $r$ is the estimated numerical rank (an integer), $\\sigma_{\\min}$ is the smallest singular value of $K_n(A,b)$ (a float), and $f$ is the classification flag (an integer, with $1$ indicating \"true deficiency\" and $0$ indicating \"slow growth or no deficiency\").\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for the four cases, in order. For example, the format must be $[[r_1,\\sigma_{\\min,1},f_1],[r_2,\\sigma_{\\min,2},f_2],[r_3,\\sigma_{\\min,3},f_3],[r_4,\\sigma_{\\min,4},f_4]]$.\n- No physical units or angles are involved; all quantities are pure numbers.", "solution": "The problem is assessed as valid. It is scientifically grounded in numerical linear algebra, well-posed with a clear objective and a deterministic procedure, and is formally expressed. It presents a non-trivial challenge in distinguishing true rank deficiency from numerical ill-conditioning within the context of Krylov subspaces.\n\n### Theoretical Derivation\n\nThe problem asks for a precise relationship between the dimension of the Krylov subspace $\\mathcal{K}_n(A,b)$ and the rank of the Krylov matrix $K_n(A,b)$, based on fundamental definitions.\n\nLet $A \\in \\mathbb{R}^{n \\times n}$ and $b \\in \\mathbb{R}^n$.\nThe Krylov subspace is defined as the span of the first $n$ Krylov vectors:\n$$\n\\mathcal{K}_n(A,b) = \\mathrm{span}\\{b, Ab, A^2 b, \\dots, A^{n-1} b\\}\n$$\nThe dimension of this subspace, $\\dim(\\mathcal{K}_n(A,b))$, is the size of the largest linearly independent subset of the generating vectors $\\{b, Ab, \\dots, A^{n-1} b\\}$.\n\nThe Krylov matrix is formed by concatenating these vectors as its columns:\n$$\nK_n(A,b) = [b | Ab | A^2 b | \\dots | A^{n-1} b]\n$$\nThe column space of a matrix is the subspace spanned by its column vectors. By definition, the column space of $K_n(A,b)$, denoted $\\mathrm{col}(K_n(A,b))$, is:\n$$\n\\mathrm{col}(K_n(A,b)) = \\mathrm{span}\\{b, Ab, A^2 b, \\dots, A^{n-1} b\\}\n$$\nBy direct comparison of their definitions, the Krylov subspace and the column space of the Krylov matrix are identical:\n$$\n\\mathcal{K}_n(A,b) = \\mathrm{col}(K_n(A,b))\n$$\nThe rank of a matrix is defined as the dimension of its column space. That is:\n$$\n\\mathrm{rank}(K_n(A,b)) = \\dim(\\mathrm{col}(K_n(A,b)))\n$$\nCombining these two equalities yields the direct relationship:\n$$\n\\dim(\\mathcal{K}_n(A,b)) = \\mathrm{rank}(K_n(A,b))\n$$\nThis establishes that the dimension of the subspace is precisely the rank of the matrix whose columns generate that subspace. The rank-nullity theorem, $\\mathrm{rank}(T) + \\mathrm{nullity}(T) = n$, applies to the linear map $T_K: \\mathbb{R}^n \\to \\mathbb{R}^n$ represented by the matrix $K_n(A,b)$. The rank of this map is the dimension of its image, which is the column space. The nullity is the dimension of the kernel (or nullspace), which consists of coefficient vectors $c$ such that $\\sum_{i=0}^{n-1} c_i A^i b = 0$. Thus, the rank-nullity theorem connects the dimension of the Krylov subspace to the dimension of the space of linear dependencies among the Krylov vectors.\n\n### Justification of the Numerical Diagnostic\n\nThe diagnostic is designed to distinguish between two scenarios that can lead to a numerically low-rank Krylov matrix.\n\n1.  **Numerical Rank via SVD**: In finite-precision arithmetic, true mathematical rank is obscured by round-off errors. A matrix that is theoretically rank-deficient will have singular values that are computationally small but not exactly zero. The Singular Value Decomposition (SVD), $K_n(A,b) = U\\Sigma V^\\top$, where $\\Sigma = \\mathrm{diag}(\\sigma_1, \\dots, \\sigma_n)$, is the ideal tool for this analysis. The singular values $\\sigma_i$ measure the \"energy\" of the matrix in different orthogonal directions. A sharp drop in the magnitude of singular values indicates near rank-deficiency. The threshold $\\tau = n \\cdot \\epsilon \\cdot \\|K_n(A,b)\\|_2$ provides a standard, dynamically scaled cutoff to separate significant singular values from those considered to be numerical artifacts. Here, $\\|K_n(A,b)\\|_2 = \\sigma_1$ is the largest singular value, and $\\epsilon$ is machine precision.\n\n2.  **Stability-Based Classification**: The core of the diagnostic rests on the principle of structural stability.\n    *   **True Rank Deficiency**: This arises from an algebraic constraint, for instance, if the minimal polynomial of $A$ with respect to $b$ has a degree $d  n$. This means $\\dim(\\mathcal{K}_n(A,b)) = d$. The Krylov matrix $K_n(A,b)$ is exactly rank-deficient, possessing $n-d$ singular values that are mathematically zero. This is a structurally stable property. Small perturbations to $A$ and $b$ will only lead to small perturbations of these zero singular values. Similarly, modest variations in the rank-determination threshold (e.g., from $\\tau_0/10$ to $10\\tau_0$) are unlikely to cross the significant gap between the small and large singular values. The computed rank $r_0$ should therefore be highly consistent across these tests. The stability score $s$, defined as the fraction of tests yielding rank $r_0$, will be close to $1$. The criterion $s \\geq 0.9$ formalizes this expectation.\n    *   **Slow Growth (Ill-Conditioning)**: This occurs when the Krylov vectors $A^k b$ become nearly linearly dependent, but are not exactly so. This is typical for defective or nearly defective matrices (e.g., those with clustered eigenvalues). The Krylov matrix $K_n(A,b)$ is theoretically full-rank but severely ill-conditioned, meaning its condition number $\\kappa_2(K_n) = \\sigma_1/\\sigma_n$ is very large. The singular values decay gradually without a clear gap. Such a matrix is close to a rank-deficient one, as quantified by the Eckart-Young-Mirsky theorem: $\\min_{\\mathrm{rank}(X)n} \\|K_n-X\\|_2 = \\sigma_n$. In this scenario, the computed numerical rank is highly sensitive to the choice of threshold $\\tau$. Furthermore, the singular values of an ill-conditioned matrix are themselves sensitive to perturbations in the matrix entries. Therefore, small changes to $A$ and $b$ will cause significant fluctuations in the small singular values, leading to a computed rank that varies under perturbation. The stability score $s$ will be low, captured by the condition $s  0.9$. This instability is the hallmark of \"slow growth\".\n\nThe diagnostic systematically probes this stability. By varying both the rank-testing threshold and the input data $(A,b)$, it distinguishes the stable, gapped singular value spectrum of a truly deficient matrix from the unstable, gradually decaying spectrum of an ill-conditioned one.", "answer": "```python\nimport numpy as np\n\ndef construct_krylov(A, b, n):\n    \"\"\"\n    Constructs the n x n Krylov matrix K_n(A,b).\n    \"\"\"\n    K = np.zeros((n, n), dtype=float)\n    if np.linalg.norm(b) == 0:\n        return K\n    \n    current_vec = b.copy()\n    K[:, 0] = current_vec\n    for i in range(1, n):\n        current_vec = A @ current_vec\n        K[:, i] = current_vec\n    return K\n\ndef run_diagnostic(A, b, n):\n    \"\"\"\n    Performs the numerical diagnostic on K_n(A,b).\n    Returns [rank, sigma_min, classification_flag].\n    \"\"\"\n    # 1. Construct the unperturbed Krylov matrix and compute its properties.\n    K = construct_krylov(A, b, n)\n    \n    s_vals = np.linalg.svd(K, compute_uv=False)\n    \n    # Handle the zero matrix case (e.g., b=0).\n    if K.any(): # K is not the zero matrix\n        norm_K = s_vals[0]\n        sigma_min = s_vals[-1]\n    else:\n        norm_K = 0.0\n        sigma_min = 0.0\n\n    eps = np.finfo(float).eps\n    tau_0 = n * eps * norm_K\n    \n    r_0 = np.sum(s_vals > tau_0)\n    \n    # 2. Perform stability analysis.\n    rng = np.random.default_rng(seed=42)\n    \n    # Define perturbation magnitudes.\n    norm_A = np.linalg.norm(A, 2)\n    norm_b = np.linalg.norm(b, 2)\n    delta_A_mag = 1e-12 * max(1.0, norm_A)\n    delta_b_mag = 1e-12 * max(1.0, norm_b)\n\n    # Generate and scale random perturbations.\n    dA_rand = rng.standard_normal(size=(n, n))\n    db_rand = rng.standard_normal(size=n)\n    \n    dA = delta_A_mag * dA_rand / np.linalg.norm(dA_rand, 2)\n    \n    norm_db_rand = np.linalg.norm(db_rand)\n    db = delta_b_mag * db_rand / norm_db_rand if norm_db_rand > 0 else np.zeros(n)\n    \n    # Define systems and thresholds to test.\n    perturbed_systems = [\n        (A, b),\n        (A, b + db),\n        (A + dA, b),\n        (A + dA, b + db)\n    ]\n    \n    thresholds = [tau_0 / 10.0, tau_0, tau_0 * 10.0] if tau_0 > 0 else [0.0, 0.0, 0.0]\n\n    all_ranks = []\n    \n    for A_p, b_p in perturbed_systems:\n        K_p = construct_krylov(A_p, b_p, n)\n        s_p = np.linalg.svd(K_p, compute_uv=False)\n        for tau in thresholds:\n            rank_p = np.sum(s_p > tau)\n            all_ranks.append(rank_p)\n            \n    # 3. Calculate stability score and classify.\n    stable_count = all_ranks.count(r_0)\n    stability_score = stable_count / len(all_ranks)\n\n    is_deficient = (r_0  n)\n    is_stable = (stability_score >= 0.9)\n\n    # Classification flag: 1 for true deficiency, 0 otherwise.\n    f = 1 if (is_deficient and is_stable) else 0\n\n    return [int(r_0), sigma_min, f]\n\ndef solve():\n    \"\"\"\n    Main function to define test cases and run the diagnostic for each.\n    \"\"\"\n    n = 6\n    \n    # Case 1: Structural deficiency in a diagonal matrix.\n    A1 = np.diag([1.0, 2.0, 3.0, 4.0, 5.0, 6.0])\n    b1 = np.array([1.0, 1.0, 0.0, 0.0, 0.0, 0.0])\n    \n    # Case 2: Defective matrix (Jordan block), slow growth.\n    A2 = np.diag(np.ones(n)) + np.diag(np.ones(n - 1), k=1)\n    b2 = np.zeros(n)\n    b2[0] = 1.0\n    \n    # Case 3: Nearly defective matrix, ill-conditioned.\n    eps_m = 1e-12\n    diag_vals = 1.0 + np.arange(n) * eps_m\n    A3 = np.diag(diag_vals) + np.diag(np.full(n - 1, eps_m), k=1)\n    rng_b = np.random.default_rng(seed=123) # Fixed seed for b reproducibility\n    b3_rand = rng_b.standard_normal(n)\n    b3 = b3_rand / np.linalg.norm(b3_rand)\n\n    # Case 4: Boundary case with zero vector.\n    A4 = np.diag([1.0, 2.0, 3.0, 4.0, 5.0, 6.0])\n    b4 = np.zeros(n)\n    \n    test_cases = [\n        (A1, b1, n),\n        (A2, b2, n),\n        (A3, b3, n),\n        (A4, b4, n)\n    ]\n\n    results = []\n    for A, b, n_val in test_cases:\n        result = run_diagnostic(A, b, n_val)\n        results.append(result)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3558881"}]}