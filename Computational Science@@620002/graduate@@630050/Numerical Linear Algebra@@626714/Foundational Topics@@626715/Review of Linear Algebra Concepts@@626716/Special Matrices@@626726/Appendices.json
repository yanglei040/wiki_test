{"hands_on_practices": [{"introduction": "Circulant matrices represent a cornerstone in the study of structured matrices, appearing frequently in signal processing, image analysis, and the numerical solution of differential equations. Their simple, elegant structure—where each row is a cyclic shift of the one above it—belies a deep connection to Fourier analysis. This practice guides you through discovering the eigenvalues and eigenvectors of any circulant matrix, revealing why they are diagonalized by the Discrete Fourier Transform matrix and how this property allows for matrix-vector products to be computed with remarkable efficiency using the Fast Fourier Transform (FFT) [@problem_id:3580686].", "problem": "Let $n \\in \\mathbb{N}$ and let $C \\in \\mathbb{C}^{n \\times n}$ be a circulant matrix with first column $(c_0,c_1,\\dots,c_{n-1})^{\\top}$, written as $C=\\mathrm{circ}(c_0,\\dots,c_{n-1})$. Define $\\omega_n=\\exp(-2\\pi \\mathrm{i}/n)$ and, for each integer $k$ with $0 \\leq k \\leq n-1$, consider the vector $v_k \\in \\mathbb{C}^n$ whose $j$-th component is $[v_k]_j=\\omega_n^{jk}$ for $0 \\leq j \\leq n-1$. Starting from the definition of a circulant matrix and the basic properties of the Discrete Fourier Transform (DFT), derive a closed-form expression for the $k$-th eigenvalue of $C$ in terms of $(c_j)_{j=0}^{n-1}$ and $\\omega_n$.\n\nSeparately, consider computing the matrix-vector product $Cx$ for a single given $x \\in \\mathbb{C}^n$ using the Fast Fourier Transform (FFT). Assume $n$ is a power of $2$ and that you use the radix-$2$ Cooley–Tukey Fast Fourier Transform (FFT), which for a length-$n$ complex input performs $n\\log_2(n)$ complex additions and $(n/2)\\log_2(n)$ complex multiplications; assume the inverse FFT has the same operation counts. Assume that computing $Cx$ via FFTs proceeds by computing the DFT of $c$, the DFT of $x$, an elementwise complex multiplication in the frequency domain, and then applying the inverse DFT to return to the time domain, and that the DFT of $c$ is not precomputed. Take the unit cost of a complex addition to be $1$ and the unit cost of a complex multiplication to be $1$, and ignore memory access costs. Under these assumptions, provide a single closed-form analytic expression in $n$ for the total number of complex arithmetic operations required to compute $Cx$ once by this FFT-based method.\n\nYour final answer must be a single analytical expression or a two-entry row matrix collecting both results, as specified. No rounding is required.", "solution": "The user has presented a two-part problem concerning circulant matrices. The first part requires the derivation of the eigenvalues of a circulant matrix. The second part asks for the computational cost of a matrix-vector product involving a circulant matrix, using an FFT-based method. The problem has been validated and found to be self-contained, scientifically grounded, and well-posed.\n\nThe solution is presented in two parts as per the problem structure.\n\nPart 1: Eigenvalues of a Circulant Matrix\n\nLet $C \\in \\mathbb{C}^{n \\times n}$ be a circulant matrix defined by its first column, $c = (c_0, c_1, \\dots, c_{n-1})^{\\top}$. The elements of the matrix, using $0$-based indexing for rows $i$ and columns $j$ (where $0 \\le i, j \\le n-1$), are given by $C_{ij} = c_{(i-j) \\pmod n}$.\n\nThe problem defines a set of vectors $v_k \\in \\mathbb{C}^n$ for $k \\in \\{0, 1, \\dots, n-1\\}$. The $j$-th component of vector $v_k$ is given by $[v_k]_j = \\omega_n^{jk}$, where $\\omega_n = \\exp(-2\\pi \\mathrm{i}/n)$.\n\nTo find the eigenvalue $\\lambda_k$ corresponding to the vector $v_k$, we must show that $v_k$ is an eigenvector of $C$ by verifying the eigenvalue equation $Cv_k = \\lambda_k v_k$. We compute the $i$-th component of the matrix-vector product $Cv_k$:\n$$\n[Cv_k]_i = \\sum_{j=0}^{n-1} C_{ij} [v_k]_j\n$$\nSubstituting the expressions for $C_{ij}$ and $[v_k]_j$:\n$$\n[Cv_k]_i = \\sum_{j=0}^{n-1} c_{(i-j) \\pmod n} \\omega_n^{jk}\n$$\nTo simplify this summation, we perform a change of variables. Let $l = (i-j) \\pmod n$. As the index $j$ runs from $0$ to $n-1$, for a fixed $i$, the index $l$ also runs through all values from $0$ to $n-1$ exactly once. From the definition of $l$, we can write $j \\equiv i-l \\pmod n$. Thus, we can substitute $m = i-l$ into the exponent of $\\omega_n$:\n$$\n\\omega_n^{jk} = \\omega_n^{((i-l) \\pmod n)k}\n$$\nSince $\\omega_n^n = \\exp(-2\\pi \\mathrm{i}) = 1$, we have $\\omega_n^{nk} = (\\omega_n^n)^k = 1$. This implies that $\\omega_n^{a \\pmod n} = \\omega_n^a$ for any integer $a$. Therefore, $\\omega_n^{((i-l) \\pmod n)k} = \\omega_n^{(i-l)k}$.\nThe summation becomes:\n$$\n[Cv_k]_i = \\sum_{l=0}^{n-1} c_l \\omega_n^{(i-l)k}\n$$\nWe can separate the terms dependent on $i$ from the summation:\n$$\n[Cv_k]_i = \\sum_{l=0}^{n-1} c_l \\omega_n^{ik} \\omega_n^{-lk} = \\omega_n^{ik} \\left( \\sum_{l=0}^{n-1} c_l \\omega_n^{-lk} \\right)\n$$\nThe term $\\omega_n^{ik}$ is the $i$-th component of the vector $v_k$, i.e., $[v_k]_i$. The term in the parenthesis is a scalar value that does not depend on the component index $i$. Let's denote this scalar by $\\lambda_k$.\n$$\n\\lambda_k = \\sum_{l=0}^{n-1} c_l \\omega_n^{-lk}\n$$\nThus, we have shown that $[Cv_k]_i = \\lambda_k [v_k]_i$ for all $i \\in \\{0, 1, \\dots, n-1\\}$, which confirms that $v_k$ is an eigenvector of $C$ with the corresponding eigenvalue $\\lambda_k$.\n\nReplacing the summation index $l$ with $j$ for notational consistency, the closed-form expression for the $k$-th eigenvalue is:\n$$\n\\lambda_k = \\sum_{j=0}^{n-1} c_j \\omega_n^{-jk}\n$$\nThis expression gives the eigenvalue associated with the specific eigenvector $v_k$ defined in the problem.\n\nPart 2: Computational Cost of $Cx$ via FFT\n\nThe problem requires the calculation of the total number of complex arithmetic operations to compute the matrix-vector product $y = Cx$ using a specified FFT-based method. The product $Cx$ is a circular convolution of the vector $c$ (the first column of $C$) and the vector $x$. The components of $y$ are given by $y_i = \\sum_{j=0}^{n-1} c_{(i-j)\\pmod n} x_j$.\n\nThe convolution theorem states that the Discrete Fourier Transform (DFT) of a circular convolution of two vectors is the element-wise product of their individual DFTs. That is, $\\text{DFT}(y) = \\text{DFT}(c) \\odot \\text{DFT}(x)$, where $\\odot$ denotes element-wise multiplication. To obtain $y$, we then apply the inverse DFT (IDFT): $y = \\text{IDFT}(\\text{DFT}(c) \\odot \\text{DFT}(x))$.\n\nThe procedure, as outlined in the problem, consists of four steps:\n1.  Compute the DFT of the vector $c$, let's call it $\\hat{c} = \\text{DFT}(c)$.\n2.  Compute the DFT of the vector $x$, let's call it $\\hat{x} = \\text{DFT}(x)$.\n3.  Compute the element-wise product $\\hat{y} = \\hat{c} \\odot \\hat{x}$.\n4.  Compute the IDFT of $\\hat{y}$ to get the final result $y = \\text{IDFT}(\\hat{y})$.\n\nThe costs of these operations are given. For a length-$n$ input, where $n$ is a power of $2$, the radix-$2$ Cooley-Tukey FFT requires:\n-   Complex additions: $A_{FFT} = n\\log_2(n)$\n-   Complex multiplications: $M_{FFT} = \\frac{n}{2}\\log_2(n)$\n\nThe problem states that the cost of an IDFT is the same as the cost of an FFT. A complex addition and a complex multiplication each have a unit cost of $1$. The total number of complex arithmetic operations for one FFT or IDFT is the sum of additions and multiplications:\n$$\nC_{transform} = A_{FFT} + M_{FFT} = n\\log_2(n) + \\frac{n}{2}\\log_2(n) = \\frac{3}{2}n\\log_2(n)\n$$\nNow, we sum the costs for each step of the procedure:\n1.  Cost of $\\text{DFT}(c)$: $C_{transform} = \\frac{3}{2}n\\log_2(n)$.\n2.  Cost of $\\text{DFT}(x)$: $C_{transform} = \\frac{3}{2}n\\log_2(n)$.\n3.  Cost of element-wise product: This step involves multiplying $n$ pairs of complex numbers. The cost is $n$ complex multiplications, which corresponds to $n$ operations.\n4.  Cost of $\\text{IDFT}(\\hat{y})$: $C_{transform} = \\frac{3}{2}n\\log_2(n)$.\n\nThe total number of complex arithmetic operations is the sum of the costs of these four steps:\n$$\n\\text{Total Cost} = C(\\text{DFT}(c)) + C(\\text{DFT}(x)) + C(\\text{product}) + C(\\text{IDFT})\n$$\n$$\n\\text{Total Cost} = \\left(\\frac{3}{2}n\\log_2(n)\\right) + \\left(\\frac{3}{2}n\\log_2(n)\\right) + n + \\left(\\frac{3}{2}n\\log_2(n)\\right)\n$$\nSumming these terms:\n$$\n\\text{Total Cost} = 3 \\times \\left(\\frac{3}{2}n\\log_2(n)\\right) + n = \\frac{9}{2}n\\log_2(n) + n\n$$\nThis is the required single closed-form analytic expression for the total number of complex arithmetic operations.", "answer": "$$\n\\boxed{\\begin{pmatrix} \\sum_{j=0}^{n-1} c_j \\omega_n^{-jk} & \\frac{9n}{2}\\log_{2}(n) + n \\end{pmatrix}}\n$$", "id": "3580686"}, {"introduction": "Many problems in science and engineering lead to matrices that are both Toeplitz (constant along diagonals) and tridiagonal. While this structure offers computational advantages, it does not automatically guarantee \"nice\" spectral properties. This exercise invites you to investigate the specific algebraic condition that makes a tridiagonal Toeplitz matrix normal—a crucial property that ensures it is unitarily diagonalizable [@problem_id:3580745]. By exploring this condition, you will uncover a connection to symmetric matrices and fast sinusoidal transforms, learning to quantify how \"far\" a matrix is from this ideal property.", "problem": "Let $T \\in \\mathbb{C}^{n \\times n}$ be a tridiagonal Toeplitz matrix with entries $t_{0} = a \\in \\mathbb{C}$ on the main diagonal, $t_{1} = b \\in \\mathbb{C}$ on the first upper diagonal, $t_{-1} = c \\in \\mathbb{C}$ on the first lower diagonal, and zeros elsewhere. The Toeplitz symbol associated with $T$ is the trigonometric polynomial $f(\\theta) = a + b \\exp(i \\theta) + c \\exp(-i \\theta)$.\n\nUsing only the core definitions from linear algebra, namely the definition of a normal matrix ($T^{*} T = T T^{*}$), the structure of Toeplitz matrices (constant diagonals) and the spectral norm ($\\| \\cdot \\|_{2}$), derive from first principles the necessary and sufficient condition on $b$ and $c$ for $T$ to be normal. Interpret this condition in terms of a symmetry of the Toeplitz symbol $f(\\theta)$ under the transformation $\\theta \\mapsto -\\theta$.\n\nNext, under the normality condition you obtained, construct a diagonal unitary scaling that equalizes the first upper and first lower diagonals of $T$ and argue why this implies that $T$ is unitarily diagonalizable by a structured transform (for example, by the Discrete Sine Transform (DST), which is implementable using the Fast Fourier Transform (FFT)), and discuss the implication this has for eigenvalue computations compared to generic algorithms with no exploitable structure.\n\nFinally, quantify the deviation from normality when the condition is violated by computing in closed form the spectral norm of the commutator $[T, T^{*}] = T T^{*} - T^{*} T$ as a function of $a$, $b$, $c$, and $n$. Your final answer must be a single closed-form analytical expression for $\\|[T, T^{*}]\\|_{2}$, with no rounding required.", "solution": "The problem asks for a multi-part analysis of a tridiagonal Toeplitz matrix $T \\in \\mathbb{C}^{n \\times n}$. We will address each part sequentially, starting with the derivation of the normality condition. For this analysis, we assume $n \\ge 2$, as the notions of first upper and lower diagonals are trivial for $n=1$.\n\nFirst, we define the matrix $T$ and its conjugate transpose $T^{*}$. The matrix $T$ has entries $T_{ii} = a$, $T_{i,i+1} = b$, and $T_{i+1,i} = c$ for $i \\in \\{1, \\dots, n-1\\}$, and zero otherwise.\n$$\nT = \\begin{pmatrix}\na & b & 0 & \\dots & 0 \\\\\nc & a & b & \\ddots & \\vdots \\\\\n0 & c & a & \\ddots & 0 \\\\\n\\vdots & \\ddots & \\ddots & \\ddots & b \\\\\n0 & \\dots & 0 & c & a\n\\end{pmatrix}\n$$\nThe conjugate transpose, $T^{*}$, is given by $(T^{*})_{ij} = \\overline{T_{ji}}$. It is also a Toeplitz matrix, with $\\bar{a}$ on the main diagonal, $\\bar{c}$ on the first upper diagonal, and $\\bar{b}$ on the first lower diagonal.\n$$\nT^{*} = \\begin{pmatrix}\n\\bar{a} & \\bar{c} & 0 & \\dots & 0 \\\\\n\\bar{b} & \\bar{a} & \\bar{c} & \\ddots & \\vdots \\\\\n0 & \\bar{b} & \\bar{a} & \\ddots & 0 \\\\\n\\vdots & \\ddots & \\ddots & \\ddots & \\bar{c} \\\\\n0 & \\dots & 0 & \\bar{b} & \\bar{a}\n\\end{pmatrix}\n$$\n\nA matrix is normal if it commutes with its conjugate transpose, i.e., $T T^{*} = T^{*} T$. We compute the entries of both products.\n\nThe entries of $T T^{*}$ are:\nFor the diagonal entries, $(T T^{*})_{ii} = \\sum_{k=1}^{n} T_{ik} \\overline{T_{ik}} = \\sum_{k=1}^{n} |T_{ik}|^2$.\n- $(T T^{*})_{11} = |T_{11}|^2 + |T_{12}|^2 = |a|^2 + |b|^2$.\n- For $1 < i < n$, $(T T^{*})_{ii} = |T_{i,i-1}|^2 + |T_{ii}|^2 + |T_{i,i+1}|^2 = |c|^2 + |a|^2 + |b|^2$.\n- $(T T^{*})_{nn} = |T_{n,n-1}|^2 + |T_{nn}|^2 = |c|^2 + |a|^2$.\n\nFor the off-diagonal entries, we only need to check the first super- and sub-diagonals, as the product of two tridiagonal matrices is a pentadiagonal matrix, but in this special case, it remains tridiagonal.\n- Superdiagonal ($1 \\le i \\le n-1$): $(T T^{*})_{i,i+1} = \\sum_k T_{ik} \\overline{T_{i+1,k}} = T_{ii}\\overline{T_{i+1,i}} + T_{i,i+1}\\overline{T_{i+1,i+1}} = a\\bar{c} + b\\bar{a}$.\n- Subdiagonal ($1 \\le i \\le n-1$): $(T T^{*})_{i+1,i} = \\sum_k T_{i+1,k} \\overline{T_{ik}} = T_{i+1,i}\\overline{T_{ii}} + T_{i+1,i+1}\\overline{T_{i,i+1}} = c\\bar{a} + a\\bar{b}$.\n\nThe entries of $T^{*} T$ are:\nFor the diagonal entries, $(T^{*} T)_{ii} = \\sum_{k=1}^{n} (T^{*})_{ik} T_{ki} = \\sum_{k=1}^{n} \\overline{T_{ki}} T_{ki} = \\sum_{k=1}^{n} |T_{ki}|^2$.\n- $(T^{*} T)_{11} = |T_{11}|^2 + |T_{21}|^2 = |a|^2 + |c|^2$.\n- For $1 < i < n$, $(T^{*} T)_{ii} = |T_{i-1,i}|^2 + |T_{ii}|^2 + |T_{i+1,i}|^2 = |b|^2 + |a|^2 + |c|^2$.\n- $(T^{*} T)_{nn} = |T_{n-1,n}|^2 + |T_{nn}|^2 = |b|^2 + |a|^2$.\n\nFor the off-diagonal entries:\n- Superdiagonal ($1 \\le i \\le n-1$): $(T^{*} T)_{i,i+1} = \\sum_k (T^{*})_{ik} T_{k,i+1} = (T^{*})_{ii}T_{i,i+1} + (T^{*})_{i,i+1}T_{i+1,i+1} = \\bar{a}b + \\bar{c}a$.\n- Subdiagonal ($1 \\le i \\le n-1$): $(T^{*} T)_{i+1,i} = \\sum_k (T^{*})_{i+1,k} T_{ki} = (T^{*})_{i+1,i}T_{ii} + (T^{*})_{i+1,i+1}T_{i+1,i} = \\bar{b}a + \\bar{a}c$.\n\nEquating $T T^{*} = T^{*} T$:\n- The superdiagonal entries are $a\\bar{c} + b\\bar{a}$ and $\\bar{a}b + \\bar{c}a$, which are equal.\n- The subdiagonal entries are $c\\bar{a} + a\\bar{b}$ and $\\bar{b}a + \\bar{a}c$, which are also equal.\n- The diagonal entries for $1 < i < n$ are both $|a|^2+|b|^2+|c|^2$.\n- The corner diagonal entries provide the only constraint.\n  - $(T T^{*})_{11} = (T^{*} T)_{11} \\implies |a|^2 + |b|^2 = |a|^2 + |c|^2 \\implies |b|^2 = |c|^2$.\n  - $(T T^{*})_{nn} = (T^{*} T)_{nn} \\implies |c|^2 + |a|^2 = |b|^2 + |a|^2 \\implies |c|^2 = |b|^2$.\nBoth give the same condition. Since the modulus is non-negative, this is equivalent to $|b| = |c|$. This is the necessary and sufficient condition for $T$ to be normal.\n\nThe problem asks to interpret this condition in terms of a symmetry of the Toeplitz symbol $f(\\theta) = a + b \\exp(i\\theta) + c \\exp(-i\\theta)$ under $\\theta \\mapsto -\\theta$. The condition $|b|=|c|$ on its own does not force a simple symmetry like $f(\\theta)=f(-\\theta)$ or $|f(\\theta)|=|f(-\\theta)|$ for the symbol. Instead, the connection is more subtle and is revealed by the next part of the problem. The condition $|b|=|c|$ is precisely what is needed to ensure that $T$ is unitarily similar to a symmetric matrix. Let $T'$ be this symmetric matrix. Its symbol $f'(\\theta)$ will be an even function, $f'(\\theta) = f'(-\\theta)$, which exhibits the required symmetry. The unitary similarity preserves normality, so this establishes an indirect link: $T$ is normal if and only if it is unitarily similar to a matrix $T'$ whose symbol is symmetric.\n\nLet's formalize this. Under the normality condition $|b|=|c|$, we can write $c/b = \\exp(i\\phi)$ for some phase $\\phi \\in \\mathbb{R}$, assuming $b \\neq 0$. If $b=0$, then $c=0$ and $T$ is diagonal, hence normal. If $b\\neq 0$, let $\\gamma = \\sqrt{c/b} = \\exp(i\\phi/2)$. Note that $|\\gamma|=1$.\nConstruct a diagonal unitary matrix $D$ with entries $d_k = \\gamma^{k-1}$ for $k=1, \\dots, n$. Since $|d_k| = |\\gamma|^{k-1} = 1$, $D$ is indeed unitary ($D^*D=I$).\nConsider the similarity transformation $T' = D^{*} T D$. The entries of $T'$ are:\n- $(T')_{ii} = \\overline{d_i} T_{ii} d_i = |d_i|^2 a = a$.\n- $(T')_{i, i+1} = \\overline{d_i} T_{i, i+1} d_{i+1} = \\overline{\\gamma^{i-1}} b \\gamma^i = \\gamma b = \\sqrt{c/b} \\cdot b = \\sqrt{bc}$.\n- $(T')_{i+1, i} = \\overline{d_{i+1}} T_{i+1, i} d_i = \\overline{\\gamma^i} c \\gamma^{i-1} = \\bar{\\gamma} c = \\sqrt{b/c} \\cdot c = \\sqrt{bc}$.\nThe resulting matrix $T'$ is a symmetric tridiagonal Toeplitz matrix:\n$$\nT' = \\begin{pmatrix}\na & \\sqrt{bc} & 0 & \\dots & 0 \\\\\n\\sqrt{bc} & a & \\sqrt{bc} & \\ddots & \\vdots \\\\\n0 & \\sqrt{bc} & a & \\ddots & 0 \\\\\n\\vdots & \\ddots & \\ddots & \\ddots & \\sqrt{bc} \\\\\n0 & \\dots & 0 & \\sqrt{bc} & a\n\\end{pmatrix}\n$$\nSince $T'$ is symmetric, its Toeplitz symbol is $f'(\\theta) = a + \\sqrt{bc}(\\exp(i\\theta) + \\exp(-i\\theta)) = a + 2\\sqrt{bc}\\cos(\\theta)$. This symbol is an even function of $\\theta$, i.e., $f'(\\theta) = f'(-\\theta)$, which is the symmetry we were seeking.\n\nA real symmetric tridiagonal Toeplitz matrix is known to be diagonalized by the Discrete Sine Transform (DST). The matrix of eigenvectors is $S$ with entries $S_{jk} = \\sqrt{\\frac{2}{n+1}} \\sin(\\frac{jk\\pi}{n+1})$. This matrix is orthogonal, hence unitary. The eigenvalues of $T'$ are given by the well-known formula $\\lambda_k = a + 2\\sqrt{bc} \\cos(\\frac{k\\pi}{n+1})$ for $k=1, \\dots, n$. The matrix $S$ diagonalizes $T'$ regardless of whether $a$ and $\\sqrt{bc}$ are real. So, $S^* T' S = \\Lambda$, where $\\Lambda = \\text{diag}(\\lambda_1, \\dots, \\lambda_n)$.\nSince $T = D T' D^{*}$, we have $T = D(S\\Lambda S^{*})D^{*} = (DS)\\Lambda(S^{*}D^{*}) = (DS)\\Lambda(DS)^{*}$.\nThe matrix $T$ is diagonalized by the unitary matrix $U = DS$. This is a structured transform, being the product of a diagonal unitary matrix and a DST matrix.\n\nThis structure has profound implications for eigenvalue computation. For a generic $n \\times n$ matrix, computing eigenvalues using, for example, the QR algorithm, costs $O(n^3)$ floating-point operations. For our normal tridiagonal Toeplitz matrix, we have a closed-form analytical expression for the eigenvalues. All $n$ eigenvalues can be computed in $O(n)$ time. The eigenvectors, which form the columns of $U=DS$, can also be computed efficiently. The DST matrix $S$ can be applied to a vector in $O(n \\log n)$ time using Fast Fourier Transform (FFT) algorithms. Thus, any eigenvector can be computed in $O(n \\log n)$ time, and the full matrix of eigenvectors $U$ can be formed in $O(n^2 \\log n)$ time, which is a significant improvement over generic methods.\n\nFinally, we quantify the deviation from normality by computing the spectral norm, $\\| \\cdot \\|_2$, of the commutator $[T, T^{*}] = T T^{*} - T^{*} T$. From our earlier calculations, we found that the off-diagonal entries and the central diagonal entries of $T T^{*}$ and $T^{*} T$ are identical. The only non-zero entries in the commutator matrix $C = [T, T^{*}]$ are at the corners:\n- $C_{11} = (T T^{*})_{11} - (T^{*} T)_{11} = (|a|^2+|b|^2) - (|a|^2+|c|^2) = |b|^2-|c|^2$.\n- $C_{nn} = (T T^{*})_{nn} - (T^{*} T)_{nn} = (|c|^2+|a|^2) - (|b|^2+|a|^2) = |c|^2-|b|^2$.\nAll other entries $C_{ij}$ are $0$. The commutator matrix is a diagonal matrix of the form:\n$$\n[T, T^{*}] = \\text{diag}(|b|^2-|c|^2, 0, \\dots, 0, |c|^2-|b|^2)\n$$\nThe spectral norm of a matrix is its largest singular value. For a normal matrix, which a diagonal matrix is, the spectral norm is the maximum of the absolute values of its eigenvalues. The eigenvalues of a diagonal matrix are its diagonal entries.\nThe eigenvalues of $[T, T^{*}]$ are $\\{|b|^2-|c|^2, 0, \\dots, 0, |c|^2-|b|^2\\}$.\nThe spectral norm is therefore:\n$$\n\\|[T, T^{*}]\\|_{2} = \\max\\{||b|^2-|c|^2|, |0|, ||c|^2-|b|^2|\\}\n$$\nSince $||c|^2-|b|^2| = |-(|b|^2-|c|^2)| = ||b|^2-|c|^2|$, the norm is simply the absolute value of the corner entry.\n$$\n\\|[T, T^{*}]\\|_{2} = \\left| |b|^2-|c|^2 \\right|\n$$\nThis quantity is zero if and only if $|b|=|c|$, which confirms our condition for normality. It provides a continuous measure of how much $T$ deviates from being a normal matrix. The deviation depends only on the difference in magnitude squared of the off-diagonal elements and is independent of $a$ and $n$ (for $n \\ge 2$).", "answer": "$$\n\\boxed{\\left| |b|^2 - |c|^2 \\right|}\n$$", "id": "3580745"}, {"introduction": "The Vandermonde matrix is a classic example in linear algebra, fundamental to polynomial interpolation. However, in numerical practice, it is famously ill-conditioned, making high-degree interpolation a perilous task. This practice tackles this critical issue of numerical stability head-on, demonstrating the power of choosing a more suitable basis and a strategic set of points [@problem_id:3580739]. You will show that by using the Chebyshev polynomial basis evaluated at corresponding Chebyshev nodes, the resulting 'Chebyshev-Vandermonde' matrix has a remarkably small and constant condition number, illustrating a key principle in numerical analysis: a well-posed problem is often the result of careful formulation.", "problem": "Let $n \\geq 2$ and consider the real Vandermonde-type evaluation matrix built on the Chebyshev polynomials of the first kind. Define the Chebyshev nodes $t_i \\in [-1,1]$ by $t_i = \\cos\\!\\left(\\frac{(2i-1)\\pi}{2n}\\right)$ for $i = 1,\\dots,n$, and let $\\{T_k\\}_{k=0}^{n-1}$ denote the Chebyshev polynomials of the first kind, characterized by the identity $T_k(\\cos \\theta) = \\cos(k\\theta)$ for all real $\\theta$. Form the $n \\times n$ matrix $\\Phi$ with entries $\\Phi_{ij} = T_{j-1}(t_i)$. Starting from the definition of the matrix $2$-norm condition number $\\kappa_{2}(A)$ as the ratio of largest to smallest singular values and using only standard facts about Chebyshev polynomials and trigonometric sums, derive a closed-form expression for $\\kappa_{2}(\\Phi)$. Then, using the fundamental change-of-basis relationship between monomials $\\{z^{k}\\}_{k=0}^{n-1}$ and Chebyshev polynomials $\\{T_k(z)\\}_{k=0}^{n-1}$, explain why choosing Chebyshev nodes reduces the growth of the condition number of the monomial Vandermonde matrix relative to equispaced nodes in $[-1,1]$. Your final answer must be a single analytic expression for $\\kappa_{2}(\\Phi)$.", "solution": "The problem asks for two things: first, to derive a closed-form expression for the $2$-norm condition number $\\kappa_2(\\Phi)$ of a specific Chebyshev-Vandermonde matrix $\\Phi$; second, to explain why choosing Chebyshev nodes is advantageous for the conditioning of the standard monomial Vandermonde matrix.\n\nFirst, we will find the condition number $\\kappa_2(\\Phi)$. The matrix $\\Phi$ is an $n \\times n$ matrix with entries $\\Phi_{ij} = T_{j-1}(t_i)$, where $i,j \\in \\{1, \\dots, n\\}$. The points $t_i$ are the Chebyshev nodes $t_i = \\cos\\left(\\frac{(2i-1)\\pi}{2n}\\right)$, and $T_k(x)$ is the Chebyshev polynomial of the first kind of degree $k$. The condition number $\\kappa_2(\\Phi)$ is defined as the ratio of the largest to the smallest singular value of $\\Phi$, $\\kappa_2(\\Phi) = \\frac{\\sigma_{\\max}(\\Phi)}{\\sigma_{\\min}(\\Phi)}$. The singular values of $\\Phi$ are the square roots of the eigenvalues of the matrix $\\Phi^T \\Phi$.\n\nLet us compute the matrix product $\\Phi^T \\Phi$. The entry at row $j$ and column $k$ of $\\Phi^T \\Phi$ is given by:\n$$ (\\Phi^T \\Phi)_{jk} = \\sum_{i=1}^{n} (\\Phi^T)_{ji} \\Phi_{ik} = \\sum_{i=1}^{n} \\Phi_{ij} \\Phi_{ik} = \\sum_{i=1}^{n} T_{j-1}(t_i) T_{k-1}(t_i) $$\nThis sum is a discrete inner product of the Chebyshev polynomials $T_{j-1}$ and $T_{k-1}$ over the set of Chebyshev nodes $\\{t_i\\}_{i=1}^n$. This set of nodes exhibits a discrete orthogonality property for these polynomials. Let us prove this property.\n\nLet $\\theta_i = \\frac{(2i-1)\\pi}{2n}$ for $i=1, \\dots, n$. Then $t_i = \\cos(\\theta_i)$. Using the defining property $T_k(\\cos\\theta) = \\cos(k\\theta)$, the sum becomes:\n$$ \\sum_{i=1}^{n} T_{j-1}(\\cos\\theta_i) T_{k-1}(\\cos\\theta_i) = \\sum_{i=1}^{n} \\cos((j-1)\\theta_i) \\cos((k-1)\\theta_i) $$\nUsing the product-to-sum trigonometric identity $\\cos(A)\\cos(B) = \\frac{1}{2}(\\cos(A+B) + \\cos(A-B))$, the sum is:\n$$ \\frac{1}{2} \\sum_{i=1}^{n} \\left[ \\cos((j+k-2)\\theta_i) + \\cos((j-k)\\theta_i) \\right] $$\nWe need to evaluate the generic sum $S_m = \\sum_{i=1}^{n} \\cos(m\\theta_i)$ for integer $m$. This sum is the real part of the complex geometric sum $\\sum_{i=1}^{n} \\exp(im\\theta_i)$.\n$$ \\sum_{i=1}^{n} \\exp(im\\theta_i) = \\sum_{i=1}^{n} \\exp\\left(im\\frac{(2i-1)\\pi}{2n}\\right) = \\exp\\left(-im\\frac{\\pi}{2n}\\right) \\sum_{i=1}^{n} \\left(\\exp\\left(im\\frac{\\pi}{n}\\right)\\right)^i $$\nLet $r = \\exp(im\\pi/n)$. The sum is a geometric series $\\sum_{i=1}^n r^i = r\\frac{1-r^n}{1-r}$, provided $r \\neq 1$.\nThe term $r^n = \\exp(im\\pi) = (-1)^m$.\nIf $m$ is a non-zero multiple of $2n$, then $r=1$. For the indices relevant to our problem, $j,k \\in \\{1,\\dots,n\\}$, so $j-k$ ranges from $-(n-1)$ to $n-1$, and $j+k-2$ ranges from $0$ to $2n-2$. Thus, $m=j-k$ or $m=j+k-2$ is never a non-zero multiple of $2n$.\nIf $m=0$, then $S_0 = \\sum_{i=1}^n \\cos(0) = n$.\nIf $m \\neq 0$ and not a multiple of $2n$, the sum of the geometric series is $\\exp(-im\\pi/2n) \\cdot \\exp(im\\pi/n) \\frac{1 - (-1)^m}{1-\\exp(im\\pi/n)} = \\exp(im\\pi/2n) \\frac{1 - (-1)^m}{1-\\exp(im\\pi/n)}$.\nIf $m$ is an even non-zero integer, $1-(-1)^m = 0$, so the sum is $0$.\nIf $m$ is an odd integer, $1-(-1)^m = 2$. The sum becomes $\\frac{2\\exp(im\\pi/2n)}{1-\\exp(im\\pi/n)} = \\frac{2\\exp(im\\pi/2n)}{\\exp(im\\pi/2n)(\\exp(-im\\pi/2n)-\\exp(im\\pi/2n))} = \\frac{2}{-2i\\sin(m\\pi/2n)} = \\frac{i}{\\sin(m\\pi/2n)}$. The real part of this is $0$.\nThus, $S_m = \\sum_{i=1}^n \\cos(m\\theta_i) = 0$ for any integer $m$ such that $0 < |m| < 2n$.\n\nNow we analyze the sum $(\\Phi^T \\Phi)_{jk} = \\frac{1}{2}(S_{j+k-2} + S_{j-k})$.\nCase 1: $j \\neq k$. Then $j-k \\neq 0$. Also, $1 \\le |j-k| \\le n-1$. So $S_{j-k}=0$. Also $1 \\le j+k-2 \\le 2n-3$ (since $j, k \\ge 1, j \\neq k$), so $S_{j+k-2}=0$. Thus, $(\\Phi^T \\Phi)_{jk} = 0$.\nCase 2: $j = k$. Then $j-k=0$ and $S_{j-k} = S_0 = n$. The sum becomes $\\frac{1}{2}(S_{2(j-1)} + n)$.\nSubcase 2a: $j=k=1$. The polynomial degree is $0$. $2(j-1)=0$. $S_{2(j-1)}=S_0=n$. The sum is $\\frac{1}{2}(n+n) = n$.\nSubcase 2b: $j=k \\in \\{2, \\dots, n\\}$. The polynomial degrees are $1, \\dots, n-1$. Let $m=2(j-1)$. Then $2 \\le m \\le 2n-2$. In this range, $m$ is a non-zero even integer, so $S_m=0$. The sum is $\\frac{1}{2}(0+n) = n/2$.\n\nCombining these results, $\\Phi^T \\Phi$ is a diagonal matrix:\n$$ \\Phi^T \\Phi = \\text{diag}\\left(n, \\frac{n}{2}, \\frac{n}{2}, \\dots, \\frac{n}{2}\\right) $$\nThe eigenvalues of $\\Phi^T \\Phi$ are its diagonal entries. They are $\\lambda_{\\max} = n$ (with multiplicity $1$) and $\\lambda = n/2$ (with multiplicity $n-1$).\nThe singular values of $\\Phi$ are the square roots of these eigenvalues. Since $n \\ge 2$, we have $n > n/2$, so the largest singular value is $\\sigma_{\\max}(\\Phi) = \\sqrt{n}$ and the smallest is $\\sigma_{\\min}(\\Phi) = \\sqrt{n/2}$.\nThe $2$-norm condition number is therefore:\n$$ \\kappa_2(\\Phi) = \\frac{\\sigma_{\\max}(\\Phi)}{\\sigma_{\\min}(\\Phi)} = \\frac{\\sqrt{n}}{\\sqrt{n/2}} = \\sqrt{\\frac{n}{n/2}} = \\sqrt{2} $$\n\nNow, for the second part of the problem, we explain why choosing Chebyshev nodes reduces the growth of the condition number of the monomial Vandermonde matrix.\nLet $p(z) = \\sum_{k=0}^{n-1} c_k z^k$ be a polynomial of degree at most $n-1$ in the monomial basis $\\{z^k\\}$. The polynomial interpolation problem, finding $p(z)$ such that $p(x_i) = y_i$ for $i=1,\\dots,n$, leads to the linear system $V\\mathbf{c} = \\mathbf{y}$, where $V$ is the monomial Vandermonde matrix with entries $V_{ij} = x_i^{j-1}$.\nThe same polynomial can be expressed in the basis of Chebyshev polynomials $\\{T_k(z)\\}_{k=0}^{n-1}$ as $p(z) = \\sum_{k=0}^{n-1} a_k T_k(z)$. The interpolation problem in this basis is $\\Phi\\mathbf{a} = \\mathbf{y}$, where $\\Phi$ is the matrix from the first part of the problem, with entries $\\Phi_{ij} = T_{j-1}(x_i)$.\nThere is a linear transformation between the coefficient vectors $\\mathbf{c}$ and $\\mathbf{a}$. Let $C$ be the change-of-basis matrix such that $\\mathbf{c} = C\\mathbf{a}$. The entries of $C$ are determined by the coefficients of the monomials in the expansion of $T_k(z)$. This matrix $C$ is independent of the choice of interpolation nodes $\\{x_i\\}$.\nFrom $\\mathbf{a} = C^{-1}\\mathbf{c}$ and $\\Phi\\mathbf{a} = \\mathbf{y}$, we get $\\Phi C^{-1} \\mathbf{c} = \\mathbf{y}$. Comparing this with $V\\mathbf{c} = \\mathbf{y}$, we find the relationship $V = \\Phi C^{-1}$.\nThe condition number of the Vandermonde matrix is thus related to the condition numbers of $\\Phi$ and $C^{-1}$: $\\kappa_2(V) \\le \\kappa_2(\\Phi) \\kappa_2(C^{-1})$. This shows that the conditioning of $V$ is influenced by the conditioning of $\\Phi$.\n\nThe crucial difference between node sets lies in the conditioning of $\\Phi$.\n1.  **Chebyshev Nodes**: As we have rigorously proven, when the nodes $\\{x_i\\}$ are the Chebyshev nodes $t_i = \\cos(\\frac{(2i-1)\\pi}{2n})$, the matrix $\\Phi$ is exceptionally well-conditioned, with $\\kappa_2(\\Phi) = \\sqrt{2}$, a small constant independent of $n$. This is a direct result of the columns of $\\Phi$ being mutually orthogonal with respect to the discrete inner product defined by summation over these nodes.\n\n2.  **Equispaced Nodes**: If we choose equispaced nodes in $[-1,1]$, e.g., $x_i = -1 + 2(i-1)/(n-1)$, the discrete orthogonality property is lost. The matrix $\\Phi_{\\text{eq}}$ formed with these nodes is not nearly orthogonal, and its condition number $\\kappa_2(\\Phi_{\\text{eq}})$ is known to grow rapidly with $n$.\n\nThe change-of-basis matrix $C$ is moderately conditioned; its condition number grows polynomially with $n$. Therefore, the dramatic difference in the growth of $\\kappa_2(V)$ for different node sets is primarily attributable to the difference in $\\kappa_2(\\Phi)$.\nFor equispaced nodes, the ill-conditioning of $\\Phi_{\\text{eq}}$ contributes to the exponential growth of $\\kappa_2(V_{\\text{eq}})$, which is known to be on the order of $O(2^n)$. For Chebyshev nodes, the excellent, constant conditioning of $\\Phi_{\\text{cheb}}$ means that the growth of $\\kappa_2(V_{\\text{cheb}})$ is much more subdued. By choosing Chebyshev nodes, we ensure that the evaluation matrix $\\Phi$ for the well-behaved Chebyshev basis is itself well-conditioned, which in turn moderates the ill-conditioning of the monomial Vandermonde matrix $V$ for that same set of nodes.", "answer": "$$\\boxed{\\sqrt{2}}$$", "id": "3580739"}]}