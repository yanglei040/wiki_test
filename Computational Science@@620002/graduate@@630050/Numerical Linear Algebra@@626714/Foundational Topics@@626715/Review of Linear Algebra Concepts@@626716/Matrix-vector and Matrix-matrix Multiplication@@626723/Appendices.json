{"hands_on_practices": [{"introduction": "We begin our hands-on exploration by revisiting a foundational property of matrix multiplication: its non-commutativity. This exercise goes beyond simply verifying that $AB \\neq BA$ for specific matrices. You will quantify the degree of non-commutativity by computing the induced 2-norm of the commutator, $[A,B] = AB - BA$, thereby connecting a fundamental algebraic property to a concrete analytical measure. [@problem_id:3559507]", "problem": "Let $n=4$. Consider the matrices $A \\in \\mathbb{R}^{4 \\times 4}$ and $B \\in \\mathbb{R}^{4 \\times 4}$ given by\n$$\nA=\\begin{pmatrix}\n0 & 1 & 0 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 1 \\\\\n0 & 0 & 0 & 0\n\\end{pmatrix}, \\qquad\nB=\\operatorname{diag}(1,3,-2,7)=\\begin{pmatrix}\n1 & 0 & 0 & 0 \\\\\n0 & 3 & 0 & 0 \\\\\n0 & 0 & -2 & 0 \\\\\n0 & 0 & 0 & 7\n\\end{pmatrix}.\n$$\nUsing only the foundational definitions of matrix-matrix multiplication and the induced matrix norm, carry out the following steps:\n- Compute the commutator $[A,B]=AB-BA$ explicitly and verify that $AB \\neq BA$.\n- Using the definition of the operator norm induced by the Euclidean norm, namely\n$$\n\\|M\\|_{2}=\\sup_{x \\neq 0} \\frac{\\|Mx\\|_{2}}{\\|x\\|_{2}},\n$$\nevaluate $\\|[A,B]\\|_{2}$ exactly, justifying each step from first principles (matrix multiplication rules, properties of symmetric matrices, and the Rayleigh quotient).\n\nProvide your final answer as a single real number. No rounding is required.", "solution": "The problem is well-posed and self-contained. All necessary matrices and definitions are provided, and there are no scientific or logical inconsistencies. We can proceed with the solution.\n\nThe problem asks for two main tasks: first, to compute the commutator $[A,B]=AB-BA$ for the given matrices $A$ and $B$ and to verify they do not commute; second, to evaluate the induced $2$-norm of this commutator, $\\|[A,B]\\|_{2}$, using foundational principles.\n\nLet the matrices $A$ and $B$ be defined as:\n$$\nA=\\begin{pmatrix}\n0 & 1 & 0 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 1 \\\\\n0 & 0 & 0 & 0\n\\end{pmatrix}, \\qquad\nB=\\begin{pmatrix}\n1 & 0 & 0 & 0 \\\\\n0 & 3 & 0 & 0 \\\\\n0 & 0 & -2 & 0 \\\\\n0 & 0 & 0 & 7\n\\end{pmatrix}.\n$$\n\nFirst, we compute the product $AB$. For a general matrix $M$ and a diagonal matrix $D$, the product $MD$ is the matrix $M$ with its $j$-th column scaled by the $j$-th diagonal entry of $D$.\nThus, the columns of $AB$ are the columns of $A$ scaled by the diagonal entries of $B$, which are $1$, $3$, $-2$, and $7$.\n$$\nAB = \\begin{pmatrix}\n0 \\cdot 1 & 1 \\cdot 3 & 0 \\cdot (-2) & 0 \\cdot 7 \\\\\n0 \\cdot 1 & 0 \\cdot 3 & 1 \\cdot (-2) & 0 \\cdot 7 \\\\\n0 \\cdot 1 & 0 \\cdot 3 & 0 \\cdot (-2) & 1 \\cdot 7 \\\\\n0 \\cdot 1 & 0 \\cdot 3 & 0 \\cdot (-2) & 0 \\cdot 7\n\\end{pmatrix} = \\begin{pmatrix}\n0 & 3 & 0 & 0 \\\\\n0 & 0 & -2 & 0 \\\\\n0 & 0 & 0 & 7 \\\\\n0 & 0 & 0 & 0\n\\end{pmatrix}.\n$$\n\nNext, we compute the product $BA$. For a general matrix $M$ and a diagonal matrix $D$, the product $DM$ is the matrix $M$ with its $i$-th row scaled by the $i$-th diagonal entry of $D$.\nThus, the rows of $BA$ are the rows of $A$ scaled by the diagonal entries of $B$.\n$$\nBA = \\begin{pmatrix}\n1 \\cdot 0 & 1 \\cdot 1 & 1 \\cdot 0 & 1 \\cdot 0 \\\\\n3 \\cdot 0 & 3 \\cdot 0 & 3 \\cdot 1 & 3 \\cdot 0 \\\\\n-2 \\cdot 0 & -2 \\cdot 0 & -2 \\cdot 0 & -2 \\cdot 1 \\\\\n7 \\cdot 0 & 7 \\cdot 0 & 7 \\cdot 0 & 7 \\cdot 0\n\\end{pmatrix} = \\begin{pmatrix}\n0 & 1 & 0 & 0 \\\\\n0 & 0 & 3 & 0 \\\\\n0 & 0 & 0 & -2 \\\\\n0 & 0 & 0 & 0\n\\end{pmatrix}.\n$$\n\nBy comparing the resulting matrices, we see that $(AB)_{12} = 3$ whereas $(BA)_{12} = 1$. Since at least one entry differs, the matrices are not equal, thus verifying that $AB \\neq BA$.\n\nNow, we compute the commutator, which we denote as $C$:\n$$\nC = [A,B] = AB - BA = \\begin{pmatrix}\n0 & 3 & 0 & 0 \\\\\n0 & 0 & -2 & 0 \\\\\n0 & 0 & 0 & 7 \\\\\n0 & 0 & 0 & 0\n\\end{pmatrix} - \\begin{pmatrix}\n0 & 1 & 0 & 0 \\\\\n0 & 0 & 3 & 0 \\\\\n0 & 0 & 0 & -2 \\\\\n0 & 0 & 0 & 0\n\\end{pmatrix} = \\begin{pmatrix}\n0 & 2 & 0 & 0 \\\\\n0 & 0 & -5 & 0 \\\\\n0 & 0 & 0 & 9 \\\\\n0 & 0 & 0 & 0\n\\end{pmatrix}.\n$$\n\nThe second part of the problem requires us to evaluate $\\|C\\|_{2}$. The induced $2$-norm of a matrix $M$ is defined as:\n$$\n\\|M\\|_{2} = \\sup_{x \\neq 0} \\frac{\\|Mx\\|_{2}}{\\|x\\|_{2}} = \\sup_{\\|x\\|_{2}=1} \\|Mx\\|_{2}.\n$$\nIt is more convenient to work with the square of the norm:\n$$\n\\|M\\|_{2}^2 = \\sup_{\\|x\\|_{2}=1} \\|Mx\\|_{2}^2.\n$$\nThe squared Euclidean norm of a vector $v$ is $v^T v$. Thus, $\\|Mx\\|_{2}^2 = (Mx)^T(Mx) = x^T M^T M x$.\nSubstituting this into the expression for the squared norm gives:\n$$\n\\|M\\|_{2}^2 = \\sup_{\\|x\\|_{2}=1} x^T M^T M x.\n$$\nThe expression $\\frac{x^T M^T M x}{x^T x}$ is the Rayleigh quotient for the matrix $M^T M$. The matrix $M^T M$ is symmetric and positive semi-definite. According to the Rayleigh-Ritz theorem, the supremum of the Rayleigh quotient for a symmetric matrix is its largest eigenvalue, $\\lambda_{\\max}(M^T M)$. Therefore, $\\|M\\|_{2}^2 = \\lambda_{\\max}(M^T M)$, which implies $\\|M\\|_{2} = \\sqrt{\\lambda_{\\max}(M^T M)}$.\n\nWe apply this principle to our commutator matrix $C$. First, we compute $C^T$ and then $C^T C$:\n$$\nC = \\begin{pmatrix}\n0 & 2 & 0 & 0 \\\\\n0 & 0 & -5 & 0 \\\\\n0 & 0 & 0 & 9 \\\\\n0 & 0 & 0 & 0\n\\end{pmatrix}, \\qquad\nC^T = \\begin{pmatrix\n0 & 0 & 0 & 0 \\\\\n2 & 0 & 0 & 0 \\\\\n0 & -5 & 0 & 0 \\\\\n0 & 0 & 9 & 0\n\\end{pmatrix}.\n$$\nNow, we compute the product $C^T C$:\n$$\nC^T C = \\begin{pmatrix}\n0 & 0 & 0 & 0 \\\\\n2 & 0 & 0 & 0 \\\\\n0 & -5 & 0 & 0 \\\\\n0 & 0 & 9 & 0\n\\end{pmatrix}\n\\begin{pmatrix}\n0 & 2 & 0 & 0 \\\\\n0 & 0 & -5 & 0 \\\\\n0 & 0 & 0 & 9 \\\\\n0 & 0 & 0 & 0\n\\end{pmatrix} = \\begin{pmatrix}\n0 & 0 & 0 & 0 \\\\\n0 & 4 & 0 & 0 \\\\\n0 & 0 & 25 & 0 \\\\\n0 & 0 & 0 & 81\n\\end{pmatrix}.\n$$\nThe matrix $C^T C$ is a diagonal matrix. The eigenvalues of a diagonal matrix are its diagonal entries. The eigenvalues of $C^T C$ are therefore $\\lambda_1 = 0$, $\\lambda_2 = 4$, $\\lambda_3 = 25$, and $\\lambda_4 = 81$.\n\nThe largest eigenvalue is $\\lambda_{\\max}(C^T C) = 81$.\nUsing the relationship derived from first principles, we have:\n$$\n\\|C\\|_{2}^2 = \\lambda_{\\max}(C^T C) = 81.\n$$\nTaking the square root gives the value of the induced $2$-norm:\n$$\n\\|C\\|_{2} = \\sqrt{81} = 9.\n$$\nThus, the exact value of $\\|[A,B]\\|_{2}$ is $9$.", "answer": "$$\\boxed{9}$$", "id": "3559507"}, {"introduction": "Moving from the realm of exact arithmetic to practical computation, we now investigate the impact of floating-point error. The seemingly simple act of summing a series of products can be performed in different ways, with significant consequences for numerical accuracy. This practice guides you through a forward error analysis of different accumulation strategies, culminating in the optimization of the algorithm to minimize error, a crucial skill in designing stable numerical methods. [@problem_id:3559509]", "problem": "Let $A \\in \\mathbb{R}^{m \\times k}$ and $B \\in \\mathbb{R}^{k \\times n}$, and consider computing $C = AB$ by forming inner products $c_{ij} = \\sum_{t=1}^{k} a_{it} b_{tj}$ for $i \\in \\{1,\\dots,m\\}$ and $j \\in \\{1,\\dots,n\\}$. Assume standard floating-point arithmetic with rounding to nearest and unit roundoff (machine precision) $u$, modeled by $\\mathrm{fl}(x \\circ y) = (x \\circ y)(1 + \\delta)$ with $|\\delta| \\le u$ for $\\circ \\in \\{+, \\times\\}$ and independent rounding errors.\n\nTwo algorithms are considered for each $c_{ij}$:\n\n- Scalar accumulation: compute $c_{ij}$ as a single sequential sum of $k$ scalar products $a_{it} b_{tj}$.\n\n- Block inner-product accumulation of chunk size $s$: partition the index set $\\{1,\\dots,k\\}$ into $r = \\lceil k/s \\rceil$ contiguous chunks of size $s$ (the last chunk may be shorter). For each chunk $q \\in \\{1,\\dots,r\\}$, form a partial sum $p^{(q)}_{ij} = \\sum_{t=(q-1)s+1}^{\\min(qs,k)} a_{it} b_{tj}$ using scalar accumulation as in the first algorithm, and then sum the $r$ partial sums sequentially to produce $c_{ij}$.\n\nStarting only from the floating-point model and the definition of matrix-matrix multiplication, derive forward error bounds for the computed $c_{ij}$ in terms of $u$, $k$, $s$, and $S_{ij} := \\sum_{t=1}^{k} |a_{it}|\\,|b_{tj}|$ for both algorithms. Your derivation must be explicit and must not assume pre-stated summation error bounds; you should build them from the floating-point model.\n\nUnder the simplifying regime $ku \\ll 1$ and $su \\ll 1$, extract the leading-order term in $u$ of the block accumulation bound and propose a chunk size $s$ (treated as a real variable for the purpose of minimization) that minimizes this leading-order bound as a function of $k$. Provide your final answer as a single closed-form symbolic expression for the optimal $s$ in terms of $k$.", "solution": "The problem statement is critically validated before proceeding to a solution.\n\n### Step 1: Extract Givens\n- Matrices: $A \\in \\mathbb{R}^{m \\times k}$ and $B \\in \\mathbb{R}^{k \\times n}$.\n- Product: $C = AB$.\n- Entry computation: $c_{ij} = \\sum_{t=1}^{k} a_{it} b_{tj}$ for $i \\in \\{1,\\dots,m\\}$ and $j \\in \\{1,\\dots,n\\}$.\n- Floating-point model: $\\mathrm{fl}(x \\circ y) = (x \\circ y)(1 + \\delta)$ with $|\\delta| \\le u$ for $\\circ \\in \\{+, \\times\\}$.\n- Unit roundoff: $u$.\n- Rounding errors are independent.\n- Algorithm 1 (Scalar accumulation): $c_{ij}$ is computed as a single sequential sum of $k$ products $a_{it} b_{tj}$.\n- Algorithm 2 (Block inner-product accumulation):\n  - Chunk size: $s$.\n  - Number of chunks: $r = \\lceil k/s \\rceil$.\n  - Partial sums: $p^{(q)}_{ij} = \\sum_{t=(q-1)s+1}^{\\min(qs,k)} a_{it} b_{tj}$ using scalar accumulation.\n  - Final sum: $c_{ij}$ is the sequential sum of the $r$ partial sums $p^{(q)}_{ij}$.\n- Error bound definition: in terms of $u$, $k$, $s$, and $S_{ij} := \\sum_{t=1}^{k} |a_{it}|\\,|b_{tj}|$.\n- Derivation requirement: Build from the floating-point model explicitly, without assuming pre-stated summation error bounds.\n- Simplified regime: $ku \\ll 1$ and $su \\ll 1$.\n- Final task: Extract the leading-order term of the block accumulation bound, find the chunk size $s$ (as a real variable) that minimizes this term, and provide $s$ in terms of $k$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded:** The problem is a standard exercise in the forward error analysis of floating-point algorithms, a core topic in numerical linear algebra. The floating-point model provided is a fundamental and widely used model for such analysis.\n- **Well-Posed:** The problem is clearly stated, providing all necessary definitions and constraints. It asks for specific derivations and an optimization, which lead to a unique, meaningful symbolic answer.\n- **Objective:** The language is formal, precise, and free of any subjective or opinion-based content.\n- **Completeness and Consistency:** The problem is self-contained and internally consistent. All terms are defined.\n- **Other criteria:** The problem does not violate any other validation criteria. It is not trivial, unrealistic, or ill-posed.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A full solution will be provided.\n\n### Derivation of Error Bounds\n\nWe begin by establishing a standard result for the error in a product of terms of the form $(1+\\delta)$. Let $\\theta_n$ be defined by $\\prod_{i=1}^n (1+\\delta_i) = 1 + \\theta_n$, where $|\\delta_i| \\le u$. For $nu < 1$, a standard bound is $|\\theta_n| \\le \\frac{nu}{1-nu}$. We denote this quantity by $\\gamma_n$. Thus, $|\\theta_n| \\le \\gamma_n$.\n\n#### Algorithm 1: Scalar Accumulation\nLet us analyze the computation of a single entry $c_{ij}$, which we temporarily denote as $c = \\sum_{t=1}^k a_t b_t$ to simplify notation. The algorithm computes $\\hat{c}$ by first computing the products $\\hat{p}_t = \\mathrm{fl}(a_t b_t)$ and then summing them sequentially.\n\nThe computation of each product introduces an error:\n$$ \\hat{p}_t = a_t b_t (1 + \\delta_t), \\quad |\\delta_t| \\le u $$\nThe sequential summation is performed as:\n$$ s_1 = \\hat{p}_1 $$\n$$ s_q = \\mathrm{fl}(s_{q-1} + \\hat{p}_q) = (s_{q-1} + \\hat{p}_q)(1 + \\eta_q), \\quad |\\eta_q| \\le u \\quad \\text{for } q = 2, \\dots, k $$\nThe final computed value is $\\hat{c} = s_k$. Expanding the recurrence for $s_k$ gives:\n$$ \\hat{c} = \\sum_{t=1}^k \\hat{p}_t \\prod_{q=t+1}^k (1+\\eta_q) $$\nwhere the empty product for $t=k$ is $1$. Substituting the expression for $\\hat{p}_t$:\n$$ \\hat{c} = \\sum_{t=1}^k a_t b_t (1+\\delta_t) \\prod_{q=t+1}^k (1+\\eta_q) $$\nLet $1+\\epsilon_t = (1+\\delta_t) \\prod_{q=t+1}^k (1+\\eta_q)$. The term $a_t b_t$ is affected by one multiplication and $k-t$ additions, for a total of $k-t+1$ floating-point operations. The accumulated relative error $\\epsilon_t$ is therefore bounded by $|\\epsilon_t| \\le \\gamma_{k-t+1}$.\n\nThe absolute error in computing $c$ is $E = \\hat{c} - c$:\n$$ E = \\hat{c} - c = \\sum_{t=1}^k a_t b_t (1+\\epsilon_t) - \\sum_{t=1}^k a_t b_t = \\sum_{t=1}^k a_t b_t \\epsilon_t $$\nThe magnitude of the error is bounded using the triangle inequality:\n$$ |E| \\le \\sum_{t=1}^k |a_t b_t| |\\epsilon_t| \\le \\sum_{t=1}^k |a_t b_t| \\gamma_{k-t+1} $$\nSince $\\gamma_n$ is a monotonically increasing function of $n$, we can use the largest possible value for the bound, which occurs for $t=1$ (i.e., $\\gamma_k$):\n$$ |E| \\le \\gamma_k \\sum_{t=1}^k |a_t b_t| $$\nReintroducing the indices $i$ and $j$ and using the definition $S_{ij} = \\sum_{t=1}^{k} |a_{it}|\\,|b_{tj}|$, we get the forward error bound for scalar accumulation:\n$$ |\\hat{c}_{ij} - c_{ij}| \\le \\gamma_k S_{ij} = \\frac{ku}{1-ku} S_{ij} $$\n\n#### Algorithm 2: Block Inner-Product Accumulation\nThis algorithm has two stages. First, $r = \\lceil k/s \\rceil$ partial sums are computed. Second, these partial sums are summed. Let $I_q$ be the index set for the $q$-th chunk, so $I_q = \\{(q-1)s+1, \\dots, \\min(qs, k)\\}$, and let $s_q = |I_q|$. Note that $s_q=s$ for $q < r$ and $s_r \\le s$. The true partial sum for chunk $q$ is $P^{(q)}_{ij} = \\sum_{t \\in I_q} a_{it} b_{tj}$.\n\n**Stage 1 Error:** Each partial sum $\\hat{p}^{(q)}_{ij}$ is computed using scalar accumulation over $s_q$ terms. Using the result from Algorithm 1, the error for each computed partial sum is:\n$$ |\\hat{p}^{(q)}_{ij} - P^{(q)}_{ij}| \\le \\gamma_{s_q} \\sum_{t \\in I_q} |a_{it}b_{tj}| $$\nSince $s_q \\le s$ for all $q$, and $\\gamma_n$ is increasing in $n$, we have $\\gamma_{s_q} \\le \\gamma_s$. Therefore:\n$$ |\\hat{p}^{(q)}_{ij} - P^{(q)}_{ij}| \\le \\gamma_s \\sum_{t \\in I_q} |a_{it}b_{tj}| $$\nSumming these bounds over all chunks gives the total error from Stage 1:\n$$ \\sum_{q=1}^r |\\hat{p}^{(q)}_{ij} - P^{(q)}_{ij}| \\le \\gamma_s \\sum_{q=1}^r \\sum_{t \\in I_q} |a_{it}b_{tj}| = \\gamma_s S_{ij} $$\n\n**Stage 2 Error:** The final value $\\hat{c}_{ij}$ is the sequential floating-point sum of the $r$ computed partial sums $\\hat{p}^{(q)}_{ij}$. This is a sum of $r$ terms, which involves $r-1$ additions. The error in this summation is:\n$$ |\\hat{c}_{ij} - \\sum_{q=1}^r \\hat{p}^{(q)}_{ij}| \\le \\gamma_{r-1} \\sum_{q=1}^r |\\hat{p}^{(q)}_{ij}| $$\nTo bound this, we need to bound $|\\hat{p}^{(q)}_{ij}|$:\n$$ |\\hat{p}^{(q)}_{ij}| = |P^{(q)}_{ij} + (\\hat{p}^{(q)}_{ij} - P^{(q)}_{ij})| \\le |P^{(q)}_{ij}| + |\\hat{p}^{(q)}_{ij} - P^{(q)}_{ij}| $$\n$$ |\\hat{p}^{(q)}_{ij}| \\le \\sum_{t \\in I_q} |a_{it}b_{tj}| + \\gamma_s \\sum_{t \\in I_q} |a_{it}b_{tj}| = (1+\\gamma_s) \\sum_{t \\in I_q} |a_{it}b_{tj}| $$\nSumming over $q$ from $1$ to $r$:\n$$ \\sum_{q=1}^r |\\hat{p}^{(q)}_{ij}| \\le (1+\\gamma_s) \\sum_{q=1}^r \\sum_{t \\in I_q} |a_{it}b_{tj}| = (1+\\gamma_s) S_{ij} $$\nSubstituting this into the Stage 2 error bound:\n$$ |\\hat{c}_{ij} - \\sum_{q=1}^r \\hat{p}^{(q)}_{ij}| \\le \\gamma_{r-1}(1+\\gamma_s) S_{ij} $$\n\n**Total Error:** The total error is the sum of errors from both stages. Let $c_{ij} = \\sum P^{(q)}_{ij}$ and $\\sum \\hat{p}^{(q)}_{ij} = \\sum(P^{(q)}_{ij} + (\\hat{p}^{(q)}_{ij} - P^{(q)}_{ij}))$.\n$$ \\hat{c}_{ij} - c_{ij} = (\\hat{c}_{ij} - \\sum_{q=1}^r \\hat{p}^{(q)}_{ij}) + (\\sum_{q=1}^r \\hat{p}^{(q)}_{ij} - \\sum_{q=1}^r P^{(q)}_{ij}) $$\nUsing the triangle inequality:\n$$ |\\hat{c}_{ij} - c_{ij}| \\le |\\hat{c}_{ij} - \\sum_{q=1}^r \\hat{p}^{(q)}_{ij}| + |\\sum_{q=1}^r (\\hat{p}^{(q)}_{ij} - P^{(q)}_{ij})| $$\n$$ |\\hat{c}_{ij} - c_{ij}| \\le \\gamma_{r-1}(1+\\gamma_s) S_{ij} + \\sum_{q=1}^r |\\hat{p}^{(q)}_{ij} - P^{(q)}_{ij}| $$\n$$ |\\hat{c}_{ij} - c_{ij}| \\le \\gamma_{r-1}(1+\\gamma_s) S_{ij} + \\gamma_s S_{ij} $$\n$$ |\\hat{c}_{ij} - c_{ij}| \\le (\\gamma_s + \\gamma_{r-1} + \\gamma_s\\gamma_{r-1}) S_{ij} $$\nThis is the forward error bound for the block accumulation algorithm.\n\n### Minimization of the Leading-Order Bound\nWe are given the simplifying regime $ku \\ll 1$ and $su \\ll 1$. This implies that any term of the form $Nu$ for $N \\le k$ is small. In this regime, we can approximate $\\gamma_N = \\frac{Nu}{1-Nu} \\approx Nu$.\nThe error bound for block accumulation becomes:\n$$ |\\hat{c}_{ij} - c_{ij}| \\lesssim (su + (r-1)u + (su)((r-1)u)) S_{ij} $$\nThe leading-order term in $u$ is found by ignoring the term with $u^2$:\n$$ |\\hat{c}_{ij} - c_{ij}|_{\\text{leading}} \\approx (s + r - 1) u S_{ij} $$\nTo minimize this bound, we need to minimize the factor $f(s) = s + r - 1$. The problem allows treating $s$ as a continuous real variable. We can therefore replace $r = \\lceil k/s \\rceil$ with the approximation $r \\approx k/s$.\nThe function to minimize becomes:\n$$ f(s) = s + \\frac{k}{s} - 1 $$\nTo find the minimum, we compute the derivative of $f(s)$ with respect to $s$ and set it to zero:\n$$ \\frac{df}{ds} = 1 - \\frac{k}{s^2} $$\nSetting the derivative to zero:\n$$ 1 - \\frac{k}{s^2} = 0 \\implies s^2 = k \\implies s = \\sqrt{k} $$\nWe take the positive root since $s$ must be a positive chunk size. To confirm this is a minimum, we check the second derivative:\n$$ \\frac{d^2f}{ds^2} = \\frac{2k}{s^3} $$\nFor $s = \\sqrt{k} > 0$ and $k > 0$, the second derivative is positive, confirming that $s = \\sqrt{k}$ corresponds to a local minimum. Thus, the optimal chunk size that minimizes the leading-order error bound is $s = \\sqrt{k}$.", "answer": "$$\\boxed{\\sqrt{k}}$$", "id": "3559509"}, {"introduction": "In our final practice, we explore the role of matrix multiplication within the broader context of matrix calculus and automatic differentiation. You will derive the adjoints for a matrix product, a cornerstone of reverse-mode differentiation used extensively in optimization and machine learning. The core of this exercise lies in implementing a robust validation test using finite differences to ensure your adjoint code is correct, a critical technique for developing reliable scientific software. [@problem_id:3559517]", "problem": "Consider the scalar functional defined on pairs of real matrices by the Frobenius inner product, where the Frobenius inner product of two matrices is given by $\\langle X,Y\\rangle_{\\mathrm{F}}=\\sum_{i,j}X_{ij}Y_{ij}$. Let $A\\in\\mathbb{R}^{m\\times n}$, $B\\in\\mathbb{R}^{n\\times p}$, and a fixed matrix $\\bar{C}\\in\\mathbb{R}^{m\\times p}$. Define the scalar map $f(A,B)=\\langle \\bar{C},\\,A B\\rangle_{\\mathrm{F}}$. Your task is to derive, from first principles, the first-order variation of $f$ under perturbations $(dA,dB)$, use it to design a central finite-difference directional derivative test, and then use this test to validate a reverse-mode adjoint implementation for matrix-matrix products that is susceptible to transpose mistakes.\n\nStart from the following fundamental bases:\n- The bilinearity of matrix multiplication and the definition of the matrix product in terms of its entries.\n- The definition of a directional derivative as a limit of a difference quotient.\n- The Frobenius inner product and its properties, especially linearity and the identity $\\langle X,YZ\\rangle_{\\mathrm{F}}=\\langle XZ^{\\top},Y\\rangle_{\\mathrm{F}}=\\langle X^{\\top}Y,Z^{\\top}\\rangle_{\\mathrm{F}}$ whenever the products are defined.\n\nFrom these bases, derive the first-order variation (the differential) of the map $(A,B)\\mapsto A B$ along $(dA,dB)$, and then deduce the corresponding adjoint (reverse-mode) expressions that relate $\\langle \\bar{C},\\,\\mathrm{d}(A B)\\rangle_{\\mathrm{F}}$ to $\\langle \\bar{A},\\,\\mathrm{d}A\\rangle_{\\mathrm{F}}$ and $\\langle \\bar{B},\\,\\mathrm{d}B\\rangle_{\\mathrm{F}}$ for appropriate adjoints $\\bar{A}$ and $\\bar{B}$. The goal is to design a finite-difference test that will:\n- Pass when the adjoint expressions are implemented correctly.\n- Fail (or raise a dimension mismatch) when a common transpose error is introduced in the adjoint code, specifically by replacing one required transpose by an identity map.\n\nImplement the following numerical test methodology:\n- For given $A$, $B$, and direction $(dA,dB)$, approximate the directional derivative of $f$ using the central difference quotient\n$$\nD_{\\mathrm{FD}} f(A,B)[dA,dB]\\approx\\frac{f(A+\\varepsilon dA,B+\\varepsilon dB)-f(A-\\varepsilon dA,B-\\varepsilon dB)}{2\\varepsilon},\n$$\nwith a small $\\varepsilon>0$.\n- Compute the adjoint-predicted directional derivative $D_{\\mathrm{ADJ}} f(A,B)[dA,dB]$ using an adjoint implementation. Do this once for the correct adjoint and once for each of the following two buggy variants:\n    1. A bug in the adjoint with respect to $A$ that omits a necessary transpose.\n    2. A bug in the adjoint with respect to $B$ that omits a necessary transpose.\n- Declare the test as passed for a given implementation if the relative error\n$$\n\\frac{\\left|D_{\\mathrm{FD}} f-D_{\\mathrm{ADJ}} f\\right|}{\\max\\{1,\\left|D_{\\mathrm{FD}} f\\right|\\}}\n$$\nis at most a specified tolerance.\n\nDesign a test suite by specifying $(m,n,p)$ and a pseudorandom seed for each case, and draw entries of $A$, $B$, $dA$, $dB$, and $\\bar{C}$ independently from a standard normal distribution for reproducibility. Use the following test suite, which probes a variety of shapes to ensure coverage of general cases and edge cases that either reveal subtle transpose mistakes or force dimension mismatches:\n\n- Test Case 1: $(m,n,p)=(5,3,4)$ with seed $123$.\n- Test Case 2: $(m,n,p)=(4,4,4)$ with seed $456$.\n- Test Case 3: $(m,n,p)=(1,2,3)$ with seed $789$.\n- Test Case 4: $(m,n,p)=(10,7,7)$ with seed $321$.\n- Test Case 5: $(m,n,p)=(2,2,3)$ with seed $135$.\n\nUse the same small central-difference step size $\\varepsilon=10^{-6}$ for all cases, and tolerance $10^{-7}$. In cases where a buggy adjoint produces a dimension mismatch, treat the test as having caught the bug.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element of the output list corresponds to one test case and is itself a three-element list of booleans $[p,c_A,c_B]$, where $p$ indicates whether the correct adjoint passes, $c_A$ indicates whether the test catches the buggy adjoint for $A$, and $c_B$ indicates whether the test catches the buggy adjoint for $B$. For example, the overall output format must be\n$[[p_1,c_{A,1},c_{B,1}],[p_2,c_{A,2},c_{B,2}],\\ldots]$.", "solution": "The problem requires the derivation and numerical validation of the adjoints for the matrix-matrix product operation within the context of a scalar functional. We will first derive the necessary mathematical expressions from first principles and then describe the design of a numerical test to validate an implementation.\n\nLet the matrices be $A \\in \\mathbb{R}^{m \\times n}$, $B \\in \\mathbb{R}^{n \\times p}$, and $\\bar{C} \\in \\mathbb{R}^{m \\times p}$. The scalar functional is defined by the Frobenius inner product as $f(A, B) = \\langle \\bar{C}, AB \\rangle_{\\mathrm{F}}$. The Frobenius inner product of two matrices $X, Y \\in \\mathbb{R}^{k \\times l}$ is $\\langle X, Y \\rangle_{\\mathrm{F}} = \\sum_{i=1}^{k} \\sum_{j=1}^{l} X_{ij} Y_{ij} = \\mathrm{tr}(X^{\\top}Y)$.\n\nOur first step is to find the first-order variation of $f$ with respect to infinitesimal perturbations $dA$ and $dB$. We begin by finding the variation of the product $C = AB$. Let's perturb $A$ by $dA$ and $B$ by $dB$. The perturbed product is:\n$$\n(A + dA)(B + dB) = AB + A(dB) + (dA)B + (dA)(dB)\n$$\nThe differential, or first-order variation, $\\mathrm{d}(AB)$, consists of terms that are linear in the perturbations $dA$ and $dB$. We neglect the higher-order term $(dA)(dB)$. Thus,\n$$\n\\mathrm{d}(AB) = A(dB) + (dA)B\n$$\nNow, we find the differential of the functional $f$. By linearity of the Frobenius inner product:\n$$\n\\mathrm{d}f = \\mathrm{d} \\langle \\bar{C}, AB \\rangle_{\\mathrm{F}} = \\langle \\bar{C}, \\mathrm{d}(AB) \\rangle_{\\mathrm{F}}\n$$\nSubstituting the expression for $\\mathrm{d}(AB)$:\n$$\n\\mathrm{d}f = \\langle \\bar{C}, A(dB) + (dA)B \\rangle_{\\mathrm{F}} = \\langle \\bar{C}, A(dB) \\rangle_{\\mathrm{F}} + \\langle \\bar{C}, (dA)B \\rangle_{\\mathrm{F}}\n$$\nThe reverse-mode or adjoint method seeks to express this differential in the form $\\mathrm{d}f = \\langle \\bar{A}, dA \\rangle_{\\mathrm{F}} + \\langle \\bar{B}, dB \\rangle_{\\mathrm{F}}$, where $\\bar{A}$ and $\\bar{B}$ are the adjoints of $A$ and $B$, respectively. We can find $\\bar{A}$ and $\\bar{B}$ by rearranging the terms in the expression for $\\mathrm{d}f$ to isolate $dA$ and $dB$ as the right-hand argument of the inner product. We use the property $\\langle X, YZ \\rangle_{\\mathrm{F}} = \\langle XZ^{\\top}, Y \\rangle_{\\mathrm{F}} = \\langle Y^{\\top}X, Z \\rangle_{\\mathrm{F}}$.\n\nFor the term involving $dB$, we have $\\langle \\bar{C}, A(dB) \\rangle_{\\mathrm{F}}$. Let $X=\\bar{C}$, $Y=A$, and $Z=dB$. Using the identity $\\langle X, YZ \\rangle_{\\mathrm{F}} = \\langle Y^{\\top}X, Z \\rangle_{\\mathrm{F}}$, we get:\n$$\n\\langle \\bar{C}, A(dB) \\rangle_{\\mathrm{F}} = \\langle A^{\\top}\\bar{C}, dB \\rangle_{\\mathrm{F}}\n$$\nThis identifies the adjoint of $B$ as $\\bar{B} = A^{\\top}\\bar{C}$. Let's verify the dimensions: $A^{\\top}$ is $n \\times m$ and $\\bar{C}$ is $m \\times p$, so their product $\\bar{B}$ is $n \\times p$, which matches the dimensions of $B$ and $dB$.\n\nFor the term involving $dA$, we have $\\langle \\bar{C}, (dA)B \\rangle_{\\mathrm{F}}$. Let $X=\\bar{C}$, $Y=dA$, and $Z=B$. Using the identity $\\langle X, YZ \\rangle_{\\mathrm{F}} = \\langle XZ^{\\top}, Y \\rangle_{\\mathrm{F}}$, we get:\n$$\n\\langle \\bar{C}, (dA)B \\rangle_{\\mathrm{F}} = \\langle \\bar{C}B^{\\top}, dA \\rangle_{\\mathrm{F}}\n$$\nThis identifies the adjoint of $A$ as $\\bar{A} = \\bar{C}B^{\\top}$. Let's verify the dimensions: $\\bar{C}$ is $m \\times p$ and $B^{\\top}$ is $p \\times n$, so their product $\\bar{A}$ is $m \\times n$, which matches the dimensions of $A$ and $dA$.\n\nSo, the correct adjoints are:\n$$\n\\bar{A} = \\bar{C}B^{\\top} \\quad \\text{and} \\quad \\bar{B} = A^{\\top}\\bar{C}\n$$\nThe directional derivative of $f$ along $(dA, dB)$ is given by $D f(A,B)[dA,dB] = \\langle \\bar{A}, dA \\rangle_{\\mathrm{F}} + \\langle \\bar{B}, dB \\rangle_{\\mathrm{F}}$.\n\nThe numerical validation test compares this analytical derivative with a finite-difference approximation. The central difference approximation for the directional derivative is:\n$$\nD_{\\mathrm{FD}} f(A,B)[dA,dB] \\approx \\frac{f(A+\\varepsilon dA, B+\\varepsilon dB) - f(A-\\varepsilon dA, B-\\varepsilon dB)}{2\\varepsilon}\n$$\nfor a small step size $\\varepsilon > 0$. The adjoint-based derivative is computed as:\n$$\nD_{\\mathrm{ADJ}} f(A,B)[dA,dB] = \\langle \\bar{C}B^{\\top}, dA \\rangle_{\\mathrm{F}} + \\langle A^{\\top}\\bar{C}, dB \\rangle_{\\mathrm{F}}\n$$\nA correct implementation should yield $D_{\\mathrm{FD}} f \\approx D_{\\mathrm{ADJ}} f$. The test passes if the relative error is below a tolerance $\\tau$:\n$$\n\\frac{|D_{\\mathrm{FD}} f - D_{\\mathrm{ADJ}} f|}{\\max\\{1, |D_{\\mathrm{FD}} f|\\}} \\le \\tau\n$$\n\nWe now analyze the buggy implementations.\n1.  **Buggy $\\bar{A}$**: The implementation omits the transpose on $B$, calculating $\\bar{A}_{\\text{buggy}} = \\bar{C}B$. The matrix product $\\bar{C}B$ is defined only if the inner dimensions match, i.e., column count of $\\bar{C}$ ($p$) equals row count of $B$ ($n$). If $p \\neq n$, a dimension mismatch error occurs, which immediately reveals the bug. If $p=n$, the calculation proceeds, but the resulting derivative will be numerically incorrect, and the relative error test should fail.\n2.  **Buggy $\\bar{B}$**: The implementation omits the transpose on $A$, calculating $\\bar{B}_{\\text{buggy}} = A\\bar{C}$. This product is defined only if the column count of $A$ ($n$) equals the row count of $\\bar{C}$ ($m$). If $n \\neq m$, a dimension mismatch occurs. If $n=m$, the calculation proceeds but yields a numerically incorrect derivative, which the test should detect.\n\nThe test suite is designed to probe these conditions. Cases where dimensions are unequal (e.g., $(m,n,p)=(5,3,4)$) are expected to cause dimension mismatch errors for buggy implementations. Cases where some dimensions are equal (e.g., $(m,n,p)=(4,4,4)$ or $(m,n,p)=(10,7,7)$) will require the numerical comparison to detect the bug. The implementation will systematically check the correct adjoint, the buggy adjoint for $A$, and the buggy adjoint for $B$ against the finite difference approximation.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Derives and validates adjoints for matrix-matrix multiplication using a \n    finite-difference directional derivative test.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    # Each tuple is (m, n, p, seed).\n    test_cases = [\n        (5, 3, 4, 123),\n        (4, 4, 4, 456),\n        (1, 2, 3, 789),\n        (10, 7, 7, 321),\n        (2, 2, 3, 135),\n    ]\n\n    epsilon = 1e-6\n    tolerance = 1e-7\n    \n    results = []\n\n    for m, n, p, seed in test_cases:\n        # Set seed for reproducibility\n        rng = np.random.default_rng(seed)\n        \n        # Generate random matrices with entries from a standard normal distribution\n        A = rng.standard_normal((m, n))\n        B = rng.standard_normal((n, p))\n        C_bar = rng.standard_normal((m, p))\n        dA = rng.standard_normal((m, n))\n        dB = rng.standard_normal((n, p))\n\n        # Helper function for the scalar functional\n        def f(mat_A, mat_B, mat_C_bar):\n            # f(A,B) = <C_bar, AB>_F\n            # This is equivalent to tr(C_bar^T * (A @ B)) or sum(C_bar * (A @ B))\n            return np.sum(mat_C_bar * (mat_A @ mat_B))\n            \n        # 1. Compute the finite-difference approximation of the directional derivative\n        f_plus = f(A + epsilon * dA, B + epsilon * dB, C_bar)\n        f_minus = f(A - epsilon * dA, B - epsilon * dB, C_bar)\n        dF_fd = (f_plus - f_minus) / (2 * epsilon)\n        \n        # Function to compute relative error\n        def check_error(dF_adj, dF_ref):\n            error = np.abs(dF_adj - dF_ref)\n            norm = np.max([1.0, np.abs(dF_ref)])\n            return error / norm\n\n        # --- Test the implementations ---\n        # p: correct adjoint passes\n        # c_A: buggy A-adjoint is caught\n        # c_B: buggy B-adjoint is caught\n        case_results = []\n        \n        # 2. Test the correct adjoint implementation\n        # A_bar = C_bar @ B.T\n        # B_bar = A.T @ C_bar\n        A_bar_correct = C_bar @ B.T\n        B_bar_correct = A.T @ C_bar\n        dF_adj_correct = np.sum(A_bar_correct * dA) + np.sum(B_bar_correct * dB)\n        \n        err_correct = check_error(dF_adj_correct, dF_fd)\n        p_pass = err_correct <= tolerance\n        case_results.append(p_pass)\n\n        # 3. Test the buggy adjoint for A (omitting transpose on B)\n        # A_bar_buggy = C_bar @ B\n        c_A_caught = False\n        try:\n            A_bar_buggy = C_bar @ B\n            # If no dimension mismatch, check numerically\n            dF_adj_buggy_A = np.sum(A_bar_buggy * dA) + np.sum(B_bar_correct * dB)\n            err_buggy_A = check_error(dF_adj_buggy_A, dF_fd)\n            if err_buggy_A > tolerance:\n                c_A_caught = True\n        except ValueError:\n            # Dimension mismatch occurred, bug is caught\n            c_A_caught = True\n        case_results.append(c_A_caught)\n\n        # 4. Test the buggy adjoint for B (omitting transpose on A)\n        # B_bar_buggy = A @ C_bar\n        c_B_caught = False\n        try:\n            B_bar_buggy = A @ C_bar\n            # If no dimension mismatch, check numerically\n            dF_adj_buggy_B = np.sum(A_bar_correct * dA) + np.sum(B_bar_buggy * dB)\n            err_buggy_B = check_error(dF_adj_buggy_B, dF_fd)\n            if err_buggy_B > tolerance:\n                c_B_caught = True\n        except ValueError:\n            # Dimension mismatch occurred, bug is caught\n            c_B_caught = True\n        case_results.append(c_B_caught)\n        \n        results.append(case_results)\n\n    # Final print statement in the exact required format\n    results_str = ','.join([f\"[{','.join(str(b).lower() for b in res)}]\" for res in results])\n    print(f\"[{results_str}]\")\n\nsolve()\n```", "id": "3559517"}]}