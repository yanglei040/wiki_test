{"hands_on_practices": [{"introduction": "Before we can build complex algorithms, we must master the fundamentals. This first exercise bridges the gap between the abstract definition of a vector subspace and its concrete geometric and algebraic properties. By determining whether a given vector lies within the span of a set of other vectors, you will practice translating the concept of subspace membership into a test for the consistency of a linear system [@problem_id:3600936]. Furthermore, by computing the orthogonal projection, you will find the best approximation of the vector within the subspace, a core concept in optimization and data analysis.", "problem": "Let $n \\in \\mathbb{N}$ and consider the real vector space $\\mathbb{R}^{n}$ equipped with the standard Euclidean inner product. The span $\\operatorname{span}\\{v_{1},\\dots,v_{k}\\}$ of vectors $v_{1},\\dots,v_{k} \\in \\mathbb{R}^{n}$ is the set of all finite linear combinations of these vectors. A vector $w \\in \\mathbb{R}^{n}$ belongs to $\\operatorname{span}\\{v_{1},\\dots,v_{k}\\}$ if and only if there exist scalars $c_{1},\\dots,c_{k} \\in \\mathbb{R}$ such that $w = \\sum_{j=1}^{k} c_{j} v_{j}$, equivalently if the linear system formed by taking the matrix with columns $v_{1},\\dots,v_{k}$ has a solution.\n\nStarting from these definitions and the well-tested facts of linear algebra concerning linear systems, ranks, and orthogonal projections, carry out the following for the specific data\n$$\nv_{1} = \\begin{pmatrix}1 \\\\ 0 \\\\ 1 \\\\ 0\\end{pmatrix},\\quad\nv_{2} = \\begin{pmatrix}0 \\\\ 1 \\\\ 1 \\\\ 0\\end{pmatrix},\\quad\nv_{3} = \\begin{pmatrix}1 \\\\ 1 \\\\ 2 \\\\ 0\\end{pmatrix},\\quad\nw = \\begin{pmatrix}2 \\\\ -1 \\\\ 1 \\\\ 3\\end{pmatrix} \\in \\mathbb{R}^{4}.\n$$\n1. Formulate the linear system that encodes the condition $w \\in \\operatorname{span}\\{v_{1},v_{2},v_{3}\\}$ and determine, by principled reasoning from the definitions and properties of linear systems and ranks, whether this system is consistent.\n2. Using foundational facts about orthogonal projections onto subspaces defined as spans of vectors, solve for the orthogonal projection of $w$ onto the subspace $V = \\operatorname{span}\\{v_{1},v_{2},v_{3}\\}$ and deduce the Euclidean distance from $w$ to $V$.\n\nExpress your final answer as the exact value of the Euclidean distance (the $2$-norm) from $w$ to $V$, with no rounding.", "solution": "The problem is valid as it is well-defined, self-contained, and grounded in the standard principles of linear algebra.\n\nThe problem consists of two parts. First, we determine if the vector $w$ lies in the subspace $V = \\operatorname{span}\\{v_{1}, v_{2}, v_{3}\\}$. Second, we compute the orthogonal projection of $w$ onto $V$ and find the Euclidean distance from $w$ to $V$.\n\n**Part 1: Consistency of the Linear System**\n\nA vector $w$ belongs to the span of a set of vectors $\\{v_{1}, v_{2}, v_{3}\\}$ if and only if it can be expressed as a linear combination of these vectors. That is, there must exist scalars $c_{1}, c_{2}, c_{3} \\in \\mathbb{R}$ such that:\n$$ c_{1}v_{1} + c_{2}v_{2} + c_{3}v_{3} = w $$\nThis vector equation can be written as a linear system in matrix form, $Ac=w$, where the matrix $A$ has the vectors $v_{1}, v_{2}, v_{3}$ as its columns, and $c$ is the vector of coefficients.\nGiven the vectors:\n$$ v_{1} = \\begin{pmatrix}1 \\\\ 0 \\\\ 1 \\\\ 0\\end{pmatrix},\\quad v_{2} = \\begin{pmatrix}0 \\\\ 1 \\\\ 1 \\\\ 0\\end{pmatrix},\\quad v_{3} = \\begin{pmatrix}1 \\\\ 1 \\\\ 2 \\\\ 0\\end{pmatrix},\\quad w = \\begin{pmatrix}2 \\\\ -1 \\\\ 1 \\\\ 3\\end{pmatrix} $$\nThe matrix $A$ and the vector $c$ are:\n$$ A = \\begin{pmatrix} 1  0  1 \\\\ 0  1  1 \\\\ 1  1  2 \\\\ 0  0  0 \\end{pmatrix}, \\quad c = \\begin{pmatrix} c_{1} \\\\ c_{2} \\\\ c_{3} \\end{pmatrix} $$\nThe linear system is consistent if and only if the rank of the coefficient matrix $A$ is equal to the rank of the augmented matrix $[A|w]$.\n\nFirst, we determine the rank of $A$ by reducing it to row echelon form.\n$$ A = \\begin{pmatrix} 1  0  1 \\\\ 0  1  1 \\\\ 1  1  2 \\\\ 0  0  0 \\end{pmatrix} \\xrightarrow{R_3 \\to R_3-R_1} \\begin{pmatrix} 1  0  1 \\\\ 0  1  1 \\\\ 0  1  1 \\\\ 0  0  0 \\end{pmatrix} \\xrightarrow{R_3 \\to R_3-R_2} \\begin{pmatrix} 1  0  1 \\\\ 0  1  1 \\\\ 0  0  0 \\\\ 0  0  0 \\end{pmatrix} $$\nThe row echelon form of $A$ has two non-zero rows, so $\\operatorname{rank}(A) = 2$. The fact that the rank is less than the number of vectors ($2  3$) demonstrates that the set $\\{v_{1}, v_{2}, v_{3}\\}$ is linearly dependent. We can observe that $v_{3} = v_{1} + v_{2}$.\n\nNext, we determine the rank of the augmented matrix $[A|w]$.\n$$ [A|w] = \\begin{pmatrix} 1  0  1  2 \\\\ 0  1  1  -1 \\\\ 1  1  2  1 \\\\ 0  0  0  3 \\end{pmatrix} $$\nWe perform the same row operations:\n$$ \\begin{pmatrix} 1  0  1  2 \\\\ 0  1  1  -1 \\\\ 1  1  2  1 \\\\ 0  0  0  3 \\end{pmatrix} \\xrightarrow{R_3 \\to R_3-R_1} \\begin{pmatrix} 1  0  1  2 \\\\ 0  1  1  -1 \\\\ 0  1  1  -1 \\\\ 0  0  0  3 \\end{pmatrix} \\xrightarrow{R_3 \\to R_3-R_2} \\begin{pmatrix} 1  0  1  2 \\\\ 0  1  1  -1 \\\\ 0  0  0  0 \\\\ 0  0  0  3 \\end{pmatrix} $$\nSwapping rows $R_3$ and $R_4$ to obtain a row echelon form:\n$$ \\begin{pmatrix} 1  0  1  2 \\\\ 0  1  1  -1 \\\\ 0  0  0  3 \\\\ 0  0  0  0 \\end{pmatrix} $$\nThe row echelon form of $[A|w]$ has three non-zero rows, thus $\\operatorname{rank}([A|w]) = 3$.\nSince $\\operatorname{rank}(A) = 2 \\neq \\operatorname{rank}([A|w]) = 3$, the linear system is inconsistent. This proves that $w \\notin \\operatorname{span}\\{v_{1}, v_{2}, v_{3}\\}$.\n\n**Part 2: Orthogonal Projection and Distance**\n\nThe subspace $V$ is given by $V = \\operatorname{span}\\{v_{1}, v_{2}, v_{3}\\}$. As established, $v_{3} = v_{1} + v_{2}$, so $v_{3}$ is redundant. A basis for $V$ is $\\{v_{1}, v_{2}\\}$. Therefore, $V = \\operatorname{span}\\{v_{1}, v_{2}\\}$.\n\nThe orthogonal projection of $w$ onto $V$, which we denote as $p = \\operatorname{proj}_{V}(w)$, is the unique vector in $V$ such that the vector $w - p$ is orthogonal to every vector in $V$. The Euclidean distance from $w$ to $V$ is then $\\|w - p\\|$.\n\nThe projection $p$ can be found by solving the normal equations. Let $A'$ be the matrix whose columns form a basis for $V$, i.e., $A' = [v_{1}|v_{2}]$.\n$$ A' = \\begin{pmatrix} 1  0 \\\\ 0  1 \\\\ 1  1 \\\\ 0  0 \\end{pmatrix} $$\nThe projection $p$ is given by the formula $p = A'(A'^{T}A')^{-1}A'^{T}w$.\nFirst, we compute the Gram matrix $A'^{T}A'$:\n$$ A'^{T}A' = \\begin{pmatrix} 1  0  1  0 \\\\ 0  1  1  0 \\end{pmatrix} \\begin{pmatrix} 1  0 \\\\ 0  1 \\\\ 1  1 \\\\ 0  0 \\end{pmatrix} = \\begin{pmatrix} 1\\cdot1+1\\cdot1  1\\cdot0+1\\cdot1 \\\\ 1\\cdot1+0\\cdot1  1\\cdot1+1\\cdot1 \\end{pmatrix} = \\begin{pmatrix} 2  1 \\\\ 1  2 \\end{pmatrix} $$\nNext, we find the inverse of this matrix:\n$$ (A'^{T}A')^{-1} = \\frac{1}{(2)(2) - (1)(1)} \\begin{pmatrix} 2  -1 \\\\ -1  2 \\end{pmatrix} = \\frac{1}{3} \\begin{pmatrix} 2  -1 \\\\ -1  2 \\end{pmatrix} $$\nNow we compute $A'^{T}w$:\n$$ A'^{T}w = \\begin{pmatrix} 1  0  1  0 \\\\ 0  1  1  0 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ -1 \\\\ 1 \\\\ 3 \\end{pmatrix} = \\begin{pmatrix} 1\\cdot2+0\\cdot(-1)+1\\cdot1+0\\cdot3 \\\\ 0\\cdot2+1\\cdot(-1)+1\\cdot1+0\\cdot3 \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ 0 \\end{pmatrix} $$\nWe can now find the projection $p$. The coefficients of the linear combination of the basis vectors are $c' = (A'^{T}A')^{-1}A'^{T}w$:\n$$ c' = \\frac{1}{3} \\begin{pmatrix} 2  -1 \\\\ -1  2 \\end{pmatrix} \\begin{pmatrix} 3 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{3}(6) \\\\ \\frac{1}{3}(-3) \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix} $$\nThe projection vector is $p = A'c' = 2v_{1} - 1v_{2}$:\n$$ p = \\begin{pmatrix} 1  0 \\\\ 0  1 \\\\ 1  1 \\\\ 0  0 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ -1 \\\\ 1 \\\\ 0 \\end{pmatrix} $$\nThe vector representing the shortest distance from $w$ to the subspace $V$ is the component of $w$ orthogonal to $V$, which is $w - p$:\n$$ w - p = \\begin{pmatrix} 2 \\\\ -1 \\\\ 1 \\\\ 3 \\end{pmatrix} - \\begin{pmatrix} 2 \\\\ -1 \\\\ 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 3 \\end{pmatrix} $$\nThe Euclidean distance from $w$ to $V$ is the norm of this orthogonal vector:\n$$ \\|w - p\\| = \\left\\| \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 3 \\end{pmatrix} \\right\\| = \\sqrt{0^{2} + 0^{2} + 0^{2} + 3^{2}} = \\sqrt{9} = 3 $$\nThus, the exact Euclidean distance from $w$ to the subspace $V$ is $3$.", "answer": "$$ \\boxed{3} $$", "id": "3600936"}, {"introduction": "The Fundamental Theorem of Linear Algebra provides a beautiful and complete description of the domain and codomain of a linear map $A$, partitioning them into four orthogonal subspaces. This practice moves from manual calculations to the powerful machinery of numerical linear algebra, asking you to use the Singular Value Decomposition (SVD) to compute orthonormal bases for all four fundamental subspaces [@problem_id:3600957]. This exercise is essential for understanding how to numerically implement and verify one of the most important theoretical results in the field.", "problem": "You are given the task of programmatically extracting orthonormal bases for the four fundamental subspaces associated with a real matrix $A$, and numerically confirming the orthogonality relations that arise from the fundamental theorem of linear algebra. The four fundamental subspaces are the column space $\\mathcal{R}(A)$, the null space $\\mathcal{N}(A)$, the row space $\\mathcal{R}(A^\\top)$, and the left null space $\\mathcal{N}(A^\\top)$. The definitions to start from are: a subspace of a vector space over $\\mathbb{R}$ is any subset closed under addition and scalar multiplication; the column space $\\mathcal{R}(A)$ is $\\{A x : x \\in \\mathbb{R}^n\\}$; the null space $\\mathcal{N}(A)$ is $\\{x \\in \\mathbb{R}^n : A x = 0\\}$; the row space $\\mathcal{R}(A^\\top)$ is $\\{A^\\top y : y \\in \\mathbb{R}^m\\}$; and the left null space $\\mathcal{N}(A^\\top)$ is $\\{y \\in \\mathbb{R}^m : A^\\top y = 0\\}$. You may use the Singular Value Decomposition (SVD) or the QR decomposition (QR), but the extraction of orthonormal bases must be numerically justified by those decompositions.\n\nFrom the Singular Value Decomposition (SVD), if $A \\in \\mathbb{R}^{m \\times n}$, then $A = U \\Sigma V^\\top$, where $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{n \\times n}$ have orthonormal columns (orthogonal matrices), and $\\Sigma \\in \\mathbb{R}^{m \\times n}$ is diagonal with nonnegative singular values $\\sigma_1 \\ge \\sigma_2 \\ge \\cdots \\ge \\sigma_{\\min(m,n)} \\ge 0$. Partition $U = [U_r \\; U_0]$ and $V = [V_r \\; V_0]$ according to the numerical rank $r$ determined by a tolerance. Then $\\mathcal{R}(A)$ is spanned by the columns of $U_r$, $\\mathcal{R}(A^\\top)$ is spanned by the columns of $V_r$, $\\mathcal{N}(A)$ is spanned by the columns of $V_0$, and $\\mathcal{N}(A^\\top)$ is spanned by the columns of $U_0$. The fundamental orthogonality relations are $\\mathcal{N}(A)$ orthogonal to $\\mathcal{R}(A^\\top)$ and $\\mathcal{N}(A^\\top)$ orthogonal to $\\mathcal{R}(A)$.\n\nYour program must:\n- For each test matrix $A$, compute an SVD $A = U \\Sigma V^\\top$ and decide the numerical rank $r$ using the tolerance\n$$\\tau = \\max(m,n) \\, \\sigma_{\\max} \\, \\epsilon,$$\nwhere $m \\times n$ is the shape of $A$, $\\sigma_{\\max}$ is the largest singular value of $A$, and $\\epsilon$ is double-precision machine epsilon ($\\epsilon \\approx 2.22 \\times 10^{-16}$).\n- Form orthonormal bases for the four fundamental subspaces:\n    1. $\\mathcal{R}(A)$ basis: columns of $U_r \\in \\mathbb{R}^{m \\times r}$.\n    2. $\\mathcal{R}(A^\\top)$ basis: columns of $V_r \\in \\mathbb{R}^{n \\times r}$.\n    3. $\\mathcal{N}(A)$ basis: columns of $V_0 \\in \\mathbb{R}^{n \\times (n-r)}$.\n    4. $\\mathcal{N}(A^\\top)$ basis: columns of $U_0 \\in \\mathbb{R}^{m \\times (m-r)}$.\n- Numerically confirm, using the Frobenius norm, both the orthonormality of each basis and the orthogonality relations:\n    - For any basis matrix $Q$ with $k$ columns, check $Q^\\top Q \\approx I_k$ by computing $\\|Q^\\top Q - I_k\\|_F$.\n    - Check $\\|V_r^\\top V_0\\|_F \\approx 0$ and $\\|U_r^\\top U_0\\|_F \\approx 0$.\n- Additionally, confirm that the truncated reconstruction $A_r = U_r \\Sigma_r V_r^\\top$ approximates $A$ with small relative residual $\\|A - A_r\\|_F / \\|A\\|_F$ (use absolute residual $\\|A - A_r\\|_F$ if $\\|A\\|_F = 0$).\n- Verify the dimension equalities $r + \\dim(\\mathcal{N}(A)) = n$ and $r + \\dim(\\mathcal{N}(A^\\top)) = m$ as exact integer equalities.\n\nFor each test matrix, aggregate a single float equal to the maximum over all residual norms described above (orthonormality residuals, orthogonality residuals, and reconstruction residual), along with a boolean indicating whether the dimension equalities hold. The final output must aggregate the results of all provided test cases into a single line formatted as a comma-separated list enclosed in square brackets, in the order\n$$[e_1, b_1, e_2, b_2, e_3, b_3, e_4, b_4],$$\nwhere $e_k$ is the maximum residual float for the $k$-th test matrix, and $b_k$ is the corresponding boolean.\n\nTest suite matrices (to be used exactly as specified):\n1. A tall, full column rank case ($4 \\times 3$):\n$$\nA_1 = \\begin{bmatrix}\n2  -1  0 \\\\\n0  1  3 \\\\\n4  -2  1 \\\\\n1  0  -1\n\\end{bmatrix}.\n$$\n2. A wide, rank-deficient case ($3 \\times 4$) with row dependencies:\n$$\nA_2 = \\begin{bmatrix}\n1  0  1  1 \\\\\n0  1  1  2 \\\\\n1  1  2  3\n\\end{bmatrix}.\n$$\n3. A square, rank $3$ case ($5 \\times 5$) constructed so that two columns are linear combinations of the first three:\nLet the columns be\n$$\nc_1 = \\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\\\ 2 \\\\ 3 \\end{bmatrix},\\quad\nc_2 = \\begin{bmatrix} 0 \\\\ 1 \\\\ 2 \\\\ -1 \\\\ 0 \\end{bmatrix},\\quad\nc_3 = \\begin{bmatrix} 2 \\\\ -1 \\\\ 0 \\\\ 1 \\\\ 1 \\end{bmatrix},\\quad\nc_4 = c_1 + c_2,\\quad\nc_5 = c_2 - c_3,\n$$\nso that\n$$\nA_3 = \\begin{bmatrix}\n1  0  2  1  -2 \\\\\n0  1  -1  1  2 \\\\\n1  2  0  3  2 \\\\\n2  -1  1  1  -2 \\\\\n3  0  1  3  -1\n\\end{bmatrix}.\n$$\n4. A near rank-deficient diagonal case ($4 \\times 4$) where one singular value is below the tolerance:\n$$\nA_4 = \\operatorname{diag}\\!\\left(10,\\; 1\\times 10^{-16},\\; 3,\\; 1\\right).\n$$\n\nAngle units are not applicable. No physical units appear, so none are required. Your program must produce exactly one output line containing the results as a comma-separated list enclosed in square brackets as described above, with no extra spaces or text.", "solution": "The problem requires the programmatic extraction and numerical validation of orthonormal bases for the four fundamental subspaces of a real matrix $A \\in \\mathbb{R}^{m \\times n}$: the column space $\\mathcal{R}(A)$, the null space $\\mathcal{N}(A)$, the row space $\\mathcal{R}(A^\\top)$, and the left null space $\\mathcal{N}(A^\\top)$. This task is a direct application of the Fundamental Theorem of Linear Algebra, which establishes the dimensions and orthogonality relationships between these subspaces. The Singular Value Decomposition (SVD) provides a powerful and numerically stable tool to compute these bases.\n\nThe SVD of a matrix $A \\in \\mathbb{R}^{m \\times n}$ is a factorization $A = U \\Sigma V^\\top$, where $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{n \\times n}$ are orthogonal matrices, and $\\Sigma \\in \\mathbb{R}^{m \\times n}$ is a rectangular diagonal matrix with non-negative real numbers on the diagonal, known as singular values, sorted in descending order: $\\sigma_1 \\ge \\sigma_2 \\ge \\cdots \\ge \\sigma_p \\ge 0$, where $p = \\min(m, n)$.\n\nThe core principle is that the columns of $U$ and $V$ (the singular vectors) provide orthonormal bases for the four fundamental subspaces. The distinction between vectors spanning the spaces and their orthogonal complements is determined by the numerical rank, $r$. The rank $r$ is the number of singular values that are \"significantly\" greater than zero. In numerical computation, we define this using a tolerance $\\tau$. The problem specifies a standard choice for this tolerance:\n$$\n\\tau = \\max(m, n) \\cdot \\sigma_{\\max} \\cdot \\epsilon,\n$$\nwhere $\\sigma_{\\max}$ is the largest singular value $\\sigma_1$, and $\\epsilon$ is the machine epsilon for double-precision floating-point arithmetic. The numerical rank $r$ is then the count of singular values $\\sigma_i > \\tau$.\n\nGiven the rank $r$, we partition the orthogonal matrices $U$ and $V$. Let the columns of $U$ be $u_1, \\dots, u_m$ and the columns of $V$ be $v_1, \\dots, v_n$. We form the partitions:\n- $U_r = [u_1, \\dots, u_r] \\in \\mathbb{R}^{m \\times r}$\n- $U_0 = [u_{r+1}, \\dots, u_m] \\in \\mathbb{R}^{m \\times (m-r)}$\n- $V_r = [v_1, \\dots, v_r] \\in \\mathbb{R}^{n \\times r}$\n- $V_0 = [v_{r+1}, \\dots, v_n] \\in \\mathbb{R}^{n \\times (n-r)}$\n\nThe SVD directly gives us the orthonormal bases for the four fundamental subspaces:\n1.  **Column Space $\\mathcal{R}(A)$**: An orthonormal basis is given by the columns of $U_r$. Its dimension is $r$.\n2.  **Left Null Space $\\mathcal{N}(A^\\top)$**: An orthonormal basis is given by the columns of $U_0$. Its dimension is $m-r$.\n3.  **Row Space $\\mathcal{R}(A^\\top)$**: An orthonormal basis is given by the columns of $V_r$. Its dimension is $r$.\n4.  **Null Space $\\mathcal{N}(A)$**: An orthonormal basis is given by the columns of $V_0$. Its dimension is $n-r$.\n\nThe Fundamental Theorem of Linear Algebra establishes that $\\mathcal{R}(A)$ is the orthogonal complement of $\\mathcal{N}(A^\\top)$ in $\\mathbb{R}^m$, and $\\mathcal{R}(A^\\top)$ is the orthogonal complement of $\\mathcal{N}(A)$ in $\\mathbb{R}^n$. Our algorithm will numerically verify these properties.\n\nThe algorithmic procedure for each test matrix $A$ is as follows:\n1.  Compute the full SVD $A = U \\Sigma V^\\top$. The matrices $U$ and $V$ must be square (orthogonal), not semi-orthogonal.\n2.  Determine the numerical rank $r$ by counting singular values $s_i$ that exceed the tolerance $\\tau$.\n3.  Partition $U$ and $V$ into $U_r, U_0, V_r, V_0$ based on the rank $r$.\n4.  Perform numerical validations and compute their residuals using the Frobenius norm, $\\| \\cdot \\|_F$:\n    a. **Orthonormality of bases**: For each non-empty basis matrix $Q \\in \\{U_r, U_0, V_r, V_0\\}$, compute the residual $\\|Q^\\top Q - I\\|_F$. Here $I$ is the identity matrix of the appropriate size.\n    b. **Orthogonality of subspaces**: Compute the residuals $\\|U_r^\\top U_0\\|_F$ and $\\|V_r^\\top V_0\\|_F$. These represent the dot products between basis vectors of a space and its orthogonal complement, which should be nearly zero.\n    c. **Reconstruction accuracy**: Compute the truncated SVD reconstruction $A_r = U_r \\Sigma_r V_r^\\top$, where $\\Sigma_r$ is the $r \\times r$ diagonal matrix of the first $r$ singular values. Calculate the relative reconstruction error $\\|A - A_r\\|_F / \\|A\\|_F$.\n5.  Verify the dimension theorem (rank-nullity theorem) identities as exact integer equalities: $r + \\dim(\\mathcal{N}(A)) = n$ and $r + \\dim(\\mathcal{N}(A^\\top)) = m$. By our construction, $\\dim(\\mathcal{N}(A))$ is the number of columns in $V_0$, which is $n-r$, and $\\dim(\\mathcal{N}(A^\\top))$ is the number of columns in $U_0$, which is $m-r$. Thus, the checks become $r + (n-r) = n$ and $r + (m-r) = m$, which are algebraic tautologies. Their programmatic verification confirms correct implementation of the matrix partitioning.\n6.  Aggregate the results for matrix $A_k$ into a pair $(e_k, b_k)$, where $e_k$ is the maximum of all computed floating-point residuals, and $b_k$ is the boolean result of the dimension equality checks.\n\nThis procedure will be applied to each of the four provided test matrices. The final output will be a list concatenating the $(e_k, b_k)$ pairs for all test cases.", "answer": "```python\nimport numpy as np\n\ndef analyze_matrix(A):\n    \"\"\"\n    Analyzes a matrix A to find its four fundamental subspaces,\n    validates their properties, and returns aggregated error metrics.\n    \"\"\"\n    m, n = A.shape\n    \n    # Step 1: Compute SVD\n    # full_matrices=True is essential to get square U and V for partitioning.\n    try:\n        U, s, Vt = np.linalg.svd(A, full_matrices=True)\n    except np.linalg.LinAlgError:\n        # Handle cases where SVD does not converge, though unlikely for these examples.\n        return float('inf'), False\n\n    # Initialize a list to store all computed numerical residuals.\n    residuals = []\n\n    # Step 2: Determine numerical rank\n    # s is sorted in descending order.\n    # Handle the case of a zero matrix where s would be empty or all zeros.\n    s_max = s[0] if len(s) > 0 else 0.0\n    eps = np.finfo(float).eps\n    tau = max(m, n) * s_max * eps\n    r = np.sum(s > tau)\n    \n    # Step 3: Partition U and V based on rank r\n    V = Vt.T\n    Ur = U[:, :r]\n    U0 = U[:, r:]\n    Vr = V[:, :r]\n    V0 = V[:, r:]\n\n    # Step 4a: Check orthonormality of each basis\n    # Q^T Q should be close to I_k.\n    # The check is only performed if the basis is non-empty (k > 0).\n    if Ur.shape[1] > 0:\n        residuals.append(np.linalg.norm(Ur.T @ Ur - np.eye(r), 'fro'))\n    if U0.shape[1] > 0:\n        residuals.append(np.linalg.norm(U0.T @ U0 - np.eye(m - r), 'fro'))\n    if Vr.shape[1] > 0:\n        residuals.append(np.linalg.norm(Vr.T @ Vr - np.eye(r), 'fro'))\n    if V0.shape[1] > 0:\n        residuals.append(np.linalg.norm(V0.T @ V0 - np.eye(n - r), 'fro'))\n\n    # Step 4b: Check orthogonality between subspaces\n    # These products should be close to zero matrices. numpy.linalg.norm handles\n    # products involving empty matrices correctly (norm is 0).\n    residuals.append(np.linalg.norm(Ur.T @ U0, 'fro'))\n    residuals.append(np.linalg.norm(Vr.T @ V0, 'fro'))\n\n    # Step 4c: Check reconstruction accuracy\n    # Ar = Ur @ diag(s_r) @ Vr.T\n    if r > 0:\n        Ar = Ur @ np.diag(s[:r]) @ Vr.T\n    else: # If rank is 0, A is the zero matrix\n        Ar = np.zeros_like(A, dtype=float)\n        \n    norm_A = np.linalg.norm(A, 'fro')\n    if norm_A == 0:\n        reconstruction_resid = np.linalg.norm(A - Ar, 'fro')\n    else:\n        reconstruction_resid = np.linalg.norm(A - Ar, 'fro') / norm_A\n    residuals.append(reconstruction_resid)\n\n    # Step 5: Verify dimension equalities\n    dim_N_A = V0.shape[1]\n    dim_N_At = U0.shape[1]\n    dim_check1 = (r + dim_N_A == n)\n    dim_check2 = (r + dim_N_At == m)\n    b_k = dim_check1 and dim_check2\n\n    # Step 6: Aggregate results\n    # If residuals list is empty (can happen for trivial cases), max would error.\n    e_k = max(residuals) if residuals else 0.0\n\n    return e_k, b_k\n\ndef solve():\n    # Define the test cases from the problem statement.\n    A1 = np.array([\n        [2, -1, 0],\n        [0, 1, 3],\n        [4, -2, 1],\n        [1, 0, -1]\n    ], dtype=float)\n\n    A2 = np.array([\n        [1, 0, 1, 1],\n        [0, 1, 1, 2],\n        [1, 1, 2, 3]\n    ], dtype=float)\n\n    c1 = np.array([1, 0, 1, 2, 3])\n    c2 = np.array([0, 1, 2, -1, 0])\n    c3 = np.array([2, -1, 0, 1, 1])\n    c4 = c1 + c2\n    c5 = c2 - c3\n    A3 = np.vstack([c1, c2, c3, c4, c5]).T\n\n    A4 = np.diag([10.0, 1e-16, 3.0, 1.0])\n\n    test_cases = [A1, A2, A3, A4]\n\n    results = []\n    for case in test_cases:\n        e_k, b_k = analyze_matrix(case)\n        results.append(e_k)\n        results.append(b_k)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3600957"}, {"introduction": "Real-world problems often involve understanding the relationship between different subspaces, such as finding common modes of behavior or identifying states that are invisible to a set of sensors. This advanced practice challenges you to design and implement an algorithm to compute the intersection of two fundamental subspaces, the range of one matrix and the null space of another [@problem_id:3600943]. You will synthesize your knowledge of different matrix factorizations and take a first step into perturbation theory, analyzing how sensitive your computed result is to small changes in the input data.", "problem": "Design and implement a principled numerical algorithm, together with a computable perturbation bound, to approximate the intersection subspace $\\,\\mathcal{R}(A)\\cap \\mathcal{N}(B)\\,$ for given real matrices $\\,A\\in\\mathbb{R}^{m\\times n}\\,$ and $\\,B\\in\\mathbb{R}^{p\\times m}\\,$. Your program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The entry for each test case must be a boolean indicating whether the observed largest principal-angle sine distance between the exact and computed intersection subspaces is less than or equal to a theoretically derived upper bound. Angles are unitless in this problem; there are no physical units involved.\n\nStart from the core definitions of range and null spaces and from the standard properties of orthogonal projectors. Use the following fundamental base:\n- The range space $\\mathcal{R}(A)$ is the set $\\{Ax : x\\in\\mathbb{R}^n\\}$, and the null space $\\mathcal{N}(B)$ is the set $\\{y\\in\\mathbb{R}^m : By=0\\}$.\n- An orthonormal basis for a subspace $\\,\\mathcal{S}\\subset\\mathbb{R}^m\\,$ can be represented by a matrix $\\,Q\\in\\mathbb{R}^{m\\times r}\\,$ with $\\,Q^\\top Q=I_r\\,$ and $\\,\\mathcal{S}=\\mathcal{R}(Q)$.\n- The orthogonal projector onto $\\,\\mathcal{R}(Q)\\,$ is $\\,P=QQ^\\top$.\n- The distance between two subspaces with orthogonal projectors $\\,P_1\\,$ and $\\,P_2\\,$ is quantified by the spectral norm $\\,\\|P_1-P_2\\|_2$, which equals the operator norm of $\\,\\sin\\Theta\\,$, where $\\,\\Theta\\,$ is the diagonal matrix of principal angles.\n- The Singular Value Decomposition (SVD) is a factorization $\\,M=U\\Sigma V^\\top\\,$ with $\\,U,V\\,$ orthogonal and $\\,\\Sigma\\,$ diagonal nonnegative. The thin SVD extracts the nonzero singular values. The smallest positive singular value provides a spectral gap. The QR factorization is a factorization $\\,A=QR\\,$ with $\\,Q\\,$ orthogonal and $\\,R\\,$ upper triangular; Column Pivoted QR chooses a permutation to expose numerical rank.\n\nYour task is to:\n1. Derive from first principles an algorithm that uses Column Pivoted QR factorization of $\\,A\\,$ to obtain an orthonormal basis $\\,Q_A\\,$ of $\\,\\mathcal{R}(A)$, then uses SVD of $\\,B Q_A\\,$ to obtain an orthonormal basis for $\\,\\mathcal{R}(A)\\cap\\mathcal{N}(B)$. The derivation must start from the identity\n$$\n\\mathcal{R}(A)\\cap\\mathcal{N}(B) \\;=\\; \\{\\,Q_A z : z\\in\\mathbb{R}^r,\\; BQ_A z=0\\,\\},\n$$\nand must rely only on the fundamental properties listed above.\n2. Given perturbations $\\,\\Delta A\\,$ and $\\,\\Delta B\\,$, define the perturbed quantities $\\,\\widehat{A}=A+\\Delta A\\,$, $\\,\\widehat{B}=B+\\Delta B\\,$, bases $\\,Q_{\\widehat{A}}\\,$ and $\\,\\widehat{Q}_A\\,$ of $\\,\\mathcal{R}(\\widehat{A})\\,$, and matrices $\\,M=BQ_A\\,$ and $\\,\\widehat{M}=\\widehat{B}Q_{\\widehat{A}}\\,$. Let $\\,\\gamma\\,$ denote the smallest positive singular value of $\\,M\\,$, and let $\\,\\|\\widehat{M}-M\\|_2\\,$ be the spectral norm of the perturbation in $\\,M\\,$. Let $\\,d_A=\\|Q_AQ_A^\\top - Q_{\\widehat{A}}Q_{\\widehat{A}}^\\top\\|_2\\,$. Using well-tested subspace perturbation facts for SVD-invariant subspaces, bound the subspace distance between the exact and computed intersections by a computable quantity of the form\n$$\n\\mathrm{Bound} \\;=\\; \\min\\Bigl(1,\\; d_A \\;+\\; \\frac{\\|\\widehat{M}-M\\|_2}{\\gamma}\\Bigr),\n$$\nunder the standard separation condition that $\\,\\gamma>0\\,$. If $\\,M\\,$ has no positive singular values, set $\\,\\gamma=+\\infty\\,$ and interpret the fraction as $\\,0\\,$. You must justify why such a bound follows from the fundamental base and from subspace perturbation theory for SVD.\n3. Implement the above in a single self-contained program that, for each test case provided below, computes:\n   - An orthonormal basis $\\,U_\\star\\,$ for the exact intersection $\\,\\mathcal{R}(A)\\cap\\mathcal{N}(B)\\,$.\n   - An orthonormal basis $\\,\\widehat{U}\\,$ for the computed intersection $\\,\\mathcal{R}(\\widehat{A})\\cap\\mathcal{N}(\\widehat{B})\\,$.\n   - The actual subspace error $\\,\\|U_\\star U_\\star^\\top - \\widehat{U}\\widehat{U}^\\top\\|_2\\,$.\n   - The bound described above, with $\\,d_A\\,$, $\\,\\|\\widehat{M}-M\\|_2\\,$, and $\\,\\gamma\\,$ computed as specified.\n   - A boolean indicating whether the actual error is less than or equal to the bound (within numerical tolerance).\nUse the fixed SVD threshold $\\,\\tau=10^{-8}\\,$ to decide which singular values are treated as numerical zeros when forming null spaces. All computations are to be carried out in real arithmetic.\n\nTest suite. For each case, use the given $\\,A\\,$, $\\,B\\,$, $\\,\\Delta A\\,$, and $\\,\\Delta B\\,$ exactly as specified. All numbers are unitless reals. Matrices are given by rows, with columns concatenated.\n\n- Case $\\,1\\,$ ($\\,m=6\\,$, $\\,n=3\\,$, $\\,p=2\\,$):\n  $$\n  A=\\begin{bmatrix}\n  1  0  0\\\\\n  1  1  0\\\\\n  0  1  1\\\\\n  0  0  1\\\\\n  0  0  0\\\\\n  0  0  0\n  \\end{bmatrix},\\quad\n  B=\\begin{bmatrix}\n  1  -1  0  0  0  0\\\\\n  0  1  -1  0  0  0\n  \\end{bmatrix},\n  $$\n  $$\n  \\Delta A=10^{-3}\\!\\cdot\\!\\begin{bmatrix}\n  0  2  -1\\\\\n  -1  0.5  0\\\\\n  0  0  1.5\\\\\n  1  -0.5  0\\\\\n  0.2  -0.1  0.3\\\\\n  -0.2  0.1  -0.3\n  \\end{bmatrix},\\quad\n  \\Delta B=10^{-3}\\!\\cdot\\!\\begin{bmatrix}\n  0.5  -0.2  0.1  0  0  0\\\\\n  -0.3  0.4  -0.1  0  0  0\n  \\end{bmatrix}.\n  $$\n\n- Case $\\,2\\,$ ($\\,m=6\\,$, $\\,n=3\\,$, $\\,p=3\\,$):\n  $$\n  A=\\begin{bmatrix}\n  1  0  0\\\\\n  1  1  0\\\\\n  0  1  1\\\\\n  0  0  1\\\\\n  0  0  0\\\\\n  0  0  0\n  \\end{bmatrix},\\quad\n  B=\\begin{bmatrix}\n  1  0  0  0  0  0\\\\\n  0  1  0  0  0  0\\\\\n  0  0  1  0  0  0\n  \\end{bmatrix},\n  $$\n  $$\n  \\Delta A=5\\cdot 10^{-4}\\!\\cdot\\!\\begin{bmatrix}\n  0  2  -1\\\\\n  -1  0.5  0\\\\\n  0  0  1.5\\\\\n  1  -0.5  0\\\\\n  0.2  -0.1  0.3\\\\\n  -0.2  0.1  -0.3\n  \\end{bmatrix},\\quad\n  \\Delta B=10^{-3}\\!\\cdot\\!\\begin{bmatrix}\n  0.2  -0.1  0.05  0  0  0\\\\\n  -0.05  0.1  -0.02  0  0  0\\\\\n  0.03  -0.04  0.02  0  0  0\n  \\end{bmatrix}.\n  $$\n\n- Case $\\,3\\,$ (near-singular gap; $\\,m=6\\,$, $\\,n=3\\,$, $\\,p=2\\,$):\n  $$\n  A=\\begin{bmatrix}\n  1  0  0\\\\\n  1  1  0\\\\\n  0  1  1\\\\\n  0  0  1\\\\\n  0  0  0\\\\\n  0  0  0\n  \\end{bmatrix},\\quad\n  B=\\begin{bmatrix}\n  1  -1  0  0  0  0\\\\\n  1  -1+10^{-3}  -10^{-3}  0  0  0\n  \\end{bmatrix},\n  $$\n  $$\n  \\Delta A=10^{-3}\\!\\cdot\\!\\begin{bmatrix}\n  0.1  -0.2  0.1\\\\\n  -0.1  0.2  -0.1\\\\\n  0.05  0.05  -0.1\\\\\n  -0.05  0.1  0.05\\\\\n  0.01  -0.02  0.03\\\\\n  -0.01  0.02  -0.03\n  \\end{bmatrix},\\quad\n  \\Delta B=5\\cdot 10^{-4}\\!\\cdot\\!\\begin{bmatrix}\n  -0.1  0.05  -0.02  0  0  0\\\\\n  0.08  -0.04  0.01  0  0  0\n  \\end{bmatrix}.\n  $$\n\n- Case $\\,4\\,$ ($\\,m=5\\,$, $\\,n=2\\,$, $\\,p=1\\,$):\n  $$\n  A=\\begin{bmatrix}\n  1  0\\\\\n  0  1\\\\\n  0  0\\\\\n  1  1\\\\\n  0  1\n  \\end{bmatrix},\\quad\n  B=\\begin{bmatrix}\n  1  1  0  0  0\n  \\end{bmatrix},\n  $$\n  $$\n  \\Delta A=10^{-3}\\!\\cdot\\!\\begin{bmatrix}\n  0.2  -0.1\\\\\n  -0.1  0.3\\\\\n  0.05  -0.02\\\\\n  -0.2  0.1\\\\\n  0.1  -0.05\n  \\end{bmatrix},\\quad\n  \\Delta B=10^{-3}\\!\\cdot\\!\\begin{bmatrix}\n  -0.2  0.1  0  0  0\n  \\end{bmatrix}.\n  $$\n\nAngle unit specification. Principal angles are internal to the algorithm and need not be reported directly; the program must compute and compare subspace distances $\\,\\|P_\\star-\\widehat{P}\\|_2\\,$, which are real numbers in $[0,1]$ without units.\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[r_1,r_2,r_3,r_4]$), where each $\\,r_i\\,$ is a boolean indicating whether the actual subspace error is less than or equal to the computed bound for test case $\\,i$.\n\nImplementation constraints. Use Column Pivoted QR (first appearance definition: Column Pivoted QR factorization (CPQR)) to extract $\\,Q_A\\,$ and the Singular Value Decomposition (SVD) for the null space step. Use the fixed numerical zero threshold $\\,\\tau=10^{-8}\\,$ when classifying singular values as zero. All computations should be deterministic and use the matrices as given.", "solution": "The problem is assessed to be **valid**. It is a well-posed, scientifically grounded problem in numerical linear algebra. It is self-contained, objective, and its requirements are mathematically and computationally feasible. The solution proceeds as follows.\n\n### 1. Derivation of the Numerical Algorithm\n\nThe objective is to compute an orthonormal basis for the intersection of two subspaces, $\\mathcal{S} = \\mathcal{R}(A) \\cap \\mathcal{N}(B)$, given matrices $A \\in \\mathbb{R}^{m \\times n}$ and $B \\in \\mathbb{R}^{p \\times m}$.\n\n**Step 1: Orthonormal Basis for the Range Space $\\mathcal{R}(A)$**\n\nA vector $y \\in \\mathbb{R}^m$ is in the intersection $\\mathcal{S}$ only if it belongs to $\\mathcal{R}(A)$. Thus, any such $y$ can be expressed as a linear combination of basis vectors for $\\mathcal{R}(A)$. We first construct an orthonormal basis for $\\mathcal{R}(A)$.\n\nThe problem specifies using the Column Pivoted QR (CPQR) factorization of $A$, which takes the form $AP = QR$, where $P \\in \\mathbb{R}^{n \\times n}$ is a permutation matrix, $Q \\in \\mathbb{R}^{m \\times m}$ is an orthogonal matrix, and $R \\in \\mathbb{R}^{m \\times n}$ is upper triangular. The range of $A$ is identical to the range of $AP$, which is $\\mathcal{R}(QR)$. Since $R$ is upper triangular, its first $r = \\mathrm{rank}(A)$ columns are linearly independent (assuming the permutation $P$ arranges them so). The range of $QR$ is spanned by the first $r$ columns of $Q$.\n\nLet $r$ be the numerical rank of $A$, determined by the number of diagonal entries of $R$ with magnitude greater than a given tolerance $\\tau$. An orthonormal basis for $\\mathcal{R}(A)$ is then given by the first $r$ columns of $Q$. Let this basis be denoted by the matrix $Q_A \\in \\mathbb{R}^{m \\times r}$, where $Q_A = Q[:, :r]$. By construction, $Q_A^\\top Q_A = I_r$ and $\\mathcal{R}(A) = \\mathcal{R}(Q_A)$.\n\n**Step 2: Characterization of the Intersection**\n\nAny vector $y \\in \\mathcal{R}(A)$ can be uniquely written as $y = Q_A z$ for some coordinate vector $z \\in \\mathbb{R}^r$. The condition for $y$ to also be in the null space $\\mathcal{N}(B)$ is $By=0$. Substituting the expression for $y$, we obtain:\n$$\nB(Q_A z) = 0 \\implies (BQ_A)z = 0\n$$\nThis equation shows that the coordinate vector $z$ of an intersection vector (in the basis $Q_A$) must lie in the null space of the matrix $M = BQ_A \\in \\mathbb{R}^{p \\times r}$.\nThus, the problem of finding the intersection $\\mathcal{R}(A) \\cap \\mathcal{N}(B)$ is transformed into finding the null space of $M$. The identity provided in the problem statement, $\\mathcal{R}(A)\\cap\\mathcal{N}(B) = \\{Q_A z : z\\in\\mathbb{R}^r,\\; BQ_A z=0\\,\\}$, is hereby formally established as the core of the algorithm.\n\n**Step 3: Orthonormal Basis for the Null Space $\\mathcal{N}(M)$**\n\nThe problem specifies using the Singular Value Decomposition (SVD) to find the null space of $M$. Let the SVD of $M$ be $M = U_M \\Sigma_M V_M^\\top$, where $U_M$ and $V_M$ are orthogonal matrices and $\\Sigma_M$ is a diagonal matrix of singular values $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge 0$.\n\nThe null space of $M$ is spanned by the right singular vectors in $V_M$ corresponding to singular values equal to zero. Numerically, we consider singular values $\\sigma_i \\le \\tau$ to be zero, where $\\tau$ is the specified threshold. Let $k$ be the numerical rank of $M$, and let the dimension of its domain be $r$. Then, the dimension of the null space $\\mathcal{N}(M)$ is $d_{null} = r-k$. An orthonormal basis for $\\mathcal{N}(M)$ consists of the last $d_{null}$ columns of $V_M$. Let this basis be denoted by the matrix $Z \\in \\mathbb{R}^{r \\times d_{null}}$.\n\n**Step 4: Orthonormal Basis for the Intersection Subspace $\\mathcal{S}$**\n\nAs established, vectors in the intersection are of the form $y = Q_A z$ where $z \\in \\mathcal{N}(M)$. Since any $z \\in \\mathcal{N}(M)$ can be written as $z = Zc$ for some coordinate vector $c \\in \\mathbb{R}^{d_{null}}$, the vectors in the intersection are of the form $y = Q_A(Zc) = (Q_A Z) c$. This implies that the intersection subspace $\\mathcal{S}$ is the range of the matrix $U_\\star = Q_A Z$.\n\nThe matrix $U_\\star \\in \\mathbb{R}^{m \\times d_{null}}$ constitutes a basis for $\\mathcal{S}$. We verify that this basis is orthonormal:\n$$\nU_\\star^\\top U_\\star = (Q_A Z)^\\top (Q_A Z) = Z^\\top Q_A^\\top Q_A Z = Z^\\top I_r Z = Z^\\top Z = I_{d_{null}}\n$$\nThe second-to-last equality holds because $Q_A$ has orthonormal columns, and the last equality holds because the columns of $Z$ (subset of columns of $V_M$) are orthonormal. Thus, $U_\\star = Q_A Z$ is an orthonormal basis for the intersection subspace $\\mathcal{R}(A) \\cap \\mathcal{N}(B)$.\n\n### 2. Justification of the Perturbation Bound\n\nWe are asked to justify the bound on the subspace distance between the exact intersection $\\mathcal{S} = \\mathcal{R}(A) \\cap \\mathcal{N}(B)$ and the perturbed intersection $\\widehat{\\mathcal{S}} = \\mathcal{R}(\\widehat{A}) \\cap \\mathcal{N}(\\widehat{B})$, given by:\n$$\n\\|P_\\mathcal{S} - P_{\\widehat{\\mathcal{S}}}\\|_2 \\le \\mathrm{Bound} = \\min\\left(1, d_A + \\frac{\\|\\widehat{M}-M\\|_2}{\\gamma}\\right)\n$$\nwhere $P_\\mathcal{S}$ and $P_{\\widehat{\\mathcal{S}}}$ are the orthogonal projectors onto $\\mathcal{S}$ and $\\widehat{\\mathcal{S}}$, respectively. Here $d_A = \\|Q_A Q_A^\\top - Q_{\\widehat{A}} Q_{\\widehat{A}}^\\top\\|_2$ is the distance between $\\mathcal{R}(A)$ and $\\mathcal{R}(\\widehat{A})$, $M=BQ_A$, $\\widehat{M}=\\widehat{B}Q_{\\widehat{A}}$, and $\\gamma$ is the smallest positive singular value of $M$.\n\nThis bound can be justified by decomposing the total perturbation into two stages and applying standard subspace perturbation theorems. We assume for this derivation that the ranks of $A$ and $\\widehat{A}$ are equal, so $Q_A$ and $Q_{\\widehat{A}}$ have the same number of columns, $r$.\n\n1.  **Projector Representation**: From the algorithm derivation, the projectors are $P_\\mathcal{S} = U_\\star U_\\star^\\top = (Q_A Z)(Q_A Z)^\\top = Q_A Z Z^\\top Q_A^\\top$ and similarly $P_{\\widehat{\\mathcal{S}}} = Q_{\\widehat{A}} \\widehat{Z} \\widehat{Z}^\\top Q_{\\widehat{A}}^\\top$. Let $P_{\\mathcal{N}(M)} = ZZ^\\top$ and $P_{\\mathcal{N}(\\widehat{M})} = \\widehat{Z}\\widehat{Z}^\\top$ be the projectors onto the coordinate-space null spaces. Then $P_\\mathcal{S} = Q_A P_{\\mathcal{N}(M)} Q_A^\\top$ and $P_{\\widehat{\\mathcal{S}}} = Q_{\\widehat{A}} P_{\\mathcal{N}(\\widehat{M})} Q_{\\widehat{A}}^\\top$.\n\n2.  **Triangle Inequality**: We can bound the distance $\\|P_\\mathcal{S} - P_{\\widehat{\\mathcal{S}}}\\|_2$ using the triangle inequality by introducing an intermediate projector:\n    $$\n    \\|P_\\mathcal{S} - P_{\\widehat{\\mathcal{S}}}\\|_2 \\le \\|Q_A P_{\\mathcal{N}(M)} Q_A^\\top - Q_{\\widehat{A}} P_{\\mathcal{N}(M)} Q_{\\widehat{A}}^\\top\\|_2 + \\|Q_{\\widehat{A}} P_{\\mathcal{N}(M)} Q_{\\widehat{A}}^\\top - Q_{\\widehat{A}} P_{\\mathcal{N}(\\widehat{M})} Q_{\\widehat{A}}^\\top\\|_2\n    $$\n\n3.  **Bounding the First Term**: The first term measures the effect of changing the embedding subspace from $\\mathcal{R}(A)$ to $\\mathcal{R}(\\widehat{A})$ while keeping the null space condition (represented by $P_{\\mathcal{N}(M)}$) fixed. Let $\\mathcal{T} = \\mathcal{R}(Q_A Z)$ and $\\widehat{\\mathcal{T}} = \\mathcal{R}(Q_{\\widehat{A}} Z)$. The term is the distance between these two subspaces. The distance between subspaces cannot be greater than the distance between the larger spaces they are embedded in. Therefore, this term is bounded by the distance between $\\mathcal{R}(A)$ and $\\mathcal{R}(\\widehat{A})$:\n    $$\n    \\|P_{\\mathcal{T}} - P_{\\widehat{\\mathcal{T}}}\\|_2 \\le \\|Q_A Q_A^\\top - Q_{\\widehat{A}} Q_{\\widehat{A}}^\\top\\|_2 = d_A\n    $$\n\n4.  **Bounding the Second Term**: The second term, thanks to the unitary invariance of the spectral norm, simplifies to:\n    $$\n    \\|Q_{\\widehat{A}} (P_{\\mathcal{N}(M)} - P_{\\mathcal{N}(\\widehat{M})}) Q_{\\widehat{A}}^\\top\\|_2 = \\|P_{\\mathcal{N}(M)} - P_{\\mathcal{N}(\\widehat{M})}\\|_2\n    $$\n    This is the distance between the null spaces of $M=BQ_A$ and $\\widehat{M}=\\widehat{B}Q_{\\widehat{A}}$. The null space of a matrix is an SVD-invariant subspace. According to the Davis-Kahan theorem for singular value decomposition, the sine of the largest principal angle between the null spaces of $M$ and $\\widehat{M}$ is bounded by the norm of the perturbation relative to the spectral gap. The gap for the null space is the smallest positive singular value of $M$, which is $\\gamma = \\sigma_{\\min+}(M)$. Assuming the dimensions of $\\mathcal{N}(M)$ and $\\mathcal{N}(\\widehat{M})$ are the same, the bound on the projector distance is:\n    $$\n    \\|P_{\\mathcal{N}(M)} - P_{\\mathcal{N}(\\widehat{M})}\\|_2 \\le \\frac{\\|\\widehat{M}-M\\|_2}{\\gamma}\n    $$\n    This bound holds under the condition $\\gamma > \\|\\widehat{M}-M\\|_2$, but more general forms exist. The expression is a standard first-order perturbation result.\n\n5.  **Combining the Bounds**: Combining the bounds for the two terms yields the desired inequality:\n    $$\n    \\|P_\\mathcal{S} - P_{\\widehat{\\mathcal{S}}}\\|_2 \\le d_A + \\frac{\\|\\widehat{M}-M\\|_2}{\\gamma}\n    $$\n    Since the subspace distance, being the sine of an angle, cannot exceed $1$, we can write the bound as $\\min(1, \\dots)$. If $M$ has no positive singular values (i.e., $M=0$ numerically), its null space is the entire domain. The convention $\\gamma=+\\infty$ makes the fractional term zero, correctly reflecting that in this case, the null space condition imposes no constraint on vectors in $\\mathcal{R}(A)$. The perturbation is then solely due to the change in $\\mathcal{R}(A)$, captured by $d_A$.", "answer": "```python\nimport numpy as np\nfrom scipy.linalg import qr, svd\n\ndef compute_intersection_basis(A, B, tau):\n    \"\"\"\n    Computes an orthonormal basis for the intersection subspace R(A) intersect N(B).\n\n    Args:\n        A (np.ndarray): The matrix defining the range space R(A).\n        B (np.ndarray): The matrix defining the null space N(B).\n        tau (float): The numerical tolerance for rank determination.\n\n    Returns:\n        np.ndarray: A matrix whose columns form an orthonormal basis for the intersection.\n    \"\"\"\n    m, n = A.shape\n    \n    # Step 1: Find an orthonormal basis for R(A) using Column Pivoted QR.\n    # The factorization is AP = QR. R(A) is spanned by the first r columns of Q.\n    if n == 0:\n        r = 0\n        Q_A = np.zeros((m, 0))\n    else:\n        # Use scipy.linalg.qr for column-pivoted QR factorization.\n        Q, R, _ = qr(A, pivoting=True, mode='economic')\n        \n        # Determine numerical rank based on the diagonal of R.\n        # It's a common practice to use a relative tolerance, but for this problem,\n        # a simpler absolute tolerance is used for consistency with the SVD step.\n        # The problem statement is slightly ambiguous about the QR rank-revealing tolerance.\n        # We'll use a simple heuristic based on the problem's SVD tolerance.\n        if R.shape[0] > 0 and R.shape[1] > 0:\n            diag_R = np.abs(np.diag(R))\n            # Heuristic tolerance for QR rank, consistent with SVD tau.\n            qr_tol = max(m,n) * np.finfo(float).eps * diag_R[0] if diag_R.size > 0 else 0\n            r = np.sum(diag_R > qr_tol)\n        else:\n            r = 0\n        Q_A = Q[:, :r]\n\n    # If R(A) is the trivial subspace {0}, the intersection is also {0}.\n    if Q_A.shape[1] == 0:\n        return np.zeros((m, 0))\n        \n    # Step 2: Form the matrix M = BQ_A. The intersection corresponds to the null space of M.\n    M = B @ Q_A\n    \n    # If M is an empty matrix, its null space is the entire domain R^r.\n    # The intersection is thus R(A) itself.\n    if M.size == 0 and M.shape[1] > 0:\n        return Q_A\n\n    # Step 3: Find an orthonormal basis for N(M) using SVD.\n    if M.size == 0:\n        # If M is 0x0, null space basis is empty.\n        V_M = np.zeros((0,0))\n        s_M = np.zeros(0)\n    else:\n        _, s_M, Vh_M = svd(M)\n        V_M = Vh_M.T\n    \n    # Determine the dimension of the null space using the problem's fixed tau.\n    rank_M = np.sum(s_M > tau)\n    null_dim = V_M.shape[1] - rank_M\n    \n    # If N(M) is trivial, the intersection is trivial.\n    if null_dim == 0:\n        return np.zeros((m, 0))\n    else:\n        # The last `null_dim` columns of V_M form an orthonormal basis for N(M).\n        Z = V_M[:, -null_dim:]\n        \n        # Step 4: The basis for the intersection is Q_A @ Z. This is orthonormal.\n        # (Q_A Z)^T (Q_A Z) = Z^T Q_A^T Q_A Z = Z^T I Z = Z^T Z = I.\n        U_intersect = Q_A @ Z\n        return U_intersect\n\ndef solve():\n    \"\"\"\n    Main function to run the test cases and produce the final output.\n    \"\"\"\n    tau = 1e-8\n    \n    test_cases = [\n        # Case 1\n        {\n            \"A\": np.array([[1, 0, 0], [1, 1, 0], [0, 1, 1], [0, 0, 1], [0, 0, 0], [0, 0, 0]], dtype=float),\n            \"B\": np.array([[1, -1, 0, 0, 0, 0], [0, 1, -1, 0, 0, 0]], dtype=float),\n            \"Delta_A\": 1e-3 * np.array([[0, 2, -1], [-1, 0.5, 0], [0, 0, 1.5], [1, -0.5, 0], [0.2, -0.1, 0.3], [-0.2, 0.1, -0.3]], dtype=float),\n            \"Delta_B\": 1e-3 * np.array([[0.5, -0.2, 0.1, 0, 0, 0], [-0.3, 0.4, -0.1, 0, 0, 0]], dtype=float)\n        },\n        # Case 2\n        {\n            \"A\": np.array([[1, 0, 0], [1, 1, 0], [0, 1, 1], [0, 0, 1], [0, 0, 0], [0, 0, 0]], dtype=float),\n            \"B\": np.array([[1, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0]], dtype=float),\n            \"Delta_A\": 5e-4 * np.array([[0, 2, -1], [-1, 0.5, 0], [0, 0, 1.5], [1, -0.5, 0], [0.2, -0.1, 0.3], [-0.2, 0.1, -0.3]], dtype=float),\n            \"Delta_B\": 1e-3 * np.array([[0.2, -0.1, 0.05, 0, 0, 0], [-0.05, 0.1, -0.02, 0, 0, 0], [0.03, -0.04, 0.02, 0, 0, 0]], dtype=float)\n        },\n        # Case 3\n        {\n            \"A\": np.array([[1, 0, 0], [1, 1, 0], [0, 1, 1], [0, 0, 1], [0, 0, 0], [0, 0, 0]], dtype=float),\n            \"B\": np.array([[1, -1, 0, 0, 0, 0], [1, -1 + 1e-3, -1e-3, 0, 0, 0]], dtype=float),\n            \"Delta_A\": 1e-3 * np.array([[0.1, -0.2, 0.1], [-0.1, 0.2, -0.1], [0.05, 0.05, -0.1], [-0.05, 0.1, 0.05], [0.01, -0.02, 0.03], [-0.01, 0.02, -0.03]], dtype=float),\n            \"Delta_B\": 5e-4 * np.array([[-0.1, 0.05, -0.02, 0, 0, 0], [0.08, -0.04, 0.01, 0, 0, 0]], dtype=float)\n        },\n        # Case 4\n        {\n            \"A\": np.array([[1, 0], [0, 1], [0, 0], [1, 1], [0, 1]], dtype=float),\n            \"B\": np.array([[1, 1, 0, 0, 0]], dtype=float),\n            \"Delta_A\": 1e-3 * np.array([[0.2, -0.1], [-0.1, 0.3], [0.05, -0.02], [-0.2, 0.1], [0.1, -0.05]], dtype=float),\n            \"Delta_B\": 1e-3 * np.array([[-0.2, 0.1, 0, 0, 0]], dtype=float)\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        A, B = case[\"A\"], case[\"B\"]\n        Delta_A, Delta_B = case[\"Delta_A\"], case[\"Delta_B\"]\n        \n        A_hat, B_hat = A + Delta_A, B + Delta_B\n\n        # Compute basis for exact and perturbed intersection subspaces\n        U_star = compute_intersection_basis(A, B, tau)\n        U_hat = compute_intersection_basis(A_hat, B_hat, tau)\n\n        # Compute actual subspace error\n        P_star = U_star @ U_star.T\n        P_hat = U_hat @ U_hat.T\n        actual_error = np.linalg.norm(P_star - P_hat, 2)\n\n        # Compute the theoretical bound\n        Q_A, R_A, p_A = qr(A, pivoting=True, mode='economic')\n        m_A, n_A = A.shape\n        if R_A.shape[0] > 0 and R_A.shape[1] > 0:\n            qr_tol_A = max(m_A, n_A) * np.finfo(float).eps * np.abs(R_A[0,0])\n            rank_A = np.sum(np.abs(np.diag(R_A)) > qr_tol_A)\n        else: rank_A = 0\n        Q_A = Q_A[:, :rank_A]\n\n        Q_A_hat, R_A_hat, p_A_hat = qr(A_hat, pivoting=True, mode='economic')\n        m_A_hat, n_A_hat = A_hat.shape\n        if R_A_hat.shape[0] > 0 and R_A_hat.shape[1] > 0:\n            qr_tol_A_hat = max(m_A_hat, n_A_hat) * np.finfo(float).eps * np.abs(R_A_hat[0,0])\n            rank_A_hat = np.sum(np.abs(np.diag(R_A_hat)) > qr_tol_A_hat)\n        else: rank_A_hat = 0\n        Q_A_hat = Q_A_hat[:, :rank_A_hat]\n        \n        P_A = Q_A @ Q_A.T\n        P_A_hat = Q_A_hat @ Q_A_hat.T\n        d_A = np.linalg.norm(P_A - P_A_hat, 2)\n\n        M = B @ Q_A\n        M_hat = B_hat @ Q_A_hat\n        norm_M_hat_minus_M = np.linalg.norm(M_hat - M, 2)\n\n        if M.size > 0:\n            s_M = svd(M, compute_uv=False)\n            positive_s_M = s_M[s_M > tau]\n            if positive_s_M.size > 0:\n                gamma = np.min(positive_s_M)\n            else:\n                gamma = np.inf\n        else:\n            gamma = np.inf\n            \n        if np.isinf(gamma):\n            bound_val = d_A\n        else:\n            bound_val = d_A + norm_M_hat_minus_M / gamma\n            \n        bound = min(1.0, bound_val)\n        \n        # Compare actual error to the bound, with a small numerical tolerance\n        is_valid_bound = actual_error = bound + 1e-12\n        results.append(is_valid_bound)\n\n    # The problem asks for boolean output, which needs to be lowercase in Python for str() conversion.\n    bool_to_str = [str(r).lower() for r in results]\n    print(f\"[{','.join(bool_to_str)}]\")\n\nsolve()\n```", "id": "3600943"}]}