{"hands_on_practices": [{"introduction": "This first practice is a foundational exercise designed to build intuition through direct computation. You will verify that for a Hermitian matrix, the general Arnoldi iteration simplifies into the compact three-term recurrence of the Lanczos iteration [@problem_id:3573190]. By explicitly calculating the basis vectors and the entries of the projected Hessenberg and tridiagonal matrices, you will see this equivalence firsthand and confirm that the algorithms produce identical results.", "problem": "Let $A \\in \\mathbb{C}^{5 \\times 5}$ be Hermitian, meaning $A = A^{*}$ where $^{*}$ denotes conjugate transpose. Consider the concrete choice\n$$\nA \\;=\\; \\begin{pmatrix}\n2 & 1 & 0 & 0 & 0 \\\\\n1 & 3 & 1 & 0 & 0 \\\\\n0 & 1 & 4 & 1 & 0 \\\\\n0 & 0 & 1 & 3 & 1 \\\\\n0 & 0 & 0 & 1 & 2\n\\end{pmatrix},\n$$\nand the initial unit vector $v_{1} = e_{1} = (1,0,0,0,0)^{\\top}$. Using only the core definitions of the Arnoldi process and the Lanczos process for constructing orthonormal Krylov bases, perform exactly two Arnoldi iterations (to obtain $v_{2}$ and $v_{3}$) and two Lanczos steps (to obtain $v_{2}$ and $v_{3}$). Explicitly compute all relevant scalar coefficients that arise in these constructions (in Arnoldi: the Hessenberg entries $h_{11}$, $h_{21}$, $h_{12}$, $h_{22}$, $h_{32}$; in Lanczos: the tridiagonal entries $\\alpha_{1}$, $\\beta_{1}$, $\\alpha_{2}$, $\\beta_{2}$). Then, determine the unimodular scalar $c \\in \\mathbb{C}$ (that is, $|c| = 1$) such that $v_{3}^{\\mathrm{Arnoldi}} = c \\, v_{3}^{\\mathrm{Lanczos}}$.\n\nProvide your final answer as the exact value of $c$ (no rounding required).", "solution": "We perform two steps of both the Arnoldi and Lanczos algorithms starting with the vector $v_1 = e_1$. Since the matrix $A$ is real and symmetric, it is Hermitian.\n\n**1. Arnoldi Iteration**\n\nThe Arnoldi process generates an orthonormal basis $\\{v_j\\}$ and a Hessenberg matrix $H$ with entries $h_{ij}$.\n*   **Step j=1 (to find $v_2$):**\n    *   Start with $v_1 = [1, 0, 0, 0, 0]^{\\top}$.\n    *   Compute $w = Av_1 = [2, 1, 0, 0, 0]^{\\top}$.\n    *   Orthogonalize against $v_1$:\n        *   $h_{11} = v_1^{\\top}w = 2$.\n        *   The new vector is $\\tilde{v}_2 = w - h_{11}v_1 = [2, 1, 0, 0, 0]^{\\top} - 2[1, 0, 0, 0, 0]^{\\top} = [0, 1, 0, 0, 0]^{\\top}$.\n    *   Normalize to find $v_2$:\n        *   $h_{21} = \\|\\tilde{v}_2\\|_2 = 1$.\n        *   $v_2 = \\tilde{v}_2 / h_{21} = [0, 1, 0, 0, 0]^{\\top} = e_2$.\n\n*   **Step j=2 (to find $v_3$):**\n    *   Compute $w = Av_2 = [1, 3, 1, 0, 0]^{\\top}$.\n    *   Orthogonalize against $v_1$ and $v_2$:\n        *   $h_{12} = v_1^{\\top}w = 1$.\n        *   $h_{22} = v_2^{\\top}w = 3$.\n        *   The new vector is $\\tilde{v}_3 = w - h_{12}v_1 - h_{22}v_2 = [1, 3, 1, 0, 0]^{\\top} - 1e_1 - 3e_2 = [0, 0, 1, 0, 0]^{\\top}$.\n    *   Normalize to find $v_3$:\n        *   $h_{32} = \\|\\tilde{v}_3\\|_2 = 1$.\n        *   $v_3^{\\mathrm{Arnoldi}} = \\tilde{v}_3 / h_{32} = [0, 0, 1, 0, 0]^{\\top} = e_3$.\n\nThe calculated Arnoldi coefficients are $h_{11}=2$, $h_{21}=1$, $h_{12}=1$, $h_{22}=3$, and $h_{32}=1$.\n\n**2. Lanczos Iteration**\n\nThe Lanczos iteration generates an orthonormal basis $\\{v_j\\}$ and tridiagonal matrix entries $\\alpha_j, \\beta_j$.\n*   **Step j=1 (to find $v_2$):**\n    *   Start with $v_1 = e_1$. Let $\\beta_0=0$ and $v_0=0$.\n    *   $\\alpha_1 = v_1^{\\top}Av_1 = e_1^{\\top}[2, 1, 0, 0, 0]^{\\top} = 2$.\n    *   $\\tilde{v}_2 = Av_1 - \\alpha_1 v_1 - \\beta_0 v_0 = [2, 1, 0, 0, 0]^{\\top} - 2e_1 = [0, 1, 0, 0, 0]^{\\top}$.\n    *   $\\beta_1 = \\|\\tilde{v}_2\\|_2 = 1$.\n    *   $v_2 = \\tilde{v}_2 / \\beta_1 = e_2$.\n\n*   **Step j=2 (to find $v_3$):**\n    *   $\\alpha_2 = v_2^{\\top}Av_2 = e_2^{\\top}[1, 3, 1, 0, 0]^{\\top} = 3$.\n    *   $\\tilde{v}_3 = Av_2 - \\alpha_2 v_2 - \\beta_1 v_1 = [1, 3, 1, 0, 0]^{\\top} - 3e_2 - 1e_1 = [0, 0, 1, 0, 0]^{\\top}$.\n    *   $\\beta_2 = \\|\\tilde{v}_3\\|_2 = 1$.\n    *   $v_3^{\\mathrm{Lanczos}} = \\tilde{v}_3 / \\beta_2 = e_3$.\n\nThe calculated Lanczos coefficients are $\\alpha_1=2, \\beta_1=1, \\alpha_2=3$, and $\\beta_2=1$.\n\n**3. Comparison and Conclusion**\n\nThe calculations show that the basis vectors generated by both standard processes are identical at each step:\n$$ v_3^{\\mathrm{Arnoldi}} = e_3 $$\n$$ v_3^{\\mathrm{Lanczos}} = e_3 $$\nThe equation $v_{3}^{\\mathrm{Arnoldi}} = c \\cdot v_{3}^{\\mathrm{Lanczos}}$ becomes $e_3 = c \\cdot e_3$. This implies that the unimodular scalar $c$ must be $1$. This result is expected, as the standard Arnoldi algorithm mathematically simplifies to the standard Lanczos algorithm when applied to a Hermitian matrix.", "answer": "$$\\boxed{1}$$", "id": "3573190"}, {"introduction": "Moving from mechanics to theory, this practice delves into the condition known as a \"lucky breakdown,\" where the iteration terminates before reaching the full dimension of the space. You will establish the fundamental connection between the degree of the minimal polynomial of the matrix with respect to the starting vector and the dimension of the Krylov subspace [@problem_id:3573197]. This exercise illuminates the geometric reason for termination: the Krylov subspace has become an invariant subspace under the action of the matrix.", "problem": "Let $A \\in \\mathbb{R}^{6 \\times 6}$ be the symmetric matrix\n$$\nA \\;=\\; \\operatorname{diag}\\big(0,\\,1,\\,2,\\,2,\\,2,\\,2\\big),\n$$\nand let $v_{1} \\in \\mathbb{R}^{6}$ be the unit vector\n$$\nv_{1} \\;=\\; \\frac{1}{\\sqrt{3}}\\big(e_{1} + e_{2} + e_{3}\\big),\n$$\nwhere $\\{e_{j}\\}_{j=1}^{6}$ denotes the standard basis of $\\mathbb{R}^{6}$. Consider applying the Arnoldi iteration to $(A,v_{1})$ in exact arithmetic.\n\nUsing only fundamental definitions from numerical linear algebra (definitions of Krylov subspace, minimal polynomial relative to a vector, and the Arnoldi relation), do the following:\n\n1. Starting from the definition of the Krylov subspace $\\mathcal{K}_{m}(A,v_{1}) = \\operatorname{span}\\{v_{1}, A v_{1}, \\dots, A^{m-1} v_{1}\\}$ and the definition of the monic minimal polynomial $p$ of $(A,v_{1})$ as the unique monic polynomial of least degree such that $p(A)v_{1} = 0$, argue why the Arnoldi iteration must terminate with a zero residual exactly when the dimension of $\\mathcal{K}_{m}(A,v_{1})$ stops increasing, and why this stopping index equals the degree of $p$.\n\n2. Using only the symmetry of $A$ and orthonormality of the Arnoldi basis $\\{v_{j}\\}$, deduce why the projected matrix $H_{m} = V_{m}^{\\top} A V_{m}$ is symmetric and upper Hessenberg, hence tridiagonal, and therefore the Arnoldi relation reduces to a three-term recurrence (the Lanczos relation) up to termination.\n\n3. Determine the monic minimal polynomial $p$ of $(A,v_{1})$ explicitly. Your final answer must be this polynomial $p(x)$ written as a single closed-form analytic expression.\n\nNo numerical rounding is required. Provide your final answer as a single analytic expression.", "solution": "The problem is addressed in three parts as requested.\n\n**1. Relationship between Arnoldi Termination, Krylov Subspace Dimension, and Minimal Polynomial Degree**\n\nLet $\\mathcal{K}_{m}(A,v_{1}) = \\operatorname{span}\\{v_{1}, A v_{1}, \\dots, A^{m-1} v_{1}\\}$ be the order-$m$ Krylov subspace generated by the matrix $A \\in \\mathbb{R}^{n \\times n}$ and the starting vector $v_{1} \\in \\mathbb{R}^{n}$. The dimension of this subspace, $\\dim(\\mathcal{K}_{m}(A,v_{1}))$, is a non-decreasing function of $m$. The dimension stops increasing at the first integer $m=d$ where $\\dim(\\mathcal{K}_{d+1}(A,v_{1})) = \\dim(\\mathcal{K}_{d}(A,v_{1}))$. This occurs when the vector $A^{d}v_{1}$ is linearly dependent on the preceding vectors $\\{v_{1}, A v_{1}, \\dots, A^{d-1} v_{1}\\}$. This linear dependence implies the existence of coefficients $c_0, c_1, \\dots, c_{d-1}$ such that:\n$$\nA^{d}v_{1} = -c_{d-1} A^{d-1} v_{1} - \\dots - c_1 A v_{1} - c_0 v_{1}\n$$\nThis can be rearranged to $(A^{d} + c_{d-1} A^{d-1} + \\dots + c_1 A + c_0 I)v_{1} = 0$.\n\nBy definition, the monic minimal polynomial $p(x)$ of $(A,v_{1})$ is the unique monic polynomial of least degree such that $p(A)v_{1} = 0$. The condition above shows that there exists a monic polynomial of degree $d$, namely $p(x) = x^d + c_{d-1}x^{d-1} + \\dots + c_0$, that annihilates $v_1$. Since $d$ is the *first* such integer for which this linear dependence occurs, the set $\\{v_{1}, A v_{1}, \\dots, A^{d-1}v_{1}\\}$ is linearly independent. Thus, no non-zero polynomial of degree less than $d$ can annihilate $v_1$. Consequently, $d$ is the degree of the minimal polynomial $p(x)$. Therefore, the degree of the minimal polynomial of $(A,v_{1})$ is precisely the integer $d$ at which the dimension of the Krylov subspace $\\mathcal{K}_{m}(A,v_{1})$ stabilizes.\n\nThe Arnoldi iteration constructs an orthonormal basis $\\{v_{1}, v_{2}, \\dots, v_{m}\\}$ for $\\mathcal{K}_{m}(A,v_{1})$. The core step of the algorithm at step $m$ is to compute a new vector $\\tilde{v}_{m+1}$ by orthogonalizing $Av_m$ against the existing basis vectors $\\{v_1, \\dots, v_m\\}$:\n$$\n\\tilde{v}_{m+1} = Av_{m} - \\sum_{i=1}^{m} h_{i,m} v_{i}, \\quad \\text{where} \\quad h_{i,m} = v_{i}^{\\top} A v_{m}\n$$\nIf $\\tilde{v}_{m+1} \\neq 0$, it is normalized to obtain $v_{m+1} = \\tilde{v}_{m+1} / h_{m+1,m}$, where $h_{m+1,m} = \\|\\tilde{v}_{m+1}\\|_2$. The iteration terminates at step $m$ if and only if $\\tilde{v}_{m+1}=0$. This is often termed a \"lucky breakdown\".\n\nIf $\\tilde{v}_{m+1}=0$, it means that $Av_{m} = \\sum_{i=1}^{m} h_{i,m} v_{i}$. This shows that $Av_{m}$ lies within the subspace $\\mathcal{K}_{m}(A,v_{1}) = \\operatorname{span}\\{v_1, \\dots, v_m\\}$. Since each $v_j \\in \\mathcal{K}_j(A,v_1) \\subseteq \\mathcal{K}_m(A,v_1)$, this implies that for any vector $u \\in \\mathcal{K}_{m}(A,v_{1})$, the vector $Au$ also lies in $\\mathcal{K}_{m}(A,v_{1})$. In other words, $\\mathcal{K}_{m}(A,v_{1})$ becomes an invariant subspace of $A$. This implies that $A^m v_1 \\in \\mathcal{K}_m(A,v_1)$, which means $A^m v_1$ is a linear combination of $\\{v_{1}, A v_{1}, \\dots, A^{m-1}v_{1}\\}$. This is exactly the condition for the dimension of the Krylov subspace sequence to stop increasing. The stopping index $m$ is therefore equal to $d$, the degree of the minimal polynomial.\n\nIn summary, the Arnoldi iteration terminates at step $m=d$ because this is the first step at which the new candidate vector is a linear combination of the previous basis vectors. This occurs precisely when $\\mathcal{K}_{d}(A,v_{1})$ is an invariant subspace, which is equivalent to the dimension of the Krylov subspace sequence stabilizing at $d$. This index $d$, by definition, is the degree of the minimal polynomial of $(A,v_{1})$.\n\n**2. Derivation of the Lanczos Relation for Symmetric A**\n\nThe Arnoldi iteration generates an orthonormal matrix $V_m = [v_1 | v_2 | \\dots | v_m]$ and an upper Hessenberg matrix $H_m$ such that $V_m^\\top V_m = I_m$. The projected matrix $H_m$ is defined by its entries $h_{i,j} = v_{i}^{\\top} A v_{j}$. In matrix form, this is $H_m = V_m^{\\top} A V_m$.\n\nGiven that the matrix $A$ is symmetric, we have $A^{\\top} = A$. We can examine the symmetry of $H_m$ by taking its transpose:\n$$\nH_m^{\\top} = (V_m^{\\top} A V_m)^{\\top} = V_m^{\\top} A^{\\top} (V_m^{\\top})^{\\top} = V_m^{\\top} A V_m\n$$\nSince $A^{\\top}=A$ and $(V_m^{\\top})^{\\top}=V_m$, we find that $H_m^{\\top} = H_m$. Thus, the matrix $H_m$ is symmetric.\n\nBy construction, the Arnoldi process produces an upper Hessenberg matrix $H_m$, which means its entries satisfy $h_{i,j} = 0$ for all $i > j+1$. A matrix that is both symmetric and upper Hessenberg must be tridiagonal. Symmetry implies $h_{i,j} = h_{j,i}$. If we take $i > j+1$, the upper Hessenberg property implies $h_{i,j}=0$. The symmetry property then implies $h_{j,i}=0$ for $i > j+1$. Letting $k=j$ and $l=i$, this means $h_{k,l}=0$ for $l > k+1$, which means all entries above the first superdiagonal are zero. A matrix where all entries are zero except for those on the main diagonal, the first subdiagonal, and the first superdiagonal is, by definition, tridiagonal.\n\nThe full Arnoldi relation is $A V_m = V_m H_m + h_{m+1,m}v_{m+1}e_m^{\\top}$. Examining the $j$-th column of this matrix equation gives the vector recurrence:\n$$\nA v_j = \\sum_{i=1}^{j+1} h_{i,j} v_i\n$$\nSince $H_m$ is tridiagonal, the only non-zero coefficients $h_{i,j}$ are for $i \\in \\{j-1, j, j+1\\}$. Let us adopt the standard Lanczos notation: $\\alpha_j = h_{j,j}$ and $\\beta_j = h_{j-1,j}$. Due to symmetry, $h_{j-1,j} = h_{j,j-1}$. Let us denote the subdiagonal entries $h_{k+1,k}$ by $\\beta_k$. Then the superdiagonal entries are $h_{k,k+1} = \\beta_k$. The recurrence simplifies to:\n$$\nA v_j = h_{j-1,j} v_{j-1} + h_{j,j} v_j + h_{j+1,j} v_{j+1} = \\beta_{j-1} v_{j-1} + \\alpha_j v_j + \\beta_{j} v_{j+1}\n$$\n(with the convention $\\beta_0 v_0 = 0$). This is the three-term recurrence characteristic of the Lanczos iteration. Thus, for a symmetric matrix $A$, the Arnoldi iteration simplifies to the Lanczos iteration.\n\n**3. Determination of the Monic Minimal Polynomial of (A, v1)**\n\nWe seek the monic polynomial $p(x)$ of lowest degree $d$ such that $p(A)v_1=0$. This is equivalent to finding the smallest $d$ for which the set of vectors $\\{v_1, Av_1, \\dots, A^{d-1}v_1, A^d v_1\\}$ is linearly dependent. Let's compute these vectors.\n\nGiven $A = \\operatorname{diag}(0, 1, 2, 2, 2, 2)$ and $v_1 = \\frac{1}{\\sqrt{3}}(e_1 + e_2 + e_3)$.\nThe action of $A$ on the standard basis vectors is $Ae_1 = 0 \\cdot e_1 = 0$, $Ae_2 = 1 \\cdot e_2 = e_2$, and $Ae_3 = 2 \\cdot e_3 = 2e_3$.\n\nLet's compute the first few vectors in the Krylov sequence:\n- $v_1 = \\frac{1}{\\sqrt{3}}(e_1 + e_2 + e_3)$\n- $Av_1 = A \\frac{1}{\\sqrt{3}}(e_1 + e_2 + e_3) = \\frac{1}{\\sqrt{3}}(Ae_1 + Ae_2 + Ae_3) = \\frac{1}{\\sqrt{3}}(0 + e_2 + 2e_3) = \\frac{1}{\\sqrt{3}}(e_2 + 2e_3)$\n- $A^2v_1 = A(Av_1) = A \\frac{1}{\\sqrt{3}}(e_2 + 2e_3) = \\frac{1}{\\sqrt{3}}(Ae_2 + 2Ae_3) = \\frac{1}{\\sqrt{3}}(e_2 + 2(2e_3)) = \\frac{1}{\\sqrt{3}}(e_2 + 4e_3)$\n- $A^3v_1 = A(A^2v_1) = A \\frac{1}{\\sqrt{3}}(e_2 + 4e_3) = \\frac{1}{\\sqrt{3}}(Ae_2 + 4Ae_3) = \\frac{1}{\\sqrt{3}}(e_2 + 4(2e_3)) = \\frac{1}{\\sqrt{3}}(e_2 + 8e_3)$\n\nThe vectors $v_1, Av_1, A^2v_1, \\dots$ all lie in the subspace $\\operatorname{span}\\{e_1, e_2, e_3\\}$, which is $3$-dimensional. Therefore, any set of $4$ or more of these vectors must be linearly dependent. The degree of the minimal polynomial can be at most $3$.\n\nWe check for linear independence of $\\{v_1, Av_1, A^2v_1\\}$. Ignoring the common factor $\\frac{1}{\\sqrt{3}}$, we check the vectors $u_0 = e_1+e_2+e_3$, $u_1 = e_2+2e_3$, and $u_2 = e_2+4e_3$. Expressing them in the basis $\\{e_1, e_2, e_3\\}$, we form the matrix:\n$$\nM = \\begin{pmatrix} 1 & 0 & 0 \\\\ 1 & 1 & 1 \\\\ 1 & 2 & 4 \\end{pmatrix}\n$$\nThe determinant is $\\det(M) = 1 \\cdot (1 \\cdot 4 - 1 \\cdot 2) - 0 + 0 = 2 \\neq 0$. Thus, $\\{v_1, Av_1, A^2v_1\\}$ is a linearly independent set. The degree of the minimal polynomial is at least $3$.\n\nSince the degree is at least $3$ and at most $3$, it must be exactly $d=3$. The minimal polynomial is of the form $p(x) = x^3 + c_2 x^2 + c_1 x + c_0$. We find the coefficients by solving $p(A)v_1 = 0$, which is equivalent to $A^3v_1 + c_2 A^2v_1 + c_1 Av_1 + c_0 v_1 = 0$.\nSubstituting the computed vectors (and cancelling the factor $\\frac{1}{\\sqrt{3}}$):\n$$\n(e_2 + 8e_3) + c_2(e_2 + 4e_3) + c_1(e_2 + 2e_3) + c_0(e_1 + e_2 + e_3) = 0\n$$\nWe collect terms for each basis vector:\n- Coefficient of $e_1$: $c_0 = 0$\n- Coefficient of $e_2$: $1 + c_2 + c_1 + c_0 = 0$\n- Coefficient of $e_3$: $8 + 4c_2 + 2c_1 + c_0 = 0$\n\nWe have a system of linear equations for $c_0, c_1, c_2$:\n1. $c_0 = 0$\n2. $c_1 + c_2 = -1$\n3. $2c_1 + 4c_2 = -8 \\implies c_1 + 2c_2 = -4$\n\nSubtracting equation $(2)$ from equation $(3)$:\n$(c_1 + 2c_2) - (c_1 + c_2) = -4 - (-1)$\n$c_2 = -3$\n\nSubstituting $c_2 = -3$ into equation $(2)$:\n$c_1 + (-3) = -1 \\implies c_1 = 2$\n\nThe coefficients are $c_0 = 0$, $c_1 = 2$, and $c_2 = -3$. The monic minimal polynomial is therefore:\n$$\np(x) = x^3 - 3x^2 + 2x\n$$\nThis polynomial can be factored as $p(x) = x(x-1)(x-2)$. The roots are the eigenvalues of $A$ ($0, 1, 2$) corresponding to the eigenvectors ($e_1, e_2, e_3$) that have non-zero components in the expansion of $v_1$. This provides a conceptual verification of the result.", "answer": "$$\n\\boxed{x^3 - 3x^2 + 2x}\n$$", "id": "3573197"}, {"introduction": "Krylov subspace methods are central to modern computational science, particularly for solving large-scale eigenvalue problems. This final exercise demonstrates how the orthonormal basis generated by the Lanczos or Arnoldi process can be used to extract approximate eigen-information [@problem_id:3573179]. You will construct a \"refined Ritz vector\"—an optimal approximation within the subspace for an eigenvector near a specified target—and assess its quality by computing the residual norm, highlighting the practical utility of the generated subspace.", "problem": "Let $A \\in \\mathbb{R}^{4 \\times 4}$ be Hermitian with $A = \\operatorname{diag}(1, 2, 4, 8)$, and let $b \\in \\mathbb{R}^{4}$ be $b = [1, 1, 0, 0]^{\\top}$. Consider the Krylov subspace of order $k=2$, $\\mathcal{K}_{2}(A, b) = \\operatorname{span}\\{b, Ab\\}$. Let $V_{2} \\in \\mathbb{R}^{4 \\times 2}$ be any orthonormal basis for $\\mathcal{K}_{2}(A, b)$ produced by the Lanczos process, and let $Q_{2} \\in \\mathbb{R}^{4 \\times 2}$ be any orthonormal basis for the same subspace produced by the Arnoldi process starting from $b$. \n\nFor the target value $\\tau = 4$, define the refined Ritz vector in a subspace $\\mathcal{S}$ with orthonormal basis $W$ as the unit vector $w_{\\mathrm{ref}} \\in \\mathcal{S}$ that minimizes the residual norm $\\|(A - \\tau I) w\\|_{2}$ over all $w \\in \\mathcal{S}$ with $\\|w\\|_{2} = 1$. Using only fundamental definitions of Krylov subspace methods and minimization principles, do the following:\n- Construct the refined Ritz vectors in the subspace $\\mathcal{K}_{2}(A, b)$ using $V_{2}$ and using $Q_{2}$, and justify whether these two refined Ritz vectors in $\\mathbb{R}^{4}$ agree up to a complex unit.\n- Assess and compute exactly the common residual norm $\\|(A - \\tau I) w_{\\mathrm{ref}}\\|_{2}$ attained by these refined Ritz vectors.\n\nYour final answer must be the exact value of this common residual norm as a single real number. No rounding is required.", "solution": "The problem asks for the construction of refined Ritz vectors for a given matrix $A$ and starting vector $b$ in the Krylov subspace $\\mathcal{K}_{2}(A, b)$, and the computation of the associated minimal residual norm for a target $\\tau$.\n\nFirst, we characterize the Krylov subspace $\\mathcal{K}_{2}(A, b) = \\operatorname{span}\\{b, Ab\\}$.\nThe given matrix $A$ and vector $b$ are:\n$$A = \\operatorname{diag}(1, 2, 4, 8) \\in \\mathbb{R}^{4 \\times 4}$$\n$$b = [1, 1, 0, 0]^{\\top} \\in \\mathbb{R}^{4}$$\nThe matrix $A$ is real and diagonal, hence it is Hermitian as stated.\nWe compute the vector $Ab$:\n$$Ab = \\begin{pmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 2 & 0 & 0 \\\\ 0 & 0 & 4 & 0 \\\\ 0 & 0 & 0 & 8 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 2 \\\\ 0 \\\\ 0 \\end{pmatrix}$$\nThe Krylov subspace is $\\mathcal{K}_{2}(A, b) = \\operatorname{span}\\{[1, 1, 0, 0]^{\\top}, [1, 2, 0, 0]^{\\top}\\}$. Since $b$ and $Ab$ are linearly independent, the dimension of this subspace is $2$.\n\nThe problem refers to two orthonormal bases, $V_{2}$ from the Lanczos process and $Q_{2}$ from the Arnoldi process, for this same subspace $\\mathcal{K}_{2}(A, b)$. Since $A$ is Hermitian, the Arnoldi process simplifies to the Lanczos process. If both begin with the same normalized starting vector, they produce the same orthonormal basis (up to choices of sign for each vector). Let $W$ be a generic orthonormal basis for $\\mathcal{K}_{2}(A, b)$. We can construct one such basis using the Gram-Schmidt process on the vectors $\\{b, Ab\\}$.\nLet $v_1 = b = [1, 1, 0, 0]^{\\top}$. The first basis vector is $w_1$:\n$$w_1 = \\frac{v_1}{\\|v_1\\|_{2}} = \\frac{1}{\\sqrt{1^2 + 1^2 + 0^2 + 0^2}} \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix}$$\nLet $v_2 = Ab = [1, 2, 0, 0]^{\\top}$. The second orthogonal vector $\\tilde{w}_2$ is found by subtracting the projection of $v_2$ onto $w_1$:\n$$\\tilde{w}_2 = v_2 - (w_1^{\\top}v_2)w_1$$\nThe inner product is $w_1^{\\top}v_2 = \\frac{1}{\\sqrt{2}}[1, 1, 0, 0] [1, 2, 0, 0]^{\\top} = \\frac{1}{\\sqrt{2}}(1+2) = \\frac{3}{\\sqrt{2}}$.\n$$\\tilde{w}_2 = \\begin{pmatrix} 1 \\\\ 2 \\\\ 0 \\\\ 0 \\end{pmatrix} - \\frac{3}{\\sqrt{2}} \\left(\\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix}\\right) = \\begin{pmatrix} 1 \\\\ 2 \\\\ 0 \\\\ 0 \\end{pmatrix} - \\frac{3}{2} \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} -1/2 \\\\ 1/2 \\\\ 0 \\\\ 0 \\end{pmatrix}$$\nNormalizing $\\tilde{w}_2$ gives the second basis vector $w_2$:\n$$w_2 = \\frac{\\tilde{w}_2}{\\|\\tilde{w}_2\\|_{2}} = \\frac{1}{\\sqrt{(-1/2)^2 + (1/2)^2}} \\begin{pmatrix} -1/2 \\\\ 1/2 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\frac{1}{\\sqrt{1/2}} \\begin{pmatrix} -1/2 \\\\ 1/2 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\sqrt{2} \\begin{pmatrix} -1/2 \\\\ 1/2 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} -1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix}$$\nThus, an orthonormal basis for $\\mathcal{K}_{2}(A, b)$ is given by the columns of the matrix $W$:\n$$W = [w_1, w_2] = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 & -1 \\\\ 1 & 1 \\\\ 0 & 0 \\\\ 0 & 0 \\end{pmatrix}$$\nBoth $V_{2}$ and $Q_{2}$ are orthonormal bases for this subspace, so they are related by a $2 \\times 2$ orthogonal matrix.\n\nThe refined Ritz vector $w_{\\mathrm{ref}}$ is defined as the unit vector $w \\in \\mathcal{K}_{2}(A, b)$ that minimizes the residual norm $\\|(A - \\tau I) w\\|_{2}$, where the target is $\\tau = 4$.\nAny vector $w \\in \\mathcal{K}_{2}(A, b)$ can be written as $w = Wy$ for some coefficient vector $y \\in \\mathbb{R}^{2}$. The condition $\\|w\\|_{2} = 1$ implies $\\|Wy\\|_{2} = 1$. Since $W$ has orthonormal columns, $\\|y\\|_{2} = 1$.\nThe minimization problem is:\n$$\\min_{w \\in \\mathcal{K}_{2}(A, b), \\|w\\|_{2}=1} \\|(A - \\tau I) w\\|_{2}^2 = \\min_{y \\in \\mathbb{R}^2, \\|y\\|_{2}=1} \\|(A - \\tau I) Wy\\|_{2}^2$$\nThe objective function can be written as a quadratic form:\n$$\\|(A - \\tau I) Wy\\|_{2}^2 = ((A - \\tau I) Wy)^{\\top} ((A - \\tau I) Wy) = y^{\\top}W^{\\top}(A - \\tau I)^{\\top}(A - \\tau I)Wy$$\nSince $A$ is real and symmetric, $(A - \\tau I)^{\\top} = A - \\tau I$. The expression simplifies to:\n$$y^{\\top} \\left( W^{\\top} (A - \\tau I)^2 W \\right) y$$\nThis is a Rayleigh quotient for the matrix $M = W^{\\top} (A - \\tau I)^2 W$. The minimum value of this quotient is the smallest eigenvalue of $M$, and the vector $y$ that achieves this minimum is the corresponding eigenvector.\n\nLet's compute the matrix $M$. First, we find $(A - \\tau I)^2$ with $\\tau=4$:\n$$A - \\tau I = \\operatorname{diag}(1-4, 2-4, 4-4, 8-4) = \\operatorname{diag}(-3, -2, 0, 4)$$\n$$(A - \\tau I)^2 = \\operatorname{diag}((-3)^2, (-2)^2, 0^2, 4^2) = \\operatorname{diag}(9, 4, 0, 16)$$\nNow we compute $M = W^{\\top} (A - \\tau I)^2 W$:\n$$M = \\left( \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 & 1 & 0 & 0 \\\\ -1 & 1 & 0 & 0 \\end{pmatrix} \\right) \\begin{pmatrix} 9 & 0 & 0 & 0 \\\\ 0 & 4 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 16 \\end{pmatrix} \\left( \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 & -1 \\\\ 1 & 1 \\\\ 0 & 0 \\\\ 0 & 0 \\end{pmatrix} \\right)$$\n$$M = \\frac{1}{2} \\begin{pmatrix} 1 & 1 & 0 & 0 \\\\ -1 & 1 & 0 & 0 \\end{pmatrix} \\begin{pmatrix} 9 & -9 \\\\ 4 & 4 \\\\ 0 & 0 \\\\ 0 & 0 \\end{pmatrix} = \\frac{1}{2} \\begin{pmatrix} 1(9)+1(4) & 1(-9)+1(4) \\\\ -1(9)+1(4) & -1(-9)+1(4) \\end{pmatrix}$$\n$$M = \\frac{1}{2} \\begin{pmatrix} 13 & -5 \\\\ -5 & 13 \\end{pmatrix}$$\nThe eigenvalues $\\lambda$ of $M$ are found from $\\det(M - \\lambda I) = 0$:\n$$\\det \\begin{pmatrix} \\frac{13}{2} - \\lambda & -\\frac{5}{2} \\\\ -\\frac{5}{2} & \\frac{13}{2} - \\lambda \\end{pmatrix} = 0$$\n$$\\left(\\frac{13}{2} - \\lambda\\right)^2 - \\left(-\\frac{5}{2}\\right)^2 = 0 \\implies \\lambda^2 - 13\\lambda + \\frac{169}{4} - \\frac{25}{4} = 0$$\n$$\\lambda^2 - 13\\lambda + \\frac{144}{4} = 0 \\implies \\lambda^2 - 13\\lambda + 36 = 0$$\n$$(\\lambda - 4)(\\lambda - 9) = 0$$\nThe eigenvalues are $\\lambda_{1} = 4$ and $\\lambda_{2} = 9$. The minimum eigenvalue is $\\lambda_{\\min} = 4$.\nThe minimum value of the squared residual norm is $\\lambda_{\\min} = 4$. Therefore, the minimum residual norm is $\\sqrt{\\lambda_{\\min}} = \\sqrt{4} = 2$.\n\nThis result is independent of the choice of orthonormal basis for $\\mathcal{K}_{2}(A, b)$. If we choose another basis $W' = WU$, where $U$ is a $2 \\times 2$ orthogonal matrix, the projected matrix becomes $M' = (W')^{\\top}(A-\\tau I)^2 W' = (WU)^{\\top}(A-\\tau I)^2(WU) = U^{\\top}MU$. $M'$ is orthogonally similar to $M$ and thus has the same eigenvalues. Since the residual norm squared is the minimum eigenvalue of this matrix, its value is invariant under a change of orthonormal basis for the subspace. Hence, the residual norm will be the same whether computed using $V_2$ or $Q_2$.\n\nTo construct the refined Ritz vector $w_{\\mathrm{ref}}$, we find the eigenvector $y$ of $M$ corresponding to $\\lambda_{\\min} = 4$:\n$$(M - 4I)y = 0 \\implies \\begin{pmatrix} \\frac{13}{2}-4 & -\\frac{5}{2} \\\\ -\\frac{5}{2} & \\frac{13}{2}-4 \\end{pmatrix} y = 0 \\implies \\begin{pmatrix} \\frac{5}{2} & -\\frac{5}{2} \\\\ -\\frac{5}{2} & \\frac{5}{2} \\end{pmatrix} \\begin{pmatrix} y_1 \\\\ y_2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$$\nThis gives the equation $\\frac{5}{2}y_1 - \\frac{5}{2}y_2 = 0$, which implies $y_1=y_2$. The eigenvector is of the form $c[1, 1]^{\\top}$. For $\\|y\\|_2=1$, we have $c^2(1^2+1^2)=1 \\implies 2c^2=1 \\implies c = \\pm \\frac{1}{\\sqrt{2}}$.\nLet's choose $y = \\frac{1}{\\sqrt{2}}[1, 1]^{\\top}$.\nThe refined Ritz vector is $w_{\\mathrm{ref}} = Wy$:\n$$w_{\\mathrm{ref}} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 & -1 \\\\ 1 & 1 \\\\ 0 & 0 \\\\ 0 & 0 \\end{pmatrix} \\left( \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} \\right) = \\frac{1}{2} \\begin{pmatrix} 1-1 \\\\ 1+1 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\frac{1}{2} \\begin{pmatrix} 0 \\\\ 2 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix}$$\nThe choice $c = -\\frac{1}{\\sqrt{2}}$ would yield $-w_{\\mathrm{ref}}$. Since the eigenspace for $\\lambda_{\\min}$ is one-dimensional, the refined Ritz vector is unique up to a sign factor of $\\pm 1$. This is a real unit, which is a specific case of a complex unit with absolute value $1$. Therefore, the refined Ritz vectors constructed from $V_2$ and $Q_2$ must agree up to a factor of $\\pm 1$.\n\nThe problem asks for the common residual norm, which we have calculated to be $2$.\nLet's verify this with the computed $w_{\\mathrm{ref}}$:\n$$\\|(A - 4I) w_{\\mathrm{ref}}\\|_{2} = \\left\\| \\begin{pmatrix} -3 & 0 & 0 & 0 \\\\ 0 & -2 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 4 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix} \\right\\|_{2} = \\left\\| \\begin{pmatrix} 0 \\\\ -2 \\\\ 0 \\\\ 0 \\end{pmatrix} \\right\\|_{2} = \\sqrt{0^2 + (-2)^2 + 0^2 + 0^2} = 2$$\nThe calculation is consistent. The computed residual norm is exact.", "answer": "$$\\boxed{2}$$", "id": "3573179"}]}