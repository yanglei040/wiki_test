{"hands_on_practices": [{"introduction": "The most effective way to grasp the mechanics of the Lanczos iteration is to perform the calculations by hand on a small, manageable system. This first practice exercise ([@problem_id:3421772]) guides you through the initial steps of the algorithm, demonstrating how the three-term recurrence generates an orthonormal basis for the Krylov subspace and the corresponding tridiagonal matrix $T_k$. This fundamental process is the computational engine behind powerful iterative methods for solving linear systems and finding eigenvalues.", "problem": "Consider the linear system arising from the centered finite-difference discretization of the one-dimensional Poisson equation $-u''(x)=f(x)$ on the unit interval with homogeneous Dirichlet boundary conditions, where the mesh parameter scaling is absorbed into the right-hand side for simplicity. The resulting symmetric positive definite matrix is\n$$\nA=\\begin{pmatrix}\n2  -1  0 \\\\\n-1  2  -1 \\\\\n0  -1  2\n\\end{pmatrix},\n$$\nwhich is a standard model problem in Krylov subspace methods used for the numerical solution of partial differential equations. In the Minimum Residual method (MINRES), the Lanczos process generates an orthonormal basis for the Krylov subspace and a symmetric tridiagonal projection of $A$. Transpose-Free Quasi-Minimal Residual (TFQMR) is a related method for nonsymmetric problems, but here the focus is on the symmetric case relevant to MINRES.\n\nStarting from the initial guess $x_0=\\mathbf{0}$ and the right-hand side vector $b=\\begin{pmatrix}1 \\\\ 0 \\\\ 0\\end{pmatrix}$, define the initial residual $r_0=b-Ax_0$ and the first Lanczos basis vector $v_1=r_0/\\|r_0\\|$. Using the fundamental definitions of Krylov subspaces and the three-term recurrence that characterizes the Lanczos process on symmetric matrices, carry out exactly two Lanczos iterations to construct the orthonormal basis vectors $v_1$ and $v_2$, and assemble the $2\\times 2$ tridiagonal matrix $T_2$ whose entries are the projection coefficients generated by these iterations.\n\nAs a final scalar diagnostic related to the spectral compression that underpins MINRES, compute the determinant of the tridiagonal matrix $T_2$. Provide this determinant as a single exact real number. No rounding is required, and no units are involved. The answer must be given as a single real-valued number.", "solution": "The problem requires the computation of the determinant of a $2 \\times 2$ tridiagonal matrix $T_2$, which is generated by performing two iterations of the Lanczos process on a given symmetric matrix $A$. The process starts with a specific right-hand side vector $b$ and initial guess $x_0$.\n\nThe given matrix is:\n$$\nA=\\begin{pmatrix}\n2  -1  0 \\\\\n-1  2  -1 \\\\\n0  -1  2\n\\end{pmatrix}\n$$\nThe right-hand side vector is $b=\\begin{pmatrix}1 \\\\ 0 \\\\ 0\\end{pmatrix}$, and the initial guess is $x_0=\\mathbf{0}=\\begin{pmatrix}0 \\\\ 0 \\\\ 0\\end{pmatrix}$.\n\nThe Lanczos process is an iterative algorithm that generates an orthonormal basis $\\{v_j\\}$ for the Krylov subspace $\\mathcal{K}_k(A, r_0) = \\text{span}\\{r_0, Ar_0, \\dots, A^{k-1}r_0\\}$ and a symmetric tridiagonal matrix $T_k$ whose entries are the coefficients of a three-term recurrence relation. The standard algorithm proceeds as follows:\n\n1.  Initialize: Calculate the initial residual $r_0 = b - Ax_0$. Set $\\beta_1 = \\|r_0\\|_2$ and $v_1 = r_0 / \\beta_1$. Set $v_0 = \\mathbf{0}$.\n2.  Iterate for $j=1, 2, \\dots$:\n    a. Compute $w_j = A v_j$.\n    b. Compute the diagonal coefficient $\\alpha_j = v_j^T w_j$.\n    c. Compute the unnormalized next vector $\\tilde{v}_{j+1} = w_j - \\alpha_j v_j - \\beta_j v_{j-1}$.\n    d. Compute the off-diagonal coefficient $\\beta_{j+1} = \\|\\tilde{v}_{j+1}\\|_2$.\n    e. If $\\beta_{j+1} \\neq 0$, normalize to get the next basis vector $v_{j+1} = \\tilde{v}_{j+1} / \\beta_{j+1}$.\n\nWe will carry out this process for two iterations to find the coefficients $\\alpha_1$, $\\beta_2$, and $\\alpha_2$, which form the matrix $T_2$.\n\n**Initialization Step:**\n\nFirst, we compute the initial residual $r_0$:\n$$\nr_0 = b - Ax_0 = \\begin{pmatrix}1 \\\\ 0 \\\\ 0\\end{pmatrix} - \\begin{pmatrix} 2  -1  0 \\\\ -1  2  -1 \\\\ 0  -1  2 \\end{pmatrix} \\begin{pmatrix}0 \\\\ 0 \\\\ 0\\end{pmatrix} = \\begin{pmatrix}1 \\\\ 0 \\\\ 0\\end{pmatrix}\n$$\nNext, we compute the Euclidean norm of $r_0$ to find $\\beta_1$ and then normalize $r_0$ to get $v_1$:\n$$\n\\beta_1 = \\|r_0\\|_2 = \\sqrt{1^2 + 0^2 + 0^2} = 1\n$$\n$$\nv_1 = \\frac{r_0}{\\beta_1} = \\frac{1}{1} \\begin{pmatrix}1 \\\\ 0 \\\\ 0\\end{pmatrix} = \\begin{pmatrix}1 \\\\ 0 \\\\ 0\\end{pmatrix}\n$$\nThe problem statement defines $v_1=r_0/\\|r_0\\|$, which is consistent with our initialization.\n\n**First Lanczos Iteration ($j=1$):**\n\nWe compute $w_1 = Av_1$:\n$$\nw_1 = Av_1 = \\begin{pmatrix} 2  -1  0 \\\\ -1  2  -1 \\\\ 0  -1  2 \\end{pmatrix} \\begin{pmatrix}1 \\\\ 0 \\\\ 0\\end{pmatrix} = \\begin{pmatrix}2 \\\\ -1 \\\\ 0\\end{pmatrix}\n$$\nThe first diagonal coefficient $\\alpha_1$ is the projection of $w_1$ onto $v_1$:\n$$\n\\alpha_1 = v_1^T w_1 = \\begin{pmatrix}1  0  0\\end{pmatrix} \\begin{pmatrix}2 \\\\ -1 \\\\ 0\\end{pmatrix} = (1)(2) + (0)(-1) + (0)(0) = 2\n$$\nNow we compute the unnormalized vector for the next step. For $j=1$, the recurrence is $\\tilde{v}_2 = w_1 - \\alpha_1 v_1 - \\beta_1 v_0$. Since $v_0 = \\mathbf{0}$, this simplifies:\n$$\n\\tilde{v}_2 = w_1 - \\alpha_1 v_1 = \\begin{pmatrix}2 \\\\ -1 \\\\ 0\\end{pmatrix} - 2 \\begin{pmatrix}1 \\\\ 0 \\\\ 0\\end{pmatrix} = \\begin{pmatrix}0 \\\\ -1 \\\\ 0\\end{pmatrix}\n$$\nThe first off-diagonal coefficient $\\beta_2$ is the norm of this vector:\n$$\n\\beta_2 = \\|\\tilde{v}_2\\|_2 = \\sqrt{0^2 + (-1)^2 + 0^2} = 1\n$$\nSince $\\beta_2 \\neq 0$, we find the second orthonormal basis vector $v_2$:\n$$\nv_2 = \\frac{\\tilde{v}_2}{\\beta_2} = \\frac{1}{1} \\begin{pmatrix}0 \\\\ -1 \\\\ 0\\end{pmatrix} = \\begin{pmatrix}0 \\\\ -1 \\\\ 0\\end{pmatrix}\n$$\n\n**Second Lanczos Iteration ($j=2$):**\n\nWe compute $w_2 = Av_2$:\n$$\nw_2 = Av_2 = \\begin{pmatrix} 2  -1  0 \\\\ -1  2  -1 \\\\ 0  -1  2 \\end{pmatrix} \\begin{pmatrix}0 \\\\ -1 \\\\ 0\\end{pmatrix} = \\begin{pmatrix}1 \\\\ -2 \\\\ 1\\end{pmatrix}\n$$\nThe second diagonal coefficient $\\alpha_2$ is the projection of $w_2$ onto $v_2$:\n$$\n\\alpha_2 = v_2^T w_2 = \\begin{pmatrix}0  -1  0\\end{pmatrix} \\begin{pmatrix}1 \\\\ -2 \\\\ 1\\end{pmatrix} = (0)(1) + (-1)(-2) + (0)(1) = 2\n$$\nThe problem only requires the construction of $T_2$, so we do not need to compute $\\beta_3$ or $v_3$.\n\n**Constructing and Evaluating $T_2$:**\n\nThe $k \\times k$ tridiagonal matrix $T_k$ generated by the Lanczos process is given by:\n$$\nT_k = \\begin{pmatrix}\n\\alpha_1  \\beta_2   \\\\\n\\beta_2  \\alpha_2  \\ddots  \\\\\n  \\ddots  \\ddots  \\beta_k \\\\\n   \\beta_k  \\alpha_k\n\\end{pmatrix}\n$$\nFor $k=2$, using the coefficients we calculated ($\\alpha_1=2$, $\\beta_2=1$, $\\alpha_2=2$), the matrix $T_2$ is:\n$$\nT_2 = \\begin{pmatrix}\n\\alpha_1  \\beta_2 \\\\\n\\beta_2  \\alpha_2\n\\end{pmatrix} = \\begin{pmatrix}\n2  1 \\\\\n1  2\n\\end{pmatrix}\n$$\nThe final step is to compute the determinant of $T_2$:\n$$\n\\det(T_2) = (\\alpha_1)(\\alpha_2) - (\\beta_2)^2 = (2)(2) - (1)^2 = 4 - 1 = 3\n$$\nThe determinant of the tridiagonal matrix $T_2$ is $3$.", "answer": "$$\\boxed{3}$$", "id": "3421772"}, {"introduction": "While the Lanczos iteration typically proceeds for many steps, it can terminate prematurely under specific conditions. This phenomenon, often called a \"lucky breakdown,\" is not a failure but a revelation about the underlying structure of the problem. This exercise ([@problem_id:3246897]) explores the scenario where the starting vector lies within an invariant subspace of the matrix, demonstrating why the algorithm stops early and how the resulting Ritz values perfectly capture the eigenvalues associated with that subspace.", "problem": "Consider a real symmetric matrix $A \\in \\mathbb{R}^{4 \\times 4}$ and a nonzero starting vector $b \\in \\mathbb{R}^{4}$ given by\n$$\nA \\;=\\; \\begin{pmatrix}\n4  0  0  0 \\\\\n0  6  0  0 \\\\\n0  0  2  0 \\\\\n0  0  0  11\n\\end{pmatrix},\n\\qquad\nb \\;=\\; \\begin{pmatrix}\n2 \\\\\n1 \\\\\n0 \\\\\n0\n\\end{pmatrix}.\n$$\nThe subspace $\\mathcal{S} \\;=\\; \\mathrm{span}\\{e_{1},e_{2}\\}$ is an invariant subspace of $A$, and $b \\in \\mathcal{S}$. Using the fundamental definitions of the Krylov subspace $\\mathcal{K}_{k}(A,b)$ and the Lanczos method for eigenvalue approximation on symmetric matrices, apply the Lanczos iteration starting from $b$ to construct the first few orthonormal Lanczos vectors and the associated symmetric tridiagonal projection $T$ until premature termination occurs due to invariance. Then determine the largest Ritz value, defined as the largest eigenvalue of the tridiagonal matrix $T$ produced at termination.\n\nProvide your final answer as a single real-valued number. No rounding is required; give the exact value. Do not include units.", "solution": "The Lanczos method for symmetric matrices constructs an orthonormal basis $\\{q_{1}, q_{2}, \\dots\\}$ for the Krylov subspaces $\\mathcal{K}_{k}(A,b) \\;=\\; \\mathrm{span}\\{b, Ab, \\dots, A^{k-1}b\\}$ via the three-term recurrence. At each step, it forms a symmetric tridiagonal matrix $T$ whose entries are the recurrence coefficients $\\alpha_{j}$ and $\\beta_{j}$, and the eigenvalues of $T$ are the Ritz values that approximate eigenvalues of $A$.\n\nWe start with the invariant subspace observation. Since $A$ is diagonal,\n$$\nA \\;=\\; \\mathrm{diag}(4,6,2,11),\n$$\nthe subspace $\\mathcal{S} \\;=\\; \\mathrm{span}\\{e_{1},e_{2}\\}$ is invariant because $A e_{1} \\in \\mathcal{S}$ and $A e_{2} \\in \\mathcal{S}$. Because $b \\;=\\; \\begin{pmatrix}2 \\\\ 1 \\\\ 0 \\\\ 0\\end{pmatrix}$ lies in $\\mathcal{S}$, every vector in $\\mathcal{K}_{k}(A,b)$ remains in $\\mathcal{S}$. Therefore, the maximal dimension of the Krylov sequence is at most $2$, and the Lanczos process will terminate prematurely with $\\beta_{2} \\;=\\; 0$.\n\nWe now carry out the iteration explicitly. Normalize $b$ to obtain\n$$\n\\|b\\| \\;=\\; \\sqrt{2^{2} + 1^{2} + 0^{2} + 0^{2}} \\;=\\; \\sqrt{5}, \\qquad q_{1} \\;=\\; \\frac{b}{\\|b\\|} \\;=\\; \\frac{1}{\\sqrt{5}} \\begin{pmatrix} 2 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix}.\n$$\nStep $1$ of Lanczos:\n- Compute $v \\;=\\; A q_{1}$:\n$$\nA q_{1} \\;=\\; \\frac{1}{\\sqrt{5}} \\begin{pmatrix} 4 \\cdot 2 \\\\ 6 \\cdot 1 \\\\ 2 \\cdot 0 \\\\ 11 \\cdot 0 \\end{pmatrix}\n\\;=\\; \\frac{1}{\\sqrt{5}} \\begin{pmatrix} 8 \\\\ 6 \\\\ 0 \\\\ 0 \\end{pmatrix}.\n$$\n- Compute the first diagonal coefficient $\\alpha_{1} \\;=\\; q_{1}^{\\top} A q_{1}$:\n$$\n\\alpha_{1} \\;=\\; q_{1}^{\\top} A q_{1}\n\\;=\\; \\frac{1}{5} \\left(2 \\cdot 8 + 1 \\cdot 6\\right)\n\\;=\\; \\frac{22}{5}.\n$$\n- Form the residual $r_{1} \\;=\\; A q_{1} - \\alpha_{1} q_{1}$:\n$$\nr_{1} \\;=\\; \\frac{1}{\\sqrt{5}} \\begin{pmatrix} 8 \\\\ 6 \\\\ 0 \\\\ 0 \\end{pmatrix}\n\\;-\\; \\frac{22}{5} \\cdot \\frac{1}{\\sqrt{5}} \\begin{pmatrix} 2 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix}\n\\;=\\; \\frac{1}{\\sqrt{5}} \\begin{pmatrix} 8 - \\frac{44}{5} \\\\ 6 - \\frac{22}{5} \\\\ 0 \\\\ 0 \\end{pmatrix}\n\\;=\\; \\frac{1}{5 \\sqrt{5}} \\begin{pmatrix} -4 \\\\ 8 \\\\ 0 \\\\ 0 \\end{pmatrix}.\n$$\n- The subdiagonal coefficient $\\beta_{1} \\;=\\; \\|r_{1}\\|$:\n$$\n\\beta_{1} \\;=\\; \\left\\| \\frac{1}{5 \\sqrt{5}} \\begin{pmatrix} -4 \\\\ 8 \\\\ 0 \\\\ 0 \\end{pmatrix} \\right\\|\n\\;=\\; \\frac{1}{5 \\sqrt{5}} \\sqrt{(-4)^{2} + 8^{2}}\n\\;=\\; \\frac{1}{5 \\sqrt{5}} \\sqrt{16 + 64}\n\\;=\\; \\frac{1}{5 \\sqrt{5}} \\cdot 4 \\sqrt{5}\n\\;=\\; \\frac{4}{5}.\n$$\n- Normalize to get $q_{2} \\;=\\; r_{1} / \\beta_{1}$:\n$$\nq_{2} \\;=\\; \\frac{ \\frac{1}{5 \\sqrt{5}} \\begin{pmatrix} -4 \\\\ 8 \\\\ 0 \\\\ 0 \\end{pmatrix} }{ \\frac{4}{5} }\n\\;=\\; \\frac{1}{\\sqrt{5}} \\begin{pmatrix} -1 \\\\ 2 \\\\ 0 \\\\ 0 \\end{pmatrix}.\n$$\n\nStep $2$ of Lanczos:\n- Compute $v \\;=\\; A q_{2} - \\beta_{1} q_{1}$:\n$$\nA q_{2} \\;=\\; \\frac{1}{\\sqrt{5}} \\begin{pmatrix} 4 \\cdot (-1) \\\\ 6 \\cdot 2 \\\\ 2 \\cdot 0 \\\\ 11 \\cdot 0 \\end{pmatrix}\n\\;=\\; \\frac{1}{\\sqrt{5}} \\begin{pmatrix} -4 \\\\ 12 \\\\ 0 \\\\ 0 \\end{pmatrix},\n$$\n$$\n\\beta_{1} q_{1} \\;=\\; \\frac{4}{5} \\cdot \\frac{1}{\\sqrt{5}} \\begin{pmatrix} 2 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix}\n\\;=\\; \\frac{1}{\\sqrt{5}} \\begin{pmatrix} \\frac{8}{5} \\\\ \\frac{4}{5} \\\\ 0 \\\\ 0 \\end{pmatrix},\n$$\n$$\nv \\;=\\; \\frac{1}{\\sqrt{5}} \\begin{pmatrix} -4 - \\frac{8}{5} \\\\ 12 - \\frac{4}{5} \\\\ 0 \\\\ 0 \\end{pmatrix}\n\\;=\\; \\frac{1}{\\sqrt{5}} \\cdot \\frac{28}{5} \\begin{pmatrix} -1 \\\\ 2 \\\\ 0 \\\\ 0 \\end{pmatrix}.\n$$\n- Compute $\\alpha_{2} \\;=\\; q_{2}^{\\top} v$:\n$$\n\\alpha_{2} \\;=\\; q_{2}^{\\top} v\n\\;=\\; \\left(\\frac{1}{\\sqrt{5}} \\begin{pmatrix} -1 \\\\ 2 \\\\ 0 \\\\ 0 \\end{pmatrix}\\right)^{\\!\\top}\n\\left( \\frac{1}{\\sqrt{5}} \\cdot \\frac{28}{5} \\begin{pmatrix} -1 \\\\ 2 \\\\ 0 \\\\ 0 \\end{pmatrix} \\right)\n\\;=\\; \\frac{1}{5} \\cdot \\frac{28}{5} \\left( (-1)^{2} + 2^{2} \\right)\n\\;=\\; \\frac{28}{5}.\n$$\n- Form the residual $r_{2} \\;=\\; v - \\alpha_{2} q_{2}$. Since $v \\;=\\; \\frac{28}{5} q_{2}$, we have $r_{2} \\;=\\; 0$, and thus\n$$\n\\beta_{2} \\;=\\; \\|r_{2}\\| \\;=\\; 0.\n$$\nThis is the premature termination caused by the starting vector $b$ lying in the invariant subspace $\\mathcal{S}$.\n\nThe tridiagonal matrix $T$ at termination is\n$$\nT \\;=\\; \\begin{pmatrix}\n\\alpha_{1}  \\beta_{1} \\\\\n\\beta_{1}  \\alpha_{2}\n\\end{pmatrix}\n\\;=\\;\n\\begin{pmatrix}\n\\frac{22}{5}  \\frac{4}{5} \\\\\n\\frac{4}{5}  \\frac{28}{5}\n\\end{pmatrix}.\n$$\nThe Ritz values are the eigenvalues of $T$, found by solving\n$$\n\\det\\!\\left( T - \\lambda I \\right) \\;=\\; 0\n\\;\\;\\Longleftrightarrow\\;\\;\n\\left(\\frac{22}{5} - \\lambda\\right)\\!\\left(\\frac{28}{5} - \\lambda\\right) - \\left(\\frac{4}{5}\\right)^{2} \\;=\\; 0.\n$$\nExpanding gives\n$$\n\\lambda^{2} - \\left( \\frac{22}{5} + \\frac{28}{5} \\right) \\lambda + \\frac{22}{5} \\cdot \\frac{28}{5} - \\frac{16}{25}\n\\;=\\; \\lambda^{2} - 10 \\lambda + 24 \\;=\\; 0,\n$$\nwhose roots are\n$$\n\\lambda \\;=\\; \\frac{10 \\pm \\sqrt{10^{2} - 4 \\cdot 24}}{2}\n\\;=\\; \\frac{10 \\pm \\sqrt{4}}{2}\n\\;=\\; \\frac{10 \\pm 2}{2}\n\\;\\in\\; \\{4, 6\\}.\n$$\nTherefore, the largest Ritz value produced at termination is $6$.", "answer": "$$\\boxed{6}$$", "id": "3246897"}, {"introduction": "The previous exercises operate in the idealized world of exact arithmetic. In practice, when implemented on a computer, the Lanczos process suffers from a critical loss of orthogonality among its basis vectors due to finite-precision floating-point errors, corrupting the resulting Ritz values. This final practice ([@problem_id:3581514]) is a coding-based exploration of this numerical instability, challenging you to implement and evaluate a powerful remedy known as selective reorthogonalization to restore the algorithm's stability and accuracy.", "problem": "Design and implement a program that investigates the stability of the symmetric Lanczos process under finite precision arithmetic by applying a selective reorthogonalization strategy to maintain orthogonality among Krylov basis vectors and quantifying its effect on the accuracy of Ritz values. Your program must implement three variants of the Lanczos iteration for real symmetric matrices: no reorthogonalization, selective reorthogonalization, and full reorthogonalization. The empirical comparison must adhere to the following specifications.\n\nFundamental base and assumptions:\n- Use the standard floating-point model of rounding for binary64 arithmetic: for any basic operation, $\\mathrm{fl}(x \\circ y) = (x \\circ y) (1 + \\delta)$ with $|\\delta| \\leq \\epsilon_{\\mathrm{mach}}$, where $\\epsilon_{\\mathrm{mach}}$ denotes machine epsilon for double precision.\n- The Krylov subspace of order $k$ for a real symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$ and a starting vector $v_{1}$ with $\\|v_{1}\\|_{2} = 1$ is $\\mathcal{K}_{k}(A, v_{1}) = \\mathrm{span}\\{v_{1}, A v_{1}, \\dots, A^{k-1} v_{1} \\}$. The Lanczos process attempts to generate an orthonormal basis $\\{v_{1}, \\dots, v_{k}\\}$ for $\\mathcal{K}_{k}(A, v_{1})$, producing a real symmetric tridiagonal matrix $T_{k} \\in \\mathbb{R}^{k \\times k}$ whose eigenvalues are the Ritz values with respect to $(A, \\mathcal{K}_{k})$.\n- In exact arithmetic, the Lanczos vectors are orthonormal and a $3$-term recurrence suffices. In finite precision arithmetic, loss of orthogonality may occur; reorthogonalization can mitigate this instability.\n\nSelective reorthogonalization directive:\n- Implement a selective reorthogonalization criterion based on measured loss of orthogonality. Let $\\eta = c \\sqrt{\\epsilon_{\\mathrm{mach}}}$ with $c = 10$. At Lanczos step $j$, after forming the candidate vector $w$, compute inner products $h_{i} = v_{i}^{\\top} w$ for $i = 1, \\dots, j$. If $|h_{i}|  \\eta$ for any $i$, perform modified Gram–Schmidt reorthogonalization against those $v_{i}$ for which the inequality holds, and repeat this check at most $2$ passes to reduce $|h_{i}|$ back near $\\mathcal{O}(\\epsilon_{\\mathrm{mach}})$.\n- For the full reorthogonalization variant, at each step $j$ perform modified Gram–Schmidt reorthogonalization against all previously computed Lanczos vectors $\\{v_{1}, \\dots, v_{j}\\}$, performing at most $2$ passes.\n- For the no reorthogonalization variant, perform only the $3$-term recurrence without any Gram–Schmidt steps.\n\nAccuracy metric:\n- For a given symmetric matrix $A$, a target subspace dimension $k$, and a target count $r \\leq k$, compute the $r$ largest Ritz values from $T_{k}$ (in algebraic order) and compare them against the $r$ largest true eigenvalues of $A$ (in algebraic order). Define the error as the maximum absolute deviation after sorting both sets in descending order:\n$$\nE = \\max_{1 \\leq i \\leq r} \\left| \\lambda^{(A)}_{(i)} - \\theta^{(T)}_{(i)} \\right|,\n$$\nwhere $\\lambda^{(A)}_{(i)}$ are the $r$ largest eigenvalues of $A$ and $\\theta^{(T)}_{(i)}$ are the $r$ largest Ritz values.\n- Define the improvement factor due to selective reorthogonalization as\n$$\n\\mathcal{I} = \\frac{E_{\\mathrm{none}}}{E_{\\mathrm{sel}}},\n$$\nwhere $E_{\\mathrm{none}}$ is the error under no reorthogonalization and $E_{\\mathrm{sel}}$ is the error under selective reorthogonalization. If $E_{\\mathrm{sel}} = 0$, report $\\mathcal{I} = 10^{16}$.\n\nImplementation requirements:\n- Implement the symmetric Lanczos process that returns the tridiagonal $T_{k}$ for each of the three variants described above.\n- Use a fixed, reproducible random starting vector $v_{1}$ with $\\|v_{1}\\|_{2} = 1$ for each test case.\n- Compute true eigenvalues of $A$ using a numerically stable symmetric eigensolver.\n\nTest suite:\n- Your program must run the following three test cases and report the improvement factor $\\mathcal{I}$ for each case as defined above. In all cases, use double precision arithmetic, no physical units are involved, and no angles are used.\n    1. Happy path with well-separated spectrum:\n        - $n = 80$, $k = 30$, $r = 5$.\n        - Construct $A = Q \\Lambda Q^{\\top}$, where $\\Lambda = \\mathrm{diag}(\\ell_{1}, \\dots, \\ell_{n})$ with $\\ell_{i}$ linearly spaced from $1$ to $100$, and $Q$ is the orthogonal factor from the $\\mathrm{QR}$ factorization of a random Gaussian matrix with seed $0$.\n        - Use a random Gaussian starting vector with seed $1$, normalized to unit $2$-norm.\n    2. Challenging cluster near the top of the spectrum:\n        - $n = 120$, $k = 60$, $r = 8$.\n        - Construct $A = Q \\Lambda Q^{\\top}$, where $\\Lambda$ has $10$ eigenvalues near $10$ given by $10 + \\delta_{i}$ with $\\delta_{i}$ i.i.d. uniformly sampled in $[-10^{-8}, 10^{-8}]$, and the remaining $110$ eigenvalues linearly spaced in $[0.1, 9.9]$. Use seed $2$ for generating $Q$ and for the uniform samples.\n        - Use a random Gaussian starting vector with seed $3$, normalized to unit $2$-norm.\n    3. Boundary case with long run and structured operator:\n        - $n = 90$, $k = 90$, $r = 10$.\n        - Construct $A$ as the tridiagonal Toeplitz matrix with zeros on the diagonal and ones on the first sub- and super-diagonals, i.e., $A_{i,i} = 0$ and $A_{i,i+1} = A_{i+1,i} = 1$ for $i = 1, \\dots, n-1$.\n        - Use a random Gaussian starting vector with seed $4$, normalized to unit $2$-norm.\n\nRequired final output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain exactly $3$ floating-point numbers corresponding to the improvement factors $\\mathcal{I}$ for the test cases $1$, $2$, and $3$ in this order. For example, the format must be exactly like `[x1,x2,x3]` with no extra spaces, where `xi` are decimal strings.\n\nScoring and acceptance criteria:\n- The program must be self-contained and runnable without user input.\n- The selective reorthogonalization must use the threshold $\\eta = 10 \\sqrt{\\epsilon_{\\mathrm{mach}}}$ and at most $2$ Gram–Schmidt passes per step.\n- Each $\\mathcal{I}$ must be a finite nonnegative float, using the rule $\\mathcal{I} = 10^{16}$ if $E_{\\mathrm{sel}} = 0$.", "solution": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import eigh_tridiagonal\n\ndef lanczos_iteration(A, v1, k, mode='none'):\n    \"\"\"\n    Performs the symmetric Lanczos iteration for a matrix A and starting vector v1.\n\n    Args:\n        A (np.ndarray): The symmetric matrix.\n        v1 (np.ndarray): The starting vector of unit norm.\n        k (int): The number of Lanczos steps (dimension of Krylov subspace).\n        mode (str): Reorthogonalization mode: 'none' or 'selective'.\n\n    Returns:\n        np.ndarray: The symmetric tridiagonal matrix T_k of size k x k.\n    \"\"\"\n    n = A.shape[0]\n    V = np.zeros((n, k + 1))\n    alphas = np.zeros(k)\n    betas = np.zeros(k)\n\n    eps_mach = np.finfo(float).eps\n    eta = 10.0 * np.sqrt(eps_mach)\n    breakdown_tol = 1e-14\n\n    V[:, 0] = v1\n\n    for j in range(k):\n        v_curr = V[:, j]\n        w = A @ v_curr\n\n        if j > 0:\n            w -= betas[j - 1] * V[:, j - 1]\n\n        alphas[j] = v_curr.T @ w\n        w -= alphas[j] * v_curr\n\n        if mode == 'selective':\n            # Perform selective reorthogonalization with at most 2 MGS passes\n            for _ in range(2):\n                # Projections of w onto the basis V\n                h = V[:, :j + 1].T @ w\n                indices_to_reortho = np.where(np.abs(h) > eta)[0]\n                \n                if len(indices_to_reortho) == 0:\n                    break  # Orthogonality is sufficient\n                \n                # Apply MGS steps for the vectors that lost orthogonality\n                for i in indices_to_reortho:\n                    w -= (V[:, i].T @ w) * V[:, i]\n\n        betas[j] = np.linalg.norm(w)\n\n        if betas[j]  breakdown_tol:\n            # Breakdown: Krylov subspace is invariant or exhausted.\n            k_actual = j + 1\n            T_k = np.zeros((k, k))\n            sub_T_alphas = alphas[:k_actual]\n            sub_T_betas = betas[:k_actual - 1]\n            T_k_sub = np.diag(sub_T_alphas) + np.diag(sub_T_betas, 1) + np.diag(sub_T_betas, -1)\n            T_k[:k_actual, :k_actual] = T_k_sub\n            return T_k\n            \n        V[:, j + 1] = w / betas[j]\n\n    T_k = np.diag(alphas) + np.diag(betas[:k - 1], 1) + np.diag(betas[:k - 1], -1)\n    return T_k\n\ndef run_test_case(case_id):\n    \"\"\"\n    Sets up and runs a single test case, returning the improvement factor.\n    \"\"\"\n    if case_id == 1:\n        n, k, r = 80, 30, 5\n        rng_Q = np.random.default_rng(0)\n        G = rng_Q.standard_normal((n, n))\n        Q, _ = np.linalg.qr(G)\n        lambda_vals = np.linspace(1, 100, n)\n        A = Q @ np.diag(lambda_vals) @ Q.T\n        rng_v1 = np.random.default_rng(1)\n        v1 = rng_v1.standard_normal(n)\n    elif case_id == 2:\n        n, k, r = 120, 60, 8\n        rng = np.random.default_rng(2)\n        cluster_vals = 10.0 + rng.uniform(-1e-8, 1e-8, size=10)\n        other_vals = np.linspace(0.1, 9.9, n - 10)\n        lambda_vals = np.concatenate((other_vals, cluster_vals))\n        G = rng.standard_normal((n, n))\n        Q, _ = np.linalg.qr(G)\n        A = Q @ np.diag(lambda_vals) @ Q.T\n        rng_v1 = np.random.default_rng(3)\n        v1 = rng_v1.standard_normal(n)\n    elif case_id == 3:\n        n, k, r = 90, 90, 10\n        A = np.diag(np.ones(n - 1), 1) + np.diag(np.ones(n - 1), -1)\n        rng_v1 = np.random.default_rng(4)\n        v1 = rng_v1.standard_normal(n)\n    else:\n        raise ValueError(\"Invalid case ID\")\n        \n    v1 /= np.linalg.norm(v1)\n\n    # Compute true eigenvalues of A\n    true_eigvals = np.linalg.eigh(A)[0]\n    largest_true_eigvals = np.flip(true_eigvals[-r:]) # descending order\n\n    # Run Lanczos with no reorthogonalization\n    T_none = lanczos_iteration(A, v1, k, mode='none')\n    ritz_vals_none = eigh_tridiagonal(np.diag(T_none), np.diag(T_none, 1))[0]\n    largest_ritz_none = np.flip(ritz_vals_none[-r:])\n\n    # Run Lanczos with selective reorthogonalization\n    T_sel = lanczos_iteration(A, v1, k, mode='selective')\n    ritz_vals_sel = eigh_tridiagonal(np.diag(T_sel), np.diag(T_sel, 1))[0]\n    largest_ritz_sel = np.flip(ritz_vals_sel[-r:])\n\n    # Compute errors\n    E_none = np.max(np.abs(largest_true_eigvals - largest_ritz_none))\n    E_sel = np.max(np.abs(largest_true_eigvals - largest_ritz_sel))\n    \n    # Compute improvement factor\n    if E_sel == 0.0:\n        if E_none == 0.0:\n            return 1.0\n        return 1e16\n    \n    return E_none / E_sel\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    # The problem statement defines the test cases.\n    # We simply iterate through them.\n    test_cases = [1, 2, 3]\n\n    results = []\n    for case_id in test_cases:\n        improvement_factor = run_test_case(case_id)\n        results.append(improvement_factor)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import eigh_tridiagonal\n\ndef lanczos_iteration(A, v1, k, mode='none'):\n    \"\"\"\n    Performs the symmetric Lanczos iteration for a matrix A and starting vector v1.\n\n    Args:\n        A (np.ndarray): The symmetric matrix.\n        v1 (np.ndarray): The starting vector of unit norm.\n        k (int): The number of Lanczos steps (dimension of Krylov subspace).\n        mode (str): Reorthogonalization mode: 'none' or 'selective'.\n\n    Returns:\n        np.ndarray: The symmetric tridiagonal matrix T_k of size k x k.\n    \"\"\"\n    n = A.shape[0]\n    V = np.zeros((n, k + 1))\n    alphas = np.zeros(k)\n    betas = np.zeros(k)\n\n    eps_mach = np.finfo(float).eps\n    eta = 10.0 * np.sqrt(eps_mach)\n    breakdown_tol = 1e-14\n\n    V[:, 0] = v1\n\n    for j in range(k):\n        v_curr = V[:, j]\n        w = A @ v_curr\n\n        if j > 0:\n            w -= betas[j - 1] * V[:, j - 1]\n\n        alphas[j] = v_curr.T @ w\n        w -= alphas[j] * v_curr\n\n        if mode == 'selective':\n            # Perform selective reorthogonalization with at most 2 MGS passes\n            for _ in range(2):\n                # Projections of w onto the basis V\n                h = V[:, :j + 1].T @ w\n                indices_to_reortho = np.where(np.abs(h) > eta)[0]\n                \n                if len(indices_to_reortho) == 0:\n                    break  # Orthogonality is sufficient\n                \n                # Apply MGS steps for the vectors that lost orthogonality\n                for i in indices_to_reortho:\n                    w -= (V[:, i].T @ w) * V[:, i]\n\n        betas[j] = np.linalg.norm(w)\n\n        if betas[j]  breakdown_tol:\n            # Breakdown: Krylov subspace is invariant or exhausted.\n            k_actual = j + 1\n            T_k = np.zeros((k, k))\n            sub_T_alphas = alphas[:k_actual]\n            sub_T_betas = betas[:k_actual - 1]\n            T_k_sub = np.diag(sub_T_alphas) + np.diag(sub_T_betas, 1) + np.diag(sub_T_betas, -1)\n            T_k[:k_actual, :k_actual] = T_k_sub\n            return T_k\n            \n        V[:, j + 1] = w / betas[j]\n\n    T_k = np.diag(alphas) + np.diag(betas[:k - 1], 1) + np.diag(betas[:k - 1], -1)\n    return T_k\n\ndef run_test_case(case_id):\n    \"\"\"\n    Sets up and runs a single test case, returning the improvement factor.\n    \"\"\"\n    if case_id == 1:\n        n, k, r = 80, 30, 5\n        rng_Q = np.random.default_rng(0)\n        G = rng_Q.standard_normal((n, n))\n        Q, _ = np.linalg.qr(G)\n        lambda_vals = np.linspace(1, 100, n)\n        A = Q @ np.diag(lambda_vals) @ Q.T\n        rng_v1 = np.random.default_rng(1)\n        v1 = rng_v1.standard_normal(n)\n    elif case_id == 2:\n        n, k, r = 120, 60, 8\n        rng = np.random.default_rng(2)\n        cluster_vals = 10.0 + rng.uniform(-1e-8, 1e-8, size=10)\n        other_vals = np.linspace(0.1, 9.9, n - 10)\n        lambda_vals = np.concatenate((other_vals, cluster_vals))\n        G = rng.standard_normal((n, n))\n        Q, _ = np.linalg.qr(G)\n        A = Q @ np.diag(lambda_vals) @ Q.T\n        rng_v1 = np.random.default_rng(3)\n        v1 = rng_v1.standard_normal(n)\n    elif case_id == 3:\n        n, k, r = 90, 90, 10\n        A = np.diag(np.ones(n - 1), 1) + np.diag(np.ones(n - 1), -1)\n        rng_v1 = np.random.default_rng(4)\n        v1 = rng_v1.standard_normal(n)\n    else:\n        raise ValueError(\"Invalid case ID\")\n        \n    v1 /= np.linalg.norm(v1)\n\n    # Compute true eigenvalues of A\n    true_eigvals = np.linalg.eigh(A)[0]\n    largest_true_eigvals = np.flip(true_eigvals[-r:]) # descending order\n\n    # Run Lanczos with no reorthogonalization\n    T_none = lanczos_iteration(A, v1, k, mode='none')\n    ritz_vals_none = eigh_tridiagonal(np.diag(T_none), np.diag(T_none, 1))[0]\n    largest_ritz_none = np.flip(ritz_vals_none[-r:])\n\n    # Run Lanczos with selective reorthogonalization\n    T_sel = lanczos_iteration(A, v1, k, mode='selective')\n    ritz_vals_sel = eigh_tridiagonal(np.diag(T_sel), np.diag(T_sel, 1))[0]\n    largest_ritz_sel = np.flip(ritz_vals_sel[-r:])\n\n    # Compute errors\n    E_none = np.max(np.abs(largest_true_eigvals - largest_ritz_none))\n    E_sel = np.max(np.abs(largest_true_eigvals - largest_ritz_sel))\n    \n    # Compute improvement factor\n    if E_sel == 0.0:\n        if E_none == 0.0:\n            return 1.0\n        return 1e16\n    \n    return E_none / E_sel\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    # The problem statement defines the test cases.\n    # We simply iterate through them.\n    test_cases = [1, 2, 3]\n\n    results = []\n    for case_id in test_cases:\n        improvement_factor = run_test_case(case_id)\n        results.append(improvement_factor)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3581514"}]}