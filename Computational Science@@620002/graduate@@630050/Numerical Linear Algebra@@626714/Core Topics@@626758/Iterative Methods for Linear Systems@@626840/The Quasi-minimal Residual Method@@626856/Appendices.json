{"hands_on_practices": [{"introduction": "The practical utility of any iterative algorithm begins with an assessment of its computational demands. The Quasi-Minimal Residual (QMR) method is powered by the bi-Lanczos process, and its efficiency is determined by the cost of each iteration. This exercise guides you through a fundamental analysis of the floating-point operations (flops) and memory storage required for one step of this process, providing a concrete understanding of the algorithm's operational cost [@problem_id:3594292].", "problem": "Consider the Quasi-Minimal Residual (QMR) method for solving a linear system with a real, nonsymmetric sparse matrix $A \\in \\mathbb{R}^{n \\times n}$. The bi-Lanczos process simultaneously builds right and left Krylov bases $V_{k}$ and $W_{k}$ such that the $(k+1)$-st step extends $V_{k}$ to $V_{k+1}$ and $W_{k}$ to $W_{k+1}$ using short three-term recurrences. Assume the following per-iteration work model for this single bi-Lanczos extension step:\n- Exactly one sparse matrix-vector multiplication with $A$ and one with $A^{\\mathsf{T}}$ occur per iteration.\n- A fixed number of inner products is performed per iteration, namely $s=4$ inner products of vectors in $\\mathbb{R}^{n}$.\n- A fixed number of axpy-like vector updates is performed per iteration, namely $t=6$ updates of the form $x \\leftarrow x + \\alpha y$, each with vectors in $\\mathbb{R}^{n}$.\n- A fixed number of vector scalings is performed per iteration, namely $u=2$ operations of the form $x \\leftarrow \\alpha x$.\n\nAdopt the standard operation cost model:\n- A sparse matrix-vector multiplication with a matrix having $m$ nonzero entries costs $2m$ floating-point operations (flops).\n- A length-$n$ inner product costs $2n$ flops.\n- A length-$n$ axpy costs $2n$ flops.\n- A length-$n$ scaling costs $n$ flops.\n\nFor storage, assume that every newly generated basis vector is stored explicitly and that, in addition to the vectors $v_{k+1}$ and $w_{k+1}$, exactly three scalar recurrence coefficients are stored per iteration.\n\nUnder these assumptions, compute:\n1. The exact per-iteration floating-point operation count $F(n,m)$ required to extend $V_{k}$ to $V_{k+1}$ and $W_{k}$ to $W_{k+1}$.\n2. The incremental storage $S(n)$, in number of scalars, required when these new basis vectors and scalar coefficients are added.\n\nExpress your final answer as the row matrix $\\begin{pmatrix} F(n,m) & S(n) \\end{pmatrix}$, with both entries given as closed-form expressions in terms of $n$ and $m$.", "solution": "The problem requires the computation of two quantities for a single iteration of the bi-Lanczos process within the Quasi-Minimal Residual (QMR) method: the total floating-point operation (flop) count, denoted by $F(n,m)$, and the incremental storage requirement, denoted by $S(n)$. The problem provides a precise model for the number of operations per iteration and their respective costs, as well as the components to be stored.\n\nFirst, we will calculate the per-iteration flop count, $F(n,m)$. This is the sum of the flops required for all specified operations within one iteration. The problem details the following operations and their costs for a sparse matrix $A \\in \\mathbb{R}^{n \\times n}$ with $m$ nonzero entries:\n\n1.  **Matrix-Vector Multiplications**: The problem states that there is exactly one sparse matrix-vector multiplication with $A$ and one with its transpose, $A^{\\mathsf{T}}$. The matrix $A^{\\mathsf{T}}$ has the same number of nonzero entries, $m$, as $A$. The cost for a single such operation is given as $2m$ flops.\n    The total cost from matrix-vector multiplications per iteration is:\n    $$ \\text{Cost}_{\\text{matvec}} = (\\text{cost for } A) + (\\text{cost for } A^{\\mathsf{T}}) = 2m + 2m = 4m \\text{ flops.} $$\n\n2.  **Inner Products**: The problem specifies that $s=4$ inner products are performed per iteration. Each inner product involves vectors of length $n$. The cost for one length-$n$ inner product is given as $2n$ flops.\n    The total cost from inner products per iteration is:\n    $$ \\text{Cost}_{\\text{inner}} = s \\times (2n) = 4 \\times 2n = 8n \\text{ flops.} $$\n\n3.  **AXPY-like Vector Updates**: The problem states that $t=6$ updates of the form $x \\leftarrow x + \\alpha y$ (axpy) are performed. These updates involve vectors in $\\mathbb{R}^{n}$. The cost for one length-$n$ axpy operation is given as $2n$ flops.\n    The total cost from axpy updates per iteration is:\n    $$ \\text{Cost}_{\\text{axpy}} = t \\times (2n) = 6 \\times 2n = 12n \\text{ flops.} $$\n\n4.  **Vector Scalings**: The problem specifies that $u=2$ vector scalings of the form $x \\leftarrow \\alpha x$ are performed. These involve vectors of length $n$. The cost for one length-$n$ scaling operation is given as $n$ flops.\n    The total cost from vector scalings per iteration is:\n    $$ \\text{Cost}_{\\text{scale}} = u \\times n = 2 \\times n = 2n \\text{ flops.} $$\n\nThe total per-iteration flop count, $F(n,m)$, is the sum of these individual costs:\n$$ F(n,m) = \\text{Cost}_{\\text{matvec}} + \\text{Cost}_{\\text{inner}} + \\text{Cost}_{\\text{axpy}} + \\text{Cost}_{\\text{scale}} $$\n$$ F(n,m) = 4m + 8n + 12n + 2n $$\n$$ F(n,m) = 4m + 22n $$\n\nNext, we will calculate the incremental storage per iteration, $S(n)$. This is the total number of new scalar values that must be stored. The problem specifies the following storage requirements:\n\n1.  **New Basis Vectors**: At each step, the process generates and stores two new basis vectors, $v_{k+1}$ and $w_{k+1}$. Both vectors are in $\\mathbb{R}^{n}$. Storing one vector of length $n$ requires storing $n$ scalar values.\n    The storage for the two new basis vectors is:\n    $$ \\text{Storage}_{\\text{vectors}} = (\\text{storage for } v_{k+1}) + (\\text{storage for } w_{k+1}) = n + n = 2n \\text{ scalars.} $$\n\n2.  **Scalar Recurrence Coefficients**: The problem states that exactly three scalar recurrence coefficients are stored per iteration.\n    The storage for these coefficients is:\n    $$ \\text{Storage}_{\\text{coeffs}} = 3 \\text{ scalars.} $$\n\nThe total incremental storage, $S(n)$, is the sum of these components:\n$$ S(n) = \\text{Storage}_{\\text{vectors}} + \\text{Storage}_{\\text{coeffs}} $$\n$$ S(n) = 2n + 3 $$\n\nThe problem asks for the final answer to be presented as the row matrix $\\begin{pmatrix} F(n,m) & S(n) \\end{pmatrix}$. Substituting the derived expressions for $F(n,m)$ and $S(n)$, we obtain the final result.", "answer": "$$ \\boxed{ \\begin{pmatrix} 4m + 22n & 2n + 3 \\end{pmatrix} } $$", "id": "3594292"}, {"introduction": "A primary motivation for the development of QMR was to create a method more robust than the Biconjugate Gradient (BiCG) algorithm. The vulnerability of BiCG lies in the potential for \"breakdown\" in the underlying bi-Lanczos process, which occurs when the biorthogonality condition fails. This practice provides a hands-on exploration of such a breakdown, allowing you to derive the exact algebraic conditions under which it occurs in a model system, thereby clarifying the theoretical challenge that QMR's \"quasi-minimal\" strategy is designed to circumvent [@problem_id:3594335].", "problem": "Consider the biorthogonal Lanczos process that underlies Biconjugate Gradient (BiCG) and Quasi-Minimal Residual (QMR) for solving a nonsymmetric linear system in numerical linear algebra. Let $A \\in \\mathbb{R}^{3 \\times 3}$ be a nonsymmetric tridiagonal Toeplitz matrix with constant subdiagonal, diagonal, and superdiagonal entries given by $s$, $m$, and $t$, respectively:\n$$\nA \\;=\\;\n\\begin{pmatrix}\nm & t & 0 \\\\\ns & m & t \\\\\n0 & s & m\n\\end{pmatrix}.\n$$\nLet the initial guess be $x_0 = \\mathbf{0}$ and the right-hand side be $b = \\mathbf{e}_1$, so the initial residual is $r_0 = b - A x_0 = \\mathbf{e}_1$. Choose the shadow residual $r_0^\\sharp = \\mathbf{e}_1$. Define the first right and left Lanczos vectors by $v_1 = r_0$ and $w_1 = r_0^\\sharp$. For the first Lanczos step, define the scalar\n$$\n\\alpha_1 \\;=\\; \\frac{w_1^{T} A v_1}{w_1^{T} v_1}.\n$$\nThen form the second right and left Lanczos vectors by\n$$\nv_2 \\;=\\; A v_1 \\;-\\; \\alpha_1 v_1, \n\\qquad\nw_2 \\;=\\; A^{T} w_1 \\;-\\; \\alpha_1 w_1.\n$$\nBiCG experiences a true breakdown at iteration $j=2$ if $w_2^{T} v_2 = 0$, while QMR with look-ahead can continue by skipping the offending step. Using only these definitions and the given data, derive the closed-form expression of $w_2^{T} v_2$ in terms of the subdiagonal and superdiagonal parameters $s$ and $t$. Based on this expression, the parameter regimes on $s$ and $t$ that trigger ($w_2^{T} v_2 = 0$) and avoid ($w_2^{T} v_2 \\neq 0$) BiCG breakdown at $j=2$ follow immediately, and QMR with look-ahead is applicable in the former case.\n\nProvide your final answer as the exact algebraic expression for $w_2^{T} v_2$ in terms of $s$ and $t$.", "solution": "The objective is to compute the scalar quantity $w_2^{T} v_2$, which determines if a breakdown occurs at the second step of the BiCG algorithm. The derivation proceeds by systematically calculating all intermediate quantities based on the provided givens.\n\nThe matrix $A \\in \\mathbb{R}^{3 \\times 3}$ is given by:\n$$\nA \\;=\\;\n\\begin{pmatrix}\nm & t & 0 \\\\\ns & m & t \\\\\n0 & s & m\n\\end{pmatrix}\n$$\nThe right-hand side vector is $b = \\mathbf{e}_1$, and the initial guess is $x_0 = \\mathbf{0}$. The initial residual is $r_0 = b - Ax_0 = \\mathbf{e}_1 - A\\mathbf{0} = \\mathbf{e}_1$.\nThe initial shadow residual is chosen as $r_0^\\sharp = \\mathbf{e}_1$.\n\nThe first right and left Lanczos vectors are defined from these residuals:\n$$\nv_1 = r_0 = \\mathbf{e}_1 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\n$$\nw_1 = r_0^\\sharp = \\mathbf{e}_1 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\nThe first step is to compute the scalar $\\alpha_1$:\n$$\n\\alpha_1 \\;=\\; \\frac{w_1^{T} A v_1}{w_1^{T} v_1}\n$$\nWe first evaluate the denominator, $w_1^{T} v_1$:\n$$\nw_1^{T} v_1 = \\begin{pmatrix} 1 & 0 & 0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} = 1\n$$\nSince $w_1^{T} v_1 \\neq 0$, the process can continue.\n\nNext, we evaluate the numerator, $w_1^{T} A v_1$. We first compute the matrix-vector product $A v_1$:\n$$\nA v_1 = \\begin{pmatrix} m & t & 0 \\\\ s & m & t \\\\ 0 & s & m \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} m \\\\ s \\\\ 0 \\end{pmatrix}\n$$\nNow, we can compute the numerator:\n$$\nw_1^{T} (A v_1) = \\begin{pmatrix} 1 & 0 & 0 \\end{pmatrix} \\begin{pmatrix} m \\\\ s \\\\ 0 \\end{pmatrix} = m\n$$\nUsing these results, we find $\\alpha_1$:\n$$\n\\alpha_1 = \\frac{m}{1} = m\n$$\nThe next step is to form the second right Lanczos vector, $v_2$:\n$$\nv_2 = A v_1 - \\alpha_1 v_1\n$$\nSubstituting the quantities we have already computed:\n$$\nv_2 = \\begin{pmatrix} m \\\\ s \\\\ 0 \\end{pmatrix} - m \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} m \\\\ s \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} m \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ s \\\\ 0 \\end{pmatrix}\n$$\nSimilarly, we form the second left Lanczos vector, $w_2$:\n$$\nw_2 = A^{T} w_1 - \\alpha_1 w_1\n$$\nFirst, we find the transpose of $A$, denoted $A^T$:\n$$\nA^{T} = \\begin{pmatrix} m & s & 0 \\\\ t & m & s \\\\ 0 & t & m \\end{pmatrix}\n$$\nNext, we compute the matrix-vector product $A^T w_1$:\n$$\nA^{T} w_1 = \\begin{pmatrix} m & s & 0 \\\\ t & m & s \\\\ 0 & t & m \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} m \\\\ t \\\\ 0 \\end{pmatrix}\n$$\nNow, we can compute $w_2$:\n$$\nw_2 = \\begin{pmatrix} m \\\\ t \\\\ 0 \\end{pmatrix} - m \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} m \\\\ t \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} m \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ t \\\\ 0 \\end{pmatrix}\n$$\nFinally, we compute the quantity $w_2^{T} v_2$, which is the inner product of the second left and right Lanczos vectors. A true breakdown in BiCG occurs if this quantity is zero.\n$$\nw_2^{T} v_2 = \\begin{pmatrix} 0 & t & 0 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ s \\\\ 0 \\end{pmatrix} = (0)(0) + (t)(s) + (0)(0) = st\n$$\nThe expression for $w_2^{T} v_2$ is $st$. This result shows that a breakdown at the second step occurs if and only if $st = 0$, which means either the subdiagonal element $s$ or the superdiagonal element $t$ is zero. In such cases, the tridiagonal matrix $A$ would become triangular. The diagonal element $m$ does not influence this particular breakdown condition.", "answer": "$$\\boxed{st}$$", "id": "3594335"}, {"introduction": "For an iterative method, a reliable stopping criterion is just as important as the update step itself. While the true residual norm $\\|r_k\\|_2 = \\|b - A x_k\\|_2$ is the ideal measure of accuracy, its direct computation can suffer from catastrophic cancellation as the solution converges. This exercise demonstrates the elegant and numerically stable technique employed by QMR to compute the residual norm implicitly, using information from the internal least-squares problem, thus preserving accuracy where a naive approach would fail [@problem_id:3594283].", "problem": "Consider solving the nonsymmetric linear system $A x = b$ by the Quasi-Minimal Residual (QMR) method, which is constructed on the nonsymmetric Lanczos process and a small least-squares problem. Let $r_k = b - A x_k$ denote the true residual at iteration $k$. In exact arithmetic, the residual norm $\\|r_k\\|_2$ can be obtained from the sequence of orthogonal transformations used to solve the QMR least-squares subproblem, without ever forming $r_k$ explicitly.\n\nStart from the following basic facts: orthogonal transformations preserve the Euclidean norm, and a least-squares problem $\\min_{y \\in \\mathbb{C}^k} \\|\\beta e_1 - \\underline{T}_k y\\|_2$, where $\\underline{T}_k$ is a $(k+1) \\times k$ matrix and $e_1$ is the first canonical basis vector, can be solved stably by a sequence of Givens rotations that triangularize $\\underline{T}_k$. The same rotations applied to the right-hand side yield a transformed vector whose last entry has magnitude equal to the minimal residual norm.\n\nSuppose after $k=3$ QMR iterations, the Givens rotations used to triangularize $\\underline{T}_3$ have parameters $(c_1,s_1)$, $(c_2,s_2)$, $(c_3,s_3)$ with\n$$\n(c_1,s_1) = \\left(\\frac{4}{5}, \\frac{3}{5}\\right), \\quad\n(c_2,s_2) = \\left(\\frac{3}{5}, \\frac{4}{5}\\right), \\quad\n(c_3,s_3) = \\left(\\frac{5}{13}, \\frac{12}{13}\\right),\n$$\nand the initial residual norm is $\\beta = \\|r_0\\|_2 = 25$.\n\nUsing only the least-squares updates implied by these rotations and the norm-preserving property of orthogonal transformations, compute the true residual norm $\\|r_3\\|_2$ without forming $r_3$ explicitly. Then, explain why explicitly forming $r_3$ as $b - A x_3$ to compute $\\|r_3\\|_2$ may suffer cancellation-induced inaccuracies in finite precision arithmetic.\n\nYour final numerical answer must be a single real number in exact form; do not round.", "solution": "The problem requires us to compute the true residual norm $\\|r_3\\|_2$ by tracking the effect of the given Givens rotations on the initial right-hand-side vector of the least-squares subproblem. For $k=3$, the subproblem is posed in a space of dimension $k+1 = 4$. The initial right-hand-side vector is $g_0 = \\beta e_1$, where $\\beta = \\|r_0\\|_2 = 25$ and $e_1$ is the first canonical basis vector in $\\mathbb{R}^4$.\n$$ g_0 = \\begin{pmatrix} 25 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} $$\nThe process involves applying a sequence of three Givens rotations, $\\Omega_1$, $\\Omega_2$, and $\\Omega_3$. The rotation $\\Omega_j$ is designed to introduce a zero in the subdiagonal of the matrix $\\underline{T}_3$ by rotating rows $j$ and $j+1$. We apply the same sequence of rotations to the vector $g_0$.\n\nAt iteration $j=1$, we apply the first rotation $\\Omega_1$ with parameters $(c_1, s_1) = \\left(\\frac{4}{5}, \\frac{3}{5}\\right)$. This rotation acts on the first and second components of the vector.\n$$ g_1 = \\Omega_1 g_0 = \\begin{pmatrix} c_1 & s_1 & 0 & 0 \\\\ -s_1 & c_1 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} 25 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} c_1 \\cdot 25 \\\\ -s_1 \\cdot 25 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{4}{5} \\cdot 25 \\\\ -\\frac{3}{5} \\cdot 25 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 20 \\\\ -15 \\\\ 0 \\\\ 0 \\end{pmatrix} $$\nAt iteration $j=2$, we apply the second rotation $\\Omega_2$ with parameters $(c_2, s_2) = \\left(\\frac{3}{5}, \\frac{4}{5}\\right)$. This rotation acts on the second and third components of the current vector $g_1$.\n$$ g_2 = \\Omega_2 g_1 = \\begin{pmatrix} 1 & 0 & 0 & 0 \\\\ 0 & c_2 & s_2 & 0 \\\\ 0 & -s_2 & c_2 & 0 \\\\ 0 & 0 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} 20 \\\\ -15 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 20 \\\\ c_2 \\cdot (-15) \\\\ -s_2 \\cdot (-15) \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 20 \\\\ \\frac{3}{5} \\cdot (-15) \\\\ -(\\frac{4}{5}) \\cdot (-15) \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 20 \\\\ -9 \\\\ 12 \\\\ 0 \\end{pmatrix} $$\nAt iteration $j=3$, we apply the third rotation $\\Omega_3$ with parameters $(c_3, s_3) = \\left(\\frac{5}{13}, \\frac{12}{13}\\right)$. This rotation acts on the third and fourth components of the current vector $g_2$.\n$$ g_3 = \\Omega_3 g_2 = \\begin{pmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & c_3 & s_3 \\\\ 0 & 0 & -s_3 & c_3 \\end{pmatrix} \\begin{pmatrix} 20 \\\\ -9 \\\\ 12 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 20 \\\\ -9 \\\\ c_3 \\cdot 12 \\\\ -s_3 \\cdot 12 \\end{pmatrix} = \\begin{pmatrix} 20 \\\\ -9 \\\\ \\frac{5}{13} \\cdot 12 \\\\ -\\frac{12}{13} \\cdot 12 \\end{pmatrix} = \\begin{pmatrix} 20 \\\\ -9 \\\\ \\frac{60}{13} \\\\ -\\frac{144}{13} \\end{pmatrix} $$\nThe final transformed vector is $g_3$. According to the problem statement, the magnitude of the last entry of this vector is the desired residual norm. The last (i.e., the $k+1 = 4$-th) entry is $-\\frac{144}{13}$.\nTherefore, the true residual norm at iteration $k=3$ is:\n$$ \\|r_3\\|_2 = \\left|-\\frac{144}{13}\\right| = \\frac{144}{13} $$\nAlternatively, one can observe that the last non-zero entry after $k$ steps has the magnitude $\\|r_0\\|_2 \\prod_{j=1}^k |s_j|$.\nFor $k=3$, this is $\\|r_3\\|_2 = \\|r_0\\|_2 |s_1 s_2 s_3| = 25 \\cdot \\frac{3}{5} \\cdot \\frac{4}{5} \\cdot \\frac{12}{13} = 25 \\cdot \\frac{144}{325} = \\frac{144}{13}$.\n\nThe second part of the problem asks why explicitly computing $r_3$ as $b - A x_3$ to find its norm might be inaccurate in finite precision arithmetic.\nThe computation involves two stages: first, the matrix-vector product $y_3 = A x_3$, and second, the vector subtraction $r_3 = b - y_3$. As an iterative method like QMR converges, the approximate solution $x_k$ approaches the true solution $x_{true}$. Consequently, the vector $A x_k$ becomes a progressively better approximation of $b = A x_{true}$.\nIn finite precision arithmetic, computers represent real numbers with a limited number of significant digits. When two numbers that are very close to each other are subtracted, the leading, identical digits cancel out. This phenomenon is known as **catastrophic cancellation** or **loss of significance**. The result of the subtraction is computed from the remaining, less significant digits, which are more heavily influenced by rounding errors from previous calculations (in this case, the computation of $A x_3$).\nAs $k$ increases, $A x_k$ and $b$ become nearly identical. Performing the subtraction $b - A x_k$ component-wise will lead to catastrophic cancellation. The computed residual vector, let's call it $\\hat{r}_k$, can have a very large relative error compared to the true residual vector $r_k$. The norm of this inaccurate vector, $\\|\\hat{r}_k\\|_2$, would therefore be an unreliable measure of the actual error.\nIn contrast, recurrence-based updates for the residual norm, such as the one described in this problem, are specifically designed to avoid this direct subtraction of two large, nearly equal quantities. They update the norm using factors (like the sine components $s_j$ of Givens rotations) that are numerically stable, thus preserving accuracy even when the residual norm becomes very small.", "answer": "$$\\boxed{\\frac{144}{13}}$$", "id": "3594283"}]}