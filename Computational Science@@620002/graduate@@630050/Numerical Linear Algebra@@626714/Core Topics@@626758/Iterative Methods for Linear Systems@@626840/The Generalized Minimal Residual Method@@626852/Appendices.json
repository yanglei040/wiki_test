{"hands_on_practices": [{"introduction": "The power of GMRES lies in its core principle of iteratively minimizing the residual norm over an expanding Krylov subspace. To grasp this fundamental idea, we begin with the simplest possible scenario: a single iteration. This exercise [@problem_id:2214790] challenges you to perform one step of GMRES by hand, which boils down to finding the optimal update along the direction of the initial residual. Mastering this foundational calculation provides a concrete entry point to understanding the more complex machinery of the full algorithm.", "problem": "The Generalized Minimum Residual (GMRES) method is an iterative algorithm used to find an approximate solution to a system of linear equations $Ax=b$. Consider the linear system defined by the matrix $A$ and the vector $b$:\n$$\nA = \\begin{pmatrix} 2 & 1 \\\\ -1 & 3 \\end{pmatrix}, \\quad b = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}\n$$\nStarting with an initial guess of $x_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$, apply one step of the GMRES algorithm to find the first approximate solution, $x_1$. Express your answer as a column vector with rational components.", "solution": "GMRES minimizes the 2-norm of the residual over the affine space $x_{0}+\\mathcal{K}_{1}(A,r_{0})$, where $r_{0}=b-Ax_{0}$ and $\\mathcal{K}_{1}(A,r_{0})=\\mathrm{span}\\{r_{0}\\}$. With $x_{0}=\\begin{pmatrix}0\\\\0\\end{pmatrix}$ we have\n$$\nr_{0}=b=\\begin{pmatrix}1\\\\2\\end{pmatrix}.\n$$\nAfter one GMRES step, $x_{1}$ has the form $x_{1}=x_{0}+\\alpha r_{0}$, where $\\alpha$ is chosen to minimize\n$$\n\\|b-A(x_{0}+\\alpha r_{0})\\|_{2}=\\|r_{0}-\\alpha A r_{0}\\|_{2}.\n$$\nLet $w=A r_{0}$. Then the minimization problem is $\\min_{\\alpha}\\|r_{0}-\\alpha w\\|_{2}^{2}$. Using the Euclidean inner product, define\n$$\n\\phi(\\alpha)=(r_{0}-\\alpha w)^{T}(r_{0}-\\alpha w)=r_{0}^{T}r_{0}-2\\alpha w^{T}r_{0}+\\alpha^{2}w^{T}w.\n$$\nSetting $\\frac{d\\phi}{d\\alpha}=0$ gives\n$$\n-2\\,w^{T}r_{0}+2\\alpha\\,w^{T}w=0 \\quad\\Rightarrow\\quad \\alpha=\\frac{w^{T}r_{0}}{w^{T}w}.\n$$\nCompute $w$ and the needed inner products:\n$$\nw=A r_{0}=A b=\\begin{pmatrix}2 & 1\\\\ -1 & 3\\end{pmatrix}\\begin{pmatrix}1\\\\2\\end{pmatrix}=\\begin{pmatrix}4\\\\5\\end{pmatrix},\\quad\nw^{T}r_{0}=\\begin{pmatrix}4 & 5\\end{pmatrix}\\begin{pmatrix}1\\\\2\\end{pmatrix}=14,\\quad\nw^{T}w=4^{2}+5^{2}=41.\n$$\nTherefore\n$$\n\\alpha=\\frac{14}{41},\\qquad x_{1}=x_{0}+\\alpha r_{0}=\\frac{14}{41}\\begin{pmatrix}1\\\\2\\end{pmatrix}=\\begin{pmatrix}\\frac{14}{41}\\\\ \\frac{28}{41}\\end{pmatrix}.\n$$\nThis $x_{1}$ is the GMRES(1) iterate, i.e., the minimizer over $x_{0}+\\mathrm{span}\\{r_{0}\\}$ of the residual norm.", "answer": "$$\\boxed{\\begin{pmatrix}\\frac{14}{41}\\\\ \\frac{28}{41}\\end{pmatrix}}$$", "id": "2214790"}, {"introduction": "Moving beyond a single step, a full understanding of GMRES requires engaging with its two main components: building an orthonormal basis for the Krylov subspace and solving the resulting projection subproblem. This practice [@problem_id:3616841] guides you through a complete two-step GMRES cycle for a small, non-symmetric system. You will apply the Arnoldi process to generate the basis and then use numerically stable Givens rotations to solve the least-squares problem, thereby demystifying the entire iterative procedure from start to finish.", "problem": "Consider a small nonsymmetric linear system that arises from a coarse finite-volume discretization of a one-dimensional steady advectionâ€“diffusion operator used in linearized seismic travel-time tomography. Let the system matrix be\n$$\nA = \\begin{pmatrix}\n1 & 0 & 0 \\\\\n1 & 3 & 0 \\\\\n0 & 1 & 2\n\\end{pmatrix},\n$$\nthe right-hand side be\n$$\nb = \\begin{pmatrix}\n1 \\\\\n0 \\\\\n0\n\\end{pmatrix},\n$$\nand the initial guess be\n$$\nx_0 = \\begin{pmatrix}\n0 \\\\\n0 \\\\\n0\n\\end{pmatrix}.\n$$\nPerform two steps of the Arnoldi process starting from the initial residual $r_0 = b - A x_0$, forming the orthonormal basis $V_{m+1}$ with $m = 2$ and the $(m+1) \\times m$ upper Hessenberg matrix $\\bar{H}_m$. Then, apply two successive Givens rotations to $\\bar{H}_m$ to eliminate its subdiagonal entries and obtain an upper triangular $R_m$, while simultaneously updating the transformed right-hand side. Use this to compute the Generalized Minimal Residual (GMRES) iterate $x_m = x_0 + V_m y$ and the residual $r_m = b - A x_m$ explicitly. Report the three components of $x_m$ followed by the three components of $r_m$ as a single row matrix. No rounding is required, and no physical units are needed. Define Generalized Minimal Residual (GMRES) on its first use and proceed from first principles, starting from the Krylov subspace construction and orthonormalization.", "solution": "The Generalized Minimal Residual (GMRES) method is an iterative algorithm for solving the linear system of equations $Ax = b$, particularly when the matrix $A$ is large, sparse, and nonsymmetric. Starting with an initial guess $x_0$, GMRES finds an approximate solution $x_m$ from the affine space $x_0 + K_m(A, r_0)$, where $K_m(A, r_0) = \\text{span}\\{r_0, Ar_0, \\dots, A^{m-1}r_0\\}$ is the $m$-th Krylov subspace generated by $A$ and the initial residual $r_0 = b - Ax_0$. The iterate $x_m$ is chosen to minimize the Euclidean norm of the residual, i.e., $x_m = \\arg \\min_{z \\in x_0 + K_m} \\|b - Az\\|_2$.\n\nThe solution process involves three main stages:\n1.  Use the Arnoldi process to generate an orthonormal basis $V_{m+1}$ for the Krylov subspace $K_{m+1}(A, r_0)$ and an upper Hessenberg matrix $\\bar{H}_m$.\n2.  Solve a small least-squares problem to find the vector $y_m$ that minimizes the residual in the Krylov subspace.\n3.  Compute the approximate solution $x_m = x_0 + V_m y_m$ and the corresponding residual $r_m = b - A x_m$.\n\nWe will now execute these steps for the given problem with $m=2$.\n\n**1. Initial Setup and Arnoldi Process**\n\nFirst, compute the initial residual $r_0$:\n$$\nr_0 = b - A x_0 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 1 & 0 & 0 \\\\ 1 & 3 & 0 \\\\ 0 & 1 & 2 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\nThe norm is $\\|r_0\\|_2 = \\sqrt{1^2 + 0^2 + 0^2} = 1$.\nThe first basis vector of the Krylov subspace is:\n$$\nv_1 = \\frac{r_0}{\\|r_0\\|_2} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\nNow, we perform $m=2$ steps of the Arnoldi process.\n\n**Step k=1:**\nCompute $w_1 = A v_1$:\n$$\nw_1 = \\begin{pmatrix} 1 & 0 & 0 \\\\ 1 & 3 & 0 \\\\ 0 & 1 & 2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix}\n$$\nOrthogonalize $w_1$ against the previous basis vectors (only $v_1$):\n$$\nh_{1,1} = v_1^T w_1 = \\begin{pmatrix} 1 & 0 & 0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix} = 1\n$$\n$$\n\\tilde{w}_1 = w_1 - h_{1,1} v_1 = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix} - 1 \\cdot \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}\n$$\nCompute the norm and the next basis vector:\n$$\nh_{2,1} = \\|\\tilde{w}_1\\|_2 = \\sqrt{0^2 + 1^2 + 0^2} = 1\n$$\n$$\nv_2 = \\frac{\\tilde{w}_1}{h_{2,1}} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}\n$$\n\n**Step k=2:**\nCompute $w_2 = A v_2$:\n$$\nw_2 = \\begin{pmatrix} 1 & 0 & 0 \\\\ 1 & 3 & 0 \\\\ 0 & 1 & 2 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 3 \\\\ 1 \\end{pmatrix}\n$$\nOrthogonalize $w_2$ against $v_1$ and $v_2$:\n$$\nh_{1,2} = v_1^T w_2 = \\begin{pmatrix} 1 & 0 & 0 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 3 \\\\ 1 \\end{pmatrix} = 0\n$$\n$$\nh_{2,2} = v_2^T w_2 = \\begin{pmatrix} 0 & 1 & 0 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 3 \\\\ 1 \\end{pmatrix} = 3\n$$\n$$\n\\tilde{w}_2 = w_2 - h_{1,2} v_1 - h_{2,2} v_2 = \\begin{pmatrix} 0 \\\\ 3 \\\\ 1 \\end{pmatrix} - 0 \\cdot \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} - 3 \\cdot \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix}\n$$\nCompute the norm and the next basis vector:\n$$\nh_{3,2} = \\|\\tilde{w}_2\\|_2 = \\sqrt{0^2 + 0^2 + 1^2} = 1\n$$\n$$\nv_3 = \\frac{\\tilde{w}_2}{h_{3,2}} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix}\n$$\nThe Arnoldi process yields the orthonormal basis matrix $V_3 = [v_1, v_2, v_3] = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} = I$ and the $(m+1) \\times m = 3 \\times 2$ upper Hessenberg matrix $\\bar{H}_2$:\n$$\n\\bar{H}_2 = \\begin{pmatrix} h_{1,1} & h_{1,2} \\\\ h_{2,1} & h_{2,2} \\\\ 0 & h_{3,2} \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 1 & 3 \\\\ 0 & 1 \\end{pmatrix}\n$$\n\n**2. Least-Squares Problem and Givens Rotations**\n\nThe GMRES iterate is $x_m = x_0 + V_m y_m$, where $y_m$ solves the least-squares problem:\n$$\ny_m = \\arg \\min_{y \\in \\mathbb{R}^m} \\| \\|r_0\\|_2 e_1 - \\bar{H}_m y \\|_2\n$$\nFor $m=2$, with $\\|r_0\\|_2=1$, this becomes:\n$$\ny_2 = \\arg \\min_{y \\in \\mathbb{R}^2} \\left\\| \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 1 & 0 \\\\ 1 & 3 \\\\ 0 & 1 \\end{pmatrix} y \\right\\|_2\n$$\nWe solve this by applying a sequence of Givens rotations to transform $\\bar{H}_2$ into an upper triangular matrix $R_2$.\n\n**First Givens Rotation $\\Omega_1$:**\nTo eliminate the element $h_{2,1}=1$, we apply a rotation on rows $1$ and $2$.\nLet $a = h_{1,1} = 1$ and $b = h_{2,1} = 1$. The rotation coefficients are $c = \\frac{a}{\\sqrt{a^2+b^2}} = \\frac{1}{\\sqrt{2}}$ and $s = \\frac{b}{\\sqrt{a^2+b^2}} = \\frac{1}{\\sqrt{2}}$.\nThe rotation matrix is $\\Omega_1 = \\begin{pmatrix} c & s & 0 \\\\ -s & c & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 1/\\sqrt{2} & 1/\\sqrt{2} & 0 \\\\ -1/\\sqrt{2} & 1/\\sqrt{2} & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}$.\nApplying $\\Omega_1$ to $\\bar{H}_2$:\n$$\n\\Omega_1 \\bar{H}_2 = \\begin{pmatrix} 1/\\sqrt{2} & 1/\\sqrt{2} & 0 \\\\ -1/\\sqrt{2} & 1/\\sqrt{2} & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 1 & 3 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} \\sqrt{2} & 3/\\sqrt{2} \\\\ 0 & 3/\\sqrt{2} \\\\ 0 & 1 \\end{pmatrix}\n$$\nApplying $\\Omega_1$ to the right-hand side vector $g = \\|r_0\\|_2 e_1 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}$:\n$$\n\\Omega_1 g = \\begin{pmatrix} 1/\\sqrt{2} & 1/\\sqrt{2} & 0 \\\\ -1/\\sqrt{2} & 1/\\sqrt{2} & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1/\\sqrt{2} \\\\ -1/\\sqrt{2} \\\\ 0 \\end{pmatrix}\n$$\n\n**Second Givens Rotation $\\Omega_2$:**\nTo eliminate the element at position $(3,2)$, which is $1$, we apply a rotation on rows $2$ and $3$ of the transformed system.\nLet $a = 3/\\sqrt{2}$ and $b = 1$. The rotation coefficients are $c = \\frac{a}{\\sqrt{a^2+b^2}} = \\frac{3/\\sqrt{2}}{\\sqrt{9/2+1}} = \\frac{3/\\sqrt{2}}{\\sqrt{11/2}} = \\frac{3}{\\sqrt{11}}$ and $s = \\frac{b}{\\sqrt{a^2+b^2}} = \\frac{1}{\\sqrt{11/2}} = \\frac{\\sqrt{2}}{\\sqrt{11}}$.\nThe rotation matrix is $\\Omega_2 = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & c & s \\\\ 0 & -s & c \\end{pmatrix} = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 3/\\sqrt{11} & \\sqrt{2}/\\sqrt{11} \\\\ 0 & -\\sqrt{2}/\\sqrt{11} & 3/\\sqrt{11} \\end{pmatrix}$.\nApplying $\\Omega_2$ to $\\Omega_1 \\bar{H}_2$:\n$$\n\\Omega_2 (\\Omega_1 \\bar{H}_2) = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 3/\\sqrt{11} & \\sqrt{2}/\\sqrt{11} \\\\ 0 & -\\sqrt{2}/\\sqrt{11} & 3/\\sqrt{11} \\end{pmatrix} \\begin{pmatrix} \\sqrt{2} & 3/\\sqrt{2} \\\\ 0 & 3/\\sqrt{2} \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} \\sqrt{2} & 3/\\sqrt{2} \\\\ 0 & \\frac{9}{\\sqrt{22}} + \\frac{\\sqrt{2}}{\\sqrt{11}} \\\\ 0 & \\frac{-3\\sqrt{2}}{\\sqrt{22}} + \\frac{3}{\\sqrt{11}} \\end{pmatrix} = \\begin{pmatrix} \\sqrt{2} & 3/\\sqrt{2} \\\\ 0 & \\sqrt{11/2} \\\\ 0 & 0 \\end{pmatrix}\n$$\nThis gives the upper triangular matrix $R_2 = \\begin{pmatrix} \\sqrt{2} & 3/\\sqrt{2} \\\\ 0 & \\sqrt{11/2} \\end{pmatrix}$.\nApplying $\\Omega_2$ to the transformed right-hand side $\\Omega_1 g$:\n$$\ng' = \\Omega_2 (\\Omega_1 g) = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 3/\\sqrt{11} & \\sqrt{2}/\\sqrt{11} \\\\ 0 & -\\sqrt{2}/\\sqrt{11} & 3/\\sqrt{11} \\end{pmatrix} \\begin{pmatrix} 1/\\sqrt{2} \\\\ -1/\\sqrt{2} \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1/\\sqrt{2} \\\\ -3/\\sqrt{22} \\\\ 1/\\sqrt{11} \\end{pmatrix}\n$$\nThe least-squares problem is now equivalent to solving $R_2 y_2 = g'_1$, where $g'_1$ consists of the first two components of $g'$.\n$$\n\\begin{pmatrix} \\sqrt{2} & 3/\\sqrt{2} \\\\ 0 & \\sqrt{11/2} \\end{pmatrix} \\begin{pmatrix} y_1 \\\\ y_2 \\end{pmatrix} = \\begin{pmatrix} 1/\\sqrt{2} \\\\ -3/\\sqrt{22} \\end{pmatrix}\n$$\nSolving by back substitution:\nFrom the second row:\n$\\sqrt{11/2} \\cdot y_2 = -3/\\sqrt{22} \\implies y_2 = \\frac{-3}{\\sqrt{22}} \\frac{\\sqrt{2}}{\\sqrt{11}} = \\frac{-3}{\\sqrt{11}\\sqrt{2}} \\frac{\\sqrt{2}}{\\sqrt{11}} = -\\frac{3}{11}$.\nFrom the first row:\n$\\sqrt{2} \\cdot y_1 + (3/\\sqrt{2}) y_2 = 1/\\sqrt{2} \\implies \\sqrt{2} \\cdot y_1 + (3/\\sqrt{2})(-3/11) = 1/\\sqrt{2}$.\n$\\sqrt{2} \\cdot y_1 = 1/\\sqrt{2} + 9/(11\\sqrt{2}) = \\frac{11+9}{11\\sqrt{2}} = \\frac{20}{11\\sqrt{2}}$.\n$y_1 = \\frac{20}{11\\sqrt{2}} \\frac{1}{\\sqrt{2}} = \\frac{20}{22} = \\frac{10}{11}$.\nSo, $y_2 = \\begin{pmatrix} 10/11 \\\\ -3/11 \\end{pmatrix}$.\n\n**3. Compute Solution and Residual**\n\nThe GMRES iterate $x_2$ is:\n$$\nx_2 = x_0 + V_2 y_2 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 0 & 0 \\end{pmatrix} \\begin{pmatrix} 10/11 \\\\ -3/11 \\end{pmatrix} = \\begin{pmatrix} 10/11 \\\\ -3/11 \\\\ 0 \\end{pmatrix}\n$$\nThe final residual $r_2$ is:\n$$\nr_2 = b - A x_2 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 1 & 0 & 0 \\\\ 1 & 3 & 0 \\\\ 0 & 1 & 2 \\end{pmatrix} \\begin{pmatrix} 10/11 \\\\ -3/11 \\\\ 0 \\end{pmatrix}\n$$\n$$\nA x_2 = \\begin{pmatrix} 1 \\cdot (10/11) \\\\ 1 \\cdot (10/11) + 3 \\cdot (-3/11) \\\\ 1 \\cdot (-3/11) \\end{pmatrix} = \\begin{pmatrix} 10/11 \\\\ 1/11 \\\\ -3/11 \\end{pmatrix}\n$$\n$$\nr_2 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 10/11 \\\\ 1/11 \\\\ -3/11 \\end{pmatrix} = \\begin{pmatrix} 1/11 \\\\ -1/11 \\\\ 3/11 \\end{pmatrix}\n$$\nThe final result consists of the three components of $x_2$ followed by the three components of $r_2$.\n$x_2 = (10/11, -3/11, 0)$\n$r_2 = (1/11, -1/11, 3/11)$", "answer": "$$\n\\boxed{\n\\begin{pmatrix} \\frac{10}{11} & -\\frac{3}{11} & 0 & \\frac{1}{11} & -\\frac{1}{11} & \\frac{3}{11} \\end{pmatrix}\n}\n$$", "id": "3616841"}, {"introduction": "An expert's understanding of an algorithm includes knowing not just how it works, but how it behaves in special or extreme cases. This conceptual problem [@problem_id:3244797] explores the behavior of GMRES under the unique condition that the initial residual is an eigenvector of the system matrix. By analyzing this scenario, you will uncover the direct relationship between the structure of the Krylov subspace and the algorithm's performance, revealing why GMRES can sometimes converge in a single step or stagnate completely.", "problem": "Consider the unpreconditioned Generalized Minimal Residual method (GMRES) applied over the complex field to the linear system $A x = b$, where $A \\in \\mathbb{C}^{n \\times n}$ and $b \\in \\mathbb{C}^{n}$. Let the initial guess be $x_0 \\in \\mathbb{C}^{n}$ with initial residual $r_0 = b - A x_0$. Assume the linear system is consistent (that is, there exists at least one $x \\in \\mathbb{C}^{n}$ such that $A x = b$). GMRES at iteration $k$ seeks $x_k \\in x_0 + \\mathcal{K}_k(A, r_0)$ that minimizes the residual norm $\\|b - A x_k\\|_2$, where the $k$th Krylov subspace is $\\mathcal{K}_k(A, r_0) = \\operatorname{span}\\{r_0, A r_0, \\dots, A^{k-1} r_0\\}$.\n\nSuppose further that $r_0$ is an eigenvector of $A$ with eigenvalue $\\lambda \\in \\mathbb{C}$, that is, $A r_0 = \\lambda r_0$.\n\nSelect all statements that are correct under these assumptions.\n\nA. If $\\lambda \\neq 0$, unrestarted GMRES converges to a (possibly nonunique) exact solution in $1$ iteration.\n\nB. If $\\lambda = 0$, then for all $k \\geq 1$, the GMRES residual norm satisfies $\\|r_k\\|_2 = \\|r_0\\|_2$, that is, GMRES stagnates completely.\n\nC. For every $k \\geq 1$, the Krylov subspace $\\mathcal{K}_k(A, r_0)$ has dimension at most $1$.\n\nD. If $\\lambda$ is nonreal, then even over $\\mathbb{C}$, GMRES cannot reduce the residual norm in any iteration.\n\nE. For any restart parameter $m \\geq 1$, restarted GMRES$(m)$ either converges in $1$ iteration when $\\lambda \\neq 0$ or stagnates completely when $\\lambda = 0$, mirroring the unrestarted behavior.", "solution": "### Derivation and Solution\n\nThe core of the unpreconditioned GMRES method at iteration $k$ is to find an approximate solution $x_k$ of the form $x_k = x_0 + z_{k-1}$, where $z_{k-1} \\in \\mathcal{K}_k(A, r_0)$, that minimizes the Euclidean norm of the residual $r_k = b - A x_k$.\n\nThe residual can be expressed as:\n$$r_k = b - A(x_0 + z_{k-1}) = (b - A x_0) - A z_{k-1} = r_0 - A z_{k-1}$$\nSince $z_{k-1} \\in \\mathcal{K}_k(A, r_0)$, it can be written as a linear combination of the Krylov basis vectors:\n$$z_{k-1} = c_0 r_0 + c_1 A r_0 + \\dots + c_{k-1} A^{k-1} r_0$$\nfor some scalars $c_0, c_1, \\dots, c_{k-1} \\in \\mathbb{C}$.\n\nThe key assumption is that $r_0$ is an eigenvector of $A$ with eigenvalue $\\lambda$, so $A r_0 = \\lambda r_0$. This has a profound impact on the structure of the Krylov subspace. By induction, we can show that $A^j r_0 = \\lambda^j r_0$ for any integer $j \\ge 0$.\n- Base case ($j=1$): $A^1 r_0 = \\lambda^1 r_0$ is given.\n- Inductive step: Assume $A^j r_0 = \\lambda^j r_0$. Then $A^{j+1} r_0 = A(A^j r_0) = A(\\lambda^j r_0) = \\lambda^j (A r_0) = \\lambda^j (\\lambda r_0) = \\lambda^{j+1} r_0$.\n\nTherefore, the $k$-th Krylov subspace is:\n$$\\mathcal{K}_k(A, r_0) = \\operatorname{span}\\{r_0, \\lambda r_0, \\lambda^2 r_0, \\dots, \\lambda^{k-1} r_0\\}$$\nIf $r_0 = 0$, the initial guess $x_0$ is an exact solution, and the problem is trivial. We assume $r_0 \\neq 0$. In this case, all generating vectors of the Krylov subspace are scalar multiples of $r_0$. Consequently, the subspace itself is simply the span of $r_0$:\n$$\\mathcal{K}_k(A, r_0) = \\operatorname{span}\\{r_0\\} \\quad \\text{for all } k \\ge 1$$\nThis means that the dimension of the search space for the update is at most $1$ and does not increase with $k$.\n\nAny vector $z_{k-1} \\in \\mathcal{K}_k(A, r_0)$ can be written as $z_{k-1} = \\alpha r_0$ for some scalar $\\alpha \\in \\mathbb{C}$. The GMRES iterate is $x_k = x_0 + \\alpha r_0$. The corresponding residual is:\n$$r_k = r_0 - A(\\alpha r_0) = r_0 - \\alpha (A r_0) = r_0 - \\alpha (\\lambda r_0) = (1 - \\alpha \\lambda) r_0$$\nGMRES chooses $\\alpha$ to minimize $\\|r_k\\|_2 = \\|(1 - \\alpha \\lambda) r_0\\|_2 = |1 - \\alpha \\lambda| \\|r_0\\|_2$. Since we assume $r_0 \\neq 0$, this is equivalent to minimizing the scalar factor $|1 - \\alpha \\lambda|$.\n\nWe analyze two cases based on the value of $\\lambda$.\n\n**Case 1: $\\lambda \\neq 0$**\nSince $\\lambda \\in \\mathbb{C}$ is non-zero, we can choose the scalar $\\alpha \\in \\mathbb{C}$ to be $\\alpha = 1/\\lambda$. With this choice:\n$$|1 - \\alpha \\lambda| = |1 - (1/\\lambda)\\lambda| = |1-1| = 0$$\nThis means the minimal residual norm is $0$. This minimum is achieved at the very first iteration ($k=1$), where the search space is $\\mathcal{K}_1(A, r_0) = \\operatorname{span}\\{r_0\\}$. GMRES will find this optimal $\\alpha$ and produce a residual $r_1=0$. Thus, $x_1$ is an exact solution, and the method converges in $1$ iteration.\n\n**Case 2: $\\lambda = 0$**\nIf the eigenvalue is $\\lambda = 0$, the residual expression becomes:\n$$r_k = (1 - \\alpha \\cdot 0) r_0 = r_0$$\nThe residual $r_k$ is equal to $r_0$ regardless of the choice of $\\alpha$. The residual norm is $\\|r_k\\|_2 = \\|r_0\\|_2$, which cannot be reduced. The method stagnates completely from the first iteration onward.\n\nNow we evaluate each statement.\n\n### Option-by-Option Analysis\n\n**A. If $\\lambda \\neq 0$, unrestarted GMRES converges to a (possibly nonunique) exact solution in $1$ iteration.**\nAs derived in Case 1, if $\\lambda \\neq 0$, GMRES finds a coefficient $\\alpha = 1/\\lambda$ that makes the residual $r_1$ equal to the zero vector. This means $x_1 = x_0 + (1/\\lambda)r_0$ is an exact solution. The problem statement guarantees that at least one solution exists (the system is consistent). If $A$ is singular, the solution may not be unique, but $x_1$ will be one of the solutions. Thus, the statement is accurate.\n**Verdict: Correct**\n\n**B. If $\\lambda = 0$, then for all $k \\geq 1$, the GMRES residual norm satisfies $\\|r_k\\|_2 = \\|r_0\\|_2$, that is, GMRES stagnates completely.**\nAs derived in Case 2, if $\\lambda = 0$, the residual at any step $k$ is $r_k = r_0$. The minimal residual norm is therefore $\\|r_0\\|_2$. The residual norm does not decrease at all. This is the definition of complete stagnation. The consistency of the system $Ax=b$ combined with $Ar_0=0$ implies that $r_0$ must lie in the range of $A$, which is a required condition for this scenario to be mathematically sound, but this is guaranteed by the problem's premises. The conclusion of stagnation holds.\n**Verdict: Correct**\n\n**C. For every $k \\geq 1$, the Krylov subspace $\\mathcal{K}_k(A, r_0)$ has dimension at most $1$.**\nAs shown in the initial derivation, $A^j r_0 = \\lambda^j r_0$ for all $j \\ge 0$. The subspace $\\mathcal{K}_k(A, r_0)$ is the span of vectors $\\{r_0, \\lambda r_0, \\dots, \\lambda^{k-1} r_0\\}$, all of which are collinear with $r_0$. If $r_0 \\neq 0$, the dimension of this subspace is exactly $1$. If $r_0 = 0$, the dimension is $0$. In all cases, the dimension is at most $1$.\n**Verdict: Correct**\n\n**D. If $\\lambda$ is nonreal, then even over $\\mathbb{C}$, GMRES cannot reduce the residual norm in any iteration.**\nA nonreal $\\lambda$ is a specific instance of $\\lambda \\neq 0$. As established in the analysis of option A, for any non-zero $\\lambda \\in \\mathbb{C}$ (including nonreal values), GMRES can find a complex scalar $\\alpha = 1/\\lambda$ that reduces the residual norm to $0$ in a single iteration. This statement is therefore false.\n**Verdict: Incorrect**\n\n**E. For any restart parameter $m \\geq 1$, restarted GMRES$(m)$ either converges in $1$ iteration when $\\lambda \\neq 0$ or stagnates completely when $\\lambda = 0$, mirroring the unrestarted behavior.**\nGMRES($m$) executes at most $m$ iterations of standard GMRES and then restarts.\n- If $\\lambda \\neq 0$: The first (unrestarted) cycle of GMRES will find the exact solution at its very first step ($k=1$), as shown in A. The algorithm will terminate with an exact solution without ever completing the first $m$-step cycle or restarting. Thus, it converges in $1$ iteration.\n- If $\\lambda = 0$: In the first cycle of $m$ steps, the residual remains $r_k = r_0$ for all $k \\in \\{1, \\dots, m\\}$. At the end of the cycle, the new approximation is $x_m$ and the residual is $r_m = r_0$. The method restarts with a new initial guess $x_{_0}^{\\prime} = x_m$. The new initial residual is $r_{_0}^{\\prime} = b - A x_{_0}^{\\prime} = b - A x_m = r_m = r_0$. The conditions for the next cycle are identical to the original problem: the initial residual is $r_0$, which is an eigenvector of $A$ with eigenvalue $\\lambda=0$. The process will repeat, with the residual never decreasing. This is complete stagnation.\nThe statement accurately describes the behavior in both scenarios.\n**Verdict: Correct**", "answer": "$$\\boxed{ABCE}$$", "id": "3244797"}]}