{"hands_on_practices": [{"introduction": "Mastering the analysis of stationary methods begins with the fundamental skill of constructing the iteration matrix and calculating its spectral radius. This exercise provides direct, hands-on practice with the Jacobi method for a simple, well-behaved system. By explicitly deriving the iteration matrix and its eigenvalues, you will reinforce the core principle that the spectral radius governs the method's convergence [@problem_id:3542428].", "problem": "Consider the linear system $A x = b$ with \n$$\nA = \\begin{pmatrix}\n5  -1  0 \\\\\n-1  5  -1 \\\\\n0  -1  5\n\\end{pmatrix}.\n$$\nThe matrix $A$ is strictly diagonally dominant. Using the basic splitting of a matrix into its diagonal part $D$ and the remainder $R$, construct the Jacobi stationary iteration matrix for this system, compute its eigenvalues exactly, and determine the spectral radius. Use the following foundational facts only: the Jacobi stationary method is obtained by isolating the diagonal of $A$ and iterating with a fixed linear operator; convergence of a stationary method is governed by the modulus of the eigenvalues of the iteration matrix, called the spectral radius. Express your final answer as a single closed-form analytic expression for the spectral radius. No rounding is required.", "solution": "The problem as stated constitutes a valid and well-posed exercise in numerical linear algebra. It provides a specific matrix $A$, specifies the use of the Jacobi stationary method, and asks for the computation of the spectral radius of the corresponding iteration matrix. All data and definitions required for the solution are present and self-consistent. The problem adheres to established scientific principles and is amenable to a unique, exact solution.\n\nThe problem is to solve the linear system $A x = b$ using a stationary iterative method, where the matrix $A$ is given by:\n$$\nA = \\begin{pmatrix}\n5  -1  0 \\\\\n-1  5  -1 \\\\\n0  -1  5\n\\end{pmatrix}\n$$\nThe Jacobi method is defined by splitting the matrix $A$ into its diagonal part, $D$, its strictly lower triangular part, $-L$, and its strictly upper triangular part, $-U$. Thus, $A = D - L - U$. The iterative scheme is derived from $A x = b$ by rewriting it as $(D - L - U)x = b$, which is rearranged to $D x = (L+U)x + b$.\n\nFor a general iteration $k$, this gives the Jacobi iterative update rule:\n$$\nD x^{(k+1)} = (L+U)x^{(k)} + b\n$$\nMultiplying by the inverse of the diagonal matrix, $D^{-1}$, we obtain the explicit form of the iteration:\n$$\nx^{(k+1)} = D^{-1}(L+U)x^{(k)} + D^{-1}b\n$$\nThis is a stationary method of the form $x^{(k+1)} = T x^{(k)} + c$, where the iteration matrix is $T_J = D^{-1}(L+U)$ and the constant vector is $c = D^{-1}b$. The convergence of this method depends on the spectral radius of the iteration matrix $T_J$, denoted $\\rho(T_J)$, which is defined as the maximum absolute value of its eigenvalues. The method converges if and only if $\\rho(T_J)  1$.\n\nFirst, we decompose the given matrix $A$ into its components $D$, $L$, and $U$.\n$$\nD = \\begin{pmatrix}\n5  0  0 \\\\\n0  5  0 \\\\\n0  0  5\n\\end{pmatrix}\n$$\n$$\n-L = \\begin{pmatrix}\n0  0  0 \\\\\n-1  0  0 \\\\\n0  -1  0\n\\end{pmatrix} \\implies L = \\begin{pmatrix}\n0  0  0 \\\\\n1  0  0 \\\\\n0  1  0\n\\end{pmatrix}\n$$\n$$\n-U = \\begin{pmatrix}\n0  -1  0 \\\\\n0  0  -1 \\\\\n0  0  0\n\\end{pmatrix} \\implies U = \\begin{pmatrix}\n0  1  0 \\\\\n0  0  1 \\\\\n0  0  0\n\\end{pmatrix}\n$$\nThe inverse of the diagonal matrix $D$ is:\n$$\nD^{-1} = \\begin{pmatrix}\n\\frac{1}{5}  0  0 \\\\\n0  \\frac{1}{5}  0 \\\\\n0  0  \\frac{1}{5}\n\\end{pmatrix} = \\frac{1}{5}I\n$$\nwhere $I$ is the $3 \\times 3$ identity matrix.\nThe sum $L+U$ is:\n$$\nL+U = \\begin{pmatrix}\n0  1  0 \\\\\n1  0  1 \\\\\n0  1  0\n\\end{pmatrix}\n$$\nNow, we construct the Jacobi iteration matrix $T_J = D^{-1}(L+U)$:\n$$\nT_J = \\frac{1}{5} \\begin{pmatrix}\n1  0  0 \\\\\n0  1  0 \\\\\n0  0  1\n\\end{pmatrix}\n\\begin{pmatrix}\n0  1  0 \\\\\n1  0  1 \\\\\n0  1  0\n\\end{pmatrix}\n= \\frac{1}{5} \\begin{pmatrix}\n0  1  0 \\\\\n1  0  1 \\\\\n0  1  0\n\\end{pmatrix}\n= \\begin{pmatrix}\n0  \\frac{1}{5}  0 \\\\\n\\frac{1}{5}  0  \\frac{1}{5} \\\\\n0  \\frac{1}{5}  0\n\\end{pmatrix}\n$$\nTo find the spectral radius of $T_J$, we must compute its eigenvalues. The eigenvalues, $\\lambda$, are the roots of the characteristic equation $\\det(T_J - \\lambda I) = 0$.\n$$\n\\det(T_J - \\lambda I) = \\det \\begin{pmatrix}\n-\\lambda  \\frac{1}{5}  0 \\\\\n\\frac{1}{5}  -\\lambda  \\frac{1}{5} \\\\\n0  \\frac{1}{5}  -\\lambda\n\\end{pmatrix} = 0\n$$\nWe compute the determinant by cofactor expansion along the first row:\n$$\n-\\lambda \\left| \\begin{matrix} -\\lambda  \\frac{1}{5} \\\\ \\frac{1}{5}  -\\lambda \\end{matrix} \\right| - \\frac{1}{5} \\left| \\begin{matrix} \\frac{1}{5}  \\frac{1}{5} \\\\ 0  -\\lambda \\end{matrix} \\right| + 0 = 0\n$$\n$$\n-\\lambda \\left( (-\\lambda)(-\\lambda) - (\\frac{1}{5})(\\frac{1}{5}) \\right) - \\frac{1}{5} \\left( (\\frac{1}{5})(-\\lambda) - (0)(\\frac{1}{5}) \\right) = 0\n$$\n$$\n-\\lambda \\left( \\lambda^2 - \\frac{1}{25} \\right) - \\frac{1}{5} \\left( -\\frac{\\lambda}{5} \\right) = 0\n$$\n$$\n-\\lambda^3 + \\frac{\\lambda}{25} + \\frac{\\lambda}{25} = 0\n$$\n$$\n-\\lambda^3 + \\frac{2\\lambda}{25} = 0\n$$\n$$\n-\\lambda \\left( \\lambda^2 - \\frac{2}{25} \\right) = 0\n$$\nThis equation yields three eigenvalues for the matrix $T_J$:\n$$\n\\lambda_1 = 0\n$$\n$$\n\\lambda^2 = \\frac{2}{25} \\implies \\lambda_{2,3} = \\pm \\sqrt{\\frac{2}{25}} = \\pm \\frac{\\sqrt{2}}{5}\n$$\nThe eigenvalues are $\\lambda_1 = 0$, $\\lambda_2 = \\frac{\\sqrt{2}}{5}$, and $\\lambda_3 = -\\frac{\\sqrt{2}}{5}$.\n\nThe spectral radius, $\\rho(T_J)$, is the maximum of the absolute values (moduli) of these eigenvalues:\n$$\n\\rho(T_J) = \\max \\left\\{ |\\lambda_1|, |\\lambda_2|, |\\lambda_3| \\right\\}\n$$\n$$\n\\rho(T_J) = \\max \\left\\{ |0|, \\left|\\frac{\\sqrt{2}}{5}\\right|, \\left|-\\frac{\\sqrt{2}}{5}\\right| \\right\\}\n$$\n$$\n\\rho(T_J) = \\max \\left\\{ 0, \\frac{\\sqrt{2}}{5}, \\frac{\\sqrt{2}}{5} \\right\\}\n$$\nTherefore, the spectral radius of the Jacobi iteration matrix is $\\frac{\\sqrt{2}}{5}$. Since $\\rho(T_J) = \\frac{\\sqrt{2}}{5} \\approx \\frac{1.414}{5} \\approx 0.2828  1$, the Jacobi method converges for this system, which is expected as the matrix $A$ is strictly diagonally dominant.", "answer": "$$\\boxed{\\frac{\\sqrt{2}}{5}}$$", "id": "3542428"}, {"introduction": "While the spectral radius is the ultimate arbiter of convergence, it only describes the long-term, asymptotic behavior of an iteration. This practice delves into the critical distinction between the spectral radius, $\\rho(M)$, and matrix norms, such as $\\\\|M\\\\|_2$, exploring how non-normal matrices can exhibit transient error growth even when they are guaranteed to converge [@problem_id:3542458]. Through carefully chosen examples, you will learn why the condition $\\rho(M) \\lt 1$ is necessary and sufficient for convergence, whereas the stricter condition $\\\\|M\\\\|_2 \\lt 1$ is sufficient but not necessary, and how this distinction impacts the practical performance of iterative methods.", "problem": "Consider the Stationary Iterative Method (SIM) for solving a linear system, defined by the recurrence $x^{(k+1)} = M x^{(k)} + c$ for a fixed iteration matrix $M \\in \\mathbb{C}^{n \\times n}$ and vector $c \\in \\mathbb{C}^{n}$. Let the error be $e^{(k)} = x^{(k)} - x^\\star$, where $x^\\star$ is a fixed point satisfying $x^\\star = M x^\\star + c$. Then $e^{(k+1)} = M e^{(k)}$ and $e^{(k)} = M^k e^{(0)}$. By definition, the spectral radius $\\rho(M)$ is $\\rho(M) = \\max\\{|\\lambda| : \\lambda \\text{ is an eigenvalue of } M\\}$, and the induced $2$-norm is $\\|M\\|_2 = \\sup_{\\|x\\|_2 = 1} \\|M x\\|_2$, equal to the largest singular value of $M$. It is a fundamental fact that for any induced matrix norm, $\\rho(M) \\le \\|M\\|$, and that $e^{(k)} \\to 0$ for all $e^{(0)}$ if and only if $\\rho(M)  1$. In practice, some stopping rules use $\\|x^{(k+1)}-x^{(k)}\\|_2$ or bounds derived from $\\|M\\|_2$ to infer or estimate convergence.\n\nEvaluate the following options about specific matrices and related convergence assessments. Select all options that are correct.\n\n- Option A: Let $M_A = \\mathrm{diag}\\!\\left(\\tfrac{3}{5}, \\tfrac{4}{5}, -\\tfrac{2}{5}\\right)$. Then $\\|M_A\\|_2  1$ and $\\rho(M_A)  1$, and the SIM converges for all initial guesses $x_0$. In this case, a norm-based contraction argument using $\\|M_A\\|_2$ correctly certifies convergence.\n\n- Option B: Let $M_B = \\begin{pmatrix} \\tfrac{1}{2}  1 \\\\ 0  \\tfrac{1}{2} \\end{pmatrix}$. Then $\\rho(M_B)  1$ but $\\|M_B\\|_2  1$, and the SIM converges for all initial guesses $x_0$. A rule of the form “declare divergence if $\\|M\\|_2  1$” would misclassify this case. Moreover, for certain initial errors $e_0$, the sequence $\\|e^{(k)}\\|_2$ and the difference $\\|x^{(k+1)}-x^{(k)}\\|_2$ can initially increase before eventually decreasing, so monotonic-decrease-based stopping criteria may misinterpret convergence.\n\n- Option C: Let $M_C = \\begin{pmatrix} 1  1 \\\\ 0  1 \\end{pmatrix}$. The SIM converges because $\\rho(M_C) = 1$ while $\\|M_C\\|_2 = \\sqrt{2}  1$, and in this case a norm-based stopping rule that requires $\\|M\\|_2  1$ would incorrectly reject a convergent iteration.\n\n- Option D: For any induced matrix norm $\\|\\cdot\\|$, the condition $\\|M\\|  1$ is equivalent to $\\rho(M)  1$ for all matrices $M \\in \\mathbb{C}^{n \\times n}$.\n\n- Option E: If $\\rho(M)  1$, then for every initial error $e_0$ the error norms must be monotonically nonincreasing, specifically $\\|e^{(k+1)}\\|_2 \\le \\|e^{(k)}\\|_2$ for all $k \\ge 0$. Therefore, any stopping rule based on the monotonic decrease of $\\|x^{(k+1)}-x^{(k)}\\|_2$ correctly detects convergence without risk of misinterpretation.\n\nChoose all correct options from A–E.", "solution": "The problem statement is critically validated before proceeding to a solution.\n\n### Step 1: Extract Givens\n-   Stationary Iterative Method (SIM): $x^{(k+1)} = M x^{(k)} + c$ for a fixed iteration matrix $M \\in \\mathbb{C}^{n \\times n}$ and vector $c \\in \\mathbb{C}^{n}$.\n-   Error vector: $e^{(k)} = x^{(k)} - x^\\star$, where $x^\\star$ is a fixed point satisfying $x^\\star = M x^\\star + c$.\n-   Error recurrence relation: $e^{(k+1)} = M e^{(k)}$ and $e^{(k)} = M^k e^{(0)}$.\n-   Spectral radius definition: $\\rho(M) = \\max\\{|\\lambda| : \\lambda \\text{ is an eigenvalue of } M\\}$.\n-   Induced $2$-norm definition: $\\|M\\|_2 = \\sup_{\\|x\\|_2 = 1} \\|M x\\|_2$, which is equal to the largest singular value of $M$.\n-   Fundamental Fact 1: For any induced matrix norm, $\\rho(M) \\le \\|M\\|$.\n-   Fundamental Fact 2 (Convergence Criterion): The error $e^{(k)} \\to 0$ for all initial errors $e^{(0)}$ if and only if $\\rho(M)  1$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is a standard exposition on the convergence of stationary iterative methods in numerical linear algebra. All definitions, such as the spectral radius and induced $2$-norm, are correct. The provided \"fundamental facts\" are cornerstone theorems of the field. The questions posed in the options are designed to test the understanding of the relationship between the spectral radius, matrix norms, and the transient and asymptotic behavior of the iterative process. The problem is scientifically grounded, well-posed, and objective. It contains no contradictions, ambiguities, or unsound premises.\n\n### Step 3: Verdict and Action\nThe problem statement is valid. A full analysis of each option is warranted.\n\n### Analysis of Options\n\n**Option A:**\nLet $M_A = \\mathrm{diag}\\!\\left(\\tfrac{3}{5}, \\tfrac{4}{5}, -\\tfrac{2}{5}\\right)$.\nThis is a diagonal matrix, which is a special case of a normal matrix (a matrix $M$ such that $M M^* = M^* M$, where $M^*$ is the conjugate transpose of $M$).\nThe eigenvalues of a diagonal matrix are its diagonal entries: $\\lambda_1 = \\frac{3}{5}$, $\\lambda_2 = \\frac{4}{5}$, and $\\lambda_3 = -\\frac{2}{5}$.\nThe spectral radius is the maximum magnitude of the eigenvalues:\n$$ \\rho(M_A) = \\max\\left\\{\\left|\\frac{3}{5}\\right|, \\left|\\frac{4}{5}\\right|, \\left|-\\frac{2}{5}\\right|\\right\\} = \\max\\left\\{\\frac{3}{5}, \\frac{4}{5}, \\frac{2}{5}\\right\\} = \\frac{4}{5} $$\nSince $\\rho(M_A) = \\frac{4}{5}  1$, the fundamental convergence criterion guarantees that the SIM converges for all initial guesses $x_0$.\nFor any normal matrix, the induced $2$-norm is equal to its spectral radius. Thus,\n$$ \\|M_A\\|_2 = \\rho(M_A) = \\frac{4}{5} $$\nSo, we have $\\|M_A\\|_2  1$ and $\\rho(M_A)  1$, and convergence is guaranteed.\nA norm-based contraction argument is based on showing that the iteration is a contraction mapping with respect to some norm. The error recurrence is $e^{(k+1)} = M e^{(k)}$. Taking the $2$-norm, we get:\n$$ \\|e^{(k+1)}\\|_2 = \\|M_A e^{(k)}\\|_2 \\le \\|M_A\\|_2 \\|e^{(k)}\\|_2 $$\nSince $\\|M_A\\|_2 = \\frac{4}{5}  1$, the error norm decreases at each step, i.e., $\\|e^{(k+1)}\\|_2 \\le \\frac{4}{5} \\|e^{(k)}\\|_2$. This proves convergence directly and certifies it, as stated. All parts of the statement are consistent and correct.\n\n**Verdict for Option A: Correct**\n\n**Option B:**\nLet $M_B = \\begin{pmatrix} \\tfrac{1}{2}  1 \\\\ 0  \\tfrac{1}{2} \\end{pmatrix}$.\nThis is an upper triangular matrix. Its eigenvalues are its diagonal entries: $\\lambda_1 = \\lambda_2 = \\frac{1}{2}$.\nThe spectral radius is $\\rho(M_B) = \\left|\\frac{1}{2}\\right| = \\frac{1}{2}$.\nSince $\\rho(M_B)  1$, the SIM converges for all initial guesses $x_0$.\nTo find the induced $2$-norm, we calculate the largest singular value of $M_B$. The singular values are the square roots of the eigenvalues of $M_B^* M_B$. Since $M_B$ is real, $M_B^* = M_B^T$.\n$$ M_B^T M_B = \\begin{pmatrix} \\tfrac{1}{2}  0 \\\\ 1  \\tfrac{1}{2} \\end{pmatrix} \\begin{pmatrix} \\tfrac{1}{2}  1 \\\\ 0  \\tfrac{1}{2} \\end{pmatrix} = \\begin{pmatrix} \\tfrac{1}{4}  \\tfrac{1}{2} \\\\ \\tfrac{1}{2}  \\tfrac{5}{4} \\end{pmatrix} $$\nThe characteristic equation for the eigenvalues $\\mu$ of $M_B^T M_B$ is $\\det(M_B^T M_B - \\mu I) = 0$:\n$$ \\left(\\frac{1}{4} - \\mu\\right)\\left(\\frac{5}{4} - \\mu\\right) - \\frac{1}{4} = 0 \\\\ \\mu^2 - \\frac{6}{4}\\mu + \\frac{5}{16} - \\frac{4}{16} = 0 \\\\ \\mu^2 - \\frac{3}{2}\\mu + \\frac{1}{16} = 0 $$\nThe largest eigenvalue is $\\mu_{\\max} = \\frac{\\frac{3}{2} + \\sqrt{\\frac{9}{4} - \\frac{4}{16}}}{2} = \\frac{\\frac{3}{2} + \\sqrt{\\frac{36}{16} - \\frac{4}{16}}}{2} = \\frac{\\frac{3}{2} + \\sqrt{\\frac{32}{16}}}{2} = \\frac{\\frac{3}{2} + \\sqrt{2}}{2} = \\frac{3 + 2\\sqrt{2}}{4}$.\nThe induced $2$-norm is $\\|M_B\\|_2 = \\sigma_{\\max} = \\sqrt{\\mu_{\\max}} = \\sqrt{\\frac{3+2\\sqrt{2}}{4}} = \\frac{\\sqrt{(1+\\sqrt{2})^2}}{2} = \\frac{1+\\sqrt{2}}{2}$.\nSince $\\sqrt{2} \\approx 1.414$, we have $\\|M_B\\|_2 \\approx \\frac{1+1.414}{2} = 1.207  1$.\nSo, $\\rho(M_B)  1$ (convergence) but $\\|M_B\\|_2  1$. A rule that declares divergence if $\\|M\\|_2  1$ would indeed misclassify this case.\nBecause $\\|M_B\\|_2  1$, there exists an initial error vector $e^{(0)}$ such that $\\|e^{(1)}\\|_2 = \\|M_B e^{(0)}\\|_2  \\|e^{(0)}\\|_2$. This demonstrates that the error norm $\\|e^{(k)}\\|_2$ can initially increase. This phenomenon is characteristic of non-normal matrices.\nSimilarly, consider the difference vector $\\Delta x^{(k)} = x^{(k+1)}-x^{(k)} = (M_B-I)e^{(k)}$. Then $\\Delta x^{(k+1)} = M_B \\Delta x^{(k)}$. The norm of this difference behaves like the error norm: $\\|\\Delta x^{(k+1)}\\|_2 = \\|M_B \\Delta x^{(k)}\\|_2$. One can choose an initial guess such that $\\Delta x^{(0)}$ is a vector that is maximally stretched by $M_B$, leading to $\\|\\Delta x^{(1)}\\|_2  \\|\\Delta x^{(0)}\\|_2$. Thus, a stopping criterion based on monotonic decrease of $\\|\\Delta x^{(k)}\\|_2$ can prematurely and incorrectly terminate.\n\n**Verdict for Option B: Correct**\n\n**Option C:**\nLet $M_C = \\begin{pmatrix} 1  1 \\\\ 0  1 \\end{pmatrix}$.\nThis is a Jordan block with eigenvalue $\\lambda=1$. The eigenvalues are $\\lambda_1 = \\lambda_2 = 1$.\nThe spectral radius is $\\rho(M_C) = 1$.\nThe fundamental criterion for convergence for *all* initial conditions is $\\rho(M)  1$. Here, this condition is violated. The statement \"The SIM converges because $\\rho(M_C) = 1$\" is false. To show it diverges, we compute powers of $M_C$. Let $M_C = I+N$ where $N=\\begin{pmatrix} 0  1 \\\\ 0  0 \\end{pmatrix}$ and $N^2=0$.\n$$ M_C^k = (I+N)^k = I + kN = \\begin{pmatrix} 1  k \\\\ 0  1 \\end{pmatrix} $$\nThe error at step $k$ is $e^{(k)} = M_C^k e^{(0)}$.\n$$ e^{(k)} = \\begin{pmatrix} 1  k \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} e_{0,1} \\\\ e_{0,2} \\end{pmatrix} = \\begin{pmatrix} e_{0,1} + k e_{0,2} \\\\ e_{0,2} \\end{pmatrix} $$\nFor $e^{(k)}$ to converge to the zero vector, we need both components to go to zero. The first component, $e_{0,1} + k e_{0,2}$, diverges to infinity if $e_{0,2} \\neq 0$. Thus, the method does not converge for arbitrary $e^{(0)}$. The first part of the statement is incorrect.\nFurthermore, the statement claims $\\|M_C\\|_2 = \\sqrt{2}$. Let's verify this.\n$$ M_C^T M_C = \\begin{pmatrix} 1  0 \\\\ 1  1 \\end{pmatrix} \\begin{pmatrix} 1  1 \\\\ 0  1 \\end{pmatrix} = \\begin{pmatrix} 1  1 \\\\ 1  2 \\end{pmatrix} $$\nEigenvalues $\\mu$ of $M_C^T M_C$ satisfy $(1-\\mu)(2-\\mu)-1 = 0$, which is $\\mu^2-3\\mu+1=0$.\nThe largest eigenvalue is $\\mu_{\\max} = \\frac{3+\\sqrt{9-4}}{2} = \\frac{3+\\sqrt{5}}{2}$.\nThe norm is $\\|M_C\\|_2 = \\sqrt{\\mu_{\\max}} = \\sqrt{\\frac{3+\\sqrt{5}}{2}} \\approx 1.618$. This is not equal to $\\sqrt{2} \\approx 1.414$.\nThe statement makes two incorrect claims: one about convergence and one about the value of the norm.\n\n**Verdict for Option C: Incorrect**\n\n**Option D:**\nThe statement is: \"For any induced matrix norm $\\|\\cdot\\|$, the condition $\\|M\\|  1$ is equivalent to $\\rho(M)  1$ for all matrices $M \\in \\mathbb{C}^{n \\times n}$.\"\nThis can be broken into two implications:\n1. $\\|M\\|  1 \\implies \\rho(M)  1$. This is true because of the fundamental fact $\\rho(M) \\le \\|M\\|$ for any induced norm. If $\\|M\\|  1$, then $\\rho(M)$ must also be less than $1$.\n2. $\\rho(M)  1 \\implies \\|M\\|  1$. This is false. A statement \"for any induced matrix norm\" requires it to hold for all of them. We have a direct counterexample from Option B.\nFor $M_B = \\begin{pmatrix} \\tfrac{1}{2}  1 \\\\ 0  \\tfrac{1}{2} \\end{pmatrix}$ and the induced $2$-norm, we found $\\rho(M_B) = \\frac{1}{2}  1$, but $\\|M_B\\|_2 = \\frac{1+\\sqrt{2}}{2}  1$.\nSince the reverse implication is false, the equivalence is false.\n\n**Verdict for Option D: Incorrect**\n\n**Option E:**\nThe statement is: \"If $\\rho(M)  1$, then for every initial error $e^{(0)}$ the error norms must be monotonically nonincreasing, specifically $\\|e^{(k+1)}\\|_2 \\le \\|e^{(k)}\\|_2$ for all $k \\ge 0$.\"\nThe condition $\\|e^{(k+1)}\\|_2 \\le \\|e^{(k)}\\|_2$ is equivalent to $\\|M e^{(k)}\\|_2 \\le \\|e^{(k)}\\|_2$. For this to hold for *every* $e^{(k)}$ (and thus for any vector in $\\mathbb{C}^n$), it would require that $\\|M v\\|_2 \\le \\|v\\|_2$ for all $v$. By definition, this is equivalent to $\\sup_{\\|v\\|_2=1} \\|M v\\|_2 \\le 1$, which means $\\|M\\|_2 \\le 1$.\nThe statement is therefore claiming that $\\rho(M)  1 \\implies \\|M\\|_2 \\le 1$.\nAs shown in the analysis of Options B and D, this implication is false. The matrix $M_B$ is a counterexample where $\\rho(M_B)  1$ but $\\|M_B\\|_2  1$. For this matrix, there exists an initial error $e^{(0)}$ such that $\\|e^{(1)}\\|_2  \\|e^{(0)}\\|_2$, contradicting the claim of monotonic non-increase.\nThe second sentence, \"Therefore, any stopping rule based on the monotonic decrease of $\\|x^{(k+1)}-x^{(k)}\\|_2$ correctly detects convergence without risk of misinterpretation,\" is a conclusion drawn from this false premise. As also shown in the analysis for Option B, the quantity $\\|x^{(k+1)}-x^{(k)}\\|_2$ can also exhibit transient growth even when the iteration converges. Thus, the conclusion is also false.\n\n**Verdict for Option E: Incorrect**", "answer": "$$\\boxed{AB}$$", "id": "3542458"}, {"introduction": "Theoretical insights into transient dynamics become concrete through computational experiments. This exercise challenges you to construct pairs of normal and non-normal matrices that share the same eigenvalues, allowing for a controlled study of how non-normality impacts convergence behavior [@problem_id:3542439]. By implementing code to track error norms over time, you will directly observe and quantify the transient growth that can occur, solidifying your understanding of the gap between the asymptotic rate predicted by the spectral radius and the actual finite-time performance of an iteration.", "problem": "Consider a linear stationary iterative method applied to a linear system, with error propagation described by the recurrence $e^{(k+1)} = M e^{(k)}$, where $M \\in \\mathbb{C}^{n \\times n}$ is the iteration matrix. The spectral radius of $M$ is defined as $\\rho(M) = \\max \\{ |\\lambda| : \\lambda \\in \\sigma(M) \\}$, where $\\sigma(M)$ denotes the spectrum of $M$. A matrix $M$ is called normal if $M^* M = M M^*$, where $M^*$ is the conjugate transpose. The spectral norm $\\|A\\|_2$ of a matrix $A$ is the largest singular value of $A$. Assume the test horizon $K \\in \\mathbb{N}$ and define the finite-time induced norm sequence $a_k(M) = \\|M^k\\|_2$ for $k \\in \\{1,2,\\dots,K\\}$. Define the finite-time geometric rate estimate $r_k(M) = a_k(M)^{1/k}$ for $k \\in \\{1,2,\\dots,K\\}$ and the finite-time excess over the asymptotic rate as $\\Delta(M;K) = \\max_{1 \\le k \\le K} \\max\\{0, r_k(M) - \\rho(M) \\}$. Define the transient growth factor $G(M;K) = \\max_{1 \\le k \\le K} a_k(M)$.\n\nYour task is to construct, for each test case specified below, two iteration matrices $M_1$ and $M_2$ such that:\n- $M_1$ is normal, $M_2$ is non-normal, and $\\sigma(M_1) = \\sigma(M_2)$,\n- $\\rho(M_i)  1$ for $i \\in \\{1,2\\}$.\n\nFor each pair $(M_1, M_2)$, compute and compare:\n- the shared spectral radius $\\rho = \\rho(M_i)$,\n- the excess rates $\\Delta(M_1;K)$ and $\\Delta(M_2;K)$,\n- the transient growth factors $G(M_1;K)$ and $G(M_2;K)$,\n- and the boolean normality indicators for $M_1$ and $M_2$ as determined numerically by the condition $\\|M^*M - MM^*\\|_F \\le \\tau$, where $\\|\\cdot\\|_F$ is the Frobenius norm and $\\tau$ is a small numerical tolerance chosen relative to $\\|M\\|_F$.\n\nConstruction requirements for each test case:\n- For test cases with repeated eigenvalue $\\alpha \\in (-1,1)$ and size $n \\in \\mathbb{N}$, use $M_1 = \\alpha I_n$ and $M_2 = J_n(\\alpha)$, where $I_n$ is the $n \\times n$ identity and $J_n(\\alpha)$ is the $n \\times n$ Jordan block with eigenvalue $\\alpha$ (that is, $(J_n(\\alpha))_{ii} = \\alpha$ and $(J_n(\\alpha))_{i,i+1} = 1$ for $i \\in \\{1,\\dots,n-1\\}$, with all other entries zero).\n- For test cases with distinct eigenvalues $\\Lambda = (\\lambda_1,\\dots,\\lambda_n)$ satisfying $\\max_i |\\lambda_i|  1$, use $M_1 = \\mathrm{diag}(\\Lambda)$ and $M_2 = V \\, \\mathrm{diag}(\\Lambda) \\, V^{-1}$ with $V = I_n + s N$, where $N$ is strictly upper triangular with ones in all entries above the main diagonal and zeros elsewhere, and $s  0$ is a given scalar.\n\nUse the following test suite of parameters:\n- Test case $1$: $(n,\\alpha,K) = (5, 0.9, 50)$.\n- Test case $2$: $\\Lambda = (0.2, 0.5, 0.8, 0.8)$, $s = 5$, $K = 50$.\n- Test case $3$: $(n,\\alpha,K) = (6, 0.99, 120)$.\n\nFor each test case, your program must:\n- Construct $M_1$ and $M_2$ as specified.\n- Verify and report normality numerically for each $M_i$.\n- Compute $\\rho$, $\\Delta(M_1;K)$, $\\Delta(M_2;K)$, $G(M_1;K)$, and $G(M_2;K)$.\n- Aggregate the results for each test case into a list of the form $[\\rho, \\Delta(M_1;K), \\Delta(M_2;K), G(M_1;K), G(M_2;K), \\mathrm{is\\_normal}(M_1), \\mathrm{is\\_normal}(M_2)]$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case as a list in the above format, for example $[[\\cdot],[\\cdot],[\\cdot]]$. No physical units are involved. All angles, if any appear, must be in radians, though this task does not involve angles. All ratios must be reported as decimal floating-point numbers. Ensure that all mathematical computations are numerically stable for the given sizes and parameters.", "solution": "We begin from the fundamental base of stationary iterative methods. A linear stationary method induces an error recurrence $e^{(k+1)} = M e^{(k)}$ for some fixed iteration matrix $M \\in \\mathbb{C}^{n \\times n}$. Convergence of the error sequence to the zero vector is guaranteed if and only if $\\rho(M)  1$, where the spectral radius $\\rho(M)$ is the maximal modulus of the eigenvalues of $M$. The asymptotic decay rate of the error is controlled by $\\rho(M)$ through Gelfand’s formula, which states that for any consistent matrix norm $\\|\\cdot\\|$, $\\lim_{k \\to \\infty} \\|M^k\\|^{1/k} = \\rho(M)$.\n\nHowever, non-normality can cause significant transient behavior before the asymptotic regime sets in. A matrix $M$ is normal if $M^* M = M M^*$. For normal matrices, the spectral theorem implies that $M$ is unitarily diagonalizable: $M = U \\Lambda U^*$ with $U$ unitary and $\\Lambda$ diagonal. Consequently, for any $k \\in \\mathbb{N}$, $M^k = U \\Lambda^k U^*$, and the spectral norm satisfies $\\|M^k\\|_2 = \\|\\Lambda^k\\|_2 = \\max_i |\\lambda_i|^k = \\rho(M)^k$. Therefore, for normal $M$, the finite-time geometric rate estimate equals the asymptotic rate, that is $r_k(M) = \\|M^k\\|_2^{1/k} = \\rho(M)$ for all $k \\in \\mathbb{N}$, and the excess $\\Delta(M;K)$ vanishes.\n\nIn contrast, for non-normal matrices the spectral norm of powers can initially increase even when $\\rho(M)  1$. This phenomenon can be quantified by two metrics:\n- The transient growth factor $G(M;K) = \\max_{1 \\le k \\le K} \\|M^k\\|_2$, which captures the largest possible amplification in the spectral norm over $K$ steps.\n- The finite-time excess $\\Delta(M;K) = \\max_{1 \\le k \\le K} \\max\\{0, \\|M^k\\|_2^{1/k} - \\rho(M)\\}$, which measures the largest gap between a finite-time observed per-step contraction rate and the asymptotic rate $\\rho(M)$.\n\nTo highlight the role of non-normality independent of eigenvalues, we construct pairs $(M_1, M_2)$ with the same spectrum but different normality properties:\n- With repeated eigenvalue $\\alpha \\in (-1,1)$ and size $n \\in \\mathbb{N}$, choose $M_1 = \\alpha I_n$ (normal) and $M_2 = J_n(\\alpha)$ (a single Jordan block, non-normal and defective). Here, $M_2^k = \\sum_{j=0}^{n-1} \\binom{k}{j} \\alpha^{k-j} N^j$ where $N$ is the nilpotent superdiagonal shift, revealing polynomial-in-$k$ factors that can produce transient amplification.\n- With distinct eigenvalues $\\Lambda = (\\lambda_1,\\dots,\\lambda_n)$ satisfying $\\max_i |\\lambda_i|  1$, choose $M_1 = \\mathrm{diag}(\\Lambda)$ (normal) and $M_2 = V \\, \\mathrm{diag}(\\Lambda) \\, V^{-1}$ with $V = I_n + s N$ for strictly upper triangular $N$ and scalar $s  0$ (non-normal but diagonalizable). Although $M_2$ is diagonalizable, the conditioning of $V$ controls the possible magnitude of $\\|M_2^k\\|_2$, since $\\|M_2^k\\|_2 \\le \\kappa_2(V) \\rho(M)^k$ where $\\kappa_2(V)$ is the $2$-norm condition number of $V$, and large $\\kappa_2(V)$ permits larger transient behavior.\n\nAlgorithmic design for each test case:\n1. Construct $M_1$ and $M_2$ per the specified rules to ensure $\\sigma(M_1) = \\sigma(M_2)$ and $\\rho(M_i)  1$.\n2. Compute the spectral radius $\\rho$ from the eigenvalues of $M_1$ (or equivalently $M_2$ by construction), via $\\rho = \\max |\\lambda|$ for $\\lambda \\in \\sigma(M_1)$.\n3. For $k$ from $1$ to $K$, compute $M^k$ iteratively using repeated multiplication to avoid redundant exponentiation. For each $k$, compute $a_k(M) = \\|M^k\\|_2$ as the largest singular value of $M^k$, and $r_k(M) = a_k(M)^{1/k}$.\n4. Record $G(M;K) = \\max_{1 \\le k \\le K} a_k(M)$ and $\\Delta(M;K) = \\max_{1 \\le k \\le K} \\max\\{0, r_k(M) - \\rho\\}$.\n5. Determine numerical normality by evaluating the Frobenius norm of the commutator $C = M^* M - M M^*$ and checking whether $\\|C\\|_F \\le \\tau$, with $\\tau$ chosen as $\\tau = 10^{-12} \\cdot (1 + \\|M\\|_F^2)$ to scale with the matrix size and entry magnitudes.\n\nTest suite and coverage rationale:\n- Test case $1$ with $(n,\\alpha,K) = (5, 0.9, 50)$ contrasts $M_1 = 0.9 I_5$ and a $5 \\times 5$ Jordan block at $0.9$, showcasing transient growth due to defectiveness against a normal baseline.\n- Test case $2$ with $\\Lambda = (0.2, 0.5, 0.8, 0.8)$, $s = 5$, and $K = 50$ uses a diagonalizable but non-normal $M_2$ arising from a poorly conditioned similarity, illustrating transient behavior without defectiveness and with repeated but diagonalizable eigenvalues.\n- Test case $3$ with $(n,\\alpha,K) = (6, 0.99, 120)$ explores a boundary condition near the stability threshold, highlighting more pronounced and longer-lasting transients for a larger Jordan block while preserving $\\rho(M)  1$.\n\nExpected qualitative outcomes:\n- For each normal $M_1$, $G(M_1;K)$ should equal $\\rho^1$ when $k=1$ and then decay monotonically, and $\\Delta(M_1;K)$ should be numerically near zero within floating-point tolerance.\n- For non-normal $M_2$, $G(M_2;K)$ may exceed $1$ for appropriate parameter choices even with $\\rho  1$, and $\\Delta(M_2;K)$ should be strictly positive, quantifying the gap between finite-time observed contraction rates and the asymptotic rate $\\rho$.\n\nThe program implements these constructions and computes the requested metrics. The final output concatenates, for each test case, the list $[\\rho, \\Delta(M_1;K), \\Delta(M_2;K), G(M_1;K), G(M_2;K), \\mathrm{is\\_normal}(M_1), \\mathrm{is\\_normal}(M_2)]$, producing a single-line list of lists.", "answer": "```python\nimport numpy as np\n\ndef jordan_block(alpha: float, n: int) - np.ndarray:\n    \"\"\"Construct an n x n Jordan block with eigenvalue alpha.\"\"\"\n    J = np.zeros((n, n), dtype=float)\n    for i in range(n):\n        J[i, i] = alpha\n        if i + 1  n:\n            J[i, i + 1] = 1.0\n    return J\n\ndef strictly_upper_ones(n: int) - np.ndarray:\n    \"\"\"Construct an n x n strictly upper triangular matrix with ones above the diagonal.\"\"\"\n    N = np.zeros((n, n), dtype=float)\n    for i in range(n):\n        for j in range(i + 1, n):\n            N[i, j] = 1.0\n    return N\n\ndef is_normal_matrix(M: np.ndarray, tol_scale: float = 1e-12) - bool:\n    \"\"\"Numerically determine if M is normal via Frobenius norm of commutator.\"\"\"\n    MtM = M.conj().T @ M\n    MMt = M @ M.conj().T\n    comm = MtM - MMt\n    norm_comm = np.linalg.norm(comm, ord='fro')\n    norm_M = np.linalg.norm(M, ord='fro')\n    tol = tol_scale * (1.0 + norm_M * norm_M)\n    return norm_comm = tol\n\ndef spectral_radius(M: np.ndarray) - float:\n    \"\"\"Compute spectral radius as max modulus of eigenvalues.\"\"\"\n    eigs = np.linalg.eigvals(M)\n    return float(np.max(np.abs(eigs)))\n\ndef max_singular_value(A: np.ndarray) - float:\n    \"\"\"Compute largest singular value via SVD.\"\"\"\n    s = np.linalg.svd(A, compute_uv=False)\n    return float(s[0]) if s.size  0 else 0.0\n\ndef finite_time_metrics(M: np.ndarray, K: int, rho: float) - tuple[float, float]:\n    \"\"\"\n    Compute:\n    - Delta(M;K) = max_{1..K} max(0, ||M^k||_2^{1/k} - rho)\n    - G(M;K) = max_{1..K} ||M^k||_2\n    \"\"\"\n    P = M.copy()\n    max_delta = 0.0\n    max_growth = 0.0\n    for k in range(1, K + 1):\n        # Norm of M^k\n        ak = max_singular_value(P)\n        if ak  max_growth:\n            max_growth = ak\n        # Geometric rate estimate\n        if ak  0.0:\n            rk = ak ** (1.0 / k)\n            delta = rk - rho\n            if delta  max_delta:\n                max_delta = delta\n        # Update P = M^(k+1)\n        if k  K:\n            P = P @ M\n    # Ensure nonnegative delta\n    if max_delta  0.0:\n        max_delta = 0.0\n    return max_delta, max_growth\n\ndef build_case_repeated_alpha(n: int, alpha: float, K: int):\n    M1 = alpha * np.eye(n, dtype=float)\n    M2 = jordan_block(alpha, n)\n    return M1, M2, K\n\ndef build_case_distinct_lambdas(lambdas: list[float], s: float, K: int):\n    lambdas = np.array(lambdas, dtype=float)\n    n = lambdas.size\n    M1 = np.diag(lambdas)\n    N = strictly_upper_ones(n)\n    V = np.eye(n, dtype=float) + s * N\n    Vinv = np.linalg.inv(V)\n    M2 = V @ M1 @ Vinv\n    return M1, M2, K\n\ndef solve():\n    # Define the test cases as per the problem statement.\n    test_cases = [\n        (\"repeated_alpha\", {\"n\": 5, \"alpha\": 0.9, \"K\": 50}),\n        (\"distinct_lambdas\", {\"lambdas\": [0.2, 0.5, 0.8, 0.8], \"s\": 5.0, \"K\": 50}),\n        (\"repeated_alpha\", {\"n\": 6, \"alpha\": 0.99, \"K\": 120}),\n    ]\n\n    results = []\n    for case_type, params in test_cases:\n        if case_type == \"repeated_alpha\":\n            n = params[\"n\"]\n            alpha = params[\"alpha\"]\n            K = params[\"K\"]\n            M1, M2, K = build_case_repeated_alpha(n, alpha, K)\n        elif case_type == \"distinct_lambdas\":\n            lambdas = params[\"lambdas\"]\n            s = params[\"s\"]\n            K = params[\"K\"]\n            M1, M2, K = build_case_distinct_lambdas(lambdas, s, K)\n        else:\n            raise ValueError(\"Unknown case type\")\n\n        # Shared spectral radius\n        rho = spectral_radius(M1)\n        # Should match for M2 by construction; we proceed with rho from M1.\n\n        # Metrics for M1 and M2\n        delta1, G1 = finite_time_metrics(M1, K, rho)\n        delta2, G2 = finite_time_metrics(M2, K, rho)\n\n        # Normality checks\n        norm1 = is_normal_matrix(M1)\n        norm2 = is_normal_matrix(M2)\n\n        # Aggregate: [rho, Delta(M1;K), Delta(M2;K), G(M1;K), G(M2;K), is_normal(M1), is_normal(M2)]\n        results.append([\n            rho, float(delta1), float(delta2), float(G1), float(G2), bool(norm1), bool(norm2)\n        ])\n\n    # Final print statement in the exact required format.\n    # Ensure a single line with Python-like list representation.\n    def format_element(x):\n        if isinstance(x, bool):\n            return \"True\" if x else \"False\"\n        elif isinstance(x, float):\n            # Use repr-like formatting to retain precision without excessive digits\n            return f\"{x:.15g}\"\n        elif isinstance(x, int):\n            return str(x)\n        elif isinstance(x, list):\n            return \"[\" + \",\".join(format_element(y) for y in x) + \"]\"\n        else:\n            return str(x)\n\n    print(\"[\" + \",\".join(format_element(res) for res in results) + \"]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3542439"}]}