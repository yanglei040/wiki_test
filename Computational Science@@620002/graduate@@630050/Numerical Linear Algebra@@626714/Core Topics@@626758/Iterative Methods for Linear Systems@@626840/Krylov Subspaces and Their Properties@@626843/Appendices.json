{"hands_on_practices": [{"introduction": "The Conjugate Gradient (CG) method is a cornerstone of numerical linear algebra, prized for its efficiency in solving symmetric positive definite (SPD) linear systems. This exercise strips the method down to its foundational principles: minimizing the energy quadratic and generating $A$-conjugate search directions within a Krylov subspace. By performing the first few iterations on a simple $2 \\times 2$ system from scratch [@problem_id:3554232], you will gain a concrete understanding of how these abstract geometric concepts give rise to a powerful and elegant algorithm.", "problem": "Let $A=\\begin{bmatrix}4&0\\\\0&1\\end{bmatrix}$, which is symmetric positive definite, and let $b=\\begin{bmatrix}1\\\\1\\end{bmatrix}$. Consider solving $A x=b$ using the Conjugate Gradient (CG) method starting from the initial guess $x_{0}=\\begin{bmatrix}0\\\\0\\end{bmatrix}$. Use only foundational principles: the definition of the Krylov subspace $K_{k}(A,r_{0})=\\operatorname{span}\\{r_{0},A r_{0},\\dots,A^{k-1} r_{0}\\}$ with $r_{0}=b-A x_{0}$, the symmetric positive definiteness of $A$, the $A$-inner product $\\langle u,v\\rangle_{A}=u^{\\mathsf{T}} A v$, the characterization of CG search directions $\\{p_{k}\\}$ as $A$-conjugate, and the property that each CG update chooses a step length along $p_{k}$ that minimizes the quadratic energy $q(x)=\\tfrac{1}{2}x^{\\mathsf{T}} A x - b^{\\mathsf{T}} x$ over the affine subspace $x_{k-1}+\\operatorname{span}\\{p_{k}\\}$. \n\nProceed as follows:\n- Construct the first two CG iterates $x_{1}$ and $x_{2}$ from $x_{0}$ by enforcing the stated optimality and orthogonality conditions, beginning with the choice $p_{0}=r_{0}$ and then defining $p_{1}$ so that it is $A$-conjugate to $p_{0}$.\n- At each step, explicitly determine the step length that minimizes $q(x)$ along the current search direction.\n- Verify the Euclidean orthogonality of successive residuals $r_{0}$ and $r_{1}$, and verify the $A$-conjugacy of $p_{0}$ and $p_{1}$.\n\nFinally, report the scalar value of $p_{0}^{\\mathsf{T}} A p_{1}$. The final answer must be a single real number. No rounding is required.", "solution": "The problem requires the step-by-step construction of the first two iterates of the Conjugate Gradient (CG) method for solving a specific linear system $A x=b$. The process must be based on the foundational principles of the method, namely the minimization of the energy quadratic and the $A$-conjugacy of the search directions.\n\nThe given system is $A x = b$ with:\n$A = \\begin{bmatrix} 4 & 0 \\\\ 0 & 1 \\end{bmatrix}$, $b = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$.\nThe matrix $A$ is symmetric and its eigenvalues are $4$ and $1$, which are positive. Thus, $A$ is symmetric positive definite (SPD), a prerequisite for the standard CG method.\nThe energy quadratic to be minimized is $q(x) = \\frac{1}{2}x^{\\mathsf{T}} A x - b^{\\mathsf{T}} x$. Minimizing $q(x)$ is equivalent to solving $A x=b$ because the gradient of $q(x)$ is $\\nabla q(x) = A x - b$, which is zero at the minimum.\nThe initial guess is $x_{0} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$.\n\nThe CG method generates a sequence of iterates $x_{k+1} = x_k + \\alpha_k p_k$, where $p_k$ is the search direction and $\\alpha_k$ is the step length.\n\n**Step Length Minimization**\nThe step length $\\alpha_k$ is chosen to minimize $q(x_k + \\alpha_k p_k)$ along the search direction $p_k$. Let $f(\\alpha) = q(x_k + \\alpha p_k)$.\n$$f(\\alpha) = \\frac{1}{2}(x_k + \\alpha p_k)^{\\mathsf{T}} A (x_k + \\alpha p_k) - b^{\\mathsf{T}}(x_k + \\alpha p_k)$$\n$$f(\\alpha) = \\frac{1}{2}(x_k^{\\mathsf{T}} A x_k + 2\\alpha p_k^{\\mathsf{T}} A x_k + \\alpha^2 p_k^{\\mathsf{T}} A p_k) - b^{\\mathsf{T}}x_k - \\alpha b^{\\mathsf{T}}p_k$$\nTo find the minimum, we set the derivative with respect to $\\alpha$ to zero:\n$$\\frac{df}{d\\alpha} = p_k^{\\mathsf{T}} A x_k + \\alpha p_k^{\\mathsf{T}} A p_k - b^{\\mathsf{T}}p_k = 0$$\nSolving for $\\alpha$, we get:\n$$\\alpha (p_k^{\\mathsf{T}} A p_k) = b^{\\mathsf{T}}p_k - p_k^{\\mathsf{T}} A x_k = p_k^{\\mathsf{T}} (b - A x_k)$$\nDefining the residual as $r_k = b - A x_k$, the optimal step length is:\n$$\\alpha_k = \\frac{p_k^{\\mathsf{T}} r_k}{p_k^{\\mathsf{T}} A p_k}$$\n\n**Iteration 0: Initialization**\nThe initial residual is calculated from $x_0$:\n$$r_0 = b - A x_0 = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} - \\begin{bmatrix} 4 & 0 \\\\ 0 & 1 \\end{bmatrix} \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$$\nThe first search direction is set to be the initial residual:\n$$p_0 = r_0 = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$$\n\n**Iteration 1: Construction of $x_1$**\nWe first compute the step length $\\alpha_0$. Using the formula derived above for $k=0$:\n$$\\alpha_0 = \\frac{p_0^{\\mathsf{T}} r_0}{p_0^{\\mathsf{T}} A p_0}$$\nSince $p_0=r_0$, this simplifies to $\\alpha_0 = \\frac{r_0^{\\mathsf{T}} r_0}{r_0^{\\mathsf{T}} A r_0}$.\nThe necessary components are:\n$$r_0^{\\mathsf{T}} r_0 = \\begin{bmatrix} 1 & 1 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} = 1 \\cdot 1 + 1 \\cdot 1 = 2$$\n$$A p_0 = A r_0 = \\begin{bmatrix} 4 & 0 \\\\ 0 & 1 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 4 \\\\ 1 \\end{bmatrix}$$\n$$p_0^{\\mathsf{T}} A p_0 = r_0^{\\mathsf{T}} A r_0 = \\begin{bmatrix} 1 & 1 \\end{bmatrix} \\begin{bmatrix} 4 \\\\ 1 \\end{bmatrix} = 1 \\cdot 4 + 1 \\cdot 1 = 5$$\nThe step length is:\n$$\\alpha_0 = \\frac{2}{5}$$\nThe first iterate $x_1$ is:\n$$x_1 = x_0 + \\alpha_0 p_0 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} + \\frac{2}{5} \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} \\frac{2}{5} \\\\ \\frac{2}{5} \\end{bmatrix}$$\nThe new residual $r_1$ is:\n$$r_1 = b - A x_1 = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} - \\begin{bmatrix} 4 & 0 \\\\ 0 & 1 \\end{bmatrix} \\begin{bmatrix} \\frac{2}{5} \\\\ \\frac{2}{5} \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} - \\begin{bmatrix} \\frac{8}{5} \\\\ \\frac{2}{5} \\end{bmatrix} = \\begin{bmatrix} -\\frac{3}{5} \\\\ \\frac{3}{5} \\end{bmatrix}$$\nAlternatively, we can use the update formula $r_1 = r_0 - \\alpha_0 A p_0$:\n$$r_1 = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} - \\frac{2}{5} \\begin{bmatrix} 4 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 1 - \\frac{8}{5} \\\\ 1 - \\frac{2}{5} \\end{bmatrix} = \\begin{bmatrix} -\\frac{3}{5} \\\\ \\frac{3}{5} \\end{bmatrix}$$\n\n**Verification of Residual Orthogonality**\nA key property of the CG method is that successive residuals are orthogonal in the Euclidean inner product. We verify this for $r_0$ and $r_1$:\n$$r_0^{\\mathsf{T}} r_1 = \\begin{bmatrix} 1 & 1 \\end{bmatrix} \\begin{bmatrix} -\\frac{3}{5} \\\\ \\frac{3}{5} \\end{bmatrix} = -\\frac{3}{5} + \\frac{3}{5} = 0$$\nThe orthogonality is verified.\n\n**Iteration 2: Construction of $x_2$**\nThe next search direction $p_1$ must be $A$-conjugate to $p_0$, i.e., $p_1^{\\mathsf{T}} A p_0 = 0$. We construct $p_1$ from $r_1$ and $p_0$ using a Gram-Schmidt-like process with respect to the $A$-inner product $\\langle u,v \\rangle_A = u^{\\mathsf{T}}A v$.\n$$p_1 = r_1 + \\beta_0 p_0$$\nThe coefficient $\\beta_0$ is chosen to enforce $A$-conjugacy with $p_0$:\n$$\\langle p_1, p_0 \\rangle_A = (r_1 + \\beta_0 p_0)^{\\mathsf{T}} A p_0 = r_1^{\\mathsf{T}} A p_0 + \\beta_0 p_0^{\\mathsf{T}} A p_0 = 0$$\n$$\\beta_0 = -\\frac{r_1^{\\mathsf{T}} A p_0}{p_0^{\\mathsf{T}} A p_0}$$\nLet's compute the numerator:\n$$r_1^{\\mathsf{T}} A p_0 = \\begin{bmatrix} -\\frac{3}{5} & \\frac{3}{5} \\end{bmatrix} \\begin{bmatrix} 4 \\\\ 1 \\end{bmatrix} = -\\frac{12}{5} + \\frac{3}{5} = -\\frac{9}{5}$$\nWe have already computed the denominator $p_0^{\\mathsf{T}} A p_0 = 5$.\n$$\\beta_0 = -\\frac{-9/5}{5} = \\frac{9}{25}$$\nThis is the Polak-Ribière formula for $\\beta_0$. An equivalent form for quadratic problems is the Fletcher-Reeves formula:\n$$\\beta_0 = \\frac{r_1^{\\mathsf{T}} r_1}{r_0^{\\mathsf{T}} r_0} = \\frac{(-\\frac{3}{5})^2 + (\\frac{3}{5})^2}{2} = \\frac{\\frac{9}{25} + \\frac{9}{25}}{2} = \\frac{18/25}{2} = \\frac{9}{25}$$\nNow we construct $p_1$:\n$$p_1 = r_1 + \\beta_0 p_0 = \\begin{bmatrix} -\\frac{3}{5} \\\\ \\frac{3}{5} \\end{bmatrix} + \\frac{9}{25} \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} -\\frac{15}{25} + \\frac{9}{25} \\\\ \\frac{15}{25} + \\frac{9}{25} \\end{bmatrix} = \\begin{bmatrix} -\\frac{6}{25} \\\\ \\frac{24}{25} \\end{bmatrix}$$\n\n**Verification of A-conjugacy**\nAs required by the problem, we verify the $A$-conjugacy of $p_0$ and $p_1$ by computing their $A$-inner product. The result of this calculation is also the final answer requested.\n$$p_0^{\\mathsf{T}} A p_1 = \\begin{bmatrix} 1 & 1 \\end{bmatrix} \\begin{bmatrix} 4 & 0 \\\\ 0 & 1 \\end{bmatrix} \\begin{bmatrix} -\\frac{6}{25} \\\\ \\frac{24}{25} \\end{bmatrix} = \\begin{bmatrix} 1 & 1 \\end{bmatrix} \\begin{bmatrix} -\\frac{24}{25} \\\\ \\frac{24}{25} \\end{bmatrix} = -\\frac{24}{25} + \\frac{24}{25} = 0$$\nThe $A$-conjugacy is verified. The scalar value of $p_0^{\\mathsf{T}} A p_1$ is $0$.\n\nWe now proceed to find $x_2$. The step length $\\alpha_1$ is given by:\n$$\\alpha_1 = \\frac{p_1^{\\mathsf{T}} r_1}{p_1^{\\mathsf{T}} A p_1}$$\nDue to the orthogonality of residuals, $p_1^{\\mathsf{T}} r_1 = (r_1 + \\beta_0 p_0)^{\\mathsf{T}} r_1 = r_1^{\\mathsf{T}} r_1 + \\beta_0 p_0^{\\mathsf{T}} r_1 = r_1^{\\mathsf{T}} r_1 + \\beta_0 r_0^{\\mathsf{T}} r_1$. We have already shown $r_0^{\\mathsf{T}} r_1=0$. So, $p_1^{\\mathsf{T}} r_1=r_1^{\\mathsf{T}} r_1$.\n$$\\alpha_1 = \\frac{r_1^{\\mathsf{T}} r_1}{p_1^{\\mathsf{T}} A p_1}$$\nWe have $r_1^{\\mathsf{T}} r_1 = \\frac{18}{25}$. We compute the denominator:\n$$A p_1 = \\begin{bmatrix} 4 & 0 \\\\ 0 & 1 \\end{bmatrix} \\begin{bmatrix} -\\frac{6}{25} \\\\ \\frac{24}{25} \\end{bmatrix} = \\begin{bmatrix} -\\frac{24}{25} \\\\ \\frac{24}{25} \\end{bmatrix}$$\n$$p_1^{\\mathsf{T}} A p_1 = \\begin{bmatrix} -\\frac{6}{25} & \\frac{24}{25} \\end{bmatrix} \\begin{bmatrix} -\\frac{24}{25} \\\\ \\frac{24}{25} \\end{bmatrix} = \\frac{144}{625} + \\frac{576}{625} = \\frac{720}{625} = \\frac{144}{125}$$\nSo, the step length is:\n$$\\alpha_1 = \\frac{18/25}{144/125} = \\frac{18}{25} \\cdot \\frac{125}{144} = \\frac{18 \\cdot 5}{144} = \\frac{90}{144} = \\frac{5}{8}$$\nThe second iterate $x_2$ is:\n$$x_2 = x_1 + \\alpha_1 p_1 = \\begin{bmatrix} \\frac{2}{5} \\\\ \\frac{2}{5} \\end{bmatrix} + \\frac{5}{8} \\begin{bmatrix} -\\frac{6}{25} \\\\ \\frac{24}{25} \\end{bmatrix} = \\begin{bmatrix} \\frac{2}{5} - \\frac{30}{200} \\\\ \\frac{2}{5} + \\frac{120}{200} \\end{bmatrix} = \\begin{bmatrix} \\frac{2}{5} - \\frac{3}{20} \\\\ \\frac{2}{5} + \\frac{12}{20} \\end{bmatrix} = \\begin{bmatrix} \\frac{8-3}{20} \\\\ \\frac{8+12}{20} \\end{bmatrix} = \\begin{bmatrix} \\frac{5}{20} \\\\ \\frac{20}{20} \\end{bmatrix} = \\begin{bmatrix} \\frac{1}{4} \\\\ 1 \\end{bmatrix}$$\nFor a problem of size $N=2$, the CG method is guaranteed to find the exact solution in at most $2$ iterations. We check this:\nThe exact solution is $x = A^{-1}b = \\begin{bmatrix} 1/4 & 0 \\\\ 0 & 1 \\end{bmatrix}\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 1/4 \\\\ 1 \\end{bmatrix}$. Our calculated $x_2$ is indeed the exact solution.\n\nThe problem asks for the scalar value of $p_0^{\\mathsf{T}} A p_1$. As verified above by direct computation, this value is zero, consistent with the definition of $A$-conjugate search directions.", "answer": "$$ \\boxed{0} $$", "id": "3554232"}, {"introduction": "When moving from symmetric to general non-symmetric systems, the Generalized Minimal Residual (GMRES) method is the standard Krylov subspace algorithm. Its engine is the Arnoldi process, which constructs an orthonormal basis for the Krylov subspace. This practice guides you through the first steps of GMRES, including an \"early breakdown\" scenario [@problem_id:3554249], which provides a crucial insight into how the method behaves when the initial residual lies in a low-dimensional invariant subspace of the matrix, often leading to rapid convergence.", "problem": "Consider the matrix $A=\\begin{bmatrix}2&1\\\\0&1\\end{bmatrix}$, the vector $b=\\begin{bmatrix}1\\\\0\\end{bmatrix}$, and the initial guess $x_{0}=\\begin{bmatrix}0\\\\0\\end{bmatrix}$. Use the Generalized Minimal Residual (GMRES) method, which builds solutions in the $m$-step Krylov subspace $\\mathcal{K}_{m}(A,r_{0})=\\operatorname{span}\\{r_{0},Ar_{0},\\dots,A^{m-1}r_{0}\\}$ for the initial residual $r_{0}=b-Ax_{0}$, and employs the Arnoldi process to construct an orthonormal basis $V_{m+1}$ and the $(m+1)\\times m$ upper-Hessenberg matrix $\\bar{H}_{m}$ satisfying $AV_{m}=V_{m+1}\\bar{H}_{m}$. Perform two GMRES steps ($m=2$) starting from $x_{0}$:\n\n- Construct the Arnoldi basis vectors $v_{1}$ and, if possible, $v_{2}$, and form the $3 \\times 2$ upper-Hessenberg matrix $\\bar{H}_{2}$. If a breakdown occurs in the Arnoldi process, embed the computed quantities into a $3 \\times 2$ zero-padded $\\bar{H}_{2}$ to pose the two-step GMRES least-squares problem.\n- Set up and solve the GMRES least-squares problem $\\min_{y\\in\\mathbb{R}^{2}}\\|\\beta e_{1}-\\bar{H}_{2}y\\|_{2}$, where $\\beta=\\|r_{0}\\|_{2}$ and $e_{1}\\in\\mathbb{R}^{3}$ is the first standard basis vector, to obtain the minimizing vector $y_{2}\\in\\mathbb{R}^{2}$.\n- Using the minimizing $y_{2}$, determine the two-step GMRES approximation $x_{2}$ and report the residual norm $\\|r_{2}\\|_{2}=\\|b-Ax_{2}\\|_{2}$.\n\nExpress the final residual norm $\\|r_{2}\\|_{2}$ as an exact value. No rounding is required.", "solution": "The GMRES method generates an approximate solution $x_m$ to the system $Ax=b$ of the form $x_m = x_0 + z_m$, where $z_m \\in \\mathcal{K}_m(A, r_0)$ is chosen to minimize the norm of the residual $\\|r_m\\|_2 = \\|b-Ax_m\\|_2$.\n\n**Arnoldi Process**\n\nFirst, we compute the initial residual $r_0$:\n$$r_0 = b - Ax_0 = \\begin{bmatrix}1\\\\0\\end{bmatrix} - \\begin{bmatrix}2&1\\\\0&1\\end{bmatrix}\\begin{bmatrix}0\\\\0\\end{bmatrix} = \\begin{bmatrix}1\\\\0\\end{bmatrix}$$\nThe norm of the initial residual is $\\beta = \\|r_0\\|_2 = 1$.\nThe first Arnoldi vector is the normalized initial residual:\n$$v_1 = \\frac{r_0}{\\|r_0\\|_2} = \\begin{bmatrix}1\\\\0\\end{bmatrix}$$\nNext, we perform the first Arnoldi iteration. Let $w_1 = Av_1$:\n$$w_1 = Av_1 = \\begin{bmatrix}2&1\\\\0&1\\end{bmatrix}\\begin{bmatrix}1\\\\0\\end{bmatrix} = \\begin{bmatrix}2\\\\0\\end{bmatrix}$$\nThe first-column entries of the Hessenberg matrix are computed. The element $h_{11}$ is the projection of $w_1$ onto $v_1$:\n$$h_{11} = v_1^T w_1 = \\begin{bmatrix}1&0\\end{bmatrix}\\begin{bmatrix}2\\\\0\\end{bmatrix} = 2$$\nThe unnormalized next vector $\\tilde{v}_2$ is:\n$$\\tilde{v}_2 = w_1 - h_{11}v_1 = \\begin{bmatrix}2\\\\0\\end{bmatrix} - 2\\begin{bmatrix}1\\\\0\\end{bmatrix} = \\begin{bmatrix}0\\\\0\\end{bmatrix}$$\nThe norm of this vector gives the subdiagonal element $h_{21} = \\|\\tilde{v}_2\\|_2 = 0$.\nSince $h_{21}=0$, a breakdown has occurred. The problem instructs us to perform a two-step ($m=2$) iteration by constructing the $3 \\times 2$ matrix $\\bar{H}_2$ by zero-padding the uncomputed second column:\n$$\\bar{H}_2 = \\begin{bmatrix} 2 & 0 \\\\ 0 & 0 \\\\ 0 & 0 \\end{bmatrix}$$\n\n**GMRES Least-Squares Problem and Final Residual Norm**\n\nWe must solve the least-squares problem $\\min_{y\\in\\mathbb{R}^{2}} \\|\\beta e_1 - \\bar{H}_2 y\\|_2$, where $\\beta=1$:\n$$\\beta e_1 - \\bar{H}_2 y = \\begin{bmatrix}1\\\\0\\\\0\\end{bmatrix} - \\begin{bmatrix}2&0\\\\0&0\\\\0&0\\end{bmatrix}\\begin{bmatrix}y_{(1)}\\\\y_{(2)}\\end{bmatrix} = \\begin{bmatrix}1 - 2y_{(1)}\\\\0\\\\0\\end{bmatrix}$$\nThe norm is minimized when $1-2y_{(1)}=0$, i.e., $y_{(1)}=1/2$. We can set $y_{(2)}=0$.\nThe GMRES residual norm $\\|r_2\\|_2$ is the minimum value of this least-squares problem:\n$$\\|r_2\\|_2 = \\left\\| \\begin{bmatrix}1 - 2(1/2)\\\\0\\\\0\\end{bmatrix} \\right\\|_2 = \\left\\| \\begin{bmatrix}0\\\\0\\\\0\\end{bmatrix} \\right\\|_2 = 0$$\nThe residual norm is 0, meaning GMRES finds the exact solution. For verification, the solution vector is $x_2 = x_0 + V_2 y_2 = \\frac{1}{2}v_1 = \\begin{bmatrix}1/2\\\\0\\end{bmatrix}$, and the residual is $r_2 = b - Ax_2 = \\begin{bmatrix}1\\\\0\\end{bmatrix} - \\begin{bmatrix}2&1\\\\0&1\\end{bmatrix}\\begin{bmatrix}1/2\\\\0\\end{bmatrix} = \\begin{bmatrix}0\\\\0\\end{bmatrix}$.", "answer": "$$\n\\boxed{0}\n$$", "id": "3554249"}, {"introduction": "The components generated by the Arnoldi process, such as the Hessenberg matrix $\\mathbf{H}_k$, are not merely computational byproducts; they are rich with information about the underlying operator $\\mathbf{A}$. This advanced practice explores the concept of harmonic Ritz values, which are special eigenvalue approximations derived using a Petrov-Galerkin condition related to the GMRES optimality condition itself. By deriving the eigenvalue problem for these values and computing them for a given Arnoldi decomposition [@problem_id:3554247], you will discover a powerful tool for analyzing and understanding the convergence behavior of GMRES.", "problem": "Consider a square matrix $\\mathbf{A} \\in \\mathbb{C}^{n \\times n}$, an initial residual vector $\\mathbf{r}_0 \\neq \\mathbf{0}$, and the Krylov subspace $\\mathcal{K}_k(\\mathbf{A}, \\mathbf{r}_0) = \\operatorname{span}\\{\\mathbf{r}_0, \\mathbf{A}\\mathbf{r}_0, \\dots, \\mathbf{A}^{k-1}\\mathbf{r}_0\\}$. Let $\\mathbf{V}_k \\in \\mathbb{C}^{n \\times k}$ have orthonormal columns forming a basis of $\\mathcal{K}_k(\\mathbf{A}, \\mathbf{r}_0)$ and suppose the Arnoldi relation holds,\n$$\n\\mathbf{A}\\mathbf{V}_k = \\mathbf{V}_k \\mathbf{H}_k + h_{k+1,k} \\mathbf{v}_{k+1} \\mathbf{e}_k^{\\top},\n$$\nwith $\\mathbf{H}_k \\in \\mathbb{C}^{k \\times k}$ upper Hessenberg, $\\mathbf{e}_k \\in \\mathbb{R}^k$ the $k$-th canonical basis vector, $h_{k+1,k} \\in \\mathbb{R}$, and $\\mathbf{v}_{k+1} \\in \\mathbb{C}^n$ orthonormal to the columns of $\\mathbf{V}_k$. Harmonic Ritz pairs $(\\theta, \\mathbf{u})$ relative to $\\mathcal{K}_k(\\mathbf{A}, \\mathbf{r}_0)$ are defined by $\\mathbf{u} = \\mathbf{V}_k \\mathbf{y}$ and the Petrov–Galerkin (test-space) condition $\\mathbf{A}\\mathbf{u} - \\theta \\mathbf{u} \\perp \\mathbf{A}\\mathcal{K}_k(\\mathbf{A}, \\mathbf{r}_0)$.\n\nUsing only the Arnoldi relation and the above orthogonality definition, derive the finite-dimensional generalized eigenvalue problem that determines harmonic Ritz values $\\theta$ in terms of $\\mathbf{H}_k$ and $h_{k+1,k}$. Then, for the specific Arnoldi data\n$$\n\\mathbf{H}_3 = \\begin{bmatrix} 1 & 2 & 0 \\\\ 0 & 1 & 3 \\\\ 0 & 0 & 1 \\end{bmatrix}, \\qquad h_{4,3} = 0.1,\n$$\ncompute the harmonic Ritz values. Explain briefly how these harmonic Ritz values are connected to the Generalized Minimal Residual (GMRES) method’s residual minimization property, highlighting the role of the test space $\\mathbf{A}\\mathcal{K}_k(\\mathbf{A}, \\mathbf{r}_0)$.\n\nExpress the final harmonic Ritz values as a row matrix using the LaTeX $\\mathrm{pmatrix}$ environment and round your numerical values to six significant figures. No physical units are involved.", "solution": "**Part 1: Derivation of the Generalized Eigenvalue Problem**\n\nA harmonic Ritz pair $(\\theta, \\mathbf{u})$ is defined by the search space condition $\\mathbf{u} \\in \\mathcal{K}_k(\\mathbf{A}, \\mathbf{r}_0)$ and the Petrov-Galerkin orthogonality condition on the residual, $\\mathbf{r}_{\\mathbf{u}} = \\mathbf{A}\\mathbf{u} - \\theta \\mathbf{u}$, with respect to the test space $\\mathbf{A}\\mathcal{K}_k(\\mathbf{A}, \\mathbf{r}_0)$.\n\nThe search space condition allows us to write $\\mathbf{u} = \\mathbf{V}_k \\mathbf{y}$ for some non-zero coefficient vector $\\mathbf{y} \\in \\mathbb{C}^k$.\n\nThe orthogonality condition $\\mathbf{A}\\mathbf{u} - \\theta \\mathbf{u} \\perp \\mathbf{A}\\mathcal{K}_k(\\mathbf{A}, \\mathbf{r}_0)$ is expressed as:\n$$ (\\mathbf{A}\\mathbf{V}_k)^{\\mathrm{H}} (\\mathbf{A}\\mathbf{u} - \\theta \\mathbf{u}) = \\mathbf{0} $$\nSubstituting $\\mathbf{u} = \\mathbf{V}_k \\mathbf{y}$:\n$$ (\\mathbf{A}\\mathbf{V}_k)^{\\mathrm{H}} (\\mathbf{A}\\mathbf{V}_k \\mathbf{y} - \\theta \\mathbf{V}_k \\mathbf{y}) = \\mathbf{0} $$\n$$ \\left( (\\mathbf{A}\\mathbf{V}_k)^{\\mathrm{H}} \\mathbf{A}\\mathbf{V}_k \\right) \\mathbf{y} = \\theta \\left( (\\mathbf{A}\\mathbf{V}_k)^{\\mathrm{H}} \\mathbf{V}_k \\right) \\mathbf{y} $$\nWe now use the Arnoldi relation $\\mathbf{A}\\mathbf{V}_k = \\mathbf{V}_k \\mathbf{H}_k + h_{k+1,k} \\mathbf{v}_{k+1} \\mathbf{e}_k^{\\top}$ and the orthonormality properties ($\\mathbf{V}_k^{\\mathrm{H}} \\mathbf{V}_k = \\mathbf{I}_k$, $\\mathbf{v}_{k+1}^{\\mathrm{H}} \\mathbf{V}_k = \\mathbf{0}^{\\top}$, $\\mathbf{v}_{k+1}^{\\mathrm{H}}\\mathbf{v}_{k+1}=1$) to simplify the matrices in this generalized eigenvalue problem.\n\nThe right-hand side matrix is:\n$$ (\\mathbf{A}\\mathbf{V}_k)^{\\mathrm{H}} \\mathbf{V}_k = (\\mathbf{V}_k \\mathbf{H}_k + h_{k+1,k} \\mathbf{v}_{k+1} \\mathbf{e}_k^{\\top})^{\\mathrm{H}} \\mathbf{V}_k = (\\mathbf{H}_k^{\\mathrm{H}} \\mathbf{V}_k^{\\mathrm{H}} + h_{k+1,k} \\mathbf{e}_k \\mathbf{v}_{k+1}^{\\mathrm{H}}) \\mathbf{V}_k = \\mathbf{H}_k^{\\mathrm{H}} $$\nThe left-hand side matrix is:\n$$ (\\mathbf{A}\\mathbf{V}_k)^{\\mathrm{H}} \\mathbf{A}\\mathbf{V}_k = (\\mathbf{H}_k^{\\mathrm{H}} \\mathbf{V}_k^{\\mathrm{H}} + h_{k+1,k} \\mathbf{e}_k \\mathbf{v}_{k+1}^{\\mathrm{H}}) (\\mathbf{V}_k \\mathbf{H}_k + h_{k+1,k} \\mathbf{v}_{k+1} \\mathbf{e}_k^{\\top}) = \\mathbf{H}_k^{\\mathrm{H}} \\mathbf{H}_k + h_{k+1,k}^2 \\mathbf{e}_k \\mathbf{e}_k^{\\top} $$\nThis yields the final form of the generalized eigenvalue problem:\n$$ (\\mathbf{H}_k^{\\mathrm{H}} \\mathbf{H}_k + h_{k+1,k}^2 \\mathbf{e}_k \\mathbf{e}_k^{\\top}) \\mathbf{y} = \\theta (\\mathbf{H}_k^{\\mathrm{H}}) \\mathbf{y} $$\n\n**Part 2: Computation of Harmonic Ritz Values**\n\nFor $k=3$, we are given $\\mathbf{H}_3 = \\begin{bmatrix} 1 & 2 & 0 \\\\ 0 & 1 & 3 \\\\ 0 & 0 & 1 \\end{bmatrix}$ and $h_{4,3} = 0.1$.\nLet $\\mathbf{M} = \\mathbf{H}_3^{\\top} \\mathbf{H}_3 + h_{4,3}^2 \\mathbf{e}_3 \\mathbf{e}_3^{\\top}$ and $\\mathbf{N} = \\mathbf{H}_3^{\\top}$.\n$$ \\mathbf{N} = \\mathbf{H}_3^{\\top} = \\begin{bmatrix} 1 & 0 & 0 \\\\ 2 & 1 & 0 \\\\ 0 & 3 & 1 \\end{bmatrix} $$\n$$ \\mathbf{M} = \\begin{bmatrix} 1 & 0 & 0 \\\\ 2 & 1 & 0 \\\\ 0 & 3 & 1 \\end{bmatrix} \\begin{bmatrix} 1 & 2 & 0 \\\\ 0 & 1 & 3 \\\\ 0 & 0 & 1 \\end{bmatrix} + (0.1)^2 \\begin{bmatrix} 0 & 0 & 0 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix} $$\n$$ \\mathbf{M} = \\begin{bmatrix} 1 & 2 & 0 \\\\ 2 & 5 & 3 \\\\ 0 & 3 & 10 \\end{bmatrix} + \\begin{bmatrix} 0 & 0 & 0 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 0.01 \\end{bmatrix} = \\begin{bmatrix} 1 & 2 & 0 \\\\ 2 & 5 & 3 \\\\ 0 & 3 & 10.01 \\end{bmatrix} $$\nThe harmonic Ritz values $\\theta$ are the solutions to $\\det(\\mathbf{M} - \\theta \\mathbf{N}) = 0$.\n$$ \\det \\begin{pmatrix} 1-\\theta & 2 & 0 \\\\ 2-2\\theta & 5-\\theta & 3 \\\\ 0 & 3-3\\theta & 10.01-\\theta \\end{pmatrix} = \\det \\begin{pmatrix} 1-\\theta & 2 & 0 \\\\ 0 & 1-\\theta & 3 \\\\ 0 & 3(1-\\theta) & 10.01-\\theta \\end{pmatrix} $$\nThe determinant is $(1-\\theta)((1-\\theta)(10.01-\\theta) - 9(1-\\theta)) = (1-\\theta)^2(1.01-\\theta)$.\nSetting this to zero gives the harmonic Ritz values: $1$, $1$, and $1.01$.\n\n**Part 3: Connection to GMRES**\n\nThe Generalized Minimal Residual (GMRES) method finds an approximation $\\mathbf{x}_k$ whose residual $\\mathbf{r}_k = \\mathbf{b} - \\mathbf{A}\\mathbf{x}_k$ has the minimal possible Euclidean norm. This minimization property is equivalent to the orthogonality condition $\\mathbf{r}_k \\perp \\mathbf{A}\\mathcal{K}_k(\\mathbf{A}, \\mathbf{r}_0)$.\n\nThe connection to harmonic Ritz values is that the definition of these values, $\\mathbf{A}\\mathbf{u} - \\theta \\mathbf{u} \\perp \\mathbf{A}\\mathcal{K}_k(\\mathbf{A}, \\mathbf{r}_0)$, uses the exact same test space $\\mathbf{A}\\mathcal{K}_k(\\mathbf{A}, \\mathbf{r}_0)$ as the GMRES optimality condition. This makes harmonic Ritz values particularly well-suited for analyzing GMRES convergence. While standard Ritz values (eigenvalues of $\\mathbf{H}_k$) are the roots of the GMRES residual polynomial, harmonic Ritz values excel at approximating eigenvalues near the origin, which are often the cause of slow GMRES convergence. Thus, the emergence of a small harmonic Ritz value can signal and explain convergence difficulties in GMRES.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1.00000 & 1.00000 & 1.01000\n\\end{pmatrix}\n}\n$$", "id": "3554247"}]}