{"hands_on_practices": [{"introduction": "To truly grasp the normal equations, we begin with their origin as a first-order optimality condition. This foundational exercise guides you through deriving the equations by minimizing the least-squares objective function. You will then explore a special, computationally ideal case where the matrix columns are orthonormal, revealing a simplified and elegant structure for the solution [@problem_id:3592614].", "problem": "Let $A \\in \\mathbb{R}^{m \\times n}$ with $m \\geq n$, and consider the linear least-squares problem of finding $x \\in \\mathbb{R}^{n}$ that minimizes the squared residual norm $x \\mapsto \\|A x - b\\|_{2}^{2}$ for a given $b \\in \\mathbb{R}^{m}$. \n\n(a) Starting from the definition of the least-squares objective and the fact that a minimizer must make the gradient of the objective vanish, derive the optimality condition that characterizes any minimizer.\n\n(b) Now assume that the columns of $A$ are orthonormal. Using only the optimality condition you derived in part (a) and the orthonormality assumption, obtain a closed-form expression for the unique minimizer in terms of $A$ and $b$. Justify uniqueness.\n\n(c) Consider the concrete instance with\n$$\nA \\;=\\; \\begin{pmatrix}\n1 & 0 \\\\\n0 & \\tfrac{1}{\\sqrt{2}} \\\\\n0 & \\tfrac{1}{\\sqrt{2}}\n\\end{pmatrix} \\in \\mathbb{R}^{3 \\times 2},\n\\qquad\nb \\;=\\; \\begin{pmatrix}\n3 \\\\ 2 \\\\ -1\n\\end{pmatrix} \\in \\mathbb{R}^{3}.\n$$\nVerify that the columns of $A$ are orthonormal, and then compute the minimizer from part (b) explicitly. Express your final answer as a row vector. No rounding is required.", "solution": "The problem is valid as it is a well-posed, scientifically grounded, and objective problem in numerical linear algebra. It is free of any of the invalidating flaws listed in the problem validation guidelines. It asks for the derivation and application of fundamental results in linear least-squares theory.\n\n**(a) Derivation of the Optimality Condition**\n\nThe linear least-squares problem seeks to find a vector $x \\in \\mathbb{R}^{n}$ that minimizes the objective function $f(x)$, defined as the squared Euclidean norm of the residual vector $r = Ax - b$:\n$$f(x) = \\|Ax - b\\|_{2}^{2}$$\nwhere $A \\in \\mathbb{R}^{m \\times n}$ with $m \\geq n$, $x \\in \\mathbb{R}^{n}$, and $b \\in \\mathbb{R}^{m}$.\n\nThe squared norm can be expressed as a dot product, which in matrix notation is equivalent to a product with the transpose:\n$$f(x) = (Ax - b)^{T}(Ax - b)$$\nExpanding this expression using the properties of matrix transpose, $(P Q)^{T} = Q^{T} P^{T}$, we get:\n$$f(x) = (x^{T}A^{T} - b^{T})(Ax - b)$$\nDistributing the terms yields:\n$$f(x) = x^{T}A^{T}Ax - x^{T}A^{T}b - b^{T}Ax + b^{T}b$$\nThe terms $x^{T}A^{T}b$ and $b^{T}Ax$ are scalars. A scalar is equal to its own transpose. Let's take the transpose of the first one: $(x^{T}A^{T}b)^{T} = b^{T}(A^{T})^{T}(x^{T})^{T} = b^{T}Ax$. Thus, the two middle terms are identical. The objective function simplifies to:\n$$f(x) = x^{T}(A^{T}A)x - 2b^{T}Ax + b^{T}b$$\nThe function $f(x)$ is a quadratic form in $x$. Since the matrix $A^{T}A$ is positive semi-definite (as $z^T(A^TA)z = (Az)^T(Az) = \\|Az\\|_2^2 \\ge 0$), the function $f(x)$ is convex. A minimizer $x$ of a differentiable convex function must occur at a point where the gradient of the function with respect to $x$ is the zero vector. We compute the gradient, $\\nabla f(x)$:\n$$\\nabla f(x) = \\nabla_{x} \\left( x^{T}(A^{T}A)x - 2b^{T}Ax + b^{T}b \\right)$$\nUsing standard rules of matrix calculus:\n\\begin{itemize}\n    \\item The gradient of a quadratic form $x^{T}Mx$ is $(M+M^{T})x$. Since $A^{T}A$ is symmetric, $\\nabla_{x}(x^{T}(A^{T}A)x) = 2(A^{T}A)x$.\n    \\item The gradient of a linear form $c^{T}x$ is $c$. Here, $c^{T} = 2b^{T}A$, so $c = (2b^{T}A)^{T} = 2A^{T}b$. Thus, $\\nabla_{x}(-2b^{T}Ax) = -2A^{T}b$.\n    \\item The gradient of a constant term $b^{T}b$ is the zero vector.\n\\end{itemize}\nCombining these results gives the gradient:\n$$\\nabla f(x) = 2(A^{T}A)x - 2A^{T}b$$\nThe optimality condition is found by setting the gradient to zero:\n$$2(A^{T}A)x - 2A^{T}b = 0$$\nDividing by $2$ gives the celebrated **normal equations**:\n$$A^{T}Ax = A^{T}b$$\nThis is the optimality condition that any minimizer $x$ of $\\|Ax - b\\|_{2}^{2}$ must satisfy.\n\n**(b) Unique Minimizer for Orthonormal Columns**\n\nWe are now given that the columns of $A$ are orthonormal. Let the columns of $A$ be denoted by the vectors $a_{1}, a_{2}, \\ldots, a_{n}$, so $A = \\begin{pmatrix} a_{1} & a_{2} & \\cdots & a_{n} \\end{pmatrix}$. The orthonormality condition means that the dot product of any two distinct columns is zero, and the norm of each column is one:\n$$a_{i}^{T}a_{j} = \\delta_{ij} = \\begin{cases} 1 & \\text{if } i=j \\\\ 0 & \\text{if } i \\neq j \\end{cases}$$\nLet us examine the matrix product $A^{T}A$. The entry in the $i$-th row and $j$-th column of $A^{T}A$ is given by the product of the $i$-th row of $A^{T}$ and the $j$-th column of $A$. The $i$-th row of $A^{T}$ is precisely the transpose of the $i$-th column of $A$, which is $a_{i}^{T}$. Therefore:\n$$(A^{T}A)_{ij} = a_{i}^{T}a_{j} = \\delta_{ij}$$\nThis shows that the matrix $A^{T}A$ is the $n \\times n$ identity matrix, $I_{n}$.\n\nSubstituting this result into the optimality condition derived in part (a), $A^{T}Ax = A^{T}b$, we get:\n$$I_{n}x = A^{T}b$$\nThis immediately simplifies to a closed-form expression for the minimizer $x$:\n$$x = A^{T}b$$\nTo justify the uniqueness of this minimizer, we note that the solution to the linear system $(A^{T}A)x = A^{T}b$ is unique if and only if the matrix $A^{T}A$ is invertible. In this case, we have shown that $A^{T}A = I_{n}$. The identity matrix is its own inverse, $(I_{n})^{-1} = I_{n}$, and is therefore invertible. Thus, the solution $x = (A^{T}A)^{-1}A^{T}b = I_{n}^{-1}A^{T}b = A^{T}b$ is the unique minimizer.\n\n**(c) Explicit Computation**\n\nWe are given the specific instance:\n$$\nA = \\begin{pmatrix}\n1 & 0 \\\\\n0 & \\frac{1}{\\sqrt{2}} \\\\\n0 & \\frac{1}{\\sqrt{2}}\n\\end{pmatrix},\n\\qquad\nb = \\begin{pmatrix}\n3 \\\\\n2 \\\\\n-1\n\\end{pmatrix}\n$$\nFirst, we verify that the columns of $A$ are orthonormal. Let the columns be $a_{1} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}$ and $a_{2} = \\begin{pmatrix} 0 \\\\ \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} \\end{pmatrix}$.\n\nWe compute the norms:\n$$\\|a_{1}\\|_{2}^{2} = a_{1}^{T}a_{1} = 1^{2} + 0^{2} + 0^{2} = 1 \\implies \\|a_{1}\\|_{2} = 1$$\n$$\\|a_{2}\\|_{2}^{2} = a_{2}^{T}a_{2} = 0^{2} + \\left(\\frac{1}{\\sqrt{2}}\\right)^{2} + \\left(\\frac{1}{\\sqrt{2}}\\right)^{2} = 0 + \\frac{1}{2} + \\frac{1}{2} = 1 \\implies \\|a_{2}\\|_{2} = 1$$\nThe columns have unit norm.\n\nWe compute their dot product to check for orthogonality:\n$$a_{1}^{T}a_{2} = (1)(0) + (0)\\left(\\frac{1}{\\sqrt{2}}\\right) + (0)\\left(\\frac{1}{\\sqrt{2}}\\right) = 0$$\nThe columns are orthogonal. Since they are also of unit norm, the columns of $A$ are orthonormal.\n\nSince the columns of $A$ are orthonormal, we can use the formula derived in part (b) to find the unique least-squares minimizer $x$:\n$$x = A^{T}b$$\n First, we find the transpose of $A$:\n$$A^{T} = \\begin{pmatrix}\n1 & 0 & 0 \\\\\n0 & \\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}}\n\\end{pmatrix}$$\nNow, we perform the matrix-vector multiplication:\n$$x = \\begin{pmatrix}\n1 & 0 & 0 \\\\\n0 & \\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}}\n\\end{pmatrix}\n\\begin{pmatrix}\n3 \\\\\n2 \\\\\n-1\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n(1)(3) + (0)(2) + (0)(-1) \\\\\n(0)(3) + \\left(\\frac{1}{\\sqrt{2}}\\right)(2) + \\left(\\frac{1}{\\sqrt{2}}\\right)(-1)\n\\end{pmatrix}\n$$\n$$x = \\begin{pmatrix}\n3 \\\\\n\\frac{2}{\\sqrt{2}} - \\frac{1}{\\sqrt{2}}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n3 \\\\\n\\frac{1}{\\sqrt{2}}\n\\end{pmatrix}$$\nThe problem asks for the answer as a row vector. Therefore, the minimizer is represented as $[3, \\frac{1}{\\sqrt{2}}]$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n3 & \\frac{1}{\\sqrt{2}}\n\\end{pmatrix}\n}\n$$", "id": "3592614"}, {"introduction": "The normal equations provide a unique solution only if the matrix $A^TA$ is invertible, which in turn requires the columns of $A$ to be linearly independent. This next practice uses a hypothetical data error scenario to explore what happens when this condition is violated. By analyzing a case where a column of $A$ is a zero vector, you will prove the connection between the rank of $A$ and the singularity of $A^TA$, and understand its implications for the uniqueness of the least-squares solution [@problem_id:2162106].", "problem": "In the context of linear regression, a model is proposed to predict a dependent variable $y$ based on a set of $n$ predictor variables. The model is represented by the matrix equation $A\\mathbf{x} = \\mathbf{b}$, where $A$ is an $m \\times n$ matrix ($m > n$) containing the values of the predictor variables for $m$ different observations, $\\mathbf{b}$ is an $m \\times 1$ vector of the observed outcomes, and $\\mathbf{x}$ is an $n \\times 1$ vector of the regression coefficients to be determined.\n\nDue to a data-logging error, one of the predictor variables, corresponding to the $j$-th column of matrix $A$, was recorded as zero for all $m$ observations. Consequently, the $j$-th column of $A$ is a zero vector. The goal is to find the least-squares solution $\\mathbf{x}_{\\text{LS}}$ that minimizes the Euclidean norm of the residual, $\\|A\\mathbf{x} - \\mathbf{b}\\|$. This solution is found by solving the normal equations, $(A^TA)\\mathbf{x} = A^T\\mathbf{b}$.\n\nGiven this scenario, which of the following statements is always true for any non-zero vector $\\mathbf{b}$?\n\nA. The normal equations matrix $A^TA$ is invertible, leading to a unique least-squares solution $\\mathbf{x}_{\\text{LS}}$.\n\nB. The least-squares problem has no solution because the columns of $A$ are linearly dependent.\n\nC. The system of normal equations $(A^TA)\\mathbf{x} = A^T\\mathbf{b}$ is inconsistent.\n\nD. The normal equations matrix $A^TA$ is singular, and the least-squares problem has infinitely many solutions.", "solution": "Let the columns of $A$ be $a_{1},\\dots,a_{n}$ with $a_{j}=\\mathbf{0}\\in \\mathbb{R}^{m}$. Then for the normal equations matrix,\n$$\nA^{T}A=\\left[a_{k}^{T}a_{\\ell}\\right]_{k,\\ell=1}^{n}.\n$$\nBecause $a_{j}=\\mathbf{0}$, for every $k$ and $\\ell$,\n$$\n(A^{T}A)_{j\\ell}=a_{j}^{T}a_{\\ell}=0,\\quad (A^{T}A)_{k j}=a_{k}^{T}a_{j}=0.\n$$\nHence the $j$-th row and $j$-th column of $A^{T}A$ are zero, so\n$$\nA^{T}A\\,e_{j}=\\mathbf{0},\n$$\nwhich implies $\\operatorname{rank}(A^{T}A)\\leq n-1$ and $A^{T}A$ is singular. Therefore statement A is false.\n\nFor the right-hand side, the $j$-th component of $A^{T}\\mathbf{b}$ is\n$$\n(A^{T}\\mathbf{b})_{j}=a_{j}^{T}\\mathbf{b}=0.\n$$\nThus the $j$-th equation in $(A^{T}A)\\mathbf{x}=A^{T}\\mathbf{b}$ is $0=0$. More generally, partition $\\mathbf{x}=(x_{j},\\mathbf{x}_{-j})$ and $A=[a_{j}\\ \\ A_{-j}]=[\\,\\mathbf{0}\\ \\ A_{-j}\\,]$. Then $A\\mathbf{x}=A_{-j}\\mathbf{x}_{-j}$ and the least-squares problem reduces to\n$$\n\\min_{\\mathbf{x}_{-j}}\\|A_{-j}\\mathbf{x}_{-j}-\\mathbf{b}\\|.\n$$\nThe set $\\mathcal{R}(A_{-j})=\\{A_{-j}\\mathbf{x}_{-j}:\\mathbf{x}_{-j}\\in\\mathbb{R}^{n-1}\\}$ is a closed subspace of $\\mathbb{R}^{m}$, so by the projection theorem there exists a unique $y^{\\star}\\in \\mathcal{R}(A_{-j})$ minimizing $\\|\\mathbf{b}-y\\|$ over $y\\in \\mathcal{R}(A_{-j})$, characterized by\n$$\nA_{-j}^{T}(y^{\\star}-\\mathbf{b})=\\mathbf{0}.\n$$\nChoose any $\\mathbf{x}_{-j}^{\\star}$ with $A_{-j}\\mathbf{x}_{-j}^{\\star}=y^{\\star}$. Then\n$$\nA_{-j}^{T}(A_{-j}\\mathbf{x}_{-j}^{\\star}-\\mathbf{b})=\\mathbf{0},\n$$\nwhich is exactly the reduced normal equations, hence $(A^{T}A)\\mathbf{x}=A^{T}\\mathbf{b}$ is consistent. Therefore statement C is false.\n\nSince $A e_{j}=a_{j}=\\mathbf{0}$, for any scalar $t$ and any least-squares minimizer $\\mathbf{x}^{\\star}$,\n$$\nA(\\mathbf{x}^{\\star}+t e_{j})=A\\mathbf{x}^{\\star},\n$$\nso all $\\mathbf{x}^{\\star}+t e_{j}$ yield the same residual norm. Thus there are infinitely many least-squares solutions. Therefore statement B is false and statement D is true: $A^{T}A$ is singular and the least-squares problem has infinitely many solutions.\n\nConsequently, the statement that is always true is D.", "answer": "$$\\boxed{D}$$", "id": "2162106"}, {"introduction": "While theoretically elegant, the normal equations are notoriously sensitive to rounding errors in practice, a consequence of squaring the condition number of the matrix $A$. This hands-on coding challenge will have you build and test solvers to witness this instability firsthand. By comparing the normal equations method against a robust solver based on the Singular Value Decomposition (SVD), you will see why explicitly forming $A^TA$ can lead to catastrophic failure for ill-conditioned problems and why alternative methods are essential tools in numerical computation [@problem_id:3205220].", "problem": "You are to design and implement a complete, runnable program that demonstrates algorithm robustness and stability in solving linear least-squares problems. From first principles, linear least-squares seeks an $\\ell_{2}$-norm minimizing vector $x \\in \\mathbb{R}^{n}$ that minimizes the residual $\\lVert A x - b \\rVert_{2}$ for a given matrix $A \\in \\mathbb{R}^{m \\times n}$ and vector $b \\in \\mathbb{R}^{m}$. Two classical approaches are: (i) forming and solving the normal equations $A^{\\top} A x = A^{\\top} b$, and (ii) using the Singular Value Decomposition (SVD) to compute a pseudoinverse solution that is numerically more stable in the presence of poor conditioning. Robustness concerns how an algorithm behaves under perturbations (for example, roundoff or small changes in data), and stability concerns the algorithmâ€™s sensitivity to the conditioning of the input.\n\nStarting only from core definitions and well-tested facts (the linear least-squares objective, the concept of conditioning, and the existence of the Singular Value Decomposition), construct and test problems where solving the normal equations is numerically fragile. Your program must:\n\n- Implement two solvers for the linear least-squares problem:\n  1. A normal-equations solver that explicitly forms $A^{\\top} A$ and $A^{\\top} b$ and solves $A^{\\top} A x = A^{\\top} b$ using direct methods.\n  2. An SVD-based pseudoinverse solver. Given the compact SVD $A = U \\Sigma V^{\\top}$ with singular values $\\sigma_{1} \\ge \\cdots \\ge \\sigma_{n} \\ge 0$, define the pseudoinverse by discarding singular values below a tolerance and inverting the rest. Use the tolerance $\\tau_{\\sigma} = u \\cdot \\max(m,n) \\cdot \\sigma_{1}$, where $u$ is the unit roundoff of double precision, $u \\approx 2.22 \\times 10^{-16}$.\n\n- Define success and failure criteria:\n  - For any candidate solution $\\hat{x}$, define the relative residual $r(\\hat{x}) = \\lVert A \\hat{x} - b \\rVert_{2} / \\lVert b \\rVert_{2}$.\n  - When a ground-truth solution $x^{\\star}$ is known, define the relative solution error $e(\\hat{x}) = \\lVert \\hat{x} - x^{\\star} \\rVert_{2} / \\lVert x^{\\star} \\rVert_{2}$.\n  - Declare the SVD solver a success if $r(\\hat{x}_{\\mathrm{SVD}}) \\le \\tau_{\\mathrm{res}}$, with $\\tau_{\\mathrm{res}} = 10^{-10}$.\n  - Declare the normal-equations solver a failure if it raises a singularity error or produces a solution with either $e(\\hat{x}_{\\mathrm{NE}}) > \\tau_{\\mathrm{sol}}$ when $x^{\\star}$ is known, or $r(\\hat{x}_{\\mathrm{NE}}) > \\tau_{\\mathrm{res,NE}}$ when $x^{\\star}$ is unknown. Use $\\tau_{\\mathrm{sol}} = 10^{-2}$ and $\\tau_{\\mathrm{res,NE}} = 10^{-6}$.\n\n- For each test case, output a boolean indicating whether solving the normal equations fails while the SVD-based approach succeeds. Specifically, the boolean for a test case must be the truth value of\n  $\n  \\big(\\text{SVD success}\\big) \\land \\big(\\text{normal-equations failure}\\big).\n  $\n\nConstruct the following test suite of matrices $A$ and vectors $b$, each with specified dimensions and parameters. In all cases, use deterministic random number generation with fixed seeds to ensure reproducibility, and compute $b$ as $b = A x^{\\star} + \\eta$ with a specified noise vector $\\eta$ (if applicable).\n\n- Test case $1$ (well-conditioned benchmark, happy path):\n  - Dimensions: $m = 60$, $n = 20$.\n  - Construction: form $A = U \\Sigma V^{\\top}$ with $U \\in \\mathbb{R}^{60 \\times 20}$ and $V \\in \\mathbb{R}^{20 \\times 20}$ orthonormal, and singular values $\\sigma_{k}$ linearly spaced between $1$ and $0.5$ for $k = 1,\\dots,20$.\n  - Ground truth: draw $x^{\\star} \\in \\mathbb{R}^{20}$ from a fixed seed and set $\\eta = 0$.\n\n- Test case $2$ (extremely ill-conditioned, poor stability of normal equations):\n  - Dimensions: $m = 80$, $n = 20$.\n  - Construction: form $A = U \\Sigma V^{\\top}$ as above with $\\sigma_{k}$ logarithmically spaced from $1$ down to $10^{-8}$, so that $\\kappa_{2}(A)$ is approximately $10^{8}$.\n  - Ground truth: draw $x^{\\star} \\in \\mathbb{R}^{20}$ from a fixed seed and add small noise $\\eta$ with $\\lVert \\eta \\rVert_{2}$ scaled to approximately $10^{-12} \\cdot \\lVert A x^{\\star} \\rVert_{2}$.\n\n- Test case $3$ (classical ill-conditioned design, Hilbert-type matrix):\n  - Dimensions: $m = 40$, $n = 15$.\n  - Construction: define $A_{ij} = \\dfrac{1}{i + j - 1}$ for $i = 1,\\dots,40$ and $j = 1,\\dots,15$.\n  - Ground truth: draw $x^{\\star} \\in \\mathbb{R}^{15}$ from a fixed seed and add noise $\\eta$ with $\\lVert \\eta \\rVert_{2}$ scaled to approximately $10^{-10} \\cdot \\lVert A x^{\\star} \\rVert_{2}$.\n\n- Test case $4$ (rank-deficient, singular normal equations):\n  - Dimensions: $m = 50$, $n = 20$.\n  - Construction: draw a random $A \\in \\mathbb{R}^{50 \\times 20}$ from a fixed seed and set its second column equal to its first column exactly, making $A$ rank-deficient.\n  - Ground truth: draw $x^{\\star} \\in \\mathbb{R}^{20}$ from a fixed seed and set $\\eta = 0$.\n\nYour program must compute, for each test case, the boolean described above. The final output must be a single line containing a comma-separated list of these booleans enclosed in square brackets, for example, $\\big[ \\text{true}, \\text{false}, \\text{true}, \\text{true} \\big]$ but using the exact capitalization and formatting produced by Python booleans.\n\nNo physical units are involved. Angles are not involved. Express any threshold values numerically as decimals (do not use percentage signs).\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, $\\big[ \\text{True}, \\text{False}, \\text{True}, \\text{True} \\big]$.", "solution": "The tasked problem is to demonstrate the superior numerical stability of the Singular Value Decomposition (SVD) method over the normal equations method for solving linear least-squares problems, particularly for ill-conditioned or rank-deficient systems. The core of the problem is to find a vector $x \\in \\mathbb{R}^{n}$ that minimizes the Euclidean norm of the residual, $\\lVert A x - b \\rVert_{2}$, for a given matrix $A \\in \\mathbb{R}^{m \\times n}$ and vector $b \\in \\mathbb{R}^{m}$. We will implement and compare two classical algorithms for this task.\n\n**The Normal Equations Method**\n\nThe first principles of optimization dictate that for a solution $x$ to be a minimizer of the convex function $f(x) = \\frac{1}{2} \\lVert Ax - b \\rVert_2^2$, its gradient must be zero. The gradient is given by $\\nabla f(x) = A^{\\top}(Ax - b)$. Setting the gradient to zero yields the celebrated normal equations:\n$$\nA^{\\top} A x = A^{\\top} b\n$$\nThis transforms the least-squares problem into a square linear system of equations. If the matrix $A$ has full column rank, then the Gram matrix $A^{\\top} A \\in \\mathbb{R}^{n \\times n}$ is symmetric and positive definite, and a unique solution $x$ can be found using standard methods like Cholesky decomposition or LU factorization.\n\nThe critical flaw of this method lies in its numerical stability. The condition number of a matrix, $\\kappa(M)$, measures the sensitivity of the solution of $Mx=y$ to perturbations in $y$. For the normal equations, the relevant matrix is $A^{\\top} A$. It is a fundamental result in numerical linear algebra that the condition number of this Gram matrix is the square of the condition number of the original matrix $A$:\n$$\n\\kappa_2(A^{\\top} A) = \\big(\\kappa_2(A)\\big)^2\n$$\nIf $A$ is ill-conditioned (i.e., $\\kappa_2(A)$ is large), $\\kappa_2(A^{\\top} A)$ can become enormous. For example, if $\\kappa_2(A) = 10^8$, then $\\kappa_2(A^{\\top} A) = 10^{16}$. In standard double-precision floating-point arithmetic, which has about $16$ decimal digits of precision, the matrix $A^{\\top} A$ becomes computationally indistinguishable from a singular matrix. The explicit formation of $A^{\\top} A$ can lead to a catastrophic loss of information.\n\n**The SVD-Based Pseudoinverse Method**\n\nA more numerically robust approach is based on the Singular Value Decomposition (SVD). Any matrix $A \\in \\mathbb{R}^{m \\times n}$ can be factored as:\n$$\nA = U \\Sigma V^{\\top}\n$$\nwhere $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{n \\times n}$ are orthogonal matrices, and $\\Sigma \\in \\mathbb{R}^{m \\times n}$ is a rectangular diagonal matrix with non-negative real numbers on the diagonal, known as the singular values, $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge \\sigma_{\\min(m,n)} \\ge 0$. For the compact SVD of a tall matrix ($m \\ge n$), $U$ is $m \\times n$, $\\Sigma$ is $n \\times n$, and $V$ is $n \\times n$.\n\nThe solution to the least-squares problem can be expressed using the Moore-Penrose pseudoinverse, $A^{\\dagger}$. The minimum-norm least-squares solution is $x = A^{\\dagger} b$. The SVD provides a stable way to compute the pseudoinverse:\n$$\nA^{\\dagger} = V \\Sigma^{\\dagger} U^{\\top}\n$$\nwhere $\\Sigma^{\\dagger}$ is the pseudoinverse of $\\Sigma$. It is obtained by taking the reciprocal of the non-zero singular values and transposing the resulting matrix. To ensure numerical stability, singular values that are very small (close to zero) are treated as if they are zero. This is achieved by thresholding. Any singular value $\\sigma_i$ below a tolerance $\\tau_{\\sigma}$ is treated as zero. The problem specifies a standard tolerance:\n$$\n\\tau_{\\sigma} = u \\cdot \\max(m,n) \\cdot \\sigma_{1}\n$$\nwhere $u$ is the machine unit roundoff (approximately $2.22 \\times 10^{-16}$ for double precision) and $\\sigma_1$ is the largest singular value. This method avoids forming $A^{\\top} A$ and thus circumvents the squaring of the condition number, making it highly robust.\n\n**Implementation and Evaluation Strategy**\n\nTwo solvers are implemented: `solve_normal_equations` which forms and solves the normal equations, and `solve_svd` which computes the solution via the SVD-based pseudoinverse.\n\nThe program evaluates these solvers on four test cases designed to expose the fragility of the normal equations:\n1.  **Well-conditioned Benchmark**: A matrix with a small condition number ($\\kappa_2(A)=2$). Both methods are expected to perform well.\n2.  **Extremely Ill-conditioned**: A matrix with a large condition number ($\\kappa_2(A)=10^8$), such that $\\kappa_2(A^{\\top}A) \\approx 10^{16}$, which is at the limit of double-precision arithmetic.\n3.  **Hilbert-type Matrix**: A classic example of an ill-conditioned matrix that arises in approximation theory.\n4.  **Rank-deficient Matrix**: A matrix whose columns are linearly dependent. Here, $A^{\\top}A$ is exactly singular, guaranteeing the failure of the normal equations method if a direct solver is used.\n\nFor each test case, we determine the boolean value of the expression $(\\text{SVD success}) \\land (\\text{normal-equations failure})$.\n-   SVD success is defined by the relative residual $\\lVert A \\hat{x}_{\\mathrm{SVD}} - b \\rVert_{2} / \\lVert b \\rVert_{2} \\le 10^{-10}$.\n-   Normal equations failure is defined by a singularity error during solution, or if the relative solution error $\\lVert \\hat{x}_{\\mathrm{NE}} - x^{\\star} \\rVert_{2} / \\lVert x^{\\star} \\rVert_{2} > 10^{-2}$, where $x^{\\star}$ is the known ground-truth solution.\n\nThe final output aggregates these boolean results, demonstrating the scenarios where the normal equations method fails while the SVD method remains robust and successful.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print the results.\n    \"\"\"\n    \n    # Define constants from the problem statement.\n    U_EPS = np.finfo(float).eps\n    TAU_RES = 1e-10  # SVD success threshold for relative residual\n    TAU_SOL = 1e-2   # NE failure threshold for relative solution error\n    TAU_RES_NE = 1e-6 # NE failure threshold for relative residual (if x_star is unknown)\n\n    def solve_normal_equations(A, b):\n        \"\"\"\n        Solves min||Ax-b||_2 using the normal equations A.T*A*x = A.T*b.\n        Returns a tuple: (solution vector, singularity_flag).\n        The flag is True if a LinAlgError (singularity) is caught.\n        \"\"\"\n        AtA = A.T @ A\n        Atb = A.T @ b\n        try:\n            x_ne = np.linalg.solve(AtA, Atb)\n            return x_ne, False\n        except np.linalg.LinAlgError:\n            return None, True\n\n    def solve_svd(A, b):\n        \"\"\"\n        Solves min||Ax-b||_2 using SVD-based pseudoinverse.\n        \"\"\"\n        m, n = A.shape\n        U, s, Vt = np.linalg.svd(A, full_matrices=False)\n        \n        sigma_1 = s[0] if s.size > 0 else 0.0\n        tau_sigma = U_EPS * max(m, n) * sigma_1\n        \n        s_pinv = np.zeros_like(s)\n        if s.size > 0:\n            s_pinv[s > tau_sigma] = 1.0 / s[s > tau_sigma]\n        \n        # This computes x = V @ diag(s_pinv) @ U.T @ b efficiently\n        x_svd = Vt.T @ (s_pinv * (U.T @ b))\n        return x_svd\n\n    def evaluate_case(A, b, x_star):\n        \"\"\"\n        Performs the evaluation for a single test case.\n        Returns a boolean for (SVD_success AND NE_failure).\n        \"\"\"\n        # SVD Solver Evaluation\n        x_svd = solve_svd(A, b)\n        norm_b = np.linalg.norm(b)\n        if norm_b > 0:\n            r_svd = np.linalg.norm(A @ x_svd - b) / norm_b\n        else: # Handle b=0 case\n            r_svd = np.linalg.norm(A @ x_svd - b)\n        svd_success = r_svd = TAU_RES\n\n        # Normal Equations Solver Evaluation\n        x_ne, singularity = solve_normal_equations(A, b)\n        ne_failure = singularity\n        if not ne_failure:\n            norm_x_star = np.linalg.norm(x_star)\n            if norm_x_star > 0:\n                e_ne = np.linalg.norm(x_ne - x_star) / norm_x_star\n                if e_ne > TAU_SOL:\n                    ne_failure = True\n            elif np.linalg.norm(x_ne) > 0: # If x_star is zero, any non-zero solution is an error\n                ne_failure = True\n        \n        return svd_success and ne_failure\n\n    # --- Test Case Generation ---\n\n    def generate_case_1():\n        # Well-conditioned benchmark\n        m, n = 60, 20\n        rng = np.random.default_rng(123)\n        U, _ = np.linalg.qr(rng.standard_normal((m, n)))\n        V, _ = np.linalg.qr(rng.standard_normal((n, n)))\n        sigma = np.linspace(1.0, 0.5, n)\n        A = U @ np.diag(sigma) @ V.T\n        x_star = rng.standard_normal(n)\n        b = A @ x_star\n        return A, b, x_star\n\n    def generate_case_2():\n        # Extremely ill-conditioned\n        m, n = 80, 20\n        rng = np.random.default_rng(456)\n        U, _ = np.linalg.qr(rng.standard_normal((m, n)))\n        V, _ = np.linalg.qr(rng.standard_normal((n, n)))\n        sigma = np.logspace(0, -8, n)\n        A = U @ np.diag(sigma) @ V.T\n        x_star = rng.standard_normal(n)\n        b_noiseless = A @ x_star\n        eta_raw = rng.standard_normal(m)\n        norm_eta_raw = np.linalg.norm(eta_raw)\n        if norm_eta_raw > 0:\n            eta = eta_raw * (1e-12 * np.linalg.norm(b_noiseless)) / norm_eta_raw\n        else:\n            eta = np.zeros(m)\n        b = b_noiseless + eta\n        return A, b, x_star\n\n    def generate_case_3():\n        # Hilbert-type matrix\n        m, n = 40, 15\n        rng = np.random.default_rng(789)\n        i = np.arange(1, m + 1)[:, np.newaxis]\n        j = np.arange(1, n + 1)\n        A = 1.0 / (i + j - 1)\n        x_star = rng.standard_normal(n)\n        b_noiseless = A @ x_star\n        eta_raw = rng.standard_normal(m)\n        norm_eta_raw = np.linalg.norm(eta_raw)\n        if norm_eta_raw > 0:\n            eta = eta_raw * (1e-10 * np.linalg.norm(b_noiseless)) / norm_eta_raw\n        else:\n            eta = np.zeros(m)\n        b = b_noiseless + eta\n        return A, b, x_star\n\n    def generate_case_4():\n        # Rank-deficient\n        m, n = 50, 20\n        rng = np.random.default_rng(101)\n        A = rng.standard_normal((m, n))\n        A[:, 1] = A[:, 0]\n        x_star = rng.standard_normal(n)\n        b = A @ x_star\n        return A, b, x_star\n\n    # --- Run Suite and Print Results ---\n    \n    test_generators = [\n        generate_case_1,\n        generate_case_2,\n        generate_case_3,\n        generate_case_4,\n    ]\n\n    results = []\n    for gen_func in test_generators:\n        A, b, x_star = gen_func()\n        result = evaluate_case(A, b, x_star)\n        results.append(result)\n\n    # Format output as specified: [True,False,True,True]\n    formatted_results = [str(r) for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3205220"}]}