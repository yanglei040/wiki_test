{"hands_on_practices": [{"introduction": "A primary application of the Schur decomposition is to simplify the computation of matrix functions. This exercise provides direct practice by having you compute the matrix exponential, $\\exp(A)$, a function central to solving systems of linear differential equations. By leveraging the real Schur form, you will see how the problem breaks down into computing exponentials of smaller diagonal blocks and solving for the off-diagonal terms, offering a concrete look at the mechanics of algorithms for matrix functions. [@problem_id:3596233]", "problem": "Let $A \\in \\mathbb{R}^{3 \\times 3}$ be given by\n$$\nA = \\begin{pmatrix}\n0 & 1 & 1 \\\\\n-1 & 0 & 0 \\\\\n0 & 0 & 2\n\\end{pmatrix}.\n$$\nUsing the real Schur form, compute the matrix exponential $\\exp(A)$ by evaluating exponentials of the $1 \\times 1$ and $2 \\times 2$ blocks of the real Schur factor $T$ and assembling $\\exp(A) = Q \\exp(T) Q^{*}$. In your derivation, begin from the definition of the matrix exponential and fundamental properties of similarity transformations and block upper triangular matrices, and justify the structure of $\\exp(T)$ for a real quasi-upper triangular $T$. As an advanced numerical consideration, discuss how scaling and squaring can improve the accuracy and stability of this computation in finite precision arithmetic, including how one would choose a scaling parameter and the role of squaring.\n\nReport as your final result the $(1,3)$ entry of $\\exp(A)$ as a closed-form analytic expression. Do not approximate or round. Interpret all trigonometric function arguments in radians.", "solution": "The problem is first validated to ensure it is scientifically sound, well-posed, and objective.\n\n### Step 1: Extract Givens\n- The matrix $A \\in \\mathbb{R}^{3 \\times 3}$ is given by $A = \\begin{pmatrix} 0 & 1 & 1 \\\\ -1 & 0 & 0 \\\\ 0 & 0 & 2 \\end{pmatrix}$.\n- The task is to compute the matrix exponential $\\exp(A)$.\n- The method prescribed is to use the real Schur form $A = Q T Q^*$, compute $\\exp(T)$ from its blocks, and then find $\\exp(A) = Q \\exp(T) Q^*$.\n- The derivation must start from the definition of the matrix exponential and justify the computation for block matrices.\n- An advanced numerical consideration is required: a discussion of the scaling and squaring method.\n- The final result to be reported is the $(1,3)$ entry of $\\exp(A)$ as a closed-form analytic expression.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded:** The problem is based on fundamental concepts of numerical linear algebra: the matrix exponential and the real Schur decomposition. These are standard, well-established topics. The procedure outlined is a recognized algorithm for computing the matrix exponential.\n- **Well-Posed:** The matrix $A$ is explicitly given. The real Schur decomposition of any real matrix exists. The matrix exponential is uniquely defined. The problem asks for a specific, calculable quantity. The problem is self-contained and unambiguous.\n- **Objective:** The problem is stated in precise mathematical language, free of any subjectivity or opinion.\n\n### Step 3: Verdict and Action\nThe problem is valid as it is mathematically sound, well-posed, and objective. A complete solution will be provided.\n\n### Solution Derivation\n\nThe matrix exponential of a square matrix $M$ is defined by the power series\n$$\n\\exp(M) = \\sum_{k=0}^{\\infty} \\frac{1}{k!} M^k.\n$$\nA fundamental property of the matrix exponential relates to similarity transformations. If a matrix $A$ can be written as $A = S B S^{-1}$ for some invertible matrix $S$, then its powers are given by $A^k = (S B S^{-1})^k = S B^k S^{-1}$. Substituting this into the power series definition gives\n$$\n\\exp(A) = \\sum_{k=0}^{\\infty} \\frac{1}{k!} (S B^k S^{-1}) = S \\left( \\sum_{k=0}^{\\infty} \\frac{1}{k!} B^k \\right) S^{-1} = S \\exp(B) S^{-1}.\n$$\nThe real Schur decomposition theorem states that for any real matrix $A \\in \\mathbb{R}^{n \\times n}$, there exists a real orthogonal matrix $Q$ (i.e., $Q^T Q = I$, so $Q^{-1} = Q^T = Q^*$) and a real quasi-upper triangular matrix $T$ such that $A = Q T Q^T$. The matrix $T$ is block upper triangular, with $1 \\times 1$ diagonal blocks corresponding to real eigenvalues and $2 \\times 2$ diagonal blocks whose eigenvalues are complex conjugate pairs.\nUsing the property above, we have $\\exp(A) = Q \\exp(T) Q^T$. We must therefore compute $\\exp(T)$.\n\nLet $T$ be a block upper triangular matrix, for instance, in a $2 \\times 2$ block form:\n$$\nT = \\begin{pmatrix} T_{11} & T_{12} \\\\ 0 & T_{22} \\end{pmatrix}.\n$$\nThe powers of $T$ maintain this block upper triangular structure:\n$$\nT^k = \\begin{pmatrix} T_{11}^k & U_k \\\\ 0 & T_{22}^k \\end{pmatrix},\n$$\nwhere $U_k$ is a more complex term. The exponential of $T$ is then\n$$\n\\exp(T) = \\sum_{k=0}^{\\infty} \\frac{1}{k!} T^k = \\begin{pmatrix} \\sum_{k=0}^{\\infty} \\frac{1}{k!} T_{11}^k & \\sum_{k=0}^{\\infty} \\frac{1}{k!} U_k \\\\ 0 & \\sum_{k=0}^{\\infty} \\frac{1}{k!} T_{22}^k \\end{pmatrix} = \\begin{pmatrix} \\exp(T_{11}) & U \\\\ 0 & \\exp(T_{22}) \\end{pmatrix}.\n$$\nThe diagonal blocks of $\\exp(T)$ are the exponentials of the diagonal blocks of $T$. The off-diagonal block $U$ can be found by solving the differential equation for $Y(t) = \\exp(tT)$, which is $Y'(t) = T Y(t)$ with $Y(0)=I$. For the block structure shown, this leads to a system of equations for the blocks of $Y(t)$. The off-diagonal block $Y_{12}(t)$ satisfies $Y'_{12}(t) = T_{11}Y_{12}(t) + T_{12}Y_{22}(t)$, with $Y_{12}(0)=0$. The solution, by variation of parameters, is $Y_{12}(t) = \\int_0^t \\exp((t-s)T_{11}) T_{12} \\exp(sT_{22}) ds$. The block $U$ is then $Y_{12}(1)$.\n\nNow, we apply this procedure to the given matrix $A = \\begin{pmatrix} 0 & 1 & 1 \\\\ -1 & 0 & 0 \\\\ 0 & 0 & 2 \\end{pmatrix}$. We first find its real Schur form $A=QTQ^T$. The characteristic equation is $\\det(A-\\lambda I) = (2-\\lambda)(\\lambda^2+1)=0$, yielding eigenvalues $\\lambda_1 = 2$, and $\\lambda_{2,3} = \\pm i$.\nThe matrix $A$ can be partitioned as\n$$\nA = \\begin{pmatrix} B & C \\\\ 0 & D \\end{pmatrix}, \\quad \\text{where} \\quad B = \\begin{pmatrix} 0 & 1 \\\\ -1 & 0 \\end{pmatrix}, \\quad C = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}, \\quad D = (2).\n$$\nThe eigenvalues of the $2 \\times 2$ block $B$ are $\\pm i$, and the eigenvalue of the $1 \\times 1$ block $D$ is $2$. This block structure precisely matches the structure of a real Schur form. Therefore, the matrix $A$ is already in a real Schur form. We can choose $T=A$ and the orthogonal matrix $Q=I$.\nThus, we need to compute $\\exp(A)$ by computing the exponential of its blocks. We have\n$$\nT_{11} = B = \\begin{pmatrix} 0 & 1 \\\\ -1 & 0 \\end{pmatrix}, \\quad T_{12} = C = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}, \\quad T_{22} = D = (2).\n$$\nThe exponentials of the diagonal blocks are:\n1.  For $T_{22}=(2)$, $\\exp(T_{22}) = \\exp(2) = e^2$.\n2.  For $T_{11} = \\begin{pmatrix} 0 & 1 \\\\ -1 & 0 \\end{pmatrix}$, we recognize it as the generator of rotations. Its exponential is given by the formula $\\exp(t T_{11}) = (\\cos t)I + (\\sin t)T_{11}$. For $t=1$:\n    $$\n    \\exp(T_{11}) = \\begin{pmatrix} \\cos 1 & \\sin 1 \\\\ -\\sin 1 & \\cos 1 \\end{pmatrix}.\n    $$\nThe off-diagonal block $U$ of $\\exp(A)$ is given by the integral formula at $t=1$:\n$$\nU = \\int_0^1 \\exp((1-s)T_{11}) T_{12} \\exp(sT_{22}) ds.\n$$\nSubstituting the known quantities:\n$$\nU = \\int_0^1 \\begin{pmatrix} \\cos(1-s) & \\sin(1-s) \\\\ -\\sin(1-s) & \\cos(1-s) \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} e^{2s} ds = \\int_0^1 \\begin{pmatrix} \\cos(1-s) e^{2s} \\\\ -\\sin(1-s) e^{2s} \\end{pmatrix} ds.\n$$\nWe integrate each component. Let $u=1-s$, so $s=1-u$ and $ds=-du$. The limits change from $s \\in [0,1]$ to $u \\in [1,0]$.\nThe first component is:\n$$\n\\int_0^1 \\cos(1-s) e^{2s} ds = \\int_1^0 \\cos(u) e^{2(1-u)} (-du) = e^2 \\int_0^1 e^{-2u} \\cos(u) du.\n$$\nUsing the standard integral $\\int e^{ax}\\cos(bx)dx = \\frac{e^{ax}}{a^2+b^2}(a\\cos(bx)+b\\sin(bx))$, with $a=-2, b=1$:\n$$\ne^2 \\left[ \\frac{e^{-2u}}{(-2)^2+1^2}(-2\\cos u + \\sin u) \\right]_0^1 = \\frac{e^2}{5} \\left[ e^{-2}(-2\\cos 1 + \\sin 1) - e^0(-2\\cos 0 + \\sin 0) \\right] = \\frac{1}{5}(\\sin 1 - 2\\cos 1 + 2e^2).\n$$\nThe second component is:\n$$\n\\int_0^1 -\\sin(1-s) e^{2s} ds = \\int_1^0 -\\sin(u) e^{2(1-u)} (-du) = -e^2 \\int_0^1 e^{-2u} \\sin(u) du.\n$$\nUsing the standard integral $\\int e^{ax}\\sin(bx)dx = \\frac{e^{ax}}{a^2+b^2}(a\\sin(bx)-b\\cos(bx))$:\n$$\n-e^2 \\left[ \\frac{e^{-2u}}{5}(-2\\sin u - \\cos u) \\right]_0^1 = -\\frac{e^2}{5} \\left[ e^{-2}(-2\\sin 1 - \\cos 1) - e^0(-2\\sin 0 - \\cos 0) \\right] = \\frac{1}{5}(2\\sin 1 + \\cos 1 - e^2).\n$$\nAssembling the full matrix $\\exp(A)$:\n$$\n\\exp(A) = \\begin{pmatrix} \\exp(T_{11}) & U \\\\ 0 & \\exp(T_{22}) \\end{pmatrix} = \\begin{pmatrix} \\cos 1 & \\sin 1 & \\frac{1}{5}(\\sin 1 - 2\\cos 1 + 2e^2) \\\\ -\\sin 1 & \\cos 1 & \\frac{1}{5}(2\\sin 1 + \\cos 1 - e^2) \\\\ 0 & 0 & e^2 \\end{pmatrix}.\n$$\nThe problem asks for the $(1,3)$ entry of this matrix.\n$$\n[\\exp(A)]_{1,3} = \\frac{1}{5} (\\sin 1 - 2\\cos 1 + 2e^2).\n$$\n\n### Discussion on Scaling and Squaring\nThe computation of the matrix exponential via its power series $\\exp(A) = \\sum_{k=0}^{\\infty} A^k/k!$ can be numerically unstable in finite precision arithmetic if the norm of $A$ is large. Large intermediate terms $A^k/k!$ can lead to catastrophic cancellation when summed. The \"scaling and squaring\" method mitigates this issue. It is based on the identity $\\exp(A) = (\\exp(A/s))^s$.\n\nThe method proceeds as follows:\n1.  **Scaling:** Choose a scaling factor $s$, typically a power of two, $s=2^m$, such that the norm of the scaled matrix, $\\|A/s\\|$, is sufficiently small (e.g., less than $1$). The integer $m$ is chosen as the smallest non-negative integer satisfying this condition for a chosen norm (e.g., $\\|A\\|_{\\infty}/2^m \\le \\theta$, where $\\theta$ is a pre-defined threshold). A smaller norm ensures rapid convergence of approximations to $\\exp(A/s)$.\n2.  **Approximation:** Compute an approximation to $X_0 = \\exp(A/s)$. Because the norm of the argument $A/s$ is small, a truncated Taylor series or, more robustly, a Padé approximant of a moderate degree can provide high accuracy with a low truncation error.\n3.  **Squaring:** Reverse the scaling by repeatedly squaring the result. If $s = 2^m$, then $\\exp(A) = (\\exp(A/2^m))^{2^m}$. This is computed via $m$ matrix squarings: $X_1 = X_0^2$, $X_2 = X_1^2, \\dots, X_m = X_{m-1}^2 = \\exp(A)$.\n\nThe main trade-off is between truncation error and round-off error. A larger scaling factor $s$ (larger $m$) reduces the truncation error of the initial approximation but requires more squaring steps. Each squaring step can accumulate round-off errors, which may grow, especially if the norms of the intermediate matrices $X_j$ become large. Choosing an optimal scaling parameter $m$ and the order of the Padé approximant is crucial for a robust algorithm and involves balancing these error sources. The scaling and squaring method forms the foundation of most modern, high-quality library routines for computing the matrix exponential. In the context of the Schur method, this technique could be applied to compute the exponentials of the diagonal blocks $\\exp(T_{ii})$ if their norms were large. For the given problem, $\\|T_{11}\\|_{\\infty} = 1$, so scaling is not strictly necessary for this block, and the analytic solution is superior.", "answer": "$$\\boxed{\\frac{1}{5}(\\sin(1) - 2\\cos(1) + 2e^2)}$$", "id": "3596233"}, {"introduction": "While eigenvalues predict the long-term behavior of a dynamical system, the Schur decomposition reveals crucial information about its short-term dynamics. This exercise uses the Schur form as a powerful diagnostic tool to quantify a matrix's departure from normality, a property that governs transient growth and pseudospectral effects. By implementing the code, you will empirically link the size of the off-diagonal part of the Schur factor $T$ to these otherwise surprising behaviors, building a deeper intuition for why non-normal matrices are so different from normal ones. [@problem_id:3596183]", "problem": "You are to write a complete, runnable program that quantifies the departure from normality of a square matrix via the strictly upper triangular part of its Schur form and relates this quantity to transient growth and pseudospectral features. Work within the context of numerical linear algebra and the Schur decomposition.\n\nStart from the following fundamental base:\n- For any square complex matrix $A \\in \\mathbb{C}^{n \\times n}$, there exists a unitary matrix $Q \\in \\mathbb{C}^{n \\times n}$ and an upper triangular matrix $T \\in \\mathbb{C}^{n \\times n}$ such that $A = Q T Q^*$, where $Q^*$ denotes the conjugate transpose of $Q$. This is the Schur decomposition.\n- The Frobenius norm is unitarily invariant, i.e., $\\|A\\|_F = \\|T\\|_F$.\n- A matrix $A$ is normal if and only if $A^* A = A A^*$. Equivalently, $A$ is normal if and only if its Schur form $T$ is diagonal.\n- The diagonal entries of $T$ are the eigenvalues of $A$.\n\nYour program must:\n1. For each test matrix $A$, compute the Schur decomposition $A = Q T Q^*$ and the quantity $D_F = \\|T - \\operatorname{diag}(T)\\|_F$, where $\\operatorname{diag}(T)$ denotes the diagonal matrix formed from the diagonal of $T$. This $D_F$ quantifies the departure from normality through the Frobenius norm of the strictly upper triangular part of $T$.\n2. Verify the identity that relates $D_F$ to the eigenvalues of $A$: report the residual\n   $$\\delta = \\left| \\|T - \\operatorname{diag}(T)\\|_F - \\sqrt{\\|A\\|_F^2 - \\sum_{i=1}^n |\\lambda_i|^2} \\right|,$$\n   where $\\{\\lambda_i\\}_{i=1}^n$ are the eigenvalues of $A$. Use the diagonal entries of $T$ for $\\{\\lambda_i\\}$.\n3. Quantify discrete-time transient growth by computing\n   $$M_K = \\max_{1 \\le k \\le K} \\|A^k\\|_2,$$\n   where $\\|\\cdot\\|_2$ denotes the induced $2$-norm (largest singular value) and $K$ is a fixed positive integer specified below.\n4. Quantify pseudospectral features via the peak resolvent norm on a specified grid of complex points. For each complex $z$ in the grid, compute\n   $$R(z) = \\|(z I - A)^{-1}\\|_2 = \\frac{1}{\\sigma_{\\min}(z I - A)},$$\n   where $\\sigma_{\\min}(\\cdot)$ denotes the smallest singular value, and report\n   $$R_{\\max} = \\max_{z \\in \\mathcal{G}} R(z).$$\n\nUse the following test suite to ensure coverage:\n- Test case 1 (boundary normal case): $A_1 = 0.9 I_6$, where $I_6$ is the $6 \\times 6$ identity matrix.\n- Test case 2 (moderately nonnormal): $A_2 = 0.9 I_6 + 0.2 N$, where $N \\in \\mathbb{R}^{6 \\times 6}$ has ones on the first superdiagonal and zeros elsewhere, i.e., $N_{i,i+1} = 1$ for $i = 1,\\dots,5$ and $N_{ij} = 0$ otherwise.\n- Test case 3 (highly nonnormal): $A_3 = 0.9 I_6 + 1.0 N$, with $N$ as above.\n\nSet $K = 50$ for the transient growth computation. For the pseudospectral grid, use\n$$\\mathcal{G} = \\{ z = x + i y \\mid x \\in \\{1.00, 1.02, 1.04, \\dots, 1.20\\},\\ y \\in \\{-0.10, 0.00, 0.10\\} \\},$$\nwith angles measured in radians where applicable (note that this problem involves complex arithmetic but does not require explicit angles).\n\nFor each test case, your program must output a list $[D_F, M_K, R_{\\max}, \\delta]$ of four floating-point numbers. Your program should produce a single line of output containing the results for all test cases as a comma-separated list enclosed in square brackets, where each element is itself a four-element list for the corresponding test case. For example:\n\"[[D_F1,M_K1,Rmax1,delta1],[D_F2,M_K2,Rmax2,delta2],[D_F3,M_K3,Rmax3,delta3]]\".\nNo additional text should be printed beyond this single line.", "solution": "The user's problem is subjected to validation.\n\n### Step 1: Extract Givens\n- **Task**: For a given square matrix $A$, compute four quantities related to its non-normality:\n    1. $D_F$: Departure from normality, defined as $\\|T - \\operatorname{diag}(T)\\|_F$, where $A=QTQ^*$ is the Schur decomposition.\n    2. $\\delta$: A residual verifying the identity $D_F^2 = \\|A\\|_F^2 - \\sum_{i=1}^n |\\lambda_i|^2$, defined as $\\delta = \\left| D_F - \\sqrt{\\|A\\|_F^2 - \\sum_{i=1}^n |\\lambda_i|^2} \\right|$.\n    3. $M_K$: A measure of discrete-time transient growth, defined as $M_K = \\max_{1 \\le k \\le K} \\|A^k\\|_2$.\n    4. $R_{\\max}$: A measure of the pseudospectrum's magnitude, defined as $R_{\\max} = \\max_{z \\in \\mathcal{G}} \\|(z I - A)^{-1}\\|_2$.\n- **Constants and Definitions**:\n    - $A \\in \\mathbb{C}^{n \\times n}$.\n    - $A = Q T Q^*$ is the Schur decomposition, with $Q$ unitary and $T$ upper triangular.\n    - $Q^*$ is the conjugate transpose of $Q$.\n    - $\\operatorname{diag}(T)$ is the diagonal matrix with the same diagonal as $T$.\n    - $\\|\\cdot\\|_F$ is the Frobenius norm.\n    - $\\|\\cdot\\|_2$ is the induced $2$-norm (spectral norm).\n    - $\\lambda_i$ are the eigenvalues of $A$, taken from the diagonal of $T$.\n    - $\\sigma_{\\min}(\\cdot)$ is the smallest singular value.\n    - $K = 50$.\n- **Test Matrices ($n=6$)**:\n    - $A_1 = 0.9 I_6$.\n    - $A_2 = 0.9 I_6 + 0.2 N$, where $N$ is a $6 \\times 6$ matrix with $N_{i,i+1} = 1$ and all other entries zero.\n    - $A_3 = 0.9 I_6 + 1.0 N$.\n- **Pseudospectral Grid**:\n    - $\\mathcal{G} = \\{ z = x + i y \\mid x \\in \\{1.00, 1.02, \\dots, 1.20\\},\\ y \\in \\{-0.10, 0.00, 0.10\\} \\}$.\n- **Output Format**: For each test case, produce a list $[D_F, M_K, R_{\\max}, \\delta]$. The final output must be a single string representing a list of these lists, e.g., `[[...],[...],[...]]`.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientifically Grounded**: The problem is a standard exercise in numerical linear algebra, based on the Schur decomposition and its relationship to normality, transient dynamics, and pseudospectra. All concepts and definitions ($A=QTQ^*$, matrix norms, eigenvalues, resolvent) are fundamental and correctly stated. The identity for $D_F$ is a known consequence of the unitary invariance of the Frobenius norm.\n2.  **Well-Posed**: The problem is well-posed. For any given square matrix $A$, the Schur decomposition exists, and all four quantities ($D_F, \\delta, M_K, R_{\\max}$) are uniquely computable through standard numerical algorithms. The parameters $K$ and the grid $\\mathcal{G}$ are explicitly defined.\n3.  **Objective**: The problem is stated in precise, objective mathematical language, free from any subjectivity or ambiguity.\n4.  **Complete and Consistent**: All necessary information to solve the problem is provided. There are no contradictions in the definitions or constraints.\n5.  **Not Trivial**: While the first test case is a simple normal matrix serving as a baseline, the subsequent cases involve non-trivial non-normal matrices that will exhibit the phenomena of interest (transient growth and large pseudospectra). The problem requires the implementation of several distinct numerical computations, which is a substantive task.\n\n### Step 3: Verdict and Action\nThe problem is **valid** as it is scientifically sound, well-posed, objective, and complete. It represents a a canonical numerical experiment in the study of non-normal matrices. The solution will proceed.\n\n---\n\nThe objective is to quantify the degree of non-normality of a matrix $A \\in \\mathbb{C}^{n \\times n}$ and to demonstrate its correlation with two major consequences: transient growth in dynamical systems and the extent of its pseudospectrum. We will compute four metrics for each of the three test matrices.\n\n**Theoretical Framework**\n\nThe foundation of this analysis is the **Schur decomposition**, which states that any square matrix $A$ can be factored as $A = Q T Q^*$, where $Q$ is a unitary matrix ($Q^*Q = I$) and $T$ is an upper triangular matrix. The diagonal entries of $T$ are the eigenvalues of $A$, denoted $\\{\\lambda_i\\}_{i=1}^n$. A matrix $A$ is defined as **normal** if it commutes with its conjugate transpose, i.e., $A A^* = A^* A$. A key theorem states that a matrix is normal if and only if its Schur form $T$ is a diagonal matrix.\n\n**1. Departure from Normality ($D_F$) and Identity Verification ($\\delta$)**\n\nThe degree to which $T$ is not diagonal provides a quantitative measure of $A$'s departure from normality. We measure this using the Frobenius norm of the strictly upper triangular part of $T$. Let $T_{off} = T - \\operatorname{diag}(T)$. The quantity is then:\n$$D_F = \\|T_{off}\\|_F = \\|T - \\operatorname{diag}(T)\\|_F$$\nA value of $D_F=0$ implies $T$ is diagonal and $A$ is normal. A larger $D_F$ signifies a greater departure from normality.\n\nA useful identity connects $D_F$ to the Frobenius norm of the original matrix $A$ and its eigenvalues. The Frobenius norm is unitarily invariant, meaning $\\|A\\|_F = \\|Q T Q^*\\|_F = \\|T\\|_F$. The squared Frobenius norm of the triangular matrix $T$ can be partitioned into the sum of squares of its diagonal and off-diagonal entries:\n$$\\|T\\|_F^2 = \\sum_{i,j=1}^n |T_{ij}|^2 = \\sum_{i=1}^n |T_{ii}|^2 + \\sum_{i < j} |T_{ij}|^2$$\nRecognizing that $T_{ii} = \\lambda_i$ and that $\\sum_{i < j} |T_{ij}|^2$ is precisely the squared Frobenius norm of the strictly upper triangular part of $T$, which is $D_F^2$, we have:\n$$\\|A\\|_F^2 = \\|T\\|_F^2 = \\sum_{i=1}^n |\\lambda_i|^2 + D_F^2$$\nRearranging this gives an explicit formula for $D_F$ in terms of $A$ and its eigenvalues:\n$$D_F = \\sqrt{\\|A\\|_F^2 - \\sum_{i=1}^n |\\lambda_i|^2}$$\nThe problem requires us to compute the residual $\\delta$ of this identity to verify its numerical validity:\n$$\\delta = \\left| D_F - \\sqrt{\\|A\\|_F^2 - \\sum_{i=1}^n |\\lambda_i|^2} \\right|$$\nIn exact arithmetic, $\\delta$ would be $0$. In floating-point arithmetic, a small non-zero $\\delta$ reflects the accumulation of numerical errors.\n\n**2. Transient Growth ($M_K$)**\n\nFor discrete dynamical systems of the form $x_{k+1} = A x_k$, the norm of the state vector evolves as $\\|x_k\\| = \\|A^k x_0\\|$. The behavior of $\\|A^k\\|$ as $k \\to \\infty$ is determined by the spectral radius $\\rho(A) = \\max_i |\\lambda_i|$. If $\\rho(A) < 1$, then $\\|A^k\\| \\to 0$. However, for non-normal matrices, the norm $\\|A^k\\|$ can experience significant growth for small $k$ before this eventual decay. This is called **transient growth**. For a normal matrix with $\\rho(A) < 1$, we have $\\|A^k\\|_2 = (\\rho(A))^k$, a strictly decreasing sequence. The presence of transient growth is a direct consequence of non-normality. We quantify this by computing the maximum norm achieved over a finite horizon $K$:\n$$M_K = \\max_{1 \\le k \\le K} \\|A^k\\|_2$$\nwhere $\\|\\cdot\\|_2$ is the spectral norm (the largest singular value).\n\n**3. Pseudospectral Features ($R_{\\max}$)**\n\nThe **pseudospectrum** of a matrix $A$, denoted $\\Lambda_{\\epsilon}(A)$, is the set of complex numbers $z$ that are eigenvalues of a nearby matrix $A+E$ with $\\|E\\|_2 \\le \\epsilon$. An equivalent definition is $\\Lambda_{\\epsilon}(A) = \\{z \\in \\mathbb{C} \\mid \\|(zI-A)^{-1}\\|_2 \\ge \\epsilon^{-1}\\}$. The matrix $(zI-A)^{-1}$ is the **resolvent** of $A$ at $z$.\n\nFor a normal matrix, the resolvent norm is simply the reciprocal of the distance to the spectrum: $\\|(zI-A)^{-1}\\|_2 = 1/\\text{dist}(z, \\Lambda(A))$. The norm is large only when $z$ is very close to an eigenvalue. For a non-normal matrix, the resolvent norm can be extremely large for $z$ far from any eigenvalue, indicating that the matrix is sensitive to perturbations. Large regions of high resolvent norm are characteristic of highly non-normal matrices.\n\nWe will probe this behavior by computing the peak resolvent norm on a specified grid $\\mathcal{G}$ of complex numbers:\n$$R_{\\max} = \\max_{z \\in \\mathcal{G}} \\|(z I - A)^{-1}\\|_2$$\nThe computation uses the identity $\\|(zI-A)^{-1}\\|_2 = 1/\\sigma_{\\min}(zI-A)$, where $\\sigma_{\\min}$ is the smallest singular value. A large value of $R_{\\max}$ on a grid away from the spectrum indicates significant non-normality.\n\nThe three test cases will illustrate the tight coupling between these three measures: as non-normality increases (from $A_1$ to $A_3$), we expect to see corresponding increases in $D_F$, $M_K$, and $R_{\\max}$.", "answer": "```python\nimport numpy as np\nimport scipy.linalg\n\ndef solve():\n    \"\"\"\n    Computes measures of non-normality for a set of test matrices.\n    \"\"\"\n    # Define problem constants\n    n = 6\n    K = 50\n    identity_n = np.identity(n)\n    \n    # Define matrix N (nilpotent, ones on first superdiagonal)\n    N = np.diag(np.ones(n - 1), k=1)\n    \n    # Define the test cases\n    test_cases = [\n        0.9 * identity_n,                     # Case 1: A1 (normal)\n        0.9 * identity_n + 0.2 * N,           # Case 2: A2 (moderately nonnormal)\n        0.9 * identity_n + 1.0 * N,           # Case 3: A3 (highly nonnormal)\n    ]\n\n    # Define the pseudospectral grid G\n    x_grid = np.linspace(1.0, 1.2, 11)\n    y_grid = np.array([-0.10, 0.00, 0.10])\n    G_grid = [x + 1j*y for x in x_grid for y in y_grid]\n\n    all_results = []\n    \n    for A in test_cases:\n        # 1. Compute D_F: Departure from normality\n        T, _ = scipy.linalg.schur(A, output='complex')\n        diag_T = np.diag(np.diag(T))\n        T_off = T - diag_T\n        D_F = np.linalg.norm(T_off, 'fro')\n\n        # 2. Verify identity and compute residual delta\n        norm_A_F_sq = np.linalg.norm(A, 'fro')**2\n        lambdas = np.diag(T)\n        sum_lambda_sq = np.sum(np.abs(lambdas)**2)\n        \n        # Handle potential floating point inaccuracies where norm_A_F_sq < sum_lambda_sq\n        val_under_sqrt = norm_A_F_sq - sum_lambda_sq\n        if val_under_sqrt < 0:\n             val_under_sqrt = 0\n\n        identity_rhs = np.sqrt(val_under_sqrt)\n        delta = np.abs(D_F - identity_rhs)\n\n        # 3. Quantify transient growth M_K\n        M_K = 0.0\n        A_power_k = np.identity(n)\n        for _ in range(K):\n            A_power_k = A_power_k @ A\n            norm_k = np.linalg.norm(A_power_k, 2)\n            if norm_k > M_K:\n                M_K = norm_k\n\n        # 4. Quantify pseudospectral peak R_max\n        R_max = 0.0\n        for z in G_grid:\n            # Compute resolvent norm as 1 / sigma_min(zI - A)\n            zI_minus_A = z * identity_n - A\n            s_min = np.linalg.svd(zI_minus_A, compute_uv=False)[-1]\n            resolvent_norm = 1.0 / s_min if s_min > 0 else np.inf\n            if resolvent_norm > R_max:\n                R_max = resolvent_norm\n        \n        # Store results for this case\n        all_results.append([D_F, M_K, R_max, delta])\n\n    # Format the final output string to remove spaces for exact match\n    final_output_str = str(all_results).replace(\" \", \"\")\n    print(final_output_str)\n\nsolve()\n```", "id": "3596183"}, {"introduction": "In many real-world applications, matrices are not static; they evolve over time. Re-computing a full Schur decomposition after every small change is computationally wasteful. This advanced practice challenges you to design and implement an efficient rank-1 update scheme for the Schur form, a task central to areas like adaptive filtering and recursive estimation. You will develop a principled criterion based on perturbation theory to decide whether a cheap approximation is sufficient or a more costly re-triangularization is necessary, providing insight into the design of robust numerical algorithms. [@problem_id:3596205]", "problem": "Given a square matrix $A \\in \\mathbb{C}^{n \\times n}$ and its Schur decomposition $A = Q T Q^{*}$ with $Q$ unitary and $T$ upper triangular (complex Schur form), consider a rank-$1$ perturbation $s \\, u v^{*}$ with $u, v \\in \\mathbb{C}^{n}$ and $s \\in \\{+1,-1\\}$. Starting only from the definitions of the Schur decomposition, unitary similarity, and first-order perturbation theory of triangular matrices, you must design, implement, and evaluate a practical rank-$1$ update scheme that produces an approximate updated Schur pair $(Q', T')$ for the perturbed matrix $A' = A + s \\, u v^{*}$. Your design must begin from the following foundational facts:\n\n- For any square matrix $A \\in \\mathbb{C}^{n \\times n}$, there exists a unitary matrix $Q \\in \\mathbb{C}^{n \\times n}$ such that $Q^{*} A Q = T$ with $T$ upper triangular (complex Schur form).\n- For any unitary matrix $Q$, unitary similarity preserves the spectrum: $Q^{*} A Q$ and $A$ have the same eigenvalues.\n- If $A = Q T Q^{*}$ and $A' = A + s \\, u v^{*}$, then, in the Schur basis, $Q^{*} A' Q = T + s \\, x y^{*}$ with $x = Q^{*} u$ and $y = Q^{*} v$.\n\nYour task is to do all of the following:\n\n- Propose an algorithm that, given $(Q,T)$ and $(u,v,s)$, produces an approximate updated $(Q',T')$ for $A' = A + s \\, u v^{*}$ by working in the Schur basis. The algorithm must:\n  - Compute $x = Q^{*} u$ and $y = Q^{*} v$, form $S = T + s \\, x y^{*}$.\n  - Decide, based on a principled, scale-aware criterion derived from first-order perturbation analysis of upper triangular matrices, whether to accept the approximation $(Q',T') = (Q,S)$ (which is exact for $A'$ but not necessarily triangular), or to perform a full re-triangularization of $S$ via a unitary similarity $S = Z R Z^{*}$ to obtain $(Q',T') = (Q Z, R)$.\n  - Your acceptance criterion must be expressed using only quantities available from $T$ and $S$, and it must depend on the magnitude of the strictly lower-triangular part of $S$ relative to a separation measure for the diagonal of $T$. Concretely, define the separation\n    $$ \\operatorname{sep}(T) = \\min_{1 \\le i < j \\le n} \\left| T_{j j} - T_{i i} \\right|, $$\n    with the convention $\\operatorname{sep}(T) = +\\infty$ if $n = 1$.\n    Let $L = \\operatorname{tril}(S,-1)$ denote the strictly lower-triangular part of $S$. Use an acceptance criterion of the form\n    $$ \\lVert L \\rVert_{\\mathrm{F}} \\le \\kappa \\, \\operatorname{sep}(T), $$\n    where $\\kappa$ is a user-chosen nonnegative scalar, and $\\lVert \\cdot \\rVert_{\\mathrm{F}}$ denotes the Frobenius norm. If the inequality holds, set $(Q',T') = (Q,S)$; otherwise, compute the complex Schur decomposition $S = Z R Z^{*}$ and set $(Q',T') = (Q Z,R)$.\n- Implement the above algorithm with the choice $\\kappa = 10^{-2}$.\n- For each test case below, report the following five scalar diagnostics:\n  - The Frobenius-norm residual\n    $$ r = \\left\\lVert Q' T' Q'^{*} - \\left( A + s \\, u v^{*} \\right) \\right\\rVert_{\\mathrm{F}}. $$\n  - The orthogonality error of $Q'$,\n    $$ o = \\left\\lVert Q'^{*} Q' - I \\right\\rVert_{\\mathrm{F}}. $$\n  - The triangularity ratio of $T'$,\n    $$ \\tau = \\frac{\\left\\lVert \\operatorname{tril}(T',-1) \\right\\rVert_{\\mathrm{F}}}{\\max\\left(\\left\\lVert T' \\right\\rVert_{\\mathrm{F}}, \\epsilon\\right)}, $$\n    where $\\epsilon = 10^{-300}$ is a tiny positive scalar used to prevent division by zero in degenerate cases.\n  - The scale-aware indicator\n    $$ \\rho = \\frac{\\left\\lVert \\operatorname{tril}(S,-1) \\right\\rVert_{\\mathrm{F}}}{\\max\\left(\\operatorname{sep}(T), \\epsilon\\right)}. $$\n  - The acceptance flag $a$, defined as $a = 1$ if the approximation $(Q',T') = (Q,S)$ is accepted (no re-triangularization performed), and $a = 0$ otherwise.\n- Your program must compute $(Q,T)$ internally from $A$ via the complex Schur decomposition and must implement the above decision and update logic exactly as described. No randomization is permitted.\n\nTest suite. Use the following four deterministic test cases, each specified by $(A, u, v, s)$:\n\n- Test case $1$ (small update on well-separated triangular $A$):\n  $$ A_{1} = \\begin{bmatrix}\n  1.0 & 2 \\times 10^{-3} & 0.0 \\\\\n  0.0 & 2.0 & 3 \\times 10^{-3} \\\\\n  0.0 & 0.0 & 3.0\n  \\end{bmatrix},$$\n  $$ u_{1} = \\begin{bmatrix} 1 \\times 10^{-10} \\\\ -2 \\times 10^{-10} \\\\ 3 \\times 10^{-10} \\end{bmatrix},$$\n  $$ v_{1} = \\begin{bmatrix} 4 \\times 10^{-10} \\\\ -5 \\times 10^{-10} \\\\ 6 \\times 10^{-10} \\end{bmatrix},$$\n  $$s_{1} = +1.$$\n- Test case $2$ (moderate update on a non-triangular $A$ with well-separated eigenvalues):\n  $$ A_{2} = \\begin{bmatrix}\n  0.0 & 1.0 & 0.2 \\\\\n  -1.0 & 0.1 & 0.3 \\\\\n  0.0 & -0.5 & 0.2\n  \\end{bmatrix},$$\n  $$ u_{2} = \\begin{bmatrix} 1 \\times 10^{-4} \\\\ 2 \\times 10^{-4} \\\\ -1 \\times 10^{-4} \\end{bmatrix},$$\n  $$ v_{2} = \\begin{bmatrix} -2 \\times 10^{-4} \\\\ 1 \\times 10^{-4} \\\\ 3 \\times 10^{-4} \\end{bmatrix},$$\n  $$ s_{2} = +1.$$\n- Test case $3$ (near-coincident eigenvalues; update large relative to separation):\n  $$ A_{3} = \\begin{bmatrix}\n  1.0 & 1 \\times 10^{-6} & 0.0 \\\\\n  0.0 & 1.0 + 10^{-8} & 1 \\times 10^{-6} \\\\\n  0.0 & 0.0 & 3.0\n  \\end{bmatrix},$$\n  $$ u_{3} = \\begin{bmatrix} 1 \\times 10^{-4} \\\\ -1 \\times 10^{-4} \\\\ 0.0 \\end{bmatrix},$$\n  $$ v_{3} = \\begin{bmatrix} 1 \\times 10^{-4} \\\\ 1 \\times 10^{-4} \\\\ 0.0 \\end{bmatrix},$$\n  $$ s_{3} = +1.$$\n- Test case $4$ (downdate on a $2 \\times 2$ triangular $A$):\n  $$ A_{4} = \\begin{bmatrix}\n  2.0 & 0.5 \\\\\n  0.0 & 2.5\n  \\end{bmatrix},$$\n  $$ u_{4} = \\begin{bmatrix} 1 \\times 10^{-3} \\\\ -1 \\times 10^{-3} \\end{bmatrix},$$\n  $$ v_{4} = \\begin{bmatrix} -1 \\times 10^{-3} \\\\ 2 \\times 10^{-3} \\end{bmatrix},$$\n  $$ s_{4} = -1.$$\n\nAngle units do not apply. No physical units are involved.\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case contributes a list of five numbers $[r, o, \\tau, \\rho, a]$ in that order, and the overall output is a list of these lists, for example $[[r_{1},o_{1},\\tau_{1},\\rho_{1},a_{1}],[r_{2},o_{2},\\tau_{2},\\rho_{2},a_{2}],\\ldots]$. All floating-point quantities must be printed in standard decimal or scientific notation; the acceptance flag $a$ must be printed as an integer $0$ or $1$.", "solution": "The task is to design, implement, and evaluate a rank-$1$ update algorithm for the complex Schur decomposition. Given the Schur decomposition $A = Q T Q^{*}$ of a matrix $A \\in \\mathbb{C}^{n \\times n}$, we seek an approximate Schur decomposition $(Q', T')$ for the perturbed matrix $A' = A + s \\, u v^{*}$, where $u, v \\in \\mathbb{C}^{n}$ and $s \\in \\{+1, -1\\}$.\n\nThe algorithm's design begins from the foundational principles of unitary similarity and perturbation theory.\n\nFirst, we transform the perturbation into the Schur basis defined by the columns of the unitary matrix $Q$. The perturbed matrix $A'$ can be represented in this basis via a unitary similarity transformation:\n$$ Q^{*} A' Q = Q^{*} (A + s \\, u v^{*}) Q $$\nUsing the properties of matrix multiplication and the fact that $Q^{*}Q = I$, this expression simplifies:\n$$ Q^{*} A Q + s (Q^{*} u) (v^{*} Q) = T + s (Q^{*} u) (Q^{*} v)^{*} $$\nLet us define the transformed vectors $x = Q^{*} u$ and $y = Q^{*} v$. The expression then becomes:\n$$ Q^{*} A' Q = T + s \\, x y^{*} $$\nWe denote this intermediate matrix by $S = T + s \\, x y^{*}$. The equation $A' = Q S Q^{*}$ is an exact representation of $A'$, and since $Q$ is unitary, the eigenvalues of $A'$ are identical to the eigenvalues of $S$. However, $S$ is the sum of an upper triangular matrix $T$ and a rank-$1$ matrix $s \\, x y^{*}$, and is therefore not guaranteed to be upper triangular itself. The rank-$1$ term introduces \"fill-in\" in the strictly lower-triangular part of $S$.\n\nThis presents a fundamental trade-off, leading to two possible paths for the algorithm:\n\nPath 1: Approximation by Acceptance\nWe can choose to accept the pair $(Q, S)$ as the updated Schur decomposition. We set $Q' = Q$ and $T' = S$.\n- Pro: This is computationally inexpensive, requiring only two matrix-vector multiplications to find $x$ and $y$, and one outer product to form $S$.\n- Con: The resulting matrix $T' = S$ is not, in general, upper triangular. Thus, $(Q', T')$ is not a true Schur decomposition but an approximation where the triangularity condition is relaxed. Nonetheless, the equality $A' = Q' T' (Q')^{*}$ holds to machine precision. The \"error\" is purely structural.\n\nPath 2: Full Re-triangularization\nTo restore the triangular structure, we can compute the Schur decomposition of the intermediate matrix $S$. Let this decomposition be $S = Z R Z^{*}$, where $Z$ is unitary and $R$ is upper triangular. Substituting this back into the expression for $A'$, we get:\n$$ A' = Q S Q^{*} = Q (Z R Z^{*}) Q^{*} = (Q Z) R (Z^{*} Q^{*}) = (Q Z) R (Q Z)^{*} $$\nWe then define the updated Schur pair as $Q' = QZ$ and $T' = R$.\n- Pro: This yields a true Schur decomposition for $A'$, as $Q'$ (a product of unitary matrices) is unitary and $T'$ is upper triangular.\n- Con: This is computationally expensive, as it requires a full Schur decomposition of the $n \\times n$ matrix $S$.\n\nThe core of the problem is to develop a principled criterion to decide between these two paths. First-order perturbation theory for eigenvalues and invariant subspaces provides the necessary insight. The stability of an invariant subspace (and by extension, the Schur vectors) under perturbation is inversely related to the separation of the corresponding eigenvalues from the rest of the spectrum. For a triangular matrix $T$, the eigenvalues are its diagonal entries $T_{ii}$. The separation of these eigenvalues is defined as:\n$$ \\operatorname{sep}(T) = \\min_{1 \\le i < j \\le n} |T_{jj} - T_{ii}| $$\nA small $\\operatorname{sep}(T)$ indicates that at least two eigenvalues are close, suggesting that the Schur basis is sensitive to perturbations. A large $\\operatorname{sep}(T)$ suggests a more stable basis.\n\nThe perturbation to the triangular structure of $T$ is the rank-$1$ term $s \\, x y^{*}$. The part of this perturbation that violates the upper triangular structure is captured by the strictly lower-triangular part of $S$, which we denote by $L = \\operatorname{tril}(S, -1)$. A small $\\lVert L \\rVert_{\\mathrm{F}}$ indicates that $S$ is \"close\" to being upper triangular.\n\nThe proposed decision criterion, $\\lVert L \\rVert_{\\mathrm{F}} \\le \\kappa \\, \\operatorname{sep}(T)$, elegantly combines these ideas. It states that we should accept the approximation (Path 1) if the magnitude of the structure-violating component of the perturbation, $\\lVert L \\rVert_{\\mathrm{F}}$, is small relative to the intrinsic spectral stability of the original problem, $\\operatorname{sep}(T)$. The scalar $\\kappa$ is a user-defined tolerance parameter controlling the trade-off between accuracy of the structure and computational cost.\n\nThe complete algorithm is as follows:\n1.  Given the Schur pair $(Q, T)$ for $A$, the perturbation vectors $u, v$, and the sign $s$.\n2.  Compute $x = Q^{*} u$ and $y = Q^{*} v$.\n3.  Form the intermediate matrix $S = T + s \\, x y^{*}$.\n4.  Compute the strictly lower-triangular part of $S$, $L = \\operatorname{tril}(S, -1)$.\n5.  Compute the Frobenius norm $\\lVert L \\rVert_{\\mathrm{F}}$.\n6.  Compute the diagonal separation $\\operatorname{sep}(T)$.\n7.  If $\\lVert L \\rVert_{\\mathrm{F}} \\le \\kappa \\, \\operatorname{sep}(T)$ (with $\\kappa = 10^{-2}$), set $(Q', T') = (Q, S)$ and the acceptance flag $a=1$.\n8.  Otherwise, compute the Schur decomposition $S = Z R Z^{*}$ and set $(Q', T') = (Q Z, R)$ and the acceptance flag $a=0$.\n9.  Calculate the required diagnostic metrics $(r, o, \\tau, \\rho, a)$ to evaluate the outcome.\n\nThe requested diagnostics serve to quantify the performance of the algorithm:\n-   $r = \\left\\lVert Q' T' (Q')^{*} - A' \\right\\rVert_{\\mathrm{F}}$: The residual norm. For a correct implementation, this should be close to machine precision in both Path 1 and Path 2, as the equality $A' = Q' T' (Q')^{*}$ is preserved algebraically.\n-   $o = \\left\\lVert (Q')^{*} Q' - I \\right\\rVert_{\\mathrm{F}}$: The orthogonality error. $Q'$ remains unitary in both paths, so this should also be close to machine precision.\n-   $\\tau = \\frac{\\left\\lVert \\operatorname{tril}(T',-1) \\right\\rVert_{\\mathrm{F}}}{\\max(\\left\\lVert T' \\right\\rVert_{\\mathrm{F}}, \\epsilon)}$: The triangularity ratio. This is the key structural metric. It will be non-zero for Path 1 and near-zero for Path 2.\n-   $\\rho = \\frac{\\left\\lVert \\operatorname{tril}(S,-1) \\right\\rVert_{\\mathrm{F}}}{\\max(\\operatorname{sep}(T), \\epsilon)}$: The scale-aware indicator. This is the value of the left-hand side of the decision criterion, normalized by $\\operatorname{sep}(T)$. The decision is equivalent to checking if $\\rho \\le \\kappa$.\n-   $a \\in \\{0, 1\\}$: The acceptance flag, indicating which path was taken.\n\nThis algorithm provides a practical and computationally aware method for updating a Schur decomposition, balancing the strict enforcement of mathematical structure against computational cost.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import schur\n\ndef solve():\n    \"\"\"\n    Implements and evaluates a rank-1 update scheme for the complex Schur decomposition.\n    \"\"\"\n    # Define the small positive scalar to prevent division by zero.\n    epsilon = 1e-300\n    kappa = 1e-2\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"A\": np.array([\n                [1.0, 2e-3, 0.0],\n                [0.0, 2.0, 3e-3],\n                [0.0, 0.0, 3.0]\n            ], dtype=np.complex128),\n            \"u\": np.array([1e-10, -2e-10, 3e-10], dtype=np.complex128),\n            \"v\": np.array([4e-10, -5e-10, 6e-10], dtype=np.complex128),\n            \"s\": 1.0\n        },\n        {\n            \"A\": np.array([\n                [0.0, 1.0, 0.2],\n                [-1.0, 0.1, 0.3],\n                [0.0, -0.5, 0.2]\n            ], dtype=np.complex128),\n            \"u\": np.array([1e-4, 2e-4, -1e-4], dtype=np.complex128),\n            \"v\": np.array([-2e-4, 1e-4, 3e-4], dtype=np.complex128),\n            \"s\": 1.0\n        },\n        {\n            \"A\": np.array([\n                [1.0, 1e-6, 0.0],\n                [0.0, 1.0 + 1e-8, 1e-6],\n                [0.0, 0.0, 3.0]\n            ], dtype=np.complex128),\n            \"u\": np.array([1e-4, -1e-4, 0.0], dtype=np.complex128),\n            \"v\": np.array([1e-4, 1e-4, 0.0], dtype=np.complex128),\n            \"s\": 1.0\n        },\n        {\n            \"A\": np.array([\n                [2.0, 0.5],\n                [0.0, 2.5]\n            ], dtype=np.complex128),\n            \"u\": np.array([1e-3, -1e-3], dtype=np.complex128),\n            \"v\": np.array([-1e-3, 2e-3], dtype=np.complex128),\n            \"s\": -1.0\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        A, u, v, s = case[\"A\"], case[\"u\"], case[\"v\"], case[\"s\"]\n        n = A.shape[0]\n\n        # Compute the initial Schur decomposition of A.\n        # We need the complex Schur form, which scipy.linalg.schur provides by default\n        # when the input is complex, or when output='complex'.\n        T, Q = schur(A, output='complex')\n\n        # Compute x and y in the Schur basis.\n        # Q* is the conjugate transpose of Q.\n        x = Q.conj().T @ u\n        y = Q.conj().T @ v\n\n        # Form the intermediate matrix S.\n        # np.outer(x, y.conj()) computes x y*.\n        S = T + s * np.outer(x, y.conj())\n\n        # Calculate a scale-aware indicator for the decision.\n        # 1. Compute sep(T)\n        diag_T = np.diag(T)\n        if n <= 1:\n            sep_T = np.inf\n        else:\n            # Create a matrix of all pairwise absolute differences of diagonal elements.\n            diffs = np.abs(diag_T[:, np.newaxis] - diag_T)\n            # Find the minimum of the strictly upper triangular part.\n            sep_T = np.min(diffs[np.triu_indices(n, k=1)])\n\n        # 2. Compute the norm of the strictly lower-triangular part of S.\n        L = np.tril(S, k=-1)\n        norm_L = np.linalg.norm(L, 'fro')\n\n        # 3. Compute the indicator rho\n        rho = norm_L / max(sep_T, epsilon)\n        \n        # Make the decision\n        if norm_L <= kappa * sep_T:\n            # Accept the approximation (Q, S)\n            Q_prime, T_prime = Q, S\n            a = 1\n        else:\n            # Perform full re-triangularization\n            R, Z = schur(S, output='complex')\n            Q_prime, T_prime = Q @ Z, R\n            a = 0\n\n        # Compute the five diagnostic scalars.\n        A_prime = A + s * np.outer(u, v.conj())\n        \n        # 1. Residual r\n        reconstruction_error = Q_prime @ T_prime @ Q_prime.conj().T - A_prime\n        r = np.linalg.norm(reconstruction_error, 'fro')\n        \n        # 2. Orthogonality error o\n        ortho_error = Q_prime.conj().T @ Q_prime - np.eye(n)\n        o = np.linalg.norm(ortho_error, 'fro')\n        \n        # 3. Triangularity ratio tau\n        norm_tril_T_prime = np.linalg.norm(np.tril(T_prime, k=-1), 'fro')\n        norm_T_prime = np.linalg.norm(T_prime, 'fro')\n        tau = norm_tril_T_prime / max(norm_T_prime, epsilon)\n        \n        # 4. rho is already computed.\n        # 5. a is already computed.\n        \n        all_results.append([r, o, tau, rho, float(a)])\n\n    # Format the final output string.\n    result_str = \"[\" + \",\".join([str(res).replace(\" \", \"\") for res in all_results]) + \"]\"\n    print(result_str)\n\nsolve()\n```", "id": "3596205"}]}