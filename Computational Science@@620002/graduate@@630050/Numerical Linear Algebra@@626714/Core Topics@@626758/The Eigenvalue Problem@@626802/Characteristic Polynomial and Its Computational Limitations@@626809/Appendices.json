{"hands_on_practices": [{"introduction": "To understand the computational challenges of the characteristic polynomial, we must first connect it to the underlying algebraic structure of the matrix. This exercise explores the profound link between a matrix's Jordan form, its characteristic polynomial $p_A(\\lambda)$, and its minimal polynomial $m_A(\\lambda)$. By analyzing a matrix with repeated eigenvalues and non-trivial Jordan blocks, you will gain a first-principles understanding of why the presence of multiple or clustered eigenvalues makes the root-finding problem for $p_A(\\lambda)$ inherently sensitive [@problem_id:3536789].", "problem": "Let $\\lambda_{0} \\in \\mathbb{R}$ and consider the Jordan block of size $k$ defined by $J_{k}(\\lambda_{0}) = \\lambda_{0} I_{k} + N_{k}$, where $I_{k}$ is the $k \\times k$ identity matrix and $N_{k}$ is the strictly upper-triangular matrix with ones on the first superdiagonal and zeros elsewhere. Define the $5 \\times 5$ matrix\n$$\nA \\;=\\; \\operatorname{diag}\\!\\big(J_{3}(\\lambda_{0}),\\, J_{2}(\\lambda_{0})\\big).\n$$\nStart from the following foundational definitions: the characteristic polynomial $p_{A}(\\lambda)$ is $p_{A}(\\lambda) = \\det(\\lambda I - A)$; the minimal polynomial $m_{A}(\\lambda)$ is the unique monic polynomial of least degree such that $m_{A}(A) = 0$; and a matrix $N$ is nilpotent if $N^{q} = 0$ for some positive integer $q$, with the smallest such $q$ called the index of nilpotency. Derive $m_{A}(\\lambda)$ and $p_{A}(\\lambda)$ from these definitions, and justify rigorously why the largest Jordan block size for an eigenvalue determines the exponent of the corresponding factor in the minimal polynomial, while the algebraic multiplicity determines the exponent in the characteristic polynomial. In particular, explain the role of the nilpotent parts $N_{3}$ and $N_{2}$ in determining the exponent of $(\\lambda - \\lambda_{0})$ in $m_{A}(\\lambda)$, and why repeated factors in $p_{A}(\\lambda)$ are numerically delicate: provide a principled argument, using only standard facts from numerical linear algebra, that computing $p_{A}(\\lambda)$ via coefficient determination is ill-conditioned when repeated eigenvalues are present, and that small perturbations generically alter the Jordan structure, thereby changing the multiplicities of roots of $p_{A}(\\lambda)$ and potentially the index in $m_{A}(\\lambda)$.\n\nGive your final answer as the explicit closed-form expression for the minimal polynomial $m_{A}(\\lambda)$ of the matrix $A$ above. Do not provide intermediate steps in your final answer. No rounding is required.", "solution": "The problem is first validated to be self-contained, scientifically grounded, and well-posed. All definitions and premises are standard in linear algebra and numerical analysis. The matrix $A$ is precisely defined as $A = \\operatorname{diag}(J_{3}(\\lambda_{0}),\\, J_{2}(\\lambda_{0}))$, where $J_{k}(\\lambda_{0})$ is a Jordan block of size $k$ with eigenvalue $\\lambda_{0} \\in \\mathbb{R}$. The tasks are to derive the characteristic and minimal polynomials of $A$, explain the connection between Jordan structure and these polynomials, and discuss the numerical implications. The problem is valid and we proceed to the solution.\n\nThe matrix $A$ is a $5 \\times 5$ block diagonal matrix:\n$$\nA = \\begin{pmatrix} J_{3}(\\lambda_{0}) & \\mathbf{0}_{3 \\times 2} \\\\ \\mathbf{0}_{2 \\times 3} & J_{2}(\\lambda_{0}) \\end{pmatrix}\n$$\nwhere $\\mathbf{0}_{m \\times n}$ is the $m \\times n$ zero matrix, and the Jordan blocks are defined as $J_{k}(\\lambda_{0}) = \\lambda_{0} I_{k} + N_{k}$. Specifically,\n$$\nJ_{3}(\\lambda_{0}) = \\begin{pmatrix} \\lambda_{0} & 1 & 0 \\\\ 0 & \\lambda_{0} & 1 \\\\ 0 & 0 & \\lambda_{0} \\end{pmatrix} \\quad \\text{and} \\quad J_{2}(\\lambda_{0}) = \\begin{pmatrix} \\lambda_{0} & 1 \\\\ 0 & \\lambda_{0} \\end{pmatrix}.\n$$\nThe matrices $N_{3}$ and $N_{2}$ are nilpotent matrices of the form:\n$$\nN_3 = \\begin{pmatrix} 0 & 1 & 0 \\\\ 0 & 0 & 1 \\\\ 0 & 0 & 0 \\end{pmatrix} \\quad \\text{and} \\quad N_2 = \\begin{pmatrix} 0 & 1 \\\\ 0 & 0 \\end{pmatrix}.\n$$\nFirst, we derive the characteristic polynomial, $p_{A}(\\lambda)$, from its definition $p_{A}(\\lambda) = \\det(\\lambda I - A)$. The matrix $\\lambda I - A$ is also block diagonal:\n$$\n\\lambda I - A = \\begin{pmatrix} \\lambda I_3 - J_3(\\lambda_0) & \\mathbf{0}_{3 \\times 2} \\\\ \\mathbf{0}_{2 \\times 3} & \\lambda I_2 - J_2(\\lambda_0) \\end{pmatrix}.\n$$\nThe determinant of a block diagonal matrix is the product of the determinants of its diagonal blocks:\n$$\np_{A}(\\lambda) = \\det(\\lambda I_3 - J_3(\\lambda_0)) \\cdot \\det(\\lambda I_2 - J_2(\\lambda_0)).\n$$\nLet us analyze a single block $\\lambda I_k - J_k(\\lambda_0)$:\n$$\n\\lambda I_k - J_k(\\lambda_0) = \\lambda I_k - (\\lambda_0 I_k + N_k) = (\\lambda - \\lambda_0) I_k - N_k.\n$$\nThis is an upper triangular matrix with the value $(\\lambda - \\lambda_0)$ at every diagonal position. The determinant of a triangular matrix is the product of its diagonal entries. Therefore, for a block of size $k$:\n$$\n\\det((\\lambda - \\lambda_0) I_k - N_k) = (\\lambda - \\lambda_0)^k.\n$$\nApplying this to our specific case:\n$$\n\\det(\\lambda I_3 - J_3(\\lambda_0)) = (\\lambda - \\lambda_0)^3 \\quad \\text{and} \\quad \\det(\\lambda I_2 - J_2(\\lambda_0)) = (\\lambda - \\lambda_0)^2.\n$$\nThus, the characteristic polynomial of $A$ is:\n$$\np_{A}(\\lambda) = (\\lambda - \\lambda_0)^3 \\cdot (\\lambda - \\lambda_0)^2 = (\\lambda - \\lambda_0)^5.\n$$\nThis derivation illustrates the general principle: the exponent of a factor $(\\lambda - \\lambda_i)$ in the characteristic polynomial is the sum of the sizes of all Jordan blocks associated with the eigenvalue $\\lambda_i$. This sum is, by definition, the algebraic multiplicity of $\\lambda_i$. For matrix $A$, the only eigenvalue is $\\lambda_0$, and its algebraic multiplicity is $3+2=5$.\n\nNext, we derive the minimal polynomial, $m_{A}(\\lambda)$. By the Cayley-Hamilton theorem, $p_{A}(A) = 0$, which implies that $m_{A}(\\lambda)$ must divide $p_{A}(\\lambda)$. Since the only irreducible factor of $p_{A}(\\lambda)$ is $(\\lambda - \\lambda_0)$, the minimal polynomial must be of the form $m_{A}(\\lambda) = (\\lambda - \\lambda_0)^q$ for some integer $q$ such that $1 \\le q \\le 5$. By definition, $m_{A}(\\lambda)$ is the monic polynomial of least degree such that $m_{A}(A) = \\mathbf{0}$. We need to find the smallest positive integer $q$ for which $(A - \\lambda_0 I)^q = \\mathbf{0}$.\n\nLet's evaluate the matrix polynomial $(A - \\lambda_0 I)^q$:\n$$\nA - \\lambda_0 I = \\operatorname{diag}(J_3(\\lambda_0) - \\lambda_0 I_3, J_2(\\lambda_0) - \\lambda_0 I_2) = \\operatorname{diag}(N_3, N_2).\n$$\nRaising this block diagonal matrix to the power of $q$ yields:\n$$\n(A - \\lambda_0 I)^q = \\operatorname{diag}(N_3^q, N_2^q).\n$$\nThis matrix is the zero matrix if and only if both $N_3^q = \\mathbf{0}$ and $N_2^q = \\mathbf{0}$.\nWe need to determine the index of nilpotency for $N_k$, which is the smallest integer $j$ such that $N_k^j = \\mathbf{0}$. For a standard nilpotent Jordan block $N_k$ of size $k$, the $1$'s on the superdiagonal move up one diagonal at a time with each power. $N_k^j$ has ones on the $j$-th superdiagonal. Thus, $N_k^{k-1}$ has a single $1$ in the top-right corner, and $N_k^k = \\mathbf{0}$. The index of nilpotency of $N_k$ is $k$.\nFor our blocks, the index of nilpotency of $N_3$ is $3$, and the index of nilpotency of $N_2$ is $2$.\nThe condition $N_3^q = \\mathbf{0}$ requires $q \\ge 3$.\nThe condition $N_2^q = \\mathbf{0}$ requires $q \\ge 2$.\nTo satisfy both conditions simultaneously, $q$ must be greater than or equal to the maximum of these required indices: $q \\ge \\max(3, 2) = 3$.\nThe minimal polynomial corresponds to the smallest such integer $q$, so $q=3$.\nTherefore, the minimal polynomial of $A$ is:\n$$\nm_A(\\lambda) = (\\lambda - \\lambda_0)^3.\n$$\nThis derivation rigorously demonstrates the general rule: the exponent of a factor $(\\lambda - \\lambda_i)$ in the minimal polynomial is the size of the *largest* Jordan block associated with the eigenvalue $\\lambda_i$. The nilpotent parts $N_3$ and $N_2$ are central to this. The matrix $(A - \\lambda_0 I)$ is nilpotent, and its index of nilpotency is precisely the degree of the minimal polynomial (minus any other distinct eigenvalue factors, of which there are none here). The index of nilpotency of $\\operatorname{diag}(N_{k_1}, N_{k_2}, \\dots)$ is $\\max(k_1, k_2, \\dots)$, which is the size of the largest block. The $N_3$ block, being the larger one, dictates the degree of the minimal polynomial because its nilpotency requirement ($q \\ge 3$) is the most stringent.\n\nFinally, we address the numerical delicacy of computing eigenvalues via the characteristic polynomial. The process of finding eigenvalues by first computing the coefficients of $p_A(\\lambda) = \\sum_{i=0}^n c_i \\lambda^i$ and then finding its roots is known to be numerically unstable, particularly when roots have high multiplicity.\nLet $p(\\lambda)$ be a polynomial with a root $\\lambda_0$ of multiplicity $m > 1$. The sensitivity of this root to perturbations in the polynomial's coefficients is high. Consider a small perturbation to the polynomial, $\\tilde{p}(\\lambda) = p(\\lambda) + \\epsilon g(\\lambda)$, where $\\epsilon$ is a small parameter. The roots of $\\tilde{p}(\\lambda)$ are perturbed from $\\lambda_0$ to $\\tilde{\\lambda}$. For $\\lambda$ near $\\lambda_0$, we can approximate $p(\\lambda) \\approx C(\\lambda - \\lambda_0)^m$ for some constant $C \\neq 0$. The equation for the perturbed roots is $\\tilde{p}(\\tilde{\\lambda}) = C(\\tilde{\\lambda} - \\lambda_0)^m + \\epsilon g(\\tilde{\\lambda}) \\approx 0$. Assuming $g(\\lambda_0) \\neq 0$, this gives $(\\tilde{\\lambda} - \\lambda_0)^m \\approx -\\epsilon g(\\lambda_0)/C$. The magnitude of the root perturbation is then:\n$$\n|\\tilde{\\lambda} - \\lambda_0| \\approx \\left| \\frac{\\epsilon g(\\lambda_0)}{C} \\right|^{1/m} = O(\\epsilon^{1/m}).\n$$\nFor $m>1$, the exponent $1/m$ is less than $1$. This means the perturbation in the root, $O(\\epsilon^{1/m})$, is much larger than the perturbation in the coefficients, $O(\\epsilon)$. For instance, if $m=5$ (as in our case for $p_{A}(\\lambda)$) and $\\epsilon=10^{-10}$, the root perturbation is of the order of $(10^{-10})^{1/5} = 10^{-2}$, an amplification of the error by a factor of $10^8$. This extreme sensitivity is a hallmark of an ill-conditioned problem.\n\nFurthermore, small perturbations to the matrix $A$ itself have profound consequences on its algebraic structure. The matrix $A$ is defective, meaning it is not diagonalizable. This is because the geometric multiplicity of $\\lambda_0$ (the number of Jordan blocks, which is $2$) is less than its algebraic multiplicity (which is $5$). It is a fundamental result of matrix perturbation theory that a defective matrix is not a \"generic\" case. A small, arbitrary perturbation $E$ to $A$, resulting in $A' = A + E$, will almost surely yield a matrix $A'$ with distinct eigenvalues.\nFor the matrix $A$, a generic perturbation $E$ of small norm will produce a perturbed matrix $A'$ with $5$ distinct eigenvalues $\\lambda_1', \\dots, \\lambda_5'$, all of a small distance from $\\lambda_0$.\nThe Jordan structure is shattered. The Jordan form of $A$ is $\\operatorname{diag}(J_3(\\lambda_0), J_2(\\lambda_0))$. The Jordan form of $A'$ will be a diagonal matrix $\\operatorname{diag}(\\lambda_1', \\dots, \\lambda_5')$, which consists of five $1 \\times 1$ Jordan blocks.\nThis drastically changes both the characteristic and minimal polynomials. The characteristic polynomial becomes $p_{A'}(\\lambda) = \\prod_{i=1}^5 (\\lambda - \\lambda_i')$, which has $5$ simple roots. The minimal polynomial, which for a diagonalizable matrix is the product of unique linear factors, becomes $m_{A'}(\\lambda) = p_{A'}(\\lambda)$. The degree of the minimal polynomial jumps from $3$ for $A$ to $5$ for $A'$.\nThis instability—the fact that an infinitesimal perturbation can radically change the Jordan structure, the multiplicities of roots, and the degree of the minimal polynomial—is why numerical methods for eigenvalues (like the QR algorithm) are designed to work directly with the matrix and avoid the intermediate, ill-conditioned step of forming the characteristic polynomial's coefficients. They iteratively transform the matrix to a form (like upper-triangular Schur form) from which eigenvalues can be read stably.", "answer": "$$\n\\boxed{(\\lambda - \\lambda_{0})^{3}}\n$$", "id": "3536789"}, {"introduction": "In exact arithmetic, the characteristic polynomial is invariant under any similarity transformation, meaning $p_A(\\lambda) = p_{S^{-1}AS}(\\lambda)$. However, this mathematical truth can be misleading in the world of finite-precision computation. This hands-on coding exercise allows you to empirically investigate this discrepancy by comparing the numerical stability of an orthogonal transformation ($Q^T A Q$) versus an ill-conditioned similarity transformation, demonstrating the vital role that stable transformations play in preserving accuracy [@problem_id:3536821].", "problem": "Consider the numerical computation of the characteristic polynomial $p_A(\\lambda)$ of an $n \\times n$ matrix $A$ defined by $p_A(\\lambda) = \\det(\\lambda I - A)$. It is a fundamental fact that $p_A$ is invariant under similarity transformations: for any invertible $S$, one has $p_A(\\lambda) = p_{S^{-1} A S}(\\lambda)$. A special case is an orthogonal similarity, where $Q$ is orthogonal (i.e., $Q^\\top Q = I$), yielding $p_A(\\lambda) = p_{Q^\\top A Q}(\\lambda)$. In floating-point arithmetic, computations are subject to rounding errors obeying the standard model $\\operatorname{fl}(x \\circ y) = (x \\circ y)(1 + \\delta)$ with $|\\delta| \\leq u$, where $u$ is the unit roundoff. In numerical linear algebra, orthogonal transformations are norm-preserving in the spectral norm, i.e., $\\|Q^\\top A Q\\|_2 = \\|A\\|_2$, while a general similarity by $S$ can magnify norms in a way controlled by the condition number $\\kappa_2(S) = \\|S\\|_2 \\|S^{-1}\\|_2$.\n\nYour task is to empirically compare the effect of orthogonal versus non-orthogonal similarity transformations on the numerical computation of the coefficients of $p_A(\\lambda)$ using a fixed, trace-based scheme. Specifically:\n\n- Use the Faddeev–LeVerrier recurrence to compute the coefficients of the characteristic polynomial. Let $B_0 = I$ and for $k = 1, 2, \\dots, n$, define\n  $$\n  c_k = -\\frac{1}{k}\\operatorname{tr}(A B_{k-1}), \\quad B_k = A B_{k-1} + c_k I,\n  $$\n  so that the characteristic polynomial is $p_A(\\lambda) = \\lambda^n + c_1 \\lambda^{n-1} + \\cdots + c_n$.\n- This recurrence is to be implemented in two numerically distinct arithmetic models:\n  1. Exact arithmetic over the rational numbers for a given integer matrix $A$, so that the output coefficients are mathematically exact (these define the ground truth for $p_A$).\n  2. Floating-point arithmetic in double precision, applied to transformed versions of $A$.\n\nUse the following scientific bases and definitions without further derivation: (i) invariance of the characteristic polynomial under similarity transformations, (ii) orthogonal matrices satisfy $Q^\\top Q = I$, (iii) the standard floating-point rounding model with unit roundoff $u$, and (iv) spectral norm invariance under orthogonal similarity, with general similarity potentially amplifying intermediate quantities proportionally to the condition number of the change of basis.\n\nFor each test case below, do the following:\n1. Construct an integer matrix $A \\in \\mathbb{Z}^{n \\times n}$ by drawing entries uniformly from $\\{-2,-1,0,1,2\\}$ using the specified random seed.\n2. Compute the exact coefficient vector $\\mathbf{c}_{\\mathrm{exact}} = (c_1,\\dots,c_n)$ using exact rational arithmetic (no floating-point used in this branch).\n3. Form an orthogonal matrix $Q$ via the $QR$ factorization of an $n \\times n$ matrix with independent standard normal entries using the specified seed, and define $A_{\\mathrm{orth}} = Q^\\top A Q$ in floating-point arithmetic.\n4. Form a diagonal, invertible, non-orthogonal similarity matrix $S = \\operatorname{diag}(10^{\\alpha_0}, \\dots, 10^{\\alpha_{n-1}})$ with specified exponents, and define $A_{\\mathrm{non}} = S^{-1} A S$ in floating-point arithmetic.\n5. Using floating-point arithmetic, compute $\\mathbf{c}_{\\mathrm{orth}}$ from $A_{\\mathrm{orth}}$ and $\\mathbf{c}_{\\mathrm{non}}$ from $A_{\\mathrm{non}}$ by the same Faddeev–LeVerrier scheme.\n6. Quantify the relative coefficient errors\n   $$\n   e_{\\mathrm{orth}} = \\frac{\\|\\mathbf{c}_{\\mathrm{orth}} - \\mathbf{c}_{\\mathrm{exact}}\\|_\\infty}{\\|\\mathbf{c}_{\\mathrm{exact}}\\|_\\infty}, \\quad\n   e_{\\mathrm{non}} = \\frac{\\|\\mathbf{c}_{\\mathrm{non}} - \\mathbf{c}_{\\mathrm{exact}}\\|_\\infty}{\\|\\mathbf{c}_{\\mathrm{exact}}\\|_\\infty}.\n   $$\n7. Report the amplification ratio $r = e_{\\mathrm{non}}/e_{\\mathrm{orth}}$ as a floating-point number.\n\nTest Suite:\n- Case 1 (happy path): $n = 6$, seed for $A$ is $2025$, seed for $Q$ is $123$, and $S = \\operatorname{diag}(10^{0}, 10^{1}, 10^{2}, 10^{3}, 10^{4}, 10^{5})$.\n- Case 2 (boundary: no non-orthogonal distortion): $n = 6$, same $A$ as Case 1, seed for $Q$ is $321$, and $S = I$ (i.e., all exponents are zero).\n- Case 3 (edge: stronger non-orthogonal distortion): $n = 6$, same $A$ as Case 1, seed for $Q$ is $999$, and $S = \\operatorname{diag}(10^{0}, 10^{2}, 10^{4}, 10^{6}, 10^{8}, 10^{10})$.\n\nAll matrices in Cases 1–3 are to be constructed deterministically from the stated seeds so that results are reproducible.\n\nYour program must:\n- Implement exact Faddeev–LeVerrier using rational arithmetic to produce $\\mathbf{c}_{\\mathrm{exact}}$ from the integer $A$.\n- Implement floating-point Faddeev–LeVerrier to produce $\\mathbf{c}_{\\mathrm{orth}}$ and $\\mathbf{c}_{\\mathrm{non}}$ from $A_{\\mathrm{orth}}$ and $A_{\\mathrm{non}}$ respectively.\n- For each case, compute and store the ratio $r = e_{\\mathrm{non}}/e_{\\mathrm{orth}}$ as a Python float.\n\nFinal Output Format:\nYour program should produce a single line of output containing the three ratios as a comma-separated list enclosed in square brackets (e.g., \"[r1,r2,r3]\"). No units are involved in this problem. Angles are not involved. Percentages are not involved; all ratios should be output as plain decimal numbers.", "solution": "The user wants to solve a numerical linear algebra problem.\nThe problem requires an empirical investigation into the numerical stability of computing the coefficients of the characteristic polynomial, $p_A(\\lambda) = \\det(\\lambda I - A)$, for a given $n \\times n$ matrix $A$. The core of the task is to compare the accuracy of results obtained after applying an orthogonal similarity transformation versus a non-orthogonal, ill-conditioned one. The invariance of the characteristic polynomial under any similarity transformation, $p_A(\\lambda) = p_{S^{-1}AS}(\\lambda)$, is true in exact arithmetic but often fails numerically due to floating-point errors. This experiment is designed to quantify this failure.\n\nThe chosen algorithm for computing the coefficients is the Faddeev–LeVerrier recurrence. Given an $n \\times n$ matrix, this method generates the coefficients $c_1, \\dots, c_n$ of the characteristic polynomial $p_A(\\lambda) = \\lambda^n + c_1 \\lambda^{n-1} + \\cdots + c_n$. The recurrence is defined by initializing $B_0 = I$ (the $n \\times n$ identity matrix) and then iterating for $k = 1, 2, \\dots, n$:\n$$c_k = -\\frac{1}{k}\\operatorname{tr}(A B_{k-1})$$\n$$B_k = A B_{k-1} + c_k I$$\nThis algorithm is known to be numerically unstable, partly because it involves repeated matrix multiplications which can amplify initial errors.\n\nTo conduct a controlled numerical experiment, we follow a three-pronged computational strategy for each test case:\n\n1.  **Exact Ground Truth Calculation**: To establish an error-free baseline, the coefficient vector $\\mathbf{c}_{\\mathrm{exact}} = (c_1, \\dots, c_n)$ is computed from the initial integer matrix $A$ using the Faddeev–LeVerrier algorithm implemented with exact rational arithmetic. This is achieved by representing all numbers and matrix entries as `fractions.Fraction` objects, ensuring that all divisions and other operations are performed without any rounding error. This vector $\\mathbf{c}_{\\mathrm{exact}}$ serves as the ground truth against which floating-point results are measured.\n\n2.  **Orthogonal Transformation Path**: An orthogonal matrix $Q$ (where $Q^\\top Q = I$) is generated, and a transformed matrix $A_{\\mathrm{orth}} = Q^\\top A Q$ is computed using standard double-precision floating-point arithmetic. Orthogonal transformations are perfectly-conditioned with respect to the spectral norm, with a condition number $\\kappa_2(Q) = 1$, and they are norm-preserving, $\\|Q^\\top A Q\\|_2 = \\|A\\|_2$. Consequently, the transformation from $A$ to $A_{\\mathrm{orth}}$ introduces minimal numerical perturbation. The Faddeev–LeVerrier algorithm is then applied to $A_{\\mathrm{orth}}$ in floating-point arithmetic to produce the coefficient vector $\\mathbf{c}_{\\mathrm{orth}}$. The error in this vector is expected to be dominated by the inherent instability of the algorithm itself, not the transformation.\n\n3.  **Non-Orthogonal Transformation Path**: A non-orthogonal, diagonal similarity matrix $S$ is constructed with entries that vary over several orders of magnitude. This makes $S$ ill-conditioned, with a large condition number $\\kappa_2(S) = \\|S\\|_2 \\|S^{-1}\\|_2$. The transformed matrix $A_{\\mathrm{non}} = S^{-1} A S$ is computed in floating-point arithmetic. The elements of $A_{\\mathrm{non}}$ are given by $(A_{\\mathrm{non}})_{ij} = (S^{-1}AS)_{ij} = A_{ij} S_{jj} / S_{ii}$. The large ratios between the diagonal elements of $S$ can drastically change the scale of the entries in $A_{\\mathrm{non}}$ compared to $A$, potentially leading to a loss of significant digits when smaller numbers are added to larger ones during matrix operations. The Faddeev–LeVerrier algorithm is applied to $A_{\\mathrm{non}}$ to find $\\mathbf{c}_{\\mathrm{non}}$. The resulting error is anticipated to be much larger than in the orthogonal case, amplified by the ill-conditioning of the similarity transformation.\n\nThe final step is to quantify and compare the errors. The relative errors for the orthogonal and non-orthogonal paths are computed using the infinity norm:\n$$e_{\\mathrm{orth}} = \\frac{\\|\\mathbf{c}_{\\mathrm{orth}} - \\mathbf{c}_{\\mathrm{exact}}\\|_\\infty}{\\|\\mathbf{c}_{\\mathrm{exact}}\\|_\\infty}, \\quad e_{\\mathrm{non}} = \\frac{\\|\\mathbf{c}_{\\mathrm{non}} - \\mathbf{c}_{\\mathrm{exact}}\\|_\\infty}{\\|\\mathbf{c}_{\\mathrm{exact}}\\|_\\infty}$$\nThe amplification ratio, $r = e_{\\mathrm{non}} / e_{\\mathrm{orth}}$, measures how much more numerical error was introduced by the ill-conditioned non-orthogonal transformation relative to the stable orthogonal one. A large ratio $r \\gg 1$ demonstrates the superior numerical stability of using orthogonal transformations in practice. For the special case where $S=I$, the \"non-orthogonal\" path results in $A_{\\mathrm{non}}=A$, and we expect $e_{\\mathrm{non}}$ and $e_{\\mathrm{orth}}$ to be of a similar magnitude, yielding $r \\approx 1$.", "answer": "```python\nimport numpy as np\nfrom fractions import Fraction\n\ndef faddeev_leverrier_exact(A_int, n):\n    \"\"\"\n    Computes characteristic polynomial coefficients using exact rational arithmetic.\n    All intermediate calculations are done using Python's Fraction type.\n    \"\"\"\n    A_frac = np.array([[Fraction(x) for x in row] for row in A_int], dtype=object)\n    I_frac = np.array([[Fraction(int(i==j)) for j in range(n)] for i in range(n)], dtype=object)\n    \n    B = I_frac.copy()\n    c_coeffs = []\n    \n    for k in range(1, n + 1):\n        # B_{k} = A*B_{k-1} + c_k*I\n        # c_k = -1/k * tr(A*B_{k-1})\n        AB = A_frac @ B\n        tr_AB = np.trace(AB)\n        c_k = -tr_AB / Fraction(k)\n        c_coeffs.append(c_k)\n        B = AB + c_k * I_frac\n        \n    return np.array(c_coeffs, dtype=object)\n\ndef faddeev_leverrier_float(A_float, n):\n    \"\"\"\n    Computes characteristic polynomial coefficients using standard floating-point arithmetic.\n    \"\"\"\n    B = np.identity(n)\n    c_coeffs = np.zeros(n)\n    \n    for k in range(1, n + 1):\n        AB = A_float @ B\n        c_k = -np.trace(AB) / k\n        c_coeffs[k-1] = c_k\n        B = AB + c_k * np.identity(n)\n        \n    return c_coeffs\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and compute error amplification ratios.\n    \"\"\"\n    test_cases = [\n        {'n': 6, 'seed_A': 2025, 'seed_Q': 123, 'S_exponents': [0, 1, 2, 3, 4, 5]},\n        {'n': 6, 'seed_A': 2025, 'seed_Q': 321, 'S_exponents': [0, 0, 0, 0, 0, 0]},\n        {'n': 6, 'seed_A': 2025, 'seed_Q': 999, 'S_exponents': [0, 2, 4, 6, 8, 10]},\n    ]\n\n    ratios = []\n\n    for case in test_cases:\n        n = case['n']\n        seed_A = case['seed_A']\n        seed_Q = case['seed_Q']\n        S_exponents = case['S_exponents']\n\n        # Step 1: Construct integer matrix A deterministically.\n        rng_A = np.random.default_rng(seed_A)\n        A_int = rng_A.integers(-2, 3, size=(n, n))\n\n        # Step 2: Compute exact coefficients c_exact using rational arithmetic.\n        c_exact = faddeev_leverrier_exact(A_int, n)\n        c_exact_float = np.array([float(c) for c in c_exact])\n        norm_c_exact = np.linalg.norm(c_exact_float, ord=np.inf)\n        \n        if norm_c_exact == 0:\n            # Handle the unlikely degenerate case where c_exact is a zero vector.\n            ratios.append(float('nan'))\n            continue\n\n        A_float = A_int.astype(float)\n\n        # Step 3: Form orthogonal matrix Q and A_orth.\n        rng_Q = np.random.default_rng(seed_Q)\n        Z = rng_Q.standard_normal(size=(n, n))\n        Q, _ = np.linalg.qr(Z)\n        A_orth = Q.T @ A_float @ Q\n\n        # Step 4: Form non-orthogonal matrix S and A_non.\n        S_diag = np.array([10.0**exp for exp in S_exponents])\n        # Efficiently compute A_non = S_inv @ A @ S for diagonal S.\n        # (A_non)_ij = A_ij * (S_j / S_i)\n        A_non = A_float * (S_diag / S_diag[:, np.newaxis])\n\n        # Step 5: Compute float coefficients for transformed matrices.\n        c_orth = faddeev_leverrier_float(A_orth, n)\n        c_non = faddeev_leverrier_float(A_non, n)\n        \n        # Step 6: Quantify relative coefficient errors.\n        err_vec_orth = c_orth - c_exact_float\n        e_orth = np.linalg.norm(err_vec_orth, ord=np.inf) / norm_c_exact\n        \n        err_vec_non = c_non - c_exact_float\n        e_non = np.linalg.norm(err_vec_non, ord=np.inf) / norm_c_exact\n\n        # Step 7: Report the amplification ratio r.\n        # If e_orth is zero, the orthogonal path was perfectly accurate.\n        if e_orth == 0:\n            r = float('inf') if e_non > 0 else 1.0\n        else:\n            r = e_non / e_orth\n        ratios.append(r)\n\n    print(f\"[{','.join(map(str, ratios))}]\")\n\nsolve()\n```", "id": "3536821"}, {"introduction": "Having established the numerical perils of the standard monomial basis $\\{1, x, x^2, \\dots\\}$, we now turn to a powerful solution: changing the basis. This practice guides you through representing the characteristic polynomial using a basis of orthogonal polynomials, specifically Chebyshev polynomials, which are naturally suited for functions on a finite interval. You will derive how this representation tames the explosive coefficient growth seen in the monomial basis and explore a stable method for converting between these representations, a cornerstone technique in modern numerical analysis [@problem_id:3536768].", "problem": "In numerical linear algebra, monomial coefficients of the characteristic polynomial $p_A(x) = \\det(x I - A)$ are elementary symmetric functions of the eigenvalues of $A$ and can grow rapidly in magnitude, which leads to numerical instability in finite precision arithmetic. Consider replacing the monomial basis $\\{1, x, x^2, \\dots, x^n\\}$ by an orthogonal polynomial basis scaled to an interval containing the spectrum. Let $\\{\\lambda_i\\}_{i=1}^n$ be the real eigenvalues of a real symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$ with spectrum contained in $[a,b]$, and let $\\phi(x) = \\dfrac{2 x - (a+b)}{b-a}$ map $[a,b]$ to $[-1,1]$. Define $q(t) = p_A(\\phi^{-1}(t))$ and expand $q$ in the Chebyshev polynomials of the first kind $\\{T_k\\}_{k=0}^n$ on $[-1,1]$, where $T_k(\\cos \\theta) = \\cos(k \\theta)$ for $\\theta \\in [0,\\pi]$.\n\nTasks:\n- Starting from the definitions of $p_A$, the affine map $\\phi$, and the orthogonality of $\\{T_k\\}$ with respect to the weight $w(t) = (1 - t^2)^{-1/2}$ on $[-1,1]$, derive the Chebyshev expansion $q(t) = \\sum_{k=0}^{n} c_k T_k(t)$ and show a bound on the Chebyshev coefficients in terms of $M = \\max_{t \\in [-1,1]} |q(t)|$. Your derivation must articulate how orthogonality and the boundedness $|T_k(t)| \\leq 1$ on $[-1,1]$ together mitigate coefficient growth relative to the monomial basis on a wide interval.\n- Explain, using the structure of $p_A(x) = \\prod_{i=1}^{n} (x - \\lambda_i)$, why monomial coefficients can grow combinatorially with $n$ and scale unfavorably with the interval width $b-a$, whereas the Chebyshev coefficients are controlled by $M$ independently of translation and scaling of the interval.\n- Propose a numerically stable transformation to recover the monomial coefficient vector from the Chebyshev coefficient vector $\\boldsymbol{c} = (c_0,\\dots,c_n)^{\\mathsf{T}}$ with minimal loss of accuracy. Your proposal must be based on first principles, using either the three-term recurrence to build monomial representations of $T_k(\\phi(x))$ with appropriate scaling, or an orthogonal factorization (e.g., a QR factorization) of the Chebyshev-to-monomial conversion, and justify backward stability in floating-point arithmetic.\n- For the concrete case $n=3$ with eigenvalues $\\{\\lambda_1,\\lambda_2,\\lambda_3\\} = \\{-0.9,\\, 0.05,\\, 0.8\\}$ and interval $[a,b] = [-1,1]$ (so that $\\phi$ is the identity), compute:\n  1. The monomial coefficient vector $\\boldsymbol{m} = (a_0,a_1,a_2,a_3)^{\\mathsf{T}}$ such that $p_A(x) = a_3 x^3 + a_2 x^2 + a_1 x + a_0$.\n  2. The Chebyshev coefficient vector $\\boldsymbol{c} = (c_0,c_1,c_2,c_3)^{\\mathsf{T}}$ such that $p_A(x) = \\sum_{k=0}^{3} c_k T_k(x)$.\n  3. The ratio $R = \\dfrac{\\|\\boldsymbol{m}\\|_2}{\\|\\boldsymbol{c}\\|_2}$.\n  \nRound your final numerical answer for $R$ to four significant figures.", "solution": "The problem statement is scientifically sound, well-posed, and internally consistent, allowing for a rigorous solution. We proceed by addressing each of the posed tasks in sequence.\n\nFirst, we derive the Chebyshev expansion for $q(t) = p_A(\\phi^{-1}(t))$ and the bound on its coefficients. The polynomial $q(t)$ is of degree $n$ and can be expanded in the basis of Chebyshev polynomials of the first kind, $\\{T_k(t)\\}_{k=0}^n$, as $q(t) = \\sum_{k=0}^{n} c_k T_k(t)$. The coefficients $c_k$ are found by exploiting the orthogonality of the Chebyshev polynomials on the interval $[-1,1]$ with respect to the weight function $w(t) = (1-t^2)^{-1/2}$.\n\nThe orthogonality relation is $\\int_{-1}^{1} T_j(t) T_k(t) w(t) dt = 0$ for $j \\neq k$. The normalization constants are:\n$$ \\int_{-1}^{1} T_k^2(t) w(t) dt = \\int_{-1}^{1} \\frac{T_k^2(t)}{\\sqrt{1-t^2}} dt = \\begin{cases} \\pi & k=0 \\\\ \\pi/2 & k>0 \\end{cases} $$\nTo find a specific coefficient $c_j$, we multiply the expansion by $T_j(t)w(t)$ and integrate over $[-1,1]$:\n$$ \\int_{-1}^{1} q(t) T_j(t) w(t) dt = \\sum_{k=0}^{n} c_k \\int_{-1}^{1} T_k(t) T_j(t) w(t) dt = c_j \\int_{-1}^{1} T_j^2(t) w(t) dt $$\nThis yields the formulas for the coefficients:\n$$ c_0 = \\frac{1}{\\pi} \\int_{-1}^{1} \\frac{q(t)}{\\sqrt{1-t^2}} dt $$\n$$ c_k = \\frac{2}{\\pi} \\int_{-1}^{1} \\frac{q(t) T_k(t)}{\\sqrt{1-t^2}} dt \\quad \\text{for } k > 0 $$\nTo bound these coefficients, we use the given maximum $M = \\max_{t \\in [-1,1]} |q(t)|$ and the property that $|T_k(t)| \\le 1$ for all $k$ on the interval $t \\in [-1,1]$.\nFor $c_0$:\n$$ |c_0| = \\left| \\frac{1}{\\pi} \\int_{-1}^{1} \\frac{q(t)}{\\sqrt{1-t^2}} dt \\right| \\le \\frac{1}{\\pi} \\int_{-1}^{1} \\frac{|q(t)|}{\\sqrt{1-t^2}} dt \\le \\frac{M}{\\pi} \\int_{-1}^{1} \\frac{1}{\\sqrt{1-t^2}} dt = \\frac{M}{\\pi} [\\arcsin(t)]_{-1}^{1} = \\frac{M}{\\pi} \\left(\\frac{\\pi}{2} - \\left(-\\frac{\\pi}{2}\\right)\\right) = M $$\nFor $c_k$ with $k > 0$:\n$$ |c_k| = \\left| \\frac{2}{\\pi} \\int_{-1}^{1} \\frac{q(t) T_k(t)}{\\sqrt{1-t^2}} dt \\right| \\le \\frac{2}{\\pi} \\int_{-1}^{1} \\frac{|q(t)| |T_k(t)|}{\\sqrt{1-t^2}} dt \\le \\frac{2M}{\\pi} \\int_{-1}^{1} \\frac{1}{\\sqrt{1-t^2}} dt = \\frac{2M}{\\pi} (\\pi) = 2M $$\nThus, the Chebyshev coefficients are bounded: $|c_0| \\le M$ and $|c_k| \\le 2M$ for $k>0$. This demonstrates that the coefficients cannot be arbitrarily large relative to the maximum value of the function on the interval. This mitigates coefficient growth because the basis functions $T_k(t)$ are themselves bounded by $1$. In contrast, monomial basis functions $x^k$ are unbounded and can become nearly linearly dependent on intervals far from $0$, leading to ill-conditioning where small changes in the function can cause large changes in the monomial coefficients. The orthogonal, bounded basis ensures that the coefficients are a stable representation of the polynomial.\n\nSecond, we explain the difference in coefficient growth. The characteristic polynomial is $p_A(x) = \\prod_{i=1}^{n} (x - \\lambda_i)$. Expanding this product gives the monomial representation $p_A(x) = \\sum_{k=0}^{n} a_k x^k$ (with $a_n=1$). The coefficients $a_k$ are given by Vieta's formulas as the elementary symmetric polynomials of the eigenvalues:\n$$ a_{n-k} = (-1)^k \\sum_{1 \\le i_1 < \\dots < i_k \\le n} \\lambda_{i_1} \\lambda_{i_2} \\cdots \\lambda_{i_k} $$\nThe sum for $a_{n-k}$ involves $\\binom{n}{k}$ terms. This number grows combinatorially with $n$, peaking at $k \\approx n/2$. If the eigenvalues $\\lambda_i$ are large in magnitude (i.e., the interval $[a,b]$ is far from $0$), the products $\\lambda_{i_1} \\cdots \\lambda_{i_k}$ can be enormous. For example, if all $\\lambda_i \\approx \\lambda_0$, then $|a_{n-k}| \\approx \\binom{n}{k} |\\lambda_0|^k$. This demonstrates unfavorable growth with both dimension $n$ and the location (translation) of the spectrum. The width $b-a$ also contributes to the potential magnitude of the eigenvalues.\nIn contrast, the Chebyshev coefficients $c_k$ are for the polynomial $q(t) = p_A(\\phi^{-1}(t))$ on $[-1,1]$. This transformation maps the eigenvalues $\\lambda_i \\in [a,b]$ to $\\mu_i = \\phi(\\lambda_i) \\in [-1,1]$. We have $q(t) = \\prod_{i=1}^n (\\phi^{-1}(t) - \\lambda_i) = \\left(\\frac{b-a}{2}\\right)^n \\prod_{i=1}^n (t - \\mu_i)$. The maximum value of this polynomial on $[-1,1]$ is $M = \\max_{t \\in [-1,1]} |q(t)|$. As shown before, the Chebyshev coefficients are bounded by $M$ and $2M$. Although $M$ itself depends on the interval width $b-a$, the crucial point is that the coefficients are bounded relative to the function's maximum on the normalized interval. The ill-conditioning from a poorly chosen basis (monomials on an arbitrary interval) is eliminated by mapping to a standard interval and using a well-behaved (orthogonal and bounded) basis. The numerical stability comes from the property of the basis itself, not from the coefficients being small in an absolute sense, but from their bounded size relative to the function's scale $M$.\n\nThird, we propose a numerically stable method to recover the monomial coefficient vector $\\boldsymbol{m}$ from the Chebyshev coefficient vector $\\boldsymbol{c}$. The polynomial is $p_A(x) = \\sum_{k=0}^{n} c_k T_k(\\phi(x))$. We want to find the coefficients $a_j$ in $p_A(x) = \\sum_{j=0}^{n} a_j x^j$. Let $\\phi(x) = \\alpha x + \\beta$ with $\\alpha = \\frac{2}{b-a}$ and $\\beta = -\\frac{a+b}{b-a}$. The basis polynomials are $P_k(x) = T_k(\\phi(x))$. We can generate the monomial representation of each $P_k(x)$ using the three-term recurrence relation for Chebyshev polynomials: $T_{k+1}(t) = 2t T_k(t) - T_{k-1}(t)$. Substituting $t = \\phi(x)$ gives a recurrence for the basis polynomials:\n$$ P_{k+1}(x) = 2(\\alpha x + \\beta) P_k(x) - P_{k-1}(x) $$\nThis relation can be applied to the vectors of monomial coefficients. Let $\\boldsymbol{p}_k$ be the coefficient vector of $P_k(x)$, such that $P_k(x) = \\sum_{j=0}^k (\\boldsymbol{p}_k)_j x^j$. Let $S$ be the upward-shift operator on coefficient vectors, corresponding to multiplication by $x$. The recurrence becomes:\n$$ \\boldsymbol{p}_{k+1} = 2\\alpha S \\boldsymbol{p}_k + 2\\beta \\boldsymbol{p}_k - \\boldsymbol{p}_{k-1} $$\nThe algorithm is as follows:\n1. Initialize $\\boldsymbol{p}_0$ as the coefficient vector for $P_0(x)=1$, so $\\boldsymbol{p}_0 = (1, 0, \\dots, 0)^{\\mathsf{T}}$.\n2. Initialize $\\boldsymbol{p}_1$ as the coefficient vector for $P_1(x)=\\phi(x)=\\alpha x + \\beta$, so $\\boldsymbol{p}_1 = (\\beta, \\alpha, 0, \\dots, 0)^{\\mathsf{T}}$.\n3. For $k=1, \\dots, n-1$, compute $\\boldsymbol{p}_{k+1}$ using the vector recurrence.\n4. The final monomial coefficient vector is the linear combination $\\boldsymbol{m} = \\sum_{k=0}^{n} c_k \\boldsymbol{p}_k$.\nThis method is numerically stable. It avoids direct calculation of the change-of-basis matrix, whose entries can involve large, alternating-sign binomial expressions that are prone to catastrophic cancellation. The three-term recurrence involves only simple arithmetic operations (scalar multiplication and vector addition), and error propagation is well-controlled. This constitutes a backward stable algorithm for computing the matrix-vector product $C\\boldsymbol{c}$, where $C$ is the (potentially ill-conditioned) conversion matrix with columns $\\boldsymbol{p}_k$.\n\nFourth, we perform the computations for the concrete case with $n=3$, eigenvalues $\\{-0.9, 0.05, 0.8\\}$, and interval $[a,b] = [-1,1]$.\nThe affine map is $\\phi(x) = \\frac{2x - (-1+1)}{1-(-1)} = x$. This simplifies $p_A(x)=q(x)$.\n\n1. Monomial coefficient vector $\\boldsymbol{m}$:\nThe characteristic polynomial is $p_A(x) = (x - (-0.9))(x - 0.05)(x - 0.8) = (x+0.9)(x-0.05)(x-0.8)$.\nExpanding this:\n$p_A(x) = (x^2 + 0.85x - 0.045)(x-0.8)$\n$p_A(x) = x^3 + 0.85x^2 - 0.045x - 0.8x^2 - 0.68x + 0.036$\n$p_A(x) = 1x^3 + 0.05x^2 - 0.725x + 0.036$\nFollowing the problem's convention $p_A(x) = a_3x^3 + a_2x^2 + a_1x + a_0$, the monomial coefficient vector is $\\boldsymbol{m} = (a_0, a_1, a_2, a_3)^{\\mathsf{T}} = (0.036, -0.725, 0.05, 1)^{\\mathsf{T}}$.\n\n2. Chebyshev coefficient vector $\\boldsymbol{c}$:\nWe express $p_A(x)$ in the Chebyshev basis $\\{T_0(x), T_1(x), T_2(x), T_3(x)\\}$. We use the relations:\n$x^3 = \\frac{1}{4}(T_3(x) + 3T_1(x))$\n$x^2 = \\frac{1}{2}(T_2(x) + T_0(x))$\n$x = T_1(x)$\n$1 = T_0(x)$\nSubstituting these into the monomial form of $p_A(x)$:\n$p_A(x) = 1 \\cdot \\left(\\frac{1}{4}T_3(x) + \\frac{3}{4}T_1(x)\\right) + 0.05 \\cdot \\left(\\frac{1}{2}T_2(x) + \\frac{1}{2}T_0(x)\\right) - 0.725 \\cdot T_1(x) + 0.036 \\cdot T_0(x)$\nGrouping terms by $T_k(x)$:\n$c_3 = \\frac{1}{4} = 0.25$\n$c_2 = \\frac{0.05}{2} = 0.025$\n$c_1 = \\frac{3}{4} - 0.725 = 0.75 - 0.725 = 0.025$\n$c_0 = \\frac{0.05}{2} + 0.036 = 0.025 + 0.036 = 0.061$\nThe Chebyshev coefficient vector is $\\boldsymbol{c} = (c_0, c_1, c_2, c_3)^{\\mathsf{T}} = (0.061, 0.025, 0.025, 0.25)^{\\mathsf{T}}$.\n\n3. Ratio $R = \\dfrac{\\|\\boldsymbol{m}\\|_2}{\\|\\boldsymbol{c}\\|_2}$:\nWe compute the Euclidean norms of the vectors.\n$\\|\\boldsymbol{m}\\|_2^2 = (0.036)^2 + (-0.725)^2 + (0.05)^2 + 1^2 = 0.001296 + 0.525625 + 0.0025 + 1 = 1.529421$\n$\\|\\boldsymbol{m}\\|_2 = \\sqrt{1.529421} \\approx 1.2366976$\n$\\|\\boldsymbol{c}\\|_2^2 = (0.061)^2 + (0.025)^2 + (0.025)^2 + (0.25)^2 = 0.003721 + 0.000625 + 0.000625 + 0.0625 = 0.067471$\n$\\|\\boldsymbol{c}\\|_2 = \\sqrt{0.067471} \\approx 0.2597518$\nThe ratio is $R = \\dfrac{1.2366976}{0.2597518} \\approx 4.76105$.\nRounding to four significant figures, we get $R = 4.761$.", "answer": "$$\n\\boxed{4.761}\n$$", "id": "3536768"}]}