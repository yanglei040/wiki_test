{"hands_on_practices": [{"introduction": "Orthogonal similarity transformations preserve eigenvalues, but their effect on matrix structure can be dramatic. This exercise [@problem_id:3572243] serves as a key motivation for the structured nature of the Householder tridiagonalization algorithm by demonstrating the phenomenon of \"fill-in.\" By computing a specific off-diagonal element created by a naively applied reflector, you will see why preserving sparsity is critical and why the standard algorithm is carefully designed to operate on trailing submatrices.", "problem": "Let $n \\geq 4$ and consider the symmetric tridiagonal matrix $A \\in \\mathbb{R}^{n \\times n}$ defined by $A_{i,i} = i$ for $1 \\leq i \\leq n$, $A_{i,i+1} = A_{i+1,i} = 1$ for $1 \\leq i \\leq n-1$, and $A_{i,j} = 0$ otherwise. Let $v \\in \\mathbb{R}^{n}$ be the unit vector with identical entries, $v = \\frac{1}{\\sqrt{n}} (1,1,\\dots,1)^{\\mathsf{T}}$, and let $H = I - 2 v v^{\\mathsf{T}}$ be the Householder reflector. Consider the naive two-sided similarity transformation $B = H A H$.\n\nStarting from the definitions of a Householder reflector and tridiagonal matrices, derive the exact value of the entry $B_{1,n}$, which lies far outside the original tridiagonal band of $A$. Provide your final answer as a closed-form expression in $n$. No rounding is required.", "solution": "The problem is first validated against the specified criteria.\n\n### Step 1: Extract Givens\n- $n \\in \\mathbb{Z}$ with $n \\geq 4$.\n- $A \\in \\mathbb{R}^{n \\times n}$ is a symmetric tridiagonal matrix.\n- The entries of $A$ are defined as:\n  - $A_{i,i} = i$ for $1 \\leq i \\leq n$.\n  - $A_{i,i+1} = A_{i+1,i} = 1$ for $1 \\leq i \\leq n-1$.\n  - $A_{i,j} = 0$ for $|i-j| > 1$.\n- $v \\in \\mathbb{R}^{n}$ is the vector $v = \\frac{1}{\\sqrt{n}} (1,1,\\dots,1)^{\\mathsf{T}}$.\n- $H$ is the Householder reflector defined by $H = I - 2 v v^{\\mathsf{T}}$.\n- $B$ is the matrix resulting from the similarity transformation $B = H A H$.\n- The objective is to find the value of the entry $B_{1,n}$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is a well-defined exercise in linear algebra.\n- **Scientifically Grounded**: The definitions of a symmetric tridiagonal matrix, a Householder reflector, and a similarity transformation are all standard and mathematically rigorous. The vector $v$ as defined is a unit vector, since $\\|v\\|^2_2 = \\sum_{i=1}^n (\\frac{1}{\\sqrt{n}})^2 = \\sum_{i=1}^n \\frac{1}{n} = 1$. The problem is mathematically sound.\n- **Well-Posed**: The matrix $B$ is uniquely determined by the definitions of $A$ and $H$. Therefore, its entries, including $B_{1,n}$, are uniquely determined. The problem asks for a specific, calculable quantity.\n- **Objective**: The problem is stated using precise mathematical language, free from any subjectivity or ambiguity.\nThe problem is found to be complete, consistent, and formalizable.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\n### Derivation of $B_{1,n}$\nThe matrix $B$ is given by the similarity transformation $B = HAH$. Substituting the definition of the Householder matrix $H = I - 2vv^{\\mathsf{T}}$, we get:\n$$B = (I - 2vv^{\\mathsf{T}})A(I - 2vv^{\\mathsf{T}})$$\nExpanding this product yields:\n$$B = IAI - 2IAvv^{\\mathsf{T}} - 2vv^{\\mathsf{T}}AI + (-2vv^{\\mathsf{T}})(-2Avv^{\\mathsf{T}})$$\n$$B = A - 2Avv^{\\mathsf{T}} - 2vv^{\\mathsf{T}}A + 4(vv^{\\mathsf{T}})A(vv^{\\mathsf{T}})$$\nSince $v^{\\mathsf{T}}A v$ is a scalar, we can write $(vv^{\\mathsf{T}})A(vv^{\\mathsf{T}}) = v(v^{\\mathsf{T}}Av)v^{\\mathsf{T}} = (v^{\\mathsf{T}}Av)vv^{\\mathsf{T}}$.\n$$B = A - 2Avv^{\\mathsf{T}} - 2vv^{\\mathsf{T}}A + 4(v^{\\mathsf{T}}Av)vv^{\\mathsf{T}}$$\nWe want to find the entry $B_{1,n}$, which can be expressed using the standard basis vectors $e_1 = (1,0,\\dots,0)^{\\mathsf{T}}$ and $e_n = (0,\\dots,0,1)^{\\mathsf{T}}$ as $B_{1,n} = e_1^{\\mathsf{T}} B e_n$.\n$$B_{1,n} = e_1^{\\mathsf{T}} A e_n - 2e_1^{\\mathsf{T}} A v v^{\\mathsf{T}} e_n - 2e_1^{\\mathsf{T}} v v^{\\mathsf{T}} A e_n + 4(v^{\\mathsf{T}}Av) e_1^{\\mathsf{T}} v v^{\\mathsf{T}} e_n$$\nUsing the associativity of matrix multiplication, we can group the scalar terms:\n$$B_{1,n} = A_{1,n} - 2(e_1^{\\mathsf{T}}Av)(v^{\\mathsf{T}}e_n) - 2(e_1^{\\mathsf{T}}v)(v^{\\mathsf{T}}Ae_n) + 4(e_1^{\\mathsf{T}}v)(v^{\\mathsf{T}}Av)(v^{\\mathsf{T}}e_n)$$\nWe now calculate each component of this expression.\n\n1.  $A_{1,n}$: Since $A$ is a tridiagonal matrix and the problem states $n \\geq 4$, we have $|1-n| = n-1 \\geq 3$. The entry $(1,n)$ is outside the tridiagonal band, so $A_{1,n} = 0$.\n\n2.  $e_1^{\\mathsf{T}}v$ and $v^{\\mathsf{T}}e_n$: The vector $v$ is given by $v_i = \\frac{1}{\\sqrt{n}}$ for all $i \\in \\{1, \\dots, n\\}$.\n    $$e_1^{\\mathsf{T}}v = v_1 = \\frac{1}{\\sqrt{n}}$$\n    $$v^{\\mathsf{T}}e_n = v_n = \\frac{1}{\\sqrt{n}}$$\n\n3.  $e_1^{\\mathsf{T}}Av$: This term is the product of the first row of $A$ and the vector $v$. The first row of $A$ is given by $(A_{1,1}, A_{1,2}, 0, ..., 0) = (1, 1, 0, ..., 0)$.\n    $$e_1^{\\mathsf{T}}Av = (e_1^{\\mathsf{T}}A)v = (1, 1, 0, \\dots, 0) \\begin{pmatrix} 1/\\sqrt{n} \\\\ 1/\\sqrt{n} \\\\ \\vdots \\\\ 1/\\sqrt{n} \\end{pmatrix} = 1 \\cdot \\frac{1}{\\sqrt{n}} + 1 \\cdot \\frac{1}{\\sqrt{n}} = \\frac{2}{\\sqrt{n}}$$\n\n4.  $v^{\\mathsf{T}}Ae_n$: This term is the product of $v^{\\mathsf{T}}$ and the $n$-th column of $A$. The $n$-th column of $A$ is given by $(0, ..., 0, A_{n-1,n}, A_{n,n})^{\\mathsf{T}} = (0, ..., 0, 1, n)^{\\mathsf{T}}$.\n    $$v^{\\mathsf{T}}Ae_n = v^{\\mathsf{T}}(Ae_n) = (\\frac{1}{\\sqrt{n}}, \\dots, \\frac{1}{\\sqrt{n}}) \\begin{pmatrix} 0 \\\\ \\vdots \\\\ 0 \\\\ 1 \\\\ n \\end{pmatrix} = \\frac{1}{\\sqrt{n}} \\cdot 1 + \\frac{1}{\\sqrt{n}} \\cdot n = \\frac{n+1}{\\sqrt{n}}$$\n\n5.  $v^{\\mathsf{T}}Av$: This is a Rayleigh quotient. Let $\\mathbf{1}$ be the vector of all ones, so $v = \\frac{1}{\\sqrt{n}}\\mathbf{1}$. Then $v^{\\mathsf{T}}Av = (\\frac{1}{\\sqrt{n}}\\mathbf{1}^{\\mathsf{T}})A(\\frac{1}{\\sqrt{n}}\\mathbf{1}) = \\frac{1}{n} \\mathbf{1}^{\\mathsf{T}}A\\mathbf{1}$. The term $\\mathbf{1}^{\\mathsf{T}}A\\mathbf{1}$ is the sum of all entries in the matrix $A$. We can calculate this sum by summing the diagonal and off-diagonal entries.\n    Sum of diagonal entries: $\\sum_{i=1}^n A_{i,i} = \\sum_{i=1}^n i = \\frac{n(n+1)}{2}$.\n    Sum of off-diagonal entries: There are $n-1$ entries equal to $1$ on the superdiagonal and $n-1$ entries equal to $1$ on the subdiagonal. The total is $2(n-1)$.\n    Sum of all entries: $\\mathbf{1}^{\\mathsf{T}}A\\mathbf{1} = \\frac{n(n+1)}{2} + 2(n-1) = \\frac{n^2+n+4n-4}{2} = \\frac{n^2+5n-4}{2}$.\n    Therefore,\n    $$v^{\\mathsf{T}}Av = \\frac{1}{n} \\left( \\frac{n^2+5n-4}{2} \\right) = \\frac{n^2+5n-4}{2n}$$\n\nNow we substitute these values back into the expression for $B_{1,n}$:\n$$B_{1,n} = 0 - 2\\left(\\frac{2}{\\sqrt{n}}\\right)\\left(\\frac{1}{\\sqrt{n}}\\right) - 2\\left(\\frac{1}{\\sqrt{n}}\\right)\\left(\\frac{n+1}{\\sqrt{n}}\\right) + 4\\left(\\frac{1}{\\sqrt{n}}\\right)\\left(\\frac{n^2+5n-4}{2n}\\right)\\left(\\frac{1}{\\sqrt{n}}\\right)$$\n$$B_{1,n} = -2\\left(\\frac{2}{n}\\right) - 2\\left(\\frac{n+1}{n}\\right) + 4\\left(\\frac{1}{n}\\right)\\left(\\frac{n^2+5n-4}{2n}\\right)$$\n$$B_{1,n} = -\\frac{4}{n} - \\frac{2n+2}{n} + \\frac{2(n^2+5n-4)}{n^2}$$\nCombine the first two terms:\n$$B_{1,n} = \\frac{-4 - 2n - 2}{n} + \\frac{2n^2+10n-8}{n^2} = \\frac{-2n-6}{n} + \\frac{2n^2+10n-8}{n^2}$$\nBring to a common denominator $n^2$:\n$$B_{1,n} = \\frac{n(-2n-6)}{n^2} + \\frac{2n^2+10n-8}{n^2}$$\n$$B_{1,n} = \\frac{-2n^2-6n + 2n^2+10n-8}{n^2}$$\n$$B_{1,n} = \\frac{4n-8}{n^2}$$\n\nThis expression gives the exact value of the entry $B_{1,n}$ as a function of $n$. This effect, where a similarity transformation using a dense reflector matrix creates non-zero entries far from the diagonal of a sparse matrix, is known as fill-in.", "answer": "$$\\boxed{\\frac{4n-8}{n^2}}$$", "id": "3572243"}, {"introduction": "Having established the need for a structured algorithm, we now turn to analyzing its computational cost. This practice [@problem_id:3572232] involves a detailed floating-point operation (FLOP) count of the standard Householder tridiagonalization process, from forming the reflectors to applying the efficient symmetric rank-2 update. This analysis provides a quantitative understanding of the algorithm's leading order complexity and is a fundamental skill for evaluating and comparing numerical methods.", "problem": "Consider a real symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$ and its reduction to tridiagonal form using Householder reflections. At each step $k$ with $1 \\leq k \\leq n-2$, the algorithm acts on the trailing submatrix $A^{(k)} \\in \\mathbb{R}^{m \\times m}$ where $m = n - k$. Let $a = A_{k+1:n, k} \\in \\mathbb{R}^{m}$ denote the subvector to be annihilated below the first component. A Householder reflector $H = I - \\tau w w^{\\mathsf{T}}$ is formed with $w \\in \\mathbb{R}^{m}$ and scalar $\\tau \\in \\mathbb{R}$ so that $H$ zeroes $a$ except possibly its first element. The symmetric update applies $H$ to $A^{(k)}$ on both sides via the rank-$2$ formula:\n$$\ny = A^{(k)} w, \\quad \\alpha = \\frac{\\tau}{2} w^{\\mathsf{T}} y, \\quad y := y - \\alpha w, \\quad A^{(k)} := A^{(k)} - w y^{\\mathsf{T}} - y w^{\\mathsf{T}},\n$$\nand only the lower triangular part of $A^{(k)}$ is updated.\n\nUse the following floating-point operation (FLOP) model: each floating-point addition or multiplication counts as $1$ FLOP; divisions, square roots, comparisons, and memory operations are not counted. When computing inner products or matrix-vector products, count all multiplications and additions explicitly. In forming the Householder vector $w$, compute the Euclidean norm $\\|a\\|_{2}$ and the inner product $w^{\\mathsf{T}} w$, and take the first-component modification $w_1 := w_1 + \\operatorname{sign}(w_1)\\|a\\|_{2}$ to cost $1$ FLOP in this model.\n\nDerive from first principles:\n- the FLOP count to form $w$ and $\\tau$ in step $k$ as a function of $m$,\n- the FLOP count to apply $H$ to $A^{(k)}$ using the symmetric rank-$2$ update above, counting only the lower triangular update, and\n- the total per-step cost $C_k$ in terms of $m$.\n\nThen sum $C_k$ over $k$ to obtain a single simplified closed-form analytic expression in $n$ for the total number of FLOPs required to reduce $A$ to tridiagonal form under this model. Provide the final expression exactly, without rounding.", "solution": "The user-provided problem is first validated against the specified criteria.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- Matrix: Real symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$.\n- Algorithm: Reduction to tridiagonal form using Householder reflections.\n- Steps: $k$ from $1$ to $n-2$.\n- Submatrix: At step $k$, the algorithm acts on $A^{(k)} \\in \\mathbb{R}^{m \\times m}$ where $m = n - k$.\n- Target vector: $a = A_{k+1:n, k} \\in \\mathbb{R}^{m}$.\n- Householder reflector: $H = I - \\tau w w^{\\mathsf{T}}$, with $w \\in \\mathbb{R}^{m}$ and $\\tau \\in \\mathbb{R}$.\n- Symmetric update formula:\n  1. $y = A^{(k)} w$\n  2. $\\alpha = \\frac{\\tau}{2} w^{\\mathsf{T}} y$\n  3. $y := y - \\alpha w$\n  4. $A^{(k)} := A^{(k)} - w y^{\\mathsf{T}} - y w^{\\mathsf{T}}$\n- Update constraint: Only the lower triangular part of $A^{(k)}$ is updated.\n- FLOP model:\n  - Addition or multiplication: $1$ FLOP.\n  - Divisions, square roots, comparisons, memory operations: $0$ FLOPs.\n  - Inner products/matrix-vector products: Count all adds and muls explicitly.\n- Householder vector formation:\n  - Compute Euclidean norm $\\|a\\|_{2}$.\n  - Compute inner product $w^{\\mathsf{T}} w$.\n  - The modification $w_1 := w_1 + \\operatorname{sign}(w_1)\\|a\\|_{2}$ costs $1$ FLOP.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem describes a standard and correct algorithm for the tridiagonalization of a symmetric matrix, a fundamental topic in numerical linear algebra. The rank-2 update formula provided is a known efficient implementation.\n- **Well-Posed:** The problem provides a clear set of instructions and a specific computational cost model, leading to a uniquely defined analytical solution for the total FLOP count.\n- **Objective:** The problem is stated in precise mathematical and algorithmic terms, free of subjectivity.\n- **Completeness and Consistency:** The problem is self-contained. All necessary formulas, variables, and cost assignments are explicitly provided. There are no contradictions.\n\n**Step 3: Verdict and Action**\nThe problem is valid as it is scientifically sound, well-posed, objective, and self-contained. I will proceed with the derivation of the solution.\n\n### Detailed Solution\n\nThe total number of floating-point operations (FLOPs) is determined by summing the costs of each step $k$ of the reduction, from $k=1$ to $k=n-2$. At each step $k$, the size of the subproblem is determined by $m=n-k$. The range of $m$ is from $n-1$ (for $k=1$) down to $2$ (for $k=n-2$). We will derive the cost for a single step as a function of $m$ and then sum over the appropriate range of $m$.\n\n**FLOP count to form $w$ and $\\tau$ in step $k$**\n\nAt step $k$, we operate on a vector $a \\in \\mathbb{R}^{m}$. The formation of the Householder vector $w$ and the scalar $\\tau$ proceeds as follows, according to the problem statement:\n1.  **Compute $\\|a\\|_{2}$**: This requires computing the sum of squares, $\\sum_{i=1}^{m} a_i^2$. This involves $m$ multiplications (for the squares $a_i^2$) and $m-1$ additions. The square root operation is specified to cost $0$ FLOPs.\n    Cost for $\\|a\\|_2$: $m + (m-1) = 2m-1$ FLOPs.\n2.  **Form $w$**: The vector $w$ is formed from $a$ by modifying its first component. Assuming $w$ is initialized with the values of $a$, the specific update is $w_1 := a_1 + \\operatorname{sign}(a_1)\\|a\\|_{2}$. The problem explicitly states this costs $1$ FLOP.\n    Cost to form $w$: $1$ FLOP.\n3.  **Compute $w^{\\mathsf{T}}w$**: The problem instructs to \"compute the inner product $w^{\\mathsf{T}}w$\". This involves calculating $\\sum_{i=1}^{m} w_i^2$, which requires $m$ multiplications and $m-1$ additions.\n    Cost for $w^{\\mathsf{T}}w$: $m + (m-1) = 2m-1$ FLOPs.\n4.  **Compute $\\tau$**: The scalar $\\tau$ is given by $\\tau = 2 / (w^{\\mathsf{T}}w)$. The division operation is specified to be free.\n    Cost for $\\tau$: $0$ FLOPs.\n\nThe total FLOP count to form $w$ and $\\tau$ is the sum of these costs:\nCost$_{w,\\tau}(m) = (2m-1) + 1 + (2m-1) = 4m-2$ FLOPs.\n\n**FLOP count to apply the update to $A^{(k)}$**\n\nThe symmetric update $A^{(k)} \\leftarrow H A^{(k)} H$ is performed via the provided rank-$2$ update formula:\n1.  **$y = A^{(k)} w$**: This is a matrix-vector product where $A^{(k)}$ is a symmetric $m \\times m$ matrix. A standard efficient algorithm for a symmetric matrix-vector product, which only accesses the lower (or upper) triangular elements, requires approximately $m^2$ multiplications and $m^2$ additions. More precisely, it costs $2m^2$ FLOPs.\n    Cost for $y$: $2m^2$ FLOPs.\n2.  **$\\alpha = \\frac{\\tau}{2} w^{\\mathsf{T}} y$**: This involves an inner product and a scalar multiplication.\n    - The inner product $w^{\\mathsf{T}}y = \\sum_{i=1}^m w_i y_i$ costs $m$ multiplications and $m-1$ additions, for a total of $2m-1$ FLOPs.\n    - The multiplication of the resulting scalar by $\\frac{\\tau}{2}$ costs $1$ FLOP.\n    Cost for $\\alpha$: $(2m-1) + 1 = 2m$ FLOPs.\n3.  **$y := y - \\alpha w$**: This is a scaled vector addition (an AXPY operation). It involves one scalar-vector multiplication ($\\alpha w$) and one vector subtraction.\n    - $\\alpha w$ costs $m$ multiplications.\n    - The subtraction costs $m$ additions.\n    Cost for updating $y$: $m+m=2m$ FLOPs.\n    Let's call the updated vector $y_{new}$ to distinguish it from the original $y=A^{(k)}w$.\n4.  **$A^{(k)} := A^{(k)} - w y_{new}^{\\mathsf{T}} - y_{new} w^{\\mathsf{T}}$**: This is a symmetric rank-2 update. We only update the lower triangular part of $A^{(k)}$, which has $m(m+1)/2$ elements.\n    - For each of the $m$ diagonal elements, the update is $A_{ii} := A_{ii} - 2 w_i (y_{new})_i$. This costs $1$ multiplication for $w_i (y_{new})_i$, $1$ for multiplying by $2$, and $1$ subtraction, for a total of $3$ FLOPs. The total for all diagonal elements is $3m$ FLOPs.\n    - For each of the $m(m-1)/2$ lower off-diagonal elements, the update is $A_{ij} := A_{ij} - w_i (y_{new})_j - w_j (y_{new})_i$. This costs $2$ multiplications and $2$ subtractions, for a total of $4$ FLOPs. The total for all off-diagonal elements is $4 \\times \\frac{m(m-1)}{2} = 2m(m-1) = 2m^2 - 2m$ FLOPs.\n    Cost for updating $A^{(k)}$: $3m + (2m^2 - 2m) = 2m^2 + m$ FLOPs.\n\nThe total FLOP count to apply the update is the sum of these costs:\nCost$_{apply}(m) = 2m^2 + 2m + 2m + (2m^2+m) = 4m^2+5m$ FLOPs.\n\n**Total per-step cost $C_k$ and total FLOP count**\n\nThe total cost for a single step $k$, $C_k$, is the sum of the cost to form the reflector and the cost to apply it:\n$C_k(m) = \\text{Cost}_{w,\\tau}(m) + \\text{Cost}_{apply}(m) = (4m-2) + (4m^2+5m) = 4m^2 + 9m - 2$.\n\nTo find the total number of FLOPs, we must sum $C_k$ over all steps $k=1, \\dots, n-2$. This is equivalent to summing over $m=n-k$ from $m=n-1$ down to $m=2$.\nTotal FLOPs = $\\sum_{k=1}^{n-2} C_k = \\sum_{m=2}^{n-1} (4m^2 + 9m - 2)$.\n\nWe can evaluate this sum using the standard formulas for sums of powers: $\\sum_{j=1}^{N} j = \\frac{N(N+1)}{2}$ and $\\sum_{j=1}^{N} j^2 = \\frac{N(N+1)(2N+1)}{6}$.\nTotal FLOPs = $4\\sum_{m=2}^{n-1} m^2 + 9\\sum_{m=2}^{n-1} m - 2\\sum_{m=2}^{n-1} 1$.\n\n1.  $\\sum_{m=2}^{n-1} m^2 = \\left(\\sum_{m=1}^{n-1} m^2\\right) - 1^2 = \\frac{(n-1)n(2n-1)}{6} - 1$.\n2.  $\\sum_{m=2}^{n-1} m = \\left(\\sum_{m=1}^{n-1} m\\right) - 1 = \\frac{(n-1)n}{2} - 1$.\n3.  $\\sum_{m=2}^{n-1} 1 = (n-1) - 2 + 1 = n-2$.\n\nSubstituting these into the total sum:\nTotal FLOPs = $4\\left(\\frac{n(n-1)(2n-1)}{6} - 1\\right) + 9\\left(\\frac{n(n-1)}{2} - 1\\right) - 2(n-2)$\n$= \\frac{2}{3}n(n-1)(2n-1) - 4 + \\frac{9}{2}n(n-1) - 9 - 2(n-2)$\n$= \\frac{2}{3}(2n^3 - 3n^2 + n) - 4 + \\frac{9}{2}(n^2 - n) - 9 - 2n + 4$\n$= \\frac{4}{3}n^3 - 2n^2 + \\frac{2}{3}n + \\frac{9}{2}n^2 - \\frac{9}{2}n - 2n - 9$\n$= \\frac{4}{3}n^3 + \\left(-2 + \\frac{9}{2}\\right)n^2 + \\left(\\frac{2}{3} - \\frac{9}{2} - 2\\right)n - 9$\n$= \\frac{4}{3}n^3 + \\left(\\frac{-4+9}{2}\\right)n^2 + \\left(\\frac{4-27-12}{6}\\right)n - 9$\n$= \\frac{4}{3}n^3 + \\frac{5}{2}n^2 - \\frac{35}{6}n - 9$.\n\nThis is the final simplified closed-form analytic expression for the total number of FLOPs.", "answer": "$$\\boxed{\\frac{4}{3}n^3 + \\frac{5}{2}n^2 - \\frac{35}{6}n - 9}$$", "id": "3572232"}, {"introduction": "The ultimate test of understanding a numerical algorithm is to implement it and confront the realities of floating-point arithmetic. This capstone project [@problem_id:3572312] challenges you to build the Householder tridiagonalization algorithm from first principles and verify its numerical integrity. By constructing matrices with known spectra and measuring the resulting errors, you will gain invaluable practical experience with the concepts of backward stability and the verification of numerical software.", "problem": "Consider a real symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$. A fundamental fact from linear algebra is that orthogonal similarity transformations preserve the spectrum: if $Q \\in \\mathbb{R}^{n \\times n}$ is orthogonal, then $Q^\\top A Q$ has the same multiset of eigenvalues as $A$. A well-tested algorithm for reducing a symmetric matrix to tridiagonal form is the Householder transformation method, which constructs a sequence of symmetric orthogonal reflectors that annihilate entries below the first subdiagonal, resulting in a tridiagonal matrix $T$ and an accumulated orthogonal matrix $Q$ such that $Q^\\top A Q = T$. The Householder reflector acting on a vector $x \\in \\mathbb{R}^m$ uses $v = \\dfrac{x - \\alpha e_1}{\\lVert x - \\alpha e_1 \\rVert_2}$ with $\\alpha = -\\operatorname{sign}(x_1)\\lVert x \\rVert_2$ and $H = I_m - 2 v v^\\top$ to map $x$ to $\\alpha e_1$.\n\nIn floating-point arithmetic, computed quantities are subject to rounding. The observed eigenvalues of $T$ and $A$ may exhibit tiny discrepancies because the computed $Q$ and $T$ correspond to a nearby matrix $A + E$, where $E$ is a small perturbation arising from rounding. Your task is to implement, from first principles, the Householder tridiagonalization for symmetric matrices, construct test matrices with known eigenstructure, and quantify the discrepancies in the spectra due to rounding.\n\nImplement a program that:\n- Constructs, for each test case, a deterministic orthogonal matrix $V \\in \\mathbb{R}^{n \\times n}$ and a diagonal matrix $\\Lambda = \\operatorname{diag}(\\lambda_1,\\dots,\\lambda_n)$ with prescribed eigenvalues, then forms the symmetric matrix $A = V \\Lambda V^\\top$. The eigenstructure is known by construction: $A$ has eigenvalues $\\{\\lambda_i\\}_{i=1}^n$ and eigenvectors given by the columns of $V$.\n- Reduces $A$ to tridiagonal form using Householder reflectors: compute $Q$ and $T$ such that $Q^\\top A Q = T$, where $T$ is real symmetric tridiagonal and $Q$ is orthogonal.\n- Computes the following quantitative outputs for each test case:\n  1. $d_{AT} = \\max_i \\left| \\mu_i(T) - \\mu_i(A) \\right|$ where $\\{\\mu_i(\\cdot)\\}$ are the eigenvalues of the indicated matrix, sorted in nondecreasing order. This quantifies the difference between the spectra of $T$ and $A$ due to rounding.\n  2. $d_{A,\\Lambda} = \\max_i \\left| \\mu_i(A) - \\lambda_i \\right|$ where both sequences are sorted in nondecreasing order. This quantifies the difference between the numerically computed eigenvalues of $A$ and the known eigenvalues used in construction.\n  3. $e_{\\text{orth}} = \\lVert Q^\\top Q - I_n \\rVert_F$, the Frobenius norm of the departure from orthogonality.\n  4. $e_{\\text{sim}} = \\lVert Q^\\top A Q - T \\rVert_F$, the Frobenius norm measuring the similarity transformation residual.\nAll norms are the standard Euclidean and Frobenius norms, respectively.\n\nUse double-precision floating point arithmetic everywhere. Angles are not involved, so no angle unit specification is needed. No physical units are involved. Express all numeric outputs as standard real numbers.\n\nTest suite to cover general and edge scenarios:\n- Case $1$ (general well-conditioned): $n = 5$, eigenvalues $\\lambda = [1, 2, 3, 4, 5]$.\n- Case $2$ (nearly repeated eigenvalues): $n = 8$, eigenvalues $\\lambda = [1, 1 + 10^{-12}, 2, 2 + 10^{-12}, 3, 3 + 10^{-12}, 4, 4 + 10^{-12}]$.\n- Case $3$ (large dynamic range): $n = 10$, eigenvalues $\\lambda = [10^{-8}, 10^{-6}, 10^{-4}, 10^{-2}, 1, 10, 10^{3}, 10^{5}, 10^{7}, 10^{9}]$.\n- Case $4$ (small dimension boundary): $n = 2$, eigenvalues $\\lambda = [1, 10^{-15}]$.\n- Case $5$ (trivial boundary): $n = 1$, eigenvalues $\\lambda = [42]$.\n\nDeterministic orthogonal basis construction requirement: for each $n$, construct $V$ by taking the matrix $M \\in \\mathbb{R}^{n \\times n}$ with entries $M_{ij} = \\sin((i+1)(j+1)) + \\cos((i+1) - 0.3(j+1)) + 0.5 \\cdot \\delta_{ij}$, performing a full column orthogonalization (for example, via the standard QR factorization) to obtain $M = QR$ with $Q$ orthogonal, and then flipping the sign of the first column of $Q$ if $\\det(Q) < 0$ to ensure a right-handed orientation. Use this $Q$ as $V$.\n\nFinal output format specification:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test caseâ€™s result, and each result is itself a list of four real numbers in the order $[d_{AT}, d_{A,\\Lambda}, e_{\\text{orth}}, e_{\\text{sim}}]$. For example, the output should look like $[[x_1, y_1, z_1, w_1],[x_2, y_2, z_2, w_2],\\dots]$ with no additional text.\n\nThe program must be self-contained, require no user input, and follow the prescribed runtime environment.", "solution": "The user-provided problem is a well-defined task in numerical linear algebra. It requires the implementation of the Householder tridiagonalization algorithm for a real symmetric matrix, and the subsequent quantitative analysis of numerical errors inherent in floating-point computations. The problem is scientifically sound, self-contained, and objective. It specifies the algorithm, the construction of test matrices, the exact metrics to be computed, and the format of the output. Therefore, the problem is valid, and I will proceed with a full solution.\n\n### Principle and Methodology\n\nThe core of the problem is to reduce a real symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$ to a similar tridiagonal matrix $T$ using an orthogonal transformation $Q$, such that $T = Q^\\top A Q$. The eigenvalues of $T$ are, in theory, identical to the eigenvalues of $A$. However, in practice, floating-point arithmetic introduces small errors. This exercise aims to implement the reduction process and quantify these errors.\n\nThe methodology is comprised of three main stages:\n1.  Construction of a test matrix $A$ with a known eigenstructure.\n2.  Application of the Householder tridiagonalization algorithm to obtain $T$ and $Q$.\n3.  Computation of error metrics to assess the accuracy and stability of the implementation.\n\n#### 1. Test Matrix Construction\n\nTo create a verifiable test case, we construct a symmetric matrix $A$ with a known set of eigenvalues, $\\{\\lambda_i\\}_{i=1}^n$. This is achieved by specifying a diagonal matrix $\\Lambda = \\operatorname{diag}(\\lambda_1, \\dots, \\lambda_n)$ and a deterministic orthogonal matrix $V \\in \\mathbb{R}^{n \\times n}$. The matrix $A$ is then formed by the similarity transformation $A = V \\Lambda V^\\top$. By the properties of similarity transformations, the eigenvalues of $A$ are precisely the diagonal entries of $\\Lambda$, and the columns of $V$ are the corresponding eigenvectors.\n\nThe orthogonal matrix $V$ is constructed deterministically as follows:\n- For a given dimension $n$, a matrix $M \\in \\mathbb{R}^{n \\times n}$ is generated with entries $M_{ij} = \\sin((i+1)(j+1)) + \\cos((i+1) - 0.3(j+1)) + 0.5 \\delta_{ij}$, where indices $i, j$ range from $0$ to $n-1$, and $\\delta_{ij}$ is the Kronecker delta.\n- A QR factorization is performed on $M$, yielding $M = QR_M$, where $Q$ is orthogonal and $R_M$ is upper triangular. This $Q$ forms our initial orthogonal basis.\n- To ensure a consistent orientation, we check the determinant of $Q$. If $\\det(Q) < 0$, the sign of the first column of $Q$ is flipped. This final orthogonal matrix is used as $V$.\n\n#### 2. Householder Tridiagonalization\n\nThe reduction of $A$ to a tridiagonal matrix $T$ is performed iteratively. The process consists of $n-2$ steps. At step $k$ (for $k=0, 1, \\dots, n-3$), we introduce zeros in the $k$-th column and $k$-th row of the matrix, below and to the right of the subdiagonal/superdiagonal elements, respectively.\n\nThis is achieved using Householder reflectors. A Householder reflector is an orthogonal, symmetric matrix $P = I - 2vv^\\top$, where $v$ is a unit vector. It reflects a vector across the hyperplane orthogonal to $v$.\n\nAt step $k$, we consider the vector $x \\in \\mathbb{R}^{n-k-1}$ formed by the elements $A_{k+1:n, k}$. We construct a reflector $P_k$ that maps $x$ to a multiple of the first standard basis vector, i.e., $P_k x = \\alpha e_1$. This annihilates all but the first element of $x$. The parameters for the reflector are:\n- $\\alpha = -\\operatorname{sign}(x_1) \\lVert x \\rVert_2$. The sign is chosen to avoid catastrophic cancellation when computing $x - \\alpha e_1$. We adopt the convention that $\\operatorname{sign}(0) = 1$.\n- The Householder vector is $v = \\frac{x - \\alpha e_1}{\\lVert x - \\alpha e_1 \\rVert_2}$.\n\nThe reflector $P_k$ is an $(n-k-1) \\times (n-k-1)$ matrix. To apply it to the full $n \\times n$ matrix $A$, it is embedded into an $n \\times n$ identity matrix, forming $Q_k = \\begin{pmatrix} I_k & 0 \\\\ 0 & P_k \\end{pmatrix}$. The matrix is then updated via a similarity transformation: $A \\to Q_k A Q_k$. This process is repeated for $k=0, \\dots, n-3$.\n\nThe total orthogonal transformation is the product of the individual reflectors: $Q = Q_0 Q_1 \\dots Q_{n-3}$. The final tridiagonal matrix is $T = Q^\\top A Q$.\n\nThe updates to $A$ and $Q$ at each step must be performed efficiently without explicitly forming the large matrices $Q_k$.\n- **Updating A:** The transformation $A \\to Q_k A Q_k$ is equivalent to $A \\to (I-2\\hat{v}\\hat{v}^\\top) A (I-2\\hat{v}\\hat{v}^\\top)$, where $\\hat{v}$ is the vector $v$ padded with $k+1$ leading zeros. This update can be computed efficiently as $A \\to A - 2(w\\hat{v}^\\top + \\hat{v}w^\\top)$, where $w = p - (\\hat{v}^\\top p)\\hat{v}$ and $p=A\\hat{v}$.\n- **Accumulating Q:** The total transformation $Q$ is initialized to $I_n$. At each step, it is updated by $Q \\to Q Q_k$. This is computed as $Q \\to Q - 2(Q\\hat{v})\\hat{v}^\\top$.\n\n#### 3. Error Quantification\n\nAfter the tridiagonalization, we compute four metrics to quantify the numerical errors:\n1.  $d_{AT} = \\max_i \\left| \\mu_i(T) - \\mu_i(A) \\right|$: The maximum absolute difference between the sorted eigenvalues of the computed tridiagonal matrix $T$ and the sorted eigenvalues of the original matrix $A$ (computed numerically). This measures the spectral drift caused by the tridiagonalization process.\n2.  $d_{A,\\Lambda} = \\max_i \\left| \\mu_i(A) - \\lambda_i \\right|$: The maximum absolute difference between the sorted numerically-computed eigenvalues of $A$ and the sorted prescribed eigenvalues $\\lambda_i$. This measures the error introduced when constructing $A$ from its known eigenstructure.\n3.  $e_{\\text{orth}} = \\lVert Q^\\top Q - I_n \\rVert_F$: The Frobenius norm of the deviation of the accumulated transformation matrix $Q$ from perfect orthogonality.\n4.  $e_{\\text{sim}} = \\lVert Q^\\top A Q - T \\rVert_F$: The Frobenius norm of the residual of the similarity transformation. This measures how well the computed $T$ and $Q$ satisfy the equation $T=Q^\\top A Q$.\n\nThese metrics provide a comprehensive picture of the numerical quality of the implemented algorithm.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the Householder tridiagonalization tests and print results.\n    \"\"\"\n\n    # Test suite as defined in the problem statement\n    test_cases = [\n        (5, np.array([1, 2, 3, 4, 5], dtype=float)),\n        (8, np.array([1, 1 + 1e-12, 2, 2 + 1e-12, 3, 3 + 1e-12, 4, 4 + 1e-12], dtype=float)),\n        (10, np.array([1e-8, 1e-6, 1e-4, 1e-2, 1, 10, 1e3, 1e5, 1e7, 1e9], dtype=float)),\n        (2, np.array([1, 1e-15], dtype=float)),\n        (1, np.array([42], dtype=float))\n    ]\n\n    results = []\n    \n    for n, lambdas in test_cases:\n        # 1. CONSTRUCT THE TEST MATRIX A\n        # Deterministically construct the orthogonal matrix V\n        i_ = np.arange(1, n + 1)\n        j_ = np.arange(1, n + 1)\n        I, J = np.meshgrid(i_, j_, indexing='ij')\n        M = np.sin(I * J) + np.cos(I - 0.3 * J) + 0.5 * np.identity(n)\n        \n        V, _ = np.linalg.qr(M)\n        if np.linalg.det(V) < 0:\n            V[:, 0] *= -1\n            \n        Lambda_diag = np.diag(lambdas)\n        A = V @ Lambda_diag @ V.T\n\n        # 2. HOUSEHOLDER TRIDIAGONALIZATION\n        T = A.copy()\n        Q = np.identity(n, dtype=float)\n\n        for k in range(n - 2):\n            # Define the vector x to be annihilated\n            x = T[k + 1:, k]\n            x_norm = np.linalg.norm(x)\n\n            # Skip if the sub-column is already zero\n            if x_norm < np.finfo(float).eps:\n                continue\n\n            # Numerically stable choice for alpha\n            sign_x0 = np.copysign(1.0, x[0]) if x.size > 0 else 1.0\n            alpha = -sign_x0 * x_norm\n\n            # Construct the Householder vector v\n            u = x.copy()\n            u[0] -= alpha\n            u_norm = np.linalg.norm(u)\n            \n            if u_norm < np.finfo(float).eps:\n                continue\n\n            v_sub = u / u_norm\n            \n            # Embed v into a full-size vector\n            v_full = np.zeros(n, dtype=float)\n            v_full[k + 1:] = v_sub\n\n            # Efficiently update T and Q\n            # Update T -> H T H = T - 2(w v^T + v w^T) where w = p - (v^T p)v, p = Tv\n            p = T @ v_full\n            w = p - np.dot(p, v_full) * v_full\n            T -= 2 * (np.outer(w, v_full) + np.outer(v_full, w))\n\n            # Update Q -> Q H = Q - 2(Qv)v^T\n            Q -= 2 * np.outer(Q @ v_full, v_full)\n\n        # Clean T to be perfectly tridiagonal by zeroing out rounding errors\n        T_clean = np.triu(np.tril(T, 1), -1)\n\n        # 3. COMPUTE QUANTITATIVE OUTPUTS\n        # Eigenvalues must be sorted for comparison\n        lambda_known_sorted = np.sort(lambdas)\n        mu_A_sorted = np.sort(np.linalg.eigvalsh(A))\n        mu_T_sorted = np.sort(np.linalg.eigvalsh(T_clean))\n\n        # d_AT: Difference between eigenvalues of T and A\n        d_AT = np.max(np.abs(mu_T_sorted - mu_A_sorted))\n        \n        # d_A,Lambda: Difference between computed eigenvalues of A and known eigenvalues\n        d_A_Lambda = np.max(np.abs(mu_A_sorted - lambda_known_sorted))\n        \n        # e_orth: Orthogonality error of Q\n        e_orth = np.linalg.norm(Q.T @ Q - np.identity(n), 'fro')\n        \n        # e_sim: Similarity transformation residual\n        e_sim = np.linalg.norm(Q.T @ A @ Q - T_clean, 'fro')\n\n        results.append([d_AT, d_A_Lambda, e_orth, e_sim])\n\n    # Final print statement in the exact required format\n    # Using str.replace to remove spaces for compact output matching the spec\n    print(str(results).replace(\" \", \"\"))\n\nsolve()\n```", "id": "3572312"}]}