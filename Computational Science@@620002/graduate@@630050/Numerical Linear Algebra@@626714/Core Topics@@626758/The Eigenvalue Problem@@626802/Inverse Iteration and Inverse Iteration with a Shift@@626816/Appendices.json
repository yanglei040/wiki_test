{"hands_on_practices": [{"introduction": "To build a solid understanding of inverse iteration, we begin with its most fundamental form: unshifted inverse iteration. This method is essentially the power method applied to the inverse of a matrix, $A^{-1}$, making it a powerful tool for finding the eigenvalue of $A$ with the smallest absolute magnitude. This first practice will guide you through a single, foundational step of the algorithm to make the abstract concept of eigenvector amplification concrete [@problem_id:3551809].", "problem": "Consider the real diagonal matrix $A=\\mathrm{diag}(1,3,10)$ and the initial vector $x_0=(1,1,1)^{\\top}$. One step of inverse iteration without shift proceeds by solving the linear system $A\\,y_1=x_0$ and then normalizing $x_1=y_1/\\|y_1\\|_2$ using the Euclidean norm $\\|x\\|_2=(x^{\\top}x)^{1/2}$. Use only the definition of inverse iteration and the definition of the Rayleigh quotient $r(x)=(x^{\\top}A x)/(x^{\\top}x)$ to compute:\n- the normalized iterate $x_1$,\n- the Rayleigh quotient $r(x_1)$,\n- and, based on the structure of the iteration and the computed $x_1$, explain which eigencomponent of $x_1$ dominates and why.\n\nProvide the Rayleigh quotient as an exact rational number in lowest terms. Report only the Rayleigh quotient as your final numeric answer; no rounding is required.", "solution": "The problem asks for three quantities related to one step of inverse iteration for a given matrix $A$ and initial vector $x_0$. We will compute these in sequence.\n\nFirst, we are given the diagonal matrix $A$ and the initial vector $x_0$:\n$$\nA = \\mathrm{diag}(1, 3, 10) = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 3 & 0 \\\\ 0 & 0 & 10 \\end{pmatrix}, \\quad x_0 = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}\n$$\n\nThe first part of the inverse iteration step is to solve the linear system $A y_1 = x_0$ for the vector $y_1$. Since $A$ is a diagonal matrix and all its diagonal entries are non-zero, it is invertible. Its inverse $A^{-1}$ is the diagonal matrix of the reciprocals of the diagonal entries of $A$:\n$$\nA^{-1} = \\mathrm{diag}\\left(\\frac{1}{1}, \\frac{1}{3}, \\frac{1}{10}\\right) = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & \\frac{1}{3} & 0 \\\\ 0 & 0 & \\frac{1}{10} \\end{pmatrix}\n$$\nWe can now solve for $y_1$ by computing $y_1 = A^{-1} x_0$:\n$$\ny_1 = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & \\frac{1}{3} & 0 \\\\ 0 & 0 & \\frac{1}{10} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\cdot 1 \\\\ \\frac{1}{3} \\cdot 1 \\\\ \\frac{1}{10} \\cdot 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ \\frac{1}{3} \\\\ \\frac{1}{10} \\end{pmatrix}\n$$\n\nThe second part of the iteration step is to normalize $y_1$ using the Euclidean norm $\\| \\cdot \\|_2$ to obtain $x_1$. First, we compute the squared Euclidean norm of $y_1$:\n$$\n\\|y_1\\|_2^2 = y_1^{\\top} y_1 = 1^2 + \\left(\\frac{1}{3}\\right)^2 + \\left(\\frac{1}{10}\\right)^2 = 1 + \\frac{1}{9} + \\frac{1}{100}\n$$\nTo sum these fractions, we find a common denominator, which is $900$:\n$$\n\\|y_1\\|_2^2 = \\frac{900}{900} + \\frac{100}{900} + \\frac{9}{900} = \\frac{1009}{900}\n$$\nThe Euclidean norm is the square root of this value:\n$$\n\\|y_1\\|_2 = \\sqrt{\\frac{1009}{900}} = \\frac{\\sqrt{1009}}{30}\n$$\nNow, we find the normalized iterate $x_1 = y_1 / \\|y_1\\|_2$:\n$$\nx_1 = \\frac{1}{\\|y_1\\|_2} y_1 = \\frac{30}{\\sqrt{1009}} \\begin{pmatrix} 1 \\\\ \\frac{1}{3} \\\\ \\frac{1}{10} \\end{pmatrix} = \\frac{1}{\\sqrt{1009}} \\begin{pmatrix} 30 \\cdot 1 \\\\ 30 \\cdot \\frac{1}{3} \\\\ 30 \\cdot \\frac{1}{10} \\end{pmatrix} = \\frac{1}{\\sqrt{1009}} \\begin{pmatrix} 30 \\\\ 10 \\\\ 3 \\end{pmatrix}\n$$\nThis is the first required result.\n\nNext, we compute the Rayleigh quotient $r(x_1)$, defined as $r(x) = (x^{\\top} A x) / (x^{\\top} x)$. For the normalized vector $x_1$, the denominator $x_1^{\\top} x_1 = \\|x_1\\|_2^2$ is equal to $1$ by definition of normalization. Thus, we only need to compute the numerator, $x_1^{\\top} A x_1$.\n$$\nr(x_1) = x_1^{\\top} A x_1 = \\left( \\frac{1}{\\sqrt{1009}} \\begin{pmatrix} 30 \\\\ 10 \\\\ 3 \\end{pmatrix}^{\\top} \\right) \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 3 & 0 \\\\ 0 & 0 & 10 \\end{pmatrix} \\left( \\frac{1}{\\sqrt{1009}} \\begin{pmatrix} 30 \\\\ 10 \\\\ 3 \\end{pmatrix} \\right)\n$$\n$$\nr(x_1) = \\frac{1}{1009} \\begin{pmatrix} 30 & 10 & 3 \\end{pmatrix} \\begin{pmatrix} 1 \\cdot 30 \\\\ 3 \\cdot 10 \\\\ 10 \\cdot 3 \\end{pmatrix} = \\frac{1}{1009} \\begin{pmatrix} 30 & 10 & 3 \\end{pmatrix} \\begin{pmatrix} 30 \\\\ 30 \\\\ 30 \\end{pmatrix}\n$$\n$$\nr(x_1) = \\frac{1}{1009} (30 \\cdot 30 + 10 \\cdot 30 + 3 \\cdot 30) = \\frac{900 + 300 + 90}{1009} = \\frac{1290}{1009}\n$$\nSince $1009$ is a prime number, this fraction is in lowest terms. This is the second required result.\n\nFinally, we must explain which eigencomponent of $x_1$ dominates and why. The matrix $A$ is diagonal, so its eigenvalues are its diagonal entries: $\\lambda_1=1$, $\\lambda_2=3$, and $\\lambda_3=10$. The corresponding eigenvectors are the standard basis vectors: $v_1=(1,0,0)^{\\top}$, $v_2=(0,1,0)^{\\top}$, and $v_3=(0,0,1)^{\\top}$.\n\nThe initial vector $x_0=(1,1,1)^{\\top}$ can be expressed as a linear combination of these eigenvectors:\n$$\nx_0 = 1 \\cdot v_1 + 1 \\cdot v_2 + 1 \\cdot v_3\n$$\nInverse iteration (without shift) is equivalent to applying the power method to the matrix $A^{-1}$. One step of the iteration transforms $x_0$ into $y_1 = A^{-1}x_0$:\n$$\ny_1 = A^{-1}(1 \\cdot v_1 + 1 \\cdot v_2 + 1 \\cdot v_3) = 1 \\cdot A^{-1}v_1 + 1 \\cdot A^{-1}v_2 + 1 \\cdot A^{-1}v_3\n$$\nSince for an eigenvector $v_i$ of $A$ with eigenvalue $\\lambda_i$, we have $A^{-1}v_i = \\frac{1}{\\lambda_i}v_i$, it follows that:\n$$\ny_1 = 1 \\cdot \\frac{1}{\\lambda_1} v_1 + 1 \\cdot \\frac{1}{\\lambda_2} v_2 + 1 \\cdot \\frac{1}{\\lambda_3} v_3 = \\frac{1}{1} v_1 + \\frac{1}{3} v_2 + \\frac{1}{10} v_3 = 1 v_1 + \\frac{1}{3} v_2 + \\frac{1}{10} v_3\n$$\nThis gives $y_1=(1, 1/3, 1/10)^{\\top}$, consistent with our earlier calculation. The power method amplifies the component of the eigenvector corresponding to the eigenvalue of largest magnitude. The eigenvalues of $A^{-1}$ are $1/\\lambda_1=1$, $1/\\lambda_2=1/3$, and $1/\\lambda_3=1/10$. The largest in magnitude is $1$.\n\nTherefore, the iteration amplifies the component of the eigenvector $v_1$ corresponding to this largest eigenvalue of $A^{-1}$ (which is the smallest eigenvalue of $A$). After one step, the vector $y_1$ has components $(1, 1/3, 1/10)$. The first component, corresponding to $v_1$, has a coefficient of $1$, which is larger than $1/3$ and $1/10$. Normalization scales all components by the same positive factor, so the relative dominance is preserved in $x_1$. The components of $x_1$ are proportional to $(30, 10, 3)$. The first component is clearly the largest.\nThus, the eigencomponent of $x_1$ corresponding to the eigenvector $v_1$ (associated with the eigenvalue $\\lambda_1=1$) dominates.", "answer": "$$\n\\boxed{\\frac{1290}{1009}}\n$$", "id": "3551809"}, {"introduction": "While unshifted inverse iteration is effective for finding the smallest eigenvalue, its true power is unlocked with the introduction of a shift, $\\sigma$. By applying the method to the matrix $(A - \\sigma I)$, we can selectively target the eigenvalue of $A$ that is closest to our chosen $\\sigma$. This exercise demonstrates the mechanics of shifted inverse iteration, where you will perform several steps to observe the convergence towards a specific, targeted eigenpair, not just the one with the smallest magnitude [@problem_id:3551821].", "problem": "Consider the real symmetric matrix $A=\\begin{bmatrix}2&1\\\\1&2\\end{bmatrix}$ and the real shift $\\sigma=1.9$. The shifted inverse iteration for a nonsingular matrix $A-\\sigma I$ is defined as follows: given a nonzero initial vector $x_{0}$, for $k\\geq 1$ solve $(A-\\sigma I)y_{k}=x_{k-1}$ and set $x_{k}=y_{k}/\\|y_{k}\\|_{2}$, where $\\|\\cdot\\|_{2}$ denotes the Euclidean norm. The Rayleigh quotient of a nonzero vector $x$ with respect to $A$ is $\\rho(x)=\\dfrac{x^{\\top}A\\,x}{x^{\\top}x}$. Starting from $x_{0}=\\begin{bmatrix}1\\\\0\\end{bmatrix}$ and using the fixed shift $\\sigma=1.9$, carry out two steps of shifted inverse iteration to obtain $x_{1}$ and $x_{2}$, and report the sequence of Rayleigh quotients $\\rho(x_{1})$ and $\\rho(x_{2})$. Express your final pair of Rayleigh quotients in exact rational form; do not round.", "solution": "The given matrix is $A=\\begin{bmatrix}2&1\\\\1&2\\end{bmatrix}$, the shift is $\\sigma=1.9$, and the initial vector is $x_{0}=\\begin{bmatrix}1\\\\0\\end{bmatrix}$. The shifted inverse iteration method is defined by the recurrence relation $(A-\\sigma I)y_{k}=x_{k-1}$, followed by normalization $x_{k}=y_{k}/\\|y_{k}\\|_{2}$. The Rayleigh quotient is $\\rho(x)=\\dfrac{x^{\\top}A\\,x}{x^{\\top}x}$. For a unit vector $x$, $\\rho(x) = x^{\\top}A\\,x$.\n\nFirst, we compute the shifted matrix, $A-\\sigma I$. Since $\\sigma=1.9=\\frac{19}{10}$, we have:\n$$\nA-\\sigma I = \\begin{bmatrix}2&1\\\\1&2\\end{bmatrix} - \\frac{19}{10}\\begin{bmatrix}1&0\\\\0&1\\end{bmatrix} = \\begin{bmatrix}2 - \\frac{19}{10} & 1 \\\\ 1 & 2 - \\frac{19}{10}\\end{bmatrix} = \\begin{bmatrix}\\frac{1}{10} & 1 \\\\ 1 & \\frac{1}{10}\\end{bmatrix}\n$$\n\n**Step 1 of the iteration ($k=1$):**\n\nWe solve the linear system $(A-\\sigma I)y_{1}=x_{0}$:\n$$\n\\begin{bmatrix}\\frac{1}{10} & 1 \\\\ 1 & \\frac{1}{10}\\end{bmatrix} y_{1} = \\begin{bmatrix}1\\\\0\\end{bmatrix}\n$$\nThis corresponds to the equations:\n$$\n\\frac{1}{10} y_{1,1} + y_{1,2} = 1\n$$\n$$\ny_{1,1} + \\frac{1}{10} y_{1,2} = 0\n$$\nFrom the second equation, we find $y_{1,1} = -\\frac{1}{10} y_{1,2}$. Substituting this into the first equation gives:\n$$\n\\frac{1}{10}\\left(-\\frac{1}{10} y_{1,2}\\right) + y_{1,2} = 1 \\implies -\\frac{1}{100} y_{1,2} + y_{1,2} = 1 \\implies \\frac{99}{100} y_{1,2} = 1 \\implies y_{1,2} = \\frac{100}{99}\n$$\nThen, $y_{1,1} = -\\frac{1}{10}\\left(\\frac{100}{99}\\right) = -\\frac{10}{99}$.\nSo, the unnormalized vector is $y_{1} = \\begin{bmatrix}-10/99 \\\\ 100/99\\end{bmatrix}$.\n\nNext, we calculate the Rayleigh quotient $\\rho(x_1)$. Since the Rayleigh quotient is homogeneous of degree zero (i.e., $\\rho(\\alpha x) = \\rho(x)$ for any scalar $\\alpha \\neq 0$), we can compute it using a vector proportional to $y_1$, such as $v_1' = \\begin{bmatrix}-1 \\\\ 10\\end{bmatrix}$.\n$$\n\\rho(x_{1}) = \\rho(v_1') = \\frac{(v_1')^{\\top}A v_1'}{(v_1')^{\\top}v_1'}\n$$\nThe denominator is $(v_1')^{\\top}v_1' = (-1)^2 + 10^2 = 1+100=101$.\nThe numerator is calculated as follows:\n$$\nA v_1' = \\begin{bmatrix}2&1\\\\1&2\\end{bmatrix} \\begin{bmatrix}-1\\\\10\\end{bmatrix} = \\begin{bmatrix}2(-1)+1(10) \\\\ 1(-1)+2(10)\\end{bmatrix} = \\begin{bmatrix}8\\\\19\\end{bmatrix}\n$$\n$$\n(v_1')^{\\top}A v_1' = \\begin{bmatrix}-1&10\\end{bmatrix} \\begin{bmatrix}8\\\\19\\end{bmatrix} = (-1)(8) + (10)(19) = -8+190=182\n$$\nSo, the first Rayleigh quotient is:\n$$\n\\rho(x_{1}) = \\frac{182}{101}\n$$\n\n**Step 2 of the iteration ($k=2$):**\n\nFirst, we need the normalized vector $x_{1}$:\n$$\n\\|y_{1}\\|_{2} = \\left\\| \\begin{bmatrix}-10/99 \\\\ 100/99\\end{bmatrix} \\right\\|_{2} = \\sqrt{\\left(-\\frac{10}{99}\\right)^2 + \\left(\\frac{100}{99}\\right)^2} = \\sqrt{\\frac{100+10000}{99^2}} = \\frac{\\sqrt{10100}}{99} = \\frac{10\\sqrt{101}}{99}\n$$\n$$\nx_{1} = \\frac{y_{1}}{\\|y_{1}\\|_{2}} = \\frac{1}{\\frac{10\\sqrt{101}}{99}} \\begin{bmatrix}-10/99 \\\\ 100/99\\end{bmatrix} = \\frac{1}{\\sqrt{101}}\\begin{bmatrix}-1\\\\10\\end{bmatrix}\n$$\nNow we solve the system $(A-\\sigma I)y_{2}=x_{1}$:\n$$\n\\begin{bmatrix}\\frac{1}{10} & 1 \\\\ 1 & \\frac{1}{10}\\end{bmatrix} y_{2} = \\frac{1}{\\sqrt{101}}\\begin{bmatrix}-1\\\\10\\end{bmatrix}\n$$\nWe can solve for $v_2 = \\sqrt{101} y_2$:\n$$\n\\begin{bmatrix}\\frac{1}{10} & 1 \\\\ 1 & \\frac{1}{10}\\end{bmatrix} v_2 = \\begin{bmatrix}-1\\\\10\\end{bmatrix}\n$$\nThis gives the system:\n$$\n\\frac{1}{10} v_{2,1} + v_{2,2} = -1\n$$\n$$\nv_{2,1} + \\frac{1}{10} v_{2,2} = 10\n$$\nFrom the second equation, $v_{2,1} = 10 - \\frac{1}{10} v_{2,2}$. Substituting into the first:\n$$\n\\frac{1}{10}\\left(10 - \\frac{1}{10} v_{2,2}\\right) + v_{2,2} = -1 \\implies 1 - \\frac{1}{100} v_{2,2} + v_{2,2} = -1 \\implies \\frac{99}{100} v_{2,2} = -2 \\implies v_{2,2} = -\\frac{200}{99}\n$$\nThen, $v_{2,1} = 10 - \\frac{1}{10}\\left(-\\frac{200}{99}\\right) = 10 + \\frac{20}{99} = \\frac{990+20}{99} = \\frac{1010}{99}$.\nSo, $y_2$ is proportional to $v_2 = \\begin{bmatrix}1010/99 \\\\ -200/99\\end{bmatrix}$, and also to $v_2'' = \\begin{bmatrix}101 \\\\ -20\\end{bmatrix}$. We use $v_2''$ to calculate the Rayleigh quotient $\\rho(x_2)$:\n$$\n\\rho(x_{2}) = \\rho(v_2'') = \\frac{(v_2'')^{\\top}A v_2''}{(v_2'')^{\\top}v_2''}\n$$\nThe denominator is $(v_2'')^{\\top}v_2'' = 101^2 + (-20)^2 = 10201+400=10601$.\nThe numerator is calculated as follows:\n$$\nA v_2'' = \\begin{bmatrix}2&1\\\\1&2\\end{bmatrix} \\begin{bmatrix}101\\\\-20\\end{bmatrix} = \\begin{bmatrix}2(101)+1(-20) \\\\ 1(101)+2(-20)\\end{bmatrix} = \\begin{bmatrix}202-20\\\\101-40\\end{bmatrix} = \\begin{bmatrix}182\\\\61\\end{bmatrix}\n$$\n$$\n(v_2'')^{\\top}A v_2'' = \\begin{bmatrix}101&-20\\end{bmatrix} \\begin{bmatrix}182\\\\61\\end{bmatrix} = (101)(182) + (-20)(61) = 18382 - 1220 = 17162\n$$\nSo, the second Rayleigh quotient is:\n$$\n\\rho(x_{2}) = \\frac{17162}{10601}\n$$\nThe sequence of Rayleigh quotients is $\\rho(x_1)=\\frac{182}{101}$ and $\\rho(x_2)=\\frac{17162}{10601}$.", "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{182}{101} & \\frac{17162}{10601} \\end{pmatrix}}\n$$", "id": "3551821"}, {"introduction": "We now transition from manual calculation to computational practice, where the true efficiency of different iterative methods can be compared. This exercise introduces Rayleigh Quotient Iteration (RQI), an advanced variant where the shift is dynamically updated at each step using the Rayleigh quotient, leading to exceptionally fast (cubic) convergence for symmetric matrices. By implementing and comparing power iteration, fixed-shift inverse iteration, and RQI on a series of test cases, you will gain practical insight into their relative performance and appreciate why RQI is often the method of choice in real-world applications [@problem_id:2427128].", "problem": "Implement an algorithm to approximate eigenpairs of a real symmetric matrix using inverse iteration with a variable shift given by the Rayleigh quotient. Let the Rayleigh quotient for a nonzero vector $x \\in \\mathbb{R}^n$ be defined as $R(x) = \\dfrac{x^\\mathsf{T} A x}{x^\\mathsf{T} x}$. Consider the following three iterative schemes applied to a real symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$ with a given nonzero initial vector $x_0 \\in \\mathbb{R}^n$ and tolerance $\\varepsilon > 0$:\n\n- Power iteration: $x_{k+1} \\leftarrow \\dfrac{A x_k}{\\lVert A x_k \\rVert_2}$, with the residual norm defined using $\\lambda_k = R(x_k)$ as $\\lVert A x_k - \\lambda_k x_k \\rVert_2$.\n- Inverse iteration with a fixed shift: with $\\sigma_0 = R(x_0)$ fixed, compute $x_{k+1}$ by solving $(A - \\sigma_0 I) y = x_k$ and setting $x_{k+1} \\leftarrow \\dfrac{y}{\\lVert y \\rVert_2}$, with the residual norm defined using $\\lambda_k = R(x_k)$ as $\\lVert A x_k - \\lambda_k x_k \\rVert_2$.\n- Inverse iteration with a variable shift equal to the Rayleigh quotient (Rayleigh quotient iteration): at each iteration, compute $\\sigma_k = R(x_k)$, solve $(A - \\sigma_k I) y = x_k$, and set $x_{k+1} \\leftarrow \\dfrac{y}{\\lVert y \\rVert_2}$, with the residual norm defined using $\\lambda_k = R(x_k)$ as $\\lVert A x_k - \\lambda_k x_k \\rVert_2$.\n\nFor each scheme, stop when the residual norm first satisfies $\\lVert A x_k - \\lambda_k x_k \\rVert_2 \\le \\varepsilon$ or when a prescribed maximum number of iterations is reached. All vector norms are the Euclidean norm, and $I$ denotes the identity matrix of size $n$.\n\nUse the following test suite. In every case, set $n = 5$, the tolerance $\\varepsilon = 10^{-10}$, and the maximum number of iterations to $1000$.\n\n- Test case $\\#1$ (tri-diagonal symmetric positive definite matrix):\n  - Matrix $A_1 \\in \\mathbb{R}^{5 \\times 5}$:\n    $$\n    A_1 =\n    \\begin{bmatrix}\n    6 & 2 & 0 & 0 & 0 \\\\\n    2 & 5 & 2 & 0 & 0 \\\\\n    0 & 2 & 4 & 2 & 0 \\\\\n    0 & 0 & 2 & 3 & 2 \\\\\n    0 & 0 & 0 & 2 & 2\n    \\end{bmatrix}.\n    $$\n  - Initial vector $x_0^{(1)} = \\dfrac{1}{\\sqrt{5}} [1, 1, 1, 1, 1]^\\mathsf{T}$.\n\n- Test case $\\#2$ (symmetric matrix with two very close eigenvalues):\n  - Define diagonal matrix $D = \\mathrm{diag}(1, 1 + 10^{-6}, 2, 3, 4)$.\n  - Define the planar rotation with angle $\\theta$ such that $\\cos \\theta = \\dfrac{4}{5}$ and $\\sin \\theta = \\dfrac{3}{5}$, and set\n    $$\n    Q = \\begin{bmatrix}\n    \\cos \\theta & -\\sin \\theta & 0 & 0 & 0 \\\\\n    \\sin \\theta & \\phantom{-}\\cos \\theta & 0 & 0 & 0 \\\\\n    0 & 0 & 1 & 0 & 0 \\\\\n    0 & 0 & 0 & 1 & 0 \\\\\n    0 & 0 & 0 & 0 & 1\n    \\end{bmatrix}.\n    $$\n  - Matrix $A_2 = Q^\\mathsf{T} D Q$.\n  - Initial vector $x_0^{(2)} = [1, 0, 0, 0, 0]^\\mathsf{T}$.\n\n- Test case $\\#3$ (Hilbert matrix):\n  - Matrix $A_3 \\in \\mathbb{R}^{5 \\times 5}$ with entries $(A_3)_{ij} = \\dfrac{1}{i + j - 1}$ for $i,j \\in \\{1, 2, 3, 4, 5\\}$.\n  - Initial vector $x_0^{(3)} = \\dfrac{1}{\\sqrt{5}} [1, -1, 1, -1, 1]^\\mathsf{T}$.\n\nFor each test case, run the three schemes independently with the same $A$ and $x_0$ and record the smallest iteration count $k$ at which the residual norm first satisfies $\\lVert A x_k - \\lambda_k x_k \\rVert_2 \\le \\varepsilon$. If convergence does not occur within the maximum number of iterations, record the maximum number of iterations.\n\nYour program must output a single line containing a comma-separated list of $9$ integers enclosed in square brackets, in the following order:\n$[k_{\\mathrm{RQI}}^{(1)}, k_{\\mathrm{fixed}}^{(1)}, k_{\\mathrm{power}}^{(1)}, k_{\\mathrm{RQI}}^{(2)}, k_{\\mathrm{fixed}}^{(2)}, k_{\\mathrm{power}}^{(2)}, k_{\\mathrm{RQI}}^{(3)}, k_{\\mathrm{fixed}}^{(3)}, k_{\\mathrm{power}}^{(3)}]$, where $k_{\\mathrm{RQI}}^{(i)}$ is the iteration count for Rayleigh quotient iteration on test case $i$, $k_{\\mathrm{fixed}}^{(i)}$ is the iteration count for inverse iteration with a fixed shift $\\sigma_0 = R(x_0^{(i)})$, and $k_{\\mathrm{power}}^{(i)}$ is the iteration count for power iteration. The output must be exactly one line in this format, with no additional characters or whitespace beyond those structurally required by the list representation.", "solution": "The problem requires the implementation and comparison of three iterative algorithms for approximating an eigenpair $(\\lambda, v)$ of a real symmetric matrix $A$. An eigenpair consists of an eigenvalue $\\lambda$ and its corresponding eigenvector $v$. The methods under consideration are power iteration, inverse iteration with a fixed shift, and inverse iteration with a variable shift, also known as Rayleigh quotient iteration (RQI). For a real symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$, all its eigenvalues are real, and there exists an orthonormal basis of eigenvectors. The Rayleigh quotient, defined for a nonzero vector $x \\in \\mathbb{R}^n$ as $R(x) = \\frac{x^\\mathsf{T} A x}{x^\\mathsf{T} x}$, provides an estimate for an eigenvalue. If $x$ is an eigenvector, then $R(x)$ is the corresponding exact eigenvalue. For all algorithms, we start with an initial vector $x_0$ and generate a sequence of vectors $\\{x_k\\}$ that converges to an eigenvector, and a sequence of Rayleigh quotients $\\{\\lambda_k = R(x_k)\\}$ that converges to the corresponding eigenvalue.\n\n1.  **Power Iteration**\n\n    The power iteration method is the simplest algorithm for finding the dominant eigenpair of a matrix, i.e., the eigenpair $(\\lambda_1, v_1)$ where $|\\lambda_1|$ is the largest magnitude among all eigenvalues. The iterative step is defined as:\n    $$\n    x_{k+1} = \\frac{A x_k}{\\lVert A x_k \\rVert_2}\n    $$\n    Starting with an initial vector $x_0$ that has a non-zero component in the direction of the dominant eigenvector $v_1$, the sequence $x_k$ converges to $v_1$. The convergence is linear, with a rate determined by the ratio $|\\lambda_2 / \\lambda_1|$, where $\\lambda_2$ is the eigenvalue with the second-largest magnitude.\n\n2.  **Inverse Iteration with Fixed Shift**\n\n    Inverse iteration is a method to find the eigenpair corresponding to the eigenvalue closest to a given shift $\\sigma$. It applies the power method to the matrix $(A - \\sigma I)^{-1}$. The eigenvalues of $(A - \\sigma I)^{-1}$ are $(\\lambda_i - \\sigma)^{-1}$, where $\\lambda_i$ are the eigenvalues of $A$. The dominant eigenvalue of $(A - \\sigma I)^{-1}$ corresponds to the smallest value of $|\\lambda_i - \\sigma|$, which means $\\lambda_i$ is the eigenvalue of $A$ closest to $\\sigma$. The iterative step is:\n    $$\n    x_{k+1} = \\frac{(A - \\sigma I)^{-1} x_k}{\\lVert (A - \\sigma I)^{-1} x_k \\rVert_2}\n    $$\n    In practice, computing the matrix inverse is avoided. Instead, we solve the linear system $(A - \\sigma I) y_k = x_k$ for $y_k$, and then normalize:\n    $$\n    x_{k+1} = \\frac{y_k}{\\lVert y_k \\rVert_2}\n    $$\n    In this problem, a fixed shift $\\sigma_0 = R(x_0)$ is used throughout the process. Convergence to the eigenvector $v_j$ is very rapid if $\\sigma_0$ is much closer to one eigenvalue $\\lambda_j$ than to any other.\n\n3.  **Rayleigh Quotient Iteration (RQI)**\n\n    Rayleigh quotient iteration is a powerful refinement of inverse iteration where the shift is updated at each step using the best current estimate for the eigenvalue: the Rayleigh quotient. The iterative process is defined by:\n    1.  Compute the shift: $\\sigma_k = R(x_k) = \\frac{x_k^\\mathsf{T} A x_k}{x_k^\\mathsf{T} x_k}$.\n    2.  Solve for $y_{k+1}$: $(A - \\sigma_k I) y_{k+1} = x_k$.\n    3.  Normalize: $x_{k+1} = \\frac{y_{k+1}}{\\lVert y_{k+1} \\rVert_2}$.\n\n    For a symmetric matrix, RQI exhibits cubic convergence once the iterate $x_k$ is sufficiently close to an eigenvector. This means that the number of correct digits in the approximation roughly triples with each iteration, leading to extremely fast convergence.\n\n**Stopping Criterion**\n\nFor all three methods, the iteration terminates when the norm of the residual vector, $\\lVert A x_k - \\lambda_k x_k \\rVert_2$, falls below a specified tolerance $\\varepsilon$, where $\\lambda_k = R(x_k)$. This residual measures how well the current approximate pair $( \\lambda_k, x_k )$ satisfies the eigenvalue equation. The iteration count $k$ at which this condition is first met is the desired output.\n\nThe implementation will proceed by defining three functions, one for each algorithm. Each function will iteratively generate the vector sequence and check the stopping criterion at each step, returning the iteration count. These functions will then be applied to the three specified test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef power_iteration(A, x0, tol, max_iter):\n    \"\"\"\n    Approximates a dominant eigenpair using Power Iteration.\n    \"\"\"\n    x = x0 / np.linalg.norm(x0)\n    for k in range(1, max_iter + 1):\n        # Calculate x_k\n        v = A @ x\n        x_k = v / np.linalg.norm(v)\n\n        # Check residual for x_k\n        # Since x_k is normalized, its L2 norm squared is 1.\n        lam_k = x_k.T @ A @ x_k\n        residual_norm = np.linalg.norm(A @ x_k - lam_k * x_k)\n\n        if residual_norm = tol:\n            return k\n\n        # Prepare for the next iteration\n        x = x_k\n\n    return max_iter\n\ndef inverse_iteration_fixed_shift(A, x0, tol, max_iter):\n    \"\"\"\n    Approximates an eigenpair using inverse iteration with a fixed shift.\n    The shift is the Rayleigh quotient of the initial vector.\n    \"\"\"\n    # Normalize initial vector for stability, although given vectors are normalized.\n    # The problem specifies sigma0 = R(x0), where x0 is the given initial vector.\n    # Since all given x0 are unit norm, x0.T @ x0 = 1.\n    sigma0 = x0.T @ A @ x0\n    \n    try:\n        M = A - sigma0 * np.eye(A.shape[0])\n    except np.linalg.LinAlgError:\n        return max_iter # Fails if shift is an exact eigenvalue\n\n    x = x0 / np.linalg.norm(x0) # Start iteration with normalized vector\n\n    for k in range(1, max_iter + 1):\n        try:\n            # Solve (A - sigma0*I) y = x_{k-1}\n            y = np.linalg.solve(M, x)\n        except np.linalg.LinAlgError:\n            # Shift is an eigenvalue or matrix is numerically singular\n            return max_iter\n\n        x_k = y / np.linalg.norm(y)\n\n        # Check residual for x_k\n        lam_k = x_k.T @ A @ x_k\n        residual_norm = np.linalg.norm(A @ x_k - lam_k * x_k)\n\n        if residual_norm = tol:\n            return k\n\n        x = x_k\n\n    return max_iter\n\ndef rayleigh_quotient_iteration(A, x0, tol, max_iter):\n    \"\"\"\n    Approximates an eigenpair using Rayleigh Quotient Iteration.\n    \"\"\"\n    x = x0 / np.linalg.norm(x0)\n    \n    for k in range(1, max_iter + 1):\n        # Update shift at each step using the Rayleigh quotient of x_{k-1}\n        sigma = x.T @ A @ x\n\n        try:\n            M = A - sigma * np.eye(A.shape[0])\n            # Solve (A - sigma_{k-1}*I) y = x_{k-1}\n            y = np.linalg.solve(M, x)\n        except np.linalg.LinAlgError:\n            # If the shift is an eigenvalue, the previous iterate was the eigenvector.\n            # Its residual should be zero or very small.\n            # The loop condition will have caught it in the previous iteration.\n            # This indicates a numerical breakdown or an exact hit.\n            return k-1 if k > 1 else max_iter\n\n        x_k = y / np.linalg.norm(y)\n\n        # Check residual for x_k\n        lam_k = x_k.T @ A @ x_k\n        residual_norm = np.linalg.norm(A @ x_k - lam_k * x_k)\n        \n        if residual_norm = tol:\n            return k\n\n        x = x_k\n\n    return max_iter\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    n = 5\n    tol = 1e-10\n    max_iter = 1000\n\n    # Test Case 1\n    A1 = np.array([\n        [6, 2, 0, 0, 0],\n        [2, 5, 2, 0, 0],\n        [0, 2, 4, 2, 0],\n        [0, 0, 2, 3, 2],\n        [0, 0, 0, 2, 2]\n    ], dtype=float)\n    x0_1 = np.ones(n) / np.sqrt(n)\n\n    # Test Case 2\n    D = np.diag([1.0, 1.0 + 1e-6, 2.0, 3.0, 4.0])\n    cos_theta = 4.0 / 5.0\n    sin_theta = 3.0 / 5.0\n    Q = np.eye(n)\n    Q[0, 0] = cos_theta\n    Q[0, 1] = -sin_theta\n    Q[1, 0] = sin_theta\n    Q[1, 1] = cos_theta\n    A2 = Q.T @ D @ Q\n    x0_2 = np.zeros(n)\n    x0_2[0] = 1.0\n\n    # Test Case 3\n    A3 = np.fromfunction(lambda i, j: 1 / (i + j + 1), (n, n), dtype=float)\n    x0_3 = np.array([1, -1, 1, -1, 1]) / np.sqrt(n)\n    \n    test_cases = [\n        (A1, x0_1),\n        (A2, x0_2),\n        (A3, x0_3)\n    ]\n\n    results = []\n    for A, x0 in test_cases:\n        k_rqi = rayleigh_quotient_iteration(A, x0, tol, max_iter)\n        k_fixed = inverse_iteration_fixed_shift(A, x0, tol, max_iter)\n        k_power = power_iteration(A, x0, tol, max_iter)\n        \n        results.extend([k_rqi, k_fixed, k_power])\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "2427128"}]}