{"hands_on_practices": [{"introduction": "This first exercise provides a foundational experience in constructing and analyzing matrices with complex eigenvalues. By building a real $2 \\times 2$ matrix that is diagonalizable over the complex numbers $\\mathbb{C}$ but not over the real numbers $\\mathbb{R}$, you will solidify your understanding of how the underlying field affects diagonalizability. This practice [@problem_id:3576903] also introduces the real Schur form, a crucial tool in numerical linear algebra for handling real matrices with complex-conjugate eigenvalues in a stable manner.", "problem": "Let $a \\in \\mathbb{R}$ and $b \\in \\mathbb{R}$ with $b \\neq 0$. Consider the real $2 \\times 2$ matrix\n$$\nB \\;=\\; \\begin{pmatrix} a & b \\\\ -b & a \\end{pmatrix},\n$$\nand let $S \\in \\mathbb{R}^{2 \\times 2}$ be any fixed real invertible matrix, for example\n$$\nS \\;=\\; \\begin{pmatrix} 1 & 2 \\\\ 0 & 1 \\end{pmatrix}.\n$$\nDefine\n$$\nA \\;=\\; S B S^{-1}.\n$$\nStarting from the fundamental definitions of similarity, eigenvalues, diagonalizability over $\\mathbb{R}$ and over $\\mathbb{C}$ (the field of complex numbers), and the real version of the Schur decomposition, do the following:\n\n- Justify that $A$ has eigenvalues $a \\pm b i$, is not diagonalizable over $\\mathbb{R}$, and is diagonalizable over $\\mathbb{C}$.\n- Present the real Schur form of $A$ as a single $2 \\times 2$ rotation-scaling block, explaining why it must have the structure\n$$\n\\begin{pmatrix} a & \\gamma \\\\ -\\gamma & a \\end{pmatrix}\n$$\nfor some $\\gamma > 0$, and identify $\\gamma$ in terms of $a$ and $b$.\n\nFinally, compute the minimal polynomial $m_A(x)$ of $A$ as a single explicit polynomial in $x$ with real coefficients. Provide your final answer as a single closed-form analytic expression for $m_A(x)$. No numerical rounding is required for this problem; do not include any units.", "solution": "The problem statement is formally valid. It is self-contained, mathematically sound, and well-posed. We may proceed with the solution.\n\nThe problem requires an analysis of the matrix $A = S B S^{-1}$, where $a \\in \\mathbb{R}$, $b \\in \\mathbb{R}$ with $b \\neq 0$, $B = \\begin{pmatrix} a & b \\\\ -b & a \\end{pmatrix}$, and $S$ is any real invertible $2 \\times 2$ matrix.\n\nFirst, we address the eigenvalues of $A$ and its diagonalizability over $\\mathbb{R}$ and $\\mathbb{C}$.\n\nBy definition, two matrices $A$ and $B$ are similar if there exists an invertible matrix $S$ such that $A = S B S^{-1}$. A fundamental property of similar matrices is that they share the same characteristic polynomial, and therefore the same eigenvalues. We can demonstrate this by considering the definition of an eigenvalue. Let $\\lambda$ be an eigenvalue of $B$ with a corresponding non-zero eigenvector $v$. Then, $Bv = \\lambda v$. Let us define a vector $w = Sv$. Since $S$ is invertible and $v \\neq 0$, $w$ is also a non-zero vector. We can then compute the action of $A$ on $w$:\n$$\nAw = (SBS^{-1})w = (SBS^{-1})(Sv) = SB(S^{-1}S)v = SBv\n$$\nSubstituting $Bv = \\lambda v$, we obtain:\n$$\nAw = S(\\lambda v) = \\lambda (Sv) = \\lambda w\n$$\nThis shows that $\\lambda$ is also an eigenvalue of $A$ with eigenvector $w$. Thus, the eigenvalues of $A$ are identical to the eigenvalues of $B$.\n\nTo find the eigenvalues of $B$, we compute its characteristic polynomial, $p_B(\\lambda) = \\det(B - \\lambda I)$:\n$$\np_B(\\lambda) = \\det \\begin{pmatrix} a - \\lambda & b \\\\ -b & a - \\lambda \\end{pmatrix} = (a - \\lambda)^2 - (b)(-b) = (a - \\lambda)^2 + b^2\n$$\nThe eigenvalues are the roots of the characteristic equation $p_B(\\lambda) = 0$:\n$$\n(a - \\lambda)^2 + b^2 = 0 \\implies (a - \\lambda)^2 = -b^2 \\implies a - \\lambda = \\pm \\sqrt{-b^2} = \\pm i \\sqrt{b^2} = \\pm i|b|\n$$\nSo, the eigenvalues are $\\lambda = a \\mp i|b|$. The set of eigenvalues is $\\{a + i|b|, a - i|b|\\}$. Since $b \\neq 0$, $|b| \\neq 0$. The problem statement presents the eigenvalues as $a \\pm bi$. This is the same set of eigenvalues regardless of the sign of $b$. We will proceed using this notation. The eigenvalues of $B$, and therefore of $A$, are $\\lambda_1 = a + bi$ and $\\lambda_2 = a - bi$.\n\nNext, we consider the diagonalizability of $A$. A matrix is diagonalizable over a field $\\mathbb{F}$ if it is similar to a diagonal matrix with entries from $\\mathbb{F}$. This is equivalent to stating that there exists a basis of eigenvectors for the underlying vector space.\nA necessary condition for a real matrix to be diagonalizable over the field of real numbers, $\\mathbb{R}$, is that all of its eigenvalues must be real. The eigenvalues of $A$ are $a \\pm bi$. Since we are given that $b \\in \\mathbb{R}$ and $b \\neq 0$, the imaginary part of the eigenvalues is non-zero. Therefore, the eigenvalues are not real. Consequently, the matrix $A$ is not diagonalizable over $\\mathbb{R}$.\n\nNow, we consider diagonalizability over the field of complex numbers, $\\mathbb{C}$. An $n \\times n$ matrix is diagonalizable over $\\mathbb{C}$ if it has $n$ linearly independent eigenvectors. A sufficient condition for this is that the matrix has $n$ distinct eigenvalues. The matrix $A$ is a $2 \\times 2$ matrix. Its eigenvalues are $\\lambda_1 = a + bi$ and $\\lambda_2 = a - bi$. Since $b \\neq 0$, it follows that $bi \\neq -bi$, and thus $\\lambda_1 \\neq \\lambda_2$. Since $A$ is a $2 \\times 2$ matrix with $2$ distinct eigenvalues, it is guaranteed to have $2$ linearly independent eigenvectors. Therefore, $A$ is diagonalizable over $\\mathbb{C}$.\n\nSecond, we analyze the real Schur form of $A$. The real Schur decomposition theorem states that for any real square matrix $A$, there exists a real orthogonal matrix $Q$ (i.e., $Q^T Q = I$) such that $T = Q^T A Q$ is an upper quasi-triangular matrix. This matrix $T$ is called a real Schur form of $A$. Its diagonal blocks are of size $1 \\times 1$ (corresponding to real eigenvalues) or $2 \\times 2$ (corresponding to pairs of complex conjugate eigenvalues).\nSince the matrix $A$ is a $2 \\times 2$ real matrix with a pair of complex conjugate eigenvalues $a \\pm bi$ (where $b \\neq 0$), its real Schur form $T$ must consist of a single $2 \\times 2$ block. This block $T$ is similar to $A$ (since $T = Q^T A Q = Q^{-1} A Q$) and thus must have the same eigenvalues, $a \\pm bi$.\nThe problem proposes that the real Schur form has the structure $C = \\begin{pmatrix} a & \\gamma \\\\ -\\gamma & a \\end{pmatrix}$ for some $\\gamma > 0$. Let's find the eigenvalues of this matrix $C$ by computing its characteristic polynomial:\n$$\n\\det(C - \\lambda I) = \\det \\begin{pmatrix} a - \\lambda & \\gamma \\\\ -\\gamma & a - \\lambda \\end{pmatrix} = (a - \\lambda)^2 + \\gamma^2\n$$\nThe eigenvalues are the roots of $(a - \\lambda)^2 + \\gamma^2 = 0$, which are $\\lambda = a \\pm i\\gamma$.\nFor these eigenvalues to match the eigenvalues of $A$, $a \\pm bi$, we must have $\\{a \\pm i\\gamma\\} = \\{a \\pm bi\\}$. This implies $\\gamma = |b|$. Since the problem specifies that $\\gamma > 0$ and we are given $b \\neq 0$, we have $\\gamma = |b|$. Thus, a real Schur form of $A$ has the specified rotation-scaling structure, with $\\gamma = |b|$.\n\nFinally, we compute the minimal polynomial $m_A(x)$ of $A$. The minimal polynomial of a matrix $A$ is the unique monic polynomial of least degree which, when evaluated at $A$, yields the zero matrix. The roots of the minimal polynomial are the eigenvalues of the matrix.\nThe eigenvalues of $A$ are $\\lambda_1 = a + bi$ and $\\lambda_2 = a - bi$. Therefore, $m_A(x)$ must be divisible by $(x - \\lambda_1)$ and $(x - \\lambda_2)$. Since we require $m_A(x)$ to have real coefficients, it must be divisible by the product:\n$$\n(x - (a + bi))(x - (a - bi)) = ((x-a) - bi)((x-a) + bi) = (x-a)^2 - (bi)^2 = (x-a)^2 + b^2\n$$\nLet this polynomial be $p(x) = x^2 - 2ax + a^2 + b^2$. This is a monic polynomial of degree $2$ with real coefficients.\nThe minimal polynomial $m_A(x)$ must divide the characteristic polynomial $p_A(x)$. We found the characteristic polynomial of $B$ to be $p_B(x) = (x-a)^2 + b^2 = x^2 - 2ax + a^2 + b^2$. Since $A$ and $B$ are similar, $p_A(x) = p_B(x)$.\nThe minimal polynomial $m_A(x)$ must have a degree of at least $2$, because a polynomial of degree $1$ with real coefficients would have a real root, which contradicts the fact that the eigenvalues of $A$ are non-real.\nSince $m_A(x)$ must divide $p_A(x)$ and both have degree $2$, and both are monic, they must be equal. This is also a direct consequence of the eigenvalues of $A$ being distinct. For any matrix with distinct eigenvalues, the minimal polynomial is identical to the characteristic polynomial.\nTherefore, the minimal polynomial of $A$ is:\n$$\nm_A(x) = x^2 - 2ax + a^2 + b^2\n$$\nThis is a single closed-form analytic expression for $m_A(x)$ with real coefficients.", "answer": "$$\n\\boxed{x^{2} - 2ax + a^{2} + b^{2}}\n$$", "id": "3576903"}, {"introduction": "While diagonalizability is a clear-cut concept in theory, numerical practice reveals that matrices 'close' to being non-diagonalizable are fraught with instability. This problem [@problem_id:3576868] explores this phenomenon by analyzing a rank-one perturbation of a defective matrix, which splits a repeated eigenvalue. You will calculate the exact asymptotic behavior of the eigenvector matrix's condition number, revealing how a theoretically 'nice' diagonalizable matrix can be numerically fragile.", "problem": "Consider the rank-$1$ perturbation of a non-normal matrix with a size-$2$ Jordan block. Let\n$$\nA \\in \\mathbb{R}^{3 \\times 3}, \\quad A = \\begin{pmatrix} 0 & 1 & 0 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 3 \\end{pmatrix}, \\quad u = \\begin{pmatrix} 1 \\\\ 2 \\\\ 0 \\end{pmatrix}, \\quad v = \\begin{pmatrix} 3 \\\\ 0 \\\\ 0 \\end{pmatrix},\n$$\nand define, for $\\varepsilon > 0$,\n$$\nA(\\varepsilon) \\coloneqq A + \\varepsilon\\, u v^{\\top}.\n$$\nWork entirely from first principles: use only the definitions of eigenvalues and eigenvectors, similarity diagonalization, and the spectral norm (largest singular value), together with standard algebraic manipulations. Do not assume any prepackaged perturbation theory results, and do not appeal to any unproven asymptotic formulas.\n\nFor all sufficiently small $\\varepsilon > 0$, the matrix $A(\\varepsilon)$ is diagonalizable. Define $S(\\varepsilon) \\in \\mathbb{R}^{3 \\times 3}$ to be the matrix whose columns are the right eigenvectors of $A(\\varepsilon)$ normalized as follows: for the two eigenvectors associated with the two eigenvalues that bifurcate from the defective eigenvalue at $0$, normalize each to have first component equal to $1$, and take the third column to be the standard basis vector $e_{3}$. With this normalization, $S(\\varepsilon)$ is unique and satisfies $S(\\varepsilon)^{-1} A(\\varepsilon) S(\\varepsilon)$ is diagonal.\n\nLet $\\kappa_{2}(S(\\varepsilon))$ denote the spectral-norm condition number of $S(\\varepsilon)$, that is, $\\kappa_{2}(S(\\varepsilon)) \\coloneqq \\|S(\\varepsilon)\\|_{2}\\,\\|S(\\varepsilon)^{-1}\\|_{2}$ where $\\|\\cdot\\|_{2}$ is the operator norm induced by the Euclidean norm.\n\nCompute the exact value of the leading-order coefficient $C$ in the asymptotic scaling of $\\kappa_{2}(S(\\varepsilon))$ as $\\varepsilon \\to 0^{+}$, defined by the limit\n$$\nC \\coloneqq \\lim_{\\varepsilon \\to 0^{+}} \\varepsilon^{1/2}\\, \\kappa_{2}(S(\\varepsilon)).\n$$\nYour final answer must be a single closed-form expression. No rounding is required.", "solution": "The problem asks for the leading-order coefficient $C$ in the asymptotic scaling of the condition number $\\kappa_{2}(S(\\varepsilon))$ of the eigenvector matrix $S(\\varepsilon)$ for a perturbed matrix $A(\\varepsilon)$. The analysis will proceed from first principles.\n\nFirst, we define the matrix $A(\\varepsilon)$. The given matrices are:\n$$\nA = \\begin{pmatrix} 0 & 1 & 0 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 3 \\end{pmatrix}, \\quad u = \\begin{pmatrix} 1 \\\\ 2 \\\\ 0 \\end{pmatrix}, \\quad v = \\begin{pmatrix} 3 \\\\ 0 \\\\ 0 \\end{pmatrix}.\n$$\nThe rank-$1$ perturbation term is\n$$\n\\varepsilon u v^{\\top} = \\varepsilon \\begin{pmatrix} 1 \\\\ 2 \\\\ 0 \\end{pmatrix} \\begin{pmatrix} 3 & 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 3\\varepsilon & 0 & 0 \\\\ 6\\varepsilon & 0 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix}.\n$$\nThus, the perturbed matrix $A(\\varepsilon)$ for $\\varepsilon > 0$ is\n$$\nA(\\varepsilon) = A + \\varepsilon u v^{\\top} = \\begin{pmatrix} 3\\varepsilon & 1 & 0 \\\\ 6\\varepsilon & 0 & 0 \\\\ 0 & 0 & 3 \\end{pmatrix}.\n$$\nTo find the eigenvector matrix $S(\\varepsilon)$, we first need the eigenvalues of $A(\\varepsilon)$. These are the roots of the characteristic equation $\\det(A(\\varepsilon) - \\lambda I) = 0$:\n$$\n\\det \\begin{pmatrix} 3\\varepsilon - \\lambda & 1 & 0 \\\\ 6\\varepsilon & -\\lambda & 0 \\\\ 0 & 0 & 3 - \\lambda \\end{pmatrix} = 0.\n$$\nExpanding the determinant along the third row gives:\n$$\n(3 - \\lambda) \\det \\begin{pmatrix} 3\\varepsilon - \\lambda & 1 \\\\ 6\\varepsilon & -\\lambda \\end{pmatrix} = (3 - \\lambda) \\big( (-\\lambda)(3\\varepsilon - \\lambda) - 6\\varepsilon \\big) = (3 - \\lambda)(\\lambda^2 - 3\\varepsilon\\lambda - 6\\varepsilon) = 0.\n$$\nThe eigenvalues are $\\lambda_3 = 3$ and the two roots of the quadratic equation $\\lambda^2 - 3\\varepsilon\\lambda - 6\\varepsilon = 0$. Using the quadratic formula, these two roots are:\n$$\n\\lambda_{1,2} = \\frac{3\\varepsilon \\pm \\sqrt{(3\\varepsilon)^2 - 4(1)(-6\\varepsilon)}}{2} = \\frac{3\\varepsilon \\pm \\sqrt{9\\varepsilon^2 + 24\\varepsilon}}{2}.\n$$\nFor sufficiently small $\\varepsilon > 0$, the discriminant $9\\varepsilon^2 + 24\\varepsilon$ is positive, so $\\lambda_1$ and $\\lambda_2$ are real and distinct. As $\\varepsilon \\to 0$, both $\\lambda_1$ and $\\lambda_2$ approach $0$, which is the defective eigenvalue of $A$.\n\nNext, we find the corresponding eigenvectors.\nFor $\\lambda_3 = 3$, an eigenvector $x = (x_1, x_2, x_3)^{\\top}$ satisfies $(A(\\varepsilon) - 3I)x = 0$:\n$$\n\\begin{pmatrix} 3\\varepsilon-3 & 1 & 0 \\\\ 6\\varepsilon & -3 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}.\n$$\nFor small $\\varepsilon > 0$, the top-left $2 \\times 2$ submatrix is non-singular, implying $x_1=0$ and $x_2=0$. Thus, the eigenvector is of the form $(0, 0, x_3)^{\\top}$. The problem specifies taking this eigenvector to be the standard basis vector $e_3 = (0, 0, 1)^{\\top}$.\n\nFor an eigenvalue $\\lambda$ from $\\{\\lambda_1, \\lambda_2\\}$, an eigenvector $x = (x_1, x_2, x_3)^{\\top}$ satisfies $(A(\\varepsilon) - \\lambda I)x = 0$:\n$$\n\\begin{pmatrix} 3\\varepsilon - \\lambda & 1 & 0 \\\\ 6\\varepsilon & -\\lambda & 0 \\\\ 0 & 0 & 3 - \\lambda \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}.\n$$\nSince $\\lambda_{1,2} \\to 0$ as $\\varepsilon \\to 0$, for small $\\varepsilon$, $\\lambda \\neq 3$. The third equation $(3-\\lambda)x_3=0$ implies $x_3=0$. The first two equations are:\n$$\n(3\\varepsilon - \\lambda)x_1 + x_2 = 0.\n$$\n$$\n6\\varepsilon x_1 - \\lambda x_2 = 0.\n$$\nThe problem specifies normalizing these two eigenvectors to have a first component equal to $1$. Setting $x_1 = 1$, the first equation gives $x_2 = \\lambda - 3\\varepsilon$. (The second equation $6\\varepsilon - \\lambda(\\lambda-3\\varepsilon) = -\\lambda^2+3\\varepsilon\\lambda+6\\varepsilon = 0$ is satisfied because $\\lambda$ is a root of this polynomial).\nSo, the eigenvectors corresponding to $\\lambda_1$ and $\\lambda_2$ are:\n$$\nx_1(\\varepsilon) = \\begin{pmatrix} 1 \\\\ \\lambda_1 - 3\\varepsilon \\\\ 0 \\end{pmatrix}, \\quad x_2(\\varepsilon) = \\begin{pmatrix} 1 \\\\ \\lambda_2 - 3\\varepsilon \\\\ 0 \\end{pmatrix}.\n$$\nThe matrix of eigenvectors $S(\\varepsilon)$ is formed by these normalized eigenvectors as columns:\n$$\nS(\\varepsilon) = \\begin{pmatrix} 1 & 1 & 0 \\\\ \\lambda_1 - 3\\varepsilon & \\lambda_2 - 3\\varepsilon & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}.\n$$\nTo analyze the condition number, we study the singular values of $S(\\varepsilon)$, which are the square roots of the eigenvalues of $S(\\varepsilon)^{\\top}S(\\varepsilon)$. Let $y_1 = \\lambda_1 - 3\\varepsilon$ and $y_2 = \\lambda_2 - 3\\varepsilon$.\n$$\nS(\\varepsilon)^{\\top}S(\\varepsilon) = \\begin{pmatrix} 1 & y_1 & 0 \\\\ 1 & y_2 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} 1 & 1 & 0 \\\\ y_1 & y_2 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 1+y_1^2 & 1+y_1y_2 & 0 \\\\ 1+y_1y_2 & 1+y_2^2 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}.\n$$\nOne eigenvalue of $S(\\varepsilon)^{\\top}S(\\varepsilon)$ is $1$. The other two, let's call them $\\mu$, are the eigenvalues of the top-left $2 \\times 2$ submatrix $M = \\begin{pmatrix} 1+y_1^2 & 1+y_1y_2 \\\\ 1+y_1y_2 & 1+y_2^2 \\end{pmatrix}$. The characteristic equation is $\\mu^2 - \\text{tr}(M)\\mu + \\det(M) = 0$.\nThe trace is $\\text{tr}(M) = 2 + y_1^2+y_2^2$.\nThe determinant is $\\det(M) = (1+y_1^2)(1+y_2^2) - (1+y_1y_2)^2 = 1+y_1^2+y_2^2+y_1^2y_2^2 - (1+2y_1y_2+y_1^2y_2^2) = y_1^2+y_2^2-2y_1y_2=(y_1-y_2)^2$.\n\nWe need to express these terms in $\\varepsilon$. From Vieta's formulas for $\\lambda^2 - 3\\varepsilon\\lambda - 6\\varepsilon = 0$, we have $\\lambda_1+\\lambda_2 = 3\\varepsilon$ and $\\lambda_1\\lambda_2 = -6\\varepsilon$.\n$y_1+y_2 = (\\lambda_1-3\\varepsilon)+(\\lambda_2-3\\varepsilon) = (\\lambda_1+\\lambda_2)-6\\varepsilon = 3\\varepsilon-6\\varepsilon = -3\\varepsilon$.\n$y_1y_2 = (\\lambda_1-3\\varepsilon)(\\lambda_2-3\\varepsilon) = \\lambda_1\\lambda_2 - 3\\varepsilon(\\lambda_1+\\lambda_2) + 9\\varepsilon^2 = -6\\varepsilon - 3\\varepsilon(3\\varepsilon) + 9\\varepsilon^2 = -6\\varepsilon$.\n$y_1^2+y_2^2 = (y_1+y_2)^2-2y_1y_2 = (-3\\varepsilon)^2 - 2(-6\\varepsilon) = 9\\varepsilon^2+12\\varepsilon$.\n$(y_1-y_2)^2 = (\\lambda_1-\\lambda_2)^2 = (\\lambda_1+\\lambda_2)^2-4\\lambda_1\\lambda_2 = (3\\varepsilon)^2-4(-6\\varepsilon) = 9\\varepsilon^2+24\\varepsilon$.\n\nSo, $\\text{tr}(M) = 2+12\\varepsilon+9\\varepsilon^2$ and $\\det(M) = 24\\varepsilon+9\\varepsilon^2$.\nThe characteristic equation for $\\mu$ is:\n$$\n\\mu^2 - (2+12\\varepsilon+9\\varepsilon^2)\\mu + (24\\varepsilon+9\\varepsilon^2) = 0.\n$$\nLet the roots be $\\mu_{max}(\\varepsilon)$ and $\\mu_{min}(\\varepsilon)$. As $\\varepsilon \\to 0^+$, this equation becomes $\\mu^2 - 2\\mu = 0$, whose roots are $0$ and $2$. Thus, $\\lim_{\\varepsilon\\to 0^+} \\mu_{max}(\\varepsilon) = 2$ and $\\lim_{\\varepsilon\\to 0^+} \\mu_{min}(\\varepsilon) = 0$.\n\nThe singular values of $S(\\varepsilon)$ are $\\sigma_i(\\varepsilon) = \\sqrt{\\mu_i}$, where $\\mu_i$ are the eigenvalues of $S(\\varepsilon)^\\top S(\\varepsilon)$. The three eigenvalues are $1$, $\\mu_{max}(\\varepsilon)$, and $\\mu_{min}(\\varepsilon)$. For small $\\varepsilon > 0$, we have $0 < \\mu_{min}(\\varepsilon) < 1 < \\mu_{max}(\\varepsilon)$. The largest and smallest singular values are $\\sigma_{max}(S(\\varepsilon)) = \\sqrt{\\mu_{max}(\\varepsilon)}$ and $\\sigma_{min}(S(\\varepsilon)) = \\sqrt{\\mu_{min}(\\varepsilon)}$.\n\nThe condition number is $\\kappa_2(S(\\varepsilon)) = \\frac{\\sigma_{max}(S(\\varepsilon))}{\\sigma_{min}(S(\\varepsilon))}$.\nWe have $\\lim_{\\varepsilon\\to 0^+} \\sigma_{max}(S(\\varepsilon)) = \\lim_{\\varepsilon\\to 0^+} \\sqrt{\\mu_{max}(\\varepsilon)} = \\sqrt{2}$.\n\nTo find the asymptotic behavior of $\\sigma_{min}(S(\\varepsilon))$, we examine $\\mu_{min}(\\varepsilon)$. From the quadratic equation for $\\mu$, the product of the roots is the constant term: $\\mu_{max}(\\varepsilon)\\mu_{min}(\\varepsilon) = 24\\varepsilon+9\\varepsilon^2$.\nTherefore, $\\mu_{min}(\\varepsilon) = \\frac{24\\varepsilon+9\\varepsilon^2}{\\mu_{max}(\\varepsilon)}$.\nDividing by $\\varepsilon$ and taking the limit as $\\varepsilon \\to 0^+$:\n$$\n\\lim_{\\varepsilon\\to 0^+} \\frac{\\mu_{min}(\\varepsilon)}{\\varepsilon} = \\lim_{\\varepsilon\\to 0^+} \\frac{24+9\\varepsilon}{\\mu_{max}(\\varepsilon)} = \\frac{24+0}{2} = 12.\n$$\nThis implies that for small $\\varepsilon$, $\\mu_{min}(\\varepsilon) \\sim 12\\varepsilon$.\nSo, $\\sigma_{min}(S(\\varepsilon)) = \\sqrt{\\mu_{min}(\\varepsilon)} \\sim \\sqrt{12\\varepsilon} = 2\\sqrt{3}\\varepsilon^{1/2}$.\n\nNow we can compute the desired limit for the coefficient $C$:\n$$\nC = \\lim_{\\varepsilon \\to 0^{+}} \\varepsilon^{1/2}\\, \\kappa_{2}(S(\\varepsilon)) = \\lim_{\\varepsilon \\to 0^{+}} \\varepsilon^{1/2} \\frac{\\sigma_{max}(S(\\varepsilon))}{\\sigma_{min}(S(\\varepsilon))}.\n$$\nWe can rearrange this as:\n$$\nC = \\frac{\\lim_{\\varepsilon \\to 0^{+}} \\sigma_{max}(S(\\varepsilon))}{\\lim_{\\varepsilon \\to 0^{+}} (\\sigma_{min}(S(\\varepsilon)) / \\varepsilon^{1/2})}.\n$$\nSubstituting the limits we found:\nThe numerator is $\\lim_{\\varepsilon\\to 0^+} \\sigma_{max}(S(\\varepsilon)) = \\sqrt{2}$.\nThe denominator is $\\lim_{\\varepsilon\\to 0^+} \\frac{\\sigma_{min}(S(\\varepsilon))}{\\varepsilon^{1/2}} = \\lim_{\\varepsilon\\to 0^+} \\frac{\\sqrt{12\\varepsilon+O(\\varepsilon^2)}}{\\varepsilon^{1/2}} = \\sqrt{12} = 2\\sqrt{3}$.\n$$\nC = \\frac{\\sqrt{2}}{2\\sqrt{3}} = \\frac{\\sqrt{2}}{2\\sqrt{3}} \\cdot \\frac{\\sqrt{3}}{\\sqrt{3}} = \\frac{\\sqrt{6}}{6}.\n$$\nThe value of the leading-order coefficient is $\\frac{\\sqrt{6}}{6}$.", "answer": "$$\\boxed{\\frac{\\sqrt{6}}{6}}$$", "id": "3576868"}, {"introduction": "Distinguishing a truly defective matrix from a nearly-defective one is a fundamental challenge in numerical computation. This exercise [@problem_id:3576934] guides you to implement a robust test for non-diagonalizability that does not require computing the Jordan form itself. By calculating the dimensions of the null spaces of powers of $(A - \\lambda I)$, you will compute a similarity-invariant signature that definitively reveals the matrix's Jordan structure and highlights the critical role of numerical tolerance in the process.", "problem": "You are to design and implement a program that constructs pairs of square matrices with identical spectra (multisets of eigenvalues) but with different Jordan structures, then numerically distinguishes their similarity-invariant properties without computing any Jordan normal form. Your program must demonstrate the use of invariants preserved under similarity transformations to detect non-diagonalizability and to distinguish matrices that are not similar despite having the same spectrum.\n\nFundamental base: Use the core definitions of similarity transformations, eigenvalues and eigenvectors, algebraic multiplicity, geometric multiplicity, rank, nullity, and the Jordan normal form. The Jordan normal form exists for any square matrix over the complex numbers, and similarity invariants include the spectrum with algebraic multiplicities, the minimal polynomial, and the dimension of kernels of powers of the nilpotent part associated with each eigenvalue. Explicitly, for an eigenvalue $\\lambda$ and a matrix $A$, the sequence of dimensions of kernels $\\dim \\ker (A - \\lambda I)^k$ for $k = 1, 2, \\dots, m$ (where $m$ is the algebraic multiplicity of $\\lambda$) is a similarity invariant that encodes the sizes and counts of Jordan blocks for $\\lambda$. A matrix is diagonalizable over the complex numbers if and only if for every eigenvalue $\\lambda$, the dimension of $\\ker (A - \\lambda I)$ equals the algebraic multiplicity of $\\lambda$.\n\nTasks:\n1. Construct for each test case a pair of matrices $(A,B)$ of the same size with the same spectrum but different Jordan structures. Choose real matrices so that eigenvalues are real, and construct at least one defective matrix (not diagonalizable) and its diagonalizable counterpart with the same spectrum.\n2. Implement numerical procedures to compute the following similarity-invariant quantities without forming any Jordan normal form:\n   - Cluster the eigenvalues of a matrix $A$ into groups of equal eigenvalues using a numerical tolerance $\\tau_{\\mathrm{eig}}$ to define equality (so eigenvalues within $\\tau_{\\mathrm{eig}}$ are considered the same). For each cluster representative $\\lambda$ with algebraic multiplicity $m$, compute the chain of nullities $\\nu_k = \\dim \\ker (A - \\lambda I)^k$ for $k = 1,2,\\dots,m$ using numerical rank via singular value decomposition. Use an absolute rank tolerance $\\tau_{\\mathrm{rank}}$, i.e., consider a singular value $s$ as zero if $s \\le \\tau_{\\mathrm{rank}}$. The nullity is $n - \\mathrm{rank}$, where $n$ is the matrix size.\n   - Decide whether $A$ is diagonalizable over the complex numbers by verifying the criterion $\\dim \\ker (A - \\lambda I) = m$ for every clustered eigenvalue $\\lambda$ with algebraic multiplicity $m$.\n3. Apply the above computations under two different rank tolerances to highlight numerical sensitivity:\n   - A strict tolerance $\\tau_{\\mathrm{rank}}^{\\mathrm{strict}} = 10^{-14}$.\n   - A relaxed tolerance $\\tau_{\\mathrm{rank}}^{\\mathrm{relaxed}} = 10^{-8}$.\n   Use $\\tau_{\\mathrm{eig}} = 10^{-12}$ for eigenvalue clustering in all runs.\n4. For each test case $(A,B)$, output six items:\n   - The boolean diagonalizability of $A$ under $\\tau_{\\mathrm{rank}}^{\\mathrm{strict}}$.\n   - The boolean diagonalizability of $B$ under $\\tau_{\\mathrm{rank}}^{\\mathrm{strict}}$.\n   - The boolean diagonalizability of $A$ under $\\tau_{\\mathrm{rank}}^{\\mathrm{relaxed}}$.\n   - The boolean diagonalizability of $B$ under $\\tau_{\\mathrm{rank}}^{\\mathrm{relaxed}}$.\n   - The similarity-invariant signature of $A$ under $\\tau_{\\mathrm{rank}}^{\\mathrm{strict}}$, defined as a list of pairs $[\\lambda, [\\nu_1,\\dots,\\nu_m]]$ over all eigenvalue clusters, with each $\\lambda$ represented as a real float and each $\\nu_k$ as an integer.\n   - The similarity-invariant signature of $B$ under $\\tau_{\\mathrm{rank}}^{\\mathrm{strict}}$, in the same format.\n\nTest suite:\n- Case 1 (happy path, small $3\\times 3$): \n  - $A_1 = \\begin{bmatrix} 1 & 1 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 2 \\end{bmatrix}$ (defective with a size-$2$ Jordan block at $\\lambda=1$).\n  - $B_1 = \\mathrm{diag}(1,1,2)$ (diagonalizable).\n- Case 2 (coverage of different eigenvalue with algebraic multiplicity $2$, size $4\\times 4$):\n  - $A_2 = \\begin{bmatrix} 0 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 2 \\end{bmatrix}$ (defective with a size-$2$ Jordan block at $\\lambda=0$).\n  - $B_2 = \\mathrm{diag}(0,0,1,2)$ (diagonalizable).\n- Case 3 (boundary case, near-defective, size $2\\times 2$):\n  - $A_3 = \\begin{bmatrix} 1 & \\epsilon \\\\ 0 & 1 \\end{bmatrix}$ with $\\epsilon = 10^{-12}$ (defective, but numerically delicate).\n  - $B_3 = \\mathrm{diag}(1,1)$ (diagonalizable).\n\nAngle units are not applicable, and no physical units are involved. All outputs must be booleans, integers, floats, or lists of these.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, one item per test case, where each item is itself a list of the six elements described above. Concretely, the output must be a Python-like literal of the form\n$[[d_{A_1}^{\\mathrm{strict}}, d_{B_1}^{\\mathrm{strict}}, d_{A_1}^{\\mathrm{relaxed}}, d_{B_1}^{\\mathrm{relaxed}}, \\mathrm{sig}(A_1), \\mathrm{sig}(B_1)], [d_{A_2}^{\\mathrm{strict}}, d_{B_2}^{\\mathrm{strict}}, d_{A_2}^{\\mathrm{relaxed}}, d_{B_2}^{\\mathrm{relaxed}}, \\mathrm{sig}(A_2), \\mathrm{sig}(B_2)], [d_{A_3}^{\\mathrm{strict}}, d_{B_3}^{\\mathrm{strict}}, d_{A_3}^{\\mathrm{relaxed}}, d_{B_3}^{\\mathrm{relaxed}}, \\mathrm{sig}(A_3), \\mathrm{sig}(B_3)]]$, where each $\\mathrm{sig}(A)$ is a list of $[\\lambda, [\\nu_1,\\dots,\\nu_m]]$ pairs. No additional text should be printed.", "solution": "The problem requires the design of a numerical procedure to distinguish between square matrices that share the same spectrum (multiset of eigenvalues) but are not similar. This distinction is to be made without explicitly computing the Jordan Normal Form (JNF). The core of the task lies in computing and comparing a more refined set of similarity invariants.\n\nA similarity transformation maps a square matrix $A$ to $P^{-1}AP$ for some invertible matrix $P$. Quantities that remain unchanged under all such transformations are called similarity invariants. The most common invariant is the spectrum, but as the problem stipulates, it is not a complete invariant. Two matrices can have identical spectra but different geometric structures, making them non-similar.\n\nThe complete classification of matrices under similarity is provided by the Jordan Normal Form. The JNF theorem states that any square matrix $A \\in \\mathbb{C}^{n \\times n}$ is similar to a block diagonal matrix $J = \\mathrm{diag}(J_1, J_2, \\ldots, J_p)$, where each $J_i$ is a Jordan block. A Jordan block for an eigenvalue $\\lambda$ of size $k$ is an upper bidiagonal matrix of the form:\n$$ J_k(\\lambda) = \\begin{bmatrix} \\lambda & 1 & & \\\\ & \\lambda & \\ddots & \\\\ & & \\ddots & 1 \\\\ & & & \\lambda \\end{bmatrix} \\in \\mathbb{C}^{k \\times k} $$\nTwo matrices are similar if and only if they have the same Jordan Normal Form, up to a permutation of the Jordan blocks. This means they must have the same eigenvalues, and for each eigenvalue $\\lambda$, the number and sizes of its corresponding Jordan blocks must be identical.\n\nA matrix is diagonalizable if and only if all its Jordan blocks are of size $1 \\times 1$. In this case, its JNF is a diagonal matrix.\n\nThe problem is to determine these structural properties—the counts and sizes of Jordan blocks—without computing $J$ or the similarity transformation $P$. This is achieved by analyzing the kernels of powers of the matrix $T_\\lambda = A - \\lambda I$. The dimensions of these kernels are similarity invariants. Let $\\lambda$ be an eigenvalue of $A$ with algebraic multiplicity $m$ (i.e., it is a root of the characteristic polynomial with multiplicity $m$). We define the sequence of nullities $\\nu_k$ as:\n$$ \\nu_k = \\dim \\ker( (A - \\lambda I)^k ) \\quad \\text{for } k = 1, 2, \\ldots, m $$\nThis sequence of integers encodes the entire Jordan structure associated with $\\lambda$. Specifically:\n- The number of Jordan blocks for $\\lambda$ is $\\nu_1$. This is the geometric multiplicity of $\\lambda$.\n- The number of Jordan blocks of size at least $k$ is given by the difference $\\nu_k - \\nu_{k-1}$ (with $\\nu_0 = 0$).\n- The sequence $(\\nu_1, \\nu_2, \\ldots, \\nu_m)$ is non-decreasing and stabilizes at $\\nu_p = \\nu_{p+1} = \\ldots = m$, where $p$ is the size of the largest Jordan block for $\\lambda$.\n\nFrom this, a matrix $A$ is diagonalizable if and only if for every eigenvalue $\\lambda$ with algebraic multiplicity $m$, its geometric multiplicity $\\nu_1$ is equal to $m$. If $\\nu_1 = m$, it implies that there are $m$ Jordan blocks, and since their sizes must sum to $m$, each block must be of size $1$.\n\nThe algorithmic procedure to solve the problem is as follows:\n\n1.  **Eigenvalue Computation and Clustering**: For a given matrix $A$, we first compute its eigenvalues using a standard numerical algorithm, such as the QR algorithm, provided by `numpy.linalg.eigvals`. Due to floating-point arithmetic, eigenvalues that are theoretically identical may be computed as a cluster of close numbers. We group these numerical eigenvalues using a tolerance $\\tau_{\\mathrm{eig}}$. For each cluster, we calculate the representative eigenvalue $\\lambda$ (e.g., the mean) and its algebraic multiplicity $m$ (the size of the cluster).\n\n2.  **Nullity Chain Computation**: For each distinct eigenvalue $\\lambda$ with multiplicity $m$, we compute the nullity chain $\\nu_1, \\nu_2, \\ldots, \\nu_m$. The nullity of a matrix $M$ is given by $\\mathrm{nullity}(M) = n - \\mathrm{rank}(M)$, where $n$ is the dimension of the matrix. The rank of the matrix $(A - \\lambda I)^k$ must be computed numerically.\n\n3.  **Numerical Rank Calculation**: Computing rank for a matrix with floating-point entries is an ill-posed problem. The most robust method is to use the Singular Value Decomposition (SVD). For any matrix $M$, its rank is the number of non-zero singular values. Numerically, we count the number of singular values $s_i$ that are greater than a specified absolute tolerance, $\\tau_{\\mathrm{rank}}$.\n    $$ \\mathrm{rank}(M) \\approx |\\{ s_i \\mid s_i > \\tau_{\\mathrm{rank}} \\}| $$\n    The choice of $\\tau_{\\mathrm{rank}}$ is critical and can affect the result, especially for \"near-defective\" matrices, as explored in the problem.\n\n4.  **Diagonalizability Check**: For each eigenvalue $\\lambda$ with algebraic multiplicity $m$, we check if $\\nu_1 = m$. The matrix $A$ is declared diagonalizable if and only if this condition holds for all its distinct eigenvalues.\n\n5.  **Signature Generation**: The similarity-invariant signature for $A$ is constructed as a list of pairs, where each pair consists of a representative eigenvalue $\\lambda$ and its corresponding nullity chain $[\\nu_1, \\ldots, \\nu_m]$. This signature uniquely characterizes the JNF of $A$. Two matrices are similar if and only if their signatures are identical (up to ordering of eigenvalues).\n\nThe problem requires performing this analysis with two different rank tolerances, $\\tau_{\\mathrm{rank}}^{\\mathrm{strict}} = 10^{-14}$ and $\\tau_{\\mathrm{rank}}^{\\mathrm{relaxed}} = 10^{-8}$, to demonstrate the sensitivity of numerical rank and, consequently, the determination of Jordan structure. For a matrix like $A_3 = \\begin{bmatrix} 1 & \\epsilon \\\\ 0 & 1 \\end{bmatrix}$ with a very small $\\epsilon=10^{-12}$, the matrix $A_3-I$ has a singular value of $10^{-12}$. The strict tolerance correctly identifies this as non-zero, revealing a rank of $1$ and hence defectiveness. The relaxed tolerance incorrectly classifies this singular value as zero, leading to an apparent rank of $0$ and a misleading conclusion of diagonalizability. This highlights a fundamental challenge in numerical linear algebra.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to construct and analyze matrix pairs.\n    \"\"\"\n\n    def analyze_matrix(A, tau_eig, tau_rank):\n        \"\"\"\n        Computes the diagonalizability and similarity-invariant signature of a matrix.\n\n        Args:\n            A (np.ndarray): The input square matrix.\n            tau_eig (float): Tolerance for clustering eigenvalues.\n            tau_rank (float): Tolerance for numerical rank calculation via SVD.\n\n        Returns:\n            tuple: A tuple containing:\n                - is_diagonalizable (bool): True if the matrix is numerically diagonalizable.\n                - signature (list): A list of [eigenvalue, nullity_chain] pairs.\n        \"\"\"\n        n = A.shape[0]\n        if n == 0:\n            return True, []\n        \n        try:\n            eigs = np.linalg.eigvals(A)\n        except np.linalg.LinAlgError:\n            # Handle cases where eigenvalue computation fails, though unlikely for test cases.\n            return False, []\n\n        # Sort eigenvalues to facilitate clustering\n        eigs = sorted(eigs, key=lambda x: (x.real, x.imag))\n\n        # Cluster eigenvalues\n        clusters = []\n        i = 0\n        while i  len(eigs):\n            j = i + 1\n            while j  len(eigs) and abs(eigs[j] - eigs[i]) = tau_eig:\n                j += 1\n            \n            cluster_eigs = eigs[i:j]\n            lambda_rep = np.mean(cluster_eigs)\n            m = len(cluster_eigs)\n            clusters.append((lambda_rep, m))\n            i = j\n        \n        is_diagonalizable = True\n        signature = []\n\n        I = np.eye(n)\n\n        for lambda_val, m in clusters:\n            nullity_chain = []\n            \n            # Compute T = A - lambda*I\n            T = A - lambda_val * I\n            \n            # Use np.linalg.matrix_power for T^k\n            Tk = np.copy(T)\n            for k in range(1, m + 1):\n                if k > 1:\n                    Tk = Tk @ T\n                \n                # Compute numerical rank using SVD\n                s = np.linalg.svd(Tk, compute_uv=False)\n                rank_k = np.sum(s > tau_rank)\n                nullity_k = n - rank_k\n                nullity_chain.append(int(nullity_k))\n            \n            # Check diagonalizability condition for this eigenvalue\n            # Geometric multiplicity (nu_1) must equal algebraic multiplicity (m)\n            if nullity_chain[0] != m:\n                is_diagonalizable = False\n\n            signature.append([lambda_val.real, nullity_chain])\n\n        # Sort signature by eigenvalue for deterministic output\n        signature.sort(key=lambda x: x[0])\n\n        return is_diagonalizable, signature\n\n    # Define tolerances\n    tau_eig = 1e-12\n    tau_rank_strict = 1e-14\n    tau_rank_relaxed = 1e-8\n    \n    # Define test cases\n    A1 = np.array([[1, 1, 0], [0, 1, 0], [0, 0, 2]], dtype=float)\n    B1 = np.diag([1, 1, 2]).astype(float)\n    \n    A2 = np.array([[0, 1, 0, 0], [0, 0, 0, 0], [0, 0, 1, 0], [0, 0, 0, 2]], dtype=float)\n    B2 = np.diag([0, 0, 1, 2]).astype(float)\n    \n    epsilon = 1e-12\n    A3 = np.array([[1, epsilon], [0, 1]], dtype=float)\n    B3 = np.diag([1, 1]).astype(float)\n\n    test_cases = [\n        (A1, B1),\n        (A2, B2),\n        (A3, B3)\n    ]\n\n    all_results = []\n    for A, B in test_cases:\n        # Analyze with strict tolerance\n        d_A_strict, sig_A = analyze_matrix(A, tau_eig, tau_rank_strict)\n        d_B_strict, sig_B = analyze_matrix(B, tau_eig, tau_rank_strict)\n\n        # Analyze with relaxed tolerance (only need diagonalizability)\n        d_A_relaxed, _ = analyze_matrix(A, tau_eig, tau_rank_relaxed)\n        d_B_relaxed, _ = analyze_matrix(B, tau_eig, tau_rank_relaxed)\n\n        case_result = [\n            d_A_strict, d_B_strict,\n            d_A_relaxed, d_B_relaxed,\n            sig_A, sig_B\n        ]\n        all_results.append(case_result)\n\n    # Format the final output string to match the required Python literal format\n    # Using a direct mapping to string handles booleans, floats, and lists correctly.\n    result_str = str(all_results).replace(\"'\", '\"')\n    \n    print(result_str)\n\nsolve()\n```", "id": "3576934"}]}