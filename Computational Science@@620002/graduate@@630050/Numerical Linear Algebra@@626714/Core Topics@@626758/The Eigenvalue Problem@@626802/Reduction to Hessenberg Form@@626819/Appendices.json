{"hands_on_practices": [{"introduction": "To truly master an algorithm, we must first understand its fundamental building blocks. The reduction to Hessenberg form is a sequence of carefully chosen reflections, and this practice homes in on the very first one. By symbolically deriving the Householder reflector for a small matrix [@problem_id:3572572], you will gain a concrete understanding of how these transformations introduce zeros and why the process is constructed to be numerically stable.", "problem": "Let $A \\in \\mathbb{R}^{4 \\times 4}$ be a real matrix with generic entries $a_{ij}$. Consider the reduction of $A$ to upper Hessenberg form using a similarity transformation built from a Householder reflector that acts on the trailing $3 \\times 3$ subspace associated with rows and columns $2$ through $4$. Let $x \\in \\mathbb{R}^{3}$ denote the subvector formed from the first column of $A$ below the $(1,1)$ entry, namely $x = \\begin{pmatrix} a_{21} \\\\ a_{31} \\\\ a_{41} \\end{pmatrix}$. Assume that at least one of $a_{21}$, $a_{31}$, $a_{41}$ is nonzero so that $x \\neq 0$.\n\nA Householder reflector is a real orthogonal and symmetric matrix of the form $H = I - 2 \\dfrac{v v^{\\top}}{v^{\\top} v}$ for some nonzero $v \\in \\mathbb{R}^{m}$, where $I$ is the identity matrix of the appropriate dimension. Such a reflector maps a given vector to a multiple of a coordinate vector by a reflection across the hyperplane orthogonal to $v$.\n\nConstruct a $3 \\times 3$ Householder reflector $Q$ that maps $x$ to a vector collinear with the first basis vector $e_{1} \\in \\mathbb{R}^{3}$. Lift $Q$ to an orthogonal $4 \\times 4$ similarity factor $H_{1} = \\operatorname{diag}(1, Q)$ and form the similarity transform $A^{(1)} = H_{1} A H_{1}^{\\top}$. Using only the definition of a Householder reflector, derive $Q$ symbolically and explain why the entries $(3,1)$ and $(4,1)$ of $A^{(1)}$ are zero, thereby exhibiting the updated Hessenberg pattern in the first column.\n\nAdopt the numerically stable sign convention that chooses the target multiple of $e_{1}$ to avoid subtractive cancellation when constructing the reflector from $x$. Express the final answer below as a single closed-form analytic expression for the scalar $\\tau$ such that $Q = I - \\tau u u^{\\top}$ with $u = x - \\alpha e_{1}$ and $\\alpha$ chosen according to the aforementioned stable convention. Your final expression must be given explicitly in terms of $a_{21}$, $a_{31}$, and $a_{41}$ only. No rounding is required.", "solution": "The problem requires the derivation of a scalar $\\tau$ used in a Householder reflection for the first step of reducing a $4 \\times 4$ matrix $A$ to upper Hessenberg form. It also requires an explanation for why this transformation introduces zeros in the correct positions of the first column.\n\nLet $A \\in \\mathbb{R}^{4 \\times 4}$ be a matrix with entries $a_{ij}$. The goal is to find a similarity transformation that introduces a zero at positions $(3,1)$ and $(4,1)$. This is the first step in creating an upper Hessenberg matrix, which has zeros below the first subdiagonal. The transformation matrix is constructed as $H_1 = \\begin{pmatrix} 1 & 0 \\\\ 0 & Q \\end{pmatrix}$, where $Q \\in \\mathbb{R}^{3 \\times 3}$ is a Householder reflector.\n\nThe vector to be transformed is the subvector of the first column of $A$ below the diagonal, which is given as $x = \\begin{pmatrix} a_{21} \\\\ a_{31} \\\\ a_{41} \\end{pmatrix} \\in \\mathbb{R}^{3}$. The problem assumes $x \\neq 0$. The Householder reflector $Q$ must map $x$ to a vector collinear with the first standard basis vector $e_1 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} \\in \\mathbb{R}^{3}$. That is, $Qx = \\alpha e_1$ for some scalar $\\alpha \\in \\mathbb{R}$.\n\nA Householder reflector is an orthogonal matrix. Orthogonal transformations preserve the Euclidean norm ($L^2$ norm). Therefore, we must have $\\|Qx\\|_2 = \\|x\\|_2$.\nApplying this property:\n$\\|\\alpha e_1\\|_2 = \\|x\\|_2$\n$|\\alpha| \\|e_1\\|_2 = \\|x\\|_2$\nSince $\\|e_1\\|_2 = 1$, we get $|\\alpha| = \\|x\\|_2$.\nThe norm of $x$ is given by $\\|x\\|_2 = \\sqrt{a_{21}^2 + a_{31}^2 + a_{41}^2}$.\nThus, $\\alpha = \\pm \\sqrt{a_{21}^2 + a_{31}^2 + a_{41}^2}$.\n\nThe problem specifies using the numerically stable sign convention. The reflector vector is constructed from $x$ and its target $\\alpha e_1$. The reflector vector is $u = x - \\alpha e_1$. To avoid subtractive cancellation, which can lead to loss of precision when two nearly equal numbers are subtracted, the sign of $\\alpha$ is chosen to be the opposite of the sign of the first component of $x$, which is $a_{21}$. This ensures that the first component of $u$, $u_1 = a_{21} - \\alpha$, involves an addition of quantities with the same sign (or subtraction of quantities with opposite signs).\nThe stable choice for $\\alpha$ is:\n$\\alpha = -\\operatorname{sgn}(a_{21}) \\|x\\|_2 = -\\operatorname{sgn}(a_{21}) \\sqrt{a_{21}^2 + a_{31}^2 + a_{41}^2}$.\nHere, $\\operatorname{sgn}(z)$ is the sign function. If $a_{21} = 0$, the sign choice is arbitrary; a common convention is to set $\\operatorname{sgn}(0) = 1$.\n\nThe Householder reflector $Q$ that maps $x$ to $\\alpha e_1$ is given by the formula $Q = I - 2 \\frac{v v^{\\top}}{v^{\\top} v}$, where the reflection vector is $v = x - \\alpha e_1$. In the problem's notation, this vector is denoted by $u$. So, we have $u = x - \\alpha e_1$.\nThe problem defines $Q$ as $Q = I - \\tau u u^{\\top}$. Comparing this with the standard definition, we can identify $\\tau = \\frac{2}{u^{\\top} u}$.\n\nWe now compute $u^{\\top} u = \\|u\\|_2^2$:\n$u^{\\top} u = (x - \\alpha e_1)^{\\top} (x - \\alpha e_1) = x^{\\top}x - 2\\alpha x^{\\top}e_1 + \\alpha^2 e_1^{\\top}e_1$.\nWe have:\n- $x^{\\top}x = \\|x\\|_2^2 = a_{21}^2 + a_{31}^2 + a_{41}^2$.\n- $x^{\\top}e_1 = a_{21}$.\n- $e_1^{\\top}e_1 = \\|e_1\\|_2^2 = 1$.\n- $\\alpha^2 = (\\|x\\|_2)^2 = \\|x\\|_2^2$.\n\nSubstituting these into the expression for $u^{\\top} u$:\n$u^{\\top} u = \\|x\\|_2^2 - 2\\alpha a_{21} + \\|x\\|_2^2 = 2\\|x\\|_2^2 - 2\\alpha a_{21}$.\nNow, we substitute the expression for $\\alpha$:\n$u^{\\top} u = 2\\|x\\|_2^2 - 2(-\\operatorname{sgn}(a_{21}) \\|x\\|_2) a_{21} = 2\\|x\\|_2^2 + 2(\\operatorname{sgn}(a_{21})a_{21}) \\|x\\|_2$.\nSince $\\operatorname{sgn}(z)z = |z|$, this simplifies to:\n$u^{\\top} u = 2\\|x\\|_2^2 + 2|a_{21}| \\|x\\|_2 = 2\\|x\\|_2 (\\|x\\|_2 + |a_{21}|)$.\n\nNow we can find $\\tau$:\n$\\tau = \\frac{2}{u^{\\top} u} = \\frac{2}{2\\|x\\|_2 (\\|x\\|_2 + |a_{21}|)} = \\frac{1}{\\|x\\|_2 (\\|x\\|_2 + |a_{21}|)}$.\n\nSubstituting $\\|x\\|_2 = \\sqrt{a_{21}^2 + a_{31}^2 + a_{41}^2}$, we obtain the final expression for $\\tau$ in terms of the matrix entries:\n$\\tau = \\frac{1}{\\sqrt{a_{21}^2 + a_{31}^2 + a_{41}^2} \\left( \\sqrt{a_{21}^2 + a_{31}^2 + a_{41}^2} + |a_{21}| \\right)}$.\n\nNext, we must explain why the entries $(3,1)$ and $(4,1)$ of $A^{(1)} = H_1 A H_1^{\\top}$ are zero. Since $H_1$ is a Householder-based matrix, it is symmetric, so $H_1^{\\top} = H_1$. Thus, $A^{(1)} = H_1 A H_1$.\nWe focus on the first column of $A^{(1)}$, which is given by $A^{(1)} e_1^{(4)}$, where $e_1^{(4)} = \\begin{pmatrix} 1 & 0 & 0 & 0 \\end{pmatrix}^\\top$.\nThe first column of $A^{(1)}$ is $H_1 A H_1 e_1^{(4)}$.\nFirst, we evaluate $H_1 e_1^{(4)}$:\n$H_1 e_1^{(4)} = \\begin{pmatrix} 1 & 0 \\\\ 0 & Q \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = e_1^{(4)}$.\nSo, the first column of $A^{(1)}$ is simply $H_1 (A e_1^{(4)})$.\n$A e_1^{(4)}$ is the first column of $A$: $A e_1^{(4)} = \\begin{pmatrix} a_{11} \\\\ a_{21} \\\\ a_{31} \\\\ a_{41} \\end{pmatrix} = \\begin{pmatrix} a_{11} \\\\ x \\end{pmatrix}$.\nNow, we apply $H_1$:\nFirst column of $A^{(1)} = H_1 \\begin{pmatrix} a_{11} \\\\ x \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 0 & Q \\end{pmatrix} \\begin{pmatrix} a_{11} \\\\ x \\end{pmatrix} = \\begin{pmatrix} a_{11} \\\\ Qx \\end{pmatrix}$.\nBy construction, the reflector $Q$ was designed such that $Qx = \\alpha e_1^{(3)}$, where $e_1^{(3)} = \\begin{pmatrix} 1 & 0 & 0 \\end{pmatrix}^\\top \\in \\mathbb{R}^3$.\nTherefore, the first column of the transformed matrix $A^{(1)}$ is:\n$\\begin{pmatrix} a_{11} \\\\ \\alpha e_1^{(3)} \\end{pmatrix} = \\begin{pmatrix} a_{11} \\\\ \\alpha \\\\ 0 \\\\ 0 \\end{pmatrix}$.\nThe $(3,1)$ entry of $A^{(1)}$ is the third element of this column vector, which is $0$.\nThe $(4,1)$ entry of $A^{(1)}$ is the fourth element of this column vector, which is $0$.\nThis confirms that the transformation successfully introduces zeros below the subdiagonal in the first column, which is the objective of this step in the Hessenberg reduction. The same procedure would then be applied to the submatrix corresponding to columns and rows $3$ through $4$ to zero out the $(4,2)$ entry, completing the reduction for a $4 \\times 4$ matrix.\n\nThe final requested quantity is the expression for $\\tau$.\n$\\tau = \\frac{1}{\\sqrt{a_{21}^2 + a_{31}^2 + a_{41}^2} \\left( \\sqrt{a_{21}^2 + a_{31}^2 + a_{41}^2} + |a_{21}| \\right)}$.", "answer": "$$\n\\boxed{\\frac{1}{\\sqrt{a_{21}^{2} + a_{31}^{2} + a_{41}^{2}} \\left( \\sqrt{a_{21}^{2} + a_{31}^{2} + a_{41}^{2}} + |a_{21}| \\right)}}\n$$", "id": "3572572"}, {"introduction": "After mastering the mechanics of a single step, the next logical question is about the efficiency of the entire process. For large matrices, the computational cost determines an algorithm's feasibility, and understanding this cost is a key skill for a numerical analyst. This exercise [@problem_id:3572651] guides you through summing the work done at each stage of the reduction to find the algorithm's overall complexity, revealing why Hessenberg reduction is a vital preliminary step for eigenvalue problems.", "problem": "Let $A \\in \\mathbb{R}^{n \\times n}$ be a dense real matrix. Consider the standard unblocked reduction to upper Hessenberg form using Householder reflectors, that is, a similarity transformation $A \\leftarrow H_{n-2} \\cdots H_{2} H_{1} \\, A \\, H_{1} H_{2} \\cdots H_{n-2}$ where, for each iteration index $k$ with $1 \\leq k \\leq n-2$, the Householder reflector $H_{k} = I - \\tau_{k} \\tilde{v}_{k} \\tilde{v}_{k}^{\\mathsf{T}}$ is constructed to annihilate the entries $A_{k+2:n,\\,k}$ below the first subdiagonal, with $\\tilde{v}_{k} \\in \\mathbb{R}^{n}$ equal to a zero padding of a vector $v_{k} \\in \\mathbb{R}^{n-k}$ into positions $k+1$ through $n$. Assume real arithmetic and define a floating-point operation (flop) as a single scalar addition or multiplication; scalar square roots and divisions are not to be counted toward the asymptotic cubic term.\n\nStarting only from the following foundational facts:\n- A dense matrix-vector product of an $a \\times b$ matrix with a $b \\times 1$ vector costs $2ab$ flops in real arithmetic.\n- A rank-one update of an $a \\times b$ matrix by $u v^{\\mathsf{T}}$ costs $2ab$ flops in real arithmetic.\n- In iteration $k$, the left application $A \\leftarrow H_{k} A$ acts nontrivially on the trailing submatrix of $A$ comprising rows $k+1$ through $n$ and columns $k$ through $n$, while the right application $A \\leftarrow A H_{k}$ acts nontrivially on the trailing block of columns $k+1$ through $n$ and all rows.\n\nDerive, by summing the per-iteration costs over $k = 1, \\dots, n-2$, the leading-order term in the total flop count for this unblocked Hessenberg reduction. Your reasoning should explicitly identify the sizes of the affected submatrices at each step and use only the operation counts stated above. Ignore all lower-order terms that do not contribute to the coefficient of $n^{3}$.\n\nProvide your final answer as a single closed-form expression in $n$ equal to the leading-order term in flops. Do not include big-$\\mathcal{O}$ notation in the final answer. No rounding is required.", "solution": "The user wants to find the leading-order term in the total floating-point operation (flop) count for the unblocked reduction of a dense matrix $A \\in \\mathbb{R}^{n \\times n}$ to upper Hessenberg form using Householder reflectors. The analysis will proceed by calculating the cost of a single iteration $k$ and summing these costs for $k$ from $1$ to $n-2$. A flop is defined as a single scalar addition or multiplication.\n\nThe transformation at iteration $k$, where $1 \\le k \\le n-2$, is given by $A \\leftarrow H_k A H_k$, where $H_k = I - \\tau_k \\tilde{v}_k \\tilde{v}_k^{\\mathsf{T}}$. The vector $\\tilde{v}_k \\in \\mathbb{R}^n$ has its first $k$ components equal to zero. Let $v_k \\in \\mathbb{R}^{n-k}$ denote the subvector of $\\tilde{v}_k$ containing the non-zero entries from position $k+1$ to $n$.\n\nThe total cost for iteration $k$, denoted by $C_k$, is the sum of the costs of the left multiplication ($A \\leftarrow H_k A$) and the right multiplication ($A \\leftarrow A H_k$).\n\n**1. Cost of the left multiplication, $A \\leftarrow H_k A$**\n\nThe left multiplication by $H_k$ is given by $A \\leftarrow (I - \\tau_k \\tilde{v}_k \\tilde{v}_k^{\\mathsf{T}})A$. This operation only affects rows $k+1$ through $n$ of matrix $A$. At the start of iteration $k$, the columns $1, \\dots, k-1$ of $A$ already have the desired Hessenberg structure, meaning entries $A_{i,j}$ are zero for $i > j+1$ and $j  k$. In particular, for any column $j  k$, the submatrix $A(k+1:n, j)$ is zero. A linear combination of these zero rows (which is what $H_k A$ computes for these columns) will remain zero. Therefore, the update only nontrivially affects columns $k$ through $n$.\n\nAs stated in the problem, this operation acts on the submatrix of $A$ comprising rows $k+1$ through $n$ and columns $k$ through $n$. Let us denote this submatrix as $A_{\\text{sub},L} = A(k+1:n, k:n)$. The size of $A_{\\text{sub},L}$ is $(n-k) \\times (n-k+1)$. The transformation on this submatrix is $A_{\\text{sub},L} \\leftarrow (I_{n-k} - \\tau_k v_k v_k^{\\mathsf{T}}) A_{\\text{sub},L}$. This can be implemented in two steps:\na. Compute the temporary row vector $w^{\\mathsf{T}} = v_k^{\\mathsf{T}} A_{\\text{sub},L}$. This is equivalent to computing the column vector $w = A_{\\text{sub},L}^{\\mathsf{T}} v_k$. This is a matrix-vector product where the matrix $A_{\\text{sub},L}^{\\mathsf{T}}$ has dimensions $(n-k+1) \\times (n-k)$ and the vector $v_k$ has dimension $(n-k) \\times 1$. Based on the provided rule (cost is $2ab$ for an $a \\times b$ matrix), the cost is $2(n-k+1)(n-k)$ flops.\nb. Perform the rank-one update $A_{\\text{sub},L} \\leftarrow A_{\\text{sub},L} - (\\tau_k v_k) w^{\\mathsf{T}}$. The matrix $A_{\\text{sub},L}$ has dimensions $(n-k) \\times (n-k+1)$. According to the problem's rule, a rank-one update to an $a \\times b$ matrix costs $2ab$ flops. Here, $a=n-k$ and $b=n-k+1$. The cost is $2(n-k)(n-k+1)$ flops.\n\nThe total cost for the left multiplication in iteration $k$ is the sum of these costs:\n$C_{k,L} = 2(n-k)(n-k+1) + 2(n-k)(n-k+1) = 4(n-k)(n-k+1)$ flops.\n\n**2. Cost of the right multiplication, $A \\leftarrow A H_k$**\n\nThe right multiplication is given by $A \\leftarrow A(I - \\tau_k \\tilde{v}_k \\tilde{v}_k^{\\mathsf{T}}) = A - \\tau_k (A \\tilde{v}_k) \\tilde{v}_k^{\\mathsf{T}}$. This operation can be broken down into two steps:\na. Compute the temporary column vector $y = A \\tilde{v}_k$. Since $\\tilde{v}_k$ has non-zero entries only from position $k+1$ to $n$, this product is a linear combination of columns $k+1$ to $n$ of $A$. That is, $y = A(:, k+1:n) v_k$. This is a matrix-vector product where the matrix $A(:, k+1:n)$ has dimensions $n \\times (n-k)$ and the vector $v_k$ has dimensions $(n-k) \\times 1$. The flop count, using the $2ab$ rule with $a=n$ and $b=n-k$, is $2n(n-k)$.\nb. Perform the rank-one update $A \\leftarrow A - (\\tau_k y) \\tilde{v}_k^{\\mathsf{T}}$. Due to the sparsity of $\\tilde{v}_k^{\\mathsf{T}}$ (non-zero entries from position $k+1$ to $n$), this update only modifies columns $k+1$ to $n$ of $A$. This corresponds to a rank-one update of the submatrix $A(:,k+1:n)$, which has dimensions $n \\times (n-k)$. The cost, with $a=n$ and $b=n-k$, is $2n(n-k)$ flops.\n\nThe total cost for the right multiplication in iteration $k$ is:\n$C_{k,R} = 2n(n-k) + 2n(n-k) = 4n(n-k)$ flops.\n\n**3. Total flop count**\n\nThe total number of flops for iteration $k$ is $C_k = C_{k,L} + C_{k,R}$:\n$C_k = 4(n-k)(n-k+1) + 4n(n-k)$.\n\nTo find the leading-order term of the total flop count, we sum $C_k$ from $k=1$ to $n-2$ and retain only the terms that contribute to the $n^3$ term. We can approximate $(n-k+1) \\approx (n-k)$ for the purpose of finding the leading term.\n$C_k \\approx 4(n-k)^2 + 4n(n-k) = 4(n^2 - 2nk + k^2) + 4n^2 - 4nk = 8n^2 - 12nk + 4k^2$.\nThe total flop count $C_{\\text{total}}$ is the sum over $k$:\n$C_{\\text{total}} = \\sum_{k=1}^{n-2} C_k \\approx \\sum_{k=1}^{n-2} (8n^2 - 12nk + 4k^2)$.\n\nWe evaluate the sum of each component:\n- $\\sum_{k=1}^{n-2} 8n^2 = 8n^2(n-2) = 8n^3 - 16n^2$. The leading term is $8n^3$.\n- $\\sum_{k=1}^{n-2} (-12nk) = -12n \\sum_{k=1}^{n-2} k = -12n \\frac{(n-2)(n-1)}{2} = -6n(n^2 - 3n + 2) = -6n^3 + 18n^2 - 12n$. The leading term is $-6n^3$.\n- $\\sum_{k=1}^{n-2} 4k^2 = 4 \\sum_{k=1}^{n-2} k^2 = 4 \\frac{(n-2)(n-1)(2(n-2)+1)}{6} = \\frac{2}{3}(n-2)(n-1)(2n-3)$. The leading term of this polynomial in $n$ is $\\frac{2}{3}(n)(n)(2n) = \\frac{4}{3}n^3$.\n\nSumming the leading-order terms from each part:\n$C_{\\text{total}} \\approx 8n^3 - 6n^3 + \\frac{4}{3}n^3 = 2n^3 + \\frac{4}{3}n^3 = \\frac{6n^3 + 4n^3}{3} = \\frac{10}{3}n^3$.\n\nThe leading-order term for the total flop count of the unblocked Hessenberg reduction is $\\frac{10}{3}n^3$.", "answer": "$$\n\\boxed{\\frac{10}{3}n^{3}}\n$$", "id": "3572651"}, {"introduction": "Theoretical algorithms operate in an idealized world of exact arithmetic, but real-world computations are subject to finite precision and rounding errors. This means the \"zeros\" introduced by a Hessenberg reduction on a computer are rarely exactly zero. This final practice [@problem_id:3572579] moves from theory to application by having you write a program to verify if a computed matrix is \"close enough\" to the true Hessenberg form, a crucial task in testing and validating numerical software.", "problem": "Design and implement a program that, given a set of test matrices, verifies in finite precision arithmetic whether each matrix is close to upper Hessenberg form by bounding the Frobenius norm of the entries that lie strictly below the first subdiagonal. Work in real double-precision floating point arithmetic and model floating point rounding by the standard model of floating point arithmetic: for any basic arithmetic operation applied to real numbers, the computed result satisfies $\\mathrm{fl}(x \\,\\mathrm{op}\\, y) = (x \\,\\mathrm{op}\\, y)\\,(1+\\delta)$ with $|\\delta| \\le u$, where $u$ is the unit roundoff. For binary64 (double precision), assume $u = \\tfrac{1}{2}\\,\\mathrm{eps}$, where $\\mathrm{eps}$ is machine epsilon. An $n \\times n$ real matrix $H$ is upper Hessenberg if $H_{ij} = 0$ for all indices $i,j$ with $i \\ge j + 2$. For any matrix $M \\in \\mathbb{R}^{n \\times n}$, define the matrix $L(M)$ to be the strictly lower-triangular part that lies below the first subdiagonal, i.e., $L(M)_{ij} = M_{ij}$ if $i \\ge j + 2$ and $L(M)_{ij} = 0$ otherwise. The Frobenius norm is defined by $\\|X\\|_{F} = \\sqrt{\\sum_{i=1}^{n}\\sum_{j=1}^{n} X_{ij}^{2}}$. The verification test to be implemented must determine, for a given matrix $M$, whether $\\|L(M)\\|_{F} \\le \\tau(n,M)$ holds, where the tolerance is prescribed as\n$$\n\\tau(n,M) = \\gamma \\, n \\, u \\, \\|M\\|_{F},\n$$\nwith a fixed constant $\\gamma = 10$. The output for each test case must be a Boolean indicating whether the inequality holds. The program must use only real arithmetic and must not assume any structure beyond what is specified for each test case.\n\nYour program must implement the following test suite. All random numbers must be generated using a real-valued standard normal distribution $\\mathcal{N}(0,1)$ with the specified seeds and sizes, and every deterministic construction must be followed exactly as stated. For a given positive integer $n$, let $\\mathrm{HessMask}(n)$ denote the index set $\\{(i,j): 1 \\le i \\le n,\\, 1 \\le j \\le n,\\, i \\le j+1\\}$ corresponding to entries that may be nonzero in an upper Hessenberg matrix, and let $\\mathrm{LowerMask}(n)$ denote the index set $\\{(i,j): i \\ge j+2\\}$.\n\nTest cases to construct and evaluate, in order:\n\n1. Happy-path exact upper Hessenberg. Let $n = 6$ and seed $s = 11$. Construct $H \\in \\mathbb{R}^{n \\times n}$ by drawing independent samples from $\\mathcal{N}(0,1)$ for entries in $\\mathrm{HessMask}(n)$, and set all entries in $\\mathrm{LowerMask}(n)$ to $0$. Evaluate whether $\\|L(H)\\|_{F} \\le \\tau(n,H)$.\n\n2. Nearly Hessenberg with small lower noise. Let $n = 10$ and seed $s_H = 13$. Construct $H \\in \\mathbb{R}^{n \\times n}$ as in test case $1$ using seed $s_H$. Next, let seed $s_E = 1313$ and draw a matrix $E \\in \\mathbb{R}^{n \\times n}$ with independent samples from $\\mathcal{N}(0,1)$ only on $\\mathrm{LowerMask}(n)$ and zeros elsewhere. If $\\|E\\|_{F} \\ne 0$, scale $E$ so that $\\|E\\|_{F}$ equals the target lower-norm $t = 5\\, n \\, u \\, \\|H\\|_{F}$; if $\\|E\\|_{F} = 0$, keep $E = 0$. Define $M = H + E$ and evaluate whether $\\|L(M)\\|_{F} \\le \\tau(n,M)$.\n\n3. Clearly non-Hessenberg lower perturbation. Let $n = 10$ and seed $s_H = 17$. Construct $H \\in \\mathbb{R}^{n \\times n}$ as in test case $1$ using seed $s_H$. Next, let seed $s_E = 1717$ and draw $E$ supported only on $\\mathrm{LowerMask}(n)$ as above. If $\\|E\\|_{F} \\ne 0$, scale $E$ so that $\\|E\\|_{F}$ equals the target lower-norm $t = 10^{-8} \\, \\|H\\|_{F}$; if $\\|E\\|_{F} = 0$, keep $E = 0$. Define $M = H + E$ and evaluate whether $\\|L(M)\\|_{F} \\le \\tau(n,M)$.\n\n4. Boundary case $n=1$. Let $n = 1$ and seed $s = 19$. Draw $M \\in \\mathbb{R}^{1 \\times 1}$ with its single entry from $\\mathcal{N}(0,1)$. Evaluate whether $\\|L(M)\\|_{F} \\le \\tau(n,M)$.\n\n5. Boundary case $n=2$. Let $n = 2$ and seed $s = 23$. Draw $M \\in \\mathbb{R}^{2 \\times 2}$ with entries independently from $\\mathcal{N}(0,1)$. Evaluate whether $\\|L(M)\\|_{F} \\le \\tau(n,M)$.\n\n6. Algorithmic Hessenberg reduction in finite precision. Let $n = 8$ and seed $s_A = 29$. Draw a dense $A \\in \\mathbb{R}^{n \\times n}$ with entries independently from $\\mathcal{N}(0,1)$. Compute a similarity transformation $\\widetilde{H} = Q^{\\top} A Q$ using a sequence of Householder reflectors that zero entries below the first subdiagonal, carried out in floating point arithmetic with unit roundoff $u$ as above. Use a standard unpivoted Householder Hessenberg reduction: for each $k$ from $1$ to $n-2$, compute a Householder reflector acting on rows $k+1$ to $n$ to zero entries below the first subdiagonal in column $k$, and apply it from the left and right to $A$ to update it in place; set $\\widetilde{H}$ to the final updated matrix. Evaluate whether $\\|L(\\widetilde{H})\\|_{F} \\le \\tau(n,\\widetilde{H})$.\n\n7. Scaling invariance sanity check. Reuse the construction of test case $2$ to form $M \\in \\mathbb{R}^{10 \\times 10}$ with seeds $s_H = 13$ and $s_E = 1313$. Define the scalar $s = 10^{6}$ and the scaled matrix $\\widehat{M} = s\\, M$. Evaluate whether $\\|L(\\widehat{M})\\|_{F} \\le \\tau(10,\\widehat{M})$.\n\nImplementation requirements:\n\n- Use double precision. Compute $u$ as $u = \\tfrac{1}{2}\\,\\mathrm{eps}$, where $\\mathrm{eps}$ is the machine epsilon in double precision.\n- All norms must be Frobenius norms as defined above.\n- The Householder reduction in test case $6$ must be implemented explicitly (do not assume exact arithmetic; use floating point operations).\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[true_or_false_1,true_or_false_2,...]\"), where each entry is the Boolean result for the corresponding test case, in the order listed above.\n\nThe final answers for all test cases are Booleans. No physical units or angles are involved; all computations are purely numerical over $\\mathbb{R}$ and must be carried out in the specified finite precision model.", "solution": "The supplied problem is a well-defined computational task in the field of numerical linear algebra. It is scientifically sound, objective, and contains all necessary information for a unique and verifiable solution. The problem asks for the implementation of a verification routine to check if a given matrix is close to upper Hessenberg form, based on a precisely defined tolerance. The problem's validity is confirmed, and a solution is provided below.\n\n### Principle of the Verification Method\nAn $n \\times n$ real matrix $H$ is defined as upper Hessenberg if all its entries below the first subdiagonal are zero. That is, $H_{ij} = 0$ for all indices $i,j$ such that $i \\ge j+2$. In finite precision arithmetic, matrices that are theoretically upper Hessenberg may acquire small non-zero values in these positions due to rounding errors. The task is to implement a test to determine if a matrix $M \\in \\mathbb{R}^{n \\times n}$ is \"close\" to upper Hessenberg form.\n\nThis is accomplished by quantifying the \"size\" of the part of the matrix that should be zero. We define a matrix $L(M)$ that contains only the entries of $M$ strictly below the first subdiagonal:\n$$\nL(M)_{ij} = \\begin{cases} M_{ij}  \\text{if } i \\ge j+2 \\\\ 0  \\text{otherwise} \\end{cases}\n$$\nThe size of this lower part is measured using the Frobenius norm, defined for any matrix $X \\in \\mathbb{R}^{n \\times n}$ as:\n$$\n\\|X\\|_{F} = \\sqrt{\\sum_{i=1}^{n}\\sum_{j=1}^{n} X_{ij}^{2}}\n$$\nThe problem prescribes a tolerance $\\tau(n,M)$ against which $\\|L(M)\\|_{F}$ is compared. This tolerance is a standard form in numerical analysis, scaling with the matrix dimension $n$, the norm of the matrix $\\|M\\|_{F}$, and the unit roundoff $u$. The unit roundoff $u$ characterizes the precision of the floating-point arithmetic. For double-precision (binary64), $u = \\frac{1}{2} \\mathrm{eps}$, where $\\mathrm{eps}$ is the machine epsilon, approximately $2.22 \\times 10^{-16}$. The tolerance is given by:\n$$\n\\tau(n,M) = \\gamma \\, n \\, u \\, \\|M\\|_{F}\n$$\nThe constant $\\gamma$ is given as $10$. A matrix $M$ is considered \"near upper Hessenberg\" if the following inequality holds:\n$$\n\\|L(M)\\|_{F} \\le \\tau(n,M)\n$$\nThe program will implement this check for a series of test matrices constructed according to the problem's specifications.\n\n### Test Case Implementation\nThe seven test cases are constructed and evaluated as follows:\n\n1.  **Exact Upper Hessenberg**: A $6 \\times 6$ matrix $H$ is constructed with entries on and above the first subdiagonal drawn from a standard normal distribution $\\mathcal{N}(0,1)$, and all entries below the first subdiagonal set to exactly $0$. For this matrix, $L(H)$ is the zero matrix, so $\\|L(H)\\|_F = 0$, and the inequality is trivially satisfied.\n\n2.  **Nearly Hessenberg**: A $10 \\times 10$ matrix $M = H+E$ is formed. $H$ is an exact upper Hessenberg matrix. $E$ is a perturbation matrix with non-zero entries only below the first subdiagonal, scaled such that $\\|E\\|_F = 5 \\, n \\, u \\, \\|H\\|_F$. Since $L(M) = E$, the test evaluates if $5 \\, n \\, u \\, \\|H\\|_F \\le 10 \\, n \\, u \\, \\|H+E\\|_F$. As $E$ is very small, $\\|H+E\\|_F \\approx \\|H\\|_F$, so the condition is approximately $5 \\le 10$, which holds.\n\n3.  **Clearly Non-Hessenberg**: Similar to case 2, but the perturbation $E$ is scaled to $\\|E\\|_F = 10^{-8} \\|H\\|_F$. The test becomes a check whether $10^{-8} \\|H\\|_F \\le 10 \\, n \\, u \\, \\|H+E\\|_F$. With $n=10$ and $u \\approx 1.11 \\times 10^{-16}$, the right-hand side is approximately $10 \\cdot 10 \\cdot (1.11 \\times 10^{-16}) \\|H\\|_F \\approx 10^{-14} \\|H\\|_F$. Since $10^{-8}  10^{-14}$, the inequality fails.\n\n4.  **Boundary Case $n=1$**: For any $1 \\times 1$ matrix, the condition $i \\ge j+2$ (i.e., $1 \\ge 1+2$) is never met. Thus, $L(M)$ is always the zero matrix, its norm is $0$, and the test passes.\n\n5.  **Boundary Case $n=2$**: For any $2 \\times 2$ matrix, the condition $i \\ge j+2$ is never satisfied for any indices $(i,j)$. $L(M)$ is the zero matrix, and the test passes.\n\n6.  **Algorithmic Hessenberg Reduction**: A dense $8 \\times 8$ random matrix $A$ is reduced to upper Hessenberg form $\\widetilde{H}$ using an explicit implementation of the Householder reduction algorithm. This algorithm iteratively applies Householder reflectors $P_k$ to zero out elements in each column below the subdiagonal. For each column $k=0, \\dots, n-3$:\n    a. A Householder vector $v$ is constructed from the subcolumn $A[k+1:n, k]$ to zero its entries from the second one downwards. The choice $v = x + \\mathrm{sign}(x_1)\\|x\\|_2 e_1$ is used to avoid subtractive cancellation.\n    b. The similarity transformation $A \\leftarrow P_k A P_k$ is applied efficiently without forming the dense reflector matrices. This is done via rank-1 updates: $A \\leftarrow A - 2 v(v^\\top A)$ and $A \\leftarrow A - 2(Av)v^\\top$, where $v$ is normalized.\n    The backward stability of Householder methods ensures that the resulting matrix $\\widetilde{H}$ has very small entries below the first subdiagonal, with a norm $\\|L(\\widetilde{H})\\|_F$ on the order of $u \\|A\\|_F$. The tolerance $\\tau$ is designed to accommodate this, so the test is expected to pass.\n\n7.  **Scaling Invariance Check**: The test condition is theoretically invariant to scaling $M$ by a non-zero scalar $s$, since $\\|L(sM)\\|_F = |s|\\|L(M)\\|_F$ and $\\tau(n,sM) = |s|\\tau(n,M)$. This test case verifies this property computationally by reusing the matrix from case 2 and scaling it by $s=10^6$. The result should match that of case 2.\n\nThe implementation uses `numpy` for matrix operations. The required Frobenius norms and matrix constructions are performed using standard library functions for efficiency and correctness.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\n# Global constants as per problem statement\nGAMMA = 10.0\n# Unit roundoff for double precision (binary64)\nU = 0.5 * np.finfo(np.float64).eps\n\ndef check_is_near_hessenberg(M: np.ndarray) - bool:\n    \"\"\"\n    Verifies if a matrix M is close to upper Hessenberg form.\n    Checks if ||L(M)||_F = gamma * n * u * ||M||_F.\n    \"\"\"\n    n = M.shape[0]\n    \n    if n = 2:\n        # For n=1 or n=2, the set of indices i = j+2 is empty.\n        # Thus L(M) is the zero matrix and its norm is 0.\n        norm_L_M = 0.0\n    else:\n        # Extract elements where i = j+2 and compute the norm.\n        # np.tril_indices(n, k=-2) gives indices for the lower triangle starting\n        # from the second subdiagonal.\n        rows, cols = np.tril_indices(n, k=-2)\n        lower_elements = M[rows, cols]\n        # The L2 norm of this vector of elements is the Frobenius norm of L(M).\n        norm_L_M = np.linalg.norm(lower_elements)\n\n    norm_M = np.linalg.norm(M, 'fro')\n    \n    # If M is the zero matrix, its norm is 0. L(M) is also zero.\n    # The tolerance is 0, and norm_L_M is 0. The test 0 = 0 passes.\n    if norm_M == 0.0:\n        return True\n\n    tolerance = GAMMA * n * U * norm_M\n    \n    return norm_L_M = tolerance\n\ndef test_case_1():\n    \"\"\"Happy-path exact upper Hessenberg.\"\"\"\n    n = 6\n    seed = 11\n    rng = np.random.default_rng(seed)\n    \n    H = np.zeros((n, n), dtype=np.float64)\n    # HessMask(n) corresponds to i = j+1.\n    rows, cols = np.triu_indices(n, k=-1)\n    H[rows, cols] = rng.normal(size=len(rows))\n    \n    return check_is_near_hessenberg(H)\n\ndef test_case_2_and_7_builder(scale: float):\n    \"\"\"Builder for test cases 2 and 7.\"\"\"\n    n = 10\n    seed_H = 13\n    seed_E = 1313\n    \n    # Construct H (upper Hessenberg part)\n    rng_H = np.random.default_rng(seed_H)\n    H = np.zeros((n, n), dtype=np.float64)\n    rows_h, cols_h = np.triu_indices(n, k=-1)\n    H[rows_h, cols_h] = rng_H.normal(size=len(rows_h))\n    \n    # Construct E (lower perturbation part)\n    rng_E = np.random.default_rng(seed_E)\n    E = np.zeros((n, n), dtype=np.float64)\n    rows_e, cols_e = np.tril_indices(n, k=-2)\n    E[rows_e, cols_e] = rng_E.normal(size=len(rows_e))\n\n    # Scale E\n    norm_H = np.linalg.norm(H, 'fro')\n    norm_E = np.linalg.norm(E, 'fro')\n    \n    target_lower_norm = 5.0 * n * U * norm_H\n\n    E_scaled = np.zeros_like(E)\n    if norm_E != 0.0:\n        E_scaled = E * (target_lower_norm / norm_E)\n        \n    M = H + E_scaled\n    M_final = scale * M\n    \n    return check_is_near_hessenberg(M_final)\n\ndef test_case_3():\n    \"\"\"Clearly non-Hessenberg lower perturbation.\"\"\"\n    n = 10\n    seed_H = 17\n    seed_E = 1717\n    \n    rng_H = np.random.default_rng(seed_H)\n    H = np.zeros((n, n), dtype=np.float64)\n    rows_h, cols_h = np.triu_indices(n, k=-1)\n    H[rows_h, cols_h] = rng_H.normal(size=len(rows_h))\n    \n    rng_E = np.random.default_rng(seed_E)\n    E = np.zeros((n, n), dtype=np.float64)\n    rows_e, cols_e = np.tril_indices(n, k=-2)\n    E[rows_e, cols_e] = rng_E.normal(size=len(rows_e))\n    \n    norm_H = np.linalg.norm(H, 'fro')\n    norm_E = np.linalg.norm(E, 'fro')\n    \n    target_lower_norm = 1e-8 * norm_H\n    \n    E_scaled = np.zeros_like(E)\n    if norm_E != 0:\n        E_scaled = E * (target_lower_norm / norm_E)\n        \n    M = H + E_scaled\n    return check_is_near_hessenberg(M)\n\ndef test_case_4():\n    \"\"\"Boundary case n=1.\"\"\"\n    n = 1\n    seed = 19\n    rng = np.random.default_rng(seed)\n    M = rng.normal(size=(n, n))\n    return check_is_near_hessenberg(M)\n\ndef test_case_5():\n    \"\"\"Boundary case n=2.\"\"\"\n    n = 2\n    seed = 23\n    rng = np.random.default_rng(seed)\n    M = rng.normal(size=(n, n))\n    return check_is_near_hessenberg(M)\n    \ndef householder_hessenberg_reduction(A: np.ndarray) - np.ndarray:\n    \"\"\"Explicitly performs Householder reduction to upper Hessenberg form.\"\"\"\n    M = A.copy()\n    n = M.shape[0]\n    \n    for k in range(n - 2):\n        x = M[k+1:n, k]\n        norm_x = np.linalg.norm(x)\n        \n        s = np.copysign(1.0, x[0] if x[0] != 0 else 1.0)\n        \n        v = x.copy()\n        v[0] += s * norm_x\n        \n        norm_v = np.linalg.norm(v)\n\n        if norm_v  1e-15: # Safeguard against zero vector\n            v /= norm_v\n            \n            # Left multiplication: M_new = P*M = M - 2*v*(v.T*M)\n            sub_M_left = M[k+1:n, k:n]\n            w_left = 2.0 * (v.T @ sub_M_left)\n            M[k+1:n, k:n] -= np.outer(v, w_left)\n            \n            # Right multiplication: M_new = M*P = M - 2*(M*v)*v.T\n            sub_M_right = M[:, k+1:n]\n            w_right = 2.0 * (sub_M_right @ v)\n            M[:, k+1:n] -= np.outer(w_right, v)\n            \n    return M\n\ndef test_case_6():\n    \"\"\"Algorithmic Hessenberg reduction in finite precision.\"\"\"\n    n = 8\n    seed_A = 29\n    rng = np.random.default_rng(seed_A)\n    A = rng.normal(size=(n, n))\n    \n    H_tilde = householder_hessenberg_reduction(A)\n    \n    return check_is_near_hessenberg(H_tilde)\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    test_functions = [\n        test_case_1,\n        lambda: test_case_2_and_7_builder(scale=1.0),\n        test_case_3,\n        test_case_4,\n        test_case_5,\n        test_case_6,\n        lambda: test_case_2_and_7_builder(scale=1e6),\n    ]\n    \n    results = [str(func()).lower() for func in test_functions]\n    \n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3572579"}]}