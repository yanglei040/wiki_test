{"hands_on_practices": [{"introduction": "This first exercise provides a foundational workout in applying the classical Gram-Schmidt process. By starting with a set of linearly dependent vectors, you will use the algorithm to construct an orthogonal basis for the subspace they span, directly connecting the algebraic steps to the geometric concepts of dimension and linear independence. This practice is essential for building a concrete understanding of how CGS transforms a set of vectors into a more structured orthogonal framework [@problem_id:3537535].", "problem": "Consider the real vector space $\\mathbb{R}^{3}$ equipped with the standard inner product $x \\cdot y = x_{1} y_{1} + x_{2} y_{2} + x_{3} y_{3}$. Let $a_{1} = (1,\\,1,\\,0)$, $a_{2} = (0,\\,1,\\,1)$, and $a_{3} = (1,\\,2,\\,1)$. Using only the definitions of inner product, projection onto a vector, and the classical Gram–Schmidt orthogonalization procedure, construct an orthogonal set $\\{q_{1}, q_{2}\\}$ that spans the subspace $S = \\operatorname{span}\\{a_{1}, a_{2}, a_{3}\\} \\subset \\mathbb{R}^{3}$, and justify that $S$ is a proper subspace of $\\mathbb{R}^{3}$. Then, characterize all vectors $v \\in \\mathbb{R}^{3}$ that are orthogonal to $S$ by conditions expressed in terms of inner products with $q_{1}$ and $q_{2}$. Finally, determine the squared Euclidean norm of the orthogonal projection of the vector $b = (3,\\,0,\\,1)$ onto the orthogonal complement $S^{\\perp}$. Provide your final answer as a single real number. No rounding is required.", "solution": "We begin with the standard inner product on $\\mathbb{R}^{3}$, defined by $x \\cdot y = \\sum_{i=1}^{3} x_{i} y_{i}$. The classical Gram–Schmidt orthogonalization constructs orthogonal vectors by iteratively subtracting from each vector its projections onto the previously constructed orthogonal directions.\n\nFirst, observe that $a_{3} = a_{1} + a_{2}$. Therefore,\n$$\nS = \\operatorname{span}\\{a_{1}, a_{2}, a_{3}\\} = \\operatorname{span}\\{a_{1}, a_{2}\\}.\n$$\nSince $a_{1} = (1,\\,1,\\,0)$ and $a_{2} = (0,\\,1,\\,1)$ are linearly independent (no scalar $\\alpha$ satisfies $(1,\\,1,\\,0) = \\alpha(0,\\,1,\\,1)$), the subspace $S$ has dimension $2$. Hence $S$ is a proper subspace of $\\mathbb{R}^{3}$.\n\nWe now apply the classical Gram–Schmidt orthogonalization to $\\{a_{1}, a_{2}\\}$ to produce an orthogonal set $\\{q_{1}, q_{2}\\}$ that spans $S$.\n\nStep 1: Set $q_{1} = a_{1} = (1,\\,1,\\,0)$.\n\nStep 2: Compute $q_{2}$ by subtracting from $a_{2}$ its projection onto $q_{1}$. The projection of $a_{2}$ onto $q_{1}$ is\n$$\n\\operatorname{proj}_{q_{1}}(a_{2}) = \\frac{a_{2} \\cdot q_{1}}{q_{1} \\cdot q_{1}}\\, q_{1}.\n$$\nCompute the inner products:\n$$\na_{2} \\cdot q_{1} = (0)(1) + (1)(1) + (1)(0) = 1, \\quad q_{1} \\cdot q_{1} = (1)^{2} + (1)^{2} + (0)^{2} = 2.\n$$\nThus,\n$$\n\\operatorname{proj}_{q_{1}}(a_{2}) = \\frac{1}{2} q_{1} = \\left(\\frac{1}{2},\\, \\frac{1}{2},\\, 0\\right),\n$$\nand\n$$\nq_{2} = a_{2} - \\operatorname{proj}_{q_{1}}(a_{2}) = \\left(0,\\,1,\\,1\\right) - \\left(\\frac{1}{2},\\, \\frac{1}{2},\\, 0\\right) = \\left(-\\frac{1}{2},\\, \\frac{1}{2},\\, 1\\right).\n$$\nWe confirm orthogonality:\n$$\nq_{1} \\cdot q_{2} = (1)\\left(-\\frac{1}{2}\\right) + (1)\\left(\\frac{1}{2}\\right) + (0)(1) = -\\frac{1}{2} + \\frac{1}{2} + 0 = 0.\n$$\nTherefore, $\\{q_{1}, q_{2}\\}$ is an orthogonal set that spans $S$.\n\nNext, we characterize $S^{\\perp}$, the orthogonal complement of $S$. By definition, a vector $v \\in \\mathbb{R}^{3}$ lies in $S^{\\perp}$ if and only if $v$ is orthogonal to every vector in $S$. Since $\\{q_{1}, q_{2}\\}$ spans $S$, this is equivalent to\n$$\nv \\cdot q_{1} = 0 \\quad \\text{and} \\quad v \\cdot q_{2} = 0.\n$$\nWriting $v = (x,\\,y,\\,z)$, the conditions become the linear system\n$$\nx + y + 0 z = 0, \\quad -\\frac{1}{2} x + \\frac{1}{2} y + 1 z = 0.\n$$\nFrom $x + y = 0$ we have $y = -x$. Substitute into the second equation:\n$$\n-\\frac{1}{2} x + \\frac{1}{2}(-x) + z = 0 \\quad \\Rightarrow \\quad -x + z = 0 \\quad \\Rightarrow \\quad z = x.\n$$\nHence $v = (x,\\,-x,\\,x) = x(1,\\,-1,\\,1)$. Therefore,\n$$\nS^{\\perp} = \\operatorname{span}\\{(1,\\,-1,\\,1)\\}.\n$$\n\nFinally, we compute the squared Euclidean norm of the orthogonal projection of $b = (3,\\,0,\\,1)$ onto $S^{\\perp}$. The orthogonal projection of $b$ onto the one-dimensional subspace spanned by $n = (1,\\,-1,\\,1)$ has the form $\\alpha n$, where $\\alpha$ is chosen such that $b - \\alpha n$ is orthogonal to $n$. The condition $(b - \\alpha n) \\cdot n = 0$ yields\n$$\nb \\cdot n - \\alpha (n \\cdot n) = 0 \\quad \\Rightarrow \\quad \\alpha = \\frac{b \\cdot n}{n \\cdot n}.\n$$\nCompute\n$$\nb \\cdot n = (3)(1) + (0)(-1) + (1)(1) = 4, \\quad n \\cdot n = (1)^{2} + (-1)^{2} + (1)^{2} = 3.\n$$\nThus the projection is $\\alpha n$ with $\\alpha = \\frac{4}{3}$, and its squared Euclidean norm is\n$$\n\\|\\alpha n\\|^{2} = \\alpha^{2} \\|n\\|^{2} = \\left(\\frac{4}{3}\\right)^{2} \\cdot 3 = \\frac{16}{9} \\cdot 3 = \\frac{16}{3}.\n$$\nTherefore, the squared norm of the orthogonal projection of $b$ onto $S^{\\perp}$ is $\\frac{16}{3}$.", "answer": "$$\\boxed{\\frac{16}{3}}$$", "id": "3537535"}, {"introduction": "While the classical Gram-Schmidt process produces an orthogonal basis for a given subspace, is that basis unique? This practice explores the sequential nature of the algorithm and reveals a crucial property: the resulting orthonormal vectors depend on the order in which the original vectors are processed. By performing the CGS procedure on a simple $2 \\times 2$ matrix and its column-permuted variant, you will see firsthand how the $Q$ and $R$ factors change, providing a key insight into the algorithm's path-dependency [@problem_id:3537517].", "problem": "Let $A \\in \\mathbb{R}^{2 \\times 2}$ have columns $a_{1}$ and $a_{2}$ given by\n$$\nA \\;=\\; \\begin{pmatrix}\n1  1 \\\\\n1  0\n\\end{pmatrix}\n\\;=\\; \\bigl[\\, a_{1} \\;\\; a_{2} \\,\\bigr], \\quad a_{1} \\;=\\; \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}, \\quad a_{2} \\;=\\; \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}.\n$$\nLet $P \\in \\mathbb{R}^{2 \\times 2}$ be the permutation matrix that swaps the two columns, that is,\n$$\nP \\;=\\; \\begin{pmatrix}\n0  1 \\\\\n1  0\n\\end{pmatrix},\n\\quad \\text{so that} \\quad AP \\;=\\; \\bigl[\\, a_{2} \\;\\; a_{1} \\,\\bigr] \\;=\\; \\begin{pmatrix}\n1  1 \\\\\n0  1\n\\end{pmatrix}.\n$$\nUsing the classical Gram–Schmidt (CGS) process and the Euclidean inner product as the fundamental base, compute orthogonal–triangular (QR) factorizations\n$$\nA \\;=\\; Q_{A} R_{A}, \\qquad AP \\;=\\; Q_{AP} R_{AP},\n$$\nwhere $Q_{A}, Q_{AP} \\in \\mathbb{R}^{2 \\times 2}$ have orthonormal columns and $R_{A}, R_{AP} \\in \\mathbb{R}^{2 \\times 2}$ are upper triangular with positive diagonal entries. Explicitly show, by carrying out CGS from first principles, that permuting the columns changes the $R$ factor, that is, $R_{A} \\neq R_{AP}$. Then, give a geometric explanation (in the plane spanned by the columns) for why the orthonormal factor $Q$ also changes under the column permutation.\n\nFinally, to quantify the change in the triangular factor, compute the Frobenius norm of their difference,\n$$\n\\bigl\\| R_{A} \\;-\\; R_{AP} \\bigr\\|_{F},\n$$\nand report this value as your final answer in an exact closed form (no rounding).", "solution": "We apply the classical Gram-Schmidt (CGS) process to the columns of both $A$ and $AP$ to find their respective QR factorizations.\n\n**1. QR Factorization of $A = Q_A R_A$**\nThe columns of $A$ are $a_1 = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$ and $a_2 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$. Let the orthonormal columns of $Q_A$ be $q_1$ and $q_2$.\n\n-   **Process $a_1$:** The first diagonal entry of $R_A$ is $r_{11} = \\|a_1\\| = \\sqrt{1^2 + 1^2} = \\sqrt{2}$. The first orthonormal vector is $q_1 = \\frac{a_1}{r_{11}} = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$.\n-   **Process $a_2$:** The off-diagonal entry is $r_{12} = q_1^T a_2 = \\frac{1}{\\sqrt{2}}$. The orthogonal vector is $v_2 = a_2 - r_{12} q_1 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} - \\frac{1}{\\sqrt{2}} \\left( \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} \\right) = \\begin{pmatrix} 1/2 \\\\ -1/2 \\end{pmatrix}$. The second diagonal entry is $r_{22} = \\|v_2\\| = \\sqrt{1/4 + 1/4} = \\frac{1}{\\sqrt{2}}$. The second orthonormal vector is $q_2 = \\frac{v_2}{r_{22}} = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$.\n\nThe resulting factors are:\n$Q_A = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1  1 \\\\ 1  -1 \\end{pmatrix}$ and $R_A = \\begin{pmatrix} \\sqrt{2}  \\frac{1}{\\sqrt{2}} \\\\ 0  \\frac{1}{\\sqrt{2}} \\end{pmatrix}$.\n\n**2. QR Factorization of $AP = Q_{AP} R_{AP}$**\nThe columns of $AP$ are $a'_1 = a_2 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$ and $a'_2 = a_1 = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$.\n\n-   **Process $a'_1$:** The first diagonal entry is $r'_{11} = \\|a'_1\\| = 1$. The first orthonormal vector is $q'_1 = \\frac{a'_1}{r'_{11}} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$.\n-   **Process $a'_2$:** The off-diagonal entry is $r'_{12} = q'_1{}^T a'_2 = 1$. The orthogonal vector is $v'_2 = a'_2 - r'_{12} q'_1 = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} - 1 \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$. The second diagonal entry is $r'_{22} = \\|v'_2\\| = 1$. The second orthonormal vector is $q'_2 = \\frac{v'_2}{r'_{22}} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$.\n\nThe resulting factors are:\n$Q_{AP} = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} = I$ and $R_{AP} = \\begin{pmatrix} 1  1 \\\\ 0  1 \\end{pmatrix}$.\n\n**3. Comparison and Explanation**\nBy direct comparison, $R_A \\neq R_{AP}$. The geometric explanation for why $Q$ also changes is rooted in the sequential nature of CGS. The first column of $Q$, $q_1$, is a unit vector in the direction of the first column of the input matrix.\n-   For $A$, the first column is $(1, 1)^T$, so $q_{A,1} = (1/\\sqrt{2}, 1/\\sqrt{2})^T$.\n-   For $AP$, the first column is $(1, 0)^T$, so $q_{AP,1} = (1, 0)^T$.\nSince the initial vectors $a_1$ and $a_2$ are not collinear, the first orthonormal vectors $q_{A,1}$ and $q_{AP,1}$ are different, which makes the full matrices $Q_A$ and $Q_{AP}$ different.\n\n**4. Frobenius Norm Calculation**\nThe difference matrix is $D = R_A - R_{AP} = \\begin{pmatrix} \\sqrt{2} - 1  \\frac{1}{\\sqrt{2}} - 1 \\\\ 0  \\frac{1}{\\sqrt{2}} - 1 \\end{pmatrix}$.\nThe squared Frobenius norm is:\n$$\n\\|D\\|_F^2 = (\\sqrt{2} - 1)^2 + 2\\left(\\frac{1}{\\sqrt{2}} - 1\\right)^2 = (3 - 2\\sqrt{2}) + 2\\left(\\frac{3}{2} - \\sqrt{2}\\right) = 3 - 2\\sqrt{2} + 3 - 2\\sqrt{2} = 6 - 4\\sqrt{2}.\n$$\nThe Frobenius norm is the square root, which can be simplified:\n$$\n\\| R_{A} - R_{AP} \\|_{F} = \\sqrt{6 - 4\\sqrt{2}} = \\sqrt{(2 - \\sqrt{2})^2} = 2 - \\sqrt{2}.\n$$", "answer": "$$\n\\boxed{2 - \\sqrt{2}}\n$$", "id": "3537517"}, {"introduction": "This advanced exercise takes you from the world of exact arithmetic into the practical realities of computation. By meticulously simulating a low-precision floating-point environment, you will uncover the numerical fragility of the classical Gram-Schmidt algorithm and witness how subtractive cancellation can lead to a dramatic loss of orthogonality. This practice powerfully demonstrates that intuitive heuristics for improving stability can sometimes fail, revealing the subtle and often counter-intuitive nature of rounding error propagation in numerical linear algebra [@problem_id:3537544].", "problem": "Let $A \\in \\mathbb{R}^{m \\times n}$ have full column rank. In exact arithmetic, the classical Gram–Schmidt (CGS) process applied to the columns of $A$ produces a matrix $Q \\in \\mathbb{R}^{m \\times n}$ with orthonormal columns and an upper triangular matrix $R \\in \\mathbb{R}^{n \\times n}$ such that $A = Q R$, hence $Q^{\\top} Q = I$. In floating-point arithmetic, however, rounding in inner products and projections can induce a loss of orthogonality, which can be quantified by the Frobenius norm $\\|Q^{\\top} Q - I\\|_{F}$. A natural column-ordering heuristic is to process the columns in nonincreasing order of their Euclidean norms (largest $2$-norm first). Consider the following finite-precision model and matrix:\n- Arithmetic model: Base-$10$ floating point with rounding to nearest with $3$ significant digits at each elementary operation (multiplication, addition, division, and square root), with ties broken by the round-to-even rule. Inner products are computed by the naive summation of the products in the given order, applying the same rounding after each multiplication and each addition. Normalization sets $q = v / \\|v\\|$ by computing the norm $\\|v\\|$ via a rounded inner product and square root, then dividing componentwise with rounding.\n- Matrix $A \\in \\mathbb{R}^{2 \\times 2}$ with columns $a_{1}$ and $a_{2}$ given by\n$$\na_{1} = \\begin{pmatrix} 1.00 \\\\ 1.00 \\end{pmatrix}, \\qquad a_{2} = \\begin{pmatrix} 1.00 \\\\ 1.01 \\end{pmatrix}.\n$$\nTasks:\n1. Using only the foundational definitions of CGS and the stated floating-point model, explain from first principles how the ordering of the columns can affect the accumulation of rounding error in the projection step and thereby influence $\\|Q^{\\top} Q - I\\|_{F}$.\n2. Perform CGS twice under the stated arithmetic model:\n   - First with the largest-norm-first (LNF) ordering $(a_{2}, a_{1})$.\n   - Then with the smallest-norm-first (SNF) ordering $(a_{1}, a_{2})$.\n   In each run, compute the rounded $Q$ produced by CGS, then evaluate the exact Frobenius norm of $Q^{\\top} Q - I$ in real arithmetic using the computed $Q$.\n3. Let $E_{\\mathrm{LNF}} = \\|Q_{\\mathrm{LNF}}^{\\top} Q_{\\mathrm{LNF}} - I\\|_{F}$ and $E_{\\mathrm{SNF}} = \\|Q_{\\mathrm{SNF}}^{\\top} Q_{\\mathrm{SNF}} - I\\|_{F}$. Report the ratio\n$$\nR \\;=\\; \\frac{E_{\\mathrm{LNF}}}{E_{\\mathrm{SNF}}}.\n$$\nRound your final value of $R$ to four significant figures. The final answer must be the single rounded value of $R$.", "solution": "The solution follows the three tasks requested in the problem. Let $\\mathrm{fl}(\\cdot)$ denote rounding to $3$ significant digits per the specified model.\n\n**1. Effect of Column Ordering on Rounding Error**\nIn classical Gram-Schmidt (CGS), the core step is computing the orthogonal vector $v_k = a_k - \\sum_{j=1}^{k-1} \\langle q_j, a_k \\rangle q_j$. In floating-point arithmetic, this subtraction can cause a catastrophic loss of relative precision if the projection term is very close to $a_k$. This happens when $a_k$ is nearly linearly dependent on the preceding vectors, making the orthogonal component $v_k$ very small. The loss of orthogonality is approximately proportional to the ratio $\\|a_k\\|_2 / \\|v_k\\|_2$. A small $\\|v_k\\|_2$ amplifies rounding errors and degrades the orthogonality of the resulting basis. Since column ordering determines which vector is processed at each step, it changes the intermediate subspaces and thus the magnitude of each $\\|v_k\\|_2$. An ordering that produces a smaller $\\|v_k\\|_2$ is expected to suffer a greater loss of orthogonality.\n\n**2. CGS Calculation in Floating-Point Arithmetic**\n\n**a) Largest-Norm-First (LNF) Ordering: $(a_{2}, a_{1})$**\nThe columns are $a'_1 = a_2 = \\begin{pmatrix} 1.00 \\\\ 1.01 \\end{pmatrix}$ and $a'_2 = a_1 = \\begin{pmatrix} 1.00 \\\\ 1.00 \\end{pmatrix}$.\n\n-   **Process $a'_1$:** The norm is $\\|a'_1\\|_2 = \\mathrm{fl}(\\sqrt{\\mathrm{fl}(1.00^2 + 1.01^2)}) = \\mathrm{fl}(\\sqrt{2.02}) = 1.42$. Normalizing gives $q_1 = \\begin{pmatrix} \\mathrm{fl}(1.00 / 1.42) \\\\ \\mathrm{fl}(1.01 / 1.42) \\end{pmatrix} = \\begin{pmatrix} 0.704 \\\\ 0.711 \\end{pmatrix}$.\n-   **Process $a'_2$:** The projection coefficient is $r_{12} = \\langle q_1, a'_2 \\rangle = \\mathrm{fl}(0.704 + 0.711) = \\mathrm{fl}(1.415) = 1.42$ (tie, rounds to even). The projection vector is $\\mathrm{fl}(1.42 \\times q_1) = \\begin{pmatrix} 1.00 \\\\ 1.01 \\end{pmatrix}$. The orthogonal vector is $v_2 = \\mathrm{fl}\\left(\\begin{pmatrix} 1.00 \\\\ 1.00 \\end{pmatrix} - \\begin{pmatrix} 1.00 \\\\ 1.01 \\end{pmatrix}\\right) = \\begin{pmatrix} 0.00 \\\\ -0.0100 \\end{pmatrix}$. Its norm is $\\|v_2\\|_2 = 0.0100$. Normalizing gives $q_2 = \\begin{pmatrix} 0.00 \\\\ -1.00 \\end{pmatrix}$.\n\nThe resulting matrix is $Q_{\\mathrm{LNF}} = \\begin{pmatrix} 0.704  0.00 \\\\ 0.711  -1.00 \\end{pmatrix}$.\n\n**b) Smallest-Norm-First (SNF) Ordering: $(a_{1}, a_{2})$**\nThe columns are $a'_1 = a_1 = \\begin{pmatrix} 1.00 \\\\ 1.00 \\end{pmatrix}$ and $a'_2 = a_2 = \\begin{pmatrix} 1.00 \\\\ 1.01 \\end{pmatrix}$.\n\n-   **Process $a'_1$:** The norm is $\\|a'_1\\|_2 = \\mathrm{fl}(\\sqrt{\\mathrm{fl}(1.00^2 + 1.00^2)}) = \\mathrm{fl}(\\sqrt{2.00}) = 1.41$. Normalizing gives $q_1 = \\begin{pmatrix} \\mathrm{fl}(1.00/1.41) \\\\ \\mathrm{fl}(1.00/1.41) \\end{pmatrix} = \\begin{pmatrix} 0.709 \\\\ 0.709 \\end{pmatrix}$.\n-   **Process $a'_2$:** The projection coefficient is $r_{12} = \\langle q_1, a'_2 \\rangle = \\mathrm{fl}(0.709 \\times 1.00 + 0.709 \\times 1.01) = \\mathrm{fl}(0.709 + 0.716) = \\mathrm{fl}(1.425) = 1.42$ (tie, rounds to even). The projection vector is $\\mathrm{fl}(1.42 \\times q_1) = \\begin{pmatrix} 1.01 \\\\ 1.01 \\end{pmatrix}$. The orthogonal vector is $v_2 = \\mathrm{fl}\\left(\\begin{pmatrix} 1.00 \\\\ 1.01 \\end{pmatrix} - \\begin{pmatrix} 1.01 \\\\ 1.01 \\end{pmatrix}\\right) = \\begin{pmatrix} -0.0100 \\\\ 0.00 \\end{pmatrix}$. Its norm is $\\|v_2\\|_2 = 0.0100$. Normalizing gives $q_2 = \\begin{pmatrix} -1.00 \\\\ 0.00 \\end{pmatrix}$.\n\nThe resulting matrix is $Q_{\\mathrm{SNF}} = \\begin{pmatrix} 0.709  -1.00 \\\\ 0.709  0.00 \\end{pmatrix}$.\n\n**3. Error Calculation and Ratio**\nWe compute the error matrices $Q^{\\top} Q - I$ and their Frobenius norms in exact arithmetic.\n\n-   **Error for LNF:** $Q_{\\mathrm{LNF}}^{\\top} Q_{\\mathrm{LNF}} - I = \\begin{pmatrix} 0.001137  -0.711 \\\\ -0.711  0.00 \\end{pmatrix}$.\n    $E_{\\mathrm{LNF}}^2 = \\|Q_{\\mathrm{LNF}}^{\\top} Q_{\\mathrm{LNF}} - I\\|_{F}^2 = (0.001137)^2 + 2(-0.711)^2 \\approx 1.011043$.\n    $E_{\\mathrm{LNF}} \\approx \\sqrt{1.011043} \\approx 1.005506$.\n\n-   **Error for SNF:** $Q_{\\mathrm{SNF}}^{\\top} Q_{\\mathrm{SNF}} - I = \\begin{pmatrix} 0.005362  -0.709 \\\\ -0.709  0.00 \\end{pmatrix}$.\n    $E_{\\mathrm{SNF}}^2 = \\|Q_{\\mathrm{SNF}}^{\\top} Q_{\\mathrm{SNF}} - I\\|_{F}^2 = (0.005362)^2 + 2(-0.709)^2 \\approx 1.005391$.\n    $E_{\\mathrm{SNF}} \\approx \\sqrt{1.005391} \\approx 1.002692$.\n\n-   **Ratio:** $R = \\frac{E_{\\mathrm{LNF}}}{E_{\\mathrm{SNF}}} = \\frac{1.005506...}{1.002692...} \\approx 1.002807...$.\n    Rounding to four significant figures gives $1.003$.", "answer": "$$\n\\boxed{1.003}\n$$", "id": "3537544"}]}