{"hands_on_practices": [{"introduction": "The theoretical elegance of reflectors lies in their perfect orthogonality, which guarantees the preservation of geometric properties like norms and distances. This exercise provides a practical test of these properties in the context of finite-precision computer arithmetic. By implementing a numerical experiment to scramble and then reconstruct data using a product of random reflectors, you will quantify the inevitable \"drift\" from theoretical perfection and gain an intuition for the numerical behavior of these fundamental transformations [@problem_id:3572839].", "problem": "You are to implement a fully reproducible numerical experiment in the domain of numerical linear algebra to study the behavior of reflectors and their properties under finite-precision arithmetic. The experiment must construct a product of random orthogonal reflectors that scrambles data while ideally preserving Euclidean pairwise distances, then reconstruct with the transpose to undo the scrambling. Finally, the experiment must quantify the drift introduced by floating-point errors, which manifests in the deviation of $Q^\\top Q$ from the identity matrix.\n\nThe fundamental base for this experiment is the following set of well-tested definitions and facts about Euclidean vector spaces and orthogonal transformations:\n- An orthogonal matrix preserves the Euclidean $2$-norm and the Euclidean distances between vectors.\n- A reflector (Householder transformation) is an orthogonal linear operator that maps a vector to its reflection across a hyperplane orthogonal to a unit direction.\n- In exact arithmetic, the product of orthogonal matrices is orthogonal. In floating-point arithmetic, roundoff and accumulation of errors can cause small deviations from exact orthogonality.\n\nYour program must:\n1. Generate a real matrix $X \\in \\mathbb{R}^{n \\times m}$ whose columns $x_i$ are independently sampled from a standard normal distribution, using a fixed random seed for reproducibility.\n2. Construct $r$ random reflectors and form their product $Q \\in \\mathbb{R}^{n \\times n}$, then compute the scrambled data $Y = Q X$, where the columns are $y_i = Q x_i$.\n3. Compute the following three quantitative metrics:\n   - Orthogonality defect: $E_{\\mathrm{orth}} = \\lVert Q^\\top Q - I \\rVert_F$, where $\\lVert \\cdot \\rVert_F$ denotes the Frobenius norm.\n   - Maximum relative pairwise-distance error after scrambling: \n     $$E_{\\mathrm{dist}}^{\\mathrm{scramble}} = \\max_{1 \\le i < j \\le m} \\frac{\\left| \\lVert y_i - y_j \\rVert_2 - \\lVert x_i - x_j \\rVert_2 \\right|}{\\max(\\lVert x_i - x_j \\rVert_2, \\varepsilon)},$$\n     where $\\varepsilon$ is a small positive constant chosen as the machine epsilon for double precision, ensuring numerical stability when denominators are near zero.\n   - Reconstruction drift: $E_{\\mathrm{recon}} = \\frac{\\lVert Q^\\top (Q X) - X \\rVert_F}{\\lVert X \\rVert_F}$.\n\n4. Use a fixed random seed $123456789$ for all random number generation.\n\n5. Implement the construction of reflectors and their product in a manner that is consistent with their definition as orthogonal reflections across hyperplanes. Do not rely on prebuilt orthogonalization routines; instead, construct the product of reflectors directly.\n\n6. Provide a test suite with the following parameter sets $(n,m,r)$, which are chosen to examine varied behaviors including the identity case, minimal and multiple reflectors, and a high count of reflectors to accumulate rounding error:\n   - Test case A: $(n,m,r) = (8,20,0)$.\n   - Test case B: $(n,m,r) = (32,50,32)$.\n   - Test case C: $(n,m,r) = (64,80,320)$.\n   - Test case D: $(n,m,r) = (16,40,1)$.\n\n7. For each test case, compute and return the triple of floats $(E_{\\mathrm{orth}}, E_{\\mathrm{dist}}^{\\mathrm{scramble}}, E_{\\mathrm{recon}})$ in that order.\n\nYour program should produce a single line of output containing the results for all test cases as a comma-separated list enclosed in square brackets. The list must contain the three floats for Test A, then the three floats for Test B, then the three floats for Test C, then the three floats for Test D, preserving the stated order; that is, the output must be of the form\n$[E_{\\mathrm{orth}}^{A}, E_{\\mathrm{dist}}^{\\mathrm{scramble},A}, E_{\\mathrm{recon}}^{A}, E_{\\mathrm{orth}}^{B}, E_{\\mathrm{dist}}^{\\mathrm{scramble},B}, E_{\\mathrm{recon}}^{B}, E_{\\mathrm{orth}}^{C}, E_{\\mathrm{dist}}^{\\mathrm{scramble},C}, E_{\\mathrm{recon}}^{C}, E_{\\mathrm{orth}}^{D}, E_{\\mathrm{dist}}^{\\mathrm{scramble},D}, E_{\\mathrm{recon}}^{D}]$.\nNo physical units apply; all values are dimensionless real numbers.", "solution": "The problem statement is a valid and well-posed numerical experiment in linear algebra. It is scientifically grounded in the theory of orthogonal transformations and the practical realities of finite-precision arithmetic. The objectives are clear, all necessary parameters and definitions are provided, and the setup is reproducible. We may therefore proceed with the solution.\n\nThe experiment is designed to investigate the properties of orthogonal matrices constructed as products of Householder reflectors. A Householder reflector, or Householder transformation, is an orthogonal matrix $H$ of the form\n$$H = I - 2vv^\\top$$\nwhere $v$ is a column vector with Euclidean norm $\\lVert v \\rVert_2 = 1$. Geometrically, $H$ reflects vectors across the hyperplane orthogonal to $v$. Being orthogonal, $H$ satisfies $H^\\top H = H H^\\top = I$ and preserves Euclidean norms and distances, i.e., $\\lVert Hx \\rVert_2 = \\lVert x \\rVert_2$ for any vector $x$.\n\nThe experiment involves the following key steps:\n\n**1. Generation of Random Reflectors and their Product**\nFor each of the $r$ reflectors to be constructed, we first generate a random vector $u \\in \\mathbb{R}^n$ with entries drawn from a standard normal distribution. This vector is then normalized to produce the unit vector $v = u / \\lVert u \\rVert_2$. This vector $v$ defines the $k$-th reflector, $H_k = I - 2v_k v_k^\\top$, for $k \\in \\{1, 2, \\dots, r\\}$.\nThe overall transformation matrix $Q$ is the product of these $r$ reflectors:\n$$Q = H_r H_{r-1} \\cdots H_1$$\nIn exact arithmetic, the product of orthogonal matrices is itself orthogonal. Thus, theoretically, $Q^\\top Q = I$. The numerical experiment will assess the deviation from this identity due to floating-point errors. The matrix $Q$ is constructed iteratively, starting with the identity matrix $Q_0 = I$ and applying each reflector in sequence: $Q_k = H_k Q_{k-1}$ for $k=1, \\dots, r$. The final matrix is $Q = Q_r$. For the base case where $r=0$, no transformations are applied, and $Q$ remains the identity matrix $I$.\n\n**2. Data Generation and Transformation**\nA data matrix $X \\in \\mathbb{R}^{n \\times m}$ is generated, where each of its $m$ columns is a vector in $\\mathbb{R}^n$ sampled independently from a standard normal distribution. A fixed random seed, specified as $123456789$, ensures the reproducibility of this matrix $X$ and the random reflectors. The transformation $Q$ is then applied to the data matrix to produce the \"scrambled\" data $Y = QX$.\n\n**3. Quantification of Numerical Errors**\nThree metrics are computed to quantify the effects of finite-precision arithmetic:\n\n   - **Orthogonality Defect, $E_{\\mathrm{orth}}$**: This metric measures how much the computed matrix $Q$ deviates from being perfectly orthogonal. It is defined as the Frobenius norm of the difference between $Q^\\top Q$ and the identity matrix $I$:\n     $$E_{\\mathrm{orth}} = \\lVert Q^\\top Q - I \\rVert_F$$\n     In a perfect theoretical setting, $E_{\\mathrm{orth}}$ would be $0$. Non-zero values are attributable to the accumulation of floating-point rounding errors during the construction of $Q$.\n\n   - **Maximum Relative Pairwise-Distance Error, $E_{\\mathrm{dist}}^{\\mathrm{scramble}}$**: Since an orthogonal transformation must preserve Euclidean distances, the distance between any two columns $x_i$ and $x_j$ of $X$ should be identical to the distance between the corresponding columns $y_i$ and $y_j$ of $Y$. This metric captures the maximum relative error in these pairwise distances over all pairs of columns:\n     $$E_{\\mathrm{dist}}^{\\mathrm{scramble}} = \\max_{1 \\le i < j \\le m} \\frac{\\left| \\lVert y_i - y_j \\rVert_2 - \\lVert x_i - x_j \\rVert_2 \\right|}{\\max(\\lVert x_i - x_j \\rVert_2, \\varepsilon)}$$\n     Here, $\\varepsilon$ is the machine epsilon for double-precision floating-point numbers, used to prevent division by a near-zero denominator. Any non-zero value indicates a deviation from ideal distance preservation.\n\n   - **Reconstruction Drift, $E_{\\mathrm{recon}}$**: The inverse of an orthogonal matrix $Q$ is its transpose, $Q^\\top$. Therefore, applying $Q^\\top$ to the scrambled data $Y$ should reconstruct the original data $X$, i.e., $Q^\\top Y = Q^\\top(QX) = (Q^\\top Q)X = IX = X$. This metric quantifies the relative error in this reconstruction process:\n     $$E_{\\mathrm{recon}} = \\frac{\\lVert Q^\\top Y - X \\rVert_F}{\\lVert X \\rVert_F} = \\frac{\\lVert Q^\\top (Q X) - X \\rVert_F}{\\lVert X \\rVert_F}$$\n     This value measures the drift from perfect reconstruction, normalized by the magnitude of the original data. It is influenced by both the non-orthogonality of the computed $Q$ and the error accumulation from two successive matrix-matrix multiplications.\n\nThe implementation will proceed by iterating through the given parameter sets $(n,m,r)$, performing these computational steps for each, and collecting the resulting triple $(E_{\\mathrm{orth}}, E_{\\mathrm{dist}}^{\\mathrm{scramble}}, E_{\\mathrm{recon}})$. For the test case where $r=0$, we expect all three error metrics to be exactly $0$, as $Q$ will be the identity matrix and no floating-point errors from transformations will accumulate. For cases where $r > 0$, we anticipate non-zero errors that increase with the number of reflectors $r$, as more operations lead to greater accumulated error.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.spatial.distance import pdist\n\ndef solve():\n    \"\"\"\n    Implements a numerical experiment to study the properties of reflectors\n    under finite-precision arithmetic.\n    \"\"\"\n\n    # Fixed random seed for reproducibility as required by the problem statement.\n    seed = 123456789\n    rng = np.random.default_rng(seed)\n\n    # Machine epsilon for double precision, for numerical stability.\n    epsilon = np.finfo(np.float64).eps\n\n    # Test cases defined by the parameter sets (n, m, r).\n    test_cases = [\n        (8, 20, 0),    # Test case A\n        (32, 50, 32),   # Test case B\n        (64, 80, 320),  # Test case C\n        (16, 40, 1),    # Test case D\n    ]\n\n    results = []\n    for n, m, r in test_cases:\n        # 1. Generate the real data matrix X.\n        X = rng.standard_normal(size=(n, m))\n\n        # 2. Construct the product of r random reflectors, Q.\n        Q = np.identity(n)\n        for _ in range(r):\n            # Generate a random vector u from a standard normal distribution.\n            u = rng.standard_normal(size=(n, 1))\n            \n            # Normalize u to get a unit vector v.\n            norm_u = np.linalg.norm(u)\n            # In the unlikely event norm_u is 0, skip this reflector.\n            if norm_u == 0:\n                continue\n            v = u / norm_u\n            \n            # Construct the Householder reflector H = I - 2*v*v^T.\n            H = np.identity(n) - 2 * (v @ v.T)\n            \n            # Apply the new reflector to the product Q.\n            # Q_new = H_k * Q_old, so Q = H_r * ... * H_1 * I\n            Q = H @ Q\n\n        # Compute the scrambled data Y = QX.\n        Y = Q @ X\n\n        # 3. Compute the three quantitative metrics.\n\n        # Orthogonality defect: E_orth = ||Q^T Q - I||_F\n        e_orth = np.linalg.norm(Q.T @ Q - np.identity(n), 'fro')\n\n        # Maximum relative pairwise-distance error: E_dist\n        if m > 1:\n            # pdist computes pairwise distances between rows. We need it for columns.\n            dists_x = pdist(X.T, metric='euclidean')\n            dists_y = pdist(Y.T, metric='euclidean')\n            \n            abs_diff = np.abs(dists_y - dists_x)\n            denominator = np.maximum(dists_x, epsilon)\n            \n            # Handle the case where all pairwise distances are zero.\n            if np.all(denominator == epsilon):\n                e_dist = 0.0\n            else:\n                rel_errors = abs_diff / denominator\n                e_dist = np.max(rel_errors)\n        else:\n            e_dist = 0.0 # No pairs to compare if m = 1.\n\n        # Reconstruction drift: E_recon = ||Q^T (QX) - X||_F / ||X||_F\n        norm_X_fro = np.linalg.norm(X, 'fro')\n        # norm_X_fro will be non-zero with virtual certainty for the given parameters.\n        if norm_X_fro == 0:\n            e_recon = 0.0\n        else:\n            e_recon = np.linalg.norm(Q.T @ Y - X, 'fro') / norm_X_fro\n\n        results.extend([e_orth, e_dist, e_recon])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3572839"}, {"introduction": "The specific formula used to construct a Householder vector is a classic example of numerically-aware algorithm design, engineered to avoid catastrophic cancellation. This practice delves into the mathematical underpinnings of this design by analyzing the continuity of the mapping from a vector $x$ to its corresponding Householder vector $u(x)$. By investigating the function's behavior across the critical hyperplane where $x_1=0$, you will uncover a discontinuity that necessitates the careful formulation used in robust numerical software [@problem_id:3572880].", "problem": "Let $n \\ge 2$ and let $e_1 \\in \\mathbb{R}^n$ denote the first standard basis vector. For any $x \\in \\mathbb{R}^n \\setminus \\{0\\}$, define\n$$\n\\alpha(x) = -\\mathrm{sign}(x_1)\\,\\|x\\|_2, \\quad \\text{with the convention } \\mathrm{sign}(0)=1,\n$$\nand\n$$\nu(x) = \\frac{x - \\alpha(x)\\,e_1}{\\|x - \\alpha(x)\\,e_1\\|_2}.\n$$\nThis construction appears in the formation of Householder reflectors in numerical linear algebra. Consider a point $x_0 \\in \\mathbb{R}^n$ with $x_{0,1} = 0$ and $\\|x_0\\|_2 = r  0$. To probe differentiability of the map $x \\mapsto u(x)$ across the hyperplane $x_1=0$, define the one-parameter curves\n$$\nx^{+}(t) = x_0 + t\\,e_1, \\quad x^{-}(t) = x_0 - t\\,e_1, \\quad t0,\n$$\nand the corresponding images $u^{+}(t) = u(x^{+}(t))$ and $u^{-}(t) = u(x^{-}(t))$. Compute the jump magnitude\n$$\nJ \\;=\\; \\lim_{t \\to 0^+} \\big\\|\\,u^{+}(t) - u^{-}(t)\\,\\big\\|_2.\n$$\nReport the value of $J$. In your reasoning, start from the definitions of the Euclidean norm, the sign function, and limits, and do not assume any pre-derived formulas beyond these. You may assume $x_0 = (0, r, 0, \\dots, 0)^{\\top}$ without loss of generality. After obtaining $J$, briefly explain what this implies about differentiability of $x \\mapsto u(x)$ at points with $x_1=0$ and the implications for the stability of algorithms that form Householder reflectors via this $u(x)$ construction. The final reported answer must be the single value of $J$ with no units and no rounding approximation.", "solution": "The problem is validated as self-contained, scientifically grounded in numerical linear algebra, and well-posed. We may proceed with the solution.\n\nThe problem asks for the computation of a jump magnitude for the function $u(x)$ across the hyperplane defined by $x_1=0$. Let $n \\ge 2$, $e_1 \\in \\mathbb{R}^n$ be the first standard basis vector. The given functions are:\n$$\n\\alpha(x) = -\\mathrm{sign}(x_1)\\,\\|x\\|_2, \\quad \\text{with the convention } \\mathrm{sign}(0)=1\n$$\n$$\nu(x) = \\frac{x - \\alpha(x)\\,e_1}{\\|x - \\alpha(x)\\,e_1\\|_2}\n$$\nWe are given a point $x_0 \\in \\mathbb{R}^n$ such that its first component is $x_{0,1}=0$ and its Euclidean norm is $\\|x_0\\|_2 = r  0$. As permitted by the problem statement, we adopt the simplification $x_0 = (0, r, 0, \\dots, 0)^{\\top}$.\n\nThe two curves approaching $x_0$ for $t0$ are defined as:\n$$\nx^{+}(t) = x_0 + t\\,e_1 = (t, r, 0, \\dots, 0)^{\\top}\n$$\n$$\nx^{-}(t) = x_0 - t\\,e_1 = (-t, r, 0, \\dots, 0)^{\\top}\n$$\nWe need to compute $J = \\lim_{t \\to 0^+} \\|u^{+}(t) - u^{-}(t)\\|_2$, where $u^{+}(t) = u(x^{+}(t))$ and $u^{-}(t) = u(x^{-}(t))$.\n\nFirst, we analyze $u^{+}(t) = u(x^{+}(t))$.\nThe first component of $x^{+}(t)$ is $x^{+}_1(t) = t$. Since $t  0$, we have $\\mathrm{sign}(x^{+}_1(t)) = 1$.\nThe Euclidean norm of $x^{+}(t)$ is $\\|x^{+}(t)\\|_2 = \\sqrt{t^2 + r^2}$.\nThus, $\\alpha(x^{+}(t)) = -\\mathrm{sign}(t) \\|x^{+}(t)\\|_2 = -1 \\cdot \\sqrt{t^2 + r^2} = -\\sqrt{t^2 + r^2}$.\n\nThe numerator of $u^{+}(t)$ is the vector $v^{+}(t) = x^{+}(t) - \\alpha(x^{+}(t))\\,e_1$:\n$$\nv^{+}(t) = (t, r, 0, \\dots, 0)^{\\top} - (-\\sqrt{t^2+r^2}) (1, 0, \\dots, 0)^{\\top} = (t + \\sqrt{t^2+r^2}, r, 0, \\dots, 0)^{\\top}\n$$\nThe denominator of $u^{+}(t)$ is the norm $\\|v^{+}(t)\\|_2$. We compute its square:\n$$\n\\|v^{+}(t)\\|_2^2 = (t + \\sqrt{t^2+r^2})^2 + r^2 = t^2 + 2t\\sqrt{t^2+r^2} + (t^2+r^2) + r^2 = 2t^2 + 2r^2 + 2t\\sqrt{t^2+r^2}\n$$\nFactoring this expression:\n$$\n\\|v^{+}(t)\\|_2^2 = 2(t^2+r^2) + 2t\\sqrt{t^2+r^2} = 2\\sqrt{t^2+r^2}(\\sqrt{t^2+r^2} + t)\n$$\nSo, $u^{+}(t) = \\frac{v^{+}(t)}{\\|v^{+}(t)\\|_2} = \\frac{(t + \\sqrt{t^2+r^2}, r, 0, \\dots, 0)^{\\top}}{\\sqrt{2\\sqrt{t^2+r^2}(\\sqrt{t^2+r^2} + t)}}$.\n\nNext, we analyze $u^{-}(t) = u(x^{-}(t))$.\nThe first component of $x^{-}(t)$ is $x^{-}_1(t) = -t$. Since $t  0$, we have $\\mathrm{sign}(x^{-}_1(t)) = -1$.\nThe Euclidean norm of $x^{-}(t)$ is $\\|x^{-}(t)\\|_2 = \\sqrt{(-t)^2 + r^2} = \\sqrt{t^2 + r^2}$.\nThus, $\\alpha(x^{-}(t)) = -\\mathrm{sign}(-t) \\|x^{-}(t)\\|_2 = -(-1) \\sqrt{t^2 + r^2} = \\sqrt{t^2 + r^2}$.\n\nThe numerator of $u^{-}(t)$ is the vector $v^{-}(t) = x^{-}(t) - \\alpha(x^{-}(t))\\,e_1$:\n$$\nv^{-}(t) = (-t, r, 0, \\dots, 0)^{\\top} - (\\sqrt{t^2+r^2}) (1, 0, \\dots, 0)^{\\top} = (-t - \\sqrt{t^2+r^2}, r, 0, \\dots, 0)^{\\top}\n$$\nThe denominator of $u^{-}(t)$ is the norm $\\|v^{-}(t)\\|_2$. We compute its square:\n$$\n\\|v^{-}(t)\\|_2^2 = (-t - \\sqrt{t^2+r^2})^2 + r^2 = (t + \\sqrt{t^2+r^2})^2 + r^2 = 2\\sqrt{t^2+r^2}(\\sqrt{t^2+r^2} + t)\n$$\nThe denominator is identical to that of $u^{+}(t)$. Let $D(t) = \\|v^{+}(t)\\|_2 = \\|v^{-}(t)\\|_2$.\nSo, $u^{-}(t) = \\frac{v^{-}(t)}{\\|v^{-}(t)\\|_2} = \\frac{(-t - \\sqrt{t^2+r^2}, r, 0, \\dots, 0)^{\\top}}{D(t)}$.\n\nNow we compute the difference vector $u^{+}(t) - u^{-}(t)$:\n$$\nu^{+}(t) - u^{-}(t) = \\frac{1}{D(t)} [v^{+}(t) - v^{-}(t)]\n$$\n$$\nv^{+}(t) - v^{-}(t) = ( (t + \\sqrt{t^2+r^2}) - (-t - \\sqrt{t^2+r^2}), r-r, 0-0, \\dots)^{\\top} = (2t + 2\\sqrt{t^2+r^2}, 0, \\dots, 0)^{\\top}\n$$\nSo, $u^{+}(t) - u^{-}(t) = \\frac{(2(t + \\sqrt{t^2+r^2}), 0, \\dots, 0)^{\\top}}{D(t)}$.\n\nThe norm of this difference is:\n$$\n\\|u^{+}(t) - u^{-}(t)\\|_2 = \\frac{\\| (2(t + \\sqrt{t^2+r^2}), 0, \\dots, 0)^{\\top} \\|_2}{D(t)} = \\frac{2(t + \\sqrt{t^2+r^2})}{D(t)}\n$$\nsince $t0$ and the square root term is positive.\nSubstituting the expression for $D(t) = \\sqrt{2\\sqrt{t^2+r^2}(t + \\sqrt{t^2+r^2})}$:\n$$\n\\|u^{+}(t) - u^{-}(t)\\|_2 = \\frac{2(t + \\sqrt{t^2+r^2})}{\\sqrt{2\\sqrt{t^2+r^2}(t + \\sqrt{t^2+r^2})}} = \\sqrt{\\frac{4(t + \\sqrt{t^2+r^2})^2}{2\\sqrt{t^2+r^2}(t + \\sqrt{t^2+r^2})}}\n$$\n$$\n= \\sqrt{\\frac{2(t + \\sqrt{t^2+r^2})}{\\sqrt{t^2+r^2}}}\n$$\nFinally, we compute the limit as $t \\to 0^+$ to find the jump magnitude $J$:\n$$\nJ = \\lim_{t \\to 0^+} \\sqrt{\\frac{2(t + \\sqrt{t^2+r^2})}{\\sqrt{t^2+r^2}}}\n$$\nSince the function inside the square root is continuous at $t=0$ (given $r0$), we can substitute $t=0$:\n$$\nJ = \\sqrt{\\frac{2(0 + \\sqrt{0^2+r^2})}{\\sqrt{0^2+r^2}}} = \\sqrt{\\frac{2\\sqrt{r^2}}{\\sqrt{r^2}}}\n$$\nAs $r  0$, $\\sqrt{r^2}=r$.\n$$\nJ = \\sqrt{\\frac{2r}{r}} = \\sqrt{2}\n$$\nThe jump magnitude is $\\sqrt{2}$.\n\nThis result has important implications for the differentiability of the map $x \\mapsto u(x)$ and the stability of algorithms using it.\nThe limits of $u(x)$ as $x$ approaches $x_0$ along the two paths $x^+(t)$ and $x^-(t)$ are different:\n$$\n\\lim_{t \\to 0^+} u^{+}(t) = \\frac{(r,r,0,\\dots)^\\top}{\\sqrt{2r^2}} = \\frac{1}{\\sqrt{2}}(1,1,0,\\dots)^\\top\n$$\n$$\n\\lim_{t \\to 0^+} u^{-}(t) = \\frac{(-r,r,0,\\dots)^\\top}{\\sqrt{2r^2}} = \\frac{1}{\\sqrt{2}}(-1,1,0,\\dots)^\\top\n$$\nSince the limit depends on the path of approach, the function $u(x)$ is not continuous at any point $x_0$ where $x_{0,1}=0$. A function cannot be differentiable at a point where it is not continuous. Therefore, the map $x \\mapsto u(x)$ is not differentiable on the hyperplane $x_1=0$.\n\nThe non-zero jump magnitude $J = \\sqrt{2}$ quantifies this discontinuity. In numerical algorithms like Householder QR factorization, this is a source of instability. If an input vector $x$ has a first component $x_1$ that is very close to $0$, a small perturbation to $x$ (e.g., from floating-point rounding error) could change the sign of $x_1$. This sign change causes the calculated vector $u(x)$ to jump between two distinct vectors separated by a distance of approximately $\\sqrt{2}$. This large, discontinuous change in $u(x)$ for a small change in $x$ can degrade the accuracy and stability of the overall algorithm.", "answer": "$$\\boxed{\\sqrt{2}}$$", "id": "3572880"}, {"introduction": "Beyond their individual properties, reflectors are essential building blocks for many of the most important algorithms in numerical linear algebra. This advanced practice demonstrates their power in the context of a blocked Hessenberg reduction, a crucial precursor to modern eigenvalue solvers. You will implement the algorithm using the compact WY representation to aggregate reflectors for performance, and in doing so, explore the trade-offs between algorithmic choices like block size and their impact on numerical phenomena such as intermediate \"bulge\" and deflation [@problem_id:3572907].", "problem": "Given a real square matrix $A \\in \\mathbb{R}^{n \\times n}$, consider similarity transformations built from Householder reflectors. A Householder reflector is any orthogonal matrix of the form $H = I - \\tau v v^\\top$ where $v \\in \\mathbb{R}^m$ is a nonzero vector, $\\tau \\in \\mathbb{R}$ is a scalar, and $I$ denotes the identity matrix of appropriate size. An upper Hessenberg matrix is a matrix $H \\in \\mathbb{R}^{n \\times n}$ such that $H_{i,j} = 0$ for all $i  j + 1$.\n\nYour task is to implement a blocked reduction of $A$ to upper Hessenberg form by a similarity transformation $A \\mapsto Q^\\top A Q$, where $Q$ is an orthogonal factor represented implicitly as a product of Householder reflectors aggregated panel-wise using a compact representation. The aggregation should follow the compact product form $Q_{\\text{panel}} = I - Y T Y^\\top$ for each panel, where $Y \\in \\mathbb{R}^{p \\times b}$ collects the $b$ Householder vectors (restricted to the active rows in the panel) and $T \\in \\mathbb{R}^{b \\times b}$ is an upper triangular matrix that depends only on the Householder coefficients and the inner products among the columns of $Y$. The reduction proceeds by partitioning the columns into panels of size $b$, applying the left Householder updates immediately within the panel, and aggregating the right updates so that they are applied once per panel to the relevant trailing columns.\n\nTo assess the effect of panel aggregation (block size) on transient fill, define the subdiagonal bulge magnitude at any intermediate step of the panel sweep as the Frobenius norm of the strictly lower part of the matrix below the first subdiagonal, i.e., the Frobenius norm of $\\operatorname{tril}(M, -2)$ where $M$ is the current in-progress matrix during the processing of a panel before its aggregated right update is applied. For a given block size $b$, define the panel bulge statistic as the maximum of these norms taken over all intermediate steps across all panels. After the full blocked reduction produces an upper Hessenberg matrix $H$, perform a single unshifted orthogonal-triangular (QR) step on $H$ to obtain $H_+ = R Q$ where $H = Q R$ is the orthogonal-triangular factorization. Define a deflation threshold $\\theta = \\text{reltol} \\cdot \\|H_+\\|_1$, where $\\|\\cdot\\|_1$ is the induced matrix $1$-norm. The deflation count is the number of subdiagonal entries $H_+^{(i+1,i)}$ with $|H_+^{(i+1,i)}| \\le \\theta$ for $i \\in \\{1,2,\\ldots,n-1\\}$.\n\nStarting only from the core definitions above (Householder reflector as $H = I - \\tau v v^\\top$, orthogonal-triangular factorization, and upper Hessenberg structure) and the well-tested fact that a product of Householder reflectors can be expressed in a compact form $I - Y T Y^\\top$ with $T$ upper triangular determined recursively by the reflectors, derive a principled algorithm that:\n- Computes a blocked similarity reduction $A \\mapsto H$ to upper Hessenberg form with panel size $b$ by immediately applying left Householder updates and aggregating the corresponding right updates via $I - Y T Y^\\top$ per panel,\n- Tracks the maximum Frobenius norm of the strictly lower part below the first subdiagonal at intermediate states before the panel’s aggregated right update (the subdiagonal bulge magnitude),\n- Performs one unshifted orthogonal-triangular (QR) step on the final $H$ and counts the number of subdiagonal entries satisfying the deflation criterion described above.\n\nImplement the algorithm as a complete program that produces quantitative outputs for the following fixed test suite. In each test case the same real matrix $A \\in \\mathbb{R}^{n \\times n}$ is used, generated deterministically from a fixed seed. Use $n = 8$ and let $A$ be filled with independent standard normal entries generated by a fixed random seed $s = 42$ so that the test is reproducible. Use the deflation relative tolerance $\\text{reltol} = 10^{-12}$. The test cases vary the block size $b$:\n- Test $1$: block size $b = 1$,\n- Test $2$: block size $b = 2$,\n- Test $3$: block size $b = 6$.\n\nFor each test case, your program must output a pair $[\\text{bulge}, \\text{defl}]$, where $\\text{bulge}$ is the maximum subdiagonal bulge magnitude rounded to eight decimal places, and $\\text{defl}$ is the deflation count as an integer. Your program should produce a single line of output containing the results for the three tests as a comma-separated list enclosed in square brackets, for example, $[[x_1,y_1],[x_2,y_2],[x_3,y_3]]$, where $x_k$ are floats and $y_k$ are integers. No physical units are involved. Angles are not used. Percentages are not used; any ratios must be represented as decimal numbers.", "solution": "## Problem Validation\n\n### Step 1: Extract Givens\n- **Matrix and Transformation**: A real square matrix $A \\in \\mathbb{R}^{n \\times n}$ is reduced to an upper Hessenberg matrix $H$ via a similarity transformation $A \\mapsto Q^\\top A Q$.\n- **Matrix Dimension**: $n = 8$.\n- **Matrix Generation**: $A$ is filled with independent standard normal entries generated using a fixed random seed $s = 42$.\n- **Householder Reflector**: An orthogonal matrix of the form $H = I - \\tau v v^\\top$, where $v \\in \\mathbb{R}^m$ is a nonzero vector and $\\tau \\in \\mathbb{R}$ is a scalar.\n- **Upper Hessenberg Form**: A matrix $H \\in \\mathbb{R}^{n \\times n}$ with $H_{i,j} = 0$ for all $i  j + 1$.\n- **Blocked Algorithm**: The reduction is performed using a blocked (paneled) approach with a specified block size $b$.\n- **Panel Aggregation**: Within each panel, reflectors are aggregated into a compact product form $Q_{\\text{panel}} = I - Y T Y^\\top$, where $Y$ collects Householder vectors and $T$ is an upper triangular matrix.\n- **Update Scheme**: Left Householder updates are applied immediately within a panel. Right updates are aggregated and applied once per panel.\n- **Metric 1 (Subdiagonal Bulge Magnitude)**: At any intermediate step within a panel's processing (i.e., after a left update but before the panel's aggregated right update), the bulge is the Frobenius norm of the strictly lower part of the matrix below the first subdiagonal, $\\|\\operatorname{tril}(M, -2)\\|_F$. The panel bulge statistic is the maximum of these norms over all intermediate steps and all panels.\n- **Metric 2 (Deflation Count)**: After obtaining the final Hessenberg matrix $H$, one unshifted QR step is performed: $H = QR$ followed by $H_+ = RQ$. The deflation count is the number of subdiagonal entries $|H_+^{(i+1,i)}|$ satisfying $|H_+^{(i+1,i)}| \\le \\theta$, where the threshold is $\\theta = \\text{reltol} \\cdot \\|H_+\\|_1$.\n- **Deflation Tolerance**: $\\text{reltol} = 10^{-12}$.\n- **Test Cases**: The algorithm is to be tested for block sizes $b \\in \\{1, 2, 6\\}$.\n- **Output Format**: For each test case, a pair $[\\text{bulge}, \\text{defl}]$ is required, with the bulge rounded to eight decimal places. The final output is a single line: `[[bulge1,defl1],[bulge2,defl2],[bulge3,defl3]]`.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientific Grounding**: The problem is firmly rooted in numerical linear algebra, specifically in the context of algorithms for solving eigenvalue problems. Blocked Hessenberg reduction, Householder reflectors, the compact WY representation ($I - YTY^\\top$), and the QR algorithm are all standard, well-established concepts. The problem does not violate any scientific or mathematical principles.\n- **Well-Posedness**: The task is to implement a deterministic algorithm. For a given input matrix $A$ and block size $b$, the sequence of operations is uniquely defined, leading to a unique Hessenberg matrix $H$ (up to signs in the intermediate reflectors which do not affect the final similarity transformation) and consequently unique values for the bulge statistic and deflation count.\n- **Objectivity**: All terms and metrics are defined with mathematical precision. The problem is devoid of subjective language or opinion-based claims.\n- **Completeness and Consistency**: The problem statement is self-contained. It provides all necessary parameters ($n$, seed $s$, $\\text{reltol}$), definitions (Householder reflector, Hessenberg form, bulge, deflation criterion), and algorithmic constraints (blocked structure, update scheme) to proceed with a unique implementation. The constraints are consistent with standard numerical algorithms.\n\n### Step 3: Verdict and Action\nThe problem is valid. It is a well-posed, scientifically grounded, and objective task from the field of numerical linear algebra. A principled derivation and implementation will be developed.\n\n## Algorithmic Derivation and Solution\n\nThe core of the problem is the reduction of a matrix $A$ to upper Hessenberg form $H$ using an orthogonal similarity transformation $Q$, such that $H = Q^\\top A Q$. A matrix $H$ is upper Hessenberg if all its entries below the first subdiagonal are zero, i.e., $H_{i,j}=0$ for $i > j+1$.\n\nThe transformation matrix $Q$ is constructed as a product of Householder reflectors. The reduction proceeds column by column. For each column $j$, from $0$ to $n-2$, we introduce zeros in positions $(j+2, j), \\dots, (n-1, j)$. This is achieved by a Householder reflector $H_j$ that acts on rows $j+1$ through $n-1$. A Householder reflector is a matrix of the form $H = I - \\tau v v^\\top$ for a nonzero vector $v$ and a scalar $\\tau = 2/(v^\\top v)$. Given a vector $x$, a reflector can be constructed to transform $x$ into a multiple of the first standard basis vector, i.e., $H x = \\alpha e_1$. This is the fundamental operation for introducing zeros.\n\nA standard (unblocked) Hessenberg reduction applies these reflectors one by one:\n$$A^{(j+1)} = H_j^\\top A^{(j)} H_j = H_j A^{(j)} H_j$$\nsince $H_j$ is symmetric ($H_j = H_j^\\top$) and orthogonal ($H_j^\\top H_j = I$). The final Hessenberg matrix is $H = A^{(n-1)}$ and the transformation is $Q = H_0 H_1 \\cdots H_{n-2}$.\n\nThe blocked algorithm aims to improve performance by replacing matrix-vector operations with more efficient matrix-matrix operations. This is achieved by grouping columns into \"panels\" of size $b$. For a panel of columns from $j_{\\text{start}}$ to $j_{\\text{end}}-1$, the algorithm proceeds differently from the unblocked version. The left multiplications by reflectors, $H_j A$, are performed immediately as they are generated. However, the right multiplications, $A H_j$, are aggregated and applied once for the entire panel.\n\nLet's detail the process for a single panel starting at column $j_{\\text{start}}$ with block size $b$. Let $j_k = j_{\\text{start}}+k$ for $k=0, \\ldots, b-1$.\nThe transformation for this panel is $Q_{\\text{panel}} = H_{j_0} H_{j_1} \\cdots H_{j_{b-1}}$. The matrix is updated as:\n$$A \\rightarrow (H_{j_{b-1}} \\cdots H_{j_0}) A (H_{j_0} \\cdots H_{j_{b-1}})$$\nThe blocked algorithm computes this in stages. Within the panel, for $k=0, \\ldots, b-1$:\n1. Determine the reflector $H_{j_k} = I - \\tau_{j_k} v_{j_k} v_{j_k}^\\top$ based on the current state of column $j_k$ of the matrix.\n2. Apply the left update immediately: $A \\leftarrow H_{j_k} A$. This update creates \"fill-in\"—non-zero elements below the desired Hessenberg structure. This transient fill-in is the \"subdiagonal bulge\". At each of these intermediate steps, we measure its magnitude $\\|\\operatorname{tril}(A,-2)\\|_F$.\n3. The reflector $H_{j_k}$ is stored for later use in the aggregated right update.\n\nAfter all left updates for the panel are complete, the matrix has been transformed to $(H_{j_{b-1}} \\cdots H_{j_0}) A$. We must then apply the right update by $Q_{\\text{panel}} = H_{j_0} \\cdots H_{j_{b-1}}$. A direct product of the $H_{j_k}$ matrices would be inefficient. Instead, we use the compact WY representation, which expresses the product of reflectors as a single, structured transformation. As stated in the problem, this form is $Q_{\\text{panel}} = I - Y T Y^\\top$.\n- $Y$ is a matrix whose columns are the Householder vectors $v_{j_k}$.\n- $T$ is an upper triangular matrix that combines the scalar coefficients $\\tau_{j_k}$ and the inner products of the Householder vectors.\n\nThe matrix $T$ can be constructed recursively. Let $Q_k = H_{j_0} \\cdots H_{j_{k-1}} = I - Y_{k-1} T_{k-1} Y_{k-1}^\\top$. Then,\n$$Q_{k+1} = Q_k H_{j_k} = (I - Y_{k-1} T_{k-1} Y_{k-1}^\\top)(I - \\tau_{j_k} v_{j_k} v_{j_k}^\\top)$$\nExpanding this product and rearranging terms yields the update rule for $T$:\n$$T_k = \\begin{pmatrix} T_{k-1}  -\\tau_{j_k} T_{k-1} (Y_{k-1}^\\top v_{j_k}) \\\\ 0  \\tau_{j_k} \\end{pmatrix}$$\nThis shows that $T$ is indeed upper triangular. In an implementation, as we generate each vector $v_{j_k}$ and scalar $\\tau_{j_k}$, we can compute the $k$-th column of $T$. Specifically, $T_{k,k} = \\tau_{j_k}$, and the elements above it are $T_{0:k, k} = -\\tau_{j_k} T_{0:k, 0:k} (Y_{:, 0:k}^\\top v_{j_k})$.\n\nOnce $Y$ and $T$ are fully constructed for the panel, the aggregated right update is applied as a Level-3 BLAS operation:\n$A \\leftarrow A Q_{\\text{panel}} = A (I - Y T Y^\\top) = A - (A Y) T Y^\\top$\nLet $W = AY$. The update is $A \\leftarrow A - W T Y^\\top$. This single matrix-matrix update restores the Hessenberg structure for the columns in the processed panel, effectively removing the subdiagonal bulge created by the series of left updates.\n\nAfter iterating through all panels, the matrix $A$ is transformed into the final upper Hessenberg matrix $H$. The second part of the task involves analyzing $H$.\nWe perform one step of the unshifted QR algorithm. First, we compute the QR factorization of $H$: $H=QR$, where $Q$ is orthogonal and $R$ is upper triangular. Then, we reverse the order of multiplication to obtain the next iterate: $H_+ = RQ$.\nFinally, we count the number of \"negligible\" subdiagonal entries in $H_+$, which is a heuristic for detecting decoupled subproblems in eigenvalue algorithms. An entry $H_+^{(i+1,i)}$ is considered negligible if its magnitude is smaller than a threshold $\\theta$ relative to the overall magnitude of the matrix. The specified threshold is $\\theta = \\text{reltol} \\cdot \\|H_+\\|_1$, where $\\|H_+\\|_1 = \\max_j \\sum_i |H_+^{(i,j)}|$ is the induced $1$-norm. The deflation count is the number of indices $i \\in \\{1,\\ldots,n-1\\}$ such that $|H_+^{(i+1,i)}| \\le \\theta$.\n\nThe implementation will follow this derived procedure for each specified block size $b$.", "answer": "```python\nimport numpy as np\n\ndef hessenberg_reduction_blocked(A_init, b, reltol):\n    \"\"\"\n    Performs blocked Hessenberg reduction on matrix A with block size b.\n\n    Args:\n        A_init (np.ndarray): The initial square matrix.\n        b (int): The block size for panel processing.\n        reltol (float): The relative tolerance for the deflation check.\n\n    Returns:\n        list: A list containing two elements:\n              - The maximum subdiagonal bulge magnitude (float, rounded).\n              - The deflation count after one QR step (int).\n    \"\"\"\n    n = A_init.shape[0]\n    A = A_init.copy()\n    max_bulge = 0.0\n\n    # Main loop over panels, striding by block size b\n    for j_start in range(0, n - 1, b):\n        j_end = min(j_start + b, n - 1)\n        b_panel = j_end - j_start\n\n        if b_panel == 0:\n            continue\n\n        Y = np.zeros((n, b_panel))\n        T = np.zeros((b_panel, b_panel))\n\n        # Inner loop over columns within the panel\n        for k in range(b_panel):\n            j = j_start + k\n\n            # Extract the vector to be modified by the reflector\n            x = A[j+1:, j]\n            m = len(x)\n\n            if m = 0:\n                # No elements to zero, reflector is identity\n                T[k, k] = 0.0\n                # Y is initialized to zeros, so Y[:,k] is zero\n                # No left update, no bulge change from this column\n                bulge = np.linalg.norm(np.tril(A, -2))\n                max_bulge = max(max_bulge, bulge)\n                continue\n\n            # Compute Householder vector u and scalar tau\n            x_norm = np.linalg.norm(x)\n            \n            # Using a small machine epsilon-like tolerance\n            if x_norm > 1e-15:\n                # Standard numerically stable Householder construction\n                sigma = np.copysign(1.0, x[0] if x[0] != 0 else 1.0)\n                u = x.copy()\n                u[0] += sigma * x_norm\n                tau = 2.0 / (u @ u)\n\n                # Store the full Householder vector in Y\n                v = np.zeros(n)\n                v[j+1:] = u\n                Y[:, k] = v\n\n                # Apply immediate left update: A = H_k * A = (I - tau*v*v.T) * A\n                w = tau * (v.T @ A)\n                A -= np.outer(v, w)\n\n                # Explicitly set the zeroed elements to avoid floating point errors\n                A[j+1, j] = -sigma * x_norm\n                if m > 1:\n                    A[j+2:, j] = 0.0\n                \n                # Update T for the compact WY representation\n                T[k, k] = tau\n                if k > 0:\n                    yTv_col = Y[:, :k].T @ v\n                    z_col = T[:k, :k] @ yTv_col\n                    T[:k, k] = -tau * z_col\n            else:\n                # x is already zeroed out, reflector is identity\n                T[k, k] = 0.0\n                # Y[:,k] remains zero\n            \n            # Measure subdiagonal bulge AFTER the left update\n            bulge = np.linalg.norm(np.tril(A, -2))\n            max_bulge = max(max_bulge, bulge)\n\n        # Apply aggregated right update for the panel: A = A * Q_panel\n        # Q_panel = I - Y * T * Y.T\n        W = A @ Y\n        A -= (W @ T) @ Y.T\n\n    H = A\n\n    # Perform one unshifted QR step on the final Hessenberg matrix H\n    if n > 0:\n        Q, R = np.linalg.qr(H)\n        H_plus = R @ Q\n\n        # Calculate deflation count\n        one_norm = np.linalg.norm(H_plus, 1)\n        threshold = reltol * one_norm\n        subdiag = np.diag(H_plus, k=-1)\n        defl_count = np.sum(np.abs(subdiag) = threshold)\n    else:\n        defl_count = 0\n\n    return [round(max_bulge, 8), int(defl_count)]\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    n = 8\n    seed = 42\n    reltol = 1e-12\n    test_cases = [\n        1,  # block size b = 1\n        2,  # block size b = 2\n        6,  # block size b = 6\n    ]\n\n    # Generate the single test matrix A used for all cases\n    rng = np.random.default_rng(seed)\n    A_init = rng.standard_normal((n, n))\n\n    results = []\n    for b in test_cases:\n        result = hessenberg_reduction_blocked(A_init, b, reltol)\n        results.append(result)\n\n    # Format the output string exactly as specified\n    formatted_results = [f\"[{res[0]:.8f},{res[1]}]\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3572907"}]}