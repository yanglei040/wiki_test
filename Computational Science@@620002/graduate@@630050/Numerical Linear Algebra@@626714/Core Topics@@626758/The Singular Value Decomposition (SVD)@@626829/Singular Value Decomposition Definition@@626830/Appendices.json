{"hands_on_practices": [{"introduction": "To build a solid understanding of the Singular Value Decomposition (SVD), it is instructive to begin with matrices that represent fundamental geometric operations. This first practice explores the SVD of two special cases: a diagonal matrix representing non-uniform scaling, and an orthogonal matrix representing a pure rotation [@problem_id:3275102]. By working through these examples, you will see how the components of the SVD—$U$, $\\Sigma$, and $V$—behave in these pure scenarios and appreciate the importance of the convention for ordering singular values.", "problem": "Let $A_{1} \\in \\mathbb{R}^{2 \\times 2}$ be the diagonal matrix $A_{1} = \\begin{pmatrix} 2 & 0 \\\\ 0 & 5 \\end{pmatrix}$ and let $A_{2} \\in \\mathbb{R}^{2 \\times 2}$ be the orthogonal rotation matrix $A_{2} = \\begin{pmatrix} \\cos\\left(\\frac{\\pi}{7}\\right) & -\\sin\\left(\\frac{\\pi}{7}\\right) \\\\ \\sin\\left(\\frac{\\pi}{7}\\right) & \\cos\\left(\\frac{\\pi}{7}\\right) \\end{pmatrix}$. Using only the definition of the Singular Value Decomposition (SVD), derive explicit SVDs for $A_{1}$ and $A_{2}$ with the canonical convention that singular values are nonnegative and ordered nonincreasing. That is, find orthogonal matrices $U_{1}, V_{1}, U_{2}, V_{2}$ and diagonal matrices with nonnegative diagonal entries $\\Sigma_{1}, \\Sigma_{2}$ such that $A_{1} = U_{1} \\Sigma_{1} V_{1}^{\\top}$ and $A_{2} = U_{2} \\Sigma_{2} V_{2}^{\\top}$, and the diagonal entries of $\\Sigma_{1}$ and $\\Sigma_{2}$ are sorted in descending order. Report your final answer as a single row containing the six matrices in the order $U_{1}, \\Sigma_{1}, V_{1}^{\\top}, U_{2}, \\Sigma_{2}, V_{2}^{\\top}$. No rounding is required.", "solution": "The problem is to find the Singular Value Decomposition (SVD) for two given matrices, $A_{1} = \\begin{pmatrix} 2 & 0 \\\\ 0 & 5 \\end{pmatrix}$ and $A_{2} = \\begin{pmatrix} \\cos\\left(\\frac{\\pi}{7}\\right) & -\\sin\\left(\\frac{\\pi}{7}\\right) \\\\ \\sin\\left(\\frac{\\pi}{7}\\right) & \\cos\\left(\\frac{\\pi}{7}\\right) \\end{pmatrix}$, such that $A = U \\Sigma V^{\\top}$. The matrices $U$ and $V$ must be orthogonal, and $\\Sigma$ must be a diagonal matrix with non-negative entries, ordered non-increasingly.\n\nFirst, we consider the matrix $A_{1} = \\begin{pmatrix} 2 & 0 \\\\ 0 & 5 \\end{pmatrix}$. This is a diagonal matrix. The singular values are, by definition, the square roots of the eigenvalues of the matrix $A_{1}^{\\top}A_{1}$.\n$$A_{1}^{\\top}A_{1} = \\begin{pmatrix} 2 & 0 \\\\ 0 & 5 \\end{pmatrix} \\begin{pmatrix} 2 & 0 \\\\ 0 & 5 \\end{pmatrix} = \\begin{pmatrix} 4 & 0 \\\\ 0 & 25 \\end{pmatrix}$$\nThe eigenvalues of this diagonal matrix are its diagonal entries, $\\lambda_a = 25$ and $\\lambda_b = 4$. The singular values are their square roots, $\\sigma_a = \\sqrt{25} = 5$ and $\\sigma_b = \\sqrt{4} = 2$.\nFollowing the convention that singular values are ordered non-increasingly, we have $\\sigma_1 = 5$ and $\\sigma_2 = 2$. The matrix $\\Sigma_{1}$ is therefore:\n$$\\Sigma_{1} = \\begin{pmatrix} 5 & 0 \\\\ 0 & 2 \\end{pmatrix}$$\nWe seek orthogonal matrices $U_{1}$ and $V_{1}$ such that $A_{1} = U_{1}\\Sigma_{1}V_{1}^{\\top}$. We can see that $\\Sigma_{1}$ is simply $A_{1}$ with its diagonal elements swapped. This suggests a permutation. Let $P$ be the permutation matrix that swaps the first and second basis vectors:\n$$P = \\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix}$$\nThis matrix is orthogonal ($P^{\\top}P = I$) and also symmetric ($P = P^{\\top}$). Let's apply this permutation to $A_{1}$:\n$$P A_{1} P^{\\top} = \\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix} \\begin{pmatrix} 2 & 0 \\\\ 0 & 5 \\end{pmatrix} \\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix} = \\begin{pmatrix} 0 & 5 \\\\ 2 & 0 \\end{pmatrix} \\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix} = \\begin{pmatrix} 5 & 0 \\\\ 0 & 2 \\end{pmatrix} = \\Sigma_{1}$$\nFrom the equation $P A_{1} P^{\\top} = \\Sigma_{1}$, we can solve for $A_{1}$. Since $P^{-1} = P^{\\top} = P$, we have $A_{1} = P^{-1} \\Sigma_{1} (P^{\\top})^{-1} = P \\Sigma_{1} P^{\\top}$.\nThis expression is in the form of an SVD, $A_{1} = U_{1} \\Sigma_{1} V_{1}^{\\top}$. We can identify $U_{1} = P$ and $V_{1}^{\\top} = P^{\\top} = P$. Both $U_{1}$ and $V_{1}=P$ are orthogonal matrices.\nThus, the SVD components for $A_{1}$ are:\n$$U_{1} = \\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix}, \\quad \\Sigma_{1} = \\begin{pmatrix} 5 & 0 \\\\ 0 & 2 \\end{pmatrix}, \\quad V_{1}^{\\top} = \\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix}$$\n\nNext, we consider the matrix $A_{2} = \\begin{pmatrix} \\cos\\left(\\frac{\\pi}{7}\\right) & -\\sin\\left(\\frac{\\pi}{7}\\right) \\\\ \\sin\\left(\\frac{\\pi}{7}\\right) & \\cos\\left(\\frac{\\pi}{7}\\right) \\end{pmatrix}$. This is an orthogonal rotation matrix.\nTo find its singular values, we compute $A_{2}^{\\top}A_{2}$. Since $A_{2}$ is orthogonal, $A_{2}^{\\top}A_{2} = I$, the identity matrix.\n$$A_{2}^{\\top}A_{2} = I = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}$$\nThe eigenvalues of the identity matrix are both $1$. The singular values are the square roots of these eigenvalues, so $\\sigma_1 = \\sqrt{1} = 1$ and $\\sigma_2 = \\sqrt{1} = 1$. The singular values are already sorted non-increasingly. The matrix $\\Sigma_{2}$ is:\n$$\\Sigma_{2} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} = I$$\nThe SVD equation for $A_{2}$ is $A_{2} = U_{2}\\Sigma_{2}V_{2}^{\\top} = U_{2} I V_{2}^{\\top} = U_{2}V_{2}^{\\top}$.\nWe need to find two orthogonal matrices $U_{2}$ and $V_{2}$ such that their product gives $A_{2}$. Since $A_{2}$ is itself an orthogonal matrix, a direct and valid choice based on the definition is to set $U_{2}$ to be $A_{2}$ and $V_{2}$ to be the identity matrix.\nIf we choose $U_{2} = A_{2}$ and $V_{2} = I$, then we have:\n- $U_{2} = A_{2}$ is orthogonal, as given.\n- $V_{2} = I$ is orthogonal.\n- $V_{2}^{\\top} = I^{\\top} = I$.\n- The decomposition is satisfied: $U_{2} \\Sigma_{2} V_{2}^{\\top} = A_{2} \\cdot I \\cdot I = A_{2}$.\nThis constitutes a valid SVD for $A_{2}$. Thus, the components are:\n$$U_{2} = \\begin{pmatrix} \\cos\\left(\\frac{\\pi}{7}\\right) & -\\sin\\left(\\frac{\\pi}{7}\\right) \\\\ \\sin\\left(\\frac{\\pi}{7}\\right) & \\cos\\left(\\frac{\\pi}{7}\\right) \\end{pmatrix}, \\quad \\Sigma_{2} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}, \\quad V_{2}^{\\top} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}$$\nThe final answer consists of the six matrices $U_1, \\Sigma_1, V_1^{\\top}, U_2, \\Sigma_2, V_2^{\\top}$ in order.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix} & \\begin{pmatrix} 5 & 0 \\\\ 0 & 2 \\end{pmatrix} & \\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix} & \\begin{pmatrix} \\cos\\left(\\frac{\\pi}{7}\\right) & -\\sin\\left(\\frac{\\pi}{7}\\right) \\\\ \\sin\\left(\\frac{\\pi}{7}\\right) & \\cos\\left(\\frac{\\pi}{7}\\right) \\end{pmatrix} & \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} & \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}\n\\end{pmatrix}\n}\n$$", "id": "3275102"}, {"introduction": "The power of the SVD extends beyond simple geometric transformations to the analysis of more abstract operators. This exercise focuses on the orthogonal projection matrix $P = A(A^{\\top}A)^{-1}A^{\\top}$, a cornerstone of linear regression and data analysis [@problem_id:3206009]. You will use the SVD of the matrix $A$ to uncover the spectral properties of $P$, revealing that its singular values are exclusively $1$s and $0$s, which perfectly captures the geometric essence of a projection.", "problem": "Let $A \\in \\mathbb{R}^{m \\times n}$ with $m \\geq n$ and $\\operatorname{rank}(A)=n$. Define the matrix $P \\in \\mathbb{R}^{m \\times m}$ by $P = A\\left(A^{T}A\\right)^{-1}A^{T}$. Using only the definition of the singular value decomposition (SVD) and basic properties of orthogonal matrices, analyze the singular values of $P$ by expressing $P$ in terms of the SVD of $A$ and identifying its action on $\\mathbb{R}^{m}$. Then, using your characterization, compute the determinant $\\det\\!\\left(I_{m}+P\\right)$ as a function of $n$ and $m$. Finally, evaluate your expression for $m=13$ and $n=5$. Provide the final value as a single number. No rounding is required.", "solution": "The problem asks for an analysis of the singular values of the projection matrix $P = A\\left(A^{T}A\\right)^{-1}A^{T}$ and for the computation of $\\det\\!\\left(I_{m}+P\\right)$. The matrix $A \\in \\mathbb{R}^{m \\times n}$ is given to have $m \\geq n$ and $\\operatorname{rank}(A)=n$.\n\nFirst, we express the matrix $P$ in terms of the singular value decomposition (SVD) of $A$. Let the SVD of $A$ be $A = U \\Sigma V^T$, where:\n- $U$ is an $m \\times m$ orthogonal matrix ($U^T U = I_m$).\n- $V$ is an $n \\times n$ orthogonal matrix ($V^T V = I_n$).\n- $\\Sigma$ is an $m \\times n$ rectangular diagonal matrix of singular values $\\sigma_i$.\n\nGiven that $\\operatorname{rank}(A)=n$, all $n$ singular values of $A$ are strictly positive. The matrix $\\Sigma$ can be written in block form as:\n$$\n\\Sigma = \\begin{pmatrix} \\Sigma_1 \\\\ 0_{(m-n) \\times n} \\end{pmatrix}\n$$\nwhere $\\Sigma_1 = \\operatorname{diag}(\\sigma_1, \\sigma_2, \\dots, \\sigma_n)$ is an $n \\times n$ invertible diagonal matrix.\n\nWe substitute the SVD of $A$ into the components of $P$. First, we compute $A^T A$:\n$$\nA^T A = (U \\Sigma V^T)^T(U \\Sigma V^T) = V \\Sigma^T U^T U \\Sigma V^T = V (\\Sigma^T \\Sigma) V^T\n$$\nThe product $\\Sigma^T \\Sigma$ is an $n \\times n$ matrix:\n$$\n\\Sigma^T \\Sigma = \\begin{pmatrix} \\Sigma_1^T & 0_{n \\times (m-n)} \\end{pmatrix} \\begin{pmatrix} \\Sigma_1 \\\\ 0_{(m-n) \\times n} \\end{pmatrix} = \\Sigma_1^T \\Sigma_1 = \\Sigma_1^2\n$$\nSo, $A^T A = V \\Sigma_1^2 V^T$. Since $\\operatorname{rank}(A)=n$, all $\\sigma_i > 0$, so $\\Sigma_1$ and $\\Sigma_1^2$ are invertible. Now we find the inverse of $A^T A$:\n$$\n(A^T A)^{-1} = (V \\Sigma_1^2 V^T)^{-1} = (V^T)^{-1} (\\Sigma_1^2)^{-1} V^{-1} = V \\Sigma_1^{-2} V^T\n$$\nwhere we used the fact that $V$ is orthogonal, so $V^{-1}=V^T$.\n\nNow, we assemble the matrix $P$:\n$$\nP = A (A^T A)^{-1} A^T = (U \\Sigma V^T)(V \\Sigma_1^{-2} V^T)(U \\Sigma V^T)^T = (U \\Sigma V^T)(V \\Sigma_1^{-2} V^T)(V \\Sigma^T U^T)\n$$\nUsing the identity $V^T V = I_n$, the expression simplifies to:\n$$\nP = U (\\Sigma \\Sigma_1^{-2} \\Sigma^T) U^T\n$$\nWe compute the central product $\\Sigma \\Sigma_1^{-2} \\Sigma^T$:\n$$\n\\Sigma \\Sigma_1^{-2} \\Sigma^T = \\begin{pmatrix} \\Sigma_1 \\\\ 0 \\end{pmatrix} \\Sigma_1^{-2} \\begin{pmatrix} \\Sigma_1 & 0 \\end{pmatrix} = \\begin{pmatrix} \\Sigma_1 \\Sigma_1^{-2} \\\\ 0 \\end{pmatrix} \\begin{pmatrix} \\Sigma_1 & 0 \\end{pmatrix} = \\begin{pmatrix} \\Sigma_1^{-1} \\\\ 0 \\end{pmatrix} \\begin{pmatrix} \\Sigma_1 & 0 \\end{pmatrix}\n$$\nThis results in an $m \\times m$ block matrix:\n$$\n\\begin{pmatrix} \\Sigma_1^{-1}\\Sigma_1 & \\Sigma_1^{-1}0 \\\\ 0 \\cdot \\Sigma_1 & 0 \\cdot 0 \\end{pmatrix} = \\begin{pmatrix} I_n & 0_{n \\times (m-n)} \\\\ 0_{(m-n) \\times n} & 0_{(m-n) \\times (m-n)} \\end{pmatrix}\n$$\nLet this matrix be denoted by $D_P$. Therefore, $P = U D_P U^T$. This is a spectral decomposition of $P$. The matrix $P$ is symmetric and idempotent ($P^2=P$), which characterizes it as an orthogonal projection matrix. For a symmetric positive semi-definite matrix, the singular values are equal to the eigenvalues. The eigenvalues of $P$ are the eigenvalues of $D_P$, which are its diagonal entries. Thus, the eigenvalues of $P$ are $1$ (with multiplicity $n$) and $0$ (with multiplicity $m-n$). The singular values of $P$ are likewise $n$ ones and $m-n$ zeros.\n\nNext, we compute $\\det(I_m + P)$. The determinant of a matrix is the product of its eigenvalues. If $\\lambda$ is an eigenvalue of $P$, then $1+\\lambda$ is an eigenvalue of $I_m + P$. The eigenvalues of $I_m+P$ are therefore:\n- $1+1=2$, with multiplicity $n$.\n- $1+0=1$, with multiplicity $m-n$.\n\nThe determinant is the product of these eigenvalues:\n$$\n\\det(I_m + P) = \\underbrace{(2 \\cdot 2 \\cdots 2)}_{n \\text{ factors}} \\cdot \\underbrace{(1 \\cdot 1 \\cdots 1)}_{m-n \\text{ factors}} = 2^n \\cdot 1^{m-n} = 2^n\n$$\nThe value of the determinant depends only on $n$, the rank of $A$.\n\nFinally, we evaluate this expression for the given values $m=13$ and $n=5$. Substituting $n=5$ into the derived formula gives:\n$$\n\\det(I_{13} + P) = 2^5 = 32\n$$", "answer": "$$\\boxed{32}$$", "id": "3206009"}, {"introduction": "A key theoretical question about the SVD is its uniqueness. While the singular values are unique, the orthogonal matrices $U$ and $V$ are not, particularly when singular values are repeated. This final practice challenges you to investigate the precise nature of this ambiguity by characterizing the group of transformations that leave the decomposition invariant for a matrix with a repeated singular value [@problem_id:3577705]. This exploration of the SVD's isotropy group provides deep insight into the structure of the underlying singular subspaces and the geometric freedom they afford.", "problem": "Let $A \\in \\mathbb{R}^{n \\times n}$ with $n \\geq 3$ be full rank and admit a singular value decomposition (SVD) $A = U \\Sigma V^{\\top}$ with $U, V \\in \\mathrm{O}(n)$ and $\\Sigma = \\mathrm{diag}(\\sigma_{1}, \\dots, \\sigma_{n})$ with $\\sigma_{i} \\geq 0$. Assume there is exactly one index $k$ with $1 \\leq k < n$ such that $\\sigma_{k} = \\sigma_{k+1} > 0$, and all other singular values are positive and pairwise distinct. Define the isotropy group\n$$\nG \\;=\\; \\{ (Q_{U}, Q_{V}) \\in \\mathrm{O}(n) \\times \\mathrm{O}(n) \\;:\\; U Q_{U} \\, \\Sigma \\, (V Q_{V})^{\\top} \\;=\\; U \\Sigma V^{\\top} \\}.\n$$\nStarting from first principles, namely the definition of the singular value decomposition and the spectral theorem for real symmetric matrices, derive the structure of $G$ by characterizing those pairs $(Q_{U}, Q_{V})$ that leave $A$ invariant under the transformation $(U, V) \\mapsto (U Q_{U}, V Q_{V})$. In particular, identify the freedom induced by the two-dimensional singular subspace associated with the repeated singular value and explain how this yields a continuous family within $G$. Finally, determine the real dimension of the identity component of $G$. Your final answer must be a single real number with no units.", "solution": "The isotropy group $G$ is defined as the set of pairs of orthogonal matrices $(Q_U, Q_V)$ that preserve the singular value decomposition (SVD) of a matrix $A$. The defining equation for an element $(Q_U, Q_V) \\in G$ is\n$$\nU Q_{U}\n\\Sigma (V Q_{V})^{\\top} = U \\Sigma V^{\\top}\n$$\nwhere $A = U \\Sigma V^{\\top}$ is the given SVD of $A$, with $U, V \\in \\mathrm{O}(n)$ and $\\Sigma = \\mathrm{diag}(\\sigma_1, \\dots, \\sigma_n)$. Since $U$ and $V$ are invertible, we can multiply by $U^{\\top}$ on the left and $V$ on the right:\n$$\nU^{\\top} (U Q_{U} \\Sigma Q_{V}^{\\top} V^{\\top}) V = U^{\\top} (U \\Sigma V^{\\top}) V\n$$\nUsing the orthogonality conditions $U^{\\top}U = I$ and $V^{\\top}V = I$, where $I$ is the $n \\times n$ identity matrix, we obtain a condition on $Q_U$, $Q_V$, and $\\Sigma$:\n$$\nQ_U \\Sigma Q_V^{\\top} = \\Sigma\n$$\nAs $A$ is full rank, all singular values $\\sigma_i > 0$, and thus $\\Sigma$ is invertible. Post-multiplying by $Q_V$ leads to the fundamental relationship:\n$$\nQ_U \\Sigma = \\Sigma Q_V\n$$\nThe SVD is intrinsically connected to the spectral decomposition of the symmetric matrices $AA^{\\top}$ and $A^{\\top}A$. From $A = U \\Sigma V^{\\top}$, we derive:\n$$\nAA^{\\top} = (U \\Sigma V^{\\top})(U \\Sigma V^{\\top})^{\\top} = U \\Sigma V^{\\top} V \\Sigma^{\\top} U^{\\top} = U \\Sigma^2 U^{\\top}\n$$\n$$\nA^{\\top}A = (U \\Sigma V^{\\top})^{\\top}(U \\Sigma V^{\\top}) = V \\Sigma^{\\top} U^{\\top} U \\Sigma V^{\\top} = V \\Sigma^2 V^{\\top}\n$$\nThese are the spectral decompositions of $AA^{\\top}$ and $A^{\\top}A$. The columns of $U$, denoted $\\{u_i\\}_{i=1}^n$, are the orthonormal eigenvectors of $AA^{\\top}$, and the columns of $V$, denoted $\\{v_i\\}_{i=1}^n$, are those of $A^{\\top}A$. The eigenvalues for both matrices are the squared singular values, $\\{\\sigma_i^2\\}_{i=1}^n$.\n\nThe problem specifies that there is exactly one index $k$ ($1 \\leq k < n$) for which $\\sigma_k = \\sigma_{k+1}$, and all other singular values are positive and pairwise distinct. This implies that the set of eigenvalues $\\{\\sigma_i^2\\}$ for $AA^{\\top}$ and $A^{\\top}A$ contains exactly one repeated value, $\\sigma_k^2 = \\sigma_{k+1}^2$, which has multiplicity $2$. All other eigenvalues $\\sigma_i^2$ for $i \\notin \\{k, k+1\\}$ are simple (multiplicity $1$).\n\nBy the spectral theorem for real symmetric matrices, eigenspaces corresponding to distinct eigenvalues are orthogonal. If $(U Q_U, V Q_V)$ constitutes a new pair of SVD matrices, then the columns of $U' = U Q_U$ must be a new orthonormal basis of eigenvectors for $AA^{\\top}$. Specifically, the $j$-th column $u'_j$ of $U'$ must be an eigenvector corresponding to the eigenvalue $\\sigma_j^2$. Since $u'_j = \\sum_{i=1}^n u_i (Q_U)_{ij}$, $u'_j$ must belong to the eigenspace of $AA^{\\top}$ associated with $\\sigma_j^2$. This is only possible if the summation for $u'_j$ is restricted to eigenvectors $u_i$ from that same eigenspace. Consequently, the entry $(Q_U)_{ij}$ can be non-zero only if $\\sigma_i = \\sigma_j$.\n\nThis condition forces $Q_U$ to be a block-diagonal matrix, where the blocks correspond to the eigenspaces of $\\Sigma^2$. Given the singular value structure, the index sets for equal singular values are $\\{i\\}$ for each $i \\notin \\{k, k+1\\}$ and the set $\\{k, k+1\\}$. Therefore, $Q_U$ must take the form:\n$$\nQ_U = \\mathrm{diag}(d_1, \\dots, d_{k-1}, Q', d_{k+2}, \\dots, d_n)\n$$\nwhere $d_i$ are $1 \\times 1$ blocks and $Q'$ is a $2 \\times 2$ block. Since $Q_U \\in \\mathrm{O}(n)$, its diagonal blocks must themselves be orthogonal. This implies $d_i \\in \\mathrm{O}(1)$, so $d_i = \\pm 1$. The block $Q'$ must be in $\\mathrm{O}(2)$.\nAn identical argument holds for $Q_V$ and the eigenvectors of $A^{\\top}A$, so $Q_V$ must have the same block-diagonal structure:\n$$\nQ_V = \\mathrm{diag}(e_1, \\dots, e_{k-1}, R', e_{k+2}, \\dots, e_n)\n$$\nwhere $e_i = \\pm 1$ and $R' \\in \\mathrm{O}(2)$.\n\nWe now return to the condition $Q_U \\Sigma = \\Sigma Q_V$. We analyze this equation block by block.\nFor an index $i \\notin \\{k, k+1\\}$, we have a $1 \\times 1$ equation: $d_i \\sigma_i = \\sigma_i e_i$. As $\\sigma_i > 0$, this simplifies to $d_i = e_i$.\nFor the $2 \\times 2$ block corresponding to indices $k, k+1$, let $\\sigma = \\sigma_k = \\sigma_{k+1}$. The corresponding submatrix of $\\Sigma$ is $\\sigma I_2$. The equation is:\n$$\nQ' (\\sigma I_2) = (\\sigma I_2) R' \\implies \\sigma Q' = \\sigma R'\n$$\nAs $\\sigma > 0$, we must have $Q' = R'$.\n\nThis establishes that for any $(Q_U, Q_V) \\in G$, we must have $Q_U = Q_V$. The isotropy group $G$ is therefore isomorphic to the subgroup $\\mathcal{Q} \\subset \\mathrm{O}(n)$ of matrices of the form:\n$$\nQ = \\mathrm{diag}(\\epsilon_1, \\dots, \\epsilon_{k-1}, Q', \\epsilon_{k+2}, \\dots, \\epsilon_n)\n$$\nwhere $\\epsilon_i \\in \\{ -1, 1 \\}$ for $i \\in \\{1, \\dots, n\\} \\setminus \\{k,k+1\\}$, and $Q' \\in \\mathrm{O}(2)$. The group structure of $\\mathcal{Q}$ is the direct product $(\\mathbb{Z}_2)^{n-2} \\times \\mathrm{O}(2)$, where $\\mathbb{Z}_2 \\cong \\{-1, 1\\}$ is the multiplicative group of order $2$.\n\nThe freedom induced by the two-dimensional singular subspace $\\mathrm{span}\\{u_k, u_{k+1}\\}$ is captured by the $Q' \\in \\mathrm{O}(2)$ block. Any orthogonal transformation of the basis $\\{u_k, u_{k+1}\\}$ results in another valid orthonormal basis for this eigenspace. The subgroup $\\mathrm{SO}(2) \\subset \\mathrm{O}(2)$ consists of rotation matrices $R(\\theta)$ for $\\theta \\in [0, 2\\pi)$. Choosing $Q' = R(\\theta)$ corresponds to rotating the basis vectors. This yields a continuous one-parameter family of transformations within $G$.\n\nThe identity component of $G$, denoted $G_0$, is the connected component containing the identity element $(I, I)$. This corresponds to the identity component of the isomorphic group $\\mathcal{Q} \\cong (\\mathbb{Z}_2)^{n-2} \\times \\mathrm{O}(2)$. The identity component of a direct product of Lie groups is the direct product of their respective identity components.\n1. The group $(\\mathbb{Z}_2)^{n-2}$ is discrete. Its identity component is the trivial group containing only the identity element.\n2. The group $\\mathrm{O}(2)$ has two connected components: $\\mathrm{SO}(2)$ (rotations, $\\det=1$) and the set of reflections ($\\det=-1$). The identity component is the one containing the identity matrix $I_2$, which is $\\mathrm{SO}(2)$.\nThus, the identity component of $G$ is isomorphic to $\\mathrm{SO}(2)$.\n\nThe real dimension of a Lie group is the dimension of its underlying manifold. The special orthogonal group $\\mathrm{SO}(2)$ is the group of rotations in the plane, parametrized by a single angle $\\theta$. It is diffeomorphic to the circle $S^1$, which is a $1$-dimensional manifold.\nTherefore, the real dimension of the identity component of $G$ is $\\dim(G_0) = \\dim(\\mathrm{SO}(2)) = 1$.", "answer": "$$\\boxed{1}$$", "id": "3577705"}]}