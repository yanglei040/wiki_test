{"hands_on_practices": [{"introduction": "The Golub-Kahan-Reinsch algorithm begins by reducing a general matrix to a much simpler bidiagonal form using a sequence of orthogonal transformations. This exercise provides a concrete, hands-on walkthrough of this critical first phase. By applying Householder reflectors to a small matrix, you will gain a practical understanding of how this reduction is achieved step-by-step and build intuition for the geometry of the algorithm [@problem_id:3588842].", "problem": "Consider the task of computing a bidiagonalization as it arises in the Golub–Kahan–Reinsch singular value decomposition (SVD) algorithm for a real rectangular matrix. Let $A \\in \\mathbb{R}^{4 \\times 3}$ be given by\n$$\nA \\;=\\;\n\\begin{pmatrix}\n3  0  0 \\\\\n4  5  0 \\\\\n0  4  6 \\\\\n0  0  1\n\\end{pmatrix}.\n$$\nUsing the Golub–Kahan–Reinsch bidiagonalization framework, apply successive left Householder reflectors to columns and right Householder reflectors to rows so as to transform $A$ to an upper bidiagonal form. At each left step, target the current column’s subvector to become a nonnegative multiple of the corresponding basis vector (that is, at step $i$, map the working subvector to $\\alpha_i \\, e_1$ with $\\alpha_i \\ge 0$), and at each right step, target the current row’s trailing subvector to become a nonnegative multiple of the first basis direction (that is, map the working subvector to $\\beta_i \\, e_1$ with $\\beta_i \\ge 0$). \n\nCarry out the following, providing intermediate quantities explicitly:\n- Determine the Householder vectors for the left reflectors $P_1$, $P_2$, $P_3$ and the right reflectors $Q_1$, $Q_2$ used in the bidiagonalization process.\n- Write the products $U_0 = P_1 P_2 P_3 \\in \\mathbb{R}^{4 \\times 4}$ and $V_0 = Q_1 Q_2 \\in \\mathbb{R}^{3 \\times 3}$ in terms of the identified Householder reflectors.\n- Identify the bidiagonal matrix $B \\in \\mathbb{R}^{4 \\times 3}$ with diagonal entries $\\alpha_1, \\alpha_2, \\alpha_3$ and superdiagonal entries $\\beta_1, \\beta_2$ produced by this process.\n\nYour final task is a calculation: report the exact value (no rounding) of the third diagonal entry $\\alpha_3$ of the bidiagonal matrix $B$ produced by this process. Provide your final answer as a single closed-form analytic expression. No rounding is required; give the exact expression.", "solution": "The Golub-Kahan-Reinsch bidiagonalization process transforms the matrix $A$ into an upper bidiagonal matrix $B$ by applying a sequence of Householder transformations from the left ($P_i$) and right ($Q_i$). The final transformation is $B = P_3 P_2 P_1 A Q_1 Q_2$.\n\n**Step 1: First Left Reflector ($P_1$)**\nWe target the first column vector, $x_1 = (3, 4, 0, 0)^T$. Its L2-norm is $\\|x_1\\|_2 = \\sqrt{3^2 + 4^2} = 5$. The target vector is $5 e_1 = (5, 0, 0, 0)^T$. The diagonal entry is $\\alpha_1 = 5$. The Householder vector is $v_1 = x_1 - 5e_1 = (-2, 4, 0, 0)^T$. Applying the corresponding reflector $P_1$ to $A$ yields:\n$$ A^{(1)} = P_1 A = \\begin{pmatrix} 5  4  0 \\\\ 0  -3  0 \\\\ 0  4  6 \\\\ 0  0  1 \\end{pmatrix} $$\n\n**Step 2: First Right Reflector ($Q_1$)**\nWe target the first-row elements past the superdiagonal, which is the subvector $(4, 0)$. Its norm is $\\|(4,0)\\|_2 = 4$. The target is $(4,0)$, which is the vector itself. Thus, the transformation is the identity, $Q_1 = I$, and $\\beta_1 = 4$. The matrix remains unchanged: $A^{(2)} = A^{(1)}$.\n\n**Step 3: Second Left Reflector ($P_2$)**\nWe target the subcolumn starting at the second diagonal element, $x_2 = (-3, 4, 0)^T$. Its L2-norm is $\\|x_2\\|_2 = \\sqrt{(-3)^2 + 4^2} = 5$. The diagonal entry is $\\alpha_2 = 5$. Applying the corresponding reflector $P_2$ (acting on rows 2, 3, and 4) yields:\n$$ A^{(3)} = P_2 A^{(2)} = \\begin{pmatrix} 5  4  0 \\\\ 0  5  24/5 \\\\ 0  0  18/5 \\\\ 0  0  1 \\end{pmatrix} $$\n\n**Step 4: Second Right Reflector ($Q_2$)**\nWe target the second-row elements past the superdiagonal, which is the single element $(24/5)$. Its norm is $24/5$. The transformation is the identity, $Q_2 = I$, and $\\beta_2 = 24/5$. The matrix remains unchanged: $A^{(4)} = A^{(3)}$.\n\n**Step 5: Third Left Reflector ($P_3$)**\nWe target the subcolumn starting at the third diagonal element, $x_3 = (18/5, 1)^T$. Its L2-norm determines the third diagonal entry, $\\alpha_3$:\n$$ \\alpha_3 = \\|x_3\\|_2 = \\sqrt{\\left(\\frac{18}{5}\\right)^2 + 1^2} = \\sqrt{\\frac{324}{25} + \\frac{25}{25}} = \\sqrt{\\frac{349}{25}} = \\frac{\\sqrt{349}}{5} $$\nApplying the reflector $P_3$ zeroes out the element below this diagonal, completing the bidiagonalization. The final bidiagonal matrix is:\n$$ B = \\begin{pmatrix} 5  4  0 \\\\ 0  5  24/5 \\\\ 0  0  \\sqrt{349}/5 \\\\ 0  0  0 \\end{pmatrix} $$\nThe question asks for the value of $\\alpha_3$, which we have calculated.", "answer": "$$\\boxed{\\frac{\\sqrt{349}}{5}}$$", "id": "3588842"}, {"introduction": "A key feature of the orthogonal transformations used in the SVD algorithm is that they preserve the \"energy\" of the matrix, as measured by the Frobenius norm. This practice connects theory with numerical reality by first asking you to prove this fundamental property, $\\|A\\|_F = \\|U^T A V\\|_F$, from first principles [@problem_id:3588807]. You will then design and implement a numerical test to verify that this energy preservation holds in finite-precision arithmetic and learn how to diagnose deviations caused by any loss of exact orthogonality.", "problem": "Let $A \\in \\mathbb{R}^{m \\times n}$ and let $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{n \\times n}$ be orthogonal matrices. The singular value decomposition (SVD) is computed in the Golub–Kahan–Reinsch algorithm by reducing $A$ to bidiagonal form with products of Householder reflectors, which are orthogonal transformations. This problem asks you to reason from first principles about the preservation of the Frobenius norm under orthogonal transformations and to design a robust numerical test that validates this property for orthogonal factors computed by a Golub–Kahan–Reinsch-style bidiagonalization in finite precision arithmetic. You must also diagnose deviations from exact orthogonality.\n\nTasks:\n1) Theoretical derivation. Starting from the definitions of the Frobenius norm and orthogonal matrices and using only standard identities such as the cyclic invariance of the trace, prove that for orthogonal $U$ and $V$,\n$$\\|A\\|_F = \\|U^T A V\\|_F.$$\nYour derivation must begin from the definition $\\|A\\|_F = \\sqrt{\\operatorname{trace}(A^T A)}$, the definition of orthogonality $U^T U = I$ and $V^T V = I$, and the cyclic property of the trace $\\operatorname{trace}(XYZ) = \\operatorname{trace}(ZXY)$ whenever the products are defined. Do not use any unproven shortcut identities.\n\n2) Numerical experiment design. Implement in double precision a Golub–Kahan–Reinsch-style reduction of $A$ to bidiagonal form using Householder reflectors, explicitly accumulating the left and right orthogonal factors $\\hat{U}$ and $\\hat{V}$ so that the computed bidiagonal satisfies $\\hat{B} \\approx \\hat{U}^T A \\hat{V}$. Your implementation must:\n- Construct Householder reflectors from first principles to annihilate subdiagonal elements in columns (left reflectors) and superdiagonal elements in rows (right reflectors).\n- Apply each reflector to the appropriate trailing submatrix and accumulate the corresponding transformation into $\\hat{U}$ or $\\hat{V}$.\n- Use double precision arithmetic and avoid calls to any black-box SVD routine.\n\n3) Energy-preservation test and diagnostics. For a given input matrix $A$, compute:\n- The relative Frobenius norm discrepancy\n$$\\mathrm{rel\\_err} = \\frac{\\big|\\|A\\|_F - \\| \\hat{U}^T A \\hat{V} \\|_F\\big|}{\\max(\\|A\\|_F, 1)}.$$\nIf $\\|A\\|_F = 0$, define $\\mathrm{rel\\_err} = 0$.\n- Orthogonality defects\n$$\\mathrm{orthU} = \\|I - \\hat{U}^T \\hat{U}\\|_F, \\quad \\mathrm{orthV} = \\|I - \\hat{V}^T \\hat{V}\\|_F.$$\nUse the unit roundoff $u$ for double precision and the acceptance threshold\n$$\\tau(m,n) = c \\, u \\, (m + n), \\quad c = 100.$$\nDeclare a case as a pass if $\\mathrm{rel\\_err} \\le \\tau(m,n)$, and a fail otherwise. In addition, design an experiment that deliberately perturbs $\\hat{U}$ and $\\hat{V}$ to be slightly non-orthogonal by adding a dense perturbation of size $\\delta$ to each and demonstrate that both the orthogonality defects and the relative Frobenius norm discrepancy increase in a manner commensurate with the perturbation magnitude.\n\n4) Test suite. Your program must run the following five reproducible test cases and aggregate the results:\n- Case $1$ (happy path): $m=50$, $n=30$, $A$ with independent standard normal entries, seeded with $1$.\n- Case $2$ (square): $m=10$, $n=10$, $A$ with independent standard normal entries, seeded with $2$.\n- Case $3$ (zero matrix): $m=20$, $n=15$, $A$ is the zero matrix.\n- Case $4$ (ill-conditioned): $m=60$, $n=5$, construct $A = U_0 \\Sigma V_0^T$ with $U_0 \\in \\mathbb{R}^{60 \\times 60}$ and $V_0 \\in \\mathbb{R}^{5 \\times 5}$ random orthogonal (generated via the $QR$ factorization of Gaussian matrices seeded with $3$ and $4$ respectively), and singular values $\\Sigma = \\operatorname{diag}(\\sigma_1,\\dots,\\sigma_5)$ with $\\sigma_i = 10^{-12 \\cdot (i-1)/(5-1)}$ so that $\\sigma_1 = 1$ and $\\sigma_5 = 10^{-12}$.\n- Case $5$ (diagnostic perturbation): $m=40$, $n=35$, $A$ with independent standard normal entries seeded with $5$, and after computing $\\hat{U}$ and $\\hat{V}$, replace them with $\\tilde{U} = \\hat{U} + \\delta R_U$ and $\\tilde{V} = \\hat{V} + \\delta R_V$, where $R_U$ and $R_V$ are dense Gaussian matrices with the same shapes seeded with $6$ and $\\delta = 10^{-8}$. Use $\\tilde{U}$ and $\\tilde{V}$ for the metrics in this case.\n\n5) Required final output format. Your program should produce a single line of output containing a comma-separated list of per-case results, with each case printed as a bracketed quadruple $[\\mathrm{rel\\_err},\\mathrm{orthU},\\mathrm{orthV},\\mathrm{pass}]$, where $\\mathrm{pass}$ is $1$ if the case passes and $0$ otherwise. All floating-point numbers must be printed in scientific notation with $16$ digits after the decimal point. There must be no spaces anywhere in the line. The entire list must be enclosed in square brackets. For example, a line with two cases should look like $[[1.2340000000000000e-10,2.0000000000000000e-15,2.0000000000000000e-15,1],[3.2100000000000000e-08,4.0000000000000000e-14,4.0000000000000000e-14,1]]$.\n\nYour program must be completely self-contained and must not read any input. It must implement the bidiagonal reduction and orthogonal accumulation as described, compute the required metrics for the specified test suite, and print the results in the exact format described above.", "solution": "### 1) Theoretical Derivation\n\nThe objective is to prove that for an orthogonal matrix $U \\in \\mathbb{R}^{m \\times m}$ and an orthogonal matrix $V \\in \\mathbb{R}^{n \\times n}$, the Frobenius norm is invariant under the transformation $A \\mapsto U^T A V$, i.e., $\\|A\\|_F = \\|U^T A V\\|_F$.\n\nThe derivation begins from the given definitions:\n1.  Frobenius norm: $\\|X\\|_F = \\sqrt{\\operatorname{trace}(X^T X)}$.\n2.  Orthogonality: $U^T U = I_m$ and $V^T V = I_n$. For a square matrix $Q$, $Q^T Q=I$ implies $QQ^T=I$.\n3.  Cyclic property of the trace: $\\operatorname{trace}(XYZ) = \\operatorname{trace}(ZXY)$ for any matrices $X, Y, Z$ for which the products are defined.\n\nLet's start by analyzing the squared norm of the transformed matrix, $\\|U^T A V\\|_F^2$.\n\nUsing the definition of the Frobenius norm:\n$$ \\|U^T A V\\|_F^2 = \\operatorname{trace}\\left((U^T A V)^T (U^T A V)\\right) $$\n\nThe transpose of a product $(XYZ)^T$ is $Z^T Y^T X^T$. Applying this rule:\n$$ (U^T A V)^T = V^T A^T (U^T)^T $$\nSince for any matrix $X$, $(X^T)^T = X$, we have $(U^T)^T = U$. The expression becomes:\n$$ (U^T A V)^T = V^T A^T U $$\nSubstituting this back into the trace expression:\n$$ \\|U^T A V\\|_F^2 = \\operatorname{trace}\\left((V^T A^T U) (U^T A V)\\right) $$\n\nThe matrices $U$ and $U^T$ are adjacent. Since $U$ is an orthogonal matrix, $U U^T = I_m$.\n$$ \\|U^T A V\\|_F^2 = \\operatorname{trace}\\left(V^T A^T (U U^T) A V\\right) = \\operatorname{trace}\\left(V^T A^T I_m A V\\right) = \\operatorname{trace}\\left(V^T A^T A V\\right) $$\n\nNow, we apply the cyclic property of the trace, $\\operatorname{trace}(XYZ) = \\operatorname{trace}(ZXY)$, by setting $X = V^T$, $Y = A^T A$, and $Z = V$.\n$$ \\operatorname{trace}\\left( (V^T) (A^T A) (V) \\right) = \\operatorname{trace}\\left( (V) (V^T) (A^T A) \\right) $$\n\nSince $V$ is an orthogonal matrix, $V V^T = I_n$.\n$$ \\|U^T A V\\|_F^2 = \\operatorname{trace}\\left(I_n (A^T A)\\right) = \\operatorname{trace}(A^T A) $$\n\nBy the definition of the Frobenius norm, $\\operatorname{trace}(A^T A) = \\|A\\|_F^2$.\nTherefore, we have shown that:\n$$ \\|U^T A V\\|_F^2 = \\|A\\|_F^2 $$\n\nSince the Frobenius norm is by definition non-negative, we can take the square root of both sides to obtain the final result:\n$$ \\|U^T A V\\|_F = \\|A\\|_F $$\nThis completes the proof.\n\n### 2) Numerical Experiment Design and Implementation\n\nThe Golub-Kahan-Reinsch algorithm for bidiagonalization of a matrix $A \\in \\mathbb{R}^{m \\times n}$ proceeds by applying a sequence of Householder transformations from the left and right to zero out elements below the main diagonal and above the first superdiagonal.\n\nA Householder transformation is defined by a reflector matrix $H = I - 2vv^T$, where $v$ is a unit vector. For a given vector $x$, the vector $v$ can be chosen to make $Hx$ a multiple of the standard basis vector $e_1$. A numerically stable choice for the (unnormalized) Householder vector is $v' = x + \\operatorname{sgn}(x_1) \\|x\\|_2 e_1$. This vector is then normalized, $v = v'/\\|v'\\|_2$.\n\nThe bidiagonalization algorithm is as follows:\nLet $A^{(0)} = A$. We iterate for $k = 0, 1, \\dots, \\min(m, n)-1$.\n\n1.  **Left Reflector (Column Annihilation)**: A Householder transformation $U_k'$ is constructed to zero out the subdiagonal elements of the $k$-th column of the current matrix $A^{(k-1)}$. This reflector acts on rows $k$ through $m-1$. The full transformation $U_k$ is an embedding of $U_k'$ into an $m \\times m$ identity matrix.\n    $$ A^{(k-1/2)} = U_k A^{(k-1)} $$\n2.  **Right Reflector (Row Annihilation)**: If $k  n-2$, a Householder transformation $V_k'$ is constructed to zero out elements in row $k$ from column $k+2$ onwards. This reflector acts on columns $k+1$ through $n-1$. The full transformation $V_k$ is an embedding of $V_k'$ into an $n \\times n$ identity matrix.\n    $$ A^{(k)} = A^{(k-1/2)} V_k $$\n\nThe final bidiagonal matrix is $\\hat{B} \\approx A^{(\\min(m,n))}$. The accumulated orthogonal matrices are $\\hat{U} = U_0 U_1 \\dots$ and $\\hat{V} = V_0 V_1 \\dots$. The implementation will explicitly construct $\\hat{U}$ and $\\hat{V}$ by initializing them as identity matrices and applying the transformations at each step.\n\n### 3) Energy-Preservation Test and Diagnostics\n\nThe theoretical proof shows that for exact orthogonal matrices $\\hat{U}$ and $\\hat{V}$, the Frobenius norm is perfectly preserved. In finite precision arithmetic, $\\hat{U}$ and $\\hat{V}$ will only be approximately orthogonal, leading to a small discrepancy in the norm.\n\nThe metrics to quantify this behavior are:\n-   **Relative Frobenius Norm Discrepancy**: This measures the change in the \"energy\" of the matrix, normalized by the original energy.\n    $$ \\mathrm{rel\\_err} = \\frac{\\big|\\|A\\|_F - \\| \\hat{U}^T A \\hat{V} \\|_F\\big|}{\\max(\\|A\\|_F, 1)} $$\n    The denominator term $\\max(\\|A\\|_F, 1)$ prevents division by zero and provides a sensible relative error for small-norm matrices.\n-   **Orthogonality Defects**: These measure how close the computed factors $\\hat{U}$ and $\\hat{V}$ are to being perfectly orthogonal.\n    $$ \\mathrm{orthU} = \\|I - \\hat{U}^T \\hat{U}\\|_F, \\quad \\mathrm{orthV} = \\|I - \\hat{V}^T \\hat{V}\\|_F $$\n    For perfectly orthogonal matrices, these defects would be zero. In practice, they should be on the order of the machine precision times some factor related to the matrix dimensions.\n\nThe acceptance threshold $\\tau(m,n) = c \\, u \\, (m + n)$ with $c=100$ and $u$ as the double-precision unit roundoff provides a reasonable bound for the expected relative error from a stable algorithm. A pass is declared if $\\mathrm{rel\\_err} \\le \\tau(m,n)$.\n\nThe diagnostic test in Case 5 deliberately introduces non-orthogonality into $\\hat{U}$ and $\\hat{V}$ by adding a small random perturbation. This is expected to increase both the orthogonality defects and the Frobenius norm discrepancy, demonstrating their sensitivity to the loss of orthogonality. The magnitudes of `orthU` and `orthV` are expected to be roughly proportional to the perturbation size $\\delta$, and `rel_err` is expected to increase significantly, likely causing the test to fail.", "answer": "```python\nimport numpy as np\n\ndef bidiagonalize(A_in):\n    \"\"\"\n    Performs Golub-Kahan-Reinsch bidiagonalization of a matrix A.\n\n    Args:\n        A_in (np.ndarray): The m x n matrix to bidiagonalize.\n\n    Returns:\n        tuple[np.ndarray, np.ndarray]: The accumulated orthogonal matrices U (m x m) and V (n x n).\n    \"\"\"\n    m, n = A_in.shape\n    A = A_in.copy().astype(float)\n    U = np.eye(m, dtype=float)\n    V = np.eye(n, dtype=float)\n\n    for k in range(min(m, n)):\n        # Left (column) reflector to annihilate A[k+1:m, k]\n        # v is constructed from the k-th column of the trailing submatrix\n        x = A[k:m, k].copy()\n        norm_x = np.linalg.norm(x)\n\n        if norm_x  0:\n            s_sign = np.copysign(1.0, x[0]) if x[0] != 0.0 else 1.0\n            s = s_sign * norm_x\n            x[0] += s\n            norm_v = np.linalg.norm(x)\n            if norm_v  0:\n                v = x / norm_v\n\n                # Apply transformation to the trailing submatrix of A\n                sub_A = A[k:m, k:n]\n                sub_A -= 2 * np.outer(v, v.T @ sub_A)\n\n                # Accumulate transformation in U\n                sub_U = U[:, k:m]\n                sub_U -= 2 * np.outer(sub_U @ v, v.T)\n\n        # Right (row) reflector to annihilate A[k, k+2:n]\n        if k  n - 2:\n            # w is constructed from the k-th row of the trailing submatrix\n            y = A[k, k+1:n].copy()\n            norm_y = np.linalg.norm(y)\n\n            if norm_y  0:\n                s_sign = np.copysign(1.0, y[0]) if y[0] != 0.0 else 1.0\n                s = s_sign * norm_y\n                y[0] += s\n                norm_w = np.linalg.norm(y)\n                if norm_w  0:\n                    w = y / norm_w\n\n                    # Apply transformation to the trailing submatrix of A\n                    sub_A = A[k:m, k+1:n]\n                    sub_A -= 2 * np.outer(sub_A @ w, w.T)\n                    \n                    # Accumulate transformation in V\n                    sub_V = V[:, k+1:n]\n                    sub_V -= 2 * np.outer(sub_V @ w, w.T)\n\n    return U, V\n\ndef run_test_case(m, n, A_generator, seeds):\n    \"\"\"\n    Sets up and runs a single test case.\n\n    Args:\n        m (int): Number of rows.\n        n (int): Number of columns.\n        A_generator (str): Method to generate matrix A.\n        seeds (dict): Seeds for random number generation.\n\n    Returns:\n        list: A list containing [rel_err, orthU, orthV, is_pass].\n    \"\"\"\n    # 1. Generate matrix A based on case description\n    if A_generator == 'normal':\n        rng = np.random.default_rng(seeds['A'])\n        A = rng.standard_normal((m, n))\n    elif A_generator == 'zero':\n        A = np.zeros((m, n))\n    elif A_generator == 'ill-cond':\n        rng_U = np.random.default_rng(seeds['U0'])\n        rand_U = rng_U.standard_normal((m, m))\n        U0, _ = np.linalg.qr(rand_U)\n        \n        rng_V = np.random.default_rng(seeds['V0'])\n        rand_V = rng_V.standard_normal((n, n))\n        V0, _ = np.linalg.qr(rand_V)\n        \n        num_sv = min(m, n)\n        sigma_vals = [10.0**(-12.0 * i / (num_sv - 1)) for i in range(num_sv)]\n        Sigma = np.zeros((m, n))\n        np.fill_diagonal(Sigma, sigma_vals)\n        A = U0 @ Sigma @ V0.T\n    \n    # 2. Perform bidiagonalization\n    U_hat, V_hat = bidiagonalize(A)\n\n    # 3. Handle diagnostic perturbation (Case 5)\n    if 'perturb' in seeds:\n        delta = 1e-8\n        rng_pert = np.random.default_rng(seeds['perturb'])\n        R_U = rng_pert.standard_normal((m, m))\n        R_V = rng_pert.standard_normal((n, n))\n        U_hat += delta * R_U\n        V_hat += delta * R_V\n        \n    # 4. Compute metrics\n    norm_A_F = np.linalg.norm(A, 'fro')\n    \n    transformed_A = U_hat.T @ A @ V_hat\n    norm_transformed_A_F = np.linalg.norm(transformed_A, 'fro')\n    \n    if norm_A_F == 0.0:\n        rel_err = 0.0\n    else:\n        rel_err = abs(norm_A_F - norm_transformed_A_F) / max(norm_A_F, 1.0)\n        \n    orthU = np.linalg.norm(np.eye(m) - U_hat.T @ U_hat, 'fro')\n    orthV = np.linalg.norm(np.eye(n) - V_hat.T @ V_hat, 'fro')\n\n    # 5. Evaluate pass/fail criterion\n    u = np.finfo(float).eps / 2.0\n    c = 100.0\n    tau = c * u * (m + n)\n    is_pass = 1 if rel_err = tau else 0\n    \n    return [rel_err, orthU, orthV, is_pass]\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    test_cases = [\n        {'m': 50, 'n': 30, 'gen': 'normal', 'seeds': {'A': 1}},\n        {'m': 10, 'n': 10, 'gen': 'normal', 'seeds': {'A': 2}},\n        {'m': 20, 'n': 15, 'gen': 'zero',   'seeds': {}},\n        {'m': 60, 'n': 5,  'gen': 'ill-cond', 'seeds': {'U0': 3, 'V0': 4}},\n        {'m': 40, 'n': 35, 'gen': 'normal', 'seeds': {'A': 5, 'perturb': 6}},\n    ]\n\n    results = []\n    for case in test_cases:\n        result = run_test_case(case['m'], case['n'], case['gen'], case['seeds'])\n        results.append(result)\n\n    result_strings = []\n    for res in results:\n        rel_err, orthU, orthV, is_pass = res\n        s = f\"[{rel_err:.16e},{orthU:.16e},{orthV:.16e},{is_pass}]\"\n        result_strings.append(s)\n\n    final_output = f\"[{','.join(result_strings)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "3588807"}, {"introduction": "While the singular values of a matrix are unique, the singular vectors are not, and the algorithm itself contains many choices that can lead to different outputs on different runs. This exercise challenges you to think like a numerical software developer by analyzing the various sources of ambiguity, from sign choices in reflectors to the handling of degenerate subspaces [@problem_id:3588838]. Your task is to identify a complete set of conventions necessary to ensure that the SVD computation is fully deterministic and reproducible, a crucial requirement for reliable scientific software.", "problem": "Consider the Golub–Kahan–Reinsch algorithm for computing the singular value decomposition (SVD) of a real matrix $A \\in \\mathbb{R}^{m \\times n}$, which proceeds by orthogonal bidiagonalization using Householder reflectors to obtain $A = U_{0} B V_{0}^{T}$ with $B$ upper bidiagonal, followed by an implicit shifted orthogonal QR-type iteration on $B$ using Givens rotations to diagonalize $B$ and produce $B = \\widehat{U} \\Sigma \\widehat{V}^{T}$, yielding the final factors $U = U_{0} \\widehat{U}$, $\\Sigma$, and $V = V_{0} \\widehat{V}$. It is known from the definition of the singular value decomposition $A = U \\Sigma V^{T}$ that for simple (pairwise distinct) singular values, the singular vectors are unique up to simultaneous sign flips of corresponding left–right pairs, and for repeated singular values, the singular subspaces are unique but any orthonormal basis within a degenerate singular subspace is admissible.\n\nThe bidiagonal QR iteration employs bulge chasing with Givens rotations and deflates when a superdiagonal element $e_{k}$ is deemed numerically zero, $|e_{k}| \\le \\tau$, for a fixed threshold $\\tau  0$. In floating-point arithmetic, several sign choices arise:\n- In Householder reflectors, the sign of the constructed reflector vector.\n- In Givens rotations, the choice of signs for cosine and sine.\n- At and near deflation, where a vanishing $e_{k}$ may induce sign flips in neighboring diagonal entries if signs are not explicitly propagated.\n\nAnalyze how these sign choices and potential sign flips during deflation affect the reproducibility of the computed $U$ and $V$ across runs and platforms. Then, identify which of the following sets of conventions are sufficient to ensure deterministic outputs $U$, $\\Sigma$, and $V$ for all inputs (including those with repeated singular values), provided the same arithmetic and the same deflation threshold $\\tau$ are used.\n\nChoose all that apply.\n\nA. Impose a fully specified, global sign convention throughout the pipeline:\n   (1) In each Householder reflector $H = I - 2 v v^{T}$ used in bidiagonalization, construct $v$ from a vector $x$ by $v = \\frac{x + \\alpha e_{1}}{\\|x + \\alpha e_{1}\\|}$ with $\\alpha = \\operatorname{sign}(x_{1}) \\|x\\|$ and the tie-breaker $\\operatorname{sign}(0) = +1$; if $x = 0$, set $H = I$. \n   (2) After bidiagonalization, absorb signs so that the bidiagonal $B$ satisfies $b_{ii} \\ge 0$ and $b_{i,i+1} \\ge 0$ by premultiplying or postmultiplying by diagonal sign matrices and updating the accumulated $U_{0}$ and $V_{0}$ accordingly. \n   (3) In every Givens rotation $G = \\begin{bmatrix} c  s \\\\ -s  c \\end{bmatrix}$, set $r = \\sqrt{a^{2} + b^{2}} \\ge 0$, $c = a/r$, $s = b/r$, with deterministic tie-breakers when $r = 0$ (take $c = 1$, $s = 0$). At deflation, when $|e_{k}| \\le \\tau$, set $e_{k} = 0$ and propagate any negative diagonal sign into the adjacent block so that diagonal entries of the working bidiagonal remain nonnegative. \n   (4) After convergence, enforce a deterministic columnwise sign convention: for each index $i$, find the smallest index $j$ at which $|v_{j i}|$ attains $\\max_{\\ell} |v_{\\ell i}|$ and flip the pair $(u_{i}, v_{i})$ simultaneously if $v_{j i}  0$. For repeated singular values, first fix a basis deterministically within each degenerate singular subspace (e.g., perform a stable orthonormalization with lexicographic tie-breaking on columns), then apply the same columnwise sign convention. With these rules, $U$, $\\Sigma$, and $V$ are deterministic.\n\nB. It suffices to enforce that $\\Sigma$ has nonnegative diagonal entries; no other sign conventions are needed because $U$ and $V$ are then uniquely determined by $\\Sigma$.\n\nC. Enforcing $\\det(U) = +1$ and $\\det(V) = +1$ at the end by possibly flipping the last columns of $U$ and $V$ is sufficient to ensure a deterministic SVD, regardless of intermediate sign choices and deflation.\n\nD. After computing any SVD (by any sequence of reflectors and rotations), postprocess by flipping each right singular vector $v_{i}$ so that the sum of its entries is positive, $\\sum_{j} v_{j i}  0$; if the sum is negative, replace $(u_{i}, v_{i})$ with $(-u_{i}, -v_{i})$. No additional constraints on bidiagonalization, Givens rotations, or deflation are necessary to guarantee determinism.\n\nE. To control sign flips introduced by deflation, adopt the rule that whenever a superdiagonal $e_{k}$ is set to $0$ by deflation, only the trailing diagonal element $b_{k+1,k+1}$ is sign-adjusted (if necessary) to be nonnegative; all other sign choices (Householder and Givens) can remain unconstrained. This local rule at deflation is sufficient to ensure deterministic $U$ and $V$.", "solution": "To guarantee a deterministic SVD output ($U, \\Sigma, V$) from an algorithm like Golub-Kahan-Reinsch, one must address all sources of mathematical and algorithmic non-uniqueness. These sources include choices made during the computation and fundamental ambiguities in the SVD definition itself.\n\nThe primary sources of non-determinism are:\n1.  **Algorithmic Choices**: At various stages, the algorithm has mathematically equivalent options that can lead to different numerical results.\n    -   **Householder Reflectors**: The vector $v$ defining a reflector $H = I - 2vv^T/v^T v$ is determined up to a sign.\n    -   **Givens Rotations**: The cosine-sine pair $(c, s)$ for a Givens rotation is determined up to a simultaneous sign flip, $(c, s) \\leftrightarrow (-c, -s)$.\n    -   **Deflation**: When a problem splits, signs can be propagated between the now-decoupled subproblems, affecting the intermediate orthogonal factors.\n2.  **Fundamental Non-Uniqueness**: The SVD definition itself is not unique.\n    -   **Sign Ambiguity**: If $(u_i, v_i)$ is a pair of singular vectors for $\\sigma_i$, then so is $(-u_i, -v_i)$.\n    -   **Subspace Ambiguity**: For a repeated singular value, the corresponding columns of $U$ and $V$ are not unique vectors but any orthonormal basis for the associated singular subspaces.\n\nA set of conventions is sufficient only if it provides a unique rule for every choice point, resolving both algorithmic and fundamental ambiguities.\n\n### Evaluation of Options\n\n**A. Impose a fully specified, global sign convention throughout the pipeline.**\nThis option is a comprehensive set of rules designed to eliminate all ambiguity:\n1.  **Householder vectors**: Specifies a deterministic sign choice, making the bidiagonalization factors $U_0, V_0$ unique.\n2.  **Canonical Bidiagonal Form**: Enforces non-negative entries in the bidiagonal matrix $B$. This ensures the input to the iterative phase is deterministic.\n3.  **Givens Rotations and Deflation**: Provides deterministic rules for calculating rotation parameters and handling signs during deflation. This makes the sequence of QR iterations unique.\n4.  **Final Basis and Signs**: Crucially, it addresses the fundamental ambiguities. It specifies a method to choose a deterministic basis for degenerate singular subspaces and a rule to fix the final sign ambiguity of all $(u_i, v_i)$ pairs.\nThis approach systematically addresses every source of non-determinism. **This is a sufficient set of conventions.**\n\n**B. It suffices to enforce that $\\Sigma$ has nonnegative diagonal entries...**\nThis is incorrect. Non-negative singular values are standard, but this does not resolve the sign and subspace ambiguities in $U$ and $V$, nor the algorithmic choices.\n\n**C. Enforcing $\\det(U) = +1$ and $\\det(V) = +1$ at the end...**\nThis is insufficient. Flipping the signs of two pairs of singular vectors, e.g., $(u_i, v_i) \\to (-u_i, -v_i)$ and $(u_j, v_j) \\to (-u_j, -v_j)$, preserves the determinants but changes the matrices $U$ and $V$. It also fails to address the basis choice for repeated singular values.\n\n**D. After computing any SVD, postprocess by flipping each right singular vector...**\nThis is insufficient. This post-processing rule attempts to fix the final signs but is incomplete (e.g., no tie-breaker if the sum is zero). Most importantly, it cannot resolve the more significant issue of basis ambiguity for repeated singular values.\n\n**E. To control sign flips introduced by deflation, adopt a local rule...**\nThis is insufficient. It addresses only one of many algorithmic choice points and explicitly leaves others (Householder, Givens) unconstrained, which is guaranteed to produce non-deterministic results.\n\nTherefore, only option A provides a complete set of rules to ensure a deterministic SVD.", "answer": "$$\\boxed{A}$$", "id": "3588838"}]}