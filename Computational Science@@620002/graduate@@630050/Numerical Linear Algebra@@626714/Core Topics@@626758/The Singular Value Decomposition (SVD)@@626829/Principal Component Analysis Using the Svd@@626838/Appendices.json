{"hands_on_practices": [{"introduction": "Applying a trained PCA model to new data requires careful adherence to the transformation pipeline established during training. This exercise provides a concrete, step-by-step calculation to solidify the core mechanics of out-of-sample projection. By contrasting the correct and incorrect handling of the data mean, this problem [@problem_id:3566943] illuminates a critical principle of statistical modeling: preprocessing steps for new data must use parameters derived *only* from the training set to avoid data leakage and ensure valid results.", "problem": "Let $X \\in \\mathbb{R}^{3 \\times 2}$ be a training data matrix with rows\n$$\n\\begin{pmatrix}\n\\frac{5}{3} & -\\frac{4}{3}\n\\end{pmatrix},\\quad\n\\begin{pmatrix}\n-\\frac{1}{3} & \\frac{2}{3}\n\\end{pmatrix},\\quad\n\\begin{pmatrix}\n\\frac{5}{3} & \\frac{2}{3}\n\\end{pmatrix}.\n$$\nLet the preprocessing step for Principal Component Analysis (PCA) be column-wise centering by the training mean vector $\\mu \\in \\mathbb{R}^{2}$, and let PCA be carried out via the Singular Value Decomposition (SVD). A new out-of-sample vector is $y = \\begin{pmatrix} 3 & 1 \\end{pmatrix}^\\top$. Using the one-dimensional principal subspace spanned by the leading PCA loading vector, do the following:\n\n1. Derive, from first principles starting with the definition of the Singular Value Decomposition (SVD), how to obtain the leading loading vector from the centered training matrix, and compute it for the given $X$.\n2. Using the derivation of out-of-sample projection in PCA, compute the squared reconstruction error $\\,\\|y - \\hat{y}\\|^{2}\\,$ when $y$ is handled with the correct mean convention (center $y$ by the training mean $\\mu$ before projection, and de-center by $\\mu$ after reconstruction).\n3. Compute the squared reconstruction error when $y$ is incorrectly handled by centering with its own feature mean (i.e., subtracting $\\mu_{y}\\,\\mathbf{1}$, where $\\mu_{y}$ is the average of the entries of $y$ and $\\mathbf{1}$ is the all-ones vector), while still de-centering by the training mean $\\mu$ after reconstruction.\n4. Provide the difference between the two squared reconstruction errors (incorrect handling minus correct handling) as a single exact value. No rounding is required.\n\nProvide your final answer as a single exact value.", "solution": "The problem asks for a series of computations related to Principal Component Analysis (PCA) on a given dataset, focusing on the projection of an out-of-sample data point and the resulting reconstruction error under different handling conventions.\n\nFirst, the validity of the problem statement is established.\n**Step 1: Extract Givens**\n- Data matrix $X \\in \\mathbb{R}^{3 \\times 2}$ with rows:\n$$\n\\begin{pmatrix} \\frac{5}{3} & -\\frac{4}{3} \\end{pmatrix}, \\quad \\begin{pmatrix} -\\frac{1}{3} & \\frac{2}{3} \\end{pmatrix}, \\quad \\begin{pmatrix} \\frac{5}{3} & \\frac{2}{3} \\end{pmatrix}\n$$\n- Preprocessing: Column-wise centering of the training data $X$ by its mean vector $\\mu \\in \\mathbb{R}^2$.\n- Method: PCA carried out via Singular Value Decomposition (SVD).\n- Out-of-sample vector: $y = \\begin{pmatrix} 3 & 1 \\end{pmatrix}$.\n- Task dimensions: Use the one-dimensional principal subspace spanned by the leading PCA loading vector.\n- Task 1: Derive how to obtain the leading loading vector from the SVD of the centered training matrix and compute it.\n- Task 2: Compute the squared reconstruction error $\\|y - \\hat{y}\\|^{2}$ with correct handling (center $y$ with $\\mu$, de-center with $\\mu$).\n- Task 3: Compute the squared reconstruction error with incorrect handling (center $y$ with its own feature mean $\\mu_y$, de-center with $\\mu$).\n- Task 4: Compute the difference between the squared errors from Task 3 and Task 2.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, being a standard application of PCA and SVD in numerical linear algebra and statistics. It is well-posed, with all necessary data and clear, objective instructions provided. There are no contradictions, ambiguities, or unrealistic conditions. The problem is a formalizable and solvable exercise testing the understanding of PCA mechanics.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A full solution will be provided.\n\nLet the training data matrix be $X \\in \\mathbb{R}^{m \\times n}$, where $m=3$ is the number of data points and $n=2$ is the number of features.\n$$\nX = \\begin{pmatrix}\n\\frac{5}{3} & -\\frac{4}{3} \\\\\n-\\frac{1}{3} & \\frac{2}{3} \\\\\n\\frac{5}{3} & \\frac{2}{3}\n\\end{pmatrix}\n$$\n\nThe first step in PCA is to center the data by subtracting the mean of each feature. The mean vector $\\mu$ is:\n$$\n\\mu = \\frac{1}{3} \\begin{pmatrix} \\frac{5}{3} - \\frac{1}{3} + \\frac{5}{3} \\\\ -\\frac{4}{3} + \\frac{2}{3} + \\frac{2}{3} \\end{pmatrix} = \\frac{1}{3} \\begin{pmatrix} \\frac{9}{3} \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}\n$$\nThe centered data matrix $B$ is obtained by subtracting $\\mu^T$ from each row of $X$:\n$$\nB = X - \\mathbf{1}\\mu^T = \\begin{pmatrix}\n\\frac{5}{3}-1 & -\\frac{4}{3}-0 \\\\\n-\\frac{1}{3}-1 & \\frac{2}{3}-0 \\\\\n\\frac{5}{3}-1 & \\frac{2}{3}-0\n\\end{pmatrix} = \\begin{pmatrix}\n\\frac{2}{3} & -\\frac{4}{3} \\\\\n-\\frac{4}{3} & \\frac{2}{3} \\\\\n\\frac{2}{3} & \\frac{2}{3}\n\\end{pmatrix}\n$$\n\n**1. Derivation and Computation of the Leading Loading Vector**\n\nThe principal component loading vectors are the orthonormal eigenvectors of the sample covariance matrix $C = \\frac{1}{m-1}B^T B$. We need to show how these are obtained from the SVD of $B$.\n\nLet the SVD of the centered matrix $B$ be $B = U\\Sigma V^T$, where $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{n \\times n}$ are orthogonal matrices, and $\\Sigma \\in \\mathbb{R}^{m \\times n}$ is a rectangular diagonal matrix of singular values $\\sigma_i$. The columns of $V$ are the right-singular vectors.\n\nConsider the matrix $B^T B$:\n$$B^T B = (U\\Sigma V^T)^T (U\\Sigma V^T) = V\\Sigma^T U^T U \\Sigma V^T$$\nSince $U$ is orthogonal, $U^T U = I_m$.\n$$B^T B = V\\Sigma^T I_m \\Sigma V^T = V(\\Sigma^T \\Sigma)V^T$$\nThe matrix $\\Sigma^T \\Sigma$ is an $n \\times n$ diagonal matrix with entries $\\sigma_i^2$. This equation shows that $V$ diagonalizes $B^T B$, which means the columns of $V$ are the eigenvectors of $B^T B$, and the diagonal entries of $\\Sigma^T \\Sigma$ (the squared singular values) are the corresponding eigenvalues.\nSince $C = \\frac{1}{m-1}B^T B$, $C$ and $B^T B$ share the same eigenvectors. Therefore, the PCA loading vectors are the columns of $V$, i.e., the right-singular vectors of the centered data matrix $B$. The leading loading vector, $v_1$, corresponds to the largest singular value $\\sigma_1$.\n\nTo compute $v_1$, we find the leading eigenvector of $B^T B$:\n$$\nB^T B = \\begin{pmatrix}\n\\frac{2}{3} & -\\frac{4}{3} & \\frac{2}{3} \\\\\n-\\frac{4}{3} & \\frac{2}{3} & \\frac{2}{3}\n\\end{pmatrix}\n\\begin{pmatrix}\n\\frac{2}{3} & -\\frac{4}{3} \\\\\n-\\frac{4}{3} & \\frac{2}{3} \\\\\n\\frac{2}{3} & \\frac{2}{3}\n\\end{pmatrix} = \\begin{pmatrix}\n\\frac{4+16+4}{9} & \\frac{-8-8+4}{9} \\\\\n\\frac{-8-8+4}{9} & \\frac{16+4+4}{9}\n\\end{pmatrix} = \\begin{pmatrix}\n\\frac{24}{9} & -\\frac{12}{9} \\\\\n-\\frac{12}{9} & \\frac{24}{9}\n\\end{pmatrix} = \\frac{4}{3}\\begin{pmatrix}\n2 & -1 \\\\\n-1 & 2\n\\end{pmatrix}\n$$\nThe eigenvalues $\\lambda$ of $B^T B$ are found from $\\det(B^T B - \\lambda I) = 0$:\n$$ (\\frac{8}{3}-\\lambda)^2 - (-\\frac{4}{3})^2 = 0 \\implies (\\frac{8}{3}-\\lambda)^2 = \\frac{16}{9} \\implies \\frac{8}{3}-\\lambda = \\pm \\frac{4}{3} $$\nThis gives eigenvalues $\\lambda_1 = \\frac{8}{3} + \\frac{4}{3} = \\frac{12}{3} = 4$ and $\\lambda_2 = \\frac{8}{3} - \\frac{4}{3} = \\frac{4}{3}$.\nThe leading loading vector $v_1$ is the eigenvector corresponding to the largest eigenvalue $\\lambda_1=4$:\n$$ (B^T B - 4I)v_1 = 0 \\implies \\begin{pmatrix} \\frac{8}{3}-4 & -\\frac{4}{3} \\\\ -\\frac{4}{3} & \\frac{8}{3}-4 \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\end{pmatrix} = \\begin{pmatrix} -\\frac{4}{3} & -\\frac{4}{3} \\\\ -\\frac{4}{3} & -\\frac{4}{3} \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} $$\nThis gives the equation $-\\frac{4}{3}x - \\frac{4}{3}y = 0$, or $x+y=0$. The eigenvector is in the direction $\\begin{pmatrix} 1 & -1 \\end{pmatrix}^T$. Normalizing it to unit length:\n$$ v_1 = \\frac{1}{\\sqrt{1^2+(-1)^2}}\\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ -\\frac{1}{\\sqrt{2}} \\end{pmatrix} $$\n\n**2. Squared Reconstruction Error (Correct Handling)**\n\nThe out-of-sample vector is $y = \\begin{pmatrix} 3 & 1 \\end{pmatrix}^T$.\nFirst, center $y$ using the training mean $\\mu$:\n$$ y_c = y - \\mu = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} $$\nProject the centered vector $y_c$ onto the one-dimensional principal subspace spanned by $v_1$:\n$$ y_{c, \\text{proj}} = (y_c^T v_1) v_1 $$\nThe projection coefficient is:\n$$ y_c^T v_1 = \\begin{pmatrix} 2 & 1 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ -\\frac{1}{\\sqrt{2}} \\end{pmatrix} = \\frac{2}{\\sqrt{2}} - \\frac{1}{\\sqrt{2}} = \\frac{1}{\\sqrt{2}} $$\nThe projected centered vector is:\n$$ y_{c, \\text{proj}} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ -\\frac{1}{\\sqrt{2}} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2} \\\\ -\\frac{1}{2} \\end{pmatrix} $$\nReconstruct the vector $\\hat{y}$ by de-centering with $\\mu$:\n$$ \\hat{y} = y_{c, \\text{proj}} + \\mu = \\begin{pmatrix} \\frac{1}{2} \\\\ -\\frac{1}{2} \\end{pmatrix} + \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{3}{2} \\\\ -\\frac{1}{2} \\end{pmatrix} $$\nThe squared reconstruction error is $\\|y - \\hat{y}\\|^2$:\n$$ \\|y - \\hat{y}\\|^2 = \\left\\| \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} \\frac{3}{2} \\\\ -\\frac{1}{2} \\end{pmatrix} \\right\\|^2 = \\left\\| \\begin{pmatrix} \\frac{3}{2} \\\\ \\frac{3}{2} \\end{pmatrix} \\right\\|^2 = \\left(\\frac{3}{2}\\right)^2 + \\left(\\frac{3}{2}\\right)^2 = \\frac{9}{4} + \\frac{9}{4} = \\frac{18}{4} = \\frac{9}{2} $$\n\n**3. Squared Reconstruction Error (Incorrect Handling)**\n\nNow, center $y$ using the mean of its own entries. The vector is $y = \\begin{pmatrix} 3 & 1 \\end{pmatrix}^T$. The mean of its entries is $\\mu_y = \\frac{3+1}{2} = 2$.\nThe incorrectly centered vector $y'_c$ is:\n$$ y'_c = y - \\mu_y \\mathbf{1} = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} $$\nProject $y'_c$ onto the subspace spanned by $v_1$:\n$$ y'_{c, \\text{proj}} = ((y'_c)^T v_1) v_1 $$\nThe projection coefficient is:\n$$ (y'_c)^T v_1 = \\begin{pmatrix} 1 & -1 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ -\\frac{1}{\\sqrt{2}} \\end{pmatrix} = \\frac{1}{\\sqrt{2}} + \\frac{1}{\\sqrt{2}} = \\frac{2}{\\sqrt{2}} = \\sqrt{2} $$\nThe projected vector is:\n$$ y'_{c, \\text{proj}} = \\sqrt{2} \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ -\\frac{1}{\\sqrt{2}} \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} $$\nNote that $y'_c$ was already in the direction of $v_1$, so the projection is itself.\nReconstruct the vector $\\hat{y}'$ by de-centering with the training mean $\\mu$:\n$$ \\hat{y}' = y'_{c, \\text{proj}} + \\mu = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} + \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix} $$\nThe squared reconstruction error is $\\|y - \\hat{y}'\\|^2$:\n$$ \\|y - \\hat{y}'\\|^2 = \\left\\| \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix} \\right\\|^2 = \\left\\| \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} \\right\\|^2 = 1^2 + 2^2 = 1 + 4 = 5 $$\n\n**4. Difference in Squared Reconstruction Errors**\n\nThe difference is the squared error from incorrect handling minus the squared error from correct handling:\n$$ \\text{Difference} = 5 - \\frac{9}{2} = \\frac{10}{2} - \\frac{9}{2} = \\frac{1}{2} $$", "answer": "$$\\boxed{\\frac{1}{2}}$$", "id": "3566943"}, {"introduction": "While powerful, the effectiveness of standard PCA can be severely compromised by the presence of outliers, as its objective function is based on minimizing squared distances. This practice [@problem_id:3566941] explores this sensitivity and guides you through implementing a robust alternative using an iteratively reweighted scheme based on Huber-type weights. This hands-on comparison demonstrates how to build models that are less susceptible to anomalous data points, a crucial skill for real-world data analysis.", "problem": "Design and implement a program that evaluates the sensitivity of principal component analysis (PCA) computed via the singular value decomposition (SVD) to outliers, and compares it with a simple robust reweighted variant that uses Huber-type weights. The setting is purely algebraic, with all computations in Euclidean spaces. Every dataset consists of a set of row-vectors in a real matrix, and the target is the leading one-dimensional principal subspace.\n\nFundamental base:\n- Let a real data matrix be denoted by $X \\in \\mathbb{R}^{n \\times d}$. The standard, unweighted PCA for the leading component proceeds by first centering the data by subtracting the arithmetic mean $\\mu = \\frac{1}{n}\\sum_{i=1}^{n} x_i$, where $x_i \\in \\mathbb{R}^d$ are the rows of $X$, forming $X_c$ with rows $x_i - \\mu$, and then computing a singular value decomposition $X_c = U \\Sigma V^{\\top}$. The first principal direction is the first column of $V$, denoted $v_1 \\in \\mathbb{R}^d$, which minimizes the sum of squared orthogonal residuals $\\sum_{i=1}^{n} \\| (I - v_1 v_1^{\\top})(x_i - \\mu) \\|_2^2$ over all unit vectors.\n- A weighted variant uses nonnegative weights $w_i$ on samples. Define the weighted mean $\\mu_w = \\left(\\sum_{i=1}^{n} w_i x_i\\right) \\big/ \\left(\\sum_{i=1}^{n} w_i\\right)$, form the weighted centered matrix with rows $x_i - \\mu_w$, and let $W = \\mathrm{diag}(w_1,\\dots,w_n)$. The leading weighted principal direction equals the leading right singular vector of $W^{1/2} (X - \\mathbf{1}\\mu_w^{\\top})$, where $\\mathbf{1}\\in\\mathbb{R}^{n}$ is the vector of ones, which minimizes $\\sum_{i=1}^{n} w_i \\| (I - v v^{\\top})(x_i - \\mu_w) \\|_2^2$ over unit vectors $v$.\n- The Huber-type reweighting considered here uses residual distances to the current one-dimensional subspace. Given a current unit direction $v$ and mean $\\mu$, define residuals $r_i = \\| (I - v v^{\\top})(x_i - \\mu) \\|_2$. For a threshold $c > 0$, define weights $w_i = 1$ if $r_i \\le c$ and $w_i = c / r_i$ if $r_i > c$. This yields an iteratively reweighted scheme: initialize with the unweighted PCA direction and mean, compute $w_i$ from $r_i$, recompute $\\mu_w$, and update the direction by the leading right singular vector of $W^{1/2}(X - \\mathbf{1}\\mu_w^{\\top})$. Repeat for a fixed number of iterations or until convergence of the direction.\n\nPrincipal angle metric:\n- For a ground-truth unit direction $v_{\\star} \\in \\mathbb{R}^d$ and an estimated unit direction $\\hat v \\in \\mathbb{R}^d$, define the principal angle error $\\theta(\\hat v, v_{\\star}) = \\arccos\\left( | \\hat v^{\\top} v_{\\star} | \\right)$ in radians.\n\nTasks to implement:\n- Implement unweighted PCA via SVD as above to compute the leading unit direction.\n- Implement the Huber-type robust reweighted PCA with the weights defined above, using the following fixed iteration scheme: initialize with the unweighted PCA direction and mean, then iterate weight computation and weighted SVD updates for at most $T = 20$ iterations or until the change in direction satisfies $1 - |v_{\\mathrm{new}}^{\\top} v_{\\mathrm{old}}| \\le \\varepsilon$, where $\\varepsilon = 10^{-12}$.\n- For each dataset, compute the principal angle errors for the unweighted and robust directions relative to a supplied ground-truth unit direction, in radians. Round each angle to $6$ decimal places.\n- Define an improvement flag that is $\\mathrm{True}$ if the robust angle error is smaller than the unweighted angle error by at least $\\delta = 10^{-3}$ radians, and $\\mathrm{False}$ otherwise.\n\nAngle unit requirement:\n- All angles must be expressed in radians.\n\nTest suite and parameters:\n- Ambient dimension is $d = 2$ for all tests. The ground-truth unit direction is $v_{\\star} = \\frac{1}{\\sqrt{5}} [1, 2]^{\\top}$.\n- Let $p = \\frac{1}{\\sqrt{5}} [2, -1]^{\\top}$ denote the unit vector orthogonal to $v_{\\star}$.\n- A base set of $6$ near-linear samples is specified by scalars $t = [ -3.0, -1.5, 0.0, 1.0, 2.5, 4.0 ]$ and small orthogonal perturbations $\\epsilon = [ 0.05, -0.02, 0.0, 0.03, -0.04, 0.02 ]$. The $i$-th base sample is $x_i = t_i v_{\\star} + \\epsilon_i p$ for $i \\in \\{1,\\dots,6\\}$.\n- Three test cases are constructed by adding outliers as follows, keeping the Huber threshold fixed at $c = 0.5$:\n  1. Test $1$ (moderate outlier): the dataset consists of the $6$ base samples plus the additional outlier $x_7 = [12.0, -8.0]^{\\top}$.\n  2. Test $2$ (no outlier, boundary case): the dataset consists of only the $6$ base samples.\n  3. Test $3$ (extreme outlier): the dataset consists of the $6$ base samples plus the additional outlier $x_7 = [200.0, -200.0]^{\\top}$.\n\nFinal output format:\n- For each test case, your program must produce a list with three entries in the order $[\\theta_{\\mathrm{unweighted}}, \\theta_{\\mathrm{robust}}, \\mathrm{improved}]$, where the first two are decimal numbers rounded to $6$ fractional digits (radians), and the third is a boolean as specified above.\n- Aggregate the three per-test-case lists into a single list in the same order as the tests and print exactly one line with this aggregate list, formatted with no spaces, for example: $[[0.123456,0.012345,\\mathrm{True}],[0.000000,0.000000,\\mathrm{False}],[0.567890,0.045678,\\mathrm{True}]]$.", "solution": "The objective is to analyze the sensitivity of principal component analysis (PCA) to outliers by comparing the standard unweighted method with a robust iteratively reweighted variant. The performance of both estimators for the leading principal direction is evaluated against a known ground-truth direction using the principal angle as an error metric. All calculations are performed in Euclidean space $\\mathbb{R}^d$.\n\nThe input data is a set of $n$ points represented as the rows of a matrix $X \\in \\mathbb{R}^{n \\times d}$. We denote the data points, treated as column vectors, by $x_1, x_2, \\dots, x_n \\in \\mathbb{R}^d$.\n\n**Unweighted Principal Component Analysis**\n\nThe standard approach to find the first principal component is to identify the direction that maximizes the variance of the projected data. This is equivalent to minimizing the sum of squared orthogonal distances from the data points to the line defined by the principal direction. The procedure is as follows:\n\n1.  **Data Centering**: The data is first centered by subtracting the arithmetic mean vector $\\mu \\in \\mathbb{R}^d$, calculated as:\n    $$\n    \\mu = \\frac{1}{n} \\sum_{i=1}^{n} x_i\n    $$\n    This yields a centered data matrix, $X_c$, whose rows are $(x_i - \\mu)^{\\top}$.\n\n2.  **Singular Value Decomposition (SVD)**: The SVD of the centered data matrix $X_c$ is computed:\n    $$\n    X_c = U \\Sigma V^{\\top}\n    $$\n    where $U \\in \\mathbb{R}^{n \\times n}$ and $V \\in \\mathbb{R}^{d \\times d}$ are orthogonal matrices, and $\\Sigma \\in \\mathbb{R}^{n \\times d}$ is a rectangular diagonal matrix of singular values. The columns of $V$ are the right singular vectors of $X_c$, which correspond to the principal directions of the data.\n\n3.  **Leading Principal Direction**: The first principal direction, denoted $\\hat{v}_{\\mathrm{unweighted}}$, is the right singular vector corresponding to the largest singular value. This is the first column of the matrix $V$. This vector $\\hat{v}_{\\mathrm{unweighted}}$ minimizes the objective function $\\sum_{i=1}^{n} \\| (I - v v^{\\top})(x_i - \\mu) \\|_2^2$ over all unit vectors $v \\in \\mathbb{R}^d$.\n\n**Robust Reweighted Principal Component Analysis**\n\nOutliers can disproportionately influence the arithmetic mean and the covariance structure, thereby skewing the principal directions computed by the standard method. Robust PCA methods aim to mitigate this by down-weighting observations that lie far from the emerging principal subspace. The implemented method is an iteratively reweighted algorithm using Huber-type weights.\n\n1.  **Initialization**: The algorithm is initialized with the results from the unweighted PCA. The initial direction is $v^{(0)} = \\hat{v}_{\\mathrm{unweighted}}$ and the initial mean is $\\mu^{(0)} = \\mu$.\n\n2.  **Iterative Refinement**: The algorithm proceeds iteratively for a maximum of $T=20$ steps. In each iteration $k = 1, 2, \\dots, T$:\n    a.  **Compute Residuals**: For each data point $x_i$, the orthogonal distance (residual) to the current principal subspace (a line spanned by $v^{(k-1)}$ passing through $\\mu^{(k-1)}$) is calculated:\n        $$\n        r_i^{(k)} = \\| (I - v^{(k-1)}(v^{(k-1)})^{\\top})(x_i - \\mu^{(k-1)}) \\|_2\n        $$\n        For the first iteration, $\\mu^{(0)}$ is the unweighted mean. For subsequent iterations, $\\mu^{(k-1)}$ is the weighted mean from the previous step.\n    b.  **Compute Huber-type Weights**: Based on the residuals, a set of weights $w_i^{(k)}$ is computed. For a given threshold $c > 0$, the weights are defined as:\n        $$\n        w_i^{(k)} = \\begin{cases} 1 & \\text{if } r_i^{(k)} \\le c \\\\ c / r_i^{(k)} & \\text{if } r_i^{(k)} > c \\end{cases}\n        $$\n        This is equivalent to $w_i^{(k)} = \\min(1, c/r_i^{(k)})$. Points with small residuals receive a weight of $1$, while points with large residuals (potential outliers) are down-weighted in inverse proportion to their distance.\n    c.  **Update Mean**: A new weighted mean $\\mu_w^{(k)}$ is computed:\n        $$\n        \\mu_w^{(k)} = \\frac{\\sum_{i=1}^{n} w_i^{(k)} x_i}{\\sum_{i=1}^{n} w_i^{(k)}}\n        $$\n    d.  **Update Direction**: The new principal direction $v^{(k)}$ is found by performing a weighted PCA. This is achieved by finding the leading right singular vector of the matrix $Y^{(k)} = \\mathrm{diag}(\\sqrt{w_1^{(k)}}, \\dots, \\sqrt{w_n^{(k)}}) (X - \\mathbf{1}(\\mu_w^{(k)})^{\\top})$, where $X$ is the original data matrix with rows $x_i^\\top$ and $\\mathbf{1}$ is a vector of ones. This corresponds to finding the eigenvector for the largest eigenvalue of the weighted covariance matrix $\\sum_{i=1}^{n} w_i^{(k)} (x_i - \\mu_w^{(k)})(x_i - \\mu_w^{(k)})^{\\top}$.\n    e.  **Check for Convergence**: The process is stopped if the change in the principal direction is negligible. The convergence condition is $1 - |(v^{(k)})^{\\top} v^{(k-1)}| \\le \\varepsilon$, where $\\varepsilon = 10^{-12}$. The absolute value accounts for the arbitrary sign of singular vectors. If convergence is not met, the process continues with $v^{(k)}$ becoming $v^{(k-1)}$ and $\\mu_w^{(k)}$ becoming $\\mu^{(k-1)}$ for the next iteration.\n\nThe final direction from this process is denoted $\\hat{v}_{\\mathrm{robust}}$.\n\n**Evaluation Metric**\n\nThe accuracy of the estimated directions $\\hat{v}_{\\mathrm{unweighted}}$ and $\\hat{v}_{\\mathrm{robust}}$ is measured by the principal angle with respect to a known ground-truth direction $v_{\\star}$. The angle $\\theta$ is given by:\n$$\n\\theta(\\hat{v}, v_{\\star}) = \\arccos\\left( | \\hat{v}^{\\top} v_{\\star} | \\right)\n$$\nThe result is in radians, with the absolute value ensuring the angle is in $[0, \\pi/2]$. An improvement is flagged if the robust method reduces the error by a significant margin: $\\theta_{\\mathrm{unweighted}} - \\theta_{\\mathrm{robust}} \\ge \\delta = 10^{-3}$.\n\n**Test Data Generation**\n\nAll test cases are in dimension $d=2$. The ground-truth direction is $v_{\\star} = \\frac{1}{\\sqrt{5}} [1, 2]^{\\top}$, and its orthogonal complement is $p = \\frac{1}{\\sqrt{5}} [2, -1]^{\\top}$. A base dataset of $n=6$ points is generated near the line spanned by $v_{\\star}$ as $x_i = t_i v_{\\star} + \\epsilon_i p$, using specified scalars $t_i$ and $\\epsilon_i$. Three test scenarios are created from this base set to probe the algorithms' behavior: with a moderate outlier, with no outlier, and with an extreme outlier. The Huber threshold is fixed at $c=0.5$. The final computed angles are rounded to $6$ decimal places.", "answer": "```python\nimport numpy as np\n\n# Global constants from the problem description\nT_MAX = 20\nEPSILON_CONVERGENCE = 1e-12\nDELTA_IMPROVEMENT = 1e-3\nHUBER_C = 0.5\nD_DIMENSION = 2\n\n\ndef get_base_data(v_star, p_ortho):\n    \"\"\"\n    Generates the base set of 6 near-linear samples.\n    \"\"\"\n    t = np.array([-3.0, -1.5, 0.0, 1.0, 2.5, 4.0])\n    epsilons = np.array([0.05, -0.02, 0.0, 0.03, -0.04, 0.02])\n    \n    # Construct base samples x_i = t_i * v_star + epsilon_i * p\n    # using broadcasting. X is (n, d), where n=6, d=2.\n    base_data = t[:, np.newaxis] * v_star.T + epsilons[:, np.newaxis] * p_ortho.T\n    return base_data\n\n\ndef get_test_cases(v_star, p_ortho):\n    \"\"\"\n    Constructs the three test case datasets.\n    \"\"\"\n    base_data = get_base_data(v_star, p_ortho)\n    outlier1 = np.array([[12.0, -8.0]])\n    outlier3 = np.array([[200.0, -200.0]])\n    \n    test_cases = [\n        # Test 1 (moderate outlier)\n        np.vstack([base_data, outlier1]),\n        # Test 2 (no outlier)\n        base_data,\n        # Test 3 (extreme outlier)\n        np.vstack([base_data, outlier3]),\n    ]\n    return test_cases\n\n\ndef unweighted_pca(X):\n    \"\"\"\n    Computes the leading principal direction and mean using standard PCA via SVD.\n    Args:\n        X (np.ndarray): Data matrix of shape (n, d).\n    Returns:\n        tuple[np.ndarray, np.ndarray]: A tuple containing the principal direction\n                                      (d, 1) and the mean (d, 1).\n    \"\"\"\n    # X is (n,d), mu must be (d,1)\n    mu = np.mean(X, axis=0, keepdims=True).T\n    X_c = X - mu.T\n    \n    # SVD of the centered data\n    _, _, Vt = np.linalg.svd(X_c, full_matrices=False)\n    \n    # First principal direction is the first row of Vt, reshaped to a column vector\n    v1 = Vt[0, :].reshape(-1, 1)\n    \n    return v1, mu\n\n\ndef robust_pca(X):\n    \"\"\"\n    Computes the leading principal direction using iteratively reweighted PCA.\n    Args:\n        X (np.ndarray): Data matrix of shape (n, d).\n    Returns:\n        np.ndarray: The robustly estimated principal direction (d, 1).\n    \"\"\"\n    # Initialize with unweighted PCA results\n    v_old, mu_old = unweighted_pca(X)\n    n_samples = X.shape[0]\n    \n    for _ in range(T_MAX):\n        # Compute residuals r_i = ||(I - vv^T)(x_i - mu)||_2\n        # (x_i - mu) are rows of X_centered_iter\n        X_centered_iter = X - mu_old.T  # (n, d)\n        # Project centered data onto current direction v_old\n        projections = X_centered_iter @ v_old  # (n, 1)\n        # Subtract projections to get orthogonal components\n        residuals_vecs_as_rows = X_centered_iter - projections @ v_old.T  # (n, d)\n        residuals = np.linalg.norm(residuals_vecs_as_rows, axis=1)  # (n,)\n\n        # Compute Huber-type weights\n        weights = np.ones(n_samples)\n        mask = residuals > HUBER_C\n        if np.any(mask):\n            weights[mask] = HUBER_C / residuals[mask]\n\n        # Compute new weighted mean\n        sum_weights = np.sum(weights)\n        if sum_weights > 1e-9:\n             mu_new_w = (np.sum(weights[:, np.newaxis] * X, axis=0, keepdims=True) / sum_weights).T\n        else: # Fallback for zero weights, though unlikely\n             mu_new_w = np.mean(X, axis=0, keepdims=True).T\n\n        # Form weighted data matrix for SVD\n        W_sqrt = np.diag(np.sqrt(weights))\n        X_w_centered = X - mu_new_w.T\n        Y = W_sqrt @ X_w_centered\n\n        # SVD to get new direction\n        _, _, Vt_new = np.linalg.svd(Y, full_matrices=False)\n        v_new = Vt_new[0, :].reshape(-1, 1)\n\n        # Check for convergence\n        convergence_metric = 1 - np.abs(v_new.T @ v_old)\n        if convergence_metric <= EPSILON_CONVERGENCE:\n            v_old = v_new\n            break\n\n        # Update for next iteration\n        v_old = v_new\n        mu_old = mu_new_w\n        \n    return v_old\n\n\ndef principal_angle_error(v_hat, v_star):\n    \"\"\"\n    Computes the principal angle error between two unit vectors.\n    Args:\n        v_hat (np.ndarray): Estimated direction (d, 1).\n        v_star (np.ndarray): Ground-truth direction (d, 1).\n    Returns:\n        float: The angle in radians.\n    \"\"\"\n    dot_product = np.abs(v_hat.T @ v_star)\n    # Clip to handle potential floating point inaccuracies > 1.0\n    dot_product = np.clip(dot_product, -1.0, 1.0)\n    return np.arccos(dot_product).item()\n\n\ndef solve():\n    \"\"\"\n    Main function to run the analysis for all test cases.\n    \"\"\"\n    v_star = (np.array([1.0, 2.0]) / np.sqrt(5)).reshape(-1, 1)\n    p_ortho = (np.array([2.0, -1.0]) / np.sqrt(5)).reshape(-1, 1)\n    \n    test_cases = get_test_cases(v_star, p_ortho)\n\n    results = []\n    for X in test_cases:\n        # Unweighted PCA\n        v_unweighted, _ = unweighted_pca(X)\n        theta_unweighted = principal_angle_error(v_unweighted, v_star)\n        \n        # Robust PCA\n        v_robust = robust_pca(X)\n        theta_robust = principal_angle_error(v_robust, v_star)\n        \n        # Improvement flag\n        improved = (theta_unweighted - theta_robust) >= DELTA_IMPROVEMENT\n        \n        # Round angles to 6 decimal places for final output list\n        theta_unweighted_rounded = round(theta_unweighted, 6)\n        theta_robust_rounded = round(theta_robust, 6)\n        \n        results.append([theta_unweighted_rounded, theta_robust_rounded, improved])\n\n    # Print the aggregated list in the required format\n    print(str(results).replace(\" \", \"\"))\n\nsolve()\n```", "id": "3566941"}, {"introduction": "Perhaps the most critical practical decision in applying PCA is choosing the number of components to retain, a choice that directly navigates the bias-variance trade-off. This exercise [@problem_id:3566986] moves beyond heuristic scree plots to implement and compare three principled methods for model order selection. You will explore an energy-retention rule, a threshold rooted in random matrix theory, and a generalized cross-validation (GCV) score, providing you with a sophisticated toolkit for determining the optimal dimensionality of your data representation.", "problem": "You are given the task of implementing component selection and model order determination for Principal Component Analysis (PCA) computed via the Singular Value Decomposition (SVD). Your program must operate purely on matrices and real numbers without any physical units. The underlying method must be derived from first principles in numerical linear algebra, specifically the Singular Value Decomposition and orthogonal projection properties.\n\nYou must implement the following, using only derivations from the foundational base:\n- Perform PCA by computing the Singular Value Decomposition of a column-centered data matrix.\n- Implement three model-order selection rules that decide the number of components to retain:\n  1. An energy-retention criterion based on a user-specified threshold.\n  2. A white-noise bulk-edge threshold derived from asymptotic random matrix theory for independent noise with specified standard deviation.\n  3. A criterion based on Generalized Cross-Validation (GCV) that penalizes model complexity via the effective degrees of freedom of a rank-constrained linear estimator.\n\nYour implementation must start from the following base:\n- The Singular Value Decomposition (SVD) of a real matrix and orthogonality properties.\n- The definition of the sample covariance matrix and its spectral decomposition.\n- The Frobenius norm and its invariance under orthogonal transformations.\n- Dimension counting for rank-constrained matrix manifolds and the concept of effective degrees of freedom for linear estimators.\n\nYou must not use or assume any pre-specified closed-form formulas for the target quantities in your program without derivation from the base principles above. Your program must implement the following for each test case:\n- Construct a synthetic data matrix $X \\in \\mathbb{R}^{m \\times n}$ with $m$ observations (rows) and $n$ variables (columns) as the sum of a low-rank signal matrix and an additive Gaussian noise matrix. To construct the signal, draw random Gaussian matrices and orthonormalize them to obtain orthonormal factors, then scale by specified singular values. The noise is independent and identically distributed with a specified standard deviation. Center the resulting data by subtracting the column means before any PCA computation.\n- Compute the Singular Value Decomposition of the centered data matrix and use the singular values to evaluate the following three component selection rules, all expressed in terms of the singular values of the centered matrix:\n  1. Energy-retention rule: choose the smallest integer $k \\ge 0$ such that the cumulative retained energy fraction meets or exceeds a given threshold $0 \\le \\tau \\le 1$. If the total energy is exactly $0$, define the selected $k$ as $0$.\n  2. White-noise bulk-edge threshold rule: assume the additive noise is independent, identically distributed Gaussian with known standard deviation $\\sigma$. Use an asymptotically justified bulk-edge threshold based on the aspect ratio and noise level to retain singular values that exceed this edge. The selected $k$ is the number of singular values that exceed this edge. This choice must be derived from the SVD properties and the invariance of the Frobenius norm under orthogonal transforms and standard asymptotic behavior of singular values in white-noise matrices.\n  3. Generalized Cross-Validation (GCV) rule: for each integer $k \\ge 0$ not exceeding the maximal identifiable rank after centering, define a GCV score that depends on the residual sum of squares of the rank-$k$ approximation and an effective degrees-of-freedom penalty that correctly counts the number of free parameters of a rank-$k$ matrix under orthogonal invariances. Select the $k$ that minimizes the GCV score, breaking ties by choosing the smallest $k$. Exclude any $k$ where the required denominator would be zero or negative.\n\nYour program must produce the results for a small test suite of parameter sets covering a general case, noise-only case, tied singular values case, a high-noise scenario, and a degenerate zero matrix case. For each test case, your program will return a list of three integers $[k_{\\text{energy}}, k_{\\text{bulk}}, k_{\\text{gcv}}]$, where the entries correspond to the three rules above.\n\nThe test suite consists of the following five cases, each specified by a tuple $(m, n, \\text{singular\\_values}, \\sigma, \\tau, \\text{seed})$, where:\n- $m$ is the number of rows (observations), an integer.\n- $n$ is the number of columns (variables), an integer.\n- $\\text{singular\\_values}$ is a list of nonnegative real numbers specifying the nonzero singular values of the signal component in nonincreasing order.\n- $\\sigma$ is the standard deviation of the additive Gaussian noise, a nonnegative real number.\n- $\\tau$ is the target cumulative energy fraction in $[0,1]$ for the energy-retention rule.\n- $\\text{seed}$ is a nonnegative integer for pseudorandom number generation to ensure reproducibility.\n\nUse the following test suite:\n- Case 1: $(m, n, \\text{singular\\_values}, \\sigma, \\tau, \\text{seed}) = (60, 20, [12, 8], 1.0, 0.9, 12345)$.\n- Case 2: $(m, n, \\text{singular\\_values}, \\sigma, \\tau, \\text{seed}) = (60, 20, [], 1.0, 0.9, 54321)$.\n- Case 3: $(m, n, \\text{singular\\_values}, \\sigma, \\tau, \\text{seed}) = (50, 50, [10, 10, 10], 0.5, 0.8, 2024)$.\n- Case 4: $(m, n, \\text{singular\\_values}, \\sigma, \\tau, \\text{seed}) = (40, 10, [9, 7, 5, 3, 1], 2.0, 0.95, 7)$.\n- Case 5: $(m, n, \\text{singular\\_values}, \\sigma, \\tau, \\text{seed}) = (30, 15, [], 0.0, 0.8, 4242)$.\n\nYour program must output a single line containing the results for all test cases aggregated into a single list of lists. The required format is a single line with no spaces containing a valid Python-like literal: a list of the five lists in order, where the list for each case is $[k_{\\text{energy}},k_{\\text{bulk}},k_{\\text{gcv}}]$. For example, a valid output has the form \"[[a,b,c],[d,e,f],[g,h,i],[j,k,l],[p,q,r]]\" with integers in place of the letters.\n\nNo external input is allowed. All randomness must be seeded exactly as specified. All singular value computations must be performed on the centered data matrix as described. If centering results in zero total energy, then for the energy-retention rule return $k_{\\text{energy}} = 0$. Ensure the Generalized Cross-Validation rule excludes any $k$ that leads to an undefined denominator.\n\nYour program must implement and report the results for the five specified cases in the required single-line format. The answers must be integers only. The angle unit is not applicable. No physical units are used anywhere in this problem.", "solution": "The posed problem is valid. It is a well-defined task in numerical linear algebra and computational statistics, grounded in established scientific principles such as Principal Component Analysis (PCA), Singular Value Decomposition (SVD), random matrix theory, and cross-validation. The problem provides a complete and consistent setup, including all necessary parameters and reproducible synthetic data generation protocols, to implement and test three distinct rules for model order selection in PCA.\n\nThe core of the problem is to determine the optimal number of principal components, denoted by an integer $k$, to retain for representing a data matrix. This is a classic bias-variance trade-off: a small $k$ may lead to a model that is too simple and underfits the data (high bias), while a large $k$ may lead to a model that overfits the noise in the data (high variance). The solution requires deriving and implementing three distinct criteria for selecting $k$ based on the singular values of the column-centered data matrix.\n\nLet the given data matrix be $X \\in \\mathbb{R}^{m \\times n}$, with $m$ observations (rows) and $n$ variables (columns).\n\nFirst, the data must be centered by subtracting the mean of each column. Let $\\boldsymbol{\\mu} \\in \\mathbb{R}^{1 \\times n}$ be the row vector of column means. The centered data matrix $\\bar{X}$ is given by:\n$$ \\bar{X} = X - \\mathbf{1}_m \\boldsymbol{\\mu} $$\nwhere $\\mathbf{1}_m$ is an $m \\times 1$ column vector of ones. The columns of $\\bar{X}$ sum to zero, which implies that the data lies in a subspace of dimension at most $m-1$. Consequently, the rank of $\\bar{X}$ is at most $\\min(m-1, n)$.\n\nPCA is performed via the Singular Value Decomposition (SVD) of the centered matrix $\\bar{X}$. The SVD of $\\bar{X}$ is:\n$$ \\bar{X} = U \\Sigma V^T $$\nHere, $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{n \\times n}$ are orthogonal matrices, and $\\Sigma \\in \\mathbb{R}^{m \\times n}$ is a diagonal matrix containing the singular values $s_1 \\ge s_2 \\ge \\dots \\ge s_r > 0$, where $r = \\text{rank}(\\bar{X})$. The columns of $V$ are the principal directions (eigenvectors of the covariance matrix $\\frac{1}{m-1}\\bar{X}^T\\bar{X}$), and the squared singular values $s_i^2$ are proportional to the corresponding eigenvalues, representing the variance captured by each principal component.\n\nThe Eckart-Young-Mirsky theorem states that the best rank-$k$ approximation of $\\bar{X}$ in the Frobenius norm is $\\bar{X}_k = U_k \\Sigma_k V_k^T$, where $U_k$ and $V_k$ contain the first $k$ columns of $U$ and $V$ respectively, and $\\Sigma_k$ contains the top $k$ singular values. The problem is to select an optimal $k$.\n\n**1. Energy-Retention Rule**\nThis rule is based on retaining a certain fraction $\\tau$ of the total \"energy\" or variance of the data. The total energy is the squared Frobenius norm of the centered data matrix, $\\|\\bar{X}\\|_F^2$. Due to the orthogonality of $U$ and $V$, this is equal to the sum of the squared singular values:\n$$ \\|\\bar{X}\\|_F^2 = \\text{tr}(\\bar{X}^T \\bar{X}) = \\text{tr}(\\Sigma^T \\Sigma) = \\sum_{i=1}^r s_i^2 $$\nThe energy captured by the rank-$k$ approximation $\\bar{X}_k$ is similarly $\\sum_{i=1}^k s_i^2$. The rule is to choose the smallest integer $k \\ge 0$ such that the fraction of retained energy meets or exceeds a threshold $\\tau \\in [0,1]$:\n$$ \\frac{\\sum_{i=1}^k s_i^2}{\\sum_{i=1}^r s_i^2} \\ge \\tau $$\nIf the total energy is zero (i.e., $\\bar{X}$ is the zero matrix), $k$ is defined as $0$.\n\n**2. White-Noise Bulk-Edge Threshold Rule**\nThis rule assumes the data consists of a low-rank signal plus additive i.i.d. Gaussian noise with a known standard deviation $\\sigma$. The singular values corresponding to noise are expected to fall within a predictable range, or \"bulk\". Signal components should produce singular values that protrude beyond the edge of this noise bulk. Asymptotic random matrix theory provides an estimate for this edge. For an $M \\times N$ random matrix with i.i.d. entries of variance $\\sigma^2$, the largest singular value asymptotically approaches $\\sigma(\\sqrt{M} + \\sqrt{N})$. Since we are analyzing the centered matrix $\\bar{X}$, its noise component behaves like that of an $(m-1) \\times n$ random matrix. Thus, a well-justified threshold for separating signal from noise is:\n$$ \\theta_{\\text{bulk}} = \\sigma (\\sqrt{m-1} + \\sqrt{n}) $$\nThe number of components to retain, $k_{\\text{bulk}}$, is the count of singular values $s_i$ of $\\bar{X}$ that are strictly greater than this threshold.\n\n**3. Generalized Cross-Validation (GCV) Rule**\nGCV provides a data-driven method for model selection by approximating leave-one-out cross-validation error. It balances the goodness of fit with model complexity. The GCV score for a rank-$k$ approximation is formulated as:\n$$ GCV(k) = \\frac{\\text{RSS}(k)}{(\\text{Effective number of observations} - \\text{Effective degrees of freedom of model})^2} $$\nThe numerator, $\\text{RSS}(k)$, is the residual sum of squares:\n$$ \\text{RSS}(k) = \\|\\bar{X} - \\bar{X}_k\\|_F^2 = \\sum_{i=k+1}^r s_i^2 $$\nThe denominator penalizes complex models. The \"effective number of observations\" in the $m \\times n$ matrix is $D = mn$. The effective degrees of freedom, $\\text{df}(k)$, for a rank-$k$ model is derived from counting the number of free parameters required to specify a rank-$k$ matrix, which is a manifold of dimension $k(m+n-k)$. This is the principled parameter count indicated by the problem statement. The GCV score to be minimized is therefore:\n$$ GCV(k) = \\frac{\\sum_{i=k+1}^r s_i^2}{(mn - k(m+n-k))^2} $$\nWe must select the integer $k$ in the valid range that minimizes this score. The valid range for $k$ excludes values where the denominator is zero or negative. A tie in scores is broken by choosing the smallest $k$. The range of $k$ to test is from $0$ up to $\\min(m, n)$, excluding any $k$ that invalidates the denominator.\n\nThe implementation will construct the synthetic data, apply centering, perform SVD, and compute $k$ according to each of these three rules for every test case specified.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite for PCA model order selection and print results.\n    \"\"\"\n    # Test suite specified by (m, n, singular_values, sigma, tau, seed)\n    test_cases = [\n        (60, 20, [12, 8], 1.0, 0.9, 12345),\n        (60, 20, [], 1.0, 0.9, 54321),\n        (50, 50, [10, 10, 10], 0.5, 0.8, 2024),\n        (40, 10, [9, 7, 5, 3, 1], 2.0, 0.95, 7),\n        (30, 15, [], 0.0, 0.8, 4242)\n    ]\n\n    results = []\n    for case_params in test_cases:\n        result = do_pca_selection(*case_params)\n        results.append(result)\n\n    # The required output is a single-line Python-like literal list of lists with no spaces.\n    # The default str() adds spaces, so we replace them.\n    result_str = str(results).replace(\" \", \"\")\n    print(result_str)\n\ndef do_pca_selection(m, n, signal_sv, sigma, tau, seed):\n    \"\"\"\n    Implements PCA model order selection for a single test case.\n    \n    Args:\n        m (int): Number of rows (observations).\n        n (int): Number of columns (variables).\n        signal_sv (list[float]): Non-increasing list of singular values for the signal.\n        sigma (float): Standard deviation of the additive Gaussian noise.\n        tau (float): Target cumulative energy fraction for the energy-retention rule.\n        seed (int): Seed for the pseudorandom number generator.\n        \n    Returns:\n        list[int]: A list of three integers [k_energy, k_bulk, k_gcv].\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    d = len(signal_sv)\n\n    # 1. Construct the synthetic data matrix X = Signal + Noise\n    signal_matrix = np.zeros((m, n))\n    if d > 0:\n        # Generate random orthonormal factor matrices U_true (m x d) and V_true (n x d)\n        # by performing QR decomposition on random Gaussian matrices.\n        U_true_rand = rng.standard_normal(size=(m, d))\n        U_true, _ = np.linalg.qr(U_true_rand)\n        \n        V_true_rand = rng.standard_normal(size=(n, d))\n        V_true, _ = np.linalg.qr(V_true_rand)\n        \n        signal_matrix = U_true @ np.diag(signal_sv) @ V_true.T\n\n    noise_matrix = rng.normal(loc=0.0, scale=sigma, size=(m, n))\n    X = signal_matrix + noise_matrix\n\n    # 2. Center the data matrix by subtracting column means\n    X_bar = X - X.mean(axis=0, keepdims=True)\n\n    # 3. Compute the Singular Value Decomposition of the centered matrix\n    s = np.linalg.svd(X_bar, compute_uv=False)\n    s_sq = s**2\n    num_sv = len(s)\n\n    # --- Rule 1: Energy-retention rule ---\n    k_energy = 0\n    total_energy = np.sum(s_sq)\n    if total_energy > 1e-15:  # Use a small tolerance for floating-point comparison\n        cumulative_energy = np.cumsum(s_sq)\n        energy_fraction = cumulative_energy / total_energy\n        # Find the smallest k >= 0 (corresponds to index + 1) such that criterion is met.\n        k_candidates = np.where(energy_fraction >= tau)[0]\n        if k_candidates.size > 0:\n            k_energy = k_candidates[0] + 1\n        else:\n            # If tau=1.0 and numerical precision prevents ratio from reaching 1, retain all components.\n            k_energy = num_sv\n\n    # --- Rule 2: White-noise bulk-edge threshold rule ---\n    # The threshold is based on the largest singular value of a centered noise matrix,\n    # whose properties are approximated by an (m-1) x n random matrix.\n    if m <= 1:\n        threshold_bulk = 0.0  # Avoid sqrt of negative or zero value if m <= 1\n    else:\n        threshold_bulk = sigma * (np.sqrt(m - 1) + np.sqrt(n))\n    k_bulk = np.sum(s > threshold_bulk)\n\n    # --- Rule 3: Generalized Cross-Validation (GCV) rule ---\n    k_gcv = 0\n    min_gcv_score = np.inf\n    \n    # We test k from 0 up to min(m, n).\n    max_k_to_test = min(m, n)\n    rss_total = total_energy\n    \n    rss_cumulative = np.cumsum(s_sq)\n\n    for k in range(max_k_to_test + 1):\n        # Denominator term is (D - df(k)) where D=mn, df(k)=k(m+n-k)\n        den_term = m * n - k * (m + n - k)\n        \n        if den_term <= 0:\n            continue\n\n        # Residual Sum of Squares (RSS) for rank-k approximation\n        if k == 0:\n            rss_k = rss_total\n        elif k > num_sv:\n            rss_k = 0.0\n        else:\n            rss_k = rss_total - rss_cumulative[k-1]\n\n        gcv_score = rss_k / (den_term**2)\n\n        # Select k that minimizes GCV. Ties are broken by choosing the smallest k\n        # due to the strict less-than comparison.\n        if gcv_score < min_gcv_score:\n            min_gcv_score = gcv_score\n            k_gcv = k\n    \n    return [int(k_energy), int(k_bulk), int(k_gcv)]\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3566986"}]}