{"hands_on_practices": [{"introduction": "The singular value decomposition provides a profound link between the algebraic properties of a matrix and the geometry of the linear transformation it represents. This exercise solidifies the fundamental connection between a matrix's singular values and its rank and nullity. By constructing a matrix from a prescribed set of singular values, including zeros, you will directly observe how the count of non-zero singular values determines the matrix's rank and, consequently, the dimensions of its fundamental subspaces [@problem_id:3568476].", "problem": "Let $m=4$ and $n=5$. Consider the multiset of prescribed singular values $\\{\\sigma_{1},\\sigma_{2},\\sigma_{3},\\sigma_{4}\\}=\\{9,3,0,0\\}$ for a real matrix $A \\in \\mathbb{R}^{m \\times n}$. Using only fundamental definitions, construct an explicit matrix $A \\in \\mathbb{R}^{4 \\times 5}$ whose singular values are exactly the given multiset, by specifying a choice of orthonormal bases of left and right singular vectors and writing out the entries of $A$. Then, starting from the definitions of singular values via the eigenvalues of $A^{\\top}A$ and the rank-nullity theorem, determine the rank $r$, the right nullity $\\nu=\\dim(\\ker(A))$, and the left nullity $\\mu=\\dim(\\ker(A^{\\top}))$ of your constructed $A$. Finally, explain the constraints that the presence of exactly two zero singular values imposes on the possible triplets $(r,\\nu,\\mu)$ for any matrix $A \\in \\mathbb{R}^{4 \\times 5}$ with singular values $\\{9,3,0,0\\}$, and report the triple $(r,\\nu,\\mu)$ as your final answer. No rounding is required.", "solution": "The problem is validated as scientifically grounded, well-posed, and objective. It is a standard exercise in numerical linear algebra concerning the properties of the Singular Value Decomposition (SVD). All necessary information is provided, and the problem is free of contradictions or ambiguities.\n\nWe are given a matrix $A \\in \\mathbb{R}^{m \\times n}$ with dimensions $m=4$ and $n=5$. The prescribed multiset of singular values is $\\{\\sigma_{1},\\sigma_{2},\\sigma_{3},\\sigma_{4}\\}=\\{9,3,0,0\\}$. The number of singular values is indeed $p = \\min(m,n) = \\min(4,5) = 4$.\n\nFirst, we construct an explicit matrix $A$ with these singular values. The Singular Value Decomposition of any matrix $A \\in \\mathbb{R}^{m \\times n}$ is given by $A = U \\Sigma V^{\\top}$, where $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{n \\times n}$ are orthogonal matrices, and $\\Sigma \\in \\mathbb{R}^{m \\times n}$ is a rectangular diagonal matrix containing the singular values $\\sigma_i \\ge 0$. The columns of $U$ are the left singular vectors $\\{u_i\\}$, and the columns of $V$ are the right singular vectors $\\{v_j\\}$.\n\nFor our problem, the matrix $\\Sigma$ has dimensions $4 \\times 5$. The singular values are placed on its main diagonal in descending order:\n$$\n\\Sigma = \\begin{pmatrix}\n\\sigma_{1} & 0 & 0 & 0 & 0 \\\\\n0 & \\sigma_{2} & 0 & 0 & 0 \\\\\n0 & 0 & \\sigma_{3} & 0 & 0 \\\\\n0 & 0 & 0 & \\sigma_{4} & 0\n\\end{pmatrix} = \\begin{pmatrix}\n9 & 0 & 0 & 0 & 0 \\\\\n0 & 3 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 0\n\\end{pmatrix}\n$$\nTo construct the simplest possible matrix $A$, we can choose the standard orthonormal bases for the columns of $U$ and $V$. That is, we choose $U$ to be the identity matrix $I_4 \\in \\mathbb{R}^{4 \\times 4}$ and $V$ to be the identity matrix $I_5 \\in \\mathbb{R}^{5 \\times 5}$.\nWith these choices, the matrix $A$ becomes:\n$$\nA = U \\Sigma V^{\\top} = I_4 \\Sigma (I_5)^{\\top} = \\Sigma = \\begin{pmatrix}\n9 & 0 & 0 & 0 & 0 \\\\\n0 & 3 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 0\n\\end{pmatrix}\n$$\nThe left singular vectors are the columns of $U=I_4$, which are the standard basis vectors $\\{e_1, e_2, e_3, e_4\\}$ in $\\mathbb{R}^4$. The right singular vectors are the columns of $V=I_5$, which are the standard basis vectors $\\{e_1, e_2, e_3, e_4, e_5\\}$ in $\\mathbb{R}^5$.\n\nNext, we determine the rank $r$, right nullity $\\nu$, and left nullity $\\mu$ for this constructed matrix $A$. The problem requires using the definition of singular values via the eigenvalues of $A^{\\top}A$.\nLet's compute $A^{\\top}A$. The transpose of $A$ is:\n$$\nA^{\\top} = \\begin{pmatrix}\n9 & 0 & 0 & 0 \\\\\n0 & 3 & 0 & 0 \\\\\n0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0\n\\end{pmatrix} \\in \\mathbb{R}^{5 \\times 4}\n$$\nThe product $A^{\\top}A$ is a $5 \\times 5$ matrix:\n$$\nA^{\\top}A = \\begin{pmatrix}\n9 & 0 & 0 & 0 \\\\\n0 & 3 & 0 & 0 \\\\\n0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0\n\\end{pmatrix}\n\\begin{pmatrix}\n9 & 0 & 0 & 0 & 0 \\\\\n0 & 3 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 0\n\\end{pmatrix}\n= \\begin{pmatrix}\n81 & 0 & 0 & 0 & 0 \\\\\n0 & 9 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 0\n\\end{pmatrix}\n$$\nThe eigenvalues of $A^{\\top}A$ are the diagonal entries $\\{\\lambda_1, \\lambda_2, \\lambda_3, \\lambda_4, \\lambda_5\\} = \\{81, 9, 0, 0, 0\\}$. The singular values are the square roots of the non-zero eigenvalues of $A^{\\top}A$. Here, $\\sigma_1 = \\sqrt{81} = 9$ and $\\sigma_2 = \\sqrt{9} = 3$. The remaining eigenvalues are zero, corresponding to the zero singular values.\n\nThe rank of a matrix, $r$, is defined as the number of its non-zero singular values. From the given multiset $\\{9, 3, 0, 0\\}$, there are exactly two non-zero singular values. Therefore, the rank is $r=2$.\n\nThe right nullity, $\\nu$, is the dimension of the null space (kernel) of $A$, i.e., $\\nu = \\dim(\\ker(A))$. The rank-nullity theorem for $A \\in \\mathbb{R}^{m \\times n}$ states that $\\text{rank}(A) + \\dim(\\ker(A)) = n$.\nUsing our values, $r + \\nu = n$, which becomes $2 + \\nu = 5$. Solving for $\\nu$ yields $\\nu = 3$.\n\nThe left nullity, $\\mu$, is the dimension of the null space of the transpose of $A$, i.e., $\\mu = \\dim(\\ker(A^{\\top}))$. A fundamental property of linear algebra is that $\\text{rank}(A) = \\text{rank}(A^{\\top})$. Applying the rank-nullity theorem to the matrix $A^{\\top} \\in \\mathbb{R}^{n \\times m}$, we have $\\text{rank}(A^{\\top}) + \\dim(\\ker(A^{\\top})) = m$.\nUsing our values, $r + \\mu = m$, which becomes $2 + \\mu = 4$. Solving for $\\mu$ yields $\\mu = 2$.\nThus, for our constructed matrix, the triplet is $(r, \\nu, \\mu) = (2, 3, 2)$.\n\nFinally, we explain the constraints imposed by the presence of exactly two zero singular values for any matrix $A \\in \\mathbb{R}^{4 \\times 5}$ with these singular values. The total number of singular values is $p = \\min(4,5)=4$. The rank $r$ of any matrix is equal to the number of its non-zero singular values. The given multiset $\\{9, 3, 0, 0\\}$ contains two non-zero values ($9$ and $3$) and two zero values. Therefore, the number of non-zero singular values is fixed at $2$. This immediately forces the rank of any such matrix to be $r=2$.\n\nThe choice of the orthogonal matrices $U$ and $V$ in the SVD $A=U\\Sigma V^\\top$ determines the specific entries of $A$, but it does not alter the singular values or the rank. Since the rank $r$ is unalterably fixed at $r=2$ by the given singular values, and the dimensions $m=4$ and $n=5$ are fixed, the nullities $\\nu$ and $\\mu$ are also uniquely determined by the rank-nullity theorem:\n1.  Right nullity: $\\nu = n - r = 5 - 2 = 3$. This corresponds to the number of zero eigenvalues of $A^{\\top}A$, which is $3$.\n2.  Left nullity: $\\mu = m - r = 4 - 2 = 2$. This corresponds to the number of zero rows in $\\Sigma$.\n\nThe presence of exactly two zero singular values (out of $\\min(m, n)=4$) is the defining constraint. It dictates that the rank must be $r = \\min(m,n) - (\\text{number of zero } \\sigma_i) = 4-2=2$. This single value, $r=2$, in conjunction with the matrix dimensions $m=4$ and $n=5$, completely determines the triplet $(r, \\nu, \\mu)$ for any real matrix $A$ satisfying the problem's conditions.\n\nTherefore, the triplet $(r,\\nu,\\mu)$ is invariant and must be $(2,3,2)$.", "answer": "$$\n\\boxed{\\begin{pmatrix} 2 & 3 & 2 \\end{pmatrix}}\n$$", "id": "3568476"}, {"introduction": "While non-negative matrices have well-known properties for their eigenvectors, such as those described by the Perron-Frobenius theorem, our intuition does not always translate directly to singular vectors. This problem explores a fascinating counter-example involving a block-structured non-negative matrix where the leading left and right singular vectors have completely disjoint supports. This practice challenges you to use first principles to reveal how a matrix's internal structure can lead to such non-obvious geometric properties, deepening your understanding beyond simple, irreducible cases [@problem_id:3568497].", "problem": "Consider the square matrix $A \\in \\mathbb{R}^{4 \\times 4}$ defined by the block structure\n$$\nA \\;=\\; \\begin{pmatrix}\n0 & 0 & \\alpha & \\beta \\\\\n0 & 0 & \\gamma & \\delta \\\\\n0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0\n\\end{pmatrix},\n$$\nwhere $\\alpha, \\beta, \\gamma, \\delta \\ge 0$ are real parameters. Let the singular value decomposition (SVD) denote any factorization of $A$ into $A = U \\Sigma V^{\\top}$ with $U \\in \\mathbb{R}^{4 \\times 4}$ and $V \\in \\mathbb{R}^{4 \\times 4}$ orthogonal and $\\Sigma \\in \\mathbb{R}^{4 \\times 4}$ diagonal with nonnegative entries, and let $\\sigma_1(A)$ be the largest singular value of $A$ with corresponding unit left and right singular vectors $u_1 \\in \\mathbb{R}^4$ and $v_1 \\in \\mathbb{R}^4$, respectively, so that $A v_1 = \\sigma_1(A)\\, u_1$ and $A^{\\top} u_1 = \\sigma_1(A)\\, v_1$.\n\nUsing only fundamental definitions of singular values and vectors and basic properties of block matrices, do the following:\n- Derive, from first principles, the structure of $Av$ for an arbitrary $v \\in \\mathbb{R}^4$ and use it to argue that any maximizing unit right singular vector $v_1$ must have support contained entirely in the last $2$ coordinates, while the associated unit left singular vector $u_1$ must have support contained entirely in the first $2$ coordinates. Conclude that the supports of $u_1$ and $v_1$ are disjoint, even though $A \\ge 0$.\n- Compute the largest singular value $\\sigma_1(A)$ explicitly as a closed-form analytic expression in terms of $\\alpha, \\beta, \\gamma, \\delta$.\n\nProvide the closed-form analytic expression for $\\sigma_1(A)$ as your final answer. No rounding is required and no units are involved. Your derivation must start from the definition of singular values as operator norms and from the relations $A v_1 = \\sigma_1(A)\\, u_1$ and $A^{\\top} u_1 = \\sigma_1(A)\\, v_1$, and should not rely on any pre-stated specialized formulas beyond well-tested facts about eigenvalues of symmetric $2 \\times 2$ matrices. You may assume $\\alpha, \\beta, \\gamma, \\delta$ are arbitrary fixed nonnegative real numbers.", "solution": "The problem asks for a derivation of the structural properties of the leading singular vectors of a given $4 \\times 4$ matrix $A$ and for an explicit formula for its largest singular value, $\\sigma_1(A)$. The derivation must proceed from first principles.\n\nThe matrix is given by:\n$$\nA \\;=\\; \\begin{pmatrix}\n0 & 0 & \\alpha & \\beta \\\\\n0 & 0 & \\gamma & \\delta \\\\\n0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0\n\\end{pmatrix}\n$$\nwhere $\\alpha, \\beta, \\gamma, \\delta \\ge 0$. We can write $A$ in a $2 \\times 2$ block form as:\n$$\nA = \\begin{pmatrix} O_{2} & B \\\\ O_{2} & O_{2} \\end{pmatrix}, \\quad \\text{where} \\quad B = \\begin{pmatrix} \\alpha & \\beta \\\\ \\gamma & \\delta \\end{pmatrix} \\quad \\text{and} \\quad O_{2} = \\begin{pmatrix} 0 & 0 \\\\ 0 & 0 \\end{pmatrix}.\n$$\n\nThe largest singular value $\\sigma_1(A)$ is defined as the operator $2$-norm of $A$:\n$$\n\\sigma_1(A) = \\max_{v \\in \\mathbb{R}^4, \\|v\\|_2=1} \\|Av\\|_2.\n$$\nLet an arbitrary vector $v \\in \\mathbb{R}^4$ with $\\|v\\|_2=1$ be partitioned conformally with $A$:\n$$\nv = \\begin{pmatrix} v_a \\\\ v_b \\end{pmatrix}, \\quad \\text{where} \\quad v_a = \\begin{pmatrix} v_1 \\\\ v_2 \\end{pmatrix} \\in \\mathbb{R}^2 \\quad \\text{and} \\quad v_b = \\begin{pmatrix} v_3 \\\\ v_4 \\end{pmatrix} \\in \\mathbb{R}^2.\n$$\nThe unit norm condition on $v$ is $\\|v\\|_2^2 = \\|v_a\\|_2^2 + \\|v_b\\|_2^2 = 1$.\n\nNow, we compute the product $Av$:\n$$\nAv = \\begin{pmatrix} O_{2} & B \\\\ O_{2} & O_{2} \\end{pmatrix} \\begin{pmatrix} v_a \\\\ v_b \\end{pmatrix} = \\begin{pmatrix} O_{2}v_a + Bv_b \\\\ O_{2}v_a + O_{2}v_b \\end{pmatrix} = \\begin{pmatrix} Bv_b \\\\ 0 \\end{pmatrix}.\n$$\nThe squared $2$-norm of $Av$ is then:\n$$\n\\|Av\\|_2^2 = \\left\\| \\begin{pmatrix} Bv_b \\\\ 0 \\end{pmatrix} \\right\\|_2^2 = \\|Bv_b\\|_2^2 + \\|0\\|_2^2 = \\|Bv_b\\|_2^2.\n$$\nThe maximization problem for $\\sigma_1(A)^2$ becomes:\n$$\n\\sigma_1(A)^2 = \\max_{\\|v_a\\|_2^2 + \\|v_b\\|_2^2 = 1} \\|Bv_b\\|_2^2.\n$$\nThe objective function $\\|Bv_b\\|_2^2$ depends only on the subvector $v_b$. To maximize this quantity subject to the constraint on the norms of $v_a$ and $v_b$, we must allocate the entire \"norm budget\" to $v_b$. This is achieved by setting $\\|v_a\\|_2 = 0$, which implies $v_a = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$, and consequently $\\|v_b\\|_2 = 1$.\n\nTherefore, any right singular vector $v_1$ corresponding to $\\sigma_1(A)$ must be of the form:\n$$\nv_1 = \\begin{pmatrix} 0 \\\\ 0 \\\\ v_{1,3} \\\\ v_{1,4} \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ v_{1,b} \\end{pmatrix}\n$$\nwhere $v_{1,b} = \\begin{pmatrix} v_{1,3} \\\\ v_{1,4} \\end{pmatrix} \\in \\mathbb{R}^2$ is a unit vector, i.e., $\\|v_{1,b}\\|_2 = 1$. The support of $v_1$, defined as the set of indices of its non-zero components, is thus contained entirely in the last $2$ coordinates, i.e., $\\text{supp}(v_1) \\subseteq \\{3, 4\\}$.\n\nNext, we find the structure of the corresponding left singular vector $u_1$. From the SVD relation $Av_1 = \\sigma_1(A) u_1$, we have:\n$$\n\\sigma_1(A) u_1 = A v_1 = \\begin{pmatrix} Bv_{1,b} \\\\ 0 \\end{pmatrix}.\n$$\nIf $\\sigma_1(A) = 0$, then $A$ must be the zero matrix (as all its entries are non-negative), and the problem is trivial. Assuming $\\sigma_1(A) > 0$, we can write:\n$$\nu_1 = \\frac{1}{\\sigma_1(A)} \\begin{pmatrix} Bv_{1,b} \\\\ 0 \\end{pmatrix}.\n$$\nThis shows that the last two components of $u_1$ are zero. Thus, $u_1$ must be of the form:\n$$\nu_1 = \\begin{pmatrix} u_{1,1} \\\\ u_{1,2} \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} u_{1,a} \\\\ 0 \\end{pmatrix}\n$$\nwhere $u_{1,a} \\in \\mathbb{R}^2$. The support of $u_1$ is contained entirely in the first $2$ coordinates, i.e., $\\text{supp}(u_1) \\subseteq \\{1, 2\\}$.\nSince $\\text{supp}(u_1) \\subseteq \\{1, 2\\}$ and $\\text{supp}(v_1) \\subseteq \\{3, 4\\}$, the intersection of their supports is the empty set. We conclude that the supports of $u_1$ and $v_1$ are disjoint. This is possible because the matrix $A$, while non-negative, is reducible.\n\nNow, we compute the value of $\\sigma_1(A)$. From our previous analysis:\n$$\n\\sigma_1(A) = \\max_{\\|v_b\\|_2=1} \\|Bv_b\\|_2.\n$$\nThis is precisely the definition of the largest singular value of the $2 \\times 2$ matrix $B$, so $\\sigma_1(A) = \\sigma_1(B)$. The singular values of $B$ are the square roots of the eigenvalues of the symmetric positive semi-definite matrix $B^{\\top}B$. Let us compute this matrix:\n$$\nM = B^{\\top}B = \\begin{pmatrix} \\alpha & \\gamma \\\\ \\beta & \\delta \\end{pmatrix} \\begin{pmatrix} \\alpha & \\beta \\\\ \\gamma & \\delta \\end{pmatrix} = \\begin{pmatrix} \\alpha^2+\\gamma^2 & \\alpha\\beta+\\gamma\\delta \\\\ \\alpha\\beta+\\gamma\\delta & \\beta^2+\\delta^2 \\end{pmatrix}.\n$$\nThe eigenvalues $\\lambda$ of this $2 \\times 2$ matrix $M$ are the roots of its characteristic equation $\\det(M - \\lambda I) = 0$. For a general $2 \\times 2$ symmetric matrix $\\begin{pmatrix} a & c \\\\ c & b \\end{pmatrix}$, the eigenvalues are given by $\\lambda = \\frac{(a+b) \\pm \\sqrt{(a-b)^2 + 4c^2}}{2}$.\nLet $S_1 = \\alpha^2+\\gamma^2$ and $S_2 = \\beta^2+\\delta^2$, and $P = \\alpha\\beta+\\gamma\\delta$. Then $M = \\begin{pmatrix} S_1 & P \\\\ P & S_2 \\end{pmatrix}$.\nThe trace of $M$ is $\\text{tr}(M) = S_1+S_2 = \\alpha^2+\\beta^2+\\gamma^2+\\delta^2$.\nThe eigenvalues of $M$ are:\n$$\n\\lambda = \\frac{(S_1+S_2) \\pm \\sqrt{(S_1-S_2)^2 + 4P^2}}{2}.\n$$\nThe largest eigenvalue, $\\lambda_{\\max}(M)$, corresponds to the '$+$' sign:\n$$\n\\lambda_{\\max}(M) = \\frac{1}{2} \\left( (S_1+S_2) + \\sqrt{(S_1-S_2)^2 + 4P^2} \\right).\n$$\nSubstituting the expressions for $S_1$, $S_2$, and $P$:\n$$\n\\lambda_{\\max}(M) = \\frac{1}{2} \\left( (\\alpha^2+\\gamma^2+\\beta^2+\\delta^2) + \\sqrt{((\\alpha^2+\\gamma^2) - (\\beta^2+\\delta^2))^2 + 4(\\alpha\\beta+\\gamma\\delta)^2} \\right).\n$$\nThe largest singular value of $A$ is the square root of this value:\n$$\n\\sigma_1(A) = \\sigma_1(B) = \\sqrt{\\lambda_{\\max}(B^{\\top}B)}.\n$$\nThus, the final expression for $\\sigma_1(A)$ is:\n$$\n\\sigma_1(A) = \\sqrt{\\frac{1}{2} \\left( \\alpha^2+\\beta^2+\\gamma^2+\\delta^2 + \\sqrt{\\left( (\\alpha^2+\\gamma^2) - (\\beta^2+\\delta^2) \\right)^2 + 4(\\alpha\\beta+\\gamma\\delta)^2} \\right)}.\n$$", "answer": "$$\n\\boxed{\\sqrt{\\frac{1}{2} \\left( \\alpha^2+\\beta^2+\\gamma^2+\\delta^2 + \\sqrt{\\left( (\\alpha^2+\\gamma^2) - (\\beta^2+\\delta^2) \\right)^2 + 4(\\alpha\\beta+\\gamma\\delta)^2} \\right)}}\n$$", "id": "3568497"}, {"introduction": "Moving from analysis to synthesis, this advanced practice explores the powerful concept of designing structured perturbations to achieve specific goals. Instead of merely observing the effects of a random perturbation, you are tasked with creating one that selectively modifies a matrix's singular spectrum while leaving its dominant components untouched. This exercise demonstrates how to use orthogonal projectors derived from the SVD to isolate and act upon specific subspaces, a sophisticated technique central to model reduction, filtering, and control theory [@problem_id:3568465].", "problem": "Let $A \\in \\mathbb{R}^{n \\times n}$ admit a Singular Value Decomposition (SVD) given by $A = U \\Sigma V^{\\top}$, where $U \\in \\mathbb{R}^{n \\times n}$ and $V \\in \\mathbb{R}^{n \\times n}$ are orthogonal matrices, and $\\Sigma = \\operatorname{diag}(\\sigma_{1}, \\sigma_{2}, \\dots, \\sigma_{n})$ with $\\sigma_{1} \\geq \\sigma_{2} \\geq \\dots \\geq \\sigma_{n} \\geq 0$. Fix an integer $k$ with $1 \\leq k \\leq n-1$ and assume $\\sigma_{k} > \\sigma_{k+1}$. Define $U_{k} := U(:,1{:}k)$ and $V_{k} := V(:,1{:}k)$, and let $P_{U} := I - U_{k}U_{k}^{\\top}$ and $P_{V} := I - V_{k}V_{k}^{\\top}$ be the orthogonal projectors onto the complements of the leading left and right singular subspaces, respectively.\n\nConsider the scheme that, given any template perturbation $F \\in \\mathbb{R}^{n \\times n}$, produces an orthogonalized perturbation $E := P_{U} F P_{V}$. Restrict attention to rank-$1$ templates $F$ with spectral norm bounded by $\\|F\\|_{2} \\leq \\gamma$, where $\\gamma$ is a positive scalar satisfying $0  \\gamma  \\sigma_{k} - \\sigma_{k+1}$.\n\nUsing only the fundamental definitions of singular values, the unitary invariance of singular values and spectral norms, and properties of orthogonal projectors, determine the supremum of the $(k+1)$-st singular value of the perturbed matrix $A + E$ over all rank-$1$ templates $F$ with $\\|F\\|_{2} \\leq \\gamma$ passed through the orthogonalization scheme above. In your reasoning, explain why the top $k$ singular values of $A + E$ are unchanged by this scheme under the stated bound on $\\gamma$, and describe a structured rank-$1$ perturbation that attains the supremum. Provide your final answer as a single closed-form analytic expression in terms of $\\sigma_{k+1}$ and $\\gamma$.", "solution": "The problem asks for the supremum of the $(k+1)$-st singular value of a perturbed matrix $A+E$, where $E$ is a structured perturbation derived from a rank-$1$ template matrix $F$. We will solve this by transforming the problem into the singular vector basis of $A$, which simplifies the structure of the matrices involved.\n\nLet the Singular Value Decomposition (SVD) of $A \\in \\mathbb{R}^{n \\times n}$ be $A = U \\Sigma V^{\\top}$, where $U, V \\in \\mathbb{R}^{n \\times n}$ are orthogonal matrices and $\\Sigma = \\operatorname{diag}(\\sigma_{1}, \\dots, \\sigma_{n})$ with $\\sigma_{1} \\geq \\dots \\geq \\sigma_{n} \\geq 0$. The columns of $U$ are the left singular vectors $\\{u_i\\}_{i=1}^n$ and the columns of $V$ are the right singular vectors $\\{v_i\\}_{i=1}^n$.\n\nThe problem defines subspaces based on the first $k$ singular vectors. Let $U_k = U(:,1{:}k) = [u_1, \\dots, u_k]$ and $V_k = V(:,1{:}k) = [v_1, \\dots, v_k]$. The projectors are $P_U = I - U_k U_k^{\\top}$ and $P_V = I - V_k V_k^{\\top}$. $P_U$ projects onto the orthogonal complement of the space spanned by the first $k$ left singular vectors, i.e., $\\operatorname{span}\\{u_{k+1}, \\dots, u_n\\}$. Similarly, $P_V$ projects onto $\\operatorname{span}\\{v_{k+1}, \\dots, v_n\\}$.\n\nThe perturbation is given by $E = P_U F P_V$ for a template matrix $F$. We consider the perturbed matrix $A+E$. The singular values of $A+E$ are identical to the singular values of $U^{\\top}(A+E)V$ because the spectral norm and singular values are invariant under orthogonal transformations. Let us analyze this transformed matrix:\n$$\nU^{\\top}(A+E)V = U^{\\top}AV + U^{\\top}EV\n$$\nThe first term is simply the diagonal matrix of singular values:\n$$\nU^{\\top}AV = U^{\\top}(U\\Sigma V^{\\top})V = (U^{\\top}U)\\Sigma(V^{\\top}V) = I\\Sigma I = \\Sigma\n$$\nFor the second term, we first express the projectors in the singular vector basis. Let's partition $U = [U_k, U_{k^\\perp}]$ and $V = [V_k, V_{k^\\perp}]$, where $U_{k^\\perp}=U(:,k+1{:}n)$ and $V_{k^\\perp}=V(:,k+1{:}n)$. Then $U_k U_k^{\\top}$ is the projector onto $\\operatorname{span}(U_k)$. We have $U^{\\top} U_k = \\begin{pmatrix} I_k \\\\ 0 \\end{pmatrix}$ and $U_k^{\\top} U = \\begin{pmatrix} I_k  0 \\end{pmatrix}$.\nThus, $U_k U_k^\\top = U \\begin{pmatrix} I_k \\\\ 0 \\end{pmatrix} \\begin{pmatrix} I_k  0 \\end{pmatrix} U^\\top = U \\begin{pmatrix} I_k  0 \\\\ 0  0 \\end{pmatrix} U^\\top$.\nIt follows that $P_U = I - U_k U_k^\\top = U I U^\\top - U \\begin{pmatrix} I_k  0 \\\\ 0  0 \\end{pmatrix} U^\\top = U \\begin{pmatrix} 0  0 \\\\ 0  I_{n-k} \\end{pmatrix} U^\\top$.\nSimilarly, $P_V = V \\begin{pmatrix} 0  0 \\\\ 0  I_{n-k} \\end{pmatrix} V^\\top$.\n\nNow, we can compute $U^{\\top}EV$:\n$$\nU^{\\top}EV = U^{\\top} \\left( U \\begin{pmatrix} 0  0 \\\\ 0  I_{n-k} \\end{pmatrix} U^\\top \\right) F \\left( V \\begin{pmatrix} 0  0 \\\\ 0  I_{n-k} \\end{pmatrix} V^\\top \\right) V\n$$\n$$\n= \\begin{pmatrix} 0  0 \\\\ 0  I_{n-k} \\end{pmatrix} (U^\\top F V) \\begin{pmatrix} 0  0 \\\\ 0  I_{n-k} \\end{pmatrix}\n$$\nLet's define the transformed template $\\tilde{F} = U^\\top F V$. Partition $\\tilde{F}$ into blocks corresponding to the partition of $\\Sigma$:\n$$\n\\tilde{F} = \\begin{pmatrix} \\tilde{F}_{11}  \\tilde{F}_{12} \\\\ \\tilde{F}_{21}  \\tilde{F}_{22} \\end{pmatrix}\n$$\nwhere $\\tilde{F}_{11} \\in \\mathbb{R}^{k \\times k}$ and $\\tilde{F}_{22} \\in \\mathbb{R}^{(n-k) \\times (n-k)}$. The transformed perturbation term becomes:\n$$\nU^{\\top}EV = \\begin{pmatrix} 0  0 \\\\ 0  I_{n-k} \\end{pmatrix} \\begin{pmatrix} \\tilde{F}_{11}  \\tilde{F}_{12} \\\\ \\tilde{F}_{21}  \\tilde{F}_{22} \\end{pmatrix} \\begin{pmatrix} 0  0 \\\\ 0  I_{n-k} \\end{pmatrix} = \\begin{pmatrix} 0  0 \\\\ 0  \\tilde{F}_{22} \\end{pmatrix}\n$$\nCombining this with the transformed unperturbed part, we get:\n$$\nU^{\\top}(A+E)V = \\Sigma + \\begin{pmatrix} 0  0 \\\\ 0  \\tilde{F}_{22} \\end{pmatrix} = \\begin{pmatrix} \\Sigma_1  0 \\\\ 0  \\Sigma_2 \\end{pmatrix} + \\begin{pmatrix} 0  0 \\\\ 0  \\tilde{F}_{22} \\end{pmatrix} = \\begin{pmatrix} \\Sigma_1  0 \\\\ 0  \\Sigma_2 + \\tilde{F}_{22} \\end{pmatrix}\n$$\nwhere $\\Sigma_1 = \\operatorname{diag}(\\sigma_1, \\dots, \\sigma_k)$ and $\\Sigma_2 = \\operatorname{diag}(\\sigma_{k+1}, \\dots, \\sigma_n)$.\n\nThe singular values of the block-diagonal matrix $U^{\\top}(A+E)V$ are the union of the singular values of the diagonal blocks $\\Sigma_1$ and $\\Sigma_2 + \\tilde{F}_{22}$.\nThe singular values of $\\Sigma_1$ are its diagonal entries $\\{\\sigma_1, \\dots, \\sigma_k\\}$.\n\nNow we explain why the top $k$ singular values of $A+E$ are unchanged. This requires showing that all singular values of $\\Sigma_2 + \\tilde{F}_{22}$ are smaller than $\\sigma_k$. The largest singular value of $\\Sigma_2 + \\tilde{F}_{22}$ can be bounded using Weyl's inequality for singular values:\n$$\n\\sigma_1(\\Sigma_2 + \\tilde{F}_{22}) \\leq \\sigma_1(\\Sigma_2) + \\sigma_1(\\tilde{F}_{22})\n$$\nWe have $\\sigma_1(\\Sigma_2) = \\sigma_{k+1}$. The norm of the block $\\tilde{F}_{22}$ is bounded by the norm of the full matrix $\\tilde{F}$:\n$$\n\\sigma_1(\\tilde{F}_{22}) = \\|\\tilde{F}_{22}\\|_2 \\leq \\|\\tilde{F}\\|_2 = \\|U^\\top F V\\|_2 = \\|F\\|_2\n$$\nThe last equality holds due to the unitary invariance of the spectral norm. We are given $\\|F\\|_2 \\leq \\gamma$. Thus, $\\sigma_1(\\tilde{F}_{22}) \\leq \\gamma$.\nCombining these, we get:\n$$\n\\sigma_1(\\Sigma_2 + \\tilde{F}_{22}) \\leq \\sigma_{k+1} + \\gamma\n$$\nThe problem states that $0  \\gamma  \\sigma_k - \\sigma_{k+1}$, which implies $\\sigma_{k+1} + \\gamma  \\sigma_k$.\nSo, the largest singular value of the bottom-right block is strictly smaller than $\\sigma_k$, which is the smallest singular value of the top-left block $\\Sigma_1$. This guarantees that the set of the largest $k$ singular values of $A+E$ is exactly $\\{\\sigma_1, \\dots, \\sigma_k\\}$. They are unchanged by the perturbation $E$.\n\nThe $(k+1)$-st singular value of $A+E$, denoted $\\sigma_{k+1}(A+E)$, must be the largest singular value of the block $\\Sigma_2 + \\tilde{F}_{22}$. Our task is to find the supremum of $\\sigma_1(\\Sigma_2 + \\tilde{F}_{22})$ over all rank-$1$ templates $F$ with $\\|F\\|_2 \\leq \\gamma$.\nFrom the bound derived above, we have:\n$$\n\\sup \\sigma_{k+1}(A+E) = \\sup \\sigma_1(\\Sigma_2 + \\tilde{F}_{22}) \\leq \\sigma_{k+1} + \\gamma\n$$\nWe now demonstrate that this upper bound is attainable. We need to construct a valid template $F$ such that $\\sigma_1(\\Sigma_2 + \\tilde{F}_{22}) = \\sigma_{k+1} + \\gamma$.\nThis equality in Weyl's inequality is achieved if the matrix perturbation is aligned with the top singular vectors of the original matrix. The top singular value of $\\Sigma_2 = \\operatorname{diag}(\\sigma_{k+1}, \\dots, \\sigma_n)$ is $\\sigma_{k+1}$, and the corresponding left and right singular vectors (in the $(n-k)$-dimensional space) are both $e_1 = (1, 0, \\dots, 0)^\\top$.\nWe need to find an $F$ such that $\\tilde{F}_{22}$ is a rank-$1$ matrix with singular value $\\gamma$ and whose top singular vectors are also $e_1$. The canonical choice for such a matrix is $\\tilde{F}_{22} = \\gamma e_1 e_1^\\top$.\n\nLet's construct the template $F$ that yields this $\\tilde{F}_{22}$.\nLet $F = \\gamma u_{k+1}v_{k+1}^{\\top}$.\nThis $F$ is a rank-$1$ matrix. Its spectral norm is $\\|F\\|_2 = \\gamma \\|u_{k+1}\\|_2 \\|v_{k+1}\\|_2 = \\gamma \\cdot 1 \\cdot 1 = \\gamma$. So, this choice of $F$ is valid.\n\nFor this $F$, let's compute the resulting perturbation $E = P_U F P_V$. The projector $P_U$ maps any vector orthogonal to $\\operatorname{span}\\{u_1, \\dots, u_k\\}$ to itself. Since $u_{k+1}$ is in this orthogonal complement, $P_U u_{k+1} = u_{k+1}$. Similarly, $P_V v_{k+1} = v_{k+1}$.\nTherefore, $E = P_U (\\gamma u_{k+1}v_{k+1}^{\\top}) P_V = \\gamma (P_U u_{k+1}) (P_V v_{k+1})^\\top = \\gamma u_{k+1}v_{k+1}^{\\top} = F$.\nNow we compute the corresponding $\\tilde{F}_{22}$:\n$$\n\\tilde{F}_{22} = U_{k^\\perp}^\\top F V_{k^\\perp} = U_{k^\\perp}^\\top (\\gamma u_{k+1}v_{k+1}^{\\top}) V_{k^\\perp} = \\gamma (U_{k^\\perp}^\\top u_{k+1}) (V_{k^\\perp}^\\top v_{k+1})^\\top\n$$\nThe vector $U_{k^\\perp}^\\top u_{k+1}$ is a column vector whose $i$-th entry is $u_{k+i}^\\top u_{k+1}$. By orthogonality of the singular vectors, this is $1$ for $i=1$ and $0$ otherwise. So, $U_{k^\\perp}^\\top u_{k+1} = e_1 \\in \\mathbb{R}^{n-k}$. Similarly, $V_{k^\\perp}^\\top v_{k+1} = e_1 \\in \\mathbb{R}^{n-k}$.\nThus, we obtain $\\tilde{F}_{22} = \\gamma e_1 e_1^\\top$.\n\nWith this specific $\\tilde{F}_{22}$, the bottom-right block becomes:\n$$\n\\Sigma_2 + \\tilde{F}_{22} = \\operatorname{diag}(\\sigma_{k+1}, \\sigma_{k+2}, \\dots, \\sigma_n) + \\gamma e_1 e_1^\\top = \\operatorname{diag}(\\sigma_{k+1}+\\gamma, \\sigma_{k+2}, \\dots, \\sigma_n)\n$$\nSince $\\sigma_{k+1} > \\sigma_{k+1+j}$ for $j \\ge 1$, and $\\gamma>0$, the diagonal entries of this matrix are sorted in descending order: $\\sigma_{k+1}+\\gamma > \\sigma_{k+2} \\ge \\dots \\ge \\sigma_n \\ge 0$.\nThe singular values of this diagonal matrix are its diagonal entries. The largest singular value is $\\sigma_{k+1}+\\gamma$.\nThis shows that the template $F=\\gamma u_{k+1}v_{k+1}^\\top$ results in $\\sigma_{k+1}(A+E) = \\sigma_{k+1}+\\gamma$.\n\nSince we found a valid perturbation that achieves the upper bound $\\sigma_{k+1}+\\gamma$, this value is the supremum.", "answer": "$$\\boxed{\\sigma_{k+1} + \\gamma}$$", "id": "3568465"}]}