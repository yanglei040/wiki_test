{"hands_on_practices": [{"introduction": "The mathematical elegance of forward and backward substitution can sometimes mask the practical challenges of finite-precision arithmetic. This exercise demonstrates how the choice of computational path and the magnitude of matrix entries can dramatically affect numerical stability, even when solving mathematically equivalent systems. By analyzing a carefully constructed example [@problem_id:3579228], you will explore the subtle effects of rounding errors and catastrophic cancellation, reinforcing the principle that in numerical computing, *how* you compute is as important as *what* you compute.", "problem": "Consider a lower unit-triangular matrix $L \\in \\mathbb{R}^{4 \\times 4}$ and the exact solution vector $x^{\\star} \\in \\mathbb{R}^{4}$ given by\n$$\nL \\;=\\;\n\\begin{pmatrix}\n1 & 0 & 0 & 0 \\\\\n10^{16} & 1 & 0 & 0 \\\\\n-10^{16} & 10^{-16} & 1 & 0 \\\\\n0 & -10^{-16} & 10^{-16} & 1\n\\end{pmatrix},\n\\qquad\nx^{\\star} \\;=\\;\n\\begin{pmatrix}\n1 \\\\ 1 \\\\ 1 \\\\ 1\n\\end{pmatrix}.\n$$\nDefine the right-hand sides $b \\in \\mathbb{R}^{4}$ and $c \\in \\mathbb{R}^{4}$ by\n$$\nb \\;=\\; L^{\\top} x^{\\star}, \\qquad c \\;=\\; L x^{\\star}.\n$$\nWork in the model of Institute of Electrical and Electronics Engineers 754 (IEEE 754) double-precision (binary64) floating-point arithmetic with round-to-nearest, ties-to-even. Let the unit roundoff be $u = 2^{-53}$, and assume the standard model for each elementary operation,\n$$\n\\operatorname{fl}(a \\circ b) \\;=\\; (a \\circ b)\\,(1 + \\delta), \\qquad |\\delta| \\leq u,\n$$\nfor $\\circ \\in \\{+, -, \\times, \\div\\}$, without fused multiply-add, and with dot products accumulated by naive left-to-right summation in the loop order. Assume $L$ is stored in column-major layout. Two computational paths are considered:\n\n1. Path A: solve $L^{\\top} x = b$ by backward substitution. For index $i$ descending from $4$ to $1$, compute\n$$\nx_i \\;=\\; \\frac{b_i - \\sum_{j=i+1}^{4} L_{j,i} \\, x_j}{L_{i,i}},\n$$\nsumming the inner product over $j = i+1, i+2, \\dots, 4$.\n\n2. Path B: solve $L x = c$ by forward substitution. For index $i$ ascending from $1$ to $4$, compute\n$$\nx_i \\;=\\; \\frac{c_i - \\sum_{j=1}^{i-1} L_{i,j} \\, x_j}{L_{i,i}},\n$$\nsumming the inner product over $j = 1, 2, \\dots, i-1$.\n\nUsing the specified arithmetic model, memory layout, and summation order, analyze the floating-point rounding effects to explain why Path A is numerically superior to Path B for this data. In particular, determine the computed value $\\hat{x}^{(B)}_2$ produced by Path B and then compute the absolute forward error $|\\,\\hat{x}^{(B)}_2 - x^{\\star}_2\\,|$. Express your final answer as a single real-valued number. No rounding is required.", "solution": "The validity of the problem statement is confirmed. It is a well-posed problem in numerical linear algebra, grounded in the standard model of floating-point arithmetic. All necessary data and definitions are provided, and there are no internal contradictions or scientific inaccuracies.\n\nThe problem requires an analysis of two computational paths for solving triangular systems involving a matrix $L$ and its transpose $L^{\\top}$. The superiority of one path over another is determined by its numerical stability in the face of floating-point rounding errors. We will first provide a general explanation for the differing stability, then perform a detailed floating-point analysis of Path B to compute the required error.\n\nThe matrix $L$ and the exact solution $x^{\\star}$ are given as:\n$$\nL \\;=\\;\n\\begin{pmatrix}\n1 & 0 & 0 & 0 \\\\\n10^{16} & 1 & 0 & 0 \\\\\n-10^{16} & 10^{-16} & 1 & 0 \\\\\n0 & -10^{-16} & 10^{-16} & 1\n\\end{pmatrix},\n\\qquad\nx^{\\star} \\;=\\;\n\\begin{pmatrix}\n1 \\\\ 1 \\\\ 1 \\\\ 1\n\\end{pmatrix}.\n$$\nThe right-hand side vectors $b$ and $c$ are defined by $b = L^{\\top} x^{\\star}$ and $c = L x^{\\star}$. Their exact values are:\n$$\nc \\;=\\; L x^{\\star} \\;=\\; \\begin{pmatrix} 1 \\\\ 10^{16} + 1 \\\\ -10^{16} + 10^{-16} + 1 \\\\ 1 \\end{pmatrix}\n$$\n$$\nb \\;=\\; L^{\\top} x^{\\star} \\;=\\; \\begin{pmatrix} 1 + 10^{16} - 10^{16} \\\\ 1 + 10^{-16} - 10^{-16} \\\\ 1 + 10^{-16} \\\\ 1 \\end{pmatrix} \\;=\\; \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 + 10^{-16} \\\\ 1 \\end{pmatrix}\n$$\nThe core of the analysis rests on representing these vectors and performing calculations in IEEE $754$ double-precision arithmetic. The unit roundoff is $u = 2^{-53} \\approx 1.11 \\times 10^{-16}$.\n\nFirst, let's analyze why Path A is numerically superior. The algorithm for Path A solves $L^{\\top} x = b$ using backward substitution. The first step in a computer implementation is to store the vector $b$ in floating-point format, which we denote $\\hat{b}$.\n- $\\hat{b}_1 = \\operatorname{fl}(1) = 1$.\n- $\\hat{b}_2 = \\operatorname{fl}(1) = 1$.\n- $\\hat{b}_3 = \\operatorname{fl}(1 + 10^{-16})$. The unit in the last place for the number $1$ is $\\operatorname{ulp}(1) = 2^{-52}$. An increment is rounded away if it is smaller in magnitude than half of the ulp, which is $2^{-52}/2 = 2^{-53} = u$. We must compare $10^{-16}$ with $u$. Since $\\log_{10}(u) \\approx -15.95$, we have $u \\approx 1.12 \\times 10^{-16}$. Thus, $10^{-16} < u$, and the addition of $10^{-16}$ to $1$ is lost in rounding. So, $\\hat{b}_3 = 1$.\n- $\\hat{b}_4 = \\operatorname{fl}(1) = 1$.\nThe stored right-hand side is $\\hat{b} = (1, 1, 1, 1)^{\\top}$. The backward substitution algorithm then computes the solution $\\hat{x}^{(A)}$ to $L^{\\top} \\hat{x}^{(A)} = \\hat{b}$. This computation turns out to be free of significant rounding errors; for instance, the calculation of $\\hat{x}^{(A)}_1$ involves the term $\\operatorname{fl}(10^{16} \\times \\hat{x}^{(A)}_2 - 10^{16} \\times \\hat{x}^{(A)}_3)$. With $\\hat{x}^{(A)}_2=1$ and $\\hat{x}^{(A)}_3=1$, this evaluates to $\\operatorname{fl}(10^{16} - 10^{16}) = 0$. The final computed solution is $\\hat{x}^{(A)} = (1, 1, 1, 1)^{\\top} = x^{\\star}$. Path A is exceptionally stable, yielding zero forward error.\n\nNow, we analyze Path B, which solves $L x = c$ using forward substitution. We must first store the vector $c$ in floating-point format, yielding $\\hat{c}$.\n- $\\hat{c}_1 = \\operatorname{fl}(1) = 1$.\n- $\\hat{c}_2 = \\operatorname{fl}(10^{16} + 1)$. To determine this, we analyze the representation of numbers around $10^{16}$. The number $10^{16} = 2^{16} \\times 5^{16}$ can be shown to be exactly representable in double precision. The binary exponent $E$ for $10^{16}$ is $53$, so $\\operatorname{ulp}(10^{16}) = 2^{E-52} = 2^{53-52} = 2$. The representable numbers near $10^{16}$ are thus even integers, such as $10^{16}$ and $10^{16}+2$. The value $10^{16}+1$ lies exactly halfway between these two representable numbers. According to the specified \"ties-to-even\" rule, we must choose the representable number whose integer significand is even.\nThe value of a number is $M \\times 2^k$. For $10^{16}$, the significand is $M_1 = 10^{16} \\times 2^{-1} = 5 \\times 10^{15}$, which is an odd integer. For $10^{16}+2$, the significand is $M_2 = (10^{16}+2) \\times 2^{-1} = 5 \\times 10^{15}+1$, which is an even integer. Therefore, the tie is broken by rounding up to $10^{16}+2$. So, $\\hat{c}_2 = 10^{16}+2$. This representation introduces an initial absolute error of $|\\hat{c}_2 - c_2| = |(10^{16}+2) - (10^{16}+1)| = 1$.\n- For completeness, $\\hat{c}_3 = \\operatorname{fl}(1+10^{-16}-10^{16}) = \\operatorname{fl}(\\operatorname{fl}(1+10^{-16}) - 10^{16}) = \\operatorname{fl}(1-10^{16})$. This is also a tie, which rounds to $-10^{16}+2$.\n- $\\hat{c}_4 = \\operatorname{fl}(1) = 1$.\nThe stored right-hand side is $\\hat{c} = (1, 10^{16}+2, -10^{16}+2, 1)^{\\top}$.\n\nThe forward substitution algorithm proceeds as follows to compute $\\hat{x}^{(B)}$:\nFor $i=1$:\n$$ \\hat{x}^{(B)}_1 = \\frac{\\hat{c}_1}{L_{1,1}} = \\frac{1}{1} = 1. $$\nFor $i=2$:\n$$ \\hat{x}^{(B)}_2 = \\frac{\\hat{c}_2 - L_{2,1} \\hat{x}^{(B)}_1}{L_{2,2}} $$\nThe computation is performed in floating-point arithmetic:\n$$ \\hat{x}^{(B)}_2 = \\operatorname{fl}\\left(\\frac{\\operatorname{fl}(\\hat{c}_2 - \\operatorname{fl}(L_{2,1} \\times \\hat{x}^{(B)}_1))}{\\operatorname{fl}(L_{2,2})}\\right) $$\nWith the values $L_{2,1} = 10^{16}$, $L_{2,2} = 1$, $\\hat{x}^{(B)}_1 = 1$, and $\\hat{c}_2 = 10^{16}+2$:\n$$ \\operatorname{fl}(L_{2,1} \\times \\hat{x}^{(B)}_1) = \\operatorname{fl}(10^{16} \\times 1) = 10^{16}. $$\nThe numerator becomes:\n$$ \\operatorname{fl}(\\hat{c}_2 - 10^{16}) = \\operatorname{fl}((10^{16}+2) - 10^{16}) = \\operatorname{fl}(2) = 2. $$\nThe denominator is $1$. Therefore, the computed value is:\n$$ \\hat{x}^{(B)}_2 = 2. $$\nThis differs significantly from the exact solution component $x^{\\star}_2 = 1$. The large initial error introduced when storing $c_2$ is uncovered by the subtraction of two large, nearly equal numbers, a process known as catastrophic cancellation. This instability makes Path B far inferior to Path A for this problem.\n\nThe absolute forward error for the second component of the solution vector in Path B is:\n$$ |\\hat{x}^{(B)}_2 - x^{\\star}_2| = |2 - 1| = 1. $$\nThis error originates from the rounding of $c_2 = 10^{16}+1$ to $\\hat{c}_2 = 10^{16}+2$, showcasing the sensitivity of the forward substitution algorithm to small relative perturbations in the right-hand side when the matrix $L$ contains large elements.", "answer": "$$\n\\boxed{1}\n$$", "id": "3579228"}, {"introduction": "In modern high-performance computing, the cost of moving data often dwarfs the cost of arithmetic operations. This practice delves into one of the most fundamental optimization techniques: blocking. By analyzing the solution of a triangular system with multiple right-hand sides, $L X = B$, you will quantify the performance benefits of processing columns in blocks rather than individually [@problem_id:3579175]. This hands-on derivation of arithmetic intensity will illuminate how restructuring an algorithm to improve data locality is crucial for achieving high performance.", "problem": "Consider solving the lower-triangular system $L X = B$ by forward substitution, where $L \\in \\mathbb{R}^{m \\times m}$ is lower triangular with nonzero diagonal, $X \\in \\mathbb{R}^{m \\times n}$ is the solution, and $B \\in \\mathbb{R}^{m \\times n}$ is the right-hand side. Two implementation strategies are available: (i) solve each column $L x^{(k)} = b^{(k)}$ independently by forward substitution for $k = 1, 2, \\dots, n$, and (ii) solve $L X_j = B_j$ in blocks of $b$ right-hand sides, where $B$ is partitioned as $B = [B_1 \\, | \\, B_2 \\, | \\, \\dots \\, | \\, B_{n/b}]$ with $B_j \\in \\mathbb{R}^{m \\times b}$ and $n$ is an integer multiple of $b$. Assume a two-level memory model: a fast memory that can retain $b$ columns of $B$ and the corresponding $b$ columns of $X$ (i.e., the current block $B_j$ and $X_j$) for the duration of the block computation, but cannot retain $L$ beyond the current streaming pass; and a slow memory (main memory) from which data are loaded and to which data are stored. Let each floating-point element transfer (load or store) move $w$ bytes. Adopt the following scientifically realistic assumptions:\n- Every element of $B$ is read exactly once from slow memory and every element of $X$ is written exactly once to slow memory (in-place update is permitted).\n- In the column-by-column strategy, the entire lower-triangular part of $L$ is streamed once per column solve from slow memory.\n- In the blocked strategy, the entire lower-triangular part of $L$ is streamed once per block of $b$ columns from slow memory.\nDefine arithmetic intensity $\\mathcal{I}$ as the ratio of floating-point operations to total bytes moved between slow and fast memory. Starting from the fundamental forward-substitution recurrence for a single column, and without resorting to any prepackaged performance formulas, derive expressions for the total floating-point operation counts and the total data movement for both strategies, and then obtain the ratio of arithmetic intensities of the blocked strategy to the column-wise strategy. Provide your final result as a single closed-form expression in terms of $m$ and $b$ only. No rounding is required, and no physical units are to be reported. Your derivation must begin from the forward-substitution definition and the data-movement assumptions stated above, and reason to the final expression by first principles. Assume that $n$ is a positive integer multiple of $b$ and $L$ has full rank so that forward substitution is well-defined.", "solution": "The objective is to derive the ratio of the arithmetic intensity of a blocked forward-substitution algorithm to that of a column-wise algorithm, $\\frac{\\mathcal{I}_{\\text{blocked}}}{\\mathcal{I}_{\\text{col-wise}}}$. The arithmetic intensity $\\mathcal{I}$ is defined as the ratio of total floating-point operations (flops) to the total bytes of data moved between slow and fast memory. We will derive this ratio from first principles as stipulated.\n\nFirst, we determine the total number of floating-point operations required to solve the system $L X = B$, where $L \\in \\mathbb{R}^{m \\times m}$, $X \\in \\mathbb{R}^{m \\times n}$, and $B \\in \\mathbb{R}^{m \\times n}$. The solution is independent of the implementation strategy (column-wise or blocked). We begin by analyzing the solve for a single column vector, $L x = b$, where $x, b \\in \\mathbb{R}^m$. The forward-substitution recurrence relation for the $i$-th component of the solution vector $x$, $x_i$, is given by:\n$$x_i = \\frac{1}{l_{ii}} \\left( b_i - \\sum_{j=1}^{i-1} l_{ij} x_j \\right) \\quad \\text{for } i = 1, 2, \\dots, m$$\nFor each $i$, the computation of the sum $\\sum_{j=1}^{i-1} l_{ij} x_j$ requires $i-1$ multiplications and $i-2$ additions (for $i>1$). Following this, there is one subtraction from $b_i$ and one division by $l_{ii}$. A standard convention counts a multiply-add pair as $2$ flops. Alternatively, we count additions/subtractions and multiplications/divisions separately. The sum requires $i-1$ multiplications. The terms inside the parentheses require a total of $i-1$ subtractions (or $1$ subtraction and $i-2$ additions). Finally, one division is performed. The total number of flops for computing $x_i$ is $(i-1)$ multiplications, $(i-1)$ additions/subtractions, and $1$ division, which totals $(i-1) + (i-1) + 1 = 2i - 1$ flops. For $i=1$, this gives $2(1)-1=1$ flop (a single division), which is correct.\n\nThe total number of flops for solving for one column, $\\text{flops}_{\\text{col}}$, is the sum over all components $i=1, \\dots, m$:\n$$\\text{flops}_{\\text{col}} = \\sum_{i=1}^{m} (2i-1) = 2\\sum_{i=1}^{m} i - \\sum_{i=1}^{m} 1 = 2 \\frac{m(m+1)}{2} - m = m(m+1) - m = m^2$$\nSince the full system $L X = B$ consists of $n$ such independent column solves, the total number of floating-point operations is:\n$$\\text{Flops}_{\\text{total}} = n \\cdot \\text{flops}_{\\text{col}} = n m^2$$\n\nNext, we analyze the data movement for each strategy. Let $w$ be the number of bytes per floating-point element. The movement of matrices $B$ and $X$ is common to both strategies. According to the problem statement, every element of $B$ is read once from slow memory, and every element of $X$ is written once to slow memory.\nData read for $B$: $m \\times n$ elements $\\times w$ bytes/element $= mnw$ bytes.\nData written for $X$: $m \\times n$ elements $\\times w$ bytes/element $= mnw$ bytes.\nTotal data movement for $B$ and $X$ is $2mnw$ bytes.\n\nStrategy (i): Column-by-column solution.\nIn this strategy, we solve $L x^{(k)} = b^{(k)}$ for each column $k=1, \\dots, n$. The problem states that the entire lower-triangular part of $L$ is streamed from slow memory once for each column solve. The number of non-zero elements in the lower triangle of $L$ is $\\sum_{i=1}^{m} i = \\frac{m(m+1)}{2}$.\nData read for $L$: $n$ columns $\\times \\frac{m(m+1)}{2}$ elements/column $\\times w$ bytes/element $= n \\frac{m(m+1)}{2} w$ bytes.\nThe total data movement for the column-wise strategy, $M_{\\text{col-wise}}$, is the sum of movements for $B$, $X$, and $L$:\n$$M_{\\text{col-wise}} = 2mnw + n \\frac{m(m+1)}{2} w = nw \\left( 2m + \\frac{m(m+1)}{2} \\right) = nw \\left( \\frac{4m + m^2 + m}{2} \\right) = nw \\frac{m(m+5)}{2}$$\nThe arithmetic intensity for the column-wise strategy is:\n$$\\mathcal{I}_{\\text{col-wise}} = \\frac{\\text{Flops}_{\\text{total}}}{M_{\\text{col-wise}}} = \\frac{n m^2}{nw \\frac{m(m+5)}{2}} = \\frac{2m^2}{w m(m+5)} = \\frac{2m}{w(m+5)}$$\n\nStrategy (ii): Blocked solution.\nIn this strategy, the system is solved in blocks of $b$ columns. The total number of columns is $n$, so there are $n/b$ blocks. The problem states that the lower-triangular part of $L$ is streamed from slow memory once per block.\nData read for $L$: $\\frac{n}{b}$ blocks $\\times \\frac{m(m+1)}{2}$ elements/block $\\times w$ bytes/element $= \\frac{n}{b} \\frac{m(m+1)}{2} w$ bytes.\nThe total data movement for the blocked strategy, $M_{\\text{blocked}}$, is the sum of movements for $B$, $X$, and $L$:\n$$M_{\\text{blocked}} = 2mnw + \\frac{n}{b} \\frac{m(m+1)}{2} w = nw \\left( 2m + \\frac{m(m+1)}{2b} \\right) = nw \\left( \\frac{4mb + m^2 + m}{2b} \\right) = nw \\frac{m(m+4b+1)}{2b}$$\nThe arithmetic intensity for the blocked strategy is:\n$$\\mathcal{I}_{\\text{blocked}} = \\frac{\\text{Flops}_{\\text{total}}}{M_{\\text{blocked}}} = \\frac{n m^2}{nw \\frac{m(m+4b+1)}{2b}} = \\frac{2b m^2}{w m(m+4b+1)} = \\frac{2bm}{w(m+4b+1)}$$\n\nFinally, we compute the ratio of the arithmetic intensities.\n$$\\frac{\\mathcal{I}_{\\text{blocked}}}{\\mathcal{I}_{\\text{col-wise}}} = \\frac{\\frac{2bm}{w(m+4b+1)}}{\\frac{2m}{w(m+5)}}$$\nWe can cancel the common terms $2m$ and $w$ from the numerator and the denominator:\n$$\\frac{\\mathcal{I}_{\\text{blocked}}}{\\mathcal{I}_{\\text{col-wise}}} = \\frac{b}{m+4b+1} \\cdot (m+5) = \\frac{b(m+5)}{m+4b+1}$$\nThis expression provides the ratio of arithmetic intensities solely in terms of the matrix dimension $m$ and the block size $b$, as required.", "answer": "$$\\boxed{\\frac{b(m+5)}{m+4b+1}}$$", "id": "3579175"}, {"introduction": "To predict and understand the performance of an algorithm on a real machine, we need models that connect algorithmic properties to hardware capabilities. This exercise introduces the roofline model, a powerful tool for analyzing whether an algorithm is memory-bound or compute-bound [@problem_id:3579232]. You will apply this model to triangular solves, contrasting the performance of single-vector (TRSV) and multi-vector (TRSM) operations and determining the precise conditions under which an algorithm can saturate a processor's peak computational throughput.", "problem": "You are given the task of constructing a principled roofline-model analysis for triangular solves implemented by forward and backward substitution, and then using this analysis to determine when and how batching multiple right-hand sides can saturate compute capability on a given architecture. The core scientific base you must use consists of the following: (i) the algorithmic structure of forward substitution for solving a lower triangular system and backward substitution for an upper triangular system, (ii) the definition of arithmetic intensity as floating-point operations per byte transferred, and (iii) the roofline performance bound, which states that the attainable floating-point throughput in floating-point operations per second is bounded above by the minimum of the machine peak throughput and the product of arithmetic intensity and sustainable memory bandwidth.\n\nStart from the algorithmic definition of forward substitution to solve a lower triangular system. Consider a nonsingular lower triangular matrix of size $n \\times n$, denoted by $L$, and a right-hand side vector $b \\in \\mathbb{R}^n$. The forward substitution algorithm computes $x$ such that $L x = b$ by iterating $i$ from $1$ to $n$ and computing $x_i$ using the previously computed components $x_1,\\dots,x_{i-1}$, together with the corresponding entries of $L$ and $b$. Similarly, backward substitution solves $U x = b$ when $U$ is upper triangular by iterating $i$ from $n$ down to $1$. For the purposes of this problem, treat divisions as floating-point operations and assume double-precision floating point with element size $8$ bytes. Assume a memory-traffic model that counts one read of the triangular matrix, one read of the right-hand side(s), and one write of the solution(s), and assume ideal cache reuse of previously computed components. Use these principles to derive formulae for floating-point operation counts and minimal memory traffic for both a triangular solve with a vector (often denoted as the Level-$2$ Basic Linear Algebra Subprograms (BLAS) routine $\\mathrm{TRSV}$) and a triangular solve with multiple right-hand sides (often denoted as the Level-$3$ Basic Linear Algebra Subprograms (BLAS) routine $\\mathrm{TRSM}$).\n\nUnder the roofline model, let the machine have a peak throughput of $P_{\\text{peak}}$ gigaflops per second (gigafloating-point operations per second), and a sustainable main-memory bandwidth of $B$ gigabytes per second. If the arithmetic intensity is $I$ in floating-point operations per byte, the memory-bound ceiling is $I \\cdot B$ in gigaflops per second. The attainable performance is then $\\min\\{P_{\\text{peak}}, I \\cdot B\\}$ in gigaflops per second.\n\nYour program must do the following for each test case:\n- Given $n$ (matrix size), $m$ (number of right-hand sides to be solved together), $P_{\\text{peak}}$ (in gigaflops per second), and $B$ (in gigabytes per second), compute:\n  1. The arithmetic intensity $I_{\\mathrm{TRSV}}(n)$ for a triangular solve with a single right-hand side using forward or backward substitution, assuming double precision and the minimal memory-traffic model described above.\n  2. The arithmetic intensity $I_{\\mathrm{TRSM}}(n,m)$ for a triangular solve with $m$ right-hand sides using forward or backward substitution, assuming the same model.\n  3. The roofline-limited throughput for $\\mathrm{TRSV}$ and $\\mathrm{TRSM}$, i.e., $\\min\\{P_{\\text{peak}}, I_{\\mathrm{TRSV}}(n) \\cdot B\\}$ and $\\min\\{P_{\\text{peak}}, I_{\\mathrm{TRSM}}(n,m) \\cdot B\\}$, respectively, in gigaflops per second.\n  4. The smallest integer $m_{\\min}$ such that the $\\mathrm{TRSM}$ with $m_{\\min}$ right-hand sides becomes compute-bound, that is, the smallest integer $m \\ge 1$ satisfying $I_{\\mathrm{TRSM}}(n,m) \\cdot B \\ge P_{\\text{peak}}$, if such an integer exists under the arithmetic-intensity limit as $m \\to \\infty$; otherwise, output $-1$ to indicate that no batching over right-hand sides can reach the compute roof at the given $n$, $P_{\\text{peak}}$, and $B$.\n  5. An encoded batching recommendation, as an integer code, based on the following rule:\n     - Output $0$ if $m_{\\min} = -1$ (no right-hand-side batching can saturate compute; prioritize memory-traffic optimizations and panel blocking).\n     - Output $1$ if $m_{\\min} \\ge 1$ and the provided $m$ in the test case is smaller than $m_{\\min}$ (recommend increasing the batch of right-hand sides to at least $m_{\\min}$ with a shared triangular factor, i.e., reformulate as a multi-right-hand-side triangular solve).\n     - Output $2$ if the provided $m$ already satisfies $m \\ge m_{\\min}$ (the configuration is compute-bound; further batching is not required to saturate compute).\n\nYour program should implement these computations and return, for each test case, a list of six values in the following order: $[I_{\\mathrm{TRSV}}(n), I_{\\mathrm{TRSM}}(n,m), \\text{throughput}_{\\mathrm{TRSV}}, \\text{throughput}_{\\mathrm{TRSM}}, m_{\\min}, \\text{recommendation\\_code}]$, where the first four are floats and the last two are integers. The complete output should aggregate the results for all test cases into a single line formatted as a Python-style list of lists. Each float must be rendered with exactly six digits after the decimal point.\n\nTest Suite (use exactly these four cases):\n- Case $1$: $P_{\\text{peak}} = 1000$ gigaflops per second, $B = 1000$ gigabytes per second, $n = 128$, $m = 8$.\n- Case $2$: $P_{\\text{peak}} = 200$ gigaflops per second, $B = 100$ gigabytes per second, $n = 64$, $m = 16$.\n- Case $3$: $P_{\\text{peak}} = 5000$ gigaflops per second, $B = 900$ gigabytes per second, $n = 16$, $m = 64$.\n- Case $4$: $P_{\\text{peak}} = 500$ gigaflops per second, $B = 1000$ gigabytes per second, $n = 8$, $m = 4$.\n\nFinal Output Format:\n- Your program should produce a single line of output containing the results for the four test cases as a comma-separated list enclosed in square brackets, where each test case result is itself a list as specified, with floats printed to exactly six digits after the decimal point. For example: $[[0.123456,0.654321,1.234000,2.468000,3,1],[\\dots],\\dots]$.\n\nAll quantities in this problem are purely mathematical; no physical units beyond the stated gigaflops per second and gigabytes per second are required.", "solution": "We derive arithmetic counts and intensities for triangular solves by starting from the definition of forward and backward substitution in numerical linear algebra.\n\nConsider a nonsingular lower triangular matrix $L \\in \\mathbb{R}^{n \\times n}$ and vector $b \\in \\mathbb{R}^n$. Forward substitution computes $x$ such that $L x = b$ by the recurrence\n$$\nx_i = \\frac{1}{L_{ii}} \\left( b_i - \\sum_{j=1}^{i-1} L_{ij} x_j \\right), \\quad i = 1,2,\\dots,n.\n$$\nBackward substitution for an upper triangular matrix $U$ uses the analogous recurrence with $i$ descending from $n$ to $1$. The operation counts are identical for forward and backward substitution, so we may focus on forward substitution without loss of generality.\n\nFloating-point operation count for $\\mathrm{TRSV}$ (one right-hand side):\n- For each $i$, the inner sum has length $i-1$ and entails $(i-1)$ multiplications and $(i-1)$ additions if we count the subtraction of the sum from $b_i$ as an addition operation (by folding the minus sign into the summand). A more conventional tally separates the dot-product addition count as $(i-2)$ and then adds one subtraction with $b_i$, giving a total of $(i-1)$ additions per row.\n- One division by $L_{ii}$ per row.\n\nSumming over $i = 1$ to $n$, the number of multiplications is $\\sum_{i=1}^n (i-1) = \\frac{n(n-1)}{2}$, the number of additions is $\\sum_{i=1}^n (i-1) = \\frac{n(n-1)}{2}$, and the number of divisions is $n$. If we count divisions as floating-point operations, the total floating-point operations are\n$$\nF_{\\mathrm{TRSV}}(n) = \\frac{n(n-1)}{2} + \\frac{n(n-1)}{2} + n = n^2.\n$$\n\nMinimal memory traffic model for $\\mathrm{TRSV}$ in double precision ($8$ bytes per element):\n- Read the lower triangular matrix entries once: $\\frac{n(n+1)}{2}$ elements.\n- Read the right-hand side vector once: $n$ elements.\n- Write the solution vector once: $n$ elements.\nThis yields\n$$\nM_{\\mathrm{TRSV}}(n) = 8 \\left( \\frac{n(n+1)}{2} + 2n \\right) = 4 n^2 + 20 n.\n$$\nTherefore, the arithmetic intensity of $\\mathrm{TRSV}$ is\n$$\nI_{\\mathrm{TRSV}}(n) = \\frac{F_{\\mathrm{TRSV}}(n)}{M_{\\mathrm{TRSV}}(n)} = \\frac{n^2}{4 n^2 + 20 n} = \\frac{n}{4 n + 20}.\n$$\n\nFloating-point operation count for $\\mathrm{TRSM}$ (multiple right-hand sides):\n- For $m$ right-hand sides packed as the columns of $B \\in \\mathbb{R}^{n \\times m}$, we solve $L X = B$ where $X \\in \\mathbb{R}^{n \\times m}$. If each column is solved by forward substitution, the count per column is $n^2$, so\n$$\nF_{\\mathrm{TRSM}}(n,m) = m \\, n^2.\n$$\nMinimal memory traffic model for $\\mathrm{TRSM}$ in double precision:\n- Read the triangular matrix once: $\\frac{n(n+1)}{2}$ elements.\n- Read all right-hand sides: $n m$ elements.\n- Write all solution columns: $n m$ elements.\nThus\n$$\nM_{\\mathrm{TRSM}}(n,m) = 8 \\left( \\frac{n(n+1)}{2} + 2 n m \\right) = 4 n^2 + 4 n + 16 n m.\n$$\nThe arithmetic intensity is\n$$\nI_{\\mathrm{TRSM}}(n,m) = \\frac{F_{\\mathrm{TRSM}}(n,m)}{M_{\\mathrm{TRSM}}(n,m)} = \\frac{m n^2}{4 n^2 + 4 n + 16 n m}.\n$$\n\nRoofline throughput bounds:\nGiven peak throughput $P_{\\text{peak}}$ in gigaflops per second and sustainable bandwidth $B$ in gigabytes per second, the memory-bound ceiling is $I \\cdot B$ in gigaflops per second because $I$ is in floating-point operations per byte and one gigabyte per second is $10^9$ bytes per second. Therefore, the attainable throughput is\n$$\n\\text{throughput}(I; P_{\\text{peak}}, B) = \\min\\left\\{ P_{\\text{peak}}, \\ I \\cdot B \\right\\}.\n$$\n\nCompute-boundedness via batching and minimal $m$:\nFor $\\mathrm{TRSM}$, as $m \\to \\infty$,\n$$\n\\lim_{m \\to \\infty} I_{\\mathrm{TRSM}}(n,m) = \\lim_{m \\to \\infty} \\frac{m n^2}{4 n^2 + 4 n + 16 n m} = \\frac{n}{16}.\n$$\nThis implies that with any amount of right-hand-side batching, the maximum memory-bound ceiling is $\\frac{n}{16} \\cdot B$ gigaflops per second. Hence, compute saturation is possible if and only if\n$$\n\\frac{n}{16} \\cdot B \\ge P_{\\text{peak}},\n$$\nequivalently $B n \\ge 16 P_{\\text{peak}}$. When this condition is violated, no finite $m$ can make the operation compute-bound.\n\nIf $B n > 16 P_{\\text{peak}}$, we can solve for the smallest integer $m_{\\min} \\ge 1$ such that $I_{\\mathrm{TRSM}}(n,m_{\\min}) \\cdot B \\ge P_{\\text{peak}}$. This inequality is\n$$\n\\frac{m n^2}{4 n^2 + 4 n + 16 n m} \\cdot B \\ \\ge \\ P_{\\text{peak}}.\n$$\nRearranging,\n$$\nm (B n^2 - 16 P_{\\text{peak}} n) \\ \\ge \\ P_{\\text{peak}} (4 n^2 + 4 n).\n$$\nSince $B n^2 - 16 P_{\\text{peak}} n = n (B n - 16 P_{\\text{peak}}) > 0$ by assumption, the minimal integer solution is\n$$\nm_{\\min} = \\left\\lceil \\frac{P_{\\text{peak}} (4 n^2 + 4 n)}{B n^2 - 16 P_{\\text{peak}} n} \\right\\rceil.\n$$\nIf $B n \\le 16 P_{\\text{peak}}$, we set $m_{\\min} = -1$ to indicate impossibility of compute saturation by right-hand-side batching.\n\nBatching recommendations:\n- If $m_{\\min} = -1$, no amount of batching over right-hand sides of a shared triangular matrix will saturate compute at the given $n$, $P_{\\text{peak}}$, and $B$. In such a regime, $\\mathrm{TRSV}$ and $\\mathrm{TRSM}$ are memory-bound; one should prioritize memory-traffic optimizations such as panel blocking, data layout, and fusing operations; we encode this as recommendation code $0$.\n- If $m_{\\min} \\ge 1$ and the provided $m$ is less than $m_{\\min}$, we recommend batching additional right-hand sides that share the same triangular factor to reach $m_{\\min}$ (reformulate as a multi-right-hand-side solve), encoded as recommendation code $1$.\n- If $m \\ge m_{\\min}$, the problem is compute-bound at the provided $m$, encoded as recommendation code $2$.\n\nApplying these derivations to each test case consists of evaluating $I_{\\mathrm{TRSV}}(n)$, $I_{\\mathrm{TRSM}}(n,m)$, the corresponding roofline-limited throughputs, and $m_{\\min}$ via the derived formula, followed by mapping to the batching recommendation code as specified. The program implements these computations and prints a single line containing all case results as a list of lists with six entries per case, where the first four entries are floats formatted to six decimal places and the last two are integers.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport math\n\ndef ai_trsv(n, elem_bytes=8.0):\n    # FLOPs: n^2\n    flops = float(n) ** 2\n    # Bytes: 8 * (n(n+1)/2 + 2n) = 4n^2 + 20n\n    bytes_ = elem_bytes * (n * (n + 1) / 2.0 + 2.0 * n)\n    return flops / bytes_\n\ndef ai_trsm(n, m, elem_bytes=8.0):\n    # FLOPs: m * n^2\n    flops = float(m) * (float(n) ** 2)\n    # Bytes: 8 * (n(n+1)/2 + 2 n m) = 4n^2 + 4n + 16 n m\n    bytes_ = elem_bytes * (n * (n + 1) / 2.0 + 2.0 * n * m)\n    return flops / bytes_\n\ndef roofline_throughput(ai, P_peak_GF, B_GBps):\n    # Achievable throughput in GF/s under roofline.\n    mem_bound_GF = ai * B_GBps\n    return min(P_peak_GF, mem_bound_GF)\n\ndef minimal_m_compute_bound(n, P_peak_GF, B_GBps):\n    # If B*n <= 16*P_peak, compute-bound is impossible with any finite m.\n    if B_GBps * n <= 16.0 * P_peak_GF:\n        return -1\n    # Solve inequality for m:\n    # ai_trsm(n,m) * B >= P_peak\n    # m (B n^2 - 16 P n) >= P (4 n^2 + 4 n)\n    numerator = P_peak_GF * (4.0 * (n ** 2) + 4.0 * n)\n    denominator = B_GBps * (n ** 2) - 16.0 * P_peak_GF * n\n    if denominator <= 0:\n        return -1\n    m_min = math.ceil(numerator / denominator)\n    if m_min < 1:\n        m_min = 1\n    return int(m_min)\n\ndef recommendation_code(n, m, P_peak_GF, B_GBps):\n    m_min = minimal_m_compute_bound(n, P_peak_GF, B_GBps)\n    if m_min == -1:\n        return 0\n    if m < m_min:\n        return 1\n    return 2\n\ndef format_float(x):\n    return f\"{x:.6f}\"\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case: (P_peak_GF/s, B_GB/s, n, m)\n    test_cases = [\n        (1000.0, 1000.0, 128, 8),\n        (200.0, 100.0, 64, 16),\n        (5000.0, 900.0, 16, 64),\n        (500.0, 1000.0, 8, 4),\n    ]\n\n    results = []\n    for P_peak, B, n, m in test_cases:\n        ai_v = ai_trsv(n)\n        ai_m = ai_trsm(n, m)\n        thr_v = roofline_throughput(ai_v, P_peak, B)\n        thr_m = roofline_throughput(ai_m, P_peak, B)\n        m_min = minimal_m_compute_bound(n, P_peak, B)\n        rec = recommendation_code(n, m, P_peak, B)\n\n        case_result = [\n            format_float(ai_v),\n            format_float(ai_m),\n            format_float(thr_v),\n            format_float(thr_m),\n            str(m_min),\n            str(rec),\n        ]\n        results.append(\"[\" + \",\".join(case_result) + \"]\")\n\n    # Final print statement in the exact required format.\n    print(f\"[[{format_float(ai_trsv(128))},{format_float(ai_trsm(128,8))},{format_float(roofline_throughput(ai_trsv(128),1000,1000))},{format_float(roofline_throughput(ai_trsm(128,8),1000,1000))},{minimal_m_compute_bound(128,1000,1000)},{recommendation_code(128,8,1000,1000)}],[{format_float(ai_trsv(64))},{format_float(ai_trsm(64,16))},{format_float(roofline_throughput(ai_trsv(64),200,100))},{format_float(roofline_throughput(ai_trsm(64,16),200,100))},{minimal_m_compute_bound(64,200,100)},{recommendation_code(64,16,200,100)}],[{format_float(ai_trsv(16))},{format_float(ai_trsm(16,64))},{format_float(roofline_throughput(ai_trsv(16),5000,900))},{format_float(roofline_throughput(ai_trsm(16,64),5000,900))},{minimal_m_compute_bound(16,5000,900)},{recommendation_code(16,64,5000,900)}],[{format_float(ai_trsv(8))},{format_float(ai_trsm(8,4))},{format_float(roofline_throughput(ai_trsv(8),500,1000))},{format_float(roofline_throughput(ai_trsm(8,4),500,1000))},{minimal_m_compute_bound(8,500,1000)},{recommendation_code(8,4,500,1000)}]]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3579232"}]}