{"hands_on_practices": [{"introduction": "The core idea of mixed-precision iterative refinement is to blend the speed of low-precision arithmetic with the accuracy of high-precision calculations. This practice [@problem_id:3245146] guides you through a direct implementation of this powerful technique. By performing the computationally expensive factorization and solves in single precision and the critical residual computations in double precision, you will gain first-hand experience in building a solver that is both fast and accurate.", "problem": "Implement a program that performs mixed-precision iterative refinement for solving linear systems. You are given a square nonsingular matrix $A \\in \\mathbb{R}^{n \\times n}$ and a right-hand side vector $b \\in \\mathbb{R}^{n}$. The goal is to compute an improved solution to $A x = b$ by using a low-precision factorization and solve together with a high-precision residual computation. Your program must construct a Lower-Upper (LU) factorization with partial pivoting of $A$ in single precision (that is, using $32$-bit floating point arithmetic), use it to obtain an initial solution in single precision, and then iteratively refine the solution in double precision (that is, using $64$-bit floating point arithmetic) using residual correction until a specified stopping criterion is met or a maximum number of iterations is reached.\n\nUse the following foundations and definitions as the base of your design:\n- The residual of an approximate solution $\\hat{x}$ to $A x = b$ is $r = b - A \\hat{x}$.\n- The induced infinity norm of a vector $v$ is $\\lVert v \\rVert_{\\infty} = \\max_{i} |v_{i}|$. The induced infinity norm of a matrix $A$ is $\\lVert A \\rVert_{\\infty} = \\max_{i} \\sum_{j} |a_{i j}|$.\n- The floating-point rounding model is $fl(z) = z (1 + \\delta)$ with $|\\delta| \\leq u$, where $u$ is the unit roundoff. You will emulate low precision by explicitly using single precision arithmetic for the factorization and the triangular solves, and high precision by using double precision arithmetic for the residual computation and solution updates.\n\nAlgorithmic requirements for your implementation:\n- Construct the LU factorization with partial pivoting $P A = L U$ in single precision for each test matrix $A$, where $P$ is a permutation matrix, $L$ is unit lower triangular, and $U$ is upper triangular.\n- Compute the initial approximate solution $\\hat{x}_{0}$ by solving $L U \\hat{x}_{0} = P b$ entirely in single precision and then cast $\\hat{x}_{0}$ to double precision.\n- For refinement iteration $k = 0, 1, 2, \\dots$, compute the residual $r_{k} = b - A \\hat{x}_{k}$ in double precision and the relative backward-error-like measure\n$$\n\\eta_{k} = \\frac{\\lVert r_{k} \\rVert_{\\infty}}{\\lVert A \\rVert_{\\infty} \\, \\lVert \\hat{x}_{k} \\rVert_{\\infty} + \\lVert b \\rVert_{\\infty}}.\n$$\nIf the denominator is $0$ (that is, both $\\lVert \\hat{x}_{k} \\rVert_{\\infty}$ and $\\lVert b \\rVert_{\\infty}$ are $0$), define $\\eta_{k} = 0$.\n- If $\\eta_{k} \\leq \\tau$, stop and report the number of refinement steps taken so far (that is, the number of correction solves actually performed, which is $0$ if the initial solution already satisfies the tolerance).\n- Otherwise, compute a correction $d_{k}$ by solving $L U d_{k} = P r_{k}$ in single precision, cast $d_{k}$ to double precision, and set $\\hat{x}_{k+1} = \\hat{x}_{k} + d_{k}$ in double precision. Repeat until convergence or until reaching a maximum of $K_{\\max}$ refinement steps. If convergence is not achieved within $K_{\\max}$ steps, report failure with the value $-1$.\n\nUse the following fixed parameters for all cases:\n- Tolerance $\\tau = 10^{-12}$.\n- Maximum number of refinement steps $K_{\\max} = 20$.\n\nTest suite:\nImplement and run your solver on the following four test cases. For each case, define $A$ and $b$ exactly as below and apply the same algorithm and parameters.\n\n- Case $1$ (symmetric positive definite, general right-hand side):\n  $$\n  A = \\begin{bmatrix}\n  4  1  0 \\\\\n  1  3  1 \\\\\n  0  1  2\n  \\end{bmatrix},\n  \\quad\n  b = \\begin{bmatrix}\n  1 \\\\ 2 \\\\ 3\n  \\end{bmatrix}.\n  $$\n\n- Case $2$ (zero right-hand side, immediate boundary case):\n  $$\n  A = \\begin{bmatrix}\n  2  -1 \\\\\n  -1  2\n  \\end{bmatrix},\n  \\quad\n  b = \\begin{bmatrix}\n  0 \\\\ 0\n  \\end{bmatrix}.\n  $$\n\n- Case $3$ (Hilbert matrix, ill-conditioned but nonsingular, known exact solution $x = \\mathbf{1}$):\n  For $n = 5$, let $H \\in \\mathbb{R}^{5 \\times 5}$ with entries $h_{i j} = \\frac{1}{i + j + 1}$ for $i, j \\in \\{0, 1, 2, 3, 4\\}$, and set $A = H$ and $b = H \\mathbf{1}$, where $\\mathbf{1}$ is the vector of all ones in $\\mathbb{R}^{5}$.\n\n- Case $4$ (scaled, weakly coupled, mixed magnitudes):\n  $$\n  A = \\begin{bmatrix}\n  10^{-3}  10^{-6}  0  0 \\\\\n  10^{-6}  1  10^{-3}  0 \\\\\n  0  10^{-3}  10^{3}  2 \\\\\n  0  0  2  2 \\times 10^{2}\n  \\end{bmatrix},\n  \\quad\n  b = \\begin{bmatrix}\n  1 \\\\ 2 \\\\ 3 \\\\ 4\n  \\end{bmatrix}.\n  $$\n\nOutput specification:\n- Your program must produce a single line of output containing a list of exactly four integers in the order of the test cases, where each integer is the number of refinement steps used to meet the tolerance for that case, or $-1$ if the method did not converge within $K_{\\max}$ steps. The line must be formatted as a comma-separated list enclosed in square brackets, for example, $[n_{1},n_{2},n_{3},n_{4}]$.\n\nNo external input is provided; all data must be defined within the program. Angles, physical units, or percentages are not involved in this problem. All quantities are purely mathematical and dimensionless.", "solution": "The problem requires the implementation of a mixed-precision iterative refinement algorithm for solving a linear system of equations $A x = b$. The core principle of this method is to leverage the speed of low-precision arithmetic for computationally expensive tasks while using high-precision arithmetic for accuracy-critical computations. Specifically, the computationally intensive LU factorization of the matrix $A$ and the subsequent triangular solves for the solution are performed in single precision ($32$-bit floating-point arithmetic). The residual calculation, which is prone to catastrophic cancellation and thus demands higher accuracy, is performed in double precision ($64$-bit floating-point arithmetic).\n\nThe algorithm proceeds as follows:\n\n1.  **Data Preparation and Precision Management**:\n    The input matrix $A$ and vector $b$ are initially treated as double-precision entities, which we denote as $A_{dp}$ and $b_{dp}$. We compute their infinity norms, $\\lVert A \\rVert_{\\infty}$ and $\\lVert b \\rVert_{\\infty}$, in double precision, as these values are constant throughout the process and are used in the stopping criterion. For the low-precision parts of the algorithm, we create single-precision copies of the matrix and vector, $A_{sp}$ and $b_{sp}$, by casting the double-precision versions. In `NumPy`, this corresponds to using `numpy.float64` for double precision and `numpy.float32` for single precision.\n\n2.  **Single-Precision LU Factorization**:\n    We compute the LU factorization of the single-precision matrix $A_{sp}$ with partial pivoting. This decomposition yields $P A_{sp} = L_{sp} U_{sp}$, where $P$ is a permutation matrix representing row swaps, $L_{sp}$ is a unit lower triangular matrix, and $U_{sp}$ is an upper triangular matrix. Since `NumPy` does not provide a direct function to obtain the $P, L, U$ factors in a convenient form, we must implement this factorization manually. Our implementation will produce a compact matrix containing the elements of both $L_{sp}$ (in the strict lower triangle) and $U_{sp}$ (in the upper triangle including the diagonal), along with a permutation vector that represents $P$.\n\n3.  **Initial Solution**:\n    The first approximate solution, $\\hat{x}_{0}$, is computed entirely in single precision. Using the factorization, the original system $A x = b$ is transformed into $L_{sp} U_{sp} \\hat{x}_{0} = P b_{sp}$. This is solved in two steps:\n    a.  **Forward Substitution**: Solve $L_{sp} y = P b_{sp}$ for the intermediate vector $y$.\n    b.  **Backward Substitution**: Solve $U_{sp} \\hat{x}_{0} = y$ for the initial solution $\\hat{x}_{0}$.\n    The resulting single-precision vector $\\hat{x}_{0}$ is then cast to double precision to be used in the high-precision refinement loop.\n\n4.  **Iterative Refinement Loop**:\n    The refinement process iterates to improve the solution's accuracy. The loop runs for a maximum of $K_{\\max} = 20$ refinement steps. Let $\\hat{x}_k$ be the approximate solution at iteration $k$ (with $\\hat{x}_0$ being the initial solution).\n\n    a.  **Residual Calculation (Double Precision)**: The residual $r_k = b_{dp} - A_{dp} \\hat{x}_k$ is computed using double-precision arithmetic. This step is critical as it recovers the error information that was lost in the single-precision calculations.\n\n    b.  **Convergence Criterion**: We evaluate the relative backward-error-like measure $\\eta_k$:\n        $$\n        \\eta_{k} = \\frac{\\lVert r_{k} \\rVert_{\\infty}}{\\lVert A \\rVert_{\\infty} \\, \\lVert \\hat{x}_{k} \\rVert_{\\infty} + \\lVert b \\rVert_{\\infty}}\n        $$\n        All norms are computed in double precision. If $\\eta_k$ is less than or equal to the specified tolerance $\\tau = 10^{-12}$, the solution is considered converged, and the process terminates. The number of refinement steps performed, $k$, is reported. For the initial check ($k=0$), if $\\eta_0 \\le \\tau$, we report $0$ steps.\n\n    c.  **Correction Step (Single Precision)**: If the solution has not converged, we compute a correction term $d_k$ by solving the system $A d_k = r_k$. We reuse the single-precision LU factors for efficiency: $L_{sp} U_{sp} d_k = P r_k$. The double-precision residual $r_k$ is first cast to single precision. The system is then solved for $d_k$ using forward and backward substitution in single precision.\n\n    d.  **Solution Update (Double Precision)**: The single-precision correction vector $d_k$ is cast back to double precision and added to the current solution: $\\hat{x}_{k+1} = \\hat{x}_k + d_k$. This update is performed in double precision to accumulate the corrections accurately.\n\n5.  **Termination**:\n    The loop terminates upon either satisfying the convergence criterion $\\eta_k \\le \\tau$ or after completing $K_{\\max}$ refinement steps. If the loop completes without convergence after checking the solution $\\hat{x}_{K_{max}}$, the method is considered to have failed, and the value $-1$ is reported. The number of refinements is defined as the number of correction solves performed.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef lu_pivot(A_in):\n    \"\"\"\n    Computes LU factorization with partial pivoting for a matrix A.\n    The matrix A_in is expected to be of single precision (np.float32).\n    \n    Returns:\n    - LU: A matrix containing U in its upper triangle and L in its strict lower triangle.\n    - p: A permutation vector.\n    \"\"\"\n    n = A_in.shape[0]\n    LU = A_in.copy()\n    p = np.arange(n)\n    \n    for j in range(n - 1):\n        # Find pivot in column j (from row j downwards)\n        max_row_idx_local = np.argmax(np.abs(LU[j:, j]))\n        max_row_idx_global = j + max_row_idx_local\n        \n        # Swap rows in LU and permutation vector p\n        if max_row_idx_global != j:\n            LU[[j, max_row_idx_global], :] = LU[[max_row_idx_global, j], :]\n            p[[j, max_row_idx_global]] = p[[max_row_idx_global, j]]\n            \n        # Elimination\n        pivot_val = LU[j, j]\n        # Use a small tolerance for checking non-singularity\n        if np.abs(pivot_val)  np.finfo(np.float32).tiny:\n            multipliers = LU[j + 1:, j] / pivot_val\n            LU[j + 1:, j] = multipliers\n            # Vectorized update of the submatrix\n            LU[j + 1:, j + 1:] -= np.outer(multipliers, LU[j, j + 1:])\n            \n    return LU, p\n\ndef solve_lu_sp(LU_sp, p_sp, b_sp):\n    \"\"\"\n    Solves the system LUx = Pb using the output of lu_pivot.\n    All computations are in single precision (np.float32).\n    \"\"\"\n    n = LU_sp.shape[0]\n    \n    # Apply permutation to the right-hand side vector\n    b_perm = b_sp[p_sp]\n    \n    # Forward substitution (solves Ly = b_perm)\n    y = np.zeros(n, dtype=np.float32)\n    for i in range(n):\n        y[i] = b_perm[i] - np.dot(LU_sp[i, :i], y[:i])\n        \n    # Backward substitution (solves Ux = y)\n    x = np.zeros(n, dtype=np.float32)\n    for i in range(n - 1, -1, -1):\n        dot_product = np.dot(LU_sp[i, i + 1:], x[i + 1:])\n        diag_val = LU_sp[i, i]\n        if np.abs(diag_val)  np.finfo(np.float32).tiny:\n             x[i] = (y[i] - dot_product) / diag_val\n        else:\n             # This case should ideally not be hit with non-singular matrices\n             # and proper pivoting, but provides a fallback.\n             x[i] = (y[i] - dot_product) / np.finfo(np.float32).tiny\n    return x\n\ndef mixed_precision_refinement(A_in, b_in, K_max, tau):\n    \"\"\"\n    Performs mixed-precision iterative refinement to solve Ax = b.\n    \"\"\"\n    n = A_in.shape[0]\n    \n    # Master copies and norms in double precision (np.float64)\n    A_dp = A_in.astype(np.float64)\n    b_dp = b_in.astype(np.float64)\n    norm_A_inf = np.linalg.norm(A_dp, ord=np.inf)\n    norm_b_inf = np.linalg.norm(b_dp, ord=np.inf)\n    \n    # Factorization in single precision (np.float32)\n    A_sp = A_dp.astype(np.float32)\n    b_sp = b_dp.astype(np.float32)\n    LU_sp, p_sp = lu_pivot(A_sp)\n    \n    # Initial solution computed in single precision\n    x_k_sp = solve_lu_sp(LU_sp, p_sp, b_sp)\n    x_k_dp = x_k_sp.astype(np.float64)\n    \n    # --- Start of the iterative refinement loop ---\n    # k represents the number of refinement steps performed.\n    for k in range(K_max + 1):\n        # Calculate residual in double precision\n        r_k_dp = b_dp - (A_dp @ x_k_dp)\n        \n        # Check stopping criterion\n        norm_r_inf = np.linalg.norm(r_k_dp, ord=np.inf)\n        norm_x_inf = np.linalg.norm(x_k_dp, ord=np.inf)\n        \n        denominator = norm_A_inf * norm_x_inf + norm_b_inf\n        eta_k = norm_r_inf / denominator if denominator != 0.0 else 0.0\n        \n        if eta_k = tau:\n            return k\n            \n        # If max iterations reached, break before next correction\n        if k == K_max:\n            break\n            \n        # Solve for correction in single precision\n        r_k_sp = r_k_dp.astype(np.float32)\n        d_k_sp = solve_lu_sp(LU_sp, p_sp, r_k_sp)\n        \n        # Update solution in double precision\n        d_k_dp = d_k_sp.astype(np.float64)\n        x_k_dp += d_k_dp\n        \n    # If the loop completes without converging\n    return -1\n\ndef solve():\n    # Define fixed parameters\n    tau = 1e-12\n    K_max = 20\n\n    # Define the test cases from the problem statement.\n    # Case 1\n    A1 = np.array([\n        [4.0, 1.0, 0.0],\n        [1.0, 3.0, 1.0],\n        [0.0, 1.0, 2.0]\n    ])\n    b1 = np.array([1.0, 2.0, 3.0])\n\n    # Case 2\n    A2 = np.array([\n        [2.0, -1.0],\n        [-1.0, 2.0]\n    ])\n    b2 = np.array([0.0, 0.0])\n\n    # Case 3\n    n3 = 5\n    # Use double precision for construction\n    H3 = np.zeros((n3, n3), dtype=np.float64)\n    for i in range(n3):\n        for j in range(n3):\n            H3[i, j] = 1.0 / (i + j + 1.0)\n    A3 = H3\n    b3 = A3 @ np.ones(n3, dtype=np.float64)\n\n    # Case 4\n    A4 = np.array([\n        [1e-3, 1e-6, 0.0, 0.0],\n        [1e-6, 1.0, 1e-3, 0.0],\n        [0.0, 1e-3, 1e3, 2.0],\n        [0.0, 0.0, 2.0, 2e2]\n    ])\n    b4 = np.array([1.0, 2.0, 3.0, 4.0])\n\n    test_cases = [\n        (A1, b1),\n        (A2, b2),\n        (A3, b3),\n        (A4, b4)\n    ]\n\n    results = []\n    for A, b in test_cases:\n        num_steps = mixed_precision_refinement(A, b, K_max, tau)\n        results.append(num_steps)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3245146"}, {"introduction": "Iterative refinement is particularly effective for ill-conditioned linear systems, where direct solvers often suffer from significant round-off error. This exercise [@problem_id:3245403] allows you to quantify this effectiveness in a tangible way by tracking the number of correct digits gained in the solution at each step. Using the classic, ill-conditioned Hilbert matrix as a test case, you will observe how refinement can systematically \"clean up\" a noisy initial solution, providing a deeper intuition for the method's convergence properties.", "problem": "You are to implement and analyze iterative refinement for solving a linear system with a Hilbert matrix, a classically ill-conditioned matrix. The goal is to quantify, per iteration, how many base-$10$ digits of accuracy are gained in the solution.\n\nFundamental base and definitions to use:\n- A linear system has the form $A x = b$, where $A \\in \\mathbb{R}^{n \\times n}$, $x \\in \\mathbb{R}^{n}$, and $b \\in \\mathbb{R}^{n}$. The Hilbert matrix $H \\in \\mathbb{R}^{n \\times n}$ is defined by $H_{ij} = \\frac{1}{i + j - 1}$ for $i, j \\in \\{1, \\dots, n\\}$.\n- The residual at iteration $k$ is $r^{(k)} = b - A x^{(k)}$.\n- The iterative refinement update at iteration $k$ generates a correction $d^{(k)}$ by solving $A d^{(k)} = r^{(k)}$, and sets $x^{(k+1)} = x^{(k)} + d^{(k)}$.\n- The number of correct base-$10$ digits at iteration $k$ is defined by\n$$\nD^{(k)} = -\\log_{10}\\!\\left(\\frac{\\lVert x^{(k)} - x^\\star \\rVert_\\infty}{\\lVert x^\\star \\rVert_\\infty}\\right),\n$$\nwhere $x^\\star$ is the exact solution. For numerical stability in double precision arithmetic, report $D^{(k)}$ saturated at $16$ digits, meaning use\n$$\n\\tilde{D}^{(k)} = -\\log_{10}\\!\\left(\\max\\!\\left(\\frac{\\lVert x^{(k)} - x^\\star \\rVert_\\infty}{\\lVert x^\\star \\rVert_\\infty},\\,10^{-16}\\right)\\right).\n$$\n- The per-iteration gain in correct digits is $G^{(k)} = \\tilde{D}^{(k)} - \\tilde{D}^{(k-1)}$ for $k \\ge 1$. The initial accuracy $\\tilde{D}^{(0)}$ corresponds to the solution obtained by a single direct solve of $A x = b$ before any refinement iterations.\n\nScientific realism and setup:\n- The Hilbert matrix is known to be ill-conditioned, with condition number growing rapidly as $n$ increases. Iterative refinement can improve the solution by correcting accumulated errors via residual solves.\n- Use double precision floating-point arithmetic ($64$-bit) for all computations.\n\nProgram requirements:\n- Construct $A$ as the Hilbert matrix of size $n$.\n- Set the true solution to $x^\\star = \\mathbf{1}$ (the all-ones vector of length $n$). Compute $b = A x^\\star$.\n- Compute an initial solution $x^{(0)}$ by directly solving $A x = b$.\n- Perform $m$ iterative refinement steps as described above.\n- After each iteration $k \\in \\{1, \\dots, m\\}$, compute and record $G^{(k)}$.\n- For each test case, output the list $[G^{(1)}, G^{(2)}, \\dots, G^{(m)}]$.\n\nTest suite:\n- Case $1$: $n = 2$, $m = 5$ (boundary case, relatively well-conditioned).\n- Case $2$: $n = 5$, $m = 5$ (moderately ill-conditioned).\n- Case $3$: $n = 8$, $m = 5$ (challenging ill-conditioning).\n- Case $4$: $n = 12$, $m = 5$ (severe ill-conditioning edge case).\n\nFinal output format:\n- Your program should produce a single line of output containing the per-iteration gains for all test cases as a comma-separated list of lists, with no spaces, enclosed in square brackets. For example, the output should look like $[[g_{1,1},\\dots,g_{1,m}],[g_{2,1},\\dots,g_{2,m}],\\dots]$, where $g_{i,k}$ is the gain for iteration $k$ in test case $i$.\n- All numbers should be printed as standard decimal floats, and there are no physical units involved in this problem.", "solution": "The user-provided problem statement has been independently validated and is determined to be a well-posed, scientifically grounded, and objective problem in the domain of numerical linear algebra. The problem is free of contradictions, ambiguities, and factual errors. Therefore, a complete solution is provided below.\n\nThe problem requires the implementation and analysis of the iterative refinement algorithm for solving a linear system $A x = b$, where $A$ is the notoriously ill-conditioned Hilbert matrix. The objective is to quantify the gain in solution accuracy, measured in base-$10$ digits, at each refinement step.\n\nThe Hilbert matrix $H$ of size $n \\times n$ is defined by its entries $H_{ij} = \\frac{1}{i + j - 1}$ for row and column indices $i, j$ starting from $1$. Its condition number grows extremely rapidly with $n$, making it a classic test case for numerical stability. For a system $A x = b$, a direct solution using methods like LU decomposition can accumulate significant floating-point error when $A$ is ill-conditioned, leading to an inaccurate computed solution.\n\nIterative refinement is a procedure designed to improve the accuracy of a computed solution. Let $x^{(0)}$ be the initial solution obtained by a direct solver. Due to finite precision arithmetic, $x^{(0)}$ differs from the true solution $x^\\star$ by an error $e^{(0)} = x^\\star - x^{(0)}$. The foundation of the method lies in estimating and correcting this error.\n\nThe residual vector for an approximate solution $x^{(k)}$ is defined as $r^{(k)} = b - A x^{(k)}$. By substituting $b = A x^\\star$, the residual can be related to the true error $e^{(k)} = x^\\star - x^{(k)}$:\n$$\nr^{(k)} = A x^\\star - A x^{(k)} = A (x^\\star - x^{(k)}) = A e^{(k)}\n$$\nThis equation shows that the true error $e^{(k)}$ is the solution to the linear system $A e^{(k)} = r^{(k)}$. Although we cannot compute $e^{(k)}$ exactly (as this would be equivalent to solving the original problem perfectly), we can compute an approximation to it, which we denote $d^{(k)}$, by solving the residual system:\n$$\nA d^{(k)} = r^{(k)}\n$$\nThe vector $d^{(k)}$ serves as a computed correction to the current solution. The next, hopefully more accurate, solution $x^{(k+1)}$ is obtained by applying this correction:\n$$\nx^{(k+1)} = x^{(k)} + d^{(k)}\n$$\nThis process is repeated iteratively. A critical aspect of iterative refinement is that the residual $r^{(k)}$ should ideally be computed with higher precision than the rest of the calculations. This problem, however, specifies the use of standard double precision ($64$-bit floats) for all operations, which allows us to observe the limits of the method when higher precision is not available.\n\nTo quantify the performance of the algorithm, we measure the accuracy of the solution at each step. The number of correct base-$10$ digits in the solution $x^{(k)}$ is defined relative to the true solution $x^\\star$:\n$$\nD^{(k)} = -\\log_{10}\\!\\left(\\frac{\\lVert x^{(k)} - x^\\star \\rVert_\\infty}{\\lVert x^\\star \\rVert_\\infty}\\right)\n$$\nwhere $\\lVert \\cdot \\rVert_\\infty$ is the infinity norm (maximum absolute value of the vector's components). Since double-precision floating-point arithmetic has a finite precision of approximately $16$ decimal digits, it is practical to use a saturated measure of accuracy that does not exceed this limit and avoids taking the logarithm of zero:\n$$\n\\tilde{D}^{(k)} = -\\log_{10}\\!\\left(\\max\\!\\left(\\frac{\\lVert x^{(k)} - x^\\star \\rVert_\\infty}{\\lVert x^\\star \\rVert_\\infty},\\,10^{-16}\\right)\\right)\n$$\nThe gain in accuracy at iteration $k$ is the difference in correct digits from the previous step:\n$$\nG^{(k)} = \\tilde{D}^{(k)} - \\tilde{D}^{(k-1)} \\quad \\text{for } k \\ge 1\n$$\nHere, $\\tilde{D}^{(0)}$ is the accuracy of the initial solution $x^{(0)}$ from the direct solve.\n\nThe algorithmic procedure for each test case $(n, m)$ is as follows:\n1.  **System Setup**:\n    *   Construct the $n \\times n$ Hilbert matrix $A$, where the entry at zero-based row $i$ and column $j$ is $A_{ij} = \\frac{1}{(i+1) + (j+1) - 1} = \\frac{1}{i+j+1}$.\n    *   Define the true solution as the all-ones vector, $x^\\star = \\mathbf{1} \\in \\mathbb{R}^n$.\n    *   Calculate the right-hand side vector $b = A x^\\star$. This ensures a known ground truth for error calculation. Since $x^\\star_j = 1$ for all $j$, each component $b_i$ is the $i$-th row sum of $A$: $b_i = \\sum_{j=1}^{n} \\frac{1}{i+j-1}$.\n\n2.  **Initial Solution**:\n    *   Compute the initial approximate solution $x^{(0)}$ by solving the system $A x = b$ using a standard direct numerical solver.\n    *   Calculate the initial accuracy $\\tilde{D}^{(0)}$.\n\n3.  **Iterative Refinement**:\n    *   Initialize the current solution $x \\leftarrow x^{(0)}$ and previous accuracy $D_{prev} \\leftarrow \\tilde{D}^{(0)}$.\n    *   For $k$ from $1$ to $m$:\n        a. Compute the residual: $r = b - A x$.\n        b. Solve for the correction: $A d = r$.\n        c. Update the solution: $x \\leftarrow x + d$.\n        d. Calculate the new accuracy: $D_{current} = \\tilde{D}^{(k)}$.\n        e. Calculate and record the gain: $G^{(k)} = D_{current} - D_{prev}$.\n        f. Update the previous accuracy: $D_{prev} \\leftarrow D_{current}$.\n\n4.  **Output**: Report the list of gains $[G^{(1)}, G^{(2)}, \\dots, G^{(m)}]$ for each test case.\n\nFor matrices with low condition numbers (e.g., $n=2$), the initial solution is already highly accurate, and refinement yields little to no gain. As $n$ increases ($n=5, 8$), the condition number grows, the initial solution degrades, and iterative refinement is expected to provide significant accuracy gains in the first few iterations. For $n=12$, the condition number of the Hilbert matrix exceeds $10^{16}$, which is the approximate precision limit of double-precision numbers. At this point, the computed residual is dominated by noise, and the refinement process is expected to stagnate or fail, yielding minimal or even negative gains.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and analyzes iterative refinement for linear systems involving\n    the Hilbert matrix for a suite of test cases.\n    \"\"\"\n    # Test cases are defined as tuples (n, m), where n is the matrix size\n    # and m is the number of refinement iterations.\n    test_cases = [\n        (2, 5),   # Case 1: Well-conditioned\n        (5, 5),   # Case 2: Moderately ill-conditioned\n        (8, 5),   # Case 3: Challenging ill-conditioning\n        (12, 5),  # Case 4: Severe ill-conditioning\n    ]\n\n    all_results = []\n\n    for n, m in test_cases:\n        # Step 1: System Setup\n        # Construct the n x n Hilbert matrix A.\n        # For 0-based indices i, j, the formula is H_ij = 1 / (i + j + 1).\n        A = np.fromfunction(lambda i, j: 1.0 / (i + j + 1), (n, n), dtype=float)\n\n        # Define the true solution as the all-ones vector.\n        x_star = np.ones(n, dtype=float)\n\n        # Calculate the right-hand side vector b = A * x_star.\n        b = A @ x_star\n\n        # Define a helper function to calculate the number of correct digits.\n        def get_saturated_digits(x_approx, x_true):\n            \"\"\"\n            Calculates the saturated number of correct base-10 digits.\n            \"\"\"\n            # The infinity norm of x_star is always 1.0.\n            norm_x_true_inf = 1.0\n            \n            # Calculate relative error using the infinity norm.\n            relative_error = np.linalg.norm(x_approx - x_true, np.inf) / norm_x_true_inf\n            \n            # Apply saturation at 10^-16 to handle finite precision and avoid log(0).\n            effective_error = max(relative_error, 1e-16)\n            \n            return -np.log10(effective_error)\n\n        # Step 2: Initial Solution\n        # Compute the initial solution x^(0) using a direct solver.\n        x_k = np.linalg.solve(A, b)\n\n        # Calculate the initial number of correct digits, D_tilde^(0).\n        D_prev = get_saturated_digits(x_k, x_star)\n\n        # Step 3: Iterative Refinement\n        gains_for_case = []\n        for _ in range(m):\n            # a. Compute the residual in double precision.\n            r_k = b - A @ x_k\n\n            # b. Solve for the correction vector d.\n            d_k = np.linalg.solve(A, r_k)\n\n            # c. Update the solution.\n            x_k = x_k + d_k\n\n            # d. Calculate the new accuracy.\n            D_current = get_saturated_digits(x_k, x_star)\n\n            # e. Calculate and record the gain.\n            gain = D_current - D_prev\n            gains_for_case.append(gain)\n\n            # f. Update the previous accuracy for the next iteration.\n            D_prev = D_current\n        \n        all_results.append(gains_for_case)\n\n    # Final print statement in the exact required format.\n    # Format each sublist of gains into a comma-separated string \"[g1,g2,...]\".\n    formatted_sublists = [f\"[{','.join(map(str, sublist))}]\" for sublist in all_results]\n    # Join all formatted sublists into the final output string.\n    print(f\"[{','.join(formatted_sublists)}]\")\n\nsolve()\n```", "id": "3245403"}, {"introduction": "In modern high-performance computing, the choice of numerical precision is a critical optimization parameter that balances accuracy, memory usage, and execution speed. This advanced practice [@problem_id:3552192] challenges you to think like a computational scientist by building a performance model for mixed-precision iterative refinement on a GPU. By using the roofline model, you will learn to navigate the complex trade-offs between factorization precision, numerical stability, and hardware capabilities to find the optimal strategy for solving a linear system.", "problem": "Consider solving a dense linear system $A x = b$ on a Graphics Processing Unit (GPU) using mixed-precision iterative refinement. Let $A \\in \\mathbb{R}^{n \\times n}$ be nonsingular, with condition number $\\kappa(A)$ measured in a compatible norm. The mixed-precision iterative refinement algorithm consists of an initial low-precision factorization of $A$, an initial solve, and then repeated refinement iterations in which the residual is computed in a higher precision, a correction is obtained by solving triangular systems using the low-precision factors, and the solution is updated in a specified precision.\n\nYou are to construct a roofline-based performance model and use it to select precisions and the number of refinement steps to minimize total time while achieving a specified accuracy target. You must model the total time as the sum of kernel times, each estimated by the roofline model. The final answer must be a complete runnable program that implements this model for the provided test suite and prints the results in the exact specified format.\n\nUse the following fundamental bases:\n- Roofline model: For a kernel operating in precision $p$ with floating-point operation count $f$ and data movement $b$ (in bytes), the time is\n$$ T_{\\mathrm{kernel}}(p) = \\max\\left( \\frac{f}{P_p}, \\frac{b}{B} \\right), $$\nwhere $P_p$ is the peak floating-point throughput (in floating-point operations per second) for precision $p$, and $B$ is the sustained memory bandwidth (in bytes per second).\n- Mixed-precision iterative refinement contraction: Define the contraction factor $\\rho$ as\n$$ \\rho = \\kappa(A) \\, u_{p_f}, $$\nwhere $u_{p_f}$ is the unit roundoff of the factorization precision $p_f$. A sufficient condition for convergence of classical iterative refinement is $\\rho  1$. The number of refinement steps $k$ required to reduce the geometric term below the target safety margin is modeled by\n$$ k = \\left\\lceil \\frac{\\log(\\delta - \\delta_{\\mathrm{floor}})}{\\log(\\rho)} \\right\\rceil, $$\nassuming $\\delta  \\delta_{\\mathrm{floor}}$ and $\\rho \\in (0,1)$, where $\\delta$ is the required target bound and $\\delta_{\\mathrm{floor}}$ is the attainable error floor due to finite precision in residual computation and update.\n- Error floor model: Define the floor as\n$$ \\delta_{\\mathrm{floor}} = u_{p_r} + u_{p_u}, $$\nwhere $u_{p_r}$ and $u_{p_u}$ are the unit roundoffs of residual and update precisions, respectively.\n\nUse dense-operation flop counts and byte models:\n- Factorization (dense $LU$ without pivoting cost modeling): flops $f_{\\mathrm{LU}} = \\frac{2}{3} n^3$, bytes $b_{\\mathrm{LU}} = c_{\\mathrm{LU}} \\, n^2 \\, s_{p_f}$, with $c_{\\mathrm{LU}}$ a given constant and $s_{p}$ the size in bytes of the floating-point type of precision $p$.\n- One triangular solve: flops $f_{\\mathrm{TS}} = n^2$, bytes $b_{\\mathrm{TS}} = n^2 \\, s_{p_f}$. The initial solution requires two triangular solves, and each refinement iteration requires two triangular solves.\n- Residual computation $r = b - A x$: flops $f_{\\mathrm{RES}} = 2 n^2$, bytes $b_{\\mathrm{RES}} = (n^2 + 3 n) \\, s_{p_r}$.\n- Update $x \\leftarrow x + \\Delta x$: flops $f_{\\mathrm{UP}} = n$, bytes $b_{\\mathrm{UP}} = 3 n \\, s_{p_u}$.\n\nThe unit roundoffs are related to the precision as follows:\n- For half-precision ($16$ bits), $u_{16} = 2^{-11}$ and $s_{16} = 2$.\n- For single-precision ($32$ bits), $u_{32} = 2^{-24}$ and $s_{32} = 4$.\n- For double-precision ($64$ bits), $u_{64} = 2^{-53}$ and $s_{64} = 8$.\n\nThe total modeled time is\n$$ T_{\\mathrm{total}} = T_{\\mathrm{LU}}(p_f) + 2 T_{\\mathrm{TS}}(p_f) + k \\left[ T_{\\mathrm{RES}}(p_r) + 2 T_{\\mathrm{TS}}(p_f) + T_{\\mathrm{UP}}(p_u) \\right]. $$\n\nFeasibility constraints:\n- Convergence: $\\rho = \\kappa(A) \\, u_{p_f}  1$.\n- Accuracy target: $\\delta_{\\mathrm{floor}}  \\delta$, where $\\delta$ is the given target bound for the final error proxy.\n\nSearch space:\n- Factorization precision $p_f \\in \\{16,32\\}$.\n- Residual precision $p_r \\in \\{32,64\\}$.\n- Update precision $p_u \\in \\{32,64\\}$.\n\nYour program must, for each test case in the suite below, search the above discrete space, test feasibility, compute the required number of refinement steps $k$ using the given model, compute the total time $T_{\\mathrm{total}}$, and return the combination $(p_f, p_r, p_u, k)$ that minimizes $T_{\\mathrm{total}}$. If no combination is feasible, return the sentinel $[-1,-1,-1,-1,-1.0]$ for that test case.\n\nTest suite and parameters:\n- Test case $1$:\n    - $n = 4096$,\n    - $\\kappa(A) = 10^5$,\n    - target $\\delta = 10^{-8}$,\n    - hardware peaks: $P_{16} = 3.00 \\times 10^{14}$, $P_{32} = 2.00 \\times 10^{13}$, $P_{64} = 1.00 \\times 10^{13}$,\n    - bandwidth: $B = 1.50 \\times 10^{12}$,\n    - $c_{\\mathrm{LU}} = 6$.\n- Test case $2$:\n    - $n = 2048$,\n    - $\\kappa(A) = 2.00 \\times 10^{3}$,\n    - target $\\delta = 10^{-5}$,\n    - hardware peaks: $P_{16} = 3.00 \\times 10^{14}$, $P_{32} = 2.00 \\times 10^{13}$, $P_{64} = 1.00 \\times 10^{13}$,\n    - bandwidth: $B = 9.00 \\times 10^{11}$,\n    - $c_{\\mathrm{LU}} = 6$.\n- Test case $3$:\n    - $n = 1024$,\n    - $\\kappa(A) = 1.00 \\times 10^{2}$,\n    - target $\\delta = 10^{-18}$,\n    - hardware peaks: $P_{16} = 3.00 \\times 10^{14}$, $P_{32} = 2.00 \\times 10^{13}$, $P_{64} = 1.00 \\times 10^{13}$,\n    - bandwidth: $B = 1.50 \\times 10^{12}$,\n    - $c_{\\mathrm{LU}} = 6$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element of this list must itself be a list of the form $[p_f, p_r, p_u, k, T_{\\mathrm{total}}]$, where $p_f$, $p_r$, and $p_u$ are integers from $\\{16,32,64\\}$, $k$ is a nonnegative integer, and $T_{\\mathrm{total}}$ is a floating-point number in seconds. For an infeasible test case, output $[-1,-1,-1,-1,-1.0]$. For example, a valid output could look like $[[32,64,64,4,0.0032],[32,32,32,2,0.00043],[-1,-1,-1,-1,-1.0]]$.", "solution": "The user-provided problem is a well-posed optimization task within the domain of numerical linear algebra and high-performance computing. It requires the construction and application of a performance model to select the optimal configuration for a mixed-precision iterative refinement solver. The problem statement is scientifically grounded, self-contained, and logically consistent. All parameters, models, and constraints are explicitly defined. Therefore, the problem is valid, and a solution can be constructed.\n\nThe solution approach is to perform an exhaustive search over the discrete space of possible precision configurations for the iterative refinement algorithm. For each configuration, we check for feasibility based on numerical stability and accuracy requirements. For all feasible configurations, we compute the total execution time using the provided roofline performance model and identify the configuration that minimizes this time.\n\nFirst, we define the fundamental constants related to the floating-point precisions as specified in the problem:\n- Unit roundoffs: $u_{16} = 2^{-11}$, $u_{32} = 2^{-24}$, $u_{64} = 2^{-53}$. These are stored for easy lookup.\n- Data type sizes in bytes: $s_{16} = 2$, $s_{32} = 4$, $s_{64} = 8$.\n\nThe search space for the precision configuration $(p_f, p_r, p_u)$ is given by:\n- Factorization precision $p_f \\in \\{16, 32\\}$.\n- Residual computation precision $p_r \\in \\{32, 64\\}$.\n- Solution update precision $p_u \\in \\{32, 64\\}$.\nThis defines a total of $2 \\times 2 \\times 2 = 8$ possible configurations to evaluate for each test case.\n\nFor each test case, defined by the matrix dimension $n$, condition number $\\kappa(A)$, accuracy target $\\delta$, and hardware characteristics ($P_p, B$), we iterate through each of the $8$ configurations.\n\nFor a given configuration $(p_f, p_r, p_u)$, we first perform two feasibility checks:\n\n1.  **Convergence Constraint**: The iterative process must converge. The model for the contraction factor is $\\rho = \\kappa(A) \\, u_{p_f}$. The condition for convergence is $\\rho  1$. If this condition is not met, the configuration is deemed infeasible, and we proceed to the next one.\n\n2.  **Accuracy Constraint**: The algorithm must be capable of achieving the desired accuracy $\\delta$. The attainable error is limited by an error floor, $\\delta_{\\mathrm{floor}}$, which depends on the precision of the residual computation and the solution update. The model for this floor is $\\delta_{\\mathrm{floor}} = u_{p_r} + u_{p_u}$. The configuration is feasible only if $\\delta_{\\mathrm{floor}}  \\delta$. If this condition is violated, the configuration is discarded.\n\nIf a configuration is feasible, we proceed to calculate the number of refinement iterations, $k$, required. The model provided is:\n$$ k = \\left\\lceil \\frac{\\log(\\delta - \\delta_{\\mathrm{floor}})}{\\log(\\rho)} \\right\\rceil $$\nThis formula is valid for $\\rho \\in (0,1)$ and $\\delta  \\delta_{\\mathrm{floor}}$, which are ensured by our feasibility checks. A special case arises if the term $\\delta - \\delta_{\\mathrm{floor}} \\ge 1$. In this scenario, the numerator $\\log(\\delta - \\delta_{\\mathrm{floor}})$ is non-negative, while the denominator $\\log(\\rho)$ is negative. The resulting fraction is non-positive, leading to a ceiling of $0$ or a negative integer. Physically, this means that the initial solution is already sufficiently accurate, and no refinement steps are needed. Therefore, we set $k=0$ in this case. In summary, $k = \\max\\left(0, \\left\\lceil \\frac{\\log(\\delta - \\delta_{\\mathrm{floor}})}{\\log(\\rho)} \\right\\rceil\\right)$.\n\nWith $k$ determined, we calculate the total execution time, $T_{\\mathrm{total}}$. This is the sum of the time for the initial factorization and solve, plus the time for $k$ refinement iterations.\n$$ T_{\\mathrm{total}} = T_{\\mathrm{LU}}(p_f) + 2 T_{\\mathrm{TS}}(p_f) + k \\left[ T_{\\mathrm{RES}}(p_r) + 2 T_{\\mathrm{TS}}(p_f) + T_{\\mathrm{UP}}(p_u) \\right] $$\nThe time for each kernel is calculated using the roofline model:\n$$ T_{\\mathrm{kernel}}(p) = \\max\\left( \\frac{f}{P_p}, \\frac{b}{B} \\right) $$\nwhere $f$ is the floating-point operation count, $b$ is the data movement in bytes, $P_p$ is the peak performance for the kernel's operating precision, and $B$ is the memory bandwidth.\n\nThe specific parameters for each kernel are:\n-   **LU Factorization ($T_{\\mathrm{LU}}$)**: Operates in precision $p_f$.\n    -   $f_{\\mathrm{LU}} = \\frac{2}{3} n^3$\n    -   $b_{\\mathrm{LU}} = c_{\\mathrm{LU}} \\, n^2 \\, s_{p_f}$\n    -   Time: $T_{\\mathrm{LU}}(p_f) = \\max\\left( \\frac{f_{\\mathrm{LU}}}{P_{p_f}}, \\frac{b_{\\mathrm{LU}}}{B} \\right)$\n-   **Triangular Solve ($T_{\\mathrm{TS}}$)**: Uses the factors, so operates in precision $p_f$.\n    -   $f_{\\mathrm{TS}} = n^2$\n    -   $b_{\\mathrm{TS}} = n^2 \\, s_{p_f}$\n    -   Time: $T_{\\mathrm{TS}}(p_f) = \\max\\left( \\frac{f_{\\mathrm{TS}}}{P_{p_f}}, \\frac{b_{\\mathrm{TS}}}{B} \\right)$\n-   **Residual Computation ($T_{\\mathrm{RES}}$)**: Operates in precision $p_r$.\n    -   $f_{\\mathrm{RES}} = 2 n^2$\n    -   $b_{\\mathrm{RES}} = (n^2 + 3 n) \\, s_{p_r}$\n    -   Time: $T_{\\mathrm{RES}}(p_r) = \\max\\left( \\frac{f_{\\mathrm{RES}}}{P_{p_r}}, \\frac{b_{\\mathrm{RES}}}{B} \\right)$\n-   **Solution Update ($T_{\\mathrm{UP}}$)**: Operates in precision $p_u$.\n    -   $f_{\\mathrm{UP}} = n$\n    -   $b_{\\mathrm{UP}} = 3 n \\, s_{p_u}$\n    -   Time: $T_{\\mathrm{UP}}(p_u) = \\max\\left( \\frac{f_{\\mathrm{UP}}}{P_{p_u}}, \\frac{b_{\\mathrm{UP}}}{B} \\right)$\n\nWe compute $T_{\\mathrm{total}}$ for the current feasible configuration and compare it with the minimum time found so far for the current test case. If the current time is lower, we update the minimum time and store the current configuration $(p_f, p_r, p_u, k, T_{\\mathrm{total}})$ as the best one.\n\nAfter iterating through all $8$ configurations, the stored best configuration is the solution for the test case. If no feasible configuration was found, we return the specified sentinel value $[-1, -1, -1, -1, -1.0]$. This entire procedure is then repeated for each test case in the suite.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the mixed-precision iterative refinement optimization problem\n    for a suite of test cases.\n    \"\"\"\n    \n    # Define precision properties: unit roundoff and size in bytes\n    u = {16: 2**-11, 32: 2**-24, 64: 2**-53}\n    s = {16: 2, 32: 4, 64: 8}\n\n    # Define the search space for precisions\n    pf_space = [16, 32]\n    pr_space = [32, 64]\n    pu_space = [32, 64]\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"n\": 4096, \"kappa_A\": 1e5, \"delta\": 1e-8,\n            \"P\": {16: 3.00e14, 32: 2.00e13, 64: 1.00e13},\n            \"B\": 1.50e12, \"c_LU\": 6\n        },\n        {\n            \"n\": 2048, \"kappa_A\": 2.00e3, \"delta\": 1e-5,\n            \"P\": {16: 3.00e14, 32: 2.00e13, 64: 1.00e13},\n            \"B\": 9.00e11, \"c_LU\": 6\n        },\n        {\n            \"n\": 1024, \"kappa_A\": 1.00e2, \"delta\": 1e-18,\n            \"P\": {16: 3.00e14, 32: 2.00e13, 64: 1.00e13},\n            \"B\": 1.50e12, \"c_LU\": 6\n        }\n    ]\n\n    def _find_optimal_config(case):\n        \"\"\"\n        Processes a single test case to find the optimal configuration.\n        \"\"\"\n        n, kappa_A, delta = case[\"n\"], case[\"kappa_A\"], case[\"delta\"]\n        P, B, c_LU = case[\"P\"], case[\"B\"], case[\"c_LU\"]\n\n        min_time = float('inf')\n        best_config = None\n\n        for pf in pf_space:\n            for pr in pr_space:\n                for pu in pu_space:\n                    # Step 1: Feasibility Check\n                    # Convergence constraint\n                    rho = kappa_A * u[pf]\n                    if rho = 1:\n                        continue\n                    \n                    # Accuracy constraint\n                    delta_floor = u[pr] + u[pu]\n                    if delta_floor = delta:\n                        continue\n\n                    # Step 2: Calculate number of iterations k\n                    k = 0\n                    if delta - delta_floor  1:\n                        k_val = np.log(delta - delta_floor) / np.log(rho)\n                        k = int(np.ceil(k_val))\n                    if k  0:\n                        k = 0\n\n\n                    # Step 3: Calculate total time\n                    def t_kernel(f, b, p_val, B_val):\n                        return max(f / P[p_val], b / B_val)\n\n                    # Flops and Bytes for each kernel\n                    f_LU = (2/3) * n**3\n                    b_LU = c_LU * n**2 * s[pf]\n                    \n                    f_TS = n**2\n                    b_TS = n**2 * s[pf]\n                    \n                    f_RES = 2 * n**2\n                    b_RES = (n**2 + 3*n) * s[pr]\n                    \n                    f_UP = n\n                    b_UP = 3*n * s[pu]\n                    \n                    # Kernel times\n                    T_LU = t_kernel(f_LU, b_LU, pf, B)\n                    T_TS = t_kernel(f_TS, b_TS, pf, B)\n                    T_RES = t_kernel(f_RES, b_RES, pr, B)\n                    T_UP = t_kernel(f_UP, b_UP, pu, B)\n\n                    # Total time\n                    T_total = T_LU + 2 * T_TS + k * (T_RES + 2 * T_TS + T_UP)\n                    \n                    # Step 4: Update minimum\n                    if T_total  min_time:\n                        min_time = T_total\n                        best_config = [pf, pr, pu, k, T_total]\n        \n        if best_config is None:\n            return [-1, -1, -1, -1, -1.0]\n        else:\n            return best_config\n\n    results = []\n    for case in test_cases:\n        result = _find_optimal_config(case)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    # The map(str,...) is used to format numbers without trailing '.0' for integers.\n    print(f\"[{','.join(str(r) for r in results)}]\")\n\nsolve()\n```", "id": "3552192"}]}