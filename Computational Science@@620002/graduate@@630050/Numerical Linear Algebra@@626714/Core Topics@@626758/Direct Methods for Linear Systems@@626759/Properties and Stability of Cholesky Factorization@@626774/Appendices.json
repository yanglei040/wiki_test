{"hands_on_practices": [{"introduction": "To truly understand the stability of an algorithm, we must first quantify its sensitivity to small perturbations in the input. This practice guides you through a first-order perturbation analysis of the Cholesky factorization for a simple $2 \\times 2$ matrix. By deriving the relationship between a small symmetric change in the matrix $A$ and the corresponding change in its Cholesky factor $L$, you will compute a component-wise condition number, revealing which elements of the factorization are most sensitive to input errors [@problem_id:3568076].", "problem": "Consider the Cholesky factorization of a real symmetric positive definite (SPD) matrix, defined by the equality $A = L L^{\\mathsf{T}}$ with $L$ lower triangular and having positive diagonal entries. Let $A \\in \\mathbb{R}^{2 \\times 2}$ be the SPD matrix\n$$\nA = \\begin{pmatrix} 4 & 1 \\\\ 1 & 3 \\end{pmatrix},\n$$\nand let $L$ denote its Cholesky factor. For a small symmetric perturbation $\\delta A$, define the corresponding first-order perturbation $\\delta L$ through the Fréchet derivative of the Cholesky map at $A$. Work under the standard entrywise maximum norm $\\| \\cdot \\|_{\\max}$ on matrices, defined by $\\|M\\|_{\\max} = \\max_{i,j} |m_{ij}|$, and let the domain of perturbations be the space of symmetric matrices parameterized by their independent entries.\n\nStarting from fundamental principles (the defining relation $A = L L^{\\mathsf{T}}$ and first-order perturbation analysis), derive the linearized relation linking $\\delta A$ and $\\delta L$ and use it to formulate a componentwise condition number for the Cholesky map $A \\mapsto L$ at the given $A$, defined as\n$$\n\\kappa_{\\max}(A) = \\sup_{\\delta A \\neq 0} \\frac{\\|\\delta L\\|_{\\max}}{\\|\\delta A\\|_{\\max}}\n$$\nat first order. Compute $\\kappa_{\\max}(A)$ exactly for the given $A$, and identify which entries of $\\delta A$ produce equality in the bound when $\\|\\delta A\\|_{\\max} = 1$, thus assessing the tightness and sensitivity of specific entries. Your final answer must be the single exact value of $\\kappa_{\\max}(A)$ and does not require rounding.", "solution": "The user wants to find the componentwise condition number for the Cholesky factorization of a given $2 \\times 2$ symmetric positive definite (SPD) matrix.\n\n### Step 1: Verification and Cholesky Factorization of A\n\nThe given matrix is $A = \\begin{pmatrix} 4 & 1 \\\\ 1 & 3 \\end{pmatrix}$.\nFirst, we verify that $A$ is symmetric positive definite. $A$ is clearly symmetric. Its leading principal minors are $\\det(A_1) = 4 > 0$ and $\\det(A_2) = \\det(A) = 4 \\times 3 - 1 \\times 1 = 11 > 0$. Since all leading principal minors are positive, $A$ is SPD and its Cholesky factorization exists and is unique.\n\nThe Cholesky factorization is $A = L L^{\\mathsf{T}}$, where $L$ is a lower triangular matrix with positive diagonal entries. Let $L = \\begin{pmatrix} l_{11} & 0 \\\\ l_{21} & l_{22} \\end{pmatrix}$.\nThen $A = L L^{\\mathsf{T}}$ expands to:\n$$\n\\begin{pmatrix} 4 & 1 \\\\ 1 & 3 \\end{pmatrix} = \\begin{pmatrix} l_{11} & 0 \\\\ l_{21} & l_{22} \\end{pmatrix} \\begin{pmatrix} l_{11} & l_{21} \\\\ 0 & l_{22} \\end{pmatrix} = \\begin{pmatrix} l_{11}^2 & l_{11}l_{21} \\\\ l_{11}l_{21} & l_{21}^2 + l_{22}^2 \\end{pmatrix}\n$$\nEquating the matrix entries, we solve for the components of $L$:\n$l_{11}^2 = 4 \\implies l_{11} = 2$ (since $l_{11} > 0$).\n$l_{11}l_{21} = 1 \\implies 2l_{21} = 1 \\implies l_{21} = \\frac{1}{2}$.\n$l_{21}^2 + l_{22}^2 = 3 \\implies \\left(\\frac{1}{2}\\right)^2 + l_{22}^2 = 3 \\implies \\frac{1}{4} + l_{22}^2 = 3 \\implies l_{22}^2 = 3 - \\frac{1}{4} = \\frac{11}{4}$.\nSince $l_{22} > 0$, we have $l_{22} = \\frac{\\sqrt{11}}{2}$.\nThus, the Cholesky factor is $L = \\begin{pmatrix} 2 & 0 \\\\ \\frac{1}{2} & \\frac{\\sqrt{11}}{2} \\end{pmatrix}$.\n\n### Step 2: Derivation of the Linearized Perturbation Equation\n\nWe start with the defining relation $A = L L^{\\mathsf{T}}$ and consider a first-order perturbation. Let $A$ be perturbed by a symmetric matrix $\\delta A$, and let the corresponding perturbation in $L$ be $\\delta L$.\n$$\nA + \\delta A = (L + \\delta L)(L + \\delta L)^{\\mathsf{T}} = (L + \\delta L)(L^{\\mathsf{T}} + \\delta L^{\\mathsf{T}}) = L L^{\\mathsf{T}} + L \\delta L^{\\mathsf{T}} + \\delta L L^{\\mathsf{T}} + \\delta L \\delta L^{\\mathsf{T}}\n$$\nSubtracting $A = L L^{\\mathsf{T}}$ and retaining only first-order terms gives the linearized relation:\n$$\n\\delta A = L \\delta L^{\\mathsf{T}} + \\delta L L^{\\mathsf{T}}\n$$\nSince $L$ is lower triangular, $\\delta L$ must also be lower triangular (the zero entries of $L$ cannot be perturbed). Let $\\delta A = \\begin{pmatrix} s_{11} & s_{12} \\\\ s_{12} & s_{22} \\end{pmatrix}$ and $\\delta L = \\begin{pmatrix} x_{11} & 0 \\\\ x_{21} & x_{22} \\end{pmatrix}$.\nSubstituting the expressions for $L$, $\\delta L$, and $\\delta A$ into the perturbation equation:\n$$\n\\begin{pmatrix} s_{11} & s_{12} \\\\ s_{12} & s_{22} \\end{pmatrix} = \\begin{pmatrix} 2 & 0 \\\\ \\frac{1}{2} & \\frac{\\sqrt{11}}{2} \\end{pmatrix} \\begin{pmatrix} x_{11} & x_{21} \\\\ 0 & x_{22} \\end{pmatrix} + \\begin{pmatrix} x_{11} & 0 \\\\ x_{21} & x_{22} \\end{pmatrix} \\begin{pmatrix} 2 & \\frac{1}{2} \\\\ 0 & \\frac{\\sqrt{11}}{2} \\end{pmatrix}\n$$\n$$\n\\begin{pmatrix} s_{11} & s_{12} \\\\ s_{12} & s_{22} \\end{pmatrix} = \\begin{pmatrix} 2x_{11} & 2x_{21} \\\\ \\frac{1}{2}x_{11} & \\frac{1}{2}x_{21} + \\frac{\\sqrt{11}}{2}x_{22} \\end{pmatrix} + \\begin{pmatrix} 2x_{11} & \\frac{1}{2}x_{11} \\\\ 2x_{21} & \\frac{1}{2}x_{21} + \\frac{\\sqrt{11}}{2}x_{22} \\end{pmatrix}\n$$\n$$\n\\begin{pmatrix} s_{11} & s_{12} \\\\ s_{12} & s_{22} \\end{pmatrix} = \\begin{pmatrix} 4x_{11} & \\frac{1}{2}x_{11} + 2x_{21} \\\\ \\frac{1}{2}x_{11} + 2x_{21} & x_{21} + \\sqrt{11}x_{22} \\end{pmatrix}\n$$\nThis gives a system of linear equations relating the independent entries of $\\delta A$ ($s_{11}, s_{12}, s_{22}$) to the independent entries of $\\delta L$ ($x_{11}, x_{21}, x_{22}$):\n1.  $s_{11} = 4x_{11}$\n2.  $s_{12} = \\frac{1}{2}x_{11} + 2x_{21}$\n3.  $s_{22} = x_{21} + \\sqrt{11}x_{22}$\n\n### Step 3: Solving for $\\delta L$ in terms of $\\delta A$\n\nWe solve this system for $x_{11}$, $x_{21}$, and $x_{22}$:\nFrom equation (1):\n$$\nx_{11} = \\frac{1}{4}s_{11}\n$$\nSubstitute this into equation (2):\n$s_{12} = \\frac{1}{2}\\left(\\frac{1}{4}s_{11}\\right) + 2x_{21} \\implies 2x_{21} = s_{12} - \\frac{1}{8}s_{11} \\implies x_{21} = \\frac{1}{2}s_{12} - \\frac{1}{16}s_{11}$.\nSo,\n$$\nx_{21} = -\\frac{1}{16}s_{11} + \\frac{1}{2}s_{12}\n$$\nSubstitute this expression for $x_{21}$ into equation (3):\n$s_{22} = \\left(-\\frac{1}{16}s_{11} + \\frac{1}{2}s_{12}\\right) + \\sqrt{11}x_{22} \\implies \\sqrt{11}x_{22} = s_{22} + \\frac{1}{16}s_{11} - \\frac{1}{2}s_{12}$.\nSo,\n$$\nx_{22} = \\frac{1}{16\\sqrt{11}}s_{11} - \\frac{1}{2\\sqrt{11}}s_{12} + \\frac{1}{\\sqrt{11}}s_{22}\n$$\n\n### Step 4: Computing the Condition Number\n\nThe condition number $\\kappa_{\\max}(A)$ is defined as $\\sup_{\\delta A \\neq 0} \\frac{\\|\\delta L\\|_{\\max}}{\\|\\delta A\\|_{\\max}}$. Let the vector of independent entries of $\\delta A$ be $\\mathbf{s} = (s_{11}, s_{12}, s_{22})^{\\top}$ and for $\\delta L$ be $\\mathbf{x} = (x_{11}, x_{21}, x_{22})^{\\top}$. The linear relationship is $\\mathbf{x} = M \\mathbf{s}$, where $M$ is the Jacobian matrix:\n$$\nM = \\begin{pmatrix}\n\\frac{1}{4} & 0 & 0 \\\\\n-\\frac{1}{16} & \\frac{1}{2} & 0 \\\\\n\\frac{1}{16\\sqrt{11}} & -\\frac{1}{2\\sqrt{11}} & \\frac{1}{\\sqrt{11}}\n\\end{pmatrix}\n$$\nThe problem uses the entrywise maximum norm, which corresponds to the infinity norm on the vectors $\\mathbf{s}$ and $\\mathbf{x}$. The condition number is therefore the induced matrix norm $\\|M\\|_{\\infty \\to \\infty} = \\|M\\|_{\\infty}$, which is the maximum absolute row sum of $M$.\nThe absolute row sums are:\nRow 1: $\\left|\\frac{1}{4}\\right| + |0| + |0| = \\frac{1}{4}$.\nRow 2: $\\left|-\\frac{1}{16}\\right| + \\left|\\frac{1}{2}\\right| + |0| = \\frac{1}{16} + \\frac{8}{16} = \\frac{9}{16}$.\nRow 3: $\\left|\\frac{1}{16\\sqrt{11}}\\right| + \\left|-\\frac{1}{2\\sqrt{11}}\\right| + \\left|\\frac{1}{\\sqrt{11}}\\right| = \\frac{1}{16\\sqrt{11}} + \\frac{8}{16\\sqrt{11}} + \\frac{16}{16\\sqrt{11}} = \\frac{1+8+16}{16\\sqrt{11}} = \\frac{25}{16\\sqrt{11}}$.\n\nWe must find the maximum of these three values: $\\frac{1}{4}$, $\\frac{9}{16}$, and $\\frac{25}{16\\sqrt{11}}$.\n$\\frac{1}{4} = \\frac{4}{16}$, which is smaller than $\\frac{9}{16}$.\nNow we compare $\\frac{9}{16}$ and $\\frac{25}{16\\sqrt{11}}$. This is equivalent to comparing $9$ and $\\frac{25}{\\sqrt{11}}$.\nSince both quantities are positive, we can compare their squares:\n$9^2 = 81$.\n$\\left(\\frac{25}{\\sqrt{11}}\\right)^2 = \\frac{625}{11} \\approx 56.8$.\nSince $81 > \\frac{625}{11}$, we have $9 > \\frac{25}{\\sqrt{11}}$, and thus $\\frac{9}{16} > \\frac{25}{16\\sqrt{11}}$.\n\nThe maximum absolute row sum is $\\frac{9}{16}$.\nTherefore, the condition number is $\\kappa_{\\max}(A) = \\|M\\|_{\\infty} = \\frac{9}{16}$.\n\n### Step 5: Identification of the Most Sensitive Perturbations\n\nThe maximum row sum corresponds to the second row of $M$, which defines the perturbation component $x_{21} = \\delta l_{21}$. This means that the entry $l_{21}$ of the Cholesky factor is the most sensitive to perturbations in $A$. The equality in the supremum is achieved for a perturbation $\\delta A$ that maximizes $|\\delta l_{21}|$.\n$|x_{21}| = |-\\frac{1}{16}s_{11} + \\frac{1}{2}s_{12}|$.\nTo maximize this value subject to $\\|\\delta A\\|_{\\max} = \\max(|s_{11}|, |s_{12}|, |s_{22}|) = 1$, we pick $|s_{11}|=1$ and $|s_{12}|=1$ such that the terms add up. For instance, let $s_{11}=-1$ and $s_{12}=1$. Then\n$|x_{21}| = |(-\\frac{1}{16})(-1) + (\\frac{1}{2})(1)| = \\frac{1}{16} + \\frac{1}{2} = \\frac{9}{16}$.\nWith this choice, we must verify that this is indeed the largest component of $|\\delta L|$.\n$|x_{11}| = |\\frac{1}{4}s_{11}| = \\frac{1}{4}$.\n$|x_{22}| = |\\frac{1}{16\\sqrt{11}}s_{11} - \\frac{1}{2\\sqrt{11}}s_{12} + \\frac{1}{\\sqrt{11}}s_{22}| = |\\frac{-1}{16\\sqrt{11}} - \\frac{1}{2\\sqrt{11}} + \\frac{s_{22}}{\\sqrt{11}}| = \\frac{1}{\\sqrt{11}} |-\\frac{9}{16} + s_{22}|$.\nFor any $|s_{22}| \\le 1$, the maximum value of $|-\\frac{9}{16} + s_{22}|$ is achieved at $s_{22}=-1$, giving $|-\\frac{9}{16}-1| = \\frac{25}{16}$. So the maximum possible value for $|x_{22}|$ in this case is $\\frac{25}{16\\sqrt{11}}$, which we have already shown is less than $\\frac{9}{16}$.\nThus, any perturbation $\\delta A$ with independent entries $(s_{11}, s_{12}, s_{22})$ where $s_{11} = \\pm 1$, $s_{12} = \\mp 1$, and $|s_{22}| \\le 1$ will yield $\\frac{\\|\\delta L\\|_{\\max}}{\\|\\delta A\\|_{\\max}} = \\frac{9/16}{1} = \\frac{9}{16}$. This confirms the supremum is achieved and the value is exact.\n\nThe final result for the componentwise condition number is $\\frac{9}{16}$.", "answer": "$$\\boxed{\\frac{9}{16}}$$", "id": "3568076"}, {"introduction": "Diagonal scaling is a powerful and common technique for preconditioning linear systems, but its effect on numerical stability can be subtle. This exercise explores the relationship between the Cholesky factorization of a matrix $A$ and its scaled counterpart $\\tilde{A} = DAD$. You will derive the elegant analytical connection between their respective factors and analyze how the stability of the overall process is impacted by the condition number of the scaling matrix $D$ itself [@problem_id:3568113].", "problem": "Let $A \\in \\mathbb{R}^{n \\times n}$ be symmetric positive definite (SPD), so there exists a unique lower-triangular matrix $L$ with positive diagonal such that $A = L L^{\\mathsf{T}}$, which is the Cholesky factorization. Let $D = \\mathrm{diag}(d_{1}, \\dots, d_{n})$ be a positive diagonal matrix with $d_{i} > 0$ for all $i$, and consider the multiplicative diagonal scaling $\\tilde{A} = D A D$. Denote by $\\tilde{L}$ the Cholesky factor of $\\tilde{A}$, i.e., the unique lower-triangular matrix with positive diagonal such that $\\tilde{A} = \\tilde{L} \\tilde{L}^{\\mathsf{T}}$.\n\nStarting from the defining properties of the Cholesky factorization and the effect of congruence transformations by positive diagonal matrices on SPD matrices, analyze how the scaling by $D$ affects the factorization structure and uniqueness. Then, derive an explicit analytic expression that recovers the original Cholesky factor $L$ from $\\tilde{L}$ and $D$.\n\nIn addition, assume a backward-stable Cholesky algorithm is applied to $\\tilde{A}$, producing a computed factor $\\tilde{L}_{\\mathrm{comp}}$ such that there exists a perturbation $\\Delta \\tilde{A}$ satisfying $\\tilde{L}_{\\mathrm{comp}} \\tilde{L}_{\\mathrm{comp}}^{\\mathsf{T}} = \\tilde{A} + \\Delta \\tilde{A}$ and $\\|\\Delta \\tilde{A}\\|_{2} \\leq \\gamma \\|\\tilde{A}\\|_{2}$ with $\\gamma = c n \\epsilon$, where $c$ is a modest constant, $n$ is the dimension, and $\\epsilon$ is the machine precision. Explain, using fundamental norm properties, how forming $L_{\\mathrm{rec}} = D^{-1} \\tilde{L}_{\\mathrm{comp}}$ induces a backward error in factoring $A$ and how that error depends on $D$ via its spectral condition number, but do not provide any inequality or numeric bound as your final answer.\n\nYour final answer must be the single closed-form analytic expression for $L$ in terms of $\\tilde{L}$ and $D$. No rounding is required, and no physical units apply.", "solution": "The problem statement is first validated to ensure it is self-contained, scientifically sound, and well-posed.\n\n### Step 1: Extract Givens\n- $A \\in \\mathbb{R}^{n \\times n}$ is a symmetric positive definite (SPD) matrix.\n- The Cholesky factorization of $A$ is $A = L L^{\\mathsf{T}}$, where $L$ is a unique lower-triangular matrix with positive diagonal entries.\n- $D = \\mathrm{diag}(d_{1}, \\dots, d_{n})$ is a positive diagonal matrix, meaning $d_i > 0$ for all $i = 1, \\dots, n$.\n- A multiplicatively scaled matrix $\\tilde{A}$ is defined as $\\tilde{A} = D A D$.\n- The Cholesky factorization of $\\tilde{A}$ is $\\tilde{A} = \\tilde{L} \\tilde{L}^{\\mathsf{T}}$, where $\\tilde{L}$ is a unique lower-triangular matrix with positive diagonal entries.\n- A backward-stable Cholesky algorithm computes a factor $\\tilde{L}_{\\mathrm{comp}}$ for $\\tilde{A}$ such that $\\tilde{L}_{\\mathrm{comp}} \\tilde{L}_{\\mathrm{comp}}^{\\mathsf{T}} = \\tilde{A} + \\Delta \\tilde{A}$, with the backward error satisfying $\\|\\Delta \\tilde{A}\\|_{2} \\leq \\gamma \\|\\tilde{A}\\|_{2}$, where $\\gamma = c n \\epsilon$ for a modest constant $c$, matrix dimension $n$, and machine precision $\\epsilon$.\n- A recovered factor for $A$ is formed as $L_{\\mathrm{rec}} = D^{-1} \\tilde{L}_{\\mathrm{comp}}$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem is set in the context of numerical linear algebra. All concepts—symmetric positive definite matrices, Cholesky factorization, diagonal scaling, and backward error analysis—are standard and rigorously defined. A congruence transformation of an SPD matrix $A$ by a nonsingular matrix $D$, such as $\\tilde{A} = DAD = DAD^{\\mathsf{T}}$, preserves the SPD property. Thus, $\\tilde{A}$ is guaranteed to be SPD and have its own unique Cholesky factorization. The problem is factually and mathematically sound.\n- **Well-Posed**: The problem asks for two things: an analytical expression and a conceptual explanation. The derivation of one Cholesky factor from another after scaling is a determinate algebraic manipulation. The uniqueness of the Cholesky factorization is key and is correctly stated. The backward error analysis is a standard procedure in numerical analysis. The problem is unambiguous and has a clear path to a unique solution.\n- **Objective**: The problem is stated using formal mathematical language and symbols. No subjective or opinion-based claims are present.\n- **Completeness**: All necessary definitions and relationships are provided. The problem is self-contained.\n- **Conclusion**: The problem is valid.\n\n### Solution Derivation\n\nThe first task is to derive an explicit analytic expression for the original Cholesky factor $L$ in terms of the scaled Cholesky factor $\\tilde{L}$ and the scaling matrix $D$.\n\nWe are given the following relationships:\n1.  $\\tilde{A} = D A D$\n2.  $A = L L^{\\mathsf{T}}$\n3.  $\\tilde{A} = \\tilde{L} \\tilde{L}^{\\mathsf{T}}$\n\nWe substitute the expression for $A$ from (2) into (1):\n$$\n\\tilde{A} = D (L L^{\\mathsf{T}}) D\n$$\nSince $D$ is a diagonal matrix, it is symmetric, i.e., $D = D^{\\mathsf{T}}$. We can rewrite the expression as:\n$$\n\\tilde{A} = D L L^{\\mathsf{T}} D^{\\mathsf{T}}\n$$\nUsing the property of the transpose of a matrix product, $(XY)^{\\mathsf{T}} = Y^{\\mathsf{T}}X^{\\mathsf{T}}$, we can group the terms as:\n$$\n\\tilde{A} = (D L) (L^{\\mathsf{T}} D^{\\mathsf{T}}) = (D L) (D L)^{\\mathsf{T}}\n$$\nWe now have two expressions for the Cholesky factorization of $\\tilde{A}$:\n$$\n\\tilde{A} = \\tilde{L} \\tilde{L}^{\\mathsf{T}} \\quad \\text{and} \\quad \\tilde{A} = (D L) (D L)^{\\mathsf{T}}\n$$\nThe Cholesky factorization of a symmetric positive definite matrix is unique. The unique factor is required to be a lower-triangular matrix with strictly positive diagonal entries. Let us examine the properties of the matrix $(D L)$.\n- **Triangularity:** $L$ is lower-triangular, and $D$ is diagonal. The product of a diagonal matrix and a lower-triangular matrix is lower-triangular. Specifically, the $(i,j)$-th element of the product is $(DL)_{ij} = \\sum_{k=1}^{n} D_{ik}L_{kj}$. Since $D$ is diagonal, $D_{ik} = 0$ for $k \\ne i$, so $(DL)_{ij} = D_{ii}L_{ij} = d_i L_{ij}$. If $j > i$, $L_{ij} = 0$, which implies $(DL)_{ij} = 0$. Thus, $DL$ is lower-triangular.\n- **Positive Diagonal:** The diagonal entries of $L$, denoted $L_{ii}$, are all positive. The diagonal entries of $D$, $d_i$, are also given to be positive. The diagonal entries of the product $DL$ are $(DL)_{ii} = d_i L_{ii}$. Since $d_i > 0$ and $L_{ii} > 0$, their product $d_i L_{ii}$ is also strictly positive.\n\nThe matrix $(D L)$ is therefore a lower-triangular matrix with positive diagonal entries. By the uniqueness of the Cholesky factorization, we must have:\n$$\n\\tilde{L} = D L\n$$\nThe problem asks for an expression to recover $L$ from $\\tilde{L}$ and $D$. Since all diagonal entries $d_i$ of $D$ are positive, $D$ is invertible. The inverse $D^{-1}$ is a diagonal matrix with entries $1/d_i$. We can pre-multiply the equation by $D^{-1}$ to solve for $L$:\n$$\nD^{-1} \\tilde{L} = D^{-1} (D L) = (D^{-1} D) L = I L = L\n$$\nThus, the expression for $L$ is:\n$$\nL = D^{-1} \\tilde{L}\n$$\n\nThe second task is to explain the backward error propagation when using a computed factor. We are given the relation for the computed factor $\\tilde{L}_{\\mathrm{comp}}$:\n$$\n\\tilde{L}_{\\mathrm{comp}} \\tilde{L}_{\\mathrm{comp}}^{\\mathsf{T}} = \\tilde{A} + \\Delta \\tilde{A}\n$$\nThe recovered factor for the original matrix $A$ is defined as $L_{\\mathrm{rec}} = D^{-1} \\tilde{L}_{\\mathrm{comp}}$. Let us determine which matrix $L_{\\mathrm{rec}}$ is the exact Cholesky factor of. Let this matrix be $A + \\Delta A$.\n$$\nA + \\Delta A = L_{\\mathrm{rec}} L_{\\mathrm{rec}}^{\\mathsf{T}} = (D^{-1} \\tilde{L}_{\\mathrm{comp}}) (D^{-1} \\tilde{L}_{\\mathrm{comp}})^{\\mathsf{T}}\n$$\nUsing the transpose property and $D^{-1} = (D^{-1})^{\\mathsf{T}}$ (since $D^{-1}$ is diagonal):\n$$\nA + \\Delta A = D^{-1} \\tilde{L}_{\\mathrm{comp}} \\tilde{L}_{\\mathrm{comp}}^{\\mathsf{T}} (D^{-1})^{\\mathsf{T}} = D^{-1} (\\tilde{L}_{\\mathrm{comp}} \\tilde{L}_{\\mathrm{comp}}^{\\mathsf{T}}) D^{-1}\n$$\nSubstituting the backward error relation for the computed factor:\n$$\nA + \\Delta A = D^{-1} (\\tilde{A} + \\Delta \\tilde{A}) D^{-1} = D^{-1} \\tilde{A} D^{-1} + D^{-1} \\Delta \\tilde{A} D^{-1}\n$$\nWe know that $\\tilde{A} = D A D$. Substituting this into the first term:\n$$\nD^{-1} \\tilde{A} D^{-1} = D^{-1} (D A D) D^{-1} = (D^{-1} D) A (D D^{-1}) = I A I = A\n$$\nTherefore, the perturbed matrix is $A + \\Delta A = A + D^{-1} \\Delta \\tilde{A} D^{-1}$. This means the backward error induced in the factorization of $A$ is $\\Delta A = D^{-1} \\Delta \\tilde{A} D^{-1}$.\n\nTo understand how this error depends on $D$, we can analyze its norm. The norm of the backward error $\\Delta A$ is related to the norm of $\\Delta \\tilde{A}$ by a congruence transformation involving $D^{-1}$. The 2-norm of $\\Delta A$ is bounded as follows:\n$$\n\\|\\Delta A\\|_2 = \\|D^{-1} \\Delta \\tilde{A} D^{-1}\\|_2 \\leq \\|D^{-1}\\|_2 \\|\\Delta \\tilde{A}\\|_2 \\|D^{-1}\\|_2 = \\|D^{-1}\\|_2^2 \\|\\Delta \\tilde{A}\\|_2\n$$\nThe backward stability of the algorithm applied to $\\tilde{A}$ guarantees that $\\|\\Delta \\tilde{A}\\|_2$ is small relative to $\\|\\tilde{A}\\|_2$. Specifically, $\\|\\Delta \\tilde{A}\\|_2 \\leq \\gamma \\|\\tilde{A}\\|_2 = \\gamma \\|DAD\\|_2$.\n$$\n\\|\\Delta A\\|_2 \\leq \\gamma \\|D^{-1}\\|_2^2 \\|DAD\\|_2 \\leq \\gamma \\|D^{-1}\\|_2^2 \\|D\\|_2^2 \\|A\\|_2\n$$\nThe product $\\|D\\|_2 \\|D^{-1}\\|_2$ is the spectral condition number of $D$, denoted $\\kappa_2(D)$. So, the relative backward error in $A$ is bounded by:\n$$\n\\frac{\\|\\Delta A\\|_2}{\\|A\\|_2} \\leq \\gamma (\\kappa_2(D))^2\n$$\nThis shows that the backward error for the original matrix $A$, obtained by recovering its factor from the scaled problem, is not guaranteed to be small. The error is amplified by a factor of $(\\kappa_2(D))^2$. The condition number $\\kappa_2(D)$ is the ratio of the largest to the smallest scaling factor, $\\kappa_2(D) = (\\max_i d_i) / (\\min_i d_i)$. If the scaling factors $d_i$ vary significantly, $\\kappa_2(D)$ will be large, and the recovered solution for $A$ may not be backward stable, even if the computation for the scaled matrix $\\tilde{A}$ was. The stability of the overall process is thus critically dependent on the condition number of the scaling matrix $D$.", "answer": "$$\n\\boxed{L = D^{-1} \\tilde{L}}\n$$", "id": "3568113"}, {"introduction": "Theoretical guarantees often meet their limits in the world of finite-precision arithmetic, where rounding errors can cause even robust algorithms to fail. Cholesky factorization can fail for nearly positive semidefinite matrices if a computed pivot becomes slightly negative. This hands-on coding practice challenges you to implement a standard fix: adding a small diagonal shift $\\delta I$ to ensure the matrix is numerically positive definite, bridging the gap between theory and robust implementation [@problem_id:3568148].", "problem": "You are given real, symmetric matrices that are either positive semidefinite (PSD) or nearly PSD, so that in exact arithmetic they are at least not strongly indefinite. In floating-point arithmetic with unit roundoff (machine epsilon) $\\varepsilon$, a standard Cholesky factorization without pivoting can fail when a computed pivot is slightly negative due to rounding, even if the matrix is PSD. A robust way to mitigate this failure is to add a diagonal perturbation and factorize $A + \\delta I$, where $\\delta \\ge 0$ and $I$ is the identity matrix. Your task is to design such a perturbation and compare a theoretically derived threshold with an empirically observed threshold.\n\nFundamental base to use:\n- Floating-point arithmetic satisfies the standard model $\\operatorname{fl}(x \\circ y) = (x \\circ y)(1 + \\theta)$ with $|\\theta| \\le \\varepsilon$ for each basic arithmetic operation $\\circ$.\n- The Cholesky algorithm is backward stable in the following sense: the computed factorization corresponds to the exact Cholesky factorization of $A + \\Delta$ for a symmetric $\\Delta$ whose spectral norm is bounded by a constant multiple of $n \\varepsilon \\lVert A \\rVert_2$, where $n$ is the matrix dimension and the constant does not depend on $A$.\n\nProblem requirements:\n1. Starting only from the fundamental base above and the definitions of eigenvalues and operator norm, derive a sufficient condition on $\\delta$ that guarantees that the Cholesky factorization of $A + \\delta I$ will succeed in floating-point arithmetic. Your derivation must express the minimal $\\delta$ that satisfies your sufficient condition in terms of the smallest eigenvalue $\\lambda_{\\min}(A)$, the spectral norm $\\lVert A \\rVert_2$, the dimension $n$, the machine epsilon $\\varepsilon$, and a modest absolute constant $c_{\\text{chol}}$ associated with the backward error bound of Cholesky. Do not use any specialized formulas beyond the fundamental base stated above.\n\n2. Using the bound you derived, define the theoretical threshold $\\delta_{\\text{th}}(A,\\varepsilon)$ by substituting $c_{\\text{chol}} = 5$ and $n = 3$.\n\n3. For each test matrix $A$ listed below, compute:\n   - The theoretical threshold $\\delta_{\\text{th}}(A,\\varepsilon)$.\n   - An empirical threshold $\\delta_{\\text{emp}}(A,\\varepsilon)$, defined as the smallest $\\delta \\ge 0$ (up to a fixed absolute or relative tolerance) for which a standard double-precision Cholesky factorization of $A + \\delta I$ succeeds without error. The empirical threshold must be found by a deterministic search procedure that starts from $\\delta = 0$ and increases $\\delta$ only as needed.\n\n4. Testing must be done in double precision (IEEE $754$ binary64), and $\\varepsilon$ must be taken from the execution environment. No physical units are involved.\n\n5. Test suite. Use the following symmetric matrices (each is of size $3 \\times 3$), written here with entries in exact decimal form:\n   - $A_1 = \\begin{bmatrix}\n      1.0 & 0.999999999999 & 0.0 \\\\\n      0.999999999999 & 1.0 & 0.0 \\\\\n      0.0 & 0.0 & 1.0\\times 10^{-12}\n   \\end{bmatrix}$\n   - $A_2 = \\begin{bmatrix}\n      1.0 & 1.0 & 1.0 \\\\\n      1.0 & 1.0 & 1.0 \\\\\n      1.0 & 1.0 & 1.0\n   \\end{bmatrix}$\n   - $A_3 = \\begin{bmatrix}\n      1.0 & 0.0 & 0.0 \\\\\n      0.0 & 1.0\\times 10^{-8} & 0.0 \\\\\n      0.0 & 0.0 & -1.0\\times 10^{-14}\n   \\end{bmatrix}$\n   - $A_4 = \\begin{bmatrix}\n      1.0\\times 10^{-16} & 0.0 & 0.0 \\\\\n      0.0 & 2.0\\times 10^{-16} & 0.0 \\\\\n      0.0 & 0.0 & 3.0\\times 10^{-16}\n   \\end{bmatrix}$\n\n6. Output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each $A_i$, output the pair $\\delta_{\\text{th}}(A_i,\\varepsilon)$ and $\\delta_{\\text{emp}}(A_i,\\varepsilon)$ in this order, flattened across all test cases. Concretely, the output must be of the form\n   $[\\delta_{\\text{th}}(A_1,\\varepsilon),\\delta_{\\text{emp}}(A_1,\\varepsilon),\\delta_{\\text{th}}(A_2,\\varepsilon),\\delta_{\\text{emp}}(A_2,\\varepsilon),\\delta_{\\text{th}}(A_3,\\varepsilon),\\delta_{\\text{emp}}(A_3,\\varepsilon),\\delta_{\\text{th}}(A_4,\\varepsilon),\\delta_{\\text{emp}}(A_4,\\varepsilon)]$\n   where each entry is a floating-point number in the native formatting of the language used.\n\nClarifications and constraints:\n- You must compute the spectral norm $\\lVert A \\rVert_2$ and the smallest eigenvalue $\\lambda_{\\min}(A)$ using the symmetric eigenvalue decomposition, i.e., via the eigenvalues of $A$.\n- For the empirical threshold, you must implement a deterministic search strategy that first tests $\\delta = 0$, then increases $\\delta$ monotonically until success, and finally refines to approximate the minimal $\\delta$ to within a reasonable tolerance. Your approach must be robust to cases where $\\delta_{\\text{emp}}$ is extremely small.\n- The program must be self-contained and produce the single line in the exact format specified above.", "solution": "The problem is assessed to be **valid**. It is a well-posed problem in numerical linear algebra, grounded in the standard principles of floating-point error analysis and matrix theory. All necessary data and definitions are provided, and the requirements are specific and objective.\n\n### 1. Derivation of the Theoretical Threshold $\\delta_{\\text{th}}$\n\nThe Cholesky factorization of a real, symmetric matrix succeeds if and only if the matrix is positive definite. In floating-point arithmetic, the computed Cholesky factors $\\hat{L}$ of a matrix $B$ are the exact factors of a perturbed matrix $B+E$, where $E$ is a symmetric matrix representing the backward error. The problem provides a bound on the spectral norm of this error:\n$$ \\lVert E \\rVert_2 \\le c_{\\text{chol}} n \\varepsilon \\lVert B \\rVert_2 $$\nwhere $n$ is the dimension of the matrix, $\\varepsilon$ is the machine epsilon, and $c_{\\text{chol}}$ is a modest constant.\n\nOur goal is to find a sufficient condition on $\\delta \\ge 0$ such that the Cholesky factorization of $B = A + \\delta I$ succeeds. This means the matrix $B+E = (A + \\delta I) + E$ must be positive definite. A matrix is positive definite if and only if all its eigenvalues are strictly positive. Thus, we require $\\lambda_{\\min}((A + \\delta I) + E) > 0$.\n\nUsing Weyl's inequality for the eigenvalues of a sum of symmetric matrices, we have:\n$$ \\lambda_{\\min}((A + \\delta I) + E) \\ge \\lambda_{\\min}(A + \\delta I) + \\lambda_{\\min}(E) $$\nThe eigenvalues of a symmetric matrix $E$ are all real. The spectral norm is $\\lVert E \\rVert_2 = \\max_i |\\lambda_i(E)|$. This implies that the smallest eigenvalue $\\lambda_{\\min}(E)$ is bounded below by $-\\lVert E \\rVert_2$. Therefore, a sufficient condition for $\\lambda_{\\min}((A + \\delta I) + E) > 0$ is:\n$$ \\lambda_{\\min}(A + \\delta I) - \\lVert E \\rVert_2 > 0 \\quad \\implies \\quad \\lambda_{\\min}(A + \\delta I) > \\lVert E \\rVert_2 $$\nThe eigenvalues of $A + \\delta I$ are $\\lambda_i(A) + \\delta$. Thus, $\\lambda_{\\min}(A + \\delta I) = \\lambda_{\\min}(A) + \\delta$. Substituting this and the backward error bound for $E$ (with $B = A + \\delta I$) into the inequality, we get:\n$$ \\lambda_{\\min}(A) + \\delta > c_{\\text{chol}} n \\varepsilon \\lVert A + \\delta I \\rVert_2 $$\nTo proceed, we need to express $\\lVert A + \\delta I \\rVert_2$ in terms of known quantities. For a symmetric matrix, the spectral norm is the maximum absolute value of its eigenvalues. We are choosing $\\delta$ to make $A+\\delta I$ positive definite, or at least positive semidefinite. For a positive semidefinite matrix $M$, $\\lVert M \\rVert_2 = \\lambda_{\\max}(M)$. Assuming our choice of $\\delta$ is sufficient to make $A+\\delta I$ at least positive semidefinite (i.e., $\\lambda_{\\min}(A) + \\delta \\ge 0$), then:\n$$ \\lVert A + \\delta I \\rVert_2 = \\lambda_{\\max}(A + \\delta I) = \\lambda_{\\max}(A) + \\delta $$\nFor the nearly positive semidefinite matrices given in the problem, $\\lambda_{\\max}(A) = \\lVert A \\rVert_2$. Substituting $\\lVert A \\rVert_2$ for $\\lambda_{\\max}(A)$ gives:\n$$ \\lVert A + \\delta I \\rVert_2 = \\lVert A \\rVert_2 + \\delta $$\nThe inequality becomes:\n$$ \\lambda_{\\min}(A) + \\delta > c_{\\text{chol}} n \\varepsilon (\\lVert A \\rVert_2 + \\delta) $$\nWe now solve for $\\delta$:\n$$ \\lambda_{\\min}(A) + \\delta > c_{\\text{chol}} n \\varepsilon \\lVert A \\rVert_2 + c_{\\text{chol}} n \\varepsilon \\delta $$\n$$ \\delta (1 - c_{\\text{chol}} n \\varepsilon) > c_{\\text{chol}} n \\varepsilon \\lVert A \\rVert_2 - \\lambda_{\\min}(A) $$\nSince $c_{\\text{chol}} n \\varepsilon \\ll 1$, the term $(1 - c_{\\text{chol}} n \\varepsilon)$ is positive. Thus, we can divide by it without changing the inequality's direction:\n$$ \\delta > \\frac{c_{\\text{chol}} n \\varepsilon \\lVert A \\rVert_2 - \\lambda_{\\min}(A)}{1 - c_{\\text{chol}} n \\varepsilon} $$\nThe minimal $\\delta$ satisfying this sufficient condition is the right-hand side. Since $\\delta$ must be non-negative, we take the maximum of this expression and $0$.\n\nThe problem specifies $c_{\\text{chol}} = 5$ and $n = 3$. The theoretical threshold $\\delta_{\\text{th}}(A, \\varepsilon)$ is therefore defined as:\n$$ \\delta_{\\text{th}}(A, \\varepsilon) = \\max\\left(0, \\frac{15 \\varepsilon \\lVert A \\rVert_2 - \\lambda_{\\min}(A)}{1 - 15 \\varepsilon}\\right) $$\nThe values for $\\lambda_{\\min}(A)$ and $\\lVert A \\rVert_2$ are computed from the eigenvalues of $A$.\n\n### 2. Method for Empirical Threshold $\\delta_{\\text{emp}}$\n\nThe empirical threshold $\\delta_{\\text{emp}}(A, \\varepsilon)$ is the smallest non-negative value $\\delta$ for which the floating-point Cholesky factorization of $A + \\delta I$ succeeds. We find this value using a deterministic search procedure:\n\n1.  **Check $\\delta = 0$**: First, attempt to compute the Cholesky factorization of $A$. If it succeeds, then $\\delta_{\\text{emp}}(A, \\varepsilon) = 0$.\n\n2.  **Find an Upper Bound**: If the factorization for $\\delta=0$ fails, we need to find a $\\delta > 0$ for which it succeeds. We start with a small initial guess, $\\delta_{\\text{high}} = \\varepsilon \\cdot \\max(1, \\lVert A \\rVert_2)$, and repeatedly double it ($\\delta_{\\text{high}} \\leftarrow 2\\delta_{\\text{high}}$) until the factorization of $A + \\delta_{\\text{high}} I$ succeeds. This process guarantees finding an upper bound for $\\delta_{\\text{emp}}$.\n\n3.  **Bisection Search**: Once we have an interval $[\\delta_{\\text{low}}, \\delta_{\\text{high}}]$ where the factorization fails at $\\delta_{\\text{low}}$ and succeeds at $\\delta_{\\text{high}}$ (initially, $\\delta_{\\text{low}}$ is either $0$ or $\\delta_{\\text{high}}/2$), we use a bisection method to refine the estimate. We repeatedly test the midpoint $\\delta_{\\text{mid}} = (\\delta_{\\text{low}} + \\delta_{\\text{high}})/2$. If the factorization succeeds, we set $\\delta_{\\text{high}} = \\delta_{\\text{mid}}$; otherwise, we set $\\delta_{\\text{low}} = \\delta_{\\text{mid}}$. This process is continued for a fixed number of iterations ($100$) to converge to a value very close to the true threshold. The final $\\delta_{\\text{emp}}$ is the smallest value found for which the test passed, i.e., the final $\\delta_{\\text{high}}$.\n\nThis combination of an exponential search followed by bisection is both deterministic and efficient. All Cholesky attempts are wrapped in a try-except block to catch the `numpy.linalg.LinAlgError` that signals failure.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef compute_delta_th(A, epsilon):\n    \"\"\"\n    Computes the theoretical threshold delta_th based on the derived formula.\n    \"\"\"\n    n = 3\n    c_chol = 5\n    \n    # For a symmetric matrix, eigenvalues are real. np.linalg.eigvalsh is efficient and stable.\n    eigvals = np.linalg.eigvalsh(A)\n    lambda_min = np.min(eigvals)\n    # The spectral norm of a symmetric matrix is the maximum absolute eigenvalue.\n    norm_A = np.max(np.abs(eigvals))\n    \n    cne = c_chol * n * epsilon\n    \n    # Formula derived in the solution part:\n    # delta > (c_chol * n * epsilon * ||A||_2 - lambda_min(A)) / (1 - c_chol * n * epsilon)\n    numerator = cne * norm_A - lambda_min\n    \n    # If numerator is non-positive, the original matrix is already \"sufficiently\" positive definite.\n    if numerator <= 0:\n        return 0.0\n    \n    denominator = 1 - cne\n    \n    return numerator / denominator\n\ndef compute_delta_emp(A):\n    \"\"\"\n    Computes the empirical threshold delta_emp using a deterministic search.\n    \"\"\"\n    def check_cholesky(delta):\n        \"\"\"Helper function to test Cholesky factorization for A + delta*I.\"\"\"\n        try:\n            # Use a copy of A to avoid modifying the original test case matrix\n            A_pert = A + delta * np.identity(A.shape[0])\n            np.linalg.cholesky(A_pert)\n            return True\n        except np.linalg.LinAlgError:\n            return False\n\n    # 1. Check if delta = 0 works.\n    if check_cholesky(0.0):\n        return 0.0\n\n    # 2. Find an upper bound for delta where Cholesky succeeds.\n    # Start with a small guess related to machine epsilon and the matrix norm.\n    norm_A = np.linalg.norm(A, 2)\n    delta_high = np.finfo(float).eps * max(1.0, norm_A)\n    if delta_high == 0: # Case where A is the zero matrix\n        delta_high = np.finfo(float).eps\n\n    # Exponentially increase delta until we find a working value.\n    initial_guess = delta_high\n    while not check_cholesky(delta_high):\n        delta_high *= 2.0\n\n    # 3. Bisection search to refine the threshold.\n    # The lower bound is a value we know fails.\n    if delta_high == initial_guess:\n        delta_low = 0.0\n    else:\n        delta_low = delta_high / 2.0\n\n    # 100 iterations are more than sufficient for double precision convergence.\n    for _ in range(100):\n        delta_mid = delta_low + (delta_high - delta_low) / 2.0\n        # If we reach the limit of floating point precision, stop.\n        if delta_mid == delta_low or delta_mid == delta_high:\n            break\n        \n        if check_cholesky(delta_mid):\n            delta_high = delta_mid  # Midpoint works, try smaller values.\n        else:\n            delta_low = delta_mid   # Midpoint fails, need larger values.\n            \n    # Return the smallest value found that allows Cholesky to succeed.\n    return delta_high\n\ndef solve():\n    \"\"\"\n    Main function to run the analysis for all test cases and print the results.\n    \"\"\"\n    # Get machine epsilon for double precision from the execution environment.\n    epsilon = np.finfo(float).eps\n    \n    # Define the test cases from the problem statement.\n    a12 = 0.999999999999\n    \n    test_cases = [\n        np.array([\n            [1.0, a12, 0.0],\n            [a12, 1.0, 0.0],\n            [0.0, 0.0, 1.0e-12]\n        ]),\n        np.array([\n            [1.0, 1.0, 1.0],\n            [1.0, 1.0, 1.0],\n            [1.0, 1.0, 1.0]\n        ]),\n        np.array([\n            [1.0, 0.0, 0.0],\n            [0.0, 1.0e-8, 0.0],\n            [0.0, 0.0, -1.0e-14]\n        ]),\n        np.array([\n            [1.0e-16, 0.0, 0.0],\n            [0.0, 2.0e-16, 0.0],\n            [0.0, 0.0, 3.0e-16]\n        ]),\n    ]\n\n    results = []\n    for A in test_cases:\n        delta_th = compute_delta_th(A, epsilon)\n        delta_emp = compute_delta_emp(A)\n        results.extend([delta_th, delta_emp])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3568148"}]}