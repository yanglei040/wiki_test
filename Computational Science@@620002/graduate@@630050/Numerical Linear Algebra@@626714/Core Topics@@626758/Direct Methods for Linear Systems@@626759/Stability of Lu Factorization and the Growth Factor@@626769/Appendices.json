{"hands_on_practices": [{"introduction": "The stability of Gaussian elimination is fundamentally linked to the growth factor, which measures the magnitude of intermediate elements relative to the original matrix. A small growth factor is a hallmark of a stable factorization. This first exercise provides essential hands-on practice in computing the growth factor directly from its definition by performing Gaussian elimination with partial pivoting on a simple parameterized matrix [@problem_id:3581067].", "problem": "Consider Gaussian elimination with partial pivoting (GEPP) applied to the matrix $A(\\epsilon) \\in \\mathbb{R}^{3 \\times 3}$ defined by\n$$\nA(\\epsilon) \\;=\\; \\begin{bmatrix}\n1 & 1 & 1 \\\\\n1 & \\epsilon & 1 \\\\\n1 & 1 & \\epsilon\n\\end{bmatrix},\n$$\nwhere $\\epsilon$ is a fixed scalar with $0<\\epsilon<1$. Use the following deterministic tie-breaking rule for partial pivoting: whenever two or more candidate pivot rows have equal maximal absolute value in the active column, select the one with the smallest row index. Define the growth factor for GEPP as\n$$\n\\rho(A) \\;=\\; \\frac{\\displaystyle \\max_{k \\in \\{0,1,2,3\\}} \\max_{i,j} \\big|a^{(k)}_{ij}\\big|}{\\displaystyle \\max_{i,j} \\big|a^{(0)}_{ij}\\big|},\n$$\nwhere $A^{(0)} = A(\\epsilon)$, and $A^{(k)}$ denotes the matrix of active trailing entries after $k$ elimination steps (so that $A^{(3)}$ is the upper triangular factor $U$). Starting from the core definitions of GEPP and the growth factor, carry out the elimination explicitly for $A(\\epsilon)$ under the stated pivoting rule, and derive a closed-form expression for $\\rho(A)$ as a function of $\\epsilon$. Then, based on your derivation, explain how the parameter $\\epsilon$ influences element growth under GEPP for this family of matrices. Provide your final answer for $\\rho(A)$ as a single closed-form analytic expression in $\\epsilon$. No rounding is required.", "solution": "The problem requires the calculation of the growth factor $\\rho(A)$ for Gaussian elimination with partial pivoting (GEPP) applied to a specific $3 \\times 3$ matrix $A(\\epsilon)$. The analysis will proceed by explicitly carrying out the elimination steps.\n\nThe given matrix is:\n$$\nA(\\epsilon) = A^{(0)} = \\begin{bmatrix}\n1 & 1 & 1 \\\\\n1 & \\epsilon & 1 \\\\\n1 & 1 & \\epsilon\n\\end{bmatrix}\n$$\nwhere $\\epsilon$ is a scalar such that $0  \\epsilon  1$.\n\nThe growth factor is defined as:\n$$\n\\rho(A) = \\frac{\\displaystyle \\max_{k \\in \\{0,1,2,3\\}} \\max_{i,j} \\big|a^{(k)}_{ij}\\big|}{\\displaystyle \\max_{i,j} \\big|a^{(0)}_{ij}\\big|}\n$$\nHere, $A^{(0)}$ is the initial matrix. The term $\\max_{i,j} |a^{(k)}_{ij}|$ refers to the maximum absolute value of any entry in the full matrix present after step $k$ of the elimination. The matrix $A^{(k)}$ for $k \\in \\{1, 2\\}$ is the active trailing submatrix, but the numerator of the growth factor definition considers all elements generated up to and including the final upper triangular matrix $U = A^{(3)}$.\n\nFirst, we determine the denominator of the growth factor expression. This is the maximum absolute value of the entries in the initial matrix $A^{(0)}$.\n$$\n\\max_{i,j} \\big|a^{(0)}_{ij}\\big| = \\max \\left\\{ |1|, |\\epsilon| \\right\\}\n$$\nSince $0  \\epsilon  1$, the maximum absolute value is $1$.\n$$\n\\max_{i,j} \\big|a^{(0)}_{ij}\\big| = 1\n$$\nThe maximum element magnitude found so far, which contributes to the numerator, is also $1$.\n\nNow, we perform the steps of GEPP.\n\n**Step 1: Elimination in column 1**\n\nThe first column of $A^{(0)}$ is $\\begin{bmatrix} 1  1  1 \\end{bmatrix}^T$. The entry with the maximum absolute value is $1$, which appears in all three rows. The specified deterministic tie-breaking rule requires selecting the candidate row with the smallest row index. Thus, row $1$ is chosen as the pivot row. The pivot element is $a_{11} = 1$. No row permutation is necessary.\n\nThe multipliers for the elimination are:\n$$\nm_{21} = \\frac{a_{21}}{a_{11}} = \\frac{1}{1} = 1\n$$\n$$\nm_{31} = \\frac{a_{31}}{a_{11}} = \\frac{1}{1} = 1\n$$\n\nThe new rows are computed as $R_2 \\leftarrow R_2 - m_{21}R_1$ and $R_3 \\leftarrow R_3 - m_{31}R_1$:\nRow 2: $[1, \\epsilon, 1] - 1 \\cdot [1, 1, 1] = [0, \\epsilon - 1, 0]$\nRow 3: $[1, 1, \\epsilon] - 1 \\cdot [1, 1, 1] = [0, 0, \\epsilon - 1]$\n\nThe matrix after the first step of elimination, which we denote as $M^{(1)}$, is:\n$$\nM^{(1)} = \\begin{bmatrix}\n1  1  1 \\\\\n0  \\epsilon-1  0 \\\\\n0  0  \\epsilon-1\n\\end{bmatrix}\n$$\nThe active trailing submatrix is $A^{(1)}$:\n$$\nA^{(1)} = \\begin{bmatrix}\n\\epsilon-1  0 \\\\\n0  \\epsilon-1\n\\end{bmatrix}\n$$\nThe new elements generated are $\\epsilon-1$ and $0$. Since $0  \\epsilon  1$, we have $-1  \\epsilon-1  0$, so $|\\epsilon-1| = 1-\\epsilon$. As $1-\\epsilon  1$, the maximum absolute value of any element seen so far ($A^{(0)}$ and $M^{(1)}$) remains $1$.\n\n**Step 2: Elimination in column 2**\n\nThe elimination proceeds on the active submatrix $A^{(1)}$. The first column of $A^{(1)}$ is $\\begin{bmatrix} \\epsilon-1 \\\\ 0 \\end{bmatrix}$. The maximum absolute value is $|\\epsilon-1| = 1-\\epsilon$, which is in the first row of the submatrix (corresponding to row $2$ of the full matrix). This is our pivot element. No row swap is needed.\n\nThe pivot element is $a^{(1)}_{11} = \\epsilon-1$. The multiplier is:\n$$\nm_{32} = \\frac{a^{(1)}_{21}}{a^{(1)}_{11}} = \\frac{0}{\\epsilon-1} = 0\n$$\nSince the multiplier is $0$, the subsequent row update $R_3 \\leftarrow R_3 - m_{32}R_2$ results in no change to row $3$. The matrix after the second step, $M^{(2)}$, is identical to $M^{(1)}$:\n$$\nM^{(2)} = \\begin{bmatrix}\n1  1  1 \\\\\n0  \\epsilon-1  0 \\\\\n0  0  \\epsilon-1\n\\end{bmatrix}\n$$\nNo new element values are generated. The active $1 \\times 1$ submatrix is $A^{(2)} = [\\epsilon-1]$.\n\n**Step 3: Final Matrix**\n\nThe elimination process is complete. The final upper triangular matrix is $U = A^{(3)} = M^{(2)}$.\n$$\nU = A^{(3)} = \\begin{bmatrix}\n1  1  1 \\\\\n0  \\epsilon-1  0 \\\\\n0  0  \\epsilon-1\n\\end{bmatrix}\n$$\n\n**Calculation of the Growth Factor**\n\nThe numerator of $\\rho(A)$ is the maximum absolute value of any element encountered throughout all stages ($k=0, 1, 2, 3$). The set of all entries that appeared in the matrices $A^{(0)}, M^{(1)}, M^{(2)}, U$ is $\\{1, \\epsilon, \\epsilon-1, 0\\}$.\nThe absolute values of these elements are $\\{1, \\epsilon, 1-\\epsilon, 0\\}$.\nSince $0  \\epsilon  1$, both $\\epsilon$ and $1-\\epsilon$ are strictly between $0$ and $1$.\nTherefore, the maximum of these absolute values is $1$.\n$$\n\\max_{k \\in \\{0,1,2,3\\}} \\max_{i,j} \\big|a^{(k)}_{ij}\\big| = 1\n$$\nNow, we can compute the growth factor:\n$$\n\\rho(A) = \\frac{1}{1} = 1\n$$\n\n**Influence of the Parameter $\\epsilon$**\n\nThe derivation shows that the growth factor $\\rho(A)$ is equal to $1$ for any value of $\\epsilon$ in the range $0  \\epsilon  1$. A growth factor of $1$ signifies that no element growth occurred during the elimination process; the largest element in magnitude was present in the original matrix. For this specific family of matrices and the given GEPP procedure, the algorithm is perfectly stable. The parameter $\\epsilon$ influences the values of the intermediate and final matrix entries (e.g., $\\epsilon-1$), but due to the problem's structure and the nature of partial pivoting, these new entries never exceed the magnitude of the largest initial entry. Consequently, $\\epsilon$ has no influence on the element growth for this problem.", "answer": "$$\\boxed{1}$$", "id": "3581067"}, {"introduction": "While the growth factor warns of potential instability in the factorization process, the ultimate goal is an accurate solution. This practice explores how we measure the quality of a computed solution using the concept of backward error. By calculating both normwise and componentwise backward error for a strategically designed system [@problem_id:3581031], you will uncover why a single norm-based metric can be misleading and why a component-by-component analysis is crucial for ill-scaled problems.", "problem": "Consider Gaussian elimination with partial pivoting used to compute an approximate solution to a linear system in exact arithmetic except for a final rounding that yields a nonzero residual. Let $M = 10^{8}$ and $\\eta = \\frac{1}{5}$. Define the matrix and right-hand side\n$$\nA = \\begin{pmatrix} M  0 \\\\ 0  1 \\end{pmatrix}, \\qquad b = \\begin{pmatrix} M \\\\ 1 - \\eta \\end{pmatrix}.\n$$\nSuppose the computed solution is\n$$\n\\hat{x} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}.\n$$\nLet the residual be $r = b - A \\hat{x}$. Work with the infinity norm $\\|\\cdot\\|_{\\infty}$.\n\n1. Compute the normwise backward error in the infinity norm,\n$$\n\\mu_{\\infty} = \\frac{\\|r\\|_{\\infty}}{\\|A\\|_{\\infty} \\,\\|\\hat{x}\\|_{\\infty} + \\|b\\|_{\\infty}},\n$$\nand the componentwise backward error,\n$$\n\\omega_{\\infty} = \\left\\|\\,|r| \\oslash \\left(|A|\\,|\\hat{x}| + |b|\\right)\\,\\right\\|_{\\infty},\n$$\nwhere $|\\,\\cdot\\,|$ denotes entrywise absolute value and $\\oslash$ denotes entrywise division.\n\n2. Compute the pivot growth factor for partial pivoting on $A$,\n$$\n\\gamma_{\\infty}(A) = \\frac{\\max_{i,j} |u_{ij}|}{\\max_{i,j} |a_{ij}|},\n$$\nwhere $U$ is the upper triangular factor produced by Gaussian elimination with partial pivoting on $A$.\n\n3. Using the values of $\\mu_{\\infty}$ and $\\omega_{\\infty}$, compute the ratio\n$$\n\\rho = \\frac{\\omega_{\\infty}}{\\mu_{\\infty}}.\n$$\n\nExplain briefly why, in this example, the normwise backward error can be misleadingly small compared with the componentwise backward error, even though the pivot growth factor is minimal.\n\nRound your final numerical answer for $\\rho$ to four significant figures.", "solution": "**Part 1: Computation of $\\mu_{\\infty}$ and $\\omega_{\\infty}$**\n\nFirst, we compute the residual vector $r$.\nThe product $A\\hat{x}$ is:\n$$\nA\\hat{x} = \\begin{pmatrix} M  0 \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} M \\cdot 1 + 0 \\cdot 1 \\\\ 0 \\cdot 1 + 1 \\cdot 1 \\end{pmatrix} = \\begin{pmatrix} M \\\\ 1 \\end{pmatrix}\n$$\nThe right-hand side vector $b$ is:\n$$\nb = \\begin{pmatrix} M \\\\ 1 - \\eta \\end{pmatrix} = \\begin{pmatrix} 10^{8} \\\\ 1 - \\frac{1}{5} \\end{pmatrix} = \\begin{pmatrix} 10^{8} \\\\ \\frac{4}{5} \\end{pmatrix}\n$$\nThe residual $r = b - A\\hat{x}$ is:\n$$\nr = \\begin{pmatrix} M \\\\ 1 - \\eta \\end{pmatrix} - \\begin{pmatrix} M \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} M - M \\\\ (1 - \\eta) - 1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ -\\eta \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ -\\frac{1}{5} \\end{pmatrix}\n$$\nNext, we compute the necessary norms to find $\\mu_{\\infty}$:\n$$\n\\|r\\|_{\\infty} = \\max\\left(|0|, |-\\frac{1}{5}|\\right) = \\frac{1}{5}\n$$\n$$\n\\|A\\|_{\\infty} = \\max\\left(|M| + |0|, |0| + |1|\\right) = \\max(M, 1) = M = 10^{8}\n$$\n$$\n\\|\\hat{x}\\|_{\\infty} = \\max(|1|, |1|) = 1\n$$\n$$\n\\|b\\|_{\\infty} = \\max\\left(|M|, |1 - \\eta|\\right) = \\max\\left(10^{8}, \\frac{4}{5}\\right) = M = 10^{8}\n$$\nNow we compute the normwise backward error $\\mu_{\\infty}$:\n$$\n\\mu_{\\infty} = \\frac{\\|r\\|_{\\infty}}{\\|A\\|_{\\infty} \\,\\|\\hat{x}\\|_{\\infty} + \\|b\\|_{\\infty}} = \\frac{\\frac{1}{5}}{M \\cdot 1 + M} = \\frac{\\frac{1}{5}}{2M} = \\frac{1}{10M} = \\frac{1}{10 \\cdot 10^{8}} = 10^{-9}\n$$\nNext, we compute the quantities needed for the componentwise backward error $\\omega_{\\infty}$. We need the vector $|r| \\oslash (|A|\\,|\\hat{x}| + |b|)$.\n$$\n|r| = \\begin{pmatrix} 0 \\\\ \\frac{1}{5} \\end{pmatrix}, \\quad |A| = \\begin{pmatrix} M  0 \\\\ 0  1 \\end{pmatrix}, \\quad |\\hat{x}| = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}, \\quad |b| = \\begin{pmatrix} M \\\\ 1 - \\eta \\end{pmatrix} = \\begin{pmatrix} M \\\\ \\frac{4}{5} \\end{pmatrix}\n$$\nThe denominator vector is:\n$$\n|A|\\,|\\hat{x}| + |b| = \\begin{pmatrix} M  0 \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} M \\\\ \\frac{4}{5} \\end{pmatrix} = \\begin{pmatrix} M \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} M \\\\ \\frac{4}{5} \\end{pmatrix} = \\begin{pmatrix} 2M \\\\ 1 + \\frac{4}{5} \\end{pmatrix} = \\begin{pmatrix} 2M \\\\ \\frac{9}{5} \\end{pmatrix}\n$$\nNow we perform the entrywise division:\n$$\n|r| \\oslash \\left(|A|\\,|\\hat{x}| + |b|\\right) = \\begin{pmatrix} 0 / (2M) \\\\ (\\frac{1}{5}) / (\\frac{9}{5}) \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ \\frac{1}{5} \\cdot \\frac{5}{9} \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ \\frac{1}{9} \\end{pmatrix}\n$$\nThe componentwise backward error $\\omega_{\\infty}$ is the infinity norm of this resulting vector:\n$$\n\\omega_{\\infty} = \\left\\| \\begin{pmatrix} 0 \\\\ \\frac{1}{9} \\end{pmatrix} \\right\\|_{\\infty} = \\max\\left(|0|, |\\frac{1}{9}|\\right) = \\frac{1}{9}\n$$\n\n**Part 2: Computation of the pivot growth factor $\\gamma_{\\infty}(A)$**\n\nThe matrix is $A = \\begin{pmatrix} M  0 \\\\ 0  1 \\end{pmatrix}$. To perform Gaussian elimination with partial pivoting, we examine the first column. The entry with the largest magnitude is $a_{11} = M$. Since it is already in the pivot position $(1,1)$, no row interchange is necessary. The permutation matrix is the identity, $P=I$.\nThe elimination step involves using the pivot $a_{11}$ to zero out entries below it. The only entry below it is $a_{21}=0$, which is already zero. Thus, no elimination operations are performed.\nThe $LU$ factorization is $A = LU$. The lower triangular matrix $L$ is the identity matrix, and the upper triangular matrix $U$ is equal to $A$.\n$$\nL = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix}, \\qquad U = A = \\begin{pmatrix} M  0 \\\\ 0  1 \\end{pmatrix}\n$$\nThe pivot growth factor is:\n$$\n\\gamma_{\\infty}(A) = \\frac{\\max_{i,j} |u_{ij}|}{\\max_{i,j} |a_{ij}|} = \\frac{\\max(|M|, |0|, |0|, |1|)}{\\max(|M|, |0|, |0|, |1|)} = \\frac{M}{M} = 1\n$$\nA growth factor of $1$ is the smallest possible value and indicates that the LU factorization process is perfectly stable.\n\n**Part 3: Computation of $\\rho$ and Explanation**\n\nThe ratio $\\rho$ is:\n$$\n\\rho = \\frac{\\omega_{\\infty}}{\\mu_{\\infty}} = \\frac{1/9}{10^{-9}} = \\frac{1}{9} \\times 10^{9} \\approx 111,111,111.11 \\dots\n$$\nRounding to four significant figures, we get:\n$$\n\\rho \\approx 1.111 \\times 10^{8}\n$$\n**Explanation:**\nThe vast difference between the normwise backward error $\\mu_{\\infty} = 10^{-9}$ and the componentwise backward error $\\omega_{\\infty} = \\frac{1}{9}$ highlights a key weakness of the normwise measure for ill-scaled problems.\n\nThe normwise backward error $\\mu_{\\infty}$ is misleadingly small because its denominator, $\\|A\\|_{\\infty} \\|\\hat{x}\\|_{\\infty} + \\|b\\|_{\\infty}$, is dominated by the largest entries in $A$ and $b$, namely $M=10^8$. The norm calculation aggregates information, and the large scale of the first equation, $M x_1 = M$, masks the details of the second, smaller-scale equation, $x_2=1-\\eta$. The residual's norm, $\\|r\\|_{\\infty}=\\frac{1}{5}$, is divided by a very large number ($2M$), producing a tiny result. This gives a false impression that the computed solution $\\hat{x}$ is \"good\" for the system as a whole.\n\nIn contrast, the componentwise backward error $\\omega_{\\infty}$ analyzes the error relative to the data for each component equation separately before taking the maximum.\n- For the first equation, the componentwise error is $0$, as $|r_1|/(|A\\hat{x}|_1 + |b|_1) = 0/(M+M) = 0$.\n- For the second equation, the componentwise error is $|r_2|/(|A\\hat{x}|_2 + |b|_2) = |-\\eta|/(|1| + |1-\\eta|) = (\\frac{1}{5})/(1+\\frac{4}{5}) = (\\frac{1}{5})/(\\frac{9}{5}) = \\frac{1}{9}$.\nThe overall $\\omega_{\\infty}$ is the maximum of these, $\\frac{1}{9}$, which is not small. This correctly signals that the second equation is not well-satisfied in a relative sense, as the data for that equation are all of order $1$.\n\nThe minimal pivot growth factor, $\\gamma_{\\infty}(A)=1$, confirms that the issue is not caused by any error amplification during the Gaussian elimination algorithm. The discrepancy is solely due to the poor scaling of the original problem data, where coefficients and right-hand side entries differ by many orders of magnitude. For such problems, componentwise backward error provides a much more insightful and reliable measure of solution quality.", "answer": "$$\n\\boxed{1.111 \\times 10^8}\n$$", "id": "3581031"}, {"introduction": "Having established the importance of componentwise analysis, we now apply this tool to a practical scenario. This exercise provides a computed solution to an upper triangular system, simulating the result of a backward substitution step performed in standard floating-point arithmetic. Your task is to calculate the componentwise backward error [@problem_id:3581065], reinforcing your understanding of this crucial diagnostic tool and giving you experience in assessing the quality of numerically obtained results.", "problem": "Consider solving the upper triangular system $U x = y$ arising as the $U$-solve stage in a lowerâ€“upper (LU) factorization with partial pivoting. The upper triangular matrix $U \\in \\mathbb{R}^{4 \\times 4}$ has ones on the diagonal,\n$$\nU \\;=\\;\n\\begin{pmatrix}\n1  2.5  -3.1  4.0 \\\\\n0  1  0.75  -2.2 \\\\\n0  0  1  1.6 \\\\\n0  0  0  1\n\\end{pmatrix},\n$$\nand the right-hand side is\n$$\ny \\;=\\; \\begin{pmatrix} 5.0 \\\\ -1.3 \\\\ 2.7 \\\\ -0.9 \\end{pmatrix}.\n$$\nBackward substitution is carried out in Institute of Electrical and Electronics Engineers (IEEE) binary64 (double precision) floating-point arithmetic with round-to-nearest, ties-to-even. The computed solution $\\hat{x}$ returned by the backward substitution is observed to be\n$$\n\\hat{x} \\;=\\; \\begin{pmatrix}\n37.39650000000001 \\\\\n-6.385000000000002 \\\\\n4.140000000000001 \\\\\n-0.8999999999999999\n\\end{pmatrix}.\n$$\nUsing the formal definition of componentwise backward error for linear system solves, determine the componentwise backward error of $\\hat{x}$ for this $U$-solve. Express your final answer as a single dimensionless scalar and round your answer to four significant figures.", "solution": "The user wants to determine the componentwise backward error for a given computed solution $\\hat{x}$ to an upper triangular linear system $Ux=y$.\n\nThe formal definition of the componentwise backward error for a computed solution $\\hat{x}$ to the linear system $Ax=b$ is the smallest non-negative scalar $\\omega$ such that the solution $\\hat{x}$ is the exact solution to a perturbed system $(A + \\Delta A)\\hat{x} = b + \\Delta b$, where the perturbations are bounded componentwise by $|\\Delta A| \\le \\omega|A|$ and $|\\Delta b| \\le \\omega|b|$. The formula for this value is:\n$$ \\omega = \\max_i \\frac{|(b - A\\hat{x})_i|}{(|A||\\hat{x}| + |b|)_i} $$\nIn this problem, the system is given by $U x = y$, where $U \\in \\mathbb{R}^{4 \\times 4}$ is an upper triangular matrix. Thus, the componentwise backward error $\\omega$ for the computed solution $\\hat{x}$ is:\n$$ \\omega = \\max_i \\frac{|(y - U\\hat{x})_i|}{(|U||\\hat{x}| + |y|)_i} $$\nThe calculation proceeds in three steps:\n1.  Compute the residual vector $r = y - U\\hat{x}$.\n2.  Compute the denominator vector $d = |U||\\hat{x}| + |y|$.\n3.  Compute the elementwise ratios of $|r_i|/d_i$ and find the maximum.\n\nThe given data are:\n$$\nU \\;=\\;\n\\begin{pmatrix}\n1  2.5  -3.1  4.0 \\\\\n0  1  0.75  -2.2 \\\\\n0  0  1  1.6 \\\\\n0  0  0  1\n\\end{pmatrix}, \\;\\;\ny \\;=\\; \\begin{pmatrix} 5.0 \\\\ -1.3 \\\\ 2.7 \\\\ -0.9 \\end{pmatrix}, \\;\\;\n\\hat{x} \\;=\\; \\begin{pmatrix}\n37.39650000000001 \\\\\n-6.385000000000002 \\\\\n4.140000000000001 \\\\\n-0.8999999999999999\n\\end{pmatrix}.\n$$\n\nStep 1: Compute the residual $r = y - U\\hat{x}$.\nFirst, we calculate the product $z = U\\hat{x}$. The calculations are performed with sufficient precision to avoid loss of significance.\n$$ z_4 = (U\\hat{x})_4 = 1 \\cdot \\hat{x}_4 = -0.8999999999999999 $$\n$$ z_3 = (U\\hat{x})_3 = 1 \\cdot \\hat{x}_3 + 1.6 \\cdot \\hat{x}_4 = 4.140000000000001 + 1.6(-0.8999999999999999) = 2.7000000000000013 $$\n$$ z_2 = (U\\hat{x})_2 = 1 \\cdot \\hat{x}_2 + 0.75 \\cdot \\hat{x}_3 - 2.2 \\cdot \\hat{x}_4 = -6.385000000000002 + 0.75(4.140000000000001) - 2.2(-0.8999999999999999) = -1.3000000000000018 $$\n$$ z_1 = (U\\hat{x})_1 = 1 \\cdot \\hat{x}_1 + 2.5 \\cdot \\hat{x}_2 - 3.1 \\cdot \\hat{x}_3 + 4.0 \\cdot \\hat{x}_4 = 37.39650000000001 + 2.5(-6.385000000000002) - 3.1(4.140000000000001) + 4.0(-0.8999999999999999) = 5.0 $$\nSo, the product is $U\\hat{x} = \\begin{pmatrix} 5.0 \\\\ -1.3000000000000018 \\\\ 2.7000000000000013 \\\\ -0.8999999999999999 \\end{pmatrix}$.\nThe residual vector $r = y - U\\hat{x}$ is:\n$$ r = \\begin{pmatrix} 5.0 \\\\ -1.3 \\\\ 2.7 \\\\ -0.9 \\end{pmatrix} - \\begin{pmatrix} 5.0 \\\\ -1.3000000000000018 \\\\ 2.7000000000000013 \\\\ -0.8999999999999999 \\end{pmatrix} = \\begin{pmatrix} 0.0 \\\\ 1.7763568394002505 \\times 10^{-15} \\\\ -1.3322676295501878 \\times 10^{-15} \\\\ -1.1102230246251565 \\times 10^{-16} \\end{pmatrix} $$\nThe componentwise absolute value is $|r| = \\begin{pmatrix} 0.0 \\\\ 1.7763568394002505 \\times 10^{-15} \\\\ 1.3322676295501878 \\times 10^{-15} \\\\ 1.1102230246251565 \\times 10^{-16} \\end{pmatrix}$.\n\nStep 2: Compute the denominator $d = |U||\\hat{x}| + |y|$.\nWe take the componentwise absolute values of $U$, $\\hat{x}$, and $y$:\n$$ |U| = \\begin{pmatrix} 1  2.5  3.1  4.0 \\\\ 0  1  0.75  2.2 \\\\ 0  0  1  1.6 \\\\ 0  0  0  1 \\end{pmatrix}, \\;\\; |\\hat{x}| = \\begin{pmatrix} 37.39650000000001 \\\\ 6.385000000000002 \\\\ 4.140000000000001 \\\\ 0.8999999999999999 \\end{pmatrix}, \\;\\; |y| = \\begin{pmatrix} 5.0 \\\\ 1.3 \\\\ 2.7 \\\\ 0.9 \\end{pmatrix} $$\nNext, we calculate the product $v = |U||\\hat{x}|$:\n$$ v_4 = 1 \\cdot |\\hat{x}_4| = 0.8999999999999999 $$\n$$ v_3 = 1 \\cdot |\\hat{x}_3| + 1.6 \\cdot |\\hat{x}_4| = 4.140000000000001 + 1.6(0.8999999999999999) = 5.580000000000001 $$\n$$ v_2 = 1 \\cdot |\\hat{x}_2| + 0.75 \\cdot |\\hat{x}_3| + 2.2 \\cdot |\\hat{x}_4| = 6.385000000000002 + 0.75(4.140000000000001) + 2.2(0.8999999999999999) = 11.470000000000002 $$\n$$ v_1 = 1 \\cdot |\\hat{x}_1| + 2.5 \\cdot |\\hat{x}_2| + 3.1 \\cdot |\\hat{x}_3| + 4.0 \\cdot |\\hat{x}_4| = 37.39650000000001 + 2.5(6.385000000000002) + 3.1(4.140000000000001) + 4.0(0.8999999999999999) = 69.79300000000002 $$\nSo, $|U||\\hat{x}| = \\begin{pmatrix} 69.79300000000002 \\\\ 11.470000000000002 \\\\ 5.580000000000001 \\\\ 0.8999999999999999 \\end{pmatrix}$.\nFinally, the denominator vector is $d = |U||\\hat{x}|+|y|$:\n$$ d = \\begin{pmatrix} 69.79300000000002 \\\\ 11.470000000000002 \\\\ 5.580000000000001 \\\\ 0.8999999999999999 \\end{pmatrix} + \\begin{pmatrix} 5.0 \\\\ 1.3 \\\\ 2.7 \\\\ 0.9 \\end{pmatrix} = \\begin{pmatrix} 74.79300000000002 \\\\ 12.770000000000003 \\\\ 8.280000000000001 \\\\ 1.7999999999999998 \\end{pmatrix} $$\n\nStep 3: Compute the ratios and find the maximum.\nLet $\\rho_i = |r_i|/d_i$.\n$$ \\rho_1 = \\frac{0.0}{74.79300000000002} = 0.0 $$\n$$ \\rho_2 = \\frac{1.7763568394002505 \\times 10^{-15}}{12.770000000000003} \\approx 1.391039 \\times 10^{-16} $$\n$$ \\rho_3 = \\frac{1.3322676295501878 \\times 10^{-15}}{8.280000000000001} \\approx 1.608995 \\times 10^{-16} $$\n$$ \\rho_4 = \\frac{1.1102230246251565 \\times 10^{-16}}{1.7999999999999998} \\approx 6.167906 \\times 10^{-17} $$\nThe componentwise backward error $\\omega$ is the maximum of these values:\n$$ \\omega = \\max(0.0, 1.391039 \\times 10^{-16}, 1.608995 \\times 10^{-16}, 6.167906 \\times 10^{-17}) $$\nThe maximum value is $\\rho_3$, which is approximately $1.6089947216789706 \\times 10^{-16}$.\n\nThe problem requires rounding the final answer to four significant figures.\n$$ \\omega \\approx 1.609 \\times 10^{-16} $$\nThis is the componentwise backward error, a single dimensionless scalar.", "answer": "$$\n\\boxed{1.609 \\times 10^{-16}}\n$$", "id": "3581065"}]}