{"hands_on_practices": [{"introduction": "The stability of LU factorization hinges on effective pivoting. While Gaussian Elimination with Partial Pivoting (GEPP) is the standard for serial computation, algorithms designed for modern parallel architectures, like Communication-Avoiding LU (CALU), must modify their pivoting rules to minimize data movement. This exercise provides a concrete, 'pen-and-paper' comparison between GEPP and a tournament-based pivoting scheme used in CALU [@problem_id:3591199]. By manually tracing the pivot selections and their consequences, you will gain a deep, mechanical understanding of how algorithmic trade-offs for performance can directly influence the element growth factor, a crucial indicator of numerical stability.", "problem": "Consider the $4 \\times 4$ matrix\n$$\nA \\;=\\;\n\\begin{pmatrix}\n100 & 10^{6} & 0 & 0 \\\\\n10^{-1} & 2 & 2000 & 0 \\\\\n90 & -1 & 0 & 0 \\\\\n5 \\times 10^{-2} & 3 & 0 & 0\n\\end{pmatrix}.\n$$\nYou will compare pivot selection by Gaussian Elimination with Partial Pivoting (GEPP) against Communication-Avoiding LU (CALU) with tournament pivoting, and then quantify element growth.\n\nUse the following fundamental bases and definitions.\n- The LU factorization writes $A = P L U$ where $P$ is a permutation matrix, $L$ is unit lower triangular, and $U$ is upper triangular. In GEPP, at each column $k$ the pivot row is chosen as the index $p \\geq k$ maximizing the absolute value $|a_{pk}^{(k)}|$, where $a_{ij}^{(k)}$ denotes the entry after $k-1$ elimination steps, and the pivot row is swapped into position $k$ before elimination.\n- In CALU with tournament pivoting on a panel of width $b = 2$ and two row domains $D_1 = \\{1,2\\}$, $D_2 = \\{3,4\\}$, local candidate pivots are selected by performing partial pivoting restricted to each domain on the $2$-column panel, and then a two-level tournament selects the global pivot for each column by comparing only the domain-local candidates. Concretely for this problem:\n  1. Column $1$: Each domain selects its local pivot row (the index in the domain with maximal $|a_{ik}|$ for column $k$). The global pivot for column $1$ is the candidate with the larger magnitude.\n  2. Column $2$ (within the same panel): Within each domain, perform the elimination associated with its own local column-$1$ pivot restricted to its domain rows, then select the domain-local pivot for column $2$ among the remaining row(s) in that domain. The global pivot for column $2$ is chosen by comparing the magnitudes of these two domain-local column-$2$ candidates.\n- The growth factor for a completed LU factorization is\n$$\n\\rho \\;=\\; \\frac{\\max_{i,j} |u_{ij}|}{\\max_{i,j} |a_{ij}|},\n$$\nwhere $u_{ij}$ are the entries of the final upper-triangular factor $U$ and $a_{ij}$ are the entries of the original matrix $A$.\n\nTasks:\n1. Apply GEPP to $A$ for the first two columns and state the pivot rows selected at columns $1$ and $2$, justifying from first principles why those rows are chosen.\n2. Apply the above tournament pivoting procedure for CALU to the same $2$-column panel and determine the pivot rows selected at columns $1$ and $2$, showing that they differ from GEPP’s choices.\n3. Complete the elimination implied by the CALU pivot sequence through column $2$ and propagate its effect to column $3$ so as to determine the largest $|u_{ij}|$ produced during the factorization. Compute the CALU growth factor $\\rho_{\\text{CALU}}$ for $A$. Report $\\rho_{\\text{CALU}}$ as a simplified exact fraction; do not round.\n4. Using the standard backward error framework for LU factorization, explain in terms of the growth factor and panel/tournament structure when CALU with tournament pivoting remains backward stable. Your explanation must start from the definition that backward stability means the computed solution is the exact solution of a nearby problem $(A + \\Delta A) x = b$ with $\\|\\Delta A\\|$ bounded by a modest multiple of machine precision and must explicitly relate this bound to the growth factor.\n\nYour final answer must be the single exact value of $\\rho_{\\text{CALU}}$ as a fraction, with no units and no rounding.", "solution": "The problem asks for a detailed comparison of pivot selection and its consequences for two LU factorization algorithms: standard Gaussian Elimination with Partial Pivoting (GEPP) and a simplified version of Communication-Avoiding LU (CALU) with tournament pivoting.\n\nLet the given matrix be $A$.\n$$\nA \\;=\\;\n\\begin{pmatrix}\n100 & 10^{6} & 0 & 0 \\\\\n10^{-1} & 2 & 2000 & 0 \\\\\n90 & -1 & 0 & 0 \\\\\n5 \\times 10^{-2} & 3 & 0 & 0\n\\end{pmatrix}\n\\;=\\;\n\\begin{pmatrix}\n100 & 1000000 & 0 & 0 \\\\\n0.1 & 2 & 2000 & 0 \\\\\n90 & -1 & 0 & 0 \\\\\n0.05 & 3 & 0 & 0\n\\end{pmatrix}\n$$\n\n**Task 1: Gaussian Elimination with Partial Pivoting (GEPP)**\n\nFor the first column ($k=1$), GEPP searches for the element with the largest magnitude in the entire first column of $A$. The elements are $100, 0.1, 90, 0.05$. The maximum absolute value is $|a_{11}| = 100$.\nThus, for column 1, the pivot is $a_{11}$, and the pivot row is $1$. No row swap is performed.\n\nElimination proceeds using multipliers $l_{i1} = a_{i1}/a_{11}$.\n$l_{21} = \\frac{0.1}{100} = 10^{-3}$, $l_{31} = \\frac{90}{100} = 0.9$, and $l_{41} = \\frac{0.05}{100} = 5 \\times 10^{-4}$.\nThe matrix after the first step of elimination, denoted $A^{(2)}$, becomes:\n\\begin{align*} R_2' &= R_2 - 10^{-3}R_1 = (0, 2 - 1000, 2000, 0) = (0, -998, 2000, 0) \\\\ R_3' &= R_3 - 0.9 R_1 = (0, -1 - 9 \\times 10^5, 0, 0) = (0, -900001, 0, 0) \\\\ R_4' &= R_4 - 5 \\times 10^{-4} R_1 = (0, 3 - 500, 0, 0) = (0, -497, 0, 0)\\end{align*}\n$$\nA^{(2)} \\;=\\;\n\\begin{pmatrix}\n100 & 10^{6} & 0 & 0 \\\\\n0 & -998 & 2000 & 0 \\\\\n0 & -900001 & 0 & 0 \\\\\n0 & -497 & 0 & 0\n\\end{pmatrix}\n$$\nFor the second column ($k=2$), GEPP searches for the largest magnitude element in the sub-column $a_{i2}^{(2)}$ for $i \\geq 2$. The candidates are $-998, -900001, -497$. The maximum absolute value is $|a_{32}^{(2)}| = 900001$.\nThus, for column 2, the pivot is $a_{32}^{(2)}$, and the pivot row is $3$. GEPP would swap rows $2$ and $3$.\n\nThe pivot rows selected by GEPP for columns $1$ and $2$ are **row 1** and **row 3**, respectively.\n\n**Task 2: Communication-Avoiding LU (CALU) with Tournament Pivoting**\n\nThe row domains are $D_1 = \\{1,2\\}$ and $D_2 = \\{3,4\\}$.\n\nFor column 1:\n- In domain $D_1$, we compare $|a_{11}| = 100$ and $|a_{21}| = 0.1$. The local pivot candidate is from row $1$ (value $100$).\n- In domain $D_2$, we compare $|a_{31}| = 90$ and $|a_{41}| = 0.05$. The local pivot candidate is from row $3$ (value $90$).\n- A tournament compares the local candidates: $|100|$ vs. $|90|$. The winner is the candidate from row $1$.\nThe global pivot for column 1 is **row 1**. This is the same choice as GEPP.\n\nFor column 2:\nThe CALU procedure finds column-2 candidates by performing local eliminations within each domain using that domain's local column-1 pivot.\n- In domain $D_1=\\{1,2\\}$, the local column-1 pivot was from row $1$ ($a_{11}=100$). We update row $2$ based on row $1$: the new entry is $a_{22} - \\frac{a_{21}}{a_{11}} a_{12} = 2 - \\frac{0.1}{100}(10^6) = 2 - 1000 = -998$. The candidate for the column-2 pivot from $D_1$ is this value, $-998$, from row $2$.\n- In domain $D_2=\\{3,4\\}$, the local column-1 pivot was from row $3$ ($a_{31}=90$). We update row $4$ based on row $3$: the new entry is $a_{42} - \\frac{a_{41}}{a_{31}} a_{32} = 3 - \\frac{0.05}{90}(-1) = 3 + \\frac{1}{1800} = \\frac{5401}{1800}$. The candidate for the column-2 pivot from $D_2$ is this value, $\\frac{5401}{1800}$, from row $4$.\n- The tournament for column 2 compares the candidates from each domain: $|-998|$ vs. $|\\frac{5401}{1800}| \\approx 3.0$. The value $|-998|=998$ is larger.\nThe global pivot for column 2 is **row 2**.\n\nThis differs from GEPP's choice for the second pivot (row 3). CALU's restricted search missed the globally largest element.\n\n**Task 3: CALU Growth Factor Calculation**\n\nThe CALU pivot sequence (row 1, row 2) means no permutations are performed for the first two steps. The elimination for column 1 is identical to the first step of GEPP, yielding the matrix $A^{(2)}$ calculated above.\nNext, we perform elimination for column 2 using the CALU-selected pivot $a_{22}^{(2)}=-998$.\nThe multipliers are $l_{32} = \\frac{a_{32}^{(2)}}{a_{22}^{(2)}} = \\frac{-900001}{-998} = \\frac{900001}{998}$ and $l_{42} = \\frac{a_{42}^{(2)}}{a_{22}^{(2)}} = \\frac{-497}{-998} = \\frac{497}{998}$.\n\nWe update rows 3 and 4. We are interested in the new entries in the upper triangular factor $U$.\nThe entries of $U$ are the entries of the matrix after all elimination steps. After step 2, the updated entry $u_{33}$ will be:\n$u_{33} = a_{33}^{(3)} = a_{33}^{(2)} - l_{32} a_{23}^{(2)} = 0 - \\frac{900001}{998} (2000) = - \\frac{1800002000}{998} = - \\frac{900001000}{499}$.\nThe updated entry $u_{43}$ will be:\n$u_{43} = a_{43}^{(3)} = a_{43}^{(2)} - l_{42} a_{23}^{(2)} = 0 - \\frac{497}{998} (2000) = - \\frac{994000}{998} = - \\frac{497000}{499}$.\nThe remaining columns are zero, so they are not affected. The final matrix $U$ will have diagonal entries $u_{11}=100, u_{22}=-998$, and $u_{33}$ and $u_{44}$ resulting from the final elimination step. The largest element generated is clearly $|u_{33}|$.\nThe matrix before the final elimination step is:\n$$\nA^{(3)} \\;=\\;\n\\begin{pmatrix}\n100 & 10^{6} & 0 & 0 \\\\\n0 & -998 & 2000 & 0 \\\\\n0 & 0 & - \\frac{900001000}{499} & 0 \\\\\n0 & 0 & - \\frac{497000}{499} & 0\n\\end{pmatrix}\n$$\nThe entries of the final upper triangular matrix $U$ are the non-zero elements shown (and the result of the final elimination step, which doesn't create larger values). The maximum magnitude entry in $U$ is $|u_{33}| = \\frac{900001000}{499}$.\nThe maximum magnitude entry in the original matrix $A$ is $|a_{12}| = 10^6$.\nThe growth factor is $\\rho_{\\text{CALU}} = \\frac{\\max_{i,j} |u_{ij}|}{\\max_{i,j} |a_{ij}|}$.\n$$\n\\rho_{\\text{CALU}} = \\frac{\\frac{900001000}{499}}{10^6} = \\frac{900001000}{499 \\times 10^6} = \\frac{900.001}{499} = \\frac{900001}{499000}\n$$\nThis fraction is irreducible as 499 is a prime number and does not divide 900001.\n\n**Task 4: Backward Stability of CALU**\n\nBackward stability for an algorithm solving $Ax=b$ means the computed solution $\\hat{x}$ is the exact solution to a perturbed problem, $(A + \\Delta A)\\hat{x} = b$, where the perturbation $\\Delta A$ is small. Quantitatively, this means the norm of the perturbation is bounded, e.g., $\\|\\Delta A\\| \\le C \\cdot u \\cdot \\text{poly}(n) \\cdot \\|A\\|$, where $u$ is the machine precision.\n\nFor LU factorization-based solvers, the standard backward error bound is directly proportional to the growth factor $\\rho$. A common form of the bound on the backward error matrix $\\Delta A$ is $\\|\\Delta A\\| / \\|A\\| \\le \\gamma_n \\rho u$, where $\\gamma_n$ is a low-degree polynomial in the matrix dimension $n$. For an algorithm to be backward stable, the growth factor $\\rho$ must be of a modest size (i.e., not growing exponentially with $n$).\n\nThe panel and tournament structure of CALU is designed to reduce communication on parallel computers by restricting the search for pivots. Unlike GEPP, which searches the entire remaining column, tournament pivoting only compares a small number of candidates, one from each domain. This means the algorithm can fail to select the globally largest pivot available.\n\nAs this problem demonstrates, CALU's tournament pivot for column 2 was $-998$, ignoring the much larger available entry of $-900001$ in a different domain. This suboptimal pivot choice led to a large multiplier, $l_{32} = \\frac{900001}{998} \\gg 1$. This large multiplier, in turn, caused significant element growth when updating other entries, creating the large entry $u_{33} \\approx -1.8 \\times 10^6$. While the resulting growth factor $\\rho_{\\text{CALU}} \\approx 1.8$ is still small, this example illustrates the mechanism by which growth can occur.\n\nCALU with tournament pivoting remains backward stable as long as this trade-off between reduced communication and pivot quality does not lead to large growth factors. Its stability relies on the assumption that the local pivots selected within each domain are also reasonably good global pivots. If a matrix's numerical properties interact poorly with the fixed domain structure, causing large potential pivots to be systematically ignored, $\\rho$ can become large. This would inflate the backward error bound $\\gamma_n \\rho u$, potentially violating the condition for backward stability.", "answer": "$$\\boxed{\\frac{900001}{499000}}$$", "id": "3591199"}, {"introduction": "In the realm of large-scale scientific computing, matrices are often sparse, meaning most of their entries are zero. For LU factorization, there is a fundamental tension between maintaining numerical stability and preserving this sparsity. Aggressive pivoting for stability can lead to catastrophic 'fill-in,' destroying the sparse structure and negating performance gains. This coding practice explores threshold pivoting, a widely used strategy that navigates this trade-off [@problem_id:3591254]. By constructing a matrix specifically designed to challenge pivoting choices, you will quantify how a tunable threshold parameter, $\\tau$, can balance the competing demands of low fill-in and controlled element growth.", "problem": "Consider the construction, factorization, and analysis of sparse matrices under different pivoting strategies within the Lower-Upper (LU) factorization framework. The foundational base for this problem is the definition of LU factorization of a nonsingular matrix $A \\in \\mathbb{R}^{n \\times n}$ into a unit lower-triangular factor $L$ and an upper-triangular factor $U$, together with partial pivoting, which selects at each step the largest magnitude entry in the current column as the pivot, and threshold pivoting, which accepts the diagonal entry as the pivot if it satisfies a magnitude threshold condition relative to the largest entry in the column.\n\nYou must algorithmically construct a family of sparse matrices $A$ using the following principle. Begin with a banded matrix that has $1$-entries on the main diagonal and small entries on the first sub- and super-diagonals to ensure nonsingularity and numerical stability. Then, inject large-magnitude “rogue” entries far from the diagonal in carefully chosen columns to force strict partial pivoting to select off-diagonal pivots and thereby trigger substantial fill-in by row interchanges, whereas threshold pivoting with a suitable threshold parameter $\\tau$ can preserve sparsity by favoring diagonal pivots when they are of acceptable magnitude.\n\nMatrix construction requirements:\n- For given $n \\in \\mathbb{N}$, construct $A \\in \\mathbb{R}^{n \\times n}$ with the pattern\n  - $A_{i,i} = 1$ for all $i$, which should be implemented as entries of magnitude $1$ on the diagonal,\n  - $A_{i,i+1} = \\epsilon$ and $A_{i+1,i} = \\epsilon$ for all valid $i$, which should be implemented as a tridiagonal band with small off-diagonal magnitude $\\epsilon$,\n  - For a specified set of columns $J \\subset \\{0,1,\\dots,n-1\\}$ and a mapping $r: J \\to \\{0,1,\\dots,n-1\\}$ with $r(j)$ far from $j$, set $A_{r(j),\\, j} = M$, where $M$ is a large magnitude relative to $1$ and $\\epsilon$.\n\nPivoting strategies to be executed:\n- Strict partial pivoting: This corresponds to threshold pivoting with parameter $\\tau = 1$, meaning the diagonal is chosen as pivot only if it equals the column maximum in magnitude; otherwise, the largest off-diagonal entry is chosen.\n- Threshold pivoting: Use a given $\\tau \\in [0,1]$ that accepts the diagonal as the pivot if its magnitude is at least $\\tau$ times the largest magnitude entry in the column; otherwise, use the largest magnitude entry.\n\nFor each matrix, perform LU factorization under both strategies using natural column ordering (that is, no fill-reducing preordering). From the factors, compute:\n- The fill ratio $f := \\dfrac{\\operatorname{nnz}(L) + \\operatorname{nnz}(U)}{\\operatorname{nnz}(A)}$, where $\\operatorname{nnz}(\\cdot)$ denotes the number of nonzero entries.\n- The element growth factor $\\rho := \\dfrac{\\max_{i,j} |U_{i,j}|}{\\max_{i,j} |A_{i,j}|}$, which quantifies the largest magnitude increase between the original matrix and the upper-triangular factor.\n\nYour program must, for each test case, return two boolean results:\n- Does threshold pivoting reduce fill compared to strict partial pivoting? That is, is $f_{\\text{threshold}} < f_{\\text{partial}}$?\n- Is the growth under threshold pivoting acceptable? That is, is $\\rho_{\\text{threshold}} \\le g$, for a specified bound $g > 0$?\n\nImplementation constraints:\n- Use Compressed Sparse Column (CSC) storage for $A$ and perform LU factorization with natural ordering. The factorization must respect the specified pivoting threshold $\\tau$.\n\nTest suite:\n- Case $1$ (general “happy path”): $n = 50$, $\\epsilon = 0.05$, $J = \\{0,1,2,3,4,5,6,7,8,9\\}$, $r(j) = 49 - j$, $M = 100$, $\\tau = 0.01$, $g = 500$.\n- Case $2$ (boundary for strict partial pivoting): $n = 50$, $\\epsilon = 0.05$, $J = \\{0,1,2,3,4,5,6,7,8,9\\}$, $r(j) = 49 - j$, $M = 100$, $\\tau = 1.0$, $g = 500$.\n- Case $3$ (smaller dimension, moderate rogue entries): $n = 12$, $\\epsilon = 0.02$, $J = \\{0,1,2,3,4,5\\}$, $r(j) = 11 - j$, $M = 50$, $\\tau = 0.05$, $g = 250$.\n- Case $4$ (edge case of always-diagonal pivoting): $n = 20$, $\\epsilon = 0.02$, $J = \\{0,1,2,3,4,5,6,7\\}$, $r(j) = 19 - j$, $M = 500$, $\\tau = 0.0$, $g = 5000$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case contributes a two-element list of booleans in the order described above. For example, the output must look like $[\\,[b_{1}^{(1)},b_{2}^{(1)}],\\,[b_{1}^{(2)},b_{2}^{(2)}],\\,\\dots\\,]$, with no additional text.", "solution": "This problem explores the fundamental trade-off between preserving sparsity and maintaining numerical stability during the LU factorization of sparse matrices. This is achieved by comparing two pivoting strategies: strict partial pivoting and threshold pivoting.\n\n### Pivoting Strategies and their Consequences\n\nThe LU factorization of a matrix $A$ decomposes it into a product $PA=LU$, where $P$ is a permutation matrix encoding row swaps, $L$ is unit lower-triangular, and $U$ is upper-triangular. For sparse matrices, the goal is to keep the factors $L$ and $U$ as sparse as possible to save memory and computational cost. The introduction of new non-zero entries during factorization is called \"fill-in\".\n\n1.  **Strict Partial Pivoting ($\\tau=1$)**: At each step $k$ of Gaussian elimination, this strategy selects the entry with the largest magnitude in the current column (on or below the diagonal) as the pivot. This guarantees numerical stability by avoiding small pivots and bounding the growth of elements, but the required row interchanges can destroy the matrix's sparse structure and cause significant fill-in.\n\n2.  **Threshold Pivoting ($\\tau \\in [0, 1]$)**: This is a compromise strategy. At step $k$, it finds the largest magnitude entry in the current column, $|A_{p,k}^{(k)}|$. It then accepts the diagonal entry $A_{k,k}^{(k)}$ as the pivot if it is \"large enough,\" specifically if $|A_{k,k}^{(k)}| \\ge \\tau \\cdot |A_{p,k}^{(k)}|$. If this condition holds, no row swap is performed, thus preserving sparsity. Otherwise, the larger off-diagonal pivot is chosen. A small $\\tau$ favors preserving sparsity at the risk of using smaller pivots, which could lead to numerical instability.\n\n### Matrix Design and Analysis\n\nThe problem uses a specially constructed matrix to highlight this trade-off. The matrix has a simple tridiagonal structure with $1$ on the diagonal and small values $\\epsilon$ on the off-diagonals. Large \"rogue\" entries $M$ are inserted far from the diagonal. For a column $j$ with a rogue entry, partial pivoting is forced to choose $M$ as the pivot (since $1  1.0 \\cdot M$), causing a distant row swap and massive fill-in. Threshold pivoting, if the condition $1 \\ge \\tau M$ holds, can instead choose the diagonal '1' as the pivot, avoiding the swap and preserving sparsity.\n\nTwo metrics are used to quantify the results:\n-   **Fill Ratio ($f$)**: Defined as $\\frac{\\operatorname{nnz}(L) + \\operatorname{nnz}(U)}{\\operatorname{nnz}(A)}$, it measures the increase in non-zero elements. A lower value is better.\n-   **Growth Factor ($\\rho$)**: Defined as $\\frac{\\max_{i,j} |U_{i,j}|}{\\max_{i,j} |A_{i,j}|}$, it measures the growth of element magnitudes during factorization, a key indicator of numerical stability. A smaller value is better.\n\n### Implementation\n\nThe solution involves implementing a program that, for each test case:\n1.  Constructs the specified sparse matrix $A$.\n2.  Computes the LU factorization of $A$ twice: once with strict partial pivoting ($\\tau=1$) and once with the given threshold $\\tau$.\n3.  Calculates the fill ratio ($f$) for both factorizations and the growth factor ($\\rho$) for the threshold case.\n4.  Evaluates two boolean conditions: whether threshold pivoting reduces fill-in ($f_{\\text{threshold}}  f_{\\text{partial}}$) and whether the resulting element growth is acceptable ($\\rho_{\\text{threshold}} \\le g$).\n\nThe following Python code implements this procedure using the `scipy.sparse` library.\n\n```python\nimport numpy as np\nimport scipy.sparse as sp\nfrom scipy.sparse.linalg import splu\n\ndef run_factorization_analysis(n, epsilon, J, r_map, M, tau_thresh, g):\n    \"\"\"\n    Constructs a sparse matrix and analyzes its LU factorization under two\n    pivoting strategies.\n\n    Args:\n        n (int): Dimension of the matrix.\n        epsilon (float): Magnitude of off-diagonal entries in the tridiagonal band.\n        J (set): Set of column indices for rogue entries.\n        r_map (callable): Function mapping a column index j to a row index r(j).\n        M (float): Magnitude of the rogue entries.\n        tau_thresh (float): Threshold for threshold pivoting.\n        g (float): Growth factor bound.\n\n    Returns:\n        list[bool, bool]: A list of two booleans:\n                           [reduces_fill, acceptable_growth]\n    \"\"\"\n    # 1. Construct the matrix A using LIL format for efficient construction.\n    A = sp.lil_matrix((n, n), dtype=np.float64)\n\n    # Add tridiagonal band\n    for i in range(n):\n        A[i, i] = 1.0\n        if i  n - 1:\n            A[i, i + 1] = epsilon\n            A[i + 1, i] = epsilon\n    \n    # Add rogue entries\n    for j in J:\n        A[r_map(j), j] = M\n\n    # Convert to CSC format for factorization\n    A_csc = A.tocsc()\n    nnz_A = A_csc.nnz\n    max_A = M  # Max magnitude in A is M by construction\n\n    # 2. Factorize with strict partial pivoting (tau = 1.0)\n    # The 'NATURAL' permutation spec ensures no column reordering.\n    lu_partial = splu(A_csc, permc_spec='NATURAL', diag_pivot_thresh=1.0)\n    nnz_partial = lu_partial.L.nnz + lu_partial.U.nnz\n    f_partial = nnz_partial / nnz_A\n\n    # 3. Factorize with threshold pivoting (given tau)\n    lu_threshold = splu(A_csc, permc_spec='NATURAL', diag_pivot_thresh=tau_thresh)\n    nnz_threshold = lu_threshold.L.nnz + lu_threshold.U.nnz\n    f_threshold = nnz_threshold / nnz_A\n    \n    # Find max element in U for growth factor calculation.\n    max_U_thresh = 0.0\n    if lu_threshold.U.data.size > 0:\n        max_U_thresh = np.max(np.abs(lu_threshold.U.data))\n    \n    rho_threshold = max_U_thresh / max_A\n\n    # 4. Evaluate the boolean conditions\n    reduces_fill = f_threshold  f_partial\n    acceptable_growth = rho_threshold = g\n\n    return [reduces_fill, acceptable_growth]\n\ndef solve():\n    \"\"\"\n    Defines and runs the test suite for the LU factorization problem.\n    \"\"\"\n    test_cases = [\n        # Case 1: General \"happy path\"\n        {'n': 50, 'epsilon': 0.05, 'J': set(range(10)), 'r_map': lambda j: 49 - j, \n         'M': 100, 'tau': 0.01, 'g': 500},\n        # Case 2: Boundary for strict partial pivoting\n        {'n': 50, 'epsilon': 0.05, 'J': set(range(10)), 'r_map': lambda j: 49 - j, \n         'M': 100, 'tau': 1.0, 'g': 500},\n        # Case 3: Smaller dimension, moderate rogue entries, threshold too high\n        {'n': 12, 'epsilon': 0.02, 'J': set(range(6)), 'r_map': lambda j: 11 - j, \n         'M': 50, 'tau': 0.05, 'g': 250},\n        # Case 4: Edge case of always-diagonal pivoting (no pivoting)\n        {'n': 20, 'epsilon': 0.02, 'J': set(range(8)), 'r_map': lambda j: 19 - j, \n         'M': 500, 'tau': 0.0, 'g': 5000},\n    ]\n\n    results = []\n    for case in test_cases:\n        result = run_factorization_analysis(\n            n=case['n'],\n            epsilon=case['epsilon'],\n            J=case['J'],\n            r_map=case['r_map'],\n            M=case['M'],\n            tau_thresh=case['tau'],\n            g=case['g']\n        )\n        results.append(result)\n\n    # Format the final output string exactly as required.\n    formatted_results = []\n    for res_pair in results:\n        s = f\"[{str(res_pair[0]).lower()},{str(res_pair[1]).lower()}]\"\n        formatted_results.append(s)\n    \n    final_string = f\"[{','.join(formatted_results)}]\"\n    # This print statement would be executed to generate the answer.\n    # print(final_string)\n\n# The following is the expected output of the solve() function.\n```", "answer": "[[true,true],[false,true],[false,true],[true,true]]", "id": "3591254"}, {"introduction": "While partial pivoting is often sufficient, truly challenging, ill-scaled matrices demand more robust pivoting strategies and a more nuanced analysis of backward error. This advanced exercise introduces Rook and Complete Pivoting, which offer superior stability guarantees at a higher computational cost. Using the notoriously difficult scaled Kahan matrix, you will investigate the subtle but critical distinction between normwise and componentwise backward error [@problem_id:3591252]. This practice will demonstrate that an algorithm can be stable in an 'average' normwise sense yet produce results with large relative errors in specific components, a vital lesson in high-precision scientific applications.", "problem": "Consider the Lower-Upper (LU) factorization with row and column permutations applied to a family of scaled Kahan-type matrices. For an integer size $n \\geq 2$, a parameter $\\sigma \\in (0,1)$, and a column scaling parameter $s \\in (0,1]$, define the unscaled Kahan-type matrix $B \\in \\mathbb{R}^{n \\times n}$ by\n$$\nb_{ij} = \\begin{cases}\n1,  i \\leq j, \\\\\n-\\sigma,  i  j,\n\\end{cases}\n$$\nand define the diagonal column scaling matrix $S \\in \\mathbb{R}^{n \\times n}$ by\n$$\nS = \\mathrm{diag}(1, s, s^2, \\dots, s^{n-1}).\n$$\nThe scaled Kahan matrix is\n$$\nA = B S,\n$$\nso that the $j$-th column of $A$ equals the $j$-th column of $B$ multiplied by $s^{j-1}$. Note that all entries of $A$ are nonzero for $\\sigma \\in (0,1)$ and $s \\in (0,1]$.\n\nFor pivoting strategies, define the following three algorithms for Gaussian elimination:\n- Partial Pivoting (PP): at step $k$, select the pivot with maximal absolute value in column $k$ among rows $k,k+1,\\dots,n$; perform a row interchange and then eliminate below.\n- Complete Pivoting (CP): at step $k$, select the pivot with maximal absolute value in the submatrix defined by rows and columns $k,k+1,\\dots,n$; perform both a row and a column interchange and then eliminate below.\n- Rook Pivoting (RP): at step $k$, start with column $k$, find the row $p$ of the maximal absolute value in that column among rows $k,\\dots,n$, then in that row find the column $q$ of the maximal absolute value among columns $k,\\dots,n$; if the element at $(p,q)$ is also maximal in column $q$, accept it as pivot; otherwise iterate the row/column search until an element is simultaneously maximal in its current row and column in the trailing submatrix, then perform corresponding row and column interchanges and eliminate below.\n\nLet $P \\in \\mathbb{R}^{n \\times n}$ and $Q \\in \\mathbb{R}^{n \\times n}$ be permutation matrices such that, for the chosen strategy, the computed factors satisfy\n$$\nP A Q = L U,\n$$\nwhere $L \\in \\mathbb{R}^{n \\times n}$ is unit lower triangular and $U \\in \\mathbb{R}^{n \\times n}$ is upper triangular. The backward error of the factorization is measured by the perturbation\n$$\n\\Delta A = P^\\top L U Q^\\top - A,\n$$\nwhich quantifies the discrepancy of the computed factors from an exact factorization of the original matrix $A$.\n\nDefine the normwise relative backward error by the Frobenius norm\n$$\n\\beta_{\\mathrm{norm}} = \\frac{\\lVert \\Delta A \\rVert_F}{\\lVert A \\rVert_F},\n$$\nand define the componentwise relative backward error by\n$$\n\\beta_{\\mathrm{comp}} = \\max_{1 \\leq i,j \\leq n} \\frac{|\\Delta A_{ij}|}{|A_{ij}|}.\n$$\nAssume arithmetic in double precision floating point with machine epsilon $\\epsilon$, and define the growth factor estimate\n$$\ng = \\frac{\\max_{i,j} |U_{ij}|}{\\max_{i,j} |A_{ij}|}.\n$$\n\nTask:\n1. Implement LU factorization with the three pivot strategies (PP, CP, RP) and compute $P$, $L$, $U$, and $Q$ for a given input matrix $A$.\n2. For each factorization, compute $\\beta_{\\mathrm{norm}}$, $\\beta_{\\mathrm{comp}}$, and $g$ as defined above.\n3. Using a principled predictor based on floating point rounding error propagation, predict whether the componentwise error dominates despite a small normwise error, according to the criterion\n$$\n\\text{predict dominance} \\iff \\left(\\beta_{\\mathrm{norm}}  10^{-12}\\right) \\;\\text{and}\\; \\left(\\epsilon \\, g \\, \\frac{\\lVert A \\rVert_F}{\\min_{i,j} |A_{ij}|}  10^{-6}\\right).\n$$\nThis predictor models the fact that rounding errors of size $\\mathcal{O}(\\epsilon)$ scaled by the growth factor $g$ can produce entries in $\\Delta A$ whose relative effect compared to the smallest entries of $A$ is disproportionately large, even when the overall normwise error is tiny.\n\n4. Compare the normwise and componentwise backward errors across pivot strategies and matrices, and report for each case whether the predictor flags componentwise dominance.\n\nDesign your program to run the following test suite of $(n, \\sigma, s)$:\n- Case 1 (baseline, no column scaling): $n = 10$, $\\sigma = 0.99$, $s = 1$.\n- Case 2 (moderate scaling): $n = 10$, $\\sigma = 0.99$, $s = 10^{-4}$.\n- Case 3 (extreme scaling): $n = 12$, $\\sigma = 0.999$, $s = 10^{-8}$.\n\nFor each case, evaluate all three pivot strategies (PP, RP, CP) and output the nine predictions (three strategies times three cases) as booleans in the following order:\n$$\n[\\text{Case 1 PP}, \\text{Case 1 RP}, \\text{Case 1 CP}, \\text{Case 2 PP}, \\text{Case 2 RP}, \\text{Case 2 CP}, \\text{Case 3 PP}, \\text{Case 3 RP}, \\text{Case 3 CP}].\n$$\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[{\\tt result1},{\\tt result2},\\dots,{\\tt result9}]$). No physical units are involved; all quantities are dimensionless. Angles are not part of this computation. All outputs are booleans and must be printed exactly in Python's boolean textual form.", "solution": "This problem delves into the numerical stability of LU factorization on ill-scaled matrices, comparing three pivoting strategies and analyzing the resulting backward error from both a normwise and componentwise perspective.\n\n### 1. Matrix Construction and Scaling\n\nThe problem uses a scaled Kahan-type matrix, $A=BS$. The unscaled matrix $B$ is nearly upper-triangular but has negative entries below the diagonal. The diagonal scaling matrix $S$ multiplies successive columns by decreasing powers of $s$. When $s  1$, this creates an ill-scaled matrix where columns have vastly different norms. This poor scaling is a challenge for numerical algorithms, as floating-point errors that are small in an absolute sense can be large relative to the small entries in the matrix.\n\n### 2. Pivoting Strategies\n\nThe core of the problem is implementing Gaussian elimination with three different pivoting strategies to compute the factorization $PAQ=LU$.\n\n-   **Partial Pivoting (PP)**: This standard strategy searches for the largest pivot in the current column only. It is computationally cheap but may not be sufficient for severely ill-scaled matrices. For this strategy, no column permutations occur, so $Q$ is the identity matrix.\n-   **Complete Pivoting (CP)**: This strategy searches the entire remaining submatrix for the largest pivot. It is the most numerically stable strategy in terms of bounding element growth but has a high computational overhead, making it rare in practice.\n-   **Rook Pivoting (RP)**: This is an intermediate strategy that offers better stability than PP with less cost than CP. It searches for a pivot that is the largest element in both its row and its column within the submatrix, which often finds a better pivot than PP without searching the entire submatrix.\n\n### 3. Error Analysis and Prediction\n\nAfter factorization, the analysis focuses on the backward error, $\\Delta A = P^\\top L U Q^\\top - A$, which measures how much the original matrix must be perturbed to make the computed factorization exact. We analyze this error in two ways:\n-   **Normwise Relative Error ($\\beta_{\\mathrm{norm}}$)**: Measures the overall size of the error relative to the size of the matrix. A small value indicates that the algorithm is stable on average.\n-   **Componentwise Relative Error ($\\beta_{\\mathrm{comp}}$)**: Measures the maximum relative error in any single entry of the matrix. This can be large even when $\\beta_{\\mathrm{norm}}$ is small, especially for ill-scaled matrices.\n\nThe problem provides a predictive criterion to identify when this divergence is likely to occur. Dominance of componentwise error is predicted if the algorithm is normwise stable (small $\\beta_{\\mathrm{norm}}$) but the matrix is poorly scaled. The term $\\frac{\\lVert A \\rVert_F}{\\min_{i,j} |A_{ij}|}$ in the predictor acts as a measure of this poor scaling.\n\nThe following Python code implements these steps to run the test suite and evaluate the predictor for each case.\n\n```python\nimport numpy as np\n\ndef build_kahan_matrix(n: int, sigma: float, s: float) -> np.ndarray:\n    \"\"\"\n    Constructs the scaled Kahan-type matrix A.\n    \"\"\"\n    B = np.ones((n, n), dtype=np.float64)\n    rows, cols = np.tril_indices(n, k=-1)\n    B[rows, cols] = -sigma\n    \n    s_powers = np.power(s, np.arange(n, dtype=np.float64))\n    A = B * s_powers\n    return A\n\ndef lu_factor(A: np.ndarray, strategy: str) -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"\n    Performs LU factorization with pivoting on matrix A in-place.\n    \"\"\"\n    n = A.shape[0]\n    p_vec = np.arange(n)\n    q_vec = np.arange(n)\n\n    for k in range(n):\n        # --- Pivoting ---\n        if strategy == 'PP':\n            sub_col = A[k:, k]\n            r_sub = np.argmax(np.abs(sub_col))\n            r, c = k + r_sub, k\n        elif strategy == 'CP':\n            sub_matrix = A[k:, k:]\n            r_sub, c_sub = np.unravel_index(np.argmax(np.abs(sub_matrix)), sub_matrix.shape)\n            r, c = k + r_sub, k + c_sub\n        elif strategy == 'RP':\n            r_search, c_search = k, k\n            while True:\n                r_current = k + np.argmax(np.abs(A[k:, c_search]))\n                c_current = k + np.argmax(np.abs(A[r_current, k:]))\n                if c_current == c_search:\n                    r, c = r_current, c_current\n                    break\n                else:\n                    c_search = c_current\n        else:\n            raise ValueError(f\"Unknown pivot strategy: {strategy}\")\n\n        # --- Permutations ---\n        if k != r:\n            A[[k, r], :] = A[[r, k], :]\n            p_vec[[k, r]] = p_vec[[r, k]]\n        \n        if k != c:\n            A[:, [k, c]] = A[:, [c, k]]\n            q_vec[[k, c]] = q_vec[[c, k]]\n\n        # --- Elimination ---\n        if k  n - 1:\n            pivot_val = A[k, k]\n            if np.abs(pivot_val)  1e-30: continue\n            multipliers = A[k+1:, k] / pivot_val\n            A[k+1:, k] = multipliers\n            A[k+1:, k+1:] -= np.outer(multipliers, A[k, k+1:])\n            \n    return A, p_vec, q_vec\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print the results.\n    \"\"\"\n    test_cases = [\n        (10, 0.99, 1.0),\n        (10, 0.99, 1e-4),\n        (12, 0.999, 1e-8),\n    ]\n    pivot_strategies = ['PP', 'RP', 'CP']\n    \n    results = []\n    epsilon = np.finfo(np.float64).eps\n\n    for n, sigma, s in test_cases:\n        for strategy in pivot_strategies:\n            A_orig = build_kahan_matrix(n, sigma, s)\n            LU_factors, p_vec, q_vec = lu_factor(A_orig.copy(), strategy)\n            \n            L = np.tril(LU_factors, -1) + np.eye(n)\n            U = np.triu(LU_factors)\n\n            lu_prod = L @ U\n            A_recon = np.zeros_like(A_orig)\n            A_recon_perm_rows = np.zeros_like(A_orig)\n            A_recon_perm_rows[:, q_vec] = lu_prod\n            A_recon[p_vec, :] = A_recon_perm_rows\n\n            delta_A = A_recon - A_orig\n\n            norm_A = np.linalg.norm(A_orig, 'fro')\n            norm_delta_A = np.linalg.norm(delta_A, 'fro')\n            beta_norm = norm_delta_A / norm_A if norm_A > 0 else 0.0\n\n            max_A = np.max(np.abs(A_orig))\n            max_U = np.max(np.abs(U))\n            g = max_U / max_A if max_A > 0 else 0.0\n\n            min_abs_A = np.min(np.abs(A_orig))\n            \n            if min_abs_A > 0:\n                predictor_val = epsilon * g * norm_A / min_abs_A\n            else:\n                predictor_val = float('inf')\n            \n            prediction = (beta_norm  1e-12) and (predictor_val > 1e-6)\n            results.append(prediction)\n\n    # This print statement would be executed to generate the answer.\n    # print(f\"[{','.join(map(str, results))}]\")\n\n# The following is the expected output of the solve() function.\n```", "answer": "[False,False,False,True,True,True,True,True,True]", "id": "3591252"}]}