{"hands_on_practices": [{"introduction": "The cornerstone of a Physics-Informed Neural Network (PINN) is its ability to embed physical laws directly into the learning process by penalizing deviations from a governing partial differential equation (PDE). This exercise provides hands-on practice in computing this deviation, known as the *physics residual*, for a given neural network ansatz. By applying the chain rule to differentiate the network's output—the core principle of automatic differentiation—you will learn to quantify how well the network satisfies the PDE, a fundamental skill for building and debugging PINNs [@problem_id:3513283].", "problem": "Consider the one-dimensional viscous Burgers equation, a canonical model in fluid mechanics derived from conservation of momentum with diffusive transport, given by $u_t + u u_x - \\nu u_{xx} = 0$ for a scalar field $u(x,t)$ on a space-time domain. Let the kinematic viscosity be fixed at $\\nu = 0.01/\\pi$. The goal is to evaluate the physics residual $r(x,t)$ for a given network ansatz and a set of collocation points, using programmatic differentiation consistent with the computational graph (automatic differentiation), and to summarize the residual statistics across the collocation set.\n\nYou must start from the governing partial differential equation and the chain rule of calculus, and treat the network ansatz as a differentiable surrogate. The residual is defined as $r(x,t) = u_t(x,t) + u(x,t)\\, u_x(x,t) - \\nu\\, u_{xx}(x,t)$. The network ansatz is specified as a single-hidden-layer form with hyperbolic tangent activation:\n$$\nu(x,t) = \\sum_{j=1}^{m} a_j\\, \\phi(z_j(x,t)), \\quad \\phi(z) = \\tanh(z), \\quad z_j(x,t) = b_j x + c_j t + d_j,\n$$\nwhere $m$ is the number of neurons, and $a_j$, $b_j$, $c_j$, $d_j$ are fixed parameters.\n\nYou must implement the residual computation by evaluating $u(x,t)$ and its derivatives $u_x(x,t)$, $u_t(x,t)$, and $u_{xx}(x,t)$ via the chain rule using the computational graph defined by the ansatz (that is, using automatic differentiation conceptually, but expressed explicitly in code). The residual norm at a point is the absolute value $|r(x,t)|$.\n\nDefine three test cases with distinct collocation point sets and network parameterizations, all on the domain $x \\in [0,1]$ and $t \\in [0,1]$:\n\n- Test Case $1$ (general interior sampling, \"happy path\"):\n  - Neuron count $m = 5$ with parameters:\n    - $a = [1.0, -0.5, 0.3, 0.7, -0.2]$,\n    - $b = [2.0, -1.0, 0.5, 3.0, -2.5]$,\n    - $c = [-1.5, 0.7, 1.2, -0.8, 0.9]$,\n    - $d = [0.1, -0.2, 0.3, -0.4, 0.5]$.\n  - Collocation points: $N=1024$ points $(x_i,t_i)$ sampled independently and uniformly from $[0,1]\\times[0,1]$ using a fixed pseudo-random seed $123$ for reproducibility. That is, draw $x_i \\sim \\mathcal{U}(0,1)$ and $t_i \\sim \\mathcal{U}(0,1)$ independently for $i=1,\\dots,1024$.\n\n- Test Case $2$ (boundary-focused sampling):\n  - Neuron count $m = 3$ with parameters:\n    - $a = [0.8, -0.6, 0.4]$,\n    - $b = [5.0, -4.0, 3.0]$,\n    - $c = [2.0, -1.0, 0.5]$,\n    - $d = [-0.1, 0.2, -0.3]$.\n  - Collocation points: choose $K=64$ equally spaced points along each boundary edge, including endpoints. Specifically, assemble the set of points by concatenating the four edge sets\n    - bottom edge: $(x,t)=(x_k, 0)$ with $x_k$ from $\\text{linspace}(0,1,K)$,\n    - top edge: $(x,t)=(x_k, 1)$ with $x_k$ from $\\text{linspace}(0,1,K)$,\n    - left edge: $(x,t)=(0, t_k)$ with $t_k$ from $\\text{linspace}(0,1,K)$,\n    - right edge: $(x,t)=(1, t_k)$ with $t_k$ from $\\text{linspace}(0,1,K)$.\n    This yields $N=4K=256$ collocation points (corners will appear in multiple edge sets, which is acceptable).\n\n- Test Case $3$ (stratified interior sampling with small-amplitude network, edge case):\n  - Neuron count $m = 4$ with parameters:\n    - $a = [0.05, -0.03, 0.02, -0.04]$,\n    - $b = [8.0, -7.0, 6.0, -5.0]$,\n    - $c = [4.0, -3.0, 2.0, -1.0]$,\n    - $d = [0.0, 0.1, -0.1, 0.2]$.\n  - Collocation points: construct a stratified grid of $Q=16$ subdivisions per axis; use the centers of the subcells. Concretely, for $i=0,\\dots,15$ and $j=0,\\dots,15$, define the point $(x_{ij}, t_{ij})$ as $x_{ij} = (i+0.5)/Q$ and $t_{ij} = (j+0.5)/Q$. This yields $N=Q^2=256$ points.\n\nFor each test case:\n- Compute the residual $r(x,t)$ at all collocation points.\n- Compute the residual norms $|r(x,t)|$ at those points.\n- Report two floats: the mean of $|r|$ over the collocation points and the maximum of $|r|$ over the collocation points.\n\nAll intermediate calculations must adhere to the above definitions, and the final result must aggregate the outcomes from the three test cases.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The required format is $[m_1,M_1,m_2,M_2,m_3,M_3]$, where $m_i$ is the mean residual norm for Test Case $i$ and $M_i$ is the maximum residual norm for Test Case $i$. No physical units are involved; all quantities are dimensionless real numbers.", "solution": "The problem requires the evaluation of the physics residual for the one-dimensional viscous Burgers' equation, given a specific neural network ansatz for the solution field $u(x,t)$. The evaluation must be performed for three distinct test cases, each with its own network parameterization and set of collocation points. The core of the task is to compute the necessary partial derivatives of the network output with respect to its inputs, $x$ and $t$, by explicitly applying the chain rule of calculus, which is the fundamental principle behind automatic differentiation.\n\nThe governing partial differential equation (PDE) is the viscous Burgers' equation:\n$$\nu_t + u u_x - \\nu u_{xx} = 0\n$$\nwhere $u_t = \\frac{\\partial u}{\\partial t}$, $u_x = \\frac{\\partial u}{\\partial x}$, and $u_{xx} = \\frac{\\partial^2 u}{\\partial x^2}$. The kinematic viscosity is given as a constant, $\\nu = 0.01/\\pi$.\n\nThe physics residual, denoted by $r(x,t)$, is the value obtained by substituting the network ansatz into the PDE. It quantifies how well the ansatz satisfies the governing equation at any given point $(x,t)$:\n$$\nr(x,t) = u_t(x,t) + u(x,t)\\, u_x(x,t) - \\nu\\, u_{xx}(x,t)\n$$\n\nThe network ansatz is a single-hidden-layer neural network with a hyperbolic tangent activation function $\\phi(z) = \\tanh(z)$. The output $u(x,t)$ is a linear combination of the activations of $m$ neurons:\n$$\nu(x,t) = \\sum_{j=1}^{m} a_j\\, \\phi(z_j(x,t))\n$$\nThe input to each neuron, $z_j$, is an affine transformation of the spatial and temporal coordinates:\n$$\nz_j(x,t) = b_j x + c_j t + d_j\n$$\nThe parameters $a_j, b_j, c_j, d_j$ for $j=1, \\dots, m$ are provided and fixed for each test case.\n\nTo compute the residual $r(x,t)$, we must find the partial derivatives $u_t$, $u_x$, and $u_{xx}$ of the network ansatz. This is achieved by applying the chain rule through the computational graph defined by the ansatz.\n\nFirst, we determine the derivatives of the activation function $\\phi(z) = \\tanh(z)$ with respect to its argument $z$:\n- First derivative: $\\phi'(z) = \\frac{d}{dz} \\tanh(z) = \\text{sech}^2(z) = 1 - \\tanh^2(z)$. Using our notation, this is $\\phi'(z) = 1 - \\phi(z)^2$.\n- Second derivative: $\\phi''(z) = \\frac{d}{dz} (1 - \\tanh^2(z)) = -2 \\tanh(z) \\cdot \\text{sech}^2(z) = -2\\phi(z)(1-\\phi(z)^2)$. Using our notation, this is $\\phi''(z) = -2\\phi(z)\\phi'(z)$.\n\nNext, we compute the partial derivatives of the network output $u(x,t)$. Since differentiation is a linear operator, we can differentiate the sum term by term.\n\n- Partial derivative with respect to time, $u_t$:\n$$\nu_t(x,t) = \\frac{\\partial}{\\partial t} \\sum_{j=1}^{m} a_j \\phi(z_j) = \\sum_{j=1}^{m} a_j \\frac{d\\phi}{dz_j} \\frac{\\partial z_j}{\\partial t} = \\sum_{j=1}^{m} a_j \\phi'(z_j) c_j\n$$\n\n- Partial derivative with respect to space, $u_x$:\n$$\nu_x(x,t) = \\frac{\\partial}{\\partial x} \\sum_{j=1}^{m} a_j \\phi(z_j) = \\sum_{j=1}^{m} a_j \\frac{d\\phi}{dz_j} \\frac{\\partial z_j}{\\partial x} = \\sum_{j=1}^{m} a_j \\phi'(z_j) b_j\n$$\n\n- Second partial derivative with respect to space, $u_{xx}$:\n$$\nu_{xx}(x,t) = \\frac{\\partial}{\\partial x} u_x(x,t) = \\frac{\\partial}{\\partial x} \\sum_{j=1}^{m} a_j b_j \\phi'(z_j) = \\sum_{j=1}^{m} a_j b_j \\frac{d\\phi'}{dz_j} \\frac{\\partial z_j}{\\partial x} = \\sum_{j=1}^{m} a_j b_j \\phi''(z_j) b_j = \\sum_{j=1}^{m} a_j b_j^2 \\phi''(z_j)\n$$\n\nSubstituting these expressions into the residual definition gives the complete formula for the residual at any point $(x,t)$:\n$$\nr(x,t) = \\left(\\sum_{j=1}^{m} a_j c_j \\phi'(z_j)\\right) + \\left(\\sum_{j=1}^{m} a_j \\phi(z_j)\\right)\\left(\\sum_{j=1}^{m} a_j b_j \\phi'(z_j)\\right) - \\nu \\left(\\sum_{j=1}^{m} a_j b_j^2 \\phi''(z_j)\\right)\n$$\nwhere $z_j = z_j(x,t)$.\n\nThe computational procedure for each test case is as follows:\n1.  Define the kinematic viscosity $\\nu$ and the network parameters $a_j, b_j, c_j, d_j$.\n2.  Generate the specified set of $N$ collocation points $(x_i, t_i)$.\n3.  Perform a vectorized computation over all collocation points. For each point $(x_i, t_i)$ and each neuron $j \\in \\{1, \\dots, m\\}$:\n    a. Calculate $z_{ij} = b_j x_i + c_j t_i + d_j$.\n    b. Evaluate the activation $\\phi(z_{ij})$ and its derivatives $\\phi'(z_{ij})$ and $\\phi''(z_{ij})$.\n4.  Compute the sums over the neurons to find the values of $u, u_t, u_x, u_{xx}$ at each collocation point.\n5.  Calculate the residual $r_i$ for each point.\n6.  Compute the residual norms $|r_i|$.\n7.  Finally, calculate the mean and maximum of the residual norms over the set of all collocation points:\n$$\n\\text{mean}(|r|) = \\frac{1}{N} \\sum_{i=1}^{N} |r(x_i, t_i)|, \\quad \\text{max}(|r|) = \\max_{i=1, \\dots, N} |r(x_i, t_i)|\n$$\nThis procedure is implemented for the three specified test cases, and the resulting six floating-point numbers (mean and max for each case) are reported.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the mean and maximum of the physics residual norm for the 1D viscous\n    Burgers' equation for three different test cases involving a neural network ansatz.\n    \"\"\"\n    \n    # Define the constant kinematic viscosity\n    nu = 0.01 / np.pi\n\n    # Define the parameters for the three test cases\n    test_cases = [\n        # Test Case 1: General interior sampling\n        {\n            \"m\": 5,\n            \"params\": {\n                \"a\": np.array([[1.0, -0.5, 0.3, 0.7, -0.2]]),\n                \"b\": np.array([[2.0, -1.0, 0.5, 3.0, -2.5]]),\n                \"c\": np.array([[-1.5, 0.7, 1.2, -0.8, 0.9]]),\n                \"d\": np.array([[0.1, -0.2, 0.3, -0.4, 0.5]]),\n            },\n            \"sampling\": {\n                \"type\": \"uniform\",\n                \"N\": 1024,\n                \"seed\": 123\n            }\n        },\n        # Test Case 2: Boundary-focused sampling\n        {\n            \"m\": 3,\n            \"params\": {\n                \"a\": np.array([[0.8, -0.6, 0.4]]),\n                \"b\": np.array([[5.0, -4.0, 3.0]]),\n                \"c\": np.array([[2.0, -1.0, 0.5]]),\n                \"d\": np.array([[-0.1, 0.2, -0.3]]),\n            },\n            \"sampling\": {\n                \"type\": \"boundary\",\n                \"K\": 64\n            }\n        },\n        # Test Case 3: Stratified interior sampling\n        {\n            \"m\": 4,\n            \"params\": {\n                \"a\": np.array([[0.05, -0.03, 0.02, -0.04]]),\n                \"b\": np.array([[8.0, -7.0, 6.0, -5.0]]),\n                \"c\": np.array([[4.0, -3.0, 2.0, -1.0]]),\n                \"d\": np.array([[0.0, 0.1, -0.1, 0.2]]),\n            },\n            \"sampling\": {\n                \"type\": \"stratified\",\n                \"Q\": 16\n            }\n        }\n    ]\n\n    results = []\n    \n    # Process each test case\n    for case in test_cases:\n        # 1. Unpack parameters\n        params = case[\"params\"]\n        a, b, c, d = params[\"a\"], params[\"b\"], params[\"c\"], params[\"d\"]\n        sampling_config = case[\"sampling\"]\n\n        # 2. Generate collocation points\n        if sampling_config[\"type\"] == \"uniform\":\n            N = sampling_config[\"N\"]\n            seed = sampling_config[\"seed\"]\n            rng = np.random.default_rng(seed)\n            x = rng.uniform(0.0, 1.0, N)\n            t = rng.uniform(0.0, 1.0, N)\n        elif sampling_config[\"type\"] == \"boundary\":\n            K = sampling_config[\"K\"]\n            x_k = np.linspace(0, 1, K)\n            t_k = np.linspace(0, 1, K)\n            \n            x_bottom = x_k\n            t_bottom = np.zeros(K)\n\n            x_top = x_k\n            t_top = np.ones(K)\n\n            x_left = np.zeros(K)\n            t_left = t_k\n            \n            x_right = np.ones(K)\n            t_right = t_k\n            \n            x = np.concatenate((x_bottom, x_top, x_left, x_right))\n            t = np.concatenate((t_bottom, t_top, t_left, t_right))\n        elif sampling_config[\"type\"] == \"stratified\":\n            Q = sampling_config[\"Q\"]\n            ticks = (np.arange(Q) + 0.5) / Q\n            x_grid, t_grid = np.meshgrid(ticks, ticks)\n            x = x_grid.flatten()\n            t = t_grid.flatten()\n        \n        # Reshape inputs for broadcasting\n        x_col = x.reshape(-1, 1)\n        t_col = t.reshape(-1, 1)\n\n        # 3. Compute ansatz and its derivatives\n        # z_j(x,t) = b_j*x + c_j*t + d_j\n        z = x_col * b + t_col * c + d\n\n        # Activation function and its derivatives\n        phi_z = np.tanh(z)\n        phi_prime_z = 1.0 - phi_z**2\n        phi_double_prime_z = -2.0 * phi_z * phi_prime_z\n\n        # Compute u and its partial derivatives via chain rule\n        u = np.sum(a * phi_z, axis=1)\n        u_t = np.sum(a * c * phi_prime_z, axis=1)\n        u_x = np.sum(a * b * phi_prime_z, axis=1)\n        u_xx = np.sum(a * b**2 * phi_double_prime_z, axis=1)\n\n        # 4. Calculate the physics residual\n        # r = u_t + u*u_x - nu*u_xx\n        r = u_t + u * u_x - nu * u_xx\n\n        # 5. Compute residual norm statistics\n        r_norms = np.abs(r)\n        mean_norm = np.mean(r_norms)\n        max_norm = np.max(r_norms)\n        \n        results.extend([mean_norm, max_norm])\n        \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3513283"}, {"introduction": "After quantifying a model's error, whether from data mismatch or a physics residual, the next critical step is to update the model to reduce that error. This exercise demystifies this \"learning\" process by focusing on its core mathematical engine: gradient-based optimization [@problem_id:3513338]. You will compute the gradient of a loss function with respect to a network parameter, providing a concrete look at how backpropagation attributes error to individual weights and guides the model's training.", "problem": "Consider a steady, one-dimensional diffusion problem on the spatial domain $[0,1]$ governed by the conservation law $-\\frac{d}{dx}\\left(k(\\theta)\\frac{du}{dx}\\right)=s(x)$ with Dirichlet boundary conditions $u(0;\\theta)=0$ and $u(1;\\theta)=0$, where the diffusivity is constant in space but depends on a scalar parameter $\\theta$ through $k(\\theta)=1+\\theta$. Let the source be $s(x)=1$. Define the Quantity of Interest (QoI) $J(\\theta)=u(0.5;\\theta)$.\n\nA machine learning-based surrogate is trained to approximate $J(\\theta)$ from $\\theta$ using a single-hidden-layer neural network with $2$ hidden units and hyperbolic tangent activation. The network mapping is\n$$\n\\widehat{J}(\\theta)=v_{1}\\,\\tanh(z_{1})+v_{2}\\,\\tanh(z_{2})+c,\\quad z_{1}=w_{11}\\,\\theta+b_{1},\\quad z_{2}=w_{21}\\,\\theta+b_{2}.\n$$\nAt the current training step, the parameters are $w_{11}=0.8$, $w_{21}=-1.2$, $b_{1}=0.1$, $b_{2}=-0.3$, $v_{1}=0.5$, $v_{2}=-0.4$, and $c=0.05$. A mini-batch of size $2$ consists of the parameter values $\\theta^{(1)}=0$ and $\\theta^{(2)}=1$, with targets given by the exact physics solution for $J(\\theta)$.\n\nStarting from the diffusion law and boundary conditions as the fundamental base, first derive an analytic expression for $J(\\theta)$ in terms of $\\theta$. Then, write the mean-squared error training loss over the mini-batch of size $2$ in terms of the network predictions and the exact $J(\\theta)$ values. Finally, compute the gradient of this loss with respect to the single weight $w_{11}$ at the specified parameter values and current network parameters. Express your final numeric gradient rounded to four significant figures.", "solution": "The problem will first be validated against the specified criteria before a solution is attempted.\n\n### Step 1: Extract Givens\n- **Governing Equation**: Steady, one-dimensional diffusion law: $-\\frac{d}{dx}\\left(k(\\theta)\\frac{du}{dx}\\right)=s(x)$ on the domain $x \\in [0,1]$.\n- **Diffusivity**: $k(\\theta)=1+\\theta$, where $\\theta$ is a scalar parameter.\n- **Source Term**: $s(x)=1$.\n- **Boundary Conditions**: $u(0;\\theta)=0$ and $u(1;\\theta)=0$.\n- **Quantity of Interest (QoI)**: $J(\\theta)=u(0.5;\\theta)$.\n- **Surrogate Model**: A single-hidden-layer neural network, $\\widehat{J}(\\theta)=v_{1}\\,\\tanh(z_{1})+v_{2}\\,\\tanh(z_{2})+c$.\n- **Network Inputs (Activations)**: $z_{1}=w_{11}\\,\\theta+b_{1}$ and $z_{2}=w_{21}\\,\\theta+b_{2}$.\n- **Network Parameters**: $w_{11}=0.8$, $w_{21}=-1.2$, $b_{1}=0.1$, $b_{2}=-0.3$, $v_{1}=0.5$, $v_{2}=-0.4$, and $c=0.05$.\n- **Training Data**: A mini-batch of size $2$ with $\\theta^{(1)}=0$ and $\\theta^{(2)}=1$.\n- **Loss Function**: Mean-squared error (MSE) between network predictions and exact physics solutions.\n- **Objective**: Compute the gradient of the MSE loss with respect to the weight $w_{11}$, evaluated at the given network parameters and mini-batch, rounded to four significant figures.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is well-defined and scientifically sound.\n1.  **Scientific Grounding**: The governing equation is the standard steady-state diffusion-reaction equation, a fundamental and well-understood concept in physics and engineering. The specified forms for diffusivity and source are simple and physically plausible.\n2.  **Well-Posedness**: For any $\\theta \\neq -1$, the diffusivity $k(\\theta)$ is a non-zero constant with respect to $x$. The problem constitutes a second-order linear ordinary differential equation with Dirichlet boundary conditions. This is a classic Sturm-Liouville problem, which is well-posed and guarantees a unique solution $u(x;\\theta)$. The values of $\\theta$ in the mini-batch ($0$ and $1$) are well away from the singularity at $\\theta=-1$.\n3.  **Objectivity and Completeness**: The problem is stated using precise mathematical language. All necessary functions, constants, parameters, and boundary conditions are explicitly provided. The task is specific and unambiguous.\n4.  **Consistency**: There are no contradictions in the provided information. The machine learning setup is standard, involving a simple neural network architecture, a common activation function ($\\tanh$), and a standard loss function (MSE). The calculation of a gradient via backpropagation is a core task in training neural networks.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. It is a self-contained, scientifically sound, and well-posed problem that combines analytical solution of a PDE with fundamental concepts of machine learning. A complete solution will be provided.\n\n### Solution Derivation\nThe solution proceeds in three stages:\n1.  Derive the analytical expression for the Quantity of Interest, $J(\\theta)$.\n2.  Define the mean-squared error loss function for the given mini-batch.\n3.  Compute the gradient of the loss with respect to the weight $w_{11}$.\n\n**1. Analytical Expression for $J(\\theta)$**\n\nThe governing differential equation is:\n$$\n-\\frac{d}{dx}\\left(k(\\theta)\\frac{du}{dx}\\right) = s(x)\n$$\nSubstituting the given forms for $k(\\theta)=1+\\theta$ and $s(x)=1$:\n$$\n-\\frac{d}{dx}\\left((1+\\theta)\\frac{du}{dx}\\right) = 1\n$$\nSince $1+\\theta$ is constant with respect to $x$, we can write:\n$$\n-(1+\\theta)\\frac{d^2u}{dx^2} = 1\n$$\n$$\n\\frac{d^2u}{dx^2} = -\\frac{1}{1+\\theta}\n$$\nWe integrate this ordinary differential equation twice with respect to $x$. The first integration yields:\n$$\n\\frac{du}{dx} = -\\frac{x}{1+\\theta} + C_1\n$$\nThe second integration gives the general solution for $u(x;\\theta)$:\n$$\nu(x;\\theta) = -\\frac{x^2}{2(1+\\theta)} + C_1 x + C_2\n$$\nThe constants of integration, $C_1$ and $C_2$, are determined by the boundary conditions.\nApplying the first boundary condition, $u(0;\\theta)=0$:\n$$\nu(0;\\theta) = -\\frac{0^2}{2(1+\\theta)} + C_1(0) + C_2 = 0 \\implies C_2 = 0\n$$\nApplying the second boundary condition, $u(1;\\theta)=0$:\n$$\nu(1;\\theta) = -\\frac{1^2}{2(1+\\theta)} + C_1(1) + 0 = 0 \\implies C_1 = \\frac{1}{2(1+\\theta)}\n$$\nSubstituting the constants back into the general solution gives the exact solution for $u(x;\\theta)$:\n$$\nu(x;\\theta) = -\\frac{x^2}{2(1+\\theta)} + \\frac{x}{2(1+\\theta)} = \\frac{x(1-x)}{2(1+\\theta)}\n$$\nThe Quantity of Interest, $J(\\theta)$, is defined as $u(0.5;\\theta)$:\n$$\nJ(\\theta) = u(0.5;\\theta) = \\frac{0.5(1-0.5)}{2(1+\\theta)} = \\frac{0.25}{2(1+\\theta)} = \\frac{1/4}{2(1+\\theta)} = \\frac{1}{8(1+\\theta)}\n$$\nThis is the analytical expression for the QoI.\n\n**2. Loss Function Definition**\n\nThe mini-batch consists of two samples, $\\theta^{(1)}=0$ and $\\theta^{(2)}=1$. The corresponding exact target values are:\n$$\nJ^{(1)} = J(\\theta^{(1)}=0) = \\frac{1}{8(1+0)} = \\frac{1}{8}\n$$\n$$\nJ^{(2)} = J(\\theta^{(2)}=1) = \\frac{1}{8(1+1)} = \\frac{1}{16}\n$$\nThe mean-squared error loss, $\\mathcal{L}$, for a mini-batch of size $N=2$ is:\n$$\n\\mathcal{L} = \\frac{1}{2} \\sum_{i=1}^{2} \\left(\\widehat{J}(\\theta^{(i)}) - J(\\theta^{(i)})\\right)^2 = \\frac{1}{2}\\left[\\left(\\widehat{J}^{(1)} - J^{(1)}\\right)^2 + \\left(\\widehat{J}^{(2)} - J^{(2)}\\right)^2\\right]\n$$\nwhere $\\widehat{J}^{(i)} = \\widehat{J}(\\theta^{(i)})$ is the neural network's prediction.\n\n**3. Gradient Computation**\n\nWe need to find the gradient of the loss $\\mathcal{L}$ with respect to the weight $w_{11}$. Using the chain rule:\n$$\n\\frac{\\partial\\mathcal{L}}{\\partial w_{11}} = \\frac{\\partial}{\\partial w_{11}} \\left( \\frac{1}{2}\\sum_{i=1}^{2} \\left(\\widehat{J}^{(i)} - J^{(i)}\\right)^2 \\right) = \\frac{1}{2} \\sum_{i=1}^{2} 2\\left(\\widehat{J}^{(i)} - J^{(i)}\\right) \\frac{\\partial\\widehat{J}^{(i)}}{\\partial w_{11}}\n$$\n$$\n\\frac{\\partial\\mathcal{L}}{\\partial w_{11}} = \\sum_{i=1}^{2} \\left(\\widehat{J}^{(i)} - J^{(i)}\\right) \\frac{\\partial\\widehat{J}^{(i)}}{\\partial w_{11}}\n$$\nNext, we find the partial derivative of the network output $\\widehat{J}(\\theta)$ with respect to $w_{11}$:\n$$\n\\widehat{J}(\\theta) = v_{1}\\,\\tanh(w_{11}\\,\\theta+b_{1}) + v_{2}\\,\\tanh(w_{21}\\,\\theta+b_{2}) + c\n$$\n$$\n\\frac{\\partial\\widehat{J}(\\theta)}{\\partial w_{11}} = \\frac{\\partial}{\\partial w_{11}} \\left(v_{1}\\,\\tanh(w_{11}\\,\\theta+b_{1})\\right) = v_{1} \\cdot \\frac{d}{d z_1}(\\tanh(z_1)) \\cdot \\frac{\\partial z_1}{\\partial w_{11}}\n$$\nwhere $z_1 = w_{11}\\theta + b_1$. The derivative of $\\tanh(z)$ is $\\text{sech}^2(z)$ or $1-\\tanh^2(z)$, and $\\frac{\\partial z_1}{\\partial w_{11}} = \\theta$.\n$$\n\\frac{\\partial\\widehat{J}(\\theta)}{\\partial w_{11}} = v_{1} \\cdot \\text{sech}^2(w_{11}\\theta+b_{1}) \\cdot \\theta = v_1 \\theta (1 - \\tanh^2(w_{11}\\theta+b_1))\n$$\nNow we evaluate the terms for each sample in the mini-batch.\n\nFor the first sample, $\\theta^{(1)}=0$:\nThe derivative term is $\\frac{\\partial\\widehat{J}^{(1)}}{\\partial w_{11}} = v_1 \\theta^{(1)} (1 - \\tanh^2(w_{11}\\theta^{(1)}+b_1)) = v_1 \\cdot 0 \\cdot (\\dots) = 0$.\nTherefore, the contribution to the gradient from the first sample is zero.\n\nFor the second sample, $\\theta^{(2)}=1$:\nThe total gradient is determined solely by this sample:\n$$\n\\frac{\\partial\\mathcal{L}}{\\partial w_{11}} = \\left(\\widehat{J}^{(2)} - J^{(2)}\\right) \\frac{\\partial\\widehat{J}^{(2)}}{\\partial w_{11}}\n$$\nWe compute the necessary components using the given parameter values: $w_{11}=0.8$, $w_{21}=-1.2$, $b_{1}=0.1$, $b_{2}=-0.3$, $v_{1}=0.5$, $v_{2}=-0.4$, $c=0.05$.\nFirst, calculate the network prediction $\\widehat{J}^{(2)} = \\widehat{J}(\\theta=1)$:\n$$\nz_{1}^{(2)} = w_{11}\\theta^{(2)} + b_{1} = (0.8)(1) + 0.1 = 0.9\n$$\n$$\nz_{2}^{(2)} = w_{21}\\theta^{(2)} + b_{2} = (-1.2)(1) - 0.3 = -1.5\n$$\n$$\n\\widehat{J}^{(2)} = v_{1}\\,\\tanh(z_{1}^{(2)}) + v_{2}\\,\\tanh(z_{2}^{(2)}) + c = 0.5\\,\\tanh(0.9) - 0.4\\,\\tanh(-1.5) + 0.05\n$$\nSince $\\tanh(-x) = -\\tanh(x)$:\n$$\n\\widehat{J}^{(2)} = 0.5\\,\\tanh(0.9) + 0.4\\,\\tanh(1.5) + 0.05\n$$\nUsing $\\tanh(0.9) \\approx 0.71629787$ and $\\tanh(1.5) \\approx 0.90514825$:\n$$\n\\widehat{J}^{(2)} \\approx 0.5(0.71629787) + 0.4(0.90514825) + 0.05 \\approx 0.35814894 + 0.36205930 + 0.05 = 0.77020824\n$$\nThe residual is $\\widehat{J}^{(2)} - J^{(2)} \\approx 0.77020824 - \\frac{1}{16} = 0.77020824 - 0.0625 = 0.70770824$.\nNext, calculate the derivative term $\\frac{\\partial\\widehat{J}^{(2)}}{\\partial w_{11}}$:\n$$\n\\frac{\\partial\\widehat{J}^{(2)}}{\\partial w_{11}} = v_1 \\theta^{(2)} (1 - \\tanh^2(w_{11}\\theta^{(2)}+b_1)) = (0.5)(1)(1-\\tanh^2(0.9))\n$$\n$$\n\\frac{\\partial\\widehat{J}^{(2)}}{\\partial w_{11}} \\approx 0.5(1 - (0.71629787)^2) \\approx 0.5(1 - 0.51308233) = 0.5(0.48691767) = 0.243458835\n$$\nFinally, multiply the residual and the derivative term to get the gradient:\n$$\n\\frac{\\partial\\mathcal{L}}{\\partial w_{11}} \\approx (0.70770824) \\times (0.243458835) \\approx 0.1722915\n$$\nRounding the final result to four significant figures gives $0.1723$.", "answer": "$$\n\\boxed{0.1723}\n$$", "id": "3513338"}, {"introduction": "In many scientific applications, generating high-fidelity training data is computationally expensive, making efficient data acquisition essential. This practice moves beyond fixed sampling grids to an intelligent, adaptive strategy known as active learning [@problem_id:3513339]. You will implement a Gaussian Process (GP) surrogate that quantifies its own uncertainty and use an acquisition function to select the next most informative sampling point, a powerful technique for building accurate models under a limited budget.", "problem": "Consider a one-dimensional, steady, coupled multiphysics model that links heat conduction and species diffusion through a scalar coupling parameter $\\theta \\in [0,1]$ in a unitless domain. The temperature field $T(x)$ and the concentration field $C(x)$ are related by the balance law $-k\\, T''(x) + \\beta\\, \\theta\\, C(x) = 0$ on $x \\in [0,1]$, where $k>0$ is the thermal conductivity and $\\beta>0$ is a coupling coefficient. Assume the known species concentration field $C(x) = \\sin(\\pi x)$ and homogeneous boundary conditions for $T(x)$, leading to an analytic family of temperature fields and a derived quantity of interest $J(\\theta)$ defined by the squared residual of the coupling balance (integrated over $x \\in [0,1]$) when the temperature field is approximated by a harmonic mode $T(x) = \\sin(2\\pi x)$. By orthogonality of sines over $[0,1]$, this yields the closed-form $J(\\theta) = A + B\\, \\theta^2$ with $A = \\frac{k^2 (2\\pi)^4}{2}$ and $B = \\frac{\\beta^2}{2}$.\n\nYou will design a Gaussian Process (GP) surrogate for $J(\\theta)$ using a squared-exponential kernel and select the next sampling location that maximizes the expected improvement in reducing the current maximum absolute error of the surrogate. The purpose is to formalize and implement, from first principles, a principled acquisition that balances predictive uncertainty and the worst-case discrepancy between the surrogate and the true $J(\\theta)$, under a sampling budget.\n\nFundamental base and definitions to be used:\n- Gaussian Process (GP) prior with zero mean and squared-exponential kernel $k(\\theta,\\theta') = \\sigma_f^2 \\exp\\!\\left(-\\frac{(\\theta-\\theta')^2}{2\\ell^2}\\right)$, where $\\sigma_f>0$ is the signal amplitude and $\\ell>0$ is the length scale.\n- Observational noise modeled as independent, zero-mean Gaussian with variance $\\sigma_n^2$ added only on the training diagonal.\n- GP posterior for a test point $\\theta$ given training inputs $\\Theta = \\{\\theta_i\\}_{i=1}^n$ and observations $\\mathbf{y} = \\{y_i\\}_{i=1}^n$ is\n$$\nm(\\theta) = \\mathbf{k}(\\theta)^\\top \\left(\\mathbf{K} + \\sigma_n^2 \\mathbf{I}\\right)^{-1} \\mathbf{y},\n\\qquad\ns^2(\\theta) = k(\\theta,\\theta) - \\mathbf{k}(\\theta)^\\top \\left(\\mathbf{K} + \\sigma_n^2 \\mathbf{I}\\right)^{-1} \\mathbf{k}(\\theta),\n$$\nwhere $\\mathbf{K}_{ij} = k(\\theta_i,\\theta_j)$ and $\\mathbf{k}(\\theta)_i = k(\\theta,\\theta_i)$.\n- Physics-Informed Neural Networks (PINNs) constrain learning by enforcing the governing residuals in the loss; in practice, one would obtain $y_i$ by running a PINN-based solver at parameters $\\theta_i$ and evaluating the induced $J(\\theta_i)$. For this assignment, use the analytic $J(\\theta)$ defined above to generate consistent, noiseless training data, which emulates a high-fidelity PINN solution free of training error.\n- To quantify the current worst-case surrogate discrepancy, define the threshold $t$ as the maximum absolute error of the surrogate over a fixed evaluation grid $\\mathcal{G}=\\{\\vartheta_j\\}_{j=1}^{M}$:\n$$\nt = \\max_{\\vartheta \\in \\mathcal{G}} \\left| m(\\vartheta) - J(\\vartheta) \\right|.\n$$\n- To choose the next sampling location, adopt an expected improvement for absolute error at a candidate $\\theta$ relative to the current threshold $t$. Under the GP posterior, the unknown $J(\\theta)$ is modeled as a Normal random variable $\\mathcal{N}\\!\\left(m(\\theta), s^2(\\theta)\\right)$, so the absolute error $\\left|J(\\theta)-m(\\theta)\\right|$ is a folded normal random variable $A$ with scale $s(\\theta)$. The expected improvement at $\\theta$ is\n$$\n\\operatorname{EI}_{\\mathrm{abs}}(\\theta) = \\mathbb{E}\\left[\\left(A - t\\right)_+\\right]\n= \\int_{t}^{\\infty} (a-t)\\, f_A(a)\\, da,\n$$\nwhere $f_A(a)$ is the probability density function of the folded normal with zero mean and scale $s(\\theta)$. This integral can be evaluated in closed form, yielding\n$$\n\\operatorname{EI}_{\\mathrm{abs}}(\\theta) =\n\\sqrt{\\frac{2}{\\pi}}\\, s(\\theta)\\, \\exp\\!\\left(-\\frac{t^2}{2 s^2(\\theta)}\\right)\n- t \\, \\operatorname{erfc}\\!\\left(\\frac{t}{\\sqrt{2}\\, s(\\theta)}\\right),\n$$\nwith the convention that $\\operatorname{EI}_{\\mathrm{abs}}(\\theta)=0$ when $s(\\theta)=0$.\n\nAlgorithmic requirements:\n- Use the zero-mean GP with the squared-exponential kernel and the analytic $J(\\theta)$ to form training data $\\{(\\theta_i, y_i)\\}_{i=1}^{n}$, where $y_i = J(\\theta_i)$.\n- Training locations must be deterministically set to interior, equally spaced points $\\theta_i = \\frac{i+1}{n+1}$ for $i=0,1,\\dots,n-1$, ensuring no endpoints are initially sampled.\n- The candidate set for selecting the next sample location and the evaluation grid must both be the uniform grid $\\mathcal{G}$ of $M$ points over $[0,1]$, where $M = 401$ and $\\vartheta_j = \\frac{j}{M-1}$ for $j=0,1,\\dots,M-1$.\n- Compute the posterior mean $m(\\vartheta)$ and variance $s^2(\\vartheta)$ for all $\\vartheta \\in \\mathcal{G}$.\n- Compute the current error threshold $t$ on $\\mathcal{G}$.\n- For each candidate $\\vartheta \\in \\mathcal{G}$, compute $\\operatorname{EI}_{\\mathrm{abs}}(\\vartheta)$ using the closed-form expression above, with the convention $\\operatorname{EI}_{\\mathrm{abs}}(\\vartheta)=0$ when $s(\\vartheta)=0$.\n- Select the next sampling location as the $\\vartheta \\in \\mathcal{G}$ that maximizes $\\operatorname{EI}_{\\mathrm{abs}}(\\vartheta)$ subject to the sampling budget. If the current number of samples $n$ is equal to the maximum allowed samples $n_{\\max}$, no further sampling is permitted: return the sentinel value $-1.0$.\n\nPhysical and numerical units:\n- All quantities are dimensionless in this assignment.\n- Angles do not apply.\n- Percentages do not apply.\n\nProgram input specification (embedded in the program; no external input):\n- Use fixed physical constants $k = 0.1$ and $\\beta = 1.0$ when constructing $A$ and $B$ in $J(\\theta) = A + B\\, \\theta^2$.\n- Use the squared-exponential kernel $k(\\theta,\\theta')$ with specified hyperparameters $\\ell$ and $\\sigma_f$, and observational noise variance $\\sigma_n^2$ only on the training diagonal.\n\nTest suite:\nFor each test case, the program must construct training data from $J(\\theta)$ at interior points and then compute the next sampling location as described above. The test suite consists of the following parameter sets $(n, n_{\\max}, \\ell, \\sigma_f, \\sigma_n)$:\n- Case $1$ (happy path): $(n = 3, n_{\\max} = 5, \\ell = 0.2, \\sigma_f = 1.5, \\sigma_n = 10^{-6})$.\n- Case $2$ (budget exhausted): $(n = 8, n_{\\max} = 8, \\ell = 0.1, \\sigma_f = 1.0, \\sigma_n = 10^{-6})$.\n- Case $3$ (short length scale): $(n = 4, n_{\\max} = 6, \\ell = 0.05, \\sigma_f = 0.7, \\sigma_n = 10^{-6})$.\n- Case $4$ (very sparse training): $(n = 2, n_{\\max} = 4, \\ell = 0.3, \\sigma_f = 2.0, \\sigma_n = 10^{-6})$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results (the next sampling location for each test case, or the sentinel $-1.0$ if the budget is exhausted) as a comma-separated list enclosed in square brackets. For example, the output must be of the form $\\left[\\text{result}_1,\\text{result}_2,\\text{result}_3,\\text{result}_4\\right]$, where each entry is a floating-point number.", "solution": "The problem requires the design and implementation of an active learning algorithm based on a Gaussian Process (GP) surrogate model. The objective is to intelligently select the next sampling location for a quantity of interest, $J(\\theta)$, derived from a simplified one-dimensional multiphysics model. The selection criterion is the maximization of the Expected Improvement for Absolute Error ($\\operatorname{EI}_{\\mathrm{abs}}$), which balances exploiting the model's predictions and exploring regions of high uncertainty.\n\n### Step 1: Validation of the Problem Statement\n\nI will first validate the problem statement as per the required procedure.\n\n**1.1. Extraction of Givens**\n\n- **Governing Equation**: $-k\\, T''(x) + \\beta\\, \\theta\\, C(x) = 0$ for $x \\in [0,1]$.\n- **Parameters**: Thermal conductivity $k>0$, coupling coefficient $\\beta>0$, scalar coupling parameter $\\theta \\in [0,1]$.\n- **Known Fields**: Concentration field $C(x) = \\sin(\\pi x)$, approximate temperature field $T(x) = \\sin(2\\pi x)$.\n- **Boundary Conditions**: Homogeneous for $T(x)$, i.e., $T(0)=0$ and $T(1)=0$.\n- **Quantity of Interest**: $J(\\theta) = A + B\\, \\theta^2$, derived from the integrated squared residual.\n- **Coefficients for $J(\\theta)$**: $A = \\frac{k^2 (2\\pi)^4}{2}$ and $B = \\frac{\\beta^2}{2}$.\n- **GP Prior**: Zero mean, squared-exponential kernel $k(\\theta,\\theta') = \\sigma_f^2 \\exp\\!\\left(-\\frac{(\\theta-\\theta')^2}{2\\ell^2}\\right)$ with signal amplitude $\\sigma_f>0$ and length scale $\\ell>0$.\n- **Observation Noise**: Independent, zero-mean Gaussian with variance $\\sigma_n^2$.\n- **GP Posterior Mean**: $m(\\theta) = \\mathbf{k}(\\theta)^\\top \\left(\\mathbf{K} + \\sigma_n^2 \\mathbf{I}\\right)^{-1} \\mathbf{y}$.\n- **GP Posterior Variance**: $s^2(\\theta) = k(\\theta,\\theta) - \\mathbf{k}(\\theta)^\\top \\left(\\mathbf{K} + \\sigma_n^2 \\mathbf{I}\\right)^{-1} \\mathbf{k}(\\theta)$.\n- **Definitions for Posterior**: $\\mathbf{K}_{ij} = k(\\theta_i,\\theta_j)$, $\\mathbf{k}(\\theta)_i = k(\\theta,\\theta_i)$, $\\mathbf{y}$ are observations.\n- **Data Generation**: Use the analytic $J(\\theta)$ to generate noiseless training data ($y_i=J(\\theta_i)$).\n- **Error Threshold**: $t = \\max_{\\vartheta \\in \\mathcal{G}} \\left| m(\\vartheta) - J(\\vartheta) \\right|$ over a fixed evaluation grid $\\mathcal{G}$.\n- **Acquisition Function**: Expected Improvement for Absolute Error, $\\operatorname{EI}_{\\mathrm{abs}}(\\theta) = \\sqrt{\\frac{2}{\\pi}}\\, s(\\theta)\\, \\exp\\!\\left(-\\frac{t^2}{2 s^2(\\theta)}\\right) - t \\, \\operatorname{erfc}\\!\\left(\\frac{t}{\\sqrt{2}\\, s(\\theta)}\\right)$. Convention: $\\operatorname{EI}_{\\mathrm{abs}}(\\theta)=0$ if $s(\\theta)=0$.\n- **Training Locations**: $\\theta_i = \\frac{i+1}{n+1}$ for $i=0,1,\\dots,n-1$.\n- **Evaluation/Candidate Grid**: $\\mathcal{G}$ is a uniform grid of $M=401$ points on $[0,1]$, $\\vartheta_j = \\frac{j}{M-1}$.\n- **Sampling Budget**: If current sample count $n$ equals $n_{\\max}$, return sentinel value $-1.0$.\n- **Physical Constants**: $k = 0.1$, $\\beta = 1.0$.\n- **Test Cases**: $(n, n_{\\max}, \\ell, \\sigma_f, \\sigma_n)$ for four specified cases.\n\n**1.2. Validation using Checklist**\n\n- **Scientifically Grounded**: The problem is grounded in the established fields of numerical simulation and machine learning. The multiphysics model, though simplified, represents a valid physical concept. The use of a Gaussian Process as a surrogate and Expected Improvement as an acquisition function are standard, well-documented techniques in Bayesian optimization and uncertainty quantification. The derivation of $J(\\theta)$ is plausible for the given functional forms of $T(x)$ and $C(x)$.\n- **Well-Posed**: The problem is well-posed. All necessary functions, parameters, and algorithmic steps are explicitly defined. The task is to find the maximum of a deterministically computed function ($\\operatorname{EI}_{\\mathrm{abs}}$) over a finite set of points, which guarantees the existence and uniqueness of a solution (up to ties, which can be resolved deterministically, e.g., by picking the first occurrence).\n- **Objective**: The problem statement is written in precise, objective, and mathematical language. There are no subjective or opinion-based components.\n- **Incomplete or Contradictory Setup**: The problem is self-contained. It provides all necessary constants ($k, \\beta$), GP hyperparameters ($\\ell, \\sigma_f, \\sigma_n$), grid definitions ($M$), and training data generation rules. There are no apparent contradictions.\n- **Unrealistic or Infeasible**: The problem is computationally feasible. The matrix inversion for the GP is on a small matrix of size $n \\times n$ where $n$ is small. The premise of using an analytic function to stand in for a complex simulation is a standard and realistic practice for algorithm development and testing.\n- **Ill-Posed or Poorly Structured**: The problem is clearly structured. The logic flows from the physical model to the surrogate model and finally to the active learning algorithm.\n- **Pseudo-Profound, Trivial, or Tautological**: The problem is not trivial. It requires a correct implementation of Gaussian Process regression and a non-standard acquisition function, involving linear algebra and special mathematical functions. The conceptual depth is appropriate for a problem in computational science.\n- **Outside Scientific Verifiability**: The results are fully verifiable by implementing the described algorithm.\n\n**1.3. Verdict**\n\nThe problem is **valid**. It is a well-defined, scientifically sound, and computationally tractable problem in the domain of surrogate modeling for scientific computing. I will now proceed with the solution.\n\n### Step 2: Principled Solution Design\n\nThe core of the task is to implement a single step of an active learning loop. Given an existing set of $n$ samples, we build a GP surrogate model and use it to decide the most informative next point to sample, aiming to reduce the maximum model error.\n\n**2.1. The Analytical Quantity of Interest $J(\\theta)$**\n\nThe true function we aim to model is $J(\\theta) = A + B\\, \\theta^2$. The constants $A$ and $B$ are determined by the physics of the problem.\nGiven the physical constants $k = 0.1$ and $\\beta = 1.0$, we can calculate $A$ and $B$:\n$$\nA = \\frac{k^2 (2\\pi)^4}{2} = \\frac{(0.1)^2 \\cdot 16 \\pi^4}{2} = 0.08 \\pi^4\n$$\n$$\nB = \\frac{\\beta^2}{2} = \\frac{1.0^2}{2} = 0.5\n$$\nSo, the function to be approximated is $J(\\theta) = 0.08 \\pi^4 + 0.5 \\theta^2$. This function serves as the \"ground truth\" or the output of a high-fidelity simulation.\n\n**2.2. Gaussian Process Surrogate Model**\n\nWe model $J(\\theta)$ using a Gaussian Process. A GP is a stochastic process where any finite collection of random variables has a multivariate Gaussian distribution. It is fully specified by a mean function $m_0(\\theta)$ and a covariance (or kernel) function $k(\\theta, \\theta')$.\n\n- **Prior**: We assume a zero-mean prior, $m_0(\\theta) = 0$. The kernel is the squared-exponential kernel:\n$$\nk(\\theta, \\theta') = \\sigma_f^2 \\exp\\left(-\\frac{(\\theta - \\theta')^2}{2\\ell^2}\\right)\n$$\nHere, $\\sigma_f$ is the signal amplitude, controlling the overall variance, and $\\ell$ is the length scale, controlling the \"smoothness\" or correlation distance.\n\n- **Training Data**: We generate $n$ training points. The input locations are deterministically chosen as $\\Theta_{\\text{train}} = \\{\\theta_i\\}_{i=0}^{n-1}$ where $\\theta_i = \\frac{i+1}{n+1}$. The corresponding outputs are noise-free observations from the true function, $\\mathbf{y}_{\\text{train}} = \\{J(\\theta_i)\\}_{i=0}^{n-1}$.\n\n- **Posterior Distribution**: Given the training data $(\\Theta_{\\text{train}}, \\mathbf{y}_{\\text{train}})$, the GP posterior for any test point $\\theta_*$ is also a Gaussian distribution with mean $m(\\theta_*)$ and variance $s^2(\\theta_*)$. These are calculated as:\n$$\nm(\\theta_*) = \\mathbf{k}_*^\\top (\\mathbf{K} + \\sigma_n^2 \\mathbf{I})^{-1} \\mathbf{y}_{\\text{train}}\n$$\n$$\ns^2(\\theta_*) = k(\\theta_*, \\theta_*) - \\mathbf{k}_*^\\top (\\mathbf{K} + \\sigma_n^2 \\mathbf{I})^{-1} \\mathbf{k}_*\n$$\nwhere:\n- $\\mathbf{K}$ is the $n \\times n$ kernel matrix of the training points, with entries $K_{ij} = k(\\theta_i, \\theta_j)$.\n- $\\mathbf{k}_*$ is the $n \\times 1$ vector of covariances between the test point $\\theta_*$ and the training points, with entries $(\\mathbf{k}_*)_i = k(\\theta_*, \\theta_i)$.\n- $k(\\theta_*, \\theta_*)$ is the prior variance at the test point.\n- $\\sigma_n^2$ is the variance of the observation noise, added to the diagonal of $\\mathbf{K}$ to ensure matrix stability and model observation error. Even though our data is noise-free, this term is crucial for numerical conditioning.\n\n**2.3. The Active Learning Criterion: $\\operatorname{EI}_{\\mathrm{abs}}$**\n\nThe goal is to select the next point that is most likely to reduce the current maximum absolute error of our surrogate.\n\n- **Candidate and Evaluation Grid**: We use a fine, uniform grid $\\mathcal{G} = \\{\\vartheta_j\\}_{j=0}^{M-1}$ with $M=401$ points over $[0,1]$ both for evaluating the model's performance and for selecting the next sample.\n\n- **Error Threshold $t$**: First, we compute the GP posterior mean $m(\\vartheta)$ for all $\\vartheta \\in \\mathcal{G}$. Then, we find the current maximum absolute error of this surrogate with respect to the true function $J(\\vartheta)$:\n$$\nt = \\max_{\\vartheta \\in \\mathcal{G}} |m(\\vartheta) - J(\\vartheta)|\n$$\nThis value $t$ represents the current worst-case performance of our model on the grid $\\mathcal{G}$.\n\n- **Expected Improvement for Absolute Error**: For any candidate point $\\vartheta \\in \\mathcal{G}$, the true value $J(\\vartheta)$ is unknown from the model's perspective. The GP posterior models it as a random variable $J(\\vartheta) \\sim \\mathcal{N}(m(\\vartheta), s^2(\\vartheta))$. The absolute error at this point, $|J(\\vartheta) - m(\\vartheta)|$, is therefore a folded normal random variable. We are interested in the expected improvement over our current threshold $t$. The improvement is defined as $\\max(0, |J(\\vartheta) - m(\\vartheta)| - t)$. The expectation of this quantity is the acquisition function $\\operatorname{EI}_{\\mathrm{abs}}(\\vartheta)$:\n$$\n\\operatorname{EI}_{\\mathrm{abs}}(\\vartheta) = \\mathbb{E}\\left[ \\max(0, |J(\\vartheta) - m(\\vartheta)| - t) \\right]\n$$\nThe problem provides the closed-form solution for this integral:\n$$\n\\operatorname{EI}_{\\mathrm{abs}}(\\vartheta) = \\sqrt{\\frac{2}{\\pi}}\\, s(\\vartheta) \\exp\\left(-\\frac{t^2}{2 s^2(\\vartheta)}\\right) - t \\, \\operatorname{erfc}\\left(\\frac{t}{\\sqrt{2}\\, s(\\vartheta)}\\right)\n$$\nwhere $s(\\vartheta)$ is the posterior standard deviation at $\\vartheta$ and $\\operatorname{erfc}$ is the complementary error function. If the posterior variance $s^2(\\vartheta)$ is zero (or numerically negligible), no improvement is possible, so we set $\\operatorname{EI}_{\\mathrm{abs}}(\\vartheta) = 0$. This typically occurs at training points where there is no uncertainty.\n\n**2.4. Algorithmic Implementation Steps**\n\nFor each test case with parameters $(n, n_{\\max}, \\ell, \\sigma_f, \\sigma_n)$:\n1.  **Check Budget**: If $n \\ge n_{\\max}$, sampling is complete. Return the sentinel value $-1.0$.\n2.  **Define Model**: Set up constants $k=0.1, \\beta=1.0$ and compute $A, B$. Define the true function $J(\\theta) = A + B \\theta^2$.\n3.  **Generate Data**: Create the training inputs $\\Theta_{\\text{train}} = \\{\\frac{i+1}{n+1}\\}_{i=0}^{n-1}$ and outputs $\\mathbf{y}_{\\text{train}} = \\{J(\\theta_i)\\}_{i=0}^{n-1}$.\n4.  **Define Grid**: Create the evaluation/candidate grid $\\mathcal{G}$ of $M=401$ points from $0$ to $1$.\n5.  **Compute Posterior**:\n    a. Construct the kernel matrix $\\mathbf{K}$ from $\\Theta_{\\text{train}}$.\n    b. Compute the matrix $\\mathbf{L} = \\mathbf{K} + \\sigma_n^2 \\mathbf{I}$.\n    c. Compute its inverse $\\mathbf{L}^{-1}$. A more stable approach is to solve the linear system $\\mathbf{L}\\mathbf{\\alpha} = \\mathbf{y}_{\\text{train}}$ for $\\mathbf{\\alpha}$. Then $m(\\theta_*) = \\mathbf{k}_*^\\top \\mathbf{\\alpha}$.\n    d. For each $\\vartheta \\in \\mathcal{G}$, compute $\\mathbf{k}_*(\\vartheta)$ and then calculate the posterior mean $m(\\vartheta)$ and variance $s^2(\\vartheta)$.\n6.  **Compute Threshold $t$**: Evaluate $J(\\vartheta)$ for all $\\vartheta \\in \\mathcal{G}$. Compute $t = \\max_{\\vartheta \\in \\mathcal{G}} |m(\\vartheta) - J(\\vartheta)|$.\n7.  **Compute Acquisition Function**: For each $\\vartheta \\in \\mathcal{G}$, calculate $\\operatorname{EI}_{\\mathrm{abs}}(\\vartheta)$ using the posterior standard deviation $s(\\vartheta)$ and the threshold $t$.\n8.  **Select Next Point**: Find the point in the grid that maximizes the acquisition function: $\\theta_{\\text{next}} = \\arg\\max_{\\vartheta \\in \\mathcal{G}} \\operatorname{EI}_{\\mathrm{abs}}(\\vartheta)$. This is the desired output.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import erfc\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print the results.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (n, n_max, l, sigma_f, sigma_n)\n        (3, 5, 0.2, 1.5, 1e-6),  # Case 1\n        (8, 8, 0.1, 1.0, 1e-6),  # Case 2\n        (4, 6, 0.05, 0.7, 1e-6), # Case 3\n        (2, 4, 0.3, 2.0, 1e-6),  # Case 4\n    ]\n\n    results = []\n    for case in test_cases:\n        n, n_max, ell, sigma_f, sigma_n = case\n        next_location = find_next_sample_location(n, n_max, ell, sigma_f, sigma_n**2) # Pass variance\n        results.append(next_location)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef find_next_sample_location(n, n_max, ell, sigma_f, sigma_n_sq):\n    \"\"\"\n    Implements the GP-based active learning algorithm to find the next sample location.\n    \n    Args:\n        n (int): Current number of samples.\n        n_max (int): Maximum number of samples allowed.\n        ell (float): Length scale for the kernel.\n        sigma_f (float): Signal amplitude for the kernel.\n        sigma_n_sq (float): Variance of the observation noise.\n    \n    Returns:\n        float: The next sampling location theta, or -1.0 if budget is exhausted.\n    \"\"\"\n    \n    # Step 1: Check the sampling budget\n    if n >= n_max:\n        return -1.0\n        \n    # Step 2: Define physical model constants and the true function J(theta)\n    k_phys = 0.1\n    beta_phys = 1.0\n    A = k_phys**2 * (2 * np.pi)**4 / 2.0\n    B = beta_phys**2 / 2.0\n    \n    def J_true(theta):\n        return A + B * theta**2\n\n    # Step 3: Generate training data\n    if n > 0:\n        theta_train = np.array([(i + 1) / (n + 1) for i in range(n)])\n        y_train = J_true(theta_train)\n    else:\n        theta_train = np.array([])\n        y_train = np.array([])\n        \n    # Step 4: Define evaluation and candidate grid\n    M = 401\n    grid = np.linspace(0.0, 1.0, M)\n\n    # Step 5: Compute GP posterior mean and variance on the grid\n    \n    def se_kernel(theta1, theta2, ell, sigma_f):\n        sqdist = np.subtract.outer(theta1, theta2)**2\n        return sigma_f**2 * np.exp(-0.5 * sqdist / ell**2)\n\n    if n == 0:\n        m_post = np.zeros(M)\n        s2_post = np.full(M, sigma_f**2)\n    else:\n        K = se_kernel(theta_train, theta_train, ell, sigma_f)\n        K_noisy = K + sigma_n_sq * np.eye(n)\n        K_star = se_kernel(grid, theta_train, ell, sigma_f)\n        K_star_star_diag = np.full(M, sigma_f**2)\n        \n        alpha = np.linalg.solve(K_noisy, y_train)\n        m_post = K_star @ alpha\n\n        v = np.linalg.solve(K_noisy, K_star.T)\n        s2_post = K_star_star_diag - np.sum(K_star * v.T, axis=1)\n        \n        s2_post[s2_post < 0] = 0\n\n    # Step 6: Compute current error threshold 't'\n    J_grid = J_true(grid)\n    abs_error = np.abs(m_post - J_grid)\n    t = np.max(abs_error)\n\n    # Step 7: Compute Expected Improvement for Absolute Error (EI_abs)\n    \n    s_post = np.sqrt(s2_post)\n    ei_abs = np.zeros(M)\n    \n    nonzero_s_mask = s_post > 1e-12 \n    \n    s_nonzero = s_post[nonzero_s_mask]\n    z = t / s_nonzero\n    \n    term1 = np.sqrt(2.0 / np.pi) * s_nonzero * np.exp(-0.5 * z**2)\n    term2 = t * erfc(z / np.sqrt(2.0))\n    \n    ei_abs[nonzero_s_mask] = term1 - term2\n    \n    # Step 8: Select next point that maximizes EI_abs\n    best_idx = np.argmax(ei_abs)\n    next_location = grid[best_idx]\n    \n    return float(next_location)\n\n# In the original problem, sigma_n was given, but the GP equations use sigma_n^2.\n# The code should use sigma_n^2, but the input is sigma_n.\n# The call `find_next_sample_location(n, n_max, ell, sigma_f, sigma_n**2)` corrects this.\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print the results.\n    \"\"\"\n    test_cases = [\n        (3, 5, 0.2, 1.5, 1e-6),\n        (8, 8, 0.1, 1.0, 1e-6),\n        (4, 6, 0.05, 0.7, 1e-6),\n        (2, 4, 0.3, 2.0, 1e-6),\n    ]\n\n    results = []\n    for case in test_cases:\n        n, n_max, ell, sigma_f, sigma_n = case\n        sigma_n_sq = sigma_n**2\n        next_location = find_next_sample_location(n, n_max, ell, sigma_f, sigma_n_sq)\n        results.append(next_location)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3513339"}]}