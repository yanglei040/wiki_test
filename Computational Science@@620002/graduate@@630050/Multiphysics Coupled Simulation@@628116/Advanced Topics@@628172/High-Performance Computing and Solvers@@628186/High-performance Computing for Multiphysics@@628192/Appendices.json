{"hands_on_practices": [{"introduction": "Many multiphysics problems culminate in large systems of coupled, nonlinear equations where simple iterative methods may fail to converge. Robustness is paramount, and achieving it requires strategies that guide the solver toward a solution even when far from it. This practice [@problem_id:3509753] explores the core of modern nonlinear solvers by tasking you with implementing two essential \"globalization\" techniques: backtracking line search and trust-region methods, which are designed to ensure convergence for challenging, tightly coupled systems.", "problem": "Consider the nonlinear system representing a coupled reaction–diffusion benchmark derived from conservation of energy and mass with source terms. Let $N \\in \\mathbb{N}$ denote the number of spatial nodes on a one-dimensional domain of length $L$, with uniform grid spacing $h = L/(N-1)$. The unknown vector is $U \\in \\mathbb{R}^{2N}$, which is the concatenation of the temperature field $u \\in \\mathbb{R}^{N}$ and the concentration field $v \\in \\mathbb{R}^{N}$. The residual function $F:\\mathbb{R}^{2N} \\rightarrow \\mathbb{R}^{2N}$ is defined componentwise by a finite-difference discretization of coupled reaction–diffusion equations with Dirichlet boundary conditions:\n- For interior indices $i \\in \\{1,2,\\dots,N-2\\}$,\n$$\nR_u[i] = -k_u \\frac{u[i-1] - 2u[i] + u[i+1]}{h^2} + q \\, v[i] \\, \\exp(\\beta \\, u[i]) - s_u,\n$$\n$$\nR_v[i] = -k_v \\frac{v[i-1] - 2v[i] + v[i+1]}{h^2} - q \\, v[i] \\, \\exp(\\beta \\, u[i]) + s_v,\n$$\nwhere $k_u > 0$ and $k_v > 0$ are diffusion coefficients, $q > 0$ is a reaction coefficient, and $\\beta > 0$ is a heat-activation coefficient. The source terms $s_u$ and $s_v$ are constants.\n- For boundary indices,\n$$\nR_u[0] = u[0] - u_{\\text{left}}, \\quad R_u[N-1] = u[N-1] - u_{\\text{right}},\n$$\n$$\nR_v[0] = v[0] - v_{\\text{left}}, \\quad R_v[N-1] = v[N-1] - v_{\\text{right}},\n$$\nenforcing Dirichlet boundary conditions.\n\nDefine the concatenated residual $F(U) = [R_u; R_v] \\in \\mathbb{R}^{2N}$ and the nonlinear least-squares merit function\n$$\n\\phi(U) = \\frac{1}{2}\\|F(U)\\|_2^2.\n$$\n\nYou must design and implement a globalization strategy that guarantees descent of $\\phi(U)$ for each iteration when computing updates for a nonlinear solver. The strategy must be chosen from either a backtracking line search with Armijo condition along a descent direction or a trust-region method using a Gauss–Newton model with dogleg steps. Your solver must:\n- Start from a given initial guess $U_0$.\n- Compute the Jacobian $J(U) \\in \\mathbb{R}^{2N \\times 2N}$ analytically from the definition of $F(U)$.\n- Use the Newton step $s$ given by solving $J(U)s = -F(U)$ when possible. If the Newton step fails to be a descent direction for $\\phi(U)$, you must fall back to a guaranteed descent direction, such as steepest descent $s = -\\nabla \\phi(U)$ where $\\nabla \\phi(U) = J(U)^\\top F(U)$.\n- For line search, enforce the Armijo condition\n$$\n\\phi(U + \\alpha s) \\le \\phi(U) + c_1 \\alpha \\nabla \\phi(U)^\\top s,\n$$\nwith $c_1 \\in (0,1)$ and backtracking parameter $\\tau \\in (0,1)$, decreasing $\\alpha$ until the condition holds.\n- For trust region, use the Gauss–Newton quadratic model\n$$\nm(s) = \\frac{1}{2}\\|F(U) + J(U)s\\|_2^2,\n$$\nconstruct a dogleg step within the trust-region radius $\\Delta > 0$, and accept steps only if they reduce $\\phi(U)$, updating $\\Delta$ by standard rules based on the ratio\n$$\n\\rho = \\frac{\\phi(U) - \\phi(U + s)}{m(0) - m(s)}.\n$$\nIf the trust-region step fails to reduce $\\phi(U)$ sufficiently, shrink $\\Delta$ and recompute the step.\n\nStarting from the chain rule and properties of the Euclidean norm, derive the expressions for $\\nabla \\phi(U)$ and the directional derivative of $\\phi(U)$ along the Newton direction, and show that an exact Newton step is a descent direction for $\\phi(U)$ whenever $F(U) \\neq 0$. Then, justify why the Armijo backtracking ensures descent when starting from a descent direction and why the trust-region acceptance criterion guarantees descent of $\\phi(U)$.\n\nHigh-Performance Computing (HPC) context: although the implementation runs in a sequential environment, you must structure the algorithm in terms of vectorized operations over degrees of freedom and block assembly for the Jacobian to reflect HPC-aware design (e.g., separating field-wise contributions and coupling terms), which is standard for multiphysics solvers.\n\nYour program must implement both globalization strategies and select which to use per test case. For numerical stability, you may clip arguments to the exponential to a bounded range to avoid overflow without altering the fundamental model.\n\nYour program must solve the nonlinear system for the following test suite and output the final values of $\\phi(U^\\ast)$ as floating-point numbers:\n\n- Test case $1$ (happy path): $N = 20$, $L = 1$, $k_u = 1.0$, $k_v = 0.8$, $q = 10.0$, $\\beta = 1.5$, $s_u = 1.0$, $s_v = 1.0$, $u_{\\text{left}} = 0.0$, $u_{\\text{right}} = 0.0$, $v_{\\text{left}} = 1.0$, $v_{\\text{right}} = 1.0$, initial guess $u_i = 0.0$, $v_i = 1.0$ for all $i$, method: line search with $c_1 = 10^{-4}$ and $\\tau = 0.5$.\n- Test case $2$ (stiff coupling): $N = 30$, $L = 1$, $k_u = 1.0$, $k_v = 1.0$, $q = 20.0$, $\\beta = 5.0$, $s_u = 1.0$, $s_v = 1.0$, $u_{\\text{left}} = 0.0$, $u_{\\text{right}} = 0.0$, $v_{\\text{left}} = 1.0$, $v_{\\text{right}} = 1.0$, initial guess $u_i = 0.0$, $v_i = 1.0$, method: trust region with initial radius $\\Delta = 1.0$, acceptance thresholds $\\eta_1 = 0.25$, $\\eta_2 = 0.75$.\n- Test case $3$ (weak reaction edge case): $N = 10$, $L = 1$, $k_u = 1.0$, $k_v = 1.0$, $q = 10^{-3}$, $\\beta = 1.5$, $s_u = 1.0$, $s_v = 1.0$, $u_{\\text{left}} = 0.0$, $u_{\\text{right}} = 0.0$, $v_{\\text{left}} = 1.0$, $v_{\\text{right}} = 1.0$, initial guess $u_i = 0.0$, $v_i = 1.0$, method: line search with $c_1 = 10^{-4}$ and $\\tau = 0.5$.\n- Test case $4$ (near singular diffusion in $v$): $N = 15$, $L = 1$, $k_u = 1.0$, $k_v = 10^{-2}$, $q = 5.0$, $\\beta = 2.0$, $s_u = 0.5$, $s_v = 1.0$, $u_{\\text{left}} = 0.0$, $u_{\\text{right}} = 0.0$, $v_{\\text{left}} = 1.0$, $v_{\\text{right}} = 1.0$, initial guess $u_i = 0.0$, $v_i = 1.0$, method: trust region with initial radius $\\Delta = 0.5$, acceptance thresholds $\\eta_1 = 0.25$, $\\eta_2 = 0.75$.\n\nFor all test cases, use a residual tolerance $\\|F(U)\\|_2 \\le 10^{-8}$ for convergence and a maximum of $50$ iterations. There are no physical unit conversions required for the final output; report only the scalar values of $\\phi(U^\\ast)$ as floating-point numbers.\n\nFinal Output Format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[r_1,r_2,r_3,r_4]$), where each $r_j$ is the final value of $\\phi(U^\\ast)$ for test case $j$ expressed as a floating-point number.", "solution": "The user-provided problem is a well-defined task in numerical analysis, specifically concerning the solution of a nonlinear system of equations arising from a multiphysics model. The problem statement is scientifically grounded, self-contained, and objective. It specifies a coupled reaction-diffusion system, which is a standard physical model, and provides all necessary parameters, equations, boundary conditions, and numerical methods (Newton's method with globalization via line search or trust regions). The requirements are formalizable into a computational algorithm. Therefore, the problem is deemed **valid**.\n\nThis solution proceeds in four parts:\n1.  **Theoretical Analysis**: We derive the necessary expressions for the nonlinear optimization problem and justify the descent properties of the proposed methods.\n2.  **Algorithmic Formulation**: We detail the structure of the Jacobian matrix, which is central to the Newton-based solver and its high-performance computing (HPC) considerations.\n3.  **Globalization Strategies**: We outline the implementation logic for both the Armijo backtracking line search and the trust-region method with a dogleg step.\n4.  **Numerical Implementation**: We discuss practical considerations for implementing the solver in a robust and efficient manner.\n\n### 1. Theoretical Analysis\n\nThe problem is to find a solution $U \\in \\mathbb{R}^{2N}$ to the nonlinear system $F(U) = 0$. This is framed as an optimization problem by minimizing the merit function $\\phi(U) = \\frac{1}{2}\\|F(U)\\|_2^2 = \\frac{1}{2}F(U)^\\top F(U)$. A solution to $F(U)=0$ corresponds to a global minimum of $\\phi(U)$ where $\\phi(U)=0$.\n\n**Gradient of the Merit Function**\nThe gradient of $\\phi(U)$ is required for the optimization algorithms. Using the chain rule, the $j$-th component of the gradient vector $\\nabla \\phi(U)$ is:\n$$\n\\frac{\\partial \\phi}{\\partial U_j} = \\frac{\\partial}{\\partial U_j} \\left( \\frac{1}{2} \\sum_{k=0}^{2N-1} F_k(U)^2 \\right) = \\sum_{k=0}^{2N-1} F_k(U) \\frac{\\partial F_k(U)}{\\partial U_j}\n$$\nThe term $\\frac{\\partial F_k(U)}{\\partial U_j}$ is the entry $(k,j)$ of the Jacobian matrix $J(U)$. In matrix notation, this becomes:\n$$\n\\nabla \\phi(U) = J(U)^\\top F(U)\n$$\nThis expression is fundamental for both line search and trust-region methods.\n\n**Newton Step as a Descent Direction**\nA direction $s$ is a descent direction for $\\phi$ at $U$ if the directional derivative is negative, i.e., $\\nabla \\phi(U)^\\top s < 0$. The Newton step $s_N$ is obtained by solving the linear system $J(U)s_N = -F(U)$. Assuming the Jacobian $J(U)$ is invertible, we have $s_N = -J(U)^{-1} F(U)$.\n\nThe directional derivative along the Newton step is:\n$$\n\\nabla \\phi(U)^\\top s_N = (J(U)^\\top F(U))^\\top (-J(U)^{-1} F(U)) = F(U)^\\top J(U) (-J(U)^{-1}) F(U) = -F(U)^\\top F(U) = -\\|F(U)\\|_2^2\n$$\nSince $\\|F(U)\\|_2^2 \\ge 0$, the directional derivative is non-positive. If $U$ is not a solution (i.e., $F(U) \\neq 0$), then $\\|F(U)\\|_2^2 > 0$, which implies $\\nabla \\phi(U)^\\top s_N  0$. Thus, the exact Newton step is a guaranteed descent direction for the merit function $\\phi(U)$ whenever $F(U) \\neq 0$ and the Jacobian is non-singular.\n\n**Guarantees of Globalization Strategies**\n-   **Armijo Backtracking Line Search**: Given a descent direction $s$ (e.g., the Newton step), the Armijo condition is $\\phi(U + \\alpha s) \\le \\phi(U) + c_1 \\alpha \\nabla \\phi(U)^\\top s$. Since $\\nabla \\phi(U)^\\top s  0$ and $c_1 \\in (0,1)$, the right-hand side is strictly less than $\\phi(U)$ for any $\\alpha  0$. The existence of a positive $\\alpha$ satisfying this condition is guaranteed by the definition of the derivative. The backtracking algorithm is designed to find such an $\\alpha$, ensuring that each step $U_{k+1} = U_k + \\alpha_k s_k$ yields $\\phi(U_{k+1})  \\phi(U_k)$.\n-   **Trust-Region Method**: A step $s$ is accepted if the ratio of actual to predicted reduction, $\\rho = \\frac{\\phi(U) - \\phi(U + s)}{m(0) - m(s)}$, is positive (in practice, greater than a small threshold $\\eta_0  0$). A positive $\\rho$ implies that the actual reduction $\\phi(U) - \\phi(U + s)$ has the same sign as the predicted reduction. Since the step $s$ is chosen to minimize the model $m(s)$, the predicted reduction $m(0) - m(s)$ is non-negative. Therefore, accepting a step with $\\rho>0$ directly ensures $\\phi(U+s)  \\phi(U)$, guaranteeing descent.\n\n### 2. Algorithmic Formulation and Jacobian Structure\n\nThe state vector is $U = [u_0, \\dots, u_{N-1}, v_0, \\dots, v_{N-1}]^\\top \\in \\mathbb{R}^{2N}$. The Jacobian $J(U)$ is a $2N \\times 2N$ matrix with a $2 \\times 2$ block structure corresponding to the couplings between the fields $u$ and $v$:\n$$\nJ(U) = \\begin{pmatrix} J_{uu}  J_{uv} \\\\ J_{vu}  J_{vv} \\end{pmatrix} = \\begin{pmatrix} \\frac{\\partial R_u}{\\partial u}  \\frac{\\partial R_u}{\\partial v} \\\\ \\frac{\\partial R_v}{\\partial u}  \\frac{\\partial R_v}{\\partial v} \\end{pmatrix}\n$$\nThis block structure is critical in HPC for multiphysics, as it allows for physics-based preconditioning and parallel assembly of matrix contributions.\n\n-   **Diagonal Blocks ($J_{uu}, J_{vv}$)**: These blocks represent the intra-field physics. For the interior nodes, the second-order finite difference stencil for diffusion results in a tridiagonal structure. The main diagonals are modified by the partial derivatives of the reaction term with respect to their own field.\n    -   $\\frac{\\partial R_u[i]}{\\partial u_i} = \\frac{2k_u}{h^2} + q \\beta v[i] e^{\\beta u[i]}$\n    -   $\\frac{\\partial R_v[i]}{\\partial v_i} = \\frac{2k_v}{h^2} - q e^{\\beta u[i]}$\n-   **Off-Diagonal Blocks ($J_{uv}, J_{vu}$)**: These blocks represent the inter-field coupling. In this model, the reaction term couples the fields locally.\n    -   $\\frac{\\partial R_u[i]}{\\partial v_i} = q e^{\\beta u[i]}$\n    -   $\\frac{\\partial R_v[i]}{\\partial u_i} = -q \\beta v[i] e^{\\beta u[i]}$\n    Since $\\frac{\\partial R_u[i]}{\\partial v_j}$ and $\\frac{\\partial R_v[i]}{\\partial u_j}$ are zero for $i \\neq j$, these coupling blocks are diagonal matrices.\n-   **Boundary Rows**: The Dirichlet boundary conditions $R_u[0] = u[0] - u_{\\text{left}}$ lead to rows in the Jacobian with a single entry of $1$ on the main diagonal (e.g., $\\frac{\\partial R_u[0]}{\\partial u_0} = 1$), making the corresponding rows of all other blocks zero.\n\nThis sparse, block structure is exploited in the implementation for efficient assembly.\n\n### 3. Implementation of Globalization Strategies\n\nThe core of the solver is a Newton iteration, globalized by one of two methods.\n\n-   **Line Search**: At each iteration, we compute the Newton step $s_N = -J(U)^{-1}F(U)$. We verify that it is a descent direction (i.e., $\\nabla\\phi^\\top s_N  0$). If not (which can happen due to severe ill-conditioning of $J(U)$), we fall back to the guaranteed descent direction of steepest descent, $s_{SD} = -\\nabla\\phi(U) = -J(U)^\\top F(U)$. With a valid descent direction $s$, we perform a backtracking search starting with step length $\\alpha=1.0$, reducing it by a factor $\\tau$ until the Armijo condition is met.\n\n-   **Trust Region**: This method defines a region of trust (a ball of radius $\\Delta$) around the current iterate, within which the quadratic model $m(s) = \\frac{1}{2}\\|F(U) + J(U)s\\|_2^2$ is considered a good approximation of $\\phi(U+s)$. The step $s$ is computed by approximately solving $\\min_{\\|s\\| \\le \\Delta} m(s)$. We use Powell's dogleg method to find this step. The dogleg path is a piecewise linear curve from the origin to the Cauchy point (the minimizer of $m(s)$ along the steepest descent direction) and then to the Newton point (the unconstrained minimizer of $m(s)$). The step $s$ is the point on this path that intersects the trust region boundary, or the Newton point if it lies within the region. The radius $\\Delta$ is adapted based on the agreement ratio $\\rho$, expanding for good steps and shrinking for poor ones.\n\n### 4. Numerical Implementation\n\nThe implementation is encapsulated in a Python class. Key considerations include:\n-   **Vectorization**: The residual $F(U)$ and Jacobian $J(U)$ are assembled using vectorized NumPy operations on array slices, avoiding slow Python loops. This is aligned with HPC-style programming. For example, the contributions to the diagonals for all interior nodes are computed in single array operations.\n-   **Numerical Stability**: The exponential term $e^{\\beta u_i}$ can cause overflow if $u_i$ becomes large. The argument $\\beta u_i$ is clipped to a machine-safe range (e.g., $[-100, 100]$) to prevent this without affecting the solution, as the exponential is already astronomically large or small at these bounds.\n-   **Linear Algebra**: The core computational cost is solving the linear system for the Newton step ($J s = -F$) and other matrix-vector products. We use `scipy.linalg.solve`, a robust and efficient direct solver suitable for the dense (but structured) matrices of these problem sizes.\n\nThe final program implements both strategies and selects the appropriate one for each test case as specified, running until convergence or a maximum number of iterations is reached and reporting the final merit function value $\\phi(U^\\ast)$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport math\nfrom scipy.linalg import solve as linsolve\n\nclass NonlinearReactionDiffusionSolver:\n    \"\"\"\n    Solves a coupled nonlinear reaction-diffusion system using a Newton method\n    with either a line search or trust-region globalization strategy.\n    \"\"\"\n    def __init__(self, N, L, ku, kv, q, beta, su, sv, u_left, u_right, v_left, v_right):\n        self.N = N\n        self.L = L\n        if N > 1:\n            self.h = L / (N - 1)\n            self.h2 = self.h**2\n        else: # Handle N=1 case\n            self.h = self.h2 = float('inf')\n\n        self.ku = ku\n        self.kv = kv\n        self.q = q\n        self.beta = beta\n        self.su = su\n        self.sv = sv\n\n        self.u_left = u_left\n        self.u_right = u_right\n        self.v_left = v_left\n        self.v_right = v_right\n        \n        self.exp_clip_range = (-100.0, 100.0)\n\n    def compute_F(self, U):\n        \"\"\"Computes the residual vector F(U).\"\"\"\n        N = self.N\n        u = U[:N]\n        v = U[N:]\n        F = np.zeros(2 * N)\n        Ru = F[:N]\n        Rv = F[N:]\n\n        # Interior nodes\n        if N > 2:\n            u_im1, u_i, u_ip1 = u[:-2], u[1:-1], u[2:]\n            v_im1, v_i, v_ip1 = v[:-2], v[1:-1], v[2:]\n\n            exp_arg = np.clip(self.beta * u_i, *self.exp_clip_range)\n            exp_term = self.q * v_i * np.exp(exp_arg)\n            \n            lap_u = (u_im1 - 2 * u_i + u_ip1) / self.h2\n            lap_v = (v_im1 - 2 * v_i + v_ip1) / self.h2\n\n            Ru[1:-1] = -self.ku * lap_u + exp_term - self.su\n            Rv[1:-1] = -self.kv * lap_v - exp_term + self.sv\n\n        # Boundary conditions\n        if N > 0:\n            Ru[0] = u[0] - self.u_left\n            Rv[0] = v[0] - self.v_left\n        if N > 1:\n            Ru[N-1] = u[N-1] - self.u_right\n            Rv[N-1] = v[N-1] - self.v_right\n\n        return F\n\n    def compute_J(self, U):\n        \"\"\"Computes the Jacobian matrix J(U) analytically.\"\"\"\n        N = self.N\n        u = U[:N]\n        v = U[N:]\n        J = np.zeros((2 * N, 2 * N))\n\n        if N == 0:\n            return J\n\n        # Set boundary condition rows first (value 1 on diagonal)\n        J[0, 0] = 1.0\n        J[N, N] = 1.0\n        if N > 1:\n            J[N-1, N-1] = 1.0\n            J[2*N-1, 2*N-1] = 1.0\n\n        if N > 2:\n            # Vectorized computation for interior nodes\n            rows = np.arange(1, N - 1)\n            u_interior = u[1:-1]\n            v_interior = v[1:-1]\n            exp_arg = np.clip(self.beta * u_interior, *self.exp_clip_range)\n            exp_val = np.exp(exp_arg)\n\n            # J_uu block\n            k_u_h2 = self.ku / self.h2\n            J[rows, rows - 1] = -k_u_h2\n            J[rows, rows + 1] = -k_u_h2\n            J[rows, rows] = 2.0 * k_u_h2 + self.q * self.beta * v_interior * exp_val\n            \n            # J_uv block\n            J[rows, N + rows] = self.q * exp_val\n            \n            # J_vu block\n            J[N + rows, rows] = -self.q * self.beta * v_interior * exp_val\n            \n            # J_vv block\n            k_v_h2 = self.kv / self.h2\n            J[N + rows, N + rows - 1] = -k_v_h2\n            J[N + rows, N + rows + 1] = -k_v_h2\n            J[N + rows, N + rows] = 2.0 * k_v_h2 - self.q * exp_val\n\n        return J\n        \n    def solve(self, U0, method, tol=1e-8, max_iter=50, **kwargs):\n        \"\"\"Main dispatcher for the solver.\"\"\"\n        if method == 'line_search':\n            return self._solve_linesearch(U0, tol, max_iter, **kwargs)\n        elif method == 'trust_region':\n            return self._solve_trustregion(U0, tol, max_iter, **kwargs)\n        else:\n            raise ValueError(\"Unknown method: {}\".format(method))\n\n    def _solve_linesearch(self, U, tol, max_iter, c1, tau):\n        for _ in range(max_iter):\n            F = self.compute_F(U)\n            if np.linalg.norm(F)  tol:\n                break\n            \n            J = self.compute_J(U)\n            \n            try:\n                s_N = linsolve(J, -F)\n            except np.linalg.LinAlgError:\n                s_N = None\n\n            grad_phi = J.T @ F\n            \n            if s_N is not None and (grad_phi.T @ s_N)  0:\n                s = s_N\n            else:\n                s = -grad_phi\n\n            phi_U = 0.5 * (F @ F)\n            grad_phi_s = grad_phi.T @ s\n            if grad_phi_s >= 0:\n                break # Not a descent direction, cannot proceed\n\n            alpha = 1.0\n            \n            for _ in range(10): # Backtracking limited to 10 steps\n                U_new = U + alpha * s\n                F_new = self.compute_F(U_new)\n                phi_U_new = 0.5 * (F_new @ F_new)\n                \n                if phi_U_new = phi_U + c1 * alpha * grad_phi_s:\n                    break\n                alpha *= tau\n            else: # If loop completes without break\n                break # Failed to find suitable step size\n\n            U = U_new\n        \n        final_F = self.compute_F(U)\n        return 0.5 * (final_F @ final_F)\n\n    def _dogleg_step(self, F, J, delta):\n        try:\n            p_N = linsolve(J, -F)\n        except np.linalg.LinAlgError:\n            p_N = None\n\n        if p_N is not None and np.linalg.norm(p_N) = delta:\n            return p_N\n\n        grad_phi = J.T @ F\n        norm_grad_phi = np.linalg.norm(grad_phi)\n        \n        if norm_grad_phi  1e-12:\n            return np.zeros_like(F)\n        \n        # Cauchy point calculation\n        J_grad_phi = J @ grad_phi\n        alpha_denom = J_grad_phi @ J_grad_phi\n        if alpha_denom  1e-12:\n            p_C = -grad_phi # Fallback to steepest descent direction\n        else:\n            alpha = (grad_phi @ grad_phi) / alpha_denom\n            p_C = -alpha * grad_phi\n        \n        norm_pC = np.linalg.norm(p_C)\n\n        if norm_pC >= delta:\n            return delta * (-grad_phi) / norm_grad_phi\n\n        if p_N is None: # Newton step failed, just use Cauchy point\n            return p_C\n            \n        # Dogleg path between Cauchy and Newton steps\n        d = p_N - p_C\n        a = d @ d\n        b = 2 * (p_C @ d)\n        c = (p_C @ p_C) - delta**2\n        \n        discriminant = b**2 - 4 * a * c\n        if discriminant  0:\n            # Should be rare, but as a failsafe return truncated Newton step\n            return delta * p_N / np.linalg.norm(p_N)\n        \n        beta = (-b + math.sqrt(discriminant)) / (2 * a)\n        return p_C + beta * d\n\n    def _solve_trustregion(self, U, tol, max_iter, delta, eta1, eta2):\n        delta_max = 1e3 \n        eta_accept = 1e-4\n\n        for _ in range(max_iter):\n            F = self.compute_F(U)\n            norm_F = np.linalg.norm(F)\n            if norm_F = tol:\n                break\n\n            J = self.compute_J(U)\n            \n            # Inner loop to find an acceptable step\n            for _ in range(10): # Max 10 attempts to find a step per outer iter\n                s = self._dogleg_step(F, J, delta)\n                if np.linalg.norm(s)  1e-12: break\n                \n                U_new = U + s\n                F_new = self.compute_F(U_new)\n                \n                actual_reduction = 0.5 * norm_F**2 - 0.5 * np.linalg.norm(F_new)**2\n                pred_reduction = 0.5 * (norm_F**2 - np.linalg.norm(F + J @ s)**2)\n                \n                if abs(pred_reduction)  1e-12:\n                    rho = -1.0 if actual_reduction > 1e-12 else 0.0\n                else:\n                    rho = actual_reduction / pred_reduction\n                \n                if rho  eta1: delta *= 0.25\n                elif rho > eta2 and np.isclose(np.linalg.norm(s), delta, rtol=1e-2):\n                    delta = min(2.0 * delta, delta_max)\n                \n                if rho > eta_accept:\n                    U = U_new\n                    break\n                \n                if delta  1e-9: break\n            else: # Inner loop finished, no acceptable step found\n                break\n\n            if np.linalg.norm(s)  1e-12 or delta  1e-9: break\n        \n        final_F = self.compute_F(U)\n        return 0.5 * (final_F @ final_F)\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    test_cases = [\n        # Test case 1\n        {'params': {'N': 20, 'L': 1.0, 'ku': 1.0, 'kv': 0.8, 'q': 10.0, 'beta': 1.5, 'su': 1.0, 'sv': 1.0, 'u_left': 0.0, 'u_right': 0.0, 'v_left': 1.0, 'v_right': 1.0},\n         'solver_opts': {'method': 'line_search', 'c1': 1e-4, 'tau': 0.5},\n         'initial_guess': (0.0, 1.0)},\n        # Test case 2\n        {'params': {'N': 30, 'L': 1.0, 'ku': 1.0, 'kv': 1.0, 'q': 20.0, 'beta': 5.0, 'su': 1.0, 'sv': 1.0, 'u_left': 0.0, 'u_right': 0.0, 'v_left': 1.0, 'v_right': 1.0},\n         'solver_opts': {'method': 'trust_region', 'delta': 1.0, 'eta1': 0.25, 'eta2': 0.75},\n         'initial_guess': (0.0, 1.0)},\n        # Test case 3\n        {'params': {'N': 10, 'L': 1.0, 'ku': 1.0, 'kv': 1.0, 'q': 1e-3, 'beta': 1.5, 'su': 1.0, 'sv': 1.0, 'u_left': 0.0, 'u_right': 0.0, 'v_left': 1.0, 'v_right': 1.0},\n         'solver_opts': {'method': 'line_search', 'c1': 1e-4, 'tau': 0.5},\n         'initial_guess': (0.0, 1.0)},\n        # Test case 4\n        {'params': {'N': 15, 'L': 1.0, 'ku': 1.0, 'kv': 1e-2, 'q': 5.0, 'beta': 2.0, 'su': 0.5, 'sv': 1.0, 'u_left': 0.0, 'u_right': 0.0, 'v_left': 1.0, 'v_right': 1.0},\n         'solver_opts': {'method': 'trust_region', 'delta': 0.5, 'eta1': 0.25, 'eta2': 0.75},\n         'initial_guess': (0.0, 1.0)},\n    ]\n\n    results = []\n    for case in test_cases:\n        N = case['params']['N']\n        u0_val, v0_val = case['initial_guess']\n        \n        U0 = np.zeros(2 * N)\n        U0[:N] = u0_val\n        U0[N:] = v0_val\n        \n        solver = NonlinearReactionDiffusionSolver(**case['params'])\n        result = solver.solve(U0, **case['solver_opts'])\n        results.append(f\"{result:.6e}\")\n    \n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3509753"}, {"introduction": "Once a robust numerical method is established, the next challenge is achieving performance at scale. When a simulation is distributed across thousands of processors, the time spent communicating data can dwarf the time spent on computation. This exercise [@problem_id:3509742] focuses on optimizing these critical communication patterns by having you apply a classic performance model to compare two different algorithms for a global reduction, a cornerstone operation in parallel scientific computing. You will gain practical insight into how algorithmic choices must balance network latency and bandwidth to maintain scalability.", "problem": "A monolithic multiphysics finite element solver is executed on $p=1024$ Message Passing Interface (MPI) ranks on a high-performance computing (HPC) cluster. At each nonlinear iteration, the solver performs an inner-product over a block of $n$ bytes per rank and then a global reduction to aggregate the partial results. You are tasked with comparing two global reduction strategies for this communication step: a conventional tree-based Allreduce and a ring-based pipelined reduce-scatter followed by an allgather.\n\nAssume the following communication performance model (the Hockney model): the time to send a message of size $m$ bytes between two ranks is $T_{\\mathrm{msg}}(m)=\\alpha+\\beta m$, where $\\alpha$ is the per-message startup latency and $\\beta$ is the inverse bandwidth in seconds per byte. Assume full duplex links and that each algorithm proceeds in synchronized communication rounds according to their structure. Use the following parameters: $p=1024$, $n=16\\times 2^{20}\\ \\mathrm{bytes}$, $\\alpha=1.5\\times 10^{-6}\\ \\mathrm{s}$, and $\\beta=8.0\\times 10^{-11}\\ \\mathrm{s/byte}$.\n\nThe conventional tree-based Allreduce uses recursive doubling on $p$ ranks and progresses in $\\lceil \\log_{2}(p)\\rceil$ communication rounds, sending the full $n$-byte payload in each round. The ring-based pipelined approach first performs a reduce-scatter over $p-1$ rounds, where in each round each rank sends a contiguous chunk of size $n/p$ bytes to its neighbor on a ring, followed by an allgather over another $p-1$ rounds with the same per-round message size; both phases are fully pipelined.\n\nStarting from the Hockney model and the structural descriptions above, derive the total time for each algorithm and compute the performance gain, defined as the speedup $S=T_{\\mathrm{tree}}/T_{\\mathrm{ring}}$, where $T_{\\mathrm{tree}}$ is the time of the conventional tree-based Allreduce and $T_{\\mathrm{ring}}$ is the time of the ring-based pipelined reduction. Express $S$ as a dimensionless number and round your answer to three significant figures.", "solution": "The user has asked for a comparison of two parallel reduction algorithms, tree-based Allreduce and a pipelined ring-based method, using the Hockney model for communication time. The problem is first validated to ensure it is scientifically sound, self-contained, and well-posed.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- Number of MPI ranks: $p=1024$.\n- Data size per rank: $n=16\\times 2^{20}\\ \\mathrm{bytes}$.\n- Communication performance model (Hockney model): $T_{\\mathrm{msg}}(m)=\\alpha+\\beta m$ for a message of size $m$ bytes.\n- Latency parameter: $\\alpha=1.5\\times 10^{-6}\\ \\mathrm{s}$.\n- Inverse bandwidth parameter: $\\beta=8.0\\times 10^{-11}\\ \\mathrm{s/byte}$.\n- Algorithm 1 (Tree-based Allreduce):\n    - Number of communication rounds: $\\lceil \\log_{2}(p)\\rceil$.\n    - Message size per round: $n$.\n- Algorithm 2 (Ring-based pipelined reduction):\n    - Structure: A reduce-scatter phase followed by an allgather phase.\n    - Reduce-scatter: $p-1$ rounds, message size per round is $n/p$.\n    - Allgather: $p-1$ rounds, message size per round is $n/p$.\n- Required output: Speedup $S=T_{\\mathrm{tree}}/T_{\\mathrm{ring}}$, rounded to three significant figures.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded in the domain of high-performance computing (HPC) and parallel algorithms. The Hockney model is a standard, first-order approximation of network performance. The described algorithms (tree-based and ring-based reductions) are canonical methods taught and used in parallel computing. The provided parameters for latency, bandwidth, processor count, and data size are realistic for modern supercomputing environments. The problem statement is objective, precise, and self-contained, providing all necessary information to derive a unique solution. It does not violate any scientific principles, is not ill-posed, and is directly relevant to the specified topic of HPC for multiphysics simulations, where large-scale reductions are a common performance-critical kernel.\n\n**Step 3: Verdict and Action**\nThe problem is deemed **valid**. Proceeding to the solution.\n\n### Derivation of Communication Times\n\nThe time to send a message of size $m$ bytes is given by the Hockney model:\n$$T_{\\mathrm{msg}}(m) = \\alpha + \\beta m$$\n\n**1. Time for Tree-based Allreduce ($T_{\\mathrm{tree}}$)**\n\nThe problem specifies that the tree-based Allreduce algorithm proceeds in $\\lceil \\log_{2}(p)\\rceil$ synchronized communication rounds. In each round, a message of the full size $n$ is communicated. The total time is the number of rounds multiplied by the time for a single round's communication.\n\nGiven $p=1024$, we have $p=2^{10}$, so $\\log_{2}(p) = \\log_{2}(1024) = 10$. The number of rounds is $\\lceil \\log_{2}(1024) \\rceil = 10$.\nThe time for one round, with message size $n$, is $T_{\\mathrm{msg}}(n) = \\alpha + \\beta n$.\nTherefore, the total time for the tree-based algorithm is:\n$$T_{\\mathrm{tree}} = (\\log_{2} p) \\times T_{\\mathrm{msg}}(n) = (\\log_{2} p)(\\alpha + \\beta n)$$\n\n**2. Time for Ring-based Pipelined Reduction ($T_{\\mathrm{ring}}$)**\n\nThis algorithm consists of two sequential phases: a reduce-scatter and an allgather.\n\nFor the reduce-scatter phase, the algorithm proceeds in $p-1$ rounds. In each round, every rank sends a message of size $n/p$ to its neighbor. The time for one such round is $T_{\\mathrm{msg}}(n/p) = \\alpha + \\beta (n/p)$. Since there are $p-1$ such synchronized rounds, the total time for the reduce-scatter phase is:\n$$T_{\\mathrm{reduce-scatter}} = (p-1) \\times T_{\\mathrm{msg}}\\left(\\frac{n}{p}\\right) = (p-1)\\left(\\alpha + \\beta \\frac{n}{p}\\right)$$\n\nThe allgather phase is described as having the same structure: $p-1$ rounds with a message size of $n/p$ per round. Thus, its time is identical to the reduce-scatter phase:\n$$T_{\\mathrm{allgather}} = (p-1)\\left(\\alpha + \\beta \\frac{n}{p}\\right)$$\n\nThe total time for the ring-based algorithm is the sum of the times for the two phases:\n$$T_{\\mathrm{ring}} = T_{\\mathrm{reduce-scatter}} + T_{\\mathrm{allgather}} = 2(p-1)\\left(\\alpha + \\beta \\frac{n}{p}\\right)$$\n\n### Numerical Calculation\n\nThe given parameters are:\n- $p = 1024$\n- $n = 16 \\times 2^{20} = 16 \\times 1048576 = 16777216 \\ \\mathrm{bytes}$\n- $\\alpha = 1.5 \\times 10^{-6} \\ \\mathrm{s}$\n- $\\beta = 8.0 \\times 10^{-11} \\ \\mathrm{s/byte}$\n\n**Calculate $T_{\\mathrm{tree}}$:**\nFirst, we compute the term $\\beta n$:\n$$\\beta n = (8.0 \\times 10^{-11} \\ \\mathrm{s/byte}) \\times (16777216 \\ \\mathrm{bytes}) = 0.00134217728 \\ \\mathrm{s}$$\nNow, substitute the values into the expression for $T_{\\mathrm{tree}}$:\n$$T_{\\mathrm{tree}} = 10 \\times (1.5 \\times 10^{-6} \\ \\mathrm{s} + 0.00134217728 \\ \\mathrm{s})$$\n$$T_{\\mathrm{tree}} = 10 \\times (0.00134367728 \\ \\mathrm{s}) = 0.0134367728 \\ \\mathrm{s}$$\n\n**Calculate $T_{\\mathrm{ring}}$:**\nFirst, we find the message size per round, $n/p$:\n$$\\frac{n}{p} = \\frac{16 \\times 2^{20}}{1024} = \\frac{16 \\times 2^{20}}{2^{10}} = 16 \\times 2^{10} = 16384 \\ \\mathrm{bytes}$$\nNext, compute the term $\\beta (n/p)$:\n$$\\beta \\frac{n}{p} = (8.0 \\times 10^{-11} \\ \\mathrm{s/byte}) \\times (16384 \\ \\mathrm{bytes}) = 1.31072 \\times 10^{-6} \\ \\mathrm{s}$$\nNow, substitute the values into the expression for $T_{\\mathrm{ring}}$:\n$$T_{\\mathrm{ring}} = 2 \\times (1024 - 1) \\times \\left(1.5 \\times 10^{-6} \\ \\mathrms + 1.31072 \\times 10^{-6} \\ \\mathrms\\right)$$\n$$T_{\\mathrm{ring}} = 2 \\times 1023 \\times (2.81072 \\times 10^{-6} \\ \\mathrms) = 2046 \\times 2.81072 \\times 10^{-6} \\ \\mathrms = 0.00575073312 \\ \\mathrm{s}$$\n\n### Performance Gain (Speedup)\n\nThe performance gain is the speedup $S$, defined as the ratio of the two communication times:\n$$S = \\frac{T_{\\mathrm{tree}}}{T_{\\mathrm{ring}}}$$\n$$S = \\frac{0.0134367728}{0.00575073312} \\approx 2.33649$$\n\nThe problem requires the answer to be rounded to three significant figures.\n$$S \\approx 2.34$$\nThis result indicates that for this specific configuration of parameters (large data size and high process count), the pipelined ring algorithm is more than twice as fast as the conventional tree-based algorithm described. This is because the ring algorithm effectively pipelines the transfer of many small messages, better balancing the latency and bandwidth costs, whereas the tree algorithm becomes heavily bandwidth-limited by sending the entire large payload in each of its few steps.", "answer": "$$\\boxed{2.34}$$", "id": "3509742"}, {"introduction": "Efficiently exploiting the massive parallelism of modern hardware like GPUs requires careful management of dependencies between computational tasks. During the assembly of matrices and vectors in a multiphysics code, for instance, concurrent writes to the same memory location can create race conditions, leading to incorrect results. This final practice [@problem_id:3509741] demonstrates how to formalize and solve this scheduling problem by modeling task dependencies as a graph, applying a coloring algorithm to find conflict-free task groups, and then predicting the performance of this strategy using a hardware occupancy model.", "problem": "Consider a parallel assembly of a block matrix arising from a multiphysics coupled simulation. Each assembly task writes into a subset of block entries, and tasks that write into overlapping locations must not execute concurrently to avoid race conditions. Model the potential write conflicts by an undirected simple graph $G = (V, E)$ where each vertex $v \\in V$ represents a single assembly task and an edge $(u, v) \\in E$ represents a conflict between tasks $u$ and $v$ that would cause a race condition if executed simultaneously. A coloring of $G$ is a function $c : V \\rightarrow \\{1, 2, \\ldots, \\chi\\}$ such that adjacent vertices are assigned distinct colors; each color class $C_k = \\{v \\in V \\mid c(v) = k\\}$ can be executed concurrently without race conditions.\n\nDefine the coupling degree $d$ of the application as the maximum degree in $G$, that is $d = \\max_{v \\in V} \\deg(v)$. You must:\n\n1. Formulate a coupling-aware greedy coloring strategy for $G$ that assigns colors to tasks to avoid conflicts during assembly.\n2. Prove an upper bound on the number of colors $\\chi$ produced by any greedy coloring in terms of the coupling degree $d$.\n3. Quantify the impact of color count and per-task resource usage on a single-kernel occupancy model for a Graphics Processing Unit (GPU)-like architecture. Assume the following resource limits per Streaming Multiprocessor (SM): a maximum of $T_{\\mathrm{SM}}$ threads, $R_{\\mathrm{SM}}$ registers, $M_{\\mathrm{SM}}$ bytes of shared memory, and a maximum of $B_{\\max}$ resident blocks. Let each task be executed by a block of $B$ threads, using $r$ registers per thread and $s$ bytes of shared memory per block. For a given color class of size $T_c$ tasks, the resident blocks per SM are bounded by\n$$\nb_{\\mathrm{res}} = \\min\\left(\\left\\lfloor \\frac{T_{\\mathrm{SM}}}{B} \\right\\rfloor, \\left\\lfloor \\frac{R_{\\mathrm{SM}}}{r B} \\right\\rfloor, \\left\\lfloor \\frac{M_{\\mathrm{SM}}}{s} \\right\\rfloor, B_{\\max}\\right),\n$$\nand the occupancy fraction for that color is modeled as\n$$\n\\mathrm{occ} = \\left(\\frac{b_{\\mathrm{res}} B}{T_{\\mathrm{SM}}}\\right) \\cdot \\min\\left(1, \\frac{T_c}{b_{\\mathrm{res}} S}\\right),\n$$\nwhere $S$ is the number of SMs. The term $\\frac{b_{\\mathrm{res}} B}{T_{\\mathrm{SM}}}$ captures the resource-limited occupancy per SM, and $\\min\\left(1, \\frac{T_c}{b_{\\mathrm{res}} S}\\right)$ accounts for supply-limited occupancy due to insufficient tasks to fully populate all SMs at the resource-limited residency.\n\nConstruct $G$ for several test cases, apply the greedy coloring, compute the number of colors used $\\chi$, compute the theoretical upper bound $U = d + 1$, test whether $\\chi \\leq U$, and compute the average occupancy across all colors by averaging the $\\mathrm{occ}$ values over color classes. All angle measures are dimensionless, and physical memory units must be treated in bytes.\n\nYour program must implement the following test suite, each described by a graph type and hardware parameters:\n\n- Test case $1$ (two-dimensional grid graph):\n    - Graph: $G$ is a $10 \\times 10$ grid graph with $|V| = 100$, each interior vertex has degree $4$, so $d = 4$.\n    - Hardware: $S = 10$, $T_{\\mathrm{SM}} = 2048$, $R_{\\mathrm{SM}} = 65536$, $M_{\\mathrm{SM}} = 98304$, $B_{\\max} = 16$.\n    - Kernel parameters: $B = 128$ threads per block, $r = 64$ registers per thread, $s = 4096$ bytes shared memory per block.\n\n- Test case $2$ (path graph):\n    - Graph: $G$ is a path on $32$ vertices, so $|V| = 32$ and $d = 2$.\n    - Hardware: $S = 2$, $T_{\\mathrm{SM}} = 2048$, $R_{\\mathrm{SM}} = 65536$, $M_{\\mathrm{SM}} = 98304$, $B_{\\max} = 16$.\n    - Kernel parameters: $B = 64$, $r = 32$, $s = 2048$.\n\n- Test case $3$ (complete graph):\n    - Graph: $G$ is a complete graph on $16$ vertices, so $|V| = 16$ and $d = 15$.\n    - Hardware: $S = 4$, $T_{\\mathrm{SM}} = 2048$, $R_{\\mathrm{SM}} = 65536$, $M_{\\mathrm{SM}} = 98304$, $B_{\\max} = 16$.\n    - Kernel parameters: $B = 256$, $r = 128$, $s = 8192$.\n\n- Test case $4$ (random bounded-degree graph):\n    - Graph: $G$ is a random undirected graph with $|V| = 200$ and maximum degree bounded by $d = 8$.\n    - Hardware: $S = 16$, $T_{\\mathrm{SM}} = 2048$, $R_{\\mathrm{SM}} = 65536$, $M_{\\mathrm{SM}} = 102400$, $B_{\\max} = 16$.\n    - Kernel parameters: $B = 128$, $r = 64$, $s = 12288$.\n\nFor each test case, your program must output a list $[ \\chi, U, \\text{ok}, \\overline{\\mathrm{occ}} ]$ where $\\chi$ is the number of colors used by the greedy coloring (an integer), $U$ is the upper bound $d+1$ (an integer), $\\text{ok}$ is a boolean indicating whether $\\chi \\leq U$, and $\\overline{\\mathrm{occ}}$ is the average occupancy across colors (a float). Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[result1,result2,result3,result4]$), where each $resulti$ is itself a list formatted similarly. No additional text should be printed.", "solution": "The problem presents a scenario from high-performance computing for multiphysics simulations, where task scheduling with dependencies must be optimized. The dependencies, arising from potential race conditions during the parallel assembly of a block matrix, are modeled as an undirected simple graph $G = (V, E)$. Each vertex $v \\in V$ represents an assembly task, and an edge $(u, v) \\in E$ indicates that tasks $u$ and $v$ cannot be executed concurrently. This problem can be solved by finding a valid vertex coloring of the graph $G$. A color class, which is the set of all vertices assigned the same color, represents a group of tasks that can be executed in parallel without conflict.\n\nThe solution will be presented in three parts as requested: first, the formulation of a greedy coloring algorithm; second, a proof for the upper bound on the number of colors required; and third, the application of a specified GPU occupancy model to quantify the performance of the parallel execution of color classes.\n\n**Part 1: Coupling-Aware Greedy Coloring Strategy**\n\nA greedy coloring algorithm provides a straightforward and computationally efficient method for assigning colors to the vertices of a graph. The \"coupling-aware\" nature of the strategy is inherent in the coloring process itself, as the graph's edges (representing the coupling-induced conflicts) directly constrain the color assignments. The algorithm proceeds as follows:\n\nLet the vertices of the graph $G$ be ordered arbitrarily as $v_1, v_2, \\ldots, v_n$, where $n = |V|$. The algorithm iterates through this sequence, assigning a color to each vertex.\n\nThe algorithm is defined as:\n1.  Initialize an empty color assignment for all vertices. For programmatic purposes, we can represent uncolored vertices with a special value, e.g., $0$. All colors are positive integers starting from $1$.\n2.  For $i$ from $1$ to $n$:\n    a.  Consider the vertex $v_i$. Let $N(v_i)$ be the set of its neighbors.\n    b.  Identify the set of colors already assigned to the neighbors of $v_i$: \n        $$\n        C_{\\text{forbidden}}(v_i) = \\{ c(v_j) \\mid v_j \\in N(v_i) \\text{ and } v_j \\text{ has been colored} \\}\n        $$\n    c.  Assign to $v_i$ the smallest positive integer color not present in $C_{\\text{forbidden}}(v_i)$:\n        $$\n        c(v_i) = \\min \\{ k \\in \\mathbb{Z}^+ \\mid k \\notin C_{\\text{forbidden}}(v_i) \\}\n        $$\n3.  After iterating through all vertices, the total number of distinct colors used is denoted by $\\chi$.\n\nThis algorithm guarantees a valid coloring, i.e., $c(u) \\neq c(v)$ for any edge $(u, v) \\in E$, because when a vertex is colored, its color is explicitly chosen to be different from the colors of all its already-colored neighbors. Any subsequent neighbor to be colored will, by the same logic, avoid the color of the current vertex.\n\n**Part 2: Upper Bound on the Number of Colors**\n\nA fundamental result in graph theory provides an upper bound on the number of colors, $\\chi$, used by any greedy coloring algorithm. This bound is expressed in terms of the coupling degree, $d$, which is defined as the maximum degree of any vertex in the graph: $d = \\max_{v \\in V} \\deg(v)$. We will prove that $\\chi \\le d + 1$.\n\n**Proof:**\nLet the vertices of the graph $G$ be ordered $v_1, v_2, \\ldots, v_n$ for the greedy algorithm. Consider an arbitrary vertex $v_i$ in this sequence. Let $\\deg(v_i)$ be its degree. By the definition of the maximum degree $d$, we have $\\deg(v_i) \\le d$.\n\nWhen the algorithm assigns a color to $v_i$, it considers only the neighbors of $v_i$ that appear before it in the ordering, i.e., $\\{ v_j \\in N(v_i) \\mid j  i \\}$. The number of such neighbors is at most $\\deg(v_i)$. Therefore, at the moment of coloring $v_i$, at most $\\deg(v_i)$ of its neighbors have already been assigned a color.\n\nIn the worst-case scenario, each of these already-colored neighbors has a distinct color. This means there are at most $\\deg(v_i)$ forbidden colors for $v_i$. The set of available colors is $\\{1, 2, 3, \\ldots\\}$. The greedy algorithm chooses the smallest positive integer not in the set of forbidden colors. The largest color that might need to be assigned to $v_i$ occurs when its neighbors have been assigned the colors $\\{1, 2, \\ldots, \\deg(v_i)\\}$. In this case, the smallest available color is $\\deg(v_i) + 1$.\n\nSince this holds for any vertex $v_i$, the color assigned to it, $c(v_i)$, must satisfy:\n$$\nc(v_i) \\le \\deg(v_i) + 1\n$$\nBecause $\\deg(v_i) \\le d$ for all $v \\in V$, we have:\n$$\nc(v_i) \\le d + 1\n$$\nThis means no vertex will ever be assigned a color greater than $d+1$. Consequently, the total number of colors used, $\\chi = \\max_{v \\in V} c(v)$, is bounded by:\n$$\n\\chi \\le d + 1\n$$\nThis completes the proof. The test `ok` = $(\\chi \\le U)$ where $U = d+1$ must therefore always be true for a correctly implemented greedy coloring algorithm.\n\n**Part 3: GPU Occupancy Quantification**\n\nThe problem provides a model to estimate the occupancy of a GPU-like architecture when executing tasks from a single color class. Occupancy is a measure of how effectively the parallel resources of the hardware are utilized.\n\nGiven the hardware limits per Streaming Multiprocessor (SM) — $T_{\\mathrm{SM}}$ (threads), $R_{\\mathrm{SM}}$ (registers), $M_{\\mathrm{SM}}$ (shared memory), and $B_{\\max}$ (blocks) — and the resource requirements per task (executed as a block) — $B$ (threads/block), $r$ (registers/thread), and $s$ (shared memory/block) — we first determine the maximum number of resident blocks per SM, $b_{\\mathrm{res}}$. This is limited by the most constrained resource:\n$$\nb_{\\mathrm{res}} = \\min\\left(\\left\\lfloor \\frac{T_{\\mathrm{SM}}}{B} \\right\\rfloor, \\left\\lfloor \\frac{R_{\\mathrm{SM}}}{r B} \\right\\rfloor, \\left\\lfloor \\frac{M_{\\mathrm{SM}}}{s} \\right\\rfloor, B_{\\max}\\right)\n$$\nThe term $rB$ represents the total registers required by a block. The floors ($\\lfloor \\cdot \\rfloor$) are necessary because only whole blocks can be scheduled.\n\nOnce $b_{\\mathrm{res}}$ is determined, the occupancy for a given color class $C_k$ is calculated. Let $T_c = |C_k|$ be the size of the color class (number of tasks), and let $S$ be the total number of SMs in the GPU. The occupancy fraction, $\\mathrm{occ}$, is given by:\n$$\n\\mathrm{occ} = \\left(\\frac{b_{\\mathrm{res}} B}{T_{\\mathrm{SM}}}\\right) \\cdot \\min\\left(1, \\frac{T_c}{b_{\\mathrm{res}} S}\\right)\n$$\nThis formula has two components:\n1.  The resource-limited occupancy, $\\frac{b_{\\mathrm{res}} B}{T_{\\mathrm{SM}}}$, which is the fraction of an SM's maximum thread capacity that is utilized by the resident blocks. It cannot exceed $1$.\n2.  The supply-limited occupancy factor, $\\min\\left(1, \\frac{T_c}{b_{\\mathrm{res}} S}\\right)$. The term $b_{\\mathrm{res}} S$ is the total number of blocks the entire GPU can run concurrently. If the number of available tasks $T_c$ is less than this capacity, the GPU is underutilized, and this factor will be less than $1$.\n\nThe overall performance across all stages of the parallel assembly is evaluated by the average occupancy, $\\overline{\\mathrm{occ}}$, over all $\\chi$ color classes:\n$$\n\\overline{\\mathrm{occ}} = \\frac{1}{\\chi} \\sum_{k=1}^{\\chi} \\mathrm{occ}_k\n$$\nwhere $\\mathrm{occ}_k$ is the occupancy calculated for color class $C_k$.\n\nTo solve the given test cases, the procedure is to first construct the specified graph, then apply the greedy coloring algorithm to find the colors and hence the color classes. With the graph's maximum degree $d$ and the number of colors $\\chi$, we compute the bound $U = d+1$ and verify $\\chi \\le U$. Finally, using the provided hardware and kernel parameters, we compute the occupancy for each color class and average them to obtain $\\overline{\\mathrm{occ}}$.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final results.\n    \"\"\"\n\n    def greedy_coloring(adj):\n        \"\"\"\n        Applies a greedy coloring algorithm to a graph given its adjacency list.\n        Vertices are colored in the order 0, 1, 2, ...\n        Colors are positive integers starting from 1.\n        \"\"\"\n        num_vertices = len(adj)\n        colors = [0] * num_vertices\n        \n        for i in range(num_vertices):\n            neighbor_colors = {colors[neighbor] for neighbor in adj[i] if colors[neighbor] != 0}\n            \n            color = 1\n            while color in neighbor_colors:\n                color += 1\n            colors[i] = color\n            \n        return colors\n\n    def calculate_metrics(graph_params, hw_params, kernel_params):\n        \"\"\"\n        Generates graph, performs coloring, and calculates all required metrics.\n        \"\"\"\n        # Step 1: Generate the graph and find its max degree\n        graph_type = graph_params['type']\n        V = graph_params['V']\n        adj = [[] for _ in range(V)]\n\n        if graph_type == 'grid':\n            rows, cols = graph_params['dim']\n            for r in range(rows):\n                for c in range(cols):\n                    u = r * cols + c\n                    if r + 1  rows:\n                        v = (r + 1) * cols + c\n                        adj[u].append(v)\n                        adj[v].append(u)\n                    if c + 1  cols:\n                        v = r * cols + (c + 1)\n                        adj[u].append(v)\n                        adj[v].append(u)\n        elif graph_type == 'path':\n            for i in range(V - 1):\n                adj[i].append(i + 1)\n                adj[i + 1].append(i)\n        elif graph_type == 'complete':\n            for i in range(V):\n                for j in range(i + 1, V):\n                    adj[i].append(j)\n                    adj[j].append(i)\n        elif graph_type == 'random_bounded':\n            max_deg_bound = graph_params['d']\n            target_edges = graph_params['E_target']\n            np.random.seed(42)\n            degrees = np.zeros(V, dtype=int)\n            edges_added = 0\n            max_trials = V * (V-1) # Sufficiently large number of trials\n\n            for _ in range(max_trials):\n                if edges_added >= target_edges:\n                    break\n                u, v = np.random.randint(0, V, 2)\n                if u == v or v in adj[u]:\n                    continue\n                if degrees[u]  max_deg_bound and degrees[v]  max_deg_bound:\n                    adj[u].append(v)\n                    adj[v].append(u)\n                    degrees[u] += 1\n                    degrees[v] += 1\n                    edges_added += 1\n        \n        # Use the provided 'd' for the upper bound U\n        d = graph_params['d']\n        U = d + 1\n\n        # Step 2: Apply greedy coloring\n        colors = greedy_coloring(adj)\n        if not colors:\n            chi = 0\n            color_classes_sizes = []\n        else:\n            chi = max(colors)\n            color_classes_sizes = [0] * chi\n            for color in colors:\n                color_classes_sizes[color - 1] += 1\n        \n        ok = (chi = U)\n\n        # Step 3: Compute average occupancy\n        S = hw_params['S']\n        T_SM = hw_params['T_SM']\n        R_SM = hw_params['R_SM']\n        M_SM = hw_params['M_SM']\n        B_max = hw_params['B_max']\n        \n        B = kernel_params['B']\n        r = kernel_params['r']\n        s = kernel_params['s']\n\n        # Calculate resident blocks per SM\n        if r * B > R_SM:\n            blocks_per_sm_by_reg = 0\n        else:\n            blocks_per_sm_by_reg = R_SM // (r * B) if r * B > 0 else B_max \n\n        b_res = min(\n            T_SM // B,\n            blocks_per_sm_by_reg,\n            M_SM // s if s > 0 else B_max,\n            B_max\n        )\n\n        # Calculate occupancy for each color and average\n        total_occupancy = 0.0\n        if chi > 0:\n            for T_c in color_classes_sizes:\n                resource_occ = (b_res * B) / T_SM\n                supply_factor = min(1.0, T_c / (b_res * S)) if b_res * S > 0 else 0\n                occ = resource_occ * supply_factor\n                total_occupancy += occ\n            \n            avg_occ = total_occupancy / chi\n        else:\n            avg_occ = 0.0\n        \n        return [chi, U, ok, avg_occ]\n\n    test_cases = [\n        {\n            'graph': {'type': 'grid', 'dim': (10, 10), 'V': 100, 'd': 4},\n            'hw': {'S': 10, 'T_SM': 2048, 'R_SM': 65536, 'M_SM': 98304, 'B_max': 16},\n            'kernel': {'B': 128, 'r': 64, 's': 4096}\n        },\n        {\n            'graph': {'type': 'path', 'V': 32, 'd': 2},\n            'hw': {'S': 2, 'T_SM': 2048, 'R_SM': 65536, 'M_SM': 98304, 'B_max': 16},\n            'kernel': {'B': 64, 'r': 32, 's': 2048}\n        },\n        {\n            'graph': {'type': 'complete', 'V': 16, 'd': 15},\n            'hw': {'S': 4, 'T_SM': 2048, 'R_SM': 65536, 'M_SM': 98304, 'B_max': 16},\n            'kernel': {'B': 256, 'r': 128, 's': 8192}\n        },\n        {\n            'graph': {'type': 'random_bounded', 'V': 200, 'd': 8, 'E_target': 700},\n            'hw': {'S': 16, 'T_SM': 2048, 'R_SM': 65536, 'M_SM': 102400, 'B_max': 16},\n            'kernel': {'B': 128, 'r': 64, 's': 12288}\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        result = calculate_metrics(case['graph'], case['hw'], case['kernel'])\n        results.append(result)\n\n    # Format output as a list of lists strings\n    result_strings = [f\"[{res[0]},{res[1]},{res[2]},{res[3]:.6f}]\" for res in results]\n    final_output = f\"[{','.join(result_strings)}]\"\n    # Correcting boolean formatting to lowercase as per common standards.\n    final_output = final_output.replace('True', 'true').replace('False', 'false')\n    print(final_output)\n\nsolve()\n```", "id": "3509741"}]}