{"hands_on_practices": [{"introduction": "Before automating the solution of complex nonlinear systems, it is essential to master the fundamental mechanics of the underlying algorithm. This practice provides a foundational, hands-on calculation of a single damped Newton-Raphson iteration for a coupled system. By manually computing the Jacobian matrix, solving for the update increment, and applying it to the state vector, you will build a concrete understanding of how linearization is used to iteratively approach a solution [@problem_id:3512855].", "problem": "Consider a nondimensionalized two-field residual vector for a coupled thermo-mechanical toy model, defined by\n$$\nR(U) \\equiv \\begin{bmatrix} R_{1}(T,u) \\\\ R_{2}(T,u) \\end{bmatrix}\n= \\begin{bmatrix} \\sin(T) + \\alpha\\, u \\\\ u^{3} - T \\end{bmatrix},\n$$\nwhere $U \\equiv \\begin{bmatrix} T \\\\ u \\end{bmatrix}$, the scalar parameter $\\alpha \\in \\mathbb{R}$ represents a coupling strength, and all quantities are nondimensional. Assume that $\\alpha \\neq -3$ to avoid a singular Jacobian at the state of interest. Let the current iterate be $U^{(k)} = \\begin{bmatrix} T^{(k)} \\\\ u^{(k)} \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$, and let the damping factor for the Newton update be $\\lambda \\in (0,1]$.\n\nStarting from the first-order Taylor linearization of the residual about $U^{(k)}$ and the definition of the Newton step for coupled systems, perform a single damped Newton iteration at $U^{(k)}$ to compute:\n- the Jacobian matrix $J(U^{(k)}) \\equiv \\dfrac{\\partial R}{\\partial U}\\big|_{U^{(k)}}$,\n- the Newton increment $\\Delta U$ solving $J(U^{(k)})\\,\\Delta U = -R(U^{(k)})$,\n- the damped update $U^{+} = U^{(k)} + \\lambda\\, \\Delta U$.\n\nReport your final answer as the row vector\n$$\n\\big(J_{11},\\, J_{12},\\, J_{21},\\, J_{22},\\, \\Delta T,\\, \\Delta u,\\, T^{+},\\, u^{+}\\big),\n$$\nwhere $J_{ij}$ are the entries of $J(U^{(k)})$, and $\\Delta T$, $\\Delta u$, $T^{+}$, $u^{+}$ are the components of $\\Delta U$ and $U^{+}$, respectively. No rounding is required. Express your answer symbolically in terms of $\\alpha$ and $\\lambda$.", "solution": "The problem is first validated to ensure it is self-contained, scientifically grounded, and well-posed.\n\n### Step 1: Extract Givens\n- **Residual Vector**: $R(U) \\equiv \\begin{bmatrix} R_{1}(T,u) \\\\ R_{2}(T,u) \\end{bmatrix} = \\begin{bmatrix} \\sin(T) + \\alpha\\, u \\\\ u^{3} - T \\end{bmatrix}$\n- **State Vector**: $U \\equiv \\begin{bmatrix} T \\\\ u \\end{bmatrix}$\n- **Coupling Parameter**: $\\alpha \\in \\mathbb{R}$\n- **Constraint**: $\\alpha \\neq -3$\n- **Current Iterate**: $U^{(k)} = \\begin{bmatrix} T^{(k)} \\\\ u^{(k)} \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$\n- **Damping Factor**: $\\lambda \\in (0,1]$\n- **Task**: Compute the Jacobian $J(U^{(k)})$, the Newton increment $\\Delta U$, and the damped update $U^{+} = U^{(k)} + \\lambda\\,\\Delta U$.\n- **Required Output**: The row vector $\\big(J_{11},\\, J_{12},\\, J_{21},\\, J_{22},\\, \\Delta T,\\, \\Delta u,\\, T^{+},\\, u^{+}\\big)$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is a standard exercise in applying Newton's method to a system of nonlinear equations, a fundamental topic in numerical analysis and computational science.\n- **Scientific/Factual Soundness**: The problem is mathematically sound. It employs established concepts: vector functions, partial derivatives, Taylor series linearization, and Newton's method for root-finding.\n- **Well-Posedness**: The problem is well-posed. It provides all necessary functions, constants, and initial values. The constraint $\\alpha \\neq -3$ is crucial as it ensures the Jacobian matrix is invertible at the given point, guaranteeing a unique solution for the Newton step.\n- **Objectivity**: The problem is stated using precise, objective mathematical language.\n- **Conclusion**: The problem is valid. There are no flaws.\n\n### Step 3: Verdict and Action\nThe problem is valid. The solution proceeds as follows.\n\nThe core of Newton's method for a system of equations is to solve the linear system $J(U^{(k)}) \\Delta U = -R(U^{(k)})$, where $J$ is the Jacobian matrix of the residual vector $R$.\n\n**1. Compute the Jacobian Matrix**\nThe Jacobian matrix $J(U)$ is the matrix of all first-order partial derivatives of the residual vector $R(U)$.\n$$\nJ(U) = \\frac{\\partial R}{\\partial U} = \\begin{bmatrix} \\frac{\\partial R_1}{\\partial T} & \\frac{\\partial R_1}{\\partial u} \\\\ \\frac{\\partial R_2}{\\partial T} & \\frac{\\partial R_2}{\\partial u} \\end{bmatrix}\n$$\nGiven $R_1(T,u) = \\sin(T) + \\alpha u$ and $R_2(T,u) = u^3 - T$, the partial derivatives are:\n- $\\frac{\\partial R_1}{\\partial T} = \\cos(T)$\n- $\\frac{\\partial R_1}{\\partial u} = \\alpha$\n- $\\frac{\\partial R_2}{\\partial T} = -1$\n- $\\frac{\\partial R_2}{\\partial u} = 3u^2$\n\nThus, the Jacobian matrix is:\n$$\nJ(T,u) = \\begin{bmatrix} \\cos(T) & \\alpha \\\\ -1 & 3u^2 \\end{bmatrix}\n$$\n\n**2. Evaluate the Jacobian at the Current Iterate $U^{(k)}$**\nWe evaluate $J(T,u)$ at the point $U^{(k)} = (T^{(k)}, u^{(k)}) = (0, 1)$.\n$$\nJ(U^{(k)}) = J(0,1) = \\begin{bmatrix} \\cos(0) & \\alpha \\\\ -1 & 3(1)^2 \\end{bmatrix} = \\begin{bmatrix} 1 & \\alpha \\\\ -1 & 3 \\end{bmatrix}\n$$\nThe components of this matrix are the first four elements of the final answer:\n- $J_{11} = 1$\n- $J_{12} = \\alpha$\n- $J_{21} = -1$\n- $J_{22} = 3$\n\n**3. Evaluate the Residual at the Current Iterate $U^{(k)}$**\nWe evaluate $R(U)$ at the point $U^{(k)} = (0, 1)$.\n$$\nR(U^{(k)}) = R(0,1) = \\begin{bmatrix} \\sin(0) + \\alpha(1) \\\\ (1)^3 - 0 \\end{bmatrix} = \\begin{bmatrix} \\alpha \\\\ 1 \\end{bmatrix}\n$$\n\n**4. Solve for the Newton Increment $\\Delta U$**\nThe linear system to solve is $J(U^{(k)})\\,\\Delta U = -R(U^{(k)})$, where $\\Delta U = \\begin{bmatrix} \\Delta T \\\\ \\Delta u \\end{bmatrix}$.\n$$\n\\begin{bmatrix} 1 & \\alpha \\\\ -1 & 3 \\end{bmatrix} \\begin{bmatrix} \\Delta T \\\\ \\Delta u \\end{bmatrix} = -\\begin{bmatrix} \\alpha \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} -\\alpha \\\\ -1 \\end{bmatrix}\n$$\nTo solve this $2 \\times 2$ system, we compute the inverse of the Jacobian. The determinant of $J(U^{(k)})$ is:\n$$\n\\det(J(U^{(k)})) = (1)(3) - (\\alpha)(-1) = 3 + \\alpha\n$$\nThe condition $\\alpha \\neq -3$ ensures $\\det(J(U^{(k)})) \\neq 0$, so the matrix is invertible. The inverse is:\n$$\n[J(U^{(k)})]^{-1} = \\frac{1}{3+\\alpha} \\begin{bmatrix} 3 & -\\alpha \\\\ 1 & 1 \\end{bmatrix}\n$$\nNow, we find $\\Delta U$ by multiplying both sides of the linear system by the inverse Jacobian:\n$$\n\\Delta U = [J(U^{(k)})]^{-1} (-R(U^{(k)})) = \\frac{1}{3+\\alpha} \\begin{bmatrix} 3 & -\\alpha \\\\ 1 & 1 \\end{bmatrix} \\begin{bmatrix} -\\alpha \\\\ -1 \\end{bmatrix}\n$$\nPerforming the matrix-vector multiplication:\n$$\n\\begin{bmatrix} \\Delta T \\\\ \\Delta u \\end{bmatrix} = \\frac{1}{3+\\alpha} \\begin{bmatrix} (3)(-\\alpha) + (-\\alpha)(-1) \\\\ (1)(-\\alpha) + (1)(-1) \\end{bmatrix} = \\frac{1}{3+\\alpha} \\begin{bmatrix} -3\\alpha + \\alpha \\\\ -\\alpha - 1 \\end{bmatrix} = \\begin{bmatrix} \\frac{-2\\alpha}{3+\\alpha} \\\\ \\frac{-(\\alpha+1)}{3+\\alpha} \\end{bmatrix}\n$$\nThe components of the Newton increment are:\n- $\\Delta T = \\frac{-2\\alpha}{3+\\alpha}$\n- $\\Delta u = -\\frac{\\alpha+1}{3+\\alpha}$\n\n**5. Compute the Damped Update $U^{+}$**\nThe new state $U^{+}$ is computed using the damped update rule: $U^{+} = U^{(k)} + \\lambda \\Delta U$.\n$$\nU^{+} = \\begin{bmatrix} T^{+} \\\\ u^{+} \\end{bmatrix} = \\begin{bmatrix} T^{(k)} \\\\ u^{(k)} \\end{bmatrix} + \\lambda \\begin{bmatrix} \\Delta T \\\\ \\Delta u \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} + \\lambda \\begin{bmatrix} \\frac{-2\\alpha}{3+\\alpha} \\\\ -\\frac{\\alpha+1}{3+\\alpha} \\end{bmatrix}\n$$\nThe components of the updated state are:\n- $T^{+} = 0 + \\lambda \\left( \\frac{-2\\alpha}{3+\\alpha} \\right) = \\frac{-2\\lambda\\alpha}{3+\\alpha}$\n- $u^{+} = 1 + \\lambda \\left( -\\frac{\\alpha+1}{3+\\alpha} \\right) = 1 - \\frac{\\lambda(\\alpha+1)}{3+\\alpha} = \\frac{3+\\alpha - \\lambda\\alpha - \\lambda}{3+\\alpha} = \\frac{3-\\lambda + \\alpha(1-\\lambda)}{3+\\alpha}$\n\n**6. Assemble the Final Answer**\nThe final answer is the row vector $\\big(J_{11}, J_{12}, J_{21}, J_{22}, \\Delta T, \\Delta u, T^{+}, u^{+}\\big)$. Collecting all computed components:\n- $J_{11} = 1$\n- $J_{12} = \\alpha$\n- $J_{21} = -1$\n- $J_{22} = 3$\n- $\\Delta T = \\frac{-2\\alpha}{3+\\alpha}$\n- $\\Delta u = -\\frac{\\alpha+1}{3+\\alpha}$\n- $T^{+} = \\frac{-2\\lambda\\alpha}{3+\\alpha}$\n- $u^{+} = \\frac{3-\\lambda + \\alpha(1-\\lambda)}{3+\\alpha}$", "answer": "$$\n\\boxed{\\begin{pmatrix} 1 & \\alpha & -1 & 3 & \\frac{-2\\alpha}{3+\\alpha} & -\\frac{\\alpha+1}{3+\\alpha} & \\frac{-2\\lambda\\alpha}{3+\\alpha} & \\frac{3-\\lambda + \\alpha(1-\\lambda)}{3+\\alpha} \\end{pmatrix}}\n$$", "id": "3512855"}, {"introduction": "The Jacobian matrix is not merely a mathematical construct; its structure and the magnitude of its entries have profound implications for the performance and robustness of numerical solvers. This exercise bridges the gap between analytical derivation and practical computation by examining the linearization of a thermochemical source term [@problem_id:3512914]. You will see how strong physical nonlinearities, such as an exponential dependence on temperature, manifest as numerical stiffness in the Jacobian, leading to ill-conditioning that can challenge the convergence of the iterative linear solvers used within each Newton step.", "problem": "Consider a thermochemical multiphysics model coupling an energy equation and a species conservation equation, where the local reaction source terms are represented by the vector-valued function $S(U)$ acting on the state vector $U = \\begin{bmatrix} T \\\\ u \\end{bmatrix}$, with $T$ denoting temperature and $u$ denoting a species concentration. Let\n$$\nS(U) = \\begin{bmatrix} \\lambda T^{2} u \\\\ \\mu \\exp(\\alpha T) \\end{bmatrix},\n$$\nwhere $\\lambda$, $\\mu$, and $\\alpha$ are positive parameters. The nonlinear system is advanced in time using a fully implicit scheme and solved at each time step with the Newton-Raphson (NR) method, where the linearized update relies on the Jacobian of $S(U)$ with respect to $U$.\n\nStarting from the definition of first-order linearization via the Jacobian, namely that for a perturbation $\\delta U$,\n$$\nS(U + \\delta U) \\approx S(U) + J(U) \\, \\delta U,\n$$\nwhere $J(U)$ is the Jacobian matrix with entries $J_{ij}(U) = \\frac{\\partial S_i}{\\partial U_j}$ evaluated at the current iterate, perform the following tasks:\n\n- Derive all entries of the Jacobian matrix $J(U)$ with respect to $T$ and $u$.\n- Explain, based on the derived Jacobian, how increasing $\\alpha$ affects the robustness of Newton-Krylov solvers (for example, Generalized Minimal Residual (GMRES) within a Krylov subspace method), including implications for conditioning, step selection, and preconditioning strategies.\n\nReport, as your final answer, the symbolic expression for the determinant of the Jacobian matrix $J(U)$ in terms of $\\lambda$, $\\mu$, $\\alpha$, $T$, and $u$. No numerical evaluation is required. If you make any approximations in intermediate steps, state and justify them clearly. Express the final determinant as a closed-form analytic expression and do not include any units in the final reported expression.", "solution": "The problem statement is subjected to validation.\n\n### Step 1: Extract Givens\n- State vector: $U = \\begin{bmatrix} T \\\\ u \\end{bmatrix}$, with $T$ representing temperature and $u$ representing a species concentration.\n- Vector-valued reaction source term function: $S(U) = \\begin{bmatrix} \\lambda T^{2} u \\\\ \\mu \\exp(\\alpha T) \\end{bmatrix}$.\n- Parameters: $\\lambda$, $\\mu$, and $\\alpha$ are positive constants.\n- Numerical context: The system is solved using a fully implicit time scheme with the Newton-Raphson (NR) method.\n- Linearization of the source term: $S(U + \\delta U) \\approx S(U) + J(U) \\, \\delta U$.\n- Definition of the Jacobian matrix: $J_{ij}(U) = \\frac{\\partial S_i}{\\partial U_j}$.\n- Task 1: Derive all entries of the Jacobian matrix $J(U)$.\n- Task 2: Explain how increasing $\\alpha$ affects the robustness of Newton-Krylov solvers.\n- Task 3: Report the determinant of the Jacobian matrix, $\\det(J(U))$, as the final answer.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, presenting a simplified yet conceptually valid model of coupled thermochemical phenomena. The forms of the source terms, while not derived from first principles, are plausible representations of reaction kinetics. The mathematical task—deriving a Jacobian, its determinant, and analyzing its properties in the context of numerical methods (Newton-Krylov solvers)—is a standard and well-posed exercise in computational science and engineering. The problem is self-contained, with all necessary information provided. The language is objective and precise. No scientific, logical, or structural flaws are detected.\n\n### Step 3: Verdict and Action\nThe problem is deemed valid. A full solution will be provided.\n\n### Solution Derivation\n\nThe analysis begins by defining the components of the state vector $U$ and the source vector $S(U)$. The state vector is $U = \\begin{bmatrix} U_1 \\\\ U_2 \\end{bmatrix} = \\begin{bmatrix} T \\\\ u \\end{bmatrix}$. The source function is $S(U) = \\begin{bmatrix} S_1(T, u) \\\\ S_2(T, u) \\end{bmatrix}$, with components:\n$$\nS_1(T, u) = \\lambda T^{2} u\n$$\n$$\nS_2(T, u) = \\mu \\exp(\\alpha T)\n$$\n\n### Task 1: Derivation of the Jacobian Matrix $J(U)$\nThe Jacobian matrix $J(U)$ of the source term $S(U)$ with respect to the state vector $U$ is a $2 \\times 2$ matrix whose entries are the partial derivatives of the components of $S$ with respect to the components of $U$.\n$$\nJ(U) = \\begin{pmatrix} \\frac{\\partial S_1}{\\partial T} & \\frac{\\partial S_1}{\\partial u} \\\\ \\frac{\\partial S_2}{\\partial T} & \\frac{\\partial S_2}{\\partial u} \\end{pmatrix}\n$$\nWe calculate each entry:\n1.  The partial derivative of $S_1$ with respect to $T$:\n    $$\n    J_{11} = \\frac{\\partial S_1}{\\partial T} = \\frac{\\partial}{\\partial T} (\\lambda T^{2} u) = 2 \\lambda T u\n    $$\n2.  The partial derivative of $S_1$ with respect to $u$:\n    $$\n    J_{12} = \\frac{\\partial S_1}{\\partial u} = \\frac{\\partial}{\\partial u} (\\lambda T^{2} u) = \\lambda T^{2}\n    $$\n3.  The partial derivative of $S_2$ with respect to $T$:\n    $$\n    J_{21} = \\frac{\\partial S_2}{\\partial T} = \\frac{\\partial}{\\partial T} (\\mu \\exp(\\alpha T)) = \\mu \\alpha \\exp(\\alpha T)\n    $$\n4.  The partial derivative of $S_2$ with respect to $u$:\n    $$\n    J_{22} = \\frac{\\partial S_2}{\\partial u} = \\frac{\\partial}{\\partial u} (\\mu \\exp(\\alpha T)) = 0\n    $$\nAssembling these entries gives the Jacobian matrix:\n$$\nJ(U) = \\begin{pmatrix} 2 \\lambda T u & \\lambda T^{2} \\\\ \\mu \\alpha \\exp(\\alpha T) & 0 \\end{pmatrix}\n$$\n\n### Task 2: Effect of Increasing $\\alpha$ on Solver Robustness\nAn implicit time-stepping scheme for a system of partial differential equations of the form $\\frac{\\partial U}{\\partial t} + \\nabla \\cdot F(U) = S(U)$ leads to a nonlinear system of algebraic equations to be solved at each time step, which can be written as $R(U^{n+1}) = 0$. The Newton-Raphson method solves this by iteratively finding an update $\\delta U$ from the linear system $\\mathcal{J}(U^k) \\delta U = -R(U^k)$, where $\\mathcal{J}$ is the full system Jacobian and $U^k$ is the current iterate. The source term Jacobian, $J(U)$, is a component of this system Jacobian, typically appearing as $\\mathcal{J} = A - J(U^k)$, where $A$ incorporates temporal and spatial discretization terms.\n\nThe robustness of a Newton-Krylov solver, which uses a Krylov subspace method like GMRES to solve the linear system for $\\delta U$, is critically dependent on the properties of the matrix $\\mathcal{J}$. The parameter $\\alpha$ influences these properties through the term $J_{21} = \\mu \\alpha \\exp(\\alpha T)$.\n\n1.  **Stiffness and Ill-Conditioning**: As $\\alpha$ increases, the entry $J_{21}$ grows exponentially, assuming $T > 0$. This introduces a very large off-diagonal term in the source Jacobian. A large disparity in the magnitude of the entries of a matrix is a primary cause of ill-conditioning. The condition number of the system Jacobian $\\mathcal{J}$, denoted $\\kappa(\\mathcal{J})$, will increase dramatically. Krylov solvers like GMRES exhibit convergence rates that degrade significantly as the condition number increases. Therefore, a larger $\\alpha$ leads to a much larger number of GMRES iterations per Newton step, or a complete failure to converge. This phenomenon is characteristic of \"stiff\" systems, where different physical processes operate on vastly different scales. Here, the exponential dependence on temperature makes the species source term extremely sensitive to small changes in temperature, creating numerical stiffness.\n\n2.  **Newton Step Quality and Convergence**: The Newton-Raphson method relies on the linear approximation $S(U + \\delta U) \\approx S(U) + J(U) \\delta U$ being accurate within a certain radius. When second-order derivatives (Hessian terms) are large, this linear approximation breaks down more quickly. The term $\\mu \\alpha \\exp(\\alpha T)$ has a second derivative with respect to $T$ of $\\mu \\alpha^2 \\exp(\\alpha T)$, which grows even faster with $\\alpha$. This means the linear model is a poor approximation, and the calculated Newton step $\\delta U$ may be too large, \"overshooting\" the solution and causing the Newton iteration to diverge. Robust implementations must incorporate globalization strategies like line searches or trust regions, which would be forced to take increasingly smaller steps as $\\alpha$ increases, thus severely slowing down the convergence to the solution of the nonlinear system.\n\n3.  **Preconditioning Strategy**: For ill-conditioned systems, preconditioning is essential. A preconditioner $P$ aims to transform the linear system into an equivalent one, e.g., $P^{-1} \\mathcal{J} \\delta U = -P^{-1} R$, where the matrix $P^{-1} \\mathcal{J}$ is better conditioned. The strong, one-way coupling from temperature $T$ to species $u$ (manifested in the large $J_{21}$ term and a $J_{12}$ term that is independent of this coupling strength) makes it very difficult to design an effective and computationally inexpensive preconditioner. Standard, general-purpose preconditioners (like Incomplete LU factorizations) may fail to capture this strong coupling and thus provide little benefit. Effective preconditioning for large $\\alpha$ would likely require a sophisticated, physics-based approach that correctly approximates the block structure and coupling within the Jacobian, which adds significant complexity to the solver.\n\nIn summary, increasing $\\alpha$ severely degrades the robustness of Newton-Krylov solvers by inducing stiffness, causing extreme ill-conditioning of the linear system, reducing the quality of the Newton linear model, and demanding more sophisticated and costly preconditioning.\n\n### Task 3: Determinant of the Jacobian Matrix\nThe determinant of a generic $2 \\times 2$ matrix $\\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}$ is given by the formula $ad - bc$. Applying this to the derived Jacobian matrix $J(U)$:\n$$\nJ(U) = \\begin{pmatrix} 2 \\lambda T u & \\lambda T^{2} \\\\ \\mu \\alpha \\exp(\\alpha T) & 0 \\end{pmatrix}\n$$\nThe determinant is:\n$$\n\\det(J(U)) = (2 \\lambda T u)(0) - (\\lambda T^{2})(\\mu \\alpha \\exp(\\alpha T))\n$$\n$$\n\\det(J(U)) = 0 - \\lambda \\mu \\alpha T^{2} \\exp(\\alpha T)\n$$\n$$\n\\det(J(U)) = - \\lambda \\mu \\alpha T^{2} \\exp(\\alpha T)\n$$\nThis is the final closed-form analytic expression for the determinant.", "answer": "$$\\boxed{-\\lambda \\mu \\alpha T^{2} \\exp(\\alpha T)}$$", "id": "3512914"}, {"introduction": "In the world of computational science, theory and implementation must go hand in hand. While an exact Jacobian is the key to the rapid convergence of Newton's method, correctly implementing it in code is a frequent source of error. This problem introduces a critical verification technique: comparing the action of your analytical Jacobian to a numerical approximation computed by finite differences [@problem_id:3512976]. This practice delves into the nuances of this method, including the crucial balance between truncation and floating-point roundoff errors to select an optimal step size, ensuring your implemented linearization is both correct and robust.", "problem": "Consider a coupled multiphysics residual map $R:\\mathbb{R}^n \\to \\mathbb{R}^n$ that depends on state $u \\in \\mathbb{R}^n$, where $u$ is partitioned as $u = [u_f; u_s]$ for fluid and structure subsystems, respectively. Let $J(u) = \\partial R/\\partial u$ denote the Jacobian. Suppose an analytic or Automatic Differentiation (AD)-based Jacobian action $J(u)v$ is available, and you wish to verify its correctness using a directional finite-difference test at a given state $u$, with a test direction $v \\in \\mathbb{R}^n$ and scalar step size $h \\in \\mathbb{R}$. Define the forward-difference directional approximation\n$$\nd_h(v) \\equiv \\frac{R(u + h v) - R(u)}{h}.\n$$\nYou will compare $d_h(v)$ to $J(u)v$ using a relative discrepancy metric\n$$\nE(h,v) \\equiv \\frac{\\lVert J(u)v - d_h(v) \\rVert}{\\lVert J(u)v \\rVert + \\lVert d_h(v) \\rVert}.\n$$\nAssume the following foundational models:\n- Taylor’s theorem with remainder for $R(u + h v)$, with derivatives that are continuous in a neighborhood of $u$.\n- The standard floating-point model: each computed residual evaluation $\\widehat{R}(w)$ satisfies $\\widehat{R}(w) = R(w)\\,(1 + \\delta(w))$ with componentwise perturbations bounded by $\\lvert \\delta(w)_i \\rvert \\le \\varepsilon_{\\mathrm{mach}}$, where $\\varepsilon_{\\mathrm{mach}}$ is machine epsilon, and arithmetic is subject to similar relative rounding effects.\n\nWhich of the following statements are correct for designing and interpreting such Jacobian-verification tests in coupled systems?\n\nA) For the forward-difference $d_h(v)$, the truncation error scales like $\\mathcal{O}(h)$ and the roundoff-induced difference-quotient error scales like $\\mathcal{O}(\\varepsilon_{\\mathrm{mach}}/h)$ times a problem-dependent magnitude (e.g., a norm of $R$). Balancing these terms implies an optimal step size scaling $h \\sim \\sqrt{\\varepsilon_{\\mathrm{mach}}}$ up to problem-dependent factors involving $R$, $J$, higher derivatives, and the scaling of $v$.\n\nB) If instead a central-difference directional formula $(R(u+h v)-R(u-h v))/(2h)$ is used, the truncation error is $\\mathcal{O}(h^2)$ and the roundoff-induced error is $\\mathcal{O}(\\varepsilon_{\\mathrm{mach}}/h)$, leading to an optimal step size scaling $h \\sim \\varepsilon_{\\mathrm{mach}}^{1/2}$.\n\nC) In a coupled partitioned system with block Jacobian\n$$\nJ(u) = \\begin{bmatrix} J_{ff}(u) & J_{fs}(u) \\\\[4pt] J_{sf}(u) & J_{ss}(u) \\end{bmatrix}, \\quad u = \\begin{bmatrix} u_f \\\\ u_s \\end{bmatrix},\n$$\nchoosing a direction $v = [0; v_s]$ isolates the effect of the off-diagonal block $J_{fs}(u)$ in the fluid residual change: the fluid block of $d_h(v)$ converges to $J_{fs}(u)\\,v_s$ as $h \\to 0$, so such directions can detect missing or incorrect cross-coupling terms.\n\nD) Decreasing $h$ always decreases $E(h,v)$ in forward differences because the finite difference is a consistent approximation to $J(u)v$ as $h \\to 0$.\n\nE) To balance perturbations across heterogeneous fields (with different units and magnitudes) while preserving the meaning of a directional derivative, it is advisable to normalize $v$ in a weighted norm before applying a single scalar step $h$. For example, choose a scaling vector $s$ with $s_j = \\max(\\lvert u_j \\rvert, 1)$, form $\\widetilde{v} = (v \\odot s)/\\lVert v \\odot s \\rVert$, then use $d_h(\\widetilde{v})$ with a scalar step size $h \\approx c\\,\\sqrt{\\varepsilon_{\\mathrm{mach}}}$ in forward differences (with $c$ an $\\mathcal{O}(1)$ constant), which helps balance truncation and roundoff effects across components.\n\nSelect all that apply.", "solution": "The problem statement is a valid and well-posed question in numerical analysis, specifically concerning the verification of Jacobians computed for nonlinear systems, a common task in scientific computing and multiphysics simulations. The provided models for Taylor's theorem and floating-point arithmetic are standard for this type of analysis. We will proceed to evaluate each option.\n\n**A) For the forward-difference $d_h(v)$, the truncation error scales like $\\mathcal{O}(h)$ and the roundoff-induced difference-quotient error scales like $\\mathcal{O}(\\varepsilon_{\\mathrm{mach}}/h)$ times a problem-dependent magnitude (e.g., a norm of $R$). Balancing these terms implies an optimal step size scaling $h \\sim \\sqrt{\\varepsilon_{\\mathrm{mach}}}$ up to problem-dependent factors involving $R$, $J$, higher derivatives, and the scaling of $v$.**\n\nLet us analyze the error in the forward-difference approximation $d_h(v) = (R(u + h v) - R(u))/h$.\n\n1.  **Truncation Error**: Assuming the residual map $R$ is at least twice continuously differentiable ($C^2$) in a neighborhood of $u$, we can use Taylor's theorem:\n    $$R(u + h v) = R(u) + h J(u)v + \\frac{h^2}{2} R''(u)(v,v) + \\mathcal{O}(h^3)$$\n    where $R''(u)(v,v)$ represents the action of the second derivative tensor on the direction $v$.\n    Substituting this into the definition of $d_h(v)$:\n    $$d_h(v) = \\frac{(R(u) + h J(u)v + \\frac{h^2}{2} R''(u)(v,v) + \\mathcal{O}(h^3)) - R(u)}{h} = J(u)v + \\frac{h}{2} R''(u)(v,v) + \\mathcal{O}(h^2)$$\n    The truncation error is the difference between the exact directional derivative and this approximation:\n    $$J(u)v - d_h(v) = -\\frac{h}{2} R''(u)(v,v) - \\mathcal{O}(h^2)$$\n    The norm of the truncation error is therefore $\\lVert J(u)v - d_h(v) \\rVert = \\mathcal{O}(h)$. The first part of the statement is correct.\n\n2.  **Roundoff Error**: In floating-point arithmetic, the computed residuals are $\\widehat{R}(u) \\approx R(u)$ and $\\widehat{R}(u+hv) \\approx R(u+hv)$. The given model states that the computed numerator $\\widehat{R}(u+hv) - \\widehat{R}(u)$ has an absolute error of roughly $(\\lVert R(u+hv) \\rVert + \\lVert R(u) \\rVert)\\varepsilon_{\\mathrm{mach}}$ due to catastrophic cancellation since $R(u+hv) \\approx R(u)$ for small $h$. This error is approximately $2\\lVert R(u) \\rVert \\varepsilon_{\\mathrm{mach}}$. When this error is divided by $h$, the resulting error in the computed $d_h(v)$ is of order $\\mathcal{O}(\\varepsilon_{\\mathrm{mach}}/h)$. The statement that this error scales like $\\mathcal{O}(\\varepsilon_{\\mathrm{mach}}/h)$ times a problem-dependent magnitude (like $\\lVert R(u) \\rVert$) is correct.\n\n3.  **Optimal Step Size**: The total error magnitude can be modeled as the sum of the truncation and roundoff error magnitudes:\n    $$E_{\\text{total}}(h) \\approx C_1 h + C_2 \\frac{\\varepsilon_{\\mathrm{mach}}}{h}$$\n    where $C_1$ depends on the second derivative of $R$ and $C_2$ depends on the magnitude of $R$. To find the step size $h$ that minimizes this error, we differentiate with respect to $h$ and set the result to zero:\n    $$\\frac{dE_{\\text{total}}}{dh} = C_1 - C_2 \\frac{\\varepsilon_{\\mathrm{mach}}}{h^2} = 0 \\implies h^2 = \\frac{C_2}{C_1} \\varepsilon_{\\mathrm{mach}}$$\n    This gives the optimal step size scaling $h_{\\text{opt}} \\sim \\sqrt{\\varepsilon_{\\mathrm{mach}}}$, with the constant of proportionality depending on problem-specific factors ($R$, $J$, higher derivatives, $v$).\n\nThe statement is a complete and accurate summary of the classical analysis of the forward-difference method in finite-precision arithmetic.\n**Verdict: Correct.**\n\n**B) If instead a central-difference directional formula $(R(u+h v)-R(u-h v))/(2h)$ is used, the truncation error is $\\mathcal{O}(h^2)$ and the roundoff-induced error is $\\mathcal{O}(\\varepsilon_{\\mathrm{mach}}/h)$, leading to an optimal step size scaling $h \\sim \\varepsilon_{\\mathrm{mach}}^{1/2}$.**\n\nLet us analyze the central-difference formula.\n\n1.  **Truncation Error**: Assuming $R$ is $C^3$, we use Taylor series for $R(u+hv)$ and $R(u-hv)$:\n    $$R(u \\pm hv) = R(u) \\pm h J(u)v + \\frac{h^2}{2} R''(u)(v,v) \\pm \\frac{h^3}{6} R'''(u)(v,v,v) + \\mathcal{O}(h^4)$$\n    Subtracting the two expansions gives:\n    $$R(u+hv) - R(u-hv) = 2h J(u)v + \\frac{h^3}{3} R'''(u)(v,v,v) + \\mathcal{O}(h^5)$$\n    Dividing by $2h$:\n    $$\\frac{R(u+hv) - R(u-hv)}{2h} = J(u)v + \\frac{h^2}{6} R'''(u)(v,v,v) + \\mathcal{O}(h^4)$$\n    The truncation error is of order $\\mathcal{O}(h^2)$. This part of the statement is correct.\n\n2.  **Roundoff Error**: The analysis of roundoff error is identical to the forward-difference case. It arises from the catastrophic cancellation in the subtraction $R(u+hv) - R(u-hv)$ and is subsequently amplified by division by the small parameter $h$. Thus, the roundoff error is of order $\\mathcal{O}(\\varepsilon_{\\mathrm{mach}}/h)$. This part is also correct.\n\n3.  **Optimal Step Size**: The total error magnitude is modeled as:\n    $$E_{\\text{total}}(h) \\approx C_1' h^2 + C_2' \\frac{\\varepsilon_{\\mathrm{mach}}}{h}$$\n    Minimizing with respect to $h$:\n    $$\\frac{dE_{\\text{total}}}{dh} = 2C_1' h - C_2' \\frac{\\varepsilon_{\\mathrm{mach}}}{h^2} = 0 \\implies h^3 = \\frac{C_2'}{2C_1'} \\varepsilon_{\\mathrm{mach}}$$\n    This gives an optimal step size scaling of $h_{\\text{opt}} \\sim \\varepsilon_{\\mathrm{mach}}^{1/3}$. The statement claims the scaling is $h \\sim \\varepsilon_{\\mathrm{mach}}^{1/2}$, which is incorrect.\n\n**Verdict: Incorrect.**\n\n**C) In a coupled partitioned system with block Jacobian $J(u) = \\begin{bmatrix} J_{ff}(u) & J_{fs}(u) \\\\ J_{sf}(u) & J_{ss}(u) \\end{bmatrix}, u = \\begin{bmatrix} u_f \\\\ u_s \\end{bmatrix}$, choosing a direction $v = [0; v_s]$ isolates the effect of the off-diagonal block $J_{fs}(u)$ in the fluid residual change: the fluid block of $d_h(v)$ converges to $J_{fs}(u)\\,v_s$ as $h \\to 0$, so such directions can detect missing or incorrect cross-coupling terms.**\n\nLet the residual also be partitioned as $R(u) = \\begin{bmatrix} R_f(u_f, u_s) \\\\ R_s(u_f, u_s) \\end{bmatrix}$. Given the test direction $v = \\begin{bmatrix} 0 \\\\ v_s \\end{bmatrix}$, the perturbed state is $u+hv = \\begin{bmatrix} u_f \\\\ u_s + hv_s \\end{bmatrix}$.\n\nThe forward-difference approximation is $d_h(v) = \\frac{1}{h}(R(u+hv) - R(u))$. Let's examine its fluid block, which we denote $(d_h(v))_f$:\n$$(d_h(v))_f = \\frac{R_f(u_f, u_s + hv_s) - R_f(u_f, u_s)}{h}$$\nBy the definition of the partial derivative (or directional Gâteaux derivative), as $h \\to 0$, this expression converges to the directional derivative of $R_f$ with respect to $u_s$ in the direction $v_s$. This is precisely the action of the Jacobian block $J_{fs}(u) = \\partial R_f / \\partial u_s$ on the vector $v_s$.\n$$\\lim_{h \\to 0} (d_h(v))_f = \\frac{\\partial R_f}{\\partial u_s}(u) v_s = J_{fs}(u)v_s$$\nThe analytical Jacobian action for this direction is $J(u)v = \\begin{bmatrix} J_{ff} & J_{fs} \\\\ J_{sf} & J_{ss} \\end{bmatrix} \\begin{bmatrix} 0 \\\\ v_s \\end{bmatrix} = \\begin{bmatrix} J_{fs} v_s \\\\ J_{ss} v_s \\end{bmatrix}$.\n\nThe comparison for the fluid-field part of the residual therefore compares the numerical approximation $(d_h(v))_f$ against the analytical result $(J(u)v)_f = J_{fs}(u)v_s$. This test is indeed specifically sensitive to errors in the implementation of the coupling block $J_{fs}$. Choosing such directions (and similarly, directions of the form $v = [v_f; 0]$ to test $J_f$ and $J_{sf}$) is a standard and crucial technique for debugging the Jacobian of coupled systems.\n**Verdict: Correct.**\n\n**D) Decreasing $h$ always decreases $E(h,v)$ in forward differences because the finite difference is a consistent approximation to $J(u)v$ as $h \\to 0$.**\n\nThis statement is false. As established in the analysis of option A, the total error in the finite-difference approximation in floating-point arithmetic is a combination of two competing effects: a truncation error that decreases with $h$ (e.g., $\\mathcal{O}(h)$) and a roundoff error that increases as $h$ decreases (e.g., $\\mathcal{O}(\\varepsilon_{\\mathrm{mach}}/h)$).\nWhile for large values of $h$ (where truncation error dominates), decreasing $h$ will indeed decrease the total error $E(h,v)$, there exists an optimal step size $h_{\\text{opt}}$. For $h < h_{\\text{opt}}$, the roundoff error term dominates, and further decreasing $h$ causes the total error to increase. The property of consistency refers to the mathematical limit in exact arithmetic, not the behavior in finite-precision computation where roundoff is a factor. The word \"always\" makes the statement definitively incorrect.\n**Verdict: Incorrect.**\n\n**E) To balance perturbations across heterogeneous fields (with different units and magnitudes) while preserving the meaning of a directional derivative, it is advisable to normalize $v$ in a weighted norm before applying a single scalar step $h$. For example, choose a scaling vector $s$ with $s_j = \\max(\\lvert u_j \\rvert, 1)$, form $\\widetilde{v} = (v \\odot s)/\\lVert v \\odot s \\rVert$, then use $d_h(\\widetilde{v})$ with a scalar step size $h \\approx c\\,\\sqrt{\\varepsilon_{\\mathrm{mach}}}$ in forward differences (with $c$ an $\\mathcal{O}(1)$ constant), which helps balance truncation and roundoff effects across components.**\n\nThis statement describes a standard and highly recommended best-practice for numerical differentiation of functions with state variables of heterogeneous scales.\n\n1.  **Scaling**: The state vector $u$ in multiphysics problems often contains components with different units and widely varying magnitudes (e.g., temperature in Kelvin, pressure in Pascals, displacement in meters). A simple perturbation $hv$ can be disproportionately large for some components and meaninglessly small for others. The proposed scaling, where a perturbation vector $v$ is transformed into $\\widetilde{v}$ using component-wise scaling factors $s_j = \\max(\\lvert u_j \\rvert, 1)$, ensures that the perturbation $h\\widetilde{v}$ is of a comparable relative magnitude for all non-zero components of $u$. The term $\\max(\\lvert u_j \\rvert, 1)$ is a robust way to handle both large-magnitude and zero or near-zero state components. Normalizing the scaled vector makes the overall magnitude of the perturbation independent of the choice of $v$. This is a crucial step for a meaningful test.\n\n2.  **Step Size**: The statement then suggests using a step size $h \\approx c\\sqrt{\\varepsilon_{\\mathrm{mach}}}$, where $c$ is an $\\mathcal{O}(1)$ constant. As demonstrated in the analysis of option A, $h \\sim \\sqrt{\\varepsilon_{\\mathrm{mach}}}$ is the optimal scaling for the step size to balance truncation and roundoff errors for a forward-difference scheme.\n\nCombining these two techniques—proper scaling of the perturbation direction and a near-optimal choice of the step size—constitutes a robust and theoretically sound methodology for performing Jacobian verification. The statement accurately describes this procedure and its rationale.\n**Verdict: Correct.**", "answer": "$$\\boxed{ACE}$$", "id": "3512976"}]}