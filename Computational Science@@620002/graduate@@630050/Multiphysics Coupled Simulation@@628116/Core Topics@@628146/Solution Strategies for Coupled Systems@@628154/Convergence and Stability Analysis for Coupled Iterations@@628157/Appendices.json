{"hands_on_practices": [{"introduction": "The analysis of coupled iterative schemes often begins with a study of their behavior on linear model problems. After discretization and linearization, many multiphysics systems can be represented by block-structured linear equations. This first practice explores the convergence of two fundamental partitioned methods—block-Jacobi and block-Gauss-Seidel—by analyzing the spectral radius ($\\rho$) of their respective iteration matrices. By working through this foundational exercise ([@problem_id:3500497]), you will gain direct experience in how the algebraic structure of an iterative method dictates its convergence rate, a cornerstone of numerical analysis.", "problem": "Consider a linearized, nondimensional monolithic system arising from a tightly coupled two-field multiphysics problem (for example, quasi-static thermoelasticity or incompressible thermal-fluid coupling treated in a partitioned manner). The linear system for the coupled update is written in $2 \\times 2$ block form as\n$$\nA \\begin{pmatrix} u \\\\ v \\end{pmatrix} = \\begin{pmatrix} f \\\\ g \\end{pmatrix}, \\quad A = \\begin{pmatrix} a I  c I \\\\ c I  b I \\end{pmatrix},\n$$\nwhere $u$ and $v$ denote the discretized fields, $I$ is the identity matrix of size $n \\times n$ with $n \\geq 1$, and $a$, $b$ are positive real scalars such that the diagonal blocks $a I$ and $b I$ are symmetric positive definite (SPD), while $c$ is a real coupling constant. Assume the dimensionless parameters are\n$$\na = 3, \\quad b = 2, \\quad c = 1,\n$$\nwith $n$ arbitrary but fixed.\n\nDefine the standard block-splittings $A = D + L + U$ with\n$$\nD = \\begin{pmatrix} a I  0 \\\\ 0  b I \\end{pmatrix}, \\quad L = \\begin{pmatrix} 0  0 \\\\ c I  0 \\end{pmatrix}, \\quad U = \\begin{pmatrix} 0  c I \\\\ 0  0 \\end{pmatrix}.\n$$\nConsider the block-Jacobi fixed-point iteration $x^{k+1} = T_{J} x^{k} + h$, generated by the splitting $A = D - (-(L+U))$, and the forward block-Gauss-Seidel fixed-point iteration $x^{k+1} = T_{GS} x^{k} + h$, generated by the splitting $A = (D+L) - (-U)$, where $x = (u, v)^{\\top}$ and $h$ is the corresponding right-hand side vector.\n\nUsing only the definitions of the spectral radius, block-splitting induced iteration matrices, and basic eigenvalue properties of block triangular matrices, perform the following:\n\n- Derive $T_{J}$ and $T_{GS}$ explicitly in terms of $a$, $b$, $c$, and $I$.\n- Compute the spectral radii $\\rho(T_{J})$ and $\\rho(T_{GS})$ for the given parameters.\n- Justify, by direct eigenvalue analysis or norm bounds consistent with these splittings, which iteration has the smaller spectral radius for the given parameters.\n- Provide the closed-form analytic expression for the ratio $\\rho(T_{J}) / \\rho(T_{GS})$ for the given parameters.\n\nExpress your final answer as a single analytic expression. No rounding is required, and no physical units are involved, as the answer is dimensionless.", "solution": "The problem statement has been validated and is deemed sound, well-posed, and scientifically grounded. It presents a standard problem in numerical linear algebra concerning the convergence analysis of iterative methods for a specific block-structured matrix. We may proceed with the solution.\n\nThe problem asks for an analysis of the block-Jacobi and forward block-Gauss-Seidel iterative methods for the linear system $A x = b_h$, where $x = \\begin{pmatrix} u \\\\ v \\end{pmatrix}$, $b_h = \\begin{pmatrix} f \\\\ g \\end{pmatrix}$, and the system matrix $A$ is given by\n$$\nA = \\begin{pmatrix} a I  c I \\\\ c I  b I \\end{pmatrix}.\n$$\nThe standard block-splitting of $A$ into diagonal ($D$), strictly lower ($L$), and strictly upper ($U$) parts is\n$$\nD = \\begin{pmatrix} a I  0 \\\\ 0  b I \\end{pmatrix}, \\quad L = \\begin{pmatrix} 0  0 \\\\ c I  0 \\end{pmatrix}, \\quad U = \\begin{pmatrix} 0  c I \\\\ 0  0 \\end{pmatrix}.\n$$\nThe given parameters are $a=3$, $b=2$, and $c=1$, with $a, b > 0$.\n\nFirst, we derive the iteration matrices $T_J$ and $T_{GS}$.\n\nThe block-Jacobi method is defined by the splitting $A = D - (-(L+U))$. The iteration matrix, $T_J$, is given by $T_J = D^{-1}(L+U)$. Since $L+U = -(-(L+U))$, some definitions use $T_J = -D^{-1}(L+U)$. Given the problem's phrasing, we must be precise. The fixed-point iteration is $x^{k+1} = T x^{k} + \\text{const}$. The update rule $D x^{k+1} = -(L+U) x^k + b_h$ leads to $x^{k+1} = -D^{-1}(L+U) x^k + D^{-1} b_h$. Thus, the iteration matrix is $T_J = -D^{-1}(L+U)$.\n\nThe inverse of the block-diagonal matrix $D$ is\n$$\nD^{-1} = \\begin{pmatrix} (aI)^{-1}  0 \\\\ 0  (bI)^{-1} \\end{pmatrix} = \\begin{pmatrix} a^{-1} I  0 \\\\ 0  b^{-1} I \\end{pmatrix}.\n$$\nThe sum $L+U$ is\n$$\nL+U = \\begin{pmatrix} 0  c I \\\\ c I  0 \\end{pmatrix}.\n$$\nTherefore, the Jacobi iteration matrix $T_J$ is\n$$\nT_J = -D^{-1}(L+U) = - \\begin{pmatrix} a^{-1} I  0 \\\\ 0  b^{-1} I \\end{pmatrix} \\begin{pmatrix} 0  c I \\\\ c I  0 \\end{pmatrix} = - \\begin{pmatrix} 0  a^{-1}c I \\\\ b^{-1}c I  0 \\end{pmatrix} = \\begin{pmatrix} 0  - (c/a) I \\\\ - (c/b) I  0 \\end{pmatrix}.\n$$\n\nThe forward block-Gauss-Seidel method is defined by the splitting $A = (D+L) - (-U)$. The iteration is $(D+L) x^{k+1} = -U x^k + b_h$, which leads to $x^{k+1} = -(D+L)^{-1}U x^k + (D+L)^{-1} b_h$. The iteration matrix is $T_{GS} = -(D+L)^{-1}U$.\n\nFirst, we find the inverse of the block lower-triangular matrix $D+L$:\n$$\nD+L = \\begin{pmatrix} a I  0 \\\\ c I  b I \\end{pmatrix}.\n$$\nWe seek $(D+L)^{-1} = \\begin{pmatrix} X  Y \\\\ Z  W \\end{pmatrix}$ such that $\\begin{pmatrix} a I  0 \\\\ c I  b I \\end{pmatrix} \\begin{pmatrix} X  Y \\\\ Z  W \\end{pmatrix} = \\begin{pmatrix} I  0 \\\\ 0  I \\end{pmatrix}$.\nThis yields the equations:\n$1.$ $aIX = I \\implies X = a^{-1}I$.\n$2.$ $aIY = 0 \\implies Y = 0$.\n$3.$ $cIX + bIZ = 0 \\implies cI(a^{-1}I) + bIZ = 0 \\implies (c/a)I + bZ = 0 \\implies Z = -(c/(ab))I$.\n$4.$ $cIY + bIW = I \\implies cI(0) + bIW = I \\implies W = b^{-1}I$.\nSo, the inverse is\n$$\n(D+L)^{-1} = \\begin{pmatrix} a^{-1} I  0 \\\\ -(c/(ab)) I  b^{-1} I \\end{pmatrix}.\n$$\nNow, we compute the Gauss-Seidel iteration matrix $T_{GS}$:\n$$\nT_{GS} = -(D+L)^{-1}U = - \\begin{pmatrix} a^{-1} I  0 \\\\ -(c/(ab)) I  b^{-1} I \\end{pmatrix} \\begin{pmatrix} 0  c I \\\\ 0  0 \\end{pmatrix} = - \\begin{pmatrix} 0  (c/a) I \\\\ 0  (-c^2/(ab)) I \\end{pmatrix} = \\begin{pmatrix} 0  -(c/a) I \\\\ 0  (c^2/(ab)) I \\end{pmatrix}.\n$$\n\nNext, we compute the spectral radii, $\\rho(T_J)$ and $\\rho(T_{GS})$. The spectral radius of a matrix is the maximum absolute value of its eigenvalues.\n\nFor $T_J$, let $\\lambda$ be an eigenvalue and $\\begin{pmatrix} \\mathbf{x} \\\\ \\mathbf{y} \\end{pmatrix}$ be the corresponding eigenvector, where $\\mathbf{x}, \\mathbf{y} \\in \\mathbb{C}^n$.\n$$\nT_J \\begin{pmatrix} \\mathbf{x} \\\\ \\mathbf{y} \\end{pmatrix} = \\begin{pmatrix} 0  -(c/a) I \\\\ -(c/b) I  0 \\end{pmatrix} \\begin{pmatrix} \\mathbf{x} \\\\ \\mathbf{y} \\end{pmatrix} = \\lambda \\begin{pmatrix} \\mathbf{x} \\\\ \\mathbf{y} \\end{pmatrix}.\n$$\nThis expands to the system of equations:\n$1.$ $-(c/a) \\mathbf{y} = \\lambda \\mathbf{x}$.\n$2.$ $-(c/b) \\mathbf{x} = \\lambda \\mathbf{y}$.\nIf $\\lambda=0$, then $\\mathbf{y}=0$ and $\\mathbf{x}=0$ (since $c, b \\neq 0$), which is not a valid eigenvector. Thus, $\\lambda \\neq 0$.\nFrom equation $(1)$, we have $\\mathbf{y} = -(\\lambda a/c) \\mathbf{x}$. Substituting this into equation $(2)$:\n$$\n-(c/b) \\mathbf{x} = \\lambda \\left( -(\\lambda a/c) \\mathbf{x} \\right) = -\\frac{\\lambda^2 a}{c} \\mathbf{x}.\n$$\nSince $\\mathbf{x}$ cannot be the zero vector, we can equate the scalar coefficients:\n$$\n\\frac{c}{b} = \\frac{\\lambda^2 a}{c} \\implies \\lambda^2 = \\frac{c^2}{ab}.\n$$\nThe eigenvalues of $T_J$ are $\\lambda = \\pm \\sqrt{\\frac{c^2}{ab}} = \\pm \\frac{|c|}{\\sqrt{ab}}$. Each eigenvalue has multiplicity $n$.\nThe spectral radius is therefore\n$$\n\\rho(T_J) = \\max \\left| \\pm \\frac{|c|}{\\sqrt{ab}} \\right| = \\frac{|c|}{\\sqrt{ab}}.\n$$\n\nFor $T_{GS}$, the matrix is block upper-triangular:\n$$\nT_{GS} = \\begin{pmatrix} 0  -(c/a) I \\\\ 0  (c^2/(ab)) I \\end{pmatrix}.\n$$\nThe eigenvalues of a block-triangular matrix are the eigenvalues of its diagonal blocks. The diagonal blocks are the $n \\times n$ zero matrix $0$ and the scalar matrix $(c^2/(ab))I$.\nThe eigenvalues of the zero matrix are all $0$ (with multiplicity $n$).\nThe eigenvalues of $(c^2/(ab))I$ are all $c^2/(ab)$ (with multiplicity $n$).\nThe set of eigenvalues of $T_{GS}$ is $\\{0, c^2/(ab)\\}$. The spectral radius is\n$$\n\\rho(T_{GS}) = \\max \\left\\{ |0|, \\left| \\frac{c^2}{ab} \\right| \\right\\} = \\frac{c^2}{ab} \\quad (\\text{since } a, b > 0).\n$$\n\nNow, we substitute the given parameters $a=3$, $b=2$, $c=1$:\n$$\n\\rho(T_J) = \\frac{|1|}{\\sqrt{3 \\cdot 2}} = \\frac{1}{\\sqrt{6}}.\n$$\n$$\n\\rho(T_{GS}) = \\frac{1^2}{3 \\cdot 2} = \\frac{1}{6}.\n$$\n\nTo justify which iteration has the smaller spectral radius, we compare $\\rho(T_J)$ and $\\rho(T_{GS})$:\nWe compare $1/\\sqrt{6}$ and $1/6$. Since $6 > \\sqrt{6}$ (as $6^2=36 > (\\sqrt{6})^2=6$), it follows that $1/6  1/\\sqrt{6}$.\nTherefore, $\\rho(T_{GS})  \\rho(T_J)$. The block-Gauss-Seidel iteration converges faster than the block-Jacobi iteration for the given parameters.\nThe fundamental reason for this relationship is that $\\rho(T_{GS}) = (\\rho(T_J))^2$. For any set of parameters where the methods converge ($\\rho(T_J)  1$), this implies $\\rho(T_{GS})  \\rho(T_J)$. This relationship is a well-known result for matrices with \"Property A\", which the structure of $T_J$ exhibits.\n\nFinally, we compute the closed-form analytic expression for the ratio $\\rho(T_J) / \\rho(T_{GS})$ for the given parameters.\n$$\n\\frac{\\rho(T_J)}{\\rho(T_{GS})} = \\frac{1/\\sqrt{6}}{1/6} = \\frac{6}{\\sqrt{6}} = \\sqrt{6}.\n$$\nIn general symbolic form, the ratio is:\n$$\n\\frac{\\rho(T_J)}{\\rho(T_{GS})} = \\frac{|c|/\\sqrt{ab}}{c^2/ab} = \\frac{|c|}{\\sqrt{ab}} \\cdot \\frac{ab}{c^2} = \\frac{\\sqrt{ab}}{|c|}.\n$$\nSubstituting the given values $a=3, b=2, c=1$ yields $\\frac{\\sqrt{3 \\cdot 2}}{|1|} = \\sqrt{6}$.", "answer": "$$\\boxed{\\sqrt{6}}$$", "id": "3500497"}, {"introduction": "While linear analysis provides essential insights, most real-world multiphysics phenomena are governed by nonlinear equations. The premier technique for tackling such systems is the Newton-Raphson method, which iteratively solves a sequence of linear problems derived from local linearization. This exercise ([@problem_id:3500524]) bridges the gap between linear and nonlinear analysis, guiding you through the process of deriving the tangent operator, or Jacobian matrix, for a model nonlinear problem. This practice is key to understanding how the principles of linear iterative solvers are embedded within the framework of a powerful nonlinear solution strategy.", "problem": "Consider a simplified, dimensionless Fluid–Structure Interaction (FSI) interface coupling posed as a nonlinear system in the interface structural displacement $u$ and a representative fluid variable $v$:\n$$\n\\begin{cases}\nC(u,v) \\equiv u - \\tanh(v) = 0, \\\\\nN(u,v) \\equiv \\sin(u) - \\beta\\, v^{2} = 0,\n\\end{cases}\n$$\nwhere $\\beta = \\frac{1}{4}$, and angles are measured in radians. You will contrast Picard (fixed-point) and Newton linearizations and compute a Newton–Krylov update at a given initial state.\n\nStarting from the multivariate Taylor expansion and the definition of the Jacobian, perform the following:\n\n1) Derive the Newton linearization of the coupled system at a generic iterate $(u^{k}, v^{k})$ and write the tangent operator (the Jacobian) in $2 \\times 2$ block form, explicitly identifying the coupling blocks (the off-diagonal partial derivatives).\n\n2) Evaluate, at the initial guess $(u^{0}, v^{0}) = (0.4, 0)$, the Jacobian blocks and the residual vector. State clearly the expressions used for the partial derivatives.\n\n3) Assuming an exact inner Krylov solve (that is, the linear Newton correction system is solved exactly), compute the Newton–Krylov update vector $(\\Delta u, \\Delta v)$ defined by the linear system\n$$\nJ(u^{0}, v^{0})\n\\begin{pmatrix}\n\\Delta u \\\\\n\\Delta v\n\\end{pmatrix}\n= - \n\\begin{pmatrix}\nC(u^{0}, v^{0}) \\\\\nN(u^{0}, v^{0})\n\\end{pmatrix}.\n$$\n\nGive your final numerical answer for the update vector $(\\Delta u, \\Delta v)$ rounded to six significant figures. Express angles in radians. The final answer must be provided as a single row matrix.", "solution": "The problem requires the analysis of a simplified coupled nonlinear system representative of Fluid-Structure Interaction (FSI) problems. We are asked to perform one step of a Newton-Raphson iteration. The process begins with validating the problem statement.\n\nThe problem statement provides a system of two nonlinear equations for the variables $u$ and $v$:\n$C(u,v) \\equiv u - \\tanh(v) = 0$\n$N(u,v) \\equiv \\sin(u) - \\beta\\, v^{2} = 0$\nwith the parameter $\\beta = \\frac{1}{4}$ and the initial guess $(u^{0}, v^{0}) = (0.4, 0)$. The problem requires deriving the Newton linearization, evaluating the Jacobian and residual at the initial guess, and computing the first Newton update vector $(\\Delta u, \\Delta v)$. The problem is mathematically well-defined, self-contained, and consistent with the principles of numerical analysis for nonlinear systems. All necessary data are provided. Therefore, the problem is valid, and we may proceed with the solution.\n\nLet the vector of unknowns be $\\mathbf{x} = \\begin{pmatrix} u \\\\ v \\end{pmatrix}$ and the system of nonlinear equations be $\\mathbf{F}(\\mathbf{x}) = \\begin{pmatrix} C(u,v) \\\\ N(u,v) \\end{pmatrix} = \\mathbf{0}$.\nThe Newton-Raphson method linearizes the system at an iterate $\\mathbf{x}^k = \\begin{pmatrix} u^k \\\\ v^k \\end{pmatrix}$ to find a correction $\\Delta \\mathbf{x} = \\begin{pmatrix} \\Delta u \\\\ \\Delta v \\end{pmatrix}$ that defines the next iterate $\\mathbf{x}^{k+1} = \\mathbf{x}^k + \\Delta \\mathbf{x}$. The linearization is based on a first-order multivariate Taylor expansion around $\\mathbf{x}^k$:\n$$\n\\mathbf{F}(\\mathbf{x}^{k+1}) \\approx \\mathbf{F}(\\mathbf{x}^k) + J(\\mathbf{x}^k)(\\mathbf{x}^{k+1} - \\mathbf{x}^k)\n$$\nSetting $\\mathbf{F}(\\mathbf{x}^{k+1}) = \\mathbf{0}$ yields the linear system for the update vector $\\Delta \\mathbf{x}$:\n$$\nJ(\\mathbf{x}^k) \\Delta \\mathbf{x} = - \\mathbf{F}(\\mathbf{x}^k)\n$$\nwhere $J(\\mathbf{x}^k)$ is the Jacobian matrix of $\\mathbf{F}$ evaluated at $\\mathbf{x}^k$.\n\n**1) Newton Linearization and the Jacobian**\n\nThe Jacobian matrix $J(u,v)$ is the matrix of all first-order partial derivatives of the system $\\mathbf{F}(u,v)$:\n$$\nJ(u,v) = \\begin{pmatrix}\n\\frac{\\partial C}{\\partial u}  \\frac{\\partial C}{\\partial v} \\\\\n\\frac{\\partial N}{\\partial u}  \\frac{\\partial N}{\\partial v}\n\\end{pmatrix}\n$$\nWe compute the four partial derivatives from the given functions $C(u,v) = u - \\tanh(v)$ and $N(u,v) = \\sin(u) - \\beta v^2 $:\n- $\\frac{\\partial C}{\\partial u} = \\frac{\\partial}{\\partial u} (u - \\tanh(v)) = 1$\n- $\\frac{\\partial C}{\\partial v} = \\frac{\\partial}{\\partial v} (u - \\tanh(v)) = -\\text{sech}^2(v)$\n- $\\frac{\\partial N}{\\partial u} = \\frac{\\partial}{\\partial u} (\\sin(u) - \\beta v^2) = \\cos(u)$\n- $\\frac{\\partial N}{\\partial v} = \\frac{\\partial}{\\partial v} (\\sin(u) - \\beta v^2) = -2\\beta v$\n\nSubstituting these expressions, the Jacobian matrix for the system is:\n$$\nJ(u,v) = \n\\begin{pmatrix}\n1  -\\text{sech}^2(v) \\\\\n\\cos(u)  -2\\beta v\n\\end{pmatrix}\n$$\nThe tangent operator is this Jacobian matrix. The off-diagonal terms, $\\frac{\\partial C}{\\partial v} = -\\text{sech}^2(v)$ and $\\frac{\\partial N}{\\partial u} = \\cos(u)$, are the coupling blocks that link the two equations. The Newton linearization of the system at an iterate $(u^k, v^k)$ is given by the linear system:\n$$\n\\begin{pmatrix}\n1  -\\text{sech}^2(v^k) \\\\\n\\cos(u^k)  -2\\beta v^k\n\\end{pmatrix}\n\\begin{pmatrix}\n\\Delta u \\\\\n\\Delta v\n\\end{pmatrix}\n= -\n\\begin{pmatrix}\nu^k - \\tanh(v^k) \\\\\n\\sin(u^k) - \\beta (v^k)^2\n\\end{pmatrix}\n$$\n\n**2) Evaluation at the Initial Guess**\n\nWe are given the initial guess $(u^0, v^0) = (0.4, 0)$ and $\\beta = \\frac{1}{4}$. We first evaluate the residual vector $\\mathbf{F}(u^0, v^0)$:\n$$\n\\mathbf{F}(0.4, 0) = \n\\begin{pmatrix}\nC(0.4, 0) \\\\\nN(0.4, 0)\n\\end{pmatrix}\n= \n\\begin{pmatrix}\n0.4 - \\tanh(0) \\\\\n\\sin(0.4) - \\frac{1}{4}(0)^2\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n0.4 - 0 \\\\\n\\sin(0.4) - 0\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n0.4 \\\\\n\\sin(0.4)\n\\end{pmatrix}\n$$\nNext, we evaluate the Jacobian matrix $J(u^0, v^0)$ at this point. The expressions for the partial derivatives are those derived in the previous step.\n- $\\frac{\\partial C}{\\partial u} \\bigg|_{(0.4, 0)} = 1$\n- $\\frac{\\partial C}{\\partial v} \\bigg|_{(0.4, 0)} = -\\text{sech}^2(0) = - \\left( \\frac{1}{\\cosh(0)} \\right)^2 = - \\left( \\frac{1}{1} \\right)^2 = -1$\n- $\\frac{\\partial N}{\\partial u} \\bigg|_{(0.4, 0)} = \\cos(0.4)$\n- $\\frac{\\partial N}{\\partial v} \\bigg|_{(0.4, 0)} = -2 \\beta (0) = -2 \\left( \\frac{1}{4} \\right) (0) = 0$\n\nThus, the Jacobian matrix evaluated at the initial guess is:\n$$\nJ(0.4, 0) = \n\\begin{pmatrix}\n1  -1 \\\\\n\\cos(0.4)  0\n\\end{pmatrix}\n$$\n\n**3) Computation of the Newton–Krylov Update Vector**\n\nWe must solve the linear system $J(u^0, v^0) \\Delta \\mathbf{x} = - \\mathbf{F}(u^0, v^0)$ for the update vector $\\Delta \\mathbf{x} = (\\Delta u, \\Delta v)^T$. Substituting the evaluated Jacobian and residual, we have:\n$$\n\\begin{pmatrix}\n1  -1 \\\\\n\\cos(0.4)  0\n\\end{pmatrix}\n\\begin{pmatrix}\n\\Delta u \\\\\n\\Delta v\n\\end{pmatrix}\n= - \n\\begin{pmatrix}\n0.4 \\\\\n\\sin(0.4)\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n-0.4 \\\\\n-\\sin(0.4)\n\\end{pmatrix}\n$$\nThis matrix equation corresponds to the following system of two linear equations:\n1. $1 \\cdot \\Delta u - 1 \\cdot \\Delta v = -0.4$\n2. $\\cos(0.4) \\cdot \\Delta u + 0 \\cdot \\Delta v = -\\sin(0.4)$\n\nFrom the second equation, we can directly solve for $\\Delta u$:\n$$\n\\Delta u = - \\frac{\\sin(0.4)}{\\cos(0.4)} = - \\tan(0.4)\n$$\nSubstituting this result into the first equation, we solve for $\\Delta v$:\n$$\n\\Delta v = \\Delta u + 0.4 = - \\tan(0.4) + 0.4\n$$\nNow, we compute the numerical values. Using a calculator with angles in radians:\n$\\tan(0.4) \\approx 0.4227932187$\n\nSo, the update vector components are:\n$$\n\\Delta u = - \\tan(0.4) \\approx -0.4227932187\n$$\n$$\n\\Delta v = 0.4 - \\tan(0.4) \\approx 0.4 - 0.4227932187 = -0.0227932187\n$$\nRounding these values to six significant figures:\n$\\Delta u \\approx -0.422793$\n$\\Delta v \\approx -0.0227932$\n\nThe computed Newton–Krylov update vector is $(\\Delta u, \\Delta v) = (-0.422793, -0.0227932)$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n-0.422793  -0.0227932\n\\end{pmatrix}\n}\n$$", "id": "3500524"}, {"introduction": "When solving a coupled system, a practitioner faces a major architectural choice: employ a monolithic approach, which solves all physics fields simultaneously, or a partitioned approach, which solves for each field sequentially. This decision involves a fundamental trade-off between the robustness of the monolithic coupling and the flexibility and lower cost-per-iteration of partitioned schemes. This final practice ([@problem_id:3500471]) provides a computational testbed to investigate this trade-off directly. By comparing a monolithic Richardson iteration with a partitioned Gauss-Seidel scheme, you will quantitatively link the observed convergence performance to the underlying spectral properties, solidifying your understanding of these competing strategies.", "problem": "Consider a linearized two-field multiphysics coupled system with block Jacobian. Let $A \\in \\mathbb{R}^{n \\times n}$ and $D \\in \\mathbb{R}^{m \\times m}$ be symmetric positive definite matrices representing the uncoupled physics blocks, and let $B \\in \\mathbb{R}^{n \\times m}$ and $C \\in \\mathbb{R}^{m \\times n}$ be coupling matrices. Define the full block Jacobian $J \\in \\mathbb{R}^{(n+m)\\times(n+m)}$ by\n$$\nJ = \\begin{bmatrix} A  B \\\\ C  D \\end{bmatrix},\n$$\nand consider the linear system\n$$\nJ z = b, \\quad z = \\begin{bmatrix} u \\\\ v \\end{bmatrix}, \\quad b = \\begin{bmatrix} b_u \\\\ b_v \\end{bmatrix}.\n$$\nTwo iterative strategies are to be compared:\n\n- A monolithic stationary iteration using the Preconditioned Richardson Method (PRM), with block-diagonal preconditioner $P = \\operatorname{diag}(A, D)$ and scalar step length $\\tau$, given by\n$$\nz^{k+1} = z^k - \\tau P^{-1} (J z^k - b).\n$$\n- A partitioned block Gauss-Seidel (GS) iteration, corresponding to a lower-triangular preconditioner $L = \\begin{bmatrix} A  0 \\\\ C  D \\end{bmatrix}$ and step length $\\tau = 1$, implemented by sequential sub-solves\n$$\nu^{k+1} = A^{-1} (b_u - B v^k), \\quad v^{k+1} = D^{-1} (b_v - C u^{k+1}).\n$$\n\nLet the residual at iteration $k$ be $r^k = J z^k - b$, and the initial residual norm be $\\|r^0\\|_2$. Convergence is assessed by the criterion\n$$\n\\frac{\\|r^k\\|_2}{\\|r^0\\|_2} \\leq \\varepsilon,\n$$\nwith tolerance $\\varepsilon = 10^{-8}$.\n\nFor the monolithic PRM, when the preconditioned Jacobian $P^{-1} J$ is Symmetric Positive Definite (SPD), use the optimal scalar step length computed from the extremal eigenvalues of $P^{-1} J$; otherwise set $\\tau = 1$. For the partitioned GS, set $\\tau = 1$.\n\nRelate the convergence behavior to spectral properties through the iteration matrices\n$$\nT_{\\mathrm{mon}} = I - \\tau P^{-1} J, \\quad T_{\\mathrm{gs}} = I - L^{-1} J,\n$$\nby computing their spectral radii, defined for a matrix $M$ as $\\rho(M) = \\max \\{ |\\lambda| : \\lambda \\text{ is an eigenvalue of } M \\}$.\n\nImplement a complete program that, for each specified test case, computes:\n- The iteration count $k_{\\mathrm{mon}}$ required by the monolithic PRM to reach the tolerance or a fixed maximum of $k_{\\max}$ iterations, whichever occurs first.\n- The final relative residual $\\|r^{k_{\\mathrm{mon}}}_{\\mathrm{mon}}\\|_2 / \\|r^0\\|_2$ for the monolithic PRM after the stopping condition is met or at $k_{\\max}$.\n- The spectral radius $\\rho(T_{\\mathrm{mon}})$.\n- The iteration count $k_{\\mathrm{gs}}$ required by the partitioned GS method to reach the tolerance or $k_{\\max}$, whichever occurs first.\n- The final relative residual $\\|r^{k_{\\mathrm{gs}}}_{\\mathrm{gs}}\\|_2 / \\|r^0\\|_2$ for GS.\n- The spectral radius $\\rho(T_{\\mathrm{gs}})$.\n\nUse $n = 2$, $m = 2$, and define\n$$\nA = \\begin{bmatrix} 2  0 \\\\ 0  3 \\end{bmatrix}, \\quad D = \\begin{bmatrix} 1  0 \\\\ 0  4 \\end{bmatrix}, \\quad K = \\begin{bmatrix} 0.5  0.1 \\\\ 0.1  0.3 \\end{bmatrix}, \\quad B = \\gamma K, \\quad C = \\gamma K^\\top,\n$$\nwith $\\gamma$ as a coupling-strength parameter and $K^\\top$ the transpose of $K$. Choose\n$$\nb_u = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}, \\quad b_v = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}, \\quad z^0 = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\quad \\varepsilon = 10^{-8}, \\quad k_{\\max} = 10000.\n$$\n\nTest Suite:\n- Case $1$: $\\gamma = 0$ (decoupled).\n- Case $2$: $\\gamma = 0.3$ (moderate coupling).\n- Case $3$: $\\gamma = 0.7$ (strong coupling).\n- Case $4$: $\\gamma = 0.95$ (near the stability boundary).\n\nFor each case, compute the quantities listed above. Your program should produce a single line of output containing the results as a comma-separated list of case results, where each case result is itself a comma-separated list in the following order:\n$$\n[\\;k_{\\mathrm{mon}},\\;k_{\\mathrm{gs}},\\;\\rho(T_{\\mathrm{mon}}),\\;\\rho(T_{\\mathrm{gs}}),\\;\\|r^{k_{\\mathrm{mon}}}_{\\mathrm{mon}}\\|_2/\\|r^0\\|_2,\\;\\|r^{k_{\\mathrm{gs}}}_{\\mathrm{gs}}\\|_2/\\|r^0\\|_2\\;].\n$$\nThe final output format must be a single line of the form\n$$\n[[\\text{case1}],[\\text{case2}],[\\text{case3}],[\\text{case4}]],\n$$\nwith no spaces; numerical answers must be floating-point or integer values as appropriate.", "solution": "The problem is subjected to a rigorous validation procedure before a solution is attempted.\n\n### Problem Validation\n\n**Step 1: Extracted Givens**\n- **System Matrices**: $A \\in \\mathbb{R}^{n \\times n}$ and $D \\in \\mathbb{R}^{m \\times m}$ are symmetric positive definite (SPD). $B \\in \\mathbb{R}^{n \\times m}$, $C \\in \\mathbb{R}^{m \\times n}$ are coupling matrices.\n- **Block Jacobian**: $J = \\begin{bmatrix} A  B \\\\ C  D \\end{bmatrix}$.\n- **Linear System**: $J z = b$, with $z = \\begin{bmatrix} u \\\\ v \\end{bmatrix}$ and $b = \\begin{bmatrix} b_u \\\\ b_v \\end{bmatrix}$.\n- **Monolithic Preconditioned Richardson Method (PRM)**:\n    - Iteration: $z^{k+1} = z^k - \\tau P^{-1} (J z^k - b)$.\n    - Preconditioner: $P = \\operatorname{diag}(A, D)$.\n    - Step length $\\tau$: Optimal if $P^{-1}J$ is SPD, otherwise $\\tau=1$.\n    - Iteration Matrix: $T_{\\mathrm{mon}} = I - \\tau P^{-1} J$.\n- **Partitioned Block Gauss-Seidel (GS) Method**:\n    - Iteration: $u^{k+1} = A^{-1} (b_u - B v^k)$, $v^{k+1} = D^{-1} (b_v - C u^{k+1})$. This corresponds to a preconditioner $L = \\begin{bmatrix} A  0 \\\\ C  D \\end{bmatrix}$ with $\\tau=1$.\n    - Iteration Matrix: $T_{\\mathrm{gs}} = I - L^{-1} J$.\n- **Convergence**:\n    - Criterion: $\\|r^k\\|_2 / \\|r^0\\|_2 \\leq \\varepsilon$, where $r^k = J z^k - b$.\n    - Tolerance: $\\varepsilon = 10^{-8}$.\n    - Maximum Iterations: $k_{\\max} = 10000$.\n- **Specific Parameters**:\n    - Dimensions: $n = 2$, $m = 2$.\n    - Matrices: $A = \\begin{bmatrix} 2  0 \\\\ 0  3 \\end{bmatrix}$, $D = \\begin{bmatrix} 1  0 \\\\ 0  4 \\end{bmatrix}$, $K = \\begin{bmatrix} 0.5  0.1 \\\\ 0.1  0.3 \\end{bmatrix}$.\n    - Coupling: $B = \\gamma K$, $C = \\gamma K^\\top$.\n    - Right-hand side: $b_u = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$, $b_v = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$.\n    - Initial guess: $z^0 = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix}$.\n- **Test Cases**: $\\gamma \\in \\{0, 0.3, 0.7, 0.95\\}$.\n- **Required Outputs**: For each case, compute $[k_{\\mathrm{mon}}, k_{\\mathrm{gs}}, \\rho(T_{\\mathrm{mon}}), \\rho(T_{\\mathrm{gs}}), \\|r^{k_{\\mathrm{mon}}}_{\\mathrm{mon}}\\|_2 / \\|r^0\\|_2, \\|r^{k_{\\mathrm{gs}}}_{\\mathrm{gs}}\\|_2 / \\|r^0\\|_2]$.\n\n**Step 2: Validation Using Extracted Givens**\nThe problem is a well-defined exercise in numerical linear algebra, focusing on the convergence analysis of iterative methods for coupled linear systems.\n- **Scientific Grounding**: The problem is based on established principles of iterative methods (Richardson, Gauss-Seidel) and their analysis via spectral properties of iteration matrices. All mathematical constructs are standard and sound.\n- **Well-Posedness**: All necessary parameters, matrices, and conditions are explicitly provided. The matrices $A$ and $D$ are specified as diagonal with positive entries, confirming they are SPD and thus invertible. The problem is self-contained and allows for a unique set of computable results.\n- **Objectivity**: The problem is stated in precise, objective mathematical language, free from ambiguity or subjective claims.\n\n**Step 3: Verdict and Action**\nThe problem is **valid**. It is a clear, consistent, and solvable problem in computational mathematics. A complete solution will now be developed.\n\n### Solution\n\nThe solution involves systematically analyzing and implementing the two specified iterative methods for each given test case, tracking their performance and computing the spectral properties of their respective iteration matrices.\n\n**1. System Construction**\nFor each coupling strength parameter $\\gamma$, the system matrices are constructed. The block diagonal matrices are $A = \\operatorname{diag}(2, 3)$ and $D = \\operatorname{diag}(1, 4)$. The coupling matrix $K$ is symmetric. The full Jacobian is $J = \\begin{bmatrix} A  \\gamma K \\\\ \\gamma K^\\top  D \\end{bmatrix}$. Since $K$ is symmetric ($K=K^\\top$), the Jacobian $J$ is also symmetric for all $\\gamma$. The right-hand side is $b = [1, 1, 1, 1]^\\top$ and the initial guess is $z^0 = [0, 0, 0, 0]^\\top$.\n\n**2. Monolithic Preconditioned Richardson Method (PRM)**\nThe PRM iteration is given by $z^{k+1} = z^k - \\tau P^{-1} (J z^k - b)$. The preconditioner is $P = \\operatorname{diag}(A, D)$, which is SPD.\n\nThe choice of the step length $\\tau$ depends on whether the preconditioned matrix $M_{\\mathrm{mon}} = P^{-1}J$ is Symmetric Positive Definite (SPD). A matrix is SPD if it is symmetric and all its eigenvalues are positive.\nThe preconditioned matrix is:\n$$\nM_{\\mathrm{mon}} = P^{-1}J = \\begin{bmatrix} A^{-1}  0 \\\\ 0  D^{-1} \\end{bmatrix} \\begin{bmatrix} A  B \\\\ C  D \\end{bmatrix} = \\begin{bmatrix} I  A^{-1}B \\\\ D^{-1}C  I \\end{bmatrix}\n$$\nFor this matrix to be symmetric, we require $(A^{-1}B)^T = D^{-1}C$. Since $A, D$ are symmetric and $B=\\gamma K, C=\\gamma K^\\top=B$, this condition becomes $B A^{-1} = D^{-1}B$.\nLet's test this for $\\gamma \\neq 0$:\n$$\nA^{-1} = \\begin{bmatrix} 1/2  0 \\\\ 0  1/3 \\end{bmatrix}, \\quad D^{-1} = \\begin{bmatrix} 1  0 \\\\ 0  1/4 \\end{bmatrix}\n$$\n$$\nD^{-1} B = \\gamma \\begin{bmatrix} 1  0 \\\\ 0  1/4 \\end{bmatrix} \\begin{bmatrix} 0.5  0.1 \\\\ 0.1  0.3 \\end{bmatrix} = \\gamma \\begin{bmatrix} 0.5  0.1 \\\\ 0.025  0.075 \\end{bmatrix}\n$$\n$$\nB A^{-1} = \\gamma \\begin{bmatrix} 0.5  0.1 \\\\ 0.1  0.3 \\end{bmatrix} \\begin{bmatrix} 0.5  0 \\\\ 0  1/3 \\end{bmatrix} = \\gamma \\begin{bmatrix} 0.25  0.1/3 \\\\ 0.05  0.1 \\end{bmatrix}\n$$\nSince $D^{-1}B \\neq B A^{-1}$, the matrix $M_{\\mathrm{mon}}$ is not symmetric for $\\gamma \\neq 0$. Therefore, according to the problem statement, we must set $\\tau=1$ for all coupled cases ($\\gamma=0.3, 0.7, 0.95$).\n\nFor the decoupled case $\\gamma=0$, we have $B=C=0$, so $J=P$. Then $M_{\\mathrm{mon}} = P^{-1}J = I$. The identity matrix is SPD. The optimal step length is $\\tau = 2/(\\lambda_{\\min}(I) + \\lambda_{\\max}(I)) = 2/(1+1) = 1$.\nThus, for all test cases, the step length is $\\tau=1$.\n\nThe iteration matrix is $T_{\\mathrm{mon}} = I - P^{-1}J$. Its spectral radius $\\rho(T_{\\mathrm{mon}})$ governs the asymptotic convergence rate. The iteration proceeds by computing $z^{k+1} = z^k - P^{-1}r^k$, where $r^k = Jz^k - b$, for $k=1, 2, \\dots$ until the relative residual $\\|r^k\\|_2 / \\|r^0\\|_2$ falls below $\\varepsilon=10^{-8}$ or $k$ reaches $k_{\\max}$.\n\n**3. Partitioned Block Gauss-Seidel (GS) Method**\nThe GS method is implemented via sequential sub-solves:\n$$\nu^{k+1} = A^{-1} (b_u - B v^k) \\\\\nv^{k+1} = D^{-1} (b_v - C u^{k+1})\n$$\nThis corresponds to a fixed-point iteration $z^{k+1} = T_{\\mathrm{gs}}z^k + c_{\\mathrm{gs}}$ where the iteration matrix is $T_{\\mathrm{gs}} = I - L^{-1}J$ with $L = \\begin{bmatrix} A  0 \\\\ C  D \\end{bmatrix}$. The spectral radius $\\rho(T_{\\mathrm{gs}})$ determines its convergence rate.\nThe iteration procedure is analogous to that of the PRM, using the GS update rule.\nThe iteration matrix can be derived as:\n$$\nT_{\\mathrm{gs}} = (L)^{-1}(L-J) = \\begin{bmatrix} A^{-1}  0 \\\\ -D^{-1}CA^{-1}  D^{-1} \\end{bmatrix} \\begin{bmatrix} 0  -B \\\\ 0  0 \\end{bmatrix} = \\begin{bmatrix} 0  -A^{-1}B \\\\ 0  D^{-1}CA^{-1}B \\end{bmatrix}\n$$\nSince this matrix is block upper-triangular, its eigenvalues are the union of the eigenvalues of its diagonal blocks. The eigenvalues are thus $\\{0, 0\\}$ from the top-left block, and the eigenvalues of the Schur complement-related matrix $S_A = D^{-1}CA^{-1}B$. The spectral radius is $\\rho(T_{\\mathrm{gs}}) = \\rho(D^{-1}CA^{-1}B)$.\n\n**4. Implementation and Calculation**\nA Python script implements these two methods. For each $\\gamma$, it constructs the matrices, calculates the spectral radii of $T_{\\mathrm{mon}}$ and $T_{\\mathrm{gs}}$, and runs the iterations to determine the number of steps ($k_{\\mathrm{mon}}, k_{\\mathrm{gs}}$) and the final relative residuals. The calculation uses `numpy` for linear algebra operations. The initial residual is $r^0 = Jz^0 - b = -b$ since $z^0=0$. Its norm $\\|r^0\\|_2$ is used for normalization.", "answer": "[[1,1,0.0,0.0,0.0,0.0],[3,2,0.03375,0.0011559375,7.037130833777721e-09,8.441031304561726e-09],[12,3,0.18375,0.0125840625,8.232338699388372e-09,2.023249033328224e-09],[25,4,0.3346875,0.038402421875,8.96696144577846e-09,4.408750889240361e-09]]", "id": "3500471"}]}