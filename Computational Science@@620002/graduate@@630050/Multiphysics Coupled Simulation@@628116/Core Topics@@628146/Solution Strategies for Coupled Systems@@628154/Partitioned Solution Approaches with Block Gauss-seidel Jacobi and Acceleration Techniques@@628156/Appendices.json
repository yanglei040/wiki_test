{"hands_on_practices": [{"introduction": "Understanding *why* partitioned schemes struggle with strongly coupled problems is the first step toward solving them. This exercise uses a simplified 'toy model' to reveal the fundamental mathematical connection between the strength of physical coupling and the numerical difficulty of the solution [@problem_id:3520332]. By analyzing this model, you will see how the same parameter that makes the problem ill-conditioned also slows iterative convergence to a crawl, motivating the need for acceleration.", "problem": "Consider a two-field, linearly coupled, steady multiphysics system written in block form as\n$$\n\\begin{pmatrix}\nA  B \\\\\nC  D\n\\end{pmatrix}\n\\begin{pmatrix}\nu \\\\\nv\n\\end{pmatrix}\n=\n\\begin{pmatrix}\nf \\\\\ng\n\\end{pmatrix},\n$$\nwhere $u$ and $v$ represent two interacting physical fields (for example, fluid velocity and structural displacement), and $A$, $B$, $C$, $D$ are square operators of compatible dimension. Assume the operators are bounded and invertible where required. In a partitioned (segregated) solution approach, one may eliminate $v$ to form a reduced system for $u$ characterized by an effective operator built from block elimination. In this toy model, let $A = I$, $D = I$, $B = \\alpha I$, and $C = \\beta I$, where $I$ is the identity operator and $\\alpha, \\beta \\in \\mathbb{R}$ are coupling parameters that quantify the strength of inter-physics interaction.\n\nTasks:\n- By performing exact block elimination of $v$, obtain the reduced operator acting on $u$ and express it in closed form in terms of $\\alpha$, $\\beta$, and $I$. Provide your final result for this operator in symbolic form.\n- Starting from the contraction mapping principle and the spectral-radius criterion for linear fixed-point iterations, derive the error-propagation operators for block Jacobi and block Gauss–Seidel iterations applied to the above toy model, and use these to explain how the magnitude $|\\alpha \\beta|$ controls both the algebraic difficulty of the reduced operator and the convergence rate of the partitioned iterations. In your explanation, relate the limit $|\\alpha \\beta| \\to 1$ to conditioning of the reduced operator and to the iteration’s contraction factor, and comment on how simple linear mixing (relaxation) modifies the effective contraction factor.\n\nAnswer format:\n- Express the reduced operator as a single closed-form analytic expression. No numerical evaluation is required.", "solution": "The multiphysics system is given in block matrix form:\n$$\n\\begin{pmatrix}\nA  B \\\\\nC  D\n\\end{pmatrix}\n\\begin{pmatrix}\nu \\\\\nv\n\\end{pmatrix}\n=\n\\begin{pmatrix}\nf \\\\\ng\n\\end{pmatrix}\n$$\nThis corresponds to a system of two coupled linear equations:\n$$\nAu + Bv = f \\quad (1)\n$$\n$$\nCu + Dv = g \\quad (2)\n$$\n\n### Part 1: Derivation of the Reduced Operator\n\nTo obtain the reduced operator for the field $u$, we perform a block elimination of the field $v$. This is analogous to solving for one variable and substituting it into the other equation. Assuming the operator $D$ is invertible, we can formally solve equation $(2)$ for $v$:\n$$\nDv = g - Cu\n$$\n$$\nv = D^{-1}(g - Cu) \\quad (3)\n$$\nNext, we substitute this expression for $v$ back into equation $(1)$:\n$$\nAu + B \\left[ D^{-1}(g - Cu) \\right] = f\n$$\nWe distribute the operator $B$ and rearrange the terms to group those involving $u$ on the left-hand side and the remaining known terms on the right-hand side.\n$$\nAu + BD^{-1}g - BD^{-1}Cu = f\n$$\n$$\nAu - BD^{-1}Cu = f - BD^{-1}g\n$$\nFactoring out $u$ on the left side, we obtain the reduced system:\n$$\n(A - BD^{-1}C)u = f - BD^{-1}g\n$$\nThe operator acting on $u$ is the reduced operator, commonly known as the Schur complement of the block $D$ in the system matrix. Let us denote this operator as $S$:\n$$\nS = A - BD^{-1}C\n$$\nNow, we substitute the specific forms of the operators given in the problem: $A = I$, $D = I$, $B = \\alpha I$, and $C = \\beta I$, where $I$ is the identity operator and $\\alpha, \\beta \\in \\mathbb{R}$. The inverse of the identity operator is the identity operator itself, $I^{-1} = I$.\n$$\nS = I - (\\alpha I)(I^{-1})(\\beta I)\n$$\nSince scalar multiplication is commutative and associative with operator multiplication, and $I \\cdot I = I$, we can simplify:\n$$\nS = I - \\alpha \\beta (I \\cdot I \\cdot I)\n$$\n$$\nS = I - \\alpha \\beta I\n$$\nThis can be factored to yield the final closed form for the reduced operator:\n$$\nS = (1 - \\alpha\\beta)I\n$$\n\n### Part 2: Analysis of Partitioned Iteration Schemes\n\nWe now analyze the convergence of partitioned (segregated) iterative schemes based on the contraction mapping principle. An iterative method of the form $x_{k+1} = G x_k + c$ converges for any initial guess if and only if the spectral radius of the iteration matrix $G$, denoted $\\rho(G)$, is less than $1$. The value of $\\rho(G)$ also serves as the asymptotic contraction factor of the error.\n\n**Block Jacobi Iteration**\nIn the block Jacobi method, the field values from the previous iteration, $k$, are used to update all fields for the new iteration, $k+1$.\n$$\nA u^{k+1} = f - B v^k\n$$\n$$\nD v^{k+1} = g - C u^k\n$$\nTo find the error propagation, we subtract the exact equations, yielding:\n$$\nA (u^{k+1}-u) = -B (v^k-v) \\implies A e_u^{k+1} = -B e_v^k\n$$\n$$\nD (v^{k+1}-v) = -C (u^k-u) \\implies D e_v^{k+1} = -C e_u^k\n$$\nWriting this in block matrix form for the error vector $e^k = \\begin{pmatrix} e_u^k \\\\ e_v^k \\end{pmatrix}$:\n$$\n\\begin{pmatrix} e_u^{k+1} \\\\ e_v^{k+1} \\end{pmatrix} = \\begin{pmatrix} 0  -A^{-1}B \\\\ -D^{-1}C  0 \\end{pmatrix} \\begin{pmatrix} e_u^k \\\\ e_v^k \\end{pmatrix}\n$$\nThe block Jacobi iteration matrix is $G_{BJ} = \\begin{pmatrix} 0  -A^{-1}B \\\\ -D^{-1}C  0 \\end{pmatrix}$. Substituting the toy model operators:\n$$\nG_{BJ} = \\begin{pmatrix} 0  -\\alpha I \\\\ -\\beta I  0 \\end{pmatrix}\n$$\nThe eigenvalues $\\lambda$ are found from $\\det(G_{BJ} - \\lambda I_{block}) = \\det(\\lambda^2 I - \\alpha\\beta I) = 0$, which implies $\\lambda^2 = \\alpha\\beta$, so $\\lambda = \\pm\\sqrt{\\alpha\\beta}$. The spectral radius is:\n$$\n\\rho(G_{BJ}) = |\\pm\\sqrt{\\alpha\\beta}| = \\sqrt{|\\alpha\\beta|}\n$$\nConvergence is guaranteed if $\\rho(G_{BJ})  1$, which requires $|\\alpha\\beta|  1$.\n\n**Block Gauss-Seidel Iteration**\nIn block Gauss-Seidel, we use the most recently computed values. Assuming we solve for $u$ first, the iteration matrix $G_{BGS}$ is derived from the standard splitting $K = D_{block} - L_{block} - U_{block}$, where $D_{block} = \\begin{pmatrix} A  0 \\\\ 0  D \\end{pmatrix}$, $L_{block} = \\begin{pmatrix} 0  0 \\\\ -C  0 \\end{pmatrix}$, and $U_{block} = \\begin{pmatrix} 0  -B \\\\ 0  0 \\end{pmatrix}$.\nThe iteration matrix is $G_{BGS} = (D_{block} - L_{block})^{-1} U_{block}$.\n$$\nG_{BGS} = \\begin{pmatrix} A  0 \\\\ C  D \\end{pmatrix}^{-1} \\begin{pmatrix} 0  -B \\\\ 0  0 \\end{pmatrix} = \\begin{pmatrix} A^{-1}  0 \\\\ -D^{-1}CA^{-1}  D^{-1} \\end{pmatrix} \\begin{pmatrix} 0  -B \\\\ 0  0 \\end{pmatrix} = \\begin{pmatrix} 0  -A^{-1}B \\\\ 0  D^{-1}CA^{-1}B \\end{pmatrix}\n$$\nThis is a block upper triangular matrix. Its eigenvalues are the eigenvalues of its diagonal blocks: the zero operator $0$ and the operator $D^{-1}CA^{-1}B$. For our toy model, this second operator is $I^{-1}(\\beta I)I^{-1}(\\alpha I) = \\alpha\\beta I$. The eigenvalues of $G_{BGS}$ are therefore $0$ and $\\alpha\\beta$. The spectral radius is:\n$$\n\\rho(G_{BGS}) = \\max(|0|, |\\alpha\\beta|) = |\\alpha\\beta|\n$$\nConvergence is guaranteed if $\\rho(G_{BGS})  1$, which again requires $|\\alpha\\beta|  1$.\n\n### Part 3: Relationship between Coupling, Conditioning, and Convergence\n\nThe analysis reveals that the quantity $|\\alpha\\beta|$, which represents the strength of the two-way coupling, is the critical parameter.\n\n**Relation to Algebraic Difficulty:** The reduced operator is $S = (1 - \\alpha\\beta)I$. An operator is singular (and thus infinitely ill-conditioned) if it is not invertible. For $S$ to be invertible, we require $1 - \\alpha\\beta \\neq 0$. As $|\\alpha\\beta| \\to 1$, the scalar factor $(1 - \\alpha\\beta)$ approaches $0$, and the operator $S$ becomes \"closer\" to the singular zero operator. The norm of its inverse, $\\|S^{-1}\\|$, grows without bound. This means the Schur complement system becomes extremely ill-conditioned, making a monolithic solution via block elimination numerically unstable.\n\n**Relation to Convergence Rate:** The convergence rates for the partitioned schemes are determined by their spectral radii, $\\rho(G_{BJ}) = \\sqrt{|\\alpha\\beta|}$ and $\\rho(G_{BGS}) = |\\alpha\\beta|$. As $|\\alpha\\beta| \\to 1$, both spectral radii approach $1$. A spectral radius of $1$ corresponds to a stalled iteration where the error is not reduced. Therefore, strong coupling (i.e., $|\\alpha\\beta|$ close to $1$) leads to a drastic slowdown in the convergence of both partitioned schemes. In summary, the same parameter $\\alpha\\beta$ that governs the conditioning of the monolithic system also governs the convergence rate of the segregated schemes.\n\n**Modification by Linear Mixing (Relaxation)**\nSimple linear relaxation modifies an iterative scheme $x^{k+1} = G x^k + c$ as follows:\n$$\nx^{k+1} = (1-\\omega)x^k + \\omega(G x^k + c) = [(1-\\omega)I + \\omega G] x^k + \\omega c\n$$\nwhere $\\omega$ is the relaxation parameter. The new iteration matrix is $G_\\omega = (1-\\omega)I + \\omega G$, and its eigenvalues $\\mu_i$ are related to the eigenvalues $\\lambda_i$ of $G$ by $\\mu_i = (1-\\omega) + \\omega\\lambda_i$. For block Gauss-Seidel, the eigenvalues of $G_{BGS}$ are $0$ and $\\alpha\\beta$. The eigenvalues of the relaxed matrix $G_{\\omega, BGS}$ are thus:\n$$\n\\mu_1 = (1-\\omega) + \\omega(0) = 1-\\omega\n$$\n$$\n\\mu_2 = (1-\\omega) + \\omega(\\alpha\\beta) = 1 - \\omega(1-\\alpha\\beta)\n$$\nThe effective contraction factor is $\\rho(G_\\omega) = \\max(|1-\\omega|, |1 - \\omega(1-\\alpha\\beta)|)$. By choosing an appropriate $\\omega$, it is possible to make this new spectral radius smaller than the original $\\rho(G_{BGS})=|\\alpha\\beta|$. However, as $|\\alpha\\beta| \\to 1$ from below, even the optimal relaxation parameter results in an effective spectral radius that approaches $1$. Relaxation helps mitigate the slowdown but cannot overcome the fundamental limit posed by singular behavior at $|\\alpha\\beta|=1$.", "answer": "$$\n\\boxed{(1 - \\alpha\\beta)I}\n$$", "id": "3520332"}, {"introduction": "One of the simplest and most common methods for improving the stability of a partitioned iteration is linear relaxation. This practice asks you to analyze the effect of applying an under-relaxation parameter to a block Gauss-Seidel scheme [@problem_id:3520308]. You will derive the new contraction factor for the relaxed iteration and calculate the resulting number of steps required to reach a target tolerance, providing a quantitative understanding of how relaxation modifies convergence behavior.", "problem": "Consider a partitioned multiphysics fixed-point scheme in which a displacement field is updated in a segregated manner using a linear block Gauss-Seidel (GS) step. Let the unrelaxed displacement update be represented as a linear fixed-point map $\\Phi(u) = T u + b$, where $T$ is the iteration operator induced by the block splitting and $b$ is a constant arising from the right-hand side and the coupling data at the current outer iteration. Assume a consistent vector norm is used so that the unrelaxed contraction factor is characterized by the spectral radius $\\rho(T)$ through $\\|\\Phi(u) - \\Phi(v)\\| \\le \\rho(T) \\|u - v\\|$ for all vectors $u$ and $v$ in the relevant space. Suppose the dominant eigenvalue of $T$ responsible for the contraction factor is real and positive, which is typical for monotone linearized couplings in block Gauss-Seidel (GS) or block Jacobi schemes.\n\nAn acceleration technique is introduced via scalar under-relaxation: the relaxed displacement update is\n$$\nu^{(k+1)} \\;=\\; u^{(k)} \\;+\\; \\omega\\big(\\Phi(u^{(k)}) - u^{(k)}\\big),\n$$\nwith relaxation parameter $\\omega \\in (0,1)$. This is the standard linear mixing used to stabilize and control the convergence of segregated iterations.\n\nStarting from the definition of contraction for linear fixed-point maps and basic facts about eigenvalues of affine linear operators, derive the contraction factor of the under-relaxed map in terms of the eigenvalues of $T$. Then, for $\\omega = 0.5$ and an unrelaxed contraction factor $\\rho(T) = 0.8$, determine the minimal integer number of iterations required for the residual norm of the displacement update to be reduced by a factor of $10^{-3}$ relative to its initial value, under the conservative assumption that the maximal-modulus eigenvalue of $T$ is positive and equals $\\rho(T)$ so that the under-relaxed contraction factor is attained on that eigenmode. Express your final answer as a single number without units. No rounding instruction is necessary because the requested quantity is an integer count.", "solution": "The user has provided a problem statement that requires validation before a solution is attempted.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\n*   **Unrelaxed update map:** $\\Phi(u) = T u + b$, where $T$ is the iteration operator.\n*   **Unrelaxed contraction:** $\\|\\Phi(u) - \\Phi(v)\\| \\le \\rho(T) \\|u - v\\|$, with $\\rho(T)$ being the spectral radius of $T$.\n*   **Dominant eigenvalue of $T$:** Assumed to be real, positive, and equal to the spectral radius $\\rho(T)$.\n*   **Relaxed update map:** $u^{(k+1)} = u^{(k)} + \\omega(\\Phi(u^{(k)}) - u^{(k)})$.\n*   **Relaxation parameter:** $\\omega \\in (0,1)$.\n*   **Specific value for $\\omega$:** $\\omega = 0.5$.\n*   **Specific value for unrelaxed contraction factor:** $\\rho(T) = 0.8$.\n*   **Convergence assumption:** The under-relaxed contraction factor is attained on the eigenmode corresponding to the maximal-modulus eigenvalue of $T$.\n*   **Task 1:** Derive the contraction factor of the relaxed map in terms of the eigenvalues of $T$.\n*   **Task 2:** Find the minimal integer number of iterations, $k$, for the residual norm to be reduced by a factor of $10^{-3}$.\n\n**Step 2: Validate Using Extracted Givens**\n\nThe problem is assessed against the validation criteria:\n\n*   **Scientifically Grounded:** The problem is firmly based on the theory of stationary linear iterative methods, a core topic in numerical analysis and computational science. The concepts of iteration operators, spectral radius, convergence factors, and relaxation techniques (specifically Aitken's relaxation or linear mixing) are standard and well-established.\n*   **Well-Posed:** The problem is clearly stated. It requests a standard derivation followed by a calculation using provided data and simplifying assumptions. The assumptions (e.g., the nature of the dominant eigenvalue) ensure that a unique, meaningful answer can be found.\n*   **Objective:** The problem is described using precise, unambiguous mathematical language.\n*   **Other Flaws:** The problem does not violate any fundamental principles, is not incomplete or contradictory, and is not based on unrealistic premises. The values provided ($\\rho(T)=0.8$, $\\omega=0.5$) are typical for such problems. The topic is directly relevant to multiphysics simulation techniques.\n\n**Step 3: Verdict and Action**\n\nThe problem is **valid**. A solution will be provided.\n\n### Solution\n\nThe solution is presented in two parts as requested: first, the derivation of the relaxed contraction factor, and second, the calculation of the required number of iterations.\n\n**Part 1: Derivation of the Relaxed Contraction Factor**\n\nThe unrelaxed fixed-point iteration is given by $u^{(k+1)} = \\Phi(u^{(k)})$, where the map is $\\Phi(u) = T u + b$. The under-relaxed iteration is defined as:\n$$\nu^{(k+1)} = u^{(k)} + \\omega\\big(\\Phi(u^{(k)}) - u^{(k)}\\big)\n$$\nWe can define a new, relaxed fixed-point map, $\\Psi(u)$, such that $u^{(k+1)} = \\Psi(u^{(k)})$. Substituting the expression for $\\Phi(u)$, we get:\n$$\n\\Psi(u) = u + \\omega\\big((T u + b) - u\\big)\n$$\n$$\n\\Psi(u) = u + \\omega(T u - u + b)\n$$\n$$\n\\Psi(u) = u + \\omega(T - I)u + \\omega b\n$$\nFactoring out $u$, we can write this in the standard affine form $\\Psi(u) = T_{\\omega} u + b_{\\omega}$:\n$$\n\\Psi(u) = \\big(I + \\omega(T - I)\\big)u + (\\omega b) = \\big((1-\\omega)I + \\omega T\\big)u + \\omega b\n$$\nThe iteration operator for the relaxed scheme is therefore $T_{\\omega} = (1-\\omega)I + \\omega T$.\n\nThe asymptotic convergence rate of a stationary linear iterative method is governed by the spectral radius of its iteration operator. Let $\\lambda$ be an eigenvalue of the original operator $T$ with a corresponding eigenvector $v$, such that $T v = \\lambda v$. We can find the corresponding eigenvalue of the relaxed operator $T_{\\omega}$ by applying it to the same eigenvector $v$:\n$$\nT_{\\omega} v = \\big((1-\\omega)I + \\omega T\\big)v = (1-\\omega)Iv + \\omega(Tv)\n$$\n$$\nT_{\\omega} v = (1-\\omega)v + \\omega(\\lambda v) = \\big((1-\\omega) + \\omega\\lambda\\big)v\n$$\nThis shows that if $\\lambda$ is an eigenvalue of $T$, then $\\lambda_{\\omega} = (1-\\omega) + \\omega\\lambda$ is an eigenvalue of $T_{\\omega}$. The spectral radius of the relaxed operator, $\\rho(T_{\\omega})$, which defines the relaxed contraction factor, is the maximum modulus of its eigenvalues:\n$$\n\\rho(T_{\\omega}) = \\max_{\\lambda \\in \\sigma(T)} |\\lambda_{\\omega}| = \\max_{\\lambda \\in \\sigma(T)} |(1-\\omega) + \\omega\\lambda|\n$$\nwhere $\\sigma(T)$ is the spectrum (the set of all eigenvalues) of $T$. This expression gives the contraction factor of the under-relaxed map in terms of the eigenvalues of $T$ and the relaxation parameter $\\omega$.\n\n**Part 2: Calculation of Iterations**\n\nWe are given the unrelaxed contraction factor $\\rho(T) = 0.8$ and the relaxation parameter $\\omega = 0.5$. The problem states we should make the conservative assumption that the maximal-modulus eigenvalue of $T$ is real and positive, so we take $\\lambda_{max} = \\rho(T) = 0.8$. Furthermore, we assume that the contraction factor of the relaxed scheme, $\\rho(T_{\\omega})$, is attained for this specific eigenvalue.\n\nUsing the formula derived above, the new dominant eigenvalue is:\n$$\n\\lambda_{\\omega, max} = (1 - \\omega) + \\omega \\lambda_{max}\n$$\nSubstituting the given values:\n$$\n\\lambda_{\\omega, max} = (1 - 0.5) + (0.5)(0.8) = 0.5 + 0.4 = 0.9\n$$\nThe contraction factor for the relaxed scheme is $\\rho(T_{\\omega}) = |\\lambda_{\\omega, max}| = 0.9$. Note that under-relaxation has slowed down the convergence in this case (from $0.8$ to $0.9$), which is a possible outcome when applying it to an already convergent scheme, although it often improves stability.\n\nWe need to find the minimal integer number of iterations, $k$, such that the residual norm is reduced by a factor of at least $10^{-3}$. The ratio of the residual norm at iteration $k$ to the initial residual norm is bounded by the $k$-th power of the contraction factor:\n$$\n\\frac{\\|r^{(k)}\\|}{\\|r^{(0)}\\|} \\le (\\rho(T_{\\omega}))^k\n$$\nWe need to solve for the smallest integer $k$ that satisfies:\n$$\n(0.9)^k \\le 10^{-3}\n$$\nTo solve for $k$, we take the natural logarithm of both sides:\n$$\n\\ln\\left((0.9)^k\\right) \\le \\ln(10^{-3})\n$$\n$$\nk \\ln(0.9) \\le -3 \\ln(10)\n$$\nSince $\\ln(0.9)$ is a negative number, dividing by it reverses the inequality sign:\n$$\nk \\ge \\frac{-3 \\ln(10)}{\\ln(0.9)}\n$$\nNow, we can evaluate the right-hand side:\n$$\nk \\ge \\frac{-3 \\times 2.302585...}{-0.105360...} \\approx 65.56\n$$\nSince the number of iterations $k$ must be an integer, we must take the smallest integer that is greater than or equal to $65.56$. This is achieved by taking the ceiling of this value.\n$$\nk = \\lceil 65.56 \\rceil = 66\n$$\nTherefore, a minimum of $66$ iterations are required to achieve the desired reduction in the residual norm.", "answer": "$$\\boxed{66}$$", "id": "3520308"}, {"introduction": "When simple relaxation is insufficient, more advanced acceleration techniques are required, many of which are based on quasi-Newton methods. This exercise provides a concrete, hands-on implementation of the Interface Quasi-Newton Inverse Least Squares (IQN-ILS) algorithm [@problem_id:3520284]. By using data from previous iterations to construct an approximate inverse Jacobian, you will see how these methods generate a much more intelligent and effective correction step for the interface unknowns.", "problem": "In a partitioned (segregated) multiphysics coupling solved by a block Gauss-Seidel iteration, the interface variable update is driven by a residual mapping $r(x)$ that quantifies the mismatch between subdomains across the interface. A quasi-Newton acceleration constructs an approximation $B$ to the inverse of the interface Jacobian $\\frac{\\partial x}{\\partial r}$ such that the correction step is $-\\!B r$. The Interface Quasi-Newton Inverse Least Squares (IQN-ILS) method seeks $B$ that best satisfies the secant conditions $B\\,\\Delta r_{i} \\approx \\Delta x_{i}$ in the least-squares sense over collected iteration history, where $\\Delta r_{i}$ and $\\Delta x_{i}$ are residual and interface-update differences across iterations.\n\nStarting from the principle of least squares, define the matrices $R$ and $X$ whose columns are the two-dimensional secant pairs $\\Delta r_{i}$ and $\\Delta x_{i}$, respectively, and derive the minimizer $B$ of the Frobenius-norm objective $\\|B R - X\\|_{F}$. Then, using the resulting $B$, predict the next interface correction $-\\!B r$ for a given residual $r$.\n\nData:\n- $\\Delta r_{0} = \\begin{pmatrix}1 \\\\ 2\\end{pmatrix}$, $\\Delta x_{0} = \\begin{pmatrix}0 \\\\ 1\\end{pmatrix}$,\n- $\\Delta r_{1} = \\begin{pmatrix}3 \\\\ 1\\end{pmatrix}$, $\\Delta x_{1} = \\begin{pmatrix}5 \\\\ 3\\end{pmatrix}$,\n- $r = \\begin{pmatrix}2 \\\\ -1\\end{pmatrix}$.\n\nConstruct $R = [\\Delta r_{0}\\ \\Delta r_{1}]$ and $X = [\\Delta x_{0}\\ \\Delta x_{1}]$, compute one IQN-ILS update of $B$, and use it to form $-\\!B r$. Report, as a single scalar, the squared Euclidean norm $\\|-\\!B r\\|_{2}^{2}$. Express the final answer as an exact value with no units and do not round.", "solution": "The problem requires the calculation of an interface correction step using the Interface Quasi-Newton Inverse Least Squares (IQN-ILS) method and then finding the squared Euclidean norm of that correction.\n\nFirst, we must derive the expression for the matrix $B$ that minimizes the Frobenius-norm objective function $J(B) = \\|B R - X\\|_{F}$. Minimizing $J(B)$ is equivalent to minimizing its square, $J(B)^2 = \\|B R - X\\|_{F}^2$. The squared Frobenius norm of a matrix $A$ is given by $\\|A\\|_{F}^2 = \\text{Tr}(A^T A)$.\n$$ J(B)^2 = \\text{Tr}\\left((B R - X)^T (B R - X)\\right) $$\nThe minimization problem can be decoupled for each row of the matrix $B$. Let $\\mathbf{b}_i^T$ be the $i$-th row of $B$ and $\\mathbf{x}_i^T$ be the $i$-th row of $X$. The objective function becomes the sum of the squared Euclidean norms of the row-wise errors:\n$$ \\|B R - X\\|_{F}^2 = \\sum_{i} \\|\\mathbf{b}_i^T R - \\mathbf{x}_i^T\\|_2^2 = \\sum_{i} \\|R^T \\mathbf{b}_i - \\mathbf{x}_i\\|_2^2 $$\nEach term in the sum is an independent linear least-squares problem for the vector $\\mathbf{b}_i$. The solution to $\\min_{\\mathbf{b}_i} \\|R^T \\mathbf{b}_i - \\mathbf{x}_i\\|_2^2$ is given by the normal equations:\n$$ (R^T)^T R^T \\mathbf{b}_i = (R^T)^T \\mathbf{x}_i \\implies R R^T \\mathbf{b}_i = R \\mathbf{x}_i $$\nAssuming that the matrix $R R^T$ is invertible, we can solve for each $\\mathbf{b}_i$:\n$$ \\mathbf{b}_i = (R R^T)^{-1} R \\mathbf{x}_i $$\nAssembling the row vectors $\\mathbf{b}_i^T$ back into the matrix $B$, we arrive at the general solution for the IQN-ILS update:\n$$ B^T = (R R^T)^{-1} R X^T \\implies B = \\left((R R^T)^{-1} R X^T\\right)^T = X R^T (R R^T)^{-1} $$\nNow, we apply this to the specific data provided. The matrices $R$ and $X$ are constructed from the given secant pairs:\n$$ \\Delta r_{0} = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}, \\Delta r_{1} = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix} \\implies R = [\\Delta r_{0}\\ \\Delta r_{1}] = \\begin{pmatrix} 1  3 \\\\ 2  1 \\end{pmatrix} $$\n$$ \\Delta x_{0} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}, \\Delta x_{1} = \\begin{pmatrix} 5 \\\\ 3 \\end{pmatrix} \\implies X = [\\Delta x_{0}\\ \\Delta x_{1}] = \\begin{pmatrix} 0  5 \\\\ 1  3 \\end{pmatrix} $$\nIn this problem, the matrix $R$ is a $2 \\times 2$ square matrix. We check if it is invertible by computing its determinant:\n$$ \\det(R) = (1)(1) - (3)(2) = 1 - 6 = -5 $$\nSince $\\det(R) \\neq 0$, $R$ is invertible. In this special case, the least-squares problem $\\|B R - X\\|_{F}$ has an exact solution where the objective function is zero, i.e., $B R - X = 0$. This simplifies the general formula for $B$:\n$$ B = X R^{-1} $$\nWe now compute the inverse of $R$:\n$$ R^{-1} = \\frac{1}{\\det(R)}\\begin{pmatrix} 1  -3 \\\\ -2  1 \\end{pmatrix} = -\\frac{1}{5}\\begin{pmatrix} 1  -3 \\\\ -2  1 \\end{pmatrix} = \\begin{pmatrix} -1/5  3/5 \\\\ 2/5  -1/5 \\end{pmatrix} $$\nNext, we compute the matrix $B$ by multiplying $X$ and $R^{-1}$:\n$$ B = X R^{-1} = \\begin{pmatrix} 0  5 \\\\ 1  3 \\end{pmatrix} \\begin{pmatrix} -1/5  3/5 \\\\ 2/5  -1/5 \\end{pmatrix} $$\n$$ B = \\begin{pmatrix} 0(-\\frac{1}{5}) + 5(\\frac{2}{5})  0(\\frac{3}{5}) + 5(-\\frac{1}{5}) \\\\ 1(-\\frac{1}{5}) + 3(\\frac{2}{5})  1(\\frac{3}{5}) + 3(-\\frac{1}{5}) \\end{pmatrix} = \\begin{pmatrix} \\frac{10}{5}  -\\frac{5}{5} \\\\ \\frac{-1+6}{5}  \\frac{3-3}{5} \\end{pmatrix} = \\begin{pmatrix} 2  -1 \\\\ 1  0 \\end{pmatrix} $$\nWith the approximated inverse Jacobian $B$, we compute the next interface correction vector, $-\\!B r$, using the given residual $r$:\n$$ r = \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix} $$\n$$ -\\!B r = - \\begin{pmatrix} 2  -1 \\\\ 1  0 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix} = - \\begin{pmatrix} (2)(2) + (-1)(-1) \\\\ (1)(2) + (0)(-1) \\end{pmatrix} = - \\begin{pmatrix} 4+1 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} -5 \\\\ -2 \\end{pmatrix} $$\nFinally, we compute the squared Euclidean norm of this correction vector, denoted as $\\|-\\!B r\\|_{2}^{2}$:\n$$ \\|-\\!B r\\|_{2}^{2} = (-5)^2 + (-2)^2 = 25 + 4 = 29 $$\nThe result is an exact integer value.", "answer": "$$ \\boxed{29} $$", "id": "3520284"}]}