{"hands_on_practices": [{"introduction": "The adjoint-state method provides an elegant and efficient way to compute gradients of complex functionals. However, implementing it correctly involves intricate code coupling the forward and adjoint solvers. This exercise introduces the \"gradient check,\" or Taylor test, a fundamental technique for verifying the correctness of your implementation by comparing the adjoint-derived gradient against a finite-difference approximation. Mastering this test is a critical skill, as it requires navigating the trade-off between Taylor series truncation error and floating-point round-off error to achieve a robust validation [@problem_id:3574132].", "problem": "Consider an isotropic acoustic wave propagation model in a bounded domain $\\Omega \\subset \\mathbb{R}^3$ with absorbing boundaries. The temporally forced pressure field $u(\\mathbf{x}, t)$ satisfies a Partial Differential Equation (PDE) of the form\n$$\n\\partial_t^2 u(\\mathbf{x}, t) - \\nabla \\cdot \\left( c^2(\\mathbf{x}) \\nabla u(\\mathbf{x}, t) \\right) = s(\\mathbf{x}, t), \\quad \\mathbf{x} \\in \\Omega, \\; t \\in [0, T],\n$$\nwith appropriate initial and boundary conditions, where $c(\\mathbf{x})$ is the spatially varying wave speed and $s(\\mathbf{x}, t)$ is a known source. Let the model parameter be $m(\\mathbf{x}) = c^{-2}(\\mathbf{x})$ and denote by $R$ the sampling operator that extracts synthetic data at receiver locations, so that predicted data are $d_{\\text{pred}}(t) = R u(m; t)$. A standard least-squares misfit functional is\n$$\nJ(m) = \\frac{1}{2} \\sum_{r=1}^{N_r} \\int_0^T \\left( R u(m; t; \\mathbf{x}_r) - d_{\\text{obs}}(t; \\mathbf{x}_r) \\right)^2 \\, dt,\n$$\nwhere $d_{\\text{obs}}$ are observed data and $N_r$ is the number of receivers. The gradient $\\nabla J(m)$ with respect to $m$ is computed via the adjoint-state method and defines the directional derivative at $m$ in direction $p$ through the Euclidean inner product $\\langle \\nabla J(m), p \\rangle$.\n\nYou wish to verify the correctness of the computed gradient by a finite-difference directional-derivative check. Define the central finite-difference quantity\n$$\n\\Delta(\\epsilon, p) = \\frac{J(m + \\epsilon p) - J(m - \\epsilon p)}{2 \\epsilon},\n$$\nand compare it to the adjoint-state inner product $G(p) = \\langle \\nabla J(m), p \\rangle$. Under smoothness of $J$, the expected convergence of $\\Delta(\\epsilon, p)$ to $G(p)$ as $\\epsilon \\to 0$ is second order.\n\nWhich option correctly formulates the check and specifies choices of $\\epsilon$ and $p$ that will robustly exhibit the expected second-order convergence in a realistic computational geophysics setting?\n\nA. Use the central finite-difference $\\Delta(\\epsilon, p)$ as defined above and compare it to $G(p) = \\langle \\nabla J(m), p \\rangle$. Choose $p$ as a random vector with unit norm in parameter space, i.e., $\\|p\\|_2 = 1$, with entries scaled consistently with the units of $m$. Sweep $\\epsilon$ over a geometric sequence, e.g., $\\epsilon \\in \\{10^{-1}, 10^{-2}, \\dots, 10^{-8}\\}$, while holding the forward and adjoint PDE solver tolerances fixed and sufficiently tight so that the discretization error is negligible relative to the truncation error $\\mathcal{O}(\\epsilon^2)$ for the range of $\\epsilon$. Plot the relative error $\\left| \\Delta(\\epsilon, p) - G(p) \\right| / \\left| G(p) \\right|$ versus $\\epsilon$ and verify a slope of approximately $2$ (quadratic decay) until a plateau due to round-off appears. Optionally, select $\\epsilon$ near the balance point $\\epsilon \\approx \\sqrt{\\epsilon_{\\text{mach}}} \\, (1 + \\|m\\|_2) / \\|p\\|_2$ to ensure truncation error dominates round-off, where $\\epsilon_{\\text{mach}}$ is machine precision.\n\nB. Use the central finite-difference $\\Delta(\\epsilon, p)$, but to emphasize sensitivity along the steepest descent direction, set $p = \\nabla J(m)$ and pick $\\epsilon = 10^{-16}$ (double-precision machine epsilon), ensuring the perturbations are as small as possible. Rely on the smallest $\\epsilon$ to reveal the highest-order consistency, and accept any deviations as solver noise.\n\nC. Replace the central finite difference with a one-sided finite difference $[J(m + \\epsilon p) - J(m)]/\\epsilon$ to avoid doubling computational cost. Choose $\\epsilon$ moderately small, e.g., $\\epsilon = 10^{-6}$, and pick $p$ as a sparse perturbation localized around the receivers for enhanced sensitivity. Expect second-order convergence because $J$ is smooth.\n\nD. Use the central finite difference $\\Delta(\\epsilon, p)$ but choose $p$ unnormalized with large amplitude in regions where the wavefield has low energy, and fix $\\epsilon = 10^{-3}$ to be consistent across runs. If the error does not decrease with $\\epsilon$, increase the time-step size in the forward solver to compensate and rely on visual agreement between $\\Delta(\\epsilon, p)$ and $G(p)$ rather than quantitative error scaling.", "solution": "This problem tests the standard procedure for verifying a numerically computed gradient, often called a \"gradient check\" or \"Taylor test.\" The core idea is to compare the analytical gradient (from the adjoint-state method) with a numerical approximation from finite differences.\n\nA Taylor series expansion of the functional $J$ around the model $m$ gives:\n$$ J(m \\pm \\epsilon p) = J(m) \\pm \\epsilon \\langle \\nabla J(m), p \\rangle + \\frac{\\epsilon^2}{2} \\langle p, H(m) p \\rangle \\pm \\mathcal{O}(\\epsilon^3) $$\nwhere $H(m)$ is the Hessian operator. Substituting these into the central finite-difference formula $\\Delta(\\epsilon, p)$ yields:\n$$ \\Delta(\\epsilon, p) = \\frac{J(m + \\epsilon p) - J(m - \\epsilon p)}{2 \\epsilon} = \\langle \\nabla J(m), p \\rangle + \\mathcal{O}(\\epsilon^2) $$\nThis shows that the central difference is a second-order accurate approximation of the directional derivative $G(p) = \\langle \\nabla J(m), p \\rangle$. The error, known as the truncation error, decreases quadratically with $\\epsilon$.\n\nHowever, in finite-precision arithmetic, another error source becomes dominant for very small $\\epsilon$: round-off error. When $\\epsilon$ is tiny, the term $J(m + \\epsilon p) - J(m - \\epsilon p)$ becomes a subtraction of two nearly equal numbers, leading to a loss of significant digits. This round-off error scales as $\\mathcal{O}(\\epsilon_{\\text{mach}}/\\epsilon)$, where $\\epsilon_{\\text{mach}}$ is the machine precision.\n\nA robust verification must demonstrate the expected $\\mathcal{O}(\\epsilon^2)$ convergence. This is achieved by sweeping $\\epsilon$ over several orders of magnitude (e.g., $10^{-1}$ down to $10^{-8}$) and plotting the relative error $|\\Delta(\\epsilon, p) - G(p)| / |G(p)|$ versus $\\epsilon$ on a log-log scale. The plot should show a line with a slope of 2 until the error hits a \"floor\" and then starts increasing as round-off error takes over. To ensure the test is general, the perturbation direction $p$ should be a generic vector, such as a random vector with a fixed norm.\n\n**Analysis of Options:**\n\n*   **A:** This option correctly describes the entire \"gold standard\" procedure: using the second-order central difference, a random perturbation vector $p$, sweeping $\\epsilon$ over a geometric sequence, and checking for the characteristic quadratic error decay on a log-log plot. It also correctly notes the need for tight solver tolerances to minimize PDE discretization error. This is the correct methodology.\n*   **B:** This option is incorrect because choosing $\\epsilon = 10^{-16}$ (machine epsilon) places the calculation squarely in the round-off dominated regime. This will produce numerical noise, not a verification of the gradient.\n*   **C:** This option is incorrect for two reasons. First, the one-sided finite difference is only a first-order accurate method ($\\mathcal{O}(\\epsilon)$ error), so one cannot expect second-order convergence. Second, choosing a sparse, localized perturbation $p$ is not a robust test of the entire gradient, which has support throughout the model.\n*   **D:** This option is incorrect. Using a single fixed $\\epsilon$ does not allow for verification of the convergence *rate*. Furthermore, increasing the solver's time step would *increase* numerical errors, contaminating the test, not improving it. Relying on \"visual agreement\" is not a substitute for quantitative error analysis.", "answer": "$$\\boxed{A}$$", "id": "3574132"}, {"introduction": "Beyond simply calculating a gradient, it is crucial to understand what the gradient, or sensitivity kernel, physically represents. This exercise delves into the structure of the adjoint-state gradient, demonstrating how it is shaped not only by the wave propagation physics but also by the design of the measurement system. By deriving and comparing the adjoint sources for point-receivers versus spatially-averaged receivers, you will gain intuition for how choices in data acquisition directly impact the resolution and character of the resulting model sensitivities [@problem_id:3574116].", "problem": "Consider a two-dimensional synthetic sedimentary basin represented by a domain $\\Omega \\subset \\mathbb{R}^2$ with absorbing boundaries. The acoustic pressure field $u(\\mathbf{x}, t)$ is governed by the constant-density acoustic wave equation\n$$\n\\partial_t^2 u(\\mathbf{x}, t) - m(\\mathbf{x}) \\nabla^2 u(\\mathbf{x}, t) = f(\\mathbf{x}, t),\n$$\nwhere $m(\\mathbf{x}) = c(\\mathbf{x})^2$ is the spatially varying squared wavespeed and $f(\\mathbf{x}, t) = s(t)\\,\\delta(\\mathbf{x} - \\mathbf{x}_s)$ is a point source at $\\mathbf{x}_s$ with source time function $s(t)$ of peak frequency $f_0$. The misfit functional is defined as\n$$\nJ(u) = \\frac{1}{2} \\int_0^T \\| B u(t) - d(t) \\|_{\\mathbf{W}}^2 \\, dt,\n$$\nwhere $B$ is a linear measurement operator mapping the wavefield to observed data at the receiver system, $d(t)$ are observed time series, $\\mathbf{W}$ is a symmetric positive-definite weighting, and $\\|\\mathbf{y}\\|_{\\mathbf{W}}^2 = \\mathbf{y}^\\top \\mathbf{W} \\mathbf{y}$. Two choices of $B$ are considered:\n\n$1.$ Point sampling at $N$ receiver locations $\\{\\mathbf{x}_r\\}_{r=1}^N$:\n$$\n(B_{\\mathrm{pt}} u)_r(t) = u(\\mathbf{x}_r, t), \\quad r = 1, \\dots, N.\n$$\n\n$2.$ Spatial averaging over receiver patches $\\{\\Gamma_r\\}_{r=1}^N$ with nonnegative, normalized weights $w_r(\\mathbf{x})$ supported on $\\Gamma_r$:\n$$\n(B_{\\mathrm{avg}} u)_r(t) = \\int_{\\Gamma_r} w_r(\\mathbf{x})\\, u(\\mathbf{x}, t)\\, d\\mathbf{x}, \\quad \\int_{\\Gamma_r} w_r(\\mathbf{x})\\, d\\mathbf{x} = 1.\n$$\n\nAssume $\\mathbf{W}$ is diagonal with entries $W_r > 0$, the time window $[0, T]$ is sufficiently long to capture the waveforms, and boundary terms vanish upon integration by parts. The basin has slower wavespeed $c_{\\mathrm{basin}}$ than the surrounding basement $c_{\\mathrm{basement}}$, with $c_{\\mathrm{basin}} = 2\\,\\mathrm{km/s}$, $c_{\\mathrm{basement}} = 4\\,\\mathrm{km/s}$, and $f_0 = 5\\,\\mathrm{Hz}$. Receiver patch widths satisfy $a \\approx 0.2\\,\\mathrm{km}$, while the characteristic wavelength inside the basin is $\\lambda_{\\mathrm{basin}} \\approx c_{\\mathrm{basin}}/f_0$.\n\nTasks:\n$1.$ Starting from the stated governing equation and the misfit definition, derive the adjoint-state equation and the gradient $\\nabla J(\\mathbf{x})$ with respect to $m(\\mathbf{x})$ for a generic linear operator $B$. Then, specialize your result to each measurement operator $B_{\\mathrm{pt}}$ and $B_{\\mathrm{avg}}$ by writing the corresponding adjoint sources in space-time.\n\n$2.$ Based on the derived forms, analyze how the measurement operator choice affects the spatial resolution of $\\nabla J(\\mathbf{x})$ inside the basin. Use the given numerical scales to argue the relative ability to resolve features with characteristic size smaller than or comparable to $a$ and $\\lambda_{\\mathrm{basin}}$.\n\nWhich of the following statements is most consistent with the derivation and resolution analysis?\n\nA. For point sampling, the adjoint source is a sum of spatial Dirac distributions at the receiver locations weighted by residuals, i.e., $r_{\\mathrm{pt}}(\\mathbf{x}, t) = \\sum_{r=1}^N W_r\\,[u(\\mathbf{x}_r, t) - d_r(t)]\\,\\delta(\\mathbf{x} - \\mathbf{x}_r)$. For spatial averaging over patches with weights $w_r(\\mathbf{x})$, the adjoint source is $r_{\\mathrm{avg}}(\\mathbf{x}, t) = \\sum_{r=1}^N W_r\\left[\\int_{\\Gamma_r} w_r(\\mathbf{y})\\,u(\\mathbf{y}, t)\\, d\\mathbf{y} - d_r(t)\\right] w_r(\\mathbf{x})$. In both cases, the gradient is $\\nabla J(\\mathbf{x}) = -\\int_0^T \\nabla u(\\mathbf{x}, t)\\cdot \\nabla \\lambda(\\mathbf{x}, t)\\, dt$. Spatial averaging broadens the adjoint field and therefore low-pass filters the gradient at approximately the averaging window scale $a$, so features smaller than $a$ are less resolvable compared to point sampling inside the basin with $\\lambda_{\\mathrm{basin}} \\approx 0.4\\,\\mathrm{km}$ and $a \\approx 0.2\\,\\mathrm{km}$.\n\nB. The measurement operator does not affect the adjoint source, which is always $r(\\mathbf{x}, t) = \\sum_{r=1}^N W_r[u(\\mathbf{x}, t) - d_r(t)]$ uniformly distributed over $\\Omega$, so the resolution of $\\nabla J(\\mathbf{x})$ is identical for point sampling and spatial averaging.\n\nC. Spatial averaging introduces an additional Laplacian into the adjoint source, $r_{\\mathrm{avg}}(\\mathbf{x}, t) = \\nabla^2\\left(\\sum_{r=1}^N W_r[(B_{\\mathrm{avg}} u)_r(t) - d_r(t)]\\right)$, which increases high-frequency content in $\\nabla J(\\mathbf{x})$ and enhances resolution within the basin.\n\nD. For point sampling, the gradient reduces to $\\nabla J(\\mathbf{x}) = -\\int_0^T u(\\mathbf{x}, t)\\,\\lambda(\\mathbf{x}, t)\\, dt$, whereas for spatial averaging it remains $\\nabla J(\\mathbf{x}) = -\\int_0^T \\nabla u(\\mathbf{x}, t)\\cdot \\nabla \\lambda(\\mathbf{x}, t)\\, dt$, implying that spatial averaging yields sharper localization of sensitivity.\n\nE. Both measurement operators produce adjoint sources that are nonzero only at $t = T$ due to terminal conditions, so any resolution differences depend solely on the source bandwidth and not on the spatial support of $B$.", "solution": "To solve this problem, we use the Lagrangian method to derive the adjoint equation and the gradient. The adjoint source term, which drives the adjoint wavefield $\\lambda$, is determined by the adjoint of the measurement operator, $B^\\dagger$.\n\n1.  **Derivation of the Adjoint Source**: The variation of the misfit functional with respect to the wavefield $u$ is given by $\\delta_u J = \\int_0^T \\int_\\Omega [B^\\dagger (\\mathbf{W}(Bu - d))] \\delta u \\, d\\mathbf{x} dt$. The term $r_\\lambda = B^\\dagger (\\mathbf{W}(Bu - d))$ becomes the source for the adjoint wave equation. We need to find the adjoint operator $B^\\dagger$ for each case.\n\n    *   **Case 1: Point Sampling ($B_{\\mathrm{pt}}$)**. The measurement is $(B_{\\mathrm{pt}} u)_r(t) = u(\\mathbf{x}_r, t)$. The adjoint operator must satisfy $\\langle \\mathbf{y}, B_{\\mathrm{pt}} u \\rangle = \\langle B_{\\mathrm{pt}}^\\dagger \\mathbf{y}, u \\rangle$. This leads to $B_{\\mathrm{pt}}^\\dagger$ being an operator that injects the data-space vector $\\mathbf{y}$ as a series of weighted spatial Dirac delta functions at the receiver locations. The adjoint source is therefore $r_{\\mathrm{pt}}(\\mathbf{x}, t) = \\sum_{r=1}^N W_r [u(\\mathbf{x}_r, t) - d_r(t)] \\delta(\\mathbf{x} - \\mathbf{x}_r)$.\n\n    *   **Case 2: Spatial Averaging ($B_{\\mathrm{avg}}$)**. The measurement is $(B_{\\mathrm{avg}} u)_r(t) = \\int_{\\Gamma_r} w_r(\\mathbf{x}) u(\\mathbf{x}, t) d\\mathbf{x}$. By finding the operator that satisfies the adjoint property, we find that $B_{\\mathrm{avg}}^\\dagger$ takes a data-space residual vector and distributes it spatially according to the same weighting function $w_r(\\mathbf{x})$. The adjoint source is $r_{\\mathrm{avg}}(\\mathbf{x}, t) = \\sum_{r=1}^N W_r \\left[ \\int_{\\Gamma_r} w_r(\\mathbf{y}) u(\\mathbf{y}, t) d\\mathbf{y} - d_r(t) \\right] w_r(\\mathbf{x})$.\n\n2.  **Derivation of the Gradient**: For the given wave equation $\\partial_t^2 u - m \\nabla^2 u = f$, a perturbation in $m$ gives $\\delta m \\nabla^2 u$. The gradient is the kernel of the integral $\\int_0^T \\int_\\Omega \\lambda (\\delta m \\nabla^2 u) d\\mathbf{x} dt$. Integrating by parts (assuming the physically standard form $\\nabla \\cdot (m \\nabla u)$) yields the gradient expression $\\nabla J(\\mathbf{x}) = -\\int_0^T \\nabla u(\\mathbf{x}, t) \\cdot \\nabla \\lambda(\\mathbf{x}, t) dt$. This form is standard and depends on the parameterization of the PDE, not the measurement operator $B$.\n\n3.  **Resolution Analysis**: The spatial resolution of the gradient $\\nabla J(\\mathbf{x})$ is determined by the spatial frequencies present in both $u$ and $\\lambda$. The adjoint field $\\lambda$ is a wavefield generated by the adjoint source.\n    *   For point sampling, the source is a series of Dirac deltas, which are spatially \"sharp\" and contain all spatial frequencies. The resulting adjoint field $\\lambda$ is limited in resolution only by the wave physics (diffraction) and the temporal bandwidth of the data.\n    *   For spatial averaging, the source is \"smeared out\" over patches of width $a$. This is equivalent to convolving a point source with the window function $w_r(\\mathbf{x})$. In the spatial frequency domain, this convolution becomes a multiplication, which acts as a low-pass filter, suppressing spatial frequencies higher than $\\sim 1/a$. The resulting adjoint field $\\lambda$ and gradient $\\nabla J$ will therefore be smoother, with reduced ability to resolve features smaller than the averaging scale $a$.\n    \n    Numerically, $\\lambda_{\\mathrm{basin}} = (2\\,\\mathrm{km/s}) / (5\\,\\mathrm{Hz}) = 0.4\\,\\mathrm{km}$, and the patch width is $a = 0.2\\,\\mathrm{km}$. Since the spatial averaging introduces a smoothing at scale $a$, it will degrade resolution compared to point sampling.\n\n**Analysis of Options:**\n\n*   **A:** This option correctly states the derived adjoint sources for both cases and the correct gradient expression. Its resolution analysis is also correct: spatial averaging acts as a low-pass filter with scale $a$, reducing the ability to resolve features smaller than $a$.\n*   **B:** Incorrect. The measurement operator fundamentally determines the adjoint source.\n*   **C:** Incorrect. The derived adjoint source for spatial averaging involves the weighting function $w_r(\\mathbf{x})$, not a Laplacian. A Laplacian would enhance high frequencies, which is the opposite effect of averaging.\n*   **D:** Incorrect. The gradient expression depends on the PDE parameterization, not the measurement operator. The form $-\\int u \\lambda dt$ is the gradient for a different PDE parameterization.\n*   **E:** Incorrect. The adjoint source is active whenever the data residual is non-zero during the time interval $[0, T]$. It is the *adjoint field* $\\lambda$ that has zero terminal conditions at $t=T$, not its source.", "answer": "$$\\boxed{A}$$", "id": "3574116"}, {"introduction": "The power of the adjoint-state method for large-scale time-dependent problems comes with a significant practical challenge: the need to access the entire forward state history during the reverse-time adjoint computation, which often exceeds available memory. This problem addresses this memory bottleneck by exploring optimal checkpointing, a cornerstone algorithm in computational science. You will analyze the \"Revolve\" strategy, which provides a provably optimal trade-off between storage and recomputation, making large-scale inversions feasible in practice [@problem_id:3574137].", "problem": "Consider a $3$-dimensional acoustic wave simulation advanced by explicit time stepping over $T$ steps, producing forward states $\\{\\mathbf{u}(t_k)\\}_{k=0}^{T}$ at times $t_k = k\\,\\Delta t$ with uniform cost per time step. The gradient of a least-squares data misfit computed by the adjoint-state method requires, during a reverse-time sweep, the forward state $\\mathbf{u}(t_k)$ to be available at each step $k$ to form space-time correlations with the adjoint field $\\mathbf{\\lambda}(t_k)$. When storing all $T$ states is infeasible, we use a limited-memory checkpointing strategy that stores at most $M$ checkpoints (snapshots of $\\mathbf{u}(t_k)$), replays forward segments from stored checkpoints when needed, and aims to minimize the total number of recomputed forward time steps.\n\nAssume the following scientifically realistic and standard conditions:\n- Each forward time step has identical computational cost and deterministic state evolution.\n- The reverse-time adjoint sweep requires visiting each time step in exact reverse order, and for each $k$ the corresponding forward state $\\mathbf{u}(t_k)$ must be materialized either from memory or by re-executing forward steps from the most recent available checkpoint.\n- The schedule is offline and optimal in the sense of the Revolve algorithm (i.e., it preplans store/restore/recompute operations given $T$ and $M$ to minimize recomputation).\n\nUnder these assumptions, design the offline checkpointing strategy conceptually (i.e., describe how the time interval is recursively partitioned and how checkpoints are stored and restored to enable the adjoint sweep), and select the expression that correctly gives the minimal number of recomputed forward time steps required to complete the adjoint sweep. Your answer must account for the case $T \\le M$.\n\nWhich option is correct?\n\nA. Use a recursively partitioned binomial checkpointing schedule: if $T \\le M$, no recomputation is needed; if $T > M$, let $d^{\\ast}$ be the smallest integer such that $\\binom{M + d^{\\ast}}{M} \\ge T$, and the minimal number of recomputed forward time steps is $R(T,M) = \\binom{M + d^{\\ast}}{M + 1}$.\n\nB. Use uniform periodic checkpoints every $\\lfloor T/M \\rfloor$ steps; the minimal number of recomputed forward time steps is $R(T,M) = T - M$.\n\nC. Store only the last $M$ forward states and replay earlier ones from the initial condition; the minimal number of recomputed forward time steps is $R(T,M) = \\dfrac{T\\,(T - 1)}{2\\,M}$.\n\nD. Use uniform periodic checkpoints every $\\lceil M/2 \\rceil$ steps; the minimal number of recomputed forward time steps is $R(T,M) = \\left\\lfloor \\dfrac{(T - M)^2}{2\\,M} \\right\\rfloor$.", "solution": "This problem describes the optimal offline checkpointing strategy for time-reversible computations, a classic problem in algorithmic differentiation solved by the \"Revolve\" algorithm developed by Griewank and Walther. The goal is to minimize recomputation cost given a fixed memory budget for storing checkpoints.\n\n**Conceptual Strategy**\n\nThe optimal strategy is based on a recursive partitioning of the time interval. Let $T_{max}(M, d)$ be the maximum number of time steps that can be processed with $M$ checkpoints and a schedule \"depth\" of $d$ (representing the maximum number of times any segment is recomputed).\n\n*   **Base Case:** A schedule of depth $d=0$ means no recomputation. We can simply store each state, so $T_{max}(M, 0) = M+1$ (if we can store $M$ states plus the initial condition).\n\n*   **Recursive Step:** To handle a longer time interval, we perform a forward sweep, storing one checkpoint at an intermediate time step $t_i$. This splits the problem into two parts: $[0, t_i]$ and $[t_i, T]$. The segment $[t_i, T]$ can be processed using $M-1$ remaining checkpoints. The segment $[0, t_i]$ is handled by discarding all stored information and restarting the computation from the initial state at $t_0$. The optimal choice of $t_i$ and subsequent recursive partitioning leads to a solution described by binomial coefficients.\n\n**Mathematical Formulation**\n\nThe maximum number of steps $T$ that can be reversed with $M$ available checkpoint slots and a schedule of depth $d$ is given by the binomial coefficient:\n$$ T_{\\text{max}}(M, d) = \\binom{M+d}{M} $$\nFor a given problem with $T$ steps and $M$ memory slots, we must find the smallest integer depth, $d^{\\ast}$, required for the schedule to work. This $d^{\\ast}$ is the smallest integer satisfying:\n$$ \\binom{M+d^{\\ast}}{M} \\ge T $$\nThe total number of forward time steps that must be computed (including the initial full sweep and all recomputations) for such an optimal schedule is:\n$$ W(T, M) = \\binom{M+d^{\\ast}+1}{M+1} $$\nThe number of *recomputed* steps is the total work minus the steps in the initial forward pass ($T$). The formula provided in the options is a known result for the recomputation cost of a full binomial schedule:\n$$ R(T, M) = \\text{Work} - \\text{Initial Pass} = \\binom{M+d^{\\ast}+1}{M+1} - \\binom{M+d^{\\ast}}{M} $$\nUsing the identity $\\binom{n+1}{k+1} - \\binom{n}{k} = \\binom{n}{k+1}$, we can simplify this to:\n$$ R(T, M) = \\binom{M+d^{\\ast}}{M+1} $$\nIf $T \\le M$, we can store every state, so no recomputation is necessary.\n\n**Analysis of Options:**\n\n*   **A:** This option correctly describes the optimal binomial checkpointing schedule. It handles the $T \\le M$ case correctly (no recomputation). For $T > M$, it correctly identifies the method for finding the required schedule depth $d^{\\ast}$ and provides the correct formula for the number of recomputed steps.\n*   **B:** A uniform checkpointing strategy is simple but known to be suboptimal. The recomputation cost is significantly higher than the binomial schedule. The formula $T-M$ is also incorrect for this strategy.\n*   **C:** Storing only the last $M$ states is a very poor strategy, leading to massive recomputation from the start for almost all steps. The cost formula is incorrect.\n*   **D:** The strategy of using a period based only on $M$ is ill-conceived, and like uniform checkpointing, it is suboptimal. The cost formula is a variant of the approximate cost for uniform checkpointing, but the strategy itself is flawed.", "answer": "$$\\boxed{A}$$", "id": "3574137"}]}