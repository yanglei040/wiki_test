{"hands_on_practices": [{"introduction": "Before applying sophisticated kriging models, we must first characterize the spatial structure inherent in our data. The empirical semivariogram is the primary tool for this task, quantifying how data similarity changes with distance. This foundational exercise ([@problem_id:3599957]) guides you through the essential steps of calculating semivariogram values from a small dataset, providing a concrete understanding of how we translate scattered measurements into a quantitative model of spatial continuity.", "problem": "In a two-dimensional isotropic geostatistical analysis of a dimensionless seismic amplitude attribute, four observations are available at spatial locations $(0,0)$, $(1,0)$, $(0,1)$, and $(1,1)$ with attribute values $2.0$, $3.0$, $2.5$, and $3.5$, respectively. Assume second-order stationarity and compute the omnidirectional empirical semivariogram estimates $\\hat{\\gamma}(h)$ at lag distances $|h|=1$ and $|h|=\\sqrt{2}$ using the classical unbiased empirical estimator. Construct lag bins centered at $|h|$ with a radial tolerance of $0.01$ in the Euclidean norm, and include each unordered pair only once if its separation distance falls within the bin. Report, in this order, the two semivariogram estimates and the numbers of pairs contributing to each estimate: $\\hat{\\gamma}(1)$, $\\hat{\\gamma}(\\sqrt{2})$, $N(1)$, $N(\\sqrt{2})$. No rounding is required. Express semivariogram values in squared attribute units, but do not attach units in the final numerical answer.", "solution": "The problem is first validated to ensure it is scientifically grounded, well-posed, objective, and complete. The problem statement provides a set of four spatial data points with associated attribute values and asks for the computation of omnidirectional empirical semivariogram estimates for two specific lag distances. All necessary information is provided: locations $\\mathbf{x}_i$, values $Z(\\mathbf{x}_i)$, the estimator to be used (classical unbiased), and the parameters for lag binning (lag distances and tolerance). The assumptions of second-order stationarity and isotropy are explicitly stated, which are standard in this context. The problem is a well-defined exercise in elementary geostatistics and is therefore deemed valid.\n\nThe classical unbiased empirical estimator for the semivariogram, $\\gamma(h)$, for a given lag distance $|h|$ is given by the formula:\n$$ \\hat{\\gamma}(|h|) = \\frac{1}{2N(|h|)} \\sum_{(\\mathbf{x}_i, \\mathbf{x}_j) \\in S(|h|)} [Z(\\mathbf{x}_i) - Z(\\mathbf{x}_j)]^2 $$\nwhere $S(|h|)$ is the set of all unique unordered pairs of data points $(\\mathbf{x}_i, \\mathbf{x}_j)$ whose separation distance, $d_{ij} = ||\\mathbf{x}_i - \\mathbf{x}_j||$, falls within the specified lag bin centered at $|h|$. $N(|h|)$ is the number of such pairs, i.e., the cardinality of the set $S(|h|)$.\n\nThe given data points are:\n$P_1: \\mathbf{x}_1 = (0,0)$, with value $Z(\\mathbf{x}_1) = 2.0$\n$P_2: \\mathbf{x}_2 = (1,0)$, with value $Z(\\mathbf{x}_2) = 3.0$\n$P_3: \\mathbf{x}_3 = (0,1)$, with value $Z(\\mathbf{x}_3) = 2.5$\n$P_4: \\mathbf{x}_4 = (1,1)$, with value $Z(\\mathbf{x}_4) = 3.5$\n\nThere are $\\binom{4}{2} = 6$ unique unordered pairs of points. We must calculate the Euclidean distance $d_{ij} = \\sqrt{(x_i-x_j)^2 + (y_i-y_j)^2}$ and the squared difference of attribute values $[Z(\\mathbf{x}_i) - Z(\\mathbf{x}_j)]^2$ for each pair.\n\n1.  Pair $(P_1, P_2)$:\n    $d_{12} = \\sqrt{(0-1)^2 + (0-0)^2} = \\sqrt{1} = 1$\n    $[Z(\\mathbf{x}_1) - Z(\\mathbf{x}_2)]^2 = (2.0 - 3.0)^2 = (-1.0)^2 = 1.0$\n\n2.  Pair $(P_1, P_3)$:\n    $d_{13} = \\sqrt{(0-0)^2 + (0-1)^2} = \\sqrt{1} = 1$\n    $[Z(\\mathbf{x}_1) - Z(\\mathbf{x}_3)]^2 = (2.0 - 2.5)^2 = (-0.5)^2 = 0.25$\n\n3.  Pair $(P_1, P_4)$:\n    $d_{14} = \\sqrt{(0-1)^2 + (0-1)^2} = \\sqrt{1+1} = \\sqrt{2}$\n    $[Z(\\mathbf{x}_1) - Z(\\mathbf{x}_4)]^2 = (2.0 - 3.5)^2 = (-1.5)^2 = 2.25$\n\n4.  Pair $(P_2, P_3)$:\n    $d_{23} = \\sqrt{(1-0)^2 + (0-1)^2} = \\sqrt{1+1} = \\sqrt{2}$\n    $[Z(\\mathbf{x}_2) - Z(\\mathbf{x}_3)]^2 = (3.0 - 2.5)^2 = (0.5)^2 = 0.25$\n\n5.  Pair $(P_2, P_4)$:\n    $d_{24} = \\sqrt{(1-1)^2 + (0-1)^2} = \\sqrt{1} = 1$\n    $[Z(\\mathbf{x}_2) - Z(\\mathbf{x}_4)]^2 = (3.0 - 3.5)^2 = (-0.5)^2 = 0.25$\n\n6.  Pair $(P_3, P_4)$:\n    $d_{34} = \\sqrt{(0-1)^2 + (1-1)^2} = \\sqrt{1} = 1$\n    $[Z(\\mathbf{x}_3) - Z(\\mathbf{x}_4)]^2 = (2.5 - 3.5)^2 = (-1.0)^2 = 1.0$\n\nNext, we group these pairs into the specified lag bins.\n\nFor the lag distance $|h|=1$:\nThe lag bin is defined by a center of $1$ and a radial tolerance of $0.01$, resulting in the interval $[1-0.01, 1+0.01] = [0.99, 1.01]$.\nThe pairs whose separation distance falls into this bin are those with $d_{ij}=1$. These are the pairs $(P_1, P_2)$, $(P_1, P_3)$, $(P_2, P_4)$, and $(P_3, P_4)$.\nThe number of pairs is $N(1) = 4$.\nThe sum of the squared differences for these pairs is:\n$$ \\sum_{S(1)} [Z(\\mathbf{x}_i) - Z(\\mathbf{x}_j)]^2 = 1.0 + 0.25 + 0.25 + 1.0 = 2.5 $$\nThe semivariogram estimate is:\n$$ \\hat{\\gamma}(1) = \\frac{1}{2N(1)} \\times 2.5 = \\frac{1}{2 \\times 4} \\times 2.5 = \\frac{2.5}{8} = \\frac{5/2}{8} = \\frac{5}{16} = 0.3125 $$\n\nFor the lag distance $|h|=\\sqrt{2}$:\nThe lag bin is defined by a center of $\\sqrt{2}$ and a radial tolerance of $0.01$. The interval is $[\\sqrt{2}-0.01, \\sqrt{2}+0.01]$. Since $\\sqrt{2} \\approx 1.4142$, this is approximately $[1.4042, 1.4242]$.\nThe pairs whose separation distance falls into this bin are those with $d_{ij}=\\sqrt{2}$. These are the pairs $(P_1, P_4)$ and $(P_2, P_3)$.\nThe number of pairs is $N(\\sqrt{2}) = 2$.\nThe sum of the squared differences for these pairs is:\n$$ \\sum_{S(\\sqrt{2})} [Z(\\mathbf{x}_i) - Z(\\mathbf{x}_j)]^2 = 2.25 + 0.25 = 2.5 $$\nThe semivariogram estimate is:\n$$ \\hat{\\gamma}(\\sqrt{2}) = \\frac{1}{2N(\\sqrt{2})} \\times 2.5 = \\frac{1}{2 \\times 2} \\times 2.5 = \\frac{2.5}{4} = \\frac{5/2}{4} = \\frac{5}{8} = 0.625 $$\n\nThe required outputs are $\\hat{\\gamma}(1)$, $\\hat{\\gamma}(\\sqrt{2})$, $N(1)$, and $N(\\sqrt{2})$.\n$\\hat{\\gamma}(1) = 0.3125$\n$\\hat{\\gamma}(\\sqrt{2}) = 0.625$\n$N(1) = 4$\n$N(\\sqrt{2}) = 2$\nThese values are exact and do not require rounding.\nThe final answer must be reported in the specified order.", "answer": "$$\n\\boxed{\\begin{pmatrix} 0.3125 & 0.625 & 4 & 2 \\end{pmatrix}}\n$$", "id": "3599957"}, {"introduction": "Geostatistical practice often involves working with data on a global scale, where the Earth's curvature cannot be ignored. This comprehensive exercise ([@problem_id:3599939]) challenges you to implement an ordinary kriging system on a sphere, directly comparing a geometrically sound spherical covariance model with a simpler chordal-distance approximation. By tackling this problem, you will gain practical experience in building a complete kriging workflow and appreciate the critical impact of choosing an appropriate distance metric for non-Euclidean spaces.", "problem": "You are tasked with implementing non-Euclidean ordinary kriging on a sphere for satellite gravity residuals using two competing distance metrics: great-circle geodesic distance and three-dimensional chordal distance. The objective is to compare a spherical Matérn family covariance that respects spherical geometry to a chordal-distance approximation that treats points as embedded in three-dimensional Euclidean space. You must evaluate and quantify the impact of these choices on prediction accuracy near the poles.\n\nYou should work under the following scientifically grounded assumptions and definitions, which serve as the fundamental base of the derivation:\n\n- Satellite gravity residuals are modeled as a realization of a second-order stationary Gaussian process on the sphere. Ordinary kriging seeks the Best Linear Unbiased Estimator (BLUE) using the model covariance, constrained to unbiasedness through an unknown constant mean.\n- The covariance family is the Matérn class parameterized by a smoothness parameter and a scale (range) parameter. The spherical Matérn model uses great-circle geodesic distance on the sphere, whereas the chordal-distance approximation uses the Euclidean chord length between points embedded on a sphere of fixed radius.\n- Great-circle distance is the geodesic distance on the sphere; chordal distance is the straight-line distance through the Earth between two surface points.\n- The Earth is modeled as a perfect sphere with radius $R = 6371\\,\\mathrm{km}$.\n\nAngles and distances:\n\n- All input angles (latitudes and longitudes) are provided in degrees. Internally, you must convert to radians for all trigonometric operations.\n- All distances must be computed and used in kilometers.\n\nYou are given a deterministic ground-truth residual field (unit: milliGalileo, mGal) over the sphere to avoid any randomness. Let latitude be $\\lambda$ (radians, positive north), longitude be $\\phi$ (radians, positive east), and colatitude be $\\theta$ with $\\cos(\\theta) = \\sin(\\lambda)$. Define the true residual field as\n$$\nf(\\lambda,\\phi) = 0.8 \\cdot \\frac{1}{2}\\left(3\\sin^2\\lambda - 1\\right) + 0.5 \\cdot \\cos^2\\lambda \\cdot \\sin\\left(2\\phi\\right) + 0.2 \\cdot \\sin\\lambda,\n$$\nevaluated in $\\mathrm{mGal}$. There is no measurement noise; the nugget effect is used purely for numerical stabilization.\n\nKriging model details:\n\n- Use ordinary kriging with an unknown constant mean enforced by the unbiasedness constraint.\n- Use the Matérn covariance with smoothness $\\nu = 1.5$, marginal variance (sill) $\\sigma^2 = 1.0$ in $\\mathrm{mGal}^2$, and three different range parameters $\\rho \\in \\{500, 2000, 6000\\}$ in $\\mathrm{km}$ as a test suite of parameter values. Use a small nugget $\\tau^2 = 10^{-6}$ in $\\mathrm{mGal}^2$ added to the diagonal of the covariance matrix for numerical stability. Do not include the nugget in the target variance term of the kriging variance.\n- Implement two versions of the covariance function:\n  1. Spherical Matérn: use geodesic great-circle distance $d_g$ on the sphere of radius $R$.\n  2. Chordal-distance Matérn: use chord length $d_c$ between the three-dimensional positions on a sphere of radius $R$.\n\nData:\n\n- Training locations (degrees): latitudes $\\{-60,-30,0,30,60\\}$ and longitudes $\\{0,72,144,216,288\\}$. Use all pairs of the five latitudes and five longitudes, for a total of $25$ training points. Do not include the poles in the training set.\n- Target locations (degrees):\n  - Polar set: latitudes $\\{85,-85\\}$ with longitudes $\\{0,90,180,270\\}$, for a total of $8$ targets.\n  - Equatorial set: latitude $\\{0\\}$ with longitudes $\\{45,135,225,315\\}$, for a total of $4$ targets.\n- Evaluate $f(\\lambda,\\phi)$ at all training and target locations to obtain the noiseless training values and the ground-truth targets.\n\nComputational tasks:\n\n- Convert all angles from degrees to radians internally. Ensure all distances are computed in kilometers on a sphere of radius $R = 6371\\,\\mathrm{km}$.\n- For each range parameter $\\rho \\in \\{500, 2000, 6000\\}$:\n  - Build the training covariance matrix using either $d_g$ or $d_c$, add the nugget $\\tau^2$ on the diagonal, and solve the ordinary kriging system for each target.\n  - Compute predictions at the polar and equatorial targets for both distance metrics.\n  - Compute the root-mean-square error (RMSE) at polar and equatorial targets separately, comparing predictions to the ground-truth values $f(\\lambda,\\phi)$.\n  - Report, for each $\\rho$, the following quantities:\n    1. RMSE at polar targets using spherical Matérn with great-circle distance.\n    2. RMSE at polar targets using chordal-distance Matérn.\n    3. RMSE at equatorial targets using spherical Matérn with great-circle distance.\n    4. RMSE at equatorial targets using chordal-distance Matérn.\n    5. The difference in polar RMSE defined as $\\mathrm{RMSE}_{\\text{chordal}} - \\mathrm{RMSE}_{\\text{spherical}}$ at polar targets.\n    6. A boolean indicating whether spherical Matérn has lower polar RMSE than chordal-distance Matérn at polar targets.\n\nNumerical and physical unit requirements:\n\n- Distances must be in $\\mathrm{km}$.\n- Angles must be converted to radians for trigonometric computations.\n- Residuals are in $\\mathrm{mGal}$, but the program’s final printed outputs must be unitless numeric values; do not include units in the output.\n\nTest suite and coverage:\n\n- The three range parameters $\\rho \\in \\{500, 2000, 6000\\}$ act as the test suite and are designed to test short-range behavior (where metric differences are negligible), moderate-range behavior, and long-range behavior (where metric differences are most pronounced).\n- The polar versus equatorial evaluation sets stress the geometry near the poles and along the equator.\n\nFinal output format:\n\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain, in order, for each $\\rho$ in ascending order $\\{500,2000,6000\\}$, the six outputs specified above. This yields a flat list of $18$ entries:\n  - For $\\rho = 500$: items $1$ through $6$.\n  - For $\\rho = 2000$: items $7$ through $12$.\n  - For $\\rho = 6000$: items $13$ through $18$.\n- Example of the required shape (values are placeholders): \"[x1,x2,x3,x4,x5,b1,x7,x8,x9,x10,x11,b2,x13,x14,x15,x16,x17,b3]\".", "solution": "The problem requires the implementation and comparison of two ordinary kriging models for a geophysical field on a sphere. The core distinction between the models lies in the distance metric used within the covariance function: one employs the true geodesic (great-circle) distance on the sphere, while the other uses a Euclidean chordal distance approximation. The goal is to quantify the difference in prediction accuracy, particularly near the poles where geometric distortions are most significant.\n\nThe solution is structured as follows: First, we define the theoretical underpinnings of ordinary kriging and the specific models used. Second, we detail the implementation strategy, including coordinate systems, distance calculations, and the kriging algorithm. Finally, we describe the evaluation process based on the Root Mean Square Error (RMSE).\n\n**1. Theoretical Framework**\n\n**Ordinary Kriging Model**\n\nWe model the satellite gravity residuals as a single realization of a second-order stationary Gaussian random field, $Z(\\mathbf{x})$, where $\\mathbf{x}$ is a location on the sphere. The field has an unknown but constant mean, $E[Z(\\mathbf{x})] = m$, and a known covariance function, $\\text{Cov}(Z(\\mathbf{x}_i), Z(\\mathbf{x}_j)) = C(d(\\mathbf{x}_i, \\mathbf{x}_j))$, where $d$ is a distance metric.\n\nOrdinary kriging provides the Best Linear Unbiased Estimator (BLUE) for the value $Z(\\mathbf{x}_0)$ at a target location $\\mathbf{x}_0$. The estimator, $\\hat{Z}(\\mathbf{x}_0)$, is a weighted linear combination of the $n$ observed values $z_i = Z(\\mathbf{x}_i)$ at training locations $\\mathbf{x}_i$:\n$$\n\\hat{Z}(\\mathbf{x}_0) = \\sum_{i=1}^{n} \\lambda_i z_i\n$$\nThe unbiasedness constraint, $E[\\hat{Z}(\\mathbf{x}_0)] = E[Z(\\mathbf{x}_0)]$, requires that the weights sum to unity:\n$$\n\\sum_{i=1}^{n} \\lambda_i = 1\n$$\nMinimizing the estimation variance, $\\text{Var}(\\hat{Z}(\\mathbf{x}_0) - Z(\\mathbf{x}_0))$, subject to this constraint, leads to the following system of linear equations, which can be solved for the weights $\\boldsymbol{\\lambda} = [\\lambda_1, \\dots, \\lambda_n]^T$ and a Lagrange multiplier $\\mu$:\n$$\n\\begin{pmatrix}\n\\mathbf{K} + \\tau^2\\mathbf{I} & \\mathbf{1} \\\\\n\\mathbf{1}^T & 0\n\\end{pmatrix}\n\\begin{pmatrix}\n\\boldsymbol{\\lambda} \\\\\n\\mu\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\mathbf{k}_0 \\\\\n1\n\\end{pmatrix}\n$$\nHere, $\\mathbf{K}$ is the $n \\times n$ covariance matrix between training points, with entries $K_{ij} = C(d(\\mathbf{x}_i, \\mathbf{x}_j))$. The term $\\tau^2\\mathbf{I}$ represents the nugget effect, added to the diagonal of $\\mathbf{K}$ for numerical stability, where $\\tau^2 = 10^{-6} \\, \\mathrm{mGal}^2$ and $\\mathbf{I}$ is the identity matrix. The vector $\\mathbf{k}_0$ contains the covariances between the target point $\\mathbf{x}_0$ and each training point, with entries $(\\mathbf{k}_0)_i = C(d(\\mathbf{x}_0, \\mathbf{x}_i))$. The vector $\\mathbf{1}$ is an $n$-dimensional column vector of ones.\n\n**Covariance Function**\n\nThe problem specifies the Matérn covariance family with smoothness parameter $\\nu = 1.5$. The function is given by:\n$$\nC(d; \\sigma^2, \\rho) = \\sigma^2 \\left(1 + \\frac{\\sqrt{3}d}{\\rho}\\right) \\exp\\left(-\\frac{\\sqrt{3}d}{\\rho}\\right)\n$$\nwhere $d$ is the distance, $\\sigma^2=1.0 \\, \\mathrm{mGal}^2$ is the marginal variance (sill), and $\\rho$ is the spatial range parameter, taking values in $\\{500, 2000, 6000\\} \\, \\mathrm{km}$.\n\n**Distance Metrics on the Sphere**\n\nThe analysis hinges on comparing two distance metrics for $d$:\n1.  **Geodesic (Great-Circle) Distance ($d_g$)**: This is the shortest distance between two points on the surface of the sphere. For two points with spherical coordinates $(\\lambda_1, \\phi_1)$ and $(\\lambda_2, \\phi_2)$ on a sphere of radius $R = 6371 \\, \\mathrm{km}$, the distance is $d_g = R \\cdot \\Delta\\sigma$, where $\\Delta\\sigma$ is the central angle between the points. A numerically stable way to compute this is via the dot product of their unit Cartesian vectors $\\mathbf{v}_1$ and $\\mathbf{v}_2$: $\\Delta\\sigma = \\arccos(\\mathbf{v}_1 \\cdot \\mathbf{v}_2)$. This is the geometrically correct distance for a process on a sphere.\n\n2.  **Chordal Distance ($d_c$)**: This is the straight-line Euclidean distance through the sphere's interior. Given Cartesian coordinates $\\mathbf{p}_1$ and $\\mathbf{p}_2$ corresponding to the two points on the sphere's surface, the distance is $d_c = \\|\\mathbf{p}_1 - \\mathbf{p}_2\\|_2$. This metric approximates $d_g$ for small separations but significantly underestimates it for large separations, ignoring the curvature of the space.\n\n**2. Implementation Strategy**\n\n**Data and Coordinate System**\nAll input latitude ($\\lambda$) and longitude ($\\phi$) coordinates are given in degrees and must be converted to radians for trigonometric calculations. The spherical coordinates $(\\lambda, \\phi)$ are converted to a 3D Cartesian system for calculating chordal distances and facilitating geodesic distance calculation:\n$$\nx = R \\cos\\lambda \\cos\\phi \\quad ; \\quad y = R \\cos\\lambda \\sin\\phi \\quad ; \\quad z = R \\sin\\lambda\n$$\nThe ground-truth field $f(\\lambda, \\phi)$ is evaluated at the specified training and target locations to generate the \"observed\" training values and the true values for error computation.\n\n**Algorithmic Procedure**\nFor each range parameter $\\rho \\in \\{500, 2000, 6000\\}$, the following process is executed for both the spherical (geodesic) and chordal models:\n\n1.  **Construct Covariance Matrix**: Compute the $25 \\times 25$ distance matrix between all pairs of training points using the chosen metric ($d_g$ or $d_c$). Apply the Matérn covariance function to this distance matrix to form the covariance matrix $\\mathbf{K}$. Add the nugget $\\tau^2$ to the diagonal elements.\n\n2.  **Solve Kriging System**: Form the an augmented $26 \\times 26$ matrix $\\mathbf{A} = \\begin{pmatrix} \\mathbf{K} + \\tau^2\\mathbf{I} & \\mathbf{1} \\\\ \\mathbf{1}^T & 0 \\end{pmatrix}$. To efficiently solve for the weights for multiple target points, we compute the inverse $\\mathbf{A}^{-1}$ once per model setup.\n\n3.  **Compute Predictions**: For each target point (in both polar and equatorial sets):\n    a.  Calculate the covariance vector $\\mathbf{k}_0$ between the target and all training points using the corresponding distance metric and Matérn function.\n    b.  Construct the right-hand side vector $\\mathbf{b} = [\\mathbf{k}_0^T, 1]^T$.\n    c.  Compute the weight vector $\\mathbf{w} = \\mathbf{A}^{-1} \\mathbf{b}$.\n    d.  The prediction is the dot product of the first $n$ weights and the training values: $\\hat{f}(\\mathbf{x}_0) = \\boldsymbol{\\lambda}^T \\mathbf{z}$.\n\n**Evaluation Metric**\nThe performance of each model is quantified by the Root Mean Square Error (RMSE), calculated separately for the polar and equatorial target sets:\n$$\n\\text{RMSE} = \\sqrt{\\frac{1}{N_{\\text{targets}}} \\sum_{i=1}^{N_{\\text{targets}}} (\\hat{f}_i - f_i)^2}\n$$\nwhere $\\hat{f}_i$ are the predicted values and $f_i$ are the ground-truth values. The final required outputs are these RMSE values, their difference at the poles, and a boolean comparison.\n\nThis comprehensive procedure allows for a rigorous, quantitative comparison of the effects of using a geometrically correct spherical distance versus a simplified Euclidean approximation in a standard geostatistical context. The choice of target locations, especially near the poles, is designed to stress the geometric deficiencies of the chordal distance model.", "answer": "```python\nimport numpy as np\nfrom scipy.linalg import inv\n\ndef solve():\n    \"\"\"\n    Implements and compares spherical vs. chordal-distance ordinary kriging\n    on a sphere to evaluate prediction accuracy for a synthetic gravity field.\n    \"\"\"\n    # ------------------\n    # 1. Constants and Setup\n    # ------------------\n    R_EARTH = 6371.0  # km\n    SIGMA2 = 1.0  # mGal^2, marginal variance (sill)\n    TAU2 = 1e-6  # mGal^2, nugget for numerical stability\n    RHO_VALUES = [500.0, 2000.0, 6000.0]  # km, range parameters\n\n    # Define observation and target locations in degrees\n    train_lats_deg = [-60, -30, 0, 30, 60]\n    train_lons_deg = [0, 72, 144, 216, 288]\n    polar_target_lats_deg = [85, -85]\n    polar_target_lons_deg = [0, 90, 180, 270]\n    eq_target_lats_deg = [0]\n    eq_target_lons_deg = [45, 135, 225, 315]\n\n    # Create lists of (latitude, longitude) pairs\n    train_locs_deg = [(lat, lon) for lat in train_lats_deg for lon in train_lons_deg]\n    polar_target_locs_deg = [(lat, lon) for lat in polar_target_lats_deg for lon in polar_target_lons_deg]\n    eq_target_locs_deg = [(lat, lon) for lat in eq_target_lats_deg for lon in eq_target_lons_deg]\n\n    # Convert all locations to radians for calculations\n    train_locs_rad = [(np.deg2rad(lat), np.deg2rad(lon)) for lat, lon in train_locs_deg]\n    polar_target_locs_rad = [(np.deg2rad(lat), np.deg2rad(lon)) for lat, lon in polar_target_locs_deg]\n    eq_target_locs_rad = [(np.deg2rad(lat), np.deg2rad(lon)) for lat, lon in eq_target_locs_deg]\n\n    # ------------------\n    # 2. Helper Functions\n    # ------------------\n\n    def ground_truth(lat_rad, lon_rad):\n        \"\"\"Computes the true residual field value f(lambda, phi).\"\"\"\n        sin_lat = np.sin(lat_rad)\n        cos_lat = np.cos(lat_rad)\n        term1 = 0.8 * 0.5 * (3 * sin_lat**2 - 1)\n        term2 = 0.5 * cos_lat**2 * np.sin(2 * lon_rad)\n        term3 = 0.2 * sin_lat\n        return term1 + term2 + term3\n\n    def to_cartesian(lat_rad, lon_rad, radius):\n        \"\"\"Converts spherical coordinates (lat, lon) to 3D Cartesian.\"\"\"\n        x = radius * np.cos(lat_rad) * np.cos(lon_rad)\n        y = radius * np.cos(lat_rad) * np.sin(lon_rad)\n        z = radius * np.sin(lat_rad)\n        return np.array([x, y, z])\n\n    def great_circle_distance(loc1_rad, loc2_rad, radius):\n        \"\"\"Computes great-circle distance using unit vectors and arccos.\"\"\"\n        lat1, lon1 = loc1_rad\n        lat2, lon2 = loc2_rad\n        \n        v1 = to_cartesian(lat1, lon1, 1.0)\n        v2 = to_cartesian(lat2, lon2, 1.0)\n        \n        dot_product = np.dot(v1, v2)\n        dot_product = np.clip(dot_product, -1.0, 1.0)\n        \n        central_angle = np.arccos(dot_product)\n        return radius * central_angle\n\n    def chordal_distance(p1_cart, p2_cart):\n        \"\"\"Computes Euclidean chordal distance between two Cartesian points.\"\"\"\n        return np.linalg.norm(p1_cart - p2_cart)\n\n    def matern_15(d, rho):\n        \"\"\"Matérn covariance with nu=1.5 and sigma^2=1.0.\"\"\"\n        arg = np.sqrt(3) * d / rho\n        return (1.0 + arg) * np.exp(-arg)\n\n    def calculate_rmse(predictions, truths):\n        \"\"\"Computes the Root Mean Square Error.\"\"\"\n        return np.sqrt(np.mean((predictions - truths)**2))\n\n    # Pre-compute Cartesian coordinates for all locations\n    train_locs_cart = [to_cartesian(lat, lon, R_EARTH) for lat, lon in train_locs_rad]\n    polar_target_locs_cart = [to_cartesian(lat, lon, R_EARTH) for lat, lon in polar_target_locs_rad]\n    eq_target_locs_cart = [to_cartesian(lat, lon, R_EARTH) for lat, lon in eq_target_locs_rad]\n\n    # Generate training values and ground-truth target values\n    train_values = np.array([ground_truth(lat, lon) for lat, lon in train_locs_rad])\n    polar_target_true_values = np.array([ground_truth(lat, lon) for lat, lon in polar_target_locs_rad])\n    eq_target_true_values = np.array([ground_truth(lat, lon) for lat, lon in eq_target_locs_rad])\n\n    # ------------------\n    # 3. Main Processing Loop\n    # ------------------\n    \n    all_results = []\n    n_train = len(train_locs_rad)\n    \n    for rho in RHO_VALUES:\n        #\n        # --- Spherical Model (Great-Circle Distance) ---\n        #\n        K_spherical = np.zeros((n_train, n_train))\n        for i in range(n_train):\n            for j in range(i, n_train):\n                dist = great_circle_distance(train_locs_rad[i], train_locs_rad[j], R_EARTH)\n                cov = matern_15(dist, rho)\n                K_spherical[i, j] = K_spherical[j, i] = cov\n        \n        K_spherical += TAU2 * np.identity(n_train)\n        \n        A_spherical = np.ones((n_train + 1, n_train + 1))\n        A_spherical[:n_train, :n_train] = K_spherical\n        A_spherical[n_train, n_train] = 0.0\n        A_inv_spherical = inv(A_spherical)\n\n        def krige(target_locs_rad, A_inv):\n            preds = []\n            for target_loc_rad in target_locs_rad:\n                k0 = np.array([matern_15(great_circle_distance(target_loc_rad, train_loc, R_EARTH), rho) for train_loc in train_locs_rad])\n                b = np.append(k0, 1.0)\n                weights = A_inv @ b\n                preds.append(weights[:n_train] @ train_values)\n            return np.array(preds)\n\n        polar_preds_spherical = krige(polar_target_locs_rad, A_inv_spherical)\n        eq_preds_spherical = krige(eq_target_locs_rad, A_inv_spherical)\n\n        #\n        # --- Chordal Model ---\n        #\n        K_chordal = np.zeros((n_train, n_train))\n        for i in range(n_train):\n            for j in range(i, n_train):\n                dist = chordal_distance(train_locs_cart[i], train_locs_cart[j])\n                cov = matern_15(dist, rho)\n                K_chordal[i, j] = K_chordal[j, i] = cov\n        \n        K_chordal += TAU2 * np.identity(n_train)\n\n        A_chordal = np.ones((n_train + 1, n_train + 1))\n        A_chordal[:n_train, :n_train] = K_chordal\n        A_chordal[n_train, n_train] = 0.0\n        A_inv_chordal = inv(A_chordal)\n\n        def krige_chordal(target_locs_cart, A_inv):\n            preds = []\n            for target_loc_cart in target_locs_cart:\n                k0 = np.array([matern_15(chordal_distance(target_loc_cart, train_loc_cart), rho) for train_loc_cart in train_locs_cart])\n                b = np.append(k0, 1.0)\n                weights = A_inv @ b\n                preds.append(weights[:n_train] @ train_values)\n            return np.array(preds)\n        \n        polar_preds_chordal = krige_chordal(polar_target_locs_cart, A_inv_chordal)\n        eq_preds_chordal = krige_chordal(eq_target_locs_cart, A_inv_chordal)\n        \n        # ------------------\n        # 4. Compute and Store Results\n        # ------------------\n        \n        rmse_polar_s = calculate_rmse(polar_preds_spherical, polar_target_true_values)\n        rmse_polar_c = calculate_rmse(polar_preds_chordal, polar_target_true_values)\n        rmse_eq_s = calculate_rmse(eq_preds_spherical, eq_target_true_values)\n        rmse_eq_c = calculate_rmse(eq_preds_chordal, eq_target_true_values)\n        \n        diff_polar_rmse = rmse_polar_c - rmse_polar_s\n        is_spherical_better_polar = rmse_polar_s < rmse_polar_c\n        \n        all_results.extend([\n            rmse_polar_s,\n            rmse_polar_c,\n            rmse_eq_s,\n            rmse_eq_c,\n            diff_polar_rmse,\n            is_spherical_better_polar\n        ])\n\n    # ------------------\n    # 5. Final Output\n    # ------------------\n    \n    # Format results to string: booleans as lowercase, floats to 10 decimal places.\n    def format_result(r):\n        if isinstance(r, (bool, np.bool_)):\n            return str(r).lower()\n        return f\"{r:.10f}\"\n\n    print(f\"[{','.join(map(format_result, all_results))}]\")\n\nsolve()\n```", "id": "3599939"}, {"introduction": "A kriging model provides not only predictions but also a measure of prediction uncertainty—the kriging variance. A crucial step in any analysis is to validate whether these uncertainty estimates are reliable. This practice ([@problem_id:3599910]) focuses on designing a statistically sound test using cross-validation residuals to diagnose systematic over- or underestimation of kriging variance, a key aspect of model validation that ensures the reliability of your geostatistical results.", "problem": "Consider a second-order stationary, mean-square continuous random field $Y(\\mathbf{x})$ on $\\mathbb{R}^d$ with covariance function $C(\\mathbf{h}) = \\operatorname{Cov}(Y(\\mathbf{x}), Y(\\mathbf{x}+\\mathbf{h}))$ and semivariogram $\\gamma(\\mathbf{h}) = \\tfrac{1}{2}\\operatorname{Var}(Y(\\mathbf{x}) - Y(\\mathbf{x}+\\mathbf{h}))$. Assume $Y(\\mathbf{x})$ is Gaussian. Let $\\{\\mathbf{x}_i\\}_{i=1}^n$ be distinct sampling locations with observed values $\\{y_i\\}_{i=1}^n$. For each $i \\in \\{1,\\dots,n\\}$, form the leave-one-out ordinary kriging predictor $\\hat{y}_{-i}(\\mathbf{x}_i)$ using $\\{y_j: j \\neq i\\}$ under a fitted covariance (or semivariogram) model, and define the cross-validation prediction error $e_i = y_i - \\hat{y}_{-i}(\\mathbf{x}_i)$. Let the corresponding kriging variance (prediction error variance under the model) at $\\mathbf{x}_i$ be $\\sigma_{K,i}^2$, and define the standardized residual $z_i = e_i / \\sigma_{K,i}$.\n\nUnder the Gaussian assumption and correct specification of the covariance model, the vector of errors $\\mathbf{e} = (e_1,\\dots,e_n)^\\top$ can be expressed as a linear transformation of $\\mathbf{y} = (y_1,\\dots,y_n)^\\top$: there exists an $n \\times n$ matrix $A$ whose $i$-th row has a $+1$ at column $i$ and entries $-\\lambda_{ij}$ in columns $j \\neq i$, where $\\{\\lambda_{ij}\\}_{j \\neq i}$ are the ordinary kriging weights used to predict at $\\mathbf{x}_i$ from $\\{\\mathbf{x}_j: j \\neq i\\}$. Thus $e_i = y_i - \\sum_{j \\neq i} \\lambda_{ij} y_j$, or compactly $\\mathbf{e} = A \\mathbf{y}$. With the modeled covariance matrix $\\Sigma_Y = [C(\\mathbf{x}_i - \\mathbf{x}_j)]_{i,j=1}^n$, it follows that $\\Sigma_E = \\operatorname{Cov}(\\mathbf{e}) = A \\Sigma_Y A^\\top$. The kriging variances satisfy $\\sigma_{K,i}^2 = (\\Sigma_E)_{ii}$ under the model. Define $D = \\operatorname{diag}(\\sigma_{K,1},\\dots,\\sigma_{K,n})$ and $\\mathbf{z} = D^{-1} \\mathbf{e}$, so that $\\operatorname{Cov}(\\mathbf{z}) = \\Sigma_z = D^{-1} \\Sigma_E D^{-1}$ has unit diagonal.\n\nSuppose we seek to test whether the kriging variances $\\{\\sigma_{K,i}^2\\}$ are systematically under- or overestimated relative to the actual prediction error variance at the left-out locations, in the sense of a multiplicative scale factor $\\psi$ such that the true error covariance is $\\psi \\Sigma_E$ while predictions and $\\{\\sigma_{K,i}^2\\}$ are computed under the modeled $\\Sigma_E$ (so $\\psi = 1$ under the null hypothesis). Using the standardized residuals $z_i = e_i / \\sigma_{K,i}$, design a statistically principled test to detect $\\psi > 1$ (underestimation of kriging variance) or $\\psi < 1$ (overestimation), taking into account the spatial dependence among $\\{z_i\\}_{i=1}^n$. Which of the following proposals is most scientifically justified under the stated assumptions?\n\nA. Compute $M_2 = \\frac{1}{n} \\sum_{i=1}^n z_i^2$ and, using the modeled covariance $\\Sigma_z$ under $\\psi = 1$, calibrate the null distribution of $M_2$ either by the Gaussian quadratic-form variance $\\operatorname{Var}(M_2) = \\frac{2}{n^2} \\operatorname{tr}(\\Sigma_z^2)$ to form the test statistic $T = \\frac{M_2 - 1}{\\sqrt{(2/n^2)\\operatorname{tr}(\\Sigma_z^2)}}$ with one-sided rejection regions, or by parametric bootstrap simulations $\\mathbf{z}^\\ast \\sim \\mathcal{N}(\\mathbf{0}, \\Sigma_z)$ to obtain one-sided $p$-values. Reject for large positive $T$ (or upper-tail $p$-values) to conclude $\\psi > 1$, and for large negative $T$ (or lower-tail $p$-values) to conclude $\\psi < 1$.\n\nB. Use $\\sum_{i=1}^n z_i^2$ and treat it as a chi-square statistic with $n$ degrees of freedom, i.e., reject $\\psi = 1$ if $\\sum_{i=1}^n z_i^2$ is too large or too small relative to the $\\chi^2_n$ distribution.\n\nC. Perform a one-sample $t$-test on the mean of the standardized residuals $\\bar{z} = \\frac{1}{n} \\sum_{i=1}^n z_i$ against zero; reject for $\\bar{z}$ significantly different from zero to conclude under- or overestimation of the kriging variance.\n\nD. Apply a Kolmogorov–Smirnov goodness-of-fit test comparing the empirical distribution of $\\{z_i\\}$ to the standard normal $\\mathcal{N}(0,1)$, and decide under- or overestimation from whether the empirical variance appears larger or smaller than one.\n\nSelect the option that correctly formulates and justifies a test aligned with the Gaussian random field and kriging framework and explicitly accounts for spatial dependence among $\\{z_i\\}$ when assessing variance mis-specification.", "solution": "The problem statement is subjected to validation prior to any attempt at a solution.\n\n### Step 1: Extract Givens\n- A second-order stationary, mean-square continuous, Gaussian random field $Y(\\mathbf{x})$ on $\\mathbb{R}^d$.\n- Covariance function: $C(\\mathbf{h}) = \\operatorname{Cov}(Y(\\mathbf{x}), Y(\\mathbf{x}+\\mathbf{h}))$.\n- Semivariogram: $\\gamma(\\mathbf{h}) = \\tfrac{1}{2}\\operatorname{Var}(Y(\\mathbf{x}) - Y(\\mathbf{x}+\\mathbf{h}))$.\n- Sample data: $\\{y_i\\}_{i=1}^n$ observed at distinct locations $\\{\\mathbf{x}_i\\}_{i=1}^n$.\n- Leave-one-out ordinary kriging predictor at $\\mathbf{x}_i$: $\\hat{y}_{-i}(\\mathbf{x}_i) = \\sum_{j \\neq i} \\lambda_{ij} y_j$.\n- Cross-validation prediction error: $e_i = y_i - \\hat{y}_{-i}(\\mathbf{x}_i)$.\n- Modeled kriging variance at $\\mathbf{x}_i$: $\\sigma_{K,i}^2$.\n- Standardized residual: $z_i = e_i / \\sigma_{K,i}$.\n- Vector of errors: $\\mathbf{e} = (e_1, \\dots, e_n)^\\top$.\n- Linear relationship: $\\mathbf{e} = A \\mathbf{y}$, where $A$ is an $n \\times n$ matrix with $A_{ii}=1$ and $A_{ij}=-\\lambda_{ij}$ for $j \\neq i$.\n- Modeled data covariance matrix: $\\Sigma_Y = [C(\\mathbf{x}_i - \\mathbf{x}_j)]_{i,j=1}^n$.\n- Modeled error covariance matrix: $\\Sigma_E = \\operatorname{Cov}(\\mathbf{e}) = A \\Sigma_Y A^\\top$.\n- Kriging variances as diagonal elements: $\\sigma_{K,i}^2 = (\\Sigma_E)_{ii}$.\n- Diagonal matrix of standard deviations: $D = \\operatorname{diag}(\\sigma_{K,1}, \\dots, \\sigma_{K,n})$.\n- Standardized residual vector: $\\mathbf{z} = D^{-1} \\mathbf{e}$.\n- Modeled standardized residual covariance matrix: $\\operatorname{Cov}(\\mathbf{z}) = \\Sigma_z = D^{-1} \\Sigma_E D^{-1}$, which has a unit diagonal.\n- Hypothesis testing framework: The true error covariance is assumed to be $\\psi \\Sigma_E$ for some scale factor $\\psi$. The null hypothesis is $H_0: \\psi=1$. The goal is to design a test for $\\psi > 1$ or $\\psi < 1$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is entirely self-contained and internally consistent.\n- **Scientifically Grounded:** The setup is a standard and rigorous description of leave-one-out cross-validation for geostatistical models. All definitions, including those for kriging predictors, errors, variances, and their matrix representations, are correct according to established geostatistical theory for Gaussian random fields. The problem of testing for a systematic mis-scaling of prediction variance is a fundamental part of model diagnostics.\n- **Well-Posed:** The problem provides a clear statistical setup and poses a specific, answerable question regarding hypothesis testing. All necessary components (data, model structure, hypothesis) are defined.\n- **Objective:** The language is formal, precise, and devoid of any subjective or ambiguous terminology.\n\n### Step 3: Verdict and Action\nThe problem statement is **valid**. It is a well-posed, scientifically sound question in computational geophysics and spatial statistics. We may proceed with the solution derivation and option analysis.\n\n### Derivation of the Correct Test\n\nThe core of the problem is to test the hypothesis about the scale factor $\\psi$. Let us formalize the statistical properties of the standardized residuals $\\mathbf{z}$ under the model and the specified hypothesis.\n\nThe vector of observations $\\mathbf{y}$ is assumed to be a realization of a Gaussian random field. Under the specified true process, its covariance matrix is $\\operatorname{Cov}(\\mathbf{y}) = \\psi \\Sigma_Y$, where $\\Sigma_Y$ is the covariance matrix corresponding to the *modeled* spatial structure. We assume an unknown constant mean, $E[\\mathbf{y}] = \\mu \\mathbf{1}$.\n\nThe vector of leave-one-out prediction errors is $\\mathbf{e} = A \\mathbf{y}$. By the properties of ordinary kriging (the sum of weights $\\sum_{j \\neq i} \\lambda_{ij} = 1$), the matrix $A$ satisfies $A\\mathbf{1}=\\mathbf{0}$. Therefore, the error vector $\\mathbf{e}$ has zero mean: $E[\\mathbf{e}] = E[A \\mathbf{y}] = A E[\\mathbf{y}] = A (\\mu \\mathbf{1}) = \\mathbf{0}$.\n\nThe true covariance matrix of the errors is $\\operatorname{Cov}(\\mathbf{e}) = A \\operatorname{Cov}(\\mathbf{y}) A^\\top = A (\\psi \\Sigma_Y) A^\\top = \\psi (A \\Sigma_Y A^\\top) = \\psi \\Sigma_E$. Here, $\\Sigma_E = A \\Sigma_Y A^\\top$ is the covariance matrix of the errors *predicted by the model* (i.e., under $\\psi=1$).\n\nThe standardized residuals are defined as $\\mathbf{z} = D^{-1} \\mathbf{e}$, where $D$ is the diagonal matrix of kriging standard deviations $\\sigma_{K,i} = \\sqrt{(\\Sigma_E)_{ii}}$, which are calculated from the *modeled* covariance. Thus, $D$ is fixed and does not depend on the true $\\psi$.\nThe mean of $\\mathbf{z}$ is $E[\\mathbf{z}] = D^{-1} E[\\mathbf{e}] = \\mathbf{0}$.\nThe true covariance matrix of the standardized residuals is:\n$$ \\operatorname{Cov}(\\mathbf{z}) = D^{-1} \\operatorname{Cov}(\\mathbf{e}) D^{-1} = D^{-1} (\\psi \\Sigma_E) D^{-1} = \\psi (D^{-1} \\Sigma_E D^{-1}) = \\psi \\Sigma_z $$\nwhere $\\Sigma_z = D^{-1} \\Sigma_E D^{-1}$ is the covariance matrix of standardized residuals *predicted by the model*. By construction, the diagonal elements of $\\Sigma_z$ are all $1$, i.e., $(\\Sigma_z)_{ii} = (\\sigma_{K,i})^{-2} (\\Sigma_E)_{ii} = (\\sigma_{K,i})^{-2} \\sigma_{K,i}^2 = 1$.\n\nSo, the statistical model for the observed standardized residuals is $\\mathbf{z} \\sim \\mathcal{N}(\\mathbf{0}, \\psi \\Sigma_z)$. We need to test $H_0: \\psi=1$ against $H_A: \\psi > 1$ or $\\psi < 1$.\n\nA natural quantity to inspect is the magnitude of the standardized residuals. Consider the mean of their squares:\n$$ M_2 = \\frac{1}{n} \\sum_{i=1}^n z_i^2 = \\frac{1}{n} \\mathbf{z}^\\top \\mathbf{z} $$\nThe expected value of this statistic is:\n$$ E[M_2] = \\frac{1}{n} E[\\mathbf{z}^\\top \\mathbf{z}] = \\frac{1}{n} \\operatorname{tr}(\\operatorname{Cov}(\\mathbf{z})) = \\frac{1}{n} \\operatorname{tr}(\\psi \\Sigma_z) = \\frac{\\psi}{n} \\operatorname{tr}(\\Sigma_z) $$\nSince $\\operatorname{tr}(\\Sigma_z) = \\sum_{i=1}^n (\\Sigma_z)_{ii} = \\sum_{i=1}^n 1 = n$, we have $E[M_2] = \\psi$.\nThus, $M_2$ is an unbiased estimator for $\\psi$. Under the null hypothesis $H_0: \\psi=1$, $E[M_2]=1$. A value of $M_2$ significantly larger than $1$ suggests $\\psi > 1$ (underestimation of variance), while a value significantly smaller than $1$ suggests $\\psi < 1$ (overestimation of variance).\n\nTo construct a formal test, we need the distribution of $M_2$ under $H_0$. Under $H_0$, $\\mathbf{z} \\sim \\mathcal{N}(\\mathbf{0}, \\Sigma_z)$. $M_2 = \\frac{1}{n}\\mathbf{z}^\\top\\mathbf{z}$ is a scaled quadratic form of a multivariate normal vector. The variance of $M_2$ under $H_0$ can be calculated using the standard formula for the variance of a quadratic form ($\\operatorname{Var}(\\mathbf{x}^\\top Q \\mathbf{x}) = 2 \\operatorname{tr}[(Q\\Sigma)^2]$ for $\\mathbf{x} \\sim \\mathcal{N}(\\mathbf{0}, \\Sigma)$):\n$$ \\operatorname{Var}(M_2) = \\operatorname{Var}\\left(\\frac{1}{n} \\mathbf{z}^\\top I \\mathbf{z}\\right) = \\frac{1}{n^2} \\operatorname{Var}(\\mathbf{z}^\\top \\mathbf{z}) = \\frac{1}{n^2} (2 \\operatorname{tr}(\\Sigma_z^2)) = \\frac{2}{n^2} \\operatorname{tr}(\\Sigma_z^2) $$\nThe crucial point is that $\\Sigma_z$ is generally not the identity matrix $I$, as the residuals $\\{z_i\\}$ are spatially correlated. A valid test must account for this covariance structure.\n\nWith the mean ($1$) and variance ($\\frac{2}{n^2} \\operatorname{tr}(\\Sigma_z^2)$) of $M_2$ under $H_0$, we can form a test statistic. For large $n$, by a central limit theorem for dependent variables or quadratic forms, $M_2$ will be approximately normally distributed. A standardized test statistic is:\n$$ T = \\frac{M_2 - E[M_2]}{\\sqrt{\\operatorname{Var}(M_2)}} = \\frac{M_2 - 1}{\\sqrt{(2/n^2)\\operatorname{tr}(\\Sigma_z^2)}} $$\nUnder $H_0$, $T \\approx \\mathcal{N}(0,1)$. We would reject $H_0$ for large positive values of $T$ (concluding $\\psi>1$) or large negative values of $T$ (concluding $\\psi<1$).\n\nAn alternative, often more accurate, method for determining the null distribution of $M_2$ is parametric bootstrapping. This involves simulating many samples $\\mathbf{z}^\\ast \\sim \\mathcal{N}(\\mathbf{0}, \\Sigma_z)$, calculating $M_2^\\ast$ for each sample, and using the empirical distribution of these $M_2^\\ast$ values as the reference distribution to compute a $p$-value for the observed $M_2$.\n\nThis complete line of reasoning matches the proposal in option A.\n\n### Option-by-Option Analysis\n\n**A. Compute $M_2 = \\frac{1}{n} \\sum_{i=1}^n z_i^2$ and, using the modeled covariance $\\Sigma_z$ under $\\psi = 1$, calibrate the null distribution of $M_2$ either by the Gaussian quadratic-form variance $\\operatorname{Var}(M_2) = \\frac{2}{n^2} \\operatorname{tr}(\\Sigma_z^2)$ to form the test statistic $T = \\frac{M_2 - 1}{\\sqrt{(2/n^2)\\operatorname{tr}(\\Sigma_z^2)}}$ with one-sided rejection regions, or by parametric bootstrap simulations $\\mathbf{z}^\\ast \\sim \\mathcal{N}(\\mathbf{0}, \\Sigma_z)$ to obtain one-sided $p$-values. Reject for large positive $T$ (or upper-tail $p$-values) to conclude $\\psi > 1$, and for large negative $T$ (or lower-tail $p$-values) to conclude $\\psi < 1$.**\n\nThis option correctly identifies that $M_2 = \\frac{1}{n}\\sum z_i^2$ is the appropriate statistic, as its expectation is $\\psi$. It correctly states its expected value under the null is $1$. Most importantly, it proposes two valid methods for calibrating the null distribution of $M_2$ which explicitly and correctly account for the spatial dependence among the standardized residuals, represented by the covariance matrix $\\Sigma_z$. The normal approximation via the variance of the quadratic form and the parametric bootstrap are both standard and rigorous techniques for this exact problem.\n**Verdict: Correct.**\n\n**B. Use $\\sum_{i=1}^n z_i^2$ and treat it as a chi-square statistic with $n$ degrees of freedom, i.e., reject $\\psi = 1$ if $\\sum_{i=1}^n z_i^2$ is too large or too small relative to the $\\chi^2_n$ distribution.**\n\nThis proposal is flawed. The statistic $\\sum_{i=1}^n z_i^2$ follows a chi-square distribution with $n$ degrees of freedom, $\\chi^2_n$, only if the variables $\\{z_i\\}_{i=1}^n$ are independent and identically distributed as standard normal, i.e., $\\mathbf{z} \\sim \\mathcal{N}(\\mathbf{0}, I)$. However, the problem setup makes it clear that the standardized residuals are correlated, with a covariance matrix $\\Sigma_z = D^{-1} A \\Sigma_Y A^\\top D^{-1}$ that is not, in general, the identity matrix $I$. Ignoring the off-diagonal terms of $\\Sigma_z$ is a serious statistical error that invalidates the test.\n**Verdict: Incorrect.**\n\n**C. Perform a one-sample $t$-test on the mean of the standardized residuals $\\bar{z} = \\frac{1}{n} \\sum_{i=1}^n z_i$ against zero; reject for $\\bar{z}$ significantly different from zero to conclude under- or overestimation of the kriging variance.**\n\nThis option proposes a test for a different hypothesis. The expected value of each $z_i$ is $0$, and thus $E[\\bar{z}] = 0$, regardless of the value of the scale factor $\\psi$. A test on the mean residual $\\bar{z}$ checks for systematic bias in the kriging predictions, which might arise from a non-stationary mean (a trend), not for a mis-specification of the variance. The test has no power to detect whether $\\psi \\neq 1$.\n**Verdict: Incorrect.**\n\n**D. Apply a Kolmogorov–Smirnov goodness-of-fit test comparing the empirical distribution of $\\{z_i\\}$ to the standard normal $\\mathcal{N}(0,1)$, and decide under- or overestimation from whether the empirical variance appears larger or smaller than one.**\n\nThis proposal contains a fundamental error similar to that in option B. The standard Kolmogorov-Smirnov (K-S) test is valid only for independent and identically distributed (i.i.d.) data. The set of standardized residuals $\\{z_i\\}$ is not a set of independent draws; they are correlated with covariance matrix $\\Sigma_z$. Applying the K-S test directly is statistically invalid. While a departure from the $\\mathcal{N}(0,1)$ distribution is expected if $\\psi \\neq 1$, this specific test procedure is inappropriate due to the violation of the independence assumption. Furthermore, tests like K-S are general goodness-of-fit tests and are typically less powerful for detecting a specific alternative, such as a change in variance, compared to a test specifically designed for that purpose, like the one in option A.\n**Verdict: Incorrect.**", "answer": "$$\\boxed{A}$$", "id": "3599910"}]}