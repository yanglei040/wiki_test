{"hands_on_practices": [{"introduction": "Recovering sharp geological interfaces, such as layer boundaries or faults, is a fundamental task where traditional methods often fall short. This practice problem provides a head-to-head comparison between classical $\\ell_2$ (Tikhonov) regularization, which tends to smooth out features, and sparsity-promoting $\\ell_1$ regularization. By analyzing a simple step-function model, you will derive from first principles the conditions under which $\\ell_1$ methods preserve sharp edges while $\\ell_2$ methods inevitably blur them, building a core intuition for the power of sparse recovery [@problem_id:3580648].", "problem": "Consider a one-dimensional layered Earth property $m(x)$ on the interval $[-L,L]$ with a single sharp interface at $x=0$, modeled as a step of amplitude $A0$:\n$$\nf(x)=A\\,H(x),\\quad x\\in[-L,L],\n$$\nwhere $H(x)$ is the Heaviside function. You are asked to compare two regularized inversions for a model $u(x)$ that balances data fidelity and smoothness/sparsity, within the standard variational framework used in computational geophysics.\n\n- $\\ell_{2}$ (Tikhonov) smoothing with squared-gradient penalty:\n$$\n\\min_{u}\\;\\frac{1}{2}\\int_{-\\infty}^{\\infty}\\big(u(x)-f(x)\\big)^{2}\\,dx+\\frac{\\lambda}{2}\\int_{-\\infty}^{\\infty}\\big(u'(x)\\big)^{2}\\,dx,\n$$\nwith regularization parameter $\\lambda0$.\n\n- $\\ell_{1}$ total variation (TV) denoising with absolute-gradient penalty:\n$$\n\\min_{u}\\;\\frac{1}{2}\\int_{-L}^{L}\\big(u(x)-f(x)\\big)^{2}\\,dx+\\lambda\\int_{-L}^{L}|u'(x)|\\,dx,\n$$\nwith the same symbol $\\lambda0$ denoting a regularization weight (noting that the two $\\lambda$’s carry different physical units in the two functionals).\n\nWork from the fundamental definitions above, without importing pre-packaged formulas, and proceed as follows.\n\n1) For the $\\ell_{2}$ (Tikhonov) problem, derive the Euler–Lagrange equation and solve the corresponding Green’s function problem on $\\mathbb{R}$ for a step input. Show that the Tikhonov solution $u_{\\ell_{2}}(x)$ is a smoothed step with an exponential transition. Define the $10$–$90$ transition width $w_{2}$ as $x_{90}-x_{10}$, where $u_{\\ell_{2}}(x_{10})=0.1\\,A$ and $u_{\\ell_{2}}(x_{90})=0.9\\,A$. Derive $w_{2}$ explicitly as a function of $\\lambda$.\n\n2) For the $\\ell_{1}$ total variation (TV) problem on $[-L,L]$, restrict the search to piecewise constant candidates with a single jump at $x=0$, namely $u(x)=u_{\\ell}$ for $x\\in[-L,0)$ and $u(x)=u_{r}$ for $x\\in[0,L]$, and minimize the resulting two-parameter convex functional. Derive necessary and sufficient conditions under which the optimizer has a nonzero preserved jump $u_{r}-u_{\\ell}0$, and give the optimal $(u_{\\ell},u_{r})$ in that regime.\n\n3) Field practice dictates using a single regularization weight $\\lambda$ for both runs (to hold the regularization budget fixed across experiments). The resolution requirement states that an inversion is deemed “oversmoothed” if its $10$–$90$ transition width exceeds a prescribed resolution length $w_{c}0$. Determine the minimal interface amplitude $A_{\\min}$ (as an exact, closed-form expression in terms of $L$ and $w_{c}$) for which there exists a single $\\lambda$ such that:\n- the $\\ell_{2}$ solution oversmooths the interface according to the criterion $w_{2}\\geq w_{c}$, and simultaneously\n- the $\\ell_{1}$ TV solution preserves a nonzero jump.\n\nExpress your final answer as a closed-form analytic expression for $A_{\\min}$ in terms of $L$ and $w_{c}$ only, using natural logarithms as needed. Express the final amplitude in the same unit as $A$ (do not convert units). No numerical substitution is required; do not round.", "solution": "The problem will first be validated for scientific and mathematical soundness.\n\n### Step 1: Extract Givens\n- **Model Function**: A one-dimensional property $f(x)$ representing a single sharp interface.\n  $$f(x)=A\\,H(x),\\quad x\\in[-L,L], \\quad A0$$\n  where $H(x)$ is the Heaviside step function.\n- **$\\ell_2$ (Tikhonov) Regularization Functional**:\n  $$J_2[u] = \\min_{u}\\;\\frac{1}{2}\\int_{-\\infty}^{\\infty}\\big(u(x)-f(x)\\big)^{2}\\,dx+\\frac{\\lambda}{2}\\int_{-\\infty}^{\\infty}\\big(u'(x)\\big)^{2}\\,dx, \\quad \\lambda0$$\n- **$\\ell_1$ (Total Variation) Regularization Functional**:\n  $$J_1[u] = \\min_{u}\\;\\frac{1}{2}\\int_{-L}^{L}\\big(u(x)-f(x)\\big)^{2}\\,dx+\\lambda\\int_{-L}^{L}|u'(x)|\\,dx, \\quad \\lambda0$$\n- **Task 1 (for $\\ell_2$)**: Derive the Euler–Lagrange equation, solve for the solution $u_{\\ell_2}(x)$ on $\\mathbb{R}$, and find the $10$–$90$ transition width $w_2$.\n- **Task 2 (for $\\ell_1$)**: For a piecewise constant candidate $u(x)$ with one jump at $x=0$, find the conditions for preserving a nonzero jump and the optimal jump values $(u_{\\ell}, u_r)$.\n- **Task 3 (Synthesis)**: A single regularization weight $\\lambda$ is used for both problems. An inversion is \"oversmoothed\" if its transition width $w_2 \\ge w_c$ for a resolution length $w_c  0$. Find the minimal amplitude $A_{\\min}$ for which there exists a $\\lambda$ such that the $\\ell_2$ solution is oversmoothed AND the $\\ell_1$ solution preserves a nonzero jump.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientific Grounding**: The problem compares Tikhonov ($\\ell_2$) and Total Variation ($\\ell_1$) regularization, which are fundamental and widely used techniques in inverse problems, particularly in geophysics and signal processing. The use of a step function to analyze resolution is a classic and valid approach.\n2.  **Well-Posedness**: The minimization problems are standard convex optimization problems that admit unique solutions. The tasks are mathematically precise and lead to a unique answer.\n3.  **Dimensional Consistency**: A critical check concerns the statement \"Field practice dictates using a single regularization weight $\\lambda$ for both runs\". The two functionals use a parameter denoted by $\\lambda$, but they have different physical units.\n    - In the $\\ell_2$ functional, let the units of $u$ be $[U]$ and the units of $x$ be $[L]$. The data fidelity term has units $[U]^2 [L]$. The penalty term is $\\lambda_2 \\int (u')^2 dx$, which has units $[\\lambda_2] ([U]/[L])^2 [L] = [\\lambda_2] [U]^2 [L]^{-1}$. For the functional to be consistent, $[\\lambda_2] = [L]^2$.\n    - In the $\\ell_1$ functional, the data fidelity term has units $[U]^2 [L]$. The penalty term is $\\lambda_1 \\int |u'| dx$, which represents the total variation of $u$ and has units $[\\lambda_1] [U]$. For consistency, $[\\lambda_1] = [U][L]$.\n    - For the numerical value of $\\lambda$ to be the same in both cases ($\\lambda_1 = \\lambda_2$), we must have $[L]^2 = [U][L]$, which implies $[U]=[L]$. This means the physical property $u(x)$ must have units of length. This is a plausible scenario in geophysics (e.g., depth to an interface, layer thickness). Assuming this physical context, the problem is dimensionally consistent.\n\n### Step 3: Verdict and Action\nThe problem is deemed valid under the reasonable assumption that the physical property being modeled has units of length, ensuring dimensional consistency. I will proceed with the solution.\n\n### Part 1: $\\ell_2$ Tikhonov Regularization\nThe Tikhonov functional is $J_2[u] = \\int_{-\\infty}^{\\infty} \\mathcal{L}(x, u, u') dx$ with the Lagrangian $\\mathcal{L} = \\frac{1}{2}(u-f)^2 + \\frac{\\lambda}{2}(u')^2$. The Euler-Lagrange equation, $\\frac{\\partial \\mathcal{L}}{\\partial u} - \\frac{d}{dx}\\frac{\\partial \\mathcal{L}}{\\partial u'} = 0$, is:\n$$ (u(x)-f(x)) - \\frac{d}{dx}(\\lambda u'(x)) = 0 \\implies u(x) - \\lambda u''(x) = f(x) $$\nThis is a second-order linear ODE. The solution can be found by convolution with the Green's function for the operator $1 - \\lambda \\frac{d^2}{dx^2}$. The Green's function $G(x)$ satisfies $G(x) - \\lambda G''(x) = \\delta(x)$. The homogeneous equation $\\lambda y'' - y = 0$ has solutions $y(x) = c_1 \\exp(x/\\sqrt{\\lambda}) + c_2 \\exp(-x/\\sqrt{\\lambda})$. For $G(x)$ to decay at $\\pm\\infty$, it must be of the form $C \\exp(-|x|/\\sqrt{\\lambda})$. Integrating the ODE for $G(x)$ across $x=0$ yields the jump condition on the derivative: $-\\lambda(G'(0^+) - G'(0^-)) = 1$. This gives $C = \\frac{1}{2\\sqrt{\\lambda}}$.\nThe Green's function is $G(x) = \\frac{1}{2\\sqrt{\\lambda}}\\exp(-|x|/\\sqrt{\\lambda})$.\nThe solution $u_{\\ell_2}(x)$ is the convolution of $f(x)=A H(x)$ with $G(x)$:\n$$u_{\\ell_2}(x) = (f*G)(x) = \\int_{-\\infty}^{\\infty} A H(\\xi) \\frac{1}{2\\sqrt{\\lambda}}\\exp\\left(-\\frac{|x-\\xi|}{\\sqrt{\\lambda}}\\right)d\\xi = \\frac{A}{2\\sqrt{\\lambda}}\\int_{0}^{\\infty}\\exp\\left(-\\frac{|x-\\xi|}{\\sqrt{\\lambda}}\\right)d\\xi$$\nFor $x  0$, $|x-\\xi| = \\xi-x$. The integral becomes $\\int_0^\\infty \\exp(-(\\xi-x)/\\sqrt{\\lambda})d\\xi = \\sqrt{\\lambda}\\exp(x/\\sqrt{\\lambda})$.\nFor $x \\ge 0$, we split the integral at $\\xi=x$: $\\int_0^x \\exp(-(x-\\xi)/\\sqrt{\\lambda})d\\xi + \\int_x^\\infty \\exp(-(\\xi-x)/\\sqrt{\\lambda})d\\xi = \\sqrt{\\lambda}(1-\\exp(-x/\\sqrt{\\lambda})) + \\sqrt{\\lambda} = \\sqrt{\\lambda}(2-\\exp(-x/\\sqrt{\\lambda}))$.\nCombining these results, we get the solution:\n$$ u_{\\ell_2}(x) = \\begin{cases} \\frac{A}{2} \\exp(x/\\sqrt{\\lambda})  \\text{if } x0 \\\\ A\\left(1-\\frac{1}{2}\\exp(-x/\\sqrt{\\lambda})\\right)  \\text{if } x\\ge 0 \\end{cases} $$\nThis function represents a smoothed step. We now find the $10$–$90$ transition width $w_2 = x_{90}-x_{10}$.\n$u_{\\ell_2}(x_{10}) = 0.1\\,A \\implies \\frac{A}{2} \\exp(x_{10}/\\sqrt{\\lambda}) = 0.1\\,A \\implies \\exp(x_{10}/\\sqrt{\\lambda}) = 0.2$.\n$$ x_{10} = \\sqrt{\\lambda}\\ln(0.2) = -\\sqrt{\\lambda}\\ln(5) $$\n$u_{\\ell_2}(x_{90}) = 0.9\\,A \\implies A(1-\\frac{1}{2}\\exp(-x_{90}/\\sqrt{\\lambda})) = 0.9\\,A \\implies \\exp(-x_{90}/\\sqrt{\\lambda}) = 0.2$.\n$$ x_{90} = -\\sqrt{\\lambda}\\ln(0.2) = \\sqrt{\\lambda}\\ln(5) $$\nThe transition width is:\n$$ w_2 = x_{90}-x_{10} = \\sqrt{\\lambda}\\ln(5) - (-\\sqrt{\\lambda}\\ln(5)) = 2\\sqrt{\\lambda}\\ln(5) $$\n\n### Part 2: $\\ell_1$ Total Variation Regularization\nWe minimize $J_1[u] = \\frac{1}{2}\\int_{-L}^{L}(u-f)^2 dx + \\lambda\\int_{-L}^{L}|u'|dx$ for a piecewise constant function $u(x) = u_{\\ell}$ for $x0$ and $u(x)=u_{r}$ for $x \\ge 0$.\nThe data term is $\\frac{1}{2}\\int_{-L}^0 (u_{\\ell}-0)^2 dx + \\frac{1}{2}\\int_0^L (u_r-A)^2 dx = \\frac{L}{2}u_{\\ell}^2 + \\frac{L}{2}(u_r-A)^2$.\nThe regularization term is the total variation, $\\lambda |u_r - u_{\\ell}|$.\nWe minimize the two-parameter function:\n$$ J_1(u_{\\ell}, u_r) = \\frac{L}{2}u_{\\ell}^2 + \\frac{L}{2}(u_r-A)^2 + \\lambda|u_r-u_{\\ell}| $$\nThis is a convex function. The minimizer must satisfy the subgradient optimality condition $0 \\in \\partial J_1$. The subgradient is:\n$$ \\partial_{u_{\\ell}} J_1 = L u_{\\ell} - \\lambda \\gamma $$\n$$ \\partial_{u_r} J_1 = L(u_r-A) + \\lambda \\gamma $$\nwhere $\\gamma \\in \\mathrm{sgn}(u_r-u_{\\ell})$. Here, $\\mathrm{sgn}(z) = z/|z|$ for $z \\neq 0$ and $\\mathrm{sgn}(0) = [-1,1]$.\nSetting the subgradient to zero: $u_{\\ell} = \\lambda\\gamma/L$ and $u_r=A-\\lambda\\gamma/L$.\nThe jump is $u_r-u_{\\ell} = A-2\\lambda\\gamma/L$.\n- For a nonzero jump $u_r-u_{\\ell}  0$, we must have $\\gamma=1$. The jump is $A-2\\lambda/L$. This being positive requires $A  2\\lambda/L$. In this case, the optimal solution is $(u_{\\ell}, u_r) = (\\lambda/L, A-\\lambda/L)$.\n- For a zero jump $u_r-u_{\\ell}=0$, we need to find $\\gamma \\in [-1,1]$ such that $A - 2\\lambda\\gamma/L = 0$, which means $\\gamma = AL/(2\\lambda)$. Since $\\gamma$ must be in $[-1,1]$, this case holds if $|AL/(2\\lambda)| \\le 1$. As $A,L,\\lambda  0$, this becomes $A \\le 2\\lambda/L$. In this case, the jump is removed, and the optimal solution is $u_{\\ell}=u_r=A/2$.\n\nThus, a nonzero jump is preserved if and only if $A  2\\lambda/L$.\n\n### Part 3: Synthesis for Minimal Amplitude $A_{\\min}$\nWe need to find the minimal $A$ for which there exists a single value of $\\lambda$ that satisfies two conditions simultaneously:\n1. The $\\ell_2$ solution is oversmoothed: $w_2 \\ge w_c$. From Part 1, this means $2\\sqrt{\\lambda}\\ln(5) \\ge w_c$. Solving for $\\lambda$:\n   $$ \\lambda \\ge \\frac{w_c^2}{4(\\ln(5))^2} $$\n2. The $\\ell_1$ solution preserves a nonzero jump. From Part 2, this means $A  2\\lambda/L$. Solving for $\\lambda$:\n   $$ \\lambda  \\frac{AL}{2} $$\nFor a value of $\\lambda$ to exist, the interval defined by these two inequalities must be non-empty. The lower bound must be strictly less than the upper bound:\n$$ \\frac{w_c^2}{4(\\ln(5))^2}  \\frac{AL}{2} $$\nWe solve this inequality for the amplitude $A$:\n$$ A  \\frac{2}{L} \\cdot \\frac{w_c^2}{4(\\ln(5))^2} = \\frac{w_c^2}{2L(\\ln(5))^2} $$\nThe set of all amplitudes $A$ for which such a $\\lambda$ exists is the open interval $(\\frac{w_c^2}{2L(\\ln(5))^2}, \\infty)$. The minimal amplitude, $A_{\\min}$, is the infimum of this set.\n$$ A_{\\min} = \\frac{w_c^2}{2L(\\ln(5))^2} $$", "answer": "$$\\boxed{\\frac{w_{c}^{2}}{2L(\\ln(5))^{2}}}$$", "id": "3580648"}, {"introduction": "The LASSO (Least Absolute Shrinkage and Selection Operator) is a cornerstone of modern sparse recovery, but how does it actually enforce sparsity? This exercise takes you under the hood of the LASSO, guiding you to derive its fundamental optimality conditions from the principles of convex analysis. Through this analysis, you will uncover the elegant mechanism that governs support recovery, relating it to the correlation between dictionary atoms and the data residual, and see how it simplifies to the intuitive soft-thresholding operator in a special case [@problem_id:3580666].", "problem": "In post-stack seismic trace inversion, a sparsity-promoting model assumes a reflectivity vector $x \\in \\mathbb{R}^{n}$ whose entries represent impedance contrasts at $n$ candidate depth samples. After wavelet deconvolution and noise prewhitening, the prewhitened data $y \\in \\mathbb{R}^{m}$ and the forward operator $A \\in \\mathbb{R}^{m \\times n}$ satisfy the linear model $y = A x + \\varepsilon$, where $\\varepsilon$ is a zero-mean noise vector with identity covariance. To recover $x$, consider the Least Absolute Shrinkage and Selection Operator (LASSO) estimator $\\hat{x}$ defined as the minimizer of the convex objective\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\;\\; \\frac{1}{2} \\|A x - y\\|_{2}^{2} + \\lambda \\|x\\|_{1},\n$$\nwhere $\\lambda  0$ is a regularization parameter controlling the sparsity of the solution.\n\nStarting from the fundamental base of convex analysis (subdifferential calculus for convex functions) and first-order optimality (Karush–Kuhn–Tucker conditions), do the following:\n\n1. Derive the necessary and sufficient optimality conditions for $\\hat{x}$ in terms of the gradient of the quadratic data term and the subdifferential of the $\\ell_{1}$-norm penalty. Your derivation must be explicit and must not assume any specialized shortcuts or pre-stated optimality formulas. Clearly state the resulting condition in both vector form and component-wise form.\n\n2. Using these optimality conditions, characterize how support recovery is determined by the interaction between the residual $r = y - A \\hat{x}$ and the columns of $A$. In particular, explain the conditions that must hold for an index to belong to the support of $\\hat{x}$ and for an index to be excluded from the support, and relate these conditions to sign consistency on the support and correlation bounds off the support. Your characterization must be derived from first principles.\n\n3. Consider a specific prewhitened seismic setting with $m = 6$ and $n = 4$ in which the columns of $A$ are orthonormal, i.e., $A^{\\top} A = I_{4}$. Let\n$$\nA = \\begin{pmatrix}\n1  0  0  0 \\\\\n0  1  0  0 \\\\\n0  0  1  0 \\\\\n0  0  0  1 \\\\\n0  0  0  0 \\\\\n0  0  0  0\n\\end{pmatrix}, \\quad\ny = \\begin{pmatrix}\n2.3 \\\\\n-0.9 \\\\\n0.4 \\\\\n-3.2 \\\\\n0.1 \\\\\n-0.2\n\\end{pmatrix}, \\quad\n\\lambda = 1.\n$$\nCompute the LASSO estimate $\\hat{x}$ for this dataset. Express your final answer as a single row vector using the $\\mathrm{pmatrix}$ environment. No rounding is required, and no physical units are needed for the reflectivity amplitudes.", "solution": "The problem asks for a three-part analysis of the LASSO estimator in the context of seismic trace inversion. The analysis must begin with a fundamental derivation of the optimality conditions, use these to characterize support recovery, and conclude with a specific calculation.\n\nThe LASSO objective function is given by:\n$$\nJ(x) = \\frac{1}{2} \\|A x - y\\|_{2}^{2} + \\lambda \\|x\\|_{1}\n$$\nWe seek the minimizer $\\hat{x} = \\arg\\min_{x \\in \\mathbb{R}^{n}} J(x)$. The objective function $J(x)$ is a sum of two convex functions: $f(x) = \\frac{1}{2} \\|A x - y\\|_{2}^{2}$ (a differentiable quadratic function, hence convex) and $g(x) = \\lambda \\|x\\|_{1}$ (a scaled norm, hence convex). The sum $J(x)$ is therefore also a convex function.\n\n**1. Derivation of Optimality Conditions**\n\nAccording to Fermat's rule for convex optimization, a point $\\hat{x}$ is a minimizer of $J(x)$ if and only if the zero vector is an element of the subdifferential of $J(x)$ at $\\hat{x}$. That is:\n$$\n0 \\in \\partial J(\\hat{x})\n$$\nFor a sum of two convex functions where one is differentiable (like $f(x)$), the subdifferential of the sum is the sum of the gradient of the differentiable part and the subdifferential of the non-differentiable part:\n$$\n\\partial J(x) = \\nabla f(x) + \\partial g(x)\n$$\nFirst, we find the gradient of $f(x)$.\n$$\nf(x) = \\frac{1}{2} (Ax - y)^{\\top}(Ax - y) = \\frac{1}{2} (x^{\\top}A^{\\top}Ax - 2y^{\\top}Ax + y^{\\top}y)\n$$\nTaking the gradient with respect to $x$ yields:\n$$\n\\nabla f(x) = \\frac{1}{2} (2A^{\\top}Ax - 2A^{\\top}y) = A^{\\top}(Ax - y)\n$$\nNext, we find the subdifferential of $g(x) = \\lambda \\|x\\|_{1} = \\lambda \\sum_{i=1}^{n} |x_i|$. The subdifferential of a sum of functions is the Cartesian product of the subdifferentials of the individual functions. Let $v \\in \\partial g(x)$. Then $v_i \\in \\partial (\\lambda |x_i|)$. The subdifferential of the absolute value function at a point $z \\in \\mathbb{R}$ is:\n$$\n\\partial|z| = \\begin{cases} \\{\\mathrm{sign}(z)\\}  \\text{if } z \\neq 0 \\\\ [-1, 1]  \\text{if } z = 0 \\end{cases}\n$$\nTherefore, for each component $v_i$ of a vector $v \\in \\partial g(x)$:\n$$\nv_i \\in \\begin{cases} \\{\\lambda \\cdot \\mathrm{sign}(x_i)\\}  \\text{if } x_i \\neq 0 \\\\ [-\\lambda, \\lambda]  \\text{if } x_i = 0 \\end{cases}\n$$\nThis vector $v$ is an element of the subdifferential $\\partial g(x)$.\n\nThe optimality condition $0 \\in \\nabla f(\\hat{x}) + \\partial g(\\hat{x})$ can be written as $-\\nabla f(\\hat{x}) \\in \\partial g(\\hat{x})$. Substituting the expressions for the gradient and subdifferential, we get:\n$$\n-A^{\\top}(A\\hat{x} - y) \\in \\lambda \\partial\\|\\hat{x}\\|_{1}\n$$\nThis is the necessary and sufficient optimality condition in vector form. Let $r = y - A\\hat{x}$ be the residual. The condition becomes:\n$$\nA^{\\top}r \\in \\lambda \\partial\\|\\hat{x}\\|_{1}\n$$\nFor the component-wise form, let $a_i$ be the $i$-th column of $A$. The $i$-th component of the vector $A^{\\top}r$ is $a_i^{\\top}r$. The optimality conditions for each component $\\hat{x}_i$ of the solution are:\n\\begin{itemize}\n    \\item If $\\hat{x}_i \\neq 0$: The subdifferential is single-valued, so we have an equality $a_i^{\\top}(y - A\\hat{x}) = \\lambda \\cdot \\mathrm{sign}(\\hat{x}_i)$.\n    \\item If $\\hat{x}_i = 0$: The subdifferential is an interval, so we have an inequality $|a_i^{\\top}(y - A\\hat{x})| \\le \\lambda$.\n\\end{itemize}\n\n**2. Characterization of Support Recovery**\n\nThe support of the solution $\\hat{x}$ is the set of indices $S = \\{i \\mid \\hat{x}_i \\neq 0\\}$. The complement of the support is $S^c = \\{i \\mid \\hat{x}_i = 0\\}$. The optimality conditions derived above provide a precise characterization of which indices belong to the support. Let $r = y - A\\hat{x}$ be the residual vector at the solution.\n\nFor an index $i$ to be in the support ($i \\in S$):\nThe condition is $a_i^{\\top}r = \\lambda \\cdot \\mathrm{sign}(\\hat{x}_i)$. This implies two things. First, the magnitude of the correlation between the basis vector $a_i$ and the final residual $r$ must be exactly equal to the regularization parameter $\\lambda$, i.e., $|a_i^{\\top}r| = \\lambda$. Second, the sign of this correlation must match the sign of the recovered coefficient $\\hat{x}_i$. This is a **sign consistency** condition. It means that the active coefficients $\\hat{x}_i$ are chosen such that the portion of the data they explain leaves a residual whose projection onto the corresponding dictionary atoms $a_i$ is saturated at the threshold $\\lambda$.\n\nFor an index $j$ to be excluded from the support ($j \\in S^c$):\nThe condition is $|a_j^{\\top}r| \\le \\lambda$. This means that the magnitude of the correlation between any inactive basis vector $a_j$ and the final residual $r$ must be less than or equal to $\\lambda$. This is a **correlation bound** for elements off the support. If this condition were violated for some $j \\in S^c$ (i.e., $|a_j^{\\top}r|  \\lambda$), the overall objective function $J(x)$ could be decreased by introducing a small non-zero coefficient $\\hat{x}_j$ with the same sign as $a_j^{\\top}r$, contradicting the presumed optimality of $\\hat{x}$.\n\nIn essence, support recovery in LASSO is a thresholding process. An index $i$ is included in the model only if its corresponding basis vector $a_i$ is sufficiently correlated with the data, after accounting for the contributions of other selected basis vectors. The parameter $\\lambda$ sets the threshold for this selection, with higher values of $\\lambda$ leading to sparser solutions.\n\n**3. Computation for the Specific Case**\n\nWe are given $m=6$, $n=4$, $\\lambda=1$, and the matrices\n$$\nA = \\begin{pmatrix}\n1  0  0  0 \\\\\n0  1  0  0 \\\\\n0  0  1  0 \\\\\n0  0  0  1 \\\\\n0  0  0  0 \\\\\n0  0  0  0\n\\end{pmatrix}, \\quad\ny = \\begin{pmatrix}\n2.3 \\\\\n-0.9 \\\\\n0.4 \\\\\n-3.2 \\\\\n0.1 \\\\\n-0.2\n\\end{pmatrix}\n$$\nA key piece of information is that the columns of $A$ are orthonormal, which we can verify: $A^{\\top}A = I_{4}$, where $I_4$ is the $4 \\times 4$ identity matrix. This simplifies the optimality condition significantly.\n\nStarting from the vector form of the optimality condition, $-A^{\\top}(A\\hat{x} - y) = s$ for some $s \\in \\lambda \\partial\\|\\hat{x}\\|_{1}$:\n$$\nA^{\\top}y - A^{\\top}A\\hat{x} = s\n$$\nSince $A^{\\top}A = I_4$, this becomes:\n$$\nA^{\\top}y - \\hat{x} = s \\quad \\implies \\quad \\hat{x} + s = A^{\\top}y\n$$\nLet's define $z = A^{\\top}y$. Each component $\\hat{x}_i$ of the solution is related to the corresponding component $z_i$ by $\\hat{x}_i + s_i = z_i$, where $s_i \\in \\lambda \\partial|\\hat{x}_i|$. This is the definition of the soft-thresholding operator, $S_{\\lambda}(z_i)$:\n$$\n\\hat{x}_i = S_{\\lambda}(z_i) = \\mathrm{sign}(z_i) \\max(|z_i| - \\lambda, 0)\n$$\nFirst, we compute the vector $z = A^{\\top}y$:\n$$\nA^{\\top} = \\begin{pmatrix}\n1  0  0  0  0  0 \\\\\n0  1  0  0  0  0 \\\\\n0  0  1  0  0  0 \\\\\n0  0  0  1  0  0\n\\end{pmatrix}\n$$\n$$\nz = A^{\\top}y = \\begin{pmatrix}\n1  0  0  0  0  0 \\\\\n0  1  0  0  0  0 \\\\\n0  0  1  0  0  0 \\\\\n0  0  0  1  0  0\n\\end{pmatrix}\n\\begin{pmatrix}\n2.3 \\\\\n-0.9 \\\\\n0.4 \\\\\n-3.2 \\\\\n0.1 \\\\\n-0.2\n\\end{pmatrix} =\n\\begin{pmatrix}\n2.3 \\\\\n-0.9 \\\\\n0.4 \\\\\n-3.2\n\\end{pmatrix}\n$$\nNow we apply the soft-thresholding operator with $\\lambda=1$ to each component of $z$.\n\nFor $i=1$: $z_1 = 2.3$. Since $|z_1|  1$, $\\hat{x}_1 = \\mathrm{sign}(2.3)(|2.3| - 1) = 1 \\cdot (2.3 - 1) = 1.3$.\nFor $i=2$: $z_2 = -0.9$. Since $|z_2| \\le 1$, $\\hat{x}_2 = \\mathrm{sign}(-0.9)\\max(|-0.9| - 1, 0) = -1 \\cdot (0) = 0$.\nFor $i=3$: $z_3 = 0.4$. Since $|z_3| \\le 1$, $\\hat{x}_3 = \\mathrm{sign}(0.4)\\max(|0.4| - 1, 0) = 1 \\cdot (0) = 0$.\nFor $i=4$: $z_4 = -3.2$. Since $|z_4|  1$, $\\hat{x}_4 = \\mathrm{sign}(-3.2)(|-3.2| - 1) = -1 \\cdot (3.2 - 1) = -2.2$.\n\nThe LASSO estimate is $\\hat{x} = (1.3, 0, 0, -2.2)^{\\top}$.", "answer": "$$\n\\boxed{\\begin{pmatrix} 1.3  0  0  -2.2 \\end{pmatrix}}\n$$", "id": "3580666"}, {"introduction": "While convex optimization methods like the LASSO provide a powerful framework for sparse recovery, computationally efficient greedy algorithms present a compelling alternative. This practice problem focuses on Orthogonal Matching Pursuit (OMP), the canonical greedy method, and challenges you to derive a key condition that guarantees its success. By linking the algorithm's performance to the mutual coherence of the seismic wavelet dictionary, you will gain insight into the crucial interplay between algorithm design and measurement properties [@problem_id:3580652].", "problem": "A one-dimensional seismic trace is modeled as a discrete linear superposition of band-limited wavelets convolved with a sparse reflectivity series. Let $A \\in \\mathbb{R}^{m \\times n}$ be a known dictionary whose columns $\\{a_{j}\\}_{j=1}^{n}$ are unit-norm atoms, each representing a shifted and scaled seismic wavelet, and let $x \\in \\mathbb{R}^{n}$ be a $k$-sparse reflectivity vector with support $S \\subset \\{1,\\dots,n\\}$, $|S|=k$. The noise-free measurement is $y = A x \\in \\mathbb{R}^{m}$. Consider Orthogonal Matching Pursuit (OMP), defined as follows: initialize the residual $r^{(0)} = y$ and the support estimate $S^{(0)} = \\emptyset$. At iteration $t \\geq 0$, select the index $j_{t} \\in \\{1,\\dots,n\\}$ that maximizes $|a_{j}^{\\top} r^{(t)}|$, update the support $S^{(t+1)} = S^{(t)} \\cup \\{j_{t}\\}$, compute the least-squares estimate on $S^{(t+1)}$, and update the residual $r^{(t+1)} = y - A_{S^{(t+1)}} \\hat{x}_{S^{(t+1)}}$.\n\nThe mutual coherence of $A$ is defined by $\\mu(A) = \\max_{i \\neq j} |a_{i}^{\\top} a_{j}|$, with all columns normalized so that $\\|a_{j}\\|_{2} = 1$. Using only these definitions and the properties of inner products and orthogonal projections, derive rigorously a sufficient condition on $\\mu(A)$, expressed as an explicit function of $k$, that guarantees OMP selects a correct atom at every iteration and exactly recovers the true support $S$ in $k$ steps under noise-free measurements. Your derivation must proceed from first principles: bound the correlations $|a_{j}^{\\top} r^{(t)}|$ for indices inside and outside the true support, compare these bounds for each iteration, and conclude an explicit $k$-dependent upper bound on $\\mu(A)$ that ensures strict separation between true and false correlations throughout the OMP run.\n\nFinally, suppose a wavelet dictionary assembled from field-calibrated source signatures has mutual coherence $\\mu(A) = 0.19$. Under the condition you derived, determine the largest integer sparsity level $k_{\\max}$ such that OMP is guaranteed to exactly recover any $k$-sparse reflectivity vector from noise-free measurements with this $A$. Report $k_{\\max}$ as an integer. No rounding specification is required because the final answer is an exact integer.", "solution": "The problem asks for two results: first, a rigorous derivation of a sufficient condition on the mutual coherence $\\mu(A)$ that guarantees Orthogonal Matching Pursuit (OMP) exactly recovers a $k$-sparse vector $x$ from noise-free measurements $y=Ax$; second, the application of this condition to find the maximum sparsity level $k_{\\max}$ for a given $\\mu(A)$.\n\nThe derivation will proceed from first principles, analyzing the correlations computed at each step of the OMP algorithm.\n\nLet $A \\in \\mathbb{R}^{m \\times n}$ be the dictionary matrix with unit-norm columns $\\{a_j\\}_{j=1}^n$. Let $x \\in \\mathbb{R}^n$ be a $k$-sparse vector with true support set $S = \\{j : x_j \\neq 0\\}$, where $|S|=k$. The noise-free measurement vector is $y = Ax = A_S x_S = \\sum_{j \\in S} x_j a_j$.\n\nThe OMP algorithm iteratively builds an estimate of the support, $S^{(t)}$, starting with $S^{(0)} = \\emptyset$. At each iteration $t \\ge 0$, it selects the index $j_t$ that maximizes the magnitude of the correlation with the current residual $r^{(t)}$, where $r^{(0)} = y$. An inductive argument will be used to show that if the condition on $\\mu(A)$ is met, the algorithm correctly identifies an element of $S$ at each step.\n\nThe condition for OMP to succeed at iteration $t$, given that all previous selections were correct (i.e., $S^{(t)} \\subset S$), is that the newly selected index $j_t$ must also be in the true support. This means the maximum correlation must correspond to an atom in $S \\setminus S^{(t)}$. Formally, we require:\n$$ \\max_{j \\in S \\setminus S^{(t)}} |a_j^{\\top} r^{(t)}|  \\max_{l \\notin S} |a_l^{\\top} r^{(t)}| $$\nThis inequality must hold for all iterations $t = 0, 1, \\dots, k-1$.\n\nLet's begin by analyzing the first iteration, $t=0$. The residual is $r^{(0)} = y = \\sum_{i \\in S} x_i a_i$.\n\nFirst, consider the correlation with a \"correct\" atom, i.e., an atom $a_j$ where $j \\in S$. The inner product is:\n$$ a_j^{\\top} r^{(0)} = a_j^{\\top} \\left( \\sum_{i \\in S} x_i a_i \\right) = x_j (a_j^{\\top} a_j) + \\sum_{i \\in S, i \\neq j} x_i (a_j^{\\top} a_i) $$\nSince the atoms are unit-norm, $a_j^{\\top} a_j = \\|a_j\\|_2^2 = 1$. Using the triangle inequality and the definition of mutual coherence, $\\mu(A) = \\max_{i \\neq j} |a_i^{\\top} a_j|$, we can establish a lower bound a \"correct\" correlation:\n$$ |a_j^{\\top} r^{(0)}| \\geq |x_j| - \\left| \\sum_{i \\in S, i \\neq j} x_i (a_j^{\\top} a_i) \\right| \\geq |x_j| - \\sum_{i \\in S, i \\neq j} |x_i| |a_j^{\\top} a_i| \\geq |x_j| - \\mu(A) \\sum_{i \\in S, i \\neq j} |x_i| $$\nTo ensure at least one correct atom is selected, we can lower bound the maximum possible correct correlation. Let $j_0 = \\arg\\max_{j \\in S} |x_j|$, so $|x_{j_0}| = \\|x_S\\|_{\\infty}$. The sum can be written in terms of the $\\ell_1$-norm of $x_S$, $\\|x_S\\|_1 = \\sum_{i \\in S} |x_i|$.\n$$ \\max_{j \\in S} |a_j^{\\top} r^{(0)}| \\geq |a_{j_0}^{\\top} r^{(0)}| \\geq \\|x_S\\|_{\\infty} - \\mu(A) (\\|x_S\\|_1 - \\|x_S\\|_{\\infty}) $$\n\nSecond, consider the correlation with an \"incorrect\" atom, i.e., an atom $a_l$ where $l \\notin S$:\n$$ a_l^{\\top} r^{(0)} = a_l^{\\top} \\left( \\sum_{i \\in S} x_i a_i \\right) = \\sum_{i \\in S} x_i (a_l^{\\top} a_i) $$\nSince $l \\notin S$, $l \\neq i$ for all $i \\in S$. An upper bound on the magnitude of any \"incorrect\" correlation is:\n$$ |a_l^{\\top} r^{(0)}| \\leq \\sum_{i \\in S} |x_i| |a_l^{\\top} a_i| \\leq \\mu(A) \\sum_{i \\in S} |x_i| = \\mu(A) \\|x_S\\|_1 $$\n\nFor OMP to succeed at the first step, we need the maximum correct correlation to be strictly greater than the maximum incorrect correlation:\n$$ \\max_{j \\in S} |a_j^{\\top} r^{(0)}|  \\max_{l \\notin S} |a_l^{\\top} r^{(0)}| $$\nA sufficient condition for this is:\n$$ \\|x_S\\|_{\\infty} - \\mu(A) (\\|x_S\\|_1 - \\|x_S\\|_{\\infty})  \\mu(A) \\|x_S\\|_1 $$\nRearranging this inequality:\n$$ \\|x_S\\|_{\\infty}  \\mu(A) \\|x_S\\|_1 + \\mu(A) (\\|x_S\\|_1 - \\|x_S\\|_{\\infty}) $$\n$$ \\|x_S\\|_{\\infty} (1 + \\mu(A))  2 \\mu(A) \\|x_S\\|_1 $$\n$$ \\frac{\\|x_S\\|_{\\infty}}{\\|x_S\\|_1}  \\frac{2 \\mu(A)}{1 + \\mu(A)} $$\nThis condition depends on the specific values in the vector $x$. However, the problem requires a condition on $\\mu(A)$ that guarantees recovery for *any* $k$-sparse vector $x$. Therefore, we must consider the worst-case scenario for $x$, which is the one that makes the inequality hardest to satisfy. The term on the left, $\\|x_S\\|_{\\infty}/\\|x_S\\|_1$, is minimized when the energy of $x$ is spread as evenly as possible among its $k$ non-zero entries. The minimum value of this ratio is $1/k$, which occurs when all $k$ non-zero entries have the same magnitude.\n\nSubstituting this worst-case ratio into the condition:\n$$ \\frac{1}{k}  \\frac{2 \\mu(A)}{1 + \\mu(A)} $$\n$$ 1 + \\mu(A)  2k \\mu(A) $$\n$$ 1  (2k - 1) \\mu(A) $$\n$$ \\mu(A)  \\frac{1}{2k - 1} $$\n\nThis establishes the condition for the first step. For the full proof, one must show this condition is sufficient for all subsequent steps. A full inductive proof is considerably more involved, as it requires bounding the effect of the orthogonal projection at each step on the correlations. However, a heuristic argument clarifies why the condition for the first step is the most stringent. At any iteration $t$, OMP attempts to find a sparse cause for the residual $r^{(t)}$. The \"true\" underlying sparse signal that gives rise to $r^{(t)}$ is related to $x_{S \\setminus S^{(t)}}$, which has a sparsity of $k-t$. The sub-problem is thus analogous to the initial problem but with a reduced sparsity. The condition would then be $\\mu(A)  1/(2(k-t)-1)$. To ensure this holds for all $t \\in \\{0, 1, \\dots, k-1\\}$, we must satisfy the condition for the largest value of the denominator, which occurs at $t=0$. Therefore, $\\mu(A)  1/(2k-1)$ is the sufficient condition for all $k$ steps.\n\nNow, we apply this derived condition to the specific case given. We have $\\mu(A) = 0.19$ and we need to find the largest integer $k_{\\max}$ such that the condition holds.\n$$ 0.19  \\frac{1}{2k_{\\max} - 1} $$\nWe can solve for $k_{\\max}$:\n$$ 2k_{\\max} - 1  \\frac{1}{0.19} $$\n$$ 2k_{\\max}  1 + \\frac{1}{0.19} $$\n$$ k_{\\max}  \\frac{1}{2} \\left( 1 + \\frac{1}{0.19} \\right) $$\nCalculating the numerical value:\n$$ \\frac{1}{0.19} = \\frac{100}{19} \\approx 5.26315\\dots $$\n$$ k_{\\max}  \\frac{1}{2} (1 + 5.26315\\dots) = \\frac{6.26315\\dots}{2} = 3.13157\\dots $$\nSince $k_{\\max}$ must be an integer, the largest integer value for $k_{\\max}$ that satisfies this inequality is $3$.\n\nTo verify:\nIf $k=3$, the condition is $\\mu(A)  1/(2 \\cdot 3 - 1) = 1/5 = 0.2$. Since $0.19  0.2$, the condition holds.\nIf $k=4$, the condition is $\\mu(A)  1/(2 \\cdot 4 - 1) = 1/7 \\approx 0.1428$. Since $0.19 \\not 0.1428$, the condition fails.\nThus, the largest integer sparsity level is indeed $3$.", "answer": "$$\\boxed{3}$$", "id": "3580652"}]}