{"hands_on_practices": [{"introduction": "The foundation of any Physics-Informed Neural Network lies in its loss function, which encodes the governing physical laws as a trainable objective. This first practice walks you through the essential process of constructing this loss function for a canonical problem in geophysics: the Poisson equation [@problem_id:3430996]. By formulating a mean-squared error loss that penalizes both the PDE residual in the domain's interior and the violation of Dirichlet boundary conditions, you will master the core mechanism that enables neural networks to approximate solutions to partial differential equations.", "problem": "Consider Poisson’s equation on the square domain $\\Omega = [0,1]^{2}$ with homogeneous Dirichlet boundary conditions,\n$$\n-\\Delta u = f \\quad \\text{in } \\Omega, \n\\qquad \nu = 0 \\quad \\text{on } \\partial \\Omega,\n$$\nwhere $u:\\mathbb{R}^{2}\\to\\mathbb{R}$ and $f:\\Omega\\to\\mathbb{R}$ is a given source term. Let $u_{\\theta}:\\mathbb{R}^{2}\\to\\mathbb{R}$ be a sufficiently smooth neural network ansatz with parameters $\\theta$. In a Physics-Informed Neural Network (PINN), the strong-form residual is constructed by applying the governing differential operator to $u_{\\theta}$ and subtracting the known right-hand side. You are given two sets of collocation points: interior points $\\{(x_{i},y_{i})\\}_{i=1}^{M}\\subset \\Omega^{\\circ}$ and boundary points $\\{(x^{(b)}_{j},y^{(b)}_{j})\\}_{j=1}^{N}\\subset \\partial\\Omega$. Derivatives of $u_{\\theta}$ with respect to $x$ and $y$ may be obtained by automatic differentiation.\n\nStarting from the method of weighted residuals and the least-squares principle, and without assuming any special structure beyond the definitions above, derive:\n\n1. The explicit strong-form residual $r_{\\theta}(x,y)$ for the Poisson problem in terms of $u_{\\theta}$, its partial derivatives, and $f$.\n2. A mean-squared empirical collocation loss that penalizes both the interior residual and the boundary condition violation. Introduce a positive boundary weighting parameter $\\lambda_{b}>0$ and use uniform weights over the given collocation sets.\n\nExpress your final answer as a single closed-form analytic expression for the total loss $L(\\theta)$ to be minimized, written in terms of $u_{\\theta}$, $f$, the point sets, $M$, $N$, and $\\lambda_{b}$. Do not compute any numerical value. Your final answer must be a single analytic expression and not an inequality or an equation to be solved.", "solution": "**Part 1: Derive the strong-form residual** $r_{\\theta}(x,y)$.\n\nThe governing partial differential equation is given as:\n$-\\Delta u = f$\n\nThis can be rewritten as:\n$\\Delta u + f = 0$\n\nHere, $\\Delta$ is the Laplacian operator. In Cartesian coordinates $(x,y)$, $\\Delta u = \\frac{\\partial^2 u}{\\partial x^2} + \\frac{\\partial^2 u}{\\partial y^2}$.\n\nThe strong-form residual, $r_{\\theta}(x,y)$, is the expression that should be zero when the exact solution $u$ is plugged into the PDE. For the neural network ansatz $u_{\\theta}(x,y)$, the residual is obtained by substituting $u$ with $u_{\\theta}$. Let's call this $r_{\\theta}(x,y)$.\n$r_{\\theta}(x,y) = \\Delta u_{\\theta}(x,y) + f(x,y)$\n\nExpanding the Laplacian:\n$r_{\\theta}(x,y) = \\frac{\\partial^2 u_{\\theta}}{\\partial x^2}(x,y) + \\frac{\\partial^2 u_{\\theta}}{\\partial y^2}(x,y) + f(x,y)$.\n\nThis is the explicit strong-form residual for the interior of the domain.\n\n**Part 2: Derive the mean-squared empirical collocation loss** $L(\\theta)$.\n\nThe problem specifies the method of weighted residuals and the least-squares principle.\nThe total loss $L(\\theta)$ is a weighted sum of the loss from the PDE interior and the loss from the boundary conditions.\nLet's denote them as $L_{PDE}(\\theta)$ and $L_{BC}(\\theta)$.\nSo, $L(\\theta) = L_{PDE}(\\theta) + \\lambda_b L_{BC}(\\theta)$, where $\\lambda_b > 0$ is the given boundary weighting parameter.\n\n**Interior Loss (PDE Loss):** $L_{PDE}(\\theta)$\n\nThe least-squares principle applied to the interior residual means we want to minimize the integral of the squared residual over the domain: $\\int_{\\Omega} [r_{\\theta}(x,y)]^2 \\,dx\\,dy$.\nIn a PINN, this integral is approximated by a Monte Carlo estimate using the set of interior collocation points $\\{(x_i, y_i)\\}_{i=1}^{M}$.\nThe problem states to use \"uniform weights\". This corresponds to a standard mean-squared error formulation.\nThe mean-squared empirical loss for the interior is the average of the squared residuals evaluated at the $M$ interior points.\n\n$L_{PDE}(\\theta) = \\frac{1}{M} \\sum_{i=1}^{M} [r_{\\theta}(x_i, y_i)]^2$\n\nSubstituting the expression for $r_{\\theta}$:\n$L_{PDE}(\\theta) = \\frac{1}{M} \\sum_{i=1}^{M} \\left( \\frac{\\partial^2 u_{\\theta}}{\\partial x^2}(x_i, y_i) + \\frac{\\partial^2 u_{\\theta}}{\\partial y^2}(x_i, y_i) + f(x_i, y_i) \\right)^2$\n\nThe problem statement has $-\\Delta u = f$, which means $\\Delta u + f = 0$. The residual can be defined as $r_{\\theta}(x,y) = -\\Delta u_{\\theta}(x,y) - f(x,y)$. The squared residual is $(-\\Delta u_\\theta - f)^2 = (\\Delta u_\\theta + f)^2$, so the sign inside the square doesn't matter for the loss value.\n\n**Boundary Loss (BC Loss):** $L_{BC}(\\theta)$\n\nThe boundary condition is $u=0$ on $\\partial\\Omega$.\nThe \"violation\" or \"residual\" on the boundary for the ansatz $u_{\\theta}$ is simply $u_{\\theta}(x,y)$ itself, for $(x,y) \\in \\partial\\Omega$.\nThe boundary points are given as $\\{(x^{(b)}_{j}, y^{(b)}_{j})\\}_{j=1}^{N} \\subset \\partial\\Omega$.\n\nUsing the least-squares principle and uniform weights, the mean-squared empirical loss for the boundary condition is the average of the squared violations at the $N$ boundary points.\n$L_{BC}(\\theta) = \\frac{1}{N} \\sum_{j=1}^{N} \\left( u_{\\theta}(x^{(b)}_{j}, y^{(b)}_{j}) - 0 \\right)^2$\n$L_{BC}(\\theta) = \\frac{1}{N} \\sum_{j=1}^{N} \\left( u_{\\theta}(x^{(b)}_{j}, y^{(b)}_{j}) \\right)^2$\n\n**Total Loss:** $L(\\theta)$\n\nNow, I combine the two parts to form the total loss function.\n$L(\\theta) = L_{PDE}(\\theta) + \\lambda_b L_{BC}(\\theta)$\n\n$L(\\theta) = \\frac{1}{M} \\sum_{i=1}^{M} \\left( \\frac{\\partial^2 u_{\\theta}}{\\partial x^2}(x_i, y_i) + \\frac{\\partial^2 u_{\\theta}}{\\partial y^2}(x_i, y_i) + f(x_i, y_i) \\right)^2 + \\lambda_b \\frac{1}{N} \\sum_{j=1}^{N} \\left( u_{\\theta}(x^{(b)}_{j}, y^{(b)}_{j}) \\right)^2$", "answer": "$$ \\boxed{ \\frac{1}{M} \\sum_{i=1}^{M} \\left( \\frac{\\partial^2 u_{\\theta}}{\\partial x^2}(x_i, y_i) + \\frac{\\partial^2 u_{\\theta}}{\\partial y^2}(x_i, y_i) + f(x_i, y_i) \\right)^2 + \\frac{\\lambda_b}{N} \\sum_{j=1}^{N} \\left( u_{\\theta}(x^{(b)}_{j}, y^{(b)}_{j}) \\right)^2 } $$", "id": "3430996"}, {"introduction": "While penalizing boundary mismatches in the loss function is a flexible approach, it is not the only one. An elegant alternative is to enforce certain conditions \"by construction,\" a technique often referred to as using hard constraints [@problem_id:3612747]. This exercise explores how to design a trial solution by multiplying the neural network output with a function that is zero on the domain boundary, thereby guaranteeing the solution satisfies the homogeneous Dirichlet condition exactly. Mastering this method provides a powerful strategy to improve training stability and eliminate the need to balance a boundary loss term.", "problem": "Consider a two-dimensional elliptic boundary-value problem on the unit square domain $\\Omega=[0,1]^2$ with homogeneous Dirichlet boundary condition $u=0$ on $\\partial\\Omega$. In a Physics-Informed Neural Network (PINN) approach, one common strategy to enforce the boundary condition exactly is to represent the trial solution as $\\tilde u_{\\theta}(x,y)=g(x,y)\\,n_{\\theta}(x,y)$, where $n_{\\theta}$ is a neural network with trainable parameters $\\theta$, and $g$ is a boundary distance function that vanishes on $\\partial\\Omega$. Let $g(x,y)=x(1-x)\\,y(1-y)$.\n\nStarting only from core definitions and rules of multivariable calculus and boundary conditions, do the following: \n- Justify that the representation $\\tilde u_{\\theta}(x,y)=g(x,y)\\,n_{\\theta}(x,y)$ enforces the homogeneous Dirichlet boundary condition exactly on $\\partial\\Omega$.\n- Compute the gradient $\\nabla g(x,y)$ explicitly in terms of $x$ and $y$.\n\nProvide your final answer as the two-component row matrix for $\\nabla g(x,y)$. No rounding is required, and no physical units are involved. Express all intermediate and final mathematical expressions using LaTeX notation.", "solution": "The problem asks for two items: first, a justification that the given trial solution enforces the homogeneous Dirichlet boundary condition, and second, the computation of the gradient of the function $g(x,y)$.\n\nFirst, we address the justification. The domain is the unit square, defined as $\\Omega = [0,1]^2 = \\{(x,y) | 0 \\le x \\le 1, 0 \\le y \\le 1\\}$. The boundary of this domain, denoted $\\partial\\Omega$, consists of the four line segments where $x=0$, $x=1$, $y=0$, or $y=1$. A homogeneous Dirichlet boundary condition requires that the solution $u$ is zero on this boundary, i.e., $u(x,y) = 0$ for all $(x,y) \\in \\partial\\Omega$.\n\nThe proposed trial solution is of the form $\\tilde{u}_{\\theta}(x,y) = g(x,y) \\, n_{\\theta}(x,y)$, where the function $g(x,y)$ is given by $g(x,y) = x(1-x)y(1-y)$. The term $n_{\\theta}(x,y)$ represents the output of a neural network, which is assumed to be a well-defined, finite-valued function for any input $(x,y)$ in the domain $\\Omega$.\n\nTo verify that the boundary condition $\\tilde{u}_{\\theta}|_{\\partial\\Omega} = 0$ is satisfied, we must evaluate the function $g(x,y)$ on the boundary $\\partial\\Omega$. The boundary consists of four parts:\n$1$. The segment where $x=0$ and $0 \\le y \\le 1$. On this segment, the function $g(x,y)$ becomes:\n$$g(0,y) = (0)(1-0)y(1-y) = 0$$\n$2$. The segment where $x=1$ and $0 \\le y \\le 1$. On this segment, the function $g(x,y)$ becomes:\n$$g(1,y) = (1)(1-1)y(1-y) = (1)(0)y(1-y) = 0$$\n$3$. The segment where $y=0$ and $0 \\le x \\le 1$. On this segment, the function $g(x,y)$ becomes:\n$$g(x,0) = x(1-x)(0)(1-0) = 0$$\n$4$. The segment where $y=1$ and $0 \\le x \\le 1$. On this segment, the function $g(x,y)$ becomes:\n$$g(x,1) = x(1-x)(1)(1-1) = x(1-x)(1)(0) = 0$$\n\nIn all four cases, for any point $(x,y)$ on the boundary $\\partial\\Omega$, the function $g(x,y)$ evaluates to $0$. Consequently, the trial solution $\\tilde{u}_{\\theta}(x,y)$ on the boundary is:\n$$\\tilde{u}_{\\theta}(x,y)|_{\\partial\\Omega} = g(x,y)|_{\\partial\\Omega} \\cdot n_{\\theta}(x,y)|_{\\partial\\Omega} = 0 \\cdot n_{\\theta}(x,y)|_{\\partial\\Omega} = 0$$\nThis holds regardless of the value of the neural network output $n_{\\theta}(x,y)$, as long as it is finite. Thus, the representation $\\tilde{u}_{\\theta}(x,y) = g(x,y) n_{\\theta}(x,y)$ exactly enforces the homogeneous Dirichlet boundary condition on $\\partial\\Omega$ by construction.\n\nSecond, we compute the gradient of $g(x,y)$. The gradient of a scalar function $g(x,y)$ is a vector field defined as $\\nabla g(x,y) = \\left( \\frac{\\partial g}{\\partial x}, \\frac{\\partial g}{\\partial y} \\right)$. The function is $g(x,y) = x(1-x)y(1-y)$. For easier differentiation, we can write this as $g(x,y) = (x-x^2)(y-y^2)$.\n\nWe compute the partial derivative with respect to $x$, treating $y$ as a constant:\n$$\\frac{\\partial g}{\\partial x} = \\frac{\\partial}{\\partial x} \\left[ (x-x^2)(y-y^2) \\right] = (y-y^2) \\frac{\\partial}{\\partial x} (x-x^2) = (y-y^2)(1-2x)$$\nRe-substituting $y-y^2 = y(1-y)$, we get:\n$$\\frac{\\partial g}{\\partial x} = (1-2x)y(1-y)$$\n\nNext, we compute the partial derivative with respect to $y$, treating $x$ as a constant:\n$$\\frac{\\partial g}{\\partial y} = \\frac{\\partial}{\\partial y} \\left[ (x-x^2)(y-y^2) \\right] = (x-x^2) \\frac{\\partial}{\\partial y} (y-y^2) = (x-x^2)(1-2y)$$\nRe-substituting $x-x^2 = x(1-x)$, we get:\n$$\\frac{\\partial g}{\\partial y} = x(1-x)(1-2y)$$\n\nCombining these two components, the gradient vector is:\n$$\\nabla g(x,y) = \\left( (1-2x)y(1-y), x(1-x)(1-2y) \\right)$$\nThe problem requires the answer as a two-component row matrix.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n(1-2x)y(1-y) & x(1-x)(1-2y)\n\\end{pmatrix}\n}\n$$", "id": "3612747"}, {"introduction": "Successfully training a PINN for complex geophysical phenomena often requires more than just formulating the loss; it requires balancing its constituent parts. When a problem involves multiple physical processes operating at vastly different scales, such as in advection-diffusion systems with high Péclet numbers, a naive loss function can lead to imbalanced gradients and failed training [@problem_id:3431051]. This practice introduces the powerful technique of nondimensionalization to derive a principled weighting strategy for the different terms in your loss function. By ensuring all loss components are of a comparable order of magnitude, you will learn a critical skill for developing robust and effective PINNs for real-world, multi-scale applications.", "problem": "Consider the advection–diffusion partial differential equation $u_t + a u_x = \\nu u_{xx}$ for a scalar field $u(x,t)$ on a one-dimensional spatial interval $x \\in [0,L]$ and time $t \\geq 0$, where $a$ is the advection speed, $\\nu$ is the diffusivity, and $L$ is the characteristic length. A Physics-Informed Neural Network (PINN) is trained by minimizing a composite loss consisting of a mean-squared residual of the partial differential equation and mean-squared mismatches at boundaries and initial conditions. In many practical implementations, the residual is computed in physical units without nondimensionalization, which can lead to imbalanced gradients and training bias, especially at large Péclet number $\\mathrm{Pe} = aL/\\nu$. The task is to derive a principled choice of loss weights for the partial differential equation residual versus boundary condition terms based on nondimensionalization, so that all terms in the loss are dimensionless and of comparable magnitude. The goal is to avoid bias toward the diffusion term or the advection term purely due to unit scaling rather than physical relevance.\n\nStarting from fundamental principles, carry out a nondimensionalization of the advection–diffusion equation using the advection time scale $T = L/a$ and a characteristic field scale $U$ for $u$, so that $x = L \\hat{x}$, $t = T \\hat{t}$, and $u = U \\hat{u}$. Use the chain rule to express the dimensional derivatives in terms of the nondimensional derivatives, and derive the nondimensional form of the equation. From this, determine the natural scale of the dimensional residual $r = u_t + a u_x - \\nu u_{xx}$ and of the boundary mismatches for Dirichlet and Neumann boundary conditions, and propose dimensionless mean-squared loss weights $w_{\\mathrm{pde}}^{\\mathrm{mean}}$, $w_{\\mathrm{bc,dir}}^{\\mathrm{mean}}$, and $w_{\\mathrm{bc,neu}}^{\\mathrm{mean}}$ that make the mean-squared losses dimensionless and of order one under the assumption that the nondimensional residual and mismatches are of order one. Explicitly state how to adjust these weights when losses are computed as a sum of squared errors rather than a mean (i.e., provide $w_{\\mathrm{pde}}^{\\mathrm{sum}}$, $w_{\\mathrm{bc,dir}}^{\\mathrm{sum}}$, and $w_{\\mathrm{bc,neu}}^{\\mathrm{sum}}$ as functions of the number of samples for each term).\n\nYour program must implement the derived formulas and compute, for each test case, the following outputs:\n- The Péclet number $\\mathrm{Pe}$.\n- The recommended mean-squared weights $w_{\\mathrm{pde}}^{\\mathrm{mean}}$, $w_{\\mathrm{bc,dir}}^{\\mathrm{mean}}$, and $w_{\\mathrm{bc,neu}}^{\\mathrm{mean}}$.\n- The corresponding sum-of-squares weights $w_{\\mathrm{pde}}^{\\mathrm{sum}}$, $w_{\\mathrm{bc,dir}}^{\\mathrm{sum}}$, and $w_{\\mathrm{bc,neu}}^{\\mathrm{sum}}$.\n\nBoundary condition types vary across test cases. If a boundary condition type is absent in a given test case, you must output $0.0$ for its weights. All weights are dimensionless and must be output as floats. No physical units are required in the output because the weights are dimensionless by construction. For completeness, the physical parameters in the test suite use the following units: $a$ in meters per second, $\\nu$ in square meters per second, $L$ in meters, and $U$ in units of $u$ consistent with the chosen problem.\n\nTest suite:\n1. Test case A (large $\\mathrm{Pe}$, Dirichlet only): $a = 1.0$ m/s, $L = 1.0$ m, $\\nu = 10^{-3}$ m$^2$/s, $U = 1.0$, number of partial differential equation collocation points $N_f = 10000$, number of Dirichlet boundary points $N_{\\mathrm{b,dir}} = 200$, number of Neumann boundary points $N_{\\mathrm{b,neu}} = 0$.\n2. Test case B (moderate $\\mathrm{Pe}$, mixed Dirichlet and Neumann): $a = 0.5$ m/s, $L = 1.0$ m, $\\nu = 0.1$ m$^2$/s, $U = 2.0$, $N_f = 5000$, $N_{\\mathrm{b,dir}} = 60$, $N_{\\mathrm{b,neu}} = 40$.\n3. Test case C (extremely large $\\mathrm{Pe}$, Neumann only): $a = 2.0$ m/s, $L = 2.0$ m, $\\nu = 10^{-5}$ m$^2$/s, $U = 1.0$, $N_f = 20000$, $N_{\\mathrm{b,dir}} = 0$, $N_{\\mathrm{b,neu}} = 400$.\n4. Test case D (large $\\mathrm{Pe}$ with small domain, mixed): $a = 1.0$ m/s, $L = 0.1$ m, $\\nu = 10^{-3}$ m$^2$/s, $U = 0.5$, $N_f = 1000$, $N_{\\mathrm{b,dir}} = 50$, $N_{\\mathrm{b,neu}} = 50$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case and is itself a list of seven floats ordered as $[\\mathrm{Pe}, w_{\\mathrm{pde}}^{\\mathrm{mean}}, w_{\\mathrm{bc,dir}}^{\\mathrm{mean}}, w_{\\mathrm{bc,neu}}^{\\mathrm{mean}}, w_{\\mathrm{pde}}^{\\mathrm{sum}}, w_{\\mathrm{bc,dir}}^{\\mathrm{sum}}, w_{\\mathrm{bc,neu}}^{\\mathrm{sum}}]$.", "solution": "The derivation proceeds as follows.\n\nThe one-dimensional advection-diffusion partial differential equation (PDE) is given by:\n$$\nu_t + a u_x = \\nu u_{xx}\n$$\nwhere $u(x,t)$ is the scalar field, $a$ is the advection speed, $\\nu$ is the diffusivity, $x \\in [0,L]$ is the spatial coordinate, and $t \\ge 0$ is time.\n\nThe first step is to nondimensionalize the equation. We introduce the following dimensionless variables, denoted with a hat ($\\hat{\\cdot}$):\n$$\nx = L \\hat{x}, \\quad t = T \\hat{t}, \\quad u = U \\hat{u}\n$$\nThe problem specifies using the advection time scale, $T = L/a$. The variables $L$ and $U$ represent the characteristic length and field scales, respectively.\n\nUsing the chain rule, we express the dimensional derivatives in terms of the dimensionless ones:\nThe time derivative $u_t$:\n$$\nu_t = \\frac{\\partial u}{\\partial t} = \\frac{\\partial (U \\hat{u})}{\\partial \\hat{t}} \\frac{\\partial \\hat{t}}{\\partial t} = U \\frac{\\partial \\hat{u}}{\\partial \\hat{t}} \\frac{1}{T} = \\frac{U}{T} \\hat{u}_{\\hat{t}} = \\frac{U a}{L} \\hat{u}_{\\hat{t}}\n$$\nThe first spatial derivative $u_x$:\n$$\nu_x = \\frac{\\partial u}{\\partial x} = \\frac{\\partial (U \\hat{u})}{\\partial \\hat{x}} \\frac{\\partial \\hat{x}}{\\partial x} = U \\frac{\\partial \\hat{u}}{\\partial \\hat{x}} \\frac{1}{L} = \\frac{U}{L} \\hat{u}_{\\hat{x}}\n$$\nThe second spatial derivative $u_{xx}$:\n$$\nu_{xx} = \\frac{\\partial^2 u}{\\partial x^2} = \\frac{\\partial}{\\partial x} \\left( \\frac{U}{L} \\hat{u}_{\\hat{x}} \\right) = \\frac{U}{L} \\frac{\\partial \\hat{u}_{\\hat{x}}}{\\partial \\hat{x}} \\frac{\\partial \\hat{x}}{\\partial x} = \\frac{U}{L^2} \\hat{u}_{\\hat{x}\\hat{x}}\n$$\n\nSubstituting these expressions back into the original PDE:\n$$\n\\left( \\frac{U a}{L} \\hat{u}_{\\hat{t}} \\right) + a \\left( \\frac{U}{L} \\hat{u}_{\\hat{x}} \\right) = \\nu \\left( \\frac{U}{L^2} \\hat{u}_{\\hat{x}\\hat{x}} \\right)\n$$\nTo obtain the canonical dimensionless form, we divide the entire equation by the coefficient of the advection terms, $\\frac{Ua}{L}$:\n$$\n\\frac{U a}{L} \\left( \\hat{u}_{\\hat{t}} + \\hat{u}_{\\hat{x}} \\right) = \\frac{\\nu U}{L^2} \\hat{u}_{\\hat{x}\\hat{x}}\n$$\n$$\n\\hat{u}_{\\hat{t}} + \\hat{u}_{\\hat{x}} = \\frac{\\nu U}{L^2} \\left( \\frac{L}{U a} \\right) \\hat{u}_{\\hat{x}\\hat{x}}\n$$\n$$\n\\hat{u}_{\\hat{t}} + \\hat{u}_{\\hat{x}} = \\frac{\\nu}{aL} \\hat{u}_{\\hat{x}\\hat{x}}\n$$\nWe recognize the dimensionless Péclet number, $\\mathrm{Pe} = \\frac{aL}{\\nu}$, which represents the ratio of advective to diffusive transport rates. The nondimensional advection-diffusion equation is:\n$$\n\\hat{u}_{\\hat{t}} + \\hat{u}_{\\hat{x}} = \\frac{1}{\\mathrm{Pe}} \\hat{u}_{\\hat{x}\\hat{x}}\n$$\nThe PINN loss function consists of weighted mean-squared errors for the PDE residual and the boundary/initial conditions. The goal is to choose weights such that each term in the total loss is dimensionless and of order one, assuming the underlying nondimensional errors are of order one.\n\nThe PDE residual is defined as $r = u_t + a u_x - \\nu u_{xx}$. Its corresponding nondimensional residual is $\\hat{r} = \\hat{u}_{\\hat{t}} + \\hat{u}_{\\hat{x}} - \\frac{1}{\\mathrm{Pe}} \\hat{u}_{\\hat{x}\\hat{x}}$. The relationship between them is:\n$$\nr = \\frac{Ua}{L} \\left( \\hat{u}_{\\hat{t}} + \\hat{u}_{\\hat{x}} - \\frac{\\nu}{aL} \\hat{u}_{\\hat{x}\\hat{x}} \\right) = \\frac{Ua}{L} \\hat{r}\n$$\nAssuming the nondimensional residual $\\hat{r}$ is of order one, $\\mathcal{O}(1)$, the characteristic scale of the dimensional residual $r$ is $S_{\\mathrm{pde}} = \\frac{Ua}{L}$. The mean-squared PDE loss is $L_{\\mathrm{pde}}^{\\mathrm{mean}} = \\frac{1}{N_f} \\sum_i r_i^2$. To make the weighted loss $w_{\\mathrm{pde}}^{\\mathrm{mean}} L_{\\mathrm{pde}}^{\\mathrm{mean}}$ dimensionless and $\\mathcal{O}(1)$, the weight $w_{\\mathrm{pde}}^{\\mathrm{mean}}$ must be the inverse square of the characteristic scale:\n$$\nw_{\\mathrm{pde}}^{\\mathrm{mean}} = \\frac{1}{S_{\\mathrm{pde}}^2} = \\left(\\frac{L}{Ua}\\right)^2\n$$\n\nFor the boundary conditions, we analyze the scale of the mismatch.\nFor a Dirichlet boundary condition, the mismatch is of the form $u - g$, where $g$ is the prescribed boundary value. The characteristic scale of $u$ is $U$. Thus, the mismatch scale is $S_{\\mathrm{bc,dir}} = U$. The corresponding mean-squared loss weight is:\n$$\nw_{\\mathrm{bc,dir}}^{\\mathrm{mean}} = \\frac{1}{S_{\\mathrm{bc,dir}}^2} = \\frac{1}{U^2}\n$$\nFor a Neumann boundary condition, the mismatch is of the form $u_x - h$, where $h$ is the prescribed boundary gradient. From our earlier derivation, the characteristic scale of $u_x$ is $\\frac{U}{L}$. Thus, the mismatch scale is $S_{\\mathrm{bc,neu}} = \\frac{U}{L}$. The corresponding mean-squared loss weight is:\n$$\nw_{\\mathrm{bc,neu}}^{\\mathrm{mean}} = \\frac{1}{S_{\\mathrm{bc,neu}}^2} = \\left(\\frac{L}{U}\\right)^2\n$$\n\nIf a loss is computed as a sum of squared errors (SSE) instead of a mean squared error (MSE), the relationship is $L^{\\mathrm{sum}} = N L^{\\mathrm{mean}}$, where $N$ is the number of sample points. To ensure the weighted loss remains the same, $w^{\\mathrm{sum}} L^{\\mathrm{sum}} = w^{\\mathrm{mean}} L^{\\mathrm{mean}}$, the weights must be adjusted:\n$$\nw^{\\mathrm{sum}} = \\frac{w^{\\mathrm{mean}}}{N}\n$$\nThis gives the following weights for the sum-of-squares losses:\n$$\nw_{\\mathrm{pde}}^{\\mathrm{sum}} = \\frac{w_{\\mathrm{pde}}^{\\mathrm{mean}}}{N_f} = \\frac{1}{N_f} \\left(\\frac{L}{Ua}\\right)^2\n$$\n$$\nw_{\\mathrm{bc,dir}}^{\\mathrm{sum}} = \\frac{w_{\\mathrm{bc,dir}}^{\\mathrm{mean}}}{N_{\\mathrm{b,dir}}} = \\frac{1}{N_{\\mathrm{b,dir}}} \\frac{1}{U^2}\n$$\n$$\nw_{\\mathrm{bc,neu}}^{\\mathrm{sum}} = \\frac{w_{\\mathrm{bc,neu}}^{\\mathrm{mean}}}{N_{\\mathrm{b,neu}}} = \\frac{1}{N_{\\mathrm{b,neu}}} \\left(\\frac{L}{U}\\right)^2\n$$\nIf a particular boundary condition type is not present for a given problem (i.e., its number of points is $0$), its corresponding loss term does not exist, and its weights should be reported as $0.0$.\nThese formulas will be implemented to compute the required values for the given test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy import ...\n\ndef solve():\n    \"\"\"\n    Computes principled loss weights for PINN training of the advection-diffusion equation\n    based on nondimensionalization for a suite of test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Format: (a, L, nu, U, N_f, N_b_dir, N_b_neu)\n    test_cases = [\n        # Test case A\n        (1.0, 1.0, 1e-3, 1.0, 10000, 200, 0),\n        # Test case B\n        (0.5, 1.0, 0.1, 2.0, 5000, 60, 40),\n        # Test case C\n        (2.0, 2.0, 1e-5, 1.0, 20000, 0, 400),\n        # Test case D\n        (1.0, 0.1, 1e-3, 0.5, 1000, 50, 50),\n    ]\n\n    results = []\n    for case in test_cases:\n        a, L, nu, U, N_f, N_b_dir, N_b_neu = case\n\n        # 1. Calculate Péclet number\n        pe = (a * L) / nu\n\n        # 2. Calculate mean-squared weights\n        # w_pde_mean = (L / (U * a))^2\n        # This can be simplified to 1 / ( (U*a/L)^2 )\n        # To avoid potential division by zero if a or U are zero,\n        # although they are not in the test cases.\n        if U == 0.0 or a == 0.0:\n            w_pde_mean = 0.0 # Or handle as an error, but 0 follows the logic for absent terms.\n        else:\n            w_pde_mean = (L / (U * a))**2\n\n        # w_bc,dir_mean = 1 / U^2\n        if N_b_dir == 0 or U == 0.0:\n            w_bc_dir_mean = 0.0\n        else:\n            w_bc_dir_mean = 1.0 / (U**2)\n\n        # w_bc,neu_mean = (L / U)^2\n        if N_b_neu == 0 or U == 0.0:\n            w_bc_neu_mean = 0.0\n        else:\n            w_bc_neu_mean = (L / U)**2\n\n        # 3. Calculate sum-of-squares weights\n        # w_pde_sum = w_pde_mean / N_f\n        if N_f == 0:\n            w_pde_sum = 0.0\n        else:\n            w_pde_sum = w_pde_mean / N_f\n\n        # w_bc,dir_sum = w_bc,dir_mean / N_b_dir\n        if N_b_dir == 0:\n            w_bc_dir_sum = 0.0\n        else:\n            w_bc_dir_sum = w_bc_dir_mean / N_b_dir\n\n        # w_bc,neu_sum = w_bc,neu_mean / N_b_neu\n        if N_b_neu == 0:\n            w_bc_neu_sum = 0.0\n        else:\n            w_bc_neu_sum = w_bc_neu_mean / N_b_neu\n\n        # Collect the results for this case\n        case_results = [\n            pe,\n            w_pde_mean,\n            w_bc_dir_mean,\n            w_bc_neu_mean,\n            w_pde_sum,\n            w_bc_dir_sum,\n            w_bc_neu_sum,\n        ]\n        results.append(f\"[{','.join(map(str, case_results))}]\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3431051"}]}