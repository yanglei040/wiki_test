{"hands_on_practices": [{"introduction": "To begin, we solidify the fundamental mechanics of the Simulated Annealing process. This first exercise focuses on the atomic step of the algorithm: calculating the change in an objective function and using the Metropolis criterion to determine the acceptance probability for a proposed model update [@problem_id:3614462]. Mastering this core calculation is the essential first step toward understanding and implementing the entire SA method.", "problem": "A single straight seismic ray traverses two homogeneous cells in a horizontally layered computational domain. The predicted travel time for model $\\mathbf{m}$ with cell velocities $\\{v_{1},v_{2}\\}$ and path lengths $\\{\\ell_{1},\\ell_{2}\\}$ is given by the fundamental kinematic relation $t(\\mathbf{m})=\\sum_{i=1}^{2}\\ell_{i}/v_{i}$. A proposed model $\\mathbf{m}'$ modifies the cell velocities to $\\{v_{1}',v_{2}'\\}$. The change in predicted travel time is defined by $\\Delta t \\equiv t(\\mathbf{m}')-t(\\mathbf{m})$.\n\nAssume a single observed datum $d$ corrupted by independent, zero-mean Gaussian noise with standard deviation $\\sigma$, so that the likelihood is proportional to $\\exp\\!\\left(-\\frac{(t(\\mathbf{m})-d)^{2}}{2\\sigma^{2}}\\right)$. In simulated annealing using the Metropolis rule at temperature $T$, define the energy as the negative log-likelihood $E(\\mathbf{m})=\\frac{(t(\\mathbf{m})-d)^{2}}{2\\sigma^{2}}$. The acceptance probability for the proposal $\\mathbf{m}\\to\\mathbf{m}'$ is $p_{\\mathrm{acc}}=\\min\\!\\left(1,\\exp\\!\\left(-\\frac{\\Delta E}{T}\\right)\\right)$, where $\\Delta E=E(\\mathbf{m}')-E(\\mathbf{m})$. Assume the current model $\\mathbf{m}$ exactly fits the datum, so that $t(\\mathbf{m})-d=0$.\n\nGiven $\\ell_{1}=3\\,\\mathrm{km}$, $\\ell_{2}=2\\,\\mathrm{km}$, $(v_{1},v_{2})=(2,4)\\,\\mathrm{km/s}$, $(v_{1}',v_{2}')=(2.2,3.8)\\,\\mathrm{km/s}$, $\\sigma=0.05\\,\\mathrm{s}$, and $T=0.5$, do the following:\n\n- Compute the exact $\\Delta t=\\ell_{1}\\!\\left(\\frac{1}{v_{1}'}-\\frac{1}{v_{1}}\\right)+\\ell_{2}\\!\\left(\\frac{1}{v_{2}'}-\\frac{1}{v_{2}}\\right)$ and report it in seconds. Round $\\Delta t$ to six significant figures.\n- Using the above statistical-mechanical formulation, compute the Metropolis acceptance probability $p_{\\mathrm{acc}}$ for this single datum. Round $p_{\\mathrm{acc}}$ to four significant figures as a pure decimal (no percent).\n\nExpress your final answer as a two-entry row vector $\\big(\\Delta t,\\;p_{\\mathrm{acc}}\\big)$.", "solution": "The problem is validated as self-contained, objective, and scientifically grounded in the principles of computational geophysics and statistical mechanics. All necessary data and definitions are provided, and there are no internal contradictions or violations of scientific principles. We may therefore proceed with the solution.\n\nThe problem asks for two quantities: the change in travel time, $\\Delta t$, and the Metropolis acceptance probability, $p_{\\mathrm{acc}}$.\n\nFirst, we compute the change in predicted travel time, $\\Delta t$. The travel time for a model $\\mathbf{m}$ with velocities $\\{v_1, v_2\\}$ and path lengths $\\{\\ell_1, \\ell_2\\}$ is $t(\\mathbf{m}) = \\frac{\\ell_1}{v_1} + \\frac{\\ell_2}{v_2}$. For the proposed model $\\mathbf{m}'$ with velocities $\\{v_1', v_2'\\}$, the travel time is $t(\\mathbf{m}') = \\frac{\\ell_1}{v_1'} + \\frac{\\ell_2}{v_2'}$. The change $\\Delta t$ is defined as $t(\\mathbf{m}') - t(\\mathbf{m})$.\n\nThe expression for $\\Delta t$ is given as:\n$$\n\\Delta t = \\ell_{1}\\!\\left(\\frac{1}{v_{1}'}-\\frac{1}{v_{1}}\\right)+\\ell_{2}\\!\\left(\\frac{1}{v_{2}'}-\\frac{1}{v_{2}}\\right)\n$$\nWe substitute the given values: $\\ell_1=3\\,\\mathrm{km}$, $\\ell_2=2\\,\\mathrm{km}$, $v_1=2\\,\\mathrm{km/s}$, $v_2=4\\,\\mathrm{km/s}$, $v_1'=2.2\\,\\mathrm{km/s}$, and $v_2'=3.8\\,\\mathrm{km/s}$. The units of each term $\\ell/v$ are $\\mathrm{km} / (\\mathrm{km/s}) = \\mathrm{s}$, so the resulting $\\Delta t$ will be in seconds.\n$$\n\\Delta t = 3 \\left(\\frac{1}{2.2} - \\frac{1}{2}\\right) + 2 \\left(\\frac{1}{3.8} - \\frac{1}{4}\\right)\n$$\nWe perform the calculations inside the parentheses:\n$$\n\\frac{1}{2.2} - \\frac{1}{2} = \\frac{1}{11/5} - \\frac{1}{2} = \\frac{5}{11} - \\frac{1}{2} = \\frac{10-11}{22} = -\\frac{1}{22}\n$$\n$$\n\\frac{1}{3.8} - \\frac{1}{4} = \\frac{1}{19/5} - \\frac{1}{4} = \\frac{5}{19} - \\frac{1}{4} = \\frac{20-19}{76} = \\frac{1}{76}\n$$\nSubstituting these fractions back into the expression for $\\Delta t$:\n$$\n\\Delta t = 3 \\left(-\\frac{1}{22}\\right) + 2 \\left(\\frac{1}{76}\\right) = -\\frac{3}{22} + \\frac{2}{76} = -\\frac{3}{22} + \\frac{1}{38}\n$$\nTo combine these fractions, we find a common denominator, which is $22 \\times 19 = 418$.\n$$\n\\Delta t = -\\frac{3 \\times 19}{22 \\times 19} + \\frac{1 \\times 11}{38 \\times 11} = \\frac{-57}{418} + \\frac{11}{418} = -\\frac{46}{418} = -\\frac{23}{209}\n$$\nConverting this exact fraction to a decimal gives:\n$$\n\\Delta t = -\\frac{23}{209} \\approx -0.11004784688... \\,\\mathrm{s}\n$$\nRounding to six significant figures as requested, we get $\\Delta t = -0.110048\\,\\mathrm{s}$.\n\nNext, we compute the Metropolis acceptance probability, $p_{\\mathrm{acc}}$. It is defined as:\n$$\np_{\\mathrm{acc}} = \\min\\!\\left(1, \\exp\\!\\left(-\\frac{\\Delta E}{T}\\right)\\right)\n$$\nwhere $\\Delta E = E(\\mathbf{m}') - E(\\mathbf{m})$ and $T$ is the temperature. The energy function is given by the negative log-likelihood:\n$$\nE(\\mathbf{m}) = \\frac{(t(\\mathbf{m}) - d)^2}{2\\sigma^2}\n$$\nWe are given the critical information that the current model $\\mathbf{m}$ exactly fits the datum $d$, which means $t(\\mathbf{m}) - d = 0$. This simplifies the energy of the current model to:\n$$\nE(\\mathbf{m}) = \\frac{(0)^2}{2\\sigma^2} = 0\n$$\nThe energy of the proposed model $\\mathbf{m}'$ is:\n$$\nE(\\mathbf{m}') = \\frac{(t(\\mathbf{m}') - d)^2}{2\\sigma^2}\n$$\nWe can rewrite the term in the numerator as $t(\\mathbf{m}') - d = (t(\\mathbf{m}')-t(\\mathbf{m}))+(t(\\mathbf{m})-d)$. Since $t(\\mathbf{m}') - t(\\mathbf{m}) = \\Delta t$ and $t(\\mathbf{m}) - d = 0$, we have $t(\\mathbf{m}') - d = \\Delta t$.\nHence, the energy of the new state is:\n$$\nE(\\mathbf{m}') = \\frac{(\\Delta t)^2}{2\\sigma^2}\n$$\nThe change in energy is therefore:\n$$\n\\Delta E = E(\\mathbf{m}') - E(\\mathbf{m}) = \\frac{(\\Delta t)^2}{2\\sigma^2} - 0 = \\frac{(\\Delta t)^2}{2\\sigma^2}\n$$\nNow we substitute the values $\\sigma=0.05\\,\\mathrm{s}$ and the exact value of $\\Delta t = -23/209\\,\\mathrm{s}$ to avoid premature rounding errors.\n$$\n\\Delta E = \\frac{\\left(-\\frac{23}{209}\\right)^2}{2(0.05)^2} = \\frac{\\frac{529}{43681}}{2(0.0025)} = \\frac{\\frac{529}{43681}}{0.005} = \\frac{529}{43681 \\times 0.005} = \\frac{529}{218.405} \\approx 2.4221057...\n$$\nNow we compute the argument of the exponential function, $-\\frac{\\Delta E}{T}$, with $T=0.5$:\n$$\n-\\frac{\\Delta E}{T} = -\\frac{2.4221057...}{0.5} = -4.8442114...\n$$\nFinally, we calculate the acceptance probability:\n$$\np_{\\mathrm{acc}} = \\min\\!\\left(1, \\exp(-4.8442114...)\\right) = \\min\\!\\left(1, 0.0078740...\\right) = 0.0078740...\n$$\nRounding to four significant figures as requested, we get $p_{\\mathrm{acc}} = 0.007874$.\n\nThe final answer is the two-entry row vector $(\\Delta t, p_{\\mathrm{acc}})$.", "answer": "$$\\boxed{\\begin{pmatrix} -0.110048 & 0.007874 \\end{pmatrix}}$$", "id": "3614462"}, {"introduction": "With the basic mechanics in place, we now turn to a critical theoretical aspect: the detailed balance condition. This thought experiment [@problem_id:3614444] challenges you to analyze a plausible but flawed acceptance rule that omits the standard $\\min(1, \\dots)$ component. By deriving the stationary distribution and quantifying the resulting bias, you will gain a deeper appreciation for why the Metropolis-Hastings rule is precisely formulated to ensure convergence to the correct Boltzmann distribution.", "problem": "In a discrete geophysical inverse problem, consider three competing Earth models with scalar objective function (energy) values $E_{0}=0$, $E_{1}=\\Delta$, and $E_{2}=2\\Delta$, where $\\Delta&gt;0$. The objective function $E_{i}$ measures the misfit plus regularization in a standard seismic travel-time inversion setting and the sampling temperature is $T&gt;0$. A practitioner attempts to use Simulated Annealing (SA) to explore the model space using a Metropolis-type accept/reject step but replaces the standard Metropolis acceptance with an “unclipped” rule $\\alpha=\\exp(-\\Delta E/T)$ applied uniformly to all proposed moves, without the usual $\\min(1,\\cdot)$ cap. To avoid negative or exceeding-one transition probabilities, they implement the following per-state normalization: from a current state $i$, they propose a neighbor $j\\neq i$ uniformly at random (each with probability $1/2$ since every state is connected to the other two), and accept the proposal with probability\n$$\nA(i\\to j)\\;=\\;\\frac{\\exp\\!\\big(-(E_{j}-E_{i})/T\\big)}{Z(i)}\\,,\\qquad Z(i)\\;=\\;\\max\\!\\left\\{1,\\;\\frac{1}{2}\\sum_{k\\neq i}\\exp\\!\\big(-(E_{k}-E_{i})/T\\big)\\right\\}.\n$$\nThe resulting Markov chain has transition probabilities $P_{ij}=\\frac{1}{2}A(i\\to j)$ for $j\\neq i$, and $P_{ii}=1-\\sum_{j\\neq i}P_{ij}$. Assume the proposal mechanism is symmetric and the neighborhood graph is complete on the three states.\n\nStarting only from the definitions of a Markov chain, transition kernel, and the Boltzmann equilibrium distribution proportional to $\\exp(-E/T)$, perform the following:\n1) Construct the transition matrix $P$ symbolically in terms of $a=\\exp(-\\Delta/T)$.\n2) Derive the stationary distribution ratio $\\pi_{2}/\\pi_{0}$ induced by this “unclipped but normalized” rule.\n3) Compare $\\pi_{2}/\\pi_{0}$ to the Boltzmann ratio $\\exp\\!\\big(-(E_{2}-E_{0})/T\\big)=\\exp(-2\\Delta/T)$ and simplify the multiplicative bias factor\n$$\nb\\;=\\;\\frac{\\left(\\pi_{2}/\\pi_{0}\\right)}{\\exp(-2\\Delta/T)}\\,.\n$$\nExplain conceptually why the unclipped rule can fail to satisfy detailed balance and how this introduces bias. Propose a necessary correction to restore asymptotic validity with the Boltzmann target (for example, by an acceptance function that provably enforces detailed balance with symmetric proposals). Your final reported result must be the closed-form expression for $b$ in terms of $\\Delta$ and $T$. Do not round; no units are required for $b$.", "solution": "The problem is subjected to validation and is found to be scientifically grounded, well-posed, objective, and self-contained. It represents a valid theoretical exercise in an MCMC sampling context. We may therefore proceed with a full solution.\n\nThe problem asks for an analysis of a custom Markov chain defined on a three-state system with energies $E_{0}=0$, $E_{1}=\\Delta$, and $E_{2}=2\\Delta$, where $\\Delta > 0$ and the temperature is $T>0$. Let the variable $a$ be defined as $a = \\exp(-\\Delta/T)$. Since $\\Delta > 0$ and $T > 0$, it follows that $0 < a < 1$.\n\n**1) Construction of the Transition Matrix $P$**\n\nThe transition probability from state $i$ to state $j \\neq i$ is given by $P_{ij} = \\frac{1}{2} A(i \\to j)$, where the acceptance probability is $A(i \\to j) = \\frac{\\exp(-(E_j - E_i)/T)}{Z(i)}$. The normalization factor $Z(i)$ is defined as $Z(i) = \\max\\{1, \\frac{1}{2}\\sum_{k \\neq i} \\exp(-(E_k - E_i)/T)\\}$.\n\nFirst, we compute $Z(i)$ for each state $i \\in \\{0, 1, 2\\}$.\n\nFor state $i=0$:\nThe neighbors are $k=1$ and $k=2$. The sum is $\\frac{1}{2} [\\exp(-(E_1 - E_0)/T) + \\exp(-(E_2 - E_0)/T)] = \\frac{1}{2}[\\exp(-\\Delta/T) + \\exp(-2\\Delta/T)] = \\frac{1}{2}(a + a^2)$.\nSince $0 < a < 1$, we have $a+a^2 < 1+1=2$, so $\\frac{1}{2}(a+a^2) < 1$.\nThus, $Z(0) = \\max\\{1, \\frac{1}{2}(a+a^2)\\} = 1$.\n\nFor state $i=1$:\nThe neighbors are $k=0$ and $k=2$. The sum is $\\frac{1}{2} [\\exp(-(E_0 - E_1)/T) + \\exp(-(E_2 - E_1)/T)] = \\frac{1}{2}[\\exp(\\Delta/T) + \\exp(-\\Delta/T)] = \\frac{1}{2}(a^{-1} + a) = \\cosh(\\Delta/T)$.\nSince $\\Delta/T > 0$, $\\cosh(\\Delta/T) > 1$.\nThus, $Z(1) = \\max\\{1, \\frac{1}{2}(a^{-1}+a)\\} = \\frac{1}{2}(a^{-1}+a)$.\n\nFor state $i=2$:\nThe neighbors are $k=0$ and $k=1$. The sum is $\\frac{1}{2} [\\exp(-(E_0 - E_2)/T) + \\exp(-(E_1 - E_2)/T)] = \\frac{1}{2}[\\exp(2\\Delta/T) + \\exp(\\Delta/T)] = \\frac{1}{2}(a^{-2} + a^{-1})$.\nSince $a < 1$, both $a^{-1} > 1$ and $a^{-2} > 1$, so their sum is greater than $2$ and the expression is greater than $1$.\nThus, $Z(2) = \\max\\{1, \\frac{1}{2}(a^{-2}+a^{-1})\\} = \\frac{1}{2}(a^{-2}+a^{-1})$.\n\nNow, we compute the off-diagonal transition probabilities $P_{ij}$ for $j \\neq i$.\n\nTransitions from state $i=0$: $Z(0)=1$.\n$P_{01} = \\frac{1}{2} A(0 \\to 1) = \\frac{1}{2} \\exp(-(E_1 - E_0)/T) = \\frac{1}{2} \\exp(-\\Delta/T) = \\frac{1}{2}a$.\n$P_{02} = \\frac{1}{2} A(0 \\to 2) = \\frac{1}{2} \\exp(-(E_2 - E_0)/T) = \\frac{1}{2} \\exp(-2\\Delta/T) = \\frac{1}{2}a^2$.\n\nTransitions from state $i=1$: $Z(1) = \\frac{1}{2}(a^{-1}+a)$.\n$P_{10} = \\frac{1}{2} A(1 \\to 0) = \\frac{1}{2} \\frac{\\exp(-(E_0 - E_1)/T)}{Z(1)} = \\frac{1}{2} \\frac{a^{-1}}{\\frac{1}{2}(a^{-1}+a)} = \\frac{a^{-1}}{a^{-1}+a} = \\frac{1}{1+a^2}$.\n$P_{12} = \\frac{1}{2} A(1 \\to 2) = \\frac{1}{2} \\frac{\\exp(-(E_2 - E_1)/T)}{Z(1)} = \\frac{1}{2} \\frac{a}{\\frac{1}{2}(a^{-1}+a)} = \\frac{a}{a^{-1}+a} = \\frac{a^2}{1+a^2}$.\n\nTransitions from state $i=2$: $Z(2) = \\frac{1}{2}(a^{-2}+a^{-1})$.\n$P_{20} = \\frac{1}{2} A(2 \\to 0) = \\frac{1}{2} \\frac{\\exp(-(E_0 - E_2)/T)}{Z(2)} = \\frac{1}{2} \\frac{a^{-2}}{\\frac{1}{2}(a^{-2}+a^{-1})} = \\frac{a^{-2}}{a^{-2}+a^{-1}} = \\frac{1}{1+a}$.\n$P_{21} = \\frac{1}{2} A(2 \\to 1) = \\frac{1}{2} \\frac{\\exp(-(E_1 - E_2)/T)}{Z(2)} = \\frac{1}{2} \\frac{a^{-1}}{\\frac{1}{2}(a^{-2}+a^{-1})} = \\frac{a^{-1}}{a^{-2}+a^{-1}} = \\frac{a}{1+a}$.\n\nThe diagonal elements are $P_{ii} = 1 - \\sum_{j \\neq i} P_{ij}$.\n$P_{00} = 1 - (P_{01} + P_{02}) = 1 - \\frac{1}{2}(a+a^2)$.\n$P_{11} = 1 - (P_{10} + P_{12}) = 1 - \\left(\\frac{1}{1+a^2} + \\frac{a^2}{1+a^2}\\right) = 1 - \\frac{1+a^2}{1+a^2} = 0$.\n$P_{22} = 1 - (P_{20} + P_{21}) = 1 - \\left(\\frac{1}{1+a} + \\frac{a}{1+a}\\right) = 1 - \\frac{1+a}{1+a} = 0$.\n\nThe transition matrix $P$ is:\n$$\nP = \\begin{pmatrix}\n1 - \\frac{1}{2}(a+a^2) & \\frac{1}{2}a & \\frac{1}{2}a^2 \\\\\n\\frac{1}{1+a^2} & 0 & \\frac{a^2}{1+a^2} \\\\\n\\frac{1}{1+a} & \\frac{a}{1+a} & 0\n\\end{pmatrix}\n$$\n\n**2) Derivation of the Stationary Distribution Ratio $\\pi_{2}/\\pi_{0}$**\n\nThe stationary distribution $\\pi = (\\pi_0, \\pi_1, \\pi_2)$ satisfies the equation $\\pi P = \\pi$, subject to $\\pi_0+\\pi_1+\\pi_2=1$. This implies the balance equations for each state. The flow into state $j$ must equal the flow out of state $j$. Writing down the balance of flow equations for states $1$ and $2$:\n$\\pi_1 = \\sum_{i} \\pi_i P_{i1} = \\pi_0 P_{01} + \\pi_1 P_{11} + \\pi_2 P_{21}$.\n$\\pi_2 = \\sum_{i} \\pi_i P_{i2} = \\pi_0 P_{02} + \\pi_1 P_{12} + \\pi_2 P_{22}$.\n\nSubstituting the matrix elements, we get a system of two linear equations in terms of $\\pi_0, \\pi_1, \\pi_2$:\n(1) $\\pi_1 = \\pi_0 \\left(\\frac{1}{2}a\\right) + \\pi_2 \\left(\\frac{a}{1+a}\\right)$.\n(2) $\\pi_2 = \\pi_0 \\left(\\frac{1}{2}a^2\\right) + \\pi_1 \\left(\\frac{a^2}{1+a^2}\\right)$.\n\nWe can solve this system for the ratios $\\pi_1/\\pi_0$ and $\\pi_2/\\pi_0$. From equation (2), we express $\\pi_1$ in terms of $\\pi_0$ and $\\pi_2$:\n$\\pi_1 \\left(\\frac{a^2}{1+a^2}\\right) = \\pi_2 - \\pi_0 \\left(\\frac{1}{2}a^2\\right) \\implies \\pi_1 = \\frac{1+a^2}{a^2} \\left(\\pi_2 - \\frac{1}{2}a^2 \\pi_0\\right) = \\frac{1+a^2}{a^2}\\pi_2 - \\frac{1+a^2}{2}\\pi_0$.\n\nSubstitute this expression for $\\pi_1$ into equation (1):\n$\\frac{1+a^2}{a^2}\\pi_2 - \\frac{1+a^2}{2}\\pi_0 = \\frac{1}{2}a\\pi_0 + \\frac{a}{1+a}\\pi_2$.\n\nNow, collect terms proportional to $\\pi_2$ on one side and terms proportional to $\\pi_0$ on the other:\n$\\pi_2 \\left(\\frac{1+a^2}{a^2} - \\frac{a}{1+a}\\right) = \\pi_0 \\left(\\frac{a}{2} + \\frac{1+a^2}{2}\\right)$.\n\nSimplify the coefficients. For the left side:\n$\\frac{(1+a^2)(1+a) - a(a^2)}{a^2(1+a)} = \\frac{1+a+a^2+a^3-a^3}{a^2(1+a)} = \\frac{1+a+a^2}{a^2(1+a)}$.\nFor the right side:\n$\\frac{a+1+a^2}{2} = \\frac{1+a+a^2}{2}$.\n\nThe equation becomes:\n$\\pi_2 \\left(\\frac{1+a+a^2}{a^2(1+a)}\\right) = \\pi_0 \\left(\\frac{1+a+a^2}{2}\\right)$.\nSince $a > 0$, the term $1+a+a^2$ is non-zero and can be cancelled from both sides:\n$\\frac{\\pi_2}{a^2(1+a)} = \\frac{\\pi_0}{2}$.\n\nFrom this, we find the desired ratio:\n$\\frac{\\pi_2}{\\pi_0} = \\frac{a^2(1+a)}{2}$.\n\n**3) Calculation of the Bias Factor $b$**\n\nThe bias factor $b$ is defined as the ratio of the actual stationary probability ratio $\\pi_2/\\pi_0$ to the target Boltzmann ratio $\\exp(-(E_2-E_0)/T)$.\nThe target ratio is $\\exp(-(2\\Delta-0)/T) = \\exp(-2\\Delta/T) = (\\exp(-\\Delta/T))^2 = a^2$.\nSo, $b = \\frac{(\\pi_2/\\pi_0)}{a^2}$.\n\nSubstituting the result from part 2:\n$b = \\frac{a^2(1+a)/2}{a^2}$.\nSince $a \\neq 0$, we can cancel the $a^2$ term:\n$b = \\frac{1+a}{2}$.\n\nFinally, expressing $b$ in terms of $\\Delta$ and $T$:\n$b = \\frac{1+\\exp(-\\Delta/T)}{2}$.\n\n**Conceptual Explanation and Correction**\n\nThe standard Metropolis-Hastings algorithm for a symmetric proposal distribution ensures that the resulting Markov chain satisfies the detailed balance condition with respect to a target distribution $\\pi_B$: $\\pi_B(i) P_{ij} = \\pi_B(j) P_{ji}$. For the Boltzmann distribution $\\pi_B(i) \\propto \\exp(-E_i/T)$, this requires the acceptance ratio to be $A(i \\to j)/A(j \\to i) = \\exp(-(E_j - E_i)/T)$. The standard Metropolis choice $A_{MH}(i \\to j) = \\min(1, \\exp(-(E_j - E_i)/T))$ satisfies this condition.\n\nThe practitioner's rule is $A(i \\to j) = \\exp(-(E_j-E_i)/T) / Z(i)$. The ratio of acceptance probabilities is:\n$\\frac{A(i \\to j)}{A(j \\to i)} = \\frac{\\exp(-(E_j-E_i)/T) / Z(i)}{\\exp(-(E_i-E_j)/T) / Z(j)} = \\frac{Z(j)}{Z(i)} \\exp(-2(E_j-E_i)/T)$.\nFor detailed balance to hold, we would need $Z(j)/Z(i) = \\exp((E_j-E_i)/T)$. However, $Z(i)$ is a function of all outgoing transitions from state $i$, not a simple term that can satisfy this pairwise relation for all $j$. For instance, we found $Z(0)=1$ and $Z(1)=\\frac{1}{2}(a^{-1}+a)$. The ratio $Z(1)/Z(0) = \\frac{1}{2}(a^{-1}+a)$ is not equal to the required $\\exp((E_1-E_0)/T) = a^{-1}$ (unless $a=1$, which is not the case).\n\nThis failure to satisfy detailed balance means the stationary distribution $\\pi$ of the chain is not the Boltzmann distribution $\\pi_B$. The introduced normalization $Z(i)$, while ensuring the transition matrix is stochastic, breaks the symmetry required for targeting $\\pi_B$, introducing a systematic bias. The factor $b$ quantifies this bias for the relative populations of states $2$ and $0$. Since $b = (1+a)/2 < 1$ for $a<1$, the rule systematically under-samples the high-energy state $E_2$ relative to the ground state $E_0$ when compared to the correct Boltzmann distribution.\n\nTo correct this and ensure convergence to the Boltzmann distribution, one must replace the ad-hoc acceptance rule with one that provably enforces detailed balance. For the given symmetric proposal, the standard and correct choice is the Metropolis acceptance probability:\n$$\nA_{\\text{corrected}}(i \\to j) = \\min\\left(1, \\exp\\left(-\\frac{E_j - E_i}{T}\\right)\\right).\n$$\nThis rule guarantees that the stationary distribution is the Boltzmann distribution, thus removing the bias and ensuring the asymptotic validity of the Simulated Annealing procedure.", "answer": "$$\n\\boxed{\\frac{1+\\exp(-\\Delta/T)}{2}}\n$$", "id": "3614444"}, {"introduction": "Finally, we integrate these concepts into a complete, practical application. This exercise guides you through implementing a hybrid optimization scheme that combines the global search power of Simulated Annealing with the efficiency of a local gradient-based polisher, LBFGS [@problem_id:3614454]. You will develop and apply criteria for the handoff between these two methods, a common and powerful strategy for solving complex, multimodal inverse problems in geophysics.", "problem": "You are to design and implement a hybrid global-local optimization scheme for a synthetic, yet geophysically motivated, one-dimensional slowness inversion objective that exhibits multimodality. The hybrid couples Simulated Annealing (SA) with Limited-memory Broyden-Fletcher-Goldfarb-Shanno (LBFGS) polishing, and the handoff from SA to LBFGS must be governed by criteria based on local curvature, gradient norms, and temperature thresholds. Your implementation must follow precise mathematical definitions and produce quantitative metrics for a fixed test suite.\n\nThe optimization problem uses a model vector $m \\in \\mathbb{R}^d$ representing a one-dimensional slowness profile. The total objective is\n$$\n\\Phi(m) \\equiv \\Phi_{\\text{data}}(m) + \\Phi_{\\text{reg}}(m) + \\Phi_{\\text{smooth}}(m),\n$$\nwith the following components:\n- The data misfit component is\n$$\n\\Phi_{\\text{data}}(m) = \\frac{1}{2 K \\sigma^2} \\left\\| L m - t^{\\text{obs}} \\right\\|_2^2,\n$$\nwhere $L \\in \\mathbb{R}^{K \\times d}$ is a path-length matrix, $t^{\\text{obs}} \\in \\mathbb{R}^K$ are synthetic observations, $K$ is the number of observations, and $\\sigma$ is the standard deviation of the observation noise.\n- The multimodal regularizer is\n$$\n\\Phi_{\\text{reg}}(m) = \\beta \\sum_{j=1}^{d} \\left(1 - \\cos(\\omega m_j) \\right),\n$$\nwhich introduces many local minima.\n- The quadratic smoothness regularizer is\n$$\n\\Phi_{\\text{smooth}}(m) = \\gamma \\sum_{j=1}^{d-1} (m_{j+1} - m_j)^2.\n$$\n\nUse the following fixed configuration that must be reproduced exactly by your program:\n- Dimension: $d = 6$.\n- Number of observations: $K = 12$.\n- Synthetic data generation: create $L$ and $t^{\\text{obs}}$ deterministically with a pseudorandom generator initialized at seed $2024$ as follows. Draw $L$ with independent entries from the uniform distribution on $[0.5, 1.5]$, draw a true model $m^{\\star}$ with independent entries from the uniform distribution on $[0.8, 1.2]$, and set $t^{\\text{obs}} = L m^{\\star} + \\eta$, where $\\eta$ is independent, mean-zero Gaussian noise with standard deviation $\\sigma$ applied entrywise.\n- Constants: $\\sigma = 0.02$, $\\beta = 0.02$, $\\omega = 6.0$, $\\gamma = 0.1$.\n- Variable bounds (to be enforced during polishing and used to clip SA proposals): $m_j \\in [0.2, 1.8]$ for all $j$.\n- Initial model for all runs: $m^{(0)} = \\mathbf{1} \\in \\mathbb{R}^d$.\n\nSimulated Annealing must be implemented with the Metropolis criterion derived from the Boltzmann distribution. Given a proposal $m'$ from a current state $m$ at temperature $T$, the acceptance probability is\n$$\np_{\\text{acc}} = \\min\\left(1, \\exp\\left(-\\frac{\\Phi(m') - \\Phi(m)}{T}\\right)\\right).\n$$\nUse a Gaussian neighborhood proposal with zero mean and isotropic covariance whose standard deviation scales with temperature, namely\n$$\nm' = \\Pi_{[0.2, 1.8]^d}\\left(m + \\delta\\right), \\quad \\delta \\sim \\mathcal{N}\\left(0, \\left(s_{\\text{prop}} T\\right)^2 I_d\\right),\n$$\nwhere $\\Pi$ denotes componentwise projection onto the bounds, $s_{\\text{prop}}$ is a scalar, and $I_d$ is the identity matrix. Use geometric cooling,\n$$\nT_{k+1} = \\alpha T_k,\n$$\nwith a fixed factor $\\alpha = 0.98$.\n\nThe deterministic polishing must use Limited-memory Broyden-Fletcher-Goldfarb-Shanno (LBFGS) as implemented in a standard scientific library. You may supply finite-difference gradients or rely on the solver’s internal approximations. The handoff from SA to LBFGS must use all of the following criteria simultaneously at an accepted SA state $m$:\n- A temperature threshold: $T \\le \\tau T_{\\text{init}}$.\n- A gradient norm threshold: $\\left\\|\\nabla \\Phi(m)\\right\\|_2 \\le g_{\\text{th}}$, where $\\nabla \\Phi(m)$ is approximated by central finite differences with a small symmetric step.\n- A local curvature threshold: the directional second derivative of $\\Phi$ along the unit gradient direction $u = \\nabla \\Phi(m) / \\left\\|\\nabla \\Phi(m)\\right\\|_2$ must satisfy\n$$\n\\kappa(m) \\equiv \\frac{\\Phi(m + h u) - 2 \\Phi(m) + \\Phi(m - h u)}{h^2} \\ge c_{\\text{th}},\n$$\nfor a small step $h$. If $\\left\\|\\nabla \\Phi(m)\\right\\|_2 = 0$, select any unit vector for $u$.\n\nIf the handoff criteria are never met within the prescribed number of SA iterations, you must still run LBFGS starting from the last SA state. Define the reported handoff iteration as the iteration count at which SA terminated in that case.\n\nYour program must compute, for each test case, a list with three entries:\n- The handoff iteration index as an integer.\n- The final objective value $\\Phi(m_{\\text{final}})$ after polishing as a float.\n- A boolean indicating whether polishing strictly reduced the objective relative to the handoff state by more than zero, that is, whether $\\Phi(m_{\\text{final}}) < \\Phi(m_{\\text{handoff}})$.\n\nAll outputs are dimensionless performance metrics. Angles, if any, are irrelevant. No physical unit is required in the output.\n\nTest suite. Run your hybrid algorithm on the following four parameter sets. For each, use an independent proposal random number generator seeded by the given integer so that the neighborhood sequence is reproducible:\n- Case $1$: seed $= 42$, $T_{\\text{init}} = 1.0$, $\\tau = 0.1$, $g_{\\text{th}} = 1.0 \\times 10^{-3}$, $c_{\\text{th}} = 1.0 \\times 10^{-3}$, number of SA iterations $N_{\\text{SA}} = 800$, proposal scale $s_{\\text{prop}} = 0.1$.\n- Case $2$: seed $= 7$, $T_{\\text{init}} = 2.5$, $\\tau = 0.2$, $g_{\\text{th}} = 3.0 \\times 10^{-3}$, $c_{\\text{th}} = 5.0 \\times 10^{-4}$, number of SA iterations $N_{\\text{SA}} = 800$, proposal scale $s_{\\text{prop}} = 0.15$.\n- Case $3$: seed $= 99$, $T_{\\text{init}} = 0.5$, $\\tau = 0.05$, $g_{\\text{th}} = 1.0 \\times 10^{-4}$, $c_{\\text{th}} = 5.0 \\times 10^{-3}$, number of SA iterations $N_{\\text{SA}} = 800$, proposal scale $s_{\\text{prop}} = 0.08$.\n- Case $4$: seed $= 123$, $T_{\\text{init}} = 1.5$, $\\tau = 0.01$, $g_{\\text{th}} = 0.0$, $c_{\\text{th}} = 0.0$, number of SA iterations $N_{\\text{SA}} = 500$, proposal scale $s_{\\text{prop}} = 0.12$.\n\nFinal output format. Your program should produce a single line of output containing the four per-case results as a comma-separated list enclosed in square brackets, where each result itself is a Python-style list in the form $[\\text{handoff\\_iter}, \\text{final\\_objective}, \\text{improved}]$. For example, a syntactically valid output has the form\n$$\n\\left[\\,[10,0.123, \\text{True}],[\\ldots],[\\ldots],[\\ldots]\\,\\right]\n$$\nbut with your program’s computed numbers and booleans in place of the placeholders.", "solution": "The user-provided problem is assessed to be **valid**. It is a well-posed, scientifically grounded, and objective problem in the domain of computational geophysics and optimization. It specifies a complete, deterministic numerical experiment involving the design and implementation of a hybrid Simulated Annealing and L-BFGS optimization algorithm. All necessary parameters, functions, and criteria are provided, enabling a unique and verifiable solution.\n\nThe core of the problem is to minimize a multimodal objective function $\\Phi(m) \\in \\mathbb{R}$ for a model vector $m \\in \\mathbb{R}^d$. The objective function is a sum of three components:\n$$\n\\Phi(m) = \\Phi_{\\text{data}}(m) + \\Phi_{\\text{reg}}(m) + \\Phi_{\\text{smooth}}(m)\n$$\nwhere:\n1.  The data misfit term, $\\Phi_{\\text{data}}(m)$, quantifies the discrepancy between model predictions and observations. It is a standard least-squares term:\n    $$\n    \\Phi_{\\text{data}}(m) = \\frac{1}{2 K \\sigma^2} \\left\\| L m - t^{\\text{obs}} \\right\\|_2^2\n    $$\n    Here, $L \\in \\mathbb{R}^{K \\times d}$ is the linear forward operator (path-length matrix), $t^{\\text{obs}} \\in \\mathbb{R}^K$ are the observed travel times, $K=12$ is the number of observations, $d=6$ is the model dimension, and $\\sigma=0.02$ is the noise standard deviation.\n\n2.  The multimodal regularizer, $\\Phi_{\\text{reg}}(m)$, introduces numerous local minima into the objective landscape, making it challenging for purely local optimizers. It is defined as:\n    $$\n    \\Phi_{\\text{reg}}(m) = \\beta \\sum_{j=1}^{d} \\left(1 - \\cos(\\omega m_j) \\right)\n    $$\n    with $\\beta = 0.02$ and $\\omega = 6.0$.\n\n3.  The smoothness regularizer, $\\Phi_{\\text{smooth}}(m)$, penalizes large variations between adjacent model parameters, promoting smoother solutions. It is a quadratic penalty:\n    $$\n    \\Phi_{\\text{smooth}}(m) = \\gamma \\sum_{j=1}^{d-1} (m_{j+1} - m_j)^2\n    $$\n    with $\\gamma = 0.1$.\n\nThe synthetic data, $L$ and $t^{\\text{obs}}$, are generated deterministically using a pseudorandom number generator seeded with $2024$. The true model, $m^{\\star}$, is drawn from a uniform distribution, and Gaussian noise is added to the synthetic data $L m^{\\star}$ to create $t^{\\text{obs}}$.\n\nThe optimization strategy is a hybrid approach. First, Simulated Annealing (SA) is employed for global exploration. Starting from an an initial model $m^{(0)} = \\mathbf{1} \\in \\mathbb{R}^d$ and an initial temperature $T_{\\text{init}}$, the algorithm proceeds iteratively. At each step $k$, a candidate model $m'$ is generated from the current model $m$ by adding a perturbation drawn from a Gaussian distribution whose variance scales with the current temperature $T_k$:\n$$\nm' = \\Pi_{[0.2, 1.8]^d}\\left(m + \\delta\\right), \\quad \\delta \\sim \\mathcal{N}\\left(0, \\left(s_{\\text{prop}} T_k\\right)^2 I_d\\right)\n$$\nwhere $\\Pi$ is a projection that enforces the box constraints $m_j \\in [0.2, 1.8]$. The candidate $m'$ is accepted with probability:\n$$\np_{\\text{acc}} = \\min\\left(1, \\exp\\left(-\\frac{\\Phi(m') - \\Phi(m)}{T_k}\\right)\\right)\n$$\nThe temperature is reduced at each step according to a geometric cooling schedule, $T_{k+1} = \\alpha T_k$, with $\\alpha = 0.98$.\n\nThe transition from the global exploration (SA) to local refinement (L-BFGS) is governed by a set of three simultaneous criteria, checked at each accepted SA state:\n1.  Temperature Threshold: $T \\le \\tau T_{\\text{init}}$\n2.  Gradient Norm Threshold: $\\left\\|\\nabla \\Phi(m)\\right\\|_2 \\le g_{\\text{th}}$\n3.  Curvature Threshold: $\\kappa(m) \\ge c_{\\text{th}}$\n\nThe gradient $\\nabla \\Phi(m)$ is computed using a central finite-difference scheme with a step size $h_g$:\n$$\n\\frac{\\partial \\Phi}{\\partial m_j}(m) \\approx \\frac{\\Phi(m + h_g e_j) - \\Phi(m - h_g e_j)}{2h_g}\n$$\nwhere $e_j$ is the $j$-th canonical basis vector. A step size of $h_g = 10^{-6}$ will be used.\n\nThe curvature $\\kappa(m)$ is the second derivative along the normalized gradient direction $u = \\nabla \\Phi(m) / \\left\\|\\nabla \\Phi(m)\\right\\|_2$. It is also approximated by a central finite difference with step size $h_c$:\n$$\n\\kappa(m) \\approx \\frac{\\Phi(m + h_c u) - 2 \\Phi(m) + \\Phi(m - h_c u)}{h_c^2}\n$$\nA step size of $h_c = 10^{-5}$ will be used. If the gradient norm is numerically zero (e.g., less than $10^{-12}$), $u$ is set to a canonical basis vector, $[1, 0, \\dots, 0]^T$.\n\nIf these criteria are met, SA terminates, and the current model $m$ becomes the handoff model $m_{\\text{handoff}}$. If the maximum number of SA iterations, $N_{\\text{SA}}$, is reached without the criteria being met, the final model from SA is used as $m_{\\text{handoff}}$.\n\nFinally, the Limited-memory Broyden-Fletcher-Goldfarb-Shanno (L-BFGS) algorithm, specifically the `L-BFGS-B` variant from the `SciPy` library which accommodates box constraints, is used to polish the solution, starting from $m_{\\text{handoff}}$. This yields the final model $m_{\\text{final}}$.\n\nThe procedure is executed for four distinct test cases, each with its own set of algorithmic parameters and random seed for the SA proposal sequence. For each case, we report the handoff iteration, the final objective value $\\Phi(m_{\\text{final}})$, and a boolean indicating if the objective value was strictly improved by the L-BFGS polishing step.", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef solve():\n    \"\"\"\n    Main function to run the hybrid SA-LBFGS optimization for all test cases.\n    \"\"\"\n\n    # --- Problem Constants and Data Generation ---\n    D_DIM = 6\n    K_OBS = 12\n    SIGMA = 0.02\n    BETA = 0.02\n    OMEGA = 6.0\n    GAMMA = 0.1\n    MODEL_BOUNDS = (0.2, 1.8)\n    M_INIT = np.ones(D_DIM)\n\n    # Generate synthetic data deterministically\n    rng_data = np.random.default_rng(2024)\n    L_matrix = rng_data.uniform(0.5, 1.5, size=(K_OBS, D_DIM))\n    m_star = rng_data.uniform(0.8, 1.2, size=D_DIM)\n    noise = rng_data.normal(0, SIGMA, size=K_OBS)\n    t_obs = L_matrix @ m_star + noise\n\n    # --- Objective Function and Derivatives ---\n\n    def get_objective_function(L, t_obs_data, sigma_val, beta_val, omega_val, gamma_val):\n        \"\"\"Returns a callable objective function.\"\"\"\n        data_denom = 2.0 * K_OBS * sigma_val**2\n\n        def phi(m):\n            phi_data = np.sum((L @ m - t_obs_data)**2) / data_denom\n            phi_reg = beta_val * np.sum(1.0 - np.cos(omega_val * m))\n            phi_smooth = gamma_val * np.sum((m[1:] - m[:-1])**2)\n            return phi_data + phi_reg + phi_smooth\n        return phi\n\n    objective_func = get_objective_function(L_matrix, t_obs, SIGMA, BETA, OMEGA, GAMMA)\n\n    def compute_gradient(phi_func, m, h_g=1e-6):\n        \"\"\"Computes gradient via central finite differences.\"\"\"\n        grad = np.zeros_like(m, dtype=float)\n        for i in range(len(m)):\n            m_plus_h = m.copy()\n            m_plus_h[i] += h_g\n            m_minus_h = m.copy()\n            m_minus_h[i] -= h_g\n            grad[i] = (phi_func(m_plus_h) - phi_func(m_minus_h)) / (2.0 * h_g)\n        return grad\n\n    def compute_curvature(phi_func, m, grad, h_c=1e-5):\n        \"\"\"Computes directional second derivative along the gradient.\"\"\"\n        grad_norm = np.linalg.norm(grad)\n        if grad_norm < 1e-12:\n            u = np.zeros_like(m)\n            u[0] = 1.0\n        else:\n            u = grad / grad_norm\n        \n        phi_m = phi_func(m)\n        phi_plus = phi_func(m + h_c * u)\n        phi_minus = phi_func(m - h_c * u)\n        \n        kappa = (phi_plus - 2.0 * phi_m + phi_minus) / (h_c**2)\n        return kappa\n\n    # --- Test Suite Definition ---\n    test_cases = [\n        {'seed': 42, 'T_init': 1.0, 'tau': 0.1, 'g_th': 1.0e-3, 'c_th': 1.0e-3, 'N_SA': 800, 's_prop': 0.1},\n        {'seed': 7, 'T_init': 2.5, 'tau': 0.2, 'g_th': 3.0e-3, 'c_th': 5.0e-4, 'N_SA': 800, 's_prop': 0.15},\n        {'seed': 99, 'T_init': 0.5, 'tau': 0.05, 'g_th': 1.0e-4, 'c_th': 5.0e-3, 'N_SA': 800, 's_prop': 0.08},\n        {'seed': 123, 'T_init': 1.5, 'tau': 0.01, 'g_th': 0.0, 'c_th': 0.0, 'N_SA': 500, 's_prop': 0.12},\n    ]\n\n    all_results = []\n\n    for case in test_cases:\n        # --- Simulated Annealing ---\n        rng_sa = np.random.default_rng(case['seed'])\n        \n        m_current = M_INIT.copy()\n        cost_current = objective_func(m_current)\n        T = case['T_init']\n        alpha = 0.98\n        \n        m_handoff = None\n        handoff_iter = case['N_SA']\n\n        for k in range(1, case['N_SA'] + 1):\n            # Generate proposal\n            delta = rng_sa.normal(0, case['s_prop'] * T, size=D_DIM)\n            m_proposal = np.clip(m_current + delta, MODEL_BOUNDS[0], MODEL_BOUNDS[1])\n            cost_proposal = objective_func(m_proposal)\n            \n            # Acceptance Criterion\n            if cost_proposal < cost_current or rng_sa.random() < np.exp(-(cost_proposal - cost_current) / T):\n                m_current = m_proposal\n                cost_current = cost_proposal\n                \n                # Check Handoff Criteria\n                if T <= case['tau'] * case['T_init']:\n                    grad = compute_gradient(objective_func, m_current)\n                    grad_norm = np.linalg.norm(grad)\n                    \n                    if grad_norm <= case['g_th']:\n                        kappa = compute_curvature(objective_func, m_current, grad)\n                        if kappa >= case['c_th']:\n                            m_handoff = m_current.copy()\n                            handoff_iter = k\n                            break\n\n            # Cooling\n            T *= alpha\n\n        if m_handoff is None:\n            m_handoff = m_current.copy()\n        \n        cost_handoff = objective_func(m_handoff)\n\n        # --- L-BFGS Polishing ---\n        bounds = [MODEL_BOUNDS for _ in range(D_DIM)]\n        res = minimize(\n            objective_func,\n            m_handoff,\n            method='L-BFGS-B',\n            bounds=bounds\n        )\n        \n        cost_final = res.fun\n        \n        # --- Collect Results ---\n        improved = cost_final < cost_handoff\n        \n        case_result = [handoff_iter, cost_final, improved]\n        all_results.append(case_result)\n\n    # --- Final Output Formatting ---\n    # Convert Python list of lists to the required string format\n    result_str = '[' + ','.join(str(r) for r in all_results) + ']'\n    print(result_str)\n\nsolve()\n```", "id": "3614454"}]}