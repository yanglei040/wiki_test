{"hands_on_practices": [{"introduction": "This practice provides a foundational, hands-on experience in constructing a complete Bayesian model from the ground up. You will implement a hierarchical linear-Gaussian model to analyze synthetic tide-gauge data, a common task in geophysics. By calculating the posterior distribution for model parameters and the resulting posterior predictive distribution for unobserved sea levels, you will gain practical skills in applying the core principles of Bayesian inference and uncertainty propagation. [@problem_id:3618115]", "problem": "You are to construct and use a hierarchical linear-Gaussian model for sea level from tide-gauge observations with seasonal components and missing observations, and to derive and compute the posterior predictive distribution for sea level at unobserved times. The model is specified in mathematical terms and should be implemented as a complete, runnable program. All angles in trigonometric functions must be expressed in radians. All sea levels must be treated in meters, and noise variance in meters squared. The final program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets, aggregating the results from all provided test cases, with each prediction represented by a two-element list containing the predictive mean (in meters) and predictive variance (in meters squared).\n\nModel specification: Consider $S$ tide-gauge stations indexed by $s \\in \\{0,1,\\dots,S-1\\}$. Let $t$ denote time in years, and let $\\cos(2\\pi t)$ and $\\sin(2\\pi t)$ denote seasonal basis functions with period $1$ year (angles in radians). Observed sea level at station $s$ and time $t$ is modeled as\n$$\ny_s(t) = \\alpha_s + \\beta_s t + A \\cos(2\\pi t) + B \\sin(2\\pi t) + \\varepsilon_s(t),\n$$\nwhere $\\varepsilon_s(t) \\sim \\mathcal{N}(0,\\sigma^2)$ independently across all observations, with known noise variance $\\sigma^2$. Define the parameter vector\n$$\n\\theta = \\begin{bmatrix} \\alpha_0 \\\\ \\beta_0 \\\\ \\alpha_1 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\alpha_{S-1} \\\\ \\beta_{S-1} \\\\ A \\\\ B \\end{bmatrix} \\in \\mathbb{R}^{2S+2}.\n$$\nAssume a Gaussian prior\n$$\n\\theta \\sim \\mathcal{N}(m_0, S_0),\n$$\nwith $m_0$ and $S_0$ specified as follows: $m_0$ is the zero vector of length $2S+2$, and $S_0$ is diagonal with prior variances $v_\\alpha$ for each station-specific intercept $\\alpha_s$, $v_\\beta$ for each station-specific slope $\\beta_s$, and $v_A$ and $v_B$ for the seasonal coefficients $A$ and $B$. This constitutes a hierarchical model in which station-specific coefficients share common hyperparameters $v_\\alpha$ and $v_\\beta$ controlling their prior variability, and seasonal coefficients are global across stations.\n\nLet $H \\in \\mathbb{R}^{N \\times (2S+2)}$ denote the design matrix constructed from the $N$ available observations, each row corresponding to one observation $(s,t)$, with entries\n$$\nH_{(s,t)} = \\big[0,\\dots,0,\\underbrace{1}_{\\alpha_s},\\underbrace{t}_{\\beta_s},0,\\dots,0,\\underbrace{\\cos(2\\pi t)}_{A},\\underbrace{\\sin(2\\pi t)}_{B}\\big],\n$$\nplacing $1$ in the column for $\\alpha_s$ and $t$ in the column for $\\beta_s$, and the last two columns for the global seasonal terms. Missing observations are handled by simply omitting their rows from $H$ and their values from the observation vector $y \\in \\mathbb{R}^{N}$.\n\nUnder the linear-Gaussian assumptions and given $\\sigma^2$, the posterior for $\\theta$ is Gaussian with mean $m_n$ and covariance $S_n$:\n$$\nS_n = \\left(S_0^{-1} + \\frac{1}{\\sigma^2} H^\\top H\\right)^{-1}, \\quad\nm_n = S_n \\left(S_0^{-1} m_0 + \\frac{1}{\\sigma^2} H^\\top y\\right).\n$$\nFor a new prediction at station $s^\\ast$ and time $t^\\ast$, let $h^\\ast \\in \\mathbb{R}^{2S+2}$ be the corresponding design row constructed analogously to $H$. The posterior predictive distribution for $y_{s^\\ast}(t^\\ast)$ is Gaussian with mean and variance\n$$\n\\mu^\\ast = h^\\ast m_n, \\quad \\sigma_\\ast^2 = \\sigma^2 + h^\\ast S_n {h^\\ast}^\\top.\n$$\nYour program must compute $(\\mu^\\ast, \\sigma_\\ast^2)$ pairs for each specified query.\n\nTest suite: Implement the model and compute the posterior predictive mean and variance for the following three parameter sets. For each case, construct the design matrix $H$ from the given observations, use the specified prior hyperparameters to build $S_0$, and compute the predictive pairs for the specified queries. Report all predictive means in meters and variances in meters squared.\n\n- Case $1$ (general case with two stations, moderate noise, some missing observations):\n  - Number of stations: $S = 2$.\n  - Noise variance: $\\sigma^2 = 0.01$ $\\mathrm{m}^2$.\n  - Prior variances: $v_\\alpha = 0.25$ $\\mathrm{m}^2$, $v_\\beta = 0.0025$ $\\mathrm{m}^2/\\mathrm{year}^2$, $v_A = 0.04$ $\\mathrm{m}^2$, $v_B = 0.04$ $\\mathrm{m}^2$.\n  - Observations (station index $s$, time $t$ in years, sea level $y$ in meters):\n    - Station $0$: $(s=0, t=0.0, y=0.252)$, $(s=0, t=0.5, y=0.065)$, $(s=0, t=1.5, y=0.082)$, $(s=0, t=2.0, y=0.292)$.\n    - Station $1$: $(s=1, t=0.25, y=0.007)$, $(s=1, t=0.75, y=0.113)$, $(s=1, t=1.25, y=0.021)$, $(s=1, t=1.75, y=0.128)$.\n  - Queries (station index $s^\\ast$, time $t^\\ast$ in years): $(s^\\ast=0, t^\\ast=2.25)$, $(s^\\ast=0, t^\\ast=0.25)$, $(s^\\ast=1, t^\\ast=2.0)$.\n\n- Case $2$ (boundary case with one station and a single observation, larger noise):\n  - Number of stations: $S = 1$.\n  - Noise variance: $\\sigma^2 = 0.05$ $\\mathrm{m}^2$.\n  - Prior variances: $v_\\alpha = 0.25$ $\\mathrm{m}^2$, $v_\\beta = 0.0025$ $\\mathrm{m}^2/\\mathrm{year}^2$, $v_A = 0.04$ $\\mathrm{m}^2$, $v_B = 0.04$ $\\mathrm{m}^2$.\n  - Observations:\n    - Station $0$: $(s=0, t=0.0, y=0.20)$.\n  - Queries: $(s^\\ast=0, t^\\ast=0.5)$, $(s^\\ast=0, t^\\ast=1.0)$.\n\n- Case $3$ (edge case with three stations, one station with no observations, moderate noise):\n  - Number of stations: $S = 3$.\n  - Noise variance: $\\sigma^2 = 0.02$ $\\mathrm{m}^2$.\n  - Prior variances: $v_\\alpha = 0.25$ $\\mathrm{m}^2$, $v_\\beta = 0.0025$ $\\mathrm{m}^2/\\mathrm{year}^2$, $v_A = 0.04$ $\\mathrm{m}^2$, $v_B = 0.04$ $\\mathrm{m}^2$.\n  - Observations:\n    - Station $0$: $(s=0, t=0.0, y=0.260)$, $(s=0, t=0.5, y=0.060)$.\n    - Station $1$: $(s=1, t=0.25, y=0.005)$, $(s=1, t=0.75, y=0.115)$.\n    - Station $2$: no observations.\n  - Queries: $(s^\\ast=2, t^\\ast=0.6)$, $(s^\\ast=1, t^\\ast=1.25)$, $(s^\\ast=0, t^\\ast=1.5)$.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case yields a list of two-element lists $[\\mu^\\ast,\\sigma_\\ast^2]$ for its queries, in the order they are listed above. For example, the output should have the structure $[[[\\mu_1^{(1)},\\sigma_{1}^{2\\,(1)}],[\\mu_2^{(1)},\\sigma_{2}^{2\\,(1)}],\\dots],[[\\mu_1^{(2)},\\sigma_{1}^{2\\,(2)}],\\dots],[[\\mu_1^{(3)},\\sigma_{1}^{2\\,(3)}],\\dots]]$, all values being real numbers in the specified units.", "solution": "The user has provided a problem statement that requires the implementation of a hierarchical linear-Gaussian model to perform Bayesian inference on simulated tide-gauge data. The task is to compute the posterior predictive distribution for sea level at specified unobserved times.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\nThe problem provides the following model, definitions, and data:\n\n*   **Model Equation**: The observed sea level $y_s(t)$ at station $s$ and time $t$ is given by\n    $$y_s(t) = \\alpha_s + \\beta_s t + A \\cos(2\\pi t) + B \\sin(2\\pi t) + \\varepsilon_s(t)$$\n*   **Noise Model**: The observational noise $\\varepsilon_s(t)$ is independent and identically distributed as $\\mathcal{N}(0, \\sigma^2)$, with known variance $\\sigma^2$.\n*   **Parameter Vector**: The model parameters are collated into a vector $\\theta \\in \\mathbb{R}^{2S+2}$:\n    $$\\theta = \\begin{bmatrix} \\alpha_0 \\\\ \\beta_0 \\\\ \\vdots \\\\ \\alpha_{S-1} \\\\ \\beta_{S-1} \\\\ A \\\\ B \\end{bmatrix}$$\n*   **Prior Distribution**: The prior on $\\theta$ is a multivariate Gaussian, $\\theta \\sim \\mathcal{N}(m_0, S_0)$.\n    *   Prior Mean: $m_0$ is the zero vector.\n    *   Prior Covariance: $S_0$ is a diagonal matrix with diagonal entries corresponding to the prior variances $v_\\alpha$ for each $\\alpha_s$, $v_\\beta$ for each $\\beta_s$, $v_A$ for $A$, and $v_B$ for $B$.\n*   **Design Matrix ($H$)**: For $N$ observations, $H$ is an $N \\times (2S+2)$ matrix where each row, corresponding to an observation at station $s$ and time $t$, has the form:\n    $$H_{(s,t)} = \\big[0,\\dots,0,\\underbrace{1}_{\\text{col for }\\alpha_s},\\underbrace{t}_{\\text{col for }\\beta_s},0,\\dots,0,\\underbrace{\\cos(2\\pi t)}_{\\text{col for }A},\\underbrace{\\sin(2\\pi t)}_{\\text{col for }B}\\big]$$\n*   **Posterior Distribution**: The posterior distribution for $\\theta$ is Gaussian, $\\theta | y \\sim \\mathcal{N}(m_n, S_n)$, with:\n    $$S_n = \\left(S_0^{-1} + \\frac{1}{\\sigma^2} H^\\top H\\right)^{-1}$$\n    $$m_n = S_n \\left(S_0^{-1} m_0 + \\frac{1}{\\sigma^2} H^\\top y\\right)$$\n*   **Posterior Predictive Distribution**: For a new prediction at $(s^\\ast, t^\\ast)$ with design vector $h^\\ast$, the predictive distribution is Gaussian with mean $\\mu^\\ast$ and variance $\\sigma_\\ast^2$:\n    $$\\mu^\\ast = h^\\ast m_n$$\n    $$\\sigma_\\ast^2 = \\sigma^2 + h^\\ast S_n {h^\\ast}^\\top$$\n*   **Test Cases**: Three specific test cases are provided, each with values for $S$, $\\sigma^2$, prior variances, a set of observations $(s, t, y)$, and a set of queries $(s^\\ast, t^\\ast)$.\n\n**Step 2: Validate Using Extracted Givens**\n\nThe problem is evaluated against the validation criteria:\n\n*   **Scientifically Grounded**: The problem is well-grounded in the principles of Bayesian statistics. The use of a linear-Gaussian model is a standard and fundamental technique for regression and data assimilation, widely applied in the sciences, including geophysics. The model structure (station-specific trends plus global seasonality) is a common and reasonable simplification.\n*   **Well-Posed**: The problem is well-posed. The prior variances ($v_\\alpha, v_\\beta, v_A, v_B$) are all positive, which ensures that the prior covariance matrix $S_0$ is positive definite. Consequently, its inverse $S_0^{-1}$ exists and is also positive definite. The matrix $H^\\top H$ is positive semi-definite. The sum of a positive definite matrix ($S_0^{-1}$) and a positive semi-definite matrix ($\\frac{1}{\\sigma^2} H^\\top H$) is always positive definite. This guarantees that the matrix $S_n^{-1}$ is invertible, ensuring that a unique posterior covariance $S_n$ and mean $m_n$ exist.\n*   **Objective**: The problem is stated with complete objectivity, using precise mathematical formulations and numerical data. There are no subjective or opinion-based components.\n*   **Completeness and Consistency**: The problem is self-contained. For each test case, all necessary parameters, data, and hyperparameters are explicitly provided. There are no contradictions in the setup. The handling of missing observations (by omitting rows from $H$) is a standard and consistent approach.\n*   **Realism and Feasibility**: The numerical values provided for sea levels, time scales, and variances are plausible for a simplified model of this type. The units are consistent throughout. The computational task is feasible.\n*   **Structure and Clarity**: The problem is clearly structured. All variables and mathematical operations are unambiguously defined. The inclusion of edge cases, such as a station with no observations (Case 3), tests the robustness of the model implementation and its adherence to Bayesian principles (in such a case, the posterior for that station's parameters should revert to the prior).\n\n**Step 3: Verdict and Action**\n\nThe problem statement is **valid**. It is scientifically sound, mathematically consistent, well-posed, and complete. I will proceed to develop the solution.\n\n### Solution Design\n\nThe problem requires implementing the standard Bayesian update and prediction steps for a linear-Gaussian model. The core of the solution is to translate the provided matrix equations into a computational algorithm. The process for each test case will be as follows:\n\n1.  **Initialization**: Given the number of stations $S$, determine the dimension of the parameter vector $\\theta$, $D = 2S + 2$. The parameters are ordered as $(\\alpha_0, \\beta_0, \\alpha_1, \\beta_1, \\dots, \\alpha_{S-1}, \\beta_{S-1}, A, B)$.\n\n2.  **Prior Specification**:\n    *   Construct the prior mean vector $m_0$, which is a zero vector of length $D$.\n    *   Construct the prior covariance matrix $S_0$. This is a $D \\times D$ diagonal matrix. The diagonal entries are populated with the given prior variances: $S$ entries of $v_\\alpha$, followed by $S$ entries of $v_\\beta$, and finally $v_A$ and $v_B$. To be precise, the diagonal elements for $\\alpha_s$ and $\\beta_s$ are placed at indices $2s$ and $2s+1$, respectively. The elements for $A$ and $B$ are at the last two indices, $2S$ and $2S+1$.\n    *   Compute the inverse of the prior covariance, $S_0^{-1}$, which is also a diagonal matrix whose entries are the reciprocals of the prior variances.\n\n3.  **Data Ingestion**:\n    *   From the list of observations, construct the $N \\times D$ design matrix $H$ and the $N \\times 1$ observation vector $y$.\n    *   For each observation $(s_i, t_i, y_i)$, a corresponding row $H_i$ is created. This row vector has four non-zero entries: a $1$ at index $2s_i$ (for $\\alpha_{s_i}$), the value $t_i$ at index $2s_i+1$ (for $\\beta_{s_i}$), $\\cos(2\\pi t_i)$ at index $2S$ (for $A$), and $\\sin(2\\pi t_i)$ at index $2S+1$ (for $B$).\n    *   The vector $y$ simply collects all observed values $y_i$.\n\n4.  **Posterior Calculation**:\n    *   Calculate the matrix $S_n^{-1} = S_0^{-1} + \\frac{1}{\\sigma^2} H^\\top H$. This matrix represents the posterior precision, which is the sum of the prior precision and the data precision (scaled likelihood).\n    *   Invert this matrix to obtain the posterior covariance matrix: $S_n = (S_n^{-1})^{-1}$.\n    *   Calculate the term $b = S_0^{-1} m_0 + \\frac{1}{\\sigma^2} H^\\top y$. Since $m_0 = 0$, this simplifies to $b = \\frac{1}{\\sigma^2} H^\\top y$.\n    *   Compute the posterior mean vector: $m_n = S_n b$. This calculation finds the mean of the updated parameter distribution, which balances prior beliefs with evidence from the data.\n\n5.  **Prediction**:\n    *   For each query $(s^\\ast, t^\\ast)$, construct the $1 \\times D$ design vector $h^\\ast$ in the same manner as a row of $H$.\n    *   The posterior predictive mean is calculated as the dot product $\\mu^\\ast = h^\\ast m_n$. This gives the best estimate of the sea level at the query point, given the model and the observed data.\n    *   The posterior predictive variance is calculated as $\\sigma_\\ast^2 = \\sigma^2 + h^\\ast S_n {h^\\ast}^\\top$. This variance has two components: the inherent observation noise $\\sigma^2$ and the uncertainty in the parameters propagated through the model, $h^\\ast S_n {h^\\ast}^\\top$. This sum represents the total uncertainty in a new prediction.\n\nThis procedure will be implemented as a function that can be applied to all three test cases provided. The case with no observations for a station (Case 3, station 2) is handled correctly by this framework: the columns of $H$ corresponding to $\\alpha_2$ and $\\beta_2$ will be all zeros. As a result, the posterior for these parameters will not be updated by the data and will remain equal to their prior distribution, which is the mathematically correct outcome. The implementation will use the `numpy` library for all linear algebra operations.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final results.\n    \"\"\"\n\n    def compute_predictions(S, sigma2, prior_variances, observations, queries):\n        \"\"\"\n        Computes posterior predictive means and variances for a single test case.\n\n        Args:\n            S (int): Number of stations.\n            sigma2 (float): Observation noise variance.\n            prior_variances (list): A list of floats [v_alpha, v_beta, v_A, v_B].\n            observations (list): A list of tuples (s, t, y).\n            queries (list): A list of tuples (s_star, t_star).\n\n        Returns:\n            list: A list of [mean, variance] pairs for each query.\n        \"\"\"\n        # Dimension of the parameter vector theta\n        D = 2 * S + 2\n        \n        # 1. Construct prior distribution parameters\n        v_alpha, v_beta, v_A, v_B = prior_variances\n        m0 = np.zeros(D)\n        \n        diag_S0 = []\n        for s in range(S):\n            diag_S0.extend([v_alpha, v_beta])\n        diag_S0.extend([v_A, v_B])\n        S0 = np.diag(diag_S0)\n        \n        # The inverse of a diagonal matrix is a diagonal matrix of reciprocals\n        S0_inv = np.diag(1 / np.array(diag_S0))\n        \n        # 2. Construct design matrix H and observation vector y\n        N = len(observations)\n        if N > 0:\n            H = np.zeros((N, D))\n            y = np.zeros(N)\n            for i, (s, t, y_val) in enumerate(observations):\n                # Row H_i for observation (s, t, y_val)\n                H[i, 2*s] = 1.0                          # Coefficient for alpha_s\n                H[i, 2*s + 1] = t                        # Coefficient for beta_s\n                H[i, 2*S] = np.cos(2 * np.pi * t)        # Coefficient for A\n                H[i, 2*S + 1] = np.sin(2 * np.pi * t)    # Coefficient for B\n                y[i] = y_val\n        else:\n            # Handle cases with no observations (not in test suite but good practice)\n            H = np.zeros((0, D))\n            y = np.zeros(0)\n\n        # 3. Compute posterior distribution parameters (mean mn, covariance Sn)\n        # Posterior precision matrix: S_n^-1 = S_0^-1 + (1/sigma^2) * H^T * H\n        H_T_H = H.T @ H\n        Sn_inv = S0_inv + (1 / sigma2) * H_T_H\n        \n        # Posterior covariance: S_n = (S_n^-1)^-1\n        Sn = np.linalg.inv(Sn_inv)\n        \n        # Posterior mean: m_n = S_n * (S_0^-1 * m_0 + (1/sigma^2) * H^T * y)\n        # Since m_0 is the zero vector, this simplifies.\n        H_T_y = H.T @ y\n        mn = Sn @ ((1 / sigma2) * H_T_y)\n        \n        # 4. Compute predictions for all queries\n        results = []\n        for s_star, t_star in queries:\n            # Construct the design vector h* for the query\n            h_star = np.zeros(D)\n            h_star[2*s_star] = 1.0                             # alpha_{s*}\n            h_star[2*s_star + 1] = t_star                      # beta_{s*}\n            h_star[2*S] = np.cos(2 * np.pi * t_star)           # A\n            h_star[2*S + 1] = np.sin(2 * np.pi * t_star)       # B\n            \n            # Posterior predictive mean: mu* = h* @ m_n\n            mu_star = h_star @ mn\n            \n            # Posterior predictive variance: sigma*^2 = sigma^2 + h* @ S_n @ h*_T\n            sigma2_star = sigma2 + h_star @ Sn @ h_star\n            \n            results.append([mu_star, sigma2_star])\n            \n        return results\n\n    # --- Test Cases ---\n    \n    # Case 1\n    case1 = {\n        \"S\": 2, \"sigma2\": 0.01,\n        \"prior_variances\": [0.25, 0.0025, 0.04, 0.04],\n        \"observations\": [\n            (0, 0.0, 0.252), (0, 0.5, 0.065), (0, 1.5, 0.082), (0, 2.0, 0.292),\n            (1, 0.25, 0.007), (1, 0.75, 0.113), (1, 1.25, 0.021), (1, 1.75, 0.128)\n        ],\n        \"queries\": [(0, 2.25), (0, 0.25), (1, 2.0)]\n    }\n\n    # Case 2\n    case2 = {\n        \"S\": 1, \"sigma2\": 0.05,\n        \"prior_variances\": [0.25, 0.0025, 0.04, 0.04],\n        \"observations\": [(0, 0.0, 0.20)],\n        \"queries\": [(0, 0.5), (0, 1.0)]\n    }\n\n    # Case 3\n    case3 = {\n        \"S\": 3, \"sigma2\": 0.02,\n        \"prior_variances\": [0.25, 0.0025, 0.04, 0.04],\n        \"observations\": [\n            (0, 0.0, 0.260), (0, 0.5, 0.060),\n            (1, 0.25, 0.005), (1, 0.75, 0.115)\n        ],\n        \"queries\": [(2, 0.6), (1, 1.25), (0, 1.5)]\n    }\n    \n    test_cases = [case1, case2, case3]\n    all_results = []\n    \n    for case in test_cases:\n        result_case = compute_predictions(\n            case[\"S\"], case[\"sigma2\"], case[\"prior_variances\"],\n            case[\"observations\"], case[\"queries\"]\n        )\n        all_results.append(result_case)\n\n    # Format the final output string to match the specified format.\n    # The str() function on a list of lists creates the desired structure,\n    # and .replace(\" \", \"\") removes all whitespace.\n    output_str = str(all_results).replace(\" \", \"\")\n    print(output_str)\n\nsolve()\n```", "id": "3618115"}, {"introduction": "After building a model and quantifying its parameter uncertainty, a crucial next step is to ask: \"Is the model any good?\" This exercise introduces the posterior predictive check (PPC), a powerful Bayesian technique for diagnosing model misspecification. You will design and implement a PPC for a gravity inversion problem, learning how to use test statistics to reveal systematic discrepancies between your model's predictions and the observed data. [@problem_id:3618119]", "problem": "Consider a one-dimensional gravity inversion model along a horizontal profile, with a two-layer subsurface discretized into rectangular cells. The goal is to design a Posterior Predictive Check (PPC) for the model by constructing a spatially coherent test statistic and to determine a decision threshold via posterior predictive p-values. The PPC must be implemented as a complete, runnable program.\n\nFundamental base and physical modeling. The vertical component of gravitational acceleration due to a point mass is derived from Newton’s law of gravitation. By superposition over volumetric cells that approximate the subsurface, the predicted vertical gravity anomaly at a surface observation location is modeled by a linear forward operator. Let there be $K$ surface observation points with horizontal positions $\\{x_k\\}_{k=1}^K$ and $N$ subsurface cells with centers at positions $\\{(x_i, z_i)\\}_{i=1}^N$, where $z_i$ is depth. Let the gravity constant be $G_{\\mathrm{N}} = 6.67430 \\times 10^{-11}$ in $\\mathrm{m}^3\\,\\mathrm{kg}^{-1}\\,\\mathrm{s}^{-2}$, and let each cell have volume $V = \\Delta x \\,\\Delta z \\, L_y$ with $\\Delta x = 100\\,\\mathrm{m}$, $\\Delta z = 100\\,\\mathrm{m}$, and out-of-plane thickness $L_y = 1000\\,\\mathrm{m}$. The cell mass is $m_i = \\rho_i V$, where $\\rho_i$ is density in $\\mathrm{kg}\\,\\mathrm{m}^{-3}$. The vertical gravity at observation $k$ due to cell $i$ is\n$$\ng_{ki} = G_{\\mathrm{N}} \\, m_i \\, \\frac{z_i}{\\left( (x_k - x_i)^2 + z_i^2 \\right)^{3/2}},\n$$\nand the total predicted anomaly at $k$ is\n$$\ng_k(\\boldsymbol{\\rho}) = \\sum_{i=1}^N G_{\\mathrm{N}} \\, (\\rho_i V) \\, \\frac{z_i}{\\left( (x_k - x_i)^2 + z_i^2 \\right)^{3/2}}.\n$$\nAll gravity values are to be handled and interpreted in $\\mathrm{m}\\,\\mathrm{s}^{-2}$.\n\nPosterior model and PPC construction. Let the posterior distribution of the density vector $\\boldsymbol{\\rho} \\in \\mathbb{R}^N$ be multivariate Gaussian with mean $\\boldsymbol{\\mu}$ and covariance $\\boldsymbol{\\Sigma}$, where\n$$\n\\Sigma_{ij} = s^2 \\exp\\left(- \\frac{d_{ij}^2}{\\ell^2} \\right) + \\nu^2 \\delta_{ij},\n$$\n$d_{ij}$ is the Euclidean distance between cell centers $(x_i,z_i)$ and $(x_j,z_j)$, $s$ is the marginal standard deviation in $\\mathrm{kg}\\,\\mathrm{m}^{-3}$, $\\ell$ is the correlation length scale in $\\mathrm{m}$, $\\nu$ is the nugget in $\\mathrm{kg}\\,\\mathrm{m}^{-3}$, and $\\delta_{ij}$ is the Kronecker delta.\n\nDefine a spatially coherent test statistic using an observation-kernel matrix $\\mathbf{W} \\in \\mathbb{R}^{K \\times K}$ constructed from observation positions $\\{x_k\\}$:\n$$\nW_{kl} = \\exp\\left( -\\frac{(x_k - x_l)^2}{2 b^2} \\right),\n$$\nwhere $b$ is a spatial bandwidth in $\\mathrm{m}$, and then renormalize $\\mathbf{W}$ so that $\\mathrm{trace}(\\mathbf{W}) = K$. For a given $\\boldsymbol{\\rho}$, define residuals $\\mathbf{r}(\\boldsymbol{\\rho}) = \\mathbf{y}_{\\mathrm{obs}} - \\mathbf{G}\\boldsymbol{\\rho}$, with $\\mathbf{G} \\in \\mathbb{R}^{K \\times N}$ denoting the forward operator with entries $G_{ki} = G_{\\mathrm{N}} V \\, z_i / \\left( (x_k - x_i)^2 + z_i^2 \\right)^{3/2}$. The test statistic is\n$$\nT(\\mathbf{y}, \\boldsymbol{\\rho}) = \\mathbf{r}(\\boldsymbol{\\rho})^\\top \\mathbf{W}\\,\\mathbf{r}(\\boldsymbol{\\rho}).\n$$\n\nPosterior predictive p-value. Draw $\\boldsymbol{\\rho}^{(s)} \\sim \\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$ for $s = 1, \\dots, S$, and for each draw generate a posterior predictive replicate\n$$\n\\mathbf{y}_{\\mathrm{rep}}^{(s)} = \\mathbf{G}\\boldsymbol{\\rho}^{(s)} + \\boldsymbol{\\varepsilon}^{(s)}, \\quad \\boldsymbol{\\varepsilon}^{(s)} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma_{\\mathrm{pred}}^2 \\mathbf{I}_K),\n$$\nwhere $\\sigma_{\\mathrm{pred}}$ is the posterior predictive noise standard deviation in $\\mathrm{m}\\,\\mathrm{s}^{-2}$, and $\\mathbf{I}_K$ is the $K \\times K$ identity matrix. Compute $T\\left(\\mathbf{y}_{\\mathrm{obs}}, \\boldsymbol{\\rho}^{(s)}\\right)$ and $T\\left(\\mathbf{y}_{\\mathrm{rep}}^{(s)}, \\boldsymbol{\\rho}^{(s)}\\right)$ for each $s$, and approximate the posterior predictive p-value by\n$$\np = \\frac{1}{S} \\sum_{s=1}^S \\mathbb{I}\\left( T\\left(\\mathbf{y}_{\\mathrm{rep}}^{(s)}, \\boldsymbol{\\rho}^{(s)}\\right) \\ge T\\left(\\mathbf{y}_{\\mathrm{obs}}, \\boldsymbol{\\rho}^{(s)}\\right) \\right),\n$$\nwhere $\\mathbb{I}(\\cdot)$ is the indicator function. Use the two-sided p-value $p_{\\mathrm{two}} = 2 \\min(p, 1-p)$ and a decision threshold $\\alpha$ such that the model is rejected if $p_{\\mathrm{two}} \\le \\alpha$ and not rejected otherwise.\n\nSetup. Use a one-dimensional profile with $K = 10$ observation points at positions $x_k = k \\Delta x$ for $k = 0, 1, \\dots, 9$, and $N = 20$ subsurface cells arranged in two depth layers at $z = 50\\,\\mathrm{m}$ and $z = 150\\,\\mathrm{m}$ with horizontal positions $x_i = i \\Delta x$ for $i$ corresponding to each layer’s index. Assume a baseline true density $\\rho_{\\mathrm{base}} = 2600\\,\\mathrm{kg}\\,\\mathrm{m}^{-3}$ with an anomalous block of amplitude $200\\,\\mathrm{kg}\\,\\mathrm{m}^{-3}$ located at the deeper layer ($z = 150\\,\\mathrm{m}$) spanning indices $i \\in \\{3,4,5,6\\}$ horizontally. Generate synthetic observed data\n$$\n\\mathbf{y}_{\\mathrm{obs}} = \\mathbf{G}\\boldsymbol{\\rho}_{\\mathrm{true}} + \\boldsymbol{\\varepsilon}_{\\mathrm{obs}}, \\quad \\boldsymbol{\\varepsilon}_{\\mathrm{obs}} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma_{\\mathrm{true}}^2 \\mathbf{I}_K),\n$$\nwith $\\sigma_{\\mathrm{true}} = 5 \\times 10^{-6}\\,\\mathrm{m}\\,\\mathrm{s}^{-2}$, and use a fixed random seed to ensure reproducibility.\n\nPosterior specification. For the posterior mean, use the same anomaly footprint at the deeper layer with amplitude $\\mu_{\\mathrm{anom}}$ and baseline $\\rho_{\\mathrm{base}}$; for cells outside the anomaly footprint, set the mean to $\\rho_{\\mathrm{base}}$. For the covariance, use $s = 150\\,\\mathrm{kg}\\,\\mathrm{m}^{-3}$ and nugget $\\nu = 50\\,\\mathrm{kg}\\,\\mathrm{m}^{-3}$, with distance $d_{ij}$ computed in the $(x,z)$ plane. The observation-kernel bandwidth is $b = 200\\,\\mathrm{m}$.\n\nTest suite. Implement the PPC for the following three parameter sets, each with $S = 2000$ posterior samples:\n- Case A (well-specified noise and moderate spatial correlation): $\\ell = 150\\,\\mathrm{m}$, $\\sigma_{\\mathrm{pred}} = 5 \\times 10^{-6}\\,\\mathrm{m}\\,\\mathrm{s}^{-2}$, $\\mu_{\\mathrm{anom}} = 180\\,\\mathrm{kg}\\,\\mathrm{m}^{-3}$, $\\alpha = 0.1$.\n- Case B (underestimated predictive noise): $\\ell = 150\\,\\mathrm{m}$, $\\sigma_{\\mathrm{pred}} = 1 \\times 10^{-6}\\,\\mathrm{m}\\,\\mathrm{s}^{-2}$, $\\mu_{\\mathrm{anom}} = 180\\,\\mathrm{kg}\\,\\mathrm{m}^{-3}$, $\\alpha = 0.1$.\n- Case C (oversmoothed posterior spatial structure): $\\ell = 500\\,\\mathrm{m}$, $\\sigma_{\\mathrm{pred}} = 5 \\times 10^{-6}\\,\\mathrm{m}\\,\\mathrm{s}^{-2}$, $\\mu_{\\mathrm{anom}} = 50\\,\\mathrm{kg}\\,\\mathrm{m}^{-3}$, $\\alpha = 0.1$.\n\nRequired outputs and units. For each case, compute the two-sided posterior predictive p-value $p_{\\mathrm{two}}$ (dimensionless, expressed as a decimal in $[0,1]$). Your program should produce a single line of output containing the three results as a comma-separated list enclosed in square brackets, for example, $\\left[ p_{\\mathrm{two},A}, p_{\\mathrm{two},B}, p_{\\mathrm{two},C} \\right]$. The gravity calculations must be performed in $\\mathrm{m}\\,\\mathrm{s}^{-2}$, the densities in $\\mathrm{kg}\\,\\mathrm{m}^{-3}$, distances in $\\mathrm{m}$, and angles are not used. No percentages are permitted; all probabilities must be decimals.\n\nDesign constraints. The implementation must start from the stated physical forward model and the Gaussian posterior definition. Do not use shortcut formulas beyond these foundations. Construct $\\mathbf{W}$ from the observation coordinates as specified and renormalize it to have $\\mathrm{trace}(\\mathbf{W}) = K$. Use a fixed random seed to ensure determinism. Ensure scientific realism by keeping the stated parameter values and units, and by using the provided test suite without modification.", "solution": "The solution implements a Posterior Predictive Check (PPC) for a geophysical gravity model. The process involves defining the physical model, generating synthetic data, specifying a posterior distribution for the model parameters, and then using a discrepancy variable (test statistic) to check for systematic differences between the model's predictions and the observed data.\n\nThe first step is to establish the geometric and physical parameters of the model. The subsurface is discretized into $N=20$ rectangular cells arranged in two layers of $10$ cells each. The upper layer is at a depth of $z=50\\,\\mathrm{m}$ and the lower layer is at $z=150\\,\\mathrm{m}$. Each cell has dimensions $\\Delta x = 100\\,\\mathrm{m}$, $\\Delta z = 100\\,\\mathrm{m}$, and an out-of-plane thickness $L_y = 1000\\,\\mathrm{m}$, giving a volume $V = 10^5\\,\\mathrm{m}^3$. Gravitational observations are made at $K=10$ surface points, located at $x_k = k \\cdot (100\\,\\mathrm{m})$ for $k \\in \\{0, 1, \\dots, 9\\}$. The cell centers are indexed $i=0, \\dots, 9$ for the shallow layer and $i=10, \\dots, 19$ for the deep layer, both with horizontal positions corresponding to the observation points.\n\nThe forward model, which maps subsurface density $\\boldsymbol{\\rho} \\in \\mathbb{R}^N$ to gravity anomalies $\\mathbf{g} \\in \\mathbb{R}^K$, is linear: $\\mathbf{g} = \\mathbf{G}\\boldsymbol{\\rho}$. The forward operator $\\mathbf{G} \\in \\mathbb{R}^{K \\times N}$ is a matrix whose entries $G_{ki}$ represent the vertical gravitational contribution of cell $i$ at observation point $k$. Using the provided point mass approximation, each entry is calculated as:\n$$\nG_{ki} = G_{\\mathrm{N}} V \\frac{z_i}{\\left( (x_k - x_i)^2 + z_i^2 \\right)^{3/2}}\n$$\nwhere $G_{\\mathrm{N}} = 6.67430 \\times 10^{-11}\\,\\mathrm{m}^3\\,\\mathrm{kg}^{-1}\\,\\mathrm{s}^{-2}$ is the gravitational constant, $(x_k, 0)$ are the coordinates of the $k$-th observation point, and $(x_i, z_i)$ are the coordinates of the center of the $i$-th cell.\n\nTo perform the PPC, we first require a set of \"observed\" data. Synthetic observed data $\\mathbf{y}_{\\mathrm{obs}}$ are generated from a known true density model $\\boldsymbol{\\rho}_{\\mathrm{true}}$. This true model consists of a background density $\\rho_{\\mathrm{base}} = 2600\\,\\mathrm{kg}\\,\\mathrm{m}^{-3}$ and a dense block with density $\\rho_{\\mathrm{base}} + 200\\,\\mathrm{kg}\\,\\mathrm{m}^{-3}$ located in the deeper layer at horizontal indices $\\{3, 4, 5, 6\\}$, corresponding to global cell indices $\\{13, 14, 15, 16\\}$. Gaussian noise with standard deviation $\\sigma_{\\mathrm{true}} = 5 \\times 10^{-6}\\,\\mathrm{m}\\,\\mathrm{s}^{-2}$ is added to the true gravity signal:\n$$\n\\mathbf{y}_{\\mathrm{obs}} = \\mathbf{G}\\boldsymbol{\\rho}_{\\mathrm{true}} + \\boldsymbol{\\varepsilon}_{\\mathrm{obs}}, \\quad \\boldsymbol{\\varepsilon}_{\\mathrm{obs}} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma_{\\mathrm{true}}^2 \\mathbf{I}_K)\n$$\n\nThe core of the PPC lies in comparing $\\mathbf{y}_{\\mathrm{obs}}$ to posterior predictive data replicates, $\\mathbf{y}_{\\mathrm{rep}}$. We begin by defining a test statistic, $T(\\mathbf{y}, \\boldsymbol{\\rho})$, designed to be sensitive to spatial-temporal misfit. A quadratic form is chosen:\n$$\nT(\\mathbf{y}, \\boldsymbol{\\rho}) = (\\mathbf{y} - \\mathbf{G}\\boldsymbol{\\rho})^\\top \\mathbf{W} (\\mathbf{y} - \\mathbf{G}\\boldsymbol{\\rho})\n$$\nThe matrix $\\mathbf{W} \\in \\mathbb{R}^{K \\times K}$ is a symmetric observation-kernel matrix that weights residuals based on their spatial proximity, with entries $W_{kl} = \\exp\\left( - (x_k - x_l)^2 / (2 b^2) \\right)$ where $b=200\\,\\mathrm{m}$. As defined, the unnormalized trace $\\sum_k W_{kk} = \\sum_k \\exp(0) = K$. Thus, the specified renormalization to ensure $\\mathrm{trace}(\\mathbf{W})=K$ is automatically satisfied but is implemented for procedural correctness.\n\nThe PPC algorithm proceeds as follows. For each of the three test cases, we define the parameters of a multivariate Gaussian posterior distribution for the density vector, $\\boldsymbol{\\rho} \\sim \\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$. The posterior mean $\\boldsymbol{\\mu}$ has the same structure as $\\boldsymbol{\\rho}_{\\mathrm{true}}$ but with a potentially different anomaly amplitude $\\mu_{\\mathrm{anom}}$. The posterior covariance matrix $\\boldsymbol{\\Sigma}$ is modeled with a squared exponential kernel and a nugget term:\n$$\n\\Sigma_{ij} = s^2 \\exp\\left(- \\frac{d_{ij}^2}{\\ell^2} \\right) + \\nu^2 \\delta_{ij}\n$$\nwhere $s=150\\,\\mathrm{kg}\\,\\mathrm{m}^{-3}$, $\\nu=50\\,\\mathrm{kg}\\,\\mathrm{m}^{-3}$, $d_{ij}$ is the Euclidean distance between cell centers $i$ and $j$, and $\\ell$ is a case-specific correlation length.\n\nFor each test case, we perform $S=2000$ iterations:\n1.  Draw a density vector sample $\\boldsymbol{\\rho}^{(s)}$ from its posterior distribution: $\\boldsymbol{\\rho}^{(s)} \\sim \\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$.\n2.  Calculate the test statistic for the observed data using this parameter draw: $T_{\\mathrm{obs}}^{(s)} = T(\\mathbf{y}_{\\mathrm{obs}}, \\boldsymbol{\\rho}^{(s)})$.\n3.  Generate a replicated dataset $\\mathbf{y}_{\\mathrm{rep}}^{(s)} = \\mathbf{G}\\boldsymbol{\\rho}^{(s)} + \\boldsymbol{\\varepsilon}^{(s)}$, where $\\boldsymbol{\\varepsilon}^{(s)} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma_{\\mathrm{pred}}^2 \\mathbf{I}_K)$ and $\\sigma_{\\mathrm{pred}}$ is case-specific.\n4.  Calculate the test statistic for the replicated data: $T_{\\mathrm{rep}}^{(s)} = T(\\mathbf{y}_{\\mathrm{rep}}^{(s)}, \\boldsymbol{\\rho}^{(s)})$. This simplifies to $T_{\\mathrm{rep}}^{(s)} = (\\boldsymbol{\\varepsilon}^{(s)})^\\top \\mathbf{W} \\boldsymbol{\\varepsilon}^{(s)}$.\n\nAfter $S$ iterations, we compute the posterior predictive p-value, which is the proportion of times the replicated statistic was more extreme than the observed statistic:\n$$\np = \\frac{1}{S} \\sum_{s=1}^S \\mathbb{I}\\left( T_{\\mathrm{rep}}^{(s)} \\ge T_{\\mathrm{obs}}^{(s)} \\right)\n$$\nFinally, the two-sided p-value, $p_{\\mathrm{two}} = 2 \\min(p, 1-p)$, is calculated. A low $p_{\\mathrm{two}}$ (e.g., below a threshold $\\alpha=0.1$) indicates that the model is unable to generate data that resembles the observed data, suggesting model misspecification. The entire procedure is implemented using vectorized `NumPy` operations for efficiency. A fixed random seed ensures reproducibility.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements a Posterior Predictive Check (PPC) for a 1D gravity inversion model.\n    \"\"\"\n    # Set a fixed random seed for reproducibility.\n    np.random.seed(42)\n\n    # 1. Setup Constants and Geometry\n    # Physical Constants\n    G_N = 6.67430e-11  # m^3 kg^-1 s^-2\n    \n    # Discretization and Geometry\n    DX = 100.0         # m\n    DZ = 100.0         # m\n    LY = 1000.0        # m\n    V = DX * DZ * LY   # m^3\n    K = 10             # Number of observation points\n    N = 20             # Number of subsurface cells\n\n    x_obs = np.arange(K) * DX\n    z_layers = np.array([50.0, 150.0])\n    \n    # Cell centers: N=20 cells, 10 per layer.\n    # Cells 0-9: z=50m; Cells 10-19: z=150m.\n    x_cell_coords = np.arange(K) * DX\n    cell_centers = np.zeros((N, 2))\n    cell_centers[:K, 0] = x_cell_coords\n    cell_centers[:K, 1] = z_layers[0]\n    cell_centers[K:, 0] = x_cell_coords\n    cell_centers[K:, 1] = z_layers[1]\n\n    # 2. Construct Forward Operator G\n    G = np.zeros((K, N))\n    for k in range(K):\n        for i in range(N):\n            xk = x_obs[k]\n            xi, zi = cell_centers[i, 0], cell_centers[i, 1]\n            denominator = ((xk - xi)**2 + zi**2)**1.5\n            G[k, i] = G_N * V * zi / denominator\n\n    # 3. Generate Synthetic Observed Data y_obs\n    rho_base = 2600.0\n    true_anomaly_amp = 200.0\n    \n    rho_true = np.full(N, rho_base)\n    # Anomaly in deeper layer (indices 10-19) at horizontal indices 3, 4, 5, 6\n    # corresponds to global indices 13, 14, 15, 16.\n    anomaly_indices = [13, 14, 15, 16]\n    rho_true[anomaly_indices] += true_anomaly_amp\n    \n    sigma_true = 5e-6\n    g_true = G @ rho_true\n    epsilon_obs = np.random.normal(0, sigma_true, size=K)\n    y_obs = g_true + epsilon_obs\n\n    # 4. Construct Observation-Kernel Matrix W\n    b = 200.0\n    W = np.zeros((K, K))\n    for k in range(K):\n        for l in range(K):\n            W[k, l] = np.exp(-((x_obs[k] - x_obs[l])**2) / (2 * b**2))\n    \n    # Renormalize W such that trace(W) = K as per instruction.\n    # Note: The trace of the original W is already K since diagonal elements are 1.\n    trace_W = np.trace(W)\n    if trace_W > 0:\n        W_norm = W * K / trace_W\n    else:\n        W_norm = W\n\n    # 5. Implement PPC for each Test Case\n    S = 2000\n    s_cov = 150.0\n    nu_cov = 50.0\n\n    # Pre-compute squared Euclidean distance matrix for covariance\n    dist_sq_matrix = np.sum((cell_centers[:, np.newaxis, :] - cell_centers[np.newaxis, :, :])**2, axis=-1)\n\n    test_cases = [\n        # Case A: ell, sigma_pred, mu_anom\n        {'ell': 150.0, 'sigma_pred': 5e-6, 'mu_anom': 180.0},\n        # Case B: ell, sigma_pred, mu_anom\n        {'ell': 150.0, 'sigma_pred': 1e-6, 'mu_anom': 180.0},\n        # Case C: ell, sigma_pred, mu_anom\n        {'ell': 500.0, 'sigma_pred': 5e-6, 'mu_anom': 50.0},\n    ]\n\n    results = []\n\n    for case in test_cases:\n        ell, sigma_pred, mu_anom = case['ell'], case['sigma_pred'], case['mu_anom']\n        \n        # a. Define Posterior Distribution\n        mu_rho = np.full(N, rho_base)\n        mu_rho[anomaly_indices] += mu_anom\n        \n        Sigma = s_cov**2 * np.exp(-dist_sq_matrix / ell**2) + np.eye(N) * nu_cov**2\n\n        # b. Sample from Posterior\n        rho_samples = np.random.multivariate_normal(mu_rho, Sigma, size=S)\n        \n        # c. PPC Calculation (vectorized)\n        \n        # T_obs for each sample\n        g_preds = rho_samples @ G.T  # Shape: (S, K)\n        r_obs_all = y_obs - g_preds  # Shape: (S, K)\n        T_obs_all = np.sum((r_obs_all @ W_norm) * r_obs_all, axis=1)\n\n        # T_rep for each sample\n        eps_rep_all = np.random.normal(0, sigma_pred, size=(S, K))\n        T_rep_all = np.sum((eps_rep_all @ W_norm) * eps_rep_all, axis=1)\n\n        # d. Calculate p-value\n        p_val = np.sum(T_rep_all >= T_obs_all) / S\n        p_two_sided = 2 * min(p_val, 1-p_val)\n        \n        results.append(p_two_sided)\n\n    # Print final output in the required format.\n    print(f\"[{','.join(f'{r:.8f}' for r in results)}]\")\n\nsolve()\n```", "id": "3618119"}, {"introduction": "In computational geophysics, our \"forward model\" is often a numerical solver with its own sources of error, which are typically ignored in statistical analyses. This advanced practice confronts this issue by integrating numerical discretization error directly into the Bayesian framework for a seismic inversion problem. You will develop a principled strategy to select a computational grid spacing that appropriately balances solver bias against statistical uncertainty, a critical skill for rigorous uncertainty quantification in complex simulations. [@problem_id:3618179]", "problem": "You are tasked with integrating numerical discretization error into a Bayesian posterior for an unknown seismic wavespeed field and designing a principled strategy to choose a grid spacing that prevents solver bias from dominating posterior credible intervals of a quantity of interest. The setting is one-dimensional seismic travel-time inversion on a line segment, with a physically plausible velocity parametrization and a measurement model that includes both observation noise and numerical discretization effects.\n\nThe forward modeling context is as follows. Consider a one-dimensional domain of length $L$ with $L = 1$ (in $\\mathrm{km}$). The seismic wavespeed is modeled as $c(x) = c_0 + c_1 \\sin(2\\pi x / L)$ for $x \\in [0, L]$, where the unknown parameter vector is $\\boldsymbol{\\theta} = (c_0, c_1)^\\top$ and $c(x)$ must remain strictly positive. The quantity of interest $q(u)$ is the first-arrival travel time $T(\\boldsymbol{\\theta})$ given by the path integral\n$$\nT(\\boldsymbol{\\theta}) = \\int_0^L \\frac{1}{c(x)} \\, dx,\n$$\nwhere the argument of $\\sin$ is in radians. Measurements are modeled as a single scalar $y$ that equals the true travel time plus observational noise with zero mean. The observational noise is modeled as Gaussian with variance $\\sigma_{\\mathrm{meas}}^2$. The units are: speed $c(x)$ in $\\mathrm{km/s}$, length in $\\mathrm{km}$, and travel time in $\\mathrm{s}$. Angles are in radians.\n\nThe numerical solver approximates $T(\\boldsymbol{\\theta})$ using the composite trapezoid rule on a uniform mesh with spacing $h$ (in $\\mathrm{km}$). The induced discretization error has leading-order bias scaling $O(h^p)$ with $p=2$ and can be estimated via a two-level calculation. In the Bayesian setting, treat the discretization error contribution for the quantity of interest $q(u)$ as an additive model discrepancy in the likelihood with a nonzero mean (bias) that scales as $O(h^p)$ and a nonnegative variance that also scales with $h^{2p}$ in a physically consistent way. The prior for $\\boldsymbol{\\theta}$ is Gaussian with mean $\\boldsymbol{m}_0$ and covariance $\\boldsymbol{\\Sigma}_0$. The posterior should be approximated by a Laplace method (Gaussian approximation around the maximum a posteriori point) using first-order sensitivities derived from the forward integral.\n\nYour tasks are:\n\n- Construct a statistically consistent posterior model for $\\boldsymbol{\\theta}$ that integrates the discretization error in the likelihood as an additive model discrepancy with nonzero mean and nonnegative variance, both scaling with $h^{p}$ in a manner supported by a two-level bias estimation procedure. Do not assume any closed-form posterior beyond the Laplace approximation. Start from Bayes' theorem and from the integral definition of $T(\\boldsymbol{\\theta})$.\n- Implement a Gauss–Newton iteration to approximate the maximum a posteriori estimator of $\\boldsymbol{\\theta}$ for a given $h$, and compute the approximate posterior covariance using the Gauss–Newton Hessian at the maximum a posteriori point.\n- For a given candidate set of grid spacings, design and implement a selection strategy to choose $h$ such that the solver bias for the quantity of interest does not dominate the posterior credible interval. Specifically, using a two-level estimate for the bias of $q(u)$ at grid spacings $h$ and $h/2$ and the approximate posterior covariance, require the ratio\n$$\nr(h) = \\frac{\\lvert b_q(h) \\rvert}{\\Delta_{0.95}(h)}\n$$\nto be less than or equal to a threshold $\\alpha$, where $b_q(h)$ is the estimated leading-order bias of $q(u)$ at grid spacing $h$ and $\\Delta_{0.95}(h)$ is the half-width of the $95\\%$ credible interval for $q(u)$ computed from the Laplace approximation of the posterior at the chosen $h$. Choose the largest $h$ in the candidate set that satisfies $r(h) \\le \\alpha$; if none satisfies the criterion, choose the smallest $h$ and report the achieved ratio.\n\n- Implement all calculations numerically. The composite trapezoid rule must be used for both the forward approximation and sensitivity integrals. Use the parameterization $c(x) = c_0 + c_1 \\sin(2\\pi x / L)$ with angles in radians. Ensure $c(x)$ remains strictly positive during optimization by enforcing $c_0 > \\lvert c_1 \\rvert$ via a minimal correction if needed.\n\nThe prior and test suite are:\n\n- Prior mean and covariance: $\\boldsymbol{m}_0 = (2.0, 0.5)^\\top$ (in $\\mathrm{km/s}$), $\\boldsymbol{\\Sigma}_0 = \\mathrm{diag}(0.3^2, 0.2^2)$ (in $(\\mathrm{km/s})^2$).\n- True speeds for generating synthetic data and measurement settings:\n    1. Case A (happy path): $c_0^\\star = 2.2$, $c_1^\\star = 0.3$, $\\sigma_{\\mathrm{meas}} = 0.002$ (in $\\mathrm{s}$), and measurement offset $\\delta = 0.001$ (in $\\mathrm{s}$).\n    2. Case B (low-noise edge): $c_0^\\star = 2.2$, $c_1^\\star = 0.3$, $\\sigma_{\\mathrm{meas}} = 0.00005$ (in $\\mathrm{s}$), and measurement offset $\\delta = 0.00002$ (in $\\mathrm{s}$).\n    3. Case C (no-heterogeneity edge): $c_0^\\star = 2.2$, $c_1^\\star = 0.0$, $\\sigma_{\\mathrm{meas}} = 0.002$ (in $\\mathrm{s}$), and measurement offset $\\delta = 0.001$ (in $\\mathrm{s}$).\n- For each case, generate the synthetic datum $y$ by computing the high-resolution travel time $T(\\boldsymbol{\\theta}^\\star)$ using a fine trapezoidal mesh with spacing $h_{\\mathrm{true}} = 1/4096$ (in $\\mathrm{km}$), then set $y = T(\\boldsymbol{\\theta}^\\star) + \\delta$.\n- Candidate grid spacings to be tested: $h \\in \\{1/32, 1/64, 1/128, 1/256\\}$ (in $\\mathrm{km}$). Use $p=2$. For the discretization error variance scaling, let it be proportional to $h^{2p}$ through a nonnegative coefficient, and use a two-level estimate to define a reasonable magnitude consistent with the estimated bias.\n\nNumerical details to implement:\n\n- Use Bayes’ theorem to form the negative log-posterior and a Gauss–Newton method to find the maximum a posteriori estimate for each candidate $h$, starting from the prior mean $\\boldsymbol{m}_0$.\n- Use the trapezoid rule for integrals and sensitivities. The sensitivity of $T(\\boldsymbol{\\theta})$ with respect to $c_0$ and $c_1$ follows from differentiating the integrand $\\left(c(x)\\right)^{-1}$ with respect to parameters and integrating over $x$.\n- Estimate the leading-order bias $b_q(h)$ for the quantity of interest $q(u) = T(\\boldsymbol{\\theta})$ using a two-level approach with spacings $h$ and $h/2$. Use this bias estimate in the likelihood as a nonzero mean model discrepancy and to compute $r(h)$.\n- Compute the $95\\%$ credible interval half-width $\\Delta_{0.95}(h)$ using the Laplace posterior covariance and the linearized propagation of uncertainty from $\\boldsymbol{\\theta}$ to $q(u)$. Use the standard normal quantile $z_{0.975}$ for $95\\%$.\n- Use the threshold $\\alpha = 0.25$ (dimensionless) in the ratio criterion.\n\nYour program should process the three cases described above and, for each case, select an $h$ from the candidate set according to the criterion $r(h) \\le \\alpha$ while maximizing $h$ to reduce computational cost. If no $h$ satisfies the criterion, choose the smallest $h$. For each case, report a two-element list containing the selected $h$ (in $\\mathrm{km}$) and the achieved ratio $r(h)$ (dimensionless). Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is itself a two-element list of floats formatted to six decimal places (for example, $\\left[[0.031250,0.123456],[0.015625,0.234567],[0.031250,0.000000]\\right]$). All angles must be in radians, all lengths in $\\mathrm{km}$, all speeds in $\\mathrm{km/s}$, and all times in $\\mathrm{s}$.", "solution": "The problem requires the construction and implementation of a Bayesian inference framework for a 1D seismic travel-time inversion problem. The key feature is the explicit modeling of numerical discretization error within the statistical model. A grid selection strategy must be designed to ensure that this numerical error does not unduly influence the posterior uncertainty of the quantity of interest.\n\n### 1. Mathematical and Statistical Formulation\n\nLet the unknown parameters be $\\boldsymbol{\\theta} = (c_0, c_1)^\\top$. The seismic wavespeed is given by the function $c(x; \\boldsymbol{\\theta}) = c_0 + c_1 \\sin(2\\pi x / L)$ for $x \\in [0, L]$, with $L=1$ km. For the speed to be physically meaningful, we must enforce $c(x) > 0$, which implies $c_0 > |c_1|$.\n\n**1.1. Forward Model and Quantity of Interest**\n\nThe quantity of interest (QoI) is the true travel time, defined by the integral:\n$$\nT(\\boldsymbol{\\theta}) = \\int_0^L \\frac{1}{c(x; \\boldsymbol{\\theta})} \\, dx\n$$\nNumerically, we approximate this integral using the composite trapezoid rule on a uniform grid with spacing $h$, yielding an approximation $T_h(\\boldsymbol{\\theta})$.\n\n**1.2. Bayesian Model**\n\nWe use Bayes' theorem to define the posterior probability distribution for $\\boldsymbol{\\theta}$:\n$$\np(\\boldsymbol{\\theta} | y, h) \\propto p(y | \\boldsymbol{\\theta}, h) \\, p(\\boldsymbol{\\theta})\n$$\nwhere $p(y | \\boldsymbol{\\theta}, h)$ is the likelihood and $p(\\boldsymbol{\\theta})$ is the prior.\n\n**1.2.1. Prior Distribution**\n\nThe prior on $\\boldsymbol{\\theta}$ is a multivariate Gaussian distribution:\n$$\np(\\boldsymbol{\\theta}) = \\mathcal{N}(\\boldsymbol{\\theta} | \\boldsymbol{m}_0, \\boldsymbol{\\Sigma}_0) \\propto \\exp\\left(-\\frac{1}{2} (\\boldsymbol{\\theta} - \\boldsymbol{m}_0)^\\top \\boldsymbol{\\Sigma}_0^{-1} (\\boldsymbol{\\theta} - \\boldsymbol{m}_0)\\right)\n$$\nwith given mean $\\boldsymbol{m}_0 = (2.0, 0.5)^\\top$ and covariance $\\boldsymbol{\\Sigma}_0 = \\text{diag}(0.3^2, 0.2^2)$.\n\n**1.2.2. Likelihood with Discretization Error Model**\n\nA measurement $y$ is related to the true travel time $T(\\boldsymbol{\\theta})$ through observation noise $\\epsilon_{\\mathrm{meas}} \\sim \\mathcal{N}(0, \\sigma_{\\mathrm{meas}}^2)$.\n$$\ny = T(\\boldsymbol{\\theta}) + \\epsilon_{\\mathrm{meas}}\n$$\nThe numerical model $T_h(\\boldsymbol{\\theta})$ differs from the true value $T(\\boldsymbol{\\theta})$ by a discretization error $\\epsilon_d(h, \\boldsymbol{\\theta}) = T_h(\\boldsymbol{\\theta}) - T(\\boldsymbol{\\theta})$. This error has a systematic component (bias) and a random component (variance). The problem asks to model this error as an additive discrepancy. We can write the measurement equation in terms of the numerical model:\n$$\ny = T_h(\\boldsymbol{\\theta}) - \\epsilon_d(h, \\boldsymbol{\\theta}) + \\epsilon_{\\mathrm{meas}}\n$$\nWe model the total residual $y - T_h(\\boldsymbol{\\theta})$ as a Gaussian random variable. Its mean is dominated by the negative of the solver bias, and its variance is the sum of measurement and discretization error variances.\n\nThe bias of the trapezoid rule scales as $O(h^p)$ with $p=2$. We can estimate the leading-order bias of $T_h(\\boldsymbol{\\theta})$ using a two-level Richardson-type formula:\n$$\nb_h(\\boldsymbol{\\theta}) = T_h(\\boldsymbol{\\theta}) - T(\\boldsymbol{\\theta}) \\approx \\frac{T_h(\\boldsymbol{\\theta}) - T_{h/2}(\\boldsymbol{\\theta})}{1 - 2^{-p}}\n$$\nFor $p=2$, this becomes $b_h(\\boldsymbol{\\theta}) = \\frac{4}{3} (T_h(\\boldsymbol{\\theta}) - T_{h/2}(\\boldsymbol{\\theta}))$.\n\nThe problem states to model the discretization error variance $\\sigma_d^2(h)$ to scale as $h^{2p}$. A reasonable choice consistent with the bias estimate is to make it proportional to the square of the bias estimate itself:\n$$\n\\sigma_d^2(h, \\boldsymbol{\\theta}) = (b_h(\\boldsymbol{\\theta}))^2\n$$\nThis couples the variance to the parameter-dependent bias and ensures the correct scaling.\n\nThe total variance for the likelihood is $\\sigma_{\\mathrm{total}}^2(h, \\boldsymbol{\\theta}) = \\sigma_{\\mathrm{meas}}^2 + \\sigma_d^2(h, \\boldsymbol{\\theta})$.\nThe likelihood is thus modeled as:\n$$\np(y | \\boldsymbol{\\theta}, h) \\propto \\exp\\left(-\\frac{1}{2} \\frac{(y - (T_h(\\boldsymbol{\\theta}) - b_h(\\boldsymbol{\\theta})))^2}{\\sigma_{\\mathrm{total}}^2(h, \\boldsymbol{\\theta})}\\right)\n$$\nThe term $T_h(\\boldsymbol{\\theta}) - b_h(\\boldsymbol{\\theta})$ is the Richardson-extrapolated, higher-order approximation of the travel time:\n$$\nT_{\\mathrm{extrap}}(\\boldsymbol{\\theta}) = T_h(\\boldsymbol{\\theta}) - \\frac{T_h(\\boldsymbol{\\theta}) - T_{h/2}(\\boldsymbol{\\theta})}{1 - 2^{-p}} = \\frac{-2^{-p} T_h(\\boldsymbol{\\theta}) + T_{h/2}(\\boldsymbol{\\theta})}{1 - 2^{-p}}\n$$\nFor $p=2$, $T_{\\mathrm{extrap}}(\\boldsymbol{\\theta}) = \\frac{4 T_{h/2}(\\boldsymbol{\\theta}) - T_h(\\boldsymbol{\\theta})}{3}$.\nThe likelihood can be expressed using this extrapolated model:\n$$\np(y | \\boldsymbol{\\theta}, h) \\propto \\exp\\left(-\\frac{1}{2} \\frac{(y - T_{\\mathrm{extrap}}(\\boldsymbol{\\theta}))^2}{\\sigma_{\\mathrm{total}}^2(h, \\boldsymbol{\\theta})}\\right)\n$$\n\n**1.3. MAP Estimation and Laplace Approximation**\n\nThe Maximum A Posteriori (MAP) estimate $\\boldsymbol{\\theta}_{\\mathrm{MAP}}$ is found by minimizing the negative log-posterior:\n$$\n\\mathcal{L}(\\boldsymbol{\\theta}) = \\frac{1}{2} \\frac{(y - T_{\\mathrm{extrap}}(\\boldsymbol{\\theta}))^2}{\\sigma_{\\mathrm{total}}^2(h, \\boldsymbol{\\theta})} + \\frac{1}{2} (\\boldsymbol{\\theta} - \\boldsymbol{m}_0)^\\top \\boldsymbol{\\Sigma}_0^{-1} (\\boldsymbol{\\theta} - \\boldsymbol{m}_0)\n$$\nWe employ a Gauss-Newton method. The gradient $\\nabla \\mathcal{L}(\\boldsymbol{\\theta})$ and Gauss-Newton Hessian $H_{GN}(\\boldsymbol{\\theta})$ are:\n$$\n\\nabla \\mathcal{L}(\\boldsymbol{\\theta}) \\approx -\\frac{y - T_{\\mathrm{extrap}}(\\boldsymbol{\\theta})}{\\sigma_{\\mathrm{total}}^2(h, \\boldsymbol{\\theta})} \\nabla T_{\\mathrm{extrap}}(\\boldsymbol{\\theta}) + \\boldsymbol{\\Sigma}_0^{-1}(\\boldsymbol{\\theta} - \\boldsymbol{m}_0)\n$$\n$$\nH_{GN}(\\boldsymbol{\\theta}) \\approx \\frac{(\\nabla T_{\\mathrm{extrap}}(\\boldsymbol{\\theta}))(\\nabla T_{\\mathrm{extrap}}(\\boldsymbol{\\theta}))^\\top}{\\sigma_{\\mathrm{total}}^2(h, \\boldsymbol{\\theta})} + \\boldsymbol{\\Sigma}_0^{-1}\n$$\nIn this formulation, the derivative of $\\sigma_{\\mathrm{total}}^2$ with respect to $\\boldsymbol{\\theta}$ is ignored, which is a standard part of the Gauss-Newton approximation. The gradient of the extrapolated model is $\\nabla T_{\\mathrm{extrap}}(\\boldsymbol{\\theta}) = \\frac{4 \\nabla T_{h/2}(\\boldsymbol{\\theta}) - \\nabla T_h(\\boldsymbol{\\theta})}{3}$. The gradients $\\nabla T_h$ and $\\nabla T_{h/2}$ are computed by differentiating the integrand of $T(\\boldsymbol{\\theta})$ and applying the trapezoid rule.\n\nThe Gauss-Newton update step is: $\\boldsymbol{\\theta}_{k+1} = \\boldsymbol{\\theta}_k - (H_{GN}(\\boldsymbol{\\theta}_k))^{-1} \\nabla \\mathcal{L}(\\boldsymbol{\\theta}_k)$.\nAfter convergence to $\\boldsymbol{\\theta}_{\\mathrm{MAP}}$, the posterior is approximated by a Gaussian (Laplace approximation): $p(\\boldsymbol{\\theta}|y,h) \\approx \\mathcal{N}(\\boldsymbol{\\theta} | \\boldsymbol{\\theta}_{\\mathrm{MAP}}, \\boldsymbol{\\Sigma}_{\\mathrm{post}})$, where the posterior covariance is $\\boldsymbol{\\Sigma}_{\\mathrm{post}} = H_{GN}(\\boldsymbol{\\theta}_{\\mathrm{MAP}})^{-1}$.\n\n### 2. Grid Selection Strategy\n\nThe goal is to select a grid spacing $h$ that balances accuracy and computational cost, ensuring the numerical bias does not dominate the statistical uncertainty.\n\nThe criterion is based on the ratio:\n$$\nr(h) = \\frac{\\lvert b_q(h) \\rvert}{\\Delta_{0.95}(h)} \\le \\alpha\n$$\nwhere $\\alpha=0.25$.\n\n-   **Bias Term $b_q(h)$**: This is the estimated bias of the QoI, calculated using the coarse grid simulation $T_h$ at the MAP estimate:\n    $$\n    b_q(h) = b_h(\\boldsymbol{\\theta}_{\\mathrm{MAP}}) = \\frac{4}{3} (T_h(\\boldsymbol{\\theta}_{\\mathrm{MAP}}) - T_{h/2}(\\boldsymbol{\\theta}_{\\mathrm{MAP}}))\n    $$\n\n-   **Credible Interval Half-Width $\\Delta_{0.95}(h)$**: This represents the posterior uncertainty in the QoI. We linearize the QoI around $\\boldsymbol{\\theta}_{\\mathrm{MAP}}$ and propagate the posterior uncertainty from $\\boldsymbol{\\theta}$ to the QoI. The QoI is best represented by our most accurate model, $T_{\\mathrm{extrap}}(\\boldsymbol{\\theta})$. The linearized posterior variance of the QoI is:\n    $$\n    \\sigma_q^2 = (\\nabla T_{\\mathrm{extrap}}(\\boldsymbol{\\theta}_{\\mathrm{MAP}}))^\\top \\boldsymbol{\\Sigma}_{\\mathrm{post}} (\\nabla T_{\\mathrm{extrap}}(\\boldsymbol{\\theta}_{\\mathrm{MAP}}))\n    $$\n    The $95\\%$ credible interval half-width is then:\n    $$\n    \\Delta_{0.95}(h) = z_{0.975} \\sqrt{\\sigma_q^2}\n    $$\n    where $z_{0.975} \\approx 1.96$ is the $97.5$-th percentile of the standard normal distribution.\n\nThe selection rule is to choose the largest $h$ from the candidate set $\\{1/32, 1/64, 1/128, 1/256\\}$ that satisfies $r(h) \\le \\alpha$. If no $h$ satisfies the condition, the smallest $h$ in the set ($h=1/256$) is chosen.\n\n### 3. Implementation Summary\n\nThe algorithm proceeds as follows for each test case:\n1.  Generate the synthetic datum $y$ using a high-resolution forward solve ($h_{\\mathrm{true}}=1/4096$).\n2.  For each candidate grid spacing $h$:\n    a. Perform Gauss-Newton iterations, starting from the prior mean $\\boldsymbol{m}_0$, to find $\\boldsymbol{\\theta}_{\\mathrm{MAP}}$. Each iteration involves computing $T_h$, $T_{h/2}$, and their gradients to form $T_{\\mathrm{extrap}}$, $\\sigma_{\\mathrm{total}}^2$, $\\nabla\\mathcal{L}$, and $H_{GN}$. A simple projection is used to enforce the constraint $c_0 > |c_1|$.\n    b. After convergence, calculate $\\boldsymbol{\\Sigma}_{\\mathrm{post}}$.\n    c. Compute the bias $b_q(h)$ and the credible interval half-width $\\Delta_{0.95}(h)$.\n    d. Compute and store the ratio $r(h)$.\n3.  Apply the selection rule to the computed pairs of $(h, r(h))$ to determine the optimal $h$ and its corresponding ratio.\n4.  Format and collect the results for all cases.\nAll integrations are performed using `numpy.trapz` for accuracy and efficiency.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Main function to run the seismic inversion problem for all test cases.\n    \"\"\"\n\n    # --- Problem Constants ---\n    L = 1.0  # Domain length in km\n    P_ORDER = 2.0  # Order of discretization error for trapezoid rule\n\n    # --- Prior Definition ---\n    prior_mean = np.array([2.0, 0.5])  # (c0, c1) in km/s\n    prior_cov = np.diag([0.3**2, 0.2**2])\n    prior_cov_inv = np.linalg.inv(prior_cov)\n\n    # --- Candidate Grid Spacings ---\n    h_candidates = np.array([1/32, 1/64, 1/128, 1/256])\n\n    # --- Grid Selection Criterion ---\n    alpha_threshold = 0.25\n    z_975 = norm.ppf(0.975)  # Quantile for 95% CI\n\n    # --- Synthetic Data Generation Parameters ---\n    h_true = 1/4096  # High-resolution grid for \"true\" data\n\n    # --- Gauss-Newton settings ---\n    gn_iterations = 10\n    constraint_eps = 1e-8\n\n\n    # --- Core Functions ---\n\n    def wavespeed(x, theta):\n        c0, c1 = theta\n        return c0 + c1 * np.sin(2 * np.pi * x / L)\n\n    def forward_model(theta, h):\n        \"\"\"Computes travel time T_h(theta) using the trapezoid rule.\"\"\"\n        if h <= 0: return np.inf\n        n_points = int(L / h) + 1\n        x = np.linspace(0, L, n_points)\n        \n        c_vals = wavespeed(x, theta)\n        if np.any(c_vals <= 0):\n            return np.inf  # Physically invalid\n            \n        integrand = 1.0 / c_vals\n        return np.trapz(integrand, x)\n\n    def forward_gradient(theta, h):\n        \"\"\"Computes gradient of T_h(theta) using the trapezoid rule.\"\"\"\n        c0, c1 = theta\n        n_points = int(L / h) + 1\n        x = np.linspace(0, L, n_points)\n        \n        c_vals = wavespeed(x, theta)\n        if np.any(c_vals <= 0):\n            return np.array([np.inf, np.inf])\n\n        inv_c_sq = c_vals**-2\n        \n        grad_c0_integrand = -inv_c_sq\n        grad_c1_integrand = -inv_c_sq * np.sin(2 * np.pi * x / L)\n        \n        grad_c0 = np.trapz(grad_c0_integrand, x)\n        grad_c1 = np.trapz(grad_c1_integrand, x)\n        \n        return np.array([grad_c0, grad_c1])\n\n    # --- Processing logic for a single case ---\n    def process_case(true_theta, sigma_meas, delta, h_candidates):\n        \n        # 1. Generate synthetic data\n        t_true = forward_model(true_theta, h_true)\n        y_obs = t_true + delta\n        \n        results_for_h = []\n        \n        for h in h_candidates:\n            h_fine = h / 2.0\n            theta_k = prior_mean.copy()\n\n            # 2. Find MAP estimate via Gauss-Newton\n            for _ in range(gn_iterations):\n                # Enforce constraint c0 > |c1|\n                if theta_k[0] <= np.abs(theta_k[1]):\n                    theta_k[0] = np.abs(theta_k[1]) + constraint_eps\n\n                # Forward models and gradients\n                T_h = forward_model(theta_k, h)\n                T_h_fine = forward_model(theta_k, h_fine)\n                grad_T_h = forward_gradient(theta_k, h)\n                grad_T_h_fine = forward_gradient(theta_k, h_fine)\n                \n                if np.isinf(T_h) or np.isinf(T_h_fine): break\n\n                # Bias and extrapolated model\n                bias_h = (T_h - T_h_fine) / (1 - 2**-P_ORDER)\n                T_extrap = T_h - bias_h\n                \n                grad_T_extrap = (grad_T_h - (grad_T_h - grad_T_h_fine) / (1 - 2**-P_ORDER))\n\n                # Total variance\n                sigma_d_sq = bias_h**2\n                sigma_total_sq = sigma_meas**2 + sigma_d_sq\n                \n                # Gradient of negative log-posterior\n                residual = y_obs - T_extrap\n                grad_log_L = - (residual / sigma_total_sq) * grad_T_extrap + \\\n                               prior_cov_inv @ (theta_k - prior_mean)\n\n                # Gauss-Newton Hessian\n                H_gn = (np.outer(grad_T_extrap, grad_T_extrap) / sigma_total_sq) + prior_cov_inv\n                \n                # Update step\n                try:\n                    update_step = np.linalg.solve(H_gn, grad_log_L)\n                    theta_k -= update_step\n                except np.linalg.LinAlgError:\n                    # If Hessian is singular, stop iterating\n                    break\n\n            theta_map = theta_k\n            \n            # --- After convergence ---\n            \n            # Recalculate final quantities at MAP\n            T_h_map = forward_model(theta_map, h)\n            T_h_fine_map = forward_model(theta_map, h_fine)\n            grad_T_h_map = forward_gradient(theta_map, h)\n            grad_T_h_fine_map = forward_gradient(theta_map, h_fine)\n\n            if np.isinf(T_h_map) or np.isinf(T_h_fine_map):\n                results_for_h.append({'h': h, 'ratio': np.inf})\n                continue\n            \n            bias_h_map = (T_h_map - T_h_fine_map) / (1 - 2**-P_ORDER)\n            sigma_d_sq_map = bias_h_map**2\n            sigma_total_sq_map = sigma_meas**2 + sigma_d_sq_map\n            \n            grad_T_extrap_map = (grad_T_h_map - (grad_T_h_map - grad_T_h_fine_map)/(1-2**-P_ORDER))\n            \n            H_gn_map = (np.outer(grad_T_extrap_map, grad_T_extrap_map) / sigma_total_sq_map) + prior_cov_inv\n            \n            # 3. Compute ratio r(h)\n            try:\n                post_cov = np.linalg.inv(H_gn_map)\n                \n                # Uncertainty propagation for QoI\n                qoi_grad = grad_T_extrap_map\n                qoi_var = qoi_grad.T @ post_cov @ qoi_grad\n                qoi_std = np.sqrt(qoi_var)\n\n                # Credible interval half-width\n                delta_95 = z_975 * qoi_std\n\n                # Final ratio\n                if delta_95 > 1e-15:\n                    ratio = np.abs(bias_h_map) / delta_95\n                else: \n                    ratio = np.inf # Bias dominates if uncertainty is zero\n\n            except np.linalg.LinAlgError:\n                ratio = np.inf\n            \n            results_for_h.append({'h': h, 'ratio': ratio})\n\n        # 4. Select h based on criterion\n        valid_h = [res for res in results_for_h if res['ratio'] <= alpha_threshold]\n        \n        if valid_h:\n            # Choose largest h that satisfies the criterion\n            best_result = max(valid_h, key=lambda x: x['h'])\n        else:\n            # If none satisfy, choose smallest h\n            best_result = min(results_for_h, key=lambda x: x['h'])\n\n        return [best_result['h'], best_result['ratio']]\n\n    # --- Test Suite ---\n    test_cases = [\n        {'name': 'Case A', 'theta_star': np.array([2.2, 0.3]), 'sigma_meas': 0.002, 'delta': 0.001},\n        {'name': 'Case B', 'theta_star': np.array([2.2, 0.3]), 'sigma_meas': 0.00005, 'delta': 0.00002},\n        {'name': 'Case C', 'theta_star': np.array([2.2, 0.0]), 'sigma_meas': 0.002, 'delta': 0.001},\n    ]\n\n    final_results = []\n    for case in test_cases:\n        result = process_case(case['theta_star'], case['sigma_meas'], case['delta'], h_candidates)\n        final_results.append(result)\n\n    # --- Final Output Formatting ---\n    output_str = \",\".join([f\"[{h:.6f},{r:.6f}]\" for h, r in final_results])\n    print(f\"[{output_str}]\")\n\n\nsolve()\n\n```", "id": "3618179"}]}