{"hands_on_practices": [{"introduction": "The continuous wavelet transform is an indispensable tool for analyzing seismic signals whose frequency content changes over time. To move from mathematical abstraction to physical insight, we must connect the wavelet's 'scale' parameter to a meaningful 'frequency'. This foundational exercise [@problem_id:3574571] guides you through the derivation of this crucial mapping for the Morlet wavelet and also addresses the practical limitation of edge effects by defining the cone of influence.", "problem": "Consider a finite-duration seismic time series of length $T$ seconds. Let $\\psi(t)$ be the Morlet mother wavelet defined by\n$$\n\\psi(t) = \\pi^{-1/4} \\exp\\!\\big(i \\,\\omega_{0}\\, t\\big)\\,\\exp\\!\\left(-\\frac{t^{2}}{2}\\right),\n$$\nwhere $\\omega_{0} > 0$ is the central angular frequency of the wavelet. The scale-$a$ wavelet is given by\n$$\n\\psi_{a}(t) = \\frac{1}{\\sqrt{a}}\\,\\psi\\!\\left(\\frac{t}{a}\\right),\n$$\nand the Continuous Wavelet Transform (CWT) of a signal $x(t)$ with respect to $\\psi(t)$ at scale $a$ and time $b$ is\n$$\nW_{x}(a,b) = \\int_{-\\infty}^{\\infty} x(t)\\,\\frac{1}{\\sqrt{a}}\\,\\psi^{*}\\!\\left(\\frac{t-b}{a}\\right)\\,\\mathrm{d}t,\n$$\nwhere $\\psi^{*}$ denotes complex conjugation. Assume standard Fourier transform conventions in angular frequency $\\omega$ such that the transform of a time-scaled function $f(t/a)/\\sqrt{a}$ is $\\sqrt{a}\\,F(a\\,\\omega)$, where $F(\\omega)$ is the Fourier transform of $f(t)$.\n\nStarting from these definitions and the scaling and modulation properties of the Fourier transform, derive the mapping between the wavelet scale $a$ and the physical frequency $f$ (in cycles per second) corresponding to the peak of the Morlet wavelet’s spectral response. Then, using the Gaussian envelope of the Morlet wavelet, define the cone of influence as the locus where the wavelet power has decayed to $\\exp(-2)$ of its maximum due to the finite time support of the wavelet, and compute the explicit boundary offset $t_{\\mathrm{COI}}(a)$ from each edge for a record of duration $T$.\n\nProvide your final answer as a two-entry row matrix containing the frequency mapping and the cone-of-influence boundary offset as functions of $a$ and $\\omega_{0}$. Express the frequency in cycles per second. No numerical approximation or rounding is required.", "solution": "The problem requires the derivation of two quantities related to the Continuous Wavelet Transform (CWT) using the Morlet wavelet: the mapping between wavelet scale and physical frequency, and the cone of influence boundary offset.\n\n**Part 1: Derivation of the Scale-to-Frequency Mapping**\n\nThe objective is to find the relationship between the wavelet scale $a$ and the physical frequency $f$ (in cycles per second) at which the spectral response of the scaled wavelet $\\psi_{a}(t)$ is maximum. This requires finding the Fourier transform of $\\psi_{a}(t)$.\n\nFirst, we find the Fourier transform of the Morlet mother wavelet, $\\psi(t)$, which is given as:\n$$\n\\psi(t) = \\pi^{-1/4} \\exp(i\\omega_{0}t) \\exp\\left(-\\frac{t^{2}}{2}\\right)\n$$\nLet $\\Psi(\\omega)$ denote the Fourier transform of $\\psi(t)$. The expression for $\\psi(t)$ is a product of a Gaussian function and a complex exponential. We use the frequency-shifting property of the Fourier transform, which states that if $g(t)$ has Fourier transform $G(\\omega)$, then the Fourier transform of $g(t)\\exp(i\\omega_{0}t)$ is $G(\\omega - \\omega_{0})$.\n\nHere, let $g(t) = \\pi^{-1/4} \\exp(-t^{2}/2)$. The standard Fourier transform of a Gaussian function $\\exp(-\\alpha t^{2})$ is $\\sqrt{\\pi/\\alpha} \\exp(-\\omega^{2}/(4\\alpha))$. For $g(t)$, the constant pre-factor is $\\pi^{-1/4}$ and the parameter $\\alpha$ in the exponent is $1/2$.\nThe Fourier transform of $\\exp(-t^{2}/2)$ is:\n$$\n\\mathcal{F}\\left\\{\\exp\\left(-\\frac{t^{2}}{2}\\right)\\right\\} = \\sqrt{\\frac{\\pi}{1/2}} \\exp\\left(-\\frac{\\omega^{2}}{4(1/2)}\\right) = \\sqrt{2\\pi} \\exp\\left(-\\frac{\\omega^{2}}{2}\\right)\n$$\nTherefore, the Fourier transform of $g(t)$ is:\n$$\nG(\\omega) = \\pi^{-1/4} \\left( \\sqrt{2\\pi} \\exp\\left(-\\frac{\\omega^{2}}{2}\\right) \\right) = 2^{1/2} \\pi^{1/4} \\exp\\left(-\\frac{\\omega^{2}}{2}\\right)\n$$\nApplying the frequency-shifting property, the Fourier transform of the mother wavelet $\\psi(t)$ is:\n$$\n\\Psi(\\omega) = G(\\omega - \\omega_{0}) = 2^{1/2} \\pi^{1/4} \\exp\\left(-\\frac{(\\omega - \\omega_{0})^{2}}{2}\\right)\n$$\nThe peak of this spectral response occurs where the magnitude $|\\Psi(\\omega)|$ is maximum. This corresponds to the minimum of the term $(\\omega - \\omega_{0})^{2}$ in the exponent, which occurs at $\\omega = \\omega_{0}$.\n\nNext, we find the Fourier transform of the scaled wavelet, $\\psi_{a}(t) = \\frac{1}{\\sqrt{a}}\\psi(t/a)$. The problem statement provides the scaling property of the Fourier transform: the transform of $f(t/a)/\\sqrt{a}$ is $\\sqrt{a}F(a\\omega)$. Let $\\Psi_{a}(\\omega)$ be the Fourier transform of $\\psi_{a}(t)$. Applying this property, we get:\n$$\n\\Psi_{a}(\\omega) = \\sqrt{a} \\Psi(a\\omega) = \\sqrt{a} \\left( 2^{1/2} \\pi^{1/4} \\exp\\left(-\\frac{(a\\omega - \\omega_{0})^{2}}{2}\\right) \\right)\n$$\n$$\n\\Psi_{a}(\\omega) = \\sqrt{2a} \\, \\pi^{1/4} \\exp\\left(-\\frac{(a\\omega - \\omega_{0})^{2}}{2}\\right)\n$$\nThe peak of the spectral magnitude $|\\Psi_{a}(\\omega)|$ occurs when the term $(a\\omega - \\omega_{0})^{2}$ in the exponent is minimized. This happens when $a\\omega - \\omega_{0} = 0$. Let $\\omega_{\\text{peak}}$ denote the angular frequency corresponding to this peak.\n$$\na\\omega_{\\text{peak}} = \\omega_{0} \\implies \\omega_{\\text{peak}} = \\frac{\\omega_{0}}{a}\n$$\nThe problem asks for the physical frequency $f$ in cycles per second (Hz). The relationship between angular frequency $\\omega$ (in radians per second) and physical frequency $f$ is $\\omega = 2\\pi f$. Therefore, the peak physical frequency $f_{\\text{peak}}$ is:\n$$\n2\\pi f_{\\text{peak}} = \\frac{\\omega_{0}}{a} \\implies f_{\\text{peak}} = \\frac{\\omega_{0}}{2\\pi a}\n$$\nThis is the mapping between the wavelet scale $a$ and the corresponding physical frequency $f$.\n\n**Part 2: Derivation of the Cone of Influence Boundary Offset**\n\nThe cone of influence (COI) represents the region in the time-scale plane where edge effects from the finite duration of the signal become significant. The problem defines the COI boundary as the locus where the wavelet power has decayed to $\\exp(-2)$ of its maximum. This decay is due to the Gaussian envelope of the wavelet.\n\nThe scaled wavelet is:\n$$\n\\psi_{a}(t) = \\frac{1}{\\sqrt{a}}\\psi\\left(\\frac{t}{a}\\right) = \\frac{1}{\\sqrt{a}} \\pi^{-1/4} \\exp\\left(i\\omega_{0}\\frac{t}{a}\\right) \\exp\\left(-\\frac{(t/a)^{2}}{2}\\right) = \\frac{1}{\\sqrt{a}} \\pi^{-1/4} \\exp\\left(i\\frac{\\omega_{0}t}{a}\\right) \\exp\\left(-\\frac{t^{2}}{2a^{2}}\\right)\n$$\nThe power of the wavelet at time $t$ is proportional to $|\\psi_{a}(t)|^{2}$.\n$$\n|\\psi_{a}(t)|^{2} = \\left|\\frac{1}{\\sqrt{a}} \\pi^{-1/4} \\exp\\left(i\\frac{\\omega_{0}t}{a}\\right) \\exp\\left(-\\frac{t^{2}}{2a^{2}}\\right)\\right|^{2}\n$$\nSince $|\\exp(i\\theta)| = 1$ for any real $\\theta$, the complex exponential term has a magnitude of $1$. The power envelope is:\n$$\n|\\psi_{a}(t)|^{2} = \\left(\\frac{1}{\\sqrt{a}} \\pi^{-1/4}\\right)^{2} \\left(\\exp\\left(-\\frac{t^{2}}{2a^{2}}\\right)\\right)^{2} = \\frac{1}{a\\sqrt{\\pi}} \\exp\\left(-\\frac{t^{2}}{a^{2}}\\right)\n$$\nThe power is a Gaussian function of time $t$, centered at $t=0$. The maximum power occurs at $t=0$:\n$$\n|\\psi_{a}(0)|^{2} = \\frac{1}{a\\sqrt{\\pi}}\n$$\nAccording to the problem's definition, the COI boundary offset, which we denote $t_{\\mathrm{COI}}(a)$, is the time at which the wavelet power drops to $\\exp(-2)$ times its maximum value. We set up the equation:\n$$\n|\\psi_{a}(t_{\\mathrm{COI}}(a))|^{2} = |\\psi_{a}(0)|^{2} \\times \\exp(-2)\n$$\nSubstituting the expressions for the power:\n$$\n\\frac{1}{a\\sqrt{\\pi}} \\exp\\left(-\\frac{t_{\\mathrm{COI}}(a)^{2}}{a^{2}}\\right) = \\frac{1}{a\\sqrt{\\pi}} \\exp(-2)\n$$\nFor this equality to hold, the exponents must be equal:\n$$\n-\\frac{t_{\\mathrm{COI}}(a)^{2}}{a^{2}} = -2\n$$\nSolving for $t_{\\mathrm{COI}}(a)$:\n$$\nt_{\\mathrm{COI}}(a)^{2} = 2a^{2}\n$$\nSince the offset must be a positive quantity representing a time duration, we take the positive root:\n$$\nt_{\\mathrm{COI}}(a) = \\sqrt{2}a\n$$\nThis time $t_{\\mathrm{COI}}(a)$ is the e-folding time for the wavelet power at scale $a$. It defines the width of the region, measured inward from each edge of the time series (at $t=0$ and $t=T$), where the wavelet analysis is affected by boundary artifacts. Thus, the explicit boundary offset from each edge is $\\sqrt{2}a$.\n\n**Final Answer Formulation**\n\nThe two requested quantities are the frequency mapping $f(a)$ and the cone-of-influence boundary offset $t_{\\mathrm{COI}}(a)$.\n1.  Frequency mapping: $f = \\frac{\\omega_{0}}{2\\pi a}$\n2.  COI boundary offset: $t_{\\mathrm{COI}}(a) = \\sqrt{2}a$\n\nThese will be presented as a two-entry row matrix.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{\\omega_{0}}{2\\pi a} & \\sqrt{2}a\n\\end{pmatrix}\n}\n$$", "id": "3574571"}, {"introduction": "Geophysical analysis often extends beyond a single time series to data from sensor arrays, allowing us to characterize the full wavefield. Delay-and-sum beamforming is the cornerstone of spatial spectral estimation, transforming array data into a map of wave power versus arrival direction. This practice [@problem_id:3574606] involves deriving the frequency-dependent beampattern, a fundamental function that quantifies an array's sensitivity and resolution.", "problem": "A linear seismic array of $M$ identical sensors is deployed along the $x$-axis at positions $x_m = m d$ for $m = 0, 1, \\dots, M-1$, where $d$ is the uniform inter-sensor spacing. The medium is homogeneous and isotropic with constant phase speed $c$. A far-field plane wave with zero-mean, wide-sense stationary source signal $s(t)$ and Power Spectral Density (PSD) $S_s(f)$ arrives from azimuth $\\theta$ measured with respect to the array axis (the $x$-axis). Under the plane-wave approximation, the signal at sensor $m$ is a delayed version of $s(t)$, i.e., $x_m(t) = s\\!\\left(t - \\tau_m(\\theta)\\right)$, where the inter-sensor delay is determined purely by geometry and propagation speed.\n\nA delay-and-sum beamformer steered to azimuth $\\theta_0$ forms\n$$\ny_{\\theta_0}(t) = \\frac{1}{M} \\sum_{m=0}^{M-1} x_m\\!\\left(t + \\tau_m(\\theta_0)\\right),\n$$\nthat is, it compensates each sensor’s signal by the hypothesized steering delay $\\tau_m(\\theta_0)$ and averages.\n\nStarting from first principles—namely, the plane-wave kinematics in a homogeneous medium, the time-delay relationship $\\tau_m(\\theta)$ implied by array geometry, and the Fourier transform time-shift property—derive the beamformer output spectrum $Y_{\\theta_0}(f)$ and the output PSD $S_y(f)$ in terms of $S_s(f)$, $f$, $M$, $d$, $c$, $\\theta$, and $\\theta_0$. Then, normalize the output PSD by $S_s(f)$ to obtain the frequency-dependent beampattern $B(f,\\theta;\\theta_0)$, defined as the output power gain for a single plane wave arriving from azimuth $\\theta$ when the beamformer is steered to $\\theta_0$.\n\nAssume no sensor self-noise and a strictly plane wavefront. Express your final answer as a single closed-form analytic expression for $B(f,\\theta;\\theta_0)$ in terms of $M$, $f$, $d$, $c$, $\\theta$, and $\\theta_0$. No numerical evaluation is required, and no units should appear in the final expression.", "solution": "First, we establish the explicit form of the time delay $\\tau_m(\\theta)$. A plane wave arriving from azimuth $\\theta$ with respect to the $x$-axis creates a right triangle with the array axis. The wavefront reaches sensor $m$ at position $x_m = m d$ after traveling an extra path length of $\\Delta L_m = x_m \\cos(\\theta)$ relative to the reference sensor at $m=0$. Given the constant phase speed $c$, the time delay at sensor $m$ is:\n$$\n\\tau_m(\\theta) = \\frac{\\Delta L_m}{c} = \\frac{x_m \\cos(\\theta)}{c} = \\frac{m d \\cos(\\theta)}{c}\n$$\nThis delay is positive for a wave traveling in the positive $x$ direction ($\\cos(\\theta)>0$) and arriving at sensor $0$ first. The signal recorded at sensor $m$ is thus $x_m(t) = s(t - \\tau_m(\\theta))$.\n\nNext, we formulate the output of the delay-and-sum beamformer, $y_{\\theta_0}(t)$. The beamformer applies a steering delay compensation $\\tau_m(\\theta_0) = \\frac{m d \\cos(\\theta_0)}{c}$ to the signal from each sensor before summing. Substituting the expression for $x_m(t)$:\n$$\ny_{\\theta_0}(t) = \\frac{1}{M} \\sum_{m=0}^{M-1} x_m(t + \\tau_m(\\theta_0)) = \\frac{1}{M} \\sum_{m=0}^{M-1} s\\left(t + \\tau_m(\\theta_0) - \\tau_m(\\theta)\\right)\n$$\nLet us define the residual time delay for each sensor as $\\Delta\\tau_m(\\theta, \\theta_0) = \\tau_m(\\theta) - \\tau_m(\\theta_0)$. The output signal simplifies to:\n$$\ny_{\\theta_0}(t) = \\frac{1}{M} \\sum_{m=0}^{M-1} s\\left(t - \\Delta\\tau_m(\\theta, \\theta_0)\\right)\n$$\nwhere\n$$\n\\Delta\\tau_m(\\theta, \\theta_0) = \\frac{m d}{c} \\left(\\cos(\\theta) - \\cos(\\theta_0)\\right)\n$$\nTo find the output spectrum, we take the Fourier transform of $y_{\\theta_0}(t)$. Let $Y_{\\theta_0}(f) = \\mathcal{F}\\{y_{\\theta_0}(t)\\}$ and $S(f) = \\mathcal{F}\\{s(t)\\}$. Using the time-shift property of the Fourier transform, $\\mathcal{F}\\{g(t-t_0)\\} = G(f)\\exp(-i 2\\pi f t_0)$, where $i$ is the imaginary unit:\n$$\nY_{\\theta_0}(f) = \\mathcal{F}\\left\\{ \\frac{1}{M} \\sum_{m=0}^{M-1} s(t - \\Delta\\tau_m) \\right\\} = \\frac{1}{M} \\sum_{m=0}^{M-1} \\mathcal{F}\\{s(t - \\Delta\\tau_m)\\}\n$$\n$$\nY_{\\theta_0}(f) = \\frac{1}{M} \\sum_{m=0}^{M-1} S(f) \\exp\\left(-i 2\\pi f \\Delta\\tau_m\\right)\n$$\nFactoring out $S(f)$ yields:\n$$\nY_{\\theta_0}(f) = S(f) \\left[ \\frac{1}{M} \\sum_{m=0}^{M-1} \\exp\\left(-i 2\\pi f \\frac{m d}{c} \\left(\\cos(\\theta) - \\cos(\\theta_0)\\right)\\right) \\right]\n$$\nThe term in the brackets is the frequency response of the beamformer, often called the array factor, which we denote as $H(f, \\theta, \\theta_0)$.\nThe Power Spectral Density (PSD) of the output signal, $S_y(f)$, is related to the input PSD, $S_s(f)$, by the squared magnitude of the system's frequency response:\n$$\nS_y(f) = S_s(f) |H(f, \\theta, \\theta_0)|^2\n$$\nThe problem defines the beampattern $B(f,\\theta;\\theta_0)$ as the ratio of the output PSD to the input PSD:\n$$\nB(f,\\theta;\\theta_0) = \\frac{S_y(f)}{S_s(f)} = |H(f, \\theta, \\theta_0)|^2 = \\left| \\frac{1}{M} \\sum_{m=0}^{M-1} \\exp\\left(-i 2\\pi f \\frac{m d}{c} \\left(\\cos(\\theta) - \\cos(\\theta_0)\\right)\\right) \\right|^2\n$$\nThe sum is a geometric series. Let $\\psi = 2\\pi \\frac{fd}{c}(\\cos(\\theta) - \\cos(\\theta_0))$. The sum is $\\sum_{m=0}^{M-1} \\exp(-i m \\psi)$. The squared magnitude of this sum is given by the formula for the Dirichlet kernel:\n$$\n\\left| \\sum_{m=0}^{M-1} \\exp(-i m \\psi) \\right|^2 = \\frac{\\sin^2(M \\psi/2)}{\\sin^2(\\psi/2)}\n$$\nSubstituting our expression for $\\psi/2$:\n$$\n\\frac{\\psi}{2} = \\pi \\frac{fd}{c}(\\cos(\\theta) - \\cos(\\theta_0))\n$$\nThus, the squared magnitude of the sum is:\n$$\n\\frac{\\sin^2\\left( M \\pi \\frac{fd}{c}(\\cos(\\theta) - \\cos(\\theta_0)) \\right)}{\\sin^2\\left( \\pi \\frac{fd}{c}(\\cos(\\theta) - \\cos(\\theta_0)) \\right)}\n$$\nFinally, we include the normalization factor $\\frac{1}{M^2}$ to obtain the beampattern:\n$$\nB(f,\\theta;\\theta_0) = \\frac{1}{M^2} \\frac{\\sin^2\\left( M \\pi \\frac{fd}{c} \\left(\\cos(\\theta) - \\cos(\\theta_0)\\right) \\right)}{\\sin^2\\left( \\pi \\frac{fd}{c} \\left(\\cos(\\theta) - \\cos(\\theta_0)\\right) \\right)}\n$$\nThis is the closed-form analytical expression for the frequency-dependent beampattern.", "answer": "$$\n\\boxed{\\frac{1}{M^{2}} \\frac{\\sin^{2}\\left(M \\pi \\frac{fd}{c} \\left(\\cos(\\theta) - \\cos(\\theta_0)\\right)\\right)}{\\sin^{2}\\left(\\pi \\frac{fd}{c} \\left(\\cos(\\theta) - \\cos(\\theta_0)\\right)\\right)}}\n$$", "id": "3574606"}, {"introduction": "A spectral estimate is incomplete without a measure of its uncertainty, especially when dealing with complex geophysical noise that violates simple theoretical assumptions. The multitaper method is a powerful estimation technique, but how reliable are its results? This advanced practice [@problem_id:3574609] challenges you to compare a classic analytical variance formula with a robust jackknife resampling approach, providing critical insight into when non-parametric error analysis is essential.", "problem": "Consider the estimation of a single complex narrowband line component embedded in noise using the multitaper method with Discrete Prolate Spheroidal Sequences (DPSS). Let the discrete-time complex signal be modeled as $x_n = A \\exp(i \\omega_0 n) + \\varepsilon_n$ for $n = 0, 1, \\dots, N-1$, where $A \\in \\mathbb{C}$ is the complex amplitude of the line, $\\omega_0$ is the true angular frequency in radians per sample, and $\\varepsilon_n$ is a zero-mean complex noise process. The time-half-bandwidth parameter is $NW$, and $K$ orthonormal DPSS tapers $w_k[n]$, for $k = 1, \\dots, K$, are applied. The demodulated tapered measurements at the line frequency are $y_k = \\sum_{n=0}^{N-1} w_k[n] x_n \\exp(-i \\omega_0 n)$, and the coherent gains are $c_k = \\sum_{n=0}^{N-1} w_k[n]$.\n\nStarting only from fundamental definitions and well-tested facts about linear estimation under additive noise and resampling-based uncertainty quantification, perform the following tasks:\n\n1. Derive a principled estimator $\\hat{A}$ for the complex amplitude $A$ using the $K$ demodulated tapered measurements $y_k$ and the coherent gains $c_k$, assuming independent additive noise across tapers. Then, under the assumption of independent and identically distributed complex circular Gaussian white noise $\\varepsilon_n$ with per-sample second moment $\\mathbb{E}[|\\varepsilon_n|^2] = \\sigma^2$, derive an analytical expression for the variance of $\\hat{A}$ in terms of $\\sigma^2$ and $\\{c_k\\}$.\n\n2. Construct a jackknife variance estimator for $\\hat{A}$ based on the $K$ leave-one-out estimators obtained by excluding each taper in turn. Treat the complex amplitude as two real parameters (real and imaginary parts), and combine their jackknife variances into a single scalar variance for the complex estimator by summation.\n\n3. Implement a Monte Carlo experiment to evaluate the reliability of the analytical variance and the jackknife variance for narrowband peaks. For each test case specified below, synthesize $M$ independent realizations of the data, compute the multitaper least-squares amplitude estimator $\\hat{A}$ for each realization, compute the jackknife variance from each realization, and estimate the empirical variance of $\\hat{A}$ across realizations. Compare the average jackknife variance to the empirical variance and compare the analytical variance to the empirical variance. Define jackknife uncertainty as \"more reliable\" if its average absolute relative error (as a decimal) with respect to the empirical variance is strictly smaller than the absolute relative error of the analytical variance.\n\nUse these well-posed test cases:\n- Case 1 (general, happy path): $N = 512$, $NW = 4$, $K = 7$, $\\omega_0 = 0.3\\pi$ (radians), $A = 0.8 + 0i$, white noise with $\\sigma^2 = 0.2$.\n- Case 2 (few tapers, boundary): $N = 512$, $NW = 2$, $K = 3$, $\\omega_0 = 0.3\\pi$ (radians), $A = 0.8 + 0i$, white noise with $\\sigma^2 = 0.2$.\n- Case 3 (colored noise, narrowband peak): $N = 512$, $NW = 4$, $K = 7$, $\\omega_0 = 0.3\\pi$ (radians), $A = 0.8 + 0i$, autoregressive of order $1$ (AR(1)) noise with parameter $\\phi = 0.9$ and stationary per-sample second moment $\\sigma^2 = 0.2$.\n- Case 4 (boundary, very few tapers and strong color): $N = 512$, $NW = 1.5$, $K = 2$, $\\omega_0 = 0.3\\pi$ (radians), $A = 0.8 + 0i$, AR(1) noise with parameter $\\phi = 0.95$ and stationary per-sample second moment $\\sigma^2 = 0.2$.\n\nIn the AR(1) cases, the complex noise is $\\varepsilon_n = \\phi \\varepsilon_{n-1} + \\eta_n$, where $\\eta_n$ is independent complex circular Gaussian innovation and chosen so that the stationary per-sample second moment satisfies $\\mathbb{E}[|\\varepsilon_n|^2] = \\sigma^2$. All angular frequencies must be treated in radians. Amplitudes and variances are dimensionless in this purely mathematical setup.\n\nImplementation details required:\n- Use $M = 200$ Monte Carlo realizations per test case.\n- DPSS tapers must be normalized to unit $\\ell_2$ norm, i.e., $\\sum_{n=0}^{N-1} |w_k[n]|^2 = 1$ for each $k$.\n- The estimator $\\hat{A}$ must be the linear least-squares estimator under the linear model defined by $\\{y_k\\}$ and $\\{c_k\\}$.\n\nYour program should produce a single line of output containing four boolean values as a comma-separated list enclosed in square brackets, in the order of the four cases above, where each boolean indicates whether the jackknife variance is more reliable than the analytical variance for that case (i.e., whether the jackknife’s average relative error is strictly smaller than the analytical variance’s relative error). For example, a valid output looks like \"[True,False,True,False]\".", "solution": "The problem requires the derivation and comparison of two variance estimators for the complex amplitude of a narrowband signal component estimated via the multitaper method. The first estimator is analytical, derived under a white noise assumption, while the second is a non-parametric jackknife resampling estimator. Their reliability is to be evaluated via a Monte Carlo simulation under different noise conditions.\n\nThe solution proceeds in three parts: first, the derivation of the least-squares amplitude estimator and its analytical variance; second, the construction of the jackknife variance estimator; and third, the design of the numerical experiment to compare them.\n\n**Part 1: Derivation of the Amplitude Estimator and Analytical Variance**\n\nThe discrete-time signal model is given by $x_n = A \\exp(i \\omega_0 n) + \\varepsilon_n$ for $n = 0, 1, \\dots, N-1$. Here, $A \\in \\mathbb{C}$ is the unknown complex amplitude, $\\omega_0$ is the known angular frequency, and $\\varepsilon_n$ is a complex noise process.\n\nThe $K$ demodulated tapered measurements are defined as $y_k = \\sum_{n=0}^{N-1} w_k[n] x_n \\exp(-i \\omega_0 n)$ for $k=1, \\dots, K$. Substituting the signal model into this definition yields:\n$$\ny_k = \\sum_{n=0}^{N-1} w_k[n] (A e^{i \\omega_0 n} + \\varepsilon_n) e^{-i \\omega_0 n} = A \\sum_{n=0}^{N-1} w_k[n] + \\sum_{n=0}^{N-1} w_k[n] \\varepsilon_n e^{-i \\omega_0 n}\n$$\nThis can be written as a linear model. Let $c_k = \\sum_{n=0}^{N-1} w_k[n]$ be the coherent gain of the $k$-th taper and $\\nu_k = \\sum_{n=0}^{N-1} w_k[n] \\varepsilon_n e^{-i \\omega_0 n}$ be the corresponding noise term. The model for the $K$ measurements becomes:\n$$\ny_k = A c_k + \\nu_k, \\quad k = 1, \\dots, K\n$$\nWe seek the linear least-squares estimator $\\hat{A}$ for the complex parameter $A$, which minimizes the sum of squared residuals, $J(A) = \\sum_{k=1}^K |y_k - A c_k|^2$. Note that the tapers $w_k[n]$ are real, so their sums $c_k$ are also real.\nTo find the minimum, we can use complex calculus, setting the derivative with respect to the complex conjugate $A^*$ to zero:\n$$\n\\frac{\\partial J(A)}{\\partial A^*} = \\frac{\\partial}{\\partial A^*} \\sum_{k=1}^K (y_k - A c_k)(y_k^* - A^* c_k) = \\sum_{k=1}^K (y_k - A c_k)(-c_k) = 0\n$$\n$$\n- \\sum_{k=1}^K y_k c_k + A \\sum_{k=1}^K c_k^2 = 0\n$$\nSolving for $A$ gives the least-squares estimator $\\hat{A}$:\n$$\n\\hat{A} = \\frac{\\sum_{k=1}^K y_k c_k}{\\sum_{k=1}^K c_k^2}\n$$\nNext, we derive the variance of this estimator, $\\mathrm{Var}(\\hat{A}) = \\mathbb{E}[|\\hat{A} - \\mathbb{E}[\\hat{A}]|^2]$. First, we find the expected value of $\\hat{A}$. Since $\\mathbb{E}[\\varepsilon_n] = 0$, it follows that $\\mathbb{E}[\\nu_k] = 0$.\n$$\n\\mathbb{E}[\\hat{A}] = \\frac{\\sum_{k=1}^K \\mathbb{E}[y_k] c_k}{\\sum_{j=1}^K c_j^2} = \\frac{\\sum_{k=1}^K (A c_k + \\mathbb{E}[\\nu_k]) c_k}{\\sum_{j=1}^K c_j^2} = \\frac{A \\sum_{k=1}^K c_k^2}{\\sum_{j=1}^K c_j^2} = A\n$$\nThe estimator is unbiased. The variance is therefore $\\mathrm{Var}(\\hat{A}) = \\mathbb{E}[|\\hat{A} - A|^2]$.\n$$\n\\hat{A} - A = \\frac{\\sum_{k=1}^K (A c_k + \\nu_k) c_k}{\\sum_{j=1}^K c_j^2} - A = \\frac{\\sum_{k=1}^K \\nu_k c_k}{\\sum_{j=1}^K c_j^2}\n$$\n$$\n\\mathrm{Var}(\\hat{A}) = \\mathbb{E}\\left[ \\left| \\frac{\\sum_k \\nu_k c_k}{\\sum_j c_j^2} \\right|^2 \\right] = \\frac{1}{\\left(\\sum_j c_j^2\\right)^2} \\mathbb{E}\\left[ \\left(\\sum_k \\nu_k c_k\\right) \\left(\\sum_l \\nu_l^* c_l\\right) \\right] = \\frac{1}{\\left(\\sum_j c_j^2\\right)^2} \\sum_{k,l} c_k c_l \\mathbb{E}[\\nu_k \\nu_l^*]\n$$\nTo evaluate $\\mathbb{E}[\\nu_k \\nu_l^*]$, we use the model assumption for this part: $\\varepsilon_n$ is IID complex circular Gaussian white noise with $\\mathbb{E}[|\\varepsilon_n|^2] = \\sigma^2$. This implies $\\mathbb{E}[\\varepsilon_n \\varepsilon_m^*] = \\sigma^2 \\delta_{nm}$.\n$$\n\\mathbb{E}[\\nu_k \\nu_l^*] = \\mathbb{E}\\left[ \\left(\\sum_n w_k[n] \\varepsilon_n e^{-i \\omega_0 n}\\right) \\left(\\sum_m w_l[m] \\varepsilon_m^* e^{i \\omega_0 m}\\right) \\right] = \\sum_{n,m} w_k[n] w_l[m] e^{-i \\omega_0 (n-m)} \\mathbb{E}[\\varepsilon_n \\varepsilon_m^*]\n$$\n$$\n\\mathbb{E}[\\nu_k \\nu_l^*] = \\sum_{n,m} w_k[n] w_l[m] e^{-i \\omega_0 (n-m)} \\sigma^2 \\delta_{nm} = \\sigma^2 \\sum_n w_k[n] w_l[n]\n$$\nThe problem states that the DPSS tapers $w_k[n]$ are orthonormal, which for real-valued tapers means $\\sum_n w_k[n] w_l[n] = \\delta_{kl}$. Therefore, $\\mathbb{E}[\\nu_k \\nu_l^*] = \\sigma^2 \\delta_{kl}$. The noise terms $\\nu_k$ are uncorrelated. Substituting this back into the variance expression:\n$$\n\\mathrm{Var}(\\hat{A}) = \\frac{1}{\\left(\\sum_j c_j^2\\right)^2} \\sum_{k,l} c_k c_l (\\sigma^2 \\delta_{kl}) = \\frac{\\sigma^2}{\\left(\\sum_j c_j^2\\right)^2} \\sum_k c_k^2 = \\frac{\\sigma^2}{\\sum_{k=1}^K c_k^2}\n$$\nThis is the analytical expression for the variance of $\\hat{A}$ under the white noise assumption.\n\n**Part 2: Construction of the Jackknife Variance Estimator**\n\nThe jackknife is a resampling technique for estimating the variance of a statistic. It involves systematically recomputing the statistic with one observation left out. Let $\\hat{A}$ be the full estimate. The leave-one-out estimators $\\hat{A}_{(j)}$ are computed by omitting the $j$-th measurement pair $(y_j, c_j)$:\n$$\n\\hat{A}_{(j)} = \\frac{\\sum_{k \\neq j} y_k c_k}{\\sum_{k \\neq j} c_k^2}, \\quad j = 1, \\dots, K\n$$\nThe problem specifies treating the complex amplitude $\\hat{A} = \\hat{A}_R + i \\hat{A}_I$ as two real parameters and summing their individual jackknife variances. The jackknife variance of a real-valued estimator $\\hat{\\theta}$ based on $K$ samples is $\\mathrm{Var}_{\\text{jack}}(\\hat{\\theta}) = \\frac{K-1}{K} \\sum_{j=1}^K (\\hat{\\theta}_{(j)} - \\bar{\\theta}_{(\\cdot)})^2$, where $\\bar{\\theta}_{(\\cdot)} = \\frac{1}{K}\\sum_j \\hat{\\theta}_{(j)}$.\nApplying this to the real and imaginary parts of $\\hat{A}$:\n$$\n\\mathrm{Var}_{\\text{jack}}(\\hat{A}_R) = \\frac{K-1}{K} \\sum_{j=1}^K (\\mathrm{Re}(\\hat{A}_{(j)}) - \\mathrm{Re}(\\bar{A}_{(\\cdot)}))^2\n$$\n$$\n\\mathrm{Var}_{\\text{jack}}(\\hat{A}_I) = \\frac{K-1}{K} \\sum_{j=1}^K (\\mathrm{Im}(\\hat{A}_{(j)}) - \\mathrm{Im}(\\bar{A}_{(\\cdot)}))^2\n$$\nwhere $\\bar{A}_{(\\cdot)} = \\frac{1}{K} \\sum_{j=1}^K \\hat{A}_{(j)}$. The total jackknife variance is the sum:\n$$\n\\mathrm{Var}_{\\text{jack}}(\\hat{A}) = \\mathrm{Var}_{\\text{jack}}(\\hat{A}_R) + \\mathrm{Var}_{\\text{jack}}(\\hat{A}_I)\n$$\n$$\n\\mathrm{Var}_{\\text{jack}}(\\hat{A}) = \\frac{K-1}{K} \\sum_{j=1}^K \\left[ (\\mathrm{Re}(\\hat{A}_{(j)}) - \\mathrm{Re}(\\bar{A}_{(\\cdot)}))^2 + (\\mathrm{Im}(\\hat{A}_{(j)}) - \\mathrm{Im}(\\bar{A}_{(\\cdot)}))^2 \\right]\n$$\nThis expression simplifies to:\n$$\n\\mathrm{Var}_{\\text{jack}}(\\hat{A}) = \\frac{K-1}{K} \\sum_{j=1}^K |\\hat{A}_{(j)} - \\bar{A}_{(\\cdot)}|^2\n$$\nThis formula provides a non-parametric estimate of the variance of $\\hat{A}$ from a single data realization. Unlike the analytical variance, its derivation does not assume that the noise components $\\nu_k$ are uncorrelated, making it potentially more robust when the underlying noise $\\varepsilon_n$ is colored.\n\n**Part 3: Monte Carlo Experiment Design**\n\nTo evaluate the reliability of the two variance estimators, we perform a Monte Carlo simulation for each of the four test cases. The procedure for a single test case is as follows:\n\n1.  **Setup**: The parameters $N, NW, K, \\omega_0, A, \\sigma^2,$ and noise type (white or AR(1) with parameter $\\phi$) are fixed. The $K$ DPSS tapers $w_k[n]$ are generated once, and their coherent gains $c_k$ are computed. The analytical variance, $\\mathrm{Var}_{\\text{an}} = \\sigma^2 / \\sum c_k^2$, is calculated. This analytical formula is theoretically valid only for white noise cases. For colored noise cases, it is based on a violated assumption and is expected to be inaccurate.\n\n2.  **Simulation Loop**: $M=200$ independent realizations are generated. For each realization $m=1, \\dots, M$:\n    a.  A noise sequence $\\varepsilon_n$ of length $N$ is synthesized.\n        -   **White Noise**: Complex circular Gaussian noise is generated by creating independent real and imaginary parts from a normal distribution $\\mathcal{N}(0, \\sigma^2/2)$.\n        -   **AR(1) Noise**: The process is $\\varepsilon_n = \\phi \\varepsilon_{n-1} + \\eta_n$. To achieve a stationary variance of $\\mathbb{E}[|\\varepsilon_n|^2] = \\sigma^2$, the complex circular white noise innovations $\\eta_n$ must have variance $\\mathbb{E}[|\\eta_n|^2] = \\sigma^2 (1-|\\phi|^2)$. The sequence $\\varepsilon_n$ is generated by filtering the innovations $\\eta_n$ with a recursive filter defined by the denominator polynomial $1 - \\phi z^{-1}$.\n    b.  The full signal $x_n = A e^{i \\omega_0 n} + \\varepsilon_n$ is formed.\n    c.  The $K$ measurements $y_k$ are computed, and from these, the full amplitude estimate $\\hat{A}^{(m)}$ and the single-realization jackknife variance estimate $\\mathrm{Var}_{\\text{jack}}^{(m)}$ are calculated using the formulas derived above.\n\n3.  **Analysis**: After the loop, we have a collection of $M$ estimates $\\{\\hat{A}^{(m)}\\}$ and $M$ jackknife variances $\\{\\mathrm{Var}_{\\text{jack}}^{(m)}\\}$.\n    a.  The **empirical variance** of $\\hat{A}$ is computed from the sample of estimates. This serves as our \"ground truth\" for the variance: $\\mathrm{Var}_{\\text{emp}} = \\frac{1}{M-1} \\sum_{m=1}^M |\\hat{A}^{(m)} - \\bar{A}|^2$, where $\\bar{A} = \\frac{1}{M} \\sum_m \\hat{A}^{(m)}$.\n    b.  The **average jackknife variance** is computed: $\\overline{\\mathrm{Var}}_{\\text{jack}} = \\frac{1}{M} \\sum_{m=1}^M \\mathrm{Var}_{\\text{jack}}^{(m)}$.\n    c.  The reliabilities are compared using the absolute relative error with respect to the empirical variance:\n        $$\n        E_{\\text{an}} = \\left| \\frac{\\mathrm{Var}_{\\text{an}} - \\mathrm{Var}_{\\text{emp}}}{\\mathrm{Var}_{\\text{emp}}} \\right|, \\quad E_{\\text{jack}} = \\left| \\frac{\\overline{\\mathrm{Var}}_{\\text{jack}} - \\mathrm{Var}_{\\text{emp}}}{\\mathrm{Var}_{\\text{emp}}} \\right|\n        $$\n    d.  The jackknife method is deemed \"more reliable\" for a given test case if $E_{\\text{jack}} < E_{\\text{an}}$. The boolean result is recorded.\n\nThis process is repeated for all four cases, and the final output is a list of four booleans.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.signal.windows import dpss\nfrom scipy.signal import lfilter\n\ndef run_monte_carlo(N, NW, K, omega0, A_true, sigma_sq, phi, M):\n    \"\"\"\n    Performs a Monte Carlo simulation for one test case.\n\n    Returns:\n        bool: True if the jackknife variance is more reliable than the analytical variance.\n    \"\"\"\n    # 1. Generate DPSS tapers and coherent gains\n    # The dpss function returns int(2*NW) tapers. We select the first K.\n    tapers = dpss(N, NW, K)\n    \n    # Coherent gains (c_k)\n    c = np.sum(tapers, axis=1)\n\n    # 2. Calculate analytical variance (based on white noise assumption)\n    var_analytical = sigma_sq / np.sum(c**2)\n\n    # 3. Monte Carlo loop\n    estimates_A_hat = np.zeros(M, dtype=np.complex128)\n    variances_jack = np.zeros(M)\n    \n    n_indices = np.arange(N)\n    signal_component = A_true * np.exp(1j * omega0 * n_indices)\n\n    # Set up random number generator for reproducibility\n    rng = np.random.default_rng(seed=42)\n\n    for m in range(M):\n        # a. Generate noise\n        if phi == 0:  # White noise case\n            noise_real = rng.normal(scale=np.sqrt(sigma_sq / 2), size=N)\n            noise_imag = rng.normal(scale=np.sqrt(sigma_sq / 2), size=N)\n            noise = noise_real + 1j * noise_imag\n        else:  # AR(1) colored noise case\n            var_eta = sigma_sq * (1 - phi**2)\n            eta_real = rng.normal(scale=np.sqrt(var_eta / 2), size=N)\n            eta_imag = rng.normal(scale=np.sqrt(var_eta / 2), size=N)\n            eta = eta_real + 1j * eta_imag\n            # Generate AR(1) process using a filter\n            # The filter implements eps[n] = phi * eps[n-1] + eta[n]\n            # with initial condition eps[-1] = 0\n            noise = lfilter([1], [1, -phi], eta)\n        \n        # b. Synthesize signal\n        x = signal_component + noise\n\n        # c. Compute demodulated measurements y_k\n        y_k = np.zeros(K, dtype=np.complex128)\n        demodulator = np.exp(-1j * omega0 * n_indices)\n        for k in range(K):\n            y_k[k] = np.sum(tapers[k, :] * x * demodulator)\n\n        # d. Compute full estimate A_hat\n        A_hat_m = np.sum(y_k * c) / np.sum(c**2)\n        estimates_A_hat[m] = A_hat_m\n\n        # e. Compute jackknife variance for this realization\n        A_hat_leave_one_out = np.zeros(K, dtype=np.complex128)\n        k_indices = np.arange(K)\n        for j in range(K):\n            # Indices for leave-one-out subset\n            subset_idx = k_indices != j\n            y_subset = y_k[subset_idx]\n            c_subset = c[subset_idx]\n            A_hat_leave_one_out[j] = np.sum(y_subset * c_subset) / np.sum(c_subset**2)\n        \n        A_hat_loo_mean = np.mean(A_hat_leave_one_out)\n        var_jack_m = (K - 1) / K * np.sum(np.abs(A_hat_leave_one_out - A_hat_loo_mean)**2)\n        variances_jack[m] = var_jack_m\n\n    # 4. Post-processing\n    # Empirical variance (sample variance with ddof=1)\n    var_empirical = np.var(estimates_A_hat, ddof=1)\n\n    # Average jackknife variance\n    avg_var_jack = np.mean(variances_jack)\n\n    # 5. Compare errors\n    err_analytical = np.abs((var_analytical - var_empirical) / var_empirical)\n    err_jackknife = np.abs((avg_var_jack - var_empirical) / var_empirical)\n\n    return err_jackknife  err_analytical\n\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: white noise, general\n        {'N': 512, 'NW': 4, 'K': 7, 'omega0': 0.3 * np.pi, 'A_true': 0.8 + 0j, 'sigma_sq': 0.2, 'phi': 0.0},\n        # Case 2: white noise, few tapers\n        {'N': 512, 'NW': 2, 'K': 3, 'omega0': 0.3 * np.pi, 'A_true': 0.8 + 0j, 'sigma_sq': 0.2, 'phi': 0.0},\n        # Case 3: colored noise, general\n        {'N': 512, 'NW': 4, 'K': 7, 'omega0': 0.3 * np.pi, 'A_true': 0.8 + 0j, 'sigma_sq': 0.2, 'phi': 0.9},\n        # Case 4: colored noise, few tapers, strong color\n        {'N': 512, 'NW': 1.5, 'K': 2, 'omega0': 0.3 * np.pi, 'A_true': 0.8 + 0j, 'sigma_sq': 0.2, 'phi': 0.95},\n    ]\n\n    M = 200  # Number of Monte Carlo realizations\n\n    results = []\n    for case in test_cases:\n        is_jackknife_more_reliable = run_monte_carlo(\n            N=case['N'],\n            NW=case['NW'],\n            K=case['K'],\n            omega0=case['omega0'],\n            A_true=case['A_true'],\n            sigma_sq=case['sigma_sq'],\n            phi=case['phi'],\n            M=M\n        )\n        results.append(is_jackknife_more_reliable)\n\n    # Final print statement in the exact required format.\n    # The output format must be a string like \"[True,False,True,False]\"\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3574609"}]}