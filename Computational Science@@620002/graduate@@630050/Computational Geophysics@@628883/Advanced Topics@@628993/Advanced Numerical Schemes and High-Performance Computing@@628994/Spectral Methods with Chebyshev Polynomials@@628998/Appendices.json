{"hands_on_practices": [{"introduction": "This first practice establishes the foundation of all spectral methods: high-order polynomial interpolation. You will implement the procedure to approximate a smooth function using a Chebyshev series, leveraging the computational power of the Discrete Cosine Transform to map nodal values to spectral coefficients [@problem_id:3614946]. This exercise is crucial for understanding the mechanics of spectral representations and witnessing their hallmark exponential convergence for analytic functions.", "problem": "Consider the task of constructing a spectral interpolant for the analytic function $f(x)=e^{x}$ on the interval $[-1,1]$ using Chebyshev polynomials of the first kind. This task is representative of a core step in spectral methods widely used in Computational Geophysics for high-accuracy approximation of smooth fields (for instance, stratified velocity profiles or temperature distributions), where Chebyshev expansions provide exponential convergence for analytic functions.\n\nYou must start from the following fundamental base:\n- The Chebyshev polynomials of the first kind $\\{T_n(x)\\}_{n \\ge 0}$ are defined by the relations $T_0(x)=1$, $T_1(x)=x$, and the recurrence $T_{n+1}(x)=2x\\,T_n(x)-T_{n-1}(x)$, and equivalently $T_n(x)=\\cos(n\\arccos x)$.\n- The Chebyshev–Lobatto nodes are $x_j=\\cos\\left(\\frac{\\pi j}{N}\\right)$ for $j=0,1,\\dots,N$, with $N\\in\\mathbb{N}$ and angles in radians.\n- The pointwise values $\\{f(x_j)\\}$ at Chebyshev–Lobatto nodes admit a discrete cosine representation that can be mapped to Chebyshev coefficients by the Discrete Cosine Transform of Type I (DCT-I), leveraging the discrete orthogonality of $\\{\\cos(k\\theta)\\}$ on the grid $\\theta_j=\\frac{\\pi j}{N}$ when endpoints are taken with half-weight.\n\nYour goal is to implement, from first principles, the mapping from gridded data to the Chebyshev interpolant and evaluate its accuracy. In particular:\n1. For a given integer $N\\geq 1$, form the $N+1$ Chebyshev–Lobatto nodes $x_j=\\cos\\left(\\frac{\\pi j}{N}\\right)$ and compute the samples $f_j=f(x_j)$ of $f(x)=e^x$.\n2. Compute the Chebyshev interpolant coefficients $\\{a_n\\}_{n=0}^{N}$ from $\\{f_j\\}_{j=0}^{N}$ using the Discrete Cosine Transform of Type I (DCT-I). You must correctly account for the endpoint half-weights implied by the discrete orthogonality on Chebyshev–Lobatto nodes when reconstructing the interpolant.\n3. Evaluate the interpolant $P_N(x)$ at $x=0.8$ using a numerically stable summation strategy consistent with the recurrence of Chebyshev polynomials. Carefully handle the endpoint contributions so that the interpolant evaluation matches the interpolation conditions at the nodes.\n4. Estimate the pointwise error at $x=0.8$ as $E_N=\\left|P_N(0.8)-e^{0.8}\\right|$.\n\nAngle measures in all trigonometric expressions must be in radians. No physical units are involved.\n\nTest Suite:\nCompute $E_N$ for the following values of $N$ to exercise different facets of the implementation:\n- Boundary case: $N=1$.\n- Small sizes: $N=2$, $N=4$.\n- Moderate sizes: $N=10$, $N=20$.\n- Larger sizes for high accuracy and non-power-of-two coverage: $N=64$, $N=127$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the seven error values $[E_{1},E_{2},E_{4},E_{10},E_{20},E_{64},E_{127}]$ as a comma-separated list enclosed in square brackets, in decimal form (floats). For example: \"[0.1,0.01,0.001,0.0001,0.00001,1e-6,1e-7]\". Your program must be fully self-contained, require no user input, and implement the computations as specified above using DCT-I to obtain the interpolant coefficients.", "solution": "The objective is to construct a Chebyshev polynomial interpolant for the analytic function $f(x) = e^x$ on the interval $[-1, 1]$ for various polynomial degrees $N$, and to evaluate the pointwise accuracy of this interpolant at $x=0.8$. The procedure involves four primary steps: sampling the function at Chebyshev-Lobatto nodes, computing the Chebyshev coefficients of the interpolant via the Discrete Cosine Transform (DCT-I), evaluating the resulting polynomial sum using a stable algorithm, and calculating the approximation error.\n\n**Step 1: Chebyshev-Lobatto Nodes and Function Sampling**\n\nFor a chosen polynomial degree $N \\in \\mathbb{N}$, the interpolant is constructed to match the function $f(x)$ at $N+1$ specific points. The Chebyshev-Lobatto nodes are an optimal choice for this purpose, minimizing the Runge phenomenon for polynomial interpolation. These nodes are the extrema of the Chebyshev polynomial $T_N(x)$ on the interval $[-1,1]$. They are defined as:\n$$\nx_j = \\cos\\left(\\frac{\\pi j}{N}\\right) \\quad \\text{for } j = 0, 1, \\dots, N\n$$\nThe angles are taken in radians. The function $f(x) = e^x$ is then sampled at these $N+1$ nodes to obtain the data points $\\{f_j\\}_{j=0}^N$, where $f_j = f(x_j) = e^{x_j}$.\n\n**Step 2: Computation of Chebyshev Coefficients**\n\nThe Chebyshev interpolating polynomial of degree $N$ is denoted by $P_N(x)$. It can be expressed as a linear combination of the first $N+1$ Chebyshev polynomials of the first kind, $\\{T_n(x)\\}_{n=0}^N$:\n$$\nP_N(x) = \\sum_{n=0}^{N} c_n T_n(x)\n$$\nThe interpolation conditions are $P_N(x_j) = f_j$ for $j=0, \\dots, N$. This leads to a system of linear equations for the coefficients $\\{c_n\\}$. A more direct and numerically efficient method to find these coefficients leverages the connection between Chebyshev polynomials and the cosine function, $T_n(\\cos\\theta) = \\cos(n\\theta)$. Substituting $x_j = \\cos(\\frac{\\pi j}{N})$ into the interpolation conditions yields:\n$$\nf_j = \\sum_{n=0}^{N} c_n T_n(x_j) = \\sum_{n=0}^{N} c_n \\cos\\left(\\frac{n \\pi j}{N}\\right)\n$$\nThis is a finite discrete cosine series. The coefficients can be found by exploiting the discrete orthogonality of the cosine functions on the grid points $\\theta_j = \\frac{\\pi j}{N}$. The standard formulation of the interpolant is often written with halved contributions from the first and last basis functions:\n$$\nP_N(x) = \\sum_{n=0}^{N} {}^{'} a_n T_n(x) \\equiv \\frac{1}{2}a_0 T_0(x) + \\sum_{n=1}^{N-1} a_n T_n(x) + \\frac{1}{2}a_N T_N(x)\n$$\nThe corresponding coefficients $\\{a_n\\}$ are given by a discrete transform:\n$$\na_n = \\frac{2}{N} \\sum_{j=0}^{N} {}^{''} f_j T_n(x_j) = \\frac{2}{N} \\sum_{j=0}^{N} {}^{''} f_j \\cos\\left(\\frac{n \\pi j}{N}\\right)\n$$\nwhere the double prime on the summation indicates that the terms for $j=0$ and $j=N$ are multiplied by $\\frac{1}{2}$. This summation is directly related to the Discrete Cosine Transform of Type I (DCT-I). For a data sequence $\\{f_j\\}_{j=0}^N$, the DCT-I is defined as:\n$$\nY_n = f_0 + (-1)^n f_N + 2 \\sum_{j=1}^{N-1} f_j \\cos\\left(\\frac{n \\pi j}{N}\\right)\n$$\nBy inspection, the sum in the formula for $a_n$ is equal to $\\frac{1}{2}Y_n$. Therefore, the coefficients are:\n$$\na_n = \\frac{2}{N} \\left(\\frac{1}{2} Y_n\\right) = \\frac{Y_n}{N}\n$$\nTo evaluate the polynomial sum in its standard form $\\sum_{n=0}^N c_n T_n(x)$, we set $c_n$ to be the coefficients of each basis function $T_n(x)$. From the formulation with the primed summation, we have:\n$$\nc_0 = \\frac{1}{2}a_0 = \\frac{Y_0}{2N}, \\quad c_N = \\frac{1}{2}a_N = \\frac{Y_N}{2N}, \\quad \\text{and} \\quad c_n = a_n = \\frac{Y_n}{N} \\quad \\text{for } 1 \\le n \\le N-1.\n$$\nThese coefficients $\\{c_n\\}_{n=0}^N$ will be used in the evaluation step.\n\n**Step 3: Stable Polynomial Evaluation via Clenshaw's Algorithm**\n\nTo evaluate $P_N(x) = \\sum_{n=0}^{N} c_n T_n(x)$ at the specified point $x = 0.8$, a naive summation would be inefficient and potentially numerically unstable. The prescribed stable method is Clenshaw's algorithm, which leverages the three-term recurrence relation of the Chebyshev polynomials: $T_{n+1}(x) = 2x T_n(x) - T_{n-1}(x)$ for $n \\ge 1$.\n\nThe algorithm proceeds as follows to compute the sum $S$:\n1. Initialize two variables, $u_{k+2} = 0$ and $u_{k+1} = 0$.\n2. Iterate downwards from $k=N$ to $k=0$:\n   $$ u_k = c_k + 2x u_{k+1} - u_{k+2} $$\n   In each step, the old $u_{k+1}$ becomes the new $u_{k+2}$ and the old $u_k$ becomes the new $u_{k+1}$.\n3. After the loop completes, the value of the sum is given by:\n   $$ P_N(x) = u_0 - x u_1 $$\nThis algorithm avoids explicit computation of $T_n(x)$ and is both efficient (requiring $O(N)$ operations) and numerically stable.\n\n**Step 4: Error Estimation**\n\nThe final step is to quantify the accuracy of the interpolation. The pointwise error $E_N$ at $x=0.8$ is calculated as the absolute difference between the interpolated value $P_N(0.8)$ and the true function value $f(0.8)$:\n$$\nE_N = |P_N(0.8) - e^{0.8}|\n$$\nThis entire procedure is repeated for each value of $N$ in the test suite: $N \\in \\{1, 2, 4, 10, 20, 64, 127\\}$. The exponential convergence characteristic of spectral methods for analytic functions should be observable as a rapid decrease in $E_N$ as $N$ increases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.fft import dct\n\ndef solve():\n    \"\"\"\n    Computes the error of Chebyshev interpolation for f(x)=e^x at x=0.8\n    for a suite of polynomial degrees N.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [1, 2, 4, 10, 20, 64, 127]\n    \n    # The point at which to evaluate the interpolant and its error.\n    eval_point = 0.8\n    true_value = np.exp(eval_point)\n\n    results = []\n    \n    for N in test_cases:\n        # Step 1: Form the N+1 Chebyshev–Lobatto nodes and compute function samples.\n        # j is an array [0, 1, ..., N]\n        j = np.arange(N + 1)\n        # x_j = cos(pi*j/N)\n        nodes = np.cos(np.pi * j / N)\n        # f_j = f(x_j) = e^(x_j)\n        f_values = np.exp(nodes)\n\n        # Step 2: Compute the Chebyshev interpolant coefficients using DCT-I.\n        # The DCT-I of the function values {f_j} gives {Y_n}.\n        # Y_n = f_0 + (-1)^n f_N + 2 * sum_{j=1}^{N-1} f_j * cos(n*pi*j/N)\n        # The scipy.fft.dct function with type=1 computes exactly this.\n        dct_vals = dct(f_values, type=1)\n\n        # The coefficients {a_n} of the interpolant form P_N(x)=sum'{a_n T_n(x)}\n        # are a_n = Y_n / N.\n        # For evaluation with a standard sum P_N(x)=sum{c_n T_n(x)}, the\n        # coefficients are c_0 = a_0/2, c_N = a_N/2, and c_n = a_n otherwise.\n        # This is equivalent to scaling Y_0 and Y_N by 1/(2N) and other Y_n by 1/N.\n        \n        # Guard against division by zero if N=0, though not in test cases.\n        if N == 0:\n            # For N=0, P_0(x) is a constant f(x_0)=f(1)=e.\n            # In this special case, c_0 = f_0 = e. Error is |e - e^0.8|.\n            # This logic block is for completeness and not required by the test suite.\n            p_N_at_point = f_values[0]\n            error = np.abs(p_N_at_point - true_value)\n            results.append(error)\n            continue\n            \n        cheb_coeffs = dct_vals / N\n        cheb_coeffs[0] /= 2.0\n        cheb_coeffs[N] /= 2.0\n        \n        # Step 3: Evaluate the interpolant P_N(x) at x=0.8 using Clenshaw's algorithm.\n        # This evaluates the sum sum_{n=0 to N} c_n T_n(x).\n        u_k_plus_2 = 0.0\n        u_k_plus_1 = 0.0\n        # Iterate downwards from k=N to 0\n        for k in range(N, -1, -1):\n            # Recurrence: u_k = c_k + 2*x*u_{k+1} - u_{k+2}\n            u_k = cheb_coeffs[k] + 2.0 * eval_point * u_k_plus_1 - u_k_plus_2\n            # Update for next iteration\n            u_k_plus_2 = u_k_plus_1\n            u_k_plus_1 = u_k\n        \n        # The final value of the sum is u_0 - x*u_1\n        # At the end of the loop, u_k_plus_1 holds u_0 and u_k_plus_2 holds u_1.\n        p_N_at_point = u_k_plus_1 - eval_point * u_k_plus_2\n\n        # Step 4: Estimate the pointwise error at x=0.8.\n        error = np.abs(p_N_at_point - true_value)\n        results.append(error)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3614946"}, {"introduction": "Building on the properties of Chebyshev polynomials, this exercise explores their application in numerical integration through Gauss-Chebyshev quadrature. You will derive and implement this powerful technique to approximate a weighted integral, demonstrating how the choice of quadrature points as polynomial roots leads to exceptional accuracy [@problem_id:3614882]. This practice is vital for developing skills in both theoretical derivation and stable numerical implementation, which are often required when assembling spectral discretizations of differential equations.", "problem": "Consider the weighted integral with Chebyshev weight of the first kind, defined by\n$$\nI \\equiv \\int_{-1}^{1} \\frac{\\ln(1+x)}{\\sqrt{1-x^2}} \\, dx.\n$$\nYour task is to start from foundational principles in spectral methods with Chebyshev polynomials and construct a complete derivation and algorithm that leads to a numerically stable and accurate approximation of this integral using Gauss–Chebyshev quadrature of the first kind. Then, compute and compare the numerical estimates against the analytic value.\n\nFundamental base and requirements:\n- Use the defining relation of Chebyshev polynomials of the first kind: for any integer $k \\ge 0$, $T_k(x) = \\cos(k \\arccos x)$, where $x \\in [-1,1]$.\n- Use the orthogonality of Chebyshev polynomials with respect to the weight $w(x) = (1-x^2)^{-1/2}$ on $[-1,1]$.\n- Use only the fact that Gaussian quadrature rules associated with orthogonal polynomials of a given weight provide exact integration for all polynomials up to degree $2n-1$ when $n$ nodes are used. Do not assume specific formulas for nodes or weights; derive them.\n- Use the angle substitution $x=\\cos\\theta$ with $\\theta \\in [0,\\pi]$ where needed. All angles must be handled in radians.\n\nPart A (derivation):\n1. Starting from the definition $T_k(x) = \\cos(k \\arccos x)$ and the substitution $x=\\cos\\theta$, derive the orthogonality of $T_k$ on $[-1,1]$ with respect to $w(x) = (1-x^2)^{-1/2}$. Then, derive from first principles the $n$-point Gauss–Chebyshev quadrature of the first kind by transforming the weighted integral to an unweighted integral in the $\\theta$-variable and identifying the corresponding quadrature nodes and weights. Express the result clearly in terms of nodes $x_k$ and weights $w_k$ that depend only on $n$, and specify that angles are in radians.\n2. Evaluate the integral $I$ analytically in closed form by a suitable change of variables and established integrals of logarithmic-trigonometric functions.\n\nPart B (algorithm design and numerical experiment):\n1. Based on your derived quadrature nodes and weights, design a numerically stable algorithm to approximate $I$ for a given $n$. Address any floating-point sensitivity near $x=-1$ by an appropriate evaluation strategy for $\\ln(1+x)$.\n2. Implement this algorithm as a program that computes the absolute error\n$$\nE_n \\equiv \\left| I_n - I \\right|,\n$$\nwhere $I_n$ is the $n$-point Gauss–Chebyshev approximation and $I$ is the analytic value from Part A.\n3. Use the following test suite for $n$:\n- $n=1$ (boundary case),\n- $n=2$,\n- $n=4$,\n- $n=8$,\n- $n=16$,\n- $n=64$.\n4. Your program must compute $E_n$ for each $n$ above and produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order shown above, for example,\n\"[E1,E2,E3,E4,E5,E6]\".\nEach $E_n$ must be output as a floating-point number.\n\nNotes:\n- This problem is purely mathematical; no physical units are involved.\n- All angles in any trigonometric expressions must be handled in radians.\n- The final answer must be a complete, runnable program that produces the specified single-line output.", "solution": "The problem requires a comprehensive derivation of the Gauss-Chebyshev quadrature of the first kind and its application to approximate a specific weighted integral. The task is divided into a theoretical derivation (Part A) and an algorithmic implementation with numerical error analysis (Part B).\n\nPart A: Derivation and Analytic Solution\n\n1. Derivation of Gauss-Chebyshev Quadrature of the First Kind\n\nWe begin by establishing the orthogonality of Chebyshev polynomials of the first kind, $T_k(x)$, with respect to the weight function $w(x) = (1-x^2)^{-1/2}$ on the interval $[-1, 1]$.\n\nThe defining relation for the Chebyshev polynomial of degree $k$ is $T_k(x) = \\cos(k \\arccos x)$. Let us consider the integral of the product of two such polynomials, $T_j(x)$ and $T_k(x)$, with the specified weight function:\n$$\n\\mathcal{I}_{jk} = \\int_{-1}^{1} T_j(x) T_k(x) \\frac{1}{\\sqrt{1-x^2}} \\, dx\n$$\nWe perform the change of variables $x = \\cos\\theta$. This implies $\\arccos x = \\theta$. As $x$ varies from $-1$ to $1$, $\\theta$ varies from $\\pi$ to $0$. The differential is $dx = -\\sin\\theta \\, d\\theta$. The term $\\sqrt{1-x^2}$ becomes $\\sqrt{1-\\cos^2\\theta} = \\sqrt{\\sin^2\\theta} = \\sin\\theta$, since $\\theta \\in [0, \\pi]$ and $\\sin\\theta \\ge 0$ on this interval.\n\nSubstituting these into the integral, we get:\n$$\n\\mathcal{I}_{jk} = \\int_{\\pi}^{0} \\cos(j\\theta) \\cos(k\\theta) \\frac{1}{\\sin\\theta} (-\\sin\\theta \\, d\\theta)\n$$\nReversing the limits of integration cancels the negative sign:\n$$\n\\mathcal{I}_{jk} = \\int_{0}^{\\pi} \\cos(j\\theta) \\cos(k\\theta) \\, d\\theta\n$$\nWe use the trigonometric identity $\\cos(A)\\cos(B) = \\frac{1}{2}[\\cos(A+B) + \\cos(A-B)]$:\n$$\n\\mathcal{I}_{jk} = \\frac{1}{2} \\int_{0}^{\\pi} \\left[ \\cos((j+k)\\theta) + \\cos((j-k)\\theta) \\right] \\, d\\theta\n$$\nWe evaluate this integral for three cases, assuming $j, k$ are non-negative integers.\n\nCase 1: $j \\neq k$.\nSince $j$ and $k$ are distinct non-negative integers, $j+k > 0$ and $j-k \\neq 0$.\n$$\n\\mathcal{I}_{jk} = \\frac{1}{2} \\left[ \\frac{\\sin((j+k)\\theta)}{j+k} + \\frac{\\sin((j-k)\\theta)}{j-k} \\right]_{0}^{\\pi} = \\frac{1}{2} [ (0-0) + (0-0) ] = 0\n$$\n\nCase 2: $j = k \\neq 0$.\nThe integral becomes:\n$$\n\\mathcal{I}_{kk} = \\frac{1}{2} \\int_{0}^{\\pi} \\left[ \\cos(2k\\theta) + \\cos(0) \\right] \\, d\\theta = \\frac{1}{2} \\int_{0}^{\\pi} (\\cos(2k\\theta) + 1) \\, d\\theta\n$$\n$$\n\\mathcal{I}_{kk} = \\frac{1}{2} \\left[ \\frac{\\sin(2k\\theta)}{2k} + \\theta \\right]_{0}^{\\pi} = \\frac{1}{2} [ (0-0) + (\\pi-0) ] = \\frac{\\pi}{2}\n$$\n\nCase 3: $j = k = 0$.\n$T_0(x) = \\cos(0) = 1$. The integral is:\n$$\n\\mathcal{I}_{00} = \\int_{0}^{\\pi} \\cos(0)\\cos(0) \\, d\\theta = \\int_{0}^{\\pi} 1 \\, d\\theta = \\pi\n$$\nCombining these results, we have the orthogonality relation:\n$$\n\\int_{-1}^{1} \\frac{T_j(x) T_k(x)}{\\sqrt{1-x^2}} \\, dx = \\begin{cases} 0 & j \\neq k \\\\ \\pi/2 & j=k \\neq 0 \\\\ \\pi & j=k=0 \\end{cases}\n$$\nNow, we derive the $n$-point Gauss-Chebyshev quadrature rule. Consider a general weighted integral:\n$$\n\\mathcal{J} = \\int_{-1}^{1} \\frac{f(x)}{\\sqrt{1-x^2}} \\, dx\n$$\nApplying the same substitution $x=\\cos\\theta$, this integral is transformed into an unweighted integral over $\\theta$:\n$$\n\\mathcal{J} = \\int_{0}^{\\pi} f(\\cos\\theta) \\, d\\theta\n$$\nWe can approximate this definite integral using a simple numerical scheme. The key insight is that a particular choice of nodes corresponds to a Gaussian quadrature. Let's approximate the integral using an $n$-point midpoint rule. We partition the interval $[0, \\pi]$ into $n$ subintervals of equal width $\\Delta\\theta = \\pi/n$. The $k$-th subinterval is $[\\frac{(k-1)\\pi}{n}, \\frac{k\\pi}{n}]$. The midpoint of this subinterval is:\n$$\n\\theta_k = \\frac{1}{2} \\left( \\frac{(k-1)\\pi}{n} + \\frac{k\\pi}{n} \\right) = \\frac{(2k-1)\\pi}{2n}, \\quad k=1, 2, \\dots, n\n$$\nThe midpoint rule approximation is the sum of the function values at these midpoints multiplied by the subinterval width:\n$$\n\\mathcal{J} \\approx \\sum_{k=1}^{n} f(\\cos\\theta_k) \\Delta\\theta = \\frac{\\pi}{n} \\sum_{k=1}^{n} f(\\cos\\theta_k)\n$$\nThis gives us the quadrature rule in the $x$-domain. The quadrature nodes $x_k$ are the images of the midpoints $\\theta_k$:\n$$\nx_k = \\cos(\\theta_k) = \\cos\\left(\\frac{(2k-1)\\pi}{2n}\\right), \\quad k=1, 2, \\dots, n\n$$\nThese are precisely the $n$ roots of the Chebyshev polynomial $T_n(x)$. The quadrature weights $w_k$ are all equal and are given by the constant multiplier of the sum:\n$$\nw_k = \\frac{\\pi}{n}\n$$\nThus, the $n$-point Gauss-Chebyshev quadrature rule of the first kind is:\n$$\n\\int_{-1}^{1} \\frac{f(x)}{\\sqrt{1-x^2}} \\, dx \\approx \\sum_{k=1}^{n} w_k f(x_k) = \\frac{\\pi}{n} \\sum_{k=1}^{n} f\\left(\\cos\\left(\\frac{(2k-1)\\pi}{2n}\\right)\\right)\n$$\nAs a Gaussian quadrature rule associated with the roots of $T_n(x)$, this formula is exact for any polynomial $f(x)$ of degree up to $2n - 1$.\n\n2. Analytic Evaluation of the Integral\n\nWe now evaluate the integral $I$ in closed form.\n$$\nI = \\int_{-1}^{1} \\frac{\\ln(1+x)}{\\sqrt{1-x^2}} \\, dx\n$$\nUsing the substitution $x = \\cos\\theta$, we transform the integral as before:\n$$\nI = \\int_{\\pi}^{0} \\frac{\\ln(1+\\cos\\theta)}{\\sin\\theta} (-\\sin\\theta \\, d\\theta) = \\int_{0}^{\\pi} \\ln(1+\\cos\\theta) \\, d\\theta\n$$\nWe use the half-angle trigonometric identity $1+\\cos\\theta = 2\\cos^2(\\theta/2)$:\n$$\nI = \\int_{0}^{\\pi} \\ln\\left(2\\cos^2\\left(\\frac{\\theta}{2}\\right)\\right) \\, d\\theta = \\int_{0}^{\\pi} \\left( \\ln 2 + 2\\ln\\left|\\cos\\left(\\frac{\\theta}{2}\\right)\\right| \\right) \\, d\\theta\n$$\nFor $\\theta \\in [0, \\pi]$, $\\theta/2 \\in [0, \\pi/2]$, so $\\cos(\\theta/2) \\ge 0$. The absolute value can be dropped.\n$$\nI = \\int_{0}^{\\pi} \\ln 2 \\, d\\theta + 2 \\int_{0}^{\\pi} \\ln\\left(\\cos\\left(\\frac{\\theta}{2}\\right)\\right) \\, d\\theta\n$$\nThe first term is $\\pi \\ln 2$. For the second integral, let $u = \\theta/2$, so $d\\theta = 2du$. The limits change from $[0, \\pi]$ to $[0, \\pi/2]$.\n$$\n2 \\int_{0}^{\\pi} \\ln\\left(\\cos\\left(\\frac{\\theta}{2}\\right)\\right) \\, d\\theta = 2 \\int_{0}^{\\pi/2} \\ln(\\cos u) (2du) = 4 \\int_{0}^{\\pi/2} \\ln(\\cos u) \\, du\n$$\nLet this standard definite integral be denoted $J = \\int_{0}^{\\pi/2} \\ln(\\cos u) \\, du$. We can evaluate it by noting that $J = \\int_{0}^{\\pi/2} \\ln(\\sin u) \\, du$ via the substitution $v = \\pi/2 - u$. Then,\n$$\n2J = \\int_{0}^{\\pi/2} \\ln(\\sin u) \\, du + \\int_{0}^{\\pi/2} \\ln(\\cos u) \\, du = \\int_{0}^{\\pi/2} \\ln(\\sin u \\cos u) \\, du\n$$\nUsing $\\sin u \\cos u = \\frac{1}{2}\\sin(2u)$:\n$$\n2J = \\int_{0}^{\\pi/2} \\ln\\left(\\frac{\\sin(2u)}{2}\\right) \\, du = \\int_{0}^{\\pi/2} \\ln(\\sin(2u)) \\, du - \\int_{0}^{\\pi/2} \\ln 2 \\, du\n$$\nThe second term is $-\\frac{\\pi}{2}\\ln 2$. For the first term, let $w=2u$, so $du=dw/2$.\n$$\n\\int_{0}^{\\pi/2} \\ln(\\sin(2u)) \\, du = \\frac{1}{2} \\int_{0}^{\\pi} \\ln(\\sin w) \\, dw = \\frac{1}{2} \\left[ \\int_{0}^{\\pi/2} \\ln(\\sin w) \\, dw + \\int_{\\pi/2}^{\\pi} \\ln(\\sin w) \\, dw \\right]\n$$\nThe first part of the sum is $J$. The second part, by symmetry of $\\sin(w)$ about $w=\\pi/2$, is also $J$. Thus, $\\frac{1}{2}[J+J] = J$.\nSubstituting back into the equation for $2J$:\n$$\n2J = J - \\frac{\\pi}{2}\\ln 2 \\implies J = -\\frac{\\pi}{2}\\ln 2\n$$\nNow we can find the value of $I$:\n$$\nI = \\pi \\ln 2 + 4J = \\pi \\ln 2 + 4\\left(-\\frac{\\pi}{2}\\ln 2\\right) = \\pi \\ln 2 - 2\\pi \\ln 2 = -\\pi \\ln 2\n$$\nThe exact analytic value of the integral is $I = -\\pi \\ln 2$.\n\nPart B: Algorithm Design and Numerical Experiment\n\n1. Algorithm Design\n\nThe integral to approximate is $I = \\int_{-1}^{1} \\frac{\\ln(1+x)}{\\sqrt{1-x^2}} \\, dx$. The function is $f(x) = \\ln(1+x)$. The $n$-point Gauss-Chebyshev approximation $I_n$ is:\n$$\nI_n = \\frac{\\pi}{n} \\sum_{k=1}^{n} f(x_k) = \\frac{\\pi}{n} \\sum_{k=1}^{n} \\ln(1+x_k)\n$$\nwhere the nodes are $x_k = \\cos\\left(\\frac{(2k-1)\\pi}{2n}\\right)$.\n\nA numerical stability issue arises when evaluating $\\ln(1+x_k)$. For large $n$, the node $x_n = \\cos(\\frac{(2n-1)\\pi}{2n}) = \\cos(\\pi - \\frac{\\pi}{2n}) = -\\cos(\\frac{\\pi}{2n})$ approaches $-1$. The evaluation of $1+x_k$ for $x_k$ very close to $-1$ can suffer from catastrophic cancellation, where subtracting two nearly identical floating-point numbers leads to a significant loss of relative precision.\n\nTo circumvent this, we reformulate the term being summed. Since $x_k = \\cos(\\theta_k)$, we have $\\ln(1+x_k) = \\ln(1+\\cos\\theta_k)$. Using the identity $1+\\cos\\theta = 2\\cos^2(\\theta/2)$, we can write:\n$$\n\\ln(1+\\cos\\theta_k) = \\ln\\left(2\\cos^2\\left(\\frac{\\theta_k}{2}\\right)\\right) = \\ln 2 + 2\\ln\\left(\\cos\\left(\\frac{\\theta_k}{2}\\right)\\right)\n$$\nSubstituting $\\theta_k = \\frac{(2k-1)\\pi}{2n}$, the argument of the cosine becomes $\\frac{\\theta_k}{2} = \\frac{(2k-1)\\pi}{4n}$. This angle lies in the interval $(0, \\pi/2)$ for all $k=1, \\dots, n$. The cosine evaluation is well-behaved, as is the logarithm of its result (a number in $(0, 1)$). This formulation avoids the problematic subtraction and is numerically stable.\n\nThe algorithm to compute the absolute error $E_n = |I_n - I|$ is:\n1.  Set the analytic value: $I = -\\pi \\ln 2$.\n2.  For a given number of points $n$:\n    a. Initialize a sum $S=0$.\n    b. For $k=1, 2, \\dots, n$:\n       i.  Compute the angle argument $\\alpha_k = \\frac{(2k-1)\\pi}{4n}$.\n       ii. Compute the term to be summed: $f_k = \\ln 2 + 2\\ln(\\cos(\\alpha_k))$.\n       iii. Add this term to the sum: $S = S + f_k$.\n    c. Calculate the numerical approximation: $I_n = \\frac{\\pi}{n} S$.\n    d. Compute the absolute error: $E_n = |I_n - I|$.\n3.  Repeat for each value of $n$ in the test suite and report the errors.\n\n2. Numerical Experiment\n\nThe following program implements this algorithm for the test cases $n \\in \\{1, 2, 4, 8, 16, 64\\}$ and prints the resulting absolute errors $E_n$ in the specified format. The calculations are vectorized using NumPy for efficiency.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the absolute error of the n-point Gauss-Chebyshev quadrature\n    for the integral of ln(1+x)/sqrt(1-x^2) from -1 to 1.\n    \"\"\"\n    # Define the test cases for n, the number of quadrature points.\n    test_cases = [1, 2, 4, 8, 16, 64]\n\n    # The analytic value of the integral is I = -pi * ln(2).\n    # This was derived in Part A.2 of the solution.\n    analytic_value = -np.pi * np.log(2)\n\n    results = []\n    for n in test_cases:\n        # Main logic to calculate the result for one case (one value of n).\n\n        # Vectorized calculation for all k from 1 to n.\n        k = np.arange(1, n + 1)\n\n        # The term to be summed in the quadrature is ln(1 + x_k).\n        # To avoid catastrophic cancellation when x_k is close to -1,\n        # we use the identity ln(1+cos(theta)) = ln(2) + 2*ln(cos(theta/2)).\n        # Here, theta_k = (2k-1)pi / (2n), so theta_k/2 = (2k-1)pi / (4n).\n        \n        # Calculate the arguments for the cosine function.\n        # This angle is always in (0, pi/2), ensuring numerical stability.\n        angle_arg = (2 * k - 1) * np.pi / (4 * n)\n        \n        # Evaluate the function at each quadrature node using the stable formula.\n        func_values = np.log(2) + 2 * np.log(np.cos(angle_arg))\n        \n        # Sum the function values and multiply by the constant weight pi/n.\n        # This gives the n-point Gauss-Chebyshev approximation I_n.\n        numerical_approx = (np.pi / n) * np.sum(func_values)\n        \n        # The absolute error is E_n = |I_n - I|.\n        abs_error = np.abs(numerical_approx - analytic_value)\n        results.append(abs_error)\n\n    # Final print statement in the exact required format.\n    # The format is a comma-separated list of floating-point numbers\n    # enclosed in square brackets.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3614882"}, {"introduction": "This final practice culminates in a full application of Chebyshev spectral methods to a core problem in geophysical fluid dynamics: the computation of vertical baroclinic modes. You will implement a Chebyshev-Galerkin method to discretize a Sturm-Liouville eigenvalue problem, transforming the differential equation into a generalized matrix eigenvalue problem [@problem_id:3614944]. Successfully completing this exercise demonstrates the power and elegance of spectral methods for solving differential equations that arise in physical modeling.", "problem": "Consider the one-dimensional vertical mode problem for a stratified Boussinesq fluid in the hydrostatic and long-wave limits, expressed on the nondimensional vertical interval $z\\in[-1,1]$. Starting from the standard linearized hydrostatic balance and buoyancy relation, one arrives at a Sturm–Liouville eigenproblem for the vertical structure that admits a self-adjoint formulation in the form\n$$\n-\\frac{d^2 \\phi}{dz^2} \\;=\\; \\lambda\\,N^2(z)\\,\\phi(z),\\qquad z\\in(-1,1),\n$$\nwith homogeneous Dirichlet boundary conditions\n$$\n\\phi(-1)=0,\\qquad \\phi(1)=0.\n$$\nHere $N^2(z)$ is the squared buoyancy frequency profile and $\\lambda$ is the (nonnegative) eigenvalue associated with a baroclinic normal mode. In this problem, use the profile\n$$\nN^2(z)=N_0^2\\,(1+z),\n$$\nwhere $N_0>0$ is a constant. The problem is well-posed in the weighted $L^2$ inner product induced by $N^2(z)$, and the operator is self-adjoint and positive definite on the subspace of functions satisfying the boundary conditions. Angles used in any trigonometric expressions must be in radians.\n\nYour task is to implement a Chebyshev polynomial Galerkin discretization to approximate the first few eigenvalues and eigenfunctions of this problem. Proceed as follows, without using any shortcut formulas beyond the fundamental properties listed below.\n\n- Use the Chebyshev polynomials of the first kind $\\{T_n(z)\\}_{n\\ge 0}$ as the starting basis, and enforce the homogeneous Dirichlet boundary conditions by adopting the admissible trial and test basis\n$$\n\\varphi_k(z) \\;=\\; T_k(z)\\;-\\;T_{k+2}(z),\\qquad k=0,1,\\dots,M-1,\n$$\nwhich satisfies $\\varphi_k(\\pm 1)=0$ because $T_n(1)=1$ and $T_n(-1)=(-1)^n$ for all $n$, implying $T_k(\\pm 1)=T_{k+2}(\\pm 1)$.\n- Use the weak (variational) form derived from integration by parts with the given boundary conditions:\n$$\n\\int_{-1}^{1}\\,\\varphi_i'(z)\\,\\varphi_j'(z)\\,dz \\;=\\; \\lambda\\;\\int_{-1}^{1} N^2(z)\\,\\varphi_i(z)\\,\\varphi_j(z)\\,dz,\\qquad i,j=0,\\dots,M-1.\n$$\n- Assemble the symmetric positive definite stiffness matrix $A\\in\\mathbb{R}^{M\\times M}$ and the symmetric positive definite mass matrix $B\\in\\mathbb{R}^{M\\times M}$ with entries\n$$\nA_{ij} \\;=\\; \\int_{-1}^{1}\\,\\varphi_i'(z)\\,\\varphi_j'(z)\\,dz,\\qquad\nB_{ij} \\;=\\; \\int_{-1}^{1} N^2(z)\\,\\varphi_i(z)\\,\\varphi_j(z)\\,dz.\n$$\n- Recover the lowest eigenpairs by solving the generalized symmetric eigenvalue problem\n$$\nA\\,\\mathbf{c} \\;=\\; \\lambda\\,B\\,\\mathbf{c}.\n$$\n- Use only the following fundamental polynomial identities: $T_n(\\cos\\theta)=\\cos(n\\theta)$ and $\\dfrac{d}{dz}T_n(z) = n\\,U_{n-1}(z)$ with $U_n(\\cos\\theta)=\\dfrac{\\sin((n+1)\\theta)}{\\sin\\theta}$, where $U_n$ denotes the Chebyshev polynomial of the second kind. Angles $\\theta$ are in radians.\n\nAlgorithmic requirements:\n- For numerical integration of the bilinear forms, use any high-order accurate quadrature on $[-1,1]$ that is appropriate for smooth integrands; a suitable choice is Gauss–Legendre quadrature with $Q$ points. Ensure $Q$ is sufficiently large relative to $M$ to avoid under-integration.\n- To evaluate $T_n(z)$ and $U_n(z)$ at quadrature nodes, use the cosine and sine representations above. Avoid any divisions by zero by using quadrature nodes that do not include the endpoints.\n- Normalize eigenfunctions in any consistent manner for internal use; the final reported quantities are the eigenvalues only.\n\nTest suite:\nCompute the three smallest eigenvalues for each of the following parameter sets $(N_0,M,Q)$:\n- Case $1$: $N_0=1.0$, $M=24$, $Q=200$.\n- Case $2$: $N_0=2.0$, $M=24$, $Q=200$.\n- Case $3$: $N_0=1.0$, $M=8$, $Q=120$.\n- Case $4$: $N_0=0.5$, $M=24$, $Q=200$.\n\nOutput specification:\n- For each case, sort the computed eigenvalues in ascending order and take the first three. Round each to $8$ decimal places.\n- Your program should produce a single line of output containing the results as a comma-separated list of lists in the order of the cases above. For example, it should look like:\n$[ [\\lambda_{1}^{(1)},\\lambda_{2}^{(1)},\\lambda_{3}^{(1)}], [\\lambda_{1}^{(2)},\\lambda_{2}^{(2)},\\lambda_{3}^{(2)}], [\\lambda_{1}^{(3)},\\lambda_{2}^{(3)},\\lambda_{3}^{(3)}], [\\lambda_{1}^{(4)},\\lambda_{2}^{(4)},\\lambda_{3}^{(4)}] ]$.\nOnly print this single line; do not print any additional text.\n\nNotes:\n- The formulation above is universally applicable as a self-adjoint weighted eigenproblem. It avoids any singular coefficients at $z=-1$ by working with the equivalent self-adjoint form $-\\,\\phi''=\\lambda\\,N^2\\phi$ and Dirichlet boundary conditions, which is consistent with a Liouville transformation of the standard hydrostatic baroclinic mode equation.\n- All angles used inside trigonometric functions must be in radians.", "solution": "The problem presented is a well-posed Sturm-Liouville eigenvalue problem, which is fundamental to the study of vertical modes in stratified fluids within geophysical fluid dynamics. It is scientifically sound, self-contained, and mathematically unambiguous. Therefore, I will proceed with a complete solution.\n\nThe governing equation for the vertical structure function $\\phi(z)$ is given by\n$$\n-\\frac{d^2 \\phi}{dz^2} = \\lambda\\,N^2(z)\\,\\phi(z),\n$$\non the domain $z \\in (-1, 1)$, subject to homogeneous Dirichlet boundary conditions $\\phi(-1) = 0$ and $\\phi(1) = 0$. The squared buoyancy frequency profile is a linear function of depth, $N^2(z) = N_0^2\\,(1+z)$.\n\nWe seek to find the smallest eigenvalues $\\lambda$ using the Chebyshev-Galerkin spectral method. The core of this method is to project the problem onto a finite-dimensional function space spanned by a set of basis functions that satisfy the problem's boundary conditions.\n\nFirst, the differential equation is converted into its weak or variational form. We multiply the equation by a test function $v(z)$ from a suitable space and integrate over the domain:\n$$\n-\\int_{-1}^{1} v(z) \\frac{d^2 \\phi}{dz^2} dz = \\lambda \\int_{-1}^{1} v(z) N^2(z) \\phi(z) dz.\n$$\nApplying integration by parts to the left-hand side yields:\n$$\n\\left[ -v(z) \\frac{d\\phi}{dz} \\right]_{-1}^{1} + \\int_{-1}^{1} \\frac{dv}{dz} \\frac{d\\phi}{dz} dz = \\lambda \\int_{-1}^{1} v(z) N^2(z) \\phi(z) dz.\n$$\nIn the Galerkin method, the test functions $v(z)$ are chosen from the same space as the trial (basis) functions for the solution $\\phi(z)$. The problem specifies trial functions that satisfy the boundary conditions, so we require $v(\\pm 1) = 0$. This makes the boundary term $\\left[ -v(z) \\frac{d\\phi}{dz} \\right]_{-1}^{1}$ vanish, leading to the weak form:\n$$\n\\int_{-1}^{1} v'(z) \\phi'(z) dz = \\lambda \\int_{-1}^{1} v(z) \\phi(z) N^2(z) dz.\n$$\n\nThe solution $\\phi(z)$ is approximated by a finite expansion in terms of $M$ basis functions $\\varphi_j(z)$:\n$$\n\\phi(z) \\approx \\phi_M(z) = \\sum_{j=0}^{M-1} c_j \\varphi_j(z),\n$$\nwhere $c_j$ are unknown coefficients. The specified basis functions, constructed from Chebyshev polynomials of the first kind $T_n(z)$, are\n$$\n\\varphi_k(z) = T_k(z) - T_{k+2}(z), \\quad k=0, 1, \\dots, M-1.\n$$\nThese functions satisfy $\\varphi_k(\\pm 1) = 0$ because $T_n(1)=1$ and $T_n(-1)=(-1)^n$, ensuring that $T_k(\\pm 1) = T_{k+2}(\\pm 1)$, and thus correctly incorporating the boundary conditions into the approximation space.\n\nSubstituting the expansion for $\\phi_M(z)$ into the weak form and choosing the test functions to be the basis functions themselves, $v(z) = \\varphi_i(z)$ for $i=0, 1, \\dots, M-1$, we obtain a system of $M$ linear equations:\n$$\n\\sum_{j=0}^{M-1} c_j \\left( \\int_{-1}^{1} \\varphi_i'(z) \\varphi_j'(z) dz \\right) = \\lambda \\sum_{j=0}^{M-1} c_j \\left( \\int_{-1}^{1} N^2(z) \\varphi_i(z) \\varphi_j(z) dz \\right).\n$$\nThis system is expressed as a generalized symmetric matrix eigenvalue problem:\n$$\nA \\mathbf{c} = \\lambda B \\mathbf{c},\n$$\nwhere $\\mathbf{c} = [c_0, c_1, \\dots, c_{M-1}]^T$ is the vector of coefficients, and the stiffness matrix $A$ and mass matrix $B$ have entries:\n$$\nA_{ij} = \\int_{-1}^{1} \\varphi_i'(z) \\varphi_j'(z) dz,\n$$\n$$\nB_{ij} = \\int_{-1}^{1} N^2(z) \\varphi_i(z) \\varphi_j(z) dz = N_0^2 \\int_{-1}^{1} (1+z) \\varphi_i(z) \\varphi_j(z) dz.\n$$\n\nThe entries of these matrices are computed via numerical quadrature. We employ Gauss-Legendre quadrature with $Q$ points $\\{z_q\\}_{q=0}^{Q-1}$ and corresponding weights $\\{w_q\\}_{q=0}^{Q-1}$ on the interval $[-1, 1]$. The matrix entries are thus approximated as:\n$$\nA_{ij} \\approx \\sum_{q=0}^{Q-1} w_q \\varphi_i'(z_q) \\varphi_j'(z_q),\n$$\n$$\nB_{ij} \\approx N_0^2 \\sum_{q=0}^{Q-1} w_q (1+z_q) \\varphi_i(z_q) \\varphi_j(z_q).\n$$\n\nTo evaluate the basis functions and their derivatives at the quadrature nodes, we use the specified trigonometric identities. With the substitution $z = \\cos\\theta$, we have $\\theta = \\arccos(z)$. The quadrature nodes $z_q$ are strictly within $(-1, 1)$, so $\\theta_q = \\arccos(z_q)$ is well-defined in $(0, \\pi)$.\nThe basis functions are evaluated using $T_n(\\cos\\theta) = \\cos(n\\theta)$:\n$$\n\\varphi_k(z_q) = T_k(\\cos\\theta_q) - T_{k+2}(\\cos\\theta_q) = \\cos(k\\theta_q) - \\cos((k+2)\\theta_q).\n$$\nThe derivatives are found using $\\frac{d}{dz}T_n(z) = n U_{n-1}(z)$ and $U_n(\\cos\\theta) = \\frac{\\sin((n+1)\\theta)}{\\sin\\theta}$:\n$$\n\\varphi_k'(z) = k U_{k-1}(z) - (k+2) U_{k+1}(z).\n$$\nEvaluated at the nodes $z_q=\\cos\\theta_q$, this becomes:\n$$\n\\varphi_k'(z_q) = k \\frac{\\sin(k\\theta_q)}{\\sin\\theta_q} - (k+2) \\frac{\\sin((k+2)\\theta_q)}{\\sin\\theta_q}.\n$$\nThe denominator $\\sin\\theta_q$ is non-zero as $\\theta_q \\in (0, \\pi)$.\n\nWith the matrices $A$ and $B$ assembled, the generalized eigenvalue problem $A \\mathbf{c} = \\lambda B \\mathbf{c}$ is solved numerically. Since $A$ and $B$ are symmetric and positive definite, specialized and stable algorithms are available. The resulting eigenvalues $\\lambda$ are approximations to the true eigenvalues of the continuous problem. We sort these in ascending order and select the smallest three as required.\n\nIt is worth noting the role of the constant $N_0$. The mass matrix $B$ is proportional to $N_0^2$. The eigenvalues $\\lambda$ are therefore inversely proportional to $N_0^2$. This means that if we compute the eigenvalues $\\lambda_{N_0=1}$ for $N_0=1.0$, the eigenvalues for any other $N_0$ are given by $\\lambda_{N_0} = \\lambda_{N_0=1} / N_0^2$. This physical scaling provides a valuable consistency check for the numerical results.\n\nThe implementation proceeds by first calculating Gauss-Legendre quadrature nodes and weights. Then, for a given $M$, the values of all basis functions and their derivatives are evaluated at these nodes and stored. These are used to assemble matrices $A$ and $B$ efficiently using vectorized operations. Finally, a generalized eigensolver from a standard scientific library (e.g., SciPy) is used to compute the eigenvalues. This process is repeated for each test case specified in the problem statement.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import eigh\n\ndef solve():\n    \"\"\"\n    Main function to solve the eigenvalue problem for the given test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (N0, M, Q)\n        (1.0, 24, 200),\n        (2.0, 24, 200),\n        (1.0, 8, 120),\n        (0.5, 24, 200),\n    ]\n\n    all_results = []\n    for N0, M, Q in test_cases:\n        eigenvalues = compute_eigenvalues(N0, M, Q)\n        # Select the three smallest eigenvalues and round them.\n        smallest_three = np.round(eigenvalues[:3], 8).tolist()\n        all_results.append(smallest_three)\n\n    inner_list_strings = [str(res) for res in all_results]\n    print(f\"[{','.join(inner_list_strings)}]\")\n\n\ndef compute_eigenvalues(N0, M, Q):\n    \"\"\"\n    Computes the eigenvalues for a single case (N0, M, Q).\n    \n    Args:\n        N0 (float): Buoyancy frequency constant.\n        M (int): Number of basis functions (size of discretization).\n        Q (int): Number of Gauss-Legendre quadrature points.\n\n    Returns:\n        np.ndarray: An array of computed eigenvalues, sorted in ascending order.\n    \"\"\"\n    # Step 1: Get Gauss-Legendre quadrature nodes and weights for the interval [-1, 1].\n    z_nodes, w_weights = np.polynomial.legendre.leggauss(Q)\n\n    # Step 2: Evaluate basis functions phi_k and their derivatives dphi_k/dz at the nodes.\n    phi_vals, dphi_vals = evaluate_basis_functions(M, z_nodes)\n\n    # Step 3: Assemble the stiffness (A) and mass (B) matrices.\n    # A_ij = integral(phi'_i * phi'_j, dz)\n    # B_ij = integral(N^2 * phi_i * phi_j, dz)\n    \n    # Efficient assembly using vectorized operations.\n    A = dphi_vals @ (dphi_vals * w_weights).T\n\n    # Buoyancy frequency squared N^2(z) = N0^2 * (1 + z) evaluated at nodes.\n    N2_on_nodes = N0**2 * (1 + z_nodes)\n    \n    B = phi_vals @ (phi_vals * (w_weights * N2_on_nodes)).T\n\n    # Step 4: Solve the generalized symmetric eigenvalue problem A c = lambda B c.\n    # eigh returns eigenvalues in ascending order, which is what we need.\n    eigenvalues = eigh(A, B, eigvals_only=True)\n    \n    return eigenvalues\n\n\ndef evaluate_basis_functions(M, z_nodes):\n    \"\"\"\n    Evaluates Chebyshev-based trial functions and their derivatives at quadrature nodes.\n    The basis functions are phi_k(z) = T_k(z) - T_{k+2}(z).\n    \n    Args:\n        M (int): Number of basis functions.\n        z_nodes (np.ndarray): Quadrature nodes in [-1, 1].\n\n    Returns:\n        tuple[np.ndarray, np.ndarray]: \n            - phi_vals (M x Q): Values of basis functions at nodes.\n            - dphi_vals (M x Q): Values of basis function derivatives at nodes.\n    \"\"\"\n    Q = len(z_nodes)\n    \n    # Use the transformation z = cos(theta)\n    # Note: z_nodes from leggauss are in (-1, 1), so arccos is well-defined in (0, pi).\n    theta_nodes = np.arccos(z_nodes)\n    sin_theta = np.sin(theta_nodes) # This will not be zero.\n    \n    # Array of basis indices: k = 0, 1, ..., M-1\n    k = np.arange(M).reshape(-1, 1)\n\n    # Evaluate phi_k(z) = T_k(z) - T_{k+2}(z) using T_n(cos(theta)) = cos(n*theta).\n    # Broadcasting k (M,1) with theta_nodes (Q,) -> (M,Q)\n    T_k = np.cos(k * theta_nodes)\n    T_k_plus_2 = np.cos((k + 2) * theta_nodes)\n    phi_vals = T_k - T_k_plus_2\n    \n    # Evaluate d(phi_k)/dz = k*U_{k-1}(z) - (k+2)*U_{k+1}(z)\n    # using U_{n-1}(cos(theta)) = sin(n*theta)/sin(theta).\n    \n    # Term 1: k*U_{k-1}(z) -> k*sin(k*theta)/sin(theta)\n    # For k=0, the numerator is sin(0)=0, so the term is 0, which is correct.\n    dphi_term1 = k * np.sin(k * theta_nodes) / sin_theta\n    \n    # Term 2: (k+2)*U_{k+1}(z) -> (k+2)*sin((k+2)*theta)/sin(theta)\n    dphi_term2 = (k + 2) * np.sin((k + 2) * theta_nodes) / sin_theta\n    \n    dphi_vals = dphi_term1 - dphi_term2\n    \n    return phi_vals, dphi_vals\n\n# The entry point of the script.\nif __name__ == \"__main__\":\n    solve()\n\n```", "id": "3614944"}]}