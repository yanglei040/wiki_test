{"hands_on_practices": [{"introduction": "Geophysical models often couple slow and fast processes, creating \"stiff\" systems where explicit time integration methods are inefficient due to severe time step restrictions. This practice introduces the Implicit-Explicit (IMEX) approach, a powerful strategy that treats fast components implicitly for stability and slow components explicitly for efficiency [@problem_id:3617586]. Through deriving and analyzing a first-order IMEX scheme for a partitioned ordinary differential equation, you will discover the foundation of its enhanced stability properties for multi-scale problems.", "problem": "Consider a semi-discrete model arising in computational geophysics for a thermochemical field in the Earth's mantle, where advection is nonstiff but compositional relaxation is stiff. After spatial discretization by the method of lines, the temporal evolution of a scalar state variable is modeled by the ordinary differential equation $y'(t) = f(y(t)) + g(y(t))$, where $f$ represents the nonstiff advective and source contributions and $g$ represents a stiff linear relaxation arising from fast local equilibration processes. Assume $f$ and $g$ are sufficiently smooth to admit a consistent first-order time integrator.\n\nTask 1. Starting from the definition of the forward Euler method for nonstiff terms and the backward Euler method for stiff terms, derive a first-order Implicit-Explicit (IMEX) Euler scheme for $y'(t) = f(y(t)) + g(y(t))$ with time step $\\Delta t > 0$. Your derivation must clearly state which term is treated explicitly and which is treated implicitly, and must provide a closed-form update for $y^{n+1}$ in terms of $y^{n}$, $\\Delta t$, $f$, and $g$.\n\nTask 2. Perform a linear stability analysis of the derived IMEX Euler scheme on the scalar test equation $y'(t) = \\lambda y(t) + \\mu y(t)$, where $\\lambda \\in \\mathbb{C}$ models the nonstiff contribution and $\\mu \\in \\mathbb{C}$ models the stiff contribution with $\\operatorname{Re}(\\mu) \\ll 0$. Let $z = \\Delta t\\,\\lambda$ and $w = \\Delta t\\,\\mu$. Derive the one-step amplification factor $R(z,w)$ such that $y^{n+1} = R(z,w)\\,y^{n}$, and write $R(z,w)$ as a single closed-form analytic expression in terms of $z$ and $w$. Based on $R(z,w)$, discuss the implications for stability with respect to the stiff component when $w$ satisfies $\\operatorname{Re}(w)  0$ and $|w|$ is large.\n\nAnswer format requirement: Provide the final answer as the single closed-form expression for $R(z,w)$. No numerical evaluation is required, and no units are involved. The final answer must be given exactly, not approximated.", "solution": "The problem is first validated to ensure it is scientifically sound, well-posed, and objective.\n\n### Step 1: Extract Givens\n-   The governing ordinary differential equation (ODE) is $y'(t) = f(y(t)) + g(y(t))$.\n-   $f(y(t))$ represents nonstiff contributions.\n-   $g(y(t))$ represents a stiff linear relaxation.\n-   The time step is $\\Delta t > 0$.\n-   The scalar test equation for linear stability analysis is $y'(t) = \\lambda y(t) + \\mu y(t)$.\n-   $\\lambda \\in \\mathbb{C}$ corresponds to the nonstiff part.\n-   $\\mu \\in \\mathbb{C}$ corresponds to the stiff part, with the property $\\operatorname{Re}(\\mu) \\ll 0$.\n-   Normalized parameters are defined as $z = \\Delta t\\,\\lambda$ and $w = \\Delta t\\,\\mu$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is well-posed and grounded in the established theory of numerical methods for ordinary differential equations. The setup involving a partitioned ODE with stiff and nonstiff components is a canonical model used to analyze Implicit-Explicit (IMEX) schemes. The use of a scalar test equation to study linear stability is a standard, fundamental technique in numerical analysis. The terms and concepts, such as \"stiff,\" \"nonstiff,\" \"IMEX,\" and \"amplification factor,\" are all well-defined within the field. The problem statement is self-contained, unambiguous, and does not violate any scientific principles or logical consistency. It is a formal mathematical problem that admits a unique, verifiable solution.\n\n### Step 3: Verdict and Action\nThe problem is deemed valid. A full solution will be provided.\n\n### Task 1: Derivation of the IMEX Euler Scheme\n\nThe problem describes a first-order Implicit-Explicit (IMEX) scheme, also known as a semi-implicit scheme. The core idea is to treat different parts of the ODE with different time-stepping methods tailored to their characteristics. Specifically, the nonstiff term $f(y)$ is treated explicitly, and the stiff term $g(y)$ is treated implicitly.\n\nWe start with the ODE:\n$$y'(t) = f(y(t)) + g(y(t))$$\nLet $y^n$ be the numerical approximation of the solution $y(t_n)$ at time $t_n = n \\Delta t$. We approximate the time derivative using a first-order forward difference:\n$$y'(t_n) \\approx \\frac{y^{n+1} - y^n}{\\Delta t}$$\nSubstituting this into the ODE, we evaluate the right-hand side using the IMEX approach. The nonstiff term $f$ is evaluated at the current, known time level $t_n$, which is an explicit treatment (specifically, forward Euler). The stiff term $g$ is evaluated at the future, unknown time level $t_{n+1}$, which is an implicit treatment (specifically, backward Euler). This leads to the following discrete equation:\n$$\\frac{y^{n+1} - y^n}{\\Delta t} = f(y^n) + g(y^{n+1})$$\nThe problem states that $g$ represents a \"stiff linear relaxation,\" which implies that $g$ is a linear function of its argument. We can therefore write $g(y) = G y$ for some linear operator $G$. In a multidimensional system, $G$ would be a matrix. Substituting this linear form for $g$ into our scheme gives:\n$$\\frac{y^{n+1} - y^n}{\\Delta t} = f(y^n) + G y^{n+1}$$\nTo find a closed-form update for $y^{n+1}$, we rearrange the equation to isolate $y^{n+1}$ on one side. First, multiply by $\\Delta t$:\n$$y^{n+1} - y^n = \\Delta t f(y^n) + \\Delta t G y^{n+1}$$\nNext, gather all terms involving $y^{n+1}$ on the left-hand side and all other terms on the right-hand side:\n$$y^{n+1} - \\Delta t G y^{n+1} = y^n + \\Delta t f(y^n)$$\nFactor out $y^{n+1}$ on the left-hand side. This involves the identity operator $I$:\n$$(I - \\Delta t G) y^{n+1} = y^n + \\Delta t f(y^n)$$\nFinally, to solve for $y^{n+1}$, we apply the inverse of the operator $(I - \\Delta t G)$ to both sides:\n$$y^{n+1} = (I - \\Delta t G)^{-1} \\left( y^n + \\Delta t f(y^n) \\right)$$\nThis is the closed-form update for the first-order IMEX Euler scheme applied to an ODE with a linear stiff part.\n\n### Task 2: Linear Stability Analysis\n\nWe now perform a linear stability analysis on the scalar test equation:\n$$y'(t) = \\lambda y(t) + \\mu y(t)$$\nHere, the nonstiff contribution is identified as $f(y) = \\lambda y$, and the stiff linear contribution is $g(y) = \\mu y$. For this scalar case, the operator $G$ is simply multiplication by the complex number $\\mu$, and the identity operator $I$ is multiplication by $1$.\n\nWe apply the derived IMEX Euler scheme to this test equation. Substituting $f(y^n) = \\lambda y^n$ and $g(y^{n+1}) = \\mu y^{n+1}$ into the basic form of the scheme:\n$$\\frac{y^{n+1} - y^n}{\\Delta t} = \\lambda y^n + \\mu y^{n+1}$$\nOur goal is to find the amplification factor $R(z,w)$ such that $y^{n+1} = R(z,w) y^n$. We rearrange the equation to solve for $y^{n+1}$ in terms of $y^n$:\n$$y^{n+1} - y^n = \\Delta t \\lambda y^n + \\Delta t \\mu y^{n+1}$$\n$$y^{n+1} - \\Delta t \\mu y^{n+1} = y^n + \\Delta t \\lambda y^n$$\nFactor out $y^{n+1}$ and $y^n$:\n$$y^{n+1} (1 - \\Delta t \\mu) = y^n (1 + \\Delta t \\lambda)$$\nIsolating $y^{n+1}$ gives:\n$$y^{n+1} = \\left( \\frac{1 + \\Delta t \\lambda}{1 - \\Delta t \\mu} \\right) y^n$$\nBy definition, the amplification factor is the term in the parenthesis. Using the given substitutions $z = \\Delta t \\lambda$ and $w = \\Delta t \\mu$, we obtain the expression for $R(z,w)$:\n$$R(z,w) = \\frac{1 + z}{1 - w}$$\nThe stability of the scheme requires $|R(z,w)| \\le 1$. The problem asks for the implications for stability with respect to the stiff component, where $\\operatorname{Re}(\\mu) \\ll 0$ and $|w|$ is large.\nSince $\\Delta t > 0$, $\\operatorname{Re}(\\mu)  0$ implies $\\operatorname{Re}(w)  0$. The stability region for the stiff component alone (by setting $z=0$) is given by $|R(0,w)| = |\\frac{1}{1-w}| \\le 1$, which simplifies to $|1-w| \\ge 1$. This region is the exterior of the unit circle centered at $w=1$ in the complex plane. This region contains the entire left half-plane, $\\{w \\in \\mathbb{C} \\mid \\operatorname{Re}(w) \\le 0\\}$. Since our stiff component satisfies $\\operatorname{Re}(w)  0$, the method is unconditionally stable for the stiff part. This property is known as A-stability.\nAs $|w| \\to \\infty$ (while maintaining $\\operatorname{Re}(w)0$), the denominator $|1-w| \\to \\infty$. Consequently, for any fixed $z$, the amplification factor's magnitude approaches zero:\n$$\\lim_{|w|\\to\\infty, \\operatorname{Re}(w)0} |R(z,w)| = \\lim_{|w|\\to\\infty} \\frac{|1+z|}{|1-w|} = 0$$\nThis demonstrates that the method is not only stable but also strongly dissipative (L-stable) for infinitely stiff components. The time step $\\Delta t$ is not restricted by the stiffness parameter $\\mu$, but rather by the nonstiff parameter $\\lambda$, for which the stability region $|1+z| \\le |1-w|$ resembles that of the explicit forward Euler method. This is the primary motivation for using IMEX schemes.\nThe requested final answer is the closed-form expression for $R(z,w)$.", "answer": "$$\\boxed{\\frac{1+z}{1-w}}$$", "id": "3617586"}, {"introduction": "Building on the idea of separating physical processes, operator splitting methods decompose a complex problem into simpler sub-problems that are solved sequentially. This practice explores a reactive transport model where fast reactions are split from slower advection-diffusion, a common scenario in computational geophysics [@problem_id:3617594]. You will determine the maximum allowable time step by balancing two critical factors: the numerical stability of the explicit transport solver and the accuracy error introduced by the splitting itself, which is governed by the non-commutativity of the physical operators.", "problem": "Consider a one-dimensional, periodic, two-species reactive transport model on the interval $\\left[0,L\\right]$ with $L=1$. A uniform grid with $N=100$ points is used, yielding grid spacing $\\Delta x=L/N$. Let $\\mathbf{u}(x,t)\\in\\mathbb{R}^{2}$ denote the species concentrations. After semi-discretization by the method of lines, the evolution of the grid-cell concentrations is governed by the linear ordinary differential equation\n$$\n\\frac{d\\mathbf{y}}{dt}=\\left(A+B\\right)\\mathbf{y},\n$$\nwhere $A$ represents the advection–diffusion transport and $B$ represents local reactions. The transport operator is\n$$\nA=A_{\\mathrm{adv}}\\otimes I_{2}+L\\otimes D,\n$$\nwhere $A_{\\mathrm{adv}}$ is the first-order upwind discrete advection operator for constant velocity $c=0.1$, $I_{2}$ is the $2\\times 2$ identity matrix acting on species, $L$ is the second-order centered finite-difference discrete Laplacian with periodic boundary conditions, and $D=\\mathrm{diag}\\!\\left(\\kappa_{1},\\kappa_{2}\\right)$ with $\\kappa_{1}=10^{-3}$ and $\\kappa_{2}=5\\times 10^{-4}$. The reaction operator is block-diagonal across grid cells,\n$$\nB=I_{N}\\otimes R,\n$$\nwith a stiff reaction matrix\n$$\nR=\\begin{pmatrix}\n-500  200\\\\\n50  -300\n\\end{pmatrix}.\n$$\nA Lie splitting is used per time step: first apply the reaction substep (solved by backward Euler), then apply the transport substep (solved by an explicit stabilized Runge–Kutta–Chebyshev (RKC) method with $s=20$ stages). As a well-tested fact for explicit stabilized RKC methods, the linear stability interval along the negative real axis scales approximately as $\\left[-2s^{2},0\\right]$. For the discrete Laplacian $L$ on a periodic grid with spacing $\\Delta x$, its eigenvalues are $\\mu(k)=-\\frac{4}{\\Delta x^{2}}\\sin^{2}\\!\\left(\\frac{\\pi k}{N}\\right)$ for $k=0,1,\\dots,N-1$, and its spectral radius is $\\rho(L)=\\frac{4}{\\Delta x^{2}}$.\n\nAssume the advection contribution to the stability constraint is negligible compared to diffusion for the chosen parameters and stages. The time step $h$ must be chosen to satisfy both of the following:\n\n- The explicit stabilized transport substep’s real-axis stability requirement, using the largest diffusive eigenvalue.\n- A bound on the leading-order local splitting error per step, derived from the Baker–Campbell–Hausdorff series and the commutator $[A,B]$, such that the ratio of the local splitting error norm to the initial norm does not exceed a prescribed tolerance $\\varepsilon=10^{-2}$.\n\nCompute the largest time step $h$ (in seconds) that satisfies both constraints. Round your answer to four significant figures and express it in seconds.", "solution": "The problem requires finding the largest time step $h$ for a Lie splitting scheme that satisfies two distinct constraints: one for the stability of the explicit transport substep and one for the accuracy of the operator splitting. We will determine the maximum time step allowed by each constraint, $h_{\\text{stability}}$ and $h_{\\text{error}}$, respectively. The overall maximum allowed time step will be the minimum of these two values, $h = \\min(h_{\\text{stability}}, h_{\\text{error}})$.\n\nFirst, we establish the given parameters. The domain has length $L=1$ discretized with $N=100$ points, so the grid spacing is $\\Delta x = L/N = 1/100 = 0.01$. The diffusion coefficients are $\\kappa_1 = 10^{-3}$ and $\\kappa_2 = 5 \\times 10^{-4}$. The explicit transport substep is solved with an RKC method of $s=20$ stages. The splitting error tolerance is $\\varepsilon = 10^{-2}$.\n\n**Constraint 1: Stability of the Transport Substep**\n\nThe transport substep involves solving the ODE $\\frac{d\\mathbf{z}}{dt} = A\\mathbf{z}$ using an explicit RKC method. The stability of this method depends on the eigenvalues of the operator $A$. The problem specifies that the contribution from advection can be neglected for the stability analysis, so we consider the diffusion part of the operator, $A \\approx L \\otimes D$.\n\nThe eigenvalues of the Kronecker product $L \\otimes D$ are the products of the eigenvalues of $L$ and $D$.\nThe eigenvalues of the diagonal matrix $D = \\mathrm{diag}(\\kappa_1, \\kappa_2)$ are its diagonal entries, $\\kappa_1=10^{-3}$ and $\\kappa_2=5 \\times 10^{-4}$.\nThe problem states the eigenvalues of the discrete Laplacian $L$ on a periodic grid are $\\mu(k) = -\\frac{4}{\\Delta x^2}\\sin^2(\\frac{\\pi k}{N})$ for $k=0, \\dots, N-1$. This operator is negative semi-definite.\n\nThe eigenvalues of the diffusion operator $L \\otimes D$ are thus $\\lambda_{k,j} = \\mu_k \\kappa_j$, which are all real and non-positive. Stability of the explicit scheme is governed by the most negative eigenvalue, $\\lambda_{\\min}$, which has the largest magnitude. This occurs for the largest magnitude eigenvalue of $L$ and the largest eigenvalue of $D$.\n\nThe spectral radius of $L$ is $\\rho(L) = \\max_k |\\mu_k| = \\frac{4}{\\Delta x^2}$.\nNumerically, $\\rho(L) = \\frac{4}{(0.01)^2} = \\frac{4}{10^{-4}} = 4 \\times 10^4$.\nThe most negative eigenvalue of $L$ is $-\\rho(L) = -4 \\times 10^4$.\nThe largest diffusion coefficient is $\\kappa_{\\max} = \\max(\\kappa_1, \\kappa_2) = 10^{-3}$.\n\nThe most negative eigenvalue of the approximate transport operator is:\n$$\n\\lambda_{\\min}(L \\otimes D) = (-\\rho(L)) \\cdot \\kappa_{\\max} = (-4 \\times 10^4) \\cdot (10^{-3}) = -40\n$$\nThe stability interval for the $s=20$ stage RKC method is given as approximately $[-2s^2, 0]$. The stability condition requires that the product of the time step $h$ and any eigenvalue $\\lambda$ of the system matrix lies within this interval. For the most restrictive eigenvalue $\\lambda_{\\min}(L \\otimes D)$, we need:\n$$\nh \\cdot \\lambda_{\\min}(L \\otimes D) \\ge -2s^2\n$$\n$$\nh \\cdot (-40) \\ge -2 \\cdot (20)^2 = -2 \\cdot 400 = -800\n$$\nSolving for $h$, we find the stability-imposed limit:\n$$\nh \\le \\frac{-800}{-40} = 20\n$$\nSo, $h_{\\text{stability}} = 20$ seconds.\n\n**Constraint 2: Splitting Error**\n\nThe Lie splitting scheme is a first-order operator splitting method. The local error for a single time step $h$ is given by the Baker-Campbell-Hausdorff (BCH) formula. The exact evolution operator is $\\exp(h(A+B))$, while the Lie splitting operator is $\\exp(hB)\\exp(hA)$. The leading-order error term is:\n$$\n\\mathcal{E}(h) = \\exp(hB)\\exp(hA) - \\exp(h(A+B)) \\approx -\\frac{h^2}{2}[A,B]\n$$\nwhere $[A,B] = AB-BA$ is the commutator of the operators. The constraint is that the norm of the local error vector, relative to the solution norm, must not exceed $\\varepsilon$:\n$$\n\\frac{\\|-\\frac{h^2}{2}[A,B]\\mathbf{y}\\|}{\\|\\mathbf{y}\\|} \\le \\varepsilon\n$$\nThis is bounded by $\\frac{h^2}{2}\\|[A,B]\\| \\le \\varepsilon$. We will use the spectral norm (2-norm). The constraint on $h$ is then:\n$$\nh \\le \\sqrt{\\frac{2\\varepsilon}{\\|[A,B]\\|_2}}\n$$\nWe need to compute the commutator $[A,B]$. Given $A = A_{\\mathrm{adv}} \\otimes I_2 + L \\otimes D$ and $B = I_N \\otimes R$, we use properties of the Kronecker product:\n$$\nAB = (A_{\\mathrm{adv}} \\otimes I_2 + L \\otimes D)(I_N \\otimes R) = A_{\\mathrm{adv}} \\otimes R + L \\otimes (DR)\n$$\n$$\nBA = (I_N \\otimes R)(A_{\\mathrm{adv}} \\otimes I_2 + L \\otimes D) = A_{\\mathrm{adv}} \\otimes R + L \\otimes (RD)\n$$\nThe commutator is:\n$$\n[A,B] = AB - BA = (A_{\\mathrm{adv}} \\otimes R + L \\otimes DR) - (A_{\\mathrm{adv}} \\otimes R + L \\otimes RD) = L \\otimes (DR - RD) = L \\otimes [D,R]\n$$\nNow we compute the matrix commutator $[D,R]$:\n$D = \\begin{pmatrix} 10^{-3}  0 \\\\ 0  5 \\times 10^{-4} \\end{pmatrix}$, $R = \\begin{pmatrix} -500  200 \\\\ 50  -300 \\end{pmatrix}$.\n$$\nDR = \\begin{pmatrix} 10^{-3}  0 \\\\ 0  5 \\times 10^{-4} \\end{pmatrix} \\begin{pmatrix} -500  200 \\\\ 50  -300 \\end{pmatrix} = \\begin{pmatrix} -0.5  0.2 \\\\ 0.025  -0.15 \\end{pmatrix}\n$$\n$$\nRD = \\begin{pmatrix} -500  200 \\\\ 50  -300 \\end{pmatrix} \\begin{pmatrix} 10^{-3}  0 \\\\ 0  5 \\times 10^{-4} \\end{pmatrix} = \\begin{pmatrix} -0.5  0.1 \\\\ 0.05  -0.15 \\end{pmatrix}\n$$\n$$\n[D,R] = DR - RD = \\begin{pmatrix} 0  0.1 \\\\ -0.025  0 \\end{pmatrix}\n$$\nNext, we find the norm $\\|[A,B]\\|_2 = \\|L \\otimes [D,R]\\|_2$. Using the property $\\|P \\otimes Q\\|_2 = \\|P\\|_2 \\|Q\\|_2$:\n$\\|L\\|_2 = \\rho(L) = \\frac{4}{\\Delta x^2} = \\frac{4}{(0.01)^2} = 4 \\times 10^4$.\nTo find $\\|[D,R]\\|_2$, we compute its largest singular value, which is the square root of the largest eigenvalue of $[D,R]^T[D,R]$. Let $C = [D,R]$.\n$$\nC^T C = \\begin{pmatrix} 0  -0.025 \\\\ 0.1  0 \\end{pmatrix} \\begin{pmatrix} 0  0.1 \\\\ -0.025  0 \\end{pmatrix} = \\begin{pmatrix} (-0.025)^2  0 \\\\ 0  (0.1)^2 \\end{pmatrix} = \\begin{pmatrix} 6.25 \\times 10^{-4}  0 \\\\ 0  10^{-2} \\end{pmatrix}\n$$\nThe eigenvalues are $6.25 \\times 10^{-4}$ and $10^{-2}$. The largest is $10^{-2}$.\n$\\|[D,R]\\|_2 = \\sqrt{10^{-2}} = 0.1$.\nSo, the norm of the full commutator is:\n$$\n\\|[A,B]\\|_2 = \\|L\\|_2 \\cdot \\|[D,R]\\|_2 = (4 \\times 10^4) \\cdot 0.1 = 4 \\times 10^3\n$$\nNow we can compute the time step limit from the error constraint:\n$$\nh^2 \\le \\frac{2\\varepsilon}{\\|[A,B]\\|_2} = \\frac{2 \\cdot 10^{-2}}{4 \\times 10^3} = 0.5 \\times 10^{-5} = 5 \\times 10^{-6}\n$$\n$$\nh \\le \\sqrt{5 \\times 10^{-6}} = \\sqrt{5} \\times 10^{-3} \\approx 2.2360679... \\times 10^{-3}\n$$\nSo, $h_{\\text{error}} \\approx 2.236 \\times 10^{-3}$ seconds.\n\n**Conclusion**\n\nWe must satisfy both constraints, so the maximum permissible time step $h$ is the minimum of the two derived bounds:\n$$\nh_{\\max} = \\min(h_{\\text{stability}}, h_{\\text{error}}) = \\min(20, 2.236 \\times 10^{-3}) = 2.236 \\times 10^{-3}\n$$\nThe splitting error is the limiting factor. The problem asks for the answer rounded to four significant figures.\n$h_{\\max} \\approx 2.236 \\times 10^{-3}$ seconds.", "answer": "$$\\boxed{2.236 \\times 10^{-3}}$$", "id": "3617594"}, {"introduction": "While analytical studies provide crucial insights, the ultimate test of a numerical method lies in its implementation for challenging nonlinear problems. This practice moves from theory to code, tackling a stiff, nonlinear radiative cooling model [@problem_id:3617599]. You will implement and compare two popular implicit methods, a Diagonally Implicit Runge-Kutta (DIRK) scheme and a Backward Differentiation Formula (BDF), focusing on how different strategies for handling the Jacobian matrix within the nonlinear solver impact stability and performance.", "problem": "Consider a radiative cooling model for a parcel of geophysical fluid with heat capacity per unit mass $C$ and effective emissivity $\\varepsilon$ radiating to space with Stefan–Boltzmann constant $\\sigma$. The balance of energy for temperature $T(t)$ in Kelvin (K) is given by the fundamental statement of energy conservation $C \\,\\frac{dT}{dt} = - Q(T)$, where $Q(T)$ is the radiative loss. For blackbody radiative loss, $Q(T) = \\varepsilon \\sigma T^4$, so dividing by $C$ yields the ordinary differential equation (ODE)\n$$\n\\frac{dT}{dt} = - k \\, T^4,\n$$\nwhere $k = \\frac{\\varepsilon \\sigma}{C}$ and $k  0$ has units $\\mathrm{s^{-1} K^{-3}}$. This ODE can be written in the form $\\dot{T} = -\\alpha(T)\\,T$ with $\\alpha(T) = k T^3$. The Jacobian of the right-hand side with respect to $T$ is\n$$\nJ(T) = \\frac{d}{dT}\\left(-\\alpha(T)\\,T\\right) = -\\alpha(T) - \\alpha'(T)\\,T = -4\\,k\\,T^3,\n$$\nso $J$ depends on time through $T(t)$, i.e., $J = J(t)$.\n\nYour task is to implement and compare two time integration methods for this nonlinear ODE under two Jacobian handling strategies, and to evaluate stability in the sense of maintaining a physically admissible decay:\n- Time integration methods:\n  - Diagonally Implicit Runge–Kutta (DIRK) of order $2$ with parameter $\\gamma = 1 - 1/\\sqrt{2}$, used in its singly diagonally implicit form with two stages and weights satisfying second-order accuracy.\n  - Backward Differentiation Formula (BDF) of order $2$ (BDF2), with BDF of order $1$ (Backward Euler) used for the first step to start the multistep method.\n- Jacobian handling strategies per time step:\n  - Freeze-$J$: compute $J$ at the beginning of each time step (at $T_n$) and keep it fixed for all implicit solves within that time step (simplified Newton with a constant Jacobian per step).\n  - Predictor–Corrector (PC): predict $T$ using an explicit formula appropriate to the method and then perform a corrector with Newton iterations updating the Jacobian at the current iterate within the same time step (full Newton with a refreshed Jacobian).\n\nDefine stability for this nonlinear decay problem as follows. For a given simulation with time step $\\Delta t$ in seconds (s) and $N$ steps, producing temperatures $\\{T_n\\}_{n=0}^{N}$ in Kelvin (K), declare the run stable if all of the following hold:\n- Positivity: $T_n  0$ for all $n$.\n- Monotone decay: $T_{n+1} \\le T_n$ for all $n$.\n- Successful nonlinear solves: all implicit stages/steps converge under the chosen Jacobian strategy.\n\nIf any of these conditions fails, declare the run unstable.\n\nImplement both methods and both Jacobian strategies for the nonlinear ODE with $\\alpha(T) = k T^3$. Use Newton iterations for the implicit equations with a reasonable maximum iteration cap and tolerance to test convergence. Do not artificially clip negative temperatures to enforce stability; negative temperatures should count as instability. Use Kelvin (K) for temperature and seconds (s) for time step sizes.\n\nTest Suite. Run the following $8$ test cases, each specified by a tuple $(\\text{method}, \\text{strategy}, k, T_0, \\Delta t, N)$:\n- Case $1$: $(\\text{DIRK}, \\text{freeze}, 10^{-10}, 300, 10, 20)$\n- Case $2$: $(\\text{DIRK}, \\text{freeze}, 10^{-6}, 300, 10, 5)$\n- Case $3$: $(\\text{DIRK}, \\text{PC}, 10^{-6}, 300, 10, 5)$\n- Case $4$: $(\\text{BDF2}, \\text{freeze}, 10^{-10}, 300, 10, 20)$\n- Case $5$: $(\\text{BDF2}, \\text{freeze}, 10^{-6}, 300, 10, 5)$\n- Case $6$: $(\\text{BDF2}, \\text{PC}, 10^{-6}, 300, 10, 5)$\n- Case $7$: $(\\text{DIRK}, \\text{PC}, 10^{-6}, 300, 100, 3)$\n- Case $8$: $(\\text{BDF2}, \\text{PC}, 10^{-6}, 300, 100, 3)$\n\nRequired Output. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the cases listed above. Each result must be a boolean indicating stability as defined above (i.e., either $\\text{True}$ or $\\text{False}$), so the output must appear as\n$[\\text{result}_1,\\text{result}_2,\\ldots,\\text{result}_8]$ on a single line.\n\nExpress any intermediate values, if printed for debugging during development, in Kelvin (K) and seconds (s), but the final submitted program must print only the specified single-line boolean list.", "solution": "We begin with the physical model based on energy conservation. For a parcel with heat capacity per unit mass $C$ under radiative cooling with blackbody flux $Q(T) = \\varepsilon \\sigma T^4$, the energy equation $C \\,\\frac{dT}{dt} = - Q(T)$ yields\n$$\n\\frac{dT}{dt} = - \\frac{\\varepsilon \\sigma}{C} \\, T^4 \\equiv - k \\, T^4,\n$$\nwith $k = \\frac{\\varepsilon \\sigma}{C}  0$ and $T(t)  0$ under physically admissible conditions. Rewriting as $\\dot{T} = -\\alpha(T)\\, T$ gives $\\alpha(T) = k T^3$. The Jacobian of the right-hand side $f(T) = -\\alpha(T)\\,T$ is\n$$\nJ(T) = \\frac{df}{dT} = -\\alpha(T) - \\alpha'(T)\\,T = -k T^3 - 3k T^2 \\, T = -4 k T^3.\n$$\nSince $T$ evolves in time, $J$ is time dependent, i.e., $J = J(t)$.\n\nFor stiff nonlinear decay problems, implicit time integration methods are favored due to stability properties. We consider two methods:\n\n$1.$ Diagonally Implicit Runge–Kutta (DIRK) of order $2$, in singly diagonally implicit form. Let $\\gamma = 1 - 1/\\sqrt{2}$, which ensures second-order accuracy and $L$-stability for the associated scheme. The method uses two stages with the structure\n$$\nY_1 = T_n + \\Delta t \\, \\gamma \\, f(Y_1), \\\\\nY_2 = T_n + \\Delta t \\, (1-\\gamma)\\, f(Y_1) + \\Delta t \\, \\gamma \\, f(Y_2),\n$$\nand then sets $T_{n+1} = Y_2$. Each stage is an implicit nonlinear equation in the scalar $Y$, requiring a nonlinear solve per stage. For each stage, define the residual function\n$$\nG_1(Y) = Y - T_n - \\Delta t \\, \\gamma \\, f(Y), \\quad\nG_2(Y) = Y - T_n - \\Delta t \\, (1-\\gamma)\\, f(Y_1) - \\Delta t \\, \\gamma \\, f(Y).\n$$\nNewton’s method updates are $Y \\leftarrow Y - \\frac{G(Y)}{G'(Y)}$, with derivative for each stage\n$$\nG_1'(Y) = 1 - \\Delta t \\, \\gamma \\, J(Y), \\quad\nG_2'(Y) = 1 - \\Delta t \\, \\gamma \\, J(Y).\n$$\nUnder the Freeze-$J$ strategy, we evaluate $J$ once at $T_n$ and treat $G'(Y)$ as constant within the time step, i.e., $G'(Y) \\approx 1 - \\Delta t \\, \\gamma \\, J(T_n)$. Under Predictor–Corrector (PC), we use a predictor (for instance, the explicit Euler predictor $Y_1^{(0)} = T_n + \\Delta t \\, \\gamma \\, f(T_n)$; $Y_2^{(0)} = T_n + \\Delta t \\, (1-\\gamma)\\, f(Y_1^{(0)}) + \\Delta t \\, \\gamma \\, f(T_n)$) and then perform Newton iterations with updated $J(Y)$ at the current iterate, i.e., full Newton.\n\n$2.$ Backward Differentiation Formula of order $2$ (BDF2). The BDF2 formula for advancing from $(T_{n-1}, T_n)$ to $T_{n+1}$ is\n$$\n\\frac{3 \\, T_{n+1} - 4\\, T_n + T_{n-1}}{2 \\, \\Delta t} = f(T_{n+1}),\n$$\nwith the first step taken by Backward Euler (BDF1)\n$$\n\\frac{T_1 - T_0}{\\Delta t} = f(T_1).\n$$\nDefine the residual for BDF2 as\n$$\nG(Y) = \\frac{3Y - 4 T_n + T_{n-1}}{2 \\, \\Delta t} - f(Y), \\quad G'(Y) = \\frac{3}{2 \\, \\Delta t} - J(Y).\n$$\nUnder Freeze-$J$, we set $G'(Y) \\approx \\frac{3}{2 \\, \\Delta t} - J(T_n)$ constant within the step. Under PC, we use a predictor such as the explicit Adams–Bashforth of order $2$ (AB2), namely\n$$\nT_{n+1}^{\\text{pred}} = T_n + \\Delta t \\left( \\frac{3}{2} f(T_n) - \\frac{1}{2} f(T_{n-1}) \\right),\n$$\nand then perform Newton iterations updating $J(Y)$ as $Y$ changes.\n\nStability evaluation. For the nonlinear decay $f(T) = -k T^4$ with Jacobian $J(T) = -4 k T^3$, we define stability for a run as the conjunction of:\n- Positivity: $T_n  0$ for all $n$.\n- Monotone decay: $T_{n+1} \\le T_n$ for all $n$.\n- Convergence of all implicit solves: all Newton iterations reach the tolerance within the iteration cap.\n\nThis definition aligns with the physical expectation of radiative cooling: temperature should decrease and remain positive.\n\nAlgorithmic design details:\n- Use Newton’s method with a maximum number of iterations (for example, $20$) and a strict residual tolerance (for example, $10^{-12}$ in Kelvin units for the residual expressed in temperature).\n- For the Freeze-$J$ strategy, treat the derivative $G'(Y)$ as constant per time step (evaluated at $T_n$). For DIRK, this applies to both stages; for BDF2, to the single stage per time step. Use the current $Y$ in $G(Y)$ but do not refresh $J$ during the iterations.\n- For PC, compute a predictor and then perform full Newton iterations refreshing $J(Y)$ at each iteration. For the first BDF step, use Backward Euler with an explicit Euler predictor $T_1^{(0)} = T_0 + \\Delta t f(T_0)$.\n- Declare failure if a Newton step proposes a negative temperature or if the residual does not converge within the iteration cap.\n- After completing the time stepping, apply the stability checks on the sequence $\\{T_n\\}$.\n\nTest suite coverage rationale:\n- Small $k$ and moderate $\\Delta t$ test cases (Cases $1$ and $4$) represent a nonstiff regime, where both methods and both Jacobian treatments should be stable.\n- Larger $k$ with moderate $\\Delta t$ (Cases $2$, $3$, $5$, $6$) represent stiff regimes. We test whether Freeze-$J$ is sufficient (it may fail due to poor linearization), and whether PC stabilization succeeds by refreshing the Jacobian.\n- Very large $\\Delta t$ with large $k$ (Cases $7$, $8$) test extreme stiffness. The DIRK scheme with $\\gamma = 1 - 1/\\sqrt{2}$ is $L$-stable and should strongly damp. BDF2 is not $L$-stable; under extreme stiffness it can exhibit slow damping or oscillatory behavior on the linear test equation, but for the nonlinear decay it may still converge with PC. The stability check imposes positivity and monotone decay.\n\nOutput specification: The final line must be a single list of booleans in the order of the test cases. No other output should be printed.\n\nBy following these principles, the program implements principled implicit solvers for a radiative cooling ODE and evaluates stability in the presence of time-dependent Jacobians under two strategies: Freeze-$J$ and Predictor–Corrector.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef f(T, k):\n    # RHS: dT/dt = -k * T^4\n    return -k * T**4\n\ndef J(T, k):\n    # Jacobian df/dT = -4 k T^3\n    return -4.0 * k * T**3\n\ndef newton_solve_scalar(G, dG_const, dG_func, y0, max_iter=20, tol=1e-12, freeze=True):\n    \"\"\"\n    Solve G(y) = 0 with Newton's method.\n    If freeze=True, use dG_const as the constant derivative.\n    Otherwise, use dG_func(y) to compute derivative at current iterate.\n    Returns (y, converged).\n    \"\"\"\n    y = y0\n    for _ in range(max_iter):\n        res = G(y)\n        if abs(res)  tol:\n            return y, True\n        if freeze:\n            dGy = dG_const\n        else:\n            dGy = dG_func(y)\n        # If derivative is zero or extremely small, fail\n        if dGy == 0.0 or not np.isfinite(dGy):\n            return y, False\n        step = -res / dGy\n        y_new = y + step\n        # Physical admissibility: negative temperature is not allowed\n        if y_new = 0.0 or not np.isfinite(y_new):\n            return y, False\n        y = y_new\n    # Not converged within iteration cap\n    return y, False\n\ndef dirk2_integrate(T0, k, dt, N, strategy):\n    \"\"\"\n    DIRK(2) with gamma = 1 - 1/sqrt(2).\n    strategy: 'freeze' or 'pc'\n    Returns (temps, all_converged)\n    \"\"\"\n    gamma = 1.0 - 1.0 / np.sqrt(2.0)\n    temps = [T0]\n    all_converged = True\n    for n in range(N):\n        Tn = temps[-1]\n        if Tn = 0.0 or not np.isfinite(Tn):\n            all_converged = False\n            break\n\n        # Stage 1: solve Y1 - Tn - dt*gamma*f(Y1) = 0\n        def G1(y):\n            return y - Tn - dt * gamma * f(y, k)\n        if strategy == 'freeze':\n            dG1_const = 1.0 - dt * gamma * J(Tn, k)\n            dG1_func = None\n            y1_guess = Tn  # reasonable initial guess\n            y1, ok1 = newton_solve_scalar(G1, dG1_const, dG1_func, y1_guess, freeze=True)\n        else:  # predictor-corrector\n            def dG1_func(y):\n                return 1.0 - dt * gamma * J(y, k)\n            y1_guess = Tn + dt * gamma * f(Tn, k)  # explicit predictor\n            y1, ok1 = newton_solve_scalar(G1, None, dG1_func, y1_guess, freeze=False)\n        if not ok1 or y1 = 0.0:\n            all_converged = False\n            break\n\n        # Stage 2: solve Y2 - Tn - dt*(1-gamma)*f(Y1) - dt*gamma*f(Y2) = 0\n        def G2(y):\n            return y - Tn - dt * (1.0 - gamma) * f(y1, k) - dt * gamma * f(y, k)\n        if strategy == 'freeze':\n            dG2_const = 1.0 - dt * gamma * J(Tn, k)\n            dG2_func = None\n            y2_guess = y1  # start near previous stage\n            y2, ok2 = newton_solve_scalar(G2, dG2_const, dG2_func, y2_guess, freeze=True)\n        else:\n            def dG2_func(y):\n                return 1.0 - dt * gamma * J(y, k)\n            # predictor: use f(Tn) for the implicit part to seed Y2\n            y2_guess = Tn + dt * (1.0 - gamma) * f(y1, k) + dt * gamma * f(Tn, k)\n            y2, ok2 = newton_solve_scalar(G2, None, dG2_func, y2_guess, freeze=False)\n        if not ok2 or y2 = 0.0:\n            all_converged = False\n            break\n\n        temps.append(y2)\n\n    return np.array(temps), all_converged\n\ndef bdf2_integrate(T0, k, dt, N, strategy):\n    \"\"\"\n    BDF2 with Backward Euler start.\n    strategy: 'freeze' or 'pc'\n    Returns (temps, all_converged)\n    \"\"\"\n    temps = [T0]\n    all_converged = True\n\n    # First step: Backward Euler\n    Tn = temps[-1]\n    def G_be(y):\n        return y - Tn - dt * f(y, k)\n    if strategy == 'freeze':\n        dG_be_const = 1.0 - dt * J(Tn, k)\n        dG_be_func = None\n        y_guess = Tn  # initial guess\n        T1, ok1 = newton_solve_scalar(G_be, dG_be_const, dG_be_func, y_guess, freeze=True)\n    else:\n        def dG_be_func(y):\n            return 1.0 - dt * J(y, k)\n        y_guess = Tn + dt * f(Tn, k)  # explicit Euler predictor\n        T1, ok1 = newton_solve_scalar(G_be, None, dG_be_func, y_guess, freeze=False)\n    if not ok1 or T1 = 0.0 or not np.isfinite(T1):\n        all_converged = False\n        return np.array(temps), all_converged\n    temps.append(T1)\n\n    # Subsequent steps: BDF2\n    for n in range(1, N):\n        Tnm1 = temps[-2]\n        Tn = temps[-1]\n        if Tn = 0.0 or Tnm1 = 0.0 or not (np.isfinite(Tn) and np.isfinite(Tnm1)):\n            all_converged = False\n            break\n\n        def G_bdf2(y):\n            return (3.0 * y - 4.0 * Tn + Tnm1) / (2.0 * dt) - f(y, k)\n        if strategy == 'freeze':\n            dG_bdf2_const = 3.0 / (2.0 * dt) - J(Tn, k)\n            dG_bdf2_func = None\n            y_guess = Tn  # start from current\n            Tnp1, ok = newton_solve_scalar(G_bdf2, dG_bdf2_const, dG_bdf2_func, y_guess, freeze=True)\n        else:\n            def dG_bdf2_func(y):\n                return 3.0 / (2.0 * dt) - J(y, k)\n            # AB2 predictor\n            y_guess = Tn + dt * (1.5 * f(Tn, k) - 0.5 * f(Tnm1, k))\n            if y_guess = 0.0 or not np.isfinite(y_guess):\n                y_guess = max(Tn * 0.5, 1e-12)\n            Tnp1, ok = newton_solve_scalar(G_bdf2, None, dG_bdf2_func, y_guess, freeze=False)\n        if not ok or Tnp1 = 0.0 or not np.isfinite(Tnp1):\n            all_converged = False\n            break\n\n        temps.append(Tnp1)\n\n    return np.array(temps), all_converged\n\ndef is_stable(temps, all_converged, tol=1e-12):\n    \"\"\"\n    Stability criteria:\n    - all_converged True\n    - all temperatures positive\n    - monotone decay: T_{n+1} = T_n for all n (within tol)\n    \"\"\"\n    if not all_converged:\n        return False\n    if np.any(~np.isfinite(temps)):\n        return False\n    if np.any(temps = 0.0):\n        return False\n    # Monotone non-increasing\n    diffs = np.diff(temps)\n    if np.any(diffs  tol):\n        return False\n    return True\n\ndef run_case(method, strategy, k, T0, dt, N):\n    \"\"\"\n    Run one test case and return boolean stability.\n    \"\"\"\n    if method == 'DIRK':\n        temps, conv = dirk2_integrate(T0=T0, k=k, dt=dt, N=N, strategy=strategy)\n    elif method == 'BDF2':\n        temps, conv = bdf2_integrate(T0=T0, k=k, dt=dt, N=N, strategy=strategy)\n    else:\n        raise ValueError(\"Unknown method.\")\n    return is_stable(temps, conv)\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        ('DIRK', 'freeze', 1e-10, 300.0, 10.0, 20),\n        ('DIRK', 'freeze', 1e-6,  300.0, 10.0, 5),\n        ('DIRK', 'pc',     1e-6,  300.0, 10.0, 5),\n        ('BDF2', 'freeze', 1e-10, 300.0, 10.0, 20),\n        ('BDF2', 'freeze', 1e-6,  300.0, 10.0, 5),\n        ('BDF2', 'pc',     1e-6,  300.0, 10.0, 5),\n        ('DIRK', 'pc',     1e-6,  300.0, 100.0, 3),\n        ('BDF2', 'pc',     1e-6,  300.0, 100.0, 3),\n    ]\n\n    results = []\n    for case in test_cases:\n        method, strategy, k, T0, dt, N = case\n        result = run_case(method, strategy, k, T0, dt, N)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3617599"}]}