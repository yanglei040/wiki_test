{"hands_on_practices": [{"introduction": "Before attempting to solve any inverse problem, two fundamental questions must be addressed: does a solution exist, and is it unique? This exercise provides a direct, hands-on exploration of these questions using a small, underdetermined system that can be solved analytically [@problem_id:3608170]. By characterizing the complete family of solutions, you will gain a concrete understanding of how the null space of the forward operator gives rise to non-uniqueness, a central challenge in geophysical inversion.", "problem": "Consider a linear forward model in computational geophysics where the data vector $\\mathbf{d} \\in \\mathbb{R}^{2}$ measures the superposed response of three subsurface source elements modeled by the parameter vector $\\mathbf{m} \\in \\mathbb{R}^{3}$. The discretized forward operator is the matrix $G \\in \\mathbb{R}^{2 \\times 3}$ such that $\\mathbf{d} = G \\mathbf{m}$. Assume linear superposition holds (a well-tested fact for many potential-field methods), and suppose the influence of the third source element is equal to the sum of the influences of the first two due to geometric symmetry of the survey configuration. This yields the synthetic matrix\n$$\nG = \\begin{pmatrix}\n1 & 0 & 1 \\\\\n0 & 1 & 1\n\\end{pmatrix},\n$$\nand observed data\n$$\n\\mathbf{d} = \\begin{pmatrix}\n3 \\\\\n5\n\\end{pmatrix}.\n$$\nUsing only core definitions from linear inverse problems, determine whether an exact solution $\\mathbf{m}$ exists and whether the solution is unique. Then, compute the complete set of all model vectors $\\mathbf{m}$ that exactly fit $\\mathbf{d} = G \\mathbf{m}$. Express the answer as a single parametric analytic expression for $\\mathbf{m}$ in terms of a real scalar parameter. No rounding is required.", "solution": "The problem requires an analysis of the linear inverse problem defined by the equation $\\mathbf{d} = G \\mathbf{m}$, where the data vector is $\\mathbf{d} = \\begin{pmatrix} 3 \\\\ 5 \\end{pmatrix}$, the model parameter vector is $\\mathbf{m} = \\begin{pmatrix} m_1 \\\\ m_2 \\\\ m_3 \\end{pmatrix}$, and the forward operator is the matrix $G = \\begin{pmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\end{pmatrix}$.\n\nThe problem is first validated. All givens are explicitly stated: the matrix $G$, the vector $\\mathbf{d}$, and the relationship $\\mathbf{d} = G \\mathbf{m}$. The context is computational geophysics, and the formulation is a standard linear system, which is scientifically grounded and mathematically formalizable. The problem is self-contained, consistent, and lacks any ambiguity or subjective elements. The question asks for an analysis of existence and uniqueness, which is a standard procedure for linear systems, and then to find the solution set. The problem is therefore deemed valid.\n\nThe core of the problem is to solve the system of linear equations:\n$$\n\\begin{pmatrix}\n1 & 0 & 1 \\\\\n0 & 1 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\nm_1 \\\\\nm_2 \\\\\nm_3\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n3 \\\\\n5\n\\end{pmatrix}\n$$\nThis matrix equation corresponds to the following system of two equations with three unknowns:\n$$\n\\begin{cases}\n1m_1 + 0m_2 + 1m_3 = 3 \\\\\n0m_1 + 1m_2 + 1m_3 = 5\n\\end{cases}\n\\implies\n\\begin{cases}\nm_1 + m_3 = 3 \\\\\nm_2 + m_3 = 5\n\\end{cases}\n$$\n\nFirst, we address the existence of a solution. A solution exists if and only if the data vector $\\mathbf{d}$ lies in the column space of the matrix $G$, denoted $\\text{col}(G)$. The matrix $G$ is of size $2 \\times 3$, mapping from $\\mathbb{R}^3$ to $\\mathbb{R}^2$. The columns of $G$ are $\\mathbf{g}_1 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$, $\\mathbf{g}_2 = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$, and $\\mathbf{g}_3 = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$. The first two columns, $\\mathbf{g}_1$ and $\\mathbf{g}_2$, are the standard basis vectors for $\\mathbb{R}^2$. As they are linearly independent, they span the entire space $\\mathbb{R}^2$. Therefore, the rank of $G$ is $2$, and its column space is all of $\\mathbb{R}^2$. Since $\\text{col}(G) = \\mathbb{R}^2$, any vector $\\mathbf{d} \\in \\mathbb{R}^2$, including the given $\\mathbf{d} = \\begin{pmatrix} 3 \\\\ 5 \\end{pmatrix}$, is in the column space of $G$. Consequently, an exact solution $\\mathbf{m}$ is guaranteed to exist.\n\nNext, we address the uniqueness of the solution. A unique solution exists if and only if the null space of $G$, denoted $\\text{Null}(G)$, contains only the zero vector. The null space is the set of all vectors $\\mathbf{m}_{\\text{null}}$ such that $G\\mathbf{m}_{\\text{null}} = \\mathbf{0}$. The Rank-Nullity Theorem states that for a matrix with $n$ columns, $\\text{rank}(G) + \\text{nullity}(G) = n$. In this case, $G$ has $n=3$ columns and we have determined its rank to be $2$. Thus, the nullity is:\n$$\n\\text{nullity}(G) = n - \\text{rank}(G) = 3 - 2 = 1\n$$\nSince the nullity is $1$, the null space is a one-dimensional subspace of $\\mathbb{R}^3$, not the trivial set $\\{\\mathbf{0}\\}$. This implies that if a solution exists, it is not unique. There is an infinite family of solutions.\n\nTo find the complete set of all model vectors $\\mathbf{m}$ that satisfy the equation, we find the general solution, which is the sum of a particular solution $\\mathbf{m}_p$ and any vector from the null space.\n$$\n\\mathbf{m} = \\mathbf{m}_p + \\mathbf{m}_{\\text{null}}\n$$\n\nFirst, we find the structure of the null space by solving $G\\mathbf{m} = \\mathbf{0}$:\n$$\n\\begin{cases}\nm_1 + m_3 = 0 \\\\\nm_2 + m_3 = 0\n\\end{cases}\n\\implies\n\\begin{cases}\nm_1 = -m_3 \\\\\nm_2 = -m_3\n\\end{cases}\n$$\nLet $m_3 = \\alpha$, where $\\alpha$ is an arbitrary real scalar parameter. Then $m_1 = -\\alpha$ and $m_2 = -\\alpha$. Any vector in the null space can thus be written as:\n$$\n\\mathbf{m}_{\\text{null}} = \\begin{pmatrix} -\\alpha \\\\ -\\alpha \\\\ \\alpha \\end{pmatrix} = \\alpha \\begin{pmatrix} -1 \\\\ -1 \\\\ 1 \\end{pmatrix}\n$$\n\nSecond, we find one particular solution $\\mathbf{m}_p$ to the inhomogeneous system $G\\mathbf{m} = \\mathbf{d}$:\n$$\n\\begin{cases}\nm_1 + m_3 = 3 \\\\\nm_2 + m_3 = 5\n\\end{cases}\n$$\nSince the system is underdetermined, we can choose a value for one of the variables. A simple choice is to set the free variable $m_3$ to $0$. This yields:\n$$\nm_1 + 0 = 3 \\implies m_1 = 3\n$$\n$$\nm_2 + 0 = 5 \\implies m_2 = 5\n$$\nSo, a particular solution is $\\mathbf{m}_p = \\begin{pmatrix} 3 \\\\ 5 \\\\ 0 \\end{pmatrix}$.\n\nFinally, the complete set of solutions is the sum of the particular solution and the general form of a vector in the null space:\n$$\n\\mathbf{m}(\\alpha) = \\mathbf{m}_p + \\mathbf{m}_{\\text{null}} = \\begin{pmatrix} 3 \\\\ 5 \\\\ 0 \\end{pmatrix} + \\alpha \\begin{pmatrix} -1 \\\\ -1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 3 - \\alpha \\\\ 5 - \\alpha \\\\ \\alpha \\end{pmatrix}\n$$\nThis expression parametrically describes all model vectors $\\mathbf{m} \\in \\mathbb{R}^3$ that exactly fit the observed data for any real number $\\alpha$.", "answer": "$$\n\\boxed{\\begin{pmatrix} 3 - \\alpha \\\\ 5 - \\alpha \\\\ \\alpha \\end{pmatrix}}\n$$", "id": "3608170"}, {"introduction": "While analytical methods are insightful for small problems, real-world applications require robust computational tools. The Singular Value Decomposition (SVD) is the cornerstone for analyzing linear systems, as it cleanly separates the model space into components the data can resolve and those it cannot. This practice guides you through using SVD to compute bases for the null space $\\mathcal{N}(G)$ and the row space $\\mathcal{R}(G^\\top)$, and numerically verify their properties, bridging the gap between abstract linear algebra and practical geophysical analysis [@problem_id:3608192].", "problem": "Consider the linear forward model in computational geophysics, where a data vector $d \\in \\mathbb{R}^m$ is related to a model vector $m \\in \\mathbb{R}^n$ through a linear operator $G \\in \\mathbb{R}^{m \\times n}$ via $d = G m$. In the analysis of linear inverse problems, understanding the structure of the null space $\\mathcal{N}(G)$ and the range of the transpose $\\mathcal{R}(G^\\top)$ is essential for quantifying model non-uniqueness and data-resolvable directions. Your task is to compute orthonormal bases for $\\mathcal{N}(G)$ and $\\mathcal{R}(G^\\top)$ using Singular Value Decomposition (SVD), and to verify the rank-nullity theorem $n = \\mathrm{rank}(G) + \\dim \\mathcal{N}(G)$ for several test cases. Use definitions and fundamental results only: $\\mathcal{N}(G) = \\{ x \\in \\mathbb{R}^n : G x = 0 \\}$, $\\mathcal{R}(G^\\top)$ is the column space of $G^\\top$ (equivalently the row space of $G$), and SVD factorizes $G$ as $G = U \\Sigma V^\\top$, where $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{n \\times n}$ are orthogonal, and $\\Sigma \\in \\mathbb{R}^{m \\times n}$ is diagonal (possibly rectangular) with nonnegative singular values.\n\nDefine the numerical rank $\\mathrm{rank}(G)$ as the number of singular values strictly greater than a threshold $t$ chosen as $t = \\max(m, n) \\, \\epsilon \\, \\sigma_{\\max}$, where $\\epsilon$ is machine epsilon for double precision and $\\sigma_{\\max}$ is the largest singular value of $G$. Using this rank, construct an orthonormal basis for $\\mathcal{N}(G)$ from the right singular vectors associated with singular values at or below $t$, and an orthonormal basis for $\\mathcal{R}(G^\\top)$ from the right singular vectors associated with singular values above $t$. Then verify the rank-nullity theorem and numerically check the orthogonality between $\\mathcal{N}(G)$ and $\\mathcal{R}(G^\\top)$ by computing the maximum absolute entry of the matrix of inner products between basis vectors from these two subspaces.\n\nUse the following test suite of matrices $G$, each specified to probe a distinct aspect of the computation:\n\n- Case $1$ (rank-deficient rectangular, constructed as a product to enforce a reduced rank): Let\n$$\nA_1 = \\begin{bmatrix}\n1 & 0 & 2 & -1 \\\\\n0 & 1 & 1 & 2 \\\\\n1 & 1 & 0 & 0 \\\\\n2 & -1 & 1 & 1 \\\\\n0 & 0 & 1 & -1\n\\end{bmatrix} \\in \\mathbb{R}^{5 \\times 4}, \\quad\nB_1 = \\begin{bmatrix}\n1 & 2 & 0 & -1 & 0 & 3 & 1 \\\\\n0 & 1 & 1 & 0 & 2 & 0 & -2 \\\\\n2 & 0 & -1 & 1 & 1 & 1 & 0 \\\\\n-1 & 1 & 2 & 0 & -1 & 0 & 1\n\\end{bmatrix} \\in \\mathbb{R}^{4 \\times 7},\n$$\nand define $G_1 = A_1 B_1 \\in \\mathbb{R}^{5 \\times 7}$.\n\n- Case $2$ (square, full rank, well-conditioned, diagonal): Let\n$$\nG_2 = \\mathrm{diag}(3, 2, 1, 4, 5, 6) \\in \\mathbb{R}^{6 \\times 6}.\n$$\n\n- Case $3$ (wide matrix with nontrivial null space, constructed as a product to enforce rank at most $2$): Let\n$$\nA_2 = \\begin{bmatrix}\n1 & 2 \\\\\n-1 & 0 \\\\\n0 & 1\n\\end{bmatrix} \\in \\mathbb{R}^{3 \\times 2}, \\quad\nB_2 = \\begin{bmatrix}\n2 & -1 & 0 & 1 & 3 \\\\\n1 & 0 & 2 & -2 & -1\n\\end{bmatrix} \\in \\mathbb{R}^{2 \\times 5},\n$$\nand define $G_3 = A_2 B_2 \\in \\mathbb{R}^{3 \\times 5}$.\n\n- Case $4$ (square, ill-conditioned with borderline singular values, diagonal): Let\n$$\nG_4 = \\mathrm{diag}(1, 10^{-8}, 10^{-17}, 0) \\in \\mathbb{R}^{4 \\times 4}.\n$$\n\nFor each case, compute:\n- The integer $\\mathrm{rank}(G)$ using the threshold $t$ defined above.\n- The integer $\\dim \\mathcal{N}(G)$ as the number of right singular vectors selected for the null space basis.\n- The integer $n$ (the number of columns of $G$).\n- A boolean indicating whether $n = \\mathrm{rank}(G) + \\dim \\mathcal{N}(G)$ holds.\n- A float equal to the maximum absolute entry of the matrix of inner products between the columns of the null space basis and the columns of the $\\mathcal{R}(G^\\top)$ basis (this quantifies numerical orthogonality; lower values indicate better orthogonality).\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with one item per test case. Each item is itself a list in the order $[\\mathrm{rank}(G), \\dim \\mathcal{N}(G), n, \\text{valid}, \\text{orth}]$, for example, $[[r_1,k_1,n_1,\\text{valid}_1,\\text{orth}_1],[r_2,k_2,n_2,\\text{valid}_2,\\text{orth}_2],\\dots]$. No physical units are involved in this problem; all computations are dimensionless.", "solution": "The problem requires the computation of orthonormal bases for the null space $\\mathcal{N}(G)$ and the range of the transpose $\\mathcal{R}(G^\\top)$ of a given matrix $G$, utilizing its Singular Value Decomposition (SVD). Additionally, we must verify the rank-nullity theorem and assess the numerical orthogonality of the computed bases for several test cases.\n\nThe relationship between the data vector $d \\in \\mathbb{R}^m$ and the model vector $m \\in \\mathbb{R}^n$ is given by the linear forward model $d = G m$, where $G \\in \\mathbb{R}^{m \\times n}$ is the forward operator. The analysis of this system's properties relies on understanding the four fundamental subspaces associated with $G$. This problem focuses on two subspaces of the model space $\\mathbb{R}^n$: the null space $\\mathcal{N}(G)$ and the row space, which is equivalent to the range of the transpose, $\\mathcal{R}(G^\\top)$.\n\nThe null space is defined as $\\mathcal{N}(G) = \\{ x \\in \\mathbb{R}^n : G x = 0 \\}$. It represents the part of the model space that is invisible to the data, leading to non-uniqueness in the inverse problem.\nThe range of the transpose, $\\mathcal{R}(G^\\top)$, is the subspace of $\\mathbb{R}^n$ spanned by the columns of $G^\\top$ (or rows of $G$). It represents the part of the model space that can be constrained by the data. A fundamental result from linear algebra is that these two subspaces are orthogonal complements in $\\mathbb{R}^n$, meaning any vector in $\\mathbb{R}^n$ can be uniquely decomposed into a component in $\\mathcal{R}(G^\\top)$ and a component in $\\mathcal{N}(G)$, and the inner product of any vector from $\\mathcal{R}(G^\\top)$ with any vector from $\\mathcal{N}(G)$ is zero. This implies $\\mathbb{R}^n = \\mathcal{R}(G^\\top) \\oplus \\mathcal{N}(G)$. The rank-nullity theorem is a direct consequence, stating that the dimensions of these subspaces sum to the dimension of the total space: $\\dim(\\mathcal{R}(G^\\top)) + \\dim(\\mathcal{N}(G)) = n$. Since $\\mathrm{rank}(G) = \\dim(\\mathcal{R}(G^\\top))$, the theorem is commonly written as $\\mathrm{rank}(G) + \\dim(\\mathcal{N}(G)) = n$.\n\nThe Singular Value Decomposition (SVD) provides a powerful tool for computing stable orthonormal bases for these subspaces. The SVD of $G$ is the factorization $G = U \\Sigma V^\\top$, where:\n- $U \\in \\mathbb{R}^{m \\times m}$ is an orthogonal matrix whose columns, $u_i$, are the left singular vectors.\n- $V \\in \\mathbb{R}^{n \\times n}$ is an orthogonal matrix whose columns, $v_i$, are the right singular vectors.\n- $\\Sigma \\in \\mathbb{R}^{m \\times n}$ is a rectangular diagonal matrix with non-negative real numbers $\\sigma_i$ on its diagonal, known as the singular values, arranged in non-increasing order: $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge 0$.\n\nLet the rank of $G$ be $r$. This means there are $r$ non-zero singular values. The SVD reveals the bases for the fundamental subspaces as follows:\n- The first $r$ columns of $V$, $\\{v_1, \\dots, v_r\\}$, form an orthonormal basis for the range of $G^\\top$, $\\mathcal{R}(G^\\top)$.\n- The remaining $n-r$ columns of $V$, $\\{v_{r+1}, \\dots, v_n\\}$, form an orthonormal basis for the null space of $G$, $\\mathcal{N}(G)$. This is because for $i > r$, $\\sigma_i = 0$, which implies $G v_i = U \\Sigma V^\\top v_i = U \\Sigma e_i = \\sigma_i u_i = 0$.\n\nIn numerical computations, due to floating-point errors, singular values that should be zero may appear as very small non-zero numbers. Therefore, a numerical rank must be defined using a threshold. The problem specifies a threshold $t = \\max(m, n) \\, \\epsilon \\, \\sigma_{\\max}$, where $\\epsilon$ is the machine epsilon for double precision and $\\sigma_{\\max}$ is the largest singular value of $G$. The numerical rank, which we will also denote by $r$, is the number of singular values $\\sigma_i$ that are strictly greater than $t$.\n\nThe procedure for each test case is as follows:\n1.  Given a matrix $G \\in \\mathbb{R}^{m \\times n}$, compute its SVD to obtain the singular values $s$ and the matrix of right singular vectors $V$.\n2.  Determine the maximum singular value $\\sigma_{\\max} = s_1$.\n3.  Calculate the threshold $t = \\max(m, n) \\cdot \\epsilon \\cdot \\sigma_{\\max}$.\n4.  Compute the numerical rank $r$ as the number of singular values $s_i > t$.\n5.  The basis for $\\mathcal{R}(G^\\top)$ is composed of the first $r$ columns of $V$.\n6.  The basis for $\\mathcal{N}(G)$ is composed of the last $n-r$ columns of $V$. The dimension of the null space is therefore $\\dim \\mathcal{N}(G) = n-r$.\n7.  Verify the rank-nullity theorem by checking if the equality $n = r + (n-r)$ holds. By our construction, this will always evaluate to `True`.\n8.  Numerically verify the orthogonality between $\\mathcal{R}(G^\\top)$ and $\\mathcal{N}(G)$. This is done by forming a matrix of inner products between the basis vectors of the two subspaces. Let $V_{\\mathcal{R}}$ be the matrix whose columns are the basis for $\\mathcal{R}(G^\\top)$ and $V_{\\mathcal{N}}$ be the matrix for $\\mathcal{N}(G)$. The matrix of inner products is $C = V_{\\mathcal{R}}^\\top V_{\\mathcal{N}}$. Because the columns of $V$ are orthonormal, all entries of $C$ should theoretically be zero. We compute the maximum absolute value of the entries in $C$, $\\max(|C_{ij}|)$, as a measure of the numerical orthogonality. A value close to zero indicates high numerical orthogonality. If one of the subspaces is trivial (i.e., its dimension is $0$), the orthogonality holds vacuously, and the maximum inner product is defined as $0$.\n\nThis procedure is applied to each of the four provided test matrices, and the results are compiled.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes properties of fundamental subspaces for a suite of test matrices.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    A1 = np.array([\n        [1, 0, 2, -1],\n        [0, 1, 1, 2],\n        [1, 1, 0, 0],\n        [2, -1, 1, 1],\n        [0, 0, 1, -1]\n    ], dtype=float)\n\n    B1 = np.array([\n        [1, 2, 0, -1, 0, 3, 1],\n        [0, 1, 1, 0, 2, 0, -2],\n        [2, 0, -1, 1, 1, 1, 0],\n        [-1, 1, 2, 0, -1, 0, 1]\n    ], dtype=float)\n    G1 = A1 @ B1\n\n    G2 = np.diag([3.0, 2.0, 1.0, 4.0, 5.0, 6.0])\n\n    A2 = np.array([\n        [1, 2],\n        [-1, 0],\n        [0, 1]\n    ], dtype=float)\n\n    B2 = np.array([\n        [2, -1, 0, 1, 3],\n        [1, 0, 2, -2, -1]\n    ], dtype=float)\n    G3 = A2 @ B2\n\n    G4 = np.diag([1.0, 1e-8, 1e-17, 0.0])\n\n    test_cases = [G1, G2, G3, G4]\n    \n    results = []\n    \n    for G in test_cases:\n        m, n = G.shape\n\n        if n == 0:\n            # Handle matrices with no columns, null space dim is 0, rank is 0.\n            results.append([0, 0, 0, True, 0.0])\n            continue\n        \n        # Compute SVD\n        try:\n            U, s, Vh = np.linalg.svd(G)\n        except np.linalg.LinAlgError:\n            # Handle potential SVD failures, although unlikely for these cases\n            # This path is not expected to be taken with the given inputs.\n            continue\n\n        # Get matrix of right singular vectors\n        V = Vh.T\n\n        # Machine epsilon for the data type of G\n        eps = np.finfo(G.dtype).eps\n        \n        # Determine the numerical rank\n        sigma_max = s[0] if s.size > 0 else 0\n        \n        # If the matrix is the zero matrix, rank is 0. Threshold would be 0.\n        if sigma_max == 0:\n            rank = 0\n        else:\n            threshold = max(m, n) * eps * sigma_max\n            rank = np.sum(s > threshold)\n            \n        dim_null_space = n - rank\n        \n        # Verify Rank-Nullity Theorem: n = rank(G) + dim(N(G))\n        # By construction, this will always be true.\n        rank_nullity_valid = (n == rank + dim_null_space)\n        \n        # Check orthogonality of subspaces\n        # Basis for R(G^T) are the first 'rank' columns of V\n        # Basis for N(G) are the last 'n - rank' columns of V\n        if rank == 0 or rank == n:\n            # If one subspace is trivial, orthogonality holds vacuously.\n            orthogonality_check = 0.0\n        else:\n            V_range = V[:, :rank]\n            V_null = V[:, rank:]\n            inner_products = V_range.T @ V_null\n            orthogonality_check = np.max(np.abs(inner_products))\n        \n        results.append([rank, dim_null_space, n, rank_nullity_valid, orthogonality_check])\n\n    # Final print statement in the exact required format.\n    # The default string representation of a list of lists matches the required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3608192"}, {"introduction": "Most geophysical inverse problems are ill-posed, requiring regularization to find a stable and meaningful solution. This capstone exercise simulates a complete workflow for 2D travel-time tomography, a classic application where you will implement Tikhonov regularization to overcome ill-posedness. By conducting a checkerboard resolution test, you will learn how to practically assess the performance of an inversion setup and understand how regularization parameters affect the ability to recover subsurface structures [@problem_id:3608136].", "problem": "You are tasked with constructing and evaluating a classical checkerboard resolution test for a two-dimensional ($2$-D) linear inverse problem representative of travel-time tomography in computational geophysics. The forward model is linear, the data are line integrals along straight rays, and the inverse problem is solved with zeroth- and second-order Tikhonov regularization. Your objective is to implement a complete program that, for several parameter sets, builds the model, simulates data through a forward operator, solves the inverse problem for specified regularization strengths, and quantitatively compares the recovered model to the true checkerboard to assess resolution.\n\nStart from the following fundamental base: the linear forward relation $d = G m + \\epsilon$ between data $d$, forward operator $G$, model $m$, and noise $\\epsilon$; and the least-squares principle, which selects $m$ by minimizing a quadratic misfit. You must use Tikhonov regularization, which augments the misfit with a stabilizer term of the form $\\lambda^{2} \\lVert L m \\rVert_{2}^{2}$, where $\\lambda \\ge 0$ is a scalar trade-off and $L$ is a discrete operator that encodes prior expectations (use the $5$-point discrete Laplacian on the grid for second-order smoothness). Do not assume any specific formula for the solution; instead, systematically construct the discrete operators and derive the linear system to be solved by setting the derivative of the quadratic objective to zero.\n\nDetailed requirements:\n\n- Model grid and checkerboard:\n  - Use a rectangular grid with $n_{x} \\times n_{y}$ cells indexed in row-major order into a vector $m \\in \\mathbb{R}^{n_{x} n_{y}}$.\n  - Construct a zero-mean checkerboard model $m_{\\mathrm{true}}$ of amplitude $1$ with square tiles of side length $s$ cells. That is, alternating blocks of $+1$ and $-1$ with block size $s \\times s$. Ensure the model is mean-centered when computing comparison metrics.\n- Forward operator $G$ (ray-path sums):\n  - Simulate straight, axis-aligned rays corresponding to line integrals across selected rows and columns. Each horizontal ray corresponds to summing the model values along one entire row; each vertical ray sums along one entire column. Use a stride (step) parameter $q \\in \\mathbb{N}$ to include every $q$-th row and every $q$-th column, starting from the first row and first column. For a horizontal ray at row index $i$, the corresponding row of $G$ should have entries $G_{k,j} = 1$ for all cells $j$ in row $i$ and $G_{k,j} = 0$ elsewhere. Similarly for vertical rays at column index $j$.\n  - Form data $d = G m_{\\mathrm{true}} + \\epsilon$, where $\\epsilon$ is zero-mean Gaussian noise with standard deviation $\\sigma$. Use a fixed random seed per test case to ensure reproducibility.\n- Inverse problem:\n  - Solve the Tikhonov-regularized least-squares problem that minimizes the objective $\\Phi(m) = \\lVert G m - d \\rVert_{2}^{2} + \\lambda^{2} \\lVert L m \\rVert_{2}^{2}$, where $L$ is the $5$-point discrete Laplacian on the $n_{x} \\times n_{y}$ grid with natural boundary treatment (fewer neighbors on edges). For $\\lambda = 0$, solve the unregularized least-squares problem. For $\\lambda > 0$, set the gradient to zero and solve the resulting symmetric positive-definite linear system.\n- Resolution evaluation metric:\n  - Compute the centered cosine similarity (correlation) between the recovered model $m_{\\mathrm{est}}$ and the true model $m_{\\mathrm{true}}$: $r = \\dfrac{(m_{\\mathrm{est}} - \\overline{m}_{\\mathrm{est}})^{\\mathsf{T}} (m_{\\mathrm{true}} - \\overline{m}_{\\mathrm{true}})}{\\lVert m_{\\mathrm{est}} - \\overline{m}_{\\mathrm{est}} \\rVert_{2} \\, \\lVert m_{\\mathrm{true}} - \\overline{m}_{\\mathrm{true}} \\rVert_{2}}$, where bars denote means. This yields a scalar in $[-1,1]$ indicating pattern recovery quality.\n- Units: All quantities are dimensionless for this exercise; no physical units are required.\n- Angle units: Not applicable.\n- Numerical output: For each test case, output the scalar correlation $r$ as a float rounded to three decimal places.\n\nTest suite (provide results for each of the following parameter sets):\n- Case $1$: $(n_{x}, n_{y}, s, \\lambda, \\sigma, q) = (16, 16, 4, 0.5, 0.0, 2)$.\n- Case $2$: $(n_{x}, n_{y}, s, \\lambda, \\sigma, q) = (16, 16, 4, 0.0, 0.0, 2)$.\n- Case $3$: $(n_{x}, n_{y}, s, \\lambda, \\sigma, q) = (16, 16, 4, 5.0, 0.0, 2)$.\n- Case $4$: $(n_{x}, n_{y}, s, \\lambda, \\sigma, q) = (16, 16, 2, 0.5, 0.0, 2)$.\n- Case $5$: $(n_{x}, n_{y}, s, \\lambda, \\sigma, q) = (16, 16, 4, 0.5, 0.05, 2)$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[r_{1},r_{2},r_{3},r_{4},r_{5}]$), where each $r_{i}$ is rounded to three decimal places.\n- The program must be fully self-contained, take no input, and use only the specified Python runtime and libraries.", "solution": "The problem requires the construction and evaluation of a checkerboard resolution test for a two-dimensional ($2$-D) linear inverse problem, solved using Tikhonov regularization. This involves several stages: defining the model space and a true model, constructing the forward operator, simulating data, setting up and solving the regularized inverse problem, and quantifying the quality of the model recovery.\n\nThe fundamental relationship governing the forward problem is the linear equation:\n$$d = G m + \\epsilon$$\nwhere $d \\in \\mathbb{R}^{N_d}$ is the data vector, $m \\in \\mathbb{R}^{N_m}$ is the model parameter vector, $G \\in \\mathbb{R}^{N_d \\times N_m}$ is the forward operator, and $\\epsilon \\in \\mathbb{R}^{N_d}$ is a vector of random noise.\n\nFirst, we define the model space. The model consists of a $2$-D rectangular grid of $n_x \\times n_y$ cells. The physical property within each cell is assumed to be constant. These $N_m = n_x n_y$ cell values are flattened into a single vector $m$ using row-major indexing. A cell at grid coordinates $(i, j)$ (where $i$ is the row index, $0 \\le i < n_y$, and $j$ is the column index, $0 \\le j < n_x$) corresponds to the index $k = i \\cdot n_x + j$ in the vector $m$.\n\nThe true model, $m_{\\mathrm{true}}$, is a checkerboard pattern. For a given tile side length $s$, the value of the cell at $(i, j)$ is determined by its tile coordinates $(\\lfloor i/s \\rfloor, \\lfloor j/s \\rfloor)$. The value is set to $(-1)^{p+q}$ where $p = \\lfloor i/s \\rfloor$ and $q = \\lfloor j/s \\rfloor$, resulting in alternating blocks of $+1$ and $-1$. For the specified test parameters, this construction yields a zero-mean model.\n\nNext, we construct the forward operator $G$. The data acquisition is modeled as straight, axis-aligned ray-path tomography. Each datum corresponds to the line integral (sum) of model parameters along a specific row or column of the grid. Rays are sampled with a stride $q$, meaning every $q$-th row and every $q$-th column are surveyed, starting from the first. A horizontal ray passing through row $i$ corresponds to a row in $G$ with entries of $1$ for all model parameters in that grid row and $0$ otherwise. Similarly, a vertical ray through column $j$ corresponds to a row in $G$ with entries of $1$ for all model parameters in that grid column. The total number of data points, $N_d$, is the sum of the number of sampled rows and columns.\n\nThe synthetic data vector $d$ is generated by applying the forward operator to the true model, $d_{\\mathrm{clean}} = G m_{\\mathrm{true}}$, and adding zero-mean Gaussian noise $\\epsilon$ with a specified standard deviation $\\sigma$. Thus, $d = G m_{\\mathrm{true}} + \\epsilon$.\n\nThe core of the task is to solve the inverse problem: given $d$ and $G$, estimate the model $m$. We use Tikhonov regularization to find a stable and meaningful solution, particularly because the problem is underdetermined ($N_d \\ll N_m$). This is achieved by minimizing the objective function $\\Phi(m)$, which balances data fidelity with a desired model property (e.g., smoothness):\n$$\\Phi(m) = \\lVert G m - d \\rVert_{2}^{2} + \\lambda^{2} \\lVert L m \\rVert_{2}^{2}$$\nThe first term, $\\lVert G m - d \\rVert_{2}^{2}$, is the data misfit, measuring how well the model prediction $Gm$ fits the observed data $d$. The second term, $\\lambda^{2} \\lVert L m \\rVert_{2}^{2}$, is the regularization term or stabilizer. The parameter $\\lambda \\ge 0$ controls the trade-off. The operator $L$ penalizes models that deviate from prior expectations. Here, $L$ is the $5$-point discrete Laplacian operator, which enforces smoothness by penalizing large second spatial derivatives.\n\nFor a model parameter $m_k$ corresponding to grid cell $(i,j)$, the action of the Laplacian is $(Lm)_k = \\sum_{p \\in N(k)} m_p - (\\deg(k)) m_k$, where $N(k)$ is the set of indices of adjacent cells on the grid and $\\deg(k)$ is the number of such neighbors (degree). For an interior cell, $\\deg(k)=4$. For cells on boundaries and corners, this number is $3$ or $2$, respectively. This is known as a natural boundary condition. The matrix $L$ is constructed such that each row $k$ encodes this finite-difference stencil.\n\nTo find the model $m_{\\mathrm{est}}$ that minimizes $\\Phi(m)$, we compute the gradient of $\\Phi(m)$ with respect to $m$ and set it to zero:\n$$\\nabla_m \\Phi(m) = \\frac{\\partial}{\\partial m} \\left( (Gm-d)^{\\mathsf{T}}(Gm-d) + \\lambda^2 (Lm)^{\\mathsf{T}}(Lm) \\right) = 0$$\n$$\\nabla_m \\Phi(m) = 2G^{\\mathsf{T}}(Gm-d) + 2\\lambda^2 L^{\\mathsf{T}}Lm = 0$$\nRearranging terms yields the normal equations, a system of linear equations for $m$:\n$$(G^{\\mathsf{T}}G + \\lambda^2 L^{\\mathsf{T}}L)m = G^{\\mathsf{T}}d$$\nThis system can be solved for $m$. However, forming the products $G^{\\mathsf{T}}G$ and $L^{\\mathsf{T}}L$ can increase the condition number and lead to numerical instability. A more robust method is to solve an equivalent augmented least-squares problem. Minimizing $\\Phi(m)$ is identical to solving the following system in a least-squares sense:\n$$ \\begin{pmatrix} G \\\\ \\lambda L \\end{pmatrix} m = \\begin{pmatrix} d \\\\ 0 \\end{pmatrix} $$\nwhere the vector $0$ has dimension equal to the number of rows in $L$. This formulation is numerically superior and handles the unregularized case ($\\lambda=0$) seamlessly. The solution $m_{\\mathrm{est}}$ is found using a standard linear least-squares solver.\n\nFinally, to evaluate the resolution of the recovery, we compute the centered cosine similarity (correlation coefficient) $r$ between the estimated model $m_{\\mathrm{est}}$ and the true model $m_{\\mathrm{true}}$:\n$$r = \\frac{(m_{\\mathrm{est}} - \\overline{m}_{\\mathrm{est}})^{\\mathsf{T}} (m_{\\mathrm{true}} - \\overline{m}_{\\mathrm{true}})}{\\lVert m_{\\mathrm{est}} - \\overline{m}_{\\mathrm{est}} \\rVert_{2} \\, \\lVert m_{\\mathrm{true}} - \\overline{m}_{\\mathrm{true}} \\rVert_{2}}$$\nwhere $\\overline{m}$ denotes the mean of vector $m$. A value of $r$ near $1$ indicates excellent pattern recovery, while a value near $0$ or $-1$ indicates poor or anti-correlated recovery. This metric effectively quantifies the success of the tomographic inversion under the given parameters. The final algorithm proceeds by systematically applying these steps for each specified test case.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef build_checkerboard(nx, ny, s):\n    \"\"\"\n    Constructs a 2D checkerboard model and flattens it to a 1D vector.\n    \n    Args:\n        nx (int): Number of cells in the x-dimension.\n        ny (int): Number of cells in the y-dimension.\n        s (int): Side length of the checkerboard tiles in cells.\n    \n    Returns:\n        np.ndarray: A 1D numpy array representing the true model m_true.\n    \"\"\"\n    x_coords = np.arange(nx)\n    y_coords = np.arange(ny)\n    xx, yy = np.meshgrid(x_coords, y_coords)\n    \n    # Determine tile indices for each cell\n    tile_indices_x = np.floor(xx / s)\n    tile_indices_y = np.floor(yy / s)\n    \n    # Create checkerboard pattern (+1 and -1)\n    checkerboard_2d = (-1)**(tile_indices_y + tile_indices_x)\n    \n    # Flatten to 1D vector in row-major order\n    return checkerboard_2d.ravel()\n\ndef build_forward_operator(nx, ny, q):\n    \"\"\"\n    Constructs the forward operator G for axis-aligned ray tomography.\n    \n    Args:\n        nx (int): Number of cells in the x-dimension.\n        ny (int): Number of cells in the y-dimension.\n        q (int): Stride for sampling rows and columns.\n    \n    Returns:\n        np.ndarray: The forward operator matrix G.\n    \"\"\"\n    num_model_params = nx * ny\n    \n    # Identify which rows and columns to sample\n    rows_to_sample = np.arange(0, ny, q)\n    cols_to_sample = np.arange(0, nx, q)\n    num_data = len(rows_to_sample) + len(cols_to_sample)\n    \n    G = np.zeros((num_data, num_model_params))\n    \n    data_idx = 0\n    # Fill rows for horizontal rays\n    for i_row in rows_to_sample:\n        start_idx = i_row * nx\n        end_idx = start_idx + nx\n        G[data_idx, start_idx:end_idx] = 1.0\n        data_idx += 1\n        \n    # Fill rows for vertical rays\n    for j_col in cols_to_sample:\n        indices = j_col + np.arange(ny) * nx\n        G[data_idx, indices] = 1.0\n        data_idx += 1\n        \n    return G\n\ndef build_laplacian(nx, ny):\n    \"\"\"\n    Constructs the 5-point discrete Laplacian operator L with natural boundary conditions.\n    \n    Args:\n        nx (int): Number of cells in the x-dimension.\n        ny (int): Number of cells in the y-dimension.\n    \n    Returns:\n        np.ndarray: The discrete Laplacian matrix L.\n    \"\"\"\n    num_model_params = nx * ny\n    L = np.zeros((num_model_params, num_model_params))\n    \n    for k in range(num_model_params):\n        i = k // nx  # Row index\n        j = k % nx   # Column index\n        \n        neighbors = []\n        # Neighbor above\n        if i > 0: neighbors.append(k - nx)\n        # Neighbor below\n        if i  ny - 1: neighbors.append(k + nx)\n        # Neighbor left\n        if j > 0: neighbors.append(k - 1)\n        # Neighbor right\n        if j  nx - 1: neighbors.append(k + 1)\n            \n        # Set diagonal to - (number of neighbors)\n        L[k, k] = -len(neighbors)\n        # Set off-diagonals to 1 for each neighbor\n        for neighbor_idx in neighbors:\n            L[k, neighbor_idx] = 1.0\n            \n    return L\n\ndef compute_correlation(m_est, m_true):\n    \"\"\"\n    Computes the centered cosine similarity between two model vectors.\n    \n    Args:\n        m_est (np.ndarray): The estimated model vector.\n        m_true (np.ndarray): The true model vector.\n    \n    Returns:\n        float: The correlation coefficient r.\n    \"\"\"\n    # Center the vectors by subtracting their means\n    m_est_centered = m_est - np.mean(m_est)\n    m_true_centered = m_true - np.mean(m_true)\n    \n    # Compute norms of the centered vectors\n    norm_est = np.linalg.norm(m_est_centered)\n    norm_true = np.linalg.norm(m_true_centered)\n    \n    # Avoid division by zero if a vector is constant\n    if norm_est == 0 or norm_true == 0:\n        return 0.0\n        \n    # Compute the dot product and normalize\n    numerator = np.dot(m_est_centered, m_true_centered)\n    denominator = norm_est * norm_true\n    \n    return numerator / denominator\n\ndef run_single_case(params, seed):\n    \"\"\"\n    Runs a single checkerboard test case.\n    \"\"\"\n    nx, ny, s, lam, sig, q = params\n    \n    # 1. Construct true model, forward operator, and Laplacian\n    m_true = build_checkerboard(nx, ny, s)\n    G = build_forward_operator(nx, ny, q)\n    L = build_laplacian(nx, ny)\n    \n    # 2. Generate synthetic data\n    d_clean = G @ m_true\n    if sig > 0:\n        rng = np.random.default_rng(seed=seed)\n        noise = rng.normal(loc=0.0, scale=sig, size=d_clean.shape)\n        d = d_clean + noise\n    else:\n        d = d_clean\n    \n    # 3. Solve the regularized inverse problem\n    num_model_params = nx * ny\n    if lam > 0:\n        # Augmented system for Tikhonov regularization\n        A_aug = np.vstack([G, lam * L])\n        d_aug = np.concatenate([d, np.zeros(num_model_params)])\n        m_est = np.linalg.lstsq(A_aug, d_aug, rcond=None)[0]\n    else:\n        # Unregularized least-squares (minimum norm solution)\n        m_est = np.linalg.lstsq(G, d, rcond=None)[0]\n\n    # 4. Evaluate resolution\n    correlation = compute_correlation(m_est, m_true)\n    \n    return round(correlation, 3)\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    test_cases = [\n        (16, 16, 4, 0.5, 0.0, 2),  # Case 1\n        (16, 16, 4, 0.0, 0.0, 2),  # Case 2\n        (16, 16, 4, 5.0, 0.0, 2),  # Case 3\n        (16, 16, 2, 0.5, 0.0, 2),  # Case 4\n        (16, 16, 4, 0.5, 0.05, 2)  # Case 5\n    ]\n    \n    results = []\n    for i, case_params in enumerate(test_cases):\n        # Use a different seed for each case for reproducibility\n        result = run_single_case(case_params, seed=i)\n        results.append(result)\n\n    # Format output as a comma-separated list in brackets\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3608136"}]}