{"hands_on_practices": [{"introduction": "This first practice guides you through the foundational steps of setting up a regularized inverse problem. You will construct a discrete first-derivative operator from scratch, a fundamental tool for enforcing smoothness, and compare its effect against a simple zeroth-order (identity) damping strategy. This exercise [@problem_id:3617426] is crucial for building intuition about how different regularization operators penalize distinct model features, moving from penalizing model amplitude to penalizing model roughness.", "problem": "You are given a linear inverse problem from computational geophysics. Let $n=5$ denote the number of model parameters defined on a uniform grid with spacing $\\Delta x = 1$. You are to consider linear data predicted by a known forward operator $G \\in \\mathbb{R}^{5 \\times 5}$ applied to a model vector $m \\in \\mathbb{R}^{5}$, with observed data $d \\in \\mathbb{R}^{5}$. The goal is to stabilize the inversion using Tikhonov regularization with two damping strategies: a first-derivative roughness penalty and a zeroth-order identity damping penalty. You must proceed from the definition of the Tikhonov objective and implement the minimizer accordingly.\n\nFundamental base:\n- The Tikhonov objective functional for a linear inverse problem is defined for $\\alpha \\ge 0$ and a chosen regularization operator $L$ by\n$$\n\\Phi(m) = \\| G m - d \\|_2^2 + \\alpha^2 \\| L m \\|_2^2,\n$$\nwhere $\\|\\cdot\\|_2$ denotes the Euclidean norm.\n- The discrete first-derivative operator $L \\in \\mathbb{R}^{(n-1) \\times n}$ on a uniform grid with spacing $\\Delta x$ under forward-difference discretization is given by\n$$\nL_{i,i} = -\\frac{1}{\\Delta x}, \\quad L_{i,i+1} = \\frac{1}{\\Delta x}, \\quad \\text{for } i=1,\\dots,n-1,\n$$\nwith all other entries equal to zero. For $\\Delta x = 1$ and $n=5$, this yields a $(4 \\times 5)$ matrix. The matrix $L^{\\top} L \\in \\mathbb{R}^{n \\times n}$ is then symmetric positive semidefinite and banded, with a tridiagonal structure reflecting the discrete gradient penalty. For zeroth-order (identity) damping, use $L = I \\in \\mathbb{R}^{n \\times n}$, the identity matrix.\n\nGiven data:\n- Let\n$$\nG = \\begin{bmatrix}\n1.0 & 0.9 & 0.0 & 0.0 & 0.0 \\\\\n0.9 & 1.0 & 0.9 & 0.0 & 0.0 \\\\\n0.0 & 0.9 & 1.0 & 0.9 & 0.0 \\\\\n0.0 & 0.0 & 0.9 & 1.0 & 0.9 \\\\\n0.0 & 0.0 & 0.0 & 0.9 & 1.0\n\\end{bmatrix}, \\quad\nd = \\begin{bmatrix}\n2.9 \\\\ 5.5 \\\\ 6.65 \\\\ 5.6 \\\\ 2.75\n\\end{bmatrix}.\n$$\n\nTasks:\n1. Discretize the first-derivative operator $L \\in \\mathbb{R}^{4 \\times 5}$ using forward differences with $\\Delta x = 1$ as specified above, and assemble $L^{\\top} L \\in \\mathbb{R}^{5 \\times 5}$.\n2. For each test case below, minimize the Tikhonov objective $\\Phi(m)$ over $m \\in \\mathbb{R}^{5}$ using the given regularization strategy. Use the first principles stated above to derive the necessary stationarity conditions and produce the numerical solution $m_{\\alpha}$.\n\nTest suite:\n- Case 1: First-derivative regularization with $\\alpha = 0$. Here, $L$ is the $(4 \\times 5)$ first-derivative operator. This case reduces to the undamped least-squares fit while still using the same $L$ in the formalism. Report the solution $m_{\\alpha}$.\n- Case 2: First-derivative regularization with $\\alpha = 0.5$. Use the same $L$ as above. Report the solution $m_{\\alpha}$.\n- Case 3: First-derivative regularization with $\\alpha = 10.0$. Use the same $L$ as above. Report the solution $m_{\\alpha}$.\n- Case 4: Zeroth-order damping with $\\alpha = 0.5$. Here, set $L = I \\in \\mathbb{R}^{5 \\times 5}$. Report the solution $m_{\\alpha}$.\n\nFinal output format:\n- Your program should produce a single line of output containing the concatenated model vectors for Cases 1 through 4 in this order. Each model vector consists of $5$ floating-point entries. Round every entry to exactly six decimal places. The overall output must be a single list of $20$ numbers in a single pair of square brackets with comma separators and no spaces. For example, the syntactic form must be\n$$\n[\\text{m1\\_1},\\text{m1\\_2},\\text{m1\\_3},\\text{m1\\_4},\\text{m1\\_5},\\text{m2\\_1},\\dots,\\text{m4\\_5}],\n$$\nwhere $\\text{mK\\_j}$ denotes the $j$-th entry of the solution for Case $K$. There are no physical units to report in this problem.", "solution": "The problem R-1 is valid. It presents a standard, well-posed linear inverse problem from computational geophysics, providing all necessary data and definitions for a unique solution. The formulation is scientifically and mathematically sound, and the objectives are clear.\n\nThe solution to the Tikhonov-regularized linear inverse problem is found by minimizing the objective functional $\\Phi(m)$ with respect to the model parameters $m \\in \\mathbb{R}^5$. The objective functional is given by\n$$\n\\Phi(m) = \\| G m - d \\|_2^2 + \\alpha^2 \\| L m \\|_2^2\n$$\nwhere $G \\in \\mathbb{R}^{5 \\times 5}$ is the forward operator, $d \\in \\mathbb{R}^5$ is the observed data vector, $L$ is the regularization operator, and $\\alpha \\ge 0$ is the regularization parameter.\n\nTo find the minimum, we compute the gradient of $\\Phi(m)$ and set it to the zero vector. The functional, expressed in matrix notation, is:\n$$\n\\Phi(m) = (G m - d)^{\\top}(G m - d) + \\alpha^2 (L m)^{\\top}(L m)\n$$\nExpanding this expression gives:\n$$\n\\Phi(m) = m^{\\top}G^{\\top}G m - 2m^{\\top}G^{\\top}d + d^{\\top}d + \\alpha^2 m^{\\top}L^{\\top}L m\n$$\nThe gradient with respect to $m$ is:\n$$\n\\nabla_m \\Phi(m) = 2 G^{\\top}G m - 2 G^{\\top}d + 2 \\alpha^2 L^{\\top}L m\n$$\nSetting $\\nabla_m \\Phi(m) = 0$ yields the normal equations for the regularized problem:\n$$\n(G^{\\top}G + \\alpha^2 L^{\\top}L) m = G^{\\top}d\n$$\nThe solution, denoted $m_\\alpha$, is obtained by solving this system of linear equations:\n$$\nm_\\alpha = (G^{\\top}G + \\alpha^2 L^{\\top}L)^{-1} G^{\\top}d\n$$\nWe will now apply this general solution to the specific cases provided. The given forward operator $G$ and data vector $d$ are:\n$$\nG = \\begin{bmatrix}\n1.0 & 0.9 & 0.0 & 0.0 & 0.0 \\\\\n0.9 & 1.0 & 0.9 & 0.0 & 0.0 \\\\\n0.0 & 0.9 & 1.0 & 0.9 & 0.0 \\\\\n0.0 & 0.0 & 0.9 & 1.0 & 0.9 \\\\\n0.0 & 0.0 & 0.0 & 0.9 & 1.0\n\\end{bmatrix}, \\quad\nd = \\begin{bmatrix}\n2.9 \\\\ 5.5 \\\\ 6.65 \\\\ 5.6 \\\\ 2.75\n\\end{bmatrix}\n$$\nThe matrix $G$ is symmetric, so $G^{\\top} = G$. We pre-compute the terms $G^{\\top}G$ and $G^{\\top}d$:\n$$\nG^{\\top}G = G^2 = \\begin{bmatrix}\n1.81 & 1.80 & 0.81 & 0.00 & 0.00 \\\\\n1.80 & 2.62 & 1.80 & 0.81 & 0.00 \\\\\n0.81 & 1.80 & 2.62 & 1.80 & 0.81 \\\\\n0.00 & 0.81 & 1.80 & 2.62 & 1.80 \\\\\n0.00 & 0.00 & 0.81 & 1.80 & 1.81\n\\end{bmatrix}, \\quad\nG^{\\top}d = Gd = \\begin{bmatrix}\n7.85 \\\\\n14.095 \\\\\n16.64 \\\\\n14.06 \\\\\n7.79\n\\end{bmatrix}\n$$\n\n**Task 1: First-Derivative Operator**\nFor $n=5$ and $\\Delta x = 1$, the first-derivative operator $L \\in \\mathbb{R}^{4 \\times 5}$ using forward differences is:\n$$\nL = \\begin{bmatrix}\n-1 & 1 & 0 & 0 & 0 \\\\\n 0 & -1 & 1 & 0 & 0 \\\\\n 0 & 0 & -1 & 1 & 0 \\\\\n 0 & 0 & 0 & -1 & 1\n\\end{bmatrix}\n$$\nThe corresponding roughness matrix $L^{\\top}L \\in \\mathbb{R}^{5 \\times 5}$ is:\n$$\nL^{\\top}L = \\begin{bmatrix}\n 1 & -1 & 0 & 0 & 0 \\\\\n-1 & 2 & -1 & 0 & 0 \\\\\n 0 & -1 & 2 & -1 & 0 \\\\\n 0 & 0 & -1 & 2 & -1 \\\\\n 0 & 0 & 0 & -1 & 1\n\\end{bmatrix}\n$$\n\n**Task 2: Solving for Test Cases**\n\n**Case 1: First-derivative regularization with $\\alpha = 0$**\nFor $\\alpha=0$, the regularization term vanishes, and the normal equations simplify to the standard least-squares problem: $G^{\\top}G m_0 = G^{\\top}d$. Since $G$ is invertible in this problem, this is equivalent to $m_0 = G^{-1}d$.\nThe system $(G^{\\top}G) m_0 = G^{\\top}d$ is solved to find the model vector:\n$$\nm_0 \\approx \\begin{bmatrix} -2.427845 & 5.919828 & 1.122423 & 4.975330 & -1.727845 \\end{bmatrix}^{\\top}\n$$\n\n**Case 2: First-derivative regularization with $\\alpha = 0.5$**\nHere, $\\alpha^2 = 0.25$. We solve $(G^{\\top}G + 0.25 L^{\\top}L) m_{0.5} = G^{\\top}d$. The matrix $(G^{\\top}G + 0.25 L^{\\top}L)$ is constructed and the system is solved, yielding:\n$$\nm_{0.5} \\approx \\begin{bmatrix} 1.261882 & 2.955779 & 3.791558 & 3.012587 & 1.332373 \\end{bmatrix}^{\\top}\n$$\nThis solution is noticeably smoother than the unregularized result, demonstrating the effect of the first-derivative penalty.\n\n**Case 3: First-derivative regularization with $\\alpha = 10.0$**\nHere, $\\alpha^2 = 100.0$. The large value of $\\alpha$ heavily penalizes model roughness. We solve $(G^{\\top}G + 100 L^{\\top}L) m_{10} = G^{\\top}d$. The solution is:\n$$\nm_{10} \\approx \\begin{bmatrix} 2.653457 & 2.653634 & 2.653665 & 2.653634 & 2.653457 \\end{bmatrix}^{\\top}\n$$\nAs expected, the resulting model is almost constant, which is the smoothest possible configuration.\n\n**Case 4: Zeroth-order damping with $\\alpha = 0.5$**\nFor this case, the regularization operator is the identity matrix, $L = I \\in \\mathbb{R}^{5 \\times 5}$, so $L^{\\top}L=I$. With $\\alpha^2 = 0.25$, we solve $(G^{\\top}G + 0.25 I) m_{0.5} = G^{\\top}d$. This form of regularization, also known as damped least squares, penalizes the Euclidean norm of the model vector. The solution is:\n$$\nm_{0.5} \\approx \\begin{bmatrix} 0.852432 & 2.378930 & 3.003920 & 2.345869 & 0.941620 \\end{bmatrix}^{\\top}\n$$\nThis solution has a smaller overall magnitude compared to the unregularized solution, consistent with the nature of zeroth-order damping.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves a Tikhonov-regularized linear inverse problem for four test cases\n    and formats the output as specified.\n    \"\"\"\n    \n    # Define givens from the problem statement.\n    G = np.array([\n        [1.0, 0.9, 0.0, 0.0, 0.0],\n        [0.9, 1.0, 0.9, 0.0, 0.0],\n        [0.0, 0.9, 1.0, 0.9, 0.0],\n        [0.0, 0.0, 0.9, 1.0, 0.9],\n        [0.0, 0.0, 0.0, 0.9, 1.0]\n    ])\n    \n    d = np.array([2.9, 5.5, 6.65, 5.6, 2.75])\n    \n    n = 5  # Number of model parameters\n\n    # Define the test cases. Each tuple contains (alpha, regularization_type).\n    test_cases = [\n        (0.0, 'derivative'),   # Case 1\n        (0.5, 'derivative'),   # Case 2\n        (10.0, 'derivative'),  # Case 3\n        (0.5, 'identity')      # Case 4\n    ]\n\n    # Pre-compute components that are constant across cases\n    GtG = G.T @ G\n    Gtd = G.T @ d\n\n    # Construct the regularization operators\n    # First-derivative operator L and L^T L\n    L_deriv = np.zeros((n - 1, n))\n    for i in range(n - 1):\n        L_deriv[i, i] = -1.0\n        L_deriv[i, i+1] = 1.0\n    LtL_deriv = L_deriv.T @ L_deriv\n\n    # Zeroth-order (identity) operator L and L^T L\n    L_ident = np.identity(n)\n    LtL_ident = L_ident.T @ L_ident\n\n    all_results = []\n    \n    for alpha, reg_type in test_cases:\n        if reg_type == 'derivative':\n            LtL = LtL_deriv\n        elif reg_type == 'identity':\n            LtL = LtL_ident\n        else:\n            # This case should not be reached with the defined test_cases\n            raise ValueError(f\"Unknown regularization type: {reg_type}\")\n            \n        # Form the matrix for the linear system\n        # (G^T G + alpha^2 L^T L) m = G^T d\n        A = GtG + (alpha**2) * LtL\n        \n        # Solve the system for the model vector m\n        m_alpha = np.linalg.solve(A, Gtd)\n        \n        all_results.extend(m_alpha)\n\n    # Format the final output string\n    # Concatenated list of 20 numbers, each rounded to 6 decimal places,\n    # with comma separators and no spaces.\n    output_str = \",\".join([f\"{x:.6f}\" for x in all_results])\n    \n    # Final print statement in the exact required format.\n    print(f\"[{output_str}]\")\n\nsolve()\n```", "id": "3617426"}, {"introduction": "A key challenge in applying regularization is choosing the optimal damping parameter, $\\lambda$, which controls the trade-off between data misfit and solution simplicity. This practice introduces the L-curve method, a powerful and widely-used graphical tool for making this critical choice. You will implement a robust procedure [@problem_id:3617390] to find the \"corner\" of the L-curve, which represents an optimal balance, by using cubic splines to ensure the selection is stable and not an artifact of parameter sampling.", "problem": "Design and implement a complete numerical experiment that investigates the stability of curvature-based corner selection on the L-curve for Tikhonov regularization in a one-dimensional linear inverse problem. Your program must be fully deterministic and must not rely on any external input. It must execute the following tasks.\n\n1. Construct a synthetic linear inverse problem based on the following definitions.\n\n   - Discretize the interval $\\left[0,1\\right]$ into $n$ uniformly spaced points, with $n=80$. Let $x_i=\\left(i+\\tfrac{1}{2}\\right)/n$ for $i=0,1,\\dots,n-1$.\n   - Define the true model $m_{\\mathrm{true}}\\in\\mathbb{R}^n$ by\n     $$\n     m_{\\mathrm{true}}(x)=\\exp\\!\\left(-\\frac{\\left(x-0.30\\right)^2}{2\\cdot 0.04^2}\\right)+0.7\\,\\exp\\!\\left(-\\frac{\\left(x-0.75\\right)^2}{2\\cdot 0.07^2}\\right)+0.1\\,\\sin\\!\\left(6\\pi x\\right).\n     $$\n     Evaluate this expression at $\\left\\{x_i\\right\\}$ to obtain $m_{\\mathrm{true}}$.\n   - Let $A\\in\\mathbb{R}^{n\\times n}$ be a Gaussian blur operator defined by\n     $$\n     A_{ij}=C\\,\\exp\\!\\left(-\\frac{\\left((i-j)h\\right)^2}{2\\sigma_b^2}\\right),\n     $$\n     where $h=1/n$ and $\\sigma_b=0.03$. Choose $C$ such that each row of $A$ sums to $1$; that is, for each $i$, enforce $\\sum_{j=0}^{n-1}A_{ij}=1$.\n   - Define the first-difference roughening operator $L\\in\\mathbb{R}^{(n-1)\\times n}$ by\n     $$\n     \\left(Lm\\right)_k=m_{k+1}-m_k,\\quad k=0,1,\\dots,n-2.\n     $$\n   - Form noise-free data $d_{\\mathrm{clean}}=A m_{\\mathrm{true}}$. For a given nonnegative scalar $s$, define noisy data $d=d_{\\mathrm{clean}}+\\epsilon$, where $\\epsilon\\sim\\mathcal{N}\\!\\left(0,\\sigma^2\\mathbf{I}\\right)$ with $\\sigma=s\\,\\lVert d_{\\mathrm{clean}}\\rVert_2/\\sqrt{n}$. Use a fixed random seed for each experiment to ensure reproducibility.\n\n2. For a regularization parameter $\\lambda>0$, consider the Tikhonov-regularized least squares problem\n   $$\n   \\min_{m\\in\\mathbb{R}^n}\\ \\lVert A m-d\\rVert_2^2+\\lambda^2\\lVert Lm\\rVert_2^2.\n   $$\n   For each $\\lambda$ in a prescribed grid, compute the unique minimizer $m_\\lambda$ by solving the first-order optimality condition.\n\n3. Define the L-curve in the $\\log$-$\\log$ plane by the parametric curve $\\gamma:\\ t\\mapsto\\left(x(t),y(t)\\right)$, where $t=\\log\\lambda$, $x(t)=\\log\\left(\\lVert A m_\\lambda-d\\rVert_2\\right)$, and $y(t)=\\log\\left(\\lVert L m_\\lambda\\rVert_2\\right)$. The signed curvature $\\kappa(t)$ of a twice-differentiable planar curve $\\gamma(t)=\\left(x(t),y(t)\\right)$ is given by the well-tested formula\n   $$\n   \\kappa(t)=\\frac{\\left|x'(t)\\,y''(t)-y'(t)\\,x''(t)\\right|}{\\left(\\left(x'(t)\\right)^2+\\left(y'(t)\\right)^2\\right)^{3/2}}.\n   $$\n   Implement a numerically stable curvature estimator for the L-curve by computing $\\kappa(t)$.\n\n4. Robust corner selection criterion. To reduce sensitivity to the discretization of $\\lambda$, implement the following corner selection procedure.\n\n   - Let $t=\\log\\lambda$ be sampled on a coarse uniform grid $t_k=t_{\\min}+k\\,\\Delta t$ with $t_{\\min}=\\log\\lambda_{\\min}$, $t_{\\max}=\\log\\lambda_{\\max}$, and $\\Delta t>0$. Use the corresponding computed pairs $\\left(x_k,y_k\\right)$ at $t_k$.\n   - Interpolate $x(t)$ and $y(t)$ as cubic splines of $t$ using the coarse samples, and evaluate $x'(t)$, $x''(t)$, $y'(t)$, and $y''(t)$ on a fine uniform grid of size $N_f$ between $t_{\\min}$ and $t_{\\max}$, where $N_f=400$.\n   - Compute $\\kappa(t)$ on the fine grid and exclude an interior trimming of fraction $0.1$ of the interval at both ends to avoid endpoint artifacts. Select $t^\\star$ as the point maximizing $\\kappa(t)$ on the retained interior. The selected regularization parameter is then $\\lambda^\\star=\\exp\\!\\left(t^\\star\\right)$.\n\n5. Sensitivity study with respect to the step size in $\\lambda$. Fix the range $\\lambda\\in\\left[\\lambda_{\\min},\\lambda_{\\max}\\right]$ with $\\lambda_{\\min}=10^{-8}$ and $\\lambda_{\\max}=10^{2}$. Define $t_{\\min}=\\log\\lambda_{\\min}$ and $t_{\\max}=\\log\\lambda_{\\max}$. For each test case below, compute\n   - a baseline reference $\\lambda_{\\mathrm{base}}^\\star$ using the robust procedure above on a dense grid with step $\\Delta t_{\\mathrm{base}}=0.05$, and\n   - a coarse-grid estimate $\\lambda_{\\mathrm{rob}}^\\star$ using the same robust procedure but with the case-specific step $\\Delta t$.\n\n   Report the absolute log-difference\n   $$\n   e=\\left|\\log_{10}\\!\\left(\\lambda_{\\mathrm{rob}}^\\star\\right)-\\log_{10}\\!\\left(\\lambda_{\\mathrm{base}}^\\star\\right)\\right|.\n   $$\n\n6. Test suite. Use the following five test cases, each specified by the noise scale $s$ and the coarse step $\\Delta t$ in $t=\\log\\lambda$, together with the stated random seed for generating $\\epsilon$.\n\n   - Case A: $s=0.01$, $\\Delta t=2.00$, seed $=3$.\n   - Case B: $s=0.01$, $\\Delta t=1.00$, seed $=5$.\n   - Case C: $s=0.01$, $\\Delta t=0.50$, seed $=7$.\n   - Case D: $s=0.01$, $\\Delta t=0.25$, seed $=11$.\n   - Case E: $s=0.05$, $\\Delta t=1.00$, seed $=13$.\n\n   For all cases, use $\\lambda_{\\min}=10^{-8}$ and $\\lambda_{\\max}=10^{2}$.\n\n7. Final output format. Your program should produce a single line of output containing the five values of $e$ for Cases A–E, in order, as a comma-separated list enclosed in square brackets, with each value rounded to $6$ decimal places, and with no additional whitespace. For example, the format should look like $\\left[\\text{r}_1,\\text{r}_2,\\text{r}_3,\\text{r}_4,\\text{r}_5\\right]$ where each $\\text{r}_i$ is a decimal string rounded to $6$ places.", "solution": "The problem statement is a well-defined numerical experiment in computational geophysics, specifically investigating the stability of a common heuristic for parameter selection in Tikhonov regularization. The problem is scientifically sound, mathematically rigorous, and algorithmically specified. All conditions and parameters are provided, making it a valid and verifiable problem.\n\n### Theoretical Framework\n\nThe problem addresses the solution of a linear inverse problem of the form $A m \\approx d$, where $m \\in \\mathbb{R}^n$ is the model to be recovered, $d \\in \\mathbb{R}^n$ are the observed data, and $A \\in \\mathbb{R}^{n \\times n}$ is the forward operator. The specified operator $A$ is a Gaussian blur, which is a smoothing operator whose inverse is unstable, making the problem ill-posed.\n\nTikhonov regularization is employed to find a stable and meaningful solution. It reformulates the problem as an optimization problem, seeking to minimize a composite objective function that balances data fidelity with model simplicity:\n$$\nJ(m) = \\underbrace{\\lVert A m-d\\rVert_2^2}_{\\text{Data Misfit}} + \\lambda^2 \\underbrace{\\lVert Lm\\rVert_2^2}_{\\text{Regularization Term}}\n$$\nHere, $\\lambda > 0$ is the regularization parameter that controls the trade-off. The matrix $L \\in \\mathbb{R}^{(n-1) \\times n}$ is a regularization operator, defined here as a first-difference operator, $(L m)_k = m_{k+1}-m_k$, which penalizes non-smoothness (oscillations) in the model $m$.\n\nThe minimizer $m_\\lambda$ of this quadratic objective function is unique for any $\\lambda > 0$ and is found by solving the first-order optimality condition $\\nabla_m J(m) = 0$. This yields the system of normal equations:\n$$\n2A^T(Am - d) + 2\\lambda^2 L^T L m = 0\n$$\n$$\n(A^T A + \\lambda^2 L^T L)m_\\lambda = A^T d\n$$\nThe matrix $(A^T A + \\lambda^2 L^T L)$ is symmetric and positive definite, guaranteeing a unique solution $m_\\lambda$ for any given $\\lambda > 0$.\n\n### The L-Curve and Parameter Selection\n\nA crucial step is the choice of the parameter $\\lambda$. The L-curve method is a widely used heuristic for this purpose. It consists of plotting the logarithm of the regularization term norm, $\\log(\\lVert L m_\\lambda \\rVert_2)$, against the logarithm of the data misfit norm, $\\log(\\lVert A m_\\lambda - d \\rVert_2)$, for a range of $\\lambda$ values. The resulting curve typically has an \"L\" shape. The corner of this \"L\" is considered to be a point of optimal balance between fitting the data and regularizing the solution.\n\nThis corner corresponds to the point of maximum curvature on the L-curve. The problem specifies parameterizing the curve by $t = \\log \\lambda$, such that $\\gamma(t) = (x(t), y(t))$ where $x(t) = \\log(\\lVert A m_\\lambda - d \\rVert_2)$ and $y(t) = \\log(\\lVert L m_\\lambda \\rVert_2)$, with $\\lambda = \\exp(t)$. The signed curvature is given by:\n$$\n\\kappa(t)=\\frac{\\left|x'(t)\\,y''(t)-y'(t)\\,x''(t)\\right|}{\\left(\\left(x'(t)\\right)^2+\\left(y'(t)\\right)^2\\right)^{3/2}}\n$$\nThe optimal parameter choice $\\lambda^\\star = \\exp(t^\\star)$ is where $t^\\star$ maximizes $\\kappa(t)$.\n\n### Numerical Implementation Strategy\n\nA direct numerical computation of the L-curve and its curvature can be sensitive to the discretization of $\\lambda$. The problem outlines a robust procedure to mitigate this sensitivity.\n\n1.  **System Construction**: We first construct the discrete model space with $n=80$ points on $[0,1]$ and define the true model $m_{\\mathrm{true}}$ as a vector. The Gaussian blur matrix $A$ is constructed with elements $A_{ij}=C\\,\\exp\\!\\left(-\\frac{\\left((i-j)h\\right)^2}{2\\sigma_b^2}\\right)$, where $h=1/n$ and $\\sigma_b=0.03$. The normalization constant $C$ is chosen on a per-row basis to ensure $\\sum_{j=0}^{n-1}A_{ij}=1$ for each row $i$. The first-difference operator $L \\in \\mathbb{R}^{(79 \\times 80)}$ is also constructed. These components are constant across all test cases.\n\n2.  **Data Generation**: For each test case, specified by a noise level $s$ and a random seed, noisy data $d$ are generated. First, noise-free data are computed as $d_{\\mathrm{clean}} = A m_{\\mathrm{true}}$. Then, Gaussian noise $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2 \\mathbf{I})$ is added, where the standard deviation is scaled by the data norm: $\\sigma = s\\lVert d_{\\mathrm{clean}} \\rVert_2 / \\sqrt{n}$.\n\n3.  **Robust Curvature Estimation**: The core of the method involves cubic spline interpolation. Instead of calculating derivatives from a finely sampled L-curve (which would be computationally expensive), we compute L-curve points $(x_k, y_k)$ on a coarse grid of the parameter $t_k = \\log\\lambda_k$. These sparse points are then used to create interpolating cubic splines for $x(t)$ and $y(t)$. Cubic splines are twice continuously differentiable, allowing for stable evaluation of the first and second derivatives ($x', x'', y', y''$) required for the curvature formula. These derivatives are evaluated on a fine, uniform grid of $N_f=400$ points spanning the interval $[t_{\\min}, t_{\\max}]$. To avoid instabilities near the endpoints of the interpolation range, the curvature is computed on this fine grid, and the selection of the maximum is restricted to the central $80\\%$ of the interval (by trimming $10\\%$ from each end). The value of $t$ that maximizes the curvature in this trimmed interval is $t^\\star$, and the selected regularization parameter is $\\lambda^\\star = \\exp(t^\\star)$.\n\n4.  **Sensitivity Analysis**: The experiment quantifies the stability of this robust procedure with respect to the coarseness of the initial grid of $t_k$ values. For each test case, we compute two values for $\\lambda^\\star$:\n    - $\\lambda_{\\mathrm{base}}^\\star$: A baseline value computed using the robust spline procedure, where the L-curve points are sampled on a dense grid with step size $\\Delta t_{\\mathrm{base}} = 0.05$.\n    - $\\lambda_{\\mathrm{rob}}^\\star$: An estimated value computed using the same robust spline procedure, but with L-curve points sampled on a coarser grid defined by a case-specific step size $\\Delta t$.\n\n    The error is measured by the absolute difference of the base-10 logarithms of these two parameters: $e = \\left|\\log_{10}(\\lambda_{\\mathrm{rob}}^\\star) - \\log_{10}(\\lambda_{\\mathrm{base}}^\\star)\\right|$. This metric reflects the difference in the order of magnitude of the chosen regularization parameters, which is a relevant measure of stability. This procedure is repeated for five test cases with varying noise levels and coarse grid spacings.", "answer": "```python\nimport numpy as np\nfrom scipy.interpolate import CubicSpline\n\ndef solve():\n    \"\"\"\n    Main function to execute the numerical experiment as specified in the problem statement.\n    \"\"\"\n    \n    # --- 1. Problem Definition and Constants ---\n    N = 80\n    SIGMA_B = 0.03\n    LAMBDA_MIN = 1e-8\n    LAMBDA_MAX = 1e2\n    NF_FINE = 400\n    TRIM_FRAC = 0.1\n    DELTA_T_BASE = 0.05\n\n    test_cases = [\n        # (s, delta_t, seed)\n        (0.01, 2.00, 3),\n        (0.01, 1.00, 5),\n        (0.01, 0.50, 7),\n        (0.01, 0.25, 11),\n        (0.05, 1.00, 13),\n    ]\n\n    t_min = np.log(LAMBDA_MIN)\n    t_max = np.log(LAMBDA_MAX)\n\n    # --- Helper function for constructing operators and true model ---\n    def construct_problem_components(n, sigma_b):\n        h = 1.0 / n\n        x = (np.arange(n) + 0.5) * h\n        \n        # True model m_true\n        m_true = (np.exp(-((x - 0.30)**2) / (2 * 0.04**2)) +\n                  0.7 * np.exp(-((x - 0.75)**2) / (2 * 0.07**2)) +\n                  0.1 * np.sin(6 * np.pi * x))\n        \n        # Gaussian blur operator A\n        i_coords, j_coords = np.meshgrid(np.arange(n), np.arange(n), indexing='ij')\n        A_unnormalized = np.exp(-(((i_coords - j_coords) * h)**2) / (2 * sigma_b**2))\n        row_sums = A_unnormalized.sum(axis=1)\n        A = A_unnormalized / row_sums[:, np.newaxis]\n        \n        # First-difference operator L\n        L = np.zeros((n - 1, n))\n        L[np.arange(n - 1), np.arange(n - 1)] = -1\n        L[np.arange(n - 1), np.arange(n - 1) + 1] = 1\n        \n        return m_true, A, L\n\n    # --- Helper function for generating noisy data ---\n    def generate_noisy_data(m_true, A, s, seed):\n        d_clean = A @ m_true\n        \n        rng = np.random.default_rng(seed)\n        sigma_noise = s * np.linalg.norm(d_clean) / np.sqrt(len(d_clean))\n        \n        noise = rng.normal(0, sigma_noise, size=len(d_clean))\n        d = d_clean + noise\n        return d\n\n    # --- Helper function for robust corner selection ---\n    def get_optimal_lambda(A, L, G, H, ATd, d, delta_t):\n        # Create coarse grid for t = log(lambda)\n        t_coarse = np.arange(t_min, t_max + delta_t / 2, delta_t)\n\n        residuals = []\n        seminorms = []\n\n        for t in t_coarse:\n            lambda_val = np.exp(t)\n            \n            # Solve normal equations: (G + lambda^2 * H) m = ATd\n            M = G + (lambda_val**2) * H\n            m_lambda = np.linalg.solve(M, ATd)\n            \n            residual_norm = np.linalg.norm(A @ m_lambda - d)\n            seminorm = np.linalg.norm(L @ m_lambda)\n            \n            residuals.append(residual_norm)\n            seminorms.append(seminorm)\n            \n        x_coarse = np.log(np.array(residuals))\n        y_coarse = np.log(np.array(seminorms))\n\n        # Handle potential inf/-inf from zero norms at boundary lambda values\n        valid_indices = np.isfinite(x_coarse) & np.isfinite(y_coarse)\n        t_coarse = t_coarse[valid_indices]\n        x_coarse = x_coarse[valid_indices]\n        y_coarse = y_coarse[valid_indices]\n\n        # Cubic spline interpolation\n        spline_x = CubicSpline(t_coarse, x_coarse, bc_type='natural')\n        spline_y = CubicSpline(t_coarse, y_coarse, bc_type='natural')\n\n        # Evaluate on a fine grid\n        t_fine = np.linspace(t_coarse[0], t_coarse[-1], NF_FINE)\n        \n        x_p = spline_x(t_fine, 1)\n        x_pp = spline_x(t_fine, 2)\n        y_p = spline_y(t_fine, 1)\n        y_pp = spline_y(t_fine, 2)\n        \n        # Curvature calculation\n        numerator = np.abs(x_p * y_pp - y_p * x_pp)\n        denominator = (x_p**2 + y_p**2)**1.5\n        # Avoid division by zero if a segment is a straight line (derivative is zero)\n        curvature = np.divide(numerator, denominator, out=np.zeros_like(numerator), where=denominator!=0)\n        \n        # Trim ends to avoid boundary artifacts\n        n_trim = int(TRIM_FRAC * NF_FINE)\n        \n        # Find index of max curvature in the trimmed interval\n        trimmed_curvature = curvature[n_trim:-n_trim]\n        \n        # Handle case where trimming makes the array empty\n        if trimmed_curvature.size == 0:\n            # Fallback to middle of the untrimmed array\n            idx_max_local = len(curvature) // 2\n        else:\n            idx_max_local = np.argmax(trimmed_curvature)\n\n        idx_max_global = n_trim + idx_max_local\n        \n        t_star = t_fine[idx_max_global]\n        lambda_star = np.exp(t_star)\n        \n        return lambda_star\n\n    # --- Execution Logic ---\n    \n    # Construct constant components once\n    m_true, A, L = construct_problem_components(N, SIGMA_B)\n    G = A.T @ A\n    H = L.T @ L\n    \n    results = []\n    \n    for s, delta_t, seed in test_cases:\n        # Generate data for the current case\n        d = generate_noisy_data(m_true, A, s, seed)\n        ATd = A.T @ d\n        \n        # Compute baseline lambda_star with dense grid\n        lambda_base_star = get_optimal_lambda(A, L, G, H, ATd, d, DELTA_T_BASE)\n        \n        # Compute robust lambda_star with coarse grid\n        lambda_rob_star = get_optimal_lambda(A, L, G, H, ATd, d, delta_t)\n        \n        # Calculate error metric\n        error = np.abs(np.log10(lambda_rob_star) - np.log10(lambda_base_star))\n        results.append(f\"{error:.6f}\")\n\n    # Print final output in the required format\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3617390"}, {"introduction": "While standard Tikhonov regularization is excellent for recovering smooth models, it tends to blur sharp boundaries, a significant limitation when imaging blocky geological structures. This advanced practice introduces Total Variation (TV) regularization, a powerful technique designed to preserve sharp edges. You will implement one iteration of a modern splitting algorithm [@problem_id:3617476] to solve a composite objective function, demonstrating how combining a smooth Tikhonov term with a non-smooth TV term can dramatically improve the recovery of piecewise-constant models.", "problem": "Consider the standard linear inverse problem in computational geophysics where a discrete model vector $m \\in \\mathbb{R}^{N}$ produces data $d \\in \\mathbb{R}^{N}$ via a known linear forward operator $G \\in \\mathbb{R}^{N \\times N}$ and additive noise $e \\in \\mathbb{R}^{N}$, so that $d = G m + e$. Suppose that the target synthetic model $m_{\\text{true}}$ is piecewise constant with sharp boundaries, and the forward operator $G$ represents a spatially invariant blur (a convolution by a normalized discrete Gaussian kernel), which is physically plausible when acquisition or processing attenuates high spatial frequencies. We aim to perform one iteration of a split algorithm that combines quadratic Tikhonov regularization and total variation (TV), compute the proximal steps, and demonstrate edge retention under a damping strategy.\n\nThe regularized objective to be minimized is the sum of a smooth term and a nonsmooth TV term. Define the discrete first-difference operator $D \\in \\mathbb{R}^{N \\times N}$ as the forward difference with a zero in the last row, yielding $D m = \\left[m_{2}-m_{1}, m_{3}-m_{2}, \\ldots, m_{N}-m_{N-1}, 0\\right]^{\\top}$. The quadratic Tikhonov term uses $D$ to penalize roughness via the norm $\\lVert D m \\rVert_{2}^{2}$. The total variation term is $\\lVert D m \\rVert_{1}$. Introduce the positive scalars $\\alpha$ (Tikhonov weight), $\\beta$ (TV weight), and a Levenberg–Marquardt (LM) damping parameter $\\mu$ to stabilize the normal equations. We consider the following composite objective:\n$$\n\\min_{m \\in \\mathbb{R}^{N}} \\left(\\frac{1}{2}\\lVert G m - d \\rVert_{2}^{2} + \\frac{\\alpha}{2} \\lVert D m \\rVert_{2}^{2} + \\beta \\lVert D m \\rVert_{1}\\right) + \\frac{\\mu}{2}\\lVert m \\rVert_{2}^{2}.\n$$\nUse a variable splitting for the TV term by introducing an auxiliary variable $v \\in \\mathbb{R}^{N}$ to enforce $v \\approx D m$, and perform one iteration of a splitting method such as Alternating Direction Method of Multipliers (ADMM). The penalty parameter for the splitting is $\\gamma > 0$. The iteration should consist of a model update (the proximal step for the smooth part), a shrinkage step (the proximal step for the TV term), and a dual variable update. The soft-thresholding function for the TV proximal step must be used element-wise and defined by the shrinkage operator $S_{\\tau}(y) = \\operatorname{sign}(y)\\cdot\\max\\{|y|-\\tau, 0\\}$ for threshold $\\tau > 0$.\n\nInitialization strategy: compute a Tikhonov-only baseline model $m^{0}$ by minimizing only the smooth part (data misfit plus quadratic Tikhonov plus LM damping), starting from the fundamental linear inverse model and quadratic penalties. Set $v^{0} = D m^{0}$ and the scaled dual variable $b^{0} = 0$. Then perform one iteration to obtain $(m^{1}, v^{1}, b^{1})$.\n\nSynthetic setup: let $N = 64$. Define $m_{\\text{true}}$ as a piecewise constant vector on a uniform grid with three regions: the first region has value $0$ for indices $1$ through $20$, the second region has value $1$ for indices $21$ through $40$, and the third region has value $0.5$ for indices $41$ through $64$. Construct $G$ as the matrix of convolution with a normalized discrete Gaussian kernel of standard deviation $\\sigma_{\\text{blur}}$ (in grid units), truncated to a finite symmetric stencil and normalized to sum $1$. Generate $d$ from $d = G m_{\\text{true}} + e$, where $e$ is independent and identically distributed zero-mean Gaussian noise with standard deviation $\\sigma_{\\text{noise}}$. Use a fixed random seed for reproducibility.\n\nEdge retention metric: to quantify edge retention after one iteration, compute the ratio\n$$\nR = \\frac{\\max_{i} \\left| (D m^{1})_{i} \\right|}{\\max_{i} \\left| (D m^{0})_{i} \\right|},\n$$\nwhich compares the largest discrete gradient magnitude after the split iteration to that of the Tikhonov-only baseline. A value $R > 1$ indicates amplified edge contrast relative to the baseline, while $R \\approx 1$ indicates no change, and $R < 1$ indicates reduced edge contrast.\n\nTest suite: evaluate the program for the following parameter sets, which together probe typical behavior, strong TV influence, weak TV influence, and stability under a highly smoothing forward operator:\n- Case $1$ (happy path): $\\alpha = 0.01$, $\\beta = 0.2$, $\\gamma = 1.0$, $\\mu = 0.001$, $\\sigma_{\\text{blur}} = 1.5$, $\\sigma_{\\text{noise}} = 0.01$.\n- Case $2$ (strong TV): $\\alpha = 0.001$, $\\beta = 1.0$, $\\gamma = 2.0$, $\\mu = 0.001$, $\\sigma_{\\text{blur}} = 1.5$, $\\sigma_{\\text{noise}} = 0.01$.\n- Case $3$ (weak TV): $\\alpha = 0.05$, $\\beta = 0.05$, $\\gamma = 0.5$, $\\mu = 0.01$, $\\sigma_{\\text{blur}} = 1.5$, $\\sigma_{\\text{noise}} = 0.01$.\n- Case $4$ (near-singular forward operator with heavier blur): $\\alpha = 0.05$, $\\beta = 0.3$, $\\gamma = 0.5$, $\\mu = 0.1$, $\\sigma_{\\text{blur}} = 3.0$, $\\sigma_{\\text{noise}} = 0.02$.\n\nAlgorithmic requirements:\n- Build $G$ explicitly as a dense matrix corresponding to the Gaussian convolution with standard deviation $\\sigma_{\\text{blur}}$, truncated to a symmetric stencil of radius $r = \\lceil 3 \\sigma_{\\text{blur}} \\rceil$, and normalized to sum $1$.\n- Build $D$ as the forward-difference operator with $(D m)_{i} = m_{i+1} - m_{i}$ for $i = 1,\\ldots,N-1$ and $(D m)_{N} = 0$.\n- Compute $m^{0}$ by solving the normal equations associated with the smooth-only objective.\n- Perform one ADMM iteration producing $(m^{1}, v^{1}, b^{1})$ using the proximal steps, with $v^{0} = D m^{0}$ and $b^{0} = 0$.\n- Compute $R$ as defined above.\n\nFinal output format: your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $\\left[ r_{1}, r_{2}, r_{3}, r_{4} \\right]$), where each $r_{k}$ is the floating-point value of $R$ for case $k$.\n\nNo physical units are required because amplitudes are dimensionless. Angles do not appear and thus angle units are not applicable. All numerical values should be treated as real numbers.", "solution": "The problem is valid as it is scientifically grounded, well-posed, objective, and contains all necessary information for a unique solution. It describes a standard procedure in computational geophysics for solving linear inverse problems with composite regularization. We will now proceed with a detailed solution.\n\n### 1. Problem Formulation\nThe problem is to recover a discrete model vector $m \\in \\mathbb{R}^{N}$ from noisy, blurred data $d \\in \\mathbb{R}^{N}$. The forward model is linear:\n$$ d = G m + e $$\nwhere $G \\in \\mathbb{R}^{N \\times N}$ is a linear forward operator representing a Gaussian blur, and $e \\in \\mathbb{R}^{N}$ is additive Gaussian noise. The model is recovered by minimizing a composite objective function that combines data fidelity with regularization terms designed to enforce prior knowledge about the model (piecewise constancy). The objective function is:\n$$ J(m) = \\frac{1}{2}\\lVert G m - d \\rVert_{2}^{2} + \\frac{\\alpha}{2} \\lVert D m \\rVert_{2}^{2} + \\beta \\lVert D m \\rVert_{1} + \\frac{\\mu}{2}\\lVert m \\rVert_{2}^{2} $$\nHere, the first term is the L2-norm data misfit. The second term is a quadratic Tikhonov regularizer controlled by parameter $\\alpha > 0$, which penalizes roughness using the first-difference operator $D$. The third term is the Total Variation (TV) regularizer, weighted by $\\beta > 0$, which promotes sharp edges by penalizing the L1-norm of the model's gradient. The final term is a Levenberg-Marquardt (LM) damping term, weighted by $\\mu > 0$, which ensures stability of the inversion by penalizing the L2-norm of the model itself.\n\n### 2. Operator Construction\nThe problem requires the explicit construction of the matrices $G$ and $D$.\n-   **Forward Operator $G$**: This matrix represents a spatially invariant convolution with a normalized discrete Gaussian kernel. For a standard deviation of $\\sigma_{\\text{blur}}$, the kernel is defined over a symmetric stencil of radius $r = \\lceil 3 \\sigma_{\\text{blur}} \\rceil$. The unnormalized kernel value at an integer offset $j \\in [-r, r]$ is $k_j' = \\exp(-j^2 / (2 \\sigma_{\\text{blur}}^2))$. The kernel $k$ is then normalized such that $\\sum_{j=-r}^{r} k_j = 1$. The matrix $G \\in \\mathbb{R}^{N \\times N}$ is a banded Toeplitz-like matrix where each row $i$ contains the shifted kernel, such that $(Gm)_i$ is the discrete convolution of the kernel with the model $m$. Specifically, $G_{ij} = k_{j-i}$ for $|i-j| \\le r$ and $G_{ij} = 0$ otherwise, adopting zero-padding at the boundaries.\n\n-   **Difference Operator $D$**: This matrix approximates the first derivative. As specified, it is the forward-difference operator with the last row set to zero. For $m \\in \\mathbb{R}^N$ (with 0-based indexing from $0$ to $N-1$), we have $(Dm)_i = m_{i+1} - m_i$ for $i \\in [0, N-2]$ and $(Dm)_{N-1} = 0$. This corresponds to an $N \\times N$ matrix $D$ with $D_{i,i} = -1$ and $D_{i,i+1} = 1$ for $i \\in [0, N-2]$, and all other entries being zero.\n\n### 3. Baseline Model $m^0$ (Tikhonov-only Inversion)\nThe initialization requires computing a baseline model $m^0$ by minimizing only the smooth parts of the objective function:\n$$ J_{\\text{smooth}}(m) = \\frac{1}{2}\\lVert G m - d \\rVert_{2}^{2} + \\frac{\\alpha}{2} \\lVert D m \\rVert_{2}^{2} + \\frac{\\mu}{2}\\lVert m \\rVert_{2}^{2} $$\nThis is a quadratic function of $m$. Its minimum is found by setting its gradient with respect to $m$ to zero:\n$$ \\nabla_m J_{\\text{smooth}}(m) = G^T (G m - d) + \\alpha D^T D m + \\mu m = 0 $$\nRearranging gives the normal equations:\n$$ (G^T G + \\alpha D^T D + \\mu I) m = G^T d $$\nwhere $I$ is the $N \\times N$ identity matrix. The matrix $A_0 = G^T G + \\alpha D^T D + \\mu I$ is symmetric and positive definite (since $\\mu > 0$), so it is invertible. The baseline model is the unique solution to this linear system:\n$$ m^0 = (G^T G + \\alpha D^T D + \\mu I)^{-1} G^T d $$\n\n### 4. Single ADMM Iteration\nThe Alternating Direction Method of Multipliers (ADMM) is used to solve the full objective by introducing a splitting variable $v \\in \\mathbb{R}^N$ such that $v = Dm$. The constrained problem is\n$$ \\min_{m,v} \\left( \\frac{1}{2}\\lVert G m - d \\rVert_{2}^{2} + \\frac{\\alpha}{2} \\lVert D m \\rVert_{2}^{2} + \\frac{\\mu}{2}\\lVert m \\rVert_{2}^{2} + \\beta \\lVert v \\rVert_{1} \\right) \\quad \\text{subject to} \\quad v = Dm $$\nThe augmented Lagrangian (in scaled form) is:\n$$ L_{\\gamma}(m, v, b) = \\mathcal{F}(m) + \\mathcal{G}(v) + \\frac{\\gamma}{2}\\lVert D m - v + b \\rVert_{2}^{2} - \\frac{\\gamma}{2}\\lVert b \\rVert_{2}^{2} $$\nwhere $\\mathcal{F}(m) = \\frac{1}{2}\\lVert G m - d \\rVert_{2}^{2} + \\frac{\\alpha}{2} \\lVert D m \\rVert_{2}^{2} + \\frac{\\mu}{2}\\lVert m \\rVert_{2}^{2}$ is the smooth part, $\\mathcal{G}(v) = \\beta \\lVert v \\rVert_{1}$ is the non-smooth part, $b$ is the scaled dual variable, and $\\gamma > 0$ is the penalty parameter.\n\nAn ADMM iteration consists of sequential updates to $m$, $v$, and $b$. We perform one iteration starting from $(m^0, v^0, b^0)$.\n-   **Initialization**: $m^0$ is computed as above. Then, $v^0 = Dm^0$ and the dual variable is initialized to $b^0 = 0$.\n\n-   **$m$-update**: We find $m^1$ by minimizing $L_{\\gamma}(m, v^0, b^0)$ with respect to $m$:\n$$ m^1 = \\arg\\min_{m} \\left( \\mathcal{F}(m) + \\frac{\\gamma}{2} \\lVert Dm - v^0 + b^0 \\rVert_2^2 \\right) $$\nSince $b^0=0$, the objective is $\\mathcal{F}(m) + \\frac{\\gamma}{2} \\lVert Dm - v^0 \\rVert_2^2$. This is a quadratic function, and setting its gradient to zero gives the linear system:\n$$ (G^T G + (\\alpha + \\gamma) D^T D + \\mu I) m = G^T d + \\gamma D^T v^0 $$\nThe matrix on the left is again invertible, yielding a unique solution for $m^1$.\n\n-   **$v$-update**: We find $v^1$ by minimizing $L_{\\gamma}(m^1, v, b^0)$ with respect to $v$:\n$$ v^1 = \\arg\\min_{v} \\left( \\beta \\lVert v \\rVert_{1} + \\frac{\\gamma}{2} \\lVert Dm^1 - v + b^0 \\rVert_2^2 \\right) $$\nWith $b^0=0$, this is a standard proximal operator problem:\n$$ v^1 = \\arg\\min_{v} \\left( \\beta \\lVert v \\rVert_{1} + \\frac{\\gamma}{2} \\lVert v - Dm^1 \\rVert_2^2 \\right) $$\nThe solution is given by the element-wise soft-thresholding operator $S_{\\tau}$ with threshold $\\tau = \\beta/\\gamma$:\n$$ v^1 = S_{\\beta/\\gamma}(Dm^1) $$\nwhere $(S_{\\tau}(y))_i = \\operatorname{sign}(y_i) \\cdot \\max(|y_i| - \\tau, 0)$.\n\n-   **$b$-update**: The dual variable is updated as:\n$$ b^1 = b^0 + Dm^1 - v^1 = Dm^1 - v^1 $$\n\n### 5. Edge Retention Metric\nTo quantify the preservation of sharp edges, we compute the ratio $R$:\n$$ R = \\frac{\\max_{i} \\left| (D m^{1})_{i} \\right|}{\\max_{i} \\left| (D m^{0})_{i} \\right|} $$\nThis metric compares the maximum magnitude of the discrete gradient of the model after one ADMM iteration ($m^1$) to that of the baseline Tikhonov-only solution ($m^0$). A value of $R>1$ indicates that the inclusion of the TV term has amplified the sharpest edge relative to the smooth baseline, which is the desired behavior for this type of regularizer.\n\n### 6. Summary of the Algorithm\nFor each test case, the following steps are performed:\n1.  Set parameters $N=64$, $\\alpha, \\beta, \\gamma, \\mu, \\sigma_{\\text{blur}}, \\sigma_{\\text{noise}}$, and a fixed random seed for noise generation.\n2.  Construct the true model $m_{\\text{true}}$, the Gaussian blur matrix $G$, and the difference matrix $D$.\n3.  Generate the data vector $d = G m_{\\text{true}} + e$, where $e \\sim \\mathcal{N}(0, \\sigma_{\\text{noise}}^2 I)$.\n4.  Solve for the baseline model $m^0$ from the system $(G^T G + \\alpha D^T D + \\mu I) m^0 = G^T d$.\n5.  Initialize $v^0 = D m^0$.\n6.  Solve for the first ADMM iterate $m^1$ from the system $(G^T G + (\\alpha + \\gamma) D^T D + \\mu I) m^1 = G^T d + \\gamma D^T v^0$.\n7.  Compute the discrete gradients $Dm^0$ and $Dm^1$.\n8.  Calculate the ratio $R = \\max |Dm^1| / \\max |Dm^0|$.\n\nThis procedure is repeated for all four parameter sets specified in the problem.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the computational geophysics inverse problem for four test cases.\n    \"\"\"\n\n    test_cases = [\n        # Case 1 (happy path)\n        {'alpha': 0.01, 'beta': 0.2, 'gamma': 1.0, 'mu': 0.001, 'sigma_blur': 1.5, 'sigma_noise': 0.01},\n        # Case 2 (strong TV)\n        {'alpha': 0.001, 'beta': 1.0, 'gamma': 2.0, 'mu': 0.001, 'sigma_blur': 1.5, 'sigma_noise': 0.01},\n        # Case 3 (weak TV)\n        {'alpha': 0.05, 'beta': 0.05, 'gamma': 0.5, 'mu': 0.01, 'sigma_blur': 1.5, 'sigma_noise': 0.01},\n        # Case 4 (near-singular forward operator)\n        {'alpha': 0.05, 'beta': 0.3, 'gamma': 0.5, 'mu': 0.1, 'sigma_blur': 3.0, 'sigma_noise': 0.02},\n    ]\n\n    N = 64\n    RANDOM_SEED = 42\n    np.random.seed(RANDOM_SEED)\n\n    def construct_m_true(n):\n        m = np.zeros(n)\n        m[0:20] = 0.0\n        m[20:40] = 1.0\n        m[40:64] = 0.5\n        return m\n\n    def construct_G(n, sigma_blur):\n        r = int(np.ceil(3 * sigma_blur))\n        x = np.arange(-r, r + 1)\n        kernel = np.exp(-x**2 / (2 * sigma_blur**2))\n        kernel /= np.sum(kernel)\n        \n        G = np.zeros((n, n))\n        for i in range(n):\n            for j_offset, k_val in zip(x, kernel):\n                j = i + j_offset\n                if 0 <= j < n:\n                    G[i, j] = k_val\n        return G\n\n    def construct_D(n):\n        D = np.zeros((n, n))\n        for i in range(n - 1):\n            D[i, i] = -1.0\n            D[i, i + 1] = 1.0\n        return D\n    \n    def soft_threshold(y, tau):\n        return np.sign(y) * np.maximum(np.abs(y) - tau, 0)\n\n    results = []\n\n    # Construct constant operators and true model\n    m_true = construct_m_true(N)\n    D = construct_D(N)\n    DT_D = D.T @ D\n    I = np.identity(N)\n\n    for params in test_cases:\n        alpha = params['alpha']\n        beta = params['beta']\n        gamma = params['gamma']\n        mu = params['mu']\n        sigma_blur = params['sigma_blur']\n        sigma_noise = params['sigma_noise']\n        \n        # Construct operators and data that depend on parameters\n        G = construct_G(N, sigma_blur)\n        noise = np.random.normal(0, sigma_noise, N)\n        d = G @ m_true + noise\n        \n        GT_G = G.T @ G\n        GT_d = G.T @ d\n\n        # --- Step 0: Baseline Tikhonov-only model m^0 ---\n        # Solve (G^T G + alpha * D^T D + mu * I) m = G^T d\n        A0 = GT_G + alpha * DT_D + mu * I\n        m0 = np.linalg.solve(A0, GT_d)\n        \n        # --- Step 1: One ADMM iteration for m^1, v^1, b^1 ---\n        # Initialize\n        v0 = D @ m0\n        # b0 = np.zeros(N) is implicit in the next steps\n\n        # m-update\n        # Solve (G^T G + (alpha + gamma) * D^T D + mu * I) m = G^T d + gamma * D^T v^0\n        A1 = GT_G + (alpha + gamma) * DT_D + mu * I\n        b1_rhs = GT_d + gamma * (D.T @ v0)\n        m1 = np.linalg.solve(A1, b1_rhs)\n\n        # v-update and b-update (calculated for completeness as per problem description)\n        # v1 = S_{beta/gamma}(D @ m1)\n        # b1 = D @ m1 - v1\n        dm1_vec = D @ m1\n        v1 = soft_threshold(dm1_vec, beta / gamma)\n        b1 = dm1_vec - v1\n        \n        # --- Compute Edge Retention Metric R ---\n        dm0_grad_mag = np.max(np.abs(D @ m0))\n        dm1_grad_mag = np.max(np.abs(D @ m1))\n        \n        # Handle potential division by zero, though unlikely in this setup\n        if dm0_grad_mag == 0:\n            # If the baseline has zero gradient, any non-zero gradient in m1 is infinite improvement.\n            # If both are zero, the ratio is 1 (no change).\n            R = np.inf if dm1_grad_mag > 0 else 1.0\n        else:\n            R = dm1_grad_mag / dm0_grad_mag\n        \n        results.append(R)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == '__main__':\n    solve()\n\n```", "id": "3617476"}]}