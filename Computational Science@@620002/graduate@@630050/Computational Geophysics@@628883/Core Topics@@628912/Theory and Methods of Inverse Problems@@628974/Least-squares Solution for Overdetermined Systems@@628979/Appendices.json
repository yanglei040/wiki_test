{"hands_on_practices": [{"introduction": "This practice provides a concrete, hands-on introduction to the critical concept of data leverage in linear regression. By working with a small, tractable dataset, you will directly calculate how a single outlying data point can disproportionately influence the Ordinary Least Squares (OLS) solution, and then explore how Weighted Least Squares (WLS) offers a straightforward method to mitigate this effect. This exercise builds a strong intuition for diagnosing influential data and the fundamentals of robust estimation [@problem_id:3606778].", "problem": "In a one-dimensional refraction experiment in computational geophysics, assume a single unknown constant subsurface slowness $s$ (in seconds per kilometer, $\\mathrm{s/km}$). A set of source–receiver offsets $L_i$ (in kilometers) and corresponding observed travel times $t_i$ (in seconds) follow the linear data model $t_i = L_i s + \\epsilon_i$, where the $\\epsilon_i$ are independent, zero-mean errors with finite variance. Consider the design matrix $G \\in \\mathbb{R}^{N \\times 1}$ whose $i$-th row is $[L_i]$, and the data vector $d \\in \\mathbb{R}^{N}$ whose components are $t_i$. You are given the following specific dataset with $N=4$:\n- Offsets: $L = [\\,1,\\;1,\\;1,\\;10\\,]^{\\mathsf{T}}$,\n- Observed times: $t = [\\,0.5,\\;0.5,\\;0.5,\\;6.0\\,]^{\\mathsf{T}}$.\n\nTreat the problem as an overdetermined system with $N \\gg 1$ in principle, but here use $N=4$ for analytical tractability. Starting from the definition of Ordinary Least Squares (OLS) as the minimizer of the sum of squared residuals and the definition of the hat matrix $H = G\\,(G^{\\mathsf{T}}G)^{-1}G^{\\mathsf{T}}$, do the following:\n\n1. Derive the OLS estimate $\\hat{s}$ for this one-parameter problem directly from minimizing the sum of squared residuals $\\sum_{i=1}^{N} (t_i - L_i s)^2$. Use your result to compute $\\hat{s}$ for the given data.\n\n2. Using the definition of the hat matrix, compute the leverage values $h_{ii}$ and interpret the row-wise leverage for this dataset. Quantify, in decimal form, the fraction of the total diagonal leverage associated with the fourth row.\n\n3. To mitigate domination by the high-leverage row, consider Weighted Least Squares (WLS) with diagonal weights $W = \\mathrm{diag}(w_1,\\dots,w_N)$ that minimize $\\sum_{i=1}^{N} w_i (t_i - L_i s)^2$. Choose weights $w_1 = w_2 = w_3 = 1$ and $w_4 = 0.01$ to downweight the fourth observation. Derive the WLS estimator $\\hat{s}_w$ and compute its value for the given data.\n\n4. Briefly explain, using equations for an $M$-estimator with Huber loss, how an Iteratively Reweighted Least Squares (IRLS) procedure would adaptively reduce the influence of an outlying residual like the fourth datum by assigning a smaller effective weight, without hard-coding $w_4$. You do not need to perform iterations.\n\nProvide, as your final answer, the numerical value of the weighted estimate $\\hat{s}_w$. Express your result in $\\mathrm{s/km}$ and round your answer to four significant figures.", "solution": "The problem statement has been validated and is deemed valid. It is scientifically grounded in the principles of linear inverse theory as applied to geophysics, is well-posed, and uses objective, formal language. All necessary data are provided and are consistent. We may proceed with the solution.\n\nThe problem asks for a four-part analysis of a simple linear inverse problem using a specific dataset. The relationship between travel time $t$ and offset $L$ is modeled as a line through the origin, $t = Ls$, where $s$ is the slowness.\n\n**1. Ordinary Least Squares (OLS) Estimate**\n\nThe Ordinary Least Squares (OLS) estimate for the parameter $s$ is the value $\\hat{s}$ that minimizes the sum of squared residuals, $S(s)$. The residual for the $i$-th measurement is $r_i = t_i - L_i s$. The objective function is:\n$$\nS(s) = \\sum_{i=1}^{N} r_i^2 = \\sum_{i=1}^{N} (t_i - L_i s)^2\n$$\nTo find the minimum, we compute the derivative of $S(s)$ with respect to $s$ and set it to zero:\n$$\n\\frac{dS}{ds} = \\sum_{i=1}^{N} \\frac{d}{ds} (t_i - L_i s)^2 = \\sum_{i=1}^{N} 2(t_i - L_i s)(-L_i) = -2 \\sum_{i=1}^{N} (L_i t_i - L_i^2 s)\n$$\nSetting $\\frac{dS}{ds} = 0$:\n$$\n\\sum_{i=1}^{N} (L_i t_i - L_i^2 s) = 0 \\implies \\sum_{i=1}^{N} L_i t_i - s \\sum_{i=1}^{N} L_i^2 = 0\n$$\nSolving for $s$ gives the OLS estimate $\\hat{s}$:\n$$\n\\hat{s} = \\frac{\\sum_{i=1}^{N} L_i t_i}{\\sum_{i=1}^{N} L_i^2}\n$$\nThis is the general formula for a one-parameter linear fit through the origin. For the given dataset, $L = [\\,1,\\;1,\\;1,\\;10\\,]^{\\mathsf{T}}$ and $t = [\\,0.5,\\;0.5,\\;0.5,\\;6.0\\,]^{\\mathsf{T}}$. We compute the sums:\n$$\n\\sum_{i=1}^{4} L_i t_i = (1)(0.5) + (1)(0.5) + (1)(0.5) + (10)(6.0) = 0.5 + 0.5 + 0.5 + 60.0 = 61.5\n$$\n$$\n\\sum_{i=1}^{4} L_i^2 = 1^2 + 1^2 + 1^2 + 10^2 = 1 + 1 + 1 + 100 = 103\n$$\nThus, the OLS estimate is:\n$$\n\\hat{s} = \\frac{61.5}{103} \\approx 0.597087\\; \\mathrm{s/km}\n$$\n\n**2. Hat Matrix and Leverage**\n\nThe hat matrix $H$ maps the observed data vector $d$ to the predicted data vector $\\hat{d}$, i.e., $\\hat{d} = Hd$. For a general linear model $d = Gm$, the hat matrix is defined as $H = G(G^{\\mathsf{T}}G)^{-1}G^{\\mathsf{T}}$. The diagonal elements of $H$, denoted $h_{ii}$, are the leverage values.\n\nIn this problem, the model parameter vector $m$ is just the scalar $s$. The design matrix $G$ is a column vector of the offsets $L_i$:\n$$\nG = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 10 \\end{pmatrix}\n$$\nFirst, we compute $G^{\\mathsf{T}}G$:\n$$\nG^{\\mathsf{T}}G = \\begin{pmatrix} 1 & 1 & 1 & 10 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 10 \\end{pmatrix} = 1^2 + 1^2 + 1^2 + 10^2 = 103\n$$\nThis is a scalar, so its inverse is simply:\n$$\n(G^{\\mathsf{T}}G)^{-1} = \\frac{1}{103}\n$$\nNow we construct the hat matrix $H$:\n$$\nH = G(G^{\\mathsf{T}}G)^{-1}G^{\\mathsf{T}} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 10 \\end{pmatrix} \\left(\\frac{1}{103}\\right) \\begin{pmatrix} 1 & 1 & 1 & 10 \\end{pmatrix} = \\frac{1}{103} \\begin{pmatrix}\n1\\cdot1 & 1\\cdot1 & 1\\cdot1 & 1\\cdot10 \\\\\n1\\cdot1 & 1\\cdot1 & 1\\cdot1 & 1\\cdot10 \\\\\n1\\cdot1 & 1\\cdot1 & 1\\cdot1 & 1\\cdot10 \\\\\n10\\cdot1 & 10\\cdot1 & 10\\cdot1 & 10\\cdot10\n\\end{pmatrix} = \\frac{1}{103} \\begin{pmatrix}\n1 & 1 & 1 & 10 \\\\\n1 & 1 & 1 & 10 \\\\\n1 & 1 & 1 & 10 \\\\\n10 & 10 & 10 & 100\n\\end{pmatrix}\n$$\nThe leverage values $h_{ii}$ are the diagonal elements of $H$:\n$h_{11} = \\frac{1}{103}$, $h_{22} = \\frac{1}{103}$, $h_{33} = \\frac{1}{103}$, and $h_{44} = \\frac{100}{103}$.\n\nLeverage $h_{ii}$ quantifies the influence of the $i$-th observation $t_i$ on its own predicted value $\\hat{t}_i$. A value close to $1$ indicates that the observation has high influence, effectively pulling the model fit towards itself. Here, the first three data points have very low leverage ($h_{11}=h_{22}=h_{33} \\approx 0.0097$), while the fourth data point has extremely high leverage ($h_{44} \\approx 0.9709$). This means the OLS solution is almost entirely determined by the fourth measurement at $L_4=10$.\n\nThe sum of the diagonal elements of the hat matrix, $\\mathrm{Tr}(H)$, equals the number of model parameters, which is $1$ in this case. $\\sum_{i=1}^{4} h_{ii} = \\frac{1}{103} + \\frac{1}{103} + \\frac{1}{103} + \\frac{100}{103} = \\frac{103}{103} = 1$. The fraction of the total diagonal leverage associated with the fourth row is $h_{44} / \\mathrm{Tr}(H) = h_{44} / 1 = h_{44}$. As a decimal, this is:\n$$\n\\frac{100}{103} \\approx 0.9709\n$$\n\n**3. Weighted Least Squares (WLS) Estimate**\n\nTo mitigate the influence of the high-leverage fourth point, we use Weighted Least Squares (WLS). The WLS estimate $\\hat{s}_w$ minimizes the weighted sum of squared residuals:\n$$\nS_w(s) = \\sum_{i=1}^{N} w_i (t_i - L_i s)^2\n$$\nFollowing the same differentiation process as for OLS, we find the WLS estimator:\n$$\n\\hat{s}_w = \\frac{\\sum_{i=1}^{N} w_i L_i t_i}{\\sum_{i=1}^{N} w_i L_i^2}\n$$\nUsing the specified weights $w_1 = w_2 = w_3 = 1$ and $w_4 = 0.01$, we compute the weighted sums:\n$$\n\\sum_{i=1}^{4} w_i L_i t_i = (1)(1)(0.5) + (1)(1)(0.5) + (1)(1)(0.5) + (0.01)(10)(6.0) = 0.5 + 0.5 + 0.5 + 0.6 = 2.1\n$$\n$$\n\\sum_{i=1}^{4} w_i L_i^2 = (1)(1^2) + (1)(1^2) + (1)(1^2) + (0.01)(10^2) = 1 + 1 + 1 + (0.01)(100) = 3 + 1 = 4\n$$\nThe WLS estimate is therefore:\n$$\n\\hat{s}_w = \\frac{2.1}{4} = 0.525\\; \\mathrm{s/km}\n$$\nThis value is much closer to the slowness of $0.5\\; \\mathrm{s/km}$ suggested by the first three points, demonstrating the successful down-weighting of the high-leverage fourth point.\n\n**4. M-estimators and Iteratively Reweighted Least Squares (IRLS)**\n\nAn M-estimator seeks to minimize a more general objective function, $\\sum_{i=1}^{N} \\rho(r_i)$, where $r_i$ is the residual and $\\rho(r)$ is a robust loss function that grows more slowly than $r^2$ for large $r$. The Huber loss function is a common choice:\n$$\n\\rho(r) = \\begin{cases}\n  \\frac{1}{2} r^2 & \\text{if } |r| \\le \\delta \\\\\n  \\delta(|r| - \\frac{1}{2}\\delta) & \\text{if } |r| > \\delta\n\\end{cases}\n$$\nwhere $\\delta$ is a tuning parameter. Minimizing this objective function leads to the non-linear estimating equation $\\sum_{i=1}^{N} L_i \\psi(r_i) = 0$, where $\\psi(r) = \\rho'(r)$ is the influence function.\n\nIteratively Reweighted Least Squares (IRLS) is a procedure to solve this equation. It works by recasting the estimating equation as a weighted least-squares problem where the weights depend on the residuals themselves. We define an adaptive weight function $W(r) = \\psi(r)/r$. For the Huber loss, the associated weight function is:\n$$\nW(r) = \\frac{\\psi(r)}{r} = \\begin{cases}\n  1 & \\text{if } |r| \\le \\delta \\\\\n  \\frac{\\delta}{|r|} & \\text{if } |r| > \\delta\n\\end{cases}\n$$\nThe IRLS procedure is:\n1.  Obtain an initial estimate, e.g., the OLS estimate $\\hat{s}^{(0)}$.\n2.  For iteration $k=1, 2, ...$:\n    a. Calculate residuals: $r_i^{(k-1)} = t_i - L_i \\hat{s}^{(k-1)}$.\n    b. Calculate new weights: $w_i^{(k)} = W(r_i^{(k-1)})$. Data points with large residuals ($|r_i| > \\delta$) will be assigned weights smaller than $1$.\n    c. Solve the WLS problem with these new weights to get the updated estimate:\n       $$\n       \\hat{s}^{(k)} = \\frac{\\sum_{i=1}^N w_i^{(k)} L_i t_i}{\\sum_{i=1}^N w_i^{(k)} L_i^2}\n       $$\n3.  Repeat until the estimate $\\hat{s}^{(k)}$ converges.\n\nIn the context of this problem, a datum is considered an outlier if its residual is large relative to some robust measure of the data's scale. The first three points imply a slowness of $s=0.5\\;\\mathrm{s/km}$. Relative to this trend, the fourth point yields a large residual: $r_4 = 6.0 - 10(0.5) = 1.0$. An IRLS procedure, after an initial step, would identify this large residual, assign a smaller effective weight $w_4 = \\delta/|r_4|  1$, and recompute the solution. This process adaptively and automatically reduces the influence of any data point that is inconsistent with the bulk of the data (an outlier), without requiring the user to hard-code weights as in WLS.", "answer": "$$\\boxed{0.5250}$$", "id": "3606778"}, {"introduction": "Moving from the influence of single data points, this exercise examines a more fundamental source of instability in least-squares solutions: an ill-conditioned design matrix $G$. You will analyze a system that is nearly rank-deficient and use principles from the Singular Value Decomposition (SVD) to visualize the misfit function as a \"long, shallow valley.\" By deriving the variance amplification factor, you will quantify how this geometric feature leads to large uncertainties in the estimated model parameters, a key concept in geophysical inverse theory [@problem_id:3606781].", "problem": "In linearized traveltime tomography for a two-cell medium, suppose the vector of data residuals $\\mathbf{d} \\in \\mathbb{R}^{3}$ is modeled as $\\mathbf{d} = G(\\varepsilon) \\, \\mathbf{m}_{\\mathrm{true}} + \\mathbf{n}$, where $\\mathbf{m}_{\\mathrm{true}} \\in \\mathbb{R}^{2}$ are perturbations of cell slownesses in scaled units and $\\mathbf{n}$ is observational noise. Consider the design matrix\n$$\nG(\\varepsilon) \\;=\\;\n\\begin{pmatrix}\n1  1 \\\\\n1  1+\\varepsilon \\\\\n0  \\varepsilon\n\\end{pmatrix},\n$$\nwith $\\varepsilon  0$ small, representing three distinct rays whose path-length sensitivities make the two columns of $G(\\varepsilon)$ nearly collinear as $\\varepsilon \\to 0$. Assume $\\mathbf{n}$ is zero-mean Gaussian with independent components of equal variance, that is, $\\mathbf{n} \\sim \\mathcal{N}(0, \\sigma^{2} I)$ for some $\\sigma^{2}  0$.\n\nThe ordinary least squares (OLS) estimate $\\hat{\\mathbf{m}}(\\varepsilon)$ minimizes the squared residual norm $J(\\mathbf{m}) = \\| G(\\varepsilon) \\, \\mathbf{m} - \\mathbf{d} \\|_{2}^{2}$. In the limit $\\varepsilon \\to 0$, the matrix $G(\\varepsilon)$ approaches rank deficiency, so the misfit landscape develops a long, shallow valley of nearly equivalent minimizers.\n\nTasks:\n- Using only the definitions of least squares and the geometry induced by the singular value decomposition (SVD, singular value decomposition), provide a construction that exhibits how near-rank-deficiency in $G(\\varepsilon)$ yields multiple nearly equivalent minimizers. In particular, argue that there exists a unit direction $\\mathbf{v}_{\\varepsilon} \\in \\mathbb{R}^{2}$ along which the second directional derivative of $J$ at the minimizer is $O(\\varepsilon^{2})$, and produce an explicit pair $\\mathbf{m}_{\\star}$ and $\\mathbf{m}_{\\star} + \\delta \\mathbf{v}_{\\varepsilon}$ (with $\\delta$ independent of $\\varepsilon$) whose misfit difference is $O(\\varepsilon^{2})$ as $\\varepsilon \\to 0$.\n- Under the assumptions above and starting from first principles (the definitions of OLS and Gaussian noise with covariance $\\sigma^{2} I$), derive the exact expression for the dimensionless variance amplification factor\n$$\nA(\\varepsilon) \\;=\\; \\sigma^{-2} \\, \\operatorname{tr}\\!\\big(\\operatorname{Cov}[\\hat{\\mathbf{m}}(\\varepsilon)]\\big),\n$$\nas a function of $\\varepsilon$ only.\n\nProvide your final answer by giving the exact closed-form analytic expression for $A(\\varepsilon)$. No numerical rounding is required, and the requested quantity is dimensionless, so no units are to be reported.", "solution": "The problem statement is first subjected to validation.\n\n### Step 1: Extract Givens\n- Data vector: $\\mathbf{d} \\in \\mathbb{R}^{3}$.\n- Model vector: $\\mathbf{m}_{\\mathrm{true}} \\in \\mathbb{R}^{2}$.\n- Linear model: $\\mathbf{d} = G(\\varepsilon) \\, \\mathbf{m}_{\\mathrm{true}} + \\mathbf{n}$.\n- Design matrix: $G(\\varepsilon) = \\begin{pmatrix} 1  1 \\\\ 1  1+\\varepsilon \\\\ 0  \\varepsilon \\end{pmatrix}$.\n- Parameter $\\varepsilon  0$ is small.\n- Noise vector $\\mathbf{n}$ has distribution $\\mathcal{N}(0, \\sigma^{2} I)$, where $\\sigma^{2}  0$ and $I$ is the $3 \\times 3$ identity matrix.\n- The ordinary least squares (OLS) estimate $\\hat{\\mathbf{m}}(\\varepsilon)$ minimizes the cost function $J(\\mathbf{m}) = \\| G(\\varepsilon) \\, \\mathbf{m} - \\mathbf{d} \\|_{2}^{2}$.\n- Task 1: Construct an argument using SVD principles to show that near-rank-deficiency in $G(\\varepsilon)$ leads to nearly equivalent minimizers. Identify a unit direction $\\mathbf{v}_{\\varepsilon}$ where the second directional derivative of $J$ at the minimizer is $O(\\varepsilon^2)$. Produce a pair of models, $\\mathbf{m}_{\\star}$ and $\\mathbf{m}_{\\star} + \\delta \\mathbf{v}_{\\varepsilon}$, whose misfit difference is $O(\\varepsilon^2)$.\n- Task 2: Derive the exact expression for the variance amplification factor $A(\\varepsilon) = \\sigma^{-2} \\, \\operatorname{tr}\\!\\big(\\operatorname{Cov}[\\hat{\\mathbf{m}}(\\varepsilon)]\\big)$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is a well-defined exercise in linear algebra and statistics, specifically as applied to linear inverse theory common in geophysics and other experimental sciences.\n\n- **Scientifically Grounded**: The problem is a canonical example of analyzing an ill-conditioned linear system $G\\mathbf{m}=\\mathbf{d}$. The matrix $G(\\varepsilon)$ represents a simplified but physically plausible scenario in tomography where rays sample two cells with very similar sensitivities, leading to near-linear dependence of the columns of $G$. The analysis of the estimator's covariance is a standard and fundamental part of inverse problem theory.\n- **Well-Posed**: The problem is mathematically well-posed. It provides all necessary definitions and matrices. The tasks are specified clearly, requesting a demonstrative construction and a specific derivation. A unique analytical solution for $A(\\varepsilon)$ exists.\n- **Objective**: The problem is stated in objective, mathematical language, free from ambiguity or subjective content.\n\nAll criteria for a valid problem are met. No flaws are identified.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A full solution will be provided.\n\n### Solution Derivation\n\nThe solution is presented in two parts, corresponding to the two tasks.\n\n#### Part 1: Misfit Landscape and Nearly Equivalent Minimizers\n\nThe cost function to be minimized is the squared $L_2$-norm of the residual vector:\n$$\nJ(\\mathbf{m}) = \\| G(\\varepsilon)\\mathbf{m} - \\mathbf{d} \\|_{2}^{2} = (G(\\varepsilon)\\mathbf{m} - \\mathbf{d})^T (G(\\varepsilon)\\mathbf{m} - \\mathbf{d})\n$$\nThis is a quadratic function of the model parameters $\\mathbf{m}$. The gradient and Hessian of $J(\\mathbf{m})$ are:\n$$\n\\nabla J(\\mathbf{m}) = 2 G(\\varepsilon)^T (G(\\varepsilon)\\mathbf{m} - \\mathbf{d})\n$$\n$$\n\\nabla^2 J(\\mathbf{m}) = 2 G(\\varepsilon)^T G(\\varepsilon)\n$$\nThe Hessian matrix is constant, meaning the misfit surface is a parabolic bowl. The \"flatness\" of this bowl in any direction is determined by the second directional derivative. For a unit direction vector $\\mathbf{v} \\in \\mathbb{R}^2$, the second directional derivative of $J$ at any point $\\mathbf{m}_0$ is given by:\n$$\nD_\\mathbf{v}^2 J(\\mathbf{m}_0) = \\mathbf{v}^T (\\nabla^2 J) \\mathbf{v} = 2 \\mathbf{v}^T G(\\varepsilon)^T G(\\varepsilon) \\mathbf{v} = 2 \\|G(\\varepsilon)\\mathbf{v}\\|_2^2\n$$\nThe problem's statement about a \"long, shallow valley\" corresponds to finding a direction $\\mathbf{v}$ in the model space where this second derivative is very small. This means we are looking for a direction $\\mathbf{v}$ such that $G(\\varepsilon)\\mathbf{v}$ has a small norm. Such a vector $\\mathbf{v}$ is an approximate element of the null space of $G(\\varepsilon)$.\n\nThe singular value decomposition (SVD) of $G(\\varepsilon)$ is $G(\\varepsilon) = U \\Sigma V^T$, where the columns of $V = [\\mathbf{v}_1, \\mathbf{v}_2]$ are the right singular vectors and form an orthonormal basis for the model space $\\mathbb{R}^2$. The singular values $\\sigma_1 \\ge \\sigma_2 \\ge 0$ are the diagonal entries of $\\Sigma$. The action of $G(\\varepsilon)$ on these basis vectors is $G(\\varepsilon)\\mathbf{v}_i = \\sigma_i \\mathbf{u}_i$, where $\\mathbf{u}_i$ is the corresponding left singular vector.\n\nThe second directional derivative in the direction of a singular vector $\\mathbf{v}_i$ is:\n$$\nD_{\\mathbf{v}_i}^2 J = 2 \\|G(\\varepsilon)\\mathbf{v}_i\\|_2^2 = 2 \\|\\sigma_i \\mathbf{u}_i\\|_2^2 = 2 \\sigma_i^2 \\|\\mathbf{u}_i\\|_2^2 = 2 \\sigma_i^2\n$$\nThe direction of the shallowest valley is therefore the right singular vector $\\mathbf{v}_2$ corresponding to the smallest singular value $\\sigma_2$. The flatness is proportional to $\\sigma_2^2$.\n\nLet's analyze $G(\\varepsilon)$ as $\\varepsilon \\to 0$. The columns are $\\mathbf{g}_1 = \\begin{pmatrix} 1  1  0 \\end{pmatrix}^T$ and $\\mathbf{g}_2 = \\begin{pmatrix} 1  1+\\varepsilon  \\varepsilon \\end{pmatrix}^T$. As $\\varepsilon \\to 0$, $\\mathbf{g}_2 \\to \\mathbf{g}_1$, so the columns become linearly dependent. A vector in the approximate null space would satisfy $m_1 \\mathbf{g}_1 + m_2 \\mathbf{g}_2 \\approx \\mathbf{0}$, which for small $\\varepsilon$ implies $m_1 \\approx -m_2$. This suggests the direction is close to $(1, -1)^T$.\n\nLet's test the unit vector $\\mathbf{v} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1  -1 \\end{pmatrix}^T$. Note that this vector is independent of $\\varepsilon$.\n$$\nG(\\varepsilon)\\mathbf{v} = \\begin{pmatrix} 1  1 \\\\ 1  1+\\varepsilon \\\\ 0  \\varepsilon \\end{pmatrix} \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1-1 \\\\ 1-(1+\\varepsilon) \\\\ 0-\\varepsilon \\end{pmatrix} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 0 \\\\ -\\varepsilon \\\\ -\\varepsilon \\end{pmatrix}\n$$\nThe squared norm is:\n$$\n\\|G(\\varepsilon)\\mathbf{v}\\|_2^2 = \\left(\\frac{1}{\\sqrt{2}}\\right)^2 (0^2 + (-\\varepsilon)^2 + (-\\varepsilon)^2) = \\frac{1}{2} (2\\varepsilon^2) = \\varepsilon^2\n$$\nThe second directional derivative in this direction is $D_\\mathbf{v}^2 J = 2\\varepsilon^2$. This is of order $O(\\varepsilon^2)$, as required. So, we can choose $\\mathbf{v}_{\\varepsilon} = \\mathbf{v} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1  -1 \\end{pmatrix}^T$.\n\nNow, let's construct the pair of models. The OLS minimizer $\\hat{\\mathbf{m}}(\\varepsilon)$ satisfies $\\nabla J(\\hat{\\mathbf{m}}) = \\mathbf{0}$. Consider a Taylor expansion of $J(\\mathbf{m})$ around $\\hat{\\mathbf{m}}(\\varepsilon)$:\n$$\nJ(\\mathbf{m}) = J(\\hat{\\mathbf{m}}) + \\nabla J(\\hat{\\mathbf{m}})^T (\\mathbf{m}-\\hat{\\mathbf{m}}) + \\frac{1}{2} (\\mathbf{m}-\\hat{\\mathbf{m}})^T (\\nabla^2 J) (\\mathbf{m}-\\hat{\\mathbf{m}}) + \\dots\n$$\nSince the Hessian is constant, the expansion is exact. With $\\nabla J(\\hat{\\mathbf{m}}) = \\mathbf{0}$:\n$$\nJ(\\mathbf{m}) - J(\\hat{\\mathbf{m}}) = \\frac{1}{2} (\\mathbf{m}-\\hat{\\mathbf{m}})^T (2 G^T G) (\\mathbf{m}-\\hat{\\mathbf{m}}) = (\\mathbf{m}-\\hat{\\mathbf{m}})^T G^T G (\\mathbf{m}-\\hat{\\mathbf{m}}) = \\|G(\\mathbf{m}-\\hat{\\mathbf{m}})\\|_2^2\n$$\nLet's choose our pair of models as $\\mathbf{m}_{\\star} = \\hat{\\mathbf{m}}(\\varepsilon)$ and the perturbed model $\\mathbf{m}_{\\star} + \\delta \\mathbf{v}_{\\varepsilon}$, where $\\delta$ is a scalar constant independent of $\\varepsilon$, and $\\mathbf{v}_{\\varepsilon} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1  -1 \\end{pmatrix}^T$.\nThe difference in their misfits is:\n$$\nJ(\\mathbf{m}_{\\star} + \\delta \\mathbf{v}_{\\varepsilon}) - J(\\mathbf{m}_{\\star}) = J(\\hat{\\mathbf{m}} + \\delta \\mathbf{v}_{\\varepsilon}) - J(\\hat{\\mathbf{m}}) = \\|G(\\delta \\mathbf{v}_{\\varepsilon})\\|_2^2 = \\delta^2 \\|G \\mathbf{v}_{\\varepsilon}\\|_2^2\n$$\nUsing our previously calculated result $\\|G \\mathbf{v}_{\\varepsilon}\\|_2^2 = \\varepsilon^2$, we have:\n$$\nJ(\\mathbf{m}_{\\star} + \\delta \\mathbf{v}_{\\varepsilon}) - J(\\mathbf{m}_{\\star}) = \\delta^2 \\varepsilon^2\n$$\nThis difference is of order $O(\\varepsilon^2)$. Thus, for a fixed model perturbation of size $\\delta$, the increase in misfit is vanishingly small as $\\varepsilon \\to 0$. This confirms the existence of a long, shallow valley of nearly equivalent minimizers along the direction $\\mathbf{v}_{\\varepsilon}$.\n\n#### Part 2: Variance Amplification Factor\n\nThe dimensionless variance amplification factor is $A(\\varepsilon) = \\sigma^{-2} \\, \\operatorname{tr}\\!\\big(\\operatorname{Cov}[\\hat{\\mathbf{m}}(\\varepsilon)]\\big)$. We must first derive the covariance matrix of the OLS estimator, $\\operatorname{Cov}[\\hat{\\mathbf{m}}(\\varepsilon)]$. For brevity, we will write $\\hat{\\mathbf{m}}$ for $\\hat{\\mathbf{m}}(\\varepsilon)$ and $G$ for $G(\\varepsilon)$.\n\nThe OLS estimator is given by the normal equations:\n$$\n\\hat{\\mathbf{m}} = (G^T G)^{-1} G^T \\mathbf{d}\n$$\nWe substitute the model for the data, $\\mathbf{d} = G \\mathbf{m}_{\\mathrm{true}} + \\mathbf{n}$:\n$$\n\\hat{\\mathbf{m}} = (G^T G)^{-1} G^T (G \\mathbf{m}_{\\mathrm{true}} + \\mathbf{n}) = (G^T G)^{-1} G^T G \\mathbf{m}_{\\mathrm{true}} + (G^T G)^{-1} G^T \\mathbf{n} = \\mathbf{m}_{\\mathrm{true}} + (G^T G)^{-1} G^T \\mathbf{n}\n$$\nThe expectation of the estimator is:\n$$\nE[\\hat{\\mathbf{m}}] = E[\\mathbf{m}_{\\mathrm{true}} + (G^T G)^{-1} G^T \\mathbf{n}] = \\mathbf{m}_{\\mathrm{true}} + (G^T G)^{-1} G^T E[\\mathbf{n}]\n$$\nSince the noise is zero-mean ($E[\\mathbf{n}]=\\mathbf{0}$), the estimator is unbiased: $E[\\hat{\\mathbf{m}}] = \\mathbf{m}_{\\mathrm{true}}$.\n\nThe covariance matrix of the estimator is defined as $\\operatorname{Cov}[\\hat{\\mathbf{m}}] = E[(\\hat{\\mathbf{m}} - E[\\hat{\\mathbf{m}}])(\\hat{\\mathbf{m}} - E[\\hat{\\mathbf{m}}])^T]$.\nUsing the expressions above:\n$$\n\\hat{\\mathbf{m}} - E[\\hat{\\mathbf{m}}] = (G^T G)^{-1} G^T \\mathbf{n}\n$$\nSo,\n$$\n\\operatorname{Cov}[\\hat{\\mathbf{m}}] = E\\Big[ \\big( (G^T G)^{-1} G^T \\mathbf{n} \\big) \\big( (G^T G)^{-1} G^T \\mathbf{n} \\big)^T \\Big] = E\\Big[ (G^T G)^{-1} G^T \\mathbf{n} \\mathbf{n}^T G (G^T G)^{-T} \\Big]\n$$\nSince $G$ is a matrix of constants with respect to the expectation, and $(G^T G)^T = G^T G$, we can write:\n$$\n\\operatorname{Cov}[\\hat{\\mathbf{m}}] = (G^T G)^{-1} G^T E[\\mathbf{n} \\mathbf{n}^T] G (G^T G)^{-1}\n$$\nThe term $E[\\mathbf{n} \\mathbf{n}^T]$ is the covariance matrix of the noise, $\\operatorname{Cov}[\\mathbf{n}]$. We are given that $\\mathbf{n} \\sim \\mathcal{N}(0, \\sigma^2 I)$, so $\\operatorname{Cov}[\\mathbf{n}] = \\sigma^2 I$.\n$$\n\\operatorname{Cov}[\\hat{\\mathbf{m}}] = (G^T G)^{-1} G^T (\\sigma^2 I) G (G^T G)^{-1} = \\sigma^2 (G^T G)^{-1} (G^T G) (G^T G)^{-1} = \\sigma^2 (G^T G)^{-1}\n$$\nNow we can express the amplification factor $A(\\varepsilon)$:\n$$\nA(\\varepsilon) = \\sigma^{-2} \\operatorname{tr}(\\operatorname{Cov}[\\hat{\\mathbf{m}}]) = \\sigma^{-2} \\operatorname{tr}(\\sigma^2 (G^T G)^{-1}) = \\operatorname{tr}((G^T G)^{-1})\n$$\nTo compute this, we first find the matrix $G^T G$:\n$$\nG(\\varepsilon)^T G(\\varepsilon) = \\begin{pmatrix} 1  1  0 \\\\ 1  1+\\varepsilon  \\varepsilon \\end{pmatrix} \\begin{pmatrix} 1  1 \\\\ 1  1+\\varepsilon \\\\ 0  \\varepsilon \\end{pmatrix} = \\begin{pmatrix} 1+1  1+(1+\\varepsilon) \\\\ 1+(1+\\varepsilon)  1+(1+\\varepsilon)^2+\\varepsilon^2 \\end{pmatrix} = \\begin{pmatrix} 2  2+\\varepsilon \\\\ 2+\\varepsilon  2+2\\varepsilon+2\\varepsilon^2 \\end{pmatrix}\n$$\nFor a general invertible $2 \\times 2$ matrix $M = \\begin{pmatrix} a  b \\\\ c  d \\end{pmatrix}$, its inverse is $M^{-1} = \\frac{1}{\\det(M)} \\begin{pmatrix} d  -b \\\\ -c  a \\end{pmatrix}$. The trace of the inverse is $\\operatorname{tr}(M^{-1}) = \\frac{a+d}{\\det(M)} = \\frac{\\operatorname{tr}(M)}{\\det(M)}$.\nApplying this property to $M = G^T G$:\n$$\n\\operatorname{tr}((G^T G)^{-1}) = \\frac{\\operatorname{tr}(G^T G)}{\\det(G^T G)}\n$$\nWe compute the trace and determinant of $G^T G$:\n$$\n\\operatorname{tr}(G^T G) = 2 + (2+2\\varepsilon+2\\varepsilon^2) = 4+2\\varepsilon+2\\varepsilon^2\n$$\n$$\n\\det(G^T G) = 2(2+2\\varepsilon+2\\varepsilon^2) - (2+\\varepsilon)^2 = (4+4\\varepsilon+4\\varepsilon^2) - (4+4\\varepsilon+\\varepsilon^2) = 3\\varepsilon^2\n$$\nSubstituting these into the expression for $A(\\varepsilon)$:\n$$\nA(\\varepsilon) = \\frac{4+2\\varepsilon+2\\varepsilon^2}{3\\varepsilon^2}\n$$\nThis is the exact closed-form analytic expression for the dimensionless variance amplification factor.", "answer": "$$\n\\boxed{\\frac{4+2\\varepsilon+2\\varepsilon^{2}}{3\\varepsilon^{2}}}\n$$", "id": "3606781"}, {"introduction": "This final practice integrates the concepts of leverage and solution stability into a realistic computational simulation, exploring the classic bias-variance trade-off. You will write code to model a geophysical experiment, identify high-leverage measurements, and then use Monte Carlo methods to quantify how removing these points affects the statistical properties of your solution. This exercise demonstrates that while removing influential data can reduce the variance of the model estimate, it may come at the cost of increasing systematic bias, a crucial consideration in any real-world data analysis workflow [@problem_id:3606791].", "problem": "Consider a one-dimensional vertical travel-time experiment in a layered medium, in which the travel-time from the surface to a receiver at depth $z$ equals the line integral of slowness along the path. The fundamental base is the path integral definition of travel-time, namely \n$$\nT(z) \\equiv \\int_{0}^{z} s(\\zeta)\\, d\\zeta,\n$$\nwhere $s(\\zeta)$ is the slowness field in seconds per meter. In computational geophysics, it is standard to discretize the unknown slowness as piecewise constant by layers, yielding a linear model $\\mathbf{t} \\approx A \\mathbf{m}$, where $\\mathbf{t} \\in \\mathbb{R}^{R}$ is the vector of measured travel-times for $R$ receivers, $A \\in \\mathbb{R}^{R \\times L}$ is the design matrix of path lengths per layer, and $\\mathbf{m} \\in \\mathbb{R}^{L}$ is the vector of layer slownesses (seconds per meter). The least-squares solution for an overdetermined system is the minimizer of $\\|A \\mathbf{m} - \\mathbf{t}\\|_2^2$, producing the estimate \n$$\n\\hat{\\mathbf{m}} = (A^{\\top} A)^{-1} A^{\\top} \\mathbf{t},\n$$\nwhen $A^{\\top} A$ is invertible. The leverage scores quantify the sensitivity of fitted values to observations and are given by\n$$\n\\ell_i = \\mathbf{a}_i^{\\top} (A^{\\top} A)^{-1} \\mathbf{a}_i,\n$$\nwhere $\\mathbf{a}_i^{\\top}$ is the $i$-th row of $A$.\n\nYou will construct $A$ for a vertical experiment with straight paths from the surface to receivers and a layered parameterization. Let there be $L$ layers with thicknesses $h_j$ for $j=1,\\dots,L$, with the top of layer $j$ at depth $z_{j}^{\\mathrm{top}}$ and bottom at $z_{j}^{\\mathrm{bot}}$. For a receiver at depth $z_i$, the path length through layer $j$ is $\\ell_{ij}^{\\mathrm{path}} = \\max\\big(0, \\min(z_i, z_{j}^{\\mathrm{bot}}) - z_{j}^{\\mathrm{top}}\\big)$, and thus $A_{ij} = \\ell_{ij}^{\\mathrm{path}}$. Let the continuous slowness be \n$$\ns(z) = s_0 + g z,\n$$\nwith $s_0$ and $g$ constants, so the noise-free travel-time is \n$$\nT_i = T(z_i) = \\int_0^{z_i} (s_0 + g \\zeta) \\, d\\zeta = s_0 z_i + \\tfrac{1}{2} g z_i^2.\n$$\nDefine the layer-average target slowness (the piecewise constant coarse-scale truth) as \n$$\nm^{\\mathrm{coarse}}_j = \\frac{1}{h_j} \\int_{z_{j}^{\\mathrm{top}}}^{z_{j}^{\\mathrm{bot}}} s(z) \\, dz = s_0 + g \\frac{z_{j}^{\\mathrm{top}} + z_{j}^{\\mathrm{bot}}}{2}.\n$$\nYou will simulate additive noise on each datum with heteroscedastic variance linked to leverage, \n$$\n\\varepsilon_i \\sim \\mathcal{N}\\Big(0, \\sigma^2 \\big(1 + \\gamma \\, \\ell_i\\big)\\Big),\n$$\nto reflect higher picking uncertainty on high-leverage picks in realistic travel-time datasets. Form noisy data as $t_i = T_i + \\varepsilon_i$ and estimate $\\hat{\\mathbf{m}}$ using ordinary least squares on the full dataset and on a dataset where the highest-leverage picks have been removed.\n\nYour tasks:\n- Compute leverage scores $\\ell_i$ for all rows of $A$.\n- Identify high-leverage picks using a specified removal fraction $p$: remove the top $\\lceil p R \\rceil$ rows ranked by $\\ell_i$.\n- Perform $K$ Monte Carlo replicates of the data simulation and least-squares inversion for both the full dataset and the dataset with high-leverage picks removed.\n- Quantify the variance of the estimated parameters by the sum of componentwise sample variances, \n$$\n\\mathrm{VarSum} = \\sum_{j=1}^{L} \\widehat{\\mathrm{Var}}(\\hat{m}_j),\n$$\nin units of $(\\mathrm{s}/\\mathrm{m})^2$.\n- Quantify the bias relative to the coarse-scale truth by the Euclidean norm of the mean estimator error, \n$$\n\\mathrm{BiasNorm} = \\left\\| \\mathbb{E}[\\hat{\\mathbf{m}}] - \\mathbf{m}^{\\mathrm{coarse}} \\right\\|_2,\n$$\nin units of $\\mathrm{s}/\\mathrm{m}$, approximated by the sample mean over replicates.\n- Demonstrate, by simulation, that removing high-leverage picks reduces $\\mathrm{VarSum}$ while increasing $\\mathrm{BiasNorm}$.\n\nUse the following scientifically plausible configuration and test suite. All quantities must be expressed in the International System of Units (SI). Slowness is in seconds per meter, depths and thicknesses are in meters, and travel-times are in seconds. Angles do not appear. The test suite specifies three cases with different noise scales and removal fractions:\n- Geometry and physics shared across cases:\n  - Layers: $L = 5$ with thicknesses $h = [200, 200, 200, 200, 200]$ meters.\n  - Receiver depths: $R = 25$ receivers at depths $z_i$ uniformly spaced from $40$ meters to $1000$ meters.\n  - Continuous slowness: $s_0 = 5 \\times 10^{-4}$ seconds per meter, $g = 1 \\times 10^{-7}$ seconds per meter squared.\n  - Replicates: $K = 400$.\n- Case $1$ (happy path): noise scale $\\sigma = 1 \\times 10^{-4}$ seconds, heteroscedastic factor $\\gamma = 20$, removal fraction $p = 0.20$.\n- Case $2$ (boundary condition: minimal removal): noise scale $\\sigma = 5 \\times 10^{-5}$ seconds, heteroscedastic factor $\\gamma = 10$, removal fraction $p = 0.04$.\n- Case $3$ (edge case: strong heteroscedasticity): noise scale $\\sigma = 2 \\times 10^{-4}$ seconds, heteroscedastic factor $\\gamma = 30$, removal fraction $p = 0.30$.\n\nFor each case, compute:\n- $\\mathrm{VarSum}_{\\mathrm{full}}$ and $\\mathrm{VarSum}_{\\mathrm{removed}}$ in $(\\mathrm{s}/\\mathrm{m})^2$.\n- $\\mathrm{BiasNorm}_{\\mathrm{full}}$ and $\\mathrm{BiasNorm}_{\\mathrm{removed}}$ in $\\mathrm{s}/\\mathrm{m}$.\n- The number of removed picks $N_{\\mathrm{removed}} = \\lceil p R \\rceil$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to a test case and is itself a list \n$$\n[\\mathrm{VarSum}_{\\mathrm{full}}, \\mathrm{VarSum}_{\\mathrm{removed}}, \\mathrm{BiasNorm}_{\\mathrm{full}}, \\mathrm{BiasNorm}_{\\mathrm{removed}}, N_{\\mathrm{removed}}].\n$$\nFor example, the output will look like \n`[[x_1,x_2,x_3,x_4,n_1],[y_1,y_2,y_3,y_4,n_2],[z_1,z_2,z_3,z_4,n_3]]`,\nwith all $x_k$, $y_k$, $z_k$ as floating-point numbers and $n_k$ as integers, reported in the specified SI units. No additional text should be printed.", "solution": "We begin from the path integral law for travel-time in a continuously varying medium, which states that the travel-time $T(z)$ to a receiver at depth $z$ equals the integral of slowness along the path, $T(z) = \\int_{0}^{z} s(\\zeta) \\, d\\zeta$, where $s(\\zeta)$ is slowness in seconds per meter. For a layered discretization with piecewise constant slowness $m_j$ in layer $j$, and for vertical paths from the surface, the measured travel-time to receiver $i$ is modeled as the linear combination $t_i \\approx \\sum_{j=1}^{L} A_{ij} m_j$, where $A_{ij}$ is the path length through layer $j$ for the path to receiver $i$. Because the path is vertical and straight, $A_{ij}$ is the intersection length of $[0, z_i]$ with the interval $[z_j^{\\mathrm{top}}, z_j^{\\mathrm{bot}}]$, namely $A_{ij} = \\max\\big(0, \\min(z_i, z_j^{\\mathrm{bot}}) - z_j^{\\mathrm{top}}\\big)$. Stacking all receivers produces $\\mathbf{t} \\in \\mathbb{R}^{R}$ and $A \\in \\mathbb{R}^{R \\times L}$.\n\nThe least-squares solution is derived by minimizing the quadratic objective $J(\\mathbf{m}) = \\|A \\mathbf{m} - \\mathbf{t}\\|_2^2$, whose gradient with respect to $\\mathbf{m}$ is $\\nabla J(\\mathbf{m}) = 2 A^{\\top} (A \\mathbf{m} - \\mathbf{t})$. Setting the gradient to zero yields the normal equations $A^{\\top} A \\, \\hat{\\mathbf{m}} = A^{\\top} \\mathbf{t}$. When $A^{\\top} A$ is invertible, the unique minimizer is $\\hat{\\mathbf{m}} = (A^{\\top} A)^{-1} A^{\\top} \\mathbf{t}$. This estimator is unbiased when the linear model is correctly specified and the noise has zero mean. However, in computational geophysics it is common to have model mis-specification due to coarse parameterization of a continuous field. In our setup, the continuous slowness is $s(z) = s_0 + g z$, so the noise-free travel-time is $T(z) = s_0 z + \\frac{1}{2} g z^2$. The coarse-scale layer-averaged slowness (the physical target) is $\\mathbf{m}^{\\mathrm{coarse}}_j = \\frac{1}{h_j} \\int_{z_j^{\\mathrm{top}}}^{z_j^{\\mathrm{bot}}} (s_0 + g z) \\, dz = s_0 + g \\, \\frac{z_j^{\\mathrm{top}} + z_j^{\\mathrm{bot}}}{2}$.\n\nBecause the continuous travel-time $\\mathbf{T}$ is not exactly in the column space of $A \\mathbf{m}$ with constant $\\mathbf{m}$ per layer (unless $g = 0$), the least-squares solution under noise-free data returns a pseudo-true parameter $\\mathbf{m}^{\\star}$ that solves $A^{\\top} A \\, \\mathbf{m}^{\\star} = A^{\\top} \\mathbf{T}$, which generally differs from $\\mathbf{m}^{\\mathrm{coarse}}$. This difference is the structural bias attributable to the discretization and the sampling by receivers. The leverage score for each observation quantifies its influence on the fitted value and is defined for the $i$-th row $\\mathbf{a}_i^{\\top}$ of $A$ as $\\ell_i = \\mathbf{a}_i^{\\top} (A^{\\top} A)^{-1} \\mathbf{a}_i$. The diagonal of the projection (hat) matrix $H = A (A^{\\top} A)^{-1} A^{\\top}$ contains these scores, and high-leverage rows are those with large $\\ell_i$ relative to others. In travel-time inversion, deeper receivers have longer paths and can be high leverage because their $\\mathbf{a}_i$ vectors have large magnitudes and distinct directional contributions across layers.\n\nTo connect leverage to uncertainty, we consider heteroscedastic picking noise whose variance increases with leverage: $\\varepsilon_i \\sim \\mathcal{N}\\big(0, \\sigma^2 (1 + \\gamma \\ell_i)\\big)$. The estimator under ordinary least squares remains unbiased for the linear model when $\\mathbf{T}$ exactly equals $A \\mathbf{m}$ on average, but with structural mis-specification ($\\mathbf{T} \\neq A \\mathbf{m}$ for any $\\mathbf{m}$) the expected estimator equals $\\mathbf{m}^{\\star}$, not $\\mathbf{m}^{\\mathrm{coarse}}$. Furthermore, heavy-tailed or heteroscedastic noise inflates the sampling variance of $\\hat{\\mathbf{m}}$, especially due to high-leverage observations. Thus, removing high-leverage rows can reduce the variance of $\\hat{\\mathbf{m}}$ because it eliminates observations with both large leverage and large noise variance; however, removal reduces the diversity of the design matrix $A$, typically worsening the structural bias by changing $A^{\\top} \\mathbf{T}$ and $A^{\\top} A$ in ways that reduce sensitivity to deeper layers. This trade-off is the classical variance–bias interplay in overdetermined least-squares with mis-specified models.\n\nAlgorithmic steps:\n1. Construct layer depths for $L = 5$ with thicknesses $h_j = 200$ meters each, so $z_1^{\\mathrm{top}} = 0$, $z_1^{\\mathrm{bot}} = 200$, $z_2^{\\mathrm{top}} = 200$, and so on up to $z_5^{\\mathrm{bot}} = 1000$ meters. Construct $R = 25$ receiver depths $z_i$ uniformly from $40$ meters to $1000$ meters.\n2. Build $A$ with entries $A_{ij} = \\max\\big(0, \\min(z_i, z_j^{\\mathrm{bot}}) - z_j^{\\mathrm{top}}\\big)$.\n3. Compute leverage scores $\\ell_i = \\mathbf{a}_i^{\\top} (A^{\\top} A)^{-1} \\mathbf{a}_i$ for all rows.\n4. For each test case parameters $(\\sigma, \\gamma, p)$, determine $N_{\\mathrm{removed}} = \\lceil p R \\rceil$ and remove the rows with the largest $\\ell_i$ to create $A_{\\mathrm{removed}}$ and the corresponding index set.\n5. For $K = 400$ Monte Carlo replicates, generate noisy data $\\mathbf{t} = \\mathbf{T} + \\boldsymbol{\\varepsilon}$ for both the full and removed-row datasets (reusing the same realization of $\\boldsymbol{\\varepsilon}$ on the full set and sub-selecting it for the removed set). Fit $\\hat{\\mathbf{m}}_{\\mathrm{full}} = (A^{\\top} A)^{-1} A^{\\top} \\mathbf{t}$ and $\\hat{\\mathbf{m}}_{\\mathrm{removed}} = (A_{\\mathrm{removed}}^{\\top} A_{\\mathrm{removed}})^{-1} A_{\\mathrm{removed}}^{\\top} \\mathbf{t}_{\\mathrm{removed}}$.\n6. Compute sample variances across replicates for each component, summing them to obtain $\\mathrm{VarSum}_{\\mathrm{full}}$ and $\\mathrm{VarSum}_{\\mathrm{removed}}$ in $(\\mathrm{s}/\\mathrm{m})^2$.\n7. Compute the sample mean of $\\hat{\\mathbf{m}}$ across replicates for full and removed sets and the Euclidean norm of the difference from $\\mathbf{m}^{\\mathrm{coarse}}$ to obtain $\\mathrm{BiasNorm}_{\\mathrm{full}}$ and $\\mathrm{BiasNorm}_{\\mathrm{removed}}$ in $\\mathrm{s}/\\mathrm{m}$.\n\nTest suite details:\n- Shared geometry and physics:\n  - $L = 5$, $h = [200, 200, 200, 200, 200]$ meters.\n  - $R = 25$, $z_i$ uniform from $40$ meters to $1000$ meters.\n  - $s_0 = 5 \\times 10^{-4}$ seconds per meter, $g = 1 \\times 10^{-7}$ seconds per meter squared.\n  - $K = 400$ replicates.\n- Case $1$: $\\sigma = 1 \\times 10^{-4}$ seconds, $\\gamma = 20$, $p = 0.20$.\n- Case $2$: $\\sigma = 5 \\times 10^{-5}$ seconds, $\\gamma = 10$, $p = 0.04$.\n- Case $3$: $\\sigma = 2 \\times 10^{-4}$ seconds, $\\gamma = 30$, $p = 0.30$.\n\nFinal output specification:\nYour program should produce a single line as \n$[[\\mathrm{VarSum}_{\\mathrm{full}},\\mathrm{VarSum}_{\\mathrm{removed}},\\mathrm{BiasNorm}_{\\mathrm{full}},\\mathrm{BiasNorm}_{\\mathrm{removed}},N_{\\mathrm{removed}}],\\dots]$\nfor the three cases, in the order Case $1$, Case $2$, Case $3$. All values must be in the SI units indicated above, reported as floating-point numbers for variance and bias, and integers for the count $N_{\\mathrm{removed}}$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef build_layers(thicknesses):\n    \"\"\"Return arrays of layer top and bottom depths.\"\"\"\n    z_top = np.cumsum([0] + thicknesses[:-1])\n    z_bot = np.cumsum(thicknesses)\n    return z_top.astype(float), z_bot.astype(float)\n\ndef build_design_matrix(z_receivers, z_top, z_bot):\n    \"\"\"\n    Build A with A[i, j] = intersection length of [0, z_i] with [z_top[j], z_bot[j]].\n    \"\"\"\n    R = len(z_receivers)\n    L = len(z_top)\n    A = np.zeros((R, L), dtype=float)\n    for i, z in enumerate(z_receivers):\n        # intersection length: max(0, min(z, z_bot[j]) - z_top[j])\n        A[i, :] = np.maximum(0.0, np.minimum(z, z_bot) - z_top)\n    return A\n\ndef true_travel_times(z_receivers, s0, g):\n    \"\"\"Compute T_i = s0*z_i + 0.5*g*z_i^2.\"\"\"\n    z = np.asarray(z_receivers, dtype=float)\n    return s0 * z + 0.5 * g * z**2\n\ndef coarse_layer_slowness(z_top, z_bot, s0, g):\n    \"\"\"Compute m_coarse_j = s0 + g * (z_top_j + z_bot_j)/2.\"\"\"\n    return s0 + g * (z_top + z_bot) / 2.0\n\ndef leverage_scores(A):\n    \"\"\"Compute leverage scores l_i = a_i^T (A^T A)^{-1} a_i.\"\"\"\n    ATA = A.T @ A\n    # Use a robust inverse; ATA should be SPD; fallback to pseudo-inverse for numerical robustness\n    ATA_inv = np.linalg.pinv(ATA)\n    # row-wise leverage: diag(A @ ATA_inv @ A^T)\n    H = A @ ATA_inv @ A.T\n    return np.diag(H)\n\ndef ls_estimate(A, t):\n    \"\"\"Ordinary least-squares estimate (A^T A)^{-1} A^T t.\"\"\"\n    ATA = A.T @ A\n    ATt = A.T @ t\n    # Use pseudo-inverse for robustness\n    m_hat = np.linalg.pinv(ATA) @ ATt\n    return m_hat\n\ndef simulate_case(A, T_true, leverages, m_coarse, sigma, gamma, frac_remove, K, rng):\n    \"\"\"\n    Simulate K replicates for full and removed datasets.\n    Returns VarSum_full, VarSum_removed, BiasNorm_full, BiasNorm_removed, N_removed.\n    \"\"\"\n    R = A.shape[0]\n    L = A.shape[1]\n    # Determine removal set: top ceil(frac_remove * R) by leverage\n    N_removed = int(np.ceil(frac_remove * R))\n    order = np.argsort(-leverages)  # descending by leverage\n    remove_idx = order[:N_removed]\n    keep_mask = np.ones(R, dtype=bool)\n    keep_mask[remove_idx] = False\n    A_removed = A[keep_mask, :]\n\n    # Preallocate arrays to store estimates\n    m_full_all = np.zeros((K, L), dtype=float)\n    m_removed_all = np.zeros((K, L), dtype=float)\n\n    # Noise std per observation (heteroscedastic)\n    std_i = sigma * np.sqrt(1.0 + gamma * leverages)\n\n    for k in range(K):\n        eps = rng.normal(loc=0.0, scale=std_i, size=R)\n        t_noisy = T_true + eps\n        # Full estimate\n        m_full = ls_estimate(A, t_noisy)\n        m_full_all[k, :] = m_full\n        # Removed estimate\n        t_removed = t_noisy[keep_mask]\n        m_removed = ls_estimate(A_removed, t_removed)\n        m_removed_all[k, :] = m_removed\n\n    # Variance sums across components\n    var_full = m_full_all.var(axis=0, ddof=1)  # sample variance per component\n    var_removed = m_removed_all.var(axis=0, ddof=1)\n    VarSum_full = float(var_full.sum())\n    VarSum_removed = float(var_removed.sum())\n\n    # Bias norms: norm of mean(m_hat) - m_coarse\n    mean_full = m_full_all.mean(axis=0)\n    mean_removed = m_removed_all.mean(axis=0)\n    BiasNorm_full = float(np.linalg.norm(mean_full - m_coarse))\n    BiasNorm_removed = float(np.linalg.norm(mean_removed - m_coarse))\n\n    return VarSum_full, VarSum_removed, BiasNorm_full, BiasNorm_removed, N_removed\n\ndef solve():\n    # Geometry and physics shared across cases\n    thicknesses = [200, 200, 200, 200, 200]  # meters\n    L = len(thicknesses)\n    z_top, z_bot = build_layers(thicknesses)\n    # Receiver depths: 25 receivers from 40 m to 1000 m\n    R = 25\n    z_receivers = np.linspace(40.0, 1000.0, R)\n    # Continuous slowness parameters\n    s0 = 5e-4  # s/m\n    g = 1e-7   # s/m^2\n    # True travel-times and coarse slowness\n    T_true = true_travel_times(z_receivers, s0, g)\n    m_coarse = coarse_layer_slowness(z_top, z_bot, s0, g)\n    # Design matrix\n    A = build_design_matrix(z_receivers, z_top, z_bot)\n    # Leverage scores\n    leverages = leverage_scores(A)\n\n    # Replicates\n    K = 400\n    rng = np.random.default_rng(seed=42)\n\n    # Test cases: (sigma, gamma, frac_remove)\n    test_cases = [\n        (1e-4, 20.0, 0.20),  # Case 1: happy path\n        (5e-5, 10.0, 0.04),  # Case 2: boundary minimal removal\n        (2e-4, 30.0, 0.30),  # Case 3: strong heteroscedasticity\n    ]\n\n    results = []\n    for sigma, gamma, frac_remove in test_cases:\n        VarSum_full, VarSum_removed, BiasNorm_full, BiasNorm_removed, N_removed = simulate_case(\n            A, T_true, leverages, m_coarse, sigma, gamma, frac_remove, K, rng\n        )\n        results.append([VarSum_full, VarSum_removed, BiasNorm_full, BiasNorm_removed, N_removed])\n\n    # Final print statement in the exact required format.\n    print(str(results))\n\nsolve()\n```", "id": "3606791"}]}