{"hands_on_practices": [{"introduction": "The L-curve method provides a powerful visual heuristic for selecting the regularization parameter, $\\lambda$, by identifying the \"corner\" of the trade-off curve. To move from a qualitative visual inspection to a quantitative method, we must precisely define this corner as the point of maximum curvature. This foundational exercise guides you through the derivation of the standard formula for the curvature of a parametric curve, starting from first principles of vector calculus [@problem_id:3613564]. Mastering this derivation ensures you have a robust mathematical understanding of the tool you are using to objectively locate the L-curve's optimal point.", "problem": "In deterministic linear inverse problems in computational geophysics, Tikhonov regularization balances data fidelity and model complexity by minimizing an objective that depends on a regularization parameter $\\lambda$. For a solution $\\mathbf{m}_{\\lambda}$, define the residual norm $\\rho(\\lambda)=\\|\\mathbf{G}\\mathbf{m}_{\\lambda}-\\mathbf{d}\\|_{2}$ and the seminorm $\\eta(\\lambda)=\\|\\mathbf{L}\\mathbf{m}_{\\lambda}\\|_{2}$, where $\\mathbf{G}$ is the forward operator, $\\mathbf{L}$ is the regularization operator, and $\\mathbf{d}$ is the data vector. The trade-off behavior as $\\lambda$ varies is summarized by the L-curve, the parametric curve in the plane\n$$\n\\mathbf{r}(\\lambda)=\\big(x(\\lambda),y(\\lambda)\\big)=\\big(\\ln \\rho(\\lambda),\\ln \\eta(\\lambda)\\big).\n$$\nAssume that $\\rho(\\lambda)>0$ and $\\eta(\\lambda)>0$ for all $\\lambda$ in an open interval $\\mathcal{I}\\subset\\mathbb{R}$, and that $x(\\lambda)$ and $y(\\lambda)$ are twice continuously differentiable on $\\mathcal{I}$. Let $x'(\\lambda)=\\frac{d x}{d\\lambda}$ and $y'(\\lambda)=\\frac{d y}{d\\lambda}$ denote first derivatives with respect to $\\lambda$, and $x''(\\lambda)=\\frac{d^{2} x}{d\\lambda^{2}}$ and $y''(\\lambda)=\\frac{d^{2} y}{d\\lambda^{2}}$ denote second derivatives. Assume $\\big(x'(\\lambda),y'(\\lambda)\\big)\\neq(0,0)$ on $\\mathcal{I}$ so that the curve has a well-defined tangent everywhere.\n\nStarting from the definition of curvature for a planar curve parameterized by $\\lambda$ as the magnitude of the rate of change of the unit tangent vector with respect to arc length, namely $\\kappa=\\left\\|\\frac{d\\hat{\\mathbf{t}}}{ds}\\right\\|$, where $s$ is the arc length and $\\hat{\\mathbf{t}}=\\frac{d\\mathbf{r}}{ds}$ is the unit tangent vector, derive a closed-form expression for the curvature $\\kappa(\\lambda)$ of the L-curve in terms of $x'(\\lambda)$, $y'(\\lambda)$, $x''(\\lambda)$, and $y''(\\lambda)$ only. Your final answer must be a single analytic expression involving only these derivatives, absolute values, sums, products, and integer powers. No numerical evaluation is required and no rounding is needed. Do not introduce any additional parameters or quantities in your final expression, and do not use any shortcut formulas not derived from the stated definition.", "solution": "The problem requires the derivation of a closed-form expression for the curvature, $\\kappa(\\lambda)$, of a planar curve defined parametrically by $\\mathbf{r}(\\lambda) = \\big(x(\\lambda), y(\\lambda)\\big)$. The derivation must begin from the fundamental definition of curvature, $\\kappa = \\left\\|\\frac{d\\hat{\\mathbf{t}}}{ds}\\right\\|$, where $s$ is the arc length and $\\hat{\\mathbf{t}}$ is the unit tangent vector. The final expression must be in terms of the first and second derivatives of $x$ and $y$ with respect to $\\lambda$: $x'(\\lambda)$, $y'(\\lambda)$, $x''(\\lambda)$, and $y''(\\lambda)$.\n\nThe position vector of a point on the curve is given by $\\mathbf{r}(\\lambda) = (x(\\lambda), y(\\lambda))$.\nThe tangent vector to the curve with respect to the parameter $\\lambda$ is the first derivative of the position vector:\n$$\n\\mathbf{r}'(\\lambda) = \\frac{d\\mathbf{r}}{d\\lambda} = \\left(\\frac{dx}{d\\lambda}, \\frac{dy}{d\\lambda}\\right) = (x'(\\lambda), y'(\\lambda))\n$$\nThe arc length element, $ds$, is related to the differential of the parameter, $d\\lambda$, by the magnitude of this tangent vector:\n$$\nds = \\left\\|\\mathbf{r}'(\\lambda)\\right\\| d\\lambda\n$$\nFrom this, we find the derivative of the arc length with respect to $\\lambda$:\n$$\n\\frac{ds}{d\\lambda} = \\left\\|\\mathbf{r}'(\\lambda)\\right\\| = \\sqrt{(x'(\\lambda))^2 + (y'(\\lambda))^2}\n$$\nFor notational brevity, we will denote $x'(\\lambda)$ as $x'$, $y'(\\lambda)$ as $y'$, and $\\frac{ds}{d\\lambda}$ as $s'$. The problem assumes that $(x', y') \\neq (0, 0)$, so $s' > 0$.\n\nThe unit tangent vector, $\\hat{\\mathbf{t}}$, is defined as the derivative of the position vector with respect to arc length, $\\hat{\\mathbf{t}} = \\frac{d\\mathbf{r}}{ds}$. Using the chain rule, we can express $\\hat{\\mathbf{t}}$ in terms of the parameter $\\lambda$:\n$$\n\\hat{\\mathbf{t}}(\\lambda) = \\frac{d\\mathbf{r}}{ds} = \\frac{d\\mathbf{r}/d\\lambda}{ds/d\\lambda} = \\frac{\\mathbf{r}'(\\lambda)}{s'(\\lambda)}\n$$\nThe curvature is defined as $\\kappa = \\left\\|\\frac{d\\hat{\\mathbf{t}}}{ds}\\right\\|$. To compute this, we first find the derivative of $\\hat{\\mathbf{t}}$ with respect to $\\lambda$ and then use the chain rule again:\n$$\n\\frac{d\\hat{\\mathbf{t}}}{ds} = \\frac{d\\hat{\\mathbf{t}}/d\\lambda}{ds/d\\lambda} = \\frac{1}{s'(\\lambda)}\\frac{d\\hat{\\mathbf{t}}}{d\\lambda}\n$$\nWe compute $\\frac{d\\hat{\\mathbf{t}}}{d\\lambda}$ by applying the quotient rule to $\\hat{\\mathbf{t}}(\\lambda) = \\frac{\\mathbf{r}'(\\lambda)}{s'(\\lambda)}$:\n$$\n\\frac{d\\hat{\\mathbf{t}}}{d\\lambda} = \\frac{\\frac{d\\mathbf{r}'}{d\\lambda}s'(\\lambda) - \\mathbf{r}'(\\lambda)\\frac{ds'}{d\\lambda}}{(s'(\\lambda))^2} = \\frac{\\mathbf{r}''(\\lambda)s'(\\lambda) - \\mathbf{r}'(\\lambda)s''(\\lambda)}{(s'(\\lambda))^2}\n$$\nwhere $\\mathbf{r}''(\\lambda) = (x''(\\lambda), y''(\\lambda))$ and $s''(\\lambda) = \\frac{d}{d\\lambda}s'(\\lambda)$. Let's compute $s''(\\lambda)$:\n$$\ns'(\\lambda) = \\left((x')^2 + (y')^2\\right)^{1/2}\n$$\n$$\ns''(\\lambda) = \\frac{d}{d\\lambda}\\left((x')^2 + (y')^2\\right)^{1/2} = \\frac{1}{2}\\left((x')^2 + (y')^2\\right)^{-1/2}\\left(2x'x'' + 2y'y''\\right) = \\frac{x'x'' + y'y''}{\\sqrt{(x')^2 + (y')^2}} = \\frac{x'x'' + y'y''}{s'}\n$$\nSubstituting this expression for $s''(\\lambda)$ back into the formula for $\\frac{d\\hat{\\mathbf{t}}}{d\\lambda}$:\n$$\n\\frac{d\\hat{\\mathbf{t}}}{d\\lambda} = \\frac{\\mathbf{r}''s' - \\mathbf{r}'\\left(\\frac{x'x'' + y'y''}{s'}\\right)}{(s')^2} = \\frac{\\mathbf{r}''(s')^2 - \\mathbf{r}'(x'x'' + y'y'')}{(s')^3}\n$$\nLet's analyze the numerator vector component-wise. Let $\\mathbf{V} = \\mathbf{r}''(s')^2 - \\mathbf{r}'(x'x'' + y'y'')$.\n$$\n\\mathbf{V} = (x'', y'')\\left((x')^2 + (y')^2\\right) - (x', y')(x'x'' + y'y'')\n$$\nThe x-component of $\\mathbf{V}$ is:\n$$\nV_x = x''\\left((x')^2 + (y')^2\\right) - x'(x'x'' + y'y'') = x''(x')^2 + x''(y')^2 - x'(x'x'') - x'(y'y'') = x''(y')^2 - x'y'y'' = y'(x''y' - x'y'')\n$$\nThe y-component of $\\mathbf{V}$ is:\n$$\nV_y = y''\\left((x')^2 + (y')^2\\right) - y'(x'x'' + y'y'') = y''(x')^2 + y''(y')^2 - y'(x'x'') - y'(y'y'') = y''(x')^2 - y'x'x'' = -x'(x'y'' - y'x'')\n$$\nSo, the numerator vector is:\n$$\n\\mathbf{V} = \\big(y'(x''y' - x'y''), -x'(x'y'' - y'x'')\\big) = (x'y'' - x''y')(-y', x')\n$$\nThus, the derivative of the unit tangent vector with respect to $\\lambda$ is:\n$$\n\\frac{d\\hat{\\mathbf{t}}}{d\\lambda} = \\frac{(x'y'' - x''y')}{(s')^3}(-y', x')\n$$\nNow we can find $\\frac{d\\hat{\\mathbf{t}}}{ds}$:\n$$\n\\frac{d\\hat{\\mathbf{t}}}{ds} = \\frac{1}{s'}\\frac{d\\hat{\\mathbf{t}}}{d\\lambda} = \\frac{1}{s'} \\frac{(x'y'' - x''y')}{(s')^3}(-y', x') = \\frac{x'y'' - x''y'}{(s')^4}(-y', x')\n$$\nFinally, we compute the curvature $\\kappa$ by taking the norm of this vector:\n$$\n\\kappa = \\left\\|\\frac{d\\hat{\\mathbf{t}}}{ds}\\right\\| = \\left\\|\\frac{x'y'' - x''y'}{(s')^4}(-y', x')\\right\\|\n$$\nSince the term $\\frac{x'y'' - x''y'}{(s')^4}$ is a scalar, we can take its absolute value out of the norm:\n$$\n\\kappa = \\frac{|x'y'' - x''y'|}{(s')^4}\\|(-y', x')\\|\n$$\nThe norm of the vector $(-y', x')$ is:\n$$\n\\|(-y', x')\\| = \\sqrt{(-y')^2 + (x')^2} = \\sqrt{(y')^2 + (x')^2} = s'\n$$\nSubstituting this back into the expression for $\\kappa$:\n$$\n\\kappa = \\frac{|x'y'' - x''y'|}{(s')^4}s' = \\frac{|x'y'' - x''y'|}{(s')^3}\n$$\nSubstituting $s' = \\sqrt{(x')^2 + (y')^2}$, we get the final expression for the curvature in terms of the derivatives of $x$ and $y$ with respect to $\\lambda$:\n$$\n\\kappa(\\lambda) = \\frac{|x'(\\lambda)y''(\\lambda) - x''(\\lambda)y'(\\lambda)|}{\\left( (x'(\\lambda))^2 + (y'(\\lambda))^2 \\right)^{3/2}}\n$$\nThis expression is a function of $\\lambda$ and depends only on $x'(\\lambda)$, $y'(\\lambda)$, $x''(\\lambda)$, and $y''(\\lambda)$ as required.", "answer": "$$\\boxed{\\frac{|x'(\\lambda)y''(\\lambda) - x''(\\lambda)y'(\\lambda)|}{\\left( (x'(\\lambda))^2 + (y'(\\lambda))^2 \\right)^{3/2}}}$$", "id": "3613564"}, {"introduction": "While an idealized L-curve presents a single, unambiguous corner, real-world problems often yield trade-off curves with multiple apparent corners, creating ambiguity in parameter selection. This phenomenon is not random but is deeply connected to the spectral properties of the forward operator, particularly the clustering of small singular values. This thought experiment challenges you to diagnose the cause of multiple corners and, more importantly, to move beyond simple curvature maximization by using more physically meaningful diagnostics—such as residual analysis and model resolution—to select the most appropriate regularization parameter [@problem_id:3613629].", "problem": "Consider a linear inverse problem in computational geophysics where the measured data vector $\\mathbf{d} \\in \\mathbb{R}^{m}$ is related to the model vector $\\mathbf{m} \\in \\mathbb{R}^{n}$ through a linear forward operator $\\mathbf{G} \\in \\mathbb{R}^{m \\times n}$ by $\\mathbf{d} = \\mathbf{G}\\mathbf{m}^{\\star} + \\boldsymbol{\\epsilon}$, where $\\mathbf{m}^{\\star}$ is the unknown true model and $\\boldsymbol{\\epsilon}$ is additive noise. Assume $\\boldsymbol{\\epsilon}$ is zero-mean Gaussian with covariance $\\sigma_{\\epsilon}^{2}\\mathbf{I}$, and the data have been whitened so that the effective noise covariance is $\\mathbf{I}$. Consider standard zero-order Tikhonov regularization with $\\mathbf{L} = \\mathbf{I}$, whose estimator is given by $\\mathbf{m}_{\\lambda} = \\left(\\mathbf{G}^{\\top}\\mathbf{G} + \\lambda^{2}\\mathbf{I}\\right)^{-1}\\mathbf{G}^{\\top}\\mathbf{d}$, where $\\lambda > 0$ is the regularization parameter. The L-curve is defined as the parametric curve $\\left(\\log\\left\\|\\mathbf{G}\\mathbf{m}_{\\lambda} - \\mathbf{d}\\right\\|_{2}, \\log\\left\\|\\mathbf{m}_{\\lambda}\\right\\|_{2}\\right)$ as $\\lambda$ varies.\n\nLet the singular value decomposition of $\\mathbf{G}$ be $\\mathbf{G} = \\mathbf{U}\\boldsymbol{\\Sigma}\\mathbf{V}^{\\top}$ with singular values $\\sigma_{1} \\geq \\sigma_{2} \\geq \\cdots \\geq \\sigma_{n} \\geq 0$. Suppose the spectrum exhibits two pronounced clusters of tiny singular values: a cluster $\\mathcal{C}_{1}$ of size $p$ centered around $\\sigma \\approx 10^{-6}$ and a cluster $\\mathcal{C}_{2}$ of size $q$ centered around $\\sigma \\approx 10^{-4}$, with the remaining singular values well-separated and significantly larger. Assume the geophysical target contains energy at multiple spatial scales, but the smallest-scale features are not expected to be recoverable given the acquisition and noise levels.\n\nUsing fundamental properties of the singular value decomposition, the filter factors of Tikhonov regularization with $\\mathbf{L} = \\mathbf{I}$ can be written along the right singular vectors $\\mathbf{v}_{i}$ as $f_{i}(\\lambda) = \\dfrac{\\sigma_{i}^{2}}{\\sigma_{i}^{2} + \\lambda^{2}}$. The resolution matrix, defined as the mapping from the true model to the noise-free estimate, is $\\mathbf{R}_{\\lambda} = \\left(\\mathbf{G}^{\\top}\\mathbf{G} + \\lambda^{2}\\mathbf{I}\\right)^{-1}\\mathbf{G}^{\\top}\\mathbf{G}$, which has eigenvalues $f_{i}(\\lambda)$ in the basis of $\\mathbf{V}$. The whitened residual is $\\mathbf{r}_{\\lambda} = \\mathbf{G}\\mathbf{m}_{\\lambda} - \\mathbf{d}$, and for an appropriate choice of $\\lambda$ it should be statistically consistent with white noise if the data misfit is dominated by noise.\n\nWhich option best predicts the emergence of multiple apparent corners on the L-curve in this scenario and correctly explains how to disambiguate the physically meaningful corner using both resolution properties and residual structure?\n\nA. Multiple apparent corners arise because the filter factors $f_{i}(\\lambda)$ transition rapidly near each cluster of tiny singular values, producing sharp changes in the trade-off between the model norm and data misfit at distinct $\\lambda$ scales. The physically meaningful corner is the one near the $\\lambda$ at which the whitened residual $\\mathbf{r}_{\\lambda}$ becomes statistically noise-like (e.g., its norm matches the expected noise level and shows no spatial correlation), while the resolution matrix $\\mathbf{R}_{\\lambda}$ provides stable, spatially localized resolution for the resolved band without amplifying components associated with the smallest singular values. Corners corresponding to aggressively fitting directions in $\\mathcal{C}_{1}$ or $\\mathcal{C}_{2}$ that yield structured residuals or highly oscillatory resolution kernels should be rejected.\n\nB. Multiple apparent corners arise because the left singular vectors $\\mathbf{u}_{i}$ change with $\\lambda$, making the data space representation nonstationary. The correct corner is the one with the smallest model norm $\\left\\|\\mathbf{m}_{\\lambda}\\right\\|_{2}$ to guarantee no overfitting, regardless of the residual structure or resolution properties.\n\nC. Multiple apparent corners arise from nonlinearity of the forward operator when $\\lambda$ varies. The correct choice is always the corner with the largest curvature on the L-curve, since curvature maximization is independent of the singular value distribution, residual statistics, and resolution.\n\nD. Multiple apparent corners are purely discretization artifacts. The correct corner is the one that minimizes the residual norm $\\left\\|\\mathbf{G}\\mathbf{m}_{\\lambda} - \\mathbf{d}\\right\\|_{2}$, because data fit should always be prioritized over resolution and statistical consistency of residuals.", "solution": "The problem investigates the origin of multiple corners on an L-curve and the criteria for selecting the physically meaningful regularization parameter.\n\n**Cause of Multiple Corners:**\nThe shape of the L-curve is determined by the Tikhonov filter factors, $f_{i}(\\lambda) = \\frac{\\sigma_{i}^{2}}{\\sigma_{i}^{2} + \\lambda^{2}}$, which control how much each singular value component of the solution is retained. Each filter factor transitions from approximately 0 to 1 as the regularization parameter $\\lambda$ decreases past the corresponding singular value $\\sigma_i$. When the singular value spectrum of the operator $\\mathbf{G}$ has distinct clusters of values at different scales (e.g., large values, a cluster at $\\sigma \\approx 10^{-4}$, and another at $\\sigma \\approx 10^{-6}$), the L-curve will exhibit sharp changes in behavior as $\\lambda$ crosses each of these scales. Each transition, where a group of filter factors simultaneously \"switches on,\" causes an abrupt change in the balance between the residual norm and the solution norm, creating a corner-like feature on the L-curve. Thus, multiple singular value clusters lead to multiple apparent corners.\n\n**Disambiguating the Optimal Corner:**\nSimply choosing the corner with the maximum curvature can be misleading. The smallest singular values ($\\mathcal{C}_{1}$ and $\\mathcal{C}_{2}$) correspond to model components that are highly sensitive to noise and, as stated, likely unrecoverable. Choosing a very small $\\lambda$ to fit these components (i.e., selecting a corner at $\\lambda \\approx 10^{-4}$ or $\\lambda \\approx 10^{-6}$) constitutes overfitting. A more robust approach requires examining the physical and statistical properties of the solution.\n1.  **Residual Analysis:** For an optimal solution, the residual $\\mathbf{r}_{\\lambda} = \\mathbf{G}\\mathbf{m}_{\\lambda} - \\mathbf{d}$ should primarily consist of the data noise that has been correctly excluded from the model. With whitened data, this means the residual should appear as random white noise, with a norm consistent with the known noise level and no coherent spatial structure. An overly aggressive fit (small $\\lambda$) will produce a residual that is too small and may show structure, indicating that noise has been fit.\n2.  **Resolution Analysis:** The model resolution matrix $\\mathbf{R}_{\\lambda}$ shows how the estimated model is a blurred version of the true model. Good resolution implies that the blurring kernels (columns of $\\mathbf{R}_{\\lambda}$) are spatially localized. Attempting to resolve components tied to tiny singular values (by choosing a small $\\lambda$) often results in resolution kernels that are highly oscillatory and non-local, indicating an unstable and physically meaningless reconstruction.\n\nThe physically meaningful corner is the one corresponding to a $\\lambda$ large enough to suppress the unstable components associated with the tiny singular values, but small enough to fit the well-determined parts of the data. This choice is validated when the resulting residual is noise-like and the model resolution is stable.\n\n**Evaluating the Options:**\n-   **A:** Correctly attributes multiple corners to filter factor transitions near singular value clusters. Correctly proposes disambiguation using residual analysis (seeking noise-like residuals) and resolution analysis (seeking stable, localized kernels), and rejecting corners corresponding to fitting unstable components. This is the correct, comprehensive approach.\n-   **B:** Incorrectly claims singular vectors change with $\\lambda$. Incorrectly proposes minimizing the model norm, which leads to underfitting.\n-   **C:** Incorrectly attributes the cause to nonlinearity. Incorrectly advocates for always using the maximum curvature criterion, which is a fragile heuristic in this scenario.\n-   **D:** Incorrectly attributes corners to discretization artifacts. Incorrectly proposes minimizing the residual, which is the unregularized, noise-amplified solution.\n\nTherefore, option A provides the best explanation and disambiguation strategy.", "answer": "$$\\boxed{A}$$", "id": "3613629"}, {"introduction": "This chapter culminates in a practical, hands-on coding exercise that synthesizes the theoretical concepts of regularization and L-curve analysis within a realistic seismic tomography scenario [@problem_id:3613601]. You will construct a complete inverse problem from the ground up, implement a more advanced anisotropic regularization scheme, and generate the L-curve to find an optimal regularization parameter $\\lambda$. By proceeding to analyze the directional resolution of the resulting model, this practice directly connects your abstract choice of $\\lambda$ to the concrete quality and interpretability of the final geophysical image, reinforcing the practical consequences of every decision in the inversion workflow.", "problem": "Consider a two-dimensional linear seismic travel-time tomography problem on a rectangular grid of size $N_z \\times N_x$, with $N_z = 12$ and $N_x = 12$. The unknown model is a vector $m \\in \\mathbb{R}^{N}$, where $N = N_z N_x$, representing dimensionless slowness values ($\\text{units} = 1$ per grid cell). The forward operator $G \\in \\mathbb{R}^{M \\times N}$ maps the model to travel-time data $d \\in \\mathbb{R}^{M}$ through a linear relation. Construct $G$ by assembling straight-ray paths that sum slowness along cells:\n\n- $N_x$ vertical ray paths, each traversing all $N_z$ cells in a column.\n- $N_z$ horizontal ray paths, each traversing all $N_x$ cells in a row.\n- $1$ main diagonal ray traversing cells $(i,j)$ with $i=j$.\n- $1$ anti-diagonal ray traversing cells $(i,j)$ with $j = N_x - 1 - i$.\n\nFor each ray, set the weight $w_{k}$ to $1$ for each cell it traverses (dimensionless length per cell), and normalize each row of $G$ to have Euclidean norm $1$ to avoid biasing rays with different numbers of cells.\n\nLet the true slowness field $m_{\\text{true}}$ be a Gaussian anomaly centered at the middle grid cell, with amplitude $1.0$ (dimensionless), standard deviation $\\sigma_x = 3$ cells along the $x$-direction and $\\sigma_z = 2$ cells along the $z$-direction. Generate synthetic data $d$ by $d = G m_{\\text{true}} + \\eta$, where $\\eta$ is additive independent Gaussian noise with zero mean and standard deviation $0.01$, and use a fixed pseudorandom seed so that results are reproducible.\n\nWe study zero-order Tikhonov regularization with an anisotropic first-derivative smoothing operator $L = [\\partial_x, \\ \\alpha \\partial_z]$ applied to the model $m$ on the grid. In discrete form, the anisotropic penalty is the squared norm of forward differences in $x$ and $z$ with the $z$-component weighted by $\\alpha^2$. That is, set the discrete seminorm to\n$$\n\\|L m\\|_2^2 = \\sum_{i=0}^{N_z-1} \\sum_{j=0}^{N_x-2} \\left(m_{i,j+1} - m_{i,j}\\right)^2 \\;+\\; \\alpha^2 \\sum_{i=0}^{N_z-2} \\sum_{j=0}^{N_x-1} \\left(m_{i+1,j} - m_{i,j}\\right)^2,\n$$\nwhere $m_{i,j}$ denotes the model at grid position $(i,j)$.\n\nDefine the regularized solution $m_\\lambda$ as the minimizer of the standard least-squares data fit with the anisotropic smoothing penalty weighted by $\\lambda^2$:\n$$\nm_\\lambda = \\arg\\min_{m \\in \\mathbb{R}^{N}} \\left( \\|G m - d\\|_2^2 + \\lambda^2 \\|L m\\|_2^2 \\right),\n$$\nfor a given regularization parameter $\\lambda > 0$.\n\nThe trade-off between data misfit and model roughness can be analyzed via the so-called L-curve: a parametric curve of points\n$$\n\\left( \\|G m_\\lambda - d\\|_2, \\ \\|L m_\\lambda\\|_2 \\right)\n$$\nfor a range of $\\lambda$ values. The \"corner\" of the L-curve is often used as a pragmatic balance between fidelity and regularity. In this problem, you must locate the L-curve corner using the maximum curvature in the $\\log$-$\\log$ plot of the L-curve and relate it to directional resolution.\n\nDirectional resolution is to be quantified via the resolution kernel's point-spread function at the central grid cell. Let $R_\\lambda \\in \\mathbb{R}^{N \\times N}$ denote the resolution matrix associated with the linear estimator for regularization parameter $\\lambda$. The point-spread function for the center cell is the column of $R_\\lambda$ corresponding to that cell; reshape it to the $N_z \\times N_x$ grid, and measure directional spreads by second moments along $x$ and $z$ using squared weights as a nonnegative measure. Define the directional resolution ratio as\n$$\n\\rho(\\lambda) = \\frac{\\sigma_x(\\lambda)}{\\sigma_z(\\lambda)},\n$$\nwhere $\\sigma_x(\\lambda)$ and $\\sigma_z(\\lambda)$ are the square roots of the second central moments along $x$ and $z$, respectively, computed from the squared point-spread function weights normalized to sum to $1$.\n\nYour tasks:\n\n1. Construct $G$ and $m_{\\text{true}}$ precisely as described, with all units dimensionless.\n2. For each anisotropy $\\alpha$ in the test suite specified below, sample $\\lambda$ values logarithmically spaced across $25$ points between $10^{-3}$ and $10^{1}$, compute the L-curve $(\\|G m_\\lambda - d\\|_2, \\|L m_\\lambda\\|_2)$, and identify the corner as the point of maximum curvature on the $\\log$-$\\log$ plot as a function of $\\lambda$.\n3. At the corner value $\\lambda^\\star$, compute the directional resolution ratio $\\rho(\\lambda^\\star)$ using the resolution kernel as described, with the central cell chosen at $(\\lfloor N_z/2 \\rfloor, \\lfloor N_x/2 \\rfloor)$.\n4. Round all final reported floating-point values to $6$ decimal places.\n\nTest suite:\n\n- Case $1$: $\\alpha = 0.5$.\n- Case $2$: $\\alpha = 1.0$.\n- Case $3$: $\\alpha = 2.0$.\n- Case $4$: $\\alpha = 3.0$.\n\nFinal output format:\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each test case in order, output the pair $\\left[\\lambda^\\star,\\ \\rho(\\lambda^\\star)\\right]$ flattened into a single list. For example, the output should be of the form\n$$\n[\\lambda^\\star(\\alpha_1),\\rho(\\alpha_1),\\lambda^\\star(\\alpha_2),\\rho(\\alpha_2),\\lambda^\\star(\\alpha_3),\\rho(\\alpha_3),\\lambda^\\star(\\alpha_4),\\rho(\\alpha_4)].\n$$\nAll numbers must be rounded to $6$ decimal places, and there should be no additional text printed.", "solution": "The problem presented is a well-defined exercise in computational geophysics, specifically focusing on the theory and application of regularization for linear inverse problems. It is scientifically sound, mathematically consistent, and all necessary parameters and definitions are provided. We can therefore proceed with a full solution.\n\nThe core of the problem is to solve a linear system of equations of the form $Gm = d$, which is a common representation for a wide range of physical inverse problems. Here, $m \\in \\mathbb{R}^N$ is the unknown model vector (slowness values on a grid), $d \\in \\mathbb{R}^M$ is the observed data vector (travel times), and $G \\in \\mathbb{R}^{M \\times N}$ is the forward operator that maps the model space to the data space. In this specific case, the problem is ill-posed, primarily because the number of measurements $M$ is smaller than the number of model parameters $N$ ($M=26, N=144$), and the matrix $G$ is likely ill-conditioned. Such problems cannot be solved by simple inversion and require regularization to obtain a stable and physically meaningful solution.\n\n### 1. Tikhonov Regularization\n\nTikhonov regularization is employed to address the ill-posed nature of the problem. It reformulates the inversion as an optimization problem where we seek a model $m$ that minimizes a composite objective function. This function consists of two terms: a data misfit term, $\\|G m - d\\|_2^2$, which measures how well the model predicts the observed data, and a solution penalty term, $\\|L m\\|_2^2$, which imposes a desired property (e.g., smoothness) on the solution. The balance between these two competing objectives is controlled by a positive regularization parameter, $\\lambda$. The regularized solution $m_\\lambda$ is defined as:\n$$\nm_\\lambda = \\arg\\min_{m \\in \\mathbb{R}^{N}} \\left( \\|G m - d\\|_2^2 + \\lambda^2 \\|L m\\|_2^2 \\right)\n$$\nThis is a standard quadratic optimization problem, and its solution can be found by solving the corresponding system of normal equations:\n$$\n(G^T G + \\lambda^2 L^T L) m_\\lambda = G^T d\n$$\nThe matrix $L$ is a regularization operator, chosen here to be an anisotropic first-derivative operator. The term $\\|L m\\|_2^2$ penalizes roughness in the model. The formulation\n$$\n\\|L m\\|_2^2 = \\sum_{i,j} \\left(m_{i,j+1} - m_{i,j}\\right)^2 \\;+\\; \\alpha^2 \\sum_{i,j} \\left(m_{i+1,j} - m_{i,j}\\right)^2\n$$\npenalizes the squared magnitude of the discrete gradient of the model. The parameter $\\alpha$ introduces anisotropy, allowing for different levels of smoothing in the horizontal ($x$) and vertical ($z$) directions. If $\\alpha < 1$, smoothing is weaker in the vertical direction, encouraging vertically elongated features. If $\\alpha > 1$, smoothing is weaker in the horizontal direction, encouraging horizontally elongated features.\n\n### 2. The L-Curve and Parameter Selection\n\nThe choice of the regularization parameter $\\lambda$ is critical. A small $\\lambda$ leads to a solution that fits the data well but may be dominated by noise, while a large $\\lambda$ results in a smooth solution that may not honor the data. The L-curve is a powerful heuristic tool for selecting an appropriate $\\lambda$. It is a parametric plot of the solution seminorm $\\|L m_\\lambda\\|_2$ versus the data misfit (residual norm) $\\|G m_\\lambda - d\\|_2$ for a range of $\\lambda$ values. When plotted on a log-log scale, this curve typically has a characteristic 'L' shape.\n\nThe \"corner\" of the L-curve represents a point where a significant decrease in the solution seminorm is achieved without a substantial increase in the data misfit. This corner is often considered to provide a good balance between data fidelity and solution regularity. The corner can be quantitatively identified as the point of maximum curvature on the log-log plot. For a parametric curve $(x(t), y(t))$, where $t = \\log(\\lambda)$, $x(t) = \\log(\\|G m_\\lambda - d\\|_2)$, and $y(t) = \\log(\\|L m_\\lambda\\|_2)$, the curvature $\\kappa$ is given by:\n$$\n\\kappa(t) = \\frac{|x'(t) y''(t) - y'(t) x''(t)|}{\\left(x'(t)^2 + y'(t)^2\\right)^{3/2}}\n$$\nWe find the optimal $\\lambda^\\star$ by locating the value of $\\lambda$ that maximizes this curvature.\n\n### 3. Resolution Analysis\n\nThe regularized solution $m_\\lambda$ is an estimate of the true model. The quality of this estimate can be assessed using the model resolution matrix, $R_\\lambda$. The linear estimator for the model is $m_\\lambda = (G^T G + \\lambda^2 L^T L)^{-1} G^T d$. Since the noise-free data would be $d_{\\text{true}} = G m_{\\text{true}}$, the relationship between the estimated model and the true model is $m_\\lambda \\approx (G^T G + \\lambda^2 L^T L)^{-1} G^T G m_{\\text{true}}$. The matrix\n$$\nR_\\lambda = (G^T G + \\lambda^2 L^T L)^{-1} G^T G\n$$\nis the model resolution matrix. Each column of $R_\\lambda$ is a point-spread function (PSF), which describes how the inversion process blurs a single point impulse in the true model.\n\nBy analyzing the PSF for the central grid cell, we can quantify the directional resolution of our inversion. We reshape the corresponding column of $R_\\lambda$ into an $N_z \\times N_x$ grid. To obtain a measure of spatial spread, we compute the second central moments of the squared PSF values (which form a non-negative distribution). Let $P_{i,j}$ be the PSF value at grid cell $(i,j)$ and $W_{i,j} = P_{i,j}^2 / \\sum_{k,l} P_{k,l}^2$ be the normalized weights. The second central moments are:\n$$\n\\sigma_z^2(\\lambda) = \\sum_{i,j} (i-\\bar{i})^2 W_{i,j} \\quad \\text{and} \\quad \\sigma_x^2(\\lambda) = \\sum_{i,j} (j-\\bar{j})^2 W_{i,j}\n$$\nwhere $\\bar{i}$ and $\\bar{j}$ are the mean positions (first moments). The square roots, $\\sigma_z(\\lambda)$ and $\\sigma_x(\\lambda)$, represent the resolution length scales in the vertical and horizontal directions. The directional resolution ratio, $\\rho(\\lambda) = \\sigma_x(\\lambda) / \\sigma_z(\\lambda)$, quantifies the anisotropy of the resolution at the chosen $\\lambda^\\star$. This ratio should be related to the anisotropy parameter $\\alpha$ used in the regularization.\n\n### 4. Algorithm Summary\n\nThe solution is implemented following these steps for each given value of the anisotropy parameter $\\alpha$:\n1.  **Setup**: Construct the forward operator $G$ based on the specified ray geometry and normalize its rows. Construct the true model $m_{\\text{true}}$ as a 2D Gaussian anomaly. Generate the synthetic data vector $d$ by applying the forward operator to $m_{\\text{true}}$ and adding Gaussian noise. This is done once with a fixed random seed for reproducibility.\n2.  **L-Curve Generation**: Construct the anisotropic regularization operator $L$ for the current $\\alpha$. Sample $25$ logarithmically spaced values of $\\lambda$ between $10^{-3}$ and $10^{1}$. For each $\\lambda$, solve the normal equations for $m_\\lambda$ and compute the corresponding residual norm $\\|G m_\\lambda - d\\|_2$ and solution seminorm $\\|L m_\\lambda\\|_2$.\n3.  **Corner Detection**: Compute the first and second derivatives of the log-log L-curve data points with respect to $\\log(\\lambda)$ using numerical finite differences. Use these derivatives to calculate the curvature at each point. The value $\\lambda^\\star$ is chosen as the $\\lambda$ corresponding to the maximum curvature.\n4.  **Resolution Ratio Calculation**: Using $\\lambda^\\star$ and the corresponding $L$, compute the model resolution matrix $R_{\\lambda^\\star}$. Extract the column corresponding to the central grid cell index to get the PSF. Reshape the PSF into a $2$D grid, compute the normalized second central moments $\\sigma_x^2$ and $\\sigma_z^2$ from its squared values, and calculate the resolution ratio $\\rho(\\lambda^\\star) = \\sigma_x(\\lambda^\\star) / \\sigma_z(\\lambda^\\star)$.\n5.  **Output**: Collect the pair $[\\lambda^\\star, \\rho(\\lambda^\\star)]$ for each $\\alpha$ and format them into the final output string.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the seismic tomography inverse problem with Tikhonov regularization.\n    Finds the optimal regularization parameter using an L-curve and analyzes\n    the directional resolution for different anisotropy factors.\n    \"\"\"\n    # Define problem parameters\n    NZ, NX = 12, 12\n    N = NZ * NX\n    NOISE_STD = 0.01\n    RANDOM_SEED = 42\n    \n    # Set seed for reproducibility\n    np.random.seed(RANDOM_SEED)\n\n    def construct_g_matrix(nz, nx):\n        \"\"\"Constructs and normalizes the forward operator G.\"\"\"\n        n = nz * nx\n        num_rays = nz + nx + 2\n        g = np.zeros((num_rays, n))\n        \n        current_ray = 0\n        # Vertical rays\n        for j in range(nx):\n            for i in range(nz):\n                g[current_ray, i * nx + j] = 1.0\n            current_ray += 1\n        \n        # Horizontal rays\n        for i in range(nz):\n            for j in range(nx):\n                g[current_ray, i * nx + j] = 1.0\n            current_ray += 1\n\n        # Main diagonal ray\n        for i in range(min(nz, nx)):\n            g[current_ray, i * nx + i] = 1.0\n        current_ray += 1\n\n        # Anti-diagonal ray\n        for i in range(min(nz, nx)):\n            j = nx - 1 - i\n            g[current_ray, i * nx + j] = 1.0\n        \n        # Normalize each row\n        for i in range(g.shape[0]):\n            norm = np.linalg.norm(g[i, :])\n            if norm > 0:\n                g[i, :] /= norm\n        return g\n\n    def construct_true_model(nz, nx):\n        \"\"\"Constructs the true slowness model with a Gaussian anomaly.\"\"\"\n        m_true_grid = np.zeros((nz, nx))\n        center_z, center_x = nz / 2, nx / 2\n        sigma_z, sigma_x = 2.0, 3.0\n        amplitude = 1.0\n        \n        z_coords, x_coords = np.mgrid[0:nz, 0:nx]\n        \n        m_true_grid = amplitude * np.exp(\n            -((x_coords - center_x)**2 / (2 * sigma_x**2))\n            -((z_coords - center_z)**2 / (2 * sigma_z**2))\n        )\n        return m_true_grid.flatten()\n\n    def construct_l_matrix(nz, nx, alpha):\n        \"\"\"Constructs the anisotropic first-derivative regularization operator L.\"\"\"\n        n = nz * nx\n\n        # Operator for x-derivatives\n        num_dx = nz * (nx - 1)\n        dx = np.zeros((num_dx, n))\n        row = 0\n        for i in range(nz):\n            for j in range(nx - 1):\n                dx[row, i * nx + j] = -1.0\n                dx[row, i * nx + j + 1] = 1.0\n                row += 1\n\n        # Operator for z-derivatives\n        num_dz = (nz - 1) * nx\n        dz = np.zeros((num_dz, n))\n        row = 0\n        for i in range(nz - 1):\n            for j in range(nx):\n                dz[row, i * nx + j] = -1.0\n                dz[row, (i + 1) * nx + j] = 1.0\n                row += 1\n        \n        return np.vstack([dx, alpha * dz])\n\n    # --- Main Calculation ---\n    \n    # 1. Pre-computation independent of alpha\n    G = construct_g_matrix(NZ, NX)\n    m_true = construct_true_model(NZ, NX)\n    \n    # Generate synthetic data\n    d_clean = G @ m_true\n    noise = np.random.normal(0, NOISE_STD, size=d_clean.shape)\n    d = d_clean + noise\n    \n    GTG = G.T @ G\n    GTd = G.T @ d\n    \n    lambdas = np.logspace(-3, 1, 25)\n    log_lambdas = np.log(lambdas)\n    \n    test_cases = [0.5, 1.0, 2.0, 3.0]\n    final_results = []\n\n    for alpha in test_cases:\n        # 2. Construct alpha-dependent matrices\n        L = construct_l_matrix(NZ, NX, alpha)\n        LTL = L.T @ L\n        \n        # 3. Compute L-curve points\n        res_norms = []\n        sol_norms = []\n        \n        for lam in lambdas:\n            A = GTG + lam**2 * LTL\n            m_lam = np.linalg.solve(A, GTd)\n            \n            res_norm = np.linalg.norm(G @ m_lam - d)\n            sol_norm = np.linalg.norm(L @ m_lam)\n            \n            res_norms.append(res_norm)\n            sol_norms.append(sol_norm)\n\n        # 4. Find L-curve corner\n        log_res_norms = np.log(res_norms)\n        log_sol_norms = np.log(sol_norms)\n        \n        # Parametric derivatives wrt log(lambda)\n        dx_dt = np.gradient(log_res_norms, log_lambdas)\n        dy_dt = np.gradient(log_sol_norms, log_lambdas)\n        \n        d2x_dt2 = np.gradient(dx_dt, log_lambdas)\n        d2y_dt2 = np.gradient(dy_dt, log_lambdas)\n        \n        # Curvature formula\n        curvature = np.abs(dx_dt * d2y_dt2 - dy_dt * d2x_dt2) / (dx_dt**2 + dy_dt**2)**1.5\n        \n        # Find lambda at max curvature\n        corner_idx = np.argmax(curvature)\n        lambda_star = lambdas[corner_idx]\n        \n        # 5. Compute resolution ratio at lambda_star\n        R_lambda = np.linalg.solve(GTG + lambda_star**2 * LTL, GTG)\n        \n        center_i = NZ // 2\n        center_j = NX // 2\n        center_idx = center_i * NX + center_j\n        \n        psf = R_lambda[:, center_idx]\n        psf_grid = psf.reshape((NZ, NX))\n        \n        weights = psf_grid**2\n        weights /= np.sum(weights)\n        \n        ii, jj = np.mgrid[0:NZ, 0:NX]\n        \n        i_bar = np.sum(ii * weights)\n        j_bar = np.sum(jj * weights)\n        \n        var_z = np.sum((ii - i_bar)**2 * weights)\n        var_x = np.sum((jj - j_bar)**2 * weights)\n        \n        sigma_z = np.sqrt(var_z)\n        sigma_x = np.sqrt(var_x)\n        \n        rho = sigma_x / sigma_z\n        \n        final_results.extend([lambda_star, rho])\n\n    # Format and print the final output\n    output_str = \",\".join([f\"{val:.6f}\" for val in final_results])\n    print(f\"[{output_str}]\")\n\nsolve()\n\n```", "id": "3613601"}]}