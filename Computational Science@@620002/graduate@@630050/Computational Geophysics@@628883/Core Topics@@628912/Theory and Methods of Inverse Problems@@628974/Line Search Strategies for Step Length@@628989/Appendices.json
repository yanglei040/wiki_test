{"hands_on_practices": [{"introduction": "To begin, we ground our understanding in the most fundamental line search procedure: backtracking. This exercise strips away complexity to focus purely on the mechanics of satisfying the Armijo sufficient decrease condition. By working through this simple quadratic problem [@problem_id:3607600], you will gain concrete experience with how a trial step length $\\alpha$ is iteratively tested and reduced until it guarantees a meaningful reduction in the objective function.", "problem": "Consider a linearized geophysical inverse problem in which the data misfit is modeled by the least-squares functional $F(m) = \\frac{1}{2}\\|A m - b\\|_{2}^{2}$. The forward operator $A$ and data $b$ are given by\n$$\nA = \\begin{pmatrix} 4  0 \\\\ 0  1 \\end{pmatrix}, \\qquad b = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}.\n$$\nStarting from the current model vector\n$$\nm_k = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix},\n$$\nadopt the steepest descent direction $p_k = -\\nabla F(m_k)$ and perform an Armijo backtracking line search with sufficient decrease parameter $c = \\frac{1}{10}$, initial step length $\\alpha_0 = 1$, and reduction factor $\\tau = \\frac{1}{2}$. By initiating the backtracking at $\\alpha = \\alpha_0$ and successively reducing $\\alpha \\leftarrow \\tau \\alpha$ until the Armijo sufficient decrease criterion along $p_k$ is met, determine the first acceptable step length $\\alpha$. Express the final answer as an exact value with no rounding and no units.", "solution": "The user has provided a well-posed problem in numerical optimization. I will proceed with a detailed solution.\n\nThe problem asks to find the first acceptable step length $\\alpha$ in a line search procedure for minimizing a given objective functional. The line search employs the Armijo backtracking strategy.\n\nThe objective functional is given by\n$$F(m) = \\frac{1}{2}\\|A m - b\\|_{2}^{2}$$\nwhere $m = \\begin{pmatrix} m_1 \\\\ m_2 \\end{pmatrix}$ is the model vector, and the forward operator $A$ and data vector $b$ are\n$$A = \\begin{pmatrix} 4  0 \\\\ 0  1 \\end{pmatrix}, \\qquad b = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$$\nSince $b=0$, the functional simplifies to $F(m) = \\frac{1}{2}\\|A m\\|_{2}^{2}$.\n\nFirst, we compute the gradient of the functional, $\\nabla F(m)$.\n$$F(m) = \\frac{1}{2}(A m - b)^T(A m - b)$$\nThe gradient is given by\n$$\\nabla F(m) = A^T(A m - b)$$\nSubstituting the given $A$ and $b$, and noting that $A$ is symmetric ($A^T=A$), we get\n$$\\nabla F(m) = A(A m) = A^2 m = \\begin{pmatrix} 4  0 \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} 4  0 \\\\ 0  1 \\end{pmatrix} m = \\begin{pmatrix} 16  0 \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} m_1 \\\\ m_2 \\end{pmatrix} = \\begin{pmatrix} 16m_1 \\\\ m_2 \\end{pmatrix}$$\nThe current model vector is $m_k = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$. We evaluate the gradient at this point:\n$$\\nabla F(m_k) = \\begin{pmatrix} 16(1) \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 16 \\\\ -1 \\end{pmatrix}$$\nThe problem specifies the steepest descent direction, which is the negative of the gradient:\n$$p_k = -\\nabla F(m_k) = -\\begin{pmatrix} 16 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} -16 \\\\ 1 \\end{pmatrix}$$\nNext, we formulate the Armijo sufficient decrease condition, which a step length $\\alpha$ must satisfy:\n$$F(m_k + \\alpha p_k) \\le F(m_k) + c \\alpha \\nabla F(m_k)^T p_k$$\nwhere the sufficient decrease parameter is given as $c = \\frac{1}{10}$.\n\nWe must evaluate each term in this inequality.\nFirst, the value of the functional at the current point $m_k$:\n$$A m_k = \\begin{pmatrix} 4  0 \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 4 \\\\ -1 \\end{pmatrix}$$\n$$F(m_k) = \\frac{1}{2}\\|A m_k\\|_{2}^{2} = \\frac{1}{2} \\left\\| \\begin{pmatrix} 4 \\\\ -1 \\end{pmatrix} \\right\\|_{2}^{2} = \\frac{1}{2}(4^2 + (-1)^2) = \\frac{1}{2}(16+1) = \\frac{17}{2}$$\nSecond, the directional derivative term $\\nabla F(m_k)^T p_k$:\n$$\\nabla F(m_k)^T p_k = \\begin{pmatrix} 16  -1 \\end{pmatrix} \\begin{pmatrix} -16 \\\\ 1 \\end{pmatrix} = (16)(-16) + (-1)(1) = -256 - 1 = -257$$\nNote that for the steepest descent direction, this is equivalent to $-\\|\\nabla F(m_k)\\|_{2}^{2}$. Indeed, $\\|\\nabla F(m_k)\\|_{2}^{2} = 16^2+(-1)^2 = 256+1=257$.\n\nNow, we evaluate the left-hand side of the Armijo condition, $F(m_k + \\alpha p_k)$:\n$$m_k + \\alpha p_k = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} + \\alpha \\begin{pmatrix} -16 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 - 16\\alpha \\\\ -1 + \\alpha \\end{pmatrix}$$\n$$A(m_k + \\alpha p_k) = \\begin{pmatrix} 4  0 \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} 1 - 16\\alpha \\\\ -1 + \\alpha \\end{pmatrix} = \\begin{pmatrix} 4(1 - 16\\alpha) \\\\ -1 + \\alpha \\end{pmatrix} = \\begin{pmatrix} 4 - 64\\alpha \\\\ -1 + \\alpha \\end{pmatrix}$$\n$$F(m_k + \\alpha p_k) = \\frac{1}{2} \\left\\| \\begin{pmatrix} 4 - 64\\alpha \\\\ -1 + \\alpha \\end{pmatrix} \\right\\|_{2}^{2} = \\frac{1}{2}((4 - 64\\alpha)^2 + (-1 + \\alpha)^2)$$\nExpanding the terms:\n$$F(m_k + \\alpha p_k) = \\frac{1}{2}(16 - 512\\alpha + 4096\\alpha^2 + 1 - 2\\alpha + \\alpha^2) = \\frac{1}{2}(17 - 514\\alpha + 4097\\alpha^2)$$\nSubstituting all terms back into the Armijo inequality:\n$$\\frac{1}{2}(17 - 514\\alpha + 4097\\alpha^2) \\le \\frac{17}{2} + \\frac{1}{10}\\alpha(-257)$$\n$$\\frac{1}{2}(17 - 514\\alpha + 4097\\alpha^2) \\le \\frac{17}{2} - \\frac{257}{10}\\alpha$$\nMultiplying both sides by $2$:\n$$17 - 514\\alpha + 4097\\alpha^2 \\le 17 - \\frac{257}{5}\\alpha$$\nSubtracting $17$ from both sides:\n$$-514\\alpha + 4097\\alpha^2 \\le -\\frac{257}{5}\\alpha$$\nSince $\\alpha  0$ for a line search, we can divide by $\\alpha$:\n$$-514 + 4097\\alpha \\le -\\frac{257}{5}$$\n$$4097\\alpha \\le 514 - \\frac{257}{5}$$\n$$4097\\alpha \\le \\frac{2570 - 257}{5} = \\frac{2313}{5}$$\n$$\\alpha \\le \\frac{2313}{5 \\times 4097} = \\frac{2313}{20485}$$\nNumerically, $\\frac{2313}{20485} \\approx 0.1129$.\n\nThe Armijo backtracking procedure starts with an initial step length $\\alpha_0 = 1$ and iteratively reduces it by a factor of $\\tau = \\frac{1}{2}$ until the condition is met. The sequence of trial step lengths is $\\alpha = \\alpha_0, \\tau \\alpha_0, \\tau^2 \\alpha_0, \\dots$. That is, $\\alpha = 1, \\frac{1}{2}, \\frac{1}{4}, \\frac{1}{8}, \\frac{1}{16}, \\dots$.\n\nWe check each value in this sequence against the derived condition $\\alpha \\le \\frac{2313}{20485}$.\n- Test $\\alpha = 1$: $1  \\frac{2313}{20485}$. The condition is not met.\n- Test $\\alpha = \\frac{1}{2} = 0.5$: $\\frac{1}{2}  \\frac{2313}{20485}$. The condition is not met.\n- Test $\\alpha = \\frac{1}{4} = 0.25$: $\\frac{1}{4}  \\frac{2313}{20485}$. The condition is not met.\n- Test $\\alpha = \\frac{1}{8} = 0.125$: $\\frac{1}{8} = \\frac{2560.625}{20485}  \\frac{2313}{20485}$. The condition is not met.\n- Test $\\alpha = \\frac{1}{16} = 0.0625$: $\\frac{1}{16} = \\frac{1280.3125}{20485}  \\frac{2313}{20485}$. The condition is met.\n\nThe first acceptable step length is $\\alpha = \\frac{1}{16}$.", "answer": "$$\\boxed{\\frac{1}{16}}$$", "id": "3607600"}, {"introduction": "While the Armijo condition ensures sufficient decrease, it does not prevent taking excessively small steps. More critically, in complex geophysical problems with non-quadratic misfits—such as those involving robust statistics to handle outliers—the one-dimensional search space can become non-convex. This exercise [@problem_id:3607649] presents a scenario where a simple sufficient decrease check is prone to failure and motivates the need for the full Wolfe conditions, which pair the Armijo condition with a curvature requirement to ensure meaningful progress.", "problem": "In gradient-based seismic amplitude inversion, the objective function is often assembled from many data residuals. When a subset of receivers suffers erratic behavior (e.g., intermittent coupling or clipping), their errors can be heavy-tailed. This can make line search acceptance with the classical Armijo backtracking strategy unreliable because function values along the trial step can exhibit nonconvexity or sudden increases. Consider the following deliberately simplified but scientifically consistent setup that isolates this effect and examines how to choose the line search parameters to improve acceptance under a robust loss.\n\nA linearized forward model contributes a dominant quadratic part from many well-behaved measurements and a single heavy-tailed outlier. Let the current model iterate be denoted by $m$, the steepest-descent direction by $p=-\\nabla f(m)$, and the univariate line-search function by $\\phi(\\alpha)=f(m+\\alpha p)$. Assume:\n- The well-behaved measurements induce a quadratic contribution with descent curvature characterized along $p$ by $\\|\\nabla f(m)\\|^{2}=A$ and $p^{\\top}\\nabla^{2}f(m)p=K$, so that their contribution to $\\phi(\\alpha)$ is $-A\\,\\alpha+\\tfrac{1}{2}K\\alpha^{2}$.\n- One outlier residual $r(\\alpha)$ behaves according to a heavy-tailed Student’s $t$ negative log-likelihood with degrees of freedom $\\nu=1$ (Cauchy) and scale $\\sigma=1$, namely\n$$\n\\ell(r)=\\frac{\\nu+1}{2}\\ln\\!\\left(1+\\frac{r^{2}}{\\nu\\sigma^{2}}\\right)=\\ln\\!\\left(1+r^{2}\\right).\n$$\n- Along the search ray, the outlier residual is affine: $r(\\alpha)=r_{0}-s\\,\\alpha$ with $r_{0}=10$ and $s=10$.\n- The aggregate quadratic contribution satisfies $A=100$ and $K=400$.\n\n1) Show explicitly that, if the misfit were least squares for all terms (ignoring robustness), the Armijo sufficient decrease with a unit trial step $\\alpha=1$ would frequently fail in the presence of the outlier given the above numbers. Use the steepest-descent model-based prediction to quantify the failure for a standard Armijo constant $c_{1}=10^{-4}$.\n\n2) Now adopt the robust objective that combines the quadratic part with the single Student’s $t$ outlier:\n$$\n\\phi(\\alpha)=\\phi(0)-A\\,\\alpha+\\frac{1}{2}K\\alpha^{2}+\\ln\\!\\left(\\frac{1+\\left(r_{0}-s\\alpha\\right)^{2}}{1+r_{0}^{2}}\\right),\n$$\nso that $\\phi(0)$ is an irrelevant constant for acceptance. Write down $\\phi'(\\alpha)$ and $\\phi'(0)$ explicitly in terms of $A,K,r_{0},s$.\n\n3) Consider a backtracking candidate set of step lengths $\\alpha\\in[\\alpha_{\\min},\\alpha_{\\max}]$ with $\\alpha_{\\min}=0.1$ and $\\alpha_{\\max}=0.25$. For the robust objective, the Armijo condition with constant $c_{1}\\in(0,1)$ is\n$$\n\\phi(\\alpha)\\le \\phi(0)+c_{1}\\,\\alpha\\,\\phi'(0),\n$$\nand the (weak) Wolfe curvature condition with constant $c_{2}\\in(c_{1},1)$ is\n$$\n\\phi'(\\alpha)\\ge c_{2}\\,\\phi'(0).\n$$\nDefine the two diagnostic functions on $[\\alpha_{\\min},\\alpha_{\\max}]$:\n$$\nT(\\alpha)=\\frac{\\phi(\\alpha)-\\phi(0)}{\\alpha\\,\\phi'(0)},\\qquad L(\\alpha)=\\frac{\\phi'(\\alpha)}{\\phi'(0)}.\n$$\nBy construction, a step $\\alpha$ in the interval satisfies the Armijo condition if and only if $T(\\alpha) \\ge c_1$, and satisfies the curvature condition if and only if $L(\\alpha) \\le c_2$.\nChoosing a single pair $(c_{1},c_{2})$ that makes all steps in $[\\alpha_{\\min},\\alpha_{\\max}]$ acceptable requires\n$$\nc_{1}\\le \\min_{\\alpha\\in[\\alpha_{\\min},\\alpha_{\\max}]}T(\\alpha),\\qquad c_{2}\\ge \\max_{\\alpha\\in[\\alpha_{\\min},\\alpha_{\\max}]}L(\\alpha).\n$$\n\nCompute these two extrema exactly for the given $A,K,r_{0},s,\\nu,\\sigma$ and then report the minimally restrictive pair\n$$\nc_{1}^{\\star}=\\min_{\\alpha\\in[\\alpha_{\\min},\\alpha_{\\max}]}T(\\alpha),\\qquad c_{2}^{\\star}=\\max_{\\alpha\\in[\\alpha_{\\min},\\alpha_{\\max}]}L(\\alpha),\n$$\nrounded to four significant figures.\n\nYour final answer must be the row vector $\\begin{pmatrix}c_{1}^{\\star}c_{2}^{\\star}\\end{pmatrix}$ with both entries rounded to four significant figures and no units.", "solution": "The problem is scientifically consistent, well-posed, and contains all necessary information for its resolution. We will proceed by addressing the three parts in sequence.\n\n### Part 1: Failure of Armijo Condition for Least-Squares Misfit\n\nFirst, we formulate the objective function under the hypothetical scenario where the outlier is also handled with a least-squares (quadratic) loss. The contribution of the outlier residual to the objective function would be of the form $\\frac{1}{2} C r(\\alpha)^2$ for some constant $C$. Consistent with the problem's formulation where the quadratic part is $\\frac{1}{2}K\\alpha^2$, we can model the total objective function $\\Phi(\\alpha)$ along the search direction $p$ as the sum of the provided quadratic part and the quadratic loss from the outlier. To match the structure $\\phi(\\alpha) = f(m+\\alpha p)$, we can write:\n$$\n\\Phi(\\alpha) = \\Phi(0) - A_{\\text{total}} \\alpha + \\frac{1}{2} K_{\\text{total}} \\alpha^2\n$$\nIn our case, the objective function is composed of the well-behaved part and the outlier part assuming a least-squares form $\\frac{1}{2} r(\\alpha)^2$. The line search function is then:\n$$\n\\Phi(\\alpha) = C_{\\text{const}} - A\\,\\alpha+\\frac{1}{2}K\\alpha^{2} + \\frac{1}{2}\\left(r_{0}-s\\,\\alpha\\right)^{2}\n$$\nwhere $C_{\\text{const}}$ includes $\\phi(0)$ from the well-behaved part and $\\frac{1}{2}r_0^2$ from the outlier at $\\alpha=0$. The change from $\\alpha=0$ is:\n$$\n\\Phi(\\alpha) - \\Phi(0) = -A\\,\\alpha+\\frac{1}{2}K\\alpha^{2} + \\frac{1}{2}\\left(r_{0}-s\\,\\alpha\\right)^{2} - \\frac{1}{2}r_0^2\n$$\nThe derivative with respect to $\\alpha$ is:\n$$\n\\Phi'(\\alpha) = -A + K\\alpha + (r_0 - s\\alpha)(-s) = -A + K\\alpha - s(r_0 - s\\alpha)\n$$\nThe derivative at $\\alpha=0$, which determines the initial descent, is:\n$$\n\\Phi'(0) = -A -sr_0\n$$\nThe Armijo-Goldstein sufficient decrease condition is:\n$$\n\\Phi(\\alpha) \\le \\Phi(0) + c_1 \\alpha \\Phi'(0)\n$$\nWe are asked to test this condition for a unit trial step, $\\alpha=1$, with the Armijo constant $c_{1}=10^{-4}$ and the given parameters $A=100$, $K=400$, $r_{0}=10$, and $s=10$.\n\nFirst, let's evaluate the left-hand side (LHS), the actual change in the objective function:\n$$\n\\Phi(1) - \\Phi(0) = -A(1) + \\frac{1}{2}K(1)^2 + \\frac{1}{2}(r_0-s(1))^2 - \\frac{1}{2}r_0^2\n$$\n$$\n\\Phi(1) - \\Phi(0) = -100 + \\frac{1}{2}(400) + \\frac{1}{2}(10-10)^2 - \\frac{1}{2}(10)^2 = -100 + 200 + 0 - 50 = 50\n$$\nThe objective function value *increases* by $50$.\n\nNext, let's evaluate the right-hand side (RHS), which is the sufficient decrease target based on the initial gradient:\n$$\n\\Phi'(0) = -A - sr_0 = -100 - (10)(10) = -200\n$$\n$$\nc_1 \\alpha \\Phi'(0) = (10^{-4})(1)(-200) = -0.02\n$$\nThe Armijo condition becomes:\n$$\n50 \\le -0.02\n$$\nThis inequality is false. The actual function value increased dramatically, while the local model (gradient) predicted a steep decrease. This large discrepancy causes the Armijo condition to fail, and the step $\\alpha=1$ would be rejected. The failure is due to the strong nonlinearity introduced by the large outlier residual whose contribution $(r_0-s\\alpha)^2$ has a large positive curvature, overwhelming the initial descent.\n\n### Part 2: Derivatives of the Robust Objective\n\nThe robust objective function is given as:\n$$\n\\phi(\\alpha)=\\phi(0)-A\\,\\alpha+\\frac{1}{2}K\\alpha^{2}+\\ln\\!\\left(\\frac{1+\\left(r_{0}-s\\alpha\\right)^{2}}{1+r_{0}^{2}}\\right)\n$$\nWe can rewrite this by separating the logarithm:\n$$\n\\phi(\\alpha) = \\phi(0)-A\\,\\alpha+\\frac{1}{2}K\\alpha^{2}+\\ln(1+(r_0-s\\alpha)^2) - \\ln(1+r_0^2)\n$$\nTo find $\\phi'(\\alpha)$, we differentiate with respect to $\\alpha$:\n$$\n\\phi'(\\alpha) = \\frac{d}{d\\alpha} \\left[ -A\\,\\alpha+\\frac{1}{2}K\\alpha^{2}+\\ln(1+(r_0-s\\alpha)^2) \\right]\n$$\nUsing the chain rule for the logarithmic term:\n$$\n\\frac{d}{d\\alpha}\\ln(1+(r_0-s\\alpha)^2) = \\frac{1}{1+(r_0-s\\alpha)^2} \\cdot \\left( 2(r_0-s\\alpha) \\cdot (-s) \\right) = -\\frac{2s(r_0-s\\alpha)}{1+(r_0-s\\alpha)^2}\n$$\nThus, the full derivative is:\n$$\n\\phi'(\\alpha) = -A + K\\alpha - \\frac{2s(r_0-s\\alpha)}{1+(r_0-s\\alpha)^2}\n$$\nEvaluating this at $\\alpha=0$ gives $\\phi'(0)$:\n$$\n\\phi'(0) = -A + K(0) - \\frac{2s(r_0-s(0))}{1+(r_0-s(0))^2} = -A - \\frac{2sr_0}{1+r_0^2}\n$$\n\n### Part 3: Computation of Minimally Restrictive Line Search Parameters\n\nWe need to compute $c_{1}^{\\star}=\\min_{\\alpha\\in[\\alpha_{\\min},\\alpha_{\\max}]}T(\\alpha)$ and $c_{2}^{\\star}=\\max_{\\alpha\\in[\\alpha_{\\min},\\alpha_{\\max}]}L(\\alpha)$ on the interval $[\\alpha_{\\min},\\alpha_{\\max}]=[0.1, 0.25]$.\n\nThe diagnostic functions are $T(\\alpha)=\\frac{\\phi(\\alpha)-\\phi(0)}{\\alpha\\,\\phi'(0)}$ and $L(\\alpha)=\\frac{\\phi'(\\alpha)}{\\phi'(0)}$.\nFirst, we compute the constant value of $\\phi'(0)$ with the given parameters $A=100$, $r_0=10$, $s=10$:\n$$\n\\phi'(0) = -100 - \\frac{2(10)(10)}{1+10^2} = -100 - \\frac{200}{101} = -\\frac{10100+200}{101} = -\\frac{10300}{101}\n$$\nThis value is negative, as expected for a descent direction.\n\n**Analysis of $L(\\alpha)$ to find $c_2^\\star$**\nWe need to find the maximum of $L(\\alpha) = \\frac{\\phi'(\\alpha)}{\\phi'(0)}$ on $[0.1, 0.25]$. The location of the maximum can be found by examining the sign of its derivative, $L'(\\alpha) = \\frac{\\phi''(\\alpha)}{\\phi'(0)}$. Since $\\phi'(0)  0$, the sign of $L'(\\alpha)$ is opposite to the sign of $\\phi''(\\alpha)$.\nLet's compute $\\phi''(\\alpha)$ by differentiating $\\phi'(\\alpha)$:\n$$\n\\phi''(\\alpha) = \\frac{d}{d\\alpha} \\left( -A + K\\alpha - \\frac{2s(r_0-s\\alpha)}{1+(r_0-s\\alpha)^2} \\right) = K - 2s \\frac{d}{d\\alpha} \\left( \\frac{r_0-s\\alpha}{1+(r_0-s\\alpha)^2} \\right)\n$$\nUsing the quotient rule, the derivative of the fraction is:\n$$\n\\frac{-s(1+(r_0-s\\alpha)^2) - (r_0-s\\alpha)(2(r_0-s\\alpha)(-s))}{(1+(r_0-s\\alpha)^2)^2} = \\frac{-s - s(r_0-s\\alpha)^2 + 2s(r_0-s\\alpha)^2}{(1+(r_0-s\\alpha)^2)^2} = \\frac{s((r_0-s\\alpha)^2-1)}{(1+(r_0-s\\alpha)^2)^2}\n$$\nSo, the second derivative is:\n$$\n\\phi''(\\alpha) = K - \\frac{2s^2((r_0-s\\alpha)^2-1)}{(1+(r_0-s\\alpha)^2)^2}\n$$\nLet's evaluate this on the interval $[0.1, 0.25]$. Let $r(\\alpha) = r_0 - s\\alpha = 10-10\\alpha$.\nFor $\\alpha \\in [0.1, 0.25]$, $r(\\alpha)$ is in $[10-2.5, 10-1] = [7.5, 9]$.\nThe term $(r(\\alpha)^2-1)$ is therefore positive, ranging from $7.5^2-1=55.25$ to $9^2-1=80$.\nThe fraction term is $\\frac{2(10)^2(r(\\alpha)^2-1)}{(1+r(\\alpha)^2)^2}$. Its maximum value on the interval occurs at $\\alpha=0.25$ ($r=7.5$): $\\frac{200(55.25)}{(1+56.25)^2} \\approx 3.37$. Its minimum is at $\\alpha=0.1$ ($r=9$): $\\frac{200(80)}{(1+81)^2} \\approx 2.38$.\nSince $K=400$, which is much larger than this fractional term, $\\phi''(\\alpha) = 400 - (\\text{a value between } 2.38 \\text{ and } 3.37)$ is always positive.\nSince $\\phi''(\\alpha)  0$ and $\\phi'(0)  0$, we have $L'(\\alpha) = \\phi''(\\alpha)/\\phi'(0)  0$. This means $L(\\alpha)$ is a strictly decreasing function on $[0.1, 0.25]$.\nTherefore, the maximum of $L(\\alpha)$ occurs at the left endpoint, $\\alpha_{\\min}=0.1$.\n$$\nc_2^\\star = L(0.1) = \\frac{\\phi'(0.1)}{\\phi'(0)}\n$$\n$$\n\\phi'(0.1) = -100 + 400(0.1) - \\frac{2(10)(10-10(0.1))}{1+(10-10(0.1))^2} = -60 - \\frac{20(9)}{1+9^2} = -60 - \\frac{180}{82} = -60 - \\frac{90}{41} = -\\frac{2550}{41}\n$$\n$$\nc_2^\\star = \\frac{-2550/41}{-10300/101} = \\frac{2550 \\times 101}{41 \\times 10300} = \\frac{257550}{422300} \\approx 0.60989817...\n$$\nRounded to four significant figures, $c_2^\\star = 0.6099$.\n\n**Analysis of $T(\\alpha)$ to find $c_1^\\star$**\nWe need to find the minimum of $T(\\alpha) = \\frac{\\phi(\\alpha)-\\phi(0)}{\\alpha\\phi'(0)}$ on $[0.1, 0.25]$. We analyze its derivative:\n$$\nT'(\\alpha) = \\frac{\\phi'(\\alpha)(\\alpha\\phi'(0)) - (\\phi(\\alpha)-\\phi(0))(\\phi'(0))}{(\\alpha\\phi'(0))^2} = \\frac{\\alpha\\phi'(\\alpha) - (\\phi(\\alpha)-\\phi(0))}{\\alpha^2\\phi'(0)}\n$$\nLet $g(\\alpha) = \\alpha\\phi'(\\alpha) - (\\phi(\\alpha)-\\phi(0))$. The sign of $T'(\\alpha)$ is opposite to the sign of $g(\\alpha)$ because $\\phi'(0)0$. The derivative of $g(\\alpha)$ is:\n$$\ng'(\\alpha) = (\\phi'(\\alpha) + \\alpha\\phi''(\\alpha)) - \\phi'(\\alpha) = \\alpha\\phi''(\\alpha)\n$$\nOn the interval $[0.1, 0.25]$, $\\alpha  0$. We already established that $\\phi''(\\alpha)  0$. Therefore, $g'(\\alpha)  0$, meaning $g(\\alpha)$ is a strictly increasing function.\nTo determine the sign of $g(\\alpha)$ on the interval, we check its value at the start of the interval, $\\alpha_{\\min}=0.1$:\n$$\ng(0.1) = 0.1\\phi'(0.1) - (\\phi(0.1)-\\phi(0))\n$$\nWe have $\\phi'(0.1) = -2550/41 \\approx -62.1951$.\n$\\phi(0.1)-\\phi(0) = -100(0.1) + \\frac{1}{2}400(0.1)^2 + \\ln(1+(10-1)^2) - \\ln(1+10^2) = -10 + 2 + \\ln(82) - \\ln(101) = -8 + \\ln(82/101) \\approx -8.2084$.\n$$\ng(0.1) \\approx 0.1(-62.1951) - (-8.2084) = -6.21951 + 8.2084 = 1.98889  0\n$$\nSince $g(\\alpha)$ is increasing and is positive at $\\alpha=0.1$, $g(\\alpha)$ is positive throughout the interval $[0.1, 0.25]$.\nAs $g(\\alpha)0$ and the denominator of $T'(\\alpha)$ is negative, $T'(\\alpha)0$ for all $\\alpha \\in [0.1, 0.25]$. Thus, $T(\\alpha)$ is a strictly decreasing function.\nThe minimum of $T(\\alpha)$ must occur at the right endpoint, $\\alpha_{\\max}=0.25$.\n$$\nc_1^\\star = T(0.25) = \\frac{\\phi(0.25)-\\phi(0)}{0.25\\phi'(0)}\n$$\n$$\n\\phi(0.25)-\\phi(0) = -100(0.25) + \\frac{1}{2}400(0.25)^2 + \\ln(1+(10-2.5)^2) - \\ln(101)\n$$\n$$\n= -25 + 200(0.0625) + \\ln(1+7.5^2) - \\ln(101) = -25 + 12.5 + \\ln(57.25) - \\ln(101) = -12.5 + \\ln(57.25/101)\n$$\nThe denominator is $0.25\\phi'(0) = 0.25(-\\frac{10300}{101}) = -\\frac{2575}{101}$.\n$$\nc_1^\\star = \\frac{-12.5 + \\ln(57.25/101)}{-2575/101} \\approx \\frac{-12.5 - 0.56766345}{-25.4950495} = \\frac{-13.06766345}{-25.4950495} \\approx 0.5125574\n$$\nRounded to four significant figures, $c_1^\\star=0.5126$.\n\nThe minimally restrictive pair $(c_1^\\star, c_2^\\star)$ is $(0.5126, 0.6099)$. Note that the weak Wolfe condition $c_2  c_1$ is satisfied as $0.6099  0.5126$.", "answer": "$$\n\\boxed{\\begin{pmatrix} 0.5126  0.6099 \\end{pmatrix}}\n$$", "id": "3607649"}, {"introduction": "Theory becomes practice when you build the algorithm. This final exercise [@problem_id:3607601] challenges you to implement a complete and robust line search procedure that finds a step length satisfying the strong Wolfe conditions. The focus is on creating an efficient algorithm using a two-phase bracketing-and-zoom strategy, paying close attention to minimizing computationally expensive function and gradient evaluations, a paramount concern in large-scale inverse problems.", "problem": "Consider a nonlinear seismic data misfit of the form $F(m)=\\sum_{i=1}^{Q}\\phi\\big(\\mathcal{F}_i(m)-d_i\\big)$, where $m\\in\\mathbb{R}^n$ denotes the model parameters, $d_i\\in\\mathbb{R}$ denotes observed data for source-receiver experiment $i$, $\\mathcal{F}_i$ denotes the forward map for experiment $i$, and $\\phi$ is a smooth convex function. Assume a differentiable forward operator and differentiable $\\phi$. In practical large-scale computational geophysics, evaluating the forward operator $\\mathcal{F}_i$ and its adjoint-state for a given model $m$ is computationally expensive. A line search algorithm for determining the step length $\\alpha$ must minimize the number of forward and adjoint evaluations while satisfying the strong Wolfe conditions along a search direction $p$. The strong Wolfe conditions comprise a sufficient decrease condition and a curvature condition along the scalarized function $\\varphi(\\alpha)=F(m+\\alpha p)$.\n\nImplement a bracketing and zoom procedure to find a step length $\\alpha0$ that satisfies strong Wolfe conditions along the search direction $p$ with minimal forward and adjoint evaluations, under the following synthetic but scientifically consistent setting that emulates nonlinear seismic modeling:\n\n- Use the synthetic nonlinear forward operator per experiment $i$ defined by $\\mathcal{F}_i(m)=\\sin(a_i^\\top m)$, where $a_i\\in\\mathbb{R}^n$ is a fixed vector. Use the smooth convex penalty $\\phi(y)=\\tfrac{1}{2}y^2$, so the misfit is $F(m)=\\tfrac{1}{2}\\sum_{i=1}^{Q} \\left(\\sin(a_i^\\top m)-d_i\\right)^2$.\n\n- Define a computational cost model to track expensive evaluations:\n    - A single evaluation of $F(m)$ at any model $m$ counts as $1$ \"forward solve\".\n    - A single evaluation of the gradient $g(m)=\\nabla F(m)$ at any model $m$ counts as $1$ \"adjoint solve\". If the gradient evaluation requires evaluating the forward state that was not already computed for that specific $m$, it also requires $1$ \"forward solve\". If the forward state for that $m$ was already computed (for example, $F(m)$ was just evaluated), then reusing it avoids the extra forward solve, and only the adjoint solve is counted.\n    - The scalarized function along the line is $\\varphi(\\alpha)=F(m+\\alpha p)$ and its directional derivative is $\\varphi'(\\alpha)=g(m+\\alpha p)^\\top p$.\n\n- Implement a bracketing strategy with an initial trial $\\alpha_00$ and an expansion rule to find an interval $[\\alpha_{\\mathrm{lo}},\\alpha_{\\mathrm{hi}}]$ that contains a step satisfying strong Wolfe conditions, using only as many gradient evaluations as necessary. Specifically, the algorithm must:\n    - Evaluate only $F$ values while expanding the bracket, and compute the directional derivative $\\varphi'(\\alpha)$ only when the sufficient decrease condition is met or when needed to decide bracket updates.\n    - Reuse previously computed forward states for the same model to avoid extra forward solves when computing $\\varphi'(\\alpha)$ at the same $\\alpha$.\n\n- Implement a zoom procedure on the bracket to hone in on an $\\alpha$ satisfying strong Wolfe conditions. The zoom must:\n    - Propose new $\\alpha$ within the bracket using a robust strategy (bisection is acceptable).\n    - Evaluate $F$ first to check sufficient decrease, and only then evaluate the directional derivative if necessary to check the curvature condition.\n    - Reuse forward states across $F$ and $g$ evaluations for the same $\\alpha$.\n    - Terminate with an $\\alpha$ that satisfies strong Wolfe conditions or return a reasonable fallback (e.g., the current best bracket value) if the maximum number of iterations is reached.\n\n- Define the gradient for the synthetic model by differentiating $F(m)$: given $z_i=a_i^\\top m$, the residual $r_i=\\sin(z_i)-d_i$, and using the chain rule, compute $g(m)=\\sum_{i=1}^Q r_i\\cos(z_i)\\,a_i$.\n\n- If the initial direction is not a descent direction, that is, if $g(m_0)^\\top p\\ge 0$, return $0.0$ for that test case.\n\nUse the following test suite. In all cases use $c_1=10^{-4}$ and $c_2=0.9$, dimensionless $\\alpha$, angles in radians, and no physical units are required. For clarity, $A$ denotes the matrix with rows $a_i^\\top$.\n\nTest case 1 (happy path, moderate nonlinearity):\n- $n=3$, $Q=6$,\n- $A=\\begin{bmatrix}\n0.8  -0.3  0.5\\\\\n1.2  0.4  -0.6\\\\\n-0.7  0.9  1.1\\\\\n0.3  -1.5  0.2\\\\\n1.0  0.0  -0.9\\\\\n0.6  0.8  -0.4\n\\end{bmatrix}$,\n- $d=\\begin{bmatrix}0.2  -0.5  0.7  -0.1  0.3  -0.4\\end{bmatrix}^\\top$,\n- $m_0=\\begin{bmatrix}0.5  -0.8  0.3\\end{bmatrix}^\\top$,\n- Initial guess $\\alpha_0=1.0$,\n- Search direction $p=-g(m_0)$ (steepest descent).\n\nTest case 2 (initial trial already good, small step):\n- $n=3$, $Q=5$,\n- $A=\\begin{bmatrix}\n0.4  -0.2  0.1\\\\\n0.9  0.3  -0.5\\\\\n-0.5  0.7  0.9\\\\\n0.2  -1.1  0.1\\\\\n0.7  -0.1  -0.6\n\\end{bmatrix}$,\n- $d=\\begin{bmatrix}0.1  -0.3  0.5  -0.05  0.2\\end{bmatrix}^\\top$,\n- $m_0=\\begin{bmatrix}0.2  -0.4  0.1\\end{bmatrix}^\\top$,\n- Initial guess $\\alpha_0=0.05$,\n- Search direction $p=-g(m_0)$.\n\nTest case 3 (large initial step, strong bracket and zoom needed):\n- $n=3$, $Q=7$,\n- $A=\\begin{bmatrix}\n1.5  -0.5  0.2\\\\\n0.7  0.6  -0.9\\\\\n-1.0  1.4  0.8\\\\\n0.5  -1.2  0.3\\\\\n1.1  0.2  -1.0\\\\\n0.3  0.9  -0.2\\\\\n-0.8  0.4  1.3\n\\end{bmatrix}$,\n- $d=\\begin{bmatrix}0.3  -0.6  0.9  -0.2  0.4  -0.3  0.5\\end{bmatrix}^\\top$,\n- $m_0=\\begin{bmatrix}0.6  -0.7  0.2\\end{bmatrix}^\\top$,\n- Initial guess $\\alpha_0=5.0$,\n- Search direction $p=-g(m_0)$.\n\nTest case 4 (non-descent direction; algorithm must return $0.0$):\n- Use the same $A$, $d$, and $m_0$ as in Test case 1,\n- Initial guess $\\alpha_0=1.0$,\n- Search direction $p=+g(m_0)$.\n\nAlgorithmic constraints and outputs:\n- Implement a bracketing phase with geometric expansion of $\\alpha$ until a bracket is found or a maximum $\\alpha_{\\max}$ is reached (use $\\alpha_{\\max}=50$).\n- Implement a zoom phase using bisection with a maximum of $50$ iterations.\n- Minimize forward and adjoint evaluations by reusing computations at identical $\\alpha$ when checking conditions.\n- For each test case, output the resulting step length $\\alpha$ as a floating-point number.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3,result4]\").\n\nNo external input is permitted; all parameters are fixed in the program. Angles in the trigonometric functions must be interpreted in radians. The step length $\\alpha$ is dimensionless. The program must adhere to the specified runtime environment.", "solution": "The problem requires the implementation of a line search algorithm to find a step length $\\alpha$ that satisfies the strong Wolfe conditions for a nonlinear optimization problem arising in computational geophysics. The implementation must be efficient, minimizing the number of computationally expensive function and gradient evaluations. The problem is well-defined, scientifically grounded in the principles of numerical optimization and inverse problems, and provides a complete set of specifications for a synthetic model, algorithmic constraints, and test cases.\n\nThe objective function, or misfit, is given by\n$$\nF(m) = \\frac{1}{2}\\sum_{i=1}^{Q} \\left(\\sin(a_i^\\top m)-d_i\\right)^2\n$$\nwhere $m \\in \\mathbb{R}^n$ is the vector of model parameters, $A$ is a $Q \\times n$ matrix whose rows are the vectors $a_i^\\top$, and $d \\in \\mathbb{R}^Q$ is the vector of observed data. This is a nonlinear least-squares problem. The function is differentiable, and its gradient, derived using the chain rule, is\n$$\ng(m) = \\nabla F(m) = \\sum_{i=1}^{Q} \\left(\\sin(a_i^\\top m)-d_i\\right) \\cos(a_i^\\top m) a_i = A^\\top (\\mathbf{r} \\odot \\cos(\\mathbf{z}))\n$$\nwhere $\\mathbf{z} = Am$, $\\mathbf{r} = \\sin(\\mathbf{z}) - d$, and $\\odot$ denotes the element-wise (Hadamard) product.\n\nGiven a current model estimate $m_k$ and a search direction $p_k$, a line search seeks to find a step length $\\alpha  0$ that minimizes the scalar function $\\varphi(\\alpha) = F(m_k + \\alpha p_k)$. The derivative of this function with respect to $\\alpha$ is $\\varphi'(\\alpha) = \\nabla F(m_k + \\alpha p_k)^\\top p_k = g(m_k + \\alpha p_k)^\\top p_k$. For the search direction to be effective, it must be a descent direction, i.e., $p_k$ must satisfy $g(m_k)^\\top p_k  0$, which implies $\\varphi'(0)  0$. If this condition is not met, no improvement is guaranteed, and the line search should terminate, an eventuality handled by returning $\\alpha=0$.\n\nThe strong Wolfe conditions provide a standard and robust set of criteria for accepting a step length $\\alpha$. They ensure both a sufficient decrease in the objective function and a sufficient decrease in the magnitude of the projected gradient. The conditions are:\n$1$. **Sufficient Decrease (Armijo) Condition**:\n$$\n\\varphi(\\alpha) \\le \\varphi(0) + c_1 \\alpha \\varphi'(0)\n$$\n$2$. **Strong Curvature Condition**:\n$$\n|\\varphi'(\\alpha)| \\le c_2 |\\varphi'(0)|\n$$\nThe constants $c_1$ and $c_2$ are typically small positive numbers satisfying $0  c_1  c_2  1$. For this problem, they are specified as $c_1 = 10^{-4}$ and $c_2 = 0.9$.\n\nThe core of the task is to implement a two-phase algorithm, bracketing followed by a zoom, to find such an $\\alpha$. This algorithm is designed to be parsimonious with function and gradient evaluations, which is critical in large-scale applications like seismic inversion. A key implementation detail is to cache the results of forward ($F(m)$) and adjoint ($g(m)$) solves for a given model $m$, or equivalently, for a given step length $\\alpha$. An evaluation of $\\varphi(\\alpha)$ requires a forward solve. An evaluation of $\\varphi'(\\alpha)$ requires an adjoint solve, which in turn requires the result of the forward solve at the same point. By caching the pair $(\\varphi(\\alpha), \\varphi'(\\alpha))$, we perform the expensive forward-adjoint pair computation only once per unique value of $\\alpha$.\n\n**Phase 1: Bracketing**\nThe bracketing phase seeks an interval $[\\alpha_{\\mathrm{lo}}, \\alpha_{\\mathrm{hi}}]$ that is guaranteed to contain a point satisfying the strong Wolfe conditions. Starting with an initial guess $\\alpha_0  0$, we generate a sequence of trial step lengths.\nLet $\\alpha_{i-1}$ be the previous step length (initially $0$) and $\\alpha_i$ be the current one.\n$1$. Evaluate $\\varphi(\\alpha_i)$. If it violates the sufficient decrease condition, or if $\\varphi(\\alpha_i) \\ge \\varphi(\\alpha_{i-1})$, then a region of non-decrease has been found. The desired step length must lie in the interval $[\\alpha_{i-1}, \\alpha_i]$. We proceed to the zoom phase with this bracket.\n$2$. If sufficient decrease holds, we evaluate $\\varphi'(\\alpha_i)$.\n$3$. If $|\\varphi'(\\alpha_i)| \\le c_2 |\\varphi'(0)|$, the strong Wolfe conditions are met, and we terminate with $\\alpha = \\alpha_i$.\n$4$. If $\\varphi'(\\alpha_i) \\ge 0$, the function has started to increase, so a local minimum must be in $[\\alpha_{i-1}, \\alpha_i]$. We proceed to the zoom phase.\n$5$. If none of the above hold (sufficient decrease holds but the gradient magnitude is still too large and negative), we must expand the search region. We set $\\alpha_{i-1} \\leftarrow \\alpha_i$ and choose a new, larger $\\alpha_i$, for example, by geometric expansion $\\alpha_i \\leftarrow \\rho \\alpha_{i-1}$ (with $\\rho=2$ in this implementation), up to a specified maximum $\\alpha_{\\max} = 50$.\n\n**Phase 2: Zoom**\nOnce a bracket $[\\alpha_{\\mathrm{lo}}, \\alpha_{\\mathrm{hi}}]$ is identified, the zoom phase refines it to find a suitable $\\alpha$. The bracket is guaranteed to have the property that $\\alpha_{\\mathrm{lo}}$ satisfies the sufficient decrease condition and has a negative directional derivative $\\varphi'(\\alpha_{\\mathrm{lo}})  0$.\nThe zoom procedure iteratively reduces the size of the bracket:\n$1$. A trial step $\\alpha_j$ is chosen strictly inside the current bracket $(\\alpha_{\\mathrm{lo}}, \\alpha_{\\mathrm{hi}})$. Bisection, $\\alpha_j = (\\alpha_{\\mathrm{lo}} + \\alpha_{\\mathrm{hi}})/2$, is a robust choice.\n$2$. Evaluate $\\varphi(\\alpha_j)$. If $\\varphi(\\alpha_j)$ violates sufficient decrease or if $\\varphi(\\alpha_j) \\ge \\varphi(\\alpha_{\\mathrm{lo}})$, the new upper bound becomes $\\alpha_j$ (i.e., set $\\alpha_{\\mathrm{hi}} \\leftarrow \\alpha_j$). The lower bound remains unchanged.\n$3$. If $\\varphi(\\alpha_j)$ satisfies sufficient decrease, we evaluate $\\varphi'(\\alpha_j)$.\n$4$. If the strong curvature condition $|\\varphi'(\\alpha_j)| \\le c_2 |\\varphi'(0)|$ is met, we have found our step, and the algorithm terminates returning $\\alpha_j$.\n$5$. If the curvature condition is not met, we must update the bracket. If $\\varphi'(\\alpha_j)(\\alpha_{\\mathrm{hi}} - \\alpha_{\\mathrm{lo}}) \\ge 0$, we set $\\alpha_{\\mathrm{hi}} \\leftarrow \\alpha_{\\mathrm{lo}}$. In all other cases under this branch, we set $\\alpha_{\\mathrm{lo}} \\leftarrow \\alpha_j$.\nThis process guarantees that the new bracket $[\\alpha_{\\mathrm{lo}}, \\alpha_{\\mathrm{hi}}]$ continues to hold an acceptable point and its size is reduced, ensuring convergence. If the maximum number of iterations ($50$) is reached, the algorithm returns the last found value of $\\alpha_j$ that satisfied sufficient decrease, which is stored in $\\alpha_{\\mathrm{lo}}$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\nclass WolfeLineSearch:\n    \"\"\"\n    Implements a line search algorithm satisfying strong Wolfe conditions.\n    The algorithm uses a bracketing and zoom procedure as described in\n    Nocedal  Wright, \"Numerical Optimization\". It's designed to minimize\n    the number of function and gradient evaluations.\n    \"\"\"\n    def __init__(self, c1=1e-4, c2=0.9):\n        self.c1 = c1\n        self.c2 = c2\n        self.cache = {}\n        self.m0 = None\n        self.p = None\n        self.A = None\n        self.d = None\n\n    def _eval(self, alpha):\n        \"\"\"\n        Evaluates the objective function F and its gradient g at m0 + alpha*p.\n        This function emulates the computational cost model by computing both\n        F (forward solve) and g (adjoint solve) and caching the result for\n        a given alpha.\n        \"\"\"\n        if alpha in self.cache:\n            return self.cache[alpha]\n\n        m_new = self.m0 + alpha * self.p\n        \n        # Forward state computation (part of F and needed for g)\n        z = self.A @ m_new\n        sin_z = np.sin(z)\n        r = sin_z - self.d\n        \n        # F evaluation\n        f_val = 0.5 * np.sum(r**2)\n        \n        # g evaluation (uses forward state)\n        cos_z = np.cos(z)\n        g_val_sum_term = r * cos_z\n        g_val = self.A.T @ g_val_sum_term\n        \n        self.cache[alpha] = (f_val, g_val)\n        return f_val, g_val\n\n    def _phi(self, alpha):\n        f_val, _ = self._eval(alpha)\n        return f_val\n\n    def _phi_prime(self, alpha):\n        _, g_val = self._eval(alpha)\n        return np.dot(g_val, self.p)\n\n    def _zoom(self, alpha_lo, alpha_hi, phi0, phi_prime0, max_iter):\n        \"\"\"\n        Zoom phase to find a point satisfying strong Wolfe conditions\n        within a bracket [alpha_lo, alpha_hi].\n        \"\"\"\n        phi_lo = self._phi(alpha_lo)\n\n        for _ in range(max_iter):\n            # Use bisection for robustness\n            alpha_j = (alpha_lo + alpha_hi) / 2\n            \n            # Check for very small bracket\n            if abs(alpha_lo - alpha_hi)  1e-12:\n                return alpha_lo\n\n            phi_j = self._phi(alpha_j)\n\n            if (phi_j > phi0 + self.c1 * alpha_j * phi_prime0) or (phi_j >= phi_lo):\n                alpha_hi = alpha_j\n            else:\n                phi_prime_j = self._phi_prime(alpha_j)\n                if abs(phi_prime_j) = self.c2 * abs(phi_prime0):\n                    return alpha_j  # Success: found a valid step\n                \n                if phi_prime_j * (alpha_hi - alpha_lo) >= 0:\n                    alpha_hi = alpha_lo\n                \n                alpha_lo = alpha_j\n                phi_lo = phi_j\n        \n        return alpha_lo  # Fallback: return best found so far\n\n    def search(self, m0, p, A, d, alpha_0=1.0, alpha_max=50.0, max_iter_bracket=20, max_iter_zoom=50):\n        \"\"\"\n        Main line search routine combining bracketing and zoom phases.\n        \"\"\"\n        self.m0, self.p, self.A, self.d = m0, p, A, d\n        self.cache = {}\n\n        # Initial evaluations at alpha = 0\n        phi0 = self._phi(0)\n        phi_prime0 = self._phi_prime(0)\n\n        if phi_prime0 >= 0:\n            # Not a descent direction\n            return 0.0\n\n        # Bracketing Phase\n        alpha_prev = 0.0\n        phi_prev = phi0\n        alpha_i = alpha_0\n        \n        for i in range(1, max_iter_bracket + 1):\n            phi_i = self._phi(alpha_i)\n\n            if (phi_i > phi0 + self.c1 * alpha_i * phi_prime0) or (i > 1 and phi_i >= phi_prev):\n                return self._zoom(alpha_prev, alpha_i, phi0, phi_prime0, max_iter_zoom)\n\n            phi_prime_i = self._phi_prime(alpha_i)\n\n            if abs(phi_prime_i) = self.c2 * abs(phi_prime0):\n                return alpha_i  # Success: found a valid step\n\n            if phi_prime_i >= 0:\n                return self._zoom(alpha_i, alpha_prev, phi0, phi_prime0, max_iter_zoom)\n\n            if alpha_i >= alpha_max:\n                return alpha_i # Hit max step length, return it as fallback.\n\n            # Expand search interval\n            alpha_prev = alpha_i\n            phi_prev = phi_i\n            alpha_i = min(2.0 * alpha_i, alpha_max)\n        \n        return alpha_i # Fallback if bracketing iterations run out\n\n\ndef g_m(m, A, d):\n    \"\"\"Computes the gradient of the misfit function F at model m.\"\"\"\n    z = A @ m\n    sin_z = np.sin(z)\n    r = sin_z - d\n    cos_z = np.cos(z)\n    g_val_sum_term = r * cos_z\n    return A.T @ g_val_sum_term\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"A\": np.array([\n                [0.8, -0.3, 0.5], [1.2, 0.4, -0.6], [-0.7, 0.9, 1.1],\n                [0.3, -1.5, 0.2], [1.0, 0.0, -0.9], [0.6, 0.8, -0.4]\n            ]),\n            \"d\": np.array([0.2, -0.5, 0.7, -0.1, 0.3, -0.4]),\n            \"m0\": np.array([0.5, -0.8, 0.3]),\n            \"alpha0\": 1.0,\n            \"p_mode\": \"steepest_descent\"\n        },\n        {\n            \"A\": np.array([\n                [0.4, -0.2, 0.1], [0.9, 0.3, -0.5], [-0.5, 0.7, 0.9],\n                [0.2, -1.1, 0.1], [0.7, -0.1, -0.6]\n            ]),\n            \"d\": np.array([0.1, -0.3, 0.5, -0.05, 0.2]),\n            \"m0\": np.array([0.2, -0.4, 0.1]),\n            \"alpha0\": 0.05,\n            \"p_mode\": \"steepest_descent\"\n        },\n        {\n            \"A\": np.array([\n                [1.5, -0.5, 0.2], [0.7, 0.6, -0.9], [-1.0, 1.4, 0.8],\n                [0.5, -1.2, 0.3], [1.1, 0.2, -1.0], [0.3, 0.9, -0.2],\n                [-0.8, 0.4, 1.3]\n            ]),\n            \"d\": np.array([0.3, -0.6, 0.9, -0.2, 0.4, -0.3, 0.5]),\n            \"m0\": np.array([0.6, -0.7, 0.2]),\n            \"alpha0\": 5.0,\n            \"p_mode\": \"steepest_descent\"\n        },\n        {\n            \"A\": np.array([\n                [0.8, -0.3, 0.5], [1.2, 0.4, -0.6], [-0.7, 0.9, 1.1],\n                [0.3, -1.5, 0.2], [1.0, 0.0, -0.9], [0.6, 0.8, -0.4]\n            ]),\n            \"d\": np.array([0.2, -0.5, 0.7, -0.1, 0.3, -0.4]),\n            \"m0\": np.array([0.5, -0.8, 0.3]),\n            \"alpha0\": 1.0,\n            \"p_mode\": \"ascent\"\n        }\n    ]\n\n    c1 = 1e-4\n    c2 = 0.9\n    searcher = WolfeLineSearch(c1=c1, c2=c2)\n    results = []\n\n    for case in test_cases:\n        m0 = case[\"m0\"]\n        A = case[\"A\"]\n        d = case[\"d\"]\n        alpha0 = case[\"alpha0\"]\n        \n        g0 = g_m(m0, A, d)\n        \n        if case[\"p_mode\"] == \"steepest_descent\":\n            p = -g0\n        else: # ascent\n            p = g0\n            \n        alpha = searcher.search(m0=m0, p=p, A=A, d=d, alpha_0=alpha0)\n        results.append(alpha)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.8f}' for r in results)}]\")\n\nsolve()\n```", "id": "3607601"}]}