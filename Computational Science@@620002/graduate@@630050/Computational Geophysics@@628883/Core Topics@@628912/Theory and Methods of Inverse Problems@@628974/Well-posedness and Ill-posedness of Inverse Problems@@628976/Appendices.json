{"hands_on_practices": [{"introduction": "A crucial first step in solving any inverse problem is to define a meaningful objective function that properly accounts for observational noise. When noise is not simple, independent, and identically distributed—a common scenario in geophysics—a standard least-squares approach is suboptimal. This foundational exercise [@problem_id:3618824] guides you through deriving the correct objective function from the principle of maximum likelihood for correlated Gaussian noise, revealing its connection to weighted least squares and demonstrating how a \"whitening\" transformation can simplify the problem back to a more familiar form.", "problem": "In a linearized computational geophysics inverse problem for travel-time tomography, the observed data vector is modeled as $d \\in \\mathbb{R}^{N}$ and the unknown model parameters are $m \\in \\mathbb{R}^{M}$. The linearized forward operator is $G \\in \\mathbb{R}^{N \\times M}$, and the data obey the additive noise model $d = G m + n$, where the noise $n$ is zero-mean Gaussian with heteroscedastic and correlated components. Specifically, $n$ is distributed as a multivariate Gaussian with mean $0$ and a known symmetric positive definite covariance matrix $C_{n} \\in \\mathbb{R}^{N \\times N}$. Assume $C_{n}$ is strictly positive definite and $G$ has full column rank.\n\nStarting only from the definition of the multivariate Gaussian probability density for the data given the model and the standard least-squares setup, perform the following:\n\n1. Derive the negative log-likelihood (up to an additive constant independent of $m$) and show that it defines a weighted least-squares misfit functional for $m$.\n2. Compute the first-order optimality condition for the maximum likelihood estimate, and express the resulting normal equations in terms of $G$, $d$, and $C_{n}$.\n3. Introduce a data preconditioning, also known as whitening, by an invertible matrix $W \\in \\mathbb{R}^{N \\times N}$ applied to the residual so that the transformed noise has identity covariance. Derive a condition that $W$ must satisfy in terms of $C_{n}$ and then express an explicit closed-form choice of $W$ in terms of $C_{n}$ only, using standard matrix functions.\n4. Show that with this choice, the misfit becomes the squared Euclidean norm of the whitened residual and write the corresponding whitened normal equations in terms of the transformed operator and data.\n\nReport only the analytic expression for the whitening matrix $W$ in your final answer. No numerical evaluation is required. If you use any acronym, define it on its first appearance. Do not include units. The final answer must be a single closed-form analytic expression.", "solution": "The problem statement constitutes a well-posed theoretical exercise in linear inverse theory, specifically in the context of maximum likelihood estimation for a linear model with correlated Gaussian noise. All provided information is self-contained, scientifically grounded in statistics and computational geophysics, and sufficient for a unique and meaningful derivation of the requested quantities. The problem is therefore valid.\n\nThe solution proceeds by addressing each of the four requested parts in sequence.\n\n1. Derivation of the Negative Log-Likelihood\n\nThe problem specifies a linear forward model with additive noise:\n$$d = Gm + n$$\nwhere $d \\in \\mathbb{R}^{N}$ is the data vector, $m \\in \\mathbb{R}^{M}$ is the model parameter vector, $G \\in \\mathbb{R}^{N \\times M}$ is the forward operator, and $n \\in \\mathbb{R}^{N}$ is the noise vector. The noise $n$ follows a multivariate Gaussian distribution with mean $0$ and a known symmetric positive definite (SPD) covariance matrix $C_{n} \\in \\mathbb{R}^{N \\times N}$. The probability density function (PDF) for the noise vector $n$ is given by:\n$$p(n) = \\frac{1}{\\sqrt{(2\\pi)^{N} \\det(C_{n})}} \\exp\\left(-\\frac{1}{2} n^T C_{n}^{-1} n\\right)$$\nFrom the model equation, we can express the noise as the data residual, $n = d - Gm$. The likelihood of observing the data $d$ given a model $m$, denoted $L(m) = p(d|m)$, is obtained by substituting $n = d - Gm$ into the PDF for the noise. This is because the distribution of $d$ is simply the distribution of $n$ shifted by the mean $Gm$.\n$$L(m) = p(d|m) = \\frac{1}{\\sqrt{(2\\pi)^{N} \\det(C_{n})}} \\exp\\left(-\\frac{1}{2} (d - Gm)^T C_{n}^{-1} (d - Gm)\\right)$$\nTo find the maximum likelihood estimate of $m$, it is convenient to work with the logarithm of the likelihood function, known as the log-likelihood, $\\ell(m)$:\n$$\\ell(m) = \\ln(L(m)) = -\\frac{N}{2}\\ln(2\\pi) - \\frac{1}{2}\\ln(\\det(C_{n})) - \\frac{1}{2} (d - Gm)^T C_{n}^{-1} (d - Gm)$$\nMaximizing $\\ell(m)$ is equivalent to minimizing its negative, the negative log-likelihood (NLL). We define a misfit functional, $\\mathcal{L}(m)$, as the NLL. Up to an additive constant that is independent of the model parameters $m$, this functional is:\n$$\\mathcal{L}(m) = \\frac{1}{2} (d - Gm)^T C_{n}^{-1} (d - Gm)$$\nThis functional has the form of a weighted least-squares problem. The term $(d-Gm)$ is the residual vector. The expression is a quadratic form in the residual, where the weighting matrix is the inverse of the noise covariance matrix, $C_{n}^{-1}$. This matrix accounts for both the varying variance (heteroscedasticity) and the correlation between data points.\n\n2. Derivation of the Normal Equations\n\nThe maximum likelihood estimate for $m$, denoted $m_{ML}$, is the value of $m$ that minimizes the misfit functional $\\mathcal{L}(m)$. The first-order optimality condition for this minimization problem requires that the gradient of $\\mathcal{L}(m)$ with respect to $m$ vanishes:\n$$\\nabla_{m} \\mathcal{L}(m) = 0$$\nTo compute this gradient, we first expand the quadratic form for $\\mathcal{L}(m)$:\n$$\\mathcal{L}(m) = \\frac{1}{2} (d^T - m^T G^T) C_{n}^{-1} (d - Gm)$$\n$$\\mathcal{L}(m) = \\frac{1}{2} (d^T C_{n}^{-1} d - d^T C_{n}^{-1} Gm - m^T G^T C_{n}^{-1} d + m^T G^T C_{n}^{-1} Gm)$$\nSince $C_{n}$ is symmetric, its inverse $C_{n}^{-1}$ is also symmetric. The term $d^T C_{n}^{-1} Gm$ is a scalar, so its transpose is equal to itself: $(d^T C_{n}^{-1} Gm)^T = m^T G^T (C_{n}^{-1})^T d = m^T G^T C_{n}^{-1} d$. Thus, the two cross-terms are identical.\n$$\\mathcal{L}(m) = \\frac{1}{2} d^T C_{n}^{-1} d - m^T G^T C_{n}^{-1} d + \\frac{1}{2} m^T (G^T C_{n}^{-1} G) m$$\nNow, we take the gradient with respect to $m$. The first term is constant with respect to $m$. For the second and third terms, we use the standard matrix calculus identities $\\nabla_{x} (x^T a) = a$ and $\\nabla_{x} (\\frac{1}{2} x^T A x) = Ax$ for a symmetric matrix $A$. The matrix $G^T C_{n}^{-1} G$ is symmetric.\n$$\\nabla_{m} \\mathcal{L}(m) = - G^T C_{n}^{-1} d + (G^T C_{n}^{-1} G) m$$\nSetting the gradient to zero gives the first-order optimality condition:\n$$(G^T C_{n}^{-1} G) m = G^T C_{n}^{-1} d$$\nThis is the system of normal equations for the maximum likelihood estimate. Since $G$ has full column rank and $C_n^{-1}$ is positive definite, the matrix $G^T C_n^{-1} G$ is invertible, guaranteeing a unique solution for $m_{ML}$.\n\n3. Derivation of the Whitening Transformation\n\nData preconditioning, or whitening, aims to transform the problem into an equivalent one where the noise has an identity covariance matrix. Let $W \\in \\mathbb{R}^{N \\times N}$ be an invertible matrix. We define a transformed noise vector $n' = Wn$. The covariance matrix of the transformed noise is:\n$$\\text{Cov}(n') = \\text{Cov}(Wn) = W \\text{Cov}(n) W^T$$\nGiven that $\\text{Cov}(n) = C_{n}$, the condition for the transformed noise to have an identity covariance matrix, $\\text{Cov}(n') = I$, is:\n$$W C_{n} W^T = I$$\nTo derive an explicit form for $W$, we can utilize the properties of the SPD matrix $C_{n}$. Since $C_{n}$ is SPD, it has a unique SPD inverse $C_{n}^{-1}$ and a unique SPD square root $C_{n}^{1/2}$. Let us propose a choice for $W$ and verify it. A standard choice for the whitening matrix is the inverse of the symmetric square root of the covariance matrix, or more directly, the symmetric square root of the inverse covariance matrix. Let's choose:\n$$W = C_{n}^{-1/2}$$\nwhere $C_{n}^{-1/2}$ is the unique SPD matrix such that $(C_{n}^{-1/2})^2 = C_{n}^{-1}$. By definition, this matrix is symmetric, so $W^T = (C_{n}^{-1/2})^T = C_{n}^{-1/2} = W$. Substituting this choice into the condition:\n$$W C_{n} W^T = C_{n}^{-1/2} C_{n} C_{n}^{-1/2} = C_{n}^{-1/2} (C_{n}^{1/2} C_{n}^{1/2}) C_{n}^{-1/2} = (C_{n}^{-1/2} C_{n}^{1/2}) (C_{n}^{1/2} C_{n}^{-1/2}) = I \\cdot I = I$$\nThe condition is satisfied. Thus, an explicit closed-form choice for the whitening matrix is $W=C_{n}^{-1/2}$. Another valid choice could be derived from the Cholesky decomposition of $C_n$, but the matrix square root is a more direct representation as a matrix function of $C_n$.\n\n4. The Whitened Misfit and Normal Equations\n\nWith the whitening matrix $W = C_{n}^{-1/2}$, we can rewrite the misfit functional $\\mathcal{L}(m)$. The inverse covariance matrix is $C_{n}^{-1} = (C_{n}^{-1/2})^2 = W^2$. Since $W$ is symmetric, $W^2 = W^T W$.\n$$\\mathcal{L}(m) = \\frac{1}{2} (d - Gm)^T C_{n}^{-1} (d - Gm) = \\frac{1}{2} (d - Gm)^T W^T W (d - Gm)$$\nUsing the property of transposes, this is:\n$$\\mathcal{L}(m) = \\frac{1}{2} (W(d - Gm))^T (W(d - Gm))$$\nLet's define the whitened data vector $d' = Wd$ and the whitened forward operator $G' = WG$. The expression inside the parentheses becomes $Wd - WGm = d' - G'm$. The misfit functional is now:\n$$\\mathcal{L}(m) = \\frac{1}{2} (d' - G'm)^T (d' - G'm) = \\frac{1}{2} \\|d' - G'm\\|_{2}^{2}$$\nThis demonstrates that the original weighted least-squares misfit is equivalent to one-half the squared Euclidean norm of the whitened residual. This is the standard objective function for an ordinary least-squares problem involving the whitened variables $d'$ and $G'$.\nThe corresponding normal equations for this ordinary least-squares problem are readily written as:\n$$(G')^T G' m = (G')^T d'$$\nThese are the whitened normal equations. Substituting the definitions of $G'$ and $d'$ and the properties of $W$ verifies their equivalence to the original normal equations:\n$$(WG)^T (WG) m = (WG)^T (Wd)$$\n$$G^T W^T W G m = G^T W^T W d$$\n$$G^T C_{n}^{-1} G m = G^T C_{n}^{-1} d$$\nThis re-derives the result from part 2, confirming the validity of the whitening transformation. The problem is transformed into a standard least-squares problem, which is often numerically more stable and simpler to solve.\n\nThe final answer requested is the analytic expression for the whitening matrix $W$. Based on the derivation in part 3, this is the principal square root of the inverse noise covariance matrix.", "answer": "$$ \\boxed{C_{n}^{-\\frac{1}{2}}} $$", "id": "3618824"}, {"introduction": "Once an inverse problem is formulated, how can we diagnose its inherent stability before attempting a solution? This practice [@problem_id:3618830] moves from pure theory to computational diagnosis, using the singular value decomposition (SVD) as a powerful lens to inspect the structure of a linear operator. You will construct a discrete ray transform for a tomography experiment and numerically analyze its singular value spectrum to see directly how ill-posedness manifests as an amplification of data error, particularly as the model becomes more detailed than the data can support.", "problem": "Consider two-dimensional, straight-ray travel-time tomography in a rectangular domain of width $L_x$ and height $L_y$. Let the model be the cellwise constant slowness field $m \\in \\mathbb{R}^n$ with units seconds per kilometer ($\\mathrm{s}/\\mathrm{km}$), discretized on an $N_x \\times N_y$ uniform grid so that $n = N_x N_y$. For a fixed survey geometry of sources and receivers on the domain boundaries, the data $d \\in \\mathbb{R}^m$ are the travel times in seconds ($\\mathrm{s}$) between each source-receiver pair under the straight-ray approximation. The discrete ray transform $R \\in \\mathbb{R}^{m \\times n}$ maps slowness to data by linear superposition of intersection lengths, i.e., each entry $R_{ij}$ equals the length in kilometers of the $i$-th ray inside the $j$-th cell. The forward model is\n$$\nd = R m.\n$$\nDefine the singular value decomposition (SVD) as the factorization\n$$\nR = U \\Sigma V^\\top,\n$$\nwhere $U \\in \\mathbb{R}^{m \\times r}$ and $V \\in \\mathbb{R}^{n \\times r}$ have orthonormal columns, $r = \\min(m,n)$, and $\\Sigma = \\mathrm{diag}(\\sigma_1, \\sigma_2, \\ldots, \\sigma_r)$ has nonnegative singular values $\\sigma_1 \\ge \\sigma_2 \\ge \\cdots \\ge \\sigma_r \\ge 0$ with units kilometers ($\\mathrm{km}$). The Moore–Penrose pseudoinverse is\n$$\nR^\\dagger = V \\Sigma^\\dagger U^\\top,\n$$\nwhere $\\Sigma^\\dagger = \\mathrm{diag}(1/\\sigma_1, \\ldots, 1/\\sigma_r)$ acts as reciprocal on nonzero singular values and zero on exact zeros. According to the Hadamard definition of well-posedness, stability requires small perturbations in $d$ to produce proportionally small changes in $m$. In this linear setting, the worst-case amplification from data perturbations $\\delta d$ (in seconds, $\\mathrm{s}$) to model perturbations $\\delta m$ (in seconds per kilometer, $\\mathrm{s}/\\mathrm{km}$) via $\n\\delta m = R^\\dagger \\delta d\n$ is controlled by the operator norm of $R^\\dagger$,\n$$\n\\|R^\\dagger\\|_2 = \\frac{1}{\\sigma_{\\min}},\n$$\nexpressed in inverse kilometers ($\\mathrm{km}^{-1}$), where $\\sigma_{\\min}$ denotes the smallest nonzero singular value of $R$. As the mesh is refined ($N_x, N_y$ increase) while the survey geometry (number of rays $m$) is fixed, the matrix $R$ typically develops a growing numerical nullspace, with $\\sigma_{\\min}$ approaching zero and $\\|R^\\dagger\\|_2$ increasing, indicating ill-posedness due to instability.\n\nYour task is to implement the following, starting from these foundational definitions and facts:\n- Construct $R$ by computing exact intersection lengths of straight rays with axis-aligned grid cells. The domain is $[0,L_x] \\times [0,L_y]$ with $L_x = L_y = 1.0$ kilometers, and rays are straight segments between source and receiver points.\n- Use singular value decomposition (SVD) to compute the singular value spectrum of $R$, determine the numerical rank $\\mathrm{rank}(R)$ using the threshold\n$$\n\\tau = \\max(m,n)\\,\\epsilon\\,\\sigma_{\\max},\n$$\nwhere $\\epsilon$ is machine precision for double precision and $\\sigma_{\\max}$ is the largest singular value, and estimate the numerical nullspace dimension $n - \\mathrm{rank}(R)$.\n- Define a deterministic perturbation $\\delta d$ aligned with the left singular vector associated with $\\sigma_{\\min}$, scaled to a magnitude $\\|\\delta d\\|_2 = \\varepsilon$, where $\\varepsilon = 0.005$ seconds. Compute $\\delta m = R^\\dagger \\delta d$ using SVD factors, with reciprocals applied only to singular values exceeding the threshold $\\tau$ and zeros otherwise. Report the amplification ratio\n$$\n\\frac{\\|\\delta m\\|_2}{\\|\\delta d\\|_2},\n$$\nin inverse kilometers ($\\mathrm{km}^{-1}$).\n\nSurvey geometry:\n- Place $N_s$ sources evenly along the left boundary at positions $(0, y_i)$ with $y_i = \\left(i+\\tfrac{1}{2}\\right)\\tfrac{L_y}{N_s}$ for $i=0,\\ldots,N_s-1$ and $N_r$ receivers evenly along the right boundary at positions $(L_x, y_j)$ with $y_j = \\left(j+\\tfrac{1}{2}\\right)\\tfrac{L_y}{N_r}$ for $j=0,\\ldots,N_r-1$. Include all left-to-right source–receiver pairs.\n- Place $N_s$ sources evenly along the bottom boundary at positions $(x_i, 0)$ with $x_i = \\left(i+\\tfrac{1}{2}\\right)\\tfrac{L_x}{N_s}$ and $N_r$ receivers evenly along the top boundary at positions $(x_j, L_y)$. Include all bottom-to-top source–receiver pairs.\n- Use $N_s = N_r = 6$ for all test cases.\n\nTest suite:\n- Case 1 (happy path, mildly overdetermined): $N_x = 8$, $N_y = 8$.\n- Case 2 (underdetermined, moderate refinement): $N_x = 16$, $N_y = 16$.\n- Case 3 (underdetermined, finer refinement): $N_x = 24$, $N_y = 24$.\n- Case 4 (underdetermined, fine refinement boundary): $N_x = 32$, $N_y = 32$.\n\nFor each case, compute and return the list\n$$\n\\left[\\sigma_{\\min}, \\ \\|R^\\dagger\\|_2, \\ \\frac{\\|\\delta m\\|_2}{\\|\\delta d\\|_2}, \\ n - \\mathrm{rank}(R)\\right],\n$$\nwhere $\\sigma_{\\min}$ is the smallest singular value above the threshold $\\tau$ (units kilometers), $\\|R^\\dagger\\|_2$ is the operator $2$-norm of the pseudoinverse (units inverse kilometers), $\\|\\delta m\\|_2 / \\|\\delta d\\|_2$ is the computed amplification ratio (units inverse kilometers), and $n - \\mathrm{rank}(R)$ is the integer numerical nullspace dimension. Express all floating-point outputs rounded to six decimal places, with $\\sigma_{\\min}$ in kilometers and the last two floating-point quantities in inverse kilometers, and the integer quantity as an integer.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a list of lists, each inner list corresponding to a test case in the specified order, with no spaces, for example:\n$$\n\\texttt{[[0.123456,8.100000,8.100000,12],[\\dots],[\\dots],[\\dots]]}.\n$$", "solution": "The forward model for straight-ray travel-time tomography is described by the integral of slowness along ray paths. For a ray $\\gamma$ connecting a source point to a receiver point, the travel time is\n$$\nt_\\gamma = \\int_\\gamma s(\\mathbf{x})\\, \\mathrm{d}\\ell,\n$$\nwhere $s(\\mathbf{x})$ is slowness in seconds per kilometer and $\\mathrm{d}\\ell$ is an infinitesimal path length in kilometers. Discretizing $s(\\mathbf{x})$ on an $N_x \\times N_y$ grid of axis-aligned rectangular cells with constant slowness per cell yields\n$$\nd_i = \\sum_{j=1}^{n} R_{ij} m_j,\n$$\nwhere $d_i$ is the travel time of the $i$-th ray, $m_j$ is the slowness of the $j$-th cell, and $R_{ij}$ is the intersection length in kilometers of ray $i$ within cell $j$. Stacking all ray equations, $d = R m$ with $R \\in \\mathbb{R}^{m \\times n}$.\n\nHadamard's well-posedness demands existence, uniqueness, and stability. In the linear discrete setting, stability in the $2$-norm is governed by the singular value decomposition (singular value decomposition (SVD)) $R = U \\Sigma V^\\top$, with singular values $\\sigma_k \\ge 0$. The Moore–Penrose pseudoinverse $R^\\dagger = V \\Sigma^\\dagger U^\\top$ maps data perturbations to model perturbations. The operator $2$-norm satisfies\n$$\n\\|R^\\dagger\\|_2 = \\max_{\\|x\\|_2=1} \\|R^\\dagger x\\|_2 = \\frac{1}{\\sigma_{\\min}},\n$$\nwhere $\\sigma_{\\min}$ is the smallest nonzero singular value of $R$. Therefore, if $\\sigma_{\\min}$ is small, any data perturbation component aligned with the corresponding left singular vector $u_{\\min}$ is amplified by approximately $1/\\sigma_{\\min}$ in the model space. As the mesh is refined (larger $N_x, N_y$) while the number of rays $m$ is held fixed, the number of unknowns $n$ grows and the matrix $R$ becomes increasingly rank-deficient. More singular values are near zero, and the numerical nullspace dimension $n - \\mathrm{rank}(R)$ increases. This leads to ill-posedness due to instability, as reflected by the growth of $\\|R^\\dagger\\|_2$.\n\nAlgorithmic construction:\n1. Geometry and rays: Define the domain $[0, L_x] \\times [0, L_y]$ with $L_x = L_y = 1.0$ kilometers. Place $N_s = 6$ sources along the left boundary at $(0, y_i)$ with $y_i = \\left(i + \\tfrac{1}{2}\\right) \\tfrac{L_y}{N_s}$ for $i = 0, \\ldots, 5$, and $N_r = 6$ receivers along the right boundary at $(L_x, y_j)$ with $y_j = \\left(j + \\tfrac{1}{2}\\right) \\tfrac{L_y}{N_r}$. Include all $N_s N_r$ left-to-right rays. Similarly, place $N_s$ sources along the bottom boundary at $(x_i, 0)$ with $x_i = \\left(i + \\tfrac{1}{2}\\right) \\tfrac{L_x}{N_s}$ and $N_r$ receivers along the top boundary at $(x_j, L_y)$, including all $N_s N_r$ bottom-to-top rays. The total number of rays is $m = 2 N_s N_r = 72$.\n2. Discrete ray transform: For each ray from point $\\mathbf{p}_0 = (x_0, y_0)$ to $\\mathbf{p}_1 = (x_1, y_1)$, parametrize the line segment as $\\mathbf{p}(t) = \\mathbf{p}_0 + t (\\mathbf{p}_1 - \\mathbf{p}_0)$ with $t \\in [0, 1]$. Compute all intersection parameters $t$ where the ray crosses vertical and horizontal grid lines $x = x_k$ and $y = y_\\ell$. Include $t=0$ and $t=1$. Sort the unique $t$ values within $[0, 1]$ to get subsegments $[t_k, t_{k+1}]$. Each subsegment lies entirely within a single cell. For each subsegment, compute its length as $L_\\mathrm{seg} = \\|\\mathbf{p}_1 - \\mathbf{p}_0\\|_2 (t_{k+1} - t_k)$ in kilometers and assign this length to the corresponding cell index determined by the midpoint $\\mathbf{p}(\\tfrac{t_k + t_{k+1}}{2})$. Accumulate lengths for all segments of the ray to form its row in $R$.\n3. Singular value analysis: Compute the SVD $R = U \\Sigma V^\\top$ with $\\Sigma = \\mathrm{diag}(\\sigma_1, \\ldots, \\sigma_r)$, $r = \\min(m, n)$. Let $\\sigma_{\\max} = \\sigma_1$. Define the numerical threshold\n$$\n\\tau = \\max(m,n) \\, \\epsilon \\, \\sigma_{\\max},\n$$\nwhere $\\epsilon$ is machine precision for double precision (approximately $2.22 \\times 10^{-16}$). The numerical rank is $\\mathrm{rank}(R) = \\#\\{k: \\sigma_k > \\tau \\}$. The numerical nullspace dimension is $n - \\mathrm{rank}(R)$. The smallest nonzero singular value is $\\sigma_{\\min} = \\min\\{\\sigma_k: \\sigma_k > \\tau\\}$.\n4. Pseudoinverse and amplification: Construct $\\Sigma^\\dagger$ by setting $(\\Sigma^\\dagger)_{kk} = 1/\\sigma_k$ if $\\sigma_k > \\tau$, and $(\\Sigma^\\dagger)_{kk} = 0$ otherwise. The pseudoinverse is $R^\\dagger = V \\Sigma^\\dagger U^\\top$. Choose a deterministic perturbation in data space aligned with the left singular vector for $\\sigma_{\\min}$, i.e., $\\delta d = \\varepsilon \\, u_{\\min}$ with $\\|\\delta d\\|_2 = \\varepsilon = 0.005$ seconds. Compute the model perturbation $\\delta m = R^\\dagger \\delta d$. The amplification ratio is\n$$\n\\frac{\\|\\delta m\\|_2}{\\|\\delta d\\|_2} = \\frac{\\|R^\\dagger \\delta d\\|_2}{\\varepsilon},\n$$\nwhich equals $1/\\sigma_{\\min}$ in the ideal case and, numerically, matches $\\|R^\\dagger\\|_2$ when $\\delta d$ is aligned with $u_{\\min}$.\n\nExpected behavior across the test suite:\n- Case 1 ($N_x = N_y = 8$) has $n = 64$ unknowns and $m = 72$ rays; it is mildly overdetermined and generally better conditioned, with a relatively larger $\\sigma_{\\min}$ and smaller $\\|R^\\dagger\\|_2$.\n- Cases 2–4 ($N_x = N_y$ equal to $16$, $24$, $32$) are increasingly underdetermined with $n = 256$, $576$, and $1024$ unknowns but fixed $m = 72$. The numerical nullspace dimension increases, $\\sigma_{\\min}$ typically decreases, and both $\\|R^\\dagger\\|_2$ and the amplification ratio grow, revealing ill-posedness due to instability under mesh refinement.\n\nImplementation details:\n- Units: Entries of $R$ are in kilometers ($\\mathrm{km}$). Singular values $\\sigma_k$ are in kilometers. The operator norm $\\|R^\\dagger\\|_2$ and the amplification ratio $\\|\\delta m\\|_2/\\|\\delta d\\|_2$ are in inverse kilometers ($\\mathrm{km}^{-1}$).\n- Numerical thresholding ensures that near-zero singular values (below $\\tau$) are treated as zero to avoid numerical blow-up from spurious reciprocals.\n- Outputs: For each case, report $\\sigma_{\\min}$ (kilometers), $\\|R^\\dagger\\|_2$ ($\\mathrm{km}^{-1}$), $\\|\\delta m\\|_2/\\|\\delta d\\|_2$ ($\\mathrm{km}^{-1}$), and $n - \\mathrm{rank}(R)$ (integer), rounding the floating-point values to six decimal places. Aggregate all case results into a single list of lists printed on one line without spaces.\n\nThis principled approach connects the core definition of travel-time integrations to a discrete operator, uses the singular value decomposition to infer stability properties, and quantifies ill-posedness through mesh refinement by directly computing how $\\delta d$ amplifies into $\\delta m$ via $R^\\dagger$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef generate_sources_receivers(Lx, Ly, Ns, Nr):\n    # Left-to-right rays\n    ys_src = (np.arange(Ns) + 0.5) * (Ly / Ns)\n    ys_rcv = (np.arange(Nr) + 0.5) * (Ly / Nr)\n    rays_lr = [((0.0, ys), (Lx, yr)) for ys in ys_src for yr in ys_rcv]\n    # Bottom-to-top rays\n    xs_src = (np.arange(Ns) + 0.5) * (Lx / Ns)\n    xs_rcv = (np.arange(Nr) + 0.5) * (Lx / Nr)\n    rays_bt = [((xs, 0.0), (xr, Ly)) for xs in xs_src for xr in xs_rcv]\n    return rays_lr + rays_bt\n\ndef ray_cell_lengths(p0, p1, x_edges, y_edges, nx, ny):\n    x0, y0 = p0\n    x1, y1 = p1\n    dx = x1 - x0\n    dy = y1 - y0\n    # Full ray length\n    L = np.hypot(dx, dy)\n    # Compute intersection parameters t where the ray crosses grid lines\n    t_vals = [0.0, 1.0]\n    if abs(dx) > 0.0:\n        tx = (x_edges - x0) / dx\n        # filter within [0,1]\n        t_vals.extend([t for t in tx if 0.0 = t = 1.0])\n    if abs(dy) > 0.0:\n        ty = (y_edges - y0) / dy\n        t_vals.extend([t for t in ty if 0.0 = t = 1.0])\n    # Sort unique t's\n    t_vals = np.array(sorted(set(t_vals)))\n    # Accumulate lengths into cells\n    lengths = {}\n    for k in range(len(t_vals) - 1):\n        t0 = t_vals[k]\n        t1 = t_vals[k + 1]\n        if t1 = t0:\n            continue\n        tm = 0.5 * (t0 + t1)\n        xm = x0 + dx * tm\n        ym = y0 + dy * tm\n        # Determine cell indices using edges\n        ix = np.searchsorted(x_edges, xm, side='right') - 1\n        iy = np.searchsorted(y_edges, ym, side='right') - 1\n        if ix  0 or ix >= nx or iy  0 or iy >= ny:\n            continue\n        seg_len = L * (t1 - t0)\n        idx = ix + iy * nx\n        lengths[idx] = lengths.get(idx, 0.0) + seg_len\n    return lengths\n\ndef build_R_matrix(nx, ny, Lx, Ly, rays):\n    x_edges = np.linspace(0.0, Lx, nx + 1)\n    y_edges = np.linspace(0.0, Ly, ny + 1)\n    num_rays = len(rays)\n    R = np.zeros((num_rays, nx * ny), dtype=float)\n    for i, (p0, p1) in enumerate(rays):\n        lengths = ray_cell_lengths(p0, p1, x_edges, y_edges, nx, ny)\n        if lengths:\n            idxs = np.fromiter(lengths.keys(), dtype=int)\n            vals = np.fromiter(lengths.values(), dtype=float)\n            R[i, idxs] = vals\n    return R\n\ndef svd_metrics(R, eps= np.finfo(float).eps):\n    # Compute SVD\n    U, s, Vt = np.linalg.svd(R, full_matrices=False)\n    m, n = R.shape\n    smax = s[0] if s.size > 0 else 0.0\n    tau = max(m, n) * eps * smax\n    # Numerical rank and nullity\n    above = s > tau\n    rank = int(np.sum(above))\n    nullity = n - rank\n    if rank > 0:\n        smin_nonzero = float(np.min(s[above]))\n        # index of smallest nonzero singular value\n        idx_min = int(np.argmin(np.where(above, s, np.inf)))\n    else:\n        smin_nonzero = 0.0\n        idx_min = None\n    # Operator norm of pseudoinverse\n    op_norm_pinv = np.inf if smin_nonzero == 0.0 else (1.0 / smin_nonzero)\n    return U, s, Vt, smin_nonzero, op_norm_pinv, tau, rank, nullity, idx_min\n\ndef apply_pseudoinverse(U, s, Vt, tau, delta_d):\n    # Build filtered reciprocal of singular values\n    inv_s = np.zeros_like(s)\n    mask = s > tau\n    inv_s[mask] = 1.0 / s[mask]\n    # Compute R^dagger * delta_d via SVD factors: V * diag(inv_s) * U^T * delta_d\n    Ut_dd = U.T @ delta_d\n    temp = inv_s * Ut_dd\n    delta_m = Vt.T @ temp\n    return delta_m\n\ndef format_results(results):\n    # Format nested list without spaces, floats rounded to 6 decimals\n    parts = []\n    for sm, opn, ampr, nullity in results:\n        parts.append(f\"[{sm:.6f},{opn:.6f},{ampr:.6f},{int(nullity)}]\")\n    return \"[\" + \",\".join(parts) + \"]\"\n\ndef solve():\n    # Define parameters\n    Lx = 1.0  # kilometers\n    Ly = 1.0  # kilometers\n    Ns = 6\n    Nr = 6\n    rays = generate_sources_receivers(Lx, Ly, Ns, Nr)\n    # Test suite: list of (Nx, Ny)\n    test_cases = [\n        (8, 8),    # Case 1: happy path, mildly overdetermined\n        (16, 16),  # Case 2: underdetermined, moderate refinement\n        (24, 24),  # Case 3: underdetermined, finer refinement\n        (32, 32),  # Case 4: underdetermined, fine refinement boundary\n    ]\n    epsilon = 0.005  # seconds magnitude for ||delta d||_2\n\n    results = []\n    for Nx, Ny in test_cases:\n        R = build_R_matrix(Nx, Ny, Lx, Ly, rays)\n        U, s, Vt, smin, opnorm, tau, rank, nullity, idx_min = svd_metrics(R)\n        # Define delta d aligned with left singular vector associated with smallest nonzero singular value\n        if idx_min is not None:\n            u_min = U[:, idx_min]\n            # Scale to have ||delta d||_2 = epsilon seconds\n            delta_d = (epsilon / np.linalg.norm(u_min)) * u_min\n            delta_m = apply_pseudoinverse(U, s, Vt, tau, delta_d)\n            amp_ratio = np.linalg.norm(delta_m) / np.linalg.norm(delta_d)\n        else:\n            # No nonzero singular values; set amplification to infinity\n            amp_ratio = np.inf\n        results.append((smin, opnorm, amp_ratio, nullity))\n\n    print(format_results(results))\n\nsolve()\n```", "id": "3618830"}, {"introduction": "Many of the most significant inverse problems in geophysics, such as Full Waveform Inversion (FWI), are fundamentally nonlinear. In this domain, ill-posedness can emerge not just from rank deficiency but from a complex, non-convex objective function landscape that traps local optimization algorithms. This final exercise [@problem_id:3618869] delves into this challenge by exploring how the choice of model parameterization—a seemingly simple decision—can drastically alter the problem's local convexity, and thus its practical solvability.", "problem": "Consider a reduced one-dimensional acoustic inversion setting that captures the essential nonlinearity of Full Waveform Inversion (FWI) with respect to the choice of parameterization. Let there be two horizontally homogeneous layers with unknown properties and straight-ray propagation from a common source to multiple receivers. Denote the velocity in layer $k$ by $v_k$ (in $\\mathrm{m/s}$) and the slowness by $s_k$ (in $\\mathrm{s/m}$), satisfying $s_k = 1 / v_k$. For receiver index $j$, let $L_{j1}$ and $L_{j2}$ denote the fixed path lengths through layers $1$ and $2$, respectively (in $\\mathrm{m}$). The predicted one-way travel time (in $\\mathrm{s}$) at receiver $j$ is\n- under slowness parameterization $s = (s_1, s_2) \\in \\mathbb{R}^2$: $t_j(s) = L_{j1} s_1 + L_{j2} s_2$,\n- under velocity parameterization $v = (v_1, v_2) \\in \\mathbb{R}^2$: $t_j(v) = L_{j1} v_1^{-1} + L_{j2} v_2^{-1}$.\n\nGiven observed travel times $d_j$ (in $\\mathrm{s}$), define the least-squares objective\n$$\nJ(p) = \\frac{1}{2} \\sum_{j=1}^{m} \\left( t_j(p) - d_j \\right)^2,\n$$\nwhere $p$ stands for either $s$ or $v$, and $m$ is the number of receivers. By definition, local convexity of $J$ at a point is characterized by the positive semidefiniteness of the Hessian matrix of second derivatives at that point; strict convexity corresponds to positive definiteness. The eigenvalues of the Hessian quantify curvature; negative eigenvalues indicate directions of negative curvature and local nonconvexity.\n\nYour task is to:\n1. Starting from the chain rule and the definition of the Hessian of a sum of squared residuals, derive expressions for the exact Hessian of $J$ under both parameterizations $s$ and $v$, written in terms of the path lengths $L_{jk}$, the model parameters $p$, and the residuals $r_j(p) = t_j(p) - d_j$. Do not use any pre-quoted FWI-specific formulas; proceed directly from the definitions given above.\n2. Implement a program that, for each test case below, computes the Hessian under both parameterizations at a specified evaluation point, computes its eigenvalues, and reports:\n   - the smallest eigenvalue for the slowness parameterization,\n   - the smallest eigenvalue for the velocity parameterization,\n   - a boolean stating whether the Hessian is positive semidefinite (all eigenvalues $\\ge 0$ within a numerical tolerance),\n   - a boolean stating whether the Hessian is positive definite (all eigenvalues $ 0$ within a numerical tolerance).\n3. Interpret how the choice of parameterization affects local convexity and hence local well-posedness, as reflected by the signs of the smallest eigenvalues across the test suite.\n\nTest suite specification:\n- Physical constants and units:\n  - Path lengths $L_{jk}$ are in $\\mathrm{m}$.\n  - Velocities $v_k$ are in $\\mathrm{m/s}$.\n  - Slownesses $s_k$ are in $\\mathrm{s/m}$.\n  - Travel times $t_j$ and $d_j$ are in $\\mathrm{s}$.\n- True model used to generate data:\n  - $v_1^{\\mathrm{true}} = 2500$ and $v_2^{\\mathrm{true}} = 2000$.\n  - $s_k^{\\mathrm{true}} = 1 / v_k^{\\mathrm{true}}$.\n- Receivers and path lengths:\n  - Receiver $1$: $(L_{11}, L_{12}) = (1000, 500)$.\n  - Receiver $2$: $(L_{21}, L_{22}) = (700, 800)$.\n- Observed data generation:\n  - For any receiver $j$, set $d_j = L_{j1} s_1^{\\mathrm{true}} + L_{j2} s_2^{\\mathrm{true}}$.\n- Three evaluation cases:\n  - Case A (happy path, sufficient data, at the true model):\n    - Use both receivers $j \\in \\{1, 2\\}$.\n    - Evaluate Hessians at $s = s^{\\mathrm{true}}$ and $v = v^{\\mathrm{true}}$.\n  - Case B (nonlinear regime for velocity, far from the true model):\n    - Use both receivers $j \\in \\{1, 2\\}$.\n    - Evaluate at $v = (100000, 100000)$ and at the corresponding $s = (1/100000, 1/100000)$.\n  - Case C (boundary case with insufficient data, rank-deficient geometry):\n    - Use only receiver $j = 1$.\n    - Evaluate at $s = s^{\\mathrm{true}}$ and $v = v^{\\mathrm{true}}$.\n- Numerical tolerance for definiteness checks:\n  - Use tolerance $\\tau = 10^{-12}$. Treat an eigenvalue $\\lambda$ as nonnegative if $\\lambda \\ge -\\tau$ and as strictly positive if $\\lambda  \\tau$.\n\nRequired final output format:\n- Your program should produce a single line of output containing a list of three inner lists, one per case, in the following order and contents:\n  - For Case A: $[\\lambda_{\\min}^{(s)}, \\lambda_{\\min}^{(v)}, \\mathrm{is\\_psd}^{(s)}, \\mathrm{is\\_psd}^{(v)}, \\mathrm{is\\_pd}^{(s)}, \\mathrm{is\\_pd}^{(v)}]$.\n  - For Case B: same structure as Case A.\n  - For Case C: same structure as Case A.\n- Here, $\\lambda_{\\min}^{(s)}$ is the smallest eigenvalue of the Hessian under slowness parameterization, and $\\lambda_{\\min}^{(v)}$ is the smallest eigenvalue under velocity parameterization. The boolean flags correspond to positive semidefinite and positive definite tests according to the tolerance above.\n- The program must output exactly one line, for example:\n  - $[[0.1,0.05,True,True,True,True],[...],[...]]$.\n\nNo external input should be read; all numbers necessary to run the program are specified above. The output is unitless floating-point numbers and booleans as defined. Express the final output exactly as specified; there is no need to report physical units in the output line.", "solution": "The task is to analyze the local convexity of a simplified one-dimensional acoustic inversion problem under two different parameterizations: slowness $s = (s_1, s_2)$ and velocity $v = (v_1, v_2)$. Local convexity at a point is determined by the properties of the Hessian matrix of the objective function at that point. Specifically, we will derive the Hessians, implement their calculation, and interpret the results for three distinct cases.\n\n### Step 1: Derivation of the Hessian Matrices\n\nThe least-squares objective function is given by:\n$$\nJ(p) = \\frac{1}{2} \\sum_{j=1}^{m} \\left( t_j(p) - d_j \\right)^2 = \\frac{1}{2} \\sum_{j=1}^{m} r_j(p)^2\n$$\nwhere $p$ is the parameter vector (either $s$ or $v$), $t_j(p)$ is the predicted travel time for receiver $j$, $d_j$ is the observed travel time, and $r_j(p) = t_j(p) - d_j$ is the residual.\n\nThe Hessian matrix $H$ has elements $H_{kl} = \\frac{\\partial^2 J}{\\partial p_k \\partial p_l}$. We first find the gradient components $\\frac{\\partial J}{\\partial p_k}$:\n$$\n\\frac{\\partial J}{\\partial p_k} = \\frac{\\partial}{\\partial p_k} \\left( \\frac{1}{2} \\sum_{j=1}^{m} r_j(p)^2 \\right) = \\sum_{j=1}^{m} r_j(p) \\frac{\\partial r_j(p)}{\\partial p_k} = \\sum_{j=1}^{m} r_j(p) \\frac{\\partial t_j(p)}{\\partial p_k}\n$$\nsince $d_j$ are constants. Now we differentiate again with respect to $p_l$ using the product rule:\n$$\nH_{kl}(p) = \\frac{\\partial^2 J}{\\partial p_k \\partial p_l} = \\frac{\\partial}{\\partial p_l} \\left( \\sum_{j=1}^{m} r_j(p) \\frac{\\partial t_j(p)}{\\partial p_k} \\right) = \\sum_{j=1}^{m} \\left( \\frac{\\partial r_j(p)}{\\partial p_l} \\frac{\\partial t_j(p)}{\\partial p_k} + r_j(p) \\frac{\\partial^2 t_j(p)}{\\partial p_k \\partial p_l} \\right)\n$$\nSubstituting $\\frac{\\partial r_j(p)}{\\partial p_l} = \\frac{\\partial t_j(p)}{\\partial p_l}$, we arrive at the general expression for the Hessian:\n$$\nH_{kl}(p) = \\sum_{j=1}^{m} \\left( \\frac{\\partial t_j(p)}{\\partial p_k} \\frac{\\partial t_j(p)}{\\partial p_l} + r_j(p) \\frac{\\partial^2 t_j(p)}{\\partial p_k \\partial p_l} \\right)\n$$\nThe first term, $\\sum_j \\frac{\\partial t_j}{\\partial p_k} \\frac{\\partial t_j}{\\partial p_l}$, corresponds to the Gauss-Newton approximation of the Hessian. The second term, involving the second derivatives of the forward model $t_j(p)$, is the source of potential non-convexity.\n\n#### Hessian for Slowness Parameterization $s = (s_1, s_2)$\n\nThe forward model is linear in slowness: $t_j(s) = L_{j1}s_1 + L_{j2}s_2$.\nThe first partial derivatives are:\n$$\n\\frac{\\partial t_j}{\\partial s_1} = L_{j1}, \\quad \\frac{\\partial t_j}{\\partial s_2} = L_{j2}\n$$\nThe second partial derivatives are all zero:\n$$\n\\frac{\\partial^2 t_j(s)}{\\partial s_k \\partial s_l} = 0 \\quad \\text{for } k,l \\in \\{1,2\\}\n$$\nConsequently, the second term in the general Hessian expression vanishes. The Hessian for slowness, $H^{(s)}$, is:\n$$\nH^{(s)}_{kl}(s) = \\sum_{j=1}^{m} \\frac{\\partial t_j(s)}{\\partial s_k} \\frac{\\partial t_j(s)}{\\partial s_l}\n$$\nThe components are:\n$$\nH^{(s)}_{11} = \\sum_{j=1}^{m} L_{j1}^2, \\quad H^{(s)}_{12} = H^{(s)}_{21} = \\sum_{j=1}^{m} L_{j1}L_{j2}, \\quad H^{(s)}_{22} = \\sum_{j=1}^{m} L_{j2}^2\n$$\nThis can be written in matrix form as $H^{(s)} = L^T L$, where $L$ is the $m \\times 2$ matrix with entries $L_{jk}$. This Hessian is independent of the model parameters $s$ and the data residuals $r_j$. As a Gram matrix, $H^{(s)}$ is always positive semidefinite.\n\n#### Hessian for Velocity Parameterization $v = (v_1, v_2)$\n\nThe forward model is nonlinear in velocity: $t_j(v) = L_{j1}v_1^{-1} + L_{j2}v_2^{-1}$.\nThe first partial derivatives are:\n$$\n\\frac{\\partial t_j}{\\partial v_1} = -L_{j1}v_1^{-2}, \\quad \\frac{\\partial t_j}{\\partial v_2} = -L_{j2}v_2^{-2}\n$$\nThe second partial derivatives are:\n$$\n\\frac{\\partial^2 t_j}{\\partial v_1^2} = 2L_{j1}v_1^{-3}, \\quad \\frac{\\partial^2 t_j}{\\partial v_2^2} = 2L_{j2}v_2^{-3}, \\quad \\frac{\\partial^2 t_j}{\\partial v_1 \\partial v_2} = 0\n$$\nSubstituting these into the general Hessian formula gives the components of $H^{(v)}(v)$:\n$$\nH^{(v)}_{11} = \\sum_{j=1}^{m} \\left( (-L_{j1}v_1^{-2})^2 + r_j(v) (2L_{j1}v_1^{-3}) \\right) = v_1^{-4} \\sum_{j=1}^{m} L_{j1}^2 + 2v_1^{-3} \\sum_{j=1}^{m} r_j(v) L_{j1}\n$$\n$$\nH^{(v)}_{22} = \\sum_{j=1}^{m} \\left( (-L_{j2}v_2^{-2})^2 + r_j(v) (2L_{j2}v_2^{-3}) \\right) = v_2^{-4} \\sum_{j=1}^{m} L_{j2}^2 + 2v_2^{-3} \\sum_{j=1}^{m} r_j(v) L_{j2}\n$$\n$$\nH^{(v)}_{12} = H^{(v)}_{21} = \\sum_{j=1}^{m} \\left( (-L_{j1}v_1^{-2})(-L_{j2}v_2^{-2}) + r_j(v) \\cdot 0 \\right) = v_1^{-2}v_2^{-2} \\sum_{j=1}^{m} L_{j1}L_{j2}\n$$\nwhere the residuals are $r_j(v) = (L_{j1}v_1^{-1} + L_{j2}v_2^{-1}) - d_j$. The resulting Hessian for velocity, $H^{(v)}(v)$, explicitly depends on the evaluation point $v$ and the data residuals $r_j(v)$.\n\n### Step 2: Interpretation of Results\n\nThe derived forms of the Hessians reveal a fundamental difference between the two parameterizations.\n\n- **Slowness ($s$)**: Because the forward problem is linear in $s$, the objective function $J(s)$ is a quadratic function (a paraboloid). Its Hessian, $H^{(s)}$, is constant everywhere and depends only on the acquisition geometry $L$. As long as the geometry matrix $L$ has full column rank (i.e., at least two receivers with non-proportional path lengths), $H^{(s)}$ is positive definite. This means $J(s)$ is strictly convex with a unique global minimum. This is a locally and globally well-posed optimization problem. Case A will demonstrate this, having a positive definite Hessian. Case C, with only one receiver, results in a rank-deficient $L$, making $H^{(s)}$ only positive semidefinite; the problem has a null space, corresponding to a family of solutions.\n\n- **Velocity ($v$)**: The forward problem is nonlinear in $v$, leading to a non-quadratic objective function $J(v)$. The Hessian $H^{(v)}$ contains a second-order term that depends on the residuals.\n  - In Case A, we evaluate at the true model where residuals are zero ($r_j=0$). The Hessian simplifies to the Gauss-Newton Hessian, which is positive definite due to the good geometry. This indicates local convexity around the true solution.\n  - In Case B, we evaluate far from the true model. The predicted times are much smaller than the observed times, resulting in large negative residuals. The second-order term $2v_k^{-3} \\sum_j r_j(v) L_{jk}$ becomes a large negative number, which can overwhelm the positive Gauss-Newton term on the diagonal of $H^{(v)}$. This can cause the Hessian to have negative eigenvalues, indicating local non-convexity (saddle points or local maxima). This is the \"cycle-skipping\" problem in FWI, where local optimization methods can get trapped far from the true solution.\n  - In Case C, at the true model ($r_j=0$) but with rank-deficient geometry, the Gauss-Newton Hessian will be singular (positive semidefinite but not positive definite), reflecting the inherent non-uniqueness from the limited data.\n\nIn summary, parameterizing in slowness linearizes this travel-time problem, guaranteeing a simple, convex objective function landscape. Parameterizing in velocity, a more physical but non-linear choice, creates a complex landscape with regions of non-convexity that complicate the inversion process and make it sensitive to the starting model.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the acoustic inversion problem by calculating Hessians and their eigenvalues\n    for three specified test cases.\n    \"\"\"\n\n    # --- Problem Definition ---\n\n    # True model parameters\n    v_true = np.array([2500.0, 2000.0])  # m/s\n    s_true = 1.0 / v_true                # s/m\n\n    # Receiver path lengths (m)\n    L_full = np.array([\n        [1000.0, 500.0],  # Receiver 1\n        [700.0, 800.0]    # Receiver 2\n    ])\n\n    # Observed data generation (noise-free)\n    d_obs_full = L_full @ s_true\n\n    # Numerical tolerance for definiteness checks\n    tau = 1.0e-12\n\n    # --- Test Case Specifications ---\n\n    test_cases_spec = [\n        # Case A: Happy path, sufficient data, at the true model\n        {\n            \"name\": \"Case A\",\n            \"receivers_slice\": slice(0, 2),\n            \"v_eval\": v_true,\n            \"s_eval\": s_true,\n        },\n        # Case B: Nonlinear regime for velocity, far from the true model\n        {\n            \"name\": \"Case B\",\n            \"receivers_slice\": slice(0, 2),\n            \"v_eval\": np.array([100000.0, 100000.0]),\n            \"s_eval\": np.array([1.0e-5, 1.0e-5]),\n        },\n        # Case C: Boundary case with insufficient data, rank-deficient geometry\n        {\n            \"name\": \"Case C\",\n            \"receivers_slice\": slice(0, 1),\n            \"v_eval\": v_true,\n            \"s_eval\": s_true,\n        },\n    ]\n\n    all_results = []\n\n    for case_spec in test_cases_spec:\n        # Extract parameters for the current case\n        L = L_full[case_spec[\"receivers_slice\"], :]\n        if L.ndim == 1: L = L.reshape(1, -1) # Ensure L is 2D\n        d_obs = d_obs_full[case_spec[\"receivers_slice\"]]\n        v_eval = case_spec[\"v_eval\"]\n        s_eval = case_spec[\"s_eval\"]\n        \n        # --- Hessian for Slowness Parameterization (s) ---\n        # H_s = L' * L, where L is the geometry matrix.\n        H_s = L.T @ L\n        \n        # --- Hessian for Velocity Parameterization (v) ---\n        # H_v = G'G + sum(r_j * H_j), where G is the Jacobian and H_j is the Hessian of t_j.\n        v1, v2 = v_eval[0], v_eval[1]\n        \n        # Calculate residuals r_j(v) = t_j(v) - d_j\n        t_pred = L @ (1.0 / v_eval)\n        residuals = t_pred - d_obs\n        \n        # Summation terms from the derivation\n        sum_L1_sq = np.sum(L[:, 0]**2)\n        sum_L2_sq = np.sum(L[:, 1]**2)\n        sum_L1L2 = np.sum(L[:, 0] * L[:, 1])\n        sum_rL1 = np.sum(residuals * L[:, 0])\n        sum_rL2 = np.sum(residuals * L[:, 1])\n        \n        # Build the Hessian H_v\n        h11_v = v1**(-4.0) * sum_L1_sq + 2.0 * v1**(-3.0) * sum_rL1\n        h22_v = v2**(-4.0) * sum_L2_sq + 2.0 * v2**(-3.0) * sum_rL2\n        h12_v = v1**(-2.0) * v2**(-2.0) * sum_L1L2\n        \n        H_v = np.array([[h11_v, h12_v], [h12_v, h22_v]])\n        \n        # --- Eigenvalue Analysis ---\n        # Use eigvalsh for symmetric matrices\n        evals_s = np.linalg.eigvalsh(H_s)\n        min_eval_s = np.min(evals_s)\n        \n        evals_v = np.linalg.eigvalsh(H_v)\n        min_eval_v = np.min(evals_v)\n        \n        # Definiteness checks using the specified tolerance tau\n        is_psd_s = min_eval_s >= -tau\n        is_pd_s = min_eval_s > tau\n        \n        is_psd_v = min_eval_v >= -tau\n        is_pd_v = min_eval_v > tau\n        \n        # Collect results for the current case\n        case_results = [\n            min_eval_s, \n            min_eval_v,\n            is_psd_s, \n            is_psd_v, \n            is_pd_s, \n            is_pd_v\n        ]\n        all_results.append(case_results)\n\n    # --- Final Output Formatting ---\n    # The output must be a single line containing a list of lists.\n    # Convert Python booleans (True/False) to strings \"True\"/\"False\" for final print.\n    output_str = \"[\"\n    for i, case_res in enumerate(all_results):\n        res_str = f\"[{case_res[0]},{case_res[1]},{case_res[2]},{case_res[3]},{case_res[4]},{case_res[5]}]\"\n        output_str += res_str\n        if i  len(all_results) - 1:\n            output_str += \",\"\n    output_str += \"]\"\n    \n    print(output_str)\n\nsolve()\n```", "id": "3618869"}]}