{"hands_on_practices": [{"introduction": "The Jacobian matrix is the cornerstone of the Gauss-Newton method, providing a linear approximation of the forward problem at each iteration. This exercise solidifies your understanding by guiding you through the derivation of a Jacobian matrix from first principles for a simple, hypothetical model. By interpreting each term, you will gain direct insight into how the Jacobian quantifies the physical sensitivity of data to model parameters, which is essential for diagnosing the stability of an inversion [@problem_id:3599237].", "problem": "Consider a two-parameter geophysical model $m=\\begin{pmatrix} m_1 \\\\ m_2 \\end{pmatrix}$, where $m_1$ represents a scalar property controlling a band-limited amplitude response and $m_2$ represents a coupling parameter associated with anisotropy. The forward map $F:\\mathbb{R}^2\\to\\mathbb{R}^2$ is defined by $F(m)=\\begin{pmatrix} \\sin(\\alpha m_1) \\\\ m_1 m_2 \\end{pmatrix}$, where $\\alpha>0$ is a known scaling constant with units chosen so that the argument of the sine function is dimensionless. The data weighting matrix is the identity $W_d=I$. The nonlinear least-squares misfit is defined as $\\phi(m)=\\tfrac{1}{2}\\|F(m)-d\\|_2^2$, where $d\\in\\mathbb{R}^2$ are the observed data.\n\nStarting from the core definitions of the Jacobian matrix of $F(m)$ and the Gauss-Newton linearization of the residual, derive the Jacobian $J(m)$ explicitly in terms of $m_1$, $m_2$, and $\\alpha$. Then, explain how each entry of $J(m)$ reflects the physical sensitivity of the data to the parameters, and interpret how the magnitude and sign of the entries influence the conditioning of the Gauss-Newton normal matrix. Your derivation should proceed from first principles without invoking pre-stated formulas for the Jacobian of $F(m)$ or the Gauss-Newton method.\n\nProvide as your final answer the explicit closed-form analytical expression for $J(m)$. No numerical rounding is required. If you introduce any angles, they must be in radians. Since the final answer is symbolic, do not include units in the final expression.", "solution": "The problem requires the derivation of the Jacobian matrix $J(m)$ for a given forward model $F(m)$, a physical interpretation of its entries, and an analysis of their impact on the Gauss-Newton method.\n\nThe forward model is a function $F: \\mathbb{R}^2 \\to \\mathbb{R}^2$ that maps the model-parameter vector $m=\\begin{pmatrix} m_1 \\\\ m_2 \\end{pmatrix}$ to a data vector. The components of the forward map are given as:\n$$\nF(m) = \\begin{pmatrix} F_1(m_1, m_2) \\\\ F_2(m_1, m_2) \\end{pmatrix} = \\begin{pmatrix} \\sin(\\alpha m_1) \\\\ m_1 m_2 \\end{pmatrix}\n$$\nwhere $\\alpha > 0$ is a known constant.\n\nThe Jacobian matrix $J(m)$ of the vector-valued function $F(m)$ is, by definition, the matrix of all first-order partial derivatives. For a function mapping from $\\mathbb{R}^2$ to $\\mathbb{R}^2$, it is a $2 \\times 2$ matrix:\n$$\nJ(m) = \\begin{pmatrix} \\frac{\\partial F_1}{\\partial m_1} & \\frac{\\partial F_1}{\\partial m_2} \\\\ \\frac{\\partial F_2}{\\partial m_1} & \\frac{\\partial F_2}{\\partial m_2} \\end{pmatrix}\n$$\n\nWe proceed to calculate each entry of $J(m)$ from first principles using the rules of partial differentiation.\n\nThe first entry, $J_{11}$, is the partial derivative of $F_1(m_1, m_2) = \\sin(\\alpha m_1)$ with respect to $m_1$. Applying the chain rule, we have:\n$$\nJ_{11} = \\frac{\\partial}{\\partial m_1} \\left( \\sin(\\alpha m_1) \\right) = \\cos(\\alpha m_1) \\cdot \\frac{\\partial}{\\partial m_1}(\\alpha m_1) = \\alpha \\cos(\\alpha m_1)\n$$\n\nThe second entry, $J_{12}$, is the partial derivative of $F_1(m_1, m_2) = \\sin(\\alpha m_1)$ with respect to $m_2$. Since $F_1$ does not depend on $m_2$, this derivative is zero:\n$$\nJ_{12} = \\frac{\\partial}{\\partial m_2} \\left( \\sin(\\alpha m_1) \\right) = 0\n$$\n\nThe third entry, $J_{21}$, is the partial derivative of $F_2(m_1, m_2) = m_1 m_2$ with respect to $m_1$:\n$$\nJ_{21} = \\frac{\\partial}{\\partial m_1} \\left( m_1 m_2 \\right) = m_2\n$$\n\nThe fourth entry, $J_{22}$, is the partial derivative of $F_2(m_1, m_2) = m_1 m_2$ with respect to $m_2$:\n$$\nJ_{22} = \\frac{\\partial}{\\partial m_2} \\left( m_1 m_2 \\right) = m_1\n$$\n\nAssembling these partial derivatives into the matrix gives the explicit form of the Jacobian $J(m)$:\n$$\nJ(m) = \\begin{pmatrix} \\alpha \\cos(\\alpha m_1) & 0 \\\\ m_2 & m_1 \\end{pmatrix}\n$$\n\nNext, we interpret the physical meaning of these entries and their influence on the conditioning of the Gauss-Newton normal matrix. Each entry $J_{ij} = \\frac{\\partial F_i}{\\partial m_j}$ represents the sensitivity of the $i$-th data component to an infinitesimal change in the $j$-th model parameter.\n- $J_{11} = \\alpha \\cos(\\alpha m_1)$: This term quantifies the sensitivity of the first datum, $F_1 = \\sin(\\alpha m_1)$, to changes in the parameter $m_1$. The sensitivity is oscillatory and is maximal in magnitude when $|\\cos(\\alpha m_1)| = 1$, which occurs when $\\alpha m_1$ is an integer multiple of $\\pi$. At these points, the function $F_1$ is passing through zero and is steepest. Conversely, the sensitivity is zero when $\\cos(\\alpha m_1) = 0$, which occurs when $\\alpha m_1 = (n + \\frac{1}{2})\\pi$ for any integer $n$. These points correspond to the peaks and troughs of the sine wave, where a small change in $m_1$ results in a negligible change in $F_1$.\n- $J_{12} = 0$: This indicates that the first datum, $F_1$, is completely insensitive to changes in the parameter $m_2$. The measurement of the band-limited amplitude response is, according to this model, entirely decoupled from the anisotropy parameter.\n- $J_{21} = m_2$: This is the sensitivity of the second datum, $F_2 = m_1 m_2$, to perturbations in $m_1$. The sensitivity is directly proportional to the value of the coupling parameter $m_2$. If $m_2$ is close to zero, the second datum becomes insensitive to $m_1$, implying a weak coupling.\n- $J_{22} = m_1$: This is the sensitivity of the second datum $F_2$ to changes in $m_2$. This sensitivity is directly proportional to $m_1$. If $m_1$ is near zero, the second datum is insensitive to the anisotropy parameter $m_2$.\n\nThe Gauss-Newton method approximates the Hessian of the misfit function $\\phi(m)$ with the matrix $H_{GN} = J(m)^T J(m)$ (since the data weighting matrix $W_d = I$). The conditioning of this normal matrix is crucial for the stability and convergence of the inversion. A poorly conditioned or singular $H_{GN}$ leads to unstable parameter updates.\nThe normal matrix is:\n$$\nH_{GN} = J^T J = \\begin{pmatrix} \\alpha \\cos(\\alpha m_1) & m_2 \\\\ 0 & m_1 \\end{pmatrix} \\begin{pmatrix} \\alpha \\cos(\\alpha m_1) & 0 \\\\ m_2 & m_1 \\end{pmatrix} = \\begin{pmatrix} \\alpha^2 \\cos^2(\\alpha m_1) + m_2^2 & m_1 m_2 \\\\ m_1 m_2 & m_1^2 \\end{pmatrix}\n$$\nThe well-posedness of the local linear problem is determined by whether $H_{GN}$ is invertible. This is equivalent to the columns of $J(m)$ being linearly independent. The determinant of $H_{GN}$ provides a measure of this. Using the property $\\det(A^T A) = (\\det A)^2$ for a square matrix $A$, we can compute:\n$$\n\\det(J) = (\\alpha \\cos(\\alpha m_1))(m_1) - (0)(m_2) = \\alpha m_1 \\cos(\\alpha m_1)\n$$\nTherefore, the determinant of the normal matrix is:\n$$\n\\det(H_{GN}) = (\\det(J))^2 = \\alpha^2 m_1^2 \\cos^2(\\alpha m_1)\n$$\nThe normal matrix $H_{GN}$ becomes singular, and the inversion problem ill-conditioned, if $\\det(H_{GN}) = 0$. This occurs under two conditions:\n1. $m_1 = 0$: If the amplitude parameter $m_1$ is zero, the Jacobian becomes $J = \\begin{pmatrix} \\alpha & 0 \\\\ m_2 & 0 \\end{pmatrix}$, whose columns are linearly dependent. Physically, if $m_1=0$, then $F(m) = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ for any value of $m_2$. The data contain no information about $m_2$, making its recovery impossible.\n2. $\\cos(\\alpha m_1) = 0$: This happens when $\\alpha m_1_k = \\frac{\\pi}{2} + k\\pi$ for any integer $k$. In this case, the Jacobian is $J = \\begin{pmatrix} 0 & 0 \\\\ m_2 & m_1 \\end{pmatrix}$. The first row is zero, making the matrix rank-deficient. Physically, this corresponds to the points of zero sensitivity of a the first datum $d_1$ with respect to $m_1$, as discussed earlier. At these extremum points of $F_1$, a small perturbation in $m_1$ cannot be \"seen\" in the first datum, leading to a loss of information and an ill-conditioned system.\n\nThe magnitude of the entries also affects conditioning. If $m_1$ is very small, $\\det(H_{GN})$ becomes very small, leading to poor conditioning. Similarly, if the model is near a point where $\\cos(\\alpha m_1) \\approx 0$, the problem is nearly ill-conditioned. The signs of the Jacobian entries, while not directly affecting the conditioning of $J^T J$ (due to the squaring of terms), are critical for determining the direction of the parameter update step $\\delta m$ in the Gauss-Newton algorithm, as the update depends on the product $J^T (d - F(m))$.", "answer": "$$\n\\boxed{\nJ(m) = \\begin{pmatrix}\n\\alpha \\cos(\\alpha m_1) & 0 \\\\\nm_2 & m_1\n\\end{pmatrix}\n}\n$$", "id": "3599237"}, {"introduction": "The success of the Gauss-Newton method depends heavily on the shape of the objective function, which can have numerous local minima that trap the algorithm. This practice provides a hands-on demonstration of \"cycle skipping,\" a classic failure mode in waveform inversion where the algorithm converges to an incorrect solution due to phase ambiguity [@problem_id:3599323]. By implementing and comparing a standard least-squares misfit with a more robust envelope-based alternative, you will learn a powerful strategy for mitigating this common challenge in highly nonlinear problems.", "problem": "You are to implement a complete, runnable program that constructs a one-dimensional waveform inversion toy model in which the forward modeling operator generates a single reflected pulse as a time-shifted Ricker wavelet. The inversion target is a single unknown parameter, the constant acoustic velocity, and the data are synthetic seismograms measured at the surface. The goal is to examine the behavior of the Gauss–Newton method for Nonlinear Least Squares (NLS) under a standard amplitude-based least-squares residual and under an alternative envelope-based misfit, illustrating cycle skipping in the former and showing the change in the Gauss–Newton step in the latter.\n\nUse the following fundamental setup and definitions:\n\n- The observed data are acquired over a single, flat subsurface reflector at depth $z$, in a constant-velocity medium of unknown velocity $m$ (the model variable). The two-way travel time is $ \\tau(m) = \\dfrac{2 z}{m} $.\n- The source wavelet is the Ricker wavelet $s(t) = \\left(1 - 2 a t^2\\right) \\exp\\left(- a t^2\\right)$, with $a = \\left(\\pi f_0\\right)^2$.\n- The noise-free forward model is $d\\!\\left(t; m\\right) = s\\!\\left(t - \\tau(m)\\right)$.\n- The amplitude-based least-squares objective is $\\Phi_{\\mathrm{wf}}(m) = \\dfrac{1}{2} \\left\\| r_{\\mathrm{wf}}(m) \\right\\|_2^2$ with residual $r_{\\mathrm{wf}}(t; m) = d_{\\mathrm{obs}}(t) - d\\!\\left(t; m\\right)$.\n- The envelope-based alternative misfit uses the amplitude envelope $e\\!\\left(t; m\\right) = \\sqrt{ d\\!\\left(t; m\\right)^2 + \\left( \\mathcal{H}\\{ d\\!\\left(t; m\\right) \\} \\right)^2 }$, where $\\mathcal{H}\\{\\cdot\\}$ denotes the Hilbert transform, and the objective $\\Phi_{\\mathrm{env}}(m) = \\dfrac{1}{2} \\left\\| r_{\\mathrm{env}}(m) \\right\\|_2^2$ with residual $r_{\\mathrm{env}}(t; m) = e_{\\mathrm{obs}}(t) - e\\!\\left(t; m\\right)$.\n\nFrom first principles, derive the Gauss–Newton step for each objective:\n\n- Start from the definition of the Nonlinear Least Squares objective $\\Phi(m) = \\dfrac{1}{2} \\left\\| r(m) \\right\\|_2^2$ and the Gauss–Newton approximation that replaces the exact Hessian with $J(m)^\\top J(m)$, where $J(m)$ is the Jacobian of the residual with respect to $m$.\n- For the waveform residual, derive the Jacobian entry $J_{\\mathrm{wf}}(t; m) = \\dfrac{\\partial}{\\partial m} d\\!\\left(t; m\\right)$, using the chain rule and the derivative of the Ricker wavelet. The derivative of the Ricker wavelet is $s'(t) = \\exp\\!\\left(- a t^2\\right) \\, t \\left( - 6 a + 4 a^2 t^2 \\right)$. Provide $J_{\\mathrm{wf}}(t; m)$ explicitly in terms of $z$, $m$, and $s'\\!\\big(t - \\tau(m)\\big)$.\n- For the envelope residual, use the fact that $\\mathcal{H}\\{\\cdot\\}$ is a linear operator to derive the Jacobian entry $J_{\\mathrm{env}}(t; m) = \\dfrac{\\partial}{\\partial m} e\\!\\left(t; m\\right)$ via the chain rule: $e(t; m) = \\sqrt{x(t; m)^2 + y(t; m)^2}$ with $x = d(t; m)$ and $y = \\mathcal{H}\\{ d(t; m) \\}$. Show that $\\dfrac{\\partial e}{\\partial m}(t; m) = \\dfrac{ x \\, \\dfrac{\\partial x}{\\partial m} + y \\, \\dfrac{\\partial y}{\\partial m} }{ e }$ and relate $\\dfrac{\\partial y}{\\partial m}$ to $\\mathcal{H}\\!\\left\\{ \\dfrac{\\partial x}{\\partial m} \\right\\}$.\n\nThen, for each objective, write the Gauss–Newton update for the single parameter $m$:\n$$\n\\Delta m_{\\bullet} = \\dfrac{ \\sum_t J_{\\bullet}(t; m_0) \\, r_{\\bullet}(t; m_0) }{ \\sum_t J_{\\bullet}(t; m_0)^2 },\n$$\nwhere $\\bullet \\in \\{ \\mathrm{wf}, \\mathrm{env} \\}$ and $m_0$ is the current model iterate. Explain why, when $m_0$ is sufficiently far from the true value, the waveform-based residual $r_{\\mathrm{wf}}$ can exhibit cycle skipping, which manifests as a multi-cycle phase mismatch causing $J_{\\mathrm{wf}}^\\top r_{\\mathrm{wf}}$ to have misleading sign or small magnitude, yielding a poor $\\Delta m_{\\mathrm{wf}}$. Contrast this with the envelope-based residual, which suppresses oscillatory phase effects, often providing a more robust step $\\Delta m_{\\mathrm{env}}$ toward the true solution.\n\nPhysical and numerical parameters:\n\n- Depth $z = 1000\\,\\mathrm{m}$.\n- True velocity $m_\\star = 2000\\,\\mathrm{m/s}$.\n- Central frequency $f_0 = 10\\,\\mathrm{Hz}$.\n- Time sampling interval $\\Delta t = 0.001\\,\\mathrm{s}$.\n- Record length $T = 2.5\\,\\mathrm{s}$.\n\nAngles, including any phase that may appear implicitly in the analytic signal, are to be considered in radians.\n\nTest suite of initial model velocities $m_0$:\n\n- Case $1$ (perfect match): $m_0 = 2000\\,\\mathrm{m/s}$.\n- Case $2$ (near, below half-cycle mismatch): $m_0 = 2100\\,\\mathrm{m/s}$.\n- Case $3$ (moderate cycle skip, around one period of mismatch): $m_0 = 1800\\,\\mathrm{m/s}$.\n- Case $4$ (severe cycle skip, multiple periods): $m_0 = 1500\\,\\mathrm{m/s}$.\n\nYour program must:\n\n- Construct $d_{\\mathrm{obs}}(t)$ using $m_\\star$ and compute, for each $m_0$ in the test suite, both $\\Delta m_{\\mathrm{wf}}$ and $\\Delta m_{\\mathrm{env}}$ according to the Gauss–Newton formulas above.\n- Use the Hilbert transform to compute the envelope and the envelope Jacobian, with appropriate numerical safeguards to avoid division by zero when the envelope amplitude is extremely small.\n- Express each $\\Delta m$ in meters per second ($\\mathrm{m/s}$) and round to $6$ decimal places.\n\nFinal output format:\n\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element of this list corresponds to one test case and must be itself a two-element list containing the Gauss–Newton steps $\\Delta m_{\\mathrm{wf}}$ and $\\Delta m_{\\mathrm{env}}$ for that case, in that order. For example, the output should look like $[\\Delta m_{\\mathrm{wf}}^{(1)}, \\Delta m_{\\mathrm{env}}^{(1)}], \\ldots, [\\Delta m_{\\mathrm{wf}}^{(4)}, \\Delta m_{\\mathrm{env}}^{(4)}]$ where each number is printed with $6$ decimal places and implicitly in $\\mathrm{m/s}$.", "solution": "The user has provided a valid problem statement. The task is to analyze the Gauss-Newton optimization method for a one-dimensional waveform inversion problem, comparing a standard waveform-based objective function with an envelope-based alternative. The analysis focuses on the phenomenon of cycle skipping. The solution requires deriving the Gauss-Newton step for each objective function and then implementing a numerical simulation to compute these steps for several initial models.\n\nFirst, we establish the theoretical framework. The goal is to find the model parameter $m$ (acoustic velocity) that minimizes a nonlinear least-squares objective function of the form $\\Phi(m) = \\frac{1}{2} \\|r(m)\\|_2^2$, where $r(m)$ is the residual vector between observed and predicted data. For a discrete time series, this is $\\Phi(m) = \\frac{1}{2} \\sum_t [r(t; m)]^2$.\n\nThe Gauss-Newton method approximates the Hessian of the objective function, $\\nabla^2 \\Phi(m)$, with $J(m)^\\top J(m)$, where $J(m)$ is the Jacobian matrix of the residual vector $r(m)$ with respect to the model parameter $m$. The entries of the Jacobian are $J(t; m) = \\frac{\\partial r(t; m)}{\\partial m}$. The update step for the model parameter is found by solving the normal equations $J(m)^\\top J(m) \\Delta m = -J(m)^\\top r(m)$. In our case, with a single parameter $m$, this simplifies to a scalar equation. The problem defines the residual as $r(t;m) = d_{\\mathrm{obs}}(t) - d(t;m)$, so its Jacobian is $J(t;m) = \\frac{\\partial r}{\\partial m} = -\\frac{\\partial d}{\\partial m}$. The gradient of the objective function is $\\nabla \\Phi(m) = \\sum_t r(t;m) \\frac{\\partial r(t;m)}{\\partial m} = - \\sum_t r(t;m) \\frac{\\partial d(t;m)}{\\partial m}$. The standard Gauss-Newton update would be $m_{k+1} = m_k - (\\nabla^2 \\Phi)^{-1} \\nabla \\Phi$. The step is $\\delta m = - (J^\\top J)^{-1} J^\\top r$.\nThe problem statement provides a formula for the step $\\Delta m$ that omits the negative sign:\n$$\n\\Delta m = \\left( \\sum_t J(t; m_0)^2 \\right)^{-1} \\left( \\sum_t J(t; m_0) r(t; m_0) \\right)\n$$\nThis implies an update rule of the form $m_{k+1} = m_k - \\Delta m$ if $J = \\frac{\\partial d}{\\partial m}$, or $m_{k+1} = m_k + \\Delta m$ if $J = -\\frac{\\partial d}{\\partial m}$. We will adhere to the provided formula, noting that the direction of the correction depends on the local correlation between the residual and the data sensitivity.\n\nThe forward model for the synthetic data is a time-shifted Ricker wavelet: $d(t; m) = s(t - \\tau(m))$, where $s(t) = (1 - 2 a t^2) \\exp(-a t^2)$ with $a = (\\pi f_0)^2$, and the two-way travel time is $\\tau(m) = \\frac{2z}{m}$.\n\n**1. Waveform-Based Objective $\\Phi_{\\mathrm{wf}}(m)$**\n\nThe residual is $r_{\\mathrm{wf}}(t; m) = d_{\\mathrm{obs}}(t) - d(t; m)$. The Jacobian of this residual with respect to $m$ is $J_{\\mathrm{wf}}(t; m) = \\frac{\\partial r_{\\mathrm{wf}}}{\\partial m} = -\\frac{\\partial d(t; m)}{\\partial m}$.\nUsing the chain rule:\n$$\n\\frac{\\partial d(t; m)}{\\partial m} = \\frac{\\partial}{\\partial m} s(t - \\tau(m)) = s'(t - \\tau(m)) \\cdot \\left( -\\frac{\\partial \\tau(m)}{\\partial m} \\right)\n$$\nThe derivative of the travel time is $\\frac{\\partial \\tau(m)}{\\partial m} = \\frac{\\partial}{\\partial m}\\left(\\frac{2z}{m}\\right) = -\\frac{2z}{m^2}$.\nSubstituting this in, we get:\n$$\n\\frac{\\partial d(t; m)}{\\partial m} = s'(t - \\tau(m)) \\cdot \\left( - \\left( -\\frac{2z}{m^2} \\right) \\right) = \\frac{2z}{m^2} s'(t - \\tau(m))\n$$\nThus, the Jacobian of the residual is $J_{\\mathrm{wf}}(t; m) = -\\frac{2z}{m^2} s'(t - \\tau(m))$. The provided step formula uses $J$ that is not specified if it's for residual or forward model. To ensure the step is corrective, let's analyze the gradient. $\\nabla\\Phi = J_{\\mathrm{wf}}^\\top r_{\\mathrm{wf}}$. If we use the problem's formula for $\\Delta m$, it represents a step along the gradient. An update $m_{k+1} = m_k - \\lambda \\Delta m$ would be gradient descent. We will compute $\\Delta m$ as per the provided formula, defining the Jacobian as the sensitivity of the *forward model*, so $J(t;m) = \\frac{\\partial d(t;m)}{\\partial m}$.\n$$\nJ_{\\mathrm{wf}}(t; m) = \\frac{2z}{m^2} s'(t - \\tau(m))\n$$\nwhere the derivative of the Ricker wavelet is given as $s'(t) = \\exp(- a t^2) \\, t \\, (-6 a + 4 a^2 t^2)$.\nThe Gauss-Newton step is then:\n$$\n\\Delta m_{\\mathrm{wf}} = \\frac{\\sum_t J_{\\mathrm{wf}}(t; m_0) \\, r_{\\mathrm{wf}}(t; m_0)}{\\sum_t J_{\\mathrm{wf}}(t; m_0)^2}\n$$\n\n**2. Envelope-Based Objective $\\Phi_{\\mathrm{env}}(m)$**\n\nThe residual is $r_{\\mathrm{env}}(t; m) = e_{\\mathrm{obs}}(t) - e(t; m)$, where $e(t; m)$ is the amplitude envelope of the signal $d(t; m)$, defined as $e(t; m) = |d(t;m) + i \\mathcal{H}\\{d(t;m)\\}|$, with $\\mathcal{H}\\{\\cdot\\}$ denoting the Hilbert transform.\nLet $x(t; m) = d(t; m)$ and $y(t; m) = \\mathcal{H}\\{d(t; m)\\}$. Then $e(t; m) = \\sqrt{x^2 + y^2}$. The Jacobian of the forward model $e(t; m)$ is:\n$$\nJ_{\\mathrm{env}}(t; m) = \\frac{\\partial e}{\\partial m} = \\frac{1}{2\\sqrt{x^2+y^2}} \\left( 2x \\frac{\\partial x}{\\partial m} + 2y \\frac{\\partial y}{\\partial m} \\right) = \\frac{x \\frac{\\partial x}{\\partial m} + y \\frac{\\partial y}{\\partial m}}{e}\n$$\nWe have $\\frac{\\partial x}{\\partial m} = \\frac{\\partial d}{\\partial m} = J_{\\mathrm{wf}}(t; m)$. Since the Hilbert transform is a linear operator, its application commutes with differentiation with respect to $m$:\n$$\n\\frac{\\partial y}{\\partial m} = \\frac{\\partial}{\\partial m} \\mathcal{H}\\{d(t; m)\\} = \\mathcal{H}\\left\\{\\frac{\\partial d(t; m)}{\\partial m}\\right\\} = \\mathcal{H}\\{J_{\\mathrm{wf}}(t; m)\\}\n$$\nSubstituting these results, we get the explicit formula for the envelope Jacobian:\n$$\nJ_{\\mathrm{env}}(t; m) = \\frac{d(t; m) J_{\\mathrm{wf}}(t; m) + \\mathcal{H}\\{d(t; m)\\} \\mathcal{H}\\{J_{\\mathrm{wf}}(t; m)\\}}{e(t; m)}\n$$\nThe Gauss-Newton step for the envelope objective is therefore:\n$$\n\\Delta m_{\\mathrm{env}} = \\frac{\\sum_t J_{\\mathrm{env}}(t; m_0) \\, r_{\\mathrm{env}}(t; m_0)}{\\sum_t J_{\\mathrm{env}}(t; m_0)^2}\n$$\n\n**3. Cycle Skipping Analysis**\n\nThe waveform-based objective $\\Phi_{\\mathrm{wf}}$ is highly oscillatory as a function of the model parameter $m$, exhibiting numerous local minima. These minima arise when the predicted wavelet $d(t; m_0)$ is misaligned with the observed data $d_{\\mathrm{obs}}(t)$ by an integer multiple of the wavelet's half-period. The numerator of the $\\Delta m_{\\mathrm{wf}}$ formula, $\\sum_t J_{\\mathrm{wf}} r_{\\mathrm{wf}}$, represents the cross-correlation of the residual with the Jacobian. When the initial guess $m_0$ is far from the true value $m_\\star$, the time-shift $\\tau(m_0) - \\tau(m_\\star)$ can be large. If this shift exceeds approximately half the dominant period of the wavelet, the correlation can become small or even switch sign. A sign switch leads to a Gauss-Newton step in the wrong direction, away from the true solution. This failure to find the correct minimum is known as cycle skipping.\n\nIn contrast, the envelope $e(t; m)$ is a smooth, non-oscillatory function of time, whose maximum is located at the group arrival time $\\tau(m)$. The envelope-based objective $\\Phi_{\\mathrm{env}}$ is consequently much smoother and more convex over a wider range of $m$ values. Its basin of attraction around the global minimum is larger, making the optimization less susceptible to cycle skipping. The residual $r_{\\mathrm{env}}$ captures the mismatch in arrival time without the oscillatory interference, and its correlation with its Jacobian $J_{\\mathrm{env}}$ generally provides a robust update direction even for large initial errors in $m_0$.\n\nThe following program implements these calculations for the specified test cases, demonstrating the robustness of the envelope misfit compared to the waveform misfit. A small constant $\\epsilon$ is added to the envelope in the denominator of $J_{\\mathrm{env}}$ to ensure numerical stability. The computed step $\\Delta m$ is the search direction from the standard Gauss-Newton formulation multiplied by $-1$. For an update rule $m_{k+1} = m_k + \\Delta m_{step}$, our computed $\\Delta m$ should be equal to $-\\Delta m_{step}$. This means if $m_0 > m_\\star$, we expect a positive $\\Delta m$, and if $m_0 < m_\\star$, a negative $\\Delta m$. However, the analysis of the gradient shows that our $\\Delta m = (J^TJ)^{-1}J^Tr$ term should be positive when $m_0 < m_\\star$ and negative when $m_0 > m_\\star$. The implementation will follow this derivation, expecting a positive step to increase the model value.", "answer": "```python\nimport numpy as np\nfrom scipy.signal import hilbert\n\ndef solve():\n    \"\"\"\n    Computes and compares Gauss-Newton steps for waveform and envelope\n    misfit functions in a 1D seismic inversion toy problem.\n    \"\"\"\n    # Physical and numerical parameters\n    z = 1000.0  # Reflector depth in meters\n    m_true = 2000.0  # True velocity in m/s\n    f0 = 10.0  # Central frequency in Hz\n    dt = 0.001  # Time sampling interval in seconds\n    t_max = 2.5  # Record length in seconds\n    epsilon = 1e-9 # Small constant for numerical stability\n\n    # Time vector\n    t = np.arange(0, t_max, dt)\n\n    # Ricker wavelet and its derivative\n    a = (np.pi * f0)**2\n    def ricker(time_vec):\n        return (1.0 - 2.0 * a * time_vec**2) * np.exp(-a * time_vec**2)\n\n    def ricker_derivative(time_vec):\n        return np.exp(-a * time_vec**2) * time_vec * (-6.0 * a + 4.0 * a**2 * time_vec**2)\n\n    # Forward modeling operator\n    def forward_model(m, time_vec):\n        tau = 2.0 * z / m\n        return ricker(time_vec - tau)\n\n    # Generate observed data\n    d_obs = forward_model(m_true, t)\n\n    # Test cases for initial model velocities\n    test_cases = [2000.0, 2100.0, 1800.0, 1500.0]\n\n    results = []\n    \n    for m0 in test_cases:\n        # Calculate predicted data for the current model parameter m0\n        d_pred = forward_model(m0, t)\n\n        # --- 1. Waveform-based inversion step ---\n        r_wf = d_obs - d_pred\n        \n        # Calculate Jacobian for waveform misfit\n        tau0 = 2.0 * z / m0\n        # The Jacobian is d(d)/dm = s'(t-tau) * d(-tau)/dm = s'(t-tau) * (2z/m^2)\n        J_wf = (2.0 * z / m0**2) * ricker_derivative(t - tau0)\n\n        # Calculate Gauss-Newton step for waveform\n        numerator_wf = np.sum(J_wf * r_wf)\n        denominator_wf = np.sum(J_wf**2)\n        \n        if np.isclose(denominator_wf, 0):\n            delta_m_wf = 0.0\n        else:\n            # The formula in the problem is p = (J^T J)^-1 J^T r, where r = d_obs - d_pred\n            # This is the standard Gauss-Newton step direction.\n            # An update would be m_k+1 = m_k + alpha * p.\n            delta_m_wf = numerator_wf / denominator_wf\n\n        # --- 2. Envelope-based inversion step ---\n        \n        # Calculate analytic signals and envelopes\n        analytic_obs = hilbert(d_obs)\n        analytic_pred = hilbert(d_pred)\n        \n        e_obs = np.abs(analytic_obs)\n        e_pred = np.abs(analytic_pred)\n\n        r_env = e_obs - e_pred\n        \n        # Calculate Jacobian for envelope misfit\n        # J_env = d(e)/dm = [d*d(d)/dm + H{d}*d(H{d})/dm] / e\n        #       = [d*J_wf + H{d}*H{J_wf}] / e\n        analytic_J_wf = hilbert(J_wf)\n        \n        # The numerator is the real part of (analytic_pred_conj * analytic_J_wf)\n        numerator_J_env = np.real(np.conj(analytic_pred) * analytic_J_wf)\n        J_env = np.divide(numerator_J_env, e_pred + epsilon, \n                          out=np.zeros_like(numerator_J_env), \n                          where=(e_pred + epsilon) != 0)\n\n        # Calculate Gauss-Newton step for envelope\n        numerator_env = np.sum(J_env * r_env)\n        denominator_env = np.sum(J_env**2)\n\n        if np.isclose(denominator_env, 0):\n            delta_m_env = 0.0\n        else:\n            delta_m_env = numerator_env / denominator_env\n\n        results.append([round(delta_m_wf, 6), round(delta_m_env, 6)])\n\n    # Format the final output string\n    output_str = \"[\" + \",\".join([f\"[{res[0]:.6f},{res[1]:.6f}]\" for res in results]) + \"]\"\n    print(output_str)\n\nsolve()\n```", "id": "3599323"}, {"introduction": "The raw Gauss-Newton update step provides a search direction, but its length may not be optimal and can lead to overshooting the minimum or slow convergence. This exercise moves from theory to application by having you implement a step-length control mechanism to refine the update, a crucial technique for ensuring robust convergence in practice [@problem_id:3384199]. You will also discover the direct relationship between the first Gauss-Newton step and the well-known Born approximation, connecting the iterative optimization framework to fundamental concepts in wave-based inversion.", "problem": "Consider a discrete one-dimensional inverse scattering problem in which the forward map is modeled by the sum of single-scattering and a second-order interaction term. Let the unknown contrast vector be $m \\in \\mathbb{R}^n$, the single-scattering operator be a known matrix $H \\in \\mathbb{R}^{n \\times n}$, and the interaction kernel be a known matrix $K \\in \\mathbb{R}^{n \\times n}$. The forward operator is defined by\n$$\nF(m) \\;=\\; H m \\;+\\; m \\odot (K m),\n$$\nwhere $m \\odot (K m)$ denotes the elementwise product of $m$ with $K m$. Suppose synthetic observations $y \\in \\mathbb{R}^n$ are generated according to $y = F(m_{\\text{true}}) + \\eta$, where $m_{\\text{true}}$ is the ground-truth contrast and $\\eta$ is a known deterministic noise vector.\n\nThe goal is to compare the first step of the Gauss–Newton (GN) method for the Nonlinear Least Squares (NLS) objective to the Born approximation, to quantify the differences in moderate-contrast regimes, and to implement a mismatch-based step-length control along the first-step direction. Specifically:\n\n1. Define the Nonlinear Least Squares (NLS) objective\n$$\n\\Phi(m) \\;=\\; \\tfrac{1}{2}\\,\\|F(m) - y\\|_2^2.\n$$\n\n2. Consider the Gauss–Newton (GN) method linearizing $F(m)$ around an initial guess $m_0$. The first GN step from $m_0$ solves the Least Squares (LS) problem for the linearized residual. For the Born approximation, linearize the forward map around the zero-contrast reference and solve an LS problem against the data using the single-scattering operator.\n\n3. In this problem, you must construct $H$ and $K$ as follows for a discrete grid of size $n$:\n   - Grid size: $n = 20$.\n   - Single-scattering operator $H$ with entries\n     $$\n     H_{ij} \\;=\\; \\exp\\!\\Big(-\\frac{|i-j|}{\\ell}\\Big), \\quad \\text{with } \\ell = 3.\n     $$\n   - Interaction kernel $K$ with entries\n     $$\n     K_{ij} \\;=\\; \\beta\\,\\exp\\!\\Big(-\\frac{|i-j|}{s}\\Big), \\quad \\text{with } s = 2 \\text{ and } \\beta = \\frac{0.6}{n}.\n     $$\n\n4. Define the ground-truth contrast $m_{\\text{true}}$ as a Gaussian bump of amplitude $a$ centered at the middle index, i.e.,\n   $$\n   m_{\\text{true}}(i) \\;=\\; a\\,\\exp\\!\\Big(-\\frac{(i - c)^2}{2\\sigma^2}\\Big), \\quad \\text{for } i \\in \\{0,1,\\dots,n-1\\},\n   $$\n   with $c = \\frac{n-1}{2}$ and $\\sigma = 3$.\n\n5. Define the deterministic noise vector $\\eta \\in \\mathbb{R}^n$ componentwise by\n   $$\n   \\eta_i \\;=\\; \\varepsilon \\,\\sin\\!\\Big(\\frac{2\\pi i}{n}\\Big), \\quad \\text{for } i \\in \\{0,1,\\dots,n-1\\}.\n   $$\n\n6. For each test case below, form $y = F(m_{\\text{true}}) + \\eta$. Compute:\n   - The Born approximation $m_{\\text{Born}}$ by solving the LS problem $H m \\approx y$, i.e., find the minimizer of $\\|H m - y\\|_2^2$.\n   - The first GN step from $m_0 = 0$. Show by construction that the GN first-step direction coincides with the Born solution direction; however, evaluate the mismatch against the nonlinear forward map to reveal differences in moderate-contrast regimes.\n   - Implement a mismatch-based step-length control along the Born direction, i.e., choose a scalar $\\alpha \\ge 0$ that minimizes the squared nonlinear residual\n     $$\n     \\|F(\\alpha\\,m_{\\text{Born}}) - y\\|_2^2\n     $$\n     along the one-dimensional ray parameterized by $\\alpha$. You must determine $\\alpha$ algorithmically, without relying on derivative-free random searches. Your choice must be reproducible and deterministic.\n\n7. For each test case, report the following three quantities:\n   - The optimal step length $\\alpha$ chosen by your mismatch-based rule.\n   - The relative nonlinear mismatch of the unscaled Born step, defined as\n     $$\n     r_{\\text{Born}} \\;=\\; \\frac{\\|F(m_{\\text{Born}}) - y\\|_2}{\\|y\\|_2}.\n     $$\n   - The relative nonlinear mismatch of the $\\alpha$-scaled first step, defined as\n     $$\n     r_{\\alpha} \\;=\\; \\frac{\\|F(\\alpha\\, m_{\\text{Born}}) - y\\|_2}{\\|y\\|_2}.\n     $$\n\n8. Test suite and coverage:\n   - Case $1$ (low contrast, noiseless): $a = 0.1$, $\\varepsilon = 0.0$.\n   - Case $2$ (moderate contrast, noiseless): $a = 0.6$, $\\varepsilon = 0.0$.\n   - Case $3$ (moderate-high contrast, low noise): $a = 1.0$, $\\varepsilon = 0.02$.\n\n9. Final output format:\n   - Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Aggregate the results for the three test cases in order, each case contributing the three floating-point values $(\\alpha, r_{\\text{Born}}, r_{\\alpha})$. Thus the final output will contain $9$ values in the order corresponding to the cases $1$ through $3$:\n     $$\n     [\\alpha_1, r_{\\text{Born},1}, r_{\\alpha,1}, \\alpha_2, r_{\\text{Born},2}, r_{\\alpha,2}, \\alpha_3, r_{\\text{Born},3}, r_{\\alpha,3}].\n     $$\n   - All outputs are unitless real numbers. Express all ratios as decimal values.", "solution": "The problem requires an analysis and implementation of the Gauss-Newton method for a specific nonlinear least squares problem motivated by inverse scattering theory. The primary tasks are to relate the first Gauss-Newton step to the Born approximation and to implement a step-length control mechanism.\n\nFirst, we formalize the problem. The forward operator $F: \\mathbb{R}^n \\to \\mathbb{R}^n$ maps a contrast vector $m \\in \\mathbb{R}^n$ to a data vector via\n$$\nF(m) = H m + m \\odot (K m)\n$$\nwhere $H, K \\in \\mathbb{R}^{n \\times n}$ are known matrices, and $\\odot$ denotes the elementwise (Hadamard) product. Given noisy data $y = F(m_{\\text{true}}) + \\eta$, we seek to estimate $m$ by minimizing the nonlinear least squares objective:\n$$\n\\Phi(m) = \\frac{1}{2} \\| F(m) - y \\|_2^2\n$$\n\nThe Gauss-Newton (GN) method is an iterative algorithm for solving such problems. Starting from an initial guess $m_k$, the next iterate is $m_{k+1} = m_k + p_k$, where the update step $p_k$ is the solution to the linear least squares problem:\n$$\n\\min_{p} \\frac{1}{2} \\| J(m_k) p + r(m_k) \\|_2^2\n$$\nHere, $r(m_k) = F(m_k) - y$ is the residual vector and $J(m)$ is the Jacobian matrix of the forward operator $F(m)$. The step $p_k$ is found by solving the normal equations:\n$$\n(J(m_k)^T J(m_k)) p_k = -J(m_k)^T r(m_k)\n$$\n\nTo proceed, we must compute the Jacobian of $F(m)$. The $i$-th component of $F(m)$ is\n$$\nF_i(m) = \\sum_{j=0}^{n-1} H_{ij} m_j + m_i \\sum_{j=0}^{n-1} K_{ij} m_j\n$$\nThe partial derivative with respect to the $k$-th component of $m$, $m_k$, gives the $(i,k)$-th entry of the Jacobian matrix:\n$$\n[J(m)]_{ik} = \\frac{\\partial F_i}{\\partial m_k} = H_{ik} + \\delta_{ik} \\sum_{j=0}^{n-1} K_{ij} m_j + m_i K_{ik}\n$$\nwhere $\\delta_{ik}$ is the Kronecker delta. In matrix notation, this is:\n$$\nJ(m) = H + \\text{diag}(Km) + \\text{diag}(m) K^T\n$$\nSince the kernel $K$ is defined symmetrically ($K_{ij} = K_{ji}$), we have $K = K^T$, so\n$$\nJ(m) = H + \\text{diag}(Km) + \\text{diag}(m) K\n$$\n\nThe problem specifies an initial guess $m_0 = 0$. At this point, the Jacobian simplifies significantly:\n$$\nJ(m_0=0) = H + \\text{diag}(K \\cdot 0) + \\text{diag}(0) K = H\n$$\nThe residual at $m_0$ is $r(m_0) = F(0) - y = 0 - y = -y$. The first GN step, denoted $p_0$, is found by solving the linear system arising from the linearized problem:\n$$\nJ(m_0) p_0 = -r(m_0) \\implies H p_0 = y\n$$\nThis requires solving the linear least squares problem $\\min_{p_0} \\|H p_0 - y\\|_2^2$.\n\nThe Born approximation, $m_{\\text{Born}}$, is defined as the solution to the same linear least squares problem, $\\min_m \\|H m - y\\|_2^2$. Therefore, by construction, the first GN search direction $p_0$ from the zero-contrast reference is identical to the Born approximation: $p_0 = m_{\\text{Born}}$. The first GN update would be $m_1 = m_0 + p_0 = 0 + m_{\\text{Born}} = m_{\\text{Born}}$.\n\nWhile the first GN update is the Born solution, this does not guarantee it minimizes the *nonlinear* residual. For moderate to high contrast, $F(m_{\\text{Born}})$ may be a poor approximation to $y$. To address this, a line search is performed along the direction $p_0 = m_{\\text{Born}}$. We seek an optimal step length $\\alpha \\ge 0$ that minimizes the nonlinear objective function:\n$$\n\\min_{\\alpha \\ge 0} g(\\alpha) = \\| F(\\alpha m_{\\text{Born}}) - y \\|_2^2\n$$\nLet's substitute the definition of $F$. Let $p = m_{\\text{Born}}$.\n$$\ng(\\alpha) = \\| H(\\alpha p) + (\\alpha p) \\odot (K(\\alpha p)) - y \\|_2^2\n$$\n$$\ng(\\alpha) = \\| \\alpha (Hp) + \\alpha^2 (p \\odot (Kp)) - y \\|_2^2\n$$\nDefine the constant vectors $v_1 = Hp$ and $v_2 = p \\odot (Kp)$. The objective becomes:\n$$\ng(\\alpha) = \\| \\alpha^2 v_2 + \\alpha v_1 - y \\|_2^2\n$$\nExpanding the squared norm yields a quartic polynomial in $\\alpha$:\n$$\ng(\\alpha) = (v_2^T v_2)\\alpha^4 + 2(v_1^T v_2)\\alpha^3 + (v_1^T v_1 - 2 v_2^T y)\\alpha^2 - 2(v_1^T y)\\alpha + y^T y\n$$\nTo find the minimum of $g(\\alpha)$ for $\\alpha \\ge 0$, we find its critical points by setting the derivative $g'(\\alpha)$ to zero:\n$$\ng'(\\alpha) = 4(v_2^T v_2)\\alpha^3 + 6(v_1^T v_2)\\alpha^2 + 2(v_1^T v_1 - 2 v_2^T y)\\alpha - 2(v_1^T y) = 0\n$$\nThis is a cubic equation in $\\alpha$. We can solve it numerically for its real roots. The optimal $\\alpha$ must be one of the non-negative real roots or the boundary point $\\alpha=0$. We evaluate $g(\\alpha)$ at each of these candidates to find the global minimum for $\\alpha \\ge 0$.\n\nThe algorithm for each test case is as follows:\n1.  Construct matrices $H$ and $K$, ground-truth model $m_{\\text{true}}$, and noise vector $\\eta$ using the given parameters ($n=20$, $\\ell=3$, $s=2$, $\\beta=0.6/n$, $c=9.5$, $\\sigma=3$, and case-specific $a$, $\\varepsilon$).\n2.  Generate the synthetic data $y = F(m_{\\text{true}}) + \\eta$.\n3.  Solve the linear least squares problem $\\min_m \\|H m - y\\|_2^2$ to find $m_{\\text{Born}}$.\n4.  Calculate the relative nonlinear mismatch of the unscaled Born step: $r_{\\text{Born}} = \\|F(m_{\\text{Born}}) - y\\|_2 / \\|y\\|_2$.\n5.  Determine the optimal step length $\\alpha$ by finding the non-negative real roots of $g'(\\alpha)=0$, and selecting the candidate (including $\\alpha=0$) that minimizes $g(\\alpha)$.\n6.  Calculate the relative nonlinear mismatch of the scaled step: $r_{\\alpha} = \\|F(\\alpha m_{\\text{Born}}) - y\\|_2 / \\|y\\|_2$.\n7.  Report the triplet $(\\alpha, r_{\\text{Born}}, r_{\\alpha})$.", "answer": "```python\nimport numpy as np\nfrom scipy.linalg import lstsq\n\ndef solve():\n    \"\"\"\n    Solves the inverse scattering problem for three test cases.\n    \"\"\"\n\n    test_cases = [\n        # Case 1: low contrast, noiseless\n        {\"a\": 0.1, \"epsilon\": 0.0},\n        # Case 2: moderate contrast, noiseless\n        {\"a\": 0.6, \"epsilon\": 0.0},\n        # Case 3: moderate-high contrast, low noise\n        {\"a\": 1.0, \"epsilon\": 0.02},\n    ]\n\n    # Global parameters\n    n = 20\n    l = 3.0\n    s = 2.0\n    beta = 0.6 / n\n    c = (n - 1) / 2.0\n    sigma = 3.0\n    \n    # Construct operators H and K\n    indices_1d = np.arange(n)\n    i, j = np.ogrid[:n, :n]\n    H = np.exp(-np.abs(i - j) / l)\n    K = beta * np.exp(-np.abs(i - j) / s)\n\n    results = []\n\n    for case in test_cases:\n        a = case[\"a\"]\n        epsilon = case[\"epsilon\"]\n\n        # 1. Construct ground truth and generate data\n        m_true = a * np.exp(-(indices_1d - c)**2 / (2 * sigma**2))\n        eta = epsilon * np.sin(2 * np.pi * indices_1d / n)\n        \n        # Forward operator F(m) = Hm + m * (Km)\n        y = H @ m_true + m_true * (K @ m_true) + eta\n        norm_y = np.linalg.norm(y)\n\n        # 2. Compute Born approximation\n        # Solves the least-squares problem min||Hm - y||^2\n        m_born, _, _, _ = lstsq(H, y, cond=None)\n\n        # 3. Compute relative nonlinear mismatch for the unscaled Born step\n        F_m_born = H @ m_born + m_born * (K @ m_born)\n        r_born = np.linalg.norm(F_m_born - y) / norm_y\n\n        # 4. Find optimal step-length alpha\n        # We want to minimize g(alpha) = ||F(alpha*m_born) - y||^2\n        p = m_born\n        v1 = H @ p\n        v2 = p * (K @ p)\n        \n        # Coefficients of the derivative g'(alpha).\n        # g'(alpha) = c3*alpha^3 + c2*alpha^2 + c1*alpha + c0 = 0\n        c3 = 4 * np.dot(v2, v2)\n        c2 = 6 * np.dot(v1, v2)\n        c1 = 2 * (np.dot(v1, v1) - 2 * np.dot(v2, y))\n        c0 = -2 * np.dot(v1, y)\n        \n        coeffs = [c3, c2, c1, c0]\n        \n        # Find roots of the cubic polynomial\n        roots = np.roots(coeffs)\n        \n        # Filter for non-negative real roots\n        real_roots = roots[np.isreal(roots)].real\n        alpha_candidates = real_roots[real_roots >= 0]\n        \n        # Also check the boundary alpha=0\n        alpha_candidates = np.unique(np.append(alpha_candidates, 0.0))\n\n        # Evaluate g(alpha) for all candidates to find the minimum\n        g_alpha_func = lambda alpha: np.linalg.norm(alpha**2 * v2 + alpha * v1 - y)**2\n        \n        g_values = np.array([g_alpha_func(val) for val in alpha_candidates])\n        \n        # Find the best alpha and the corresponding minimum value of g\n        best_alpha = alpha_candidates[np.argmin(g_values)]\n        min_g = np.min(g_values)\n\n        # 5. Compute relative nonlinear mismatch for the alpha-scaled step\n        r_alpha = np.sqrt(min_g) / norm_y\n        \n        results.extend([best_alpha, r_born, r_alpha])\n\n    # 6. Format and print final output\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3384199"}]}