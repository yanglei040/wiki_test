{"hands_on_practices": [{"introduction": "Understanding the minimum-length solution begins with its fundamental construction. This first practice challenges you to derive the solution from first principles using the Singular Value Decomposition (SVD), the bedrock of many linear inverse methods. By manually computing the SVD for a small, underdetermined system, you will gain a concrete understanding of how the minimum-norm criterion systematically suppresses contributions from the nullspace to select a unique, stable solution from an infinite family of possibilities [@problem_id:3610316].", "problem": "In computational geophysics, linearized forward models often lead to underdetermined systems where the number of model parameters exceeds the number of data observations. Consider the linear forward model $d = A m$ with $A \\in \\mathbb{R}^{2 \\times 3}$ and $\\operatorname{rank}(A) = 2$. The minimum-length solution is defined as the model vector $m^{\\star}$ with minimum Euclidean norm among all vectors $m$ that satisfy the data equation.\n\nLet\n$$\nA = \\begin{pmatrix}\n\\sqrt{2} & 0 & \\sqrt{2} \\\\\n0 & 1 & 0\n\\end{pmatrix}, \\quad d = \\begin{pmatrix} d_{1} \\\\ d_{2} \\end{pmatrix},\n$$\nwhere $d_{1} \\in \\mathbb{R}$ and $d_{2} \\in \\mathbb{R}$ are given data. Using only the definition of the Singular Value Decomposition (SVD) and the definition of the minimum-length solution, proceed as follows:\n\n- Construct an SVD $A = U \\Sigma V^{\\top}$ that is consistent with the given $A$ and satisfies $U \\in \\mathbb{R}^{2 \\times 2}$ orthogonal, $V \\in \\mathbb{R}^{3 \\times 3}$ orthogonal, and $\\Sigma \\in \\mathbb{R}^{2 \\times 3}$ diagonal with nonnegative diagonal entries in descending order.\n- Using this SVD and orthogonality properties, derive an analytic expression for the minimum-length solution $m^{\\star}$ to the constraint $A m = d$.\n- Show in your reasoning how the null-space component is suppressed in $m^{\\star}$ compared to the general solution set.\n\nProvide your final answer for $m^{\\star}$ as a single row matrix in terms of $d_{1}$ and $d_{2}$. No numerical rounding is required. No physical units are required.", "solution": "The problem is validated as follows.\n\n**Step 1: Extract Givens**\n- Linear forward model: $d = A m$\n- Model matrix: $A = \\begin{pmatrix} \\sqrt{2} & 0 & \\sqrt{2} \\\\ 0 & 1 & 0 \\end{pmatrix}$\n- Dimensions and properties of $A$: $A \\in \\mathbb{R}^{2 \\times 3}$, $\\operatorname{rank}(A) = 2$.\n- Data vector: $d = \\begin{pmatrix} d_{1} \\\\ d_{2} \\end{pmatrix}$, with $d_{1} \\in \\mathbb{R}$ and $d_{2} \\in \\mathbb{R}$.\n- Definition of minimum-length solution: The model vector $m^{\\star}$ with minimum Euclidean norm among all vectors $m$ that satisfy $A m = d$.\n- Required SVD form: $A = U \\Sigma V^{\\top}$, with $U \\in \\mathbb{R}^{2 \\times 2}$ orthogonal, $V \\in \\mathbb{R}^{3 \\times 3}$ orthogonal, and $\\Sigma \\in \\mathbb{R}^{2 \\times 3}$ diagonal with nonnegative, descending diagonal entries.\n- Required tasks:\n    1. Construct the specified SVD of $A$.\n    2. Derive an analytic expression for the minimum-length solution $m^{\\star}$ using this SVD.\n    3. Show how the null-space component is suppressed in $m^{\\star}$.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded (Critical)**: The problem is fundamentally a linear algebra problem concerning the solution of an underdetermined system using Singular Value Decomposition. This is a standard, well-established method in many STEM fields, including computational geophysics. The problem is scientifically and mathematically sound.\n- **Well-Posed**: The problem is well-posed. The matrix $A$ has full row rank ($\\operatorname{rank}(A) = 2$), which guarantees that the system $A m = d$ has solutions for any $d$. The request for a minimum-norm solution leads to a unique solution.\n- **Objective (Critical)**: The problem is stated using precise mathematical language and definitions. There are no subjective or opinion-based statements.\n- **Completeness and Consistency**: All necessary information (the matrix $A$, vector $d$, and definitions) is provided. The givens are internally consistent (e.g., the given matrix $A$ indeed has rank $2$).\n- **Other criteria**: The problem does not violate any other validation criteria. It is formalizable, directly relevant to the specified topic, realistic within a mathematical context, and non-trivial.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A full solution will be provided.\n\nThe problem asks for the minimum-length solution $m^{\\star}$ to the underdetermined linear system $A m = d$. This is the vector $m^{\\star}$ that minimizes the Euclidean norm $\\|m\\|_{2}$ subject to the constraint $A m = d$. We will solve this using the an SVD of $A$.\n\nFirst, we construct the Singular Value Decomposition (SVD) of $A$, which is given by $A = U \\Sigma V^\\top$.\n\nThe singular values $\\sigma_i$ of $A$ are the square roots of the non-zero eigenvalues of $A A^\\top$.\n$$\nA A^\\top = \\begin{pmatrix} \\sqrt{2} & 0 & \\sqrt{2} \\\\ 0 & 1 & 0 \\end{pmatrix} \\begin{pmatrix} \\sqrt{2} & 0 \\\\ 0 & 1 \\\\ \\sqrt{2} & 0 \\end{pmatrix} = \\begin{pmatrix} 2+0+2 & 0 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 4 & 0 \\\\ 0 & 1 \\end{pmatrix}\n$$\nThe eigenvalues of $A A^\\top$ are $\\lambda_1 = 4$ and $\\lambda_2 = 1$. The singular values are their square roots, sorted in descending order: $\\sigma_1 = \\sqrt{4} = 2$ and $\\sigma_2 = \\sqrt{1} = 1$.\nThe matrix $\\Sigma \\in \\mathbb{R}^{2 \\times 3}$ is constructed from these singular values:\n$$\n\\Sigma = \\begin{pmatrix} \\sigma_1 & 0 & 0 \\\\ 0 & \\sigma_2 & 0 \\end{pmatrix} = \\begin{pmatrix} 2 & 0 & 0 \\\\ 0 & 1 & 0 \\end{pmatrix}\n$$\nThe matrix $U \\in \\mathbb{R}^{2 \\times 2}$ consists of the normalized eigenvectors of $A A^\\top$. Since $A A^\\top$ is a diagonal matrix, its eigenvectors are the standard basis vectors.\nFor $\\lambda_1 = 4$: $u_1 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$.\nFor $\\lambda_2 = 1$: $u_2 = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$.\nThus, $U$ is the identity matrix $I_2$:\n$$\nU = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}\n$$\nThe matrix $V \\in \\mathbb{R}^{3 \\times 3}$ consists of the normalized eigenvectors of $A^\\top A$.\n$$\nA^\\top A = \\begin{pmatrix} \\sqrt{2} & 0 \\\\ 0 & 1 \\\\ \\sqrt{2} & 0 \\end{pmatrix} \\begin{pmatrix} \\sqrt{2} & 0 & \\sqrt{2} \\\\ 0 & 1 & 0 \\end{pmatrix} = \\begin{pmatrix} 2 & 0 & 2 \\\\ 0 & 1 & 0 \\\\ 2 & 0 & 2 \\end{pmatrix}\n$$\nThe eigenvalues of $A^\\top A$ are $\\lambda_1 = 4$, $\\lambda_2 = 1$, and $\\lambda_3 = 0$.\nFor $\\lambda_1 = 4$: $(A^\\top A - 4I)v_1 = 0 \\implies \\begin{pmatrix} -2 & 0 & 2 \\\\ 0 & -3 & 0 \\\\ 2 & 0 & -2 \\end{pmatrix} v_1 = 0$. This gives $x=z$ and $y=0$. A corresponding eigenvector is $\\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix}$. Normalizing gives $v_1 = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix}$.\nFor $\\lambda_2 = 1$: $(A^\\top A - 1I)v_2 = 0 \\implies \\begin{pmatrix} 1 & 0 & 2 \\\\ 0 & 0 & 0 \\\\ 2 & 0 & 1 \\end{pmatrix} v_2 = 0$. This gives $x=0$ and $z=0$. A corresponding eigenvector is $\\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}$. This is already normalized, so $v_2 = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}$.\nFor $\\lambda_3 = 0$: $(A^\\top A - 0I)v_3 = 0 \\implies \\begin{pmatrix} 2 & 0 & 2 \\\\ 0 & 1 & 0 \\\\ 2 & 0 & 2 \\end{pmatrix} v_3 = 0$. This gives $x=-z$ and $y=0$. An eigenvector is $\\begin{pmatrix} 1 \\\\ 0 \\\\ -1 \\end{pmatrix}$. Normalizing gives $v_3 = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ 0 \\\\ -1 \\end{pmatrix}$.\nThe orthogonal matrix $V$ is formed by these eigenvectors as columns:\n$$\nV = \\begin{pmatrix} v_1 & v_2 & v_3 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{\\sqrt{2}} & 0 & \\frac{1}{\\sqrt{2}} \\\\ 0 & 1 & 0 \\\\ \\frac{1}{\\sqrt{2}} & 0 & -\\frac{1}{\\sqrt{2}} \\end{pmatrix}\n$$\nNow we use the SVD to solve for the minimum-length solution $m^{\\star}$. The constraint equation is $A m = d$. Substituting the SVD:\n$$\nU \\Sigma V^\\top m = d\n$$\nSince $U$ is orthogonal ($U^\\top U = I$), we can multiply by $U^\\top$ from the left:\n$$\n\\Sigma V^\\top m = U^\\top d\n$$\nLet's define a transformed model vector $m' = V^\\top m$. Since $V$ is orthogonal, the Euclidean norm is preserved: $\\|m'\\|_{2} = \\|V^\\top m\\|_{2} = \\|m\\|_{2}$. Therefore, minimizing $\\|m\\|_{2}$ is equivalent to minimizing $\\|m'\\|_{2}$.\nThe equation becomes $\\Sigma m' = U^\\top d$.\nWith $U=I_2$, we have $U^\\top d = d$. Let $m' = \\begin{pmatrix} m'_1 \\\\ m'_2 \\\\ m'_3 \\end{pmatrix}$. The system is:\n$$\n\\begin{pmatrix} 2 & 0 & 0 \\\\ 0 & 1 & 0 \\end{pmatrix} \\begin{pmatrix} m'_1 \\\\ m'_2 \\\\ m'_3 \\end{pmatrix} = \\begin{pmatrix} d_1 \\\\ d_2 \\end{pmatrix}\n$$\nThis yields two equations:\n$2 m'_1 = d_1 \\implies m'_1 = \\frac{d_1}{2}$\n$1 m'_2 = d_2 \\implies m'_2 = d_2$\nThe component $m'_3$ is not constrained by the data equation. The general solution for $m'$ is $m' = \\begin{pmatrix} d_1/2 \\\\ d_2 \\\\ \\alpha \\end{pmatrix}$ for any $\\alpha \\in \\mathbb{R}$.\nWe want to find the solution with the minimum norm. The squared norm of $m'$ is:\n$$\n\\|m'\\|_{2}^{2} = (m'_1)^2 + (m'_2)^2 + (m'_3)^2 = \\left(\\frac{d_1}{2}\\right)^2 + d_2^2 + \\alpha^2\n$$\nTo minimize this norm, we must choose $\\alpha=0$. The minimum-length transformed solution is therefore:\n$$\nm'^{\\star} = \\begin{pmatrix} d_1/2 \\\\ d_2 \\\\ 0 \\end{pmatrix}\n$$\nFinally, we transform back to the original model space to find $m^{\\star}$ using the relation $m = V m'$, so $m^{\\star} = V m'^{\\star}$:\n$$\nm^{\\star} = \\begin{pmatrix} \\frac{1}{\\sqrt{2}} & 0 & \\frac{1}{\\sqrt{2}} \\\\ 0 & 1 & 0 \\\\ \\frac{1}{\\sqrt{2}} & 0 & -\\frac{1}{\\sqrt{2}} \\end{pmatrix} \\begin{pmatrix} \\frac{d_1}{2} \\\\ d_2 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{\\sqrt{2}}\\frac{d_1}{2} + 0 + 0 \\\\ 0 + d_2 + 0 \\\\ \\frac{1}{\\sqrt{2}}\\frac{d_1}{2} + 0 + 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{d_1}{2\\sqrt{2}} \\\\ d_2 \\\\ \\frac{d_1}{2\\sqrt{2}} \\end{pmatrix}\n$$\nRationalizing the denominator gives:\n$$\nm^{\\star} = \\begin{pmatrix} \\frac{\\sqrt{2}}{4}d_1 \\\\ d_2 \\\\ \\frac{\\sqrt{2}}{4}d_1 \\end{pmatrix}\n$$\nTo show how the null-space component is suppressed, we consider the general solution $m_{gen}$. The null space of $A$ is the set of all vectors $m_n$ such that $A m_n = 0$. From the SVD, the null space is spanned by the columns of $V$ corresponding to zero singular values. Here, $\\sigma_3=0$, so the null space is spanned by $v_3 = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ 0 \\\\ -1 \\end{pmatrix}$. Any vector in the null space is of the form $m_n = c v_3$ for some scalar $c$.\nThe general solution to $A m = d$ is the sum of any particular solution and a vector from the null space. We can use $m^{\\star}$ as the particular solution:\n$$\nm_{gen} = m^{\\star} + m_n = m^{\\star} + c v_3\n$$\nThe solution $m^{\\star}$ lies in the span of $v_1$ and $v_2$: $m^{\\star} = (\\frac{d_1}{2\\sqrt{2}}) \\sqrt{2} v_1 + d_2 v_2 = \\frac{d_1}{2} v_1 + d_2 v_2$. The columns of $V$ are orthonormal, so $m^{\\star}$ is orthogonal to $m_n=c v_3$.\nThe squared norm of the general solution is:\n$$\n\\|m_{gen}\\|_{2}^{2} = \\|m^{\\star} + c v_3\\|_{2}^{2} = (m^{\\star} + c v_3)^\\top(m^{\\star} + c v_3) = \\|m^{\\star}\\|_{2}^{2} + 2 c (m^{\\star})^\\top v_3 + c^2 \\|v_3\\|_{2}^{2}\n$$\nSince $(m^{\\star})^\\top v_3 = 0$ (due to orthogonality) and $\\|v_3\\|_{2}^{2} = 1$:\n$$\n\\|m_{gen}\\|_{2}^{2} = \\|m^{\\star}\\|_{2}^{2} + c^2\n$$\nThis expression is minimized when $c=0$. This corresponds to selecting the solution with a zero component in the null space. Thus, the minimum-length solution $m^{\\star}$ is the one for which the null-space component is completely suppressed.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{\\sqrt{2}}{4} d_{1} & d_{2} & \\frac{\\sqrt{2}}{4} d_{1}\n\\end{pmatrix}\n}\n$$", "id": "3610316"}, {"introduction": "While the minimum-length solution $x^{\\star}$ provides a unique answer, it is often just a starting point. This exercise demonstrates how to systematically explore the infinite set of other data-fitting models to incorporate geological knowledge, a process known as nullspace exploration [@problem_id:3610331]. You will learn to parameterize all solutions as $x = x^{\\star} + N \\alpha$ and then solve for the nullspace coefficients $\\alpha$ that enforce \"hard\" constraints, such as forcing parts of the model to be zero, with minimal perturbation to the original solution.", "problem": "Consider a linear inverse problem in computational geophysics where a discretized sensitivity matrix $A \\in \\mathbb{R}^{m \\times n}$ links subsurface model parameters $x \\in \\mathbb{R}^{n}$ (e.g., density or resistivity contrasts on a grid) to observed data $b \\in \\mathbb{R}^{m}$ via $A x = b$. In many practical settings $m < n$, so the system is underdetermined and admits infinitely many solutions. A commonly accepted base in inverse theory is that the minimum-length (minimum Euclidean norm) solution $x^\\ast$ is obtained by projecting $b$ back into model space with the Moore–Penrose pseudoinverse of $A$. Geological priors may require additional structure, for example $x$ should be sparse or zero in a known zone (e.g., sedimentary basin with negligible contrast). Exploring the nullspace of $A$ provides a way to enforce such features while preserving the minimum-length property among all solutions consistent with the features.\n\nYour task is to formalize and implement the following steps:\n\n1. Starting from the definitions of nullspace and minimum-length solutions, compute a basis $N \\in \\mathbb{R}^{n \\times k}$ for the nullspace $\\ker(A)$, where $k$ is the nullity of $A$. Use a basis with orthonormal columns obtained by Singular Value Decomposition (SVD).\n\n2. Express all solutions to $A x = b$ as $x = x^\\ast + N \\alpha$, where $x^\\ast$ is the minimum-length solution and $\\alpha \\in \\mathbb{R}^{k}$ parameterizes movement in the nullspace.\n\n3. Design a criterion to choose $\\alpha$ that enforces $x$ to be exactly zero on a prescribed index set $S \\subset \\{0,1,\\dots,n-1\\}$ (representing a geological zone), while preserving minimum-length among all solutions that satisfy both $A x = b$ and $x_i = 0$ for all $i \\in S$. Formulate this criterion in purely mathematical terms and derive the resulting algebraic system for $\\alpha$.\n\n4. Implement an algorithm that:\n   - Computes $x^\\ast$ from $A$ and $b$ using the Moore–Penrose pseudoinverse.\n   - Computes an orthonormal basis $N$ for $\\ker(A)$ using Singular Value Decomposition (SVD).\n   - Builds a selector matrix $C \\in \\mathbb{R}^{|S| \\times n}$ that imposes $C x = 0$ for the indices in $S$.\n   - Solves for $\\alpha$ using the derived criterion. If the constraint system is infeasible, detect infeasibility and do not alter $x^\\ast$; if feasible, return the constrained minimum-length solution.\n\nThe mathematical base you must use includes:\n- The definition of the nullspace $\\ker(A) = \\{x \\in \\mathbb{R}^n : A x = 0\\}$.\n- The characterization of the minimum-length solution via the Moore–Penrose pseudoinverse $A^+$, where $x^\\ast = A^+ b$ solves $A x = b$ and minimizes $\\|x\\|_2$.\n- Orthogonal projection identities derived from Singular Value Decomposition (SVD), without providing shortcut formulas beyond these bases.\n\nYou must implement the program and evaluate it on the following explicit test suite. Each test case specifies a matrix $A$, a data vector $b$, and a zone $S$ to be zeroed. All numbers are dimensionless, and there are no physical units. Angles do not appear, and percentages are not involved.\n\nDefine the test suite matrices and vectors as follows:\n\n- Case $1$ (happy path feasibility):\n  $$\n  A_1 = \\begin{bmatrix}\n  1 & 0 & 1 & 2 & 0 \\\\\n  0 & 1 & -1 & 0 & 1 \\\\\n  1 & 1 & 0 & 1 & -1\n  \\end{bmatrix},\\quad\n  x^{(0)}_1 = \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\\\ -2 \\\\ 0.5 \\end{bmatrix},\\quad\n  b_1 = A_1 x^{(0)}_1,\\quad\n  S_1 = \\{0,1\\}.\n  $$\n\n- Case $2$ (multiple nullspace directions, feasible):\n  $$\n  A_2 = \\begin{bmatrix}\n  1 & 2 & 0 & 1 \\\\\n  0 & 1 & 1 & -1\n  \\end{bmatrix},\\quad\n  x^{(0)}_2 = \\begin{bmatrix} 1 \\\\ -1 \\\\ 0 \\\\ 0 \\end{bmatrix},\\quad\n  b_2 = A_2 x^{(0)}_2,\\quad\n  S_2 = \\{3\\}.\n  $$\n\n- Case $3$ (infeasible constraints by design):\n  Let the columns of $A_3$ be $c_0 = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}$, $c_1 = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix}$, $c_2 = \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix}$, $c_3 = \\begin{bmatrix} 0 \\\\ 1 \\\\ 1 \\end{bmatrix}$, $c_4 = \\begin{bmatrix} 0 \\\\ 2 \\\\ -1 \\end{bmatrix}$, and\n  $$\n  A_3 = \\begin{bmatrix} c_0 & c_1 & c_2 & c_3 & c_4 \\end{bmatrix}\n  = \\begin{bmatrix}\n  1 & 0 & 0 & 0 & 0 \\\\\n  0 & 1 & 0 & 1 & 2 \\\\\n  0 & 0 & 1 & 1 & -1\n  \\end{bmatrix},\\quad\n  b_3 = \\begin{bmatrix} 1 \\\\ 1 \\\\ 0 \\end{bmatrix},\\quad\n  S_3 = \\{0\\}.\n  $$\n  In this case $c_0$ is not in the span of the other columns, so any solution must have a fixed nonzero component at index $0$, making $x_0 = 0$ infeasible.\n\n- Case $4$ (boundary case: empty zone, no extra constraints):\n  Reuse $A_1$ and $b_1$ as above, and set\n  $$\n  S_4 = \\varnothing.\n  $$\n\nFor each case, compute:\n- The unconstrained minimum-length solution $x^\\ast$.\n- The constrained minimum-length solution $\\hat{x}$ obtained by choosing $\\alpha$ through your criterion if feasible; otherwise, set $\\hat{x} = x^\\ast$ and mark infeasibility.\n- The increase in squared Euclidean norm $\\Delta = \\|\\hat{x}\\|_2^2 - \\|x^\\ast\\|_2^2$.\n- A feasibility flag indicating whether the constraints $x_i = 0$ for all $i \\in S$ were satisfied exactly.\n- The maximum absolute value on the constrained indices $M = \\max_{i \\in S} | \\hat{x}_i |$ (define $M = 0$ if $S$ is empty).\n\nYour program must produce a single line of output containing the results for all test cases as a comma-separated list enclosed in square brackets, where each test case contributes a list of the form $[\\Delta, \\text{feasible}, M]$. For example, the format should be like $[[\\Delta_1,\\text{feasible}_1,M_1],[\\Delta_2,\\text{feasible}_2,M_2],[\\Delta_3,\\text{feasible}_3,M_3],[\\Delta_4,\\text{feasible}_4,M_4]]$.", "solution": "The problem requires the formulation and implementation of an algorithm to find a minimum-length solution to an underdetermined linear system $A x = b$ subject to the additional constraint that certain components of the solution vector $x$ must be zero. This is a common task in computational geophysics where prior information about the subsurface model must be incorporated.\n\nThe derivation proceeds in several steps, starting from the fundamental properties of linear systems and building up to the constrained optimization problem.\n\nFirst, we characterize the set of all solutions to the underdetermined system $A x = b$, where $A \\in \\mathbb{R}^{m \\times n}$ with $m < n$. The Moore-Penrose pseudoinverse $A^+$ provides the unique solution with the minimum Euclidean norm, denoted as $x^\\ast = A^+ b$. This solution lies entirely in the row space of $A$, denoted $\\text{range}(A^\\top)$. Any other solution $x$ can be expressed by adding a vector from the nullspace of $A$, $\\ker(A) = \\{z \\in \\mathbb{R}^n : Az = 0\\}$. Thus, the general form of any solution is $x = x^\\ast + x_n$, where $x_n \\in \\ker(A)$.\n\nThe Singular Value Decomposition (SVD) of $A$ provides an orthonormal basis for both the row space and the nullspace. Let the SVD of $A$ be $A = U \\Sigma V^\\top$, where $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{n \\times n}$ are orthogonal matrices and $\\Sigma \\in \\mathbb{R}^{m \\times n}$ is a rectangular diagonal matrix of singular values $\\sigma_i$. If the rank of $A$ is $r$, there are $r$ non-zero singular values. The last $k = n-r$ columns of $V$ form an orthonormal basis for $\\ker(A)$. We can group these basis vectors into a matrix $N \\in \\mathbb{R}^{n \\times k}$. Any vector $x_n \\in \\ker(A)$ can then be written as a linear combination of these basis vectors, $x_n = N \\alpha$, for some coefficient vector $\\alpha \\in \\mathbb{R}^k$. The complete set of solutions to $A x = b$ is thus parameterized by $\\alpha$:\n$$x(\\alpha) = x^\\ast + N \\alpha$$\n\nThe problem introduces an additional constraint: for a given set of indices $S \\subset \\{0, 1, \\dots, n-1\\}$, we require $x_i = 0$ for all $i \\in S$. This can be expressed using a selector matrix $C \\in \\mathbb{R}^{|S| \\times n}$, where each row of $C$ is a standard basis vector $e_i^T$ for some $i \\in S$. The constraint is then compactly written as $C x = 0$.\n\nOur goal is to find the solution $x$ that satisfies both $A x = b$ and $C x = 0$ and has the minimum possible Euclidean norm $\\|x\\|_2$. We substitute the general solution form $x(\\alpha)$ into the new constraint:\n$$C(x^\\ast + N \\alpha) = 0$$\n$$C N \\alpha = -C x^\\ast$$\nThis is a linear system for the unknown coefficients $\\alpha$. Let us denote $G = CN$ and $d = -Cx^\\ast$. The system is $G \\alpha = d$.\n\nSimultaneously, we want to minimize the norm of the solution $\\|x(\\alpha)\\|_2$. Because $x^\\ast \\in \\text{range}(A^\\top)$ and $N\\alpha \\in \\ker(A)$, and these two subspaces are orthogonal complements, the vectors $x^\\ast$ and $N\\alpha$ are orthogonal. By the Pythagorean theorem:\n$$\\|x(\\alpha)\\|_2^2 = \\|x^\\ast + N\\alpha\\|_2^2 = \\|x^\\ast\\|_2^2 + \\|N\\alpha\\|_2^2$$\nSince the columns of $N$ form an orthonormal basis, $\\|N\\alpha\\|_2^2 = \\alpha^\\top N^\\top N \\alpha = \\alpha^\\top I_k \\alpha = \\|\\alpha\\|_2^2$. The optimization problem is then to minimize $\\|x^\\ast\\|_2^2 + \\|\\alpha\\|_2^2$. As $\\|x^\\ast\\|_2^2$ is a constant, this is equivalent to minimizing $\\|\\alpha\\|_2^2$.\n\nThe problem has been reduced to finding the minimum-norm vector $\\alpha$ that satisfies the linear system $G \\alpha = d$. This is a classic problem whose solution is given by the pseudoinverse of $G$:\n$$\\alpha = G^+ d = (CN)^+ (-Cx^\\ast)$$\n\nA solution for $\\alpha$ exists if and only if $d$ is in the range of $G$. This is the feasibility condition for the constraints. We can verify this condition by checking if the computed $\\alpha$ satisfies the system, i.e., whether $G\\alpha$ is numerically close to $d$. If the system is infeasible, no solution satisfying both $Ax=b$ and $Cx=0$ exists. In this case, as per the problem statement, we do not modify the original minimum-length solution, so the constrained solution $\\hat{x}$ is set to $x^\\ast$.\n\nIf the system is feasible, the optimal coefficient vector $\\alpha$ is computed, and the constrained minimum-length solution is:\n$$\\hat{x} = x^\\ast + N \\alpha$$\n\nThe increase in the squared norm is $\\Delta = \\|\\hat{x}\\|_2^2 - \\|x^\\ast\\|_2^2$. Due to the orthogonality, this simplifies to $\\Delta = \\|N\\alpha\\|_2^2 = \\|\\alpha\\|_2^2$.\n\nThe final algorithm is as follows:\n1.  Given $A$, $b$, and $S$, compute the unconstrained minimum-length solution $x^\\ast = A^+ b$.\n2.  Compute the SVD of $A$ to find an orthonormal basis $N$ for its nullspace $\\ker(A)$. The columns of $N$ are the right singular vectors corresponding to singular values that are numerically zero.\n3.  If the constraint set $S$ is empty, no action is needed. The solution is $\\hat{x} = x^\\ast$, and $\\Delta = 0$.\n4.  If $S$ is not empty, form the linear system $G \\alpha = d$ where $G=CN$ and $d=-Cx^\\ast$. In implementation, this is achieved by indexing: $G$ consists of the rows of $N$ indexed by $S$, and $d$ consists of the negated elements of $x^\\ast$ indexed by $S$.\n5.  Solve for the minimum-norm $\\alpha$ using the pseudoinverse: $\\alpha = G^+ d$.\n6.  Check for feasibility by testing if $G \\alpha \\approx d$.\n7.  If feasible, compute the constrained solution $\\hat{x} = x^\\ast + N \\alpha$. Calculate the norm increase $\\Delta = \\|\\alpha\\|_2^2$ and the maximum absolute value $M$ on the constrained indices.\n8.  If infeasible, the solution is $\\hat{x} = x^\\ast$. The norm increase $\\Delta$ is $0$, and $M$ is calculated from the components of $x^\\ast$ at indices in $S$.", "answer": "```python\nimport numpy as np\n\ndef solve_constrained_min_length(A, b, S):\n    \"\"\"\n    Computes the minimum-length solution to Ax=b with constraints x_i=0 for i in S.\n\n    Args:\n        A (np.ndarray): The matrix A.\n        b (np.ndarray): The vector b.\n        S (set): A set of indices to be constrained to zero.\n\n    Returns:\n        tuple: A tuple containing (delta, feasible, M), where delta is the increase\n               in squared norm, feasible is a boolean flag, and M is the max\n               absolute value on constrained indices.\n    \"\"\"\n    m, n = A.shape\n    \n    # 1. Compute the unconstrained minimum-length solution\n    x_ast = np.linalg.pinv(A) @ b\n    \n    # 2. Compute an orthonormal basis for the nullspace of A using SVD\n    try:\n        U, s, Vt = np.linalg.svd(A)\n        # Tolerance for identifying zero singular values\n        tol = s.max() * max(A.shape) * np.finfo(s.dtype).eps\n        rank = np.sum(s > tol)\n        null_space_basis = Vt.T[:, rank:]\n    except np.linalg.LinAlgError:\n        # Handle cases where SVD might fail, though unlikely with test data\n        null_space_basis = np.zeros((n, 0))\n\n    # 3. Handle empty constraint set\n    if not S:\n        hat_x = x_ast\n        delta = 0.0\n        feasible = True\n        M = 0.0\n        return [delta, feasible, M]\n    \n    S_list = sorted(list(S))\n\n    # Handle trivial nullspace\n    if null_space_basis.shape[1] == 0:\n        hat_x = x_ast\n        M = np.max(np.abs(hat_x[S_list]))\n        feasible = np.allclose(M, 0)\n        delta = 0.0\n        return [delta, feasible, M]\n\n    # 4. Form the linear system for alpha\n    G = null_space_basis[S_list, :]\n    d = -x_ast[S_list]\n    \n    # 5. Solve for the minimum-norm alpha\n    alpha = np.linalg.pinv(G) @ d\n    \n    # 6. Check feasibility\n    if np.allclose(G @ alpha, d):\n        feasible = True\n        # 7. Compute the constrained solution\n        hat_x = x_ast + null_space_basis @ alpha\n        # Calculate performance metrics\n        delta = np.sum(alpha**2)\n        M = np.max(np.abs(hat_x[S_list]))\n    else:\n        feasible = False\n        # 8. If infeasible, use the unconstrained solution\n        hat_x = x_ast\n        delta = 0.0\n        M = np.max(np.abs(hat_x[S_list]))\n\n    return [delta, feasible, M]\n\n\ndef solve():\n    \"\"\"\n    Defines test cases, runs the solver, and prints the formatted results.\n    \"\"\"\n    # Case 1\n    A1 = np.array([\n        [1., 0., 1., 2., 0.],\n        [0., 1., -1., 0., 1.],\n        [1., 1., 0., 1., -1.]\n    ])\n    x0_1 = np.array([0., 0., 1., -2., 0.5])\n    b1 = A1 @ x0_1\n    S1 = {0, 1}\n\n    # Case 2\n    A2 = np.array([\n        [1., 2., 0., 1.],\n        [0., 1., 1., -1.]\n    ])\n    x0_2 = np.array([1., -1., 0., 0.])\n    b2 = A2 @ x0_2\n    S2 = {3}\n\n    # Case 3\n    A3 = np.array([\n        [1., 0., 0., 0., 0.],\n        [0., 1., 0., 1., 2.],\n        [0., 0., 1., 1., -1.]\n    ])\n    b3 = np.array([1., 1., 0.])\n    S3 = {0}\n\n    # Case 4\n    A4 = A1\n    b4 = b1\n    S4 = set()\n\n    test_cases = [\n        (A1, b1, S1),\n        (A2, b2, S2),\n        (A3, b3, S3),\n        (A4, b4, S4)\n    ]\n\n    results = []\n    for A, b, S in test_cases:\n        result = solve_constrained_min_length(A, b, S)\n        results.append(result)\n\n    # Format boolean as lowercase string for consistency if needed,\n    # but problem description implies standard python str() is fine.\n    # e.g., '[{r[0]}, {str(r[1]).lower()}, {r[2]}]'\n    # The asked format is `[[...], [...]]` which is naturally produced by `str()`\n    # of a list of lists. The joiner `','.join` correctly handles this.\n    \n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3610331"}, {"introduction": "Geophysical models are often constrained by physical laws that take the form of inequalities, such as requiring densities or concentrations to be non-negative. This practice introduces the powerful Karush-Kuhn-Tucker (KKT) conditions to find the minimum-length solution when it is subject to such lower-bound constraints [@problem_id:3610287]. By applying the KKT framework to a simple but illustrative problem, you will see how an active inequality constraint qualitatively changes the solution and gain insight into the role of Lagrange multipliers in certifying optimality.", "problem": "Consider a linearized, single-station gravity forward model in computational geophysics, where the observed normalized anomaly $b$ is generated by three equal-sized subsurface cells with unknown normalized amplitudes $x_1$, $x_2$, and $x_3$. The forward operator $A$ for this single station is the row vector $A = [1\\ \\ 1\\ \\ 1]$, so that the forward model reads $b = A x$ with $x = (x_1, x_2, x_3)^\\top$. Suppose the measured anomaly is $b = 1$. Among all models that fit the data exactly, the minimum-length solution is defined as the minimizer of the squared Euclidean norm subject to the data fit constraint.\n\nNow impose a physically motivated lower-bound floor $m \\ge 0$ on the model, representing prior knowledge that the first cell’s normalized amplitude must not be smaller than a prescribed baseline, while the other two cells are allowed to be as small as zero. Specifically, take\n$$\nm = \\begin{pmatrix} 0.5 \\\\ 0 \\\\ 0 \\end{pmatrix}.\n$$\nFormulate the bound-constrained minimum-length problem as minimizing the squared Euclidean norm of $x$ subject to the exact data fit $A x = b$ and the elementwise lower bounds $x \\ge m$. Derive the solution by enforcing the first-order optimality conditions of Karush-Kuhn-Tucker (KKT) and show that adding the lower bound $m$ changes the minimum-length solution qualitatively compared to the unconstrained case.\n\nCompute the exact bound-constrained minimum-length solution $x^{\\star}$ and express your final answer as a row vector. No rounding is needed; provide exact fractions if they arise. Use no physical units; treat all quantities as dimensionless.", "solution": "We start from the linear forward model $b = A x$ with $A = [1\\ \\ 1\\ \\ 1]$ and $b = 1$. The minimum-length solution among all $x$ satisfying $A x = b$ is the solution to the optimization problem\n$$\n\\min_{x \\in \\mathbb{R}^{3}} \\ \\frac{1}{2} \\|x\\|_{2}^{2} \\quad \\text{subject to} \\quad A x = b.\n$$\nThis is a convex quadratic subject to an affine constraint. The bound-constrained version enforces the elementwise lower bounds $x \\ge m$ with $m = (0.5, 0, 0)^\\top$:\n$$\n\\min_{x \\in \\mathbb{R}^{3}} \\ \\frac{1}{2} \\|x\\|_{2}^{2} \\quad \\text{subject to} \\quad A x = b, \\quad x \\ge m,\n$$\nwhere inequalities are interpreted componentwise.\n\nTo derive the solution, we invoke the Karush-Kuhn-Tucker (KKT) conditions. First, write the equality constraint as $g(x) = \\mathbf{1}^\\top x - 1 = 0$, where $\\mathbf{1} = (1,1,1)^\\top$, and the inequality constraints as $h_i(x) = m_i - x_i \\le 0$, which encode $x_i \\ge m_i$ for $i=1,2,3$. The Lagrangian for this problem is\n$$\n\\mathcal{L}(x,\\lambda,\\mu) = \\frac{1}{2} \\|x\\|_{2}^{2} + \\lambda \\big(\\mathbf{1}^\\top x - 1\\big) + \\mu^\\top (m - x),\n$$\nwhere $\\lambda \\in \\mathbb{R}$ is the multiplier for the equality constraint and $\\mu \\in \\mathbb{R}^{3}$ with $\\mu \\ge 0$ (componentwise) are the multipliers for the inequality constraints.\n\nThe KKT conditions are:\n1. Stationarity: $\\nabla_{x} \\mathcal{L}(x,\\lambda,\\mu) = 0$, which yields\n$$\nx + \\lambda \\mathbf{1} - \\mu = 0 \\quad \\Longrightarrow \\quad x = -\\lambda \\mathbf{1} + \\mu.\n$$\n2. Primal feasibility: $\\mathbf{1}^\\top x = 1$ and $x \\ge m$.\n3. Dual feasibility: $\\mu \\ge 0$.\n4. Complementary slackness: $\\mu_i \\big(m_i - x_i\\big) = 0$ for each $i=1,2,3$.\n\nWe will determine the active set (indices where $x_i = m_i$) that satisfies these conditions and minimizes the norm. The unconstrained minimum-length solution for the equality-only problem is the orthogonal projection of the origin onto the plane $\\mathbf{1}^\\top x = 1$, which, by symmetry, is $x = \\big(\\frac{1}{3}, \\frac{1}{3}, \\frac{1}{3}\\big)^\\top$. This violates the lower bound on the first component because $\\frac{1}{3} < 0.5$, hence the bound on $x_1$ must be active in the constrained solution. We test the active set $S = \\{1\\}$, with the remaining bounds inactive.\n\nAssume $x_1 = m_1 = 0.5$ is active, so $\\mu_1 \\ge 0$ may be nonzero, and assume $x_2 > m_2 = 0$ and $x_3 > m_3 = 0$, so $\\mu_2 = \\mu_3 = 0$ by complementary slackness. Under these assumptions, stationarity implies componentwise\n$$\nx_1 = -\\lambda + \\mu_1, \\quad x_2 = -\\lambda, \\quad x_3 = -\\lambda.\n$$\nEnforcing $x_1 = 0.5$ gives\n$$\n-\\lambda + \\mu_1 = 0.5 \\quad \\Longrightarrow \\quad \\mu_1 = 0.5 + \\lambda.\n$$\nThe equality constraint $\\mathbf{1}^\\top x = 1$ becomes\n$$\nx_1 + x_2 + x_3 = 1 \\quad \\Longrightarrow \\quad (-\\lambda + \\mu_1) + (-\\lambda) + (-\\lambda) = 1,\n$$\nwhich simplifies to\n$$\n-3\\lambda + \\mu_1 = 1.\n$$\nSubstituting $\\mu_1 = 0.5 + \\lambda$ yields\n$$\n-3\\lambda + (0.5 + \\lambda) = 1 \\quad \\Longrightarrow \\quad -2\\lambda + 0.5 = 1 \\quad \\Longrightarrow \\quad \\lambda = -\\frac{1}{4}.\n$$\nThen\n$$\n\\mu_1 = 0.5 + \\lambda = \\frac{1}{2} - \\frac{1}{4} = \\frac{1}{4}, \\quad \\mu_2 = 0, \\quad \\mu_3 = 0,\n$$\nand\n$$\nx_2 = -\\lambda = \\frac{1}{4}, \\quad x_3 = -\\lambda = \\frac{1}{4}, \\quad x_1 = 0.5.\n$$\nWe verify feasibility: $x_2 = \\frac{1}{4} \\ge 0$ and $x_3 = \\frac{1}{4} \\ge 0$, so the inactive bounds are satisfied; $\\mu_1 = \\frac{1}{4} \\ge 0$ satisfies dual feasibility; complementary slackness holds because $m_1 - x_1 = 0$ implies $\\mu_1 (m_1 - x_1) = 0$, and for $i=2,3$, $m_i - x_i < 0$ implies $\\mu_i = 0$.\n\nNo other active set can minimize the norm while satisfying $x \\ge m$ and the equality. In particular, an inactive bound for $x_1$ is impossible because it would require $x_1 > 0.5$, which would force $x_2 + x_3 < 0.5$ and increase the norm due to asymmetry, while violating stationarity with $\\mu_1 = 0$. The solution found is the unique optimizer due to strict convexity of the objective and linear constraints.\n\nTherefore, the bound-constrained minimum-length solution is\n$$\nx^{\\star} = \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{1}{4} \\\\ \\frac{1}{4} \\end{pmatrix}.\n$$\nThis differs qualitatively from the unconstrained minimum-length solution $\\big(\\frac{1}{3}, \\frac{1}{3}, \\frac{1}{3}\\big)^\\top$: the lower-bound constraint has become active on the first component, pushing it to its floor and redistributing the remaining amplitude equally among the unconstrained components. The Karush-Kuhn-Tucker (KKT) multiplier $\\mu_1 = \\frac{1}{4} > 0$ certifies the active inequality constraint, while $\\mu_2 = \\mu_3 = 0$ certify the inactivity of the others, thereby justifying the qualitative change.", "answer": "$$\\boxed{\\begin{pmatrix}\\frac{1}{2} & \\frac{1}{4} & \\frac{1}{4}\\end{pmatrix}}$$", "id": "3610287"}]}