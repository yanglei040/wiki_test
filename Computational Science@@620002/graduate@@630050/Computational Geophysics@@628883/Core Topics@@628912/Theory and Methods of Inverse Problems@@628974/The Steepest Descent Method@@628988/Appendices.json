{"hands_on_practices": [{"introduction": "The notion of a \"steepest\" direction is fundamentally tied to the inner product used to define angles and lengths in the model space. For problems originating from the discretization of continuous physical laws, such as with the finite element method, the standard Euclidean gradient can introduce a dependency on the mesh resolution, compromising the robustness of the optimization. This exercise [@problem_id:3617265] provides a crucial hands-on experience in defining a physically consistent discrete inner product via mass lumping, ensuring that the steepest descent direction reflects the underlying continuum problem and behaves predictably even on irregular grids.", "problem": "Consider the one-dimensional diffusion model representative of steady groundwater flow in computational geophysics on the domain $[0,1]$ with homogeneous Dirichlet boundary conditions at $x=0$ and $x=1$. Discretize using continuous, piecewise-linear finite elements on an irregular mesh with nodes $x_0=0$, $x_1=\\epsilon$, $x_2=1-\\epsilon$, and $x_3=1$, where $\\epsilon \\in (0,\\tfrac{1}{2})$. The element lengths are $h_1=\\epsilon$, $h_2=1-2\\epsilon$, and $h_3=\\epsilon$. The two interior unknowns are the nodal values at $x_1$ and $x_2$, denoted by the vector $u \\in \\mathbb{R}^2$.\n\nLet the discrete inner product be defined by $\\langle u,v\\rangle_h=\\sum_{i=1}^2 w_i u_i v_i$ with unknown positive weights $w_i$. Your objective is to ensure that $\\langle \\cdot,\\cdot\\rangle_h$ is a consistent approximation of the continuum $L^2$ inner product $\\int_0^1 u v \\, dx$ when $u$ and $v$ are approximated by the finite element basis functions. That is, the discrete inner product must arise from mass lumping of the continuum $L^2$ product.\n\nAssume a constant source term $f(x)=f_0$ with $f_0>0$. The algebraic right-hand side entries are $b_i=\\int_0^1 f_0 \\,\\phi_i(x)\\,dx$, where $\\phi_i$ are the usual nodal hat functions associated with the two interior nodes $x_1$ and $x_2$. Consider the discrete quadratic functional $J(u)=\\tfrac{1}{2}u^\\top K u - b^\\top u$, where $K$ is the symmetric positive-definite stiffness matrix arising from the finite element discretization and $b\\in\\mathbb{R}^2$ is the load vector defined above. The steepest descent direction at a point $u$ with respect to the discrete inner product $\\langle \\cdot,\\cdot\\rangle_h$ is defined as the direction that maximizes the first-order decrease in $J$ among all directions of unit norm measured in $\\langle \\cdot,\\cdot\\rangle_h$.\n\nTasks:\n- Determine the weights $w_1$ and $w_2$ that make $\\langle \\cdot,\\cdot\\rangle_h$ a consistent mass-lumped $L^2$ inner product on this mesh.\n- Starting from the initial guess $u^{(0)}=0$, compute the ratio $\\rho(\\epsilon)=\\frac{|p^{(0)}_1|}{|p^{(0)}_2|}$ of the magnitudes of the two components of the steepest descent direction $p^{(0)}$ at the first iteration, when the steepest descent direction is taken with respect to your discrete inner product $\\langle \\cdot,\\cdot\\rangle_h$.\n\nGive your final answer for $\\rho(\\epsilon)$ as a single real number (no units). No rounding is required.", "solution": "The problem is divided into two parts: first, determining the weights $w_1$ and $w_2$ for the discrete inner product, and second, computing the ratio of the components of the initial steepest descent direction.\n\n**Part 1: Determination of weights $w_1$ and $w_2$**\n\nThe discrete inner product $\\langle u,v\\rangle_h = \\sum_{i=1}^2 w_i u_i v_i$ is specified to be a consistent, mass-lumped approximation of the continuum $L^2$ inner product $\\int_0^1 u(x)v(x)dx$. In the finite element context, a function $u(x)$ is approximated as $u(x) \\approx \\sum_j u_j \\phi_j(x)$. The continuum inner product becomes:\n$$ \\int_0^1 u(x)v(x)dx \\approx \\int_0^1 \\left(\\sum_i u_i \\phi_i(x)\\right) \\left(\\sum_j v_j \\phi_j(x)\\right) dx = \\sum_{i,j} u_i v_j \\int_0^1 \\phi_i(x)\\phi_j(x)dx = u^\\top M v $$\nwhere $M$ is the consistent mass matrix with entries $M_{ij} = \\int_0^1 \\phi_i(x)\\phi_j(x)dx$.\n\nMass lumping approximates the full matrix $M$ with a diagonal matrix $M_L$. The discrete inner product is defined in terms of this lumped matrix, so $u^\\top W v = u^\\top M_L v$, meaning $W=M_L$ and $w_i = (M_L)_{ii}$.\n\nA standard method for mass lumping for continuous, piecewise-linear elements is to define the diagonal entries as the integral of the corresponding basis function:\n$$ w_i = \\int_0^1 \\phi_i(x) dx $$\nGeometrically, this integral represents the area under the \"hat\" basis function $\\phi_i(x)$. A hat function $\\phi_i$ associated with node $x_i$ is non-zero over the interval $(x_{i-1}, x_{i+1})$. The area is the sum of the areas of two triangles, each with a height of $1$:\n$$ w_i = \\frac{1}{2} \\times (x_i - x_{i-1}) \\times 1 + \\frac{1}{2} \\times (x_{i+1} - x_i) \\times 1 = \\frac{1}{2}((x_i - x_{i-1}) + (x_{i+1} - x_i)) $$\nThe mesh nodes are $x_0=0$, $x_1=\\epsilon$, $x_2=1-\\epsilon$, and $x_3=1$. The element lengths are $h_1=x_1-x_0=\\epsilon$, $h_2=x_2-x_1=1-2\\epsilon$, and $h_3=x_3-x_2=\\epsilon$.\n\nFor the weight $w_1$ at the interior node $x_1$:\n$$ w_1 = \\frac{1}{2}(h_1 + h_2) = \\frac{1}{2}(\\epsilon + (1-2\\epsilon)) = \\frac{1-\\epsilon}{2} $$\n\nFor the weight $w_2$ at the interior node $x_2$:\n$$ w_2 = \\frac{1}{2}(h_2 + h_3) = \\frac{1}{2}((1-2\\epsilon) + \\epsilon) = \\frac{1-\\epsilon}{2} $$\nThus, the weights are $w_1 = w_2 = \\frac{1-\\epsilon}{2}$.\n\n**Part 2: Computation of the ratio $\\rho(\\epsilon)$**\n\nThe steepest descent method aims to minimize the functional $J(u) = \\frac{1}{2}u^\\top K u - b^\\top u$. The gradient of $J(u)$ with respect to the standard Euclidean inner product is $\\nabla J(u) = Ku - b$.\n\nThe steepest descent direction $p$ at a point $u$ with respect to the inner product $\\langle \\cdot, \\cdot \\rangle_h$ defined by the diagonal matrix $W = \\text{diag}(w_1, w_2)$ is given by the negative preconditioned gradient:\n$$ p = -W^{-1} \\nabla J(u) $$\nWe start from the initial guess $u^{(0)}=0$. The gradient at this point is:\n$$ g^{(0)} = \\nabla J(u^{(0)}) = K u^{(0)} - b = K(0) - b = -b $$\nThe steepest descent direction at the first iteration, $p^{(0)}$, is therefore:\n$$ p^{(0)} = -W^{-1}g^{(0)} = -W^{-1}(-b) = W^{-1}b $$\nSince $W$ is a diagonal matrix, $W = \\begin{pmatrix} w_1 & 0 \\\\ 0 & w_2 \\end{pmatrix}$, its inverse is $W^{-1} = \\begin{pmatrix} 1/w_1 & 0 \\\\ 0 & 1/w_2 \\end{pmatrix}$.\nThe components of the direction vector $p^{(0)}$ are:\n$$ p^{(0)}_1 = \\frac{b_1}{w_1} \\quad \\text{and} \\quad p^{(0)}_2 = \\frac{b_2}{w_2} $$\nNext, we compute the components of the load vector $b$. The definition is $b_i = \\int_0^1 f(x) \\phi_i(x) dx$. Given that the source term is a constant, $f(x)=f_0$:\n$$ b_i = \\int_0^1 f_0 \\phi_i(x) dx = f_0 \\int_0^1 \\phi_i(x) dx $$\nFrom Part 1, we established that $\\int_0^1 \\phi_i(x) dx = w_i$. Therefore, we have a direct relationship between the load vector components and the weights:\n$$ b_1 = f_0 w_1 \\quad \\text{and} \\quad b_2 = f_0 w_2 $$\nNow we can compute the components of the steepest descent direction $p^{(0)}$:\n$$ p^{(0)}_1 = \\frac{b_1}{w_1} = \\frac{f_0 w_1}{w_1} = f_0 $$\n$$ p^{(0)}_2 = \\frac{b_2}{w_2} = \\frac{f_0 w_2}{w_2} = f_0 $$\nThe problem asks for the ratio $\\rho(\\epsilon) = \\frac{|p^{(0)}_1|}{|p^{(0)}_2|}$.\n$$ \\rho(\\epsilon) = \\frac{|f_0|}{|f_0|} $$\nGiven that $f_0 > 0$, we have $|f_0| = f_0$.\n$$ \\rho(\\epsilon) = \\frac{f_0}{f_0} = 1 $$\nThis result is independent of $\\epsilon$. The symmetry of the mesh ($h_1=h_3$) and the source term ($f(x)$ is constant) leads to symmetric weights ($w_1=w_2$) and symmetric load vector components ($b_1=b_2$), which in turn results in identical components for the initial steepest descent direction.", "answer": "$$\n\\boxed{1}\n$$", "id": "3617265"}, {"introduction": "Conventional least-squares inversion implicitly assumes that our forward model is an exact representation of reality, which is an idealization we rarely meet in practice. This practice [@problem_id:3617243] explores a more sophisticated approach through robust optimization, where the goal is to find a model that performs well even under the worst-case scenario of bounded modeling errors. You will derive the form of this robust objective function and its corresponding gradient, seeing firsthand how the core principles of steepest descent can be extended to handle uncertainty in a principled and powerful manner.", "problem": "In seismic parameter estimation, suppose one seeks to estimate a model vector $\\mathbf{m} \\in \\mathbb{R}^{n}$ from data $\\mathbf{d} \\in \\mathbb{R}^{p}$, where the predicted data are given by a differentiable forward map $F(\\mathbf{m})$. Consider a robust formulation in which the predicted data suffer an additive modeling error $\\delta F$ that is unknown but bounded in Euclidean norm by a radius $\\eta > 0$. The robust misfit is defined as\n$$\nJ(\\mathbf{m}) \\equiv \\max_{\\|\\delta F\\|_{2} \\leq \\eta} \\frac{1}{2}\\,\\|F(\\mathbf{m}) + \\delta F - \\mathbf{d}\\|_{2}^{2}.\n$$\nStarting from the definitions of the Euclidean norm and the Cauchyâ€“Schwarz inequality, and using a first-order linearization for $F(\\mathbf{m})$ about a current iterate $\\mathbf{m}_{k}$, address the following:\n\n1. For a fixed residual $\\mathbf{r}(\\mathbf{m}) \\equiv F(\\mathbf{m}) - \\mathbf{d}$, evaluate the inner maximization over $\\delta F$ exactly and express $J(\\mathbf{m})$ in closed form in terms of $\\|\\mathbf{r}(\\mathbf{m})\\|_{2}$ and $\\eta$. Also identify a maximizer $\\delta F^{\\star}$ that attains the maximum.\n\n2. Let $J_{F}(\\mathbf{m}_{k})$ denote the Jacobian of $F$ at $\\mathbf{m}_{k}$ and consider the first-order approximation $F(\\mathbf{m}) \\approx F(\\mathbf{m}_{k}) + J_{F}(\\mathbf{m}_{k})(\\mathbf{m} - \\mathbf{m}_{k})$. Using the result from part 1 and the chain rule, derive an approximate gradient $\\nabla J(\\mathbf{m}_{k})$ in terms of $\\mathbf{r}_{k} \\equiv F(\\mathbf{m}_{k}) - \\mathbf{d}$ and $J_{F}(\\mathbf{m}_{k})$.\n\n3. Specialize to the linear identity forward map $F(\\mathbf{m}) = \\mathbf{m}$ in $\\mathbb{R}^{2}$, with current model $\\mathbf{m}_{0} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$, data $\\mathbf{d} = \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix}$, and modeling-error radius $\\eta = 1$. Using the gradient from part 2, form the steepest descent direction $-\\nabla J(\\mathbf{m}_{0})$ and perform an exact line search along this direction to minimize the robust objective $J(\\mathbf{m}_{0} - \\alpha \\nabla J(\\mathbf{m}_{0}))$ over $\\alpha \\geq 0$. Report the exact optimal step length $\\alpha^{\\star}$ as a single number. Do not round.", "solution": "The problem is divided into three parts. We will address them sequentially.\n\n### Part 1: Evaluation of the Inner Maximization\n\nThe robust misfit function is defined as\n$$\nJ(\\mathbf{m}) \\equiv \\max_{\\|\\delta F\\|_{2} \\leq \\eta} \\frac{1}{2}\\,\\|F(\\mathbf{m}) + \\delta F - \\mathbf{d}\\|_{2}^{2}\n$$\nwhere $\\eta > 0$. For a fixed model vector $\\mathbf{m}$, we define the residual vector as $\\mathbf{r}(\\mathbf{m}) \\equiv F(\\mathbf{m}) - \\mathbf{d}$. The expression to be maximized with respect to $\\delta F$ is $\\frac{1}{2}\\|\\mathbf{r}(\\mathbf{m}) + \\delta F\\|_{2}^{2}$. Maximizing this is equivalent to maximizing its argument, the squared Euclidean norm $\\|\\mathbf{r}(\\mathbf{m}) + \\delta F\\|_{2}^{2}$.\n\nLet's expand the squared norm:\n$$\n\\|\\mathbf{r}(\\mathbf{m}) + \\delta F\\|_{2}^{2} = (\\mathbf{r}(\\mathbf{m}) + \\delta F)^{T}(\\mathbf{r}(\\mathbf{m}) + \\delta F) = \\mathbf{r}(\\mathbf{m})^{T}\\mathbf{r}(\\mathbf{m}) + 2\\mathbf{r}(\\mathbf{m})^{T}\\delta F + \\delta F^{T}\\delta F\n$$\nThis can be written as:\n$$\n\\|\\mathbf{r}(\\mathbf{m}) + \\delta F\\|_{2}^{2} = \\|\\mathbf{r}(\\mathbf{m})\\|_{2}^{2} + 2\\mathbf{r}(\\mathbf{m})^{T}\\delta F + \\|\\delta F\\|_{2}^{2}\n$$\nBy the Cauchy-Schwarz inequality, the term $\\mathbf{r}(\\mathbf{m})^{T}\\delta F$ is maximized when $\\delta F$ is collinear with and in the same direction as $\\mathbf{r}(\\mathbf{m})$. To maximize the entire expression, we want to maximize both $2\\mathbf{r}(\\mathbf{m})^{T}\\delta F$ and $\\|\\delta F\\|_{2}^{2}$. Both terms are maximized for a vector $\\delta F$ that is aligned with $\\mathbf{r}(\\mathbf{m})$ and has the largest possible magnitude allowed by the constraint $\\|\\delta F\\|_{2} \\leq \\eta$. This occurs on the boundary, where $\\|\\delta F\\|_{2} = \\eta$.\n\nThe maximizer $\\delta F^{\\star}$ must therefore be a vector of length $\\eta$ that is parallel to $\\mathbf{r}(\\mathbf{m})$. Assuming $\\mathbf{r}(\\mathbf{m}) \\neq \\mathbf{0}$, this vector is unique:\n$$\n\\delta F^{\\star} = \\eta \\frac{\\mathbf{r}(\\mathbf{m})}{\\|\\mathbf{r}(\\mathbf{m})\\|_{2}}\n$$\nSubstituting $\\delta F^{\\star}$ back into the expression for the squared norm:\n$$\n\\max_{\\|\\delta F\\|_{2} \\leq \\eta} \\|\\mathbf{r}(\\mathbf{m}) + \\delta F\\|_{2}^{2} = \\left\\|\\mathbf{r}(\\mathbf{m}) + \\eta \\frac{\\mathbf{r}(\\mathbf{m})}{\\|\\mathbf{r}(\\mathbf{m})\\|_{2}}\\right\\|_{2}^{2} = \\left\\|\\left(1 + \\frac{\\eta}{\\|\\mathbf{r}(\\mathbf{m})\\|_{2}}\\right)\\mathbf{r}(\\mathbf{m})\\right\\|_{2}^{2}\n$$\n$$\n= \\left(1 + \\frac{\\eta}{\\|\\mathbf{r}(\\mathbf{m})\\|_{2}}\\right)^{2} \\|\\mathbf{r}(\\mathbf{m})\\|_{2}^{2} = \\left(\\frac{\\|\\mathbf{r}(\\mathbf{m})\\|_{2} + \\eta}{\\|\\mathbf{r}(\\mathbf{m})\\|_{2}}\\right)^{2} \\|\\mathbf{r}(\\mathbf{m})\\|_{2}^{2} = (\\|\\mathbf{r}(\\mathbf{m})\\|_{2} + \\eta)^{2}\n$$\nThis formula also holds for the case $\\mathbf{r}(\\mathbf{m}) = \\mathbf{0}$. Therefore, the closed-form expression for the robust misfit is:\n$$\nJ(\\mathbf{m}) = \\frac{1}{2}(\\|\\mathbf{r}(\\mathbf{m})\\|_{2} + \\eta)^{2} = \\frac{1}{2}(\\|F(\\mathbf{m}) - \\mathbf{d}\\|_{2} + \\eta)^{2}\n$$\n\n### Part 2: Derivation of the Approximate Gradient\n\nWe seek the gradient of $J(\\mathbf{m})$ with respect to $\\mathbf{m}$, evaluated at a point $\\mathbf{m}_{k}$. We use the chain rule. Let $u(\\mathbf{m}) = \\|F(\\mathbf{m}) - \\mathbf{d}\\|_{2} = \\|\\mathbf{r}(\\mathbf{m})\\|_{2}$. Then $J(\\mathbf{m}) = \\frac{1}{2}(u(\\mathbf{m}) + \\eta)^{2}$.\n\nThe gradient is $\\nabla J(\\mathbf{m}) = \\frac{dJ}{du} \\nabla u(\\mathbf{m})$.\nThe first term is:\n$$\n\\frac{dJ}{du} = \\frac{1}{2} \\cdot 2(u(\\mathbf{m}) + \\eta) = u(\\mathbf{m}) + \\eta = \\|\\mathbf{r}(\\mathbf{m})\\|_{2} + \\eta\n$$\nThe second term is the gradient of the Euclidean norm of the residual, $\\nabla u(\\mathbf{m}) = \\nabla \\|\\mathbf{r}(\\mathbf{m})\\|_{2}$. Assuming $\\mathbf{r}(\\mathbf{m}) \\neq \\mathbf{0}$:\n$$\n\\nabla \\|\\mathbf{r}(\\mathbf{m})\\|_{2} = \\nabla (\\mathbf{r}(\\mathbf{m})^{T}\\mathbf{r}(\\mathbf{m}))^{1/2} = \\frac{1}{2}(\\mathbf{r}(\\mathbf{m})^{T}\\mathbf{r}(\\mathbf{m}))^{-1/2} \\nabla(\\mathbf{r}(\\mathbf{m})^{T}\\mathbf{r}(\\mathbf{m}))\n$$\nThe gradient of the squared norm is $\\nabla(\\mathbf{r}^{T}\\mathbf{r}) = 2 J_{\\mathbf{r}}(\\mathbf{m})^{T}\\mathbf{r}(\\mathbf{m})$, where $J_{\\mathbf{r}}(\\mathbf{m})$ is the Jacobian of $\\mathbf{r}(\\mathbf{m})$ with respect to $\\mathbf{m}$. Since $\\mathbf{r}(\\mathbf{m}) = F(\\mathbf{m}) - \\mathbf{d}$, its Jacobian is $J_{\\mathbf{r}}(\\mathbf{m}) = J_{F}(\\mathbf{m})$.\nThus,\n$$\n\\nabla \\|\\mathbf{r}(\\mathbf{m})\\|_{2} = \\frac{1}{2\\|\\mathbf{r}(\\mathbf{m})\\|_{2}} (2 J_{F}(\\mathbf{m})^{T}\\mathbf{r}(\\mathbf{m})) = \\frac{J_{F}(\\mathbf{m})^{T}\\mathbf{r}(\\mathbf{m})}{\\|\\mathbf{r}(\\mathbf{m})\\|_{2}}\n$$\nCombining the terms, the gradient of $J(\\mathbf{m})$ is:\n$$\n\\nabla J(\\mathbf{m}) = (\\|\\mathbf{r}(\\mathbf{m})\\|_{2} + \\eta) \\frac{J_{F}(\\mathbf{m})^{T}\\mathbf{r}(\\mathbf{m})}{\\|\\mathbf{r}(\\mathbf{m})\\|_{2}} = \\left(1 + \\frac{\\eta}{\\|\\mathbf{r}(\\mathbf{m})\\|_{2}}\\right) J_{F}(\\mathbf{m})^{T}\\mathbf{r}(\\mathbf{m})\n$$\nEvaluating this at the iterate $\\mathbf{m}_{k}$, with $\\mathbf{r}_{k} \\equiv F(\\mathbf{m}_{k}) - \\mathbf{d}$, we obtain the required gradient:\n$$\n\\nabla J(\\mathbf{m}_{k}) = \\left(1 + \\frac{\\eta}{\\|\\mathbf{r}_{k}\\|_{2}}\\right) J_{F}(\\mathbf{m}_{k})^{T}\\mathbf{r}_{k}\n$$\n\n### Part 3: Specific Case and Exact Line Search\n\nWe are given the following:\n- Forward map: $F(\\mathbf{m}) = \\mathbf{m}$. This is a linear map in $\\mathbb{R}^{2}$.\n- Initial model: $\\mathbf{m}_{0} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$.\n- Data: $\\mathbf{d} = \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix}$.\n- Modeling-error radius: $\\eta = 1$.\n\nFirst, we compute the gradient at $\\mathbf{m}_{0}$.\nThe Jacobian of $F(\\mathbf{m}) = \\mathbf{m}$ is the identity matrix $I$. So, $J_{F}(\\mathbf{m}_{0}) = I$.\nThe residual at $\\mathbf{m}_{0}$ is:\n$$\n\\mathbf{r}_{0} = F(\\mathbf{m}_{0}) - \\mathbf{d} = \\mathbf{m}_{0} - \\mathbf{d} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix}\n$$\nThe Euclidean norm of the residual is:\n$$\n\\|\\mathbf{r}_{0}\\|_{2} = \\sqrt{2^{2} + 0^{2}} = 2\n$$\nUsing the formula from Part 2, the gradient $\\nabla J(\\mathbf{m}_{0})$ is:\n$$\n\\nabla J(\\mathbf{m}_{0}) = \\left(1 + \\frac{\\eta}{\\|\\mathbf{r}_{0}\\|_{2}}\\right) J_{F}(\\mathbf{m}_{0})^{T}\\mathbf{r}_{0} = \\left(1 + \\frac{1}{2}\\right) I^{T} \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix} = \\frac{3}{2} \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ 0 \\end{pmatrix}\n$$\nThe steepest descent direction is $\\mathbf{p} = -\\nabla J(\\mathbf{m}_{0}) = \\begin{pmatrix} -3 \\\\ 0 \\end{pmatrix}$.\n\nNext, we perform an exact line search to find the optimal step length $\\alpha^{\\star} \\geq 0$ that minimizes $J(\\mathbf{m}_{0} + \\alpha \\mathbf{p})$. Let's define the model as a function of $\\alpha$:\n$$\n\\mathbf{m}(\\alpha) = \\mathbf{m}_{0} + \\alpha \\mathbf{p} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} + \\alpha \\begin{pmatrix} -3 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 - 3\\alpha \\\\ 1 \\end{pmatrix}\n$$\nThe objective function to minimize is $J(\\alpha) = J(\\mathbf{m}(\\alpha))$. We compute the residual as a function of $\\alpha$:\n$$\n\\mathbf{r}(\\alpha) = F(\\mathbf{m}(\\alpha)) - \\mathbf{d} = \\mathbf{m}(\\alpha) - \\mathbf{d} = \\begin{pmatrix} 1 - 3\\alpha \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 2 - 3\\alpha \\\\ 0 \\end{pmatrix}\n$$\nThe norm of this residual is:\n$$\n\\|\\mathbf{r}(\\alpha)\\|_{2} = \\sqrt{(2 - 3\\alpha)^{2} + 0^{2}} = |2 - 3\\alpha|\n$$\nNow we write the objective function in terms of $\\alpha$:\n$$\nJ(\\alpha) = \\frac{1}{2}(\\|\\mathbf{r}(\\alpha)\\|_{2} + \\eta)^{2} = \\frac{1}{2}(|2 - 3\\alpha| + 1)^{2}\n$$\nWe need to find $\\alpha^{\\star} = \\arg\\min_{\\alpha \\geq 0} J(\\alpha)$. Minimizing $J(\\alpha)$ is equivalent to minimizing $|2 - 3\\alpha| + 1$. This, in turn, is equivalent to minimizing $|2 - 3\\alpha|$.\nThe function $|2 - 3\\alpha|$ is a V-shaped function whose minimum value is $0$. This minimum is achieved when the argument of the absolute value is zero:\n$$\n2 - 3\\alpha = 0 \\implies 3\\alpha = 2 \\implies \\alpha = \\frac{2}{3}\n$$\nSince this value of $\\alpha$ is non-negative ($\\frac{2}{3} \\geq 0$), it is the optimal step length.\nThe exact optimal step length is $\\alpha^{\\star} = \\frac{2}{3}$.", "answer": "$$\\boxed{\\frac{2}{3}}$$", "id": "3617243"}, {"introduction": "Geophysical inversion often involves integrating data from different physical surveys, such as seismic and gravity, in a process known as joint inversion. A key challenge is that the corresponding misfit functionals may have vastly different scales and units, making it difficult to combine them. This exercise [@problem_id:3617285] tackles this practical issue by demonstrating how to construct a single aggregate objective function through principled normalization and weighting, allowing the steepest descent method to effectively balance competing data constraints.", "problem": "Consider a two-parameter linearized joint inversion in computational geophysics for combining travel-time and gravity-anomaly residuals. Let the model vector be $m \\in \\mathbb{R}^{2}$ and consider two least-squares (LS) component misfit functionals derived from independent Gaussian error models for each data type,\n$$\n\\phi_{1}(m) \\equiv \\tfrac{1}{2}\\|A m - r_{1}\\|_{2}^{2}, \n\\qquad\n\\phi_{2}(m) \\equiv \\tfrac{1}{2}\\|B m - r_{2}\\|_{2}^{2},\n$$\nwith $A = I_{2}$ and $B = I_{2}$, where $I_{2}$ denotes the $2 \\times 2$ identity matrix. The linearized residuals are\n$$\nr_{1} = \\begin{pmatrix} 3 \\\\ -4 \\end{pmatrix}, \n\\qquad\nr_{2} = \\begin{pmatrix} 4 \\\\ 3 \\end{pmatrix},\n$$\nand the initial model is $m^{(0)} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$.\n\nTo handle the different scales of the two objectives in a principled way, define normalization factors based on the initial gradient magnitudes,\n$$\ns_{k} \\equiv \\|\\nabla \\phi_{k}(m^{(0)})\\|_{2}, \\quad k \\in \\{1,2\\},\n$$\nand the weighted, normalized aggregate objective\n$$\n\\Phi(m) \\equiv \\sum_{k=1}^{2} w_{k} \\,\\frac{\\phi_{k}(m)}{s_{k}},\n$$\nwith weights $w_{1} = 2$ and $w_{2} = 1$.\n\nUsing the steepest descent method with exact line search applied to $\\Phi$, form the first steepest descent direction $p^{(0)} \\equiv -\\nabla \\Phi(m^{(0)})$ and determine the optimal step length $\\alpha^{\\star}$ that minimizes $\\Phi(m^{(0)} + \\alpha p^{(0)})$ along this direction. Provide the value of $\\alpha^{\\star}$ as a single real number. No rounding is required, and no units are needed.", "solution": "The problem asks for the optimal step length $\\alpha^{\\star}$ for the first iteration of the steepest descent method applied to a weighted aggregate objective function $\\Phi(m)$. The process involves several steps.\n\n**1. Compute Initial Gradients and Normalization Factors**\n\nFirst, we find the gradients of the component misfit functionals, $\\phi_1(m)$ and $\\phi_2(m)$.\nGiven $\\phi_1(m) = \\frac{1}{2}\\|A m - r_1\\|_2^2$ with $A=I_2$, we have $\\phi_1(m) = \\frac{1}{2}\\|m - r_1\\|_2^2$. The gradient is:\n$$ \\nabla \\phi_1(m) = m - r_1 $$\nSimilarly, for $\\phi_2(m) = \\frac{1}{2}\\|B m - r_2\\|_2^2$ with $B=I_2$, we have $\\phi_2(m) = \\frac{1}{2}\\|m - r_2\\|_2^2$. The gradient is:\n$$ \\nabla \\phi_2(m) = m - r_2 $$\nNext, we evaluate these gradients at the initial model $m^{(0)} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$:\n$$ \\nabla \\phi_1(m^{(0)}) = m^{(0)} - r_1 = -\\begin{pmatrix} 3 \\\\ -4 \\end{pmatrix} = \\begin{pmatrix} -3 \\\\ 4 \\end{pmatrix} $$\n$$ \\nabla \\phi_2(m^{(0)}) = m^{(0)} - r_2 = -\\begin{pmatrix} 4 \\\\ 3 \\end{pmatrix} = \\begin{pmatrix} -4 \\\\ -3 \\end{pmatrix} $$\nNow, we compute the normalization factors $s_k = \\|\\nabla \\phi_k(m^{(0)})\\|_2$:\n$$ s_1 = \\left\\|\\begin{pmatrix} -3 \\\\ 4 \\end{pmatrix}\\right\\|_2 = \\sqrt{(-3)^2 + 4^2} = \\sqrt{9 + 16} = \\sqrt{25} = 5 $$\n$$ s_2 = \\left\\|\\begin{pmatrix} -4 \\\\ -3 \\end{pmatrix}\\right\\|_2 = \\sqrt{(-4)^2 + (-3)^2} = \\sqrt{16 + 9} = \\sqrt{25} = 5 $$\n\n**2. Construct the Aggregate Objective and its Gradient**\n\nThe aggregate objective function $\\Phi(m)$ is defined as:\n$$ \\Phi(m) = w_1 \\frac{\\phi_1(m)}{s_1} + w_2 \\frac{\\phi_2(m)}{s_2} $$\nWith $w_1 = 2$, $w_2 = 1$, and $s_1 = s_2 = 5$, this becomes:\n$$ \\Phi(m) = \\frac{2}{5}\\phi_1(m) + \\frac{1}{5}\\phi_2(m) $$\nThe gradient of the aggregate function is a linear combination of the component gradients:\n$$ \\nabla \\Phi(m) = \\frac{2}{5}\\nabla \\phi_1(m) + \\frac{1}{5}\\nabla \\phi_2(m) $$\nWe evaluate this gradient at the initial model $m^{(0)}$:\n$$ \\nabla \\Phi(m^{(0)}) = \\frac{2}{5}\\nabla \\phi_1(m^{(0)}) + \\frac{1}{5}\\nabla \\phi_2(m^{(0)}) = \\frac{2}{5}\\begin{pmatrix} -3 \\\\ 4 \\end{pmatrix} + \\frac{1}{5}\\begin{pmatrix} -4 \\\\ -3 \\end{pmatrix} $$\n$$ \\nabla \\Phi(m^{(0)}) = \\frac{1}{5} \\left( \\begin{pmatrix} -6 \\\\ 8 \\end{pmatrix} + \\begin{pmatrix} -4 \\\\ -3 \\end{pmatrix} \\right) = \\frac{1}{5}\\begin{pmatrix} -10 \\\\ 5 \\end{pmatrix} = \\begin{pmatrix} -2 \\\\ 1 \\end{pmatrix} $$\n\n**3. Determine the Steepest Descent Direction and Optimal Step Length**\n\nThe first steepest descent direction $p^{(0)}$ is the negative of the initial gradient:\n$$ p^{(0)} = -\\nabla \\Phi(m^{(0)}) = -\\begin{pmatrix} -2 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix} $$\nFor an exact line search, we need to find the step length $\\alpha^{\\star}$ that minimizes $\\Phi(m^{(0)} + \\alpha p^{(0)})$. Since $\\Phi(m)$ is a quadratic function of $m$, the optimal step length for the steepest descent method is given by the formula:\n$$ \\alpha^{\\star} = \\frac{(p^{(0)})^T (-\\nabla \\Phi(m^{(0)}))}{(p^{(0)})^T H p^{(0)}} = \\frac{(p^{(0)})^T p^{(0)}}{(p^{(0)})^T H p^{(0)}} = \\frac{\\|p^{(0)}\\|_2^2}{(p^{(0)})^T H p^{(0)}} $$\nwhere $H$ is the Hessian of $\\Phi(m)$.\n\nFirst, let's compute the numerator:\n$$ \\|p^{(0)}\\|_2^2 = 2^2 + (-1)^2 = 4 + 1 = 5 $$\nNext, we find the Hessian $H$. The Hessians of the component functions are $\\nabla^2 \\phi_1(m) = I_2$ and $\\nabla^2 \\phi_2(m) = I_2$. Therefore, the Hessian of the aggregate function is constant:\n$$ H = \\nabla^2 \\Phi(m) = \\frac{2}{5}\\nabla^2 \\phi_1(m) + \\frac{1}{5}\\nabla^2 \\phi_2(m) = \\frac{2}{5}I_2 + \\frac{1}{5}I_2 = \\frac{3}{5}I_2 $$\nNow we compute the denominator:\n$$ (p^{(0)})^T H p^{(0)} = \\begin{pmatrix} 2 & -1 \\end{pmatrix} \\left( \\frac{3}{5} \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} \\right) \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix} = \\frac{3}{5} \\begin{pmatrix} 2 & -1 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix} = \\frac{3}{5} \\|p^{(0)}\\|_2^2 $$\n$$ (p^{(0)})^T H p^{(0)} = \\frac{3}{5} \\times 5 = 3 $$\nFinally, we compute the optimal step length $\\alpha^{\\star}$:\n$$ \\alpha^{\\star} = \\frac{5}{3} $$", "answer": "$$\n\\boxed{\\frac{5}{3}}\n$$", "id": "3617285"}]}