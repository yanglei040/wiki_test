{"hands_on_practices": [{"introduction": "Incomplete LU (ILU) factorizations are workhorses in preconditioning, but its effectiveness can be surprisingly fragile. This exercise illuminates a common failure mode—pivot breakdown—where the factorization process encounters a zero or near-zero pivot, halting the algorithm. By working through the stabilization of a small but problematic matrix, you will gain a first-principles understanding of how diagonal shifting can restore the viability of an ILU preconditioner, a crucial skill for robustly applying these methods in practice. [@problem_id:3613319]", "problem": "In modeling steady single-phase groundwater flow in a heterogeneous, weakly advective medium, a cell-centered finite-volume discretization on a short one-dimensional subgrid produces a nonsymmetric, weakly diagonally dominant linear system for the pressure correction. Consider the resulting sparse coefficient matrix $A \\in \\mathbb{R}^{4 \\times 4}$ in the natural node ordering, with entries\n$$\nA \\;=\\;\n\\begin{pmatrix}\n1 & -0.5 & 0 & 0\\\\\n-1 & 0.5 & -0.3 & 0\\\\\n0 & -0.2 & 1 & -0.4\\\\\n0 & 0 & -0.6 & 0.9\n\\end{pmatrix}.\n$$\nSuch matrices are routinely preconditioned using Incomplete Lower-Upper (ILU) factorization to accelerate Krylov subspace solvers such as the Generalized Minimal Residual (GMRES) method. You will examine the stability of the zero-fill incomplete factorization and design a stabilizing modification.\n\nTasks:\n- Starting from the definition of Gaussian elimination and restricting all updates to the original sparsity pattern of $A$ (i.e., zero fill), construct the Incomplete Lower-Upper factorization with zero fill, denoted $\\text{ILU}(0)$, in natural ordering for $A$. Identify any pivot breakdown risk and explain at what step and why it occurs.\n- A common stabilization is to apply a diagonal shift, forming $A_{\\alpha} = A + \\alpha I$ with $\\alpha \\ge 0$, and then computing $\\text{ILU}(0)$ of $A_{\\alpha}$. Let the pivot magnitude safeguard threshold be $\\tau = 0.1$. Derive, from first principles of elimination restricted to the sparsity pattern, explicit expressions for the intermediate pivots $u_{11}(\\alpha)$, $u_{22}(\\alpha)$, $u_{33}(\\alpha)$, and $u_{44}(\\alpha)$ that arise in the Doolittle form of $\\text{ILU}(0)$ applied to $A_{\\alpha}$.\n- Using these expressions, determine the minimal nonnegative diagonal shift $\\alpha^{\\star}$ such that the $\\text{ILU}(0)$ of $A_{\\alpha^{\\star}}$ exists in natural ordering without zero pivots and satisfies $|u_{kk}(\\alpha^{\\star})| \\ge \\tau$ for $k \\in \\{1,2,3,4\\}$. Provide the exact value of $\\alpha^{\\star}$ in closed form.\n- Briefly justify, based on graph and sparsity considerations, one reordering or modified $\\text{ILU}$ strategy that would further stabilize the preconditioner for matrices of this type, and explain why it is expected to reduce pivot breakdown risk.\n\nAnswer specification: Your final answer must be the single exact closed-form value of $\\alpha^{\\star}$, with no units. Do not provide an inequality or an equation. Do not round; give the exact algebraic form.", "solution": "The problem requires an analysis of the Incomplete Lower-Upper factorization with zero fill, denoted $\\text{ILU}(0)$, for a given $4 \\times 4$ matrix $A$. The analysis involves identifying pivot breakdown, deriving a stabilization strategy using a diagonal shift $\\alpha$, determining the minimal shift $\\alpha^{\\star}$ to satisfy a pivot threshold, and suggesting an alternative stabilization method.\n\nThe given matrix is:\n$$\nA \\;=\\;\n\\begin{pmatrix}\n1 & -0.5 & 0 & 0\\\\\n-1 & 0.5 & -0.3 & 0\\\\\n0 & -0.2 & 1 & -0.4\\\\\n0 & 0 & -0.6 & 0.9\n\\end{pmatrix}\n$$\n\n**Part 1: ILU(0) Factorization of $A$ and Pivot Breakdown**\n\nThe $\\text{ILU}(0)$ factorization finds a unit lower triangular matrix $L$ and an upper triangular matrix $U$ such that the product $M = LU$ matches $A$ on its original sparsity pattern. All other entries of $M$, known as fill-in, are discarded. The factors $L$ and $U$ are constrained to have the same sparsity pattern as the lower and upper triangular parts of $A$, respectively.\n\nLet the factors be:\n$$\nL = \\begin{pmatrix} 1 & 0 & 0 & 0 \\\\ l_{21} & 1 & 0 & 0 \\\\ 0 & l_{32} & 1 & 0 \\\\ 0 & 0 & l_{43} & 1 \\end{pmatrix}, \\quad\nU = \\begin{pmatrix} u_{11} & u_{12} & 0 & 0 \\\\ 0 & u_{22} & u_{23} & 0 \\\\ 0 & 0 & u_{33} & u_{34} \\\\ 0 & 0 & 0 & u_{44} \\end{pmatrix}\n$$\nThe product $LU$ is:\n$$\nLU = \\begin{pmatrix}\nu_{11} & u_{12} & 0 & 0 \\\\\nl_{21}u_{11} & l_{21}u_{12} + u_{22} & u_{23} & 0 \\\\\n0 & l_{32}u_{22} & l_{32}u_{23} + u_{33} & u_{34} \\\\\n0 & 0 & l_{43}u_{33} & l_{43}u_{34} + u_{44}\n\\end{pmatrix}\n$$\nWe equate the entries of $LU$ to the corresponding non-zero entries of $A$ sequentially.\n\nStep 1 (Row 1):\n$u_{11} = A_{11} = 1$\n$u_{12} = A_{12} = -0.5$\n\nStep 2 (Row 2):\n$l_{21}u_{11} = A_{21} \\implies l_{21}(1) = -1 \\implies l_{21} = -1$\n$l_{21}u_{12} + u_{22} = A_{22} \\implies (-1)(-0.5) + u_{22} = 0.5 \\implies 0.5 + u_{22} = 0.5 \\implies u_{22} = 0$\n\nAt this point, a pivot breakdown occurs. The second pivot, $u_{22}$, is zero. The $\\text{ILU}(0)$ algorithm cannot proceed because the next step would require division by $u_{22}$. Specifically, to compute $l_{32}$, we would need to solve $l_{32}u_{22} = A_{32}$, which is $l_{32}(0) = -0.2$, an impossible equation. Thus, the $\\text{ILU}(0)$ factorization of $A$ in natural ordering does not exist.\n\n**Part 2: Pivots of the Shifted Matrix $A_{\\alpha}$**\n\nWe consider the shifted matrix $A_{\\alpha} = A + \\alpha I$ with $\\alpha \\ge 0$:\n$$\nA_{\\alpha} \\;=\\;\n\\begin{pmatrix}\n1+\\alpha & -0.5 & 0 & 0\\\\\n-1 & 0.5+\\alpha & -0.3 & 0\\\\\n0 & -0.2 & 1+\\alpha & -0.4\\\\\n0 & 0 & -0.6 & 0.9+\\alpha\n\\end{pmatrix}\n$$\nWe perform the $\\text{ILU}(0)$ factorization of $A_{\\alpha}$. The general recurrence for the pivots $u_{kk}$ in $\\text{ILU}(0)$ for a tridiagonal matrix is:\n$u_{11} = (A_{\\alpha})_{11}$\n$u_{kk} = (A_{\\alpha})_{kk} - \\frac{(A_{\\alpha})_{k,k-1} (A_{\\alpha})_{k-1,k}}{u_{k-1,k-1}}$ for $k > 1$.\n\nFor $k=1$:\n$u_{11}(\\alpha) = (A_{\\alpha})_{11} = 1+\\alpha$\n\nFor $k=2$:\n$u_{22}(\\alpha) = (A_{\\alpha})_{22} - \\frac{(A_{\\alpha})_{21} (A_{\\alpha})_{12}}{u_{11}(\\alpha)} = (0.5+\\alpha) - \\frac{(-1)(-0.5)}{1+\\alpha} = 0.5+\\alpha - \\frac{0.5}{1+\\alpha}$\n$u_{22}(\\alpha) = \\frac{(0.5+\\alpha)(1+\\alpha) - 0.5}{1+\\alpha} = \\frac{0.5 + 1.5\\alpha + \\alpha^2 - 0.5}{1+\\alpha} = \\frac{\\alpha^2 + 1.5\\alpha}{1+\\alpha}$\n\nFor $k=3$:\n$u_{33}(\\alpha) = (A_{\\alpha})_{33} - \\frac{(A_{\\alpha})_{32} (A_{\\alpha})_{23}}{u_{22}(\\alpha)} = (1+\\alpha) - \\frac{(-0.2)(-0.3)}{u_{22}(\\alpha)} = 1+\\alpha - \\frac{0.06}{u_{22}(\\alpha)}$\nSubstituting the expression for $u_{22}(\\alpha)$:\n$u_{33}(\\alpha) = 1+\\alpha - \\frac{0.06(1+\\alpha)}{\\alpha^2 + 1.5\\alpha} = (1+\\alpha) \\left( 1 - \\frac{0.06}{\\alpha(\\alpha+1.5)} \\right) = (1+\\alpha) \\frac{\\alpha^2+1.5\\alpha-0.06}{\\alpha(\\alpha+1.5)}$\n\nFor $k=4$:\n$u_{44}(\\alpha) = (A_{\\alpha})_{44} - \\frac{(A_{\\alpha})_{43} (A_{\\alpha})_{34}}{u_{33}(\\alpha)} = (0.9+\\alpha) - \\frac{(-0.6)(-0.4)}{u_{33}(\\alpha)} = 0.9+\\alpha - \\frac{0.24}{u_{33}(\\alpha)}$\n\n**Part 3: Minimal Diagonal Shift $\\alpha^{\\star}$**\n\nWe need to find the minimal $\\alpha \\ge 0$ such that $|u_{kk}(\\alpha)| \\ge \\tau = 0.1$ for all $k \\in \\{1, 2, 3, 4\\}$.\n\nCondition for $u_{11}(\\alpha)$:\n$|u_{11}(\\alpha)| = |1+\\alpha| \\ge 0.1$. Since $\\alpha \\ge 0$, $1+\\alpha \\ge 1$, so this condition is always satisfied.\n\nCondition for $u_{22}(\\alpha)$:\n$|u_{22}(\\alpha)| = |\\frac{\\alpha^2 + 1.5\\alpha}{1+\\alpha}| \\ge 0.1$. For $\\alpha \\ge 0$, $u_{22}(\\alpha) \\ge 0$, so we solve $\\frac{\\alpha(\\alpha+1.5)}{1+\\alpha} \\ge 0.1$.\nMultiplying by $10(1+\\alpha)$ (which is positive for $\\alpha \\ge 0$):\n$10\\alpha(\\alpha+1.5) \\ge 1+\\alpha$\n$10\\alpha^2 + 15\\alpha \\ge 1+\\alpha$\n$10\\alpha^2 + 14\\alpha - 1 \\ge 0$\nThe roots of the quadratic $10\\alpha^2 + 14\\alpha - 1 = 0$ are $\\alpha = \\frac{-14 \\pm \\sqrt{14^2 - 4(10)(-1)}}{20} = \\frac{-14 \\pm \\sqrt{196+40}}{20} = \\frac{-14 \\pm \\sqrt{236}}{20} = \\frac{-7 \\pm \\sqrt{59}}{10}$.\nSince we require $\\alpha \\ge 0$, we take the positive root. The quadratic opens upwards, so the inequality holds for $\\alpha \\ge \\frac{-7+\\sqrt{59}}{10}$. Let this value be $\\alpha_{c2}$.\n\nCondition for $u_{33}(\\alpha)$:\nThe function $u_{22}(\\alpha)$ is strictly increasing for $\\alpha > 0$. Hence, $u_{33}(\\alpha) = 1+\\alpha - \\frac{0.06}{u_{22}(\\alpha)}$ is also strictly increasing for $\\alpha>0$.\nLet's evaluate $u_{33}$ at the critical value $\\alpha_{c2}$. At this point, $u_{22}(\\alpha_{c2})=0.1$.\n$u_{33}(\\alpha_{c2}) = (1+\\alpha_{c2}) - \\frac{0.06}{0.1} = 1+\\alpha_{c2} - 0.6 = 0.4+\\alpha_{c2}$.\n$u_{33}(\\alpha_{c2}) = 0.4 + \\frac{-7+\\sqrt{59}}{10} = \\frac{4-7+\\sqrt{59}}{10} = \\frac{-3+\\sqrt{59}}{10}$.\nSince $5 = \\sqrt{25} < \\sqrt{59} < \\sqrt{64} = 8$, this value is positive.\n$|\\frac{-3+\\sqrt{59}}{10}| \\ge 0.1 \\implies -3+\\sqrt{59} \\ge 1 \\implies \\sqrt{59} \\ge 4 \\implies 59 \\ge 16$. This is true.\nSince $u_{33}(\\alpha)$ is increasing for $\\alpha > 0$, for any $\\alpha \\ge \\alpha_{c2}$, we have $u_{33}(\\alpha) \\ge u_{33}(\\alpha_{c2}) > 0.1$. Thus, the condition for $u_{33}$ is satisfied for all $\\alpha \\ge \\alpha_{c2}$.\n\nCondition for $u_{44}(\\alpha)$:\nThe function $u_{33}(\\alpha)$ is positive and increasing for $\\alpha \\ge \\alpha_{c2}$. The function $u_{44}(\\alpha) = 0.9+\\alpha - \\frac{0.24}{u_{33}(\\alpha)}$ is therefore also strictly increasing for $\\alpha \\ge \\alpha_{c2}$.\nLet's evaluate $u_{44}$ at $\\alpha_{c2}$:\n$u_{44}(\\alpha_{c2}) = 0.9+\\alpha_{c2} - \\frac{0.24}{u_{33}(\\alpha_{c2})} = 0.9+\\alpha_{c2} - \\frac{0.24}{0.4+\\alpha_{c2}}$.\nSubstituting $\\alpha_{c2} = \\frac{-7+\\sqrt{59}}{10}$:\n$0.9+\\alpha_{c2} = \\frac{9}{10} + \\frac{-7+\\sqrt{59}}{10} = \\frac{2+\\sqrt{59}}{10}$.\n$0.4+\\alpha_{c2} = \\frac{4}{10} + \\frac{-7+\\sqrt{59}}{10} = \\frac{-3+\\sqrt{59}}{10}$.\n$u_{44}(\\alpha_{c2}) = \\frac{2+\\sqrt{59}}{10} - \\frac{0.24}{(-3+\\sqrt{59})/10} = \\frac{2+\\sqrt{59}}{10} - \\frac{2.4}{-3+\\sqrt{59}}$.\n$u_{44}(\\alpha_{c2}) = \\frac{2+\\sqrt{59}}{10} - \\frac{2.4(-3-\\sqrt{59})}{9-59} = \\frac{2+\\sqrt{59}}{10} + \\frac{2.4(3+\\sqrt{59})}{40} = \\frac{2+\\sqrt{59}}{10} + \\frac{0.06(3+\\sqrt{59})}{1}$.\n$u_{44}(\\alpha_{c2}) = 0.2 + 0.1\\sqrt{59} + 0.18 + 0.06\\sqrt{59} = 0.38 + 0.16\\sqrt{59}$.\nThis value is clearly positive and much larger than $0.1$. Since $u_{44}(\\alpha)$ is increasing for $\\alpha \\ge \\alpha_{c2}$, the condition for $u_{44}$ is also satisfied for all $\\alpha \\ge \\alpha_{c2}$.\n\nThe set of $\\alpha \\ge 0$ values satisfying all four conditions is $[\\alpha_{c2}, \\infty)$. The minimal such value is $\\alpha_{c2}$.\nThus, the minimal non-negative diagonal shift is $\\alpha^{\\star} = \\frac{-7+\\sqrt{59}}{10}$.\n\n**Part 4: Alternative Stabilization Strategy**\n\nAn alternative strategy to stabilize the ILU factorization is to apply a **symmetric reordering** to the matrix before factorization. The goal of this reordering is to place entries with large magnitudes onto the diagonal. This can be formalized by finding a permutation matrix $P$ and applying the factorization to $P^TAP$. For instance, a maximum weight bipartite matching algorithm on the graph of the matrix can find a permutation that maximizes the product of the diagonal magnitudes.\n\nThis strategy directly addresses the root cause of pivot breakdown: small diagonal entries relative to off-diagonal entries. By permuting the matrix to make it more diagonally dominant, the initial pivots $u_{kk}$ (which are related to $A_{kk}$) are larger, and subsequent pivots are less likely to become dangerously small or zero during the elimination process. This is in contrast to the diagonal shift, which modifies the matrix values rather than reordering them. For matrices arising from physical problems, such orderings often have a physical interpretation and can be very effective at improving the robustness of ILU preconditioners.", "answer": "$$\\boxed{\\frac{-7+\\sqrt{59}}{10}}$$", "id": "3613319"}, {"introduction": "Geophysical simulations involving coupled physics, such as fluid flow or seismoelectric effects, often produce large, sparse linear systems with a distinct saddle-point block structure. Treating such matrices as generic sparse systems can be inefficient. This practice demonstrates how to exploit this known structure by constructing a block-diagonal preconditioner and, more importantly, how to use scaling to balance the physical and numerical scales of the different fields, a key step in designing effective preconditioners for multiphysics problems. [@problem_id:3613315]", "problem": "In computational geophysics, mixed finite element formulations of coupled fields (for example, velocity and pressure in acoustics or fluid flow) lead to saddle-point linear systems with block structure. Consider the linear system operator\n$$\nA \\;=\\;\n\\begin{pmatrix}\nK & B^{\\top} \\\\\nB & -M\n\\end{pmatrix},\n$$\nwhere $K \\in \\mathbb{R}^{n_u \\times n_u}$ and $M \\in \\mathbb{R}^{n_p \\times n_p}$ are symmetric positive definite (SPD) matrices representing the stiffness and mass contributions, respectively, and $B \\in \\mathbb{R}^{n_p \\times n_u}$ is the discrete coupling operator. A block-diagonal preconditioner is defined by\n$$\nP \\;=\\;\n\\begin{pmatrix}\nK & 0 \\\\\n0 & M\n\\end{pmatrix}.\n$$\nTo improve balance between the blocks for iterative solvers, a common strategy is to apply a congruence diagonal scaling with block-diagonal $S = \\mathrm{diag}(D_u, D_p)$, yielding the normalized operator\n$$\nA_{\\mathrm{n}} \\;=\\; S \\, A \\, S \\;=\\;\n\\begin{pmatrix}\nD_u K D_u & D_u B^{\\top} D_p \\\\\nD_p B D_u & -D_p M D_p\n\\end{pmatrix}.\n$$\nYou are given a $2 \\times 2$ case with diagonal blocks\n$$\nK \\;=\\; \\begin{pmatrix}\n9.0 \\times 10^{8} & 0 \\\\\n0 & 2.25 \\times 10^{9}\n\\end{pmatrix}, \\qquad\nM \\;=\\; \\begin{pmatrix}\n1.0 \\times 10^{5} & 0 \\\\\n0 & 4.0 \\times 10^{5}\n\\end{pmatrix},\n$$\nand diagonal coupling\n$$\nB \\;=\\; \\begin{pmatrix}\n3.0 \\times 10^{6} & 0 \\\\\n0 & 6.0 \\times 10^{6}\n\\end{pmatrix}.\n$$\nStarting from the definitions of symmetric positive definiteness and the spectral radius, and using only first principles of block scaling and eigenvalue analysis, perform the following:\n\n1. Derive the diagonal scaling matrices $D_u$ and $D_p$ that normalize the diagonal blocks of $A_{\\mathrm{n}}$ to the identity when $A$ is scaled by $S$ on both sides (congruence scaling).\n2. Write the resulting normalized operator $A_{\\mathrm{n}}$ explicitly in terms of $K$, $M$, $B$, $D_u$, and $D_p$, and identify the decoupled $2 \\times 2$ subproblems.\n3. Using the block-diagonal preconditioner formed by the normalized diagonal blocks (which are identity), compute the spectral radius $\\rho$ of the preconditioned operator $A_{\\mathrm{n}}$.\n\nExpress your final answer for the spectral radius $\\rho$ as a single real number, rounded to five significant figures. No units are required.", "solution": "The problem requires a multi-step analysis of a saddle-point linear system arising in computational geophysics. The analysis involves deriving scaling matrices, constructing the scaled operator, and computing its spectral radius. The process is broken down into three main parts as requested.\n\n### Part 1: Derivation of Scaling Matrices $D_u$ and $D_p$\n\nThe goal of the congruence scaling $A_{\\mathrm{n}} = S A S$ with $S = \\mathrm{diag}(D_u, D_p)$ is to normalize the diagonal blocks of the resulting operator $A_{\\mathrm{n}}$. The normalized operator is given by\n$$\nA_{\\mathrm{n}} \\;=\\;\n\\begin{pmatrix}\nD_u K D_u & D_u B^{\\top} D_p \\\\\nD_p B D_u & -D_p M D_p\n\\end{pmatrix}.\n$$\nThe normalization condition requires that the resulting diagonal blocks, $D_u K D_u$ and $D_p M D_p$, become identity matrices.\n$$\nD_u K D_u = I, \\qquad D_p M D_p = I.\n$$\nThe matrices $K$ and $M$ are given as symmetric positive definite (SPD). This ensures that their square roots, $K^{1/2}$ and $M^{1/2}$, are real, unique, and also SPD. We are looking for diagonal scaling matrices $D_u$ and $D_p$.\n\nFor the first condition, $D_u K D_u = I$, we can rearrange it, pre- and post-multiplying by $D_u^{-1}$: $K = D_u^{-1} I D_u^{-1} = (D_u^{-1})^2$. Taking the square root gives $K^{1/2} = D_u^{-1}$, which implies $D_u = (K^{1/2})^{-1} = K^{-1/2}$.\nSimilarly, for the second condition, $D_p M D_p = I$, we find that $D_p = M^{-1/2}$.\n\nThe given matrices $K$ and $M$ are diagonal:\n$$\nK \\;=\\; \\begin{pmatrix}\n9.0 \\times 10^{8} & 0 \\\\\n0 & 2.25 \\times 10^{9}\n\\end{pmatrix}, \\qquad\nM \\;=\\; \\begin{pmatrix}\n1.0 \\times 10^{5} & 0 \\\\\n0 & 4.0 \\times 10^{5}\n\\end{pmatrix}.\n$$\nSince they are diagonal, their square roots and inverse square roots are also diagonal, obtained by taking the corresponding scalar operation on each diagonal entry.\nThe square root of $K$ is:\n$$\nK^{1/2} \\;=\\; \\begin{pmatrix}\n\\sqrt{9.0 \\times 10^{8}} & 0 \\\\\n0 & \\sqrt{2.25 \\times 10^{9}}\n\\end{pmatrix} \\;=\\; \\begin{pmatrix}\n3.0 \\times 10^{4} & 0 \\\\\n0 & 1.5 \\times 10^{4.5}\n\\end{pmatrix}.\n$$\nThe square root of $M$ is:\n$$\nM^{1/2} \\;=\\; \\begin{pmatrix}\n\\sqrt{1.0 \\times 10^{5}} & 0 \\\\\n0 & \\sqrt{4.0 \\times 10^{5}}\n\\end{pmatrix} \\;=\\; \\begin{pmatrix}\n1.0 \\times 10^{2.5} & 0 \\\\\n0 & 2.0 \\times 10^{2.5}\n\\end{pmatrix}.\n$$\nThe scaling matrices $D_u$ and $D_p$ are the inverses of these:\n$$\nD_u = K^{-1/2} = \\begin{pmatrix} (3.0 \\times 10^{4})^{-1} & 0 \\\\ 0 & (1.5 \\times 10^{4.5})^{-1} \\end{pmatrix},\n$$\n$$\nD_p = M^{-1/2} = \\begin{pmatrix} (1.0 \\times 10^{2.5})^{-1} & 0 \\\\ 0 & (2.0 \\times 10^{2.5})^{-1} \\end{pmatrix}.\n$$\n\n### Part 2: The Normalized Operator $A_n$ and Decoupled Subproblems\n\nWith the scaling matrices derived, the normalized operator $A_{\\mathrm{n}}$ takes the form:\n$$\nA_{\\mathrm{n}} \\;=\\;\n\\begin{pmatrix}\nI & D_u B^{\\top} D_p \\\\\nD_p B D_u & -I\n\\end{pmatrix},\n$$\nwhere $I$ is the $2 \\times 2$ identity matrix. The off-diagonal blocks need to be computed. The coupling matrix $B$ is given as\n$$\nB \\;=\\; \\begin{pmatrix}\n3.0 \\times 10^{6} & 0 \\\\\n0 & 6.0 \\times 10^{6}\n\\end{pmatrix}.\n$$\nSince $B$ is diagonal, it is symmetric, i.e., $B^{\\top} = B$.\nLet us compute the block $\\mathcal{C} = D_p B D_u$. Since $D_p$, $B$, and $D_u$ are all diagonal, their product is a diagonal matrix, and the order of multiplication does not matter. The entries of $\\mathcal{C}$ are the products of the corresponding diagonal entries.\n$$\n\\mathcal{C}_{11} = (D_p)_{11} (B)_{11} (D_u)_{11} = (1.0 \\times 10^{2.5})^{-1} \\times (3.0 \\times 10^{6}) \\times (3.0 \\times 10^{4})^{-1} = \\frac{3.0 \\times 10^{6}}{(1.0 \\times 10^{2.5}) \\times (3.0 \\times 10^{4})} = \\frac{10^6}{10^{6.5}} = 10^{-0.5} = \\frac{1}{\\sqrt{10}}.\n$$\n$$\n\\mathcal{C}_{22} = (D_p)_{22} (B)_{22} (D_u)_{22} = (2.0 \\times 10^{2.5})^{-1} \\times (6.0 \\times 10^{6}) \\times (1.5 \\times 10^{4.5})^{-1} = \\frac{6.0 \\times 10^{6}}{(2.0 \\times 10^{2.5}) \\times (1.5 \\times 10^{4.5})} = \\frac{6.0 \\times 10^{6}}{3.0 \\times 10^{7}} = \\frac{2.0}{10} = 0.2.\n$$\nThus, the off-diagonal block is $\\mathcal{C} = \\begin{pmatrix} 1/\\sqrt{10} & 0 \\\\ 0 & 0.2 \\end{pmatrix}$.\nThe other off-diagonal block is $D_u B^{\\top} D_p = D_u B D_p$. Since all matrices are diagonal, $D_u B D_p = D_p B D_u = \\mathcal{C}$.\nSo, the normalized operator $A_{\\mathrm{n}}$ is\n$$\nA_{\\mathrm{n}} \\;=\\;\n\\begin{pmatrix}\nI & \\mathcal{C} \\\\\n\\mathcal{C} & -I\n\\end{pmatrix}\n\\;=\\;\n\\left( \\begin{array}{cc|cc}\n1 & 0 & 1/\\sqrt{10} & 0 \\\\\n0 & 1 & 0 & 0.2 \\\\\n\\hline\n1/\\sqrt{10} & 0 & -1 & 0 \\\\\n0 & 0.2 & 0 & -1\n\\end{array} \\right).\n$$\nBecause the block matrices $I$ and $\\mathcal{C}$ are diagonal, the $4 \\times 4$ system matrix $A_n$ decouples into two independent $2 \\times 2$ subproblems. The first subproblem connects the first components of the velocity and pressure fields (index $1$ in $u$ and $p$), and the second subproblem connects the second components (index $2$).\nBy permuting the basis to $(u_1, p_1, u_2, p_2)$, the matrix becomes block-diagonal:\n$$\nP^{\\top} A_{\\mathrm{n}} P \\;=\\;\n\\left( \\begin{array}{cc|cc}\n1 & 1/\\sqrt{10} & 0 & 0 \\\\\n1/\\sqrt{10} & -1 & 0 & 0 \\\\\n\\hline\n0 & 0 & 1 & 0.2 \\\\\n0 & 0 & 0.2 & -1\n\\end{array} \\right).\n$$\nThe two decoupled $2 \\times 2$ subproblem matrices are:\n$$\nA_1 = \\begin{pmatrix} 1 & 1/\\sqrt{10} \\\\ 1/\\sqrt{10} & -1 \\end{pmatrix}, \\qquad A_2 = \\begin{pmatrix} 1 & 0.2 \\\\ 0.2 & -1 \\end{pmatrix}.\n$$\n\n### Part 3: Spectral Radius of the Preconditioned Operator\n\nThe problem asks for the spectral radius of the \"preconditioned operator $A_n$\". The preconditioner for the scaled system $A_n$ is formed by analogy to the original preconditioner $P$, using the scaled diagonal blocks. The original preconditioner was $P = \\mathrm{diag}(K, M)$. The scaled version is $P_n = \\mathrm{diag}(D_u K D_u, D_p M D_p)$. By our construction, $D_u K D_u = I$ and $D_p M D_p = I$. Thus, the preconditioner is $P_n = \\mathrm{diag}(I, I) = I_{4 \\times 4}$, the identity matrix.\nThe preconditioned operator is $P_n^{-1} A_n = I^{-1} A_n = A_n$. We must therefore compute the spectral radius $\\rho(A_n)$.\n\nThe spectrum of $A_n$ is the union of the spectra of the decoupled subproblem matrices $A_1$ and $A_2$. Let's find the eigenvalues for a generic subproblem matrix of the form $M_c = \\begin{pmatrix} 1 & c \\\\ c & -1 \\end{pmatrix}$. The characteristic equation is $\\det(M_c - \\lambda I) = 0$.\n$$\n\\det\\begin{pmatrix} 1-\\lambda & c \\\\ c & -1-\\lambda \\end{pmatrix} \\;=\\; (1-\\lambda)(-1-\\lambda) - c^2 \\;=\\; -(1-\\lambda^2) - c^2 \\;=\\; \\lambda^2 - 1 - c^2 \\;=\\; 0.\n$$\nThis gives $\\lambda^2 = 1 + c^2$, so the eigenvalues are $\\lambda = \\pm\\sqrt{1+c^2}$.\n\nFor subproblem $A_1$, we have $c=c_1=1/\\sqrt{10}$. The eigenvalues are:\n$$\n\\lambda_{1, \\pm} = \\pm\\sqrt{1 + (1/\\sqrt{10})^2} = \\pm\\sqrt{1 + \\frac{1}{10}} = \\pm\\sqrt{1.1}.\n$$\nFor subproblem $A_2$, we have $c=c_2=0.2$. The eigenvalues are:\n$$\n\\lambda_{2, \\pm} = \\pm\\sqrt{1 + (0.2)^2} = \\pm\\sqrt{1 + 0.04} = \\pm\\sqrt{1.04}.\n$$\nThe set of all eigenvalues of $A_n$ is $\\{\\sqrt{1.1}, -\\sqrt{1.1}, \\sqrt{1.04}, -\\sqrt{1.04}\\}$.\nThe spectral radius $\\rho(A_n)$ is defined as the maximum of the absolute values of the eigenvalues:\n$$\n\\rho(A_n) = \\max \\left( |\\sqrt{1.1}|, |-\\sqrt{1.1}|, |\\sqrt{1.04}|, |-\\sqrt{1.04}| \\right) = \\max(\\sqrt{1.1}, \\sqrt{1.04}).\n$$\nSince $1.1 > 1.04$, it follows that $\\sqrt{1.1} > \\sqrt{1.04}$. Therefore, the spectral radius is:\n$$\n\\rho = \\sqrt{1.1}.\n$$\nNumerically, $\\sqrt{1.1} \\approx 1.048808848...$\nRounding this to five significant figures gives $1.0488$.", "answer": "$$\\boxed{1.0488}$$", "id": "3613315"}, {"introduction": "The Helmholtz equation, which governs time-harmonic wave phenomena, is notoriously challenging for iterative solvers due to its indefinite nature, leading to poor convergence. A powerful strategy is to employ a complex-shifted Laplacian preconditioner, which introduces artificial damping to avoid numerical resonances that stall the solver. This practice combines analytical derivation with computation to explore how this shift beneficially alters the spectrum of the preconditioned operator, providing a concrete example of how physics-informed preconditioning can tame a difficult class of problems. [@problem_id:3613279]", "problem": "Consider the frequency-domain Helmholtz operator $A = -\\Delta - k^2$ discretized on a one-dimensional unit interval with homogeneous Dirichlet boundary conditions using the standard second-order centered finite difference scheme on a uniform grid of $n$ interior points. You are to derive the complex-shifted Laplacian preconditioner $M = -\\Delta + \\alpha k^2 + i \\beta k^2$, explain the damping effect introduced by the complex shift, and quantify its impact on the spectral properties of the preconditioned operator.\n\nYour tasks are as follows:\n\n1. Starting from the definition of the second-order centered finite difference approximation of the Laplacian operator, derive the discrete one-dimensional operator $L$ that approximates $-\\Delta$ on a grid with spacing $h = 1/(n+1)$, and recall the well-tested eigen-decomposition of $L$ under homogeneous Dirichlet boundary conditions. Show that its eigenvalues are given by\n$$\n\\lambda_j = \\frac{4}{h^2} \\sin^2\\left(\\frac{j \\pi}{2 (n+1)}\\right), \\quad j = 1,2,\\ldots,n,\n$$\nand that the corresponding eigenvectors form an orthogonal basis.\n\n2. Using the operators $A = L - k^2 I$ and $M = L + (\\alpha + i \\beta) k^2 I$ with $I$ the identity, prove that $A$ and $M$ commute and are simultaneously diagonalizable by the eigenvectors of $L$. Derive the eigenvalues of the right-preconditioned operator $M^{-1} A$ as\n$$\n\\rho_j = \\frac{\\lambda_j - k^2}{\\lambda_j + (\\alpha + i \\beta) k^2}, \\quad j = 1,2,\\ldots,n.\n$$\n\n3. Explain, based on the structure of the eigenvalues $\\rho_j$, why adding a strictly positive imaginary shift (that is, choosing $\\beta > 0$) introduces damping that improves the spectral properties relevant for Krylov subspace methods such as the Generalized Minimal Residual method (GMRES). In particular, explain why the imaginary part in the denominator $\\lambda_j + (\\alpha + i \\beta) k^2$ prevents near-resonant small denominators when $\\lambda_j \\approx k^2$, and why choosing $\\alpha \\approx -1$ makes $M$ a consistent approximation of $A$ augmented with damping.\n\n4. To quantify spectral clustering relevant to iterative solver performance, define the cluster radius around $1$ as\n$$\nr(n,k,\\alpha,\\beta) = \\max_{1 \\le j \\le n} \\left|1 - \\rho_j\\right|.\n$$\nA smaller value of $r(n,k,\\alpha,\\beta)$ indicates that the eigenvalues $\\rho_j$ are more tightly clustered around $1$, which generally benefits the convergence of Krylov subspace methods for normal or near-normal operators.\n\nYour program must compute $r(n,k,\\alpha,\\beta)$ using the formula for $\\rho_j$ for the following test suite of parameter values, which is designed to cover distinct regimes:\n\n- Test case $1$ (baseline complex shift approximating the Helmholtz operator): $n = 200$, $k = 3.2$, $\\alpha = -1.0$, $\\beta = 0.5$.\n- Test case $2$ (no real shift, only damping): $n = 200$, $k = 3.2$, $\\alpha = 0.0$, $\\beta = 0.5$.\n- Test case $3$ (higher wavenumber with weak damping): $n = 200$, $k = 20.0$, $\\alpha = -1.0$, $\\beta = 0.05$.\n- Test case $4$ (near-resonant case with extremely small damping): $n = 200$, $k = 3.14159$, $\\alpha = -1.0$, $\\beta = 10^{-6}$.\n\nImplement a program that computes $r(n,k,\\alpha,\\beta)$ for each test case using the closed-form eigenvalues $\\lambda_j$ and the expression for $\\rho_j$. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[result_1,result_2,result_3,result_4]$). The outputs must be real numbers (floats). No physical units are required for this problem, and there are no angles. The result for each test case must be a float.", "solution": "The problem is evaluated as valid. It is scientifically grounded in the field of numerical analysis for partial differential equations, well-posed with a clear objective, and all necessary information for derivation and computation is provided.\n\nThe solution proceeds by following the four specified tasks.\n\n### Task 1: Discrete Laplacian and its Eigendecomposition\n\nWe consider a function $u(x)$ defined on the unit interval $[0, 1]$ with homogeneous Dirichlet boundary conditions, $u(0) = 0$ and $u(1) = 0$. The interval is discretized by a uniform grid of $n$ interior points $x_j = j h$ for $j=1, \\ldots, n$, where the grid spacing is $h = 1/(n+1)$. The boundary points are $x_0 = 0$ and $x_{n+1} = 1$. Let $u_j$ be the approximation of $u(x_j)$.\n\nThe one-dimensional Laplacian operator is $\\Delta = \\frac{d^2}{dx^2}$. The standard second-order centered finite difference approximation for the second derivative at a point $x_j$ is:\n$$\n\\frac{d^2 u}{dx^2}\\bigg|_{x_j} \\approx \\frac{u(x_{j+1}) - 2u(x_j) + u(x_{j-1})}{h^2} = \\frac{u_{j+1} - 2u_j + u_{j-1}}{h^2}.\n$$\nThe operator $-\\Delta$ is therefore approximated by $-\\frac{1}{h^2}(u_{j+1} - 2u_j + u_{j-1})$. Applying this to each interior grid point $j=1, \\ldots, n$ and incorporating the boundary conditions $u_0 = 0$ and $u_{n+1} = 0$, we obtain a system of linear equations. This system can be written in matrix form as $L \\mathbf{u} = \\mathbf{f}$, where $\\mathbf{u} = [u_1, u_2, \\ldots, u_n]^T$ and $L$ is the discrete Laplacian operator. The matrix $L$ is an $n \\times n$ symmetric tridiagonal matrix given by:\n$$\nL = \\frac{1}{h^2}\n\\begin{pmatrix}\n2 & -1 & & & \\\\\n-1 & 2 & -1 & & \\\\\n& \\ddots & \\ddots & \\ddots & \\\\\n& & -1 & 2 & -1 \\\\\n& & & -1 & 2\n\\end{pmatrix}.\n$$\nThis matrix, a scaled version of the tridiagonal Toeplitz matrix with $(2, -1, -1)$ on its diagonals, has a well-known eigendecomposition. Its eigenvalues $\\lambda_j$ and corresponding eigenvectors $v_j$ for $j=1, \\ldots, n$ are:\n$$\n\\lambda_j = \\frac{2}{h^2} \\left(1 - \\cos\\left(\\frac{j\\pi}{n+1}\\right)\\right).\n$$\nUsing the half-angle trigonometric identity $1 - \\cos(\\theta) = 2\\sin^2(\\theta/2)$, we can rewrite the eigenvalues as:\n$$\n\\lambda_j = \\frac{4}{h^2} \\sin^2\\left(\\frac{j\\pi}{2(n+1)}\\right), \\quad j = 1, 2, \\ldots, n.\n$$\nThis matches the formula provided in the problem statement. The corresponding eigenvectors have components $(v_j)_k$ given by the discrete sine transform:\n$$\n(v_j)_k = \\sqrt{\\frac{2}{n+1}} \\sin\\left(\\frac{jk\\pi}{n+1}\\right), \\quad k = 1, 2, \\ldots, n.\n$$\nThese eigenvectors $\\{v_j\\}_{j=1}^n$ form an orthonormal basis for $\\mathbb{R}^n$.\n\n### Task 2: Commutativity and Eigenvalues of the Preconditioned Operator\n\nThe discrete Helmholtz operator is $A = L - k^2 I$, where $I$ is the $n \\times n$ identity matrix. The complex-shifted Laplacian preconditioner is defined as $M = L + (\\alpha + i\\beta)k^2 I$. Let $c = \\alpha + i\\beta$ for notational convenience, so $M = L + ck^2 I$.\n\nTo prove that $A$ and $M$ commute, we compute their product in both orders:\n$$\nAM = (L - k^2 I)(L + ck^2 I) = L^2 + ck^2 L - k^2 L - ck^4 I = L^2 + (c-1)k^2 L - ck^4 I.\n$$\n$$\nMA = (L + ck^2 I)(L - k^2 I) = L^2 - k^2 L + ck^2 L - ck^4 I = L^2 + (c-1)k^2 L - ck^4 I.\n$$\nSince $AM = MA$, the operators commute.\n\nA fundamental theorem in linear algebra states that two commuting diagonalizable matrices are simultaneously diagonalizable. The matrix $L$ is real and symmetric, hence it is diagonalizable with the orthonormal basis of eigenvectors $\\{v_j\\}$. The matrices $A$ and $M$ are both polynomials of $L$ ($A = p_1(L)$ and $M = p_2(L)$ with $p_1(x) = x-k^2$ and $p_2(x) = x+ck^2$). Therefore, they share the same eigenvectors $v_j$ as $L$.\n\nLet's find the eigenvalues of $A$ and $M$. Let $v_j$ be an eigenvector of $L$ with eigenvalue $\\lambda_j$.\nThe eigenvalue of $A$ corresponding to $v_j$, let's call it $\\mu_j$, is:\n$$\nA v_j = (L - k^2 I)v_j = L v_j - k^2 I v_j = \\lambda_j v_j - k^2 v_j = (\\lambda_j - k^2)v_j.\n$$\nSo, $\\mu_j = \\lambda_j - k^2$.\n\nThe eigenvalue of $M$ corresponding to $v_j$, let's call it $\\nu_j$, is:\n$$\nM v_j = (L + ck^2 I)v_j = L v_j + ck^2 I v_j = \\lambda_j v_j + ck^2 v_j = (\\lambda_j + ck^2)v_j.\n$$\nSo, $\\nu_j = \\lambda_j + (\\alpha + i\\beta)k^2$.\n\nThe right-preconditioned operator is $M^{-1}A$. Its eigenvalues, denoted by $\\rho_j$, can be found by applying the operator to the common eigenvector $v_j$:\n$$\nM^{-1} A v_j = M^{-1} (\\mu_j v_j) = \\mu_j (M^{-1} v_j).\n$$\nSince $M v_j = \\nu_j v_j$, it follows that $M^{-1} v_j = \\frac{1}{\\nu_j}v_j$. Substituting this back, we get:\n$$\nM^{-1} A v_j = \\mu_j \\left(\\frac{1}{\\nu_j}v_j\\right) = \\frac{\\mu_j}{\\nu_j} v_j.\n$$\nThus, the eigenvalues of the preconditioned operator $M^{-1}A$ are:\n$$\n\\rho_j = \\frac{\\mu_j}{\\nu_j} = \\frac{\\lambda_j - k^2}{\\lambda_j + (\\alpha + i\\beta)k^2}, \\quad j = 1, 2, \\ldots, n.\n$$\n\n### Task 3: Damping Effect of the Complex Shift\n\nIterative solvers like GMRES are used to solve the linear system $A\\mathbf{u} = \\mathbf{f}$. The convergence of such methods depends strongly on the spectral properties of the system matrix $A$. The Helmholtz operator $A = L - k^2 I$ is indefinite: its eigenvalues $\\mu_j = \\lambda_j - k^2$ can be positive, negative, or zero. When the wavenumber $k$ is such that $k^2$ is close to an eigenvalue $\\lambda_j$ of the Laplacian $L$, the corresponding eigenvalue $\\mu_j$ of $A$ is close to zero. This near-singularity, or resonance, severely degrades the performance of iterative solvers.\n\nPreconditioning aims to transform the system into an equivalent one, $M^{-1}A\\mathbf{u} = M^{-1}\\mathbf{f}$, where the preconditioned matrix $M^{-1}A$ has more favorable spectral properties. Ideally, its eigenvalues $\\{\\rho_j\\}$ should be clustered around $1$ and away from the origin.\n\nLet's examine the eigenvalues of the preconditioned system:\n$$\n\\rho_j = \\frac{\\lambda_j - k^2}{\\lambda_j + \\alpha k^2 + i \\beta k^2}.\n$$\nThe choice $\\alpha \\approx -1$ is natural because it makes the real part of the preconditioner's \"shifted\" term, $\\alpha k^2$, match the shift in the original operator $A$, $-k^2$. If we set $\\alpha = -1$, the preconditioner becomes $M = L - k^2 I + i \\beta k^2 I = A + i \\beta k^2 I$. This means $M$ is a direct, regularized approximation of $A$. In this case, the eigenvalues are:\n$$\n\\rho_j = \\frac{\\lambda_j - k^2}{(\\lambda_j - k^2) + i \\beta k^2}.\n$$\nThe resonance problem occurs when $\\lambda_j \\approx k^2$, making the numerator $\\lambda_j - k^2$ small. Without a complex shift ($\\beta = 0$), the denominator would also be $\\lambda_j - k^2$, and $\\rho_j$ would be close to $1$. However, for eigenvalues $\\lambda_l \\neq \\lambda_j$, the ratio can be very different, leading to a wide spectral spread. If there is a slight mismatch in discretization between $A$ and the preconditioner, the denominator could become very small while the numerator is not, leading to large eigenvalues.\n\nWith a strictly positive imaginary shift, $\\beta > 0$, the term $i \\beta k^2$ is added to the denominator. Even when the real part $\\lambda_j - k^2$ is close to zero, the magnitude of the denominator is:\n$$\n|\\lambda_j - k^2 + i \\beta k^2| = \\sqrt{(\\lambda_j - k^2)^2 + (\\beta k^2)^2}.\n$$\nSince $\\beta > 0$ and $k \\neq 0$, we have $(\\beta k^2)^2 > 0$. This ensures that the denominator is bounded away from zero, regardless of how close $\\lambda_j$ is to $k^2$. This \"damping\" effect prevents the eigenvalues $\\rho_j$ from becoming excessively large or undefined, thus stabilizing the preconditioned system. The eigenvalues are mapped into a bounded region in the complex plane, which is highly beneficial for the convergence of GMRES. Specifically, for $\\alpha=-1$, all eigenvalues $\\rho_j$ lie on the circle in the complex plane with diameter connecting $0$ and $1$.\n\n### Task 4: Quantification of Spectral Clustering\n\nThe cluster radius $r(n,k,\\alpha,\\beta)$ is defined as the maximum deviation of the preconditioned eigenvalues from $1$:\n$$\nr(n,k,\\alpha,\\beta) = \\max_{1 \\le j \\le n} |1 - \\rho_j|.\n$$\nWe first derive a simplified expression for $|1 - \\rho_j|$:\n$$\n1 - \\rho_j = 1 - \\frac{\\lambda_j - k^2}{\\lambda_j + \\alpha k^2 + i \\beta k^2} = \\frac{(\\lambda_j + \\alpha k^2 + i \\beta k^2) - (\\lambda_j - k^2)}{\\lambda_j + \\alpha k^2 + i \\beta k^2} = \\frac{(\\alpha+1)k^2 + i \\beta k^2}{\\lambda_j + \\alpha k^2 + i \\beta k^2}.\n$$\nTaking the complex modulus:\n$$\n|1 - \\rho_j| = \\frac{|(\\alpha+1)k^2 + i \\beta k^2|}{|\\lambda_j + \\alpha k^2 + i \\beta k^2|} = \\frac{\\sqrt{((\\alpha+1)k^2)^2 + (\\beta k^2)^2}}{\\sqrt{(\\lambda_j + \\alpha k^2)^2 + (\\beta k^2)^2}} = \\frac{k^2 \\sqrt{(\\alpha+1)^2 + \\beta^2}}{\\sqrt{(\\lambda_j + \\alpha k^2)^2 + (\\beta k^2)^2}}.\n$$\nTo calculate $r$, we need to find the maximum of this quantity over all $j \\in \\{1, \\ldots, n\\}$. The numerator is constant with respect to $j$. Therefore, maximizing the fraction is equivalent to minimizing its denominator, $\\sqrt{(\\lambda_j + \\alpha k^2)^2 + (\\beta k^2)^2}$. Since $(\\beta k^2)^2$ is also constant, we must find the minimum of $(\\lambda_j + \\alpha k^2)^2$. This is achieved when $\\lambda_j$ is closest to $-\\alpha k^2$.\nLet $d_{min}^2 = \\min_{1 \\le j \\le n} (\\lambda_j + \\alpha k^2)^2$. Then the cluster radius is:\n$$\nr(n,k,\\alpha,\\beta) = \\frac{k^2 \\sqrt{(\\alpha+1)^2 + \\beta^2}}{\\sqrt{d_{min}^2 + (\\beta k^2)^2}}.\n$$\nThe provided Python program implements this formula to compute the cluster radius for the given test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the spectral cluster radius for a complex-shifted Laplacian\n    preconditioner applied to the 1D Helmholtz equation.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (n, k, alpha, beta)\n        (200, 3.2, -1.0, 0.5),      # Test case 1\n        (200, 3.2, 0.0, 0.5),       # Test case 2\n        (200, 20.0, -1.0, 0.05),    # Test case 3\n        (200, 3.14159, -1.0, 1e-6), # Test case 4\n    ]\n\n    results = []\n    for params in test_cases:\n        n, k, alpha, beta = params\n\n        # Grid spacing\n        h = 1.0 / (n + 1.0)\n\n        # Vector of indices j = 1, 2, ..., n\n        j = np.arange(1, n + 1)\n\n        # Eigenvalues of the discrete Laplacian operator L\n        # lambda_j = (4/h^2) * sin^2(j*pi / (2*(n+1)))\n        lambdas = (4.0 / h**2) * np.sin(j * np.pi / (2.0 * (n + 1.0)))**2\n\n        # The cluster radius r is max(|1 - rho_j|).\n        # |1 - rho_j| = (k^2 * sqrt((alpha+1)^2 + beta^2)) / sqrt((lambda_j + alpha*k^2)^2 + (beta*k^2)^2)\n        # To maximize this, we need to minimize the denominator, which means minimizing\n        # (lambda_j + alpha*k^2)^2.\n\n        # Calculate the numerator (constant with respect to j)\n        k_sq = k**2\n        numerator = k_sq * np.sqrt((alpha + 1.0)**2 + beta**2)\n\n        # Find the minimum of the real part of the denominator's argument squared.\n        # This corresponds to finding the lambda_j closest to -alpha*k^2.\n        real_part_sq = (lambdas + alpha * k_sq)**2\n        d_min_sq = np.min(real_part_sq)\n\n        # Calculate the denominator using the minimum value found.\n        imag_part_sq = (beta * k_sq)**2\n        denominator = np.sqrt(d_min_sq + imag_part_sq)\n\n        # Calculate the cluster radius for this case.\n        # Handle the case where the denominator might be zero, though unlikely with beta > 0.\n        if denominator == 0:\n            # This would imply a division by zero, which is not expected\n            # in a well-posed physical problem with damping.\n            # Assigning infinity or a very large number might be appropriate.\n            # However, for the given problem parameters, this case will not occur.\n            radius = np.inf\n        else:\n            radius = numerator / denominator\n\n        results.append(radius)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.7f}' for r in results)}]\")\n\nsolve()\n```", "id": "3613279"}]}