{"hands_on_practices": [{"introduction": "The two-dimensional Poisson equation is a cornerstone model in numerous fields of physics and geophysics, describing phenomena from steady-state heat conduction to gravitational potentials. This exercise connects the continuous PDE to its discrete linear algebraic representation, a crucial first step in any numerical simulation. By analyzing the matrix structure resulting from a standard finite-difference discretization, you will analytically derive the spectral radius of the Jacobi iteration matrix, which directly governs the method's convergence rate [@problem_id:3615365]. This practice is fundamental for developing the skill to predict solver performance from the underlying physics and grid configuration.", "problem": "Consider the steady-state diffusion of temperature in a homogeneous, isotropic, two-dimensional medium with homogeneous Dirichlet boundary conditions on the unit square, modeled by the partial differential equation (PDE) $-\\nabla^{2} u(x,y) = f(x,y)$ for $(x,y) \\in (0,1) \\times (0,1)$ with $u(x,y) = 0$ on the boundary. Discretize this PDE using a uniform $n \\times n$ grid of interior points obtained by $n$ equally spaced nodes in each coordinate direction, with grid spacing $h = \\frac{1}{n+1}$. Use the standard five-point stencil for the Laplacian and natural row-wise lexicographic ordering of the unknowns $u_{i,j}$ into a vector $u \\in \\mathbb{R}^{n^{2}}$, where $i$ is the column index and $j$ is the row index, and the linear system is $A u = b$ with $A \\in \\mathbb{R}^{n^{2} \\times n^{2}}$.\n\nTasks:\n1. Starting from the finite-difference approximation of $-\\nabla^{2} u$, write the matrix $A$ explicitly in terms of Kronecker products and also describe its block structure in terms of a tridiagonal matrix in one dimension. Clearly define the diagonal matrix $D$ and the strictly lower and strictly upper triangular matrices $L$ and $U$ in the splitting $A = D - L - U$, consistent with the signs produced by the five-point stencil.\n2. Define the half-bandwidth $b$ of a sparse matrix as $b = \\max\\{\\,|i-j| : A_{ij} \\neq 0\\,\\}$ and the full bandwidth $B$ as $B = 2b + 1$. Compute $b$ and $B$ for $A$ under the specified ordering.\n3. Compute the exact total number of nonzero entries $\\operatorname{nnz}(A)$ as a function of $n$, accounting for corner, edge, and interior grid points.\n4. Using separation of variables for the discrete problem with homogeneous Dirichlet boundary conditions, derive the eigenpairs of $A$, and from these, derive the eigenvalues of the Jacobi iteration matrix $B_{J} = I - D^{-1} A$. Finally, provide the spectral radius $\\rho_{J}(n)$ of $B_{J}$ as a closed-form analytic expression in terms of $n$.\n\nExpress the final spectral radius as a single closed-form analytic expression in terms of $n$. Do not evaluate numerically. If you introduce any acronyms, spell them out on first use. Use only the mathematical definitions and derivations appropriate to the described physical and computational setting; do not assume or quote formulas that have not been derived from this base in your solution.", "solution": "The user-provided problem is assessed to be valid. It is scientifically grounded in the fields of numerical analysis and computational physics, is well-posed with a clear and complete setup, and is expressed objectively. Therefore, a full solution is provided.\n\nThe problem asks for an analysis of the matrix resulting from the finite-difference discretization of the two-dimensional Poisson equation with homogeneous Dirichlet boundary conditions.\n\n**Task 1: Matrix Structure and Splitting**\n\nThe Partial Differential Equation (PDE) is $-\\nabla^{2} u(x,y) = f(x,y)$ on the unit square $(0,1) \\times (0,1)$, with $u=0$ on the boundary. We discretize this using a uniform grid with $n$ interior points in each direction, so there are $n \\times n = n^2$ total interior grid points. The grid spacing is $h = \\frac{1}{n+1}$.\n\nThe standard five-point stencil for the negative Laplacian operator, $-\\nabla^2$, at a grid point $(x_i, y_j)$ is:\n$$ -\\nabla^{2} u(x_i, y_j) \\approx \\frac{1}{h^2} [4u_{i,j} - u_{i-1,j} - u_{i+1,j} - u_{i,j-1} - u_{i,j+1}] $$\nwhere $u_{i,j} = u(x_i, y_j)$. The discretized PDE becomes a system of linear equations. At each interior grid point $(i,j)$ for $i,j = 1, \\dots, n$, we have:\n$$ 4u_{i,j} - u_{i-1,j} - u_{i+1,j} - u_{i,j-1} - u_{i,j+1} = h^2 f_{i,j} $$\nThis system is written as $Au=b$, where $A$ is the coefficient matrix of the unknowns $u_{i,j}$, $u$ is the vector of unknowns, and $b$ is the vector of right-hand side values $h^2 f_{i,j}$. The matrix $A$ has dimensions $n^2 \\times n^2$. The vector $u$ is formed by ordering the unknowns $u_{i,j}$ using row-wise lexicographic ordering: $u = [u_{1,1}, u_{2,1}, \\dots, u_{n,1}, u_{1,2}, \\dots, u_{n,2}, \\dots, u_{1,n}, \\dots, u_{n,n}]^T$.\n\nThe structure of $A$ can be described using the Kronecker product. Let $I_n$ be the $n \\times n$ identity matrix. Let $K_n$ be the $n \\times n$ tridiagonal matrix that represents the one-dimensional second-order centered difference operator:\n$$ K_n = \\begin{pmatrix} 2  -1    \\\\ -1  2  -1   \\\\  \\ddots  \\ddots  \\ddots  \\\\   -1  2  -1 \\\\    -1  2 \\end{pmatrix} $$\nThe matrix $A$ for the two-dimensional operator with row-wise ordering is the Kronecker sum of $K_n$ with itself:\n$$ A = I_n \\otimes K_n + K_n \\otimes I_n $$\nThe term $I_n \\otimes K_n$ represents the connections in the $x$-direction (within rows), and the term $K_n \\otimes I_n$ represents the connections in the $y$-direction (between rows).\n\nThis Kronecker product structure gives $A$ a block-tridiagonal form. Let $T_n = K_n + 2I_n$. $T_n$ is an $n \\times n$ matrix:\n$$ T_n = \\begin{pmatrix} 4  -1    \\\\ -1  4  -1   \\\\  \\ddots  \\ddots  \\ddots  \\\\   -1  4  -1 \\\\    -1  4 \\end{pmatrix} $$\nThen $A$ can be written as an $n \\times n$ block matrix, where each block is of size $n \\times n$:\n$$ A = \\begin{pmatrix} T_n  -I_n    \\\\ -I_n  T_n  -I_n   \\\\  \\ddots  \\ddots  \\ddots  \\\\   -I_n  T_n  -I_n \\\\    -I_n  T_n \\end{pmatrix} $$\nFor the splitting $A = D - L - U$:\n- $D$ is the diagonal part of $A$. From the structure of $A$, every diagonal element is $4$. Thus, $D = 4I_{n^2}$, where $I_{n^2}$ is the $n^2 \\times n^2$ identity matrix.\n- $L$ is the strictly lower triangular part of $D-A$. The entries of $L$ are $L_{ij} = -A_{ij}$ for $ij$ and $0$ otherwise. Specifically, using the global index $k = (j-1)n+i$, the non-zero entries of $L$ are $L_{k, k-1}=1$ (for $i1$) and $L_{k, k-n}=1$.\n- $U$ is the strictly upper triangular part of $D-A$. The entries of $U$ are $U_{ij} = -A_{ij}$ for $ij$ and $0$ otherwise. The non-zero entries of $U$ are $U_{k, k+1}=1$ (for $in$) and $U_{k, k+n}=1$.\n\n**Task 2: Bandwidth**\n\nThe half-bandwidth $b$ is defined as $b = \\max\\{\\,|i-j| : A_{ij} \\neq 0\\,\\}$. The non-zero off-diagonal entries of $A_{k,m}$ occur when the node $k$ is connected to node $m$. With row-wise lexicographic ordering, a node with index $k = (j-1)n+i$ is connected to nodes with indices:\n- $(j-1)n+(i-1) = k-1$ (left neighbor)\n- $(j-1)n+(i+1) = k+1$ (right neighbor)\n- $(j-2)n+i = k-n$ (bottom neighbor)\n- $jn+i = k+n$ (top neighbor)\nThe differences in indices are $|k-m| \\in \\{1, n\\}$. The maximum of these is $n$.\nTherefore, the half-bandwidth is $b=n$.\nThe full bandwidth $B$ is $B = 2b+1$, so $B = 2n+1$.\n\n**Task 3: Number of Nonzero Entries**\n\nThe number of nonzero entries, denoted $\\operatorname{nnz}(A)$, can be calculated by summing the non-zero entries for each row.\nThe matrix $A$ is symmetric. The number of diagonal entries is $n^2$.\nThe number of off-diagonal non-zero entries can be counted by considering the grid connections.\n- There are $n$ rows of grid points, and in each row there are $n-1$ horizontal connections between adjacent points. This gives $n(n-1)$ horizontal connections.\n- There are $n$ columns of grid points, and in each column there are $n-1$ vertical connections between adjacent points. This gives $n(n-1)$ vertical connections.\nThe total number of connections is $n(n-1) + n(n-1) = 2n(n-1)$. Each connection corresponds to two off-diagonal entries in the symmetric matrix $A$ (e.g., $A_{ij}$ and $A_{ji}$). So, the total number of off-diagonal non-zero entries is $2 \\times [2n(n-1)] = 4n^2-4n$.\nThe total number of nonzero entries is the sum of diagonal and off-diagonal entries:\n$$ \\operatorname{nnz}(A) = n^2 + (4n^2-4n) = 5n^2-4n $$\n\n**Task 4: Eigenvalues and Spectral Radius**\n\nThe problem suggests using separation of variables for the discrete problem, which is equivalent to finding the eigenpairs of the matrix $A$. The eigenvalues of $A = I_n \\otimes K_n + K_n \\otimes I_n$ are sums of the eigenvalues of $K_n$.\nThe eigenvalues of the $n \\times n$ matrix $K_n = \\text{tridiag}(-1, 2, -1)$ are known to be:\n$$ \\lambda_k(K_n) = 2 - 2\\cos\\left(\\frac{k\\pi}{n+1}\\right) \\quad \\text{for } k=1, 2, \\dots, n. $$\nThe corresponding eigenvectors have components $(v_k)_j = \\sin\\left(\\frac{jk\\pi}{n+1}\\right)$.\nThe eigenvectors of $A$ are the Kronecker products $v_p \\otimes v_q$ of the eigenvectors of $K_n$. The eigenvalues of $A$, denoted $\\mu_{p,q}(A)$, are the sums of the eigenvalues of $K_n$:\n$$ \\mu_{p,q}(A) = \\lambda_p(K_n) + \\lambda_q(K_n) = \\left(2 - 2\\cos\\left(\\frac{p\\pi}{n+1}\\right)\\right) + \\left(2 - 2\\cos\\left(\\frac{q\\pi}{n+1}\\right)\\right) $$\n$$ \\mu_{p,q}(A) = 4 - 2\\left(\\cos\\left(\\frac{p\\pi}{n+1}\\right) + \\cos\\left(\\frac{q\\pi}{n+1}\\right)\\right) \\quad \\text{for } p,q=1, 2, \\dots, n. $$\nThe eigenpairs of $A$ are $(\\mu_{p,q}(A), v_p \\otimes v_q)$. In terms of grid functions, the eigenvector $u^{(p,q)}$ corresponding to $\\mu_{p,q}(A)$ has components $u^{(p,q)}_{i,j} = \\sin\\left(\\frac{ip\\pi}{n+1}\\right)\\sin\\left(\\frac{jq\\pi}{n+1}\\right)$.\n\nNext, we derive the eigenvalues of the Jacobi iteration matrix, $B_J = I - D^{-1}A$.\nWe found $D = 4I_{n^2}$, so $D^{-1} = \\frac{1}{4}I_{n^2}$.\n$$ B_J = I_{n^2} - \\frac{1}{4}A $$\nIf $v$ is an eigenvector of $A$ with eigenvalue $\\mu$, then it is also an eigenvector of $B_J$ with eigenvalue $1 - \\frac{1}{4}\\mu$.\nThe eigenvalues of $B_J$, denoted $\\lambda_{p,q}(B_J)$, are:\n$$ \\lambda_{p,q}(B_J) = 1 - \\frac{1}{4} \\mu_{p,q}(A) = 1 - \\frac{1}{4}\\left[4 - 2\\left(\\cos\\left(\\frac{p\\pi}{n+1}\\right) + \\cos\\left(\\frac{q\\pi}{n+1}\\right)\\right)\\right] $$\n$$ \\lambda_{p,q}(B_J) = 1 - 1 + \\frac{1}{2}\\left(\\cos\\left(\\frac{p\\pi}{n+1}\\right) + \\cos\\left(\\frac{q\\pi}{n+1}\\right)\\right) $$\n$$ \\lambda_{p,q}(B_J) = \\frac{1}{2}\\left(\\cos\\left(\\frac{p\\pi}{n+1}\\right) + \\cos\\left(\\frac{q\\pi}{n+1}\\right)\\right) $$\nThe spectral radius $\\rho_J(n)$ is the maximum absolute value of these eigenvalues over all valid $p, q$:\n$$ \\rho_J(n) = \\max_{p,q \\in \\{1,...,n\\}} \\left| \\frac{1}{2}\\left(\\cos\\left(\\frac{p\\pi}{n+1}\\right) + \\cos\\left(\\frac{q\\pi}{n+1}\\right)\\right) \\right| $$\nThe arguments of the cosine functions, $\\frac{p\\pi}{n+1}$ and $\\frac{q\\pi}{n+1}$, are in the interval $(0, \\pi)$. The cosine function is decreasing on this interval. To maximize the expression, we need to choose the smallest possible arguments for the cosine functions, which occurs for $p=1$ and $q=1$. This gives the maximum positive eigenvalue:\n$$ \\lambda_{\\max} = \\frac{1}{2}\\left(\\cos\\left(\\frac{\\pi}{n+1}\\right) + \\cos\\left(\\frac{\\pi}{n+1}\\right)\\right) = \\cos\\left(\\frac{\\pi}{n+1}\\right) $$\nTo find the minimum (most negative) eigenvalue, we choose the largest possible arguments, which occurs for $p=n$ and $q=n$:\n$$ \\lambda_{\\min} = \\frac{1}{2}\\left(\\cos\\left(\\frac{n\\pi}{n+1}\\right) + \\cos\\left(\\frac{n\\pi}{n+1}\\right)\\right) = \\cos\\left(\\frac{n\\pi}{n+1}\\right) $$\nUsing the identity $\\cos(\\pi - x) = -\\cos(x)$, we have $\\cos\\left(\\frac{n\\pi}{n+1}\\right) = \\cos\\left(\\pi - \\frac{\\pi}{n+1}\\right) = -\\cos\\left(\\frac{\\pi}{n+1}\\right)$.\nSo, $\\lambda_{\\min} = -\\cos\\left(\\frac{\\pi}{n+1}\\right)$.\nThe eigenvalues of $B_J$ lie in the interval $\\left[-\\cos\\left(\\frac{\\pi}{n+1}\\right), \\cos\\left(\\frac{\\pi}{n+1}\\right)\\right]$.\nThe spectral radius is the maximum of the absolute values of the eigenvalues, which is:\n$$ \\rho_J(n) = \\left| \\pm \\cos\\left(\\frac{\\pi}{n+1}\\right) \\right| = \\cos\\left(\\frac{\\pi}{n+1}\\right) $$\nSince $n \\ge 1$, the argument $\\frac{\\pi}{n+1}$ is in $(0, \\pi/2]$, so its cosine is non-negative.", "answer": "$$\\boxed{\\cos\\left(\\frac{\\pi}{n+1}\\right)}$$", "id": "3615365"}, {"introduction": "When implementing iterative solvers, we typically monitor convergence by observing the decrease in the norm of the residual vector, $r^k = b - Ax^k$. However, this metric can sometimes be misleading. This exercise reveals a non-intuitive but important property of the Jacobi method for certain symmetric positive definite systems: the residual norm can stagnate or oscillate even as the solution is steadily approaching the correct answer. You will analytically derive the sequences for both the error and the residual, demonstrating that while the $2$-norm of the residual fails to decrease at every step, the error converges monotonically in the energy norm ($A$-norm) [@problem_id:3615421]. This practice builds a more robust understanding of convergence, emphasizing the importance of choosing the appropriate mathematical lens to analyze a solver's behavior.", "problem": "Consider a symmetric positive definite (SPD) linear system arising from a simple two-parameter discretized conductivity model in computational geophysics, with matrix\n$$\nA \\equiv \\begin{pmatrix} 1  1 \\\\ 1  2 \\end{pmatrix},\n$$\nunknown vector $x \\in \\mathbb{R}^{2}$, and right-hand side $b \\in \\mathbb{R}^{2}$. Let the initial guess be $x^{0} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ and choose $b = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$ so that the initial residual is $r^{0} = b - A x^{0} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$. Consider the classical Jacobi method based on the diagonal splitting $A = D + (L+U)$ with $D = \\operatorname{diag}(A)$ and the iteration\n$$\nx^{k+1} = x^{k} + D^{-1}\\bigl(b - A x^{k}\\bigr).\n$$\nDefine the error $e^{k} \\equiv x^{\\ast} - x^{k}$, where $x^{\\ast}$ solves $A x^{\\ast} = b$, and the residual $r^{k} \\equiv b - A x^{k}$. Work from first principles, using only the definitions above and standard properties of SPD matrices and induced norms, to do the following:\n\n1) Derive the error and residual iteration matrices for Jacobi,\n$$\nM \\equiv I - D^{-1} A, \\qquad R \\equiv I - A D^{-1},\n$$\nand compute them explicitly for the given $A$.\n\n2) Using only linear algebraic identities and matrix powers, compute closed-form expressions for the full sequences $\\{r^{k}\\}_{k \\ge 0}$ and $\\{e^{k}\\}_{k \\ge 0}$.\n\n3) Show that the residual $2$-norm stagnates every other step, in the precise sense that\n$$\n\\|r^{2m+1}\\|_{2} = \\|r^{2m}\\|_{2} \\quad \\text{for all integers } m \\ge 0,\n$$\neven though the error decreases in the $A$-norm at each iteration.\n\n4) Give the exact closed-form expression for the $A$-norm of the error after $k$ Jacobi iterations, $\\|e^{k}\\|_{A} \\equiv \\sqrt{(e^{k})^{\\mathsf{T}} A e^{k}}$, as a function of the iteration count $k$. State your answer as a single analytic expression in $k$. No rounding is required.", "solution": "The problem as stated is mathematically and scientifically sound, well-posed, and contains all necessary information for a unique solution. It is a standard, yet insightful, problem in the field of numerical linear algebra, exploring the properties of the Jacobi iterative method. All premises are factually correct and all definitions are standard. The matrix $A = \\begin{pmatrix} 1  1 \\\\ 1  2 \\end{pmatrix}$ is verified to be symmetric and its eigenvalues, $\\frac{3 \\pm \\sqrt{5}}{2}$, are positive, confirming it is positive definite (SPD). The problem is therefore valid, and we may proceed with the solution.\n\nThe solution is presented in four parts, corresponding to the four tasks in the problem statement.\n\n1) Derivation and computation of the error and residual iteration matrices.\n\nThe Jacobi iteration is given by\n$$\nx^{k+1} = x^{k} + D^{-1}(b - A x^{k})\n$$\nwhere $D = \\operatorname{diag}(A)$. The true solution $x^{\\ast}$ satisfies $A x^{\\ast} = b$.\n\nThe error vector at iteration $k$ is defined as $e^{k} \\equiv x^{\\ast} - x^{k}$. Subtracting the Jacobi iteration equation from $x^{\\ast}$ yields:\n$$\nx^{\\ast} - x^{k+1} = x^{\\ast} - x^{k} - D^{-1}(b - A x^{k})\n$$\n$$\ne^{k+1} = e^{k} - D^{-1}(A x^{\\ast} - A x^{k})\n$$\n$$\ne^{k+1} = e^{k} - D^{-1}A(x^{\\ast} - x^{k})\n$$\n$$\ne^{k+1} = (I - D^{-1}A) e^{k}\n$$\nThus, the error iteration matrix is $M \\equiv I - D^{-1}A$.\n\nThe residual vector at iteration $k$ is defined as $r^{k} \\equiv b - A x^{k}$. The update rule for the residual is derived as follows:\n$$\nr^{k+1} = b - A x^{k+1} = b - A \\left(x^{k} + D^{-1}(b - A x^{k})\\right)\n$$\n$$\nr^{k+1} = (b - A x^{k}) - A D^{-1}(b - Ax^{k})\n$$\n$$\nr^{k+1} = r^{k} - A D^{-1} r^{k}\n$$\n$$\nr^{k+1} = (I - A D^{-1}) r^{k}\n$$\nThus, the residual iteration matrix is $R \\equiv I - A D^{-1}$.\n\nFor the given matrix $A = \\begin{pmatrix} 1  1 \\\\ 1  2 \\end{pmatrix}$, the diagonal matrix is $D = \\begin{pmatrix} 1  0 \\\\ 0  2 \\end{pmatrix}$ and its inverse is $D^{-1} = \\begin{pmatrix} 1  0 \\\\ 0  \\frac{1}{2} \\end{pmatrix}$.\n\nThe error iteration matrix $M$ is:\n$$\nM = I - D^{-1}A = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} - \\begin{pmatrix} 1  0 \\\\ 0  \\frac{1}{2} \\end{pmatrix} \\begin{pmatrix} 1  1 \\\\ 1  2 \\end{pmatrix} = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} - \\begin{pmatrix} 1  1 \\\\ \\frac{1}{2}  1 \\end{pmatrix} = \\begin{pmatrix} 0  -1 \\\\ -\\frac{1}{2}  0 \\end{pmatrix}\n$$\nThe residual iteration matrix $R$ is:\n$$\nR = I - A D^{-1} = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} - \\begin{pmatrix} 1  1 \\\\ 1  2 \\end{pmatrix} \\begin{pmatrix} 1  0 \\\\ 0  \\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} - \\begin{pmatrix} 1  \\frac{1}{2} \\\\ 1  1 \\end{pmatrix} = \\begin{pmatrix} 0  -\\frac{1}{2} \\\\ -1  0 \\end{pmatrix}\n$$\nNote that since $A$ and $D$ are symmetric, $R = (I - AD^{-1}) = (I - D^{-1}A)^{\\mathsf{T}} = M^{\\mathsf{T}}$.\n\n2) Closed-form expressions for the sequences $\\{r^{k}\\}$ and $\\{e^{k}\\}$.\n\nThe sequences are given by $r^{k} = R^{k} r^{0}$ and $e^{k} = M^{k} e^{0}$. We first need the initial vectors $r^0$ and $e^0$.\nGiven $x^{0} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ and $b = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$, the initial residual is $r^{0} = b - A x^{0} = b = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$.\nThe true solution $x^{\\ast}$ is found by solving $A x^{\\ast} = b$:\n$$\nx^{\\ast} = A^{-1} b = \\left(\\frac{1}{1 \\cdot 2 - 1 \\cdot 1} \\begin{pmatrix} 2  -1 \\\\ -1  1 \\end{pmatrix}\\right) \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 2  -1 \\\\ -1  1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix}\n$$\nThe initial error is $e^{0} = x^{\\ast} - x^{0} = x^{\\ast} = \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix}$.\n\nTo find $R^k$, we diagonalize $R$. The characteristic equation for $R$ is $\\det(R-\\lambda I) = \\lambda^2 - \\frac{1}{2} = 0$, so the eigenvalues are $\\lambda_{R,1} = \\frac{1}{\\sqrt{2}}$ and $\\lambda_{R,2} = -\\frac{1}{\\sqrt{2}}$. The corresponding eigenvectors are $v_{R,1} = \\begin{pmatrix} 1 \\\\ -\\sqrt{2} \\end{pmatrix}$ and $v_{R,2} = \\begin{pmatrix} 1 \\\\ \\sqrt{2} \\end{pmatrix}$.\nWe express $r^0$ in this eigenbasis: $r^{0} = c_1 v_{R,1} + c_2 v_{R,2}$.\n$$\n\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = c_1 \\begin{pmatrix} 1 \\\\ -\\sqrt{2} \\end{pmatrix} + c_2 \\begin{pmatrix} 1 \\\\ \\sqrt{2} \\end{pmatrix} \\implies \\left\\{ \\begin{array}{rcr} c_1 + c_2  =  1 \\\\ -\\sqrt{2} c_1 + \\sqrt{2} c_2  =  0 \\end{array} \\right. \\implies c_1 = c_2 = \\frac{1}{2}.\n$$\nSo, $r^{k} = R^k r^0 = \\frac{1}{2} \\lambda_{R,1}^k v_{R,1} + \\frac{1}{2} \\lambda_{R,2}^k v_{R,2}$:\n$$\nr^{k} = \\frac{1}{2} \\left(\\frac{1}{\\sqrt{2}}\\right)^k \\begin{pmatrix} 1 \\\\ -\\sqrt{2} \\end{pmatrix} + \\frac{1}{2} \\left(-\\frac{1}{\\sqrt{2}}\\right)^k \\begin{pmatrix} 1 \\\\ \\sqrt{2} \\end{pmatrix} = \\frac{1}{2^{k/2+1}} \\left( \\begin{pmatrix} 1 \\\\ -\\sqrt{2} \\end{pmatrix} + (-1)^k \\begin{pmatrix} 1 \\\\ \\sqrt{2} \\end{pmatrix} \\right) = \\frac{1}{2^{k/2+1}} \\begin{pmatrix} 1+(-1)^k \\\\ \\sqrt{2}((-1)^k - 1) \\end{pmatrix}\n$$\nSimilarly, $M$ has eigenvalues $\\lambda_{M,1} = \\frac{1}{\\sqrt{2}}$, $\\lambda_{M,2} = -\\frac{1}{\\sqrt{2}}$. The eigenvectors are $v_{M,1} = \\begin{pmatrix} -\\sqrt{2} \\\\ 1 \\end{pmatrix}$ and $v_{M,2} = \\begin{pmatrix} \\sqrt{2} \\\\ 1 \\end{pmatrix}$.\nWe express $e^0$ in this basis: $e^0 = d_1 v_{M,1} + d_2 v_{M,2}$.\n$$\n\\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix} = d_1 \\begin{pmatrix} -\\sqrt{2} \\\\ 1 \\end{pmatrix} + d_2 \\begin{pmatrix} \\sqrt{2} \\\\ 1 \\end{pmatrix} \\implies \\left\\{ \\begin{array}{rcr} -\\sqrt{2} d_1 + \\sqrt{2} d_2  =  2 \\\\ d_1 + d_2  =  -1 \\end{array} \\right. \\implies d_1 = -\\frac{\\sqrt{2}+1}{2}, d_2 = \\frac{\\sqrt{2}-1}{2}.\n$$\nSo, $e^{k} = M^k e^0 = d_1 \\lambda_{M,1}^k v_{M,1} + d_2 \\lambda_{M,2}^k v_{M,2}$:\n$$\ne^{k} = -\\frac{\\sqrt{2}+1}{2} \\left(\\frac{1}{\\sqrt{2}}\\right)^k \\begin{pmatrix} -\\sqrt{2} \\\\ 1 \\end{pmatrix} + \\frac{\\sqrt{2}-1}{2} \\left(-\\frac{1}{\\sqrt{2}}\\right)^k \\begin{pmatrix} \\sqrt{2} \\\\ 1 \\end{pmatrix}\n$$\n\n3) Stagnation of the residual $2$-norm.\n\nFrom the closed form of $r^k$, we analyze the cases for even and odd $k$.\nLet $k=2m$ for an integer $m \\ge 0$.\n$$\nr^{2m} = \\frac{1}{2^{m+1}} \\begin{pmatrix} 1+1 \\\\ \\sqrt{2}(1-1) \\end{pmatrix} = \\frac{1}{2^{m+1}} \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix} = \\frac{1}{2^m} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}\n$$\nThe squared $2$-norm is $\\|r^{2m}\\|_{2}^2 = \\left(\\frac{1}{2^m}\\right)^2 (1^2+0^2) = \\frac{1}{4^m}$.\n\nLet $k=2m+1$ for an integer $m \\ge 0$.\n$$\nr^{2m+1} = \\frac{1}{2^{(2m+1)/2+1}} \\begin{pmatrix} 1-1 \\\\ \\sqrt{2}(-1-1) \\end{pmatrix} = \\frac{1}{2^{m+3/2}} \\begin{pmatrix} 0 \\\\ -2\\sqrt{2} \\end{pmatrix} = \\frac{-2^{3/2}}{2^{m+3/2}} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = -\\frac{1}{2^m} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}\n$$\nThe squared $2$-norm is $\\|r^{2m+1}\\|_{2}^2 = \\left(-\\frac{1}{2^m}\\right)^2 (0^2+1^2) = \\frac{1}{4^m}$.\nSince $\\|r^{2m}\\|_{2}^2 = \\|r^{2m+1}\\|_{2}^2 = \\frac{1}{4^m}$, we have $\\|r^{2m+1}\\|_{2} = \\|r^{2m}\\|_{2} = \\frac{1}{2^m}$. The residual norm stagnates every other step.\n\nTo show the error decreases in the $A$-norm, we analyze the change in the squared $A$-norm, $\\|e^k\\|_A^2 = (e^k)^{\\mathsf{T}} A e^k$.\nThe error update is $e^{k+1} = e^k - D^{-1} r^k$.\n$$\n\\|e^{k+1}\\|_A^2 = (e^k - D^{-1} r^k)^{\\mathsf{T}} A (e^k - D^{-1} r^k)\n$$\n$$\n= (e^k)^{\\mathsf{T}} A e^k - (e^k)^{\\mathsf{T}} A D^{-1} r^k - (r^k)^{\\mathsf{T}} (D^{-1})^{\\mathsf{T}} A e^k + (r^k)^{\\mathsf{T}} (D^{-1})^{\\mathsf{T}} A D^{-1} r^k\n$$\nUsing $r^k = A e^k$, $A^{\\mathsf{T}} = A$, and $D^{\\mathsf{T}} = D$:\n$$\n\\|e^{k+1}\\|_A^2 = \\|e^k\\|_A^2 - (r^k)^{\\mathsf{T}} D^{-1} r^k - (r^k)^{\\mathsf{T}} D^{-1} r^k + (r^k)^{\\mathsf{T}} D^{-1} A D^{-1} r^k\n$$\n$$\n\\|e^{k+1}\\|_A^2 = \\|e^k\\|_A^2 - (r^k)^{\\mathsf{T}} (2 D^{-1} - D^{-1} A D^{-1}) r^k\n$$\nThe error decreases if the matrix $S = 2 D^{-1} - D^{-1} A D^{-1}$ is positive definite. $D$ is PD, so we need only check $2I - D^{-1/2} A D^{-1/2}$ is PD. The eigenvalues of the scaled matrix $B=D^{-1/2} A D^{-1/2}$ are $1 \\pm 1/\\sqrt{2}$. The eigenvalues of $2I-B$ are $2 - (1 \\pm 1/\\sqrt{2}) = 1 \\mp 1/\\sqrt{2}$. Both are positive, hence $S$ is positive definite. Therefore, $\\|e^{k+1}\\|_A^2  \\|e^k\\|_A^2$ as long as $r^k \\neq 0$, showing strict error reduction.\n\n4) Exact closed-form expression for $\\|e^{k}\\|_{A}$.\n\nWe compute $\\|e^k\\|_A^2 = (e^k)^{\\mathsf{T}} A e^k$ using the eigendecomposition of $e^k$.\n$$\ne^k = d_1 \\lambda_{M,1}^k v_{M,1} + d_2 \\lambda_{M,2}^k v_{M,2}\n$$\n$$\n\\|e^k\\|_A^2 = (d_1 \\lambda_1^k v_1 + d_2 \\lambda_2^k v_2)^{\\mathsf{T}} A (d_1 \\lambda_1^k v_1 + d_2 \\lambda_2^k v_2)\n$$\nAs shown in standard theory and verifiable by direct computation, the eigenvectors of $M$ are $A$-orthogonal for an SPD matrix $A$.\n$$\nv_{M,1}^{\\mathsf{T}} A v_{M,2} = \\begin{pmatrix} -\\sqrt{2}  1 \\end{pmatrix} \\begin{pmatrix} 1  1 \\\\ 1  2 \\end{pmatrix} \\begin{pmatrix} \\sqrt{2} \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} -\\sqrt{2}+1  -\\sqrt{2}+2 \\end{pmatrix} \\begin{pmatrix} \\sqrt{2} \\\\ 1 \\end{pmatrix} = -2+\\sqrt{2}-\\sqrt{2}+2 = 0\n$$\nThe cross-terms vanish, simplifying the expression:\n$$\n\\|e^k\\|_A^2 = (d_1 \\lambda_1^k)^2 (v_1^{\\mathsf{T}} A v_1) + (d_2 \\lambda_2^k)^2 (v_2^{\\mathsf{T}} A v_2)\n$$\nWe have $(\\lambda_1^k)^2 = (\\lambda_2^k)^2 = (1/2)^k = 2^{-k}$.\n$d_1^2 = \\left(-\\frac{\\sqrt{2}+1}{2}\\right)^2 = \\frac{3+2\\sqrt{2}}{4}$. $d_2^2 = \\left(\\frac{\\sqrt{2}-1}{2}\\right)^2 = \\frac{3-2\\sqrt{2}}{4}$.\n$v_1^{\\mathsf{T}} A v_1 = \\begin{pmatrix} -\\sqrt{2}  1 \\end{pmatrix} A \\begin{pmatrix} -\\sqrt{2} \\\\ 1 \\end{pmatrix} = 4 - 2\\sqrt{2} = 2(2-\\sqrt{2})$.\n$v_2^{\\mathsf{T}} A v_2 = \\begin{pmatrix} \\sqrt{2}  1 \\end{pmatrix} A \\begin{pmatrix} \\sqrt{2} \\\\ 1 \\end{pmatrix} = 4 + 2\\sqrt{2} = 2(2+\\sqrt{2})$.\nSubstituting these values:\n$$\n\\|e^k\\|_A^2 = 2^{-k} \\left[ \\left(\\frac{3+2\\sqrt{2}}{4}\\right) (4-2\\sqrt{2}) + \\left(\\frac{3-2\\sqrt{2}}{4}\\right) (4+2\\sqrt{2}) \\right]\n$$\n$$\n\\|e^k\\|_A^2 = 2^{-k} \\left[ \\frac{12 - 6\\sqrt{2} + 8\\sqrt{2} - 8}{4} + \\frac{12 + 6\\sqrt{2} - 8\\sqrt{2} - 8}{4} \\right]\n$$\n$$\n\\|e^k\\|_A^2 = 2^{-k} \\left[ \\frac{4+2\\sqrt{2}}{4} + \\frac{4-2\\sqrt{2}}{4} \\right] = 2^{-k} \\left[ \\left(1+\\frac{\\sqrt{2}}{2}\\right) + \\left(1-\\frac{\\sqrt{2}}{2}\\right) \\right] = 2^{-k} \\cdot 2 = 2^{1-k}\n$$\nThe $A$-norm of the error is the square root of this value:\n$$\n\\|e^k\\|_A = \\sqrt{2^{1-k}} = (2^{1-k})^{\\frac{1}{2}} = 2^{\\frac{1-k}{2}}\n$$", "answer": "$$ \\boxed{2^{\\frac{1-k}{2}}} $$", "id": "3615421"}, {"introduction": "Geophysical models often feature high-contrast material properties, such as abrupt changes in subsurface conductivity, which can lead to linear systems that are challenging for basic iterative solvers. The standard Jacobi and Gauss-Seidel methods may converge very slowly in these situations. This practice explores a powerful technique to accelerate convergence: introducing a relaxation parameter $\\omega$ to the Gauss-Seidel iteration. By analyzing a discretized 1D diffusion problem modeling a layered medium, you will derive the optimal value of $\\omega$ that minimizes the spectral radius of the iteration matrix, thereby maximizing the convergence speed [@problem_id:3615391]. This exercise provides direct, hands-on experience with solver optimization and serves as a stepping stone to understanding more advanced methods like Successive Over-Relaxation (SOR).", "problem": "Consider the one-dimensional steady diffusion equation $-\\frac{d}{dx}\\left(\\kappa(x)\\frac{du}{dx}\\right)=f(x)$ on the interval $[0,1]$ with homogeneous Dirichlet boundary conditions $u(0)=0$ and $u(1)=0$. The conductivity is layered: $\\kappa(x)=\\kappa_{1}$ for $x\\in[0,0.5]$ and $\\kappa(x)=\\kappa_{2}$ for $x\\in(0.5,1]$. Discretize the operator using a uniform grid with interior nodes at $x_{1}=\\frac{1}{3}$ and $x_{2}=\\frac{2}{3}$, yielding a linear system $A\\mathbf{u}=\\mathbf{b}$ with $\\mathbf{u}=\\begin{pmatrix}u_{1} \\\\ u_{2}\\end{pmatrix}$. Use second-order finite differences with harmonic averaging of $\\kappa$ at the half-points $x_{1/2}$, $x_{3/2}$, and $x_{5/2}$:\n$$\n\\kappa_{i+1/2}=\\left(\\frac{1}{\\Delta x}\\int_{x_{i}}^{x_{i+1}}\\frac{dx}{\\kappa(x)}\\right)^{-1},\n$$\nwith $\\Delta x=\\frac{1}{3}$, and treat contributions from the Dirichlet boundaries as known terms in $\\mathbf{b}$. Denote $\\alpha_{L}=\\frac{\\kappa_{1}}{\\Delta x^{2}}$, $\\alpha_{R}=\\frac{\\kappa_{2}}{\\Delta x^{2}}$, and $\\beta=\\frac{\\kappa_{3/2}}{\\Delta x^{2}}$, and assemble the $2\\times 2$ matrix $A$ with diagonal entries $a=\\alpha_{L}+\\beta$ and $b=\\beta+\\alpha_{R}$ and off-diagonal entries $A_{12}=A_{21}=-\\beta$.\n\nStationary iterations are built from the splitting $A=D-L-U$, with $D$ the diagonal of $A$, $L$ the strictly lower triangular part, and $U$ the strictly upper triangular part. The damped Gauss–Seidel method uses the update rule:\n$$\n\\mathbf{u}^{(k+1)}=\\mathbf{u}^{(k)}-\\omega\\,(D-L)^{-1}\\left(A\\mathbf{u}^{(k)}-\\mathbf{b}\\right),\n$$\nwith relaxation parameter $\\omega\\in(0,2)$. A related method, Successive Over-Relaxation (SOR), can be formulated with the update:\n$$\n\\mathbf{u}^{(k+1)}=\\mathbf{u}^{(k)}+\\left(D-\\omega L\\right)^{-1}\\left((1-\\omega)D+\\omega U\\right)\\left(\\mathbf{u}^{(k)}-\\mathbf{u}^{(k-1)}\\right)\n$$\nIn computational geophysics, high-contrast layered media produce slow-to-damp error modes tied to interface physics.\n\nAssume, for the above $2\\times 2$ discretization, that the error propagation of the damped Gauss–Seidel method is dominated by two eigenmodes of the Gauss–Seidel iteration matrix, and that the optimal $\\omega$ is obtained by minimizing the maximum magnitude of the two mapped error factors under damping. Starting from the definitions of $A=D-L-U$ and the iteration matrices, derive a closed-form analytic expression for the optimal relaxation parameter $\\omega_{\\mathrm{opt}}$ under a two-eigenvalue approximation, and then evaluate $\\omega_{\\mathrm{opt}}$ for the layered case $\\kappa_{1}=100$ and $\\kappa_{2}=1$. Round your final numeric value for $\\omega_{\\mathrm{opt}}$ to four significant figures. No units.", "solution": "The problem statement has been validated and is determined to be valid. It presents a well-posed question in the field of numerical analysis applied to computational geophysics. The problem is scientifically grounded, self-contained, and objective. While a non-standard formulation for the Successive Over-Relaxation (SOR) method is mentioned, it is extraneous to the core task. The problem explicitly directs the derivation based on a well-defined damped Gauss-Seidel iteration and a two-eigenvalue approximation, which is a legitimate and common simplification in advanced numerical methods.\n\nThe problem asks for the optimal relaxation parameter $\\omega_{\\mathrm{opt}}$ for a damped Gauss-Seidel method applied to a $2 \\times 2$ linear system arising from the discretization of a steady-state diffusion equation.\n\nFirst, we construct the system matrix $A$. The one-dimensional diffusion equation is $-\\frac{d}{dx}\\left(\\kappa(x)\\frac{du}{dx}\\right)=f(x)$. We use a centered finite difference scheme on a uniform grid with spacing $\\Delta x = \\frac{1}{3}$. The nodes are $x_0=0$, $x_1=\\frac{1}{3}$, $x_2=\\frac{2}{3}$, and $x_3=1$. The discretization at an interior node $x_i$ is:\n$$\n-\\frac{1}{\\Delta x} \\left( \\kappa_{i+1/2} \\frac{u_{i+1}-u_i}{\\Delta x} - \\kappa_{i-1/2} \\frac{u_i-u_{i-1}}{\\Delta x} \\right) = f_i\n$$\nRearranging this gives the $i$-th row of the linear system:\n$$\n\\frac{1}{\\Delta x^2} \\left[ -\\kappa_{i-1/2} u_{i-1} + (\\kappa_{i-1/2} + \\kappa_{i+1/2}) u_i - \\kappa_{i+1/2} u_{i+1} \\right] = f_i\n$$\nWe need to calculate the harmonically averaged conductivities $\\kappa_{i+1/2}$ at the cell faces $x_{1/2}=\\frac{1}{6}$, $x_{3/2}=\\frac{1}{2}$, and $x_{5/2}=\\frac{5}{6}$. The conductivity $\\kappa(x)$ is piecewise constant: $\\kappa(x)=\\kappa_1$ for $x \\in [0, 0.5]$ and $\\kappa(x)=\\kappa_2$ for $x \\in (0.5, 1]$.\n\nFor the interval $[x_0, x_1] = [0, \\frac{1}{3}]$, $\\kappa(x) = \\kappa_1$ is constant. The harmonic average is simply $\\kappa_1$:\n$$\n\\kappa_{1/2} = \\left(\\frac{1}{\\Delta x}\\int_{0}^{1/3}\\frac{dx}{\\kappa_1}\\right)^{-1} = \\left(\\frac{1}{1/3} \\frac{1/3}{\\kappa_1}\\right)^{-1} = \\kappa_1\n$$\nSimilarly, for the interval $[x_2, x_3] = [\\frac{2}{3}, 1]$, $\\kappa(x) = \\kappa_2$ is constant, so $\\kappa_{5/2} = \\kappa_2$.\n\nFor the interval $[x_1, x_2] = [\\frac{1}{3}, \\frac{2}{3}]$, the interface at $x=0.5$ is included. We apply the given formula for harmonic averaging:\n$$\n\\frac{1}{\\kappa_{3/2}} = \\frac{1}{\\Delta x} \\int_{1/3}^{2/3} \\frac{dx}{\\kappa(x)} = \\frac{1}{1/3} \\left[ \\int_{1/3}^{0.5} \\frac{dx}{\\kappa_1} + \\int_{0.5}^{2/3} \\frac{dx}{\\kappa_2} \\right]\n$$\n$$\n\\frac{1}{\\kappa_{3/2}} = 3 \\left[ \\frac{0.5 - 1/3}{\\kappa_1} + \\frac{2/3 - 0.5}{\\kappa_2} \\right] = 3 \\left[ \\frac{1/6}{\\kappa_1} + \\frac{1/6}{\\kappa_2} \\right] = \\frac{1}{2\\kappa_1} + \\frac{1}{2\\kappa_2}\n$$\nThus, $\\kappa_{3/2}$ is the harmonic mean of $\\kappa_1$ and $\\kappa_2$:\n$$\n\\kappa_{3/2} = \\left( \\frac{\\kappa_1+\\kappa_2}{2\\kappa_1\\kappa_2} \\right)^{-1} = \\frac{2\\kappa_1\\kappa_2}{\\kappa_1+\\kappa_2}\n$$\nNow, we write the two equations for the interior nodes $u_1=u(x_1)$ and $u_2=u(x_2)$, using the homogeneous Dirichlet boundary conditions $u_0 = u(0) = 0$ and $u_3=u(1)=0$.\nFor $i=1$:\n$$ \\frac{1}{\\Delta x^2} [(\\kappa_{1/2} + \\kappa_{3/2})u_1 - \\kappa_{3/2}u_2] = f_1 \\implies \\frac{1}{\\Delta x^2} [(\\kappa_1 + \\kappa_{3/2})u_1 - \\kappa_{3/2}u_2] = f_1 $$\nFor $i=2$:\n$$ \\frac{1}{\\Delta x^2} [-\\kappa_{3/2}u_1 + (\\kappa_{3/2} + \\kappa_{5/2})u_2] = f_2 \\implies \\frac{1}{\\Delta x^2} [-\\kappa_{3/2}u_1 + (\\kappa_{3/2} + \\kappa_2)u_2] = f_2 $$\nThe system matrix $A$ in $A\\mathbf{u}=\\mathbf{b}$ is:\n$$\nA = \\frac{1}{\\Delta x^2} \\begin{pmatrix} \\kappa_1 + \\kappa_{3/2}  -\\kappa_{3/2} \\\\ -\\kappa_{3/2}  \\kappa_2 + \\kappa_{3/2} \\end{pmatrix}\n$$\nUsing the problem's definitions $\\alpha_L = \\frac{\\kappa_1}{\\Delta x^2}$, $\\alpha_R = \\frac{\\kappa_2}{\\Delta x^2}$, and $\\beta = \\frac{\\kappa_{3/2}}{\\Delta x^2}$, the matrix becomes:\n$$\nA = \\begin{pmatrix} \\alpha_L + \\beta  -\\beta \\\\ -\\beta  \\alpha_R + \\beta \\end{pmatrix} = \\begin{pmatrix} a  -\\beta \\\\ -\\beta  b \\end{pmatrix}\n$$\nwhere $a = \\alpha_L + \\beta$ and $b = \\alpha_R + \\beta$. This matches the problem description.\n\nNext, we analyze the damped Gauss-Seidel method. The iteration is $\\mathbf{u}^{(k+1)}=\\mathbf{u}^{(k)}-\\omega\\,(D-L)^{-1}\\left(A\\mathbf{u}^{(k)}-\\mathbf{b}\\right)$. The error $\\mathbf{e}^{(k)}=\\mathbf{u}^{(k)}-\\mathbf{u}$ propagates according to $\\mathbf{e}^{(k+1)}=T_{dGS}\\mathbf{e}^{(k)}$, where the iteration matrix is:\n$$\nT_{dGS} = I - \\omega(D-L)^{-1}A\n$$\nUsing the splitting $A=D-L-U$, this becomes:\n$$\nT_{dGS} = I - \\omega(D-L)^{-1}(D-L-U) = I - \\omega(I - (D-L)^{-1}U) = (1-\\omega)I + \\omega T_{GS}\n$$\nwhere $T_{GS}=(D-L)^{-1}U$ is the standard Gauss-Seidel iteration matrix.\nThe eigenvalues $\\mu$ of $T_{dGS}$ are related to the eigenvalues $\\lambda$ of $T_{GS}$ by the mapping $\\mu = (1-\\omega) + \\omega\\lambda$.\n\nWe find the eigenvalues of $T_{GS}$. The components of the splitting are:\n$$\nD = \\begin{pmatrix} a  0 \\\\ 0  b \\end{pmatrix}, \\quad L = \\begin{pmatrix} 0  0 \\\\ \\beta  0 \\end{pmatrix}, \\quad U = \\begin{pmatrix} 0  \\beta \\\\ 0  0 \\end{pmatrix}\n$$\nSo, $D-L = \\begin{pmatrix} a  0 \\\\ -\\beta  b \\end{pmatrix}$ and its inverse is $(D-L)^{-1} = \\frac{1}{ab}\\begin{pmatrix} b  0 \\\\ \\beta  a \\end{pmatrix}$.\n$$\nT_{GS} = (D-L)^{-1}U = \\frac{1}{ab}\\begin{pmatrix} b  0 \\\\ \\beta  a \\end{pmatrix} \\begin{pmatrix} 0  \\beta \\\\ 0  0 \\end{pmatrix} = \\frac{1}{ab}\\begin{pmatrix} 0  b\\beta \\\\ 0  \\beta^2 \\end{pmatrix} = \\begin{pmatrix} 0  \\beta/a \\\\ 0  \\beta^2/(ab) \\end{pmatrix}\n$$\nThe eigenvalues of this upper triangular matrix are its diagonal entries: $\\lambda_1 = 0$ and $\\lambda_2 = \\frac{\\beta^2}{ab}$.\nLet $\\lambda_{max} = \\lambda_2 = \\frac{\\beta^2}{ab}$. Since $A$ is symmetric positive definite, we know that $0 \\le \\lambda_{max}  1$.\n\nThe eigenvalues of the damped Gauss-Seidel matrix $T_{dGS}$ are:\n$$\n\\mu_1 = (1-\\omega) + \\omega\\lambda_1 = 1-\\omega\n$$\n$$\n\\mu_2 = (1-\\omega) + \\omega\\lambda_2 = 1-\\omega + \\omega\\lambda_{max}\n$$\nThe goal is to find the optimal relaxation parameter $\\omega_{\\mathrm{opt}}$ that minimizes the spectral radius $\\rho(T_{dGS}) = \\max(|\\mu_1|, |\\mu_2|)$. We need to solve:\n$$\n\\min_{\\omega \\in (0,2)} \\max(|1-\\omega|, |1-\\omega + \\omega\\lambda_{max}|)\n$$\nThe minimum value of the maximum of two functions typically occurs where their magnitudes are equal:\n$$\n|1-\\omega| = |1-\\omega(1-\\lambda_{max})|\n$$\nSince $\\lambda_{max}  1$ and we seek $\\omega  0$, we consider the case where the arguments have opposite signs, as this balances the convergence rates of the two modes:\n$$\n1 - \\omega = -(1-\\omega + \\omega\\lambda_{max}) = -1+\\omega - \\omega\\lambda_{max}\n$$\n$$\n2 = 2\\omega - \\omega\\lambda_{max} = \\omega(2-\\lambda_{max})\n$$\nThis gives the optimal relaxation parameter:\n$$\n\\omega_{\\mathrm{opt}} = \\frac{2}{2-\\lambda_{max}}\n$$\nNow we compute the numerical value. Given $\\kappa_1=100$, $\\kappa_2=1$, and $\\Delta x = \\frac{1}{3}$.\n$$\n\\Delta x^2 = \\frac{1}{9}\n$$\n$$\n\\kappa_{3/2} = \\frac{2(100)(1)}{100+1} = \\frac{200}{101}\n$$\nThe parameters are:\n$$\n\\alpha_L = \\frac{\\kappa_1}{\\Delta x^2} = \\frac{100}{1/9} = 900\n$$\n$$\n\\alpha_R = \\frac{\\kappa_2}{\\Delta x^2} = \\frac{1}{1/9} = 9\n$$\n$$\n\\beta = \\frac{\\kappa_{3/2}}{\\Delta x^2} = \\frac{200/101}{1/9} = \\frac{1800}{101}\n$$\nThe diagonal matrix entries are:\n$$\na = \\alpha_L + \\beta = 900 + \\frac{1800}{101} = \\frac{90900+1800}{101} = \\frac{92700}{101}\n$$\n$$\nb = \\alpha_R + \\beta = 9 + \\frac{1800}{101} = \\frac{909+1800}{101} = \\frac{2709}{101}\n$$\nWe can now calculate $\\lambda_{max}$:\n$$\n\\lambda_{max} = \\frac{\\beta^2}{ab} = \\frac{(\\frac{1800}{101})^2}{(\\frac{92700}{101})(\\frac{2709}{101})} = \\frac{1800^2}{92700 \\cdot 2709} = \\frac{3240000}{251148300} = \\frac{400}{31003}\n$$\nFinally, we compute $\\omega_{\\mathrm{opt}}$:\n$$\n\\omega_{\\mathrm{opt}} = \\frac{2}{2 - \\lambda_{max}} = \\frac{2}{2 - \\frac{400}{31003}} = \\frac{2}{\\frac{2 \\times 31003 - 400}{31003}} = \\frac{2 \\times 31003}{62006 - 400} = \\frac{62006}{61606}\n$$\nThe numerical value is:\n$$\n\\omega_{\\mathrm{opt}} \\approx 1.0064928...\n$$\nRounding to four significant figures gives $1.006$.", "answer": "$$\\boxed{1.006}$$", "id": "3615391"}]}