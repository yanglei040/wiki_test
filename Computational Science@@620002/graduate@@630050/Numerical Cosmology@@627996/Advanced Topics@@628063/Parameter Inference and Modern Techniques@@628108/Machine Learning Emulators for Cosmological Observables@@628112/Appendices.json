{"hands_on_practices": [{"introduction": "Before embarking on an expensive simulation campaign, a crucial first step is to estimate the number of samples required. This exercise provides a foundational understanding of sample complexity by examining Polynomial Chaos Expansions (PCE), a type of surrogate model where the minimum number of required simulations can be determined analytically. By deriving this number, you will gain a concrete appreciation for the \"curse of dimensionality\" and explore the fundamental trade-offs between model complexity, parameter space dimension, and simulation cost, contrasting the rigid requirements of PCE with the flexibility of Gaussian Processes [@problem_id:3478376].", "problem": "You are building a machine learning emulator for a cosmological observable $y(\\boldsymbol{\\theta})$ (for example, the nonlinear matter power spectrum at fixed wavenumbers) defined on a bounded parameter domain $\\Theta \\subset \\mathbb{R}^{d}$, where $\\boldsymbol{\\theta} \\in \\Theta$ encodes $d$ cosmological parameters. You decide to fit a Polynomial Chaos Expansion (PCE) surrogate of total order $p$ using a non-intrusive linear regression on $N$ model evaluations (high-fidelity simulations) at distinct design points in $\\Theta$. Assume the input measure on $\\Theta$ is absolutely continuous and bounded so that an orthonormal basis of multivariate polynomials $\\{\\Psi_{\\boldsymbol{\\alpha}}(\\boldsymbol{\\theta})\\}$ indexed by multi-indices $\\boldsymbol{\\alpha} \\in \\mathbb{N}_{0}^{d}$ exists, and you truncate the expansion to all multi-indices with total degree $\\|\\boldsymbol{\\alpha}\\|_{1} \\le p$. Further assume noiseless evaluations, and that the design is such that the regression design matrix has full column rank whenever algebraically possible.\n\nStarting from the definition of the truncated total-order polynomial space and the linear-algebraic requirement for uniquely determining all coefficients in the truncated PCE via regression, derive the exact minimal number of simulations $N_{\\min}(d,p)$ required to uniquely identify all expansion coefficients in terms of $d$ and $p$.\n\nThen, still grounded in first principles, discuss the key trade-offs in sample complexity and computational cost when choosing this PCE design versus a Gaussian Process (GP) emulator with a stationary kernel in $d$ dimensions, including how each approach scales with $d$ and $p$, and implications for uncertainty quantification. Your discussion should be qualitative and justified from core definitions and widely used properties; do not invoke any specialized or pre-derived sampling formulas.\n\nProvide your final answer for $N_{\\min}(d,p)$ as a single closed-form expression. No numerical evaluation is required. Do not include units. Do not round.", "solution": "The problem statement is critically evaluated for validity before proceeding to a solution.\n\n### Step 1: Extract Givens\n- Cosmological observable: $y(\\boldsymbol{\\theta})$\n- Parameter domain: $\\boldsymbol{\\theta} \\in \\Theta \\subset \\mathbb{R}^{d}$\n- Emulator type: Polynomial Chaos Expansion (PCE) of total order $p$.\n- Fitting method: Non-intrusive linear regression on $N$ model evaluations.\n- Design points: $N$ distinct points in $\\Theta$.\n- Input measure: Absolutely continuous and bounded.\n- Polynomial basis: Orthonormal basis $\\{\\Psi_{\\boldsymbol{\\alpha}}(\\boldsymbol{\\theta})\\}$ with $\\boldsymbol{\\alpha} \\in \\mathbb{N}_{0}^{d}$.\n- Truncation rule: Total degree $\\|\\boldsymbol{\\alpha}\\|_{1} \\le p$.\n- Evaluations: Noiseless.\n- Design matrix: Assumed to have full column rank whenever algebraically possible.\n- First objective: Derive the exact minimal number of simulations $N_{\\min}(d,p)$.\n- Second objective: Discuss trade-offs in sample complexity and computational cost for PCE versus a Gaussian Process (GP) emulator, focusing on scaling with $d$ and $p$, and implications for uncertainty quantification.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is assessed against the validation criteria:\n- **Scientifically Grounded**: The problem is well-grounded in the fields of numerical cosmology, uncertainty quantification, and machine learning. Polynomial Chaos Expansions and Gaussian Processes are standard, state-of-the-art techniques for emulating complex computer models. The mathematical formulation is correct and standard.\n- **Well-Posed**: The problem is well-posed. The first part asks for the derivation of a specific quantity, $N_{\\min}(d,p)$, under clear and sufficient assumptions. The condition that the design matrix has full column rank is the key to making the problem of finding a minimal number of points solvable. The second part requests a qualitative but reasoned discussion based on established principles, which is a well-defined task.\n- **Objective**: The problem is stated in precise, objective, and technical language, free from ambiguity or subjective claims.\n- **Completeness and Consistency**: The problem provides all necessary information and definitions to proceed. The assumptions (e.g., total-order truncation, linear regression, full-rank matrix) are self-consistent and sufficient for the derivation.\n- **Feasibility and Realism**: The scenario described is a standard, albeit idealized, setup for constructing surrogate models. It is a fundamental problem in the field.\n\nThe problem statement exhibits none of the flaws listed in the validation checklist. It is scientifically sound, well-posed, objective, and formally specified.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A solution will be derived.\n\n### Derivation of $N_{\\min}(d,p)$\n\nThe Polynomial Chaos Expansion (PCE) surrogate model, $\\hat{y}(\\boldsymbol{\\theta})$, approximates the true model output $y(\\boldsymbol{\\theta})$. Given the specified truncation scheme, the expansion includes all multivariate polynomials $\\Psi_{\\boldsymbol{\\alpha}}(\\boldsymbol{\\theta})$ where the multi-index $\\boldsymbol{\\alpha} = (\\alpha_1, \\alpha_2, \\ldots, \\alpha_d) \\in \\mathbb{N}_0^d$ satisfies the condition that its $L_1$-norm (total degree) is at most $p$:\n$$ \\hat{y}(\\boldsymbol{\\theta}) = \\sum_{\\boldsymbol{\\alpha} \\in \\mathcal{A}} c_{\\boldsymbol{\\alpha}} \\Psi_{\\boldsymbol{\\alpha}}(\\boldsymbol{\\theta}) \\quad \\text{where} \\quad \\mathcal{A} = \\left\\{ \\boldsymbol{\\alpha} \\in \\mathbb{N}_0^d : \\|\\boldsymbol{\\alpha}\\|_1 = \\sum_{i=1}^d \\alpha_i \\le p \\right\\} $$\nThe unknown quantities to be determined are the coefficients $\\{c_{\\boldsymbol{\\alpha}}\\}_{\\boldsymbol{\\alpha} \\in \\mathcal{A}}$. To find the minimal number of simulations required to uniquely determine these coefficients, we must first find the total number of such coefficients. Let this number be $K = |\\mathcal{A}|$.\n\nThe problem of finding $K$ is a combinatorial one: counting the number of non-negative integer solutions to the inequality $\\alpha_1 + \\alpha_2 + \\ldots + \\alpha_d \\le p$. By introducing a non-negative slack variable $s$, we can convert this inequality into an equation:\n$$ \\alpha_1 + \\alpha_2 + \\ldots + \\alpha_d + s = p $$\nThis is a classic \"stars and bars\" problem. We are looking for the number of ways to partition $p$ (the \"stars\") into $d+1$ non-negative integer bins (the variables $\\alpha_1, \\ldots, \\alpha_d, s$). This is equivalent to arranging $p$ stars and $(d+1)-1 = d$ bars. The total number of arrangements, and thus the number of solutions, is given by the binomial coefficient:\n$$ K = \\binom{p + (d+1) - 1}{(d+1) - 1} = \\binom{p+d}{d} $$\nSo, there are $K = \\binom{p+d}{d}$ coefficients to be determined.\n\nThe coefficients are found via non-intrusive linear regression using $N$ simulation runs. We have $N$ input-output pairs $(\\boldsymbol{\\theta}_j, y_j)$ for $j=1, \\ldots, N$. This sets up a system of $N$ linear equations:\n$$ y_j = \\sum_{\\boldsymbol{\\alpha} \\in \\mathcal{A}} c_{\\boldsymbol{\\alpha}} \\Psi_{\\boldsymbol{\\alpha}}(\\boldsymbol{\\theta}_j), \\quad j=1, \\ldots, N $$\nThis can be written in matrix form as $\\mathbf{y} = \\mathbf{\\Psi c}$, where:\n- $\\mathbf{y}$ is an $N \\times 1$ column vector of the observed simulation outputs.\n- $\\mathbf{c}$ is a $K \\times 1$ column vector of the unknown PCE coefficients.\n- $\\mathbf{\\Psi}$ is the $N \\times K$ design matrix, with entries $\\Psi_{j, \\boldsymbol{\\alpha}} = \\Psi_{\\boldsymbol{\\alpha}}(\\boldsymbol{\\theta}_j)$.\n\nFor the vector of coefficients $\\mathbf{c}$ to be \"uniquely identified\", the linear system $\\mathbf{y} = \\mathbf{\\Psi c}$ must have a unique solution. A fundamental result from linear algebra states that for an $N \\times K$ system, a unique solution for $\\mathbf{c}$ exists if and only if the matrix $\\mathbf{\\Psi}$ has full column rank, which requires that the number of rows is greater than or equal to the number of columns, i.e., $N \\ge K$. If $N  K$, the system is underdetermined, and there are infinitely many solutions for $\\mathbf{c}$.\n\nThe problem states to assume the design (the choice of points $\\boldsymbol{\\theta}_j$) is such that $\\mathbf{\\Psi}$ has full column rank whenever it is algebraically possible. The minimal value of $N$ for which full column rank is possible is $N=K$. Therefore, the minimal number of simulations required to uniquely determine all $K$ coefficients is:\n$$ N_{\\min}(d, p) = K = \\binom{p+d}{d} $$\n\n### Discussion of PCE vs. GP Emulator Trade-offs\n\nHerein, we discuss the trade-offs between the Polynomial Chaos Expansion (as designed above) and a Gaussian Process emulator.\n\n**Sample Complexity and Scaling:**\n\n- **PCE:** The sample complexity is rigidly determined by the model structure. As derived, the minimum number of samples is $N_{\\min} = \\binom{p+d}{d}$. For a fixed polynomial order $p$, this number grows polynomially with the dimension $d$ as $N_{\\min} \\propto d^p$ for large $d$. For a fixed dimension $d$, it grows polynomially with the order $p$ as $N_{\\min} \\propto p^d$ for large $p$. This rapid growth, often termed the \"curse of dimensionality,\" makes total-order PCEs impractical for problems with even moderate dimension ($d \\gtrsim 10$) and order ($p \\gtrsim 3$), as the required number of high-fidelity simulations becomes prohibitively large.\n- **GP:** A GP does not have a hard lower bound on the number of samples $N$ based on its structural parameters. A GP model can be constructed with any number of points $N > 0$. The quality of the emulator (e.g., its accuracy) improves as $N$ increases. However, GPs also suffer from the curse of dimensionality, as the parameter space volume grows exponentially with $d$. To maintain a given sampling density, $N$ must grow exponentially, meaning that for a fixed $N$ in high dimensions, the data points become very sparse. This can degrade the performance of standard stationary kernels (e.g., squared exponential), which may struggle to learn correlations between distant points.\n\n**Computational Cost:**\n\n- **PCE:** The primary computational cost in fitting the PCE via linear regression is solving the normal equations $(\\mathbf{\\Psi}^T \\mathbf{\\Psi}) \\mathbf{c} = \\mathbf{\\Psi}^T \\mathbf{y}$. This involves the inversion of the $K \\times K$ matrix $\\mathbf{\\Psi}^T \\mathbf{\\Psi}$, which scales as $O(K^3)$, where $K = \\binom{p+d}{d}$. Thus, the training cost scales severely with both dimension $d$ and order $p$. However, once the coefficients $\\mathbf{c}$ are computed, making a new prediction is extremely fast, involving only the evaluation of a polynomial sum of $K$ terms, which scales as $O(K)$.\n- **GP:** The primary computational cost in training a standard GP is the inversion of the $N \\times N$ covariance matrix of the training data, which scales as $O(N^3)$. This cost depends on the number of training points $N$, not directly on the dimension $d$. For large datasets ($N \\gg 1000$), this becomes the main bottleneck. Prediction at a new point requires matrix-vector operations, scaling as $O(N)$ for the mean and $O(N^2)$ for the variance. Thus, GP prediction is computationally more expensive than PCE prediction.\n\n**Implications for Uncertainty Quantification (UQ):**\n\n- **PCE:** In the non-intrusive regression setting described, the PCE provides a deterministic point estimate $\\hat{y}(\\boldsymbol{\\theta})$. It does not inherently provide a measure of its own predictive (epistemic) uncertainty. To quantify the confidence in the PCE model itself, one would need to use external methods like bootstrapping or cross-validation. However, PCEs excel at propagating *input* (aleatoric) uncertainty. If the input parameters $\\boldsymbol{\\theta}$ are described by a probability distribution, the orthonormal property of the basis polynomials allows for the analytical computation of the statistical moments (e.g., mean, variance) of the output $\\hat{y}$. The variance is simply the sum of the squares of the coefficients (excluding the constant term), $\\text{Var}[\\hat{y}] = \\sum_{\\boldsymbol{\\alpha} \\in \\mathcal{A}, \\boldsymbol{\\alpha} \\neq \\mathbf{0}} c_{\\boldsymbol{\\alpha}}^2$.\n- **GP:** UQ is a native and principal feature of the GP framework. A GP is a probabilistic model that returns a full posterior predictive distribution (a Gaussian) for any new input point $\\boldsymbol{\\theta}_{\\text{new}}$. This distribution is characterized by a predictive mean (the emulation) and a predictive variance. This variance is a measure of the epistemic uncertainty, which naturally increases in regions of the parameter space that are sparsely populated by training data. This makes GPs particularly powerful for applications like active learning or Bayesian optimization, where principled exploration of the parameter space is required.\n\nIn summary, the choice between PCE and GP involves a trade-off between the rigid, global structure of PCE with its severe sample requirements in high dimensions but fast predictions, and the flexible, non-parametric nature of GPs with their built-in UQ but high computational cost for large $N$.", "answer": "$$\\boxed{\\binom{p+d}{d}}$$", "id": "3478376"}, {"introduction": "While theoretical estimates of sample complexity are insightful, real-world emulator design often relies on a more practical, data-driven approach. In this exercise, you will develop a tool to plan a simulation campaign by modeling the emulator's error as a function of the training set size, a concept known as a \"learning curve.\" By fitting a model to empirical performance data, you can predict the optimal number of simulations needed to achieve a desired accuracy, a core skill for planning efficient and cost-effective research projects in computational cosmology [@problem_id:3478390].", "problem": "You are tasked with designing a complete, runnable program that determines optimal Latin hypercube sample sizes for machine learning emulators of cosmological observables, using empirical learning curves and verifying scaling behavior across observables and parameter-space dimensions. The setting is numerical cosmology at the advanced graduate level, where the program must use principled modeling of learning curves from first-principles considerations.\n\nConsider a parameter vector $\\theta \\in \\mathbb{R}^d$ with dimension $d$, and an emulator for a cosmological observable $O$ trained on a Latin hypercube design of size $N$. In many cases, the generalization error of emulators built with smooth kernels over well-behaved cosmological response surfaces can be modeled as a power law in the effective fill distance $h$ of the design. For Latin hypercube sampling in $d$ dimensions, $h$ scales as $h \\propto N^{-1/d}$. Under smoothness assumptions and well-tested convergence properties of kernel regressors and Gaussian process emulators, the emulator error for observable $O$ may be modeled by a learning curve\n$$\n\\epsilon_O(N; d) \\approx \\epsilon_{\\mathrm{floor}}(O) + c(O)\\, N^{-\\alpha_O(d)},\n$$\nwhere $c(O)  0$ is a constant reflecting the calibration difficulty of $O$, $\\epsilon_{\\mathrm{floor}}(O) \\ge 0$ is an irreducible floor due to numerical noise and model mismatch, and $\\alpha_O(d)  0$ is an effective exponent that captures smoothness and dimensionality effects. All errors $\\epsilon$ are dimensionless.\n\nFor this problem, you will use synthetic but physically plausible empirical learning curves constructed from the above model with a small deterministic perturbation to mimic mild heteroscedasticity. For each observable $O \\in \\{\\mathrm{Pk}, \\mathrm{Cl}, \\mathrm{B}\\}$ and dimension $d \\in \\{3,5,8\\}$, the empirical error values at training sizes $N \\in \\{32, 64, 128, 256, 512\\}$ are given by\n$$\n\\epsilon_O(N; d) = \\epsilon_{\\mathrm{floor}}(O) + c(O)\\, N^{-\\alpha_O(d)} + 0.0005 \\frac{d}{N},\n$$\nwith observable-specific constants\n$$\nc(\\mathrm{Pk}) = 0.8,\\quad \\epsilon_{\\mathrm{floor}}(\\mathrm{Pk}) = 0.008,\n$$\n$$\nc(\\mathrm{Cl}) = 0.5,\\quad \\epsilon_{\\mathrm{floor}}(\\mathrm{Cl}) = 0.006,\n$$\n$$\nc(\\mathrm{B}) = 1.2,\\quad \\epsilon_{\\mathrm{floor}}(\\mathrm{B}) = 0.015,\n$$\nand dimension-dependent exponents\n$$\n\\alpha_{\\mathrm{Pk}}(d) = \\frac{0.6}{1 + 0.08 d},\\quad\n\\alpha_{\\mathrm{Cl}}(d) = \\frac{0.75}{1 + 0.05 d},\\quad\n\\alpha_{\\mathrm{B}}(d) = \\frac{0.55}{1 + 0.10 d}.\n$$\n\nYour program must:\n1. For each requested observable $O$ and dimension $d$, treat the above values at $N \\in \\{32,64,128,256,512\\}$ as empirical learning curve data and fit the model\n$$\n\\hat{\\epsilon}_O(N; d) = \\hat{\\epsilon}_{\\mathrm{floor}}(O,d) + \\hat{c}(O,d)\\, N^{-\\hat{\\alpha}(O,d)}\n$$\nusing nonlinear least squares to estimate $\\hat{\\epsilon}_{\\mathrm{floor}}(O,d)$, $\\hat{c}(O,d)$, and $\\hat{\\alpha}(O,d)$ from the empirical data. The fit must respect the physical constraints $\\hat{\\epsilon}_{\\mathrm{floor}} \\ge 0$, $\\hat{c}  0$, and $\\hat{\\alpha}  0$.\n\n2. Given a target error threshold $\\epsilon_{\\mathrm{target}}  0$, derive from first principles the minimal Latin hypercube size $N_{\\mathrm{opt}}(O,d;\\epsilon_{\\mathrm{target}})$ needed so that the fitted learning curve satisfies $\\hat{\\epsilon}_O(N_{\\mathrm{opt}}; d) \\le \\epsilon_{\\mathrm{target}}$. The derivation should use the fitted model and the rule that sample sizes are integers, returning\n$$\nN_{\\mathrm{opt}}(O,d;\\epsilon_{\\mathrm{target}}) =\n\\left\\lceil \\left( \\frac{\\hat{c}(O,d)}{\\epsilon_{\\mathrm{target}} - \\hat{\\epsilon}_{\\mathrm{floor}}(O,d)} \\right)^{1/\\hat{\\alpha}(O,d)} \\right\\rceil,\n$$\nif $\\epsilon_{\\mathrm{target}}  \\hat{\\epsilon}_{\\mathrm{floor}}(O,d)$, and returning $\\infty$ if $\\epsilon_{\\mathrm{target}} \\le \\hat{\\epsilon}_{\\mathrm{floor}}(O,d)$.\n\n3. Validate scaling across observables and dimensions using fitted parameters and implied optimal sizes. Specifically, check whether $N_{\\mathrm{opt}}$ increases with $d$ for a fixed observable and $\\epsilon_{\\mathrm{target}}$ (reflecting the curse of dimensionality), whether smoother observables require fewer samples at fixed $d$ and $\\epsilon_{\\mathrm{target}}$, and whether the fitted $\\hat{\\alpha}(O,d)$ decreases with $d$ for rougher observables.\n\nImplement and run the following test suite. For each test case, compute the requested quantity and aggregate all results into a single output line as specified below.\n\n- Test case 1 (happy path): Compute $N_{\\mathrm{opt}}(\\mathrm{Pk}, d=5; \\epsilon_{\\mathrm{target}}=0.02)$ and return the integer.\n- Test case 2 (boundary condition near the irreducible floor): Compute $N_{\\mathrm{opt}}(\\mathrm{Pk}, d=5; \\epsilon_{\\mathrm{target}}=0.0060)$ and return the result; if impossible, return $\\infty$.\n- Test case 3 (alternate observable, lower dimension): Compute $N_{\\mathrm{opt}}(\\mathrm{Cl}, d=3; \\epsilon_{\\mathrm{target}}=0.015)$ and return the integer.\n- Test case 4 (rough observable, high dimension): Compute $N_{\\mathrm{opt}}(\\mathrm{B}, d=8; \\epsilon_{\\mathrm{target}}=0.03)$ and return the integer.\n- Test case 5 (dimension scaling validation): For $\\mathrm{Pk}$ at $\\epsilon_{\\mathrm{target}}=0.02$, check if $N_{\\mathrm{opt}}(d=8)  N_{\\mathrm{opt}}(d=3)$ and return a boolean.\n- Test case 6 (cross-observable scaling validation): At $d=5$ and $\\epsilon_{\\mathrm{target}}=0.02$, check if $N_{\\mathrm{opt}}(\\mathrm{Cl})  N_{\\mathrm{opt}}(\\mathrm{Pk})$ and return a boolean.\n- Test case 7 (exponent scaling validation for rough observable): Check if $\\hat{\\alpha}(\\mathrm{B}, d=8)  \\hat{\\alpha}(\\mathrm{B}, d=3)$ and return a boolean.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3,result4,result5,result6,result7]\"). The results must be in the exact order of the test cases above. All results must be of fundamental types: integers, floats, or booleans. If an optimal size is infinite, output \"inf\" by using the programming language’s representation of infinity.", "solution": "The problem requires the design of a program to determine the optimal training set size for machine learning emulators in cosmology. This is achieved by fitting a parametric learning curve model to synthetically generated data and then using the fitted model to calculate the sample size needed to reach a specified error threshold. The solution proceeds in three main steps: data generation, model fitting, and optimal size calculation.\n\nFirst, we generate the synthetic \"empirical\" data for the emulator error $\\epsilon_O(N; d)$ for each cosmological observable $O \\in \\{\\mathrm{Pk}, \\mathrm{Cl}, \\mathrm{B}\\}$, parameter-space dimension $d \\in \\{3, 5, 8\\}$, and a range of training set sizes $N \\in \\{32, 64, 128, 256, 512\\}$. The data are generated according to the model specified in the problem:\n$$\n\\epsilon_O(N; d) = \\epsilon_{\\mathrm{floor}}(O) + c(O)\\, N^{-\\alpha_O(d)} + 0.0005 \\frac{d}{N}\n$$\nThe observable-specific constants $c(O)$ and $\\epsilon_{\\mathrm{floor}}(O)$, and the dimension-dependent exponents $\\alpha_O(d)$, are given by the problem statement's definitions. This formula simulates a realistic learning curve with a dominant power-law decay, an irreducible error floor, and a small perturbation term that introduces slight heteroscedasticity and model mismatch.\n\nSecond, for each pair of observable $O$ and dimension $d$, we fit a three-parameter learning curve model to the generated data points. The model to be fitted is:\n$$\n\\hat{\\epsilon}_O(N; d) = \\hat{\\epsilon}_{\\mathrm{floor}}(O,d) + \\hat{c}(O,d)\\, N^{-\\hat{\\alpha}(O,d)}\n$$\nThe parameters to be estimated are the error floor $\\hat{\\epsilon}_{\\mathrm{floor}}(O,d)$, the amplitude $\\hat{c}(O,d)$, and the convergence exponent $\\hat{\\alpha}(O,d)$. This estimation is performed using nonlinear least squares, a standard technique for fitting parametric models to data. To ensure the physical viability of the model, the fitting process enforces the constraints $\\hat{\\epsilon}_{\\mathrm{floor}} \\ge 0$, $\\hat{c}  0$, and $\\hat{\\alpha}  0$. This is implemented using the `scipy.optimize.curve_fit` function with appropriate bounds on the parameters. To improve the stability and accuracy of the fit, we provide initial guesses for the parameters that are based on the true values used to generate the data.\n\nThird, once the model parameters $(\\hat{\\epsilon}_{\\mathrm{floor}}, \\hat{c}, \\hat{\\alpha})$ are estimated for a given $(O, d)$ pair, we can calculate the minimum sample size $N_{\\mathrm{opt}}$ required to achieve a target error $\\epsilon_{\\mathrm{target}}$. This is derived from the inequality:\n$$\n\\hat{\\epsilon}_O(N_{\\mathrm{opt}}; d) \\le \\epsilon_{\\mathrm{target}}\n$$\nSubstituting the fitted model, we have:\n$$\n\\hat{\\epsilon}_{\\mathrm{floor}} + \\hat{c}\\, N_{\\mathrm{opt}}^{-\\hat{\\alpha}} \\le \\epsilon_{\\mathrm{target}}\n$$\nA solution for $N_{\\mathrm{opt}}$ exists only if the target error is greater than the irreducible error floor, i.e., $\\epsilon_{\\mathrm{target}}  \\hat{\\epsilon}_{\\mathrm{floor}}$. If this condition is not met, the target error is unachievable, and the required sample size is effectively infinite, which we denote as $\\infty$. If $\\epsilon_{\\mathrm{target}}  \\hat{\\epsilon}_{\\mathrm{floor}}$, we solve for $N_{\\mathrm{opt}}$:\n$$\n\\hat{c}\\, N_{\\mathrm{opt}}^{-\\hat{\\alpha}} \\le \\epsilon_{\\mathrm{target}} - \\hat{\\epsilon}_{\\mathrm{floor}}\n$$\n$$\nN_{\\mathrm{opt}}^{-\\hat{\\alpha}} \\le \\frac{\\epsilon_{\\mathrm{target}} - \\hat{\\epsilon}_{\\mathrm{floor}}}{\\hat{c}}\n$$\nSince $N  0$ and $\\hat{\\alpha}  0$, we can raise both sides to the power of $-1/\\hat{\\alpha}$, which reverses the inequality:\n$$\nN_{\\mathrm{opt}} \\ge \\left( \\frac{\\hat{c}}{\\epsilon_{\\mathrm{target}} - \\hat{\\epsilon}_{\\mathrm{floor}}} \\right)^{1/\\hat{\\alpha}}\n$$\nThe minimal sample size must be an integer. Therefore, we take the ceiling of the right-hand side to find the smallest integer $N$ that satisfies the condition:\n$$\nN_{\\mathrm{opt}}(O,d;\\epsilon_{\\mathrm{target}}) = \\left\\lceil \\left( \\frac{\\hat{c}(O,d)}{\\epsilon_{\\mathrm{target}} - \\hat{\\epsilon}_{\\mathrm{floor}}(O,d)} \\right)^{1/\\hat{\\alpha}(O,d)} \\right\\rceil\n$$\nThis principled derivation provides the basis for the program's core calculation. The program implements this entire workflow—data generation, fitting, and calculation—and applies it to the specified test cases. To handle multiple requests for the same $(O, d)$ pair efficiently, the fitted parameters are cached. The validation test cases involve comparing the computed $N_{\\mathrm{opt}}$ values or the fitted $\\hat{\\alpha}$ parameters across different dimensions and observables to verify expected scaling behaviors, such as the curse of dimensionality.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import curve_fit\n\ndef solve():\n    \"\"\"\n    Computes optimal Latin hypercube sample sizes for cosmological emulators\n    by fitting learning curves and evaluating a series of test cases.\n    \"\"\"\n    # Define constants and functions from the problem statement.\n    CONSTANTS = {\n        'Pk': {'c': 0.8, 'eps_floor': 0.008},\n        'Cl': {'c': 0.5, 'eps_floor': 0.006},\n        'B':  {'c': 1.2, 'eps_floor': 0.015},\n    }\n\n    ALPHA_FUNCS = {\n        'Pk': lambda d: 0.6 / (1.0 + 0.08 * d),\n        'Cl': lambda d: 0.75 / (1.0 + 0.05 * d),\n        'B':  lambda d: 0.55 / (1.0 + 0.10 * d),\n    }\n\n    N_VALUES = np.array([32, 64, 128, 256, 512], dtype=float)\n\n    # Cache for fitted parameters to avoid re-computation\n    fitted_params_cache = {}\n\n    def generate_empirical_data(observable, dimension):\n        \"\"\"Generates synthetic error data based on the provided model.\"\"\"\n        c = CONSTANTS[observable]['c']\n        eps_floor = CONSTANTS[observable]['eps_floor']\n        alpha = ALPHA_FUNCS[observable](dimension)\n        \n        # The given model for \"empirical\" data with a small perturbation\n        errors = eps_floor + c * N_VALUES**(-alpha) + 0.0005 * dimension / N_VALUES\n        return N_VALUES, errors\n\n    def fitting_model(N, eps_floor_hat, c_hat, alpha_hat):\n        \"\"\"The parametric model to be fitted to the data.\"\"\"\n        return eps_floor_hat + c_hat * N**(-alpha_hat)\n\n    def get_fitted_params(observable, dimension):\n        \"\"\"\n        Fits the learning curve model to the synthetic data for a given \n        observable and dimension. Caches the results.\n        \"\"\"\n        if (observable, dimension) in fitted_params_cache:\n            return fitted_params_cache[(observable, dimension)]\n\n        N_data, error_data = generate_empirical_data(observable, dimension)\n        \n        # Initial guess for the parameters using the true values to aid convergence\n        p0_guess = [\n            CONSTANTS[observable]['eps_floor'],\n            CONSTANTS[observable]['c'],\n            ALPHA_FUNCS[observable](dimension)\n        ]\n        \n        # Bounds for parameters: eps_floor = 0, c  0, alpha  0\n        # A small positive lower bound is used for strict positivity.\n        bounds = ([0.0, 1e-9, 1e-9], [np.inf, np.inf, np.inf])\n        \n        popt, _ = curve_fit(fitting_model, N_data, error_data, p0=p0_guess, bounds=bounds)\n        \n        fitted_params_cache[(observable, dimension)] = popt\n        return popt\n\n    def calculate_N_opt(observable, dimension, epsilon_target):\n        \"\"\"\n        Calculates the optimal sample size N_opt for a given target error.\n        \"\"\"\n        eps_floor_hat, c_hat, alpha_hat = get_fitted_params(observable, dimension)\n        \n        if epsilon_target = eps_floor_hat:\n            return np.inf\n        \n        base = c_hat / (epsilon_target - eps_floor_hat)\n        exponent = 1.0 / alpha_hat\n        N_val = base**exponent\n        \n        # Sample size must be an integer.\n        return int(np.ceil(N_val))\n\n    results = []\n\n    # Test case 1: Happy path\n    results.append(calculate_N_opt('Pk', 5, 0.02))\n\n    # Test case 2: Boundary condition near the irreducible floor\n    results.append(calculate_N_opt('Pk', 5, 0.0060))\n\n    # Test case 3: Alternate observable, lower dimension\n    results.append(calculate_N_opt('Cl', 3, 0.015))\n\n    # Test case 4: Rough observable, high dimension\n    results.append(calculate_N_opt('B', 8, 0.03))\n\n    # Test case 5: Dimension scaling validation\n    N_opt_pk_d8 = calculate_N_opt('Pk', 8, 0.02)\n    N_opt_pk_d3 = calculate_N_opt('Pk', 3, 0.02)\n    results.append(N_opt_pk_d8  N_opt_pk_d3)\n\n    # Test case 6: Cross-observable scaling validation\n    N_opt_cl_d5 = calculate_N_opt('Cl', 5, 0.02)\n    N_opt_pk_d5 = calculate_N_opt('Pk', 5, 0.02) # Value is cached from case 1\n    results.append(N_opt_cl_d5  N_opt_pk_d5)\n\n    # Test case 7: Exponent scaling validation for rough observable\n    _, _, alpha_b_d8 = get_fitted_params('B', 8)\n    _, _, alpha_b_d3 = get_fitted_params('B', 3)\n    results.append(alpha_b_d8  alpha_b_d3)\n\n    # Format the final output string as per requirements\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3478390"}, {"introduction": "The immense cost of high-fidelity simulations is a major bottleneck in cosmology. This practice introduces multi-fidelity emulation, a powerful strategy to mitigate this cost by synergistically combining a few expensive, high-accuracy simulations with many cheap, lower-accuracy ones. You will tackle an optimization problem using co-kriging, a multi-fidelity extension of Gaussian Processes, to determine the ideal allocation of computational resources that minimizes total runtime while satisfying a strict error tolerance, demonstrating a cutting-edge technique that makes large-scale scientific inquiries computationally feasible [@problem_id:3478372].", "problem": "You are tasked with designing and solving a multi-fidelity allocation problem for an emulator of the matter power spectrum $P(k)$ in numerical cosmology. The emulator combines low-resolution Particle Mesh (PM) simulations and high-resolution $N$-body simulations through a co-kriging model, following the auto-regressive formulation used in multi-fidelity Gaussian Process (GP) co-kriging. The goal is to meet a target predictive error constraint on the high-fidelity $P(k)$ at a single target wave number $k_\\star$ while minimizing total runtime. You must write a complete, runnable program that solves the allocation problem for a specified test suite and prints the required outputs in the exact format described below.\n\nModeling assumptions and fundamental base:\n- The multi-fidelity auto-regressive model is given by the Kennedy–O’Hagan structure $f_H(\\theta) = \\rho\\, f_L(\\theta) + \\delta(\\theta)$ at a single design point $\\theta_\\star$ aligned with $k_\\star$, where $f_H$ denotes the latent high-fidelity response of $P(k)$ at $k_\\star$, $f_L$ denotes the latent low-fidelity response, $\\rho$ is the cross-fidelity scaling factor, and $\\delta$ is the discrepancy process.\n- At the single location $\\theta_\\star$, treat $f_L$ and $\\delta$ as independent Gaussian random variables under a Gaussian Process prior restricted to the point: $f_L \\sim \\mathcal{N}(0, \\tau_L^2)$ and $\\delta \\sim \\mathcal{N}(0, \\tau_\\delta^2)$.\n- Observations are noisy and co-located at $\\theta_\\star$: each low-fidelity PM run returns $y^L_i = f_L + \\epsilon_i$ with $\\epsilon_i \\sim \\mathcal{N}(0, s_L^2)$ and each high-fidelity $N$-body run returns $y^H_j = \\rho\\, f_L + \\delta + \\eta_j$ with $\\eta_j \\sim \\mathcal{N}(0, s_H^2)$. All noises are independent and identically distributed across replicates and independent of $(f_L, \\delta)$.\n- The predictive target is the posterior standard deviation of the latent high-fidelity quantity $z \\equiv \\rho\\, f_L + \\delta$ at $\\theta_\\star$ under the joint linear-Gaussian model, after $n_L$ low-fidelity and $n_H$ high-fidelity runs. You must enforce the constraint that the posterior standard deviation of $z$ is at most the specified tolerance $\\epsilon$.\n- The runtime model is additive: total runtime $T$ is $T = n_L\\, t_L + n_H\\, t_H$, where $t_L$ and $t_H$ are the wall-clock times per PM and $N$-body run, respectively. All runtimes must be reported in $\\mathrm{s}$ (seconds).\n\nDesign goal:\n- For each test case in the suite below, choose nonnegative integers $n_L$ and $n_H$ to minimize $T$ subject to the predictive posterior standard deviation of $z$ being less than or equal to $\\epsilon$.\n\nWhat you must compute:\n- Use a first-principles derivation from linear-Gaussian Bayesian updating, starting with the auto-regressive co-kriging structure, to express the posterior variance of $z$ in terms of $(n_L, n_H)$ and the model hyperparameters $(\\rho, \\tau_L^2, \\tau_\\delta^2, s_L^2, s_H^2)$.\n- Determine feasibility of the constraint for given $(n_L, n_H)$ and derive a correct selection rule for optimal $(n_L, n_H)$ that minimizes $T$.\n- Your program must implement this derivation and optimization in a numerically stable and self-contained manner, without external input or files.\n\nTest suite:\n- For each test case, the parameters are $(\\rho, \\tau_L^2, \\tau_\\delta^2, s_L^2, s_H^2, t_L, t_H, \\epsilon)$ with units as applicable. All quantities in the test suite are dimensionless except $t_L$ and $t_H$, which must be treated in $\\mathrm{s}$.\n    1. Happy path with moderate coupling and tight tolerance: $\\rho = 0.9$, $\\tau_L^2 = 0.25$, $\\tau_\\delta^2 = 0.04$, $s_L^2 = 0.01$, $s_H^2 = 0.0025$, $t_L = 10\\,\\mathrm{s}$, $t_H = 200\\,\\mathrm{s}$, $\\epsilon = 0.05$.\n    2. High-fidelity expensive, weak coupling: $\\rho = 0.5$, $\\tau_L^2 = 0.36$, $\\tau_\\delta^2 = 0.09$, $s_L^2 = 0.02$, $s_H^2 = 0.01$, $t_L = 20\\,\\mathrm{s}$, $t_H = 800\\,\\mathrm{s}$, $\\epsilon = 0.15$.\n    3. Loose tolerance, zero-simulation feasible: $\\rho = 0.8$, $\\tau_L^2 = 0.01$, $\\tau_\\delta^2 = 0.005$, $s_L^2 = 0.01$, $s_H^2 = 0.01$, $t_L = 5\\,\\mathrm{s}$, $t_H = 100\\,\\mathrm{s}$, $\\epsilon = 0.12$.\n    4. Very tight tolerance requiring multiple high-fidelity runs: $\\rho = 0.95$, $\\tau_L^2 = 0.49$, $\\tau_\\delta^2 = 0.01$, $s_L^2 = 0.04$, $s_H^2 = 0.0025$, $t_L = 30\\,\\mathrm{s}$, $t_H = 300\\,\\mathrm{s}$, $\\epsilon = 0.02$.\n    5. Near-decoupled low fidelity, high-fidelity relatively cheap: $\\rho = 0.1$, $\\tau_L^2 = 1.0$, $\\tau_\\delta^2 = 0.25$, $s_L^2 = 0.1$, $s_H^2 = 0.01$, $t_L = 10\\,\\mathrm{s}$, $t_H = 50\\,\\mathrm{s}$, $\\epsilon = 0.3$.\n\nFinal output format:\n- For each test case, your program must compute the optimal nonnegative integers $n_L$ and $n_H$ and the minimal total runtime $T$ in $\\mathrm{s}$ rounded to three decimal places.\n- Your program should produce a single line of output containing the results for all test cases as a comma-separated list enclosed in square brackets, where each test case result is itself a list of the form $[n_L, n_H, T]$. For example: $[[n_L^{(1)}, n_H^{(1)}, T^{(1)}],[n_L^{(2)}, n_H^{(2)}, T^{(2)}],\\dots]$.\n\nAngle units do not apply. No percentages are used; any fraction-like quantities must appear as decimal numbers.\n\nYour program must be complete, runnable, and self-contained, using only the specified libraries and execution environment. No user input is permitted. Ensure scientific realism by adhering to the linear-Gaussian co-kriging assumptions and consistent units.", "solution": "The problem requires finding the optimal allocation of computational resources, specifically the number of low-fidelity Particle Mesh (PM) simulations, $n_L$, and high-fidelity $N$-body simulations, $n_H$, to minimize the total runtime $T = n_L t_L + n_H t_H$, while satisfying a constraint on the predictive uncertainty of the matter power spectrum, $P(k)$, at a specific wave number $k_\\star$.\n\nThe problem has been validated and is determined to be a well-posed, scientifically grounded optimization problem based on the established Kennedy-O'Hagan framework for multi-fidelity emulation. All necessary parameters are provided, and the problem is internally consistent. We can therefore proceed with a solution.\n\nThe solution process involves two main steps:\n1.  Derivation of the posterior variance of the high-fidelity quantity of interest as a function of $n_L$ and $n_H$.\n2.  Formulation and solution of the constrained integer optimization problem for $(n_L, n_H)$.\n\n### Step 1: Derivation of the Posterior Variance\n\nThe model is defined at a single design point $\\theta_\\star$. The latent low-fidelity response $f_L$ and the discrepancy term $\\delta$ are modeled as independent Gaussian random variables with priors:\n$$f_L \\sim \\mathcal{N}(0, \\tau_L^2)$$\n$$\\delta \\sim \\mathcal{N}(0, \\tau_\\delta^2)$$\nThe latent high-fidelity response is $z \\equiv f_H(\\theta_\\star) = \\rho f_L + \\delta$. We seek to find the posterior variance of $z$.\n\nAn observation from a low-fidelity simulation is $y_i^L = f_L + \\epsilon_i$ with noise $\\epsilon_i \\sim \\mathcal{N}(0, s_L^2)$. An observation from a high-fidelity simulation is $y_j^H = z + \\eta_j$ with noise $\\eta_j \\sim \\mathcal{N}(0, s_H^2)$. We have $n_L$ low-fidelity and $n_H$ high-fidelity observations.\n\nThis can be formulated as a Bayesian linear-Gaussian model. Let the state vector be $\\mathbf{x} = [f_L, \\delta]^T$. The prior on $\\mathbf{x}$ is $\\mathbf{x} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{\\Sigma}_0)$, where the prior covariance is:\n$$\\mathbf{\\Sigma}_0 = \\begin{pmatrix} \\tau_L^2  0 \\\\ 0  \\tau_\\delta^2 \\end{pmatrix}$$\nThe data consists of the sample means of the observations, which are sufficient statistics. For $n_L0$, $\\bar{y}_L = \\frac{1}{n_L}\\sum_{i=1}^{n_L} y_i^L = f_L + \\bar{\\epsilon}$, where $\\bar{\\epsilon} \\sim \\mathcal{N}(0, s_L^2/n_L)$. For $n_H0$, $\\bar{y}_H = \\frac{1}{n_H}\\sum_{j=1}^{n_H} y_j^H = \\rho f_L + \\delta + \\bar{\\eta}$, where $\\bar{\\eta} \\sim \\mathcal{N}(0, s_H^2/n_H)$.\n\nThe posterior precision (inverse covariance) matrix $\\mathbf{\\Sigma}_n^{-1}$ for $\\mathbf{x}$ after observing the data is given by the standard update rule $\\mathbf{\\Sigma}_n^{-1} = \\mathbf{\\Sigma}_0^{-1} + \\mathbf{L}$, where $\\mathbf{L}$ is the data precision (log-likelihood Hessian).\nThe prior precision is $\\mathbf{\\Sigma}_0^{-1} = \\text{diag}(1/\\tau_L^2, 1/\\tau_\\delta^2)$.\nThe data precision is the sum of contributions from the low- and high-fidelity data. For $n_L0$, the observation $\\bar{y}_L$ provides information only on $f_L$. For $n_H0$, $\\bar{y}_H$ provides information on $\\rho f_L + \\delta$.\nThe full data precision matrix from $n_L$ low-fidelity runs and $n_H$ high-fidelity runs is:\n$$\\mathbf{L} = \\begin{pmatrix} n_L/s_L^2  0 \\\\ 0  0 \\end{pmatrix} + \\begin{pmatrix} \\rho \\\\ 1 \\end{pmatrix} (s_H^2/n_H)^{-1} \\begin{pmatrix} \\rho  1 \\end{pmatrix} = \\begin{pmatrix} n_L/s_L^2 + n_H\\rho^2/s_H^2  n_H\\rho/s_H^2 \\\\ n_H\\rho/s_H^2  n_H/s_H^2 \\end{pmatrix}$$\nThe posterior precision matrix is thus:\n$$\\mathbf{\\Sigma}_n^{-1} = \\begin{pmatrix} 1/\\tau_L^2 + n_L/s_L^2 + n_H\\rho^2/s_H^2  n_H\\rho/s_H^2 \\\\ n_H\\rho/s_H^2  1/\\tau_\\delta^2 + n_H/s_H^2 \\end{pmatrix}$$\nThe quantity of interest is $z = \\rho f_L + \\delta = \\mathbf{c}^T \\mathbf{x}$ with $\\mathbf{c} = [\\rho, 1]^T$. The posterior variance of $z$ is $V(n_L, n_H) = \\text{Var}(z|\\text{data}) = \\mathbf{c}^T \\mathbf{\\Sigma}_n \\mathbf{c}$.\nA direct but algebraically intensive calculation yields:\n$$V(n_L, n_H) = \\frac{1/\\tau_L^2 + n_L/s_L^2 + \\rho^2/\\tau_\\delta^2}{(1/\\tau_L^2 + n_L/s_L^2)(1/\\tau_\\delta^2 + n_H/s_H^2) + n_H\\rho^2/(s_H^2\\tau_\\delta^2)}$$\nA more insightful form is obtained by expressing the posterior precision (information) of $z$, denoted $I(n_L, n_H) = 1/V(n_L, n_H)$:\n$$I(n_L, n_H) = \\frac{(1/\\tau_L^2 + n_L/s_L^2) / \\tau_\\delta^2}{1/\\tau_L^2 + n_L/s_L^2 + \\rho^2/\\tau_\\delta^2} + \\frac{n_H}{s_H^2}$$\nThis structure shows that the total information on $z$ is a sum of two terms: one term, $n_H/s_H^2$, represents the information gained directly from high-fidelity observations, and the other term represents the combined information from the prior and the low-fidelity data. Let's denote the latter term as $I_L(n_L)$.\n\n### Step 2: Optimization Problem and Solution Strategy\n\nThe problem is to minimize the cost $T(n_L, n_H) = n_L t_L + n_H t_H$ subject to the constraint on the posterior standard deviation, $\\sqrt{V(n_L, n_H)} \\le \\epsilon$. This is equivalent to constraining the information:\n$$I(n_L, n_H) \\ge 1/\\epsilon^2$$\nSubstituting the derived expression for information gives the full constraint on the non-negative integers $(n_L, n_H)$:\n$$I_L(n_L) + \\frac{n_H}{s_H^2} \\ge \\frac{1}{\\epsilon^2}$$\nFor a fixed number of low-fidelity runs $n_L$, we can find the minimum required number of high-fidelity runs, $n_H^*(n_L)$, by rearranging the inequality:\n$$n_H \\ge s_H^2 \\left(\\frac{1}{\\epsilon^2} - I_L(n_L)\\right)$$\nSince $n_H$ must be a non-negative integer, the minimum value is:\n$$n_H^*(n_L) = \\left\\lceil \\max\\left(0, s_H^2 \\left(\\frac{1}{\\epsilon^2} - I_L(n_L)\\right)\\right) \\right\\rceil$$\nThe optimization problem reduces to finding the minimum of a single-variable function over non-negative integers $n_L$:\n$$\\min_{n_L \\ge 0} T(n_L) = n_L t_L + n_H^*(n_L) t_H$$\nThe function $I_L(n_L)$ is monotonically increasing with $n_L$. Consequently, $n_H^*(n_L)$ is a non-increasing step function of $n_L$. The total cost $T(n_L)$ is the sum of a linearly increasing term $n_L t_L$ and a non-increasing step function. A global minimum must exist.\n\nWe can solve this by performing a search over $n_L$.\n1.  First, check the case with zero simulations ($n_L=0, n_H=0$). The variance is the prior variance $V(0,0)=\\rho^2\\tau_L^2+\\tau_\\delta^2$. If $V(0,0) \\le \\epsilon^2$, the optimal solution is $(0, 0, 0)$.\n2.  Otherwise, we establish an initial solution and an upper bound on cost by considering the high-fidelity-only case ($n_L=0$). We compute $n_H^*(0)$ and the corresponding cost $T_{min} = n_H^*(0) t_H$. This is our current best solution: $(0, n_H^*(0), T_{min})$.\n3.  Any solution with $n_L  0$ can only be better if its total cost is less than $T_{min}$. This implies that $n_L t_L  T_{min}$, which gives a finite search bound for $n_L$: $n_{L,max} = \\lfloor (T_{min} - 1)/t_L \\rfloor$.\n4.  We then iterate $n_L$ from $1$ up to this bound, calculating $T(n_L)$ at each step. If a lower cost is found, we update $T_{min}$ and the optimal $(n_L, n_H)$, and we also update the search bound $n_{L,max}$ based on the new, lower $T_{min}$. This dynamically prunes the search space.\n5.  The final stored $(n_L, n_H, T_{min})$ after the search completes is the optimal solution.\n\nThis algorithm is guaranteed to find the global minimum.", "answer": "```python\nimport numpy as np\nimport math\n\ndef solve():\n    \"\"\"\n    Solves the multi-fidelity allocation problem for the given test suite.\n    \"\"\"\n    test_cases = [\n        # (rho, tau_L^2, tau_delta^2, s_L^2, s_H^2, t_L, t_H, epsilon)\n        (0.9, 0.25, 0.04, 0.01, 0.0025, 10, 200, 0.05),\n        (0.5, 0.36, 0.09, 0.02, 0.01, 20, 800, 0.15),\n        (0.8, 0.01, 0.005, 0.01, 0.01, 5, 100, 0.12),\n        (0.95, 0.49, 0.01, 0.04, 0.0025, 30, 300, 0.02),\n        (0.1, 1.0, 0.25, 0.1, 0.01, 10, 50, 0.3),\n    ]\n\n    results = []\n    for case in test_cases:\n        rho, tau_L2, tau_d2, s_L2, s_H2, t_L, t_H, epsilon = case\n\n        # Inverse variances (precisions)\n        U_L = 1.0 / tau_L2\n        U_d = 1.0 / tau_d2\n        P_L = 1.0 / s_L2\n        P_H = 1.0 / s_H2\n        rho2 = rho**2\n        \n        target_info = 1.0 / (epsilon**2)\n\n        # Check for zero-simulation case\n        prior_var = rho2 * tau_L2 + tau_d2\n        if prior_var = epsilon**2:\n            results.append([0, 0, 0.0])\n            continue\n\n        def get_i_l(n_L):\n            \"\"\"Calculates the information contribution from low-fidelity data.\"\"\"\n            num_term = U_L + n_L * P_L\n            den_term = num_term + rho2 * U_d\n            if den_term == 0: # Should not happen with valid inputs\n                return 0.0\n            return (num_term * U_d) / den_term\n        \n        def get_n_h(n_L):\n            \"\"\"Calculates the minimum required n_H for a given n_L.\"\"\"\n            i_l = get_i_l(n_L)\n            if target_info - i_l = 0:\n                return 0\n            \n            required_n_h_float = s_H2 * (target_info - i_l)\n            return math.ceil(required_n_h_float)\n\n        # Start with the high-fidelity only case (n_L = 0)\n        best_nl = 0\n        best_nh = get_n_h(0)\n        best_T = float(best_nh * t_H)\n\n        # Determine the maximum n_L to search\n        # If best_T is 0, t_L must also be 0 for loop to run, which is not the case\n        if t_L  0:\n            n_L_max = math.floor(best_T / t_L)\n        else:\n            n_L_max = 0\n\n        # Search for a better solution by increasing n_L\n        for n_L_current in range(1, n_L_max + 1):\n            n_h_current = get_n_h(n_L_current)\n            T_current = float(n_L_current * t_L + n_h_current * t_H)\n            \n            if T_current  best_T:\n                best_T = T_current\n                best_nl = n_L_current\n                best_nh = n_h_current\n                # Update search bound with better T\n                if t_L  0:\n                    n_L_max = math.floor(best_T / t_L)\n\n        results.append([best_nl, best_nh, best_T])\n\n    # Format output according to specification\n    # e.g., [[n_L1,n_H1,T1],[n_L2,n_H2,T2]] with no spaces\n    inner_results_str = [f\"[{r[0]},{r[1]},{r[2]:.3f}]\" for r in results]\n    final_output = f\"[{','.join(inner_results_str)}]\"\n    print(final_output)\n\nsolve()\n\n```", "id": "3478372"}]}