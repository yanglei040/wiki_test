{"hands_on_practices": [{"introduction": "The Metropolis-Hastings algorithm forms the foundation of many Markov chain Monte Carlo ($MCMC$) applications in cosmology. At its heart is the acceptance probability, which determines whether a proposed new point in the parameter space is accepted or rejected, ensuring the chain correctly samples the target posterior distribution. This first exercise [@problem_id:3478680] provides a concrete calculation of this probability, helping you connect the abstract formula to the practical behavior of a sampler as it navigates the posterior landscape.", "problem": "A numerical cosmology pipeline performs Bayesian parameter inference for a flat $\\Lambda$ Cold Dark Matter model using Type Ia supernovae and cosmic microwave background summary statistics, sampling the posterior of $\\boldsymbol{\\theta} = (\\Omega_{\\mathrm{m}}, \\sigma_{8}, H_{0})$ via a Metropolis–Hastings Markov chain Monte Carlo (MCMC) transition that enforces detailed balance with respect to the posterior density $\\pi(\\boldsymbol{\\theta}) \\propto \\mathcal{L}(\\mathbf{d} \\mid \\boldsymbol{\\theta})\\,p(\\boldsymbol{\\theta})$, and a generally non-symmetric proposal density $q(\\boldsymbol{\\theta}' \\mid \\boldsymbol{\\theta})$. At a particular iteration, from current state $\\boldsymbol{\\theta}$ to proposed state $\\boldsymbol{\\theta}'$, the code has computed the posterior density ratio $\\pi(\\boldsymbol{\\theta}')/\\pi(\\boldsymbol{\\theta}) = 10$ and the proposal density ratio $q(\\boldsymbol{\\theta} \\mid \\boldsymbol{\\theta}')/q(\\boldsymbol{\\theta}' \\mid \\boldsymbol{\\theta}) = 2$.\n\nStarting from the fundamental requirement of detailed balance for Markov chains targeting $\\pi(\\boldsymbol{\\theta})$ and from the definition of a Metropolis–Hastings transition kernel in terms of a proposal and an acceptance function, determine the Metropolis–Hastings acceptance probability $\\alpha(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}')$ for this move. Then interpret what this value implies for the behavior of the chain at this step in the context of cosmological parameter sampling. Provide the acceptance probability as a pure number. No rounding is required.", "solution": "The problem is first subjected to a rigorous validation process.\n\n### Step 1: Extract Givens\nThe following data and definitions are explicitly provided in the problem statement:\n- **Model:** Flat $\\Lambda$ Cold Dark Matter ($\\Lambda$CDM) model.\n- **Parameters of interest:** $\\boldsymbol{\\theta} = (\\Omega_{\\mathrm{m}}, \\sigma_{8}, H_{0})$.\n- **Inference method:** Metropolis–Hastings Markov chain Monte Carlo (MCMC).\n- **Target probability density:** The posterior density $\\pi(\\boldsymbol{\\theta}) \\propto \\mathcal{L}(\\mathbf{d} \\mid \\boldsymbol{\\theta})\\,p(\\boldsymbol{\\theta})$, where $\\mathcal{L}$ is the likelihood and $p$ is the prior.\n- **Proposal density:** A generally non-symmetric density, $q(\\boldsymbol{\\theta}' \\mid \\boldsymbol{\\theta})$.\n- **Core principle:** The MCMC transition enforces detailed balance with respect to $\\pi(\\boldsymbol{\\theta})$.\n- **At a specific iteration from state $\\boldsymbol{\\theta}$ to $\\boldsymbol{\\theta}'$:**\n    - The posterior density ratio is $\\frac{\\pi(\\boldsymbol{\\theta}')}{\\pi(\\boldsymbol{\\theta})} = 10$.\n    - The proposal density ratio is $\\frac{q(\\boldsymbol{\\theta} \\mid \\boldsymbol{\\theta}')}{q(\\boldsymbol{\\theta}' \\mid \\boldsymbol{\\theta})} = 2$.\n- **Objective:** Determine the Metropolis–Hastings acceptance probability $\\alpha(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}')$ and interpret its meaning.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is assessed for validity against the established criteria.\n- **Scientifically Grounded:** The problem is firmly rooted in the standard theoretical framework of Bayesian statistical inference and computational physics. The Metropolis-Hastings algorithm is a cornerstone of MCMC methods, and its application to parameter estimation in cosmology (specifically for the $\\Lambda$CDM model) is a routine and fundamental task in the field. All concepts—posterior density, likelihood, prior, proposal density, detailed balance, and the parameters $\\Omega_{\\mathrm{m}}$, $\\sigma_{8}$, $H_{0}$—are standard and well-defined.\n- **Well-Posed:** The problem provides all necessary information to compute the acceptance probability. The definition of the Metropolis-Hastings acceptance rule leads to a unique and stable solution from the given ratios.\n- **Objective:** The language is technical, precise, and free of any subjectivity, ambiguity, or opinion.\n\nThe problem does not exhibit any of the listed invalidity flaws. It is not scientifically unsound, is directly formalizable, is complete, describes a computationally realistic scenario, and is well-structured.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. A full solution will be provided.\n\n### Solution Derivation\nThe fundamental requirement for a Markov chain to have a stationary distribution $\\pi(\\boldsymbol{\\theta})$ is the condition of detailed balance. For any two states $\\boldsymbol{\\theta}$ and $\\boldsymbol{\\theta}'$, detailed balance requires that the rate of transitioning from $\\boldsymbol{\\theta}$ to $\\boldsymbol{\\theta}'$ is equal to the rate of transitioning from $\\boldsymbol{\\theta}'$ to $\\boldsymbol{\\theta}$ when the chain is in its stationary state. This is expressed mathematically as:\n$$\n\\pi(\\boldsymbol{\\theta}) P(\\boldsymbol{\\theta}' \\mid \\boldsymbol{\\theta}) = \\pi(\\boldsymbol{\\theta}') P(\\boldsymbol{\\theta} \\mid \\boldsymbol{\\theta}')\n$$\nwhere $P(\\boldsymbol{\\theta}' \\mid \\boldsymbol{\\theta})$ is the transition probability (or kernel) of moving from state $\\boldsymbol{\\theta}$ to state $\\boldsymbol{\\theta}'$.\n\nIn the Metropolis–Hastings algorithm, the transition is a two-step process: proposing a new state and then accepting or rejecting it. The transition probability for a move from $\\boldsymbol{\\theta}$ to a different state $\\boldsymbol{\\theta}'$ is the product of the probability of proposing $\\boldsymbol{\\theta}'$ and the probability of accepting it:\n$$\nP(\\boldsymbol{\\theta}' \\mid \\boldsymbol{\\theta}) = q(\\boldsymbol{\\theta}' \\mid \\boldsymbol{\\theta}) \\alpha(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}') \\quad \\text{for } \\boldsymbol{\\theta} \\neq \\boldsymbol{\\theta}'\n$$\nwhere $q(\\boldsymbol{\\theta}' \\mid \\boldsymbol{\\theta})$ is the proposal density and $\\alpha(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}')$ is the acceptance probability.\n\nSubstituting this form of the transition kernel into the detailed balance equation gives:\n$$\n\\pi(\\boldsymbol{\\theta}) q(\\boldsymbol{\\theta}' \\mid \\boldsymbol{\\theta}) \\alpha(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}') = \\pi(\\boldsymbol{\\theta}') q(\\boldsymbol{\\theta} \\mid \\boldsymbol{\\theta}') \\alpha(\\boldsymbol{\\theta}', \\boldsymbol{\\theta})\n$$\nThis equation can be rearranged to show the relationship that the acceptance probabilities must satisfy:\n$$\n\\frac{\\alpha(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}')}{\\alpha(\\boldsymbol{\\theta}', \\boldsymbol{\\theta})} = \\frac{\\pi(\\boldsymbol{\\theta}')}{\\pi(\\boldsymbol{\\theta})} \\frac{q(\\boldsymbol{\\theta} \\mid \\boldsymbol{\\theta}')}{q(\\boldsymbol{\\theta}' \\mid \\boldsymbol{\\theta})}\n$$\nThe standard choice for the acceptance probability in the Metropolis–Hastings algorithm, which satisfies this condition while maximizing the acceptance rate, is:\n$$\n\\alpha(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}') = \\min \\left( 1, \\frac{\\pi(\\boldsymbol{\\theta}')}{\\pi(\\boldsymbol{\\theta})} \\frac{q(\\boldsymbol{\\theta} \\mid \\boldsymbol{\\theta}')}{q(\\boldsymbol{\\theta}' \\mid \\boldsymbol{\\theta})} \\right)\n$$\nThe problem provides the numerical values for the two ratios inside the minimum function.\nThe posterior density ratio is given as:\n$$\n\\frac{\\pi(\\boldsymbol{\\theta}')}{\\pi(\\boldsymbol{\\theta})} = 10\n$$\nThe proposal density ratio, known as the Hastings correction factor, is given as:\n$$\n\\frac{q(\\boldsymbol{\\theta} \\mid \\boldsymbol{\\theta}')}{q(\\boldsymbol{\\theta}' \\mid \\boldsymbol{\\theta})} = 2\n$$\nWe substitute these values into the formula for the acceptance probability:\n$$\n\\alpha(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}') = \\min \\left( 1, (10) \\times (2) \\right)\n$$\n$$\n\\alpha(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}') = \\min(1, 20)\n$$\nEvaluating the minimum function gives:\n$$\n\\alpha(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}') = 1\n$$\n### Interpretation\nThe calculated acceptance probability is $\\alpha(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}') = 1$. This means that the proposed move from the current state $\\boldsymbol{\\theta}$ to the new state $\\boldsymbol{\\theta}'$ is accepted with certainty.\n\nIn the context of cosmological parameter sampling, this is a highly desirable outcome at this step. The chain has found a new point $\\boldsymbol{\\theta}'$ in the parameter space $\\{ \\Omega_{\\mathrm{m}}, \\sigma_{8}, H_{0} \\}$ that is characterized by a posterior probability density $10$ times greater than that of the current point $\\boldsymbol{\\theta}$. This indicates that the proposed set of cosmological parameters provides a much better fit to the combined Type Ia supernovae and CMB data (as captured by the likelihood $\\mathcal{L}$) and/or is more favored by the prior knowledge encoded in $p(\\boldsymbol{\\theta})$.\n\nEven though the proposal distribution made the forward move ($\\boldsymbol{\\theta} \\to \\boldsymbol{\\theta}'$) half as probable as the reverse move ($\\boldsymbol{\\theta}' \\to \\boldsymbol{\\theta}$), as indicated by the Hastings factor of $2$, the vast improvement in posterior density overwhelmingly favors accepting the new state. The sampler is efficiently \"climbing the hill\" of the posterior probability landscape, moving aggressively toward the regions of highest probability which contain the most likely values for the cosmological parameters. Accepting this move ensures the chain is effectively exploring and converging towards the true posterior distribution.", "answer": "$$\n\\boxed{1}\n$$", "id": "3478680"}, {"introduction": "While Metropolis-Hastings is a general-purpose algorithm, more specialized $MCMC$ methods can be far more efficient when the statistical model has a particular structure. This practice [@problem_id:3478681] introduces the Gibbs sampler, a powerful technique that is applicable when the full conditional distributions of the parameters can be sampled from directly. You will derive these conditionals for a common hierarchical model, a task that demonstrates the power of conjugate priors and is a key skill for building efficient custom samplers.", "problem": "A numerical cosmology team is performing cross-instrument calibration of small additive zero-point offsets in standardized units for a wide-field survey. For each of $N$ photometric bands, $i=1,\\dots,N$, a set of stellar standards is observed multiple times within the same night, producing calibrated residuals $\\{y_{ij}\\}_{j=1}^{M_i}$ after subtracting the best-fit cosmology-dependent photometric model. The residuals are modeled as arising from an unknown band-specific offset $\\theta_i$ and independent measurement noise. To pool information across bands, the offsets are tied by a shared hyper-mean $\\mu$ that encodes the overall calibration level.\n\nAssume the following hierarchical conjugate model with Gaussian likelihood and Gaussian priors, with known precisions and variances:\n- Likelihood: for each band $i$, and replicate $j$, $y_{ij} \\mid \\theta_i \\sim \\mathcal{N}(\\theta_i, \\sigma^2)$, independently across $i$ and $j$, where $\\sigma^2$ is known.\n- Band-level prior: $\\theta_i \\mid \\mu \\sim \\mathcal{N}(\\mu, \\tau^2)$, independently across $i$, where $\\tau^2$ is known.\n- Hyperprior: $\\mu \\sim \\mathcal{N}(m_0, s_0^2)$, where $m_0$ and $s_0^2$ are known.\n\nStarting from Bayes’ rule and the properties of the Gaussian family, derive the full conditional distributions required for a two-block Gibbs sampler over $\\{\\theta_i\\}_{i=1}^{N}$ and $\\mu$. Your derivation must show how each conditional distribution arises from completing the square in the exponent and must be expressed explicitly in terms of the data $\\{y_{ij}\\}$ and the known hyperparameters $\\sigma^2$, $\\tau^2$, $m_0$, and $s_0^2$.\n\nThen, for a cross-instrument calibration scenario with $N=4$ bands, known band-level variance $\\tau^2 = 1.0 \\times 10^{-5}$, and hyperprior variance $s_0^2 = 1.0 \\times 10^{-4}$, compute the posterior variance of the hyper-mean $\\mu$ under the derived conditional. All quantities are measured in standardized units; no physical units are required. Round your final numerical answer to four significant figures.", "solution": "The problem requires the derivation of the full conditional distributions for a Gibbs sampler applied to a hierarchical Bayesian model, followed by a numerical calculation of a specific posterior variance.\n\nThe specified hierarchical model is:\n- Likelihood: $y_{ij} \\mid \\theta_i \\sim \\mathcal{N}(\\theta_i, \\sigma^2)$ for $i=1, \\dots, N$ and $j=1, \\dots, M_i$.\n- Band-level prior: $\\theta_i \\mid \\mu \\sim \\mathcal{N}(\\mu, \\tau^2)$ for $i=1, \\dots, N$.\n- Hyperprior: $\\mu \\sim \\mathcal{N}(m_0, s_0^2)$.\n\nAll variances $\\sigma^2$, $\\tau^2$, and $s_0^2$, along with the hyper-mean $m_0$, are known constants.\n\nThe Gibbs sampler requires iteratively sampling from the full conditional distributions of the parameters. The two blocks of parameters are $\\{\\theta_i\\}_{i=1}^{N}$ and $\\mu$.\n\nFirst, we write down the joint posterior distribution for all unknown parameters, which is proportional to the product of the likelihood and all priors. Using Bayes' rule:\n$$\np(\\{\\theta_i\\}_{i=1}^{N}, \\mu \\mid \\{y_{ij}\\}) \\propto p(\\{y_{ij}\\} \\mid \\{\\theta_i\\}_{i=1}^{N}, \\mu) \\, p(\\{\\theta_i\\}_{i=1}^{N} \\mid \\mu) \\, p(\\mu)\n$$\nGiven the conditional independence structure of the model, this simplifies to:\n$$\np(\\{\\theta_i\\}_{i=1}^{N}, \\mu \\mid \\{y_{ij}\\}) \\propto \\left( \\prod_{i=1}^{N} \\prod_{j=1}^{M_i} p(y_{ij} \\mid \\theta_i) \\right) \\left( \\prod_{i=1}^{N} p(\\theta_i \\mid \\mu) \\right) p(\\mu)\n$$\nSubstituting the Gaussian probability density functions (PDFs), where the PDF for $\\mathcal{N}(x_0, \\nu^2)$ is proportional to $\\exp\\left(-\\frac{1}{2\\nu^2}(x-x_0)^2\\right)$:\n$$\np(\\{\\theta_i\\}_{i=1}^{N}, \\mu \\mid \\{y_{ij}\\}) \\propto \\exp\\left( -\\frac{1}{2\\sigma^2} \\sum_{i=1}^{N} \\sum_{j=1}^{M_i} (y_{ij} - \\theta_i)^2 \\right) \\times \\exp\\left( -\\frac{1}{2\\tau^2} \\sum_{i=1}^{N} (\\theta_i - \\mu)^2 \\right) \\times \\exp\\left( -\\frac{1}{2s_0^2} (\\mu - m_0)^2 \\right)\n$$\nThe logarithm of the joint posterior is therefore proportional to the sum of the exponents:\n$$\n\\ln p(\\{\\theta_i\\}_{i=1}^{N}, \\mu \\mid \\{y_{ij}\\}) = C - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{N} \\sum_{j=1}^{M_i} (y_{ij} - \\theta_i)^2 - \\frac{1}{2\\tau^2} \\sum_{i=1}^{N} (\\theta_i - \\mu)^2 - \\frac{1}{2s_0^2} (\\mu - m_0)^2\n$$\nwhere $C$ is a normalization constant.\n\n**1. Full Conditional Distribution for $\\theta_k$**\n\nTo find the full conditional distribution for a specific offset $\\theta_k$, we isolate all terms in the log-posterior that depend on $\\theta_k$, treating all other parameters ($\\mu$, $\\{\\theta_{i \\neq k}\\}$) and the data as constants.\n$$\n\\ln p(\\theta_k \\mid \\mu, \\{\\theta_{i \\neq k}\\}, \\{y_{ij}\\}) = C_k - \\frac{1}{2\\sigma^2} \\sum_{j=1}^{M_k} (y_{kj} - \\theta_k)^2 - \\frac{1}{2\\tau^2} (\\theta_k - \\mu)^2\n$$\nWe expand the quadratic terms involving $\\theta_k$:\n$$\n= C_k - \\frac{1}{2\\sigma^2} \\left( \\sum_{j=1}^{M_k} y_{kj}^2 - 2\\theta_k \\sum_{j=1}^{M_k} y_{kj} + M_k \\theta_k^2 \\right) - \\frac{1}{2\\tau^2} (\\theta_k^2 - 2\\mu\\theta_k + \\mu^2)\n$$\nCollecting terms in $\\theta_k^2$ and $\\theta_k$:\n$$\n= C_k' - \\frac{1}{2} \\left[ \\left(\\frac{M_k}{\\sigma^2} + \\frac{1}{\\tau^2}\\right)\\theta_k^2 - 2\\left(\\frac{1}{\\sigma^2}\\sum_{j=1}^{M_k} y_{kj} + \\frac{\\mu}{\\tau^2}\\right)\\theta_k \\right]\n$$\nThis expression is a quadratic function of $\\theta_k$, which means the distribution $p(\\theta_k \\mid \\dots)$ is Gaussian. We identify its parameters by completing the square. The general form of the log-PDF of a Gaussian $\\mathcal{N}(m, s^2)$ is $-\\frac{1}{2s^2}(x-m)^2 = -\\frac{1}{2s^2}(x^2 - 2mx + m^2)$. The coefficient of $x^2$ gives the variance $s^2$, and the coefficient of the linear term gives the mean $m$.\n\nThe precision (inverse variance) of the conditional distribution for $\\theta_k$ is the coefficient of $\\frac{1}{2}\\theta_k^2$:\n$$\n\\lambda_{\\theta_k} = \\frac{M_k}{\\sigma^2} + \\frac{1}{\\tau^2}\n$$\nThe variance is $s_{\\theta_k}^2 = \\lambda_{\\theta_k}^{-1} = \\left(\\frac{M_k}{\\sigma^2} + \\frac{1}{\\tau^2}\\right)^{-1}$.\nThe mean $m_{\\theta_k}$ is found from the linear term: $\\lambda_{\\theta_k} m_{\\theta_k} = \\frac{1}{\\sigma^2}\\sum_{j=1}^{M_k} y_{kj} + \\frac{\\mu}{\\tau^2}$.\nDefining the sample mean for band $k$ as $\\bar{y}_k = \\frac{1}{M_k} \\sum_{j=1}^{M_k} y_{kj}$, we have:\n$$\nm_{\\theta_k} = \\frac{1}{\\lambda_{\\theta_k}} \\left(\\frac{M_k \\bar{y}_k}{\\sigma^2} + \\frac{\\mu}{\\tau^2}\\right) = \\left(\\frac{M_k}{\\sigma^2} + \\frac{1}{\\tau^2}\\right)^{-1} \\left(\\frac{M_k \\bar{y}_k}{\\sigma^2} + \\frac{\\mu}{\\tau^2}\\right)\n$$\nThus, the full conditional for each $\\theta_k$ is a Gaussian distribution:\n$$\n\\theta_k \\mid \\mu, \\{y_{ij}\\} \\sim \\mathcal{N}\\left(m_{\\theta_k}, s_{\\theta_k}^2\\right)\n$$\n\n**2. Full Conditional Distribution for $\\mu$**\n\nTo find the full conditional for the hyper-mean $\\mu$, we isolate terms in the log-posterior dependent on $\\mu$. From the model's conditional independence structure, $\\mu$ only depends on $\\{\\theta_i\\}$ and its hyperprior; it is conditionally independent of the data $\\{y_{ij}\\}$ given $\\{\\theta_i\\}$.\n$$\n\\ln p(\\mu \\mid \\{\\theta_i\\}_{i=1}^{N}, \\{y_{ij}\\}) = C_\\mu - \\frac{1}{2\\tau^2} \\sum_{i=1}^{N} (\\theta_i - \\mu)^2 - \\frac{1}{2s_0^2} (\\mu - m_0)^2\n$$\nExpanding the quadratic terms involving $\\mu$:\n$$\n= C_\\mu - \\frac{1}{2\\tau^2} \\left( \\sum_{i=1}^{N} \\theta_i^2 - 2\\mu \\sum_{i=1}^{N} \\theta_i + N \\mu^2 \\right) - \\frac{1}{2s_0^2} (\\mu^2 - 2m_0\\mu + m_0^2)\n$$\nCollecting terms in $\\mu^2$ and $\\mu$:\n$$\n= C_\\mu' - \\frac{1}{2} \\left[ \\left(\\frac{N}{\\tau^2} + \\frac{1}{s_0^2}\\right)\\mu^2 - 2\\left(\\frac{1}{\\tau^2}\\sum_{i=1}^{N} \\theta_i + \\frac{m_0}{s_0^2}\\right)\\mu \\right]\n$$\nThis is a quadratic function of $\\mu$, so the conditional distribution is Gaussian. We complete the square to find its parameters.\nThe precision of the conditional distribution for $\\mu$ is:\n$$\n\\lambda_\\mu = \\frac{N}{\\tau^2} + \\frac{1}{s_0^2}\n$$\nThe variance is $s_\\mu^2 = \\lambda_\\mu^{-1} = \\left(\\frac{N}{\\tau^2} + \\frac{1}{s_0^2}\\right)^{-1}$.\nThe mean $m_\\mu$ is found from the linear term: $\\lambda_\\mu m_\\mu = \\frac{1}{\\tau^2}\\sum_{i=1}^{N} \\theta_i + \\frac{m_0}{s_0^2}$.\nDefining the mean of the band offsets as $\\bar{\\theta} = \\frac{1}{N}\\sum_{i=1}^{N}\\theta_i$, we have:\n$$\nm_\\mu = \\frac{1}{\\lambda_\\mu} \\left(\\frac{N\\bar{\\theta}}{\\tau^2} + \\frac{m_0}{s_0^2}\\right) = \\left(\\frac{N}{\\tau^2} + \\frac{1}{s_0^2}\\right)^{-1} \\left(\\frac{N\\bar{\\theta}}{\\tau^2} + \\frac{m_0}{s_0^2}\\right)\n$$\nThus, the full conditional for $\\mu$ is a Gaussian distribution:\n$$\n\\mu \\mid \\{\\theta_i\\}_{i=1}^{N} \\sim \\mathcal{N}\\left(m_\\mu, s_\\mu^2\\right)\n$$\n\n**3. Numerical Calculation**\n\nThe problem asks for the posterior variance of the hyper-mean $\\mu$ under the derived conditional distribution. This is the quantity $s_\\mu^2$ derived above.\n$$\ns_\\mu^2 = \\left(\\frac{N}{\\tau^2} + \\frac{1}{s_0^2}\\right)^{-1}\n$$\nWe are given the following values:\n- Number of bands $N = 4$.\n- Band-level variance $\\tau^2 = 1.0 \\times 10^{-5}$.\n- Hyperprior variance $s_0^2 = 1.0 \\times 10^{-4}$.\n\nFirst, we compute the precisions:\n$$\n\\frac{1}{\\tau^2} = \\frac{1}{1.0 \\times 10^{-5}} = 10^5\n$$\n$$\n\\frac{1}{s_0^2} = \\frac{1}{1.0 \\times 10^{-4}} = 10^4\n$$\nNow we compute the total precision for the conditional distribution of $\\mu$:\n$$\n\\lambda_\\mu = \\frac{N}{\\tau^2} + \\frac{1}{s_0^2} = \\frac{4}{1.0 \\times 10^{-5}} + \\frac{1}{1.0 \\times 10^{-4}} = 4 \\times 10^5 + 1 \\times 10^4\n$$\n$$\n\\lambda_\\mu = 400000 + 10000 = 410000 = 4.1 \\times 10^5\n$$\nThe conditional variance $s_\\mu^2$ is the inverse of this precision:\n$$\ns_\\mu^2 = \\frac{1}{\\lambda_\\mu} = \\frac{1}{4.1 \\times 10^5} = \\frac{1}{410000}\n$$\nPerforming the division:\n$$\ns_\\mu^2 \\approx 0.00000243902439...\n$$\nIn scientific notation, this is $2.43902439... \\times 10^{-6}$.\nRounding to four significant figures, we get $2.439 \\times 10^{-6}$.", "answer": "$$\\boxed{2.439 \\times 10^{-6}}$$", "id": "3478681"}, {"introduction": "Cosmological posteriors are often complex, featuring multiple, well-separated modes that can trap a standard $MCMC$ sampler. Parallel Tempering is an advanced algorithm designed to overcome this challenge by running multiple chains at different temperatures and allowing them to swap states. This final exercise [@problem_id:3478677] delves into the core mechanism of this method, guiding you to derive the acceptance probability for a swap move from the principle of detailed balance.", "problem": "A common feature in numerical cosmology is multimodality in the posterior distribution for parameters, for example when fitting degenerate models of the Cosmic Microwave Background (CMB). Consider a toy one-dimensional cosmological parameter $\\theta$ with a bimodal posterior potential given by $U(\\theta)=a(\\theta^{2}-1)^{2}$, which defines the untempered posterior $\\pi(\\theta)\\propto \\exp(-U(\\theta))$. To accelerate exploration across modes, we employ a two-replica parallel tempering Markov chain Monte Carlo (MCMC) scheme in which replica $i$ targets the tempered posterior $\\pi_{i}(\\theta)\\propto \\exp(-\\beta_{i}U(\\theta))$ at inverse temperature $\\beta_{i}$.\n\nStarting from first principles of detailed balance for the joint target $\\pi_{1}(\\theta_{1})\\pi_{2}(\\theta_{2})$, derive the Metropolis acceptance probability for a swap proposal that exchanges the states of the two replicas, $(\\theta_{1},\\theta_{2})\\mapsto (\\theta_{2},\\theta_{1})$. Then, evaluate this acceptance probability explicitly for the following scientifically plausible configuration:\n- The double-well amplitude is $a=1.7$.\n- The inverse temperatures are $\\beta_{1}=3.0$ (a colder chain) and $\\beta_{2}=0.5$ (a hotter chain).\n- The current states are $\\theta_{1}=-1.1$ (near a well minimum) and $\\theta_{2}=0.2$ (near the barrier).\n\nCarry out all derivations from foundational definitions and detailed balance without invoking pre-memorized swap formulas. Express the final swap acceptance probability as a decimal, and round your answer to four significant figures. No physical units are required for the probability.", "solution": "The problem is well-posed, scientifically grounded, and provides all necessary information to derive and compute the swap acceptance probability in a parallel tempering MCMC scheme. It is therefore valid.\n\nThe core principle underpinning Markov chain Monte Carlo (MCMC) methods is the construction of a Markov chain whose stationary distribution is the target probability distribution, $\\pi(x)$. A sufficient condition to ensure this is the principle of detailed balance, which states that for any two states $x$ and $x'$, the rate of transitions from $x$ to $x'$ must equal the rate of transitions from $x'$ to $x$ in equilibrium:\n$$ \\pi(x) P(x \\to x') = \\pi(x') P(x' \\to x) $$\nwhere $P(x \\to x')$ is the transition probability.\n\nIn the Metropolis-Hastings algorithm, the transition probability is factored into a proposal probability $g(x'|x)$ and an acceptance probability $\\alpha(x'|x)$, such that $P(x \\to x') = g(x'|x) \\alpha(x'|x)$ for $x \\neq x'$. The acceptance probability is defined to satisfy detailed balance:\n$$ \\alpha(x'|x) = \\min\\left(1, \\frac{\\pi(x') g(x|x')}{\\pi(x) g(x'|x)}\\right) $$\n\nIn this problem, the state of the system is the pair of parameter values from the two replicas, $x = (\\theta_1, \\theta_2)$. The joint target distribution for this extended ensemble is the product of the individual tempered posteriors, as the replicas are independent:\n$$ \\pi(x) = \\pi(\\theta_1, \\theta_2) = \\pi_1(\\theta_1) \\pi_2(\\theta_2) $$\nWe are given that $\\pi_i(\\theta) \\propto \\exp(-\\beta_i U(\\theta))$. Letting $Z_i$ be the normalization constant for replica $i$, we can write $\\pi_i(\\theta) = \\frac{1}{Z_i} \\exp(-\\beta_i U(\\theta))$. The joint distribution is thus:\n$$ \\pi(\\theta_1, \\theta_2) = \\frac{1}{Z_1 Z_2} \\exp(-\\beta_1 U(\\theta_1) - \\beta_2 U(\\theta_2)) $$\n\nThe proposed move is a swap of the states between the two replicas. The current state is $x = (\\theta_1, \\theta_2)$, and the proposed state is $x' = (\\theta_2, \\theta_1)$. The proposal distribution for this swap, $g(x'|x) = g((\\theta_2, \\theta_1)|(\\theta_1, \\theta_2))$, is symmetric to the reverse proposal, $g(x|x') = g((\\theta_1, \\theta_2)|(\\theta_2, \\theta_1))$, because swapping the states of replica $1$ and $2$ is the same operation regardless of the direction. Therefore, the ratio of proposal probabilities is $g(x|x')/g(x'|x) = 1$. The acceptance probability simplifies to the Metropolis form:\n$$ \\alpha(x'|x) = \\min\\left(1, \\frac{\\pi(x')}{\\pi(x)}\\right) = \\min\\left(1, \\frac{\\pi(\\theta_2, \\theta_1)}{\\pi(\\theta_1, \\theta_2)}\\right) $$\n\nNow, we substitute the expression for the joint distribution into this ratio:\n$$ \\frac{\\pi(\\theta_2, \\theta_1)}{\\pi(\\theta_1, \\theta_2)} = \\frac{\\pi_1(\\theta_2) \\pi_2(\\theta_1)}{\\pi_1(\\theta_1) \\pi_2(\\theta_2)} = \\frac{\\frac{1}{Z_1}\\exp(-\\beta_1 U(\\theta_2)) \\cdot \\frac{1}{Z_2}\\exp(-\\beta_2 U(\\theta_1))}{\\frac{1}{Z_1}\\exp(-\\beta_1 U(\\theta_1)) \\cdot \\frac{1}{Z_2}\\exp(-\\beta_2 U(\\theta_2))} $$\nThe normalization constants $Z_1$ and $Z_2$ cancel, leaving:\n$$ \\frac{\\pi(\\theta_2, \\theta_1)}{\\pi(\\theta_1, \\theta_2)} = \\frac{\\exp(-\\beta_1 U(\\theta_2) - \\beta_2 U(\\theta_1))}{\\exp(-\\beta_1 U(\\theta_1) - \\beta_2 U(\\theta_2))} $$\nUsing the property $\\exp(A)/\\exp(B) = \\exp(A-B)$, we get:\n$$ \\frac{\\pi(\\theta_2, \\theta_1)}{\\pi(\\theta_1, \\theta_2)} = \\exp\\left( (-\\beta_1 U(\\theta_2) - \\beta_2 U(\\theta_1)) - (-\\beta_1 U(\\theta_1) - \\beta_2 U(\\theta_2)) \\right) $$\n$$ = \\exp\\left( -\\beta_1 U(\\theta_2) - \\beta_2 U(\\theta_1) + \\beta_1 U(\\theta_1) + \\beta_2 U(\\theta_2) \\right) $$\nGrouping terms by $U(\\theta_1)$ and $U(\\theta_2)$:\n$$ = \\exp\\left( (\\beta_1 - \\beta_2) U(\\theta_1) - (\\beta_1 - \\beta_2) U(\\theta_2) \\right) $$\nFactoring out the common term $(\\beta_1 - \\beta_2)$:\n$$ = \\exp\\left( (\\beta_1 - \\beta_2)(U(\\theta_1) - U(\\theta_2)) \\right) $$\nThis can also be written as $\\exp((\\beta_2 - \\beta_1)(U(\\theta_2) - U(\\theta_1)))$. Let's define $\\Delta = (\\beta_1 - \\beta_2)(U(\\theta_1) - U(\\theta_2))$. The swap acceptance probability is:\n$$ \\alpha = \\min(1, \\exp(\\Delta)) $$\nThis completes the derivation from first principles.\n\nNext, we evaluate this probability for the given numerical values:\n- Potential function: $U(\\theta) = a(\\theta^2-1)^2$ with $a=1.7$.\n- Inverse temperatures: $\\beta_1 = 3.0$ and $\\beta_2 = 0.5$.\n- Current states: $\\theta_1 = -1.1$ and $\\theta_2 = 0.2$.\n\nFirst, calculate the potential energy $U(\\theta)$ for each state.\nFor $\\theta_1 = -1.1$:\n$$ U(\\theta_1) = 1.7 \\times ((-1.1)^2 - 1)^2 = 1.7 \\times (1.21 - 1)^2 = 1.7 \\times (0.21)^2 $$\n$$ U(\\theta_1) = 1.7 \\times 0.0441 = 0.07497 $$\nFor $\\theta_2 = 0.2$:\n$$ U(\\theta_2) = 1.7 \\times ((0.2)^2 - 1)^2 = 1.7 \\times (0.04 - 1)^2 = 1.7 \\times (-0.96)^2 $$\n$$ U(\\theta_2) = 1.7 \\times 0.9216 = 1.56672 $$\n\nNow, calculate the exponent $\\Delta$:\n$$ \\Delta = (\\beta_1 - \\beta_2)(U(\\theta_1) - U(\\theta_2)) $$\n$$ \\beta_1 - \\beta_2 = 3.0 - 0.5 = 2.5 $$\n$$ U(\\theta_1) - U(\\theta_2) = 0.07497 - 1.56672 = -1.49175 $$\n$$ \\Delta = (2.5) \\times (-1.49175) = -3.729375 $$\n\nFinally, compute the acceptance probability $\\alpha$:\n$$ \\alpha = \\min(1, \\exp(-3.729375)) $$\nSince the exponent is negative, $\\exp(-3.729375)$ is less than $1$. Therefore:\n$$ \\alpha = \\exp(-3.729375) \\approx 0.024009848 $$\nThe problem requires the answer to be rounded to four significant figures. The first four significant figures are $2$, $4$, $0$, $0$. The fifth significant figure is $9$, so we round up the fourth.\n$$ \\alpha \\approx 0.02401 $$\nThis is the probability that the swap of states between the colder chain (near a potential minimum) and the hotter chain (near the potential barrier) will be accepted.", "answer": "$$\n\\boxed{0.02401}\n$$", "id": "3478677"}]}