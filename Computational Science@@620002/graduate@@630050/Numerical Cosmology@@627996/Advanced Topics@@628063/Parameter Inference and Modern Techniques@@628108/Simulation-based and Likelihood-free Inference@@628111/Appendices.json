{"hands_on_practices": [{"introduction": "The first step to mastering simulation-based inference (SBI) is to implement a foundational algorithm from the ground up. This practice guides you through building a complete Approximate Bayesian Computation (ABC) pipeline for a simplified cosmological model based on weak lensing peak counts. By coding the generative forward model, the rejection sampler, and evaluating the impact of different distance metrics, you will gain a concrete intuition for the entire SBI workflow and the crucial role of its individual components [@problem_id:3489626].", "problem": "Implement a complete, runnable program that performs Approximate Bayesian Computation (ABC) for a simplified weak gravitational lensing peak-count model and quantifies nominal $0.68$ credible-region posterior coverage as a function of distance metric, simulation budget $N_{\\mathrm{sim}}$, and tolerance $\\epsilon$. The program must estimate coverage for a set of specified test cases that vary these inputs, and output the coverage results in a single line in the exact format described at the end.\n\nYou must adhere to the following scientifically grounded setup.\n\n- Generative model (weak-lensing peak-count histogram):\n  - Let the cosmological parameter vector be $\\boldsymbol{\\theta} = (\\Omega_{\\mathrm{m}}, \\sigma_8)$.\n  - The prior is uniform: $\\Omega_{\\mathrm{m}} \\sim \\mathcal{U}(0.2, 0.4)$ and $\\sigma_8 \\sim \\mathcal{U}(0.6, 1.0)$, independent.\n  - Define $K = 8$ peak-height bins with unit bin width $\\Delta x = 1$ and abstracted unit bin centers. Let the expected counts per bin under $\\boldsymbol{\\theta}$ be\n    $$\\lambda_k(\\boldsymbol{\\theta}) = s \\, A_k \\left(\\frac{\\sigma_8}{\\sigma_{8,\\mathrm{ref}}}\\right)^{\\alpha_k} \\left(\\frac{\\Omega_{\\mathrm{m}}}{\\Omega_{\\mathrm{m,ref}}}\\right)^{\\beta_k}, \\quad k = 1,\\dots, K,$$\n    where $s = 1$, $\\sigma_{8,\\mathrm{ref}} = 0.8$, $\\Omega_{\\mathrm{m,ref}} = 0.3$, and\n    $$A_k = 200 \\exp\\!\\big(-0.4\\,(k-1)\\big), \\quad \\alpha_k = 1.1 + 0.1\\,(k-1), \\quad \\beta_k = 0.6 + 0.05\\,(k-1).$$\n  - Observed peak counts are drawn as independent Poisson variables:\n    $$N_k \\sim \\mathrm{Poisson}\\big(\\lambda_k(\\boldsymbol{\\theta})\\big), \\quad k = 1,\\dots,K.$$\n  - The summary statistic is the normalized histogram (empirical probabilities) $\\mathbf{p} \\in \\mathbb{R}^K$ with components\n    $$p_k = \\frac{N_k}{\\sum_{j=1}^K N_j + \\delta}, \\quad \\delta = 10^{-12}.$$\n\n- Distance metrics between two normalized histograms $\\mathbf{p}$ and $\\mathbf{q}$:\n  1. Euclidean distance:\n     $$d_{\\mathrm{E}}(\\mathbf{p}, \\mathbf{q}) = \\left(\\sum_{k=1}^K (p_k - q_k)^2\\right)^{1/2}.$$\n  2. Sliced Wasserstein distance in one dimension (which equals the first Wasserstein distance in $1\\mathrm{D}$):\n     $$d_{\\mathrm{SW}}(\\mathbf{p}, \\mathbf{q}) = \\sum_{k=1}^K \\left|F_{\\mathbf{p}}(k) - F_{\\mathbf{q}}(k)\\right| \\, \\Delta x,$$\n     where $F_{\\mathbf{p}}(k) = \\sum_{j=1}^k p_j$ and $F_{\\mathbf{q}}(k) = \\sum_{j=1}^k q_j$, and $\\Delta x = 1$.\n\n- Approximate Bayesian Computation (ABC) by rejection sampling:\n  - Given an observed normalized histogram $\\mathbf{p}_{\\mathrm{obs}}$, simulation budget $N_{\\mathrm{sim}}$, tolerance $\\epsilon$, and a chosen distance $d \\in \\{d_{\\mathrm{E}}, d_{\\mathrm{SW}}\\}$:\n    1. For $i = 1,\\dots, N_{\\mathrm{sim}}$, sample $\\boldsymbol{\\theta}^{(i)}$ from the prior, simulate counts $\\{N_k^{(i)}\\}_{k=1}^K$, form $\\mathbf{p}^{(i)}$, compute $d\\big(\\mathbf{p}^{(i)}, \\mathbf{p}_{\\mathrm{obs}}\\big)$; accept $\\boldsymbol{\\theta}^{(i)}$ if $d \\le \\epsilon$.\n    2. The set of accepted samples approximates the posterior.\n  - If no parameter is accepted, define the replicate as non-covering (see coverage definition below).\n\n- Coverage estimation:\n  - Fix the ground-truth parameters $\\boldsymbol{\\theta}_\\star = (0.3, 0.8)$.\n  - For each test case, perform $R$ independent replicates as follows:\n    1. Generate a new observed data set from $\\boldsymbol{\\theta}_\\star$ to obtain $\\mathbf{p}_{\\mathrm{obs}}$.\n    2. Run ABC using the specified $d$, $N_{\\mathrm{sim}}$, and $\\epsilon$ to obtain accepted samples $\\{\\boldsymbol{\\theta}^{(i)}\\}_{i=1}^{M}$.\n    3. If $M = 0$, record this replicate as non-covering.\n    4. Otherwise, compute the marginal equal-tailed $0.68$ credible intervals for $\\Omega_{\\mathrm{m}}$ and $\\sigma_8$ from the accepted samples:\n       $$I_{\\Omega} = \\big[\\mathrm{Quantile}_{0.16}\\{\\Omega_{\\mathrm{m}}^{(i)}\\},\\ \\mathrm{Quantile}_{0.84}\\{\\Omega_{\\mathrm{m}}^{(i)}\\}\\big],$$\n       $$I_{\\sigma} = \\big[\\mathrm{Quantile}_{0.16}\\{\\sigma_8^{(i)}\\},\\ \\mathrm{Quantile}_{0.84}\\{\\sigma_8^{(i)}\\}\\big].$$\n    5. Declare this replicate as covering if and only if both $\\Omega_{\\mathrm{m},\\star} \\in I_{\\Omega}$ and $\\sigma_{8,\\star} \\in I_{\\sigma}$ hold simultaneously.\n  - The coverage for that test case is the fraction of covering replicates out of $R$.\n\n- Test suite:\n  - Use $R = 24$ replicates per test case.\n  - Evaluate the following six test cases, each specified as a triple $(d, N_{\\mathrm{sim}}, \\epsilon)$:\n    1. $(d_{\\mathrm{E}}, 400, 0.12)$\n    2. $(d_{\\mathrm{E}}, 1200, 0.12)$\n    3. $(d_{\\mathrm{E}}, 1200, 0.08)$\n    4. $(d_{\\mathrm{SW}}, 400, 0.40)$\n    5. $(d_{\\mathrm{SW}}, 1200, 0.40)$\n    6. $(d_{\\mathrm{SW}}, 1200, 0.28)$\n\n- Required final output format:\n  - Your program should produce a single line of output containing the coverage results for the six test cases, in the same order as above, as a comma-separated list enclosed in square brackets.\n  - Each coverage value must be a decimal rounded to three decimal places.\n  - For example, valid output looks like: \"[0.583,0.625,0.667,0.542,0.708,0.750]\".\n\nNotes:\n- No physical units are required in this problem; all quantities are dimensionless by construction.\n- Angles are not used in this problem.\n- Percentages must be expressed as decimals; do not use a percentage sign.", "solution": "The user has provided a well-defined computational problem in the domain of numerical cosmology and simulation-based inference. All aspects of the problem are scientifically and mathematically sound, complete, and unambiguous. The problem is deemed valid and a solution will be provided.\n\nThe core task is to implement and evaluate an Approximate Bayesian Computation (ABC) procedure on a simplified model of weak gravitational lensing peak counts. This involves simulating data, comparing it to a mock observation using specific distance metrics, and using the results to approximate a Bayesian posterior distribution for two cosmological parameters, $\\boldsymbol{\\theta} = (\\Omega_{\\mathrm{m}}, \\sigma_8)$. The quality of this approximation is assessed by calculating the frequentist coverage of its credible intervals.\n\n### Principle 1: The Forward Generative Model\n\nThe foundation of any simulation-based inference is a forward model that can generate synthetic data given a set of parameters. Here, the model maps the cosmological parameters $\\boldsymbol{\\theta} = (\\Omega_{\\mathrm{m}}, \\sigma_8)$ to a summary statistic, which is a normalized histogram of peak counts.\n\n1.  **Parameter-to-Expectation Mapping**: The parameters $\\boldsymbol{\\theta}$ are first drawn from a uniform prior distribution: $\\Omega_{\\mathrm{m}} \\sim \\mathcal{U}(0.2, 0.4)$ and $\\sigma_8 \\sim \\mathcal{U}(0.6, 1.0)$. These parameters determine the expected number of peak counts, $\\lambda_k$, in each of $K=8$ bins via the phenomenological scaling relation:\n    $$\\lambda_k(\\boldsymbol{\\theta}) = s \\, A_k \\left(\\frac{\\sigma_8}{\\sigma_{8,\\mathrm{ref}}}\\right)^{\\alpha_k} \\left(\\frac{\\Omega_{\\mathrm{m}}}{\\Omega_{\\mathrm{m,ref}}}\\right)^{\\beta_k}$$\n    The constants $s=1$, $\\sigma_{8,\\mathrm{ref}} = 0.8$, $\\Omega_{\\mathrm{m,ref}} = 0.3$, and the functions $A_k$, $\\alpha_k$, and $\\beta_k$ are defined in the problem and encapsulate the abstracted physics of structure formation.\n\n2.  **Stochastic Data Generation**: The actual number of observed counts in each bin, $N_k$, is a random variable. Consistent with counting experiments, it is modeled as an independent draw from a Poisson distribution with the previously calculated expectation:\n    $$N_k \\sim \\mathrm{Poisson}\\big(\\lambda_k(\\boldsymbol{\\theta})\\big), \\quad k = 1,\\dots,K.$$\n\n3.  **Summary Statistic**: To compare datasets, we use a summary statistic, $\\mathbf{p}$, which is the normalized histogram of the counts. This reduces the dimensionality of the data from counts to an empirical probability distribution over the $K$ bins.\n    $$p_k = \\frac{N_k}{\\sum_{j=1}^K N_j + \\delta}$$\n    The small constant $\\delta = 10^{-12}$ ensures the denominator is never zero.\n\nThe algorithmic implementation will require a function that takes a parameter vector $\\boldsymbol{\\theta}$ as input and performs these steps to output a simulated summary statistic $\\mathbf{p}$.\n\n### Principle 2: Likelihood-Free Inference via ABC Rejection Sampling\n\nIn Bayesian inference, we seek the posterior distribution $P(\\boldsymbol{\\theta}|\\mathbf{D}_{\\mathrm{obs}}) \\propto P(\\mathbf{D}_{\\mathrm{obs}}|\\boldsymbol{\\theta}) P(\\boldsymbol{\\theta})$, where $P(\\mathbf{D}_{\\mathrm{obs}}|\\boldsymbol{\\theta})$ is the likelihood and $P(\\boldsymbol{\\theta})$ is the prior. For many complex models in cosmology, the likelihood is intractable or too computationally expensive to evaluate. ABC circumvents direct evaluation of the likelihood.\n\nThe ABC rejection algorithm approximates the posterior by sampling from the prior and accepting only those parameters that generate simulated data \"close\" to the observed data. Closeness is measured by a distance metric $d$ on the summary statistics. The algorithm is:\n\n1.  Given an observed summary statistic $\\mathbf{p}_{\\mathrm{obs}}$, a simulation budget $N_{\\mathrm{sim}}$, and a tolerance $\\epsilon$.\n2.  For $i = 1, \\dots, N_{\\mathrm{sim}}$:\n    a. Sample a parameter vector $\\boldsymbol{\\theta}^{(i)}$ from its prior $P(\\boldsymbol{\\theta})$.\n    b. Generate a synthetic dataset and its summary statistic $\\mathbf{p}^{(i)}$ using the forward model with $\\boldsymbol{\\theta}^{(i)}$.\n    c. Calculate the distance $d\\big(\\mathbf{p}^{(i)}, \\mathbf{p}_{\\mathrm{obs}}\\big)$.\n    d. If $d \\le \\epsilon$, accept $\\boldsymbol{\\theta}^{(i)}$.\n3.  The collection of accepted parameters, $\\{\\boldsymbol{\\theta}^{(i)}\\}_{\\mathrm{accepted}}$, forms a sample from an approximation to the true posterior, $P(\\boldsymbol{\\theta}|\\mathbf{p}_{\\mathrm{obs}})$. If $\\epsilon \\to 0$ and $N_{\\mathrm{sim}} \\to \\infty$, this approximation becomes exact.\n\nThe implementation will consist of a primary function that orchestrates this loop, calling the generative model and the distance functions.\n\n### Principle 3: Distance Metrics for Comparing Data\n\nThe choice of distance metric $d$ is crucial as it implicitly defines the aspects of the data the inference is sensitive to. The problem specifies two metrics for comparing a simulated histogram $\\mathbf{p}$ with an observed one $\\mathbf{q}$:\n\n1.  **Euclidean Distance ($d_{\\mathrm{E}}$)**: This is a standard L2-norm, measuring the pointwise squared differences between the histograms.\n    $$d_{\\mathrm{E}}(\\mathbf{p}, \\mathbf{q}) = \\left(\\sum_{k=1}^K (p_k - q_k)^2\\right)^{1/2}$$\n    It treats each bin difference independently.\n\n2.  **Sliced Wasserstein Distance ($d_{\\mathrm{SW}}$)**: In one dimension, this is equivalent to the first Wasserstein distance, which measures the \"work\" required to transform one distribution into another. It is calculated as the integrated absolute difference between the cumulative distribution functions (CDFs) of the histograms. For discrete distributions with bin width $\\Delta x = 1$:\n    $$d_{\\mathrm{SW}}(\\mathbf{p}, \\mathbf{q}) = \\sum_{k=1}^K \\left|F_{\\mathbf{p}}(k) - F_{\\mathbf{q}}(k)\\right| \\Delta x$$\n    where $F_{\\mathbf{p}}(k) = \\sum_{j=1}^k p_j$ is the CDF. Unlike $d_{\\mathrm{E}}$, this metric is sensitive to the relative positions of bins, penalizing shifts in the distribution's mass more naturally.\n\nSeparate functions will be implemented for each metric. The $d_{\\mathrm{SW}}$ implementation will involve computing the cumulative sum of the histogram probabilities.\n\n### Principle 4: Performance Evaluation via Posterior Coverage\n\nA key diagnostic for any statistical inference method is its calibration. For a Bayesian method, we desire that our $X\\%$ credible intervals contain the true parameter value $X\\%$ of the time over many repeated experiments. This property is called frequentist coverage. A mismatch between nominal and actual coverage indicates a bias or inaccuracy in the posterior approximation.\n\nThe coverage estimation procedure is as follows:\n\n1.  Fix the ground-truth parameters, here $\\boldsymbol{\\theta}_\\star = (0.3, 0.8)$.\n2.  Perform $R=24$ independent replicates. For each replicate:\n    a. Generate a new \"observed\" dataset $\\mathbf{p}_{\\mathrm{obs}}$ from the true parameters $\\boldsymbol{\\theta}_\\star$.\n    b. Run the ABC algorithm to obtain a set of $M$ accepted samples, approximating the posterior.\n    c. If $M=0$, the replicate is declared non-covering.\n    d. If $M>0$, compute the marginal $0.68$ credible intervals for both $\\Omega_{\\mathrm{m}}$ and $\\sigma_8$. This is done by finding the $16$-th and $84$-th percentiles of the accepted samples for each parameter.\n    e. The replicate is \"covering\" if and only if the true value for *both* parameters falls within their respective credible intervals: $\\Omega_{\\mathrm{m},\\star} \\in I_{\\Omega}$ and $\\sigma_{8,\\star} \\in I_{\\sigma}$.\n3.  The estimated coverage for a given test case (a specific combination of $d$, $N_{\\mathrm{sim}}$, and $\\epsilon$) is the fraction of replicates that were covering.\n\nThis entire procedure will be encapsulated in a main loop that iterates through the $R=24$ replicates for each of the six specified test cases, calculates the coverage fraction, and stores the result. The final output is a formatted list of these coverage fractions.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and evaluates an Approximate Bayesian Computation (ABC) procedure\n    for a simplified weak lensing peak-count model, and computes posterior coverage.\n    \"\"\"\n    # Fix the random seed for reproducibility\n    np.random.seed(42)\n\n    # -- Model and Global Constants --\n    K = 8  # Number of peak-height bins\n    DELTA_X = 1.0  # Bin width\n    S_SCALE = 1.0  # Overall scaling factor\n    SIGMA8_REF = 0.8\n    OMEGAM_REF = 0.3\n    DELTA_STAB = 1e-12  # Stabilization constant for normalization\n\n    # Ground-truth parameters for generating \"observed\" data\n    THETA_STAR = np.array([0.3, 0.8])  # (Omega_m_star, sigma_8_star)\n\n    # Prior bounds\n    PRIOR_BOUNDS = {\n        'om': (0.2, 0.4),\n        's8': (0.6, 1.0)\n    }\n\n    # Pre-compute model constants for k = 1,...,K\n    k_vals = np.arange(1, K + 1)\n    A_k = 200.0 * np.exp(-0.4 * (k_vals - 1))\n    ALPHA_k = 1.1 + 0.1 * (k_vals - 1)\n    BETA_k = 0.6 + 0.05 * (k_vals - 1)\n\n    # -- Generative Model and Distance Functions --\n\n    def get_lambda(theta):\n        \"\"\"Calculates the expected counts lambda_k for a given theta.\"\"\"\n        omega_m, sigma_8 = theta\n        lambda_k = S_SCALE * A_k * \\\n                   (sigma_8 / SIGMA8_REF)**ALPHA_k * \\\n                   (omega_m / OMEGAM_REF)**BETA_k\n        return lambda_k\n\n    def generate_summary_stat(theta):\n        \"\"\"Generates a normalized peak-count histogram for a given theta.\"\"\"\n        lambda_k = get_lambda(theta)\n        # Draw counts from a Poisson distribution\n        N_k = np.random.poisson(lambda_k)\n        # Compute the normalized summary statistic\n        total_counts = np.sum(N_k)\n        p_k = N_k / (total_counts + DELTA_STAB)\n        return p_k\n\n    def distance_euclidean(p, q):\n        \"\"\"Computes Euclidean distance between two histograms.\"\"\"\n        return np.linalg.norm(p - q)\n\n    def distance_sw(p, q):\n        \"\"\"Computes 1D Sliced Wasserstein distance between two histograms.\"\"\"\n        cdf_p = np.cumsum(p)\n        cdf_q = np.cumsum(q)\n        return np.sum(np.abs(cdf_p - cdf_q)) * DELTA_X\n\n    # -- ABC and Coverage Estimation --\n\n    def run_abc(p_obs, N_sim, epsilon, distance_func):\n        \"\"\"Performs ABC rejection sampling.\"\"\"\n        accepted_thetas = []\n        # Sample N_sim parameters from the prior\n        thetas_om = np.random.uniform(PRIOR_BOUNDS['om'][0], PRIOR_BOUNDS['om'][1], N_sim)\n        thetas_s8 = np.random.uniform(PRIOR_BOUNDS['s8'][0], PRIOR_BOUNDS['s8'][1], N_sim)\n        \n        for i in range(N_sim):\n            theta_i = np.array([thetas_om[i], thetas_s8[i]])\n            # Generate simulated summary statistic\n            p_sim = generate_summary_stat(theta_i)\n            # Calculate distance\n            dist = distance_func(p_sim, p_obs)\n            # Accept if distance is within tolerance\n            if dist <= epsilon:\n                accepted_thetas.append(theta_i)\n        \n        return np.array(accepted_thetas)\n\n    def check_coverage(accepted_thetas, theta_star):\n        \"\"\"Checks if the true parameters are within the 0.68 credible intervals.\"\"\"\n        if accepted_thetas.shape[0] == 0:\n            return False  # Non-covering if no samples are accepted\n\n        # Calculate 16th and 84th percentiles for each parameter\n        # These define the 68% equal-tailed credible interval\n        ci_om = np.percentile(accepted_thetas[:, 0], [16, 84])\n        ci_s8 = np.percentile(accepted_thetas[:, 1], [16, 84])\n\n        # Check if true parameters fall within their respective intervals\n        om_covered = (ci_om[0] <= theta_star[0] <= ci_om[1])\n        s8_covered = (ci_s8[0] <= theta_star[1] <= ci_s8[1])\n\n        # Replicate is covering only if both parameters are covered\n        return om_covered and s8_covered\n\n    # -- Test Suite Execution --\n    R = 24  # Number of replicates per test case\n    test_cases = [\n        ('euclidean', 400, 0.12),\n        ('euclidean', 1200, 0.12),\n        ('euclidean', 1200, 0.08),\n        ('sw', 400, 0.40),\n        ('sw', 1200, 0.40),\n        ('sw', 1200, 0.28),\n    ]\n\n    distance_map = {\n        'euclidean': distance_euclidean,\n        'sw': distance_sw\n    }\n\n    results = []\n    for d_name, N_sim, epsilon in test_cases:\n        n_covering_replicates = 0\n        dist_func = distance_map[d_name]\n        \n        for _ in range(R):\n            # 1. Generate a new \"observed\" data set from the ground truth\n            p_obs = generate_summary_stat(THETA_STAR)\n            \n            # 2. Run ABC to get posterior samples\n            accepted_samples = run_abc(p_obs, N_sim, epsilon, dist_func)\n            \n            # 3. Check if the replicate covers the true parameters\n            if check_coverage(accepted_samples, THETA_STAR):\n                n_covering_replicates += 1\n        \n        # 4. Calculate coverage fraction for the test case\n        coverage = n_covering_replicates / R\n        results.append(f\"{coverage:.3f}\")\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n\n```", "id": "3489626"}, {"introduction": "While simple ABC is powerful, its representation of the posterior as a collection of samples can be coarse. Modern SBI leverages expressive neural networks, such as Normalizing Flows, to learn a continuous and analytically defined posterior density. This exercise challenges you to analyze the architectural requirements for a flow-based model to capture complex posterior features, like the multimodality that can arise from physical symmetries in cosmological models, providing insight into the capabilities and limits of these advanced techniques [@problem_id:3489687].", "problem": "Consider a Simulation-Based Inference (SBI) task in numerical cosmology where the goal is to learn the posterior density $p(\\boldsymbol{\\theta}\\,|\\,\\boldsymbol{d})$ over cosmological and astrophysical parameters $\\boldsymbol{\\theta} = (\\theta_1,\\theta_2) \\in \\mathbb{R}^2$ given a fixed summary $\\boldsymbol{d}$ of Large-Scale Structure (LSS) data. Assume the forward simulator and prior together induce a symmetry under $\\theta_1 \\mapsto -\\theta_1$, and that the true posterior is smooth, strictly positive, and exhibits two well-separated, symmetric modes near $(+\\mu,0)$ and $(-\\mu,0)$ for some $\\mu > 0$, with a low-density valley around $\\theta_1 \\approx 0$ (so that the marginal over $\\theta_1$ is strongly bimodal). You are tasked with designing an expressive but minimal invertible Normalizing Flow (NF) based on affine coupling layers of the Real Non-Volume Preserving (RealNVP) type to represent this posterior starting from a standard Gaussian base $p_{\\boldsymbol{Z}}(\\boldsymbol{z}) = \\mathcal{N}(\\boldsymbol{0}, \\mathbf{I}_2)$.\n\nA RealNVP affine coupling layer on $\\boldsymbol{z}=(z_1,z_2)$ with a binary mask $\\boldsymbol{m} \\in \\{0,1\\}^2$ leaves the subset with $m_i=1$ unchanged and transforms the complementary subset with an affine map whose scale and shift are arbitrary functions of the unchanged subset, i.e., for the mask $\\boldsymbol{m}=(1,0)$,\n$$\ny_1 \\equiv z_1,\\quad\ny_2 \\equiv z_2 \\,\\exp\\big(s_1(z_1)\\big) + t_1(z_1),\n$$\nand for the mask $\\boldsymbol{m}=(0,1)$,\n$$\nx_1 \\equiv y_1 \\,\\exp\\big(s_2(y_2)\\big) + t_2(y_2),\\quad\nx_2 \\equiv y_2,\n$$\nwhere $s_\\ell(\\cdot)$ and $t_\\ell(\\cdot)$ are outputs of conditioner neural networks. The overall flow $f:\\mathbb{R}^2 \\to \\mathbb{R}^2$ is the composition of such layers and is a diffeomorphism with triangular Jacobian blocks at each layer. The model density is given by the change-of-variables formula\n$$\np_{\\boldsymbol{X}}(\\boldsymbol{x}) = p_{\\boldsymbol{Z}}\\!\\big(f^{-1}(\\boldsymbol{x})\\big)\\,\\left|\\det \\mathbf{J}_{f^{-1}}(\\boldsymbol{x})\\right|.\n$$\n\nFrom first principles and the above definitions, analyze the expressivity required to represent the stated symmetric bimodal posterior, and identify a minimal architecture that can capture two separated modes. Which of the following statements are correct?\n\nA. A single affine coupling layer with mask $\\boldsymbol{m}=(1,0)$ and any choice of conditioner functions $s_1(\\cdot), t_1(\\cdot)$ is sufficient to exactly represent the target posterior with two separated modes centered near $(\\pm \\mu,0)$.\n\nB. Two affine coupling layers with alternating masks $\\boldsymbol{m}=(1,0)$ then $\\boldsymbol{m}=(0,1)$, and with conditioner networks that are universal approximators on compact subsets of $\\mathbb{R}$ (e.g., one hidden layer with a nonpolynomial activation), are sufficient in principle to represent the symmetric bimodal posterior to arbitrary accuracy while preserving invertibility.\n\nC. No continuous, everywhere-invertible flow $f$ with a strictly positive base density $p_{\\boldsymbol{Z}}(\\boldsymbol{z})>0$ for all $\\boldsymbol{z}\\in\\mathbb{R}^2$ can produce a model density $p_{\\boldsymbol{X}}(\\boldsymbol{x})$ that has exactly zero density along any nontrivial curve separating the two modes; achieving disconnected support would require augmenting the model class (e.g., a mixture base or a discrete latent variable).\n\nD. Replacing affine coupling with elementwise strictly monotone spline transforms applied independently to each coordinate (no cross-dimensional coupling) and using a single transform per coordinate is sufficient to represent a two-dimensional bimodal posterior whose two modes are not axis-aligned with the coordinate axes.\n\nE. In two dimensions, any composition of a finite number of affine coupling layers with smooth conditioner networks cannot increase the number of modes beyond that of the base; thus, representing multimodal posteriors with a standard Gaussian base is impossible without introducing non-invertible mappings.", "solution": "The user wants me to validate the problem statement and, if valid, provide a detailed solution.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\n- **Task**: Learn the posterior density $p(\\boldsymbol{\\theta}\\,|\\,\\boldsymbol{d})$ for parameters $\\boldsymbol{\\theta} = (\\theta_1, \\theta_2) \\in \\mathbb{R}^2$ given a data summary $\\boldsymbol{d}$.\n- **Context**: Simulation-Based Inference (SBI) in numerical cosmology.\n- **Target Posterior Properties**:\n    - The simulator and prior induce symmetry under the transformation $\\theta_1 \\mapsto -\\theta_1$.\n    - The true posterior is smooth, strictly positive, with two well-separated, symmetric modes near $(+\\mu, 0)$ and $(-\\mu, 0)$ for some $\\mu > 0$.\n    - The posterior has a low-density valley around $\\theta_1 \\approx 0$.\n    - The marginal distribution over $\\theta_1$ is strongly bimodal.\n- **Model Architecture**: An invertible Normalizing Flow (NF) of the Real Non-Volume Preserving (RealNVP) type.\n- **Base Distribution**: A standard Gaussian in two dimensions, $p_{\\boldsymbol{Z}}(\\boldsymbol{z}) = \\mathcal{N}(\\boldsymbol{0}, \\mathbf{I}_2)$.\n- **RealNVP Affine Coupling Layer Definition**: For a mask $\\boldsymbol{m}=(1,0)$, the map is $(z_1, z_2) \\mapsto (y_1, y_2)$ where $y_1 \\equiv z_1$ and $y_2 \\equiv z_2 \\,\\exp\\big(s_1(z_1)\\big) + t_1(z_1)$. For a mask $\\boldsymbol{m}=(0,1)$, the map is $(y_1, y_2) \\mapsto (x_1, x_2)$ where $x_1 \\equiv y_1 \\,\\exp\\big(s_2(y_2)\\big) + t_2(y_2)$ and $x_2 \\equiv y_2$. The functions $s_\\ell(\\cdot)$ and $t_\\ell(\\cdot)$ are arbitrary functions provided by conditioner networks.\n- **Model Density**: The density is given by the change-of-variables formula: $p_{\\boldsymbol{X}}(\\boldsymbol{x}) = p_{\\boldsymbol{Z}}\\!\\big(f^{-1}(\\boldsymbol{x})\\big)\\,\\left|\\det \\mathbf{J}_{f^{-1}}(\\boldsymbol{x})\\right|$, where $f$ is the composite flow map.\n\n**Step 2: Validate Using Extracted Givens**\n\n- **Scientific Grounding**: The problem is well-grounded in modern statistical machine learning and its application to scientific domains like cosmology. Normalizing Flows, RealNVP, Simulation-Based Inference, and Bayesian posteriors are all standard, mathematically rigorous concepts. The scenario of a bimodal posterior is common in real-world inference problems. The setup is scientifically and mathematically sound.\n- **Well-Posedness**: The problem is well-posed. It asks for an analysis of the expressive capacity of a specific, well-defined class of models (RealNVP flows) to represent a target distribution with specific, clearly stated properties (symmetric, bimodal). This is a standard and meaningful question in the study of generative models.\n- **Objectivity**: The problem is stated using precise mathematical definitions and objective descriptions. There is no ambiguity, subjectivity, or opinion-based language.\n\n**Step 3: Verdict and Action**\n\nThe problem statement is valid. It is scientifically sound, well-posed, and objective, providing all necessary information for a rigorous analysis. Proceeding to solution.\n\n### Solution Derivation\n\nLet the base space be $\\mathcal{Z} = \\mathbb{R}^2$ with coordinates $\\boldsymbol{z}=(z_1, z_2)$ and the target space be $\\mathcal{X} = \\mathbb{R}^2$ with coordinates $\\boldsymbol{x}=(x_1, x_2)$, which we identify with the cosmological parameters $\\boldsymbol{\\theta}=(\\theta_1, \\theta_2)$. The base density is the standard normal distribution $p_{\\boldsymbol{Z}}(\\boldsymbol{z}) = (2\\pi)^{-1} \\exp\\left(-\\frac{1}{2}(z_1^2 + z_2^2)\\right)$, which is unimodal with its peak at the origin. The normalizing flow $f: \\mathcal{Z} \\to \\mathcal{X}$ is a diffeomorphism. The core of the problem is to assess whether the specific class of diffeomorphisms built from RealNVP layers can transform the unimodal base distribution into the specified bimodal target distribution.\n\n#### **Analysis of Option A**\n\nA single affine coupling layer with mask $\\boldsymbol{m}=(1,0)$ defines the transformation $f: \\boldsymbol{z} \\mapsto \\boldsymbol{x}$ as:\n$$\nx_1 = z_1\n$$\n$$\nx_2 = z_2 \\exp\\big(s_1(z_1)\\big) + t_1(z_1)\n$$\nThe marginal distribution of the transformed variable $X_1$ is obtained by integrating the joint density $p_{\\boldsymbol{X}}(x_1, x_2)$ over $x_2$. However, we can analyze this more directly. Since $x_1 = z_1$, and the marginal distribution of $Z_1$ is the standard normal distribution $\\mathcal{N}(0,1)$, the marginal distribution of $X_1$ must also be $\\mathcal{N}(0,1)$.\n$$\np_{X_1}(x_1) = p_{Z_1}(z_1) = \\mathcal{N}(x_1; 0, 1)\n$$\nA standard normal distribution is unimodal. The problem states that the target posterior has a strongly bimodal marginal over $\\theta_1$ (which we have identified with $x_1$). A single coupling layer that leaves the first coordinate unchanged cannot create bimodality in that coordinate's marginal distribution. It only applies an affine transformation to $z_2$ conditioned on $z_1$, which can be viewed as shearing and scaling the base distribution parallel to the $z_2$-axis. This cannot split the single mode of the $z_1$ marginal distribution into two.\n\n**Verdict: Incorrect.**\n\n#### **Analysis of Option B**\n\nThis option proposes a composition of two affine coupling layers, $f = f_2 \\circ f_1$, with alternating masks.\nLet $f_1$ have mask $\\boldsymbol{m}=(1,0)$, mapping $\\boldsymbol{z} \\mapsto \\boldsymbol{y}$:\n$$\ny_1 = z_1\n$$\n$$\ny_2 = z_2 \\exp\\big(s_1(z_1)\\big) + t_1(z_1)\n$$\nLet $f_2$ have mask $\\boldsymbol{m}=(0,1)$, mapping $\\boldsymbol{y} \\mapsto \\boldsymbol{x}$:\n$$\nx_1 = y_1 \\exp\\big(s_2(y_2)\\big) + t_2(y_2)\n$$\n$$\nx_2 = y_2\n$$\nThe key is the second transformation. The variable $x_1$ is now a function of both $y_1$ and $y_2$. Since $y_1 = z_1$ and $y_2$ is a function of $z_1$ and $z_2$, $x_1$ is a non-trivial function of both base variables. This composition allows for much greater flexibility than a single layer. We can create bimodality by choosing the conditioner networks appropriately. For example, let $s_1, t_1, s_2$ all be zero functions for simplicity. The map becomes $y_1=z_1$, $y_2=z_2$, and $x_1=z_1+t_2(z_2)$, $x_2=z_2$.\nIf we choose $t_2(\\cdot)$ to be a function that splits the space, for example $t_2(z_2) \\approx \\mu \\cdot \\text{sign}(z_2)$, which can be approximated by $\\mu \\tanh(k z_2)$ for large $k$ using a neural network, then:\n- For $z_2 > 0$ (half of the probability mass), $x_1 \\approx z_1 + \\mu$.\n- For $z_2 < 0$ (the other half), $x_1 \\approx z_1 - \\mu$.\nSince $Z_1 \\sim \\mathcal{N}(0,1)$, the marginal distribution of $X_1$ becomes an approximation of a mixture of two Gaussians, $\\frac{1}{2}\\mathcal{N}(\\mu, 1) + \\frac{1}{2}\\mathcal{N}(-\\mu, 1)$, which is bimodal for sufficiently large $\\mu$. The second layer $f_2$ has created bimodality in the $x_1$ direction by using information from the other dimension ($y_2=z_2$). The first layer $f_1$ can then be used to shape the distribution in the $x_2$ dimension (e.g., by choosing $t_1(z_1)$ appropriately to ensure the modes are centered near $x_2=0$). Since the conditioner networks are universal approximators, this architecture has sufficient expressivity in principle to approximate the target posterior to arbitrary accuracy.\n\n**Verdict: Correct.**\n\n#### **Analysis of Option C**\n\nThe model density is given by $p_{\\boldsymbol{X}}(\\boldsymbol{x}) = p_{\\boldsymbol{Z}}\\!\\big(f^{-1}(\\boldsymbol{x})\\big)\\,\\left|\\det \\mathbf{J}_{f^{-1}}(\\boldsymbol{x})\\right|$.\nThe base density $p_{\\boldsymbol{Z}}(\\boldsymbol{z})$ is a standard Gaussian, which is strictly positive for all $\\boldsymbol{z} \\in \\mathbb{R}^2$. So, $p_{\\boldsymbol{Z}}(\\boldsymbol{w}) > 0$ for any $\\boldsymbol{w} \\in \\mathbb{R}^2$.\nThe map $f$ is a diffeomorphism composed of affine coupling layers. The Jacobian of a single such layer is lower or upper triangular with $1$s and exponentials on the diagonal. For instance, for the mask $\\boldsymbol{m}=(1,0)$, the Jacobian matrix is $\\begin{pmatrix} 1 & 0 \\\\ \\partial x_2/\\partial z_1 & \\exp(s_1(z_1)) \\end{pmatrix}$, and its determinant is $\\exp(s_1(z_1))$. Since the exponential function is always strictly positive, the Jacobian determinant of any single layer is strictly positive everywhere. The Jacobian of a composition of flows is the product of the individual Jacobians, so its determinant is the product of the individual determinants, which is also strictly positive. Thus, $\\det \\mathbf{J}_f(\\boldsymbol{z}) > 0$ for all $\\boldsymbol{z}$.\nConsequently, its inverse, $\\det \\mathbf{J}_{f^{-1}}(\\boldsymbol{x}) = 1 / (\\det \\mathbf{J}_f(f^{-1}(\\boldsymbol{x})))$, is also strictly positive for all $\\boldsymbol{x}$.\nThe model density $p_{\\boldsymbol{X}}(\\boldsymbol{x})$ is the product of two strictly positive terms: $p_{\\boldsymbol{Z}}(\\cdot) > 0$ and $|\\det \\mathbf{J}_{f^{-1}}(\\cdot)| > 0$. Therefore, $p_{\\boldsymbol{X}}(\\boldsymbol{x}) > 0$ for all $\\boldsymbol{x} \\in \\mathbb{R}^2$. The density can be arbitrarily small in the \"valley\" between modes, but it can never be exactly zero.\nThis implies that the support of the learned distribution is the entire (connected) space $\\mathbb{R}^2$. Creating disconnected modes (i.e., regions of non-zero probability separated by regions of exactly zero probability) is impossible with this setup. To do so, one would need to alter the model, for instance by using a base distribution with disconnected support or by introducing discrete latent variables to model a mixture distribution, as the statement correctly suggests.\n\n**Verdict: Correct.**\n\n#### **Analysis of Option D**\n\nThis option proposes a transformation where each coordinate is transformed independently:\n$$\nx_1 = g_1(z_1)\n$$\n$$\nx_2 = g_2(z_2)\n$$\nHere, $g_1$ and $g_2$ are strictly monotone functions (splines). This is a factorized transformation, meaning it does not introduce any dependencies between the coordinates. If the base distribution is factorized, as the standard Gaussian is ($p_{\\boldsymbol{Z}}(z_1, z_2) = p_{Z_1}(z_1)p_{Z_2}(z_2)$), then the resulting distribution will also be factorized:\n$$\np_{\\boldsymbol{X}}(x_1, x_2) = p_{X_1}(x_1)p_{X_2}(x_2)\n$$\nA probability distribution that factorizes in this manner has statistically independent variables $X_1$ and $X_2$. This implies that their covariance is zero, $\\text{Cov}(X_1, X_2) = 0$.\nThe option asks if this model can represent a bimodal posterior \"whose two modes are not axis-aligned\". Non-axis-aligned modes, for example at $(\\mu, \\nu)$ and $(-\\mu, -\\nu)$ with $\\mu, \\nu \\ne 0$, would imply a correlation between the variables, which a factorized distribution cannot have. The principal axes of the probability contours of a factorized distribution are always aligned with the coordinate axes. Therefore, such an architecture is insufficient for representing general bimodal posteriors with correlated modes.\n\n**Verdict: Incorrect.**\n\n#### **Analysis of Option E**\n\nThis statement claims that any composition of affine coupling layers cannot increase the number of modes beyond that of the base distribution (which is one). This is a statement about the topology of the transformation. A diffeomorphism is a homeomorphism, which preserves topological properties like connectedness. The support of the base distribution is $\\mathbb{R}^2$, which is connected. The flow maps this to $\\mathbb{R}^2$, which is also connected. This is consistent with Option C.\nHowever, the number of modes (local maxima) of a density function is *not* a topological invariant preserved by a general homeomorphism. A flow can stretch and compress the probability space in a way that creates new maxima and minima. Our analysis for Option B already demonstrated constructively how two affine coupling layers can turn a unimodal Gaussian into a bimodal distribution. The function $x_1 \\approx z_1 + \\mu \\tanh(k z_2)$ effectively \"folds\" the probability mass along the $z_1$ dimension based on the value of $z_2$, creating two peaks from one. Therefore, the claim that it is impossible to increase the number of modes is false.\n\n**Verdict: Incorrect.**", "answer": "$$\\boxed{BC}$$", "id": "3489687"}, {"introduction": "In numerical cosmology, simulations are often the most computationally expensive part of an analysis. This practice advances from the inference algorithm itself to the strategic problem of Bayesian experimental design: how to allocate a finite simulation budget to learn as much as possible about the parameters. You will apply principles from information theory and optimization to derive an optimal simulation strategy, demonstrating how to make the SBI process more efficient and powerful in practice [@problem_id:3489624].", "problem": "Consider a simulation-based design problem for Simulation-Based Inference (SBI) in numerical cosmology. The cosmological parameter space is partitioned into two disjoint regions, denoted by region $1$ and region $2$, with prior masses $w_{1}$ and $w_{2}$ that satisfy $w_{1} + w_{2} = 1$. For region $k \\in \\{1,2\\}$, the forward model admits a locally linear Gaussian approximation for a fixed summary statistic, and the mutual information between the parameter and $n_{k}$ independent simulations drawn using the design focused on region $k$ is accurately approximated by\n$$\n\\frac{1}{2} w_{k} \\ln\\!\\big(1 + \\lambda_{k} n_{k}\\big),\n$$\nwhere $\\lambda_{k} > 0$ is an information-rate parameter that depends on the local Fisher information of the design and the prior variance restricted to region $k$. Assume that simulations across regions are conditionally independent given the region index and additively contribute to the expected information gain, so that the total mutual information for an allocation $\\{n_{1}, n_{2}\\}$ is\n$$\n\\mathcal{I}(n_{1}, n_{2}) = \\frac{1}{2} \\sum_{k=1}^{2} w_{k} \\ln\\!\\big(1 + \\lambda_{k} n_{k}\\big).\n$$\nYou are given a total simulation budget of $N$ units, so that $n_{1} + n_{2} = N$ and $n_{k} \\ge 0$. Assume the continuous relaxation of $n_{k}$ is appropriate for allocating a large batch and that both regions are active at the optimum (so the Lagrange multiplier optimality conditions hold with equality for both $k=1,2$).\n\nPart A. Starting from the definitions of mutual information for Gaussian-linear experiments and the additivity of information across independent designs, derive the optimal allocation $\\{n_{1}^{\\star}, n_{2}^{\\star}\\}$ that maximizes $\\mathcal{I}(n_{1}, n_{2})$ subject to $n_{1} + n_{2} = N$. Express your solution in closed form in terms of $w_{1}$, $w_{2}$, $\\lambda_{1}$, $\\lambda_{2}$, and $N$.\n\nPart B. Specialize to the numerically specified, scientifically plausible case\n$$\nw_{1} = 0.6,\\quad w_{2} = 0.4,\\quad \\lambda_{1} = 0.02,\\quad \\lambda_{2} = 0.05,\\quad N = 120.\n$$\nCompute the corresponding optimal allocation $\\{n_{1}^{\\star}, n_{2}^{\\star}\\}$.\n\nPart C. Now suppose you implement a one-step myopic policy to choose the next region using Thompson sampling on the marginal information increment random variables. At the current allocation $\\{n_{1}, n_{2}\\}$, define the one-step marginal information increment for region $k$ as\n$$\n\\Delta I_{k}(\\lambda_{k}; n_{k}) = \\frac{1}{2} w_{k} \\ln\\!\\left(\\frac{1 + \\lambda_{k} (n_{k} + 1)}{1 + \\lambda_{k} n_{k}}\\right).\n$$\nAssume that due to Monte Carlo estimation noise, you maintain independent Gaussian posteriors for these increments,\n$$\n\\Delta I_{k} \\sim \\mathcal{N}\\!\\big(\\mu_{k}, s_{k}^{2}\\big),\n$$\nwith posterior means set to the plug-in values $\\mu_{k} = \\Delta I_{k}(\\lambda_{k}; n_{k})$ at the current $n_{k}$, and known posterior variances $s_{1}^{2} = 1.0 \\times 10^{-6}$ and $s_{2}^{2} = 0.8 \\times 10^{-6}$. Under Thompson sampling, you draw one sample from each posterior and choose the region with the larger draw. For $K = 2$ regions, the probability that region $1$ is selected next is\n$$\np = \\mathbb{P}\\!\\big(X_{1} > X_{2}\\big) = \\Phi\\!\\left(\\frac{\\mu_{1} - \\mu_{2}}{\\sqrt{s_{1}^{2} + s_{2}^{2}}}\\right),\n$$\nwhere $X_{k} \\sim \\mathcal{N}(\\mu_{k}, s_{k}^{2})$ independently and $\\Phi$ is the standard normal cumulative distribution function. Using the optimal allocation obtained in Part B as the current allocation $\\{n_{1}, n_{2}\\}$, compute $p$ as a decimal. Round $p$ to four significant figures.\n\nProvide your final answer as a row vector $\\big(n_{1}^{\\star}, n_{2}^{\\star}, p\\big)$ using the $\\pmatrix{\\cdot & \\cdot & \\cdot}$ format. Express the probability $p$ as a decimal (no percent sign). If any numerical rounding is required, follow the stated significant-figure rule strictly.", "solution": "The problem asks for the optimal allocation of a simulation budget in a cosmological context, and then for the calculation of a decision probability under a myopic Thompson sampling policy. The problem is presented in three parts, which we will address sequentially.\n\n### Part A: Optimal Allocation\n\nThe goal is to maximize the total mutual information $\\mathcal{I}(n_{1}, n_{2})$ subject to a budget constraint.\nThe objective function is:\n$$\n\\mathcal{I}(n_{1}, n_{2}) = \\frac{1}{2} \\sum_{k=1}^{2} w_{k} \\ln(1 + \\lambda_{k} n_{k}) = \\frac{1}{2} w_{1} \\ln(1 + \\lambda_{1} n_{1}) + \\frac{1}{2} w_{2} \\ln(1 + \\lambda_{2} n_{2})\n$$\nThe constraint is that the total number of simulations is fixed at $N$:\n$$\nn_{1} + n_{2} = N\n$$\nWe are also given that $n_{k} \\ge 0$ for $k \\in \\{1, 2\\}$. The problem states that the optimal solution is interior (i.e., $n_{k}^{\\star} > 0$), which allows us to use the method of Lagrange multipliers without considering boundary constraints.\n\nWe form the Lagrangian $\\mathcal{L}$ by introducing a Lagrange multiplier $\\mu$:\n$$\n\\mathcal{L}(n_{1}, n_{2}, \\mu) = \\mathcal{I}(n_{1}, n_{2}) - \\mu(n_{1} + n_{2} - N)\n$$\nTo find the optimal allocation $\\{n_{1}^{\\star}, n_{2}^{\\star}\\}$, we set the partial derivatives of $\\mathcal{L}$ with respect to $n_{1}$, $n_{2}$, and $\\mu$ to zero.\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial n_{1}} = \\frac{1}{2} \\frac{w_{1} \\lambda_{1}}{1 + \\lambda_{1} n_{1}} - \\mu = 0\n$$\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial n_{2}} = \\frac{1}{2} \\frac{w_{2} \\lambda_{2}}{1 + \\lambda_{2} n_{2}} - \\mu = 0\n$$\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\mu} = N - n_{1} - n_{2} = 0\n$$\nFrom the first two equations, we can express $\\mu$ in two ways and set them equal. This equates the marginal information gain per simulation for each region, scaled by a constant factor of $1/2$.\n$$\n\\mu = \\frac{1}{2} \\frac{w_{1} \\lambda_{1}}{1 + \\lambda_{1} n_{1}} = \\frac{1}{2} \\frac{w_{2} \\lambda_{2}}{1 + \\lambda_{2} n_{2}}\n$$\nLet's define a constant $C' = 2\\mu$. Then we have:\n$$\n\\frac{w_{1} \\lambda_{1}}{1 + \\lambda_{1} n_{1}} = C' \\implies 1 + \\lambda_{1} n_{1} = \\frac{w_{1} \\lambda_{1}}{C'} \\implies n_{1} = \\frac{w_{1}}{C'} - \\frac{1}{\\lambda_{1}}\n$$\n$$\n\\frac{w_{2} \\lambda_{2}}{1 + \\lambda_{2} n_{2}} = C' \\implies 1 + \\lambda_{2} n_{2} = \\frac{w_{2} \\lambda_{2}}{C'} \\implies n_{2} = \\frac{w_{2}}{C'} - \\frac{1}{\\lambda_{2}}\n$$\nNow we use the budget constraint $n_{1} + n_{2} = N$:\n$$\n\\left(\\frac{w_{1}}{C'} - \\frac{1}{\\lambda_{1}}\\right) + \\left(\\frac{w_{2}}{C'} - \\frac{1}{\\lambda_{2}}\\right) = N\n$$\n$$\n\\frac{w_{1} + w_{2}}{C'} - \\left(\\frac{1}{\\lambda_{1}} + \\frac{1}{\\lambda_{2}}\\right) = N\n$$\nUsing the given condition $w_{1} + w_{2} = 1$:\n$$\n\\frac{1}{C'} = N + \\frac{1}{\\lambda_{1}} + \\frac{1}{\\lambda_{2}}\n$$\nNow we can substitute this expression for $1/C'$ back into the equations for $n_{1}$ and $n_{2}$ to find the optimal allocation, denoted by $\\{n_{1}^{\\star}, n_{2}^{\\star}\\}$:\n$$\nn_{1}^{\\star} = w_{1} \\left(N + \\frac{1}{\\lambda_{1}} + \\frac{1}{\\lambda_{2}}\\right) - \\frac{1}{\\lambda_{1}}\n$$\n$$\nn_{2}^{\\star} = w_{2} \\left(N + \\frac{1}{\\lambda_{1}} + \\frac{1}{\\lambda_{2}}\\right) - \\frac{1}{\\lambda_{2}}\n$$\nThese are the closed-form expressions for the optimal allocation.\n\n### Part B: Numerical Calculation of Optimal Allocation\n\nWe are given the following numerical values:\n$$\nw_{1} = 0.6, \\quad w_{2} = 0.4, \\quad \\lambda_{1} = 0.02, \\quad \\lambda_{2} = 0.05, \\quad N = 120\n$$\nFirst, we calculate the inverse information-rate parameters:\n$$\n\\frac{1}{\\lambda_{1}} = \\frac{1}{0.02} = 50\n$$\n$$\n\\frac{1}{\\lambda_{2}} = \\frac{1}{0.05} = 20\n$$\nNow, we can compute the term in the parenthesis from the Part A solution:\n$$\nN + \\frac{1}{\\lambda_{1}} + \\frac{1}{\\lambda_{2}} = 120 + 50 + 20 = 190\n$$\nSubstituting these values into the expressions for $n_{1}^{\\star}$ and $n_{2}^{\\star}$:\n$$\nn_{1}^{\\star} = w_{1} \\left(190\\right) - \\frac{1}{\\lambda_{1}} = 0.6 \\times 190 - 50 = 114 - 50 = 64\n$$\n$$\nn_{2}^{\\star} = w_{2} \\left(190\\right) - \\frac{1}{\\lambda_{2}} = 0.4 \\times 190 - 20 = 76 - 20 = 56\n$$\nAs a check, we confirm that $n_{1}^{\\star} + n_{2}^{\\star} = 64 + 56 = 120 = N$. The optimal allocation is $\\{64, 56\\}$.\n\n### Part C: Thompson Sampling Probability\n\nWe now consider a one-step myopic policy where the current allocation is the optimal allocation found in Part B, i.e., $n_{1} = 64$ and $n_{2} = 56$. We need to compute the probability $p$ that region $1$ is chosen next, given by:\n$$\np = \\mathbb{P}(X_{1} > X_{2}) = \\Phi\\left(\\frac{\\mu_{1} - \\mu_{2}}{\\sqrt{s_{1}^{2} + s_{2}^{2}}}\\right)\n$$\nwhere $\\Phi$ is the standard normal cumulative distribution function (CDF).\nThe means $\\mu_{k}$ are given by the marginal information increments:\n$$\n\\mu_{k} = \\Delta I_{k}(\\lambda_{k}; n_{k}) = \\frac{1}{2} w_{k} \\ln\\left(\\frac{1 + \\lambda_{k} (n_{k} + 1)}{1 + \\lambda_{k} n_{k}}\\right)\n$$\nLet's calculate $\\mu_{1}$ and $\\mu_{2}$ using the values from Part B:\nFor region $1$: $n_{1} = 64, w_{1} = 0.6, \\lambda_{1} = 0.02$.\n$$\n\\mu_{1} = \\frac{1}{2} (0.6) \\ln\\left(\\frac{1 + 0.02 (64 + 1)}{1 + 0.02 \\times 64}\\right) = 0.3 \\ln\\left(\\frac{1 + 1.3}{1 + 1.28}\\right) = 0.3 \\ln\\left(\\frac{2.3}{2.28}\\right)\n$$\nFor region $2$: $n_{2} = 56, w_{2} = 0.4, \\lambda_{2} = 0.05$.\n$$\n\\mu_{2} = \\frac{1}{2} (0.4) \\ln\\left(\\frac{1 + 0.05 (56 + 1)}{1 + 0.05 \\times 56}\\right) = 0.2 \\ln\\left(\\frac{1 + 2.85}{1 + 2.8}\\right) = 0.2 \\ln\\left(\\frac{3.85}{3.8}\\right)\n$$\nNow we compute their numerical values:\n$$\n\\mu_{1} \\approx 0.3 \\times \\ln(1.0087719) \\approx 0.3 \\times 0.008733602 = 0.0026200806\n$$\n$$\n\\mu_{2} \\approx 0.2 \\times \\ln(1.0131579) \\approx 0.2 \\times 0.013071942 = 0.0026143884\n$$\nThe difference in the means is:\n$$\n\\mu_{1} - \\mu_{2} \\approx 0.0026200806 - 0.0026143884 = 0.0000056922\n$$\nThe posterior variances are given as $s_{1}^{2} = 1.0 \\times 10^{-6}$ and $s_{2}^{2} = 0.8 \\times 10^{-6}$. The sum of the variances is:\n$$\ns_{1}^{2} + s_{2}^{2} = 1.0 \\times 10^{-6} + 0.8 \\times 10^{-6} = 1.8 \\times 10^{-6}\n$$\nThe standard deviation of the difference is:\n$$\n\\sqrt{s_{1}^{2} + s_{2}^{2}} = \\sqrt{1.8 \\times 10^{-6}} \\approx 0.0013416408\n$$\nNow we compute the argument of the standard normal CDF, which we denote by $z$:\n$$\nz = \\frac{\\mu_{1} - \\mu_{2}}{\\sqrt{s_{1}^{2} + s_{2}^{2}}} \\approx \\frac{0.0000056922}{0.0013416408} \\approx 0.00424285\n$$\nFinally, we compute $p = \\Phi(z)$:\n$$\np = \\Phi(0.00424285) \\approx 0.5016922\n$$\nThe problem requires rounding $p$ to four significant figures.\nThe first four significant figures of $0.5016922...$ are $5, 0, 1, 6$. The next digit is $9$, so we round the last significant figure up.\n$$\np \\approx 0.5017\n$$\nThe final answer is the row vector $(n_{1}^{\\star}, n_{2}^{\\star}, p)$.\n$$\n(64, 56, 0.5017)\n$$", "answer": "$$ \\boxed{ \\begin{pmatrix} 64 & 56 & 0.5017 \\end{pmatrix} } $$", "id": "3489624"}]}