{"hands_on_practices": [{"introduction": "The first step in any numerical measurement is to verify your tools. This practice guides you through a fundamental validation test: measuring the two-point correlation function (2PCF) for a completely random (Poisson) distribution of points. Since a random field has no intrinsic clustering, the true correlation function $\\xi(r)$ is zero, allowing you to test if your estimator is unbiased and to understand the nature of statistical noise ([@problem_id:3499918]).", "problem": "You are asked to design and implement a numerical experiment to test the unbiasedness and sampling distribution of an estimator of the two-point correlation function in a homogeneous Poisson point process within a three-dimensional periodic box.\n\nBase definitions and assumptions:\n- The two-point correlation function (2PCF) is defined by the statement that for a statistically homogeneous and isotropic point field of number density $n$ in a volume $V$, the differential probability to find a pair of points in infinitesimal volume elements $dV_1$ and $dV_2$ separated by a distance $r$ is $n^2\\left[1+\\xi(r)\\right]\\,dV_1\\,dV_2$. For a homogeneous Poisson process, $\\xi(r)=0$ for all $r$.\n- Consider a periodic cube of side length $L$, total volume $V=L^3$, containing $N$ points drawn from a Poisson process with fixed $N$ and uniform spatial distribution, so that the number density is $n=N/V$. Use periodic boundary conditions with the minimal image convention to define pair separations.\n- The estimator $\\hat{\\xi}(r)$ is to be constructed from first principles by comparing the observed number of pairs in a separation bin with the expected number of pairs for a spatially uniform (Poisson) distribution. For a bin with inner radius $r_{\\min}$ and outer radius $r_{\\max}$, the expected number of pairs per realization under the Poisson hypothesis is $\\mathbb{E}[RR]=\\frac{N(N-1)}{2}\\times \\frac{\\frac{4\\pi}{3}\\left(r_{\\max}^{3}-r_{\\min}^{3}\\right)}{V}$. The estimator $\\hat{\\xi}(r)$ is defined as the ratio of the observed pair count in that bin to $\\mathbb{E}[RR]$, minus $1$.\n- For a homogeneous Poisson process, the counting noise of the observed pairs in a bin is well-approximated by a Poisson distribution with mean $\\mathbb{E}[RR]$ provided $\\mathbb{E}[RR]$ is sufficiently large. Consequently, for large $\\mathbb{E}[RR]$, the distribution of $\\hat{\\xi}(r)$ over many independent realizations is approximately Gaussian with mean $0$ and variance $\\operatorname{Var}\\left[\\hat{\\xi}(r)\\right]\\approx 1/\\mathbb{E}[RR]$. The sampling distribution of the average of $\\hat{\\xi}(r)$ over $M$ independent realizations is then approximately Gaussian with mean $0$ and variance $1/\\left(M\\mathbb{E}[RR]\\right)$.\n\nTask:\n1. Implement an algorithm that, for each parameter set:\n   - Generates $M$ independent realizations of $N$ points uniformly at random in the periodic cube $[0,L)^3$.\n   - Computes all pairwise separations using the minimal image convention. That is, for a component-wise difference $\\Delta x$, map it to $\\Delta x-L \\times \\mathrm{round}(\\Delta x/L)$, and likewise for $\\Delta y$ and $\\Delta z$, before computing the Euclidean distance.\n   - Bins the pair separations into $B$ linearly spaced radial bins between $0$ and $r_{\\max}$, with the restriction that $r_{\\max} \\leq L/2$ to avoid geometric complications, and for each bin computes the estimator $\\hat{\\xi}(r)$ as described above.\n   - Across the $M$ realizations, for each radial bin $i$, computes the sample mean $\\overline{\\hat{\\xi}}_i$ and sample variance $s_i^2$ of the estimator.\n2. Verification criteria:\n   - Only consider bins where the per-realization expected number of pairs $\\mathbb{E}[RR]_i$ satisfies $\\mathbb{E}[RR]_i \\geq 25$.\n   - For each considered bin $i$, form the $z$-score of the sample mean, $z_i = \\frac{\\overline{\\hat{\\xi}}_i - 0}{\\sqrt{1/(M\\mathbb{E}[RR]_i)}}$. The mean-unbiasedness check passes if $\\max_i |z_i|  4.0$.\n   - For each considered bin $i$, check the variance against the Poisson prediction via the relative error $\\delta_i = \\left|s_i^2 - 1/\\mathbb{E}[RR]_i\\right| \\big/ \\left(1/\\mathbb{E}[RR]_i\\right)$. The variance-consistency check passes if $\\max_i \\delta_i \\leq 0.5$.\n   - The overall pass/fail for a parameter set is the conjunction of the two checks above across all considered bins. If no bins meet $\\mathbb{E}[RR]_i \\geq 25$, mark the parameter set as failing.\n3. Numerical notes:\n   - Use only distances up to $r_{\\max} \\le L/2$.\n   - Exclude self-pairs; count only unique unordered pairs.\n   - All distances and lengths are dimensionless.\n   - Random number generation must be seeded to ensure reproducible results.\n4. Test suite:\n   - Case 1 (happy path): $L=1.0$, $N=300$, $M=300$, $B=12$, $r_{\\max}=0.5$.\n   - Case 2 (moderate counts): $L=1.0$, $N=80$, $M=600$, $B=10$, $r_{\\max}=0.5$.\n   - Case 3 (smaller $N$): $L=1.0$, $N=50$, $M=1000$, $B=8$, $r_{\\max}=0.5$.\n5. Required final output format:\n   - Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3]\"), where each entry is a boolean indicating whether the corresponding test case passes the checks.\n\nYour solution must be a single, complete, runnable program that performs the experiment exactly as specified and prints the boolean results in the required format. No user input is permitted. The program must be self-contained and use the random seeds it sets internally to be reproducible.", "solution": "The problem statement is a valid, well-posed, and scientifically grounded exercise in numerical cosmology. It directs the implementation of a Monte Carlo simulation to verify the statistical properties of a simple estimator for the two-point correlation function (2PCF) applied to a homogeneous Poisson point process. The provided definitions, constraints, and verification criteria are clear, consistent, and standard within the field.\n\nThe solution proceeds by implementing a numerical experiment for each of the three specified parameter sets. The parameters for a given experiment are the cube side length $L$, the number of points $N$, the number of Monte Carlo realizations $M$, the number of radial bins $B$, and the maximum radius of interest $r_{\\max}$.\n\nThe overall algorithm is as follows:\n\nFirst, for a given parameter set, we pre-calculate values that are constant across all $M$ realizations. The radial domain $[0, r_{\\max}]$ is divided into $B$ bins of equal width. For each bin $i$, with inner radius $r_{\\min,i}$ and outer radius $r_{\\max,i}$, the volume of the corresponding spherical shell is $V_{\\text{shell},i} = \\frac{4\\pi}{3}(r_{\\max,i}^3 - r_{\\min,i}^3)$. The total volume of the periodic cube is $V = L^3$. For a set of $N$ points, there are $\\frac{N(N-1)}{2}$ unique pairs. The expected number of pairs from a uniform random distribution falling into bin $i$, denoted $\\mathbb{E}[RR_i]$, is given by the product of the total number of pairs and the ratio of the shell volume to the total volume:\n$$\n\\mathbb{E}[RR_i] = \\frac{N(N-1)}{2} \\frac{V_{\\text{shell},i}}{V}\n$$\nThese $\\mathbb{E}[RR_i]$ values are computed for all $B$ bins.\n\nNext, a loop over $M$ realizations is executed. In each realization $m \\in \\{1, \\dots, M\\}$:\n1.  $N$ points are generated with coordinates drawn from a uniform distribution over the half-open interval $[0, L)^3$.\n2.  The pairwise separations for all $\\frac{N(N-1)}{2}$ unique pairs are calculated. This is performed efficiently using vectorization. For a set of point coordinates $\\{ \\vec{p}_k \\}_{k=1}^N$, all separation vectors $\\Delta \\vec{p}_{jk} = \\vec{p}_j - \\vec{p}_k$ are computed. The periodic boundary conditions are enforced via the minimal image convention, where each component of the separation vector, e.g., $\\Delta x$, is adjusted according to $\\Delta x' = \\Delta x - L \\cdot \\text{round}(\\Delta x/L)$. The scalar separation is then the Euclidean norm of the adjusted vector, $r = |\\Delta \\vec{p}'|$.\n3.  The calculated pair separations are sorted into the $B$ predefined radial bins, yielding the observed data-data pair counts, $DD_i^{(m)}$, for each bin $i$.\n4.  The estimator for the 2PCF, $\\hat{\\xi}_i^{(m)}$, is computed for each bin using the problem's definition:\n    $$\n    \\hat{\\xi}_i^{(m)} = \\frac{DD_i^{(m)}}{\\mathbb{E}[RR_i]} - 1\n    $$\nThe values of $\\hat{\\xi}_i^{(m)}$ for all bins $i=1, \\dots, B$ and all realizations $m=1, \\dots, M$ are stored.\n\nAfter completing all $M$ realizations, a statistical analysis is performed on the collected data. First, bins that do not meet the statistical significance criterion $\\mathbb{E}[RR_i] \\geq 25$ are excluded from the analysis. If no bins satisfy this condition, the test for the parameter set fails.\n\nFor the remaining valid bins, the following statistics are computed:\n- The sample mean of the estimator across the $M$ realizations for each bin $i$:\n  $$\n  \\overline{\\hat{\\xi}}_i = \\frac{1}{M} \\sum_{m=1}^{M} \\hat{\\xi}_i^{(m)}\n  $$\n- The sample variance for each bin $i$, using the standard unbiased estimator with $ddof=1$:\n  $$\n  s_i^2 = \\frac{1}{M-1} \\sum_{m=1}^{M} (\\hat{\\xi}_i^{(m)} - \\overline{\\hat{\\xi}}_i)^2\n  $$\n\nFinally, two verification checks are performed on these statistics for all valid bins:\n1.  **Mean-Unbiasedness Check**: The theoretical mean of $\\hat{\\xi}_i$ for a Poisson process is $0$. The theoretical variance of the sample mean $\\overline{\\hat{\\xi}}_i$ is $\\operatorname{Var}(\\overline{\\hat{\\xi}}_i) = \\frac{\\operatorname{Var}(\\hat{\\xi}_i)}{M} \\approx \\frac{1}{M \\mathbb{E}[RR_i]}$. The $z$-score for the sample mean in each bin is calculated as:\n    $$\n    z_i = \\frac{\\overline{\\hat{\\xi}}_i - 0}{\\sqrt{1/(M\\mathbb{E}[RR]_i)}}\n    $$\n    The check passes if the maximum absolute $z$-score across all valid bins is less than $4.0$, i.e., $\\max_i |z_i|  4.0$.\n\n2.  **Variance-Consistency Check**: The sample variance $s_i^2$ is compared to its theoretical prediction, $\\operatorname{Var}(\\hat{\\xi}_i) \\approx 1/\\mathbb{E}[RR_i]$. The relative error $\\delta_i$ is computed:\n    $$\n    \\delta_i = \\frac{\\left|s_i^2 - 1/\\mathbb{E}[RR_i]\\right|}{1/\\mathbb{E}[RR_i]}\n    $$\n    This check passes if the maximum relative error across all valid bins is no more than $0.5$, i.e., $\\max_i \\delta_i \\leq 0.5$.\n\nA given parameter set receives a final verdict of 'pass' (True) if and only if both the mean and variance checks pass. Otherwise, it fails (False). The random number generator is seeded to ensure the entire process is reproducible.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_experiment(L, N, M, B, r_max, seed):\n    \"\"\"\n    Performs the numerical experiment for a single parameter set.\n\n    Args:\n        L (float): Side length of the periodic cube.\n        N (int): Number of points per realization.\n        M (int): Number of independent realizations.\n        B (int): Number of radial bins.\n        r_max (float): Maximum separation distance to consider.\n        seed (int): Seed for the random number generator.\n\n    Returns:\n        bool: True if the test case passes, False otherwise.\n    \"\"\"\n    np.random.seed(seed)\n\n    # 1. Pre-calculate bin properties and expected pair counts (RR)\n    # These are constant for a given parameter set.\n    r_bins = np.linspace(0.0, r_max, B + 1)\n    # The volume of a spherical shell\n    v_shell = (4.0 / 3.0) * np.pi * (r_bins[1:]**3 - r_bins[:-1]**3)\n    V = L**3\n    total_pairs = N * (N - 1) / 2.0\n    expected_rr = total_pairs * v_shell / V\n\n    # This array will store the xi_hat values for each realization and bin\n    all_xi_hats = np.zeros((M, B))\n\n    # 2. Main loop over M realizations\n    for m in range(M):\n        # Generate N points uniformly in the cube [0, L)^3\n        points = np.random.uniform(0.0, L, size=(N, 3))\n\n        # Vectorized calculation of pairwise separations with Minimal Image Convention\n        # Broadcasting points array to get all pairwise difference vectors\n        deltas = points[:, np.newaxis, :] - points[np.newaxis, :, :]\n        # Apply periodic boundary conditions\n        deltas -= L * np.round(deltas / L)\n\n        # Calculate squared distances\n        dists_sq_matrix = np.sum(deltas**2, axis=-1)\n\n        # Extract unique pairs (upper triangle of the distance matrix, k=1 to exclude self-pairs)\n        iu = np.triu_indices(N, k=1)\n        pairwise_dists_sq = dists_sq_matrix[iu]\n\n        # Binning the pair counts (DD)\n        # We can bin squared distances to avoid a sqrt calculation for every pair\n        dd_counts, _ = np.histogram(pairwise_dists_sq, bins=r_bins**2)\n\n        # Calculate the 2PCF estimator xi_hat for this realization\n        # Using np.divide for safe division where expected_rr might be zero\n        xi_hat_m = np.divide(\n            dd_counts, expected_rr, \n            out=np.zeros_like(expected_rr, dtype=float), \n            where=(expected_rr != 0)\n        ) - 1.0\n        \n        all_xi_hats[m, :] = xi_hat_m\n\n    # 3. Filter bins based on expected counts\n    valid_bins_mask = expected_rr >= 25.0\n    if not np.any(valid_bins_mask):\n        return False\n\n    # Apply mask to all relevant arrays\n    valid_expected_rr = expected_rr[valid_bins_mask]\n    valid_all_xi_hats = all_xi_hats[:, valid_bins_mask]\n\n    # 4. Compute statistics (mean and variance) over M realizations for valid bins\n    mean_xi_hat = np.mean(valid_all_xi_hats, axis=0)\n    # ddof=1 for unbiased sample variance\n    var_xi_hat = np.var(valid_all_xi_hats, axis=0, ddof=1)\n\n    # 5. Perform verification checks\n\n    # Mean-unbiasedness check\n    # Theoretical variance of the sample mean\n    theory_var_of_mean = 1.0 / (M * valid_expected_rr)\n    theory_std_of_mean = np.sqrt(theory_var_of_mean)\n    z_scores = np.abs(mean_xi_hat / theory_std_of_mean)\n    mean_check_passed = np.max(z_scores)  4.0\n\n    # Variance-consistency check\n    # Theoretical variance of the estimator xi_hat\n    theory_var = 1.0 / valid_expected_rr\n    rel_errors = np.abs(var_xi_hat - theory_var) / theory_var\n    var_check_passed = np.max(rel_errors) = 0.5\n    \n    return mean_check_passed and var_check_passed\n\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (L, N, M, B, r_max)\n        (1.0, 300, 300, 12, 0.5),  # Case 1\n        (1.0, 80, 600, 10, 0.5),   # Case 2\n        (1.0, 50, 1000, 8, 0.5),   # Case 3\n    ]\n\n    results = []\n    # Using a different seed for each case to ensure their random streams are independent\n    # while keeping the entire run reproducible.\n    for i, case in enumerate(test_cases):\n        L, N, M, B, r_max = case\n        result = run_experiment(L, N, M, B, r_max, seed=i)\n        results.append(str(result).lower()) # Convert boolean to lowercase string ('true'/'false')\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3499918"}, {"introduction": "Cosmological surveys observe galaxies not in a static cube, but across vast distances and times on our past \"light-cone.\" This exercise bridges the gap between idealized theoretical models and realistic observations by exploring how the measured 2PCF is affected by the evolution of galaxy bias and the survey's selection function over cosmic time. You will derive and quantify how these light-cone effects systematically alter the inferred clustering amplitude compared to a simple \"snapshot\" measurement ([@problem_id:3499958]).", "problem": "Consider a statistically homogeneous and isotropic galaxy distribution observed on a past light-cone in a spatially flat Friedmann–Lemaître–Robertson–Walker cosmology with matter density parameter $\\Omega_{\\mathrm{m},0}$, dark energy density parameter $\\Omega_{\\Lambda,0}$, and Hubble parameter $H_0$. The two-point correlation function $\\xi(r)$ of the galaxy overdensity field at comoving separation $r$ quantifies the excess probability of finding pairs beyond a random Poisson sample. The observed sample is modulated by a redshift-dependent selection function $S(z)$ that multiplies the intrinsic comoving number density, and the galaxy overdensity $\\delta_{\\mathrm{g}}$ is assumed to be linearly biased with respect to the matter overdensity $\\delta_{\\mathrm{m}}$ through a redshift-dependent bias $b(z)$, i.e., $\\delta_{\\mathrm{g}}(z) = b(z)\\,\\delta_{\\mathrm{m}}(z)$. The matter clustering amplitude evolves with the linear growth factor $D(z)$, defined as the growing-mode solution normalized to $D(0) = 1$.\n\nStarting from the core definition that the excess joint probability of finding two galaxies in comoving volume elements $\\mathrm{d}V_1$ and $\\mathrm{d}V_2$ at separation $r$ is $\\mathrm{d}P = \\bar{n}^2(z)\\,[1+\\xi_{\\mathrm{g}}(r,z)]\\,\\mathrm{d}V_1\\,\\mathrm{d}V_2$, where $\\bar{n}(z)$ is the intrinsic comoving number density, derive from first principles how the expectation value of the estimator for the light-cone measured two-point correlation function $\\xi_{\\mathrm{LC}}(r)$ at fixed comoving separation $r$ is impacted by a redshift-dependent selection function $S(z)$ and linear bias $b(z)$, under the following scientifically standard and self-consistent assumptions:\n\n- The bias is linear and deterministic: $\\delta_{\\mathrm{g}}(z) = b(z)\\,\\delta_{\\mathrm{m}}(z)$.\n- The matter two-point correlation function factorizes into an amplitude and a fixed comoving-shape component: $\\xi_{\\mathrm{m}}(r,z) = D^2(z)\\,\\xi_{\\mathrm{m}}(r,0)$, where $D(z)$ is the linear growth factor normalized to $D(0)=1$.\n- The selection function $S(z)$ multiplies the observed comoving number density, such that the observed density is $S(z)\\,\\bar{n}(z)$, and the probability of finding pairs in a thin redshift shell is proportional to $[S(z)\\,\\bar{n}(z)]^2\\,\\mathrm{d}V$.\n- The small-separation, thin-shell approximation holds for the pair counts at fixed separation $r$ (with $r \\ll \\chi(z)$, where $\\chi(z)$ is comoving distance), so that the dominant contributions to pair counts at separation $r$ come from galaxies within the same thin redshift shells, and cross-shell mixing can be neglected at leading order.\n- The comoving differential volume element per unit redshift is $\\mathrm{d}V/\\mathrm{d}z \\propto \\chi^2(z)\\,\\mathrm{d}\\chi/\\mathrm{d}z$, with $\\mathrm{d}\\chi/\\mathrm{d}z = c/H(z)$ and $H(z) = H_0\\,E(z)$, where $E(z) = \\sqrt{\\Omega_{\\mathrm{m},0}(1+z)^3 + \\Omega_{\\Lambda,0}}$.\n- The snapshot two-point correlation function $\\xi_{\\mathrm{snap}}(r\\,|\\,z_{\\mathrm{snap}})$ refers to the value measured at a single redshift $z_{\\mathrm{snap}}$, with fixed bias $b(z_{\\mathrm{snap}})$ and growth factor $D(z_{\\mathrm{snap}})$.\n\nYour task is to:\n\n1. Using the above assumptions and the fundamental definitions, derive an explicit expression for the effective, light-cone–averaged clustering amplitude at fixed comoving separation $r$, expressed as a redshift-weighted average of the amplitude factor in $\\xi_{\\mathrm{g}}(r,z) = b^2(z)\\,D^2(z)\\,\\xi_{\\mathrm{m}}(r,0)$. Carefully justify the weighting by selection-modulated pair counts and the comoving volume element. Then, define the dimensionless ratio $R_{\\mathrm{both}}(r)$ of the light-cone–averaged amplitude to the snapshot amplitude at $z_{\\mathrm{snap}}$, which isolates the combined impact of evolution and selection. State all steps and assumptions clearly and in LaTeX.\n\n2. Define two additional diagnostic ratios that separately isolate the impact of (i) evolution alone and (ii) selection alone on the inferred clustering amplitude, each normalized by the snapshot amplitude at $z_{\\mathrm{snap}}$:\n   - $R_{\\mathrm{evol}}(r)$: the ratio when the selection is fixed to be flat in redshift, i.e., $S(z)=1$, but the redshift evolution of $b(z)$ and $D(z)$ is included.\n   - $R_{\\mathrm{sel}}(r)$: the ratio when the bias is fixed to $b(z_{\\mathrm{snap}})$ at all redshifts, but the actual selection function $S(z)$ is applied, with the physical growth $D(z)$ included.\n\n3. Implement a complete, runnable program that numerically evaluates the three ratios $R_{\\mathrm{both}}(r)$, $R_{\\mathrm{evol}}(r)$, and $R_{\\mathrm{sel}}(r)$ by performing redshift integrals over the specified ranges for a flat $\\Lambda$ Cold Dark Matter ($\\Lambda$CDM) cosmology with $\\Omega_{\\mathrm{m},0} = 0.3$, $\\Omega_{\\Lambda,0} = 0.7$, $H_0 = 70\\,\\mathrm{km\\,s^{-1}\\,Mpc^{-1}}$, and speed of light $c = 299792.458\\,\\mathrm{km\\,s^{-1}}$. Use the well-tested approximation $D(z) = g(z)/(1+z)$ with\n$$\ng(z) = \\frac{5\\,\\Omega_{\\mathrm{m}}(z)}{2}\\left[\\Omega_{\\mathrm{m}}(z)^{4/7} - \\Omega_{\\Lambda}(z) + \\left(1 + \\frac{\\Omega_{\\mathrm{m}}(z)}{2}\\right)\\left(1 + \\frac{\\Omega_{\\Lambda}(z)}{70}\\right)\\right]^{-1},\n$$\nnormalized to $D(0) = 1$, where $\\Omega_{\\mathrm{m}}(z) = \\Omega_{\\mathrm{m},0}(1+z)^3 / E^2(z)$ and $\\Omega_{\\Lambda}(z) = \\Omega_{\\Lambda,0} / E^2(z)$. Compute the comoving distance $\\chi(z)$ via\n$$\n\\chi(z) = \\frac{c}{H_0}\\int_0^z \\frac{\\mathrm{d}z'}{E(z')}.\n$$\n\n4. Use the following scientifically plausible test suite. In all cases, evaluate the ratios at a fiducial comoving separation $r = 10\\,h^{-1}\\,\\mathrm{Mpc}$ (you do not need to convert units in the code if the ratio is independent of $r$ under the above assumptions; this separation is specified for context, and all outputs are dimensionless). For each case, specify the redshift range $[z_{\\min}, z_{\\max}]$, the selection function $S(z)$, the bias $b(z)$, and the snapshot redshift $z_{\\mathrm{snap}}$:\n   - Case 1 (general, evolving selection and bias):\n     - $[z_{\\min}, z_{\\max}] = [0.1, 1.0]$,\n     - $S(z) = \\exp\\!\\left(-\\left(\\frac{z - 0.6}{0.25}\\right)^2\\right)$,\n     - $b(z) = 1 + 0.8\\,z$,\n     - $z_{\\mathrm{snap}} = 0.6$.\n   - Case 2 (boundary: no evolution and flat selection):\n     - $[z_{\\min}, z_{\\max}] = [0.2, 0.8]$,\n     - $S(z) = 1$,\n     - $b(z) = 1.5$,\n     - $z_{\\mathrm{snap}} = 0.5$.\n   - Case 3 (narrow selection around a lower redshift):\n     - $[z_{\\min}, z_{\\max}] = [0.1, 0.6]$,\n     - $S(z) = \\exp\\!\\left(-\\left(\\frac{z - 0.3}{0.05}\\right)^2\\right)$,\n     - $b(z) = 1 + 1.2\\,z$,\n     - $z_{\\mathrm{snap}} = 0.3$.\n   - Case 4 (selection increasing with redshift and rapidly increasing bias):\n     - $[z_{\\min}, z_{\\max}] = [0.1, 1.2]$,\n     - $S(z) = z^2$,\n     - $b(z) = 1 + 2.0\\,z$,\n     - $z_{\\mathrm{snap}} = 1.0$.\n\nYour program must:\n\n- Numerically integrate over $z$ to compute the redshift-weighted averages using the comoving volume element, implementing the three ratios $R_{\\mathrm{both}}(r)$, $R_{\\mathrm{evol}}(r)$, and $R_{\\mathrm{sel}}(r)$ exactly as defined in items 1 and 2.\n- Produce a single line of output containing the results as a comma-separated list of lists, in the form $\\left[[R_{\\mathrm{both}}, R_{\\mathrm{evol}}, R_{\\mathrm{sel}}]_{\\text{Case 1}}, [R_{\\mathrm{both}}, R_{\\mathrm{evol}}, R_{\\mathrm{sel}}]_{\\text{Case 2}}, [R_{\\mathrm{both}}, R_{\\mathrm{evol}}, R_{\\mathrm{sel}}]_{\\text{Case 3}}, [R_{\\mathrm{both}}, R_{\\mathrm{evol}}, R_{\\mathrm{sel}}]_{\\text{Case 4}}\\right]$.\n- Each numerical answer must be a decimal float. Angles do not appear and therefore require no unit specification. Distances are in $h^{-1}\\,\\mathrm{Mpc}$ for contextual interpretation; the requested ratios are dimensionless and should be output as decimal floats.\n\nEnsure scientific realism by adhering to the stated cosmology, definitions, and approximations. Do not assume or use any formulas for the target ratios beyond what you derive from the base definitions. The final program must be complete and runnable with no external input.", "solution": "The problem requires the derivation and computation of several ratios that quantify the impact of cosmological evolution and observational selection effects on the measured two-point correlation function of galaxies, $\\xi(r)$, observed on a past light-cone. We begin by deriving the expression for the light-cone-averaged correlation function, $\\xi_{\\mathrm{LC}}(r)$, from first principles.\n\nThe starting point is the excess probability of finding two galaxies in comoving volume elements $\\mathrm{d}V_1$ and $\\mathrm{d}V_2$ at the same redshift $z$, separated by a comoving distance $r$. This is given by $\\mathrm{d}P = \\bar{n}^2(z)\\,[1+\\xi_{\\mathrm{g}}(r,z)]\\,\\mathrm{d}V_1\\,\\mathrm{d}V_2$, where $\\bar{n}(z)$ is the intrinsic comoving number density and $\\xi_{\\mathrm{g}}(r,z)$ is the galaxy two-point correlation function.\n\nThe problem states that the observed sample is modulated by a redshift-dependent selection function $S(z)$, so the observed number density is $n_{\\mathrm{obs}}(z) = S(z)\\,\\bar{n}(z)$. The number of observed galaxies in a volume element $\\mathrm{d}V$ is thus $\\mathrm{d}N(z) = S(z)\\,\\bar{n}(z)\\,\\mathrm{d}V$.\n\nThe number of pairs of galaxies is proportional to the square of the number density. Specifically, the number of excess pairs (above a random distribution) in volume elements $\\mathrm{d}V_1$ and $\\mathrm{d}V_2$ at redshift $z$ and separation $r$ is:\n$$ \\mathrm{d}N_{\\mathrm{pairs, excess}}(r, z) = [S(z)\\,\\bar{n}(z)]^2\\,\\xi_{\\mathrm{g}}(r, z)\\,\\mathrm{d}V_1\\,\\mathrm{d}V_2 $$\nThe total number of excess pairs with separation $r$ in a thin redshift shell of thickness $\\mathrm{d}z$ is obtained by integrating over one volume element and over the shell of radius $r$ around it. This yields a quantity proportional to $[S(z)\\,\\bar{n}(z)]^2\\,\\xi_{\\mathrm{g}}(r, z)\\,\\frac{\\mathrm{d}V}{\\mathrm{d}z}\\mathrm{d}z$, where $\\frac{\\mathrm{d}V}{\\mathrm{d}z}$ is the comoving volume per unit redshift. Similarly, the number of pairs in a random sample is proportional to $[S(z)\\,\\bar{n}(z)]^2\\,\\frac{\\mathrm{d}V}{\\mathrm{d}z}\\mathrm{d}z$.\n\nThe light-cone measured correlation function, $\\xi_{\\mathrm{LC}}(r)$, is the total number of excess pairs at separation $r$ over the entire survey volume, divided by the total number of pairs from a random distribution. This is equivalent to a weighted average of the redshift-dependent correlation function $\\xi_{\\mathrm{g}}(r, z)$:\n$$ \\xi_{\\mathrm{LC}}(r) = \\frac{\\int_{z_{\\min}}^{z_{\\max}} [S(z)\\,\\bar{n}(z)]^2\\,\\xi_{\\mathrm{g}}(r, z)\\,\\frac{\\mathrm{d}V}{\\mathrm{d}z}\\,\\mathrm{d}z}{\\int_{z_{\\min}}^{z_{\\max}} [S(z)\\,\\bar{n}(z)]^2\\,\\frac{\\mathrm{d}V}{\\mathrm{d}z}\\,\\mathrm{d}z} $$\nThe term $[S(z)\\bar{n}(z)]^2 \\frac{\\mathrm{d}V}{\\mathrm{d}z}$ acts as the weighting function, representing the number of pairs available at each redshift $z$. Assuming the intrinsic comoving density $\\bar{n}(z)$ is constant, it cancels from the numerator and denominator.\n\nWe are given the factorization of the galaxy correlation function based on linear bias and linear growth:\n$$ \\xi_{\\mathrm{g}}(r, z) = b^2(z)\\,\\xi_{\\mathrm{m}}(r, z) = b^2(z)\\,D^2(z)\\,\\xi_{\\mathrm{m}}(r, 0) $$\nwhere $b(z)$ is the galaxy bias, $D(z)$ is the linear growth factor, and $\\xi_{\\mathrm{m}}(r, 0)$ is the matter correlation function at redshift $z=0$. The shape of the correlation function, encoded in $\\xi_{\\mathrm{m}}(r, 0)$, is assumed to be constant in comoving coordinates, while its amplitude evolves as $b^2(z)D^2(z)$.\n\nSubstituting this into the expression for $\\xi_{\\mathrm{LC}}(r)$ and factoring out the redshift-independent term $\\xi_{\\mathrm{m}}(r, 0)$:\n$$ \\xi_{\\mathrm{LC}}(r) = \\left( \\frac{\\int_{z_{\\min}}^{z_{\\max}} S^2(z)\\,b^2(z)\\,D^2(z)\\,\\frac{\\mathrm{d}V}{\\mathrm{d}z}\\,\\mathrm{d}z}{\\int_{z_{\\min}}^{z_{\\max}} S^2(z)\\,\\frac{\\mathrm{d}V}{\\mathrm{d}z}\\,\\mathrm{d}z} \\right) \\xi_{\\mathrm{m}}(r, 0) $$\nThe term in the parentheses is the effective, light-cone–averaged clustering amplitude, which we will denote as $A_{\\mathrm{LC}}$.\nThe differential comoving volume element over a solid angle $\\mathrm{d}\\Omega$ is $\\mathrm{d}V = \\chi^2(z)\\,\\mathrm{d}\\chi\\,\\mathrm{d}\\Omega$. Integrating over the full sky ($\\mathrm{d}\\Omega = 4\\pi$) and using the relation $\\mathrm{d}\\chi = \\frac{c}{H(z)}\\mathrm{d}z$, we have $\\frac{\\mathrm{d}V}{\\mathrm{d}z} = 4\\pi\\,\\chi^2(z)\\frac{c}{H(z)}$. The constants $4\\pi$ and $c$ will cancel in the ratio, so we can use the proportionality $\\frac{\\mathrm{d}V}{\\mathrm{d}z} \\propto \\frac{\\chi^2(z)}{H(z)}$.\nThe effective amplitude is thus:\n$$ A_{\\mathrm{LC}} = \\frac{\\int_{z_{\\min}}^{z_{\\max}} S^2(z)\\,b^2(z)\\,D^2(z)\\,\\frac{\\chi^2(z)}{H(z)}\\,\\mathrm{d}z}{\\int_{z_{\\min}}^{z_{\\max}} S^2(z)\\,\\frac{\\chi^2(z)}{H(z)}\\,\\mathrm{d}z} $$\n\nA \"snapshot\" measurement at a single redshift $z_{\\mathrm{snap}}$ would measure the amplitude $A_{\\mathrm{snap}} = b^2(z_{\\mathrm{snap}})D^2(z_{\\mathrm{snap}})$.\n\n**1. The ratio $R_{\\mathrm{both}}(r)$**\nThis ratio quantifies the combined impact of redshift evolution and selection effects. It is defined as the ratio of the light-cone averaged amplitude to the snapshot amplitude. Since the spatial dependence $\\xi_{\\mathrm{m}}(r,0)$ is the same, this ratio is independent of separation $r$.\n$$ R_{\\mathrm{both}} = \\frac{A_{\\mathrm{LC}}}{A_{\\mathrm{snap}}} = \\frac{1}{b^2(z_{\\mathrm{snap}})D^2(z_{\\mathrm{snap}})} \\frac{\\int_{z_{\\min}}^{z_{\\max}} S^2(z)\\,b^2(z)\\,D^2(z)\\,\\frac{\\chi^2(z)}{H(z)}\\,\\mathrm{d}z}{\\int_{z_{\\min}}^{z_{\\max}} S^2(z)\\,\\frac{\\chi^2(z)}{H(z)}\\,\\mathrm{d}z} $$\n\n**2. Diagnostic Ratios $R_{\\mathrm{evol}}(r)$ and $R_{\\mathrm{sel}}(r)$**\nWe define two additional ratios to isolate the effects of evolution and selection.\n\n- **$R_{\\mathrm{evol}}(r)$**: This ratio isolates the impact of evolution alone by assuming a flat selection function, $S(z)=1$. The redshift evolution of bias $b(z)$ and growth $D(z)$ is retained.\n$$ R_{\\mathrm{evol}} = \\frac{1}{b^2(z_{\\mathrm{snap}})D^2(z_{\\mathrm{snap}})} \\frac{\\int_{z_{\\min}}^{z_{\\max}} b^2(z)\\,D^2(z)\\,\\frac{\\chi^2(z)}{H(z)}\\,\\mathrm{d}z}{\\int_{z_{\\min}}^{z_{\\max}} \\frac{\\chi^2(z)}{H(z)}\\,\\mathrm{d}z} $$\nThis expression represents the volume-weighted average of the clustering amplitude $b^2(z)D^2(z)$, normalized to the snapshot value.\n\n- **$R_{\\mathrm{sel}}(r)$**: This ratio isolates the impact of the selection function by fixing the bias to its snapshot value, $b(z)=b(z_{\\mathrm{snap}})$, while keeping the actual selection function $S(z)$ and physical growth $D(z)$.\n$$ R_{\\mathrm{sel}} = \\frac{1}{b^2(z_{\\mathrm{snap}})D^2(z_{\\mathrm{snap}})} \\frac{\\int_{z_{\\min}}^{z_{\\max}} S^2(z)\\,b^2(z_{\\mathrm{snap}})\\,D^2(z)\\,\\frac{\\chi^2(z)}{H(z)}\\,\\mathrm{d}z}{\\int_{z_{\\min}}^{z_{\\max}} S^2(z)\\,\\frac{\\chi^2(z)}{H(z)}\\,\\mathrm{d}z} $$\nThe constant term $b^2(z_{\\mathrm{snap}})$ cancels, yielding:\n$$ R_{\\mathrm{sel}} = \\frac{1}{D^2(z_{\\mathrm{snap}})} \\frac{\\int_{z_{\\min}}^{z_{\\max}} S^2(z)\\,D^2(z)\\,\\frac{\\chi^2(z)}{H(z)}\\,\\mathrm{d}z}{\\int_{z_{\\min}}^{z_{\\max}} S^2(z)\\,\\frac{\\chi^2(z)}{H(z)}\\,\\mathrm{d}z} $$\nThis expression measures how the selection function $S(z)$ re-weights the redshift evolution of the matter clustering amplitude $D^2(z)$, relative to its value at the snapshot redshift $z_{\\mathrm{snap}}$.\n\nThe numerical implementation will proceed by evaluating these final expressions for the three ratios using the provided cosmological model and test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.integrate import quad, cumulative_trapezoid\nfrom scipy.interpolate import interp1d\n\ndef solve():\n    \"\"\"\n    Derives and computes the impact of evolution and selection effects on the\n    light-cone-averaged two-point correlation function.\n    \"\"\"\n\n    # Define cosmological constants and parameters\n    C = 299792.458  # Speed of light in km/s\n    H0 = 70.0       # Hubble constant in km/s/Mpc\n    OMEGA_M0 = 0.3\n    OMEGA_L0 = 0.7\n\n    # Cosmological functions\n    def E(z):\n        return np.sqrt(OMEGA_M0 * (1 + z)**3 + OMEGA_L0)\n\n    def inv_E(z):\n        return 1.0 / E(z)\n\n    # Pre-compute an interpolator for comoving distance chi(z) for efficiency.\n    # The maximum redshift in the test cases is 1.2. We integrate up to 1.5.\n    Z_MAX_GLOBAL = 1.5\n    z_grid = np.linspace(0, Z_MAX_GLOBAL, 2000)\n    inv_E_vals = inv_E(z_grid)\n    chi_integral_vals = cumulative_trapezoid(inv_E_vals, z_grid, initial=0)\n    chi_vals = (C / H0) * chi_integral_vals\n    chi_interp = interp1d(z_grid, chi_vals, kind='cubic', fill_value=\"extrapolate\")\n\n    def chi(z):\n        # The interpolator handles arrays and floats efficiently.\n        # Set chi(0)=0 explicitly to avoid potential small float errors at z=0.\n        is_scalar = np.isscalar(z)\n        z = np.atleast_1d(z)\n        results = chi_interp(z)\n        results[z == 0] = 0.0\n        return results[0] if is_scalar else results\n\n    # Growth factor D(z) calculation, normalized to D(0)=1\n    def Omega_m_z(z):\n        E_z = E(z)\n        return OMEGA_M0 * (1 + z)**3 / E_z**2\n\n    def Omega_Lambda_z(z):\n        E_z = E(z)\n        return OMEGA_L0 / E_z**2\n\n    def g_unnorm(z):\n        om_z = Omega_m_z(z)\n        ol_z = Omega_Lambda_z(z)\n        # Per problem statement\n        term1 = om_z**(4.0/7.0)\n        term2 = ol_z\n        term3 = (1 + om_z / 2.0) * (1 + ol_z / 70.0)\n        return (5.0 * om_z / 2.0) * (term1 - term2 + term3)**(-1)\n\n    def D_unnorm(z):\n        return g_unnorm(z) / (1.0 + z)\n\n    D0_val = D_unnorm(0.0)\n\n    def D(z):\n        # Normalized growth factor\n        return D_unnorm(z) / D0_val\n\n    # Differential comoving volume element factor dV/dz ~ chi(z)^2 / H(z)\n    def dV_dz_prop(z):\n        # H(z) = H0 * E(z)\n        return chi(z)**2 / (H0 * E(z))\n\n    def calculate_ratios(z_min, z_max, S_func, b_func, z_snap):\n        \"\"\"\n        Calculates the three ratios R_both, R_evol, and R_sel for a given case.\n        \"\"\"\n        # Snapshot values\n        b_snap = b_func(z_snap)\n        D_snap = D(z_snap)\n\n        # --- R_both ---\n        A_snap_both = b_snap**2 * D_snap**2\n        \n        integrand_num_both = lambda z: S_func(z)**2 * b_func(z)**2 * D(z)**2 * dV_dz_prop(z)\n        integrand_den_both = lambda z: S_func(z)**2 * dV_dz_prop(z)\n        \n        num_both, _ = quad(integrand_num_both, z_min, z_max, limit=200, epsabs=1.49e-9, epsrel=1.49e-9)\n        den_both, _ = quad(integrand_den_both, z_min, z_max, limit=200, epsabs=1.49e-9, epsrel=1.49e-9)\n        \n        R_both = (num_both / den_both) / A_snap_both if den_both > 0 else np.nan\n\n        # --- R_evol ---\n        A_snap_evol = b_snap**2 * D_snap**2\n\n        integrand_num_evol = lambda z: b_func(z)**2 * D(z)**2 * dV_dz_prop(z)\n        integrand_den_evol = lambda z: dV_dz_prop(z)\n\n        num_evol, _ = quad(integrand_num_evol, z_min, z_max, limit=200, epsabs=1.49e-9, epsrel=1.49e-9)\n        den_evol, _ = quad(integrand_den_evol, z_min, z_max, limit=200, epsabs=1.49e-9, epsrel=1.49e-9)\n        \n        R_evol = (num_evol / den_evol) / A_snap_evol if den_evol > 0 else np.nan\n\n        # --- R_sel ---\n        A_snap_sel = D_snap**2\n\n        integrand_num_sel = lambda z: S_func(z)**2 * D(z)**2 * dV_dz_prop(z)\n        # Denominator is the same as for R_both\n        # integrand_den_sel = integrand_den_both\n        \n        num_sel, _ = quad(integrand_num_sel, z_min, z_max, limit=200, epsabs=1.49e-9, epsrel=1.49e-9)\n        den_sel = den_both\n        \n        R_sel = (num_sel / den_sel) / A_snap_sel if den_sel > 0 else np.nan\n        \n        return [R_both, R_evol, R_sel]\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {'z_range': [0.1, 1.0], 'S_func': lambda z: np.exp(-((z - 0.6)/0.25)**2), 'b_func': lambda z: 1.0 + 0.8*z, 'z_snap': 0.6},\n        {'z_range': [0.2, 0.8], 'S_func': lambda z: 1.0, 'b_func': lambda z: 1.5, 'z_snap': 0.5},\n        {'z_range': [0.1, 0.6], 'S_func': lambda z: np.exp(-((z - 0.3)/0.05)**2), 'b_func': lambda z: 1.0 + 1.2*z, 'z_snap': 0.3},\n        {'z_range': [0.1, 1.2], 'S_func': lambda z: z**2, 'b_func': lambda z: 1.0 + 2.0*z, 'z_snap': 1.0},\n    ]\n\n    results = []\n    for case in test_cases:\n        res = calculate_ratios(\n            case['z_range'][0],\n            case['z_range'][1],\n            case['S_func'],\n            case['b_func'],\n            case['z_snap']\n        )\n        results.append(res)\n    \n    # Format the final output string exactly as required.\n    formatted_results = []\n    for res_list in results:\n        formatted_list = [f\"{x:.8f}\" for x in res_list]\n        formatted_results.append(f\"[{','.join(formatted_list)}]\")\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3499958"}, {"introduction": "The 2PCF framework can be extended to test fundamental physical principles, such as parity (mirror) symmetry. This advanced practice introduces \"marked\" correlation functions, where each galaxy is weighted by a physical property like spin or handedness, to construct a parity-odd statistic. By implementing an estimator designed to be zero in a parity-conserving universe like our standard $\\Lambda$CDM model, you will build a pipeline to search for new physics and test its robustness against systematic errors ([@problem_id:3499960]).", "problem": "You are tasked with designing and implementing a parity-sensitive marked two-point correlation function in a finite, periodic, three-dimensional cubic domain, suitable for numerical cosmology. The overarching goal is to formalize and compute a parity-odd marked two-point statistic based on galaxy spin or handedness marks, quantify its expected null signal in the Lambda Cold Dark Matter (ΛCDM) paradigm, and test an analysis pipeline for robustness against spurious parity violation.\n\nThe context and constraints are as follows.\n\n- The two-point correlation function is built on the core definition of spatial correlations of the density contrast field, which in a discrete sample reduces to counting pairs in separation bins. In a cubic domain of side length $L$ with periodic boundary conditions, the minimum-image convention is used to define separation vectors. Let $N$ be the number of points; let $\\boldsymbol{x}_i \\in [0,L)^3$ be the position of point $i$.\n\n- Parity symmetry is defined by the inversion $\\boldsymbol{x} \\mapsto -\\boldsymbol{x}$, under which polar vectors (e.g., $\\boldsymbol{r}$) invert sign and axial vectors (e.g., spins $\\boldsymbol{s}$) remain unchanged. A parity-odd scalar changes sign under parity inversion, and has vanishing ensemble mean if the Universe respects parity symmetry.\n\n- You must use the following foundational constructs:\n  - Pair separation vectors computed via the minimum-image convention:\n    $$\\boldsymbol{r}_{ij} = \\boldsymbol{x}_j - \\boldsymbol{x}_i - L \\,\\mathrm{round}\\!\\left(\\frac{\\boldsymbol{x}_j - \\boldsymbol{x}_i}{L}\\right), \\quad r_{ij} = \\|\\boldsymbol{r}_{ij}\\|, \\quad \\hat{\\boldsymbol{r}}_{ij} = \\frac{\\boldsymbol{r}_{ij}}{r_{ij}}.$$\n  - A fixed line-of-sight polar vector $\\boldsymbol{\\ell} = (0,0,1)$.\n  - Isotropic unit spin axial vectors $\\boldsymbol{s}_i$ when spins are required. The galaxy handedness $h_i \\in \\{-1,+1\\}$ may be defined as the sign of the pseudo-scalar projection $h_i = \\mathrm{sign}(\\boldsymbol{s}_i \\cdot \\boldsymbol{\\ell})$ or assigned by an explicit probabilistic rule, as specified in the test suite.\n\n- Construct two parity-odd marked two-point estimators as functions of separation $r$:\n  1. A spin-marked estimator based on a parity-odd scalar triple product, weighted by pair orientation with respect to the line-of-sight:\n     $$W^{(\\mathrm{spin})}_{ij} \\equiv \\mathrm{sign}(\\boldsymbol{\\ell}\\cdot \\boldsymbol{r}_{ij}) \\left[\\boldsymbol{\\ell} \\cdot \\left(\\boldsymbol{s}_i \\times \\boldsymbol{s}_j\\right)\\right].$$\n     For a separation bin labeled by edges $[r_a,r_b)$, define the binned estimator\n     $$\\widehat{S}(r_a,r_b) \\equiv \\frac{1}{N_\\mathrm{pairs}(r_a,r_b)} \\sum_{\\substack{ij\\\\ r_{ij}\\in [r_a,r_b)}} W^{(\\mathrm{spin})}_{ij},$$\n     where $N_\\mathrm{pairs}(r_a,r_b)$ is the number of pairs in the bin. This statistic is parity-odd because $\\boldsymbol{\\ell}$ is a polar vector, $\\boldsymbol{s}_i \\times \\boldsymbol{s}_j$ is an axial vector, their dot product is a pseudo-scalar, and the sign factor is invariant under parity. In a parity-symmetric Universe under Lambda Cold Dark Matter (ΛCDM), its ensemble mean is zero.\n\n  2. A handedness-marked estimator designed to respond to a parity-odd handedness gradient along the line-of-sight:\n     $$W^{(\\mathrm{hand})}_{ij} \\equiv \\frac{1}{2}\\,\\big(h_j - h_i\\big)\\,\\mathrm{sign}(\\boldsymbol{\\ell}\\cdot \\boldsymbol{r}_{ij}).$$\n     The corresponding binned estimator is\n     $$\\widehat{H}(r_a,r_b) \\equiv \\frac{1}{N_\\mathrm{pairs}(r_a,r_b)} \\sum_{\\substack{ij\\\\ r_{ij}\\in [r_a,r_b)}} W^{(\\mathrm{hand})}_{ij}.$$\n     This statistic is parity-odd due to the pseudo-scalar handedness difference combined with the sign of a polar-polar dot product, and likewise has zero ensemble mean under parity symmetry in Lambda Cold Dark Matter (ΛCDM). It is sensitive to parity-violating handedness patterns aligned with $\\boldsymbol{\\ell}$.\n\n- For each binned estimator, define an empirical standard error using pair weights within the bin. Denoting the bin-average by $\\bar{W}$ and the bin second moment by $\\overline{W^2}$, estimate the variance of the mean as $\\sigma^2 = \\left(\\overline{W^2}-\\bar{W}^2\\right)/N_\\mathrm{pairs}$, and the two-sided standardized score as $z = \\bar{W}/\\sigma$ for $\\sigma0$.\n\n- All distances are to be treated as comoving in units of $h^{-1}\\,\\mathrm{Mpc}$, but the final answers are unitless booleans or floats, so you do not need to report units. Angles, if any, are in radians.\n\nImplement the following test suite. In all cases, positions are sampled in a cube with periodic boundary conditions. Use the same set of radial separation bins, with edges\n$$\\{r_k\\}_{k=0}^{8} = \\{5, 12.857142857142858, 20.714285714285715, 28.571428571428573, 36.42857142857143, 44.285714285714285, 52.142857142857146, 60.0, 67.85714285714286\\},$$\ni.e., $8$ linearly spaced bins covering $[5,67.85714285714286)$ with bin width $7.857142857142858$. The line-of-sight is $\\boldsymbol{\\ell}=(0,0,1)$. For reproducibility, all random number generators must be initialized with the specified seeds.\n\n- Test case $1$ (spin-null under parity-symmetric Lambda Cold Dark Matter (ΛCDM)):\n  - Parameters: $L=200$, $N=900$, seed $=12345$.\n  - Positions $\\boldsymbol{x}_i$ are independent and identically distributed uniform in $[0,L)^3$.\n  - Spins $\\boldsymbol{s}_i$ are independent isotropic unit axial vectors.\n  - Compute $\\widehat{S}$ in each bin and its $z$-scores, and report a single boolean: $\\mathrm{True}$ if the maximum absolute $z$ across all bins is less than or equal to $4.0$, and $\\mathrm{False}$ otherwise.\n\n- Test case $2$ (handedness-null under parity-symmetric Lambda Cold Dark Matter (ΛCDM)):\n  - Parameters: $L=200$, $N=900$, seed $=67890$.\n  - Positions $\\boldsymbol{x}_i$ are independent and identically distributed uniform in $[0,L)^3$.\n  - Spins $\\boldsymbol{s}_i$ are independent isotropic unit axial vectors; define handedness $h_i = \\mathrm{sign}(\\boldsymbol{s}_i \\cdot \\boldsymbol{\\ell})$.\n  - Compute $\\widehat{H}$ in each bin and its $z$-scores, and report a single boolean: $\\mathrm{True}$ if the maximum absolute $z$ across all bins is less than or equal to $4.0$, and $\\mathrm{False}$ otherwise.\n\n- Test case $3$ (injected parity violation in handedness to test detection):\n  - Parameters: $L=200$, $N=900$, seed $=24680$, handedness bias amplitude $\\alpha=0.8$.\n  - Positions $\\boldsymbol{x}_i$ are independent and identically distributed uniform in $[0,L)^3$.\n  - Handedness $h_i \\in \\{-1,+1\\}$ is drawn conditionally on the $z$-coordinate via a linear parity-odd bias along $\\boldsymbol{\\ell}$:\n    $$\\mathbb{P}(h_i=+1\\mid z_i) = \\frac{1}{2}\\left[1 + \\alpha \\left(\\frac{2 z_i}{L} - 1\\right)\\right], \\quad \\mathbb{P}(h_i=-1\\mid z_i) = 1 - \\mathbb{P}(h_i=+1\\mid z_i).$$\n  - Compute $\\widehat{H}$ in each bin and its $z$-scores. Report a single float: the aggregate signal-to-noise ratio defined by\n    $$\\mathrm{SNR}_\\mathrm{tot} = \\sqrt{\\sum_{\\text{bins}} z^2},$$\n    evaluated over bins with $N_\\mathrm{pairs}0$.\n\n- Test case $4$ (pipeline robustness against selection-function anisotropy without parity violation):\n  - Parameters: $L=200$, $N=900$, seed $=11223$, selection-function slope $c=0.8$.\n  - Positions are distributed with a linear selection function along $z$:\n    $$n(z) \\propto 1 + c\\left(\\frac{2 z}{L} - 1\\right), \\quad z \\in [0,L),$$\n    while $x$ and $y$ are uniform in $[0,L)$.\n  - Handedness $h_i$ is independent of $z$, with $\\mathbb{P}(h_i=+1)=\\mathbb{P}(h_i=-1)=\\frac{1}{2}$.\n  - Compute $\\widehat{H}$ in each bin and its $z$-scores, and report a single boolean: $\\mathrm{True}$ if the maximum absolute $z$ across all bins is less than or equal to $4.0$, and $\\mathrm{False}$ otherwise.\n\nYour program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain the $4$ answers in order for the test cases $1$ to $4$. Specifically, print\n$$[\\mathrm{ans}_1,\\mathrm{ans}_2,\\mathrm{ans}_3,\\mathrm{ans}_4],$$\nwhere $\\mathrm{ans}_1$ is a boolean, $\\mathrm{ans}_2$ is a boolean, $\\mathrm{ans}_3$ is a float, and $\\mathrm{ans}_4$ is a boolean. No additional text should be printed.", "solution": "The problem requires the design and implementation of two parity-odd marked two-point correlation statistics, $\\widehat{S}(r)$ and $\\widehat{H}(r)$, for analyzing point distributions in a periodic cubic volume, a common task in numerical cosmology. The solution involves generating mock data under different physical assumptions and then computing these statistics to validate the analysis pipeline.\n\n### Principle-Based Design\n\nThe solution is founded on principles of symmetry in physics, statistical estimation, and numerical computation.\n\n#### 1. Parity-Odd Estimators in a $\\Lambda$CDM Context\n\nParity is a fundamental symmetry corresponding to inversion of spatial coordinates, $\\boldsymbol{x} \\mapsto -\\boldsymbol{x}$. Under this transformation, physical quantities transform in specific ways. A polar vector $\\boldsymbol{v}$ (like position $\\boldsymbol{x}$ or separation $\\boldsymbol{r}_{ij}$) inverts sign, $\\boldsymbol{v} \\mapsto -\\boldsymbol{v}$. An axial vector $\\boldsymbol{a}$ (like spin $\\boldsymbol{s}$ or orbital angular momentum) is invariant, $\\boldsymbol{a} \\mapsto \\boldsymbol{a}$. A scalar is invariant, while a pseudo-scalar changes sign.\n\nThe standard cosmological model, Lambda Cold Dark Matter ($\\Lambda$CDM), assumes that the universe is statistically homogeneous and isotropic. This statistical symmetry implies conservation of parity. Consequently, the ensemble expectation value of any parity-odd observable must be zero. The estimators are designed to be parity-odd to test this prediction.\n\n*   **Spin-marked estimator weight**: $W^{(\\mathrm{spin})}_{ij} \\equiv \\mathrm{sign}(\\boldsymbol{\\ell}\\cdot \\boldsymbol{r}_{ij}) \\left[\\boldsymbol{\\ell} \\cdot \\left(\\boldsymbol{s}_i \\times \\boldsymbol{s}_j\\right)\\right]$.\n    *   The separation vector $\\boldsymbol{r}_{ij}$ is a polar vector. The fixed line-of-sight $\\boldsymbol{\\ell}$ is also a polar vector. Their dot product $\\boldsymbol{\\ell}\\cdot \\boldsymbol{r}_{ij}$ is a true scalar, invariant under parity: $(\\!-\\boldsymbol{\\ell})\\cdot(\\!-\\boldsymbol{r}_{ij}) = \\boldsymbol{\\ell}\\cdot \\boldsymbol{r}_{ij}$. The term $\\mathrm{sign}(\\boldsymbol{\\ell}\\cdot \\boldsymbol{r}_{ij})$ is therefore parity-even.\n    *   The spins $\\boldsymbol{s}_i$ and $\\boldsymbol{s}_j$ are axial vectors, invariant under parity. Their cross product $\\boldsymbol{s}_i \\times \\boldsymbol{s}_j$ is also an axial vector (a pseudo-vector).\n    *   The dot product of a polar vector ($\\boldsymbol{\\ell}$) and an axial vector ($\\boldsymbol{s}_i \\times \\boldsymbol{s}_j$) is a pseudo-scalar, which changes sign under parity: $(\\!-\\boldsymbol{\\ell})\\cdot(\\boldsymbol{s}_i \\times \\boldsymbol{s}_j) = -(\\boldsymbol{\\ell} \\cdot (\\boldsymbol{s}_i \\times \\boldsymbol{s}_j))$.\n    *   Therefore, $W^{(\\mathrm{spin})}_{ij}$ is the product of a parity-even term and a parity-odd term, making it parity-odd. Its mean, $\\widehat{S}$, should be consistent with zero in a parity-symmetric universe.\n\n*   **Handedness-marked estimator weight**: $W^{(\\mathrm{hand})}_{ij} \\equiv \\frac{1}{2}\\,\\big(h_j - h_i\\big)\\,\\mathrm{sign}(\\boldsymbol{\\ell}\\cdot \\boldsymbol{r}_{ij})$.\n    *   The handedness $h_i = \\mathrm{sign}(\\boldsymbol{s}_i \\cdot \\boldsymbol{\\ell})$ is the sign of a pseudo-scalar, making it a pseudo-scalar itself. Under parity, $h_i \\to \\mathrm{sign}(\\boldsymbol{s}_i \\cdot (-\\boldsymbol{\\ell})) = -h_i$.\n    *   The difference $(h_j - h_i)$ is therefore also a pseudo-scalar: $(-h_j) - (-h_i) = -(h_j - h_i)$.\n    *   As before, $\\mathrm{sign}(\\boldsymbol{\\ell}\\cdot \\boldsymbol{r}_{ij})$ is parity-even.\n    *   $W^{(\\mathrm{hand})}_{ij}$ is thus parity-odd, and its mean, $\\widehat{H}$, is also expected to be zero under parity symmetry.\n\n#### 2. Algorithmic Implementation\n\nThe overall algorithm is structured to execute the four test cases described. Each test case involves data generation followed by statistical estimation.\n\n*   **Data Generation**: Mock galaxy catalogs (sets of positions and marks) are generated according to the specifications of each test case. This involves:\n    *   **Positions**: Sampling from a uniform distribution in $[0, L)^3$ or from a non-uniform distribution $n(z) \\propto 1 + c(2z/L-1)$ along the $z$-axis. The latter is implemented using the inverse transform sampling method. The cumulative distribution function for $u = z/L$ is $F(u) = (1-c)u + cu^2$, which is inverted to find $u$ from a uniform random variate $\\xi \\in [0,1)$.\n    *   **Spins and Handedness**: Isotropic unit spin vectors are generated by normalizing $3$D vectors of standard normal deviates. Handedness marks $h_i \\in \\{-1, +1\\}$ are either derived from these spins via $h_i = \\mathrm{sign}(\\boldsymbol{s}_i \\cdot \\boldsymbol{\\ell})$ or assigned probabilistically based on the $z$-coordinate to simulate a parity-violating signal.\n\n*   **Core Computation**: The heart of the algorithm is a loop over all unique pairs of points $(i, j)$ where $i  j$. For each pair:\n    1.  The separation vector $\\boldsymbol{r}_{ij}$ is calculated using the minimum-image convention for the periodic cubic domain: $\\boldsymbol{r}_{ij} = (\\boldsymbol{x}_j - \\boldsymbol{x}_i) - L \\cdot \\mathrm{round}((\\boldsymbol{x}_j - \\boldsymbol{x}_i) / L)$.\n    2.  The scalar separation distance $r_{ij} = \\|\\boldsymbol{r}_{ij}\\|$ is computed.\n    3.  The pair is assigned to the appropriate radial separation bin $[r_a, r_b)$ based on $r_{ij}$.\n    4.  The corresponding parity-odd weight, $W^{(\\mathrm{spin})}_{ij}$ or $W^{(\\mathrm{hand})}_{ij}$, is calculated based on the pair's marks and separation vector.\n    5.  For the identified bin, three quantities are updated: the pair count $N_\\mathrm{pairs}$, the sum of weights $\\sum W_{ij}$, and the sum of squared weights $\\sum W_{ij}^2$.\n\n*   **Statistical Analysis**: After iterating through all pairs, the final statistics for each bin are computed:\n    1.  Mean weight: $\\bar{W} = (\\sum W_{ij}) / N_{\\mathrm{pairs}}$.\n    2.  Mean squared weight: $\\overline{W^2} = (\\sum W_{ij}^2) / N_{\\mathrm{pairs}}$.\n    3.  Variance of the mean: $\\sigma^2 = (\\overline{W^2} - \\bar{W}^2) / N_{\\mathrm{pairs}}$.\n    4.  Standardized score: $z = \\bar{W} / \\sigma$. This is calculated for bins where $N_{\\mathrm{pairs}}  0$ and $\\sigma  0$. If these conditions are not met, the $z$-score is taken to be $0$.\n\n*   **Test Suite Execution**: The four test cases are executed sequentially.\n    *   Cases $1$ and $2$ serve as null tests, verifying that the pipeline finds no significant signal ($|\\text{z-score}| \\le 4.0$) when the data is generated under parity-symmetric assumptions.\n    *   Case $3$ is a detection test, injecting a parity-violating handedness bias to confirm the pipeline's sensitivity. The result is an aggregate signal-to-noise ratio, $\\mathrm{SNR}_\\mathrm{tot} = \\sqrt{\\sum z^2}$.\n    *   Case $4$ is a robustness test, introducing a systematic effect (anisotropic selection function) that is not parity-violating, to ensure it doesn't create a spurious signal. The result is again a check on the maximum absolute $z$-score.\n\nThis structured approach ensures a rigorous and verifiable test of the statistical estimators and their numerical implementation.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef generate_spins_isotropic(N, rng):\n    \"\"\"Generates N isotropic unit vectors.\"\"\"\n    vecs = rng.standard_normal(size=(N, 3))\n    norms = np.linalg.norm(vecs, axis=1, keepdims=True)\n    # Avoid division by zero, though highly unlikely with floats\n    zero_norm_mask = (norms == 0)\n    norms[zero_norm_mask] = 1.0\n    return vecs / norms\n\ndef calculate_statistics(positions, marks, L, bins, estimator_type):\n    \"\"\"\n    Calculates the parity-odd statistics for a given set of points and marks.\n    \"\"\"\n    N = len(positions)\n    num_bins = len(bins) - 1\n    \n    sum_W = np.zeros(num_bins)\n    sum_W2 = np.zeros(num_bins)\n    N_pairs = np.zeros(num_bins, dtype=int)\n    \n    ell = np.array([0.0, 0.0, 1.0])\n    \n    for i in range(N):\n        for j in range(i + 1, N):\n            rij_vec = positions[j] - positions[i]\n            rij_vec -= L * np.round(rij_vec / L)\n            rij_dist = np.linalg.norm(rij_vec)\n            \n            bin_idx = np.searchsorted(bins, rij_dist, side='right') - 1\n            \n            if 0 = bin_idx  num_bins:\n                sign_ell_r = np.sign(rij_vec[2])\n                \n                if estimator_type == 'spin':\n                    s_i = marks[i]\n                    s_j = marks[j]\n                    s_cross = np.cross(s_i, s_j)\n                    weight = sign_ell_r * np.dot(ell, s_cross)\n                elif estimator_type == 'hand':\n                    h_i = marks[i]\n                    h_j = marks[j]\n                    weight = 0.5 * (h_j - h_i) * sign_ell_r\n                else:\n                    raise ValueError(\"Unknown estimator type\")\n                    \n                sum_W[bin_idx] += weight\n                sum_W2[bin_idx] += weight**2\n                N_pairs[bin_idx] += 1\n    \n    # Calculate z-scores, handling bins with no pairs or zero variance\n    z_scores = np.zeros(num_bins)\n    valid_bins = N_pairs > 0\n    \n    if np.any(valid_bins):\n        mean_W = np.divide(sum_W[valid_bins], N_pairs[valid_bins])\n        mean_W2 = np.divide(sum_W2[valid_bins], N_pairs[valid_bins])\n        \n        var_W = mean_W2 - mean_W**2\n        \n        # Ensure variance is non-negative due to potential float precision issues\n        var_W[var_W  0] = 0\n        \n        var_mean_W = np.divide(var_W, N_pairs[valid_bins])\n        \n        sigma = np.sqrt(var_mean_W)\n        \n        # Calculate z-score where sigma > 0, otherwise it's 0\n        non_zero_sigma = sigma > 0\n        z_scores_valid = np.zeros_like(mean_W)\n        z_scores_valid[non_zero_sigma] = np.divide(mean_W[non_zero_sigma], sigma[non_zero_sigma])\n        \n        z_scores[valid_bins] = z_scores_valid\n        \n    return z_scores, N_pairs\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    \n    # Common parameters\n    L = 200.0\n    N = 900\n    bins = np.array([5.0, 12.857142857142858, 20.714285714285715, \n                     28.571428571428573, 36.42857142857143, 44.285714285714285, \n                     52.142857142857146, 60.0, 67.85714285714286])\n    ell = np.array([0.0, 0.0, 1.0])\n    results = []\n\n    # Test Case 1\n    seed1 = 12345\n    rng1 = np.random.default_rng(seed1)\n    positions1 = rng1.uniform(0, L, size=(N, 3))\n    spins1 = generate_spins_isotropic(N, rng1)\n    z_scores1, _ = calculate_statistics(positions1, spins1, L, bins, 'spin')\n    max_abs_z1 = np.max(np.abs(z_scores1))\n    ans1 = max_abs_z1 = 4.0\n    results.append(ans1)\n    \n    # Test Case 2\n    seed2 = 67890\n    rng2 = np.random.default_rng(seed2)\n    positions2 = rng2.uniform(0, L, size=(N, 3))\n    spins2 = generate_spins_isotropic(N, rng2)\n    h2 = np.sign(spins2 @ ell)\n    h2[h2 == 0] = 1 # Treat zero projection as positive handedness\n    z_scores2, _ = calculate_statistics(positions2, h2, L, bins, 'hand')\n    max_abs_z2 = np.max(np.abs(z_scores2))\n    ans2 = max_abs_z2 = 4.0\n    results.append(ans2)\n    \n    # Test Case 3\n    seed3 = 24680\n    alpha = 0.8\n    rng3 = np.random.default_rng(seed3)\n    positions3 = rng3.uniform(0, L, size=(N, 3))\n    z_coords3 = positions3[:, 2]\n    prob_h_plus_1 = 0.5 * (1.0 + alpha * (2.0 * z_coords3 / L - 1.0))\n    rand_nums = rng3.uniform(0, 1, size=N)\n    h3 = np.where(rand_nums  prob_h_plus_1, 1.0, -1.0)\n    z_scores3, N_pairs3 = calculate_statistics(positions3, h3, L, bins, 'hand')\n    ans3 = np.sqrt(np.sum(z_scores3[N_pairs3 > 0]**2))\n    results.append(ans3)\n\n    # Test Case 4\n    seed4 = 11223\n    c = 0.8\n    rng4 = np.random.default_rng(seed4)\n    pos_x4 = rng4.uniform(0, L, size=N)\n    pos_y4 = rng4.uniform(0, L, size=N)\n    xi = rng4.uniform(0, 1, size=N)\n    # Using inverse transform sampling for z\n    # F(u) = (1-c)u + cu^2 = xi. Solve for u=z/L: cu^2 + (1-c)u - xi = 0\n    u = (c - 1 + np.sqrt((1 - c)**2 + 4 * c * xi)) / (2 * c)\n    pos_z4 = L * u\n    positions4 = np.stack([pos_x4, pos_y4, pos_z4], axis=1)\n    h4 = rng4.choice([-1.0, 1.0], size=N)\n    z_scores4, _ = calculate_statistics(positions4, h4, L, bins, 'hand')\n    max_abs_z4 = np.max(np.abs(z_scores4))\n    ans4 = max_abs_z4 = 4.0\n    results.append(ans4)\n\n    # Format output as required\n    output_str = f\"[{str(results[0]).lower()},{str(results[1]).lower()},{float(results[2])},{str(results[3]).lower()}]\"\n    print(output_str)\n\n```", "id": "3499960"}]}