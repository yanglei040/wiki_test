{"hands_on_practices": [{"introduction": "Numerical methods often involve a trade-off between simplicity and accuracy. While low-order methods like the forward Euler scheme are easy to implement, their accuracy can be limited for practical applications. This exercise introduces Richardson extrapolation, a powerful and general technique to boost accuracy by combining results from two different step sizes to cancel the leading-order error term. You will derive the classic extrapolation formula from first principles, turning a first-order approximation into a more accurate second-order one, a fundamental skill in numerical error control [@problem_id:3471820].", "problem": "Consider a flat, matter-dominated Universe, where the scale factor $a(t)$ satisfies the Friedmann ordinary differential equation (ODE) $\\dot{a}(t) = f(t,a(t))$ with $f(t,a) = H_{0}\\,a^{-1/2}$, and $a(t)$ is strictly positive on a finite interval $[t_{0}, t_{f}]$. Assume $a(t_{0})$ is known exactly. In numerical cosmology, forward Euler time stepping is frequently used to integrate such ODEs: starting from $a(t_{0})$, one computes a grid function $a_{h}(t_{f})$ by applying the forward Euler method with a uniform step $h$ across $[t_{0}, t_{f}]$, and similarly $a_{h/2}(t_{f})$ with the step halved. Using only well-established properties of consistent, zero-stable first-order one-step methods (forward Euler) and the definition of local truncation error, reason from first principles to show that the global discretization error for $a_{h}(t_{f})$ admits an asymptotic expansion in powers of $h$ with a leading term linear in $h$. Then, apply Richardson extrapolation to eliminate the leading-order error term and construct a second-order accurate estimate $\\widetilde{a}(t_{f})$ of $a(t_{f})$ in terms of $a_{h}(t_{f})$ and $a_{h/2}(t_{f})$. Derive the extrapolation formula explicitly as a closed-form analytic expression for $\\widetilde{a}(t_{f})$ in terms of $a_{h}(t_{f})$ and $a_{h/2}(t_{f})$. Express your final answer as a single analytic expression. You do not need to compute any numerical values.", "solution": "The problem requires a derivation of the Richardson extrapolation formula for a numerical solution to an ordinary differential equation (ODE) obtained via the forward Euler method.\n\nLet the exact solution to the initial value problem $\\dot{a}(t) = f(t, a(t))$ with $a(t_0) = a_0$ be denoted by $a(t)$. The ODE is specified as $\\dot{a}(t) = H_{0}\\,a^{-1/2}$, with $a(t)$ being strictly positive on the integration interval $[t_0, t_f]$. The function $f(t,a) = H_0 a^{-1/2}$ is infinitely differentiable with respect to $a$ for $a0$. This ensures that the exact solution $a(t)$ is sufficiently smooth, which is a prerequisite for the validity of asymptotic error expansions.\n\nThe forward Euler method is a one-step numerical method defined by the recurrence relation:\n$$ a_{n+1} = a_n + h f(t_n, a_n) $$\nwhere $t_n = t_0 + n h$, $h$ is the step size, and $a_n$ is the numerical approximation to $a(t_n)$. The problem states that this method is consistent, zero-stable, and first-order.\n\nA fundamental result in the theory of numerical methods for ODEs, often known as Gragg's theorem or the error expansion theorem, states that for a sufficiently smooth ODE and a consistent, zero-stable one-step method of order $p$, the global discretization error at a fixed time $t_f = t_0 + N h$ admits an asymptotic expansion in powers of the step size $h$. The expansion takes the form:\n$$ e_h(t_f) = a(t_f) - a_h(t_f) = C_1 h^p + C_2 h^{p+1} + C_3 h^{p+2} + \\dots $$\nwhere $a_h(t_f)$ is the numerical solution at $t_f$ computed with step size $h$, and the coefficients $C_k$ are independent of $h$ but depend on $t_f$ and the ODE itself.\n\nThe forward Euler method has an order of accuracy $p=1$. Applying the aforementioned theorem, the global error for the approximation $a_h(t_f)$ has an asymptotic expansion with a leading term that is linear in $h$:\n$$ a(t_f) - a_h(t_f) = C_1 h + C_2 h^2 + O(h^3) $$\nThis can be rewritten as:\n$$ a(t_f) = a_h(t_f) + C_1 h + C_2 h^2 + O(h^3) \\quad (1) $$\nThis expression fulfills the first requirement of the problem, showing that the global error contains a leading term proportional to $h$.\n\nRichardson extrapolation is a technique used to improve the accuracy of a numerical method by eliminating the leading-order term of the error series. To apply it, we compute a second numerical solution, $a_{h/2}(t_f)$, using a halved step size, $h/2$. The number of steps required to reach $t_f$ is now $2N$, where $N = (t_f - t_0)/h$. The asymptotic error expansion for this second solution is obtained by substituting $h$ with $h/2$ in equation $(1)$:\n$$ a(t_f) = a_{h/2}(t_f) + C_1 \\left(\\frac{h}{2}\\right) + C_2 \\left(\\frac{h}{2}\\right)^2 + O\\left(\\left(\\frac{h}{2}\\right)^3\\right) $$\n$$ a(t_f) = a_{h/2}(t_f) + \\frac{C_1}{2} h + \\frac{C_2}{4} h^2 + O(h^3) \\quad (2) $$\n\nWe now have a system of two equations, $(1)$ and $(2)$, for two primary unknowns, the exact solution $a(t_f)$ and the unknown coefficient $C_1$. The goal is to eliminate the $C_1 h$ term. We can achieve this by multiplying equation $(2)$ by $2$ and then subtracting equation $(1)$:\n\nFirst, multiply equation $(2)$ by $2$:\n$$ 2 a(t_f) = 2 a_{h/2}(t_f) + C_1 h + \\frac{C_2}{2} h^2 + O(h^3) \\quad (3) $$\n\nNow, subtract equation $(1)$ from equation $(3)$:\n$$ 2 a(t_f) - a(t_f) = \\left(2 a_{h/2}(t_f) + C_1 h + \\frac{C_2}{2} h^2\\right) - \\left(a_h(t_f) + C_1 h + C_2 h^2\\right) + O(h^3) $$\nSimplifying the expression gives:\n$$ a(t_f) = 2 a_{h/2}(t_f) - a_h(t_f) + \\left(\\frac{C_2}{2} - C_2\\right) h^2 + O(h^3) $$\n$$ a(t_f) = 2 a_{h/2}(t_f) - a_h(t_f) - \\frac{C_2}{2} h^2 + O(h^3) $$\n\nLet us define the new, extrapolated estimate for $a(t_f)$ as $\\widetilde{a}(t_f)$:\n$$ \\widetilde{a}(t_f) = 2 a_{h/2}(t_f) - a_h(t_f) $$\nThe error of this new estimate is:\n$$ a(t_f) - \\widetilde{a}(t_f) = - \\frac{C_2}{2} h^2 + O(h^3) $$\nThe leading error term is now proportional to $h^2$, which means that $\\widetilde{a}(t_f)$ is a second-order accurate estimate of the true solution $a(t_f)$. The procedure has successfully eliminated the leading-order error term.\n\nThe explicit, closed-form analytic expression for the second-order accurate estimate $\\widetilde{a}(t_f)$ in terms of the first-order estimates $a_h(t_f)$ and $a_{h/2}(t_f)$ is therefore given by the linear combination derived above.", "answer": "$$\\boxed{2 a_{h/2}(t_{f}) - a_{h}(t_{f})}$$", "id": "3471820"}, {"introduction": "Physical laws often impose fundamental constraints on solutions, such as the non-negativity of energy density or particle number. A numerical scheme is not guaranteed to respect these constraints and can produce unphysical results, like negative densities, if not used carefully. This practice demonstrates how to perform a stability analysis to derive step-size limits that ensure a solution remains positive for several common explicit time-stepping methods, a crucial task when modeling inherently positive physical quantities [@problem_id:3471815].", "problem": "Consider the ordinary differential equation for decaying dark matter energy density in a spatially homogeneous Friedmann–Lemaître–Robertson–Walker background, derived from covariant energy conservation with a decay sink, given by\n$$\n\\frac{d\\rho}{dt} = -3\\,H(t)\\,\\rho(t) - \\Gamma\\,\\rho(t),\n$$\nwhere $H(t)$ is the Hubble expansion rate and $\\Gamma \\ge 0$ is a constant decay rate. Define the combined rate\n$$\n\\Lambda(t) \\equiv 3\\,H(t) + \\Gamma,\n$$\nso that the equation is linear and homogeneous,\n$$\n\\frac{d\\rho}{dt} = -\\Lambda(t)\\,\\rho(t).\n$$\nYou will construct explicit finite-difference updates that are positivity-preserving and derive step-size limits ensuring nonnegativity of the numerical solution.\n\nYour tasks are:\n\n- Starting from the conservation law and the definition of $H(t)$, formally justify the model $\\frac{d\\rho}{dt} = -3\\,H(t)\\,\\rho(t) - \\Gamma \\rho(t)$, and the linear form $\\frac{d\\rho}{dt} = -\\Lambda(t)\\,\\rho(t)$ with $\\Lambda(t) \\ge 0$.\n- For each of the following explicit time discretizations, construct a one-step update for $\\rho_{n+1}$ and derive a sufficient step-size restriction expressed in terms of an upper bound on $\\Lambda(t)$ that guarantees $\\rho_{n+1} \\ge 0$ whenever the required previous values are nonnegative:\n  - Forward Euler (one-step explicit method).\n  - Two-stage Strong Stability Preserving Runge–Kutta of order two (SSP Runge–Kutta, second order).\n  - Two-step Adams–Bashforth of order two (explicit multistep method), assuming the first step is taken by Forward Euler under its own positivity condition.\n  Your derivation must begin from the method’s defining update and use only inequalities involving $\\Lambda(t)$ and a bound for $\\Lambda(t)$ over a step. Do not assume $\\Lambda(t)$ is constant on a step; you may use any bound $\\ell$ such that $\\Lambda(t) \\le \\ell$ on the interval of interest.\n- Implement a program that:\n  - For each test case below, computes a uniform time step $\\Delta t_{\\max}$ for each method that suffices to preserve positivity over the entire integration interval, by using the supremum bound $\\lambda_{\\max} \\equiv \\sup_{t \\in [t_0, t_0+T]} \\Lambda(t)$ implied by your derived conditions.\n  - Integrates the equation numerically on $[t_0, t_0+T]$ using each method with a step $\\Delta t = 0.99\\,\\Delta t_{\\max}$, starting from $\\rho(t_0) = \\rho_0$, and verifies that all discrete densities remain nonnegative.\n  - Reports, for each test case, a list containing the three step-size bounds in $\\mathrm{Gyr}$, followed by three booleans indicating whether the corresponding numerical integration remained nonnegative at all grid points.\n- Units and formatting:\n  - Time $t$ must be in $\\mathrm{Gyr}$, the Hubble rate $H$ must be in $\\mathrm{Gyr}^{-1}$, the decay rate $\\Gamma$ must be in $\\mathrm{Gyr}^{-1}$, and the reported step sizes must be in $\\mathrm{Gyr}$.\n  - Angles are not used.\n  - Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is itself a list for one test case in the order given below.\n\nTest suite (use exactly these three cases):\n- Case $1$ (constant expansion): $H(t) = H_0$ with $H_0 = 0.07\\,\\mathrm{Gyr}^{-1}$, $\\Gamma = 0.01\\,\\mathrm{Gyr}^{-1}$, $t_0 = 0.0\\,\\mathrm{Gyr}$, $T = 10.0\\,\\mathrm{Gyr}$, $\\rho_0 = 1.0$.\n- Case $2$ (matter-dominated scaling): $H(t) = \\dfrac{2}{3\\,t}$, $\\Gamma = 0.2\\,\\mathrm{Gyr}^{-1}$, $t_0 = 1.0\\,\\mathrm{Gyr}$, $T = 5.0\\,\\mathrm{Gyr}$, $\\rho_0 = 1.0$.\n- Case $3$ (radiation-dominated scaling): $H(t) = \\dfrac{1}{2\\,t}$, $\\Gamma = 0.0\\,\\mathrm{Gyr}^{-1}$, $t_0 = 0.5\\,\\mathrm{Gyr}$, $T = 0.5\\,\\mathrm{Gyr}$, $\\rho_0 = 1.0$.\n\nFinal output format:\n- The program must print a single line containing a list of three elements, one per test case, and each element must be a list of six entries in the order $[\\Delta t_{\\max}^{\\mathrm{FE}}, \\Delta t_{\\max}^{\\mathrm{SSP2}}, \\Delta t_{\\max}^{\\mathrm{AB2}}, \\mathrm{pos}_{\\mathrm{FE}}, \\mathrm{pos}_{\\mathrm{SSP2}}, \\mathrm{pos}_{\\mathrm{AB2}}]$, where each $\\Delta t_{\\max}$ is a floating-point number in $\\mathrm{Gyr}$ and each $\\mathrm{pos}$ is a boolean.", "solution": "The problem is assessed to be valid as it is scientifically grounded, mathematically well-posed, objective, and complete. We proceed with the solution.\n\n### Justification of the Governing Equation\n\nThe governing ordinary differential equation (ODE) for the energy density $\\rho(t)$ of decaying dark matter can be derived from the principles of general relativity in a cosmological context. The starting point is the covariant conservation of the energy-momentum tensor, $T^{\\mu\\nu}$, which in the presence of an energy-momentum sink/source $J^\\nu$ is expressed as $\\nabla_\\mu T^{\\mu\\nu} = J^\\nu$.\n\nFor a perfect fluid, the energy-momentum tensor is $T^{\\mu\\nu} = (\\rho+p)u^\\mu u^\\nu + p g^{\\mu\\nu}$, where $\\rho$ is the energy density, $p$ is the pressure, $u^\\mu$ is the fluid four-velocity, and $g^{\\mu\\nu}$ is the metric tensor. In a spatially homogeneous and isotropic Friedmann–Lemaître–Robertson–Walker (FLRW) universe, and in a comoving reference frame where $u^\\mu = (1, 0, 0, 0)$, the time component ($\\nu=0$) of the conservation law simplifies to the fluid equation:\n$$\n\\dot{\\rho} + 3H(t)(\\rho+p) = J^0,\n$$\nwhere the dot denotes a derivative with respect to cosmic time $t$, and $H(t) \\equiv \\dot{a}(t)/a(t)$ is the Hubble expansion rate, with $a(t)$ being the scale factor of the universe.\n\nThe problem considers dark matter, which is modeled as a non-relativistic species. For such matter, the kinetic energy of its constituent particles is much smaller than their rest mass energy, which implies that the pressure is negligible compared to the energy density, i.e., $p \\ll \\rho$. We thus make the standard approximation $p=0$.\n\nThe decay of dark matter particles into other species (e.g., radiation) acts as a sink for the dark matter energy density. A common phenomenological model for this process assumes the rate of energy loss is proportional to the energy density itself. This is represented by a sink term $J^0 = -\\Gamma\\rho$, where $\\Gamma \\ge 0$ is the constant decay rate.\n\nSubstituting $p=0$ and $J^0 = -\\Gamma\\rho$ into the fluid equation gives:\n$$\n\\dot{\\rho} + 3H(t)\\rho = -\\Gamma\\rho.\n$$\nRearranging this equation yields the ODE provided in the problem statement:\n$$\n\\frac{d\\rho}{dt} = -3H(t)\\rho(t) - \\Gamma\\rho(t).\n$$\nBy defining the combined rate $\\Lambda(t) \\equiv 3H(t) + \\Gamma$, the equation takes the simple linear homogeneous form:\n$$\n\\frac{d\\rho}{dt} = -\\Lambda(t)\\rho(t).\n$$\nFor an expanding universe, the scale factor $a(t)$ is non-decreasing, so the Hubble rate $H(t) = \\dot{a}/a$ is non-negative. Since the decay rate is given as $\\Gamma \\ge 0$, the combined rate $\\Lambda(t) = 3H(t) + \\Gamma$ is guaranteed to be non-negative for all $t$. The exact solution $\\rho(t) = \\rho(t_0) \\exp(-\\int_{t_0}^t \\Lambda(\\tau)d\\tau)$ is therefore always non-negative if the initial density $\\rho(t_0)$ is non-negative.\n\n### Derivation of Positivity-Preserving Step-Size Limits\n\nWe derive sufficient step-size restrictions for three explicit numerical methods to ensure that the numerical solution $\\rho_n$ remains non-negative at all time steps, assuming non-negative values at previous steps. Let $\\ell$ be any upper bound for the combined rate, i.e., $\\Lambda(t) \\le \\ell$ over the integration interval $[t_0, t_0+T]$.\n\n#### Forward Euler (FE) Method\nThis is a $1$-step explicit method. The update rule for the equation $\\dot{\\rho} = f(t, \\rho)$ is $\\rho_{n+1} = \\rho_n + \\Delta t f(t_n, \\rho_n)$. Substituting $f(t_n, \\rho_n) = -\\Lambda(t_n)\\rho_n$, we obtain:\n$$\n\\rho_{n+1} = \\rho_n + \\Delta t (-\\Lambda(t_n)\\rho_n) = \\rho_n(1 - \\Delta t \\Lambda(t_n)).\n$$\nTo ensure $\\rho_{n+1} \\ge 0$ given that $\\rho_n \\ge 0$, we require the coefficient of $\\rho_n$ to be non-negative:\n$$\n1 - \\Delta t \\Lambda(t_n) \\ge 0 \\implies \\Delta t \\Lambda(t_n) \\le 1.\n$$\nFor a uniform step size $\\Delta t$ to preserve positivity at every step, this condition must hold for all $n$. A sufficient condition is that $\\Delta t \\cdot \\sup_t \\Lambda(t) \\le 1$. Using the bound $\\ell$, the step size must satisfy:\n$$\n\\Delta t \\le \\frac{1}{\\ell}.\n$$\n\n#### Two-Stage Strong Stability Preserving Runge–Kutta (SSP2)\nThe $2$-stage SSP Runge-Kutta method of order $2$ (SSP$2$) is structured as a convex combination of Forward Euler-like stages. For $\\dot{\\rho} = f(t, \\rho)$, the update proceeds as:\n\\begin{align*}\n\\rho^{(1)} = \\rho_n + \\Delta t f(t_n, \\rho_n) \\\\\n\\rho_{n+1} = \\frac{1}{2}\\rho_n + \\frac{1}{2}\\left(\\rho^{(1)} + \\Delta t f(t_n+\\Delta t, \\rho^{(1)})\\right)\n\\end{align*}\nSubstituting $f(t, \\rho) = -\\Lambda(t)\\rho$:\n\\begin{align*}\n\\rho^{(1)} = \\rho_n - \\Delta t \\Lambda(t_n)\\rho_n = \\rho_n(1 - \\Delta t \\Lambda(t_n)) \\\\\n\\rho_{n+1} = \\frac{1}{2}\\rho_n + \\frac{1}{2}\\left(\\rho^{(1)} - \\Delta t \\Lambda(t_{n+1})\\rho^{(1)}\\right) = \\frac{1}{2}\\rho_n + \\frac{1}{2}\\rho^{(1)}(1 - \\Delta t \\Lambda(t_{n+1}))\n\\end{align*}\nAssuming $\\rho_n \\ge 0$, we want to find a condition on $\\Delta t$ that guarantees $\\rho_{n+1} \\ge 0$.\n1.  For the intermediate stage value $\\rho^{(1)}$ to be non-negative, we require $1 - \\Delta t \\Lambda(t_n) \\ge 0$.\n2.  With $\\rho^{(1)} \\ge 0$ and $\\rho_n \\ge 0$, for $\\rho_{n+1}$ (which is a sum of two terms) to be non-negative, it is sufficient that the coefficient of $\\rho^{(1)}$ is non-negative: $1 - \\Delta t \\Lambda(t_{n+1}) \\ge 0$.\nBoth conditions are satisfied if $\\Delta t \\Lambda(t) \\le 1$ for all $t$ in the step interval $[t_n, t_{n+1}]$. A uniform step-size restriction sufficient for positivity is therefore:\n$$\n\\Delta t \\le \\frac{1}{\\ell}.\n$$\n\n#### Two-Step Adams–Bashforth (AB2)\nThe $2$-step Adams-Bashforth method of order $2$ (AB$2$) is an explicit linear multistep method. Its update rule is:\n$$\n\\rho_{n+1} = \\rho_n + \\frac{\\Delta t}{2} \\left( 3f(t_n, \\rho_n) - f(t_{n-1}, \\rho_{n-1}) \\right).\n$$\nSubstituting $f(t, \\rho) = -\\Lambda(t)\\rho$, we have:\n$$\n\\rho_{n+1} = \\rho_n + \\frac{\\Delta t}{2} \\left( -3\\Lambda(t_n)\\rho_n + \\Lambda(t_{n-1})\\rho_{n-1} \\right).\n$$\nRearranging terms by grouping $\\rho_n$ and $\\rho_{n-1}$ gives:\n$$\n\\rho_{n+1} = \\rho_n \\left( 1 - \\frac{3}{2}\\Delta t \\Lambda(t_n) \\right) + \\rho_{n-1} \\left( \\frac{1}{2}\\Delta t \\Lambda(t_{n-1}) \\right).\n$$\nAssume the previous values are non-negative: $\\rho_n \\ge 0$ and $\\rho_{n-1} \\ge 0$. The second term involving $\\rho_{n-1}$ is manifestly non-negative since $\\Delta t  0$ and $\\Lambda(t) \\ge 0$. A sufficient condition to ensure $\\rho_{n+1} \\ge 0$ is that the coefficient of $\\rho_n$ is also non-negative:\n$$\n1 - \\frac{3}{2}\\Delta t \\Lambda(t_n) \\ge 0 \\implies \\Delta t \\Lambda(t_n) \\le \\frac{2}{3}.\n$$\nThe first step of this $2$-step method must be computed with a $1$-step method, specified as Forward Euler. The positivity of the FE step requires $\\Delta t \\Lambda(t_0) \\le 1$. Since $\\frac{2}{3}  1$, the AB$2$ condition is stricter and, if enforced for all steps, will automatically satisfy the positivity requirement for the startup step. A uniform step-size restriction sufficient for positivity throughout the integration is:\n$$\n\\Delta t \\le \\frac{2}{3\\ell}.\n$$", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test cases.\n    It computes positivity-preserving step-size limits for three numerical methods\n    and verifies that the solutions remain non-negative when integrated with these step sizes.\n    \"\"\"\n    test_cases = [\n        {\n            \"H_func\": lambda t: 0.07,\n            \"Gamma\": 0.01,\n            \"t0\": 0.0,\n            \"T\": 10.0,\n            \"rho0\": 1.0,\n        },\n        {\n            \"H_func\": lambda t: 2.0 / (3.0 * t),\n            \"Gamma\": 0.2,\n            \"t0\": 1.0,\n            \"T\": 5.0,\n            \"rho0\": 1.0,\n        },\n        {\n            \"H_func\": lambda t: 1.0 / (2.0 * t),\n            \"Gamma\": 0.0,\n            \"t0\": 0.5,\n            \"T\": 0.5,\n            \"rho0\": 1.0,\n        },\n    ]\n\n    all_results = []\n    \n    for case in test_cases:\n        H_func = case[\"H_func\"]\n        Gamma = case[\"Gamma\"]\n        t0 = case[\"t0\"]\n        T = case[\"T\"]\n        rho0 = case[\"rho0\"]\n\n        Lambda_func = lambda t: 3.0 * H_func(t) + Gamma\n        \n        # For the given H(t) functions, Lambda(t) is monotonically non-increasing on t > 0.\n        # The integration intervals are [t0, t0+T] where t0 > 0 or t0=0 and H is constant.\n        # Thus, the supremum of Lambda(t) occurs at t = t0.\n        lambda_max = Lambda_func(t0)\n\n        # Calculate maximum step sizes based on derived positivity conditions\n        dt_max_fe = 1.0 / lambda_max\n        dt_max_ssp2 = 1.0 / lambda_max\n        dt_max_ab2 = (2.0 / 3.0) / lambda_max\n\n        case_results = [dt_max_fe, dt_max_ssp2, dt_max_ab2]\n\n        # --- Run simulations for each method ---\n        # The factor 0.99 is used to ensure the step size is strictly within the bound.\n        # A small negative tolerance is used for the positivity check to account for floating-point inaccuracies.\n        fp_tol = -1e-15\n\n        # Method 1: Forward Euler\n        dt = 0.99 * dt_max_fe\n        num_steps = int(np.ceil(T / dt))\n        t_grid = t0 + np.arange(num_steps + 1) * dt\n        rho = np.zeros(num_steps + 1)\n        rho[0] = rho0\n        for n in range(num_steps):\n            rho[n+1] = rho[n] * (1.0 - dt * Lambda_func(t_grid[n]))\n        pos_fe = np.all(rho >= fp_tol)\n        case_results.append(pos_fe)\n\n        # Method 2: SSP Runge-Kutta 2\n        dt = 0.99 * dt_max_ssp2\n        num_steps = int(np.ceil(T / dt))\n        t_grid = t0 + np.arange(num_steps + 1) * dt\n        rho = np.zeros(num_steps + 1)\n        rho[0] = rho0\n        for n in range(num_steps):\n            rho_stage1 = rho[n] * (1.0 - dt * Lambda_func(t_grid[n]))\n            rho[n+1] = 0.5 * rho[n] + 0.5 * rho_stage1 * (1.0 - dt * Lambda_func(t_grid[n+1]))\n        pos_ssp2 = np.all(rho >= fp_tol)\n        case_results.append(pos_ssp2)\n\n        # Method 3: Adams-Bashforth 2\n        dt = 0.99 * dt_max_ab2\n        num_steps = int(np.ceil(T / dt))\n        t_grid = t0 + np.arange(num_steps + 1) * dt\n        rho = np.zeros(num_steps + 1)\n        rho[0] = rho0\n        \n        # At least two points (t0, t1) are needed for the AB2 formula.\n        if num_steps > 0:\n            # Startup step using Forward Euler for rho at t1\n            rho[1] = rho[0] * (1.0 - dt * Lambda_func(t_grid[0]))\n            \n            # Main AB2 loop for subsequent steps\n            for n in range(1, num_steps):\n                f_n = -Lambda_func(t_grid[n]) * rho[n]\n                f_n_minus_1 = -Lambda_func(t_grid[n-1]) * rho[n-1]\n                rho[n+1] = rho[n] + (dt / 2.0) * (3.0 * f_n - f_n_minus_1)\n        \n        pos_ab2 = np.all(rho >= fp_tol)\n        case_results.append(pos_ab2)\n        \n        all_results.append(case_results)\n\n    # Format the final output string as a list of lists. The standard `str` representation\n    # for lists in Python is used, which includes spaces after commas and capitalized booleans.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```", "id": "3471815"}, {"introduction": "After implementing a numerical solver, a critical question arises: does the code work as intended? A cornerstone of code verification is the convergence test, where you empirically measure the method's order of accuracy and check if it matches the theoretical expectation. This hands-on exercise guides you through building a test harness to perform such a measurement for a simplified cosmological ODE, and also explores how practical features like adaptive time-stepping can affect the observed performance, a key lesson in interpreting computational results [@problem_id:3471890].", "problem": "You are to construct a complete, runnable program that implements a test harness to measure the observed order of accuracy under grid refinement for finite difference time-integration methods applied to an ordinary differential equation motivated by cosmology. Start from the Friedmann equation for a spatially flat, matter-dominated Einstein–de Sitter universe, which yields a scale factor $a(t)$ obeying $H(a) = H_0 a^{-3/2}$ and, in cosmic time $t$, $da/dt = a H(a)$. Introduce the nondimensional time $\\tau = H_0 t$ so that the evolution equation reduces to\n$$\n\\frac{da}{d\\tau} = a^{-1/2}.\n$$\nThis equation has the exact solution\n$$\na(\\tau) = \\left(\\frac{3}{2}\\,\\tau\\right)^{2/3},\n$$\nwhich follows from separation of variables, with the big bang at $\\tau = 0$. To avoid the singularity at $\\tau = 0$, integrate from a strictly positive start time $\\tau_0  0$ with initial condition $a(\\tau_0)$ consistent with the exact solution.\n\nYour program must implement a finite difference time-stepping harness that, for each specified method and refinement level, integrates the nondimensional Friedmann ordinary differential equation from $\\tau_0$ to $\\tau_f$ and computes the global error at the final time. Use this to estimate the observed order by Richardson-style comparison across refinement levels. Specifically, given a sequence of refinements indexed by $k$ with characteristic steps $h_k$, construct errors $E_k = \\lvert a_k(\\tau_f) - a(\\tau_f)\\rvert$, where $a_k(\\tau_f)$ is the numerical solution at $\\tau_f$ at refinement level $k$. For uniform halving refinements, the observed order between two successive levels is\n$$\np_{\\mathrm{obs}} = \\log_2\\!\\left(\\frac{E_k}{E_{k+1}}\\right).\n$$\nWhen step-size constraints or adaptivity produce non-uniform changes of $h$ across the interval, apply the same formula to the sequence generated by halving a reference parameter, and interpret $p_{\\mathrm{obs}}$ as an empirical estimate that may deviate from the nominal order.\n\nImplement the following one-step methods:\n- An explicit midpoint method of nominal order $p=2$.\n- A classical Runge–Kutta method of nominal order $p=4$.\n\nFor constrained or adaptive stepping, model a cosmology-motivated step-size cap based on the Hubble time. In nondimensional variables, the Hubble time is $H(\\tau)^{-1} = a(\\tau)^{3/2}$. Impose a pointwise step-size constraint\n$$\nh(\\tau) \\le \\eta\\,a(\\tau)^{3/2},\n$$\nwith a fixed constant $\\eta  0$, while also attempting to halve a reference step $h_{\\mathrm{ref}}$ across refinement levels. Use $h(\\tau) = \\min\\!\\big(h_{\\mathrm{ref}},\\,\\eta\\,a(\\tau)^{3/2}\\big)$ inside the integrator, and force the final step to land exactly on $\\tau_f$ by shortening the last step if necessary.\n\nUse the following test suite of parameter values, which is designed to exercise a happy path, a higher-order case, and a constrained/adaptive scenario:\n- Cosmological interval and initial condition shared by all tests: $\\tau_0 = 0.1$, $\\tau_f = 1.0$, with $a(\\tau_0) = \\left(\\frac{3}{2}\\,\\tau_0\\right)^{2/3}$.\n- Test case $\\#1$ (happy path, second order): explicit midpoint method on a uniform grid with refinements $\\{N_0, 2N_0, 4N_0\\}$, where $N_0 = 50$ and $h_k = (\\tau_f - \\tau_0)/N_k$.\n- Test case $\\#2$ (happy path, fourth order): classical Runge–Kutta method on a uniform grid with refinements $\\{N_0, 2N_0, 4N_0\\}$, where $N_0 = 20$ and $h_k = (\\tau_f - \\tau_0)/N_k$.\n- Test case $\\#3$ (discrepancy under constraint): classical Runge–Kutta method with constrained step size $h(\\tau) = \\min\\!\\big(h_{\\mathrm{ref}},\\,\\eta\\,a(\\tau)^{3/2}\\big)$, at refinements $h_{\\mathrm{ref}} \\in \\{h_0, h_0/2, h_0/4\\}$ with $h_0 = 0.05$ and $\\eta = 0.1$.\n\nFor each test case, compute the observed order $p_{\\mathrm{obs}}$ between the last two refinement levels only (i.e., between $\\{2N_0, 4N_0\\}$ for uniform-grid tests or between $\\{h_0/2, h_0/4\\}$ for the constrained test). Your program should produce a single line of output containing the three observed orders as a comma-separated list enclosed in square brackets, for example, $\"[p_1,p_2,p_3]\"$. The values must be printed as floating-point numbers. All quantities are nondimensional, so no physical units are required, and angles are not involved. Percentages are not used; any ratio should be represented as a decimal number.", "solution": "The problem requires the construction of a numerical test harness to measure the observed order of accuracy for finite difference methods applied to a simplified cosmological ordinary differential equation (ODE). The problem is valid as it is scientifically grounded in the Friedmann equation, mathematically well-posed, and all specifications for its implementation are provided completely and unambiguously.\n\nThe physical basis is the Friedmann equation for a spatially flat, matter-dominated universe, where the Hubble parameter $H$ dependency on the scale factor $a$ is $H(a) = H_0 a^{-3/2}$. The evolution of the scale factor in cosmic time $t$ is given by the ODE $\\frac{da}{dt} = a H(a)$. By introducing a nondimensional time $\\tau = H_0 t$, this equation simplifies to:\n$$\n\\frac{da}{d\\tau} = a^{-1/2}\n$$\nThis ODE describes the expansion of the universe in this simplified model. The initial value is taken at a time $\\tau_0 = 0.1$ to avoid the \"big bang\" singularity at $\\tau = 0$. The equation can be solved analytically by separation of variables, yielding the exact solution for the scale factor:\n$$\na(\\tau) = \\left(\\frac{3}{2}\\,\\tau\\right)^{2/3}\n$$\nThe program will integrate the ODE from an initial time $\\tau_0 = 0.1$ to a final time $\\tau_f = 1.0$. The initial condition $a(\\tau_0)$ is set to be consistent with the exact solution, $a(\\tau_0) = (\\frac{3}{2}\\tau_0)^{2/3}$.\n\nThe core of the task is to solve this initial value problem (IVP) numerically using two one-step finite difference methods and to analyze their accuracy. A general one-step method approximates the solution $y(t)$ to an IVP $y'(t) = f(t, y(t))$ by producing a sequence of values $y_n \\approx y(t_n)$ at discrete time points $t_n = t_0 + n h$. The two methods specified are:\n\n$1$. **Explicit Midpoint Method**: This is a second-order Runge-Kutta method ($p=2$). The update rule from $a_n$ at time $\\tau_n$ to $a_{n+1}$ at time $\\tau_{n+1} = \\tau_n + h$ is:\n$$\n\\begin{align*}\nk_1 = f(\\tau_n, a_n) \\\\\nk_2 = f\\left(\\tau_n + \\frac{h}{2}, a_n + \\frac{h}{2} k_1\\right) \\\\\na_{n+1} = a_n + h k_2\n\\end{align*}\n$$\nwhere $f(\\tau, a) = a^{-1/2}$.\n\n$2$. **Classical Runge–Kutta Method (RK4)**: This is a fourth-order method ($p=4$). Its update rule is:\n$$\n\\begin{align*}\nk_1 = f(\\tau_n, a_n) \\\\\nk_2 = f\\left(\\tau_n + \\frac{h}{2}, a_n + \\frac{h}{2} k_1\\right) \\\\\nk_3 = f\\left(\\tau_n + \\frac{h}{2}, a_n + \\frac{h}{2} k_2\\right) \\\\\nk_4 = f\\left(\\tau_n + h, a_n + h k_3\\right) \\\\\na_{n+1} = a_n + \\frac{h}{6}(k_1 + 2k_2 + 2k_3 + k_4)\n\\end{align*}\n$$\n\nThe accuracy of these methods is quantified by the **global error** at the final time, $E = |a_{\\text{numerical}}(\\tau_f) - a_{\\text{exact}}(\\tau_f)|$. For a method of order $p$, the error is expected to scale as $E \\propto h^p$, where $h$ is a characteristic step size. By computing the error for a sequence of refined step sizes, we can estimate the method's **observed order of accuracy**, $p_{\\text{obs}}$. For two successive refinements with errors $E_k$ and $E_{k+1}$ where the step size is halved ($h_{k+1} = h_k/2$), the observed order is given by Richardson-style comparison:\n$$\np_{\\text{obs}} = \\log_2\\left(\\frac{E_k}{E_{k+1}}\\right)\n$$\n\nThe implementation will test three scenarios:\n- **Test Case $1$**: The explicit midpoint method with a uniform step size $h_k = (\\tau_f - \\tau_0) / N_k$ for a sequence of grid points $N_k = \\{50, 100, 200\\}$. The expected observed order is close to $p=2$.\n- **Test Case $2$**: The classical RK4 method with a uniform step size $h_k = (\\tau_f - \\tau_0) / N_k$ for a sequence of grid points $N_k = \\{20, 40, 80\\}$. The expected observed order is close to $p=4$.\n- **Test Case $3$**: The classical RK4 method with a variable, constrained step size. This models a common practice in cosmological simulations where the timestep is limited by the dynamical timescale of the system, here the Hubble time $H^{-1} = a^{3/2}$. The step size $h$ at each point $(\\tau, a)$ is determined by:\n$$\nh(\\tau) = \\min\\!\\big(h_{\\mathrm{ref}},\\,\\eta\\,a(\\tau)^{3/2}\\big)\n$$\nwhere $\\eta = 0.1$ is a safety factor. The refinement is achieved by halving a reference step size, $h_{\\mathrm{ref}} \\in \\{0.05, 0.025, 0.0125\\}$. Since the step size is not uniform and depends on the solution path, the observed order may deviate from the nominal order of $4$. The integrator logic ensures the final step lands exactly on $\\tau_f$ by shortening the last step if necessary.\n\nFor each of the three test cases, the program will compute the observed order using the errors from the last two refinement levels. The result will be a list of these three computed orders.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Constructs a test harness to measure the observed order of accuracy for\n    finite difference time-integration methods on a simplified Friedmann ODE.\n    \"\"\"\n    \n    # --- Problem Definition ---\n    # Global parameters, ODE, and its exact solution\n    TAU_0 = 0.1\n    TAU_F = 1.0\n\n    def ode_rhs(tau, a):\n        \"\"\" The right-hand side of the ODE: da/d(tau) = a**(-1/2) \"\"\"\n        return a**(-0.5)\n\n    def exact_solution(tau):\n        \"\"\" The exact analytical solution: a(tau) = (3/2 * tau)**(2/3) \"\"\"\n        return (3.0/2.0 * tau)**(2.0/3.0)\n\n    A_0 = exact_solution(TAU_0)\n    A_F_EXACT = exact_solution(TAU_F)\n\n    # --- Numerical Method Steppers ---\n    def step_midpoint(f, t, y, h):\n        \"\"\"\n        Performs a single step using the explicit midpoint method (p=2).\n        \"\"\"\n        k1 = f(t, y)\n        y_mid = y + 0.5 * h * k1\n        k2 = f(t + 0.5 * h, y_mid)\n        return y + h * k2\n        \n    def step_rk4(f, t, y, h):\n        \"\"\"\n        Performs a single step using the classical Runge-Kutta method (p=4).\n        \"\"\"\n        k1 = f(t, y)\n        k2 = f(t + 0.5 * h, y + 0.5 * h * k1)\n        k3 = f(t + 0.5 * h, y + 0.5 * h * k2)\n        k4 = f(t + h, y + h * k3)\n        return y + (h / 6.0) * (k1 + 2.0 * k2 + 2.0 * k3 + k4)\n\n    # --- Generic Integration Driver ---\n    def integrate(stepper, step_logic_func):\n        \"\"\"\n        Integrates the ODE from TAU_0 to TAU_F using a given stepper and\n        step-size logic. The final step is adjusted to land exactly on TAU_F.\n        \"\"\"\n        t = TAU_0\n        a = A_0\n        \n        while not np.isclose(t, TAU_F):\n            h = step_logic_func(t, a)\n            \n            # Ensure the final step lands exactly on the final time\n            if t + h > TAU_F:\n                h = TAU_F - t\n            \n            a = stepper(ode_rhs, t, a, h)\n            t += h\n            \n        return a\n\n    # --- Test Harness ---\n    results = []\n    \n    # Test Case #1: Explicit Midpoint, Uniform Grid\n    case1_errors = []\n    case1_N0 = 50\n    case1_refinements = [case1_N0, 2 * case1_N0, 4 * case1_N0]\n    for N in case1_refinements:\n        h_uniform = (TAU_F - TAU_0) / N\n        step_logic = lambda t, a: h_uniform\n        a_final = integrate(step_midpoint, step_logic)\n        case1_errors.append(abs(a_final - A_F_EXACT))\n    \n    p_obs1 = np.log2(case1_errors[1] / case1_errors[2])\n    results.append(p_obs1)\n\n    # Test Case #2: Classical RK4, Uniform Grid\n    case2_errors = []\n    case2_N0 = 20\n    case2_refinements = [case2_N0, 2 * case2_N0, 4 * case2_N0]\n    for N in case2_refinements:\n        h_uniform = (TAU_F - TAU_0) / N\n        step_logic = lambda t, a: h_uniform\n        a_final = integrate(step_rk4, step_logic)\n        case2_errors.append(abs(a_final - A_F_EXACT))\n        \n    p_obs2 = np.log2(case2_errors[1] / case2_errors[2])\n    results.append(p_obs2)\n\n    # Test Case #3: Classical RK4, Constrained Step Size\n    case3_errors = []\n    case3_h0 = 0.05\n    case3_eta = 0.1\n    case3_refinements = [case3_h0, case3_h0 / 2.0, case3_h0 / 4.0]\n    for h_ref in case3_refinements:\n        # The lambda captures h_ref and case3_eta from the enclosing scope\n        step_logic = lambda t, a: min(h_ref, case3_eta * a**1.5)\n        a_final = integrate(step_rk4, step_logic)\n        case3_errors.append(abs(a_final - A_F_EXACT))\n    \n    p_obs3 = np.log2(case3_errors[1] / case3_errors[2])\n    results.append(p_obs3)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3471890"}]}