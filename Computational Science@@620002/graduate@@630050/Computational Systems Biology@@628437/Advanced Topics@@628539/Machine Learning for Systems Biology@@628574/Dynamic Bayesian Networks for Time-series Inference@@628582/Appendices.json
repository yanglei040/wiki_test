{"hands_on_practices": [{"introduction": "The power of a Dynamic Bayesian Network (DBN) lies in its graphical structure, which encodes the conditional independence assumptions of a dynamic process. The concept of d-separation provides formal rules to read these assumptions directly from the graph, allowing us to reason about the flow of information over time. This practice sharpens your ability to determine how variables influence each other across different time points by analyzing the paths that connect them in the unrolled network. [@problem_id:3303906]", "problem": "Consider a time-unrolled Dynamic Bayesian Network (DBN) over discrete time indices $t \\in \\mathbb{Z}$ with variables $X_t$, $Z_t$, and $Y_t$. The DBN is defined by the following template of directed edges that is repeated across time:\n- $X_{t-1} \\to X_t$,\n- $Z_{t-1} \\to X_t$,\n- $X_t \\to Y_t$.\nAssume these are the only edges in the DBN; specifically, there are no other intra-slice or inter-slice edges (e.g., there are no edges of the form $Z_{t-1} \\to Z_t$, $Y_t \\to Y_{t+1}$, or $Z_t \\to Y_t$). All variables are represented as nodes in a Directed Acyclic Graph (DAG). The DBN obeys the local Markov property and conditional independence relations are characterized by d-separation in the unrolled DAG.\n\nStarting from the fundamental definitions:\n- In a DAG, conditional independence is determined by d-separation. A path of the form $A \\to B \\to C$ or $A \\leftarrow B \\to C$ is blocked by conditioning on $B$. A path containing a collider $A \\to B \\leftarrow C$ is naturally blocked unless $B$ or a descendant of $B$ is conditioned on, in which case the collider activates that path.\n- In a DBN, the two-slice temporal structure and time-unrolling allow one to reason about active and blocked paths across time indices.\n\nWe focus on the time window from $t-2$ through $t+1$. Determine which of the following conditioning sets are minimal (in cardinality and by set inclusion) and render $Z_{t-2}$ independent of $Y_{t+1}$ in the sense of d-separation in the unrolled DAG. Select all that apply.\n\nA. $\\{X_{t-1}\\}$\n\nB. $\\{X_t\\}$\n\nC. $\\{X_{t+1}\\}$\n\nD. $\\{Z_{t-1}\\}$\n\nE. $\\{Y_t\\}$\n\nF. $\\{X_{t-1}, Z_{t-1}\\}$\n\nA conditioning set $S$ is deemed minimal if $Z_{t-2} \\perp Y_{t+1} \\mid S$ holds and no proper subset of $S$ has the same property.", "solution": "### Step 1: Extract Givens\n\nThe problem defines a Dynamic Bayesian Network (DBN) with the following structure:\n- Variables: $X_t$, $Y_t$, $Z_t$ for discrete time indices $t \\in \\mathbb{Z}$.\n- Directed edges (template repeated for all $t$):\n    1.  $X_{t-1} \\to X_t$\n    2.  $Z_{t-1} \\to X_t$\n    3.  $X_t \\to Y_t$\n- Constraint: These are the only edges in the DBN.\n- Task: Determine which of the provided conditioning sets are minimal and render $Z_{t-2}$ independent of $Y_{t+1}$, denoted as $Z_{t-2} \\perp Y_{t+1} \\mid S$.\n- Minimality criterion: A set $S$ is minimal if $Z_{t-2} \\perp Y_{t+1} \\mid S$ holds, and for any proper subset $S' \\subset S$, the independence $Z_{t-2} \\perp Y_{t+1} \\mid S'$ does not hold.\n- Conditional independence is determined by d-separation in the unrolled graph.\n\n### Step 2: Validate Using Extracted Givens\n\nThe problem statement is a standard exercise in the theory of graphical models, specifically Dynamic Bayesian Networks. It provides a clear and unambiguous definition of the network structure. The concept of d-separation is a fundamental tool for determining conditional independence in Directed Acyclic Graphs (DAGs), and its application to unrolled DBNs is a standard procedure. The definition of a minimal separating set is also standard and well-defined. The problem is self-contained, logically consistent, and scientifically grounded within the field of probabilistic graphical models. It is well-posed and has a unique solution derivable from the provided definitions. Thus, the problem is valid.\n\n### Step 3: Derivation of the Solution\n\nTo determine the conditional independence relationship $Z_{t-2} \\perp Y_{t+1} \\mid S$, we must first unroll the DBN and identify all paths between the nodes $Z_{t-2}$ and $Y_{t+1}$ in the resulting DAG. The relevant time slices are from $t-2$ to $t+1$.\n\nThe structure of the unrolled graph is as follows:\n- **At time $t-1$**: $X_{t-2} \\to X_{t-1}$ and $Z_{t-2} \\to X_{t-1}$.\n- **At time $t$**: $X_{t-1} \\to X_t$ and $Z_{t-1} \\to X_t$.\n- **At time $t+1$**: $X_t \\to X_{t+1}$ and $Z_t \\to X_{t+1}$.\n- **Intra-slice edges**: $X_{t-1} \\to Y_{t-1}$, $X_t \\to Y_t$, $X_{t+1} \\to Y_{t+1}$.\n\nWe search for all simple paths (paths that do not repeat nodes) between $Z_{t-2}$ and $Y_{t+1}$.\n1.  Any path starting from $Z_{t-2}$ must first traverse the edge $Z_{t-2} \\to X_{t-1}$, as $X_{t-1}$ is the only node adjacent to $Z_{t-2}$.\n2.  Any path ending at $Y_{t+1}$ must arrive from the edge $X_{t+1} \\to Y_{t+1}$, as $X_{t+1}$ is the only node adjacent to $Y_{t+1}$.\n3.  Therefore, any path from $Z_{t-2}$ to $Y_{t+1}$ must contain a subpath from $X_{t-1}$ to $X_{t+1}$.\n\nLet's find all simple paths from $X_{t-1}$ to $X_{t+1}$. The graph structure shows a \"backbone\" of $X$ variables connected through time: $... \\to X_{t-2} \\to X_{t-1} \\to X_t \\to X_{t+1} \\to ...$.\n- A direct path exists: $X_{t-1} \\to X_t \\to X_{t+1}$.\n- Can we find other paths? Any other path would have to deviate from this backbone. For instance, from $X_{t-1}$, one could move to $X_t$, and from $X_t$ to its other parent, $Z_{t-1}$. This gives the path segment $X_{t-1} \\to X_t \\leftarrow Z_{t-1}$. However, based on the problem statement, there is no edge leaving $Z_{t-1}$ (like $Z_{t-1} \\to Z_t$ or $Z_{t-1} \\to X_{t-1}$) that could continue a path towards $X_{t+1}$. The only edge involving $Z_{t-1}$ is $Z_{t-1} \\to X_t$. Thus, this leads to a dead end for a simple path.\n- Similarly, a path going \"backwards\" in time, e.g., $X_{t-1} \\leftarrow X_{t-2}$, cannot reach $X_{t+1}$ without eventually traversing forward through $X_{t-1}$ again, which would not constitute a simple path.\n\nA rigorous check confirms that there is only one simple path connecting $X_{t-1}$ and $X_{t+1}$, which is the direct chain $X_{t-1} \\to X_t \\to X_{t+1}$.\n\nCombining these segments, we find there is exactly one simple path from $Z_{t-2}$ to $Y_{t+1}$:\n$$ P: Z_{t-2} \\to X_{t-1} \\to X_t \\to X_{t+1} \\to Y_{t+1} $$\n\nNow, we analyze this path for d-separation.\n- The path $P$ is a serial connection (a chain). It has no colliders ($A \\to B \\leftarrow C$).\n- According to the rules of d-separation, a serial path is active if none of its intermediate nodes are in the conditioning set $S$. The path is blocked if at least one intermediate node is in $S$.\n- The intermediate nodes on path $P$ are $X_{t-1}$, $X_t$, and $X_{t+1}$.\n\nFor $Z_{t-2}$ and $Y_{t+1}$ to be d-separated (conditionally independent), the conditioning set $S$ must block path $P$. This means $S$ must contain at least one of these intermediate nodes. The possible minimal sets are therefore the singleton sets consisting of each of these nodes.\n- $S_1 = \\{X_{t-1}\\}$\n- $S_2 = \\{X_t\\}$\n- $S_3 = \\{X_{t+1}\\}$\n\nThese sets are minimal because:\n1.  They each achieve d-separation.\n2.  The only proper subset of a singleton set is the empty set $\\emptyset$. For $S=\\emptyset$, the path $P$ is not blocked, so $Z_{t-2}$ is not independent of $Y_{t+1}$. Thus, no proper subset provides independence.\n\n### Step 4: Option-by-Option Analysis\n\nWe now evaluate each option against this conclusion.\n\n**A. $\\{X_{t-1}\\}$**\n- This set contains an intermediate node, $X_{t-1}$, of the only path between $Z_{t-2}$ and $Y_{t+1}$.\n- Conditioning on $\\{X_{t-1}\\}$ blocks the path, rendering $Z_{t-2}$ and $Y_{t+1}$ independent.\n- As a singleton set, it is minimal.\n- **Verdict: Correct**\n\n**B. $\\{X_t\\}$**\n- This set contains an intermediate node, $X_t$, of the only path.\n- Conditioning on $\\{X_t\\}$ blocks the path, rendering $Z_{t-2}$ and $Y_{t+1}$ independent.\n- As a singleton set, it is minimal.\n- **Verdict: Correct**\n\n**C. $\\{X_{t+1}\\}$**\n- This set contains an intermediate node, $X_{t+1}$, of the only path.\n- Conditioning on $\\{X_{t+1}\\}$ blocks the path, rendering $Z_{t-2}$ and $Y_{t+1}$ independent.\n- As a singleton set, it is minimal.\n- **Verdict: Correct**\n\n**D. $\\{Z_{t-1}\\}$**\n- The node $Z_{t-1}$ is not on the path $Z_{t-2} \\to X_{t-1} \\to X_t \\to X_{t+1} \\to Y_{t+1}$.\n- Conditioning on a node not on a serial path does not block it. There are no other paths that could be opened by conditioning on $Z_{t-1}$ (since there are no paths containing colliders between the start and end nodes).\n- Therefore, conditioning on $\\{Z_{t-1}\\}$ does not render $Z_{t-2}$ and $Y_{t+1}$ independent.\n- **Verdict: Incorrect**\n\n**E. $\\{Y_t\\}$**\n- The node $Y_t$ is not on the path $Z_{t-2} \\to X_{t-1} \\to X_t \\to X_{t+1} \\to Y_{t+1}$.\n- Conditioning on $Y_t$ does not block this serial path.\n- $Y_t$ is a descendant of $X_t$, which is a potential collider in other contexts (e.g., on the path $X_{t-1} \\to X_t \\leftarrow Z_{t-1}$). However, there is no path from $Z_{t-2}$ to $Y_{t+1}$ that contains a collider, so conditioning on $\\{Y_t\\}$ does not open any new paths between them.\n- Therefore, conditioning on $\\{Y_t\\}$ does not render $Z_{t-2}$ and $Y_{t+1}$ independent.\n- **Verdict: Incorrect**\n\n**F. $\\{X_{t-1}, Z_{t-1}\\}$**\n- This set contains $X_{t-1}$, which is an intermediate node on the path. Conditioning on this set therefore blocks the path and renders $Z_{t-2}$ and $Y_{t+1}$ independent.\n- However, we must check for minimality. The problem defines a minimal set $S$ as one for which no proper subset $S' \\subset S$ also provides independence.\n- Consider the proper subset $S' = \\{X_{t-1}\\}$. As established in the analysis of option A, $S'$ does render $Z_{t-2}$ and $Y_{t+1}$ independent.\n- Since a proper subset has the desired property, the set $\\{X_{t-1}, Z_{t-1}\\}$ is not minimal.\n- **Verdict: Incorrect**", "answer": "$$\\boxed{ABC}$$", "id": "3303906"}, {"introduction": "After defining a DBN's structure and parameters, a fundamental task is to evaluate how well it explains an observed time series. The log-likelihood is the central metric for this purpose, quantifying the joint probability of the data under the model's assumptions. This exercise provides hands-on practice in applying the probabilistic factorization of a linear-Gaussian DBN to compute this crucial value, a core skill required for parameter estimation and model comparison. [@problem_id:3303919]", "problem": "A gene regulatory module is modeled as a Dynamic Bayesian Network (DBN), where the latent transcriptional activity $X_t$ drives observed gene expression $Y_t$ over discrete time points $t = 1, \\dots, T$. The DBN follows a two-slice template with a first-order Markov transition for $X_t$ and a conditional emission for $Y_t$ given $X_t$, each specified by linear-Gaussian families. Specifically, the biological assumptions are:\n- The latent dynamics are auto-regressive (AR) of order one: $X_t \\mid X_{t-1} \\sim \\mathcal{N}(\\alpha X_{t-1}, \\sigma_x^2)$.\n- The observation model is linear with additive noise: $Y_t \\mid X_t \\sim \\mathcal{N}(\\gamma X_t, \\sigma_y^2)$.\n\nAssume the natural logarithm is used throughout and express the final numerical answer in nats. The model parameters are $T = 4$, $\\alpha = 0.5$, $\\gamma = 1.0$, $\\sigma_x^2 = 0.5$, and $\\sigma_y^2 = 0.5$. A complete-data time series (both latent and observed variables are treated as given for this calculation) is provided:\n- Latent states: $(X_1, X_2, X_3, X_4) = (2, 1.1, 0.5, 0.1)$.\n- Observations: $(Y_1, Y_2, Y_3, Y_4) = (1.8, 1.05, 0.6, 0.05)$.\n\nUsing the standard probabilistic factorization implied by the DBN two-slice template and the definitions of the Gaussian family, compute the complete-data log-likelihood of the sequence under this model. Round your final numerical answer to five significant figures and express it in nats.", "solution": "The problem is assessed to be valid. It is scientifically grounded in the established theory of Dynamic Bayesian Networks (DBNs) and linear-Gaussian state-space models, which are standard tools in computational systems biology. The problem is well-posed, providing all necessary parameters, model definitions, and a complete data sequence to compute a unique, meaningful log-likelihood value. The terminology is precise and objective.\n\nThe complete-data log-likelihood for the sequence of latent states $X_{1:T} = (X_1, \\dots, X_T)$ and observations $Y_{1:T} = (Y_1, \\dots, Y_T)$ is the natural logarithm of the joint probability density $P(X_{1:T}, Y_{1:T})$. Given the conditional dependencies defined by the DBN's two-slice template, this joint probability factorizes as:\n$$P(X_{1:T}, Y_{1:T}) = P(X_1) \\left( \\prod_{t=2}^{T} P(X_t \\mid X_{t-1}) \\right) \\left( \\prod_{t=1}^{T} P(Y_t \\mid X_t) \\right)$$\nThe model specification provides the transition probabilities $P(X_t \\mid X_{t-1})$ and the emission probabilities $P(Y_t \\mid X_t)$, but not the initial state distribution $P(X_1)$. However, the problem provides a complete data trace, including the value of $X_1$. In this context, it is standard practice to compute the likelihood of the rest of the sequence conditional on the initial state $X_1$. The quantity to be computed is therefore the conditional log-likelihood $\\mathcal{L} = \\ln P(X_{2:T}, Y_{1:T} \\mid X_1)$.\n\nThe complete-data log-likelihood, using the natural logarithm as specified (units of nats), is given by:\n$$\\mathcal{L} = \\ln \\left[ \\left( \\prod_{t=2}^{T} P(X_t \\mid X_{t-1}) \\right) \\left( \\prod_{t=1}^{T} P(Y_t \\mid X_t) \\right) \\right] = \\sum_{t=2}^{T} \\ln P(X_t \\mid X_{t-1}) + \\sum_{t=1}^{T} \\ln P(Y_t \\mid X_t)$$\nThis expression can be decomposed into two components: the transition log-likelihood, $\\mathcal{L}_X$, and the emission log-likelihood, $\\mathcal{L}_Y$.\n$$\\mathcal{L} = \\mathcal{L}_X + \\mathcal{L}_Y$$\nThe probability density function (PDF) for a Gaussian distribution $\\mathcal{N}(z \\mid \\mu, \\sigma^2)$ is $p(z) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(z-\\mu)^2}{2\\sigma^2}\\right)$. The corresponding log-PDF is:\n$$\\ln p(z) = -\\frac{1}{2}\\ln(2\\pi\\sigma^2) - \\frac{(z-\\mu)^2}{2\\sigma^2}$$\n\nFirst, we compute the transition log-likelihood component, $\\mathcal{L}_X$.\nThe transition model is $X_t \\mid X_{t-1} \\sim \\mathcal{N}(\\alpha X_{t-1}, \\sigma_x^2)$. The given parameters are $\\alpha = 0.5$ and $\\sigma_x^2 = 0.5$. The time series has length $T=4$.\n$$\\mathcal{L}_X = \\sum_{t=2}^{4} \\left[ -\\frac{1}{2}\\ln(2\\pi\\sigma_x^2) - \\frac{(X_t - \\alpha X_{t-1})^2}{2\\sigma_x^2} \\right]$$\nSubstituting $\\sigma_x^2 = 0.5$, the expression for each term simplifies to:\n$$\\ln P(X_t \\mid X_{t-1}) = -\\frac{1}{2}\\ln(2\\pi \\cdot 0.5) - \\frac{(X_t - 0.5 X_{t-1})^2}{2 \\cdot 0.5} = -\\frac{1}{2}\\ln(\\pi) - (X_t - 0.5 X_{t-1})^2$$\nUsing the provided data $(X_1, X_2, X_3, X_4) = (2, 1.1, 0.5, 0.1)$:\n- For $t=2$: $\\ln P(X_2 \\mid X_1) = -\\frac{1}{2}\\ln(\\pi) - (1.1 - 0.5 \\cdot 2)^2 = -\\frac{1}{2}\\ln(\\pi) - (1.1 - 1)^2 = -\\frac{1}{2}\\ln(\\pi) - 0.01$.\n- For $t=3$: $\\ln P(X_3 \\mid X_2) = -\\frac{1}{2}\\ln(\\pi) - (0.5 - 0.5 \\cdot 1.1)^2 = -\\frac{1}{2}\\ln(\\pi) - (0.5 - 0.55)^2 = -\\frac{1}{2}\\ln(\\pi) - 0.0025$.\n- For $t=4$: $\\ln P(X_4 \\mid X_3) = -\\frac{1}{2}\\ln(\\pi) - (0.1 - 0.5 \\cdot 0.5)^2 = -\\frac{1}{2}\\ln(\\pi) - (0.1 - 0.25)^2 = -\\frac{1}{2}\\ln(\\pi) - 0.0225$.\nSumming these terms gives $\\mathcal{L}_X$:\n$$\\mathcal{L}_X = 3 \\left(-\\frac{1}{2}\\ln(\\pi)\\right) - (0.01 + 0.0025 + 0.0225) = -\\frac{3}{2}\\ln(\\pi) - 0.035$$\n\nNext, we compute the emission log-likelihood component, $\\mathcal{L}_Y$.\nThe emission model is $Y_t \\mid X_t \\sim \\mathcal{N}(\\gamma X_t, \\sigma_y^2)$. The given parameters are $\\gamma = 1.0$ and $\\sigma_y^2 = 0.5$.\n$$\\mathcal{L}_Y = \\sum_{t=1}^{4} \\left[ -\\frac{1}{2}\\ln(2\\pi\\sigma_y^2) - \\frac{(Y_t - \\gamma X_t)^2}{2\\sigma_y^2} \\right]$$\nSubstituting $\\sigma_y^2=0.5$ and $\\gamma=1.0$, the expression for each term is:\n$$\\ln P(Y_t \\mid X_t) = -\\frac{1}{2}\\ln(2\\pi \\cdot 0.5) - \\frac{(Y_t - 1.0 \\cdot X_t)^2}{2 \\cdot 0.5} = -\\frac{1}{2}\\ln(\\pi) - (Y_t - X_t)^2$$\nUsing the provided data $(X_1, X_2, X_3, X_4) = (2, 1.1, 0.5, 0.1)$ and $(Y_1, Y_2, Y_3, Y_4) = (1.8, 1.05, 0.6, 0.05)$:\n- For $t=1$: $\\ln P(Y_1 \\mid X_1) = -\\frac{1}{2}\\ln(\\pi) - (1.8 - 2)^2 = -\\frac{1}{2}\\ln(\\pi) - (-0.2)^2 = -\\frac{1}{2}\\ln(\\pi) - 0.04$.\n- For $t=2$: $\\ln P(Y_2 \\mid X_2) = -\\frac{1}{2}\\ln(\\pi) - (1.05 - 1.1)^2 = -\\frac{1}{2}\\ln(\\pi) - (-0.05)^2 = -\\frac{1}{2}\\ln(\\pi) - 0.0025$.\n- For $t=3$: $\\ln P(Y_3 \\mid X_3) = -\\frac{1}{2}\\ln(\\pi) - (0.6 - 0.5)^2 = -\\frac{1}{2}\\ln(\\pi) - (0.1)^2 = -\\frac{1}{2}\\ln(\\pi) - 0.01$.\n- For $t=4$: $\\ln P(Y_4 \\mid X_4) = -\\frac{1}{2}\\ln(\\pi) - (0.05 - 0.1)^2 = -\\frac{1}{2}\\ln(\\pi) - (-0.05)^2 = -\\frac{1}{2}\\ln(\\pi) - 0.0025$.\nSumming these terms gives $\\mathcal{L}_Y$:\n$$\\mathcal{L}_Y = 4 \\left(-\\frac{1}{2}\\ln(\\pi)\\right) - (0.04 + 0.0025 + 0.01 + 0.0025) = -2\\ln(\\pi) - 0.055$$\n\nFinally, the total complete-data log-likelihood is the sum of the two components:\n$$\\mathcal{L} = \\mathcal{L}_X + \\mathcal{L}_Y = \\left(-\\frac{3}{2}\\ln(\\pi) - 0.035\\right) + \\left(-2\\ln(\\pi) - 0.055\\right)$$\n$$\\mathcal{L} = -\\frac{7}{2}\\ln(\\pi) - 0.09$$\nTo obtain the numerical value, we use $\\ln(\\pi) \\approx 1.1447298858$:\n$$\\mathcal{L} \\approx -3.5 \\times 1.1447298858 - 0.09 \\approx -4.0065546 - 0.09 = -4.0965546$$\nRounding the result to five significant figures, we get $-4.0966$.", "answer": "$$\\boxed{-4.0966}$$", "id": "3303919"}, {"introduction": "In many real-world applications, such as reconstructing gene regulatory networks, the DBN structure is unknown and must be learned from data. A key biological insight is that these networks are typically sparse, a feature that can be enforced using $\\ell_1$ regularization. This exercise moves from theory to practice by guiding you through a single update step of a proximal gradient algorithm, the core computational engine for learning sparse network structures from time-series data. [@problem_id:3303898]", "problem": "You are tasked with implementing a proximal update for selected entries of a cross-time adjacency matrix in a Dynamic Bayesian Network (DBN) for gene regulatory time-series modeling. The DBN represents directed dependencies from time $t$ to time $t+1$ among $n$ genes, with an adjacency matrix $A \\in \\mathbb{R}^{n \\times n}$ where entry $A_{ij}$ encodes the directed influence strength from gene $j$ at time $t$ to gene $i$ at time $t+1$. Sparse structure in $A$ corresponds to few regulatory interactions, which is biologically plausible. The inference proceeds by Maximum A Posteriori (MAP) estimation under an $\\ell_1$-regularized objective, encouraging sparsity in $A$. A proximal gradient step refines $A$ using a gradient matrix $G \\in \\mathbb{R}^{n \\times n}$ derived from the time-series likelihood and a regularization parameter controlling sparsity. Your program must compute the proximal update for a specified subset of entries of $A$, and then interpret the resulting sparsity for those entries in terms of potential gene interactions.\n\nBegin from the following foundational bases:\n- The definition of a Dynamic Bayesian Network (DBN) as a probabilistic graphical model encoding conditional dependencies between gene expression variables across successive time points.\n- The Maximum A Posteriori (MAP) estimation principle that balances data fidelity (likelihood) and structural parsimony via an $\\ell_1$ penalty to encourage sparsity.\n- The proximal operator concept for $\\ell_1$ regularization, applied to a gradient-based iterate to produce a sparse update.\n\nYou must not use or assume any particular closed-form formulas provided here; instead, implement the correct proximal update following these foundational bases.\n\nYour program must handle the following test suite, each with explicit parameters. Indices are zero-based, and each pair $(i,j)$ denotes the update for entry $A_{ij}$, corresponding to the directed influence from gene $j$ to gene $i$ across time. All quantities are dimensionless.\n\nTest Case $1$:\n- Size $n = 3$.\n- Initial adjacency\n$$\nA^{(1)} =\n\\begin{bmatrix}\n0 & 0.3 & -0.1 \\\\\n0.2 & 0 & 0.05 \\\\\n-0.25 & 0.1 & 0\n\\end{bmatrix}.\n$$\n- Gradient\n$$\nG^{(1)} =\n\\begin{bmatrix}\n0 & 0.5 & -0.2 \\\\\n-0.3 & 0 & 0.1 \\\\\n0.4 & -0.05 & 0.2\n\\end{bmatrix}.\n$$\n- Step size $\\,\\alpha^{(1)} = 0.5$.\n- Regularization parameter $\\,\\lambda^{(1)} = 0.2$.\n- Selected indices $\\,S^{(1)} = \\{(0,1),(1,0),(2,2),(2,1)\\}$.\n\nTest Case $2$:\n- Size $n = 4$.\n- Initial adjacency\n$$\nA^{(2)} =\n\\begin{bmatrix}\n0 & 0.4 & 0 & -0.1 \\\\\n0.05 & 0 & 0.2 & 0 \\\\\n-0.15 & 0.1 & 0 & 0.8 \\\\\n0 & -0.05 & 0.25 & 0\n\\end{bmatrix}.\n$$\n- Gradient\n$$\nG^{(2)} =\n\\begin{bmatrix}\n0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0\n\\end{bmatrix}.\n$$\n- Step size $\\,\\alpha^{(2)} = 1.0$.\n- Regularization parameter $\\,\\lambda^{(2)} = 0.5$.\n- Selected indices $\\,S^{(2)} = \\{(0,1),(2,3),(3,2),(1,2),(0,3)\\}$.\n\nTest Case $3$:\n- Size $n = 2$.\n- Initial adjacency\n$$\nA^{(3)} =\n\\begin{bmatrix}\n0 & 0.15 \\\\\n-0.05 & 0\n\\end{bmatrix}.\n$$\n- Gradient\n$$\nG^{(3)} =\n\\begin{bmatrix}\n0 & 0.2 \\\\\n0.2 & 0\n\\end{bmatrix}.\n$$\n- Step size $\\,\\alpha^{(3)} = 0.25$.\n- Regularization parameter $\\,\\lambda^{(3)} = 0.4$.\n- Selected indices $\\,S^{(3)} = \\{(0,1),(1,0)\\}$.\n\nFor each test case, your program must:\n1. Perform a single proximal gradient update for the selected entries in $S^{(k)}$, using the given $A^{(k)}$, $G^{(k)}$, $\\alpha^{(k)}$, and $\\lambda^{(k)}$. Only entries in the selected index set should be updated; all other entries must remain unchanged.\n2. Extract the updated values of the selected entries in the exact order they are listed in $S^{(k)}$, and return them as a list of floats.\n3. Interpret sparsity for those selected entries by producing a list of booleans indicating whether each updated entry is nonzero (a nonzero entry indicates a candidate directed interaction from gene $j$ to gene $i$ across time).\n4. Return the count of zeros among the updated selected entries as an integer.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case result must be a list of three elements:\n- The list of updated selected values (floats) in the order of $S^{(k)}$.\n- The list of booleans indicating nonzero (interaction present) or zero (no interaction) for those entries.\n- The integer count of zeros among those entries.\n\nThe final output format must be exactly:\n`[result_case_1,result_case_2,result_case_3]`,\nwhere each result_case_k follows the structure described above. Angles are not involved, and there are no physical units; all reported values are dimensionless. The program must use zero-based indexing for matrix positions.", "solution": "The problem requires the implementation of a single proximal gradient update step for inferring a sparse cross-time adjacency matrix, $A$, in a Dynamic Bayesian Network (DBN) model of gene regulation. The context is computational systems biology, where such models are used to deduce regulatory interactions from time-series gene expression data. The matrix entry $A_{ij}$ represents the directed influence from gene $j$ at time $t$ to gene $i$ at time $t+1$.\n\nThe inference of the matrix $A$ is formulated as a Maximum A Posteriori (MAP) estimation problem. Biological networks are typically sparse, meaning a gene is regulated by only a small number of other genes. To enforce this biologically plausible structure, the objective function includes an $\\ell_1$-regularization term. The objective function, $F(A)$, to be minimized is of the form:\n$$F(A) = L(A) + \\lambda \\sum_{i,j} |A_{ij}|$$\nHere, $L(A)$ is the negative log-likelihood of the data given the network structure $A$, which is a differentiable function measuring the model's fit to the observed time-series data. The second term is the $\\ell_1$-norm of the matrix $A$, weighted by a regularization parameter $\\lambda > 0$. The $\\ell_1$-norm is the sum of the absolute values of the matrix entries and is known to induce sparsity (i.e., drive many entries to exactly zero).\n\nThis objective function is a composite of a smooth, differentiable term $L(A)$ and a non-smooth, non-differentiable but convex term $\\lambda \\|A\\|_1$. Proximal gradient methods are ideally suited for such optimization problems. A single update step, starting from the current estimate $A^{(k)}$, is given by:\n$$A^{(k+1)} = \\text{prox}_{\\alpha \\lambda \\|\\cdot\\|_1}(A^{(k)} - \\alpha \\nabla_A L(A^{(k)}))$$\nwhere $\\alpha$ is the step size (learning rate), $\\nabla_A L(A^{(k)})$ is the gradient of the likelihood term (provided as the matrix $G^{(k)}$), and $\\text{prox}_{\\gamma f}(\\cdot)$ is the proximal operator for a function $f$ with parameter $\\gamma$.\n\nThe update can be conceptually broken down into two stages:\n1.  **Gradient Step**: We first take a standard gradient descent step on the smooth part of the objective, $L(A)$. This produces an intermediate matrix, let's call it $Z$:\n    $$Z = A^{(k)} - \\alpha G^{(k)}$$\n    where $G^{(k)} = \\nabla_A L(A^{(k)})$.\n\n2.  **Proximal Mapping Step**: We then apply the proximal operator associated with the $\\ell_1$-norm to the intermediate matrix $Z$. The proximal operator for the scaled $\\ell_1$-norm, $\\gamma \\|\\cdot\\|_1$, is the element-wise soft-thresholding operator, defined for a scalar value $z$ as:\n    $$S_\\gamma(z) = \\text{prox}_{\\gamma |\\cdot|}(z) = \\text{sign}(z) \\max(|z| - \\gamma, 0)$$\n    This operator shrinks the value of $z$ towards zero by an amount $\\gamma$, and sets it to exactly zero if its magnitude is less than or equal to $\\gamma$.\n\nCombining these, the update rule for a single entry $A_{ij}$ is:\n$$A_{ij}^{(k+1)} = S_{\\alpha^{(k)} \\lambda^{(k)}}(Z_{ij}) = S_{\\alpha^{(k)} \\lambda^{(k)}}(A_{ij}^{(k)} - \\alpha^{(k)} G_{ij}^{(k)})$$\nThe problem specifies that this update is applied only to a pre-selected set of indices $S^{(k)}$. Entries not in this set remain unchanged. An updated entry $A_{ij}^{(k+1)}$ being non-zero implies a persistent regulatory link from gene $j$ to gene $i$, whereas a value of zero implies the absence of such a link under the current model parameters.\n\nWe will now apply this procedure to each test case.\n\n**Test Case 1**\n- $A^{(1)} = \\begin{bmatrix} 0 & 0.3 & -0.1 \\\\ 0.2 & 0 & 0.05 \\\\ -0.25 & 0.1 & 0 \\end{bmatrix}$, $G^{(1)} = \\begin{bmatrix} 0 & 0.5 & -0.2 \\\\ -0.3 & 0 & 0.1 \\\\ 0.4 & -0.05 & 0.2 \\end{bmatrix}$\n- $\\alpha^{(1)} = 0.5$, $\\lambda^{(1)} = 0.2$\n- $S^{(1)} = \\{(0,1),(1,0),(2,2),(2,1)\\}$\n- The soft-thresholding parameter is $\\tau = \\alpha^{(1)} \\lambda^{(1)} = 0.5 \\times 0.2 = 0.1$.\n\nFor each $(i,j) \\in S^{(1)}$:\n- $(0,1)$: $Z_{01} = 0.3 - 0.5 \\times 0.5 = 0.05$.  Update: $\\text{sign}(0.05) \\max(|0.05| - 0.1, 0) = 0.0$.\n- $(1,0)$: $Z_{10} = 0.2 - 0.5 \\times (-0.3) = 0.35$. Update: $\\text{sign}(0.35) \\max(|0.35| - 0.1, 0) = 0.25$.\n- $(2,2)$: $Z_{22} = 0 - 0.5 \\times 0.2 = -0.1$. Update: $\\text{sign}(-0.1) \\max(|-0.1| - 0.1, 0) = 0.0$.\n- $(2,1)$: $Z_{21} = 0.1 - 0.5 \\times (-0.05) = 0.125$. Update: $\\text{sign}(0.125) \\max(|0.125| - 0.1, 0) = 0.025$.\n\nResults:\n- Updated values: $[0.0, 0.25, 0.0, 0.025]$\n- Non-zero status: $[False, True, False, True]$\n- Zero count: $2$\n\n**Test Case 2**\n- $A^{(2)}$ as given, $G^{(2)}$ is the zero matrix.\n- $\\alpha^{(2)} = 1.0$, $\\lambda^{(2)} = 0.5$\n- $S^{(2)} = \\{(0,1),(2,3),(3,2),(1,2),(0,3)\\}$\n- The soft-thresholding parameter is $\\tau = \\alpha^{(2)} \\lambda^{(2)} = 1.0 \\times 0.5 = 0.5$.\n- Since $G^{(2)}$ is zero, $Z_{ij} = A_{ij}^{(2)}$.\n\nFor each $(i,j) \\in S^{(2)}$:\n- $(0,1)$: $Z_{01} = 0.4$. Update: $\\text{sign}(0.4) \\max(|0.4| - 0.5, 0) = 0.0$.\n- $(2,3)$: $Z_{23} = 0.8$. Update: $\\text{sign}(0.8) \\max(|0.8| - 0.5, 0) = 0.3$.\n- $(3,2)$: $Z_{32} = 0.25$. Update: $\\text{sign}(0.25) \\max(|0.25| - 0.5, 0) = 0.0$.\n- $(1,2)$: $Z_{12} = 0.2$. Update: $\\text{sign}(0.2) \\max(|0.2| - 0.5, 0) = 0.0$.\n- $(0,3)$: $Z_{03} = -0.1$. Update: $\\text{sign}(-0.1) \\max(|-0.1| - 0.5, 0) = 0.0$.\n\nResults:\n- Updated values: $[0.0, 0.3, 0.0, 0.0, 0.0]$\n- Non-zero status: $[False, True, False, False, False]$\n- Zero count: $4$\n\n**Test Case 3**\n- $A^{(3)} = \\begin{bmatrix} 0 & 0.15 \\\\ -0.05 & 0 \\end{bmatrix}$, $G^{(3)} = \\begin{bmatrix} 0 & 0.2 \\\\ 0.2 & 0 \\end{bmatrix}$\n- $\\alpha^{(3)} = 0.25$, $\\lambda^{(3)} = 0.4$\n- $S^{(3)} = \\{(0,1),(1,0)\\}$\n- The soft-thresholding parameter is $\\tau = \\alpha^{(3)} \\lambda^{(3)} = 0.25 \\times 0.4 = 0.1$.\n\nFor each $(i,j) \\in S^{(3)}$:\n- $(0,1)$: $Z_{01} = 0.15 - 0.25 \\times 0.2 = 0.1$. Update: $\\text{sign}(0.1) \\max(|0.1| - 0.1, 0) = 0.0$.\n- $(1,0)$: $Z_{10} = -0.05 - 0.25 \\times 0.2 = -0.1$. Update: $\\text{sign}(-0.1) \\max(|-0.1| - 0.1, 0) = 0.0$.\n\nResults:\n- Updated values: $[0.0, 0.0]$\n- Non-zero status: $[False, False]$\n- Zero count: $2$", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes a single proximal gradient update for specified entries of a\n    cross-time adjacency matrix in a Dynamic Bayesian Network model.\n    \"\"\"\n    test_cases = [\n        {\n            \"A\": np.array([\n                [0.0, 0.3, -0.1],\n                [0.2, 0.0, 0.05],\n                [-0.25, 0.1, 0.0]\n            ]),\n            \"G\": np.array([\n                [0.0, 0.5, -0.2],\n                [-0.3, 0.0, 0.1],\n                [0.4, -0.05, 0.2]\n            ]),\n            \"alpha\": 0.5,\n            \"lambda\": 0.2,\n            \"indices\": [(0, 1), (1, 0), (2, 2), (2, 1)]\n        },\n        {\n            \"A\": np.array([\n                [0.0, 0.4, 0.0, -0.1],\n                [0.05, 0.0, 0.2, 0.0],\n                [-0.15, 0.1, 0.0, 0.8],\n                [0.0, -0.05, 0.25, 0.0]\n            ]),\n            \"G\": np.array([\n                [0.0, 0.0, 0.0, 0.0],\n                [0.0, 0.0, 0.0, 0.0],\n                [0.0, 0.0, 0.0, 0.0],\n                [0.0, 0.0, 0.0, 0.0]\n            ]),\n            \"alpha\": 1.0,\n            \"lambda\": 0.5,\n            \"indices\": [(0, 1), (2, 3), (3, 2), (1, 2), (0, 3)]\n        },\n        {\n            \"A\": np.array([\n                [0.0, 0.15],\n                [-0.05, 0.0]\n            ]),\n            \"G\": np.array([\n                [0.0, 0.2],\n                [0.2, 0.0]\n            ]),\n            \"alpha\": 0.25,\n            \"lambda\": 0.4,\n            \"indices\": [(0, 1), (1, 0)]\n        }\n    ]\n\n    results = []\n    \n    def soft_thresholding(z, tau):\n        \"\"\"\n        Applies the element-wise soft-thresholding operator.\n        S_tau(z) = sign(z) * max(|z| - tau, 0)\n        \"\"\"\n        return np.sign(z) * np.maximum(np.abs(z) - tau, 0.0)\n\n    for case in test_cases:\n        A = case[\"A\"]\n        G = case[\"G\"]\n        alpha = case[\"alpha\"]\n        lambda_val = case[\"lambda\"]\n        indices = case[\"indices\"]\n\n        # Calculate the threshold for the soft-thresholding operator\n        tau = alpha * lambda_val\n\n        updated_values = []\n        is_nonzero_list = []\n        zero_count = 0\n\n        for i, j in indices:\n            # 1. Gradient step to get the intermediate value\n            z_ij = A[i, j] - alpha * G[i, j]\n            \n            # 2. Proximal mapping (soft-thresholding) step\n            updated_val = soft_thresholding(z_ij, tau)\n\n            updated_values.append(float(updated_val))\n\n            # 3. Interpret sparsity and count zeros\n            is_nonzero = (updated_val != 0.0)\n            is_nonzero_list.append(is_nonzero)\n            if not is_nonzero:\n                zero_count += 1\n        \n        case_result = [updated_values, is_nonzero_list, zero_count]\n        results.append(case_result)\n\n    # Format the final output string exactly as required\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3303898"}]}