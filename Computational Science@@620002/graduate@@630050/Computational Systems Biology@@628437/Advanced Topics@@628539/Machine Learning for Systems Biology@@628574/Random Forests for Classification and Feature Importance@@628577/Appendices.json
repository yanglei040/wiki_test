{"hands_on_practices": [{"introduction": "The power of a decision tree lies in its hierarchical process of partitioning data. At each step, the algorithm must answer a critical question: what is the single best split to make? This exercise walks you through the mechanics of this decision by using the Gini impurity decrease, a standard criterion for evaluating split quality. By calculating the change in Gini impurity for several candidate splits on a gene expression feature, you will gain a concrete understanding of how a tree learns to separate different classes [@problem_id:3342859].", "problem": "A consortium study in computational systems biology seeks to predict whether a cancer cell line is drug-sensitive or drug-resistant based on a single normalized messenger ribonucleic acid (mRNA) expression feature of gene $X$. As a subroutine of training a Random Forests (RF) classifier, a binary split is evaluated on this continuous feature using the Gini impurity criterion. For a node with empirical class distribution $\\mathbf{p} = (p_{\\text{s}}, p_{\\text{r}})$ over the two classes (sensitive s, resistant r), Gini impurity is defined as $G(\\mathbf{p}) = 1 - \\sum_{k} p_{k}^{2}$. The impurity decrease for a split is defined as the parent-node impurity minus the expected post-split impurity, where the expectation is taken with respect to the empirical probabilities of a sample landing in each child node.\n\nYou are given a parent node of size $N = 120$ with class counts $(48\\ \\text{sensitive}, 72\\ \\text{resistant})$. Four candidate thresholds on the gene $X$ expression produce the following left/right child class counts, where each threshold $t$ sends samples with expression $\\leq t$ to the left child and the rest to the right child:\n\n- Threshold $t = 3.0$: left $(6, 18)$, right $(42, 54)$.\n- Threshold $t = 5.5$: left $(18, 30)$, right $(30, 42)$.\n- Threshold $t = 7.0$: left $(30, 40)$, right $(18, 32)$.\n- Threshold $t = 8.5$: left $(40, 55)$, right $(8, 17)$.\n\nStarting only from the definition of Gini impurity and the principle that the expected post-split impurity equals the child-node impurities weighted by their empirical sample proportions (that is, the law of total probability applied to node membership), compute the impurity decrease $\\Delta G$ for each candidate threshold and identify the best split (the one maximizing $\\Delta G$). Provide, as your final answer, the exact value of the maximum impurity decrease $\\Delta G_{\\max}$ expressed as a single rational number in simplest terms. Do not include units, and do not round.", "solution": "The problem requires the evaluation of four candidate splits of a dataset node in a decision tree, based on the Gini impurity decrease criterion. The goal is to identify the split that maximizes this decrease and to provide the exact value of this maximum decrease.\n\nThe Gini impurity for a node with empirical class proportions $\\mathbf{p} = (p_1, p_2, \\ldots, p_C)$ for $C$ classes is given by the formula:\n$$G(\\mathbf{p}) = 1 - \\sum_{k=1}^{C} p_{k}^{2}$$\nFor a binary split, the impurity decrease, denoted by $\\Delta G$, is the difference between the parent node's impurity and the weighted average of the two child nodes' impurities:\n$$\\Delta G = G_{\\text{parent}} - \\left( \\frac{N_{\\text{left}}}{N} G_{\\text{left}} + \\frac{N_{\\text{right}}}{N} G_{\\text{right}} \\right)$$\nwhere $N$ is the total number of samples in the parent node, and $N_{\\text{left}}$ and $N_{\\text{right}}$ are the number of samples in the left and right child nodes, respectively.\n\nFirst, we calculate the Gini impurity of the parent node, $G_{\\text{parent}}$.\nThe parent node has a total size of $N=120$ samples, with class counts of $N_s = 48$ for the 'sensitive' class and $N_r = 72$ for the 'resistant' class.\nThe class proportions are:\n$$p_s = \\frac{N_s}{N} = \\frac{48}{120} = \\frac{2}{5}$$\n$$p_r = \\frac{N_r}{N} = \\frac{72}{120} = \\frac{3}{5}$$\nThe Gini impurity of the parent node is:\n$$G_{\\text{parent}} = 1 - \\left( p_{s}^{2} + p_{r}^{2} \\right) = 1 - \\left( \\left(\\frac{2}{5}\\right)^2 + \\left(\\frac{3}{5}\\right)^2 \\right) = 1 - \\left( \\frac{4}{25} + \\frac{9}{25} \\right) = 1 - \\frac{13}{25} = \\frac{12}{25}$$\n\nNext, we evaluate each of the four candidate splits.\n\n**1. Threshold $t = 3.0$**\n- Left child counts: $(N_{s,L}, N_{r,L}) = (6, 18)$. Total size $N_L = 6+18=24$.\n- Right child counts: $(N_{s,R}, N_{r,R}) = (42, 54)$. Total size $N_R = 42+54=96$.\nThe proportions for the left child are $p_{s,L} = \\frac{6}{24} = \\frac{1}{4}$ and $p_{r,L} = \\frac{18}{24} = \\frac{3}{4}$.\nThe Gini impurity of the left child is:\n$$G_L = 1 - \\left( \\left(\\frac{1}{4}\\right)^2 + \\left(\\frac{3}{4}\\right)^2 \\right) = 1 - \\left( \\frac{1}{16} + \\frac{9}{16} \\right) = 1 - \\frac{10}{16} = \\frac{6}{16} = \\frac{3}{8}$$\nThe proportions for the right child are $p_{s,R} = \\frac{42}{96} = \\frac{7}{16}$ and $p_{r,R} = \\frac{54}{96} = \\frac{9}{16}$.\nThe Gini impurity of the right child is:\n$$G_R = 1 - \\left( \\left(\\frac{7}{16}\\right)^2 + \\left(\\frac{9}{16}\\right)^2 \\right) = 1 - \\left( \\frac{49}{256} + \\frac{81}{256} \\right) = 1 - \\frac{130}{256} = \\frac{126}{256} = \\frac{63}{128}$$\nThe impurity decrease for this split is:\n$$\\Delta G_1 = G_{\\text{parent}} - \\left(\\frac{N_L}{N}G_L + \\frac{N_R}{N}G_R\\right) = \\frac{12}{25} - \\left(\\frac{24}{120}\\left(\\frac{3}{8}\\right) + \\frac{96}{120}\\left(\\frac{63}{128}\\right)\\right)$$\n$$\\Delta G_1 = \\frac{12}{25} - \\left(\\frac{1}{5}\\left(\\frac{3}{8}\\right) + \\frac{4}{5}\\left(\\frac{63}{128}\\right)\\right) = \\frac{12}{25} - \\left(\\frac{3}{40} + \\frac{63}{160}\\right) = \\frac{12}{25} - \\left(\\frac{12}{160} + \\frac{63}{160}\\right) = \\frac{12}{25} - \\frac{75}{160} = \\frac{12}{25} - \\frac{15}{32}$$\n$$\\Delta G_1 = \\frac{12 \\times 32 - 15 \\times 25}{25 \\times 32} = \\frac{384 - 375}{800} = \\frac{9}{800}$$\n\n**2. Threshold $t = 5.5$**\n- Left child counts: $(18, 30)$. $N_L = 48$. Proportions: $p_{s,L} = \\frac{18}{48} = \\frac{3}{8}, p_{r,L} = \\frac{30}{48} = \\frac{5}{8}$.\n- Right child counts: $(30, 42)$. $N_R = 72$. Proportions: $p_{s,R} = \\frac{30}{72} = \\frac{5}{12}, p_{r,R} = \\frac{42}{72} = \\frac{7}{12}$.\n$$G_L = 1 - \\left(\\left(\\frac{3}{8}\\right)^2 + \\left(\\frac{5}{8}\\right)^2\\right) = 1 - \\left(\\frac{9}{64} + \\frac{25}{64}\\right) = 1 - \\frac{34}{64} = \\frac{30}{64} = \\frac{15}{32}$$\n$$G_R = 1 - \\left(\\left(\\frac{5}{12}\\right)^2 + \\left(\\frac{7}{12}\\right)^2\\right) = 1 - \\left(\\frac{25}{144} + \\frac{49}{144}\\right) = 1 - \\frac{74}{144} = \\frac{70}{144} = \\frac{35}{72}$$\nThe impurity decrease is:\n$$\\Delta G_2 = \\frac{12}{25} - \\left(\\frac{48}{120}\\left(\\frac{15}{32}\\right) + \\frac{72}{120}\\left(\\frac{35}{72}\\right)\\right) = \\frac{12}{25} - \\left(\\frac{2}{5}\\left(\\frac{15}{32}\\right) + \\frac{3}{5}\\left(\\frac{35}{72}\\right)\\right)$$\n$$\\Delta G_2 = \\frac{12}{25} - \\left(\\frac{3}{16} + \\frac{7}{24}\\right) = \\frac{12}{25} - \\left(\\frac{9}{48} + \\frac{14}{48}\\right) = \\frac{12}{25} - \\frac{23}{48}$$\n$$\\Delta G_2 = \\frac{12 \\times 48 - 23 \\times 25}{25 \\times 48} = \\frac{576 - 575}{1200} = \\frac{1}{1200}$$\n\n**3. Threshold $t = 7.0$**\n- Left child counts: $(30, 40)$. $N_L = 70$. Proportions: $p_{s,L} = \\frac{30}{70} = \\frac{3}{7}, p_{r,L} = \\frac{40}{70} = \\frac{4}{7}$.\n- Right child counts: $(18, 32)$. $N_R = 50$. Proportions: $p_{s,R} = \\frac{18}{50} = \\frac{9}{25}, p_{r,R} = \\frac{32}{50} = \\frac{16}{25}$.\n$$G_L = 1 - \\left(\\left(\\frac{3}{7}\\right)^2 + \\left(\\frac{4}{7}\\right)^2\\right) = 1 - \\left(\\frac{9}{49} + \\frac{16}{49}\\right) = 1 - \\frac{25}{49} = \\frac{24}{49}$$\n$$G_R = 1 - \\left(\\left(\\frac{9}{25}\\right)^2 + \\left(\\frac{16}{25}\\right)^2\\right) = 1 - \\left(\\frac{81}{625} + \\frac{256}{625}\\right) = 1 - \\frac{337}{625} = \\frac{288}{625}$$\nThe impurity decrease is:\n$$\\Delta G_3 = \\frac{12}{25} - \\left(\\frac{70}{120}\\left(\\frac{24}{49}\\right) + \\frac{50}{120}\\left(\\frac{288}{625}\\right)\\right) = \\frac{12}{25} - \\left(\\frac{7}{12}\\left(\\frac{24}{49}\\right) + \\frac{5}{12}\\left(\\frac{288}{625}\\right)\\right)$$\n$$\\Delta G_3 = \\frac{12}{25} - \\left(\\frac{2}{7} + \\frac{24}{125}\\right) = \\frac{12}{25} - \\left(\\frac{250}{875} + \\frac{168}{875}\\right) = \\frac{12}{25} - \\frac{418}{875}$$\n$$\\Delta G_3 = \\frac{12 \\times 35 - 418}{875} = \\frac{420 - 418}{875} = \\frac{2}{875}$$\n\n**4. Threshold $t = 8.5$**\n- Left child counts: $(40, 55)$. $N_L = 95$. Proportions: $p_{s,L} = \\frac{40}{95} = \\frac{8}{19}, p_{r,L} = \\frac{55}{95} = \\frac{11}{19}$.\n- Right child counts: $(8, 17)$. $N_R = 25$. Proportions: $p_{s,R} = \\frac{8}{25}, p_{r,R} = \\frac{17}{25}$.\n$$G_L = 1 - \\left(\\left(\\frac{8}{19}\\right)^2 + \\left(\\frac{11}{19}\\right)^2\\right) = 1 - \\left(\\frac{64}{361} + \\frac{121}{361}\\right) = 1 - \\frac{185}{361} = \\frac{176}{361}$$\n$$G_R = 1 - \\left(\\left(\\frac{8}{25}\\right)^2 + \\left(\\frac{17}{25}\\right)^2\\right) = 1 - \\left(\\frac{64}{625} + \\frac{289}{625}\\right) = 1 - \\frac{353}{625} = \\frac{272}{625}$$\nThe impurity decrease is:\n$$\\Delta G_4 = \\frac{12}{25} - \\left(\\frac{95}{120}\\left(\\frac{176}{361}\\right) + \\frac{25}{120}\\left(\\frac{272}{625}\\right)\\right) = \\frac{12}{25} - \\left(\\frac{19}{24}\\left(\\frac{176}{19^2}\\right) + \\frac{5}{24}\\left(\\frac{272}{5^4}\\right)\\right)$$\n$$\\Delta G_4 = \\frac{12}{25} - \\left(\\frac{176}{24 \\times 19} + \\frac{272}{24 \\times 125}\\right) = \\frac{12}{25} - \\left(\\frac{22}{57} + \\frac{34}{375}\\right) = \\frac{12}{25} - \\left(\\frac{10188}{21375}\\right) = \\frac{10260-10188}{21375} = \\frac{72}{21375} = \\frac{8}{2375}$$\n\nFinally, we compare the four impurity decrease values to find the maximum.\n$\\Delta G_1 = \\frac{9}{800} = 0.01125$\n$\\Delta G_2 = \\frac{1}{1200} \\approx 0.000833$\n$\\Delta G_3 = \\frac{2}{875} \\approx 0.002286$\n$\\Delta G_4 = \\frac{8}{2375} \\approx 0.003368$\nThe largest value is $\\Delta G_1$.\nThus, the best split is at threshold $t = 3.0$, and the maximum impurity decrease is $\\Delta G_{\\max} = \\frac{9}{800}$. This fraction is in simplest terms.", "answer": "$$\\boxed{\\frac{9}{800}}$$", "id": "3342859"}, {"introduction": "Once a tree is built, we often want to know which features were most influential in its decisions. The Mean Decrease in Impurity (MDI) provides one way to answer this by summing the Gini impurity reductions contributed by a feature at every node where it was used to split. This practice problem demonstrates how to aggregate these local gains into a single importance score, providing insight into the feature's role within a specific tree structure [@problem_id:3342894].", "problem": "A computational systems biology laboratory is using a Random Forest classifier to stratify single cells into two phenotypic classes based on molecular measurements: class label $Y \\in \\{0,1\\}$, where $Y=1$ denotes a pathway-active state and $Y=0$ denotes an inactive state. Two features are measured for each cell: $x_{1}$ is the log-transformed expression of a transcription factor and $x_{2}$ is the promoter methylation fraction of a pathway gene. Consider one decision tree within the forest trained on a balanced dataset of $N_{\\text{total}} = 100$ cells, all with equal sample weight.\n\nThis tree has the following structure. The root node (denoted $t_{0}$) splits on feature $x_{1}$ at threshold $\\tau_{1}$, sending $N_{L}(t_{0}) = 50$ cells left and $N_{R}(t_{0}) = 50$ cells right. The class counts at $t_{0}$ and its children are:\n- At $t_{0}$: $(55,45)$ for $(Y=1,Y=0)$.\n- Left child $t_{1}$: $(40,10)$ for $(Y=1,Y=0)$; node $t_{1}$ further splits on feature $x_{2}$ at threshold $\\tau_{2}$.\n- Right child $t_{2}$: $(15,35)$ for $(Y=1,Y=0)$; node $t_{2}$ further splits on feature $x_{1}$ at threshold $\\tau'_{1}$.\n\nThe terminal leaves from these subsequent splits have the following class counts and sample sizes:\n- From $t_{1}$ (split on $x_{2}$): $t_{3}$ with $N(t_{3}) = 30$ and counts $(30,0)$, and $t_{4}$ with $N(t_{4}) = 20$ and counts $(10,10)$.\n- From $t_{2}$ (split on $x_{1}$): $t_{5}$ with $N(t_{5}) = 20$ and counts $(10,10)$, and $t_{6}$ with $N(t_{6}) = 30$ and counts $(5,25)$.\n\nAssume the impurity at a node is quantified by the Gini impurity, defined for a node $t$ with class probabilities $\\{p_{k}(t)\\}_{k \\in \\{0,1\\}}$ as $G(t) = 1 - \\sum_{k \\in \\{0,1\\}} p_{k}(t)^{2}$. Mean Decrease in Impurity (MDI) feature importance for a feature $x_{j}$ aggregates, over all split nodes that use $x_{j}$, the impurity reductions induced by those splits, weighted by the proportion of the total training samples that reach each split node. Terminal leaves do not contribute.\n\nUsing only these definitions and the provided counts, compute the Mean Decrease in Impurity $VI^{\\text{MDI}}(x_{1})$ and $VI^{\\text{MDI}}(x_{2})$ for this single tree, without any post-hoc normalization across features. Express your final answer as a two-entry row matrix $\\big[VI^{\\text{MDI}}(x_{1}) \\;\\; VI^{\\text{MDI}}(x_{2})\\big]$ using exact values; do not round.", "solution": "The Mean Decrease in Impurity (MDI) for a feature $x_{j}$ is calculated by summing the impurity decreases for all nodes $t$ where a split on $x_{j}$ occurs. Each decrease is weighted by the fraction of samples that reach node $t$. The formula is:\n$$VI^{\\text{MDI}}(x_{j}) = \\sum_{t \\text{ splits on } x_{j}} \\frac{N(t)}{N_{\\text{total}}} \\Delta G(t)$$\nwhere $\\Delta G(t)$ is the impurity reduction at node $t$, given by:\n$$\\Delta G(t) = G(t) - \\frac{N_{L}(t)}{N(t)}G(t_{L}) - \\frac{N_{R}(t)}{N(t)}G(t_{R})$$\nHere $t_L$ and $t_R$ are the left and right children of $t$, and $G(t)$ is the Gini impurity. For a node $t$ with class counts $(n_1, n_0)$ for classes $Y=1$ and $Y=0$ respectively, the Gini impurity is:\n$$G(t) = 1 - \\left( \\left(\\frac{n_1}{n_1+n_0}\\right)^2 + \\left(\\frac{n_0}{n_1+n_0}\\right)^2 \\right)$$\n\nFirst, we calculate the Gini impurity for all nodes in the tree.\n- $t_{0}$: counts $(55, 45)$, $N(t_{0})=100$.\n$G(t_{0}) = 1 - \\left( \\left(\\frac{55}{100}\\right)^2 + \\left(\\frac{45}{100}\\right)^2 \\right) = 1 - \\left( \\left(\\frac{11}{20}\\right)^2 + \\left(\\frac{9}{20}\\right)^2 \\right) = 1 - \\frac{121+81}{400} = 1 - \\frac{202}{400} = \\frac{198}{400} = \\frac{99}{200}$.\n\n- $t_{1}$: counts $(40, 10)$, $N(t_{1})=50$.\n$G(t_{1}) = 1 - \\left( \\left(\\frac{40}{50}\\right)^2 + \\left(\\frac{10}{50}\\right)^2 \\right) = 1 - \\left( \\left(\\frac{4}{5}\\right)^2 + \\left(\\frac{1}{5}\\right)^2 \\right) = 1 - \\frac{16+1}{25} = 1 - \\frac{17}{25} = \\frac{8}{25}$.\n\n- $t_{2}$: counts $(15, 35)$, $N(t_{2})=50$.\n$G(t_{2}) = 1 - \\left( \\left(\\frac{15}{50}\\right)^2 + \\left(\\frac{35}{50}\\right)^2 \\right) = 1 - \\left( \\left(\\frac{3}{10}\\right)^2 + \\left(\\frac{7}{10}\\right)^2 \\right) = 1 - \\frac{9+49}{100} = 1 - \\frac{58}{100} = \\frac{42}{100} = \\frac{21}{50}$.\n\n- $t_{3}$ (leaf): counts $(30, 0)$, $N(t_{3})=30$.\n$G(t_{3}) = 1 - \\left( \\left(\\frac{30}{30}\\right)^2 + \\left(\\frac{0}{30}\\right)^2 \\right) = 1 - 1 = 0$.\n\n- $t_{4}$ (leaf): counts $(10, 10)$, $N(t_{4})=20$.\n$G(t_{4}) = 1 - \\left( \\left(\\frac{10}{20}\\right)^2 + \\left(\\frac{10}{20}\\right)^2 \\right) = 1 - \\left( \\left(\\frac{1}{2}\\right)^2 + \\left(\\frac{1}{2}\\right)^2 \\right) = 1 - \\frac{1}{4} - \\frac{1}{4} = \\frac{1}{2}$.\n\n- $t_{5}$ (leaf): counts $(10, 10)$, $N(t_{5})=20$.\n$G(t_{5}) = G(t_{4}) = \\frac{1}{2}$.\n\n- $t_{6}$ (leaf): counts $(5, 25)$, $N(t_{6})=30$.\n$G(t_{6}) = 1 - \\left( \\left(\\frac{5}{30}\\right)^2 + \\left(\\frac{25}{30}\\right)^2 \\right) = 1 - \\left( \\left(\\frac{1}{6}\\right)^2 + \\left(\\frac{5}{6}\\right)^2 \\right) = 1 - \\frac{1+25}{36} = 1 - \\frac{26}{36} = \\frac{10}{36} = \\frac{5}{18}$.\n\nNext, we calculate the MDI contributions from each split.\nThe split nodes are $t_0$ (on $x_1$), $t_1$ (on $x_2$), and $t_2$ (on $x_1$).\n\nContribution from the split at $t_{0}$ (feature $x_{1}$):\nThe weight is $\\frac{N(t_{0})}{N_{\\text{total}}} = \\frac{100}{100} = 1$. The impurity reduction is:\n$\\Delta G(t_{0}) = G(t_{0}) - \\frac{N(t_{1})}{N(t_{0})}G(t_{1}) - \\frac{N(t_{2})}{N(t_{0})}G(t_{2}) = \\frac{99}{200} - \\frac{50}{100}\\left(\\frac{8}{25}\\right) - \\frac{50}{100}\\left(\\frac{21}{50}\\right)$\n$\\Delta G(t_{0}) = \\frac{99}{200} - \\frac{1}{2}\\left(\\frac{8}{25}\\right) - \\frac{1}{2}\\left(\\frac{21}{50}\\right) = \\frac{99}{200} - \\frac{4}{25} - \\frac{21}{100} = \\frac{99 - 32 - 42}{200} = \\frac{25}{200} = \\frac{1}{8}$.\nThe contribution to $VI^{\\text{MDI}}(x_{1})$ from this split is $1 \\times \\frac{1}{8} = \\frac{1}{8}$.\n\nContribution from the split at $t_{1}$ (feature $x_{2}$):\nThe weight is $\\frac{N(t_{1})}{N_{\\text{total}}} = \\frac{50}{100} = \\frac{1}{2}$. The impurity reduction is:\n$\\Delta G(t_{1}) = G(t_{1}) - \\frac{N(t_{3})}{N(t_{1})}G(t_{3}) - \\frac{N(t_{4})}{N(t_{1})}G(t_{4}) = \\frac{8}{25} - \\frac{30}{50}(0) - \\frac{20}{50}\\left(\\frac{1}{2}\\right)$\n$\\Delta G(t_{1}) = \\frac{8}{25} - \\frac{2}{5}\\left(\\frac{1}{2}\\right) = \\frac{8}{25} - \\frac{1}{5} = \\frac{8-5}{25} = \\frac{3}{25}$.\nThe contribution to $VI^{\\text{MDI}}(x_{2})$ from this split is $\\frac{1}{2} \\times \\frac{3}{25} = \\frac{3}{50}$.\n\nContribution from the split at $t_{2}$ (feature $x_{1}$):\nThe weight is $\\frac{N(t_{2})}{N_{\\text{total}}} = \\frac{50}{100} = \\frac{1}{2}$. The impurity reduction is:\n$\\Delta G(t_{2}) = G(t_{2}) - \\frac{N(t_{5})}{N(t_{2})}G(t_{5}) - \\frac{N(t_{6})}{N(t_{2})}G(t_{6}) = \\frac{21}{50} - \\frac{20}{50}\\left(\\frac{1}{2}\\right) - \\frac{30}{50}\\left(\\frac{5}{18}\\right)$\n$\\Delta G(t_{2}) = \\frac{21}{50} - \\frac{1}{5} - \\frac{3}{5}\\left(\\frac{5}{18}\\right) = \\frac{21}{50} - \\frac{1}{5} - \\frac{1}{6} = \\frac{63}{150} - \\frac{30}{150} - \\frac{25}{150} = \\frac{63-55}{150} = \\frac{8}{150} = \\frac{4}{75}$.\nThe contribution to $VI^{\\text{MDI}}(x_{1})$ from this split is $\\frac{1}{2} \\times \\frac{4}{75} = \\frac{2}{75}$.\n\nFinally, we sum the contributions for each feature.\nFor feature $x_{1}$, the splits are at $t_{0}$ and $t_{2}$.\n$VI^{\\text{MDI}}(x_{1}) = (\\text{contribution from } t_{0}) + (\\text{contribution from } t_{2}) = \\frac{1}{8} + \\frac{2}{75}$.\nThe least common multiple of $8$ and $75$ is $8 \\times 75 = 600$.\n$VI^{\\text{MDI}}(x_{1}) = \\frac{1 \\times 75}{600} + \\frac{2 \\times 8}{600} = \\frac{75+16}{600} = \\frac{91}{600}$.\n\nFor feature $x_{2}$, the only split is at $t_{1}$.\n$VI^{\\text{MDI}}(x_{2}) = (\\text{contribution from } t_{1}) = \\frac{3}{50}$.\n\nThe final answer is the row matrix $[VI^{\\text{MDI}}(x_{1}) \\;\\; VI^{\\text{MDI}}(x_{2})]$.\n$VI^{\\text{MDI}}(x_{1}) = \\frac{91}{600}$\n$VI^{\\text{MDI}}(x_{2}) = \\frac{3}{50} = \\frac{3 \\times 12}{50 \\times 12} = \\frac{36}{600}$.\nThe requested matrix is $\\big[ \\frac{91}{600} \\;\\; \\frac{3}{50} \\big]$.", "answer": "$$\\boxed{\\begin{pmatrix} \\frac{91}{600} & \\frac{3}{50} \\end{pmatrix}}$$", "id": "3342894"}, {"introduction": "While MDI is calculated from training data statistics, permutation importance offers a different and often more reliable perspective by evaluating a feature's impact on a model's performance on unseen data. This exercise guides you through the process of calculating permutation importance for a feature using Out-of-Bag (OOB) samples, a unique advantage of the Random Forest algorithm. By measuring the drop in accuracy after shuffling a feature's values, and assessing the variability of this measure, you will practice a powerful technique for understanding a model's real-world predictive dependencies [@problem_id:3342911].", "problem": "Consider a simplified computational systems biology classification task in which a Random Forest (RF) classifier is trained to distinguish phenotype $y \\in \\{0,1\\}$ from gene expression features $(G_1,G_2,G_3)$. The RF comprises $T=3$ decision trees trained with bootstrap resampling from a dataset of $N=6$ samples. The dataset is\n$i=1: (x_{11},x_{12},x_{13};y_1)=(1.0,5.5,6.0;1)$,\n$i=2: (x_{21},x_{22},x_{23};y_2)=(2.5,3.0,7.5;1)$,\n$i=3: (x_{31},x_{32},x_{33};y_3)=(3.5,6.5,5.5;1)$,\n$i=4: (x_{41},x_{42},x_{43};y_4)=(1.5,2.5,4.0;0)$,\n$i=5: (x_{51},x_{52},x_{53};y_5)=(4.0,4.5,6.5;0)$,\n$i=6: (x_{61},x_{62},x_{63};y_6)=(2.0,5.0,3.5;0)$,\nwhere $x_{i1}$, $x_{i2}$, and $x_{i3}$ denote the measured expressions of $G_1$, $G_2$, and $G_3$, respectively.\n\nThe three trees are trained on the following bootstrap samples (index lists of training samples, with replacement): for tree $t=1$, $B_1 = [1,2,5,5,6,6]$; for tree $t=2$, $B_2 = [1,3,3,4,6,6]$; for tree $t=3$, $B_3 = [2,2,3,5,5,6]$. The Out-of-Bag (OOB) set for tree $t$ is the complement $O_t=\\{1,\\dots,N\\}\\setminus B_t$ ignoring multiplicities: thus $O_1=\\{3,4\\}$, $O_2=\\{2,5\\}$, $O_3=\\{1,4\\}$.\n\nEach trained tree deterministically predicts a class $\\hat{y}_t(x)$ according to the following rules:\n- Tree $t=1$: predict $\\hat{y}_1(x)=1$ if $x_{2} \\ge 5.0$, and $\\hat{y}_1(x)=0$ otherwise.\n- Tree $t=2$: predict $\\hat{y}_2(x)=1$ if $x_{3} \\ge 6.0$; else if $x_{2} \\ge 4.0$, predict $\\hat{y}_2(x)=1$; otherwise predict $\\hat{y}_2(x)=0$.\n- Tree $t=3$: predict $\\hat{y}_3(x)=0$ if $x_{1} \\le 2.0$; else if $x_{2} \\ge 6.0$, predict $\\hat{y}_3(x)=1$; otherwise predict $\\hat{y}_3(x)=0$.\n\nDefine the OOB misclassification rate for tree $t$ as $L_t^{\\mathrm{OOB}}=\\frac{1}{|O_t|}\\sum_{i\\in O_t}\\mathbf{1}\\{\\hat{y}_t(x_i)\\ne y_i\\}$, where $\\mathbf{1}\\{\\cdot\\}$ is the indicator function. To assess the permutation importance of gene $G_2$ (denoted $j=2$), perform a single OOB permutation of $x_{i2}$ values within each treeâ€™s OOB set according to the following mappings:\n- For $t=1$ on $O_1=\\{3,4\\}$, swap $x_{32}$ and $x_{42}$ so that the permuted values are $x_{32}^{\\pi}=2.5$ and $x_{42}^{\\pi}=6.5$.\n- For $t=2$ on $O_2=\\{2,5\\}$, swap $x_{22}$ and $x_{52}$ so that $x_{22}^{\\pi}=4.5$ and $x_{52}^{\\pi}=3.0$.\n- For $t=3$ on $O_3=\\{1,4\\}$, swap $x_{12}$ and $x_{42}$ so that $x_{12}^{\\pi}=2.5$ and $x_{42}^{\\pi}=5.5$.\n\nUsing these permuted values (and holding all other features $x_{i1}$ and $x_{i3}$ fixed), compute the post-permutation OOB misclassification rate $L_t^{\\mathrm{perm}}=\\frac{1}{|O_t|}\\sum_{i\\in O_t}\\mathbf{1}\\{\\hat{y}_t(x_i^{\\pi})\\ne y_i\\}$, where $x_i^{\\pi}$ replaces $x_{i2}$ by $x_{i2}^{\\pi}$ for $i\\in O_t$. For each tree, define the single-permutation importance increment $\\Delta_t=L_t^{\\mathrm{perm}}-L_t^{\\mathrm{OOB}}$.\n\nFinally, compute:\n- the aggregate importance $\\bar{\\Delta}=\\frac{1}{T}\\sum_{t=1}^{T}\\Delta_t$; and\n- the across-tree unbiased sample variance $s^2=\\frac{1}{T-1}\\sum_{t=1}^{T}\\left(\\Delta_t-\\bar{\\Delta}\\right)^2$.\n\nReport your final answer as a two-entry row matrix $\\left(\\bar{\\Delta}, s^2\\right)$ in exact form with rational numbers where appropriate. No rounding is required. In your solution, briefly interpret the magnitude of $s^2$ relative to $\\bar{\\Delta}$ in terms of uncertainty in the importance estimate arising from the single OOB permutation per tree.", "solution": "The task is to compute the aggregate permutation importance $\\bar{\\Delta}$ and its sample variance $s^2$ for gene $G_2$. This requires a step-by-step calculation for each of the $T=3$ trees in the forest.\n\nThe dataset consists of $N=6$ samples:\n$x_1=(1.0, 5.5, 6.0; y_1=1)$\n$x_2=(2.5, 3.0, 7.5; y_2=1)$\n$x_3=(3.5, 6.5, 5.5; y_3=1)$\n$x_4=(1.5, 2.5, 4.0; y_4=0)$\n$x_5=(4.0, 4.5, 6.5; y_5=0)$\n$x_6=(2.0, 5.0, 3.5; y_6=0)$\n\nFor each tree $t$, we first compute the Out-of-Bag (OOB) misclassification rate, $L_t^{\\mathrm{OOB}}$, then the rate after permuting the feature $G_2$ for the OOB samples, $L_t^{\\mathrm{perm}}$, and finally the importance increment $\\Delta_t = L_t^{\\mathrm{perm}} - L_t^{\\mathrm{OOB}}$.\n\n**Tree $t=1$**\nThe decision rule is $\\hat{y}_1(x)=1$ if $x_2 \\ge 5.0$, and $\\hat{y}_1(x)=0$ otherwise.\nThe OOB set is $O_1=\\{3,4\\}$, so $|O_1|=2$. The corresponding samples are $x_3=(3.5, 6.5, 5.5; y_3=1)$ and $x_4=(1.5, 2.5, 4.0; y_4=0)$.\n\n1.  **Calculate $L_1^{\\mathrm{OOB}}$**:\n    - For sample $i=3$: $x_{32} = 6.5 \\ge 5.0$, so $\\hat{y}_1(x_3)=1$. The true label is $y_3=1$. The prediction is correct. Misclassification indicator $\\mathbf{1}\\{\\hat{y}_1(x_3) \\ne y_3\\} = 0$.\n    - For sample $i=4$: $x_{42} = 2.5 < 5.0$, so $\\hat{y}_1(x_4)=0$. The true label is $y_4=0$. The prediction is correct. Misclassification indicator $\\mathbf{1}\\{\\hat{y}_1(x_4) \\ne y_4\\} = 0$.\n    - $L_1^{\\mathrm{OOB}} = \\frac{1}{|O_1|} \\sum_{i \\in O_1} \\mathbf{1}\\{\\hat{y}_1(x_i) \\ne y_i\\} = \\frac{1}{2}(0+0) = 0$.\n\n2.  **Calculate $L_1^{\\mathrm{perm}}$**:\n    The values for feature $G_2$ ($x_{i2}$) are permuted for samples in $O_1$. We are given $x_{32}^{\\pi}=2.5$ and $x_{42}^{\\pi}=6.5$.\n    The permuted OOB samples are $x_3^{\\pi}=(3.5, 2.5, 5.5)$ with $y_3=1$, and $x_4^{\\pi}=(1.5, 6.5, 4.0)$ with $y_4=0$.\n    - For sample $i=3$: The permuted feature value is $x_{32}^{\\pi}=2.5 < 5.0$, so $\\hat{y}_1(x_3^{\\pi})=0$. The true label is $y_3=1$. The prediction is incorrect. $\\mathbf{1}\\{\\hat{y}_1(x_3^{\\pi}) \\ne y_3\\} = 1$.\n    - For sample $i=4$: The permuted feature value is $x_{42}^{\\pi}=6.5 \\ge 5.0$, so $\\hat{y}_1(x_4^{\\pi})=1$. The true label is $y_4=0$. The prediction is incorrect. $\\mathbf{1}\\{\\hat{y}_1(x_4^{\\pi}) \\ne y_4\\} = 1$.\n    - $L_1^{\\mathrm{perm}} = \\frac{1}{|O_1|} \\sum_{i \\in O_1} \\mathbf{1}\\{\\hat{y}_1(x_i^{\\pi}) \\ne y_i\\} = \\frac{1}{2}(1+1) = 1$.\n\n3.  **Calculate $\\Delta_1$**:\n    $\\Delta_1 = L_1^{\\mathrm{perm}} - L_1^{\\mathrm{OOB}} = 1 - 0 = 1$.\n\n**Tree $t=2$**\nThe decision rule is $\\hat{y}_2(x)=1$ if $x_3 \\ge 6.0$; else if $x_2 \\ge 4.0$, predict $\\hat{y}_2(x)=1$; otherwise predict $\\hat{y}_2(x)=0$.\nThe OOB set is $O_2=\\{2,5\\}$, so $|O_2|=2$. The samples are $x_2=(2.5, 3.0, 7.5; y_2=1)$ and $x_5=(4.0, 4.5, 6.5; y_5=0)$.\n\n1.  **Calculate $L_2^{\\mathrm{OOB}}$**:\n    - For sample $i=2$: $x_{23} = 7.5 \\ge 6.0$, so $\\hat{y}_2(x_2)=1$. The true label is $y_2=1$. Correct. $\\mathbf{1}\\{\\hat{y}_2(x_2) \\ne y_2\\} = 0$.\n    - For sample $i=5$: $x_{53} = 6.5 \\ge 6.0$, so $\\hat{y}_2(x_5)=1$. The true label is $y_5=0$. Incorrect. $\\mathbf{1}\\{\\hat{y}_2(x_5) \\ne y_5\\} = 1$.\n    - $L_2^{\\mathrm{OOB}} = \\frac{1}{2}(0+1) = \\frac{1}{2}$.\n\n2.  **Calculate $L_2^{\\mathrm{perm}}$**:\n    Permutation gives $x_{22}^{\\pi}=4.5$ and $x_{52}^{\\pi}=3.0$. The permuted samples are $x_2^{\\pi}=(2.5, 4.5, 7.5)$ with $y_2=1$, and $x_5^{\\pi}=(4.0, 3.0, 6.5)$ with $y_5=0$.\n    - For sample $i=2$: $x_{23} = 7.5 \\ge 6.0$, so the first rule fires. $\\hat{y}_2(x_2^{\\pi})=1$. True label $y_2=1$. Correct. $\\mathbf{1}\\{\\hat{y}_2(x_2^{\\pi}) \\ne y_2\\} = 0$.\n    - For sample $i=5$: $x_{53} = 6.5 \\ge 6.0$, so the first rule fires. $\\hat{y}_2(x_5^{\\pi})=1$. True label $y_5=0$. Incorrect. $\\mathbf{1}\\{\\hat{y}_2(x_5^{\\pi}) \\ne y_5\\} = 1$.\n    - $L_2^{\\mathrm{perm}} = \\frac{1}{2}(0+1) = \\frac{1}{2}$.\n\n3.  **Calculate $\\Delta_2$**:\n    $\\Delta_2 = L_2^{\\mathrm{perm}} - L_2^{\\mathrm{OOB}} = \\frac{1}{2} - \\frac{1}{2} = 0$.\n\n**Tree $t=3$**\nThe decision rule is $\\hat{y}_3(x)=0$ if $x_1 \\le 2.0$; else if $x_2 \\ge 6.0$, predict $\\hat{y}_3(x)=1$; otherwise predict $\\hat{y}_3(x)=0$.\nThe OOB set is $O_3=\\{1,4\\}$, so $|O_3|=2$. The samples are $x_1=(1.0, 5.5, 6.0; y_1=1)$ and $x_4=(1.5, 2.5, 4.0; y_4=0)$.\n\n1.  **Calculate $L_3^{\\mathrm{OOB}}$**:\n    - For sample $i=1$: $x_{11} = 1.0 \\le 2.0$, so $\\hat{y}_3(x_1)=0$. True label $y_1=1$. Incorrect. $\\mathbf{1}\\{\\hat{y}_3(x_1) \\ne y_1\\} = 1$.\n    - For sample $i=4$: $x_{41} = 1.5 \\le 2.0$, so $\\hat{y}_3(x_4)=0$. True label $y_4=0$. Correct. $\\mathbf{1}\\{\\hat{y}_3(x_4) \\ne y_4\\} = 0$.\n    - $L_3^{\\mathrm{OOB}} = \\frac{1}{2}(1+0) = \\frac{1}{2}$.\n\n2.  **Calculate $L_3^{\\mathrm{perm}}$**:\n    Permutation gives $x_{12}^{\\pi}=2.5$ and $x_{42}^{\\pi}=5.5$. The permuted samples are $x_1^{\\pi}=(1.0, 2.5, 6.0)$ with $y_1=1$, and $x_4^{\\pi}=(1.5, 5.5, 4.0)$ with $y_4=0$.\n    - For sample $i=1$: $x_{11} = 1.0 \\le 2.0$, so the first rule fires. $\\hat{y}_3(x_1^{\\pi})=0$. True label $y_1=1$. Incorrect. $\\mathbf{1}\\{\\hat{y}_3(x_1^{\\pi}) \\ne y_1\\} = 1$.\n    - For sample $i=4$: $x_{41} = 1.5 \\le 2.0$, so the first rule fires. $\\hat{y}_3(x_4^{\\pi})=0$. True label $y_4=0$. Correct. $\\mathbf{1}\\{\\hat{y}_3(x_4^{\\pi}) \\ne y_4\\} = 0$.\n    - $L_3^{\\mathrm{perm}} = \\frac{1}{2}(1+0) = \\frac{1}{2}$.\n\n3.  **Calculate $\\Delta_3$**:\n    $\\Delta_3 = L_3^{\\mathrm{perm}} - L_3^{\\mathrm{OOB}} = \\frac{1}{2} - \\frac{1}{2} = 0$.\n\n**Final Aggregation**\nThe individual importance increments are $\\Delta_1=1$, $\\Delta_2=0$, and $\\Delta_3=0$.\n\n-   **Aggregate importance $\\bar{\\Delta}$**:\n    $$ \\bar{\\Delta} = \\frac{1}{T}\\sum_{t=1}^{T}\\Delta_t = \\frac{1}{3}(1+0+0) = \\frac{1}{3} $$\n\n-   **Across-tree unbiased sample variance $s^2$**:\n    $$ s^2 = \\frac{1}{T-1}\\sum_{t=1}^{T}\\left(\\Delta_t-\\bar{\\Delta}\\right)^2 = \\frac{1}{3-1}\\left[ \\left(1-\\frac{1}{3}\\right)^2 + \\left(0-\\frac{1}{3}\\right)^2 + \\left(0-\\frac{1}{3}\\right)^2 \\right] $$\n    $$ s^2 = \\frac{1}{2}\\left[ \\left(\\frac{2}{3}\\right)^2 + \\left(-\\frac{1}{3}\\right)^2 + \\left(-\\frac{1}{3}\\right)^2 \\right] $$\n    $$ s^2 = \\frac{1}{2}\\left[ \\frac{4}{9} + \\frac{1}{9} + \\frac{1}{9} \\right] = \\frac{1}{2}\\left( \\frac{6}{9} \\right) = \\frac{1}{2}\\left( \\frac{2}{3} \\right) = \\frac{1}{3} $$\n\nThe aggregate importance is $\\bar{\\Delta} = \\frac{1}{3}$ and the sample variance is $s^2 = \\frac{1}{3}$.\n\n**Interpretation**\nThe mean importance of gene $G_2$ is estimated to be $\\bar{\\Delta} = 1/3$. The sample variance of this estimate across the three trees is $s^2 = 1/3$. The standard deviation is thus $s = \\sqrt{s^2} = \\sqrt{1/3} \\approx 0.577$. The fact that the variance is equal to the mean (and thus the standard deviation is larger than the mean) indicates a very high degree of uncertainty in the importance estimate. This instability arises from the stochasticity of the process, particularly the single permutation performed on very small OOB sets for each tree. Two of the three trees show a zero importance increment, while one shows a large increment of $1$. This high variance suggests that the estimate of $\\bar{\\Delta}$ is not reliable and that more trees and/or more permutations per tree would be required to obtain a stable measure of feature importance.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{1}{3} & \\frac{1}{3}\n\\end{pmatrix}\n}\n$$", "id": "3342911"}]}