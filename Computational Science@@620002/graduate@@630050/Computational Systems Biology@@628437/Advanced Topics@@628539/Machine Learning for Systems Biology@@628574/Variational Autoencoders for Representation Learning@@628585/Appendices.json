{"hands_on_practices": [{"introduction": "A powerful aspect of VAEs is their ability to learn abstract representations, but this flexibility can also lead to fundamental ambiguities. This first practice explores the concept of non-identifiability, a critical consideration when interpreting latent spaces. By constructing two models that are indistinguishable from observed data yet offer opposing latent explanations [@problem_id:3357963], you will gain a deeper appreciation for the care required in assigning biological meaning, such as pathway activation or repression, to the learned dimensions.", "problem": "Consider a synthetic single-gene expression setting in computational systems biology, where a latent pathway activity variable $z$ influences a log-scale expression measurement $x$. A Variational Autoencoder (VAE; Variational Autoencoder) defines a generative model with prior $p(z)$, a decoder $p_{\\theta}(x \\mid z)$, and an encoder $q_{\\phi}(z \\mid x)$ that approximates the true posterior $p_{\\theta}(z \\mid x)$. Assume $z$ is a one-dimensional continuous latent. Let the measurement noise be additive and Gaussian, reflecting technical and biological variability after appropriate normalization.\n\nConstruct two encoder-decoder pairs (Model $1$ and Model $2$) that yield the same marginal distribution $p_{\\theta}(x)$ but induce different latent representations $z$ for the same $x$. Concretely, for a fixed slope parameter $w \\neq 0$, a fixed mean level $\\mu$, and a fixed noise variance $\\sigma_{x}^{2} > 0$, define the following:\n\n- Model $1$: prior $p_{1}(z) = \\mathcal{N}(0, 1)$ and decoder $p_{1}(x \\mid z) = \\mathcal{N}(w z + \\mu, \\sigma_{x}^{2})$. The encoder is constrained to be Gaussian $q_{1}(z \\mid x) = \\mathcal{N}(m_{1}(x), s^{2})$ with variance $s^{2}$ equal to the exact posterior variance under Model $1$, and mean $m_{1}(x)$ equal to the exact posterior mean under Model $1$.\n- Model $2$: prior $p_{2}(z') = \\mathcal{N}(0, 1)$ and decoder $p_{2}(x \\mid z') = \\mathcal{N}((-w) z' + \\mu, \\sigma_{x}^{2})$. The encoder is constrained to be Gaussian $q_{2}(z' \\mid x) = \\mathcal{N}(m_{2}(x), s^{2})$ with the same variance $s^{2}$ equal to the exact posterior variance under Model $2$, and mean $m_{2}(x)$ equal to the exact posterior mean under Model $2$.\n\nStarting from the definitions of the Gaussian prior, the Gaussian likelihood, and Bayes’ rule for linear-Gaussian models, perform the following:\n\n- Derive the marginal distribution $p_{1}(x)$ for Model $1$ and $p_{2}(x)$ for Model $2$, and show that $p_{1}(x) = p_{2}(x)$.\n- Derive the exact posterior means $m_{1}(x)$ and $m_{2}(x)$, and the shared posterior variance $s^{2}$, under the respective models.\n- For a fixed observation $x$, derive the Kullback–Leibler divergence (KL; Kullback–Leibler divergence) from $q_{1}(z \\mid x)$ to $q_{2}(z' \\mid x)$.\n- Compute the expectation of this KL with respect to the common marginal $p_{\\theta}(x)$, and simplify to a closed-form analytic expression in terms of $w$ and $\\sigma_{x}^{2}$.\n\nDiscuss, in words, the implications of your construction for biological interpretability of the latent $z$ in terms of pathway activity directionality and identifiability. As your final answer, provide only the closed-form analytic expression for the expected KL divergence $\\mathbb{E}_{p_{\\theta}(x)}\\left[ \\mathrm{KL}\\big(q_{1}(z \\mid x) \\,\\|\\, q_{2}(z' \\mid x)\\big) \\right]$. No rounding is required, and no units are to be reported in the final answer.", "solution": "The problem statement has been validated and is deemed valid. It is a well-posed, scientifically grounded problem in statistical machine learning and its application to computational systems biology. It is self-contained, consistent, and free of any logical or factual flaws. A complete solution will now be provided.\n\nThe problem requires the construction and analysis of two distinct generative models that produce identical observable data distributions but differ in their latent representations. This explores a fundamental non-identifiability issue in latent variable models. We will proceed through the required derivations step by step.\n\n**$1$. Derivation of the Marginal Distributions $p_{1}(x)$ and $p_{2}(x)$**\n\nThe marginal distribution of the observation $x$ is found by integrating the joint distribution $p(x, z) = p(x \\mid z)p(z)$ over the latent variable $z$. For both models, this corresponds to a convolution of two Gaussian distributions, which results in a Gaussian distribution.\n\nFor a general hierarchical model where $z \\sim \\mathcal{N}(\\mu_z, \\sigma_z^2)$ and $x \\mid z \\sim \\mathcal{N}(az+b, \\sigma_x^2)$, the marginal distribution of $x$ is also Gaussian, $x \\sim \\mathcal{N}(\\mu_x, \\sigma_{total}^2)$, with mean and variance given by:\n$\\mu_x = \\mathbb{E}[x] = \\mathbb{E}[\\mathbb{E}[x \\mid z]] = \\mathbb{E}[az+b] = a\\mu_z + b$\n$\\sigma_{total}^2 = \\mathrm{Var}(x) = \\mathbb{E}[\\mathrm{Var}(x \\mid z)] + \\mathrm{Var}(\\mathbb{E}[x \\mid z]) = \\mathbb{E}[\\sigma_x^2] + \\mathrm{Var}(az+b) = \\sigma_x^2 + a^2\\sigma_z^2$\n\nFor Model $1$:\nThe prior is $p_{1}(z) = \\mathcal{N}(z \\mid 0, 1)$, so $\\mu_z = 0$ and $\\sigma_z^2 = 1$.\nThe decoder is $p_{1}(x \\mid z) = \\mathcal{N}(x \\mid wz + \\mu, \\sigma_{x}^{2})$, so $a=w$ and $b=\\mu$.\n\nThe marginal mean of $x$ is:\n$$\\mathbb{E}_{p_1}[x] = w(0) + \\mu = \\mu$$\nThe marginal variance of $x$ is:\n$$\\mathrm{Var}_{p_1}(x) = \\sigma_{x}^{2} + w^2(1) = w^2 + \\sigma_{x}^{2}$$\nThus, the marginal distribution for Model $1$ is:\n$$p_{1}(x) = \\mathcal{N}(x \\mid \\mu, w^2 + \\sigma_{x}^{2})$$\n\nFor Model $2$:\nThe prior is $p_{2}(z') = \\mathcal{N}(z' \\mid 0, 1)$, so $\\mu_{z'} = 0$ and $\\sigma_{z'}^2 = 1$.\nThe decoder is $p_{2}(x \\mid z') = \\mathcal{N}(x \\mid (-w)z' + \\mu, \\sigma_{x}^{2})$, so $a=-w$ and $b=\\mu$.\n\nThe marginal mean of $x$ is:\n$$\\mathbb{E}_{p_2}[x] = (-w)(0) + \\mu = \\mu$$\nThe marginal variance of $x$ is:\n$$\\mathrm{Var}_{p_2}(x) = \\sigma_{x}^{2} + (-w)^2(1) = w^2 + \\sigma_{x}^{2}$$\nThus, the marginal distribution for Model $2$ is:\n$$p_{2}(x) = \\mathcal{N}(x \\mid \\mu, w^2 + \\sigma_{x}^{2})$$\n\nComparing the two marginal distributions, we find that $p_{1}(x) = p_{2}(x)$. Both models generate data from the same distribution, which we will denote as $p_{\\theta}(x) = \\mathcal{N}(\\mu, w^2 + \\sigma_{x}^{2})$. This confirms that the models are indistinguishable from the observations $x$ alone.\n\n**$2$. Derivation of Posterior Means and Variance**\n\nFor a linear-Gaussian model with prior $p(z) = \\mathcal{N}(z \\mid \\mu_z, \\Sigma_z)$ and likelihood $p(x \\mid z) = \\mathcal{N}(x \\mid Az+b, \\Sigma_x)$, the posterior $p(z \\mid x)$ is also Gaussian, $p(z \\mid x) = \\mathcal{N}(z \\mid \\mu_{z \\mid x}, \\Sigma_{z \\mid x})$, with parameters:\n$$\\Sigma_{z \\mid x} = (\\Sigma_z^{-1} + A^T \\Sigma_x^{-1} A)^{-1}$$\n$$\\mu_{z \\mid x} = \\Sigma_{z \\mid x} (A^T \\Sigma_x^{-1}(x-b) + \\Sigma_z^{-1}\\mu_z)$$\nIn our $1$-dimensional case, matrices become scalars.\n\nFor Model $1$:\n$p_{1}(z) = \\mathcal{N}(z \\mid 0, 1) \\implies \\mu_z = 0, \\Sigma_z = 1$.\n$p_{1}(x \\mid z) = \\mathcal{N}(x \\mid wz + \\mu, \\sigma_x^2) \\implies A = w, b = \\mu, \\Sigma_x = \\sigma_x^2$.\n\nThe posterior variance $s^2$ is:\n$$s^2 = (1^{-1} + w (\\sigma_x^2)^{-1} w)^{-1} = (1 + \\frac{w^2}{\\sigma_x^2})^{-1} = \\left(\\frac{\\sigma_x^2 + w^2}{\\sigma_x^2}\\right)^{-1} = \\frac{\\sigma_x^2}{w^2 + \\sigma_x^2}$$\nThe posterior mean $m_{1}(x)$ is:\n$$m_{1}(x) = s^2 (w (\\sigma_x^2)^{-1} (x - \\mu) + 1^{-1} \\cdot 0) = \\frac{\\sigma_x^2}{w^2 + \\sigma_x^2} \\left( \\frac{w}{\\sigma_x^2} (x - \\mu) \\right) = \\frac{w(x - \\mu)}{w^2 + \\sigma_x^2}$$\nSo, $q_{1}(z \\mid x) = \\mathcal{N}(z \\mid m_{1}(x), s^2)$.\n\nFor Model $2$:\n$p_{2}(z') = \\mathcal{N}(z' \\mid 0, 1) \\implies \\mu_{z'} = 0, \\Sigma_{z'} = 1$.\n$p_{2}(x \\mid z') = \\mathcal{N}(x \\mid (-w)z' + \\mu, \\sigma_x^2) \\implies A = -w, b = \\mu, \\Sigma_x = \\sigma_x^2$.\n\nThe posterior variance is:\n$$(1^{-1} + (-w) (\\sigma_x^2)^{-1} (-w))^{-1} = (1 + \\frac{w^2}{\\sigma_x^2})^{-1} = \\frac{\\sigma_x^2}{w^2 + \\sigma_x^2}$$\nThis is identical to the posterior variance for Model $1$, so the shared posterior variance is indeed $s^2 = \\frac{\\sigma_x^2}{w^2 + \\sigma_x^2}$.\nThe posterior mean $m_{2}(x)$ is:\n$$m_{2}(x) = s^2 ((-w) (\\sigma_x^2)^{-1} (x - \\mu) + 1^{-1} \\cdot 0) = \\frac{\\sigma_x^2}{w^2 + \\sigma_x^2} \\left( \\frac{-w}{\\sigma_x^2} (x - \\mu) \\right) = \\frac{-w(x - \\mu)}{w^2 + \\sigma_x^2}$$\nSo, $q_{2}(z' \\mid x) = \\mathcal{N}(z' \\mid m_{2}(x), s^2)$. Note that $m_{2}(x) = -m_{1}(x)$.\n\n**$3$. Derivation of the Kullback-Leibler Divergence**\n\nThe KL divergence between two $1$-dimensional Gaussian distributions $q_a = \\mathcal{N}(\\mu_a, \\sigma_a^2)$ and $q_b = \\mathcal{N}(\\mu_b, \\sigma_b^2)$ is given by:\n$$\\mathrm{KL}(q_a \\,\\|\\, q_b) = \\ln\\left(\\frac{\\sigma_b}{\\sigma_a}\\right) + \\frac{\\sigma_a^2 + (\\mu_a - \\mu_b)^2}{2\\sigma_b^2} - \\frac{1}{2}$$\nIn our case, we are calculating $\\mathrm{KL}\\big(q_{1}(z \\mid x) \\,\\|\\, q_{2}(z' \\mid x)\\big)$. The distributions are $q_{1} = \\mathcal{N}(m_{1}(x), s^2)$ and $q_{2} = \\mathcal{N}(m_{2}(x), s^2)$.\nSince the variances are equal, $\\sigma_a^2 = \\sigma_b^2 = s^2$, the formula simplifies significantly:\n$$\\mathrm{KL}(q_1 \\,\\|\\, q_2) = \\ln(1) + \\frac{s^2 + (m_{1}(x) - m_{2}(x))^2}{2s^2} - \\frac{1}{2} = \\frac{1}{2} + \\frac{(m_{1}(x) - m_{2}(x))^2}{2s^2} - \\frac{1}{2} = \\frac{(m_{1}(x) - m_{2}(x))^2}{2s^2}$$\nWe substitute the expressions for the means:\n$$m_{1}(x) - m_{2}(x) = \\frac{w(x - \\mu)}{w^2 + \\sigma_x^2} - \\left( \\frac{-w(x - \\mu)}{w^2 + \\sigma_x^2} \\right) = \\frac{2w(x - \\mu)}{w^2 + \\sigma_x^2}$$\nNow, we substitute this difference and the variance $s^2$ into the KL divergence expression:\n$$\\mathrm{KL}\\big(q_{1}(z \\mid x) \\,\\|\\, q_{2}(z' \\mid x)\\big) = \\frac{1}{2s^2} \\left( \\frac{2w(x - \\mu)}{w^2 + \\sigma_x^2} \\right)^2 = \\frac{1}{2 \\frac{\\sigma_x^2}{w^2 + \\sigma_x^2}} \\frac{4w^2(x - \\mu)^2}{(w^2 + \\sigma_x^2)^2}$$\n$$\\mathrm{KL}\\big(q_{1}(z \\mid x) \\,\\|\\, q_{2}(z' \\mid x)\\big) = \\frac{w^2 + \\sigma_x^2}{2\\sigma_x^2} \\frac{4w^2(x - \\mu)^2}{(w^2 + \\sigma_x^2)^2} = \\frac{2w^2(x-\\mu)^2}{\\sigma_x^2(w^2 + \\sigma_x^2)}$$\n\n**$4$. Derivation of the Expected KL Divergence**\n\nWe must compute the expectation of the KL divergence with respect to the common marginal data distribution $p_{\\theta}(x) = \\mathcal{N}(x \\mid \\mu, w^2 + \\sigma_{x}^{2})$.\n$$\\mathbb{E}_{p_{\\theta}(x)}\\left[ \\mathrm{KL}\\big(q_{1}(z \\mid x) \\,\\|\\, q_{2}(z' \\mid x)\\big) \\right] = \\mathbb{E}_{p_{\\theta}(x)}\\left[ \\frac{2w^2(x-\\mu)^2}{\\sigma_x^2(w^2 + \\sigma_x^2)} \\right]$$\nThe term $\\frac{2w^2}{\\sigma_x^2(w^2 + \\sigma_x^2)}$ is a constant with respect to $x$, so we can factor it out of the expectation:\n$$= \\frac{2w^2}{\\sigma_x^2(w^2 + \\sigma_x^2)} \\mathbb{E}_{p_{\\theta}(x)}\\left[ (x-\\mu)^2 \\right]$$\nThe term $\\mathbb{E}_{p_{\\theta}(x)}\\left[ (x-\\mu)^2 \\right]$ is, by definition, the variance of the random variable $x$ under the distribution $p_{\\theta}(x)$, since $\\mu$ is the mean of $x$. We already established that $\\mathrm{Var}_{p_\\theta}(x) = w^2 + \\sigma_x^2$.\nSubstituting this result back:\n$$\\mathbb{E}_{p_{\\theta}(x)}\\left[ \\mathrm{KL} \\right] = \\frac{2w^2}{\\sigma_x^2(w^2 + \\sigma_x^2)} (w^2 + \\sigma_x^2) = \\frac{2w^2}{\\sigma_x^2}$$\n\n**$5$. Discussion of Implications for Biological Interpretability**\n\nThis result exposes a critical issue of non-identifiability in latent variable models like VAEs when used for representation learning in systems biology.\nThe two models, one with gene activation driven by $wz$ and the other by $(-w)z'$, are observationally equivalent. They produce the exact same distribution of log-expression values $x$. This means no amount of data generated by this process can distinguish whether a high latent value $z$ corresponds to high gene expression (Model $1$, assuming $w>0$) or low gene expression (Model $2$).\n\nFor a given observation $x$, the models provide opposing explanations for the latent state: the inferred mean activity is $m_1(x)$ in one model and $m_2(x) = -m_1(x)$ in the other. If a biologist were to interpret $z$ as a \"pathway activity\", one model would suggest the pathway is activated while the other would suggest it is repressed to explain the same data point. The directionality of the latent variable's effect on the observation is unresolvable from the data alone.\n\nThis ambiguity, or sign-indeterminacy, is a direct consequence of the symmetries in the model: the prior $p(z)=\\mathcal{N}(0,1)$ is symmetric around $0$, and the function mapping the latent variable to the mean of the observation has a sign ambiguity $w \\leftrightarrow -w$. Any VAE trained on this data could converge to either of these solutions (or a mixture), making the interpretation of the learned latent dimensions' signs arbitrary.\n\nThe final result, $\\mathbb{E}[\\mathrm{KL}] = \\frac{2w^2}{\\sigma_x^2}$, quantifies the average dissimilarity between these two equally valid posterior beliefs. This value is proportional to the squared signal-to-noise ratio $\\frac{w^2}{\\sigma_x^2}$. When the signal ($w$) is strong relative to the noise ($\\sigma_x$), the posterior distributions become very certain (low variance $s^2$) but centered on very different means ($m_1(x)$ vs. $-m_1(x)$). This leads to a large KL divergence, indicating that the two models offer sharply conflicting, yet equally plausible, interpretations. Resolving this ambiguity would require breaking the model's symmetry, for instance, by incorporating prior biological knowledge that a specific pathway is known to be an activator or inhibitor of the gene in question.", "answer": "$$\n\\boxed{\\frac{2w^{2}}{\\sigma_{x}^{2}}}\n$$", "id": "3357963"}, {"introduction": "While the Evidence Lower Bound (ELBO) is the objective function we maximize during VAE training, it is only a proxy for the true marginal log-likelihood of the data. This exercise [@problem_id:3357955] bridges the gap between training and evaluation, challenging you to implement a more accurate estimator for the log-likelihood using importance sampling. By comparing model performance based on the training ELBO versus the held-out importance-sampled log-likelihood, you will learn a more principled approach to model selection and assessing generalization.", "problem": "Consider a Variational Autoencoder (VAE) for representation learning in computational systems biology, where the latent variable model is used to capture low-dimensional structure in gene expression profiles. Let the generative model be defined by a standard normal prior on the latent variables and a linear Gaussian decoder. Specifically, for any observation vector $x \\in \\mathbb{R}^{d_x}$ and latent vector $z \\in \\mathbb{R}^{d_z}$:\n$$\np(z) = \\mathcal{N}(z; 0, I), \\quad p_{\\theta}(x \\mid z) = \\mathcal{N}\\left(x; W z + b, \\sigma_x^2 I\\right),\n$$\nwhere $W \\in \\mathbb{R}^{d_x \\times d_z}$, $b \\in \\mathbb{R}^{d_x}$, and $\\sigma_x^2 \\in \\mathbb{R}_{>0}$. The variational family is a diagonal Gaussian encoder\n$$\nq_{\\phi}(z \\mid x) = \\mathcal{N}\\left(z; \\mu_{\\phi}(x), \\operatorname{diag}(\\sigma_{\\phi}^2)\\right), \\quad \\mu_{\\phi}(x) = A x + a,\n$$\nwith $A \\in \\mathbb{R}^{d_z \\times d_x}$, $a \\in \\mathbb{R}^{d_z}$, and constant diagonal standard deviations $\\sigma_{\\phi} \\in \\mathbb{R}_{>0}^{d_z}$.\n\nYou are given two candidate models $\\mathcal{M}_1$ and $\\mathcal{M}_2$ with fixed decoders $(W, b, \\sigma_x)$ and encoders $(A, a, \\sigma_{\\phi})$. The task is to derive from first principles and implement a program that computes, for each model:\n- The average training Evidence Lower Bound (ELBO) over a fixed training set.\n- The average held-out log marginal likelihood $\\log p_{\\theta}(x)$ over a fixed validation set, estimated by importance sampling using samples from $q_{\\phi}(z \\mid x)$.\n\nBegin from the following fundamental bases:\n- Bayes’ rule and the definition of marginal likelihood, namely $\\log p_{\\theta}(x) = \\log \\int p_{\\theta}(x, z)\\, dz$.\n- The change-of-measure identity for expectations under absolutely continuous distributions, which justifies importance sampling.\n- The definition of the Evidence Lower Bound (ELBO) as the variational lower bound on the marginal log-likelihood derived via Jensen’s inequality.\n\nYour derivation must proceed by:\n- Expressing $\\log p_{\\theta}(x)$ as an expectation with respect to $q_{\\phi}(z \\mid x)$ that is amenable to Monte Carlo estimation by importance sampling. Do not assume any specific estimator form a priori; derive it from the change-of-measure principle.\n- Deriving a closed-form expression for the ELBO under the linear Gaussian decoder with isotropic observation noise and diagonal Gaussian encoder, by evaluating the expectation of quadratic forms and the Kullback–Leibler divergence between diagonal and standard normal Gaussians.\n\nThen implement the derived estimators to compare the two models for model selection.\n\nUse the following fixed dimensions and parameters, which constitute the test suite:\n\nDimensions:\n- $d_z = 2$, $d_x = 3$.\n\nModel $\\mathcal{M}_1$:\n$$\nW_1 = \\begin{bmatrix}\n1.0 & 0.5 \\\\\n0.0 & 1.0 \\\\\n-0.5 & 0.5\n\\end{bmatrix},\n$$\n$$\nb_1 = \\begin{bmatrix}\n0.1 \\\\ -0.2 \\\\ 0.0\n\\end{bmatrix},\n$$\n$\\sigma_{x,1} = 0.2$.\n\nEncoder for $\\mathcal{M}_1$:\n$$\nA_1 = \\begin{bmatrix}\n0.6 & 0.1 & -0.2 \\\\\n0.0 & 0.5 & 0.3\n\\end{bmatrix},\n$$\n$$\na_1 = \\begin{bmatrix}\n0.0 \\\\ 0.0\n\\end{bmatrix},\n$$\n$$\n\\sigma_{\\phi,1} = \\begin{bmatrix}\n0.6 \\\\ 0.6\n\\end{bmatrix}.\n$$\n\nModel $\\mathcal{M}_2$:\n$$\nW_2 = \\begin{bmatrix}\n0.8 & -0.3 \\\\\n0.2 & 0.9 \\\\\n0.3 & 0.3\n\\end{bmatrix},\n$$\n$$\nb_2 = \\begin{bmatrix}\n-0.1 \\\\ 0.1 \\\\ 0.2\n\\end{bmatrix},\n$$\n$\\sigma_{x,2} = 0.2$.\n\nEncoder for $\\mathcal{M}_2$:\n$$\nA_2 = \\begin{bmatrix}\n0.5 & 0.2 & -0.1 \\\\\n0.1 & 0.4 & 0.2\n\\end{bmatrix},\n$$\n$$\na_2 = \\begin{bmatrix}\n0.1 \\\\ -0.1\n\\end{bmatrix},\n$$\n$$\n\\sigma_{\\phi,2} = \\begin{bmatrix}\n0.7 \\\\ 0.5\n\\end{bmatrix}.\n$$\n\nDatasets:\n- Training set \n$$X_{\\text{train}} = \\left\\{\n\\begin{bmatrix} 1.0 \\\\ 0.5 \\\\ -0.2 \\end{bmatrix},\n\\begin{bmatrix} 0.0 \\\\ -0.5 \\\\ 0.3 \\end{bmatrix},\n\\begin{bmatrix} -1.2 \\\\ 0.7 \\\\ 0.8 \\end{bmatrix}\n\\right\\}.$$\n- Validation set \n$$X_{\\text{val}} = \\left\\{\n\\begin{bmatrix} 0.9 \\\\ 0.4 \\\\ -0.1 \\end{bmatrix},\n\\begin{bmatrix} -0.8 \\\\ 0.6 \\\\ 0.9 \\end{bmatrix}\n\\right\\}.$$\n\nProgram requirements:\n- Compute the average training ELBO for each model using the closed-form expression you derived for the linear Gaussian case.\n- Estimate the average held-out $\\log p_{\\theta}(x)$ on $X_{\\text{val}}$ for each model by importance sampling using $K \\in \\{1, 10, 1000\\}$ samples per validation point, with samples drawn from the corresponding $q_{\\phi}(z \\mid x)$. Use a fixed random seed equal to $12345$ for reproducibility. Implement the estimator in a numerically stable way using the log-sum-exp trick.\n- For model selection, for each criterion (training ELBO, importance sampling with $K = 1$, $K = 10$, and $K = 1000$), choose the model with the higher average score on the corresponding dataset and estimation procedure. Index the models as $1$ for $\\mathcal{M}_1$ and $2$ for $\\mathcal{M}_2$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order $[\\text{model\\_by\\_ELBO}, \\text{model\\_by\\_IS\\_K=1}, \\text{model\\_by\\_IS\\_K=10}, \\text{model\\_by\\_IS\\_K=1000}]$, where each entry is an integer in $\\{1,2\\}$.\n\nAngle units, physical units, and percentages are not applicable to this problem. All computations are dimensionless real-valued quantities. The test suite consists of the specified parameters, datasets, and the three values of $K$, which together exercise the correctness, stability, and convergence behavior of the estimators across small and large sample regimes.", "solution": "The problem statement is a valid, well-posed, and self-contained exercise in computational statistics, specifically concerning the analysis of Variational Autoencoders (VAEs). All required parameters, data, and procedural instructions are provided. We will proceed with the derivation of the necessary mathematical formulae, followed by their implementation to perform the requested model comparison.\n\nThe problem requires the derivation and implementation of two key quantities for a linear-Gaussian VAE: the Evidence Lower Bound (ELBO) and an importance sampling estimator for the marginal log-likelihood, $\\log p_{\\theta}(x)$.\n\nLet the VAE model be defined as follows:\n- Prior on latent variables $z \\in \\mathbb{R}^{d_z}$: $p(z) = \\mathcal{N}(z; 0, I)$.\n- Decoder (generative model) for observations $x \\in \\mathbb{R}^{d_x}$: $p_{\\theta}(x \\mid z) = \\mathcal{N}(x; Wz + b, \\sigma_x^2 I)$.\n- Encoder (variational posterior): $q_{\\phi}(z \\mid x) = \\mathcal{N}(z; \\mu_{\\phi}(x), \\operatorname{diag}(\\sigma_{\\phi}^2))$, where $\\mu_{\\phi}(x) = Ax + a$. The parameters are $\\theta = \\{W, b, \\sigma_x\\}$ and $\\phi = \\{A, a, \\sigma_{\\phi}\\}$. For notational simplicity, we will denote $\\mu_q(x) = \\mu_{\\phi}(x)$, and the covariance matrix of the encoder as $\\Sigma_q = \\operatorname{diag}(\\sigma_{\\phi}^2)$.\n\n### Derivation 1: Importance Sampling Estimator for the Marginal Log-Likelihood\n\nThe marginal likelihood of an observation $x$, denoted $p_{\\theta}(x)$, is defined by integrating out the latent variables from the joint probability distribution:\n$$\np_{\\theta}(x) = \\int p_{\\theta}(x, z) dz = \\int p_{\\theta}(x \\mid z) p(z) dz\n$$\nWe wish to estimate its logarithm, $\\log p_{\\theta}(x)$. Direct Monte Carlo estimation by sampling from the prior $p(z)$ is often inefficient. A more effective method is importance sampling, which is justified by the change-of-measure identity. We introduce the variational posterior $q_{\\phi}(z \\mid x)$ as the proposal distribution.\n\nBy multiplying and dividing the integrand by $q_{\\phi}(z \\mid x)$, we can express $p_{\\theta}(x)$ as an expectation with respect to $q_{\\phi}(z \\mid x)$:\n$$\np_{\\theta}(x) = \\int \\frac{p_{\\theta}(x \\mid z) p(z)}{q_{\\phi}(z \\mid x)} q_{\\phi}(z \\mid x) dz = \\mathbb{E}_{z \\sim q_{\\phi}(z \\mid x)}\\left[ \\frac{p_{\\theta}(x \\mid z) p(z)}{q_{\\phi}(z \\mid x)} \\right]\n$$\nThe term inside the expectation is the importance weight, $w(z) = \\frac{p_{\\theta}(x, z)}{q_{\\phi}(z \\mid x)}$.\nA Monte Carlo estimator for $p_{\\theta}(x)$ is formed by drawing $K$ independent samples $\\{z^{(k)}\\}_{k=1}^K$ from $q_{\\phi}(z \\mid x)$ and computing the sample mean of the weights:\n$$\n\\hat{p}_{\\theta}(x) = \\frac{1}{K} \\sum_{k=1}^K w(z^{(k)}) = \\frac{1}{K} \\sum_{k=1}^K \\frac{p_{\\theta}(x \\mid z^{(k)}) p(z^{(k)})}{q_{\\phi}(z^{(k)} \\mid x)}\n$$\nThe corresponding estimator for the marginal log-likelihood is $\\log \\hat{p}_{\\theta}(x)$. For numerical stability, it is crucial to perform calculations in log-space. Let $\\log w^{(k)}$ be the log-importance-weight for sample $z^{(k)}$:\n$$\n\\log w^{(k)} = \\log p_{\\theta}(x \\mid z^{(k)}) + \\log p(z^{(k)}) - \\log q_{\\phi}(z^{(k)} \\mid x)\n$$\nThe log-likelihood estimate is then:\n$$\n\\log \\hat{p}_{\\theta}(x) = \\log \\left( \\frac{1}{K} \\sum_{k=1}^K e^{\\log w^{(k)}} \\right) = \\log\\left(\\sum_{k=1}^K e^{\\log w^{(k)}}\\right) - \\log K\n$$\nTo prevent numerical underflow or overflow, we utilize the log-sum-exp trick. Let $c = \\max_{k} \\{\\log w^{(k)}\\}$. The expression becomes:\n$$\n\\log \\hat{p}_{\\theta}(x) = c + \\log\\left(\\sum_{k=1}^K e^{\\log w^{(k)} - c}\\right) - \\log K\n$$\nThis is the estimator we shall implement.\n\n### Derivation 2: Closed-Form Expression for the Evidence Lower Bound (ELBO)\n\nThe Evidence Lower Bound, $\\mathcal{L}(\\theta, \\phi; x)$, is derived from the decomposition of the marginal log-likelihood: $\\log p_{\\theta}(x) = \\mathcal{L}(\\theta, \\phi; x) + D_{KL}(q_{\\phi}(z \\mid x) \\| p_{\\theta}(z \\mid x))$. By Jensen's inequality, $\\mathcal{L}(\\theta, \\phi; x) \\leq \\log p_{\\theta}(x)$. The standard expression for the ELBO is:\n$$\n\\mathcal{L}(\\theta, \\phi; x) = \\mathbb{E}_{z \\sim q_{\\phi}(z \\mid x)}[\\log p_{\\theta}(x, z) - \\log q_{\\phi}(z \\mid x)] = \\mathbb{E}_{z \\sim q_{\\phi}(z \\mid x)}[\\log p_{\\theta}(x \\mid z)] - D_{KL}(q_{\\phi}(z \\mid x) \\| p(z))\n$$\nWe will now derive a closed-form expression for each term under the specified linear-Gaussian model.\n\n**First Term: Expected Reconstruction Log-Likelihood**\nThe log-pdf of the decoder is:\n$$\n\\log p_{\\theta}(x \\mid z) = \\log \\mathcal{N}(x; Wz + b, \\sigma_x^2 I) = -\\frac{1}{2\\sigma_x^2}\\|x - (Wz + b)\\|_2^2 - \\frac{d_x}{2}\\log(2\\pi\\sigma_x^2)\n$$\nWe take the expectation with respect to $z \\sim q_{\\phi}(z \\mid x) = \\mathcal{N}(z; \\mu_q(x), \\Sigma_q)$:\n$$\n\\mathbb{E}_{q_{\\phi}}[\\log p_{\\theta}(x \\mid z)] = -\\frac{1}{2\\sigma_x^2} \\mathbb{E}_{q_{\\phi}}[\\|x - b - Wz\\|_2^2] - \\frac{d_x}{2}\\log(2\\pi\\sigma_x^2)\n$$\nThe expectation of the squared norm is expanded using the law of total expectation: $\\mathbb{E}[Y^T Y] = \\mathbb{E}[Y]^T \\mathbb{E}[Y] + \\text{Tr}(\\text{Cov}(Y))$. Let $Y = x - b - Wz$. Then $\\mathbb{E}[Y] = x - b - W\\mathbb{E}[z] = x - b - W\\mu_q(x)$. The covariance is $\\text{Cov}(Y) = \\text{Cov}(Wz) = W \\text{Cov}(z) W^T = W \\Sigma_q W^T$.\nThus, $\\mathbb{E}_{q_{\\phi}}[\\|x - b - Wz\\|_2^2] = \\|x - b - W\\mu_q(x)\\|_2^2 + \\text{Tr}(W \\Sigma_q W^T)$. Using the cyclic property of the trace, $\\text{Tr}(W \\Sigma_q W^T) = \\text{Tr}(W^T W \\Sigma_q)$.\nThe first term of the ELBO is:\n$$\n\\mathbb{E}_{q_{\\phi}}[\\log p_{\\theta}(x \\mid z)] = -\\frac{1}{2\\sigma_x^2}\\left( \\|x - b - W\\mu_q(x)\\|_2^2 + \\text{Tr}(W^T W \\Sigma_q) \\right) - \\frac{d_x}{2}\\log(2\\pi\\sigma_x^2)\n$$\n\n**Second Term: Kullback-Leibler Divergence**\nThe second term is the KL divergence between the variational posterior $q_{\\phi}(z \\mid x) = \\mathcal{N}(z; \\mu_q(x), \\Sigma_q)$ and the standard normal prior $p(z) = \\mathcal{N}(z; 0, I)$. For two multivariate Gaussians $q = \\mathcal{N}(\\mu_1, \\Sigma_1)$ and $p = \\mathcal{N}(\\mu_2, \\Sigma_2)$, the KL divergence is:\n$$\nD_{KL}(q\\|p) = \\frac{1}{2}\\left( \\log\\frac{\\det\\Sigma_2}{\\det\\Sigma_1} - d_z + \\text{Tr}(\\Sigma_2^{-1}\\Sigma_1) + (\\mu_2 - \\mu_1)^T\\Sigma_2^{-1}(\\mu_2 - \\mu_1) \\right)\n$$\nSubstituting our parameters ($\\mu_1 = \\mu_q(x)$, $\\Sigma_1 = \\Sigma_q = \\operatorname{diag}(\\sigma_{\\phi}^2)$, $\\mu_2 = 0$, $\\Sigma_2 = I$), we get:\n$$\nD_{KL}(q_{\\phi}\\|p) = \\frac{1}{2}\\left( \\log\\frac{1}{\\det\\Sigma_q} - d_z + \\text{Tr}(\\Sigma_q) + \\mu_q(x)^T \\mu_q(x) \\right)\n$$\nSince $\\Sigma_q$ is diagonal, $\\det\\Sigma_q = \\prod_{j=1}^{d_z} \\sigma_{\\phi,j}^2$ and $\\text{Tr}(\\Sigma_q) = \\sum_{j=1}^{d_z} \\sigma_{\\phi,j}^2$.\n$$\nD_{KL}(q_{\\phi}\\|p) = \\frac{1}{2}\\left( -\\sum_{j=1}^{d_z}\\log(\\sigma_{\\phi,j}^2) - d_z + \\sum_{j=1}^{d_z}\\sigma_{\\phi,j}^2 + \\|\\mu_q(x)\\|_2^2 \\right)\n$$\nRe-arranging terms on a per-dimension basis gives the familiar form:\n$$\nD_{KL}(q_{\\phi}\\|p) = \\frac{1}{2} \\sum_{j=1}^{d_z} \\left( \\mu_{q,j}(x)^2 + \\sigma_{\\phi,j}^2 - \\log(\\sigma_{\\phi,j}^2) - 1 \\right)\n$$\n\n**Final ELBO Expression**\nCombining the two terms, we obtain the final closed-form expression for the ELBO:\n$$\n\\mathcal{L}(x) = -\\frac{1}{2\\sigma_x^2}\\left( \\|x - b - W\\mu_q(x)\\|_2^2 + \\text{Tr}(W^T W \\Sigma_q) \\right) - \\frac{d_x}{2}\\log(2\\pi\\sigma_x^2) - \\frac{1}{2} \\sum_{j=1}^{d_z} \\left( \\mu_{q,j}(x)^2 + \\sigma_{\\phi,j}^2 - \\log(\\sigma_{\\phi,j}^2) - 1 \\right)\n$$\nwhere $\\mu_q(x) = Ax+a$ and $\\Sigma_q = \\operatorname{diag}(\\sigma_{\\phi}^2)$. This expression will be implemented to compute the average ELBO on the training set.\n\n### Computational Procedure\n\n1.  We will implement a class to represent each model, encapsulating its specific parameters.\n2.  A method for this class will compute the ELBO for a given data point $x$ using the derived closed-form formula.\n3.  A second method will estimate the marginal log-likelihood $\\log p_{\\theta}(x)$ using the derived importance sampling estimator. This method will require helper functions to compute the log-pdfs of the prior, decoder, and encoder distributions. Samples will be drawn from the encoder using a fixed random seed for reproducibility.\n4.  For model selection, we will iterate through the four specified criteria:\n    a.  Average training ELBO: We compute the mean of $\\mathcal{L}(x)$ for each model over all $x \\in X_{\\text{train}}$. The model with the higher value is chosen.\n    b.  Average held-out log-likelihood: For each $K \\in \\{1, 10, 1000\\}$, we compute the mean of $\\log \\hat{p}_{\\theta}(x)$ for each model over all $x \\in X_{\\text{val}}$. The model with the higher value is chosen for each $K$.\n5.  The final output will be a list of model indices ($1$ or $2$) corresponding to the winning model for each of the four criteria.", "answer": "```python\nimport numpy as np\n\ndef log_pdf_multivariate_normal_diag_cov(y, mu, sigma_sq, d):\n    \"\"\"\n    Computes the log PDF of a multivariate normal with a diagonal covariance matrix.\n    log N(y; mu, diag(sigma_sq))\n    \"\"\"\n    log_prob = -0.5 * np.sum((y - mu)**2 / sigma_sq + np.log(2 * np.pi * sigma_sq))\n    return log_prob\n\ndef log_pdf_multivariate_normal_iso_cov(y, mu, sigma_sq, d):\n    \"\"\"\n    Computes the log PDF of a multivariate normal with an isotropic covariance matrix.\n    log N(y; mu, sigma_sq * I)\n    \"\"\"\n    log_prob = -0.5 * (np.sum((y - mu)**2) / sigma_sq + d * np.log(2 * np.pi * sigma_sq))\n    return log_prob\n\n\nclass LinearGaussianVAE:\n    def __init__(self, W, b, sigma_x, A, a, sigma_phi, dz, dx):\n        self.W = W\n        self.b = b\n        self.sigma_x = sigma_x\n        self.sigma_x_sq = sigma_x**2\n        self.A = A\n        self.a = a\n        self.sigma_phi = sigma_phi\n        self.sigma_phi_sq = sigma_phi**2\n        self.dz = dz\n        self.dx = dx\n        self.Sigma_q = np.diag(self.sigma_phi_sq)\n\n    def compute_elbo(self, x):\n        \"\"\"Computes the closed-form ELBO for a single data point x.\"\"\"\n        mu_q = self.A @ x + self.a\n        mu_q_sq = mu_q**2\n\n        # 1. Expected reconstruction log-likelihood term\n        recon_error = x - self.b - self.W @ mu_q\n        recon_term_val = np.sum(recon_error**2)\n        trace_term = np.trace(self.W.T @ self.W @ self.Sigma_q)\n        \n        log_p_x_given_z_exp = -0.5 / self.sigma_x_sq * (recon_term_val + trace_term) - \\\n                               (self.dx / 2) * np.log(2 * np.pi * self.sigma_x_sq)\n\n        # 2. KL divergence term\n        kl_term = 0.5 * np.sum(mu_q_sq + self.sigma_phi_sq - np.log(self.sigma_phi_sq) - 1)\n        \n        elbo = log_p_x_given_z_exp - kl_term\n        return elbo\n\n    def estimate_log_likelihood_is(self, x, K, rng):\n        \"\"\"Estimates log p(x) using importance sampling with K samples.\"\"\"\n        mu_q = self.A @ x + self.a\n        \n        # Sample z from q(z|x)\n        z_samples = rng.normal(loc=mu_q, scale=self.sigma_phi, size=(K, self.dz))\n        \n        log_weights = np.zeros(K)\n        for k in range(K):\n            z_k = z_samples[k]\n            \n            # log p(z)\n            log_p_z = log_pdf_multivariate_normal_iso_cov(z_k, np.zeros(self.dz), 1.0, self.dz)\n            \n            # log p(x|z)\n            recon_mean = self.W @ z_k + self.b\n            log_p_x_given_z = log_pdf_multivariate_normal_iso_cov(x, recon_mean, self.sigma_x_sq, self.dx)\n            \n            # log q(z|x)\n            log_q_z_given_x = log_pdf_multivariate_normal_diag_cov(z_k, mu_q, self.sigma_phi_sq, self.dz)\n            \n            log_weights[k] = log_p_x_given_z + log_p_z - log_q_z_given_x\n\n        # Log-sum-exp for stability\n        if K == 0:\n            return -np.inf\n        c = np.max(log_weights)\n        log_sum_exp = c + np.log(np.sum(np.exp(log_weights - c)))\n        \n        log_p_hat = log_sum_exp - np.log(K)\n        return log_p_hat\n\n\ndef solve():\n    # --- Givens ---\n    dz = 2\n    dx = 3\n\n    # Model M1 parameters\n    W1 = np.array([[1.0, 0.5], [0.0, 1.0], [-0.5, 0.5]])\n    b1 = np.array([0.1, -0.2, 0.0])\n    sigma_x1 = 0.2\n    A1 = np.array([[0.6, 0.1, -0.2], [0.0, 0.5, 0.3]])\n    a1 = np.array([0.0, 0.0])\n    sigma_phi1 = np.array([0.6, 0.6])\n\n    # Model M2 parameters\n    W2 = np.array([[0.8, -0.3], [0.2, 0.9], [0.3, 0.3]])\n    b2 = np.array([-0.1, 0.1, 0.2])\n    sigma_x2 = 0.2\n    A2 = np.array([[0.5, 0.2, -0.1], [0.1, 0.4, 0.2]])\n    a2 = np.array([0.1, -0.1])\n    sigma_phi2 = np.array([0.7, 0.5])\n\n    # Datasets\n    X_train = np.array([\n        [1.0, 0.5, -0.2],\n        [0.0, -0.5, 0.3],\n        [-1.2, 0.7, 0.8]\n    ])\n    X_val = np.array([\n        [0.9, 0.4, -0.1],\n        [-0.8, 0.6, 0.9]\n    ])\n    \n    # Import sampling parameters\n    K_values = [1, 10, 1000]\n    seed = 12345\n    rng = np.random.default_rng(seed)\n\n    # Initialize models\n    model1 = LinearGaussianVAE(W1, b1, sigma_x1, A1, a1, sigma_phi1, dz, dx)\n    model2 = LinearGaussianVAE(W2, b2, sigma_x2, A2, a2, sigma_phi2, dz, dx)\n    \n    results = []\n\n    # --- 1. Model selection by average training ELBO ---\n    avg_elbo1 = np.mean([model1.compute_elbo(x) for x in X_train])\n    avg_elbo2 = np.mean([model2.compute_elbo(x) for x in X_train])\n    results.append(1 if avg_elbo1 > avg_elbo2 else 2)\n\n    # --- 2. Model selection by average held-out log-likelihood (IS) ---\n    for K in K_values:\n        avg_loglik1 = np.mean([model1.estimate_log_likelihood_is(x, K, rng) for x in X_val])\n        avg_loglik2 = np.mean([model2.estimate_log_likelihood_is(x, K, rng) for x in X_val])\n        results.append(1 if avg_loglik1 > avg_loglik2 else 2)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3357955"}, {"introduction": "A frequent challenge in training VAEs is 'posterior collapse,' where the model learns to ignore the latent variables, rendering the representation uninformative. This practice [@problem_id:3357966] moves from theory to troubleshooting by equipping you with a quantitative diagnostic to detect collapse on a per-dimension basis. You will analyze encoder outputs to identify which latent dimensions are inactive and devise a targeted intervention using a dimension-wise $\\beta$-VAE objective to encourage the model to use its full representational capacity.", "problem": "You are studying a variational autoencoder (VAE) for representation learning on single-cell transcriptomes in computational systems biology. The latent variable model uses a factorized approximate posterior and an isotropic standard normal prior. Specifically, for each input $x$, the encoder outputs parameters of a diagonal Gaussian approximate posterior $q_{\\phi}(z \\mid x)$ with $q_{\\phi}(z \\mid x) = \\mathcal{N}\\!\\left(\\mu(x), \\mathrm{diag}\\left(\\sigma^{2}(x)\\right)\\right)$, where $z \\in \\mathbb{R}^{d}$ has coordinates $z_{j}$ with $q_{\\phi}(z_{j} \\mid x) = \\mathcal{N}\\!\\left(\\mu_{j}(x), \\sigma_{j}^{2}(x)\\right)$, and the prior is $p(z) = \\mathcal{N}(0, I)$. Here $\\phi$ denotes encoder parameters, $\\mu(x) \\in \\mathbb{R}^{d}$, and $\\sigma^{2}(x) \\in \\mathbb{R}_{+}^{d}$.\n\nYour goal is to construct a principled, per-dimension Kullback–Leibler (KL) diagnostic to detect posterior collapse and to propose a targeted training intervention. You will do this by deriving and computing the per-dimension KL divergence $\\mathrm{KL}\\!\\left(q_{\\phi}(z_{j} \\mid x) \\,\\|\\, p(z_{j})\\right)$, aggregating it across a dataset, and then suggesting a dimension-wise weighting for the KL term to counteract collapse.\n\nFundamental definitions to use as the base:\n- The Evidence Lower BOund (ELBO) for a VAE is, for data point $x$, given by $\\mathcal{L}(x) = \\mathbb{E}_{q_{\\phi}(z \\mid x)}[\\log p_{\\theta}(x \\mid z)] - \\mathrm{KL}\\!\\left(q_{\\phi}(z \\mid x) \\,\\|\\, p(z)\\right)$.\n- The Kullback–Leibler divergence between continuous distributions $q$ and $p$ is $\\mathrm{KL}(q \\,\\|\\, p) = \\int q(u)\\,\\log\\frac{q(u)}{p(u)}\\,du$.\n- For independent coordinates, $\\mathrm{KL}\\!\\left(q_{\\phi}(z \\mid x) \\,\\|\\, p(z)\\right)$ decomposes additively as $\\sum_{j=1}^{d} \\mathrm{KL}\\!\\left(q_{\\phi}(z_{j} \\mid x) \\,\\|\\, p(z_{j})\\right)$.\n\nTasks:\n1. Derive from first principles the closed-form expression for the univariate Gaussian KL divergence $\\mathrm{KL}\\!\\left(\\mathcal{N}(\\mu_{j}, \\sigma_{j}^{2}) \\,\\|\\, \\mathcal{N}(0, 1)\\right)$ in terms of $\\mu_{j}$ and $\\sigma_{j}^{2}$, and then specialize it to the encoder outputs $\\mu_{j}(x)$ and $\\log \\sigma_{j}^{2}(x)$.\n2. Given a dataset $\\mathcal{D} = \\{x_{i}\\}_{i=1}^{n}$ with encoder outputs $\\mu(x_{i})$ and $\\log \\sigma^{2}(x_{i})$ for each $x_{i}$, compute the per-dimension, dataset-averaged KL values $\\overline{\\mathrm{KL}}_{j} = \\frac{1}{n}\\sum_{i=1}^{n} \\mathrm{KL}\\!\\left(q_{\\phi}(z_{j} \\mid x_{i}) \\,\\|\\, p(z_{j})\\right)$.\n3. Define a dimension $j$ to be collapsed if $\\overline{\\mathrm{KL}}_{j} < \\varepsilon$, where $\\varepsilon$ is a user-specified nonnegative threshold in nats. Detect and report collapsed dimensions.\n4. Propose a targeted training intervention via a dimension-wise weighting $\\beta_{j}$ for a $\\beta$-VAE style objective $\\mathcal{L}_{\\text{weighted}}(x) = \\mathbb{E}_{q_{\\phi}(z \\mid x)}[\\log p_{\\theta}(x \\mid z)] - \\sum_{j=1}^{d} \\beta_{j}\\,\\mathrm{KL}\\!\\left(q_{\\phi}(z_{j} \\mid x) \\,\\|\\, p(z_{j})\\right)$ that aims to increase information usage in collapsed dimensions without destabilizing training. Use the following rule to set $\\beta_{j}$ from the diagnostic:\n   - Choose a per-dimension target information level $\\tau$ (in nats). Set $\\beta_{j} = \\mathrm{clip}\\!\\left(\\frac{\\overline{\\mathrm{KL}}_{j}}{\\tau}, \\beta_{\\min}, 1\\right)$, where $\\mathrm{clip}(u, a, b)$ returns $a$ if $u < a$, $b$ if $u > b$, and $u$ otherwise, and $\\beta_{\\min} \\in (0, 1]$ is a floor to avoid zero weight. This reduces the KL penalty (smaller $\\beta_{j}$) for dimensions with $\\overline{\\mathrm{KL}}_{j} < \\tau$, thereby encouraging them to carry more information, while keeping $\\beta_{j} \\le 1$ to avoid over-penalizing informative dimensions.\n\nYour program must implement the above steps for the following test suite. Each test case provides the encoder outputs as matrices of $\\mu$ and $\\log \\sigma^{2}$ values across $n$ samples and $d$ latent dimensions, along with thresholds $(\\varepsilon, \\tau, \\beta_{\\min})$.\n\nTest Suite:\n- Case $1$:\n  - $n = 4$, $d = 3$.\n  - $$\\mu = \\begin{bmatrix}\n  0.0 & 0.5 & 1.0 \\\\\n  0.0 & 0.4 & 1.2 \\\\\n  0.0 & 0.6 & 0.9 \\\\\n  0.0 & 0.45 & 1.1\n  \\end{bmatrix}$$,\n    $$\\log \\sigma^{2} = \\begin{bmatrix}\n  0.0 & -0.2 & 0.3 \\\\\n  0.0 & -0.2 & 0.3 \\\\\n  0.0 & -0.2 & 0.3 \\\\\n  0.0 & -0.2 & 0.3\n  \\end{bmatrix}$$.\n  - $\\varepsilon = 0.05$, $\\tau = 0.2$, $\\beta_{\\min} = 0.1$.\n- Case $2$ (boundary condition):\n  - $n = 2$, $d = 2$.\n  - $$\\mu = \\begin{bmatrix}\n  0.4472136 & 0.0 \\\\\n  0.4472136 & 0.0\n  \\end{bmatrix}$$,\n    $$\\log \\sigma^{2} = \\begin{bmatrix}\n  0.0 & 0.0 \\\\\n  0.0 & 0.0\n  \\end{bmatrix}$$.\n  - $\\varepsilon = 0.1$, $\\tau = 0.15$, $\\beta_{\\min} = 0.1$.\n- Case $3$ (heteroscedastic extremes):\n  - $n = 3$, $d = 4$.\n  - $$\\mu = \\begin{bmatrix}\n  0.0 & 0.3 & 0.2 & 0.0 \\\\\n  0.0 & 0.2 & 0.25 & 0.0 \\\\\n  0.0 & 0.4 & 0.15 & 0.0\n  \\end{bmatrix}$$,\n    $$\\log \\sigma^{2} = \\begin{bmatrix}\n  0.0 & -0.1 & -3.0 & 1.5 \\\\\n  0.0 & -0.1 & -3.0 & 1.5 \\\\\n  0.0 & -0.1 & -3.0 & 1.5\n  \\end{bmatrix}$$.\n  - $\\varepsilon = 0.02$, $\\tau = 0.5$, $\\beta_{\\min} = 0.2$.\n- Case $4$ (all dimensions informative):\n  - $n = 3$, $d = 2$.\n  - $$\\mu = \\begin{bmatrix}\n  0.4 & 0.7 \\\\\n  0.35 & 0.6 \\\\\n  0.45 & 0.65\n  \\endbmatrix}$$,\n    $$\\log \\sigma^{2} = \\begin{bmatrix}\n  -0.2 & -0.1 \\\\\n  -0.2 & -0.1 \\\\\n  -0.2 & -0.1\n  \\endbmatrix}$$.\n  - $\\varepsilon = 0.05$, $\\tau = 0.2$, $\\beta_{\\min} = 0.1$.\n\nRequired computations and outputs for each case:\n- Compute the per-sample, per-dimension KL values using your derived closed form.\n- Compute the dataset-averaged per-dimension KL values $\\overline{\\mathrm{KL}}_{j}$.\n- Determine the set of collapsed indices $\\{j : \\overline{\\mathrm{KL}}_{j} < \\varepsilon\\}$, using strict inequality.\n- Compute the recommended $\\beta_{j}$ using $\\beta_{j} = \\mathrm{clip}\\!\\left(\\frac{\\overline{\\mathrm{KL}}_{j}}{\\tau}, \\beta_{\\min}, 1\\right)$.\n- Compute the average total KL across the dataset, defined as $\\overline{\\mathrm{KL}}_{\\text{total}} = \\sum_{j=1}^{d} \\overline{\\mathrm{KL}}_{j}$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case and is itself a list with the following elements:\n  - the integer count of collapsed dimensions,\n  - the float $\\overline{\\mathrm{KL}}_{\\text{total}}$ rounded to $6$ decimal places,\n  - the list of collapsed indices (integers),\n  - the list of recommended $\\beta_{j}$ values rounded to $4$ decimal places and presented in dimension order.\n- For example, a single test case might yield an output fragment like $[1,0.123456,[0],[0.5,1.0,0.8]]$. The final output must be the list across all four test cases, e.g., $[[\\ldots],[\\ldots],[\\ldots],[\\ldots]]$.", "solution": "The problem requires the derivation and application of a per-dimension Kullback–Leibler (KL) divergence diagnostic for a variational autoencoder (VAE) to detect and counteract posterior collapse. The VAE model is specified with a factorized diagonal Gaussian approximate posterior $q_{\\phi}(z \\mid x) = \\mathcal{N}(\\mu(x), \\mathrm{diag}(\\sigma^{2}(x)))$ and a standard normal prior $p(z) = \\mathcal{N}(0, I)$.\n\nThe solution proceeds in two main parts. First, we derive the closed-form expression for the KL divergence between the univariate posterior and prior for a single latent dimension. Second, we establish the algorithm to compute the required diagnostics and intervention weights for a given dataset of encoder outputs.\n\n**1. Derivation of the Per-Dimension KL Divergence**\n\nThe primary task is to derive the closed-form expression for the KL divergence between the approximate posterior $q_{\\phi}(z_j \\mid x) = \\mathcal{N}(z_j; \\mu_j(x), \\sigma_j^2(x))$ and the prior $p(z_j) = \\mathcal{N}(z_j; 0, 1)$ for a single latent dimension $j$. For simplicity of notation, we will drop the dependencies on $x$ and $\\phi$ and denote the distributions as $q(z_j) = \\mathcal{N}(\\mu_j, \\sigma_j^2)$ and $p(z_j) = \\mathcal{N}(0, 1)$.\n\nThe KL divergence is defined as:\n$$\n\\mathrm{KL}(q(z_j) \\,\\|\\, p(z_j)) = \\int_{-\\infty}^{\\infty} q(z_j) \\log \\frac{q(z_j)}{p(z_j)} dz_j\n$$\nThis can be expanded into two terms involving the expectation with respect to $q(z_j)$:\n$$\n\\mathrm{KL}(q(z_j) \\,\\|\\, p(z_j)) = \\mathbb{E}_{q(z_j)}[\\log q(z_j)] - \\mathbb{E}_{q(z_j)}[\\log p(z_j)]\n$$\nThis is the negative of the differential entropy of $q(z_j)$, denoted $-H(q(z_j))$, minus the cross-entropy between $q(z_j)$ and $p(z_j)$.\n\nThe probability density functions (PDFs) for $q(z_j)$ and $p(z_j)$ are:\n$$\nq(z_j) = \\frac{1}{\\sqrt{2\\pi\\sigma_j^2}} \\exp\\left(-\\frac{(z_j - \\mu_j)^2}{2\\sigma_j^2}\\right)\n$$\n$$\np(z_j) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{z_j^2}{2}\\right)\n$$\nTheir logarithms are:\n$$\n\\log q(z_j) = -\\frac{(z_j - \\mu_j)^2}{2\\sigma_j^2} - \\frac{1}{2}\\log(2\\pi\\sigma_j^2)\n$$\n$$\n\\log p(z_j) = -\\frac{z_j^2}{2} - \\frac{1}{2}\\log(2\\pi)\n$$\n\nNow we compute the two expectation terms.\n\nThe first term, $\\mathbb{E}_{q(z_j)}[\\log q(z_j)]$, is related to the entropy of a Gaussian.\n$$\n\\mathbb{E}_{q(z_j)}[\\log q(z_j)] = \\mathbb{E}_{q(z_j)}\\left[-\\frac{(z_j - \\mu_j)^2}{2\\sigma_j^2} - \\frac{1}{2}\\log(2\\pi\\sigma_j^2)\\right]\n$$\n$$\n= -\\frac{1}{2\\sigma_j^2} \\mathbb{E}_{q(z_j)}[(z_j - \\mu_j)^2] - \\frac{1}{2}\\log(2\\pi\\sigma_j^2)\n$$\nBy definition, $\\mathbb{E}_{q(z_j)}[(z_j - \\mu_j)^2]$ is the variance of $q(z_j)$, which is $\\sigma_j^2$.\n$$\n\\mathbb{E}_{q(z_j)}[\\log q(z_j)] = -\\frac{\\sigma_j^2}{2\\sigma_j^2} - \\frac{1}{2}\\log(2\\pi\\sigma_j^2) = -\\frac{1}{2} - \\frac{1}{2}\\log(2\\pi\\sigma_j^2)\n$$\n\nThe second term is the expectation of $\\log p(z_j)$:\n$$\n\\mathbb{E}_{q(z_j)}[\\log p(z_j)] = \\mathbb{E}_{q(z_j)}\\left[-\\frac{z_j^2}{2} - \\frac{1}{2}\\log(2\\pi)\\right]\n$$\n$$\n= -\\frac{1}{2} \\mathbb{E}_{q(z_j)}[z_j^2] - \\frac{1}{2}\\log(2\\pi)\n$$\nFor a random variable $Z_j \\sim \\mathcal{N}(\\mu_j, \\sigma_j^2)$, we have $\\mathrm{Var}(Z_j) = \\mathbb{E}[Z_j^2] - (\\mathbb{E}[Z_j])^2$. Thus, $\\mathbb{E}[Z_j^2] = \\mathrm{Var}(Z_j) + (\\mathbb{E}[Z_j])^2 = \\sigma_j^2 + \\mu_j^2$.\n$$\n\\mathbb{E}_{q(z_j)}[\\log p(z_j)] = -\\frac{1}{2}(\\sigma_j^2 + \\mu_j^2) - \\frac{1}{2}\\log(2\\pi)\n$$\n\nCombining the terms to find the KL divergence:\n$$\n\\mathrm{KL}(q(z_j) \\,\\|\\, p(z_j)) = \\left(-\\frac{1}{2} - \\frac{1}{2}\\log(2\\pi\\sigma_j^2)\\right) - \\left(-\\frac{1}{2}(\\sigma_j^2 + \\mu_j^2) - \\frac{1}{2}\\log(2\\pi)\\right)\n$$\n$$\n= -\\frac{1}{2} - \\frac{1}{2}\\log(2\\pi) - \\frac{1}{2}\\log(\\sigma_j^2) + \\frac{1}{2}\\sigma_j^2 + \\frac{1}{2}\\mu_j^2 + \\frac{1}{2}\\log(2\\pi)\n$$\n$$\n= \\frac{1}{2} \\left( \\mu_j^2 + \\sigma_j^2 - \\log(\\sigma_j^2) - 1 \\right)\n$$\nThis is the closed-form expression for the KL divergence between a univariate Gaussian $\\mathcal{N}(\\mu_j, \\sigma_j^2)$ and the standard normal $\\mathcal{N}(0, 1)$.\n\nThe problem specifies that the VAE encoder outputs $\\mu_j(x)$ and $\\log \\sigma_j^2(x)$. Let us denote $v_j(x) = \\log \\sigma_j^2(x)$, which implies $\\sigma_j^2(x) = \\exp(v_j(x))$. Substituting these into our derived formula gives the final expression for computation:\n$$\n\\mathrm{KL}(q_{\\phi}(z_j \\mid x) \\,\\|\\, p(z_j)) = \\frac{1}{2}\\left( \\mu_j(x)^2 + \\exp(v_j(x)) - v_j(x) - 1 \\right)\n$$\n\n**2. Algorithmic Procedure for Diagnostics and Intervention**\n\nGiven a dataset $\\mathcal{D} = \\{x_i\\}_{i=1}^{n}$ and the corresponding encoder outputs $(\\mu(x_i), \\log \\sigma^2(x_i))$ for each data point, we perform the following sequence of computations for each test case.\n\n1.  **Compute Per-Sample, Per-Dimension KL Divergence**: For each sample $x_i$ and each latent dimension $j \\in \\{1, \\dots, d\\}$, we compute the KL divergence, let's call it $\\mathrm{KL}_{ij}$, using the derived formula:\n    $$\n    \\mathrm{KL}_{ij} = \\frac{1}{2}\\left( \\mu_j(x_i)^2 + \\exp(\\log \\sigma_j^2(x_i)) - \\log \\sigma_j^2(x_i) - 1 \\right)\n    $$\n2.  **Compute Dataset-Averaged Per-Dimension KL Divergence**: For each dimension $j$, we average the KL values across all $n$ samples in the dataset:\n    $$\n    \\overline{\\mathrm{KL}}_{j} = \\frac{1}{n} \\sum_{i=1}^{n} \\mathrm{KL}_{ij}\n    $$\n    This value, $\\overline{\\mathrm{KL}}_{j}$, quantifies the average amount of information (in nats) that dimension $j$ is forced to encode relative to the prior.\n3.  **Detect Collapsed Dimensions**: A dimension $j$ is identified as \"collapsed\" if its average KL divergence is close to zero, meaning the posterior $q_{\\phi}(z_j \\mid x)$ is almost indistinguishable from the prior $p(z_j)$, and thus the dimension is not being used to encode information about the data. We use the given threshold $\\varepsilon$ to formalize this: dimension $j$ is collapsed if $\\overline{\\mathrm{KL}}_{j} < \\varepsilon$. We will report the count and the indices of all such dimensions. Indices are $0$-based.\n4.  **Propose Targeted Intervention**: To counteract collapse, a modified $\\beta$-VAE objective is proposed with dimension-specific weights $\\beta_j$. The weights are designed to reduce the KL penalty on collapsed or near-collapsed dimensions, encouraging the model to use them. The rule is:\n    $$\n    \\beta_j = \\mathrm{clip}\\left(\\frac{\\overline{\\mathrm{KL}}_{j}}{\\tau}, \\beta_{\\min}, 1\\right)\n    $$\n    where $\\tau$ is a target information level, $\\beta_{\\min}$ is a minimum weight, and $\\mathrm{clip}(u, a, b)$ clamps the value $u$ to the range $[a, b]$. This will be computed for all dimensions $j \\in \\{1, \\dots, d\\}$.\n5.  **Compute Average Total KL Divergence**: The total KL penalty, averaged over the dataset, is the sum of the per-dimension average KLs:\n    $$\n    \\overline{\\mathrm{KL}}_{\\text{total}} = \\sum_{j=1}^{d} \\overline{\\mathrm{KL}}_{j}\n    $$\nThese steps are implemented in the provided program to analyze the test cases.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the VAE posterior collapse diagnostic problem for a given test suite.\n    \"\"\"\n    test_cases = [\n        {\n            \"mu\": np.array([\n                [0.0, 0.5, 1.0],\n                [0.0, 0.4, 1.2],\n                [0.0, 0.6, 0.9],\n                [0.0, 0.45, 1.1]\n            ]),\n            \"log_var\": np.array([\n                [0.0, -0.2, 0.3],\n                [0.0, -0.2, 0.3],\n                [0.0, -0.2, 0.3],\n                [0.0, -0.2, 0.3]\n            ]),\n            \"params\": (0.05, 0.2, 0.1)  # (epsilon, tau, beta_min)\n        },\n        {\n            \"mu\": np.array([\n                [0.4472136, 0.0],\n                [0.4472136, 0.0]\n            ]),\n            \"log_var\": np.array([\n                [0.0, 0.0],\n                [0.0, 0.0]\n            ]),\n            \"params\": (0.1, 0.15, 0.1)\n        },\n        {\n            \"mu\": np.array([\n                [0.0, 0.3, 0.2, 0.0],\n                [0.0, 0.2, 0.25, 0.0],\n                [0.0, 0.4, 0.15, 0.0]\n            ]),\n            \"log_var\": np.array([\n                [0.0, -0.1, -3.0, 1.5],\n                [0.0, -0.1, -3.0, 1.5],\n                [0.0, -0.1, -3.0, 1.5]\n            ]),\n            \"params\": (0.02, 0.5, 0.2)\n        },\n        {\n            \"mu\": np.array([\n                [0.4, 0.7],\n                [0.35, 0.6],\n                [0.45, 0.65]\n            ]),\n            \"log_var\": np.array([\n                [-0.2, -0.1],\n                [-0.2, -0.1],\n                [-0.2, -0.1]\n            ]),\n            \"params\": (0.05, 0.2, 0.1)\n        }\n    ]\n\n    all_results_str = []\n\n    for case in test_cases:\n        mu_matrix = case[\"mu\"]\n        log_var_matrix = case[\"log_var\"]\n        epsilon, tau, beta_min = case[\"params\"]\n\n        # 1. Compute per-sample, per-dimension KL divergence\n        # KL = 0.5 * (mu^2 + exp(log_var) - log_var - 1)\n        var_matrix = np.exp(log_var_matrix)\n        kl_matrix = 0.5 * (np.square(mu_matrix) + var_matrix - log_var_matrix - 1)\n\n        # 2. Compute dataset-averaged per-dimension KL divergence\n        avg_kl_per_dim = np.mean(kl_matrix, axis=0)\n\n        # 3. Detect collapsed dimensions\n        collapsed_indices = np.where(avg_kl_per_dim < epsilon)[0].tolist()\n        num_collapsed = len(collapsed_indices)\n\n        # 4. Propose targeted intervention\n        # beta_j = clip(avg_kl_j / tau, beta_min, 1)\n        beta_ratios = avg_kl_per_dim / tau\n        beta_values = np.clip(beta_ratios, beta_min, 1.0).tolist()\n        \n        # 5. Compute average total KL divergence\n        total_avg_kl = np.sum(avg_kl_per_dim)\n\n        # Format the output string for the current case\n        s_c = str(num_collapsed)\n        s_t = f\"{total_avg_kl:.6f}\"\n        s_i = f\"[{','.join(map(str, collapsed_indices))}]\"\n        s_b = f\"[{','.join([f'{b:.4f}' for b in beta_values])}]\"\n        \n        case_str = f\"[{s_c},{s_t},{s_i},{s_b}]\"\n        all_results_str.append(case_str)\n        \n    # Final print statement in the exact required format\n    print(f\"[{','.join(all_results_str)}]\")\n\nsolve()\n```", "id": "3357966"}]}