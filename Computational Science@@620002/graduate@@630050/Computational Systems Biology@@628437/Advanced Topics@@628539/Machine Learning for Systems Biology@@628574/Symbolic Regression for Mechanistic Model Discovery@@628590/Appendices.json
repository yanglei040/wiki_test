{"hands_on_practices": [{"introduction": "The foundation of symbolic regression is the library of candidate functions we provide to the algorithm. If this library contains redundant or highly similar terms, it becomes mathematically difficult or impossible to uniquely identify the correct model structure. This exercise [@problem_id:3353792] provides a hands-on guide to diagnosing and resolving this critical issue of identifiability using standard metrics like mutual coherence and the condition number of the Gram matrix.", "problem": "Sparse Identification of Nonlinear Dynamics (SINDy) seeks parsimonious mechanistic models by regressing measured time derivatives onto a library of candidate basis functions. Consider a single-state system with state $x(t)$ measured at $N=5$ time points, with observed values $x=\\{0,1,2,3,4\\}$. A candidate library is constructed from three basis functions $\\phi_1(x)=1$, $\\phi_2(x)=x$, and $\\phi_3(x)=1+0.99\\,x$, producing the $N\\times 3$ library matrix $\\Theta=[\\theta_1\\,\\,\\theta_2\\,\\,\\theta_3]$ whose columns are the evaluations\n$\\theta_1=[1,1,1,1,1]^\\top$, $\\theta_2=[0,1,2,3,4]^\\top$, and $\\theta_3=[1.00,1.99,2.98,3.97,4.96]^\\top$.\nAssume we normalize columns by their Euclidean ($\\ell_2$) norms prior to regression.\n\nTwo standard measures of identifiability in sparse regression are:\n- Mutual coherence $\\mu(\\Theta)$, defined for a normalized library as $\\mu(\\Theta)=\\max_{i\\neq j}\\left|\\langle \\theta_i,\\theta_j\\rangle\\right|$, where $\\langle \\cdot,\\cdot\\rangle$ denotes the Euclidean inner product. High $\\mu(\\Theta)$ increases the risk of non-unique sparse solutions.\n- The condition number $\\kappa(G)$ of the Gram matrix $G=\\Theta^\\top\\Theta$, defined as $\\kappa(G)=\\lambda_{\\max}(G)/\\lambda_{\\min}(G)$, where $\\lambda_{\\max}$ and $\\lambda_{\\min}$ are the largest and smallest eigenvalues. Large $\\kappa(G)$ signals ill-conditioning and sensitivity to noise.\n\nYou are told that, due to experimental design and noise, your identifiability control policy is to enforce $\\mu(\\Theta)\\le \\tau$ with threshold $\\tau=0.95$ and $\\kappa(G)\\le \\kappa_{\\max}$ with $\\kappa_{\\max}=50$ before applying Least Absolute Shrinkage and Selection Operator (LASSO) for sparse estimation. You also adopt a mechanistic interpretability prior: when breaking up highly coherent pairs, prefer to retain elemental basis functions (e.g., $\\phi_1(x)=1$ and $\\phi_2(x)=x$) over composite mixtures (e.g., $\\phi_3(x)=1+0.99\\,x$), unless doing so violates the identifiability policy.\n\nBased on first principles and the provided data, analyze the identifiability implications of including $\\phi_3$ in the SINDy library and select the pruning strategy that best enforces the identifiability policy while respecting mechanistic interpretability.\n\nWhich option is correct?\n\nA. Normalize $\\Theta$; compute pairwise inner products to obtain $\\mu(\\Theta)$. Since the pair $\\{\\phi_2,\\phi_3\\}$ has mutual coherence exceeding $\\tau=0.95$, prune $\\phi_3$ (the composite mixture). After pruning, the remaining library $\\{\\phi_1,\\phi_2\\}$ satisfies both $\\mu(\\Theta)\\le 0.95$ and $\\kappa(G)\\le 50$, and the choice respects the mechanistic interpretability prior.\n\nB. Keep all three basis functions and rely on Least Absolute Shrinkage and Selection Operator (LASSO) regularization to resolve redundancy; high mutual coherence does not fundamentally impact identifiability when an $\\ell_1$ penalty is used.\n\nC. Prune $\\phi_2$ instead of $\\phi_3$ because the pair $\\{\\phi_1,\\phi_3\\}$ yields $\\mu(\\Theta)\\le 0.95$ and $\\kappa(G)\\le 50$; composite mixtures are acceptable substitutes for elemental basis functions under the identifiability policy.\n\nD. Do not prune; instead perform Gram–Schmidt orthogonalization of the library to reduce mutual coherence to $0$ and condition number to a small value, thereby enforcing the identifiability policy without changing the set of basis functions considered in the regression.", "solution": "The user wants me to act as a STEM professor and solve a problem related to Sparse Identification of Nonlinear Dynamics (SINDy).\n\nThe first step is to validate the problem statement.\n\n### Step 1: Extract Givens\n- **System**: A single-state system with state $x(t)$.\n- **Data**: State measured at $N=5$ time points, with values $x=\\{0, 1, 2, 3, 4\\}$.\n- **Basis Functions**: $\\phi_1(x)=1$, $\\phi_2(x)=x$, $\\phi_3(x)=1+0.99\\,x$.\n- **Library Matrix**: $\\Theta=[\\theta_1\\,\\,\\theta_2\\,\\,\\theta_3]$, where the columns are evaluations of the basis functions at the measured points.\n  - $\\theta_1=[1,1,1,1,1]^\\top$\n  - $\\theta_2=[0,1,2,3,4]^\\top$\n  - $\\theta_3=[1.00,1.99,2.98,3.97,4.96]^\\top$\n- **Normalization**: Columns of $\\Theta$ are to be normalized by their Euclidean ($\\ell_2$) norms prior to regression and analysis.\n- **Identifiability Metrics & Policy**:\n  - Mutual coherence of the normalized library, $\\mu(\\Theta)=\\max_{i\\neq j}\\left|\\langle \\hat{\\theta}_i,\\hat{\\theta}_j\\rangle\\right|$, must be $\\le \\tau=0.95$.\n  - Condition number of the Gram matrix $G=\\Theta^\\top\\Theta$, $\\kappa(G)=\\lambda_{\\max}(G)/\\lambda_{\\min}(G)$, must be $\\le \\kappa_{\\max}=50$. (This will be calculated for the normalized library, as is standard practice and implied by the problem statement).\n- **Interpretability Prior**: When resolving high coherence, prefer retaining elemental basis functions (e.g., $\\phi_1, \\phi_2$) over composite ones (e.g., $\\phi_3$), unless this violates the identifiability policy.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem is well-grounded in the theory of sparse regression for system identification, specifically the SINDy method. The concepts of basis function libraries, mutual coherence, the Gram matrix, condition number, and LASSO are all standard and correctly defined.\n- **Well-Posed**: The problem provides all necessary data and a clear, quantitative policy to follow. It asks for a specific analysis and decision, which can be reached through direct calculation.\n- **Objective**: The problem is stated in objective, technical language. The criteria for success (the policy thresholds and the interpretability prior) are unambiguous.\n\n### Step 3: Verdict and Action\nThe problem is valid. It is a well-formed exercise in applying the concepts of identifiability analysis in sparse regression. I will proceed with the solution.\n\n### Derivation\n\nThe core task is to assess the identifiability of the given library $\\{\\phi_1, \\phi_2, \\phi_3\\}$ and apply the stated pruning strategy. This requires calculating the mutual coherence $\\mu(\\Theta)$ and the condition number $\\kappa(G)$ for the normalized library.\n\n**1. Normalization of Library Columns**\n\nFirst, we calculate the $\\ell_2$-norm of each column vector $\\theta_i$.\n- For $\\theta_1 = [1, 1, 1, 1, 1]^\\top$:\n$$||\\theta_1||_2 = \\sqrt{1^2 + 1^2 + 1^2 + 1^2 + 1^2} = \\sqrt{5}$$\n- For $\\theta_2 = [0, 1, 2, 3, 4]^\\top$:\n$$||\\theta_2||_2 = \\sqrt{0^2 + 1^2 + 2^2 + 3^2 + 4^2} = \\sqrt{0+1+4+9+16} = \\sqrt{30}$$\n- For $\\theta_3 = [1.00, 1.99, 2.98, 3.97, 4.96]^\\top$:\nWe notice that $\\phi_3(x) = 1 + 0.99x$, which implies $\\theta_3 = \\theta_1 + 0.99\\,\\theta_2$. We can compute the norm as:\n$$||\\theta_3||_2^2 = ||\\theta_1 + 0.99\\,\\theta_2||_2^2 = \\langle \\theta_1 + 0.99\\,\\theta_2, \\theta_1 + 0.99\\,\\theta_2 \\rangle$$\n$$||\\theta_3||_2^2 = ||\\theta_1||_2^2 + 2(0.99)\\langle \\theta_1, \\theta_2 \\rangle + (0.99)^2||\\theta_2||_2^2$$\nWe need the inner product $\\langle \\theta_1, \\theta_2 \\rangle$:\n$$\\langle \\theta_1, \\theta_2 \\rangle = 1(0) + 1(1) + 1(2) + 1(3) + 1(4) = 10$$\nSubstituting the values:\n$$||\\theta_3||_2^2 = 5 + 2(0.99)(10) + (0.99)^2(30) = 5 + 19.8 + (0.9801)(30) = 24.8 + 29.403 = 54.203$$\n$$||\\theta_3||_2 = \\sqrt{54.203} \\approx 7.362$$\nThe normalized column vectors are $\\hat{\\theta}_i = \\theta_i / ||\\theta_i||_2$.\n\n**2. Calculation of Mutual Coherence for the Full Library**\n\nMutual coherence requires the pairwise inner products of the normalized vectors. These are the off-diagonal elements of the Gram matrix $G = \\hat{\\Theta}^\\top \\hat{\\Theta}$ of the normalized library $\\hat{\\Theta} = [\\hat{\\theta}_1, \\hat{\\theta}_2, \\hat{\\theta}_3]$.\n\n- $G_{12} = \\langle \\hat{\\theta}_1, \\hat{\\theta}_2 \\rangle = \\frac{\\langle \\theta_1, \\theta_2 \\rangle}{||\\theta_1||_2 ||\\theta_2||_2} = \\frac{10}{\\sqrt{5}\\sqrt{30}} = \\frac{10}{\\sqrt{150}} = \\frac{10}{5\\sqrt{6}} = \\frac{2}{\\sqrt{6}} \\approx 0.8165$\n- $G_{13} = \\langle \\hat{\\theta}_1, \\hat{\\theta}_3 \\rangle = \\frac{\\langle \\theta_1, \\theta_3 \\rangle}{||\\theta_1||_2 ||\\theta_3||_2}$. First, $\\langle \\theta_1, \\theta_3 \\rangle = \\langle \\theta_1, \\theta_1 + 0.99\\,\\theta_2 \\rangle = ||\\theta_1||_2^2 + 0.99\\langle \\theta_1, \\theta_2 \\rangle = 5 + 0.99(10) = 14.9$.\n$$G_{13} = \\frac{14.9}{\\sqrt{5}\\sqrt{54.203}} \\approx \\frac{14.9}{2.236 \\times 7.362} \\approx \\frac{14.9}{16.46} \\approx 0.9052$$\n- $G_{23} = \\langle \\hat{\\theta}_2, \\hat{\\theta}_3 \\rangle = \\frac{\\langle \\theta_2, \\theta_3 \\rangle}{||\\theta_2||_2 ||\\theta_3||_2}$. First, $\\langle \\theta_2, \\theta_3 \\rangle = \\langle \\theta_2, \\theta_1 + 0.99\\,\\theta_2 \\rangle = \\langle \\theta_2, \\theta_1 \\rangle + 0.99||\\theta_2||_2^2 = 10 + 0.99(30) = 39.7$.\n$$G_{23} = \\frac{39.7}{\\sqrt{30}\\sqrt{54.203}} \\approx \\frac{39.7}{5.477 \\times 7.362} \\approx \\frac{39.7}{40.32} \\approx 0.9846$$\n\nThe mutual coherence is the maximum absolute off-diagonal element:\n$$\\mu(\\Theta) = \\max(|G_{12}|, |G_{13}|, |G_{23}|) = \\max(0.8165, 0.9052, 0.9846) = 0.9846$$\nThis value exceeds the policy threshold $\\tau=0.95$. Therefore, the full library is not acceptable. The most offending pair is $\\{\\phi_2, \\phi_3\\}$.\n\n**3. Calculation of Condition Number for the Full Library**\n\nThe basis functions are linearly dependent: $\\phi_3(x) = \\phi_1(x) + 0.99\\,\\phi_2(x)$. This means the columns of the library matrix $\\Theta$ are also linearly dependent: $\\theta_3 = \\theta_1 + 0.99\\,\\theta_2$. The matrix $\\Theta$ has rank $2$, not $3$. Consequently, the Gram matrix $G = \\Theta^\\top\\Theta$ (whether from normalized or unnormalized columns) will also be rank-deficient (rank $2$). For a $3 \\times 3$ matrix, this means one of its eigenvalues must be zero.\n$\\lambda_{\\min}(G) = 0$.\nThe condition number is $\\kappa(G) = \\lambda_{\\max}(G) / \\lambda_{\\min}(G) = \\lambda_{\\max}(G) / 0$, which is infinite. An infinite condition number drastically exceeds the policy threshold $\\kappa_{\\max}=50$.\nThe full library fails both identifiability checks. Pruning is necessary.\n\n**4. Evaluating Pruning Strategies**\n\nThe source of the issue is the inclusion of $\\phi_3$, which is a composite of $\\phi_1$ and $\\phi_2$. The interpretability prior suggests pruning $\\phi_3$ and keeping the elemental functions $\\phi_1$ and $\\phi_2$, provided this satisfies the identifiability policy.\n\n**Strategy 1: Prune $\\phi_3$. Retain $\\{\\phi_1, \\phi_2\\}$.**\n- **New Library**: $\\Theta' = [\\theta_1, \\theta_2]$.\n- **Mutual Coherence**: The only value is $|\\langle \\hat{\\theta}_1, \\hat{\\theta}_2 \\rangle| = |G_{12}| \\approx 0.8165$. Since $0.8165 \\le 0.95$, the coherence policy is satisfied.\n- **Condition Number**: The new Gram matrix is a $2 \\times 2$ matrix:\n$$G' = \\begin{pmatrix} 1 & G_{12} \\\\ G_{12} & 1 \\end{pmatrix} = \\begin{pmatrix} 1 & 0.8165 \\\\ 0.8165 & 1 \\end{pmatrix}$$\nThe eigenvalues of such a matrix are $\\lambda = 1 \\pm |G_{12}|$.\n$$\\lambda_{\\max} = 1 + 0.8165 = 1.8165$$\n$$\\lambda_{\\min} = 1 - 0.8165 = 0.1835$$\nThe condition number is:\n$$\\kappa(G') = \\frac{\\lambda_{\\max}}{\\lambda_{\\min}} = \\frac{1.8165}{0.1835} \\approx 9.899$$\nSince $9.899 \\le 50$, the condition number policy is also satisfied.\n- **Interpretability Prior**: This strategy removes the composite function, adhering to the prior.\nThis strategy is fully compliant with all rules.\n\n**Strategy 2: Prune $\\phi_2$. Retain $\\{\\phi_1, \\phi_3\\}$.**\nThis would be considered only if Strategy 1 failed.\n- **New Library**: $\\Theta'' = [\\theta_1, \\theta_3]$.\n- **Mutual Coherence**: The value is $|\\langle \\hat{\\theta}_1, \\hat{\\theta}_3 \\rangle| = |G_{13}| \\approx 0.9052$. Since $0.9052 \\le 0.95$, the coherence policy is satisfied.\n- **Condition Number**: The eigenvalues are $1 \\pm |G_{13}|$.\n$$\\kappa(G'') = \\frac{1+|G_{13}|}{1-|G_{13}|} \\approx \\frac{1+0.9052}{1-0.9052} = \\frac{1.9052}{0.0948} \\approx 20.097$$\nSince $20.097 \\le 50$, the condition number policy is satisfied.\n- **Interpretability Prior**: This strategy is numerically valid but violates the prior. The prior states a preference for elemental functions unless that path is blocked. Since Strategy 1 (keeping the elemental functions) works, there is no reason to choose this path.\n\nBased on this analysis, the correct course of action is to prune $\\phi_3$.\n\n### Option-by-Option Analysis\n\n**A. Normalize $\\Theta$; compute pairwise inner products to obtain $\\mu(\\Theta)$. Since the pair $\\{\\phi_2,\\phi_3\\}$ has mutual coherence exceeding $\\tau=0.95$, prune $\\phi_3$ (the composite mixture). After pruning, the remaining library $\\{\\phi_1,\\phi_2\\}$ satisfies both $\\mu(\\Theta)\\le 0.95$ and $\\kappa(G)\\le 50$, and the choice respects the mechanistic interpretability prior.**\nThis option correctly follows the entire logical chain derived above. It correctly identifies the problematic pair $\\{\\phi_2,\\phi_3\\}$ with coherence $\\approx 0.9846 > 0.95$. It invokes the interpretability prior to prune the composite function $\\phi_3$. It correctly states that the resulting library $\\{\\phi_1,\\phi_2\\}$ satisfies both the coherence policy ($\\mu \\approx 0.8165 \\le 0.95$) and the condition number policy ($\\kappa \\approx 9.9 \\le 50$).\n**Verdict: Correct.**\n\n**B. Keep all three basis functions and rely on Least Absolute Shrinkage and Selection Operator (LASSO) regularization to resolve redundancy; high mutual coherence does not fundamentally impact identifiability when an $\\ell_1$ penalty is used.**\nThis option contains a false premise. While LASSO is a powerful tool for sparse regression, its theoretical guarantees and practical performance degrade severely in the presence of highly correlated variables (high mutual coherence). In such cases, LASSO may arbitrarily select one of a set of correlated variables, leading to unstable and uninterpretable models. Furthermore, this option ignores the explicit policy given in the problem, which requires pre-processing to ensure $\\mu(\\Theta) \\le 0.95$ and $\\kappa(G) \\le 50$ *before* applying LASSO. Our analysis showed the full library violates this policy.\n**Verdict: Incorrect.**\n\n**C. Prune $\\phi_2$ instead of $\\phi_3$ because the pair $\\{\\phi_1,\\phi_3\\}$ yields $\\mu(\\Theta)\\le 0.95$ and $\\kappa(G)\\le 50$; composite mixtures are acceptable substitutes for elemental basis functions under the identifiability policy.**\nOur analysis of \"Strategy 2\" confirms that pruning $\\phi_2$ does result in a library that satisfies the numerical identifiability thresholds. However, this option neglects the \"mechanistic interpretability prior\". This prior mandates that we prefer to keep elemental functions over composite ones *unless* doing so violates the policy. Since keeping the elemental functions $\\{\\phi_1, \\phi_2\\}$ (by pruning $\\phi_3$) is a valid solution that meets all policies, there is no justification under the stated rules to select this less interpretable option.\n**Verdict: Incorrect.**\n\n**D. Do not prune; instead perform Gram–Schmidt orthogonalization of the library to reduce mutual coherence to $0$ and condition number to a small value, thereby enforcing the identifiability policy without changing the set of basis functions considered in the regression.**\nThis is an inappropriate solution in the context of mechanistic model discovery. Gram-Schmidt orthogonalization replaces the original, interpretable basis functions (like $\\phi_1(x)=1$, $\\phi_2(x)=x$) with new, orthogonal functions which are linear combinations of the originals (e.g., $q_2(x) = a\\phi_2(x) + b\\phi_1(x)$). While this solves the numerical issues of coherence and conditioning, it destroys the mechanistic interpretability of the model coefficients. The goal of SINDy is to find a sparse model in terms of the *original* candidate functions. This option fundamentally misunderstands the purpose of the method and the importance of the interpretability prior.\n**Verdict: Incorrect.**", "answer": "$$\\boxed{A}$$", "id": "3353792"}, {"introduction": "A key advantage of mechanistic modeling is the ability to incorporate physical principles, a feature often missing in black-box machine learning. For biochemical systems, a fundamental constraint is that concentrations cannot be negative. This practice [@problem_id:3353725] challenges you to derive the mathematical condition that guarantees this non-negativity and to construct a penalty functional that can be used to enforce this constraint during the symbolic regression process.", "problem": "Consider a discovered system of ordinary differential equations (ODEs) for $n$ biochemical species with concentrations $x \\in \\mathbb{R}_{\\geq 0}^{n}$, written as $\\dot{x} = f(x)$, where $f : \\mathbb{R}_{\\geq 0}^{n} \\to \\mathbb{R}^{n}$ is locally Lipschitz and represents net production rates. In mechanistic chemical reaction network models under the law of mass action, $f(x)$ can be represented as $f(x) = S\\,v(x)$, where $S \\in \\mathbb{R}^{n \\times m}$ is the stoichiometric matrix and $v : \\mathbb{R}_{\\geq 0}^{n} \\to \\mathbb{R}_{\\geq 0}^{m}$ is the vector of elementary reaction propensities (fluxes), which are nonnegative functions on $\\mathbb{R}_{\\geq 0}^{n}$ and vanish whenever any required reactant concentration is zero. In symbolic regression for mechanistic model discovery, candidate forms of $f$ and $v$ may violate physical constraints such as nonnegativity of concentrations and fluxes.\n\nStarting from the physical constraint that concentrations are nonnegative and reaction propensities are nonnegative under the law of mass action, and from the definition of forward invariance of a closed set for an ODE, derive conditions that ensure positivity invariance of the state space $\\Omega = \\mathbb{R}_{\\geq 0}^{n}$; that is, conditions under which trajectories starting in $\\Omega$ remain in $\\Omega$ for all forward time. Then, to regularize symbolic regression, construct a single differentiable penalty functional that discourages candidate violations of:\n- inward-pointing boundary conditions for $\\Omega$ on each coordinate hyperface $\\{x \\in \\Omega : x_{i} = 0\\}$, and\n- nonnegativity of each candidate elementary reaction propensity $v_{r}(x)$ on $\\Omega$.\n\nLet the evaluation region be the bounded hypercube $\\mathcal{D}_{L} = [0,L]^{n}$ with $L > 0$, and let the hyperfaces be $\\mathcal{F}_{i}(L) = \\{x \\in \\mathcal{D}_{L} : x_{i} = 0\\}$ for $i \\in \\{1,\\dots,n\\}$. Use uniform weights on $\\mathcal{F}_{i}(L)$ and $\\mathcal{D}_{L}$, and let $\\alpha > 0$ and $\\beta > 0$ be hyperparameters that balance the penalties for boundary violations and flux negativity, respectively. Express the boundary violation penalty using the positive-part operator reparameterized via absolute values only, and do the same for the flux nonnegativity penalty. Your penalty must be normalized by the $(n-1)$-dimensional volume of each hyperface and by the $n$-dimensional volume of $\\mathcal{D}_{L}$.\n\nProvide the final penalty functional as a single closed-form analytic expression involving only algebraic operations, absolute values, and integrals, in terms of $f$, $v$, $L$, $\\alpha$, and $\\beta$. Your final answer must be the single analytic expression. No numerical rounding is required and no units should be reported in the final answer.", "solution": "The problem asks for two main components: first, the derivation of conditions ensuring the non-negativity of species concentrations over time, and second, the construction of a penalty functional to enforce these conditions within a symbolic regression framework for model discovery.\n\nFirst, we derive the condition for positivity invariance. The state space of concentrations is the non-negative orthant, $\\Omega = \\mathbb{R}_{\\geq 0}^{n}$, which is a closed and convex set in $\\mathbb{R}^{n}$. The dynamics of the system are given by the ordinary differential equation (ODE) $\\dot{x} = f(x)$. For a trajectory starting in $\\Omega$ (i.e., $x(0) \\in \\Omega$) to remain in $\\Omega$ for all forward time $t > 0$, the set $\\Omega$ must be forward invariant.\n\nA sufficient condition for forward invariance of a closed convex set $\\Omega$ is given by Nagumo's theorem (also known as the tangency condition). The theorem states that $\\Omega$ is forward invariant if, and only if, for every point $x$ on the boundary of $\\Omega$, denoted $\\partial \\Omega$, the vector field $f(x)$ is tangent to $\\Omega$ or points into the interior of $\\Omega$.\n\nThe boundary of the non-negative orthant $\\Omega = \\mathbb{R}_{\\geq 0}^{n}$ is the union of the coordinate hyperfaces where at least one concentration is zero: $\\partial \\Omega = \\bigcup_{i=1}^{n} \\{x \\in \\mathbb{R}_{\\geq 0}^{n} : x_i = 0\\}$.\n\nLet's consider a point $x$ on one of these hyperfaces, where $x_i = 0$ for some particular index $i \\in \\{1, \\dots, n\\}$, and $x_j \\ge 0$ for all $j \\neq i$. At this point, the outward-pointing normal vector to the boundary face is $-e_i$, where $e_i$ is the $i$-th standard basis vector. The tangency condition requires that the vector field $f(x)$ does not have a component in the direction of the outward normal. Mathematically, this is expressed as $f(x) \\cdot (-e_i) \\le 0$. This dot product simplifies to $-f_i(x) \\le 0$, which is equivalent to $f_i(x) \\ge 0$.\n\nTherefore, the condition for positivity invariance of the state space $\\Omega = \\mathbb{R}_{\\geq 0}^{n}$ is that for each species $i=1, \\dots, n$, the net rate of change $f_i(x)$ must be non-negative whenever the concentration of that species is zero. Formally:\nFor each $i \\in \\{1, \\dots, n\\}$, we must have $f_i(x) \\ge 0$ for all $x \\in \\Omega$ such that $x_i = 0$.\nThis is the \"inward-pointing boundary condition\" mentioned in the problem. For a mechanistic model based on mass action, where $f(x)=S v(x)$, this condition is naturally satisfied if the reaction propensities $v_r(x)$ are non-negative and vanish when a necessary reactant concentration is zero. However, a model discovered via symbolic regression might not automatically satisfy these physical constraints.\n\nNext, we construct the penalty functional $J(f, v)$ to regularize symbolic regression. The functional should discourage violations of two conditions on a bounded test domain $\\mathcal{D}_L = [0,L]^n$:\n1. The inward-pointing boundary condition: $f_i(x) \\ge 0$ on the hyperfaces $\\mathcal{F}_i(L) = \\{x \\in \\mathcal{D}_L : x_i=0\\}$.\n2. The non-negativity of reaction propensities: $v_r(x) \\ge 0$ on the domain $\\mathcal{D}_L$.\n\nThe total penalty functional $J(f, v)$ is the sum of two terms, $J_{boundary}$ and $J_{flux}$, weighted by hyperparameters $\\alpha > 0$ and $\\beta > 0$.\n\nFor the boundary violation penalty, a violation for species $i$ occurs at a point $x \\in \\mathcal{F}_i(L)$ if $f_i(x) < 0$. We want to penalize this violation. A function that captures this is the positive-part of $-f_i(x)$, which is $\\max(0, -f_i(x))$. The problem specifies using an equivalent representation involving only absolute values. The identity for the positive-part function $(z)_+ = \\max(0, z)$ is $(z+|z|)/2$. Applying this to $-f_i(x)$, the penalty at point $x$ is:\n$$ p_i(x) = \\max(0, -f_i(x)) = \\frac{-f_i(x) + |-f_i(x)|}{2} = \\frac{-f_i(x) + |f_i(x)|}{2} $$\nThis function is zero if $f_i(x) \\ge 0$ and equals $-f_i(x)$ if $f_i(x) < 0$. To obtain the total penalty for the $i$-th hyperface, we integrate this quantity over $\\mathcal{F}_i(L)$. The problem requires normalization by the $(n-1)$-dimensional volume of $\\mathcal{F}_i(L)$, which is $L^{n-1}$. The total boundary penalty, $J_{boundary}$, is the sum over all faces, weighted by $\\alpha$:\n$$ J_{boundary} = \\alpha \\sum_{i=1}^{n} \\left( \\frac{1}{L^{n-1}} \\int_{\\mathcal{F}_i(L)} \\frac{|f_i(x)| - f_i(x)}{2} \\, d\\sigma_i \\right) $$\nwhere $d\\sigma_i$ is the $(n-1)$-dimensional surface measure on $\\mathcal{F}_i(L)$.\n\nFor the flux non-negativity penalty, a violation for reaction $r$ occurs at a point $x \\in \\mathcal{D}_L$ if $v_r(x) < 0$. Similarly, we penalize this with the function $\\max(0, -v_r(x))$, which is represented as:\n$$ q_r(x) = \\max(0, -v_r(x)) = \\frac{-v_r(x) + |-v_r(x)|}{2} = \\frac{-v_r(x) + |v_r(x)|}{2} $$\nTo obtain the total penalty for the $r$-th reaction, we integrate this quantity over the entire domain $\\mathcal{D}_L$. This is then normalized by the $n$-dimensional volume of $\\mathcal{D}_L$, which is $L^n$. The total flux penalty, $J_{flux}$, is the sum over all reactions, weighted by $\\beta$:\n$$ J_{flux} = \\beta \\sum_{r=1}^{m} \\left( \\frac{1}{L^n} \\int_{\\mathcal{D}_L} \\frac{|v_r(x)| - v_r(x)}{2} \\, dV \\right) $$\nwhere $dV$ is the $n$-dimensional volume element.\n\nThe final penalty functional is the sum $J(f, v) = J_{boundary} + J_{flux}$. Combining the expressions and factoring out constants, we obtain the single closed-form analytic expression:\n$$ J(f, v) = \\frac{\\alpha}{2L^{n-1}} \\sum_{i=1}^{n} \\int_{\\mathcal{F}_i(L)} \\left(|f_i(x)| - f_i(x)\\right) d\\sigma_i + \\frac{\\beta}{2L^n} \\sum_{r=1}^{m} \\int_{\\mathcal{D}_L} \\left(|v_r(x)| - v_r(x)\\right) dV $$\nThis functional is differentiable with respect to the parameters of $f$ and $v$ almost everywhere, suitable for gradient-based optimization in symbolic regression.", "answer": "$$ \\boxed{ \\frac{\\alpha}{2 L^{n-1}} \\sum_{i=1}^{n} \\int_{\\mathcal{F}_i(L)} (|f_i(x)| - f_i(x)) \\, d\\sigma_i + \\frac{\\beta}{2 L^n} \\sum_{r=1}^{m} \\int_{\\mathcal{D}_L} (|v_r(x)| - v_r(x)) \\, dV } $$", "id": "3353725"}, {"introduction": "With a well-defined and constrained library of candidate models, the central task becomes selecting the single best structure that explains the observed data. This coding exercise [@problem_id:3353773] walks you through implementing a Bayesian model selection framework, a powerful and principled approach for this task. By numerically computing the marginal likelihood, or \"evidence,\" for different gene regulation models, you will see how this method quantitatively balances goodness-of-fit with model complexity to identify the most plausible hypothesis.", "problem": "Implement a complete, runnable program that performs Symbolic Regression (SR) with Bayesian priors over mechanistic gene activation models to infer the posterior probability over candidate expression families given simulated time-series data. The mechanistic base is the ordinary differential equation $dx/dt = f(S) - \\gamma x$ in a dimensionless setting, where $x$ is a dimensionless gene product level, $S$ is a dimensionless input signal, and $\\gamma$ is a positive degradation rate constant. The program must enumerate a small grammar of candidate regulatory expressions $f(S)$, assign scientifically informed priors over discrete Hill exponents and continuous biochemical parameters, compute a Bayesian posterior over expression families by marginalizing parameters, and then report the posterior probability assigned to the true generating family for multiple test cases. All mathematical entities must be written in LaTeX below.\n\nThe derivation must start from the following base:\n- The gene regulation dynamics model $dx/dt = f(S) - \\gamma x$ with $\\gamma > 0$.\n- Canonical activation and repression Hill functions used in biochemistry:\n  - Hill activation: $f_{\\mathrm{HA}}(S;V,K,n) = \\dfrac{V S^n}{K^n + S^n}$.\n  - Hill repression: $f_{\\mathrm{HR}}(S;V,K,n) = \\dfrac{V K^n}{K^n + S^n}$.\n  - Linear mass-action activation: $f_{\\mathrm{LM}}(S;k) = k S$.\n- Bayesian model selection with Bayes’ theorem: $p(M \\mid D) \\propto p(D \\mid M) \\, p(M)$, where $M$ is a model family and $D$ is the observed data. The marginal likelihood is $p(D \\mid M) = \\int p(D \\mid \\theta, M) \\, p(\\theta \\mid M) \\, d\\theta$, where $\\theta$ are model parameters.\n\nYou must implement a finite, deterministic approximation of the marginal likelihood for each expression family by integrating (via a uniform grid over logarithms of parameters) the Gaussian likelihood of the simulated observations under the mechanistic model, while encoding informed priors over the Hill exponent $n$. The program must proceed as follows:\n- Candidate expression families to enumerate:\n  1. Hill activation family: $f_{\\mathrm{HA}}(S;V,K,n)$ with Hill exponent $n \\in \\{1,2,3,4\\}$.\n  2. Hill repression family: $f_{\\mathrm{HR}}(S;V,K,n)$ with Hill exponent $n \\in \\{1,2,3,4\\}$.\n  3. Linear mass-action activation family: $f_{\\mathrm{LM}}(S;k)$.\n- Structure prior: $p(M)$ is uniform over the three families, $p(M) = 1/3$ for each family.\n- Hill exponent prior: $p(n=1) = 0.4$, $p(n=2) = 0.3$, $p(n=3) = 0.2$, $p(n=4) = 0.1$.\n- Parameter priors (all dimensionless):\n  - For Hill families, $V$ and $K$ have log-uniform priors over $[V_{\\min},V_{\\max}]$ and $[K_{\\min},K_{\\max}]$, respectively, with $V_{\\min} = 0.3$, $V_{\\max} = 3.0$, $K_{\\min} = 0.2$, $K_{\\max} = 2.0$.\n  - For linear mass-action, $k$ has a log-uniform prior over $[k_{\\min},k_{\\max}]$ with $k_{\\min} = 0.2$, $k_{\\max} = 2.0$.\n- Numerical marginalization:\n  - Use grids uniform in $\\log$ space with $N_V = 6$ and $N_K = 6$ points for Hill families (for $V$ and $K$), and $N_k = 6$ points for the linear mass-action family (for $k$). Treat grid points as equal-weight samples under a log-uniform prior, so that the marginal likelihood is approximated by the mean of the likelihood over the grid.\n- Likelihood model:\n  - Observations are simulated as noisy time-series $x_{\\mathrm{obs}}(t_i) = x_{\\mathrm{true}}(t_i) + \\varepsilon_i$ with independent Gaussian noise $\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma_x^2)$, where $\\sigma_x$ is known for each test case. Given parameters $\\theta$ and family $M$, the predicted trajectory $x_{\\theta,M}(t)$ solves $dx/dt = f_M(S(t);\\theta) - \\gamma x$; the likelihood is\n    $$\\log p(D \\mid \\theta, M) = \\sum_{i=1}^{N} \\left[ -\\dfrac{\\left(x_{\\mathrm{obs}}(t_i) - x_{\\theta,M}(t_i)\\right)^2}{2\\sigma_x^2} - \\dfrac{1}{2}\\log\\left(2\\pi\\sigma_x^2\\right) \\right].$$\n  - For numerical integration of the dynamics, use a stable forward Euler discretization with step size $\\Delta t$ over $N$ points:\n    $$x_{i+1} = x_i + \\Delta t \\left( f_M(S(t_i);\\theta) - \\gamma x_i \\right), \\quad x_0 \\text{ given.}$$\n- Posterior over families:\n  - For each family $M$, compute the marginal likelihood $p(D \\mid M)$ by averaging $p(D \\mid \\theta, M)$ across the log-space grid; for Hill families, average for each $n$ separately to get $p(D \\mid n, M)$, then combine with $p(n)$ to get $p(D \\mid M) = \\sum_{n} p(n) \\, p(D \\mid n, M)$.\n  - Combine with the uniform structure prior to get the normalized posterior $p(M \\mid D) = \\dfrac{p(D \\mid M) p(M)}{\\sum_{M'} p(D \\mid M') p(M')}$.\n\nTest suite specification (all quantities dimensionless):\n- Time discretization uses $N$ points uniformly spaced over $[0,T]$, with $\\Delta t = T/(N-1)$. The initial condition $x_0$ is specified per case.\n- Define the input signal $S(t)$ for each case:\n  1. Case A (happy path Hill activation): $T = 20$, $N = 201$, $S(t) = 0$ for $t < 5$ and $S(t) = 1$ for $t \\ge 5$, $\\gamma = 0.7$, $\\sigma_x = 0.05$, $x_0 = 0.0$. True generating model: Hill activation with $V = 1.5$, $K = 0.5$, $n = 2$.\n  2. Case B (boundary low exponent): $T = 15$, $N = 151$, $S(t) = \\min\\left(2 \\, t / T, 2\\right)$, $\\gamma = 1.0$, $\\sigma_x = 0.08$, $x_0 = 0.0$. True generating model: Hill activation with $V = 1.0$, $K = 1.0$, $n = 1$.\n  3. Case C (repression under high step): $T = 18$, $N = 181$, $S(t) = 0.2$ for $t < 3$ and $S(t) = 2.0$ for $t \\ge 3$, $\\gamma = 0.6$, $\\sigma_x = 0.05$. True generating model: Hill repression with $V = 2.0$, $K = 0.3$, $n = 3$. Use $x_0 = f_{\\mathrm{HR}}(0.2;V,K,n)/\\gamma$ to start near steady-state before the step.\n  4. Case D (linear mass-action under sinusoidal input): $T = 20$, $N = 201$, $S(t) = 0.5\\left[\\sin\\left(6\\pi t/T\\right) + 1\\right]$, $\\gamma = 0.5$, $\\sigma_x = 0.03$, $x_0 = 0.0$. True generating model: linear mass-action with $k = 0.8$.\n\nImplementation requirements:\n- Simulate $x_{\\mathrm{true}}(t)$ for each case from the true generating model using the forward Euler scheme, then add independent Gaussian noise $\\mathcal{N}(0,\\sigma_x^2)$ to obtain $x_{\\mathrm{obs}}(t)$.\n- For inference, compute $p(M \\mid D)$ for each candidate family $M \\in \\{\\mathrm{HA}, \\mathrm{HR}, \\mathrm{LM}\\}$ following the Bayesian marginalization described above, using the specified log-space grids:\n  - Hill families: $V \\in [0.3,3.0]$ and $K \\in [0.2,2.0]$, each with $N_V = N_K = 6$ logarithmically spaced values, and $n \\in \\{1,2,3,4\\}$ with the given prior.\n  - Linear mass-action family: $k \\in [0.2,2.0]$ with $N_k = 6$ logarithmically spaced values.\n- Your final output for the program must be a single line containing a comma-separated list enclosed in square brackets with four numbers, one per test case, representing the posterior probability assigned to the true generating family for Cases A, B, C, and D, in that order. For example, the output format should be exactly like $[p_A,p_B,p_C,p_D]$, where each $p_\\bullet$ is a floating-point number in standard decimal notation.\n\nAll variables are dimensionless; no physical unit conversions are required. Angles in the sinusoidal input are in radians. The program must be deterministic and self-contained, requiring no external input or files, and must use only the specified runtime environment libraries.", "solution": "The problem requires the implementation of a Bayesian model selection framework to discover the most probable gene regulatory function from a predefined set of candidates, given simulated time-series data. This task is a form of symbolic regression guided by mechanistic principles, a common problem in computational systems biology. The solution involves simulating data from known models and then using Bayesian inference to calculate the posterior probability of each candidate model family, ultimately reporting the probability assigned to the true generating family.\n\nThe core of the analysis is the dimensionless ordinary differential equation (ODE) governing the concentration of a gene product, $x$:\n$$\n\\frac{dx}{dt} = f(S) - \\gamma x\n$$\nHere, $x(t)$ represents the gene product level over time, $S(t)$ is an input signal, $\\gamma$ is the first-order degradation rate constant ($\\gamma > 0$), and $f(S)$ is the production rate function, which depends on the signal $S$. The goal is to determine the most plausible functional form for $f(S)$ from a discrete library of candidates.\n\nThe candidate functions for $f(S)$ are standard biochemical models:\n1.  **Hill Activation (HA)**: $f_{\\mathrm{HA}}(S;V,K,n) = \\dfrac{V S^n}{K^n + S^n}$. This function describes a cooperative activation process, where $V$ is the maximum production rate, $K$ is the activation constant (the signal level for half-maximal activation), and $n$ is the Hill exponent, quantifying the cooperativity. The model family for HA consists of sub-models for $n \\in \\{1, 2, 3, 4\\}$.\n2.  **Hill Repression (HR)**: $f_{\\mathrm{HR}}(S;V,K,n) = \\dfrac{V K^n}{K^n + S^n}$. This function describes cooperative repression. The parameters $V$, $K$, and $n$ have analogous interpretations. The model family for HR also includes sub-models for $n \\in \\{1, 2, 3, 4\\}$.\n3.  **Linear Mass-Action (LM)**: $f_{\\mathrm{LM}}(S;k) = k S$. This is the simplest model, representing a non-saturating, direct proportionality between signal and production rate, with $k$ as the rate constant.\n\nThe model selection is performed within a Bayesian framework. According to Bayes' theorem, the posterior probability of a model family $M$ given observed data $D$ is proportional to the product of the marginal likelihood of the data under the model and the prior probability of the model:\n$$\np(M \\mid D) \\propto p(D \\mid M) \\, p(M)\n$$\nThe term $p(M)$ is the structure prior, which reflects any pre-existing belief about the likelihood of each model family. The problem specifies a uniform structure prior, $p(M) = 1/3$ for each of the three families (HA, HR, LM), indicating no initial preference.\n\nThe key quantity to compute is the marginal likelihood, or evidence, $p(D \\mid M)$. It is obtained by integrating the likelihood $p(D \\mid \\theta, M)$ over all possible values of the model's parameters $\\theta$, weighted by their prior distribution $p(\\theta \\mid M)$:\n$$\np(D \\mid M) = \\int p(D \\mid \\theta, M) \\, p(\\theta \\mid M) \\, d\\theta\n$$\nThe likelihood function $p(D \\mid \\theta, M)$ quantifies how well a model with specific parameters $\\theta$ explains the data. The data $D$ consists of observations $x_{\\mathrm{obs}}(t_i)$ at discrete time points, which are assumed to be the true trajectory $x_{\\theta,M}(t_i)$ plus independent Gaussian noise: $x_{\\mathrm{obs}}(t_i) = x_{\\theta,M}(t_i) + \\varepsilon_i$, where $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma_x^2)$. The log-likelihood for the entire time series is:\n$$\n\\log p(D \\mid \\theta, M) = \\sum_{i=1}^{N} \\left[ -\\dfrac{\\left(x_{\\mathrm{obs}}(t_i) - x_{\\theta,M}(t_i)\\right)^2}{2\\sigma_x^2} - \\dfrac{1}{2}\\log\\left(2\\pi\\sigma_x^2\\right) \\right]\n$$\nThe prior distributions for the parameters are specified as follows:\n- For Hill families, $V$ and $K$ have log-uniform priors over $[0.3, 3.0]$ and $[0.2, 2.0]$, respectively.\n- For the LM family, $k$ has a log-uniform prior over $[0.2, 2.0]$.\n- For Hill families, the exponent $n$ has a specified discrete prior: $p(n=1)=0.4$, $p(n=2)=0.3$, $p(n=3)=0.2$, and $p(n=4)=0.1$.\n\nThe algorithmic implementation proceeds as follows:\nFirst, for each test case, synthetic data $x_{\\mathrm{obs}}$ is generated by solving the ODE for the true model using a forward Euler scheme, $x_{i+1} = x_i + \\Delta t (f(S(t_i)) - \\gamma x_i)$, and adding Gaussian noise with the specified standard deviation $\\sigma_x$.\n\nSecond, the marginal likelihood for each candidate family is approximated numerically. The integral is replaced by a sum over a discrete grid of parameter values. The problem specifies using a grid that is uniform in logarithmic space for parameters $V$, $K$, and $k$ (with $N_V=6, N_K=6, N_k=6$ points), simplifying the marginalization to an arithmetic mean of the likelihoods over the grid points. This simplification is equivalent to a Monte Carlo approximation where the grid points are treated as samples from the log-uniform prior. To avoid numerical underflow when converting log-likelihoods back to likelihoods for averaging, a numerically stable approach is used:\n$$\np(D \\mid M) \\approx \\frac{1}{N_{\\text{grid}}} \\sum_{j} p(D \\mid \\theta_j, M) = \\frac{1}{N_{\\text{grid}}} \\exp(\\ell_{\\max}) \\sum_{j} \\exp(\\log p(D \\mid \\theta_j, M) - \\ell_{\\max})\n$$\nwhere $\\ell_{\\max}$ is the maximum log-likelihood value over the grid.\n\nFor the Hill families (HA and HR), which have the discrete parameter $n$, the marginalization is a two-step process. First, for each value of $n \\in \\{1, 2, 3, 4\\}$, the marginal likelihood $p(D \\mid n, M)$ is computed by averaging over the $(V, K)$ grid. Then, the total marginal likelihood for the family is calculated as a weighted sum using the prior on $n$:\n$$\np(D \\mid M_{\\text{Hill}}) = \\sum_{n \\in \\{1,2,3,4\\}} p(n) \\, p(D \\mid n, M_{\\text{Hill}})\n$$\n\nFinally, with the marginal likelihoods (evidences) $E_{HA}, E_{HR}, E_{LM}$ for the three families computed, the posterior probabilities are found by normalizing. Since the structure prior $p(M)$ is uniform, it cancels out, and the posterior for a family $M$ is simply its evidence divided by the total evidence:\n$$\np(M \\mid D) = \\frac{E_M}{E_{HA} + E_{HR} + E_{LM}}\n$$\nFor each test case, the algorithm calculates these posteriors and reports the value corresponding to the family from which the data was actually generated. This procedure provides a quantitative, principled measure of confidence in each candidate regulatory mechanism.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements Symbolic Regression with Bayesian priors to infer gene activation models.\n    \"\"\"\n    # Set a deterministic seed for noise generation as required by the problem.\n    RNG = np.random.default_rng(seed=0)\n\n    # --- Define candidate model functions ---\n    def f_ha(S, V, K, n):\n        \"\"\"Hill activation function.\"\"\"\n        # Add a small epsilon to prevent division by zero in edge cases.\n        S_n = S**n\n        K_n = K**n\n        return (V * S_n) / (K_n + S_n + 1e-12)\n\n    def f_hr(S, V, K, n):\n        \"\"\"Hill repression function.\"\"\"\n        S_n = S**n\n        K_n = K**n\n        return (V * K_n) / (K_n + S_n + 1e-12)\n\n    def f_lm(S, k):\n        \"\"\"Linear mass-action function.\"\"\"\n        return k * S\n\n    # --- Define ODE solver ---\n    def euler_solver(f_prod_func, s_signal, gamma, x0, t_points, params):\n        \"\"\"Solves dx/dt = f(S) - gamma*x using forward Euler.\"\"\"\n        dt = t_points[1] - t_points[0]\n        n_points = len(t_points)\n        x = np.zeros(n_points)\n        x[0] = x0\n        for i in range(n_points - 1):\n            production = f_prod_func(s_signal[i], **params)\n            degradation = gamma * x[i]\n            x[i+1] = x[i] + dt * (production - degradation)\n        return x\n\n    # --- Define Likelihood and Evidence functions ---\n    def log_likelihood(x_obs, x_pred, sigma_x):\n        \"\"\"Computes the log-likelihood of observations given a prediction.\"\"\"\n        n_obs = len(x_obs)\n        sse = np.sum((x_obs - x_pred)**2)\n        log_lik = -0.5 * n_obs * np.log(2 * np.pi * sigma_x**2) - sse / (2 * sigma_x**2)\n        return log_lik\n\n    def average_likelihood_from_logs(log_likelihoods):\n        \"\"\"Calculates the mean of likelihoods from a list of log-likelihoods stably.\"\"\"\n        if not log_likelihoods:\n            return 0.0\n        log_lik_array = np.array(log_likelihoods)\n        max_log_lik = np.max(log_lik_array)\n        \n        # If all log-likelihoods are very small, the likelihoods are effectively zero.\n        if max_log_lik < -700:\n            return 0.0\n        \n        # Use log-sum-exp trick for the sum, then average.\n        sum_of_likelihoods = np.sum(np.exp(log_lik_array - max_log_lik))\n        avg_lik = np.exp(max_log_lik) * sum_of_likelihoods / len(log_likelihoods)\n        return avg_lik\n\n    # --- Define test cases from problem statement ---\n    test_cases = [\n        # Case A: Hill activation\n        {'T': 20.0, 'N': 201, 'S_func': lambda t: np.where(t < 5.0, 0.0, 1.0), 'gamma': 0.7, 'sigma_x': 0.05, 'x0': 0.0,\n         'true_model': {'type': 'HA', 'params': {'V': 1.5, 'K': 0.5, 'n': 2.0}, 'func': f_ha}},\n        # Case B: Hill activation, low exponent\n        {'T': 15.0, 'N': 151, 'S_func': lambda t: np.minimum(2.0 * t / 15.0, 2.0), 'gamma': 1.0, 'sigma_x': 0.08, 'x0': 0.0,\n         'true_model': {'type': 'HA', 'params': {'V': 1.0, 'K': 1.0, 'n': 1.0}, 'func': f_ha}},\n        # Case C: Hill repression\n        {'T': 18.0, 'N': 181, 'S_func': lambda t: np.where(t < 3.0, 0.2, 2.0), 'gamma': 0.6, 'sigma_x': 0.05, 'x0': 'from_true_params',\n         'true_model': {'type': 'HR', 'params': {'V': 2.0, 'K': 0.3, 'n': 3.0}, 'func': f_hr}},\n        # Case D: Linear mass-action\n        {'T': 20.0, 'N': 201, 'S_func': lambda t: 0.5 * (np.sin(6.0 * np.pi * t / 20.0) + 1.0), 'gamma': 0.5, 'sigma_x': 0.03, 'x0': 0.0,\n         'true_model': {'type': 'LM', 'params': {'k': 0.8}, 'func': f_lm}}\n    ]\n    \n    # --- Define parameter grids and priors for inference ---\n    V_grid = np.logspace(np.log10(0.3), np.log10(3.0), 6)\n    K_grid = np.logspace(np.log10(0.2), np.log10(2.0), 6)\n    k_grid = np.logspace(np.log10(0.2), np.log10(2.0), 6)\n    n_values = [1, 2, 3, 4]\n    n_priors = np.array([0.4, 0.3, 0.2, 0.1])\n\n    final_results = []\n    \n    # --- Main Loop over Test Cases ---\n    for case in test_cases:\n        # Generate time series data\n        t_points = np.linspace(0, case['T'], case['N'])\n        s_signal = case['S_func'](t_points)\n        x0 = case['x0']\n        if x0 == 'from_true_params': # Special handling for case C initial condition\n            s_initial = s_signal[0]\n            true_params = case['true_model']['params']\n            x0 = case['true_model']['func'](s_initial, **true_params) / case['gamma']\n\n        x_true = euler_solver(case['true_model']['func'], s_signal, case['gamma'], x0, t_points, case['true_model']['params'])\n        noise = RNG.normal(0, case['sigma_x'], size=case['N'])\n        x_obs = x_true + noise\n\n        # --- Inference Stage: Compute Evidence for each model family ---\n        evidences = {}\n\n        # 1. Hill Activation (HA)\n        evidence_per_n_ha = []\n        for n in n_values:\n            log_liks = [log_likelihood(x_obs, euler_solver(f_ha, s_signal, case['gamma'], x0, t_points, {'V': v, 'K': k, 'n': float(n)}), case['sigma_x'])\n                        for v in V_grid for k in K_grid]\n            evidence_per_n_ha.append(average_likelihood_from_logs(log_liks))\n        evidences['HA'] = np.sum(n_priors * np.array(evidence_per_n_ha))\n\n        # 2. Hill Repression (HR)\n        evidence_per_n_hr = []\n        for n in n_values:\n            log_liks = [log_likelihood(x_obs, euler_solver(f_hr, s_signal, case['gamma'], x0, t_points, {'V': v, 'K': k, 'n': float(n)}), case['sigma_x'])\n                        for v in V_grid for k in K_grid]\n            evidence_per_n_hr.append(average_likelihood_from_logs(log_liks))\n        evidences['HR'] = np.sum(n_priors * np.array(evidence_per_n_hr))\n        \n        # 3. Linear Mass-Action (LM)\n        log_liks_lm = [log_likelihood(x_obs, euler_solver(f_lm, s_signal, case['gamma'], x0, t_points, {'k': k_val}), case['sigma_x'])\n                       for k_val in k_grid]\n        evidences['LM'] = average_likelihood_from_logs(log_liks_lm)\n        \n        # --- Calculate Posteriors ---\n        total_evidence = sum(evidences.values())\n        if total_evidence > 0:\n            posteriors = {model: ev / total_evidence for model, ev in evidences.items()}\n        else: # Handle unlikely case where all evidences are zero\n            posteriors = {'HA': 1./3., 'HR': 1./3., 'LM': 1./3.}\n        \n        # Append the posterior of the true model for this case\n        true_model_type = case['true_model']['type']\n        final_results.append(posteriors[true_model_type])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.8f}' for r in final_results)}]\")\n\nsolve()\n```", "id": "3353773"}]}