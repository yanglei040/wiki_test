{"hands_on_practices": [{"introduction": "A fundamental approach to contextualizing a generic protein-protein interaction (PPI) network is to identify which interactions are likely active under specific experimental conditions. This practice demonstrates a classic method where the expression levels of genes corresponding to interacting proteins are used as evidence of functional association. You will learn to quantify this association using Pearson correlation and apply the Fisher $z$-transformation to rigorously assess the statistical significance of co-expression for each interaction, thereby pruning the network to a context-specific sub-graph [@problem_id:3320742].", "problem": "You are given a transcriptomics data matrix for four genes across eight independent samples and a protein-protein interaction (PPI) subgraph. The goal is to integrate transcriptomic coexpression with the PPI subgraph by assigning a weight to each PPI edge based on the Pearson correlation of the corresponding gene expression profiles and then using the Fisher transformation to test whether the edge is significantly coexpressed at a specified significance level.\n\nBackground and allowed starting points:\n- Central Dogma of Molecular Biology states that deoxyribonucleic acid (DNA) is transcribed to ribonucleic acid (RNA), and RNA is translated to protein. Transcriptomics measures messenger RNA (mRNA) abundance per gene across samples.\n- A Protein-Protein Interaction (PPI) network encodes physical interaction relationships among proteins. Integrating transcriptomic coexpression with PPI edges is a well-established approach to prioritize condition-specific interactions.\n- Pearson product-moment correlation coefficient between two sample vectors $x = (x_1,\\dots,x_n)$ and $y = (y_1,\\dots,y_n)$ is defined by\n  $$ r_{xy} = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^{n} (x_i - \\bar{x})^2} \\sqrt{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}}. $$\n- For $n \\ge 4$, the Fisher $z$-transformation $z = \\operatorname{arctanh}(r)$ renders the sampling distribution approximately normal, with standard error $1/\\sqrt{n-3}$ when the true correlation is $0$ under the assumption of approximate bivariate normality of $(x,y)$.\n\nYour task:\n1. For each undirected edge $\\{u,v\\}$ in a given PPI subgraph, compute the edge weight $w_{uv}$ defined as the Pearson correlation $r$ between the expression profiles of genes $u$ and $v$ across samples.\n2. Apply the Fisher $z$-transformation $z = \\operatorname{arctanh}(r)$ and form the test statistic\n   $$ Z = z \\cdot \\sqrt{n - 3} $$\n   to test the two-sided null hypothesis $H_0: \\rho = 0$ at the given significance level $\\alpha$, where $\\rho$ denotes the population correlation.\n3. Declare an edge significant if $|Z| \\ge z_{1 - \\alpha/2}$, where $z_{1 - \\alpha/2}$ is the $(1 - \\alpha/2)$ quantile of the standard normal distribution. All angles are irrelevant; no physical units apply. Express $\\alpha$ as a decimal (e.g., $0.05$). If numerical evaluation of $\\operatorname{arctanh}(r)$ is undefined because $r = \\pm 1$, use the continuous limit by clipping $r$ to lie within $(-1, 1)$ before applying $\\operatorname{arctanh}$.\n\nData:\n- Number of genes: $4$\n- Number of samples: $8$ ($n = 8$)\n- Gene indices: $0, 1, 2, 3$ correspond to genes $G0, G1, G2, G3$, respectively.\n- Expression profiles (gene-by-sample, each profile listed as a vector of eight sample values):\n  - $G0$: $[1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0]$\n  - $G1$: $[0.8, 1.9, 3.1, 4.0, 5.2, 5.9, 7.1, 8.2]$\n  - $G2$: $[8.2, 6.5, 6.1, 5.0, 4.1, 3.7, 2.8, 1.2]$\n  - $G3$: $[2.0, 5.0, 2.0, 5.0, 2.0, 5.0, 2.0, 5.0]$\n\nTest suite:\nProvide a program that evaluates the following three test cases. Each test case specifies an undirected PPI edge set (as integer index pairs $(u,v)$ with $u \\lt v$) and a significance level $\\alpha$.\n- Test case $1$: edges $E_1 = \\{(0,1),(0,2),(0,3),(1,3),(2,3)\\}$, $\\alpha_1 = 0.05$.\n- Test case $2$: edges $E_2 = \\{(0,1),(0,2),(0,3),(1,3),(2,3)\\}$, $\\alpha_2 = 0.01$.\n- Test case $3$: edges $E_3 = \\{(1,2),(1,3)\\}$, $\\alpha_3 = 0.05$.\n\nOutput specification:\n- For each test case, output the list of all significant edges as a list of integer pairs $[u,v]$ with $u \\lt v$, in ascending lexicographic order by $(u,v)$.\n- Your program should produce a single line of output containing the results for all three test cases as a comma-separated list enclosed in square brackets. Concretely, the final output must be a single line of the form\n  $$[[\\ldots],[\\ldots],[\\ldots]]$$\n  where each inner list is the list of significant edges for the corresponding test case. Each edge is represented as a two-element list of integers. No additional text should be printed.\n\nRequirements:\n- Implement Pearson correlation exactly as defined. Use the Fisher transformation and a two-sided $Z$-test with the standard normal quantile $z_{1-\\alpha/2}$. Assume independence across samples and approximate bivariate normality for the test to hold. Use $n=8$ in the standard error factor $\\sqrt{n-3}$.\n- Handle numerical stability for $r$ near $\\pm 1$ by clipping $r$ to the open interval $(-1,1)$ before computing $\\operatorname{arctanh}(r)$.\n- All computations must be completed without user input or external files.", "solution": "The problem is valid as it is scientifically grounded, well-posed, and objective. It presents a standard computational systems biology task involving the integration of transcriptomic data with protein-protein interaction networks using established statistical methods. All necessary data, formulae, and conditions are provided, and the problem is free of contradictions or ambiguities.\n\nThe solution proceeds by implementing the specified statistical workflow. This involves calculating Pearson correlation coefficients for gene expression profiles corresponding to edges in a protein-protein interaction (PPI) network, followed by a significance test using the Fisher $z$-transformation.\n\nFirst, the provided gene expression data for $4$ genes across $8$ samples is defined. Let the expression profile for gene $i$ be denoted by the vector $G_i$. The number of samples is $n=8$.\n\nThe core of the task is to analyze a set of PPI edges for each of the three test cases. For a given edge $\\{u, v\\}$, representing an interaction between the proteins encoded by genes $u$ and $v$, the following steps are executed:\n\n1.  **Pearson Correlation Coefficient ($w_{uv}$)**: The weight of the edge, $w_{uv}$, is defined as the Pearson correlation coefficient $r$ between the expression profiles of the corresponding genes, $G_u$ and $G_v$. Let $x = G_u$ and $y = G_v$. The correlation $r_{xy}$ is calculated according to the formula:\n    $$ r_{xy} = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^{n} (x_i - \\bar{x})^2} \\sqrt{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}} $$\n    where $\\bar{x}$ and $\\bar{y}$ are the sample means of the expression profiles and $n=8$.\n\n2.  **Fisher's $z$-transformation**: To stabilize the variance and approximate normality, the correlation coefficient $r$ is transformed using the inverse hyperbolic tangent function ($\\operatorname{arctanh}$).\n    $$ z = \\operatorname{arctanh}(r) $$\n    As required, if a calculated correlation $r$ is exactly $\\pm 1$, it is first clipped to the open interval $(-1, 1)$ to ensure $\\operatorname{arctanh}(r)$ is finite.\n\n3.  **$Z$-test for Significance**: The null hypothesis $H_0: \\rho = 0$ (the true population correlation is zero) is tested using the $Z$-statistic. Under $H_0$, the sampling distribution of $z$ is approximately normal with mean $0$ and standard error $1/\\sqrt{n-3}$. The test statistic $Z$ is thus:\n    $$ Z = z \\cdot \\sqrt{n-3} = z \\cdot \\sqrt{8-3} = z \\cdot \\sqrt{5} $$\n    This statistic follows an approximate standard normal distribution, $\\mathcal{N}(0,1)$.\n\n4.  **Decision Rule**: For a given two-sided significance level $\\alpha$, an edge is deemed significant if the absolute value of its $Z$-statistic is greater than or equal to the critical value $z_{1-\\alpha/2}$, which is the $(1-\\alpha/2)$-quantile of the standard normal distribution.\n    $$ |Z| \\ge z_{1-\\alpha/2} $$\n\nThe relevant edges from all test cases are $\\{0,1\\}, \\{0,2\\}, \\{0,3\\}, \\{1,2\\}, \\{1,3\\}, \\{2,3\\}$. The calculated correlation coefficients ($r$) and the absolute values of the $Z$-statistics ($|Z|$) for these edges are as follows:\n-   Edge $\\{0,1\\}$: $r \\approx 0.999$, $|Z| \\approx 8.948$\n-   Edge $\\{0,2\\}$: $r \\approx -0.963$, $|Z| \\approx 4.449$\n-   Edge $\\{0,3\\}$: $r \\approx 0.218$, $|Z| \\approx 0.496$\n-   Edge $\\{1,2\\}$: $r \\approx -0.958$, $|Z| \\approx 4.286$\n-   Edge $\\{1,3\\}$: $r \\approx 0.231$, $|Z| \\approx 0.526$\n-   Edge $\\{2,3\\}$: $r \\approx -0.199$, $|Z| \\approx 0.451$\n\nNow, we evaluate each test case.\n\n**Test Case 1**: Edges $E_1 = \\{(0,1),(0,2),(0,3),(1,3),(2,3)\\}$ and $\\alpha_1 = 0.05$.\nThe critical value is $z_{1 - 0.05/2} = z_{0.975} \\approx 1.960$.\n-   Edge $\\{0,1\\}$: $|Z| \\approx 8.948 \\ge 1.960$. Significant.\n-   Edge $\\{0,2\\}$: $|Z| \\approx 4.449 \\ge 1.960$. Significant.\n-   Edge $\\{0,3\\}$: $|Z| \\approx 0.496 < 1.960$. Not significant.\n-   Edge $\\{1,3\\}$: $|Z| \\approx 0.526 < 1.960$. Not significant.\n-   Edge $\\{2,3\\}$: $|Z| \\approx 0.451 < 1.960$. Not significant.\nThe significant edges, sorted lexicographically, are $[[0,1], [0,2]]$.\n\n**Test Case 2**: Edges $E_2 = \\{(0,1),(0,2),(0,3),(1,3),(2,3)\\}$ and $\\alpha_2 = 0.01$.\nThe critical value is $z_{1 - 0.01/2} = z_{0.995} \\approx 2.576$.\n-   Edge $\\{0,1\\}$: $|Z| \\approx 8.948 \\ge 2.576$. Significant.\n-   Edge $\\{0,2\\}$: $|Z| \\approx 4.449 \\ge 2.576$. Significant.\n-   Edge $\\{0,3\\}$: $|Z| \\approx 0.496 < 2.576$. Not significant.\n-   Edge $\\{1,3\\}$: $|Z| \\approx 0.526 < 2.576$. Not significant.\n-   Edge $\\{2,3\\}$: $|Z| \\approx 0.451 < 2.576$. Not significant.\nThe significant edges, sorted lexicographically, are $[[0,1], [0,2]]$.\n\n**Test Case 3**: Edges $E_3 = \\{(1,2),(1,3)\\}$ and $\\alpha_3 = 0.05$.\nThe critical value is $z_{1 - 0.05/2} = z_{0.975} \\approx 1.960$.\n-   Edge $\\{1,2\\}$: $|Z| \\approx 4.286 \\ge 1.960$. Significant.\n-   Edge $\\{1,3\\}$: $|Z| \\approx 0.526 < 1.960$. Not significant.\nThe significant edge is $[[1,2]]$.\n\nCombining these results yields the final list for all three test cases.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Computes significant gene co-expression edges based on Pearson correlation\n    and Fisher's z-transformation test.\n    \"\"\"\n\n    # --- Data Definition ---\n    # Number of samples\n    n = 8\n\n    # Gene expression data matrix (4 genes x 8 samples)\n    gene_data = np.array([\n        [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0],  # G0\n        [0.8, 1.9, 3.1, 4.0, 5.2, 5.9, 7.1, 8.2],  # G1\n        [8.2, 6.5, 6.1, 5.0, 4.1, 3.7, 2.8, 1.2],  # G2\n        [2.0, 5.0, 2.0, 5.0, 2.0, 5.0, 2.0, 5.0]   # G3\n    ])\n\n    # Test suite definition\n    test_cases = [\n        {'edges': [(0, 1), (0, 2), (0, 3), (1, 3), (2, 3)], 'alpha': 0.05},\n        {'edges': [(0, 1), (0, 2), (0, 3), (1, 3), (2, 3)], 'alpha': 0.01},\n        {'edges': [(1, 2), (1, 3)], 'alpha': 0.05}\n    ]\n\n    # Factor for Z-statistic calculation\n    z_stat_factor = np.sqrt(n - 3)\n\n    all_results = []\n\n    # --- Processing Logic ---\n    for case in test_cases:\n        edges_to_test = case['edges']\n        alpha = case['alpha']\n\n        # Determine the critical value from the standard normal distribution\n        z_critical = norm.ppf(1 - alpha / 2.0)\n        \n        significant_edges = []\n        for u, v in edges_to_test:\n            x = gene_data[u]\n            y = gene_data[v]\n\n            # 1. Compute Pearson correlation coefficient as defined in the problem\n            mean_x = np.mean(x)\n            mean_y = np.mean(y)\n            \n            numerator = np.sum((x - mean_x) * (y - mean_y))\n            \n            sum_sq_x = np.sum((x - mean_x)**2)\n            sum_sq_y = np.sum((y - mean_y)**2)\n            denominator = np.sqrt(sum_sq_x * sum_sq_y)\n            \n            if denominator == 0:\n                r = 0.0\n            else:\n                r = numerator / denominator\n\n            # 2. Apply Fisher z-transformation and form the Z-statistic\n            # Clip r to be in (-1, 1) to handle r = +/-1 case for arctanh\n            r_clipped = np.clip(r, np.nextafter(-1.0, 1.0), np.nextafter(1.0, -1.0))\n            \n            z = np.arctanh(r_clipped)\n            Z_stat = z * z_stat_factor\n\n            # 3. Declare significance based on the two-sided test\n            if np.abs(Z_stat) >= z_critical:\n                significant_edges.append([u, v])\n        \n        # Sort the list of significant edges lexicographically\n        significant_edges.sort()\n        all_results.append(significant_edges)\n\n    # --- Final Output ---\n    # Format the results into the required single-line string format\n    # e.g., [[[0,1],[0,2]],[[0,1],[0,2]],[[1,2]]]\n    print(str(all_results).replace(' ', ''))\n\nsolve()\n\n```", "id": "3320742"}, {"introduction": "Moving beyond local edge analysis, network propagation methods leverage the global topology of the PPI network to identify active modules. This exercise introduces the concept of Laplacian smoothing, a powerful technique that diffuses signals (like gene expression scores) from high-confidence nodes to their neighbors across the graph [@problem_id:3320715]. By implementing this method, you will see how network structure can be used to smooth noisy data and highlight entire regions of the network that are coherently active, providing a more robust view than isolated node or edge scores alone.", "problem": "Consider the integration of transcriptomic measurements with a Protein-Protein Interaction (PPI) network, represented as an undirected weighted graph. Let the graph have $5$ nodes corresponding to genes, and let $\\mathbf{A} \\in \\mathbb{R}^{5 \\times 5}$ denote its symmetric adjacency matrix. Let $\\mathbf{x} \\in \\mathbb{R}^{5}$ denote the transcript abundance vector (for example, log-transformed or normalized counts) mapped to the same nodes. Use the following formal definitions as the fundamental base:\n- Degree centrality of node $i$ is the sum of weights of edges incident to $i$, which for an adjacency matrix is $\\sum_{j=1}^{5} A_{ij}$.\n- Eigenvector centrality is defined as the (principal) eigenvector $\\mathbf{v}$ associated with the largest eigenvalue of $\\mathbf{A}$, satisfying $\\mathbf{A}\\mathbf{v} = \\mu_{\\max}\\mathbf{v}$, and normalized to have Euclidean norm $1$.\n- The graph Laplacian is $\\mathbf{L} = \\mathbf{D} - \\mathbf{A}$, where $\\mathbf{D}$ is the diagonal matrix with $D_{ii} = \\sum_{j=1}^{5} A_{ij}$.\n- The Laplacian smoothing objective combines the transcript signal with network structure as $J(\\mathbf{f}) = \\|\\mathbf{f} - \\mathbf{x}\\|_2^2 + \\lambda \\,\\mathbf{f}^{\\top}\\mathbf{L}\\mathbf{f}$, where $\\lambda > 0$ controls the strength of smoothness.\n\nYour task is to write a complete, runnable program that, for each test case, computes:\n1. The degree centrality for each node.\n2. The eigenvector centrality for each node, defined as the unit-norm principal eigenvector of $\\mathbf{A}$. To resolve the sign indeterminacy, choose the representative with $\\sum_{i=1}^{5} v_i \\ge 0$.\n3. The Laplacian-smoothed scores $\\mathbf{f}^{\\star}$ for $\\lambda = 0.5$, defined as the unique minimizer of $J(\\mathbf{f})$ for the given $\\mathbf{A}$ and $\\mathbf{x}$.\n\nReturn all numeric outputs rounded to six decimal places.\n\nTest Suite:\n- Case $1$ (connected, unweighted):\n  $$\\mathbf{A}_1 = \\begin{bmatrix}\n  0 & 1 & 1 & 0 & 0 \\\\\n  1 & 0 & 1 & 1 & 0 \\\\\n  1 & 1 & 0 & 0 & 1 \\\\\n  0 & 1 & 0 & 0 & 0 \\\\\n  0 & 0 & 1 & 0 & 0\n  \\end{bmatrix}, \\quad\n  \\mathbf{x}_1 = \\begin{bmatrix} 2.0 \\\\ 0.5 \\\\ 1.2 \\\\ 0.0 \\\\ 3.0 \\end{bmatrix}, \\quad \\lambda = 0.5.$$\n- Case $2$ (isolated node present):\n  $$\\mathbf{A}_2 = \\begin{bmatrix}\n  0 & 1 & 0 & 0 & 0 \\\\\n  1 & 0 & 1 & 0 & 0 \\\\\n  0 & 1 & 0 & 0 & 1 \\\\\n  0 & 0 & 0 & 0 & 0 \\\\\n  0 & 0 & 1 & 0 & 0\n  \\end{bmatrix}, \\quad\n  \\mathbf{x}_2 = \\begin{bmatrix} 0.0 \\\\ 1.0 \\\\ 4.0 \\\\ -1.0 \\\\ 0.5 \\end{bmatrix}, \\quad \\lambda = 0.5.$$\n- Case $3$ (disconnected, weighted edges):\n  $$\\mathbf{A}_3 = \\begin{bmatrix}\n  0 & 1 & 0.5 & 0 & 0 \\\\\n  1 & 0 & 1 & 0 & 0 \\\\\n  0.5 & 1 & 0 & 0 & 0 \\\\\n  0 & 0 & 0 & 0 & 2 \\\\\n  0 & 0 & 0 & 2 & 0\n  \\end{bmatrix}, \\quad\n  \\mathbf{x}_3 = \\begin{bmatrix} 1.0 \\\\ 0.0 \\\\ 0.0 \\\\ 5.0 \\\\ 0.0 \\end{bmatrix}, \\quad \\lambda = 0.5.$$\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each test case, output a list of three lists: the degree centralities for nodes $1$ through $5$, the eigenvector centralities for nodes $1$ through $5$, and the Laplacian-smoothed scores $\\mathbf{f}^{\\star}$ for nodes $1$ through $5$. Concretely, the final output must be of the form\n$$\n[\\,[\\,[d^{(1)}_1,\\dots,d^{(1)}_5],\\,[v^{(1)}_1,\\dots,v^{(1)}_5],\\,[f^{\\star (1)}_1,\\dots,f^{\\star (1)}_5]\\,],\\,\\,[\\,[d^{(2)}_1,\\dots,d^{(2)}_5],\\,[v^{(2)}_1,\\dots,v^{(2)}_5],\\,[f^{\\star (2)}_1,\\dots,f^{\\star (2)}_5]\\,],\\,\\,[\\,[d^{(3)}_1,\\dots,d^{(3)}_5],\\,[v^{(3)}_1,\\dots,v^{(3)}_5],\\,[f^{\\star (3)}_1,\\dots,f^{\\star (3)}_5]\\,]\\,]\n$$\nwith all numeric entries rounded to six decimal places.", "solution": "The problem is scientifically well-posed and contains all necessary information for a unique solution. We proceed with the derivation and computation for the three required network metrics. All mathematical entities are rendered in LaTeX as per the specified formatting rules.\n\n### 1. Degree Centrality\n\nDegree centrality is a measure of the direct influence of a node in a network. For a weighted, undirected graph represented by a symmetric adjacency matrix $\\mathbf{A}$, the degree centrality $d_i$ of node $i$ is defined as the sum of the weights of all edges connected to it. This corresponds to the sum of the elements in the $i$-th row (or column) of $\\mathbf{A}$.\n\n$$\nd_i = \\sum_{j=1}^{5} A_{ij}\n$$\n\nThe degree centrality vector $\\mathbf{d}$ is therefore computed by summing the rows of the adjacency matrix $\\mathbf{A}$.\n\n### 2. Eigenvector Centrality\n\nEigenvector centrality extends the concept of degree centrality by considering not just the number or weight of connections, but also the importance of the connected nodes. A node's eigenvector centrality is proportional to the sum of the centralities of its neighbors. This recursive definition leads to the formulation of an eigenvector problem. The centrality vector $\\mathbf{v}$ is the principal eigenvector of the adjacency matrix $\\mathbf{A}$, corresponding to the largest eigenvalue $\\mu_{\\max}$:\n\n$$\n\\mathbf{A}\\mathbf{v} = \\mu_{\\max}\\mathbf{v}\n$$\n\nFor a non-negative, symmetric matrix $\\mathbf{A}$ representing an undirected graph, the Perron-Frobenius theorem (or its extensions for reducible matrices) guarantees that $\\mu_{\\max}$ is real and non-negative, and a corresponding non-negative eigenvector $\\mathbf{v}$ exists.\n\nThe computational procedure is as follows:\n1.  Compute the eigenvalues and eigenvectors of the matrix $\\mathbf{A}$.\n2.  Identify the largest eigenvalue, $\\mu_{\\max}$.\n3.  Select the corresponding eigenvector, $\\mathbf{v}_{\\text{raw}}$.\n4.  Normalize the eigenvector to have a Euclidean norm of $1$: $\\mathbf{v} = \\mathbf{v}_{\\text{raw}} / \\|\\mathbf{v}_{\\text{raw}}\\|_2$. Standard numerical libraries typically return normalized eigenvectors.\n5.  Resolve sign ambiguity. An eigenvector $\\mathbf{v}$ is defined only up to a scalar multiple. If $\\mathbf{v}$ is an eigenvector, so is $-\\mathbf{v}$. The problem specifies a convention: choose the vector representation such that the sum of its components is non-negative. If $\\sum_{i=1}^{5} v_i < 0$, the vector is replaced by $-\\mathbf{v}$. Otherwise, it is used as is.\n\n### 3. Laplacian-Smoothed Scores\n\nLaplacian smoothing, also known as network diffusion or heat kernel smoothing, integrates nodal attributes (the vector $\\mathbf{x}$) with the graph topology to produce smoothed scores $\\mathbf{f}$ that reflect both the initial signal and network proximity. The process is formulated as an optimization problem, minimizing the objective function $J(\\mathbf{f})$:\n\n$$\nJ(\\mathbf{f}) = \\underbrace{\\|\\mathbf{f} - \\mathbf{x}\\|_2^2}_{\\text{Fidelity to signal}} + \\underbrace{\\lambda \\,\\mathbf{f}^{\\top}\\mathbf{L}\\mathbf{f}}_{\\text{Smoothness on graph}}\n$$\n\nHere, $\\mathbf{L} = \\mathbf{D} - \\mathbf{A}$ is the graph Laplacian, where $\\mathbf{D}$ is the diagonal degree matrix with $D_{ii} = \\sum_j A_{ij}$. The term $\\mathbf{f}^{\\top}\\mathbf{L}\\mathbf{f}$ is the Laplacian quadratic form, which can be written as $\\mathbf{f}^{\\top}\\mathbf{L}\\mathbf{f} = \\frac{1}{2}\\sum_{i,j} A_{ij}(f_i - f_j)^2$. This term penalizes large differences in scores between connected nodes. The parameter $\\lambda > 0$ balances the trade-off between fitting the original data $\\mathbf{x}$ and enforcing smoothness across the network.\n\nTo find the unique minimizer $\\mathbf{f}^{\\star}$, we compute the gradient of $J(\\mathbf{f})$ with respect to $\\mathbf{f}$ and set it to the zero vector.\nThe objective function can be expanded as:\n$J(\\mathbf{f}) = (\\mathbf{f} - \\mathbf{x})^{\\top}(\\mathbf{f} - \\mathbf{x}) + \\lambda \\mathbf{f}^{\\top}\\mathbf{L}\\mathbf{f} = \\mathbf{f}^{\\top}\\mathbf{f} - 2\\mathbf{f}^{\\top}\\mathbf{x} + \\mathbf{x}^{\\top}\\mathbf{x} + \\lambda \\mathbf{f}^{\\top}\\mathbf{L}\\mathbf{f}$.\n\nThe gradient is:\n$$\n\\nabla_{\\mathbf{f}} J(\\mathbf{f}) = 2\\mathbf{f} - 2\\mathbf{x} + 2\\lambda\\mathbf{L}\\mathbf{f}\n$$\nSetting the gradient to zero gives:\n$$\n2\\mathbf{f}^{\\star} - 2\\mathbf{x} + 2\\lambda\\mathbf{L}\\mathbf{f}^{\\star} = \\mathbf{0} \\\\\n\\implies \\mathbf{f}^{\\star} + \\lambda\\mathbf{L}\\mathbf{f}^{\\star} = \\mathbf{x} \\\\\n\\implies (\\mathbf{I} + \\lambda\\mathbf{L})\\mathbf{f}^{\\star} = \\mathbf{x}\n$$\nwhere $\\mathbf{I}$ is the $5 \\times 5$ identity matrix.\n\nThe graph Laplacian $\\mathbf{L}$ is positive semi-definite, meaning all its eigenvalues $\\gamma_i$ are non-negative ($\\gamma_i \\ge 0$). The eigenvalues of the matrix $(\\mathbf{I} + \\lambda\\mathbf{L})$ are therefore $(1 + \\lambda\\gamma_i)$. Since $\\lambda > 0$, all $(1 + \\lambda\\gamma_i) \\ge 1$, so they are strictly positive. A matrix with no zero eigenvalues is invertible. Thus, $(\\mathbf{I} + \\lambda\\mathbf{L})$ is always invertible, guaranteeing a unique solution $\\mathbf{f}^{\\star}$ for any $\\mathbf{x}$.\n\nThe smoothed scores $\\mathbf{f}^{\\star}$ are found by solving this system of linear equations:\n$$\n\\mathbf{f}^{\\star} = (\\mathbf{I} + \\lambda\\mathbf{L})^{-1}\\mathbf{x}\n$$\nThis is computationally handled using a standard linear solver.", "answer": "```python\nimport numpy as np\n\ndef compute_metrics(A, x, lambda_val):\n    \"\"\"\n    Computes degree centrality, eigenvector centrality, and Laplacian-smoothed scores.\n\n    Args:\n        A (np.ndarray): The 5x5 adjacency matrix.\n        x (np.ndarray): The 5-element transcript abundance vector.\n        lambda_val (float): The smoothing parameter.\n\n    Returns:\n        tuple[list[float], list[float], list[float]]: A tuple containing the\n        lists of degree centralities, eigenvector centralities, and smoothed scores.\n    \"\"\"\n    # 1. Degree Centrality\n    # Degree centrality of node i is the sum of the weights of its incident edges.\n    # This is equivalent to the sum of the i-th row of the adjacency matrix.\n    degree_centrality = np.sum(A, axis=1).tolist()\n\n    # 2. Eigenvector Centrality\n    # Defined as the principal eigenvector of the adjacency matrix A.\n    eigenvalues, eigenvectors = np.linalg.eig(A)\n    # Find the index of the largest eigenvalue\n    max_eigenvalue_idx = np.argmax(eigenvalues)\n    # Get the corresponding eigenvector (principal eigenvector)\n    # eigenvectors are stored as columns in the returned matrix.\n    principal_eigenvector = eigenvectors[:, max_eigenvalue_idx]\n    \n    # Resolve sign indeterminacy: sum of components must be non-negative.\n    if np.sum(principal_eigenvector) < 0:\n        principal_eigenvector = -principal_eigenvector\n    \n    eigenvector_centrality = principal_eigenvector.tolist()\n\n    # 3. Laplacian-Smoothed Scores\n    # We solve the linear system (I + lambda*L)f* = x\n    # First, construct the graph Laplacian L = D - A\n    D = np.diag(np.sum(A, axis=1))\n    L = D - A\n    \n    # Construct the matrix M = I + lambda*L\n    I = np.identity(A.shape[0])\n    M = I + lambda_val * L\n    \n    # Solve M * f_star = x for f_star\n    f_star = np.linalg.solve(M, x)\n    laplacian_scores = f_star.tolist()\n    \n    return degree_centrality, eigenvector_centrality, laplacian_scores\n\ndef solve():\n    \"\"\"\n    Main function to process all test cases and print the final result.\n    \"\"\"\n    lambda_val = 0.5\n\n    # Test Suite\n    test_cases = [\n        # Case 1\n        (\n            np.array([\n                [0, 1, 1, 0, 0],\n                [1, 0, 1, 1, 0],\n                [1, 1, 0, 0, 1],\n                [0, 1, 0, 0, 0],\n                [0, 0, 1, 0, 0]\n            ]),\n            np.array([2.0, 0.5, 1.2, 0.0, 3.0])\n        ),\n        # Case 2\n        (\n            np.array([\n                [0, 1, 0, 0, 0],\n                [1, 0, 1, 0, 0],\n                [0, 1, 0, 0, 1],\n                [0, 0, 0, 0, 0],\n                [0, 0, 1, 0, 0]\n            ]),\n            np.array([0.0, 1.0, 4.0, -1.0, 0.5])\n        ),\n        # Case 3\n        (\n            np.array([\n                [0, 1, 0.5, 0, 0],\n                [1, 0, 1, 0, 0],\n                [0.5, 1, 0, 0, 0],\n                [0, 0, 0, 0, 2],\n                [0, 0, 0, 2, 0]\n            ]),\n            np.array([1.0, 0.0, 0.0, 5.0, 0.0])\n        )\n    ]\n\n    all_results_str = []\n    for A, x in test_cases:\n        d, v, f = compute_metrics(A, x, lambda_val)\n\n        # Format each list of numbers into the required string format,\n        # rounding to six decimal places.\n        d_str = f\"[{','.join([f'{val:.6f}' for val in d])}]\"\n        v_str = f\"[{','.join([f'{val:.6f}' for val in v])}]\"\n        f_str = f\"[{','.join([f'{val:.6f}' for val in f])}]\"\n        \n        # Combine the three lists for the current case.\n        case_str = f\"[{d_str},{v_str},{f_str}]\"\n        all_results_str.append(case_str)\n\n    # Combine all case results into the final output string.\n    final_output = f\"[{','.join(all_results_str)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "3320715"}, {"introduction": "Advanced systems biology often formulates data integration as a formal optimization problem, allowing for the simultaneous balancing of multiple biological objectives. This practice introduces a sophisticated multi-objective optimization framework to infer latent protein activities from transcriptomic data [@problem_id:3320671]. You will implement a proximal gradient descent algorithm to find solutions that not only fit the expression data but are also sparse and coherent with the underlying PPI network, exploring the trade-offs between these competing goals via an approximate Pareto front.", "problem": "You are given a computational formulation for integrating transcriptomics with a protein-protein interaction network to estimate latent protein activity. Let $x \\in \\mathbb{R}^m$ denote a transcript-level log expression vector, $B \\in \\mathbb{R}^{m \\times n}$ a linear mapping from protein activity to transcript abundance (for example, a linearized regulatory influence matrix), and $w \\in \\mathbb{R}^n$ a vector of latent protein activity levels. Let $A \\in \\mathbb{R}^{n \\times n}$ be a symmetric binary adjacency matrix representing an undirected protein-protein interaction network, $D = \\mathrm{diag}(d_1,\\dots,d_n)$ the diagonal degree matrix with $d_i = \\sum_j A_{ij}$, and $L = D - A$ the combinatorial graph Laplacian. To penalize activation of isolated proteins (nodes with zero degree), define the isolation penalty matrix $R = \\mathrm{diag}(r_1,\\dots,r_n)$ with $r_i = 1$ if $d_i = 0$ and $r_i = 0$ otherwise, and the coherence matrix $Q = L + R$.\n\nConsider the three objectives:\n- Data fit: $f_1(w) = \\|x - B w\\|_2$,\n- Sparsity: $f_2(w) = \\|w\\|_1$,\n- Network coherence: $f_3(w) = \\sqrt{w^\\top Q w}$.\n\nYour task is to study multi-objective trade-offs among $f_1$, $f_2$, and $f_3$ using a weighted-sum scalarization. For scalar weights $(\\alpha,\\beta,\\gamma)$ with $\\alpha,\\beta,\\gamma \\ge 0$ and not all zero, define the surrogate scalar objective\n$$\nJ(w;\\alpha,\\beta,\\gamma) = \\frac{\\alpha}{2}\\,\\|x - B w\\|_2^2 + \\frac{\\gamma}{2}\\,w^\\top Q w + \\beta\\,\\|w\\|_1.\n$$\nNote that minimizing $\\|x - B w\\|_2$ is equivalent in terms of minimizers to minimizing $\\tfrac{1}{2}\\|x - B w\\|_2^2$ since the square-root is monotonically increasing on $[0,\\infty)$; therefore the surrogate objective yields the same minimizers for the fit component. For each given weight triple $(\\alpha,\\beta,\\gamma)$, define $w^\\star(\\alpha,\\beta,\\gamma)$ as any minimizer of $J(\\cdot;\\alpha,\\beta,\\gamma)$.\n\nImplement an algorithm that, for each test case and each provided weight triple, computes an approximate minimizer $w^\\star(\\alpha,\\beta,\\gamma)$ using proximal-gradient descent on the smooth part $\\tfrac{\\alpha}{2}\\|x - B w\\|_2^2 + \\tfrac{\\gamma}{2} w^\\top Q w$ with the non-smooth part $\\beta\\|w\\|_1$ handled by soft-thresholding. Use a fixed step size based on a Lipschitz constant $L_s = \\alpha\\,\\|B\\|_2^2 + \\gamma\\,\\|Q\\|_2$, where $\\|\\cdot\\|_2$ denotes the spectral norm, and iterate until the relative change in $w$ is below $10^{-8}$ or a maximum of $5000$ iterations is reached. In the edge case where $L_s = 0$, return $w^\\star = 0$ as the minimizer of the purely non-smooth term $\\beta\\|w\\|_1$.\n\nFor each computed $w^\\star(\\alpha,\\beta,\\gamma)$, evaluate the objective triplet\n$$\n\\big(f_1(w^\\star),\\; f_2(w^\\star),\\; f_3(w^\\star)\\big) = \\left(\\|x - B w^\\star\\|_2,\\; \\|w^\\star\\|_1,\\; \\sqrt{(w^\\star)^\\top Q w^\\star}\\right).\n$$\n\nThen, for the set of triplets corresponding to the provided weight grid within each test case, compute the set of non-dominated points (an approximate Pareto front): a point is non-dominated if there is no other point whose three objective values are all less than or equal to it with at least one strictly less. Report the non-dominated triplets after sorting them lexicographically by $(f_1,f_2,f_3)$ in ascending order. For numerical determinism, round each number in each triplet to $6$ decimal places in the final report. All computations are unitless and require no physical unit conversions.\n\nTest Suite. For each test case below, use the given matrices and vectors exactly as specified. Each case lists a set of weight triples $(\\alpha,\\beta,\\gamma)$ to evaluate.\n\n- Case 1:\n    - Network size: $n = 5$, transcripts $m = 3$.\n    - Adjacency $A$ (chain graph $1\\!-\\!2\\!-\\!3\\!-\\!4\\!-\\!5$): $A_{12} = A_{21} = A_{23} = A_{32} = A_{34} = A_{43} = A_{45} = A_{54} = 1$, others $0$.\n    - Mapping $B \\in \\mathbb{R}^{3 \\times 5}$:\n      $$\n      B = \\begin{bmatrix}\n      1.0 & 0.5 & 0.0 & 0.0 & 0.2 \\\\\n      0.0 & 0.3 & 0.8 & 0.1 & 0.0 \\\\\n      0.2 & 0.0 & 0.2 & 0.7 & 0.4\n      \\end{bmatrix}\n      $$\n    - Transcript vector $x \\in \\mathbb{R}^3$: $x = [1.2,\\,0.5,\\,0.8]^\\top$.\n    - Weights: $\\{(1.0,0.0,0.0),\\,(0.6,0.4,0.0),\\,(0.6,0.0,0.4),\\,(0.34,0.33,0.33),\\,(0.0,1.0,0.0),\\,(0.0,0.2,0.8)\\}$.\n\n- Case 2:\n    - Network size: $n = 4$, transcripts $m = 4$.\n    - Adjacency $A$ (path $1\\!-\\!2\\!-\\!3$ and isolated node $4$): $A_{12} = A_{21} = A_{23} = A_{32} = 1$, others $0$.\n    - Mapping $B = I_4$ (the $4 \\times 4$ identity).\n    - Transcript vector $x \\in \\mathbb{R}^4$: $x = [0.0,\\,0.0,\\,1.0,\\,0.2]^\\top$.\n    - Weights: $\\{(1.0,0.0,0.0),\\,(0.5,0.0,0.5),\\,(0.5,0.5,0.0),\\,(0.0,1.0,0.0),\\,(0.0,0.0,1.0)\\}$.\n\n- Case 3:\n    - Network size: $n = 6$, transcripts $m = 3$.\n    - Adjacency $A$ (triangle on nodes $\\{1,2,3\\}$ and path $4\\!-\\!5\\!-\\!6$):\n      $A_{12} = A_{21} = A_{13} = A_{31} = A_{23} = A_{32} = 1$ and $A_{45} = A_{54} = A_{56} = A_{65} = 1$, others $0$.\n    - Mapping $B \\in \\mathbb{R}^{3 \\times 6}$:\n      $$\n      B = \\begin{bmatrix}\n      0.7 & 0.6 & 0.0 & 0.0 & 0.0 & 0.0 \\\\\n      0.0 & 0.2 & 0.9 & 0.5 & 0.0 & 0.0 \\\\\n      0.0 & 0.0 & 0.0 & 0.3 & 0.8 & 0.6\n      \\end{bmatrix}\n      $$\n    - Transcript vector $x \\in \\mathbb{R}^3$: $x = [1.0,\\,0.7,\\,1.1]^\\top$.\n    - Weights: $\\{(1.0,0.0,0.0),\\,(0.3,0.6,0.1),\\,(0.3,0.1,0.6),\\,(0.0,1.0,0.0),\\,(0.0,0.2,0.8)\\}$.\n\nImplementation and Output Requirements.\n- Implement proximal-gradient descent with soft-thresholding using the step size $t = 1/L_s$ with $L_s = \\alpha\\,\\|B\\|_2^2 + \\gamma\\,\\|Q\\|_2$ and a conservative factor if desired for numerical stability. Use the soft-thresholding operator $\\mathrm{soft}(u,\\tau)_i = \\mathrm{sign}(u_i)\\cdot \\max\\{|u_i| - \\tau, 0\\}$.\n- For each test case, compute the list of objective triplets for all specified weights, filter to the non-dominated subset, sort lexicographically by $(f_1,f_2,f_3)$ ascending, and round all numbers to $6$ decimals.\n- Your program should produce a single line of output containing the results for all test cases as a list of three lists (one per test case), where each inner list contains the sorted non-dominated triplets represented as lists of three floats. For example: \"[[[f11,f12,f13],[...],...],[[...]],[[...]]]\". No other text should be printed.", "solution": "The user has provided a computational problem in the field of systems biology, which requires the implementation of an optimization algorithm to explore trade-offs between three competing objectives. The problem is scientifically and mathematically well-posed. The aforestated validation procedure is therefore passed, and a complete solution can be provided.\n\nThe problem requires us to find an approximate Pareto front for a multi-objective optimization problem. The three objectives to be minimized are:\n1.  Data fit, $f_1(w) = \\|x - B w\\|_2$, which quantifies the discrepancy between the observed transcript-level expression data $x \\in \\mathbb{R}^m$ and the expression predicted by the latent protein activities $w \\in \\mathbb{R}^n$ through the linear model $B \\in \\mathbb{R}^{m \\times n}$.\n2.  Sparsity, $f_2(w) = \\|w\\|_1$, which encourages solutions where most latent protein activities are zero, corresponding to a simpler model where fewer proteins are considered active.\n3.  Network coherence, $f_3(w) = \\sqrt{w^\\top Q w}$, which penalizes solutions where the activity levels of proteins connected in the interaction network are dissimilar. The coherence matrix is defined as $Q = L + R$, where $L=D-A$ is the graph Laplacian of the protein interaction network adjacency matrix $A$, and $R$ is a diagonal matrix that penalizes activity of proteins with no interactions (degree $d_i=0$).\n\nTo explore the trade-offs, we employ a weighted-sum scalarization method. The three objectives are combined into a single surrogate objective function $J(w;\\alpha,\\beta,\\gamma)$ for a given set of non-negative weights $(\\alpha, \\beta, \\gamma)$:\n$$\nJ(w;\\alpha,\\beta,\\gamma) = \\frac{\\alpha}{2}\\,\\|x - B w\\|_2^2 + \\beta\\,\\|w\\|_1 + \\frac{\\gamma}{2}\\,w^\\top Q w.\n$$\nNote that the quadratic forms for $f_1$ and $f_3$ are used in $J(w)$ because they share the same minimizers as their square-root counterparts and result in a more tractable optimization problem. Specifically, the function $z \\mapsto z^2$ is strictly increasing for non-negative arguments.\n\nThe objective function $J(w)$ is convex, as it is a non-negative weighted sum of convex functions. We can decompose $J(w)$ into a smooth, differentiable part $g(w)$ and a non-smooth part $h(w)$:\n$$\ng(w) = \\frac{\\alpha}{2}\\|x - B w\\|_2^2 + \\frac{\\gamma}{2}w^\\top Q w\n$$\n$$\nh(w) = \\beta\\|w\\|_1\n$$\nThis structure makes $J(w)$ amenable to minimization via the Proximal Gradient Descent (PGD) algorithm, also known as the Iterative Shrinkage-Thresholding Algorithm (ISTA) in this context. The core idea of PGD is to iterate a gradient descent step on the smooth part followed by a proximal mapping step for the non-smooth part.\n\nThe gradient of the smooth part, $\\nabla g(w)$, is:\n$$\n\\nabla g(w) = \\alpha B^\\top(Bw - x) + \\gamma Qw\n$$\nThe proximal operator for the $L_1$-norm term $h(w)$ is the soft-thresholding operator, defined as:\n$$\n\\mathrm{prox}_{t h}(u) = \\mathrm{soft}(u, t\\beta)_i = \\mathrm{sign}(u_i)\\cdot \\max\\{|u_i| - t\\beta, 0\\},\n$$\nwhere $t$ is the algorithm step size.\n\nThe PGD iteration is thus given by:\n$$\nw_{k+1} = \\mathrm{prox}_{t h}(w_k - t \\nabla g(w_k))\n$$\nFor the algorithm to converge, the step size $t$ must be chosen such that $0 < t \\le 1/L_s$, where $L_s$ is the Lipschitz constant of the gradient $\\nabla g(w)$. The problem specifies using an upper bound for this constant:\n$$\nL_s = \\alpha\\,\\|B\\|_2^2 + \\gamma\\,\\|Q\\|_2,\n$$\nwhere $\\|\\cdot\\|_2$ denotes the spectral norm (largest singular value) of a matrix. We will use the step size $t = 1/L_s$. The iteration starts from an initial guess, typically $w_0 = \\vec{0}$, and continues until the relative change in the vector $w$ falls below a tolerance (e.g., $10^{-8}$) or a maximum number of iterations is reached.\n\nFor each given weight triple $(\\alpha, \\beta, \\gamma)$, we execute this PGD algorithm to find a minimizer $w^\\star$. Then, we evaluate the original three objective functions $(f_1(w^\\star), f_2(w^\\star), f_3(w^\\star))$ to obtain a point in the objective space.\n\nAfter computing these triplets for all specified weight combinations, we must identify the set of non-dominated points. A point $p$ is non-dominated if no other computed point $p'$ is better or equal in all objectives and strictly better in at least one. That is, there is no $p' \\neq p$ such that $p'_i \\leq p_i$ for all three objectives $i \\in \\{1, 2, 3\\}$. This set of non-dominated points forms an approximation of the true Pareto front.\n\nFinally, for each test case, this set of non-dominated triplets is sorted lexicographically in ascending order. The numbers within each triplet are rounded to $6$ decimal places for the final report. The overall algorithm is implemented for each provided test case, and the results are aggregated into the specified output format.", "answer": "```python\nimport numpy as np\n\ndef soft_thresholding(u, tau):\n    \"\"\"Soft-thresholding operator.\"\"\"\n    return np.sign(u) * np.maximum(np.abs(u) - tau, 0)\n\ndef proximal_gradient_descent(alpha, beta, gamma, B, x, Q, B_norm_sq, Q_norm):\n    \"\"\"\n    Minimizes the objective J(w) using proximal gradient descent.\n    J(w) = (alpha/2)*||x - Bw||^2 + beta*||w||_1 + (gamma/2)*w^T*Q*w\n    \"\"\"\n    n = Q.shape[0]\n    \n    # Lipschitz constant of the gradient of the smooth part\n    Ls = alpha * B_norm_sq + gamma * Q_norm\n\n    # Handle edge case where the smooth part is absent\n    if Ls == 0:\n        # Objective is beta*||w||_1, minimized at w=0\n        return np.zeros(n)\n\n    t = 1.0 / Ls\n    \n    max_iter = 5000\n    tol = 1e-8\n    \n    w = np.zeros(n)\n    \n    # Pre-compute constant matrix products for efficiency\n    BtB = B.T @ B\n    Btx = B.T @ x\n    \n    for _ in range(max_iter):\n        w_prev = w.copy()\n        \n        # Gradient of the smooth part: grad(g(w)) = alpha*B^T(Bw-x) + gamma*Qw\n        grad_g = alpha * (BtB @ w - Btx) + gamma * (Q @ w)\n        \n        # Gradient descent step for the smooth part\n        u = w - t * grad_g\n        \n        # Proximal step for the non-smooth part (L1 norm)\n        w = soft_thresholding(u, t * beta)\n        \n        # Check for convergence\n        norm_w_prev = np.linalg.norm(w_prev)\n        if np.linalg.norm(w - w_prev) / (norm_w_prev + np.finfo(float).eps) < tol:\n            break\n            \n    return w\n\ndef find_non_dominated(points):\n    \"\"\"\n    Filters a list of points to find the non-dominated subset.\n    A point p1 is dominated if another point p2 exists such that\n    all(p2 <= p1) and any(p2 < p1).\n    \"\"\"\n    num_points = len(points)\n    if num_points == 0:\n        return []\n    \n    points_arr = np.array(points)\n    is_dominated = np.zeros(num_points, dtype=bool)\n\n    for i in range(num_points):\n        for j in range(num_points):\n            if i == j:\n                continue\n            if np.all(points_arr[j] <= points_arr[i]) and np.any(points_arr[j] < points_arr[i]):\n                is_dominated[i] = True\n                break\n    \n    return [points[i] for i in range(num_points) if not is_dominated[i]]\n\ndef solve_case(A, B, x, weights_grid):\n    \"\"\"\n    Solves a single test case from problem setup to final Pareto front.\n    \"\"\"\n    n = A.shape[0]\n    \n    # Pre-computation of matrices\n    d = A.sum(axis=1)\n    D = np.diag(d)\n    L = D - A\n    r = (d == 0).astype(float)\n    R = np.diag(r)\n    Q = L + R\n    \n    # Pre-compute squared spectral norm of B and spectral norm of Q\n    B_norm_sq = np.linalg.norm(B, ord=2)**2\n    Q_norm = np.linalg.norm(Q, ord=2)\n\n    objective_triplets = []\n    \n    for alpha, beta, gamma in weights_grid:\n        w_star = proximal_gradient_descent(alpha, beta, gamma, B, x, Q, B_norm_sq, Q_norm)\n        \n        f1 = np.linalg.norm(x - B @ w_star)\n        f2 = np.linalg.norm(w_star, ord=1)\n        f3_sq = w_star.T @ Q @ w_star\n        f3 = np.sqrt(f3_sq) if f3_sq > 0 else 0.0\n        \n        objective_triplets.append([f1, f2, f3])\n        \n    # Find non-dominated points\n    non_dominated_points = find_non_dominated(objective_triplets)\n    \n    # Sort lexicographically\n    sorted_points = sorted(non_dominated_points, key=lambda p: (p[0], p[1], p[2]))\n    \n    # Round to 6 decimal places for final reporting\n    final_list = [[round(val, 6) for val in p] for p in sorted_points]\n    \n    return final_list\n\ndef solve():\n    # Case 1\n    A1 = np.array([\n        [0, 1, 0, 0, 0],\n        [1, 0, 1, 0, 0],\n        [0, 1, 0, 1, 0],\n        [0, 0, 1, 0, 1],\n        [0, 0, 0, 1, 0]\n    ], dtype=float)\n    B1 = np.array([\n        [1.0, 0.5, 0.0, 0.0, 0.2],\n        [0.0, 0.3, 0.8, 0.1, 0.0],\n        [0.2, 0.0, 0.2, 0.7, 0.4]\n    ])\n    x1 = np.array([1.2, 0.5, 0.8])\n    weights1 = [(1.0, 0.0, 0.0), (0.6, 0.4, 0.0), (0.6, 0.0, 0.4), (0.34, 0.33, 0.33), (0.0, 1.0, 0.0), (0.0, 0.2, 0.8)]\n    case1 = (A1, B1, x1, weights1)\n\n    # Case 2\n    A2 = np.array([\n        [0, 1, 0, 0],\n        [1, 0, 1, 0],\n        [0, 1, 0, 0],\n        [0, 0, 0, 0]\n    ], dtype=float)\n    B2 = np.identity(4)\n    x2 = np.array([0.0, 0.0, 1.0, 0.2])\n    weights2 = [(1.0, 0.0, 0.0), (0.5, 0.0, 0.5), (0.5, 0.5, 0.0), (0.0, 1.0, 0.0), (0.0, 0.0, 1.0)]\n    case2 = (A2, B2, x2, weights2)\n\n    # Case 3\n    A3 = np.array([\n        [0, 1, 1, 0, 0, 0],\n        [1, 0, 1, 0, 0, 0],\n        [1, 1, 0, 0, 0, 0],\n        [0, 0, 0, 0, 1, 0],\n        [0, 0, 0, 1, 0, 1],\n        [0, 0, 0, 0, 1, 0]\n    ], dtype=float)\n    B3 = np.array([\n        [0.7, 0.6, 0.0, 0.0, 0.0, 0.0],\n        [0.0, 0.2, 0.9, 0.5, 0.0, 0.0],\n        [0.0, 0.0, 0.0, 0.3, 0.8, 0.6]\n    ])\n    x3 = np.array([1.0, 0.7, 1.1])\n    weights3 = [(1.0, 0.0, 0.0), (0.3, 0.6, 0.1), (0.3, 0.1, 0.6), (0.0, 1.0, 0.0), (0.0, 0.2, 0.8)]\n    case3 = (A3, B3, x3, weights3)\n\n    test_cases = [case1, case2, case3]\n    all_results = []\n    for A, B, x, weights_grid in test_cases:\n        pareto_front = solve_case(A, B, x, weights_grid)\n        all_results.append(pareto_front)\n        \n    print(repr(all_results).replace(\" \", \"\"))\n\nsolve()\n```", "id": "3320671"}]}