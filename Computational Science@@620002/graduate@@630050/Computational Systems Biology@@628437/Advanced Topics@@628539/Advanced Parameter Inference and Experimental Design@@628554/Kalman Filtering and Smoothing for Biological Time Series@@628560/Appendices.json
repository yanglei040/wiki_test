{"hands_on_practices": [{"introduction": "Before applying a Kalman filter, we must answer a fundamental question: can the latent states be uniquely determined from the measurements? This property, known as observability, is a critical prerequisite for successful state estimation and ensures that our model is well-posed for the task. This practice provides a hands-on opportunity to test for observability in a simple two-species gene expression model, where you will construct the observability matrix and determine its rank to make a definitive conclusion about the system's structural properties [@problem_id:3322163].", "problem": "A two-species gene expression system is modeled over a fixed sampling interval by a discrete-time Linear Time-Invariant (LTI) state-space model intended for use with Kalman filtering and smoothing of fluorescence time series. The latent state $x_{t} \\in \\mathbb{R}^{2}$ collects two log-transformed concentrations, and the observation $y_{t} \\in \\mathbb{R}$ is a single-channel fluorescence readout. The dynamics and observation equations are given by\n$$\nx_{t+1} = A x_{t}, \\quad y_{t} = C x_{t} + v_{t},\n$$\nwhere $v_{t}$ is zero-mean Gaussian measurement noise and the matrices are\n$$\nA = \\begin{bmatrix} 1 & 0 \\\\ 0 & 0.5 \\end{bmatrix}, \\quad C = \\begin{bmatrix} 1 & 1 \\end{bmatrix}.\n$$\nIn the noise-free thought experiment underlying observability, distinct initial states $x_{0}$ should generate distinct semi-infinite output sequences $\\{y_{t}\\}_{t=0}^{\\infty}$.\n\nStarting strictly from the definition that a discrete-time linear system is observable if and only if the mapping from the initial state $x_{0}$ to a finite-time stack of outputs has a trivial null space, proceed as follows:\n- Express the first two outputs $y_{0}$ and $y_{1}$ as linear functions of the initial state $x_{0}$.\n- Construct the corresponding linear map from $x_{0}$ to the stacked outputs $\\begin{bmatrix} y_{0} \\\\ y_{1} \\end{bmatrix}$ and determine the dimension of its image.\n- From this analysis, compute the rank of the associated observability map and state whether the pair $(A,C)$ renders the state $x_{t}$ fully observable (i.e., whether distinct $x_{0}$ produce distinct output sequences).\n\nReport the rank of the observability map as your final numeric answer. Express your final answer as an integer with no units.", "solution": "The problem statement is validated as scientifically grounded, well-posed, and objective. It presents a standard problem in linear systems theory concerning the observability of a discrete-time state-space model, providing all necessary matrices and definitions for a rigorous analysis.\n\nThe analysis of observability for a state-space system is predicated on establishing whether the initial state $x_{0}$ can be uniquely determined from a sequence of observations $\\{y_{t}\\}$. Following standard practice, this property is assessed in a noise-free context. We therefore set the measurement noise term $v_{t}$ to $0$ for the purpose of the analysis. The governing system equations are:\n$$\nx_{t+1} = A x_{t}\n$$\n$$\ny_{t} = C x_{t}\n$$\nThe state at any time $t$ can be expressed as a function of the initial state $x_{0}$ by recursive application of the state dynamics equation:\n$$\nx_{t} = A^{t} x_{0}\n$$\nSubstituting this into the observation equation yields the output at time $t$ as a linear function of the initial state:\n$$\ny_{t} = C A^{t} x_{0}\n$$\n\nThe first part of the problem requires expressing the first two outputs, $y_{0}$ and $y_{1}$, as linear functions of the initial state $x_{0} \\in \\mathbb{R}^{2}$.\nFor time $t=0$, the output $y_{0}$ is related to $x_{0}$ by:\n$$\ny_{0} = C A^{0} x_{0} = C I x_{0} = C x_{0}\n$$\nwhere $I$ is the $2 \\times 2$ identity matrix. Using the given observation matrix $C = \\begin{bmatrix} 1 & 1 \\end{bmatrix}$ and defining the initial state vector as $x_{0} = \\begin{bmatrix} x_{0,1} \\\\ x_{0,2} \\end{bmatrix}$, the expression for $y_{0}$ becomes:\n$$\ny_{0} = \\begin{bmatrix} 1 & 1 \\end{bmatrix} \\begin{bmatrix} x_{0,1} \\\\ x_{0,2} \\end{bmatrix} = x_{0,1} + x_{0,2}\n$$\nFor time $t=1$, the output $y_{1}$ is related to $x_{0}$ by:\n$$\ny_{1} = C A^{1} x_{0} = C A x_{0}\n$$\nTo proceed, we must compute the matrix product $CA$:\n$$\nCA = \\begin{bmatrix} 1 & 1 \\end{bmatrix} \\begin{bmatrix} 1 & 0 \\\\ 0 & 0.5 \\end{bmatrix} = \\begin{bmatrix} (1)(1) + (1)(0) & (1)(0) + (1)(0.5) \\end{bmatrix} = \\begin{bmatrix} 1 & 0.5 \\end{bmatrix}\n$$\nUsing this result, we can express $y_{1}$ as a function of $x_{0}$:\n$$\ny_{1} = \\begin{bmatrix} 1 & 0.5 \\end{bmatrix} \\begin{bmatrix} x_{0,1} \\\\ x_{0,2} \\end{bmatrix} = x_{0,1} + 0.5 x_{0,2}\n$$\n\nThe second part of the problem involves constructing the linear map from $x_{0}$ to the stacked outputs $\\begin{bmatrix} y_{0} \\\\ y_{1} \\end{bmatrix}$ and determining the dimension of its image. We can write the stacked system of equations in matrix form:\n$$\n\\begin{bmatrix} y_{0} \\\\ y_{1} \\end{bmatrix} = \\begin{bmatrix} C x_{0} \\\\ C A x_{0} \\end{bmatrix} = \\begin{bmatrix} C \\\\ CA \\end{bmatrix} x_{0}\n$$\nThe linear map from the initial state $x_{0}$ to the output vector $\\begin{bmatrix} y_{0} \\\\ y_{1} \\end{bmatrix}$ is represented by the matrix $\\mathcal{O}_{2} = \\begin{bmatrix} C \\\\ CA \\end{bmatrix}$. This matrix is the observability matrix for a finite observation horizon of $2$ time steps (from $t=0$ to $t=1$). Substituting the previously computed matrices $C$ and $CA$:\n$$\n\\mathcal{O}_{2} = \\begin{bmatrix} 1 & 1 \\\\ 1 & 0.5 \\end{bmatrix}\n$$\nThe image of this linear map is the column space of the matrix $\\mathcal{O}_{2}$. The dimension of the image is, by definition, the rank of the matrix. For a square matrix, the rank can be assessed by computing its determinant.\n$$\n\\det(\\mathcal{O}_{2}) = (1)(0.5) - (1)(1) = 0.5 - 1 = -0.5\n$$\nSince the determinant is non-zero ($\\det(\\mathcal{O}_{2}) \\neq 0$), the matrix is invertible and thus has full rank. The rank is equal to the number of columns (or rows), which is $2$. Therefore, the dimension of the image of the map is $2$.\n\nThe final part of the problem asks for the rank of the associated observability map and a conclusion regarding the system's observability. For a discrete-time LTI system with a state dimension of $n$, the observability map (or observability matrix) $\\mathcal{O}$ is defined by the Kalman observability criterion as:\n$$\n\\mathcal{O} = \\begin{bmatrix} C \\\\ CA \\\\ CA^{2} \\\\ \\vdots \\\\ CA^{n-1} \\end{bmatrix}\n$$\nIn this problem, the state dimension is $n=2$. The observability map is therefore:\n$$\n\\mathcal{O} = \\begin{bmatrix} C \\\\ CA^{2-1} \\end{bmatrix} = \\begin{bmatrix} C \\\\ CA \\end{bmatrix}\n$$\nThis is precisely the matrix $\\mathcal{O}_{2}$ that was constructed and analyzed in the preceding step. The rank of the observability map is thus:\n$$\n\\text{rank}(\\mathcal{O}) = \\text{rank}(\\mathcal{O}_{2}) = 2\n$$\nA system is defined as fully observable if and only if its observability map $\\mathcal{O}$ has full column rank, i.e., $\\text{rank}(\\mathcal{O}) = n$. In this case, the state dimension is $n=2$ and we have found that $\\text{rank}(\\mathcal{O}) = 2$. The condition $\\text{rank}(\\mathcal{O}) = n$ is satisfied. Consequently, the pair $(A, C)$ renders the state $x_{t}$ fully observable. This implies that the null space of $\\mathcal{O}$ is trivial, containing only the zero vector. Therefore, if $\\mathcal{O} x_{0,a} = \\mathcal{O} x_{0,b}$ for two initial states, it must be that $x_{0,a} = x_{0,b}$. Distinct initial states necessarily produce distinct output sequences, and the initial state $x_{0}$ can be uniquely determined from the first $n=2$ measurements.\n\nThe problem specifically asks for the rank of the observability map. Based on the formal calculation, this rank is $2$.", "answer": "$$\\boxed{2}$$", "id": "3322163"}, {"introduction": "While the standard Kalman filter equations are mathematically exact, their implementation in finite-precision arithmetic can lead to numerical instability, where the covariance matrix may lose its essential properties of symmetry and positive semi-definiteness. This practice introduces the Joseph form, a stabilized version of the covariance update that preserves these properties by its very structure. By deriving and applying this robust formulation, you will gain a deeper appreciation for the practical challenges of filter implementation and learn a crucial technique for ensuring reliable performance [@problem_id:3322205].", "problem": "A two-dimensional latent molecular state $\\mathbf{x}_t = \\begin{bmatrix}x_{m,t} \\\\ x_{p,t}\\end{bmatrix}$ models messenger ribonucleic acid (mRNA) and protein abundances in a gene expression module evolving over time in a linear-Gaussian state-space model. At time $t$, before incorporating the fluorescence measurement, the prior (prediction) covariance is\n$$\nP_{t|t-1} \\;=\\; \\begin{bmatrix}1 & 0 \\\\ 0 & 0.5\\end{bmatrix}.\n$$\nThe fluorescence readout $y_t$ linearly senses only mRNA with measurement matrix\n$$\nC \\;=\\; \\begin{bmatrix}1 & 0\\end{bmatrix},\n$$\nand measurement noise variance\n$$\nR \\;=\\; \\begin{bmatrix}0.2\\end{bmatrix}.\n$$\nAssume standard linear-Gaussian assumptions: the prior state $\\mathbf{x}_t | y_{1:t-1}$ is Gaussian with covariance $P_{t|t-1}$, the observation model is $y_t = C \\mathbf{x}_t + v_t$ with $v_t \\sim \\mathcal{N}(0,R)$ independent of $\\mathbf{x}_t$, and all distributions are well-defined.\n\nUsing only fundamental properties of multivariate Gaussian conditioning and linear algebra, do the following:\n\n1) Starting from the block Gaussian conditioning identity for the covariance of $\\mathbf{x}_t | y_t$ implied by the joint Gaussianity of $(\\mathbf{x}_t, y_t)$, derive the stabilized covariance update that preserves symmetry and positive semidefiniteness in finite precision arithmetic (commonly known as the Joseph form). Your derivation must express this stabilized update explicitly in terms of the prior covariance $P_{t|t-1}$, the Kalman gain $K_t$, the measurement matrix $C$, and the measurement noise covariance $R$, and must justify why the resulting expression is symmetric and positive semidefinite.\n\n2) Compute the innovation covariance $S_t$, the Kalman gain $K_t$, and then apply your stabilized covariance update from part $1)$ to obtain the posterior covariance $P_{t|t}$. Verify numerically that $P_{t|t}$ is symmetric and positive semidefinite by inspecting its eigenvalues.\n\n3) Report the smallest eigenvalue of $P_{t|t}$ as an exact fraction. Do not include any units in your final answer.", "solution": "We begin from the standard linear-Gaussian state-observation model: the state $\\mathbf{x}_t | y_{1:t-1}$ is Gaussian with mean (not needed here) and covariance $P_{t|t-1}$, and the observation model is $y_t = C \\mathbf{x}_t + v_t$ with $v_t \\sim \\mathcal{N}(0,R)$ independent of $\\mathbf{x}_t$. The joint distribution of $(\\mathbf{x}_t, y_t)$ conditional on $y_{1:t-1}$ is Gaussian. The formula for the conditional covariance of a partitioned Gaussian vector partitioned into $(\\mathbf{a},\\mathbf{b})$ with covariance blocks $P_{\\mathbf{a}\\mathbf{a}}$, $P_{\\mathbf{a}\\mathbf{b}}$, $P_{\\mathbf{b}\\mathbf{a}}$, $P_{\\mathbf{b}\\mathbf{b}}$ is\n$$\n\\text{Cov}(\\mathbf{a} | \\mathbf{b}) \\;=\\; P_{\\mathbf{a}\\mathbf{a}} \\;-\\; P_{\\mathbf{a}\\mathbf{b}}\\, P_{\\mathbf{b}\\mathbf{b}}^{-1} \\, P_{\\mathbf{b}\\mathbf{a}}.\n$$\nApplying this with $\\mathbf{a}=\\mathbf{x}_t$ and $\\mathbf{b}=y_t$, we obtain\n$$\nP_{t|t} \\;=\\; P_{t|t-1} \\;-\\; P_{t|t-1} C^{\\mathsf{T}} \\left(C P_{t|t-1} C^{\\mathsf{T}} + R\\right)^{-1} C P_{t|t-1}.\n$$\nDefine the innovation covariance\n$$\nS_t \\;=\\; C P_{t|t-1} C^{\\mathsf{T}} + R,\n$$\nand the Kalman gain\n$$\nK_t \\;=\\; P_{t|t-1} C^{\\mathsf{T}} S_t^{-1}.\n$$\nThen the conditional covariance simplifies to the standard update equation:\n$$\nP_{t|t} \\;=\\; P_{t|t-1} \\;-\\; K_t S_t K_t^{\\mathsf{T}} \\;=\\; P_{t|t-1} \\;-\\; K_t C P_{t|t-1}.\n$$\nAlthough this expression is exact, in finite precision arithmetic the subtraction can lead to a loss of symmetry and positive definiteness. A numerically stabilized update known as the Joseph form rewrites the same covariance as\n$$\nP_{t|t} \\;=\\; \\left(I - K_t C\\right) P_{t|t-1} \\left(I - K_t C\\right)^{\\mathsf{T}} \\;+\\; K_t R K_t^{\\mathsf{T}}.\n$$\nTo verify equivalence, we can expand the right-hand side:\n$$\n\\begin{aligned}\n \\left(I - K_t C\\right) P_{t|t-1} \\left(I - C^{\\mathsf{T}}K_t^{\\mathsf{T}}\\right) + K_t R K_t^{\\mathsf{T}} \\\\\n = P_{t|t-1} - K_t C P_{t|t-1} - P_{t|t-1} C^{\\mathsf{T}} K_t^{\\mathsf{T}} + K_t C P_{t|t-1} C^{\\mathsf{T}} K_t^{\\mathsf{T}} + K_t R K_t^{\\mathsf{T}} \\\\\n = P_{t|t-1} - K_t C P_{t|t-1} - P_{t|t-1} C^{\\mathsf{T}} K_t^{\\mathsf{T}} + K_t \\left(C P_{t|t-1} C^{\\mathsf{T}} + R\\right) K_t^{\\mathsf{T}} \\\\\n = P_{t|t-1} - K_t C P_{t|t-1} - P_{t|t-1} C^{\\mathsf{T}} K_t^{\\mathsf{T}} + K_t S_t K_t^{\\mathsf{T}}.\n\\end{aligned}\n$$\nUsing the definitions $K_t S_t = P_{t|t-1} C^{\\mathsf{T}}$ and its transpose $S_t K_t^{\\mathsf{T}} = C P_{t|t-1}$, we can substitute $K_t S_t K_t^{\\mathsf{T}} = P_{t|t-1} C^{\\mathsf{T}} K_t^{\\mathsf{T}}$ into the expression above, which simplifies to $P_{t|t-1} - K_t C P_{t|t-1}$. This matches the standard update. The Joseph form is manifestly symmetric and positive semidefinite (PSD) because it is a sum of two PSD matrices: $\\left(I - K_t C\\right) P_{t|t-1} \\left(I - K_t C\\right)^{\\mathsf{T}} \\succeq 0$ whenever $P_{t|t-1} \\succeq 0$, and $K_t R K_t^{\\mathsf{T}} \\succeq 0$ whenever $R \\succeq 0$.\n\nWe now apply these formulas to the given biological sensing instance. The provided quantities are\n$$\nP_{t|t-1} \\;=\\; \\begin{bmatrix}1  0 \\\\ 0  0.5\\end{bmatrix}, \\quad C \\;=\\; \\begin{bmatrix}1  0\\end{bmatrix}, \\quad R \\;=\\; 0.2.\n$$\nFirst compute the innovation covariance (which is a scalar):\n$$\nS_t \\;=\\; C P_{t|t-1} C^{\\mathsf{T}} + R \\;=\\; \\begin{bmatrix}1  0\\end{bmatrix} \\begin{bmatrix}1  0 \\\\ 0  0.5\\end{bmatrix} \\begin{bmatrix}1 \\\\ 0\\end{bmatrix} + 0.2 \\;=\\; 1 + 0.2 \\;=\\; 1.2.\n$$\nEquivalently, $S_t = \\frac{6}{5}$.\n\nNext compute the Kalman gain:\n$$\nK_t \\;=\\; P_{t|t-1} C^{\\mathsf{T}} S_t^{-1} \\;=\\; \\begin{bmatrix}1  0 \\\\ 0  0.5\\end{bmatrix} \\begin{bmatrix}1 \\\\ 0\\end{bmatrix} (1.2)^{-1} \\;=\\; \\begin{bmatrix}1 \\\\ 0\\end{bmatrix} \\cdot \\frac{1}{1.2} \\;=\\; \\begin{bmatrix}\\frac{5}{6} \\\\ 0\\end{bmatrix}.\n$$\nApply the Joseph form:\n$$\nP_{t|t} \\;=\\; \\left(I - K_t C\\right) P_{t|t-1} \\left(I - K_t C\\right)^{\\mathsf{T}} + K_t R K_t^{\\mathsf{T}}.\n$$\nCompute $I - K_t C$:\n$$\nK_t C \\;=\\; \\begin{bmatrix}\\frac{5}{6} \\\\ 0\\end{bmatrix} \\begin{bmatrix}1  0\\end{bmatrix} \\;=\\; \\begin{bmatrix}\\frac{5}{6}  0 \\\\ 0  0\\end{bmatrix}, \\quad\nI - K_t C \\;=\\; \\begin{bmatrix}1  0 \\\\ 0  1\\end{bmatrix} - \\begin{bmatrix}\\frac{5}{6}  0 \\\\ 0  0\\end{bmatrix} \\;=\\; \\begin{bmatrix}\\frac{1}{6}  0 \\\\ 0  1\\end{bmatrix}.\n$$\nThe first term of the Joseph form is:\n$$\n\\left(I - K_t C\\right) P_{t|t-1} \\left(I - K_t C\\right)^{\\mathsf{T}}\n= \\begin{bmatrix}\\frac{1}{6}  0 \\\\ 0  1\\end{bmatrix} \\begin{bmatrix}1  0 \\\\ 0  0.5\\end{bmatrix} \\begin{bmatrix}\\frac{1}{6}  0 \\\\ 0  1\\end{bmatrix}\n= \\begin{bmatrix}\\frac{1}{36}  0 \\\\ 0  \\frac{1}{2}\\end{bmatrix}.\n$$\nThe second term is:\n$$\nK_t R K_t^{\\mathsf{T}} \\;=\\; \\begin{bmatrix}\\frac{5}{6} \\\\ 0\\end{bmatrix} (0.2) \\begin{bmatrix}\\frac{5}{6}  0\\end{bmatrix}\n\\;=\\; \\begin{bmatrix}\\frac{5}{6} \\cdot 0.2 \\\\ 0\\end{bmatrix} \\begin{bmatrix}\\frac{5}{6}  0\\end{bmatrix}\n\\;=\\; \\begin{bmatrix}\\frac{1}{6} \\\\ 0\\end{bmatrix} \\begin{bmatrix}\\frac{5}{6}  0\\end{bmatrix}\n\\;=\\; \\begin{bmatrix}\\frac{5}{36}  0 \\\\ 0  0\\end{bmatrix}.\n$$\nSumming the two terms:\n$$\nP_{t|t} \\;=\\; \\begin{bmatrix}\\frac{1}{36}  0 \\\\ 0  \\frac{1}{2}\\end{bmatrix} + \\begin{bmatrix}\\frac{5}{36}  0 \\\\ 0  0\\end{bmatrix}\n\\;=\\; \\begin{bmatrix}\\frac{6}{36}  0 \\\\ 0  \\frac{1}{2}\\end{bmatrix}\n\\;=\\; \\begin{bmatrix}\\frac{1}{6}  0 \\\\ 0  \\frac{1}{2}\\end{bmatrix}.\n$$\nThis posterior covariance matrix $P_{t|t}$ is symmetric. Its eigenvalues are the diagonal entries because it is a diagonal matrix:\n$$\n\\lambda_1 \\;=\\; \\frac{1}{6}, \\qquad \\lambda_2 \\;=\\; \\frac{1}{2}.\n$$\nBoth eigenvalues are positive, so $P_{t|t}$ is positive definite. The smallest eigenvalue is\n$$\n\\min\\!\\left\\{\\frac{1}{6}, \\frac{1}{2}\\right\\} \\;=\\; \\frac{1}{6}.\n$$\nAs requested, we report this as an exact fraction.", "answer": "$$\\boxed{\\frac{1}{6}}$$", "id": "3322205"}, {"introduction": "The standard Kalman filter produces an estimate of the state at time $t$ using measurements up to time $t$. However, for offline analysis of biological time-series data, we can achieve superior accuracy by using all available measurements, both past and future. This is the goal of smoothing, and this exercise focuses on the classic Rauch-Tung-Striebel (RTS) smoother, which efficiently refines the filtered estimates in a backward pass from the final time point [@problem_id:3322157].", "problem": "A two-dimensional latent state represents log-abundances of messenger ribonucleic acid (mRNA) and protein in a single cell, denoted by the vector $x_t \\in \\mathbb{R}^2$ with components $x_{t,1}$ (mRNA) and $x_{t,2}$ (protein). The dynamics are modeled as a linear Gaussian state-space model with a random-walk state evolution and direct noisy observation:\n$$\nx_{t+1} = A x_t + w_t, \\quad w_t \\sim \\mathcal{N}(0,Q), \\\\\ny_t = C x_t + v_t, \\quad v_t \\sim \\mathcal{N}(0,R),\n$$\nwhere $A \\in \\mathbb{R}^{2 \\times 2}$, $Q \\in \\mathbb{R}^{2 \\times 2}$, $C \\in \\mathbb{R}^{2 \\times 2}$, and $R \\in \\mathbb{R}^{2 \\times 2}$ are constant in time. Consider a three-step sequence at times $t \\in \\{1,2,3\\}$ with\n$$\nA = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix}, \\quad\nQ = \\begin{pmatrix} 0.09  0 \\\\ 0  0.04 \\end{pmatrix}, \\quad\nC = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix}, \\quad\nR = \\begin{pmatrix} 0.50  0 \\\\ 0  0.50 \\end{pmatrix}.\n$$\nFrom a completed forward pass of the Kalman filter, the filtered posterior means and covariances are given as\n$$\nx_{1|1} = \\begin{pmatrix} 1.0 \\\\ 2.0 \\end{pmatrix}, \\quad\nP_{1|1} = \\begin{pmatrix} 0.25  0 \\\\ 0  0.16 \\end{pmatrix},\n$$\n$$\nx_{2|2} = \\begin{pmatrix} 1.2 \\\\ 1.8 \\end{pmatrix}, \\quad\nP_{2|2} = \\begin{pmatrix} 0.20  0 \\\\ 0  0.12 \\end{pmatrix},\n$$\n$$\nx_{3|3} = \\begin{pmatrix} 0.9 \\\\ 2.1 \\end{pmatrix}, \\quad\nP_{3|3} = \\begin{pmatrix} 0.15  0 \\\\ 0  0.10 \\end{pmatrix}.\n$$\nUsing only the fundamental definitions of linear Gaussian state-space models and properties of jointly Gaussian variables, derive the Rauch–Tung–Striebel (RTS) smoothing recursions appropriate for this setting and use them to compute the smoothed posterior means $x_{t|3}$ and covariances $P_{t|3}$ for $t \\in \\{1,2,3\\}$. As your final reported quantity, provide the first diagonal entry of the smoothed covariance at time $t=1$, namely the scalar $(P_{1|3})_{11}$. Express your final answer as an exact value (no rounding). The log-abundance variables are dimensionless; report the scalar without units.", "solution": "The problem is assessed to be valid as it is scientifically grounded, well-posed, objective, and contains a complete and consistent set of givens. It poses a standard, solvable problem in computational systems biology involving Kalman smoothing.\n\nThe problem asks for the derivation of the Rauch–Tung–Striebel (RTS) smoothing recursions and their application to a specific linear Gaussian state-space model. The model describes the dynamics of mRNA and protein log-abundances, $x_t = \\begin{pmatrix} x_{t,1} \\\\ x_{t,2} \\end{pmatrix}$, over a time sequence $t \\in \\{1, 2, 3\\}$. The total number of time steps is $T=3$. The smoothed posterior moments $x_{t|T}$ and $P_{t|T}$ are the mean and covariance of the state $x_t$ given all measurements $y_{1:T}$, i.e., $p(x_t | y_1, \\dots, y_T)$.\n\n**Derivation of the RTS Smoothing Recursions**\n\nThe RTS recursions can be derived by considering the joint distribution of $x_t$ and $x_{t+1}$ conditioned on the measurements. The key insight is to combine the filtered estimate at time $t$, $p(x_t | y_{1:t})$, with information propagating backward from the smoothed estimate at time $t+1$, $p(x_{t+1} | y_{1:T})$.\n\nThe Kalman filter provides the filtered posterior $p(x_t | y_{1:t}) = \\mathcal{N}(x_t; x_{t|t}, P_{t|t})$ and the predicted prior $p(x_{t+1} | y_{1:t}) = \\mathcal{N}(x_{t+1}; x_{t+1|t}, P_{t+1|t})$, where $x_{t+1|t} = Ax_{t|t}$ and $P_{t+1|t} = AP_{t|t}A^T + Q$. We define the smoother gain matrix as $G_t = P_{t|t} A^T P_{t+1|t}^{-1}$.\n\nThe smoothed mean $x_{t|T}$ can be derived by considering the conditional expectation $E[x_t | y_{1:T}]$. Using properties of jointly Gaussian variables, this can be shown to follow the backward recursion:\n$$x_{t|T} = x_{t|t} + G_t (x_{t+1|T} - x_{t+1|t})$$\nThis equation intuitively corrects the filtered estimate $x_{t|t}$ with a term proportional to the difference between the smoothed estimate of the next state, $x_{t+1|T}$, and the predicted estimate of that state, $x_{t+1|t}$.\n\nThe smoothed covariance $P_{t|T}$ can be derived using the law of total covariance. A compact derivation recognizes that the smoothing error $x_t - x_{t|T}$ is composed of orthogonal components. This leads to the following backward recursion for the covariance:\n$$P_{t|T} = P_{t|t} + G_t (P_{t+1|T} - P_{t+1|t}) G_t^T$$\nThis shows that the smoothed covariance $P_{t|T}$ is a refinement of the filtered covariance $P_{t|t}$. The update term depends on the change in uncertainty at the next time step, $(P_{t+1|T} - P_{t+1|t})$, propagated backward via the smoother gain $G_t$.\n\n**Application to the Problem**\n\nIn this problem, $A$ is the identity matrix, and all covariance matrices $Q, R, P_{t|t}$ are diagonal. This implies that the dynamics of the two state variables, $x_{t,1}$ and $x_{t,2}$, are decoupled. The Kalman filtering and smoothing can be performed independently for each scalar component. We are asked for $(P_{1|3})_{11}$, the variance of the first component. Let us denote the scalar variance for the first component as $p_{t|s} = (P_{t|s})_{11}$ and the process noise variance as $q = Q_{11} = 0.09$. The recursions for this scalar component, with $A=1$, are:\n$$ p_{t+1|t} = p_{t|t} + q $$\n$$ g_t = p_{t|t} / p_{t+1|t} $$\n$$ p_{t|T} = p_{t|t} + g_t^2 (p_{t+1|T} - p_{t+1|t}) $$\nThe smoothing is performed backward from $t=T-1=2$ to $t=1$. The initialization is $p_{3|3} = (P_{3|3})_{11} = 0.15$.\n\n**Smoothed Variance at $t=2$**:\nFirst, we compute the predicted variance $p_{3|2}$ using the filtered variance from the forward pass, $p_{2|2} = (P_{2|2})_{11} = 0.20$.\n$$p_{3|2} = p_{2|2} + q = 0.20 + 0.09 = 0.29$$\nNext, we compute the smoother gain $g_2$:\n$$g_2 = \\frac{p_{2|2}}{p_{3|2}} = \\frac{0.20}{0.29} = \\frac{20}{29}$$\nNow we compute the smoothed variance $p_{2|3}$:\n$$p_{2|3} = p_{2|2} + g_2^2 (p_{3|3} - p_{3|2}) = 0.20 + \\left(\\frac{20}{29}\\right)^2 (0.15 - 0.29)$$\n$$p_{2|3} = \\frac{1}{5} + \\frac{400}{841}(-0.14) = \\frac{1}{5} - \\frac{400}{841} \\frac{14}{100} = \\frac{1}{5} - \\frac{56}{841}$$\n$$p_{2|3} = \\frac{841 - 5 \\times 56}{5 \\times 841} = \\frac{841-280}{4205} = \\frac{561}{4205}$$\n\n**Smoothed Variance at $t=1$**:\nWe repeat the procedure for $t=1$. First, we compute the predicted variance $p_{2|1}$ using the filtered variance $p_{1|1} = (P_{1|1})_{11} = 0.25$.\n$$p_{2|1} = p_{1|1} + q = 0.25 + 0.09 = 0.34$$\nNext, we compute the smoother gain $g_1$:\n$$g_1 = \\frac{p_{1|1}}{p_{2|1}} = \\frac{0.25}{0.34} = \\frac{25}{34}$$\nNow we compute the final smoothed variance $p_{1|3}$:\n$$p_{1|3} = p_{1|1} + g_1^2 (p_{2|3} - p_{2|1})$$\n$$p_{1|3} = 0.25 + \\left(\\frac{25}{34}\\right)^2 \\left(\\frac{561}{4205} - 0.34\\right)$$\n$$p_{1|3} = \\frac{1}{4} + \\frac{625}{1156} \\left(\\frac{561}{4205} - \\frac{17}{50}\\right)$$\nThe term in the parenthesis is:\n$$\\frac{561 \\times 50 - 17 \\times 4205}{4205 \\times 50} = \\frac{28050 - 71485}{210250} = \\frac{-43435}{210250} = -\\frac{8687}{42050}$$\nSubstituting this back:\n$$p_{1|3} = \\frac{1}{4} - \\frac{625}{1156} \\left(\\frac{8687}{42050}\\right)$$\nWe simplify the product of fractions:\n$$ \\frac{625}{1156} \\cdot \\frac{8687}{42050} = \\frac{12775}{114376} $$\nFinally, we compute $p_{1|3}$:\n$$p_{1|3} = \\frac{1}{4} - \\frac{12775}{114376} = \\frac{28594}{114376} - \\frac{12775}{114376} = \\frac{15819}{114376}$$\nThis fraction is irreducible.", "answer": "$$\\boxed{\\frac{15819}{114376}}$$", "id": "3322157"}]}