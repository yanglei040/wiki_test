{"hands_on_practices": [{"introduction": "The core of a particle filter involves updating particle weights by multiplying them with the likelihood of the latest observation. In many real-world models, such as those in systems biology, these likelihoods can vary over many orders of magnitude, leading to numerical underflow if handled naively. This practice introduces the standard solution: performing all calculations in log-space and using the \"log-sum-exp\" trick for stable normalization, a vital technique for robustly implementing probabilistic models `[@problem_id:3347782]`.", "problem": "Consider a hidden-state model of transcriptional activity for a single gene in a cell population, where the latent promoter activity $x_t$ at discrete time $t$ evolves according to a Markov process and the observed readout $y_t$ is a count generated by a standard observation model (for example, a Poisson or Negative Binomial model) whose likelihood $p(y_t \\mid x_t)$ can vary over many orders of magnitude across different latent states. In a sequential Monte Carlo (SMC) particle filter under the prior-as-proposal setting, the $N$ particles $\\{x_t^{(i)}\\}_{i=1}^{N}$ are propagated forward, and the unnormalized weights are updated multiplicatively by the observation likelihood. To maintain numerical stability in the presence of extreme likelihood ratios, one propagates log-weights $\\ell_t^{(i)}$ and performs a stable normalization using the logarithm of a sum of exponentials.\n\nStarting from the principles of particle filtering, define the log-weights and show how normalization can be performed stably using only additions, subtractions, and exponentials of differences. Then, apply this to the following toy example that mimics an SMC update in computational systems biology with $N=4$ particles. Assume previous normalized weights are uniform, $w_{t-1}^{(i)} = 1/4$ for $i \\in \\{1,2,3,4\\}$, and suppose that the incremental log-likelihood contributions at time $t$ induced by the new observation $y_t$ are\n$$\\delta_1 = -300,\\quad \\delta_2 = -0.1,\\quad \\delta_3 = 0,\\quad \\delta_4 = -200.$$\nUsing the log-weight propagation and a numerically stable log-sum-exp normalization, compute the normalized weights $w_t^{(i)}$ for $i \\in \\{1,2,3,4\\}$ and report the weight vector as a single closed-form analytic expression in terms of the exponential function. No rounding is required; express your final answer exactly as functions of $\\exp(\\cdot)$.", "solution": "The problem asks for a derivation of the numerically stable log-weight update and normalization procedure used in sequential Monte Carlo (SMC) methods, and its application to a specific numerical example. The validation of the problem statement has been performed and it is deemed valid.\n\nFirst, we detail the principles of the weight update in a particle filter. Let a set of $N$ particles $\\{x_{t-1}^{(i)}, w_{t-1}^{(i)}\\}_{i=1}^{N}$ represent the posterior distribution of a latent state at time $t-1$, where $w_{t-1}^{(i)}$ are the normalized weights such that $\\sum_{i=1}^{N} w_{t-1}^{(i)} = 1$. In a standard SMC algorithm with a prior-as-proposal, each particle's state is propagated according to the system dynamics: $x_t^{(i)} \\sim p(x_t | x_{t-1}^{(i)})$.\n\nUpon receiving a new observation $y_t$, the importance weight for each particle is updated. The unnormalized weight at time $t$, denoted $\\tilde{w}_t^{(i)}$, is the product of the previous weight and the likelihood of the observation given the particle's new state:\n$$ \\tilde{w}_t^{(i)} = w_{t-1}^{(i)} \\cdot p(y_t | x_t^{(i)}) $$\nThe weights are then normalized to sum to unity:\n$$ w_t^{(i)} = \\frac{\\tilde{w}_t^{(i)}}{\\sum_{j=1}^{N} \\tilde{w}_t^{(j)}} $$\nIn many applications, including the one described, the likelihood values $p(y_t | x_t^{(i)})$ can be extremely small, leading to numerical underflow if the weights are computed directly. To circumvent this, computations are performed in the logarithmic domain.\n\nLet $\\ell_t^{(i)} = \\ln(w_t^{(i)})$ be the log-weight. The update for the unnormalized log-weight, $\\tilde{\\ell}_t^{(i)} = \\ln(\\tilde{w}_t^{(i)})$, becomes an additive operation:\n$$ \\tilde{\\ell}_t^{(i)} = \\ln(w_{t-1}^{(i)}) + \\ln(p(y_t | x_t^{(i)})) $$\nThis can be written as $\\tilde{\\ell}_t^{(i)} = \\ell_{t-1}^{(i)} + \\delta_t^{(i)}$, where $\\ell_{t-1}^{(i)}$ is the normalized log-weight from the previous step and $\\delta_t^{(i)} = \\ln(p(y_t | x_t^{(i)}))$ is the incremental log-likelihood contribution.\n\nTo obtain the normalized weights $w_t^{(i)}$, we must compute:\n$$ w_t^{(i)} = \\frac{\\exp(\\tilde{\\ell}_t^{(i)})}{\\sum_{j=1}^{N} \\exp(\\tilde{\\ell}_t^{(j)})} $$\nThe sum in the denominator, $\\sum_{j=1}^{N} \\exp(\\tilde{\\ell}_t^{(j)})$, is a potential source of numerical instability. If the values of $\\tilde{\\ell}_t^{(j)}$ are large, $\\exp(\\tilde{\\ell}_t^{(j)})$ can overflow. If they are large and negative, it can underflow to zero, leading to division by zero.\n\nThe standard and numerically stable method for this normalization, known as the log-sum-exp trick, involves shifting the log-weights by their maximum value. Let $L_{\\max} = \\max_{j} \\{ \\tilde{\\ell}_t^{(j)} \\}$. The sum can be rewritten as:\n$$ \\sum_{j=1}^{N} \\exp(\\tilde{\\ell}_t^{(j)}) = \\sum_{j=1}^{N} \\exp(L_{\\max} + \\tilde{\\ell}_t^{(j)} - L_{\\max}) = \\exp(L_{\\max}) \\sum_{j=1}^{N} \\exp(\\tilde{\\ell}_t^{(j)} - L_{\\max}) $$\nSubstituting this back into the expression for the normalized weights:\n$$ w_t^{(i)} = \\frac{\\exp(\\tilde{\\ell}_t^{(i)})}{\\exp(L_{\\max}) \\sum_{j=1}^{N} \\exp(\\tilde{\\ell}_t^{(j)} - L_{\\max})} = \\frac{\\exp(\\tilde{\\ell}_t^{(i)} - L_{\\max})}{\\sum_{j=1}^{N} \\exp(\\tilde{\\ell}_t^{(j)} - L_{\\max})} $$\nThis formulation is numerically stable. The term $\\tilde{\\ell}_t^{(j)} - L_{\\max}$ is always non-positive, with at least one term being exactly zero. Therefore, its exponential, $\\exp(\\tilde{\\ell}_t^{(j)} - L_{\\max})$, is always between $0$ and $1$, preventing overflow. The denominator is a sum of these well-behaved terms, and since one term is equal to $1$, the sum is guaranteed to be at least $1$, preventing underflow and division by zero. This calculation relies only on subtraction of log-weights, exponentiation of these differences, summation, and division, as required.\n\nNow, we apply this procedure to the provided toy example.\nThe givens are:\n- Number of particles: $N=4$.\n- Previous normalized weights: $w_{t-1}^{(i)} = 1/4$ for $i \\in \\{1,2,3,4\\}$.\n- Incremental log-likelihoods: $\\delta_1 = -300$, $\\delta_2 = -0.1$, $\\delta_3 = 0$, $\\delta_4 = -200$.\n\n1.  Calculate the unnormalized log-weights at time $t$.\nThe previous normalized log-weights are $\\ell_{t-1}^{(i)} = \\ln(w_{t-1}^{(i)}) = \\ln(1/4) = -\\ln(4)$ for all $i$.\nThe unnormalized log-weights are $\\tilde{\\ell}_t^{(i)} = \\ell_{t-1}^{(i)} + \\delta_i = -\\ln(4) + \\delta_i$.\n$$ \\tilde{\\ell}_t^{(1)} = -\\ln(4) - 300 $$\n$$ \\tilde{\\ell}_t^{(2)} = -\\ln(4) - 0.1 $$\n$$ \\tilde{\\ell}_t^{(3)} = -\\ln(4) + 0 = -\\ln(4) $$\n$$ \\tilde{\\ell}_t^{(4)} = -\\ln(4) - 200 $$\n\n2.  Find the maximum unnormalized log-weight, $L_{\\max}$.\n$$ L_{\\max} = \\max_{i} \\{ \\tilde{\\ell}_t^{(i)} \\} = \\max \\{ -\\ln(4) - 300, -\\ln(4) - 0.1, -\\ln(4), -\\ln(4) - 200 \\} $$\nThe maximum value is clearly $\\tilde{\\ell}_t^{(3)} = -\\ln(4)$.\n$$ L_{\\max} = -\\ln(4) $$\n\n3.  Compute the normalized weights $w_t^{(i)}$ using the stable formula.\n$$ w_t^{(i)} = \\frac{\\exp(\\tilde{\\ell}_t^{(i)} - L_{\\max})}{\\sum_{j=1}^{4} \\exp(\\tilde{\\ell}_t^{(j)} - L_{\\max})} $$\nFirst, compute the differences for the exponents: $\\tilde{\\ell}_t^{(i)} - L_{\\max} = (-\\ln(4) + \\delta_i) - (-\\ln(4)) = \\delta_i$.\nThe denominator, which we denote as $S$, is the sum of the exponentials of these differences:\n$$ S = \\sum_{j=1}^{4} \\exp(\\delta_j) = \\exp(\\delta_1) + \\exp(\\delta_2) + \\exp(\\delta_3) + \\exp(\\delta_4) $$\n$$ S = \\exp(-300) + \\exp(-0.1) + \\exp(0) + \\exp(-200) $$\nReordering for clarity:\n$$ S = 1 + \\exp(-0.1) + \\exp(-200) + \\exp(-300) $$\nThe normalized weights $w_t^{(i)}$ are given by $w_t^{(i)} = \\frac{\\exp(\\delta_i)}{S}$:\n$$ w_t^{(1)} = \\frac{\\exp(-300)}{1 + \\exp(-0.1) + \\exp(-200) + \\exp(-300)} $$\n$$ w_t^{(2)} = \\frac{\\exp(-0.1)}{1 + \\exp(-0.1) + \\exp(-200) + \\exp(-300)} $$\n$$ w_t^{(3)} = \\frac{\\exp(0)}{1 + \\exp(-0.1) + \\exp(-200) + \\exp(-300)} = \\frac{1}{1 + \\exp(-0.1) + \\exp(-200) + \\exp(-300)} $$\n$$ w_t^{(4)} = \\frac{\\exp(-200)}{1 + \\exp(-0.1) + \\exp(-200) + \\exp(-300)} $$\nThe final weight vector $\\mathbf{w}_t = (w_t^{(1)}, w_t^{(2)}, w_t^{(3)}, w_t^{(4)})$ can be written as a scalar multiplied by a vector:\n$$ \\mathbf{w}_t = \\frac{1}{1 + \\exp(-0.1) + \\exp(-200) + \\exp(-300)} \\begin{pmatrix} \\exp(-300) & \\exp(-0.1) & 1 & \\exp(-200) \\end{pmatrix} $$\nThis expression represents the exact, unrounded normalized weights for the four particles.", "answer": "$$\n\\boxed{\\frac{1}{1 + \\exp(-0.1) + \\exp(-200) + \\exp(-300)} \\begin{pmatrix} \\exp(-300) & \\exp(-0.1) & 1 & \\exp(-200) \\end{pmatrix}}\n$$", "id": "3347782"}, {"introduction": "After updating the weights, it is crucial to assess the \"health\" of the particle population, as a common ailment is weight degeneracy, where nearly all the importance weight collapses onto a single particle. This practice explores the standard diagnostic for this problem, the Effective Sample Size ($N_{\\text{eff}}$), by deriving and analyzing its mathematical properties in an idealized but highly insightful scenario `[@problem_id:3417298]`. By doing so, you will gain a deeper intuition for how weight collapse occurs and its connection to the infamous \"curse of dimensionality\".", "problem": "Consider a Sequential Monte Carlo (SMC) particle filter in a data assimilation setting for an inverse problem with $N$ particles. After assimilating a single observation, suppose the (nonnegative) importance weights are $\\{w_{i}\\}_{i=1}^{N}$ and that the weight vector exhibits a single dominant weight, with one entry $w_{1}$ larger than the other $N-1$ entries. Assume that the $N-1$ non-dominant weights are equal. Define normalized weights by $\\tilde{w}_{i} = w_{i} / \\sum_{j=1}^{N} w_{j}$, define the dominance level by $\\rho = w_{1} / \\sum_{j=1}^{N} w_{j}$, and consider the Effective Sample Size (ESS), $N_{\\mathrm{eff}}$, as the standard degeneracy diagnostic in Sequential Monte Carlo.\n\nStarting from the canonical definition of degeneracy based on normalized weights used in particle filtering, derive a closed-form analytical expression for $N_{\\mathrm{eff}}$ as a function of the particle count $N$ and the dominance level $\\rho$, under the assumption that the $N-1$ non-dominant normalized weights are equal. Then, analyze the sensitivity of $N_{\\mathrm{eff}}$ with respect to $\\rho$ by characterizing its monotonicity and extremum as $\\rho$ varies over $[0,1]$, and interpret the implications for weight degeneracy and the curse of dimensionality.\n\nExpress your final answer for $N_{\\mathrm{eff}}$ as a single closed-form analytical expression in terms of $N$ and $\\rho$. No numerical evaluation or rounding is required.", "solution": "The problem statement is first validated to ensure it is scientifically sound, well-posed, and objective.\n\n### Step 1: Extract Givens\n- A Sequential Monte Carlo (SMC) particle filter is considered.\n- The number of particles is $N$.\n- The set of nonnegative importance weights is $\\{w_{i}\\}_{i=1}^{N}$.\n- The weight vector has a single dominant weight, $w_{1}$, which is larger than the other $N-1$ entries.\n- The $N-1$ non-dominant weights are equal to each other.\n- The normalized weights are defined as $\\tilde{w}_{i} = w_{i} / \\sum_{j=1}^{N} w_{j}$.\n- The dominance level is defined as $\\rho = w_{1} / \\sum_{j=1}^{N} w_{j}$.\n- The Effective Sample Size, $N_{\\mathrm{eff}}$, is used as the degeneracy diagnostic.\n- The task is to derive a closed-form expression for $N_{\\mathrm{eff}}$ as a function of $N$ and $\\rho$ using the canonical definition of degeneracy.\n- The task includes analyzing the sensitivity of $N_{\\mathrm{eff}}$ with respect to $\\rho$ by characterizing its monotonicity and extremum over $\\rho \\in [0,1]$.\n- The task requires an interpretation of the results in the context of weight degeneracy and the curse of dimensionality.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientific or Factual Soundness**: The problem is scientifically sound. It is based on standard, fundamental concepts in the field of data assimilation and statistical signal processing, specifically Sequential Monte Carlo methods (particle filters). The definitions of normalized weights, Effective Sample Size, weight degeneracy, and the curse of dimensionality are all canonical within this domain. The canonical definition of $N_{\\mathrm{eff}}$ is $N_{\\mathrm{eff}} = (\\sum_{i=1}^{N} \\tilde{w}_{i}^{2})^{-1}$.\n2.  **Non-Formalizable or Irrelevant**: The problem is highly relevant and directly formalizable. It poses a precise mathematical question within the specified topic.\n3.  **Incomplete or Contradictory Setup**: The setup is complete and internally consistent. The provided definitions and assumptions are sufficient to derive the requested expression. The assumption that $w_1$ is the largest weight implies $\\tilde{w}_1 > \\tilde{w}_j$ for $j \\neq 1$. This means $\\rho > (1-\\rho)/(N-1)$, which simplifies to $\\rho > 1/N$. The request to analyze the function over $\\rho \\in [0,1]$ is a standard mathematical exercise to understand the function's behavior, even if the sub-interval $\\rho \\in [0, 1/N]$ contradicts the \"dominance\" assumption. This does not invalidate the problem but is a point to be noted during the analysis. The core derivation is unaffected.\n4.  **Unrealistic or Infeasible**: The scenario described is highly realistic. Weight collapse onto a single or few particles is a common and critical issue in practical applications of particle filters, particularly in high-dimensional systems.\n5.  **Ill-Posed or Poorly Structured**: The problem is well-posed. The assumptions lead to a unique analytical solution. The terms are defined unambiguously.\n6.  **Pseudo-Profound, Trivial, or Tautological**: The problem is not trivial. It requires a formal derivation and analytical reasoning based on first principles of the method.\n7.  **Outside Scientific Verifiability**: The problem is entirely mathematical and its solution is verifiable through logical deduction and calculation.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. We may proceed with the solution.\n\n### Derivation of the Expression for $N_{\\mathrm{eff}}$\n\nThe canonical definition of the Effective Sample Size, $N_{\\mathrm{eff}}$, based on the normalized importance weights $\\{\\tilde{w}_{i}\\}_{i=1}^{N}$ is given by:\n$$N_{\\mathrm{eff}} = \\frac{1}{\\sum_{i=1}^{N} \\tilde{w}_{i}^{2}}$$\nThe sum of the normalized weights must equal $1$:\n$$\\sum_{i=1}^{N} \\tilde{w}_{i} = 1$$\nFrom the problem statement, the dominance level $\\rho$ is the normalized weight of the first particle:\n$$\\tilde{w}_{1} = \\rho$$\nThe problem also states that the remaining $N-1$ non-dominant weights are equal. Let us denote the value of these weights by $\\tilde{w}_{k}$ for $k \\in \\{2, 3, \\ldots, N\\}$. We can express $\\tilde{w}_{k}$ in terms of $\\rho$ and $N$.\nUsing the normalization condition:\n$$\\tilde{w}_{1} + \\sum_{k=2}^{N} \\tilde{w}_{k} = 1$$\nSubstituting $\\tilde{w}_{1} = \\rho$ and noting that the $N-1$ other weights are equal:\n$$\\rho + (N-1)\\tilde{w}_{k} = 1$$\nSolving for $\\tilde{w}_{k}$ (assuming $N > 1$, which is implicit in the problem's phrasing):\n$$\\tilde{w}_{k} = \\frac{1-\\rho}{N-1}$$\nNow, we can compute the sum of the squares of the normalized weights, $\\sum_{i=1}^{N} \\tilde{w}_{i}^{2}$:\n$$\\sum_{i=1}^{N} \\tilde{w}_{i}^{2} = \\tilde{w}_{1}^{2} + \\sum_{k=2}^{N} \\tilde{w}_{k}^{2}$$\nSubstituting the expressions for $\\tilde{w}_{1}$ and $\\tilde{w}_{k}$:\n$$\\sum_{i=1}^{N} \\tilde{w}_{i}^{2} = \\rho^{2} + (N-1) \\left( \\frac{1-\\rho}{N-1} \\right)^{2}$$\nSimplifying the expression:\n$$\\sum_{i=1}^{N} \\tilde{w}_{i}^{2} = \\rho^{2} + (N-1) \\frac{(1-\\rho)^{2}}{(N-1)^{2}} = \\rho^{2} + \\frac{(1-\\rho)^{2}}{N-1}$$\nTo combine the terms, we find a common denominator:\n$$\\sum_{i=1}^{N} \\tilde{w}_{i}^{2} = \\frac{(N-1)\\rho^{2} + (1-\\rho)^{2}}{N-1} = \\frac{(N-1)\\rho^{2} + (1 - 2\\rho + \\rho^{2})}{N-1}$$\n$$= \\frac{N\\rho^{2} - \\rho^{2} + 1 - 2\\rho + \\rho^{2}}{N-1} = \\frac{N\\rho^{2} - 2\\rho + 1}{N-1}$$\nFinally, we substitute this result back into the definition of $N_{\\mathrm{eff}}$:\n$$N_{\\mathrm{eff}} = \\frac{1}{\\frac{N\\rho^{2} - 2\\rho + 1}{N-1}} = \\frac{N-1}{N\\rho^{2} - 2\\rho + 1}$$\nThis is the closed-form analytical expression for $N_{\\mathrm{eff}}$ as a function of $N$ and $\\rho$.\n\n### Sensitivity Analysis and Interpretation\n\nTo analyze the sensitivity of $N_{\\mathrm{eff}}$ with respect to $\\rho$, we compute the first derivative $\\frac{d N_{\\mathrm{eff}}}{d \\rho}$. Let the denominator be $f(\\rho) = N\\rho^{2} - 2\\rho + 1$.\n$$\\frac{d N_{\\mathrm{eff}}}{d \\rho} = \\frac{d}{d\\rho} \\left( (N-1) (N\\rho^{2} - 2\\rho + 1)^{-1} \\right)$$\nUsing the chain rule:\n$$\\frac{d N_{\\mathrm{eff}}}{d \\rho} = (N-1) (-1) (N\\rho^{2} - 2\\rho + 1)^{-2} (2N\\rho - 2)$$\n$$\\frac{d N_{\\mathrm{eff}}}{d \\rho} = -2(N-1) \\frac{N\\rho - 1}{(N\\rho^{2} - 2\\rho + 1)^{2}}$$\nThe denominator $(N\\rho^{2} - 2\\rho + 1)^{2}$ is always non-negative. Its roots are complex for $N>1$, so it is strictly positive. The sign of the derivative is therefore determined by the sign of the numerator's non-constant term, $-(N\\rho - 1)$.\n1.  If $N\\rho - 1 > 0 \\implies \\rho > 1/N$, then $\\frac{d N_{\\mathrm{eff}}}{d \\rho} < 0$. $N_{\\mathrm{eff}}$ is a monotonically decreasing function of $\\rho$.\n2.  If $N\\rho - 1 < 0 \\implies \\rho < 1/N$, then $\\frac{d N_{\\mathrm{eff}}}{d \\rho} > 0$. $N_{\\mathrm{eff}}$ is a monotonically increasing function of $\\rho$.\n3.  If $N\\rho - 1 = 0 \\implies \\rho = 1/N$, then $\\frac{d N_{\\mathrm{eff}}}{d \\rho} = 0$. This indicates a critical point.\n\nThe analysis reveals an extremum at $\\rho = 1/N$. Since the derivative changes from positive to negative at this point, it is a local maximum. Let's evaluate $N_{\\mathrm{eff}}$ at the boundaries of the interval $[0,1]$ and at this critical point.\n\n-   **Maximum value**: At $\\rho = 1/N$,\n    $$N_{\\mathrm{eff}}\\left(\\rho = \\frac{1}{N}\\right) = \\frac{N-1}{N\\left(\\frac{1}{N}\\right)^{2} - 2\\left(\\frac{1}{N}\\right) + 1} = \\frac{N-1}{\\frac{1}{N} - \\frac{2}{N} + 1} = \\frac{N-1}{\\frac{N-1}{N}} = N$$\n    This case corresponds to all weights being equal, $\\tilde{w}_{i} = 1/N$ for all $i$, representing the absence of degeneracy. The effective sample size is equal to the actual number of particles.\n\n-   **Minimum value**: At $\\rho = 1$,\n    $$N_{\\mathrm{eff}}(\\rho = 1) = \\frac{N-1}{N(1)^{2} - 2(1) + 1} = \\frac{N-1}{N-1} = 1$$\n    This case corresponds to complete degeneracy, where one particle has all the weight ($\\tilde{w}_{1} = 1$) and all other particles have zero weight. The effective sample size collapses to $1$.\n\n-   At $\\rho = 0$:\n    $$N_{\\mathrm{eff}}(\\rho = 0) = \\frac{N-1}{N(0)^{2} - 2(0) + 1} = N-1$$\n    This corresponds to the dominant particle having zero weight, and the weight being uniformly distributed among the other $N-1$ particles.\n\nThe physically meaningful domain for $\\rho$, given that $\\tilde{w}_1$ is the largest weight, is $[\\frac{1}{N}, 1]$. Over this domain, $N_{\\mathrm{eff}}$ is a strictly monotonically decreasing function of $\\rho$.\n\n**Interpretation**:\n-   **Weight Degeneracy**: The derived expression and its analysis mathematically formalize the concept of weight degeneracy. As the weight distribution becomes more skewed (i.e., as $\\rho$ increases from its most uniform value of $1/N$ towards $1$), the effective number of particles that contribute to the approximation of the posterior distribution decreases drastically from $N$ to $1$. A low $N_{\\mathrm{eff}}$ is a signal that the particle representation is poor and a resampling step is necessary to mitigate the degeneracy by eliminating low-weight particles and replicating high-weight ones.\n-   **Curse of Dimensionality**: In high-dimensional state spaces, it is well-known that the likelihood function $p(\\text{observation}|\\text{state})$ tends to be highly concentrated in a small volume of the space. When updating the particle weights, it becomes exceedingly rare for particles to fall within this high-likelihood region. Consequently, after the update, most particles get near-zero weights, while one or a very few particles that happened to be in the right place capture nearly all the total weight. This scenario directly leads to a $\\rho$ value approaching $1$. Our analysis shows that as $\\rho \\to 1$, $N_{\\mathrm{eff}} \\to 1$ irrespective of the total number of particles $N$. This illustrates a key aspect of the curse of dimensionality for particle filters: simply increasing $N$ is not an efficient solution, as the number of particles required to adequately sample the high-likelihood region (and thus avoid weight collapse) typically grows exponentially with the dimension of the state space.", "answer": "$$\n\\boxed{\\frac{N-1}{N\\rho^2 - 2\\rho + 1}}\n$$", "id": "3417298"}, {"introduction": "When weight degeneracy becomes severe, as diagnosed by a low Effective Sample Size, the particle filter's performance degrades, and the standard corrective measure is resampling. This process rejuvenates the particle set by duplicating high-weight particles and eliminating low-weight ones, effectively focusing computational resources on more promising regions of the state space. This exercise provides a concrete, step-by-step implementation of systematic resampling, a widely used low-variance algorithm that is both efficient and effective at mitigating degeneracy `[@problem_id:3347857]`. By doing so, you will gain a deeper intuition for how weight collapse occurs and its connection to the infamous \"curse of dimensionality\".", "problem": "A single-cell transcriptional bursting model is tracked over time using a Sequential Monte Carlo (SMC) particle filter in a computational systems biology setting. After assimilating one fluorescence measurement at time step $t$, you have $N=8$ particles representing latent promoter activity states, with normalized importance weights $w_{1:8}$ obtained from Bayesian updating with a Gaussian observation model. The goal of resampling is to draw ancestor indices according to these normalized weights to mitigate weight degeneracy while preserving unbiasedness of the empirical measure.\n\nYou are given the normalized weight vector\n$$\nw_{1:8} = \\big(0.04,\\; 0.08,\\; 0.12,\\; 0.06,\\; 0.20,\\; 0.10,\\; 0.30,\\; 0.10\\big)\n$$,\nwhich satisfies $\\sum_{i=1}^{8} w_i = 1$. Implement systematic resampling at this time step using the $1$-based indexing convention, where the single random offset is drawn as $U \\sim \\mathrm{Uniform}(0, 1/N)$ and is realized as $U = 0.03$. Compute the cumulative sum vector of the weights and the $N$ equally spaced thresholds that start at $U$ and are separated by $1/N$, then determine, for each threshold, the smallest ancestor index whose cumulative sum exceeds or equals that threshold.\n\nReport the selected ancestor index vector of length $8$ as eight integers using $1$-based indexing. Express your final answer as a single row vector and do not include any units. No rounding is required.", "solution": "The problem statement is evaluated to be valid. It is scientifically grounded in the established principles of Sequential Monte Carlo methods, specifically particle filtering. The problem is well-posed, providing all necessary data—the number of particles $N$, the normalized weight vector $w_{1:8}$, and the realized random offset $U$—to compute a unique, deterministic answer. The problem is objective, complete, and free of contradictions. The requested task is a standard computational procedure in the field.\n\nThe problem requires the implementation of systematic resampling for a set of $N=8$ particles with given normalized importance weights. The procedure involves three main steps:\n1.  Compute the cumulative sum of the normalized weights.\n2.  Generate a set of $N$ ordered, uniformly spaced random numbers (thresholds) starting from a given random offset $U$.\n3.  For each threshold, identify the corresponding ancestor particle by finding the first particle whose cumulative weight exceeds or equals the threshold.\n\nThe given parameters are:\n-   Number of particles: $N=8$.\n-   Normalized weight vector: $w_{1:8} = \\left(0.04,\\; 0.08,\\; 0.12,\\; 0.06,\\; 0.20,\\; 0.10,\\; 0.30,\\; 0.10\\right)$.\n-   The random offset, drawn from $U \\sim \\mathrm{Uniform}(0, 1/N)$, is realized as $U=0.03$.\n-   The indexing is $1$-based.\n\nFirst, we compute the cumulative sum vector $c$ where $c_k = \\sum_{i=1}^{k} w_i$.\n$$\n\\begin{aligned}\nc_1 &= w_1 = 0.04 \\\\\nc_2 &= w_1 + w_2 = 0.04 + 0.08 = 0.12 \\\\\nc_3 &= c_2 + w_3 = 0.12 + 0.12 = 0.24 \\\\\nc_4 &= c_3 + w_4 = 0.24 + 0.06 = 0.30 \\\\\nc_5 &= c_4 + w_5 = 0.30 + 0.20 = 0.50 \\\\\nc_6 &= c_5 + w_6 = 0.50 + 0.10 = 0.60 \\\\\nc_7 &= c_6 + w_7 = 0.60 + 0.30 = 0.90 \\\\\nc_8 &= c_7 + w_8 = 0.90 + 0.10 = 1.00\n\\end{aligned}\n$$\nThe cumulative sum vector is $c_{1:8} = \\left(0.04,\\; 0.12,\\; 0.24,\\; 0.30,\\; 0.50,\\; 0.60,\\; 0.90,\\; 1.00\\right)$.\n\nSecond, we compute the $N=8$ equally spaced thresholds, which we denote by the vector $u_{1:8}$. The thresholds are generated according to the formula $u_j = U + \\frac{j-1}{N}$ for $j = 1, 2, \\ldots, N$. The spacing is $\\frac{1}{N} = \\frac{1}{8} = 0.125$.\n$$\n\\begin{aligned}\nu_1 &= U = 0.03 \\\\\nu_2 &= 0.03 + 1 \\times 0.125 = 0.155 \\\\\nu_3 &= 0.03 + 2 \\times 0.125 = 0.03 + 0.25 = 0.280 \\\\\nu_4 &= 0.03 + 3 \\times 0.125 = 0.03 + 0.375 = 0.405 \\\\\nu_5 &= 0.03 + 4 \\times 0.125 = 0.03 + 0.5 = 0.530 \\\\\nu_6 &= 0.03 + 5 \\times 0.125 = 0.03 + 0.625 = 0.655 \\\\\nu_7 &= 0.03 + 6 \\times 0.125 = 0.03 + 0.75 = 0.780 \\\\\nu_8 &= 0.03 + 7 \\times 0.125 = 0.03 + 0.875 = 0.905\n\\end{aligned}\n$$\nThe vector of thresholds is $u_{1:8} = \\left(0.03,\\; 0.155,\\; 0.280,\\; 0.405,\\; 0.530,\\; 0.655,\\; 0.780,\\; 0.905\\right)$.\n\nThird, we determine the selected ancestor indices. For each threshold $u_j$, we find the smallest integer index $k \\in \\{1, 2, ..., 8\\}$ such that $u_j \\le c_k$. Let this index be $a_j$.\n\n-   For $u_1 = 0.03$: We seek the smallest $k$ where $c_k \\ge 0.03$. Since $c_1 = 0.04 \\ge 0.03$, the selected index is $a_1=1$.\n-   For $u_2 = 0.155$: We seek the smallest $k$ where $c_k \\ge 0.155$. We have $c_2 = 0.12 < 0.155$ and $c_3 = 0.24 \\ge 0.155$. The selected index is $a_2=3$.\n-   For $u_3 = 0.280$: We seek the smallest $k$ where $c_k \\ge 0.280$. We have $c_3 = 0.24 < 0.280$ and $c_4 = 0.30 \\ge 0.280$. The selected index is $a_3=4$.\n-   For $u_4 = 0.405$: We seek the smallest $k$ where $c_k \\ge 0.405$. We have $c_4 = 0.30 < 0.405$ and $c_5 = 0.50 \\ge 0.405$. The selected index is $a_4=5$.\n-   For $u_5 = 0.530$: We seek the smallest $k$ where $c_k \\ge 0.530$. We have $c_5 = 0.50 < 0.530$ and $c_6 = 0.60 \\ge 0.530$. The selected index is $a_5=6$.\n-   For $u_6 = 0.655$: We seek the smallest $k$ where $c_k \\ge 0.655$. We have $c_6 = 0.60 < 0.655$ and $c_7 = 0.90 \\ge 0.655$. The selected index is $a_6=7$.\n-   For $u_7 = 0.780$: We seek the smallest $k$ where $c_k \\ge 0.780$. We have $c_6 = 0.60 < 0.780$ and $c_7 = 0.90 \\ge 0.780$. The selected index is $a_7=7$.\n-   For $u_8 = 0.905$: We seek the smallest $k$ where $c_k \\ge 0.905$. We have $c_7 = 0.90 < 0.905$ and $c_8 = 1.00 \\ge 0.905$. The selected index is $a_8=8$.\n\nCombining these results, the vector of selected ancestor indices is $(1, 3, 4, 5, 6, 7, 7, 8)$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1 & 3 & 4 & 5 & 6 & 7 & 7 & 8\n\\end{pmatrix}\n}\n$$", "id": "3347857"}]}