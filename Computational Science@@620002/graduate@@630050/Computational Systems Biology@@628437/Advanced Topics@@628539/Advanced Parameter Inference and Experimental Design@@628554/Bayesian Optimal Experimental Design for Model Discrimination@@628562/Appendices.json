{"hands_on_practices": [{"introduction": "The first step in optimal experimental design is to formally quantify the value of an experiment. This foundational exercise introduces one of the most common metrics, Mutual Information ($I$), which measures the expected information gain about the models after observing the experimental outcome. You will apply this principle to a concrete biological scenario, calculating the mutual information from first principles to select an optimal light-dark cycle that best distinguishes between two competing hypotheses of circadian clock regulation [@problem_id:3290038].", "problem": "Consider two competing mechanistic models of a circadian bioluminescence reporter in a cell population: a model without transcriptional feedback repression and a model with transcriptional feedback repression. Let the binary model indicator be $M \\in \\{0,1\\}$, where $M=0$ denotes the model without repression and $M=1$ denotes the model with repression. A Light–Dark (LD) cycle is specified by a design vector $d=(T,\\delta)$, where $T$ is the LD period in hours and $\\delta \\in [0,1]$ is the duty cycle (the fraction of the period during which light is on, expressed as a decimal). The experiment yields an averaged bioluminescence readout $Y$ constructed as the sample mean of $n$ replicate measurements taken under a fixed LD cycle $d$, where each replicate is a noisy observation of the underlying deterministic mean response.\n\nAssume a prior over models $p(M)$ with $p(M=0)=p_0$ and $p(M=1)=1-p_0$. Assume a Gaussian noise model consistent with the Central Limit Theorem for the sample mean: conditional on $M$ and $d$, the readout $Y$ is Gaussian with mean $\\mu_M(T,\\delta)$ and variance $\\sigma^2/n$, i.e., $Y \\mid M,d \\sim \\mathcal{N}(\\mu_M(T,\\delta), \\sigma^2/n)$, where $\\sigma^2$ is the replicate-level variance. The mean responses encode entrainment to the circadian resonance and differential sensitivity to light for the two models:\n- The resonance factor is $R(T) = \\exp\\left(-\\frac{1}{2}\\left(\\frac{T-\\tau_0}{\\sigma_T}\\right)^2\\right)$, where $\\tau_0$ is the intrinsic period (in hours) and $\\sigma_T$ is a scale parameter (in hours) representing the width of entrainment resonance.\n- For $M=0$ (no repression), the mean is $\\mu_0(T,\\delta) = A_0 \\, R(T) \\, \\delta$, where $A_0$ is a sensitivity parameter (in arbitrary units).\n- For $M=1$ (with repression), the mean is $\\mu_1(T,\\delta) = A_1 \\, R(T) \\, \\frac{\\delta}{1 + k \\delta}$, where $A_1$ is a sensitivity parameter (in arbitrary units) and $k>0$ controls repression strength (dimensionless).\n\nLet the experiment design objective be the Expected Information Gain, which in Bayesian optimal experimental design is the Mutual Information (MI) between the model indicator $M$ and the observed readout $Y$ given $d$, denoted $U(d)=I(M;Y \\mid d)$. This quantity is defined by the core Shannon mutual information identity\n$$\nI(M;Y \\mid d) = \\mathbb{E}_{p(m,y \\mid d)}\\left[\\log \\frac{p(m \\mid y,d)}{p(m)}\\right],\n$$\nwhere $p(m,y \\mid d) = p(m) \\, p(y \\mid m,d)$ and $p(m \\mid y,d)$ denotes the posterior over models given the observed readout and design.\n\nYour task is to implement a program that, for each provided parameter set, evaluates the objective $U(d)$ for a finite set of candidate designs $d=(T,\\delta)$, and selects the design with the largest $U(d)$. The program must compute $U(d)$ exactly according to the fundamental definition of mutual information, without using any shortcut formulas, by correctly marginalizing over the predictive distribution $p(y \\mid d)$ induced by the prior over $M$ and the conditional distributions $p(y \\mid m,d)$, and by taking the expectation over $y$ as required. The program should use scientifically realistic parameter values, a numerically stable approach to compute the required expectations, and adhere to the units and output format specifications below.\n\nUnits and quantities:\n- The LD period $T$ must be expressed in hours.\n- The duty cycle $\\delta$ must be expressed as a decimal fraction (no percentage sign).\n- The mutual information $U(d)$ must be computed using the natural logarithm, and thus expressed in nats.\n- The final reported $T$ values must be in hours and the final reported $U(d)$ values must be in nats.\n- Round $T$ and $\\delta$ to three decimal places and $U(d)$ to six decimal places in the final output.\n\nTest suite:\nFor each test case, you are given $(p_0, \\tau_0, \\sigma_T, A_0, A_1, k, \\sigma, n)$ and a finite candidate set $\\mathcal{D} = \\{(T_i, \\delta_j)\\}$ built from specified lists of $T$ values (in hours) and duty cycles (as decimals). For each case, evaluate $U(d)$ for all $d \\in \\mathcal{D}$, then return the single design with the largest $U(d)$ along with that maximum $U(d)$.\n\n- Test Case $1$ (general case, balanced prior):\n  - Parameters: $p_0=0.5$, $\\tau_0=24.0 \\, \\text{hours}$, $\\sigma_T=2.0 \\, \\text{hours}$, $A_0=1.0$, $A_1=1.2$, $k=2.0$, $\\sigma=0.2$, $n=50$.\n  - Candidate periods: $T \\in \\{20.0, 24.0, 28.0\\}$ hours.\n  - Candidate duty cycles: $\\delta \\in \\{0.3, 0.5, 0.8\\}$.\n- Test Case $2$ (boundary duty cycles, low-light regime):\n  - Parameters: $p_0=0.5$, $\\tau_0=24.0 \\, \\text{hours}$, $\\sigma_T=2.0 \\, \\text{hours}$, $A_0=1.0$, $A_1=1.2$, $k=2.0$, $\\sigma=0.2$, $n=50$.\n  - Candidate periods: $T \\in \\{22.0, 24.0, 26.0\\}$ hours.\n  - Candidate duty cycles: $\\delta \\in \\{0.01, 0.05, 0.1\\}$.\n- Test Case $3$ (near-indistinguishable models):\n  - Parameters: $p_0=0.5$, $\\tau_0=24.0 \\, \\text{hours}$, $\\sigma_T=2.0 \\, \\text{hours}$, $A_0=1.0$, $A_1=1.0$, $k=0.0$, $\\sigma=0.2$, $n=50$.\n  - Candidate periods: $T \\in \\{20.0, 24.0, 28.0\\}$ hours.\n  - Candidate duty cycles: $\\delta \\in \\{0.3, 0.5, 0.8\\}$.\n\nFinal output specification:\n- For each test case, return the triple $[T^\\star, \\delta^\\star, U^\\star]$, where $(T^\\star, \\delta^\\star)$ is the design in units specified above that maximizes $U(d)$ over the candidate set and $U^\\star = \\max_{d \\in \\mathcal{D}} U(d)$ in nats.\n- Your program should produce a single line of output containing the three results, in the exact format: a comma-separated list enclosed in square brackets, where each element is itself a list $[T^\\star,\\delta^\\star,U^\\star]$ with the rounding rules applied, e.g., $\\big[ [T^\\star_1,\\delta^\\star_1,U^\\star_1],[T^\\star_2,\\delta^\\star_2,U^\\star_2],[T^\\star_3,\\delta^\\star_3,U^\\star_3] \\big]$.", "solution": "The problem requires the identification of an optimal experimental design $d^{\\star}=(T^{\\star},\\delta^{\\star})$ from a finite candidate set $\\mathcal{D}$. The optimality criterion is the maximization of the Expected Information Gain, which is equivalent to the Shannon Mutual Information $U(d) = I(M;Y \\mid d)$ between the model indicator $M$ and the experimental readout $Y$, given a design $d=(T,\\delta)$.\n\nThe fundamental definition of mutual information is given by the expectation\n$$\nU(d) = I(M;Y \\mid d) = \\mathbb{E}_{p(m,y \\mid d)}\\left[\\log \\frac{p(m \\mid y,d)}{p(m)}\\right]\n$$\nThis expectation is taken over the joint distribution of the model indicator $M$ and the data $Y$, which is $p(m,y \\mid d) = p(m) p(y \\mid m, d)$. We can expand the expectation as a sum over the discrete model variable $M \\in \\{0,1\\}$ and an integral over the continuous data variable $Y$:\n$$\nU(d) = \\sum_{m \\in \\{0,1\\}} \\int_{-\\infty}^{\\infty} p(m) p(y \\mid m, d) \\log \\frac{p(m \\mid y,d)}{p(m)} \\, dy\n$$\nUsing the definition of the posterior probability $p(m \\mid y,d) = \\frac{p(y \\mid m,d) p(m)}{p(y \\mid d)}$, where $p(y \\mid d) = \\sum_{m'} p(m') p(y \\mid m', d)$ is the marginal predictive distribution of the data, we can rewrite the term inside the logarithm:\n$$\n\\frac{p(m \\mid y,d)}{p(m)} = \\frac{p(y \\mid m,d) p(m)}{p(y \\mid d) p(m)} = \\frac{p(y \\mid m,d)}{p(y \\mid d)}\n$$\nSubstituting this into the expression for $U(d)$ yields:\n$$\nU(d) = \\sum_{m \\in \\{0,1\\}} p(m) \\int_{-\\infty}^{\\infty} p(y \\mid m, d) \\log \\frac{p(y \\mid m,d)}{p(y \\mid d)} \\, dy\n$$\nThis form reveals that the mutual information is the expected Kullback-Leibler divergence from the marginal predictive distribution $p(y \\mid d)$ to the model-conditional distribution $p(y \\mid m, d)$, averaged over the prior model probabilities.\n\nThe specific components for this problem are:\n1.  **Prior Model Probabilities**: $p(M=0) = p_0$ and $p(M=1) = 1-p_0$.\n2.  **Conditional Data Distributions (Likelihoods)**: The readout $Y$ is the sample mean of $n$ replicates. By the Central Limit Theorem, its distribution is approximately Gaussian. The problem specifies this as exact: $Y \\mid M=m, d \\sim \\mathcal{N}(y; \\mu_m(d), \\sigma^2/n)$. Let's denote the effective variance as $\\sigma_{\\text{eff}}^2 = \\sigma^2/n$. The mean responses are:\n    $$\n    \\mu_0(d) = \\mu_0(T,\\delta) = A_0 \\, R(T) \\, \\delta\n    $$\n    $$\n    \\mu_1(d) = \\mu_1(T,\\delta) = A_1 \\, R(T) \\, \\frac{\\delta}{1 + k \\delta}\n    $$\n    where the resonance factor is $R(T) = \\exp\\left(-\\frac{1}{2}\\left(\\frac{T-\\tau_0}{\\sigma_T}\\right)^2\\right)$.\n3.  **Marginal Predictive Distribution**: This is a Gaussian Mixture Model (GMM):\n    $$\n    p(y \\mid d) = p_0 \\cdot \\mathcal{N}(y; \\mu_0(d), \\sigma_{\\text{eff}}^2) + (1-p_0) \\cdot \\mathcal{N}(y; \\mu_1(d), \\sigma_{\\text{eff}}^2)\n    $$\n\nThe objective function can be written as the sum of two terms:\n$U(d) = p_0 \\cdot \\text{Term}_0 + (1-p_0) \\cdot \\text{Term}_1$, where\n$$\n\\text{Term}_0 = \\int_{-\\infty}^{\\infty} \\mathcal{N}(y; \\mu_0, \\sigma_{\\text{eff}}^2) \\log \\frac{\\mathcal{N}(y; \\mu_0, \\sigma_{\\text{eff}}^2)}{p(y \\mid d)} \\, dy = \\mathbb{E}_{Y \\sim \\mathcal{N}(\\mu_0, \\sigma_{\\text{eff}}^2)}\\left[ \\log \\frac{\\mathcal{N}(Y; \\mu_0, \\sigma_{\\text{eff}}^2)}{p(Y \\mid d)} \\right]\n$$\n$$\n\\text{Term}_1 = \\int_{-\\infty}^{\\infty} \\mathcal{N}(y; \\mu_1, \\sigma_{\\text{eff}}^2) \\log \\frac{\\mathcal{N}(y; \\mu_1, \\sigma_{\\text{eff}}^2)}{p(y \\mid d)} \\, dy = \\mathbb{E}_{Y \\sim \\mathcal{N}(\\mu_1, \\sigma_{\\text{eff}}^2)}\\left[ \\log \\frac{\\mathcal{N}(Y; \\mu_1, \\sigma_{\\text{eff}}^2)}{p(Y \\mid d)} \\right]\n$$\nThese integrals do not have a general closed-form solution and must be computed numerically. A standard and numerically stable method for integrals of the form $\\int_{-\\infty}^{\\infty} e^{-x^2}g(x)dx$ is Gauss-Hermite quadrature. The expectations above are of the form $\\mathbb{E}_{Z \\sim \\mathcal{N}(0,1)}[f(\\mu + \\sigma Z)]$, which can be evaluated using quadrature. We utilize probabilist's Hermite polynomials, which are orthogonal with respect to the weight function $e^{-z^2/2}$, matching the kernel of the standard normal distribution. The expectation $\\mathbb{E}_{Z \\sim \\mathcal{N}(0,1)}[f(Z)] = \\int_{-\\infty}^{\\infty} \\frac{1}{\\sqrt{2\\pi}} e^{-z^2/2} f(z) dz$ can be approximated as $\\frac{1}{\\sqrt{2\\pi}} \\sum_{i=1}^{N_q} w_i f(z_i)$, where $(z_i, w_i)$ are the quadrature points and weights for the weight function $e^{-z^2/2}$.\n\nLet us define the integrands inside the expectations. For $\\text{Term}_0$, the argument of the logarithm is:\n$$\n\\frac{\\mathcal{N}(y; \\mu_0, \\sigma_{\\text{eff}}^2)}{p_0 \\mathcal{N}(y; \\mu_0, \\sigma_{\\text{eff}}^2) + (1-p_0) \\mathcal{N}(y; \\mu_1, \\sigma_{\\text{eff}}^2)} = \\frac{1}{p_0 + (1-p_0) \\frac{\\mathcal{N}(y; \\mu_1, \\sigma_{\\text{eff}}^2)}{\\mathcal{N}(y; \\mu_0, \\sigma_{\\text{eff}}^2)}}\n$$\nThe ratio of the Gaussian probability density functions simplifies to:\n$$\n\\frac{\\mathcal{N}(y; \\mu_1, \\sigma_{\\text{eff}}^2)}{\\mathcal{N}(y; \\mu_0, \\sigma_{\\text{eff}}^2)} = \\exp\\left( \\frac{(\\mu_1-\\mu_0)(y - (\\mu_0+\\mu_1)/2)}{\\sigma_{\\text{eff}}^2} \\right)\n$$\nThis formulation significantly improves numerical stability by avoiding potential underflow or overflow from computing individual PDF values. The expectations are then computed using quadrature over functions involving this exponential term.\n\nAn important special case arises when the models are parametrically indistinguishable. In Test Case $3$, the parameters $A_1=1.0$ and $k=0.0$ cause the mean response for model $M=1$ to become identical to that of model $M=0$:\n$$\n\\mu_1(d) = A_1 R(T) \\frac{\\delta}{1+k\\delta} = 1.0 \\cdot R(T) \\frac{\\delta}{1+0 \\cdot \\delta} = R(T) \\delta = A_0 R(T) \\delta = \\mu_0(d)\n$$\nWhen $\\mu_0(d) = \\mu_1(d)$, the conditional distributions $p(y \\mid M=0,d)$ and $p(y \\mid M=1,d)$ are identical. Consequently, the marginal $p(y \\mid d)$ is also the same distribution. The ratio $\\frac{p(y \\mid m,d)}{p(y \\mid d)}$ becomes $1$, its logarithm is $0$, and thus the mutual information $U(d)$ is necessarily $0$. No experiment can distinguish between two identical models, so the expected information gain is zero.\n\nThe overall algorithm is as follows:\n1. For each test case, define the model parameters and the set of candidate designs $\\mathcal{D}$.\n2. Initialize variables to store the optimal design $(T^{\\star}, \\delta^{\\star})$ and the maximum utility $U^{\\star}=-\\infty$.\n3. For each design $d=(T, \\delta)$ in $\\mathcal{D}$:\n    a. Calculate the mean responses $\\mu_0(d)$ and $\\mu_1(d)$.\n    b. If $\\mu_0(d) = \\mu_1(d)$, $U(d) = 0$. Otherwise, proceed.\n    c. Compute the two expectation integrals using Gauss-Hermite quadrature with a sufficiently high degree for accuracy.\n    d. Calculate $U(d) = p_0 \\cdot \\text{Term}_0 + (1-p_0) \\cdot \\text{Term}_1$.\n    e. If $U(d) > U^{\\star}$, update $U^{\\star} = U(d)$, $T^{\\star} = T$, and $\\delta^{\\star} = \\delta$.\n4. After evaluating all designs in $\\mathcal{D}$, the final $(T^{\\star}, \\delta^{\\star}, U^{\\star})$ is the result for the test case.\n5. Collate the results from all test cases into the specified output format, rounding the values as required.", "answer": "```python\nimport numpy as np\nfrom scipy.special import roots_hermitenorm\n\ndef solve():\n    \"\"\"\n    Solves the Bayesian optimal experimental design problem for model discrimination.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"params\": (0.5, 24.0, 2.0, 1.0, 1.2, 2.0, 0.2, 50),\n            \"T_candidates\": [20.0, 24.0, 28.0],\n            \"delta_candidates\": [0.3, 0.5, 0.8],\n        },\n        {\n            \"params\": (0.5, 24.0, 2.0, 1.0, 1.2, 2.0, 0.2, 50),\n            \"T_candidates\": [22.0, 24.0, 26.0],\n            \"delta_candidates\": [0.01, 0.05, 0.1],\n        },\n        {\n            \"params\": (0.5, 24.0, 2.0, 1.0, 1.0, 0.0, 0.2, 50),\n            \"T_candidates\": [20.0, 24.0, 28.0],\n            \"delta_candidates\": [0.3, 0.5, 0.8],\n        },\n    ]\n\n    all_results = []\n    \n    # Quadrature points and weights for numerical integration\n    # Using probabilist's Hermite polynomials (weight func exp(-x^2/2))\n    # A degree of 100 provides high accuracy.\n    quad_deg = 100\n    z_i, w_i = roots_hermitenorm(quad_deg)\n\n    for case in test_cases:\n        p0, tau0, sigma_T, A0, A1, k, sigma, n = case[\"params\"]\n        T_candidates = case[\"T_candidates\"]\n        delta_candidates = case[\"delta_candidates\"]\n\n        best_T = -1.0\n        best_delta = -1.0\n        max_U = -1.0\n\n        sigma_eff_sq = sigma**2 / n\n        sigma_eff = np.sqrt(sigma_eff_sq)\n\n        for T in T_candidates:\n            for delta in delta_candidates:\n                # Calculate resonance factor\n                R_T = np.exp(-0.5 * ((T - tau0) / sigma_T)**2)\n\n                # Calculate mean responses for the two models\n                mu0 = A0 * R_T * delta\n                mu1 = A1 * R_T * (delta / (1.0 + k * delta))\n                \n                # If models are indistinguishable, information gain is zero\n                if np.isclose(mu0, mu1):\n                    U_d = 0.0\n                else:\n                    # Define integrands for expectation calculation\n                    # The value y for the integrands will be sampled from the respective Gaussians\n                    # via change of variables in the quadrature.\n                    \n                    # Log-integrand for the expectation with respect to model 0\n                    def log_integrand_0(y):\n                        exponent = ((mu1 - mu0) * (y - (mu0 + mu1) / 2.0)) / sigma_eff_sq\n                        # Use log-sum-exp trick for stability, though direct computation is fine here\n                        # log(1 / (p0 + (1-p0) * exp(exponent))) = -log(p0 + (1-p0) * exp(exponent))\n                        return -np.log(p0 + (1.0 - p0) * np.exp(exponent))\n\n                    # Log-integrand for the expectation with respect to model 1\n                    def log_integrand_1(y):\n                        exponent = ((mu0 - mu1) * (y - (mu0 + mu1) / 2.0)) / sigma_eff_sq\n                        return -np.log(p0 * np.exp(exponent) + (1.0 - p0))\n\n                    # Perform numerical integration using Gauss-Hermite quadrature\n                    # E[f(Y)] where Y ~ N(mu, sigma^2) is E[f(mu + sigma*Z)] where Z ~ N(0,1)\n                    # The quadrature points z_i are effectively samples of Z.\n                    \n                    # Expectation w.r.t. model 0\n                    y_samples_0 = mu0 + sigma_eff * z_i\n                    integrand_vals_0 = log_integrand_0(y_samples_0)\n                    term_0 = np.sum(w_i * integrand_vals_0) / np.sqrt(2.0 * np.pi)\n\n                    # Expectation w.r.t. model 1\n                    y_samples_1 = mu1 + sigma_eff * z_i\n                    integrand_vals_1 = log_integrand_1(y_samples_1)\n                    term_1 = np.sum(w_i * integrand_vals_1) / np.sqrt(2.0 * np.pi)\n                    \n                    U_d = p0 * term_0 + (1.0 - p0) * term_1\n\n                if U_d > max_U:\n                    max_U = U_d\n                    best_T = T\n                    best_delta = delta\n\n        # Round to specified precision for output\n        T_star = round(best_T, 3)\n        delta_star = round(best_delta, 3)\n        U_star = round(max_U, 6)\n        \n        all_results.append(f\"[{T_star:.3f},{delta_star:.3f},{U_star:.6f}]\")\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(all_results)}]\")\n\nsolve()\n```", "id": "3290038"}, {"introduction": "Real-world models have parameters that are never known with perfect certainty. This practice advances our approach by incorporating this uncertainty, a hallmark of the Bayesian framework, using the Laplace approximation to propagate it into our predictions. The design criterion will be the Kullback-Leibler ($D_{\\mathrm{KL}}$) divergence between model predictive distributions, providing a powerful tool for designing an optimal temperature-shift experiment to distinguish kinetic models when their parameters are uncertain [@problem_id:3290050].", "problem": "You are tasked with constructing a complete program that computes an approximate Bayesian optimal experimental design criterion for discriminating between two competing kinetic models for a first-order biochemical conversion under temperature shifts. The models are an Arrhenius rate model versus a non-Arrhenius rate model based on a temperature coefficient formulation. The experiment consists of a two-step, piecewise-constant temperature protocol characterized by a baseline absolute temperature and a sequence of temperature changes. The observable is the converted fraction after the two-step protocol. You must use first principles and well-tested approximations to formulate and implement the computation of the design criterion.\n\nThe foundational base for this problem is as follows.\n\n- Temperature-dependent rate under the Arrhenius model: for absolute temperature $T$, the rate is $k_{\\text{Arr}}(T;\\theta_{\\text{Arr}}) = A \\exp\\!\\left(-E_{a}/(R T)\\right)$, where $A$ is the pre-exponential factor, $E_{a}$ is the activation energy, and $R$ is the universal gas constant.\n- Temperature-dependent rate under the non-Arrhenius model (temperature coefficient model): for absolute temperature $T$, the rate is $k_{\\text{non}}(T;\\theta_{\\text{non}}) = k_{\\text{ref}} \\, Q_{10}^{(T - T_{\\text{ref}})/10}$, where $k_{\\text{ref}}$ is the reference rate at $T_{\\text{ref}}$, and $Q_{10}$ is the temperature coefficient factor.\n- First-order conversion under piecewise constant rate: for step $i$ at temperature $T_{i}$ with duration $\\tau_{i}$, with rate $k(T_{i})$, the fraction converted at the end of the two-step protocol is $y = 1 - \\exp\\!\\left(-\\sum_{i=1}^{2} k(T_{i}) \\tau_{i}\\right)$.\n- Bayesian predictive distribution for the observable under model $m \\in \\{\\text{Arr}, \\text{non}\\}$ and design $\\Delta T$: $p(y \\mid m, \\Delta T) = \\int p(y \\mid \\theta_{m}, m, \\Delta T) \\, p(\\theta_{m} \\mid m) \\, d\\theta_{m}$, where $p(y \\mid \\theta_{m}, m, \\Delta T)$ is the likelihood of the measurement and $p(\\theta_{m} \\mid m)$ is the parameter prior.\n- Measurement model: additive Gaussian measurement noise, $y_{\\text{obs}} = y_{\\text{true}} + \\varepsilon$, with $\\varepsilon \\sim \\mathcal{N}(0,\\sigma_{y}^{2})$.\n- Kullback–Leibler divergence for model discrimination: the design objective for a given $\\Delta T$ is to maximize $D_{\\mathrm{KL}}\\!\\left(p(y \\mid m_{\\text{Arr}}, \\Delta T) \\,\\|\\, p(y \\mid m_{\\text{non}}, \\Delta T)\\right)$.\n\nYour task is to implement the following computational procedure grounded in the above base.\n\n- Use the Laplace (delta) method to approximate the predictive distribution $p(y \\mid m, \\Delta T)$ as a univariate Gaussian by linearizing the deterministic map from parameters to the observable around the prior mean of the parameters under each model. The resulting predictive distribution under each model is characterized by a mean and variance that incorporate both parameter uncertainty (via the linearization and the parameter prior covariance) and measurement noise variance $\\sigma_{y}^{2}$.\n- Using the resulting two Gaussian predictive distributions (one per model), compute the Kullback–Leibler divergence $D_{\\mathrm{KL}}\\!\\left(p(y \\mid m_{\\text{Arr}}, \\Delta T) \\,\\|\\, p(y \\mid m_{\\text{non}}, \\Delta T)\\right)$.\n- For a finite set of candidate two-step temperature-shift sequences $\\Delta T = [\\Delta T_{1}, \\Delta T_{2}]$ with specified step durations $[\\tau_{1}, \\tau_{2}]$, compute the divergence for each candidate and select the index (zero-based) of the sequence that maximizes the divergence, along with the corresponding maximum divergence value.\n\nAll temperatures must be in Kelvin (K). All times must be in seconds (s). All energies must be in joules per mole (J/mol). Angles are not used. The Kullback–Leibler divergence is dimensionless. The universal gas constant is $R = 8.314 \\,\\text{J}\\,\\text{mol}^{-1}\\,\\text{K}^{-1}$ unless otherwise specified within a test case. Use zero-based indexing for sequence indices. For the non-Arrhenius model, use $T_{\\text{ref}}$ equal to the baseline absolute temperature $T_{0}$ of the corresponding test case.\n\nParameterizations and priors to be used for linearization:\n\n- Arrhenius model parameters are $\\theta_{\\text{Arr}} = [\\ln A, E_{a}]^{\\top}$ with a Gaussian prior $\\mathcal{N}(\\mu_{\\text{Arr}}, \\Sigma_{\\text{Arr}})$.\n- Non-Arrhenius model parameters are $\\theta_{\\text{non}} = [\\ln k_{\\text{ref}}, \\ln Q_{10}]^{\\top}$ with a Gaussian prior $\\mathcal{N}(\\mu_{\\text{non}}, \\Sigma_{\\text{non}})$.\n\nYour program must implement the above procedure and apply it to the following test suite. Each test case provides a baseline absolute temperature $T_{0}$, a pair of step durations $[\\tau_{1}, \\tau_{2}]$, a list of candidate temperature-shift sequences $[\\Delta T_{1}, \\Delta T_{2}]$, parameter prior means and covariances for both models, measurement noise standard deviation $\\sigma_{y}$, the universal gas constant $R$, and the reference temperature $T_{\\text{ref}} = T_{0}$.\n\nTest suite:\n\n- Test case $1$ (happy path):\n  - $T_{0} = 310.0 \\,\\text{K}$, $[\\tau_{1}, \\tau_{2}] = [300.0, 300.0] \\,\\text{s}$.\n  - Candidate sequences (Kelvin): $[[-5.0, 5.0], [5.0, -5.0], [0.0, 0.0], [8.0, 8.0]]$.\n  - Arrhenius prior: $\\mu_{\\text{Arr}} = [\\ln(10^{5}), 60000.0]$, $\\Sigma_{\\text{Arr}} = \\mathrm{diag}([0.3^{2}, 5000.0^{2}])$.\n  - Non-Arrhenius prior: $\\mu_{\\text{non}} = [\\ln(0.1), \\ln(2.0)]$, $\\Sigma_{\\text{non}} = \\mathrm{diag}([0.2^{2}, 0.1^{2}])$.\n  - $\\sigma_{y} = 0.02$, $R = 8.314$, $T_{\\text{ref}} = T_{0}$.\n\n- Test case $2$ (boundary with zero duration):\n  - $T_{0} = 310.0 \\,\\text{K}$, $[\\tau_{1}, \\tau_{2}] = [0.0, 0.0] \\,\\text{s}$.\n  - Candidate sequences (Kelvin): $[[-5.0, 5.0], [5.0, -5.0]]$.\n  - Arrhenius prior: $\\mu_{\\text{Arr}} = [\\ln(10^{5}), 60000.0]$, $\\Sigma_{\\text{Arr}} = \\mathrm{diag}([0.3^{2}, 5000.0^{2}])$.\n  - Non-Arrhenius prior: $\\mu_{\\text{non}} = [\\ln(0.1), \\ln(2.0)]$, $\\Sigma_{\\text{non}} = \\mathrm{diag}([0.2^{2}, 0.1^{2}])$.\n  - $\\sigma_{y} = 0.02$, $R = 8.314$, $T_{\\text{ref}} = T_{0}$.\n\n- Test case $3$ (sharper discrimination with longer durations and smaller noise):\n  - $T_{0} = 305.0 \\,\\text{K}$, $[\\tau_{1}, \\tau_{2}] = [600.0, 600.0] \\,\\text{s}$.\n  - Candidate sequences (Kelvin): $[[-10.0, 10.0], [10.0, -10.0], [10.0, 10.0], [-10.0, -10.0], [0.0, 0.0], [5.0, -5.0]]$.\n  - Arrhenius prior: $\\mu_{\\text{Arr}} = [\\ln(5.0 \\times 10^{6}), 80000.0]$, $\\Sigma_{\\text{Arr}} = \\mathrm{diag}([0.25^{2}, 4000.0^{2}])$.\n  - Non-Arrhenius prior: $\\mu_{\\text{non}} = [\\ln(0.05), \\ln(1.8)]$, $\\Sigma_{\\text{non}} = \\mathrm{diag}([0.15^{2}, 0.08^{2}])$.\n  - $\\sigma_{y} = 0.005$, $R = 8.314$, $T_{\\text{ref}} = T_{0}$.\n\n- Test case $4$ (non-diagonal parameter covariances):\n  - $T_{0} = 315.0 \\,\\text{K}$, $[\\tau_{1}, \\tau_{2}] = [400.0, 800.0] \\,\\text{s}$.\n  - Candidate sequences (Kelvin): $[[-7.0, 3.0], [3.0, -7.0], [7.0, 7.0], [0.0, 0.0]]$.\n  - Arrhenius prior: $\\mu_{\\text{Arr}} = [\\ln(2.0 \\times 10^{5}), 70000.0]$, with standard deviations $\\sigma_{\\ln A} = 0.3$, $\\sigma_{E_{a}} = 6000.0$, correlation $\\rho = 0.25$, so\n    $\\Sigma_{\\text{Arr}} = \\begin{bmatrix} \\sigma_{\\ln A}^{2}  \\rho \\, \\sigma_{\\ln A} \\, \\sigma_{E_{a}} \\\\ \\rho \\, \\sigma_{\\ln A} \\, \\sigma_{E_{a}}  \\sigma_{E_{a}}^{2} \\end{bmatrix}$.\n  - Non-Arrhenius prior: $\\mu_{\\text{non}} = [\\ln(0.08), \\ln(2.1)]$, with standard deviations $\\sigma_{\\ln k_{\\text{ref}}} = 0.2$, $\\sigma_{\\ln Q_{10}} = 0.1$, correlation $\\rho = -0.2$, so\n    $\\Sigma_{\\text{non}} = \\begin{bmatrix} \\sigma_{\\ln k_{\\text{ref}}}^{2}  \\rho \\, \\sigma_{\\ln k_{\\text{ref}}} \\, \\sigma_{\\ln Q_{10}} \\\\ \\rho \\, \\sigma_{\\ln k_{\\text{ref}}} \\, \\sigma_{\\ln Q_{10}}  \\sigma_{\\ln Q_{10}}^{2} \\end{bmatrix}$.\n  - $\\sigma_{y} = 0.01$, $R = 8.314$, $T_{\\text{ref}} = T_{0}$.\n\nFor each test case, your program must:\n- Enumerate all candidate sequences in the provided order and compute $D_{\\mathrm{KL}}\\!\\left(p(y \\mid m_{\\text{Arr}}, \\Delta T) \\,\\|\\, p(y \\mid m_{\\text{non}}, \\Delta T)\\right)$ for each candidate using the Gaussian approximation derived via the Laplace (delta) method.\n- Select the zero-based index of the candidate sequence with the largest divergence. In case of ties, select the smallest index.\n- Report for each test case a pair consisting of the selected index (an integer) and the corresponding maximum divergence value (a float rounded to exactly six digits after the decimal point).\n\nFinal output format:\n- Your program should produce a single line of output containing the results for all test cases as a comma-separated list of pairs enclosed in square brackets, where each pair is formatted as $[i, d]$ with $i$ the selected zero-based index and $d$ the maximum divergence rounded to six decimals. For example: $[[0,0.123456],[2,0.000000],\\ldots]$.", "solution": "The problem is valid as it is scientifically grounded in chemical kinetics and Bayesian statistics, well-posed with a clear objective and all necessary information, and computationally tractable. The solution proceeds by first deriving the necessary mathematical formulas and then implementing them numerically.\n\nThe objective is to find the optimal experimental design $\\Delta T = [\\Delta T_{1}, \\Delta T_{2}]^{\\top}$ from a finite set of candidates. The optimal design is the one that maximizes the Kullback-Leibler (KL) divergence between the Bayesian predictive distributions of the observable $y$ for two competing models, an Arrhenius model ($m_{\\text{Arr}}$) and a non-Arrhenius model ($m_{\\text{non}}$).\nThe objective function is:\n$$\n\\Delta T^* = \\arg\\max_{\\Delta T} D_{\\mathrm{KL}}\\!\\left(p(y \\mid m_{\\text{Arr}}, \\Delta T) \\,\\|\\, p(y \\mid m_{\\text{non}}, \\Delta T)\\right)\n$$\n\nThe core of the problem lies in approximating the predictive distribution $p(y \\mid m, \\Delta T)$ for each model $m$. The problem specifies using the Laplace (or delta) method, which involves linearizing the model output with respect to its parameters.\n\nLet $g_m(\\theta_m; \\Delta T)$ represent the deterministic prediction of the observable $y$ for a given model $m$, parameter vector $\\theta_m$, and experimental design $\\Delta T$. The parameter prior for each model is given as a Gaussian distribution, $p(\\theta_m) = \\mathcal{N}(\\theta_m \\mid \\mu_m, \\Sigma_m)$. The measurement model is $y_{\\text{obs}} = y_{\\text{true}} + \\varepsilon$ with $\\varepsilon \\sim \\mathcal{N}(0, \\sigma_y^2)$.\n\nThe Laplace approximation linearizes $g_m(\\theta_m)$ around the prior mean $\\mu_m$:\n$$\ny \\approx g_m(\\mu_m) + \\nabla_{\\theta_m} g_m(\\mu_m)^{\\top} (\\theta_m - \\mu_m)\n$$\nUnder this linearization, the distribution of the true model output $y$, marginalized over the prior distribution of $\\theta_m$, is also a Gaussian. Its mean is $\\mathbb{E}[y] = g_m(\\mu_m)$, and its variance is $\\text{Var}[y] = \\nabla_{\\theta_m} g_m(\\mu_m)^{\\top} \\Sigma_m \\nabla_{\\theta_m} g_m(\\mu_m)$. Let us denote this parameter-induced variance as $\\sigma_{m, \\text{param}}^2$.\n\nThe final observable $y_{\\text{obs}}$ includes measurement noise. The approximate predictive distribution $p(y \\mid m, \\Delta T)$ is the distribution of the sum of the linearized model output and the measurement noise. Since both are independent Gaussians, the resulting predictive distribution is also a Gaussian, $p(y \\mid m, \\Delta T) \\approx \\mathcal{N}(y \\mid \\mu_{y,m}, \\sigma_{y,m}^2)$, with:\n\\begin{itemize}\n    \\item Predictive Mean: $\\mu_{y,m} = g_m(\\mu_m; \\Delta T)$\n    \\item Predictive Variance: $\\sigma_{y,m}^2 = \\sigma_{m, \\text{param}}^2 + \\sigma_y^2 = \\nabla_{\\theta_m} g_m(\\mu_m)^{\\top} \\Sigma_m \\nabla_{\\theta_m} g_m(\\mu_m) + \\sigma_y^2$\n\\end{itemize}\n\nNext, we specify the function $g_m$ and its gradient $\\nabla_{\\theta_m} g_m$ for both models.\nThe observable is the fraction converted, $y = 1 - \\exp(-\\lambda_m)$, where $\\lambda_m = \\sum_{i=1}^{2} k_m(T_i) \\tau_i$. The temperatures for the two steps are $T_1 = T_0 + \\Delta T_1$ and $T_2 = T_0 + \\Delta T_2$.\nUsing the chain rule, the gradient of $y = g_m(\\theta_m)$ with respect to $\\theta_m$ is:\n$$\n\\nabla_{\\theta_m} g_m = \\frac{\\partial g_m}{\\partial \\lambda_m} \\nabla_{\\theta_m} \\lambda_m = \\exp(-\\lambda_m) \\nabla_{\\theta_m} \\lambda_m\n$$\nwhere $\\nabla_{\\theta_m} \\lambda_m = \\tau_1 \\nabla_{\\theta_m} k_m(T_1; \\theta_m) + \\tau_2 \\nabla_{\\theta_m} k_m(T_2; \\theta_m)$. All derivatives are evaluated at the prior mean $\\theta_m = \\mu_m$.\n\n**Arrhenius Model ($m_{\\text{Arr}}$)**\n- Parameters: $\\theta_{\\text{Arr}} = [\\ln A, E_a]^{\\top}$.\n- Rate: $k_{\\text{Arr}}(T) = \\exp(\\ln A - E_a/(RT))$.\n- Gradients of the rate with respect to the parameters:\n  - $\\frac{\\partial k_{\\text{Arr}}}{\\partial (\\ln A)} = k_{\\text{Arr}}(T)$\n  - $\\frac{\\partial k_{\\text{Arr}}}{\\partial E_a} = -\\frac{k_{\\text{Arr}}(T)}{RT}$\n- The gradient vector is $\\nabla_{\\theta_{\\text{Arr}}} k_{\\text{Arr}} = [k_{\\text{Arr}}, -k_{\\text{Arr}}/(RT)]^{\\top}$.\n\n**Non-Arrhenius Model ($m_{\\text{non}}$)**\n- Parameters: $\\theta_{\\text{non}} = [\\ln k_{\\text{ref}}, \\ln Q_{10}]^{\\top}$.\n- Rate: $k_{\\text{non}}(T) = k_{\\text{ref}} Q_{10}^{(T-T_{\\text{ref}})/10} = \\exp\\left(\\ln k_{\\text{ref}} + (\\ln Q_{10}) \\frac{T-T_{\\text{ref}}}{10}\\right)$.\n- Gradients of the rate with respect to the parameters:\n  - $\\frac{\\partial k_{\\text{non}}}{\\partial (\\ln k_{\\text{ref}})} = k_{\\text{non}}(T)$\n  - $\\frac{\\partial k_{\\text{non}}}{\\partial (\\ln Q_{10})} = k_{\\text{non}}(T) \\frac{T-T_{\\text{ref}}}{10}$\n- The gradient vector is $\\nabla_{\\theta_{\\text{non}}} k_{\\text{non}} = [k_{\\text{non}}, k_{\\text{non}}\\frac{T-T_{\\text{ref}}}{10}]^{\\top}$.\n\nWith the means and variances of the two Gaussian predictive distributions, $(\\mu_{y, \\text{Arr}}, \\sigma_{y, \\text{Arr}}^2)$ and $(\\mu_{y, \\text{non}}, \\sigma_{y, \\text{non}}^2)$, we can compute the Kullback-Leibler divergence. For two univariate Gaussian distributions $P_1 = \\mathcal{N}(\\mu_1, \\sigma_1^2)$ and $P_2 = \\mathcal{N}(\\mu_2, \\sigma_2^2)$, the KL divergence is given by:\n$$\nD_{\\mathrm{KL}}(P_1 \\| P_2) = \\ln\\left(\\frac{\\sigma_2}{\\sigma_1}\\right) + \\frac{\\sigma_1^2 + (\\mu_1 - \\mu_2)^2}{2\\sigma_2^2} - \\frac{1}{2}\n$$\nIn our context, $P_1$ corresponds to the Arrhenius model's predictive distribution and $P_2$ to the non-Arrhenius model's.\n\nThe computational procedure for each test case is as follows:\n1. For each candidate design $\\Delta T = [\\Delta T_1, \\Delta T_2]$, calculate $T_1 = T_0 + \\Delta T_1$ and $T_2 = T_0 + \\Delta T_2$.\n2. For the Arrhenius model:\n   a. Evaluate rates $k_{\\text{Arr}}(T_1)$ and $k_{\\text{Arr}}(T_2)$ at the prior mean $\\mu_{\\text{Arr}}$.\n   b. Compute the predictive mean $\\mu_{y, \\text{Arr}}$.\n   c. Compute the gradient $\\nabla_{\\theta_{\\text{Arr}}} g_{\\text{Arr}}$ at $\\mu_{\\text{Arr}}$.\n   d. Compute the predictive variance $\\sigma_{y, \\text{Arr}}^2 = (\\nabla_{\\theta_{\\text{Arr}}} g_{\\text{Arr}})^{\\top} \\Sigma_{\\text{Arr}} (\\nabla_{\\theta_{\\text{Arr}}} g_{\\text{Arr}}) + \\sigma_y^2$.\n3. For the non-Arrhenius model:\n   a. Evaluate rates $k_{\\text{non}}(T_1)$ and $k_{\\text{non}}(T_2)$ at the prior mean $\\mu_{\\text{non}}$.\n   b. Compute the predictive mean $\\mu_{y, \\text{non}}$.\n   c. Compute the gradient $\\nabla_{\\theta_{\\text{non}}} g_{\\text{non}}$ at $\\mu_{\\text{non}}$.\n   d. Compute the predictive variance $\\sigma_{y, \\text{non}}^2 = (\\nabla_{\\theta_{\\text{non}}} g_{\\text{non}})^{\\top} \\Sigma_{\\text{non}} (\\nabla_{\\theta_{\\text{non}}} g_{\\text{non}}) + \\sigma_y^2$.\n4. Calculate $D_{\\mathrm{KL}}$ using the derived means and variances.\n5. After calculating the divergence for all candidate designs, identify the index of the design that yields the maximum divergence. Report this index and the corresponding maximum divergence value, rounded to six decimal places.\n\nThis procedure is implemented for each provided test case. For test case 2 where $\\tau_1 = \\tau_2 = 0$, $\\lambda_m$ is $0$ for all models and parameters, so $y=0$. The gradients are zero vectors, leading to zero parameter-induced variance. Both predictive distributions become identical, $\\mathcal{N}(0, \\sigma_y^2)$, and thus the KL divergence is $0$ for all designs. For test case $4$, the non-diagonal covariance matrices are first constructed from the provided standard deviations and correlation coefficients.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the Bayesian optimal experimental design criterion for model discrimination\n    between Arrhenius and non-Arrhenius kinetic models.\n    \"\"\"\n\n    def get_predictive_dist_params(\n        model_type, T0, delta_T, taus, mu_theta, Sigma_theta, sigma_y, R\n    ):\n        \"\"\"\n        Calculates the mean and variance of the Gaussian-approximated predictive distribution.\n        \"\"\"\n        T_ref = T0\n        T1 = T0 + delta_T[0]\n        T2 = T0 + delta_T[1]\n        tau1, tau2 = taus\n\n        if model_type == 'arrhenius':\n            lnA, Ea = mu_theta\n            \n            def k_rate(T, p):\n                lnA_p, Ea_p = p\n                return np.exp(lnA_p - Ea_p / (R * T))\n\n            k1 = k_rate(T1, mu_theta)\n            k2 = k_rate(T2, mu_theta)\n            \n            # Gradient of k w.r.t theta = [lnA, Ea]\n            grad_k1 = np.array([k1, -k1 / (R * T1)])\n            grad_k2 = np.array([k2, -k2 / (R * T2)])\n\n        elif model_type == 'non_arrhenius':\n            ln_k_ref, ln_Q10 = mu_theta\n\n            def k_rate(T, p):\n                ln_k_ref_p, ln_Q10_p = p\n                return np.exp(ln_k_ref_p + ln_Q10_p * (T - T_ref) / 10.0)\n\n            k1 = k_rate(T1, mu_theta)\n            k2 = k_rate(T2, mu_theta)\n\n            # Gradient of k w.r.t theta = [ln_k_ref, ln_Q10]\n            grad_k1 = np.array([k1, k1 * (T1 - T_ref) / 10.0])\n            grad_k2 = np.array([k2, k2 * (T2 - T_ref) / 10.0])\n        else:\n            raise ValueError(\"Unknown model type\")\n\n        # Total integrated rate argument\n        lambda_val = k1 * tau1 + k2 * tau2\n        \n        # Predictive mean\n        mu_y = 1.0 - np.exp(-lambda_val)\n\n        # Gradient of lambda w.r.t theta\n        grad_lambda = tau1 * grad_k1 + tau2 * grad_k2\n        \n        # Gradient of y w.r.t theta\n        grad_y = np.exp(-lambda_val) * grad_lambda\n        \n        # Parameter-induced variance\n        var_param = grad_y.T @ Sigma_theta @ grad_y\n        \n        # Total predictive variance\n        var_y = var_param + sigma_y**2\n        \n        return mu_y, var_y\n\n    def kl_divergence_gaussians(mu1, var1, mu2, var2):\n        \"\"\"\n        Computes KL divergence D_KL(N(mu1, var1) || N(mu2, var2)).\n        \"\"\"\n        if var1 = 0 or var2 = 0:\n            return 0.0 # Should not happen with sigma_y > 0\n        \n        # Using formula with variances to avoid sqrt\n        # D_KL = 0.5 * (log(var2/var1) + (var1 + (mu1-mu2)^2)/var2 - 1)\n        term1 = np.log(var2 / var1)\n        term2 = (var1 + (mu1 - mu2)**2) / var2\n        term3 = -1.0\n        \n        kl_div = 0.5 * (term1 + term2 + term3)\n        return kl_div if kl_div > 0 else 0.0\n\n    test_cases = [\n        {\n            \"T0\": 310.0, \"taus\": [300.0, 300.0],\n            \"candidates\": [[-5.0, 5.0], [5.0, -5.0], [0.0, 0.0], [8.0, 8.0]],\n            \"mu_arr\": [np.log(1e5), 60000.0],\n            \"Sigma_arr\": np.diag([0.3**2, 5000.0**2]),\n            \"mu_non\": [np.log(0.1), np.log(2.0)],\n            \"Sigma_non\": np.diag([0.2**2, 0.1**2]),\n            \"sigma_y\": 0.02, \"R\": 8.314\n        },\n        {\n            \"T0\": 310.0, \"taus\": [0.0, 0.0],\n            \"candidates\": [[-5.0, 5.0], [5.0, -5.0]],\n            \"mu_arr\": [np.log(1e5), 60000.0],\n            \"Sigma_arr\": np.diag([0.3**2, 5000.0**2]),\n            \"mu_non\": [np.log(0.1), np.log(2.0)],\n            \"Sigma_non\": np.diag([0.2**2, 0.1**2]),\n            \"sigma_y\": 0.02, \"R\": 8.314\n        },\n        {\n            \"T0\": 305.0, \"taus\": [600.0, 600.0],\n            \"candidates\": [[-10.0, 10.0], [10.0, -10.0], [10.0, 10.0], [-10.0, -10.0], [0.0, 0.0], [5.0, -5.0]],\n            \"mu_arr\": [np.log(5e6), 80000.0],\n            \"Sigma_arr\": np.diag([0.25**2, 4000.0**2]),\n            \"mu_non\": [np.log(0.05), np.log(1.8)],\n            \"Sigma_non\": np.diag([0.15**2, 0.08**2]),\n            \"sigma_y\": 0.005, \"R\": 8.314\n        },\n        {\n            \"T0\": 315.0, \"taus\": [400.0, 800.0],\n            \"candidates\": [[-7.0, 3.0], [3.0, -7.0], [7.0, 7.0], [0.0, 0.0]],\n            \"mu_arr\": [np.log(2e5), 70000.0],\n            \"Sigma_arr\": None, # Will be constructed\n            \"mu_non\": [np.log(0.08), np.log(2.1)],\n            \"Sigma_non\": None, # Will be constructed\n            \"sigma_y\": 0.01, \"R\": 8.314\n        },\n    ]\n\n    # Construct covariance matrices for test case 4\n    s_lnA, s_Ea, rho_arr = 0.3, 6000.0, 0.25\n    cov_arr = rho_arr * s_lnA * s_Ea\n    test_cases[3][\"Sigma_arr\"] = np.array([[s_lnA**2, cov_arr], [cov_arr, s_Ea**2]])\n    \n    s_lnk, s_lnQ, rho_non = 0.2, 0.1, -0.2\n    cov_non = rho_non * s_lnk * s_lnQ\n    test_cases[3][\"Sigma_non\"] = np.array([[s_lnk**2, cov_non], [cov_non, s_lnQ**2]])\n\n    \n    final_results = []\n    for case in test_cases:\n        divergences = []\n        for delta_T in case[\"candidates\"]:\n            mu_arr, var_arr = get_predictive_dist_params(\n                'arrhenius', case[\"T0\"], delta_T, case[\"taus\"], \n                case[\"mu_arr\"], case[\"Sigma_arr\"], case[\"sigma_y\"], case[\"R\"]\n            )\n            mu_non, var_non = get_predictive_dist_params(\n                'non_arrhenius', case[\"T0\"], delta_T, case[\"taus\"], \n                case[\"mu_non\"], case[\"Sigma_non\"], case[\"sigma_y\"], case[\"R\"]\n            )\n            \n            kl_div = kl_divergence_gaussians(mu_arr, var_arr, mu_non, var_non)\n            divergences.append(kl_div)\n            \n        max_div = -1.0\n        best_idx = -1\n        for i, div in enumerate(divergences):\n            if div > max_div:\n                max_div = div\n                best_idx = i\n        \n        final_results.append([best_idx, max_div])\n\n    # Format output as specified\n    formatted_results = [f\"[{res[0]},{res[1]:.6f}]\" for res in final_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3290050"}, {"introduction": "The most powerful experimental campaigns are often adaptive, where the result of one experiment informs the design of the next. This practice introduces the concept of a one-step lookahead policy for sequential experimental design, where the goal is to choose the *next* best experiment by maximizing the *expected* future information gain. By implementing this policy, you will build a strategy that can be applied iteratively to zero in on the true underlying biological model with maximal efficiency [@problem_id:3290046].", "problem": "Consider a finite set of competing dynamical hypotheses for a biological regulatory mechanism, represented as a discrete model variable $M \\in \\{1,\\dots,K\\}$, with a current Bayesian posterior $p(M=m \\mid \\mathcal{D})$ given prior data $\\mathcal{D}$. A new experiment consists of choosing a design $d$ from a finite candidate set $\\mathcal{A}$ and then observing a discrete count outcome $Y \\in \\{0,1,2,\\dots\\}$. For each model $m$ and design $d$, the predictive sampling distribution for $Y$ is Poisson with a model- and design-specific rate parameter $\\lambda_{m}(d)$, so that $p(Y=y \\mid M=m, d) = \\mathrm{Poisson}(y; \\lambda_{m}(d))$ for all nonnegative integers $y$. The model posterior $p(M \\mid \\mathcal{D})$ and the within-model predictive distributions $p(Y \\mid M=m, d)$ are assumed known. Let $H(\\cdot)$ denote Shannon entropy in natural units (nats), defined for a discrete distribution $q$ on a finite set by $H(q) = -\\sum_{i} q_{i} \\log q_{i}$ with the convention $0 \\log 0 = 0$.\n\nYour task is to construct a one-step lookahead Bayesian optimal experimental design policy for sequential model discrimination, selecting $d \\in \\mathcal{A}$ to maximize the expected reduction in uncertainty about $M$ after observing $Y$. The selection objective for a fixed $d$ is the expected entropy reduction\n$$\n\\mathbb{E}_{Y \\sim p(\\cdot \\mid \\mathcal{D}, d)} \\big[ H(M \\mid \\mathcal{D}) - H(M \\mid \\mathcal{D} \\cup \\{(d,Y)\\}) \\big],\n$$\nwhere $p(y \\mid \\mathcal{D}, d) = \\sum_{m=1}^{K} p(M=m \\mid \\mathcal{D}) \\, p(y \\mid M=m, d)$ and $p(M=m \\mid \\mathcal{D} \\cup \\{(d,y)\\}) \\propto p(M=m \\mid \\mathcal{D}) \\, p(y \\mid M=m, d)$ by Bayes’ rule. Because $H(M \\mid \\mathcal{D})$ does not depend on $d$, maximizing the expected reduction is equivalent to minimizing the expected posterior entropy $\\mathbb{E}_{Y} \\big[ H(M \\mid \\mathcal{D} \\cup \\{(d,Y)\\}) \\big]$.\n\nImplement a program that, for each test case, computes the expected entropy reduction for each design $d \\in \\mathcal{A}$ and returns the index of a maximizing design together with its expected reduction value. The Poisson outcomes are countably infinite; for numerical evaluation, approximate the expectation by summing over nonnegative integers until the remaining predictive tail probability is below a tolerance $\\varepsilon$, where $\\varepsilon$ is specified below. Use natural logarithms so that entropy is reported in nats. If multiple designs achieve the same maximal value up to an absolute difference less than a tie threshold $\\delta$, break ties by selecting the smallest index.\n\nAll computations must be implemented in a way that is independent of any physical unit. No physical units are involved. Angles are not used. All probabilities must be valid (nonnegative and summing to $1$ within numerical tolerance). The final outputs must be numerical.\n\nTest suite and required numerical parameters:\n- Use truncation tolerance $\\varepsilon = 10^{-8}$ for the predictive tail per design. Use tie threshold $\\delta = 10^{-12}$ for comparing expected reductions across designs.\n- For each test case, the candidate design set $\\mathcal{A}$ is indexed as $\\{0,1,\\dots, D-1\\}$ where $D$ is the number of designs in that case. Report the chosen design by its index.\n\nProvide the following four test cases. Each test case specifies $K$, a prior over models, and a matrix of Poisson rates $\\lambda_{m}(d)$ for each design $d$ and model $m$. In each case, compute and report the pair consisting of the chosen design index and the corresponding expected entropy reduction rounded to exactly $6$ decimal places.\n\n- Test case A (happy path, two models, three designs):\n  - Number of models $K = 2$.\n  - Prior $p(M \\mid \\mathcal{D}) = [0.5, 0.5]$.\n  - Designs $\\mathcal{A} = \\{0,1,2\\}$ with rates\n    - $d=0$: $(\\lambda_{1}(0), \\lambda_{2}(0)) = (2.0, 8.0)$,\n    - $d=1$: $(\\lambda_{1}(1), \\lambda_{2}(1)) = (5.0, 5.0)$,\n    - $d=2$: $(\\lambda_{1}(2), \\lambda_{2}(2)) = (1.0, 9.0)$.\n\n- Test case B (boundary prior, two models, three designs):\n  - Number of models $K = 2$.\n  - Prior $p(M \\mid \\mathcal{D}) = [0.95, 0.05]$.\n  - Designs $\\mathcal{A} = \\{0,1,2\\}$ with the same rates as in test case A.\n\n- Test case C (tie case, two models, three designs):\n  - Number of models $K = 2$.\n  - Prior $p(M \\mid \\mathcal{D}) = [0.5, 0.5]$.\n  - Designs $\\mathcal{A} = \\{0,1,2\\}$ with rates\n    - $d=0$: $(\\lambda_{1}(0), \\lambda_{2}(0)) = (4.0, 6.0)$,\n    - $d=1$: $(\\lambda_{1}(1), \\lambda_{2}(1)) = (4.0, 6.0)$,\n    - $d=2$: $(\\lambda_{1}(2), \\lambda_{2}(2)) = (5.0, 5.0)$.\n\n- Test case D (three models, four designs):\n  - Number of models $K = 3$.\n  - Prior $p(M \\mid \\mathcal{D}) = [0.2, 0.3, 0.5]$.\n  - Designs $\\mathcal{A} = \\{0,1,2,3\\}$ with rates\n    - $d=0$: $(\\lambda_{1}(0), \\lambda_{2}(0), \\lambda_{3}(0)) = (3.0, 3.5, 3.2)$,\n    - $d=1$: $(\\lambda_{1}(1), \\lambda_{2}(1), \\lambda_{3}(1)) = (2.0, 6.0, 10.0)$,\n    - $d=2$: $(\\lambda_{1}(2), \\lambda_{2}(2), \\lambda_{3}(2)) = (8.0, 4.0, 6.0)$,\n    - $d=3$: $(\\lambda_{1}(3), \\lambda_{2}(3), \\lambda_{3}(3)) = (1.0, 3.0, 9.0)$.\n\nAlgorithmic requirements:\n- For each design $d$, compute the mixture predictive mass function $p(y \\mid \\mathcal{D}, d)$ and the corresponding posterior model probabilities $p(M \\mid \\mathcal{D} \\cup \\{(d,y)\\})$ for each nonnegative integer $y$ in a truncated range $\\{0,1,\\dots,y_{\\max}\\}$ chosen so that $\\sum_{y=0}^{y_{\\max}} p(y \\mid \\mathcal{D}, d) \\ge 1 - \\varepsilon$.\n- Use the identity $p(M=m \\mid \\mathcal{D} \\cup \\{(d,y)\\}) \\propto p(M=m \\mid \\mathcal{D}) \\, p(y \\mid M=m, d)$ to compute posteriors for each $y$ and then compute the entropy $H(M \\mid \\mathcal{D} \\cup \\{(d,y)\\})$.\n- Compute the expected posterior entropy $\\sum_{y=0}^{y_{\\max}} p(y \\mid \\mathcal{D}, d) \\, H(M \\mid \\mathcal{D} \\cup \\{(d,y)\\})$ and subtract it from $H(M \\mid \\mathcal{D})$ to obtain the expected entropy reduction for that $d$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case in the order A, B, C, D and is itself a two-element list $[d^{\\star}, u^{\\star}]$ with $d^{\\star}$ the chosen design index and $u^{\\star}$ the expected entropy reduction rounded to exactly $6$ decimal places. For example, a valid overall output might look like $[[0,0.123456],[2,0.000321],[0,0.045678],[1,0.234567]]$.", "solution": "The problem requires us to devise a one-step lookahead optimal experimental design policy for model discrimination. We are given a finite set of $K$ competing models, indexed by $m \\in \\{1, \\dots, K\\}$, and a prior probability distribution over them, $p(M=m \\mid \\mathcal{D})$. The goal is to select an experimental design $d$ from a finite set $\\mathcal{A}$ that is expected to provide the most information to distinguish between these models. The experimental outcome $Y$ is a discrete count, modeled by a Poisson distribution with a rate $\\lambda_m(d)$ that depends on both the true model $m$ and the chosen design $d$.\n\nThe core of the policy is to maximize the expected reduction in Shannon entropy of the model distribution. For a given design $d$, this utility function $U(d)$ is defined as:\n$$\nU(d) = \\mathbb{E}_{Y \\sim p(\\cdot \\mid \\mathcal{D}, d)} \\big[ H(M \\mid \\mathcal{D}) - H(M \\mid \\mathcal{D} \\cup \\{(d,Y)\\}) \\big]\n$$\nThis quantity represents the expected information gain about $M$ from an experiment with design $d$. The initial entropy, $H(M \\mid \\mathcal{D})$, is calculated from the given model probabilities $p(M=m \\mid \\mathcal{D})$ and is constant across all candidate designs. Maximizing $U(d)$ is therefore equivalent to minimizing the expected posterior entropy, $\\mathbb{E}_{Y}[H(M \\mid \\mathcal{D} \\cup \\{(d,Y)\\})]$.\n\nThe computational strategy for each design $d \\in \\mathcal{A}$ involves the following steps:\n\n1.  **Prior Entropy**: The initial entropy is computed from the prior (or current posterior) distribution $p(M \\mid \\mathcal{D})$:\n    $$\n    H(M \\mid \\mathcal{D}) = - \\sum_{m=1}^{K} p(M=m \\mid \\mathcal{D}) \\log p(M=m \\mid \\mathcal{D})\n    $$\n    The natural logarithm is used, so entropy is in units of nats.\n\n2.  **Predictive Distribution and Bayesian Update**: For each possible outcome $y \\in \\{0, 1, 2, \\dots\\}$, we must evaluate the posterior probability of each model. This requires two components:\n    -   The marginal predictive probability of observing $y$, obtained by marginalizing over the models:\n        $$\n        p(y \\mid \\mathcal{D}, d) = \\sum_{m=1}^{K} p(M=m \\mid \\mathcal{D}) \\, p(y \\mid M=m, d)\n        $$\n        where $p(y \\mid M=m, d)$ is the Poisson PMF, $\\mathrm{Poisson}(y; \\lambda_m(d)) = \\frac{\\lambda_m(d)^y e^{-\\lambda_m(d)}}{y!}$.\n    -   The updated posterior probability for each model $m$ given outcome $y$, via Bayes' rule:\n        $$\n        p(M=m \\mid \\mathcal{D}, d, y) = \\frac{p(M=m \\mid \\mathcal{D}) \\, p(y \\mid M=m, d)}{p(y \\mid \\mathcal{D}, d)}\n        $$\n\n3.  **Expected Posterior Entropy**: The expectation is an infinite sum over all non-negative integers $y$. For numerical computation, this sum is truncated at a value $y_{\\max}$ determined such that the remaining tail probability is negligible. We find the smallest $y_{\\max}$ for which $\\sum_{y=0}^{y_{\\max}} p(y \\mid \\mathcal{D}, d) \\ge 1 - \\varepsilon$, with a tolerance of $\\varepsilon = 10^{-8}$. The expected posterior entropy is then approximated by:\n    $$\n    \\mathbb{E}_{Y}[H(M \\mid \\dots)] \\approx \\sum_{y=0}^{y_{\\max}} p(y \\mid \\mathcal{D}, d) \\left( - \\sum_{m=1}^{K} p(M=m \\mid \\mathcal{D}, d, y) \\log p(M=m \\mid \\mathcal{D}, d, y) \\right)\n    $$\n\n4.  **Utility and Optimal Design**: The expected entropy reduction $U(d)$ is computed as $U(d) = H(M \\mid \\mathcal{D}) - \\mathbb{E}_{Y}[H(M \\mid \\dots)]$. This process is repeated for every design $d \\in \\mathcal{A}$. The optimal design $d^\\star$ is the one with the highest utility $U(d)$. If multiple designs yield a maximal utility value that are equal within a tolerance $\\delta = 10^{-12}$, the tie is broken by selecting the design with the smallest index.\n\nTo ensure numerical stability, especially when dealing with the Poisson PMF which involves factorials and powers, all intermediate probability calculations are performed in the logarithmic domain. The `log-sum-exp` trick is used for stable summation of exponentiated values. The final algorithm calculates $U(d)$ for all $d$ and selects the optimal one, $d^\\star$, reporting both $d^\\star$ and its utility $U(^\\star)$.", "answer": "```python\nimport numpy as np\nfrom scipy.special import gammaln, logsumexp\nfrom scipy.stats import entropy\n\ndef solve():\n    \"\"\"\n    Solves the Bayesian optimal experimental design problem for model discrimination\n    for a series of test cases.\n    \"\"\"\n    test_cases = [\n        {  # Test case A\n            \"prior\": np.array([0.5, 0.5]),\n            \"lambdas\": np.array([\n                [2.0, 8.0],\n                [5.0, 5.0],\n                [1.0, 9.0]\n            ])\n        },\n        {  # Test case B\n            \"prior\": np.array([0.95, 0.05]),\n            \"lambdas\": np.array([\n                [2.0, 8.0],\n                [5.0, 5.0],\n                [1.0, 9.0]\n            ])\n        },\n        {  # Test case C\n            \"prior\": np.array([0.5, 0.5]),\n            \"lambdas\": np.array([\n                [4.0, 6.0],\n                [4.0, 6.0],\n                [5.0, 5.0]\n            ])\n        },\n        {  # Test case D\n            \"prior\": np.array([0.2, 0.3, 0.5]),\n            \"lambdas\": np.array([\n                [3.0, 3.5, 3.2],\n                [2.0, 6.0, 10.0],\n                [8.0, 4.0, 6.0],\n                [1.0, 3.0, 9.0]\n            ])\n        }\n    ]\n\n    EPSILON = 1e-8\n    DELTA = 1e-12\n    \n    results = []\n\n    for case in test_cases:\n        prior = case[\"prior\"]\n        all_lambdas = case[\"lambdas\"]\n        \n        # Pre-calculate log prior and prior entropy\n        log_prior = np.log(prior)\n        prior_entropy = entropy(prior, base=np.e)\n        \n        design_utilities = []\n        \n        for lambdas_d in all_lambdas:\n            expected_posterior_entropy = 0.0\n            y = 0\n            cumulative_prob = 0.0\n            \n            # The loop determines y_max implicitly by summing until the tail is negligible\n            while cumulative_prob  1.0 - EPSILON:\n                # Use log scale for numerical stability\n                # log(y!) = log(Gamma(y+1))\n                log_y_factorial = gammaln(y + 1)\n                \n                # log P(y | M=m, d) for all m\n                log_likelihoods = y * np.log(lambdas_d) - lambdas_d - log_y_factorial\n                \n                # log P(M=m, y | d) = log P(M=m) + log P(y | M=m, d)\n                log_joints = log_prior + log_likelihoods\n                \n                # log P(y | d) = log(sum_m(exp(log_joints_m)))\n                log_marginal_y = logsumexp(log_joints)\n                marginal_y = np.exp(log_marginal_y)\n\n                if marginal_y > 1e-30:  # Avoid operations on near-zero probabilities\n                    # log P(M=m | y, d) = log_joint - log_marginal\n                    log_posteriors = log_joints - log_marginal_y\n                    posteriors = np.exp(log_posteriors)\n                    \n                    # H(M | y,d)\n                    posterior_entropy_y = entropy(posteriors, base=np.e)\n                    \n                    # Accumulate expected posterior entropy\n                    expected_posterior_entropy += marginal_y * posterior_entropy_y\n\n                cumulative_prob += marginal_y\n                y += 1\n                \n                # Safety break for pathologically large lambdas not in test cases\n                # The y_max for given test cases is well below this.\n                if y > 1000:\n                    break\n\n            utility = prior_entropy - expected_posterior_entropy\n            design_utilities.append(utility)\n            \n        # Determine the best design based on utility and tie-breaking rule\n        max_utility = -np.inf\n        if design_utilities:\n            max_utility = np.max(design_utilities)\n\n        best_design_index = -1\n        # Find the smallest index of designs that are close to the max utility\n        for i, u in enumerate(design_utilities):\n            if max_utility - u  DELTA:\n                best_design_index = i\n                break\n        \n        best_utility = design_utilities[best_design_index]\n        results.append(f\"[{best_design_index},{best_utility:.6f}]\")\n\n    # Print the final result in the specified format\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3290046"}]}