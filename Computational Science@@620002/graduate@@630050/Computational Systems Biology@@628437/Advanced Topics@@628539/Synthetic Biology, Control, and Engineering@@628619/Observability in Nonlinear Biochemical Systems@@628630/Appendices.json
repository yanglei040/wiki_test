{"hands_on_practices": [{"introduction": "To build a solid understanding of nonlinear observability, we begin with the fundamental mathematical machinery. This practice requires the direct application of Lie derivatives to test the local observability rank condition for a cooperative genetic circuit. By calculating the successive derivatives of the output along the system's vector field, you will see how information about an unmeasured state, $x_2$, propagates into the time-series of the measured state, $x_1$, even when its direct influence appears to vanish at a specific operating point [@problem_id:3334957]. This exercise is a foundational skill, revealing the core mechanism that makes state estimation possible in nonlinear systems.", "problem": "Consider a two-species biochemical regulatory module with cooperative activation, modeled by the nonlinear ordinary differential equations\n$$\n\\dot{x}_1 \\;=\\; \\alpha_1\\,\\frac{x_2^2}{K_1^2 + x_2^2} \\;-\\; \\beta_1\\,x_1, \n\\qquad\n\\dot{x}_2 \\;=\\; \\alpha_2\\,\\frac{x_1^m}{K_2^m + x_1^m} \\;-\\; \\beta_2\\,x_2,\n$$\nwhere $x_1$ and $x_2$ are concentrations, $\\alpha_1, \\alpha_2, \\beta_1, \\beta_2, K_1, K_2$ are strictly positive constants, and $m \\geq 1$ is an integer Hill exponent capturing cooperativity of $x_1$ on the production of $x_2$. Assume that only the first species is measured, with output \n$$\nh(x) \\;=\\; x_1.\n$$\nUse the definition of the Lie derivative of a scalar function $h$ along a vector field $f$, namely $L_f h(x) = \\nabla h(x)\\cdot f(x)$, and its recursion $L_f^{k+1} h(x) = \\nabla \\big(L_f^k h(x)\\big)\\cdot f(x)$ for $k \\geq 0$. Explicitly compute $h(x)$, $L_f h(x)$, and $L_f^2 h(x)$, together with their gradients $\\nabla h(x)$, $\\nabla \\big(L_f h(x)\\big)$, and $\\nabla \\big(L_f^2 h(x)\\big)$. Then, form the observability matrix $\\mathcal{O}(x)$ whose rows are these gradients and evaluate its rank at the state \n$$\nx^\\star \\;=\\; (K_2,\\,0),\n$$\nwhich represents the second species near zero while the first species is at a nonzero baseline. Provide the integer value of the rank of $\\mathcal{O}(x^\\star)$ as your final answer.", "solution": "The problem requires an analysis of the local observability of a nonlinear biochemical system at a specific point in the state space. The system is described by the state vector $x = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}$ and its dynamics are governed by the vector field $f(x) = \\begin{pmatrix} f_1(x) \\\\ f_2(x) \\end{pmatrix}$, where\n$$\nf_1(x) = \\alpha_1\\,\\frac{x_2^2}{K_1^2 + x_2^2} - \\beta_1\\,x_1\n$$\n$$\nf_2(x) = \\alpha_2\\,\\frac{x_1^m}{K_2^m + x_1^m} - \\beta_2\\,x_2\n$$\nThe output function is $h(x) = x_1$.\n\nObservability is assessed using the observability matrix $\\mathcal{O}(x)$, which is constructed from the gradients of successive Lie derivatives of the output function $h(x)$. The rows of the matrix are the transposed gradients.\n$$\n\\mathcal{O}(x) = \\begin{pmatrix} (\\nabla h(x))^T \\\\ (\\nabla (L_f h(x)))^T \\\\ (\\nabla (L_f^2 h(x)))^T \\end{pmatrix}\n$$\nWe are asked to find the rank of this matrix at the point $x^\\star = (K_2, 0)$.\n\nFirst, we compute the required Lie derivatives and their gradients in symbolic form.\n\n**Step 1: Zeroth-order term, $h(x)$**\n\nThe output function is given as $h(x) = x_1$.\nThe gradient of $h(x)$ is a constant vector:\n$$\n\\nabla h(x) = \\begin{pmatrix} \\frac{\\partial h}{\\partial x_1} \\\\ \\frac{\\partial h}{\\partial x_2} \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}\n$$\nThe first row of the observability matrix is $(\\nabla h(x))^T = \\begin{pmatrix} 1 & 0 \\end{pmatrix}$. This row is constant and thus the same at $x^\\star$.\n\n**Step 2: First-order term, $L_f h(x)$**\n\nThe first Lie derivative is defined as $L_f h(x) = \\nabla h(x) \\cdot f(x)$.\n$$\nL_f h(x) = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\cdot \\begin{pmatrix} f_1(x) \\\\ f_2(x) \\end{pmatrix} = f_1(x) = \\alpha_1\\,\\frac{x_2^2}{K_1^2 + x_2^2} - \\beta_1\\,x_1\n$$\nNext, we compute the gradient of $L_f h(x)$:\n$$\n\\frac{\\partial (L_f h)}{\\partial x_1} = \\frac{\\partial}{\\partial x_1} \\left( \\alpha_1\\,\\frac{x_2^2}{K_1^2 + x_2^2} - \\beta_1\\,x_1 \\right) = -\\beta_1\n$$\n$$\n\\frac{\\partial (L_f h)}{\\partial x_2} = \\frac{\\partial}{\\partial x_2} \\left( \\alpha_1\\,\\frac{x_2^2}{K_1^2 + x_2^2} - \\beta_1\\,x_1 \\right) = \\alpha_1 \\frac{(2x_2)(K_1^2 + x_2^2) - x_2^2(2x_2)}{(K_1^2 + x_2^2)^2} = \\frac{2 \\alpha_1 K_1^2 x_2}{(K_1^2 + x_2^2)^2}\n$$\nSo, the gradient is $\\nabla(L_f h(x)) = \\begin{pmatrix} -\\beta_1 \\\\ \\frac{2 \\alpha_1 K_1^2 x_2}{(K_1^2 + x_2^2)^2} \\end{pmatrix}$.\nThe second row of the observability matrix is $(\\nabla (L_f h(x)))^T = \\begin{pmatrix} -\\beta_1 & \\frac{2 \\alpha_1 K_1^2 x_2}{(K_1^2 + x_2^2)^2} \\end{pmatrix}$.\n\n**Step 3: Second-order term, $L_f^2 h(x)$**\n\nThe second Lie derivative is $L_f^2 h(x) = \\nabla(L_f h(x)) \\cdot f(x)$.\n$$\nL_f^2 h(x) = \\begin{pmatrix} -\\beta_1 \\\\ \\frac{2 \\alpha_1 K_1^2 x_2}{(K_1^2 + x_2^2)^2} \\end{pmatrix} \\cdot \\begin{pmatrix} f_1(x) \\\\ f_2(x) \\end{pmatrix} = -\\beta_1 f_1(x) + \\frac{2 \\alpha_1 K_1^2 x_2}{(K_1^2 + x_2^2)^2} f_2(x)\n$$\nTo find the third row of the observability matrix, we need to compute the gradient $\\nabla(L_f^2 h(x))$. Rather than computing the full symbolic expression, it is more efficient to evaluate the gradient at the specified point $x^\\star = (K_2, 0)$.\nThe third row is $(\\nabla (L_f^2 h(x^\\star)))^T = \\begin{pmatrix} \\frac{\\partial (L_f^2 h)}{\\partial x_1}\\Big|_{x^\\star} & \\frac{\\partial (L_f^2 h)}{\\partial x_2}\\Big|_{x^\\star} \\end{pmatrix}$.\n\nLet us denote $C(x_2) = \\frac{2 \\alpha_1 K_1^2 x_2}{(K_1^2 + x_2^2)^2}$. So $L_f^2 h(x) = -\\beta_1 f_1(x) + C(x_2) f_2(x)$.\nThen $\\nabla(L_f^2 h(x)) = -\\beta_1 \\nabla f_1(x) + \\nabla(C(x_2) f_2(x))$.\nUsing the product rule for gradients, $\\nabla(C f_2) = C \\nabla f_2 + f_2 \\nabla C$.\nSo, $\\nabla(L_f^2 h(x)) = -\\beta_1 \\nabla f_1(x) + C(x_2) \\nabla f_2(x) + f_2(x) \\nabla C(x_2)$.\n\nWe evaluate each term at $x^\\star = (K_2, 0)$:\n1. At $x_2=0$, $C(0) = 0$. Thus the term $C(x_2) \\nabla f_2(x)$ vanishes at $x^\\star$.\n2. $\\nabla f_1(x)$ is $\\nabla(L_f h(x))$. At $x^\\star$, $\\nabla f_1(x^\\star) = \\begin{pmatrix} -\\beta_1 \\\\ 0 \\end{pmatrix}$. So, $-\\beta_1 \\nabla f_1(x^\\star) = \\begin{pmatrix} \\beta_1^2 \\\\ 0 \\end{pmatrix}$.\n3. We need $f_2(x^\\star)$ and $\\nabla C(x_2)|_{x^\\star}$.\n$$\nf_2(x^\\star) = \\alpha_2\\,\\frac{K_2^m}{K_2^m + K_2^m} - \\beta_2(0) = \\frac{\\alpha_2}{2}\n$$\nThe gradient of $C(x_2)$ is $\\nabla C(x_2) = \\begin{pmatrix} 0 \\\\ C'(x_2) \\end{pmatrix}$.\n$$\nC'(x_2) = \\frac{d}{d x_2} \\left( \\frac{2 \\alpha_1 K_1^2 x_2}{(K_1^2 + x_2^2)^2} \\right) = 2 \\alpha_1 K_1^2 \\frac{(K_1^2 + x_2^2)^2 - x_2 \\cdot 2(K_1^2+x_2^2)(2x_2)}{(K_1^2 + x_2^2)^4} = 2 \\alpha_1 K_1^2 \\frac{K_1^2 - 3x_2^2}{(K_1^2 + x_2^2)^3}\n$$\nEvaluating at $x_2=0$:\n$$\nC'(0) = 2 \\alpha_1 K_1^2 \\frac{K_1^2}{(K_1^2)^3} = \\frac{2 \\alpha_1}{K_1^2}\n$$\nSo, $\\nabla C(x_2)|_{x^\\star} = \\begin{pmatrix} 0 \\\\ \\frac{2 \\alpha_1}{K_1^2} \\end{pmatrix}$.\n4. The last term becomes $f_2(x^\\star) \\nabla C(x_2)|_{x^\\star} = \\frac{\\alpha_2}{2} \\begin{pmatrix} 0 \\\\ \\frac{2 \\alpha_1}{K_1^2} \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ \\frac{\\alpha_1 \\alpha_2}{K_1^2} \\end{pmatrix}$.\n\nCombining the parts, the gradient of the second Lie derivative at $x^\\star$ is:\n$$\n\\nabla(L_f^2 h(x^\\star)) = \\begin{pmatrix} \\beta_1^2 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ \\frac{\\alpha_1 \\alpha_2}{K_1^2} \\end{pmatrix} = \\begin{pmatrix} \\beta_1^2 \\\\ \\frac{\\alpha_1 \\alpha_2}{K_1^2} \\end{pmatrix}\n$$\nThe third row of the observability matrix at $x^\\star$ is $\\begin{pmatrix} \\beta_1^2 & \\frac{\\alpha_1 \\alpha_2}{K_1^2} \\end{pmatrix}$.\n\n**Step 4: Assemble and evaluate the rank of $\\mathcal{O}(x^\\star)$**\n\nWe assemble the observability matrix by evaluating each row at $x^\\star=(K_2, 0)$:\n- Row 1: $\\begin{pmatrix} 1 & 0 \\end{pmatrix}$\n- Row 2: $\\begin{pmatrix} -\\beta_1 & \\frac{2 \\alpha_1 K_1^2 x_2}{(K_1^2 + x_2^2)^2} \\end{pmatrix}\\Big|_{x_2=0} = \\begin{pmatrix} -\\beta_1 & 0 \\end{pmatrix}$\n- Row 3: $\\begin{pmatrix} \\beta_1^2 & \\frac{\\alpha_1 \\alpha_2}{K_1^2} \\end{pmatrix}$\n\nThe matrix is:\n$$\n\\mathcal{O}(x^\\star) = \\begin{pmatrix} 1 & 0 \\\\ -\\beta_1 & 0 \\\\ \\beta_1^2 & \\frac{\\alpha_1 \\alpha_2}{K_1^2} \\end{pmatrix}\n$$\nTo find the rank of this $3 \\times 2$ matrix, we check the linear independence of its rows.\nThe second row is $-\\beta_1$ times the first row, so they are linearly dependent. The rank of the matrix is determined by the span of the first and third rows.\nThe first row is $R_1 = \\begin{pmatrix} 1 & 0 \\end{pmatrix}$.\nThe third row is $R_3 = \\begin{pmatrix} \\beta_1^2 & \\frac{\\alpha_1 \\alpha_2}{K_1^2} \\end{pmatrix}$.\n\nThese two rows are linearly independent if and only if the determinant of the matrix they form is non-zero:\n$$\n\\det \\begin{pmatrix} 1 & 0 \\\\ \\beta_1^2 & \\frac{\\alpha_1 \\alpha_2}{K_1^2} \\end{pmatrix} = (1) \\left( \\frac{\\alpha_1 \\alpha_2}{K_1^2} \\right) - (0) (\\beta_1^2) = \\frac{\\alpha_1 \\alpha_2}{K_1^2}\n$$\nAccording to the problem statement, the parameters $\\alpha_1$, $\\alpha_2$, and $K_1$ are all strictly positive. Therefore, their product and ratio $\\frac{\\alpha_1 \\alpha_2}{K_1^2}$ is strictly positive and non-zero.\nSince the determinant is non-zero, the vectors $\\begin{pmatrix} 1 & 0 \\end{pmatrix}$ and $\\begin{pmatrix} \\beta_1^2 & \\frac{\\alpha_1 \\alpha_2}{K_1^2} \\end{pmatrix}$ are linearly independent.\nThe row space of $\\mathcal{O}(x^\\star)$ is spanned by these two vectors, so its dimension is $2$.\nThe rank of the matrix $\\mathcal{O}(x^\\star)$ is therefore $2$. This implies that the system is locally observable at the state $x^\\star$, even though $x_2$ is unmeasured and its direct impact on the measurement derivative $\\dot{x}_1$ vanishes at $x_2=0$.", "answer": "$$\n\\boxed{2}\n$$", "id": "3334957"}, {"introduction": "While symbolic calculations are essential for theoretical understanding, they can become intractable for more complex, real-world systems. This practice introduces a powerful computational alternative: the empirical observability Gramian. You will write code to simulate a Michaelis-Menten system and quantify its observability by analyzing the output's sensitivity to small perturbations in the initial state [@problem_id:3334890]. By computing the eigenvalues and eigenvectors of this Gramian, you will gain a geometric intuition for observability, identifying the specific combinations of states that are most and least \"visible\" from the output measurements.", "problem": "You are given a nonlinear two-state biochemical reaction system representing the Michaelis–Menten mechanism under the quasi-constant enzyme assumption, expressed as an ordinary differential equation (ODE) in state variables $x(t) = [s(t), c(t)]^\\top$, where $s(t)$ denotes the substrate concentration and $c(t)$ denotes the enzyme–substrate complex concentration. The total enzyme concentration $E_{\\mathrm{tot}}$ is constant, the free enzyme concentration is $e(t) = E_{\\mathrm{tot}} - c(t)$, and the dynamics follow mass-action kinetics:\n$$\n\\frac{ds}{dt} = -k_1\\, e(t)\\, s(t) + k_{-1}\\, c(t), \\quad\n\\frac{dc}{dt} = k_1\\, e(t)\\, s(t) - \\left(k_{-1} + k_{\\mathrm{cat}}\\right) c(t).\n$$\nThe measured output is scalar and defined as $y(t) = s(t)$.\n\nObjective: Compute an empirical observability Gramian for this nonlinear system based on output sensitivity to small perturbations in the initial condition, and interpret its eigenvectors as the most and least observable directions in the state space.\n\nFundamental base to use:\n- Mass-action kinetics and the above ODE model.\n- For a nonlinear system with output $y(t)$ and initial condition $x(0) = x_0$, define the output sensitivity with respect to the initial condition as the vector \n$$\ng(t) = \\frac{\\partial y(t)}{\\partial x_0} \\in \\mathbb{R}^{2}.\n$$\n- Define the empirical observability Gramian as\n$$\nW_o = \\int_{0}^{T} g(t)\\, g(t)^\\top\\, dt \\in \\mathbb{R}^{2 \\times 2}.\n$$\n\nNumerical approximation requirements:\n- Approximate $g(t)$ using a central finite difference with magnitude $\\delta$ along the canonical basis directions $e_1 = [1,0]^\\top$ and $e_2 = [0,1]^\\top$:\n$$\ng_i(t) \\approx \\frac{y\\!\\left(t; x_0 + \\delta e_i\\right) - y\\!\\left(t; x_0 - \\delta e_i\\right)}{2 \\delta}, \\quad i \\in \\{1,2\\}.\n$$\n- Numerically integrate the ODE for each perturbed initial condition on a uniform time grid with $N$ points over $[0, T]$ using a stable ODE solver. Use a composite trapezoidal rule to approximate the time integral:\n$$\nW_o \\approx \\sum_{k=0}^{N-2} \\frac{\\Delta t_k}{2}\\left(g(t_k) g(t_k)^\\top + g(t_{k+1}) g(t_{k+1})^\\top\\right),\n$$\nwhere $\\Delta t_k = t_{k+1} - t_k$, $t_0 = 0$, and $t_{N-1} = T$.\n- Use $N = 2001$ uniformly spaced time points on $[0, T]$.\n\nEigen-decomposition and interpretation:\n- Compute the eigenvalues and eigenvectors of $W_o$. Let $\\lambda_{\\min}$ and $\\lambda_{\\max}$ denote the smallest and largest eigenvalues, with corresponding unit-norm eigenvectors $v_{\\min}$ and $v_{\\max}$.\n- Interpret $v_{\\max}$ as the most observable direction and $v_{\\min}$ as the least observable direction in the state space, respectively.\n- To fix the sign indeterminacy of eigenvectors, enforce the convention: for each eigenvector $v = [v_1, v_2]^\\top$, if the first nonzero component is negative, multiply $v$ by $-1$.\n\nNumerical tolerances:\n- Use absolute tolerance $\\mathrm{atol} = 10^{-12}$ and relative tolerance $\\mathrm{rtol} = 10^{-9}$ for the ODE solver.\n\nTest suite:\nFor each test case, the parameters are $(k_1, k_{-1}, k_{\\mathrm{cat}}, E_{\\mathrm{tot}}, s_0, c_0, T, \\delta)$ and the initial condition is $x_0 = [s_0, c_0]^\\top$. Use the following four cases:\n- Case $1$: $(k_1, k_{-1}, k_{\\mathrm{cat}}, E_{\\mathrm{tot}}, s_0, c_0, T, \\delta) = (1.0, 0.5, 0.2, 1.0, 2.0, 0.05, 10.0, 10^{-4})$.\n- Case $2$: $(k_1, k_{-1}, k_{\\mathrm{cat}}, E_{\\mathrm{tot}}, s_0, c_0, T, \\delta) = (3.0, 1.0, 1.5, 0.2, 5.0, 0.05, 5.0, 10^{-5})$.\n- Case $3$: $(k_1, k_{-1}, k_{\\mathrm{cat}}, E_{\\mathrm{tot}}, s_0, c_0, T, \\delta) = (1.5, 0.1, 2.0, 0.05, 1.0, 0.01, 8.0, 10^{-5})$.\n- Case $4$: $(k_1, k_{-1}, k_{\\mathrm{cat}}, E_{\\mathrm{tot}}, s_0, c_0, T, \\delta) = (0.05, 0.02, 0.01, 1.0, 0.2, 0.01, 20.0, 10^{-5})$.\n\nRequired outputs:\n- For each test case, compute and return the list\n$$\n\\left[\\mathrm{round}\\left(\\lambda_{\\max}, 6\\right), \\ \\mathrm{round}\\left(\\lambda_{\\min}, 6\\right), \\ \\mathrm{round}\\left(v_{\\max,1}, 6\\right), \\ \\mathrm{round}\\left(v_{\\max,2}, 6\\right), \\ \\mathrm{round}\\left(v_{\\min,1}, 6\\right), \\ \\mathrm{round}\\left(v_{\\min,2}, 6\\right)\\right],\n$$\nwhere $v_{\\max,i}$ and $v_{\\min,i}$ denote the $i$-th component of $v_{\\max}$ and $v_{\\min}$, respectively. All rounding must be to $6$ decimal places.\n- The final program output must be a single line containing a list of the four case results, each formatted as a list as above, concatenated into a single list-of-lists, with commas as separators and no spaces.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[[\\cdot],[\\cdot],[\\cdot],[\\cdot]]$). No additional text must be printed.", "solution": "The problem requires the computation and interpretation of the empirical observability Gramian for a nonlinear biochemical system described by the Michaelis-Menten kinetics. The analysis will proceed by validating the problem statement, followed by a detailed exposition of the numerical method to obtain the solution.\n\n### Step 1: Extract Givens\nThe problem provides the following information:\n- **State Vector**: $x(t) = [s(t), c(t)]^\\top$, where $s(t)$ is the substrate concentration and $c(t)$ is the enzyme-substrate complex concentration.\n- **System Dynamics (ODEs)**:\n$$\n\\frac{ds}{dt} = -k_1\\, e(t)\\, s(t) + k_{-1}\\, c(t) \\\\\n\\frac{dc}{dt} = k_1\\, e(t)\\, s(t) - \\left(k_{-1} + k_{\\mathrm{cat}}\\right) c(t)\n$$\n- **Auxiliary Relation**: The free enzyme concentration is $e(t) = E_{\\mathrm{tot}} - c(t)$, where $E_{\\mathrm{tot}}$ is the constant total enzyme concentration.\n- **Measured Output**: $y(t) = s(t)$.\n- **Output Sensitivity**: The sensitivity of the output to the initial condition $x_0$ is defined as $g(t) = \\frac{\\partial y(t)}{\\partial x_0} \\in \\mathbb{R}^{2}$.\n- **Empirical Observability Gramian**: The Gramian is defined by the integral $W_o = \\int_{0}^{T} g(t)\\, g(t)^\\top\\, dt \\in \\mathbb{R}^{2 \\times 2}$.\n- **Numerical Approximation of Sensitivity**: The components of the sensitivity vector, $g_i(t)$, are to be approximated using a central finite difference:\n$$\ng_i(t) \\approx \\frac{y\\!\\left(t; x_0 + \\delta e_i\\right) - y\\!\\left(t; x_0 - \\delta e_i\\right)}{2 \\delta}, \\quad i \\in \\{1,2\\}\n$$\nwhere $e_1 = [1,0]^\\top$ and $e_2 = [0,1]^\\top$.\n- **Numerical Integration of Gramian**: The integral for $W_o$ is to be approximated using the composite trapezoidal rule over a uniform time grid:\n$$\nW_o \\approx \\sum_{k=0}^{N-2} \\frac{\\Delta t_k}{2}\\left(g(t_k) g(t_k)^\\top + g(t_{k+1}) g(t_{k+1})^\\top\\right)\n$$\n- **Numerical Parameters**:\n  - Time grid: $N = 2001$ uniformly spaced points on $[0, T]$.\n  - ODE solver tolerances: absolute tolerance $\\mathrm{atol} = 10^{-12}$, relative tolerance $\\mathrm{rtol} = 10^{-9}$.\n- **Eigen-decomposition**: The eigenvalues ($\\lambda_{\\min}, \\lambda_{\\max}$) and eigenvectors ($v_{\\min}, v_{\\max}$) of $W_o$ are to be computed. $v_{\\max}$ corresponds to the most observable direction, and $v_{\\min}$ to the least observable direction.\n- **Eigenvector Sign Convention**: For each eigenvector, the first non-zero component must be positive.\n- **Test Cases**:\n  - Case $1$: $(k_1, k_{-1}, k_{\\mathrm{cat}}, E_{\\mathrm{tot}}, s_0, c_0, T, \\delta) = (1.0, 0.5, 0.2, 1.0, 2.0, 0.05, 10.0, 10^{-4})$\n  - Case $2$: $(k_1, k_{-1}, k_{\\mathrm{cat}}, E_{\\mathrm{tot}}, s_0, c_0, T, \\delta) = (3.0, 1.0, 1.5, 0.2, 5.0, 0.05, 5.0, 10^{-5})$\n  - Case $3$: $(k_1, k_{-1}, k_{\\mathrm{cat}}, E_{\\mathrm{tot}}, s_0, c_0, T, \\delta) = (1.5, 0.1, 2.0, 0.05, 1.0, 0.01, 8.0, 10^{-5})$\n  - Case $4$: $(k_1, k_{-1}, k_{\\mathrm{cat}}, E_{\\mathrm{tot}}, s_0, c_0, T, \\delta) = (0.05, 0.02, 0.01, 1.0, 0.2, 0.01, 20.0, 10^{-5})$\n- **Output Format**: For each case, a list of six rounded values: $[\\mathrm{round}(\\lambda_{\\max}, 6), \\mathrm{round}(\\lambda_{\\min}, 6), \\mathrm{round}(v_{\\max,1}, 6), \\mathrm{round}(v_{\\max,2}, 6), \\mathrm{round}(v_{\\min,1}, 6), \\mathrm{round}(v_{\\min,2}, 6)]$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is evaluated against the established criteria:\n- **Scientifically Grounded**: The problem is based on the Michaelis-Menten reaction mechanism, a fundamental concept in biochemistry, and employs standard mass-action kinetics. The concept of the observability Gramian is a cornerstone of control theory and its application to systems biology is well-established. The proposed numerical methods are standard and appropriate.\n- **Well-Posed**: The problem is fully specified. It provides the governing ODEs, the definition of the quantity to be computed ($W_o$), the numerical methods to be used, all necessary parameters and initial conditions for four distinct test cases, and a precise output format. A unique numerical result can be determined.\n- **Objective**: The problem is stated in precise, mathematical language, free from any subjectivity or ambiguity.\n- **Flaw Checklist**: The problem does not violate any of the invalidity criteria. It is scientifically sound, formalizable, complete, realistic, well-posed, and non-trivial. The mention of the \"quasi-constant enzyme assumption\" is contextual flavor and does not create a contradiction, as the explicit full ODEs to be used are provided.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A complete solution will be developed.\n\n### Principle-Based Solution Design\nThe core of this problem is to assess system observability, which is the possibility of determining the internal state of a system by observing its outputs. For nonlinear systems, this is often analyzed locally. The empirical observability Gramian, $W_o$, provides a quantitative measure of this property. A large eigenvalue of $W_o$ indicates that perturbations in the direction of the corresponding eigenvector are highly visible in the output, whereas a small eigenvalue indicates poor observability in that direction.\n\nThe algorithm to solve this problem is as follows:\n\n1.  **Define the System Dynamics**: The provided ODE system must be formulated as a function suitable for a numerical solver. The state is $x = [s, c]^\\top$. The ODE function, let's call it `ode_func`, will take time $t$, state $x$, and the parameters $(k_1, k_{-1}, k_{\\mathrm{cat}}, E_{\\mathrm{tot}})$ as input and return the state derivative $dx/dt = [\\dot{s}, \\dot{c}]^\\top$.\n\n2.  **Generate Perturbed Trajectories**: To calculate the output sensitivities $g(t) = \\partial y(t) / \\partial x_0$ via finite differences, we must simulate the system's evolution from perturbed initial conditions. For each component $i$ of the initial state $x_0 = [s_0, c_0]^\\top$, we define two perturbed initial conditions:\n    - $x_{0,i}^+ = x_0 + \\delta e_i$\n    - $x_{0,i}^- = x_0 - \\delta e_i$\n    where $e_1 = [1, 0]^\\top$ and $e_2 = [0, 1]^\\top$. This results in four initial conditions to simulate:\n    - For $i=1$: $[s_0 + \\delta, c_0]^\\top$ and $[s_0 - \\delta, c_0]^\\top$.\n    - For $i=2$: $[s_0, c_0 + \\delta]^\\top$ and $[s_0, c_0 - \\delta]^\\top$.\n    Each of these initial conditions is integrated from $t=0$ to $t=T$ using a high-precision ODE solver (`scipy.integrate.solve_ivp`) with the specified tolerances ($\\mathrm{atol}=10^{-12}, \\mathrm{rtol}=10^{-9}$) over a uniform time grid of $N=2001$ points.\n\n3.  **Calculate Sensitivity Vectors**: From the four simulated trajectories, we extract the output $y(t) = s(t)$. Let $y(t; x_{0,i}^{\\pm})$ denote the output trajectory corresponding to the initial condition $x_{0,i}^{\\pm}$. The two components of the sensitivity vector $g(t) = [g_1(t), g_2(t)]^\\top$ are then computed for each time point in the grid:\n    $$\n    g_1(t) = \\frac{y(t; x_{0,1}^+) - y(t; x_{0,1}^-)}{2 \\delta} = \\frac{s(t; [s_0+\\delta, c_0]^\\top) - s(t; [s_0-\\delta, c_0]^\\top)}{2 \\delta}\n    $$\n    $$\n    g_2(t) = \\frac{y(t; x_{0,2}^+) - y(t; x_{0,2}^-)}{2 \\delta} = \\frac{s(t; [s_0, c_0+\\delta]^\\top) - s(t; [s_0, c_0-\\delta]^\\top)}{2 \\delta}\n    $$\n    This yields two time-series vectors, one for each sensitivity component.\n\n4.  **Compute the Observability Gramian**: The Gramian $W_o$ is the time integral of the outer product $g(t)g(t)^\\top$.\n    $$\n    W_o = \\int_{0}^{T} \\begin{pmatrix} g_1(t)^2 & g_1(t) g_2(t) \\\\ g_1(t) g_2(t) & g_2(t)^2 \\end{pmatrix} dt = \\begin{pmatrix} \\int_{0}^{T} g_1(t)^2 dt & \\int_{0}^{T} g_1(t) g_2(t) dt \\\\ \\int_{0}^{T} g_1(t) g_2(t) dt & \\int_{0}^{T} g_2(t)^2 dt \\end{pmatrix}\n    $$\n    Each integral is computed numerically using the composite trapezoidal rule, as specified. This is efficiently accomplished using a numerical library function (e.g., `numpy.trapz`) on the time-series arrays for $g_1(t)^2$, $g_2(t)^2$, and $g_1(t)g_2(t)$.\n\n5.  **Eigen-decomposition and Interpretation**: Once the $2 \\times 2$ symmetric matrix $W_o$ is assembled, its eigenvalues and eigenvectors are computed using a standard linear algebra routine (`numpy.linalg.eigh`). Because $W_o$ is symmetric and at least positive semi-definite, its eigenvalues will be real and non-negative. `eigh` conveniently returns eigenvalues in ascending order and corresponding eigenvectors.\n    - $\\lambda_{\\min}$ is the first eigenvalue; its eigenvector $v_{\\min}$ is the least observable direction.\n    - $\\lambda_{\\max}$ is the second eigenvalue; its eigenvector $v_{\\max}$ is the most observable direction.\n\n6.  **Finalize and Format**: The computed eigenvectors must be adjusted to satisfy the sign convention: the first non-zero component must be positive. This is done by checking the sign and, if necessary, multiplying the entire eigenvector by $-1$. Finally, the required six values ($\\lambda_{\\max}, \\lambda_{\\min}, v_{\\max,1}, v_{\\max,2}, v_{\\min,1}, v_{\\min,2}$) are rounded to $6$ decimal places.\n\nThis entire procedure is repeated for each of the four test cases provided.", "answer": "```python\nimport numpy as np\nfrom scipy.integrate import solve_ivp\n\ndef solve():\n    \"\"\"\n    Computes the empirical observability Gramian and its eigensystem for a \n    nonlinear biochemical reaction system for four given test cases.\n    \"\"\"\n\n    test_cases = [\n        # (k1, k_m1, k_cat, E_tot, s0, c0, T, delta)\n        (1.0, 0.5, 0.2, 1.0, 2.0, 0.05, 10.0, 1e-4),\n        (3.0, 1.0, 1.5, 0.2, 5.0, 0.05, 5.0, 1e-5),\n        (1.5, 0.1, 2.0, 0.05, 1.0, 0.01, 8.0, 1e-5),\n        (0.05, 0.02, 0.01, 1.0, 0.2, 0.01, 20.0, 1e-5),\n    ]\n\n    results = []\n\n    # ODE solver tolerances\n    atol = 1e-12\n    rtol = 1e-9\n    \n    # Number of time points\n    N = 2001\n\n    def ode_func(t, x, k1, k_m1, k_cat, E_tot):\n        \"\"\"\n        Defines the ODE system for the Michaelis-Menten kinetics.\n        x = [s, c]\n        \"\"\"\n        s, c = x\n        e = E_tot - c\n        ds_dt = -k1 * e * s + k_m1 * c\n        dc_dt = k1 * e * s - (k_m1 + k_cat) * c\n        return [ds_dt, dc_dt]\n\n    def fix_eigenvector_sign(v):\n        \"\"\"\n        Enforces a sign convention on an eigenvector: the first non-zero\n        component must be positive.\n        \"\"\"\n        for i in range(len(v)):\n            if not np.isclose(v[i], 0.0):\n                if v[i] < 0:\n                    return -v\n                break\n        return v\n\n    for case in test_cases:\n        k1, k_m1, k_cat, E_tot, s0, c0, T, delta = case\n        \n        x0 = np.array([s0, c0])\n        t_span = (0, T)\n        t_eval = np.linspace(0, T, N)\n        \n        args = (k1, k_m1, k_cat, E_tot)\n\n        # Perturbations along a canonical basis direction e1 = [1, 0]\n        x0_s_p = x0 + np.array([delta, 0])\n        x0_s_m = x0 - np.array([delta, 0])\n        \n        sol_s_p = solve_ivp(ode_func, t_span, x0_s_p, args=args, t_eval=t_eval, rtol=rtol, atol=atol, method='RK45')\n        sol_s_m = solve_ivp(ode_func, t_span, x0_s_m, args=args, t_eval=t_eval, rtol=rtol, atol=atol, method='RK45')\n        \n        y_s_p = sol_s_p.y[0]\n        y_s_m = sol_s_m.y[0]\n\n        # Perturbations along a canonical basis direction e2 = [0, 1]\n        x0_c_p = x0 + np.array([0, delta])\n        x0_c_m = x0 - np.array([0, delta])\n\n        sol_c_p = solve_ivp(ode_func, t_span, x0_c_p, args=args, t_eval=t_eval, rtol=rtol, atol=atol, method='RK45')\n        sol_c_m = solve_ivp(ode_func, t_span, x0_c_m, args=args, t_eval=t_eval, rtol=rtol, atol=atol, method='RK45')\n        \n        y_c_p = sol_c_p.y[0]\n        y_c_m = sol_c_m.y[0]\n        \n        # Calculate sensitivity vectors g1(t) and g2(t) using central finite differences\n        g1_t = (y_s_p - y_s_m) / (2 * delta)\n        g2_t = (y_c_p - y_c_m) / (2 * delta)\n        \n        # Define the integrands for the Gramian matrix elements\n        integrand_11 = g1_t**2\n        integrand_12 = g1_t * g2_t\n        integrand_22 = g2_t**2\n        \n        # Integrate using the composite trapezoidal rule\n        W_o_11 = np.trapz(integrand_11, t_eval)\n        W_o_12 = np.trapz(integrand_12, t_eval)\n        W_o_22 = np.trapz(integrand_22, t_eval)\n        \n        # Assemble the observability Gramian\n        W_o = np.array([[W_o_11, W_o_12], [W_o_12, W_o_22]])\n        \n        # Compute eigenvalues and eigenvectors\n        # np.linalg.eigh returns eigenvalues in ascending order for symmetric matrices\n        eigvals, eigvecs = np.linalg.eigh(W_o)\n        \n        lambda_min, lambda_max = eigvals[0], eigvals[1]\n        v_min, v_max = eigvecs[:, 0], eigvecs[:, 1]\n        \n        # Enforce sign convention\n        v_min = fix_eigenvector_sign(v_min)\n        v_max = fix_eigenvector_sign(v_max)\n        \n        # Format results for the current case\n        case_result = [\n            round(lambda_max, 6),\n            round(lambda_min, 6),\n            round(v_max[0], 6),\n            round(v_max[1], 6),\n            round(v_min[0], 6),\n            round(v_min[1], 6),\n        ]\n        results.append(case_result)\n\n    # Print the final output in the required format\n    print(str(results).replace(\" \", \"\"))\n\nsolve()\n\n```", "id": "3334890"}, {"introduction": "A critical concept in nonlinear systems is the distinction between local and global observability. A system can satisfy the local observability rank condition almost everywhere, yet still have distinct internal states that are impossible to tell apart from the output. This thought experiment explores this subtlety using a classic symmetric toggle-switch model known for its bistability [@problem_id:3334899]. By leveraging the system's inherent symmetry, you will demonstrate why different initial conditions can produce identical output trajectories, proving that the system is not globally observable and highlighting a fundamental challenge in biological state estimation.", "problem": "Consider a symmetric mutual-repression genetic toggle-switch model without external input, a standard motif in computational systems biology, with state vector $x = (x_1, x_2)$ representing the concentrations of two transcription factors that repress each other. The dynamics are given by\n$$\n\\begin{aligned}\n\\dot{x}_1 &= \\frac{\\alpha}{1 + \\left(\\frac{x_2}{K}\\right)^{n}} - \\delta\\, x_1,\\\\\n\\dot{x}_2 &= \\frac{\\alpha}{1 + \\left(\\frac{x_1}{K}\\right)^{n}} - \\delta\\, x_2,\n\\end{aligned}\n$$\nwhere $\\alpha > 0$ is a maximal synthesis rate, $\\delta > 0$ is a first-order degradation/dilution rate, $K > 0$ is a dissociation constant, and $n \\ge 2$ is a Hill coefficient describing cooperativity. Assume parameters are chosen in a regime known to produce bistability (two linearly stable off-diagonal equilibria and one unstable equilibrium on the diagonal), which is a well-documented property of such toggles for sufficiently large $n$ and $\\alpha/\\delta$.\n\nSuppose the sole measured output is the total concentration\n$$\ny = h(x) = x_1 + x_2.\n$$\n\nUsing only foundational definitions from nonlinear systems theory (the local observability rank condition based on Lie derivatives, and the concept of structural observability as a generic property with respect to parameters), do the following:\n\n1. Construct the $2 \\times 2$ local observability matrix at a generic state $x$ by stacking the gradients of $h(x)$ and its first Lie derivative along the vector field $f(x)$, where $f(x)$ denotes the right-hand side of the state equations. Compute the determinant of this matrix explicitly as a function of $x_1$, $x_2$, $\\alpha$, $\\delta$, $K$, and $n$.\n\n2. Using your expression, argue from first principles why the system is structurally observable, that is, why the local observability rank condition holds on a full-measure subset of the state space for an open dense set of parameter values.\n\n3. Exploit the exchange symmetry $x_1 \\leftrightarrow x_2$ and the assumed bistability to justify that the input–output map $x_0 \\mapsto y(t \\mid x_0)$ is not globally injective, because there exist distinct initial conditions that generate identical output trajectories for all $t \\ge 0$, and, in particular, distinct stable steady states with identical steady-state output. Define $m$ as the generic cardinality of the set of indistinguishable initial conditions $x_0$ and $x_0'$ that yield identical output trajectories $y(t)$ for all $t \\ge 0$ under the given output $h(x) = x_1 + x_2$.\n\nReport the value of $m$ as a single number with no units. No rounding is required.", "solution": "The problem requires an analysis of the observability properties of a symmetric genetic toggle-switch model. The analysis hinges on the local observability rank condition, derived from Lie derivatives, and the system's inherent exchange symmetry.\n\nLet the state of the system be $x = (x_1, x_2)^T \\in \\mathbb{R}^2$, where $x_1 \\ge 0$ and $x_2 \\ge 0$. The dynamics are governed by the vector field $f(x) = (f_1(x), f_2(x))^T$, where\n$$\n\\begin{aligned}\nf_1(x) &= \\frac{\\alpha}{1 + \\left(\\frac{x_2}{K}\\right)^{n}} - \\delta\\, x_1 \\\\\nf_2(x) &= \\frac{\\alpha}{1 + \\left(\\frac{x_1}{K}\\right)^{n}} - \\delta\\, x_2\n\\end{aligned}\n$$\nThe output is given by the function $y = h(x) = x_1 + x_2$.\n\nThe local observability of a nonlinear system $\\dot{x} = f(x)$, $y = h(x)$ at a point $x$ is assessed using the observability matrix, $\\mathcal{O}(x)$, which is constructed from the gradients of successive Lie derivatives of the output function $h(x)$. For a two-dimensional system, we typically need to check the rank of the matrix formed by the gradients of $h(x)$ and its first Lie derivative, $L_f h(x)$.\n\n**1. Construction of the Observability Matrix and its Determinant**\n\nThe $2 \\times 2$ local observability matrix is given by:\n$$\n\\mathcal{O}(x) = \\begin{pmatrix} \\nabla h(x) \\\\ \\nabla(L_f h(x)) \\end{pmatrix}\n$$\nFirst, we compute the gradient of the output function $h(x) = x_1 + x_2$:\n$$\n\\nabla h(x) = \\left[ \\frac{\\partial h}{\\partial x_1} \\quad \\frac{\\partial h}{\\partial x_2} \\right] = \\begin{pmatrix} 1 & 1 \\end{pmatrix}\n$$\nNext, we compute the first Lie derivative of $h(x)$ along the vector field $f(x)$:\n$$\nL_f h(x) = \\nabla h(x) \\cdot f(x) = 1 \\cdot f_1(x) + 1 \\cdot f_2(x)\n$$\n$$\nL_f h(x) = \\left( \\frac{\\alpha}{1 + \\left(\\frac{x_2}{K}\\right)^{n}} - \\delta\\, x_1 \\right) + \\left( \\frac{\\alpha}{1 + \\left(\\frac{x_1}{K}\\right)^{n}} - \\delta\\, x_2 \\right) = \\frac{\\alpha}{1 + (x_2/K)^n} + \\frac{\\alpha}{1 + (x_1/K)^n} - \\delta(x_1 + x_2)\n$$\nNow, we compute the gradient of $L_f h(x)$. We find its partial derivatives with respect to $x_1$ and $x_2$:\n$$\n\\frac{\\partial}{\\partial x_1} (L_f h(x)) = \\frac{\\partial}{\\partial x_1} \\left( \\frac{\\alpha}{1 + (x_1/K)^n} - \\delta x_1 \\right) = \\alpha \\frac{-\\frac{n}{K} \\left(\\frac{x_1}{K}\\right)^{n-1}}{\\left(1 + (x_1/K)^n\\right)^2} - \\delta = -\\frac{\\alpha n K^n x_1^{n-1}}{(K^n + x_1^n)^2} - \\delta\n$$\n$$\n\\frac{\\partial}{\\partial x_2} (L_f h(x)) = \\frac{\\partial}{\\partial x_2} \\left( \\frac{\\alpha}{1 + (x_2/K)^n} - \\delta x_2 \\right) = \\alpha \\frac{-\\frac{n}{K} \\left(\\frac{x_2}{K}\\right)^{n-1}}{\\left(1 + (x_2/K)^n\\right)^2} - \\delta = -\\frac{\\alpha n K^n x_2^{n-1}}{(K^n + x_2^n)^2} - \\delta\n$$\nWe can now assemble the observability matrix $\\mathcal{O}(x)$:\n$$\n\\mathcal{O}(x) = \\begin{pmatrix} 1 & 1 \\\\ -\\frac{\\alpha n K^n x_1^{n-1}}{(K^n + x_1^n)^2} - \\delta & -\\frac{\\alpha n K^n x_2^{n-1}}{(K^n + x_2^n)^2} - \\delta \\end{pmatrix}\n$$\nThe determinant of this matrix is:\n$$\n\\det(\\mathcal{O}(x)) = 1 \\cdot \\left(-\\frac{\\alpha n K^n x_2^{n-1}}{(K^n + x_2^n)^2} - \\delta\\right) - 1 \\cdot \\left(-\\frac{\\alpha n K^n x_1^{n-1}}{(K^n + x_1^n)^2} - \\delta\\right)\n$$\n$$\n\\det(\\mathcal{O}(x)) = -\\frac{\\alpha n K^n x_2^{n-1}}{(K^n + x_2^n)^2} - \\delta + \\frac{\\alpha n K^n x_1^{n-1}}{(K^n + x_1^n)^2} + \\delta\n$$\n$$\n\\det(\\mathcal{O}(x)) = \\alpha n K^n \\left( \\frac{x_1^{n-1}}{(K^n + x_1^n)^2} - \\frac{x_2^{n-1}}{(K^n + x_2^n)^2} \\right)\n$$\n\n**2. Argument for Structural Observability**\n\nA system is locally observable at a state $x$ if the observability matrix $\\mathcal{O}(x)$ has full rank. For this $2$-dimensional system, this means $\\text{rank}(\\mathcal{O}(x)) = 2$, which is equivalent to $\\det(\\mathcal{O}(x)) \\neq 0$. The system is structurally observable if the set of states $x$ where the rank condition fails (i.e., $\\det(\\mathcal{O}(x)) = 0$) is a set of measure zero in the state space for an open dense set of parameters.\n\nThe determinant is zero if and only if:\n$$\n\\frac{x_1^{n-1}}{(K^n + x_1^n)^2} = \\frac{x_2^{n-1}}{(K^n + x_2^n)^2}\n$$\nThis equation is clearly satisfied for all points on the line $x_1 = x_2$. This line is a one-dimensional submanifold in the two-dimensional state space, and thus has Lebesgue measure zero. The function $g(z) = \\frac{z^{n-1}}{(K^n + z^n)^2}$ is not monotonic for $z \\ge 0$, so there may be other curves of solutions where $g(x_1)=g(x_2)$ for $x_1 \\neq x_2$. However, the set of all such points $(x_1, x_2)$ forms a lower-dimensional algebraic variety in $\\mathbb{R}^2$. As the parameters $\\alpha, n, K, \\delta$ are non-zero, the determinant is a non-trivial analytic function of $x_1$ and $x_2$. Its zero-set is therefore a set of measure zero. This fulfills the condition for structural observability. The system is locally observable for almost all states $x$.\n\n**3. Global Non-Injectivity and the Cardinality of Indistinguishable States**\n\nThe system is not globally observable, meaning the map from an initial condition $x_0$ to the entire output trajectory $y(t \\mid x_0)$ is not injective. This is a direct consequence of the system's symmetry.\n\nLet $S$ be the swap operator: $S(x_1, x_2) = (x_2, x_1)$.\nThe vector field $f(x)$ is equivariant with respect to $S$. To see this, let $x' = S(x) = (x_2, x_1)$.\n$$\nf(x') = f(x_2, x_1) = \\begin{pmatrix} \\frac{\\alpha}{1 + (x_1/K)^n} - \\delta x_2 \\\\ \\frac{\\alpha}{1 + (x_2/K)^n} - \\delta x_1 \\end{pmatrix} = \\begin{pmatrix} f_2(x_1, x_2) \\\\ f_1(x_1, x_2) \\end{pmatrix} = S(f(x))\n$$\nThe output function $h(x) = x_1 + x_2$ is invariant under $S$:\n$$\nh(S(x)) = h(x_2, x_1) = x_2 + x_1 = x_1 + x_2 = h(x)\n$$\nNow, consider a solution trajectory $x(t)$ starting from an initial condition $x_0$, such that $\\dot{x}(t) = f(x(t))$ and $x(0)=x_0$. Let us define a new trajectory $z(t) = S(x(t))$. The initial condition for $z(t)$ is $z(0) = S(x(0)) = S(x_0)$. The dynamics of $z(t)$ are given by:\n$$\n\\dot{z}(t) = \\frac{d}{dt} S(x(t)) = S(\\dot{x}(t)) = S(f(x(t)))\n$$\nUsing the equivariance of $f$, we have $S(f(x(t))) = f(S(x(t))) = f(z(t))$. Thus, $\\dot{z}(t) = f(z(t))$. This shows that if $x(t)$ is a valid trajectory starting at $x_0$, then $z(t) = S(x(t))$ is a valid trajectory starting at the distinct initial point $S(x_0)$ (provided $x_0$ is not on the symmetry line $x_1=x_2$).\n\nLet's compare the outputs generated from initial conditions $x_0$ and $S(x_0)$.\nThe output from $x_0$ is $y(t) = h(x(t))$.\nThe output from $S(x_0)$ is $y'(t) = h(z(t)) = h(S(x(t)))$.\nDue to the invariance of $h$, we have $h(S(x(t))) = h(x(t))$.\nTherefore, $y'(t) = y(t)$ for all $t \\ge 0$.\n\nThis proves that any two initial conditions $x_0$ and $S(x_0)$ that are different (i.e., $x_0$ does not lie on the line $x_1=x_2$) are indistinguishable from the output $y(t)$.\n\nThe problem's assumption of bistability provides a concrete example. The two stable steady states, call them $x_A^* = (x_{hi}, x_{lo})$ and $x_B^* = (x_{lo}, x_{hi})$, are related by this symmetry: $x_B^* = S(x_A^*)$. The steady-state output for both is $y^* = x_{hi} + x_{lo}$, making them indistinguishable. An initial condition at $x_A^*$ yields a constant output $y^*$, and an initial condition at $x_B^*$ also yields the constant output $y^*$.\n\nThe question asks for $m$, the generic cardinality of the set of indistinguishable initial conditions. A \"generic\" initial condition $x_0$ is one that does not lie on any special, lower-dimensional manifold. In this case, this means $x_0$ does not lie on the set of unobservable points, and specifically not on the symmetry line $x_1=x_2$. For such a generic $x_0 = (x_{1,0}, x_{2,0})$ with $x_{1,0} \\neq x_{2,0}$, the set of initial conditions that produce the same output trajectory is $\\{x_0, S(x_0)\\} = \\{(x_{1,0}, x_{2,0}), (x_{2,0}, x_{1,0})\\}$.\nThe cardinality of this set is $2$.\nTherefore, $m=2$.", "answer": "$$\n\\boxed{2}\n$$", "id": "3334899"}]}