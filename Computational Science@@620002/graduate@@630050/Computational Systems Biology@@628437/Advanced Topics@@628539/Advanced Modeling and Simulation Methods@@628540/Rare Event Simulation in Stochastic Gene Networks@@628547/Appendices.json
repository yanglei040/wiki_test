{"hands_on_practices": [{"introduction": "Before tackling the full complexity of stochastic simulations, it is invaluable to understand the underlying landscape that governs rare transitions. This first practice explores the concept of the quasipotential, a cornerstone of Freidlin–Wentzell theory, which provides a powerful link between a system's deterministic behavior and its rare stochastic switching. By working through this problem [@problem_id:3343275], you will derive the potential barrier in a continuous approximation of a gene network, giving you a tangible measure of an event's rarity and building a solid theoretical foundation for the path sampling methods to come.", "problem": "Consider a one-gene positive-feedback module often used to model bistable gene expression in computational systems biology. The discrete, well-mixed molecular system is governed by the Chemical Master Equation, which, under a standard Kramers–Moyal expansion and van Kampen system-size expansion, leads in the diffusion limit to a continuous-state stochastic differential equation with additive noise of the form\n$$\ndX_t \\;=\\; f(X_t)\\,dt \\;+\\; \\sqrt{\\varepsilon}\\,dW_t,\n$$\nwhere $X_t$ is the nondimensionalized protein copy-number surrogate, $f(x)$ is the deterministic drift capturing production and degradation, $\\varepsilon$ is a small nondimensional noise intensity, and $W_t$ is a standard Wiener process. We consider the Hill-type drift\n$$\nf(x) \\;=\\; \\beta_0 \\;+\\; \\beta \\,\\dfrac{x^n}{K^n + x^n} \\;-\\; \\delta\\,x,\n$$\nwith basal production rate $\\beta_0$, maximal inducible rate $\\beta$, Hill coefficient $n$, half-saturation constant $K$, and first-order degradation rate $\\delta$. All quantities are nondimensional (no physical units).\n\nIn the small-noise limit $0  \\varepsilon \\ll 1$, Freidlin–Wentzell (FW) theory of large deviations quantifies rare transitions between deterministic attractors. The minimal action (also called the FW quasipotential barrier) from a stable equilibrium $x_a$ to a target point $x$ is defined by the variational problem for the FW action functional\n$$\nS_{[0,T]}[x(\\cdot)] \\;=\\; \\dfrac{1}{2}\\int_0^T \\big\\lVert \\dot{x}(t) - f\\!\\big(x(t)\\big)\\big\\rVert^2\\,dt,\n$$\nand the quasipotential\n$$\nV(x) \\;=\\; \\inf_{T>0}\\ \\inf_{\\substack{x(0)=x_a\\\\ x(T)=x}} S_{[0,T]}[x(\\cdot)].\n$$\nIn one dimension with additive noise, the quasipotential solves a stationary Hamilton–Jacobi equation that can be derived from the action’s Euler–Lagrange or Hamiltonian formulation. Your task is to start from these foundations and, for the one-dimensional setting above, derive the relationship that allows computing $V(x)$ exactly by quadrature in terms of $f(x)$ and then implement it numerically. Do not assume any pre-tabulated result; derive the governing equation and the implied relationship directly from the definitions given here.\n\nComputational tasks to be implemented:\n- Given a parameter set $(\\beta_0,\\beta,K,n,\\delta)$ and a bounded search domain $[0,x_{\\max}]$, locate all equilibria, i.e., real $x^\\star\\in[0,x_{\\max}]$ such that $f(x^\\star)=0$, using robust bracketing-based root finding. Classify each equilibrium by its deterministic stability via the sign of $f'(x^\\star)$: stable if $f'(x^\\star)0$, unstable if $f'(x^\\star)>0$.\n- Determine whether the system is bistable on $[0,x_{\\max}]$ in the sense of having exactly two stable equilibria separated by one unstable equilibrium.\n- If bistable, compute the two FW quasipotential barriers:\n  1. The barrier from the lower stable equilibrium $x_{\\mathrm{low}}$ to the intermediate unstable equilibrium $x_{\\mathrm{sad}}$.\n  2. The barrier from the higher stable equilibrium $x_{\\mathrm{high}}$ to the same $x_{\\mathrm{sad}}$.\n  Use the one-dimensional FW Hamilton–Jacobi relationship you derived to express the quasipotential difference as a single definite integral of $f(x)$ between the corresponding limits, and evaluate it numerically to high accuracy.\n- If the system is not bistable on $[0,x_{\\max}]$, report both barrier values as $0.0$ by convention.\n\nNumerical and logical requirements:\n- Use only nondimensional quantities; no physical units are to be reported.\n- Use a sign-consistent numerical quadrature and ensure the reported barrier heights are nonnegative real numbers.\n- When locating equilibria, use a monotone scan over $[0,x_{\\max}]$ with a sufficiently fine step to bracket all sign changes of $f(x)$, then refine each bracket by a root solver that guarantees convergence on a sign change.\n- Deduplicate numerically coincident roots using a tolerance strictly smaller than the scanning step.\n- The derivative needed for stability classification should be computed from the analytic derivative\n$$\nf'(x) \\;=\\; \\beta \\,\\dfrac{n\\,x^{n-1} K^n}{\\big(K^n + x^n\\big)^2} \\;-\\; \\delta,\n$$\nevaluated at each equilibrium.\n\nTest suite:\nProvide outputs for the following three parameter sets, each given together with a domain bound $x_{\\max}$ for equilibrium search. All parameters are nondimensional numbers.\n\n- Case A (expected clearly bistable):\n  $(\\beta_0,\\beta,K,n,\\delta,x_{\\max}) \\;=\\; (\\,0.2,\\,120.0,\\,50.0,\\,4.0,\\,1.0,\\,200.0\\,)$.\n- Case B (near a saddle–node threshold, marginally bistable):\n  $(\\beta_0,\\beta,K,n,\\delta,x_{\\max}) \\;=\\; (\\,0.2,\\,55.0,\\,50.0,\\,4.0,\\,1.0,\\,200.0\\,)$.\n- Case C (monostable on the domain):\n  $(\\beta_0,\\beta,K,n,\\delta,x_{\\max}) \\;=\\; (\\,0.2,\\,20.0,\\,50.0,\\,4.0,\\,1.0,\\,200.0\\,)$.\n\nFinal output format:\nYour program should produce a single line of output containing a list of length $3$, one entry per case, where each entry is itself a list\n$$\n[\\ \\text{bistable},\\ [x_1,x_2,\\ldots],\\ B_{\\mathrm{low}\\to \\mathrm{sad}},\\ B_{\\mathrm{high}\\to \\mathrm{sad}}\\ ],\n$$\nwith $\\text{bistable}$ a boolean, $[x_1,x_2,\\ldots]$ the sorted list of all equilibria found in $[0,x_{\\max}]$, and $B_{\\mathrm{low}\\to \\mathrm{sad}}$ and $B_{\\mathrm{high}\\to \\mathrm{sad}}$ the two nonnegative barrier heights (both $0.0$ if not bistable). For example, the program must print exactly one line in the form\n$$\n\\big[\\ [\\text{bool},[x\\_1,\\dots],b\\_1,b\\_2],\\ [\\text{bool},[x\\_1,\\dots],b\\_1,b\\_2],\\ [\\text{bool},[x\\_1,\\dots],b\\_1,b\\_2]\\ \\big],\n$$\nwith no additional characters or whitespace constraints beyond Python’s default list formatting.", "solution": "The objective is to analyze the bistability of a one-dimensional stochastic gene expression model and compute the Freidlin–Wentzell (FW) quasipotential barriers between its stable and unstable equilibria. This requires two main steps: first, deriving the one-dimensional formula for the quasipotential from the provided action functional, and second, implementing a numerical algorithm to apply this formula to specific parameter sets.\n\n### Derivation of the Quasipotential Formula\n\nThe problem concerns a system described by the one-dimensional stochastic differential equation (SDE):\n$$\ndX_t \\;=\\; f(X_t)\\,dt \\;+\\; \\sqrt{\\varepsilon}\\,dW_t\n$$\nThe FW quasipotential $V(x)$ measures the minimum \"cost\" (action) for the system to move from a stable equilibrium point $x_a$ to a target point $x$, against the deterministic drift $f(x)$. The action functional is given by:\n$$\nS_{[0,T]}[x(\\cdot)] \\;=\\; \\dfrac{1}{2}\\int_0^T \\big( \\dot{x}(t) - f(x(t))\\big)^2\\,dt\n$$\nThe quasipotential is the infimum of this action over all paths from $x_a$ to $x$ and all positive time horizons $T$. To find the minimizing path, we use the calculus of variations. The integrand, $L(x, \\dot{x}) = \\frac{1}{2}(\\dot{x} - f(x))^2$, is the Lagrangian. A more direct route to the desired result is via the Hamiltonian formulation.\n\nFirst, we define the canonical momentum $p$ conjugate to the path variable $x$:\n$$\np = \\dfrac{\\partial L}{\\partial \\dot{x}} = \\dfrac{\\partial}{\\partial \\dot{x}} \\left[ \\dfrac{1}{2}(\\dot{x} - f(x))^2 \\right] = \\dot{x} - f(x)\n$$\nFrom this, we can express the velocity $\\dot{x}$ as a function of momentum and position:\n$$\n\\dot{x} = p + f(x)\n$$\nThe Hamiltonian $H(x, p)$ is obtained through a Legendre transformation of the Lagrangian:\n$$\nH(x, p) = p\\dot{x} - L(x, \\dot{x}) = p(p + f(x)) - \\dfrac{1}{2}p^2 = p^2 + pf(x) - \\dfrac{1}{2}p^2 = \\dfrac{1}{2}p^2 + pf(x)\n$$\nThe path that minimizes the action functional for a transition from an attractor corresponds to a zero-energy trajectory of this Hamiltonian system, i.e., $H(x, p) = 0$.\n$$\n\\dfrac{1}{2}p^2 + pf(x) = 0 \\implies p \\left( \\dfrac{1}{2}p + f(x) \\right) = 0\n$$\nThis equation has two solutions for the momentum $p$:\n1.  $p = 0$: This implies $\\dot{x} = 0 + f(x) = f(x)$. This is the deterministic trajectory, which describes the system relaxing towards a stable equilibrium. The action along this path is $S = \\int \\frac{1}{2}(f(x) - f(x))^2 dt = 0$. This describes dynamics within a basin of attraction.\n2.  $p = -2f(x)$: This implies $\\dot{x} = p + f(x) = -2f(x) + f(x) = -f(x)$. This describes the \"most probable escape path\" (MPEP), which moves the system \"uphill\" against the deterministic flow. This is the path relevant for calculating the quasipotential cost of rare transitions.\n\nThe quasipotential $V(x)$ is related to the momentum along the MPEP by $p = \\frac{\\partial V}{\\partial x}$. This relationship stems from the stationary Hamilton-Jacobi equation (HJE) for the quasipotential, $H(x, \\frac{\\partial V}{\\partial x}) = 0$. Substituting $p = V'(x)$ into the HJE gives:\n$$\n\\dfrac{1}{2}(V'(x))^2 + V'(x)f(x) = 0\n$$\nThis yields two solutions for the gradient of the quasipotential: $V'(x)=0$ (the trivial case) and $V'(x)=-2f(x)$, which describes the change in potential along the MPEP.\n\nTo find the quasipotential difference between two points, say an attractor $x_a$ and another point $x_b$, we integrate this gradient:\n$$\nV(x_b) - V(x_a) = \\int_{x_a}^{x_b} V'(y)\\,dy = -2 \\int_{x_a}^{x_b} f(y)\\,dy\n$$\nThis is the central relationship we need. By definition, the quasipotential at any stable equilibrium is zero, so $V(x_{\\text{stable}}) = 0$. The barrier height for a transition from a stable point $x_a$ to an unstable saddle point $x_b$ is the potential difference $V(x_b) - V(x_a)$.\n\n### Computational Method and Application\n\nThe derived formula allows for the numerical computation of the potential barriers in a bistable system. A system is considered bistable if it possesses three equilibria $x_{\\mathrm{low}}  x_{\\mathrm{sad}}  x_{\\mathrm{high}}$, where $x_{\\mathrm{low}}$ and $x_{\\mathrm{high}}$ are stable ($f'(x^\\star)0$) and $x_{\\mathrm{sad}}$ is unstable ($f'(x^\\star)>0$).\n\n1.  **Finding Equilibria**: Equilibria are roots of $f(x)=0$. They are found by scanning the domain $[0, x_{\\max}]$ with a fine grid to identify intervals $[x_i, x_{i+1}]$ where $f(x)$ changes sign. A robust root-finding algorithm (like Brent's method) is then used on each such bracket to find the root $x^\\star$ to high precision.\n\n2.  **Stability Analysis**: Each equilibrium $x^\\star$ is classified by evaluating the sign of the analytical derivative $f'(x^\\star) = \\beta \\frac{n\\,x^{n-1} K^n}{(K^n + x^n)^2} - \\delta$.\n\n3.  **Barrier Calculation**:\n    If the system is bistable with stable equilibria $x_{\\mathrm{low}}, x_{\\mathrm{high}}$ and unstable equilibrium $x_{\\mathrm{sad}}$:\n    -   The barrier from the low state is $B_{\\mathrm{low}\\to\\mathrm{sad}} = V(x_{\\mathrm{sad}}) - V(x_{\\mathrm{low}})$. Since $V(x_{\\mathrm{low}})=0$, this is:\n        $$\n        B_{\\mathrm{low}\\to\\mathrm{sad}} = -2 \\int_{x_{\\mathrm{low}}}^{x_{\\mathrm{sad}}} f(y)\\,dy\n        $$\n        In the interval $(x_{\\mathrm{low}}, x_{\\mathrm{sad}})$, $f(y)$ is negative, ensuring the barrier height is positive.\n    -   The barrier from the high state is $B_{\\mathrm{high}\\to\\mathrm{sad}} = V(x_{\\mathrm{sad}}) - V(x_{\\mathrm{high}})$. Since $V(x_{\\mathrm{high}})=0$, this must be calculated with respect to the high stable state as the reference. The formula directly gives:\n        $$\n        B_{\\mathrm{high}\\to\\mathrm{sad}} = -2 \\int_{x_{\\mathrm{high}}}^{x_{\\mathrm{sad}}} f(y)\\,dy = 2 \\int_{x_{\\mathrm{sad}}}^{x_{\\mathrm{high}}} f(y)\\,dy\n        $$\n        In the interval $(x_{\\mathrm{sad}}, x_{\\mathrm{high}})$, $f(y)$ is positive, ensuring this barrier height is also positive.\n\nThese integrals are computed using high-accuracy numerical quadrature. If the system is not bistable (i.e., does not have the specified `stable-unstable-stable` configuration of equilibria), the barriers are reported as $0.0$.\n\nThe following Python code implements this procedure. It defines the functions $f(x)$ and $f'(x)$, systematically finds and classifies all equilibria within the given domain, checks for bistability, and computes the two potential barriers using numerical integration via `scipy.integrate.quad`. The results for the three test cases are then formatted and printed as required.", "answer": "```python\nimport numpy as np\nfrom scipy import optimize, integrate\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test cases.\n    It processes each case, finds equilibria, checks for bistability,\n    calculates potential barriers, and prints the formatted results.\n    \"\"\"\n    test_cases = [\n        # Case A (bistable)\n        (0.2, 120.0, 50.0, 4.0, 1.0, 200.0),\n        # Case B (marginally bistable)\n        (0.2, 55.0, 50.0, 4.0, 1.0, 200.0),\n        # Case C (monostable)\n        (0.2, 20.0, 50.0, 4.0, 1.0, 200.0),\n    ]\n\n    results = []\n    for case in test_cases:\n        results.append(process_case(*case))\n\n    # Format the final output string exactly as specified in the template.\n    # str() on a list adds spaces, which we join with a comma.\n    # The f-string then wraps this in brackets. This creates a string\n    # representation of a list of lists, but without a space after the\n    # comma separating the top-level list elements.\n    # e.g., [[...],[...],[...]] instead of [[...], [...], [...]]\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef process_case(beta0, beta, K, n, delta, x_max):\n    \"\"\"\n    Processes a single parameter set to find equilibria and barriers.\n    \"\"\"\n    params = (beta0, beta, K, n, delta)\n\n    def f(x, p):\n        beta0_p, beta_p, K_p, n_p, delta_p = p\n        # To avoid potential overflow for large x and n, pre-calculate x/K\n        ratio_x_k = x / K_p\n        xn = ratio_x_k**n_p\n        return beta0_p + beta_p * xn / (1.0 + xn) - delta_p * x\n\n    def f_prime(x, p):\n        beta0_p, beta_p, K_p, n_p, delta_p = p\n        x_n = x**n_p\n        K_n = K_p**n_p\n        numerator = beta_p * n_p * x**(n_p - 1) * K_n\n        denominator = (K_n + x_n)**2\n        return numerator / denominator - delta_p\n\n    # Step 1: Find equilibria\n    equilibria = []\n    stability = []\n    \n    # Use a fine grid to bracket roots\n    scan_points = np.linspace(0, x_max, 20001)\n    f_values = f(scan_points, params)\n\n    # Find brackets where f(x) changes sign\n    for i in range(len(scan_points) - 1):\n        if np.sign(f_values[i]) != np.sign(f_values[i+1]):\n            try:\n                root = optimize.brentq(f, scan_points[i], scan_points[i+1], args=(params,))\n                equilibria.append(root)\n            except ValueError:\n                # brentq can fail if signs are not different, though the check should prevent this.\n                pass\n    \n    # Check for roots at the boundaries or grid points\n    for i, x_val in enumerate(scan_points):\n        if np.isclose(f_values[i], 0.0, atol=1e-9):\n            equilibria.append(x_val)\n            \n    # Deduplicate and sort roots\n    if equilibria:\n        equilibria = sorted(list(set(np.round(equilibria, 9))))\n\n    # Step 2: Classify equilibria\n    root_info = []\n    for x_star in equilibria:\n        deriv = f_prime(x_star, params)\n        if deriv  0:\n            stype = 'stable'\n        elif deriv > 0:\n            stype = 'unstable'\n        else:\n            stype = 'degenerate' # For saddle-node, etc.\n        root_info.append({'root': x_star, 'type': stype})\n    \n    # Step 3: Check for bistability\n    is_bistable = False\n    if len(root_info) == 3:\n        stabilities = [info['type'] for info in root_info]\n        if stabilities == ['stable', 'unstable', 'stable']:\n            is_bistable = True\n\n    # Step 4: Calculate barriers\n    barrier_low_sad = 0.0\n    barrier_high_sad = 0.0\n    \n    if is_bistable:\n        x_low = root_info[0]['root']\n        x_sad = root_info[1]['root']\n        x_high = root_info[2]['root']\n\n        # Barrier = -2 * integral(f(y) dy)\n        # scipy.integrate.quad returns (integral, error_estimate)\n        integral_low_sad, _ = integrate.quad(f, x_low, x_sad, args=(params,))\n        barrier_low_sad = -2.0 * integral_low_sad\n        \n        # Or alternatively: 2 * integral_sad_high\n        integral_sad_high, _ = integrate.quad(f, x_sad, x_high, args=(params,))\n        barrier_high_sad = 2.0 * integral_sad_high\n\n    # Ensure barriers are non-negative\n    barrier_low_sad = max(0.0, barrier_low_sad)\n    barrier_high_sad = max(0.0, barrier_high_sad)\n    \n    # Format output for this case\n    return [is_bistable, [info['root'] for info in root_info], barrier_low_sad, barrier_high_sad]\n\nif __name__ == '__main__':\n    solve()\n\n```", "id": "3343275"}, {"introduction": "With a theoretical understanding of what makes an event rare, the next step is to compute its probability or rate in a full stochastic model. Since direct simulation is prohibitively expensive, we rely on sophisticated path sampling algorithms. This comprehensive exercise [@problem_id:3343233] challenges you to implement three workhorse methods of the field—Forward Flux Sampling, Multilevel Splitting, and Weighted Ensemble—from first principles, providing an unparalleled hands-on experience in the core techniques of rare event simulation.", "problem": "Consider a single-gene positive-feedback network modeled as a continuous-time Markov chain governed by the Chemical Master Equation. The gene expresses a single protein species, denoted by $X$, with integer-valued copy number $x \\in \\{0,1,2,\\dots\\}$. Production and degradation are modeled by two reactions: a birth reaction $\\varnothing \\to X$ and a death reaction $X \\to \\varnothing$. The production propensity is a Hill-type function $a_{+}(x)$ and the degradation propensity is linear in $x$. The system evolves with standard stochastic kinetics, where reaction waiting times are exponentially distributed with rate equal to the sum of the propensities, and the next reaction is sampled proportionally to each propensity. The region $A$ is defined as a low-expression basin and the region $B$ is defined as a high-expression basin. A transition $A \\to B$ is a rare event for suitable parameters.\n\nYour task is to implement a complete, runnable program that estimates the transition rate from $A$ to $B$ using the following methods, grounded in first principles:\n\n$1.$ The Splitting method (multilevel splitting with weights).\n\n$2.$ Forward Flux Sampling (FFS).\n\n$3.$ The Weighted Ensemble method (WE).\n\nThe gene network is defined by the following propensities for state $x$:\n\n$$\na_{+}(x) = k_{0} + k_{1}\\,\\frac{x^{h}}{K^{h} + x^{h}}, \\quad a_{-}(x) = \\gamma\\,x, \\quad a_{0}(x) = a_{+}(x) + a_{-}(x).\n$$\n\nThe event of interest is the $A \\to B$ transition under a renewal protocol: after a transition to $B$, trajectories are conceptually restarted in $A$; similarly, for methods that recycle walkers, walkers that reach $B$ are returned to $A$. Interfaces are defined by a monotonically increasing sequence of integer thresholds $\\lambda_{0}, \\lambda_{1}, \\dots, \\lambda_{m}$ with $\\lambda_{0}$ marking the boundary of $A$ and $\\lambda_{m}$ being the threshold for $B$. Use exact stochastic simulation according to the Direct Method of Gillespie: for state $x$, draw a waiting time $\\tau$ from an exponential distribution with rate $a_{0}(x)$, then choose birth with probability $a_{+}(x)/a_{0}(x)$ or death with probability $a_{-}(x)/a_{0}(x)$.\n\nImplement each estimator as follows, starting only from the above definitions and without invoking any shortcut formulas:\n\n$1.$ Multilevel Splitting: start at interface $\\lambda_{1}$ with unit weight, perform level-wise branching with a fixed cloning factor at each interface, propagate each clone until it either reaches the next interface or returns to $A$, and carry forward weights only for successful clones. At the last interface, collect the total weight of clones that reach $B$.\n\n$2.$ Forward Flux Sampling: estimate the flux of first crossings from $A$ to $\\lambda_{1}$ per unit time under renewal from $A$, then estimate the conditional advancement probabilities between successive interfaces, starting at each interface state. Combine these ingredients to obtain the transition rate under the renewal definition.\n\n$3.$ Weighted Ensemble: maintain an ensemble of walkers with weights partitioned into bins defined by the interfaces, propagate each walker for a fixed physical time interval, record weighted flux into $B$ with recycling to $A$, and perform per-bin resampling to maintain a target number of walkers with conserved total weight.\n\nAll results must be expressed in inverse seconds, i.e., in $\\mathrm{s}^{-1}$.\n\nProgram Requirements:\n\n$1.$ Implement exact stochastic dynamics using the Direct Method of Gillespie for all propagation tasks.\n\n$2.$ Use a fixed pseudorandom seed for reproducibility.\n\n$3.$ For each test case (parameter set) below, compute three floating-point estimates (one per method) of the $A \\to B$ transition rate under the renewal protocol.\n\n$4.$ Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets of lists, one inner list per test case in order, where each inner list contains the three floats corresponding to Splitting, Forward Flux Sampling, and Weighted Ensemble, respectively. For example, the output format must be like $\\left[\\left[r_{11},r_{12},r_{13}\\right],\\left[r_{21},r_{22},r_{23}\\right],\\left[r_{31},r_{32},r_{33}\\right]\\right]$, with each $r_{ij}$ a float in $\\mathrm{s}^{-1}$.\n\nTest Suite:\n\nUse the following parameter sets and algorithm hyperparameters. All numbers below must be used exactly as specified.\n\n$-$ Test Case $1$ (happy path, moderately rare):\n\nParameters: $k_{0} = 0.5$, $k_{1} = 20.0$, $K = 30.0$, $h = 2$, $\\gamma = 1.0$.\n\nInterfaces: $\\lambda = [10,20,30,40,50]$, with $A$ defined by $x \\le \\lambda_{0} = 10$ and $B$ defined by $x \\ge \\lambda_{4} = 50$.\n\nForward Flux Sampling hyperparameters: number of renewal attempts $N_{\\mathrm{flux}} = 200$, flux time cap per attempt $T_{\\mathrm{flux}} = 100.0$ seconds, number of paths per level $N_{\\mathrm{level}} = 200$, per-path time cap $T_{\\mathrm{level}} = 50.0$ seconds.\n\nSplitting hyperparameters: cloning factor per level $M = 3$, independent rounds $R = 50$, per-clone time cap $T_{\\mathrm{split}} = 50.0$ seconds.\n\nWeighted Ensemble hyperparameters: total walkers $W = 60$, cycles $C = 150$, time per cycle $\\Delta t = 0.5$ seconds, target walkers per bin equal to $\\max(1,\\lfloor W/m \\rfloor)$ where $m$ is the number of interfaces.\n\n$-$ Test Case $2$ (less rare, lower $B$):\n\nParameters: $k_{0} = 0.5$, $k_{1} = 30.0$, $K = 25.0$, $h = 2$, $\\gamma = 1.0$.\n\nInterfaces: $\\lambda = [8,16,24,32,40]$, with $A$ defined by $x \\le \\lambda_{0} = 8$ and $B$ defined by $x \\ge \\lambda_{4} = 40$.\n\nForward Flux Sampling hyperparameters: $N_{\\mathrm{flux}} = 150$, $T_{\\mathrm{flux}} = 60.0$ seconds, $N_{\\mathrm{level}} = 150$, $T_{\\mathrm{level}} = 40.0$ seconds.\n\nSplitting hyperparameters: $M = 3$, $R = 40$, $T_{\\mathrm{split}} = 40.0$ seconds.\n\nWeighted Ensemble hyperparameters: $W = 60$, $C = 120$, $\\Delta t = 0.4$ seconds, target walkers per bin equal to $\\max(1,\\lfloor W/m \\rfloor)$.\n\n$-$ Test Case $3$ (edge case, stronger cooperativity, rarer):\n\nParameters: $k_{0} = 0.3$, $k_{1} = 15.0$, $K = 35.0$, $h = 4$, $\\gamma = 1.0$.\n\nInterfaces: $\\lambda = [12,24,36,48,60]$, with $A$ defined by $x \\le \\lambda_{0} = 12$ and $B$ defined by $x \\ge \\lambda_{4} = 60$.\n\nForward Flux Sampling hyperparameters: $N_{\\mathrm{flux}} = 300$, $T_{\\mathrm{flux}} = 120.0$ seconds, $N_{\\mathrm{level}} = 250$, $T_{\\mathrm{level}} = 60.0$ seconds.\n\nSplitting hyperparameters: $M = 4$, $R = 60$, $T_{\\mathrm{split}} = 60.0$ seconds.\n\nWeighted Ensemble hyperparameters: $W = 80$, $C = 200$, $\\Delta t = 0.5$ seconds, target walkers per bin equal to $\\max(1,\\lfloor W/m \\rfloor)$.\n\nPhysical units: all time-related quantities must be in seconds, and all reported transition rates must be in $\\mathrm{s}^{-1}$.\n\nYour program must be self-contained, take no input, and adhere to the output format described above. The scientific scenario and parameterization must be implemented exactly as stated, and all methods must use the Gillespie Direct Method for propagation. The final answer must be the complete program.", "solution": "The user seeks to estimate the transition rate between a low-expression state ($A$) and a high-expression state ($B$) in a stochastic single-gene positive-feedback network. The problem is well-posed and scientifically grounded, providing all necessary parameters and equations for a computational solution. The core of the problem is to implement three distinct rare-event simulation algorithms: Multilevel Splitting, Forward Flux Sampling (FFS), and the Weighted Ensemble (WE) method. All three methods will rely on an exact stochastic simulation of the system dynamics using the Gillespie Direct Method.\n\n### 1. System Dynamics and Simulation Core\n\nThe system state is given by the integer copy number $x$ of a protein $X$. The state changes through two reactions: production ($\\varnothing \\to X$) and degradation ($X \\to \\varnothing$). The propensities are:\n$$\na_{+}(x) = k_{0} + k_{1}\\,\\frac{x^{h}}{K^{h} + x^{h}} \\quad (\\text{production})\n$$\n$$\na_{-}(x) = \\gamma\\,x \\quad (\\text{degradation})\n$$\nThe total propensity is $a_{0}(x) = a_{+}(x) + a_{-}(x)$.\n\nThe Gillespie Direct Method is used for propagation. Given the system is in state $x$ at time $t$:\n1.  A time to the next reaction, $\\tau$, is drawn from an exponential distribution with rate $a_{0}(x)$: $\\tau \\sim \\exp(a_{0}(x))$. The time of the next reaction is $t + \\tau$.\n2.  The reaction to occur is chosen based on the relative propensities. A random number $r \\in [0,1)$ is drawn. If $r  a_{+}(x)/a_{0}(x)$, a production event occurs, and the state becomes $x+1$. Otherwise, a degradation event occurs, and the state becomes $x-1$.\n\nA core simulation function, `gillespie_step(x, params, rng)`, will implement this logic, returning the time elapsed $\\tau$ and the new state. A wrapper function, `propagate`, will repeatedly call this step function to simulate a trajectory until a specified condition is met (e.g., reaching a boundary or a time limit).\n\n### 2. Rare Event Sampling Algorithms\n\nThe transition from a low-expression basin $A = \\{x \\le \\lambda_0\\}$ to a high-expression basin $B = \\{x \\ge \\lambda_m\\}$ is a rare event. We use a series of interfaces, $\\lambda_0, \\lambda_1, \\dots, \\lambda_m$, to break down the transition into more probable steps.\n\n#### 2.1 Forward Flux Sampling (FFS)\n\nThe FFS method computes the rate $k_{A \\to B}$ as the product of an initial flux into the first interface and a series of conditional probabilities of advancing between subsequent interfaces.\nThe rate is given by:\n$$\nk_{A \\to B} = \\Phi_{A \\to \\lambda_1} \\times \\prod_{i=1}^{m-1} P(\\lambda_{i+1}|\\lambda_i)\n$$\nwhere $\\Phi_{A \\to \\lambda_1}$ is the flux of trajectories originating in $A$ that cross $\\lambda_1$ for the first time, and $P(\\lambda_{i+1}|\\lambda_i)$ is the conditional probability that a trajectory starting at interface $\\lambda_i$ will reach $\\lambda_{i+1}$ before returning to $A$.\n\nThe algorithm proceeds in stages:\n- **Stage 0: Flux Calculation**: A set of long trajectories are simulated, starting from a state in $A$ (e.g., $x=0$). We count the number of times trajectories cross $\\lambda_1$ for the first time after being in $A$, let this be $N_{\\text{cross}}$. The total simulation time is $T_{\\text{total}}$. The flux is estimated as $\\Phi_{A \\to \\lambda_1} = N_{\\text{cross}} / T_{\\text{total}}$. The configurations (states) where these crossings occur are saved for the next stage.\n- **Stages $i=1, \\dots, m-1$**: To estimate $P(\\lambda_{i+1}|\\lambda_i)$, a number of trial trajectories ($N_{\\mathrm{level}}$) are initiated from configurations collected at interface $\\lambda_i$. Each trial is run until it either reaches $\\lambda_{i+1}$ (a success) or returns to $A$ (a failure). A time cap $T_{\\mathrm{level}}$ is imposed to prevent infinite loops. The probability is estimated as the fraction of successful trials: $P(\\lambda_{i+1}|\\lambda_i) \\approx N_{\\text{success}} / N_{\\mathrm{level}}$.\n\n#### 2.2 Multilevel Splitting (Splitting)\n\nThe problem description for Splitting is somewhat ambiguous. A standard interpretation that produces a rate in $\\mathrm{s}^{-1}$ and fits the given hyperparameters involves combining an initial flux measurement with a probability estimation via cloning.\n1.  **Initial Flux ($\\Phi_0$)**: Similar to FFS Stage 0, we must first estimate the rate at which trajectories enter the rare event pathway. We compute $\\Phi_{A \\to \\lambda_1}$ by running long simulations from $A$. For consistency, we use the FFS Stage 0 hyperparameters for this step.\n2.  **Conditional Probability Estimation**: We estimate the probability $P_{B|\\lambda_1}$ that a trajectory starting at $\\lambda_1$ reaches $B$ before returning to $A$. This is done over $R$ independent rounds, and the results are averaged.\n    - In each round, we start a single trajectory at a representative state on the first interface, e.g., $x=\\lambda_1$. This root trajectory is propagated until it either reaches the next interface, $\\lambda_2$, or returns to $A$.\n    - If it returns to $A$, the round terminates with a probability estimate of $0$.\n    - If it reaches $\\lambda_2$, it is \"cloned\" into $M$ identical copies. Each of these $M$ trajectories is then propagated independently from the point of success until it reaches $\\lambda_3$ or returns to $A$.\n    - This process continues level-wise. If $S_i$ trajectories successfully reach $\\lambda_{i+1}$ from $\\lambda_i$, they spawn a total of $S_i \\times M$ new trajectories for the next level.\n    - The final probability for one round is the number of trajectories that successfully reach $B$ ($x \\ge \\lambda_m$), divided by the total number of trajectories that would have existed if all paths were successful, which is $M^{m-1}$.\n    - The final estimate for $P_{B|\\lambda_1}$ is the average of the probabilities from all $R$ rounds.\nThe overall rate is $k_{\\text{split}} = \\Phi_{A \\to \\lambda_1} \\times P_{B|\\lambda_1}$.\n\n#### 2.3 Weighted Ensemble (WE)\n\nThe WE method maintains a constant number of weighted walkers distributed across the state space. It estimates the steady-state flux into region $B$.\n1.  **Initialization**: An ensemble of $W$ walkers is created, typically starting in region $A$ (e.g., at $x=0$), each with an initial weight of $1/W$. The total weight of the ensemble is conserved and remains $1$.\n2.  **Simulation Cycles**: The simulation proceeds in discrete time cycles of duration $\\Delta t$.\n    - **Propagation**: In each cycle, every walker $(x_k, w_k)$ is propagated independently for time $\\Delta t$ using the Gillespie algorithm. During this propagation, if a walker's state $x$ reaches or exceeds $\\lambda_m$ (enters $B$), its weight $w_k$ is added to a cumulative flux counter. The walker is then immediately \"recycled\" by resetting its state to a chosen location in $A$ (e.g., $x=0$) to continue simulating for any remaining time within the $\\Delta t$ interval. This implements the renewal protocol.\n    - **Resampling**: After all walkers have been propagated for $\\Delta t$, they are grouped into predefined bins based on their final positions. The state space is partitioned into bins based on the interfaces: $(-\\infty, \\lambda_0], (\\lambda_0, \\lambda_1], \\dots, (\\lambda_{m-2}, \\lambda_{m-1}], (\\lambda_{m-1}, \\lambda_m)$. The number of walkers in each bin is then adjusted to a target number. This is done by \"splitting\" walkers in underpopulated bins and \"merging\" walkers in overpopulated bins, in a way that conserves the total weight within each bin. For instance, to bring the number of walkers in a bin from $n_j$ to a target $T_j$, we resample with replacement $T_j$ times from the $n_j$ walkers, where the probability of picking a walker is proportional to its weight. The total weight of the bin, $W_j$, is then evenly distributed among the $T_j$ new walkers, each receiving weight $W_j/T_j$.\n3.  **Rate Calculation**: After $C$ cycles, the total simulation time is $T = C \\times \\Delta t$. The total weight that has flowed into region $B$ is $\\mathcal{F}_B$. The transition rate is the steady-state flux, estimated as $k_{\\text{WE}} = \\mathcal{F}_B / T$.\n\nBy implementing these three distinct but related methods, we can generate three independent estimates for the same physical quantity, providing a robust analysis of the rare transition event. The fixed pseudorandom seed ensures that the stochastic simulations are reproducible.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport math\n\n# Use a fixed seed for reproducibility as required.\nRNG = np.random.default_rng(12345)\n\ndef gillespie_step(x, params, rng):\n    \"\"\"Performs a single step of the Gillespie Direct Method.\"\"\"\n    k0, k1, K, h, gamma = params\n    \n    a_plus = k0 + k1 * (x**h) / (K**h + x**h)\n    a_minus = gamma * x\n    a_total = a_plus + a_minus\n\n    if a_total == 0:\n        return float('inf'), x\n\n    dt = rng.exponential(1.0 / a_total)\n    \n    if rng.random()  a_plus / a_total:\n        x_new = x + 1\n    else:\n        x_new = x - 1\n        \n    return dt, x_new\n\ndef propagate(x_start, params, stop_cond, time_cap, rng):\n    \"\"\"Propagates a trajectory until a stop condition or time cap is met.\"\"\"\n    x = x_start\n    t = 0.0\n    while t  time_cap:\n        if stop_cond(x):\n            return x, t\n        \n        dt, x_new = gillespie_step(x, params, rng)\n        \n        if t + dt > time_cap:\n            # Step would exceed time_cap, so stop before taking it.\n            # State 'x' is the last state within the time cap 't'.\n            # We return time_cap to indicate the full duration was used.\n            return x, time_cap\n\n        t += dt\n        x = x_new\n    return x, t\n\ndef calculate_rate_ffs(params, interfaces, hyperparams, rng):\n    \"\"\"Estimates transition rate using Forward Flux Sampling.\"\"\"\n    lambda_list = interfaces\n    lam0, lam_final = lambda_list[0], lambda_list[-1]\n    \n    n_flux, t_flux, n_level, t_level = hyperparams\n    \n    # Stage 0: Flux calculation\n    crossings = []\n    total_time = 0.0\n    initial_state_A = 0\n    \n    for _ in range(n_flux):\n        x = initial_state_A\n        t_sim = 0.0\n        in_A = True\n        \n        while t_sim  t_flux:\n            dt, x_new = gillespie_step(x, params, rng)\n            \n            if t_sim + dt > t_flux:\n                total_time += t_flux - t_sim\n                break\n            \n            t_sim += dt\n            \n            if in_A and x >= lam0 and x_new  lam0: # Re-entered A boundary\n                 in_A = True # Redundant but for clarity\n            \n            if in_A and x  lambda_list[1] and x_new >= lambda_list[1]:\n                # First crossing of lambda_1 from A\n                crossings.append(x_new)\n                in_A = False # To not double count without returning to A\n                \n            if not in_A and x_new = lam0:\n                in_A = True # Reset for next potential crossing\n\n            x = x_new\n        \n        total_time += min(t_sim, t_flux)\n\n    if not crossings:\n        return 0.0\n\n    flux0 = len(crossings) / total_time\n    \n    current_configs = crossings\n    \n    # Stages 1 to m-1\n    total_prob = 1.0\n    for i in range(1, len(lambda_list) - 1):\n        lam_i = lambda_list[i]\n        lam_i_plus_1 = lambda_list[i+1]\n        \n        n_success = 0\n        \n        start_configs = rng.choice(current_configs, size=n_level, replace=True)\n        next_configs = []\n        \n        for start_x in start_configs:\n            x_end, _ = propagate(start_x, params, lambda x: x >= lam_i_plus_1 or x = lam0, t_level, rng)\n            if x_end >= lam_i_plus_1:\n                n_success += 1\n                next_configs.append(x_end)\n        \n        if n_level == 0:\n             prob_i = 0.0\n        else:\n             prob_i = n_success / n_level\n        total_prob *= prob_i\n        \n        if not next_configs:\n            return 0.0 # Extinction\n        current_configs = next_configs\n        \n    return flux0 * total_prob\n\ndef calculate_rate_splitting(params, interfaces, hyperparams_s, hyperparams_f, rng):\n    \"\"\"Estimates transition rate using Multilevel Splitting.\"\"\"\n    lambda_list = interfaces\n    lam0, lam_final = lambda_list[0], lambda_list[-1]\n    \n    M, R, t_split = hyperparams_s\n    n_flux, t_flux, _, _ = hyperparams_f\n\n    # Stage 0: Flux calculation (reusing FFS logic and params)\n    total_time = 0.0\n    n_crossings = 0\n    initial_state_A = 0\n    for _ in range(n_flux):\n        x = initial_state_A\n        t_sim = 0.0\n        in_A = True\n        while t_sim  t_flux:\n            dt, x_new = gillespie_step(x, params, rng)\n            if t_sim + dt > t_flux:\n                total_time += t_flux - t_sim\n                break\n            t_sim += dt\n            if in_A and x  lambda_list[1] and x_new >= lambda_list[1]:\n                n_crossings += 1\n                in_A = False\n            if not in_A and x_new = lam0:\n                in_A = True\n            x = x_new\n        total_time += min(t_sim, t_flux)\n    \n    if total_time == 0:\n      flux0 = 0.0\n    else:\n      flux0 = n_crossings / total_time\n\n    if flux0 == 0.0:\n        return 0.0\n\n    # Probability estimation over R rounds\n    prob_estimates = []\n    num_levels = len(lambda_list) - 1\n    \n    for _ in range(R):\n        walkers = [lambda_list[1]]\n        \n        for i in range(1, num_levels):\n            lam_i_plus_1 = lambda_list[i+1]\n            next_walkers = []\n            \n            if not walkers:\n                break\n\n            for x_start in walkers:\n                for _ in range(M):\n                    x_end, _ = propagate(x_start, params, lambda x: x >= lam_i_plus_1 or x = lam0, t_split, rng)\n                    if x_end >= lam_i_plus_1:\n                        next_walkers.append(x_end)\n            walkers = next_walkers\n\n        # Final level to B\n        n_success_final = 0\n        if walkers:\n            for x_start in walkers:\n                x_end, _ = propagate(x_start, params, lambda x: x >= lam_final or x = lam0, t_split, rng)\n                if x_end >= lam_final:\n                    n_success_final += 1\n        \n        prob_round = n_success_final / (M**(num_levels - 1))\n        prob_estimates.append(prob_round)\n\n    avg_prob = np.mean(prob_estimates) if prob_estimates else 0.0\n    return flux0 * avg_prob\n\ndef calculate_rate_we(params, interfaces, hyperparams, rng):\n    \"\"\"Estimates transition rate using Weighted Ensemble.\"\"\"\n    W, C, dt_cycle = hyperparams['W'], hyperparams['C'], hyperparams['dt']\n    target_per_bin = hyperparams['target_per_bin']\n    \n    lambda_list = interfaces\n    lam0, lam_final = lambda_list[0], lambda_list[-1]\n    num_bins = len(lambda_list) \n    \n    # Bins: (-inf, lam0], (lam0, lam1], ..., (lam_{m-2}, lam_{m-1}], (lam_{m-1}, lam_final]\n    # For this implementation, let's define bins as:\n    # bin 0: x = lam0\n    # bin 1: lam0  x = lam1\n    # ...\n    # bin m-1: lam_{m-2}  x = lam_{m-1}\n    # bin m: x > lam_{m-1}\n    # We will resample in bins 0 to m-1.\n    # The last defined interface is lam_{m-1}, where m=num_bins\n    \n    def get_bin_index(x):\n        if x = lam0: return 0\n        for i in range(len(lambda_list) - 1):\n            if x > lambda_list[i] and x = lambda_list[i+1]:\n                return i + 1\n        return len(lambda_list) # Final bin for x > last interface\n    \n    # Initialization\n    initial_state_A = 0\n    walkers = [{'x': initial_state_A, 'w': 1.0 / W} for _ in range(W)]\n    \n    total_flux_to_B = 0.0\n\n    for _ in range(C):\n        # Propagation\n        for i in range(len(walkers)):\n            walker = walkers[i]\n            t_rem = dt_cycle\n            while t_rem > 0:\n                current_x = walker['x']\n                dt, next_x = gillespie_step(current_x, params, rng)\n                \n                if dt > t_rem:\n                    \n                    walker['x'] = current_x \n                    break \n\n                t_rem -= dt\n                walker['x'] = next_x\n                \n                if walker['x'] >= lam_final:\n                    total_flux_to_B += walker['w']\n                    walker['x'] = initial_state_A # Recycle to A\n\n        # Resampling\n        bins = [[] for _ in range(num_bins)]\n        for walker in walkers:\n            idx = get_bin_index(walker['x'])\n            if idx  num_bins:  # Only bin states up to lam_final\n                bins[idx].append(walker)\n\n        new_walkers = []\n        for i in range(num_bins):\n            bin_walkers = bins[i]\n            if not bin_walkers:\n                continue\n\n            total_weight_in_bin = sum(w['w'] for w in bin_walkers)\n            \n            # Use fixed target number per bin\n            target_n = target_per_bin\n\n            # Resample\n            weights = np.array([w['w'] for w in bin_walkers])\n            p = weights / total_weight_in_bin\n            \n            indices = rng.choice(len(bin_walkers), size=target_n, p=p, replace=True)\n            \n            for index in indices:\n                new_walkers.append({'x': bin_walkers[index]['x'], 'w': total_weight_in_bin / target_n})\n        \n        walkers = new_walkers\n        if not walkers: # Extinction\n            break\n            \n    total_sim_time = C * dt_cycle\n    return total_flux_to_B / total_sim_time\n\ndef solve():\n    test_cases = [\n        {\n            \"params\": (0.5, 20.0, 30.0, 2, 1.0),\n            \"interfaces\": [10, 20, 30, 40, 50],\n            \"hyperparams_ffs\": (200, 100.0, 200, 50.0),\n            \"hyperparams_split\": (3, 50, 50.0),\n            \"hyperparams_we\": {'W': 60, 'C': 150, 'dt': 0.5},\n        },\n        {\n            \"params\": (0.5, 30.0, 25.0, 2, 1.0),\n            \"interfaces\": [8, 16, 24, 32, 40],\n            \"hyperparams_ffs\": (150, 60.0, 150, 40.0),\n            \"hyperparams_split\": (3, 40, 40.0),\n            \"hyperparams_we\": {'W': 60, 'C': 120, 'dt': 0.4},\n        },\n        {\n            \"params\": (0.3, 15.0, 35.0, 4, 1.0),\n            \"interfaces\": [12, 24, 36, 48, 60],\n            \"hyperparams_ffs\": (300, 120.0, 250, 60.0),\n            \"hyperparams_split\": (4, 60, 60.0),\n            \"hyperparams_we\": {'W': 80, 'C': 200, 'dt': 0.5},\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        params = case[\"params\"]\n        interfaces = case[\"interfaces\"]\n        hyperparams_ffs = case[\"hyperparams_ffs\"]\n        hyperparams_split = case[\"hyperparams_split\"]\n        hyperparams_we = case[\"hyperparams_we\"]\n        W = hyperparams_we['W']\n        # Per problem: m is number of interfaces\n        m = len(interfaces)\n        hyperparams_we['target_per_bin'] = max(1, math.floor(W / m))\n\n        \n        rate_split = calculate_rate_splitting(params, interfaces, hyperparams_split, hyperparams_ffs, RNG)\n        rate_ffs = calculate_rate_ffs(params, interfaces, hyperparams_ffs, RNG)\n        rate_we = calculate_rate_we(params, interfaces, hyperparams_we, RNG)\n        \n        all_results.append([rate_split, rate_ffs, rate_we])\n\n    # Format output as [[r11,r12,r13],[r21,r22,r23],...]\n    outer_list_str = []\n    for res_list in all_results:\n        inner_list_str = f\"[{','.join(map(str, res_list))}]\"\n        outer_list_str.append(inner_list_str)\n    print(f\"[{','.join(outer_list_str)}]\")\n\nsolve()\n```", "id": "3343233"}, {"introduction": "A successful implementation of a path sampling algorithm is not always an efficient one, and performance often hinges on a subtle but critical choice: the reaction coordinate used to define progress. This final practice delves into a common and challenging pitfall where a naive coordinate leads to a non-monotone committor function, severely degrading estimator efficiency. By diagnosing the failure modes and designing a robust solution based on adaptive interfaces [@problem_id:3343241], you will develop the critical thinking needed to troubleshoot and optimize rare-event simulations in complex, multidimensional systems.", "problem": "Consider a stochastic gene expression network in which a promoter switches between an OFF state and an ON state. Let the promoter state be denoted by $s \\in \\{0,1\\}$ where $s=0$ means OFF and $s=1$ means ON. Let the protein copy number be denoted by $x \\in \\mathbb{N}$. Assume the following dynamics: while $s=1$, protein births occur at rate $\\beta0$ and degradation occurs at rate $\\delta x$ with $\\delta0$; while $s=0$, protein births occur at a negligible rate $\\epsilon \\approx 0$ and degradation still occurs at rate $\\delta x$. The promoter switches $0 \\to 1$ at rate $k_{\\mathrm{on}}0$ and $1 \\to 0$ at rate $k_{\\mathrm{off}}0$. The joint process $(x,s)$ is Markovian and is governed by the Chemical Master Equation (CME). Define a low-expression basin $A=\\{(x,s): x \\le a\\}$ and a high-expression basin $B=\\{(x,s): x \\ge b\\}$ with $ab$.\n\nLet the committor function $q(x,s)$ be the probability, starting from $(x,s)$, to hit $B$ before $A$. Equivalently, $q$ solves the backward equation for the generator $\\mathcal{L}$ of the CME,\n$$\n\\mathcal{L} q(x,s) = 0 \\quad \\text{for } (x,s) \\notin A \\cup B, \\quad q(x,s) = 0 \\text{ on } A, \\quad q(x,s) = 1 \\text{ on } B.\n$$\nSuppose we wish to estimate the rare-event probability $P=\\mathbb{P}(\\tau_B\\tau_A \\mid X_0\\in I_0)$ using a splitting strategy. Here $\\tau_B$ and $\\tau_A$ are the hitting times of $B$ and $A$, respectively; $X_t=(x_t,s_t)$ is the Markov state; and $I_0$ is an initial interface inside the domain between $A$ and $B$. A common naive choice of interfaces is to use threshold sets of the protein count alone, $I_\\ell = \\{(x,s): x = a_\\ell\\}$ with $a=a_0  a_1  \\dots  a_L=b$ for some integer $L\\ge 1$. Let the per-level success probabilities be $p_\\ell = \\mathbb{P}(\\tau_{I_{\\ell+1}}\\tau_A \\mid X_0 \\in I_\\ell)$ for $\\ell=0,1,\\dots,L-1$, so that the ideal factorization $P=\\prod_{\\ell=0}^{L-1}p_\\ell$ holds by the Markov property and the law of total probability.\n\nIn this gene network, because the promoter state $s$ strongly modulates the birth rate, the committor $q(x,s)$ can be non-monotone as a function of the protein count $x$ when marginalized over $s$. In particular, there may exist $x_1x_2$ such that $\\mathbb{E}[q(X,S)\\mid X=x_1] > \\mathbb{E}[q(X,S)\\mid X=x_2]$, where the expectation averages over the distribution of $S$ conditional on $X$ near the interfaces. This non-monotonicity leads to failure modes in splitting when interfaces are chosen as level sets of $x$: many cloned trajectories repeatedly cross interfaces without true progress toward $B$, creating genealogical correlation and inflating estimator variance.\n\nTo reason from first principles, recall that:\n- By the Markov property and the law of total probability, a splitting estimator $\\widehat{P}=\\prod_{\\ell=0}^{L-1}\\widehat{p}_\\ell$ with $\\widehat{p}_\\ell$ the empirical fraction of clones at level $\\ell$ that reach level $\\ell+1$ before $A$ is unbiased for $P$, i.e., $\\mathbb{E}[\\widehat{P}]=P$, regardless of the interface choice, provided each level’s sampling is unbiased and the interfaces form a nested sequence bridging $A$ to $B$.\n- Under independent Bernoulli sampling with $N_\\ell$ clones per level, $\\mathrm{Var}(\\widehat{p}_\\ell)=\\frac{p_\\ell(1-p_\\ell)}{N_\\ell}$, and a first-order delta-method approximation gives\n$$\n\\frac{\\mathrm{Var}(\\widehat{P})}{P^2} \\approx \\sum_{\\ell=0}^{L-1} \\frac{\\mathrm{Var}(\\widehat{p}_\\ell)}{p_\\ell^2} = \\sum_{\\ell=0}^{L-1} \\frac{1-p_\\ell}{p_\\ell\\,N_\\ell}.\n$$\nThis shows that, for a fixed total computational budget $\\sum_{\\ell}N_\\ell$, variance is reduced when the $p_\\ell$ are not extremely small and preferably comparable across $\\ell$.\n\nDiagnose the failure modes and design adaptive interfaces to restore estimator efficiency. Which of the following statements are correct?\n\nA. Estimating the committor $q$ locally by short unbiased trajectory bursts from states near the current interfaces, then redefining the interfaces as level sets of an estimated committor $\\hat{q}(x,s)$ (e.g., $\\{(x,s): \\hat{q}(x,s)=\\alpha_\\ell\\}$ for evenly spaced $\\alpha_\\ell \\in (0,1)$), approximately equalizes per-level success probabilities and mitigates backtracking, thereby reducing the variance of $\\widehat{P}$ without introducing bias.\n\nB. Increasing the number of interfaces $L$ along the naive protein-count coordinate $x$ cannot increase variance and will always improve the efficiency of the splitting estimator, because more granular thresholds necessarily make progress more monotone.\n\nC. The unbiasedness of $\\widehat{P}$ does not depend on the interface choice, as it follows from the Markov property and the law of total probability; however, for a fixed budget $\\sum_{\\ell}N_\\ell$, the variance is minimized when interfaces approximate iso-committor surfaces and the per-level success probabilities $p_\\ell$ are approximately equal, since this avoids levels with very small $p_\\ell$ and reduces genealogical correlations.\n\nD. Reweighting each trajectory segment by its residence time between interfaces eliminates the variance inflation caused by non-monotone committor, because slower segments receive larger weights that compensate for backtracking and restore monotonic progress.\n\nE. When promoter switching is slow, averaging over $s$ to use only the protein count $x$ makes the committor $\\mathbb{E}[q(X,S)\\mid X=x]$ monotone in $x$ in the long-time limit, so splitting on $x$ thresholds is always appropriate and cannot suffer from the non-monotonicity failure mode described.", "solution": "The user has requested a meticulous validation of the problem statement, followed by a full solution and evaluation of options.\n\n### Problem Validation\n\nFirst, I will validate the problem statement according to the specified criteria.\n\n#### Step 1: Extract Givens\n\n-   **System**: A stochastic gene expression network.\n-   **State**: The state is given by the pair $(x,s)$, where $s \\in \\{0, 1\\}$ is the promoter state (0 for OFF, 1 for ON) and $x \\in \\mathbb{N}$ is the protein copy number.\n-   **Dynamics**:\n    -   Promoter ON ($s=1$): protein birth rate $\\beta  0$, protein degradation rate $\\delta x$ with $\\delta  0$.\n    -   Promoter OFF ($s=0$): protein birth rate $\\epsilon \\approx 0$, protein degradation rate $\\delta x$.\n    -   Promoter switching rates: $0 \\to 1$ at rate $k_{\\mathrm{on}}  0$; $1 \\to 0$ at rate $k_{\\mathrm{off}}  0$.\n-   **Mathematical Framework**: The joint process $(x,s)$ is a Markov process governed by the Chemical Master Equation (CME).\n-   **State space partition**: A low-expression basin $A=\\{(x,s): x \\le a\\}$ and a high-expression basin $B=\\{(x,s): x \\ge b\\}$, with $ab$.\n-   **Committor Function**: $q(x,s)$ is the probability of hitting set $B$ before set $A$, starting from state $(x,s)$. It is the solution to the backward equation $\\mathcal{L} q(x,s) = 0$ for $(x,s) \\notin A \\cup B$, with boundary conditions $q(x,s) = 0$ on $A$ and $q(x,s) = 1$ on $B$. $\\mathcal{L}$ is the generator of the CME.\n-   **Simulation Problem**: Estimate the rare-event probability $P=\\mathbb{P}(\\tau_B\\tau_A \\mid X_0\\in I_0)$ using a splitting strategy, where $\\tau_A$ and $\\tau_B$ are hitting times of $A$ and $B$, and $I_0$ is an initial interface.\n-   **Interface Definition**: A naive choice for interfaces is $I_\\ell = \\{(x,s): x = a_\\ell\\}$ with $a=a_0a_1\\dotsa_L=b$.\n-   **Success Probability**: The per-level success probability is $p_\\ell = \\mathbb{P}(\\tau_{I_{\\ell+1}}\\tau_A \\mid X_0 \\in I_\\ell)$. The total probability is given by the product $P=\\prod_{\\ell=0}^{L-1}p_\\ell$.\n-   **Identified Issue**: The committor function may be non-monotone in $x$ when marginalized over $s$, i.e., there can exist $x_1x_2$ such that $\\mathbb{E}[q(X,S)\\mid X=x_1]\\mathbb{E}[q(X,S)\\mid X=x_2]$. This leads to variance inflation in the splitting estimator.\n-   **Given Formulas and Principles**:\n    1.  The splitting estimator $\\widehat{P}=\\prod_{\\ell=0}^{L-1}\\widehat{p}_\\ell$ is unbiased, i.e., $\\mathbb{E}[\\widehat{P}]=P$.\n    2.  The approximate relative variance of the estimator is $\\frac{\\mathrm{Var}(\\widehat{P})}{P^2} \\approx \\sum_{\\ell=0}^{L-1} \\frac{1-p_\\ell}{p_\\ell\\,N_\\ell}$, where $N_\\ell$ is the number of samples at level $\\ell$.\n\n#### Step 2: Validate Using Extracted Givens\n\n-   **Scientifically Grounded (Critical)**: The problem is perfectly grounded in computational systems biology and the theory of rare events in stochastic processes. The two-state model of gene expression is a canonical model in the field. The Chemical Master Equation is the correct framework. The committor function and splitting algorithms (like Forward Flux Sampling or Adaptive Multilevel Splitting) are standard, powerful tools for studying such systems. The issue of a non-monotone reaction coordinate and its detrimental effect on estimator efficiency is a well-documented and critical research topic. The problem is scientifically and mathematically sound.\n-   **Well-Posed**: The problem is well-posed. It presents a clear scenario and asks for a qualitative diagnosis of different proposed strategies. It does not require a numerical answer, but a conceptual evaluation, for which sufficient information is provided.\n-   **Objective (Critical)**: The description is objective, using precise terminology from mathematics and computational physics. It is free from subjective or ambiguous language.\n-   **Other Flaws**: The problem is self-contained, not contradictory, scientifically plausible, and addresses a non-trivial issue. It does not suffer from any of the listed invalidity criteria.\n\n#### Step 3: Verdict and Action\n\nThe problem statement is **valid**. I will proceed to derive the solution and evaluate the options.\n\n### Solution Derivation and Option Analysis\n\nThe central issue is the efficiency of the splitting estimator, which is determined by its variance. The provided formula for the relative variance, $\\frac{\\mathrm{Var}(\\widehat{P})}{P^2} \\approx \\sum_{\\ell=0}^{L-1} \\frac{1-p_\\ell}{p_\\ell N_\\ell}$, demonstrates a critical point: the variance is large if any of the per-level success probabilities $p_\\ell$ is very small. This is because a small $p_\\ell$ implies that a large number of trial trajectories ($N_\\ell$) are needed to obtain a statistically reliable estimate $\\widehat{p}_\\ell$. For a fixed total computational budget $\\sum_\\ell N_\\ell$, minimizing the variance involves making the $p_\\ell$ values as large and as uniform as possible.\n\nThe ideal reaction coordinate for a transition between two sets $A$ and $B$ is the committor function $q(x,s)$. By definition, a system's progress towards $B$ is perfectly measured by the value of $q(x,s)$. If interfaces are chosen as iso-committor surfaces, i.e., $I_\\ell = \\{(x,s): q(x,s) = \\alpha_\\ell\\}$ for a sequence $0  \\alpha_0  \\dots  \\alpha_L  1$, then by construction, any trajectory starting on $I_\\ell$ has a probability $\\alpha_\\ell$ of reaching $B$ before $A$. The probability of reaching $I_{\\ell+1}$ before $A$ (which is a superset of $I_0$) is given by properties of martingales, and leads to well-behaved $p_\\ell$. Choosing the $\\alpha_\\ell$ values appropriately (e.g., such that $\\alpha_{\\ell+1}/\\alpha_\\ell$ is constant) allows one to equalize the $p_\\ell$ values, which is optimal for variance reduction.\n\nThe problem arises because the naive coordinate, the protein count $x$, is a poor proxy for the true committor $q(x,s)$. The non-monotonicity of $\\mathbb{E}[q(X,S)|X=x]$ confirms this. A state with low protein count but an active promoter ($s=1$) might be \"closer\" to the high-expression state $B$ than a state with a higher protein count but an inactive promoter ($s=0$). Using level sets of $x$ as interfaces forces the algorithm to treat these states as being at the same level of progress, leading to frequent backtracking and low success probabilities $p_\\ell$.\n\nNow I shall evaluate each statement.\n\n**A. Estimating the committor $q$ locally by short unbiased trajectory bursts from states near the current interfaces, then redefining the interfaces as level sets of an estimated committor $\\hat{q}(x,s)$ (e.g., $\\{(x,s): \\hat{q}(x,s)=\\alpha_\\ell\\}$ for evenly spaced $\\alpha_\\ell \\in (0,1)$), approximately equalizes per-level success probabilities and mitigates backtracking, thereby reducing the variance of $\\widehat{P}$ without introducing bias.**\n\nThis statement describes the core idea of adaptive rare event simulation algorithms like Adaptive Multilevel Splitting (AMS).\n1.  **Estimating the committor**: The committor $q(x,s)$ at a point is the success probability from that point. It can be estimated using short trial trajectories (bursts).\n2.  **Redefining interfaces**: Using these estimates to define interfaces as level sets of the estimated committor, $\\hat{q}(x,s) = \\alpha_\\ell$, creates interfaces that approximate the true iso-committor surfaces.\n3.  **Equalizing success probabilities**: This choice of interfaces naturally leads to more uniform per-level success probabilities $p_\\ell$, as progress is now measured by a much better reaction coordinate.\n4.  **Mitigating backtracking**: Since the new coordinate $\\hat{q}$ is, by construction, monotonic along reactive paths, the issue of fruitless interface crossings is largely resolved.\n5.  **Reducing variance**: Points 3 and 4 directly lead to a reduction in the estimator variance for a fixed computational cost.\n6.  **Bias**: The procedure of adapting interfaces based on simulation data does not introduce bias into the final estimate $\\widehat{P}$, which is calculated in a subsequent run using the fixed, adapted interfaces. The unbiasedness property of splitting holds for any valid set of nested interfaces.\nThis statement is a complete and accurate description of a state-of-the-art solution to the problem.\n\n**Verdict**: **Correct**.\n\n**B. Increasing the number of interfaces $L$ along the naive protein-count coordinate $x$ cannot increase variance and will always improve the efficiency of the splitting estimator, because more granular thresholds necessarily make progress more monotone.**\n\nThis statement is incorrect on multiple counts.\n1.  **Effect on variance**: Placing interfaces closer together (increasing $L$) along a poor reaction coordinate like $x$ can drastically *increase* variance. If interfaces are at $x=a_\\ell$ and $x=a_\\ell+1$, a trajectory might cross from $a_\\ell$ to $a_\\ell+1$ but, if its promoter state is $s=0$, it is very likely to degrade back to $a_\\ell$ almost immediately. This makes the success probability $p_\\ell$ extremely small. The term $(1-p_\\ell)/(p_\\ell N_\\ell)$ in the variance sum can become very large, overwhelming any benefit from having more levels.\n2.  **Monotonicity**: Making thresholds more granular does not make progress more monotone. The underlying problem is that $x$ is not the correct measure of progress. Slicing a flawed metric more finely does not correct the flaw; it often amplifies its negative consequences by highlighting local, non-progressive fluctuations.\n\n**Verdict**: **Incorrect**.\n\n**C. The unbiasedness of $\\widehat{P}$ does not depend on the interface choice, as it follows from the Markov property and the law of total probability; however, for a fixed budget $\\sum_{\\ell}N_\\ell$, the variance is minimized when interfaces approximate iso-committor surfaces and the per-level success probabilities $p_\\ell$ are approximately equal, since this avoids levels with very small $p_\\ell$ and reduces genealogical correlations.**\n\nThis statement provides a correct and concise theoretical summary.\n1.  **Unbiasedness**: The first clause correctly states that the unbiasedness of the estimator $\\widehat{P}$ is a general property of the splitting method, independent of the quality of the interfaces (as long as they are properly nested). This is stated as a given in the problem text.\n2.  **Variance Minimization**: The second clause correctly identifies the dual goals for minimizing variance:\n    -   Using interfaces that approximate iso-committor surfaces. This makes the chosen reaction coordinate align with the true measure of progress, minimizing backtracking.\n    -   Making the $p_\\ell$ values approximately equal. This optimally distributes the sampling effort across levels, avoiding the variance explosion caused by any single level having a very small $p_\\ell$.\n    -   Reducing genealogical correlations. This is a crucial, more advanced point. When backtracking is frequent, a single \"lucky\" trajectory that makes progress might be the parent of all successful trajectories at the next stage. This lack of diversity in the genealogy of successful paths is a major source of variance, which is not captured by the simple delta-method formula. Good interfaces mitigate this by ensuring that progress is more deterministic, allowing many different trajectories to contribute to the final result.\nThis statement is entirely accurate.\n\n**Verdict**: **Correct**.\n\n**D. Reweighting each trajectory segment by its residence time between interfaces eliminates the variance inflation caused by non-monotone committor, because slower segments receive larger weights that compensate for backtracking and restore monotonic progress.**\n\nThis statement proposes an incorrect mechanism. While path reweighting is a valid concept in some advanced simulation methods (like Transition Path Sampling or some forms of Weighted Ensemble), the specific proposal here is flawed. A trajectory that is \"stuck\" backtracking between two interfaces will have a long residence time. Giving this \"stuck\" trajectory a larger weight would reward inefficiency and would likely increase, not decrease, variance. Efficient paths are typically the faster ones. The logic is inverted. This approach does not address the core problem, which is the poor sampling caused by bad interface placement. It attempts a post-hoc fix that is not theoretically sound.\n\n**Verdict**: **Incorrect**.\n\n**E. When promoter switching is slow, averaging over $s$ to use only the protein count $x$ makes the committor $\\mathbb{E}[q(X,S)\\mid X=x]$ monotone in $x$ in the long-time limit, so splitting on $x$ thresholds is always appropriate and cannot suffer from the non-monotonicity failure mode described.**\n\nThis statement has the causality backward. The failure mode described is *most pronounced* when promoter switching is slow.\n-   **Slow switching**: If $k_{\\mathrm{on}}$ and $k_{\\mathrm{off}}$ are small, the promoter state $s$ is a \"slow\" variable. A system with $s=1$ will produce many proteins before it has a chance to switch off. A system with $s=0$ will lose many proteins before it can switch on. Therefore, at the same protein count $x$, the state $(x,1)$ has a very high committor value $q(x,1) \\approx 1$ (if $x$ is not too small), while $(x,0)$ has a very low value $q(x,0) \\approx 0$. This large disparity is precisely what creates the non-monotonic behavior in the averaged committor. For instance, the system at $(x_1, 1)$ can easily be more likely to reach $B$ than the system at $(x_2, 0)$ for $x_1  x_2$.\n-   **Fast switching**: Conversely, if switching is very fast, the promoter state equilibrates rapidly. The system behaves as if it has a single, effective protein production rate. In this limit, the promoter state $s$ can be integrated out, and the protein count $x$ becomes a reasonably good reaction coordinate, leading to a monotonic committor.\nThe statement claims the opposite, so it is fundamentally flawed.\n\n**Verdict**: **Incorrect**.", "answer": "$$\\boxed{AC}$$", "id": "3343241"}]}