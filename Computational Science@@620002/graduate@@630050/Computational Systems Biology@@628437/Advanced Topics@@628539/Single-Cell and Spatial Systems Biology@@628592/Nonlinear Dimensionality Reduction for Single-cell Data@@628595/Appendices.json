{"hands_on_practices": [{"introduction": "Nonlinear dimensionality reduction algorithms are highly sensitive to the initial representation of data geometry, which is encoded in a pairwise affinity matrix. This practice explores how a seemingly simple preprocessing step, z-scoring gene expression values, can fundamentally alter these affinities and, consequently, the final embedding [@problem_id:3334321]. By computationally measuring the change in the eigenspectrum of the Gaussian affinity matrix, you will gain a rigorous, quantitative understanding of how scaling choices reshape the data's perceived structure before any embedding is even calculated.", "problem": "Consider a single-cell transcriptomic dataset represented as a matrix with $n$ cells and $p$ genes, where each cell $i$ is a point $x_i \\in \\mathbb{R}^p$. For any two cells $i$ and $j$, define the Gaussian affinity matrix entry by $W_{ij} = \\exp\\left(-\\|x_i - x_j\\|_2^2 / (2 \\sigma^2)\\right)$, where $\\| \\cdot \\|_2$ denotes the Euclidean norm and $\\sigma > 0$ is the kernel bandwidth parameter. Z-scoring each gene means subtracting the per-gene mean across cells and dividing by the per-gene standard deviation, resulting in standardized points $z_i \\in \\mathbb{R}^p$; if a gene has zero variance across cells, define its standardized values to be $0$ for all cells. Construct the Gaussian affinity matrix $W^{(0)}$ using the original data $\\{x_i\\}$ and the matrix $W^{(z)}$ using the standardized data $\\{z_i\\}$. The spectrum of an affinity matrix refers to its multiset of eigenvalues. For a symmetric affinity matrix such as $W^{(0)}$ or $W^{(z)}$, all eigenvalues are real. Define the spectral change magnitude for a given dataset and bandwidth by\n$$\nS = \\left\\| \\lambda\\!\\left(W^{(z)}\\right) - \\lambda\\!\\left(W^{(0)}\\right) \\right\\|_2,\n$$\nwhere $\\lambda(\\cdot)$ denotes the vector of eigenvalues sorted in descending order and $\\|\\cdot\\|_2$ is the Euclidean norm.\n\nStarting only from the definitions of z-scoring, Euclidean distance, the Gaussian kernel, and basic facts about real symmetric matrices and their eigenvalues, design and implement a program that computes $S$ for each of the specified test cases below. Your program must use the exact datasets and bandwidths provided, apply the zero-variance z-scoring rule for genes with zero variance, compute the affinity matrices, obtain their eigenvalues, sort them in descending order, and compute $S$ as specified. Each reported $S$ must be rounded to $6$ decimal places.\n\nTest suite:\n- Test case $1$ (happy path, moderate bandwidth): Use dataset $X^{(A)} \\in \\mathbb{R}^{5 \\times 3}$ with rows given by $x_1 = (2.0, 5.0, 8.0)$, $x_2 = (3.0, 6.0, 7.5)$, $x_3 = (4.0, 6.5, 7.0)$, $x_4 = (5.0, 7.0, 6.0)$, $x_5 = (7.0, 7.5, 5.0)$ and bandwidth $\\sigma = 1.0$.\n- Test case $2$ (small bandwidth boundary): Use dataset $X^{(A)}$ as above with bandwidth $\\sigma = 0.1$.\n- Test case $3$ (large bandwidth boundary): Use dataset $X^{(A)}$ as above with bandwidth $\\sigma = 100.0$.\n- Test case $4$ (zero-variance gene edge case): Use dataset $X^{(B)} \\in \\mathbb{R}^{5 \\times 3}$ with rows given by $x_1 = (0.0, 0.0, 10.0)$, $x_2 = (10.0, 0.0, 10.0)$, $x_3 = (20.0, 0.0, 10.0)$, $x_4 = (30.0, 0.0, 10.0)$, $x_5 = (40.0, 0.0, 10.0)$ and bandwidth $\\sigma = 5.0$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, ordered as $[S_1,S_2,S_3,S_4]$, where $S_k$ is the rounded spectral change magnitude for test case $k$. The outputs are dimensionless real numbers, and must be floats rounded to $6$ decimal places.", "solution": "The task is to quantify the effect of z-score standardization on the eigenspectrum of a Gaussian affinity matrix derived from single-cell data. We are given the precise mathematical definitions for the data transformation, the affinity matrix construction, and the metric for spectral change. The problem is a well-posed computational exercise grounded in the principles of linear algebra, statistics, and computational biology.\n\nThe overall procedure involves several distinct, sequential steps:\n1. For a given dataset $X$ and bandwidth $\\sigma$, construct the Gaussian affinity matrix $W^{(0)}$.\n2. Compute and sort the eigenvalues of $W^{(0)}$.\n3. Apply z-score standardization to the dataset $X$ to obtain a new dataset $Z$, handling genes with zero variance as specified.\n4. Construct the Gaussian affinity matrix $W^{(z)}$ from the standardized data $Z$ using the same bandwidth $\\sigma$.\n5. Compute and sort the eigenvalues of $W^{(z)}$.\n6. Calculate the Euclidean distance between the two sorted eigenvalue vectors to find the spectral change magnitude $S$.\n\nWe will now elaborate on the mathematical and conceptual underpinnings of each step.\n\nA single-cell dataset is represented as a matrix $X \\in \\mathbb{R}^{n \\times p}$, where $n$ is the number of cells and $p$ is the number of genes. Each cell $i$ corresponds to a row vector $x_i \\in \\mathbb{R}^p$.\n\n**1. Z-Score Standardization**\n\nZ-scoring is a common preprocessing technique used to render features (genes) comparable by transforming them to a common scale. It rescales each gene's expression values to have a mean of $0$ and a standard deviation of $1$.\n\nFor each gene $j \\in \\{1, \\dots, p\\}$, we first compute its mean $\\mu_j$ and population standard deviation $s_j$ across all $n$ cells:\n$$\n\\mu_j = \\frac{1}{n} \\sum_{i=1}^n x_{ij}\n$$\n$$\ns_j = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (x_{ij} - \\mu_j)^2}\n$$\nThe original data point $x_i = (x_{i1}, \\dots, x_{ip})$ is transformed into a standardized point $z_i = (z_{i1}, \\dots, z_{ip})$. If a gene $j$ has non-zero variance (i.e., $s_j > 0$), its standardized value for cell $i$ is:\n$$\nz_{ij} = \\frac{x_{ij} - \\mu_j}{s_j}\n$$\nThe problem specifies a critical edge case: if a gene $j$ has zero variance ($s_j = 0$), its standardized value is defined to be $0$ for all cells, i.e., $z_{ij} = 0$ for all $i \\in \\{1, \\dots, n\\}$. This rule prevents division by zero and effectively removes the contribution of constant-expression genes from distance calculations in the standardized space.\n\n**2. Gaussian Affinity Matrix Construction**\n\nThe relationship between cells is captured by an affinity matrix, which measures pairwise similarity. The Gaussian kernel is a standard choice for this purpose. The affinity between two cells $i$ and $j$ with data vectors $u_i$ and $u_j$ (where $u$ can be the original data $x$ or standardized data $z$) is defined as:\n$$\nW_{ij} = \\exp\\left(-\\frac{\\|u_i - u_j\\|_2^2}{2 \\sigma^2}\\right)\n$$\nHere, $\\|u_i - u_j\\|_2^2$ is the squared Euclidean distance between the two cell vectors and $\\sigma > 0$ is the kernel bandwidth parameter, which controls the width of the Gaussian and thus the scale of the neighborhood. A smaller $\\sigma$ leads to a more local, sparse notion of similarity, while a larger $\\sigma$ considers broader relationships.\n\nWe construct two such matrices:\n- $W^{(0)}$, using the original data points $\\{x_i\\}$.\n- $W^{(z)}$, using the z-scored data points $\\{z_i\\}$.\n\nBy construction, these matrices are real and symmetric ($W_{ij} = W_{ji}$), since the Euclidean distance is symmetric. The diagonal elements are always $W_{ii} = \\exp(0) = 1$.\n\n**3. Spectral Analysis**\n\nThe spectrum of a matrix is its multiset of eigenvalues. A fundamental theorem of linear algebra states that any real symmetric matrix, such as our affinity matrices $W^{(0)}$ and $W^{(z)}$, is diagonalizable and has only real eigenvalues. These eigenvalues provide profound insight into the structure of the data graph represented by the affinity matrix.\n\nWe compute the eigenvalues for both $W^{(0)}$ and $W^{(z)}$. Let these multisets of eigenvalues be $\\{\\lambda_k^{(0)}\\}_{k=1}^n$ and $\\{\\lambda_k^{(z)}\\}_{k=1}^n$. To enable a meaningful comparison, we sort them in descending order to form the eigenvalue vectors:\n$$\n\\lambda\\left(W^{(0)}\\right) = \\left(\\lambda_1^{(0)}, \\lambda_2^{(0)}, \\dots, \\lambda_n^{(0)}\\right) \\text{ where } \\lambda_1^{(0)} \\ge \\lambda_2^{(0)} \\ge \\dots \\ge \\lambda_n^{(0)}\n$$\n$$\n\\lambda\\left(W^{(z)}\\right) = \\left(\\lambda_1^{(z)}, \\lambda_2^{(z)}, \\dots, \\lambda_n^{(z)}\\right) \\text{ where } \\lambda_1^{(z)} \\ge \\lambda_2^{(z)} \\ge \\dots \\ge \\lambda_n^{(z)}\n$$\n\n**4. Spectral Change Magnitude Calculation**\n\nThe final step is to quantify the total change in the spectrum caused by the z-scoring transformation. The problem defines the spectral change magnitude $S$ as the Euclidean ($L_2$) norm of the difference between the two sorted eigenvalue vectors:\n$$\nS = \\left\\| \\lambda\\!\\left(W^{(z)}\\right) - \\lambda\\!\\left(W^{(0)}\\right) \\right\\|_2 = \\sqrt{\\sum_{k=1}^n \\left(\\lambda_k^{(z)} - \\lambda_k^{(0)}\\right)^2}\n$$\nThis metric provides a single scalar value summarizing the extent to which z-scoring has altered the geometric structure of the data as captured by the spectrum of the Gaussian affinity matrix.\n\nThe implementation will follow these steps precisely for each test case, using robust numerical libraries to perform the matrix and vector operations.", "answer": "```python\nimport numpy as np\nfrom scipy.spatial.distance import pdist, squareform\n\ndef calculate_spectral_change(X: np.ndarray, sigma: float) -> float:\n    \"\"\"\n    Computes the spectral change magnitude S for a given dataset and bandwidth.\n\n    The function performs the following steps:\n    1. Computes the Gaussian affinity matrix W^(0) from the original data X.\n    2. Computes the sorted eigenvalues of W^(0).\n    3. Standardizes (z-scores) the data X to get Z, handling zero-variance genes.\n    4. Computes the Gaussian affinity matrix W^(z) from the standardized data Z.\n    5. Computes the sorted eigenvalues of W^(z).\n    6. Calculates the Euclidean norm of the difference between the two eigenvalue vectors.\n\n    Args:\n        X: A numpy array of shape (n, p) representing n cells and p genes.\n        sigma: The Gaussian kernel bandwidth parameter.\n\n    Returns:\n        The spectral change magnitude S.\n    \"\"\"\n    # 1. Compute affinity matrix W^(0) from original data X\n    # The 'sqeuclidean' metric computes the squared Euclidean distance.\n    dist_sq_0 = squareform(pdist(X, 'sqeuclidean'))\n    W0 = np.exp(-dist_sq_0 / (2 * sigma**2))\n\n    # 2. Compute sorted eigenvalues of W^(0)\n    # np.linalg.eigh is specialized for Hermitian (symmetric for real matrices)\n    # and returns eigenvalues in ascending order. We reverse them.\n    eigvals_0 = np.linalg.eigh(W0)[0][::-1]\n\n    # 3. Z-score the data X to get Z\n    # We use population standard deviation (ddof=0 is the default in np.std)\n    means = np.mean(X, axis=0)\n    stds = np.std(X, axis=0)\n\n    # Initialize standardized matrix with zeros. This correctly handles the\n    # zero-variance case where standardized values must be 0.\n    Z = np.zeros_like(X, dtype=float)\n    \n    # Create a boolean mask for columns with non-zero standard deviation\n    non_zero_std_mask = stds > 1e-15 # Use a small tolerance for floating point safety\n    \n    # Apply standardization only to columns with non-zero standard deviation\n    if np.any(non_zero_std_mask):\n        Z[:, non_zero_std_mask] = (X[:, non_zero_std_mask] - means[non_zero_std_mask]) / stds[non_zero_std_mask]\n\n    # 4. Compute affinity matrix W^(z) from standardized data Z\n    dist_sq_z = squareform(pdist(Z, 'sqeuclidean'))\n    Wz = np.exp(-dist_sq_z / (2 * sigma**2))\n    \n    # 5. Compute sorted eigenvalues of W^(z)\n    eigvals_z = np.linalg.eigh(Wz)[0][::-1]\n\n    # 6. Calculate spectral change magnitude S\n    S = np.linalg.norm(eigvals_z - eigvals_0)\n\n    return S\n\ndef solve():\n    \"\"\"\n    Main function to run the specified test cases and print the results.\n    \"\"\"\n    # Test case 1: Happy path, moderate bandwidth\n    X_A = np.array([\n        [2.0, 5.0, 8.0],\n        [3.0, 6.0, 7.5],\n        [4.0, 6.5, 7.0],\n        [5.0, 7.0, 6.0],\n        [7.0, 7.5, 5.0]\n    ])\n    sigma_1 = 1.0\n\n    # Test case 2: Small bandwidth boundary\n    sigma_2 = 0.1\n\n    # Test case 3: Large bandwidth boundary\n    sigma_3 = 100.0\n\n    # Test case 4: Zero-variance gene edge case\n    X_B = np.array([\n        [0.0, 0.0, 10.0],\n        [10.0, 0.0, 10.0],\n        [20.0, 0.0, 10.0],\n        [30.0, 0.0, 10.0],\n        [40.0, 0.0, 10.0]\n    ])\n    sigma_4 = 5.0\n\n    test_cases = [\n        (X_A, sigma_1),\n        (X_A, sigma_2),\n        (X_A, sigma_3),\n        (X_B, sigma_4)\n    ]\n\n    results = []\n    for X, sigma in test_cases:\n        S = calculate_spectral_change(X, sigma)\n        # Format the result to 6 decimal places as a string\n        results.append(f\"{S:.6f}\")\n\n    # Print the final output in the specified format\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3334321"}, {"introduction": "Modern single-cell datasets often contain hundreds of thousands of cells, which would require terabytes of memory if affinity matrices were stored densely. This exercise addresses the critical issue of scalability by demonstrating the necessity of sparse matrix representations, which form the computational backbone of algorithms like UMAP and t-SNE [@problem_id:3334326]. By calculating the memory savings from first principles, you will appreciate how a shift from a quadratic $O(N^2)$ to a near-linear $O(Nk)$ memory requirement makes large-scale single-cell analysis computationally feasible.", "problem": "In nonlinear dimensionality reduction applied to single-cell transcriptomic data, algorithms such as t-distributed Stochastic Neighbor Embedding (t-SNE) and Uniform Manifold Approximation and Projection (UMAP) construct a $k$-nearest neighbor graph to capture local relationships among cells. Let the data set contain $N$ cells. Define a weight matrix $W \\in \\mathbb{R}^{N \\times N}$, where $W_{ij}$ encodes the similarity between cell $i$ and cell $j$, and a high-dimensional affinity matrix $P \\in \\mathbb{R}^{N \\times N}$, where $P_{ij}$ represents the probability-like affinity between cells $i$ and $j$. In practice, to reflect the locality constraint of the $k$-nearest neighbor graph, each row of $W$ and $P$ contains only the $k$ nonzero entries corresponding to the nearest neighbors of that row’s cell, with all other entries equal to zero, yielding sparse matrices.\n\nYou will compare two storage strategies for simultaneously holding both $W$ and $P$ during the embedding:\n1. Dense storage: store both $W$ and $P$ as full $N \\times N$ arrays of $64$-bit floating-point values.\n2. Sparse storage: store both $W$ and $P$ using Compressed Sparse Row (CSR) format, with each nonzero entry stored as a $64$-bit floating-point value and its column index as a $32$-bit integer, and the row pointer array (“indptr”) as $32$-bit integers of length $N+1$. Assume each row has exactly $k$ nonzero entries.\n\nStarting from first principles of sparse representation and memory accounting, show how sparse representations reduce the memory footprint relative to dense storage. Then, for $N = 100{,}000$ cells and $k = 15$, compute the total memory savings of using sparse CSR representations for both $W$ and $P$ compared to dense storage of both matrices. Express the final memory savings in gigabytes (GB), using the convention that $1 \\, \\mathrm{GB} = 10^9$ bytes. Round your answer to four significant figures.", "solution": "The problem is subjected to validation according to the established criteria.\n\n### Step 1: Extract Givens\n- Data set size: $N$ cells.\n- Matrices: Weight matrix $W \\in \\mathbb{R}^{N \\times N}$ and affinity matrix $P \\in \\mathbb{R}^{N \\times N}$.\n- Sparsity: Each row of $W$ and $P$ has exactly $k$ nonzero entries.\n- Storage Strategy 1 (Dense): Both $W$ and $P$ are stored as full $N \\times N$ arrays of $64$-bit floating-point values.\n- Storage Strategy 2 (Sparse CSR):\n    - Format: Compressed Sparse Row (CSR).\n    - Nonzero values: $64$-bit floating-point numbers.\n    - Column indices: $32$-bit integers.\n    - Row pointer array (`indptr`): Array of length $N+1$ using $32$-bit integers.\n- Numerical values:\n    - $N = 100,000$.\n    - $k = 15$.\n- Unit conversion: $1 \\, \\mathrm{GB} = 10^9$ bytes.\n- Output requirement: Total memory savings in GB, rounded to four significant figures.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, as it describes a standard and critical computational consideration in the analysis of large-scale single-cell data using established algorithms like t-SNE and UMAP. The concept of using sparse matrix representations to handle the $k$-nearest neighbor graph is fundamental to the scalability of these methods. The problem is well-posed, providing all necessary parameters ($N$, $k$, data type bit-depths) to perform the calculation. The language is objective and precise. The problem does not violate any of the invalidity criteria; it is complete, consistent, realistic, and formalizable.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\n### Solution Derivation\nThe problem requires a comparison of memory usage between dense and sparse storage for two $N \\times N$ matrices, $W$ and $P$, followed by a specific numerical calculation. We begin by deriving the memory cost for each storage strategy from first principles.\n\nFirst, let us define the memory size of the primitive data types in bytes.\n- A $64$-bit floating-point value requires $\\frac{64}{8} = 8$ bytes.\n- A $32$-bit integer requires $\\frac{32}{8} = 4$ bytes.\n\n**1. Dense Storage Memory Calculation**\n\nIn a dense storage format, every element of the matrix is stored explicitly, regardless of its value.\n- An $N \\times N$ matrix contains $N^2$ elements.\n- Each element is a $64$-bit float, consuming $8$ bytes.\n- The total memory required to store a single dense matrix, $M_{\\text{dense, single}}$, is:\n$$ M_{\\text{dense, single}} = N^2 \\times 8 \\text{ bytes} $$\n- Since we need to store both matrices $W$ and $P$, the total memory for dense storage, $M_{\\text{dense, total}}$, is:\n$$ M_{\\text{dense, total}} = 2 \\times M_{\\text{dense, single}} = 2 \\times 8 N^2 = 16 N^2 \\text{ bytes} $$\n\n**2. Sparse Storage (CSR) Memory Calculation**\n\nThe Compressed Sparse Row (CSR) format stores only the nonzero elements and their positions. It comprises three arrays:\n- `data`: Stores the values of the nonzero elements.\n- `indices`: Stores the column index for each corresponding value in `data`.\n- `indptr`: An array of size $N+1$ where `indptr[i]` points to the start of row `i` in the `data` and `indices` arrays.\n\nThe problem states that each of the $N$ rows has exactly $k$ nonzero entries.\n- Total number of nonzero entries in one matrix is $N \\times k$.\n- Memory for the `data` array (storing $Nk$ floating-point values):\n$$ M_{\\text{data}} = (N \\times k) \\times 8 \\text{ bytes} $$\n- Memory for the `indices` array (storing $Nk$ integer column indices):\n$$ M_{\\text{indices}} = (N \\times k) \\times 4 \\text{ bytes} $$\n- Memory for the `indptr` array (storing $N+1$ integer pointers):\n$$ M_{\\text{indptr}} = (N+1) \\times 4 \\text{ bytes} $$\n- The total memory for a single matrix in CSR format, $M_{\\text{sparse, single}}$, is the sum of these components:\n$$ M_{\\text{sparse, single}} = M_{\\text{data}} + M_{\\text{indices}} + M_{\\text{indptr}} = 8Nk + 4Nk + 4(N+1) = 12Nk + 4N + 4 \\text{ bytes} $$\n- For both matrices $W$ and $P$, the total memory for sparse storage, $M_{\\text{sparse, total}}$, is:\n$$ M_{\\text{sparse, total}} = 2 \\times M_{\\text{sparse, single}} = 2(12Nk + 4N + 4) = 24Nk + 8N + 8 \\text{ bytes} $$\n\n**3. Conceptual Comparison and Justification for Sparse Storage**\n\nSparse storage reduces the memory footprint when $M_{\\text{sparse, single}} < M_{\\text{dense, single}}$. This inequality is:\n$$ 12Nk + 4N + 4 < 8N^2 $$\nFor large $N$, the lower-order terms $4N$ and $4$ are negligible. The condition approximates to:\n$$ 12Nk < 8N^2 \\implies 12k < 8N \\implies k < \\frac{2}{3}N $$\nIn single-cell analysis, the number of nearest neighbors $k$ (typically $10-100$) is vastly smaller than the number of cells $N$ (typically $10^4-10^6$). The condition $k \\ll N$ ensures that $k < \\frac{2}{3}N$ is satisfied by many orders of magnitude. The memory requirement for dense storage scales quadratically with $N$ (as $O(N^2)$), while for sparse storage it scales linearly with $N$ (as $O(Nk)$). This difference in scaling makes dense storage computationally intractable for large datasets, whereas sparse storage remains feasible.\n\n**4. Numerical Calculation of Memory Savings**\n\nThe total memory savings, $\\Delta M$, is the difference between the total dense storage memory and the total sparse storage memory.\n$$ \\Delta M = M_{\\text{dense, total}} - M_{\\text{sparse, total}} $$\n$$ \\Delta M = 16N^2 - (24Nk + 8N + 8) \\text{ bytes} $$\n\nNow, we substitute the given values $N = 100,000 = 10^5$ and $k=15$.\n\n- Calculate total dense memory:\n$$ M_{\\text{dense, total}} = 16 \\times (10^5)^2 = 16 \\times 10^{10} \\text{ bytes} $$\n\n- Calculate total sparse memory:\n$$ M_{\\text{sparse, total}} = 24 \\times (10^5) \\times 15 + 8 \\times (10^5) + 8 $$\n$$ M_{\\text{sparse, total}} = 360 \\times 10^5 + 8 \\times 10^5 + 8 $$\n$$ M_{\\text{sparse, total}} = (360 + 8) \\times 10^5 + 8 = 368 \\times 10^5 + 8 = 36,800,000 + 8 = 36,800,008 \\text{ bytes} $$\n\n- Calculate the memory savings in bytes:\n$$ \\Delta M = 160,000,000,000 - 36,800,008 = 159,963,199,992 \\text{ bytes} $$\n\nFinally, we convert the savings to gigabytes (GB), using the convention $1 \\, \\mathrm{GB} = 10^9$ bytes.\n$$ \\Delta M_{\\text{GB}} = \\frac{159,963,199,992}{10^9} = 159.963199992 \\,\\mathrm{GB} $$\n\nThe problem asks to round the answer to four significant figures. The value $159.963...$ rounded to four significant figures is $160.0$. The trailing zero is significant in this context.", "answer": "$$\\boxed{160.0}$$", "id": "3334326"}, {"introduction": "The frontier of single-cell biology increasingly involves integrating information from multiple data types, such as gene expression and spatial location. This advanced practice moves beyond using off-the-shelf tools to designing a novel embedding method from the ground up [@problem_id:3334340]. By deriving the gradient for a custom objective function and implementing a gradient descent optimizer, you will learn the principles of multi-modal data fusion, creating a unified representation that simultaneously respects the structures present in both scRNA-seq and spatial transcriptomics data.", "problem": "You are given two modalities measured on the same set of single cells: droplet-based single-cell RNA sequencing (scRNA-seq) and spatial transcriptomics. The scRNA-seq modality provides a high-dimensional expression profile for each cell, while the spatial modality provides two-dimensional coordinates locating each cell in tissue. The goal is to derive and implement a principle-based nonlinear dimensionality reduction that aligns an embedding of the scRNA-seq data with spatial distances, by optimizing a combined loss over pairwise distances.\n\nStarting from the following fundamental definitions and well-tested facts:\n\n- The Central Dogma of Molecular Biology (CDMB) asserts that genetic information flows from DNA to RNA to protein. For single-cell RNA sequencing, messenger RNA abundance is an informative proxy of cellular state. This motivates the use of pairwise distances between expression profiles as a structural descriptor of cellular state space. We will restrict to pairwise Euclidean distances computed on standardized features as a widely accepted baseline descriptor.\n\n- For any points $\\{x_i\\}_{i=1}^N$ in $\\mathbb{R}^m$, the Euclidean distance is defined as $d(x_i,x_j) = \\|x_i - x_j\\|_2$, where $\\|\\cdot\\|_2$ denotes the $\\ell_2$ norm. For spatial coordinates $\\{s_i\\}_{i=1}^N$ in $\\mathbb{R}^2$, the spatial distance is $d(s_i,s_j) = \\|s_i - s_j\\|_2$.\n\n- In metric Multidimensional Scaling (MDS), an embedding $\\{y_i\\}_{i=1}^N$ in $\\mathbb{R}^p$ is learned by minimizing a stress objective that penalizes deviations between embedding distances and a target distance matrix. Here, we extend this principle to simultaneously align to two target distance matrices, one derived from scRNA-seq expression and one from spatial coordinates, using a convex combination of penalties.\n\nFormulate and implement the following alignment objective. Let $D^{(\\text{expr})}_{ij}$ denote the Euclidean distances computed from the scRNA-seq expression profiles, and $D^{(\\text{spatial})}_{ij}$ denote the Euclidean distances computed from the spatial coordinates. Let the embedding be $Y \\in \\mathbb{R}^{N \\times p}$ with rows $y_i \\in \\mathbb{R}^p$ and pairwise embedding distances $R_{ij} = \\|y_i - y_j\\|_2$. Define the combined loss\n$$\n\\mathcal{L}(Y;\\alpha) \\;=\\; \\sum_{1 \\le i < j \\le N} \\left[ \\alpha \\left(R_{ij} - D^{(\\text{expr})}_{ij}\\right)^2 \\;+\\; (1 - \\alpha)\\left(R_{ij} - D^{(\\text{spatial})}_{ij}\\right)^2 \\right],\n$$\nwhere $\\alpha \\in [0,1]$ is a modality weight that interpolates between purely expression-driven alignment ($\\alpha = 1$) and purely spatial alignment ($\\alpha = 0$). To ensure commensurability of distances across modalities, you must normalize each target distance matrix by dividing by its mean over all nonzero upper-triangular entries.\n\nYour tasks:\n\n- Derive from first principles the gradient of $\\mathcal{L}(Y;\\alpha)$ with respect to the embedding coordinates $Y$, starting from the definition of Euclidean distance and the chain rule of differentiation. Explicitly handle the case $R_{ij} = 0$ by a limiting argument that yields a numerically stable update.\n\n- Design an algorithm that minimizes $\\mathcal{L}(Y;\\alpha)$ via iterative gradient descent on $Y$, with the following design constraints and justifications:\n  - Initialize $Y$ using the spatial coordinates scaled so that the mean of the nonzero upper-triangular embedding distances matches $1$, which is the post-normalization mean of the target distances.\n  - Recenter $Y$ at each iteration to have zero mean across cells to avoid drift.\n  - Use a fixed step size chosen to ensure convergence for the provided test suite, and stop either after a fixed number of iterations or when the absolute change in $\\mathcal{L}(Y;\\alpha)$ over the last $20$ iterations falls below a small threshold.\n  - Ensure numerical stability by replacing any division by $R_{ij}$ with division by $\\max(R_{ij}, \\varepsilon)$ for a small $\\varepsilon > 0$.\n\n- Implement the algorithm in a single, complete program that runs without input and produces the required outputs for the specified test suite.\n\nTest suite specification:\n\nLet the number of cells be $N = 6$ and the embedding dimension be $p = 2$ for all test cases. Spatial coordinates and scRNA-seq expression features are fixed per test case as follows.\n\nFor Test Cases $1$ through $3$, use the same spatial coordinates and expression features:\n\nSpatial coordinates matrix $S \\in \\mathbb{R}^{6 \\times 2}$:\n$$\nS \\;=\\; \\begin{bmatrix}\n0.0 & 0.0 \\\\\n1.0 & 0.0 \\\\\n2.0 & 0.0 \\\\\n0.0 & 1.0 \\\\\n1.0 & 1.0 \\\\\n2.0 & 1.0\n\\end{bmatrix}.\n$$\n\nExpression feature matrix $X \\in \\mathbb{R}^{6 \\times 3}$ with rows defined by\n$x_i^{(1)} = s_{i,1} + \\delta_i^{(1)}$,\n$x_i^{(2)} = s_{i,2} + \\delta_i^{(2)}$,\n$x_i^{(3)} = 0.5 s_{i,1} + 0.5 s_{i,2} + \\delta_i^{(3)}$,\nwhere the spatial coordinates $s_{i,1}$ and $s_{i,2}$ are the entries of the corresponding row of $S$, and the deterministic perturbations are\n$$\n\\delta^{(1)} \\;=\\; \\begin{bmatrix} 0.00 & 0.05 & -0.05 & 0.025 & -0.025 & 0.00 \\end{bmatrix}^\\top, \\quad\n\\delta^{(2)} \\;=\\; \\begin{bmatrix} 0.01 & -0.01 & 0.015 & -0.015 & 0.00 & 0.02 \\end{bmatrix}^\\top, \\quad\n\\delta^{(3)} \\;=\\; \\begin{bmatrix} 0.02 & 0.00 & -0.02 & 0.03 & -0.03 & 0.01 \\end{bmatrix}^\\top.\n$$\nTest Case $1$: use $\\alpha = 0.5$.\n\nTest Case $2$: use $\\alpha = 1.0$.\n\nTest Case $3$: use $\\alpha = 0.0$.\n\nFor Test Case $4$, use the same spatial coordinates $S$ but scale the expression features by a factor of $3.0$, i.e., use $X^{(\\text{scaled})} = 3.0 \\cdot X$, and set $\\alpha = 0.3$.\n\nFor Test Case $5$, modify the spatial coordinates by duplicating the last point with the fifth point to induce zero spatial distance for one pair:\n$$\nS^{(\\text{dup})} \\;=\\; \\begin{bmatrix}\n0.0 & 0.0 \\\\\n1.0 & 0.0 \\\\\n2.0 & 0.0 \\\\\n0.0 & 1.0 \\\\\n1.0 & 1.0 \\\\\n1.0 & 1.0\n\\end{bmatrix},\n$$\nuse the original expression features $X$, and set $\\alpha = 0.5$.\n\nDistance computation and normalization:\n\n- For each test case, compute $D^{(\\text{expr})}$ as the pairwise Euclidean distances on the rows of the relevant expression matrix ($X$ or $X^{(\\text{scaled})}$) and $D^{(\\text{spatial})}$ as the pairwise Euclidean distances on the rows of the relevant spatial matrix ($S$ or $S^{(\\text{dup})}$).\n\n- Normalize each of $D^{(\\text{expr})}$ and $D^{(\\text{spatial})}$ by dividing by the mean of the nonzero upper-triangular entries, so the post-normalization mean of nonzero distances is $1$.\n\nAlgorithm hyperparameters:\n\n- Use a small positive constant $\\varepsilon = 10^{-8}$ for numerical stability in divisions by $R_{ij}$.\n\n- Use a fixed step size $\\eta = 0.05$.\n\n- Use a maximum of $1000$ iterations, with early stopping if the absolute change in $\\mathcal{L}(Y;\\alpha)$ over the last $20$ iterations is below $10^{-9}$.\n\nFinal output format:\n\nYour program should produce a single line of output containing the results for the five test cases as a comma-separated list enclosed in square brackets, in the order of Test Cases $1$ through $5$, where each result is the final value of $\\mathcal{L}(Y;\\alpha)$ as a floating-point number (in dimensionless units). For example, the output must be of the form\n$$\n[\\ell_1,\\ell_2,\\ell_3,\\ell_4,\\ell_5],\n$$\nwhere $\\ell_k$ denotes the final loss for Test Case $k$.", "solution": "The problem of aligning an embedding with both single-cell expression data and spatial coordinates is formulated as the minimization of a combined stress-like loss function. The solution requires deriving the gradient of this loss function and implementing an iterative gradient descent algorithm to find the optimal low-dimensional embedding.\n\n### 1. Gradient Derivation\n\nThe combined loss function $\\mathcal{L}(Y;\\alpha)$ is defined as a sum over all unique pairs of cells $(i,j)$ with $1 \\le i < j \\le N$:\n$$\n\\mathcal{L}(Y;\\alpha) \\;=\\; \\sum_{1 \\le i < j \\le N} \\left[ \\alpha \\left(R_{ij} - D^{(\\text{expr})}_{ij}\\right)^2 \\;+\\; (1 - \\alpha)\\left(R_{ij} - D^{(\\text{spatial})}_{ij}\\right)^2 \\right]\n$$\nHere, $Y \\in \\mathbb{R}^{N \\times p}$ is the matrix of embedding coordinates $y_i \\in \\mathbb{R}^p$, $R_{ij} = \\|y_i - y_j\\|_2$ is the Euclidean distance between points $i$ and $j$ in the embedding, $D^{(\\text{expr})}_{ij}$ and $D^{(\\text{spatial})}_{ij}$ are the target distances from the expression and spatial modalities, respectively, and $\\alpha \\in [0,1]$ is a weighting parameter. For simplicity, we define a weighted target distance $W_{ij} = \\alpha D^{(\\text{expr})}_{ij} + (1 - \\alpha)D^{(\\text{spatial})}_{ij}$. The loss term for a single pair $(i,j)$ can be rewritten by expanding the squares:\n$$\n\\mathcal{L}_{ij} = \\alpha(R_{ij}^2 - 2R_{ij}D^{(\\text{expr})}_{ij} + (D^{(\\text{expr})}_{ij})^2) + (1-\\alpha)(R_{ij}^2 - 2R_{ij}D^{(\\text{spatial})}_{ij} + (D^{(\\text{spatial})}_{ij})^2)\n$$\n$$\n\\mathcal{L}_{ij} = R_{ij}^2 - 2R_{ij}(\\alpha D^{(\\text{expr})}_{ij} + (1-\\alpha)D^{(\\text{spatial})}_{ij}) + \\text{const} = R_{ij}^2 - 2R_{ij}W_{ij} + \\text{const}\n$$\nThe full loss is $\\mathcal{L} = \\sum_{1 \\le i < j \\le N} \\mathcal{L}_{ij}$. We need to compute the gradient with respect to the coordinates of a single cell $k$, which is the vector $\\frac{\\partial \\mathcal{L}}{\\partial y_k}$. A coordinate vector $y_k$ appears in any term $\\mathcal{L}_{ij}$ where $i=k$ or $j=k$. The total gradient for $y_k$ is the sum of partial derivatives from these terms:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial y_k} = \\sum_{j > k} \\frac{\\partial \\mathcal{L}_{kj}}{\\partial y_k} + \\sum_{i < k} \\frac{\\partial \\mathcal{L}_{ik}}{\\partial y_k}\n$$\nWe apply the chain rule, $\\frac{\\partial \\mathcal{L}_{ij}}{\\partial y_k} = \\frac{\\partial \\mathcal{L}_{ij}}{\\partial R_{ij}} \\frac{\\partial R_{ij}}{\\partial y_k}$.\n\nFirst, we find the derivative of $\\mathcal{L}_{ij}$ with respect to $R_{ij}$:\n$$\n\\frac{\\partial \\mathcal{L}_{ij}}{\\partial R_{ij}} = 2\\alpha(R_{ij} - D^{(\\text{expr})}_{ij}) + 2(1-\\alpha)(R_{ij} - D^{(\\text{spatial})}_{ij}) = 2(R_{ij} - W_{ij})\n$$\nNext, we find the derivative of $R_{ij} = \\|y_i - y_j\\|_2$ with respect to $y_i$ and $y_j$. Using the standard derivative of the Euclidean norm:\n$$\n\\frac{\\partial R_{ij}}{\\partial y_i} = \\frac{y_i - y_j}{\\|y_i - y_j\\|_2} = \\frac{y_i - y_j}{R_{ij}} \\quad \\text{and} \\quad \\frac{\\partial R_{ij}}{\\partial y_j} = \\frac{y_j - y_i}{\\|y_i - y_j\\|_2} = -\\frac{y_i - y_j}{R_{ij}}\n$$\nFor any $k \\ne i, j$, $\\frac{\\partial R_{ij}}{\\partial y_k}$ is the zero vector.\n\nNow we combine these results. For a term $\\mathcal{L}_{kj}$ with $j>k$:\n$$\n\\frac{\\partial \\mathcal{L}_{kj}}{\\partial y_k} = \\frac{\\partial \\mathcal{L}_{kj}}{\\partial R_{kj}} \\frac{\\partial R_{kj}}{\\partial y_k} = 2(R_{kj} - W_{kj}) \\frac{y_k - y_j}{R_{kj}} = 2\\left(1 - \\frac{W_{kj}}{R_{kj}}\\right)(y_k - y_j)\n$$\nFor a term $\\mathcal{L}_{ik}$ with $i<k$:\n$$\n\\frac{\\partial \\mathcal{L}_{ik}}{\\partial y_k} = \\frac{\\partial \\mathcal{L}_{ik}}{\\partial R_{ik}} \\frac{\\partial R_{ik}}{\\partial y_k} = 2(R_{ik} - W_{ik}) \\frac{y_k - y_i}{R_{ik}} = 2\\left(1 - \\frac{W_{ik}}{R_{ik}}\\right)(y_k - y_i)\n$$\nSumming these contributions over all $j \\neq k$:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial y_k} = \\sum_{j > k} 2\\left(1 - \\frac{W_{kj}}{R_{kj}}\\right)(y_k - y_j) + \\sum_{i < k} 2\\left(1 - \\frac{W_{ik}}{R_{ik}}\\right)(y_k - y_i)\n$$\nSince the summation indices are local, we can combine this into a single sum over all $j \\neq k$:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial y_k} = \\sum_{j \\neq k} 2\\left(1 - \\frac{W_{kj}}{R_{kj}}\\right)(y_k - y_j)\n$$\nThe case where $R_{kj} = 0$ (i.e., $y_k=y_j$) leads to a singularity. As requested, we introduce a small constant $\\varepsilon > 0$ for numerical stability. The final, numerically stable expression for the gradient with respect to $y_k$ is:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial y_k} = 2 \\sum_{j \\neq k} \\left(1 - \\frac{W_{kj}}{\\max(R_{kj}, \\varepsilon)}\\right) (y_k - y_j)\n$$\nThis formula provides the update direction for each point in the embedding.\n\n### 2. Algorithm Design and Implementation\n\nThe optimization is performed using gradient descent on the embedding coordinates $Y$. The algorithm is designed based on the principles of metric MDS, incorporating the specific constraints of the problem.\n\n**Step 1: Preprocessing.**\nThe input data consists of a spatial coordinate matrix $S \\in \\mathbb{R}^{N \\times 2}$ and an expression feature matrix $X \\in \\mathbb{R}^{N \\times m}$.\nFirst, the pairwise Euclidean distance matrices $D^{(\\text{spatial})}$ and $D^{(\\text{expr})}$ are computed from $S$ and $X$, respectively.\nTo ensure the two modalities contribute on a comparable scale to the loss function, both distance matrices are normalized. As specified, each matrix is divided by the mean of its non-zero upper-triangular entries. This scales both modalities such that the average non-zero distance is $1$. Let the normalized matrices be $D^{(\\text{spatial, norm})}$ and $D^{(\\text{expr, norm})}$.\nFinally, the weighted target distance matrix $W$ is computed as $W_{ij} = \\alpha D^{(\\text{expr, norm})}_{ij} + (1-\\alpha)D^{(\\text{spatial, norm})}_{ij}$.\n\n**Step 2: Initialization.**\nA good initialization is crucial for non-convex optimization. The embedding $Y$ is initialized using the spatial coordinates $S$. This provides a starting configuration that is already optimal for one part of the loss function (when $\\alpha=0$).\nThe initial embedding $Y^{(0)} = S$ is then scaled. We compute the mean of its non-zero pairwise distances and divide all coordinates by this mean. This ensures the initial embedding distances have an average of $1$, matching the scale of the target distances $W$, which helps to start with a reasonable gradient magnitude.\n\n**Step 3: Iterative Optimization.**\nThe core of the algorithm is an iterative gradient descent loop. At each iteration $t$:\n- **Recentering**: The embedding $Y^{(t)}$ is centered by subtracting the mean coordinate vector. This removes the translational degree of freedom, as the loss function is invariant to global shifts of the embedding, preventing numerical drift.\n- **Loss Calculation**: The current embedding distances $R_{ij}^{(t)}$ are computed from $Y^{(t)}$, and the total loss $\\mathcal{L}(Y^{(t)}; \\alpha)$ is calculated and stored.\n- **Gradient Calculation**: The gradient matrix $\\nabla_Y \\mathcal{L}$ is computed. Each row $k$ is calculated using the derived formula. For efficiency, this is vectorized. Let $C$ be a matrix with entries $C_{ij} = 2(1 - W_{ij} / \\max(R_{ij}^{(t)}, \\varepsilon))$ for $i \\ne j$ and $C_{ii}=0$. The gradient can be expressed as $\\nabla_Y \\mathcal{L} = L Y^{(t)}$, where $L$ is a Laplacian-like matrix defined as $L = \\text{diag}(C\\mathbf{1}) - C$, with $\\mathbf{1}$ being a vector of ones.\n- **Update**: The embedding is updated using the rule $Y^{(t+1)} = Y^{(t)} - \\eta \\nabla_Y \\mathcal{L}$, where $\\eta=0.05$ is the fixed step size.\n\n**Step 4: Termination.**\nThe loop runs for a maximum of $1000$ iterations. It may terminate early if the optimization has converged. Convergence is assessed by checking if the absolute change in the loss function over a window of the last $20$ iterations is below a tolerance of $10^{-9}$. That is, if $|\\mathcal{L}^{(t)} - \\mathcal{L}^{(t-20)}| < 10^{-9}$, the algorithm stops. This criterion ensures that the process halts when significant progress is no longer being made. The final loss value is then reported.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.spatial.distance import pdist, squareform\n\n# Hyperparameters as specified in the problem statement\nEPS = 1e-8\nSTEP_SIZE = 0.05\nMAX_ITER = 1000\nSTOP_WINDOW = 20\nSTOP_TOL = 1e-9\n\ndef solve():\n    \"\"\"\n    Main function to orchestrate the test cases and print the final result.\n    \"\"\"\n\n    def run_optimization(alpha, S_in, X_in):\n        \"\"\"\n        Performs the gradient descent optimization for a single test case.\n        \"\"\"\n        # 1. Preprocessing: compute and normalize distance matrices\n        D_spatial = squareform(pdist(S_in, 'euclidean'))\n        D_expr = squareform(pdist(X_in, 'euclidean'))\n\n        # Normalize by mean of non-zero upper-triangular entries.\n        D_spatial_ut = pdist(S_in, 'euclidean')\n        mean_spatial = D_spatial_ut[D_spatial_ut > 0].mean() if np.any(D_spatial_ut > 0) else 0.\n        D_spatial_norm = D_spatial / mean_spatial if mean_spatial > 0 else D_spatial\n\n        D_expr_ut = pdist(X_in, 'euclidean')\n        mean_expr = D_expr_ut[D_expr_ut > 0].mean() if np.any(D_expr_ut > 0) else 0.\n        D_expr_norm = D_expr / mean_expr if mean_expr > 0 else D_expr\n        \n        # Weighted target distance matrix\n        W = alpha * D_expr_norm + (1 - alpha) * D_spatial_norm\n        \n        # 2. Initialization\n        Y = S_in.copy().astype(np.float64) # Use float64 for precision\n\n        # Scale Y so that the mean of its nonzero upper-triangular distances is 1.\n        R_init_ut = pdist(Y, 'euclidean')\n        mean_R_init = R_init_ut[R_init_ut > 0].mean() if np.any(R_init_ut > 0) else 0.\n        if mean_R_init > 0:\n            Y /= mean_R_init\n            \n        # 3. Iterative Optimization\n        loss_history = []\n        \n        for i in range(MAX_ITER):\n            # Recenter at each iteration to prevent drift\n            Y -= Y.mean(axis=0)\n\n            # Compute current embedding distances\n            R = squareform(pdist(Y, 'euclidean'))\n\n            # Compute loss\n            loss_term1 = alpha * np.square(R - D_expr_norm)\n            loss_term2 = (1 - alpha) * np.square(R - D_spatial_norm)\n            loss = np.sum(np.triu(loss_term1 + loss_term2, k=1))\n            loss_history.append(loss)\n\n            # Check for early stopping\n            if i >= STOP_WINDOW:\n                if abs(loss_history[-1] - loss_history[-STOP_WINDOW]) < STOP_TOL:\n                    break\n            \n            # Compute gradient using the vectorized approach\n            R_safe = np.maximum(R, EPS)\n            ratio_matrix = W / R_safe\n            np.fill_diagonal(ratio_matrix, 0)\n            \n            C_matrix = 2 * (1 - ratio_matrix)\n            np.fill_diagonal(C_matrix, 0)\n                    \n            L_lap = np.diag(C_matrix.sum(axis=1)) - C_matrix\n            \n            grad = L_lap @ Y\n            \n            # Update embedding coordinates\n            Y -= STEP_SIZE * grad\n\n        # Final loss calculation after optimization loop\n        Y -= Y.mean(axis=0) # Final recenter for consistent loss value\n        R_final = squareform(pdist(Y, 'euclidean'))\n        loss_term1_final = alpha * np.square(R_final - D_expr_norm)\n        loss_term2_final = (1 - alpha) * np.square(R_final - D_spatial_norm)\n        final_loss = np.sum(np.triu(loss_term1_final + loss_term2_final, k=1))\n        \n        return final_loss\n\n    # Define the test cases from the problem statement.\n    S = np.array([\n        [0.0, 0.0], [1.0, 0.0], [2.0, 0.0],\n        [0.0, 1.0], [1.0, 1.0], [2.0, 1.0]\n    ])\n\n    # Construct expression matrix X based on S and deltas\n    s_i_1 = S[:, 0]\n    s_i_2 = S[:, 1]\n    \n    delta1 = np.array([0.00, 0.05, -0.05, 0.025, -0.025, 0.00])\n    delta2 = np.array([0.01, -0.01, 0.015, -0.015, 0.00, 0.02])\n    delta3 = np.array([0.02, 0.00, -0.02, 0.03, -0.03, 0.01])\n    \n    X = np.zeros((6, 3))\n    X[:, 0] = s_i_1 + delta1\n    X[:, 1] = s_i_2 + delta2\n    X[:, 2] = 0.5 * s_i_1 + 0.5 * s_i_2 + delta3\n\n    X_scaled = 3.0 * X\n    S_dup = np.array([\n        [0.0, 0.0], [1.0, 0.0], [2.0, 0.0],\n        [0.0, 1.0], [1.0, 1.0], [1.0, 1.0]\n    ])\n\n    test_cases = [\n        {'alpha': 0.5, 'S': S, 'X': X},\n        {'alpha': 1.0, 'S': S, 'X': X},\n        {'alpha': 0.0, 'S': S, 'X': X},\n        {'alpha': 0.3, 'S': S, 'X': X_scaled},\n        {'alpha': 0.5, 'S': S_dup, 'X': X}\n    ]\n\n    results = []\n    for case in test_cases:\n        final_loss = run_optimization(case['alpha'], case['S'], case['X'])\n        results.append(final_loss)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.12f}' for r in results)}]\")\n\nsolve()\n```", "id": "3334340"}]}