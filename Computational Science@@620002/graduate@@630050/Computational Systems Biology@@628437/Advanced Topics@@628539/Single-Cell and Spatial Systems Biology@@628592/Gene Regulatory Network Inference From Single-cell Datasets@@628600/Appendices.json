{"hands_on_practices": [{"introduction": "Before inferring regulatory networks, raw single-cell count data must be processed to remove technical noise and stabilize variance. This exercise [@problem_id:3314516] guides you through implementing and comparing two essential transformations: the standard log-normalization and a more advanced regularized log transform. By quantifying how these distinct preprocessing strategies alter the resulting gene-gene correlations, you will gain a hands-on understanding of how foundational data-handling choices can profoundly influence the outcome of downstream network analysis.", "problem": "You are given single-cell ribonucleic acid sequencing count data for multiple genes across multiple cells as a nonnegative integer matrix and a vector of positive size factors representing per-cell sequencing depth. Your task is to implement two transformations of the count data: a log-normalization with a pseudocount and a variance-stabilizing regularized log transform, and then to quantify how the regularization affects the Pearson product-moment correlation coefficient between specified gene pairs.\n\nStart from the following fundamental base for single-cell gene regulatory network analysis:\n- The Central Dogma of Molecular Biology states that deoxyribonucleic acid is transcribed into ribonucleic acid and translated into protein; gene expression is quantified by ribonucleic acid transcript counts per gene per cell.\n- Single-cell ribonucleic acid sequencing counts are well modeled as overdispersed count data, often with a Negative Binomial distribution. Sequencing depth confounds raw counts, so dividing by per-cell size factors is a widely accepted normalization step.\n- The natural logarithm reduces multiplicative scale and partially stabilizes variance in count data when applied with a positive pseudocount to avoid the undefined $\\log(0)$.\n- Regularization, such as ridge-type shrinkage, reduces variance by shrinking deviations toward a central tendency.\n- The Pearson product-moment correlation coefficient measures linear association between two variables across samples.\n\nLet $C \\in \\mathbb{N}_0^{G \\times N}$ denote the count matrix with $G$ genes and $N$ cells, and $s \\in \\mathbb{R}_+^{N}$ denote the positive size factor vector. For a positive pseudocount $\\alpha \\in \\mathbb{R}_+$, define the log-normalized expression matrix $L \\in \\mathbb{R}^{G \\times N}$ entrywise by\n$$\nL_{ij} = \\log\\left(\\frac{C_{ij}}{s_j} + \\alpha\\right),\n$$\nwhere $\\log$ denotes the natural logarithm. To construct a variance-stabilizing regularized log transform, shrink gene-wise log-expression deviations toward a gene-wise mean using a gene-specific ridge parameter derived from the normalized mean expression. Specifically, let the normalized (non-log) expression be $X_{ij} = \\frac{C_{ij}}{s_j}$ and the gene-wise normalized mean be $\\bar{X}_i = \\frac{1}{N}\\sum_{j=1}^{N} X_{ij}$. For a regularization scale $\\beta \\in \\mathbb{R}_+$ and a small positive constant $\\varepsilon \\in \\mathbb{R}_+$ to avoid division by zero, define the gene-specific regularization strength\n$$\n\\lambda_i = \\frac{\\beta}{\\bar{X}_i + \\varepsilon}.\n$$\nLet the gene-wise log mean be $\\mu_i = \\frac{1}{N} \\sum_{j=1}^{N} L_{ij}$. The regularized log transform $R \\in \\mathbb{R}^{G \\times N}$ is then defined entrywise by shrinking deviations around $\\mu_i$ via\n$$\nR_{ij} = \\mu_i + \\frac{1}{1+\\lambda_i}\\left(L_{ij} - \\mu_i\\right).\n$$\n\nFor any pair of genes $(a,b)$ with $a \\in \\{0,1,\\dots,G-1\\}$ and $b \\in \\{0,1,\\dots,G-1\\}$, define the Pearson product-moment correlation coefficient across cells for a transform $T \\in \\{L,R\\}$ as\n$$\n\\rho_T(a,b) = \\frac{\\sum_{j=1}^{N} \\left(T_{a j} - \\bar{T}_a\\right)\\left(T_{b j} - \\bar{T}_b\\right)}{\\sqrt{\\sum_{j=1}^{N} \\left(T_{a j} - \\bar{T}_a\\right)^2}\\sqrt{\\sum_{j=1}^{N} \\left(T_{b j} - \\bar{T}_b\\right)^2}},\n$$\nwhere $\\bar{T}_a = \\frac{1}{N}\\sum_{j=1}^{N} T_{a j}$ and $\\bar{T}_b = \\frac{1}{N}\\sum_{j=1}^{N} T_{b j}$. If either gene has zero variance under a transform (i.e., the denominator is zero), define $\\rho_T(a,b)$ to be $0$ for that transform.\n\nEvaluate how variance stabilization alters correlation by computing, for each specified gene pair $(a,b)$, the difference\n$$\n\\Delta(a,b) = \\rho_R(a,b) - \\rho_L(a,b).\n$$\n\nImplement a program that, for a small test suite of count matrices with accompanying parameters, computes the arithmetic mean of $\\Delta(a,b)$ over the specified gene pairs for each test case. The final output must aggregate the results of all provided test cases into a single line as a comma-separated list enclosed in square brackets.\n\nUse the following test suite:\n\n- Test case $1$ (general case with moderate counts, some zeros, and heterogeneous size factors):\n$$\nC^{(1)} = \\begin{bmatrix}\n12 & 0 & 5 & 20\\\\\n0 & 2 & 0 & 8\\\\\n30 & 25 & 0 & 10\n\\end{bmatrix}, \\quad\ns^{(1)} = [0.9, 1.1, 0.8, 1.3], \\quad\n\\alpha^{(1)} = 0.5, \\quad\n\\beta^{(1)} = 1.0, \\quad\n\\varepsilon^{(1)} = 10^{-6}, \\quad\nP^{(1)} = \\{(0,1), (0,2)\\}.\n$$\n\n- Test case $2$ (boundary case with many zeros and equal size factors):\n$$\nC^{(2)} = \\begin{bmatrix}\n0 & 0 & 0 & 1 & 0\\\\\n3 & 0 & 0 & 0 & 2\n\\end{bmatrix}, \\quad\ns^{(2)} = [1.0, 1.0, 1.0, 1.0, 1.0], \\quad\n\\alpha^{(2)} = 10^{-3}, \\quad\n\\beta^{(2)} = 2.0, \\quad\n\\varepsilon^{(2)} = 10^{-6}, \\quad\nP^{(2)} = \\{(0,1)\\}.\n$$\n\n- Test case $3$ (edge case with extreme size factor heterogeneity):\n$$\nC^{(3)} = \\begin{bmatrix}\n100 & 200 & 300\\\\\n5 & 10 & 15\\\\\n0 & 0 & 1\n\\end{bmatrix}, \\quad\ns^{(3)} = [0.5, 1.0, 5.0], \\quad\n\\alpha^{(3)} = 1.0, \\quad\n\\beta^{(3)} = 0.5, \\quad\n\\varepsilon^{(3)} = 10^{-3}, \\quad\nP^{(3)} = \\{(0,1), (1,2)\\}.\n$$\n\nYour program must:\n- For each test case, compute $L$, $R$, then $\\Delta(a,b)$ for all $(a,b) \\in P^{(k)}$ for test case $k$, and finally the arithmetic mean over $P^{(k)}$.\n- Produce a single line of output containing the mean differences for the test cases in order $k = 1, 2, 3$ as a comma-separated list enclosed in square brackets (for example, $[x_1,x_2,x_3]$). No units are involved; all computations are dimensionless real numbers.", "solution": "The problem requires the implementation and comparison of two common data transformation methods in single-cell transcriptomics: a standard log-normalization and a variance-stabilizing regularized log transform. The goal is to quantify the impact of regularization on the Pearson correlation between pairs of genes. The procedure is grounded in established principles of single-cell data analysis, where raw counts are adjusted for technical artifacts (like sequencing depth) and transformed to stabilize variance and facilitate downstream analyses such as gene regulatory network inference.\n\nThe process for each test case is as follows:\n\nFirst, we address the confounding effect of variable sequencing depth across cells. The raw count matrix $C \\in \\mathbb{N}_0^{G \\times N}$, where $G$ is the number of genes and $N$ is the number of cells, is normalized by the per-cell size factors $s \\in \\mathbb{R}_+^{N}$. This yields the normalized expression matrix $X \\in \\mathbb{R}_{\\ge 0}^{G \\times N}$, with entries $X_{ij} = \\frac{C_{ij}}{s_j}$. This step places the counts from each cell onto a comparable scale.\n\nSecond, we apply the standard log-normalization. Gene expression data often spans several orders of magnitude and exhibits mean-variance relationships where higher counts have higher variance. The natural logarithm is applied to compress this range and mitigate heteroskedasticity. To handle the issue of zero counts, for which $\\log(0)$ is undefined, a positive pseudocount $\\alpha \\in \\mathbb{R}_+$ is added. The log-normalized expression matrix $L \\in \\mathbb{R}^{G \\times N}$ is thus computed entrywise as:\n$$\nL_{ij} = \\log\\left(X_{ij} + \\alpha\\right) = \\log\\left(\\frac{C_{ij}}{s_j} + \\alpha\\right)\n$$\n\nThird, we construct the variance-stabilizing regularized log transform. While log-transformation helps, it does not perfectly stabilize the variance. Regularization, or shrinkage, is a technique to further reduce variance by pulling expression values toward a central tendency. Here, we implement a gene-specific ridge-type shrinkage. The strength of this shrinkage for each gene $i$, denoted by $\\lambda_i$, is designed to be inversely proportional to its mean normalized expression, $\\bar{X}_i = \\frac{1}{N}\\sum_{j=1}^{N} X_{ij}$. This is motivated by the observation that lowly expressed genes tend to have higher relative variance (are noisier) and thus benefit from stronger regularization. For a given regularization scale $\\beta \\in \\mathbb{R}_+$ and a small constant $\\varepsilon \\in \\mathbb{R}_+$ to prevent division by zero, the regularization strength is:\n$$\n\\lambda_i = \\frac{\\beta}{\\bar{X}_i + \\varepsilon}\n$$\nThe regularized matrix $R \\in \\mathbb{R}^{G \\times N}$ is then obtained by shrinking the log-expression deviations $(L_{ij} - \\mu_i)$ towards the gene-wise log-mean $\\mu_i = \\frac{1}{N} \\sum_{j=1}^{N} L_{ij}$. The formula for the regularized value $R_{ij}$ is:\n$$\nR_{ij} = \\mu_i + \\frac{1}{1+\\lambda_i}\\left(L_{ij} - \\mu_i\\right)\n$$\nThe term $\\frac{1}{1+\\lambda_i}$ acts as the shrinkage factor. When a gene's mean expression $\\bar{X}_i$ is low, $\\lambda_i$ is large, making the shrinkage factor small and pulling the values $L_{ij}$ strongly toward the mean $\\mu_i$. Conversely, for highly expressed genes, $\\lambda_i$ is small, and the values are only weakly regularized.\n\nFourth, we quantify the linear association between specified gene pairs $(a,b)$ using the Pearson product-moment correlation coefficient, $\\rho$. This is calculated for both the log-normalized data ($L$) and the regularized data ($R$). For a given transformed matrix $T \\in \\{L, R\\}$, the correlation between gene $a$ and gene $b$ is:\n$$\n\\rho_T(a,b) = \\frac{\\sum_{j=1}^{N} \\left(T_{a j} - \\bar{T}_a\\right)\\left(T_{b j} - \\bar{T}_b\\right)}{\\sqrt{\\sum_{j=1}^{N} \\left(T_{a j} - \\bar{T}_a\\right)^2}\\sqrt{\\sum_{j=1}^{N} \\left(T_{b j} - \\bar{T}_b\\right)^2}}\n$$\nwhere $\\bar{T}_a$ and $\\bar{T}_b$ are the gene-wise means of the transformed expression values across cells. As specified, if the expression of either gene in a pair has zero variance across cells (i.e., its expression is constant), the denominator becomes zero, and the correlation $\\rho_T(a,b)$ is defined to be $0$.\n\nFinally, to assess the impact of the regularization, we compute the difference in the correlation coefficients, $\\Delta(a,b) = \\rho_R(a,b) - \\rho_L(a,b)$, for each gene pair $(a,b)$ in the specified set $P^{(k)}$ for test case $k$. The final result for each test case is the arithmetic mean of these differences over all pairs in $P^{(k)}$. The implementation will carry out these steps for each provided test case and aggregate the mean differences into a single output list.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements the full workflow for all test cases and prints the final result.\n    \"\"\"\n    \n    # Test case 1\n    C1 = np.array([[12, 0, 5, 20],\n                   [0, 2, 0, 8],\n                   [30, 25, 0, 10]], dtype=np.float64)\n    s1 = np.array([0.9, 1.1, 0.8, 1.3], dtype=np.float64)\n    alpha1 = 0.5\n    beta1 = 1.0\n    epsilon1 = 1e-6\n    P1 = [(0, 1), (0, 2)]\n\n    # Test case 2\n    C2 = np.array([[0, 0, 0, 1, 0],\n                   [3, 0, 0, 0, 2]], dtype=np.float64)\n    s2 = np.array([1.0, 1.0, 1.0, 1.0, 1.0], dtype=np.float64)\n    alpha2 = 1e-3\n    beta2 = 2.0\n    epsilon2 = 1e-6\n    P2 = [(0, 1)]\n\n    # Test case 3\n    C3 = np.array([[100, 200, 300],\n                   [5, 10, 15],\n                   [0, 0, 1]], dtype=np.float64)\n    s3 = np.array([0.5, 1.0, 5.0], dtype=np.float64)\n    alpha3 = 1.0\n    beta3 = 0.5\n    epsilon3 = 1e-3\n    P3 = [(0, 1), (1, 2)]\n\n    test_cases = [\n        (C1, s1, alpha1, beta1, epsilon1, P1),\n        (C2, s2, alpha2, beta2, epsilon2, P2),\n        (C3, s3, alpha3, beta3, epsilon3, P3),\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        C, s, alpha, beta, epsilon, P = case\n        \n        # Step 1: Normalize by size factors\n        X = C / s\n        \n        # Step 2: Log-normalize with pseudocount\n        L = np.log(X + alpha)\n        \n        # Step 3: Calculate parameters for regularization\n        # Gene-wise normalized mean\n        X_bar = np.mean(X, axis=1)\n        \n        # Gene-specific regularization strength\n        lambda_i = beta / (X_bar + epsilon)\n        \n        # Gene-wise log mean\n        mu_i = np.mean(L, axis=1)\n        \n        # Step 4: Apply regularized log transform\n        # Reshape for broadcasting:\n        # L has shape (G, N)\n        # mu_i and lambda_i have shape (G,). Reshape to (G, 1) to operate on each row.\n        mu_i_reshaped = mu_i.reshape(-1, 1)\n        lambda_i_reshaped = lambda_i.reshape(-1, 1)\n        \n        shrinkage_factor = 1.0 / (1.0 + lambda_i_reshaped)\n        deviations = L - mu_i_reshaped\n        R = mu_i_reshaped + shrinkage_factor * deviations\n        \n        # Step 5: Compute correlation differences\n        \n        def pearson_corr(v_a, v_b):\n            \"\"\"\n            Computes Pearson correlation, returning 0 if variance of either vector is 0.\n            \"\"\"\n            # Using a small tolerance for floating point comparisons\n            if np.isclose(np.var(v_a), 0.0) or np.isclose(np.var(v_b), 0.0):\n                return 0.0\n            \n            # np.corrcoef calculates the correlation matrix. We need the off-diagonal element.\n            # It handles the mean-centering and normalization internally.\n            corr_matrix = np.corrcoef(v_a, v_b)\n            return corr_matrix[0, 1]\n\n        deltas = []\n        for a, b in P:\n            # Correlation on log-normalized data\n            rho_L = pearson_corr(L[a, :], L[b, :])\n            \n            # Correlation on regularized data\n            rho_R = pearson_corr(R[a, :], R[b, :])\n            \n            # Difference\n            delta = rho_R - rho_L\n            deltas.append(delta)\n            \n        # Step 6: Calculate mean difference for the test case\n        mean_delta = np.mean(deltas)\n        results.append(mean_delta)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3314516"}, {"introduction": "Robust gene regulatory network inference often involves integrating evidence from multiple data modalities. This practice [@problem_id:3314530] introduces a powerful Bayesian framework to combine prior knowledge about transcription factor binding potential, derived from scATAC-seq motif accessibility, with the functional evidence of gene expression changes from scRNA-seq data. You will implement the core components of this approach, calculating Bayes factors to weigh the evidence and computing the final posterior probability for each potential regulatory edge.", "problem": "Given a single Transcription Factor (TF) $j$ and a set of candidate target genes indexed by $i$, consider the problem of inferring directed regulatory edges $j \\to i$ from single-cell assays. We will combine Single-cell Assay for Transposase-Accessible Chromatin using sequencing (scATAC-seq) motif accessibility scores with single-cell messenger ribonucleic acid (mRNA) counts to form a Bayesian posterior probability for each edge. Assume the following foundational principles and facts: (1) Bayes' theorem for hypotheses, (2) the Negative Binomial (NB) distribution is a well-tested model for single-cell count data, and (3) accessibility-derived motif scores are informative priors about TF–gene regulatory potential.\n\nDefinitions and modeling assumptions:\n- Let $m_{j \\to i} \\in \\mathbb{R}$ denote the scATAC-seq-derived motif accessibility score for TF $j$ at the promoter or enhancer region of gene $i$.\n- Define the prior probability that the edge $j \\to i$ is present as\n$$\n\\pi_{j \\to i} = \\sigma\\!\\left(\\alpha + \\beta \\, m_{j \\to i}\\right) = \\frac{1}{1 + \\exp\\!\\left( -(\\alpha + \\beta \\, m_{j \\to i}) \\right)},\n$$\nwhere $\\sigma(\\cdot)$ is the logistic function, and $\\alpha, \\beta \\in \\mathbb{R}$ are fixed hyperparameters.\n- Let $y_{i c} \\in \\{0,1,2,\\dots\\}$ denote the observed single-cell mRNA count for gene $i$ in cell $c$, and let $s_c \\in \\mathbb{R}_{>0}$ denote the known per-cell exposure (e.g., a library size normalization factor).\n- Let $x_{j c} \\in \\mathbb{R}$ denote the TF $j$ activity proxy per cell $c$. To operationalize a two-group model, define a threshold $\\tau \\in \\mathbb{R}$ and a group label $g(c) \\in \\{0,1\\}$ such that $g(c) = 1$ if $x_{j c} \\ge \\tau$ and $g(c) = 0$ otherwise.\n- Under the no-edge hypothesis $H_0$, assume a Negative Binomial model with a single mean parameter $\\theta \\in \\mathbb{R}_{\\ge 0}$ and shared dispersion $k \\in \\mathbb{R}_{>0}$:\n$$\ny_{i c} \\sim \\mathrm{NB}\\left(\\mu_c = s_c \\theta, \\; k\\right).\n$$\n- Under the edge-present hypothesis $H_1$, assume a Negative Binomial model with group-specific mean parameters $\\theta_0, \\theta_1 \\in \\mathbb{R}_{\\ge 0}$ and the same dispersion $k$:\n$$\ny_{i c} \\sim \\mathrm{NB}\\left(\\mu_c = s_c \\, \\theta_{g(c)}, \\; k\\right).\n$$\n- Use the following parameterization of the Negative Binomial probability mass function with mean $\\mu$ and dispersion (size) $k$:\n$$\n\\Pr(Y=y \\mid \\mu, k) = \\binom{y + k - 1}{y} \\left(\\frac{k}{k + \\mu}\\right)^{k} \\left(\\frac{\\mu}{k + \\mu}\\right)^y,\n$$\nfor $y \\in \\{0,1,2,\\dots\\}$, $\\mu \\in \\mathbb{R}_{\\ge 0}$, and $k \\in \\mathbb{R}_{>0}$. This parameterization implies $\\mathbb{E}[Y]=\\mu$ and $\\mathrm{Var}(Y) = \\mu + \\mu^2/k$.\n- For model comparison, approximate the marginal likelihood under each hypothesis by plugging in the maximum likelihood estimates (MLEs). The log-likelihood (up to an additive constant independent of $\\mu$) for a set of observations $\\{y_{i c}\\}$ with cell-specific means $\\mu_c$ is\n$$\n\\ell(\\{y_{i c}\\} \\mid \\{\\mu_c\\}, k) = \\sum_{c} \\left[ y_{i c} \\log(\\mu_c) - (y_{i c} + k) \\log(k + \\mu_c) \\right].\n$$\n- The MLEs under $H_0$ and $H_1$ are defined by maximizing the respective log-likelihoods; under $H_0$, $\\mu_c = s_c \\theta$, and under $H_1$, $\\mu_c = s_c \\theta_{g(c)}$. The derivative conditions for the MLEs are, for group $G \\subseteq \\{c\\}$,\n$$\n\\frac{\\partial \\ell}{\\partial \\theta} = \\sum_{c \\in G} \\left[ \\frac{y_{i c}}{\\theta} - \\frac{(y_{i c} + k) s_c}{k + s_c \\theta} \\right] = 0,\n$$\nand similarly the second derivative can be used to ensure concavity and for Newton updates. In edge cases where $\\sum_{c \\in G} y_{i c} = 0$, the MLE occurs at the boundary $\\theta = 0$.\n\nInference target and computation:\n- Define the Bayes factor as\n$$\n\\mathrm{BF}_{j \\to i} = \\exp\\!\\left( \\ell_{H_1}^{\\star} - \\ell_{H_0}^{\\star} \\right),\n$$\nwhere $\\ell_{H_0}^{\\star}$ and $\\ell_{H_1}^{\\star}$ are the maximized log-likelihoods under $H_0$ and $H_1$, respectively, using the MLEs described above.\n- Using Bayes' theorem, compute the posterior probability of the edge $j \\to i$:\n$$\n\\Pr(E_{j \\to i}=1 \\mid \\text{data}) = \\frac{\\pi_{j \\to i} \\, \\mathrm{BF}_{j \\to i}}{\\pi_{j \\to i} \\, \\mathrm{BF}_{j \\to i} + (1 - \\pi_{j \\to i})}.\n$$\n\nTest suite and required output:\nImplement a program that, for the following fixed inputs, outputs the posterior probabilities for four candidate edges as a single line, formatted as a comma-separated list enclosed in square brackets. No physical units are involved; all outputs must be decimal floats.\n\nGlobal parameters shared across all edges:\n- Number of cells $C = 8$.\n- Per-cell exposures $s = [1.00, 0.90, 1.10, 0.95, 1.05, 1.20, 0.80, 1.00]$.\n- TF activities $x_{j} = [-0.50, -0.20, 0.10, 0.30, 0.70, 1.20, -1.00, 0.50]$.\n- Threshold $\\tau = 0.30$ so that $g(c) = 1$ if $x_{j c} \\ge 0.30$ and $g(c) = 0$ otherwise.\n- Dispersion $k = 2.0$.\n- Prior logistic parameters $\\alpha = -2.0$, $\\beta = 1.0$.\n\nPer-edge test cases:\n- Edge $1$: motif score $m_{j \\to 1} = 2.5$; counts $y_{1} = [2, 1, 1, 4, 5, 6, 0, 3]$.\n- Edge $2$: motif score $m_{j \\to 2} = 0.1$; counts $y_{2} = [3, 2, 2, 3, 3, 2, 1, 3]$.\n- Edge $3$: motif score $m_{j \\to 3} = 6.0$; counts $y_{3} = [0, 0, 0, 0, 0, 0, 0, 0]$.\n- Edge $4$: motif score $m_{j \\to 4} = 0.0$; counts $y_{4} = [5, 4, 3, 1, 1, 0, 6, 2]$.\n\nCoverage rationale:\n- Edge $1$ represents a case with higher counts in the high-activity group, expected to favor $H_1$ given sufficient prior support.\n- Edge $2$ represents similar counts across groups, expected to yield a Bayes factor near $1$ and allow the prior to dominate.\n- Edge $3$ represents an all-zero case, a boundary condition where both hypotheses have the same likelihood and the posterior equals the prior.\n- Edge $4$ represents higher counts in the low-activity group, challenging the directional grouping, testing robustness when the group with $g(c)=1$ does not show an increase.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3,result4]\"), where each result is $\\Pr(E_{j \\to i}=1 \\mid \\text{data})$ for edges $i=1,2,3,4$ in that order.", "solution": "The objective is to compute the Bayesian posterior probability for four candidate gene regulatory edges, denoted $j \\to i$. This probability, $\\Pr(E_{j \\to i}=1 \\mid \\text{data})$, synthesizes prior knowledge derived from chromatin accessibility with evidence from single-cell gene expression counts. The framework is grounded in Bayes' theorem, using a Negative Binomial model for count data.\n\nThe overall procedure for each edge $j \\to i$ involves several computational steps:\n\n1.  **Cell Grouping**: The cells are first partitioned into two groups based on the transcription factor (TF) activity proxy, $x_{jc}$. A cell $c$ is assigned to group $1$ if its TF activity meets or exceeds a threshold $\\tau$, i.e., $g(c)=1$ if $x_{jc} \\ge \\tau$, and to group $0$ otherwise.\n    Given the TF activities $x_j = [-0.50, -0.20, 0.10, 0.30, 0.70, 1.20, -1.00, 0.50]$ and threshold $\\tau = 0.30$, the cell indices $\\{0, 1, \\dots, 7\\}$ are partitioned as follows:\n    -   Group $0$ (low TF activity, $g(c)=0$): Cells with indices $\\{0, 1, 2, 6\\}$.\n    -   Group $1$ (high TF activity, $g(c)=1$): Cells with indices $\\{3, 4, 5, 7\\}$.\n\n2.  **Prior Probability Calculation**: The prior probability of an edge, $\\pi_{j \\to i}$, is determined by the scATAC-seq motif accessibility score $m_{j \\to i}$. This is formalized using the logistic function:\n    $$\n    \\pi_{j \\to i} = \\sigma(\\alpha + \\beta \\, m_{j \\to i}) = \\frac{1}{1 + \\exp(-(\\alpha + \\beta \\, m_{j \\to i}))}\n    $$\n    With the given hyperparameters $\\alpha = -2.0$ and $\\beta = 1.0$, this prior is calculated for each of the four test cases based on their respective motif scores.\n\n3.  **Maximum Likelihood Estimation (MLE)**: To evaluate the evidence from gene expression data, we compare two hypotheses using a likelihood-based approach. This requires finding the Maximum Likelihood Estimates (MLEs) of the mean parameters of the Negative Binomial (NB) distribution.\n    -   Under the null hypothesis $H_0$ (no edge), all cells share a single mean parameter $\\theta$. The cell-specific mean count is $\\mu_c = s_c \\theta$.\n    -   Under the alternative hypothesis $H_1$ (edge exists), cells have group-specific mean parameters, $\\theta_0$ and $\\theta_1$. The cell-specific mean count is $\\mu_c = s_c \\theta_{g(c)}$.\n    \n    The MLE for a parameter $\\theta$ (for a given group of cells) is found by solving the equation $\\frac{\\partial \\ell}{\\partial \\theta} = 0$, where $\\ell$ is the log-likelihood. The equation to solve is:\n    $$\n    \\sum_{c \\in G} \\left[ \\frac{y_{ic}}{\\theta} - \\frac{(y_{ic} + k) s_c}{k + s_c \\theta} \\right] = 0\n    $$\n    This non-linear equation is solved numerically. A suitable method is Newton-Raphson, which uses iterative updates of the form $\\theta_{\\text{new}} = \\theta_{\\text{old}} - f(\\theta_{\\text{old}})/f'(\\theta_{\\text{old}})$, where $f(\\theta) = \\frac{\\partial \\ell}{\\partial \\theta}$. An appropriate initial guess for $\\theta$ is the method of moments estimate, $\\theta_0 = (\\sum y_c) / (\\sum s_c)$.\n    \n    A special case occurs when all counts in a group are zero ($\\sum_{c \\in G} y_{ic} = 0$). In this scenario, the likelihood is maximized at the boundary, yielding an MLE of $\\theta=0$.\n\n4.  **Log-Likelihood Calculation**: Once the MLEs are found ($\\theta^{\\star}$ under $H_0$; $\\theta_0^{\\star}$ and $\\theta_1^{\\star}$ under $H_1$), the maximized log-likelihoods are computed. The log-likelihood function (ignoring terms constant with respect to $\\mu$) is:\n    $$\n    \\ell^{\\star}(\\{y_{ic}\\} \\mid \\{\\mu_c^{\\star}\\}, k) = \\sum_{c} \\left[ y_{ic} \\log(\\mu_c^{\\star}) - (y_{ic} + k) \\log(k + \\mu_c^{\\star}) \\right]\n    $$\n    For $H_0$, this gives $\\ell_{H_0}^{\\star}$. For $H_1$, the log-likelihoods are calculated separately for each group and summed: $\\ell_{H_1}^{\\star} = \\ell_{G_0}^{\\star} + \\ell_{G_1}^{\\star}$. In the special case where $\\theta^{\\star}=0$ (and all corresponding $y_{ic}=0$), the log-likelihood for a group of size $|G|$ simplifies to $-|G| k \\log(k)$.\n\n5.  **Bayes Factor Calculation**: The Bayes factor, $\\mathrm{BF}_{j \\to i}$, quantifies the evidence provided by the data in favor of $H_1$ over $H_0$. It is the ratio of the marginal likelihoods, which are approximated here by the likelihoods evaluated at the MLEs:\n    $$\n    \\mathrm{BF}_{j \\to i} = \\frac{\\Pr(\\text{data} \\mid H_1)}{\\Pr(\\text{data} \\mid H_0)} \\approx \\exp(\\ell_{H_1}^{\\star} - \\ell_{H_0}^{\\star})\n    $$\n\n6.  **Posterior Probability Calculation**: Finally, the prior probability and the Bayes factor are combined using Bayes' theorem to yield the posterior probability of the edge:\n    $$\n    \\Pr(E_{j \\to i}=1 \\mid \\text{data}) = \\frac{\\Pr(E_{j \\to i}=1) \\Pr(\\text{data} \\mid E_{j \\to i}=1)}{\\Pr(\\text{data})} = \\frac{\\pi_{j \\to i} \\, \\mathrm{BF}_{j \\to i}}{\\pi_{j \\to i} \\, \\mathrm{BF}_{j \\to i} + (1 - \\pi_{j \\to i})}\n    $$\n    This calculation is performed for each of the four edges specified in the test suite. The implementation will systematically apply these steps to produce the final vector of posterior probabilities.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the Bayesian posterior probability for gene regulatory edges\n    based on scATAC-seq priors and scRNA-seq likelihoods.\n    \"\"\"\n\n    # Global parameters shared across all edges\n    s_all = np.array([1.00, 0.90, 1.10, 0.95, 1.05, 1.20, 0.80, 1.00])\n    x_j = np.array([-0.50, -0.20, 0.10, 0.30, 0.70, 1.20, -1.00, 0.50])\n    tau = 0.30\n    k = 2.0\n    alpha = -2.0\n    beta = 1.0\n\n    # Per-edge test cases\n    test_cases = [\n        # (motif_score, counts_array)\n        (2.5, np.array([2, 1, 1, 4, 5, 6, 0, 3])),\n        (0.1, np.array([3, 2, 2, 3, 3, 2, 1, 3])),\n        (6.0, np.array([0, 0, 0, 0, 0, 0, 0, 0])),\n        (0.0, np.array([5, 4, 3, 1, 1, 0, 6, 2])),\n    ]\n\n    # Pre-compute cell groupings\n    g = (x_j >= tau).astype(int)\n    g0_indices = np.where(g == 0)[0]\n    g1_indices = np.where(g == 1)[0]\n    s_g0 = s_all[g0_indices]\n    s_g1 = s_all[g1_indices]\n\n    def mle_theta(y, s, k, tol=1e-9, max_iter=100):\n        \"\"\"\n        Finds the Maximum Likelihood Estimate for the NB parameter theta\n        using Newton-Raphson iteration.\n        \"\"\"\n        if np.sum(y) == 0.0:\n            return 0.0\n        \n        # Initial guess using method of moments\n        theta = np.sum(y) / np.sum(s)\n        if theta <= 0:  # Fallback for unusual inputs\n            theta = 1.0\n\n        for _ in range(max_iter):\n            # Use a safe value for theta to avoid division by zero or log(0)\n            theta_safe = max(theta, 1e-12)\n            \n            mu = s * theta_safe\n            \n            # First derivative of log-likelihood (the function to find root of)\n            f_val = np.sum(y / theta_safe - (y + k) * s / (k + mu))\n            \n            if abs(f_val) < tol:\n                break\n            \n            # Second derivative of log-likelihood\n            df_val = np.sum(-y / theta_safe**2 + (y + k) * s**2 / (k + mu)**2)\n            \n            # Avoid division by zero if derivative is flat\n            if abs(df_val) < 1e-12:\n                break\n\n            # Newton-Raphson step\n            step = f_val / df_val\n            theta = theta - step\n        \n        # Ensure theta is positive\n        return max(theta, 1e-12)\n\n    def log_likelihood(y, s, theta, k):\n        \"\"\"\n        Calculates the log-likelihood for the NB model given parameters.\n        \"\"\"\n        if theta < 1e-12:\n            if np.sum(y) > 0:\n                return -np.inf\n            # If all y_i are 0 and theta is 0\n            return -len(y) * k * np.log(k)\n        \n        mu = s * theta\n        \n        # The term y*log(mu) can be written as y*(log(s)+log(theta)).\n        # If y_i is 0, the term is 0. If y_i > 0, mu > 0, so log is safe.\n        # We need to handle the case where y_i > 0 but mu_i is 0 (which\n        # shouldn't happen if theta > 0).\n        # A vectorized approach that avoids 0*log(0) = nan:\n        term1 = np.where(y > 0, y * np.log(mu), 0)\n        term2 = (y + k) * np.log(k + mu)\n        \n        return np.sum(term1 - term2)\n\n    results = []\n    for m, y_all in test_cases:\n        # 1. Calculate prior probability\n        pi = 1.0 / (1.0 + np.exp(-(alpha + beta * m)))\n        \n        # 2. Calculate maximized log-likelihood under H0 (one group)\n        theta_h0 = mle_theta(y_all, s_all, k)\n        ll_h0 = log_likelihood(y_all, s_all, theta_h0, k)\n        \n        # 3. Calculate maximized log-likelihood under H1 (two groups)\n        y_g0 = y_all[g0_indices]\n        y_g1 = y_all[g1_indices]\n        \n        theta_g0 = mle_theta(y_g0, s_g0, k)\n        ll_g0 = log_likelihood(y_g0, s_g0, theta_g0, k)\n        \n        theta_g1 = mle_theta(y_g1, s_g1, k)\n        ll_g1 = log_likelihood(y_g1, s_g1, theta_g1, k)\n        \n        ll_h1 = ll_g0 + ll_g1\n        \n        # 4. Calculate Bayes Factor\n        # Check for -inf likelihoods, which can occur if MLE is 0\n        # for a group with non-zero counts (a scenario our code avoids, but\n        # is good practice to handle robustly).\n        if ll_h1 == -np.inf:\n            bf = 0.0\n        else:\n            bf = np.exp(ll_h1 - ll_h0)\n        \n        # 5. Calculate posterior probability\n        if np.isinf(bf):\n            posterior = 1.0\n        else:\n            denominator = pi * bf + (1.0 - pi)\n            if denominator == 0:\n                posterior = 0.0\n            else:\n                posterior = (pi * bf) / denominator\n        \n        results.append(posterior)\n\n    # Format the final output\n    print(f\"[{','.join(f'{r:.10f}' for r in results)}]\")\n\nsolve()\n```", "id": "3314530"}, {"introduction": "The output of a Bayesian inference model is not a single network but a full posterior distribution over all possible edge weights, which presents a challenge for interpretation. This final practice [@problem_id:3314570] addresses the critical step of translating these probabilistic results into a concrete set of high-confidence regulatory interactions. You will learn to compute Bayes factors directly from posterior samples using the Savage-Dickey density ratio and then apply a Bayesian decision rule to control the False Discovery Rate (FDR) at a desired level.", "problem": "You are given posterior samples of edge weights from a Markov Chain Monte Carlo (MCMC) procedure for a gene regulatory network. For each directed edge, you seek to quantify support for the alternative hypothesis that the edge weight is nonzero and to control the expected proportion of false discoveries using a Bayesian decision rule. The foundational base consists of Bayes’ theorem, the definition of Bayes factor, the Savage–Dickey density ratio for nested models with a continuous prior, and Kernel Density Estimation (KDE) with a Gaussian kernel as a well-tested nonparametric method to estimate continuous densities.\n\nModeling assumptions: For each edge parameter $w$, you consider two models: the null model $\\mathcal{M}_0$ where $w = 0$ and the alternative model $\\mathcal{M}_1$ where $w \\sim \\mathcal{N}(0,\\tau^2)$ for a known prior scale parameter $\\tau > 0$. You assume that the MCMC posterior samples for $w$ are drawn under $\\mathcal{M}_1$ with this prior. The prior model probabilities are $\\Pr(\\mathcal{M}_1) = p_1 \\in (0,1)$ and $\\Pr(\\mathcal{M}_0) = 1 - p_1$.\n\nBayes factor via Savage–Dickey: Under regularity for nested models with a continuous prior that is proper and nonzero at $w = 0$, the Bayes factor in favor of $\\mathcal{M}_1$ versus $\\mathcal{M}_0$ equals\n$$\n\\mathrm{BF}_{10} = \\frac{p(w=0 \\mid \\mathcal{M}_1)}{p(w=0 \\mid \\text{data}, \\mathcal{M}_1)},\n$$\nwhere $p(w=0 \\mid \\mathcal{M}_1)$ is the prior density at $w=0$ and $p(w=0 \\mid \\text{data}, \\mathcal{M}_1)$ is the posterior density at $w=0$ under $\\mathcal{M}_1$. For the prior $w \\sim \\mathcal{N}(0,\\tau^2)$, the prior density at zero is\n$$\np(w=0 \\mid \\mathcal{M}_1) = \\frac{1}{\\sqrt{2\\pi}\\,\\tau}.\n$$\nBecause you only have samples from the posterior under $\\mathcal{M}_1$, you must estimate $p(w=0 \\mid \\text{data}, \\mathcal{M}_1)$ nonparametrically using KDE with a Gaussian kernel. Given $n$ posterior samples $\\{w_i\\}_{i=1}^n$, the KDE at $w=0$ with bandwidth $h>0$ is\n$$\n\\widehat{p}(0) = \\frac{1}{n h} \\sum_{i=1}^n \\phi\\!\\left(\\frac{0 - w_i}{h}\\right),\n$$\nwhere $\\phi(u) = \\frac{1}{\\sqrt{2\\pi}} \\exp(-u^2/2)$ is the standard normal density. Use Silverman’s rule of thumb $h = 1.06 \\,\\widehat{\\sigma}\\, n^{-1/5}$, where $\\widehat{\\sigma}$ is the scale defined as $\\min\\{\\text{sample standard deviation}, \\text{interquartile range}/1.34\\}$. If $\\widehat{\\sigma}$ is numerically $0$, set $h$ to a small positive value to avoid division by $0$.\n\nPosterior model probabilities and local false discovery rates: The posterior odds for $\\mathcal{M}_1$ versus $\\mathcal{M}_0$ are given by\n$$\n\\frac{\\Pr(\\mathcal{M}_1 \\mid \\text{data})}{\\Pr(\\mathcal{M}_0 \\mid \\text{data})} = \\mathrm{BF}_{10} \\times \\frac{p_1}{1 - p_1}.\n$$\nDefine the local false discovery rate for an edge as\n$$\n\\ell = \\Pr(\\mathcal{M}_0 \\mid \\text{data}) = \\frac{1}{1 + \\mathrm{BF}_{10} \\times \\frac{p_1}{1 - p_1}}.\n$$\nBayesian false discovery rate (FDR) control: To select a set of edges at target FDR level $q \\in (0,1)$, compute $\\ell_j$ for each edge $j$, sort the edges by $\\ell_j$ in ascending order, and choose the largest $k$ such that the running mean $\\frac{1}{k}\\sum_{i=1}^k \\ell_{(i)} \\le q$, where $\\ell_{(1)} \\le \\ell_{(2)} \\le \\cdots$ are the ordered local false discovery rates. Select edges corresponding to the first $k$ ordered values. If no $k \\ge 1$ satisfies the inequality, return the empty set.\n\nYour task: Implement a program that, for each test case, generates MCMC posterior samples for each edge from a Gaussian distribution $\\mathcal{N}(\\mu, \\sigma^2)$ to emulate posterior draws under $\\mathcal{M}_1$. Then, for each edge, compute $\\mathrm{BF}_{10}$ via the Savage–Dickey ratio using KDE at zero, transform to $\\ell$, and apply the Bayesian FDR selection rule to output the indices of selected edges.\n\nAll mathematical quantities are dimensionless in this problem; there are no physical units. If any percentage is conceptually invoked, always encode it as a decimal (for example, use $0.1$ instead of $10\\%$).\n\nTest suite: For reproducibility, you must use the specified seeds, sample sizes, priors, and posterior-generating-parameters. For each test case, generate $n$ samples per edge using a pseudorandom number generator seeded as indicated.\n\nTest case $1$:\n- Seed $= 17$\n- Number of edges $= 5$\n- Samples per edge $n = 5000$\n- Prior scale $\\tau = 1.0$\n- Prior inclusion probability $p_1 = 0.2$\n- Target FDR $q = 0.1$\n- Edge posterior generators $(\\mu, \\sigma)$, in order: $(1.5, 0.25)$, $(-2.0, 0.25)$, $(0.0, 0.2)$, $(0.2, 0.2)$, $(-0.1, 0.2)$\n\nTest case $2$:\n- Seed $= 23$\n- Number of edges $= 6$\n- Samples per edge $n = 6000$\n- Prior scale $\\tau = 2.0$\n- Prior inclusion probability $p_1 = 0.5$\n- Target FDR $q = 0.2$\n- Edge posterior generators $(\\mu, \\sigma)$, in order: $(0.0, 0.4)$, $(0.6, 0.3)$, $(-0.6, 0.3)$, $(1.2, 0.35)$, $(0.3, 0.25)$, $(-1.0, 0.5)$\n\nTest case $3$:\n- Seed $= 5$\n- Number of edges $= 5$\n- Samples per edge $n = 7000$\n- Prior scale $\\tau = 1.5$\n- Prior inclusion probability $p_1 = 0.7$\n- Target FDR $q = 0.05$\n- Edge posterior generators $(\\mu, \\sigma)$, in order: $(0.0, 0.6)$, $(0.2, 0.6)$, $(-0.1, 0.5)$, $(0.3, 0.7)$, $(-0.2, 0.7)$\n\nFinal output format: Your program should produce a single line containing the results for all test cases as a comma-separated list enclosed in square brackets. Each test case result must itself be a list of the selected edge indices (zero-based) in strictly increasing order, enclosed in square brackets. For example, a valid output shape is $[[i_1,i_2],[j_1],[]]$ with no whitespace characters. The outputs must be deterministic given the seeds above.", "solution": "The problem requires the implementation of a Bayesian hypothesis testing procedure to identify significant non-zero edge weights in a gene regulatory network, controlling the False Discovery Rate (FDR). The solution proceeds by first validating the problem statement, which is found to be scientifically sound, well-posed, and objective, and then constructing a program to perform the specified calculations. The core of the method is the computation of a Bayes Factor for each edge, followed by a decision rule based on local false discovery rates.\n\nThe overall algorithm is executed for each test case provided. For each test case, we first initialize a pseudorandom number generator with the given seed to ensure reproducibility. Then, for each edge defined in the test case, we perform a series of calculations to determine its local false discovery rate, $\\ell$.\n\n**Step 1: Posterior Sample Generation**\nFor each edge, we simulate the output of a Markov Chain Monte Carlo (MCMC) procedure by drawing $n$ samples from a Gaussian distribution, $\\mathcal{N}(\\mu, \\sigma^2)$, with parameters $(\\mu, \\sigma)$ specified for that edge. These samples represent draws from the posterior distribution of the edge weight, $w$, under the alternative model $\\mathcal{M}_1$.\n\n**Step 2: Bayes Factor Calculation using the Savage-Dickey Density Ratio**\nThe problem specifies a nested model comparison between the null model $\\mathcal{M}_0: w=0$ and the alternative model $\\mathcal{M}_1: w \\neq 0$. The Bayes Factor, $\\mathrm{BF}_{10}$, quantifies the evidence for $\\mathcal{M}_1$ over $\\mathcal{M}_0$. It is calculated using the Savage-Dickey density ratio:\n$$\n\\mathrm{BF}_{10} = \\frac{p(w=0 \\mid \\mathcal{M}_1)}{p(w=0 \\mid \\text{data}, \\mathcal{M}_1)}\n$$\nThis ratio involves two components: the prior density and the posterior density, both evaluated at the parameter value of the null hypothesis, $w=0$.\n\n**2a. Prior Density at Zero, $p(w=0 \\mid \\mathcal{M}_1)$**\nThe prior for the edge weight under $\\mathcal{M}_1$ is given as a Gaussian distribution, $w \\sim \\mathcal{N}(0, \\tau^2)$. The density of this distribution at $w=0$ is an analytical expression:\n$$\np(w=0 \\mid \\mathcal{M}_1) = \\frac{1}{\\sqrt{2\\pi}\\,\\tau}\n$$\nThis value is computed using the given prior scale parameter $\\tau$.\n\n**2b. Posterior Density at Zero, $p(w=0 \\mid \\text{data}, \\mathcal{M}_1)$**\nSince we only have samples from the posterior distribution, we must estimate its density at $w=0$ nonparametrically. The problem mandates the use of Kernel Density Estimation (KDE) with a Gaussian kernel. For a set of $n$ posterior samples $\\{w_i\\}_{i=1}^n$, the KDE at $w=0$ is:\n$$\n\\widehat{p}(0) = \\frac{1}{n h} \\sum_{i=1}^n \\phi\\left(\\frac{-w_i}{h}\\right)\n$$\nwhere $\\phi(u)$ is the standard normal probability density function, and $h$ is the bandwidth.\n\nThe bandwidth $h$ is determined using Silverman's rule of thumb:\n$$\nh = 1.06 \\,\\widehat{\\sigma}\\, n^{-1/5}\n$$\nHere, $\\widehat{\\sigma}$ is a robust estimate of the scale of the posterior sample distribution, defined as the minimum of the sample standard deviation and the interquartile range (IQR) divided by $1.34$. The factor $1.34$ makes the IQR-based scale estimate comparable to the standard deviation for normally distributed data.\n\n**Step 3: Local False Discovery Rate (lFDR) Calculation**\nThe Bayes Factor is converted into a more interpretable posterior probability. The local false discovery rate, $\\ell$, is the posterior probability of the null model $\\mathcal{M}_0$ given the data:\n$$\n\\ell = \\Pr(\\mathcal{M}_0 \\mid \\text{data})\n$$\nUsing the relationship between posterior odds, prior odds, and the Bayes Factor, we arrive at the expression:\n$$\n\\ell = \\frac{1}{1 + \\mathrm{BF}_{10} \\times \\frac{\\Pr(\\mathcal{M}_1)}{\\Pr(\\mathcal{M}_0)}} = \\frac{1}{1 + \\mathrm{BF}_{10} \\times \\frac{p_1}{1 - p_1}}\n$$\nwhere $p_1$ is the prior probability of an edge being included in the network ($\\Pr(\\mathcal{M}_1)$). A small $\\ell$ value indicates strong evidence against the null hypothesis (i.e., for the presence of an edge).\n\n**Step 4: Bayesian False Discovery Rate (FDR) Control**\nAfter computing the $\\ell_j$ value for each edge $j=1, \\dots, m$, we apply a procedure to select a set of edges while controlling the expected FDR at a target level $q$. The steps are:\n1. Sort the edges based on their $\\ell_j$ values in ascending order: $\\ell_{(1)} \\le \\ell_{(2)} \\le \\cdots \\le \\ell_{(m)}$.\n2. Find the largest integer $k \\ge 1$ such that the average of the $k$ smallest $\\ell$ values does not exceed the target FDR $q$:\n$$\n\\frac{1}{k} \\sum_{i=1}^k \\ell_{(i)} \\le q\n$$\n3. If such a $k$ exists, the selected edges are those corresponding to the first $k$ sorted $\\ell$ values: $\\{\\ell_{(1)}, \\dots, \\ell_{(k)}\\}$. If no such $k \\ge 1$ exists, no edges are selected.\n\nThe final output for each test case is a list of the 0-based indices of the selected edges, sorted in increasing order. The results from all test cases are then aggregated into a single formatted string as specified.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport math\n\ndef solve():\n    \"\"\"\n    Solves all test cases for Bayesian FDR control in a gene regulatory network.\n    \"\"\"\n    test_cases = [\n        (17, 5, 5000, 1.0, 0.2, 0.1, [(1.5, 0.25), (-2.0, 0.25), (0.0, 0.2), (0.2, 0.2), (-0.1, 0.2)]),\n        (23, 6, 6000, 2.0, 0.5, 0.2, [(0.0, 0.4), (0.6, 0.3), (-0.6, 0.3), (1.2, 0.35), (0.3, 0.25), (-1.0, 0.5)]),\n        (5, 5, 7000, 1.5, 0.7, 0.05, [(0.0, 0.6), (0.2, 0.6), (-0.1, 0.5), (0.3, 0.7), (-0.2, 0.7)]),\n    ]\n\n    results = []\n    for case in test_cases:\n        seed, num_edges, n, tau, p1, q, posterior_params = case\n        \n        rng = np.random.default_rng(seed)\n        \n        l_values = []\n        for j in range(num_edges):\n            mu, sigma = posterior_params[j]\n            samples = rng.normal(loc=mu, scale=sigma, size=n)\n            \n            # --- Step 1: Calculate bandwidth h for KDE ---\n            std_dev = np.std(samples, ddof=1)\n            q75, q25 = np.percentile(samples, [75, 25])\n            iqr = q75 - q25\n            \n            sigma_hat = min(std_dev, iqr / 1.34)\n            \n            h = 1.06 * sigma_hat * (n ** -0.2)\n            if h == 0.0:\n                h = 1e-8 # Failsafe for numerically zero bandwidth\n\n            # --- Step 2: Estimate posterior density at w=0 using KDE ---\n            # phi(u) = (1/sqrt(2pi)) * exp(-u^2/2)\n            # Due to symmetry of Gaussian kernel, phi(-u) = phi(u)\n            phi_terms = (1.0 / math.sqrt(2 * math.pi)) * np.exp(-0.5 * (samples / h) ** 2)\n            p_hat_0 = np.mean(phi_terms) / h\n            \n            # --- Step 3: Calculate Bayes Factor and local FDR ---\n            l_val = 0.0\n            if p_hat_0 > 0:\n                p_prior_0 = 1.0 / (math.sqrt(2 * math.pi) * tau)\n                bf_10 = p_prior_0 / p_hat_0\n                prior_odds = p1 / (1.0 - p1)\n                l_val = 1.0 / (1.0 + bf_10 * prior_odds)\n            \n            l_values.append(l_val)\n            \n        # --- Step 4: Bayesian FDR control ---\n        ell_with_indices = sorted([(l_values[j], j) for j in range(num_edges)])\n        \n        sorted_l = np.array([item[0] for item in ell_with_indices])\n        \n        k_max = 0\n        cumulative_mean_l = np.cumsum(sorted_l) / np.arange(1, num_edges + 1)\n        \n        k_satisfying_indices = np.where(cumulative_mean_l <= q)[0]\n        \n        if len(k_satisfying_indices) > 0:\n            k_max = k_satisfying_indices[-1] + 1\n            \n        selected_edges = []\n        if k_max > 0:\n            original_indices = [item[1] for item in ell_with_indices[:k_max]]\n            selected_edges = sorted(original_indices)\n            \n        results.append(selected_edges)\n\n    # --- Final Output Formatting ---\n    print(f\"[{','.join(str(r).replace(' ', '') for r in results)}]\")\n\nsolve()\n```", "id": "3314570"}]}