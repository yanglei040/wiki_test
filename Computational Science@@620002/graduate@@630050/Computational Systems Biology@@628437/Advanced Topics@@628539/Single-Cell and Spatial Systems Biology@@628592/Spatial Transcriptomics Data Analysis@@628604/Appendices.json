{"hands_on_practices": [{"introduction": "A robust analysis of spatial transcriptomics data begins with a solid understanding of its underlying statistical properties. The raw measurements are counts of molecules, which are inherently discrete and stochastic. This exercise [@problem_id:3350175] walks you through a first-principles derivation of the expected UMI counts per spot and, crucially, explores how variance is modeled. By comparing a simple Poisson model with a more realistic gamma-Poisson mixture (which gives rise to the Negative Binomial distribution), you will gain a deep intuition for the phenomenon of overdispersion—where data is more variable than the mean—a defining characteristic of sequencing data that fundamentally influences the choice of statistical methods.", "problem": "Consider a spot-based spatial transcriptomics assay measuring Unique Molecular Identifiers (UMIs) on a tissue section. Assume the tissue has uniform cell density $\\lambda$ (cells per unit area), and each spot has area $A$. Let the number of cells per spot be modeled by a spatial Poisson point process so that the spot-level cell count $N$ has $\\mathbb{E}[N]=\\lambda A$. Within each cell, the number of transcribed molecules is modeled as an independent Poisson random variable with mean $\\mu$, and each transcribed molecule is captured independently with probability $\\eta$ (capture efficiency), yielding an expected captured count per cell of $\\eta \\mu$. Let $Y$ denote the total captured UMI count per spot.\n\n(a) Using only linearity of expectation and independence of the capture process, derive $\\mathbb{E}[Y]$ from the above assumptions.\n\n(b) Under the modeling assumption that $Y$ follows a Poisson distribution with mean equal to the expectation derived in part (a), compute $\\operatorname{Var}(Y)$.\n\n(c) Now introduce spot-level heterogeneity via a gamma-Poisson mixture: let the latent pre-capture spot-level transcript intensity be $\\Lambda \\sim \\mathrm{Gamma}(\\kappa,\\theta)$ with shape parameter $\\kappa>0$ and scale $\\theta$ chosen so that $\\mathbb{E}[\\Lambda]=\\lambda A \\mu$. Conditional on $\\Lambda$, assume $Y \\mid \\Lambda \\sim \\mathrm{Poisson}(\\eta \\Lambda)$. Using the law of total expectation and the law of total variance, compute $\\mathbb{E}[Y]$ and $\\operatorname{Var}(Y)$ under this gamma-Poisson mixture. Express your final answers symbolically in terms of $\\eta$, $\\lambda$, $A$, $\\mu$, and $\\kappa$.\n\nProvide the final answer as a single row vector containing, in order: the mean $\\mathbb{E}[Y]$, the variance under the Poisson assumption, and the variance under the gamma-Poisson (negative binomial) assumption. No rounding is required, and no physical units should be included in your final expression.", "solution": "The user-provided problem is assessed to be valid. It is scientifically grounded, well-posed, objective, and internally consistent, representing a standard exercise in statistical modeling relevant to computational systems biology. We may therefore proceed with a full solution.\n\nThe problem is divided into three parts. We will solve each part sequentially to find the required quantities for the final answer. The required quantities are the expectation of the total UMI count per spot, $\\mathbb{E}[Y]$, the variance of $Y$ under a simple Poisson assumption, and the variance of $Y$ under a more complex gamma-Poisson mixture model.\n\n(a) Derivation of $\\mathbb{E}[Y]$ using linearity of expectation.\n\nLet $N$ be the number of cells in a spot. We are given that $N$ follows a spatial Poisson point process such that its expectation is $\\mathbb{E}[N] = \\lambda A$, where $\\lambda$ is the cell density and $A$ is the spot area.\nLet $X_i$ be the number of captured UMIs from cell $i$, for $i = 1, \\dots, N$.\nThe total captured UMI count per spot, $Y$, is the sum of the counts from all cells in that spot:\n$$Y = \\sum_{i=1}^{N} X_i$$\nThis is a sum of a random number of random variables. We can find its expectation using the law of total expectation (also known as Wald's identity in this context):\n$$\\mathbb{E}[Y] = \\mathbb{E}[\\mathbb{E}[Y \\mid N]]$$\nFirst, we find the conditional expectation of $Y$ given the number of cells, $N$. For a fixed number of cells $N=n$, $Y = \\sum_{i=1}^{n} X_i$. By the linearity of expectation:\n$$\\mathbb{E}[Y \\mid N=n] = \\mathbb{E}\\left[\\sum_{i=1}^{n} X_i\\right] = \\sum_{i=1}^{n} \\mathbb{E}[X_i]$$\nWe need to determine $\\mathbb{E}[X_i]$. The problem states that the number of transcribed molecules in a cell is a Poisson random variable with mean $\\mu$. Each of these molecules is then captured with an independent probability $\\eta$. This is a process known as thinning a Poisson distribution. If a random variable $K \\sim \\mathrm{Poisson}(\\mu)$, and each of the $K$ items is kept with probability $\\eta$, the resulting count of kept items, $X_i$, follows a Poisson distribution with mean $\\eta\\mu$.\nThus, the expected number of captured UMIs per cell is $\\mathbb{E}[X_i] = \\eta\\mu$.\nSince the process is identical for each cell, all $X_i$ are independent and identically distributed. So:\n$$\\mathbb{E}[Y \\mid N=n] = \\sum_{i=1}^{n} (\\eta\\mu) = n\\eta\\mu$$\nThis shows that the conditional expectation $\\mathbb{E}[Y \\mid N]$ is the random variable $N\\eta\\mu$.\nNow, we take the expectation over $N$:\n$$\\mathbb{E}[Y] = \\mathbb{E}[N\\eta\\mu]$$\nUsing linearity of expectation again:\n$$\\mathbb{E}[Y] = \\eta\\mu \\mathbb{E}[N]$$\nSubstituting the given $\\mathbb{E}[N] = \\lambda A$:\n$$\\mathbb{E}[Y] = \\lambda A \\eta \\mu$$\n\n(b) Computation of $\\operatorname{Var}(Y)$ under a simple Poisson model.\n\nFor this part, we are asked to assume that the total UMI count $Y$ follows a Poisson distribution with a mean equal to the expectation derived in part (a).\nLet the parameter of this Poisson distribution be $\\nu = \\mathbb{E}[Y]$. From part (a), we have:\n$$\\nu = \\lambda A \\eta \\mu$$\nSo, we assume $Y \\sim \\mathrm{Poisson}(\\nu) = \\mathrm{Poisson}(\\lambda A \\eta \\mu)$.\nA fundamental property of the Poisson distribution is that its variance is equal to its mean.\nTherefore, the variance of $Y$ under this assumption is:\n$$\\operatorname{Var}(Y) = \\nu = \\lambda A \\eta \\mu$$\n\n(c) Computation of $\\mathbb{E}[Y]$ and $\\operatorname{Var}(Y)$ under a gamma-Poisson mixture model.\n\nIn this model, the total UMI count $Y$ is modeled hierarchically.\nFirst, a latent variable $\\Lambda$, representing the total pre-capture transcript intensity in a spot, is drawn from a gamma distribution:\n$$\\Lambda \\sim \\mathrm{Gamma}(\\kappa, \\theta)$$\nwhere $\\kappa > 0$ is the shape parameter and $\\theta$ is the scale parameter.\nThe scale parameter $\\theta$ is specified by the constraint on the expectation of $\\Lambda$:\n$$\\mathbb{E}[\\Lambda] = \\lambda A \\mu$$\nThe expectation of a $\\mathrm{Gamma}(\\kappa, \\theta)$ distribution is $\\mathbb{E}[\\Lambda] = \\kappa\\theta$. Equating this with the constraint gives:\n$$\\kappa\\theta = \\lambda A \\mu \\implies \\theta = \\frac{\\lambda A \\mu}{\\kappa}$$\nConditional on this latent intensity $\\Lambda$, the observed UMI count $Y$ follows a Poisson distribution with mean $\\eta\\Lambda$:\n$$Y \\mid \\Lambda \\sim \\mathrm{Poisson}(\\eta\\Lambda)$$\nThis hierarchical structure is a gamma-Poisson mixture, which results in $Y$ following a negative binomial distribution. We are asked to compute $\\mathbb{E}[Y]$ and $\\operatorname{Var}(Y)$ using the laws of total expectation and total variance.\n\nFirst, the expectation of $Y$:\n$$\\mathbb{E}[Y] = \\mathbb{E}[\\mathbb{E}[Y \\mid \\Lambda]]$$\nThe inner expectation is the mean of the conditional Poisson distribution:\n$$\\mathbb{E}[Y \\mid \\Lambda] = \\eta\\Lambda$$\nNow, taking the expectation over $\\Lambda$:\n$$\\mathbb{E}[Y] = \\mathbb{E}[\\eta\\Lambda] = \\eta\\mathbb{E}[\\Lambda]$$\nUsing the given constraint $\\mathbb{E}[\\Lambda] = \\lambda A \\mu$:\n$$\\mathbb{E}[Y] = \\eta(\\lambda A \\mu) = \\lambda A \\eta \\mu$$\nThis result is consistent with the one obtained in part (a).\n\nNext, the variance of $Y$:\nThe law of total variance states:\n$$\\operatorname{Var}(Y) = \\mathbb{E}[\\operatorname{Var}(Y \\mid \\Lambda)] + \\operatorname{Var}(\\mathbb{E}[Y \\mid \\Lambda])$$\nWe compute each term separately.\nThe first term is the expected conditional variance. The variance of the conditional Poisson distribution $Y \\mid \\Lambda \\sim \\mathrm{Poisson}(\\eta\\Lambda)$ is equal to its mean:\n$$\\operatorname{Var}(Y \\mid \\Lambda) = \\eta\\Lambda$$\nTaking the expectation of this quantity over $\\Lambda$:\n$$\\mathbb{E}[\\operatorname{Var}(Y \\mid \\Lambda)] = \\mathbb{E}[\\eta\\Lambda] = \\eta\\mathbb{E}[\\Lambda] = \\eta(\\lambda A \\mu) = \\lambda A \\eta \\mu$$\nThe second term is the variance of the conditional expectation. We already found the conditional expectation:\n$$\\mathbb{E}[Y \\mid \\Lambda] = \\eta\\Lambda$$\nThe variance of this quantity is:\n$$\\operatorname{Var}(\\mathbb{E}[Y \\mid \\Lambda]) = \\operatorname{Var}(\\eta\\Lambda) = \\eta^2 \\operatorname{Var}(\\Lambda)$$\nWe need $\\operatorname{Var}(\\Lambda)$. For a $\\mathrm{Gamma}(\\kappa, \\theta)$ distribution, the variance is $\\operatorname{Var}(\\Lambda) = \\kappa\\theta^2$. Substituting $\\theta = \\frac{\\lambda A \\mu}{\\kappa}$:\n$$\\operatorname{Var}(\\Lambda) = \\kappa \\left(\\frac{\\lambda A \\mu}{\\kappa}\\right)^2 = \\kappa \\frac{(\\lambda A \\mu)^2}{\\kappa^2} = \\frac{(\\lambda A \\mu)^2}{\\kappa}$$\nTherefore, the second term is:\n$$\\operatorname{Var}(\\mathbb{E}[Y \\mid \\Lambda]) = \\eta^2 \\left(\\frac{(\\lambda A \\mu)^2}{\\kappa}\\right) = \\frac{\\eta^2 (\\lambda A \\mu)^2}{\\kappa}$$\nFinally, we sum the two terms to get the total variance:\n$$\\operatorname{Var}(Y) = \\lambda A \\eta \\mu + \\frac{\\eta^2 (\\lambda A \\mu)^2}{\\kappa}$$\nThis expression shows that the variance under the gamma-Poisson model is greater than the mean ($\\lambda A \\eta \\mu$), a phenomenon known as overdispersion. The term $\\frac{\\eta^2 (\\lambda A \\mu)^2}{\\kappa}$ quantifies this extra variance.\n\nSummary of results:\n1.  Mean of $Y$: $\\mathbb{E}[Y] = \\lambda A \\eta \\mu$.\n2.  Variance of $Y$ under the Poisson assumption: $\\operatorname{Var}(Y) = \\lambda A \\eta \\mu$.\n3.  Variance of $Y$ under the gamma-Poisson assumption: $\\operatorname{Var}(Y) = \\lambda A \\eta \\mu + \\frac{\\eta^2 (\\lambda A \\mu)^2}{\\kappa}$.\n\nThese three expressions will form the final row vector.", "answer": "$$\\boxed{\\begin{pmatrix} \\lambda A \\eta \\mu & \\lambda A \\eta \\mu & \\lambda A \\eta \\mu + \\frac{\\eta^{2} (\\lambda A \\mu)^{2}}{\\kappa} \\end{pmatrix}}$$", "id": "3350175"}, {"introduction": "Moving from the statistics of individual spots to the relationships between them is the essence of spatial analysis. To uncover spatial patterns, we must first mathematically encode the notion of \"space\" itself. This practice [@problem_id:3350213] delves into the most common and powerful approach: representing the tissue as a weighted neighborhood graph. You will implement the construction of the graph Laplacian, a central matrix in spectral graph theory, which captures the connectivity and geometry of the spatial locations. By building and verifying the fundamental properties of this matrix, you will acquire the foundational coding skills necessary for a wide array of downstream applications, including spatial clustering, data smoothing, and identifying domains of gene activity.", "problem": "Consider a spatial transcriptomics setting where spatial spots are represented in a two-dimensional plane and relationships between nearby spots are modeled as an undirected weighted graph. For a graph with $n$ nodes labeled $\\{0,1,\\dots,n-1\\}$, let $W \\in \\mathbb{R}^{n \\times n}$ be a symmetric weighted adjacency matrix with entries $w_{ij}$ satisfying $w_{ii} = 0$ for all $i$. For a specified set of undirected neighbor pairs $\\mathcal{E} \\subset \\{(i,j) \\mid i \\neq j\\}$ and coordinates $\\{(x_i, y_i)\\}_{i=0}^{n-1}$ for each node, define the Euclidean distance $d_{ij} = \\sqrt{(x_i - x_j)^2 + (y_i - y_j)^2}$. For $(i,j) \\in \\mathcal{E}$, define weights by the radial basis function $w_{ij} = \\exp\\left(-\\frac{d_{ij}^2}{\\sigma^2}\\right)$ and set $w_{ij} = 0$ otherwise. Let $D \\in \\mathbb{R}^{n \\times n}$ be the diagonal degree matrix with entries $D_{ii} = \\sum_{j=0}^{n-1} w_{ij}$. Define the unnormalized graph Laplacian $L = D - W$. Define the symmetric normalized Laplacian $L_{\\text{sym}} = I - D^{-1/2} W D^{-1/2}$, where $I$ is the $n \\times n$ identity matrix and $D^{-1/2}$ is defined by $(D^{-1/2})_{ii} = d_i^{-1/2}$ if $d_i = D_{ii} > 0$ and $(D^{-1/2})_{ii} = 0$ if $d_i = 0$.\n\nFrom first principles of weighted graphs and diffusion on networks, spectral properties of $L$ and $L_{\\text{sym}}$ include nonnegativity of all eigenvalues and, for $L_{\\text{sym}}$, an eigenvalue upper bound of $2$. The multiplicity of the zero eigenvalue of both $L$ and $L_{\\text{sym}}$ equals the number of connected components. The sum of the eigenvalues of $L$ equals $\\operatorname{trace}(L)$.\n\nYour task is to implement a complete program that:\n- Constructs $W$ using $w_{ij} = \\exp\\left(-\\frac{d_{ij}^2}{\\sigma^2}\\right)$ for $(i,j) \\in \\mathcal{E}$, with $w_{ij} = w_{ji}$ and $w_{ii} = 0$.\n- Computes $D$, $L = D - W$, and $L_{\\text{sym}} = I - D^{-1/2} W D^{-1/2}$ with the stated convention for $D^{-1/2}$ when $D_{ii} = 0$.\n- Computes the full eigenvalue spectra of $L$ and $L_{\\text{sym}}$.\n- Verifies the following for each test case:\n  1. Nonnegativity of the spectrum of $L$: all eigenvalues $\\lambda$ of $L$ satisfy $\\lambda \\geq 0$ within a numerical tolerance $\\varepsilon = 10^{-8}$.\n  2. Nonnegativity of the spectrum of $L_{\\text{sym}}$: all eigenvalues $\\lambda$ of $L_{\\text{sym}}$ satisfy $\\lambda \\geq 0$ within tolerance $\\varepsilon$.\n  3. Upper bound of the spectrum of $L_{\\text{sym}}$: the maximum eigenvalue $\\lambda_{\\max}$ of $L_{\\text{sym}}$ satisfies $\\lambda_{\\max} \\leq 2$ within tolerance $\\varepsilon$.\n  4. Multiplicity of the zero eigenvalue of $L$: the count of eigenvalues of $L$ equal to $0$ within tolerance $\\varepsilon$ (this is an integer output to be reported).\n  5. Multiplicity of the zero eigenvalue of $L_{\\text{sym}}$: the count of eigenvalues of $L_{\\text{sym}}$ equal to $0$ within tolerance $\\varepsilon$ (this is an integer output to be reported).\n  6. Trace-eigenvalue equality for $L$: verify that $\\left|\\sum_{i=1}^{n} \\lambda_i(L) - \\operatorname{trace}(L)\\right| \\leq \\varepsilon$, and output a boolean accordingly.\n\nUse the following test suite. For each case, the coordinates, neighbor pairs $\\mathcal{E}$, and $\\sigma$ are specified. Distances $d_{ij}$ must be computed from the coordinates for each pair in $\\mathcal{E}$.\n\nTest case $1$ (connected chain; happy path):\n- $n = 4$\n- Coordinates: $(0,0)$, $(1,0)$, $(2,0)$, $(3,0)$\n- Neighbor pairs $\\mathcal{E}$: $(0,1)$, $(1,2)$, $(2,3)$\n- $\\sigma = 1.0$\n\nTest case $2$ (two disconnected components):\n- $n = 6$\n- Coordinates: $(0,0)$, $(1,0)$, $(2,0)$, $(10,0)$, $(11,0)$, $(12,0)$\n- Neighbor pairs $\\mathcal{E}$: $(0,1)$, $(1,2)$, $(3,4)$, $(4,5)$\n- $\\sigma = 0.5$\n\nTest case $3$ (triangle plus isolated node; zero-degree handling):\n- $n = 4$\n- Coordinates: $(0,0)$, $(1,0)$, $(0,1)$, $(5,5)$\n- Neighbor pairs $\\mathcal{E}$: $(0,1)$, $(0,2)$, $(1,2)$\n- $\\sigma = 2.0$\n\nTest case $4$ ($2 \\times 2$ grid; large smoothing scale):\n- $n = 4$\n- Coordinates: $(0,0)$, $(0,1)$, $(1,0)$, $(1,1)$\n- Neighbor pairs $\\mathcal{E}$: $(0,1)$, $(0,2)$, $(1,3)$, $(2,3)$\n- $\\sigma = 10.0$\n\nNumerical tolerance is $\\varepsilon = 10^{-8}$. Angles do not appear, and there are no physical units in this problem.\n\nFinal output format:\n- Your program should produce a single line of output containing the concatenated results for all test cases as a comma-separated list enclosed in square brackets.\n- For each test case, append the following six values to the output list in this order:\n  1. $z_L$: the integer count of eigenvalues of $L$ equal to $0$ within tolerance $\\varepsilon$.\n  2. $z_{L_{\\text{sym}}}$: the integer count of eigenvalues of $L_{\\text{sym}}$ equal to $0$ within tolerance $\\varepsilon$.\n  3. $b_{L,\\text{nonneg}}$: a boolean indicating whether all eigenvalues of $L$ are $\\geq 0$ within tolerance.\n  4. $b_{L_{\\text{sym}},\\text{nonneg}}$: a boolean indicating whether all eigenvalues of $L_{\\text{sym}}$ are $\\geq 0$ within tolerance.\n  5. $b_{L_{\\text{sym}},\\max \\le 2}$: a boolean indicating whether $\\max \\lambda(L_{\\text{sym}}) \\leq 2$ within tolerance.\n  6. $b_{\\text{trace-eigs}}$: a boolean indicating whether $\\left|\\sum_{i=1}^{n} \\lambda_i(L) - \\operatorname{trace}(L)\\right| \\leq \\varepsilon$.\n- Therefore, with $4$ test cases, the output list must contain $24$ entries in total, in the order of test cases $1$ through $4$.\n\nYour program must implement all computations explicitly and produce the single-line output described above.", "solution": "The problem requires the implementation of an algorithm to construct and analyze graph Laplacian matrices derived from spatial data, a common task in computational systems biology for analyzing spatial transcriptomics datasets. The solution involves constructing two types of graph Laplacians, the unnormalized Laplacian $L$ and the symmetric normalized Laplacian $L_{\\text{sym}}$, computing their eigenvalue spectra, and verifying several fundamental properties of these spectra against established theorems from spectral graph theory.\n\nThe procedure is executed for several test cases, each defined by a set of nodes with 2D coordinates $\\{(x_i, y_i)\\}_{i=0}^{n-1}$, a set of neighbor pairs $\\mathcal{E}$, and a length-scale parameter $\\sigma$.\n\n**Step 1: Construction of the Weighted Adjacency Matrix $W$**\n\nThe graph is represented by an $n \\times n$ weighted adjacency matrix $W$. The entries $w_{ij}$ of this matrix quantify the strength of the connection between nodes $i$ and $j$.\n- The graph is undirected, so $W$ is symmetric, i.e., $w_{ij} = w_{ji}$.\n- By convention, there are no self-loops, so the diagonal entries are zero: $w_{ii} = 0$ for all $i \\in \\{0, \\dots, n-1\\}$.\n- For a pair of nodes $(i,j)$ specified in the neighbor set $\\mathcal{E}$, the weight $w_{ij}$ is calculated using a radial basis function (Gaussian kernel), which depends on the Euclidean distance $d_{ij} = \\sqrt{(x_i - x_j)^2 + (y_i - y_j)^2}$ between the nodes:\n$$ w_{ij} = \\exp\\left(-\\frac{d_{ij}^2}{\\sigma^2}\\right) $$\nThis function ensures that closer nodes have a stronger connection (larger weight), with the parameter $\\sigma$ controlling the spatial scale of influence.\n- If a pair $(i,j)$ is not in $\\mathcal{E}$, there is no direct edge between them, and their weight is set to zero: $w_{ij} = 0$.\n\n**Step 2: Construction of the Graph Laplacians $L$ and $L_{\\text{sym}}$**\n\nFrom the adjacency matrix $W$, two Laplacian matrices are derived.\n\n- **Degree Matrix $D$**: First, we define the degree matrix $D$, which is a diagonal matrix. Each diagonal entry $D_{ii}$ is the degree of node $i$, defined as the sum of weights of all edges connected to it:\n$$ D_{ii} = d_i = \\sum_{j=0}^{n-1} w_{ij} $$\n- **Unnormalized Graph Laplacian $L$**: This Laplacian is defined as the difference between the degree matrix and the adjacency matrix:\n$$ L = D - W $$\nThe matrix $L$ is guaranteed to be real and symmetric, since $D$ is diagonal and $W$ is symmetric.\n\n- **Symmetric Normalized Graph Laplacian $L_{\\text{sym}}$**: This Laplacian is a normalized version of $L$ that accounts for differences in node degrees. It is defined as:\n$$ L_{\\text{sym}} = I - D^{-1/2} W D^{-1/2} $$\nwhere $I$ is the $n \\times n$ identity matrix. The matrix $D^{-1/2}$ is a diagonal matrix whose entries are given by:\n$$ (D^{-1/2})_{ii} = \\begin{cases} d_i^{-1/2} & \\text{if } d_i > 0 \\\\ 0 & \\text{if } d_i = 0 \\end{cases} $$\nThis special handling for nodes with zero degree ($d_i=0$) is crucial. These are isolated nodes, and this definition ensures they are properly handled in the mathematical framework, effectively decoupling them and leading to a well-defined $L_{\\text{sym}}$. Like $L$, $L_{\\text{sym}}$ is also real and symmetric.\n\n**Step 3: Spectral Analysis and Property Verification**\n\nThe core of the task is to compute the eigenvalues of $L$ and $L_{\\text{sym}}$ and verify their theoretical properties. Since both matrices are real and symmetric, their eigenvalues are all real. The `numpy.linalg.eigh` function is ideally suited for this, as it is optimized for Hermitian (real-symmetric) matrices and guarantees real-valued results. Let $\\lambda_k(M)$ denote the $k$-th eigenvalue of a matrix $M$. The following six properties are verified for each test case, using a numerical tolerance of $\\varepsilon = 10^{-8}$.\n\n1.  **Multiplicity of Zero Eigenvalue of $L$ ($z_L$)**: The number of eigenvalues $\\lambda_k(L)$ such that $|\\lambda_k(L)| \\leq \\varepsilon$ is counted.\n2.  **Multiplicity of Zero Eigenvalue of $L_{\\text{sym}}$ ($z_{L_{\\text{sym}}}$)**: The number of eigenvalues $\\lambda_k(L_{\\text{sym}})$ such that $|\\lambda_k(L_{\\text{sym}})| \\leq \\varepsilon$ is counted.\n    - *Theoretical Principle*: A fundamental theorem of spectral graph theory states that for both $L$ and $L_{\\text{sym}}$, the multiplicity of the eigenvalue $0$ is equal to the number of connected components in the graph. These two outputs are thus expected to be integers corresponding to this count.\n3.  **Non-negativity of $L$'s Spectrum ($b_{L,\\text{nonneg}}$)**: This boolean check verifies if all eigenvalues of $L$ are non-negative, i.e., $\\min_k(\\lambda_k(L)) \\geq -\\varepsilon$.\n4.  **Non-negativity of $L_{\\text{sym}}$'s Spectrum ($b_{L_{\\text{sym}},\\text{nonneg}}$)**: This boolean check verifies if all eigenvalues of $L_{\\text{sym}}$ are non-negative, i.e., $\\min_k(\\lambda_k(L_{\\text{sym}})) \\geq -\\varepsilon$.\n    - *Theoretical Principle*: For any graph with non-negative edge weights $w_{ij} \\geq 0$, both $L$ and $L_{\\text{sym}}$ are positive semi-definite matrices. This property mathematically implies that all of their eigenvalues must be real and non-negative.\n5.  **Upper Bound on $L_{\\text{sym}}$'s Spectrum ($b_{L_{\\text{sym}},\\max \\le 2}$)**: This boolean check verifies if the maximum eigenvalue of $L_{\\text{sym}}$ is at most $2$, i.e., $\\max_k(\\lambda_k(L_{\\text{sym}})) \\leq 2 + \\varepsilon$.\n    - *Theoretical Principle*: It is another established result that for any graph with non-negative weights, all eigenvalues of $L_{\\text{sym}}$ are bounded above by $2$.\n6.  **Trace-Eigenvalue Sum Equality for $L$ ($b_{\\text{trace-eigs}}$)**: This boolean check verifies if the sum of $L$'s eigenvalues equals its trace, i.e., $|\\sum_{k=0}^{n-1} \\lambda_k(L) - \\operatorname{trace}(L)| \\leq \\varepsilon$.\n    - *Theoretical Principle*: This is a general property for any square matrix. The trace of a matrix is invariant under change of basis and is always equal to the sum of its eigenvalues. This serves as a numerical sanity check for the eigenvalue computation.\n\nThe algorithm is systematically applied to each test case, and the six resulting values ($z_L$, $z_{L_{\\text{sym}}}$, and the four booleans) are concatenated into a single list for the final output.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all test cases.\n    It defines the test cases, processes each one, and prints the final formatted result.\n    \"\"\"\n    \n    # Numerical tolerance for floating-point comparisons\n    epsilon = 1e-8\n\n    # Define the test cases as specified in the problem statement.\n    test_cases = [\n        {\n            \"n\": 4,\n            \"coords\": np.array([(0, 0), (1, 0), (2, 0), (3, 0)]),\n            \"neighbor_pairs\": [(0, 1), (1, 2), (2, 3)],\n            \"sigma\": 1.0,\n        },\n        {\n            \"n\": 6,\n            \"coords\": np.array([(0, 0), (1, 0), (2, 0), (10, 0), (11, 0), (12, 0)]),\n            \"neighbor_pairs\": [(0, 1), (1, 2), (3, 4), (4, 5)],\n            \"sigma\": 0.5,\n        },\n        {\n            \"n\": 4,\n            \"coords\": np.array([(0, 0), (1, 0), (0, 1), (5, 5)]),\n            \"neighbor_pairs\": [(0, 1), (0, 2), (1, 2)],\n            \"sigma\": 2.0,\n        },\n        {\n            \"n\": 4,\n            \"coords\": np.array([(0, 0), (0, 1), (1, 0), (1, 1)]),\n            \"neighbor_pairs\": [(0, 1), (0, 2), (1, 3), (2, 3)],\n            \"sigma\": 10.0,\n        },\n    ]\n\n    all_results = []\n    for case in test_cases:\n        results = process_case(case['n'], case['coords'], case['neighbor_pairs'], case['sigma'], epsilon)\n        all_results.extend(results)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\ndef process_case(n, coords, neighbor_pairs, sigma, epsilon):\n    \"\"\"\n    Processes a single test case to compute and verify graph Laplacian properties.\n    \"\"\"\n    \n    # Step 1: Construct the weighted adjacency matrix W\n    W = np.zeros((n, n), dtype=np.float64)\n    sigma_sq = sigma**2\n    for i, j in neighbor_pairs:\n        dist_sq = np.sum((coords[i] - coords[j])**2)\n        weight = np.exp(-dist_sq / sigma_sq)\n        W[i, j] = weight\n        W[j, i] = weight\n\n    # Step 2: Construct D, L, and L_sym\n    # Degree matrix D\n    d_vec = np.sum(W, axis=1)\n    D = np.diag(d_vec)\n\n    # Unnormalized Laplacian L\n    L = D - W\n\n    # Symmetric normalized Laplacian L_sym\n    # First, compute D^{-1/2} carefully handling zero-degree nodes\n    d_inv_sqrt_vec = np.zeros(n, dtype=np.float64)\n    # Find indices where degree is positive\n    positive_degree_indices = d_vec > 0\n    # Compute d^{-1/2} only for positive degrees\n    d_inv_sqrt_vec[positive_degree_indices] = d_vec[positive_degree_indices]**(-0.5)\n    D_inv_sqrt = np.diag(d_inv_sqrt_vec)\n    \n    I = np.identity(n)\n    L_sym = I - D_inv_sqrt @ W @ D_inv_sqrt\n\n    # Step 3: Compute Eigenvalues\n    # Use eigh for symmetric matrices; it's more stable and guarantees real eigenvalues.\n    # It returns eigenvalues sorted in non-decreasing order.\n    eigs_L = np.linalg.eigh(L)[0]\n    eigs_L_sym = np.linalg.eigh(L_sym)[0]\n\n    # Step 4: Perform Verifications\n    \n    # 1. Multiplicity of zero eigenvalue of L\n    z_L = np.sum(np.abs(eigs_L) <= epsilon)\n    \n    # 2. Multiplicity of zero eigenvalue of L_sym\n    z_L_sym = np.sum(np.abs(eigs_L_sym) <= epsilon)\n    \n    # 3. Nonnegativity of L's spectrum\n    b_L_nonneg = bool(eigs_L[0] >= -epsilon)\n    \n    # 4. Nonnegativity of L_sym's spectrum\n    b_L_sym_nonneg = bool(eigs_L_sym[0] >= -epsilon)\n    \n    # 5. Upper bound of L_sym's spectrum\n    b_L_sym_max_le_2 = bool(eigs_L_sym[-1] <= 2 + epsilon)\n    \n    # 6. Trace-eigenvalue equality for L\n    trace_L = np.trace(L)\n    sum_eigs_L = np.sum(eigs_L)\n    b_trace_eigs = bool(np.abs(sum_eigs_L - trace_L) <= epsilon)\n\n    return [z_L, z_L_sym, b_L_nonneg, b_L_sym_nonneg, b_L_sym_max_le_2, b_trace_eigs]\n\n# Execute the main function\nsolve()\n```", "id": "3350213"}, {"introduction": "This final practice synthesizes statistical modeling and spatial representation to tackle a critical, high-level challenge: optimal experimental design. Given that spatial transcriptomics experiments involve significant financial and resource investment, making informed decisions before data collection is paramount. This exercise [@problem_id:3350152] places you in the role of a study designer, tasked with allocating a fixed budget between the number of spatial spots and the sequencing depth per spot. You will develop a model for statistical power based on the Negative Binomial distribution and use it to find the experimental parameters that maximize your ability to detect spatially variable genes, providing a concrete example of how computational modeling directly informs and improves scientific strategy.", "problem": "You are designing a spatial transcriptomics study where the total cost trades off the number of spots and the sequencing depth per spot. Let $n$ be the number of spatial spots and $s$ be the sequencing depth per spot. The total cost is modeled as $C = c_n n + c_s n s$, where $c_n$ is the per-spot setup cost and $c_s$ is the per-read (or per-depth unit) cost. You must choose integer $n$ and integer $s$ subject to a fixed budget $C \\leq C_{\\max}$ and minimum feasibility constraints $n \\geq n_{\\min}$ and $s \\geq s_{\\min}$.\n\nModeling assumptions and definitions:\n- For each gene $g$ in a specified set $\\mathcal{G}$, counts per spot follow a Negative Binomial (NB) distribution with mean $\\mu_{g i}$ and variance $\\mu_{g i} + \\phi_g \\mu_{g i}^2$, where $\\phi_g \\geq 0$ is the gene-specific dispersion.\n- A Generalized Linear Model (GLM) with a log link is used to detect spatial signal via a single standardized spatial contrast $z_i$ across the $n$ spots, obeying $\\sum_{i=1}^{n} z_i = 0$ and $\\sum_{i=1}^{n} z_i^2 = n$. The mean is parameterized as $\\log \\mu_{g i} = \\log(s m_g) + \\theta_g z_i$, where $m_g$ is the gene’s baseline abundance and $\\theta_g$ is its true spatial effect size.\n- Detection for each gene uses a two-sided Wald test at significance level $\\alpha$ within the GLM framework. The null hypothesis is $H_0: \\theta_g = 0$, and the alternative is $H_1: \\theta_g \\neq 0$. Use the standard large-sample normal approximation implied by GLM theory and the Negative Binomial variance specification to derive the expected power as a function of $n$ and $s$. The derivation must start from these definitions, not from any shortcut formula.\n\nObjective:\n- For each provided test case, compute the integer allocation $(n^\\star, s^\\star)$ that maximizes the average expected power across the specified gene set $\\mathcal{G}$, subject to $c_n n + c_s n s \\leq C_{\\max}$, $n \\geq n_{\\min}$, and $s \\geq s_{\\min}$.\n- If multiple allocations achieve the same maximal average expected power up to numerical tolerance, break ties by choosing the largest $n$, and if still tied, the largest $s$.\n- If no allocation is feasible, return $[0,0,0.0]$ for that test case.\n\nTest suite:\n- For each test case, you are given $(C_{\\max}, c_n, c_s, n_{\\min}, s_{\\min}, \\alpha, \\mathcal{G})$, where $\\mathcal{G}$ is provided as a list of triples $(m_g, \\phi_g, \\theta_g)$.\n- Use the following four test cases:\n  1. Test case $1$:\n     - $C_{\\max} = 12000$, $c_n = 100$, $c_s = 1$, $n_{\\min} = 50$, $s_{\\min} = 10$, $\\alpha = 0.05$.\n     - $\\mathcal{G} = \\{(0.02, 0.05, 0.30), (0.05, 0.10, 0.25), (0.10, 0.20, 0.20), (0.20, 0.20, 0.15), (0.50, 0.30, 0.12), (1.00, 0.40, 0.10)\\}$.\n  2. Test case $2$:\n     - $C_{\\max} = 12000$, $c_n = 500$, $c_s = 1$, $n_{\\min} = 10$, $s_{\\min} = 10$, $\\alpha = 0.05$.\n     - $\\mathcal{G} = \\{(0.02, 0.05, 0.30), (0.05, 0.10, 0.25), (0.10, 0.20, 0.20), (0.20, 0.20, 0.15), (0.50, 0.30, 0.12), (1.00, 0.40, 0.10)\\}$.\n  3. Test case $3$:\n     - $C_{\\max} = 12000$, $c_n = 100$, $c_s = 5$, $n_{\\min} = 30$, $s_{\\min} = 5$, $\\alpha = 0.05$.\n     - $\\mathcal{G} = \\{(0.05, 0.20, 0.20), (0.10, 0.30, 0.15), (0.20, 0.30, 0.12), (0.30, 0.40, 0.10)\\}$.\n  4. Test case $4$ (boundary feasibility):\n     - $C_{\\max} = 4400$, $c_n = 200$, $c_s = 2$, $n_{\\min} = 20$, $s_{\\min} = 10$, $\\alpha = 0.01$.\n     - $\\mathcal{G} = \\{(0.10, 0.40, 0.25), (0.20, 0.50, 0.20), (0.40, 0.60, 0.15)\\}$.\n\nProgram requirements:\n- Your program must compute, for each test case, the optimal $(n^\\star, s^\\star)$ and the corresponding average expected power across $\\mathcal{G}$ at two-sided significance $\\alpha$.\n- The expected power for each gene must be computed from first principles using the GLM and Negative Binomial assumptions above with the log link and the standardized spatial contrast, relying on the large-sample Wald test approximation and standard properties of the Normal distribution’s Cumulative Distribution Function (CDF). Do not assume any pre-derived shortcut formulas in your reasoning; derive what you need from the given definitions.\n- The final output format must be a single line containing a comma-separated list of results, where each result is the triple $[n^\\star,s^\\star,\\overline{p}]$ with $\\overline{p}$ equal to the average expected power across $\\mathcal{G}$, rounded to exactly $6$ decimal places. The full line should have no spaces. For example, it should look like $[[n_1,s_1,p_1],[n_2,s_2,p_2],[n_3,s_3,p_3],[n_4,s_4,p_4]]$.", "solution": "We begin from the Negative Binomial (NB) distribution and Generalized Linear Model (GLM) framework. For each gene $g$, counts $Y_{g i}$ at spot $i$ have mean $\\mu_{g i}$ and variance $\\operatorname{Var}(Y_{g i}) = \\mu_{g i} + \\phi_g \\mu_{g i}^2$, with $\\phi_g \\geq 0$. We use a log link with a single standardized spatial contrast $z_i$ across $n$ spots satisfying $\\sum_{i=1}^{n} z_i = 0$ and $\\sum_{i=1}^{n} z_i^2 = n$. The mean structure is\n$$\n\\log \\mu_{g i} = \\log(s m_g) + \\theta_g z_i,\n$$\nso that $\\mu_{g i} = s m_g \\exp(\\theta_g z_i)$. For inferential power under the alternative, we consider the two-sided Wald test for $H_0: \\theta_g = 0$ versus $H_1: \\theta_g \\neq 0$ at significance level $\\alpha$.\n\nFrom GLM theory with canonical link and large-sample approximations, the Wald statistic for a single parameter uses the Fisher information for $\\theta_g$. To first order in $\\theta_g$ (small to moderate effect sizes), we approximate $\\mu_{g i}$ under $H_0$ by $\\mu_{g i} \\approx s m_g$, which is constant across spots. The derivative of the mean with respect to $\\theta_g$ for the log link is\n$$\n\\frac{\\partial \\mu_{g i}}{\\partial \\theta_g} = \\mu_{g i} z_i \\approx (s m_g) z_i.\n$$\nUnder a quasi-likelihood or mean-variance specification for the NB, the variance function is $V(\\mu) = \\mu + \\phi_g \\mu^2 = \\mu (1 + \\phi_g \\mu)$. The per-spot contribution to the Fisher information for $\\theta_g$ at $H_0$ is then, by standard GLM derivations,\n$$\nI_{g i} = \\frac{\\left(\\frac{\\partial \\mu_{g i}}{\\partial \\theta_g}\\right)^2}{\\operatorname{Var}(Y_{g i})}\n\\approx \\frac{(s m_g)^2 z_i^2}{s m_g + \\phi_g (s m_g)^2}\n= \\frac{(s m_g) z_i^2}{1 + \\phi_g (s m_g)}.\n$$\nSumming across $i = 1,\\dots,n$ and using $\\sum_{i=1}^{n} z_i^2 = n$ yields the Fisher information\n$$\nI_g(n,s) \\approx \\sum_{i=1}^{n} I_{g i}\n= \\frac{n \\, s \\, m_g}{1 + \\phi_g \\, s \\, m_g}.\n$$\n\nThe Wald test statistic can be written as $Z_g = \\widehat{\\theta}_g \\sqrt{I_g}$, which under $H_0$ is approximately standard Normal. Under the true effect $\\theta_g \\ne 0$, $\\widehat{\\theta}_g$ is approximately Normal with mean $\\theta_g$ and variance $1/I_g$, so $Z_g$ is approximately Normal with mean $\\theta_g \\sqrt{I_g}$ and unit variance. Define the noncentrality parameter\n$$\n\\lambda_g(n,s) = \\theta_g^2 \\, I_g(n,s) = \\theta_g^2 \\, \\frac{n \\, s \\, m_g}{1 + \\phi_g \\, s \\, m_g}.\n$$\nLet $z_{1-\\alpha/2}$ be the $(1-\\alpha/2)$ quantile of the standard Normal distribution, and let $\\Phi(\\cdot)$ denote the standard Normal Cumulative Distribution Function (CDF). The two-sided power for gene $g$ is\n$$\n\\operatorname{power}_g(n,s)\n= \\Pr\\big(|Z_g| > z_{1-\\alpha/2}\\big)\n= \\Phi\\big(-z_{1-\\alpha/2} - \\sqrt{\\lambda_g(n,s)}\\big)\n+ 1 - \\Phi\\big(z_{1-\\alpha/2} - \\sqrt{\\lambda_g(n,s)}\\big).\n$$\nThe average power across the gene set $\\mathcal{G}$ is\n$$\n\\overline{P}(n,s) = \\frac{1}{|\\mathcal{G}|} \\sum_{g \\in \\mathcal{G}} \\operatorname{power}_g(n,s).\n$$\n\nWe must maximize $\\overline{P}(n,s)$ subject to the budget and feasibility constraints: $c_n n + c_s n s \\leq C_{\\max}$, $n \\geq n_{\\min}$, and $s \\geq s_{\\min}$, with $n$ and $s$ integers. For fixed $n$, the Fisher information $I_g(n,s)$ is strictly increasing in $s$ because\n$$\n\\frac{d}{ds} \\left( \\frac{s m_g}{1 + \\phi_g s m_g} \\right)\n= \\frac{m_g}{\\big(1 + \\phi_g s m_g\\big)^2} > 0,\n$$\nso for any fixed $n$, it is optimal to set $s$ to the largest integer permitted by the budget: $s \\leq \\left\\lfloor \\frac{C_{\\max} - c_n n}{c_s n} \\right\\rfloor$. Consequently, we can search over feasible $n$ in the range $n_{\\min} \\leq n \\leq \\left\\lfloor \\frac{C_{\\max}}{c_n + c_s s_{\\min}} \\right\\rfloor$, compute $s_{\\max}(n) = \\left\\lfloor \\frac{C_{\\max} - c_n n}{c_s n} \\right\\rfloor$, discard $n$ if $s_{\\max}(n) < s_{\\min}$, and otherwise evaluate $\\overline{P}(n,s_{\\max}(n))$. Among all feasible pairs, choose the one with the largest $\\overline{P}$; for ties within numerical tolerance, choose the largest $n$, and if still tied, the largest $s$.\n\nAlgorithmic steps for each test case:\n1. Enumerate integers $n$ from $n_{\\min}$ to $\\left\\lfloor \\frac{C_{\\max}}{c_n + c_s s_{\\min}} \\right\\rfloor$.\n2. For each $n$, compute $s_{\\max}(n) = \\left\\lfloor \\frac{C_{\\max} - c_n n}{c_s n} \\right\\rfloor$; if $s_{\\max}(n) < s_{\\min}$ then skip.\n3. Compute $\\overline{P}(n,s_{\\max}(n))$ using the formulas above with $\\lambda_g(n,s)$ and the Normal CDF at level $\\alpha$.\n4. Track the best $(n,s)$ by maximum $\\overline{P}$; resolve ties by largest $n$, then largest $s$.\n5. If no feasible $(n,s)$ exists, return $[0,0,0.0]$ for that test.\n6. Round the best average power to exactly $6$ decimal places for output.\n\nApplying this to the provided test suite yields a deterministic set of optimal integer allocations and their rounded average expected powers. The final program prints a single line containing the four results as $[[n_1^\\star,s_1^\\star,\\overline{p}_1],[n_2^\\star,s_2^\\star,\\overline{p}_2],[n_3^\\star,s_3^\\star,\\overline{p}_3],[n_4^\\star,s_4^\\star,\\overline{p}_4]]$ with no spaces.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom math import floor, sqrt, isclose\nfrom scipy.stats import norm\n\ndef fisher_information_nbinom(n, s, m, phi):\n    \"\"\"\n    Fisher information for theta (single standardized spatial contrast)\n    under NB GLM with log link and standardized z (sum z^2 = n),\n    using first-order approximation at the null.\n    I_g(n,s) = n * s * m / (1 + phi * s * m)\n    \"\"\"\n    mu = s * m\n    denom = 1.0 + phi * mu\n    return n * mu / denom\n\ndef gene_power(n, s, m, phi, theta, alpha):\n    \"\"\"\n    Two-sided Wald test power for one gene under NB GLM assumptions.\n    Power = Phi(-z - sqrt(lambda)) + 1 - Phi(z - sqrt(lambda)),\n    where lambda = theta^2 * I and z = z_{1 - alpha/2}.\n    \"\"\"\n    I = fisher_information_nbinom(n, s, m, phi)\n    lam = (theta * theta) * I\n    delta = sqrt(lam) if lam > 0.0 else 0.0\n    zthr = norm.ppf(1.0 - alpha / 2.0)\n    # Compute using CDF of standard normal\n    return norm.cdf(-zthr - delta) + (1.0 - norm.cdf(zthr - delta))\n\ndef average_power(n, s, genes, alpha):\n    \"\"\"\n    Average power across a set of genes, each as (m, phi, theta).\n    \"\"\"\n    if n <= 0 or s <= 0:\n        return 0.0\n    powers = [gene_power(n, s, m, phi, theta, alpha) for (m, phi, theta) in genes]\n    return float(np.mean(powers)) if powers else 0.0\n\ndef optimize_allocation(Cmax, c_n, c_s, n_min, s_min, alpha, genes, tie_tol=1e-12):\n    \"\"\"\n    Search over feasible n, set s to max allowed for each n, and pick the pair\n    that maximizes average expected power; ties broken by largest n, then largest s.\n    If infeasible, return (0, 0, 0.0).\n    \"\"\"\n    # Upper bound for n given minimum s\n    if c_n + c_s * s_min <= 0:\n        return (0, 0, 0.0)\n    n_upper = Cmax // (c_n + c_s * s_min)\n    if n_upper < n_min:\n        return (0, 0, 0.0)\n\n    best_n, best_s, best_power = 0, 0, -1.0\n    for n in range(n_min, int(n_upper) + 1):\n        # Max s for this n under budget\n        denom = c_s * n\n        if denom <= 0:\n            continue\n        s_max = (Cmax - c_n * n) // denom\n        if s_max < s_min:\n            continue\n        s = int(s_max)\n        avg_p = average_power(n, s, genes, alpha)\n\n        if avg_p > best_power + tie_tol:\n            best_n, best_s, best_power = n, s, avg_p\n        elif isclose(avg_p, best_power, rel_tol=0.0, abs_tol=tie_tol):\n            # Tie-break: prefer larger n, then larger s\n            if n > best_n or (n == best_n and s > best_s):\n                best_n, best_s, best_power = n, s, avg_p\n\n    if best_power < 0.0:\n        return (0, 0, 0.0)\n    return (best_n, best_s, best_power)\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test case 1\n        {\n            \"Cmax\": 12000, \"c_n\": 100, \"c_s\": 1, \"n_min\": 50, \"s_min\": 10, \"alpha\": 0.05,\n            \"genes\": [\n                (0.02, 0.05, 0.30),\n                (0.05, 0.10, 0.25),\n                (0.10, 0.20, 0.20),\n                (0.20, 0.20, 0.15),\n                (0.50, 0.30, 0.12),\n                (1.00, 0.40, 0.10),\n            ]\n        },\n        # Test case 2\n        {\n            \"Cmax\": 12000, \"c_n\": 500, \"c_s\": 1, \"n_min\": 10, \"s_min\": 10, \"alpha\": 0.05,\n            \"genes\": [\n                (0.02, 0.05, 0.30),\n                (0.05, 0.10, 0.25),\n                (0.10, 0.20, 0.20),\n                (0.20, 0.20, 0.15),\n                (0.50, 0.30, 0.12),\n                (1.00, 0.40, 0.10),\n            ]\n        },\n        # Test case 3\n        {\n            \"Cmax\": 12000, \"c_n\": 100, \"c_s\": 5, \"n_min\": 30, \"s_min\": 5, \"alpha\": 0.05,\n            \"genes\": [\n                (0.05, 0.20, 0.20),\n                (0.10, 0.30, 0.15),\n                (0.20, 0.30, 0.12),\n                (0.30, 0.40, 0.10),\n            ]\n        },\n        # Test case 4 (boundary feasibility)\n        {\n            \"Cmax\": 4400, \"c_n\": 200, \"c_s\": 2, \"n_min\": 20, \"s_min\": 10, \"alpha\": 0.01,\n            \"genes\": [\n                (0.10, 0.40, 0.25),\n                (0.20, 0.50, 0.20),\n                (0.40, 0.60, 0.15),\n            ]\n        },\n    ]\n\n    results_str_parts = []\n    for case in test_cases:\n        n_star, s_star, avg_power_star = optimize_allocation(\n            Cmax=case[\"Cmax\"],\n            c_n=case[\"c_n\"],\n            c_s=case[\"c_s\"],\n            n_min=case[\"n_min\"],\n            s_min=case[\"s_min\"],\n            alpha=case[\"alpha\"],\n            genes=case[\"genes\"],\n        )\n        # Round average power to exactly 6 decimal places for output\n        avg_str = f\"{avg_power_star:.6f}\"\n        results_str_parts.append(f\"[{n_star},{s_star},{avg_str}]\")\n\n    # Final print statement in the exact required format: no spaces.\n    print(\"[\" + \",\".join(results_str_parts) + \"]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3350152"}]}