{"hands_on_practices": [{"introduction": "At the heart of the Barabási-Albert (BA) model lies a simple and intuitive \"rich-get-richer\" mechanism known as preferential attachment. This principle dictates that new nodes in an evolving network are more likely to connect to nodes that are already well-connected. This first practice challenges you to derive the mathematical consequence of this rule from first principles, demonstrating how it gives rise to the model's signature scale-free degree distribution. By applying the master equation formalism, you will determine the exact functional form of the degree distribution $P(k)$ and its normalization constant, a foundational exercise for understanding generative network models [@problem_id:3316388].", "problem": "Consider a generative model for an evolving undirected molecular interaction network in computational systems biology, where new molecular species (nodes) enter the system sequentially and establish interactions (edges) with existing species according to the Barabási-Albert (BA) preferential attachment mechanism. The Barabási-Albert (BA) model is defined as follows: at each discrete time step, a single new node is introduced with exactly $m \\ge 1$ edges; each edge attaches to an existing node with probability proportional to the existing node’s current degree. Let $k$ denote degree, and let $P(k)$ denote the stationary fraction of nodes with degree $k$ in the large-network limit, restricted to $k \\ge m$.\n\nStarting from the fundamental BA rules above (preferential attachment proportional to degree and conservation of total degree increment per added edge), derive the properly normalized discrete form of $P(k)$ for $k \\ge m$ by writing and solving the large-time master equation for the expected counts of nodes in each degree class, and determine the unique prefactor that ensures $\\sum_{k=m}^{\\infty} P(k) = 1$. Your final answer must be a single closed-form analytic expression for $P(k)$ in terms of $k$ and $m$, valid for all integer $k \\ge m$. No numerical approximation is required, and no units should be included in the final expression.", "solution": "The problem as stated is valid. It is scientifically grounded in the established principles of network theory, specifically the Barabási-Albert (BA) model. The problem is well-posed, objective, and contains all necessary information for a rigorous derivation. We shall proceed to derive the degree distribution $P(k)$ using the master equation approach.\n\nLet $N_k(t)$ be the expected number of nodes with degree $k$ at time step $t$. The total number of nodes in the network is $N(t)$. The model starts with an initial core of $m_0$ nodes, and at each time step, one new node is added. Thus, $N(t) = m_0 + t$. For a large network ($t \\to \\infty$), we can approximate $N(t) \\approx t$.\n\nEach new node introduces $m$ edges. The total number of edges at time $t$ is $E(t) = E_0 + mt$, where $E_0$ is the number of edges in the initial core. The sum of degrees in an undirected network is twice the number of edges, so $\\sum_{i} k_i(t) = 2E(t) = 2(E_0 + mt)$. In the large-time limit, this sum is dominated by the linear term in $t$, giving $\\sum_i k_i(t) \\approx 2mt$.\n\nThe BA model employs preferential attachment, where the probability $\\Pi(i)$ that a new edge connects to an existing node $i$ is proportional to its degree $k_i$. This probability is given by:\n$$\n\\Pi(k_i) = \\frac{k_i}{\\sum_{j} k_j(t)} \\approx \\frac{k_i}{2mt}\n$$\n\nWe will now formulate the master equation for the rate of change of $N_k(t)$, denoted $\\frac{dN_k}{dt}$, within a continuum approximation valid for large $t$. The number of nodes of degree $k$ changes due to two processes:\n1.  A node of degree $k-1$ gains an edge, becoming a node of degree $k$.\n2.  A node of degree $k$ gains an edge, becoming a node of degree $k+1$.\n\nAt each time step $dt$, one new node is added, which forms $m$ edges. The number of new edges added per unit time is $m$. The rate at which these $m$ new edges attach to nodes of degree $k$ is $m$ times the total probability of hitting any such node. This probability is the number of such nodes, $N_k(t)$, multiplied by the probability of hitting a single one, $\\frac{k}{2mt}$.\nThus, the rate at which nodes of degree $k-1$ are converted to degree $k$ is:\n$$\n\\text{Gain Term for } N_k = m \\cdot N_{k-1}(t) \\cdot \\frac{k-1}{2mt} = \\frac{(k-1) N_{k-1}(t)}{2t}\n$$\nThe rate at which nodes of degree $k$ are converted to degree $k+1$ is:\n$$\n\\text{Loss Term for } N_k = m \\cdot N_k(t) \\cdot \\frac{k}{2mt} = \\frac{k N_k(t)}{2t}\n$$\nCombining these terms yields the master equation for $k  m$:\n$$\n\\frac{dN_k}{dt} = \\frac{(k-1) N_{k-1}}{2t} - \\frac{k N_k}{2t}\n$$\n\nA special case arises for $k=m$. Nodes of degree $m$ can only be created by the introduction of new nodes, as each new node has degree $m$. Therefore, at each time step, $N_m$ increases by $1$. Nodes of degree $m$ are lost when they gain an edge. Thus, the master equation for $N_m$ is:\n$$\n\\frac{dN_m}{dt} = 1 - \\frac{m N_m}{2t}\n$$\nNote that we have ignored the small probability that a new node attaches to itself or that two of its $m$ edges attach to the same target, as these events are negligible in the large-network limit.\n\nWe are interested in the stationary degree distribution $P(k)$ in the large-time limit. In this limit, the fraction of nodes with degree $k$, $P(k) = \\frac{N_k(t)}{N(t)}$, becomes constant. Using the approximation $N(t) \\approx t$, we have $N_k(t) = P(k) N(t) \\approx P(k) t$. Differentiating with respect to $t$ gives $\\frac{dN_k}{dt} \\approx P(k)$.\n\nSubstituting these relations into the master equations:\nFor $k  m$:\n$$\nP(k) = \\frac{(k-1) P(k-1)t}{2t} - \\frac{k P(k)t}{2t}\n$$\n$$\nP(k) = \\frac{k-1}{2} P(k-1) - \\frac{k}{2} P(k)\n$$\nRearranging for $P(k)$:\n$$\nP(k) \\left(1 + \\frac{k}{2}\\right) = \\frac{k-1}{2} P(k-1)\n$$\n$$\nP(k) \\left(\\frac{k+2}{2}\\right) = \\frac{k-1}{2} P(k-1)\n$$\n$$\nP(k) = \\frac{k-1}{k+2} P(k-1)\n$$\nThis is a recurrence relation for $P(k)$ for $k  m$.\n\nFor $k=m$:\n$$\nP(m) = 1 - \\frac{m P(m)t}{2t}\n$$\n$$\nP(m) = 1 - \\frac{m}{2} P(m)\n$$\nRearranging for $P(m)$:\n$$\nP(m) \\left(1 + \\frac{m}{2}\\right) = 1\n$$\n$$\nP(m) \\left(\\frac{m+2}{2}\\right) = 1\n$$\n$$\nP(m) = \\frac{2}{m+2}\n$$\nWe now solve the recurrence relation for $P(k)$ by unrolling it from $k$ down to $m$:\n$$\nP(k) = \\frac{k-1}{k+2} P(k-1) = \\frac{k-1}{k+2} \\frac{k-2}{k+1} P(k-2) = \\dots\n$$\n$$\nP(k) = \\left( \\prod_{j=m+1}^{k} \\frac{j-1}{j+2} \\right) P(m)\n$$\nThe product can be evaluated as:\n$$\n\\prod_{j=m+1}^{k} \\frac{j-1}{j+2} = \\frac{(m)(m+1)\\dots(k-1)}{(m+3)(m+4)\\dots(k+2)} = \\frac{\\frac{(k-1)!}{(m-1)!}}{\\frac{(k+2)!}{(m+2)!}} = \\frac{(k-1)!}{(m-1)!} \\frac{(m+2)!}{(k+2)!}\n$$\n$$\n= \\frac{(m+2)(m+1)m}{k(k+1)(k+2)}\n$$\nSubstituting this back into the expression for $P(k)$:\n$$\nP(k) = \\frac{m(m+1)(m+2)}{k(k+1)(k+2)} P(m)\n$$\nNow, using the value we found for $P(m)$:\n$$\nP(k) = \\frac{m(m+1)(m+2)}{k(k+1)(k+2)} \\cdot \\frac{2}{m+2} = \\frac{2m(m+1)}{k(k+1)(k+2)}\n$$\nThis is the closed-form expression for the degree distribution for $k \\ge m$.\n\nFinally, we must verify that this distribution is properly normalized, i.e., $\\sum_{k=m}^{\\infty} P(k) = 1$. The sum is:\n$$\n\\sum_{k=m}^{\\infty} P(k) = \\sum_{k=m}^{\\infty} \\frac{2m(m+1)}{k(k+1)(k+2)} = 2m(m+1) \\sum_{k=m}^{\\infty} \\frac{1}{k(k+1)(k+2)}\n$$\nThe term in the sum can be decomposed using partial fractions:\n$$\n\\frac{1}{k(k+1)(k+2)} = \\frac{1}{2k} - \\frac{1}{k+1} + \\frac{1}{2(k+2)} = \\frac{1}{2}\\left[\\left(\\frac{1}{k} - \\frac{1}{k+1}\\right) - \\left(\\frac{1}{k+1} - \\frac{1}{k+2}\\right)\\right]\n$$\nThis is a telescoping series. Let $S = \\sum_{k=m}^{\\infty} \\frac{1}{k(k+1)(k+2)}$.\nUsing the identity $\\sum_{i=n}^{\\infty} \\frac{1}{i(i+1)(i+2)} = \\frac{1}{2n(n+1)}$, which can be proven by the telescoping sum method:\n$$\nS = \\frac{1}{2} \\sum_{k=m}^{\\infty} \\left[ \\frac{1}{k(k+1)} - \\frac{1}{(k+1)(k+2)} \\right] = \\frac{1}{2} \\left[ \\frac{1}{m(m+1)} \\right]\n$$\nSubstituting this result back into the normalization sum:\n$$\n\\sum_{k=m}^{\\infty} P(k) = 2m(m+1) \\cdot \\left( \\frac{1}{2m(m+1)} \\right) = 1\n$$\nThe distribution is correctly normalized. The prefactor was not assumed but was derived directly from the master equation formalism, satisfying the problem's requirements. The final expression for $P(k)$ is therefore validated. For large $k$, $P(k) \\sim k^{-3}$, which is the well-known power-law signature of the BA model.", "answer": "$$\\boxed{\\frac{2m(m+1)}{k(k+1)(k+2)}}$$", "id": "3316388"}, {"introduction": "After deriving the asymptotic, or long-term, properties of the Barabási-Albert model, it is instructive to see these properties emerge from a computational simulation. This hands-on exercise transitions from pure theory to implementation, tasking you with building a BA network from scratch according to its generative rules [@problem_id:3316319]. By simulating the network's growth over a finite number of steps, you can directly observe its transient behavior. You will then quantify how closely this transient degree distribution matches the theoretical $P(k) \\propto k^{-3}$ asymptote, providing valuable insight into the dynamics of network evolution and the concept of convergence in complex systems.", "problem": "You are asked to formalize, analyze, and implement a simulation-based quantification of the transient degree distribution in the Barabási-Albert (BA) preferential attachment model, and to measure its convergence toward a power-law with exponent $3$. The modeling context is computational systems biology, where scale-free topologies are used as generative models of molecular interaction networks. Your program must reproduce a specified test suite and output a single line aggregating the requested numerical results.\n\nThe generative process must be defined as follows.\n\n- Begin with a seed network that is a complete graph on $m_0$ nodes. That is, the initial number of nodes is $m_0$, and each node is connected to all other nodes, implying an initial degree $m_0 - 1$ for every seed node and an initial total degree $m_0(m_0 - 1)$.\n\n- At each discrete time step $t \\in \\{1,2,\\dots\\}$, one new node is added with exactly $m$ edges, where $m \\leq m_0$. Each of the $m$ edges attaches to a distinct existing node, chosen without replacement, with probability proportional to the existing node’s current degree. Within a time step, the $m$ targets are chosen without replacement using the existing degrees frozen at their values before adding any of the $m$ edges. After selecting all $m$ targets in that step, the $m$ edges are added, increasing the degrees of the targets by $1$ each, and setting the new node’s degree to $m$. Self-loops and multi-edges are disallowed by construction.\n\n- Let $N(t) = m_0 + t$ denote the number of nodes after $t$ steps. Let $k_i(t)$ denote the degree of node $i$ at time $t$. Let $P_t(k)$ denote the empirical probability mass function (PMF) over degrees among nodes with degree at least $m$ at time $t$, that is, $P_t(k) = \\frac{1}{|\\{i : k_i(t) \\ge m\\}|}\\sum_{i=1}^{N(t)} \\mathbf{1}\\{k_i(t)=k,\\,k \\ge m\\}$.\n\n- Use repeated independent realizations to estimate $P_t(k)$ as an average PMF. Specifically, for a given parameter tuple $(m_0,m,t)$, simulate $R$ independent networks, each grown for exactly $t$ steps from a complete graph of size $m_0$, using the model above. For each realization, compute the histogram of degrees restricted to $k \\ge m$, normalize it to a PMF, and then average the PMFs across the $R$ realizations. Equivalently, you may pool the degree counts for $k \\ge m$ across all realizations and then normalize once.\n\nYour analysis must start from first principles appropriate for the BA model:\n\n- The total degree $S(t)$ satisfies $S(t) = S(0) + 2mt$ with $S(0) = m_0(m_0 - 1)$.\n\n- The probability that a node of degree $k_i(t)$ receives an attachment at time $t+1$ is $k_i(t)/S(t)$.\n\n- Use a continuum-rate or master-equation argument to derive the asymptotic degree distribution $P_{\\infty}(k)$ (for $k \\ge m$) and its power-law exponent. Do not assume the form of $P_{\\infty}(k)$; derive it from the model definition above and the stated conservation relation for $S(t)$.\n\nTo quantify convergence of $P_t(k)$ toward the asymptote, define the following distance.\n\n- Let $K_{\\max}(t)$ denote the maximum degree observed among nodes with degree at least $m$ when aggregating across the $R$ realizations at time $t$. Define a reference distribution $Q_t(k)$ on the finite support $\\{m, m+1, \\dots, K_{\\max}(t)\\}$ proportional to the asymptotic power-law with exponent $3$, that is, $Q_t(k) \\propto k^{-3}$ for $k \\in \\{m,\\dots,K_{\\max}(t)\\}$ and $Q_t(k) = 0$ otherwise, normalized so that $\\sum_{k=m}^{K_{\\max}(t)} Q_t(k) = 1$. Define the Total Variation (TV) distance between the empirical $P_t(k)$ (restricted and normalized on the same support) and $Q_t(k)$ by\n$$\n\\mathrm{TV}(P_t, Q_t) \\;=\\; \\frac{1}{2} \\sum_{k=m}^{K_{\\max}(t)} \\bigl| P_t(k) - Q_t(k) \\bigr| \\,.\n$$\n\nYour task is twofold:\n\n- Analytically derive the power-law exponent of the asymptotic distribution $P_{\\infty}(k)$ starting from the model’s preferential attachment rule and conservation of total degree. Your derivation must clearly justify all approximations and limits used.\n\n- Implement a program that, for each test case specified below, simulates the model and computes the TV distance $\\mathrm{TV}(P_t,Q_t)$ using the procedure above.\n\nTest suite:\n\n- Case $1$: $(m_0, m, t, R, \\mathrm{seed}) = (3, 1, 1, 10000, 17)$.\n\n- Case $2$: $(m_0, m, t, R, \\mathrm{seed}) = (5, 2, 30, 3000, 42)$.\n\n- Case $3$: $(m_0, m, t, R, \\mathrm{seed}) = (2, 1, 10, 4000, 12345)$.\n\n- Case $4$: $(m_0, m, t, R, \\mathrm{seed}) = (8, 3, 50, 2000, 2024)$.\n\nProgram requirements:\n\n- The program must implement the simulation exactly as specified and use the given seeds for reproducibility, with a pseudo-random number generator initialized per test case.\n\n- For each test case, compute a single real number equal to $\\mathrm{TV}(P_t,Q_t)$.\n\n- Output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[0.123456,0.234567,0.345678,0.456789]\"). Each number must be rounded to exactly six digits after the decimal point.\n\n- No physical units are involved. Angles do not appear. All outputs are real numbers in decimal form.\n\nYour final submission must be a standalone, runnable program and must not require any input. The single line of output must aggregate the TV distances for the four cases in the order given above.", "solution": "The problem asks for an analytical derivation of the asymptotic power-law exponent for the Barabási-Albert (BA) model and a numerical simulation to quantify the convergence of the transient degree distribution to this asymptotic form.\n\n### Analytical Derivation of the Asymptotic Degree Distribution\n\nWe derive the asymptotic degree distribution $P_{\\infty}(k)$ for the BA model as specified. We use a continuum rate equation approach, which models the change in the number of nodes of a given degree over time.\n\nLet $N_k(t)$ be the number of nodes with degree $k$ at time $t$. The total number of nodes is $N(t) = m_0 + t$, and the total degree is $S(t) = m_0(m_0 - 1) + 2mt$. For large times $t$, we can make the approximations $N(t) \\approx t$ and $S(t) \\approx 2mt$.\n\nThe degree of a node increases by one when it is chosen as a target for one of the $m$ new edges added at each time step. The probability that a specific node $i$ with degree $k_i(t)$ is chosen for a single new edge is $\\Pi(i) = k_i(t)/S(t)$. Since $m$ edges are added at each time step, and assuming for the continuum model that these choices are independent (a valid approximation for large networks), the rate at which a node of degree $k$ gains an edge is $m \\cdot (k/S(t))$.\n\nThe number of nodes with degree $k$, $N_k$, changes due to two processes:\n1.  **Gain**: A node with degree $k-1$ is selected and its degree becomes $k$. The total rate of this process is the number of such nodes, $N_{k-1}(t)$, times the rate at which each gains an edge: $m \\frac{k-1}{S(t)} N_{k-1}(t)$.\n2.  **Loss**: A node with degree $k$ is selected and its degree becomes $k+1$. The total rate of this process is $m \\frac{k}{S(t)} N_k(t)$.\n\nAdditionally, at each time step, one new node is introduced with degree $m$. This acts as a source term for $N_m$, contributing a rate of $1$.\n\nThe rate equation for the change in $N_k(t)$ is therefore:\n$$\n\\frac{d N_k(t)}{dt} = m \\frac{k-1}{S(t)} N_{k-1}(t) - m \\frac{k}{S(t)} N_k(t) + \\delta_{k,m}\n$$\nwhere $\\delta_{k,m}$ is the Kronecker delta, equal to $1$ if $k=m$ and $0$ otherwise. The term for $k-1$ is zero if $k=m$, as no nodes have degree less than $m$.\n\nWe seek a steady-state solution for large $t$, where the degree distribution $P(k)$ becomes time-independent. We assume that $N_k(t) = N(t) P(k) = (m_0+t) P(k)$. Differentiating with respect to $t$ gives $\\frac{d N_k(t)}{dt} = P(k)$.\n\nSubstituting this and the large-$t$ approximations $N(t) \\approx t$ and $S(t) \\approx 2mt$ into the rate equation:\n$$\nP(k) \\approx m \\frac{k-1}{2mt} (t \\cdot P(k-1)) - m \\frac{k}{2mt} (t \\cdot P(k)) + \\delta_{k,m}\n$$\n$$\nP(k) \\approx \\frac{k-1}{2} P(k-1) - \\frac{k}{2} P(k) + \\delta_{k,m}\n$$\n\nFor $k  m$, the source term is zero:\n$$\nP(k) = \\frac{k-1}{2} P(k-1) - \\frac{k}{2} P(k)\n$$\nRearranging the terms:\n$$\nP(k) \\left(1 + \\frac{k}{2}\\right) = \\frac{k-1}{2} P(k-1)\n$$\n$$\nP(k) \\left(\\frac{2+k}{2}\\right) = \\frac{k-1}{2} P(k-1)\n$$\nThis yields the recurrence relation:\n$$\nP(k) = \\frac{k-1}{k+2} P(k-1)\n$$\nWe can solve this by iteration:\n$$\nP(k) = P(m) \\prod_{j=m+1}^{k} \\frac{j-1}{j+2}\n$$\nThe product can be expanded as:\n$$\n\\prod_{j=m+1}^{k} \\frac{j-1}{j+2} = \\frac{m}{m+3} \\cdot \\frac{m+1}{m+4} \\cdot \\frac{m+2}{m+5} \\cdots \\frac{k-1}{k+2}\n$$\nThis can be expressed using factorials or, more generally, the Gamma function $\\Gamma(z+1) = z!$:\n$$\n\\prod_{j=m+1}^{k} \\frac{j-1}{j+2} = \\frac{\\prod_{j=m+1}^{k} (j-1)}{\\prod_{j=m+1}^{k} (j+2)} = \\frac{\\Gamma(k)/\\Gamma(m)}{\\Gamma(k+3)/\\Gamma(m+3)} = \\frac{\\Gamma(m+3)}{\\Gamma(m)} \\frac{\\Gamma(k)}{\\Gamma(k+3)}\n$$\nUsing the property $\\Gamma(z+1) = z\\Gamma(z)$, we have:\n$\\Gamma(m+3) = (m+2)(m+1)m\\Gamma(m)$ and $\\Gamma(k+3) = (k+2)(k+1)k\\Gamma(k)$.\nThus, the product simplifies to:\n$$\n\\prod_{j=m+1}^{k} \\frac{j-1}{j+2} = \\frac{(m+2)(m+1)m\\Gamma(m)}{\\Gamma(m)} \\frac{\\Gamma(k)}{(k+2)(k+1)k\\Gamma(k)} = \\frac{m(m+1)(m+2)}{k(k+1)(k+2)}\n$$\nSo, the asymptotic degree distribution has the form:\n$$\nP(k) = P(m) \\frac{m(m+1)(m+2)}{k(k+1)(k+2)} \\quad \\text{for } k \\ge m\n$$\nFor large $k$, the term $k(k+1)(k+2) \\sim k^3$. Therefore, the distribution follows a power law:\n$$\nP(k) \\propto k^{-3}\n$$\nThe power-law exponent is $\\gamma=3$. This derivation confirms the value provided in the problem statement. The constant of proportionality depends on $m$ but not on $k$.\n\n### Simulation and Measurement Algorithm\n\nThe task requires implementing a simulation to generate BA networks and compute the Total Variation (TV) distance between the empirical degree distribution and the theoretical power-law distribution.\n\n1.  **Simulation Setup**: For each test case $(m_0, m, t, R, \\mathrm{seed})$, we perform $R$ independent simulations. A pseudo-random number generator is initialized with the given `seed` for each case to ensure reproducibility.\n\n2.  **Network Generation**: Each simulation run proceeds as follows:\n    -   **Initialization**: The network starts at time $t=0$ with $m_0$ nodes in a complete graph. The degree of each initial node is $m_0 - 1$. An array stores the degrees of all nodes in the network.\n    -   **Growth**: The simulation runs for $t$ time steps. In each step:\n        a. A new node is added.\n        b. $m$ distinct existing nodes are selected as targets for attachment. The selection is probabilistic, where the probability of choosing any node is proportional to its current degree. This is implemented by sampling $m$ node indices without replacement from the set of current nodes, using their degrees normalized by the total degree as the probability distribution. The total degree at the beginning of step $s \\in \\{0, \\dots, t-1\\}$ is $S(s) = m_0(m_0-1) + 2ms$.\n        c. The degrees of the $m$ chosen target nodes are each incremented by $1$.\n        d. The new node is added to the network with a degree of $m$. Its degree is appended to the degree array.\n\n3.  **Degree Aggregation**: After $R$ simulations for a given test case, the final degree lists from all realizations are pooled. A frequency map (or dictionary) is constructed to store the total count for each degree value $k$ observed across all $R \\times (m_0 + t)$ nodes.\n\n4.  **PMF Calculation**:\n    -   Let the aggregated degree counts be denoted by $C(k)$.\n    -   The maximum observed degree across all simulations is $K_{\\max} = \\max\\{k \\mid C(k)0\\}$.\n    -   The analysis is restricted to degrees $k \\ge m$. The support for our distributions is the set of integers $\\{m, m+1, \\dots, K_{\\max}\\}$.\n    -   **Empirical PMF $P_t(k)$**: The total number of nodes with degree $k \\ge m$ is $N_{total} = \\sum_{j=m}^{K_{\\max}} C(j)$. The empirical PMF is then $P_t(k) = C(k) / N_{total}$ for $k \\in \\{m, \\dots, K_{\\max}\\}$.\n    -   **Reference PMF $Q_t(k)$**: This is a normalized power-law distribution with exponent $3$ on the same finite support. For each $k \\in \\{m, \\dots, K_{\\max}\\}$, we set an unnormalized value $q'(k) = k^{-3}$. The normalization constant is $C_Q = \\sum_{j=m}^{K_{\\max}} j^{-3}$. The reference PMF is $Q_t(k) = q'(k) / C_Q$.\n\n5.  **Total Variation Distance**: The distance between the two distributions is calculated as:\n    $$\n    \\mathrm{TV}(P_t, Q_t) = \\frac{1}{2} \\sum_{k=m}^{K_{\\max}} |P_t(k) - Q_t(k)|\n    $$\n    This value is computed for each test case and rounded to six decimal places.\n\nThe final output is a list of these TV distance values for the specified test cases, formatted as a comma-separated string within square brackets.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_simulation(m0, m, t, rng):\n    \"\"\"\n    Runs a single realization of the Barabási-Albert model.\n\n    Args:\n        m0 (int): Number of nodes in the initial complete graph.\n        m (int): Number of edges added with each new node.\n        t (int): Number of time steps to simulate.\n        rng (np.random.Generator): The random number generator to use.\n\n    Returns:\n        np.ndarray: An array of the degrees of all nodes at the end of the simulation.\n    \"\"\"\n    # Initial network: a complete graph on m0 nodes.\n    # Each node has degree m0 - 1.\n    degrees = np.full(m0, m0 - 1, dtype=int)\n    \n    # Growth process for t steps.\n    for step in range(t):\n        current_num_nodes = m0 + step\n        \n        # Total degree can be computed analytically to be slightly faster\n        # S(t) = S(0) + 2mt. At start of step `step` (0-indexed), total nodes added is `step`.\n        total_degree = m0 * (m0 - 1) + 2 * m * step\n        \n        # Attachment probabilities are proportional to degree.\n        if total_degree == 0:\n            # Handle the case of no edges, though not possible with m0 = 2.\n            # If m0=1, total_degree is 0. But m = m0 implies m=1. This case is not in the test suite.\n            # If so, attachment is uniform.\n            probs = np.ones(current_num_nodes) / current_num_nodes\n        else:\n            probs = degrees / total_degree\n\n        # Select m distinct nodes to attach to, without replacement.\n        node_indices = np.arange(current_num_nodes)\n        targets = rng.choice(node_indices, size=m, replace=False, p=probs)\n        \n        # Update degrees of target nodes.\n        degrees[targets] += 1\n        \n        # Add the new node with degree m.\n        degrees = np.append(degrees, m)\n        \n    return degrees\n\ndef calculate_tv_distance(m0, m, t, R, seed):\n    \"\"\"\n    Calculates the Total Variation distance for a given set of parameters.\n\n    Args:\n        m0 (int): Initial number of nodes.\n        m (int): Edges per new node.\n        t (int): Number of time steps.\n        R (int): Number of independent realizations.\n        seed (int): Seed for the random number generator.\n\n    Returns:\n        float: The computed TV distance.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    total_degree_counts = {}\n\n    for _ in range(R):\n        final_degrees = run_simulation(m0, m, t, rng)\n        for k in final_degrees:\n            total_degree_counts[k] = total_degree_counts.get(k, 0) + 1\n\n    if not total_degree_counts:\n        return 0.0\n\n    k_max_obs = max(total_degree_counts.keys())\n\n    # The problem defines the distributions on the support k = m.\n    if m  k_max_obs:\n        return 0.0\n\n    k_support = np.arange(m, k_max_obs + 1)\n\n    # Compute empirical PMF P_t(k)\n    counts_in_support = np.array([total_degree_counts.get(k, 0) for k in k_support])\n    total_nodes_in_support = np.sum(counts_in_support)\n    \n    if total_nodes_in_support == 0:\n        return 0.0\n    \n    p_t = counts_in_support / total_nodes_in_support\n    \n    # Compute reference PMF Q_t(k)\n    q_unnormalized = k_support.astype(np.float64)**-3\n    norm_const = np.sum(q_unnormalized)\n\n    if norm_const == 0:\n      # This case is unlikely unless k_support is pathological\n      q_t = np.zeros_like(p_t)\n    else:    \n      q_t = q_unnormalized / norm_const\n    \n    # Compute Total Variation distance\n    tv_dist = 0.5 * np.sum(np.abs(p_t - q_t))\n    \n    return tv_dist\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    test_cases = [\n        (3, 1, 1, 10000, 17),\n        (5, 2, 30, 3000, 42),\n        (2, 1, 10, 4000, 12345),\n        (8, 3, 50, 2000, 2024),\n    ]\n\n    results = []\n    for case in test_cases:\n        m0, m, t, R, seed = case\n        tv_distance = calculate_tv_distance(m0, m, t, R, seed)\n        results.append(f\"{tv_distance:.6f}\")\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3316319"}, {"introduction": "While the BA model successfully explains the origin of scale-free degree distributions, real biological networks exhibit other complex topological features. This final practice moves beyond the degree distribution to explore the clustering spectrum, $C(k)$, which describes how the cohesiveness of a node's local neighborhood changes with its degree, $k$. You will implement not only the BA model but also two important alternatives: the fitness model and the duplication-divergence model [@problem_id:2427984]. By comparing the clustering spectra produced by these three models against that of a reference network, you will engage in the critical scientific practice of model selection, learning to evaluate which generative mechanism best captures the nuanced structure of a given network.", "problem": "You are given three generative mechanisms for biological interaction networks that are widely used to model scale-free structure in computational biology and bioinformatics: the Barabási-Albert preferential attachment mechanism, the Bianconi-Barabási fitness mechanism, and the duplication-divergence mechanism. Your task is to implement these mechanisms, compute the clustering spectrum of the resulting networks, and decide which mechanism best reproduces the clustering spectrum of a provided reference network that serves as a proxy for a metabolic network. Your program must be a complete, runnable program that produces the required outputs, with no external input.\n\nFundamental base and required derivations:\n- A network is represented as a simple undirected graph with an adjacency list. For a node $v$, its degree is $k_v = \\lvert N(v) \\rvert$ where $N(v)$ is the set of its neighbors. The number of triangles incident to $v$ is $t_v$, equal to the number of unordered pairs of neighbors of $v$ that are adjacent to each other. The local clustering coefficient (LCC) at $v$ is defined by\n$$\nC_v = \n\\begin{cases}\n\\frac{2 t_v}{k_v(k_v-1)},  \\text{if } k_v \\ge 2, \\\\\n0,  \\text{if } k_v  2.\n\\end{cases}\n$$\nThe clustering spectrum $C(k)$ is the degree-resolved mean of $C_v$ across nodes of similar degree. To make this quantity numerically stable and comparable across models, define $B$ logarithmically spaced degree bins spanning from $1$ to $k_{\\max}$ where $k_{\\max}$ is the maximum degree observed in the reference network. Each node with degree $k_v$ contributes its $C_v$ to the bin containing $\\max(1,k_v)$, and the binned spectrum is the mean of $C_v$ within each bin. If a bin receives no nodes for a given model, define its $C$ value as $0$ for that model.\n- The Barabási-Albert model grows a network from an initial connected core by adding one node at a time, attaching $m$ new edges to existing nodes with probability proportional to their degree. The Bianconi-Barabási fitness model generalizes this by assigning each node a fixed positive fitness $\\eta_i$ at its creation, and attaching with probability proportional to $\\eta_i k_i$. The duplication-divergence model starts from a small connected seed; at each step it picks a node $u$ uniformly at random, creates a new node $v$, and for each neighbor $w$ of $u$ independently connects $v$ to $w$ with probability $p$, and additionally connects $u$ and $v$ with probability $q$. If the new node $v$ would be isolated, connect it to $u$ to keep the graph simple and connected.\n- To compare a candidate model to the reference network, compute the mean squared error between their binned clustering spectra:\n$$\n\\mathrm{MSE} = \\frac{1}{B} \\sum_{b=1}^{B} \\left( \\widehat{C}_{\\text{cand}}[b] - \\widehat{C}_{\\text{ref}}[b] \\right)^2,\n$$\nwhere $\\widehat{C}[\\cdot]$ denotes the binned spectrum. For stochastic models, average the binned spectrum over a specified number of independent replicates before computing the error.\n\nImplement the three generators precisely from these definitions, ensuring graphs are simple and undirected, and compute $C_v$ exactly from the definition using triangle counts derived from neighbor intersections.\n\nYour program must evaluate all three models on each test case, compute their errors, and output the index of the model with the smallest error using the tie-breaking rule “smallest index wins.” Use the model index mapping $0 \\mapsto$ Barabási-Albert, $1 \\mapsto$ fitness, and $2 \\mapsto$ duplication-divergence.\n\nReference networks are provided procedurally within each test case, together with seeds and all parameters necessary for reproducibility. All random number generation must be driven by the specified seeds using a reproducible pseudo-random number generator. Angles and physical units are not involved. All probabilities must be treated as real numbers in the unit interval. All outputs must be integers.\n\nTest suite:\n- Case $1$:\n  - Number of nodes: $N = 120$.\n  - Number of bins: $B = 6$.\n  - Replicates per candidate model: $R = 6$.\n  - Candidate base seed: $5000$.\n  - Reference network: duplication-divergence with duplication-retention probability $p = 0.72$, link-back probability $q = 0.30$, seed $1337$.\n  - Candidate model parameters:\n    - Barabási-Albert: $m = 2$.\n    - Fitness: $m = 2$, fitness distribution $\\text{log-normal}$ with $\\sigma = 1.0$ and $\\mu = 0$ in log-space.\n    - Duplication-divergence: $p = 0.72$, $q = 0.30$.\n- Case $2$:\n  - Number of nodes: $N = 150$.\n  - Number of bins: $B = 7$.\n  - Replicates per candidate model: $R = 5$.\n  - Candidate base seed: $6000$.\n  - Reference network: Barabási-Albert with $m = 2$, seed $2024$.\n  - Candidate model parameters:\n    - Barabási-Albert: $m = 2$.\n    - Fitness: $m = 2$, fitness distribution Pareto with shape $\\alpha = 2.5$ and scale $1$.\n    - Duplication-divergence: $p = 0.65$, $q = 0.25$.\n- Case $3$:\n  - Number of nodes: $N = 200$.\n  - Number of bins: $B = 7$.\n  - Replicates per candidate model: $R = 4$.\n  - Candidate base seed: $7000$.\n  - Reference network: fitness with $m = 2$, fitness distribution $\\text{log-normal}$ with $\\sigma = 1.2$, $\\mu = 0$, seed $7$.\n  - Candidate model parameters:\n    - Barabási-Albert: $m = 2$.\n    - Fitness: $m = 2$, fitness distribution $\\text{log-normal}$ with $\\sigma = 1.2$, $\\mu = 0$.\n    - Duplication-divergence: $p = 0.60$, $q = 0.10$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list of model indices enclosed in square brackets, in the same order as the test cases, for example, “$[0,1,2]$”. No spaces are allowed inside the brackets.\n\nDeliverable:\n- A single, complete program that, when run, constructs each specified reference network, evaluates the three candidate models using the indicated parameters and seeds, computes the binned clustering spectra and their mean squared errors, selects the winner per test case according to the smallest error with the specified tie-breaking rule, and prints the results in the exact required format. No further text should be printed.", "solution": "The problem presented is a well-defined task in computational network biology, requiring the implementation and comparison of three canonical generative models for scale-free networks: the Barabási-Albert ($BA$) model, the Bianconi-Barabási ($BB$) fitness model, and the Duplication-Divergence ($DD$) model. The problem is scientifically grounded, algorithmically specific, and all parameters and procedures for validation are provided. It is therefore deemed a valid problem.\n\nThe solution proceeds by implementing the specified components in a structured, step-by-step manner. For each test case, the primary goal is to determine which of the three candidate models best reproduces the clustering properties of a procedurally generated reference network.\n\nFirst, the core data structure for a network is an adjacency list, implemented as a dictionary mapping each node index (an integer from $0$ to $N-1$) to a set of its neighbors. This choice facilitates efficient neighbor lookups and edge modifications, which are critical for calculating network properties and for the growth algorithms. A graph is kept simple and undirected by ensuring no self-loops and adding each edge $(u,v)$ symmetrically to the adjacency lists of both node $u$ and node $v$.\n\nThe primary metric for comparison is the clustering spectrum, $C(k)$. This requires computing the local clustering coefficient ($LCC$), $C_v$, for each node $v$. Given a node $v$ with degree $k_v = |N(v)|$, where $N(v)$ is the set of neighbors of $v$, the number of triangles $t_v$ incident to $v$ is calculated. This is achieved by iterating through all unique pairs of neighbors of $v$, say $(u,w)$, and checking if an edge exists between them (i.e., if $u \\in N(w)$). The LCC is then given by the formula:\n$$\nC_v = \n\\begin{cases}\n\\frac{2 t_v}{k_v(k_v-1)},  \\text{if } k_v \\ge 2, \\\\\n0,  \\text{if } k_v  2.\n\\end{cases}\n$$\nTo obtain the clustering spectrum, node degrees are partitioned into $B$ logarithmically spaced bins. First, the maximum degree $k_{\\max}$ of the reference network is determined. The bin edges are then defined using `numpy.logspace` from $1$ to $k_{\\max}$ to create $B$ consecutive intervals. For each node $v$, its degree $k_v$ (or $1$ if $k_v=0$) is mapped to one of these bins. The value of the spectrum for a given bin, $\\widehat{C}[b]$, is the average of all $C_v$ values from nodes whose degrees fall into that bin. If a bin is empty, its value is $0$.\n\nThe three generative models are implemented as follows, each driven by a seeded pseudorandom number generator for reproducibility:\n\n$1$. **Barabási-Albert (BA) Model**: The network starts with a small initial core, specifically a complete graph of $m+1$ nodes, to ensure initial degrees are non-zero. The network then grows to the target size $N$ by adding one node at a time. Each new node forms $m$ edges to existing nodes, chosen via preferential attachment. The probability of a new node connecting to an existing node $j$ is proportional to the degree of that node, $k_j$. This is implemented by sampling $m$ distinct nodes without replacement from the existing population, with weights given by their degrees.\n\n$2$. **Bianconi-Barabási (BB) Fitness Model**: This model generalizes the BA mechanism. At the outset, each of the $N$ potential nodes is assigned a fitness value $\\eta_i$ drawn from a specified distribution (e.g., log-normal or Pareto). The network growth process is similar to BA, starting with an initial complete graph of $m+1$ nodes. However, the connection probability for a new node to an existing node $j$ is now proportional to the product of its fitness and degree, $\\eta_j k_j$. Again, $m$ distinct nodes are chosen without replacement based on these weighted probabilities.\n\n$3$. **Duplication-Divergence (DD) Model**: This model simulates gene duplication and subsequent mutation. It starts with a minimal seed network of two nodes connected by an edge. The network grows by iteratively picking an existing node $u$ uniformly at random to serve as a template. A new node $v$ is created, and for each neighbor $w$ of $u$, an edge $(v,w)$ is formed with probability $p$. Additionally, an edge $(u,v)$ is formed with probability $q$. A crucial final step ensures the graph remains connected: if the new node $v$ has no connections after this process, an edge is deterministically created between $v$ and its template $u$.\n\nFor each test case, the overall procedure is as follows:\n$1$. A reference network is generated using its specified model, parameters, and seed.\n$2$. The maximum degree $k_{\\max}$ of the reference network is determined, and the $B$ degree bins are established.\n$3$. The binned clustering spectrum of the reference network, $\\widehat{C}_{\\text{ref}}$, is computed.\n$4$. For each of the three candidate models, $R$ replicate networks are generated. For each replicate, its binned clustering spectrum is computed using the bins defined from the reference network. These spectra are then averaged across the $R$ replicates to produce a single representative spectrum, $\\widehat{C}_{\\text{cand}}$.\n$5$. The performance of each candidate model is quantified by the Mean Squared Error ($MSE$) between its averaged spectrum and the reference spectrum:\n$$\n\\mathrm{MSE} = \\frac{1}{B} \\sum_{b=1}^{B} \\left( \\widehat{C}_{\\text{cand}}[b] - \\widehat{C}_{\\text{ref}}[b] \\right)^2\n$$\n$6$. The model with the lowest $MSE$ is declared the winner for that test case. In case of a tie, the model with the smallest index ($0$ for BA, $1$ for BB, $2$ for DD) wins.\n\nThis entire process is automated in a single Python script. The script iterates through the provided test cases, performs the simulations and comparisons, and outputs the final list of winning model indices in the required format.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom collections import defaultdict\n\ndef _calculate_lcc_and_deg(adj, n_nodes):\n    \"\"\"Calculates degrees and local clustering coefficients for all nodes.\"\"\"\n    degrees = {i: len(adj.get(i, set())) for i in range(n_nodes)}\n    lccs = {}\n    for i in range(n_nodes):\n        k_v = degrees[i]\n        if k_v  2:\n            lccs[i] = 0.0\n            continue\n        \n        neighbors = list(adj.get(i, set()))\n        t_v = 0\n        for idx1 in range(len(neighbors)):\n            for idx2 in range(idx1 + 1, len(neighbors)):\n                u, w = neighbors[idx1], neighbors[idx2]\n                if w in adj.get(u, set()):\n                    t_v += 1\n        \n        lccs[i] = (2 * t_v) / (k_v * (k_v - 1))\n        \n    return degrees, lccs\n\ndef compute_binned_spectrum(adj, n_nodes, B, k_max):\n    \"\"\"Computes the binned clustering spectrum for a given network.\"\"\"\n    if k_max = 1:\n        bin_edges = np.array([1.0, 2.0]) # A single bin for k=1\n        effective_B = 1\n    else:\n        bin_edges = np.logspace(0, np.log10(k_max), B + 1)\n        effective_B = B\n\n    degrees, lccs = _calculate_lcc_and_deg(adj, n_nodes)\n    \n    lcc_sum = np.zeros(effective_B)\n    node_count = np.zeros(effective_B)\n    \n    internal_edges = bin_edges[1:-1]\n\n    for i in range(n_nodes):\n        k = degrees[i]\n        c = lccs[i]\n        \n        k_eff = max(1, k)\n        \n        if k_max = 1:\n            bin_idx = 0\n        else:\n            bin_idx = np.digitize(k_eff, internal_edges)\n        \n        lcc_sum[bin_idx] += c\n        node_count[bin_idx] += 1\n        \n    spectrum = np.zeros(effective_B)\n    non_zero_counts = node_count  0\n    spectrum[non_zero_counts] = lcc_sum[non_zero_counts] / node_count[non_zero_counts]\n\n    if effective_B  B:\n        # Pad with zeros if fewer bins were used\n        padded_spectrum = np.zeros(B)\n        padded_spectrum[:effective_B] = spectrum\n        return padded_spectrum\n    \n    return spectrum\n\ndef generate_ba(N, m, seed):\n    \"\"\"Generates a Barabási-Albert network.\"\"\"\n    rng = np.random.default_rng(seed)\n    adj = defaultdict(set)\n    m0 = m + 1\n    \n    if N  m0:\n        return adj\n\n    # Initial complete graph K_{m+1}\n    for i in range(m0):\n        for j in range(i + 1, m0):\n            adj[i].add(j)\n            adj[j].add(i)\n\n    degrees = {i: m for i in range(m0)}\n    node_list = list(range(m0))\n    \n    for i in range(m0, N):\n        current_nodes = np.array(node_list)\n        current_degrees = np.array([degrees[node] for node in current_nodes])\n        \n        total_degree = np.sum(current_degrees)\n        if total_degree == 0:\n            targets = rng.choice(current_nodes, size=m, replace=False)\n        else:\n            probs = current_degrees / total_degree\n            targets = rng.choice(current_nodes, size=m, replace=False, p=probs)\n        \n        adj[i] = set(targets)\n        for target in targets:\n            adj[target].add(i)\n        \n        degrees[i] = m\n        for target in targets:\n            degrees[target] += 1\n        node_list.append(i)\n        \n    return adj\n\ndef generate_bb(N, m, fitness, seed):\n    \"\"\"Generates a Bianconi-Barabási fitness network.\"\"\"\n    rng = np.random.default_rng(seed)\n    adj = defaultdict(set)\n    m0 = m + 1\n    \n    if N  m0:\n        return adj\n\n    for i in range(m0):\n        for j in range(i + 1, m0):\n            adj[i].add(j)\n            adj[j].add(i)\n\n    degrees = {i: m for i in range(m0)}\n    node_list = list(range(m0))\n\n    for i in range(m0, N):\n        current_nodes = np.array(node_list)\n        current_degrees = np.array([degrees[node] for node in current_nodes])\n        current_fitness = np.array([fitness[node] for node in current_nodes])\n\n        weights = current_degrees * current_fitness\n        total_weight = np.sum(weights)\n\n        if total_weight == 0:\n            targets = rng.choice(current_nodes, size=m, replace=False)\n        else:\n            probs = weights / total_weight\n            targets = rng.choice(current_nodes, size=m, replace=False, p=probs)\n        \n        adj[i] = set(targets)\n        for target in targets:\n            adj[target].add(i)\n        \n        degrees[i] = m\n        for target in targets:\n            degrees[target] += 1\n        node_list.append(i)\n        \n    return adj\n\ndef generate_dd(N, p, q, seed):\n    \"\"\"Generates a Duplication-Divergence network.\"\"\"\n    rng = np.random.default_rng(seed)\n    adj = defaultdict(set)\n    \n    if N  2:\n        return adj\n    \n    # Initial seed: 2 nodes, 1 edge\n    adj[0].add(1)\n    adj[1].add(0)\n    \n    nodes = [0, 1]\n    for i in range(2, N):\n        template_node = rng.choice(nodes)\n        \n        new_neighbors = set()\n        \n        # Divergence from template's neighbors\n        for neighbor in adj[template_node]:\n            if rng.random()  p:\n                new_neighbors.add(neighbor)\n        \n        # Link-back to template\n        if rng.random()  q:\n            new_neighbors.add(template_node)\n        \n        # Check for isolation\n        if not new_neighbors:\n            adj[i].add(template_node)\n            adj[template_node].add(i)\n        else:\n            adj[i] = new_neighbors\n            for neighbor in new_neighbors:\n                adj[neighbor].add(i)\n        nodes.append(i)\n        \n    return adj\n\ndef solve():\n    test_cases = [\n        {\n            \"N\": 120, \"B\": 6, \"R\": 6, \"candidate_base_seed\": 5000,\n            \"ref_model\": \"dd\", \"ref_params\": {\"p\": 0.72, \"q\": 0.30}, \"ref_seed\": 1337,\n            \"cand_params\": [\n                {\"model\": \"ba\", \"m\": 2},\n                {\"model\": \"bb\", \"m\": 2, \"dist\": \"lognormal\", \"dist_params\": {\"mean\": 0, \"sigma\": 1.0}},\n                {\"model\": \"dd\", \"p\": 0.72, \"q\": 0.30},\n            ]\n        },\n        {\n            \"N\": 150, \"B\": 7, \"R\": 5, \"candidate_base_seed\": 6000,\n            \"ref_model\": \"ba\", \"ref_params\": {\"m\": 2}, \"ref_seed\": 2024,\n            \"cand_params\": [\n                {\"model\": \"ba\", \"m\": 2},\n                {\"model\": \"bb\", \"m\": 2, \"dist\": \"pareto\", \"dist_params\": {\"a\": 2.5, \"scale\": 1}},\n                {\"model\": \"dd\", \"p\": 0.65, \"q\": 0.25},\n            ]\n        },\n        {\n            \"N\": 200, \"B\": 7, \"R\": 4, \"candidate_base_seed\": 7000,\n            \"ref_model\": \"bb\", \"ref_params\": {\"m\": 2, \"dist\": \"lognormal\", \"dist_params\": {\"mean\": 0, \"sigma\": 1.2}}, \"ref_seed\": 7,\n            \"cand_params\": [\n                {\"model\": \"ba\", \"m\": 2},\n                {\"model\": \"bb\", \"m\": 2, \"dist\": \"lognormal\", \"dist_params\": {\"mean\": 0, \"sigma\": 1.2}},\n                {\"model\": \"dd\", \"p\": 0.60, \"q\": 0.10},\n            ]\n        }\n    ]\n\n    final_results = []\n\n    for case in test_cases:\n        N, B, R = case[\"N\"], case[\"B\"], case[\"R\"]\n        \n        # Generate reference network\n        if case[\"ref_model\"] == \"dd\":\n            ref_adj = generate_dd(N, case[\"ref_params\"][\"p\"], case[\"ref_params\"][\"q\"], case[\"ref_seed\"])\n        elif case[\"ref_model\"] == \"ba\":\n            ref_adj = generate_ba(N, case[\"ref_params\"][\"m\"], case[\"ref_seed\"])\n        else: # bb\n            rng_fit = np.random.default_rng(case[\"ref_seed\"])\n            if case[\"ref_params\"][\"dist\"] == \"lognormal\":\n                fitness = rng_fit.lognormal(mean=case[\"ref_params\"][\"dist_params\"][\"mean\"], sigma=case[\"ref_params\"][\"dist_params\"][\"sigma\"], size=N)\n            adj = generate_bb(N, case[\"ref_params\"][\"m\"], fitness, case[\"ref_seed\"])\n            ref_adj = adj\n\n        ref_degrees, _ = _calculate_lcc_and_deg(ref_adj, N)\n        k_max = max(ref_degrees.values()) if ref_degrees else 0\n        ref_spectrum = compute_binned_spectrum(ref_adj, N, B, k_max)\n\n        mse_scores = []\n        for model_idx, params in enumerate(case[\"cand_params\"]):\n            avg_cand_spectrum = np.zeros(B)\n            \n            # Pre-generate fitness values for BB model if needed\n            if params[\"model\"] == \"bb\":\n                rng_fit = np.random.default_rng(case[\"candidate_base_seed\"])\n                if params[\"dist\"] == \"lognormal\":\n                    fitness_vals = rng_fit.lognormal(mean=params[\"dist_params\"][\"mean\"], sigma=params[\"dist_params\"][\"sigma\"], size=N)\n                elif params[\"dist\"] == \"pareto\":\n                    fitness_vals = (rng_fit.pareto(a=params[\"dist_params\"][\"a\"], size=N) + 1) * params[\"dist_params\"][\"scale\"]\n\n            for rep_idx in range(R):\n                seed = case[\"candidate_base_seed\"] + model_idx * R + rep_idx\n                if params[\"model\"] == \"ba\":\n                    cand_adj = generate_ba(N, params[\"m\"], seed)\n                elif params[\"model\"] == \"bb\":\n                    cand_adj = generate_bb(N, params[\"m\"], fitness_vals, seed)\n                else: # dd\n                    cand_adj = generate_dd(N, params[\"p\"], params[\"q\"], seed)\n\n                cand_spectrum = compute_binned_spectrum(cand_adj, N, B, k_max)\n                avg_cand_spectrum += cand_spectrum\n            \n            avg_cand_spectrum /= R\n            \n            mse = np.mean((avg_cand_spectrum - ref_spectrum)**2)\n            mse_scores.append(mse)\n            \n        winner_idx = np.argmin(mse_scores)\n        final_results.append(winner_idx)\n\n    print(f\"[{','.join(map(str, final_results))}]\")\n\nsolve()\n```", "id": "2427984"}]}