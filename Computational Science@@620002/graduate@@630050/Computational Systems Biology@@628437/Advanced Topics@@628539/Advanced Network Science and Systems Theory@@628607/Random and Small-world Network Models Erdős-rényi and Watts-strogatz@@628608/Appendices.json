{"hands_on_practices": [{"introduction": "A cornerstone of network analysis is determining whether observed structural features, such as network motifs, are statistically significant or simply artifacts of randomness. The Erdős-Rényi (ER) random graph serves as a fundamental null model for this purpose. This first exercise [@problem_id:3342456] provides practice in using the powerful principle of linearity of expectation to calculate the expected number of basic subgraphs, a foundational skill for any quantitative analysis of network structure.", "problem": "In computational systems biology, random graph null models help assess whether observed motif abundances in cellular networks (such as protein–protein interaction networks) deviate from chance. Consider the Erdős–Rényi (ER) model $G(n,p)$ on $n$ labeled vertices, where each possible undirected edge appears independently with probability $p \\in (0,1)$. Let $m$ denote the total number of edges, $T$ denote the total number of triangles (simple $3$-cycles), and $C_4$ denote the total number of simple undirected $4$-cycles. For $C_4$, count each distinct undirected simple $4$-cycle exactly once regardless of traversal direction or starting vertex; the cycle need not be induced, so additional chords among the $4$ vertices do not affect counting.\n\nStarting only from the core definitions of $G(n,p)$ and the linearity of expectation, derive closed-form expressions in terms of $n$ and $p$ for the expectations $\\mathbb{E}[m]$, $\\mathbb{E}[T]$, and $\\mathbb{E}[C_4]$. Express your final answer as closed-form analytic expressions involving $n$ and $p$ only. No numerical approximation is required. The final answer must be a single object: list the three expressions in a single row matrix in the order $\\mathbb{E}[m]$, $\\mathbb{E}[T]$, $\\mathbb{E}[C_4]$.", "solution": "We begin from the definition of the Erdős–Rényi (ER) model $G(n,p)$: for a labeled vertex set of size $n$, each of the $\\binom{n}{2}$ possible undirected edges is included independently with probability $p \\in (0,1)$. We use indicator random variables and the linearity of expectation to compute the expected counts of subgraphs.\n\nFirst, the expected number of edges $\\mathbb{E}[m]$. Let $X_{e}$ be the indicator that a particular undirected edge $e$ is present. Then $m = \\sum_{e} X_{e}$ where the sum is over all $\\binom{n}{2}$ possible edges. By independence and the definition of $G(n,p)$, $\\mathbb{P}(X_{e}=1)=p$, hence $\\mathbb{E}[X_{e}] = p$. By linearity of expectation,\n$$\n\\mathbb{E}[m] \\;=\\; \\sum_{e} \\mathbb{E}[X_{e}] \\;=\\; \\binom{n}{2}\\, p.\n$$\n\nSecond, the expected number of triangles $\\mathbb{E}[T]$. A triangle is specified by a $3$-element vertex subset. For each unordered triple $\\{i,j,k\\}$, define an indicator $Y_{\\{i,j,k\\}}$ that the three edges among these vertices are all present. Since each of the $3$ edges occurs independently with probability $p$, we have $\\mathbb{P}(Y_{\\{i,j,k\\}}=1)=p^{3}$, so $\\mathbb{E}[Y_{\\{i,j,k\\}}]=p^{3}$. The total number of unordered triples is $\\binom{n}{3}$. Therefore,\n$$\n\\mathbb{E}[T] \\;=\\; \\sum_{\\{i,j,k\\}} \\mathbb{E}[Y_{\\{i,j,k\\}}]\n\\;=\\; \\binom{n}{3}\\, p^{3}.\n$$\n\nThird, the expected number of simple undirected $4$-cycles $\\mathbb{E}[C_4]$, counted without orientation or starting point and not necessarily induced. There are two equivalent counting approaches.\n\nApproach $1$ (via ordered cycles and an automorphism factor): Consider ordered $4$-tuples $(v_{1},v_{2},v_{3},v_{4})$ of distinct vertices. There are $n$ choices for $v_{1}$, then $(n-1)$ for $v_{2}$, $(n-2)$ for $v_{3}$, and $(n-3)$ for $v_{4}$, yielding $n(n-1)(n-2)(n-3)$ ordered tuples. Each such ordered tuple determines a directed simple $4$-cycle if and only if the $4$ edges $\\{v_{1},v_{2}\\}$, $\\{v_{2},v_{3}\\}$, $\\{v_{3},v_{4}\\}$, and $\\{v_{4},v_{1}\\}$ are present. These $4$ edges occur independently with probability $p^{4}$, hence the expected number of ordered simple $4$-cycles is $n(n-1)(n-2)(n-3)\\, p^{4}$. Each undirected $4$-cycle is represented by exactly $8$ ordered tuples (there are $4$ rotations and $2$ orientations), i.e., the automorphism group of the cycle has size $8$. Therefore,\n$$\n\\mathbb{E}[C_4] \\;=\\; \\frac{n(n-1)(n-2)(n-3)}{8}\\, p^{4}.\n$$\n\nApproach $2$ (via $4$-vertex subsets): For each $4$-element vertex subset, there are exactly $3$ distinct undirected simple $4$-cycles on that vertex set (the number of Hamiltonian cycles on $4$ labeled vertices up to rotation and reflection is $3$). For any one such undirected cycle, the probability that its $4$ requisite edges are present is $p^{4}$ (edges independent). Summing over the $\\binom{n}{4}$ vertex subsets and the $3$ cycles per subset,\n$$\n\\mathbb{E}[C_4] \\;=\\; 3 \\binom{n}{4} \\, p^{4}\n\\;=\\; \\frac{n(n-1)(n-2)(n-3)}{8}\\, p^{4},\n$$\nwhich agrees with Approach $1$ since $3 \\binom{n}{4} = \\frac{n(n-1)(n-2)(n-3)}{8}$.\n\nCollecting the results,\n$$\n\\mathbb{E}[m] \\;=\\; \\binom{n}{2} p,\\quad\n\\mathbb{E}[T] \\;=\\; \\binom{n}{3} p^{3},\\quad\n\\mathbb{E}[C_4] \\;=\\; \\frac{n(n-1)(n-2)(n-3)}{8} p^{4}.\n$$\nThese expressions are closed-form in $n$ and $p$ and rely only on the independence structure of $G(n,p)$ and the linearity of expectation.", "answer": "$$\\boxed{\\begin{pmatrix}\\binom{n}{2}\\, p & \\binom{n}{3}\\, p^{3} & \\dfrac{n(n-1)(n-2)(n-3)}{8}\\, p^{4}\\end{pmatrix}}$$", "id": "3342456"}, {"introduction": "While expected values provide a baseline, understanding the variability of network properties is crucial for statistical inference. This practice problem [@problem_id:3342490] moves beyond first moments to the second, asking you to derive the variance of the triangle count in an ER graph. This calculation requires careful accounting of the dependencies between overlapping subgraphs and provides deeper insight into the distribution of network motifs and the conditions under which their counts concentrate around the mean.", "problem": "In a coarse-grained model of protein-protein interaction networks used in computational systems biology, suppose the network of $n$ proteins is modeled as an undirected Erdős–Rényi (ER) random graph $G(n,p)$, where each of the $\\binom{n}{2}$ possible protein-protein interactions is present independently with probability $p \\in (0,1)$. A $3$-protein complex that is fully mutually interacting corresponds to an undirected triangle (a $3$-clique). Let $T$ denote the random variable counting the number of triangles in $G(n,p)$.\n\nStarting from core definitions and independence structure only, and without assuming any pre-existing formulas for subgraph counts:\n- Derive $\\mathbb{E}[T]$.\n- Derive the exact closed-form expression for $\\mathrm{Var}(T)$ by writing $T$ as a sum of indicator random variables and carefully enumerating all dependence contributions.\n- Using your variance decomposition, give a principled discussion of when a normal approximation for $T$ (after centering by $\\mathbb{E}[T]$ and scaling by $\\sqrt{\\mathrm{Var}(T)}$) is justified as $n \\to \\infty$, in terms of asymptotic regimes for $p = p(n)$. Your discussion should be grounded in first principles such as the Central Limit Theorem (CLT) heuristic for weakly dependent summands or the dependency-graph perspective, and should explicitly identify at least two qualitatively distinct sparse-to-dense regimes in which the asymptotic variance is dominated by different terms.\n\nYour final reported answer must be the exact closed-form analytic expression you obtain for $\\mathrm{Var}(T)$ as a function of $n$ and $p$. No numerical approximation is required. Do not include units.", "solution": "We formalize the model and proceed from first principles. In an undirected Erdős–Rényi (ER) random graph $G(n,p)$ on vertex set $[n] = \\{1,2,\\dots,n\\}$, each of the $\\binom{n}{2}$ possible edges is present independently with probability $p \\in (0,1)$. A triangle is a set of $3$ distinct vertices whose $3$ pairwise edges are all present.\n\nDefine indicator random variables as follows. For each unordered triple $a = \\{i,j,k\\} \\subset [n]$ with $|a| = 3$, let\n$$\nI_{a} \\;=\\; \\mathbf{1}\\{\\text{all } 3 \\text{ edges among } i,j,k \\text{ are present}\\}.\n$$\nThen the total triangle count is\n$$\nT \\;=\\; \\sum_{a \\in \\binom{[n]}{3}} I_{a},\n$$\nwhere $\\binom{[n]}{3}$ denotes the set of all $\\binom{n}{3}$ unordered triples.\n\nExpectation. For any fixed triple $a$, the $3$ required edges are independent and each is present with probability $p$, so\n$$\n\\mathbb{E}[I_{a}] \\;=\\; p^{3}.\n$$\nBy linearity of expectation,\n$$\n\\mathbb{E}[T] \\;=\\; \\sum_{a} \\mathbb{E}[I_{a}]\n\\;=\\; \\binom{n}{3} p^{3}.\n$$\n\nVariance. We use the identity\n$$\n\\mathrm{Var}(T) \\;=\\; \\sum_{a} \\mathrm{Var}(I_{a}) \\;+\\; 2 \\sum_{a<b} \\mathrm{Cov}(I_{a}, I_{b}),\n$$\nwhere the sum over $a<b$ ranges over unordered distinct pairs of triples. First, for any $a$,\n$$\n\\mathrm{Var}(I_{a}) \\;=\\; \\mathbb{E}[I_{a}^{2}] - (\\mathbb{E}[I_{a}])^{2} \\;=\\; p^{3} - p^{6} \\;=\\; p^{3}(1 - p^{3}).\n$$\nHence\n$$\n\\sum_{a} \\mathrm{Var}(I_{a}) \\;=\\; \\binom{n}{3} p^{3} (1 - p^{3}).\n$$\n\nNext, we analyze $\\mathrm{Cov}(I_{a}, I_{b})$ for $a \\neq b$. There are three overlap types for two distinct triples $a$ and $b$:\n- Type $0$: $a$ and $b$ are vertex-disjoint (share $0$ vertices). Then $I_{a}$ depends on $3$ edges, $I_{b}$ on another disjoint set of $3$ edges; by edge-independence, $I_{a}$ and $I_{b}$ are independent, so $\\mathrm{Cov}(I_{a}, I_{b}) = 0$.\n- Type $1$: $a$ and $b$ share exactly $1$ vertex. Then the edge sets underlying $I_{a}$ and $I_{b}$ are still disjoint (each is confined to its own $3$-vertex set); thus $I_{a}$ and $I_{b}$ are independent and $\\mathrm{Cov}(I_{a}, I_{b}) = 0$.\n- Type $2$: $a$ and $b$ share exactly $2$ vertices, equivalently, the two triangles share exactly $1$ edge. In this case, the union of the two triangles spans $4$ vertices and uses $5$ distinct edges: the shared edge appears once in the union, and each triangle contributes its two non-shared edges. Therefore\n$$\n\\mathbb{E}[I_{a} I_{b}] \\;=\\; p^{5}, \\qquad \\mathbb{E}[I_{a}] \\mathbb{E}[I_{b}] \\;=\\; p^{6},\n$$\nso\n$$\n\\mathrm{Cov}(I_{a}, I_{b}) \\;=\\; p^{5} - p^{6} \\;=\\; p^{5}(1 - p).\n$$\n\nOnly Type $2$ pairs contribute to the covariance sum. We now enumerate unordered distinct pairs $\\{a,b\\}$ of triangles that share exactly one edge. Pick the shared edge first: there are $\\binom{n}{2}$ choices. Then pick the two distinct third vertices (other than the endpoints of the shared edge) in an unordered manner: there are $\\binom{n-2}{2}$ choices. Each such choice determines exactly one unordered pair of distinct triangles that share that specific edge. Therefore, the number of unordered pairs is\n$$\n\\binom{n}{2} \\binom{n-2}{2}.\n$$\nHence the covariance sum equals\n$$\n2 \\sum_{a<b} \\mathrm{Cov}(I_{a}, I_{b})\n\\;=\\;\n2 \\cdot \\binom{n}{2} \\binom{n-2}{2} \\cdot \\bigl(p^{5} - p^{6}\\bigr).\n$$\nCombining the variance and covariance contributions, we obtain\n$$\n\\mathrm{Var}(T)\n\\;=\\;\n\\binom{n}{3} p^{3} (1 - p^{3})\n\\;+\\;\n2 \\binom{n}{2} \\binom{n-2}{2} \\bigl(p^{5} - p^{6}\\bigr).\n$$\nIt is sometimes convenient to rewrite the combinatorial factor as\n$$\n2 \\binom{n}{2} \\binom{n-2}{2}\n\\;=\\;\n\\frac{n(n-1)}{1\\cdot 2} \\cdot \\frac{(n-2)(n-3)}{1\\cdot 2} \\cdot 2\n\\;=\\;\n\\frac{n(n-1)(n-2)(n-3)}{2}\n\\;=\\;\n12 \\binom{n}{4},\n$$\nyielding the equivalent closed form\n$$\n\\mathrm{Var}(T)\n\\;=\\;\n\\binom{n}{3} p^{3} (1 - p^{3})\n\\;+\\;\n12 \\binom{n}{4} \\, p^{5} (1 - p).\n$$\n\nNormal approximation regime. We now discuss when a normal approximation for $T$ is justified as $n \\to \\infty$. Two guiding principles are: (i) the Central Limit Theorem (CLT) for sums of many weakly dependent indicator variables arranged in a dependency graph, and (ii) the requirement that the variance diverges, i.e., $\\mathrm{Var}(T) \\to \\infty$, so that normalization by $\\sqrt{\\mathrm{Var}(T)}$ is meaningful. The dependency structure here is local: each triangle indicator $I_{a}$ depends only on its $3$ edges and is independent of all triangle indicators whose edge sets are disjoint. Each $I_{a}$ is dependent only with those triangle indicators that share at least $1$ edge with it. The number of triangles that share an edge with a given triangle is at most $3(n-3)$, i.e., the maximum dependency degree is of order $O(n)$, whereas the total number of summands is $\\binom{n}{3} = \\Theta(n^{3})$.\n\nThe variance decomposition shows two qualitatively distinct growth regimes:\n- Sparse-but-not-too-sparse regime $n^{-1} \\ll p \\ll n^{-1/2}$. In this regime,\n$$\n\\binom{n}{3} p^{3} \\;\\text{ dominates }, \\qquad \\mathrm{Var}(T) \\sim \\binom{n}{3} p^{3}.\n$$\nIndeed, the ratio of the covariance term to the variance term is of order\n$$\n\\frac{12 \\binom{n}{4} p^{5} (1 - p)}{\\binom{n}{3} p^{3}} \\;=\\; \\Theta(n p^{2}) \\;\\to\\; 0,\n$$\nso dependencies are negligible in aggregate, and the sum behaves like a sum of almost independent indicators with diverging mean and variance ($\\binom{n}{3} p^{3} \\to \\infty$ under $n^{-1} \\ll p$). The dependency-graph CLT heuristics (many summands, local dependence of bounded relative size, and $\\mathrm{Var}(T)\\to\\infty$) support asymptotic normality after centering and scaling.\n- Semi-dense-to-dense regime $n^{-1/2} \\ll p \\le c < 1$. In this regime,\n$$\n12 \\binom{n}{4} p^{5} (1 - p) \\;\\text{ dominates }, \\qquad \\mathrm{Var}(T) \\sim 12 \\binom{n}{4} p^{5} (1 - p),\n$$\nof order $\\Theta(n^{4} p^{5})$. Although dependence contributes the leading-order variance, it arises from many weakly correlated overlapping pairs spread across the graph. Classical tools such as Hoeffding decomposition for $U$-statistics or dependency-graph central limit theorems justify asymptotic normality for fixed $p \\in (0,1)$ and for $p$ in this semi-dense range as $n \\to \\infty$, provided $\\mathrm{Var}(T) \\to \\infty$ (which it does when $n^{4} p^{5} \\to \\infty$).\n\nBy contrast, in the ultra-sparse regime $p \\ll n^{-1}$, $\\mathbb{E}[T] \\to 0$ and the distribution of $T$ is degenerate at $0$, while at the critical sparse regime $p \\asymp n^{-1}$, $T$ typically exhibits a Poisson limit rather than a normal one. Summarizing: a normal approximation for $T$ is well supported when $\\mathrm{Var}(T) \\to \\infty$ and the dependency is sufficiently local relative to the number of summands, for example throughout $n^{-1} \\ll p \\ll 1$ with the caveat that at $p \\asymp n^{-1}$ a Poisson approximation is more appropriate.\n\nThe requested final closed-form expression for the variance is given above.", "answer": "$$\\boxed{\\binom{n}{3} \\, p^{3} \\left(1 - p^{3}\\right) \\;+\\; 12 \\binom{n}{4} \\, p^{5} \\left(1 - p\\right)}$$", "id": "3342490"}, {"introduction": "Biological networks are rarely pure random graphs; they often exhibit a 'small-world' structure, characterized by high local clustering and short global path lengths. This final exercise [@problem_id:3342454] puts theory into practice by simulating a model selection task, challenging you to fit both the Erdős-Rényi and the more structured Watts-Strogatz models to hypothetical network data. This hands-on, computational approach demonstrates how different models capture distinct topological features and provides a framework for quantitatively assessing their suitability.", "problem": "You are given four observed summary statistics for a protein–protein interaction network: the number of nodes $n$, the number of edges $m$, the average clustering coefficient $C_{\\mathrm{obs}}$, and the mean shortest path length $L_{\\mathrm{obs}}$. Your task is to perform a method-of-moments fitting procedure to two canonical network models and decide which model better explains the observed data:\n\n- The Erdős–Rényi random graph $G(n,p)$, where each pair of nodes is connected independently with probability $p$.\n- The Watts–Strogatz small-world model parameterized by the ring-lattice degree $k$ (assumed even) and rewiring probability $\\beta$.\n\nFundamental base and core definitions to be used:\n\n1. In the Erdős–Rényi $G(n,p)$ model, the expected number of edges is $E[m] = p \\binom{n}{2}$, the expected clustering coefficient is $C_{\\mathrm{ER}} \\approx p$, and, in the connected regime $np > 1$, the mean path length is well-approximated by\n$$\nL_{\\mathrm{ER}} \\approx \\frac{\\ln n}{\\ln (np)}.\n$$\n\n2. In the Watts–Strogatz model, start from a ring lattice with even degree $k$ and rewire each edge independently with probability $\\beta$. For the regular ring lattice before rewiring, the clustering coefficient is\n$$\nC_{\\mathrm{lat}}(k) = \\begin{cases}\n\\frac{3(k-2)}{4(k-1)}, & k \\ge 2, \\\\\n0, & k < 2,\n\\end{cases}\n$$\nand the mean path length is approximately\n$$\nL_{\\mathrm{lat}}(k) \\approx \\frac{n}{2k}.\n$$\nUpon rewiring with probability $\\beta$, a widely used moment approximation for clustering is\n$$\nC_{\\mathrm{WS}}(\\beta) \\approx C_{\\mathrm{lat}}(k) \\,(1-\\beta)^3,\n$$\nreflecting that each of the three edges of a triangle must avoid rewiring to remain a triangle. For the mean path length, use an interpolation that is consistent with the lattice limit and the random-like shortcut limit,\n$$\nL_{\\mathrm{rand}}(n,k) \\approx \\frac{\\ln n}{\\ln k}, \\quad k > 1,\n$$\nand\n$$\nL_{\\mathrm{WS}}(n,k,\\beta) \\approx L_{\\mathrm{lat}}(k)\\,\\exp\\!\\left(-\\frac{\\beta n k}{2}\\right) + L_{\\mathrm{rand}}(n,k)\\left(1 - \\exp\\!\\left(-\\frac{\\beta n k}{2}\\right)\\right),\n$$\nwhich uses the expected number of shortcuts $\\beta m \\approx \\beta n k / 2$ as the crossover scale.\n\nMethod-of-moments fitting protocol:\n\n- For $G(n,p)$, fit $p$ by matching the observed $m$ through the identity $E[m] = p \\binom{n}{2}$. Use $\\hat{p} = \\frac{2m}{n(n-1)}$, and then predict clustering by $\\hat{C}_{\\mathrm{ER}} = \\hat{p}$ and mean path length by $\\hat{L}_{\\mathrm{ER}} \\approx \\frac{\\ln n}{\\ln(n\\hat{p})}$ if $n\\hat{p} > 1$; otherwise take $\\hat{L}_{\\mathrm{ER}} = +\\infty$ as the asymptotic signature of a disconnected regime.\n\n- For Watts–Strogatz, fit $k$ by matching the observed $m$ with $E[m] = \\frac{nk}{2}$. Use $\\hat{k} = \\mathrm{round}\\!\\left(\\frac{2m}{n}\\right)$ adjusted to the nearest even integer in the range $[2,n-1]$. Then fit $\\beta$ by matching the observed clustering via $\\hat{\\beta} = 1 - \\left(\\frac{C_{\\mathrm{obs}}}{C_{\\mathrm{lat}}(\\hat{k})}\\right)^{1/3}$, truncated to the interval $[0,1]$. If $C_{\\mathrm{lat}}(\\hat{k}) = 0$ and $C_{\\mathrm{obs}} > 0$, set $\\hat{\\beta} = 1$; if $C_{\\mathrm{lat}}(\\hat{k}) = 0$ and $C_{\\mathrm{obs}} = 0$, set $\\hat{\\beta} = 0$. Predict mean path length by the interpolation formula above.\n\nModel selection criterion:\n\n- For each model, define the squared relative error\n$$\nE = \\left(\\frac{\\hat{C} - C_{\\mathrm{obs}}}{\\max(C_{\\mathrm{obs}}, \\epsilon)}\\right)^2 + \\left(\\frac{\\hat{L} - L_{\\mathrm{obs}}}{\\max(L_{\\mathrm{obs}}, \\epsilon)}\\right)^2,\n$$\nwith stabilization constant $\\epsilon = 10^{-12}$. Select the model with the smaller $E$. In case of a tie, select the Erdős–Rényi model.\n\nTest suite:\n\nEvaluate your program on the following four parameter sets $(n,m,C_{\\mathrm{obs}},L_{\\mathrm{obs}})$, expressed as dimensionless quantities:\n\n1. $(n,m,C_{\\mathrm{obs}},L_{\\mathrm{obs}}) = (800, 2920, 0.0092, 3.36)$,\n2. $(n,m,C_{\\mathrm{obs}},L_{\\mathrm{obs}}) = (1000, 5000, 0.4, 3.1)$,\n3. $(n,m,C_{\\mathrm{obs}},L_{\\mathrm{obs}}) = (500, 300, 0.002, 35.0)$,\n4. $(n,m,C_{\\mathrm{obs}},L_{\\mathrm{obs}}) = (2000, 8000, 0.5, 3.7)$.\n\nFinal output:\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each entry is an integer: $0$ if the Erdős–Rényi $G(n,p)$ model better explains the data, and $1$ if the Watts–Strogatz $(k,\\beta)$ model does. For the test suite above, the output must be in the exact format\n$$\n[\\text{result}_1,\\text{result}_2,\\text{result}_3,\\text{result}_4].\n$$", "solution": "We apply the specified fitting and selection protocol to each of the four test cases.\n\n**Case 1:** $(n,m,C_{\\mathrm{obs}},L_{\\mathrm{obs}}) = (800, 2920, 0.0092, 3.36)$\n- **ER Model:** $\\hat{p} = \\frac{2 \\cdot 2920}{800 \\cdot 799} \\approx 0.009136$.\n  - Predictions: $\\hat{C}_{\\mathrm{ER}} = 0.009136$. Since $n\\hat{p} \\approx 7.31 > 1$, $\\hat{L}_{\\mathrm{ER}} \\approx \\frac{\\ln(800)}{\\ln(7.31)} \\approx 3.35$.\n  - Error: $E_{\\mathrm{ER}} = \\left(\\frac{0.009136 - 0.0092}{0.0092}\\right)^2 + \\left(\\frac{3.35 - 3.36}{3.36}\\right)^2 \\approx 0.000048 + 0.000009 \\approx 5.7 \\times 10^{-5}$.\n- **WS Model:** $\\hat{k} = \\mathrm{round}(\\frac{2 \\cdot 2920}{800}) \\text{ to nearest even} = \\mathrm{round}(7.3) \\to 8$.\n  - $C_{\\mathrm{lat}}(8) = \\frac{3(6)}{4(7)} \\approx 0.643$. $\\hat{\\beta} = 1 - (\\frac{0.0092}{0.643})^{1/3} \\approx 0.758$.\n  - Predictions: $\\hat{C}_{\\mathrm{WS}} \\approx 0.0092$ (by fit). $\\hat{L}_{\\mathrm{WS}} \\approx \\frac{\\ln(800)}{\\ln(8)} \\approx 3.21$.\n  - Error: $E_{\\mathrm{WS}} = 0 + \\left(\\frac{3.21 - 3.36}{3.36}\\right)^2 \\approx 0.00196$.\n- **Selection:** $E_{\\mathrm{ER}}  E_{\\mathrm{WS}}$. The Erdős–Rényi model (0) is selected.\n\n**Case 2:** $(n,m,C_{\\mathrm{obs}},L_{\\mathrm{obs}}) = (1000, 5000, 0.4, 3.1)$\n- **ER Model:** $\\hat{p} = \\frac{2 \\cdot 5000}{1000 \\cdot 999} \\approx 0.01001$.\n  - Predictions: $\\hat{C}_{\\mathrm{ER}} = 0.01001$. Since $n\\hat{p} \\approx 10.01 > 1$, $\\hat{L}_{\\mathrm{ER}} \\approx \\frac{\\ln(1000)}{\\ln(10.01)} \\approx 3.00$.\n  - Error: $E_{\\mathrm{ER}} = \\left(\\frac{0.01001 - 0.4}{0.4}\\right)^2 + \\left(\\frac{3.00 - 3.1}{3.1}\\right)^2 \\approx 0.949 + 0.001 \\approx 0.95$.\n- **WS Model:** $\\hat{k} = \\mathrm{round}(\\frac{2 \\cdot 5000}{1000}) \\text{ to nearest even} = 10$.\n  - $C_{\\mathrm{lat}}(10) = \\frac{3(8)}{4(9)} \\approx 0.667$. $\\hat{\\beta} = 1 - (\\frac{0.4}{0.667})^{1/3} \\approx 0.157$.\n  - Predictions: $\\hat{C}_{\\mathrm{WS}} \\approx 0.4$ (by fit). $\\hat{L}_{\\mathrm{WS}} \\approx \\frac{\\ln(1000)}{\\ln(10)} \\approx 3.00$.\n  - Error: $E_{\\mathrm{WS}} = 0 + \\left(\\frac{3.00 - 3.1}{3.1}\\right)^2 \\approx 0.001$.\n- **Selection:** $E_{\\mathrm{WS}}  E_{\\mathrm{ER}}$. The Watts–Strogatz model (1) is selected. This network exhibits classic small-world properties (high clustering, low path length) that the ER model cannot capture.\n\n**Case 3:** $(n,m,C_{\\mathrm{obs}},L_{\\mathrm{obs}}) = (500, 300, 0.002, 35.0)$\n- **ER Model:** $\\hat{p} = \\frac{2 \\cdot 300}{500 \\cdot 499} \\approx 0.0024$.\n  - Predictions: $\\hat{C}_{\\mathrm{ER}} = 0.0024$. Since $n\\hat{p} \\approx 1.2 > 1$, $\\hat{L}_{\\mathrm{ER}} \\approx \\frac{\\ln(500)}{\\ln(1.2)} \\approx 34.05$.\n  - Error: $E_{\\mathrm{ER}} = \\left(\\frac{0.0024 - 0.002}{0.002}\\right)^2 + \\left(\\frac{34.05 - 35.0}{35.0}\\right)^2 \\approx 0.04 + 0.0007 \\approx 0.0407$.\n- **WS Model:** $\\hat{k} = \\mathrm{round}(\\frac{2 \\cdot 300}{500}) \\text{ to nearest even} = \\mathrm{round}(1.2) \\to 2$.\n  - $C_{\\mathrm{lat}}(2) = 0$. Since $C_{\\mathrm{obs}} > 0$, $\\hat{\\beta}=1$.\n  - Predictions: $\\hat{C}_{\\mathrm{WS}} = 0$. $\\hat{L}_{\\mathrm{WS}} \\approx \\frac{\\ln(500)}{\\ln(2)} \\approx 8.96$.\n  - Error: $E_{\\mathrm{WS}} = \\left(\\frac{0 - 0.002}{0.002}\\right)^2 + \\left(\\frac{8.96 - 35.0}{35.0}\\right)^2 \\approx 1 + 0.55 \\approx 1.55$.\n- **Selection:** $E_{\\mathrm{ER}}  E_{\\mathrm{WS}}$. The Erdős–Rényi model (0) is selected. The network is sparse and has low clustering, consistent with a random graph near the percolation threshold.\n\n**Case 4:** $(n,m,C_{\\mathrm{obs}},L_{\\mathrm{obs}}) = (2000, 8000, 0.5, 3.7)$\n- **ER Model:** $\\hat{p} = \\frac{2 \\cdot 8000}{2000 \\cdot 1999} \\approx 0.004$.\n  - Predictions: $\\hat{C}_{\\mathrm{ER}} = 0.004$. Since $n\\hat{p} \\approx 8 > 1$, $\\hat{L}_{\\mathrm{ER}} \\approx \\frac{\\ln(2000)}{\\ln(8)} \\approx 3.65$.\n  - Error: $E_{\\mathrm{ER}} = \\left(\\frac{0.004 - 0.5}{0.5}\\right)^2 + \\left(\\frac{3.65 - 3.7}{3.7}\\right)^2 \\approx 0.984 + 0.00018 \\approx 0.984$.\n- **WS Model:** $\\hat{k} = \\mathrm{round}(\\frac{2 \\cdot 8000}{2000}) \\text{ to nearest even} = 8$.\n  - $C_{\\mathrm{lat}}(8) = \\frac{3(6)}{4(7)} \\approx 0.643$. $\\hat{\\beta} = 1 - (\\frac{0.5}{0.643})^{1/3} \\approx 0.08$.\n  - Predictions: $\\hat{C}_{\\mathrm{WS}} \\approx 0.5$ (by fit). $\\hat{L}_{\\mathrm{WS}} \\approx \\frac{\\ln(2000)}{\\ln(8)} \\approx 3.65$.\n  - Error: $E_{\\mathrm{WS}} = 0 + \\left(\\frac{3.65 - 3.7}{3.7}\\right)^2 \\approx 0.00018$.\n- **Selection:** $E_{\\mathrm{WS}}  E_{\\mathrm{ER}}$. The Watts–Strogatz model (1) is selected. This is another classic small-world case.\n\n**Final Results Summary:**\nThe selection results for the four test cases are:\n1.  Erdős–Rényi (0)\n2.  Watts–Strogatz (1)\n3.  Erdős–Rényi (0)\n4.  Watts–Strogatz (1)\n\nThis yields the final output list: `[0,1,0,1]`.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef calculate_er_error(n, m, C_obs, L_obs, epsilon):\n    \"\"\"\n    Fits the Erd-os-Renyi model and calculates the squared relative error.\n    \"\"\"\n    # 1. Parameter Fitting\n    if n  2:\n        # Avoid division by zero if n(n-1) is zero\n        p_hat = 0.0\n    else:\n        p_hat = (2 * m) / (n * (n - 1))\n\n    # 2. Prediction\n    # Predicted clustering\n    C_er_hat = p_hat\n\n    # Predicted mean path length\n    np_hat = n * p_hat\n    if np_hat > 1:\n        L_er_hat = np.log(n) / np.log(np_hat)\n    else:\n        L_er_hat = np.inf\n\n    # 3. Error Calculation\n    C_err_term = ((C_er_hat - C_obs) / max(C_obs, epsilon))**2\n    L_err_term = ((L_er_hat - L_obs) / max(L_obs, epsilon))**2\n\n    return C_err_term + L_err_term\n\ndef calculate_ws_error(n, m, C_obs, L_obs, epsilon):\n    \"\"\"\n    Fits the Watts-Strogatz model and calculates the squared relative error.\n    \"\"\"\n    # 1. Parameter Fitting (k)\n    k_raw = (2 * m) / n\n    # Adjust to the nearest even integer\n    k_hat_temp = 2 * np.round(k_raw / 2)\n    # Clamp to the valid range [2, n-1]\n    k_hat = np.clip(k_hat_temp, 2, n - 1 if n > 1 else 2)\n\n    # 2. Parameter Fitting (beta)\n    # Calculate clustering of base lattice\n    if k_hat  2:\n        C_lat_k = 0.0\n    else:\n        C_lat_k = (3 * (k_hat - 2)) / (4 * (k_hat - 1))\n\n    # Fit beta\n    if C_lat_k > 0:\n        ratio = C_obs / C_lat_k\n        # Ensure ratio is non-negative for cube root\n        beta_raw = 1 - np.cbrt(max(0, ratio))\n    else: # C_lat_k == 0, which happens if k_hat == 2\n        if C_obs > 0:\n            beta_raw = 1.0\n        else: # C_obs == 0\n            beta_raw = 0.0\n    \n    # Truncate beta to [0, 1]\n    beta_hat = np.clip(beta_raw, 0, 1)\n\n    # 3. Prediction\n    # Predicted clustering\n    C_ws_hat = C_lat_k * (1 - beta_hat)**3\n\n    # Predicted mean path length\n    L_lat_k = n / (2 * k_hat)\n    if k_hat > 1:\n      L_rand_nk = np.log(n) / np.log(k_hat)\n    else: # Should not happen due to k_hat >= 2\n      L_rand_nk = np.inf\n    \n    crossover_exp = np.exp(-beta_hat * n * k_hat / 2)\n    L_ws_hat = L_lat_k * crossover_exp + L_rand_nk * (1 - crossover_exp)\n    \n    # 4. Error Calculation\n    C_err_term = ((C_ws_hat - C_obs) / max(C_obs, epsilon))**2\n    L_err_term = ((L_ws_hat - L_obs) / max(L_obs, epsilon))**2\n\n    return C_err_term + L_err_term\n\ndef solve():\n    \"\"\"\n    Main function to run the model selection procedure on the test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (800, 2920, 0.0092, 3.36),\n        (1000, 5000, 0.4, 3.1),\n        (500, 300, 0.002, 35.0),\n        (2000, 8000, 0.5, 3.7),\n    ]\n\n    epsilon = 1e-12\n    results = []\n    \n    for n, m, C_obs, L_obs in test_cases:\n        E_er = calculate_er_error(n, m, C_obs, L_obs, epsilon)\n        E_ws = calculate_ws_error(n, m, C_obs, L_obs, epsilon)\n        \n        # Model selection: 0 for ER, 1 for WS\n        # ER is chosen in case of a tie (E_ws >= E_er)\n        if E_ws  E_er:\n            results.append(1)\n        else:\n            results.append(0)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3342454"}]}