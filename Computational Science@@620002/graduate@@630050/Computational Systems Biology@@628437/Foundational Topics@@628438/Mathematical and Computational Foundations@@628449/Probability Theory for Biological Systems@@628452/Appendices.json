{"hands_on_practices": [{"introduction": "A common challenge in analyzing biological count data, such as from single-cell RNA sequencing, is accounting for overdispersion, where the variance is greater than the mean. The Negative Binomial distribution is a workhorse model for this scenario. This exercise provides practice with the method of moments, a direct and intuitive technique for estimating model parameters by equating theoretical moments to their sample counterparts [@problem_id:3340523]. You will not only derive and apply these estimators but also analyze their behavior in the limit of low overdispersion, revealing important insights into model identifiability.", "problem": "A clonal population of cells exhibits transcriptional bursting for a particular messenger ribonucleic acid (mRNA) species, leading to overdispersed counts across single cells measured by single-cell RNA sequencing (RNA-seq). A standard model for such counts is the Negative Binomial distribution with parameters $(r,p)$, denoted $\\mathrm{NB}(r,p)$, under the following parameterization: for $k \\in \\{0,1,2,\\dots\\}$,\n$$\n\\mathbb{P}(Y=k) \\;=\\; \\binom{k+r-1}{k} (1-p)^{k} p^{r}, \\quad r>0,\\; p\\in(0,1).\n$$\nUnder this parameterization, the first two moments are\n$$\n\\mathbb{E}[Y] \\;=\\; \\frac{r(1-p)}{p}, \n\\qquad\n\\mathrm{Var}(Y) \\;=\\; \\frac{r(1-p)}{p^{2}}.\n$$\nA large independent and identically distributed sample from this population yields an empirical mean $\\bar{y}$ and variance $s^{2}$. In a particular experiment, the summary statistics are $\\bar{y} = 7.5$ and $s^{2} = 22.5$.\n\nUsing the method of moments, do the following:\n- Derive the method-of-moments estimators $\\hat{r}$ and $\\hat{p}$ by equating the sample moments to the model moments and solving for $(r,p)$ in terms of $\\bar{y}$ and $s^{2}$.\n- Evaluate your expressions at $\\bar{y} = 7.5$ and $s^{2} = 22.5$ and report the numerical values of $(\\hat{r},\\hat{p})$. Round your answer to $4$ significant figures.\n- From first principles, analyze the identifiability of $(r,p)$ under method of moments in the regime $s^{2} \\to \\bar{y}^{+}$ with $\\bar{y}$ fixed. Explicitly state the limiting behavior of $\\hat{r}$ and $\\hat{p}$ and interpret it in terms of model distinguishability.\n\nExpress the final numerical answer as the ordered pair $(\\hat{r},\\hat{p})$. No units are required.", "solution": "The task is to derive and evaluate the method-of-moments estimators for the parameters of a Negative Binomial distribution, and to analyze their behavior in a specific limiting case.\n\nFirst, we derive the method-of-moments (MoM) estimators, $\\hat{r}$ and $\\hat{p}$, for the parameters $(r,p)$. The MoM procedure equates the theoretical moments of the probability distribution to the corresponding sample moments calculated from the data.\n\nWe are given the theoretical mean and variance for the Negative Binomial distribution, $\\mathrm{NB}(r,p)$:\n$$ \\mathbb{E}[Y] = \\frac{r(1-p)}{p} $$\n$$ \\mathrm{Var}(Y) = \\frac{r(1-p)}{p^2} $$\nThe sample moments are the sample mean, $\\bar{y}$, and the sample variance, $s^2$. The system of equations for the MoM is:\n$$ \\bar{y} = \\frac{\\hat{r}(1-\\hat{p})}{\\hat{p}} \\quad (1) $$\n$$ s^2 = \\frac{\\hat{r}(1-\\hat{p})}{\\hat{p}^2} \\quad (2) $$\nHere, $\\hat{r}$ and $\\hat{p}$ denote the estimators for $r$ and $p$.\n\nA straightforward way to solve this system is to notice the relationship between the variance and the mean. We can express the right-hand side of equation $(2)$ using the right-hand side of equation $(1)$:\n$$ s^2 = \\left( \\frac{\\hat{r}(1-\\hat{p})}{\\hat{p}} \\right) \\frac{1}{\\hat{p}} = \\frac{\\bar{y}}{\\hat{p}} $$\nFrom this relationship, we can directly solve for the estimator $\\hat{p}$ in terms of the sample moments:\n$$ \\hat{p} = \\frac{\\bar{y}}{s^2} $$\nTo find the estimator $\\hat{r}$, we substitute this expression for $\\hat{p}$ back into equation $(1)$:\n$$ \\bar{y} = \\frac{\\hat{r}\\left(1 - \\frac{\\bar{y}}{s^2}\\right)}{\\frac{\\bar{y}}{s^2}} $$\nAssuming $\\bar{y} > 0$, we can multiply both sides by $\\frac{1}{\\bar{y}}$ to get:\n$$ 1 = \\frac{\\hat{r}\\left(1 - \\frac{\\bar{y}}{s^2}\\right)}{\\left(\\frac{\\bar{y}}{s^2}\\right) \\bar{y}} = \\frac{\\hat{r}\\left(\\frac{s^2-\\bar{y}}{s^2}\\right)}{\\frac{\\bar{y}^2}{s^2}} $$\n$$ 1 = \\hat{r} \\frac{s^2 - \\bar{y}}{\\bar{y}^2} $$\nSolving for $\\hat{r}$ yields:\n$$ \\hat{r} = \\frac{\\bar{y}^2}{s^2 - \\bar{y}} $$\nFor the parameters to be valid ($r>0$ and $p \\in (0,1)$), the sample statistics must satisfy $\\bar{y}>0$ and $s^2 > \\bar{y}$. The condition $s^2 > \\bar{y}$ signifies overdispersion, a key feature that the Negative Binomial distribution models in contrast to the Poisson distribution (where variance equals the mean).\n\nNext, we evaluate these estimators using the provided experimental data: $\\bar{y} = 7.5$ and $s^2 = 22.5$.\nWe calculate $\\hat{p}$:\n$$ \\hat{p} = \\frac{7.5}{22.5} = \\frac{1}{3} $$\nAnd we calculate $\\hat{r}$:\n$$ \\hat{r} = \\frac{(7.5)^2}{22.5 - 7.5} = \\frac{56.25}{15} = 3.75 $$\nThe problem requires reporting the numerical values rounded to $4$ significant figures.\n$$ \\hat{p} = 0.\\overline{3} \\approx 0.3333 $$\n$$ \\hat{r} = 3.750 $$\nSo, the estimated parameter pair is $(\\hat{r}, \\hat{p}) = (3.750, 0.3333)$.\n\nFinally, we analyze the identifiability of $(r,p)$ under the method of moments in the regime where $s^2 \\to \\bar{y}^{+}$ with $\\bar{y}$ fixed. This corresponds to the case where the observed overdispersion is minimal. We examine the limiting behavior of our derived estimators:\n$$ \\lim_{s^2 \\to \\bar{y}^{+}} \\hat{p} = \\lim_{s^2 \\to \\bar{y}^{+}} \\frac{\\bar{y}}{s^2} = \\frac{\\bar{y}}{\\bar{y}} = 1 $$\n$$ \\lim_{s^2 \\to \\bar{y}^{+}} \\hat{r} = \\lim_{s^2 \\to \\bar{y}^{+}} \\frac{\\bar{y}^2}{s^2 - \\bar{y}} $$\nFor the limit of $\\hat{r}$, the numerator $\\bar{y}^2$ is a fixed positive constant, while the denominator $(s^2 - \\bar{y})$ approaches $0$ from the positive side. Therefore, the limit of $\\hat{r}$ is positive infinity:\n$$ \\lim_{s^2 \\to \\bar{y}^{+}} \\hat{r} = +\\infty $$\nThis limiting behavior, $\\hat{r} \\to \\infty$ and $\\hat{p} \\to 1$, corresponds to the well-known convergence of the Negative Binomial distribution to the Poisson distribution. A Poisson distribution is characterized by its mean being equal to its variance, i.e., $\\mathbb{E}[X] = \\mathrm{Var}(X) = \\lambda$. The condition $s^2 \\to \\bar{y}^{+}$ implies the data are becoming equidispersed, a characteristic property of a Poisson process. The MoM estimators reflect this by driving the NB parameters towards their values in the Poisson limit.\n\nIn terms of model distinguishability, this result highlights a practical and theoretical issue. As the sample variance $s^2$ gets closer to the sample mean $\\bar{y}$, the estimated parameter $\\hat{r}$ becomes extremely large and highly sensitive to minute changes in the sample moments. This numerical instability indicates poor identifiability of the parameter $r$ in the low-overdispersion regime. It becomes difficult to statistically distinguish a Negative Binomial distribution with very large $r$ and $p \\approx 1$ from a Poisson distribution with mean $\\bar{y}$. The two models become nearly indistinguishable, and the MoM estimation for the Negative Binomial model becomes ill-conditioned.", "answer": "$$ \\boxed{\\begin{pmatrix} 3.750 & 0.3333 \\end{pmatrix}} $$", "id": "3340523"}, {"introduction": "Moving from frequentist estimation to Bayesian inference allows us to formally incorporate prior knowledge into our models. This practice introduces the concept of conjugate priors, a cornerstone of Bayesian analysis that provides an elegant, analytical solution for updating our beliefs in light of new data. By working through the classic Poisson-Gamma conjugate model, you will derive the posterior distribution from first principles, gaining a concrete understanding of how Bayesian learning works [@problem_id:3340505].", "problem": "A single-cell gene expression experiment records the number of messenger ribonucleic acid (mRNA) molecules for a specific gene in a single cell, where unique molecular identifier (UMI)-based counting yields an observed count $x \\in \\{0,1,2,\\dots\\}$. To model the generative mechanism of counts in computational systems biology, suppose the count arises from a Poisson process over a unit-normalized sampling window, with rate parameter $\\theta > 0$, so that the conditional distribution of $x$ given $\\theta$ is Poisson with mean $\\theta$. Prior biological knowledge from previous experiments is summarized by a Gamma prior on $\\theta$ with shape parameter $\\alpha > 0$ and rate parameter $\\beta > 0$. The following foundational definitions are to be used:\n\n- The Poisson probability mass function for $x$ given $\\theta$ is\n$$\np(x \\mid \\theta) \\;=\\; \\frac{\\theta^{x} \\exp(-\\theta)}{x!} \\quad \\text{for} \\quad x \\in \\{0,1,2,\\dots\\}.\n$$\n\n- The Gamma probability density function for $\\theta$ with shape $\\alpha$ and rate $\\beta$ is\n$$\np(\\theta) \\;=\\; \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} \\,\\theta^{\\alpha - 1} \\exp(-\\beta \\theta) \\quad \\text{for} \\quad \\theta > 0.\n$$\n\nUsing Bayes’ theorem and these definitions as the fundamental base, derive the posterior density $p(\\theta \\mid x)$ for the single observation $x$ in this conjugate model and identify the posterior distribution class and its parameters. Then compute the posterior mean $\\mathbb{E}[\\theta \\mid x]$ as a closed-form analytic expression in terms of $\\alpha$, $\\beta$, and $x$. Your final answer must be a single closed-form expression for $\\mathbb{E}[\\theta \\mid x]$. No rounding is required.", "solution": "The goal is to derive the posterior distribution for the parameter $\\theta$ given an observation $x$, and then to compute its mean. This is a classic Bayesian update problem using a conjugate prior.\n\nAccording to Bayes' theorem, the posterior density $p(\\theta \\mid x)$ is proportional to the product of the likelihood $p(x \\mid \\theta)$ and the prior $p(\\theta)$:\n$$\np(\\theta \\mid x) \\propto p(x \\mid \\theta) \\, p(\\theta)\n$$\nWe are given the likelihood (Poisson) and the prior (Gamma):\n$$\np(x \\mid \\theta) = \\frac{\\theta^{x} e^{-\\theta}}{x!}\n$$\n$$\np(\\theta) = \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} \\theta^{\\alpha - 1} e^{-\\beta \\theta}\n$$\nSubstituting these into the proportionality for the posterior, we can drop any factors that do not depend on $\\theta$, as they will be absorbed into the normalization constant:\n$$\np(\\theta \\mid x) \\propto \\left( \\theta^{x} e^{-\\theta} \\right) \\times \\left( \\theta^{\\alpha - 1} e^{-\\beta \\theta} \\right)\n$$\nNow, we group the terms involving $\\theta$:\n$$\np(\\theta \\mid x) \\propto \\theta^{x} \\theta^{\\alpha - 1} e^{-\\theta} e^{-\\beta \\theta}\n$$\n$$\np(\\theta \\mid x) \\propto \\theta^{(x + \\alpha) - 1} e^{-(\\beta + 1) \\theta}\n$$\nThis expression is the kernel of a Gamma distribution. A Gamma distribution with shape parameter $\\alpha'$ and rate parameter $\\beta'$ has a probability density function proportional to $\\theta^{\\alpha' - 1} e^{-\\beta' \\theta}$. By comparing this general form with our result, we can identify the parameters of the posterior distribution:\n-   Posterior shape: $\\alpha' = x + \\alpha$\n-   Posterior rate: $\\beta' = \\beta + 1$\n\nThus, the posterior distribution of $\\theta$ given the data $x$ is a Gamma distribution:\n$$\n\\theta \\mid x \\sim \\mathrm{Gamma}(x + \\alpha, \\beta + 1)\n$$\nThe problem also asks for the posterior mean, $\\mathbb{E}[\\theta \\mid x]$. The mean of a Gamma distribution with shape $\\alpha'$ and rate $\\beta'$ is given by the ratio $\\frac{\\alpha'}{\\beta'}$.\n\nUsing our posterior parameters, the posterior mean is:\n$$\n\\mathbb{E}[\\theta \\mid x] = \\frac{x + \\alpha}{\\beta + 1}\n$$\nThis is the closed-form analytical expression for the posterior mean of $\\theta$.", "answer": "$$\\boxed{\\frac{x + \\alpha}{\\beta + 1}}$$", "id": "3340505"}, {"introduction": "While conjugate priors are elegant, many realistic biological models do not have such simple analytical solutions for their posterior distributions. This is where computational methods like Markov chain Monte Carlo (MCMC) become essential. This advanced practice guides you through the setup of a Metropolis-Hastings algorithm, a powerful and flexible MCMC method, to perform Bayesian inference on the parameters of a birth-death process [@problem_id:3340583]. Deriving the acceptance ratio from scratch will demystify this computational engine and equip you with the foundational skills needed to build custom samplers for complex problems in systems biology.", "problem": "A messenger ribonucleic acid (mRNA) species in a clonal population of cells is modeled by a linear birth–death process with zero-order synthesis rate $k$ (molecules per unit time) and first-order degradation rate $\\gamma$ (per unit time). Under standard assumptions of the chemical master equation and ergodicity, at steady state the molecular count $Y$ in an individual cell is distributed as a Poisson random variable with mean $m = k/\\gamma$. You collect counts $y_{1},\\dots,y_{n}$ from $n$ independent cells, each assumed to be at steady state and conditionally independent given $(k,\\gamma)$.\n\nYou wish to perform Bayesian inference for the parameter vector $\\theta = (k,\\gamma)$ with independent Gamma priors: $k \\sim \\mathrm{Gamma}(a_{k},b_{k})$ and $\\gamma \\sim \\mathrm{Gamma}(a_{\\gamma},b_{\\gamma})$, using the shape–rate parameterization with density $p(x) = \\frac{b^{a}}{\\Gamma(a)} x^{a-1} \\exp(-b x)$ for $x>0$, $a>0$, $b>0$.\n\nConsider a Markov chain Monte Carlo (MCMC) algorithm of the Metropolis–Hastings type to sample from the posterior distribution of $\\theta$. At each iteration, from the current state $\\theta=(k,\\gamma)$ you propose a new state $\\theta'=(k',\\gamma')$ via an independent log-normal random-walk on each coordinate:\n- $\\ln k' = \\ln k + \\varepsilon_{k}$ with $\\varepsilon_{k} \\sim \\mathcal{N}(0,\\sigma_{k}^{2})$,\n- $\\ln \\gamma' = \\ln \\gamma + \\varepsilon_{\\gamma}$ with $\\varepsilon_{\\gamma} \\sim \\mathcal{N}(0,\\sigma_{\\gamma}^{2})$,\nand $\\varepsilon_{k}$ and $\\varepsilon_{\\gamma}$ are independent of each other and of the past.\n\nStarting from the foundational definitions of the Poisson likelihood for independent observations, the Gamma prior densities, and the Metropolis–Hastings acceptance rule in terms of the target density and the proposal density, and without assuming any “shortcut” formulas, do the following:\n- Briefly specify the steps of one iteration of the Metropolis–Hastings algorithm appropriate for this setting.\n- Derive the exact analytic expression for the Metropolis–Hastings acceptance ratio $r$ for the move $\\theta \\to \\theta'$ under the above log-normal random-walk proposal and model assumptions, fully simplified in terms of $n$, the data $y_{1},\\dots,y_{n}$, the current and proposed parameters $(k,\\gamma)$ and $(k',\\gamma')$, and the hyperparameters $a_{k},b_{k},a_{\\gamma},b_{\\gamma}$.\n\nYour final answer must be a single closed-form expression for $r$. No numerical evaluation is required, and no rounding should be performed. Do not include any units in the final expression.", "solution": "The problem asks for a brief description of a Metropolis-Hastings (MH) iteration and the derivation of the acceptance ratio $r$ for a specific Bayesian model.\n\n### Metropolis-Hastings Algorithm Steps\n\nOne iteration of the MH algorithm for this problem proceeds as follows:\n1.  Given the current state of the parameters $\\theta^{(t)} = (k^{(t)}, \\gamma^{(t)})$, propose a new state $\\theta' = (k', \\gamma')$ by drawing independent normal variates $\\varepsilon_{k} \\sim \\mathcal{N}(0, \\sigma_{k}^{2})$ and $\\varepsilon_{\\gamma} \\sim \\mathcal{N}(0, \\sigma_{\\gamma}^{2})$, and setting $k' = k^{(t)} \\exp(\\varepsilon_k)$ and $\\gamma' = \\gamma^{(t)} \\exp(\\varepsilon_\\gamma)$.\n2.  Calculate the acceptance ratio $r$ using the formula derived below.\n3.  Draw a uniform random number $u \\sim U(0,1)$. If $u  r$, accept the proposal and set $\\theta^{(t+1)} = \\theta'$. Otherwise, reject the proposal and set $\\theta^{(t+1)} = \\theta^{(t)}$.\n\n### Derivation of the Acceptance Ratio\n\nThe Metropolis-Hastings acceptance ratio $r$ is given by:\n$$r = \\frac{p(\\theta' | \\mathbf{y}) q(\\theta | \\theta')}{p(\\theta | \\mathbf{y}) q(\\theta' | \\theta)}$$\nwhere $p(\\theta | \\mathbf{y})$ is the posterior density (the target) and $q(\\theta' | \\theta)$ is the proposal density. Since the posterior is proportional to the likelihood times the prior, $p(\\theta | \\mathbf{y}) \\propto p(\\mathbf{y} | \\theta)p(\\theta)$, we can use the unnormalized posterior $\\pi(\\theta) = p(\\mathbf{y} | \\theta)p(\\theta)$ in the ratio:\n$$r = \\frac{\\pi(\\theta')}{\\pi(\\theta)} \\frac{q(\\theta | \\theta')}{q(\\theta' | \\theta)}$$\n\n**1. Unnormalized Posterior Ratio:**\nThe unnormalized posterior $\\pi(\\theta)$ is the product of the likelihood and the priors. Let $S = \\sum_{i=1}^{n} y_{i}$.\nThe likelihood is $p(\\mathbf{y} | k, \\gamma) = \\prod_{i=1}^n \\mathrm{Poisson}(y_i | k/\\gamma) \\propto (k/\\gamma)^S \\exp(-n k/\\gamma)$.\nThe prior is $p(k, \\gamma) = \\mathrm{Gamma}(k|a_k, b_k) \\mathrm{Gamma}(\\gamma|a_\\gamma, b_\\gamma) \\propto k^{a_k-1}e^{-b_k k} \\cdot \\gamma^{a_\\gamma-1}e^{-b_\\gamma \\gamma}$.\nCombining them:\n$$ \\pi(k, \\gamma) \\propto \\left(k^S \\gamma^{-S} e^{-nk/\\gamma}\\right) \\left(k^{a_k-1}e^{-b_k k}\\right) \\left(\\gamma^{a_\\gamma-1}e^{-b_\\gamma \\gamma}\\right) $$\n$$ \\pi(k, \\gamma) \\propto k^{S+a_k-1} \\gamma^{-S+a_\\gamma-1} \\exp\\left( -n k \\gamma^{-1} - b_k k - b_\\gamma \\gamma \\right) $$\nThe ratio of posteriors is:\n$$ \\frac{\\pi(k', \\gamma')}{\\pi(k, \\gamma)} = \\frac{(k')^{S+a_k-1} (\\gamma')^{-S+a_\\gamma-1}}{(k)^{S+a_k-1} (\\gamma)^{-S+a_\\gamma-1}} \\frac{\\exp\\left( -n k'/\\gamma' - b_k k' - b_\\gamma \\gamma' \\right)}{\\exp\\left( -n k/\\gamma - b_k k - b_\\gamma \\gamma \\right)} $$\n$$ \\frac{\\pi(\\theta')}{\\pi(\\theta)} = \\left(\\frac{k'}{k}\\right)^{S+a_k-1} \\left(\\frac{\\gamma'}{\\gamma}\\right)^{-S+a_\\gamma-1} \\exp\\left[ \\left( n\\frac{k}{\\gamma} + b_k k + b_\\gamma\\gamma \\right) - \\left( n\\frac{k'}{\\gamma'} + b_k k' + b_\\gamma\\gamma' \\right) \\right] $$\n\n**2. Proposal Density Ratio:**\nThe proposal for $\\theta'=(k', \\gamma')$ from $\\theta=(k, \\gamma)$ is defined by $\\ln k' = \\ln k + \\varepsilon_k$ and $\\ln \\gamma' = \\ln \\gamma + \\varepsilon_\\gamma$. The proposal distribution for $(\\ln k', \\ln \\gamma')$ is a symmetric normal distribution centered at $(\\ln k, \\ln \\gamma)$. To find the proposal density $q(k', \\gamma' | k, \\gamma)$ in terms of $k'$ and $\\gamma'$, we must perform a change of variables, which introduces a Jacobian term.\nThe density of proposing $\\theta'$ from $\\theta$ is:\n$$q(\\theta'|\\theta) = f_{\\mathcal{N}}(\\ln k' - \\ln k; 0, \\sigma_k^2) f_{\\mathcal{N}}(\\ln \\gamma' - \\ln \\gamma; 0, \\sigma_\\gamma^2) \\left| \\det \\frac{\\partial(\\ln k', \\ln \\gamma')}{\\partial(k', \\gamma')} \\right|$$\nThe Jacobian determinant is $\\left| \\begin{vmatrix} 1/k'  0 \\\\ 0  1/\\gamma' \\end{vmatrix} \\right| = \\frac{1}{k'\\gamma'}$.\nSo, $q(\\theta'|\\theta) \\propto \\frac{1}{k'\\gamma'}$.\nSimilarly, the reverse proposal density is $q(\\theta|\\theta') \\propto \\frac{1}{k\\gamma}$.\nThe ratio of proposal densities is:\n$$ \\frac{q(\\theta | \\theta')}{q(\\theta' | \\theta)} = \\frac{1/(k\\gamma)}{1/(k'\\gamma')} = \\frac{k'\\gamma'}{k\\gamma} $$\nThis term is the Jacobian correction for the log-normal random walk.\n\n**3. Final Acceptance Ratio:**\nWe combine the two ratios:\n$$ r = \\left[ \\left(\\frac{k'}{k}\\right)^{S+a_k-1} \\left(\\frac{\\gamma'}{\\gamma}\\right)^{-S+a_\\gamma-1} \\exp\\left[ \\left( n\\frac{k}{\\gamma} + b_k k + b_\\gamma\\gamma \\right) - \\left( n\\frac{k'}{\\gamma'} + b_k k' + b_\\gamma\\gamma' \\right) \\right] \\right] \\cdot \\left[ \\frac{k'\\gamma'}{k\\gamma} \\right] $$\nWe rewrite the Jacobian term as $\\left(\\frac{k'}{k}\\right)\\left(\\frac{\\gamma'}{\\gamma}\\right)$ and combine the exponents:\n$$ r = \\left(\\frac{k'}{k}\\right)^{(S+a_k-1)+1} \\left(\\frac{\\gamma'}{\\gamma}\\right)^{(-S+a_\\gamma-1)+1} \\exp\\left[ n\\left(\\frac{k}{\\gamma} - \\frac{k'}{\\gamma'}\\right) + b_k(k - k') + b_\\gamma(\\gamma - \\gamma') \\right] $$\nSimplifying the exponents gives the final expression:\n$$ r = \\left(\\frac{k'}{k}\\right)^{S+a_k} \\left(\\frac{\\gamma'}{\\gamma}\\right)^{-S+a_\\gamma} \\exp\\left[ n\\left(\\frac{k}{\\gamma} - \\frac{k'}{\\gamma'}\\right) + b_k(k - k') + b_\\gamma(\\gamma - \\gamma') \\right] $$\nwhere $S = \\sum_{i=1}^{n} y_{i}$. This expression is fully simplified as requested.", "answer": "$$\n\\boxed{\n\\left(\\frac{k'}{k}\\right)^{\\left(\\sum_{i=1}^{n} y_i\\right) + a_k} \\left(\\frac{\\gamma'}{\\gamma}\\right)^{-\\left(\\sum_{i=1}^{n} y_i\\right) + a_\\gamma} \\exp\\left[ n\\left(\\frac{k}{\\gamma} - \\frac{k'}{\\gamma'}\\right) + b_k\\left(k - k'\\right) + b_\\gamma\\left(\\gamma - \\gamma'\\right) \\right]\n}\n$$", "id": "3340583"}]}