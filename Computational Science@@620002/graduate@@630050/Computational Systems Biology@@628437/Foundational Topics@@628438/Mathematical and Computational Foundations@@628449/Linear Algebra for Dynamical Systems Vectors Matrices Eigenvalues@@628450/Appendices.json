{"hands_on_practices": [{"introduction": "The first step in analyzing the stability of a dynamical system is to characterize its Jacobian matrix at a steady state. This practice focuses on the foundational skill of eigen-analysis. By calculating eigenvalues, their algebraic and geometric multiplicities, and the corresponding eigenspaces, you will uncover the fundamental modes of behavior near a steady state and diagnose key structural properties of the system's dynamics [@problem_id:3323551].", "problem": "In a linearized model of a small gene regulatory network near a steady state, the local dynamics of small perturbations are represented by the linear system $\\dot{\\mathbf{x}} = A \\mathbf{x}$, where $\\mathbf{x} \\in \\mathbb{R}^{3}$ encodes deviations in transcription factor activities and $A \\in \\mathbb{R}^{3 \\times 3}$ is the Jacobian matrix of the regulation rates evaluated at the steady state. Consider the Jacobian\n$$\nA = \\begin{bmatrix}\n4  1  -1\\\\\n0  4  0\\\\\n0  0  2\n\\end{bmatrix}.\n$$\nUsing the foundational definitions that eigenvalues of $A$ are roots $\\lambda \\in \\mathbb{C}$ of the characteristic polynomial $p_{A}(\\lambda) = \\det(A - \\lambda I)$, that the algebraic multiplicity of an eigenvalue is its multiplicity as a root of $p_{A}$, and that the geometric multiplicity is the dimension of the eigenspace $\\ker(A - \\lambda I)$, compute all eigenvalues of $A$, determine their algebraic and geometric multiplicities, and find a basis for each eigenspace.\n\nTo ensure a unique canonical representative for each eigenspace basis, you must select for each eigenspace a single basis vector $b = (b_{x}, b_{y}, b_{z})$ that is a primitive integer eigenvector, meaning that $b_{x}, b_{y}, b_{z} \\in \\mathbb{Z}$, $\\gcd(|b_{x}|,|b_{y}|,|b_{z}|) = 1$, and the first nonzero component of $b$ is positive.\n\nReport your final answer as a single row matrix containing, in order, $\\lambda_{1}$, $m_{a}(\\lambda_{1})$, $m_{g}(\\lambda_{1})$, $b_{1x}$, $b_{1y}$, $b_{1z}$, $\\lambda_{2}$, $m_{a}(\\lambda_{2})$, $m_{g}(\\lambda_{2})$, $b_{2x}$, $b_{2y}$, $b_{2z}$, where $\\lambda_{1}  \\lambda_{2}$ are the distinct eigenvalues, $m_{a}$ denotes algebraic multiplicity, $m_{g}$ denotes geometric multiplicity, and $b_{k} = (b_{kx}, b_{ky}, b_{kz})$ is the canonical basis vector chosen as specified above. No rounding is required.", "solution": "The problem is validated as scientifically grounded, well-posed, objective, and complete. It constitutes a standard exercise in linear algebra applied to the analysis of linear dynamical systems. All required data and definitions are provided, and the constraints for selecting canonical basis vectors ensure a unique solution. We may proceed with the solution.\n\nThe first step is to compute the eigenvalues of the matrix $A$. By definition, the eigenvalues $\\lambda$ are the roots of the characteristic polynomial $p_{A}(\\lambda) = \\det(A - \\lambda I)$, where $I$ is the $3 \\times 3$ identity matrix.\nThe matrix $A - \\lambda I$ is given by:\n$$\nA - \\lambda I = \\begin{bmatrix}\n4 - \\lambda  1  -1\\\\\n0  4 - \\lambda  0\\\\\n0  0  2 - \\lambda\n\\end{bmatrix}\n$$\nSince this is an upper triangular matrix, its determinant is the product of its diagonal entries:\n$$\np_{A}(\\lambda) = \\det(A - \\lambda I) = (4 - \\lambda)(4 - \\lambda)(2 - \\lambda) = (4 - \\lambda)^{2}(2 - \\lambda)\n$$\nSetting the characteristic polynomial to zero, $p_{A}(\\lambda) = 0$, yields the eigenvalues. The roots are $\\lambda = 4$ and $\\lambda = 2$. Following the problem's ordering convention, $\\lambda_{1}  \\lambda_{2}$, we have $\\lambda_{1} = 4$ and $\\lambda_{2} = 2$.\n\nThe algebraic multiplicity of an eigenvalue is its multiplicity as a root of the characteristic polynomial.\nFor $\\lambda_{1} = 4$, the factor $(4 - \\lambda)$ appears with an exponent of $2$. Therefore, the algebraic multiplicity of $\\lambda_{1}=4$ is $m_{a}(\\lambda_{1}) = 2$.\nFor $\\lambda_{2} = 2$, the factor $(2 - \\lambda)$ appears with an exponent of $1$. Therefore, the algebraic multiplicity of $\\lambda_{2}=2$ is $m_{a}(\\lambda_{2}) = 1$.\n\nNext, we determine the geometric multiplicity and find a basis for the eigenspace for each eigenvalue. The geometric multiplicity, $m_{g}(\\lambda)$, is the dimension of the null space (kernel) of the matrix $A - \\lambda I$. The eigenspace is the set of all vectors $\\mathbf{v}$ such that $(A - \\lambda I)\\mathbf{v} = \\mathbf{0}$.\n\nFor $\\lambda_{1} = 4$:\nWe need to find the null space of $A - 4I$.\n$$\nA - 4I = \\begin{bmatrix}\n4 - 4  1  -1\\\\\n0  4 - 4  0\\\\\n0  0  2 - 4\n\\end{bmatrix} = \\begin{bmatrix}\n0  1  -1\\\\\n0  0  0\\\\\n0  0  -2\n\\end{bmatrix}\n$$\nLet $\\mathbf{v} = \\begin{pmatrix} v_{x} \\\\ v_{y} \\\\ v_{z} \\end{pmatrix}$ be an eigenvector. The system $(A - 4I)\\mathbf{v} = \\mathbf{0}$ becomes:\n$$\n\\begin{bmatrix}\n0  1  -1\\\\\n0  0  0\\\\\n0  0  -2\n\\end{bmatrix}\n\\begin{pmatrix} v_{x} \\\\ v_{y} \\\\ v_{z} \\end{pmatrix}\n=\n\\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\nThis corresponds to the system of linear equations:\n$1) \\quad 0v_{x} + 1v_{y} - 1v_{z} = 0 \\implies v_{y} = v_{z}$\n$2) \\quad 0 = 0$\n$3) \\quad -2v_{z} = 0 \\implies v_{z} = 0$\nFrom equation $3$, we have $v_{z}=0$. Substituting this into equation $1$ gives $v_{y}=0$. The variable $v_{x}$ is not constrained and can be any scalar, say $t$. So, the eigenvectors are of the form:\n$$\n\\mathbf{v} = \\begin{pmatrix} t \\\\ 0 \\\\ 0 \\end{pmatrix} = t \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\nThe eigenspace for $\\lambda_{1} = 4$ is the span of the vector $\\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}$. The dimension of this space is $1$, so the geometric multiplicity is $m_{g}(\\lambda_{1}) = 1$.\nTo find the canonical basis vector $\\mathbf{b}_{1} = (b_{1x}, b_{1y}, b_{1z})$, we use the vector $(1, 0, 0)$. It has integer components, $\\gcd(|1|, |0|, |0|) = 1$, and its first nonzero component ($1$) is positive. Thus, $\\mathbf{b}_{1} = (1, 0, 0)$.\n\nFor $\\lambda_{2} = 2$:\nWe find the null space of $A - 2I$.\n$$\nA - 2I = \\begin{bmatrix}\n4 - 2  1  -1\\\\\n0  4 - 2  0\\\\\n0  0  2 - 2\n\\end{bmatrix} = \\begin{bmatrix}\n2  1  -1\\\\\n0  2  0\\\\\n0  0  0\n\\end{bmatrix}\n$$\nThe system $(A - 2I)\\mathbf{v} = \\mathbf{0}$ becomes:\n$$\n\\begin{bmatrix}\n2  1  -1\\\\\n0  2  0\\\\\n0  0  0\n\\end{bmatrix}\n\\begin{pmatrix} v_{x} \\\\ v_{y} \\\\ v_{z} \\end{pmatrix}\n=\n\\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\nThis corresponds to the system of linear equations:\n$1) \\quad 2v_{x} + v_{y} - v_{z} = 0$\n$2) \\quad 2v_{y} = 0 \\implies v_{y} = 0$\n$3) \\quad 0 = 0$\nSubstituting $v_{y}=0$ into equation $1$ gives $2v_{x} - v_{z} = 0$, which implies $v_{z} = 2v_{x}$. The variable $v_{x}$ is not constrained and can be any scalar, say $s$. So, the eigenvectors are of the form:\n$$\n\\mathbf{v} = \\begin{pmatrix} s \\\\ 0 \\\\ 2s \\end{pmatrix} = s \\begin{pmatrix} 1 \\\\ 0 \\\\ 2 \\end{pmatrix}\n$$\nThe eigenspace for $\\lambda_{2} = 2$ is the span of the vector $\\begin{pmatrix} 1 \\\\ 0 \\\\ 2 \\end{pmatrix}$. The dimension of this space is $1$, so the geometric multiplicity is $m_{g}(\\lambda_{2}) = 1$, a fact that is always true when the algebraic multiplicity is 1.\nTo find the canonical basis vector $\\mathbf{b}_{2} = (b_{2x}, b_{2y}, b_{2z})$, we use the vector $(1, 0, 2)$. It has integer components, $\\gcd(|1|, |0|, |2|) = 1$, and its first nonzero component ($1$) is positive. Thus, $\\mathbf{b}_{2} = (1, 0, 2)$.\n\nWe can now assemble the final answer as requested.\nFor $\\lambda_{1}=4$: $m_{a}(4) = 2$, $m_{g}(4) = 1$, $\\mathbf{b}_{1} = (1, 0, 0)$.\nFor $\\lambda_{2}=2$: $m_{a}(2) = 1$, $m_{g}(2) = 1$, $\\mathbf{b}_{2} = (1, 0, 2)$.\n\nThe final result is a row matrix containing these twelve values in the specified order.", "answer": "$$\n\\boxed{\\begin{pmatrix} 4  2  1  1  0  0  2  1  1  1  0  2 \\end{pmatrix}}\n$$", "id": "3323551"}, {"introduction": "Eigenvalues directly inform the nature of system trajectories, with complex eigenvalues signaling oscillatory behavior. This exercise makes that connection concrete by tasking you with computing the matrix exponential, the system's fundamental solution. By deriving the solution for a canonical matrix that generates rotation and scaling, you will build a powerful intuition for how damped or amplified spiral dynamics emerge from the entries of the Jacobian matrix [@problem_id:3323549].", "problem": "In a two-species regulatory module in computational systems biology, consider small perturbations $\\mathbf{x}(t) \\in \\mathbb{R}^{2}$ around a steady state of a smooth nonlinear system. The linearization yields a linear time-invariant ordinary differential equation (ODE) of the form $\\frac{d\\mathbf{x}}{dt} = B\\,\\mathbf{x}$, where the Jacobian matrix $B$ is\n$$\nB = \\begin{pmatrix}\na  -b \\\\\nb  a\n\\end{pmatrix},\n$$\nwith $a \\in \\mathbb{R}$ and $b \\in \\mathbb{R}_{\\ge 0}$ representing the net local growth/decay rate and the strength of cross-coupling, respectively. Assume $a$ and $b$ have units of inverse time and $t$ is measured in seconds so that $B\\,t$ is dimensionless.\n\nStarting from the core definition of the matrix exponential and without assuming any pre-derived formula specific to this matrix form, compute the closed-form expression for the matrix exponential $\\exp(B\\,t)$ as a $2 \\times 2$ matrix in terms of $a$, $b$, and $t$. Then, interpret the resulting linear flow as an exponentially damped or amplified planar rotation, stating the rotation frequency and the exponential rate, and explain the implications for trajectories in the phase plane.\n\nYour final answer should be the single closed-form analytic expression for $\\exp(B\\,t)$ written as a $2 \\times 2$ matrix. No numerical approximation is required, and no rounding should be applied. No physical units should appear in the final expression.", "solution": "The problem as stated is scientifically grounded, well-posed, objective, and self-contained. It presents a standard problem in the analysis of linear dynamical systems, which is a core topic in computational systems biology and related fields. The physical units are consistent, and all necessary information for a unique solution is provided. Therefore, the problem is valid, and we may proceed with the solution.\n\nThe task is to compute the matrix exponential $\\exp(B\\,t)$ for the matrix $B = \\begin{pmatrix} a  -b \\\\ b  a \\end{pmatrix}$, starting from the fundamental definition of the matrix exponential as a power series. The solution to the linear ODE $\\frac{d\\mathbf{x}}{dt} = B\\,\\mathbf{x}$ is given by $\\mathbf{x}(t) = \\exp(B\\,t)\\mathbf{x}(0)$.\n\nWe begin by decomposing the matrix $B$ into a sum of two commuting matrices. This is a standard and effective strategy. Let $I$ be the $2 \\times 2$ identity matrix and $J$ be the matrix $J = \\begin{pmatrix} 0  -1 \\\\ 1  0 \\end{pmatrix}$. We can write $B$ as:\n$$\nB = a \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} + b \\begin{pmatrix} 0  -1 \\\\ 1  0 \\end{pmatrix} = aI + bJ\n$$\nThe matrices $aI$ and $bJ$ commute, since $(aI)(bJ) = abJ$ and $(bJ)(aI) = baJ$, and scalar multiplication is commutative. For any two commuting matrices $M$ and $N$, the property $\\exp(M+N) = \\exp(M)\\exp(N)$ holds. Applying this to $B\\,t$, we have:\n$$\n\\exp(B\\,t) = \\exp((aI + bJ)t) = \\exp(aIt + bJt)\n$$\nSince $aIt$ and $bJt$ commute, we can separate the exponentials:\n$$\n\\exp(B\\,t) = \\exp(aIt) \\exp(bJt)\n$$\nWe now compute each matrix exponential term separately using the power series definition $\\exp(M) = \\sum_{k=0}^{\\infty} \\frac{M^k}{k!}$.\n\nFirst, for the term $\\exp(aIt)$:\nThe matrix $aIt$ is a scalar matrix $\\begin{pmatrix} at  0 \\\\ 0  at \\end{pmatrix}$.\n$$\n\\exp(aIt) = \\sum_{k=0}^{\\infty} \\frac{(aIt)^k}{k!} = \\sum_{k=0}^{\\infty} \\frac{(at)^k I^k}{k!}\n$$\nSince $I^k = I$ for any integer $k \\ge 1$ and $I^0=I$, this simplifies to:\n$$\n\\exp(aIt) = \\left( \\sum_{k=0}^{\\infty} \\frac{(at)^k}{k!} \\right) I = \\exp(at)I = \\begin{pmatrix} \\exp(at)  0 \\\\ 0  \\exp(at) \\end{pmatrix}\n$$\nThis term represents a uniform scaling (expansion or contraction) of the phase space.\n\nSecond, for the term $\\exp(bJt)$:\nWe must compute the powers of the matrix $J$:\n$$\nJ = \\begin{pmatrix} 0  -1 \\\\ 1  0 \\end{pmatrix}\n$$\n$$\nJ^2 = \\begin{pmatrix} 0  -1 \\\\ 1  0 \\end{pmatrix} \\begin{pmatrix} 0  -1 \\\\ 1  0 \\end{pmatrix} = \\begin{pmatrix} -1  0 \\\\ 0  -1 \\end{pmatrix} = -I\n$$\n$$\nJ^3 = J \\cdot J^2 = J(-I) = -J\n$$\n$$\nJ^4 = (J^2)^2 = (-I)^2 = I\n$$\nThe powers of $J$ are cyclic with a period of $4$: $J, -I, -J, I, \\dots$. Now we substitute this pattern into the power series for $\\exp(bJt)$:\n$$\n\\exp(bJt) = \\sum_{k=0}^{\\infty} \\frac{(bJt)^k}{k!} = \\sum_{k=0}^{\\infty} \\frac{(bt)^k}{k!} J^k\n$$\n$$\n\\exp(bJt) = \\frac{(bt)^0}{0!}J^0 + \\frac{(bt)^1}{1!}J^1 + \\frac{(bt)^2}{2!}J^2 + \\frac{(bt)^3}{3!}J^3 + \\frac{(bt)^4}{4!}J^4 + \\dots\n$$\nSubstituting the powers of $J$:\n$$\n\\exp(bJt) = I + (bt)J - \\frac{(bt)^2}{2!}I - \\frac{(bt)^3}{3!}J + \\frac{(bt)^4}{4!}I + \\dots\n$$\nWe can group the terms containing $I$ and the terms containing $J$:\n$$\n\\exp(bJt) = \\left( 1 - \\frac{(bt)^2}{2!} + \\frac{(bt)^4}{4!} - \\dots \\right)I + \\left( bt - \\frac{(bt)^3}{3!} + \\frac{(bt)^5}{5!} - \\dots \\right)J\n$$\nThe series in the parentheses are the Taylor series for $\\cos(bt)$ and $\\sin(bt)$, respectively. Therefore:\n$$\n\\exp(bJt) = \\cos(bt)I + \\sin(bt)J\n$$\nIn matrix form, this is:\n$$\n\\exp(bJt) = \\cos(bt)\\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} + \\sin(bt)\\begin{pmatrix} 0  -1 \\\\ 1  0 \\end{pmatrix} = \\begin{pmatrix} \\cos(bt)  -\\sin(bt) \\\\ \\sin(bt)  \\cos(bt) \\end{pmatrix}\n$$\nThis is the standard matrix for a counter-clockwise rotation by an angle of $bt$.\n\nFinally, we combine the two parts to obtain $\\exp(B\\,t)$:\n$$\n\\exp(B\\,t) = \\exp(aIt) \\exp(bJt) = (\\exp(at)I) \\begin{pmatrix} \\cos(bt)  -\\sin(bt) \\\\ \\sin(bt)  \\cos(bt) \\end{pmatrix}\n$$\n$$\n\\exp(B\\,t) = \\exp(at) \\begin{pmatrix} \\cos(bt)  -\\sin(bt) \\\\ \\sin(bt)  \\cos(bt) \\end{pmatrix} = \\begin{pmatrix} \\exp(at)\\cos(bt)  -\\exp(at)\\sin(bt) \\\\ \\exp(at)\\sin(bt)  \\exp(at)\\cos(bt) \\end{pmatrix}\n$$\nThis is the closed-form expression for the matrix exponential.\n\nThe interpretation of the linear flow $\\mathbf{x}(t) = \\exp(B\\,t)\\mathbf{x}(0)$ follows directly from this result. The operator $\\exp(B\\,t)$ acts on an initial state $\\mathbf{x}(0)$ by simultaneously rotating it and scaling its magnitude.\nThe matrix $\\begin{pmatrix} \\cos(bt)  -\\sin(bt) \\\\ \\sin(bt)  \\cos(bt) \\end{pmatrix}$ represents a pure rotation in the plane by an angle $\\theta(t) = bt$. The angular frequency of this rotation is $\\omega = \\frac{d\\theta}{dt} = b$. Given $b \\ge 0$, the rotation is counter-clockwise for $b0$ and absent for $b=0$. The **rotation frequency** is $b$.\nThe scalar factor $\\exp(at)$ scales the magnitude of the vector at every instant. The norm of the state vector evolves as $||\\mathbf{x}(t)|| = \\exp(at) ||\\mathbf{x}(0)||$. The **exponential rate** of growth or decay is $a$.\n\nThe implications for trajectories in the phase plane depend on the sign of $a$ (for $b0$):\n1.  If $a  0$, the factor $\\exp(at)$ decays to $0$ as $t \\to \\infty$. The trajectories are spirals converging to the origin. This corresponds to a **stable focus** (or stable spiral), indicating that perturbations from a steady state decay over time.\n2.  If $a  0$, the factor $\\exp(at)$ grows unboundedly as $t \\to \\infty$. The trajectories are spirals diverging from the origin. This corresponds to an **unstable focus** (or unstable spiral), indicating that small perturbations are amplified, leading the system away from the steady state.\n3.  If $a = 0$, the factor $\\exp(at)$ is always $1$. The magnitude of the state vector is conserved. The trajectories are circles centered at the origin. This corresponds to a **center**, indicating sustained oscillations around the steady state.\n\nThis behavior is consistent with the analysis of the eigenvalues of $B$, which are $\\lambda_{1,2} = a \\pm ib$. The real part, $a$, governs the exponential growth or decay, while the imaginary part, $b$, governs the frequency of oscillation.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\exp(at)\\cos(bt)  -\\exp(at)\\sin(bt) \\\\\n\\exp(at)\\sin(bt)  \\exp(at)\\cos(bt)\n\\end{pmatrix}\n}\n$$", "id": "3323549"}, {"introduction": "In practical modeling, system parameters are never known with perfect precision. This raises a critical question: how robust are our stability predictions to small uncertainties? This exercise introduces the concept of eigenvalue conditioning, which quantifies the sensitivity of an eigenvalue to perturbations in the Jacobian matrix. By calculating the condition number using both left and right eigenvectors, you will learn to assess the reliability of your stability analysis, an essential skill for building credible biological models [@problem_id:3323592].", "problem": "Consider a linearized model of a two-species regulatory module near a steady state in a continuous-time ordinary differential equation (ODE) framework, where the Jacobian matrix at the steady state is\n$$\nA=\\begin{bmatrix}1  1\\\\ 0  2\\end{bmatrix}.\n$$\nThe eigenvalue $\\lambda=1$ corresponds to a mode of the linearized dynamics. Using the fundamental definitions of right and left eigenvectors for non-symmetric matrices and the standard notion of eigenvalue sensitivity in the induced vector $2$-norm, perform the following steps:\n- Determine a nonzero right eigenvector $u$ associated with $\\lambda=1$.\n- Determine a nonzero left eigenvector $w$ associated with $\\lambda=1$.\n- From first principles of first-order eigenvalue perturbation theory for non-Hermitian matrices, use the standard definition of the eigenvalue condition number in the vector $2$-norm based on the left and right eigenvectors to evaluate the conditioning of $\\lambda=1$ for the given matrix.\n\nReport only the final value of the eigenvalue condition number for $\\lambda=1$ as your answer. The condition number is dimensionless; provide an exact expression. No rounding is required.", "solution": "The user wants me to solve a problem from computational systems biology.\n\nHere's a plan:\n1.  **Validate the Problem**:\n    *   **Step 1: Extract Givens**:\n        *   The system is a linearized model of a two-species regulatory module.\n        *   The framework is a continuous-time ODE.\n        *   The Jacobian matrix at the steady state is $A = \\begin{bmatrix} 1  1 \\\\ 0  2 \\end{bmatrix}$.\n        *   There is an eigenvalue $\\lambda = 1$.\n        *   The task is to find a right eigenvector $u$ and a left eigenvector $w$ for $\\lambda=1$.\n        *   Then, using first-order eigenvalue perturbation theory, I need to calculate the eigenvalue condition number for $\\lambda=1$.\n        *   The condition number should be based on the vector $2$-norm.\n        *   I need to report the exact value of this condition number.\n\n    *   **Step 2: Validate Using Extracted Givens**:\n        *   **Scientific Grounding**: The problem is well-grounded in linear algebra and its application to dynamical systems, specifically stability analysis in computational biology. The concepts of Jacobian matrices, eigenvalues, eigenvectors, and eigenvalue sensitivity are standard. The matrix is non-symmetric, which is common in biological models. This is scientifically sound.\n        *   **Well-Posed**: The problem is well-posed. The matrix is given, and the eigenvalue is specified. The tasks are clearly defined: find eigenvectors and compute the condition number. There is a unique definition for the eigenvalue condition number for a simple eigenvalue of a non-symmetric matrix. I should first check if $\\lambda=1$ is a simple eigenvalue.\n            *   The characteristic polynomial is $\\det(A - \\lambda I) = (1-\\lambda)(2-\\lambda) - (1)(0) = (1-\\lambda)(2-\\lambda)$.\n            *   The eigenvalues are $\\lambda_1 = 1$ and $\\lambda_2 = 2$.\n            *   Since the eigenvalues are distinct, they are simple (algebraic multiplicity is 1). Therefore, the theory for simple eigenvalues applies.\n            *   A unique solution for the condition number exists.\n        *   **Objective**: The language is objective and precise.\n        *   **Completeness**: All necessary information is provided. The matrix $A$ and the eigenvalue $\\lambda$ are given. The definition of the condition number is standard in this context.\n        *   **Realism**: The matrix is a simple $2 \\times 2$ matrix, but it represents a plausible linearization of a biological system. It's not physically impossible.\n        *   **Structure**: The problem is well-structured and leads to a specific, computable result.\n        *   **Triviality**: The problem is not trivial. It requires knowledge of left and right eigenvectors and the formula for eigenvalue condition number for non-symmetric matrices.\n        *   **Verifiability**: The calculations are mathematically verifiable.\n\n    *   **Step 3: Verdict and Action**: The problem is valid. I will proceed with the solution.\n\n2.  **Solve the Problem**:\n    *   **Part 1: Find the right eigenvector $u$ for $\\lambda=1$.**\n        *   The definition of a right eigenvector $u$ is $Au = \\lambda u$.\n        *   This is equivalent to $(A - \\lambda I)u = 0$.\n        *   For $\\lambda = 1$, we have $(A - 1 \\cdot I)u = 0$.\n        *   $A - I = \\begin{bmatrix} 1  1 \\\\ 0  2 \\end{bmatrix} - \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix} = \\begin{bmatrix} 0  1 \\\\ 0  1 \\end{bmatrix}$.\n        *   Let $u = \\begin{pmatrix} u_1 \\\\ u_2 \\end{pmatrix}$.\n        *   The equation is $\\begin{bmatrix} 0  1 \\\\ 0  1 \\end{bmatrix} \\begin{pmatrix} u_1 \\\\ u_2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$.\n        *   This gives the equation $0 \\cdot u_1 + 1 \\cdot u_2 = 0$, which simplifies to $u_2 = 0$.\n        *   $u_1$ can be any non-zero value. Let's choose $u_1 = 1$.\n        *   So, a nonzero right eigenvector is $u = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$.\n\n    *   **Part 2: Find the left eigenvector $w$ for $\\lambda=1$.**\n        *   The definition of a left eigenvector $w$ is $w^H A = \\lambda w^H$ (or $w^T A = \\lambda w^T$ for real matrices). The problem uses non-Hermitian matrices, so the conjugate transpose $w^H$ is the general form. Since the matrix is real, $w^H = w^T$.\n        *   This is equivalent to $A^T w = \\lambda w$ (taking the transpose/conjugate transpose of the whole equation).\n        *   So, we need to solve $(A^T - \\lambda I)w = 0$.\n        *   $A^T = \\begin{bmatrix} 1  0 \\\\ 1  2 \\end{bmatrix}$.\n        *   For $\\lambda = 1$, we have $(A^T - 1 \\cdot I)w = 0$.\n        *   $A^T - I = \\begin{bmatrix} 1  0 \\\\ 1  2 \\end{bmatrix} - \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix} = \\begin{bmatrix} 0  0 \\\\ 1  1 \\end{bmatrix}$.\n        *   Let $w = \\begin{pmatrix} w_1 \\\\ w_2 \\end{pmatrix}$.\n        *   The equation is $\\begin{bmatrix} 0  0 \\\\ 1  1 \\end{bmatrix} \\begin{pmatrix} w_1 \\\\ w_2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$.\n        *   This gives the equation $w_1 + w_2 = 0$, which means $w_1 = -w_2$.\n        *   Let's choose $w_2 = -1$, then $w_1 = 1$.\n        *   So, a nonzero left eigenvector is $w = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$.\n\n    *   **Part 3: Calculate the eigenvalue condition number.**\n        *   The problem asks for the condition number from first principles of first-order eigenvalue perturbation theory for non-Hermitian matrices.\n        *   Let's consider a perturbed matrix $A + \\delta A$. The perturbed eigenvalue is $\\lambda + \\delta \\lambda$.\n        *   The first-order perturbation formula for a simple eigenvalue $\\lambda$ is:\n            $\\delta \\lambda \\approx \\frac{w^H (\\delta A) u}{w^H u}$.\n            Here, $u$ and $w$ are the right and left eigenvectors, respectively, corresponding to $\\lambda$. The superscript $H$ denotes the conjugate transpose. Since our matrix and vectors are real, $w^H = w^T$.\n        *   From this, we can derive the condition number. The standard definition of the condition number $\\kappa(\\lambda)$ is the maximum amplification of the perturbation:\n            $\\kappa(\\lambda) = \\lim_{\\epsilon \\to 0} \\sup_{\\|\\delta A\\|_2 \\le \\epsilon} \\frac{|\\delta \\lambda|}{\\|\\delta A\\|_2}$.\n        *   Using the perturbation formula:\n            $|\\delta \\lambda| \\approx \\frac{|w^T (\\delta A) u|}{|w^T u|}$.\n        *   By the Cauchy-Schwarz inequality, $|w^T (\\delta A) u| \\le \\|w^T\\|_2 \\|(\\delta A) u\\|_2$.\n        *   Also, $\\|(\\delta A) u\\|_2 \\le \\|\\delta A\\|_2 \\|u\\|_2$.\n        *   Combining these, we get $|\\delta \\lambda| \\lesssim \\frac{\\|w^T\\|_2 \\|\\delta A\\|_2 \\|u\\|_2}{|w^T u|} = \\frac{\\|w\\|_2 \\|u\\|_2}{|w^T u|} \\|\\delta A\\|_2$.\n        *   The equality can be achieved for a specific choice of $\\delta A$ (a rank-one perturbation of the form $\\delta A = \\epsilon w u^T / (\\|w\\|_2 \\|u\\|_2)$).\n        *   Thus, the condition number is given by the formula:\n            $\\kappa(\\lambda) = \\frac{\\|w\\|_2 \\|u\\|_2}{|w^T u|}$.\n        *   This is the standard definition the problem refers to. It measures the sensitivity of the eigenvalue $\\lambda$ to perturbations in the matrix $A$. The formula can also be written as $\\kappa(\\lambda) = \\frac{1}{|\\cos(\\theta)|}$, where $\\theta$ is the angle between the left and right eigenvectors.\n\n    *   **Calculations**:\n        *   We found the right eigenvector $u = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$.\n        *   We found the left eigenvector $w = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$.\n        *   The choice of scaling for the eigenvectors does not matter, as the scaling factor will cancel out in the numerator and denominator of the condition number formula. For example, if we replace $u$ with $\\alpha u$ and $w$ with $\\beta w$, the new condition number is $\\frac{\\|\\beta w\\|_2 \\|\\alpha u\\|_2}{|\\beta^* \\alpha (w^H u)|} = \\frac{|\\beta| |\\alpha| \\|w\\|_2 \\|u\\|_2}{|\\beta| |\\alpha| |w^H u|} = \\frac{\\|w\\|_2 \\|u\\|_2}{|w^H u|}$.\n        *   Calculate the norms (vector 2-norm):\n            $\\|u\\|_2 = \\sqrt{1^2 + 0^2} = \\sqrt{1} = 1$.\n            $\\|w\\|_2 = \\sqrt{1^2 + (-1)^2} = \\sqrt{2}$.\n        *   Calculate the inner product $w^T u$ (since vectors are real):\n            $w^T u = \\begin{pmatrix} 1  -1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = (1)(1) + (-1)(0) = 1$.\n        *   The absolute value is $|w^T u| = |1| = 1$.\n        *   Now, substitute these into the formula for the condition number $\\kappa(\\lambda=1)$:\n            $\\kappa(1) = \\frac{\\|w\\|_2 \\|u\\|_2}{|w^T u|} = \\frac{(\\sqrt{2})(1)}{1} = \\sqrt{2}$.\n\n3.  **Final formatting**:\n    *   The solution needs to be written out step-by-step, with all mathematical entities in LaTeX.\n    *   The final answer must be in the `\\boxed{...}` format.\n    *   The final answer is $\\sqrt{2}$.\nThe problem has been validated and is sound. I will now proceed with the formal solution.\n\nThe problem requires the calculation of the condition number for the eigenvalue $\\lambda=1$ of the given matrix $A$. The analysis proceeds in three steps: determination of the right eigenvector, determination of the left eigenvector, and computation of the condition number.\n\nThe given matrix is:\n$$\nA = \\begin{bmatrix} 1  1 \\\\ 0  2 \\end{bmatrix}\n$$\nThe eigenvalue in question is $\\lambda = 1$.\n\nStep 1: Determine a nonzero right eigenvector $u$.\nA right eigenvector $u$ corresponding to an eigenvalue $\\lambda$ satisfies the equation $A u = \\lambda u$, which can be rewritten as $(A - \\lambda I)u = 0$, where $I$ is the identity matrix.\nFor $\\lambda = 1$, we have:\n$$\n(A - 1 \\cdot I) u = \\left( \\begin{bmatrix} 1  1 \\\\ 0  2 \\end{bmatrix} - \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix} \\right) \\begin{pmatrix} u_1 \\\\ u_2 \\end{pmatrix} = \\begin{bmatrix} 0  1 \\\\ 0  1 \\end{bmatrix} \\begin{pmatrix} u_1 \\\\ u_2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n$$\nThis matrix equation corresponds to the single linear equation $u_2 = 0$. The variable $u_1$ is a free variable. To obtain a nonzero eigenvector, we can choose any non-zero value for $u_1$. Let us set $u_1 = 1$.\nThus, a right eigenvector associated with $\\lambda=1$ is:\n$$\nu = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}\n$$\n\nStep 2: Determine a nonzero left eigenvector $w$.\nA left eigenvector $w$ corresponding to an eigenvalue $\\lambda$ satisfies the equation $w^H A = \\lambda w^H$, where $w^H$ is the conjugate transpose of $w$. For real matrices and vectors, this simplifies to $w^T A = \\lambda w^T$. Taking the transpose of this equation yields $A^T w = \\lambda w$. This means that the left eigenvectors of $A$ are the right eigenvectors of its transpose, $A^T$.\nWe solve $(A^T - \\lambda I)w = 0$ for $\\lambda=1$.\nThe transpose of $A$ is:\n$$\nA^T = \\begin{bmatrix} 1  0 \\\\ 1  2 \\end{bmatrix}\n$$\nFor $\\lambda = 1$, we have:\n$$\n(A^T - 1 \\cdot I) w = \\left( \\begin{bmatrix} 1  0 \\\\ 1  2 \\end{bmatrix} - \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix} \\right) \\begin{pmatrix} w_1 \\\\ w_2 \\end{pmatrix} = \\begin{bmatrix} 0  0 \\\\ 1  1 \\end{bmatrix} \\begin{pmatrix} w_1 \\\\ w_2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n$$\nThis matrix equation corresponds to the single linear equation $w_1 + w_2 = 0$, which implies $w_1 = -w_2$. We can choose any non-zero value for $w_2$. Let us set $w_2 = -1$, which gives $w_1 = 1$.\nThus, a left eigenvector associated with $\\lambda=1$ is:\n$$\nw = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}\n$$\n\nStep 3: Evaluate the conditioning of $\\lambda=1$.\nFor a simple eigenvalue $\\lambda$ of a non-Hermitian matrix $A$, the condition number $\\kappa(\\lambda)$ measures the sensitivity of the eigenvalue to small perturbations in the matrix. First-order perturbation theory shows that a small change $\\delta A$ in the matrix $A$ results in a change $\\delta \\lambda$ in the eigenvalue given by $\\delta \\lambda \\approx \\frac{w^H (\\delta A) u}{w^H u}$.\nThe condition number with respect to the vector $2$-norm is defined as:\n$$\n\\kappa(\\lambda) = \\frac{1}{|s|} \\quad \\text{where} \\quad s = \\frac{w^H u}{\\|w\\|_2 \\|u\\|_2}\n$$\nThe term $s$ is the cosine of the angle between the left and right eigenvectors. The condition number can therefore be written as:\n$$\n\\kappa(\\lambda) = \\frac{\\|w\\|_2 \\|u\\|_2}{|w^H u|}\n$$\nThe matrix $A$ and its eigenvectors $u$ and $w$ are real, so the conjugate transpose $w^H$ is simply the transpose $w^T$.\nWe now compute the necessary quantities:\nThe $2$-norm of the right eigenvector $u$:\n$$\n\\|u\\|_2 = \\sqrt{1^2 + 0^2} = 1\n$$\nThe $2$-norm of the left eigenvector $w$:\n$$\n\\|w\\|_2 = \\sqrt{1^2 + (-1)^2} = \\sqrt{2}\n$$\nThe inner product $w^T u$:\n$$\nw^T u = \\begin{pmatrix} 1  -1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = (1)(1) + (-1)(0) = 1\n$$\nThe absolute value of the inner product is $|w^T u| = |1| = 1$. It is important to note that since the right and left eigenvectors for a simple eigenvalue cannot be orthogonal, this denominator is guaranteed to be non-zero.\n\nSubstituting these values into the formula for the condition number of $\\lambda=1$:\n$$\n\\kappa(1) = \\frac{\\|w\\|_2 \\|u\\|_2}{|w^T u|} = \\frac{(\\sqrt{2})(1)}{1} = \\sqrt{2}\n$$\nThe value of the condition number is independent of the scaling chosen for the eigenvectors. A condition number greater than $1$ indicates that the eigenvalue is sensitive to perturbations. For a normal matrix (e.g., a symmetric matrix), the left and right eigenvectors are the same, and the condition number is always $1$. For this non-symmetric matrix, the condition number is $\\sqrt{2}$.", "answer": "$$\n\\boxed{\\sqrt{2}}\n$$", "id": "3323592"}]}