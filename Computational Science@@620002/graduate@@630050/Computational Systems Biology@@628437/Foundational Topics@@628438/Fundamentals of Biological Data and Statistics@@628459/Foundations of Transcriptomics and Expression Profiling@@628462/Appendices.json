{"hands_on_practices": [{"introduction": "A foundational challenge in transcriptomics is making expression levels comparable across different samples. This practice delves into the critical concept of normalization by exposing the pitfalls of naive library-size scaling in the face of compositional bias—a common scenario where a few highly expressed genes change, distorting the proportions of all others. By manually calculating Trimmed Mean of $M$-values (TMM) factors, you will gain a first-principles understanding of how robust methods correct for these artifacts, ensuring that downstream differential expression analysis is built on a sound statistical footing [@problem_id:3311834].", "problem": "A sequencing-based transcriptome profiling experiment yields read count data for two samples, denoted sample $A$ and sample $B$, across $10$ genes. Assume the underlying sequencing process can be modeled as multinomial sampling from a population of messenger ribonucleic acid (mRNA) molecules: each sample’s observed counts are random draws with probabilities proportional to true relative transcript abundances, scaled by its total library size. Normalization aims to recover between-sample comparability by removing depth and composition effects so that, for the majority of genes whose true abundances do not change between samples, normalized expression ratios are centered at one.\n\nYou are told that the dataset exhibits strong composition bias: two highly abundant ribosomal genes are strongly upregulated in sample $B$, while the remaining genes do not change in true abundance. The observed counts are:\n\n- Sample $A$ has total library size $N_{A} = 1{,}000{,}000$ and counts:\n  gene $1$: $100{,}000$; gene $2$: $80{,}000$; gene $3$: $150{,}000$; gene $4$: $130{,}000$; gene $5$: $120{,}000$; gene $6$: $100{,}000$; gene $7$: $90{,}000$; gene $8$: $70{,}000$; gene $9$: $80{,}000$; gene $10$: $80{,}000$.\n\n- Sample $B$ has total library size $N_{B} = 1{,}048{,}000$ and counts:\n  gene $1$: $400{,}000$; gene $2$: $320{,}000$; gene $3$: $60{,}000$; gene $4$: $52{,}000$; gene $5$: $48{,}000$; gene $6$: $40{,}000$; gene $7$: $36{,}000$; gene $8$: $28{,}000$; gene $9$: $32{,}000$; gene $10$: $32{,}000$.\n\nUsing the Trimmed Mean of $M$-values (TMM) normalization (Robinson and Oshlack), compute the normalization factor for sample $B$ relative to sample $A$. For this dataset, employ equal weights for retained genes and trim by excluding the two genes with the largest absolute log expression ratios (i.e., genes $1$ and $2$). Report the TMM normalization factor as a unitless scalar rounded to four significant figures.\n\nIn addition, provide a first-principles explanation, grounded in the multinomial sampling view of read generation and the goal of centering the majority of unchanged genes, for why naive library size scaling based solely on $\\frac{N_{A}}{N_{B}}$ fails under this composition bias.\n\nRound your final numerical answer to four significant figures and report it as a unitless factor.", "solution": "The problem statement will first be validated against the specified criteria.\n\n### Step 1: Extract Givens\n- Samples: sample $A$, sample $B$.\n- Number of genes: $10$.\n- Model: Multinomial sampling from a population of messenger ribonucleic acid (mRNA) molecules.\n- Composition Bias: Two highly abundant ribosomal genes (gene $1$ and gene $2$) are strongly upregulated in sample $B$. The remaining genes do not change in true abundance.\n- Sample $A$ data:\n  - Total library size: $N_{A} = 1,000,000$.\n  - Gene counts ($C_{gA}$):\n    - gene $1$: $100,000$\n    - gene $2$: $80,000$\n    - gene $3$: $150,000$\n    - gene $4$: $130,000$\n    - gene $5$: $120,000$\n    - gene $6$: $100,000$\n    - gene $7$: $90,000$\n    - gene $8$: $70,000$\n    - gene $9$: $80,000$\n    - gene $10$: $80,000$\n- Sample $B$ data:\n  - Total library size: $N_{B} = 1,048,000$.\n  - Gene counts ($C_{gB}$):\n    - gene $1$: $400,000$\n    - gene $2$: $320,000$\n    - gene $3$: $60,000$\n    - gene $4$: $52,000$\n    - gene $5$: $48,000$\n    - gene $6$: $40,000$\n    - gene $7$: $36,000$\n    - gene $8$: $28,000$\n    - gene $9$: $32,000$\n    - gene $10$: $32,000$\n- Normalization method: Trimmed Mean of $M$-values (TMM).\n- TMM parameters:\n  - Reference sample: sample $A$.\n  - Target sample: sample $B$.\n  - Weights: Equal weights for retained genes.\n  - Trimming: Exclude the two genes with the largest absolute log expression ratios.\n- Required output: TMM normalization factor for sample $B$ rounded to four significant figures, and a first-principles explanation for the failure of naive library size scaling.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientifically Grounded:** The problem is firmly rooted in computational systems biology, specifically in the analysis of transcriptomics data (RNA-seq). The multinomial sampling model is a standard theoretical framework for sequencing counts. Composition bias is a well-documented artifact in such data, and TMM normalization is a widely accepted method to address it. The data provided, while designed for a clear-cut example, is numerically consistent (counts sum to library sizes) and represents a plausible, albeit exaggerated, biological scenario. The problem is scientifically sound.\n2.  **Well-Posed:** The problem provides all necessary data and defines the methodology (TMM), reference sample ($A$), trimming criteria (top $2$ genes by absolute M-value), and weighting scheme (equal weights). These specifications are sufficient to compute a unique normalization factor.\n3.  **Objective:** The problem is phrased in precise, technical language, free from subjectivity or opinion. The data and tasks are stated factually.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete, reasoned solution will be provided.\n\n### Solution\n\nThe solution consists of two parts: first, the calculation of the Trimmed Mean of $M$-values (TMM) normalization factor, and second, an explanation of why naive library size scaling is inappropriate under the given conditions.\n\n#### Part 1: Calculation of the TMM Normalization Factor\n\nThe TMM method aims to estimate an effective scaling factor for library sizes that accounts for composition bias. The procedure involves calculating log-fold-changes ($M$-values) and average abundances ($A$-values), trimming genes at the extremes of these metrics, and then computing a weighted mean of the $M$-values for the remaining genes to derive the normalization factor.\n\nLet $C_{gi}$ be the observed read count for gene $g$ in sample $i \\in \\{A, B\\}$, and let $N_i$ be the total library size for sample $i$. The normalized expression proportion for gene $g$ in sample $i$ is $P_{gi} = C_{gi} / N_i$.\n\nThe $M$-value (log-ratio) for gene $g$ compares the expression in sample $B$ relative to the reference sample $A$:\n$$M_g = \\log_2\\left(\\frac{P_{gB}}{P_{gA}}\\right) = \\log_2\\left(\\frac{C_{gB}/N_B}{C_{gA}/N_A}\\right) = \\log_2\\left(\\frac{C_{gB}}{C_{gA}} \\cdot \\frac{N_A}{N_B}\\right)$$\n\nWe are given $N_A = 1,000,000$ and $N_B = 1,048,000$. The library size ratio is $\\frac{N_A}{N_B} = \\frac{1,000,000}{1,048,000} \\approx 0.9542$.\n\nLet's calculate the $M$-values for all $10$ genes.\n\nFor gene $1$: $C_{1A} = 100,000$, $C_{1B} = 400,000$. The count ratio is $\\frac{C_{1B}}{C_{1A}} = 4$.\n$$M_1 = \\log_2\\left(4 \\cdot \\frac{1,000,000}{1,048,000}\\right) \\approx \\log_2(3.8168) \\approx 1.932$$\n\nFor gene $2$: $C_{2A} = 80,000$, $C_{2B} = 320,000$. The count ratio is $\\frac{C_{2B}}{C_{2A}} = 4$.\n$$M_2 = \\log_2\\left(4 \\cdot \\frac{1,000,000}{1,048,000}\\right) \\approx \\log_2(3.8168) \\approx 1.932$$\n\nFor genes $3$ through $10$, the problem is constructed such that the ratio of counts $C_{gB}/C_{gA}$ is constant. Let's verify for gene $3$:\n$C_{3A} = 150,000$, $C_{3B} = 60,000$. The count ratio is $\\frac{C_{3B}}{C_{3A}} = \\frac{60,000}{150,000} = 0.4$.\nFor gene $4$: $C_{4A} = 130,000$, $C_{4B} = 52,000$. The count ratio is $\\frac{C_{4B}}{C_{4A}} = \\frac{52,000}{130,000} = 0.4$.\nThis pattern holds for genes $g \\in \\{3, 4, \\dots, 10\\}$. For this set of genes, the $M$-value is:\n$$M_{g \\in \\{3..10\\}} = \\log_2\\left(0.4 \\cdot \\frac{1,000,000}{1,048,000}\\right) \\approx \\log_2(0.38168) \\approx -1.389$$\n\nThe problem requires trimming the two genes with the largest absolute $M$-values.\nWe have $|M_1| = |M_2| \\approx 1.932$ and $|M_{g \\in \\{3..10\\}}| \\approx 1.389$.\nClearly, genes $1$ and $2$ have the largest absolute $M$-values and are thus trimmed.\n\nThe set of retained genes is $G' = \\{3, 4, 5, 6, 7, 8, 9, 10\\}$, with $|G'| = 8$.\nThe TMM normalization factor for sample $B$, denoted $F_B$, is determined from the weighted mean of the $M$-values of the retained genes. The problem specifies using equal weights, making the calculation a simple arithmetic mean:\n$$\\log_2(F_B) = \\frac{\\sum_{g \\in G'} M_g}{\\sum_{g \\in G'} 1} = \\frac{1}{|G'|} \\sum_{g \\in G'} M_g$$\nSince $M_g$ is constant for all $g \\in G'$, the mean is simply that value:\n$$\\log_2(F_B) = \\log_2\\left(0.4 \\cdot \\frac{N_A}{N_B}\\right)$$\nExponentiating both sides with base $2$ gives the normalization factor:\n$$F_B = 2^{\\log_2\\left(0.4 \\cdot \\frac{N_A}{N_B}\\right)} = 0.4 \\cdot \\frac{N_A}{N_B}$$\nSubstituting the given library sizes:\n$$F_B = 0.4 \\cdot \\frac{1,000,000}{1,048,000} = \\frac{400,000}{1,048,000} \\approx 0.381679389...$$\nRounding to four significant figures, the TMM normalization factor is $0.3817$.\n\n#### Part 2: Failure of Naive Library Size Scaling\n\nThe failure of naive library size scaling under composition bias can be explained from first principles using the multinomial sampling model of RNA sequencing.\n\n1.  **True Molecular Abundance vs. Proportions:** Let $\\mu_{gi}$ be the true (unobserved) number of mRNA molecules of gene $g$ in the biological material for sample $i \\in \\{A, B\\}$. The total number of mRNA molecules in sample $i$ is $T_i = \\sum_{g} \\mu_{gi}$. The true relative abundance, or proportion, of gene $g$ is $\\pi_{gi} = \\mu_{gi} / T_i$.\n\n2.  **Multinomial Sampling:** The sequencing process draws $N_i$ reads (the library size) from this population of molecules. The expected count for gene $g$ in sample $i$ is thus $E[C_{gi}] = N_i \\cdot \\pi_{gi} = N_i \\cdot (\\mu_{gi} / T_i)$.\n\n3.  **Impact of Composition Bias:** The problem states that for genes $g \\in \\{3, ..., 10\\}$, their true abundances do not change, which means $\\mu_{gA} = \\mu_{gB}$ for this majority set of genes. However, for genes $1$ and $2$, there is strong upregulation in sample $B$, meaning $\\mu_{1B} \\gg \\mu_{1A}$ and $\\mu_{2B} \\gg \\mu_{2A}$.\n    This large increase in a few highly abundant transcripts dramatically increases the total mRNA pool size in sample $B$ relative to sample $A$. That is, $T_B = \\sum_g \\mu_{gB} > \\sum_g \\mu_{gA} = T_A$.\n\n4.  **Distortion of Proportions:** For any non-differentially expressed gene $g'$ (where $\\mu_{g'A} = \\mu_{g'B}$), the ratio of its true proportions between the two samples is:\n    $$\\frac{\\pi_{g'B}}{\\pi_{g'A}} = \\frac{\\mu_{g'B}/T_B}{\\mu_{g'A}/T_A} = \\frac{\\mu_{g'A}/T_B}{\\mu_{g'A}/T_A} = \\frac{T_A}{T_B}$$\n    Since $T_B > T_A$, this ratio is less than $1$. This is the essence of composition bias: the massive production of some transcripts dilutes the *proportion* of all other transcripts, even those whose absolute molecular counts are unchanged.\n\n5.  **Failure of Naive Scaling:** Naive library size scaling computes expression ratios as $\\frac{C_{gB}/N_B}{C_{gA}/N_A}$. The underlying assumption is that for a non-differentially expressed gene, this ratio should be $1$. However, based on our model, the expected value of this ratio is:\n    $$\\frac{E[C_{gB}]/N_B}{E[C_{gA}]/N_A} = \\frac{(N_B \\cdot \\pi_{gB})/N_B}{(N_A \\cdot \\pi_{gA})/N_A} = \\frac{\\pi_{gB}}{\\pi_{gA}} = \\frac{T_A}{T_B}$$\n    Because of the composition change, $T_A/T_B < 1$. In this problem, the ratio $T_A/T_B$ is estimated by the TMM factor $F_B \\approx 0.3817$. Thus, naive scaling would incorrectly report that the majority of genes are downregulated by a factor of approximately $0.3817$. This method fails because it incorrectly assumes that the sequenced library size $N_i$ is a direct and sufficient proxy for the effective library size, ignoring the profound impact of compositional changes on the underlying proportions ($\\pi_{gi}$) from which reads are sampled. TMM correctly estimates and compensates for this distortion, which is caused by the change in the total RNA pool size ($T_i$).", "answer": "$$\\boxed{0.3817}$$", "id": "3311834"}, {"introduction": "While many normalization methods correct for library composition, they still yield relative expression values. This exercise explores a more profound challenge: how changes in the total amount of RNA per cell can create artifacts even in seemingly robust metrics like Transcripts Per Million (TPM). You will analytically demonstrate this effect and then use external spike-in controls to perform absolute quantification, revealing the true, unchanged expression of a gene that appears to be downregulated [@problem_id:3311844]. This practice highlights the crucial distinction between relative and absolute changes, a key consideration for interpreting biological effects accurately.", "problem": "A sequencing-based messenger ribonucleic acid (mRNA) expression experiment compares two samples, denoted as $\\mathcal{A}$ and $\\mathcal{B}$, with different total endogenous ribonucleic acid (RNA) content per cell. The experimental workflow adds a known quantity of external spike-in RNA molecules before library preparation to enable absolute normalization. Consider three endogenous genes, $X$, $Y$, and $Z$, with transcript lengths $L_X = 2$ kilobases, $L_Y = 1$ kilobase, and $L_Z = 4$ kilobases, respectively. In sample $\\mathcal{A}$, the absolute per-cell endogenous transcript abundances are $M_X^{\\mathcal{A}} = 10$ molecules per cell, $M_Y^{\\mathcal{A}} = 20$ molecules per cell, and $M_Z^{\\mathcal{A}} = 100$ molecules per cell. In sample $\\mathcal{B}$, they are $M_X^{\\mathcal{B}} = 10$ molecules per cell, $M_Y^{\\mathcal{B}} = 20$ molecules per cell, and $M_Z^{\\mathcal{B}} = 300$ molecules per cell. Thus, the total endogenous RNA per cell is larger in $\\mathcal{B}$ than in $\\mathcal{A}$ due to changes in gene $Z$. An external spike-in control $S$ with transcript length $L_S = 2$ kilobases is added at a known amount of $S = 50$ molecules per cell to both samples before extraction and library preparation.\n\nSequencing yields the following uniquely mapped read counts for each transcript in each sample: in $\\mathcal{A}$, $c_X^{\\mathcal{A}} = 2000$, $c_Y^{\\mathcal{A}} = 2000$, $c_Z^{\\mathcal{A}} = 40000$, and $c_S^{\\mathcal{A}} = 10000$; in $\\mathcal{B}$, $c_X^{\\mathcal{B}} = 2400$, $c_Y^{\\mathcal{B}} = 2400$, $c_Z^{\\mathcal{B}} = 144000$, and $c_S^{\\mathcal{B}} = 12000$. Assume the following foundational model: under uniform fragmentation and unbiased sampling, the expected count $c_i$ for transcript $i$ is proportional to the product of its per-cell molecule count $M_i$ and its length $L_i$, up to a sample-specific proportionality factor that captures sequencing depth and capture efficiency. This model applies equally to the spike-in transcript $S$.\n\nUsing only fundamental definitions and principles appropriate to transcriptomics and expression profiling, do the following:\n\n- Define Transcripts Per Million (TPM) and use it to compute $\\mathrm{TPM}_X^{\\mathcal{A}}$ and $\\mathrm{TPM}_X^{\\mathcal{B}}$, restricting the TPM calculation to the endogenous gene set $\\{X,Y,Z\\}$, and compute the TPM-based fold change for gene $X$, given by $\\frac{\\mathrm{TPM}_X^{\\mathcal{B}}}{\\mathrm{TPM}_X^{\\mathcal{A}}}$.\n\n- Starting from the compositional nature of relative measures that sum to a constant and the provided absolute abundances, explain analytically why reliance on TPM can induce compositional artifacts when total RNA per cell differs between samples.\n\n- Derive a spike-in based normalization equation to estimate absolute molecules per cell for an endogenous transcript $i$ from length-normalized counts relative to the spike-in, and apply it to gene $X$ in samples $\\mathcal{A}$ and $\\mathcal{B}$ to compute the spike-in normalized fold change\n$$\nF_X \\equiv \\frac{\\widehat{M}_X^{\\mathcal{B}}}{\\widehat{M}_X^{\\mathcal{A}}}.\n$$\n\nReport the final numeric answer as the spike-in normalized fold change $F_X$. Round your final answer to four significant figures. No physical units are required for the fold change.", "solution": "The problem statement has been evaluated and is determined to be valid. It is scientifically grounded in the principles of transcriptomic data analysis, is well-posed with a complete and consistent set of givens, and is expressed in objective, formal language. The provided numerical data are consistent with the underlying model, confirming the problem's internal logical consistency. We may therefore proceed with a full solution.\n\nThe problem asks for three distinct tasks: analysis using Transcripts Per Million (TPM), an analytical explanation of compositional artifacts, and analysis using spike-in normalization to find the fold change of gene $X$.\n\n**1. Transcripts Per Million (TPM) Analysis**\n\nFirst, we define Transcripts Per Million (TPM). TPM is a measure of the relative abundance of a transcript. It is calculated by normalizing for gene length first, then normalizing for sequencing depth by scaling the values so that the sum of all TPMs in a sample is $1,000,000$.\n\nFor a given gene $i$ from a set of genes $\\mathcal{G}$, its TPM value is calculated as follows:\n1.  For each gene $j \\in \\mathcal{G}$, normalize its read count $c_j$ by its length $L_j$ (in kilobases) to get a rate $r_j = \\frac{c_j}{L_j}$.\n2.  Calculate the sum of all such rates in the sample: $T = \\sum_{j \\in \\mathcal{G}} r_j = \\sum_{j \\in \\mathcal{G}} \\frac{c_j}{L_j}$.\n3.  The TPM for gene $i$ is then given by:\n$$\n\\mathrm{TPM}_i = \\left(\\frac{r_i}{T}\\right) \\times 10^6 = \\left(\\frac{c_i/L_i}{\\sum_{j \\in \\mathcal{G}} c_j/L_j}\\right) \\times 10^6\n$$\nThe problem specifies that the TPM calculation should be restricted to the endogenous gene set $\\{X, Y, Z\\}$.\n\n**For sample $\\mathcal{A}$:**\nThe given values are $L_X = 2$, $L_Y = 1$, $L_Z = 4$ (in kb), and counts $c_X^{\\mathcal{A}} = 2000$, $c_Y^{\\mathcal{A}} = 2000$, $c_Z^{\\mathcal{A}} = 40000$.\nThe rates are:\n$r_X^{\\mathcal{A}} = \\frac{c_X^{\\mathcal{A}}}{L_X} = \\frac{2000}{2} = 1000$\n$r_Y^{\\mathcal{A}} = \\frac{c_Y^{\\mathcal{A}}}{L_Y} = \\frac{2000}{1} = 2000$\n$r_Z^{\\mathcal{A}} = \\frac{c_Z^{\\mathcal{A}}}{L_Z} = \\frac{40000}{4} = 10000$\n\nThe sum of rates is $T^{\\mathcal{A}} = r_X^{\\mathcal{A}} + r_Y^{\\mathcal{A}} + r_Z^{\\mathcal{A}} = 1000 + 2000 + 10000 = 13000$.\nThe TPM for gene $X$ in sample $\\mathcal{A}$ is:\n$\\mathrm{TPM}_X^{\\mathcal{A}} = \\left(\\frac{1000}{13000}\\right) \\times 10^6 = \\frac{1}{13} \\times 10^6$.\n\n**For sample $\\mathcal{B}$:**\nThe given values are $L_X = 2$, $L_Y = 1$, $L_Z = 4$ (in kb), and counts $c_X^{\\mathcal{B}} = 2400$, $c_Y^{\\mathcal{B}} = 2400$, $c_Z^{\\mathcal{B}} = 144000$.\nThe rates are:\n$r_X^{\\mathcal{B}} = \\frac{c_X^{\\mathcal{B}}}{L_X} = \\frac{2400}{2} = 1200$\n$r_Y^{\\mathcal{B}} = \\frac{c_Y^{\\mathcal{B}}}{L_Y} = \\frac{2400}{1} = 2400$\n$r_Z^{\\mathcal{B}} = \\frac{c_Z^{\\mathcal{B}}}{L_Z} = \\frac{144000}{4} = 36000$\n\nThe sum of rates is $T^{\\mathcal{B}} = r_X^{\\mathcal{B}} + r_Y^{\\mathcal{B}} + r_Z^{\\mathcal{B}} = 1200 + 2400 + 36000 = 39600$.\nThe TPM for gene $X$ in sample $\\mathcal{B}$ is:\n$\\mathrm{TPM}_X^{\\mathcal{B}} = \\left(\\frac{1200}{39600}\\right) \\times 10^6 = \\frac{12}{396} \\times 10^6 = \\frac{1}{33} \\times 10^6$.\n\n**TPM-based fold change for gene $X$:**\nThe fold change is the ratio $\\frac{\\mathrm{TPM}_X^{\\mathcal{B}}}{\\mathrm{TPM}_X^{\\mathcal{A}}}$.\n$$\n\\frac{\\mathrm{TPM}_X^{\\mathcal{B}}}{\\mathrm{TPM}_X^{\\mathcal{A}}} = \\frac{\\frac{1}{33} \\times 10^6}{\\frac{1}{13} \\times 10^6} = \\frac{13}{33} \\approx 0.3939\n$$\nThis result suggests that gene $X$ is strongly downregulated in sample $\\mathcal{B}$ compared to sample $\\mathcal{A}$.\n\n**2. Analytical Explanation of TPM-induced Compositional Artifacts**\n\nThe foundational model states that the expected count $c_i$ for a transcript $i$ is proportional to the product of its molecule count $M_i$ and its length $L_i$. Let $k$ be the sample-specific proportionality factor (related to sequencing depth).\n$c_i = k \\cdot M_i \\cdot L_i$.\n\nSubstituting this into the TPM formula (ignoring the $10^6$ scaling factor which cancels in ratios):\n$$\n\\mathrm{TPM}_i \\propto \\frac{c_i/L_i}{\\sum_{j \\in \\mathcal{G}} c_j/L_j} = \\frac{(k \\cdot M_i \\cdot L_i)/L_i}{\\sum_{j \\in \\mathcal{G}} (k \\cdot M_j \\cdot L_j)/L_j} = \\frac{k \\cdot M_i}{k \\sum_{j \\in \\mathcal{G}} M_j} = \\frac{M_i}{\\sum_{j \\in \\mathcal{G}} M_j}\n$$\nThis shows that TPM is a measure of the relative molar abundance of a transcript $i$ with respect to the total molar amount of all transcripts in the considered set $\\mathcal{G}$, denoted $M_{\\mathrm{total}} = \\sum_{j \\in \\mathcal{G}} M_j$.\n\nThe TPM-based fold change between samples $\\mathcal{B}$ and $\\mathcal{A}$ is:\n$$\n\\frac{\\mathrm{TPM}_i^{\\mathcal{B}}}{\\mathrm{TPM}_i^{\\mathcal{A}}} = \\frac{M_i^{\\mathcal{B}}/M_{\\mathrm{total}}^{\\mathcal{B}}}{M_i^{\\mathcal{A}}/M_{\\mathrm{total}}^{\\mathcal{A}}} = \\left(\\frac{M_i^{\\mathcal{B}}}{M_i^{\\mathcal{A}}}\\right) \\cdot \\left(\\frac{M_{\\mathrm{total}}^{\\mathcal{A}}}{M_{\\mathrm{total}}^{\\mathcal{B}}}\\right)\n$$\nThe true absolute fold change is $\\frac{M_i^{\\mathcal{B}}}{M_i^{\\mathcal{A}}}$. The TPM-based fold change is distorted by the factor $\\frac{M_{\\mathrm{total}}^{\\mathcal{A}}}{M_{\\mathrm{total}}^{\\mathcal{B}}}$, the inverse ratio of the total endogenous transcript molecules per cell in the two samples. This is a compositional artifact. Because TPM is a relative measure, a large change in the abundance of one highly expressed gene (like gene $Z$) alters the denominator $M_{\\mathrm{total}}$, thereby changing the apparent abundance of all other genes, even if their absolute abundance is constant.\n\nIn this problem:\n$M_{\\mathrm{total}}^{\\mathcal{A}} = M_X^{\\mathcal{A}} + M_Y^{\\mathcal{A}} + M_Z^{\\mathcal{A}} = 10 + 20 + 100 = 130$ molecules/cell.\n$M_{\\mathrm{total}}^{\\mathcal{B}} = M_X^{\\mathcal{B}} + M_Y^{\\mathcal{B}} + M_Z^{\\mathcal{B}} = 10 + 20 + 300 = 330$ molecules/cell.\nThe true fold change for gene $X$ is $\\frac{M_X^{\\mathcal{B}}}{M_X^{\\mathcal{A}}} = \\frac{10}{10} = 1$.\nThe TPM-based fold change is predicted to be $1 \\times \\frac{130}{330} = \\frac{13}{33}$, which exactly matches our calculation. The massive increase in $M_Z$ in sample $\\mathcal{B}$ makes the constant-abundance gene $X$ appear downregulated.\n\n**3. Spike-in Based Normalization**\n\nThe spike-in control provides a stable reference against which absolute abundances can be estimated. The same model $c_i = k \\cdot M_i \\cdot L_i$ applies to the spike-in transcript $S$. For a generic sample:\n$c_i = k \\cdot M_i \\cdot L_i$\n$c_S = k \\cdot M_S \\cdot L_S$\nHere, $M_S$ is the known number of spike-in molecules added, which is given as $S=50$ molecules per cell.\n\nWe can solve for the unknown depth factor $k$ using the spike-in equation: $k = \\frac{c_S}{S \\cdot L_S}$.\nSubstituting this into the equation for the endogenous gene $i$:\n$c_i = \\left(\\frac{c_S}{S \\cdot L_S}\\right) \\cdot M_i \\cdot L_i$\n\nWe can now solve for $M_i$ to get its estimated absolute abundance, $\\widehat{M}_i$:\n$$\n\\widehat{M}_i = c_i \\cdot \\frac{S \\cdot L_S}{c_S \\cdot L_i} = S \\cdot \\frac{c_i/L_i}{c_S/L_S}\n$$\nThis equation shows that the absolute abundance of gene $i$ is estimated by scaling the known spike-in abundance $S$ by the ratio of the length-normalized counts of gene $i$ and the spike-in $S$.\n\nNow we apply this to gene $X$ in both samples.\nThe relevant values are: $S=50$, $L_X=2$, $L_S=2$.\n\n**For sample $\\mathcal{A}$:**\n$c_X^{\\mathcal{A}} = 2000$, $c_S^{\\mathcal{A}} = 10000$.\n$$\n\\widehat{M}_X^{\\mathcal{A}} = 50 \\cdot \\frac{2000/2}{10000/2} = 50 \\cdot \\frac{1000}{5000} = 50 \\cdot \\frac{1}{5} = 10\n$$\nThis correctly recovers the true absolute abundance $M_X^{\\mathcal{A}} = 10$ molecules per cell.\n\n**For sample $\\mathcal{B}$:**\n$c_X^{\\mathcal{B}} = 2400$, $c_S^{\\mathcal{B}} = 12000$.\n$$\n\\widehat{M}_X^{\\mathcal{B}} = 50 \\cdot \\frac{2400/2}{12000/2} = 50 \\cdot \\frac{1200}{6000} = 50 \\cdot \\frac{1}{5} = 10\n$$\nThis also correctly recovers the true absolute abundance $M_X^{\\mathcal{B}} = 10$ molecules per cell.\n\n**Spike-in normalized fold change for gene $X$:**\nThe requested fold change is $F_X = \\frac{\\widehat{M}_X^{\\mathcal{B}}}{\\widehat{M}_X^{\\mathcal{A}}}$.\n$$\nF_X = \\frac{10}{10} = 1\n$$\nThe spike-in normalization correctly determines that the absolute abundance of gene $X$ has not changed between the two samples, yielding a fold-change of $1$. This successfully corrects the compositional artifact observed with TPM.\n\nThe problem asks for the final answer rounded to four significant figures. The exact answer is $1$. To express this with the requested precision, we write $1.000$.", "answer": "$$\\boxed{1.000}$$", "id": "3311844"}, {"introduction": "Single-cell transcriptomics has revolutionized biology, but it introduces unique data characteristics, most notably a high proportion of zero counts. A key debate is whether these zeros reflect true biological absence or technical artifacts, leading to the use of complex zero-inflated models. This exercise challenges you to move from assumption to evidence by deriving the expected zero frequency from a fundamental Poisson-Gamma mixture model and comparing it to observed data [@problem_id:3311792]. This hands-on statistical test will empower you to make an informed, data-driven decision about the appropriate underlying model for UMI-based single-cell counts.", "problem": "You are given count profiles for a single gene measured across multiple cells using Unique Molecular Identifier (UMI) sequencing in single-cell transcriptomics. You must decide, from first principles, whether a zero-inflated model is warranted for these UMI counts or whether a Negative Binomial (NB) mixture capturing cell-to-cell heterogeneity is sufficient. The decision must be made by comparing the observed zero count frequency to the zero frequency predicted by an NB mixture derived from a mechanistic sampling model.\n\nFoundational base to use:\n- The Central Dogma of molecular biology describes the flow of information from DNA to RNA to protein, and messenger ribonucleic acid (mRNA) molecules are sampled and counted in expression profiling.\n- In UMI sequencing, quantification is based on counting molecules captured and deduplicated, which can be modeled as a sampling process.\n- A typical assumption for UMI counts is that, conditional on a per-cell capture efficiency, the number of captured molecules for a given gene in a cell is a realization of a Poisson process.\n- Cell-to-cell heterogeneity in true underlying expression can be modeled by a Gamma distribution across cells, leading to a Poisson-Gamma mixture for the counts, which is a Negative Binomial (NB) distribution.\n- A mixture of cellular subpopulations, each with its own NB parameters, produces an NB mixture across the entire population.\n- A Zero-Inflated Negative Binomial (ZINB) model adds an additional point mass at zero beyond what the NB (or NB mixture) predicts.\n\nYour derivation must start from the Poisson sampling assumption and Gamma heterogeneity and logically deduce the NB distribution, the probability of a zero count under NB, and then aggregate to an NB mixture across cellular subpopulations and variable library sizes. You may not use shortcut formulas in this problem statement; derive them in your solution.\n\nDecision rule to implement:\n- Consider $n$ cells indexed by $i \\in \\{1,\\dots,n\\}$.\n- Let $s_i$ denote the size factor (library size scaling) for cell $i$, treated as known.\n- Let there be $m$ latent subpopulations indexed by $j \\in \\{1,\\dots,m\\}$ with mixture weights $w_j$ (nonnegative and summing to $1$), and per-subpopulation NB parameters given by a mean parameter $\\mu_j$ and an inverse-dispersion parameter $k_j$.\n- Under the NB mixture model, compute the predicted probability of zero for each cell $i$ by integrating over the mixture components and then average across cells to obtain a global predicted zero fraction $p_0$.\n- Let $Z$ be the random variable denoting the number of zero counts across $n$ cells. Under the NB mixture model, treat $Z$ as a Binomial random variable with parameters $(n, p_0)$ as a first-principles approximation for the number of zeros.\n- Given the observed number of zeros $z_{\\text{obs}}$, compute the right-tail Binomial probability $\\mathbb{P}(Z \\ge z_{\\text{obs}})$ and compare to a user-specified significance threshold $\\alpha$.\n- If $\\mathbb{P}(Z \\ge z_{\\text{obs}}) < \\alpha$, conclude that the observed zeros are in excess of the NB mixture prediction and that a zero-inflated model is warranted; otherwise, conclude that a zero-inflated model is not warranted.\n\nYour program must implement this decision rule directly based on the derivation and produce a boolean decision for each test case.\n\nTest suite:\nFor each test case, the program must construct the counts array of length $n = 100$ by concatenating a specified number of zeros followed by a periodic pattern of positive integers until the array reaches length $100$. All numbers in the arrays are counts and are dimensionless integers. For each case, the mixture parameters $(w_j, \\mu_j, k_j)$, the size factors $\\{s_i\\}$, the significance level $\\alpha$, and the construction of the counts are specified below.\n\n- Case $1$ (happy path: NB mixture sufficient):\n  - $n = 100$\n  - Mixture weights: $w = [\\,0.4,\\,0.6\\,]$\n  - Means: $\\mu = [\\,1.5,\\,5.0\\,]$\n  - Inverse-dispersions: $k = [\\,2.0,\\,2.0\\,]$\n  - Size factors: $s_i = 1$ for all $i$ (i.e., $\\{s_i\\} = [\\,1, 1, \\dots, 1\\,]$ of length $100$)\n  - Counts construction: Place $18$ zeros, then repeat the positive pattern $[\\,1,\\,2,\\,1,\\,3,\\,2,\\,4,\\,1,\\,5,\\,2,\\,3\\,]$ eight times and append $[\\,4,\\,2\\,]$ to reach length $100$.\n  - Significance: $\\alpha = 0.01$\n\n- Case $2$ (excess zeros: zero-inflation warranted):\n  - $n = 100$\n  - Mixture weights: $w = [\\,0.4,\\,0.6\\,]$\n  - Means: $\\mu = [\\,1.5,\\,5.0\\,]$\n  - Inverse-dispersions: $k = [\\,2.0,\\,2.0\\,]$\n  - Size factors: $s_i = 1$ for all $i$\n  - Counts construction: Place $40$ zeros, then repeat the positive pattern $[\\,1,\\,2,\\,3,\\,2,\\,4,\\,3,\\,5,\\,2,\\,4,\\,3\\,]$ six times to reach length $100$.\n  - Significance: $\\alpha = 0.01$\n\n- Case $3$ (boundary low expression: many zeros expected under NB):\n  - $n = 100$\n  - Mixture weights: $w = [\\,1.0\\,]$\n  - Means: $\\mu = [\\,0.2\\,]$\n  - Inverse-dispersions: $k = [\\,5.0\\,]$\n  - Size factors: $s_i = 1$ for all $i$\n  - Counts construction: Place $82$ zeros, then repeat the positive pattern $[\\,1,\\,1,\\,2,\\,1,\\,1,\\,2,\\,1,\\,2,\\,1\\,]$ two times to reach length $100$.\n  - Significance: $\\alpha = 0.01$\n\n- Case $4$ (heterogeneous library sizes: NB mixture with variable $s_i$):\n  - $n = 100$\n  - Mixture weights: $w = [\\,0.5,\\,0.5\\,]$\n  - Means: $\\mu = [\\,2.0,\\,8.0\\,]$\n  - Inverse-dispersions: $k = [\\,1.2,\\,3.0\\,]$\n  - Size factors: first $50$ cells with $s_i = 1$, next $50$ cells with $s_i = 0.5$, i.e., $\\{s_i\\} = [\\,\\underbrace{1, \\dots, 1}_{50}, \\underbrace{0.5, \\dots, 0.5}_{50}\\,]$\n  - Counts construction: Place $22$ zeros, then repeat the positive pattern $[\\,1,\\,2,\\,3,\\,4,\\,5,\\,3,\\,4,\\,2,\\,6,\\,3,\\,2,\\,5,\\,4,\\,3,\\,7\\,]$ five times and append $[\\,4,\\,3,\\,2\\,]$ to reach length $100$.\n  - Significance: $\\alpha = 0.01$\n\nRequired output format:\n- Your program should produce a single line of output containing the boolean decisions for the four cases as a comma-separated list enclosed in square brackets, for example: $[\\,\\text{False},\\text{True},\\text{False},\\text{False}\\,]$.", "solution": "The problem requires a determination of whether a zero-inflated model is necessary for single-cell UMI counts, based on a first-principles derivation and a statistical test. The decision hinges on comparing the observed frequency of zero counts to the frequency predicted by a Negative Binomial (NB) mixture model.\n\nThe derivation begins with the fundamental process of UMI sequencing. For a given gene in a single cell $i$, let the true (unobserved) relative abundance of its mRNA be represented by a rate parameter $\\lambda_i$. The process of capturing and sequencing these molecules is a discrete sampling process. Assuming sufficient molecules are present, the number of unique molecules (UMIs) counted, $c_i$, can be modeled as a Poisson random variable conditional on this rate:\n$$c_i | \\lambda_i \\sim \\text{Poisson}(\\lambda_i)$$\nThe probability mass function (PMF) is given by:\n$$P(c_i=y | \\lambda_i) = \\frac{e^{-\\lambda_i} \\lambda_i^y}{y!} \\quad \\text{for } y \\in \\{0, 1, 2, \\dots\\}$$\n\nCell-to-cell variability in gene expression is a hallmark of single-cell data. This biological heterogeneity means that the rate parameter $\\lambda_i$ is not constant across a population of cells but is itself a random variable. A common and mathematically convenient choice for modeling this variability is the Gamma distribution. Let the distribution of $\\lambda$ across a homogeneous subpopulation of cells be:\n$$\\lambda \\sim \\text{Gamma}(\\text{shape}=k, \\text{rate}=\\beta)$$\nThe probability density function (PDF) is:\n$$f(\\lambda; k, \\beta) = \\frac{\\beta^k}{\\Gamma(k)} \\lambda^{k-1} e^{-\\beta \\lambda}$$\nHere, $\\Gamma(k)$ is the Gamma function.\n\nThe marginal distribution of the counts $c$ across the cell population is obtained by integrating the conditional Poisson probability over the distribution of $\\lambda$:\n$$P(c=y) = \\int_0^\\infty P(c=y | \\lambda) f(\\lambda; k, \\beta) \\,d\\lambda$$\n$$P(c=y) = \\int_0^\\infty \\left( \\frac{e^{-\\lambda} \\lambda^y}{y!} \\right) \\left( \\frac{\\beta^k}{\\Gamma(k)} \\lambda^{k-1} e^{-\\beta \\lambda} \\right) \\,d\\lambda$$\nRearranging terms, we get:\n$$P(c=y) = \\frac{\\beta^k}{y! \\Gamma(k)} \\int_0^\\infty \\lambda^{y+k-1} e^{-(\\beta+1)\\lambda} \\,d\\lambda$$\nThe integral is the kernel of a Gamma distribution with shape $y+k$ and rate $\\beta+1$. The value of the integral is $\\frac{\\Gamma(y+k)}{(\\beta+1)^{y+k}}$. Substituting this back gives:\n$$P(c=y) = \\frac{\\beta^k}{y! \\Gamma(k)} \\frac{\\Gamma(y+k)}{(\\beta+1)^{y+k}} = \\frac{\\Gamma(y+k)}{y! \\Gamma(k)} \\left(\\frac{\\beta}{\\beta+1}\\right)^k \\left(\\frac{1}{\\beta+1}\\right)^y$$\nThis is the PMF of a Negative Binomial (NB) distribution. This demonstrates that a Poisson-Gamma mixture is equivalent to an NB distribution.\n\nThe problem specifies the NB distribution using a mean parameter $\\mu$ and an inverse-dispersion parameter $k$. We must relate these to the Gamma parameters. The mean of the NB distribution is the mean of the underlying Gamma distribution:\n$$E[c] = \\mu = E[\\lambda] = \\frac{k}{\\beta} \\implies \\beta = \\frac{k}{\\mu}$$\nThe variance of the NB distribution is $\\text{Var}(c) = \\mu + \\frac{\\mu^2}{k}$. This parameterization identifies the Gamma shape parameter as the inverse-dispersion parameter $k$.\n\nWe can now derive the probability of observing a zero count ($y=0$) under this NB model. Using the derived PMF:\n$$P(c=0) = \\frac{\\Gamma(0+k)}{0! \\Gamma(k)} \\left(\\frac{\\beta}{\\beta+1}\\right)^k \\left(\\frac{1}{\\beta+1}\\right)^0 = \\left(\\frac{\\beta}{\\beta+1}\\right)^k$$\nSubstituting $\\beta = k/\\mu$:\n$$P(c=0) = \\left(\\frac{k/\\mu}{k/\\mu + 1}\\right)^k = \\left(\\frac{k/\\mu}{(k+\\mu)/\\mu}\\right)^k = \\left(\\frac{k}{k+\\mu}\\right)^k$$\n\nSingle-cell experiments often have varying library sizes (sequencing depths) per cell. This is accounted for by a cell-specific size factor $s_i$, which scales the mean. The effective mean for cell $i$ is $s_i \\mu$. The probability of a zero count for cell $i$, assuming it comes from a population with parameters $(\\mu, k)$, is therefore:\n$$P(c_i=0 | \\mu, k, s_i) = \\left(\\frac{k}{k+s_i\\mu}\\right)^k$$\n\nThe total cell population may be a mixture of $m$ distinct subpopulations, each with its own parameters $(\\mu_j, k_j)$ and proportion $w_j$ in the total population (where $\\sum_{j=1}^m w_j = 1$). The predicted probability of a zero count for a randomly selected cell $i$ is the weighted average over all possible subpopulations it could belong to:\n$$p_{0,i} = P(c_i=0) = \\sum_{j=1}^m w_j P(c_i=0 | \\mu_j, k_j, s_i) = \\sum_{j=1}^m w_j \\left(\\frac{k_j}{k_j+s_i\\mu_j}\\right)^{k_j}$$\n\nTo get a single predicted zero frequency for the entire dataset of $n$ cells, we average the individual probabilities:\n$$p_0 = \\frac{1}{n} \\sum_{i=1}^n p_{0,i}$$\n\nFinally, the decision rule is based on a statistical test. The null hypothesis is that the observed number of zeros, $z_{\\text{obs}}$, is consistent with the NB mixture model. We model the total number of zero counts, $Z$, across the $n$ cells as a Binomial random variable with the average success (zero count) probability $p_0$:\n$$Z \\sim \\text{Binomial}(n, p_0)$$\nThis is an approximation where the differing per-cell probabilities $p_{0,i}$ are replaced by their average. We compute the p-value by calculating the probability of observing at least $z_{\\text{obs}}$ zeros under this null model:\n$$\\mathbb{P}(Z \\ge z_{\\text{obs}}) = \\sum_{y=z_{\\text{obs}}}^n \\binom{n}{y} p_0^y (1-p_0)^{n-y}$$\nIf this right-tail probability is less than a specified significance level $\\alpha$, i.e., $\\mathbb{P}(Z \\ge z_{\\text{obs}}) < \\alpha$, we reject the null hypothesis. This implies the observed number of zeros is significantly greater than what the NB mixture model predicts, warranting the use of a zero-inflated model. Otherwise, the NB mixture model is considered sufficient.\n\nThis step-by-step derivation from the Poisson sampling process to the final statistical test provides the logical foundation for the implemented program.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import binom\n\ndef solve():\n    \"\"\"\n    Validates if a zero-inflated model is warranted for UMI counts by comparing observed\n    zero counts to those predicted by a Negative Binomial (NB) mixture model.\n    \"\"\"\n\n    # Test suite definition as specified in the problem statement.\n    test_cases = [\n        {\n            \"n\": 100, \"z_obs\": 18, \"alpha\": 0.01,\n            \"mixture_weights\": np.array([0.4, 0.6]),\n            \"means\": np.array([1.5, 5.0]),\n            \"inv_dispersions\": np.array([2.0, 2.0]),\n            \"size_factors\": np.ones(100)\n        },\n        {\n            \"n\": 100, \"z_obs\": 40, \"alpha\": 0.01,\n            \"mixture_weights\": np.array([0.4, 0.6]),\n            \"means\": np.array([1.5, 5.0]),\n            \"inv_dispersions\": np.array([2.0, 2.0]),\n            \"size_factors\": np.ones(100)\n        },\n        {\n            \"n\": 100, \"z_obs\": 82, \"alpha\": 0.01,\n            \"mixture_weights\": np.array([1.0]),\n            \"means\": np.array([0.2]),\n            \"inv_dispersions\": np.array([5.0]),\n            \"size_factors\": np.ones(100)\n        },\n        {\n            \"n\": 100, \"z_obs\": 22, \"alpha\": 0.01,\n            \"mixture_weights\": np.array([0.5, 0.5]),\n            \"means\": np.array([2.0, 8.0]),\n            \"inv_dispersions\": np.array([1.2, 3.0]),\n            \"size_factors\": np.concatenate([np.ones(50), np.full(50, 0.5)])\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        n = case[\"n\"]\n        z_obs = case[\"z_obs\"]\n        alpha = case[\"alpha\"]\n        w = case[\"mixture_weights\"]\n        mu = case[\"means\"]\n        k = case[\"inv_dispersions\"]\n        s = case[\"size_factors\"]\n\n        # Vectorized calculation of predicted zero probabilities\n        # s has shape (n,), mu and k have shape (m,)\n        # Broadcasting s[:, np.newaxis] with mu[np.newaxis, :] creates an (n, m) matrix\n        # of scaled means for each cell and each mixture component.\n        scaled_means = s[:, np.newaxis] * mu[np.newaxis, :]\n\n        # Calculate the probability of zero for each cell under each mixture component.\n        # This results in an (n, m) matrix.\n        # The formula derived is P(c=0) = (k / (k + s*mu))^k\n        component_zero_probs = (k / (k + scaled_means)) ** k\n\n        # Calculate the per-cell zero probability by taking the weighted average\n        # over the mixture components. This results in a vector of shape (n,).\n        per_cell_zero_probs = np.sum(w * component_zero_probs, axis=1)\n\n        # Calculate the global average zero probability p0, as specified.\n        p_zero_predicted = np.mean(per_cell_zero_probs)\n\n        # Calculate the right-tail probability (p-value) under the Binomial model.\n        # P(Z >= z_obs) where Z ~ Binomial(n, p_zero_predicted)\n        # Using survival function sf(k) = P(X > k), so sf(k-1) = P(X >= k)\n        p_value = binom.sf(z_obs - 1, n, p_zero_predicted)\n        \n        # Make the decision based on the significance level alpha.\n        # True means zero-inflation is warranted.\n        decision = p_value < alpha\n        results.append(decision)\n\n    # Format the final output as a comma-separated list of booleans in brackets.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3311792"}]}