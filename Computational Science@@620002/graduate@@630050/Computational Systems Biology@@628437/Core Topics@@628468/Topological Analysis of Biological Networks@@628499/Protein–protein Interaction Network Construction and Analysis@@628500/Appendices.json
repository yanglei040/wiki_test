{"hands_on_practices": [{"introduction": "A reliable protein-protein interaction (PPI) network is built upon evidence from diverse and often conflicting sources. This practice addresses the foundational challenge of data integration by guiding you through the implementation of a principled Bayesian framework. By harmonizing heterogeneous evidence types and confidence scores, you will learn to compute a robust posterior probability for each potential interaction, forming a solid statistical basis for network construction. [@problem_id:3341723]", "problem": "You are given multiple heterogeneous sources of evidence about candidate protein–protein interactions. Your task is to design and implement a program that harmonizes conflicting interactions across databases by reconciling evidence types and confidences, and then computes consensus edges with uncertainty intervals. Work within a mathematically principled framework grounded in Bayes’ theorem and conjugate priors for Bernoulli observations, and express all probabilities as decimals.\n\nFundamental base and core definitions to use:\n- Bayes’ theorem: for a hypothesis $\\mathcal{H}$ and data $\\mathcal{D}$, the posterior probability satisfies $P(\\mathcal{H}\\mid \\mathcal{D}) \\propto P(\\mathcal{D}\\mid \\mathcal{H}) P(\\mathcal{H})$.\n- A Bernoulli random variable $Y$ with parameter $p$ has likelihood $P(Y=y\\mid p) = p^y (1-p)^{1-y}$ for $y\\in\\{0,1\\}$.\n- The Beta distribution $\\mathrm{Beta}(\\alpha,\\beta)$ is a conjugate prior for the Bernoulli likelihood.\n\nScenario and harmonization design:\n- Consider candidate edges between pairs of proteins where each edge is evaluated by a set of databases. Each database emits one piece of evidence with:\n  1. An evidence type $t$ from the set $\\{ \\text{Yeast Two-Hybrid (Y2H)}, \\text{Affinity Purification–Mass Spectrometry (AP-MS)}, \\text{Co-expression (Coexp)}, \\text{Literature Curation (Curated)} \\}$.\n  2. A continuous confidence score $r \\in [0,1]$.\n  3. A verdict $v \\in \\{ \\text{support}, \\text{refute} \\}$ for the interaction.\n  4. A database identity $d$ from the set $\\{ \\text{BioGRID}, \\text{IntAct}, \\text{STRING}, \\text{Literature} \\}$.\n- Harmonize heterogeneous evidence into a unified Bayesian update by:\n  1. Mapping each confidence $r$ to a nonnegative pseudo-count weight $w$ that monotonically increases with $r$ via a calibrated odds transformation and bounded influence:\n     $$ w = \\min\\left( W_{\\max},\\ \\lambda_{t}\\ \\eta_{d}\\ \\frac{r}{1-r} \\right), $$\n     where $\\lambda_{t}$ is an evidence-type reliability multiplier and $\\eta_{d}$ is a database-level calibration factor. Use $W_{\\max} = 10$, $\\lambda_{\\text{Y2H}}=1$, $\\lambda_{\\text{AP-MS}}=0.8$, $\\lambda_{\\text{Coexp}}=0.5$, $\\lambda_{\\text{Curated}}=1.2$, and $\\eta_{\\text{BioGRID}}=1$, $\\eta_{\\text{IntAct}}=1$, $\\eta_{\\text{STRING}}=0.9$, $\\eta_{\\text{Literature}}=1.1$.\n  2. Treat supportive evidence as a fractional Bernoulli “success” and refuting evidence as a fractional “failure,” combining them under a symmetric, non-informative prior on the edge existence probability $p$. Use a Beta prior with parameters $\\alpha_0=\\beta_0=1$.\n\nTarget quantities to compute for each edge:\n- The posterior distribution over the edge existence probability $p$ after harmonizing all evidence for the edge.\n- The posterior mean of $p$.\n- The central credible interval of mass $0.95$ for $p$ (i.e., the $0.025$ and $0.975$ posterior quantiles).\n- A consensus decision for the edge, defined as a boolean that is true if and only if the lower bound of the $0.95$ credible interval is at least $0.5$.\n\nYour program must implement the above harmonization and Bayesian updating from first principles without using shortcut formulas provided in this problem statement. All probabilities must be expressed as decimals, not percentages. Angles are not involved in this problem. There are no physical units.\n\nTest suite:\nImplement your program to evaluate the following $5$ edges, each described by a list of evidence items of the form $(d, t, v, r)$:\n\n- Edge $A$–$B$ (general “happy path” case with predominantly supportive high-confidence evidence and one weak refutation):\n  - $(\\text{BioGRID}, \\text{Y2H}, \\text{support}, r=0.95)$\n  - $(\\text{IntAct}, \\text{AP-MS}, \\text{support}, r=0.85)$\n  - $(\\text{Literature}, \\text{Curated}, \\text{support}, r=0.90)$\n  - $(\\text{STRING}, \\text{Coexp}, \\text{refute}, r=0.20)$\n- Edge $C$–$D$ (conflicting evidence with roughly balanced support and refutation):\n  - $(\\text{STRING}, \\text{Coexp}, \\text{support}, r=0.60)$\n  - $(\\text{BioGRID}, \\text{Y2H}, \\text{refute}, r=0.70)$\n  - $(\\text{IntAct}, \\text{AP-MS}, \\text{support}, r=0.70)$\n  - $(\\text{Literature}, \\text{Curated}, \\text{refute}, r=0.65)$\n- Edge $E$–$F$ (boundary case with exclusively strong refutations):\n  - $(\\text{STRING}, \\text{Coexp}, \\text{refute}, r=0.95)$\n  - $(\\text{IntAct}, \\text{AP-MS}, \\text{refute}, r=0.90)$\n  - $(\\text{Literature}, \\text{Curated}, \\text{refute}, r=0.80)$\n- Edge $G$–$H$ (edge case testing upper weight cap and near-zero confidence):\n  - $(\\text{BioGRID}, \\text{Y2H}, \\text{support}, r=0.99)$\n  - $(\\text{STRING}, \\text{Coexp}, \\text{support}, r=0.51)$\n  - $(\\text{IntAct}, \\text{AP-MS}, \\text{refute}, r=0.05)$\n- Edge $I$–$J$ (mixed moderate evidence with different reliability multipliers):\n  - $(\\text{Literature}, \\text{Curated}, \\text{support}, r=0.70)$\n  - $(\\text{BioGRID}, \\text{Y2H}, \\text{refute}, r=0.60)$\n\nOutput specification:\n- For each edge, output a list containing the posterior mean of $p$, the lower bound and upper bound of the $0.95$ credible interval for $p$, and the consensus decision boolean as defined above. Round all floating-point numbers to $6$ decimal places.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each edge’s result itself being a list of the form $[p_{\\text{mean}}, p_{\\text{lower}}, p_{\\text{upper}}, \\text{decision}]$. For example, the final output should look like $[[0.123456,0.012345,0.234567,\\text{True}],[\\dots],\\dots]$.", "solution": "The problem requires the design of a Bayesian framework to synthesize heterogeneous evidence for protein-protein interactions and to compute consensus decisions with associated uncertainty. The problem is well-posed, scientifically grounded in standard statistical principles, and provides all necessary data and definitions for a unique solution. We can therefore proceed with a formal derivation and implementation.\n\nThe core of the problem is to determine the posterior probability distribution for an edge's existence, denoted by the parameter $p \\in [0, 1]$, where $p=1$ signifies a true interaction and $p=0$ signifies no interaction. We model $p$ as a random variable and use Bayes' theorem to update our belief about $p$ as we incorporate evidence.\n\n**1. Bayesian Model Formulation**\n\nThe relationship between the prior, posterior, likelihood, and evidence is given by Bayes' theorem:\n$$ P(p \\mid \\mathcal{D}) \\propto P(\\mathcal{D} \\mid p) P(p) $$\nHere, $P(p)$ is the prior distribution of the interaction probability $p$, representing our belief before considering any specific evidence $\\mathcal{D}$. $P(\\mathcal{D} \\mid p)$ is the likelihood, which quantifies how probable the observed evidence $\\mathcal{D}$ is for a given value of $p$. $P(p \\mid \\mathcal{D})$ is the posterior distribution, representing our updated belief about $p$ after observing $\\mathcal{D}$.\n\n**2. Prior Distribution**\n\nWe are instructed to use a symmetric, non-informative prior on $p$. The Beta distribution, $\\mathrm{Beta}(\\alpha, \\beta)$, is the conjugate prior for the Bernoulli likelihood, which simplifies the Bayesian update. A non-informative choice for the Beta prior is the case where the parameters are $\\alpha_0=1$ and $\\beta_0=1$. This corresponds to a Uniform distribution on the interval $[0, 1]$, meaning all values of $p$ are considered equally likely beforehand. The prior probability density function (PDF) is:\n$$ P(p) = \\frac{p^{\\alpha_0-1}(1-p)^{\\beta_0-1}}{B(\\alpha_0, \\beta_0)} = \\frac{p^{1-1}(1-p)^{1-1}}{B(1, 1)} = 1 \\quad \\text{for } p \\in [0, 1] $$\nwhere $B(\\alpha, \\beta)$ is the Beta function.\n\n**3. Evidence Harmonization and Likelihood Function**\n\nThe evidence for each edge comes from multiple sources, each with a type $t$, confidence score $r$, verdict $v$, and database origin $d$. To combine these disparate pieces of information, we first harmonize them into a common \"pseudo-count\" weight, $w$. The given transformation is:\n$$ w = \\min\\left( W_{\\max},\\ \\lambda_{t}\\ \\eta_{d}\\ \\frac{r}{1-r} \\right) $$\nwhere $W_{\\max}=10$, and $\\lambda_t$ and $\\eta_d$ are specified reliability and calibration multipliers. The term $r/(1-r)$ represents the odds of the evidence being correct. This weight $w$ quantifies the strength of a single piece of evidence.\n\nWe interpret a supportive verdict as a fractional Bernoulli trial with $w$ \"successes,\" and a refuting verdict as a fractional trial with $w$ \"failures.\" For a collection of $N$ evidence items $\\mathcal{D} = \\{(v_i, w_i)\\}_{i=1}^N$, the combined likelihood function is constructed by treating each piece of evidence as an independent observation. For a single observation with weight $w_i$ and verdict $v_i$, the likelihood contribution is proportional to $p^{w_i}$ if $v_i = \\text{support}$ and $(1-p)^{w_i}$ if $v_i = \\text{refute}$. The total likelihood is the product of these individual contributions:\n$$ P(\\mathcal{D} \\mid p) \\propto \\prod_{i=1}^N \\begin{cases} p^{w_i} & \\text{if } v_i = \\text{support} \\\\ (1-p)^{w_i} & \\text{if } v_i = \\text{refute} \\end{cases} $$\nLet $W_{\\text{support}}$ be the sum of weights for all supportive evidence and $W_{\\text{refute}}$ be the sum for all refuting evidence. The likelihood function simplifies to:\n$$ P(\\mathcal{D} \\mid p) \\propto p^{W_{\\text{support}}} (1-p)^{W_{\\text{refute}}} $$\n\n**4. Posterior Distribution**\n\nApplying Bayes' theorem, we combine the prior and the likelihood:\n$$ P(p \\mid \\mathcal{D}) \\propto P(\\mathcal{D} \\mid p) P(p) \\propto \\left( p^{W_{\\text{support}}} (1-p)^{W_{\\text{refute}}} \\right) \\left( p^{\\alpha_0-1} (1-p)^{\\beta_0-1} \\right) $$\n$$ P(p \\mid \\mathcal{D}) \\propto p^{\\alpha_0 + W_{\\text{support}} - 1} (1-p)^{\\beta_0 + W_{\\text{refute}} - 1} $$\nThis expression is the kernel of a Beta distribution. Therefore, the posterior distribution is also a Beta distribution:\n$$ p \\mid \\mathcal{D} \\sim \\mathrm{Beta}(\\alpha_{\\text{post}}, \\beta_{\\text{post}}) $$\nwith posterior parameters:\n$$ \\alpha_{\\text{post}} = \\alpha_0 + W_{\\text{support}} = 1 + W_{\\text{support}} $$\n$$ \\beta_{\\text{post}} = \\beta_0 + W_{\\text{refute}} = 1 + W_{\\text{refute}} $$\n\n**5. Calculation of Target Quantities**\n\nOnce we have the posterior distribution $\\mathrm{Beta}(\\alpha_{\\text{post}}, \\beta_{\\text{post}})$, we can compute the required quantities:\n\n- **Posterior Mean:** The expected value of a $\\mathrm{Beta}(\\alpha, \\beta)$ random variable provides a point estimate for $p$.\n  $$ p_{\\text{mean}} = E[p \\mid \\mathcal{D}] = \\frac{\\alpha_{\\text{post}}}{\\alpha_{\\text{post}} + \\beta_{\\text{post}}} $$\n\n- **Credible Interval:** The $95\\%$ central credible interval gives a range of plausible values for $p$. It is defined by the $0.025$ and $0.975$ quantiles of the posterior distribution. These are found using the inverse of the Beta cumulative distribution function (CDF), also known as the percent-point function (PPF).\n  $$ p_{\\text{lower}} = \\text{PPF}_{\\mathrm{Beta}(\\alpha_{\\text{post}}, \\beta_{\\text{post}})}(0.025) $$\n  $$ p_{\\text{upper}} = \\text{PPF}_{\\mathrm{Beta}(\\alpha_{\\text{post}}, \\beta_{\\text{post}})}(0.975) $$\n\n- **Consensus Decision:** The decision rule is a direct application of the credible interval. The interaction is accepted if there is strong confidence that its probability is above $0.5$.\n  $$ \\text{decision} = (p_{\\text{lower}} \\ge 0.5) $$\n\nThis structured, principle-based approach allows for a robust and interpretable synthesis of evidence, culminating in a clear, quantifiable conclusion for each candidate interaction. We will now implement this procedure.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import beta as beta_dist\nimport math\n\ndef solve():\n    \"\"\"\n    Solves the protein-protein interaction evidence harmonization problem.\n    \"\"\"\n    # Define constants and parameters provided in the problem statement.\n    LAMBDA_T = {\n        'Y2H': 1.0,\n        'AP-MS': 0.8,\n        'Coexp': 0.5,\n        'Curated': 1.2\n    }\n    ETA_D = {\n        'BioGRID': 1.0,\n        'IntAct': 1.0,\n        'STRING': 0.9,\n        'Literature': 1.1\n    }\n    W_MAX = 10.0\n    ALPHA_0 = 1.0\n    BETA_0 = 1.0\n\n    # Define the test suite from the problem statement.\n    test_cases = [\n        # Edge A–B\n        [\n            ('BioGRID', 'Y2H', 'support', 0.95),\n            ('IntAct', 'AP-MS', 'support', 0.85),\n            ('Literature', 'Curated', 'support', 0.90),\n            ('STRING', 'Coexp', 'refute', 0.20)\n        ],\n        # Edge C–D\n        [\n            ('STRING', 'Coexp', 'support', 0.60),\n            ('BioGRID', 'Y2H', 'refute', 0.70),\n            ('IntAct', 'AP-MS', 'support', 0.70),\n            ('Literature', 'Curated', 'refute', 0.65)\n        ],\n        # Edge E–F\n        [\n            ('STRING', 'Coexp', 'refute', 0.95),\n            ('IntAct', 'AP-MS', 'refute', 0.90),\n            ('Literature', 'Curated', 'refute', 0.80)\n        ],\n        # Edge G–H\n        [\n            ('BioGRID', 'Y2H', 'support', 0.99),\n            ('STRING', 'Coexp', 'support', 0.51),\n            ('IntAct', 'AP-MS', 'refute', 0.05)\n        ],\n        # Edge I–J\n        [\n            ('Literature', 'Curated', 'support', 0.70),\n            ('BioGRID', 'Y2H', 'refute', 0.60)\n        ]\n    ]\n\n    results = []\n    for edge_evidence in test_cases:\n        w_support = 0.0\n        w_refute = 0.0\n\n        for d, t, v, r in edge_evidence:\n            lambda_val = LAMBDA_T[t]\n            eta_val = ETA_D[d]\n\n            # The term r/(1-r) represents the odds.\n            # It is undefined for r=1. Handle this by treating it as infinite odds,\n            # which will always result in the maximum weight W_MAX.\n            if r == 1.0:\n                odds = float('inf')\n            elif r == 0.0:\n                odds = 0.0\n            else:\n                odds = r / (1.0 - r)\n\n            raw_weight = lambda_val * eta_val * odds\n            weight = min(W_MAX, raw_weight)\n\n            if v == 'support':\n                w_support += weight\n            elif v == 'refute':\n                w_refute += weight\n\n        # Calculate posterior Beta distribution parameters\n        alpha_post = ALPHA_0 + w_support\n        beta_post = BETA_0 + w_refute\n\n        # Calculate target quantities from the posterior distribution\n        \n        # 1. Posterior mean\n        if (alpha_post + beta_post) == 0:\n            # Should not happen with alpha_0, beta_0 > 0\n            p_mean = 0.5\n        else:\n            p_mean = alpha_post / (alpha_post + beta_post)\n\n        # 2. 95% central credible interval\n        p_lower = beta_dist.ppf(0.025, alpha_post, beta_post)\n        p_upper = beta_dist.ppf(0.975, alpha_post, beta_post)\n        \n        # 3. Consensus decision\n        decision = p_lower >= 0.5\n\n        # Format results for the current edge, rounding to 6 decimal places.\n        # np.round is used for safe rounding, e.g., for numbers like 2.5.\n        # Python's round() rounds to nearest even for .5 cases.\n        current_result = [\n            np.round(p_mean, 6),\n            np.round(p_lower, 6),\n            np.round(p_upper, 6),\n            decision\n        ]\n        results.append(current_result)\n\n    # Format the final output string as specified\n    # The str() function on a list and a boolean produces the required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3341723"}, {"introduction": "Once a weighted PPI network is constructed, a primary analytical goal is to identify functional modules, which often correspond to protein complexes. This hands-on exercise challenges you to implement the Markov Clustering (MCL) algorithm from first principles, a powerful and widely used method for discovering community structure in graphs. By simulating the flow of random walks, you will gain a deep, practical understanding of how network topology can reveal underlying biological organization. [@problem_id:3341694]", "problem": "You are given a protein–protein interaction (PPI) network as an undirected, weighted graph, represented by a square, symmetric adjacency matrix $A \\in \\mathbb{R}_{\\ge 0}^{n \\times n}$, where $n$ is the number of proteins (nodes), and $A_{ij}$ encodes the nonnegative interaction weight between protein $i$ and protein $j$. Starting from the foundational concept of a discrete-time Markov chain defined on a graph, derive from first principles a clustering procedure based on repeated multi-step random walk diffusion and a probability reweighting operation. In your derivation, use only core definitions of a Markov chain transition matrix and basic matrix operations; do not invoke any specialized clustering formulas. Clearly state the expansion and inflation operators on the random walk matrix that you derive, and explain how the inflation parameter controls the granularity of the resulting clusters.\n\nImplement your derived procedure as a complete, runnable program that takes no external input and operates on the following test suite. For each test case, you must:\n\n- Construct the random walk matrix $W$ for the given adjacency matrix $A$ by adding self-loops and normalizing to obtain a column-stochastic matrix.\n- Apply the expansion operator with integer exponent $e$ by repeated matrix multiplication of $W$.\n- Apply the inflation operator with parameter $r > 1$ as an element-wise power followed by column normalization to maintain a valid transition matrix.\n- Iterate expansion and inflation until convergence under a specified tolerance $\\epsilon$ or until a specified maximum number of iterations $T$ is reached; incorporate a pruning threshold $\\tau$ to set very small values to zero after inflation for numerical stability.\n- After convergence, assign each node to a cluster by interpreting the nonzero pattern of the converged matrix as an undirected graph and computing connected components after symmetrization via thresholding with $\\tau$.\n- Return, for each test case, a list whose first element is the integer number of clusters $k$ and whose remaining elements are the integer cluster labels for nodes $0$ through $n-1$ in ascending index order.\n\nTest suite parameters:\n\n1. A graph with two dense modules and a weak inter-module connection. The adjacency matrix $A_1 \\in \\mathbb{R}^{6 \\times 6}$ is:\n$$\nA_1 =\n\\begin{bmatrix}\n0 & 1 & 1 & 0 & 0 & 0 \\\\\n1 & 0 & 1 & 0 & 0 & 0 \\\\\n1 & 1 & 0 & 0.05 & 0 & 0 \\\\\n0 & 0 & 0.05 & 0 & 1 & 1 \\\\\n0 & 0 & 0 & 1 & 0 & 1 \\\\\n0 & 0 & 0 & 1 & 1 & 0\n\\end{bmatrix}\n$$\nRun three cases on $A_1$: $(e=2, r=1.2, \\epsilon=10^{-6}, \\tau=10^{-4}, T=50)$, $(e=2, r=2.0, \\epsilon=10^{-6}, \\tau=10^{-4}, T=50)$, and $(e=2, r=4.0, \\epsilon=10^{-6}, \\tau=10^{-4}, T=50)$.\n\n2. A star graph capturing hub–spoke structure. The adjacency matrix $A_2 \\in \\mathbb{R}^{6 \\times 6}$ is:\n$$\nA_2 =\n\\begin{bmatrix}\n0 & 1 & 1 & 1 & 1 & 1 \\\\\n1 & 0 & 0 & 0 & 0 & 0 \\\\\n1 & 0 & 0 & 0 & 0 & 0 \\\\\n1 & 0 & 0 & 0 & 0 & 0 \\\\\n1 & 0 & 0 & 0 & 0 & 0 \\\\\n1 & 0 & 0 & 0 & 0 & 0\n\\end{bmatrix}\n$$\nRun one case on $A_2$: $(e=2, r=2.0, \\epsilon=10^{-6}, \\tau=10^{-4}, T=50)$.\n\n3. A complete graph modeling a nearly homogeneous interaction environment. The adjacency matrix $A_3 \\in \\mathbb{R}^{5 \\times 5}$ is:\n$$\nA_3 =\n\\begin{bmatrix}\n0 & 1 & 1 & 1 & 1 \\\\\n1 & 0 & 1 & 1 & 1 \\\\\n1 & 1 & 0 & 1 & 1 \\\\\n1 & 1 & 1 & 0 & 1 \\\\\n1 & 1 & 1 & 1 & 0\n\\end{bmatrix}\n$$\nRun one case on $A_3$: $(e=2, r=2.0, \\epsilon=10^{-6}, \\tau=10^{-4}, T=50)$.\n\nImportant implementation requirements:\n\n- Self-loops: Before constructing the random walk matrix, add an identity matrix $I$ scaled by $1$ (i.e., $A \\leftarrow A + I$) to ensure aperiodicity and nonzero column sums.\n- Column-stochastic normalization: For a matrix $M$, enforce $M_{ij} \\ge 0$ and $\\sum_{i} M_{ij} = 1$ for each column $j$ when constructing and maintaining the random walk matrix.\n- Convergence: Use the $\\ell_1$ norm on the matrix difference to check convergence, i.e., stop when $\\sum_{i,j} \\lvert W^{(t+1)}_{ij} - W^{(t)}_{ij} \\rvert  \\epsilon$ or when $t$ reaches $T$.\n- Pruning: After inflation, set values strictly less than $\\tau$ to $0$ and then renormalize columns.\n\nFinal output specification:\n\n- For each test case, output a list whose first element is the integer number of clusters $k$, followed by $n$ integers giving the cluster label of each node $0$ through $n-1$.\n- Your program should produce a single line of output containing the results for all test cases as a comma-separated list enclosed in square brackets, for example, $\\texttt{[[k,\\ell_0,\\dots,\\ell_{n-1}],\\dots]}$. There should be no additional text printed.", "solution": "The problem asks for the derivation and implementation of a graph clustering algorithm based on simulated random walks. This procedure is known in the literature as the Markov Clustering (MCL) algorithm. Here, we derive its core components from first principles as requested.\n\n### Derivation from First Principles\n\nOur starting point is a protein-protein interaction (PPI) network represented as an undirected, weighted graph. The graph's structure is encoded in a symmetric adjacency matrix $A \\in \\mathbb{R}_{\\ge 0}^{n \\times n}$, where $n$ is the number of proteins (nodes) and $A_{ij} \\ge 0$ is the weight of the interaction between protein $i$ and protein $j$. The fundamental insight is that a random walk on this graph is more likely to remain within a densely connected region (a putative cluster) than to traverse a sparse connection to another region. The algorithm is designed to amplify this tendency by alternating between two operators: expansion and inflation.\n\n**1. Constructing the Markov Chain**\n\nFirst, we formalize the notion of a random walk on the graph as a discrete-time Markov chain. The states of the chain are the nodes of the graph (the proteins). We need to define the transition probabilities between states.\n\nA walker at node $j$ can move to an adjacent node $i$ with a probability proportional to the edge weight $A_{ij}$. To construct a valid stochastic process, we must ensure the graph is aperiodic and that every node has at least one outgoing path (including to itself), preventing the walk from getting trapped. This is achieved by adding a self-loop of weight $1$ to every node. The modified adjacency matrix becomes $A' = A + I$, where $I$ is the $n \\times n$ identity matrix.\n\nThe probability of transitioning from node $j$ to node $i$, denoted $W_{ij}$, is the weight of the edge $(j, i)$ normalized by the sum of weights of all edges emanating from node $j$. This sum is the sum of the entries in the $j$-th column of $A'$.\n$$\nW_{ij} = \\frac{A'_{ij}}{\\sum_{k=1}^{n} A'_{kj}}\n$$\nThe resulting matrix $W$ is the transition matrix of our Markov chain. By this construction, it is a column-stochastic matrix, meaning each of its entries $W_{ij}$ is non-negative and each column sums to $1$ ($\\sum_{i=1}^{n} W_{ij} = 1$ for all $j$). The vector in column $j$ of $W$ represents the probability distribution for the walker's position after one step, given it started at node $j$.\n\n**2. The Expansion Operator**\n\nThe expansion step simulates the diffusion of a random walk across the graph over multiple steps. The matrix $W^2 = W \\times W$ gives the probabilities of a $2$-step walk. Specifically, the entry $(W^2)_{ij}$ is the probability of a random walker starting at node $j$ reaching node $i$ in exactly two steps. This is computed by summing over all possible intermediate nodes $k$:\n$$\n(W^2)_{ij} = \\sum_{k=1}^{n} W_{ik} W_{kj}\n$$\nGeneralizing this, the matrix $W^e$ for an integer exponent $e  1$ contains the probabilities for $e$-step walks. The expansion operator is thus defined as taking a stochastic matrix $M$ to its $e$-th power:\n$$\n\\text{Expand}(M) = M^e\n$$\nThis operation allows probability \"flow\" to explore the neighborhood of a starting node. In a graph with natural clusters, this flow will predominantly spread within those clusters, thus strengthening intra-cluster connections relative to inter-cluster connections.\n\n**3. The Inflation Operator**\n\nAfter expansion has spread the flow, we need a mechanism to contract it, thereby accentuating the distinction between strong and weak flow paths. The inflation operator accomplishes this by modifying the probability distribution in each column to favor high-probability transitions and penalize low-probability ones.\n\nThis is achieved through an element-wise exponentiation followed by re-normalization. For a given stochastic matrix $M$ and an inflation parameter $r  1$, we first compute a matrix $M'$ by raising every element of $M$ to the power of $r$: $M'_{ij} = (M_{ij})^r$. A value of $r  1$ ensures that higher probabilities are magnified more than lower ones (e.g., if $p_1  p_2$, then $p_1^r / p_2^r  p_1 / p_2$).\n\nAfter this operation, the columns of $M'$ no longer sum to $1$. To restore the stochastic property, we must re-normalize each column. The inflation operator, $\\Gamma_r$, is therefore defined as:\n$$\n(\\Gamma_r(M))_{ij} = \\frac{(M_{ij})^r}{\\sum_{k=1}^{n} (M_{kj})^r}\n$$\nThe inflation parameter $r$ directly controls the granularity of the clustering. A higher value of $r$ imposes a stronger \"winner-takes-all\" dynamic, enhancing the contrast between probabilities more aggressively. This leads to harder cluster boundaries and tends to partition the graph into more, smaller (finer-grained) clusters. Conversely, a value of $r$ closer to $1$ has a milder effect, favoring the discovery of larger, more coarse-grained clusters.\n\n**4. The Iterative Procedure**\n\nThe MCL algorithm iterates these two operators:\n1. Start with the initial transition matrix $W = W^{(0)}$.\n2. For each iteration $t=0, 1, 2, \\dots$:\n   a. **Expansion:** $W^{(t, \\text{exp})} = \\text{Expand}(W^{(t)}) = (W^{(t)})^e$.\n   b. **Inflation:** $W^{(t+1)} = \\Gamma_r(W^{(t, \\text{exp})})$.\nThis process continues until the matrix $W^{(t)}$ converges, meaning the change between successive iterations falls below a given tolerance $\\epsilon$. Upon convergence, the matrix represents a partitioning of the graph's flow. It becomes a nearly idempotent matrix where the graph of non-zero entries consists of several disconnected components, corresponding to the identified clusters. Within each component, the columns corresponding to its nodes become identical, representing the stationary distribution of a random walk confined to that cluster. The final clusters are extracted by identifying these disjoint components from the converged matrix.", "answer": "```python\nimport numpy as np\nfrom scipy.sparse.csgraph import connected_components\nfrom scipy.sparse import csr_matrix\n\ndef solve():\n    \"\"\"\n    Main function to run the MCL algorithm on the specified test suite.\n    \"\"\"\n\n    # --- Test Suite Definition ---\n\n    A1 = np.array([\n        [0, 1, 1, 0, 0, 0],\n        [1, 0, 1, 0, 0, 0],\n        [1, 1, 0, 0.05, 0, 0],\n        [0, 0, 0.05, 0, 1, 1],\n        [0, 0, 0, 1, 0, 1],\n        [0, 0, 0, 1, 1, 0]\n    ])\n\n    A2 = np.array([\n        [0, 1, 1, 1, 1, 1],\n        [1, 0, 0, 0, 0, 0],\n        [1, 0, 0, 0, 0, 0],\n        [1, 0, 0, 0, 0, 0],\n        [1, 0, 0, 0, 0, 0],\n        [1, 0, 0, 0, 0, 0]\n    ])\n\n    A3 = np.array([\n        [0, 1, 1, 1, 1],\n        [1, 0, 1, 1, 1],\n        [1, 1, 0, 1, 1],\n        [1, 1, 1, 0, 1],\n        [1, 1, 1, 1, 0]\n    ])\n\n    test_cases = [\n        {'A': A1, 'e': 2, 'r': 1.2, 'epsilon': 1e-6, 'tau': 1e-4, 'T': 50},\n        {'A': A1, 'e': 2, 'r': 2.0, 'epsilon': 1e-6, 'tau': 1e-4, 'T': 50},\n        {'A': A1, 'e': 2, 'r': 4.0, 'epsilon': 1e-6, 'tau': 1e-4, 'T': 50},\n        {'A': A2, 'e': 2, 'r': 2.0, 'epsilon': 1e-6, 'tau': 1e-4, 'T': 50},\n        {'A': A3, 'e': 2, 'r': 2.0, 'epsilon': 1e-6, 'tau': 1e-4, 'T': 50},\n    ]\n\n    def run_mcl(A, e, r, epsilon, tau, T):\n        \"\"\"\n        Implements the Markov Clustering (MCL) algorithm.\n        \"\"\"\n        n = A.shape[0]\n\n        # 1. Add self-loops and normalize to get the initial transition matrix W\n        A_self_looped = A + np.identity(n)\n        col_sums = A_self_looped.sum(axis=0)\n        \n        # Handle columns that might sum to zero in a general case\n        # (though adding identity prevents this for non-empty graphs)\n        col_sums[col_sums == 0] = 1.0\n        W = A_self_looped / col_sums\n        \n        for _ in range(T):\n            W_prev = W\n\n            # 2. Expansion step: Simulate random walks\n            W = np.linalg.matrix_power(W, e)\n\n            # 3. Inflation step: Reweight probabilities\n            W = np.power(W, r)\n            \n            # Pruning step for numerical stability and sparsity\n            W[W  tau] = 0.0\n\n            # 4. Normalization step to restore column-stochastic property\n            col_sums = W.sum(axis=0)\n            \n            # Avoid division by zero for columns that become all-zero after pruning\n            non_zero_cols_mask = col_sums != 0\n            # Use np.divide for safe division\n            W = np.divide(W, col_sums, out=np.zeros_like(W), where=non_zero_cols_mask)\n\n            # 5. Check for convergence using the L1 norm of the difference\n            if np.sum(np.abs(W - W_prev))  epsilon:\n                break\n        \n        # 6. Cluster assignment from the converged matrix\n        # Create an undirected graph based on the non-zero pattern of W\n        adj_matrix_thresholded = W > tau\n        adj_matrix_symmetric = adj_matrix_thresholded | adj_matrix_thresholded.T\n\n        # Find connected components in the resulting graph\n        graph_sparse = csr_matrix(adj_matrix_symmetric)\n        n_components, labels = connected_components(csgraph=graph_sparse, directed=False, connection='weak')\n        \n        return [n_components] + labels.tolist()\n    \n    results = []\n    for case in test_cases:\n        result = run_mcl(case['A'], case['e'], case['r'], case['epsilon'], case['tau'], case['T'])\n        results.append(result)\n\n    # Print the final result in the specified format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3341694"}, {"introduction": "While standard clustering algorithms identify dense subgraphs, they often ignore a crucial aspect of biochemistry: protein complexes assemble with specific subunit ratios, or stoichiometry. This advanced practice moves beyond pure topological analysis by formulating complex inference as a constrained optimization problem. You will develop a solver that respects both subunit availability and stoichiometric requirements, enabling a more biologically realistic prediction of the cell's complement of protein complexes. [@problem_id:3341707]", "problem": "You are tasked with formulating and solving a stoichiometry-aware complex inference problem as a mixed-integer network flow on a protein–protein interaction network. The foundation must be drawn from mass conservation and stoichiometric constraints, and the integrality of assembled complexes must be enforced to exclude impossible assembly states.\n\nStart from the following base principles: (i) conservation of mass, which requires that subunits consumed in assembly equal the stoichiometrically required subunits, (ii) network flow conservation, which requires that the amount of subunit allocated does not exceed supply, and (iii) integrality of complex counts, which requires that assembled complexes be integer-valued. Use these to derive a mixed-integer formulation suitable for computation.\n\nDefine a Protein–Protein Interaction (PPI) network as a simple undirected graph $G = (V,E)$ where $V$ is a set of subunit types and $E$ is an edge set representing observed physical interactions. Let $C$ be a finite set of complexes. For each complex $c \\in C$, define a stoichiometry vector $\\mathbf{s}^{(c)} \\in \\mathbb{Z}_{\\ge 0}^{|V|}$ whose component $s_{u,c}$ is the number of copies of subunit $u \\in V$ required per assembled copy of complex $c$. Let $\\mathbf{n} \\in \\mathbb{Z}_{\\ge 0}^{|V|}$ be the available copy number of each subunit. Define decision variables $x_c \\in \\mathbb{Z}_{\\ge 0}$ for each $c \\in C$ representing the number of assembled copies of complex $c$, and integer flows $f_{u,c} \\in \\mathbb{Z}_{\\ge 0}$ representing the number of copies of subunit $u$ allocated to complex $c$ across a conceptual bipartite flow from subunits to complexes. The constraints must include:\n- For each subunit $u \\in V$: $$\\sum_{c \\in C} f_{u,c} \\le n_u$$\n- For each complex $c \\in C$ and subunit $u \\in V$: $$f_{u,c} = s_{u,c} \\, x_c$$\n- For each complex $c \\in C$, assembly is allowed only if the induced subgraph on the set $\\{u \\in V \\mid s_{u,c}  0\\}$ is connected in $G$; otherwise, $x_c = 0$.\n\nYour goal is to compute the integer vector $\\mathbf{x} = (x_c)_{c \\in C}$ that maximizes the total number of assembled complexes $$\\sum_{c \\in C} x_c$$ subject to the above constraints. In case multiple optimal integer solutions exist, break ties by selecting the lexicographically smallest $\\mathbf{x}$ under a fixed complex ordering.\n\nImplement a program that, given the following test suite of parameter sets, outputs the optimal integer complex counts $\\mathbf{x}$ for each case. There is no input; hard-code the test suite.\n\nTest Suite:\n- Case $1$:\n  - Subunits $V = \\{A,B,C\\}$\n  - Edges $E = \\{\\{A,B\\}, \\{B,C\\}, \\{A,C\\}\\}$ (a triangle)\n  - Complexes $C = \\{X,Y\\}$ with stoichiometries:\n    - $X$: $s_{A,X} = 1$, $s_{B,X} = 1$, $s_{C,X} = 0$\n    - $Y$: $s_{A,Y} = 0$, $s_{B,Y} = 2$, $s_{C,Y} = 1$\n  - Supplies: $n_A = 3$, $n_B = 4$, $n_C = 2$\n  - Complex ordering: $[X,Y]$\n- Case $2$:\n  - Subunits $V = \\{A,B,D\\}$\n  - Edges $E = \\{\\{A,B\\}\\}$ (no edge between $A$ and $D$)\n  - Complexes $C = \\{X,Z\\}$ with stoichiometries:\n    - $X$: $s_{A,X} = 1$, $s_{B,X} = 1$, $s_{D,X} = 0$\n    - $Z$: $s_{A,Z} = 1$, $s_{B,Z} = 0$, $s_{D,Z} = 1$\n  - Supplies: $n_A = 2$, $n_B = 2$, $n_D = 1$\n  - Complex ordering: $[X,Z]$\n- Case $3$:\n  - Subunits $V = \\{A,B,C\\}$\n  - Edges $E = \\{\\{A,B\\}, \\{B,C\\}, \\{A,C\\}\\}$ (a triangle)\n  - Complexes $C = \\{T,W\\}$ with stoichiometries:\n    - $T$: $s_{A,T} = 1$, $s_{B,T} = 1$, $s_{C,T} = 1$\n    - $W$: $s_{A,W} = 2$, $s_{B,W} = 0$, $s_{C,W} = 1$\n  - Supplies: $n_A = 2$, $n_B = 1$, $n_C = 1$\n  - Complex ordering: $[T,W]$\n\nYour program must verify connectivity feasibility for each complex, compute upper bounds on $x_c$ from stoichiometry and supplies, and search integer solutions with pruning to respect $\\sum_{c \\in C} s_{u,c} x_c \\le n_u$ for each $u \\in V$. The final output shall be a single line containing the results as a comma-separated list of lists, with each inner list giving the optimal integer counts in the specified complex ordering for that case. For example, output format should be like $[[x_{X},x_{Y}], [x_{X},x_{Z}], [x_{T},x_{W}]]$, printed exactly as a single Python list literal on one line. No physical units or angles apply in this problem; all outputs are integers without units.", "solution": "The user has provided a well-defined mixed-integer optimization problem for inferring protein complex assemblies from a protein-protein interaction (PPI) network. I will first validate the problem statement and then provide a full solution.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\n- **Network:** A simple undirected graph $G = (V,E)$, where $V$ is a set of subunit types and $E$ is a set of physical interactions.\n- **Complexes:** A finite set $C$ of potential complexes.\n- **Stoichiometry:** For each complex $c \\in C$, a vector $\\mathbf{s}^{(c)} \\in \\mathbb{Z}_{\\ge 0}^{|V|}$ where $s_{u,c}$ is the number of copies of subunit $u \\in V$ in one copy of complex $c$.\n- **Supplies:** A vector $\\mathbf{n} \\in \\mathbb{Z}_{\\ge 0}^{|V|}$ where $n_u$ is the available copy number of subunit $u$.\n- **Decision Variables:**\n    - $x_c \\in \\mathbb{Z}_{\\ge 0}$: number of assembled copies of complex $c$.\n    - $f_{u,c} \\in \\mathbb{Z}_{\\ge 0}$: number of copies of subunit $u$ allocated to complex $c$.\n- **Constraints:**\n    1.  Supply Constraint: For each subunit $u \\in V$, $\\sum_{c \\in C} f_{u,c} \\le n_u$.\n    2.  Stoichiometry Constraint: For each complex $c \\in C$ and subunit $u \\in V$, $f_{u,c} = s_{u,c} \\, x_c$.\n    3.  Connectivity Constraint: For each complex $c \\in C$, if the subgraph of $G$ induced by its constituent subunits $\\{u \\in V \\mid s_{u,c}  0\\}$ is not connected, then $x_c = 0$.\n- **Objective Function:** Maximize the total number of assembled complexes, $\\sum_{c \\in C} x_c$.\n- **Tie-Breaking Rule:** Among multiple solutions with the same maximal sum, select the one whose vector $\\mathbf{x} = (x_c)_{c \\in C}$ is lexicographically smallest, based on a specified complex ordering.\n- **Test Suite:** Three specific instances of the problem are provided with all necessary parameters (subunits, edges, complexes, stoichiometries, supplies, and ordering).\n\n**Step 2: Validate Using Extracted Givens**\n\n- **Scientifically Grounded:** The problem is a standard formulation in computational systems biology for protein complex prediction. It is based on fundamental principles of mass conservation (stoichiometry and supply constraints) and biophysical feasibility (connectivity constraint). This is scientifically sound.\n- **Well-Posed:** The problem is a bounded Integer Linear Program (ILP). The decision variables $x_c$ are non-negative integers. The constraints define a bounded feasible region (a convex polytope for the continuous relaxation). Maximizing a linear objective function over the integer points within this region guarantees that an optimal solution exists. The specified tie-breaking rule ensures the solution is unique.\n- **Objective:** The problem is defined with precise mathematical notation and seeks a quantifiable, objective result. It is free from subjective or ambiguous language.\n- **Completeness and Consistency:** All required data for the test cases are provided. The constraints can be combined into a single, consistent set of inequalities. By substituting the stoichiometry constraint into the supply constraint, we get $\\sum_{c \\in C} s_{u,c} x_c \\le n_u$ for each subunit $u$, which simplifies the problem by eliminating the flow variables $f_{u,c}$. There are no contradictions.\n\n**Step 3: Verdict and Action**\n\nThe problem is valid as it is scientifically grounded, well-posed, objective, and fully specified. I will proceed with formulating and implementing a solution.\n\n### Solution Derivation and Algorithmic Design\n\nThe problem is to solve a small-scale Integer Linear Program (ILP). Given the small size of the test cases, a backtracking recursive search is a suitable and effective method.\n\n**1. Simplification of the Model**\nThe flow variables $f_{u,c}$ can be substituted out of the model. The constraint $f_{u,c} = s_{u,c} x_c$ allows us to rewrite the supply constraint $\\sum_{c \\in C} f_{u,c} \\le n_u$ directly in terms of the primary decision variables $x_c$:\n$$ \\sum_{c \\in C} s_{u,c} x_c \\le n_u \\quad \\forall u \\in V $$\nThe optimization problem thus becomes:\n- **Maximize:** $Z = \\sum_{c \\in C} x_c$\n- **Subject to:**\n    1. $\\sum_{c \\in C} s_{u,c} x_c \\le n_u$ for each subunit $u \\in V$.\n    2. $x_c = 0$ if complex $c$ is not connectively feasible.\n    3. $x_c \\in \\mathbb{Z}_{\\ge 0}$ for each complex $c \\in C$.\n\n**2. Connectivity Feasibility Check**\nFor each complex $c$, we must first verify its structural feasibility. This involves identifying the set of required subunits $V_c = \\{u \\in V \\mid s_{u,c}  0\\}$ and checking if the subgraph of $G$ induced by $V_c$ is connected. This check can be performed efficiently using a graph traversal algorithm like Breadth-First Search (BFS) or Depth-First Search (DFS).\nThe algorithm is as follows:\n1. If the set $V_c$ is empty or contains a single subunit, it is considered connected.\n2. Otherwise, pick an arbitrary starting subunit $u_{start} \\in V_c$.\n3. Perform a traversal (e.g., BFS) starting from $u_{start}$, visiting only neighbors that are also in $V_c$.\n4. Count the number of unique subunits visited. If this count equals the size of $V_c$, the subgraph is connected. Otherwise, it is not.\nIf a complex is found to be infeasible, its count $x_c$ is fixed to $0$ for the remainder of the problem.\n\n**3. Backtracking Search Algorithm**\nA recursive backtracking search will explore the solution space of integer vectors $\\mathbf{x}$. The search proceeds by making a decision for one complex count at a time, according to the specified complex ordering.\n\nThe state of our recursive function `search(k, x_path, current_supplies)` can be defined by:\n- $k$: The index of the complex in the specified ordering for which we are determining a count.\n- `x_path`: The list of counts determined for the first $k$ complexes.\n- `current_supplies`: The vector of remaining subunit supplies after forming the complexes in `x_path`.\n\nThe algorithm proceeds as follows:\n1.  **Base Case:** If all complex counts have been determined ($k$ equals the total number of complexes), we have found a complete and feasible solution vector. We compare its sum $\\sum x_c$ with the best sum found so far.\n    - If the current sum is greater, we update our best-found solution.\n    - If the current sum is equal, we apply the tie-breaking rule: if the current solution vector is lexicographically smaller than the best-found solution, we update it.\n\n2.  **Recursive Step:** For the $k$-th complex in the order:\n    a. Check if it is connectively feasible. If not, its count must be $0$. We proceed to the next complex in the recursion with the count for the current one set to $0$.\n    b. If it is feasible, calculate the maximum possible integer count for this complex, $x_{k,max}$, given the `current_supplies`. This is determined by the most limiting subunit:\n       $$ x_{k,max} = \\min_{u \\in V \\text{ s.t. } s_{u,k} > 0} \\left\\lfloor \\frac{n_{u, \\text{current}}}{s_{u,k}} \\right\\rfloor $$\n    c. Iterate through all possible integer counts for this complex, from $x_{k,max}$ down to $0$. The downward iteration is a greedy heuristic that helps find high-sum solutions faster. For each potential count `val`:\n       i.  Update the supplies by subtracting the subunits consumed: $\\mathbf{n}_{\\text{new}} = \\mathbf{n}_{\\text{current}} - \\text{val} \\cdot \\mathbf{s}^{(k)}$.\n       ii. Make a recursive call for the next complex: `search(k+1, x_path + [val], n_new)`.\n\nThis search systematically explores all valid combinations of complex counts, guaranteeing that the optimal solution according to the objective and tie-breaking rule is found.", "answer": "```python\nimport numpy as np\n\nclass StoichiometrySolver:\n    \"\"\"\n    Solves the stoichiometry-aware complex inference problem for a single case\n    using a backtracking recursive search.\n    \"\"\"\n    def __init__(self, V, E, C, S, n, complex_order):\n        # Map subunit and complex names to integer indices for easier processing\n        self.subunit_names = list(V)\n        self.subunit_map = {name: i for i, name in enumerate(self.subunit_names)}\n        self.num_subunits = len(V)\n        \n        self.complex_names_in_order = list(complex_order)\n        self.complex_map = {name: i for i, name in enumerate(C)}\n        self.num_complexes = len(C)\n        \n        # The search will follow the specified complex ordering\n        self.search_order_indices = [self.complex_map[name] for name in self.complex_names_in_order]\n\n        # Store initial supplies as a NumPy array\n        self.initial_supplies = np.zeros(self.num_subunits, dtype=int)\n        for u_name, count in n.items():\n            if u_name in self.subunit_map:\n                self.initial_supplies[self.subunit_map[u_name]] = count\n\n        # Build adjacency list for the PPI graph\n        self.adj = {i: set() for i in range(self.num_subunits)}\n        for u_name, v_name in E:\n            u_idx, v_idx = self.subunit_map[u_name], self.subunit_map[v_name]\n            self.adj[u_idx].add(v_idx)\n            self.adj[v_idx].add(u_idx)\n\n        # Build stoichiometry matrix S[c_idx, u_idx]\n        self.stoichiometry = np.zeros((self.num_complexes, self.num_subunits), dtype=int)\n        for c_name, s_dict in S.items():\n            c_idx = self.complex_map[c_name]\n            for u_name, count in s_dict.items():\n                if u_name in self.subunit_map:\n                    u_idx = self.subunit_map[u_name]\n                    self.stoichiometry[c_idx, u_idx] = count\n        \n        # Placeholders for solver state\n        self.is_feasible = []\n        self.best_x = []\n        self.best_sum = -1\n\n    def _is_connected(self, subunit_indices):\n        \"\"\"Checks if the subgraph induced by a set of subunits is connected using BFS.\"\"\"\n        nodes = set(subunit_indices)\n        if not nodes or len(nodes) == 1:\n            return True\n\n        q = [next(iter(nodes))]\n        visited = {q[0]}\n        head = 0\n        while head  len(q):\n            u = q[head]\n            head += 1\n            for v in self.adj.get(u, set()):\n                if v in nodes and v not in visited:\n                    visited.add(v)\n                    q.append(v)\n        \n        return len(visited) == len(nodes)\n\n    def _check_all_complexes_feasibility(self):\n        \"\"\"Pre-computes connectivity feasibility for all complexes.\"\"\"\n        self.is_feasible = [False] * self.num_complexes\n        for c_idx in range(self.num_complexes):\n            required_subunit_indices = np.where(self.stoichiometry[c_idx] > 0)[0]\n            if self._is_connected(required_subunit_indices):\n                self.is_feasible[c_idx] = True\n\n    def _search(self, k, path, supplies):\n        \"\"\"\n        Recursive backtracking search.\n        k: index in the search order.\n        path: list of counts for complexes decided so far.\n        supplies: np.array of remaining subunit supplies.\n        \"\"\"\n        if k == len(self.search_order_indices):\n            # Base case: a complete, feasible assignment is found\n            current_sum = sum(path)\n            if current_sum > self.best_sum:\n                self.best_sum = current_sum\n                self.best_x = list(path)\n            elif current_sum == self.best_sum:\n                # Tie-breaking: choose lexicographically smallest vector\n                if path  self.best_x:\n                    self.best_x = list(path)\n            return\n\n        c_idx = self.search_order_indices[k]\n\n        if not self.is_feasible[c_idx]:\n            self._search(k + 1, path + [0], supplies)\n            return\n\n        s_vec = self.stoichiometry[c_idx]\n        \n        # Calculate max possible count for this complex given current supplies\n        max_count = float('inf')\n        if np.any(s_vec > 0):\n            # Mask to avoid division by zero\n            valid_mask = s_vec > 0\n            # Floor division for all required subunits\n            counts_per_subunit = supplies[valid_mask] // s_vec[valid_mask]\n            max_count = np.min(counts_per_subunit)\n        else:\n            # A complex with no subunits - shouldn't happen in a meaningful problem.\n            # No resources are consumed, but it contributes to the sum.\n            # The problem context implies s_vec is not all-zero for any c.\n            # Setting max_count to 0 is a safe assumption for this problem.\n            max_count = 0\n        \n        # Iterate from max possible count down to 0\n        for count in range(int(max_count), -1, -1):\n            new_supplies = supplies - s_vec * count\n            self._search(k + 1, path + [count], new_supplies)\n\n    def run(self):\n        \"\"\"Main method to run the solver for the configured case.\"\"\"\n        self._check_all_complexes_feasibility()\n        self.best_x = [0] * self.num_complexes\n        self.best_sum = 0\n        \n        self._search(0, [], self.initial_supplies)\n        \n        # The result self.best_x is already in the specified complex_order\n        return self.best_x\n\n\ndef solve():\n    \"\"\"\n    Main function to define test cases and run the solver for each.\n    \"\"\"\n    test_cases = [\n        {\n            'V': {'A', 'B', 'C'},\n            'E': {('A', 'B'), ('B', 'C'), ('A', 'C')},\n            'C': {'X', 'Y'},\n            'S': {\n                'X': {'A': 1, 'B': 1, 'C': 0},\n                'Y': {'A': 0, 'B': 2, 'C': 1}\n            },\n            'n': {'A': 3, 'B': 4, 'C': 2},\n            'complex_order': ['X', 'Y']\n        },\n        {\n            'V': {'A', 'B', 'D'},\n            'E': {('A', 'B')},\n            'C': {'X', 'Z'},\n            'S': {\n                'X': {'A': 1, 'B': 1, 'D': 0},\n                'Z': {'A': 1, 'B': 0, 'D': 1}\n            },\n            'n': {'A': 2, 'B': 2, 'D': 1},\n            'complex_order': ['X', 'Z']\n        },\n        {\n            'V': {'A', 'B', 'C'},\n            'E': {('A', 'B'), ('B', 'C'), ('A', 'C')},\n            'C': {'T', 'W'},\n            'S': {\n                'T': {'A': 1, 'B': 1, 'C': 1},\n                'W': {'A': 2, 'B': 0, 'C': 1}\n            },\n            'n': {'A': 2, 'B': 1, 'C': 1},\n            'complex_order': ['T', 'W']\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        solver = StoichiometrySolver(\n            V=case['V'], \n            E=case['E'], \n            C=case['C'], \n            S=case['S'], \n            n=case['n'], \n            complex_order=case['complex_order']\n        )\n        result = solver.run()\n        all_results.append(result)\n\n    # Print the final result in the exact required format: [[a,b],[c,d],...]\n    print(str(all_results).replace(\" \", \"\"))\n\nsolve()\n```", "id": "3341707"}]}