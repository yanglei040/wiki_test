{"hands_on_practices": [{"introduction": "Before any sophisticated analysis of a network's properties can begin, we must first accurately describe its basic structure. This foundational practice [@problem_id:3299639] addresses the crucial, real-world task of processing raw, potentially noisy network data to compute its degree distribution. By working through the steps of symmetrizing connections, removing self-loops, and constructing the degree histogram, normalized distribution $P(k)$, and complementary cumulative distribution function (CCDF), you will build the essential data-processing pipeline for any empirical network study.", "problem": "You are given adjacency lists representing undirected protein–protein interaction networks, where each key is a protein identifier and its associated list contains neighbor identifiers. The networks may contain repeated neighbor entries, missing reciprocal neighbor entries, and self-loops. Treat each network as an undirected simple graph: the set of nodes is the union of all keys and neighbor identifiers; self-loops must be removed; parallel edges must be collapsed; and adjacency must be symmetrized. From first principles of empirical distributions for discrete random variables, construct the degree histogram, the normalized degree distribution, and the complementary cumulative distribution function, and provide concrete computed outputs.\n\nDefinitions to use:\n- The degree of a node is the number of unique neighbors it is connected to after forming the undirected simple graph.\n- The empirical degree histogram is the count of nodes at each degree.\n- The normalized degree distribution $P(k)$ is the empirical fraction of nodes with degree $k$.\n- The complementary cumulative distribution function $\\bar{F}(k)$ is the probability $\\Pr(K \\ge k)$.\n\nYour program must:\n- Implement precise steps grounded in core definitions to transform each adjacency list into an undirected simple graph, compute node degrees, construct the degree histogram (counts per degree $k$ from $0$ to $k_{\\max}$), compute the normalized distribution $P(k)$, and compute the complementary cumulative distribution function $\\bar{F}(k)$.\n- Justify normalization choices by adhering to the interpretation that selecting a node uniformly at random defines the degree random variable $K$; therefore $P(k)$ must sum to $1$ across all observed degrees, and $\\bar{F}(k)$ must be non-increasing in $k$ with $\\bar{F}(0) = 1$.\n\nTest suite:\n- Case A (mixed connectivity): P1: [P2,P3], P2: [P1,P3,P4], P3: [P1,P2,P5], P4: [P2], P5: [P3,P6], P6: [P5].\n- Case B (hub-and-spoke): P0: [P1,P2,P3,P4,P5], P1: [P0], P2: [P0], P3: [P0], P4: [P0], P5: [P0].\n- Case C (isolated nodes): P1: [], P2: [], P3: [].\n- Case D (noisy list with duplicates, self-loop, and missing symmetry): P1: [P2,P2,P1], P2: [P1], P3: [P4], P4: [].\n\nFor each test case, your program must produce a triple of lists:\n- The degree histogram as a list of integers for $k = 0, 1, \\dots, k_{\\max}$.\n- The normalized distribution $P(k)$ as a list of real numbers for the same range.\n- The complementary cumulative distribution $\\bar{F}(k)$ as a list of real numbers for the same range.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for instance, $[result\\_A,result\\_B,result\\_C,result\\_D]$), where each $result\\_*$ is the triple of lists described above. The final output must be a single line and contain only this bracketed expression. No physical units or angles are involved in this problem. All results must be purely numerical lists derived from the definitions above.", "solution": "The task is to compute the degree histogram, normalized degree distribution, and complementary cumulative distribution function (CCDF) for several raw protein-protein interaction networks. The process is grounded in the first principles of graph theory and empirical probability distributions. It involves two primary phases: first, constructing a canonical undirected simple graph from the provided adjacency lists, and second, performing statistical analysis on the degrees of the nodes in the constructed graph.\n\n### Phase 1: Construction of the Undirected Simple Graph\n\nThe raw input is a set of adjacency lists that may contain inconsistencies such as self-loops, duplicate edges, and asymmetric connections. To perform a valid network analysis, this raw data must be standardized into an undirected simple graph.\n\n1.  **Node Set Identification**: The complete set of nodes, denoted by $V$, is the union of all protein identifiers appearing as keys in the adjacency lists and all identifiers appearing as neighbors in the value lists. Let the input adjacency list be $A_{raw}$. The node set is $V = \\{u \\mid u \\in \\text{keys}(A_{raw})\\} \\cup \\{v \\mid v \\in \\text{values}(A_{raw}[u]) \\text{ for some } u\\}$. The total number of nodes is $N = |V|$.\n\n2.  **Canonical Adjacency Representation**: We construct a final, symmetrical adjacency list, $A_{final}$, that represents the undirected simple graph. A dictionary where keys are nodes and values are sets of neighbors is an ideal data structure, as sets automatically handle duplicate entries.\n    - For every node $u \\in V$, we initialize an empty set of neighbors.\n    - We iterate through each entry $(u, \\text{neighbors}_u)$ in the raw adjacency list $A_{raw}$.\n    - For each neighbor $v \\in \\text{neighbors}_u$, we process the potential edge $(u, v)$:\n        - **Remove Self-Loops**: If $u = v$, the edge is a self-loop and is discarded.\n        - **Symmetrize and Collapse Parallel Edges**: If $u \\ne v$, the edge is valid. We add $v$ to the neighbor set of $u$ and, to ensure symmetry, we add $u$ to the neighbor set of $v$. The use of sets ensures that if the edge $(u, v)$ or $(v, u)$ is encountered multiple times, it is only stored once, effectively collapsing parallel edges.\n\nAfter this process, $A_{final}$ contains a clean, symmetric representation of the network's connectivity. The degree of any node $u$, denoted $k_u$, is then simply the number of unique neighbors in its set, i.e., $k_u = |A_{final}[u]|$.\n\n### Phase 2: Computation of Degree Distributions\n\nWith the node degrees correctly calculated, we can define a discrete random variable $K$ representing the degree of a node chosen uniformly at random from the set $V$. The statistical distributions are empirical estimates derived from the observed degrees of all $N$ nodes.\n\n1.  **Degree Histogram**: The degree histogram, $H(k)$, is a function that maps a degree $k$ to the number of nodes in the network that have that degree.\n    $$H(k) = |\\{u \\in V \\mid k_u = k\\}|$$\n    The histogram is computed for all integer degrees from $k=0$ to the maximum observed degree, $k_{\\max} = \\max_{u \\in V}(k_u)$. The sum of all histogram counts equals the total number of nodes: $\\sum_{k=0}^{k_{\\max}} H(k) = N$.\n\n2.  **Normalized Degree Distribution, $P(k)$**: The normalized degree distribution is the empirical probability mass function (PMF) of the random variable $K$. It gives the fraction of nodes having degree $k$.\n    $$P(k) = \\Pr(K=k) = \\frac{H(k)}{N}$$\n    As a PMF, it must satisfy the normalization condition. The justification follows directly from the definition:\n    $$\\sum_{k=0}^{k_{\\max}} P(k) = \\sum_{k=0}^{k_{\\max}} \\frac{H(k)}{N} = \\frac{1}{N} \\sum_{k=0}^{k_{\\max}} H(k) = \\frac{N}{N} = 1$$\n\n3.  **Complementary Cumulative Distribution Function (CCDF), $\\bar{F}(k)$**: The CCDF, also known as the survival function, gives the probability that a randomly chosen node has a degree of at least $k$.\n    $$\\bar{F}(k) = \\Pr(K \\ge k) = \\sum_{j=k}^{k_{\\max}} P(j)$$\n    This function has two key properties:\n    - It must be non-increasing with $k$, since the set of nodes with degree $\\ge k+1$ is a subset of those with degree $\\ge k$.\n    - It must satisfy $\\bar{F}(0) = 1$, because every node has a degree of $0$ or more. This is shown by $\\bar{F}(0) = \\sum_{j=0}^{k_{\\max}} P(j) = 1$.\n    A computationally efficient method to calculate the CCDF is to use a backward cumulative sum. We start from the tail of the distribution:\n    - $\\bar{F}(k_{\\max}) = P(k_{\\max})$\n    - For $k = k_{\\max}-1, \\dots, 0$, we have $\\bar{F}(k) = \\bar{F}(k+1) + P(k)$.\n\n### Algorithm Synthesis\n\nThe complete algorithm to be implemented is as follows:\n1.  **Input Parsing**: For a given raw adjacency list, determine the complete set of unique node identifiers, $V$.\n2.  **Graph Construction**: Initialize a dictionary of sets, one for each node in $V$. Iterate through the raw adjacency list, adding symmetric edges to the dictionary while ignoring self-loops.\n3.  **Degree Calculation**: Compute a list of degrees, where each element is the size of the neighbor set for a node.\n4.  **Histogram Generation**: Determine the maximum degree, $k_{\\max}$. Create an integer array of size $k_{\\max}+1$, initialized to zeros. Populate this array by counting the occurrences of each degree value.\n5.  **Distribution Calculation**:\n    - Divide the histogram array by the total number of nodes, $N$, to obtain the normalized distribution $P(k)$.\n    - Compute the CCDF, $\\bar{F}(k)$, by performing a reverse cumulative sum on the $P(k)$ array.\n6.  **Output Formatting**: Package the integer histogram, the float $P(k)$ list, and the float $\\bar{F}(k)$ list into a single result tuple for the given network. Repeat for all test cases.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by processing a series of network adjacency lists\n    to compute their degree distributions.\n    \"\"\"\n\n    def process_network(adj_raw: dict) - list:\n        \"\"\"\n        Transforms a raw adjacency list into an undirected simple graph and\n        computes its degree histogram, normalized distribution, and CCDF.\n\n        Args:\n            adj_raw: A dictionary representing a raw adjacency list.\n\n        Returns:\n            A list containing three lists: the degree histogram, the\n            normalized degree distribution P(k), and the CCDF F_bar(k).\n        \"\"\"\n        # Phase 1: Construct the Undirected Simple Graph\n        \n        # 1. Node Set Identification\n        nodes = set(adj_raw.keys())\n        for u in adj_raw:\n            nodes.update(adj_raw[u])\n        \n        num_nodes = len(nodes)\n        \n        if num_nodes == 0:\n            # Handle empty graph case: k_max=0, one node of degree 0 (convention)\n            # This case is not in test suite, but is good practice.\n            # Here we assume at least one node exists as per test cases.\n            # For a graph with no nodes, num_nodes = 0, division by zero occurs.\n            # But test cases imply nodes always exist.\n            # Case C has 3 nodes.\n            return [[0], [0.0], [0.0]] # Or handle as error/special value\n\n        # 2. Canonical Adjacency Representation\n        adj_final = {node: set() for node in nodes}\n        for u, neighbors in adj_raw.items():\n            for v in neighbors:\n                if u != v:  # Ignore self-loops\n                    adj_final[u].add(v)\n                    adj_final[v].add(u) # Symmetrize\n\n        # Phase 2: Compute Degree Distributions\n\n        # 3. Degree Calculation\n        degrees = [len(neighbors) for neighbors in adj_final.values()]\n\n        # 4. Degree Histogram\n        k_max = max(degrees) if degrees else 0\n        histogram = np.zeros(k_max + 1, dtype=int)\n        \n        unique_degrees, counts = np.unique(degrees, return_counts=True)\n        histogram[unique_degrees] = counts\n        \n        # 5. Normalized Distribution P(k) and CCDF F_bar(k)\n        p_k = histogram / num_nodes\n        f_bar_k = np.cumsum(p_k[::-1])[::-1]\n\n        # 6. Ouput Formatting\n        return [\n            histogram.tolist(),\n            p_k.tolist(),\n            f_bar_k.tolist()\n        ]\n\n    test_cases = [\n        # Case A: Mixed connectivity\n        {'P1': ['P2','P3'], 'P2': ['P1','P3','P4'], 'P3': ['P1','P2','P5'], 'P4': ['P2'], 'P5': ['P3','P6'], 'P6': ['P5']},\n        # Case B: Hub-and-spoke\n        {'P0': ['P1','P2','P3','P4','P5'], 'P1': ['P0'], 'P2': ['P0'], 'P3': ['P0'], 'P4': ['P0'], 'P5': ['P0']},\n        # Case C: Isolated nodes\n        {'P1': [], 'P2': [], 'P3': []},\n        # Case D: Noisy list with duplicates, self-loop, and missing symmetry\n        {'P1': ['P2','P2','P1'], 'P2': ['P1'], 'P3': ['P4'], 'P4': []},\n    ]\n\n    results = [process_network(case) for case in test_cases]\n\n    # Manual string formatting to avoid spaces and match problem spec precisely.\n    result_strings = []\n    for res in results:\n        hist_str = f\"[{','.join(map(str, res[0]))}]\"\n        pk_str = f\"[{','.join(map(str, res[1]))}]\"\n        fbar_str = f\"[{','.join(map(str, res[2]))}]\"\n        result_strings.append(f\"[{hist_str},{pk_str},{fbar_str}]\")\n\n    print(f\"[{','.join(result_strings)}]\")\n\nsolve()\n```", "id": "3299639"}, {"introduction": "A straight line on a log-log plot is suggestive of a power law, but rigorous science requires quantitative parameter estimation. This exercise [@problem_id:3299654] guides you through the derivation of the Maximum Likelihood Estimator (MLE) for the power-law exponent $\\gamma$, a cornerstone technique in the field. This practice not only equips you with a fundamental tool for data analysis but also illuminates the critical theoretical distinction between continuous approximations and the more precise discrete models required for integer-valued degree data.", "problem": "In a computational analysis of a large protein–protein interaction network, suppose the degree values observed above a lower cutoff are modeled as coming from a heavy-tailed law, consistent with the scale-free hypothesis. Specifically, let $k_{1},\\dots,k_{n}$ be independent degree samples with $k_{i}\\geq k_{\\min}$, drawn from a continuous Pareto-type tail with probability density function $p(k\\mid \\gamma)$ supported on $[k_{\\min},\\infty)$ for a tail exponent $\\gamma1$. Using only the fundamental principles that (i) the probability density function must normalize to one on its support and (ii) Maximum Likelihood Estimation (MLE) chooses the parameter value that maximizes the likelihood of the observed data under the model, derive from first principles a closed-form expression for the MLE $\\hat{\\gamma}$ of the tail exponent under the continuous model.\n\nThen, explain—without invoking pre-stated estimators—why real network degrees being integers requires a discrete treatment, and how two principled remedies arise: a continuity correction that replaces $k_{\\min}$ by $k_{\\min}-\\frac{1}{2}$ in the continuous derivation, and the exact discrete likelihood for $k\\in\\{k_{\\min},k_{\\min}+1,\\dots\\}$ whose normalization involves a series. Describe the score equation whose root defines the exact discrete MLE and why it typically requires numerical solution.\n\nYour final reported answer must be the derived closed-form expression for the continuous-model $\\hat{\\gamma}$. No rounding is required, and no physical units apply to the final answer.", "solution": "The problem statement will first be subjected to a rigorous validation process.\n\n### Step 1: Extract Givens\n- **Data**: A set of $n$ independent degree samples, $k_{1}, \\dots, k_{n}$.\n- **Constraint**: Each sample satisfies $k_{i} \\geq k_{\\min}$.\n- **Model**: The data are drawn from a continuous Pareto-type tail distribution.\n- **Probability Density Function (PDF)**: The PDF is denoted $p(k\\mid \\gamma)$ and is supported on the interval $[k_{\\min},\\infty)$.\n- **Parameter**: The model has a single parameter, the tail exponent $\\gamma$, with the constraint $\\gamma1$.\n- **Principle (i)**: The PDF must normalize to unity: $\\int_{k_{\\min}}^{\\infty} p(k\\mid \\gamma) \\, dk = 1$.\n- **Principle (ii)**: The Maximum Likelihood Estimation (MLE) method is to be used.\n- **Task 1**: Derive a closed-form expression for the MLE $\\hat{\\gamma}$ of the tail exponent for the continuous model from first principles.\n- **Task 2**: Explain why the discrete nature of network degrees necessitates a discrete statistical model.\n- **Task 3**: Describe two principled remedies for handling discrete data: continuity correction and an exact discrete likelihood function.\n- **Task 4**: Describe the score equation for the exact discrete MLE and explain why it typically requires a numerical solution.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is assessed for validity against the established criteria.\n\n- **Scientific Grounding**: The problem is well-grounded in the fields of statistical physics, network science, and computational biology. The study of power-law or Pareto distributions for network degrees (the \"scale-free\" hypothesis) is a central topic. The use of Maximum Likelihood Estimation is a standard and rigorous statistical method. The distinction between continuous approximations and exact discrete treatments is a crucial and well-understood issue in this domain. The problem is scientifically and mathematically sound.\n- **Well-Posedness**: The problem is well-posed. It provides all necessary information and first principles—normalization of the PDF and the definition of MLE—to derive the required estimator. The structure of the problem leads to a unique and meaningful solution for the continuous estimator.\n- **Objectivity**: The problem is stated in precise, objective, and formal language, free from ambiguity or subjective content.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid** as it is scientifically grounded, well-posed, and objective. There are no identifiable flaws. I will now proceed with a full, reasoned solution.\n\nThe derivation and explanation will proceed in segments as outlined by the problem statement.\n\nFirst, we derive the MLE for the continuous Pareto-type model. The functional form of a Pareto-type tail is $p(k) \\propto k^{-\\gamma}$. We define the probability density function (PDF) for $k \\in [k_{\\min}, \\infty)$ as:\n$$p(k \\mid \\gamma) = C k^{-\\gamma}$$\nwhere $C$ is a normalization constant. According to principle (i), the integral of the PDF over its support must be $1$.\n$$ \\int_{k_{\\min}}^{\\infty} C k^{-\\gamma} \\, dk = 1 $$\nWe compute the integral:\n$$ C \\left[ \\frac{k^{-\\gamma+1}}{-\\gamma+1} \\right]_{k_{\\min}}^{\\infty} = C \\left( \\lim_{k \\to \\infty} \\frac{k^{1-\\gamma}}{1-\\gamma} - \\frac{k_{\\min}^{1-\\gamma}}{1-\\gamma} \\right) = 1 $$\nGiven the constraint that $\\gamma  1$, the exponent $1-\\gamma$ is negative. Therefore, $\\lim_{k \\to \\infty} k^{1-\\gamma} = 0$. The equation becomes:\n$$ C \\left( 0 - \\frac{k_{\\min}^{1-\\gamma}}{1-\\gamma} \\right) = C \\frac{k_{\\min}^{1-\\gamma}}{\\gamma-1} = 1 $$\nSolving for the normalization constant $C$, we find:\n$$ C = (\\gamma-1) k_{\\min}^{\\gamma-1} $$\nThus, the normalized PDF for the continuous Pareto distribution is:\n$$ p(k \\mid \\gamma) = (\\gamma-1) k_{\\min}^{\\gamma-1} k^{-\\gamma} $$\nNext, we apply principle (ii), Maximum Likelihood Estimation. For a set of $n$ independent and identically distributed observations $k_1, k_2, \\dots, k_n$, the likelihood function $L(\\gamma)$ is the product of the individual probabilities:\n$$ L(\\gamma \\mid \\{k_i\\}) = \\prod_{i=1}^{n} p(k_i \\mid \\gamma) = \\prod_{i=1}^{n} \\left( (\\gamma-1) k_{\\min}^{\\gamma-1} k_i^{-\\gamma} \\right) $$\n$$ L(\\gamma) = (\\gamma-1)^n (k_{\\min}^{\\gamma-1})^n \\left( \\prod_{i=1}^{n} k_i \\right)^{-\\gamma} = (\\gamma-1)^n k_{\\min}^{n(\\gamma-1)} \\exp\\left(-\\gamma \\sum_{i=1}^{n} \\ln(k_i)\\right) $$\nTo find the value of $\\gamma$ that maximizes $L(\\gamma)$, it is mathematically more convenient to maximize the log-likelihood function, $\\ell(\\gamma) = \\ln L(\\gamma)$, since the logarithm is a strictly monotonic function.\n$$ \\ell(\\gamma) = \\ln\\left( (\\gamma-1)^n k_{\\min}^{n(\\gamma-1)} \\left( \\prod_{i=1}^{n} k_i \\right)^{-\\gamma} \\right) $$\n$$ \\ell(\\gamma) = n \\ln(\\gamma-1) + n(\\gamma-1)\\ln(k_{\\min}) - \\gamma \\sum_{i=1}^{n} \\ln(k_i) $$\nTo find the maximum, we compute the derivative of $\\ell(\\gamma)$ with respect to $\\gamma$ and set it to zero. This defines the score equation.\n$$ \\frac{d\\ell}{d\\gamma} = \\frac{n}{\\gamma-1} + n\\ln(k_{\\min}) - \\sum_{i=1}^{n} \\ln(k_i) = 0 $$\nLet $\\hat{\\gamma}$ be the value of $\\gamma$ that solves this equation.\n$$ \\frac{n}{\\hat{\\gamma}-1} = \\sum_{i=1}^{n} \\ln(k_i) - n\\ln(k_{\\min}) $$\n$$ \\frac{n}{\\hat{\\gamma}-1} = \\sum_{i=1}^{n} \\left( \\ln(k_i) - \\ln(k_{\\min}) \\right) $$\n$$ \\frac{n}{\\hat{\\gamma}-1} = \\sum_{i=1}^{n} \\ln\\left(\\frac{k_i}{k_{\\min}}\\right) $$\nSolving for $\\hat{\\gamma}-1$:\n$$ \\hat{\\gamma}-1 = \\frac{n}{\\sum_{i=1}^{n} \\ln\\left(\\frac{k_i}{k_{\\min}}\\right)} $$\nFinally, we obtain the closed-form expression for the MLE of the tail exponent:\n$$ \\hat{\\gamma} = 1 + \\frac{n}{\\sum_{i=1}^{n} \\ln\\left(\\frac{k_i}{k_{\\min}}\\right)} $$\nTo confirm that this corresponds to a maximum, we examine the second derivative:\n$$ \\frac{d^2\\ell}{d\\gamma^2} = -\\frac{n}{(\\gamma-1)^2} $$\nSince $n  0$, the second derivative is always negative for $\\gamma1$, confirming that $\\hat{\\gamma}$ is indeed a maximum likelihood estimator.\n\nSecond, we address why the discrete nature of network degrees requires a discrete statistical treatment. Network degrees are, by definition, integers representing a count of connections. A continuous probability density function, such as the Pareto PDF derived above, is defined over a continuous range of real numbers. A fundamental property of any continuous PDF $p(x)$ is that the probability of observing any single specific value $x_0$ is zero, i.e., $P(X=x_0) = \\int_{x_0}^{x_0} p(x)dx = 0$. This creates a direct contradiction with the empirical reality of the data, which consists of specific integer counts. While for large values of degree $k$ the continuous approximation $P(k-0.5 \\leq X  k+0.5) \\approx p(k)$ can be adequate, it is an approximation that can introduce systematic bias, especially for smaller values of $k$ near $k_{\\min}$. A principled statistical model must respect the sample space of the data, which in this case is the set of integers $\\{k_{\\min}, k_{\\min}+1, \\dots\\}$. The proper tool for this is a probability mass function (PMF), not a PDF.\n\nThird, we describe two principled remedies.\n1.  **Continuity Correction**: This is a standard statistical heuristic used to approximate a discrete distribution with a continuous one. The core idea is to treat the integer value $k$ as representing the continuous interval $[k - \\frac{1}{2}, k + \\frac{1}{2})$. Thus, a discrete dataset where each observation $k_i \\geq k_{\\min}$ is conceptually mapped to a continuous problem where samples are drawn from the range $[k_{\\min} - \\frac{1}{2}, \\infty)$. A direct application of this remedy is to replace $k_{\\min}$ with $k_{\\min} - \\frac{1}{2}$ in the continuous derivation. This modifies the lower bound of integration for normalization and consequently adjusts the log-likelihood function. The resulting estimator for $\\hat{\\gamma}$ would take the same form as the continuous one, but with $k_{\\min}$ replaced by $k_{\\min}-1/2$ throughout the expression: $\\hat{\\gamma} = 1 + n \\left( \\sum_{i=1}^{n} \\ln\\left(\\frac{k_i}{k_{\\min}-1/2}\\right) \\right)^{-1}$. This approach is an improvement over the naive continuous model but remains an approximation.\n\n2.  **Exact Discrete Likelihood**: The most rigorous method is to define a discrete power-law distribution directly on the integers. The probability mass function (PMF) is defined as $P(k \\mid \\gamma) = C' k^{-\\gamma}$ for integers $k \\in \\{k_{\\min}, k_{\\min}+1, \\dots\\}$. The normalization constant $C'$ is determined by the condition that the sum of probabilities over the support must be $1$:\n    $$ \\sum_{k=k_{\\min}}^{\\infty} P(k \\mid \\gamma) = C' \\sum_{k=k_{\\min}}^{\\infty} k^{-\\gamma} = 1 $$\n    The infinite series is a special function known as the Hurwitz zeta function, $\\zeta(\\gamma, q) = \\sum_{j=0}^{\\infty} (j+q)^{-\\gamma} = \\sum_{k=q}^{\\infty} k^{-\\gamma}$. Therefore, $C' = 1 / \\zeta(\\gamma, k_{\\min})$, and the exact PMF is:\n    $$ P(k \\mid \\gamma) = \\frac{k^{-\\gamma}}{\\zeta(\\gamma, k_{\\min})} $$\n    The likelihood is then constructed using this PMF.\n\nFourth, we describe the score equation for the exact discrete MLE. The log-likelihood function for the discrete model is:\n$$ \\ell_{\\text{discrete}}(\\gamma) = \\ln \\left( \\prod_{i=1}^n \\frac{k_i^{-\\gamma}}{\\zeta(\\gamma, k_{\\min})} \\right) = \\sum_{i=1}^n \\left( -\\gamma \\ln(k_i) - \\ln(\\zeta(\\gamma, k_{\\min})) \\right) $$\n$$ \\ell_{\\text{discrete}}(\\gamma) = -\\gamma \\sum_{i=1}^n \\ln(k_i) - n \\ln(\\zeta(\\gamma, k_{\\min})) $$\nThe score equation is found by setting the derivative with respect to $\\gamma$ to zero:\n$$ \\frac{d\\ell_{\\text{discrete}}}{d\\gamma} = - \\sum_{i=1}^n \\ln(k_i) - n \\frac{1}{\\zeta(\\gamma, k_{\\min})} \\frac{d\\zeta(\\gamma, k_{\\min})}{d\\gamma} = 0 $$\nThe derivative of the Hurwitz zeta function is $\\frac{d}{d\\gamma} \\zeta(\\gamma, k_{\\min}) = -\\sum_{k=k_{\\min}}^{\\infty} k^{-\\gamma} \\ln(k)$. Substituting this gives the score equation for the discrete MLE $\\hat{\\gamma}_{\\text{discrete}}$:\n$$ \\sum_{i=1}^n \\ln(k_i) - n \\frac{\\sum_{k=k_{\\min}}^{\\infty} k^{-\\hat{\\gamma}_{\\text{discrete}}} \\ln(k)}{\\sum_{k=k_{\\min}}^{\\infty} k^{-\\hat{\\gamma}_{\\text{discrete}}}} = 0 $$\nThis equation is transcendental because it involves the Hurwitz zeta function and its derivative, which are defined by infinite series. There is no general closed-form algebraic solution for $\\hat{\\gamma}_{\\text{discrete}}$. Consequently, its value must be found using numerical root-finding algorithms, such as Newton's method or bisection, to solve the score equation for the specific dataset $\\{k_i\\}$ and parameter $k_{\\min}$.", "answer": "$$\\boxed{1 + \\frac{n}{\\sum_{i=1}^{n} \\ln\\left(\\frac{k_i}{k_{\\min}}\\right)}}$$", "id": "3299654"}, {"introduction": "The power-law exponent $\\gamma$ is more than just a fitting parameter; it has profound consequences for the global architecture of a network. This practice [@problem_id:3299686] connects the abstract exponent to a tangible structural property: the expected size of the network's largest hub. By deriving this relationship and comparing it to the network's structural cutoff, you will gain deep intuition about how $\\gamma$ dictates the emergence of massive hubs and the overall feasibility of embedding a scale-free degree sequence into a simple graph.", "problem": "A protein–protein interaction network in a particular organism is modeled, at first approximation, by the configuration model with degrees that are independent and identically distributed (i.i.d.) from a power-law degree distribution with minimum degree $k_{\\min}$ and exponent $\\gamma$, that is, $P(k) \\propto k^{-\\gamma}$ for $k \\geq k_{\\min}$. Such heavy-tailed degree distributions and their scale-free properties have been widely observed in computational systems biology. Consider a network with $N$ nodes, with $N = 10^{4}$, $k_{\\min} = 2$, and $\\gamma = 2.5$. Starting only from the definition of a power-law distribution and standard probabilistic reasoning for extremes in i.i.d. samples, derive an asymptotic expression for the expected maximum degree as a function of $N$, $k_{\\min}$, and $\\gamma$. Then, adopt the conventional structural cutoff estimate for simple graphs as $k_{s} = N^{1/2}$ (unit prefactor) and compute the ratio $r$ of the expected maximum degree to the structural cutoff. Provide $r$ as a single closed-form analytic expression. Do not round your answer and do not include units.", "solution": "The user wants to solve a problem based on the provided problem statement.\n\n### Step 1: Extract Givens\n- **Network Model**: Configuration model with $N$ nodes.\n- **Node Degrees**: Independent and identically distributed (i.i.d.) random variables.\n- **Degree Distribution**: Power-law, $P(k) \\propto k^{-\\gamma}$ for $k \\geq k_{\\min}$.\n- **Number of Nodes**: $N = 10^4$.\n- **Minimum Degree**: $k_{\\min} = 2$.\n- **Power-law Exponent**: $\\gamma = 2.5$.\n- **Structural Cutoff**: $k_{s} = N^{1/2}$.\n- **Objective**: Derive an asymptotic expression for the expected maximum degree, $\\langle k_{\\max} \\rangle$, and compute the ratio $r = \\frac{\\langle k_{\\max} \\rangle}{k_{s}}$. The final answer for $r$ should be a single closed-form analytic expression.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem is based on standard, well-established concepts in network science, particularly the study of scale-free networks. The configuration model, power-law distributions, and the concept of a structural cutoff are fundamental to this field. The specified parameters are realistic.\n- **Well-Posed**: The problem is clearly stated, with all necessary data and definitions provided to derive a unique solution. The steps required are logical and the quantities to be calculated are well-defined.\n- **Objective**: The language is precise, quantitative, and free of any subjective or ambiguous terminology.\n\nThe problem does not exhibit any of the flaws listed in the validation criteria. It is scientifically sound, formally stated, and self-contained.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\n### Derivation\nThe problem requires the derivation of the expected maximum degree in a network of $N$ nodes, where degrees are i.i.d. random variables drawn from a power-law distribution.\n\nFirst, we define the probability distribution $P(k)$. It is given that $P(k) \\propto k^{-\\gamma}$ for $k \\geq k_{\\min}$. We can write this as $P(k) = Ck^{-\\gamma}$, where $C$ is a normalization constant. To find $C$, we enforce the condition that the sum of probabilities over all possible degrees is equal to $1$.\n$$ \\sum_{k=k_{\\min}}^{\\infty} P(k) = C \\sum_{k=k_{\\min}}^{\\infty} k^{-\\gamma} = 1 $$\nFor large networks and for analytical tractability, we can approximate this discrete sum with an integral. This is a standard and valid approximation for power-law distributions.\n$$ \\int_{k_{\\min}}^{\\infty} C x^{-\\gamma} dx = 1 $$\nWe solve this integral to find $C$:\n$$ C \\left[ \\frac{x^{-\\gamma+1}}{-\\gamma+1} \\right]_{k_{\\min}}^{\\infty} = 1 $$\nSince $\\gamma = 2.5  1$, the term at the upper limit of integration vanishes:\n$$ C \\left( 0 - \\frac{k_{\\min}^{1-\\gamma}}{1-\\gamma} \\right) = 1 \\implies C \\left( \\frac{k_{\\min}^{1-\\gamma}}{\\gamma-1} \\right) = 1 $$\nThis gives the normalization constant:\n$$ C = (\\gamma-1)k_{\\min}^{\\gamma-1} $$\nSo, the probability density function (PDF) for the degree is $p(k) = (\\gamma-1)k_{\\min}^{\\gamma-1} k^{-\\gamma}$ for $k \\geq k_{\\min}$.\n\nNext, we find the complementary cumulative distribution function (CCDF), $P_{}(k)$, which is the probability that a randomly chosen node has a degree greater than $k$.\n$$ P_{}(k) = P(K  k) = \\int_{k}^{\\infty} p(x) dx = \\int_{k}^{\\infty} (\\gamma-1)k_{\\min}^{\\gamma-1} x^{-\\gamma} dx $$\n$$ P_{}(k) = (\\gamma-1)k_{\\min}^{\\gamma-1} \\left[ \\frac{x^{1-\\gamma}}{1-\\gamma} \\right]_{k}^{\\infty} = (\\gamma-1)k_{\\min}^{\\gamma-1} \\left( \\frac{k^{1-\\gamma}}{\\gamma-1} \\right) = k_{\\min}^{\\gamma-1} k^{1-\\gamma} $$\nThus, the CCDF is:\n$$ P_{}(k) = \\left(\\frac{k}{k_{\\min}}\\right)^{1-\\gamma} $$\nThe problem asks for an asymptotic expression for the expected maximum degree, $\\langle k_{\\max} \\rangle$, among $N$ nodes. A standard method in extreme value theory for heavy-tailed distributions is to estimate the maximum value as the point where the expected number of occurrences beyond that value is approximately $1$. Let $k_{\\max}$ be this value. The expected number of nodes with degree greater than $k_{\\max}$ is $N \\cdot P_{}(k_{\\max})$. We set this to $1$:\n$$ N \\cdot P_{}(\\langle k_{\\max} \\rangle) \\approx 1 $$\nSubstituting the expression for the CCDF:\n$$ N \\left(\\frac{\\langle k_{\\max} \\rangle}{k_{\\min}}\\right)^{1-\\gamma} = 1 $$\nNow, we solve for $\\langle k_{\\max} \\rangle$:\n$$ \\left(\\frac{\\langle k_{\\max} \\rangle}{k_{\\min}}\\right)^{1-\\gamma} = \\frac{1}{N} $$\n$$ \\frac{\\langle k_{\\max} \\rangle}{k_{\\min}} = \\left(\\frac{1}{N}\\right)^{\\frac{1}{1-\\gamma}} = N^{-\\frac{1}{1-\\gamma}} = N^{\\frac{1}{\\gamma-1}} $$\nThis gives the asymptotic expression for the expected maximum degree:\n$$ \\langle k_{\\max} \\rangle = k_{\\min} N^{\\frac{1}{\\gamma-1}} $$\nThe problem provides the structural cutoff $k_{s} = N^{1/2}$. We are asked to compute the ratio $r = \\frac{\\langle k_{\\max} \\rangle}{k_{s}}$.\n$$ r = \\frac{k_{\\min} N^{\\frac{1}{\\gamma-1}}}{N^{\\frac{1}{2}}} = k_{\\min} N^{\\left(\\frac{1}{\\gamma-1} - \\frac{1}{2}\\right)} $$\nNow we substitute the given values: $N=10^4$, $k_{\\min}=2$, and $\\gamma=2.5$.\nFirst, let's calculate the exponent of $N$:\n$$ \\frac{1}{\\gamma-1} - \\frac{1}{2} = \\frac{1}{2.5 - 1} - \\frac{1}{2} = \\frac{1}{1.5} - \\frac{1}{2} = \\frac{2}{3} - \\frac{1}{2} = \\frac{4-3}{6} = \\frac{1}{6} $$\nSubstituting this exponent back into the expression for $r$:\n$$ r = k_{\\min} N^{\\frac{1}{6}} $$\nNow, we plug in the values for $k_{\\min}$ and $N$:\n$$ r = 2 \\cdot (10^4)^{\\frac{1}{6}} $$\nSimplifying the expression:\n$$ r = 2 \\cdot 10^{4 \\cdot \\frac{1}{6}} = 2 \\cdot 10^{\\frac{4}{6}} = 2 \\cdot 10^{\\frac{2}{3}} $$\nThis is the required single closed-form analytic expression for the ratio $r$.", "answer": "$$\n\\boxed{2 \\cdot 10^{\\frac{2}{3}}}\n$$", "id": "3299686"}]}