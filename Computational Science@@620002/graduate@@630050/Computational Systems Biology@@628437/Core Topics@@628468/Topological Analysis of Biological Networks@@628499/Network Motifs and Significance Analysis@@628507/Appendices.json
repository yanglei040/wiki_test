{"hands_on_practices": [{"introduction": "Before a network pattern can be declared a \"motif,\" we must demonstrate that it appears more frequently than expected by chance. This requires a well-defined null model to serve as a statistical baseline. This foundational exercise takes you through the first principles of this comparison, asking you to derive the exact expected count and variance of the Feed-Forward Loop within the classic Erdős–Rényi random graph. Mastering these calculations is essential for understanding the statistical underpinnings of the Z-score and appreciating how stochastic effects govern subgraph frequencies in random networks.", "problem": "Consider a directed network of $n$ molecular species generated by the directed Erdős–Rényi (ER) random digraph model, in which for every ordered pair of distinct nodes $(u,v)$ the directed edge $u \\to v$ is present independently with probability $p \\in (0,1)$ and absent with probability $1-p$. In computational systems biology, the Feed-Forward Loop (FFL) is a $3$-node directed triad motif consisting of three distinct nodes $(i,j,k)$ with exactly the three directed edges $i \\to j$, $j \\to k$, and $i \\to k$ present among them and all other directed edges between these three nodes absent. Throughout, interpret the motif count as the number of induced subgraphs on $3$ distinct nodes that are isomorphic to the FFL. The FFL is central to motif significance analysis, and null-model baselines such as the ER model and the Configuration Model (CM) require analytic expressions for its expected abundance and variability.\n\nStarting from first principles appropriate to null-model analysis—namely edge independence in the ER model, indicator random variables for subgraph occurrences, linearity of expectation, and the law of total variance—derive closed-form analytic expressions for:\n- the expected number of FFLs as a function of $n$ and $p$, and\n- the variance of the number of FFLs as a function of $n$ and $p$,\n\nexplicitly accounting for edge orientation and the induced-subgraph constraint. Treat two $3$-node sets that share fewer than $2$ nodes as independent with respect to motif occurrence, and carefully enumerate the dependence structure when two distinct $3$-node sets share exactly $2$ nodes. Assume $n \\geq 3$. Express your final results as closed-form expressions in terms of $n$ and $p$. No numerical approximations are required. Provide your final answer as two expressions in a single row matrix. No units are needed.", "solution": "Let $V$ be the set of $n$ nodes in the network. The total number of Feed-Forward Loops (FFLs), denoted $N_{FFL}$, is the sum of indicator random variables over all distinct $3$-node subsets of $V$. Let $S$ be a subset of $V$ with $|S|=3$. Let $I_S$ be the indicator variable such that $I_S = 1$ if the induced subgraph on $S$ is an FFL, and $I_S = 0$ otherwise. The total count is $N_{FFL} = \\sum_{S \\subset V, |S|=3} I_S$.\n\n### Derivation of the Expected Number of FFLs, $E[N_{FFL}]$\nBy the linearity of expectation, the expected number of FFLs is:\n$$E[N_{FFL}] = E\\left[\\sum_{S \\subset V, |S|=3} I_S\\right] = \\sum_{S \\subset V, |S|=3} E[I_S]$$\nThe number of distinct $3$-node sets is $\\binom{n}{3}$. By symmetry, the expectation $E[I_S]$ is the same for all sets $S$. Let $P_1 = E[I_S] = P(I_S=1)$ for any given set $S = \\{i,j,k\\}$.\nAn induced subgraph on $\\{i,j,k\\}$ is an FFL if the nodes can be assigned roles (source, intermediate, sink) such that the three required directed edges are present and the other three possible directed edges between these nodes are absent. For a specific role assignment (e.g., $i \\to j, j \\to k, i \\to k$), the probability of this specific configuration of $6$ edges is $p^3(1-p)^3$, due to edge independence in the Erdős–Rényi model.\nFor a given set of $3$ nodes, there are $3! = 6$ ways to assign the roles of source, intermediate, and sink. These $6$ assignments correspond to mutually exclusive edge configurations.\nTherefore, the total probability for a set $S$ to form an FFL is:\n$$P_1 = P(I_S=1) = 3! \\times p^3(1-p)^3 = 6p^3(1-p)^3$$\nThe expected number of FFLs is the product of the number of $3$-node sets and the probability $P_1$:\n$$E[N_{FFL}] = \\binom{n}{3} P_1 = \\frac{n(n-1)(n-2)}{6} \\times 6p^3(1-p)^3 = n(n-1)(n-2)p^3(1-p)^3$$\n\n### Derivation of the Variance of the Number of FFLs, $Var(N_{FFL})$\nThe variance of a sum of random variables is given by:\n$$Var(N_{FFL}) = Var\\left(\\sum_S I_S\\right) = \\sum_S Var(I_S) + \\sum_{S \\neq T} Cov(I_S, I_T)$$\nThe first term is the sum of variances of the indicator variables. Since $I_S$ is an indicator, its variance is $Var(I_S) = E[I_S^2] - (E[I_S])^2 = P_1 - P_1^2 = P_1(1-P_1)$.\n$$\\sum_S Var(I_S) = \\binom{n}{3} P_1(1-P_1) = n(n-1)(n-2)p^3(1-p)^3\\left(1 - 6p^3(1-p)^3\\right)$$\nThe second term is the sum of covariances, $Cov(I_S, I_T) = E[I_S I_T] - E[I_S]E[I_T] = P(I_S=1, I_T=1) - P_1^2$. The covariance is non-zero only if the underlying edge sets overlap, which occurs if $|S \\cap T| \\ge 2$. Per the problem, we only consider the case where two distinct sets $S$ and $T$ share exactly $2$ nodes.\nLet $|S \\cap T|=2$. Let $S=\\{a,b,c\\}$ and $T=\\{a,b,d\\}$. We need to calculate $P_2 = E[I_S I_T] = P(I_S=1, I_T=1)$.\nAn FFL on any $3$ nodes requires exactly one directed edge between any pair of nodes within it. Thus, between the shared nodes $a,b$, there must be a single directed edge (either $a \\to b$ or $b \\to a$). Let's condition on the case ($a \\to b$ and $b \\not\\to a$), which has probability $p(1-p)$.\nGiven $a \\to b$, for $S=\\{a,b,c\\}$ to be an FFL, there are $3$ mutually exclusive possibilities for the roles of $(a,b)$:\n1. $(a,b)$ as (source, intermediate): requires $a \\to c, b \\to c$, and no other edges involving $c$. Probability is $p^2(1-p)^2$.\n2. $(a,b)$ as (source, sink): requires $c \\to b, a \\to c$, and no other edges involving $c$. Probability is $p^2(1-p)^2$.\n3. $(a,b)$ as (intermediate, sink): requires $c \\to a, c \\to b$, and no other edges involving $c$. Probability is $p^2(1-p)^2$.\nThe total probability for $S$ to be an FFL, given $a \\to b$, is the sum: $3p^2(1-p)^2$. By conditional independence of edges involving $c$ and $d$, $P(I_S=1, I_T=1 | a \\to b, b \\not\\to a) = (3p^2(1-p)^2)^2 = 9p^4(1-p)^4$.\nUsing the law of total probability (and symmetry for the $b \\to a$ case), we get:\n$$P_2 = 2 \\times [P(a \\to b, b \\not\\to a)] \\times [P(I_S=1, I_T=1 | a \\to b, b \\not\\to a)] = 2 \\times [p(1-p)] \\times [9p^4(1-p)^4] = 18p^5(1-p)^5$$\nNow, we count the number of ordered pairs of sets $(S,T)$ with $|S \\cap T|=2$. This is equivalent to choosing 4 nodes, choosing 2 of them to be the shared pair, and then forming the two sets. The number of such pairs is $12\\binom{n}{4} = \\frac{n(n-1)(n-2)(n-3)}{2}$.\nThe total contribution from the covariance terms is:\n$$\\sum_{S \\neq T, |S \\cap T|=2} Cov(I_S, I_T) = 12\\binom{n}{4} (P_2 - P_1^2)$$\n$$= \\frac{n(n-1)(n-2)(n-3)}{2} [18p^5(1-p)^5 - (6p^3(1-p)^3)^2]$$\n$$= 9n(n-1)(n-2)(n-3) [p^5(1-p)^5 - 2p^6(1-p)^6]$$\n$$= 9n(n-1)(n-2)(n-3)p^5(1-p)^5(1 - 2p(1-p)) = 9n(n-1)(n-2)(n-3)p^5(1-p)^5(1 - 2p + 2p^2)$$\nCombining the two parts of the variance:\n$$Var(N_{FFL}) = \\sum_S Var(I_S) + \\sum_{|S \\cap T|=2} Cov(I_S, I_T)$$\n$$Var(N_{FFL}) = n(n-1)(n-2)p^3(1-p)^3(1-6p^3(1-p)^3) + 9n(n-1)(n-2)(n-3)p^5(1-p)^5(1 - 2p + 2p^2)$$", "answer": "$$ \\boxed{ \\begin{pmatrix} n(n-1)(n-2)p^{3}(1-p)^{3}  n(n-1)(n-2)p^{3}(1-p)^{3}(1-6p^{3}(1-p)^{3}) + 9n(n-1)(n-2)(n-3)p^{5}(1-p)^{5}(1 - 2p + 2p^{2}) \\end{pmatrix} } $$", "id": "3332172"}, {"introduction": "Real-world biological data, such as from gene co-expression studies, often produces weighted networks where edge weights signify interaction strength. However, canonical motif analysis operates on unweighted graphs, posing a critical challenge: how do we select a threshold to binarize the network without introducing artifacts? This practical coding exercise addresses this issue head-on, guiding you to implement a method that identifies an optimal threshold by finding a region where motif significance scores are most stable. This practice is crucial for ensuring that scientific conclusions drawn from motif analysis are robust and not merely an accident of arbitrary parameter choices.", "problem": "Consider a weighted, symmetric, zero-diagonal coexpression matrix $W \\in [0,1]^{n \\times n}$ representing an undirected simple network (no self-loops, no multiedges). Thresholding at cutoff $ \\tau \\in [0,1] $ produces an unweighted adjacency matrix $A(\\tau)$ defined by $A_{ij}(\\tau) = 1$ if $i \\neq j$ and $W_{ij} \\ge \\tau$, otherwise $A_{ij}(\\tau) = 0$. For a fixed $ \\tau $, let $G(\\tau)$ be the corresponding simple undirected graph on $n$ vertices with edge set induced by $A(\\tau)$.\n\nNetwork motifs are small, connected or disconnected subgraph patterns. For three-node induced subgraphs in an undirected simple graph, there are four isomorphism classes determined by the number of edges among the three nodes: $m=0$ (no edges), $m=1$ (exactly one edge), $m=2$ (exactly two edges, an open triad), and $m=3$ (three edges, a triangle). Let $C_m(\\tau)$ denote the count of three-node induced subgraphs of motif class $m$ in $G(\\tau)$, computed over all unordered triplets of distinct vertices.\n\nTo assess motif significance, define a null ensemble consisting of all simple undirected graphs on $n$ vertices having exactly $E(\\tau)$ edges, where $E(\\tau)$ is the number of edges in $G(\\tau)$. Assume the null ensemble is sampled uniformly at random. For each $ \\tau $, compute the motif significance $z$-score for motif $m$,\n$$\nz_m(\\tau) = \\frac{C_m(\\tau) - \\mu_m(\\tau)}{\\sigma_m(\\tau)},\n$$\nwhere $\\mu_m(\\tau)$ and $\\sigma_m(\\tau)$ are, respectively, the mean and standard deviation of $C_m$ under independent samples from the null ensemble with edge count $E(\\tau)$. If $\\sigma_m(\\tau) = 0$, define $z_m(\\tau) = 0$.\n\nDefine a grid of thresholds $ \\{\\tau_k\\}_{k=0}^{K-1} $ with strictly increasing values in $[0,1]$, and approximate the derivative $ \\frac{d}{d\\tau} z_m(\\tau) $ via finite differences. Use the forward difference at the left boundary, the backward difference at the right boundary, and the central difference at internal points:\n- Left boundary ($k=0$): $$ \\left.\\frac{d}{d\\tau} z_m(\\tau)\\right|_{\\tau=\\tau_0} \\approx \\frac{z_m(\\tau_1) - z_m(\\tau_0)}{\\tau_1 - \\tau_0}. $$\n- Right boundary ($k=K-1$): $$ \\left.\\frac{d}{d\\tau} z_m(\\tau)\\right|_{\\tau=\\tau_{K-1}} \\approx \\frac{z_m(\\tau_{K-1}) - z_m(\\tau_{K-2})}{\\tau_{K-1} - \\tau_{K-2}}. $$\n- Internal ($0  k  K-1$): $$ \\left.\\frac{d}{d\\tau} z_m(\\tau)\\right|_{\\tau=\\tau_k} \\approx \\frac{z_m(\\tau_{k+1}) - z_m(\\tau_{k-1})}{\\tau_{k+1} - \\tau_{k-1}}. $$\n\nFor each grid point $\\tau_k$, define the aggregate sensitivity\n$$\nS(\\tau_k) = \\sum_{m \\in \\{0,1,2,3\\}} \\left| \\left.\\frac{d}{d\\tau} z_m(\\tau) \\right|_{\\tau=\\tau_k} \\right|.\n$$\nGiven a window radius $r \\in \\mathbb{Z}_{\\ge 0}$, define the local stability functional\n$$\n\\mathcal{M}(\\tau_k) = \\max_{j \\in \\{ \\max(0,k-r),\\ldots,\\min(K-1,k+r) \\}} S(\\tau_j).\n$$\nLet the optimal threshold $ \\tau^\\ast $ be the grid value minimizing $\\mathcal{M}(\\tau_k)$; in case of ties, select the largest $\\tau_k$ among the minimizers.\n\nYour task is to implement a program that:\n1. For each provided test case, thresholds the given coexpression matrix at the specified grid of cutoffs.\n2. Computes $C_m(\\tau_k)$ for all $m \\in \\{0,1,2,3\\}$ and all $k$, using induced three-node subgraphs.\n3. Constructs the null ensemble by uniformly sampling simple undirected graphs with the same number of edges $E(\\tau_k)$, using the specified number of samples, and computes $z_m(\\tau_k)$.\n4. Approximates derivatives, computes $S(\\tau_k)$ and $\\mathcal{M}(\\tau_k)$, and returns $ \\tau^\\ast $ for each test case following the tie-breaking rule.\n5. Outputs a single line containing the list of $ \\tau^\\ast $ values for all test cases, as a comma-separated list of decimal numbers enclosed in square brackets, rounded to three decimal places (for example, $[0.350,0.650,0.725]$).\n\nUse the following test suite. Each test case provides $(W, \\{\\tau_k\\}_{k=0}^{K-1}, r, R)$, where $r$ is the window radius and $R$ is the number of null samples per grid point.\n\nTest Case $1$ ($n=6$):\n- Matrix $W^{(1)}$ (symmetric, zero diagonal). Entries $W^{(1)}_{ij}$ for $ij$ are:\n  - Within cluster $\\{0,1,2\\}$: $W^{(1)}_{0,1}=0.92$, $W^{(1)}_{0,2}=0.88$, $W^{(1)}_{1,2}=0.90$.\n  - Within cluster $\\{3,4,5\\}$: $W^{(1)}_{3,4}=0.91$, $W^{(1)}_{3,5}=0.87$, $W^{(1)}_{4,5}=0.89$.\n  - Between clusters: $W^{(1)}_{0,3}=0.35$, $W^{(1)}_{0,4}=0.28$, $W^{(1)}_{0,5}=0.40$, $W^{(1)}_{1,3}=0.38$, $W^{(1)}_{1,4}=0.30$, $W^{(1)}_{1,5}=0.42$, $W^{(1)}_{2,3}=0.33$, $W^{(1)}_{2,4}=0.25$, $W^{(1)}_{2,5}=0.37$.\n- Threshold grid: $\\{\\tau_k\\} = \\{0.20, 0.35, 0.50, 0.65, 0.80, 0.90\\}$.\n- Window radius: $r = 1$.\n- Null samples per grid point: $R = 60$.\n\nTest Case $2$ ($n=7$):\n- Matrix $W^{(2)}$ (symmetric, zero diagonal). Entries $W^{(2)}_{ij}$ for $ij$ are:\n  - Cluster $\\{0,1,2\\}$: $W^{(2)}_{0,1}=0.85$, $W^{(2)}_{0,2}=0.90$, $W^{(2)}_{1,2}=0.83$.\n  - Cluster $\\{3,4,5\\}$: $W^{(2)}_{3,4}=0.62$, $W^{(2)}_{3,5}=0.58$, $W^{(2)}_{4,5}=0.60$.\n  - Between clusters: $W^{(2)}_{0,3}=0.50$, $W^{(2)}_{0,4}=0.48$, $W^{(2)}_{0,5}=0.52$, $W^{(2)}_{1,3}=0.47$, $W^{(2)}_{1,4}=0.51$, $W^{(2)}_{1,5}=0.49$, $W^{(2)}_{2,3}=0.53$, $W^{(2)}_{2,4}=0.46$, $W^{(2)}_{2,5}=0.50$.\n  - Node $6$ bridging: $W^{(2)}_{0,6}=0.77$, $W^{(2)}_{1,6}=0.74$, $W^{(2)}_{2,6}=0.79$, $W^{(2)}_{3,6}=0.44$, $W^{(2)}_{4,6}=0.43$, $W^{(2)}_{5,6}=0.42$.\n- Threshold grid: $\\{\\tau_k\\} = \\{0.10, 0.25, 0.40, 0.55, 0.70, 0.85, 0.95\\}$.\n- Window radius: $r = 1$.\n- Null samples per grid point: $R = 50$.\n\nTest Case $3$ ($n=8$):\n- Matrix $W^{(3)}$ (symmetric, zero diagonal). Entries $W^{(3)}_{ij}$ for $ij$ are:\n  - Cluster $\\{0,1,2,3\\}$: $W^{(3)}_{0,1}=0.82$, $W^{(3)}_{0,2}=0.76$, $W^{(3)}_{0,3}=0.80$, $W^{(3)}_{1,2}=0.78$, $W^{(3)}_{1,3}=0.74$, $W^{(3)}_{2,3}=0.81$.\n  - Cluster $\\{4,5,6,7\\}$: $W^{(3)}_{4,5}=0.65$, $W^{(3)}_{4,6}=0.66$, $W^{(3)}_{4,7}=0.64$, $W^{(3)}_{5,6}=0.67$, $W^{(3)}_{5,7}=0.62$, $W^{(3)}_{6,7}=0.68$.\n  - Between clusters: $W^{(3)}_{0,4}=0.48$, $W^{(3)}_{0,5}=0.42$, $W^{(3)}_{0,6}=0.44$, $W^{(3)}_{0,7}=0.37$, $W^{(3)}_{1,4}=0.46$, $W^{(3)}_{1,5}=0.43$, $W^{(3)}_{1,6}=0.45$, $W^{(3)}_{1,7}=0.39$, $W^{(3)}_{2,4}=0.49$, $W^{(3)}_{2,5}=0.41$, $W^{(3)}_{2,6}=0.47$, $W^{(3)}_{2,7}=0.36$, $W^{(3)}_{3,4}=0.50$, $W^{(3)}_{3,5}=0.40$, $W^{(3)}_{3,6}=0.46$, $W^{(3)}_{3,7}=0.35$.\n- Threshold grid: $\\{\\tau_k\\} = \\{0.30, 0.45, 0.60, 0.75\\}$.\n- Window radius: $r = 1$.\n- Null samples per grid point: $R = 80$.\n\nAll computations are dimensionless. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each $ \\tau^\\ast $ rounded to three decimal places, in the order of Test Case $1$, Test Case $2$, and Test Case $3$.", "solution": "The problem requires the implementation of an algorithm to determine an optimal threshold $\\tau^\\ast$ for a given weighted coexpression matrix $W$. The optimization criterion is based on minimizing the local maximum of the sensitivity of motif significance profiles. The method proceeds through several computational stages, which are detailed below.\n\n### I. Problem Validation\n\nThe problem statement has been evaluated and is deemed valid.\n\n1.  **Givens**: The problem provides a weighted, symmetric, zero-diagonal coexpression matrix $W$, a grid of thresholds $\\{\\tau_k\\}$, a window radius $r$, and a number of null samples $R$. It defines the procedure for thresholding $W$ to obtain an adjacency matrix $A(\\tau)$, defines four classes of three-node motifs, specifies the calculation of motif counts $C_m(\\tau)$, defines a null ensemble of random graphs with a fixed edge count, defines the z-score $z_m(\\tau)$ for motif significance, provides finite difference formulas for approximating the derivative of the z-score, and defines the aggregate sensitivity $S(\\tau_k)$ and the local stability functional $\\mathcal{M}(\\tau_k)$. The optimal threshold $\\tau^\\ast$ is defined as the minimizer of $\\mathcal{M}(\\tau_k)$ with a specific tie-breaking rule.\n2.  **Scientific Grounding**: The concepts employed—coexpression networks, thresholding, network motifs, significance analysis using z-scores against a null model, and stability analysis—are standard and well-established within computational systems biology and network science. The methodology is scientifically coherent.\n3.  **Well-Posedness**: The problem is algorithmically well-defined, with all terms and steps specified. The provision of a tie-breaking rule ensures a unique solution $\\tau^\\ast$ for a given input.\n4.  **Objectivity**: The problem is stated in precise, quantitative, and objective language, free from ambiguity or subjective claims.\n\nThe problem is thus a formal and solvable computational task. We may proceed with the solution.\n\n### II. Algorithmic Solution\n\nThe solution is implemented by following the prescribed sequence of steps for each test case provided.\n\n**Step 1: Network Construction at Each Threshold**\n\nFor each threshold $\\tau_k$ in the given grid $\\{\\tau_k\\}_{k=0}^{K-1}$, the weighted matrix $W$ is converted into an unweighted, simple, undirected graph $G(\\tau_k)$. The corresponding adjacency matrix $A(\\tau_k)$ is constructed such that its elements $A_{ij}(\\tau_k)$ are $1$ if the weight $W_{ij} \\ge \\tau_k$ and $i \\neq j$, and $0$ otherwise. The number of vertices $n$ is the dimension of $W$, and the number of edges $E(\\tau_k)$ is calculated as $E(\\tau_k) = \\frac{1}{2}\\sum_{i,j} A_{ij}(\\tau_k)$.\n\n**Step 2: Motif Counting in the Observed Network**\n\nFor each graph $G(\\tau_k)$, we must count the occurrences of the four isomorphism classes of three-node induced subgraphs. These classes are defined by the number of edges $m \\in \\{0, 1, 2, 3\\}$ connecting the three vertices. Let $C_m(\\tau_k)$ be the count for motif class $m$. A direct and robust method for counting is to iterate through all unique unordered triplets of vertices $\\{i, j, k\\}$ and, for each triplet, sum the number of edges in the subgraph induced by them: $e = A_{ij}(\\tau_k) + A_{ik}(\\tau_k) + A_{jk}(\\tau_k)$. This sum $e$ directly corresponds to the motif class $m$. The total number of such triplets is $\\binom{n}{3}$.\n\n**Step 3: Null Model Simulation and Significance Scoring**\n\nTo assess the statistical significance of the observed motif counts, we compare them to the expected counts in a null ensemble. The specified null ensemble consists of all simple undirected graphs with $n$ vertices and $E(\\tau_k)$ edges, sampled uniformly at random.\n\nFor each $\\tau_k$, we perform the following procedure:\n1.  Generate $R$ random graphs, where $R$ is the specified number of null samples. Each random graph is constructed by selecting $E(\\tau_k)$ edges uniformly at random without replacement from the set of all $\\binom{n}{2}$ possible edges.\n2.  For each of the $R$ random graphs, compute the motif counts $C_m^{(rand)}$ for $m \\in \\{0, 1, 2, 3\\}$.\n3.  From the resulting $R$ samples of counts for each motif class $m$, calculate the sample mean $\\mu_m(\\tau_k)$ and the sample standard deviation $\\sigma_m(\\tau_k)$. The use of the sample standard deviation (with Bessel's correction, i.e., division by $R-1$) is appropriate as we are estimating the population parameters from a finite sample.\n4.  The significance of each motif is quantified by its z-score:\n    $$\n    z_m(\\tau_k) = \\frac{C_m(\\tau_k) - \\mu_m(\\tau_k)}{\\sigma_m(\\tau_k)}\n    $$\n    If the standard deviation $\\sigma_m(\\tau_k)$ is zero (which occurs if all random samples yield the same count, e.g., for empty or complete graphs), the z-score is defined to be $z_m(\\tau_k) = 0$.\n\nThis process yields a vector of z-scores $[z_m(\\tau_0), \\dots, z_m(\\tau_{K-1})]$ for each motif class $m$.\n\n**Step 4: Sensitivity and Stability Analysis**\n\nThe next stage of the algorithm analyzes the stability of the motif significance profiles across the threshold grid.\n1.  **Derivative Approximation**: The rate of change of each z-score profile with respect to the threshold, $\\frac{d}{d\\tau} z_m(\\tau)$, is approximated at each grid point $\\tau_k$ using the specified finite difference formulas: forward difference at the start, backward difference at the end, and central difference at interior points.\n2.  **Aggregate Sensitivity**: At each $\\tau_k$, an aggregate sensitivity measure $S(\\tau_k)$ is computed by summing the absolute values of the approximated derivatives over all four motif classes:\n    $$\n    S(\\tau_k) = \\sum_{m=0}^{3} \\left| \\left.\\frac{d}{d\\tau} z_m(\\tau) \\right|_{\\tau=\\tau_k} \\right|\n    $$\n3.  **Local Stability Functional**: To smooth out local fluctuations and identify regions of overall stability, a local stability functional $\\mathcal{M}(\\tau_k)$ is calculated. For each $\\tau_k$, this is the maximum value of $S$ within a window of radius $r$ centered at $k$:\n    $$\n    \\mathcal{M}(\\tau_k) = \\max_{j \\in \\{ \\max(0, k-r), \\dots, \\min(K-1, k+r) \\}} S(\\tau_j)\n    $$\n\n**Step 5: Determination of the Optimal Threshold**\n\nThe optimal threshold $\\tau^\\ast$ is defined as the grid point $\\tau_k$ that minimizes the local stability functional $\\mathcal{M}(\\tau_k)$. This choice favors thresholds located in regions where the significance profile of the network's motif structure is most stable. In the case of multiple thresholds yielding the same minimum value of $\\mathcal{M}$, the tie is broken by selecting the largest such threshold value. This corresponds to the index $k^\\ast = \\max \\{k \\mid \\mathcal{M}(\\tau_k) = \\min_j \\mathcal{M}(\\tau_j)\\}$, and thus $\\tau^\\ast = \\tau_{k^\\ast}$.\n\nThis procedure is executed for each test case, and the resulting $\\tau^\\ast$ values are collected for the final output.", "answer": "```python\nimport numpy as np\nfrom itertools import combinations\n\ndef solve():\n    \"\"\"\n    Solves the optimal threshold problem for the given test cases.\n    \"\"\"\n    \n    # Using a fixed random seed for reproducibility of the null model simulation.\n    np.random.seed(42)\n\n    # Test Case 1\n    W1_upper = {\n        (0,1): 0.92, (0,2): 0.88, (0,3): 0.35, (0,4): 0.28, (0,5): 0.40,\n        (1,2): 0.90, (1,3): 0.38, (1,4): 0.30, (1,5): 0.42,\n        (2,3): 0.33, (2,4): 0.25, (2,5): 0.37,\n        (3,4): 0.91, (3,5): 0.87,\n        (4,5): 0.89\n    }\n    W1 = np.zeros((6, 6))\n    for (i, j), val in W1_upper.items():\n        W1[i, j] = W1[j, i] = val\n    \n    taus1 = np.array([0.20, 0.35, 0.50, 0.65, 0.80, 0.90])\n    r1 = 1\n    R1 = 60\n\n    # Test Case 2\n    W2_upper = {\n        (0,1): 0.85, (0,2): 0.90, (0,3): 0.50, (0,4): 0.48, (0,5): 0.52, (0,6): 0.77,\n        (1,2): 0.83, (1,3): 0.47, (1,4): 0.51, (1,5): 0.49, (1,6): 0.74,\n        (2,3): 0.53, (2,4): 0.46, (2,5): 0.50, (2,6): 0.79,\n        (3,4): 0.62, (3,5): 0.58, (3,6): 0.44,\n        (4,5): 0.60, (4,6): 0.43,\n        (5,6): 0.42\n    }\n    W2 = np.zeros((7, 7))\n    for (i, j), val in W2_upper.items():\n        W2[i, j] = W2[j, i] = val\n\n    taus2 = np.array([0.10, 0.25, 0.40, 0.55, 0.70, 0.85, 0.95])\n    r2 = 1\n    R2 = 50\n\n    # Test Case 3\n    W3_upper = {\n        (0,1): 0.82, (0,2): 0.76, (0,3): 0.80, (0,4): 0.48, (0,5): 0.42, (0,6): 0.44, (0,7): 0.37,\n        (1,2): 0.78, (1,3): 0.74, (1,4): 0.46, (1,5): 0.43, (1,6): 0.45, (1,7): 0.39,\n        (2,3): 0.81, (2,4): 0.49, (2,5): 0.41, (2,6): 0.47, (2,7): 0.36,\n        (3,4): 0.50, (3,5): 0.40, (3,6): 0.46, (3,7): 0.35,\n        (4,5): 0.65, (4,6): 0.66, (4,7): 0.64,\n        (5,6): 0.67, (5,7): 0.62,\n        (6,7): 0.68\n    }\n    W3 = np.zeros((8, 8))\n    for (i, j), val in W3_upper.items():\n        W3[i, j] = W3[j, i] = val\n\n    taus3 = np.array([0.30, 0.45, 0.60, 0.75])\n    r3 = 1\n    R3 = 80\n\n    test_cases = [\n        (W1, taus1, r1, R1),\n        (W2, taus2, r2, R2),\n        (W3, taus3, r3, R3),\n    ]\n\n    results = []\n    \n    for W, taus, r, R in test_cases:\n        tau_star = find_optimal_threshold(W, taus, r, R)\n        results.append(tau_star)\n\n    print(f\"[{','.join(f'{x:.3f}' for x in results)}]\")\n\ndef count_three_node_motifs(A, n):\n    \"\"\"Counts 3-node induced subgraphs based on edge count (0, 1, 2, 3).\"\"\"\n    counts = np.zeros(4, dtype=int)\n    node_triplets = combinations(range(n), 3)\n    for i, j, k in node_triplets:\n        num_edges = A[i, j] + A[i, k] + A[j, k]\n        counts[int(num_edges)] += 1\n    return counts\n\ndef find_optimal_threshold(W, taus, r, R):\n    \"\"\"\n    Main function to find the optimal threshold for a single test case.\n    \"\"\"\n    n = W.shape[0]\n    K = len(taus)\n    z_scores = np.zeros((4, K))\n    \n    possible_edges = list(combinations(range(n), 2))\n    max_edges = len(possible_edges)\n\n    for k, tau in enumerate(taus):\n        A = (W >= tau).astype(int)\n        np.fill_diagonal(A, 0)\n        \n        E = int(np.sum(A) / 2)\n        \n        C_real = count_three_node_motifs(A, n)\n\n        if E == 0 or E == max_edges:\n            z_scores[:, k] = 0.0\n            continue\n\n        C_null_samples = np.zeros((R, 4))\n        for i in range(R):\n            rand_indices = np.random.choice(max_edges, size=E, replace=False)\n            rand_A = np.zeros((n, n), dtype=int)\n            for edge_idx in rand_indices:\n                u, v = possible_edges[edge_idx]\n                rand_A[u, v] = rand_A[v, u] = 1\n            C_null_samples[i, :] = count_three_node_motifs(rand_A, n)\n            \n        mu = np.mean(C_null_samples, axis=0)\n        sigma = np.std(C_null_samples, axis=0, ddof=1) # Sample standard deviation\n        \n        for m in range(4):\n            if sigma[m]  1e-9:\n                z_scores[m, k] = 0.0\n            else:\n                z_scores[m, k] = (C_real[m] - mu[m]) / sigma[m]\n    \n    # Approximate derivatives\n    dzdt = np.zeros_like(z_scores)\n    if K > 1:\n        # Left boundary\n        dzdt[:, 0] = (z_scores[:, 1] - z_scores[:, 0]) / (taus[1] - taus[0])\n        # Right boundary\n        dzdt[:, K-1] = (z_scores[:, K-1] - z_scores[:, K-2]) / (taus[K-1] - taus[K-2])\n        # Internal points\n        for k in range(1, K - 1):\n             dzdt[:, k] = (z_scores[:, k+1] - z_scores[:, k-1]) / (taus[k+1] - taus[k-1])\n    \n    # Aggregate sensitivity S(tau_k)\n    S = np.sum(np.abs(dzdt), axis=0)\n    \n    # Local stability functional M(tau_k)\n    M = np.zeros(K)\n    for k in range(K):\n        j_start = max(0, k - r)\n        j_end = min(K - 1, k + r)\n        M[k] = np.max(S[j_start : j_end + 1])\n        \n    # Find optimal tau*\n    # `np.isclose` is used to handle potential floating point inaccuracies.\n    min_M = np.min(M)\n    min_indices = np.where(np.isclose(M, min_M))[0]\n    \n    # Tie-breaking: select the largest tau_k among the minimizers\n    best_k = np.max(min_indices)\n    tau_star = taus[best_k]\n    \n    return tau_star\n    \nsolve()\n```", "id": "3332155"}, {"introduction": "A key challenge in network science is to determine whether a local pattern, such as a motif, is a genuine feature or simply an inevitable consequence of a larger-scale network organization, like community structure. Simple null models that only preserve the degree sequence may not be sufficient to disentangle these effects. This advanced practice introduces a more sophisticated null model, the Degree-Corrected Stochastic Block Model (DCSBM), to control for both degree heterogeneity and community structure. By calculating residual motif significance after fitting a DCSBM, you will learn to test whether motif enrichments are truly localized phenomena or merely echoes of the network's mesoscale architecture.", "problem": "You are given a set of directed, simple graphs without self-loops, each represented by an adjacency matrix $A \\in \\{0,1\\}^{n \\times n}$ and a block-assignment vector $b \\in \\{0,1,\\dots,B-1\\}^n$. For each graph, consider the Degree-Corrected Stochastic Block Model (DCSBM) in its Poisson formulation, where, conditional on parameters, the directed edges are independent Poisson random variables. The DCSBM specifies the expected edge rate between nodes $i$ and $j$ as $\\lambda_{ij} = \\theta_i^{\\text{out}} \\theta_j^{\\text{in}} \\omega_{r s}$, where $r = b_i$ and $s = b_j$, with block-specific normalizations that render the maximum likelihood estimation identifiable. Fit the DCSBM parameters by maximum likelihood from the observed adjacency matrices, and use the fitted model to form a Bernoulli independence approximation with edge probabilities $p_{ij} = \\mathbb{E}[A_{ij}]$ for $i \\neq j$ and $p_{ii} = 0$.\n\nDefine two directed $3$-node subgraph motifs on ordered triples $(i,j,k)$ of distinct nodes:\n- The feed-forward loop (FFL) motif $m_{\\text{FFL}}$, which occurs on $(i,j,k)$ if and only if $A_{ij} = 1$, $A_{ik} = 1$, and $A_{jk} = 1$.\n- The directed $3$-cycle motif $m_{\\circlearrowleft}$, which occurs on $(i,j,k)$ if and only if $A_{ij} = 1$, $A_{jk} = 1$, and $A_{ki} = 1$.\n\nFor each graph, compute:\n1. The observed counts $C_{\\text{FFL}}$ and $C_{\\circlearrowleft}$ as the total number of ordered triples $(i,j,k)$ satisfying the motif edge conditions above. There are $n \\cdot (n-1) \\cdot (n-1)$ ordered triples if repeated indices were allowed, but because $A_{ii} = 0$ and the products below include $A_{ii}$ when indices repeat, you may sum over all ordered triples $(i,j,k)$ without explicitly enforcing distinctness, provided you explicitly set $A_{ii} = 0$.\n2. The expected counts under the fitted DCSBM with the Bernoulli independence approximation by replacing each $A_{uv}$ with $p_{uv}$ and summing over all ordered triples, namely\n   - $\\mathbb{E}[C_{\\text{FFL}}] = \\sum_{i,j,k} p_{ij} \\, p_{ik} \\, p_{jk}$,\n   - $\\mathbb{E}[C_{\\circlearrowleft}] = \\sum_{i,j,k} p_{ij} \\, p_{jk} \\, p_{ki}$,\n   where in both cases the sums run over all $i,j,k \\in \\{0,1,\\dots,n-1\\}$, and $p_{ii} = 0$.\n3. An approximation to the variances that neglects dependencies arising from overlapping triples in the sparse regime by treating each triple’s motif-indicator as an independent Bernoulli random variable with mean equal to the corresponding product of probabilities:\n   - $\\operatorname{Var}(C_{\\text{FFL}}) \\approx \\sum_{i,j,k} q_{ijk}^{\\text{FFL}} \\, (1 - q_{ijk}^{\\text{FFL}})$ with $q_{ijk}^{\\text{FFL}} = p_{ij} \\, p_{ik} \\, p_{jk}$,\n   - $\\operatorname{Var}(C_{\\circlearrowleft}) \\approx \\sum_{i,j,k} q_{ijk}^{\\circlearrowleft} \\, (1 - q_{ijk}^{\\circlearrowleft})$ with $q_{ijk}^{\\circlearrowleft} = p_{ij} \\, p_{jk} \\, p_{ki}$.\n4. The residual $z$-scores $z_m = \\dfrac{C_m - \\mathbb{E}[C_m]}{\\sqrt{\\operatorname{Var}(C_m)}}$ for $m \\in \\{\\text{FFL}, \\circlearrowleft\\}$. If $\\operatorname{Var}(C_m) = 0$, define $z_m = 0$.\n\nYour goal is to investigate whether motif enrichments can be entirely explained by community assortativity using degree-corrected stochastic block model nulls fitted via maximum likelihood, by computing the residual $z_m$ after accounting for blocks.\n\nUse the following test suite of graphs and block labels. In each case, the adjacency matrix $A^{(t)}$ is given explicitly with rows and columns indexed from $0$ to $n-1$, and all diagonal entries equal to $0$.\n\nTest case $1$ ($n = 6$, $B = 2$):\n- Block labels $b^{(1)} = [\\,0,\\,0,\\,0,\\,1,\\,1,\\,1\\,]$.\n- Adjacency matrix $A^{(1)}$:\n  $$\n  \\begin{bmatrix}\n  0  1  1  1  0  0 \\\\\n  0  0  1  0  0  0 \\\\\n  0  0  0  1  0  0 \\\\\n  1  0  0  0  1  1 \\\\\n  0  0  0  0  0  1 \\\\\n  1  0  0  0  0  0\n  \\end{bmatrix}\n  $$\n\nTest case $2$ ($n = 6$, $B = 2$):\n- Block labels $b^{(2)} = [\\,0,\\,0,\\,0,\\,1,\\,1,\\,1\\,]$.\n- Adjacency matrix $A^{(2)}$:\n  $$\n  \\begin{bmatrix}\n  0  1  0  1  0  0 \\\\\n  0  0  1  1  0  0 \\\\\n  0  0  0  0  1  0 \\\\\n  0  1  0  0  1  0 \\\\\n  0  0  1  0  0  1 \\\\\n  0  0  0  0  0  0\n  \\end{bmatrix}\n  $$\n\nTest case $3$ ($n = 4$, $B = 2$):\n- Block labels $b^{(3)} = [\\,0,\\,0,\\,1,\\,1\\,]$.\n- Adjacency matrix $A^{(3)}$:\n  $$\n  \\begin{bmatrix}\n  0  0  0  0 \\\\\n  0  0  0  0 \\\\\n  0  0  0  0 \\\\\n  0  0  0  0\n  \\end{bmatrix}\n  $$\n\nImplementation details and assumptions to be used uniformly across test cases:\n- Fit the Poisson DCSBM by maximum likelihood and then use $p_{ij} = \\mathbb{E}[A_{ij}]$ under the fitted model for $i \\neq j$ and $p_{ii} = 0$. Use a numerically stable clipping rule $p_{ij} \\leftarrow \\min\\{p_{ij}, 1 - \\varepsilon\\}$ with a small $\\varepsilon  0$ to avoid degenerate probabilities. If any fitted component in the denominator is zero (for example, no outgoing edges from a block), set the corresponding $p_{ij} = 0$.\n- Compute the observed motif counts $C_{\\text{FFL}}$ and $C_{\\circlearrowleft}$ on ordered triples as described above.\n- Approximate the variances by summing $q(1-q)$ over all ordered triples, with $q$ equal to the product of the relevant edge probabilities for that triple, and set the residual $z$-score to $0$ whenever the variance evaluates to $0$.\n\nYour program should produce a single line of output containing the six residual $z$-scores in the order $[\\,z_{\\text{FFL}}^{(1)}, z_{\\circlearrowleft}^{(1)}, z_{\\text{FFL}}^{(2)}, z_{\\circlearrowleft}^{(2)}, z_{\\text{FFL}}^{(3)}, z_{\\circlearrowleft}^{(3)}\\,]$ as a comma-separated list enclosed in square brackets, with each value rounded to exactly $6$ decimal places and no extra spaces. No physical units are involved in this task, and all quantities are pure numbers. Angles are not involved. Percentages are not to be used; all outputs are decimal numbers.", "solution": "The problem requires the computation of residual z-scores for two types of network motifs—the feed-forward loop (FFL) and the 3-cycle—using the Degree-Corrected Stochastic Block Model (DCSBM) as a null model. The procedure for each test case is outlined below.\n\n### Step 1: Maximum Likelihood Estimation for the DCSBM\n\nThe Degree-Corrected Stochastic Block Model (DCSBM), in its Poisson formulation, models the number of edges from node $i$ to node $j$, $A_{ij}$, as a Poisson random variable with an expected rate $\\lambda_{ij}$. This rate is parameterized as:\n$$ \\lambda_{ij} = \\theta_i^{\\text{out}} \\theta_j^{\\text{in}} \\omega_{b_i b_j} $$\nwhere $\\theta_i^{\\text{out}}$ and $\\theta_j^{\\text{in}}$ are degree-correction parameters, and $\\omega_{rs}$ is the affinity between block $r$ and block $s$. The maximum likelihood estimate (MLE) for the expected number of edges, $\\mathbb{E}[A_{ij}]$, is given by:\n$$ \\hat{\\lambda}_{ij} = \\frac{k_i^{\\text{out}} k_j^{\\text{in}} M_{b_i b_j}}{\\kappa_{b_i}^{\\text{out}} \\kappa_{b_j}^{\\text{in}}} $$\nwhere:\n- $k_i^{\\text{out}} = \\sum_j A_{ij}$ is the out-degree of node $i$.\n- $k_j^{\\text{in}} = \\sum_i A_{ij}$ is the in-degree of node $j$.\n- $M_{rs} = \\sum_{i: b_i=r, j: b_j=s} A_{ij}$ is the total number of edges from block $r$ to block $s$.\n- $\\kappa_r^{\\text{out}} = \\sum_{i: b_i=r} k_i^{\\text{out}}$ is the total out-degree of block $r$.\n- $\\kappa_s^{\\text{in}} = \\sum_{j: b_j=s} k_j^{\\text{in}}$ is the total in-degree of block $s$.\n\nIf a denominator term is zero, the corresponding $\\hat{\\lambda}_{ij}$ is set to $0$. We then form a matrix of Bernoulli edge probabilities $P = [p_{ij}]$, where $p_{ij} = \\min\\{\\hat{\\lambda}_{ij}, 1 - \\varepsilon\\}$ for a small $\\varepsilon > 0$, and $p_{ii} = 0$.\n\n### Step 2: Observed Motif Counts\n\nThe observed counts are calculated from the adjacency matrix $A$ by summing over all ordered triples of nodes $(i, j, k)$.\n\n- **Feed-Forward Loop (FFL):** An FFL on $(i,j,k)$ consists of edges $i \\to j$, $i \\to k$, and $j \\to k$. The total count is given by $C_{\\text{FFL}} = \\sum_{i,j,k} A_{ij} A_{ik} A_{jk}$. Using matrix operations, this can be computed as the sum of the Hadamard (element-wise) product of $A$ and $A^T A$:\n$$ C_{\\text{FFL}} = \\sum_{j,k} A_{jk} (A^T A)_{jk} $$\n\n- **Directed 3-Cycle:** A 3-cycle on $(i,j,k)$ consists of edges $i \\to j$, $j \\to k$, and $k \\to i$. The total count is the trace of the matrix $A^3$:\n$$ C_{\\circlearrowleft} = \\sum_{i,j,k} A_{ij} A_{jk} A_{ki} = \\operatorname{Tr}(A^3) $$\n\n### Step 3: Expected Motif Counts\n\nThe expected counts under the fitted DCSBM are computed by replacing the adjacency matrix $A$ with the probability matrix $P$:\n$$ \\mathbb{E}[C_{\\text{FFL}}] = \\sum_{i,j,k} p_{ij} p_{ik} p_{jk} = \\sum_{j,k} p_{jk} (P^T P)_{jk} $$\n$$ \\mathbb{E}[C_{\\circlearrowleft}] = \\sum_{i,j,k} p_{ij} p_{jk} p_{ki} = \\operatorname{Tr}(P^3) $$\n\n### Step 4: Approximate Variances\n\nThe variance is approximated by assuming that the presence of a motif on different triples are independent Bernoulli events. The variance of the sum is the sum of individual variances.\n\n- **FFL Variance:** Let $q_{ijk}^{\\text{FFL}} = p_{ij} p_{ik} p_{jk}$.\n$$ \\operatorname{Var}(C_{\\text{FFL}}) \\approx \\sum_{i,j,k} q_{ijk}^{\\text{FFL}} (1 - q_{ijk}^{\\text{FFL}}) = \\mathbb{E}[C_{\\text{FFL}}] - \\sum_{i,j,k} (q_{ijk}^{\\text{FFL}})^2 $$\nLetting $P_2$ be the matrix of squared probabilities, $(P_2)_{ij} = p_{ij}^2$, the second term is $\\sum_{j,k} (P_2)_{jk} (P_2^T P_2)_{jk}$.\n\n- **3-Cycle Variance:** Let $q_{ijk}^{\\circlearrowleft} = p_{ij} p_{jk} p_{ki}$.\n$$ \\operatorname{Var}(C_{\\circlearrowleft}) \\approx \\sum_{i,j,k} q_{ijk}^{\\circlearrowleft} (1 - q_{ijk}^{\\circlearrowleft}) = \\mathbb{E}[C_{\\circlearrowleft}] - \\sum_{i,j,k} (p_{ij} p_{jk} p_{ki})^2 $$\nThe second term is $\\operatorname{Tr}(P_2^3)$.\n\n### Step 5: Residual Z-Scores\n\nThe final z-score for each motif type $m$ is the standardized difference between the observed and expected counts:\n$$ z_m = \\frac{C_m - \\mathbb{E}[C_m]}{\\sqrt{\\operatorname{Var}(C_m)}} $$\nIf $\\operatorname{Var}(C_m) = 0$, then $z_m$ is defined as $0$. The implementation will apply these steps to each test case.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test cases.\n    It computes the residual z-scores for FFL and 3-cycle motifs\n    using a DCSBM null model.\n    \"\"\"\n\n    # Epsilon for numerical stability in clipping probabilities\n    EPSILON = 1e-10\n\n    test_cases = [\n        # Test case 1\n        (\n            np.array([\n                [0, 1, 1, 1, 0, 0],\n                [0, 0, 1, 0, 0, 0],\n                [0, 0, 0, 1, 0, 0],\n                [1, 0, 0, 0, 1, 1],\n                [0, 0, 0, 0, 0, 1],\n                [1, 0, 0, 0, 0, 0]\n            ]),\n            np.array([0, 0, 0, 1, 1, 1])\n        ),\n        # Test case 2\n        (\n            np.array([\n                [0, 1, 0, 1, 0, 0],\n                [0, 0, 1, 1, 0, 0],\n                [0, 0, 0, 0, 1, 0],\n                [0, 1, 0, 0, 1, 0],\n                [0, 0, 1, 0, 0, 1],\n                [0, 0, 0, 0, 0, 0]\n            ]),\n            np.array([0, 0, 0, 1, 1, 1])\n        ),\n        # Test case 3\n        (\n            np.array([\n                [0, 0, 0, 0],\n                [0, 0, 0, 0],\n                [0, 0, 0, 0],\n                [0, 0, 0, 0]\n            ]),\n            np.array([0, 0, 1, 1])\n        )\n    ]\n\n    all_z_scores = []\n    for A, b in test_cases:\n        z_ffl, z_cyc = compute_z_scores(A, b, EPSILON)\n        all_z_scores.extend([z_ffl, z_cyc])\n\n    print(f\"[{','.join([f'{z:.6f}' for z in all_z_scores])}]\")\n\ndef compute_z_scores(A, b, epsilon):\n    \"\"\"\n    Computes the residual z-scores for FFL and 3-cycle motifs for a single graph.\n    \n    Args:\n        A (np.ndarray): The adjacency matrix of the graph.\n        b (np.ndarray): The block assignment vector for the nodes.\n        epsilon (float): A small value for probability clipping.\n\n    Returns:\n        tuple: A tuple containing the z-scores (z_ffl, z_cyc).\n    \"\"\"\n    n = A.shape[0]\n    if n == 0:\n        return 0.0, 0.0\n        \n    num_blocks = np.max(b) + 1 if b.size > 0 else 0\n\n    # Step 1: Fit DCSBM and get probability matrix P\n    k_out = A.sum(axis=1)\n    k_in = A.sum(axis=0)\n\n    M = np.zeros((num_blocks, num_blocks))\n    for r in range(num_blocks):\n        for s in range(num_blocks):\n            nodes_r = np.where(b == r)[0]\n            nodes_s = np.where(b == s)[0]\n            if nodes_r.size > 0 and nodes_s.size > 0:\n                M[r, s] = A[np.ix_(nodes_r, nodes_s)].sum()\n\n    kappa_out = np.zeros(num_blocks)\n    kappa_in = np.zeros(num_blocks)\n    for r in range(num_blocks):\n        nodes_r = np.where(b == r)[0]\n        if nodes_r.size > 0:\n            kappa_out[r] = k_out[nodes_r].sum()\n            kappa_in[r] = k_in[nodes_r].sum()\n\n    P = np.zeros((n, n), dtype=float)\n    for i in range(n):\n        for j in range(n):\n            if i == j:\n                continue\n            r, s = b[i], b[j]\n            denom = kappa_out[r] * kappa_in[s]\n            if denom > 0:\n                lambda_ij = k_out[i] * k_in[j] * M[r, s] / denom\n                P[i, j] = min(lambda_ij, 1.0 - epsilon)\n\n    # Step 2: Observed Motif Counts\n    C_ffl = np.sum(A * (A.T @ A))\n    C_cyc = np.trace(A @ A @ A)\n\n    # Step 3: Expected Motif Counts\n    E_ffl = np.sum(P * (P.T @ P))\n    E_cyc = np.trace(P @ P @ P)\n\n    # Step 4: Approximate Variances\n    P2 = P**2\n    \n    # FFL Variance\n    sum_q_sq_ffl = np.sum(P2 * (P2.T @ P2))\n    var_ffl = E_ffl - sum_q_sq_ffl\n    \n    # 3-Cycle Variance\n    if n > 0:\n        P2_cubed = P2 @ P2 @ P2\n        sum_q_sq_cyc = np.trace(P2_cubed)\n    else:\n        sum_q_sq_cyc = 0\n    var_cyc = E_cyc - sum_q_sq_cyc\n    \n    # Prevent negative variance due to numerical precision issues\n    var_ffl = max(0, var_ffl)\n    var_cyc = max(0, var_cyc)\n\n    # Step 5: Residual Z-Scores\n    z_ffl = 0.0\n    if var_ffl > 0:\n        z_ffl = (C_ffl - E_ffl) / np.sqrt(var_ffl)\n        \n    z_cyc = 0.0\n    if var_cyc > 0:\n        z_cyc = (C_cyc - E_cyc) / np.sqrt(var_cyc)\n\n    return z_ffl, z_cyc\n\nsolve()\n```", "id": "3332201"}]}