{"hands_on_practices": [{"introduction": "The likelihood of observing a specific stochastic trajectory is a cornerstone of statistical inference for biochemical networks. This probability is fundamentally determined by the integrated hazard over the path's duration. This first exercise provides essential practice in calculating this integral for a piecewise-constant trajectory generated by a Continuous-Time Markov Chain, a foundational skill for building likelihood-based estimators [@problem_id:3303233].", "problem": "In computational systems biology, the stochastic dynamics of a biochemical reaction network can be modeled as a Continuous-Time Markov Chain (CTMC), where the system state $X_t \\in \\mathbb{N}$ is piecewise constant and changes by jumps at random times. The instantaneous total event rate (also called the hazard or intensity) $\\lambda(X_t;\\theta)$ governs the distribution of waiting times and jump events. Consider a single-species network with three reactions: constitutive synthesis, first-order decay, and bimolecular annihilation. The total hazard is modeled as\n$$\n\\lambda(x;\\theta) \\;=\\; \\theta_1 \\;+\\; \\theta_2\\, x \\;+\\; \\theta_3 \\binom{x}{2},\n$$\nwhere $\\theta = (\\theta_1,\\theta_2,\\theta_3)$ are unknown parameters to be estimated from observed trajectories, and $\\binom{x}{2} = \\frac{x(x-1)}{2}$.\n\nA realized jump trajectory on the interval $[0,T]$ with $T = 6.0$ is observed, with jump times $t_0 = 0$, $t_1 = 0.5$, $t_2 = 1.1$, $t_3 = 2.0$, $t_4 = 4.0$, and $t_5 = 6.0$, and with piecewise-constant states $X_s = x_k$ for $s \\in [t_k, t_{k+1})$ given by\n$$\nx_0 = 3,\\quad x_1 = 4,\\quad x_2 = 2,\\quad x_3 = 2,\\quad x_4 = 5.\n$$\n\nStarting from the fundamental definition of the survival function for a time-inhomogeneous jump process and the role of the integrated hazard $\\int_0^T \\lambda(X_s;\\theta)\\,ds$ in the path distribution, derive how the integral reduces under the piecewise-constant property of $X_s$ along the trajectory. Then, implement this reduction for the given trajectory, and simplify the resulting expression to a single closed-form analytic expression in $\\theta_1$, $\\theta_2$, and $\\theta_3$.\n\nYour final answer must be a single analytic expression. No rounding is required, and no physical units are involved.", "solution": "We begin from the standard construction of a Continuous-Time Markov Chain (CTMC) path distribution via its hazard (intensity) function. For a time-inhomogeneous jump process with instantaneous total rate $\\lambda(X_t;\\theta)$, the probability that no event occurs in the interval $[0,t]$ conditional on the trajectory of $X_s$ is\n$$\n\\Pr(\\text{no jump in }[0,t] \\mid \\{X_s\\}_{s \\in [0,t]}) \\;=\\; \\exp\\!\\left(-\\int_0^t \\lambda(X_s;\\theta)\\,ds\\right).\n$$\nThis formula follows from the fundamental definition of the survival function for a non-homogeneous Poisson process (NHPP), generalized to a state-dependent intensity through the CTMC framework. The integral $\\int_0^T \\lambda(X_s;\\theta)\\,ds$ is thus the integrated hazard that enters the path likelihood.\n\nIn a CTMC on a countable state space, the sample paths are piecewise constant with jumps at isolated times. Let the jump times be $0 = t_0 < t_1 < \\dots < t_m < t_{m+1} = T$, and the state be constant between jumps: $X_s = x_k$ for $s \\in [t_k, t_{k+1})$, $k = 0,\\dots,m$. Under this piecewise-constant property, the integrand $\\lambda(X_s;\\theta)$ is constant on each interval $[t_k, t_{k+1})$, equal to $\\lambda(x_k;\\theta)$. Therefore, the integral decomposes as a sum over intervals:\n$$\n\\int_0^T \\lambda(X_s;\\theta)\\,ds\n\\;=\\; \\sum_{k=0}^{m} \\int_{t_k}^{t_{k+1}} \\lambda(x_k;\\theta)\\,ds\n\\;=\\; \\sum_{k=0}^{m} \\lambda(x_k;\\theta)\\,(t_{k+1}-t_k).\n$$\nThis reduction is a direct consequence of the fundamental property of Riemann integration for step functions.\n\nWe now implement this reduction for the given trajectory. The trajectory is specified by $t_0 = 0$, $t_1 = 0.5$, $t_2 = 1.1$, $t_3 = 2.0$, $t_4 = 4.0$, $t_5 = 6.0$ and $x_0 = 3$, $x_1 = 4$, $x_2 = 2$, $x_3 = 2$, $x_4 = 5$. There are $m = 4$ jumps and $m+1 = 5$ intervals. The interval durations are\n$$\n\\Delta_0 = t_1 - t_0 = 0.5,\\quad\n\\Delta_1 = t_2 - t_1 = 0.6,\\quad\n\\Delta_2 = t_3 - t_2 = 0.9,\\quad\n\\Delta_3 = t_4 - t_3 = 2.0,\\quad\n\\Delta_4 = t_5 - t_4 = 2.0,\n$$\nand these sum to $T = 6.0$ as a consistency check.\n\nThe total hazard is\n$$\n\\lambda(x;\\theta) \\;=\\; \\theta_1 \\;+\\; \\theta_2\\, x \\;+\\; \\theta_3 \\binom{x}{2}\n\\;=\\; \\theta_1 \\;+\\; \\theta_2\\, x \\;+\\; \\theta_3 \\,\\frac{x(x-1)}{2}.\n$$\nWe evaluate $\\lambda(x_k;\\theta)$ for each $x_k$:\n- For $x_0 = 3$,\n$$\n\\lambda(3;\\theta) \\;=\\; \\theta_1 \\;+\\; 3\\theta_2 \\;+\\; \\theta_3 \\binom{3}{2}\n\\;=\\; \\theta_1 \\;+\\; 3\\theta_2 \\;+\\; 3\\theta_3.\n$$\n- For $x_1 = 4$,\n$$\n\\lambda(4;\\theta) \\;=\\; \\theta_1 \\;+\\; 4\\theta_2 \\;+\\; \\theta_3 \\binom{4}{2}\n\\;=\\; \\theta_1 \\;+\\; 4\\theta_2 \\;+\\; 6\\theta_3.\n$$\n- For $x_2 = 2$,\n$$\n\\lambda(2;\\theta) \\;=\\; \\theta_1 \\;+\\; 2\\theta_2 \\;+\\; \\theta_3 \\binom{2}{2}\n\\;=\\; \\theta_1 \\;+\\; 2\\theta_2 \\;+\\; \\theta_3.\n$$\n- For $x_3 = 2$,\n$$\n\\lambda(2;\\theta) \\;=\\; \\theta_1 \\;+\\; 2\\theta_2 \\;+\\; \\theta_3.\n$$\n- For $x_4 = 5$,\n$$\n\\lambda(5;\\theta) \\;=\\; \\theta_1 \\;+\\; 5\\theta_2 \\;+\\; \\theta_3 \\binom{5}{2}\n\\;=\\; \\theta_1 \\;+\\; 5\\theta_2 \\;+\\; 10\\theta_3.\n$$\n\nMultiplying by the corresponding durations and summing yields the integrated hazard:\n$$\n\\int_0^{6.0} \\lambda(X_s;\\theta)\\,ds\n\\;=\\; 0.5\\big(\\theta_1 + 3\\theta_2 + 3\\theta_3\\big)\n\\;+\\; 0.6\\big(\\theta_1 + 4\\theta_2 + 6\\theta_3\\big)\n\\;+\\; 0.9\\big(\\theta_1 + 2\\theta_2 + \\theta_3\\big)\n\\;+\\; 2.0\\big(\\theta_1 + 2\\theta_2 + \\theta_3\\big)\n\\;+\\; 2.0\\big(\\theta_1 + 5\\theta_2 + 10\\theta_3\\big).\n$$\nWe collect terms by parameter:\n- Coefficient of $\\theta_1$:\n$$\n0.5 + 0.6 + 0.9 + 2.0 + 2.0 \\;=\\; 6.0.\n$$\n- Coefficient of $\\theta_2$:\n$$\n0.5\\cdot 3 + 0.6\\cdot 4 + 0.9\\cdot 2 + 2.0\\cdot 2 + 2.0\\cdot 5\n\\;=\\; 1.5 + 2.4 + 1.8 + 4.0 + 10.0 \\;=\\; 19.7 \\;=\\; \\frac{197}{10}.\n$$\n- Coefficient of $\\theta_3$:\n$$\n0.5\\cdot 3 + 0.6\\cdot 6 + 0.9\\cdot 1 + 2.0\\cdot 1 + 2.0\\cdot 10\n\\;=\\; 1.5 + 3.6 + 0.9 + 2.0 + 20.0 \\;=\\; 28.0 \\;=\\; 28.\n$$\n\nThus, the integrated hazard simplifies to the closed-form expression\n$$\n\\int_0^{6.0} \\lambda(X_s;\\theta)\\,ds \\;=\\; 6\\,\\theta_1 \\;+\\; \\frac{197}{10}\\,\\theta_2 \\;+\\; 28\\,\\theta_3.\n$$\nThis expression implements the integral term by summing the hazard evaluated at the piecewise-constant states, weighted by the interval durations, as required for distributional analysis of stochastic trajectories in a CTMC.", "answer": "$$\\boxed{6\\theta_1+\\frac{197}{10}\\theta_2+28\\theta_3}$$", "id": "3303233"}, {"introduction": "When we analyze long trajectories, we often assume the system has reached a statistical equilibrium, or stationary distribution. This practice delves into the theoretical underpinnings of this assumption by connecting the spectral gap of a process to its mixing time. Deriving this bound is crucial for understanding how quickly a system converges to stationarity, which informs the necessary length of simulations for reliable statistical analysis [@problem_id:3303209].", "problem": "A common model for stochastic gene expression dynamics in computational systems biology is a continuous-time, reversible birth–death Markov chain on a finite state space $\\{0,1,\\ldots,N\\}$ with reflecting boundaries, representing, for instance, the count of a molecular species undergoing synthesis and degradation. Let the chain be irreducible with unique stationary distribution $\\pi$, generator $Q$, and transition semigroup $T_t = \\exp(t Q)$. Denote the spectral gap by $\\lambda_1 > 0$, defined as the smallest positive eigenvalue of $-Q$ acting on functions with mean zero under $\\pi$. Let the total variation (TV) mixing time to $\\epsilon$ be defined by\n$$\nt_{\\mathrm{mix}}(\\epsilon) \\equiv \\inf\\left\\{t \\ge 0 : \\sup_{x \\in \\{0,1,\\ldots,N\\}} \\left\\|T_t(x,\\cdot) - \\pi\\right\\|_{\\mathrm{TV}} \\le \\epsilon \\right\\},\n$$\nwhere for probability measures $\\mu$ and $\\nu$ on $\\{0,1,\\ldots,N\\}$,\n$$\n\\|\\mu - \\nu\\|_{\\mathrm{TV}} \\equiv \\frac{1}{2} \\sum_{i=0}^{N} \\left|\\mu(i) - \\nu(i)\\right|.\n$$\nIn distributional analysis of stochastic trajectories, a key object is the autocorrelation of observables along the stationary trajectory, which for reversible dynamics admits a spectral decomposition where the slowest decaying mode is controlled by $\\lambda_1$. Using only the foundational properties of reversible continuous-time Markov chains, the definition of total variation distance through its dual characterization, and the spectral characterization of $T_t$ via eigenfunctions and eigenvalues, derive a universal lower bound on $t_{\\mathrm{mix}}(\\epsilon)$ that depends only on $\\lambda_1$ and $\\epsilon$. Then, briefly relate this bound to the decorrelation time scale of the slowest mode in trajectory space. \n\nExpress your final lower bound as a single closed-form expression in terms of $\\lambda_1$ and $\\epsilon$. No numerical rounding is required and no units should be included in the final expression.", "solution": "The objective is to derive a universal lower bound on the total variation mixing time, $t_{\\mathrm{mix}}(\\epsilon)$, for a reversible continuous-time Markov chain. The bound should depend only on the spectral gap, $\\lambda_1$, and the mixing threshold, $\\epsilon$. The problem specifies that the derivation should rely on the dual characterization of the total variation distance and the spectral properties of the transition semigroup $T_t$.\n\nLet the state space be $\\mathcal{S} = \\{0, 1, \\ldots, N\\}$. The total variation distance between two probability measures $\\mu$ and $\\nu$ on $\\mathcal{S}$ is given by\n$$\n\\|\\mu - \\nu\\|_{\\mathrm{TV}} = \\frac{1}{2} \\sum_{i=0}^{N} |\\mu(i) - \\nu(i)|.\n$$\nA key tool for connecting this combinatorial definition to spectral theory is the dual characterization of the total variation norm, which states:\n$$\n\\|\\mu - \\nu\\|_{\\mathrm{TV}} = \\sup_{f: \\|\\cdot\\|_{\\infty} \\le 1} \\frac{1}{2} \\left| \\mathbb{E}_{\\mu}[f] - \\mathbb{E}_{\\nu}[f] \\right|,\n$$\nwhere the supremum is taken over all real-valued functions $f$ on $\\mathcal{S}$ with infinity norm $\\|f\\|_{\\infty} = \\sup_{i \\in \\mathcal{S}} |f(i)| \\le 1$.\n\nThe definition of the mixing time is\n$$\nt_{\\mathrm{mix}}(\\epsilon) \\equiv \\inf \\left\\{ t \\ge 0 : d(t) \\le \\epsilon \\right\\},\n$$\nwhere $d(t) \\equiv \\sup_{x \\in \\mathcal{S}} \\|T_t(x, \\cdot) - \\pi\\|_{\\mathrm{TV}}$. Here, $T_t(x, \\cdot)$ is the probability distribution of the chain at time $t$ having started from state $x$, and $\\pi$ is the unique stationary distribution.\n\nLet us apply the dual characterization to the distance $d(t)$:\n$$\n\\|T_t(x, \\cdot) - \\pi\\|_{\\mathrm{TV}} = \\sup_{f: \\|f\\|_{\\infty} \\le 1} \\frac{1}{2} \\left| \\mathbb{E}_{T_t(x, \\cdot)}[f] - \\mathbb{E}_{\\pi}[f] \\right|.\n$$\nThe expectation $\\mathbb{E}_{T_t(x, \\cdot)}[f]$ is the expectation of $f$ when the state is distributed according to $T_t(x, \\cdot)$. This is precisely the action of the semigroup $T_t$ on the function $f$, evaluated at the starting state $x$:\n$$\n\\mathbb{E}_{T_t(x, \\cdot)}[f] = \\sum_{i=0}^{N} f(i) T_t(x, i) = (T_t f)(x).\n$$\nTherefore, we can write the distance from a starting state $x$ as:\n$$\n\\|T_t(x, \\cdot) - \\pi\\|_{\\mathrm{TV}} = \\sup_{f: \\|f\\|_{\\infty} \\le 1} \\frac{1}{2} \\left| (T_t f)(x) - \\mathbb{E}_{\\pi}[f] \\right|.\n$$\nTo find a lower bound for $d(t) = \\sup_{x \\in \\mathcal{S}} \\|T_t(x, \\cdot) - \\pi\\|_{\\mathrm{TV}}$, it is sufficient to choose a specific, well-behaved test function $f$ and evaluate the expression, as the supremum will be greater than or equal to the value for any particular choice of $f$.\n\nThe chain is reversible, so the generator $Q$ is self-adjoint in the Hilbert space $L^2(\\pi)$ of functions with inner product $\\langle g, h \\rangle_\\pi = \\sum_{i=0}^N g(i) h(i) \\pi(i)$. Consequently, the operator $-Q$ has real, non-negative eigenvalues $0 = \\lambda_0 < \\lambda_1 \\le \\lambda_2 \\dots \\le \\lambda_N$. Let $\\phi_0, \\phi_1, \\ldots, \\phi_N$ be the corresponding orthonormal eigenfunctions. The eigenfunction for $\\lambda_0=0$ is the constant function $\\phi_0(i) = 1$. The spectral gap is $\\lambda_1 > 0$. The action of the semigroup $T_t = \\exp(tQ)$ on an eigenfunction $\\phi_k$ is $T_t \\phi_k = \\exp(-t\\lambda_k) \\phi_k$.\n\nThe slowest decaying non-stationary mode of the system corresponds to the eigenfunction $\\phi_1$ associated with the spectral gap $\\lambda_1$. This makes $\\phi_1$ an excellent candidate for a test function. The eigenfunction $\\phi_1$ is orthogonal to the constant function $\\phi_0$ in $L^2(\\pi)$:\n$$\n\\langle \\phi_1, \\phi_0 \\rangle_\\pi = \\sum_{i=0}^N \\phi_1(i) \\cdot 1 \\cdot \\pi(i) = \\mathbb{E}_{\\pi}[\\phi_1] = 0.\n$$\nThe test function $f$ must satisfy $\\|f\\|_{\\infty} \\le 1$. The eigenfunction $\\phi_1$ is normalized in $L^2(\\pi)$ (i.e., $\\|\\phi_1\\|_{L^2(\\pi)}=1$), but not necessarily in the infinity norm. Let us define a suitable test function $f_1$ by normalizing $\\phi_1$ by its infinity norm:\n$$\nf_1 = \\frac{\\phi_1}{\\|\\phi_1\\|_{\\infty}}.\n$$\nThis function satisfies $\\|f_1\\|_{\\infty} = 1$ and, since $\\mathbb{E}_{\\pi}[\\phi_1]=0$, we also have $\\mathbb{E}_{\\pi}[f_1] = 0$.\n\nSubstituting $f_1$ into the dual formulation provides a lower bound for the TV distance:\n$$\n\\|T_t(x, \\cdot) - \\pi\\|_{\\mathrm{TV}} \\ge \\frac{1}{2} \\left| (T_t f_1)(x) - \\mathbb{E}_{\\pi}[f_1] \\right| = \\frac{1}{2} \\left| T_t \\left( \\frac{\\phi_1}{\\|\\phi_1\\|_{\\infty}} \\right) (x) - 0 \\right|.\n$$\nUsing the linearity of $T_t$ and its action on $\\phi_1$:\n$$\n\\|T_t(x, \\cdot) - \\pi\\|_{\\mathrm{TV}} \\ge \\frac{1}{2\\|\\phi_1\\|_{\\infty}} |(T_t \\phi_1)(x)| = \\frac{1}{2\\|\\phi_1\\|_{\\infty}} |\\exp(-t\\lambda_1) \\phi_1(x)| = \\frac{\\exp(-t\\lambda_1) |\\phi_1(x)|}{2\\|\\phi_1\\|_{\\infty}}.\n$$\nThis inequality holds for any starting state $x$. To get a lower bound for $d(t)$, we take the supremum over all $x \\in \\mathcal{S}$:\n$$\nd(t) = \\sup_{x \\in \\mathcal{S}} \\|T_t(x, \\cdot) - \\pi\\|_{\\mathrm{TV}} \\ge \\sup_{x \\in \\mathcal{S}} \\left( \\frac{\\exp(-t\\lambda_1) |\\phi_1(x)|}{2\\|\\phi_1\\|_{\\infty}} \\right).\n$$\nThe supremum of $|\\phi_1(x)|$ over $x$ is by definition $\\|\\phi_1\\|_{\\infty}$. Thus, we obtain:\n$$\nd(t) \\ge \\frac{\\exp(-t\\lambda_1)}{2\\|\\phi_1\\|_{\\infty}} \\sup_{x \\in \\mathcal{S}} |\\phi_1(x)| = \\frac{\\exp(-t\\lambda_1)}{2\\|\\phi_1\\|_{\\infty}} \\|\\phi_1\\|_{\\infty} = \\frac{1}{2}\\exp(-t\\lambda_1).\n$$\nWe have now established a universal lower bound on the decay of the total variation distance.\n\nBy definition, at $t = t_{\\mathrm{mix}}(\\epsilon)$, we have $d(t_{\\mathrm{mix}}(\\epsilon)) \\le \\epsilon$. Combining this with our lower bound gives:\n$$\n\\frac{1}{2}\\exp(-\\lambda_1 t_{\\mathrm{mix}}(\\epsilon)) \\le d(t_{\\mathrm{mix}}(\\epsilon)) \\le \\epsilon.\n$$\nRearranging this inequality to solve for $t_{\\mathrm{mix}}(\\epsilon)$:\n$$\n\\exp(-\\lambda_1 t_{\\mathrm{mix}}(\\epsilon)) \\le 2\\epsilon\n$$\n$$\n-\\lambda_1 t_{\\mathrm{mix}}(\\epsilon) \\le \\ln(2\\epsilon)\n$$\n$$\n\\lambda_1 t_{\\mathrm{mix}}(\\epsilon) \\ge -\\ln(2\\epsilon) = \\ln\\left(\\frac{1}{2\\epsilon}\\right).\n$$\nThis yields the final lower bound on the mixing time:\n$$\nt_{\\mathrm{mix}}(\\epsilon) \\ge \\frac{1}{\\lambda_1} \\ln\\left(\\frac{1}{2\\epsilon}\\right).\n$$\nThis inequality holds for any $\\epsilon \\in (0, 1)$. For $\\epsilon \\ge 1/2$, the right-hand side is non-positive, which is a trivial bound since time is non-negative.\n\nTo relate this bound to the decorrelation time scale of the slowest mode, we consider the stationary autocorrelation function of an observable $g$. For a mean-zero observable, $\\mathbb{E}_\\pi[g]=0$, this is $C_g(t) = \\mathbb{E}_\\pi[g(X_0) g(X_t)] = \\langle g, T_t g \\rangle_\\pi$. The slowest mode corresponds to the eigenfunction $g=\\phi_1$. Its autocorrelation function is $C_{\\phi_1}(t) = \\langle \\phi_1, T_t \\phi_1 \\rangle_\\pi = \\langle \\phi_1, \\exp(-t\\lambda_1) \\phi_1 \\rangle_\\pi = \\exp(-t\\lambda_1) \\|\\phi_1\\|^2_{L^2(\\pi)} = \\exp(-t\\lambda_1)$. The characteristic time scale $\\tau_1$ for this exponential decay is the time required for the correlation to fall by a factor of $1/e$, which is $\\tau_1 = 1/\\lambda_1$. The lower bound on the mixing time can therefore be expressed as $t_{\\mathrm{mix}}(\\epsilon) \\ge \\tau_1 \\ln\\left(\\frac{1}{2\\epsilon}\\right)$. This shows that the mixing time is fundamentally limited by the decorrelation time of the slowest dynamical mode in the system, scaled by a logarithmic factor depending on the desired proximity to stationarity.", "answer": "$$\n\\boxed{\\frac{1}{\\lambda_1} \\ln\\left(\\frac{1}{2\\epsilon}\\right)}\n$$", "id": "3303209"}, {"introduction": "Building upon the concepts of path likelihood and stationarity, this final practice demonstrates how to perform robust statistical inference from a single, long simulation. You will implement an estimator for the relative entropy rate—a key measure for comparing statistical models—and its asymptotic variance using the batch means method. This comprehensive exercise brings together theory and computation to tackle a realistic problem in model selection and uncertainty quantification [@problem_id:3303203].", "problem": "You are given a continuous-time, finite-state chemical reaction network modeled as a continuous-time Markov chain (CTMC). The network is simulated by the Stochastic Simulation Algorithm (SSA), also known as the Gillespie algorithm. Let $X_t \\in \\mathbb{N}^d$ denote the state at time $t$, with $d \\in \\mathbb{N}$ species. There are $R \\in \\mathbb{N}$ reactions, each with stoichiometric vector $\\nu_r \\in \\mathbb{Z}^d$ and propensity (intensity) function $a_\\theta(x,r)$ depending on a parameter vector $\\theta \\in \\Theta \\subset \\mathbb{R}^p$. The total intensity is $\\lambda_\\theta(x) = \\sum_{r=1}^R a_\\theta(x,r)$. For two parameter values $\\theta$ and $\\theta'$, consider their induced path measures on trajectories $\\{X_t\\}_{t \\in [0,T]}$.\n\nYour tasks are as follows.\n\n1) Starting from the standard likelihood representation for marked point processes that underlies the Stochastic Simulation Algorithm (SSA), express the log-likelihood ratio of observing a trajectory $\\{X_t\\}_{t \\in [0,T]}$ under parameter $\\theta$ relative to $\\theta'$ in terms of the propensities $a_\\theta(x,r)$, the total intensities $\\lambda_\\theta(x)$, and the realized reaction marks. Define the relative entropy rate $\\mathcal{R}(\\theta \\| \\theta')$ as the long-time limit of the expected log-likelihood ratio per unit time under the stationary distribution of the process with parameter $\\theta$.\n\n2) Let $\\Psi_{\\theta,\\theta'}(x)$ be the stationary integrand that emerges from part $1$ when expressing $\\mathcal{R}(\\theta \\| \\theta')$ as a stationary expectation with respect to the stationary distribution of $X_t$ under $\\theta$. Using the ergodic theorem for CTMCs, construct a consistent estimator $\\widehat{\\mathcal{R}}_T$ of $\\mathcal{R}(\\theta \\| \\theta')$ based on a single long SSA trajectory $\\{X_t\\}_{t \\in [0,T]}$, written as a continuous-time average of a function of the state. Your estimator must be computable from the piecewise-constant path produced by the SSA, which consists of a sequence of holding times and states.\n\n3) Using the Martingale Central Limit Theorem (MCLT) for ergodic additive functionals of CTMCs, derive the asymptotic distribution of the time-average estimator of a centered observable $g(X_t)$, and specialize your result to $g(x) = \\Psi_{\\theta,\\theta'}(x) - \\mathcal{R}(\\theta \\| \\theta')$. Express the asymptotic variance $\\sigma^2_{\\theta,\\theta'}$ in at least one of the following equivalent ways: as an integrated autocovariance over continuous time, or via the Poisson equation associated with the generator of the CTMC. Then, propose and justify a statistically consistent, simulation-based estimator $\\widehat{\\sigma^2}_{\\theta,\\theta'}$ of $\\sigma^2_{\\theta,\\theta'}$ that uses only a single long SSA trajectory and does not require solving the Poisson equation.\n\n4) Implement a complete program that:\n- Simulates long SSA trajectories under parameter $\\theta$ for each of the test cases specified below.\n- Computes the consistent estimator $\\widehat{\\mathcal{R}}_T$ from part $2$ using the SSA trajectory.\n- Estimates the asymptotic variance $\\widehat{\\sigma^2}_{\\theta,\\theta'}$ using a batch-means estimator over continuous time: partition the post–burn-in time interval into $B$ equal-duration batches of width $w$, compute the batch averages of $\\Psi_{\\theta,\\theta'}(X_t)$ over each batch, and use the sample variance of these batch averages to estimate $\\sigma^2_{\\theta,\\theta'}$ via the relationship $\\mathrm{Var}(\\text{batch average}) \\approx \\sigma^2_{\\theta,\\theta'} / w$.\n- Uses a fixed random seed so that all outputs are reproducible.\n- Produces the final output on a single line as a comma-separated list enclosed in square brackets, where each test case contributes a two-element list $[\\widehat{\\mathcal{R}}_T,\\widehat{\\sigma^2}_{\\theta,\\theta'}]$. Round each reported number to $6$ decimals.\n\nConstraints and modeling details:\n- The networks in the test suite are independent birth–death processes per species. For species index $i \\in \\{1,\\dots,d\\}$, the reactions are: birth $X_i \\to X_i + 1$ with rate $k_{b,i}$, and death $X_i \\to X_i - 1$ with rate $k_{d,i} X_i$. Thus, for each species, $a_\\theta(x,\\text{birth}_i) = k_{b,i}$ and $a_\\theta(x,\\text{death}_i) = k_{d,i}\\, x_i$. The total intensity is $\\lambda_\\theta(x) = \\sum_{i=1}^d \\left(k_{b,i} + k_{d,i}\\, x_i\\right)$. Only valid reactions with nonnegative propensities may fire. All components of $\\theta$ and $\\theta'$ are strictly positive in the tests below.\n- You must simulate using the Stochastic Simulation Algorithm (SSA) up to a total time horizon $T_{\\text{total}}$, with a burn-in $T_b$ discarded from all estimators. Use $B$ batches of equal duration $w = (T_{\\text{total}} - T_b)/B$.\n- Use initial condition $X_0$ set per test as specified below. If a component would become negative, suppress that reaction by its zero propensity in SSA.\n\nTest suite:\n- Test $1$: Dimension $d=1$. Parameters $\\theta=(k_b,k_d)=(1.8,1.0)$, $\\theta'=(1.5,1.0)$. Initial condition $X_0 = \\lfloor k_b/k_d \\rfloor = 1$. Time horizon $T_{\\text{total}}=20000$, burn-in $T_b=2000$, batches $B=50$.\n- Test $2$: Dimension $d=2$. Parameters $\\theta=(k_b,k_d)=([2.0,1.5],[1.0,0.75])$, $\\theta'=([2.1,1.6],[1.0,0.75])$. Initial condition $X_0 = \\left(\\lfloor 2.0/1.0 \\rfloor,\\lfloor 1.5/0.75 \\rfloor\\right)=(2,2)$. Time horizon $T_{\\text{total}}=20000$, burn-in $T_b=2000$, batches $B=50$.\n- Test $3$: Dimension $d=2$. Parameters $\\theta=(k_b,k_d)=([1.2,0.9],[0.9,0.6])$, $\\theta'=(k_b',k_d')=\\theta$. Initial condition $X_0 = \\left(\\lfloor 1.2/0.9 \\rfloor,\\lfloor 0.9/0.6 \\rfloor\\right)=(1,1)$. Time horizon $T_{\\text{total}}=20000$, burn-in $T_b=2000$, batches $B=50$.\n\nNumerical and output requirements:\n- Use a fixed random seed equal to $1729$.\n- For each test, compute and return a list $[\\widehat{\\mathcal{R}}_T,\\widehat{\\sigma^2}_{\\theta,\\theta'}]$, with both entries rounded to $6$ decimals.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with the test cases in order $1$, $2$, $3$. For example, the output format must be of the form $[[x_{1},y_{1}],[x_{2},y_{2}],[x_{3},y_{3}]]$ where each $x_i$ and $y_i$ is a float rounded to $6$ decimals.", "solution": "### Part 1: Log-Likelihood Ratio and Relative Entropy Rate\n\nA trajectory of a continuous-time Markov chain on a finite state space, simulated by the Stochastic Simulation Algorithm (SSA), can be represented as a marked point process. The trajectory over a time interval $[0, T]$ is fully described by the initial state $X_0=x_0$, the sequence of $N$ reaction events (marks) $r_1, r_2, \\ldots, r_N$ that occur at times $0 < t_1 < t_2 < \\ldots < t_N < T$, and the states visited $x_0, x_1, \\ldots, x_N$, where $x_k = x_{k-1} + \\nu_{r_k}$ for $k=1, \\ldots, N$. The state of the system is $X_t = x_{k-1}$ for $t \\in [t_{k-1}, t_k)$, with $t_0=0$.\n\nThe likelihood of observing this specific path history under a parameter vector $\\theta$ is the product of the probabilities of each event and the probabilities of the inter-event holding times. The probability density for the $k$-th reaction $r_k$ occurring at time $t_k$, given the state $x_{k-1}$, is given by its propensity $a_\\theta(x_{k-1}, r_k)$. The probability of no reaction occurring between times $t_{k-1}$ and $t_k$ is $\\exp\\left(-\\lambda_\\theta(x_{k-1})(t_k - t_{k-1})\\right)$, where $\\lambda_\\theta(x) = \\sum_{r=1}^R a_\\theta(x, r)$ is the total intensity.\n\nThe full likelihood of the trajectory is:\n$$ L(\\text{traj}|\\theta) = \\left( \\prod_{k=1}^N a_\\theta(x_{k-1}, r_k) \\right) \\exp\\left( -\\sum_{k=1}^N \\lambda_\\theta(x_{k-1})(t_k - t_{k-1}) - \\lambda_\\theta(x_N)(T - t_N) \\right) $$\nThe sum in the exponent can be rewritten as a continuous-time integral of the total intensity over the path $\\{X_s\\}_{s \\in [0,T]}$:\n$$ \\int_0^T \\lambda_\\theta(X_s) ds = \\sum_{k=1}^N \\lambda_\\theta(x_{k-1})(t_k - t_{k-1}) + \\lambda_\\theta(x_N)(T - t_N) $$\nThe log-likelihood, denoted $\\log L(\\theta)$, is therefore:\n$$ \\log L(\\theta) = \\sum_{k=1}^N \\log a_\\theta(X_{t_k^-}, r_k) - \\int_0^T \\lambda_\\theta(X_s) ds $$\nwhere $X_{t_k^-} = x_{k-1}$ is the state just before the $k$-th jump.\n\nThe log-likelihood ratio of observing the trajectory under parameter $\\theta$ relative to $\\theta'$ is:\n$$ \\mathcal{LLR} = \\log\\frac{L(\\theta)}{L(\\theta')} = \\log L(\\theta) - \\log L(\\theta') $$\n$$ \\mathcal{LLR} = \\left( \\sum_{k=1}^N \\log a_\\theta(X_{t_k^-}, r_k) - \\int_0^T \\lambda_\\theta(X_s) ds \\right) - \\left( \\sum_{k=1}^N \\log a_{\\theta'}(X_{t_k^-}, r_k) - \\int_0^T \\lambda_{\\theta'}(X_s) ds \\right) $$\nCombining terms, we get the expression for the log-likelihood ratio:\n$$ \\mathcal{LLR} = \\sum_{k=1}^N \\log\\left(\\frac{a_\\theta(X_{t_k^-}, r_k)}{a_{\\theta'}(X_{t_k^-}, r_k)}\\right) - \\int_0^T (\\lambda_\\theta(X_s) - \\lambda_{\\theta'}(X_s)) ds $$\n\nThe relative entropy rate (or Kullback-Leibler divergence rate) is the long-time limit of the expected log-likelihood ratio per unit time, where the expectation $E_\\theta[\\cdot]$ is taken with respect to the path measure induced by $\\theta$.\n$$ \\mathcal{R}(\\theta \\| \\theta') = \\lim_{T \\to \\infty} \\frac{1}{T} E_\\theta[\\mathcal{LLR}] $$\nTo evaluate this expectation, we use the fact that the counting process for each reaction $r$, $N_t^r$, can be decomposed into its compensator and a martingale: $dN_t^r = a_\\theta(X_t, r)dt + dM_t^r$, where $E_\\theta[dM_t^r]=0$. The sum in the $\\mathcal{LLR}$ is a stochastic integral $\\sum_{k=1}^N \\dots = \\sum_{r=1}^R \\int_0^T \\log\\frac{a_\\theta(X_s, r)}{a_{\\theta'}(X_s, r)} dN_s^r$.\nTaking the expectation and substituting the compensator, the martingale terms vanish, and we obtain:\n$$ E_\\theta[\\mathcal{LLR}] = E_\\theta\\left[ \\int_0^T \\sum_{r=1}^R a_\\theta(X_s, r) \\log\\left(\\frac{a_\\theta(X_s, r)}{a_{\\theta'}(X_s, r)}\\right) ds - \\int_0^T (\\lambda_\\theta(X_s) - \\lambda_{\\theta'}(X_s)) ds \\right] $$\nBy linearity of expectation and Fubini's theorem:\n$$ E_\\theta[\\mathcal{LLR}] = \\int_0^T E_\\theta\\left[ \\sum_{r=1}^R a_\\theta(X_s, r) \\log\\left(\\frac{a_\\theta(X_s, r)}{a_{\\theta'}(X_s, r)}\\right) - (\\lambda_\\theta(X_s) - \\lambda_{\\theta'}(X_s)) \\right] ds $$\nWe identify the integrand as a function of state, $\\Psi_{\\theta,\\theta'}(x)$:\n$$ \\Psi_{\\theta,\\theta'}(x) = \\sum_{r=1}^R a_\\theta(x, r) \\log\\left(\\frac{a_\\theta(x, r)}{a_{\\theta'}(x, r)}\\right) - (\\lambda_\\theta(x) - \\lambda_{\\theta'}(x)) $$\nFor an ergodic CTMC that has reached its stationary distribution $\\pi_\\theta$, the expectation $E_\\theta[\\Psi_{\\theta,\\theta'}(X_s)]$ becomes independent of time $s$, equalling $E_{\\pi_\\theta}[\\Psi_{\\theta,\\theta'}(X)]$. Thus, the relative entropy rate is the stationary expectation of this integrand:\n$$ \\mathcal{R}(\\theta \\| \\theta') = E_{\\pi_\\theta}[\\Psi_{\\theta,\\theta'}(X)] = \\sum_{x} \\pi_\\theta(x) \\Psi_{\\theta,\\theta'}(x) $$\n\n### Part 2: Consistent Estimator for the Relative Entropy Rate\n\nThe ergodic theorem for irreducible, positive recurrent CTMCs states that for any function $f$ on the state space such that $E_{\\pi_\\theta}[|f(X)|] < \\infty$, the time average converges to the stationary expectation almost surely:\n$$ \\lim_{T\\to\\infty} \\frac{1}{T}\\int_0^T f(X_s)ds = E_{\\pi_\\theta}[f(X)] $$\nWe can construct a consistent estimator for $\\mathcal{R}(\\theta \\| \\theta')$ by applying this theorem with $f(x) = \\Psi_{\\theta,\\theta'}(x)$. The resulting estimator, $\\widehat{\\mathcal{R}}_T$, is the time average of $\\Psi_{\\theta,\\theta'}(X_t)$ over a single long trajectory:\n$$ \\widehat{\\mathcal{R}}_T = \\frac{1}{T} \\int_0^T \\Psi_{\\theta,\\theta'}(X_s) ds $$\nFor a trajectory generated by SSA, the path $X_s$ is piecewise-constant. Let the jump times be $t_1, \\ldots, t_N$ in $[0, T]$ and the states be $x_0, \\ldots, x_N$. The integral is computed as a sum over these constant-state intervals:\n$$ \\widehat{\\mathcal{R}}_T = \\frac{1}{T} \\left( \\sum_{k=1}^N \\Psi_{\\theta,\\theta'}(x_{k-1}) (t_k - t_{k-1}) + \\Psi_{\\theta,\\theta'}(x_N) (T - t_N) \\right) $$\nIn practice, a burn-in period $T_b$ is discarded to allow the process to approach stationarity. The estimator is then computed over the interval $[T_b, T_{\\text{total}}]$ of duration $T_e = T_{\\text{total}} - T_b$:\n$$ \\widehat{\\mathcal{R}}_{T_e} = \\frac{1}{T_e} \\int_{T_b}^{T_{\\text{total}}} \\Psi_{\\theta,\\theta'}(X_s) ds $$\n\n### Part 3: Asymptotic Variance and Batch Means Estimator\n\nThe Martingale Central Limit Theorem (MCLT) for additive functionals of ergodic CTMCs describes the asymptotic distribution of the time-average estimator. For a centered observable $g(X_t) = \\Psi_{\\theta,\\theta'}(X_t) - \\mathcal{R}(\\theta \\| \\theta')$, the theorem states:\n$$ \\sqrt{T_e} \\left( \\widehat{\\mathcal{R}}_{T_e} - \\mathcal{R}(\\theta \\| \\theta') \\right) = \\frac{1}{\\sqrt{T_e}} \\int_{T_b}^{T_{\\text{total}}} g(X_s) ds \\quad \\xrightarrow{d} \\quad \\mathcal{N}(0, \\sigma^2_{\\theta,\\theta'}) $$\nas $T_e \\to \\infty$. The asymptotic variance $\\sigma^2_{\\theta,\\theta'}$ quantifies the magnitude of fluctuations of the estimator. It can be expressed as the integrated time-autocovariance of the centered observable under stationarity:\n$$ \\sigma^2_{\\theta,\\theta'} = 2 \\int_0^\\infty \\text{Cov}_{\\pi_\\theta}(g(X_0), g(X_t)) dt = 2 \\int_0^\\infty E_{\\pi_\\theta}[g(X_0) g(X_t)] dt $$\nDirectly estimating this from data is difficult. A practical and consistent method is the batch means estimator. The post-burn-in trajectory of duration $T_e$ is divided into $B$ contiguous batches of equal duration $w = T_e/B$. For each batch $j \\in \\{1, \\ldots, B\\}$, the batch mean is computed:\n$$ Y_j = \\frac{1}{w} \\int_{T_b + (j-1)w}^{T_b + j w} \\Psi_{\\theta,\\theta'}(X_s) ds $$\nIf the batch width $w$ is large enough to be greater than the correlation time of the process, the batch means $Y_1, \\ldots, Y_B$ become approximately independent and identically distributed. The variance of a single batch mean is approximately $\\text{Var}(Y_j) \\approx \\sigma^2_{\\theta,\\theta'} / w$.\n\nWe can estimate $\\sigma^2_{\\theta,\\theta'}$ by scaling the sample variance of the batch means. The sample variance of $\\{Y_j\\}_{j=1}^B$ is:\n$$ S_Y^2 = \\frac{1}{B-1} \\sum_{j=1}^B (Y_j - \\bar{Y})^2, \\quad \\text{where } \\bar{Y} = \\frac{1}{B}\\sum_{j=1}^B Y_j = \\widehat{\\mathcal{R}}_{T_e} $$\nRearranging the relationship, we get the batch means estimator for the asymptotic variance:\n$$ \\widehat{\\sigma^2}_{\\theta,\\theta'} = w \\cdot S_Y^2 = \\frac{w}{B-1} \\sum_{j=1}^B (Y_j - \\widehat{\\mathcal{R}}_{T_e})^2 $$\nThis estimator is consistent for $\\sigma^2_{\\theta,\\theta'}$ in the limit where $B \\to \\infty$ and $w \\to \\infty$.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation and analysis for all test cases.\n    \"\"\"\n    np.random.seed(1729)\n\n    test_cases = [\n        {\n            \"d\": 1,\n            \"theta\": {\"kb\": np.array([1.8]), \"kd\": np.array([1.0])},\n            \"theta_prime\": {\"kb\": np.array([1.5]), \"kd\": np.array([1.0])},\n            \"x0\": np.array([1]),\n            \"T_total\": 20000, \"T_b\": 2000, \"B\": 50,\n        },\n        {\n            \"d\": 2,\n            \"theta\": {\"kb\": np.array([2.0, 1.5]), \"kd\": np.array([1.0, 0.75])},\n            \"theta_prime\": {\"kb\": np.array([2.1, 1.6]), \"kd\": np.array([1.0, 0.75])},\n            \"x0\": np.array([2, 2]),\n            \"T_total\": 20000, \"T_b\": 2000, \"B\": 50,\n        },\n        {\n            \"d\": 2,\n            \"theta\": {\"kb\": np.array([1.2, 0.9]), \"kd\": np.array([0.9, 0.6])},\n            \"theta_prime\": {\"kb\": np.array([1.2, 0.9]), \"kd\": np.array([0.9, 0.6])},\n            \"x0\": np.array([1, 1]),\n            \"T_total\": 20000, \"T_b\": 2000, \"B\": 50,\n        },\n    ]\n\n    all_results = []\n\n    for case in test_cases:\n        d = case[\"d\"]\n        theta = case[\"theta\"]\n        theta_prime = case[\"theta_prime\"]\n        x0 = case[\"x0\"]\n        T_total = case[\"T_total\"]\n        T_b = case[\"T_b\"]\n        B = case[\"B\"]\n\n        # Run SSA simulation\n        times, states = ssa(d, theta, x0, T_total)\n        \n        # Calculate Psi integrand constants\n        c0, c_vec = calculate_psi_constants(theta, theta_prime)\n\n        # Post-process trajectory to get estimators\n        r_hat, sigma2_hat = analyze_trajectory(times, states, T_total, T_b, B, c0, c_vec)\n        \n        all_results.append([round(r_hat, 6), round(sigma2_hat, 6)])\n\n    print(str(all_results).replace(\" \", \"\"))\n\ndef ssa(d, theta, x0, t_total):\n    \"\"\"\n    Performs a Stochastic Simulation Algorithm (Gillespie) run.\n    \"\"\"\n    kb, kd = theta[\"kb\"], theta[\"kd\"]\n    \n    times = [0.0]\n    states = [np.copy(x0)]\n    \n    t = 0.0\n    x = np.copy(x0)\n    \n    # Stoichiometry matrix: first d rows are births, next d are deaths\n    stoich = np.vstack([np.eye(d, dtype=int), -np.eye(d, dtype=int)])\n\n    while t < t_total:\n        propensities = np.concatenate([kb, kd * x])\n        \n        # Suppress death reactions if count is zero\n        propensities[d:] = np.where(x > 0, propensities[d:], 0)\n        \n        lambda_total = np.sum(propensities)\n\n        if lambda_total <= 0:\n            # No more reactions possible, absorbing state.\n            break\n\n        dt = np.random.exponential(1.0 / lambda_total)\n        \n        # Choose reaction\n        r = np.random.choice(2 * d, p=propensities / lambda_total)\n\n        t += dt\n        if t > t_total:\n            # We only need the trajectory up to t_total.\n            # The last state is held until t_total.\n            break\n\n        x += stoich[r]\n        \n        times.append(t)\n        states.append(np.copy(x))\n        \n    return np.array(times), np.array(states)\n\ndef calculate_psi_constants(theta, theta_prime):\n    \"\"\"\n    Calculates the constants for the affine function Psi(x) = c0 + c_vec.dot(x).\n    \"\"\"\n    kb, kd = theta[\"kb\"], theta[\"kd\"]\n    kb_p, kd_p = theta_prime[\"kb\"], theta_prime[\"kd\"]\n\n    # Term for a_theta * log(a_theta/a_theta_prime) where a_theta=0 should be 0.\n    # Handled by numpy's x*log(x) -> 0 limit.\n    # Here, we pre-calculate based on the derived affine form, which is safe.\n\n    with np.errstate(divide='ignore', invalid='ignore'):\n        log_term_b = kb * np.log(kb / kb_p)\n        log_term_d = kd * np.log(kd / kd_p)\n\n    log_term_b = np.nan_to_num(log_term_b)\n    log_term_d = np.nan_to_num(log_term_d)\n    \n    c0 = np.sum(log_term_b - (kb - kb_p))\n    c_vec = log_term_d - (kd - kd_p)\n    \n    return c0, c_vec\n\ndef analyze_trajectory(times, states, T_total, T_b, B, c0, c_vec):\n    \"\"\"\n    Computes the estimators for R and sigma^2 using batch means.\n    \"\"\"\n    # Calculate Psi(x) for each state in the trajectory\n    psi_vals = c0 + states @ c_vec\n\n    T_eff = T_total - T_b\n    w = T_eff / B\n    batch_integrals = np.zeros(B)\n\n    # Add the final time point to correctly handle the last interval\n    if times[-1] < T_total:\n        times = np.append(times, T_total)\n        states = np.vstack([states, states[-1]])\n        psi_vals = np.append(psi_vals, psi_vals[-1])\n        \n    # Iterate through each segment of the piecewise-constant path\n    for k in range(1, len(times)):\n        t_start, t_end = times[k-1], times[k]\n        psi = psi_vals[k-1]\n        \n        # Find the portion of this segment that is post-burn-in\n        interval_start = max(t_start, T_b)\n        interval_end = min(t_end, T_total)\n\n        if interval_start >= interval_end:\n            continue\n        \n        # Distribute the integral contribution of this segment to the correct batches\n        t = interval_start\n        while t < interval_end:\n            batch_idx = int((t - T_b) / w)\n            if batch_idx >= B: batch_idx = B - 1 # Handle floating point edge cases\n            \n            batch_end_time = T_b + (batch_idx + 1) * w\n            \n            contribution_end = min(interval_end, batch_end_time)\n            dt = contribution_end - t\n            \n            batch_integrals[batch_idx] += psi * dt\n            t = contribution_end\n    \n    # Calculate the final estimators\n    total_integral = np.sum(batch_integrals)\n    r_hat = total_integral / T_eff if T_eff > 0 else 0.0\n\n    batch_means = batch_integrals / w\n    # Use ddof=1 for sample variance (1/(B-1) factor)\n    sigma2_hat = w * np.var(batch_means, ddof=1) if B > 1 else 0.0\n\n    return r_hat, sigma2_hat\n\nsolve()\n```", "id": "3303203"}]}