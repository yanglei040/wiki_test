{"hands_on_practices": [{"introduction": "The sharp, switch-like transitions that govern cell cycle progression are not magic; they emerge from the underlying biochemistry. This exercise delves into the mechanisms of ultrasensitivity, challenging you to derive how a combination of multisite phosphorylation and enzyme saturation can collectively generate a response far steeper than a simple Michaelis-Menten curve. Mastering this derivation [@problem_id:3293523] provides a foundational understanding of how cells build robust biological switches from basic enzymatic components.", "problem": "Cyclin-dependent kinase 1 (Cdk1) integrates inhibitory multisite phosphorylation by Wee1 kinase and activating dephosphorylation by Cdc25 phosphatase. Consider a substrate with $m$ identical, independent inhibitory phosphorylation sites, where Cdk1 is active only when all $m$ sites are dephosphorylated. Each site is modified by a covalent-modification cycle with Michaelis-Menten saturation for both enzymes. Let the kinase and phosphatase have maximal velocities $V_{\\mathrm{k}}$ and $V_{\\mathrm{p}}$, and Michaelis constants $K_{\\mathrm{k}}$ and $K_{\\mathrm{p}}$, respectively, acting on a substrate pool with total concentration $S_{\\mathrm{T}}$. Define the dimensionless saturation parameters $a = K_{\\mathrm{k}}/S_{\\mathrm{T}}$ and $b = K_{\\mathrm{p}}/S_{\\mathrm{T}}$. Under quasi-steady-state and time-scale separation assumptions, the steady-state fraction $Y$ of a single site that is phosphorylated satisfies the covalent-cycle balance relation arising from saturable enzyme kinetics. Introduce the activity ratio $\\phi = V_{\\mathrm{p}}/V_{\\mathrm{k}}$ so that Cdk1 activation increases with $\\phi$. The fraction of Cdk1 molecules that are active is $A(\\phi) = \\left(1 - Y(\\phi)\\right)^{m}$.\n\nDefine the effective Hill coefficient $n_{\\mathrm{eff}}$ of the input-output mapping $A(\\phi)$ at its half-activation point by\n$$\nn_{\\mathrm{eff}} \\equiv \\left. \\frac{d \\ln \\left( \\frac{A}{1 - A} \\right)}{d \\ln \\phi} \\right|_{A = 1/2}.\n$$\nStarting from the standard quasi-steady-state balance for a covalent modification cycle with saturable enzymes and the above independence assumptions across sites, derive a closed-form analytic expression for $n_{\\mathrm{eff}}$ in terms of $m$, $a$, and $b$. Your final answer must be a single symbolic expression. Do not provide intermediate steps in the final answer.", "solution": "The user wants to find the effective Hill coefficient of a multisite phosphorylation system.\n\n### Step 1: Validation of the Problem Statement\nThe problem describes a classic model in systems biology for signal transduction, specifically the covalent modification of a substrate by a kinase and a phosphatase, where the substrate has multiple independent sites. This is an extension of the Goldbeter-Koshland model.\n\n**Extraction of Givens:**\n- Substrate with $m$ identical, independent inhibitory phosphorylation sites.\n- Cdk1 is active only when all $m$ sites are dephosphorylated.\n- Enzymes follow Michaelis-Menten kinetics.\n- Kinase maximal velocity: $V_{\\mathrm{k}}$.\n- Phosphatase maximal velocity: $V_{\\mathrm{p}}$.\n- Kinase Michaelis constant: $K_{\\mathrm{k}}$.\n- Phosphatase Michaelis constant: $K_{\\mathrm{p}}$.\n- Total substrate concentration: $S_{\\mathrm{T}}$.\n- Dimensionless saturation parameters: $a = K_{\\mathrm{k}}/S_{\\mathrm{T}}$ and $b = K_{\\mathrm{p}}/S_{\\mathrm{T}}$.\n- The system is at quasi-steady-state.\n- $Y$ is the steady-state fraction of a single site that is phosphorylated.\n- Activity ratio: $\\phi = V_{\\mathrm{p}}/V_{\\mathrm{k}}$.\n- Fraction of active Cdk1 molecules: $A(\\phi) = \\left(1 - Y(\\phi)\\right)^{m}$.\n- Definition of effective Hill coefficient: $n_{\\mathrm{eff}} \\equiv \\left. \\frac{d \\ln \\left( \\frac{A}{1 - A} \\right)}{d \\ln \\phi} \\right|_{A = 1/2}$.\n\n**Validation against Criteria:**\n- **Scientifically Grounded:** The problem is based on the well-established Goldbeter-Koshland model for covalent modification cycles, a cornerstone of computational systems biology. The assumptions are standard for modeling such networks. The problem is scientifically sound.\n- **Well-Posed:** All necessary variables, definitions ($A(\\phi)$, $n_{\\mathrm{eff}}$), and relationships (Michaelis-Menten kinetics, independence of sites) are provided. The objective is to derive a specific quantity ($n_{\\mathrm{eff}}$) in terms of specified parameters ($m, a, b$), making the problem well-posed.\n- **Objective:** The problem is stated using precise, formal, and unbiased mathematical and scientific language.\n\n**Verdict:** The problem is scientifically sound, well-posed, and formalizable. It is deemed **valid**.\n\n### Step 2: Derivation of the Solution\n\nThe effective Hill coefficient is defined as:\n$$\nn_{\\mathrm{eff}} = \\left. \\frac{d \\ln \\left( \\frac{A}{1 - A} \\right)}{d \\ln \\phi} \\right|_{A = 1/2}\n$$\nWe can use the chain rule to express this derivative in terms of the single-site phosphorylation fraction, $Y$.\n$$\nn_{\\mathrm{eff}} = \\left. \\frac{d \\ln \\left( \\frac{A}{1 - A} \\right)}{d Y} \\cdot \\frac{d Y}{d \\ln \\phi} \\right|_{A=1/2} = \\left. \\frac{\\frac{d}{dY} \\ln \\left( \\frac{A}{1 - A} \\right)}{\\frac{d}{dY} \\ln \\phi} \\right|_{A=1/2}\n$$\n\nFirst, we find the \"standard quasi-steady-state balance for a covalent modification cycle\". Let $S_p$ be the concentration of the phosphorylated substrate form for a single site, and $S$ be the concentration of the dephosphorylated form. The total concentration is $S_\\mathrm{T} = S_p + S$. The fraction of phosphorylated sites is $Y = S_p/S_\\mathrm{T}$, and dephosphorylated sites is $1-Y = S/S_\\mathrm{T}$.\n\nThe rates of phosphorylation (kinase) and dephosphorylation (phosphatase) are:\n$$\nv_{\\mathrm{k}} = \\frac{V_{\\mathrm{k}} S}{K_{\\mathrm{k}} + S} = \\frac{V_{\\mathrm{k}} (1-Y)S_{\\mathrm{T}}}{K_{\\mathrm{k}} + (1-Y)S_{\\mathrm{T}}} = \\frac{V_{\\mathrm{k}} (1-Y)}{a + 1 - Y}\n$$\n$$\nv_{\\mathrm{p}} = \\frac{V_{\\mathrm{p}} S_p}{K_{\\mathrm{p}} + S_p} = \\frac{V_{\\mathrm{p}} Y S_{\\mathrm{T}}}{K_{\\mathrm{p}} + Y S_{\\mathrm{T}}} = \\frac{V_{\\mathrm{p}} Y}{b + Y}\n$$\nAt steady state, $v_{\\mathrm{k}} = v_{\\mathrm{p}}$.\n$$\n\\frac{V_{\\mathrm{k}} (1-Y)}{a + 1 - Y} = \\frac{V_{\\mathrm{p}} Y}{b + Y}\n$$\nRearranging for the activity ratio $\\phi = V_{\\mathrm{p}}/V_{\\mathrm{k}}$ gives:\n$$\n\\phi = \\frac{(1-Y)(b+Y)}{Y(a+1-Y)}\n$$\nTaking the natural logarithm, we get the input-output relationship for a single site:\n$$\n\\ln \\phi = \\ln(1-Y) + \\ln(b+Y) - \\ln Y - \\ln(a+1-Y)\n$$\n\nNow we compute the two derivatives needed for $n_{\\mathrm{eff}}$.\n\n**Numerator derivative:** $\\frac{d}{dY} \\ln \\left( \\frac{A}{1-A} \\right)$\nGiven $A = (1-Y)^m$.\n$$\n\\frac{A}{1-A} = \\frac{(1-Y)^m}{1 - (1-Y)^m}\n$$\n$$\n\\ln \\left(\\frac{A}{1-A}\\right) = m \\ln(1-Y) - \\ln(1 - (1-Y)^m)\n$$\nDifferentiating with respect to $Y$:\n$$\n\\frac{d}{dY} \\ln \\left( \\frac{A}{1-A} \\right) = \\frac{-m}{1-Y} - \\frac{-m(1-Y)^{m-1}(-1)}{1 - (1-Y)^m} = \\frac{-m}{1-Y} - \\frac{m(1-Y)^{m-1}}{1 - (1-Y)^m}\n$$\nCombining terms:\n$$\n\\frac{-m(1-(1-Y)^m) - m(1-Y)^m}{(1-Y)(1-(1-Y)^m)} = \\frac{-m}{(1-Y)(1-(1-Y)^m)}\n$$\n\n**Denominator derivative:** $\\frac{d}{dY} \\ln \\phi$\n$$\n\\frac{d}{dY} \\ln \\phi = \\frac{d}{dY} \\left( \\ln(1-Y) + \\ln(b+Y) - \\ln Y - \\ln(a+1-Y) \\right)\n$$\n$$\n\\frac{d \\ln \\phi}{d Y} = -\\frac{1}{1-Y} + \\frac{1}{b+Y} - \\frac{1}{Y} - \\frac{-1}{a+1-Y} = -\\frac{1}{1-Y} - \\frac{1}{Y} + \\frac{1}{a+1-Y} + \\frac{1}{b+Y}\n$$\nIt is more convenient to group the terms as follows:\n$$\n\\frac{d \\ln \\phi}{d Y} = \\left( \\frac{1}{a+1-Y} - \\frac{1}{1-Y} \\right) + \\left( \\frac{1}{b+Y} - \\frac{1}{Y} \\right)\n$$\n$$\n\\frac{d \\ln \\phi}{d Y} = \\frac{(1-Y) - (a+1-Y)}{(1-Y)(a+1-Y)} + \\frac{Y - (b+Y)}{Y(b+Y)} = \\frac{-a}{(1-Y)(a+1-Y)} - \\frac{b}{Y(b+Y)}\n$$\n\n**Combine and evaluate at $A=1/2$:**\nThe condition $A=1/2$ implies $(1-Y)^m = 1/2$. Let's denote the value of $Y$ at this point as $Y_{1/2}$.\nFrom this, $1 - Y_{1/2} = (1/2)^{1/m} = 2^{-1/m}$, and $Y_{1/2} = 1 - 2^{-1/m}$.\n\nLet's evaluate the numerator derivative at $Y_{1/2}$:\n$$\n\\left. \\frac{-m}{(1-Y)(1-(1-Y)^m)} \\right|_{Y_{1/2}} = \\frac{-m}{2^{-1/m}(1-1/2)} = \\frac{-m}{2^{-1/m}(1/2)} = -2m \\cdot 2^{1/m}\n$$\nThe denominator derivative is evaluated at $Y_{1/2}$:\n$$\n\\left. \\frac{-a}{(1-Y)(a+1-Y)} - \\frac{b}{Y(b+Y)} \\right|_{Y_{1/2}} = \\frac{-a}{2^{-1/m}(a+2^{-1/m})} - \\frac{b}{(1-2^{-1/m})(b+1-2^{-1/m})}\n$$\nNow, we compute $n_{\\mathrm{eff}}$:\n$$\nn_{\\mathrm{eff}} = \\frac{-2m \\cdot 2^{1/m}}{\\frac{-a}{2^{-1/m}(a+2^{-1/m})} - \\frac{b}{(1-2^{-1/m})(b+1-2^{-1/m})}}\n$$\n$$\nn_{\\mathrm{eff}} = \\frac{2m \\cdot 2^{1/m}}{\\frac{a}{2^{-1/m}(a+2^{-1/m})} + \\frac{b}{(1-2^{-1/m})(b+1-2^{-1/m})}}\n$$\nMultiply the numerator and denominator by $2^{-1/m}$:\n$$\nn_{\\mathrm{eff}} = \\frac{2m}{\\frac{a}{a+2^{-1/m}} + \\frac{b \\cdot 2^{-1/m}}{(1-2^{-1/m})(b+1-2^{-1/m})}}\n$$\nThis expression depends only on $m$, $a$, and $b$, as requested. Although it contains the term $2^{-1/m}$, this is a well-defined analytic function of $m$, and no further simplification to a purely rational form is possible without additional assumptions. This is the closed-form analytic expression for the effective Hill coefficient. To make the final answer more compact, let $x=2^{-1/m}$.\n\nThe expression is:\n$$\nn_{\\mathrm{eff}} = \\frac{2m}{\\frac{a}{a+x} + \\frac{bx}{(1-x)(b+1-x)}}\n$$\nwhere $x = 2^{-1/m}$.", "answer": "$$ \\boxed{ \\frac{2m}{\\frac{a}{a+2^{-1/m}} + \\frac{b \\cdot 2^{-1/m}}{(1-2^{-1/m})(b+1-2^{-1/m})}} } $$", "id": "3293523"}, {"introduction": "Cellular decisions, such as arresting the cell cycle in response to DNA damage, are fundamentally stochastic events. This practice moves beyond deterministic models to explore how random molecular events, modeled here as a Poisson process of damage bursts, integrate over time to trigger a checkpoint response [@problem_id:3293531]. By calculating the arrest probability, you will gain hands-on experience in applying stochastic process theory to understand the reliability and timing of cellular control systems.", "problem": "A cell cycle arrest decision can be abstracted as a stochastic hybrid process coupling deoxyribonucleic acid damage checkpoint signaling to cyclin-dependent kinase control. Ataxia telangiectasia and Rad3-related (ATR) and ataxia telangiectasia mutated (ATM) kinase activity in response to deoxyribonucleic acid damage stimulates tumor protein p53 (p53), which induces Cyclin-dependent kinase inhibitor 1A (p21). Elevated p21 inhibits Cyclin-dependent kinase (CDK), producing arrest when p21 exceeds a decision threshold. Consider the following mathematically formalized scenario.\n\nA homogeneous Poisson burst process models deoxyribonucleic acid damage input. Bursts occur with rate parameter $\\lambda$ (in $\\mathrm{h}^{-1}$), producing instantaneous increments to the p21 level. Let the p21 concentration be represented by a piecewise deterministic state variable $p(t)$ satisfying $p(0) = p_0$ and, under bursts at times $\\{t_k\\}$, evolving by\n$$\np(t) = p_0 + \\sum_{k : t_k \\le t} a,\n$$\nwhere $a$ is a fixed increment (in consistent concentration units) per burst due to p53-mediated transcription and translation. Over the short decision window, assume negligible p21 decay, justified by a small first-order degradation rate $\\gamma$ satisfying $\\gamma T_{\\mathrm{CDK}} \\ll 1$, where $T_{\\mathrm{CDK}}$ is the deterministic time (in $\\mathrm{h}$) at which CDK commits the cell to cycle progression in the absence of sufficient p21. The arrest decision is defined as follows: the cell arrests if and only if there exists $t \\in [0, T_{\\mathrm{CDK}}]$ such that $p(t) \\ge \\theta$, where $\\theta$ is the p21 arrest threshold (in the same concentration units as $p_0$ and $a$).\n\nStarting from fundamental facts about the homogeneous Poisson process, and the threshold crossing condition in this hybrid system, derive a mathematically correct expression for the probability that the cell arrests before CDK commits, as a function of $(\\lambda, T_{\\mathrm{CDK}}, a, \\theta, p_0)$. Then implement this expression as an algorithm to compute the arrest probability numerically for a given parameter set. You must treat edge cases correctly, including: zero decision window, baseline already above threshold, zero burst increment, and extremely low or high burst rates.\n\nAll time quantities must be considered in $\\mathrm{h}$, and rate parameters in $\\mathrm{h}^{-1}$. Concentration units are arbitrary but must be consistent across $a$, $p_0$, and $\\theta$. The final probabilities are unitless and must be returned as real numbers. No angles are involved. Percentages must not be used; probabilities must be reported as decimals in $[0,1]$.\n\nTest suite. Use the following parameter sets, each provided as $(\\lambda, T_{\\mathrm{CDK}}, a, \\theta, p_0, \\gamma)$:\n- Case $1$ (general case, negligible decay): $(\\lambda = 2.0, T_{\\mathrm{CDK}} = 1.0, a = 0.5, \\theta = 2.0, p_0 = 0.2, \\gamma = 0.05)$.\n- Case $2$ (baseline already above threshold): $(\\lambda = 0.1, T_{\\mathrm{CDK}} = 0.5, a = 0.3, \\theta = 2.0, p_0 = 2.5, \\gamma = 0.01)$.\n- Case $3$ (zero decision window): $(\\lambda = 3.0, T_{\\mathrm{CDK}} = 0.0, a = 0.2, \\theta = 0.3, p_0 = 0.1, \\gamma = 0.02)$.\n- Case $4$ (low burst rate over moderate window): $(\\lambda = 0.1, T_{\\mathrm{CDK}} = 3.0, a = 0.5, \\theta = 1.2, p_0 = 0.0, \\gamma = 0.02)$.\n- Case $5$ (high burst rate, short window): $(\\lambda = 5.0, T_{\\mathrm{CDK}} = 0.5, a = 0.4, \\theta = 2.2, p_0 = 0.1, \\gamma = 0.05)$.\n\nFor scientific realism, verify that for all cases $\\gamma T_{\\mathrm{CDK}} \\le 0.1$, ensuring the negligible decay assumption is consistent.\n\nFinal output format. Your program should produce a single line of output containing the results for the five cases as a comma-separated list enclosed in square brackets (e.g., $[r_1,r_2,r_3,r_4,r_5]$), where each $r_i$ is the computed arrest probability for case $i$ as a decimal number.", "solution": "The problem is valid. It is scientifically grounded as a simplified model of cell cycle control, mathematically well-posed, and free of contradictions or ambiguities. The assumption of negligible p21 decay is explicitly justified by the condition $\\gamma T_{\\mathrm{CDK}} \\ll 1$, which is met by all provided test cases (e.g., for Case $1$, $\\gamma T_{\\mathrm{CDK}} = 0.05 \\times 1.0 = 0.05 \\le 0.1$). We may proceed with the solution.\n\nThe objective is to derive an expression for the probability that a cell arrests its cycle. The state variable representing the concentration of p21 is $p(t)$. Its initial value is $p(0) = p_0$. The concentration increases due to deoxyribonucleic acid damage bursts, which are modeled as a homogeneous Poisson process with rate $\\lambda$. Each burst, occurring at time $t_k$, causes an instantaneous increase of magnitude $a$ to the p21 level. With the assumption of negligible decay, the state evolves according to:\n$$\np(t) = p_0 + \\sum_{k : t_k \\le t} a\n$$\nLet $K(t)$ be the random variable representing the number of bursts in the time interval $[0, t]$. The state can be written as $p(t) = p_0 + K(t) \\cdot a$. Since $a$ is a non-negative concentration increment, $p(t)$ is a non-decreasing function of $t$.\n\nCellular arrest occurs if the p21 level $p(t)$ reaches or exceeds a threshold $\\theta$ at any time $t$ within the decision window $[0, T_{\\mathrm{CDK}}]$. The arrest condition is formally stated as: there exists $t \\in [0, T_{\\mathrm{CDK}}]$ such that $p(t) \\ge \\theta$.\n\nDue to the non-decreasing nature of $p(t)$, if the condition $p(t) \\ge \\theta$ is met for some time $t^* \\in [0, T_{\\mathrm{CDK}}]$, it will also be met for all subsequent times $t \\in [t^*, T_{\\mathrm{CDK}}]$. Consequently, the condition of crossing the threshold at *any* time in the interval is equivalent to the condition that the threshold is met or exceeded at the *end* of the interval. Therefore, the cell arrests if and only if $p(T_{\\mathrm{CDK}}) \\ge \\theta$.\n\nThe number of bursts $K(T_{\\mathrm{CDK}})$ over the interval $[0, T_{\\mathrm{CDK}}]$ follows a Poisson distribution with mean parameter $\\mu = \\lambda T_{\\mathrm{CDK}}$. The probability mass function for $K \\equiv K(T_{\\mathrm{CDK}})$ is:\n$$\nP(K=k) = \\frac{e^{-\\mu} \\mu^k}{k!} \\quad \\text{for } k = 0, 1, 2, \\dots\n$$\n\nThe arrest condition $p(T_{\\mathrm{CDK}}) \\ge \\theta$ can be expressed in terms of the number of bursts $K$:\n$$\np_0 + K \\cdot a \\ge \\theta\n$$\nWe seek the probability that $K$ satisfies this inequality.\n\nFirst, we address several edge cases:\n$1$. If the initial concentration is already at or above the threshold, $p_0 \\ge \\theta$, arrest occurs at $t=0$. Since $0 \\in [0, T_{\\mathrm{CDK}}]$ (for $T_{\\mathrm{CDK}} \\ge 0$), the arrest is certain and its probability is $1$.\n$2$. If the decision window has zero duration, $T_{\\mathrm{CDK}} = 0$, no bursts can occur within the interval. The number of bursts $K(0)$ is $0$ with probability $1$. Arrest occurs only if $p_0 \\ge \\theta$. If $p_0 < \\theta$, the probability of arrest is $0$.\n$3$. If the burst increment is non-positive, $a \\le 0$, and the initial concentration is below threshold, $p_0 < \\theta$, the p21 level can never increase to reach $\\theta$. The probability of arrest is $0$.\n\nFor the general case where $p_0 < \\theta$, $T_{\\mathrm{CDK}} > 0$, and $a > 0$, we rearrange the inequality for $K$:\n$$\nK \\cdot a \\ge \\theta - p_0\n$$\n$$\nK \\ge \\frac{\\theta - p_0}{a}\n$$\nSince the number of bursts $K$ must be an integer, the cell arrests if the number of bursts is at least $N_{req}$, where $N_{req}$ is the minimum integer satisfying the condition. This is given by the ceiling function:\n$$\nN_{req} = \\left\\lceil \\frac{\\theta - p_0}{a} \\right\\rceil\n$$\nSince $p_0 < \\theta$ and $a > 0$, we are assured that $N_{req} \\ge 1$.\n\nThe probability of arrest is the probability that the random variable $K$ is greater than or equal to $N_{req}$, i.e., $P(\\text{arrest}) = P(K \\ge N_{req})$. It is computationally more convenient to express this using the complementary event, which is that the number of bursts is less than $N_{req}$.\n$$\nP(K \\ge N_{req}) = 1 - P(K < N_{req}) = 1 - \\sum_{k=0}^{N_{req}-1} P(K=k)\n$$\nSubstituting the Poisson probability mass function with $\\mu = \\lambda T_{\\mathrm{CDK}}$:\n$$\nP(\\text{arrest}) = 1 - \\sum_{k=0}^{N_{req}-1} \\frac{e^{-\\lambda T_{\\mathrm{CDK}}} (\\lambda T_{\\mathrm{CDK}})^k}{k!}\n$$\nThis expression is the survival function (SF) of the Poisson distribution evaluated at $N_{req}-1$. Modern scientific libraries provide stable numerical implementations for this function, which is preferable to summing the terms manually, especially for large $\\mu$ or $N_{req}$. The algorithm is thus:\n$1$. Check if $p_0 \\ge \\theta$. If so, the probability is $1$.\n$2$. Check if $T_{\\mathrm{CDK}} \\le 0$ or $a \\le 0$. If so, and $p_0 < \\theta$, the probability is $0$.\n$3$. Otherwise, calculate $N_{req} = \\lceil (\\theta - p_0) / a \\rceil$ and $\\mu = \\lambda T_{\\mathrm{CDK}}$.\n$4$. Compute the probability $P(K \\ge N_{req})$ for a Poisson-distributed variable $K$ with mean $\\mu$. This is equivalent to the survival function $SF(N_{req}-1, \\mu)$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import poisson\n\ndef solve():\n    \"\"\"\n    Calculates the cell cycle arrest probability for a series of test cases based on a \n    stochastic hybrid model.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (lambda, T_CDK, a, theta, p_0, gamma)\n        # Case 1 (general case, negligible decay)\n        (2.0, 1.0, 0.5, 2.0, 0.2, 0.05),\n        # Case 2 (baseline already above threshold)\n        (0.1, 0.5, 0.3, 2.0, 2.5, 0.01),\n        # Case 3 (zero decision window)\n        (3.0, 0.0, 0.2, 0.3, 0.1, 0.02),\n        # Case 4 (low burst rate over moderate window)\n        (0.1, 3.0, 0.5, 1.2, 0.0, 0.02),\n        # Case 5 (high burst rate, short window)\n        (5.0, 0.5, 0.4, 2.2, 0.1, 0.05),\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        lambda_rate, T_CDK, a, theta, p_0, _ = case # gamma is not used in the calculation\n\n        # Edge Case 1: Initial p21 level is already at or above the threshold.\n        # Arrest occurs immediately (t=0), so the probability is 1.0.\n        if p_0 >= theta:\n            results.append(1.0)\n            continue\n\n        # Edge Case 2: The threshold cannot be reached from the initial state.\n        # This occurs if the decision window has zero duration (no time for bursts)\n        # or if the burst increment 'a' is non-positive (p21 level cannot increase).\n        if T_CDK <= 0 or a <= 0:\n            results.append(0.0)\n            continue\n\n        # General case calculation\n        # The number of bursts, K, in the interval [0, T_CDK] follows a\n        # Poisson distribution with mean mu = lambda * T_CDK.\n        mu = lambda_rate * T_CDK\n        \n        # Calculate the minimum number of bursts, k_req, required to\n        # make the p21 level cross the threshold theta.\n        # The condition is: p_0 + k_req * a >= theta\n        # This gives: k_req >= (theta - p_0) / a\n        # Since k_req must be an integer, we take the ceiling.\n        # At this point in the logic, we know p_0 < theta and a > 0.\n        p_needed = theta - p_0\n        k_req = int(np.ceil(p_needed / a))\n        \n        # The probability of arrest is the probability that the number of bursts K\n        # is greater than or equal to k_req.\n        # P(arrest) = P(K >= k_req)\n        # This is equivalent to 1 - P(K < k_req) = 1 - CDF(k_req - 1).\n        # A more numerically stable way to compute this is using the\n        # survival function (sf), where sf(k, mu) = P(K > k).\n        # We need P(K >= k_req), which is equivalent to P(K > k_req - 1).\n        # The input to sf must be an integer, `k_req - 1`.\n        prob_arrest = poisson.sf(k_req - 1, mu)\n        results.append(prob_arrest)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3293531"}, {"introduction": "A quantitative model is only as good as its parameters, and understanding their uncertainty is critical for making robust predictions. This computational exercise focuses on the crucial interface between a model and experimental data, using the Fisher Information Matrix to analyze parameter identifiability [@problem_id:3293565]. You will implement an analysis pipeline to estimate confidence intervals and correlations for the parameters of a classic Hill function, a key skill for any modeler seeking to connect theory with real-world measurements.", "problem": "Consider a minimal ultrasensitive activation module within a cell cycle regulatory network that approximates the steady-state fraction of active Cyclin-dependent kinase (CDK) as a Hill-type input-output function. Let the observable be $y(x; \\boldsymbol{\\theta})$, where the input $x$ is the cyclin concentration and the parameter vector is $\\boldsymbol{\\theta} = (n, K)$. The output is defined by a normalized maximal activation $V$ and the ultrasensitivity parameters $n$ (Hill coefficient, dimensionless) and $K$ (half-activation concentration, in nanomolar). The experiment consists of independent measurements at fixed inputs with additive noise that is well approximated by independent and identically distributed Gaussian measurement noise with zero mean and constant variance. Use the following fundamental bases:\n- For independent observations with additive Gaussian noise, the log-likelihood is the sum of squared residuals scaled by the inverse variance, and the Fisher Information Matrix (FIM) aggregates the sensitivity of the model output with respect to parameters over the measurement set.\n- Under standard regularity conditions, the asymptotic covariance of a regular maximum likelihood estimator is the inverse of the FIM, and approximate two-sided confidence intervals can be computed using the asymptotic normal approximation.\n\nModel:\n- The observable is given by $y(x; n, K) = V \\dfrac{x^{n}}{K^{n} + x^{n}}$, with $V$ fixed and known.\n- Measurements at inputs $\\{x_i\\}$ are corrupted by independent additive Gaussian noise of variance $\\sigma^{2}$.\n- All inputs $x_i$ are strictly positive reals and all parameters satisfy $n > 0$ and $K > 0$.\n\nTasks:\n1. Starting from the model definition and the Gaussian noise assumption, derive the Fisher Information Matrix $I(\\boldsymbol{\\theta})$ for $\\boldsymbol{\\theta} = (n, K)$ in terms of the gradient of $y(x; \\boldsymbol{\\theta})$ with respect to $n$ and $K$, aggregated across the given measurement design. Use the independence of measurements and the constant variance assumption to express $I(\\boldsymbol{\\theta})$ as a sum over inputs.\n2. Use the asymptotic normal approximation to compute the approximate covariance matrix $\\Sigma(\\boldsymbol{\\theta})$ as the inverse (or Moore-Penrose pseudo-inverse if non-invertible) of $I(\\boldsymbol{\\theta})$.\n3. From $\\Sigma(\\boldsymbol{\\theta})$, compute the approximate two-sided $95\\%$ confidence intervals for $n$ (dimensionless) and $K$ (in nanomolar), using the normal quantile $z$-value $z_{0.975} = 1.96$; express the $K$ intervals in nanomolar (nM) and the $n$ intervals as dimensionless numbers.\n4. Compute the approximate Pearson correlation coefficient between $n$ and $K$ implied by $\\Sigma(\\boldsymbol{\\theta})$.\n5. Implement a program that performs these computations for the specified test suite and outputs the results in the exact format described below.\n\nAssumptions and computation details:\n- Replicated measurements at the same input $x_i$ can be represented by multiplying their contribution to the Fisher Information Matrix by the number of replicates at that input.\n- If the Fisher Information Matrix is not invertible or is ill-conditioned, use the Moore-Penrose pseudo-inverse to obtain $\\Sigma(\\boldsymbol{\\theta})$.\n- All logarithms are natural logarithms.\n\nTest suite:\n- Case $1$ (balanced design, moderate ultrasensitivity):\n    - $V = 1$\n    - $n = 4$\n    - $K = 50$ nM\n    - $\\sigma = 0.05$ (dimensionless, since $y$ is a fraction)\n    - Inputs $x_i$ in nM: $[10, 30, 50, 70, 100]$\n    - Replicates per input: $m = 3$\n- Case $2$ (low dynamic range, below threshold, identifiability stress-test):\n    - $V = 1$\n    - $n = 4$\n    - $K = 50$ nM\n    - $\\sigma = 0.05$\n    - Inputs $x_i$ in nM: $[5, 8, 12, 16]$\n    - Replicates per input: $m = 2$\n- Case $3$ (high ultrasensitivity, fewer inputs, higher noise):\n    - $V = 1$\n    - $n = 10$\n    - $K = 50$ nM\n    - $\\sigma = 0.10$\n    - Inputs $x_i$ in nM: $[45, 50, 55]$\n    - Replicates per input: $m = 5$\n\nRequired outputs per case:\n- The matrix elements of the Fisher Information Matrix for the parameter ordering $(n, K)$: $I_{nn}$, $I_{nK}$, and $I_{KK}$ (floats).\n- The lower and upper bounds of the approximate two-sided $95\\%$ confidence interval for $n$ (floats, dimensionless).\n- The lower and upper bounds of the approximate two-sided $95\\%$ confidence interval for $K$ (floats, in nanomolar).\n- The approximate Pearson correlation coefficient between $n$ and $K$ implied by $\\Sigma(\\boldsymbol{\\theta})$ (float).\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets with no whitespace, where each case is represented by a list of eight floats in the order $[I_{nn}, I_{nK}, I_{KK}, n_{\\text{low}}, n_{\\text{high}}, K_{\\text{low}}, K_{\\text{high}}, \\rho_{nK}]$, and the overall output is the list of the three case-lists. For example: $[[1.0,0.0,2.0,3.0,4.0,5.0,6.0,0.1],[...],[...]]$.\n\nAll quantities that carry physical units must be expressed in the specified units: $K$ in nanomolar (nM), $x$ in nanomolar (nM), and $y$ as a dimensionless fraction. The angles do not apply. Percentages must be expressed as decimals when needed, but the confidence intervals are intervals of parameter values rather than percentages.\n\nYour task is to derive the necessary expressions and implement a program that computes these outputs for the provided test suite. The final program must not require any user input and must strictly follow the final output format described above.", "solution": "The problem is well-defined, scientifically grounded in the principles of biochemical modeling and statistical parameter estimation, and internally consistent. It poses a standard, albeit computationally intensive, task of Fisher Information Analysis for a common biological model. Therefore, I will proceed with a full solution.\n\nThe core of the task is to perform a Fisher Information Analysis for the parameters $\\boldsymbol{\\theta} = (n, K)$ of a Hill-type function describing the steady-state activation of a Cyclin-dependent kinase (CDK). The analysis is based on a specified experimental design, which consists of a set of cyclin concentrations $\\{x_i\\}$ where measurements are taken, and a given noise model.\n\nThe model for the observable, the fraction of active CDK, is given by:\n$$ y(x; n, K) = V \\frac{x^{n}}{K^{n} + x^{n}} $$\nwhere $V$ is a known fixed maximal activation, $n$ is the Hill coefficient, and $K$ is the half-activation concentration. The measurements are assumed to be corrupted by independent and identically distributed (i.i.d.) additive Gaussian noise with mean $0$ and constant variance $\\sigma^2$.\n\n### 1. Derivation of the Fisher Information Matrix (FIM)\n\nThe Fisher Information Matrix (FIM), $I(\\boldsymbol{\\theta})$, quantifies the amount of information that a set of observations carries about the unknown parameters $\\boldsymbol{\\theta}$. For a model with i.i.d. additive Gaussian noise of constant variance $\\sigma^2$, the element $(j,k)$ of the FIM is given by:\n$$ I_{jk}(\\boldsymbol{\\theta}) = \\frac{1}{\\sigma^2} \\sum_{i=1}^{N} \\frac{\\partial y(x_i; \\boldsymbol{\\theta})}{\\partial \\theta_j} \\frac{\\partial y(x_i; \\boldsymbol{\\theta})}{\\partial \\theta_k} $$\nwhere the sum is over all $N$ individual measurements. The problem specifies an experimental design with $M$ unique inputs $\\{x_i\\}$, each with $m_i$ replicates. The formula can be rewritten as a sum over unique inputs:\n$$ I_{jk}(\\boldsymbol{\\theta}) = \\frac{1}{\\sigma^2} \\sum_{i=1}^{M} m_i \\frac{\\partial y(x_i; \\boldsymbol{\\theta})}{\\partial \\theta_j} \\frac{\\partial y(x_i; \\boldsymbol{\\theta})}{\\partial \\theta_k} $$\nOur parameter vector is $\\boldsymbol{\\theta} = (\\theta_1, \\theta_2) = (n, K)$. We must compute the partial derivatives of $y$ with respect to $n$ and $K$.\n\n**Partial Derivative with respect to $n$:**\nTo find $\\frac{\\partial y}{\\partial n}$, we can use logarithmic differentiation.\n$$ \\ln y = \\ln V + n \\ln x - \\ln(K^n + x^n) $$\nDifferentiating with respect to $n$:\n$$ \\frac{1}{y} \\frac{\\partial y}{\\partial n} = \\ln x - \\frac{1}{K^n + x^n} (K^n \\ln K + x^n \\ln x) = \\frac{(K^n+x^n)\\ln x - (K^n \\ln K + x^n \\ln x)}{K^n+x^n} = \\frac{K^n(\\ln x - \\ln K)}{K^n+x^n} $$\n$$ \\frac{\\partial y}{\\partial n} = y \\cdot \\frac{K^n(\\ln x - \\ln K)}{K^n+x^n} = \\left(V \\frac{x^n}{K^n+x^n}\\right) \\frac{K^n(\\ln x - \\ln K)}{K^n+x^n} = V \\frac{x^n K^n (\\ln x - \\ln K)}{(K^n+x^n)^2} $$\n\n**Partial Derivative with respect to $K$:**\nWe differentiate $y = V x^n (K^n + x^n)^{-1}$ with respect to $K$ using the chain rule.\n$$ \\frac{\\partial y}{\\partial K} = V x^n \\cdot (-1)(K^n + x^n)^{-2} \\cdot \\frac{\\partial}{\\partial K}(K^n + x^n) = -V x^n (K^n + x^n)^{-2} (n K^{n-1}) $$\n$$ \\frac{\\partial y}{\\partial K} = -V \\frac{n K^{n-1} x^n}{(K^n+x^n)^2} $$\n\nWith these derivatives, the three unique elements of the $2 \\times 2$ symmetric FIM are:\n$$ I_{nn} = \\frac{1}{\\sigma^2} \\sum_{i} m_i \\left( V \\frac{x_i^n K^n (\\ln x_i - \\ln K)}{(K^n+x_i^n)^2} \\right)^2 $$\n$$ I_{KK} = \\frac{1}{\\sigma^2} \\sum_{i} m_i \\left( -V \\frac{n K^{n-1} x_i^n}{(K^n+x_i^n)^2} \\right)^2 $$\n$$ I_{nK} = \\frac{1}{\\sigma^2} \\sum_{i} m_i \\left( V \\frac{x_i^n K^n (\\ln x_i - \\ln K)}{(K^n+x_i^n)^2} \\right) \\left( -V \\frac{n K^{n-1} x_i^n}{(K^n+x_i^n)^2} \\right) $$\n\n### 2. The Asymptotic Covariance Matrix\n\nUnder standard regularity conditions, the asymptotic covariance matrix of the maximum likelihood estimators for $\\boldsymbol{\\theta}$, denoted $\\Sigma(\\boldsymbol{\\theta})$, is the inverse of the Fisher Information Matrix:\n$$ \\Sigma(\\boldsymbol{\\theta}) = I(\\boldsymbol{\\theta})^{-1} = \\begin{pmatrix} \\sigma_n^2 & \\sigma_{nK} \\\\ \\sigma_{Kn} & \\sigma_K^2 \\end{pmatrix} $$\nFor a $2 \\times 2$ matrix $I = \\begin{pmatrix} a & b \\\\ b & c \\end{pmatrix}$, the inverse is $I^{-1} = \\frac{1}{ac-b^2} \\begin{pmatrix} c & -b \\\\ -b & a \\end{pmatrix}$.\nIf the FIM is singular or ill-conditioned (i.e., its determinant $I_{nn}I_{KK} - I_{nK}^2$ is zero or close to zero), the standard inverse does not exist or is numerically unstable. In such cases, the Moore-Penrose pseudo-inverse is used, as specified in the problem.\n\n### 3. Approximate Confidence Intervals\n\nThe asymptotic normality of maximum likelihood estimators implies that for a large number of measurements, the estimator $\\hat{\\theta}_j$ is approximately normally distributed with mean equal to the true value $\\theta_j$ and variance equal to the corresponding diagonal element of the covariance matrix, $\\Sigma_{jj}$.\nThe standard error (SE) of the estimator for $\\theta_j$ is $SE(\\hat{\\theta}_j) = \\sqrt{\\Sigma_{jj}}$.\nAn approximate two-sided $(1-\\alpha) \\times 100\\%$ confidence interval (CI) for $\\theta_j$ is given by:\n$$ \\theta_j \\pm z_{1-\\alpha/2} \\cdot SE(\\hat{\\theta}_j) $$\nFor a $95\\%$ confidence interval, $\\alpha = 0.05$, and we are given the normal quantile $z_{1-\\alpha/2} = z_{0.975} = 1.96$.\nThe confidence intervals for $n$ and $K$ are therefore:\n$$ \\text{CI}_n = [n - 1.96 \\sqrt{\\Sigma_{nn}}, n + 1.96 \\sqrt{\\Sigma_{nn}}] $$\n$$ \\text{CI}_K = [K - 1.96 \\sqrt{\\Sigma_{KK}}, K + 1.96 \\sqrt{\\Sigma_{KK}}] $$\nwhere $\\Sigma_{nn}$ and $\\Sigma_{KK}$ are the diagonal elements of the (pseudo-)inverted FIM.\n\n### 4. Pearson Correlation Coefficient\n\nThe Pearson correlation coefficient between the estimators for $n$ and $K$, denoted $\\rho_{nK}$, measures the linear correlation between them. It is derived from the covariance matrix:\n$$ \\rho_{nK} = \\frac{\\text{cov}(\\hat{n}, \\hat{K})}{\\sqrt{\\text{var}(\\hat{n}) \\text{var}(\\hat{K})}} = \\frac{\\Sigma_{nK}}{\\sqrt{\\Sigma_{nn} \\Sigma_{KK}}} $$\nwhere $\\Sigma_{nK}$ is the off-diagonal element of the covariance matrix. This value ranges from $-1$ to $1$, indicating perfect negative or positive linear correlation, respectively. A value near $0$ indicates little to no linear correlation. High absolute values of correlation are often a sign of poor parameter identifiability from the given experimental data.\n\n### Implementation Strategy\n\nThe following program will implement these calculations. For each test case, it will:\n1.  Iterate through the unique input concentrations $x_i$.\n2.  For each $x_i$, compute the partial derivatives $\\frac{\\partial y}{\\partial n}$ and $\\frac{\\partial y}{\\partial K}$ using the derived formulas.\n3.  Calculate the contribution of this input point to each element of the FIM, scaled by the number of replicates $m_i$ and inverse variance $1/\\sigma^2$.\n4.  Sum these contributions to obtain the final FIM, $I(\\boldsymbol{\\theta})$.\n5.  Compute the covariance matrix $\\Sigma$ by taking the Moore-Penrose pseudo-inverse of $I(\\boldsymbol{\\theta})$ using `numpy.linalg.pinv`.\n6.  Extract the variances from the diagonal of $\\Sigma$ to calculate the standard errors and confidence intervals.\n7.  Extract the off-diagonal element of $\\Sigma$ to calculate the Pearson correlation coefficient.\n8.  Collect and format the eight required output values for each case.\nThe use of the natural logarithm `numpy.log` is necessary. To avoid numerical overflow with large $n$, expressions like $x^n / (K^n + x^n)$ are best computed as $1 / ((K/x)^n + 1)$, which is numerically more stable.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the Fisher Information Analysis problem for the given test cases.\n    \"\"\"\n\n    test_cases = [\n        # Case 1 (balanced design, moderate ultrasensitivity)\n        {\n            \"V\": 1.0, \"n\": 4.0, \"K\": 50.0, \"sigma\": 0.05,\n            \"inputs\": [10.0, 30.0, 50.0, 70.0, 100.0],\n            \"replicates\": 3\n        },\n        # Case 2 (low dynamic range, below threshold)\n        {\n            \"V\": 1.0, \"n\": 4.0, \"K\": 50.0, \"sigma\": 0.05,\n            \"inputs\": [5.0, 8.0, 12.0, 16.0],\n            \"replicates\": 2\n        },\n        # Case 3 (high ultrasensitivity, fewer inputs, higher noise)\n        {\n            \"V\": 1.0, \"n\": 10.0, \"K\": 50.0, \"sigma\": 0.10,\n            \"inputs\": [45.0, 50.0, 55.0],\n            \"replicates\": 5\n        }\n    ]\n\n    all_results = []\n    z_value = 1.96\n\n    for case in test_cases:\n        V = case[\"V\"]\n        n = case[\"n\"]\n        K = case[\"K\"]\n        sigma = case[\"sigma\"]\n        inputs = case[\"inputs\"]\n        m = case[\"replicates\"]\n\n        fim = np.zeros((2, 2))\n\n        for x in inputs:\n            # To avoid potential overflow for large n, we can rewrite terms.\n            # (K/x)^n can be problematic if K >> x.\n            # x/K can be problematic if x >> K.\n            # We use try-except for extreme cases, though with given inputs it's not strictly necessary.\n            try:\n                log_x = np.log(x)\n                log_K = np.log(K)\n                K_over_x_n = (K / x)**n\n                \n                # Check for infinity\n                if np.isinf(K_over_x_n):\n                    # This happens when x is very small.\n                    # y -> 0, dy/dn -> 0, dy/dk -> 0\n                    dy_dn = 0.0\n                    dy_dk = 0.0\n                else:\n                    denominator_term = K_over_x_n + 1.0\n                    \n                    # Common factor in derivatives: V * (K^n * x^n) / (K^n + x^n)^2\n                    # = V * ( (K/x)^n / ( (K/x)^n + 1 )^2 )\n                    common_factor = V * K_over_x_n / (denominator_term**2)\n\n                    # Partial derivative w.r.t n\n                    dy_dn = common_factor * (log_x - log_K)\n                    \n                    # Partial derivative w.r.t K\n                    dy_dk = -common_factor * (n / K)\n\n            except (OverflowError, ZeroDivisionError):\n                dy_dn = 0.0\n                dy_dk = 0.0\n\n            # Jacobian (sensitivity vector) for this data point\n            grad_y = np.array([dy_dn, dy_dk])\n            \n            # Add to FIM. (1/sigma^2) is factored in at the end.\n            fim += m * np.outer(grad_y, grad_y)\n\n        fim /= sigma**2\n\n        I_nn, I_nk = fim[0, 0], fim[0, 1]\n        I_kk = fim[1, 1]\n\n        # Use Moore-Penrose pseudo-inverse for stability\n        cov_matrix = np.linalg.pinv(fim)\n\n        var_n = cov_matrix[0, 0]\n        var_k = cov_matrix[1, 1]\n        cov_nk = cov_matrix[0, 1]\n\n        # Ensure variances are non-negative to avoid domain errors\n        se_n = np.sqrt(max(0, var_n))\n        se_k = np.sqrt(max(0, var_k))\n\n        # 95% Confidence Intervals\n        n_low = n - z_value * se_n\n        n_high = n + z_value * se_n\n        k_low = K - z_value * se_k\n        k_high = K + z_value * se_k\n\n        # Pearson Correlation Coefficient\n        denom_corr = se_n * se_k\n        if denom_corr == 0:\n            rho_nk = 0.0\n        else:\n            rho_nk = cov_nk / denom_corr\n\n        case_results = [\n            I_nn, I_nk, I_kk,\n            n_low, n_high,\n            k_low, k_high,\n            rho_nk\n        ]\n        all_results.append(case_results)\n\n    # Format the final output string\n    # E.g., [[val1,val2,...],[val1,val2,...]]\n    output_str_parts = []\n    for res_list in all_results:\n        res_str = f\"[{','.join(f'{x:.12g}' for x in res_list)}]\"\n        output_str_parts.append(res_str)\n    \n    final_output = f\"[{','.join(output_str_parts)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "3293565"}]}