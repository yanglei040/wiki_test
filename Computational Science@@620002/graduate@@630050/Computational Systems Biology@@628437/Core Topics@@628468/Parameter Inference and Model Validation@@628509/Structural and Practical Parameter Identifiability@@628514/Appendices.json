{"hands_on_practices": [{"introduction": "The first step in understanding identifiability is often to analyze the simplest case: a system at steady state. This exercise explores structural identifiability using only algebraic relationships derived from a model of a two-site phosphorylation cycle [@problem_id:3352624]. By setting the time derivatives to zero and applying the conservation law, you will discover that even with limited steady-state data, it is possible to uniquely determine specific combinations of rate constants, which in this case correspond to equilibrium constants. This practice builds foundational skills in manipulating model equations to reveal their inherent structural properties without relying on complex dynamic data or computational tools.", "problem": "Consider a two-site phosphorylation cycle for a single substrate in a well-mixed cellular compartment. The substrate exists in three modification states: unphosphorylated $S_{0}$, singly phosphorylated $S_{1}$, and doubly phosphorylated $S_{2}$. Effective first-order mass-action transitions describe phosphorylation and dephosphorylation between adjacent states:\n$$S_{0} \\xrightleftharpoons[k_{-1}]{k_{1}} S_{1} \\xrightleftharpoons[k_{-2}]{k_{2}} S_{2},$$\nwhere $k_{1}$ and $k_{2}$ are forward rate constants for phosphorylation and $k_{-1}$ and $k_{-2}$ are reverse rate constants for dephosphorylation. Assume all rate constants are strictly positive. The total substrate is conserved:\n$$S_{T} = S_{0} + S_{1} + S_{2}.$$\nAt steady state, $S_{1}^{\\ast}$ and $S_{2}^{\\ast}$ are experimentally measurable, and $S_{T}$ is known. No time-course data is available; only steady-state is observed. Using the definitions of mass-action kinetics, the steady-state condition, and the conservation law, derive which combinations of the forward and reverse rate constants are structurally identifiable from the steady-state measurements $(S_{1}^{\\ast}, S_{2}^{\\ast}, S_{T})$. Express your final answer as analytic expressions in terms of the measurable steady-state fractions\n$$x_{1} = \\frac{S_{1}^{\\ast}}{S_{T}}, \\quad x_{2} = \\frac{S_{2}^{\\ast}}{S_{T}},$$\nand provide the identifiable combinations as a single row matrix containing closed-form expressions. No numerical approximation is required, and no units are to be included in the final answer box.", "solution": "The user wants me to identify which combinations of rate constants are structurally identifiable from steady-state measurements in a two-site phosphorylation model.\n\n### Step 1: Extract Givens\n- **Model:** A two-site phosphorylation cycle with states $S_{0}$, $S_{1}$, and $S_{2}$.\n- **Reactions:** $S_{0} \\xrightleftharpoons[k_{-1}]{k_{1}} S_{1} \\xrightleftharpoons[k_{-2}]{k_{2}} S_{2}$.\n- **Kinetics:** Effective first-order mass-action.\n- **Rate Constants:** $k_{1}, k_{-1}, k_{2}, k_{-2}$ are all strictly positive real numbers.\n- **Conservation Law:** The total substrate concentration $S_{T}$ is conserved: $S_{T} = S_{0} + S_{1} + S_{2}$.\n- **Measurements:** Steady-state concentrations $S_{1}^{\\ast}$ and $S_{2}^{\\ast}$ are measurable, and $S_{T}$ is known.\n- **Data Condition:** Only steady-state data is available; no time-course data.\n- **Objective:** Derive the structurally identifiable combinations of the rate constants in terms of the measurable steady-state fractions $x_{1} = \\frac{S_{1}^{\\ast}}{S_{T}}$ and $x_{2} = \\frac{S_{2}^{\\ast}}{S_{T}}$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is evaluated against the validation criteria.\n\n- **Scientifically Grounded:** The problem describes a canonical model in systems biology, the futile cycle or phosphorylation chain. The use of mass-action kinetics and conservation laws are fundamental principles. The model is scientifically sound and widely used.\n- **Well-Posed:** The problem is well-posed. It asks for structurally identifiable parameter combinations, which is a standard and well-defined question in parameter estimation and systems identification. A unique solution for these combinations exists based on the provided information.\n- **Objective:** The problem is stated using precise, objective, and formal mathematical language, free from ambiguity or subjective claims.\n- **Other Flaws:**\n  - The problem does not violate any scientific principles.\n  - It is directly relevant to the topic of structural identifiability in computational systems biology.\n  - The setup is complete and consistent, providing all necessary information (reactions, kinetics, conservation law, and measurements) to solve the problem.\n  - The assumptions are standard in the field and do not represent unrealistic or infeasible conditions.\n  - The terminology is clearly defined, and the structure of the problem is logical.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. I will proceed to derive the solution.\n\nTo determine the structurally identifiable parameter combinations, we begin by writing the system of ordinary differential equations (ODEs) that describe the dynamics according to the law of mass-action. Let $S_{0}(t)$, $S_{1}(t)$, and $S_{2}(t)$ be the concentrations of the respective species at time $t$.\n\nThe system of ODEs is:\n$$\n\\frac{dS_{0}}{dt} = -k_{1}S_{0} + k_{-1}S_{1} \\\\\n\\frac{dS_{1}}{dt} = k_{1}S_{0} - k_{-1}S_{1} - k_{2}S_{1} + k_{-2}S_{2} \\\\\n\\frac{dS_{2}}{dt} = k_{2}S_{1} - k_{-2}S_{2}\n$$\nAt steady state, the concentrations of all species are constant, so their time derivatives are zero. Let the steady-state concentrations be denoted by $S_{0}^{\\ast}$, $S_{1}^{\\ast}$, and $S_{2}^{\\ast}$.\n$$\n\\frac{dS_{0}}{dt} = 0 \\implies -k_{1}S_{0}^{\\ast} + k_{-1}S_{1}^{\\ast} = 0 \\quad (1) \\\\\n\\frac{dS_{1}}{dt} = 0 \\implies k_{1}S_{0}^{\\ast} - k_{-1}S_{1}^{\\ast} - k_{2}S_{1}^{\\ast} + k_{-2}S_{2}^{\\ast} = 0 \\quad (2) \\\\\n\\frac{dS_{2}}{dt} = 0 \\implies k_{2}S_{1}^{\\ast} - k_{-2}S_{2}^{\\ast} = 0 \\quad (3)\n$$\nNote that the sum of these three equations is zero, $\\frac{d(S_{0}+S_{1}+S_{2})}{dt} = 0$, which is consistent with the conservation law. This means that only two of the three equations are algebraically independent. We will use equations $(1)$ and $(3)$.\n\nFrom equation $(1)$, we can express a ratio of rate constants in terms of steady-state concentrations:\n$$k_{1}S_{0}^{\\ast} = k_{-1}S_{1}^{\\ast} \\implies \\frac{k_{1}}{k_{-1}} = \\frac{S_{1}^{\\ast}}{S_{0}^{\\ast}}$$\nFrom equation $(3)$, we find another ratio of rate constants:\n$$k_{2}S_{1}^{\\ast} = k_{-2}S_{2}^{\\ast} \\implies \\frac{k_{2}}{k_{-2}} = \\frac{S_{2}^{\\ast}}{S_{1}^{\\ast}}$$\nThe problem states that $S_{1}^{\\ast}$, $S_{2}^{\\ast}$, and the total concentration $S_{T}$ are known quantities. The concentration $S_{0}^{\\ast}$ is not directly measured but can be determined from the conservation law:\n$$S_{T} = S_{0}^{\\ast} + S_{1}^{\\ast} + S_{2}^{\\ast} \\implies S_{0}^{\\ast} = S_{T} - S_{1}^{\\ast} - S_{2}^{\\ast}$$\nNow we substitute this expression for $S_{0}^{\\ast}$ into our equation for the first rate constant ratio:\n$$\\frac{k_{1}}{k_{-1}} = \\frac{S_{1}^{\\ast}}{S_{T} - S_{1}^{\\ast} - S_{2}^{\\ast}}$$\nThe problem requires the answer to be in terms of the measurable fractions $x_{1} = \\frac{S_{1}^{\\ast}}{S_{T}}$ and $x_{2} = \\frac{S_{2}^{\\ast}}{S_{T}}$. To achieve this, we divide the numerator and denominator of our expressions by the known total concentration $S_{T}$.\n\nFor the first ratio:\n$$\\frac{k_{1}}{k_{-1}} = \\frac{S_{1}^{\\ast}/S_{T}}{(S_{T} - S_{1}^{\\ast} - S_{2}^{\\ast})/S_{T}} = \\frac{S_{1}^{\\ast}/S_{T}}{1 - S_{1}^{\\ast}/S_{T} - S_{2}^{\\ast}/S_{T}} = \\frac{x_{1}}{1 - x_{1} - x_{2}}$$\nFor the second ratio:\n$$\\frac{k_{2}}{k_{-2}} = \\frac{S_{2}^{\\ast}}{S_{1}^{\\ast}} = \\frac{S_{2}^{\\ast}/S_{T}}{S_{1}^{\\ast}/S_{T}} = \\frac{x_{2}}{x_{1}}$$\nThese two expressions, $\\frac{k_{1}}{k_{-1}}$ and $\\frac{k_{2}}{k_{-2}}$, relate the parameters to the measurable quantities. Since we have two independent equations and four unknown parameters ($k_{1}, k_{-1}, k_{2}, k_{-2}$), we cannot solve for each parameter individually. However, we can uniquely determine the values of these two specific combinations of parameters. Therefore, the structurally identifiable combinations are precisely these two ratios. For any non-trivial steady state where $x_1 > 0$ and $1 - x_1 - x_2 > 0$, these expressions are well-defined. The problem stating that all rate constants are strictly positive ensures such a non-trivial steady state can exist.\n\nThe identifiable combinations are:\n$$\n\\text{Combination 1: } \\frac{k_{1}}{k_{-1}} = \\frac{x_{1}}{1 - x_{1} - x_{2}} \\\\\n\\text{Combination 2: } \\frac{k_{2}}{k_{-2}} = \\frac{x_{2}}{x_{1}}\n$$\nThese represent the equilibrium constants for the two reaction steps.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{x_{1}}{1 - x_{1} - x_{2}} & \\frac{x_{2}}{x_{1}}\n\\end{pmatrix}\n}\n$$", "id": "3352624"}, {"introduction": "While structural analysis tells us what is possible with perfect data, practical identifiability addresses the reality of finite, noisy measurements. This practice transitions from the structural to the practical domain by introducing a cornerstone of statistical inference: the Fisher Information Matrix (FIM) [@problem_id:3352673]. You will derive the FIM for a simple two-parameter model, learning how it quantifies the amount of information an experiment provides about unknown parameters. This exercise illuminates the FIM's connection to the Cramér-Rao lower bound, providing a powerful way to predict the best possible precision of parameter estimates and understand how experimental design, such as the choice of sampling times, directly impacts identifiability.", "problem": "Consider a two-parameter linear-in-parameters model for a measured output in computational systems biology, given by $y(t;\\theta)=\\theta_1 \\exp(-t)+\\theta_2 \\exp(-2 t)$, where $\\theta=\\begin{pmatrix}\\theta_1 & \\theta_2\\end{pmatrix}^{\\top}$ denotes unknown parameters. Suppose one collects $n$ independent observations at sampling times $\\{t_i\\}_{i=1}^{n}$, with measurement model $z_i=y(t_i;\\theta)+\\varepsilon_i$, where the noise $\\varepsilon_i$ is independent and identically distributed Gaussian with zero mean and variance $\\sigma^2$, i.e., $\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$.\n\nStarting from the definition of the Fisher Information Matrix (FIM) as the expected negative Hessian of the log-likelihood under independent Gaussian observations with known variance, derive the Fisher Information Matrix with respect to $\\theta$ for the described model. Then, using the relationship between the Fisher Information Matrix and the Cramér–Rao lower bound (CRLB), state a clear condition on the sampling times $\\{t_i\\}$ that ensures practical identifiability of $\\theta$ under this measurement design.\n\nAnswer specification: Your final reported quantity must be a single closed-form analytic expression for the determinant of the Fisher Information Matrix in terms of $\\{t_i\\}_{i=1}^{n}$ and $\\sigma^2$. No rounding is required, and no units are to be reported. Provide the identifiability condition in words within your derivation, but do not include it in the final boxed answer.", "solution": "The problem requires the derivation of the Fisher Information Matrix (FIM) for a given two-parameter model, the statement of the condition for practical parameter identifiability, and the calculation of the determinant of the FIM.\n\nThe model for the measured output is given by $y(t;\\theta) = \\theta_1 \\exp(-t) + \\theta_2 \\exp(-2t)$, where the parameter vector is $\\theta = \\begin{pmatrix} \\theta_1 & \\theta_2 \\end{pmatrix}^{\\top}$. The measurement model for $n$ independent observations at times $\\{t_i\\}_{i=1}^{n}$ is $z_i = y(t_i;\\theta) + \\varepsilon_i$, with noise $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$ being independent and identically distributed Gaussian with zero mean and known variance $\\sigma^2$.\n\nThe probability density function for a single measurement $z_i$ is given by the Gaussian distribution:\n$$p(z_i|\\theta, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(z_i - y(t_i;\\theta))^2}{2\\sigma^2}\\right)$$\nDue to the independence of the measurements, the likelihood function for the entire dataset $\\mathbf{z} = \\{z_i\\}_{i=1}^{n}$ is the product of the individual probabilities:\n$$\\mathcal{L}(\\theta; \\mathbf{z}) = \\prod_{i=1}^{n} p(z_i|\\theta, \\sigma^2) = \\left(2\\pi\\sigma^2\\right)^{-n/2} \\exp\\left(-\\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (z_i - y(t_i;\\theta))^2\\right)$$\nThe log-likelihood function, denoted $\\ell(\\theta)$, is:\n$$\\ell(\\theta) = \\ln \\mathcal{L}(\\theta; \\mathbf{z}) = -\\frac{n}{2} \\ln(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (z_i - y(t_i;\\theta))^2$$\nThe Fisher Information Matrix, $F$, is defined as the negative of the expectation of the Hessian matrix of the log-likelihood. The elements of the FIM are given by $F_{jk} = -E\\left[\\frac{\\partial^2 \\ell(\\theta)}{\\partial \\theta_j \\partial \\theta_k}\\right]$.\n\nFirst, we find the first partial derivatives of $\\ell(\\theta)$ with respect to the parameters $\\theta_j$:\n$$\\frac{\\partial \\ell}{\\partial \\theta_j} = -\\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} 2(z_i - y(t_i;\\theta)) \\left(-\\frac{\\partial y(t_i;\\theta)}{\\partial \\theta_j}\\right) = \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} (z_i - y(t_i;\\theta)) \\frac{\\partial y(t_i;\\theta)}{\\partial \\theta_j}$$\nNext, we find the second partial derivatives:\n$$\\frac{\\partial^2 \\ell}{\\partial \\theta_k \\partial \\theta_j} = \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} \\left[ \\left(-\\frac{\\partial y(t_i;\\theta)}{\\partial \\theta_k}\\right) \\frac{\\partial y(t_i;\\theta)}{\\partial \\theta_j} + (z_i - y(t_i;\\theta)) \\frac{\\partial^2 y(t_i;\\theta)}{\\partial \\theta_k \\partial \\theta_j} \\right]$$\nTaking the expectation, we use the fact that $E[z_i] = y(t_i;\\theta)$, which implies $E[z_i - y(t_i;\\theta)] = E[\\varepsilon_i] = 0$. This makes the second term in the summation vanish:\n$$E\\left[\\frac{\\partial^2 \\ell}{\\partial \\theta_k \\partial \\theta_j}\\right] = -\\frac{1}{\\sigma^2} \\sum_{i=1}^{n} \\frac{\\partial y(t_i;\\theta)}{\\partial \\theta_k} \\frac{\\partial y(t_i;\\theta)}{\\partial \\theta_j}$$\nTherefore, the elements of the FIM are:\n$$F_{jk} = -E\\left[\\frac{\\partial^2 \\ell}{\\partial \\theta_k \\partial \\theta_j}\\right] = \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} \\frac{\\partial y(t_i;\\theta)}{\\partial \\theta_j} \\frac{\\partial y(t_i;\\theta)}{\\partial \\theta_k}$$\nThis is the general formula for the FIM for a model with additive, i.i.d. Gaussian noise.\nFor our specific model, we calculate the partial derivatives of $y(t;\\theta)$ with respect to $\\theta_1$ and $\\theta_2$, which are the model sensitivities:\n$$\\frac{\\partial y(t;\\theta)}{\\partial \\theta_1} = \\exp(-t)$$\n$$\\frac{\\partial y(t;\\theta)}{\\partial \\theta_2} = \\exp(-2t)$$\nNote that since the model is linear in the parameters, these sensitivities are independent of $\\theta$.\n\nWe can now construct the $2 \\times 2$ FIM, $F$:\n$$F_{11} = \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} \\left(\\frac{\\partial y(t_i;\\theta)}{\\partial \\theta_1}\\right)^2 = \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} (\\exp(-t_i))^2 = \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} \\exp(-2t_i)$$\n$$F_{22} = \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} \\left(\\frac{\\partial y(t_i;\\theta)}{\\partial \\theta_2}\\right)^2 = \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} (\\exp(-2t_i))^2 = \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} \\exp(-4t_i)$$\n$$F_{12} = F_{21} = \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} \\frac{\\partial y(t_i;\\theta)}{\\partial \\theta_1} \\frac{\\partial y(t_i;\\theta)}{\\partial \\theta_2} = \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} \\exp(-t_i) \\exp(-2t_i) = \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} \\exp(-3t_i)$$\nThe Fisher Information Matrix is thus:\n$$F = \\frac{1}{\\sigma^2} \\begin{pmatrix} \\sum_{i=1}^{n} \\exp(-2t_i) & \\sum_{i=1}^{n} \\exp(-3t_i) \\\\ \\sum_{i=1}^{n} \\exp(-3t_i) & \\sum_{i=1}^{n} \\exp(-4t_i) \\end{pmatrix}$$\n\nThe Cramér-Rao Lower Bound (CRLB) establishes that the covariance matrix of any unbiased estimator $\\hat{\\theta}$ is bounded below by the inverse of the FIM, i.e., $\\text{Cov}(\\hat{\\theta}) \\ge F^{-1}$. Practical identifiability requires that parameters can be estimated with finite variance. This implies that the CRLB, $F^{-1}$, must exist and be finite. For $F^{-1}$ to exist, the FIM, $F$, must be invertible (non-singular). A matrix is invertible if and only if its determinant is non-zero. Since $\\sigma^2 > 0$, the condition $\\det(F) \\ne 0$ is equivalent to the matrix $S^\\top S$ being invertible, where $S$ is the $n \\times 2$ sensitivity matrix with columns corresponding to the sensitivities with respect to $\\theta_1$ and $\\theta_2$. Invertibility of $S^\\top S$ requires the columns of $S$ to be linearly independent.\nThe columns of $S$ are the vectors $(\\exp(-t_1), \\dots, \\exp(-t_n))^\\top$ and $(\\exp(-2t_1), \\dots, \\exp(-2t_n))^\\top$. These vectors are linearly dependent if and only if one is a scalar multiple of the other, i.e., $\\exp(-2t_i) = c \\cdot \\exp(-t_i)$ for some constant $c$ and for all $i=1, \\dots, n$. This simplifies to $\\exp(-t_i) = c$. Since $\\exp(-t)$ is a strictly monotonic function, this can only hold for all $i$ if all sampling times $t_i$ are identical. If all $t_i$ are the same, $\\det(F) = 0$, and the parameters are not practically identifiable. Therefore, a clear condition on the sampling times that ensures practical identifiability of $\\theta$ is that the set of sampling times $\\{t_i\\}_{i=1}^{n}$ must contain at least two distinct values.\n\nFinally, we compute the determinant of the FIM as requested. Using the property $\\det(kA) = k^d \\det(A)$ for a $d \\times d$ matrix, with $d=2$:\n$$\\det(F) = \\left(\\frac{1}{\\sigma^2}\\right)^2 \\det \\begin{pmatrix} \\sum_{i=1}^{n} \\exp(-2t_i) & \\sum_{i=1}^{n} \\exp(-3t_i) \\\\ \\sum_{i=1}^{n} \\exp(-3t_i) & \\sum_{i=1}^{n} \\exp(-4t_i) \\end{pmatrix}$$\n$$\\det(F) = \\frac{1}{\\sigma^4} \\left[ \\left(\\sum_{i=1}^{n} \\exp(-2t_i)\\right) \\left(\\sum_{i=1}^{n} \\exp(-4t_i)\\right) - \\left(\\sum_{i=1}^{n} \\exp(-3t_i)\\right)^2 \\right]$$\nThis is the required closed-form analytic expression for the determinant of the Fisher Information Matrix.", "answer": "$$\\boxed{\\frac{1}{\\sigma^4} \\left[ \\left(\\sum_{i=1}^{n} \\exp(-2t_i)\\right) \\left(\\sum_{i=1}^{n} \\exp(-4t_i)\\right) - \\left(\\sum_{i=1}^{n} \\exp(-3t_i)\\right)^2 \\right]}$$", "id": "3352673"}, {"introduction": "For complex, nonlinear systems, analytical identifiability analysis can become intractable. This final practice introduces a powerful and widely used computational approach to diagnose structural non-identifiability in realistic biological models [@problem_id:3352650]. You will implement an algorithm that uses sensitivity analysis and Singular Value Decomposition (SVD) to numerically discover parameter combinations that leave the model output unchanged. This exercise provides a versatile, hands-on toolkit for applying concepts from linear algebra to uncover a model's structural properties, demonstrating how to automate identifiability analysis for systems where manual derivation is not feasible.", "problem": "Consider deterministic ordinary differential equation systems of the form $x'(t) = f(x(t), \\theta)$ with output $y(t) = h(x(t), \\theta)$, where $\\theta \\in \\mathbb{R}^p$ denotes unknown parameters. Structural identifiability assesses whether distinct parameter vectors produce distinct outputs under ideal, noise-free, continuous-time observation. A parameter vector is said to be structurally identifiable if $y(t; \\theta)$ uniquely determines $\\theta$, and structurally unidentifiable if there exist nontrivial continuous transformations of $\\theta$ (invariance groups) that leave $y(t)$ unchanged. Practical identifiability further concerns the conditioning of the inverse mapping in the presence of finite, noisy data; here we focus on the structural aspect.\n\nA central concept is identifiable reparameterization: when $\\theta$ is not structurally identifiable, there exist functions $\\phi(\\theta)$ such that $y(t)$ depends on $\\theta$ only through $\\phi(\\theta)$, and the components of $\\phi(\\theta)$ are themselves structurally identifiable. Differentiating $y(t)$ with respect to $\\theta$ yields a sensitivity operator. Under standard assumptions (smoothness of $f$ and $h$, positive parameters and outputs), directions $v \\in \\mathbb{R}^p$ in parameter space that leave the output unchanged satisfy $S(t; \\theta) v = 0$ for all $t$ in the observation interval, where $S(t; \\theta)$ is the Jacobian of $y(t)$ with respect to $\\theta$. In mass-action and related biochemical systems, many invariance groups act multiplicatively on parameters. In such cases, working in logarithmic coordinates for parameters and outputs converts multiplicative transformations into additive ones; nullspace directions of the logarithmic sensitivity matrix indicate infinitesimal parameter transformations that do not change $\\ln y(t)$, and functions constant along these directions yield identifiable combinations.\n\nYour task is to design and implement an algorithm that discovers identifiable reparameterizations $\\phi(\\theta)$ from trajectory data by leveraging differential algebraic structure, operationalized numerically via logarithmic sensitivities and Singular Value Decomposition (SVD). The algorithm must:\n\n1. Simulate $y(t)$ over a fixed time grid for each model and parameter vector in a given test suite.\n2. Construct the logarithmic sensitivity matrix $S_{\\log}(\\theta)$ whose $(i,j)$ entry approximates $\\partial \\ln y(t_i; \\theta)/\\partial \\ln \\theta_j$ using small multiplicative perturbations of $\\theta_j$.\n3. Compute the right nullspace of $S_{\\log}(\\theta)$ via SVD. Each nullspace basis vector $v$ encodes an infinitesimal multiplicative invariance in parameter space that leaves the output unchanged. The dimension of the nullspace equals the number of structurally unidentifiable degrees of freedom.\n4. Construct a basis of identifiable monomial combinations by selecting a basis of the orthogonal complement of the nullspace in logarithmic coordinates; if $B$ is a full-rank matrix whose rows span the orthogonal complement, then the combinations $\\phi_j(\\theta) = \\exp\\left(B_j \\cdot \\ln \\theta\\right)$ are structurally identifiable. For interpretability, rationalize exponents to small integers when possible.\n5. Validate that the discovered nullspace basis contains the expected invariance directions for each model in the test suite. The validation must be numerical: treat two directions $v$ and $w$ as matching if the cosine of the angle between them (using Euclidean inner product) is at least $0.98$.\n\nYou must implement and test the algorithm on the following three scientifically realistic networks, each specified with fixed parameter values. All rates are in $1/\\text{time}$, concentrations and outputs are positive, and no unit conversion is required in the final outputs because all requested outputs are dimensionless booleans.\n\n- Test case A (gene expression with unobserved messenger ribonucleic acid (mRNA) and measurement gain): The state is $x(t) = (m(t), p(t))$. The dynamics are\n$$\n\\frac{dm}{dt} = k_{\\mathrm{tx}} - d_m m, \\quad \\frac{dp}{dt} = k_{\\mathrm{tl}} m - d_p p,\n$$\nand the output is\n$$\ny(t) = \\alpha \\, p(t).\n$$\nUse parameters\n$$\n\\theta_A = \\left(k_{\\mathrm{tx}}, d_m, k_{\\mathrm{tl}}, d_p, m(0), p(0), \\alpha\\right) = (50.0, 1.0, 5.0, 0.5, 20.0, 10.0, 2.0).\n$$\nSimulate over $t \\in [0,10]$ using $200$ uniformly spaced time points.\n\n- Test case B (Michaelis–Menten catalysis with unknown enzyme amount): The state is substrate $S(t)$, with dynamics\n$$\n\\frac{dS}{dt} = -\\frac{k_{\\mathrm{cat}} E_0}{K_M + S} S,\n$$\nand output\n$$\ny(t) = P(t) = S(0) - S(t).\n$$\nUse $S(0) = 10.0$ and parameters\n$$\n\\theta_B = \\left(k_{\\mathrm{cat}}, E_0, K_M\\right) = (1.2, 3.0, 0.8).\n$$\nSimulate over $t \\in [0,10]$ using $200$ uniformly spaced time points.\n\n- Test case C (linear degradation driven by an unknown input amplitude): The state is $x(t)$, with dynamics\n$$\n\\frac{dx}{dt} = k_1 \\, \\beta \\, u_0(t) - k_2 x, \\quad x(0) = 0,\n$$\nand output\n$$\ny(t) = x(t),\n$$\nwhere the known input shape is\n$$\nu_0(t) = 1 - e^{-t}.\n$$\nUse parameters\n$$\n\\theta_C = \\left(k_1, \\beta, k_2\\right) = (2.0, 4.0, 0.7).\n$$\nSimulate over $t \\in [0,10]$ using $200$ uniformly spaced time points.\n\nAlgorithmic and numerical requirements:\n\n- Construct $S_{\\log}(\\theta)$ using a small multiplicative perturbation factor $1+\\varepsilon$ with $\\varepsilon = 10^{-4}$, and approximate derivatives componentwise via finite differences in $\\ln$ coordinates. Ensure positivity by flooring outputs $y(t)$ at $10^{-12}$ before taking logarithms.\n- Compute the SVD of $S_{\\log}(\\theta)$ and take the right singular vectors associated with singular values below a tolerance $\\tau = 10^{-6} \\cdot \\max_i s_i$ as the nullspace basis.\n- For validation, compare each model’s discovered nullspace basis against a corresponding expected invariance direction derived from the system structure. Report success if any discovered basis vector matches the expected direction up to cosine similarity $0.98$.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[resultA,resultB,resultC]\"). Each result must be a boolean indicating whether the discovered identifiable reparameterization validation succeeded for the corresponding test case.\n\nNo external input is allowed; all values are fixed. The program must be self-contained and runnable as-is. No physical units appear in the output, which consists only of booleans.", "solution": "The task is to implement a numerical algorithm to determine structural unidentifiability in ordinary differential equation (ODE) models and validate it on three test cases. The core idea is that if a model is structurally unidentifiable, there exist continuous transformations of its parameters, $\\theta$, that leave the model output, $y(t)$, unchanged. Infinitesimally, these transformations correspond to directions in parameter space along which the derivative of the output is zero.\n\nThe algorithm leverages logarithmic sensitivities, a natural choice for biochemical systems where many parameter dependencies are multiplicative. The logarithmic sensitivity matrix, $S_{\\log}$, has elements $(S_{\\log})_{ij} = \\frac{\\partial \\ln y(t_i)}{\\partial \\ln \\theta_j}$. A direction $v$ in the log-parameter space that corresponds to an invariance satisfies $S_{\\log} v = 0$. Such vectors $v$ form the nullspace of $S_{\\log}$. We can numerically compute a basis for this nullspace using the Singular Value Decomposition (SVD). The right singular vectors of $S_{\\log}$ corresponding to singular values of (or close to) zero form this basis.\n\nEach basis vector $v = (v_1, \\dots, v_p)$ of the nullspace in logarithmic coordinates corresponds to an infinitesimal multiplicative invariance. A finite transformation can be approximated as $\\theta_j \\to \\theta_j \\cdot c^{v_j}$ for some constant $c$. The components of $v_j$ thus represent the exponents in a monomial that relates the parameters involved in the unidentifiability.\n\nThe algorithm proceeds as follows for each test case:\n1.  **Analytical Derivation of Expected Invariance**: Before implementation, we analyze each system to predict the unidentifiable parameter combinations and the corresponding nullspace vector in log-parameter space.\n    - **Case A**: The system is $\\frac{dm}{dt} = k_{\\mathrm{tx}} - d_m m$, $\\frac{dp}{dt} = k_{\\mathrm{tl}} m - d_p p$, with output $y(t) = \\alpha \\, p(t)$. A scaling analysis reveals that the output is invariant under the transformation $(k_{\\mathrm{tl}}, p(0), \\alpha) \\to (c \\cdot k_{\\mathrm{tl}}, c \\cdot p(0), \\alpha / c)$ for any $c>0$, while other parameters are held fixed. In logarithmic coordinates, this corresponds to $(\\ln k_{\\mathrm{tl}}, \\ln p(0), \\ln \\alpha) \\to (\\ln k_{\\mathrm{tl}} + \\ln c, \\ln p(0) + \\ln c, \\ln \\alpha - \\ln c)$. The infinitesimal direction vector for the parameter list $(\\theta_A) = (k_{\\mathrm{tx}}, d_m, k_{\\mathrm{tl}}, d_p, m(0), p(0), \\alpha)$ is thus $v_A = (0, 0, 1, 0, 0, 1, -1)$.\n    - **Case B**: The system is $\\frac{dS}{dt} = -\\frac{k_{\\mathrm{cat}} E_0}{K_M + S} S$. The rate is proportional to the product $k_{\\mathrm{cat}} E_0$. The system is therefore invariant to the transformation $(k_{\\mathrm{cat}}, E_0) \\to (c \\cdot k_{\\mathrm{cat}}, E_0/c)$. The infinitesimal direction vector for $(\\theta_B) = (k_{\\mathrm{cat}}, E_0, K_M)$ is $v_B = (1, -1, 0)$.\n    - **Case C**: The system is $\\frac{dx}{dt} = k_1 \\beta u_0(t) - k_2 x$. The input term is proportional to the product $k_1 \\beta$. The system is invariant to the transformation $(k_1, \\beta) \\to (c \\cdot k_1, \\beta/c)$. The infinitesimal direction vector for $(\\theta_C) = (k_1, \\beta, k_2)$ is $v_C = (1, -1, 0)$.\n\n2.  **Simulation and Sensitivity Matrix Construction**: The core of the numerical implementation involves simulating the system's output for the base parameters and for multiplicatively perturbed parameters.\n    - For each parameter $\\theta_j$, a perturbed parameter vector $\\theta'$ is created where $\\theta'_j = \\theta_j \\cdot (1+\\varepsilon)$.\n    - The ODE system is solved numerically using `scipy.integrate.solve_ivp` for both $\\theta$ and $\\theta'$ to obtain trajectories $y(t)$ and $y'(t)$.\n    - The corresponding column of the logarithmic sensitivity matrix is calculated using the finite difference formula: $(S_{\\log})_{:,j} = \\frac{\\ln y'(t) - \\ln y(t)}{\\ln \\theta'_j - \\ln \\theta_j} = \\frac{\\ln y'(t) - \\ln y(t)}{\\ln(1+\\varepsilon)}$. A floor of $10^{-12}$ is applied to the output before taking the logarithm to prevent numerical errors.\n\n3.  **Nullspace Identification via SVD**: The SVD of $S_{\\log}$ is computed as $S_{\\log} = U \\Sigma V^T$. The rows of $V^T$ (which are the columns of $V$) are the right singular vectors. The singular values in $\\Sigma$ indicate the \"strength\" of each direction. A singular value near zero implies the corresponding right singular vector is in the nullspace. We identify nullspace vectors as those corresponding to singular values $s_k < 10^{-6} \\cdot s_{\\max}$, where $s_{\\max}$ is the largest singular value.\n\n4.  **Validation**: For each test case, the computed nullspace basis vectors are compared against the analytically derived expected vector. The comparison is done using the cosine similarity. Let $v_{\\text{num}}$ be a computed nullspace vector and $v_{\\text{exp}}$ be the expected one. The validation succeeds if $|\\frac{v_{\\text{num}} \\cdot v_{\\text{exp}}}{\\|v_{\\text{num}}\\| \\|v_{\\text{exp}}\\|}| \\ge 0.98$ for any of the computed basis vectors. The absolute value is used because the direction of a basis vector (e.g., $v$ vs $-v$) is arbitrary.\n\nThe implementation will process each of the three test cases according to these steps and collect the boolean validation outcomes.", "answer": "```python\nimport numpy as np\nfrom scipy.integrate import solve_ivp\n\ndef solve():\n    \"\"\"\n    Main function to run identifiability analysis on three test cases.\n    \"\"\"\n\n    # --- Problem Constants ---\n    T_SPAN = [0.0, 10.0]\n    T_EVAL = np.linspace(T_SPAN[0], T_SPAN[1], 200)\n    EPSILON = 1e-4\n    LOG_FLOOR = 1e-12\n    SVD_TOL_FACTOR = 1e-6\n    COS_SIM_THRESHOLD = 0.98\n\n    # --- Model Definitions ---\n\n    # Case A: Gene Expression\n    def ode_a(t, x, k_tx, dm, k_tl, dp):\n        m, p = x\n        dm_dt = k_tx - dm * m\n        dp_dt = k_tl * m - dp * p\n        return [dm_dt, dp_dt]\n\n    def output_a(x_t, alpha):\n        # x_t shape: (num_times, num_states)\n        return alpha * x_t[:, 1]\n\n    # Case B: Michaelis-Menten\n    def ode_b(t, x, k_cat, E0, K_M):\n        S = x[0]\n        # Denominator is floored to avoid division by zero, though S > 0 here.\n        return [-(k_cat * E0 * S) / (K_M + S + LOG_FLOOR)]\n\n    def output_b(x_t, S0):\n        return S0 - x_t[:, 0]\n\n    # Case C: Linear Degradation\n    def ode_c(t, x, k1, beta, k2):\n        u0_t = 1.0 - np.exp(-t)\n        return [k1 * beta * u0_t - k2 * x[0]]\n\n    def output_c(x_t):\n        return x_t[:, 0]\n\n    def analyze_model(ode_func, output_func, theta, x0_func, ode_params_func, output_params_func, expected_v, extra_args=None):\n        \"\"\"\n        Generic analysis pipeline for one model.\n        \"\"\"\n        if extra_args is None:\n            extra_args = {}\n\n        # 1. Simulate base case\n        ode_params_base = ode_params_func(theta)\n        x0_base = x0_func(theta)\n        \n        sol_base = solve_ivp(\n            ode_func, T_SPAN, x0_base, args=tuple(ode_params_base), \n            t_eval=T_EVAL, method='RK45'\n        )\n        x_t_base = sol_base.y.T\n        output_params_base = output_params_func(theta)\n        y_base = output_func(x_t_base, **output_params_base, **extra_args)\n        log_y_base = np.log(np.maximum(y_base, LOG_FLOOR))\n        \n        # 2. Construct logarithmic sensitivity matrix\n        num_params = len(theta)\n        S_log = np.zeros((len(T_EVAL), num_params))\n        \n        for j in range(num_params):\n            theta_p = theta.copy()\n            theta_p[j] *= (1 + EPSILON)\n            \n            ode_params_p = ode_params_func(theta_p)\n            x0_p = x0_func(theta_p)\n            \n            sol_p = solve_ivp(\n                ode_func, T_SPAN, x0_p, args=tuple(ode_params_p),\n                t_eval=T_EVAL, method='RK45'\n            )\n            x_t_p = sol_p.y.T\n            output_params_p = output_params_func(theta_p)\n            y_p = output_func(x_t_p, **output_params_p, **extra_args)\n            log_y_p = np.log(np.maximum(y_p, LOG_FLOOR))\n            \n            S_log[:, j] = (log_y_p - log_y_base) / np.log(1 + EPSILON)\n\n        # 3. Compute nullspace via SVD\n        try:\n            U, s, Vh = np.linalg.svd(S_log, full_matrices=False)\n            if s.size == 0:\n                return False\n            tol = SVD_TOL_FACTOR * s[0]\n            null_indices = np.where(s < tol)[0]\n            \n            if null_indices.size == 0:\n                return False\n            \n            nullspace_basis = Vh[null_indices, :]\n        except np.linalg.LinAlgError:\n            return False\n\n        # 4. Validate against expected vector\n        is_valid = False\n        norm_e = np.linalg.norm(expected_v)\n        \n        for v_discovered in nullspace_basis:\n            norm_d = np.linalg.norm(v_discovered)\n            if norm_d > 1e-9 and norm_e > 1e-9:\n                cos_sim = np.abs(np.dot(v_discovered, expected_v) / (norm_d * norm_e))\n                if cos_sim >= COS_SIM_THRESHOLD:\n                    is_valid = True\n                    break\n        return is_valid\n\n    results = []\n\n    # --- Test Case A ---\n    theta_A = np.array([50.0, 1.0, 5.0, 0.5, 20.0, 10.0, 2.0])\n    expected_v_A = np.array([0.0, 0.0, 1.0, 0.0, 0.0, 1.0, -1.0])\n    result_A = analyze_model(\n        ode_func=ode_a,\n        output_func=output_a,\n        theta=theta_A,\n        x0_func=lambda p: p[4:6],\n        ode_params_func=lambda p: p[0:4],\n        output_params_func=lambda p: {'alpha': p[6]},\n        expected_v=expected_v_A\n    )\n    results.append(result_A)\n\n    # --- Test Case B ---\n    theta_B = np.array([1.2, 3.0, 0.8])\n    S0_B = 10.0\n    expected_v_B = np.array([1.0, -1.0, 0.0])\n    result_B = analyze_model(\n        ode_func=ode_b,\n        output_func=output_b,\n        theta=theta_B,\n        x0_func=lambda p: [S0_B],\n        ode_params_func=lambda p: p,\n        output_params_func=lambda p: {},\n        expected_v=expected_v_B,\n        extra_args={'S0': S0_B}\n    )\n    results.append(result_B)\n\n    # --- Test Case C ---\n    theta_C = np.array([2.0, 4.0, 0.7])\n    expected_v_C = np.array([1.0, -1.0, 0.0])\n    result_C = analyze_model(\n        ode_func=ode_c,\n        output_func=output_c,\n        theta=theta_C,\n        x0_func=lambda p: [0.0],\n        ode_params_func=lambda p: p,\n        output_params_func=lambda p: {},\n        expected_v=expected_v_C\n    )\n    results.append(result_C)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3352650"}]}