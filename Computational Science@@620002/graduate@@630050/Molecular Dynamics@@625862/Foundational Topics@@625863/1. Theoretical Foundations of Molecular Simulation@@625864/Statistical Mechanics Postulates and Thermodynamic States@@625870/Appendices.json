{"hands_on_practices": [{"introduction": "This first exercise is a cornerstone of statistical mechanics, demonstrating the direct link between its fundamental postulates and macroscopic thermodynamics. Starting with the microcanonical ensemble for an ideal gas, you will derive the renowned ideal gas law from the ground up by explicitly counting microstates. This practice [@problem_id:3448854] will solidify your understanding of how phase space volume, entropy, and thermodynamic derivatives come together to explain the behavior of even simple systems.", "problem": "A classical monatomic ideal gas composed of $N$ identical particles of mass $m$ is confined in a container of volume $V$ with periodic boundary conditions. The system is isolated with fixed total energy $E$ and described by the microcanonical ensemble. The Hamiltonian is $H(\\{\\mathbf{p}_i\\},\\{\\mathbf{r}_i\\}) = \\sum_{i=1}^{N} \\frac{|\\mathbf{p}_i|^{2}}{2m}$, and all microstates consistent with $E$ are taken to be equally probable according to the fundamental postulate of statistical mechanics.\n\nDefine the microcanonical entropy as $S(E,V,N) = k_{B} \\ln \\Omega(E,V,N)$, where $k_{B}$ is the Boltzmann constant, and $\\Omega(E,V,N)$ is the classical phase-space volume under the energy shell,\n$$\n\\Omega(E,V,N) = \\frac{1}{N! \\, h^{3N}} \\int_{H(\\mathbf{p},\\mathbf{r}) \\le E} d^{3N}\\mathbf{r}\\, d^{3N}\\mathbf{p},\n$$\nwith $h$ the Planck constant. Using only these definitions and fundamental facts about the volumes of high-dimensional spheres, derive $S(E,V,N)$ up to an additive constant that is independent of $E$ and $V$, and compute the pressure from\n$$\np \\;=\\; \\frac{\\left(\\frac{\\partial S}{\\partial V}\\right)_{E,N}}{\\left(\\frac{\\partial S}{\\partial E}\\right)_{V,N}}.\n$$\nExpress the final $p$ as a single closed-form analytic expression in terms of $E$, $V$, and $N$. Then, using the microcanonical definition of temperature $1/T = \\left(\\frac{\\partial S}{\\partial E}\\right)_{V,N}$, explain why the result is consistent with $p = \\frac{N k_{B} T}{V}$ in the thermodynamic limit $N \\to \\infty$. No numerical evaluation is required; provide the analytic expression for $p$ only. Do not assume any result that is not derivable from the given setup and standard geometric facts. All steps must be justified from first principles and core definitions. The final answer must be a single analytic expression.", "solution": "The problem is subjected to validation.\n\n### Step 1: Extract Givens\n- System: A classical monatomic ideal gas of $N$ identical particles.\n- Particle mass: $m$.\n- Container: Volume $V$ with periodic boundary conditions.\n- Ensemble: Microcanonical, isolated with fixed total energy $E$.\n- Hamiltonian: $H(\\{\\mathbf{p}_i\\},\\{\\mathbf{r}_i\\}) = \\sum_{i=1}^{N} \\frac{|\\mathbf{p}_i|^{2}}{2m}$.\n- Fundamental postulate: All accessible microstates are equally probable.\n- Microcanonical entropy: $S(E,V,N) = k_{B} \\ln \\Omega(E,V,N)$, where $k_{B}$ is the Boltzmann constant.\n- Phase-space volume: $\\Omega(E,V,N) = \\frac{1}{N! \\, h^{3N}} \\int_{H(\\mathbf{p},\\mathbf{r}) \\le E} d^{3N}\\mathbf{r}\\, d^{3N}\\mathbf{p}$, with $h$ being Planck's constant.\n- Pressure definition: $p = \\frac{(\\partial S/\\partial V)_{E,N}}{(\\partial S/\\partial E)_{V,N}}$.\n- Temperature definition: $1/T = (\\partial S/\\partial E)_{V,N}$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded:** The problem is a standard, fundamental exercise in classical statistical mechanics. All definitions and physical principles (Hamiltonian of an ideal gas, microcanonical ensemble, definitions of entropy and pressure) are correct and well-established.\n- **Well-Posed:** The problem is well-posed. It provides a clear set of definitions and a specific set of tasks leading to a unique, derivable result. The use of the cumulative phase-space volume $\\int_{H \\le E}$ is a standard convention, which, in the thermodynamic limit, yields the same thermodynamic properties as using the volume of an infinitesimally thin energy shell, due to the rapid growth of the volume with energy.\n- **Objective:** The problem is stated in precise, objective, and formal scientific language.\n- **Incompleteness/Contradiction:** The problem is self-contained and free of contradictions. It provides all necessary information to proceed with the derivation.\n- **Unrealistic/Infeasible:** The system model is a theoretical idealization (ideal gas), which is a cornerstone of thermodynamics and statistical mechanics. It is not physically unrealistic in this context.\n- **Other Flaws:** The problem is non-trivial, requiring a sound understanding of statistical mechanics and calculus in high dimensions. It is not pseudo-profound, tautological, or unverifiable.\n\n### Step 3: Verdict and Action\nThe problem is valid. A full, reasoned solution will be provided.\n\n### Derivation\n\nThe analysis begins with the definition of the microcanonical phase-space volume $\\Omega(E,V,N)$.\n$$\n\\Omega(E,V,N) = \\frac{1}{N! \\, h^{3N}} \\int_{H(\\mathbf{p},\\mathbf{r}) \\le E} d^{3N}\\mathbf{r}\\, d^{3N}\\mathbf{p}\n$$\nThe Hamiltonian $H = \\sum_{i=1}^{N} \\frac{|\\mathbf{p}_i|^2}{2m}$ is a function of the momenta $\\{\\mathbf{p}_i\\}$ only and does not depend on the positions $\\{\\mathbf{r}_i\\}$. This allows the integral over the $6N$-dimensional phase space to be separated into an integral over the $3N$ position coordinates and an integral over the $3N$ momentum coordinates.\n$$\n\\Omega(E,V,N) = \\frac{1}{N! \\, h^{3N}} \\left( \\int d^{3N}\\mathbf{r} \\right) \\left( \\int_{\\sum_{i=1}^{N} \\frac{|\\mathbf{p}_i|^2}{2m} \\le E} d^{3N}\\mathbf{p} \\right)\n$$\nThe position integral is over the volume $V$ for each of the $N$ particles. As the particles are non-interacting and each is confined to the volume $V$, the integral is:\n$$\n\\int d^{3N}\\mathbf{r} = \\prod_{i=1}^{N} \\int_V d^3\\mathbf{r}_i = V^N\n$$\nThe momentum integral is over the region of the $3N$-dimensional momentum space defined by the energy constraint:\n$$\n\\sum_{i=1}^{N} |\\mathbf{p}_i|^2 \\le 2mE\n$$\nThis inequality describes the interior of a hypersphere in $3N$ dimensions. Let the components of the momenta be indexed by a single index $j = 1, \\dots, 3N$. Then the condition is $\\sum_{j=1}^{3N} (p_j)^2 \\le (\\sqrt{2mE})^2$. The momentum integral is thus the volume of a $3N$-dimensional hypersphere of radius $R_p = \\sqrt{2mE}$.\n\nThe volume of an $n$-dimensional hypersphere of radius $R$ is given by the formula:\n$$\nV_n(R) = \\frac{\\pi^{n/2}}{\\Gamma(\\frac{n}{2} + 1)} R^n\n$$\nIn our case, the dimension is $n = 3N$ and the radius is $R_p = \\sqrt{2mE}$. The volume of the accessible momentum space is:\n$$\n\\int d^{3N}\\mathbf{p} = V_{3N}(R_p) = \\frac{\\pi^{3N/2}}{\\Gamma(\\frac{3N}{2} + 1)} (2mE)^{3N/2}\n$$\nSubstituting the results for the position and momentum integrals back into the expression for $\\Omega(E,V,N)$:\n$$\n\\Omega(E,V,N) = \\frac{1}{N! \\, h^{3N}} V^N \\frac{\\pi^{3N/2}}{\\Gamma(\\frac{3N}{2} + 1)} (2mE)^{3N/2}\n$$\nNow, we compute the microcanonical entropy $S(E,V,N) = k_B \\ln \\Omega(E,V,N)$:\n$$\nS(E,V,N) = k_B \\ln \\left[ \\frac{V^N}{N! \\, h^{3N}} \\frac{(2\\pi m E)^{3N/2}}{\\Gamma(\\frac{3N}{2} + 1)} \\right]\n$$\nUsing the properties of the logarithm, we expand this expression:\n$$\nS(E,V,N) = k_B \\left[ N \\ln V + \\frac{3N}{2} \\ln E + \\ln \\left( \\frac{(2\\pi m)^{3N/2}}{N! \\, h^{3N} \\Gamma(\\frac{3N}{2} + 1)} \\right) \\right]\n$$\nThe problem asks for the entropy up to an additive constant independent of $E$ and $V$. The last logarithmic term depends only on $N$ and constants, so we can write:\n$$\nS(E,V,N) = N k_B \\ln V + \\frac{3N}{2} k_B \\ln E + C(N)\n$$\nwhere $C(N)$ is a function of $N$ only.\n\nNext, we compute the partial derivatives of $S$ required for the pressure calculation.\n$$\n\\left(\\frac{\\partial S}{\\partial V}\\right)_{E,N} = \\frac{\\partial}{\\partial V} \\left( N k_B \\ln V + \\frac{3N}{2} k_B \\ln E + C(N) \\right) = \\frac{N k_B}{V}\n$$\n$$\n\\left(\\frac{\\partial S}{\\partial E}\\right)_{V,N} = \\frac{\\partial}{\\partial E} \\left( N k_B \\ln V + \\frac{3N}{2} k_B \\ln E + C(N) \\right) = \\frac{3N k_B}{2E}\n$$\nNow we use the given formula for pressure $p$:\n$$\np = \\frac{\\left(\\frac{\\partial S}{\\partial V}\\right)_{E,N}}{\\left(\\frac{\\partial S}{\\partial E}\\right)_{V,N}} = \\frac{\\frac{N k_B}{V}}{\\frac{3N k_B}{2E}}\n$$\nSimplifying this expression, we find the pressure:\n$$\np = \\frac{N k_B}{V} \\cdot \\frac{2E}{3N k_B} = \\frac{2E}{3V}\n$$\nThis is the pressure of the classical monatomic ideal gas in the microcanonical ensemble.\n\nFinally, we are asked to explain why this result is consistent with the ideal gas law, $p = \\frac{N k_B T}{V}$, in the thermodynamic limit. The microcanonical definition of temperature $T$ is given by:\n$$\n\\frac{1}{T} = \\left(\\frac{\\partial S}{\\partial E}\\right)_{V,N}\n$$\nWe have already calculated this derivative:\n$$\n\\frac{1}{T} = \\frac{3N k_B}{2E}\n$$\nThis relation can be inverted to express the total energy $E$ in terms of the temperature $T$:\n$$\nE = \\frac{3}{2} N k_B T\n$$\nThis is the well-known caloric equation of state for a monatomic ideal gas, asserting that the average energy per particle is $E/N = \\frac{3}{2} k_B T$, consistent with the equipartition theorem.\n\nNow, we substitute this expression for $E$ into our derived pressure equation $p = \\frac{2E}{3V}$:\n$$\np = \\frac{2}{3V} \\left( \\frac{3}{2} N k_B T \\right) = \\frac{N k_B T}{V}\n$$\nThis is precisely the ideal gas law. The derivation is therefore fully consistent. The notion of the thermodynamic limit ($N \\to \\infty$, $V \\to \\infty$ with $N/V$ constant) ensures that thermodynamic quantities like temperature and pressure are well-defined and that the different statistical ensembles (microcanonical, canonical, etc.) yield equivalent results for macroscopic observables. The successful recovery of the ideal gas law from the microcanonical formulation confirms this equivalence.", "answer": "$$\n\\boxed{\\frac{2E}{3V}}\n$$", "id": "3448854"}, {"introduction": "In the more experimentally relevant canonical ensemble, systems exchange energy with a heat bath, leading to observable fluctuations. This practice reveals a profound connection: the magnitude of these microscopic energy fluctuations is directly related to a macroscopic thermodynamic response function, the constant-volume heat capacity $C_V$. Through this exercise [@problem_id:3448850], you will first derive this key fluctuation-dissipation relation and then apply it to data from a hypothetical molecular dynamics simulation, bridging the gap between abstract theory and computational practice.", "problem": "A classical system with Hamiltonian $H(\\boldsymbol{p},\\boldsymbol{q})$ is weakly coupled to a large heat bath at temperature $T$, so that the system is described by the canonical ensemble. Starting from the canonical postulates, the canonical partition function $Z(\\beta)$ with $\\beta = 1/(k_B T)$, and the definitions of ensemble averages and response functions, derive the exact relation that connects the equilibrium variance of the total energy $\\langle (\\Delta E)^2 \\rangle$ to the constant-volume heat capacity $C_V$ of the system. Your derivation must begin from fundamental definitions of $Z(\\beta)$ and the canonical probabilities and proceed by logical steps without assuming any fluctuation formula.\n\nThen, consider an equilibrium constant-temperature molecular dynamics (MD) simulation of a monatomic fluid with $N=1000$ identical particles at fixed volume. The MD thermostat is known to generate the correct canonical distribution, and the dynamics are ergodic on the sampling timescale. After removing initial transients and using statistically independent block averages to account for time correlations, the unbiased estimate of the variance of the total energy time series is found to be $\\mathrm{Var}(E)=2.402\\times 10^{-38}$ in units of $\\mathrm{J}^2$ at temperature $T=300\\,\\mathrm{K}$. Using only these data and fundamental constants, compute the dimensionless heat capacity per particle\n$$\nc_V \\equiv \\frac{C_V}{N k_B}.\n$$\nRound your answer to four significant figures. Express your final result as a pure number (dimensionless).\n\nFinally, justify in words why time-averaged fluctuations of the total energy in an $NVT$ simulation provide a consistent estimator of $C_V$, and state at least two practical considerations that must be addressed to obtain an unbiased and statistically efficient estimate in MD.", "solution": "The problem is divided into three parts: a derivation, a calculation, and a conceptual justification. We address each in turn.\n\nFirst, we derive the exact relation between the equilibrium variance of the total energy, $\\langle (\\Delta E)^2 \\rangle$, and the constant-volume heat capacity, $C_V$. The derivation starts from the fundamental postulates of the canonical ensemble.\n\nThe canonical partition function for a classical system with Hamiltonian $H(\\boldsymbol{p}, \\boldsymbol{q})$ is given by\n$$\nZ(\\beta) = C \\int \\exp(-\\beta H(\\boldsymbol{p}, \\boldsymbol{q})) d\\boldsymbol{p} d\\boldsymbol{q}\n$$\nwhere $\\beta = 1/(k_B T)$, $k_B$ is the Boltzmann constant, $T$ is the absolute temperature, and the integration is over all phase space. The prefactor $C$ (which includes factors like $1/h^{3N}$ and $1/N!$) is a constant with respect to temperature and phase space coordinates, so it will not affect the thermodynamic derivatives we will perform. The probability density of finding the system in a microstate with coordinates $(\\boldsymbol{p}, \\boldsymbol{q})$ is\n$$\n\\rho(\\boldsymbol{p}, \\boldsymbol{q}) = \\frac{\\exp(-\\beta H(\\boldsymbol{p}, \\boldsymbol{q}))}{Z(\\beta)/C}\n$$\nThe ensemble average of any observable $A(\\boldsymbol{p}, \\boldsymbol{q})$ is given by\n$$\n\\langle A \\rangle = \\int A(\\boldsymbol{p}, \\boldsymbol{q}) \\rho(\\boldsymbol{p}, \\boldsymbol{q}) d\\boldsymbol{p} d\\boldsymbol{q} = \\frac{\\int A \\exp(-\\beta H) d\\boldsymbol{p} d\\boldsymbol{q}}{\\int \\exp(-\\beta H) d\\boldsymbol{p} d\\boldsymbol{q}}\n$$\nWe identify the average total energy of the system, $\\langle E \\rangle$, with the ensemble average of the Hamiltonian, $\\langle H \\rangle$:\n$$\n\\langle E \\rangle = \\frac{\\int H \\exp(-\\beta H) d\\boldsymbol{p} d\\boldsymbol{q}}{\\int \\exp(-\\beta H) d\\boldsymbol{p} d\\boldsymbol{q}}\n$$\nWe can express $\\langle E \\rangle$ as a derivative of the partition function $Z(\\beta)$. Consider the derivative of $Z(\\beta)$ with respect to $\\beta$:\n$$\n\\frac{\\partial Z(\\beta)}{\\partial \\beta} = C \\int (-H) \\exp(-\\beta H) d\\boldsymbol{p} d\\boldsymbol{q}\n$$\nDividing by $Z(\\beta)$ gives:\n$$\n\\frac{1}{Z(\\beta)}\\frac{\\partial Z(\\beta)}{\\partial \\beta} = \\frac{\\partial \\ln Z(\\beta)}{\\partial \\beta} = \\frac{\\int (-H) \\exp(-\\beta H) d\\boldsymbol{p} d\\boldsymbol{q}}{\\int \\exp(-\\beta H) d\\boldsymbol{p} d\\boldsymbol{q}} = -\\langle H \\rangle = -\\langle E \\rangle\n$$\nThus, the average energy is $\\langle E \\rangle = -\\frac{\\partial \\ln Z(\\beta)}{\\partial \\beta}$.\n\nThe constant-volume heat capacity, $C_V$, is defined as the partial derivative of the average energy with respect to temperature at constant volume:\n$$\nC_V = \\left(\\frac{\\partial \\langle E \\rangle}{\\partial T}\\right)_V\n$$\nUsing the chain rule to change the variable from $T$ to $\\beta$:\n$$\n\\frac{\\partial}{\\partial T} = \\frac{d\\beta}{dT} \\frac{\\partial}{\\partial \\beta} = \\left(-\\frac{1}{k_B T^2}\\right) \\frac{\\partial}{\\partial \\beta}\n$$\nSo, the heat capacity becomes:\n$$\nC_V = \\left(-\\frac{1}{k_B T^2}\\right) \\frac{\\partial \\langle E \\rangle}{\\partial \\beta}\n$$\nNow we must evaluate the derivative of $\\langle E \\rangle$ with respect to $\\beta$:\n$$\n\\frac{\\partial \\langle E \\rangle}{\\partial \\beta} = \\frac{\\partial}{\\partial \\beta} \\left(-\\frac{\\partial \\ln Z(\\beta)}{\\partial \\beta}\\right) = -\\frac{\\partial^2 \\ln Z(\\beta)}{\\partial \\beta^2}\n$$\nAlternatively, and more fundamentally, we can differentiate the expression for $\\langle E \\rangle$ directly using the quotient rule:\n$$\n\\frac{\\partial \\langle E \\rangle}{\\partial \\beta} = \\frac{\\partial}{\\partial \\beta} \\left( \\frac{\\int H \\exp(-\\beta H) d\\Gamma}{\\int \\exp(-\\beta H) d\\Gamma} \\right) \\quad (\\text{where } d\\Gamma = d\\boldsymbol{p}d\\boldsymbol{q})\n$$\n$$\n\\frac{\\partial \\langle E \\rangle}{\\partial \\beta} = \\frac{\\left( \\int (-H^2) \\exp(-\\beta H) d\\Gamma \\right) \\left( \\int \\exp(-\\beta H) d\\Gamma \\right) - \\left( \\int H \\exp(-\\beta H) d\\Gamma \\right) \\left( \\int (-H) \\exp(-\\beta H) d\\Gamma \\right)}{\\left( \\int \\exp(-\\beta H) d\\Gamma \\right)^2}\n$$\nDividing the numerator and denominator by $(Z/C)^2 = (\\int \\exp(-\\beta H) d\\Gamma)^2$:\n$$\n\\frac{\\partial \\langle E \\rangle}{\\partial \\beta} = -\\frac{\\int H^2 \\exp(-\\beta H) d\\Gamma}{\\int \\exp(-\\beta H) d\\Gamma} + \\left( \\frac{\\int H \\exp(-\\beta H) d\\Gamma}{\\int \\exp(-\\beta H) d\\Gamma} \\right)^2\n$$\nRecognizing the terms as ensemble averages, we have:\n$$\n\\frac{\\partial \\langle E \\rangle}{\\partial \\beta} = -\\langle H^2 \\rangle + \\langle H \\rangle^2 = -\\langle E^2 \\rangle + \\langle E \\rangle^2 = -(\\langle E^2 \\rangle - \\langle E \\rangle^2)\n$$\nThe term $\\langle E^2 \\rangle - \\langle E \\rangle^2$ is the definition of the variance of the energy, $\\langle (\\Delta E)^2 \\rangle$. So, we have found that\n$$\n\\frac{\\partial \\langle E \\rangle}{\\partial \\beta} = -\\langle (\\Delta E)^2 \\rangle\n$$\nFinally, substituting this result back into the expression for $C_V$:\n$$\nC_V = \\left(-\\frac{1}{k_B T^2}\\right) (-\\langle (\\Delta E)^2 \\rangle) = \\frac{\\langle (\\Delta E)^2 \\rangle}{k_B T^2}\n$$\nThis gives the desired relation:\n$$\n\\langle (\\Delta E)^2 \\rangle = k_B T^2 C_V\n$$\n\nSecond, we compute the dimensionless heat capacity per particle, $c_V$, using data from the molecular dynamics (MD) simulation. The problem provides:\nNumber of particles, $N = 1000$.\nTemperature, $T = 300\\,\\mathrm{K}$.\nVariance of the total energy, $\\mathrm{Var}(E) = 2.402 \\times 10^{-38}\\,\\mathrm{J}^2$.\nThe MD simulation generates the canonical distribution, so by the ergodic hypothesis, the time-averaged variance is a valid estimator of the ensemble-averaged variance, i.e., $\\langle (\\Delta E)^2 \\rangle = \\mathrm{Var}(E)$.\nWe use the value of the Boltzmann constant, $k_B \\approx 1.380649 \\times 10^{-23}\\,\\mathrm{J/K}$.\n\nWe need to compute $c_V \\equiv \\frac{C_V}{N k_B}$. We first express $c_V$ in terms of the given quantities using the derived formula:\n$$\nC_V = \\frac{\\langle (\\Delta E)^2 \\rangle}{k_B T^2}\n$$\n$$\nc_V = \\frac{C_V}{N k_B} = \\frac{1}{N k_B} \\left( \\frac{\\langle (\\Delta E)^2 \\rangle}{k_B T^2} \\right) = \\frac{\\langle (\\Delta E)^2 \\rangle}{N (k_B T)^2}\n$$\nNow, we substitute the numerical values:\n$$\nk_B T = (1.380649 \\times 10^{-23}\\,\\mathrm{J/K}) \\times (300\\,\\mathrm{K}) = 4.141947 \\times 10^{-21}\\,\\mathrm{J}\n$$\n$$\n(k_B T)^2 = (4.141947 \\times 10^{-21}\\,\\mathrm{J})^2 \\approx 1.715570 \\times 10^{-41}\\,\\mathrm{J}^2\n$$\n$$\nN (k_B T)^2 = 1000 \\times (1.715570 \\times 10^{-41}\\,\\mathrm{J}^2) = 1.715570 \\times 10^{-38}\\,\\mathrm{J}^2\n$$\nFinally, we compute $c_V$:\n$$\nc_V = \\frac{2.402 \\times 10^{-38}\\,\\mathrm{J}^2}{1.715570 \\times 10^{-38}\\,\\mathrm{J}^2} = \\frac{2.402}{1.715570} \\approx 1.400133\n$$\nRounding to four significant figures, the result is $1.400$.\n\nThird, we justify why time-averaged fluctuations of the total energy in an $NVT$ simulation provide a consistent estimator of $C_V$ and state two practical considerations.\nThe fundamental principle connecting simulation time averages to thermodynamic ensemble averages is the **ergodic hypothesis**. This hypothesis states that for an ergodic system in equilibrium, the time average of an observable along a single, sufficiently long trajectory is equal to the ensemble average of that observable in the corresponding statistical ensemble. The problem specifies that the MD thermostat generates the correct canonical ($NVT$) distribution and that the dynamics are ergodic. Therefore, the variance of the total energy computed from the time series, $\\mathrm{Var}(E)$, converges to the canonical ensemble variance $\\langle (\\Delta E)^2 \\rangle$ as the simulation time becomes large. Since we proved that $\\langle (\\Delta E)^2 \\rangle$ is directly proportional to $C_V$, the time-series variance from the simulation is a valid estimator for the heat capacity.\n\nTo obtain an unbiased and statistically efficient estimate from a practical MD simulation, several factors must be addressed. Two crucial considerations are:\n\n1.  **Equilibration**: Any MD simulation starts from an initial configuration that is typically not a representative state of the equilibrium ensemble. The system requires a period of simulation time, known as the equilibration phase, to relax from this initial state to the target thermodynamic equilibrium. Data collected during this initial transient phase do not belong to the equilibrium distribution and would introduce a systematic error (bias) if included in property calculations. Therefore, the initial portion of the trajectory must be discarded, and data collection for averaging should only begin after the system has reached a steady state, where macroscopic properties like energy and pressure no longer show systematic drifts.\n\n2.  **Time Correlations and Statistical Uncertainty**: Data points (e.g., energy values) in an MD time series are not statistically independent; a configuration at time $t$ is strongly correlated with the configuration at time $t + \\Delta t$ for small time steps $\\Delta t$. Calculating statistical properties (like variance) and their errors naively from correlated data leads to an underestimation of the true statistical uncertainty and can result in inefficient or biased estimates. To address this, methods must be used to account for these correlations. The **block averaging method**, mentioned in the problem, is a standard technique. The full data series is divided into several blocks, each longer than the characteristic correlation time of the observable. The average of the observable is computed for each block. These block averages are significantly less correlated with each other than the raw data points. The final estimate of the property is the average of these block averages, and the statistical error is calculated from the standard deviation of the block averages. This procedure yields a more robust and reliable estimate of both the property and its statistical uncertainty.", "answer": "$$\n\\boxed{1.400}\n$$", "id": "3448850"}, {"introduction": "Entropy is often linked to disorder, but it can also be viewed as a measure of missing information associated with a particular macroscopic description. This practice explores this idea in the modern context of coarse-graining, a common technique in molecular modeling where fine-grained atomistic details are simplified into larger \"beads\". Here [@problem_id:3448798], you will use the Boltzmann postulate, $S = k_{\\mathrm{B}} \\ln \\Omega$, to quantify the change in entropy that results purely from making formerly distinguishable particles indistinguishable, highlighting how entropy fundamentally depends on the chosen level of description.", "problem": "Consider a molecular dynamics (MD) coarse-graining procedure that maps an atomistic representation with $N$ atoms into $B$ beads. The mapping is many-to-one: each bead $b$ is defined by grouping $g_b$ atoms, and the coarse-grained state of bead $b$ is invariant under any permutation of the $g_b$ atom labels inside that bead. All beads remain index-distinct in the coarse-grained description (for example, beads along a polymer chain retain their position labels), so indistinguishability is introduced only at the level of atom labels within each bead. Assume that the underlying atomistic microstates are counted using labeled atoms and that the coarse-grained description lumps together atomistic microstates that differ only by permutations of atom labels inside each bead. Use the Boltzmann postulate $S = k_{\\mathrm{B}} \\ln \\Omega$, where $S$ is the entropy, $k_{\\mathrm{B}}$ is the Boltzmann constant, and $\\Omega$ is the number of microstates, to reason from first principles and derive the change in entropy $\\Delta S$ induced purely by this indistinguishability due to coarse-graining. Your derivation should start from the combinatorial effect of permutations within beads on $\\Omega$ and proceed logically to an expression for $\\Delta S$ in terms of $\\{g_b\\}_{b=1}^{B}$.\n\nAfter presenting the derivation, implement a program that computes $\\Delta S$ for given bead-size lists $\\{g_b\\}$ using the natural logarithm. For numerical stability for large integer $g_b$, evaluate $\\ln(g_b!)$ via the logarithm of the gamma function using $\\ln \\Gamma(g_b + 1)$. Use $k_{\\mathrm{B}} = 1.380649 \\times 10^{-23}$ joules per kelvin, and express $\\Delta S$ in joules per kelvin. The program must be self-contained and produce outputs for the following test suite:\n\n1. Happy path case with moderate coarse-graining: $[3,3,3,3]$.\n2. Boundary case where no indistinguishability is introduced by coarse-graining (each bead has one atom): $[1,1,1,1,1,1,1,1,1,1]$.\n3. Mixed bead sizes testing additivity of indistinguishability contributions: $[2,3,4,5]$.\n4. Significant edge case with a single large bead to test numerical stability: $[100]$.\n\nFor each test case, compute a single float equal to $\\Delta S$ in joules per kelvin. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[x_1,x_2,x_3,x_4]$), in the order listed above.", "solution": "The problem asks for the derivation of the change in entropy, $\\Delta S$, resulting from a coarse-graining procedure in molecular dynamics. The procedure maps an atomistic model to a bead-based model, introducing indistinguishability for atoms grouped within the same bead. The derivation must start from the Boltzmann postulate, $S = k_{\\mathrm{B}} \\ln \\Omega$.\n\nLet us define the initial and final states of our description.\n\nThe initial description is the fully atomistic model. The problem states that microstates are counted using labeled atoms. Let us denote the number of microstates in this description as $\\Omega_{\\text{atomic}}$. The corresponding entropy is given by the Boltzmann formula:\n$$\nS_{\\text{atomic}} = k_{\\mathrm{B}} \\ln \\Omega_{\\text{atomic}}\n$$\nHere, $k_{\\mathrm{B}}$ is the Boltzmann constant. In this description, every atom is distinguishable from every other atom.\n\nThe final description is the coarse-grained model. In this model, the system is represented by $B$ beads. Bead $b$ (for $b=1, 2, \\dots, B$) is formed by grouping $g_b$ atoms. The crucial aspect of the coarse-graining is that the state of bead $b$ is invariant under any permutation of the $g_b$ atom labels inside it. This means that atomistic microstates that differ only by swapping atom labels *within* a single bead are considered as a single, identical microstate in the coarse-grained picture. The beads themselves remain distinguishable.\n\nLet us determine the relationship between the number of microstates in the atomistic description, $\\Omega_{\\text{atomic}}$, and the number of microstates in the coarse-grained description, $\\Omega_{\\text{CG}}$.\nFor a single bead $b$ composed of $g_b$ atoms, these $g_b$ atoms were distinguishable in the atomistic picture. The number of ways to arrange these $g_b$ labeled atoms is $g_b!$. In the coarse-grained picture, all these $g_b!$ arrangements are collapsed into a single state because the atoms within the bead are now treated as indistinguishable. This represents a reduction in the number of distinct microstates by a factor of $g_b!$ for that bead.\n\nSince the permutations of atoms within one bead are independent of the permutations within any other bead, the total reduction factor for all $B$ beads is the product of the individual permutation factors. The total overcounting factor, which is the number of atomistic microstates corresponding to a single coarse-grained microstate, is:\n$$\nW = \\prod_{b=1}^{B} g_b! = g_1! \\, g_2! \\dots g_B!\n$$\nTherefore, the number of microstates in the coarse-grained description, $\\Omega_{\\text{CG}}$, is related to the number of microstates in the atomistic description, $\\Omega_{\\text{atomic}}$, by:\n$$\n\\Omega_{\\text{CG}} = \\frac{\\Omega_{\\text{atomic}}}{W} = \\frac{\\Omega_{\\text{atomic}}}{\\prod_{b=1}^{B} g_b!}\n$$\nThe entropy of the coarse-grained description is:\n$$\nS_{\\text{CG}} = k_{\\mathrm{B}} \\ln \\Omega_{\\text{CG}}\n$$\nThe problem asks for the change in entropy, $\\Delta S$, induced by this change in description. This is the difference between the entropy of the final (coarse-grained) description and the initial (atomistic) description:\n$$\n\\Delta S = S_{\\text{CG}} - S_{\\text{atomic}}\n$$\nSubstituting the expressions for entropy:\n$$\n\\Delta S = k_{\\mathrm{B}} \\ln \\Omega_{\\text{CG}} - k_{\\mathrm{B}} \\ln \\Omega_{\\text{atomic}}\n$$\nUsing the property of logarithms, $\\ln(x) - \\ln(y) = \\ln(x/y)$:\n$$\n\\Delta S = k_{\\mathrm{B}} \\ln \\left( \\frac{\\Omega_{\\text{CG}}}{\\Omega_{\\text{atomic}}} \\right)\n$$\nNow, we substitute the relationship between $\\Omega_{\\text{CG}}$ and $\\Omega_{\\text{atomic}}$:\n$$\n\\Delta S = k_{\\mathrm{B}} \\ln \\left( \\frac{\\Omega_{\\text{atomic}} / \\left(\\prod_{b=1}^{B} g_b!\\right)}{\\Omega_{\\text{atomic}}} \\right)\n$$\nThe $\\Omega_{\\text{atomic}}$ terms cancel out, leaving:\n$$\n\\Delta S = k_{\\mathrm{B}} \\ln \\left( \\frac{1}{\\prod_{b=1}^{B} g_b!} \\right)\n$$\nUsing the property $\\ln(1/x) = -\\ln(x)$:\n$$\n\\Delta S = -k_{\\mathrm{B}} \\ln \\left( \\prod_{b=1}^{B} g_b! \\right)\n$$\nFinally, using the property $\\ln(\\prod x_i) = \\sum \\ln(x_i)$:\n$$\n\\Delta S = -k_{\\mathrm{B}} \\sum_{b=1}^{B} \\ln(g_b!)\n$$\nThis is the final expression for the change in entropy due purely to the introduction of indistinguishability within each bead. The negative sign indicates a decrease in entropy, which corresponds to the loss of information resulting from the coarse-graining processâ€”we no longer distinguish between permutations of atoms inside a bead.\n\nFor numerical computation with large $g_b$, the term $\\ln(g_b!)$ is best calculated using the logarithm of the gamma function, $\\Gamma(z)$, as $\\ln(n!) = \\ln(\\Gamma(n+1))$. This avoids computing the very large intermediate value of $n!$. The implementation will use this identity.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import gammaln\n\ndef solve():\n    \"\"\"\n    Calculates the change in entropy due to a coarse-graining procedure\n    for a series of test cases.\n    \"\"\"\n    \n    # Define the Boltzmann constant in J/K.\n    k_B = 1.380649e-23  # joules per kelvin\n\n    # Define the test cases from the problem statement.\n    # Each inner list represents a set of bead sizes {g_b}.\n    test_cases = [\n        [3, 3, 3, 3],                          # Case 1: Happy path\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],      # Case 2: Boundary case (no coarsening)\n        [2, 3, 4, 5],                          # Case 3: Mixed bead sizes\n        [100]                                  # Case 4: Significant edge case (large bead)\n    ]\n\n    results = []\n    \n    # Process each test case\n    for bead_sizes in test_cases:\n        # The entropy change is given by Delta_S = -k_B * sum(ln(g_b!))\n        # For numerical stability, ln(n!) is computed as ln(Gamma(n+1)),\n        # which is provided by scipy.special.gammaln(n+1).\n        \n        # Calculate the sum of log-factorials for all bead sizes in the current case.\n        # Note: ln(1!) = ln(1) = 0, so beads of size 1 contribute nothing to the sum.\n        # The gammaln function handles this correctly as gammaln(1+1) = gammaln(2) = ln(Gamma(2)) = ln(1!) = 0.\n        \n        # Using a generator expression with sum() for efficiency.\n        sum_log_factorials = sum(gammaln(g + 1) for g in bead_sizes)\n        \n        # Calculate the change in entropy\n        delta_S = -k_B * sum_log_factorials\n        \n        results.append(delta_S)\n\n    # Final print statement in the exact required format.\n    # The format required is a comma-separated list of floats inside square brackets.\n    # Using f-string formatting to achieve this.\n    print(f\"[{','.join(map(str, results))}]\")\n\n# Execute the solver function.\nsolve()\n```", "id": "3448798"}]}