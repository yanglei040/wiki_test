{"hands_on_practices": [{"introduction": "Ergodicity is a fundamental requirement for any valid Markov Chain Monte Carlo simulation, ensuring that the sampler can, in principle, explore the entire accessible state space. This practice provides a tangible demonstration of how a seemingly reasonable set of local moves can fail to be ergodic when confronted with disconnected energy basins. By working through a simple two-particle model [@problem_id:3427345], you will not only prove the breakdown of ergodicity but also design a collective move to restore it, building crucial intuition for diagnosing and solving poor sampling in complex systems.", "problem": "Consider a two-particle one-dimensional system intended to highlight core properties of Markov Chain Monte Carlo (MCMC) sampling in molecular dynamics. Let the configuration space be $\\Omega \\subset \\mathbb{R}^2$ with particle positions $(x_1,x_2)$. Define two basins and a hard-wall potential energy function $U:\\Omega \\rightarrow \\{0,+\\infty\\}$ as follows. The admissible basins are\n$$\n\\mathcal{A} = \\{(x_1,x_2) \\in \\mathbb{R}^2 \\mid -1 \\le x_1 \\le 0,\\,-1 \\le x_2 \\le 0,\\, x_1 \\le x_2\\}\n$$\nand\n$$\n\\mathcal{B} = \\{(x_1,x_2) \\in \\mathbb{R}^2 \\mid 1 \\le x_1 \\le 2,\\,1 \\le x_2 \\le 2,\\, x_1 \\le x_2\\}.\n$$\nSet the potential to be $U(x_1,x_2) = 0$ if $(x_1,x_2) \\in \\mathcal{A} \\cup \\mathcal{B}$ and $U(x_1,x_2) = +\\infty$ otherwise. The Metropolis acceptance rule at inverse temperature $\\beta$ is applied to proposals $(x_1,x_2) \\mapsto (x_1',x_2')$: accept with probability $\\min\\{1, \\exp(-\\beta[U(x_1',x_2') - U(x_1,x_2)])\\}$.\n\nTwo proposal move sets are considered:\n\n- Single-particle displacement moves: at each step, choose $i \\in \\{1,2\\}$ uniformly and propose $x_i' = x_i + \\delta$ with $\\delta$ drawn uniformly from $[-d,d]$, leaving the other coordinate unchanged. The proposal is rejected if $(x_1',x_2') \\notin \\mathcal{A} \\cup \\mathcal{B}$ due to the hard-wall constraint.\n\n- Collective translation moves: at each step, with probability $p_c$ select a collective move that proposes $(x_1',x_2') = (x_1 + s, x_2 + s)$ with $s \\in \\{+2,-2\\}$ chosen uniformly, and with probability $1 - p_c$ select a single-particle displacement move as above.\n\nTasks:\n\n1. Using only first principles of MCMC and the above definitions, demonstrate that with single-particle displacement moves alone, the Markov chain on $\\Omega$ restricted by $U$ is non-ergodic. Specifically, show that no admissible path comprised solely of single-particle displacements can transition from basin $\\mathcal{A}$ to basin $\\mathcal{B}$.\n\n2. Identify a necessary form of collective move to restore ergodicity in the presence of the hard-wall potential and justify that it maintains detailed balance. Then, at the level of macrostates $\\{\\mathcal{A}, \\mathcal{B}\\}$, derive the mixing time $t_{\\mathrm{mix}}(\\varepsilon)$ in terms of $p_c$ and a total variation threshold $\\varepsilon$ for the resulting two-state Markov chain under the symmetric choice $s \\in \\{+2,-2\\}$.\n\n3. Implement a program that performs two computations:\n   - Construct a discretized state graph using step size $h = 0.5$ by enumerating admissible grid points in $\\mathcal{A}$ and $\\mathcal{B}$ and connecting states by edges if they differ by a single-particle displacement of magnitude $h$ on one coordinate and remain admissible. Return a boolean indicating non-ergodicity, defined here as the graph having more than one connected component under these single-particle moves. The boolean must be computed from the graph connectivity on this discretization; it must not be hard-coded.\n   - For four specified values of the collective-move probability $p_c \\in \\{1.0, 0.2, 0.02, 0.0\\}$ and a fixed total variation threshold $\\varepsilon = 10^{-3}$, compute the mixing time $t_{\\mathrm{mix}}(\\varepsilon)$ for the macro two-state chain $\\{\\mathcal{A},\\mathcal{B}\\}$ induced by the collective move. Express $t_{\\mathrm{mix}}(\\varepsilon)$ as an integer number of steps using the ceiling of your analytical expression, except when it is infinite (for $p_c = 0.0$). In the special case where the macro transition probability equals $0.5$, take $t_{\\mathrm{mix}}(\\varepsilon)$ to be $1$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets in the exact order\n$$\n[\\text{non\\_ergodic\\_boolean},\\, t_{\\mathrm{mix}}(p_c=1.0),\\, t_{\\mathrm{mix}}(p_c=0.2),\\, t_{\\mathrm{mix}}(p_c=0.02),\\, t_{\\mathrm{mix}}(p_c=0.0)].\n$$\nThe first entry is a boolean, and the remaining entries are integers, except the last which is a float $+\\infty$ when appropriate. No physical units are involved; all times are dimensionless counts of steps. Angles do not appear. Percentages must not be used; probabilities are to be treated as real numbers in $[0,1]$.\n\nTest suite for coverage:\n- Graph construction with $h = 0.5$ to detect non-ergodicity under single-particle moves.\n- Collective-move probabilities $p_c \\in \\{1.0, 0.2, 0.02, 0.0\\}$ at threshold $\\varepsilon = 10^{-3}$ to probe fast mixing, moderate mixing, slow mixing, and the boundary case of no mixing across basins.", "solution": "The problem is assessed to be valid. It is a well-posed, scientifically grounded problem in statistical mechanics that tests fundamental concepts of Markov Chain Monte Carlo (MCMC) sampling, specifically ergodicity, detailed balance, and mixing time. All definitions and parameters are sufficient for a rigorous solution.\n\n### Part 1: Non-Ergodicity of Single-Particle Displacement Moves\n\nAn MCMC sampler is ergodic if it is possible to transition from any state in the configuration space to any other state in a finite number of steps. The configuration space with finite potential energy is the union of two disjoint basins, $\\mathcal{A}$ and $\\mathcal{B}$.\n$$\n\\mathcal{A} = \\{(x_1,x_2) \\in \\mathbb{R}^2 \\mid -1 \\le x_1 \\le 0,\\,-1 \\le x_2 \\le 0,\\, x_1 \\le x_2\\}\n$$\n$$\n\\mathcal{B} = \\{(x_1,x_2) \\in \\mathbb{R}^2 \\mid 1 \\le x_1 \\le 2,\\,1 \\le x_2 \\le 2,\\, x_1 \\le x_2\\}\n$$\nThe potential $U(x_1,x_2)$ is $0$ for $(x_1,x_2) \\in \\mathcal{A} \\cup \\mathcal{B}$ and $+\\infty$ otherwise.\n\nLet the initial state of the system be $(x_1, x_2) \\in \\mathcal{A}$. By definition of $\\mathcal{A}$, this implies $-1 \\le x_1 \\le 0$ and $-1 \\le x_2 \\le 0$. We consider a single-particle displacement move, which proposes a new state $(x_1', x_2')$. The Metropolis acceptance probability for a transition from a state $x$ to $x'$ is $\\min\\{1, \\exp(-\\beta[U(x') - U(x)])\\}$. Since $U(x)=0$ within $\\mathcal{A}$, any accepted move to a new state $x'$ must have $U(x')=0$, meaning $x' \\in \\mathcal{A} \\cup \\mathcal{B}$.\n\nThere are two cases for a single-particle move from $(x_1, x_2) \\in \\mathcal{A}$:\n\n1.  **Displace particle $1$**: The proposed new state is $(x_1', x_2') = (x_1 + \\delta, x_2)$, where $\\delta$ is a random displacement. The coordinate $x_2$ remains unchanged, so $x_2' = x_2 \\in [-1, 0]$. A state $(y_1, y_2)$ in basin $\\mathcal{B}$ must satisfy $1 \\le y_2 \\le 2$. Since $x_2' \\notin [1, 2]$, the proposed state $(x_1', x_2')$ cannot be in $\\mathcal{B}$. Therefore, for the move to be accepted (i.e., for the potential to remain finite), the new state must lie in $\\mathcal{A}$.\n\n2.  **Displace particle $2$**: The proposed new state is $(x_1', x_2') = (x_1, x_2 + \\delta)$. The coordinate $x_1$ remains unchanged, so $x_1' = x_1 \\in [-1, 0]$. A state $(y_1, y_2)$ in basin $\\mathcal{B}$ must satisfy $1 \\le y_1 \\le 2$. Since $x_1' \\notin [1, 2]$, the proposed state $(x_1', x_2')$ cannot be in $\\mathcal{B}$. Therefore, for the move to be accepted, the new state must lie in $\\mathcal{A}$.\n\nIn both cases, any accepted single-particle move originating from a state in $\\mathcal{A}$ results in a state that is also in $\\mathcal{A}$. It is impossible to generate a state in $\\mathcal{B}$ from a state in $\\mathcal{A}$ using only single-particle displacements. An analogous argument shows that one cannot transition from $\\mathcal{B}$ to $\\mathcal{A}$. The state space is partitioned into two disconnected regions. Consequently, the Markov chain is non-ergodic.\n\n### Part 2: Restoring Ergodicity and Derivation of Mixing Time\n\nThe problem proposes a collective translation move to restore ergodicity: $(x_1', x_2') = (x_1 + s, x_2 + s)$, where $s$ is chosen uniformly from $\\{+2, -2\\}$.\n\n**Restoring Ergodicity**:\n- If $(x_1, x_2) \\in \\mathcal{A}$, then $-1 \\le x_1, x_2 \\le 0$ and $x_1 \\le x_2$. Applying a translation with $s = +2$ gives $(x_1', x_2') = (x_1+2, x_2+2)$. The new coordinates satisfy $1 \\le x_1', x_2' \\le 2$. The ordering is preserved: $x_1 \\le x_2 \\implies x_1+2 \\le x_2+2 \\implies x_1' \\le x_2'$. Thus, the new state $(x_1', x_2')$ is guaranteed to be in $\\mathcal{B}$.\n- If $(x_1, x_2) \\in \\mathcal{B}$, then $1 \\le x_1, x_2 \\le 2$ and $x_1 \\le x_2$. Applying a translation with $s = -2$ gives $(x_1', x_2') = (x_1-2, x_2-2)$. The new coordinates satisfy $-1 \\le x_1', x_2' \\le 0$ and $x_1' \\le x_2'$. Thus, the new state $(x_1', x_2')$ is guaranteed to be in $\\mathcal{A}$.\nThis move successfully connects the two basins. When combined with the single-particle moves (which ensure ergodicity within each basin), the entire Markov chain becomes ergodic.\n\n**Detailed Balance**:\nThe stationary distribution $\\pi(x)$ for this system is uniform over the allowed region $\\mathcal{A} \\cup \\mathcal{B}$, because the potential $U(x)$ is constant (zero) there. Detailed balance requires $\\pi(x)P(x \\to x') = \\pi(x')P(x' \\to x)$, where $P(x \\to x') = T(x \\to x')A(x \\to x')$ is the transition probability, composed of the proposal probability $T$ and the acceptance probability $A$.\nFor the collective move between $x \\in \\mathcal{A}$ and $x' \\in \\mathcal{B}$, we have $\\pi(x) = \\pi(x')$ since the distribution is uniform. The potential energy is $U(x) = U(x') = 0$, so the acceptance probability is $A(x \\to x') = \\min\\{1, \\exp(-0)\\} = 1$. The proposal from $x$ to $x' = x+(s,s)$ and from $x'$ to $x = x'-(s,s)$ is symmetric because $s$ is chosen uniformly from $\\{+2, -2\\}$. Thus, $T(x \\to x') = T(x' \\to x)$. With $\\pi(x)=\\pi(x')$, $T(x \\to x')=T(x' \\to x)$, and $A(x \\to x')=A(x' \\to x)=1$, the detailed balance condition is satisfied.\n\n**Mixing Time for the Macrostate Chain**:\nWe analyze the two-state Markov chain on the macrostates $\\{\\mathcal{A}, \\mathcal{B}\\}$. The area of $\\mathcal{A}$ is $\\frac{1}{2}(1 \\times 1) = 1/2$ and the area of $\\mathcal{B}$ is $\\frac{1}{2}(1 \\times 1) = 1/2$. Thus, the stationary probabilities are equal: $\\pi(\\mathcal{A}) = \\pi(\\mathcal{B}) = 1/2$.\nA transition between macrostates occurs only via a collective move.\n- The probability of transitioning from $\\mathcal{A}$ to $\\mathcal{B}$ in one step is $P(\\mathcal{A} \\to \\mathcal{B}) = P(\\text{select collective}) \\times P(\\text{select } s=+2) = p_c \\times (1/2) = p_c/2$.\n- The probability of transitioning from $\\mathcal{B}$ to $\\mathcal{A}$ is $P(\\mathcal{B} \\to \\mathcal{A}) = p_c \\times (1/2) = p_c/2$.\nLet $\\alpha = p_c/2$. The transition matrix for the macrostates $(\\mathcal{A}, \\mathcal{B})$ is:\n$$\nM = \\begin{pmatrix} 1-\\alpha & \\alpha \\\\ \\alpha & 1-\\alpha \\end{pmatrix}\n$$\nThe eigenvalues are $\\lambda_1 = 1$ and $\\lambda_2 = 1 - 2\\alpha = 1 - p_c$. The rate of convergence to the stationary distribution is governed by the second largest eigenvalue modulus, $|\\lambda_2| = |1 - p_c|$. For $p_c \\in [0, 1]$, this is $1-p_c$.\nThe total variation distance $d_{TV}$ from the stationary distribution $\\pi_{macro} = [1/2, 1/2]^T$ after $t$ steps, starting from a pure state (worst case), is $d_{TV}(t) = \\frac{1}{2}|\\lambda_2|^t = \\frac{1}{2}(1-p_c)^t$.\nWe seek the mixing time $t_{\\mathrm{mix}}(\\varepsilon)$, the smallest integer $t$ such that $d_{TV}(t) \\le \\varepsilon$.\n$$\n\\frac{1}{2}(1-p_c)^t \\le \\varepsilon \\implies (1-p_c)^t \\le 2\\varepsilon\n$$\nTaking the natural logarithm:\n$$\nt \\ln(1-p_c) \\le \\ln(2\\varepsilon)\n$$\nFor $p_c \\in (0, 1)$, $\\ln(1-p_c)$ is negative, so we reverse the inequality:\n$$\nt \\ge \\frac{\\ln(2\\varepsilon)}{\\ln(1-p_c)}\n$$\nThe mixing time is the ceiling of this value:\n$$\nt_{\\mathrm{mix}}(\\varepsilon) = \\left\\lceil \\frac{\\ln(2\\varepsilon)}{\\ln(1-p_c)} \\right\\rceil\n$$\nSpecial cases:\n- If $p_c=0$, the denominator is $\\ln(1)=0$, so $t_{\\mathrm{mix}} = \\infty$.\n- If $p_c=1$, the macro transition probability $\\alpha = 1/2$. As per the problem, $t_{\\mathrm{mix}}=1$. This aligns with the fact that $\\lambda_2=0$, indicating immediate convergence.\n\n### Part 3: Implementation Plan\n\n1.  **Non-Ergodicity Test**:\n    - Discretize the state space with step size $h=0.5$. The valid coordinates for basin $\\mathcal{A}$ are $\\{-1.0, -0.5, 0.0\\}$ and for basin $\\mathcal{B}$ are $\\{1.0, 1.5, 2.0\\}$.\n    - Enumerate all states $(x_1, x_2)$ in $\\mathcal{A} \\cup \\mathcal{B}$ satisfying the constraints (e.g., $x_1 \\le x_2$). This results in $6$ states in $\\mathcal{A}$ and $6$ states in $\\mathcal{B}$, for a total of $12$ states.\n    - Build a graph where states are nodes. An edge exists between two nodes if one can be reached from the other via a single-particle displacement of magnitude $h=0.5$.\n    - Perform a graph traversal (e.g., Breadth-First or Depth-First Search) to count the number of connected components. If the count is greater than $1$, the system is non-ergodic on this discretization, and the boolean is `True`.\n\n2.  **Mixing Time Calculation**:\n    - Use the derived formula $t_{\\mathrm{mix}}(\\varepsilon) = \\lceil \\frac{\\ln(2\\varepsilon)}{\\ln(1-p_c)} \\rceil$ with $\\varepsilon = 10^{-3}$.\n    - For $p_c = 1.0$, use the special case rule $t_{\\mathrm{mix}}=1$.\n    - For $p_c \\in \\{0.2, 0.02\\}$, compute the formula. With $2\\varepsilon = 0.002$:\n      - $p_c=0.2$: $t = \\ln(0.002)/\\ln(0.8) \\approx 27.85 \\implies \\lceil 27.85 \\rceil = 28$.\n      - $p_c=0.02$: $t = \\ln(0.002)/\\ln(0.98) \\approx 307.65 \\implies \\lceil 307.65 \\rceil = 308$.\n    - For $p_c=0.0$, the mixing time is infinite.\n    - The results will be collected and formatted as specified.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the two-part problem:\n    1. Determines ergodicity of a discretized system via graph connectivity.\n    2. Calculates MCMC mixing times for a macrostate model.\n    \"\"\"\n\n    # --- Task 1: Check for non-ergodicity on a discretized grid ---\n\n    # Define the step size and coordinate grids for the two basins.\n    h = 0.5\n    coords_A = [-1.0, -0.5, 0.0]\n    coords_B = [1.0, 1.5, 2.0]\n\n    # Enumerate all valid states in basin A satisfying x1 <= x2.\n    states_A = []\n    for x1 in coords_A:\n        for x2 in coords_A:\n            if x1 <= x2:\n                states_A.append((x1, x2))\n    \n    # Enumerate all valid states in basin B satisfying x1 <= x2.\n    states_B = []\n    for x1 in coords_B:\n        for x2 in coords_B:\n            if x1 <= x2:\n                states_B.append((x1, x2))\n                \n    # Combine states from both basins and create a mapping for efficient lookup.\n    all_states = states_A + states_B\n    num_states = len(all_states)\n    state_to_idx = {state: i for i, state in enumerate(all_states)}\n\n    # Build an adjacency list for the graph.\n    adj = [[] for _ in range(num_states)]\n    for i, state in enumerate(all_states):\n        x1, x2 = state\n        # Define the four possible single-particle moves of magnitude h.\n        potential_moves = [(h, 0), (-h, 0), (0, h), (0, -h)]\n        for dx, dy in potential_moves:\n            # Calculate the neighbor coordinate.\n            # a direct key lookup will work as coordinates are exact multiples of h=0.5\n            neighbor_coord = (x1 + dx, x2 + dy)\n            if neighbor_coord in state_to_idx:\n                neighbor_idx = state_to_idx[neighbor_coord]\n                adj[i].append(neighbor_idx)\n\n    # Count connected components using Breadth-First Search (BFS) to determine ergodicity.\n    # Non-ergodicity is defined as having more than one connected component.\n    visited = [False] * num_states\n    num_components = 0\n    for i in range(num_states):\n        if not visited[i]:\n            num_components += 1\n            q = [i]\n            visited[i] = True\n            head = 0\n            while head < len(q):\n                u = q[head]\n                head += 1\n                for v in adj[u]:\n                    if not visited[v]:\n                        visited[v] = True\n                        q.append(v)\n    \n    non_ergodic_boolean = (num_components > 1)\n\n    # --- Task 2: Calculate mixing times for the two-state macrochain ---\n\n    pc_values = [1.0, 0.2, 0.02, 0.0]\n    epsilon = 1e-3\n    mixing_times = []\n\n    for pc in pc_values:\n        # The macrostate transition probability p(A->B) is alpha = pc/2.\n        # The problem defines a special case when this probability is 0.5.\n        if pc == 1.0: # Corresponds to macro transition prob = 0.5\n            mixing_times.append(1)\n        elif pc == 0.0: # No collective moves, so basins are disconnected.\n            mixing_times.append(float('inf'))\n        else:\n            # For 0 < pc < 1, use the derived analytical formula:\n            # t_mix = ceil(ln(2*eps) / ln(1-pc))\n            t_mix_float = np.log(2 * epsilon) / np.log(1 - pc)\n            mixing_times.append(int(np.ceil(t_mix_float)))\n\n    # --- Assemble and print the final output in the required format ---\n    \n    # Combine the boolean result with the list of mixing times.\n    # The boolean is formatted to lowercase as per common data standards.\n    results = [str(non_ergodic_boolean).lower()] + mixing_times\n    \n    # The final print statement must produce a single line in the specified format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3427345"}, {"introduction": "The power of the Metropolis-Hastings algorithm lies in its flexibility, but this requires careful implementation to satisfy the detailed balance condition. When proposal moves are not symmetric—for instance, when sampling rotational orientations using Euler angles—a specific correction, the Hastings factor, is essential for ensuring the correct target distribution. This exercise [@problem_id:3427343] guides you through the first-principles derivation of this factor, connecting the abstract theory of measure-theoretic probability to the practical challenge of writing a correct and efficient simulation code.", "problem": "In a Monte Carlo sampling of rotational degrees of freedom for a rigid asymmetric molecule immersed in an external field, the configuration space of orientations is the Special Orthogonal Group $\\mathrm{SO}(3)$ parameterized by $\\mathrm{ZYZ}$ Euler angles $(\\phi,\\theta,\\psi)$ with ranges $\\phi \\in [0,2\\pi)$, $\\theta \\in [0,\\pi]$, and $\\psi \\in [0,2\\pi)$. The invariant volume element on $\\mathrm{SO}(3)$, known as the Haar measure, has the local form $d\\mu(\\phi,\\theta,\\psi) = \\sin\\theta\\, d\\phi\\, d\\theta\\, d\\psi$ in these coordinates. The physical target distribution over orientations has a density with respect to the Haar measure given by $\\pi(R) \\propto \\exp(-\\beta U(R))$, where $R \\in \\mathrm{SO}(3)$ encodes the orientation and $U(R)$ is the potential energy; $\\beta$ is the inverse temperature defined as $\\beta = 1/(k_{B}T)$ with $k_{B}$ the Boltzmann constant and $T$ the absolute temperature.\n\nA Metropolis-Hastings (MH) update is constructed by proposing a new orientation in angle coordinates via an independence proposal that draws $(\\phi',\\theta',\\psi')$ from the product measure with $\\phi'$ and $\\psi'$ uniform on $[0,2\\pi)$ and $\\theta'$ uniform on $[0,\\pi]$, independently of the current $(\\phi,\\theta,\\psi)$. This proposal is non-uniform with respect to the Haar measure on $\\mathrm{SO}(3)$ because it is uniform in the Euler angles rather than in the invariant measure.\n\nStarting from the definition of detailed balance on $\\mathrm{SO}(3)$ and using change-of-variables and absolute continuity of densities under the coordinate map $(\\phi,\\theta,\\psi) \\mapsto R(\\phi,\\theta,\\psi)$, derive the Hastings correction factor that arises solely from the Jacobian $\\sin\\theta$ of the angular measure when expressing the target density with respect to Lebesgue measure in angle coordinates. Provide the final Hastings factor as a closed-form expression depending only on the current polar angle $\\theta$ and the proposed polar angle $\\theta'$. Do not include energy, temperature, or proposal density terms; isolate only the factor that corrects for the Jacobian. The final answer must be a single analytical expression.", "solution": "The problem requires the derivation of the Hastings correction factor for a Metropolis-Hastings (MH) Monte Carlo update on the orientation space $\\mathrm{SO}(3)$ of a rigid molecule. The correction arises because the proposal mechanism is uniform in the ZYZ Euler angle coordinates $(\\phi, \\theta, \\psi)$, while the invariant measure on the group $\\mathrm{SO}(3)$ is not.\n\nThe core of the Metropolis-Hastings algorithm is the detailed balance condition, which ensures that the desired target probability distribution, $\\pi$, is the stationary distribution of the generated Markov chain. For any two states $x$ and $x'$, detailed balance requires:\n$$\n\\pi(x) q(x'|x) A(x'|x) = \\pi(x') q(x|x') A(x|x')\n$$\nwhere $q(x'|x)$ is the density for proposing a move from state $x$ to $x'$, and $A(x'|x)$ is the probability of accepting that move. The standard solution for the acceptance probability is:\n$$\nA(x'|x) = \\min\\left(1, \\frac{\\pi(x') q(x|x')}{\\pi(x) q(x'|x)}\\right)\n$$\nThe term $\\frac{q(x|x')}{q(x'|x)}$ is known as the Hastings correction factor. A critical requirement for this formalism is that all probability densities, $\\pi$ and $q$, must be expressed with respect to the same underlying reference measure.\n\nLet the state of the system be an orientation $R \\in \\mathrm{SO}(3)$. The natural and most elegant choice for the reference measure on this space is the invariant Haar measure, $d\\mu(R)$. In terms of the ZYZ Euler angles $(\\phi, \\theta, \\psi)$, this measure has the local expression $d\\mu(R) = \\sin\\theta \\, d\\phi \\, d\\theta \\, d\\psi$.\n\nFirst, we define the target density with respect to this Haar measure. The problem states that the physical target distribution has a density $\\pi(R)$ with respect to the Haar measure given by $\\pi(R) \\propto \\exp(-\\beta U(R))$. We can write this as $\\pi(R) = C_{\\pi} \\exp(-\\beta U(R))$, where $C_{\\pi}$ is a normalization constant.\n\nNext, we must determine the proposal density $q(R'|R)$ with respect to the same Haar measure $d\\mu$. The problem describes an independence proposal where the new Euler angles $(\\phi', \\theta', \\psi')$ are drawn independently of the current state. The angles $\\phi'$ and $\\psi'$ are drawn uniformly from $[0, 2\\pi)$, and $\\theta'$ is drawn uniformly from $[0, \\pi]$. The probability density of this proposal, with respect to the Lebesgue measure on the coordinate space, $d\\lambda = d\\phi' d\\theta' d\\psi'$, is:\n$$\ng_{\\lambda}(\\phi', \\theta', \\psi') = \\frac{1}{2\\pi} \\cdot \\frac{1}{\\pi} \\cdot \\frac{1}{2\\pi} = \\frac{1}{4\\pi^2}\n$$\nThis is the proposal density in the coordinate space. To find the proposal density $q_{\\mu}(R'|R)$ with respect to the Haar measure $d\\mu(R')$, we use the principle of conservation of probability. The probability of proposing a state in an infinitesimal volume must be independent of the coordinate system used to describe that volume.\n$$\nq_{\\mu}(R'|R) \\, d\\mu(R') = g_{\\lambda}(\\phi', \\theta', \\psi') \\, d\\lambda\n$$\nWe know the relationship between the two volume elements is $d\\mu(R') = \\sin\\theta' \\, d\\lambda$. Substituting $d\\lambda = d\\mu(R') / \\sin\\theta'$ into the equation gives:\n$$\nq_{\\mu}(R'|R) \\, d\\mu(R') = g_{\\lambda}(\\phi', \\theta', \\psi') \\, \\frac{d\\mu(R')}{\\sin\\theta'}\n$$\nThus, the proposal density with respect to the Haar measure is:\n$$\nq_{\\mu}(R'|R) = \\frac{g_{\\lambda}(\\phi', \\theta', \\psi')}{\\sin\\theta'} = \\frac{1}{4\\pi^2 \\sin\\theta'}\n$$\nThis is an independence sampler, so the proposal density depends only on the proposed state $R'$, which corresponds to angles $(\\phi', \\theta', \\psi')$, and not on the current state $R$. So, $q_{\\mu}(R'|R) = q_{\\mu}(R')$. The density for the reverse proposal, from $R'$ to $R$, is found by replacing the primed variables with unprimed ones:\n$$\nq_{\\mu}(R|R') = \\frac{1}{4\\pi^2 \\sin\\theta}\n$$\nNow we can compute the Hastings correction factor, which is the ratio of the reverse to the forward proposal densities:\n$$\n\\frac{q_{\\mu}(R|R')}{q_{\\mu}(R'|R)} = \\frac{\\frac{1}{4\\pi^2 \\sin\\theta}}{\\frac{1}{4\\pi^2 \\sin\\theta'}} = \\frac{4\\pi^2 \\sin\\theta'}{4\\pi^2 \\sin\\theta} = \\frac{\\sin\\theta'}{\\sin\\theta}\n$$\nThis factor accounts for the fact that proposing angles uniformly is not equivalent to proposing orientations uniformly on the sphere of directions associated with the polar angle. Specifically, a uniform proposal in $\\theta$ is more likely to generate orientations near the poles ($\\theta=0$ or $\\theta=\\pi$) where the invariant volume element $\\sin\\theta \\, d\\theta$ is small. The Hastings factor correctly compensates for this bias.\n\nThe full acceptance ratio would be $\\frac{\\pi(R')}{\\pi(R)} \\frac{q_{\\mu}(R|R')}{q_{\\mu}(R'|R)} = \\exp(-\\beta(U(R')-U(R))) \\frac{\\sin\\theta'}{\\sin\\theta}$. The problem asks to isolate only the Hastings correction factor itself, which depends only on the current and proposed polar angles, $\\theta$ and $\\theta'$.", "answer": "$$\\boxed{\\frac{\\sin\\theta'}{\\sin\\theta}}$$", "id": "3427343"}, {"introduction": "Averages computed from Monte Carlo simulations are not guaranteed to be meaningful; their reliability rests on foundational statistical results like the Law of Large Numbers and the Central Limit Theorem. This practice explores a critical failure mode that occurs when sampling observables with heavy-tailed distributions, where rare but extreme events can lead to an infinite variance. By analyzing the consequences for the convergence of the sample mean [@problem_id:3427346], you will develop a deeper understanding of the conditions required for robust error analysis and learn to identify situations where standard statistical estimators may be misleading.", "problem": "Consider equilibrium Molecular Dynamics (MD) sampling of configurations $X \\in \\mathbb{R}^{3N}$ from the canonical (Boltzmann) distribution $\\pi(x) \\propto \\exp(-\\beta U(x))$ at temperature $T$, where $U(x)$ is the potential energy and $\\beta = 1/(k_{\\mathrm{B}} T)$ with $k_{\\mathrm{B}}$ the Boltzmann constant. Let $A(x)$ be a scalar observable of interest whose distribution under $\\pi(x)$ is heavy-tailed due to rare but large-magnitude contributions (for example, near-contact singularities in a virial-type estimator). Define the Monte Carlo estimator based on $n$ samples $X_1, \\dots, X_n \\sim \\pi$ (independent and identically distributed (i.i.d.) or arising from an ergodic Markov chain Monte Carlo (MCMC) trajectory with stationary distribution $\\pi$) by\n$$\n\\hat{\\mu}_n = \\frac{1}{n} \\sum_{i=1}^{n} A(X_i),\n$$\nwhich aims to estimate the ensemble average\n$$\n\\mu = \\mathbb{E}_{\\pi}[A(X)].\n$$\nAssume the heavy-tailed behavior of $A(X)$ under $\\pi$ is characterized by a tail index $\\alpha > 0$ in the sense that $\\mathbb{P}_{\\pi}(|A| > t)$ decays regularly with exponent $\\alpha$ as $t \\to \\infty$. Using fundamental definitions of expectation and variance, the Law of Large Numbers (LLN), and the Central Limit Theorem (CLT), evaluate how the existence (or non-existence) of $\\mathbb{E}_{\\pi}[A]$ and $\\mathrm{Var}_{\\pi}(A)$ affects the validity and rate of convergence of $\\hat{\\mu}_n$.\n\nSelect all statements that are correct.\n\nA. If $\\mathbb{E}_{\\pi}[A]$ exists but $\\mathrm{Var}_{\\pi}(A)$ is infinite, then $\\hat{\\mu}_n$ is consistent for $\\mu$ by the Law of Large Numbers, but the classical Central Limit Theorem and the $\\mathcal{O}(n^{-1/2})$ error decay fail; when the tails have index $\\alpha \\in (1,2)$, the fluctuations of $\\hat{\\mu}_n$ scale as $n^{-(1 - 1/\\alpha)}$ and the properly normalized error converges to an $\\alpha$-stable law.\n\nB. If $\\mathbb{E}_{\\pi}[A]$ does not exist (for example, tail index $\\alpha \\le 1$), then $\\hat{\\mu}_n$ remains an unbiased estimator for $\\mu$ and converges almost surely to a finite constant despite the infinite mean.\n\nC. If $\\mathrm{Var}_{\\pi}(A)$ is finite, and samples are i.i.d. or come from a geometrically ergodic Markov chain with summable autocorrelations, then $\\hat{\\mu}_n$ satisfies a Central Limit Theorem with asymptotically normal errors and root-$n$ convergence rate, with the effective variance inflated by autocorrelation relative to the i.i.d. case.\n\nD. Symmetry of the heavy-tailed distribution of $A$ (for example, $A$ has symmetric Pareto tails) is sufficient to restore the classical Central Limit Theorem even when $\\mathrm{Var}_{\\pi}(A)$ is infinite, so $\\hat{\\mu}_n$ achieves $\\mathcal{O}(n^{-1/2})$ error decay.\n\nE. When $\\mathrm{Var}_{\\pi}(A)$ is infinite but $\\mathbb{E}_{\\pi}[A]$ exists, no convergence of $\\hat{\\mu}_n$ can be guaranteed; the sample mean fails to converge in probability to $\\mu$.", "solution": "The problem statement is first subjected to validation.\n\n### Step 1: Extract Givens\n- **System**: Equilibrium Molecular Dynamics (MD) sampling of configurations $X \\in \\mathbb{R}^{3N}$.\n- **Distribution**: Canonical (Boltzmann) distribution $\\pi(x) \\propto \\exp(-\\beta U(x))$ at temperature $T$, with $\\beta = 1/(k_{\\mathrm{B}} T)$.\n- **Observable**: A scalar function $A(x)$ whose distribution under $\\pi(x)$ is heavy-tailed.\n- **Heavy-Tail Property**: $\\mathbb{P}_{\\pi}(|A| > t)$ decays regularly with exponent $\\alpha > 0$ as $t \\to \\infty$. This implies that $\\mathbb{P}_{\\pi}(|A| > t) \\sim L(t)t^{-\\alpha}$ for some slowly varying function $L(t)$. For simplicity in analysis, we can consider the case $L(t)$ is a constant, so $\\mathbb{P}_{\\pi}(|A| > t) \\propto t^{-\\alpha}$.\n- **Estimator**: The Monte Carlo sample mean $\\hat{\\mu}_n = \\frac{1}{n} \\sum_{i=1}^{n} A(X_i)$.\n- **Target**: The ensemble average $\\mu = \\mathbb{E}_{\\pi}[A(X)]$.\n- **Sampling**: Samples $X_1, \\dots, X_n$ are either independent and identically distributed (i.i.d.) from $\\pi$ or from an ergodic Markov chain Monte Carlo (MCMC) trajectory with stationary distribution $\\pi$.\n- **Task**: Evaluate the convergence properties of $\\hat{\\mu}_n$ based on the existence of the mean $\\mathbb{E}_{\\pi}[A]$ and variance $\\mathrm{Var}_{\\pi}(A)$, which are determined by the tail index $\\alpha$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is scientifically grounded, well-posed, and objective.\n- **Scientific Grounding**: The problem is set in the standard framework of statistical mechanics and Monte Carlo simulation, using fundamental concepts like the Boltzmann distribution, ensemble averages, and statistical estimators. The issue of heavy-tailed observables is a known and significant challenge in molecular simulations, particularly for observables like the virial or components of the pressure tensor.\n- **Well-Posedness**: The question is clearly defined and asks for an evaluation of provided statements based on established theorems in probability theory, namely the Law of Large Numbers (LLN) and the Central Limit Theorem (CLT), including their generalizations to heavy-tailed distributions (stable laws). There is sufficient information to proceed with a rigorous analysis.\n- **Objectivity**: The problem uses precise mathematical and physical terminology, free from ambiguity or subjective content.\n\n### Step 3: Verdict and Action\nThe problem statement is **valid**. The analysis can proceed.\n\n### Derivation of Principles\nThe convergence properties of the estimator $\\hat{\\mu}_n$ depend critically on the existence of the moments of the random variable $A(X)$. For a random variable $A$ with a probability distribution such that $\\mathbb{P}(|A| > t) \\propto t^{-\\alpha}$ for large $t$, the $k$-th absolute moment $\\mathbb{E}[|A|^k]$ is finite if and only if $k < \\alpha$.\n\n1.  **Existence of the Mean ($\\mu$)**: The mean $\\mu = \\mathbb{E}_{\\pi}[A]$ exists and is finite if the first absolute moment $\\mathbb{E}_{\\pi}[|A|]$ is finite. This condition holds if and only if $1 < \\alpha$. If $\\alpha \\le 1$, the mean is either infinite or, in the case of symmetric distributions, may be considered zero by convention, but the integral for the expected value does not converge absolutely.\n\n2.  **Existence of the Variance ($\\mathrm{Var}_{\\pi}(A)$)**: The variance $\\mathrm{Var}_{\\pi}(A) = \\mathbb{E}_{\\pi}[A^2] - \\mu^2$ exists and is finite if the second moment $\\mathbb{E}_{\\pi}[A^2]$ is finite. This condition holds if and only if $2 < \\alpha$.\n\n3.  **Law of Large Numbers (LLN)**: Khinchine's Weak LLN states that if $A_1, A_2, \\dots$ are i.i.d. random variables with finite mean $\\mathbb{E}[A_i] = \\mu$, then the sample mean $\\hat{\\mu}_n$ converges in probability to $\\mu$. The stronger version, Kolmogorov's Strong LLN, guarantees almost sure convergence under the same condition. Therefore, $\\hat{\\mu}_n$ is a consistent estimator for $\\mu$ if $\\alpha > 1$.\n\n4.  **Central Limit Theorem (CLT)**:\n    - **Classical CLT**: If $A_1, A_2, \\dots$ are i.i.d. with finite mean $\\mu$ and finite variance $\\sigma^2 > 0$, then $\\sqrt{n}(\\hat{\\mu}_n - \\mu)$ converges in distribution to a normal distribution $\\mathcal{N}(0, \\sigma^2)$. This applies when $\\alpha > 2$. The error $|\\hat{\\mu}_n - \\mu|$ scales as $\\mathcal{O}(n^{-1/2})$.\n    - **Generalized CLT**: If the variance is infinite ($\\alpha \\le 2$) but the mean is finite ($\\alpha > 1$), i.e., for $\\alpha \\in (1, 2]$, the classical CLT does not apply. Instead, a generalized version applies. For $\\alpha \\in (1, 2)$, the properly normalized sum converges to a non-Gaussian, strictly $\\alpha$-stable distribution. The normalization factor for the sum $\\sum_{i=1}^n (A_i - \\mu)$ is $n^{1/\\alpha}$. Consequently, the error of the mean, $(\\hat{\\mu}_n - \\mu) = \\frac{1}{n}\\sum_{i=1}^n(A_i - \\mu)$, has fluctuations that scale as $n^{1/\\alpha}/n = n^{1/\\alpha - 1} = n^{-(1-1/\\alpha)}$. Since $1 < \\alpha < 2$, we have $1/2 < 1/\\alpha < 1$, and $0 < 1-1/\\alpha < 1/2$. The convergence rate is slower than $\\mathcal{O}(n^{-1/2})$.\n\n5.  **MCMC Sampling**: For samples from an ergodic Markov chain, the LLN holds. For a CLT to hold (in the finite variance case, $\\alpha > 2$), stronger conditions are needed, such as geometric ergodicity and summable autocorrelations. In this case, the asymptotic variance in the CLT is modified to $\\sigma^2_{\\text{eff}} = \\sigma^2(1 + 2\\sum_{k=1}^\\infty \\rho_k)$, where $\\sigma^2$ is the variance of $A$ and $\\rho_k$ is the autocorrelation function of the process $\\{A(X_i)\\}_{i \\ge 1}$ at lag $k$.\n\n### Option-by-Option Analysis\n\n**A. If $\\mathbb{E}_{\\pi}[A]$ exists but $\\mathrm{Var}_{\\pi}(A)$ is infinite, then $\\hat{\\mu}_n$ is consistent for $\\mu$ by the Law of Large Numbers, but the classical Central Limit Theorem and the $\\mathcal{O}(n^{-1/2})$ error decay fail; when the tails have index $\\alpha \\in (1,2)$, the fluctuations of $\\hat{\\mu}_n$ scale as $n^{-(1 - 1/\\alpha)}$ and the properly normalized error converges to an $\\alpha$-stable law.**\n\n- The condition \"$\\mathbb{E}_{\\pi}[A]$ exists but $\\mathrm{Var}_{\\pi}(A)$ is infinite\" corresponds to a tail index $\\alpha \\in (1, 2]$.\n- Since $\\alpha > 1$, the mean $\\mu$ is finite. By the Law of Large Numbers, the sample mean $\\hat{\\mu}_n$ is a consistent estimator for $\\mu$ (it converges in probability and almost surely). This part is correct.\n- Since $\\alpha \\le 2$, the variance is infinite. The classical CLT requires finite variance, so it fails. Consequently, the standard $\\mathcal{O}(n^{-1/2})$ error decay also fails. This part is correct.\n- For $\\alpha \\in (1, 2)$, the Generalized CLT applies. The error of the mean, $\\hat{\\mu}_n - \\mu$, has fluctuations that scale as $n^{-(1-1/\\alpha)}$. The normalized error, e.g., $n^{1-1/\\alpha}(\\hat{\\mu}_n - \\mu)$, converges in distribution to an $\\alpha$-stable law. This part is a correct statement of the Generalized CLT.\n- **Verdict: Correct.**\n\n**B. If $\\mathbb{E}_{\\pi}[A]$ does not exist (for example, tail index $\\alpha \\le 1$), then $\\hat{\\mu}_n$ remains an unbiased estimator for $\\mu$ and converges almost surely to a finite constant despite the infinite mean.**\n\n- If $\\mathbb{E}_{\\pi}[A]$ does not exist, the quantity $\\mu$ it is supposed to estimate is not well-defined as a finite number. The definition of an unbiased estimator $\\hat{\\theta}$ for a parameter $\\theta$ is $\\mathbb{E}[\\hat{\\theta}] = \\theta$. If $\\mathbb{E}[A]$ does not exist, then $\\mathbb{E}[\\hat{\\mu}_n] = \\mathbb{E}[A]$ also does not exist, so the concept of being unbiased is meaningless.\n- The Strong Law of Large Numbers requires a finite mean. If $\\alpha \\le 1$, the LLN does not hold. The sample mean $\\hat{\\mu}_n$ does not converge to a finite constant. The behavior of $\\hat{\\mu}_n$ is dominated by the most extreme value in the sample, leading to erratic jumps rather than convergence.\n- **Verdict: Incorrect.**\n\n**C. If $\\mathrm{Var}_{\\pi}(A)$ is finite, and samples are i.i.d. or come from a geometrically ergodic Markov chain with summable autocorrelations, then $\\hat{\\mu}_n$ satisfies a Central Limit Theorem with asymptotically normal errors and root-$n$ convergence rate, with the effective variance inflated by autocorrelation relative to the i.i.d. case.**\n\n- \"$\\mathrm{Var}_{\\pi}(A)$ is finite\" implies $\\alpha > 2$. This is the domain of the classical CLT.\n- For i.i.d. samples, the statement is a textbook definition of the Lindeberg-Lévy CLT, which gives a normal distribution for the error and a convergence rate of $\\mathcal{O}(n^{-1/2})$ (root-$n$).\n- For MCMC samples, the conditions \"geometrically ergodic\" and \"summable autocorrelations\" are standard sufficient conditions for a CLT for Markov chains to hold.\n- The asymptotic variance in the MCMC CLT is given by $\\sigma^2_{\\text{eff}} = \\sigma^2(1 + 2\\sum_{k=1}^\\infty \\rho_k)$. Since physical processes in MD typically exhibit positive correlations ($\\rho_k > 0$ for small $k$), the sum is positive, and the effective variance is indeed inflated compared to the i.i.d. variance $\\sigma^2$.\n- **Verdict: Correct.**\n\n**D. Symmetry of the heavy-tailed distribution of $A$ (for example, $A$ has symmetric Pareto tails) is sufficient to restore the classical Central Limit Theorem even when $\\mathrm{Var}_{\\pi}(A)$ is infinite, so $\\hat{\\mu}_n$ achieves $\\mathcal{O}(n^{-1/2})$ error decay.**\n\n- The fundamental requirement for the classical CLT is a finite second moment (variance). Symmetry of the distribution does not make an infinite variance finite.\n- If the variance is infinite (e.g., $\\alpha \\in (1, 2)$), the limit distribution for the normalized sample mean is an $\\alpha$-stable law, as stated in the Generalized CLT. Symmetry of the distribution of $A$ implies that the limiting stable law is also symmetric, but it is not a Gaussian law (unless $\\alpha=2$).\n- The rate of convergence is $n^{-(1-1/\\alpha)}$, which is slower than $n^{-1/2}$ for $\\alpha < 2$. Symmetry does not change this scaling.\n- **Verdict: Incorrect.**\n\n**E. When $\\mathrm{Var}_{\\pi}(A)$ is infinite but $\\mathbb{E}_{\\pi}[A]$ exists, no convergence of $\\hat{\\mu}_n$ can be guaranteed; the sample mean fails to converge in probability to $\\mu$.**\n\n- The condition \"$\\mathrm{Var}_{\\pi}(A)$ is infinite but $\\mathbb{E}_{\\pi}[A]$ exists\" corresponds to $\\alpha \\in (1, 2]$.\n- This statement directly contradicts the Law of Large Numbers. The LLN guarantees that if the mean $\\mu$ is finite (which it is, since $\\alpha > 1$), then the sample mean $\\hat{\\mu}_n$ converges to $\\mu$ (both in probability and almost surely).\n- A finite variance is a condition for the CLT, not for the LLN. This statement incorrectly conflates the requirements of the two theorems.\n- **Verdict: Incorrect.**", "answer": "AC", "id": "3427346"}]}