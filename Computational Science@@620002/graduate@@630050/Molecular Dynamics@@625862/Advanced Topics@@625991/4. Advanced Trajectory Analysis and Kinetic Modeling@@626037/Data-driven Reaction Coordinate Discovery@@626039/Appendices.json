{"hands_on_practices": [{"introduction": "This problem provides a foundational exercise connecting the abstract formalism of time-lagged independent component analysis (tICA) to a concrete, analytically solvable physical system. By deriving the tICA eigenvalues for a particle in a simple quadratic potential (an Ornstein-Uhlenbeck process), you will see how the method's output relates directly to the underlying physical parameters like diffusion and potential stiffness. This practice is invaluable for building intuition about what tICA fundamentally measures: the timescales of decorrelation for the system's slowest collective motions. [@problem_id:3407091]", "problem": "Consider an overdamped Langevin dynamics in two spatial dimensions for a particle in a quadratic potential. The state vector is $x(t) \\in \\mathbb{R}^{2}$ and evolves according to\n$$\n\\mathrm{d}x(t) \\;=\\; - D \\, \\beta \\, K \\, x(t)\\, \\mathrm{d}t \\;+\\; \\sqrt{2D}\\, \\mathrm{d}W(t),\n$$\nwhere $D$ is the scalar diffusion coefficient, $\\beta = 1/(k_{B}T)$ is the inverse thermal energy at constant temperature $T$, $K \\in \\mathbb{R}^{2\\times 2}$ is a symmetric positive-definite stiffness matrix defining the quadratic potential $U(x) = \\tfrac{1}{2} x^{\\top} K x$, and $W(t)$ is a standard two-dimensional Wiener process. Assume the process is stationary and ergodic under the Boltzmann distribution $\\pi(x) \\propto \\exp(-\\beta U(x))$.\n\nDefine the time-lagged covariance matrix at lag time $\\tau > 0$ by\n$$\nC(\\tau) \\;=\\; \\mathbb{E}_{\\mathrm{stat}}\\!\\left[\\, x(t) \\, x(t+\\tau)^{\\top} \\,\\right],\n$$\nand the instantaneous covariance by $C(0) = \\mathbb{E}_{\\mathrm{stat}}\\!\\left[\\, x(t)\\, x(t)^{\\top} \\,\\right]$. For a long discrete-time trajectory $\\{x_{n}\\}_{n=0}^{N}$ sampled at interval $\\Delta t$, the standard ergodic estimator of $C(\\tau)$ at $\\tau = m \\Delta t$ is\n$$\n\\widehat{C}(\\tau) \\;=\\; \\frac{1}{N-m} \\sum_{n=0}^{N-m-1} x_{n} \\, x_{n+m}^{\\top},\n$$\nand similarly $\\widehat{C}(0) = \\frac{1}{N} \\sum_{n=0}^{N-1} x_{n} x_{n}^{\\top}$. Assume $N$ is sufficiently large that $\\widehat{C}(\\tau)$ and $\\widehat{C}(0)$ converge to $C(\\tau)$ and $C(0)$, respectively.\n\nTime-lagged independent component analysis (tICA) defines reaction coordinates by solving the generalized eigenvalue problem\n$$\nC(\\tau)\\, v \\;=\\; \\lambda(\\tau)\\, C(0)\\, v,\n$$\nwith eigenvalues $\\lambda(\\tau)$ and generalized eigenvectors $v \\neq 0$. Let the physical parameters be\n$$\nK \\;=\\; \\begin{pmatrix} 3 & 1 \\\\ 1 & 2 \\end{pmatrix} \\;\\;\\text{in units of}\\;\\; k_{B}T/\\text{nm}^{2}, \\qquad D \\;=\\; 0.5 \\;\\;\\text{in units of}\\;\\; \\text{nm}^{2}/\\text{ps}.\n$$\n\nStarting only from the definitions above and fundamental properties of linear stochastic differential equations and equilibrium statistical mechanics, derive the analytic expression for the leading (largest) tICA generalized eigenvalue $\\lambda_{\\max}(\\tau)$ in terms of $D$, $\\beta$, the spectrum of $K$, and $\\tau$. Then evaluate $\\lambda_{\\max}(\\tau)$ at lag time $\\tau = 1.5$ (in units of ps). Your answer must be a single real number. Round your answer to four significant figures. The eigenvalue is dimensionless, so report a pure number without units.", "solution": "The time-lagged covariance matrix $C(\\tau)$ for an Ornstein-Uhlenbeck process is related to the instantaneous covariance matrix $C(0)$ by $C(\\tau) = C(0) e^{-D\\beta K \\tau}$, where we have used the fact that the stiffness matrix $K$ is symmetric.\n\nSubstituting this relationship into the tICA generalized eigenvalue problem, $C(\\tau)v = \\lambda(\\tau)C(0)v$, we get:\n$$\nC(0) e^{-D\\beta K \\tau} v = \\lambda(\\tau) C(0) v\n$$\nSince $K$ is positive-definite, $C(0)$ is invertible. Left-multiplying by $C(0)^{-1}$ simplifies the equation to a standard eigenvalue problem:\n$$\ne^{-D\\beta K \\tau} v = \\lambda(\\tau) v\n$$\nThis shows that the tICA generalized eigenvalues $\\lambda(\\tau)$ are the eigenvalues of the matrix $e^{-D\\beta K \\tau}$.\n\nThe eigenvalues of a matrix exponential $e^M$ are $e^{\\mu_i}$, where $\\mu_i$ are the eigenvalues of the matrix $M$. Here, $M = -D\\beta K \\tau$. The eigenvalues of $M$ are $-D\\beta k_i \\tau$, where $k_i$ are the eigenvalues of $K$. Therefore, the tICA eigenvalues are:\n$$\n\\lambda_i(\\tau) = e^{-D\\beta k_i \\tau}\n$$\nTo find the largest tICA eigenvalue, $\\lambda_{\\max}(\\tau)$, we must use the smallest eigenvalue of $K$, denoted $k_{\\min}$, because the exponent is negative. The problem states $K$ is in units of $k_B T / \\text{nm}^2$, so we can use the numerical values of $K$ directly for the quantity $\\beta K$.\n$$\n\\lambda_{\\max}(\\tau) = e^{-D k_{\\min} \\tau}\n$$\nFirst, we find the eigenvalues of $K = \\begin{pmatrix} 3 & 1 \\\\ 1 & 2 \\end{pmatrix}$ by solving the characteristic equation $\\det(K - kI) = 0$:\n$$\n(3-k)(2-k) - 1 = k^2 - 5k + 5 = 0\n$$\nThe roots are $k = \\frac{5 \\pm \\sqrt{25 - 20}}{2} = \\frac{5 \\pm \\sqrt{5}}{2}$. The minimum eigenvalue is $k_{\\min} = \\frac{5 - \\sqrt{5}}{2}$.\n\nNow, we substitute the given values: $D = 0.5 \\text{ nm}^2/\\text{ps}$ and $\\tau = 1.5 \\text{ ps}$.\n$$\n\\lambda_{\\max}(1.5) = \\exp\\left( -0.5 \\cdot \\left(\\frac{5 - \\sqrt{5}}{2}\\right) \\cdot 1.5 \\right)\n$$\n$$\n\\lambda_{\\max}(1.5) = \\exp\\left( -0.375 \\cdot (5 - \\sqrt{5}) \\right)\n$$\n$$\n\\lambda_{\\max}(1.5) \\approx \\exp\\left( -0.375 \\cdot (5 - 2.236068) \\right) \\approx \\exp(-1.0364745) \\approx 0.35471\n$$\nRounding to four significant figures gives $0.3547$.", "answer": "$$\\boxed{0.3547}$$", "id": "3407091"}, {"introduction": "In practice, tICA is often a precursor to building a Markov State Model (MSM), which describes the dynamics as a set of jumps between discrete states. This exercise demonstrates the deep connection between these two methods by showing that tICA, when applied to indicator functions of metastable states, recovers the eigenvalues of the MSM's transition matrix. You will practice calculating these eigenvalues and identifying the \"spectral gap,\" a crucial quantity that measures the separation between fast and slow timescales and quantifies the system's metastability. [@problem_id:3407097]", "problem": "A stationary, ergodic molecular dynamics trajectory is coarse-grained into three disjoint metastable regions, producing a reversible discrete-time Markov chain at lag time $\\tau$ with stationary distribution $\\boldsymbol{\\pi} = (\\pi_1,\\pi_2,\\pi_3)^{\\top}$ and row-stochastic transition matrix $P$. The reversibility condition (detailed balance) states that $\\pi_i P_{ij} = \\pi_j P_{ji}$ for all $i,j$. Consider the data-driven discovery of slow reaction coordinates via Time-lagged Independent Component Analysis (tICA), defined by the generalized eigenproblem constructed from the zero-lag and time-lagged covariance matrices of a zero-mean feature vector built from indicator functions of the metastable regions.\n\nYou are given the stationary distribution and transition matrix\n$$\n\\boldsymbol{\\pi} = \\begin{pmatrix} 0.5 \\\\ 0.4 \\\\ 0.1 \\end{pmatrix}, \\qquad\nP = \\begin{pmatrix}\n0.83 & 0.16 & 0.01 \\\\\n0.20 & 0.79 & 0.01 \\\\\n0.05 & 0.04 & 0.91\n\\end{pmatrix}.\n$$\nStarting from the fundamental definitions of the Markov transfer operator acting on functions on the state space and the tICA generalized eigenproblem constructed from stationary time correlations, derive the operator whose spectrum encodes the slow dynamics under the detailed balance assumption, and compute its nontrivial spectrum from $P$. Then, define the metastability spectral gap $g$ as the difference between the second and third largest eigenvalues of this operator and compute $g$ exactly.\n\nExpress your final answer as a single dimensionless real number. Do not round; provide the exact value.", "solution": "The problem statement has been validated and is found to be self-contained, scientifically grounded, and well-posed. The provided stationary distribution $\\boldsymbol{\\pi}$ and transition matrix $P$ are consistent with the definitions of a reversible Markov chain. Specifically, $P$ is row-stochastic, $\\boldsymbol{\\pi}^\\top P = \\boldsymbol{\\pi}^\\top$, and the detailed balance condition $\\pi_i P_{ij} = \\pi_j P_{ji}$ holds for all $i, j$.\n\nThe problem asks for the derivation of the operator whose spectrum encodes the slow dynamics of the system and the computation of its nontrivial spectrum. In the context of a reversible Markov State Model (MSM), the slow dynamics are described by the eigenvalues of the transition operator, which is represented by the matrix $P$. While $P$ itself is not generally symmetric, the detailed balance condition implies that it is self-adjoint with respect to the weighted inner product $\\langle \\mathbf{f}, \\mathbf{g} \\rangle_{\\pi} = \\sum_i \\pi_i f_i g_i$, where $\\mathbf{f}$ and $\\mathbf{g}$ are functions on the state space represented as vectors.\n\nLet $\\Pi = \\text{diag}(\\boldsymbol{\\pi})$ be the diagonal matrix with the stationary probabilities on its diagonal. The detailed balance condition can be written in matrix form as $\\Pi P = (\\Pi P)^\\top$. This property ensures that $P$ is similar to a symmetric matrix. Consider the matrix $T = \\Pi^{1/2} P \\Pi^{-1/2}$. The similarity transformation preserves the eigenvalues. $T$ is symmetric because\n$$\nT^\\top = (\\Pi^{1/2} P \\Pi^{-1/2})^\\top = (\\Pi^{-1/2})^\\top P^\\top (\\Pi^{1/2})^\\top = \\Pi^{-1/2} P^\\top \\Pi^{1/2}.\n$$\nMultiplying by $\\Pi^{1/2}$ on the left and right, we have $\\Pi^{1/2} T^\\top \\Pi^{1/2} = P^\\top \\Pi$. Since $\\Pi P = (\\Pi P)^\\top = P^\\top \\Pi$, we get $\\Pi^{1/2} T^\\top \\Pi^{1/2} = \\Pi P$. This leads to $T^\\top = \\Pi^{-1/2} (\\Pi P) \\Pi^{-1/2} = \\Pi^{1/2} P \\Pi^{-1/2} = T$.\nBecause $T$ is a symmetric matrix, its eigenvalues are real. As $T$ and $P$ are similar, they share the same spectrum. This operator $T$ (or equivalently, $P$ in the appropriate inner product space) is the operator whose spectrum describes the dynamics. The eigenvalues of $P$ correspond to the relaxation timescales of the system via $\\tau_k = -\\tau / \\ln(|\\lambda_k|)$ for eigenvalue $\\lambda_k$.\n\nThe problem further links this to Time-lagged Independent Component Analysis (tICA). The tICA method solves the generalized eigenproblem $C(\\tau) \\mathbf{v} = \\lambda^{\\text{tICA}} C(0) \\mathbf{v}$, where $C(0)$ and $C(\\tau)$ are the zero-lag and time-lagged covariance matrices of a chosen feature vector. For a feature vector of indicator functions $\\boldsymbol{\\chi}(x)$ on the discrete states, which is made zero-mean, the covariance matrices are $C(0) = \\Pi - \\boldsymbol{\\pi}\\boldsymbol{\\pi}^\\top$ and $C(\\tau) = \\Pi P - \\boldsymbol{\\pi}\\boldsymbol{\\pi}^\\top$.\n\nLet $\\mathbf{w}_k$ be a right eigenvector of $P$ with eigenvalue $\\lambda_k$. For a stochastic matrix, there is always an eigenvalue $\\lambda_1 = 1$ with eigenvector $\\mathbf{w}_1 = \\mathbf{1}$ (a vector of ones). For any other eigenvalue $\\lambda_k \\neq 1$, the corresponding eigenvector is orthogonal to the stationary distribution, i.e., $\\boldsymbol{\\pi}^\\top \\mathbf{w}_k = 0$. For such an eigenvector, we can evaluate its action by the covariance matrices:\n$$\nC(0) \\mathbf{w}_k = (\\Pi - \\boldsymbol{\\pi}\\boldsymbol{\\pi}^\\top) \\mathbf{w}_k = \\Pi \\mathbf{w}_k - \\boldsymbol{\\pi}(\\boldsymbol{\\pi}^\\top \\mathbf{w}_k) = \\Pi \\mathbf{w}_k\n$$\n$$\nC(\\tau) \\mathbf{w}_k = (\\Pi P - \\boldsymbol{\\pi}\\boldsymbol{\\pi}^\\top) \\mathbf{w}_k = \\Pi P \\mathbf{w}_k - \\boldsymbol{\\pi}(\\boldsymbol{\\pi}^\\top \\mathbf{w}_k) = \\Pi (P \\mathbf{w}_k) = \\Pi (\\lambda_k \\mathbf{w}_k) = \\lambda_k (\\Pi \\mathbf{w}_k)\n$$\nSubstituting these into the tICA eigenproblem, we get:\n$$\n\\lambda_k (\\Pi \\mathbf{w}_k) = \\lambda^{\\text{tICA}}_k (\\Pi \\mathbf{w}_k)\n$$\nThis demonstrates that the tICA eigenvalues $\\lambda^{\\text{tICA}}_k$ are identical to the eigenvalues $\\lambda_k$ of the transition matrix $P$. Thus, the task reduces to computing the eigenvalues of the given matrix $P$.\n\nThe transition matrix is:\n$$\nP = \\begin{pmatrix}\n0.83 & 0.16 & 0.01 \\\\\n0.20 & 0.79 & 0.01 \\\\\n0.05 & 0.04 & 0.91\n\\end{pmatrix}\n$$\nThe eigenvalues $\\lambda$ are the roots of the characteristic equation $\\det(P - \\lambda I) = 0$. Since $P$ is a row-stochastic matrix, one eigenvalue is $\\lambda_1 = 1$. The other eigenvalues, which compose the nontrivial spectrum, can be found using the trace and determinant of the matrix.\nThe sum of the eigenvalues equals the trace of the matrix:\n$$\n\\text{Tr}(P) = \\lambda_1 + \\lambda_2 + \\lambda_3 = 0.83 + 0.79 + 0.91 = 2.53\n$$\nWith $\\lambda_1 = 1$, we have $1 + \\lambda_2 + \\lambda_3 = 2.53$, which simplifies to $\\lambda_2 + \\lambda_3 = 1.53$.\n\nThe product of the eigenvalues equals the determinant of the matrix:\n$$\n\\det(P) = \\lambda_1 \\lambda_2 \\lambda_3 = \\lambda_2 \\lambda_3\n$$\n$$\n\\det(P) = 0.83(0.79 \\cdot 0.91 - 0.01 \\cdot 0.04) - 0.16(0.20 \\cdot 0.91 - 0.01 \\cdot 0.05) + 0.01(0.20 \\cdot 0.04 - 0.79 \\cdot 0.05)\n$$\n$$\n\\det(P) = 0.83(0.7189 - 0.0004) - 0.16(0.182 - 0.0005) + 0.01(0.008 - 0.0395)\n$$\n$$\n\\det(P) = 0.83(0.7185) - 0.16(0.1815) + 0.01(-0.0315)\n$$\n$$\n\\det(P) = 0.596355 - 0.02904 - 0.000315 = 0.567\n$$\nSo we have a system of two equations for the nontrivial eigenvalues $\\lambda_2$ and $\\lambda_3$:\n$$\n\\lambda_2 + \\lambda_3 = 1.53\n$$\n$$\n\\lambda_2 \\lambda_3 = 0.567\n$$\nThese are the roots of the quadratic equation $x^2 - (\\lambda_2 + \\lambda_3)x + \\lambda_2 \\lambda_3 = 0$:\n$$\nx^2 - 1.53x + 0.567 = 0\n$$\nUsing the quadratic formula, $x = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$:\n$$\nx = \\frac{1.53 \\pm \\sqrt{(-1.53)^2 - 4(1)(0.567)}}{2} = \\frac{1.53 \\pm \\sqrt{2.3409 - 2.268}}{2}\n$$\n$$\nx = \\frac{1.53 \\pm \\sqrt{0.0729}}{2} = \\frac{1.53 \\pm 0.27}{2}\n$$\nThe two roots are:\n$$\n\\lambda_2 = \\frac{1.53 + 0.27}{2} = \\frac{1.80}{2} = 0.90\n$$\n$$\n\\lambda_3 = \\frac{1.53 - 0.27}{2} = \\frac{1.26}{2} = 0.63\n$$\nBy convention, the eigenvalues are ordered $\\lambda_1 > \\lambda_2 \\ge \\lambda_3 \\ge \\dots$. So, $\\lambda_2 = 0.90$ and $\\lambda_3 = 0.63$. The nontrivial spectrum is $\\{0.90, 0.63\\}$.\n\nThe metastability spectral gap $g$ is defined as the difference between the second and third largest eigenvalues:\n$$\ng = \\lambda_2 - \\lambda_3\n$$\n$$\ng = 0.90 - 0.63 = 0.27\n$$\nThis value is exact, as all intermediate calculations involved terminating decimals.", "answer": "$$\\boxed{0.27}$$", "id": "3407097"}, {"introduction": "Applying any data-driven method successfully requires careful model selection and validation. This practice focuses on a critical, and often mishandled, step in the tICA workflow: choosing hyperparameters like the lag time $\\tau$ and the number of components $d$ to retain. Because molecular dynamics data is a time series, standard cross-validation techniques fail; this exercise challenges you to identify a statistically robust protocol that correctly handles temporal correlations to provide an unbiased estimate of your model's predictive power. [@problem_id:3407111]", "problem": "You are given $M$ independent molecular dynamics trajectories $\\{\\mathbf{x}^{(m)}_t\\}_{t=0}^{T_m}$ generated by a time-homogeneous Markov process that mixes on a characteristic correlation time $t_{\\mathrm{corr}}$. You extract a feature map $\\boldsymbol{\\chi}:\\mathbb{R}^{3N}\\to\\mathbb{R}^p$ to obtain a time series $\\mathbf{z}^{(m)}_t=\\boldsymbol{\\chi}(\\mathbf{x}^{(m)}_t)$. You wish to select hyperparameters for time-lagged independent component analysis (tICA), specifically the lag time $\\tau$ and the number of components $d$, using $K$-fold cross-validation. The goal is to obtain an approximately unbiased out-of-sample estimate of the Variational Approach for Markov Processes (VAMP)-$2$ score for each candidate pair $(\\tau,d)$ while minimizing statistical dependence between training and test data. Assume the data length is sufficient to allow partitioning the time series into blocks that are longer than $t_{\\mathrm{corr}}$ and that $\\tau \\ll \\min_m T_m$.\n\nWhich of the following cross-validation protocols will achieve the stated goal most reliably?\n\nA. For each trajectory and for a fixed $\\tau$, partition the time indices into $K$ contiguous, non-overlapping blocks of equal length $L$, choosing $L$ such that $L \\gg t_{\\mathrm{corr}}$ and $L > \\tau$. In each fold, hold out one block from every trajectory as test and use only the remaining blocks as training. Estimate all preprocessing (feature centering, scaling), covariances, and the tICA model exclusively on the training blocks. When constructing lagged pairs for the test score, use only pairs $(t,t+\\tau)$ that lie entirely within the held-out blocks, discarding pairs that would cross any block boundary. Compute the VAMP-$2$ score on the held-out data using whitening or projections determined from the training set only. Average the per-fold test scores, weighting by the number of valid test pairs in each fold, to compare $(\\tau,d)$ across candidates.\n\nB. Randomly assign individual time indices across all trajectories to $K$ folds without regard to temporal order so that each fold has approximately the same number of frames. In each fold, train tICA on $K-1$ folds and compute the VAMP-$2$ score on the remaining fold using lagged pairs constructed from any available adjacent times $(t,t+\\tau)$ even if $t$ and $t+\\tau$ lie in different folds. For numerical stability, center and whiten the test fold using statistics estimated from the test fold itself.\n\nC. Partition each trajectory into $K$ contiguous blocks, but to stabilize estimates, pool all blocks (both training and held-out) to estimate the mean, covariance, whitening, and tICA components. In each fold, compute the VAMP-$2$ score on the held-out block using these pooled transformations, and then average the scores over folds.\n\nD. Partition each trajectory into contiguous blocks and hold out one block per fold. Fit tICA and compute the VAMP-$2$ score on the training blocks in each fold. Select $(\\tau,d)$ by maximizing this in-sample training score across folds, and finally report the VAMP-$2$ score by refitting tICA on the full dataset with the selected $(\\tau,d)$ and evaluating the score on the same full dataset.\n\nChoose the option that best describes a statistically sound $K$-fold cross-validation protocol for tICA model selection by splitting trajectories into statistically independent blocks and evaluating the VAMP-$2$ score on held-out data. There may be one correct option or more than one; select all that apply.", "solution": "The problem asks for the most reliable $K$-fold cross-validation protocol to select hyperparameters $(\\tau, d)$ for time-lagged independent component analysis (tICA). The selection criterion is the VAMP-$2$ score. The protocol must provide an approximately unbiased out-of-sample estimate of this score while minimizing statistical dependence between the training and test sets. The data consists of molecular dynamics trajectories, which are time series with a characteristic correlation time $t_{\\mathrm{corr}}$.\n\nThe fundamental principle of cross-validation is to estimate a model's performance on unseen data. This requires a strict separation between the data used for training the model (the training set) and the data used for evaluating its performance (the test set). For time series data, standard random splitting of data points is invalid because it ignores temporal correlations. A point $\\mathbf{z}_t$ in the test set may be highly correlated with its neighbors $\\mathbf{z}_{t-1}$ and $\\mathbf{z}_{t+1}$ in the training set. This \"information leakage\" leads to an overly optimistic, upwardly biased estimate of the model's out-of-sample performance.\n\nThe correct approach for time series is blocked cross-validation. The time series is partitioned into contiguous blocks of length $L$, where $L$ is chosen to be much larger than the correlation time of the process ($L \\gg t_{\\mathrm{corr}}$). By assigning entire blocks to training or test folds, one ensures that the training and test sets are approximately statistically independent.\n\nFurthermore, a true out-of-sample evaluation requires that every step of the model-building process, including any data preprocessing like mean subtraction or whitening, is learned *only* from the training data. These learned transformations are then applied to the test data, on which the performance metric (the VAMP-$2$ score) is computed.\n\nLet's analyze each option based on these principles.\n\n**A. For each trajectory and for a fixed $\\tau$, partition the time indices into $K$ contiguous, non-overlapping blocks of equal length $L$, choosing $L$ such that $L \\gg t_{\\mathrm{corr}}$ and $L > \\tau$. In each fold, hold out one block from every trajectory as test and use only the remaining blocks as training. Estimate all preprocessing (feature centering, scaling), covariances, and the tICA model exclusively on the training blocks. When constructing lagged pairs for the test score, use only pairs $(t,t+\\tau)$ that lie entirely within the held-out blocks, discarding pairs that would cross any block boundary. Compute the VAMP-$2$ score on the held-out data using whitening or projections determined from the training set only. Average the per-fold test scores, weighting by the number of valid test pairs in each fold, to compare $(\\tau,d)$ across candidates.**\n\nThis option describes a statistically rigorous blocked cross-validation protocol for time series data.\n1.  **Partitioning into contiguous blocks with $L \\gg t_{\\mathrm{corr}}$**: This is the correct way to partition time series data to ensure approximate independence between the training and test sets.\n2.  **Strict separation of training and testing**: The protocol correctly specifies that all model parameters—preprocessing (centering, scaling), covariance matrices, and the tICA components themselves—are estimated *exclusively* from the training data.\n3.  **Correct out-of-sample evaluation**: The VAMP-$2$ score is computed on the held-out test data using transformations (whitening, projections) learned from the training set. This is the definition of an out-of-sample evaluation.\n4.  **Handling of lagged pairs**: The instruction to use only lagged pairs $(t, t+\\tau)$ that lie entirely within blocks prevents information leakage at the boundaries between training and test segments.\n\nThis protocol adheres to all best practices for estimating the generalization performance of a time series model and directly addresses the problem's stated goals.\n**Verdict: Correct.**\n\n**B. Randomly assign individual time indices across all trajectories to $K$ folds without regard to temporal order so that each fold has approximately the same number of frames. In each fold, train tICA on $K-1$ folds and compute the VAMP-$2$ score on the remaining fold using lagged pairs constructed from any available adjacent times $(t,t+\\tau)$ even if $t$ and $t+\\tau$ lie in different folds. For numerical stability, center and whiten the test fold using statistics estimated from the test fold itself.**\n\nThis protocol has multiple severe flaws.\n1.  **Random assignment of time indices**: As discussed, this violates the independence assumption for time series data and leads to significant information leakage between training and test sets, resulting in a highly optimistic and biased performance estimate.\n2.  **Lagged pairs crossing fold boundaries**: This further exacerbates the information leakage problem.\n3.  **Preprocessing the test set with its own statistics**: This is a critical error in cross-validation methodology. The test set is meant to simulate genuinely unseen data. One cannot compute statistics over the entire unseen dataset before making predictions or evaluating performance. The test data must be transformed using the parameters derived *only* from the training data.\n\nThis protocol would fail to provide a reliable estimate of out-of-sample performance.\n**Verdict: Incorrect.**\n\n**C. Partition each trajectory into $K$ contiguous blocks, but to stabilize estimates, pool all blocks (both training and held-out) to estimate the mean, covariance, whitening, and tICA components. In each fold, compute the VAMP-$2$ score on the held-out block using these pooled transformations, and then average the scores over folds.**\n\nThis protocol violates the core principle of cross-validation.\n1.  **Pooling all data for estimation**: By using the held-out (test) blocks to estimate the model parameters (mean, covariances, tICA components), the model has \"seen\" the test data during training. This contaminates the training process with information from the test set.\n2.  **Biased evaluation**: The resulting VAMP-$2$ score is not an out-of-sample estimate but rather a partially in-sample estimate. It will be optimistically biased and will not reflect the model's true performance on unseen data. While the partitioning into blocks is correct, the subsequent pooling of data nullifies the benefit of the separation.\n\nThis protocol constitutes a form of data leakage and is not a valid cross-validation procedure.\n**Verdict: Incorrect.**\n\n**D. Partition each trajectory into contiguous blocks and hold out one block per fold. Fit tICA and compute the VAMP-$2$ score on the training blocks in each fold. Select $(\\tau,d)$ by maximizing this in-sample training score across folds, and finally report the VAMP-$2$ score by refitting tICA on the full dataset with the selected $(\\tau,d)$ and evaluating the score on the same full dataset.**\n\nThis protocol fundamentally misunderstands the purpose of cross-validation.\n1.  **Evaluating on training data**: The procedure evaluates the model on the same data it was trained on (the \"in-sample training score\"). The purpose of a test set is to evaluate performance on *different* data. Maximizing the training score is a direct path to overfitting; a more complex model (e.g., larger $d$) will almost always yield a better score on the data it was trained on.\n2.  **No out-of-sample evaluation**: The protocol never uses the held-out blocks to evaluate the model trained on the other blocks. It completely fails to estimate the generalization error.\n3.  **Reporting in-sample score**: The final step of reporting the score on the full dataset after training on the full dataset is also an in-sample measure, which is known to be biased.\n\nThis process does not perform cross-validation; it simply computes and maximizes the training score.\n**Verdict: Incorrect.**\n\nIn summary, only option A describes a sound and rigorous cross-validation procedure for selecting hyperparameters for tICA on time series data that achieves the stated goal of obtaining an unbiased out-of-sample performance estimate.", "answer": "$$\\boxed{A}$$", "id": "3407111"}]}