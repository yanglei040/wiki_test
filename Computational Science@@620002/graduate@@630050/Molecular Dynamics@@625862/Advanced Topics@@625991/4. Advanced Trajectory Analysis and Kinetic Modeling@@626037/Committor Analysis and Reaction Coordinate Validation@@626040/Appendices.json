{"hands_on_practices": [{"introduction": "A cornerstone of quantitative reaction coordinate (RC) validation is the ability to connect it to the committor function through a simple mathematical model. Because the committor is a probability, it is naturally bounded between 0 and 1, making the logistic (sigmoid) function an excellent candidate for this model. This practice will guide you through the analytical process of deriving the maximum likelihood estimate for the parameters of such a model, a fundamental technique for fitting RCs to the outcomes of trajectory shooting experiments [@problem_id:3402803].", "problem": "Consider a rare-event transition in a high-dimensional Molecular Dynamics (MD) simulation between two metastable basins $A$ and $B$. The committor function $q_{B}(\\mathbf{x})$ is defined as the probability that a trajectory initiated at configuration $\\mathbf{x}$ reaches basin $B$ before basin $A$. Assume the dynamics are Markovian and obey microscopic reversibility, and that there exists a one-dimensional reaction coordinate $r(\\mathbf{x})$ such that $r=0$ approximates the dividing surface and the mapping $r \\mapsto q_{B}$ is strictly increasing and satisfies $q_{B}(-r)=1-q_{B}(r)$ due to the symmetry of the basins along $r$.\n\nA standard and scientifically justified modeling choice is to posit a logistic link between the reaction coordinate and the committor:\n$$\nq_{B}(r)=\\frac{1}{1+\\exp\\!\\left(-\\beta r\\right)},\n$$\nwhere $\\beta$ is an unknown slope parameter to be inferred from short-trajectory “shooting” experiments. Under the independence of shooting outcomes, each trajectory launched from a fixed $r$ yields a Bernoulli outcome with success probability $q_{B}(r)$.\n\nYou perform the following experiment. At $r=+r_{0}$, you launch $N_{+}$ independent short trajectories and observe $m_{+}$ trajectories that reach basin $B$ before $A$. At $r=-r_{0}$, you launch $N_{-}$ independent short trajectories and observe $m_{-}$ trajectories that reach basin $B$ before $A$. The reaction coordinate $r$ is dimensionless, so the slope $\\beta$ is dimensionless. Given the symmetry $q_{B}(-r)=1-q_{B}(r)$ implied by microscopic reversibility and the logistic link above, construct the likelihood of the observed data and derive, from first principles, the maximum likelihood estimate of $\\beta$ in terms of $N_{+}$, $N_{-}$, $m_{+}$, $m_{-}$, and $r_{0}$.\n\nThen, using the numerical values $r_{0}=0.85$, $N_{+}=40$, $m_{+}=29$, $N_{-}=45$, and $m_{-}=11$, compute the numerical value of the maximum likelihood estimate $\\hat{\\beta}$. Round your final answer to four significant figures. Since $\\beta$ is dimensionless, report the value without units.", "solution": "The problem statement has been validated and is deemed valid. It is scientifically grounded in the principles of statistical mechanics and maximum likelihood estimation, is well-posed with a complete and consistent setup, and is expressed in objective, formal language. We may therefore proceed with a solution.\n\nThe goal is to find the maximum likelihood estimate (MLE) of the slope parameter $\\beta$ for the given logistic model of the committor function, based on data from two sets of shooting experiments.\n\nThe committor function, modeled as a logistic function of the reaction coordinate $r$, is given by:\n$$\nq_{B}(r) = \\frac{1}{1 + \\exp(-\\beta r)}\n$$\nThe outcomes of the short trajectories are independent Bernoulli trials. For a trajectory launched from a configuration with reaction coordinate value $r$, the probability of reaching basin $B$ is $p = q_{B}(r)$, and the probability of reaching basin $A$ is $1-p = 1-q_{B}(r)$.\n\nThe experiment consists of two parts:\n1.  At $r = +r_{0}$, we have $N_{+}$ trials with $m_{+}$ successes (reaching $B$). The probability of this outcome is given by the binomial probability mass function:\n    $$\n    P(m_{+}|N_{+}, \\beta, r_0) = \\binom{N_{+}}{m_{+}} [q_{B}(r_{0})]^{m_{+}} [1-q_{B}(r_{0})]^{N_{+}-m_{+}}\n    $$\n2.  At $r = -r_{0}$, we have $N_{-}$ trials with $m_{-}$ successes. The probability of this outcome is:\n    $$\n    P(m_{-}|N_{-}, \\beta, r_0) = \\binom{N_{-}}{m_{-}} [q_{B}(-r_{0})]^{m_{-}} [1-q_{B}(-r_{0})]^{N_{-}-m_{-}}\n    $$\n\nSince the two experiments are independent, the total likelihood of the observed data, $L(\\beta)$, is the product of these probabilities:\n$$\nL(\\beta) = \\binom{N_{+}}{m_{+}} [q_{B}(r_{0})]^{m_{+}} [1-q_{B}(r_{0})]^{N_{+}-m_{+}} \\binom{N_{-}}{m_{-}} [q_{B}(-r_{0})]^{m_{-}} [1-q_{B}(-r_{0})]^{N_{-}-m_{-}}\n$$\nTo find the MLE $\\hat\\beta$, we maximize $L(\\beta)$ with respect to $\\beta$. It is more convenient to maximize the log-likelihood, $\\ell(\\beta) = \\ln L(\\beta)$, since the logarithm is a monotonic function. The binomial coefficients do not depend on $\\beta$ and can be ignored during optimization.\n$$\n\\ell(\\beta) = m_{+} \\ln(q_{B}(r_{0})) + (N_{+} - m_{+}) \\ln(1 - q_{B}(r_{0})) + m_{-} \\ln(q_{B}(-r_{0})) + (N_{-} - m_{-}) \\ln(1 - q_{B}(-r_{0})) + C\n$$\nwhere $C$ is a constant. We now substitute the expressions for $q_B(r)$ and $1-q_B(r)$.\nFor $q_B(r_0)$:\n$$\nq_{B}(r_{0}) = \\frac{1}{1+\\exp(-\\beta r_{0})} \\implies \\ln(q_{B}(r_{0})) = -\\ln(1+\\exp(-\\beta r_{0}))\n$$\n$$\n1-q_{B}(r_{0}) = \\frac{\\exp(-\\beta r_{0})}{1+\\exp(-\\beta r_{0})} \\implies \\ln(1-q_{B}(r_{0})) = -\\beta r_{0} - \\ln(1+\\exp(-\\beta r_{0}))\n$$\nAnd for $q_B(-r_0)$:\n$$\nq_{B}(-r_{0}) = \\frac{1}{1+\\exp(\\beta r_{0})} \\implies \\ln(q_{B}(-r_{0})) = -\\ln(1+\\exp(\\beta r_{0}))\n$$\n$$\n1-q_{B}(-r_{0}) = \\frac{\\exp(\\beta r_{0})}{1+\\exp(\\beta r_{0})} \\implies \\ln(1-q_{B}(-r_{0})) = \\beta r_{0} - \\ln(1+\\exp(\\beta r_{0}))\n$$\nSubstituting these into the log-likelihood function:\n$$\n\\ell(\\beta) = m_{+}[-\\ln(1+\\exp(-\\beta r_{0}))] + (N_{+}-m_{+})[-\\beta r_{0} - \\ln(1+\\exp(-\\beta r_{0}))] + m_{-}[-\\ln(1+\\exp(\\beta r_{0}))] + (N_{-}-m_{-})[\\beta r_{0} - \\ln(1+\\exp(\\beta r_{0}))]\n$$\nCombining terms:\n$$\n\\ell(\\beta) = - N_{+} \\ln(1+\\exp(-\\beta r_{0})) - N_{-} \\ln(1+\\exp(\\beta r_{0})) - (N_{+}-m_{+})\\beta r_{0} + (N_{-}-m_{-})\\beta r_{0}\n$$\nTo find the maximum, we differentiate $\\ell(\\beta)$ with respect to $\\beta$ and set the derivative to zero:\n$$\n\\frac{d\\ell}{d\\beta} = -N_{+}\\left(\\frac{-r_{0}\\exp(-\\beta r_{0})}{1+\\exp(-\\beta r_{0})}\\right) - N_{-}\\left(\\frac{r_{0}\\exp(\\beta r_{0})}{1+\\exp(\\beta r_{0})}\\right) - (N_{+}-m_{+})r_{0} + (N_{-}-m_{-})r_{0} = 0\n$$\nSince $r_0 \\neq 0$, we can divide the entire equation by $r_{0}$:\n$$\nN_{+}\\frac{\\exp(-\\beta r_{0})}{1+\\exp(-\\beta r_{0})} - N_{-}\\frac{\\exp(\\beta r_{0})}{1+\\exp(\\beta r_{0})} - N_{+} + m_{+} + N_{-} - m_{-} = 0\n$$\nRecognizing that $\\frac{\\exp(-\\beta r_0)}{1+\\exp(-\\beta r_0)} = 1 - q_B(r_0)$ and $\\frac{\\exp(\\beta r_0)}{1+\\exp(\\beta r_0)} = 1-q_B(-r_0)$, we can write:\n$$\nN_{+}(1-q_{B}(r_{0})) - N_{-}(1-q_{B}(-r_{0})) - N_{+} + m_{+} + N_{-} - m_{-} = 0\n$$\n$$\nN_{+} - N_{+}q_{B}(r_{0}) - N_{-} + N_{-}q_{B}(-r_{0}) - N_{+} + m_{+} + N_{-} - m_{-} = 0\n$$\nSimplifying this expression yields the condition for the MLE, $\\hat{\\beta}$:\n$$\n-N_{+}q_{B}(r_{0}) + N_{-}q_{B}(-r_{0}) + m_{+} - m_{-} = 0\n$$\n$$\nm_{+} - m_{-} = N_{+}q_{B}(r_{0}) - N_{-}q_{B}(-r_{0})\n$$\nNow, we substitute the logistic forms back in to solve for $\\beta$:\n$$\nm_{+} - m_{-} = \\frac{N_{+}}{1+\\exp(-\\beta r_{0})} - \\frac{N_{-}}{1+\\exp(\\beta r_{0})}\n$$\nTo solve this equation for $\\beta$, let $x = \\exp(\\beta r_{0})$. Then $\\exp(-\\beta r_{0}) = 1/x$.\n$$\nm_{+} - m_{-} = \\frac{N_{+}}{1+1/x} - \\frac{N_{-}}{1+x} = \\frac{N_{+}x}{x+1} - \\frac{N_{-}}{x+1} = \\frac{N_{+}x - N_{-}}{x+1}\n$$\nNow, we solve for $x$ algebraically:\n$$\n(m_{+} - m_{-})(x+1) = N_{+}x - N_{-}\n$$\n$$\n(m_{+} - m_{-})x + (m_{+} - m_{-}) = N_{+}x - N_{-}\n$$\n$$\n(m_{+} - m_{-}) + N_{-} = N_{+}x - (m_{+} - m_{-})x\n$$\n$$\nN_{-} + m_{+} - m_{-} = (N_{+} - m_{+} + m_{-})x\n$$\n$$\nx = \\frac{N_{-} + m_{+} - m_{-}}{N_{+} - m_{+} + m_{-}}\n$$\nSubstituting back $x = \\exp(\\beta r_0)$, we get:\n$$\n\\exp(\\hat{\\beta} r_0) = \\frac{N_{-} + m_{+} - m_{-}}{N_{+} - m_{+} + m_{-}}\n$$\nFinally, solving for the maximum likelihood estimate $\\hat{\\beta}$ by taking the natural logarithm:\n$$\n\\hat{\\beta} r_0 = \\ln\\left(\\frac{N_{-} + m_{+} - m_{-}}{N_{+} - m_{+} + m_{-}}\\right)\n$$\n$$\n\\hat{\\beta} = \\frac{1}{r_0} \\ln\\left(\\frac{N_{-} + m_{+} - m_{-}}{N_{+} - m_{+} + m_{-}}\\right)\n$$\nThis is the analytical expression for the maximum likelihood estimate of $\\beta$.\n\nWe now compute the numerical value using the provided data: $r_{0}=0.85$, $N_{+}=40$, $m_{+}=29$, $N_{-}=45$, and $m_{-}=11$.\n\nFirst, we calculate the numerator and denominator of the argument of the logarithm:\nNumerator: $N_{-} + m_{+} - m_{-} = 45 + 29 - 11 = 63$.\nDenominator: $N_{+} - m_{+} + m_{-} = 40 - 29 + 11 = 11 + 11 = 22$.\n\nSubstituting these into the expression for $\\hat{\\beta}$:\n$$\n\\hat{\\beta} = \\frac{1}{0.85} \\ln\\left(\\frac{63}{22}\\right)\n$$\nWe compute the values:\n$$\n\\frac{63}{22} \\approx 2.863636...\n$$\n$$\n\\ln\\left(\\frac{63}{22}\\right) \\approx \\ln(2.863636...) \\approx 1.052184...\n$$\n$$\n\\hat{\\beta} \\approx \\frac{1.052184...}{0.85} \\approx 1.237863...\n$$\nRounding the result to four significant figures gives $1.238$.", "answer": "$$\n\\boxed{1.238}\n$$", "id": "3402803"}, {"introduction": "The ideal reaction coordinate, $\\xi$, should capture all of the system's slow dynamics, which implies that the committor probability, $q(\\mathbf{x})$, depends only on the value of $\\xi(\\mathbf{x})$. A direct consequence is that any surface where $\\xi(\\mathbf{x})$ is constant should also be an \"isocommittor\" surface, where $q(\\mathbf{x})$ is constant. This exercise puts this principle to a direct computational test by simulating molecular dynamics, estimating committor values, and statistically evaluating whether the committor distribution on a proposed RC's isosurface is indeed unimodal and narrow [@problem_id:3402833].", "problem": "You are tasked with building a principled cross-check for validating a proposed one-dimensional reaction coordinate in Molecular Dynamics (MD), using committor analysis based on the fundamental stochastic dynamics of a molecular system. Consider a two-dimensional configuration space with coordinates $x$ and $y$. The dynamics are modeled in dimensionless units by the overdamped Langevin equation (also known as the Smoluchowski dynamics), which for a potential energy $U(x,y)$ reads\n$$\ndx_t = -\\mu \\,\\partial_x U(x_t,y_t)\\, dt + \\sqrt{2 D \\, dt}\\, \\eta_x(t), \\quad dy_t = -\\mu \\,\\partial_y U(x_t,y_t)\\, dt + \\sqrt{2 D \\, dt}\\, \\eta_y(t),\n$$\nwhere $\\mu$ is the mobility, $D$ is the diffusion coefficient, and $\\eta_x(t)$, $\\eta_y(t)$ are independent standard normal random variables at each time step. The committor function $q(x_0,y_0)$ is defined as the probability that a trajectory starting at $(x_0,y_0)$ reaches basin $B$ before basin $A$. Basins are defined here by threshold conditions on $x$, specifically basin $A$ is the set of configurations with $x \\le x_A$, and basin $B$ is the set with $x \\ge x_B$, with $x_A < 0 < x_B$. A proposed reaction coordinate is a scalar function $\\xi(x,y)$; “isosurfaces” are the sets of points satisfying $\\xi(x,y)=c$ for a constant $c$.\n\nYour program must implement the following validation procedure:\n1. Use the overdamped Langevin dynamics as the fundamental evolution law (no inertial term). Integrate the stochastic differential equation using the Euler–Maruyama method with the specified parameters in dimensionless units.\n2. For each proposed reaction coordinate $\\xi(x,y)$ and each specified isosurface level $c$, construct a set of initial conditions on the isosurface $\\xi(x,y)=c$ by varying $y$ uniformly in a symmetric interval and determining the corresponding $x$ that satisfies the isosurface condition.\n3. For each initial condition $(x_0,y_0)$ on the isosurface, approximate the committor $q(x_0,y_0)$ by “shooting”: launch multiple independent overdamped Langevin trajectories from $(x_0,y_0)$, evolve each until it hits either basin $A$ or basin $B$ or a maximum number of steps, and record whether basin $B$ was reached before basin $A$. The estimated committor $q(x_0,y_0)$ is the fraction of trajectories that reach basin $B$ first.\n4. At fixed isosurface level $c$, aggregate the set $\\{q(x_0,y_0)\\}$ over the different initial conditions and test whether the empirical distribution of $q$ is unimodal and narrow. Conclude that a reaction coordinate is insufficient if, for any tested $c$, the distribution of $q$ at fixed $c$ fails unimodality or is not narrow.\n5. Unimodality must be assessed by estimating a smooth density of the empirical $q$ values and counting the number of distinct modes. Narrowness must be assessed by comparing the empirical standard deviation of the $q$ values against a threshold.\n\nThe potential energy is specified as\n$$\nU(x,y) = (x^2 - 1)^2 + k \\, y^2,\n$$\nwith gradient\n$$\n\\partial_x U(x,y) = 4x(x^2 - 1), \\quad \\partial_y U(x,y) = 2k y.\n$$\nThis yields a symmetric double well in $x$ with minima near $x=\\pm 1$ and a weakly confining quadratic term in $y$. Work entirely in dimensionless units. The basins are defined by $x_A=-0.9$ and $x_B=0.9$. The mobility is $\\mu=1$, the diffusion coefficient is $D=0.4$, and the potential parameter is $k=0.1$. Use a time step $dt=0.005$ and a maximum number of steps $N_{\\text{max}}=1500$ per trajectory. For each isosurface and each initial condition, use $N_{\\text{shoot}}=30$ shooting trajectories. For constructing isosurface initial conditions, sample $N_{\\text{iso}}=21$ values of $y$ uniformly in the interval $[-Y_{\\max},Y_{\\max}]$ with $Y_{\\max}=1$. Treat hitting of a basin as absorbing: once a trajectory satisfies $x\\le x_A$ or $x\\ge x_B$, it is terminated and considered to have reached $A$ or $B$ respectively. If a trajectory does not reach either basin within $N_{\\text{max}}$ steps, count it as neither; for committor estimation, only the recorded first hits are counted toward the fraction. If all trajectories for a given initial condition fail to reach either basin, the committor estimate for that initial condition should be set to $0.5$ by symmetry.\n\nYou must validate two candidate reaction coordinates:\n1. $\\xi_1(x,y) = x$.\n2. $\\xi_2(x,y) = x + \\alpha y$ with $\\alpha = 1.5$.\n\nThe isosurface levels to be tested are $c \\in \\{-0.5, 0.0, 0.5\\}$. For each candidate $\\xi$ and each $c$, compute the set of committor estimates $\\{q(x_0,y_0)\\}$ over the constructed isosurface initial conditions, then:\n- Estimate a smooth density over $q \\in [0,1]$ and determine whether there is at most one mode (unimodality).\n- Compute the empirical standard deviation $\\sigma_q$ of the $\\{q\\}$ values and determine whether it is at most the narrowness threshold $\\sigma_{\\text{th}}=0.12$.\n\nA reaction coordinate $\\xi$ is considered “sufficient” if, for all tested $c$, the distribution of $q$ at fixed $c$ is unimodal and narrow. Otherwise, conclude “insufficient.”\n\nYour program must produce a single line of output containing the sufficiency verdicts for $\\xi_1$ and $\\xi_2$ respectively, as a comma-separated Python list of booleans enclosed in square brackets, for example `[True, False]`. No other output is permitted.\n\nTest Suite Parameters to Implement Exactly:\n- Potential parameter: $k=0.1$.\n- Basins: $x_A=-0.9$, $x_B=0.9$.\n- Dynamics: $\\mu=1$, $D=0.4$, $dt=0.005$, $N_{\\text{max}}=1500$.\n- Shooting: $N_{\\text{shoot}}=30$ trajectories per initial condition.\n- Isosurface sampling: $N_{\\text{iso}}=21$ points with $y$ uniformly spaced in $[-1,1]$, $Y_{\\max}=1$.\n- Reaction coordinates: $\\xi_1(x,y)=x$, $\\xi_2(x,y)=x+\\alpha y$ with $\\alpha=1.5$.\n- Isosurface levels: $c=-0.5, 0.0, 0.5$.\n- Unimodality detection: use a smooth density estimate over $q \\in [0,1]$ and count modes.\n- Narrowness threshold: $\\sigma_{\\text{th}}=0.12$.\n- Units: report and compute in dimensionless units.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with booleans in Python syntax, in the order $[\\text{sufficient}(\\xi_1),\\text{sufficient}(\\xi_2)]$.", "solution": "The user has provided a well-defined computational problem in the domain of molecular dynamics and reaction coordinate validation. I will first validate the problem statement according to the specified criteria.\n\n### Step 1: Extract Givens\n- **Potential Energy Function**: $U(x,y) = (x^2 - 1)^2 + k \\, y^2$, with $k=0.1$.\n- **Gradient of Potential**: $\\partial_x U(x,y) = 4x(x^2 - 1)$, $\\partial_y U(x,y) = 2k y$.\n- **Dynamics**: Overdamped Langevin equation (Smoluchowski dynamics), integrated with the Euler-Maruyama method.\n  - Mobility $\\mu=1$.\n  - Diffusion coefficient $D=0.4$.\n  - Time step $dt=0.005$.\n- **Basins**: Basin A is defined by $x \\le x_A = -0.9$. Basin B is defined by $x \\ge x_B = 0.9$.\n- **Trajectory Termination**: Trajectories are terminated upon hitting a basin or after a maximum of $N_{\\text{max}}=1500$ steps.\n- **Committor Estimation**: Based on $N_{\\text{shoot}}=30$ trajectories per initial point. The committor $q$ is the fraction of trajectories hitting B before A. If no trajectory reaches either basin, $q$ is set to $0.5$.\n- **Isosurface Sampling**: For each isosurface, $N_{\\text{iso}}=21$ initial points are generated by sampling $y$ uniformly in $[-1,1]$ ($Y_{\\max}=1$) and solving for $x$.\n- **Reaction Coordinates (RCs) to Test**:\n  1. $\\xi_1(x,y) = x$.\n  2. $\\xi_2(x,y) = x + \\alpha y$, with $\\alpha = 1.5$.\n- **Isosurface Levels to Test**: $c \\in \\{-0.5, 0.0, 0.5\\}$.\n- **Validation Criteria**: An RC is deemed \"sufficient\" if for all tested isosurface levels $c$, the distribution of its committor values $\\{q\\}$ is both:\n  1. **Unimodal**: The estimated smooth probability density of the $\\{q\\}$ values has at most one mode.\n  2. **Narrow**: The empirical standard deviation $\\sigma_q$ of the $\\{q\\}$ values is less than or equal to the threshold $\\sigma_{\\text{th}}=0.12$.\n- **Output Format**: A Python list of booleans indicating sufficiency for $\\xi_1$ and $\\xi_2$: `[sufficient(xi_1), sufficient(xi_2)]`.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientifically Grounded**: The problem is rooted in fundamental principles of statistical mechanics and computational chemistry. The use of the Langevin equation, committor analysis, and isosurface testing to validate a reaction coordinate are standard, well-established methods in the field of molecular simulation and rare event dynamics. The potential function is a classic double-well model used for such studies.\n2.  **Well-Posed**: The problem is clearly defined with all necessary parameters, equations, and numerical procedures specified. The criteria for judgment (\"sufficient\" vs. \"insufficient\") are made objective and quantitative. This structure ensures that a unique solution can be algorithmically determined.\n3.  **Objective**: The problem is stated in precise, unbiased terms. The validation criteria (unimodality, narrowness) are based on quantitative measures (mode counting of a KDE, standard deviation threshold), removing subjectivity.\n4.  **Completeness and Consistency**: The problem statement is self-contained. There are no contradictions. While \"smooth density estimate\" is not specified down to the kernel and bandwidth, using a standard method like Gaussian Kernel Density Estimation (KDE) with a default bandwidth heuristic (like Scott's or Silverman's rule, which `scipy.stats.gaussian_kde` uses) is a reasonable and standard interpretation that does not compromise the problem's validity.\n5.  **Feasibility**: The computational load is significant but manageable. The total number of simulation steps is approximately $2 \\text{ (RCs)} \\times 3 \\text{ (levels)} \\times 21 \\text{ (points)} \\times 30 \\text{ (shots)} \\times 1500 \\text{ (steps)} \\approx 5.67 \\times 10^6$. This is well within the capabilities of a modern computer for a single run.\n6.  **No other flaws detected**: The problem is not trivial, metaphorical, or outside the realm of scientific verification.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. I will proceed to construct the solution.\n\nThe solution will be implemented as a single Python script. The core logic involves a series of nested functions to manage the simulation and analysis workflow.\n\n1.  **`solve()`**: The main function that orchestrates the entire process. It will define all problem parameters and helper functions. It will loop through the two reaction coordinates, evaluate their sufficiency, and format the final output.\n2.  **`grad_U(x, y)`**: A helper function to compute the gradient of the potential energy, which determines the deterministic force in the Langevin equation.\n3.  **`run_trajectory(x0, y0)`**: This function simulates a single trajectory starting from `(x0, y0)` using the Euler-Maruyama method. It returns which basin ('A' or 'B') was hit first, or 'None' if the trajectory times out.\n4.  **`calculate_committor(x0, y0)`**: This function performs the \"shooting\" by calling `run_trajectory` $N_{\\text{shoot}}$ times from a given starting point. It then computes the committor estimate as the fraction of trajectories that reached basin B first, applying the special rule for the case where no trajectories reach either basin.\n5.  **`count_modes(data)`**: This function will take a list of committor values, compute a smooth probability density using `scipy.stats.gaussian_kde`, and then count the number of modes in the resulting density curve. This is essential for the unimodality test. My implementation will count local maxima in the discretized density function. To robustly handle various distribution shapes (including monotonic ones), it will check for peaks in the interior of the domain as well as at the boundaries.\n6.  **`test_rc(rc_x_generator, levels)`**: This function consolidates the validation for a single reaction coordinate. It iterates through the specified isosurface `levels`. For each level, it generates the initial conditions on the isosurface, calculates the committor values for these points, and then checks for unimodality and narrowness. If any level fails either test, the function immediately returns `False`, indicating the RC is insufficient. If all levels pass, it returns `True`.\n\nThis structured approach directly maps the problem's requirements into a computational algorithm, ensuring all conditions and parameters are respected.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import gaussian_kde\n\ndef solve():\n    \"\"\"\n    Main function to validate reaction coordinates using committor analysis.\n    \"\"\"\n    # Define problem parameters\n    K = 0.1\n    X_A = -0.9\n    X_B = 0.9\n    MU = 1.0\n    D = 0.4\n    DT = 0.005\n    N_MAX = 1500\n    N_SHOOT = 30\n    N_ISO = 21\n    Y_MAX = 1.0\n    ALPHA = 1.5\n    SIGMA_TH = 0.12\n    ISO_LEVELS = [-0.5, 0.0, 0.5]\n    \n    # Pre-calculate noise standard deviation for efficiency\n    NOISE_STD = np.sqrt(2 * D * DT)\n\n    def grad_U(x, y):\n        \"\"\"Computes the gradient of the potential energy U(x,y).\"\"\"\n        grad_x = 4.0 * x * (x**2 - 1.0)\n        grad_y = 2.0 * K * y\n        return grad_x, grad_y\n\n    def run_trajectory(x0, y0):\n        \"\"\"\n        Simulates a single overdamped Langevin trajectory.\n        Returns 'A' or 'B' if a basin is reached, otherwise 'None'.\n        \"\"\"\n        x, y = x0, y0\n        # Generate all random numbers at once for vectorization\n        noise_x = np.random.randn(N_MAX) * NOISE_STD\n        noise_y = np.random.randn(N_MAX) * NOISE_STD\n        \n        for i in range(N_MAX):\n            grad_x, grad_y = grad_U(x, y)\n            x += -MU * grad_x * DT + noise_x[i]\n            y += -MU * grad_y * DT + noise_y[i]\n            \n            if x <= X_A:\n                return 'A'\n            if x >= X_B:\n                return 'B'\n        return 'None'\n\n    def calculate_committor(x0, y0):\n        \"\"\"\n        Estimates the committor value q(x0, y0) by shooting multiple trajectories.\n        \"\"\"\n        a_hits = 0\n        b_hits = 0\n        for _ in range(N_SHOOT):\n            result = run_trajectory(x0, y0)\n            if result == 'A':\n                a_hits += 1\n            elif result == 'B':\n                b_hits += 1\n        \n        total_hits = a_hits + b_hits\n        if total_hits == 0:\n            return 0.5\n        return b_hits / total_hits\n\n    def count_modes(data, grid_size=100):\n        \"\"\"\n        Estimates the number of modes in a dataset using KDE.\n        At most one mode is considered 'unimodal'.\n        \"\"\"\n        if len(data) < 2 or np.allclose(data, data[0]):\n            return 1\n\n        try:\n            # Use default bandwidth selection (Scott's rule)\n            kde = gaussian_kde(data)\n            grid = np.linspace(0, 1, grid_size)\n            density = kde.evaluate(grid)\n        except (np.linalg.LinAlgError, ValueError):\n            return 1  # Fallback for singular matrix cases\n\n        # Find local maxima to count modes\n        modes = 0\n        # Check for internal modes (peaks)\n        for i in range(1, len(density) - 1):\n            if density[i] > density[i-1] and density[i] > density[i+1]:\n                modes += 1\n        \n        # Check for modes at the boundaries\n        if density[0] > density[1]:\n            modes += 1\n        if density[-1] > density[-2]:\n            modes += 1\n            \n        # A montonic distribution will have 1 mode at a boundary.\n        # A flat distribution will have 0 modes. We consider 0 modes as passing the\n        # \"at most one mode\" test.\n        if modes == 0 and not np.allclose(density, density[0]):\n             # This means monotonic or a single very broad peak, treat as unimodal\n             return 1\n        \n        return modes\n\n    def test_rc (rc_x_generator, levels_to_test):\n        \"\"\"\n        Tests a reaction coordinate for sufficiency across specified isosurface levels.\n        \"\"\"\n        for c in levels_to_test:\n            y_values = np.linspace(-Y_MAX, Y_MAX, N_ISO)\n            initial_conditions = [(rc_x_generator(y, c), y) for y in y_values]\n            \n            q_values = [calculate_committor(x0, y0) for x0, y0 in initial_conditions]\n            \n            q_std = np.std(q_values)\n            is_narrow = (q_std <= SIGMA_TH)\n            \n            num_modes = count_modes(q_values)\n            is_unimodal = (num_modes <= 1)\n            \n            if not (is_unimodal and is_narrow):\n                return False  # Fails early if any level is insufficient\n        return True\n\n    # Define generators for x-coordinates on isosurfaces\n    rc1_gen = lambda y, c: c\n    rc2_gen = lambda y, c: c - ALPHA * y\n    \n    # Perform validation for both reaction coordinates\n    results = [\n        test_rc(rc1_gen, ISO_LEVELS),\n        test_rc(rc2_gen, ISO_LEVELS)\n    ]\n    \n    # Print the final result in the specified format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3402833"}, {"introduction": "When computational tests reveal that a proposed reaction coordinate's isosurfaces are not perfect isocommittor surfaces, a critical question arises: is this due to a genuine deficiency in the RC, or is it an artifact of the numerical integration? This final practice addresses this essential \"reality check\" by equipping you with the statistical tools to diagnose and disentangle these effects. You will use Richardson extrapolation to estimate the zero-timestep committor and employ rigorous statistical tests to distinguish systematic integrator bias from the excess variability that signals a truly invalid RC [@problem_id:3402822].", "problem": "Consider a system evolving under the overdamped Langevin stochastic differential equation, where a configuration $\\mathbf{x}$ undergoes dynamics according to $d\\mathbf{x}_t = -\\beta D \\nabla U(\\mathbf{x}_t)\\, dt + \\sqrt{2D}\\, d\\mathbf{W}_t$, with temperature parameter $\\beta$, diffusion coefficient $D$, potential energy $U(\\mathbf{x})$, and Wiener process $\\mathbf{W}_t$. The committor function $q(\\mathbf{x})$ is defined as the probability that a trajectory initiated at $\\mathbf{x}$ will reach a target set $B$ before a reactant set $A$, and satisfies the backward Kolmogorov equation with absorbing boundary conditions on $A$ and $B$. In molecular simulation practice, $q(\\mathbf{x})$ is estimated empirically as the fraction of shooting trajectories that hit $B$ before $A$ when integrated with a finite time step $\\Delta t$ and a chosen integrator. Denote such a finite-time-step estimate by $q_{\\Delta t}(\\mathbf{x})$. For an integrator with weak order $p$ (where $p$ is a positive integer), the weak discretization bias in the committor estimate at fixed $\\mathbf{x}$ often admits an expansion of the form $q_{\\Delta t}(\\mathbf{x}) = q_{0}(\\mathbf{x}) + c(\\mathbf{x}) \\, (\\Delta t)^{p} + \\mathcal{O}((\\Delta t)^{p+1})$, where $q_{0}(\\mathbf{x})$ is the $\\Delta t \\to 0$ limit and $c(\\mathbf{x})$ is an integrator- and state-dependent coefficient. In addition, each empirical estimate of $q_{\\Delta t}(\\mathbf{x})$ computed from $n$ independent shooting trajectories is a binomial proportion with random sampling error of order $\\sqrt{q_{\\Delta t}(\\mathbf{x})(1-q_{\\Delta t}(\\mathbf{x}))/n}$.\n\nYour task is to implement a principled procedure that, given empirical counts from finite-time-step committor experiments at two step sizes $\\Delta t$ and $\\Delta t/2$ for a single reaction-coordinate value $s$, will:\n- Use Richardson extrapolation based on the known weak order $p$ of the integrator to estimate $q_{0}(\\mathbf{x})$.\n- Perform a statistical test to decide whether the observed difference between $q_{\\Delta t}(\\mathbf{x})$ and $q_{\\Delta t/2}(\\mathbf{x})$ is statistically significant relative to binomial sampling noise, thereby indicating detectable integrator bias at the tested scales.\n- Perform an \"isocommittor consistency\" overdispersion test across multiple microstates $\\{\\mathbf{x}_i\\}$ that share the same reaction coordinate value $s$ but differ in orthogonal degrees of freedom. Using only the smaller step size $\\Delta t/2$ to minimize bias, determine whether the variability of the empirical committor estimates across the $\\{\\mathbf{x}_i\\}$ exceeds what is expected from binomial sampling alone (which would indicate reaction coordinate invalidity at $s$).\n\nFrom first principles, base your approach on the following core definitions and well-tested facts:\n- The committor $q(\\mathbf{x})$ is the splitting probability for a Markov process with absorbing sets $A$ and $B$.\n- For a weak order $p$ integrator, weak errors in expectations of sufficiently smooth observables scale as $\\mathcal{O}((\\Delta t)^p)$.\n- An empirical committor fraction $\\hat{q} = k/n$ from $n$ independent trials with $k$ \"successes\" (hits to $B$ before $A$) is a binomial proportion.\n- For two independent binomial proportions $\\hat{q}_1$ and $\\hat{q}_2$ with counts $(k_1,n_1)$ and $(k_2,n_2)$, a pooled-proportion $z$-test can assess whether $\\hat{q}_1 - \\hat{q}_2$ deviates from zero beyond sampling error at a specified significance level.\n- A Pearson chi-square statistic comparing observed counts $\\{k_i\\}$ to a common-probability null model across microstates with trials $\\{n_i\\}$ can be used to test for overdispersion (excess variability) beyond binomial sampling.\n\nImplement the following, in pure mathematical and logical terms:\n- Given weak order $p$, and empirical proportions $\\hat{q}_{\\Delta t}$ and $\\hat{q}_{\\Delta t/2}$ at step sizes $\\Delta t$ and $\\Delta t/2$, respectively, construct a bias-reduced estimator for $q_0$ using Richardson extrapolation that cancels the leading $\\mathcal{O}((\\Delta t)^p)$ term. Do not assume any form for higher-order coefficients beyond their order of magnitude.\n- Use a two-proportion pooled $z$-test at a two-sided significance level $\\alpha = 0.05$ to determine if the difference between $\\hat{q}_{\\Delta t}$ and $\\hat{q}_{\\Delta t/2}$ indicates detectable integrator bias at the tested scales. Answer the decision as a Boolean, where $True$ means \"detectable integrator bias\" and $False$ means \"no detectable integrator bias.\"\n- For the isocommittor consistency test, using only the data at $\\Delta t/2$ across multiple microstates at the same $s$, test the null hypothesis that all microstates share a common committor $q$ and any variability arises solely from binomial noise. Use the Pearson chi-square statistic with degrees of freedom equal to the number of microstates minus one, and a two-sided significance level $\\alpha = 0.05$. Return a Boolean, where $True$ means \"reaction coordinate invalidity detected\" and $False$ means \"no evidence of reaction coordinate invalidity at the tested precision.\"\n- Additionally, return a Boolean that reports whether the Richardson-extrapolated estimate of $q_0$ falls outside the physically admissible interval $[0,1]$, which would indicate model mismatch or unresolved higher-order effects at the tested scales.\n\nTest Suite:\nYour program must apply the above procedure to the following three cases. Each case provides the weak order $p$, aggregate counts at $\\Delta t$ and $\\Delta t/2$, and a set of microstate-level counts at $\\Delta t/2$ for an isocommittor consistency test. All counts are independent binomial outcomes.\n\n- Case $1$ (mid-barrier, first-order integrator):\n  - Weak order: $p = 1$.\n  - Aggregate at $\\Delta t$: $n_1 = 20000$, $k_1 = 10400$.\n  - Aggregate at $\\Delta t/2$: $n_2 = 20000$, $k_2 = 10200$.\n  - Microstates at $\\Delta t/2$ (same $s$, four distinct microstates): $(n_i, k_i) \\in \\{(4000, 1400), (4000, 2000), (4000, 2600), (4000, 2000)\\}$.\n\n- Case $2$ (near-basin, second-order integrator):\n  - Weak order: $p = 2$.\n  - Aggregate at $\\Delta t$: $n_1 = 10000$, $k_1 = 520$.\n  - Aggregate at $\\Delta t/2$: $n_2 = 10000$, $k_2 = 505$.\n  - Microstates at $\\Delta t/2$ (same $s$, four distinct microstates): $(n_i, k_i) \\in \\{(4000, 192), (4000, 204), (4000, 208), (4000, 196)\\}$.\n\n- Case $3$ (near-product, first-order integrator, boundary effect):\n  - Weak order: $p = 1$.\n  - Aggregate at $\\Delta t$: $n_1 = 8000$, $k_1 = 7760$.\n  - Aggregate at $\\Delta t/2$: $n_2 = 8000$, $k_2 = 7920$.\n  - Microstates at $\\Delta t/2$ (same $s$, four distinct microstates): $(n_i, k_i) \\in \\{(4000, 3960), (4000, 3975), (4000, 3965), (4000, 3972)\\}$.\n\nFinal Output Specification:\n- For each case, produce a list with four entries: $[q^{\\mathrm{Rich}}_{0}, \\mathrm{bias\\_detected}, \\mathrm{rc\\_invalid}, \\mathrm{out\\_of\\_bounds}]$, where:\n  - $q^{\\mathrm{Rich}}_{0}$ is the Richardson-extrapolated estimate of the zero-time-step committor, rounded to six decimal places.\n  - $\\mathrm{bias\\_detected}$ is a Boolean from the two-proportion $z$-test at significance $\\alpha = 0.05$.\n  - $\\mathrm{rc\\_invalid}$ is a Boolean from the isocommittor chi-square test at significance $\\alpha = 0.05$.\n  - $\\mathrm{out\\_of\\_bounds}$ is a Boolean that is $True$ if $q^{\\mathrm{Rich}}_{0} \\notin [0,1]$ and $False$ otherwise.\n- Your program should produce a single line of output containing the results for the three cases as a comma-separated list enclosed in square brackets, with each case formatted as its own bracketed list and $q^{\\mathrm{Rich}}_{0}$ printed with exactly six digits after the decimal point. For example: `[[0.123456,True,False,False],[...],[...]]`.", "solution": "The problem is valid as it is scientifically grounded in computational statistical mechanics, well-posed, and presented with objective, precise language. The provided data and requested tasks are consistent and feasible. We will proceed with a step-by-step derivation of the required procedures.\n\nThe overall task is to analyze empirical committor data from numerical simulations. For each test case, we must perform four subtasks:\n1.  Estimate the true zero-time-step committor $q_0$ using Richardson extrapolation.\n2.  Test for statistically significant integrator bias between simulations at time step $\\Delta t$ and $\\Delta t/2$.\n3.  Test for isocommittor consistency to assess the validity of the reaction coordinate.\n4.  Check if the extrapolated committor estimate is physically plausible.\n\nLet the empirical committor estimate from $k$ successes in $n$ trials be denoted by $\\hat{q} = k/n$. We are given counts $(k_1, n_1)$ for time step $\\Delta t$ and $(k_2, n_2)$ for time step $\\Delta t/2$.\n\n### 1. Richardson Extrapolation for $q_0$\n\nThe problem states that for an integrator of weak order $p$, the committor estimate $q_{\\Delta t}$ has a weak error expansion:\n$$q_{\\Delta t}(\\mathbf{x}) = q_{0}(\\mathbf{x}) + c(\\mathbf{x}) (\\Delta t)^{p} + \\mathcal{O}((\\Delta t)^{p+1})$$\nwhere $q_0(\\mathbf{x})$ is the exact committor in the $\\Delta t \\to 0$ limit. We can write two equations using the empirical estimates at two different time steps, $\\Delta t$ and $\\Delta t/2$, treating them as approximations of the expectations:\n$$ \\hat{q}_1 \\approx q_0 + c (\\Delta t)^p $$\n$$ \\hat{q}_2 \\approx q_0 + c (\\Delta t/2)^p = q_0 + \\frac{c (\\Delta t)^p}{2^p} $$\nwhere $\\hat{q}_1 = k_1/n_1$ and $\\hat{q}_2 = k_2/n_2$. This forms a system of two linear equations in the unknowns $q_0$ and the error coefficient term $C' = c(\\Delta t)^p$. To eliminate $C'$ and solve for $q_0$, we can multiply the second equation by $2^p$ and subtract the first equation:\n$$ 2^p \\hat{q}_2 \\approx 2^p q_0 + c (\\Delta t)^p $$\n$$ 2^p \\hat{q}_2 - \\hat{q}_1 \\approx (2^p q_0 + c (\\Delta t)^p) - (q_0 + c (\\Delta t)^p) = q_0(2^p - 1) $$\nSolving for $q_0$ gives the Richardson-extrapolated estimate, which we denote as $q^{\\mathrm{Rich}}_{0}$:\n$$ q^{\\mathrm{Rich}}_{0} = \\frac{2^p \\hat{q}_2 - \\hat{q}_1}{2^p - 1} $$\nThis estimator has a leading error of higher order than $\\mathcal{O}((\\Delta t)^p)$, assuming the expansion holds.\n\n### 2. Integrator Bias Detection\n\nTo determine if the observed difference between $\\hat{q}_1$ and $\\hat{q}_2$ is statistically significant, we test the null hypothesis $H_0: q_{\\Delta t} = q_{\\Delta t/2}$ against the alternative $H_A: q_{\\Delta t} \\neq q_{\\Delta t/2}$. This is a two-proportion $z$-test. Under $H_0$, there is a single, common committor probability, which is best estimated by the pooled proportion $\\hat{q}_{pool}$:\n$$ \\hat{q}_{pool} = \\frac{k_1 + k_2}{n_1 + n_2} $$\nThe standard error of the difference in proportions under $H_0$ is:\n$$ SE(\\hat{q}_1 - \\hat{q}_2) = \\sqrt{\\hat{q}_{pool}(1 - \\hat{q}_{pool}) \\left( \\frac{1}{n_1} + \\frac{1}{n_2} \\right)} $$\nThe $z$-statistic is the observed difference normalized by the standard error:\n$$ z = \\frac{\\hat{q}_1 - \\hat{q}_2}{SE(\\hat{q}_1 - \\hat{q}_2)} $$\nFor a two-sided test at significance level $\\alpha = 0.05$, we find the critical value $z_{crit} = \\Phi^{-1}(1-\\alpha/2)$, where $\\Phi^{-1}$ is the inverse of the standard normal cumulative distribution function. For $\\alpha=0.05$, $z_{crit} \\approx 1.95996$.\nThe decision rule is: if $|z| > z_{crit}$, we reject $H_0$ and conclude that there is a detectable integrator bias.\n\n### 3. Isocommittor Consistency Test\n\nA good reaction coordinate $s$ should capture all slow dynamics, implying that all microstates $\\mathbf{x}_i$ with the same value $s(\\mathbf{x}_i) = s_0$ should have the same committor value, $q(\\mathbf{x}_i) = q_{s_0}$. We test this null hypothesis, $H_0: q_1 = q_2 = \\dots = q_M$ for $M$ microstates, using a Pearson's chi-square test for homogeneity of proportions. The data consists of counts $\\{(k_i, n_i)\\}_{i=1}^M$ from the smaller time step $\\Delta t/2$.\n\nUnder $H_0$, the best estimate for the common committor is the pooled proportion over all microstates:\n$$ \\hat{q}_{pool, \\text{micro}} = \\frac{\\sum_{i=1}^M k_i}{\\sum_{i=1}^M n_i} $$\nFor each microstate $i$, the expected count of successes is $E_i = n_i \\hat{q}_{pool, \\text{micro}}$. The observed count is $k_i$. The chi-square statistic is calculated as:\n$$ \\chi^2 = \\sum_{i=1}^M \\frac{(k_i - E_i)^2}{n_i \\hat{q}_{pool, \\text{micro}}(1-\\hat{q}_{pool, \\text{micro}})} $$\nThis statistic follows a chi-square distribution with $df = M-1$ degrees of freedom. At significance level $\\alpha = 0.05$, we find the critical value $\\chi^2_{crit} = F_{\\chi^2_{df}}^{-1}(1-\\alpha)$, where $F_{\\chi^2_{df}}^{-1}$ is the inverse CDF of the chi-square distribution with $df$ degrees of freedom.\nThe decision rule is: if $\\chi^2 > \\chi^2_{crit}$, we reject $H_0$ and conclude that the reaction coordinate is invalid at this value, as the variability between microstates is too large to be explained by sampling noise alone.\n\n### 4. Physical Bounds Check\n\nThe committor $q(\\mathbf{x})$ is a probability and must lie in the interval $[0, 1]$. The extrapolated estimate $q^{\\mathrm{Rich}}_{0}$ is a numerical construct and is not guaranteed to respect these physical bounds. If $q^{\\mathrm{Rich}}_{0} < 0$ or $q^{\\mathrm{Rich}}_{0} > 1$, it indicates that the simple error model used for extrapolation is inadequate for the given data, possibly due to strong higher-order effects or behavior near the boundaries that is not well-described by a simple power-law expansion. We report whether $q^{\\mathrm{Rich}}_{0}$ is out of bounds.\n\nThese four procedures are implemented for each test case as specified.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm, chi2\n\ndef solve():\n    \"\"\"\n    Applies a validation procedure for committor analysis to three test cases.\n\n    The procedure includes:\n    1. Richardson extrapolation to estimate the zero-timestep committor.\n    2. A two-proportion z-test to detect integrator bias.\n    3. A chi-square test for isocommittor consistency to validate the reaction coordinate.\n    4. A check for the physical validity of the extrapolated committor value.\n    \"\"\"\n    \n    # Critical value for the statistical tests at a two-sided significance level alpha = 0.05\n    alpha = 0.05\n    z_critical = norm.ppf(1 - alpha / 2)\n\n    test_cases = [\n        {\n            \"p\": 1,\n            \"agg_dt\": {\"n\": 20000, \"k\": 10400},\n            \"agg_dt_half\": {\"n\": 20000, \"k\": 10200},\n            \"microstates_dt_half\": [\n                {\"n\": 4000, \"k\": 1400},\n                {\"n\": 4000, \"k\": 2000},\n                {\"n\": 4000, \"k\": 2600},\n                {\"n\": 4000, \"k\": 2000},\n            ]\n        },\n        {\n            \"p\": 2,\n            \"agg_dt\": {\"n\": 10000, \"k\": 520},\n            \"agg_dt_half\": {\"n\": 10000, \"k\": 505},\n            \"microstates_dt_half\": [\n                {\"n\": 4000, \"k\": 192},\n                {\"n\": 4000, \"k\": 204},\n                {\"n\": 4000, \"k\": 208},\n                {\"n\": 4000, \"k\": 196},\n            ]\n        },\n        {\n            \"p\": 1,\n            \"agg_dt\": {\"n\": 8000, \"k\": 7760},\n            \"agg_dt_half\": {\"n\": 8000, \"k\": 7920},\n            \"microstates_dt_half\": [\n                {\"n\": 4000, \"k\": 3960},\n                {\"n\": 4000, \"k\": 3975},\n                {\"n\": 4000, \"k\": 3965},\n                {\"n\": 4000, \"k\": 3972},\n            ]\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        p = case[\"p\"]\n        n1, k1 = case[\"agg_dt\"][\"n\"], case[\"agg_dt\"][\"k\"]\n        n2, k2 = case[\"agg_dt_half\"][\"n\"], case[\"agg_dt_half\"][\"k\"]\n        microstates = case[\"microstates_dt_half\"]\n\n        # --- 1. Richardson Extrapolation ---\n        q1_hat = k1 / n1\n        q2_hat = k2 / n2\n        q_rich_0 = (2**p * q2_hat - q1_hat) / (2**p - 1)\n\n        # --- 4. Physical Bounds Check ---\n        out_of_bounds = not (0 <= q_rich_0 <= 1)\n\n        # --- 2. Integrator Bias Test (Two-Proportion Z-Test) ---\n        q_pool_bias = (k1 + k2) / (n1 + n2)\n        if q_pool_bias == 0 or q_pool_bias == 1:\n            z_stat_bias = 0.0\n        else:\n            se_bias = np.sqrt(q_pool_bias * (1 - q_pool_bias) * (1/n1 + 1/n2))\n            z_stat_bias = (q1_hat - q2_hat) / se_bias\n        \n        bias_detected = np.abs(z_stat_bias) > z_critical\n\n        # --- 3. Isocommittor Consistency Test (Chi-Square Test) ---\n        num_microstates = len(microstates)\n        df_rc = num_microstates - 1\n        chi2_critical = chi2.ppf(1 - alpha, df=df_rc)\n\n        k_micro = np.array([m[\"k\"] for m in microstates])\n        n_micro = np.array([m[\"n\"] for m in microstates])\n\n        k_total_micro = np.sum(k_micro)\n        n_total_micro = np.sum(n_micro)\n        \n        q_pool_rc = k_total_micro / n_total_micro\n\n        if q_pool_rc == 0 or q_pool_rc == 1:\n            chi2_stat_rc = 0.0\n        else:\n            expected_k = n_micro * q_pool_rc\n            # This is the Pearson's chi-square test statistic for homogeneity of proportions\n            chi2_stat_rc = np.sum((k_micro - expected_k)**2 / (n_micro * q_pool_rc * (1-q_pool_rc)))\n        \n        rc_invalid = chi2_stat_rc > chi2_critical\n\n        current_result = [q_rich_0, bias_detected, rc_invalid, out_of_bounds]\n        results.append(current_result)\n    \n    # Format the final output string to match the problem specification\n    formatted_results = []\n    for res in results:\n        q_str = f\"{res[0]:.6f}\"\n        # Convert Python booleans to required string representation\n        bias_str = str(res[1])\n        rc_str = str(res[2])\n        oob_str = str(res[3])\n        formatted_results.append(f\"[{q_str},{bias_str},{rc_str},{oob_str}]\")\n    \n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3402822"}]}