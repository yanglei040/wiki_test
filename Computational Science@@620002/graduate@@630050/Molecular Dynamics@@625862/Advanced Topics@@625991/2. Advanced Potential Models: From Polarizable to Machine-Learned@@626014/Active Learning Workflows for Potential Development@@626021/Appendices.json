{"hands_on_practices": [{"introduction": "Active learning fundamentally relies on quantifying model uncertainty to guide data acquisition. This first practice delves into the mathematical heart of this concept within the framework of Gaussian Processes (GPs). By deriving the posterior predictive variance and interpreting it geometrically, you will gain a foundational understanding of how a model's uncertainty is shaped by the configuration of its training data [@problem_id:3394179]. This exercise is crucial for building intuition about *why* certain configurations are more informative than others.", "problem": "In the development of interatomic potentials via active learning for molecular dynamics, consider modeling a single Cartesian component of the force field, denoted by the scalar function $f(\\mathbf{R})$, as a Gaussian process (GP) with zero mean and positive definite kernel $k(\\mathbf{R},\\mathbf{R}')$. Suppose we construct a descriptor map $s: \\mathbf{R} \\mapsto u \\in \\mathbb{R}$, and assume that the kernel depends only on these descriptors via a squared-exponential form with amplitude and length scale parameters, so that $k(\\mathbf{R},\\mathbf{R}') \\equiv k(u,u') = \\sigma_{f}^{2} \\exp\\!\\big(-\\frac{(u-u')^{2}}{2\\ell^{2}}\\big)$. Measurements of $f$ at training configurations are corrupted by independent Gaussian noise with variance $\\sigma_{n}^{2}$.\n\nStarting from the joint Gaussian prior of $f$ evaluated on the training inputs and a new test input, and using the conditioning rule for multivariate Gaussian distributions, derive the predictive posterior variance of $f$ at a new configuration $\\mathbf{R}_{*}$ with descriptor $u_{*}$ in terms of the kernel Gram matrix on the training set and the kernel vector between $u_{*}$ and the training descriptors. Then, interpret this variance geometrically using the feature map of a positive definite kernel: explain how it equals the squared norm of the residual after orthogonal projection of the feature vector $\\varphi(u_{*})$ onto the span of the training feature vectors, with the observation noise entering as a Tikhonov regularization of the Gram matrix. Explicitly connect this to the geometry of the training set in feature space.\n\nFinally, evaluate the predictive posterior variance for the following specific and self-consistent scenario relevant to an active learning query:\n- Use the squared-exponential kernel $k(u,u') = \\sigma_{f}^{2}\\exp\\!\\big(-\\frac{(u-u')^{2}}{2\\ell^{2}}\\big)$ with hyperparameters $\\sigma_{f}^{2} = 2$ and $\\ell = 1$.\n- The training descriptors are $u_{1} = 0$ and $u_{2} = 2$.\n- The observation noise variance is $\\sigma_{n}^{2} = 0.1$.\n- The test descriptor is $u_{*} = 1$.\n\nReport the predictive posterior variance at $u_{*}$ as a single real number. Round your answer to four significant figures. The quantity is nondimensional; report it without units.", "solution": "The problem asks for the derivation and calculation of the predictive posterior variance of a function modeled as a Gaussian process (GP), along with a geometric interpretation. The problem statement is valid as it is scientifically grounded, well-posed, and provides all necessary information for a unique solution.\n\nLet the set of training inputs be represented by their scalar descriptors $U = \\{u_1, \\dots, u_N\\}$, and the corresponding noisy observations of the function $f$ be collected in a vector $\\mathbf{y} = (y_1, \\dots, y_N)^T$. The model assumes that each observation $y_i$ is related to the latent function value $f_i = f(u_i)$ by $y_i = f_i + \\epsilon_i$, where $\\epsilon_i$ are independent and identically distributed Gaussian noise variables with mean $0$ and variance $\\sigma_n^2$, i.e., $\\epsilon_i \\sim \\mathcal{N}(0, \\sigma_n^2)$.\n\nThe function $f$ is modeled as a GP with zero mean and a covariance function (kernel) $k(u, u')$. Let $\\mathbf{f} = (f(u_1), \\dots, f(u_N))^T$ be the vector of latent function values at the training points, and $f_* = f(u_*)$ be the latent function value at a new test point $u_*$.\n\nAccording to the GP prior, the joint distribution of any collection of function values is a multivariate Gaussian. Thus, the joint distribution of the training observations $\\mathbf{y}$ and the test function value $f_*$ is a zero-mean Gaussian. We derive its covariance matrix. The covariance between two observations $y_i$ and $y_j$ is:\n$$ \\text{cov}(y_i, y_j) = \\text{cov}(f_i + \\epsilon_i, f_j + \\epsilon_j) = \\text{cov}(f_i, f_j) + \\text{cov}(\\epsilon_i, \\epsilon_j) = k(u_i, u_j) + \\sigma_n^2 \\delta_{ij} $$\nwhere $\\delta_{ij}$ is the Kronecker delta. This can be written in matrix form as $K_U + \\sigma_n^2 I$, where $(K_U)_{ij} = k(u_i, u_j)$ is the Gram matrix of the training inputs.\nThe covariance between an observation $y_i$ and the test value $f_*$ is:\n$$ \\text{cov}(y_i, f_*) = \\text{cov}(f_i + \\epsilon_i, f_*) = \\text{cov}(f_i, f_*) = k(u_i, u_*) $$\nLet $\\mathbf{k}_*$ be the vector of covariances between the test point and each training point, i.e., $(\\mathbf{k}_*)_i = k(u_i, u_*)$.\nThe variance of the test value is $\\text{var}(f_*) = \\text{cov}(f_*, f_*) = k(u_*, u_*)$.\n\nThe joint distribution is therefore:\n$$ \\begin{pmatrix} \\mathbf{y} \\\\ f_* \\end{pmatrix} \\sim \\mathcal{N} \\left( \\begin{pmatrix} \\mathbf{0} \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} K_U + \\sigma_n^2 I & \\mathbf{k}_* \\\\ \\mathbf{k}_*^T & k(u_*, u_*) \\end{pmatrix} \\right) $$\nFor a general multivariate Gaussian distribution for variables $\\mathbf{x}_a$ and $\\mathbf{x}_b$ given by\n$$ \\begin{pmatrix} \\mathbf{x}_a \\\\ \\mathbf{x}_b \\end{pmatrix} \\sim \\mathcal{N} \\left( \\begin{pmatrix} \\boldsymbol{\\mu}_a \\\\ \\boldsymbol{\\mu}_b \\end{pmatrix}, \\begin{pmatrix} \\Sigma_{aa} & \\Sigma_{ab} \\\\ \\Sigma_{ba} & \\Sigma_{bb} \\end{pmatrix} \\right) $$\nthe conditional distribution $p(\\mathbf{x}_b | \\mathbf{x}_a)$ is also Gaussian with mean $\\boldsymbol{\\mu}_{b|a} = \\boldsymbol{\\mu}_b + \\Sigma_{ba}\\Sigma_{aa}^{-1}(\\mathbf{x}_a - \\boldsymbol{\\mu}_a)$ and covariance $\\Sigma_{b|a} = \\Sigma_{bb} - \\Sigma_{ba}\\Sigma_{aa}^{-1}\\Sigma_{ab}$.\n\nApplying this conditioning rule to our case, we identify $\\mathbf{x}_a = \\mathbf{y}$, $\\mathbf{x}_b = f_*$, $\\boldsymbol{\\mu}_a = \\mathbf{0}$, $\\boldsymbol{\\mu}_b = 0$, $\\Sigma_{aa} = K_U + \\sigma_n^2 I$, $\\Sigma_{ab} = \\mathbf{k}_*$, $\\Sigma_{ba} = \\mathbf{k}_*^T$, and $\\Sigma_{bb} = k(u_*, u_*)$.\nThe posterior predictive distribution $p(f_* | \\mathbf{y})$ is Gaussian. The posterior mean is $\\bar{f}_* = \\mathbf{k}_*^T (K_U + \\sigma_n^2 I)^{-1} \\mathbf{y}$. The posterior variance, which is what the problem asks for, is:\n$$ \\text{var}(f_* | \\mathbf{y}) = k(u_*, u_*) - \\mathbf{k}_*^T (K_U + \\sigma_n^2 I)^{-1} \\mathbf{k}_* $$\n\nFor the geometric interpretation, we invoke the concept of a Reproducing Kernel Hilbert Space (RKHS), denoted $\\mathcal{H}$, associated with the positive definite kernel $k$. There exists a feature map $\\varphi: u \\mapsto \\varphi(u) \\in \\mathcal{H}$ such that the kernel function is the inner product in this space: $k(u, u') = \\langle \\varphi(u), \\varphi(u') \\rangle_{\\mathcal{H}}$.\nThe predictive variance can be rewritten using this property:\n$$ \\text{var}(f_*) = \\|\\varphi(u_*)\\|^2_{\\mathcal{H}} - \\langle \\varphi(u_*), \\Phi \\rangle ( \\langle \\Phi, \\Phi \\rangle + \\sigma_n^2 I )^{-1} \\langle \\Phi, \\varphi(u_*) \\rangle $$\nwhere $\\Phi$ is an operator whose action on a vector $\\boldsymbol{\\alpha} \\in \\mathbb{R}^N$ is defined as $\\Phi\\boldsymbol{\\alpha} = \\sum_{i=1}^N \\alpha_i \\varphi(u_i)$. The term $\\langle \\varphi(u_*), \\Phi \\rangle$ is a row vector with elements $\\langle \\varphi(u_*), \\varphi(u_i) \\rangle = k(u_*, u_i)$, which is $\\mathbf{k}_*^T$. The term $\\langle \\Phi, \\Phi \\rangle$ is a matrix with elements $\\langle \\varphi(u_i), \\varphi(u_j) \\rangle = k(u_i, u_j)$, which is the Gram matrix $K_U$.\nThe second term, $\\mathbf{k}_*^T (K_U + \\sigma_n^2 I)^{-1} \\mathbf{k}_*$, represents the squared norm of the regularized orthogonal projection of the feature vector $\\varphi(u_*)$ onto the subspace spanned by the training feature vectors $\\{\\varphi(u_i)\\}_{i=1}^N$. The prior variance, $k(u_*, u_*) = \\|\\varphi(u_*)\\|^2_{\\mathcal{H}}$, is the total squared \"length\" of the feature vector for the test point. The predictive variance is thus the squared norm of the component of $\\varphi(u_*)$ that is orthogonal to this regularized projection. This is the squared norm of the residual vector after projecting $\\varphi(u_*)$ onto the span of the training data in feature space. The observation noise variance $\\sigma_n^2$ acts as a Tikhonov regularization term, adding to the diagonal of the Gram matrix $K_U$. This regularization is crucial for numerical stability, as $K_U$ can be ill-conditioned or singular if some training points are very close to each other (i.e., highly correlated). Geometrically, it means we are not projecting onto the exact span of the training features, but rather solving a regularized problem that penalizes large coefficients, effectively shrinking the projection.\n\nFinally, we evaluate the predictive posterior variance for the given scenario.\nThe kernel is $k(u,u') = \\sigma_{f}^{2}\\exp(-\\frac{(u-u')^{2}}{2\\ell^{2}})$ with $\\sigma_{f}^{2} = 2$ and $\\ell = 1$. So, $k(u,u') = 2\\exp(-\\frac{(u-u')^{2}}{2})$.\nThe training descriptors are $u_{1} = 0$ and $u_{2} = 2$.\nThe test descriptor is $u_{*} = 1$.\nThe noise variance is $\\sigma_{n}^{2} = 0.1$.\n\nFirst, we compute the necessary kernel values:\nThe prior variance at the test point is $k(u_*, u_*) = k(1, 1) = 2\\exp(0) = 2$.\nThe kernel vector between the test point and the training points is:\n$$ \\mathbf{k}_* = \\begin{pmatrix} k(1, 0) \\\\ k(1, 2) \\end{pmatrix} = \\begin{pmatrix} 2\\exp(-\\frac{(1-0)^2}{2}) \\\\ 2\\exp(-\\frac{(1-2)^2}{2}) \\end{pmatrix} = \\begin{pmatrix} 2\\exp(-0.5) \\\\ 2\\exp(-0.5) \\end{pmatrix} $$\nThe Gram matrix for the training points is:\n$$ K_U = \\begin{pmatrix} k(0, 0) & k(0, 2) \\\\ k(2, 0) & k(2, 2) \\end{pmatrix} = \\begin{pmatrix} 2\\exp(0) & 2\\exp(-\\frac{(0-2)^2}{2}) \\\\ 2\\exp(-\\frac{(2-0)^2}{2}) & 2\\exp(0) \\end{pmatrix} = \\begin{pmatrix} 2 & 2\\exp(-2) \\\\ 2\\exp(-2) & 2 \\end{pmatrix} $$\nThe matrix to be inverted is $M = K_U + \\sigma_n^2 I$:\n$$ M = \\begin{pmatrix} 2 & 2\\exp(-2) \\\\ 2\\exp(-2) & 2 \\end{pmatrix} + \\begin{pmatrix} 0.1 & 0 \\\\ 0 & 0.1 \\end{pmatrix} = \\begin{pmatrix} 2.1 & 2\\exp(-2) \\\\ 2\\exp(-2) & 2.1 \\end{pmatrix} $$\nThe reduction in variance is given by the quadratic form $\\mathbf{k}_*^T M^{-1} \\mathbf{k}_*$.\nLet $k_s = 2\\exp(-0.5)$ and $k_d = 2\\exp(-2)$. Then $\\mathbf{k}_* = \\begin{pmatrix} k_s \\\\ k_s \\end{pmatrix}$ and $M = \\begin{pmatrix} 2.1 & k_d \\\\ k_d & 2.1 \\end{pmatrix}$.\nThe term is $k_s^2 \\begin{pmatrix} 1 & 1 \\end{pmatrix} M^{-1} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$.\nThe inverse of $M$ is $M^{-1} = \\frac{1}{(2.1)^2 - k_d^2} \\begin{pmatrix} 2.1 & -k_d \\\\ -k_d & 2.1 \\end{pmatrix}$.\nThe product $\\begin{pmatrix} 1 & 1 \\end{pmatrix} M^{-1} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$ is the sum of all elements of $M^{-1}$, which is:\n$$ \\frac{2.1 - k_d - k_d + 2.1}{(2.1)^2 - k_d^2} = \\frac{2(2.1 - k_d)}{(2.1 - k_d)(2.1 + k_d)} = \\frac{2}{2.1 + k_d} $$\nThe reduction in variance is thus $k_s^2 \\frac{2}{2.1 + k_d} = (2\\exp(-0.5))^2 \\frac{2}{2.1 + 2\\exp(-2)} = \\frac{8\\exp(-1)}{2.1 + 2\\exp(-2)}$.\nNow, we compute the numerical value:\n$$ \\frac{8\\exp(-1)}{2.1 + 2\\exp(-2)} \\approx \\frac{8 \\times 0.36787944}{2.1 + 2 \\times 0.13533528} = \\frac{2.9430355}{2.1 + 0.27067056} = \\frac{2.9430355}{2.37067056} \\approx 1.241434 $$\nThe predictive posterior variance is:\n$$ \\text{var}(f_*) = k(u_*, u_*) - \\frac{8\\exp(-1)}{2.1 + 2\\exp(-2)} \\approx 2 - 1.241434 = 0.758566 $$\nRounding to four significant figures, the predictive posterior variance is $0.7586$.", "answer": "$$\\boxed{0.7586}$$", "id": "3394179"}, {"introduction": "With a theoretical grasp of uncertainty, the next step is to build a practical algorithm that uses it to make decisions. This exercise tasks you with implementing a greedy selection strategy, a cornerstone of many active learning workflows, to prune a redundant pool of candidate structures [@problem_id:3394147]. By translating the principle of variance reduction into code, you will develop the essential skill of constructing an efficient data selection engine and discover the practical equivalence of different information-theoretic criteria.", "problem": "You are developing an active learning workflow for interatomic potential development in molecular dynamics. The unlabeled training pool consists of atomic environment descriptors, and you will model similarity between environments through a positive-definite kernel. Your goal is to formalize redundancy in the pool via a pairwise similarity function, derive principled pruning criteria that preserve downstream surrogate performance, and implement a program that applies these criteria and quantifies the outcome on a test suite.\n\nAssume the following foundational setting.\n\n- Each atomic environment is represented by a descriptor vector $x \\in \\mathbb{R}^d$.\n- Similarity between two environments $x_i, x_j$ is defined by a radial basis function kernel $s(x_i,x_j) = k(x_i,x_j) = \\exp\\!\\left(-\\frac{\\lVert x_i - x_j \\rVert_2^2}{2 \\ell^2}\\right)$ with lengthscale $\\ell > 0$ and unit amplitude.\n- A Gaussian Process (GP) surrogate model for an energy or force component $f(x)$ uses the kernel $k(\\cdot, \\cdot)$ with independent and identically distributed observation noise of variance $\\sigma^2$.\n- The posterior predictive variance at a pool point $x$ after selecting and labeling a subset $S$ of indices is\n$$\n\\mathrm{Var}_S(x) = k(x,x) - k(x,X_S)\\left(K_{SS} + \\sigma^2 I\\right)^{-1} k(X_S,x),\n$$\nwhere $X_S$ denotes the selected descriptors, $K_{SS}$ is the Gram matrix on $S$, $I$ is the identity matrix, and $k(x,X_S)$ is the vector of kernel evaluations between $x$ and each element of $X_S$.\n- As a performance proxy, use the integrated posterior variance over the pool $T$ (take $T$ equal to the entire pool of size $N$) defined by\n$$\n\\mathcal{V}(S) = \\sum_{i=1}^N \\mathrm{Var}_S(x_i).\n$$\nThis quantity is to be minimized by pruning while preserving surrogate accuracy.\n\nDefine a redundancy score of the pool relative to a selected subset $S$ as\n$$\n\\mathcal{R}(S) = \\frac{1}{N} \\sum_{i=1}^N \\max_{j \\in S} s(x_i, x_j),\n$$\nwhich is large if many pool points are highly similar to at least one retained representative.\n\nYou must implement two pruning criteria derived from principled objectives:\n\n- D-optimal selection via greedy log-determinant maximization: choose a subset $S$ of a specified budget $b$ that greedily maximizes the log-determinant\n$$\n\\Phi_D(S) = \\log \\det\\!\\left(K_{SS} + \\sigma^2 I\\right),\n$$\nusing the block determinant identity to compute marginal gains. This criterion proxies information gain and diversity.\n- Variance-greedy selection via greedy posterior variance reduction (equivalently, pivoted Cholesky on the kernel): iteratively select the point $j$ with the largest current $\\mathrm{Var}_S(x_j)$ until budget $b$ is exhausted, updating the posterior variance efficiently using Cholesky updates. This criterion directly targets the reduction of $\\mathcal{V}(S)$.\n\nStarting from the definitions above and without assuming any particular potential functional form beyond the kernel, you must:\n\n- Derive each greedy selection’s marginal gain from the stated base formulas and identities.\n- Implement both greedy selectors using only linear algebra primitives compatible with the stated observation noise model, ensuring numerical stability.\n- Compute the resulting integrated posterior variance $\\mathcal{V}(S)$ for each selector on a set of prescribed test cases.\n\nYour program must hard-code the following test suite of pools, parameters, and budgets. All coordinates are dimensionless descriptor values, and similarities are dimensionless. No physical units are required.\n\n- Test case $1$ (clustered pool, happy path):\n  - Descriptors $X \\subset \\mathbb{R}^2$ with $8$ points:\n    - Cluster near $(0,0)$: $(0,0)$, $(0.1,-0.05)$, $(-0.1,0.05)$, $(0.05,0.1)$.\n    - Cluster near $(3,3)$: $(3,3)$, $(3.1,2.9)$, $(2.9,3.1)$, $(3.05,3.0)$.\n  - Kernel lengthscale $\\ell = 0.5$.\n  - Noise standard deviation $\\sigma = 0.05$.\n  - Budget $b = 2$.\n\n- Test case $2$ (boundary case, zero budget):\n  - Descriptors $X \\subset \\mathbb{R}^2$ with $5$ points: $(0,0)$, $(1,0)$, $(0,1)$, $(1,1)$, $(0.5,0.5)$.\n  - Kernel lengthscale $\\ell = 1.0$.\n  - Noise standard deviation $\\sigma = 0.1$.\n  - Budget $b = 0$.\n\n- Test case $3$ (boundary case, full budget):\n  - Descriptors $X \\subset \\mathbb{R}^2$ with $5$ points: $(0,0)$, $(2,0)$, $(0,2)$, $(2,2)$, $(1,1)$.\n  - Kernel lengthscale $\\ell = 0.7$.\n  - Noise standard deviation $\\sigma = 0.01$.\n  - Budget $b = 5$.\n\n- Test case $4$ (high redundancy with duplicates in $\\mathbb{R}$):\n  - Descriptors $X \\subset \\mathbb{R}^1$ with $10$ points: $0$, $0$, $1$, $1$, $2$, $2$, $3$, $3$, $4$, $4$.\n  - Kernel lengthscale $\\ell = 0.3$.\n  - Noise standard deviation $\\sigma = 0.05$.\n  - Budget $b = 5$.\n\nFor each test case, your program must compute two floats:\n\n- The integrated posterior variance $\\mathcal{V}(S_D)$ where $S_D$ is obtained by D-optimal greedy selection with budget $b$.\n- The integrated posterior variance $\\mathcal{V}(S_A)$ where $S_A$ is obtained by variance-greedy selection with budget $b$.\n\nYour program should produce a single line of output containing all results, ordered by test case and by method $\\left(\\mathcal{V}(S_D), \\mathcal{V}(S_A)\\right)$, flattened into a list. For example, the output format should be\n$[$result$_{1,D}$, result$_{1,A}$, result$_{2,D}$, result$_{2,A}$, result$_{3,D}$, result$_{3,A}$, result$_{4,D}$, result$_{4,A}]$,\nwith each float rounded to $6$ decimal places. No other text should be printed. Angles are not involved. Percentages are not involved. All quantities are dimensionless. The program must not read any input and must not access any files or networks.", "solution": "The user-provided problem is assessed as valid. It is scientifically grounded in the principles of Gaussian Process regression and active learning, employing standard mathematical formulations such as the radial basis function kernel and posterior predictive variance. The problem is well-posed, providing all necessary data and parameters for the defined tasks, leading to a unique, computable solution. The language is objective and unambiguous. Consequently, a full solution is warranted.\n\n### Principle-Based Design and Derivations\n\nThe core of the problem is to select a subset of atomic environments, represented by descriptor vectors, from a larger pool to train a surrogate model. The goal is to do this intelligently to maximize the information content of the selected set, a process known as active learning or experimental design. The problem specifies two greedy selection criteria to build a selected set $S$ of size $b$. We will derive the marginal gain for each criterion, which dictates the choice at each step of the greedy algorithm.\n\nLet the set of selected indices after $k$ steps be $S_k$. We want to choose the next index $j$ from the set of available indices $U$ to add to $S_k$, forming $S_{k+1} = S_k \\cup \\{j\\}$.\n\n#### 1. D-optimal Greedy Selection\n\nThis criterion aims to maximize the information content of the selected set by greedily maximizing the logarithm of the determinant of the data covariance matrix, $\\Phi_D(S) = \\log \\det(K_{SS} + \\sigma^2 I)$. The marginal gain from adding point $x_j$ to the set $S_k$ is:\n$$\n\\Delta \\Phi_D(j | S_k) = \\Phi_D(S_k \\cup \\{j\\}) - \\Phi_D(S_k)\n$$\nTo compute this, we use the block matrix determinant formula. The matrix for $S_{k+1}$ can be partioned as:\n$$\nK_{S_{k+1}S_{k+1}} + \\sigma^2 I =\n\\begin{pmatrix}\nK_{S_kS_k} + \\sigma^2 I & k(X_{S_k}, x_j) \\\\\nk(x_j, X_{S_k}) & k(x_j, x_j) + \\sigma^2\n\\end{pmatrix}\n$$\nUsing the identity $\\det \\begin{pmatrix} A & B \\\\ C & D \\end{pmatrix} = \\det(A) \\det(D - CA^{-1}B)$, we get:\n$$\n\\det(K_{S_{k+1}S_{k+1}} + \\sigma^2 I) = \\det(K_{S_kS_k} + \\sigma^2 I) \\left( (k(x_j, x_j) + \\sigma^2) - k(x_j, X_{S_k}) (K_{S_kS_k} + \\sigma^2 I)^{-1} k(X_{S_k}, x_j) \\right)\n$$\nTaking the logarithm of both sides gives:\n$$\n\\Phi_D(S_{k+1}) = \\Phi_D(S_k) + \\log \\left( (k(x_j, x_j) + \\sigma^2) - k(x_j, X_{S_k}) (K_{S_kS_k} + \\sigma^2 I)^{-1} k(X_{S_k}, x_j) \\right)\n$$\nThe marginal gain is the second term. We can simplify the expression inside the logarithm by relating it to the posterior predictive variance. The posterior variance at point $x_j$ given the selection $S_k$ is:\n$$\n\\mathrm{Var}_{S_k}(x_j) = k(x_j, x_j) - k(x_j, X_{S_k}) (K_{S_kS_k} + \\sigma^2 I)^{-1} k(X_{S_k}, x_j)\n$$\nSubstituting this into the marginal gain expression yields:\n$$\n\\Delta \\Phi_D(j | S_k) = \\log(\\mathrm{Var}_{S_k}(x_j) + \\sigma^2)\n$$\nTo maximize the marginal gain $\\Delta \\Phi_D(j | S_k)$, we must choose the index $j$ that maximizes its argument, $\\mathrm{Var}_{S_k}(x_j) + \\sigma^2$. Since $\\sigma^2$ is a constant, this is equivalent to choosing the point $x_j$ that has the maximum current posterior variance.\n\n#### 2. Variance-Greedy Selection\n\nThis criterion is explicitly defined in the problem as \"iteratively select the point $j$ with the largest current $\\mathrm{Var}_S(x_j)$\". This is precisely the same selection strategy derived from the D-optimal criterion.\n\nTherefore, for the given problem setup (GP with noise), the D-optimal greedy selection and the variance-greedy selection are equivalent procedures. They will produce the exact same sequence of selected points and thus the same final set $S$. Consequently, the resulting integrated posterior variances, $\\mathcal{V}(S_D)$ and $\\mathcal{V}(S_A)$, will be identical for each test case.\n\n### Algorithmic Implementation\n\nThe implementation will consist of three main components:\n1.  A function to compute the RBF kernel matrix between two sets of descriptor vectors.\n2.  A greedy selection function that implements the variance-maximization strategy. This function will iterate up to the specified budget $b$. In each iteration, it computes the posterior variance for all unselected points and adds the one with the highest variance to the selected set. For numerical stability, linear systems of the form $(K_{SS} + \\sigma^2 I)z = k(X_S, x_j)$ are solved instead of explicitly computing matrix inverses where possible.\n3.  A function to compute the final integrated posterior variance $\\mathcal{V}(S) = \\sum_{i=1}^N \\mathrm{Var}_S(x_i)$ for a given selected set $S$. This function calculates the posterior variance $\\mathrm{Var}_S(x_i)$ for every point $x_i$ in the entire pool and sums them up. Special handling is implemented for boundary cases:\n    - If $S$ is empty ($b=0$), $\\mathrm{Var}_{\\emptyset}(x_i) = k(x_i,x_i) = 1$. Thus, $\\mathcal{V}(\\emptyset) = N$.\n    - If $S$ comprises the entire pool ($b=N$), a more efficient formula is used: $\\mathcal{V}(X) = \\sigma^2 (N - \\sigma^2 \\mathrm{Tr}((K_{XX}+\\sigma^2 I)^{-1}))$.\n\nThe program will execute this logic for each of the four test cases provided, generating pairs of identical values for $\\mathcal{V}(S_D)$ and $\\mathcal{V}(S_A)$ which are then formatted into the required output string.", "answer": "```python\nimport numpy as np\nfrom scipy.spatial.distance import cdist\n\ndef rbf_kernel(X1, X2, length_scale):\n    \"\"\"\n    Computes the Radial Basis Function (RBF) kernel between two sets of vectors.\n\n    Args:\n        X1 (np.ndarray): A (N1, d) array of N1 vectors.\n        X2 (np.ndarray): A (N2, d) array of N2 vectors.\n        length_scale (float): The lengthscale parameter l of the kernel.\n\n    Returns:\n        np.ndarray: The (N1, N2) kernel matrix.\n    \"\"\"\n    # Using scipy.spatial.distance.cdist for a robust and efficient calculation\n    # of squared Euclidean distances.\n    sq_dists = cdist(X1, X2, 'sqeuclidean')\n    return np.exp(-sq_dists / (2 * length_scale**2))\n\ndef greedy_selector(X, b, ell, sigma):\n    \"\"\"\n    Selects a subset of indices of size b from X using a greedy strategy\n    that maximizes the posterior variance at each step.\n\n    Args:\n        X (np.ndarray): The full (N, d) data pool.\n        b (int): The selection budget.\n        ell (float): The kernel lengthscale.\n        sigma (float): The observation noise standard deviation.\n\n    Returns:\n        list: A list of b selected indices.\n    \"\"\"\n    N = X.shape[0]\n\n    if b == 0:\n        return []\n    if b >= N:\n        return list(range(N))\n\n    selected_indices = []\n    available_indices = list(range(N))\n\n    # Initial Step: All points have variance 1.0. Tie-break by picking the smallest index.\n    first_pick = available_indices[0]\n    selected_indices.append(first_pick)\n    available_indices.remove(first_pick)\n\n    for _ in range(1, b):\n        if not available_indices:\n            break\n\n        S_arr = np.array(selected_indices)\n        X_S = X[S_arr]\n\n        K_SS = rbf_kernel(X_S, X_S, ell)\n        M = K_SS + sigma**2 * np.eye(len(selected_indices))\n\n        max_var = -1.0\n        best_candidate_idx = -1\n\n        for idx in available_indices:\n            x_j = X[idx:idx + 1]\n            k_S_j = rbf_kernel(X_S, x_j, ell)\n\n            # Var = k(j,j) - k(S,j)^T * (K_SS + sigma^2*I)^-1 * k(S,j)\n            # We solve the linear system Mz = k(S,j) for z to avoid explicit inversion.\n            z = np.linalg.solve(M, k_S_j)\n            # k(j,j) = 1 for RBF kernel with unit amplitude\n            var = 1.0 - k_S_j.T @ z\n            \n            if var.item() > max_var:\n                max_var = var.item()\n                best_candidate_idx = idx\n        \n        selected_indices.append(best_candidate_idx)\n        available_indices.remove(best_candidate_idx)\n\n    return selected_indices\n\ndef calculate_integrated_variance(X, S, ell, sigma):\n    \"\"\"\n    Calculates the integrated posterior variance over the whole pool X,\n    given a selected subset of indices S.\n\n    Args:\n        X (np.ndarray): The full (N, d) data pool.\n        S (list): The list of selected indices.\n        ell (float): The kernel lengthscale.\n        sigma (float): The observation noise standard deviation.\n\n    Returns:\n        float: The integrated posterior variance V(S).\n    \"\"\"\n    N = X.shape[0]\n    \n    if not S:\n        # For an empty selection, posterior variance is prior variance at each point.\n        # Var(x_i) = k(x_i, x_i) = 1. Sum over N points gives N.\n        return float(N)\n\n    S_arr = np.array(S)\n    X_S = X[S_arr]\n\n    if len(S) == N: # Full budget case\n        K_XX = rbf_kernel(X, X, ell)\n        M = K_XX + sigma**2 * np.eye(N)\n        M_inv = np.linalg.inv(M)\n        # Efficient trace-based formula for V(X)\n        trace_M_inv = np.trace(M_inv)\n        return sigma**2 * (N - sigma**2 * trace_M_inv)\n\n    # General case for b < N\n    K_SS = rbf_kernel(X_S, X_S, ell)\n    M = K_SS + sigma**2 * np.eye(len(S))\n\n    total_variance = 0.0\n    for i in range(N):\n        x_i = X[i:i + 1]\n        k_S_i = rbf_kernel(X_S, x_i, ell)\n        z = np.linalg.solve(M, k_S_i)\n        var_i = 1.0 - k_S_i.T @ z\n        total_variance += var_i.item()\n        \n    return total_variance\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    test_cases = [\n        {\n            \"X\": [(0,0), (0.1,-0.05), (-0.1,0.05), (0.05,0.1), (3,3), (3.1,2.9), (2.9,3.1), (3.05,3.0)],\n            \"ell\": 0.5, \"sigma\": 0.05, \"b\": 2\n        },\n        {\n            \"X\": [(0,0), (1,0), (0,1), (1,1), (0.5,0.5)],\n            \"ell\": 1.0, \"sigma\": 0.1, \"b\": 0\n        },\n        {\n            \"X\": [(0,0), (2,0), (0,2), (2,2), (1,1)],\n            \"ell\": 0.7, \"sigma\": 0.01, \"b\": 5\n        },\n        {\n            \"X\": [0, 0, 1, 1, 2, 2, 3, 3, 4, 4],\n            \"ell\": 0.3, \"sigma\": 0.05, \"b\": 5\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        X_data = np.array(case[\"X\"])\n        # Ensure X is a 2D array for cdist\n        if X_data.ndim == 1:\n            X_data = X_data.reshape(-1, 1)\n\n        b = case[\"b\"]\n        ell = case[\"ell\"]\n        sigma = case[\"sigma\"]\n\n        # As derived, both D-optimal and variance-greedy selections are equivalent.\n        # We run the selector once and use the result for both.\n        S_selected = greedy_selector(X_data, b, ell, sigma)\n        \n        # Calculate the integrated variance for the selected set.\n        V_S = calculate_integrated_variance(X_data, S_selected, ell, sigma)\n\n        # Since S_D = S_A, the resulting variances are identical.\n        results.append(V_S)\n        results.append(V_S)\n    \n    # Format the final output string exactly as required.\n    print(f\"[{','.join([f'{res:.6f}' for res in results])}]\")\n\nsolve()\n```", "id": "3394147"}, {"introduction": "Simple uncertainty sampling can sometimes lead to myopic selections, focusing too heavily on refining predictions in one region of the configuration space. Advanced workflows often require balancing multiple competing goals, such as reducing prediction error (exploitation) and discovering novel structures (exploration). This final practice introduces you to the sophisticated world of multi-objective active learning, where you will implement an acquisition strategy that navigates the trade-off between accuracy and diversity using the principles of Pareto optimization [@problem_id:3394183].", "problem": "You are tasked with formalizing and implementing a two-objective acquisition strategy for active learning in molecular dynamics potential development. Consider a pool of unlabeled candidate atomic environments represented by fixed-length descriptors, a current labeled training set, and a validation set used to estimate generalization. Assume the force prediction error process is modeled by a zero-mean Gaussian Process (GP) over the descriptor space with an isotropic squared-exponential kernel. Your goals are to (i) compute a force Root Mean Square Error (RMSE) decrease objective based on predictive variance reduction from adding a single candidate to the training set, (ii) compute a diversity objective based on a kernelized distance in descriptor space, (iii) derive the Pareto front over the candidate set under the two objectives, and (iv) perform selection via the epsilon-constraint method.\n\nFundamental base and definitions:\n- Let the squared-exponential kernel be defined by\n$$\nk(\\mathbf{x},\\mathbf{y}) \\;=\\; \\sigma_k^2 \\exp\\!\\left(-\\dfrac{\\|\\mathbf{x}-\\mathbf{y}\\|_2^2}{2\\ell^2}\\right),\n$$\nwhere $\\sigma_k^2 \\!>\\! 0$ is the signal variance and $\\ell \\!>\\! 0$ is the lengthscale. Let $\\sigma_n^2 \\!\\ge\\! 0$ denote an observation noise variance.\n- Given a training descriptor matrix $X_T \\in \\mathbb{R}^{n_T \\times d}$, the posterior predictive variance at a validation point $\\mathbf{z} \\in \\mathbb{R}^d$ before adding a new point is\n$$\nv_{\\mathrm{old}}(\\mathbf{z}) \\;=\\; k(\\mathbf{z},\\mathbf{z}) \\;-\\; \\mathbf{k}_{T\\mathbf{z}}^\\top \\left(K_{TT} + \\sigma_n^2 I\\right)^{-1} \\mathbf{k}_{T\\mathbf{z}},\n$$\nwhere $K_{TT} \\in \\mathbb{R}^{n_T \\times n_T}$ has entries $[K_{TT}]_{ij} = k(\\mathbf{x}_i,\\mathbf{x}_j)$ for $\\mathbf{x}_i,\\mathbf{x}_j \\in X_T$ and $\\mathbf{k}_{T\\mathbf{z}} \\in \\mathbb{R}^{n_T}$ has entries $[\\mathbf{k}_{T\\mathbf{z}}]_i = k(\\mathbf{x}_i,\\mathbf{z})$.\n- For a candidate $\\mathbf{x}$, define the current posterior covariance between $\\mathbf{z}$ and $\\mathbf{x}$ as\n$$\nc_{\\mathrm{old}}(\\mathbf{z},\\mathbf{x}) \\;=\\; k(\\mathbf{z},\\mathbf{x}) \\;-\\; \\mathbf{k}_{T\\mathbf{z}}^\\top \\left(K_{TT} + \\sigma_n^2 I\\right)^{-1} \\mathbf{k}_{T\\mathbf{x}} ,\n$$\nwith $\\mathbf{k}_{T\\mathbf{x}} \\in \\mathbb{R}^{n_T}$, $[\\mathbf{k}_{T\\mathbf{x}}]_i = k(\\mathbf{x}_i,\\mathbf{x})$.\n- After hypothetically adding a noisy observation at $\\mathbf{x}$, the posterior variance at $\\mathbf{z}$ updates by a rank-$1$ correction (via the Gaussian conditioning identity and the Sherman–Morrison–Woodbury formula) to\n$$\nv_{\\mathrm{new}}(\\mathbf{z}\\,;\\mathbf{x}) \\;=\\; v_{\\mathrm{old}}(\\mathbf{z}) \\;-\\; \\dfrac{c_{\\mathrm{old}}(\\mathbf{z},\\mathbf{x})^2}{v_{\\mathrm{old}}(\\mathbf{x}) + \\sigma_n^2} .\n$$\n- Let a finite validation set be $X_V = \\{\\mathbf{z}_m\\}_{m=1}^{n_V}$. Define the force RMSE decrease objective for a candidate $\\mathbf{x}$ as\n$$\n\\Delta \\mathrm{RMSE}_F(\\mathbf{x}) \\;=\\; \\sqrt{ \\dfrac{1}{n_V} \\sum_{m=1}^{n_V} v_{\\mathrm{old}}(\\mathbf{z}_m)} \\;-\\; \\sqrt{ \\dfrac{1}{n_V} \\sum_{m=1}^{n_V} v_{\\mathrm{new}}(\\mathbf{z}_m\\,;\\mathbf{x}) } .\n$$\n- Define the diversity objective using the kernel-induced distance to the empirical training distribution (the squared Maximum Mean Discrepancy between the singleton $\\{\\mathbf{x}\\}$ and the empirical measure on $X_T$):\n$$\nD_k(\\mathbf{x}) \\;=\\; \\sqrt{\\, k(\\mathbf{x},\\mathbf{x}) \\;-\\; \\dfrac{2}{n_T} \\sum_{i=1}^{n_T} k(\\mathbf{x},\\mathbf{x}_i) \\;+\\; \\dfrac{1}{n_T^2} \\sum_{i=1}^{n_T} \\sum_{j=1}^{n_T} k(\\mathbf{x}_i,\\mathbf{x}_j) \\,} .\n$$\n- For two candidates $\\mathbf{x}$ and $\\mathbf{y}$, say $\\mathbf{x}$ Pareto-dominates $\\mathbf{y}$ if $\\Delta \\mathrm{RMSE}_F(\\mathbf{x}) \\ge \\Delta \\mathrm{RMSE}_F(\\mathbf{y})$ and $D_k(\\mathbf{x}) \\ge D_k(\\mathbf{y})$ with at least one strict inequality. The Pareto front is the set of non-dominated candidates.\n- Given an $\\epsilon$-constraint $\\epsilon \\ge 0$ on diversity, select a candidate by maximizing $\\Delta \\mathrm{RMSE}_F(\\mathbf{x})$ subject to $D_k(\\mathbf{x}) \\ge \\epsilon$. If there are ties, prefer larger $D_k(\\mathbf{x})$, and if still tied, choose the smallest zero-based index. If no candidate satisfies the constraint, return the integer $-1$.\n\nYour program must:\n- Implement the above objectives for each test case.\n- Compute the Pareto front indices and perform $\\epsilon$-constraint selection for each specified $\\epsilon$.\n- Use zero-based indexing for candidate labeling.\n- Produce a single line of output containing, for each test case in order, a list with two elements: the first is the list of Pareto front indices sorted in ascending order; the second is the list of selected indices for each $\\epsilon$ in the order given in that test case. Aggregate the lists for all test cases into one outer list, and print it in a Python-like list format with no spaces, for example, $[ [ [a,b],[c,d] ], [ [e],[f,g] ] ]$ but without spaces.\n\nUse the following test suite.\n\nTest case $1$ (one-dimensional descriptors):\n- Kernel parameters: $\\sigma_k^2 = 1.0$, $\\ell = 0.5$, $\\sigma_n^2 = 10^{-6}$.\n- Training descriptors $X_T \\in \\mathbb{R}^{2 \\times 1}$: $\\big[[-1.0],[1.0]\\big]$.\n- Candidate pool $X_P \\in \\mathbb{R}^{5 \\times 1}$: $\\big[[-0.8],[-0.2],[0.0],[0.5],[1.2]\\big]$.\n- Validation set $X_V \\in \\mathbb{R}^{31 \\times 1}$: the $31$ equally spaced points from $-1.5$ to $1.5$ inclusive.\n- $\\epsilon$ values (diversity thresholds): $[0.02, 0.06, 0.12]$.\n\nTest case $2$ (two-dimensional descriptors):\n- Kernel parameters: $\\sigma_k^2 = 0.8$, $\\ell = 0.7$, $\\sigma_n^2 = 0.01$.\n- Training descriptors $X_T \\in \\mathbb{R}^{3 \\times 2}$: $\\big[[0.0,0.0],[1.0,0.0],[0.0,1.0]\\big]$.\n- Candidate pool $X_P \\in \\mathbb{R}^{6 \\times 2}$: $\\big[[0.5,0.5],[1.0,1.0],[-0.5,0.5],[0.2,-0.8],[0.8,0.2],[1.5,-0.5]\\big]$.\n- Validation set $X_V \\in \\mathbb{R}^{25 \\times 2}$: the $5 \\times 5$ Cartesian grid over each coordinate taking values in $\\{-0.5, 0.0, 0.5, 1.0, 1.5\\}$.\n- $\\epsilon$ values: $[0.05, 0.12, 0.20]$.\n\nTest case $3$ (edge case for feasibility):\n- Kernel parameters: $\\sigma_k^2 = 0.5$, $\\ell = 2.0$, $\\sigma_n^2 = 10^{-6}$.\n- Training descriptors $X_T \\in \\mathbb{R}^{1 \\times 1}$: $\\big[[0.0]\\big]$.\n- Candidate pool $X_P \\in \\mathbb{R}^{3 \\times 1}$: $\\big[[0.0],[0.1],[0.2]\\big]$.\n- Validation set $X_V \\in \\mathbb{R}^{11 \\times 1}$: the $11$ equally spaced points from $0.0$ to $0.2$ inclusive.\n- $\\epsilon$ values: $[0.20, 0.40]$.\n\nFinal output format requirement:\n- Your program should produce a single line of output containing the results as a Python-like list without spaces. For each test case, output a list with two elements:\n  $[ \\text{pareto\\_indices\\_sorted}, \\text{selected\\_indices\\_per\\_epsilon} ]$,\n  where both elements are lists of integers. The outer list aggregates these lists for all test cases in the order given. For example, the format is\n  $[[[i_1,i_2],[j_1,j_2,j_3]],[[i_3,i_4,i_5],[j_4,j_5]]]$,\n  where each $i_\\cdot$ and $j_\\cdot$ is an integer.", "solution": "The posed problem requires the development and implementation of a two-objective active learning selection strategy for molecular dynamics potentials. This is a well-defined computational problem rooted in Gaussian Process (GP) regression and multi-objective optimization theory. The solution will be derived by first implementing the two objectives—force RMSE decrease and diversity—and then applying standard techniques for Pareto front identification and $\\epsilon$-constraint-based selection.\n\nOur algorithmic approach consists of the following steps for each test case:\n1.  **Preprocessing**: Essential matrices and constants that do not depend on the specific candidate point are computed once to optimize calculations. The core of this is the inverse of the regularized kernel matrix of the training set, $M = (K_{TT} + \\sigma_n^2 I)^{-1}$, where $[K_{TT}]_{ij} = k(\\mathbf{x}_i, \\mathbf{x}_j)$ for all pairs of training descriptors $\\mathbf{x}_i, \\mathbf{x}_j \\in X_T$.\n\n2.  **Objective Function Calculation**: For each candidate descriptor $\\mathbf{x}$ from the pool $X_P$, we compute two objective values which we aim to maximize.\n\n    a. **Force RMSE Decrease, $\\Delta \\mathrm{RMSE}_F(\\mathbf{x})$**: This objective estimates the reduction in the model's generalization error on a validation set $X_V$ upon adding $\\mathbf{x}$ to the training set. It is defined as the difference between the root mean square of the predictive variances before and after the hypothetical update:\n    $$\n    \\Delta \\mathrm{RMSE}_F(\\mathbf{x}) = \\sqrt{ \\frac{1}{n_V} \\sum_{m=1}^{n_V} v_{\\mathrm{old}}(\\mathbf{z}_m)} - \\sqrt{ \\frac{1}{n_V} \\sum_{m=1}^{n_V} v_{\\mathrm{new}}(\\mathbf{z}_m; \\mathbf{x}) }\n    $$\n    The term $\\mathrm{RMSE}_{\\mathrm{old}} = \\sqrt{\\frac{1}{n_V} \\sum_{m=1}^{n_V} v_{\\mathrm{old}}(\\mathbf{z}_m)}$ is computed once. For each candidate $\\mathbf{x}$, the new variances $v_{\\mathrm{new}}(\\mathbf{z}_m; \\mathbf{x})$ are determined using the efficient rank-$1$ update formula:\n    $$\n    v_{\\mathrm{new}}(\\mathbf{z}_m; \\mathbf{x}) = v_{\\mathrm{old}}(\\mathbf{z}_m) - \\frac{c_{\\mathrm{old}}(\\mathbf{z}_m, \\mathbf{x})^2}{v_{\\mathrm{old}}(\\mathbf{x}) + \\sigma_n^2}\n    $$\n    where $v_{\\mathrm{old}}$ and $c_{\\mathrm{old}}$ represent the GP posterior variance and covariance, respectively, based on the current training set $X_T$. These quantities are calculated using the precomputed matrix $M$.\n\n    b. **Diversity, $D_k(\\mathbf{x})$**: This objective quantifies how dissimilar a candidate $\\mathbf{x}$ is from the existing training points in the kernel-induced feature space. It is defined as the square root of the Maximum Mean Discrepancy (MMD) between a singleton measure at $\\mathbf{x}$ and the empirical measure of the training set $X_T$:\n    $$\n    D_k(\\mathbf{x}) = \\sqrt{k(\\mathbf{x},\\mathbf{x}) - \\frac{2}{n_T} \\sum_{i=1}^{n_T} k(\\mathbf{x},\\mathbf{x}_i) + \\frac{1}{n_T^2} \\sum_{i,j=1}^{n_T} k(\\mathbf{x}_i,\\mathbf{x}_j)}\n    $$\n    The term $k(\\mathbf{x},\\mathbf{x})$ equals the kernel variance $\\sigma_k^2$. The double summation term is constant for all candidates and represents the mean of the training kernel matrix $K_{TT}$.\n\n3.  **Pareto Front Identification**: After computing the objective vector $(\\Delta \\mathrm{RMSE}_F(\\mathbf{x}), D_k(\\mathbf{x}))$ for every candidate, we identify the Pareto front. A candidate is on the Pareto front if it is not Pareto-dominated by any other candidate. Candidate $\\mathbf{x}_a$ dominates $\\mathbf{x}_b$ if $\\Delta \\mathrm{RMSE}_F(\\mathbf{x}_a) \\ge \\Delta \\mathrm{RMSE}_F(\\mathbf{x}_b)$ and $D_k(\\mathbf{x}_a) \\ge D_k(\\mathbf{x}_b)$, with at least one inequality being strict. The indices of all non-dominated candidates are collected and sorted in ascending order.\n\n4.  **$\\epsilon$-Constraint Selection**: For each given diversity threshold $\\epsilon$, a single candidate is selected. First, we form a feasible set of candidates where $D_k(\\mathbf{x}) \\ge \\epsilon$. If this set is empty, no selection is possible, and the result is $-1$. Otherwise, from this feasible set, we select the candidate that maximizes $\\Delta \\mathrm{RMSE}_F(\\mathbf{x})$. Ties are broken by selecting the candidate with the higher $D_k(\\mathbf{x})$. If a tie still persists, the candidate with the smallest zero-based index is chosen.\n\nThis structured procedure is implemented for each test case to produce the required results.", "answer": "```python\nimport numpy as np\nfrom scipy.spatial.distance import cdist\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final result.\n    \"\"\"\n    test_cases = [\n        {\n            \"kernel_params\": {\"sigma_k_sq\": 1.0, \"l\": 0.5, \"sigma_n_sq\": 1e-6},\n            \"X_T\": np.array([[-1.0], [1.0]]),\n            \"X_P\": np.array([[-0.8], [-0.2], [0.0], [0.5], [1.2]]),\n            \"X_V\": np.linspace(-1.5, 1.5, 31).reshape(-1, 1),\n            \"epsilons\": [0.02, 0.06, 0.12],\n        },\n        {\n            \"kernel_params\": {\"sigma_k_sq\": 0.8, \"l\": 0.7, \"sigma_n_sq\": 0.01},\n            \"X_T\": np.array([[0.0, 0.0], [1.0, 0.0], [0.0, 1.0]]),\n            \"X_P\": np.array([[0.5, 0.5], [1.0, 1.0], [-0.5, 0.5], [0.2, -0.8], [0.8, 0.2], [1.5, -0.5]]),\n            \"X_V\": np.array(np.meshgrid([-0.5, 0.0, 0.5, 1.0, 1.5], [-0.5, 0.0, 0.5, 1.0, 1.5])).T.reshape(-1, 2),\n            \"epsilons\": [0.05, 0.12, 0.20],\n        },\n        {\n            \"kernel_params\": {\"sigma_k_sq\": 0.5, \"l\": 2.0, \"sigma_n_sq\": 1e-6},\n            \"X_T\": np.array([[0.0]]),\n            \"X_P\": np.array([[0.0], [0.1], [0.2]]),\n            \"X_V\": np.linspace(0.0, 0.2, 11).reshape(-1, 1),\n            \"epsilons\": [0.20, 0.40],\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        result = process_case(\n            case[\"kernel_params\"],\n            case[\"X_T\"],\n            case[\"X_P\"],\n            case[\"X_V\"],\n            case[\"epsilons\"]\n        )\n        all_results.append(result)\n\n    # Format output string without spaces as per requirement\n    output_str = str(all_results).replace(\" \", \"\")\n    print(output_str)\n\ndef kernel(X1, X2, sigma_k_sq, l_sq):\n    \"\"\"\n    Computes the squared-exponential kernel matrix between two sets of vectors.\n    \"\"\"\n    sq_dists = cdist(X1, X2, 'sqeuclidean')\n    return sigma_k_sq * np.exp(-sq_dists / (2 * l_sq))\n\ndef process_case(kernel_params, X_T, X_P, X_V, epsilons):\n    \"\"\"\n    Processes a single test case to find the Pareto front and epsilon-constraint selections.\n    \"\"\"\n    sigma_k_sq = kernel_params[\"sigma_k_sq\"]\n    l = kernel_params[\"l\"]\n    sigma_n_sq = kernel_params[\"sigma_n_sq\"]\n    l_sq = l**2\n    \n    n_T = X_T.shape[0]\n    n_P = X_P.shape[0]\n    n_V = X_V.shape[0]\n\n    # Pre-computation\n    K_TT = kernel(X_T, X_T, sigma_k_sq, l_sq)\n    K_TT_inv_reg = np.linalg.inv(K_TT + sigma_n_sq * np.eye(n_T))\n    \n    # --- Objective 1: Delta RMSE ---\n    # Calculate old RMSE (constant for all candidates)\n    K_TV = kernel(X_T, X_V, sigma_k_sq, l_sq)\n    v_old_at_V = sigma_k_sq - np.sum((K_TT_inv_reg @ K_TV) * K_TV, axis=0)\n    rmse_old = np.sqrt(np.mean(v_old_at_V))\n\n    delta_rmses = []\n    for i in range(n_P):\n        x_p = X_P[i:i+1] # Keep it 2D\n        k_Tx = kernel(X_T, x_p, sigma_k_sq, l_sq)\n        k_Vx = kernel(X_V, x_p, sigma_k_sq, l_sq)\n        \n        v_old_at_x = sigma_k_sq - k_Tx.T @ K_TT_inv_reg @ k_Tx\n        \n        c_old_at_V_for_x = k_Vx - (K_TV.T @ K_TT_inv_reg @ k_Tx)\n        \n        var_update_term = (c_old_at_V_for_x**2) / (v_old_at_x.item() + sigma_n_sq)\n        v_new_at_V = v_old_at_V.reshape(-1, 1) - var_update_term\n        \n        rmse_new_for_x = np.sqrt(np.mean(v_new_at_V))\n        delta_rmses.append(rmse_old - rmse_new_for_x)\n\n    # --- Objective 2: Diversity ---\n    d_ks = []\n    const_term_d = np.mean(K_TT)\n    for i in range(n_P):\n        x_p = X_P[i:i+1]\n        k_pT = kernel(x_p, X_T, sigma_k_sq, l_sq)\n        term2_d = 2.0 * np.mean(k_pT)\n        d_k_sq = sigma_k_sq - term2_d + const_term_d\n        d_ks.append(np.sqrt(np.maximum(0, d_k_sq)))\n\n    objectives = list(zip(delta_rmses, d_ks))\n\n    # --- Pareto Front Identification ---\n    pareto_indices = []\n    for i in range(n_P):\n        is_dominated = False\n        for j in range(n_P):\n            if i == j:\n                continue\n            # Check if j dominates i\n            if (objectives[j][0] >= objectives[i][0] and objectives[j][1] >= objectives[i][1]) and \\\n               (objectives[j][0] > objectives[i][0] or objectives[j][1] > objectives[i][1]):\n                is_dominated = True\n                break\n        if not is_dominated:\n            pareto_indices.append(i)\n    pareto_indices.sort()\n\n    # --- Epsilon-Constraint Selection ---\n    selected_indices = []\n    for eps in epsilons:\n        feasible_candidates = []\n        for i in range(n_P):\n            if d_ks[i] >= eps:\n                feasible_candidates.append({'delta': delta_rmses[i], 'd': d_ks[i], 'idx': i})\n\n        if not feasible_candidates:\n            selected_indices.append(-1)\n        else:\n            # Sort by delta (desc), d (desc), then index (asc)\n            feasible_candidates.sort(key=lambda c: (c['delta'], c['d'], -c['idx']), reverse=True)\n            selected_indices.append(feasible_candidates[0]['idx'])\n            \n    return [pareto_indices, selected_indices]\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3394183"}]}