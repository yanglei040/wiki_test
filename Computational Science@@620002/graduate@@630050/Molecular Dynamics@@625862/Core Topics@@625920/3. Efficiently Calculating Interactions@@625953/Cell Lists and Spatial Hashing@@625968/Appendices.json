{"hands_on_practices": [{"introduction": "Before any optimization, an algorithm's primary requirement is correctness. This practice lays the essential groundwork by tasking you with building a validation suite for a cell list algorithm in a standard cubic domain [@problem_id:3400627]. By comparing the neighbor pairs generated by your cell list against those from an exhaustive, brute-force search, you will develop a \"gold standard\" for verifying your implementation's completeness and correctness across a range of challenging scenarios.", "problem": "You are asked to write a complete, runnable program that validates the completeness of a neighbor search based on a uniform cell list (spatial hashing) by comparing it to a high-precision brute-force reference. The context is Molecular Dynamics (MD), where pairwise interactions depend on the Euclidean distance between particles. The domain is a periodic hypercube of side length $L$ with Periodic Boundary Conditions (PBC), and the neighbor relation is defined by the Minimum Image Convention (MIC). The program must work for dimension $d \\in \\{2,3\\}$.\n\nFundamental base and definitions:\n- Newtonian particle systems interact pairwise via distances; thus identifying all particle pairs within a cutoff radius $r_c$ is a prerequisite to compute forces consistently, from Newton's laws of motion and the definition of pairwise interactions.\n- The Euclidean norm in $\\mathbb{R}^d$ defines distances. Under PBC, the MIC defines the displacement between two positions $\\mathbf{x}_i$ and $\\mathbf{x}_j$ as $\\mathbf{r}_{ij} = \\mathbf{x}_j - \\mathbf{x}_i - L \\,\\mathrm{round}\\!\\left((\\mathbf{x}_j - \\mathbf{x}_i)/L\\right)$, where division and rounding are applied component-wise.\n- A cell list discretizes the domain into a uniform grid of cells with edge length $h$, hashing each particle to a cell by integer division, and then only inspecting particles in neighboring cells sufficient to guarantee that any two particles within $r_c$ are considered.\n\nYour program must, for each test case, do the following:\n1. Generate the particle positions either uniformly at random in $[0,L)^d$ or by a prescribed construction, using the provided random seed where applicable.\n2. Compute the set $S_{\\mathrm{BF}}$ of all unordered index pairs $\\{i,j\\}$ with $i < j$ such that $\\lVert \\mathbf{r}_{ij} \\rVert \\le r_c$ using a brute-force check with double precision. Use the MIC as defined above, and include a small nonnegative tolerance to handle floating-point comparisons.\n3. Compute the set $S_{\\mathrm{CL}}$ of all unordered index pairs $\\{i,j\\}$ found by a correct cell-list neighbor search that uses a uniform grid with nominal cell edge $h$, Periodic Boundary Conditions, and a search over all offset cells that are required by Euclidean geometry to ensure that any pair with separation at most $r_c$ must be enumerated. The implementation must not assume $h \\ge r_c$ and must remain correct for any $h > 0$.\n4. Return a boolean for the test case indicating whether $S_{\\mathrm{CL}} = S_{\\mathrm{BF}}$.\n\nAssumptions and constraints:\n- Positions are in reduced, dimensionless units; you must report dimensionless results. No physical units are required.\n- The cutoff relation is \"within $r_c$\" interpreted as $\\lVert \\mathbf{r}_{ij} \\rVert \\le r_c$.\n- The box is a $d$-dimensional cube of side length $L$ with PBC along each dimension.\n- All indices are zero-based integers.\n\nTest suite:\nProvide results for the following six test cases. For each, you are given dimension $d$, number of particles $N$, side length $L$, cutoff $r_c$, nominal cell size $h$, a random seed if needed, and the configuration type.\n\n- Case A (happy path, three dimensions, $h = r_c$): $d=3$, $N=256$, $L=10.0$, $r_c=1.1$, $h=1.1$, seed $=1234$, configuration: uniform random in $[0,L)^d$.\n- Case B (non-integer grid, $h < r_c$, stress the neighbor offset radius): $d=3$, $N=1000$, $L=20.0$, $r_c=1.9$, $h=1.71$, seed $=7$, configuration: uniform random in $[0,L)^d$.\n- Case C (two dimensions, $h = r_c$): $d=2$, $N=400$, $L=10.0$, $r_c=2.5$, $h=2.5$, seed $=2023$, configuration: uniform random in $[0,L)^d$.\n- Case D (boundary concentration to stress PBC): $d=3$, $N=200$, $L=6.0$, $r_c=2.9$, $h=2.9$, seed $=99$, configuration: half of the particles uniformly in $[0,w)^d$ and half in $[L-w,L)^d$ with $w=0.3$.\n- Case E (tiny system with analytic expectation under PBC): $d=3$, $N=3$, $L=1.0$, $r_c=0.25$, $h=0.25$, configuration: explicit positions $\\mathbf{x}_0=(0.02,0.02,0.02)$, $\\mathbf{x}_1=(0.21,0.02,0.02)$, $\\mathbf{x}_2=(0.85,0.02,0.02)$.\n- Case F (dense, many neighbors, $h$ smaller than $r_c$): $d=3$, $N=500$, $L=5.0$, $r_c=1.2$, $h=0.6$, seed $=555$, configuration: uniform random in $[0,L)^d$.\n\nOutput specification:\n- For each case, compute a boolean indicating whether the neighbor pairs found by the cell list exactly match the brute-force neighbor pairs.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[true,false,true]\"). Use the Python boolean literals \"True\" or \"False\" without additional whitespace.\n\nDeliverables:\n- A single, complete, runnable program that performs all computations and prints the specified final line. No user input is needed. All computations must be self-contained and reproducible from the parameters above.", "solution": "The problem statement is assessed to be valid. It presents a well-defined task from the field of computational physics, specifically the validation of a cell-list algorithm for neighbor finding in a molecular dynamics simulation. The problem is scientifically grounded, logically consistent, and all parameters are specified, allowing for a unique and verifiable solution. A minor ambiguity regarding the exact method for discretizing the simulation box into cells given a \"nominal\" cell edge length $h$ is resolved by adopting a standard and reasonable implementation choice, which will be detailed below. Similarly, the note on floating-point tolerance is addressed by ensuring bit-wise identical arithmetic in both the reference and test algorithms, which is the most stringent validation method.\n\nThe solution is implemented by performing two independent calculations for each test case and comparing their results. The first is a brute-force ($S_{\\mathrm{BF}}$) calculation that serves as a high-precision reference. The second is the cell-list ($S_{\\mathrm{CL}}$) algorithm, which is the method under test. The validation is successful if and only if $S_{\\mathrm{CL}} = S_{\\mathrm{BF}}$.\n\n**1. Brute-Force Reference Calculation ($S_{\\mathrm{BF}}$)**\nThe set of true neighbor pairs, $S_{\\mathrm{BF}}$, is computed by examining every possible unique pair of particles $\\{i,j\\}$ with particle indices $i < j$. For each pair, the displacement vector $\\Delta\\mathbf{x} = \\mathbf{x}_j - \\mathbf{x}_i$ is calculated. To account for the periodic boundary conditions (PBC), this vector is corrected using the minimum image convention (MIC). The MIC displacement vector $\\mathbf{r}_{ij}$ is given by the formula specified in the problem:\n$$\n\\mathbf{r}_{ij} = \\Delta\\mathbf{x} - L \\cdot \\mathrm{round}(\\Delta\\mathbf{x} / L)\n$$\nwhere $L$ is the side length of the cubic domain and the operations are performed component-wise. The squared Euclidean distance is then $\\lVert \\mathbf{r}_{ij} \\rVert^2$. A pair $\\{i,j\\}$ is added to the set $S_{\\mathrm{BF}}$ if this distance satisfies the cutoff criterion:\n$$\n\\lVert \\mathbf{r}_{ij} \\rVert^2 \\le r_c^2\n$$\nwhere $r_c$ is the cutoff radius. This process is performed for all $\\frac{N(N-1)}{2}$ unique pairs of $N$ particles. The resulting set of pairs, stored as sorted tuples $(i,j)$ with $i<j$, constitutes the ground truth.\n\n**2. Cell-List Algorithm ($S_{\\mathrm{CL}}$)**\nThe cell-list algorithm accelerates the neighbor search by spatially sorting particles into a grid.\n\n**2.1. Grid Discretization**\nThe simulation domain, a hypercube of side length $L$, is divided into a uniform grid of smaller cells. Given the nominal cell edge length $h$, the number of cells along each dimension, $m_{\\text{dim}}$, is determined. We make the reasonable assumption that the grid should be as close as possible to the nominal size while fitting the domain perfectly. This is achieved by setting $m_{\\text{dim}} = \\max(1, \\mathrm{round}(L/h))$. The actual cell edge length is then $h_{\\text{actual}} = L / m_{\\text{dim}}$. This ensures the entire domain is covered by a uniform grid.\n\n**2.2. Particle Hashing**\nEach particle is assigned to a cell based on its position $\\mathbf{x} = (x_1, \\dots, x_d)$. The integer index vector of the cell is calculated as:\n$$\n\\mathbf{c} = \\lfloor \\mathbf{x} / h_{\\text{actual}} \\rfloor\n$$\nwhere the floor operation is component-wise. A hash map (dictionary in Python) is used to store lists of particle indices for each cell index tuple $\\mathbf{c}$.\n\n**2.3. Neighbor Search**\nThe core principle is that two particles can only be neighbors if their respective cells are sufficiently close. The maximum possible distance between particles requires us to check not just adjacent cells, but a larger stencil of cells, particularly when $h_{\\text{actual}} < r_c$. The search radius in terms of cells, $s_{\\text{rad}}$, is determined by:\n$$\ns_{\\text{rad}} = \\lceil r_c / h_{\\text{actual}} \\rceil\n$$\nFor each particle $i$, we must check for neighbors in a hypercubic block of $(2s_{\\text{rad}}+1)^d$ cells centered on the home cell of particle $i$.\n\nTo ensure each pair $\\{i,j\\}$ is checked exactly once, we iterate through all particles $i$ from $0$ to $N-1$. For each particle $i$, we identify its home cell and iterate through the $(2s_{\\text{rad}}+1)^d$ cell stencil (including the home cell itself). Periodic boundary conditions are applied to the cell indices using the modulo operator. For each particle $j$ found within this stencil of cells, we enforce the condition $j > i$ before proceeding. This simple condition correctly avoids both self-pairing ($i=j$) and double-counting pairs. For each valid candidate pair $(i,j)$, the MIC distance is computed and checked against $r_c$, identical to the brute-force method. If the criterion is met, the pair $(i,j)$ is added to the set $S_{\\mathrm{CL}}$.\n\n**3. Validation**\nFor each test case, the program computes both sets $S_{\\mathrm{BF}}$ and $S_{\\mathrm{CL}}$. The final result for the test case is the boolean value of the expression $S_{\\mathrm{CL}} == S_{\\mathrm{BF}}$. The implementation for all test cases is designed to be general for any valid dimension $d$, including the specified $d \\in \\{2,3\\}$. The provided test cases correctly probe various aspects of the algorithm, such as behavior when $h<r_c$ (Case B, F), two-dimensional systems (Case C), and systems designed to stress the periodic boundary conditions (Case D, E).", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport math\nimport itertools\nfrom collections import defaultdict\n\ndef run_case(d, N, L, r_c, h, seed, config_type, config_params=None):\n    \"\"\"\n    Runs a single test case, comparing a brute-force neighbor search\n    with a cell-list-based search. Returns True if the results match.\n    \n    Args:\n        d (int): Dimension of the system.\n        N (int): Number of particles.\n        L (float): Side length of the periodic box.\n        r_c (float): Cutoff radius for neighbor search.\n        h (float): Nominal cell edge length.\n        seed (int or None): Random seed for particle generation.\n        config_type (str): Type of particle configuration ('uniform', 'boundary', 'explicit').\n        config_params (dict or None): Additional parameters for configuration.\n        \n    Returns:\n        bool: True if the set of pairs from cell list matches brute force, False otherwise.\n    \"\"\"\n    \n    # 1. Generate Particle Positions\n    if seed is not None:\n        rng = np.random.default_rng(seed)\n    \n    if config_type == \"uniform\":\n        positions = rng.uniform(0.0, L, size=(N, d))\n    elif config_type == \"boundary\":\n        w = config_params['w']\n        N_half = N // 2\n        pos1 = rng.uniform(0.0, w, size=(N_half, d))\n        pos2 = rng.uniform(L - w, L, size=(N - N_half, d))\n        positions = np.vstack((pos1, pos2))\n    elif config_type == \"explicit\":\n        positions = np.array(config_params['positions'], dtype=np.float64)\n    else:\n        raise ValueError(f\"Unknown configuration type: {config_type}\")\n\n    r_c_sq = r_c * r_c\n\n    # 2. Brute-Force Reference Calculation (S_BF)\n    s_bf = set()\n    for i in range(N):\n        for j in range(i + 1, N):\n            dr = positions[j] - positions[i]\n            # Apply Minimum Image Convention (MIC)\n            dr_mic = dr - L * np.round(dr / L)\n            dist_sq = np.sum(dr_mic**2)\n            \n            # The problem mentions a tolerance, but for validation of two computational\n            # methods on the same machine, bit-wise identical comparisons are the most\n            # stringent test. Both methods use the same floating-point arithmetic.\n            if dist_sq <= r_c_sq:\n                s_bf.add((i, j))\n\n    # 3. Cell List Algorithm (S_CL)\n    s_cl = set()\n\n    # 3.1. Grid Discretization\n    m_per_dim = max(1, int(round(L / h)))\n    h_actual = L / m_per_dim\n    \n    # 3.2. Particle Hashing\n    cell_list = defaultdict(list)\n    for i in range(N):\n        # Particle coordinates are in [0, L), so floor(pos/h_actual) gives indices in [0, m_per_dim-1].\n        cell_idx = tuple(np.floor(positions[i] / h_actual).astype(int))\n        cell_list[cell_idx].append(i)\n\n    # 3.3. Neighbor Search\n    s_rad = math.ceil(r_c / h_actual)\n    offsets = list(itertools.product(range(-s_rad, s_rad + 1), repeat=d))\n    \n    for i in range(N):\n        pos_i = positions[i]\n        home_cell_idx = np.floor(pos_i / h_actual).astype(int)\n        \n        for offset in offsets:\n            np_offset = np.array(offset)\n            neighbor_cell_idx = tuple((home_cell_idx + np_offset) % m_per_dim)\n            \n            if neighbor_cell_idx in cell_list:\n                for j in cell_list[neighbor_cell_idx]:\n                    # Enforce j > i to check each pair only once and avoid self-pairs.\n                    if j <= i:\n                        continue\n                    \n                    pos_j = positions[j]\n                    dr = pos_j - pos_i\n                    dr_mic = dr - L * np.round(dr / L)\n                    dist_sq = np.sum(dr_mic**2)\n                    \n                    if dist_sq <= r_c_sq:\n                        s_cl.add((i, j))\n    \n    # 4. Validation: Compare the two sets of pairs\n    return s_bf == s_cl\n\ndef solve():\n    \"\"\"\n    Defines and runs the test suite, then prints the results.\n    \"\"\"\n    test_cases = [\n        # Case A: happy path, 3D, h = r_c\n        {'d': 3, 'N': 256, 'L': 10.0, 'r_c': 1.1, 'h': 1.1, 'seed': 1234, 'config_type': 'uniform', 'config_params': None},\n        # Case B: h < r_c, stress neighbor offset radius\n        {'d': 3, 'N': 1000, 'L': 20.0, 'r_c': 1.9, 'h': 1.71, 'seed': 7, 'config_type': 'uniform', 'config_params': None},\n        # Case C: 2D, h = r_c\n        {'d': 2, 'N': 400, 'L': 10.0, 'r_c': 2.5, 'h': 2.5, 'seed': 2023, 'config_type': 'uniform', 'config_params': None},\n        # Case D: boundary concentration to stress PBC\n        {'d': 3, 'N': 200, 'L': 6.0, 'r_c': 2.9, 'h': 2.9, 'seed': 99, 'config_type': 'boundary', 'config_params': {'w': 0.3}},\n        # Case E: tiny system with analytic expectation\n        {'d': 3, 'N': 3, 'L': 1.0, 'r_c': 0.25, 'h': 0.25, 'seed': None, 'config_type': 'explicit', 'config_params': {'positions': [[0.02, 0.02, 0.02], [0.21, 0.02, 0.02], [0.85, 0.02, 0.02]]}},\n        # Case F: dense, many neighbors, h << r_c\n        {'d': 3, 'N': 500, 'L': 5.0, 'r_c': 1.2, 'h': 0.6, 'seed': 555, 'config_type': 'uniform', 'config_params': None},\n    ]\n\n    results = []\n    for case in test_cases:\n        result = run_case(**case)\n        results.append(str(result))\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3400627"}, {"introduction": "Many important physical systems, such as crystals or materials under shear stress, are modeled using non-orthorhombic (triclinic) periodic cells. This practice generalizes the spatial hashing concept to these arbitrary geometries, which are described by a lattice matrix $H$ [@problem_id:3400609]. You will implement the coordinate transformation from Cartesian to fractional coordinates and confront the critical issue of numerical stability, learning how ill-conditioned cell shapes can amplify floating-point errors.", "problem": "Consider a periodic Molecular Dynamics domain represented by a lattice matrix $H \\in \\mathbb{R}^{3 \\times 3}$, where the Cartesian position vector $r \\in \\mathbb{R}^{3}$ is related to the fractional coordinates $s \\in \\mathbb{R}^{3}$ by the linear relation $r = H s$. The periodic boundary condition is enforced by identifying positions that differ by an integer combination of lattice vectors, that is, $s$ and $s + k$ represent equivalent points for any integer vector $k \\in \\mathbb{Z}^{3}$. A common requirement in constructing cell lists and spatial hashing is to wrap fractional coordinates back into the canonical unit cell using modulo arithmetic, and then map wrapped fractional coordinates to discrete cell indices.\n\nStarting from the foundational definitions of periodic boundaries and linear coordinate transforms, implement the following:\n\n1. Given $H$ and a set of Cartesian positions $\\{r_i\\}$, compute the fractional coordinates $s_i$ by solving $H s_i = r_i$ using a numerically stable linear solver that does not explicitly form $H^{-1}$.\n2. Wrap each component of $s_i$ into the half-open interval $[0, 1)$ using modulo-$1$ arithmetic to obtain $s_i^{\\mathrm{wrap}}$. Use a wrapping rule that is robust to floating-point roundoff near $0$ and $1$.\n3. Given a vector of per-dimension cell counts $m = (m_x, m_y, m_z) \\in \\mathbb{N}^3$, map each wrapped fractional coordinate $s_i^{\\mathrm{wrap}}$ to integer cell indices $(c_x, c_y, c_z)$ via uniform binning of $[0, 1)$ into $m_d$ bins along dimension $d \\in \\{x, y, z\\}$. Then compute a spatial hash $h_i \\in \\mathbb{N}$ by a collision-free linearization $h_i = c_x \\cdot m_y \\cdot m_z + c_y \\cdot m_z + c_z$.\n4. Quantify numerical stability by computing the spectral condition number $\\kappa_2(H)$ and the amplification factor $A(H, r_0, \\delta)$, where $A(H, r_0, \\delta) = \\frac{\\lVert s(r_0 + \\delta) - s(r_0) \\rVert_2}{\\lVert \\delta \\rVert_2}$ and $s(\\cdot)$ denotes the solution of $H s = r$. Discuss the expected relationship between $A$ and $\\kappa_2(H)$.\n5. Verify that the wrapping operation is consistent with the periodic identification by computing, for each test case, the maximum absolute deviation from integer of $k_i = s_i - s_i^{\\mathrm{wrap}}$ across all positions and dimensions, namely $\\max_{i,d} \\left| k_{i,d} - \\mathrm{round}(k_{i,d}) \\right|$.\n\nPhysical units: Express all entries of $H$ and $r_i$ in nanometers (nm). Angles do not appear in this problem. The program output is unitless because it comprises dimensionless indices, condition numbers, and ratios.\n\nTest suite: Use the following $3$ test cases. For each case, compute the quantities described above.\n\n- Case $1$ (orthorhombic box):\n  - $H = \\mathrm{diag}(3.0, 2.0, 1.5)$ (nm).\n  - Positions $r_1 = (3.1, -0.1, 1.6)$ (nm), $r_2 = (-0.01, 0.99, -1.5)$ (nm), $r_3 = (2.9999999999, 1.9999999999, 1.4999999999)$ (nm).\n  - Cells $m = (4, 5, 6)$.\n  - Perturbation for amplification: $\\delta = (10^{-9}, -2 \\cdot 10^{-9}, 10^{-9})$ (nm), applied to $r_1$.\n\n- Case $2$ (moderately skewed triclinic box):\n  - $H = \\begin{pmatrix} 2.0 & 0.5 & 0.2 \\\\ 0.0 & 2.5 & 0.3 \\\\ 0.0 & 0.0 & 3.0 \\end{pmatrix}$ (nm).\n  - Positions $r_1 = (2.3, 1.2, 3.1)$ (nm), $r_2 = (-0.2, 2.6, -0.1)$ (nm), $r_3 = (4.1, 0.0, 0.0)$ (nm).\n  - Cells $m = (5, 4, 3)$.\n  - Perturbation for amplification: $\\delta = (10^{-9}, -2 \\cdot 10^{-9}, 10^{-9})$ (nm), applied to $r_1$.\n\n- Case $3$ (highly skewed and nearly singular box):\n  - $H = \\begin{pmatrix} 1.0 & 0.999999 & 0.0 \\\\ 0.0 & 10^{-6} & 0.0 \\\\ 0.0 & 0.0 & 2.0 \\end{pmatrix}$ (nm).\n  - Positions $r_1 = (1.0, 0.0, 0.0)$ (nm), $r_2 = (10^{-6}, 10^{-6}, 0.0)$ (nm), $r_3 = (0.0, 0.0, 0.1)$ (nm).\n  - Cells $m = (10, 10, 2)$.\n  - Perturbation for amplification: $\\delta = (10^{-9}, -2 \\cdot 10^{-9}, 10^{-9})$ (nm), applied to $r_1$.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element corresponds to one test case and is itself a list with four elements:\n- the condition number $\\kappa_2(H)$ as a float,\n- the amplification factor $A(H, r_0, \\delta)$ as a float,\n- the maximum absolute deviation from integer for $k_i = s_i - s_i^{\\mathrm{wrap}}$ as a float,\n- the list of spatial hash integers $[h_1, h_2, \\dots]$ for the provided positions.\n\nFor example, the structure of the output must be $[[\\kappa_2, A, \\mathrm{dev}, [h_1,h_2,h_3]], [\\kappa_2, A, \\mathrm{dev}, [h_1,h_2,h_3]], [\\kappa_2, A, \\mathrm{dev}, [h_1,h_2,h_3]]]$ for the three cases.", "solution": "The problem statement has been validated and is deemed sound. It is a well-posed, scientifically grounded problem in the domain of computational physics and numerical methods, with all necessary data and definitions provided.\n\nThe solution proceeds by implementing the five specified tasks for each test case. The underlying principles and implementation details for each task are outlined below.\n\n**1. Computation of Fractional Coordinates ($s_i$)**\n\nThe relationship between a Cartesian position vector $r$ and its corresponding fractional coordinate vector $s$ is given by the linear system $r = H s$, where $H$ is the $3 \\times 3$ lattice matrix. To find the fractional coordinates for a given set of Cartesian positions $\\{r_i\\}$, we must solve the system of linear equations $H s_i = r_i$ for each $s_i$.\n\nThe problem requires a numerically stable solver that does not explicitly compute the matrix inverse $H^{-1}$. The use of explicit inverses is generally discouraged in numerical linear algebra as it can be less accurate and computationally more expensive than direct solution methods. Standard numerical libraries provide solvers based on matrix decompositions (e.g., LU decomposition with partial pivoting) that satisfy these requirements. We will use `numpy.linalg.solve`, which implements such a stable algorithm. For a set of $N$ position vectors, we can assemble the vectors into a matrix and solve the system efficiently. If the Cartesian coordinates are stored as rows in a matrix $R \\in \\mathbb{R}^{N \\times 3}$, we solve for the fractional coordinates $S \\in \\mathbb{R}^{N \\times 3}$ by solving the system $H S^T = R^T$.\n\n**2. Wrapping Fractional Coordinates ($s_i^{\\mathrm{wrap}}$)**\n\nPeriodic boundary conditions in molecular simulations mean that a particle leaving the simulation box on one side re-enters from the opposite side. This is handled mathematically by identifying fractional coordinates $s$ and $s+k$ for any integer vector $k \\in \\mathbb{Z}^3$. A canonical representation is typically chosen by mapping all fractional coordinates into a unit cell, such as $[0, 1) \\times [0, 1) \\times [0, 1)$.\n\nThis wrapping operation is performed using modulo-$1$ arithmetic on each component of the fractional coordinate vector $s_i$. A numerically robust method to map a value $x$ to the half-open interval $[0, 1)$ is given by the formula $x^{\\mathrm{wrap}} = x - \\lfloor x \\rfloor$. This approach correctly handles positive, negative, and zero-valued coordinates and avoids floating-point issues near integer boundaries. For example, if $s_d = 2.1$, $s_d^{\\mathrm{wrap}} = 2.1 - 2.0 = 0.1$. If $s_d = -0.2$, $s_d^{\\mathrm{wrap}} = -0.2 - (-1.0) = 0.8$. If $s_d = 3.0$, $s_d^{\\mathrm{wrap}} = 3.0 - 3.0 = 0.0$.\n\n**3. Cell Indexing and Spatial Hashing ($c_i$, $h_i$)**\n\nTo accelerate the search for neighboring particles, the simulation domain is often partitioned into a grid of smaller cells. A particle's cell is determined by its position. Given the wrapped fractional coordinates $s_i^{\\mathrm{wrap}} \\in [0, 1)^3$ and a grid of $m = (m_x, m_y, m_z)$ cells, we can map $s_i^{\\mathrm{wrap}}$ to an integer cell index vector $c_i = (c_x, c_y, c_z)$ via uniform binning. The index $c_d$ for dimension $d \\in \\{x, y, z\\}$ is calculated as:\n$$c_d = \\lfloor s_{i,d}^{\\mathrm{wrap}} \\cdot m_d \\rfloor$$\nSince $s_{i,d}^{\\mathrm{wrap}} \\in [0, 1)$, the product $s_{i,d}^{\\mathrm{wrap}} \\cdot m_d$ is in $[0, m_d)$, and the floor function correctly maps it to one of the $m_d$ integer indices $\\{0, 1, \\dots, m_d - 1\\}$.\n\nThe $3$D cell index $c_i$ is then linearized into a single integer hash value $h_i$. The provided formula performs a row-major mapping:\n$$h_i = c_x \\cdot (m_y \\cdot m_z) + c_y \\cdot m_z + c_z$$\nThis mapping is unique and collision-free as long as the cell indices $(c_x, c_y, c_z)$ are within their valid ranges, i.e., $c_d \\in [0, m_d-1]$.\n\n**4. Numerical Stability Analysis ($\\kappa_2(H)$, $A$)**\n\nThe stability of the solution to $H s = r$ with respect to perturbations in $r$ is a critical concern, especially for highly skewed (non-orthorhombic) simulation cells.\n\nThe spectral condition number, $\\kappa_2(H) = \\lVert H \\rVert_2 \\lVert H^{-1} \\rVert_2$, provides a general measure of this sensitivity. A large $\\kappa_2(H)$ indicates that the matrix $H$ is close to singular (ill-conditioned), and small relative errors in the input $r$ can lead to large relative errors in the output $s$. We compute this using `numpy.linalg.cond`.\n\nThe amplification factor, $A(H, r_0, \\delta) = \\frac{\\lVert s(r_0 + \\delta) - s(r_0) \\rVert_2}{\\lVert \\delta \\rVert_2}$, quantifies the amplification for a specific perturbation $\\delta$ applied to a specific position $r_0$. Let $\\Delta r = \\delta$ and $\\Delta s = s(r_0 + \\delta) - s(r_0)$. From the linearity of the system, $H(s_0 + \\Delta s) = r_0 + \\Delta r$, which implies $H \\Delta s = \\Delta r$. Thus, $\\Delta s = H^{-1} \\Delta r$. The amplification factor is then:\n$$A = \\frac{\\lVert H^{-1} \\delta \\rVert_2}{\\lVert \\delta \\rVert_2}$$\nFrom the definition of the induced matrix $2$-norm, $\\lVert H^{-1} \\rVert_2 = \\sup_{\\delta \\neq 0} \\frac{\\lVert H^{-1} \\delta \\rVert_2}{\\lVert \\delta \\rVert_2}$. Therefore, the amplification factor $A$ is bounded by the norm of the inverse matrix: $A \\le \\lVert H^{-1} \\rVert_2$. The maximum possible amplification is achieved when $\\delta$ is aligned with the right singular vector of $H$ corresponding to its smallest singular value.\nThe expected relationship is that a large condition number $\\kappa_2(H)$ implies a large $\\lVert H^{-1} \\rVert_2$ (since $\\lVert H^{-1} \\rVert_2 = \\kappa_2(H) / \\lVert H \\rVert_2$), which in turn implies a high potential for amplification $A$. The test cases, particularly Case 3, are designed to demonstrate this relationship: an ill-conditioned matrix leads to a large amplification factor for a well-chosen perturbation.\n\n**5. Verification of Wrapping Consistency**\n\nThis task verifies that the integer vector $k_i = s_i - s_i^{\\mathrm{wrap}}$ is indeed integer-valued, as it should be from its definition. We define $k_i = s_i - s_i^{\\mathrm{wrap}}$. Substituting the definition of the wrapping operation, $s_i^{\\mathrm{wrap}} = s_i - \\lfloor s_i \\rfloor$, we find that $k_i = s_i - (s_i - \\lfloor s_i \\rfloor) = \\lfloor s_i \\rfloor$. In exact arithmetic, the components of $k_i$ are integers.\n\nIn floating-point arithmetic, the computed value of $\\lfloor s_i \\rfloor$ may have a minute representational error. The quantity $\\max_{i,d} \\left| k_{i,d} - \\mathrm{round}(k_{i,d}) \\right|$ measures the maximum deviation of the computed components of $k_i$ from their nearest integers. For a numerically sound implementation, this value should be on the order of machine epsilon, confirming the integrity of the wrapping procedure.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem for all test cases, performing coordinate transformation,\n    wrapping, hashing, and numerical stability analysis.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"H\": np.diag([3.0, 2.0, 1.5]),\n            \"r\": np.array([\n                [3.1, -0.1, 1.6],\n                [-0.01, 0.99, -1.5],\n                [2.9999999999, 1.9999999999, 1.4999999999]\n            ]),\n            \"m\": np.array([4, 5, 6]),\n            \"delta\": np.array([1e-9, -2e-9, 1e-9]),\n            \"r0_idx\": 0  # Apply perturbation to r1\n        },\n        {\n            \"H\": np.array([\n                [2.0, 0.5, 0.2],\n                [0.0, 2.5, 0.3],\n                [0.0, 0.0, 3.0]\n            ]),\n            \"r\": np.array([\n                [2.3, 1.2, 3.1],\n                [-0.2, 2.6, -0.1],\n                [4.1, 0.0, 0.0]\n            ]),\n            \"m\": np.array([5, 4, 3]),\n            \"delta\": np.array([1e-9, -2e-9, 1e-9]),\n            \"r0_idx\": 0  # Apply perturbation to r1\n        },\n        {\n            \"H\": np.array([\n                [1.0, 0.999999, 0.0],\n                [0.0, 1e-6, 0.0],\n                [0.0, 0.0, 2.0]\n            ]),\n            \"r\": np.array([\n                [1.0, 0.0, 0.0],\n                [1e-6, 1e-6, 0.0],\n                [0.0, 0.0, 0.1]\n            ]),\n            \"m\": np.array([10, 10, 2]),\n            \"delta\": np.array([1e-9, -2e-9, 1e-9]),\n            \"r0_idx\": 0  # Apply perturbation to r1\n        }\n    ]\n\n    all_results = []\n\n    for case in test_cases:\n        H = case[\"H\"]\n        r_cartesian = case[\"r\"]\n        m = case[\"m\"]\n        delta = case[\"delta\"]\n        r0 = r_cartesian[case[\"r0_idx\"]]\n\n        # Task 1: Compute fractional coordinates s_i\n        # We solve H * s_T = r_T, where T denotes transpose.\n        # r_cartesian is (N, 3), so r_cartesian.T is (3, N).\n        # s_T will be (3, N), so s is (N, 3).\n        s_T = np.linalg.solve(H, r_cartesian.T)\n        s = s_T.T\n\n        # Task 2: Wrap fractional coordinates into [0, 1)\n        s_wrap = s - np.floor(s)\n\n        # Task 3: Map to cell indices and compute spatial hash\n        m_x, m_y, m_z = m\n        # Broadcasting s_wrap (N, 3) with m (3,) automatically works element-wise\n        cell_indices = np.floor(s_wrap * m).astype(np.int64)\n        c_x = cell_indices[:, 0]\n        c_y = cell_indices[:, 1]\n        c_z = cell_indices[:, 2]\n        hashes = (c_x * m_y * m_z + c_y * m_z + c_z).tolist()\n\n        # Task 4: Quantify numerical stability\n        # Condition number\n        kappa_2 = np.linalg.cond(H, 2)\n        \n        # Amplification factor\n        s0 = np.linalg.solve(H, r0)\n        s_pert = np.linalg.solve(H, r0 + delta)\n        delta_s = s_pert - s0\n        \n        norm_delta_s_2 = np.linalg.norm(delta_s, 2)\n        norm_delta_2 = np.linalg.norm(delta, 2)\n        \n        amplification_A = norm_delta_s_2 / norm_delta_2 if norm_delta_2 > 0 else 0.0\n\n        # Task 5: Verify wrapping consistency\n        k = s - s_wrap  # By definition, this is floor(s)\n        # The deviation from the nearest integer should be near machine epsilon\n        deviation = np.max(np.abs(k - np.round(k)))\n\n        # Compile results for this case\n        case_results = [kappa_2, amplification_A, deviation, hashes]\n        all_results.append(case_results)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```", "id": "3400609"}, {"introduction": "With a correct and general algorithm, the final frontier is performance, especially on massively parallel architectures like GPUs. This exercise explores the practical trade-offs between two canonical parallel strategies for constructing cell lists: direct, atomic-based insertion versus a sort-based compaction approach [@problem_id:3400614]. By implementing and evaluating a performance model, you will gain insight into how algorithmic choices interact with hardware characteristics and data distribution to determine real-world efficiency.", "problem": "You are given a three-dimensional periodic simulation box of side length $L=1$ (dimensionless) populated by $N$ point particles at positions $\\mathbf{r}_i \\in [0,1)^3$ for $i \\in \\{0,\\dots,N-1\\}$. A spatial cell list partitions the domain into a uniform grid of $n_x \\times n_y \\times n_z$ cells with cubic edge length $h$, where $h$ is chosen so that $L/h$ is an integer. Particles are assigned to cells by computing the integer triple $(i_x,i_y,i_z)$ for each particle according to $i_\\alpha = \\lfloor r_{i,\\alpha}/h \\rfloor$ with periodic wrap, and a linear cell identifier $c = i_x + n_x (i_y + n_y i_z)$. Two standard constructions of the per-cell bins are to be implemented and compared:\n\n- Atomic insertion into fixed-capacity bins: preallocate a per-cell array of capacity $C$ and use a per-cell counter to perform atomic insertions of particle indices into cell bins. If the counter exceeds $C-1$, record an overflow. This models push-based, counter-driven binning.\n\n- Sort-based bin compaction: compute the cell identifier $c$ for each particle, sort particle indices by $c$, and compute prefix sums to obtain per-cell start and length, thus forming per-cell bins by slices into the sorted index array. This models gather-based, sort-and-scan binning.\n\nYour tasks are:\n\n1. Derive from first principles the mapping from continuous positions to discrete cell identifiers using spatial hashing on a periodic domain, and implement both the atomic-insertion method (with a fixed capacity $C$ per cell) and the sort-based method. Use a single linear index function mapping $(i_x,i_y,i_z)$ to $c \\in \\{0,\\dots,n_x n_y n_z - 1\\}$.\n\n2. Define correctness as follows: if there is no overflow in the atomic-insertion method for a given case, then the multiset of particle indices assigned to each cell by atomic-insertion must match the multiset produced by the sort-based method for that same cell, up to permutation. If overflow occurs in any cell, declare correctness to be false for that case.\n\n3. Adopt the following simple performance model for a target Graphics Processing Unit (GPU) architecture to predict a dimensionless time $T$:\n\n- For atomic-insertion, $T_{\\mathrm{atomic}} = t_a N + t_{\\mathrm{conf}} \\sum_{c=0}^{n_{\\mathrm{cells}}-1} \\frac{n_c(n_c - 1)}{2}$, where $n_c$ is the true number of particles in cell $c$ (without capacity truncation), $t_a$ is the base cost per atomic operation, and $t_{\\mathrm{conf}}$ is the penalty per colliding pair updating the same cell counter. The sum $\\sum_c \\frac{n_c(n_c - 1)}{2}$ is the total number of colliding thread pairs, capturing serialization cost due to contention.\n\n- For sort-based binning, $T_{\\mathrm{sort}} = a N \\log_2 N + b N$, where $a$ is the cost per comparison-equivalent for sorting, and $b$ is the linear cost for key generation and scatter-gather.\n\nThese formulas are dimensionless and serve as a well-defined, architecture-dependent comparative model. Use the sort-based per-cell counts to compute $n_c$ in the performance model.\n\n4. Use the following two abstract GPU architectures, specified by $(t_a,t_{\\mathrm{conf}},a,b)$:\n\n- Architecture $0$: $(t_a,t_{\\mathrm{conf}},a,b) = (1.0, 1.0, 8.0, 2.0)$.\n\n- Architecture $1$: $(t_a,t_{\\mathrm{conf}},a,b) = (4.0, 2.0, 4.0, 1.0)$.\n\n5. Implement a program that, for each test case below, constructs both cell lists, checks correctness, computes $T_{\\mathrm{atomic}}$ and $T_{\\mathrm{sort}}$ for the specified architecture, and decides which method is predicted faster. Define a method identifier $f$ with $f=0$ if atomic-insertion is faster or tied, and $f=1$ if sort-based is faster. Also compute the speedup $s$ defined as the ratio of the slower time to the faster time, i.e., $s = \\max(T_{\\mathrm{atomic}},T_{\\mathrm{sort}})/\\min(T_{\\mathrm{atomic}},T_{\\mathrm{sort}})$.\n\n6. Spatial hashing and periodicity: For any coordinate exactly equal to $1$, map it to $0$ before binning to respect periodicity. For other coordinates, use floor division by $h$ to determine cell indices. Ensure that all positions are wrapped into $[0,1)$ by modulo arithmetic before binning.\n\n7. Grid dimensions: For each test case with prescribed $h$, set $n_x = n_y = n_z = L/h$. The values of $h$ are chosen so that $L/h$ is an integer.\n\n8. Distributions: You will generate particle positions according to a distribution identifier $d$:\n\n- $d=0$: Uniform, independent sampling on $[0,1)^3$ using the given seed.\n\n- $d=1$: Clustered Gaussian with mean at $\\boldsymbol{\\mu} = (0.25,0.6,0.8)$ and standard deviation $\\sigma = 0.03$, modulo $1$ into $[0,1)$, using the given seed.\n\n- $d=2$: Boundary-aligned lattice points on the grid with spacing $h$, enumerated in lexicographic order and repeated until $N$ points are generated, with all coordinates wrapped into $[0,1)$; this creates particles on cell boundaries.\n\n9. Test suite: For each tuple $(N,h,C,\\mathrm{seed},d,\\mathrm{arch})$ below, run the computation. The five test cases are:\n\n- Case $1$: $(2000, 0.1, 64, 12345, 0, 0)$.\n\n- Case $2$: $(2000, 0.05, 32, 424242, 1, 1)$.\n\n- Case $3$: $(5000, 0.2, 16, 777, 0, 0)$.\n\n- Case $4$: $(1000, 0.1, 8, 2024, 2, 1)$.\n\n- Case $5$: $(4096, 0.1, 64, 314159, 1, 0)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case produces a list $[c,o,f,s]$ with $c \\in \\{0,1\\}$ indicating correctness, $o \\in \\{0,1\\}$ indicating whether any overflow occurred, $f \\in \\{0,1\\}$ indicating the faster method as defined above, and $s$ a floating-point number giving the speedup. For example, the overall output format is $[[c_1,o_1,f_1,s_1],[c_2,o_2,f_2,s_2],\\dots]$. No other text should be printed.", "solution": "The problem requires the implementation and performance comparison of two standard cell list construction algorithms in computational physics: atomic-insertion and sort-based compaction. The validation of the problem statement finds it to be scientifically sound, well-posed, and internally consistent. It provides a clear, formalizable challenge in algorithmics and performance modeling relevant to the field of molecular dynamics.\n\nFirst, we derive the mapping from continuous particle positions to discrete cell identifiers, a process known as spatial hashing. Given a three-dimensional periodic simulation domain of side length $L=1$, it is partitioned into a uniform grid of $n_x \\times n_y \\times n_z$ cubic cells, each with an edge length of $h$. The problem specifies that $n_\\alpha = L/h$, which is an integer.\n\nA particle at position $\\mathbf{r}_i = (r_{i,x}, r_{i,y}, r_{i,z})$ must first be mapped into the primary domain $[0, L)^3$ to handle periodic boundary conditions. This is achieved by taking each coordinate modulo $L$, i.e., $r'_{i,\\alpha} = r_{i,\\alpha} \\pmod L$. The problem states that coordinates exactly equal to $L=1$ should be mapped to $0$, which is consistent with the half-open interval $[0,1)$.\n\nOnce the position is in the primary domain, we map the continuous coordinate $r'_{i,\\alpha} \\in [0, L)$ to a discrete integer grid index $i_\\alpha \\in \\{0, 1, \\dots, n_\\alpha-1\\}$. This is accomplished by dividing by the cell size $h$ and taking the floor:\n$$\ni_\\alpha = \\lfloor r'_{i,\\alpha} / h \\rfloor\n$$\nThis gives a unique 3D integer tuple $(i_x, i_y, i_z)$ for each particle. To facilitate memory access and processing, this 3D index is \"flattened\" into a single linear cell identifier $c \\in \\{0, 1, \\dots, n_x n_y n_z - 1\\}$. The problem specifies a row-major-like mapping:\n$$\nc = i_x + n_x (i_y + n_y i_z)\n$$\nThis mapping is deterministic and provides a unique cell index for every particle.\n\nNext, we describe the two binning algorithms to be implemented.\n\nThe first method is **atomic insertion into fixed-capacity bins**. This method models a common parallel implementation on architectures like GPUs, where many threads might try to add particles to the same cell bin simultaneously.\n1.  For each of the $n_{\\mathrm{cells}} = n_x n_y n_z$ cells, we pre-allocate a storage bin of fixed capacity $C$.\n2.  A counter is maintained for each cell, initialized to zero.\n3.  For each particle $i=0, \\dots, N-1$, its cell index $c$ is computed.\n4.  The counter for cell $c$ is atomically incremented. In a serial implementation, this is a simple increment. The \"atomic\" nature models the need for synchronization in a parallel context. The value of the counter before the increment, say $k$, gives the next available slot in the bin.\n5.  If $k < C$, the particle's index $i$ is stored in the bin for cell $c$ at position $k$.\n6.  If $k \\ge C$, the bin is full. An overflow condition is recorded for this cell. The total number of particles attempting to enter the cell is still tracked by the counter.\n\nThe second method is **sort-based bin compaction**. This method avoids the contention issues of atomic operations by reordering the particles themselves.\n1.  **Key Generation**: For each particle $i=0, \\dots, N-1$, compute its cell identifier $c_i$.\n2.  **Sorting**: Create a list of particle indices, e.g., an array $P = [0, 1, \\dots, N-1]$. Sort this array $P$ using the corresponding cell identifiers $c_i$ as keys. The result is a new array, $P'$, where particle indices are grouped by their cell ID.\n3.  **Scan and Segment**: To find the particles belonging to each cell, we need to know where each cell's block begins and how long it is. This can be efficiently done by first counting the number of particles per cell, $n_c$, by iterating through the computed $c_i$ values. Then, an exclusive prefix sum (scan) on the array of counts $n_c$ yields the starting index for each cell's segment within the sorted particle array $P'$. The particles for cell $c$ are the slice of $P'$ starting at index $(\\sum_{j=0}^{c-1} n_j)$ with length $n_c$.\n\n**Correctness** is evaluated by comparing the outcomes of the two methods. An overflow is flagged if, for any cell $c$, its true particle count $n_c$ (as determined by the sort-based method, which has no capacity limit) exceeds the atomic-insertion bin capacity $C$. If an overflow occurs, correctness is declared false. If no overflow occurs, correctness requires that for every cell, the multiset of particle indices obtained from the atomic-insertion method is identical (up to permutation) to the multiset obtained from the sort-based method. Since both algorithms, if implemented correctly, are deterministic ways of assigning particles to cells, this condition should hold if and only if no overflow occurs.\n\nFinally, a **performance model** is used to predict which algorithm is faster on a given abstract architecture. The model captures the fundamental trade-offs between the two approaches.\nFor the atomic-insertion method, the time is:\n$$\nT_{\\mathrm{atomic}} = t_a N + t_{\\mathrm{conf}} \\sum_{c=0}^{n_{\\mathrm{cells}}-1} \\frac{n_c(n_c - 1)}{2}\n$$\nHere, $t_a N$ represents the base cost of processing $N$ particles (computing cell IDs, memory access). The second term models the cost of contention: for a cell with $n_c$ particles, there are $\\binom{n_c}{2} = n_c(n_c-1)/2$ pairs of particles that could collide when updating the cell's counter, each collision incurring a penalty $t_{\\mathrm{conf}}$. This term is highly sensitive to the particle distribution; clustered distributions lead to high contention and a large $T_{\\mathrm{atomic}}$.\n\nFor the sort-based method, the time is:\n$$\nT_{\\mathrm{sort}} = a N \\log_2 N + b N\n$$\nThis reflects the complexity of typical comparison-based sorting algorithms ($O(N \\log N)$), with parameter $a$, plus a linear cost for key generation and data movement (scatter/gather), with parameter $b$. This cost is largely independent of the spatial distribution of particles.\n\nBy implementing these algorithms and performance models for the specified test cases, we can determine for each scenario whether an overflow occurs, verify correctness, and predict which algorithm is more performant based on particle count, distribution, and abstract hardware characteristics.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the cell list construction problem for a series of test cases.\n    \"\"\"\n    \n    # Test cases: (N, h, C, seed, d, arch_id)\n    test_cases = [\n        (2000, 0.1, 64, 12345, 0, 0),\n        (2000, 0.05, 32, 424242, 1, 1),\n        (5000, 0.2, 16, 777, 0, 0),\n        (1000, 0.1, 8, 2024, 2, 1),\n        (4096, 0.1, 64, 314159, 1, 0),\n    ]\n\n    # Architectures: (t_a, t_conf, a, b)\n    architectures = [\n        (1.0, 1.0, 8.0, 2.0),  # Arch 0\n        (4.0, 2.0, 4.0, 1.0),  # Arch 1\n    ]\n\n    results = []\n    L = 1.0\n\n    for N, h, C, seed, d, arch_id in test_cases:\n        # --- 1. Setup ---\n        n = int(round(L / h))\n        n_x, n_y, n_z = n, n, n\n        n_cells = n_x * n_y * n_z\n        t_a, t_conf, a, b = architectures[arch_id]\n        rng = np.random.default_rng(seed)\n\n        # --- 2. Generate Particle Positions ---\n        if d == 0:  # Uniform\n            positions = rng.uniform(0, L, size=(N, 3))\n        elif d == 1:  # Clustered Gaussian\n            mean = np.array([0.25, 0.6, 0.8])\n            std_dev = 0.03\n            positions = rng.normal(loc=mean, scale=std_dev, size=(N, 3))\n            positions = positions % L\n        elif d == 2:  # Lattice\n            if N == 0:\n                positions = np.empty((0,3))\n            else:\n                grid_points_1d = np.arange(n) * h\n                gx, gy, gz = np.meshgrid(grid_points_1d, grid_points_1d, grid_points_1d, indexing='ij')\n                lattice_pts = np.stack([gx.ravel(), gy.ravel(), gz.ravel()], axis=-1)\n                num_lattice_pts = len(lattice_pts)\n                num_repeats = N // num_lattice_pts\n                remainder = N % num_lattice_pts\n                positions = np.vstack([\n                    np.tile(lattice_pts, (num_repeats, 1)),\n                    lattice_pts[:remainder]\n                ])\n\n        # Ensure positions are in [0, L)\n        positions = positions % L\n\n        # --- 3. Compute Cell IDs for all particles ---\n        # Using floor division after scaling is equivalent to floor(pos/h)\n        indices = np.floor(positions / h).astype(int)\n        # Clamp indices to be safe, although modulo should prevent out-of-bounds\n        indices = np.clip(indices, 0, n - 1)\n        ix, iy, iz = indices[:, 0], indices[:, 1], indices[:, 2]\n        particle_cell_ids = ix + n_x * (iy + n_y * iz)\n\n        # --- 4. Method 1: Atomic Insertion ---\n        atomic_cell_bins = np.full((n_cells, C), -1, dtype=int)\n        true_cell_counts = np.zeros(n_cells, dtype=int)\n        \n        for i in range(N):\n            c = particle_cell_ids[i]\n            k = true_cell_counts[c]\n            true_cell_counts[c] += 1\n            if k < C:\n                atomic_cell_bins[c, k] = i\n\n        # --- 5. Method 2: Sort-based Compaction ---\n        # Get true counts directly using bincount for efficiency and comparison\n        sort_cell_counts = np.bincount(particle_cell_ids, minlength=n_cells).astype(int)\n        \n        # Build cell starts array (exclusive prefix sum)\n        sort_cell_starts = np.zeros(n_cells, dtype=int)\n        sort_cell_starts[1:] = np.cumsum(sort_cell_counts[:-1])\n\n        # Sort particle indices by cell ID\n        particle_indices = np.arange(N)\n        sort_keys = np.argsort(particle_cell_ids, kind='stable')\n        sorted_particle_indices = particle_indices[sort_keys]\n\n        # --- 6. Correctness and Overflow Check ---\n        overflow_occurred = 1 if np.any(true_cell_counts > C) else 0\n        o = overflow_occurred\n        \n        is_correct = 1\n        if o == 1:\n            is_correct = 0\n        else:\n            # If no overflow, verify the contents of each bin\n            for c in range(n_cells):\n                # Atomic method multiset\n                atomic_count = true_cell_counts[c]\n                atomic_list = atomic_cell_bins[c, :atomic_count]\n                \n                # Sort-based method multiset\n                start = sort_cell_starts[c]\n                count = sort_cell_counts[c]\n                sort_list = sorted_particle_indices[start : start + count]\n\n                # Multisets must be identical up to permutation\n                if not np.array_equal(np.sort(atomic_list), np.sort(sort_list)):\n                    is_correct = 0\n                    break\n        c_out = is_correct\n\n        # --- 7. Performance Model Calculation ---\n        # Use the definitively correct counts from the sort-based method\n        nc = sort_cell_counts\n        \n        T_atomic = t_a * N + t_conf * np.sum(nc * (nc - 1) / 2.0)\n        \n        # Avoid log2(0) if N=0 which is not in test cases but good practice\n        log2N = np.log2(N) if N > 0 else 0\n        T_sort = a * N * log2N + b * N\n        \n        f = 0 if T_atomic <= T_sort else 1\n        \n        min_T = min(T_atomic, T_sort)\n        max_T = max(T_atomic, T_sort)\n        # Avoid division by zero if both times are zero\n        s = max_T / min_T if min_T > 0 else 1.0\n\n        results.append(f\"[{c_out},{o},{f},{s:.10f}]\")\n\n    # --- 8. Final Output ---\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3400614"}]}