{"hands_on_practices": [{"introduction": "Spatial decomposition is a cornerstone of parallelizing molecular dynamics simulations with short-range forces. This approach involves partitioning the simulation box into a grid of smaller subdomains, or cells, which are then distributed among processing units. This exercise [@problem_id:3431951] guides you through the crucial first step of this process: calculating the appropriate cell dimensions based on the interaction cutoff radius $r_c$ and system density, which is fundamental to ensuring both correctness and efficiency.", "problem": "A three-dimensional molecular dynamics simulation of a simple fluid of uniform number density is run in a cubic box of side length $L$ with periodic boundary conditions. To enable efficient parallel neighbor searching with a cell list, the domain is partitioned into a uniform grid of cubic cells. The cell edge length $a$ must be chosen such that it satisfies the neighbor-list safety condition $a \\geq l_c$, where the list cell size $l_c$ equals the sum of a conservative cutoff radius $r_c$ for pair interactions and a nonbonded skin distance $\\Delta$ used to amortize neighbor-list rebuilds. The implementation selects an integer number of cells $n$ per edge so that $a = L/n \\geq l_c$ while maximizing $n$ under this constraint, and the total number of cells is $N_{\\text{cell}} = n^3$. The average number of atoms per cell is the total atoms divided by the number of cells.\n\nGiven $r_c = 1.0$ nm, $\\Delta = 0.2$ nm, $\\rho = 33$ nm$^{-3}$, and a cubic box of side $L = 20$ nm, compute:\n- the total number of cells $N_{\\text{cell}}$ in the grid, and\n- the average number of atoms per cell.\n\nUse the list cell size $l_c = r_c + \\Delta$, treat the number density $\\rho$ as uniform, and assume the total number of atoms is $N = \\rho L^3$. Report the number of cells as a dimensionless integer and the average number of atoms per cell as a dimensionless quantity rounded to four significant figures.", "solution": "The problem is validated as self-contained, consistent, and scientifically sound. It represents a standard procedure in setting up a molecular dynamics simulation with domain decomposition. We proceed with the solution.\n\nThe objective is to compute $2$ quantities: the total number of cells, $N_{\\text{cell}}$, and the average number of atoms per cell.\n\nFirst, we determine the optimal number of cells per edge, $n$. The implementation requires that the cell edge length, $a$, must satisfy the safety condition $a \\geq l_c$, where $l_c$ is the list cell size. The list cell size is the sum of the cutoff radius, $r_c$, and the nonbonded skin distance, $\\Delta$. Given $r_c = 1.0$ nm and $\\Delta = 0.2$ nm, we calculate $l_c$:\n$$l_c = r_c + \\Delta = 1.0 \\text{ nm} + 0.2 \\text{ nm} = 1.2 \\text{ nm}$$\nThe cell edge length $a$ is defined by the total box length $L$ and the number of cells per edge $n$ as $a = L/n$. Substituting this into the safety condition:\n$$\\frac{L}{n} \\geq l_c$$\nThe problem specifies that $n$ must be an integer and maximized under this constraint. We can rearrange the inequality to solve for $n$:\n$$n \\leq \\frac{L}{l_c}$$\nUsing the given box side length $L = 20$ nm and the calculated value of $l_c = 1.2$ nm:\n$$n \\leq \\frac{20 \\text{ nm}}{1.2 \\text{ nm}} = \\frac{200}{12} = \\frac{50}{3} \\approx 16.667$$\nSince $n$ must be an integer, the maximum value for $n$ that satisfies this condition is the floor of $16.667$:\n$$n = \\lfloor 16.667 \\rfloor = 16$$\nNow that we have determined the number of cells per edge, we can calculate the total number of cells in the $3$-dimensional grid, $N_{\\text{cell}}$.\n$$N_{\\text{cell}} = n^3 = 16^3 = 4096$$\nThis is the first required quantity.\n\nSecond, we compute the average number of atoms per cell. This is given by the total number of atoms, $N$, divided by the total number of cells, $N_{\\text{cell}}$. The total number of atoms is determined by the uniform number density, $\\rho$, and the volume of the simulation box, $V = L^3$. Given $\\rho = 33 \\text{ nm}^{-3}$ and $L = 20$ nm:\n$$N = \\rho V = \\rho L^3 = (33 \\text{ nm}^{-3}) \\times (20 \\text{ nm})^3 = 33 \\times 8000 = 264000$$\nThe average number of atoms per cell, which we denote as $\\langle N_{\\text{atoms/cell}} \\rangle$, is therefore:\n$$\\langle N_{\\text{atoms/cell}} \\rangle = \\frac{N}{N_{\\text{cell}}} = \\frac{264000}{4096}$$\nPerforming the division gives the exact value:\n$$\\langle N_{\\text{atoms/cell}} \\rangle = 64.453125$$\nThe problem requires this value to be reported rounded to $4$ significant figures. The first $4$ significant figures are $6$, $4$, $4$, and $5$. The fifth significant digit is $3$, which is less than $5$, so we round down.\n$$\\langle N_{\\text{atoms/cell}} \\rangle \\approx 64.45$$\nThus, the $2$ computed quantities are $N_{\\text{cell}} = 4096$ and the average number of atoms per cell is approximately $64.45$.", "answer": "$$\\boxed{\\begin{pmatrix} 4096 & 64.45 \\end{pmatrix}}$$", "id": "3431951"}, {"introduction": "After distributing the workload, overall performance is dictated by the efficiency of the core computational kernel. Modern processors achieve high throughput using Single Instruction, Multiple Data (SIMD) vectorization, but this capability is only effective when data is arranged favorably in memory. This practice [@problem_id:3431984] challenges you to develop an analytical performance model to compare the Array-of-Structures (AoS) and Structure-of-Arrays (SoA) data layouts, offering insight into the critical trade-offs between memory access patterns and arithmetic speed.", "problem": "Consider a classical short-range pairwise molecular dynamics kernel that, for each central particle $i$, evaluates pair interactions with $w$ neighbors $\\{j_{1},\\dots,j_{w}\\}$ using Single Instruction Multiple Data (SIMD) width $w$. Assume the following well-tested performance modeling bases:\n\n1. The time per SIMD vector iteration is the sum of an arithmetic term and a memory traffic term, where the latter is proportional to the number of cache lines moved. This is consistent with the roofline-style decomposition in which arithmetic and data movement contribute additively to execution time when neither is fully overlapped.\n2. For memory, the cost to transfer one cache line is a constant $c_{L}$ (in cycles per line) from the dominant memory hierarchy level, and non-unit-stride vector loads (gather) and non-unit-stride vector stores (scatter) add per-line overheads $c_{g}$ and $c_{s}$, respectively. Unit-stride loads and stores incur no gather/scatter overhead.\n3. Cache lines have size $B$ bytes. Scalar values (e.g., one coordinate component) have size $a$ bytes. For an Array of Structures (AoS) layout, each particle occupies a structure of size $s$ bytes containing at least three position components and three force components. For a Structure of Arrays (SoA) layout, positions and forces are stored in separate arrays, one per component (e.g., $x$, $y$, $z$ for positions and $f_{x}$, $f_{y}$, $f_{z}$ for forces), each element occupying $a$ bytes.\n\nAssume a short-range neighbor list such that, within one SIMD vector iteration, the $w$ neighbor indices $\\{j_{k}\\}$ are effectively uncorrelated in memory (no reuse within the vector), so that:\n- Under SoA, the number of cache lines touched to load the $w$ neighbor positions is $L_{\\mathrm{SoA},j}^{\\mathrm{load}} = 3 \\left\\lceil \\frac{w a}{B} \\right\\rceil$, and to store the $w$ neighbor force updates is $L_{\\mathrm{SoA},j}^{\\mathrm{store}} = 3 \\left\\lceil \\frac{w a}{B} \\right\\rceil$.\n- Under AoS, the number of cache lines touched to load the $w$ neighbor positions (as structures) is $L_{\\mathrm{AoS},j}^{\\mathrm{load}} = \\left\\lceil \\frac{w s}{B} \\right\\rceil$, and to store the $w$ neighbor force updates is $L_{\\mathrm{AoS},j}^{\\mathrm{store}} = \\left\\lceil \\frac{w s}{B} \\right\\rceil$.\n\nFor the central particle $i$, assume its data are accessed with unit stride so that:\n- Under SoA, the positions of $i$ touch $L_{\\mathrm{SoA},i} = \\left\\lceil \\frac{3 a}{B} \\right\\rceil$ cache lines.\n- Under AoS, the structure of $i$ touches $L_{\\mathrm{AoS},i} = \\left\\lceil \\frac{s}{B} \\right\\rceil$ cache lines.\n\nLet the arithmetic cost per pair interaction be $c_{c}$ cycles (this includes floating-point force evaluation but excludes memory costs). Assume no other memory traffic dominates (e.g., neighbor indices are negligible in comparison to positions and forces for the purposes of this question), and that there is no overlap between arithmetic and memory costs.\n\nDefine throughput as the number of pair interactions completed per processor cycle. Using only the above modeling assumptions and definitions, derive closed-form expressions for the throughput of the SIMD vector iteration under AoS and SoA layouts, denoted $T_{\\mathrm{AoS}}(w,B,a,s,c_{c},c_{L},c_{g},c_{s})$ and $T_{\\mathrm{SoA}}(w,B,a,s,c_{c},c_{L},c_{g},c_{s})$, respectively, each measured in interactions per cycle. Express your final answer as analytical expressions in terms of $w$, $B$, $a$, $s$, $c_{c}$, $c_{L}$, $c_{g}$, and $c_{s}$. Do not include any units in your final expressions. No numerical evaluation is required.", "solution": "We begin with the roofline-style principle that the time per SIMD vector iteration is the sum of an arithmetic contribution and a data movement contribution. Let one SIMD vector iteration process $w$ pair interactions, namely the interactions of the central particle $i$ with the $w$ neighbors $\\{j_{1},\\dots,j_{w}\\}$. The throughput, measured in interactions per cycle, is the number of interactions completed divided by the total cycles taken for that iteration. Therefore, if the total cycles per iteration is $C_{\\mathrm{iter}}$, then the throughput is $T = \\frac{w}{C_{\\mathrm{iter}}}$.\n\nWe now construct $C_{\\mathrm{iter}}$ under each memory layout from first principles.\n\nArithmetic cost. By assumption, the arithmetic cost per pair interaction is $c_{c}$ cycles and is independent of layout. For $w$ pairs per iteration, the arithmetic cost is\n$$\nC_{\\mathrm{comp}} = c_{c} \\, w.\n$$\n\nMemory cost model. Each cache line transfer costs $c_{L}$ cycles, independently of whether it is a load or a store, when counted at the dominant memory hierarchy level. Additionally, non-unit-stride vector loads (gather) incur an overhead $c_{g}$ per cache line touched by the gather, and non-unit-stride vector stores (scatter) incur an overhead $c_{s}$ per cache line touched by the scatter. Unit-stride accesses do not incur these gather/scatter overheads.\n\nAccess pattern characterization for a short-range neighbor list. Within one SIMD vector iteration, the neighbor indices $\\{j_{k}\\}$ are assumed uncorrelated in memory. This implies that, at vector granularity, the $w$ elements drawn from arrays (SoA) or from structures (AoS) span enough addresses that the number of distinct cache lines touched is well approximated by the total bytes addressed divided by the cache line size $B$, rounded up.\n\nUnder Structure of Arrays (SoA), each coordinate component is stored in a dedicated array with element size $a$ bytes. To load the $w$ neighbor positions, we must gather from the $x$, $y$, and $z$ arrays, $w$ elements each. The number of cache lines touched to load these is\n$$\nL_{\\mathrm{SoA},j}^{\\mathrm{load}} = 3 \\left\\lceil \\frac{w a}{B} \\right\\rceil.\n$$\nSimilarly, to scatter the force updates for the $w$ neighbors to the $f_{x}$, $f_{y}$, and $f_{z}$ arrays, the number of cache lines touched is\n$$\nL_{\\mathrm{SoA},j}^{\\mathrm{store}} = 3 \\left\\lceil \\frac{w a}{B} \\right\\rceil.\n$$\nFor the central particle $i$, whose data are accessed with unit stride, the number of cache lines touched to load its position components is\n$$\nL_{\\mathrm{SoA},i} = \\left\\lceil \\frac{3 a}{B} \\right\\rceil.\n$$\nBecause these are unit-stride, there is no gather overhead for $i$.\n\nUnder Array of Structures (AoS), each particle occupies a structure of size $s$ bytes that includes at least the three position components and three force components. The $w$ neighbor positions are gathered by reading $w$ structures. The number of cache lines touched to load is\n$$\nL_{\\mathrm{AoS},j}^{\\mathrm{load}} = \\left\\lceil \\frac{w s}{B} \\right\\rceil,\n$$\nand the number of cache lines touched to store the $w$ neighbor force updates is\n$$\nL_{\\mathrm{AoS},j}^{\\mathrm{store}} = \\left\\lceil \\frac{w s}{B} \\right\\rceil.\n$$\nFor the central particle $i$, accessed with unit stride, the number of cache lines touched to load its structure is\n$$\nL_{\\mathrm{AoS},i} = \\left\\lceil \\frac{s}{B} \\right\\rceil.\n$$\n\nCycle accounting under SoA. The total number of cache lines moved (loads plus stores) per iteration is\n$$\nL_{\\mathrm{SoA}}^{\\mathrm{tot}} = L_{\\mathrm{SoA},j}^{\\mathrm{load}} + L_{\\mathrm{SoA},j}^{\\mathrm{store}} + L_{\\mathrm{SoA},i} = 2 \\cdot 3 \\left\\lceil \\frac{w a}{B} \\right\\rceil + \\left\\lceil \\frac{3 a}{B} \\right\\rceil.\n$$\nThe gather overhead applies to the neighbor loads only, adding $c_{g}$ per cache line touched in $L_{\\mathrm{SoA},j}^{\\mathrm{load}}$. The scatter overhead applies to the neighbor stores only, adding $c_{s}$ per cache line touched in $L_{\\mathrm{SoA},j}^{\\mathrm{store}}$. There is no gather/scatter overhead for the unit-stride central particle $i$ loads. Therefore, the total cycles per iteration under SoA are\n$$\nC_{\\mathrm{iter}}^{\\mathrm{SoA}} = c_{c} \\, w \\;+\\; c_{L} \\left( 2 \\cdot 3 \\left\\lceil \\frac{w a}{B} \\right\\rceil + \\left\\lceil \\frac{3 a}{B} \\right\\rceil \\right) \\;+\\; c_{g} \\cdot 3 \\left\\lceil \\frac{w a}{B} \\right\\rceil \\;+\\; c_{s} \\cdot 3 \\left\\lceil \\frac{w a}{B} \\right\\rceil.\n$$\nIt is convenient to combine the gather and scatter overheads for neighbors:\n$$\nC_{\\mathrm{iter}}^{\\mathrm{SoA}} = c_{c} \\, w \\;+\\; c_{L} \\left( 6 \\left\\lceil \\frac{w a}{B} \\right\\rceil + \\left\\lceil \\frac{3 a}{B} \\right\\rceil \\right) \\;+\\; (c_{g} + c_{s}) \\cdot 3 \\left\\lceil \\frac{w a}{B} \\right\\rceil.\n$$\nThus the SoA throughput, in interactions per cycle, is\n$$\nT_{\\mathrm{SoA}}(w,B,a,s,c_{c},c_{L},c_{g},c_{s}) \\;=\\; \\frac{w}{\\,c_{c} \\, w \\;+\\; c_{L} \\left( 6 \\left\\lceil \\frac{w a}{B} \\right\\rceil + \\left\\lceil \\frac{3 a}{B} \\right\\rceil \\right) \\;+\\; (c_{g} + c_{s}) \\cdot 3 \\left\\lceil \\frac{w a}{B} \\right\\rceil\\,}.\n$$\n\nCycle accounting under AoS. The total number of cache lines moved (loads plus stores) per iteration is\n$$\nL_{\\mathrm{AoS}}^{\\mathrm{tot}} = L_{\\mathrm{AoS},j}^{\\mathrm{load}} + L_{\\mathrm{AoS},j}^{\\mathrm{store}} + L_{\\mathrm{AoS},i} = 2 \\left\\lceil \\frac{w s}{B} \\right\\rceil + \\left\\lceil \\frac{s}{B} \\right\\rceil.\n$$\nAs above, the neighbor loads incur gather overhead and the neighbor stores incur scatter overhead, each per cache line. There is no gather/scatter overhead for the unit-stride central particle $i$. Therefore, the total cycles per iteration under AoS are\n$$\nC_{\\mathrm{iter}}^{\\mathrm{AoS}} = c_{c} \\, w \\;+\\; c_{L} \\left( 2 \\left\\lceil \\frac{w s}{B} \\right\\rceil + \\left\\lceil \\frac{s}{B} \\right\\rceil \\right) \\;+\\; c_{g} \\left\\lceil \\frac{w s}{B} \\right\\rceil \\;+\\; c_{s} \\left\\lceil \\frac{w s}{B} \\right\\rceil,\n$$\nor equivalently,\n$$\nC_{\\mathrm{iter}}^{\\mathrm{AoS}} = c_{c} \\, w \\;+\\; c_{L} \\left( 2 \\left\\lceil \\frac{w s}{B} \\right\\rceil + \\left\\lceil \\frac{s}{B} \\right\\rceil \\right) \\;+\\; (c_{g} + c_{s}) \\left\\lceil \\frac{w s}{B} \\right\\rceil.\n$$\nThus the AoS throughput, in interactions per cycle, is\n$$\nT_{\\mathrm{AoS}}(w,B,a,s,c_{c},c_{L},c_{g},c_{s}) \\;=\\; \\frac{w}{\\,c_{c} \\, w \\;+\\; c_{L} \\left( 2 \\left\\lceil \\frac{w s}{B} \\right\\rceil + \\left\\lceil \\frac{s}{B} \\right\\rceil \\right) \\;+\\; (c_{g} + c_{s}) \\left\\lceil \\frac{w s}{B} \\right\\rceil\\,}.\n$$\n\nThese expressions explicitly exhibit the dependence of throughput on the SIMD width $w$, cache line size $B$, scalar size $a$, structure size $s$, arithmetic cost $c_{c}$, cache line transfer cost $c_{L}$, and gather/scatter overheads $c_{g}$ and $c_{s}$, under the stated short-range, uncorrelated-neighbor assumptions.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\dfrac{w}{\\,c_{c} \\, w \\;+\\; c_{L} \\left( 2 \\left\\lceil \\dfrac{w s}{B} \\right\\rceil + \\left\\lceil \\dfrac{s}{B} \\right\\rceil \\right) \\;+\\; (c_{g} + c_{s}) \\left\\lceil \\dfrac{w s}{B} \\right\\rceil\\,} &\n\\dfrac{w}{\\,c_{c} \\, w \\;+\\; c_{L} \\left( 6 \\left\\lceil \\dfrac{w a}{B} \\right\\rceil + \\left\\lceil \\dfrac{3 a}{B} \\right\\rceil \\right) \\;+\\; (c_{g} + c_{s}) \\cdot 3 \\left\\lceil \\dfrac{w a}{B} \\right\\rceil\\,}\n\\end{pmatrix}\n}\n$$", "id": "3431984"}, {"introduction": "A static partition of the simulation domain often proves insufficient for realistic systems where particle clustering and density fluctuations create computational hot spots and voids. This leads to severe load imbalance, leaving some processors overworked while others are idle. In this exercise [@problem_id:3431949], you will implement a discrete-event simulation of a work-stealing scheduler, a powerful technique for dynamic load balancing, to explore the interplay between workload distribution, communication latency, and algorithmic tuning.", "problem": "Consider a classical Molecular Dynamics (MD) short-range force computation with a spatial domain decomposition into $P$ subdomains, one per worker. Each subdomain $i \\in \\{0,1,\\dots,P-1\\}$ holds $W_i$ equal-cost interaction tiles. Processing a single tile deterministically costs $t_u$ microseconds (time per tile). Workers execute concurrently and follow a local-first work-stealing (WS) policy across subdomains to mitigate voids (empty or sparse subdomains). The WS policy uses a fixed chunk size $c$ tiles per steal and imposes a stealing latency $\\ell_s$ microseconds per successful steal.\n\nThe scheduler is defined as follows.\n- All workers start at time $0$ and execute in continuous time. Each worker $i$ has an initial local queue containing $W_i$ tiles.\n- A worker that has local tiles remaining always processes a local chunk next. A chunk of size $q$ tiles, with $q = \\min(c,\\text{local remaining})$, takes $q \\cdot t_u$ microseconds to complete, with no additional overhead.\n- When a worker’s local queue is empty, it attempts to steal from a victim subdomain chosen as the one with the largest remaining number of tiles among all subdomains (ties broken by the smallest subdomain index). If the victim has $r$ tiles remaining, the thief steals a chunk of size $q = \\min(c,r)$. Before processing that chunk, the thief pays a latency of $\\ell_s$ microseconds. The processing time for that stolen chunk is then $q \\cdot t_u$ microseconds after the latency. The victim’s remaining tile count is reduced by $q$ immediately when the steal is decided. If no tiles remain in the entire system, the worker becomes idle permanently.\n- All chunk executions (local and stolen) are non-preemptive. The global execution continues until all tiles have been processed.\n\nDefine the performance metric as the parallel makespan under WS, denoted $T_{\\text{steal}}$, which is the time at which the last chunk finishes. Define the static baseline makespan without stealing as $T_{\\text{base}} = \\max_{i} \\left(W_i \\cdot t_u\\right)$, where each worker only processes its own subdomain’s tiles with no stealing and no additional overheads. Define the speedup as $S = T_{\\text{base}} / T_{\\text{steal}}$ (dimensionless). Define the reproducibility exposure metric as $R$, the fraction of tiles processed by thieves (not by their owner subdomain’s original worker), i.e., $R = (\\text{total stolen tiles})/\\left(\\sum_{i=0}^{P-1} W_i\\right)$, expressed as a decimal fraction (no percentage sign).\n\nYour task is to implement the above scheduler exactly as specified and compute, for each test case, the tuple $\\left(T_{\\text{steal}}, S, R\\right)$. The time $T_{\\text{steal}}$ must be expressed in microseconds, as an integer number of microseconds. The speedup $S$ and reproducibility exposure $R$ must be returned as decimal floating-point values.\n\nFundamental bases you may assume:\n- The definition of makespan as the maximum completion time across concurrent workers.\n- The cost model in which sequential chunk processing time is additive: a chunk of $q$ tiles takes $q \\cdot t_u$ microseconds, and stealing adds a fixed latency $\\ell_s$ before the chunk’s compute time begins.\n- Deterministic tie-breaking by lowest index to ensure a well-defined schedule.\n\nTest Suite. For each case, use $P=4$ workers (one per subdomain), with the following parameters, all times in microseconds:\n- Case $1$: $W=[800,50,0,50]$, $c=20$, $\\ell_s=40$, $t_u=1$.\n- Case $2$: $W=[200,200,200,200]$, $c=50$, $\\ell_s=5$, $t_u=1$.\n- Case $3$: $W=[0,0,0,1000]$, $c=10$, $\\ell_s=100$, $t_u=1$.\n- Case $4$: $W=[300,10,10,10]$, $c=64$, $\\ell_s=30$, $t_u=1$.\n- Case $5$: $W=[101,0,101,0]$, $c=1$, $\\ell_s=1$, $t_u=1$.\n\nRequired final output format. Your program should produce a single line of output containing all results flattened into one list as a comma-separated list enclosed in square brackets. The per-case results must be appended in order as triples $\\left[T_{\\text{steal}}, S, R\\right]$, yielding a single flat list in the order\n$[T_{\\text{steal}}^{(1)}, S^{(1)}, R^{(1)}, T_{\\text{steal}}^{(2)}, S^{(2)}, R^{(2)}, \\dots, T_{\\text{steal}}^{(5)}, S^{(5)}, R^{(5)}]$,\nwhere $T_{\\text{steal}}^{(k)}$ is an integer (microseconds), and $S^{(k)}$ and $R^{(k)}$ are decimal floating-point values. Express $T_{\\text{steal}}$ in microseconds, $S$ and $R$ as decimals (no percentage sign), and do not include any additional text.", "solution": "The problem requires the simulation of a parallel work-stealing scheduler to determine its performance characteristics. The system consists of $P$ workers, each with an initial queue of $W_i$ work tiles. The scheduling policy is deterministic, making it suitable for a precise simulation. The problem is a well-posed discrete-event simulation task.\n\nThe solution is architected as a discrete-event simulation, a standard methodology for modeling systems that evolve at asynchronous, discrete points in time. The core components of this simulation are the system state, the events, and an event-driven control loop.\n\n1.  **System State**: The global state of the simulation is primarily defined by the number of unprocessed tiles remaining in each subdomain's queue. This is represented by an array, `rem_tiles`, of size $P$, initialized with the workload distribution $W$. Additional state variables track the total number of tiles stolen (`total_stolen_tiles`) and the simulation makespan.\n\n2.  **Events**: An \"event\" is defined as a worker completing its current chunk of work and becoming available for a new task. Each event is characterized by a tuple $(t, i)$, representing that worker $i$ becomes free at time $t$.\n\n3.  **Event Queue and Simulation Loop**: A priority queue, ordered by event time $t$, manages the sequence of events. The simulation starts by populating the queue with initial events for all $P$ workers at time $t=0$. The main loop repeatedly extracts the event with the minimum time from the queue, advances the simulation's `current_time` to this event's time, and processes the event.\n\n4.  **Event Processing Logic**: When a worker $i$ becomes available at `current_time`, a new task is assigned based on the specified policy:\n    a. **Local Work**: The worker first checks its own tile queue, `rem_tiles[i]`. If `rem_tiles[i] > 0`, it processes a local chunk of size $q = \\min(c, \\text{rem\\_tiles}[i])$. The task takes $q \\cdot t_u$ microseconds. A new event corresponding to the completion time, `current_time` + $q \\cdot t_u$, is added to the priority queue for this worker.\n    \n    b. **Work-Stealing**: If the local queue is empty (`rem_tiles[i] == 0`), the worker attempts to steal. It identifies a victim subdomain $v$ as the one with the maximum number of remaining tiles. Ties are broken by selecting the subdomain with the smallest index, ensuring determinism. If a victim with non-zero tiles is found, the worker steals a chunk of size $q = \\min(c, \\text{rem\\_tiles}[v])$. The state of the victim's queue, `rem_tiles[v]`, is immediately decremented. The thief pays a latency cost $\\ell_s$. The completion time for this stolen task is `current_time` + $\\ell_s$ + $q \\cdot t_u$. A corresponding completion event is enqueued.\n    \n    c. **Idleness**: If a worker's local queue is empty and no tiles remain anywhere in the system, the worker becomes permanently idle and is not re-inserted into the event queue.\n\n5.  **Termination and Metrics Calculation**: The simulation terminates when the event queue becomes empty, which occurs once all tiles have been processed and all workers have become idle. The makespan under work-stealing, $T_{\\text{steal}}$, is the maximum completion time recorded across all tasks. The baseline makespan, $T_{\\text{base}}$, is calculated as $\\max_i(W_i \\cdot t_u)$. The speedup $S$ is the ratio $T_{\\text{base}} / T_{\\text{steal}}$, and the reproducibility exposure $R$ is the ratio of total stolen tiles to the total initial tiles. All time calculations use integer arithmetic as specified.\n\nThis simulation model faithfully implements the deterministic rules of the scheduler, allowing for the precise calculation of the required performance metrics for each test case.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport heapq\n\ndef run_simulation(P, W, c, l_s, t_u):\n    \"\"\"\n    Runs the discrete-event simulation for one test case to compute\n    makespan, speedup, and reproducibility exposure.\n    \n    Args:\n        P (int): Number of workers/subdomains.\n        W (list): Initial workload (number of tiles) for each subdomain.\n        c (int): Chunk size for processing and stealing.\n        l_s (int): Latency for a successful steal in microseconds.\n        t_u (int): Time to process a single tile in microseconds.\n        \n    Returns:\n        tuple: A tuple containing (T_steal, S, R).\n    \"\"\"\n    # Initialize state variables\n    rem_tiles = list(W)\n    total_initial_tiles = sum(W)\n    \n    # Handle the edge case of no work.\n    if total_initial_tiles == 0:\n        return (0, 1.0, 0.0)\n\n    total_stolen_tiles = 0\n    \n    # The event queue stores (finish_time, worker_id) tuples.\n    # heapq implements a min-priority queue perfectly suited for this.\n    # Python's tuple comparison breaks ties on finish_time by worker_id,\n    # ensuring deterministic processing of simultaneous events.\n    event_queue = []\n    for i in range(P):\n        heapq.heappush(event_queue, (0, i))\n\n    makespan = 0\n\n    # The simulation loop continues as long as there are workers with\n    # pending tasks in the event queue.\n    while event_queue:\n        # Dequeue the event with the earliest finish time.\n        current_time, worker_id = heapq.heappop(event_queue)\n\n        # Assign a new task to the now-available worker.\n\n        # PRIORITY 1: Process local work if available.\n        if rem_tiles[worker_id] > 0:\n            # Determine local chunk size.\n            q = min(c, rem_tiles[worker_id])\n            rem_tiles[worker_id] -= q\n            \n            # Calculate task duration and the worker's next finish time.\n            duration = q * t_u\n            finish_time = current_time + duration\n            makespan = max(makespan, finish_time)\n            \n            # Add the worker's next availability event to the queue.\n            heapq.heappush(event_queue, (finish_time, worker_id))\n\n        # PRIORITY 2: Attempt to steal work if local queue is empty.\n        else:\n            # Find a victim based on the specified policy: the subdomain with\n            # the largest number of tiles, with ties broken by smallest index.\n            max_tiles = -1\n            victim_idx = -1\n            for j in range(P):\n                if rem_tiles[j] > max_tiles:\n                    max_tiles = rem_tiles[j]\n                    victim_idx = j\n            \n            # If a victim with work is found, perform the steal.\n            if victim_idx != -1 and max_tiles > 0:\n                # Determine stolen chunk size.\n                q = min(c, rem_tiles[victim_idx])\n                \n                # Atomically update victim's tile count and total stolen count.\n                rem_tiles[victim_idx] -= q\n                total_stolen_tiles += q\n                \n                # Calculate task duration (including latency) and finish time.\n                duration = l_s + q * t_u\n                finish_time = current_time + duration\n                makespan = max(makespan, finish_time)\n                \n                # Add the thief's next availability event to the queue.\n                heapq.heappush(event_queue, (finish_time, worker_id))\n            # If no work is left anywhere, the worker becomes idle and is not\n            # pushed back onto the event queue, naturally ending its work life.\n\n    # Post-simulation metric calculations.\n    T_steal = int(makespan)\n    \n    T_base = max(W) * t_u if W else 0\n    \n    # S = T_base / T_steal\n    S = T_base / T_steal if T_steal > 0 else 1.0\n    \n    # R = (total stolen tiles) / (total initial tiles)\n    R = total_stolen_tiles / total_initial_tiles if total_initial_tiles > 0 else 0.0\n\n    return (T_steal, S, R)\n\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (W, c, l_s, t_u)\n        ([800, 50, 0, 50], 20, 40, 1),\n        ([200, 200, 200, 200], 50, 5, 1),\n        ([0, 0, 0, 1000], 10, 100, 1),\n        ([300, 10, 10, 10], 64, 30, 1),\n        ([101, 0, 101, 0], 1, 1, 1),\n    ]\n\n    P = 4 # Number of workers is fixed for all test cases.\n    \n    all_results = []\n    for case_params in test_cases:\n        W, c, l_s, t_u = case_params\n        # Run the simulation for the current case.\n        result_tuple = run_simulation(P, W, c, l_s, t_u)\n        # Append the flattened results to the final list.\n        all_results.extend(result_tuple)\n\n    # Final print statement in the exact required format.\n    # Note: The problem asks for the final answer to be wrapped in brackets\n    # and comma-separated, which implies a direct print of the list.\n    # The example output shows floats, so str() conversion should be fine.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```", "id": "3431949"}]}