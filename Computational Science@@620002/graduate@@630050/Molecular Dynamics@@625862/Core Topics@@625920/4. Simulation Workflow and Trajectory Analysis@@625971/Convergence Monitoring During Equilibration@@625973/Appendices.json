{"hands_on_practices": [{"introduction": "The total length of a simulation is often a deceptive measure of its statistical power. Because consecutive snapshots in a molecular dynamics trajectory are correlated, the number of truly independent samples is much smaller than the total number of frames. This exercise [@problem_id:3405213] introduces the crucial concept of the effective sample size ($N_{\\text{eff}}$), which quantifies the number of independent data points your simulation has produced. By deriving $N_{\\text{eff}}$ from the integrated autocorrelation time, you will gain a fundamental tool for estimating statistical error and planning simulation lengths required to achieve a desired level of precision.", "problem": "A Molecular Dynamics (MD) equilibration run generates a correlated time series of an observable (for example, the potential energy) sampled at uniform interval $\\Delta t$. Let $X_{i}$ denote the sample at time $t_{i} = i \\Delta t$, and assume the process is stationary after discarding an initial transient. Define the normalized autocorrelation function $\\rho(t)$ and the Integrated Autocorrelation Time (IAT) $\\tau_{\\mathrm{int}}$ via the standard time-domain definition that uses the sum (or integral) of $\\rho(t)$ over nonnegative lags. Starting from the definition of the sample mean $\\bar{X} = \\frac{1}{N} \\sum_{i=1}^{N} X_{i}$ and the variance of $\\bar{X}$ for correlated data expressed in terms of the autocovariance function, derive an expression for the effective sample size $N_{\\mathrm{eff}}$ in terms of $N$, $\\Delta t$, and $\\tau_{\\mathrm{int}}$ by equating $\\mathrm{Var}(\\bar{X})$ of the correlated series to $\\sigma^{2}/N_{\\mathrm{eff}}$, where $\\sigma^{2}$ is the variance of the underlying stationary process. Then, consider an MD run where the normalized autocorrelation function is well described by a single-exponential model $\\rho(t) = \\exp\\!\\left(-t/\\tau_{c}\\right)$, for which the IAT equals the correlation time, $\\tau_{\\mathrm{int}} = \\tau_{c}$. Suppose the sampling interval is $\\Delta t = 2\\,\\mathrm{fs}$, the current production segment has length $T_{\\mathrm{cur}} = 3.000\\,\\mathrm{ns}$, and a reliable analysis of the equilibrated portion yields $\\tau_{c} = 18.7\\,\\mathrm{ps}$. You wish to monitor convergence during equilibration by using a stopping rule that requires achieving a target effective sample size $N_{\\mathrm{eff}}^{\\star} = 200$. Based on your derived expression and the exponential model, compute the minimal additional production time $\\Delta T_{\\mathrm{add}}$ needed beyond $T_{\\mathrm{cur}}$ to reach $N_{\\mathrm{eff}}^{\\star}$. Round your final numerical answer to four significant figures and express it in nanoseconds (ns). In your derivation, start from the fundamental definitions of autocorrelation and the variance of the sample mean for correlated observations; do not assume any shortcut formulas.", "solution": "The problem asks for two main tasks: first, to derive an expression for the effective sample size $N_{\\mathrm{eff}}$ from fundamental principles, and second, to apply this expression to calculate the additional simulation time required to reach a target effective sample size.\n\n**Part 1: Derivation of the Effective Sample Size, $N_{\\mathrm{eff}}$**\n\nLet $X_i$ be the value of a stationary observable sampled at time $t_i = i \\Delta t$ for $i=1, 2, \\dots, N$. The sample mean, $\\bar{X}$, is defined as:\n$$\n\\bar{X} = \\frac{1}{N} \\sum_{i=1}^{N} X_{i}\n$$\nThe variance of the sample mean is given by:\n$$\n\\mathrm{Var}(\\bar{X}) = \\mathrm{Var}\\left(\\frac{1}{N} \\sum_{i=1}^{N} X_{i}\\right) = \\frac{1}{N^2} \\mathrm{Var}\\left(\\sum_{i=1}^{N} X_{i}\\right)\n$$\nUsing the property of variance for a sum of random variables, we have:\n$$\n\\mathrm{Var}\\left(\\sum_{i=1}^{N} X_{i}\\right) = \\sum_{i=1}^{N} \\sum_{j=1}^{N} \\mathrm{Cov}(X_i, X_j)\n$$\nFor a stationary process, the covariance $\\mathrm{Cov}(X_i, X_j)$ depends only on the time lag $|t_i - t_j| = |i-j|\\Delta t$. We define the autocovariance function at a discrete lag of $k$ steps as $\\gamma_k = \\mathrm{Cov}(X_i, X_{i+k})$. The variance of the process is $\\sigma^2 = \\mathrm{Var}(X_i) = \\gamma_0$. Due to stationarity, $\\gamma_k = \\gamma_{-k}$. We can rewrite the double summation by grouping terms with the same lag $k = |i-j|$:\n$$\n\\sum_{i=1}^{N} \\sum_{j=1}^{N} \\mathrm{Cov}(X_i, X_j) = \\sum_{i=1}^{N} \\sum_{j=1}^{N} \\gamma_{|i-j|} = \\sum_{k=-(N-1)}^{N-1} (N-|k|) \\gamma_k\n$$\nThis can be split into terms for $k=0$, $k>0$, and $k<0$:\n$$\n\\sum_{k=-(N-1)}^{N-1} (N-|k|) \\gamma_k = (N-0)\\gamma_0 + \\sum_{k=1}^{N-1} (N-k)\\gamma_k + \\sum_{k=-(N-1)}^{-1} (N-|k|)\\gamma_k\n$$\nUsing $\\gamma_k = \\gamma_{-k}$, the expression becomes:\n$$\nN\\gamma_0 + 2\\sum_{k=1}^{N-1} (N-k)\\gamma_k\n$$\nSubstituting this back into the expression for $\\mathrm{Var}(\\bar{X})$:\n$$\n\\mathrm{Var}(\\bar{X}) = \\frac{1}{N^2} \\left[ N\\gamma_0 + 2\\sum_{k=1}^{N-1} (N-k)\\gamma_k \\right] = \\frac{\\gamma_0}{N} \\left[ 1 + \\frac{2}{N}\\sum_{k=1}^{N-1} (N-k)\\frac{\\gamma_k}{\\gamma_0} \\right]\n$$\nThe normalized autocorrelation function (ACF) at lag $k$ is $\\rho_k = \\gamma_k / \\gamma_0$. Substituting this and $\\sigma^2=\\gamma_0$:\n$$\n\\mathrm{Var}(\\bar{X}) = \\frac{\\sigma^2}{N} \\left[ 1 + 2\\sum_{k=1}^{N-1} \\left(1-\\frac{k}{N}\\right)\\rho_k \\right]\n$$\nThis expression is exact. For a typical MD simulation where the total time is much longer than the correlation time, the ACF $\\rho_k$ decays to zero for $k \\ll N$. In this limit, we can make two approximations:\n1.  For values of $k$ where $\\rho_k$ is non-negligible, $k/N \\ll 1$, so $(1-k/N) \\approx 1$.\n2.  The upper limit of the summation can be extended from $N-1$ to $\\infty$ since $\\rho_k \\approx 0$ for large $k$.\n\nWith these approximations, the variance becomes:\n$$\n\\mathrm{Var}(\\bar{X}) \\approx \\frac{\\sigma^2}{N} \\left( 1 + 2\\sum_{k=1}^{\\infty} \\rho_k \\right)\n$$\nThe problem defines the Integrated Autocorrelation Time (IAT), $\\tau_{\\mathrm{int}}$, using the continuous-time ACF, $\\rho(t)$:\n$$\n\\tau_{\\mathrm{int}} = \\int_0^\\infty \\rho(t) dt\n$$\nWe can relate the discrete sum $\\sum \\rho_k$ to this integral. The integral can be approximated by a sum using the trapezoidal rule with a step size of $\\Delta t$:\n$$\n\\tau_{\\mathrm{int}} = \\int_0^\\infty \\rho(t) dt \\approx \\Delta t \\left[ \\frac{1}{2}\\rho(0) + \\sum_{k=1}^{\\infty} \\rho(k \\Delta t) \\right]\n$$\nSince $\\rho(0)=1$ by definition and $\\rho_k = \\rho(k \\Delta t)$, we get:\n$$\n\\tau_{\\mathrm{int}} \\approx \\Delta t \\left( \\frac{1}{2} + \\sum_{k=1}^{\\infty} \\rho_k \\right)\n$$\nSolving for the summation term:\n$$\n\\sum_{k=1}^{\\infty} \\rho_k \\approx \\frac{\\tau_{\\mathrm{int}}}{\\Delta t} - \\frac{1}{2}\n$$\nNow, substitute this back into our approximate expression for $\\mathrm{Var}(\\bar{X})$:\n$$\n\\mathrm{Var}(\\bar{X}) \\approx \\frac{\\sigma^2}{N} \\left( 1 + 2 \\left( \\frac{\\tau_{\\mathrm{int}}}{\\Delta t} - \\frac{1}{2} \\right) \\right) = \\frac{\\sigma^2}{N} \\left( 1 + \\frac{2\\tau_{\\mathrm{int}}}{\\Delta t} - 1 \\right) = \\frac{\\sigma^2}{N} \\left( \\frac{2\\tau_{\\mathrm{int}}}{\\Delta t} \\right)\n$$\nThe problem defines the effective sample size, $N_{\\mathrm{eff}}$, by equating the variance of the correlated mean to the variance of a mean of $N_{\\mathrm{eff}}$ independent samples:\n$$\n\\mathrm{Var}(\\bar{X}) = \\frac{\\sigma^2}{N_{\\mathrm{eff}}}\n$$\nBy comparing the two expressions for $\\mathrm{Var}(\\bar{X})$:\n$$\n\\frac{\\sigma^2}{N_{\\mathrm{eff}}} = \\frac{\\sigma^2}{N} \\frac{2\\tau_{\\mathrm{int}}}{\\Delta t}\n$$\nSolving for $N_{\\mathrm{eff}}$ gives:\n$$\nN_{\\mathrm{eff}} = \\frac{N \\Delta t}{2\\tau_{\\mathrm{int}}}\n$$\nSince $T = N \\Delta t$ is the total simulation time, the final expression is:\n$$\nN_{\\mathrm{eff}} = \\frac{T}{2\\tau_{\\mathrm{int}}}\n$$\nThis is the desired expression for the effective sample size in terms of $N$, $\\Delta t$, and $\\tau_{\\mathrm{int}}$ (or $T$ and $\\tau_{\\mathrm{int}}$).\n\n**Part 2: Calculation of the Additional Production Time, $\\Delta T_{\\mathrm{add}}$**\n\nWe are given a single-exponential model for the ACF, $\\rho(t) = \\exp(-t/\\tau_c)$, for which the IAT is equal to the correlation time, $\\tau_{\\mathrm{int}} = \\tau_c$. Our derived formula for $N_{\\mathrm{eff}}$ becomes:\n$$\nN_{\\mathrm{eff}} = \\frac{T}{2\\tau_c}\n$$\nThe goal is to reach a target effective sample size of $N_{\\mathrm{eff}}^{\\star} = 200$. Let $T_{\\mathrm{total}}$ be the total simulation time required to achieve this.\n$$\nN_{\\mathrm{eff}}^{\\star} = \\frac{T_{\\mathrm{total}}}{2\\tau_c}\n$$\nWe can solve for $T_{\\mathrm{total}}$:\n$$\nT_{\\mathrm{total}} = 2 N_{\\mathrm{eff}}^{\\star} \\tau_c\n$$\nThe given values are:\n-   $N_{\\mathrm{eff}}^{\\star} = 200$\n-   $\\tau_c = 18.7\\,\\mathrm{ps}$\n\nSubstituting these values:\n$$\nT_{\\mathrm{total}} = 2 \\times 200 \\times 18.7\\,\\mathrm{ps} = 400 \\times 18.7\\,\\mathrm{ps} = 7480\\,\\mathrm{ps}\n$$\nTo express this in nanoseconds, we use the conversion $1\\,\\mathrm{ns} = 1000\\,\\mathrm{ps}$:\n$$\nT_{\\mathrm{total}} = 7480\\,\\mathrm{ps} \\times \\frac{1\\,\\mathrm{ns}}{1000\\,\\mathrm{ps}} = 7.480\\,\\mathrm{ns}\n$$\nThe current production segment has a length of $T_{\\mathrm{cur}} = 3.000\\,\\mathrm{ns}$. The minimal additional production time, $\\Delta T_{\\mathrm{add}}$, is the difference between the required total time and the current time:\n$$\n\\Delta T_{\\mathrm{add}} = T_{\\mathrm{total}} - T_{\\mathrm{cur}}\n$$\n$$\n\\Delta T_{\\mathrm{add}} = 7.480\\,\\mathrm{ns} - 3.000\\,\\mathrm{ns} = 4.480\\,\\mathrm{ns}\n$$\nThe problem requires the final answer to be rounded to four significant figures. Our result, $4.480\\,\\mathrm{ns}$, is already in this form. The sampling interval $\\Delta t = 2\\,\\mathrm{fs}$ was necessary for the theoretical derivation but not for the final numerical calculation once the formula $N_{\\mathrm{eff}} = T/(2\\tau_{\\mathrm{int}})$ was established.", "answer": "$$\n\\boxed{4.480}\n$$", "id": "3405213"}, {"introduction": "While analyzing a single long trajectory is informative, a more rigorous test of convergence involves comparing multiple independent simulations started from different initial conditions. This practice [@problem_id:3405203] delves into the Potential Scale Reduction Factor (PSRF), or $\\hat{R}$ statistic, a cornerstone of modern convergence diagnostics. By calculating both the univariate and multivariate versions of $\\hat{R}$, you will learn to formally assess whether your parallel simulations have forgotten their starting points and are sampling from the same underlying equilibrium distribution.", "problem": "A molecular dynamics equilibration is monitored using $M$ independent replicate trajectories (started from the same configuration with independently randomized velocities) to assess convergence of multiple observables. Suppose two observables are recorded every fixed time interval over the last segment of each trajectory of length $n$ samples, producing $M=3$ chains with $n=20$ samples per chain for each observable. Denote by $\\boldsymbol{\\mu}_m \\in \\mathbb{R}^2$ the sample mean vector of chain $m$ for the two observables and by $\\mathbf{S}_m \\in \\mathbb{R}^{2 \\times 2}$ the unbiased within-chain sample covariance matrix of chain $m$. The three replicate chains yield the following summaries over the final analysis window:\n- Chain means:\n$$\n\\boldsymbol{\\mu}_1=\\begin{pmatrix}0.50 \\\\ 1.00\\end{pmatrix},\\quad\n\\boldsymbol{\\mu}_2=\\begin{pmatrix}0.62 \\\\ 0.96\\end{pmatrix},\\quad\n\\boldsymbol{\\mu}_3=\\begin{pmatrix}0.54 \\\\ 1.06\\end{pmatrix}.\n$$\n- Within-chain covariance matrices:\n$$\n\\mathbf{S}_1=\\begin{pmatrix}0.050 & 0.012 \\\\ 0.012 & 0.042\\end{pmatrix},\\quad\n\\mathbf{S}_2=\\begin{pmatrix}0.048 & 0.009 \\\\ 0.009 & 0.039\\end{pmatrix},\\quad\n\\mathbf{S}_3=\\begin{pmatrix}0.052 & 0.011 \\\\ 0.011 & 0.041\\end{pmatrix}.\n$$\nAssume all chains are targeting the same stationary distribution of the observables in the equilibrated regime, and that the replicates are mutually independent. Starting from first principles of variance decomposition across and within independent replicates and the law of large numbers, derive expressions for:\n1) The univariate Potential Scale Reduction Factor (PSRF, $\\hat{R}$) for each observable separately, using the ratio of a pooled estimator of the marginal variance to the average within-chain variance.\n2) The multivariate PSRF (denoted $\\hat{R}_{\\mathrm{multi}}$) for the two-dimensional observable, defined via the largest generalized eigenvalue of the pooled variance estimator relative to the pooled within-chain covariance.\n\nThen compute the numerical values of the univariate and multivariate $\\hat{R}$ for the given data. Based on asymptotic consistency arguments, specify principled stopping thresholds and trend requirements over time (e.g., across successive, non-overlapping windows of the last $n$ samples) that would justify declaring convergence during equilibration monitoring in molecular dynamics. Your threshold specifications must be dimensionless and justified from the variance ratio interpretation.\n\nFinally, report the multivariate $\\hat{R}_{\\mathrm{multi}}$ computed for the data above as your answer. Express the final value as a dimensionless number and round your answer to four significant figures.", "solution": "The problem requires the derivation and computation of the univariate and multivariate Potential Scale Reduction Factor (PSRF), denoted $\\hat{R}$, as a diagnostic for convergence in molecular dynamics simulations.\n\nLet $M$ be the number of independent replicate trajectories and $n$ be the number of samples in the analysis window for each trajectory. We are given $M=3$ and $n=20$. For each trajectory $m \\in \\{1, \\dots, M\\}$, we have the sample mean vector $\\boldsymbol{\\mu}_m$ and the sample covariance matrix $\\mathbf{S}_m$ for two observables.\n\n**1. Univariate Potential Scale Reduction Factor ($\\hat{R}$)**\n\nLet us consider a single observable, indexed by $k \\in \\{1, 2\\}$. The corresponding component of the mean vector $\\boldsymbol{\\mu}_m$ is $\\mu_{m,k}$, and the corresponding diagonal element of the covariance matrix $\\mathbf{S}_m$ is $S_{m,kk}$, which is the unbiased sample variance of the observable in chain $m$.\n\nThe analysis is based on the decomposition of the total variance into within-chain and between-chain components.\n\nThe average within-chain variance, $W_k$, is the mean of the individual chain variances:\n$$W_k = \\frac{1}{M} \\sum_{m=1}^{M} S_{m,kk}$$\n\nThe between-chain variance, $B_k$, measures the variance of the sample means across the chains, scaled by the sample size $n$:\n$$B_k = \\frac{n}{M-1} \\sum_{m=1}^{M} (\\mu_{m,k} - \\bar{\\mu}_k)^2$$\nwhere $\\bar{\\mu}_k = \\frac{1}{M} \\sum_{m=1}^{M} \\mu_{m,k}$ is the grand mean for observable $k$ across all chains.\n\nA pooled estimator for the marginal variance of the observable, $\\hat{V}_k$, combines these two variance components. It is a weighted average of the within-chain variance and the variance of the chain means:\n$$\\hat{V}_k = \\frac{n-1}{n} W_k + \\frac{1}{n} B_k$$\nThe factor $\\frac{n-1}{n}$ corrects for the fact that $W_k$ is an estimate from finite samples. When the chains have converged to the stationary distribution, both $W_k$ and $B_k/n$ estimate components of the total variance. If there is a discrepancy (e.g., chains are sampling different regions), the between-chain variance will be inflated.\n\nThe Potential Scale Reduction Factor, $\\hat{R}_k$, is the square root of the ratio of the pooled variance estimate to the average within-chain variance:\n$$\\hat{R}_k = \\sqrt{\\frac{\\hat{V}_k}{W_k}} = \\sqrt{\\frac{\\frac{n-1}{n} W_k + \\frac{1}{n} B_k}{W_k}} = \\sqrt{\\frac{n-1}{n} + \\frac{B_k}{nW_k}}$$\nIf the chains have converged, this ratio should be close to $1$.\n\n**Calculations for Univariate $\\hat{R}$:**\n\nFor observable $1$:\n$\\mu_{1,1}=0.50$, $\\mu_{2,1}=0.62$, $\\mu_{3,1}=0.54$.\n$\\bar{\\mu}_1 = \\frac{0.50+0.62+0.54}{3} = \\frac{1.66}{3}$.\n$B_1 = \\frac{20}{3-1} \\left[ \\left(0.50 - \\frac{1.66}{3}\\right)^2 + \\left(0.62 - \\frac{1.66}{3}\\right)^2 + \\left(0.54 - \\frac{1.66}{3}\\right)^2 \\right] = 10 \\left[ \\left(-\\frac{0.16}{3}\\right)^2 + \\left(\\frac{0.20}{3}\\right)^2 + \\left(-\\frac{0.04}{3}\\right)^2 \\right] = \\frac{10}{9}(0.0256 + 0.0400 + 0.0016) = \\frac{0.672}{9} \\approx 0.07467$.\n$S_{1,11}=0.050$, $S_{2,11}=0.048$, $S_{3,11}=0.052$.\n$W_1 = \\frac{0.050+0.048+0.052}{3} = \\frac{0.150}{3} = 0.050$.\n$\\hat{R}_1 = \\sqrt{\\frac{19}{20} + \\frac{0.07467}{20 \\times 0.050}} = \\sqrt{0.95 + 0.07467} = \\sqrt{1.02467} \\approx 1.0122$.\n\nFor observable $2$:\n$\\mu_{1,2}=1.00$, $\\mu_{2,2}=0.96$, $\\mu_{3,2}=1.06$.\n$\\bar{\\mu}_2 = \\frac{1.00+0.96+1.06}{3} = \\frac{3.02}{3}$.\n$B_2 = \\frac{20}{3-1} \\left[ \\left(1.00 - \\frac{3.02}{3}\\right)^2 + \\left(0.96 - \\frac{3.02}{3}\\right)^2 + \\left(1.06 - \\frac{3.02}{3}\\right)^2 \\right] = 10 \\left[ \\left(-\\frac{0.02}{3}\\right)^2 + \\left(-\\frac{0.14}{3}\\right)^2 + \\left(\\frac{0.16}{3}\\right)^2 \\right] = \\frac{10}{9}(0.0004 + 0.0196 + 0.0256) = \\frac{0.456}{9} \\approx 0.05067$.\n$S_{1,22}=0.042$, $S_{2,22}=0.039$, $S_{3,22}=0.041$.\n$W_2 = \\frac{0.042+0.039+0.041}{3} = \\frac{0.122}{3}$.\n$\\hat{R}_2 = \\sqrt{\\frac{19}{20} + \\frac{0.05067}{20 \\times (0.122/3)}} = \\sqrt{0.95 + \\frac{0.15201}{2.44}} \\approx \\sqrt{0.95 + 0.0623} = \\sqrt{1.0123} \\approx 1.0061$.\n\n**2. Multivariate Potential Scale Reduction Factor ($\\hat{R}_{\\mathrm{multi}}$)**\n\nThe univariate analysis is extended to the $p=2$ dimensional case by replacing scalar variances with covariance matrices.\n\nThe pooled within-chain covariance matrix, $\\mathbf{W}$, is the average of the chain covariance matrices:\n$$\\mathbf{W} = \\frac{1}{M} \\sum_{m=1}^{M} \\mathbf{S}_m$$\n\nThe between-chain covariance matrix, $\\mathbf{B}$, is:\n$$\\mathbf{B} = \\frac{n}{M-1} \\sum_{m=1}^{M} (\\boldsymbol{\\mu}_m - \\bar{\\boldsymbol{\\mu}})(\\boldsymbol{\\mu}_m - \\bar{\\boldsymbol{\\mu}})^T$$\nwhere $\\bar{\\boldsymbol{\\mu}} = \\frac{1}{M} \\sum_{m=1}^{M} \\boldsymbol{\\mu}_m$ is the grand mean vector.\n\nThe pooled estimator for the marginal covariance matrix, $\\hat{\\mathbf{V}}$, is:\n$$\\hat{\\mathbf{V}} = \\frac{n-1}{n} \\mathbf{W} + \\frac{1}{n} \\mathbf{B}$$\n\nThe multivariate PSRF, $\\hat{R}_{\\mathrm{multi}}$, is defined via the largest generalized eigenvalue, $\\lambda_{\\max}$, of $\\hat{\\mathbf{V}}$ with respect to $\\mathbf{W}$. This eigenvalue is the solution to the generalized eigenvalue problem $\\hat{\\mathbf{V}}\\mathbf{v} = \\lambda\\mathbf{W}\\mathbf{v}$.\nSubstituting the expression for $\\hat{\\mathbf{V}}$:\n$$\\left(\\frac{n-1}{n} \\mathbf{W} + \\frac{1}{n} \\mathbf{B}\\right)\\mathbf{v} = \\lambda\\mathbf{W}\\mathbf{v}$$\nAssuming $\\mathbf{W}$ is invertible, we can multiply by $\\mathbf{W}^{-1}$:\n$$\\left(\\frac{n-1}{n} \\mathbf{I} + \\frac{1}{n} \\mathbf{W}^{-1}\\mathbf{B}\\right)\\mathbf{v} = \\lambda\\mathbf{v}$$\nThis is a standard eigenvalue problem. The eigenvalues $\\lambda$ are related to the eigenvalues $\\eta$ of the matrix $\\mathbf{W}^{-1}\\mathbf{B}$ by $\\lambda = \\frac{n-1}{n} + \\frac{\\eta}{n}$.\nThe multivariate PSRF is the square root of the largest eigenvalue, $\\lambda_{\\max}$:\n$$\\hat{R}_{\\mathrm{multi}} = \\sqrt{\\lambda_{\\max}} = \\sqrt{\\frac{n-1}{n} + \\frac{\\eta_{\\max}}{n}}$$\nwhere $\\eta_{\\max}$ is the largest eigenvalue of $\\mathbf{W}^{-1}\\mathbf{B}$.\n\n**Calculations for Multivariate $\\hat{R}_{\\mathrm{multi}}$:**\n\nGrand mean vector:\n$\\bar{\\boldsymbol{\\mu}} = \\frac{1}{3} \\left( \\begin{pmatrix}0.50 \\\\ 1.00\\end{pmatrix} + \\begin{pmatrix}0.62 \\\\ 0.96\\end{pmatrix} + \\begin{pmatrix}0.54 \\\\ 1.06\\end{pmatrix} \\right) = \\frac{1}{3} \\begin{pmatrix}1.66 \\\\ 3.02\\end{pmatrix}$.\n\nPooled within-chain covariance matrix $\\mathbf{W}$:\n$\\mathbf{W} = \\frac{1}{3} \\left( \\begin{pmatrix}0.050 & 0.012 \\\\ 0.012 & 0.042\\end{pmatrix} + \\begin{pmatrix}0.048 & 0.009 \\\\ 0.009 & 0.039\\end{pmatrix} + \\begin{pmatrix}0.052 & 0.011 \\\\ 0.011 & 0.041\\end{pmatrix} \\right) = \\frac{1}{3} \\begin{pmatrix}0.150 & 0.032 \\\\ 0.032 & 0.122\\end{pmatrix}$.\n\nBetween-chain covariance matrix $\\mathbf{B}$:\nThe deviation vectors are $\\boldsymbol{\\mu}_1 - \\bar{\\boldsymbol{\\mu}} = \\frac{1}{3}\\begin{pmatrix}-0.16 \\\\ -0.02\\end{pmatrix}$, $\\boldsymbol{\\mu}_2 - \\bar{\\boldsymbol{\\mu}} = \\frac{1}{3}\\begin{pmatrix}0.20 \\\\ -0.14\\end{pmatrix}$, $\\boldsymbol{\\mu}_3 - \\bar{\\boldsymbol{\\mu}} = \\frac{1}{3}\\begin{pmatrix}-0.04 \\\\ 0.16\\end{pmatrix}$.\n$\\mathbf{B} = \\frac{20}{3-1} \\sum_{m=1}^{3} (\\boldsymbol{\\mu}_m - \\bar{\\boldsymbol{\\mu}})(\\boldsymbol{\\mu}_m - \\bar{\\boldsymbol{\\mu}})^T = \\frac{10}{9} \\left( \\begin{pmatrix}-0.16 \\\\ -0.02\\end{pmatrix}\\begin{pmatrix}-0.16 & -0.02\\end{pmatrix} + \\dots \\right)$\n$\\mathbf{B} = \\frac{10}{9} \\begin{pmatrix} 0.0672 & -0.0312 \\\\ -0.0312 & 0.0456 \\end{pmatrix} \\approx \\begin{pmatrix} 0.07467 & -0.03467 \\\\ -0.03467 & 0.05067 \\end{pmatrix}$.\n\nNext, we compute $\\mathbf{W}^{-1}\\mathbf{B}$.\n$\\det(\\mathbf{W}) = \\frac{1}{9}(0.150 \\times 0.122 - 0.032^2) = \\frac{1}{9}(0.0183 - 0.001024) = \\frac{0.017276}{9}$.\n$\\mathbf{W}^{-1} = \\frac{9}{0.017276} \\frac{1}{3} \\begin{pmatrix} 0.122 & -0.032 \\\\ -0.032 & 0.150 \\end{pmatrix} = \\frac{3}{0.017276} \\begin{pmatrix} 0.122 & -0.032 \\\\ -0.032 & 0.150 \\end{pmatrix}$.\n$\\mathbf{W}^{-1}\\mathbf{B} = \\frac{3}{0.017276} \\begin{pmatrix} 0.122 & -0.032 \\\\ -0.032 & 0.150 \\end{pmatrix} \\frac{10}{9} \\begin{pmatrix} 0.0672 & -0.0312 \\\\ -0.0312 & 0.0456 \\end{pmatrix} = \\frac{10}{3 \\times 0.017276} \\begin{pmatrix} 0.0091968 & -0.0052656 \\\\ -0.0068304 & 0.0078384 \\end{pmatrix}$.\nThe pre-factor is $\\frac{10}{0.051828} \\approx 192.943$.\n$\\mathbf{W}^{-1}\\mathbf{B} \\approx 192.943 \\begin{pmatrix} 0.0091968 & -0.0052656 \\\\ -0.0068304 & 0.0078384 \\end{pmatrix} \\approx \\begin{pmatrix} 1.7745 & -1.0159 \\\\ -1.3178 & 1.5123 \\end{pmatrix}$.\nThe eigenvalues $\\eta$ of this matrix are the roots of the characteristic equation $\\eta^2 - \\text{tr}(\\mathbf{W}^{-1}\\mathbf{B})\\eta + \\det(\\mathbf{W}^{-1}\\mathbf{B}) = 0$.\n$\\text{tr}(\\mathbf{W}^{-1}\\mathbf{B}) \\approx 1.7745 + 1.5123 = 3.2868$.\n$\\det(\\mathbf{W}^{-1}\\mathbf{B}) \\approx (1.7745)(1.5123) - (-1.0159)(-1.3178) \\approx 2.6835 - 1.3388 = 1.3447$.\n$\\eta^2 - 3.2868\\eta + 1.3447 = 0$.\n$\\eta = \\frac{3.2868 \\pm \\sqrt{3.2868^2 - 4(1.3447)}}{2} = \\frac{3.2868 \\pm \\sqrt{10.803 - 5.3788}}{2} = \\frac{3.2868 \\pm \\sqrt{5.4242}}{2} = \\frac{3.2868 \\pm 2.3290}{2}$.\nThe eigenvalues are $\\eta_1 \\approx 2.8079$ and $\\eta_2 \\approx 0.4789$. The largest is $\\eta_{\\max} \\approx 2.8079$.\nFinally, we compute $\\hat{R}_{\\mathrm{multi}}$:\n$\\hat{R}_{\\mathrm{multi}} = \\sqrt{\\frac{20-1}{20} + \\frac{\\eta_{\\max}}{20}} = \\sqrt{0.95 + \\frac{2.8079}{20}} = \\sqrt{0.95 + 0.140395} = \\sqrt{1.090395} \\approx 1.04422$.\n\n**3. Convergence Thresholds and Trend Requirements**\n\nThe PSRF, both univariate and multivariate, provides a dimensionless measure of convergence. The square of the PSRF, $\\hat{R}^2$, estimates the factor by which the marginal variance is overestimated relative to the within-chain variance. An ideal value is $\\hat{R}=1.0$, indicating that between-chain and within-chain variations are consistent.\n\n**Stopping Thresholds**: A commonly accepted, though problem-dependent, threshold for declaring convergence is when the PSRF for all monitored observables falls below a value such as $1.1$. More stringent applications might require a threshold of $1.05$ or $1.01$. For the multivariate case, $\\hat{R}_{\\mathrm{multi}} < 1.1$ is the corresponding criterion. A value of $\\hat{R}=1.1$ implies a potential scale reduction of $10\\%$, as the estimated variance of the observable is inflated by a factor of $1.1^2 \\approx 1.21$ due to incomplete convergence.\n\n**Trend Requirements**: Achieving the threshold at a single point in time is insufficient. True convergence requires stability. The PSRF should be monitored over successive, non-overlapping time windows of the simulation. A robust criterion for stopping the equilibration phase is that the PSRF, computed over these windows, must not only consistently remain below the chosen threshold but also exhibit no significant increasing trend. A stable or decreasing PSRF value over a substantial period suggests that the chains are reliably sampling from the same stationary distribution. Visual inspection of a plot of $\\hat{R}$ versus simulation time is standard practice to confirm this stability.\n\nRounding the requested final answer to four significant figures gives $1.044$.", "answer": "$$\n\\boxed{1.044}\n$$", "id": "3405203"}, {"introduction": "Observing slow relaxation in an observable does not always imply a slow physical process within your system. The algorithms used to control temperature and pressure themselves have characteristic timescales that can introduce artificial dynamics and slow down convergence. This exercise [@problem_id:3405236] challenges you to think like a simulation detective, using a series of hypothetical experimental results to disentangle intrinsic physical relaxation from algorithmic artifacts. Mastering this skill is essential for setting up robust simulations and correctly interpreting their results.", "problem": "A solvated biomolecular system of $N$ particles is equilibrated by molecular dynamics governed by Newtonâ€™s Second Law, $m_i \\, d^2 \\mathbf{r}_i/dt^2 = - \\nabla_i U(\\mathbf{r}_1,\\dots,\\mathbf{r}_N)$, with the addition of standard coupling terms for a thermostat and a barostat to sample the appropriate statistical ensembles. In the weak-perturbation, near-equilibrium regime, linear response theory and the Onsager regression hypothesis imply that small deviations of a macroscopic observable $X(t)$ from its equilibrium value $X_{\\mathrm{eq}}$ relax exponentially. When two independent relaxation channels act in parallel on the same scalar observable (for example, intrinsic physical energy exchange within the system and algorithmic energy exchange with a thermostat), the net rate is expected to be the sum of the individual rates. Similarly, for volume fluctuations under a barostat, the cell degrees of freedom couple to hydrodynamic modes; in isotropic compressible fluids, small volume strain $\\epsilon(t) = \\Delta V(t)/V_0$ can be modeled in the simplest case by a damped harmonic oscillator representing a Parrinelloâ€“Rahman-type extended variable, $W \\, d^2 \\epsilon/dt^2 + \\gamma_P \\, d \\epsilon/dt + K_T \\, \\epsilon = 0$, where $W$ is an inertia-like parameter, $\\gamma_P$ is a damping coefficient related to the barostat, and $K_T$ is the isothermal bulk modulus, while the physical fluid supports acoustic modes with characteristic travel time $L/c_s$ across a box of linear size $L$ and speed of sound $c_s$.\n\nYou are given the following controlled experiments performed during equilibration:\n\n- The kinetic temperature $T(t)$ is monitored in constant Number-Volume-Temperature (NVT) simulations using a stochastic thermostat characterized by a relaxation time $\\tau_T$. When $\\tau_T = 0.5 \\, \\mathrm{ps}$, the fitted exponential approach of $T(t)$ to its target yields an observed relaxation time $\\tau_{\\mathrm{obs}}^{(T)} = 0.4 \\, \\mathrm{ps}$. When $\\tau_T = 5.0 \\, \\mathrm{ps}$, the fitted observed relaxation time is $\\tau_{\\mathrm{obs}}^{(T)} = 1.4286 \\, \\mathrm{ps}$.\n\n- The instantaneous volume $V(t)$ is monitored in constant Number-Pressure-Temperature (NPT) simulations using an isotropic barostat. The cubic box has linear size $L = 10 \\, \\mathrm{nm}$, and the speed of sound in the solvent is $c_s = 1500 \\, \\mathrm{m/s}$. With a strongly damped barostat whose effective damping time is set to $\\tau_P = 50 \\, \\mathrm{ps}$, the volume autocorrelation function decays monotonically with an observed exponential time of approximately $\\tau_{\\mathrm{obs}}^{(V)} \\approx 8 \\, \\mathrm{ps}$. With a more aggressive barostat, $\\tau_P = 1 \\, \\mathrm{ps}$, the volume exhibits underdamped oscillations with a dominant period comparable to the acoustic travel time across the box and a slower envelope decay.\n\nBased on first principles and these measurements, choose the statement(s) that correctly explain how thermostat relaxation time and barostat damping interact with intrinsic system relaxation times to produce apparent slow convergence, and that outline a scientifically sound protocol to disentangle algorithmic from physical timescales:\n\nA. In the linear-response regime for a scalar observable like the kinetic temperature, independent physical and thermostat relaxation channels act in parallel, leading to an additive rate law $1/\\tau_{\\mathrm{obs}} = 1/\\tau_{\\mathrm{phys}} + 1/\\tau_T$. This can be validated by scanning $\\tau_T$ and fitting exponential decays of $T(t)$; a plot of $1/\\tau_{\\mathrm{obs}}$ versus $1/\\tau_T$ should be a straight line with slope $1$ and intercept $1/\\tau_{\\mathrm{phys}}$, and the measured data are consistent with a constant $\\tau_{\\mathrm{phys}}$.\n\nB. For a Parrinelloâ€“Rahman barostat, choosing $\\tau_P$ much smaller than the acoustic period across the simulation cell always decreases the volume relaxation time without side effects, because the barostat directly suppresses density waves.\n\nC. To disentangle algorithmic and physical timescales, a robust protocol is: first perform short constant Number-Volume-Energy (NVE) runs from slightly perturbed equilibrated states to measure intrinsic relaxation via autocorrelation functions (yielding estimates of $\\tau_{\\mathrm{phys}}$ and hydrodynamic times such as $L/c_s$); next, perform NVT and NPT sweeps of $\\tau_T$ and $\\tau_P$ over logarithmic grids, fit $1/\\tau_{\\mathrm{obs}}$ versus $1/\\tau_T$ for energy-related observables, inspect volume autocorrelations for underdamped oscillations when $\\tau_P$ approaches acoustic times, and choose coupling times that are long compared to the fastest physical modes but short compared to targeted slow structural relaxations. Use block averaging of thermodynamic observables to confirm convergence independent of algorithmic parameters.\n\nD. The correct equilibration strategy is to minimize both $\\tau_T$ and $\\tau_P$ until the instantaneous temperature and pressure match their targets within tolerance; this guarantees that any remaining slow convergence is purely physical because the thermostat and barostat have been made infinitely fast.\n\nE. When two independent relaxation channels act on the same observable, the slower channel dominates so that $\\tau_{\\mathrm{obs}} = \\max(\\tau_T, \\tau_{\\mathrm{phys}})$; therefore, scanning $\\tau_T$ cannot reveal $\\tau_{\\mathrm{phys}}$, and only changing system size can expose the physical timescale.\n\nSelect all correct options.", "solution": "The problem asks us to evaluate statements about the interplay between algorithmic coupling parameters ($\\tau_T, \\tau_P$) and intrinsic system dynamics. To do this, we must first analyze the provided experimental data within the given theoretical framework.\n\n**Analysis of Provided Data**\n\nFirst, let's analyze the data for temperature relaxation in the NVT simulations. The proposed model for parallel relaxation pathways is that the observed rate is the sum of the physical and algorithmic rates: $1/\\tau_{\\mathrm{obs}}^{(T)} = 1/\\tau_{\\mathrm{phys}} + 1/\\tau_T$.\nUsing the data from the first run ($\\tau_T = 0.5 \\, \\mathrm{ps}$, $\\tau_{\\mathrm{obs}}^{(T)} = 0.4 \\, \\mathrm{ps}$), we can solve for the intrinsic physical relaxation time, $\\tau_{\\mathrm{phys}}$:\n$$ \\frac{1}{0.4 \\, \\mathrm{ps}} = \\frac{1}{\\tau_{\\mathrm{phys}}} + \\frac{1}{0.5 \\, \\mathrm{ps}} \\implies 2.5 \\, \\mathrm{ps}^{-1} = \\frac{1}{\\tau_{\\mathrm{phys}}} + 2.0 \\, \\mathrm{ps}^{-1} $$\nThis gives $1/\\tau_{\\mathrm{phys}} = 0.5 \\, \\mathrm{ps}^{-1}$, which means $\\tau_{\\mathrm{phys}} = 2.0 \\, \\mathrm{ps}$.\nWe can check this result using the second run ($\\tau_T = 5.0 \\, \\mathrm{ps}$, $\\tau_{\\mathrm{obs}}^{(T)} \\approx 1.4286 \\, \\mathrm{ps}$):\n$$ \\frac{1}{\\tau_{\\mathrm{phys}}} + \\frac{1}{\\tau_T} = 0.5 \\, \\mathrm{ps}^{-1} + \\frac{1}{5.0 \\, \\mathrm{ps}} = 0.5 \\, \\mathrm{ps}^{-1} + 0.2 \\, \\mathrm{ps}^{-1} = 0.7 \\, \\mathrm{ps}^{-1} $$\nThe observed relaxation time corresponding to this rate is $1/(0.7 \\, \\mathrm{ps}^{-1}) \\approx 1.4286 \\, \\mathrm{ps}$, which matches the experimental data. The data are therefore perfectly consistent with the additive rate model for a constant $\\tau_{\\mathrm{phys}} = 2.0 \\, \\mathrm{ps}$.\n\nFor the volume relaxation in the NPT simulations, we can calculate the characteristic acoustic travel time across the box: $t_{acoustic} = L/c_s = (10 \\, \\mathrm{nm}) / (1500 \\, \\mathrm{m/s}) = (10 \\times 10^{-9} \\, \\mathrm{m}) / (1500 \\, \\mathrm{m/s}) \\approx 6.67 \\times 10^{-12} \\, \\mathrm{s} = 6.67 \\, \\mathrm{ps}$. The observation that an aggressive barostat with $\\tau_P = 1 \\, \\mathrm{ps}$ (which is shorter than $t_{acoustic}$) causes underdamped oscillations is a key piece of physical evidence.\n\nWith this analysis complete, we can evaluate each option.\n\n**Option A Analysis**\nThis option states that independent, parallel relaxation channels for temperature lead to an additive rate law: $1/\\tau_{\\mathrm{obs}} = 1/\\tau_{\\mathrm{phys}} + 1/\\tau_T$. It suggests validating this by plotting $1/\\tau_{\\mathrm{obs}}$ versus $1/\\tau_T$, which should yield a straight line with slope 1 and intercept $1/\\tau_{\\mathrm{phys}}$. Finally, it claims the data are consistent. As shown in our initial analysis, the additive rate law is the correct model for parallel first-order processes. The proposed graphical validation method is a direct consequence of this linear relationship. Our calculations confirmed that the provided data are perfectly consistent with this model. Therefore, this statement is correct.\n\n**Option B Analysis**\nThis option claims that choosing a barostat time constant $\\tau_P$ much smaller than the acoustic period always decreases volume relaxation time without side effects. This is incorrect. Making the coupling too strong (very small $\\tau_P$) can excite unphysical oscillations, especially when $\\tau_P$ is comparable to or shorter than the time it takes for a sound wave to traverse the simulation box ($t_{acoustic}$). The barostat tries to change the volume faster than the system can physically respond, leading to ringing artifacts. The problem statement explicitly provides evidence for this: for $\\tau_P = 1 \\, \\mathrm{ps}$, \"the volume exhibits underdamped oscillations\". These are significant, undesirable side effects.\n\n**Option C Analysis**\nThis option outlines a detailed, multi-step protocol.\n1.  **Use NVE runs for intrinsic dynamics**: This is correct. NVE simulations are free from algorithmic perturbations and are the ideal way to measure the system's natural relaxation times.\n2.  **Sweep coupling parameters**: Systematically varying $\\tau_T$ and $\\tau_P$ and observing the system's response (e.g., fitting relaxation rates, checking for oscillations) is the correct experimental approach to characterize the coupling.\n3.  **Choose coupling times appropriately**: The guidance to choose coupling times that are longer than the fastest physical modes (to avoid artifacts) but shorter than the slow processes of interest (to avoid perturbing them) is the canonical best practice for setting up stable and physically meaningful simulations.\n4.  **Confirm with block averaging**: This is a crucial final step to verify that thermodynamic averages are converged and independent of the specific (valid) choice of coupling parameters.\nThis entire protocol represents the state-of-the-art for robustly setting up and validating MD simulations. Therefore, this statement is correct.\n\n**Option D Analysis**\nThis option suggests making the thermostat and barostat \"infinitely fast\" by minimizing $\\tau_T$ and $\\tau_P$. This is a common and dangerous misconception. An infinitely fast thermostat would clamp the kinetic energy, destroying the natural fluctuations required for the canonical (NVT) ensemble. An infinitely fast barostat leads to severe unphysical oscillations, as discussed in Option B. This strategy introduces major artifacts rather than removing them.\n\n**Option E Analysis**\nThis option proposes that the slowest relaxation channel dominates, such that $\\tau_{\\mathrm{obs}} = \\max(\\tau_T, \\tau_{\\mathrm{phys}})$. This mathematical form describes processes in series (where one must wait for the other), not in parallel. As established in Option A, physical relaxation and algorithmic heat exchange are parallel pathways for the system to equilibrate. The rates add ($1/\\tau_{\\mathrm{obs}} = 1/\\tau_{\\mathrm{phys}} + 1/\\tau_T$), meaning the observed relaxation is always *faster* than either individual process. The premise of this option is based on an incorrect physical model.\n\nBased on this analysis, options A and C are correct.", "answer": "$$\\boxed{AC}$$", "id": "3405236"}]}