{"hands_on_practices": [{"introduction": "A crucial practical question when performing a bootstrap analysis is determining how many resamples, $B$, are sufficient. This exercise moves beyond simple heuristics by guiding you through a theoretical derivation to quantify the Monte Carlo error of a bootstrap quantile estimate. By connecting the number of resamples to the desired precision and properties of the underlying data, you will develop a rigorous understanding of how to choose $B$ effectively [@problem_id:3399625].", "problem": "A Molecular Dynamics (MD) simulation of a Lennard–Jones fluid is run long enough that block averages of the potential energy per mole over non-overlapping time blocks can be treated as approximately independent and identically distributed. Let these $m$ block-averaged values be denoted by $\\{X_{i}\\}_{i=1}^{m}$ with units $\\mathrm{kJ}\\ \\mathrm{mol}^{-1}$. To quantify uncertainty in the estimated mean potential energy arising from finite sampling, a nonparametric bootstrap is employed: from the empirical distribution of $\\{X_{i}\\}_{i=1}^{m}$, generate $B$ bootstrap resamples, and from each resample compute the corresponding statistic $T^{*}$ (the resampled mean). Denote by $F^{*}$ the bootstrap distribution of $T^{*}$ and by $q_{\\alpha}^{*}$ its $\\alpha$-quantile.\n\nFocus on the Monte Carlo error due solely to the finite number $B$ of bootstrap resamples when estimating the $\\alpha$-quantile $q_{\\alpha}^{*}$ by the empirical $\\alpha$-quantile $\\hat{q}_{\\alpha}^{(B)}$ of the $B$ bootstrap replicates $\\{T^{*}_{b}\\}_{b=1}^{B}$. Starting from the definition of the empirical cumulative distribution function (CDF) and the definition of a quantile as an inverse CDF, and using only basic probability facts about indicator random variables and differentiability of the CDF at the quantile, derive an upper bound on the Monte Carlo standard error of $\\hat{q}_{\\alpha}^{(B)}$ as a function of $B$ under the regularity assumption that the bootstrap distribution has a density $f^{*}$ that is continuous at $q_{\\alpha}^{*}$ and bounded below there by a known constant $c>0$, i.e., $f^{*}(q_{\\alpha}^{*}) \\ge c$.\n\nThen, for the specific MD study described above, suppose a pilot bootstrap with a very large number of resamples and a kernel density estimate suggest that at $\\alpha = 0.95$ the density satisfies $f^{*}(q_{0.95}^{*}) \\ge c$ with $c = 2.0 \\times 10^{-2}\\ (\\mathrm{kJ}\\ \\mathrm{mol}^{-1})^{-1}$. If you require that the Monte Carlo standard error of the $\\alpha$-quantile estimator due to finite $B$ be no larger than $\\delta = 0.20\\ \\mathrm{kJ}\\ \\mathrm{mol}^{-1}$, choose $B$ to meet this requirement using your derived bound. \n\n- Derive the bound symbolically in terms of $\\alpha$, $B$, and $c$.\n- Then compute the smallest integer value of $B$ that guarantees the bound on the Monte Carlo standard error is at most $\\delta$.\n\nExpress the final answer as the minimal integer number of bootstrap resamples $B$. No units should be included in the final answer.", "solution": "The problem requires us to first derive an upper bound for the Monte Carlo standard error of a bootstrap quantile estimator and then use this bound to determine the minimum number of bootstrap resamples, $B$, needed to achieve a desired level of precision.\n\nLet the true cumulative distribution function (CDF) of the bootstrap statistic $T^{*}$ be $F^{*}(t) = P(T^{*} \\le t)$, and let its probability density function (PDF) be $f^{*}(t) = (F^{*})'(t)$. The true $\\alpha$-quantile, $q_{\\alpha}^{*}$, is defined by the relation $F^{*}(q_{\\alpha}^{*}) = \\alpha$.\n\nWe have a set of $B$ bootstrap replicates $\\{T^{*}_{b}\\}_{b=1}^{B}$, which are independent and identically distributed (i.i.d.) random variables drawn from the distribution $F^{*}$. The empirical CDF based on these replicates is given by:\n$$ \\hat{F}_{B}^{*}(t) = \\frac{1}{B} \\sum_{b=1}^{B} I(T^{*}_{b} \\le t) $$\nwhere $I(\\cdot)$ is the indicator function. The empirical $\\alpha$-quantile, $\\hat{q}_{\\alpha}^{(B)}$, is the estimator for $q_{\\alpha}^{*}$ and is defined as the inverse of the empirical CDF: $\\hat{q}_{\\alpha}^{(B)} = (\\hat{F}_{B}^{*})^{-1}(\\alpha) = \\inf\\{t : \\hat{F}_{B}^{*}(t) \\ge \\alpha\\}$.\n\nOur goal is to find the Monte Carlo standard error, $\\mathrm{SE}(\\hat{q}_{\\alpha}^{(B)}) = \\sqrt{\\mathrm{Var}(\\hat{q}_{\\alpha}^{(B)})}$, where the variance is taken over the distribution of the $B$ bootstrap replicates.\n\nThe core of the derivation relies on relating the variability of the quantile estimator $\\hat{q}_{\\alpha}^{(B)}$ to the variability of the empirical CDF estimator $\\hat{F}_{B}^{*}(t)$. The latter is easier to analyze. This relationship can be established through a first-order approximation, which is valid for large $B$ under the assumption that $f^{*}$ is continuous and non-zero at $q_{\\alpha}^{*}$. This is a well-established result in the asymptotic theory of sample quantiles (related to the Bahadur representation). The approximation is:\n$$ \\hat{q}_{\\alpha}^{(B)} - q_{\\alpha}^{*} \\approx -\\frac{\\hat{F}_{B}^{*}(q_{\\alpha}^{*}) - F^{*}(q_{\\alpha}^{*})}{f^{*}(q_{\\alpha}^{*})} $$\nSubstituting $F^{*}(q_{\\alpha}^{*}) = \\alpha$, we get:\n$$ \\hat{q}_{\\alpha}^{(B)} \\approx q_{\\alpha}^{*} - \\frac{\\hat{F}_{B}^{*}(q_{\\alpha}^{*}) - \\alpha}{f^{*}(q_{\\alpha}^{*})} $$\nWe can now compute the variance of $\\hat{q}_{\\alpha}^{(B)}$. Since $q_{\\alpha}^{*}$, $\\alpha$, and $f^{*}(q_{\\alpha}^{*})$ are constants with respect to the Monte Carlo sampling (they are properties of the true, underlying bootstrap distribution $F^{*}$), the variance of the expression is determined by the variance of $\\hat{F}_{B}^{*}(q_{\\alpha}^{*})$.\n$$ \\mathrm{Var}(\\hat{q}_{\\alpha}^{(B)}) \\approx \\mathrm{Var}\\left( q_{\\alpha}^{*} - \\frac{\\hat{F}_{B}^{*}(q_{\\alpha}^{*}) - \\alpha}{f^{*}(q_{\\alpha}^{*})} \\right) = \\mathrm{Var}\\left( - \\frac{\\hat{F}_{B}^{*}(q_{\\alpha}^{*})}{f^{*}(q_{\\alpha}^{*})} \\right) = \\frac{1}{[f^{*}(q_{\\alpha}^{*})]^2} \\mathrm{Var}(\\hat{F}_{B}^{*}(q_{\\alpha}^{*})) $$\nNext, we determine $\\mathrm{Var}(\\hat{F}_{B}^{*}(q_{\\alpha}^{*}))$. The term $\\hat{F}_{B}^{*}(q_{\\alpha}^{*})$ is the average of $B$ i.i.d. Bernoulli random variables, $Y_b = I(T^{*}_{b} \\le q_{\\alpha}^{*})$. The probability of \"success\" for each trial is $p = P(T^{*}_{b} \\le q_{\\alpha}^{*}) = F^{*}(q_{\\alpha}^{*}) = \\alpha$. The variance of a single Bernoulli($p$) random variable is $p(1-p)$.\nThe variance of the average of $B$ such i.i.d. variables is $\\frac{1}{B}$ times the variance of a single variable:\n$$ \\mathrm{Var}(\\hat{F}_{B}^{*}(q_{\\alpha}^{*})) = \\mathrm{Var}\\left(\\frac{1}{B}\\sum_{b=1}^{B} Y_b\\right) = \\frac{1}{B^2} \\sum_{b=1}^{B} \\mathrm{Var}(Y_b) = \\frac{1}{B^2} (B \\cdot p(1-p)) = \\frac{p(1-p)}{B} $$\nSubstituting $p=\\alpha$, we have:\n$$ \\mathrm{Var}(\\hat{F}_{B}^{*}(q_{\\alpha}^{*})) = \\frac{\\alpha(1-\\alpha)}{B} $$\nSubstituting this result back into the expression for the variance of the quantile estimator:\n$$ \\mathrm{Var}(\\hat{q}_{\\alpha}^{(B)}) \\approx \\frac{\\alpha(1-\\alpha)}{B [f^{*}(q_{\\alpha}^{*})]^2} $$\nThe Monte Carlo standard error is the square root of the variance:\n$$ \\mathrm{SE}(\\hat{q}_{\\alpha}^{(B)}) = \\sqrt{\\mathrm{Var}(\\hat{q}_{\\alpha}^{(B)})} \\approx \\frac{\\sqrt{\\alpha(1-\\alpha)}}{\\sqrt{B} f^{*}(q_{\\alpha}^{*})} $$\nThe problem provides the regularity condition that the density $f^{*}(q_{\\alpha}^{*})$ is bounded below by a known positive constant $c$, i.e., $f^{*}(q_{\\alpha}^{*}) \\ge c > 0$. Since $f^{*}(q_{\\alpha}^{*})$ appears in the denominator, this lower bound allows us to establish an upper bound on the standard error:\n$$ \\mathrm{SE}(\\hat{q}_{\\alpha}^{(B)}) \\le \\frac{\\sqrt{\\alpha(1-\\alpha)}}{c\\sqrt{B}} $$\nThis is the symbolic upper bound on the Monte Carlo standard error of the $\\alpha$-quantile estimator.\n\nNow, we proceed to the numerical part of the problem. We are given the following values:\nThe quantile of interest is for $\\alpha = 0.95$.\nThe lower bound on the density is $c = 2.0 \\times 10^{-2}\\ (\\mathrm{kJ}\\ \\mathrm{mol}^{-1})^{-1}$.\nThe required upper limit on the standard error is $\\delta = 0.20\\ \\mathrm{kJ}\\ \\mathrm{mol}^{-1}$.\n\nWe need to find the smallest integer $B$ that satisfies the condition $\\mathrm{SE}(\\hat{q}_{\\alpha}^{(B)}) \\le \\delta$. To guarantee this, we enforce the condition on our derived upper bound:\n$$ \\frac{\\sqrt{\\alpha(1-\\alpha)}}{c\\sqrt{B}} \\le \\delta $$\nWe solve this inequality for $B$. Since all quantities are positive, we can rearrange and square without changing the inequality direction.\n$$ \\sqrt{B} \\ge \\frac{\\sqrt{\\alpha(1-\\alpha)}}{c\\delta} $$\n$$ B \\ge \\frac{\\alpha(1-\\alpha)}{(c\\delta)^2} $$\nNow, we substitute the numerical values:\n$$ \\alpha(1-\\alpha) = 0.95 \\times (1 - 0.95) = 0.95 \\times 0.05 = 0.0475 $$\n$$ c\\delta = (2.0 \\times 10^{-2}) \\times 0.20 = 0.02 \\times 0.2 = 0.004 = 4 \\times 10^{-3} $$\n$$ (c\\delta)^2 = (4 \\times 10^{-3})^2 = 16 \\times 10^{-6} = 1.6 \\times 10^{-5} $$\nSubstituting these into the inequality for $B$:\n$$ B \\ge \\frac{0.0475}{1.6 \\times 10^{-5}} = \\frac{4.75 \\times 10^{-2}}{1.6 \\times 10^{-5}} = \\frac{4.75}{1.6} \\times 10^3 = 2.96875 \\times 10^3 = 2968.75 $$\nSince the number of bootstrap resamples $B$ must be an integer, we must take the ceiling of this value to satisfy the inequality.\n$$ B = \\lceil 2968.75 \\rceil = 2969 $$\nTherefore, the smallest integer number of bootstrap resamples required to ensure the Monte Carlo standard error of the estimated $0.95$-quantile is no more than $0.20\\ \\mathrm{kJ}\\ \\mathrm{mol}^{-1}$ is $2969$.", "answer": "$$ \\boxed{2969} $$", "id": "3399625"}, {"introduction": "Estimating free energy differences via the Jarzynski equality is a cornerstone of non-equilibrium molecular dynamics, but the estimator is famously sensitive to rare, low-work events that create heavy-tailed distributions. This hands-on coding practice demonstrates how to implement a stratified bootstrap, a robust technique that ensures these critical tail events are properly represented during resampling. This method stabilizes the uncertainty analysis, leading to more reliable confidence intervals for observables dominated by rare events [@problem_id:3399592].", "problem": "You are given a nonequilibrium Molecular Dynamics (MD) context in which the free energy difference is estimated via the Jarzynski equality. The fundamental base is the Jarzynski equality from nonequilibrium statistical mechanics, which states that for a system initially in equilibrium at temperature $T$, the free energy difference $\\Delta G$ between two states can be obtained from repeated realizations of non-equilibrium work $W$ performed during a switching process, according to\n$$\n\\Delta G = - k_{\\mathrm{B}} T \\ln \\left\\langle e^{-\\beta W} \\right\\rangle,\n$$\nwhere $k_{\\mathrm{B}}$ is the Boltzmann constant, $T$ is the absolute temperature, $\\beta = \\frac{1}{k_{\\mathrm{B}} T}$, angle brackets denote an ensemble average over independent realizations, and $W$ is the work performed along each trajectory. The empirical estimator uses a finite sample $\\{W_i\\}_{i=1}^N$ of independent work values obtained from Molecular Dynamics trajectories. Because of the exponential weighting $e^{-\\beta W}$, rare low-work events can dominate the estimate, producing heavy-tail behavior and large uncertainty. The goal is to quantify this uncertainty using a stratified bootstrap that resamples within quantile-defined strata of the work distribution to stabilize the contribution of the heavy tail.\n\nStarting from the above fundamental base and the definition of nonparametric bootstrap resampling, design an algorithm that:\n- Constructs a stratified bootstrap over $W$ quantiles by partitioning the sample $\\{W_i\\}_{i=1}^N$ into $S$ strata of approximately equal frequency using sample quantiles or ranks.\n- Generates $B$ bootstrap replicates by sampling with replacement within each stratum, preserving the original stratum sizes, and for each replicate computes the Jarzynski estimator $\\widehat{\\Delta G}$ using the resampled data.\n- Reports the point estimate $\\widehat{\\Delta G}$ from the original data, the bootstrap standard error $\\sigma_{\\mathrm{boot}}$, and a two-sided confidence interval $[\\mathrm{CI}_{\\mathrm{low}}, \\mathrm{CI}_{\\mathrm{high}}]$ based on the percentile method at level $95$ (i.e., lower and upper empirical quantiles at $2.5$ and $97.5$ expressed as decimals, not percentages).\n\nNumerical stability is required in evaluating $\\ln \\left\\langle e^{-\\beta W} \\right\\rangle$ for heavy-tail data. Your algorithm must implement a stable computation that avoids overflow and underflow, for example by using a log-sum-exp identity.\n\nExpress all energy quantities in joules $\\mathrm{J}$. Temperatures must be in kelvin $\\mathrm{K}$. Angles are not involved. The Boltzmann constant to be used is $k_{\\mathrm{B}} = 1.380649 \\times 10^{-23}\\,\\mathrm{J}/\\mathrm{K}$. For reproducibility, set the pseudorandom generator seed to $123$.\n\nTest suite. Your program must generate synthetic work data $\\{W_i\\}$ from a scientifically plausible mixture model that mimics heavy-tail behavior observed in nonequilibrium MD pulling, where $W$ is sampled independently from a mixture of two normal distributions:\n- With probability $p$, $W$ is drawn from a normal distribution with mean $\\mu_{\\mathrm{main}}$ and standard deviation $\\sigma_{\\mathrm{main}}$.\n- With probability $1-p$, $W$ is drawn from a normal distribution with mean $\\mu_{\\mathrm{tail}}$ and standard deviation $\\sigma_{\\mathrm{tail}}$.\nTo maintain physical scaling, parameterize the means and standard deviations in units of $k_{\\mathrm{B}} T$, i.e., set $\\mu_{\\mathrm{main}} = c_{\\mu,\\mathrm{main}}\\,k_{\\mathrm{B}}T$, $\\sigma_{\\mathrm{main}} = c_{\\sigma,\\mathrm{main}}\\,k_{\\mathrm{B}}T$, $\\mu_{\\mathrm{tail}} = c_{\\mu,\\mathrm{tail}}\\,k_{\\mathrm{B}}T$, and $\\sigma_{\\mathrm{tail}} = c_{\\sigma,\\mathrm{tail}}\\,k_{\\mathrm{B}}T$, and then convert to joules. Let $N$ denote the sample size, $S$ the number of strata, and $B$ the number of bootstrap replicates. Use the following three test cases that cover a typical case, a small-sample case, and an extreme heavy-tail case:\n\n- Case $1$ (typical heavy-tail): $N = 1000$, $T = 300\\,\\mathrm{K}$, $p = 0.95$, $c_{\\mu,\\mathrm{main}} = 10$, $c_{\\sigma,\\mathrm{main}} = 3$, $c_{\\mu,\\mathrm{tail}} = -5$, $c_{\\sigma,\\mathrm{tail}} = 2$, $S = 10$, $B = 1000$.\n- Case $2$ (small sample, heavier tail): $N = 200$, $T = 300\\,\\mathrm{K}$, $p = 0.90$, $c_{\\mu,\\mathrm{main}} = 6$, $c_{\\sigma,\\mathrm{main}} = 2$, $c_{\\mu,\\mathrm{tail}} = -8$, $c_{\\sigma,\\mathrm{tail}} = 3$, $S = 8$, $B = 1500$.\n- Case $3$ (rare but extreme negative-work events): $N = 1000$, $T = 300\\,\\mathrm{K}$, $p = 0.98$, $c_{\\mu,\\mathrm{main}} = 12$, $c_{\\sigma,\\mathrm{main}} = 2$, $c_{\\mu,\\mathrm{tail}} = -20$, $c_{\\sigma,\\mathrm{tail}} = 5$, $S = 10$, $B = 1000$.\n\nYour program must:\n- Generate $\\{W_i\\}$ for each case using the mixture model defined above.\n- Compute the point estimate $\\widehat{\\Delta G}$ in $\\mathrm{J}$, the bootstrap standard error $\\sigma_{\\mathrm{boot}}$ in $\\mathrm{J}$, and the percentile confidence interval endpoints $\\mathrm{CI}_{\\mathrm{low}}$ and $\\mathrm{CI}_{\\mathrm{high}}$ in $\\mathrm{J}$.\n- Produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each per-case result is the list $[\\widehat{\\Delta G},\\sigma_{\\mathrm{boot}},\\mathrm{CI}_{\\mathrm{low}},\\mathrm{CI}_{\\mathrm{high}}]$. All numbers must be printed in scientific notation with six significant digits and in $\\mathrm{J}$. For example, the output format must be of the form $[[x_1,y_1,z_1,w_1],[x_2,y_2,z_2,w_2],[x_3,y_3,z_3,w_3]]$, where each $x_i$, $y_i$, $z_i$, $w_i$ is a float in scientific notation.", "solution": "The problem requires the design and implementation of an algorithm to compute the free energy difference, $\\Delta G$, from a set of non-equilibrium work values, $\\{W_i\\}$, using the Jarzynski equality. The core of the task is to provide a robust uncertainty quantification for the $\\Delta G$ estimate using a stratified bootstrap procedure, which is particularly suited for the heavy-tailed work distributions commonly encountered in such problems.\n\nThe foundational principle is the Jarzynski equality:\n$$\n\\Delta G = -k_{\\mathrm{B}} T \\ln \\left\\langle e^{-\\beta W} \\right\\rangle\n$$\nwhere $\\Delta G$ is the Helmholtz free energy difference, $k_{\\mathrm{B}}$ is the Boltzmann constant ($1.380649 \\times 10^{-23}\\,\\mathrm{J}/\\mathrm{K}$), $T$ is the absolute temperature, $\\beta = (k_{\\mathrm{B}} T)^{-1}$, and the angle brackets $\\left\\langle \\cdot \\right\\rangle$ denote an average over an infinite ensemble of non-equilibrium trajectories. In practice, we have a finite sample of $N$ work values $\\{W_i\\}_{i=1}^N$, leading to the empirical estimator:\n$$\n\\widehat{\\Delta G} = -k_{\\mathrm{B}} T \\ln \\left( \\frac{1}{N} \\sum_{i=1}^{N} e^{-\\beta W_i} \\right)\n$$\nA significant challenge arises from the exponential weighting term, $e^{-\\beta W}$. Trajectories with small or negative work values, though rare, can contribute disproportionately to the sum, causing the estimator to be dominated by a few events and exhibit high variance. This makes the estimate numerically unstable and its uncertainty difficult to assess.\n\nA standard technique to mitigate numerical overflow in the summation is the log-sum-exp identity. We can rewrite the estimator in a more stable form. Let $W_{\\min} = \\min_{i} \\{W_i\\}$. The argument of the logarithm becomes:\n$$\n\\frac{1}{N} \\sum_{i=1}^{N} e^{-\\beta W_i} = \\frac{1}{N} \\sum_{i=1}^{N} e^{-\\beta (W_i - W_{\\min} + W_{\\min})} = \\frac{e^{-\\beta W_{\\min}}}{N} \\sum_{i=1}^{N} e^{-\\beta (W_i - W_{\\min})}\n$$\nSubstituting this back into the estimator for $\\widehat{\\Delta G}$:\n$$\n\\widehat{\\Delta G} = -k_{\\mathrm{B}} T \\ln \\left( \\frac{e^{-\\beta W_{\\min}}}{N} \\sum_{i=1}^{N} e^{-\\beta (W_i - W_{\\min})} \\right)\n$$\n$$\n\\widehat{\\Delta G} = -k_{\\mathrm{B}} T \\left( -\\beta W_{\\min} + \\ln \\left( \\frac{1}{N} \\sum_{i=1}^{N} e^{-\\beta (W_i - W_{\\min})} \\right) \\right)\n$$\nSince $\\beta = (k_{\\mathrm{B}} T)^{-1}$, this simplifies to a numerically robust expression:\n$$\n\\widehat{\\Delta G} = W_{\\min} - k_{\\mathrm{B}} T \\ln \\left( \\frac{1}{N} \\sum_{i=1}^{N} e^{-\\frac{W_i - W_{\\min}}{k_{\\mathrm{B}} T}} \\right)\n$$\nThis form prevents floating-point overflow because the largest exponent in the sum is $0$, corresponding to $W_i = W_{\\min}$.\n\nTo quantify the uncertainty, a stratified bootstrap procedure is employed. This method is superior to a simple bootstrap for heavy-tailed distributions because it ensures that all parts of the distribution, including the crucial tail region, are represented in each bootstrap replicate. The algorithm proceeds as follows:\n\n1.  **Stratification**: The original sample of $N$ work values $\\{W_i\\}$ is sorted in ascending order. The sorted sample is then partitioned into $S$ strata, where each stratum contains approximately $N/S$ contiguous data points. This partitioning ensures that the lowest work values are isolated in their own stratum, guaranteeing their representation during resampling.\n\n2.  **Resampling**: $B$ bootstrap replicates are generated. Each replicate is a new sample of size $N$ constructed by the following procedure: for each stratum $j$ containing $n_j$ data points, we draw $n_j$ samples *with replacement* from that stratum. The resampled data from all strata are combined to form one bootstrap replicate. This process preserves the number of data points originating from each quantile range of the original distribution.\n\n3.  **Estimation and Analysis**: For each of the $B$ bootstrap replicates, denoted $\\{W_i^*\\}_{k}$ for $k=1, \\dots, B$, the Jarzynski estimate $\\widehat{\\Delta G}_k^*$ is calculated using the numerically stable formula. This yields a collection of $B$ bootstrap estimates $\\{\\widehat{\\Delta G}_k^*\\}_{k=1}^B$.\n    - The bootstrap standard error, $\\sigma_{\\mathrm{boot}}$, is the sample standard deviation of this collection:\n      $$\n      \\sigma_{\\mathrm{boot}} = \\sqrt{\\frac{1}{B-1} \\sum_{k=1}^{B} (\\widehat{\\Delta G}_k^* - \\overline{\\widehat{\\Delta G}^*})^2}\n      $$\n      where $\\overline{\\widehat{\\Delta G}^*}$ is the mean of the bootstrap estimates.\n    - A $95\\%$ percentile confidence interval $[\\mathrm{CI}_{\\mathrm{low}}, \\mathrm{CI}_{\\mathrm{high}}]$ is constructed by finding the $2.5$th and $97.5$th percentiles of the sorted distribution of bootstrap estimates $\\{\\widehat{\\Delta G}_k^*\\}$.\n\nThe synthetic work data for testing this algorithm is generated from a two-component normal mixture model, which is a plausible representation of work distributions from single-molecule pulling experiments. A data point $W$ is drawn with probability $p$ from a 'main' distribution $\\mathcal{N}(\\mu_{\\mathrm{main}}, \\sigma_{\\mathrm{main}}^2)$ and with probability $1-p$ from a 'tail' distribution $\\mathcal{N}(\\mu_{\\mathrm{tail}}, \\sigma_{\\mathrm{tail}}^2)$. The parameters for these distributions are scaled by the thermal energy $k_{\\mathrm{B}} T$ to maintain physical relevance.\n\nThe final algorithm synthesizes these components: it generates the specified synthetic data, calculates the point estimate $\\widehat{\\Delta G}$ from the full dataset, then executes the stratified bootstrap procedure to compute $\\sigma_{\\mathrm{boot}}$ and the confidence interval endpoints, $\\mathrm{CI}_{\\mathrm{low}}$ and $\\mathrm{CI}_{\\mathrm{high}}$. All energy values are reported in joules.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the Jarzynski equality problem with stratified bootstrap for uncertainty.\n    \"\"\"\n    \n    # Define physical constant and random number generator\n    K_B = 1.380649e-23  # Boltzmann constant in J/K\n    RNG = np.random.default_rng(123)\n\n    # Define test cases as per the problem description\n    test_cases = [\n        # Case 1 (typical heavy-tail)\n        {'N': 1000, 'T': 300.0, 'p': 0.95, 'c_mu_main': 10.0, 'c_sigma_main': 3.0, \n         'c_mu_tail': -5.0, 'c_sigma_tail': 2.0, 'S': 10, 'B': 1000},\n        # Case 2 (small sample, heavier tail)\n        {'N': 200, 'T': 300.0, 'p': 0.90, 'c_mu_main': 6.0, 'c_sigma_main': 2.0, \n         'c_mu_tail': -8.0, 'c_sigma_tail': 3.0, 'S': 8, 'B': 1500},\n        # Case 3 (rare but extreme negative-work events)\n        {'N': 1000, 'T': 300.0, 'p': 0.98, 'c_mu_main': 12.0, 'c_sigma_main': 2.0, \n         'c_mu_tail': -20.0, 'c_sigma_tail': 5.0, 'S': 10, 'B': 1000},\n    ]\n\n    results = []\n\n    def jarzynski_estimator(work_values, T):\n        \"\"\"\n        Calculates the Jarzynski free energy estimate using a numerically stable\n        log-sum-exp formulation.\n        \n        Args:\n            work_values (np.ndarray): Array of work values in Joules.\n            T (float): Temperature in Kelvin.\n        \n        Returns:\n            float: The estimated free energy difference in Joules.\n        \"\"\"\n        if len(work_values) == 0:\n            return np.nan\n        \n        kBT = K_B * T\n        beta = 1.0 / kBT\n        \n        w_min = np.min(work_values)\n        \n        # Numerically stable calculation: delta_G = W_min - kBT * log(mean(exp(-beta*(W-W_min))))\n        shifted_exp_terms = np.exp(-beta * (work_values - w_min))\n        mean_of_exponentials = np.mean(shifted_exp_terms)\n        \n        delta_g = w_min - kBT * np.log(mean_of_exponentials)\n        return delta_g\n\n    for case in test_cases:\n        N, T, p, S, B = case['N'], case['T'], case['p'], case['S'], case['B']\n        kBT = K_B * T\n\n        # Generate synthetic work data from the mixture model\n        # 1. Determine number of samples from each distribution\n        num_main = RNG.binomial(N, p)\n        num_tail = N - num_main\n\n        # 2. Define distribution parameters in Joules\n        mu_main = case['c_mu_main'] * kBT\n        sigma_main = case['c_sigma_main'] * kBT\n        mu_tail = case['c_mu_tail'] * kBT\n        sigma_tail = case['c_sigma_tail'] * kBT\n\n        # 3. Generate samples\n        work_main = RNG.normal(loc=mu_main, scale=sigma_main, size=num_main)\n        work_tail = RNG.normal(loc=mu_tail, scale=sigma_tail, size=num_tail)\n        W = np.concatenate((work_main, work_tail))\n        RNG.shuffle(W)\n\n        # Calculate the point estimate from the original data\n        delta_g_hat = jarzynski_estimator(W, T)\n\n        # --- Stratified Bootstrap Procedure ---\n        \n        # 1. Stratify the data\n        W_sorted = np.sort(W)\n        strata = np.array_split(W_sorted, S)\n        \n        # 2. Resampling and Estimation\n        bootstrap_estimates = np.empty(B)\n        for i in range(B):\n            bootstrap_sample_parts = []\n            for stratum in strata:\n                n_stratum = len(stratum)\n                resampled_part = RNG.choice(stratum, size=n_stratum, replace=True)\n                bootstrap_sample_parts.append(resampled_part)\n            \n            bootstrap_sample = np.concatenate(bootstrap_sample_parts)\n            bootstrap_estimates[i] = jarzynski_estimator(bootstrap_sample, T)\n            \n        # 3. Analyze bootstrap results\n        # Bootstrap standard error (sample standard deviation, ddof=1)\n        sigma_boot = np.std(bootstrap_estimates, ddof=1)\n        \n        # 95% percentile confidence interval\n        ci_low = np.percentile(bootstrap_estimates, 2.5)\n        ci_high = np.percentile(bootstrap_estimates, 97.5)\n        \n        # Store results for this case\n        results.append([delta_g_hat, sigma_boot, ci_low, ci_high])\n\n    # Format the final output string\n    output_str = f\"[{','.join([f'[{\",\".join([f\"{v:.6e}\" for v in r])}]' for r in results])}]\"\n    print(output_str)\n\nsolve()\n```", "id": "3399592"}, {"introduction": "In advanced statistical mechanics, certain observables like the heat capacity can exhibit distributions with infinite variance, a scenario where the standard bootstrap is theoretically inconsistent and fails to produce valid uncertainty estimates. This practice introduces the $m$-out-of-$n$ bootstrap, a powerful technique that restores statistical consistency by resampling a smaller subset of the data ($m \\lt n$). By implementing this method, you will learn to tackle a fundamental failure mode of the bootstrap and correctly quantify uncertainty for systems with extreme, non-Gaussian fluctuations [@problem_id:3399551].", "problem": "Consider a canonical-ensemble molecular dynamics observable whose instantaneous energy is denoted by $E$. In the canonical ensemble, the distribution of $E$ is governed by the Boltzmann weight, and the heat capacity at constant volume is defined as the derivative of the mean energy with respect to temperature. Heavy-tailed fluctuations may invalidate Gaussian assumptions on the sampling distribution of estimators. Your task is to design a program to perform uncertainty quantification of the heat capacity using the $m$-out-of-$n$ bootstrap when the energy samples are heavy-tailed.\n\nStart from the following foundations:\n- The canonical ensemble has inverse temperature $\\beta$ given by $\\beta = 1/(k_B T)$, where $k_B$ is the Boltzmann constant and $T$ is absolute temperature.\n- The partition function is $Z(\\beta) = \\sum_E g(E)\\, e^{-\\beta E}$ for discrete $E$ (or the analogous integral for continuous $E$), where $g(E)$ is the density of states.\n- The mean energy is $\\langle E \\rangle = -\\frac{\\partial}{\\partial \\beta}\\ln Z(\\beta)$ and its fluctuation is connected to derivatives of $\\ln Z(\\beta)$.\n\nFrom these, derive the heat capacity in terms of equilibrium energy fluctuations and implement an estimator that uses a finite sample of energies. Do not use any Gaussian-approximation shortcut; base your reasoning on the canonical ensemble definitions above.\n\nSynthetic heavy-tailed energy samples should be generated as follows. For a given sample size $n$, define parameters $E_0$ (baseline energy offset), $\\sigma$ (Gaussian fluctuation scale), $p_{\\mathrm{tail}}$ (probability of a heavy-tailed jump), $x_m$ (Pareto scale), and $\\nu$ (Pareto shape). For each $i \\in \\{1,\\dots,n\\}$:\n- Draw a Gaussian fluctuation $G_i \\sim \\mathcal{N}(0,\\sigma^2)$.\n- Draw a Bernoulli indicator $J_i \\in \\{0,1\\}$ with $\\mathbb{P}(J_i=1) = p_{\\mathrm{tail}}$.\n- Draw a Pareto jump $P_i = x_m \\left(1 + Y_i\\right)$ where $Y_i \\sim \\mathrm{Pareto}(\\nu)$ with support $Y_i \\ge 1$.\n- Define the energy sample by $E_i = E_0 + G_i + J_i\\, P_i$.\n\nFor each synthetic dataset, compute the heat capacity estimator from the full sample of size $n$ using the fluctuation-based definition you derived, and quantify its uncertainty using the $m$-out-of-$n$ bootstrap with $m = \\lfloor n^\\alpha \\rfloor$ for a specified $0 < \\alpha < 1$. In the bootstrap:\n- Resample $m$ indices with replacement from $\\{1,\\dots,n\\}$.\n- Compute the same heat capacity estimator on the resampled energy values.\n- Repeat for $B$ bootstrap replicates to obtain a bootstrap distribution of the estimator.\n- From the bootstrap distribution, compute the bootstrap standard error and the two-sided percentile confidence interval at confidence level $0.95$.\n\nReport results in physical units: the heat capacity must be expressed in joules per kelvin (J/K), the bootstrap standard error must be in joules per kelvin (J/K), and the confidence interval endpoints must be in joules per kelvin (J/K). Temperatures must be in kelvin (K). No angles are involved. Express numerical outputs as decimal floats.\n\nTest suite. Your program must run the following four parameter sets and aggregate the outputs:\n- Case $1$ (general heavy-tailed, finite variance): $n=1000$, $\\alpha=0.7$, $T=300\\,\\mathrm{K}$, $E_0=1.0\\times 10^{-19}\\,\\mathrm{J}$, $\\sigma=2.0\\times 10^{-21}\\,\\mathrm{J}$, $p_{\\mathrm{tail}}=0.02$, $x_m=3.0\\times 10^{-20}\\,\\mathrm{J}$, $\\nu=2.5$, $B=1000$.\n- Case $2$ (strong heavy tail, infinite variance): $n=100$, $\\alpha=0.5$, $T=300\\,\\mathrm{K}$, $E_0=8.0\\times 10^{-20}\\,\\mathrm{J}$, $\\sigma=4.0\\times 10^{-21}\\,\\mathrm{J}$, $p_{\\mathrm{tail}}=0.05$, $x_m=1.0\\times 10^{-19}\\,\\mathrm{J}$, $\\nu=1.5$, $B=1000$.\n- Case $3$ (large sample, mild heavy tail): $n=10000$, $\\alpha=0.9$, $T=300\\,\\mathrm{K}$, $E_0=1.2\\times 10^{-19}\\,\\mathrm{J}$, $\\sigma=2.0\\times 10^{-21}\\,\\mathrm{J}$, $p_{\\mathrm{tail}}=0.01$, $x_m=2.0\\times 10^{-20}\\,\\mathrm{J}$, $\\nu=3.0$, $B=300$.\n- Case $4$ (small $m$ boundary behavior): $n=500$, $\\alpha=0.3$, $T=350\\,\\mathrm{K}$, $E_0=1.0\\times 10^{-19}\\,\\mathrm{J}$, $\\sigma=1.5\\times 10^{-21}\\,\\mathrm{J}$, $p_{\\mathrm{tail}}=0.03$, $x_m=5.0\\times 10^{-20}\\,\\mathrm{J}$, $\\nu=2.0$, $B=1000$.\n\nTake Boltzmann’s constant to be $k_B = 1.380649\\times 10^{-23}\\,\\mathrm{J}/\\mathrm{K}$. Use fixed, reproducible pseudorandom seeds for each case to make results deterministic.\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list of four-element lists, one per case, with each inner list containing the full-sample heat capacity estimate, the bootstrap standard error, and the lower and upper endpoints of the two-sided percentile confidence interval at confidence level $0.95$. The output must be enclosed in square brackets and contain no spaces, for example: $[[c_1,s_1,\\ell_1,u_1],[c_2,s_2,\\ell_2,u_2],[c_3,s_3,\\ell_3,u_3],[c_4,s_4,\\ell_4,u_4]]$ where $c_i$, $s_i$, $\\ell_i$, $u_i$ are decimal floats in $\\mathrm{J}/\\mathrm{K}$ for case $i$.", "solution": "The problem requires the design of a program to estimate the heat capacity at constant volume, $C_V$, from a finite sample of energies drawn from a canonical ensemble and to quantify the uncertainty of this estimate using the $m$-out-of-$n$ bootstrap. The energy samples are generated from a synthetic model exhibiting heavy-tailed distributions. The solution is presented in four parts: first, a derivation of the heat capacity formula from the principles of statistical mechanics; second, the formulation of an estimator based on a finite sample; third, a description of the synthetic data generation process; and fourth, a detailed exposition of the $m$-out-of-$n$ bootstrap procedure for uncertainty quantification.\n\n**1. Derivation of the Heat Capacity Formula**\n\nThe heat capacity at constant volume, $C_V$, is defined as the partial derivative of the mean energy, $\\langle E \\rangle$, with respect to temperature, $T$, at constant volume $V$:\n$$\nC_V = \\left( \\frac{\\partial \\langle E \\rangle}{\\partial T} \\right)_V\n$$\nIn the canonical ensemble, it is more convenient to work with the inverse temperature, $\\beta = 1/(k_B T)$, where $k_B$ is the Boltzmann constant. Using the chain rule, we can express the derivative with respect to $T$ in terms of a derivative with respect to $\\beta$:\n$$\n\\frac{\\partial}{\\partial T} = \\frac{d\\beta}{dT} \\frac{\\partial}{\\partial \\beta} = \\left( -\\frac{1}{k_B T^2} \\right) \\frac{\\partial}{\\partial \\beta} = -k_B \\beta^2 \\frac{\\partial}{\\partial \\beta}\n$$\nSubstituting this into the definition of $C_V$ yields:\n$$\nC_V = -k_B \\beta^2 \\left( \\frac{\\partial \\langle E \\rangle}{\\partial \\beta} \\right)_V\n$$\nThe mean energy $\\langle E \\rangle$ is related to the canonical partition function $Z(\\beta) = \\sum_i g(E_i) e^{-\\beta E_i}$, where $g(E_i)$ is the density of states for energy level $E_i$. The relation is given by:\n$$\n\\langle E \\rangle = -\\frac{\\partial}{\\partial \\beta} \\ln Z(\\beta)\n$$\nSubstituting this expression for $\\langle E \\rangle$ into the equation for $C_V$, we obtain:\n$$\nC_V = -k_B \\beta^2 \\frac{\\partial}{\\partial \\beta} \\left( -\\frac{\\partial}{\\partial \\beta} \\ln Z(\\beta) \\right) = k_B \\beta^2 \\frac{\\partial^2}{\\partial \\beta^2} \\ln Z(\\beta)\n$$\nTo evaluate the second derivative, we first recall that $\\frac{\\partial}{\\partial \\beta} \\ln Z(\\beta) = -\\langle E \\rangle$. Differentiating this with respect to $\\beta$ gives:\n$$\n\\frac{\\partial^2}{\\partial \\beta^2} \\ln Z(\\beta) = -\\frac{\\partial \\langle E \\rangle}{\\partial \\beta}\n$$\nThe derivative of the mean energy $\\langle E \\rangle = (\\sum_i E_i e^{-\\beta E_i})/Z$ with respect to $\\beta$ is:\n$$\n\\frac{\\partial \\langle E \\rangle}{\\partial \\beta} = \\frac{(\\sum_i -E_i^2 e^{-\\beta E_i})Z - (\\sum_i E_i e^{-\\beta E_i})(\\sum_j -E_j e^{-\\beta E_j})}{Z^2} = -\\frac{\\langle E^2 \\rangle Z^2}{Z^2} + \\frac{(\\langle E \\rangle Z)^2}{Z^2} = -(\\langle E^2 \\rangle - \\langle E \\rangle^2)\n$$\nThe term $\\langle E^2 \\rangle - \\langle E \\rangle^2$ is the variance of the energy, denoted $\\sigma_E^2$. Thus, we have:\n$$\n\\frac{\\partial^2}{\\partial \\beta^2} \\ln Z(\\beta) = \\langle E^2 \\rangle - \\langle E \\rangle^2 = \\sigma_E^2\n$$\nFinally, substituting this result back into our expression for $C_V$, we arrive at the fluctuation-dissipation formula for heat capacity:\n$$\nC_V = k_B \\beta^2 \\sigma_E^2 = k_B \\left( \\frac{1}{k_B T} \\right)^2 \\sigma_E^2 = \\frac{\\sigma_E^2}{k_B T^2}\n$$\nThis fundamental result connects a macroscopic thermodynamic property, the heat capacity, to the microscopic fluctuations in the system's energy.\n\n**2. Estimator for Finite Samples**\n\nGiven a finite sample of $n$ energy values $\\{E_1, E_2, \\dots, E_n\\}$ from a molecular dynamics simulation, we can construct an estimator for $C_V$. We replace the true population variance $\\sigma_E^2$ with its sample-based estimate. The sample variance is given by:\n$$\n\\hat{\\sigma}_E^2 = \\frac{1}{n} \\sum_{i=1}^n (E_i - \\bar{E})^2 = \\left(\\frac{1}{n} \\sum_{i=1}^n E_i^2\\right) - \\left(\\frac{1}{n} \\sum_{i=1}^n E_i\\right)^2\n$$\nwhere $\\bar{E} = \\frac{1}{n} \\sum_{i=1}^n E_i$ is the sample mean. The estimator for the heat capacity, $\\hat{C}_V$, is therefore:\n$$\n\\hat{C}_{V,n} = \\frac{\\hat{\\sigma}_E^2}{k_B T^2}\n$$\nThis is the estimator to be computed from the full synthetic dataset of size $n$.\n\n**3. Synthetic Heavy-Tailed Energy Model**\n\nThe problem specifies a synthetic data generation process to model energy fluctuations that include rare, large-magnitude events, characteristic of heavy-tailed distributions. For a sample of size $n$, each energy value $E_i$ for $i \\in \\{1, \\dots, n\\}$ is generated as:\n$$\nE_i = E_0 + G_i + J_i P_i\n$$\nThe components are:\n- $E_0$: A constant baseline energy.\n- $G_i$: A Gaussian fluctuation, $G_i \\sim \\mathcal{N}(0, \\sigma^2)$, representing typical thermal fluctuations around the mean.\n- $J_i$: A Bernoulli random variable, $J_i \\sim \\text{Bernoulli}(p_{\\mathrm{tail}})$, which acts as an indicator for a heavy-tailed event. $J_i=1$ with probability $p_{\\mathrm{tail}}$ and $J_i=0$ otherwise.\n- $P_i$: The magnitude of the heavy-tailed jump. It is defined as $P_i = x_m(1 + Y_i)$, where $x_m$ is a scale parameter and $Y_i$ is a random variate from a Pareto distribution with shape parameter $\\nu$ and support on $[1, \\infty)$. A standard Pareto distribution with shape $\\nu$ and scale (minimum value) $1$ has the required support. The variance of this Pareto distribution is finite only if $\\nu > 2$. Consequently, the variance of the total energy $E_i$ is finite only if $\\nu > 2$. This model allows for testing the statistical methods under conditions of both finite and infinite variance.\n\n**4. $m$-out-of-$n$ Bootstrap for Uncertainty Quantification**\n\nWhen the underlying data distribution has heavy tails, particularly infinite variance (i.e., $\\nu \\le 2$), the standard bootstrap method (where the resample size is $n$) fails to produce a consistent estimate of the sampling distribution. The $m$-out-of-$n$ bootstrap is a modification designed to handle such cases. It involves resampling a smaller number of data points, $m < n$, where $m$ is chosen to grow more slowly than $n$ (e.g., $m = \\lfloor n^\\alpha \\rfloor$ for $0 < \\alpha < 1$). This sub-sampling approach ensures the consistency of the bootstrap approximation under weaker moment conditions.\n\nThe procedure is as follows:\n1.  Compute the full-sample estimate $\\hat{C}_{V,n}$ from the dataset of size $n$.\n2.  Set the bootstrap resample size to $m = \\lfloor n^\\alpha \\rfloor$.\n3.  For each of $B$ bootstrap replicates:\n    a. Draw a sample of size $m$ with replacement from the original $n$ energy values. This gives a bootstrap sample $\\{E_1^*, \\dots, E_m^*\\}$.\n    b. Compute the heat capacity estimate on this bootstrap sample: $\\hat{C}_{V,m}^* = \\frac{\\text{Var}(\\{E_j^*\\})}{k_B T^2}$.\n4.  This process yields a bootstrap distribution of $B$ estimates, $\\{\\hat{C}_{V,m,1}^*, \\dots, \\hat{C}_{V,m,B}^*\\}$.\n\nFrom this bootstrap distribution, we compute the standard error and a confidence interval. The formulas must account for the difference in sample sizes between the original estimate ($n$) and the bootstrap replicates ($m$).\n\n-   **Bootstrap Standard Error:** The standard deviation of the bootstrap distribution, $\\text{std}(\\{\\hat{C}_{V,m,i}^*\\})$, is an estimate for the standard error of an estimator based on a sample of size $m$. To estimate the standard error for the original estimator based on size $n$, a scaling factor is required. Assuming the variance of the estimator scales as $1/(\\text{sample size})$, the standard error scales as $1/\\sqrt{\\text{sample size}}$. Thus, the estimated standard error of $\\hat{C}_{V,n}$ is:\n    $$\n    \\widehat{\\text{SE}}(\\hat{C}_{V,n}) = \\sqrt{\\frac{m}{n}} \\cdot \\text{std}(\\{\\hat{C}_{V,m,i}^*\\})\n    $$\n\n-   **Percentile Confidence Interval:** A simple percentile interval constructed directly from the quantiles of $\\{\\hat{C}_{V,m,i}^*\\}$ would be incorrect as it estimates an interval for a statistic based on size $m$, not $n$. A theoretically sound approach for the $m$-out-of-$n$ bootstrap is to use a properly scaled pivotal method. The distribution of the pivotal quantity $\\sqrt{n}(\\hat{C}_{V,n} - C_V)$ is approximated by the bootstrap distribution of $\\sqrt{m}(\\hat{C}_{V,m}^* - \\hat{C}_{V,n})$.\n    Let $q^*_{p}$ denote the $p$-th quantile of the empirical distribution of $\\{\\sqrt{m}(\\hat{C}_{V,m,i}^* - \\hat{C}_{V,n})\\}_{i=1}^B$. A $(1-\\gamma)$ confidence interval for $C_V$ is then given by:\n    $$\n    \\left[ \\hat{C}_{V,n} - \\frac{q^*_{1-\\gamma/2}}{\\sqrt{n}}, \\quad \\hat{C}_{V,n} - \\frac{q^*_{\\gamma/2}}{\\sqrt{n}} \\right]\n    $$\n    For a $95\\%$ confidence interval, $\\gamma = 0.05$, so we use the $2.5$-th and $97.5$-th percentiles of the centered and scaled bootstrap distribution.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import pareto\n\ndef run_case(n, alpha, T, E0, sigma, p_tail, xm, nu, B, seed):\n    \"\"\"\n    Runs a single test case for heat capacity estimation and uncertainty quantification.\n\n    Args:\n        n (int): Total number of energy samples.\n        alpha (float): Exponent for determining m-out-of-n bootstrap sample size.\n        T (float): Temperature in Kelvin.\n        E0 (float): Baseline energy offset in Joules.\n        sigma (float): Gaussian fluctuation scale in Joules.\n        p_tail (float): Probability of a heavy-tailed jump.\n        xm (float): Pareto jump scale parameter in Joules.\n        nu (float): Pareto shape parameter.\n        B (int): Number of bootstrap replicates.\n        seed (int): Seed for the random number generator.\n\n    Returns:\n        tuple: A 4-element tuple containing:\n            - c_v_n (float): Heat capacity estimate from the full sample (J/K).\n            - se_c_v (float): Bootstrap standard error of the estimate (J/K).\n            - ci_low (float): Lower bound of the 95% confidence interval (J/K).\n            - ci_high (float): Upper bound of the 95% confidence interval (J/K).\n    \"\"\"\n    # Boltzmann constant in J/K\n    K_B = 1.380649e-23\n\n    # Initialize random number generator for reproducibility\n    rng = np.random.default_rng(seed)\n\n    # 1. Generate synthetic heavy-tailed energy samples\n    gauss_fluct = rng.normal(0.0, sigma, n)\n    jump_indicator = rng.binomial(1, p_tail, n)\n    pareto_y = pareto.rvs(b=nu, size=n, random_state=rng)\n    pareto_jump = xm * (1.0 + pareto_y)\n    \n    energies = E0 + gauss_fluct + jump_indicator * pareto_jump\n\n    # 2. Compute heat capacity estimator from the full sample of size n\n    # C_V = Var(E) / (k_B * T^2)\n    var_n = np.var(energies, ddof=0)\n    c_v_n = var_n / (K_B * T**2)\n    \n    # 3. Perform m-out-of-n bootstrap\n    m = int(np.floor(n**alpha))\n    bootstrap_estimates = np.zeros(B)\n    \n    # Generate all indices for bootstrap samples at once for efficiency\n    bootstrap_indices = rng.choice(n, size=(B, m), replace=True)\n    \n    # Use array operations to compute variances for all bootstrap samples\n    bootstrap_samples = energies[bootstrap_indices]\n    var_m_star = np.var(bootstrap_samples, axis=1, ddof=0)\n    \n    bootstrap_estimates = var_m_star / (K_B * T**2)\n\n    # 4. Compute bootstrap standard error and confidence interval\n    \n    # Bootstrap Standard Error\n    # SE(C_V_n) ≈ sqrt(m/n) * std(C_V_m*)\n    std_bootstrap_dist = np.std(bootstrap_estimates, ddof=1)\n    se_c_v = np.sqrt(m / n) * std_bootstrap_dist\n    \n    # Bootstrap Confidence Interval (scaled pivotal method)\n    # The interval for C_V is [C_V_n - q*_0.975/√n, C_V_n - q*_0.025/√n]\n    # where q* are quantiles of sqrt(m)(C_V_m* - C_V_n)\n    scaled_deltas = np.sqrt(m) * (bootstrap_estimates - c_v_n)\n    \n    # Quantiles at 2.5% and 97.5%\n    q_low_star = np.percentile(scaled_deltas, 2.5)\n    q_high_star = np.percentile(scaled_deltas, 97.5)\n    \n    # Confidence Interval\n    ci_low = c_v_n - q_high_star / np.sqrt(n)\n    ci_high = c_v_n - q_low_star / np.sqrt(n)\n    \n    return c_v_n, se_c_v, ci_low, ci_high\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    test_cases = [\n        # Case 1: general heavy-tailed, finite variance\n        {'n': 1000, 'alpha': 0.7, 'T': 300.0, 'E0': 1.0e-19, 'sigma': 2.0e-21, \n         'p_tail': 0.02, 'xm': 3.0e-20, 'nu': 2.5, 'B': 1000, 'seed': 0},\n        # Case 2: strong heavy tail, infinite variance\n        {'n': 100, 'alpha': 0.5, 'T': 300.0, 'E0': 8.0e-20, 'sigma': 4.0e-21,\n         'p_tail': 0.05, 'xm': 1.0e-19, 'nu': 1.5, 'B': 1000, 'seed': 1},\n        # Case 3: large sample, mild heavy tail\n        {'n': 10000, 'alpha': 0.9, 'T': 300.0, 'E0': 1.2e-19, 'sigma': 2.0e-21,\n         'p_tail': 0.01, 'xm': 2.0e-20, 'nu': 3.0, 'B': 300, 'seed': 2},\n        # Case 4: small m boundary behavior\n        {'n': 500, 'alpha': 0.3, 'T': 350.0, 'E0': 1.0e-19, 'sigma': 1.5e-21,\n         'p_tail': 0.03, 'xm': 5.0e-20, 'nu': 2.0, 'B': 1000, 'seed': 3}\n    ]\n\n    results_str_list = []\n    for params in test_cases:\n        result = run_case(**params)\n        results_str_list.append(f\"[{result[0]},{result[1]},{result[2]},{result[3]}]\")\n    \n    # Final print statement in the exact required format\n    print(f\"[{','.join(results_str_list)}]\")\n\nsolve()\n```", "id": "3399551"}]}