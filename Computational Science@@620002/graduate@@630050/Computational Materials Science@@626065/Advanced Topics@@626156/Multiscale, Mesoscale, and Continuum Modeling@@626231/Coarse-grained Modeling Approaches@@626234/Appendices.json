{"hands_on_practices": [{"introduction": "One of the most common goals in coarse-graining is to create a model that reproduces the structure of a more detailed system. The Iterative Boltzmann Inversion (IBI) method is a powerful and intuitive technique for achieving this by systematically refining a potential to match a target radial distribution function, $g(r)$. This exercise provides direct practice with the core update rule of the IBI algorithm [@problem_id:138327], allowing you to focus on the fundamental relationship between the potential correction, the temperature, and the logarithmic ratio of the simulated and target distribution functions, which is a cornerstone of structure-based coarse-graining.", "problem": "The pair correlation function, $g(r)$, also known as the radial distribution function, is a central quantity in statistical mechanics describing the structure of a fluid. In coarse-graining methods, one often seeks an effective pair potential, $U(r)$, that can reproduce a known target pair correlation function, $g_{target}(r)$, in a computer simulation. The Iterative Boltzmann Inversion (IBI) method is a popular technique for this purpose.\n\nThe IBI algorithm refines the potential iteratively. Starting with an initial guess for the potential, $U_0(r)$, a simulation is performed to obtain the corresponding pair correlation function $g_0(r)$. The potential is then updated according to the rule:\n$$ U_{i+1}(r) = U_i(r) + k_B T \\ln\\left(\\frac{g_i(r)}{g_{target}(r)}\\right) $$\nwhere $i$ is the iteration number, $k_B$ is the Boltzmann constant, and $T$ is the absolute temperature.\n\nConsider a system where the target pair correlation function is given by the analytical expression:\n$$ g_{target}(r) = \\left(1 - \\exp\\left(-\\frac{r}{\\lambda}\\right)\\right) \\left( 1 + A \\exp\\left(-\\frac{(r-r_p)^2}{2w^2}\\right) \\right) $$\nAn initial guess for the potential, $U_{guess}(r)$, is used in a simulation, which yields a pair correlation function given by:\n$$ g_{guess}(r) = \\left(1 - \\exp\\left(-\\frac{r}{\\lambda}\\right)\\right) \\left( 1 + \\frac{A}{2} \\exp\\left(-\\frac{(r-r_p)^2}{2w^2}\\right) \\right) $$\nHere, $A$, $\\lambda$, $r_p$, and $w$ are positive constants.\n\nUsing the first step of the IBI method, calculate the correction to the potential, $\\Delta U(r) = U_1(r) - U_{guess}(r)$, at the specific radial distance corresponding to the peak of the structural feature, $r = r_p$. Express your answer in terms of $k_B$, $T$, and $A$.", "solution": "1.  **State the IBI Correction Formula**\n    The potential correction $\\Delta U(r)$ in a single IBI step is given by:\n    $$\\Delta U(r) = k_B T \\ln\\left(\\frac{g_{\\text{guess}}(r)}{g_{\\text{target}}(r)}\\right)$$\n\n2.  **Evaluate the Distribution Functions at $r = r_p$**\n    At the peak position $r=r_p$, the exponential term $\\exp\\left(-\\frac{(r-r_p)^2}{2w^2}\\right)$ becomes $\\exp(0) = 1$.\n    Substituting $r=r_p$ into the given expressions for $g_{target}(r)$ and $g_{guess}(r)$:\n    $$g_{\\text{target}}(r_p)=(1-e^{-r_p/\\lambda})(1+A)$$\n    $$g_{\\text{guess}}(r_p)=(1-e^{-r_p/\\lambda})\\left(1+\\frac{A}{2}\\right)$$\n\n3.  **Calculate the Ratio**\n    The ratio of the guessed to the target distribution function at $r=r_p$ is:\n    $$\\frac{g_{\\text{guess}}(r_p)}{g_{\\text{target}}(r_p)} = \\frac{(1-e^{-r_p/\\lambda})(1+\\frac{A}{2})}{(1-e^{-r_p/\\lambda})(1+A)} = \\frac{1+\\frac{A}{2}}{1+A}$$\n\n4.  **Determine the Potential Correction**\n    Substitute the ratio back into the IBI correction formula:\n    $$\\Delta U(r_p) = k_B T \\ln\\left(\\frac{1+\\frac{A}{2}}{1+A}\\right)$$", "answer": "$$\\boxed{k_B T \\ln\\left(\\frac{1+\\frac{A}{2}}{1+A}\\right)}$$", "id": "138327"}, {"introduction": "Beyond structure, a good coarse-grained model should also capture key macroscopic properties of the system, such as its response to pressure. Dissipative Particle Dynamics (DPD) is a popular mesoscale method where parameters are often tuned to match such thermodynamic quantities. This problem guides you through the process of linking a macroscopic observable—the isothermal bulk modulus—to a microscopic interaction parameter in a DPD simulation [@problem_id:3438715], demonstrating a \"top-down\" parameterization strategy that is vital for developing models that are not only structurally accurate but also thermodynamically consistent.", "problem": "Consider a single-component fluid modeled by Dissipative Particle Dynamics (DPD), where the conservative interactions are characterized by a pairwise force amplitude $a$ and the pressure as a function of number density $\\rho$ obeys an empirical equation of state of the form $p(\\rho)=\\rho k_{B}T+\\alpha a \\rho^{2}$. Here $k_{B}$ is the Boltzmann constant, $T$ is the absolute temperature, and $\\alpha$ is a dimensionless coefficient determined by the choice of weight function and cutoff. Let the isothermal compressibility be defined from first principles by $\\kappa_{T}=-\\frac{1}{V}\\left(\\frac{\\partial V}{\\partial p}\\right)_{T}$ for a system of fixed particle number. \n\nStarting from this definition and the given equation of state, derive an expression for the conservative force amplitude $a$ that yields a target isothermal bulk modulus $B_{T}=\\kappa_{T}^{-1}$ at a specified density $\\rho$. Then evaluate $a$ numerically for the following parameters expressed in reduced DPD units: $\\alpha=0.101$, $\\rho=3.000$, $k_{B}T=1.000$, and target isothermal bulk modulus $B_{T}=48.45$. Express the final value of $a$ in reduced DPD units, and round your answer to four significant figures.", "solution": "The objective is to derive an expression for the conservative force amplitude, $a$, in a Dissipative Particle Dynamics (DPD) model that reproduces a target isothermal bulk modulus, $B_T$.\n\n1.  **Relate Bulk Modulus to the Equation of State**\n\n    The isothermal bulk modulus, $B_T$, is the inverse of the isothermal compressibility, $\\kappa_T$:\n    $$B_T = \\frac{1}{\\kappa_T}$$\n    The definition of isothermal compressibility for a system with a fixed number of particles $N$ is:\n    $$\\kappa_T = -\\frac{1}{V}\\left(\\frac{\\partial V}{\\partial p}\\right)_T$$\n    To use the given equation of state, which is a function of number density $\\rho = N/V$, we express the derivative in terms of $\\rho$. Using the chain rule, one can derive the standard thermodynamic relation:\n    $$B_T = \\rho\\left(\\frac{\\partial p}{\\partial \\rho}\\right)_T$$\n\n2.  **Derive the Expression for 'a'**\n\n    The given DPD equation of state is:\n    $$p(\\rho) = \\rho k_{B}T + \\alpha a \\rho^{2}$$\n    We take its derivative with respect to density at constant temperature:\n    $$\\left(\\frac{\\partial p}{\\partial \\rho}\\right)_T = \\frac{\\partial}{\\partial \\rho}\\left(\\rho k_{B}T + \\alpha a \\rho^{2}\\right) = k_{B}T + 2\\alpha a \\rho$$\n    Substituting this into the expression for $B_T$:\n    $$B_T = \\rho(k_{B}T + 2\\alpha a \\rho) = \\rho k_{B}T + 2\\alpha a \\rho^{2}$$\n    Now, we rearrange the equation to solve for the force amplitude $a$:\n    $$a = \\frac{B_T - \\rho k_{B}T}{2\\alpha \\rho^{2}}$$\n    This is the required analytical expression.\n\n3.  **Numerical Calculation**\n\n    We substitute the given values in reduced DPD units:\n    *   $\\alpha = 0.101$\n    *   $\\rho = 3.000$\n    *   $k_{B}T = 1.000$\n    *   $B_{T} = 48.45$\n\n    Plugging these into our derived formula for $a$:\n    $$a = \\frac{48.45 - (3.000)(1.000)}{2(0.101)(3.000)^{2}} = \\frac{45.45}{2(0.101)(9.000)} = \\frac{45.45}{1.818} = 25.0$$\n    Rounding to four significant figures as requested, the final value is $25.00$.", "answer": "$$\n\\boxed{25.00}\n$$", "id": "3438715"}, {"introduction": "A powerful \"bottom-up\" approach to coarse-graining is force matching, where the effective forces in the coarse-grained model are directly parameterized to match the mean forces from an underlying atomistic simulation. This is fundamentally a regression problem, often requiring modern data science techniques to solve robustly. This exercise challenges you to implement a complete force-matching workflow, from constructing the design matrix to using Tikhonov regularization and cross-validation to find the optimal model parameters [@problem_id:3438724], providing invaluable practical experience in building robust and predictive coarse-grained models from raw simulation data.", "problem": "You are given a coarse-grained force-matching task in dimensionless reduced units. In coarse-grained (CG) modeling of materials from atomistic (AA) simulations, one frequently approximates the CG force as a linear combination of basis functions evaluated on the coarse-grained coordinates. Let $\\mathbf{R}_{t} \\in \\mathbb{R}^{d}$ denote the coarse-grained coordinates at discrete times $t$, and let $F^{\\mathrm{AA}\\rightarrow \\mathrm{CG}}_{t} \\in \\mathbb{R}$ denote the scalar component of the mapped atomistic force along a chosen coarse coordinate. Consider a set of basis functions $\\{\\phi_{k}(\\mathbf{R})\\}_{k=1}^{K}$, and suppose the CG force is parameterized as $F^{\\mathrm{CG}}(\\mathbf{R};\\boldsymbol{\\theta}) = \\sum_{k=1}^{K} \\theta_{k}\\,\\phi_{k}(\\mathbf{R})$, where $\\boldsymbol{\\theta}\\in\\mathbb{R}^{K}$ are unknown coefficients to be inferred.\n\nStarting from the principle of least squares and Tikhonov regularization, define and minimize the objective that balances data fit and parameter magnitude. Construct the design matrix so that rows correspond to samples $t$ and columns to basis functions $k$. Derive the stationarity condition for the minimizer, and implement a solution that returns $\\boldsymbol{\\theta}$ for a given regularization strength $\\lambda$. Explain and implement a selection of $\\lambda$ using $K$-fold cross-validation to avoid overfitting, including a clear tie-breaking rule. All computations are to be carried out in dimensionless reduced units.\n\nYou must implement a single program that:\n- Builds the design matrix from the provided basis functions and sample coordinates.\n- Solves for $\\boldsymbol{\\theta}$ using Tikhonov regularization with candidate $\\lambda$ values, selected by $K$-fold cross-validation minimizing average validation mean squared error.\n- In case of exact ties in average validation error (to within machine precision), select the smallest $\\lambda$.\n- Produces a single-line output aggregating the results for all test cases as specified below.\n\nUse the following test suite of three cases, all in dimensionless reduced units (that is, no physical unit conversion is required):\n\nCase A (well-conditioned, leave-one-out):\n- Basis functions: $\\phi_{1}(\\mathbf{R})=1$, $\\phi_{2}(\\mathbf{R})=R$, $\\phi_{3}(\\mathbf{R})=R^{2}$.\n- True parameters: $\\boldsymbol{\\theta}^{\\mathrm{true}}=[0.4,-1.5,0.7]$.\n- Samples: $R_{t}\\in\\mathbb{R}$ with values $[-1.0,-0.5,0.0,0.5,1.0]$.\n- Deterministic residual: $\\delta(R)=0.01\\sin(\\pi R)$.\n- Targets: $F^{\\mathrm{AA}\\rightarrow \\mathrm{CG}}_{t}=\\sum_{k}\\theta^{\\mathrm{true}}_{k}\\,\\phi_{k}(R_{t})+\\delta(R_{t})$.\n- Candidate regularizations: $\\lambda\\in[0.0,10^{-6},10^{-3},10^{-2},10^{-1}]$.\n- Folds: $K=5$.\n\nCase B (nearly collinear basis, moderate $K$):\n- Basis functions: $\\phi_{1}(\\mathbf{R})=1$, $\\phi_{2}(\\mathbf{R})=R$, $\\phi_{3}(\\mathbf{R})=R+10^{-8}$.\n- True parameters: $\\boldsymbol{\\theta}^{\\mathrm{true}}=[1.0,0.5,-0.5]$.\n- Samples: $R_{t}\\in\\mathbb{R}$ with values $[-1.0,-0.75,-0.5,-0.25,0.0,0.25,0.5,0.75,1.0]$.\n- Deterministic residual: $\\delta(R)=0.02\\sin(\\pi R)$.\n- Targets: $F^{\\mathrm{AA}\\rightarrow \\mathrm{CG}}_{t}=\\sum_{k}\\theta^{\\mathrm{true}}_{k}\\,\\phi_{k}(R_{t})+\\delta(R_{t})$.\n- Candidate regularizations: $\\lambda\\in[10^{-6},10^{-4},10^{-2},10^{-1},1.0]$.\n- Folds: $K=3$.\n\nCase C (underdetermined system, strict regularization):\n- Basis functions: $\\phi_{1}(\\mathbf{R})=1$, $\\phi_{2}(\\mathbf{R})=R$, $\\phi_{3}(\\mathbf{R})=R^{2}$.\n- True parameters: $\\boldsymbol{\\theta}^{\\mathrm{true}}=[-0.2,0.5,0.3]$.\n- Samples: $R_{t}\\in\\mathbb{R}$ with values $[-0.2,0.8]$.\n- Deterministic residual: $\\delta(R)=0.05\\sin(\\pi R)$.\n- Targets: $F^{\\mathrm{AA}\\rightarrow \\mathrm{CG}}_{t}=\\sum_{k}\\theta^{\\mathrm{true}}_{k}\\,\\phi_{k}(R_{t})+\\delta(R_{t})$.\n- Candidate regularizations: $\\lambda\\in[10^{-3},10^{-2},10^{-1},1.0]$.\n- Folds: $K=2$.\n\nAlgorithmic requirements:\n- For each case, perform $K$-fold cross-validation over candidate $\\lambda$ to select the one that minimizes the average validation mean squared error.\n- Use the selected $\\lambda$ to fit $\\boldsymbol{\\theta}$ on the full dataset.\n- If the linear system with a given $\\lambda$ is ill-conditioned or singular, use a numerically stable solver such as a pseudoinverse to obtain $\\boldsymbol{\\theta}$.\n\nOutput specification:\n- For each case, output a list containing the selected $\\lambda$ as a float, the average validation mean squared error as a float, followed by the fitted $\\boldsymbol{\\theta}$ entries in order.\n- Your program should produce a single line of output containing the results for the three test cases as a comma-separated list enclosed in square brackets, where each element is the per-case list described above, for example $[[\\lambda_{A},\\mathrm{MSE}_{A},\\theta^{A}_{1},\\theta^{A}_{2},\\theta^{A}_{3}],[\\lambda_{B},\\mathrm{MSE}_{B},\\theta^{B}_{1},\\theta^{B}_{2},\\theta^{B}_{3}],[\\lambda_{C},\\mathrm{MSE}_{C},\\theta^{C}_{1},\\theta^{C}_{2},\\theta^{C}_{3}]]$.\n\nAngle units: All angles in the residual definition use radians. No percentages are used anywhere; any ratios should be returned as decimal numbers. All quantities are dimensionless reduced units.\n\nEnsure scientific realism and implement from first principles: start from least squares and regularization, construct the design matrix explicitly, derive the first-order optimality condition for the minimizer, and implement $K$-fold cross-validation to select $\\lambda$ based on validation error. No formulas beyond these principles are to be provided in the problem statement itself.", "solution": "The problem is to determine the optimal coarse-grained (CG) force parameters $\\boldsymbol{\\theta}$ by fitting to mapped atomistic (AA) force data. The CG force, $F^{\\mathrm{CG}}$, is modeled as a linear combination of $K$ basis functions, $\\phi_k(\\mathbf{R})$, evaluated at the CG coordinates $\\mathbf{R}$.\n\nThe model is given by:\n$$F^{\\mathrm{CG}}(\\mathbf{R};\\boldsymbol{\\theta}) = \\sum_{k=1}^{K} \\theta_{k}\\,\\phi_{k}(\\mathbf{R})$$\nwhere $\\boldsymbol{\\theta} = [\\theta_1, \\theta_2, \\dots, \\theta_K]^T$ is the vector of unknown coefficients.\n\nGiven a set of $N$ discrete-time samples, where at each time $t$ we have the CG coordinates $\\mathbf{R}_t$ and the target mapped force $y_t = F^{\\mathrm{AA}\\rightarrow \\mathrm{CG}}_{t}$, we can express the problem in matrix form. Let $\\mathbf{y} \\in \\mathbb{R}^{N}$ be the vector of target forces, and let $\\mathbf{\\Phi}$ be the $N \\times K$ design matrix, where each element $\\Phi_{tk} = \\phi_k(\\mathbf{R}_t)$. The vector of predicted forces for all samples is then $\\hat{\\mathbf{y}} = \\mathbf{\\Phi}\\boldsymbol{\\theta}$.\n\nThe principle of least squares seeks to find $\\boldsymbol{\\theta}$ by minimizing the sum of squared residuals, which is the squared Euclidean norm of the difference between the target forces and the predicted forces:\n$$J_{\\text{LS}}(\\boldsymbol{\\theta}) = \\sum_{t=1}^{N} (y_t - \\hat{y}_t)^2 = ||\\mathbf{y} - \\mathbf{\\Phi}\\boldsymbol{\\theta}||_2^2$$\n\nTo prevent overfitting and to handle cases where the design matrix leads to an ill-conditioned or singular system (e.g., nearly collinear basis functions or an underdetermined system where $N  K$), we introduce Tikhonov regularization. This adds a penalty term to the objective function proportional to the squared magnitude of the parameter vector, controlled by a regularization strength parameter $\\lambda \\ge 0$. The new objective function is:\n$$J(\\boldsymbol{\\theta}) = ||\\mathbf{y} - \\mathbf{\\Phi}\\boldsymbol{\\theta}||_2^2 + \\lambda ||\\boldsymbol{\\theta}||_2^2$$\nThis is also known as ridge regression.\n\nTo find the optimal parameter vector $\\hat{\\boldsymbol{\\theta}}$ that minimizes $J(\\boldsymbol{\\theta})$, we find the stationarity condition by taking the gradient of $J(\\boldsymbol{\\theta})$ with respect to $\\boldsymbol{\\theta}$ and setting it to zero.\nExpanding the objective function:\n$$J(\\boldsymbol{\\theta}) = (\\mathbf{y} - \\mathbf{\\Phi}\\boldsymbol{\\theta})^T(\\mathbf{y} - \\mathbf{\\Phi}\\boldsymbol{\\theta}) + \\lambda \\boldsymbol{\\theta}^T\\mathbf{I}\\boldsymbol{\\theta} = \\mathbf{y}^T\\mathbf{y} - 2\\boldsymbol{\\theta}^T\\mathbf{\\Phi}^T\\mathbf{y} + \\boldsymbol{\\theta}^T\\mathbf{\\Phi}^T\\mathbf{\\Phi}\\boldsymbol{\\theta} + \\lambda \\boldsymbol{\\theta}^T\\mathbf{I}\\boldsymbol{\\theta}$$\n$$J(\\boldsymbol{\\theta}) = \\mathbf{y}^T\\mathbf{y} - 2\\boldsymbol{\\theta}^T\\mathbf{\\Phi}^T\\mathbf{y} + \\boldsymbol{\\theta}^T(\\mathbf{\\Phi}^T\\mathbf{\\Phi} + \\lambda\\mathbf{I})\\boldsymbol{\\theta}$$\nThe gradient with respect to $\\boldsymbol{\\theta}$ is:\n$$\\nabla_{\\boldsymbol{\\theta}} J(\\boldsymbol{\\theta}) = -2\\mathbf{\\Phi}^T\\mathbf{y} + 2(\\mathbf{\\Phi}^T\\mathbf{\\Phi} + \\lambda\\mathbf{I})\\boldsymbol{\\theta}$$\nSetting the gradient to zero, $\\nabla_{\\boldsymbol{\\theta}} J(\\boldsymbol{\\theta}) = \\mathbf{0}$, yields the normal equations for the regularized system:\n$$(\\mathbf{\\Phi}^T\\mathbf{\\Phi} + \\lambda\\mathbf{I})\\boldsymbol{\\theta} = \\mathbf{\\Phi}^T\\mathbf{y}$$\nThe solution for $\\boldsymbol{\\theta}$ is:\n$$\\hat{\\boldsymbol{\\theta}} = (\\mathbf{\\Phi}^T\\mathbf{\\Phi} + \\lambda\\mathbf{I})^{-1} \\mathbf{\\Phi}^T\\mathbf{y}$$\nFor $\\lambda > 0$, the matrix $(\\mathbf{\\Phi}^T\\mathbf{\\Phi} + \\lambda\\mathbf{I})$ is guaranteed to be invertible, providing a unique solution even for ill-posed problems. The problem statement requires using a numerically stable solver, which is crucial for ill-conditioned matrices. We will solve the linear system $A\\boldsymbol{x}=\\boldsymbol{b}$ with $A = \\mathbf{\\Phi}^T\\mathbf{\\Phi} + \\lambda\\mathbf{I}$ and $\\boldsymbol{b} = \\mathbf{\\Phi}^T\\mathbf{y}$.\n\nThe choice of $\\lambda$ is critical: if $\\lambda$ is too small, the model may overfit; if too large, it may underfit. We employ $K$-fold cross-validation to select an optimal $\\lambda$ from a list of candidates. The procedure is as follows:\n$1.$ The dataset of $N$ samples is partitioned into $K$ disjoint subsets (folds) of approximately equal size.\n$2.$ For each candidate value of $\\lambda$:\n    a. An average validation error is initialized to zero.\n    b. For each fold $k \\in \\{1, \\dots, K\\}$:\n        i. The fold $k$ is designated as the validation set. The remaining $K-1$ folds are combined to form the training set.\n        ii. The model parameters, $\\boldsymbol{\\theta}_{\\text{train}}$, are fitted using the training data and the current $\\lambda$.\n        iii. The fitted model is used to predict a force for each sample in the validation set.\n        iv. The mean squared error (MSE) on the validation set is computed: $\\text{MSE}_k(\\lambda) = \\frac{1}{N_k} ||\\mathbf{y}_{\\text{val}} - \\mathbf{\\Phi}_{\\text{val}}\\boldsymbol{\\theta}_{\\text{train}}||_2^2$, where $N_k$ is the number of samples in fold $k$.\n    c. The average validation error for $\\lambda$ is calculated as $\\overline{\\text{MSE}}(\\lambda) = \\frac{1}{K} \\sum_{k=1}^{K} \\text{MSE}_k(\\lambda)$.\n$3.$ The optimal regularization strength, $\\lambda^*$, is chosen as the one that minimizes $\\overline{\\text{MSE}}(\\lambda)$. The problem specifies a tie-breaking rule: if multiple $\\lambda$ values yield the same minimum $\\overline{\\text{MSE}}$ (within machine precision), the smallest $\\lambda$ is selected.\n$4.$ Finally, the definitive model parameters, $\\boldsymbol{\\theta}_{\\text{final}}$, are obtained by training on the entire dataset using the selected $\\lambda^*$.\n\nThis complete procedure is implemented for each of the three test cases. The implementation builds the design matrix $\\mathbf{\\Phi}$ and target vector $\\mathbf{y}$ for each case, performs $K$-fold cross-validation to find $\\lambda^*$, and then computes the final $\\boldsymbol{\\theta}_{\\text{final}}$. The results, including $\\lambda^*$, the corresponding minimum average validation MSE, and the components of $\\boldsymbol{\\theta}_{\\text{final}}$, are aggregated and formatted as per the output specification.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the coarse-grained force-matching problem for three test cases,\n    using Tikhonov regularization with lambda selected by K-fold cross-validation.\n    \"\"\"\n\n    # Case A: Well-conditioned, leave-one-out\n    R_A = np.array([-1.0, -0.5, 0.0, 0.5, 1.0])\n    theta_true_A = np.array([0.4, -1.5, 0.7])\n    basis_A = [lambda R: 1.0, lambda R: R, lambda R: R**2]\n    delta_A = lambda R: 0.01 * np.sin(np.pi * R)\n    phi_matrix_A = np.array([[func(r) for func in basis_A] for r in R_A])\n    y_A = phi_matrix_A @ theta_true_A + delta_A(R_A)\n    lambdas_A = [0.0, 1e-6, 1e-3, 1e-2, 1e-1]\n    K_A = 5\n\n    # Case B: Nearly collinear basis, moderate K\n    R_B = np.linspace(-1.0, 1.0, 9)\n    theta_true_B = np.array([1.0, 0.5, -0.5])\n    basis_B = [lambda R: 1.0, lambda R: R, lambda R: R + 1e-8]\n    delta_B = lambda R: 0.02 * np.sin(np.pi * R)\n    phi_matrix_B = np.array([[func(r) for func in basis_B] for r in R_B])\n    y_B = phi_matrix_B @ theta_true_B + delta_B(R_B)\n    lambdas_B = [1e-6, 1e-4, 1e-2, 1e-1, 1.0]\n    K_B = 3\n\n    # Case C: Underdetermined system, strict regularization\n    R_C = np.array([-0.2, 0.8])\n    theta_true_C = np.array([-0.2, 0.5, 0.3])\n    basis_C = [lambda R: 1.0, lambda R: R, lambda R: R**2]\n    delta_C = lambda R: 0.05 * np.sin(np.pi * R)\n    phi_matrix_C = np.array([[func(r) for func in basis_C] for r in R_C])\n    y_C = phi_matrix_C @ theta_true_C + delta_C(R_C)\n    lambdas_C = [1e-3, 1e-2, 1e-1, 1.0]\n    K_C = 2\n\n    test_cases = [\n        (phi_matrix_A, y_A, lambdas_A, K_A),\n        (phi_matrix_B, y_B, lambdas_B, K_B),\n        (phi_matrix_C, y_C, lambdas_C, K_C),\n    ]\n\n    results = []\n    \n    for Phi, y_targets, lambdas, K_folds in test_cases:\n        num_samples, num_basis = Phi.shape\n        indices = np.arange(num_samples)\n        \n        # We need to handle numpy's deprecation warning for ragged arrays\n        # by creating a list of arrays instead of a single object-dtype array\n        if num_samples % K_folds == 0:\n            fold_indices = np.split(indices, K_folds)\n        else:\n            # np.array_split is more general\n            fold_indices = np.array_split(indices, K_folds)\n\n        avg_mses = []\n\n        for lam in lambdas:\n            fold_mses = []\n            for i in range(K_folds):\n                val_idx = fold_indices[i]\n                train_idx = np.concatenate([fold_indices[j] for j in range(K_folds) if i != j])\n                \n                Phi_train, y_train = Phi[train_idx], y_targets[train_idx]\n                Phi_val, y_val = Phi[val_idx], y_targets[val_idx]\n                \n                A = Phi_train.T @ Phi_train + lam * np.identity(num_basis)\n                b = Phi_train.T @ y_train\n                \n                try:\n                    theta_train = np.linalg.solve(A, b)\n                except np.linalg.LinAlgError:\n                    # Fallback to pseudoinverse for singular matrix (e.g., lambda=0)\n                    theta_train = np.linalg.pinv(A) @ b\n\n                y_pred_val = Phi_val @ theta_train\n                mse = np.mean((y_val - y_pred_val)**2)\n                fold_mses.append(mse)\n            \n            avg_mses.append(np.mean(fold_mses))\n\n        # Select best lambda with tie-breaking\n        min_mse = np.min(avg_mses)\n        best_lambda_candidates = [lam for lam, mse in zip(lambdas, avg_mses) if np.isclose(mse, min_mse)]\n        best_lambda = min(best_lambda_candidates)\n        \n        # Retrieve the MSE for the selected lambda\n        best_avg_mse = avg_mses[lambdas.index(best_lambda)]\n\n        # Final fit on all data with the best lambda\n        A_final = Phi.T @ Phi + best_lambda * np.identity(num_basis)\n        b_final = Phi.T @ y_targets\n        try:\n            theta_final = np.linalg.solve(A_final, b_final)\n        except np.linalg.LinAlgError:\n            theta_final = nplinalg.pinv(A_final) @ b_final\n\n        case_result = [best_lambda, best_avg_mse] + theta_final.tolist()\n        results.append(case_result)\n\n    # Final print statement in the exact required format.\n    print(str(results).replace(\" \", \"\"))\n\nsolve()\n```", "id": "3438724"}]}