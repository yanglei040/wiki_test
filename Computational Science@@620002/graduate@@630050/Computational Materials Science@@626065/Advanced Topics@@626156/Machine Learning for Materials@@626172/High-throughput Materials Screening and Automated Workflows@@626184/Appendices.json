{"hands_on_practices": [{"introduction": "The first step in high-throughput materials discovery is to define the vast search space of potential compounds. This practice introduces a fundamental technique for enumerating the number of unique crystal structures that can be generated by decorating a prototype lattice with different elements [@problem_id:3456708]. By treating symmetry-equivalent sites as indistinguishable, we can translate this complex crystallographic problem into a classic combinatorial challenge, providing a powerful method to estimate the scale of a computational screening campaign.", "problem": "You are given prototype crystal structures and allowed element substitution sets intended for high-throughput materials screening in computational materials science. The goal is to algorithmically estimate the number of distinct decorated structures modulo space-group symmetries, assuming an automated workflow where symmetry-equivalent positions are treated as indistinguishable within a Wyckoff orbit. The derivation must start from the mathematical notion of a group action and the definition of an orbit, without using canned enumeration formulas.\n\nFundamental base to use:\n- A space group acts on the set of Wyckoff positions by permuting symmetry-equivalent sites.\n- An assignment of elements to sites can be viewed as a labeling under this action.\n- Distinct decorated structures modulo symmetry correspond to orbits of the labeling under the group action.\n\nScenario:\n- Consider the prototype set $P=\\{\\text{perovskite},\\text{spinel}\\}$.\n- For perovskite of type $ABO_3$ in space group $\\mathrm{Pm}\\bar{3}\\mathrm{m}$, assume Wyckoff orbit multiplicities $n_A=1$, $n_B=1$, $n_O=3$ for the $A$, $B$, and $O$ sublattices, respectively.\n- For spinel of type $AB_2O_4$ in space group $\\mathrm{Fd}\\bar{3}\\mathrm{m}$, assume Wyckoff orbit multiplicities $n_A=8$, $n_B=16$, $n_O=32$ for the $A$, $B$, and $O$ sublattices, respectively.\n- For each sublattice $X\\in\\{A,B,O\\}$, you are given an allowed element substitution set $E_X$ with cardinality $m_X=|E_X|$, meaning there are $m_X$ distinct element types that can occupy sites on sublattice $X$.\n\nAssumptions:\n- Within each Wyckoff orbit, space-group symmetries render positions indistinguishable, so labelings that differ only by permutations within the orbit are considered equivalent.\n- Substitutions across different sublattices are treated independently, and there are no global composition constraints beyond the orbit multiplicities $n_X$.\n\nTask:\n- Starting from the provided fundamental base, construct a principled algorithm that, for a given prototype and $m_A$, $m_B$, $m_O$, estimates the cardinality of the set of distinct decorated structures modulo space-group symmetries by counting inequivalent labelings on each orbit and aggregating over sublattices.\n- Implement this as a complete, runnable program that applies the algorithm to the test suite below and outputs the results in the specified format.\n\nTest suite:\n- Case $1$: prototype $=$ perovskite, $m_A=2$, $m_B=3$, $m_O=2$.\n- Case $2$: prototype $=$ perovskite, $m_A=1$, $m_B=1$, $m_O=1$.\n- Case $3$: prototype $=$ perovskite, $m_A=5$, $m_B=4$, $m_O=3$.\n- Case $4$: prototype $=$ spinel, $m_A=2$, $m_B=2$, $m_O=1$.\n- Case $5$: prototype $=$ spinel, $m_A=2$, $m_B=3$, $m_O=2$.\n\nAnswer specification:\n- For each case, the answer is an integer giving the estimated number of distinct decorated structures modulo space-group symmetries.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, $[\\text{result}_1,\\text{result}_2,\\text{result}_3,\\ldots]$.", "solution": "The problem statement is evaluated as valid. It is scientifically grounded in the principles of computational materials science, specifically relating to crystallographic symmetry and high-throughput screening. It is well-posed, with all necessary parameters and assumptions clearly defined, leading to a unique and meaningful solution. The problem is objective and free of ambiguities.\n\nThe task is to derive and implement an algorithm to count the number of distinct decorated crystal structures, given a prototype structure, its Wyckoff orbit multiplicities, and a set of allowed elemental substitutions for each sublattice. The derivation must be rooted in fundamental principles.\n\nThe total number of distinct decorated structures, $N_{total}$, is the product of the number of possible unique decorations for each independent sublattice. This follows from the assumption that substitutions across different sublattices ($A$, $B$, and $O$) are treated independently.\n$$N_{total} = N_A \\cdot N_B \\cdot N_O$$\nwhere $N_X$ is the number of distinct decorations for sublattice $X$.\n\nWe must now derive a formula for $N_X$. A sublattice $X$ is defined by a single Wyckoff orbit containing $n_X$ sites. We are given a set of $m_X$ distinct element types that can be placed on these sites. The crucial assumption is that \"Within each Wyckoff orbit, space-group symmetries render positions indistinguishable, so labelings that differ only by permutations within the orbit are considered equivalent.\"\n\nThis assumption simplifies the problem significantly. Instead of considering the specific action of the space group on the set of sites, the assumption implies that any permutation of elements among the $n_X$ sites of the orbit results in the same, indistinguishable structure. This reduces the problem from a complex group-theoretical enumeration (like one requiring Burnside's Lemma or the Pólya Enumeration Theorem for the specific space group) to a fundamental combinatorial problem.\n\nWe are essentially trying to determine the number of ways to choose $n_X$ elements from a set of $m_X$ element types, with replacement, where the order of selection is irrelevant. This is a problem of counting multisets of size $n_X$ from a set of $m_X$ items.\n\nLet the $m_X$ element types be $E_1, E_2, \\dots, E_{m_X}$. A specific decoration of the sublattice $X$ is defined by the number of sites occupied by each element type. Let $k_i$ be the number of sites occupied by element type $E_i$. Since there are $n_X$ sites in total, the set of non-negative integers $\\{k_1, k_2, \\dots, k_{m_X}\\}$ must satisfy the condition:\n$$k_1 + k_2 + \\dots + k_{m_X} = n_X, \\quad k_i \\ge 0$$\nThe number of distinct decorations $N_X$ is therefore equal to the number of non-negative integer solutions to this equation.\n\nThis is a classic combinatorial problem that can be solved using the \"stars and bars\" method. Imagine we have $n_X$ indistinguishable items (stars, $*$) that we wish to place into $m_X$ distinguishable bins (the element types). We can represent this arrangement by lining up the $n_X$ stars and separating them with $m_X - 1$ dividers (bars, $|$). For example, if $n_X=5$ and $m_X=4$, the arrangement `**|*||**` corresponds to the solution $k_1=2$, $k_2=1$, $k_3=0$, and $k_4=2$.\n\nThe total number of symbols (stars and bars) is $n_X + (m_X - 1)$. Any sequence of these symbols represents a unique solution. The number of unique sequences is equivalent to choosing the positions for the $n_X$ stars from the total of $n_X + m_X - 1$ available positions. This is given by the binomial coefficient:\n$$N_X = \\binom{n_X + m_X - 1}{n_X}$$\nThis can also be written as choosing the positions for the $m_X - 1$ bars:\n$$N_X = \\binom{n_X + m_X - 1}{m_X - 1}$$\nThese two expressions are equivalent, as $\\binom{n}{k} = \\binom{n}{n-k}$.\n\nCombining this result for each sublattice, the total number of distinct decorated structures is given by the formula:\n$$N_{total} = \\binom{n_A + m_A - 1}{n_A} \\cdot \\binom{n_B + m_B - 1}{n_B} \\cdot \\binom{n_O + m_O - 1}{n_O}$$\n\nThis formula will be implemented to solve for the given test cases.\n\nThe specific parameters for the prototypes are:\n-   Perovskite ($ABO_3$): $n_A=1$, $n_B=1$, $n_O=3$.\n-   Spinel ($AB_2O_4$): $n_A=8$, $n_B=16$, $n_O=32$.\n\nFor any sublattice where the multiplicity $n_X=1$, the formula simplifies to:\n$$N_X = \\binom{1 + m_X - 1}{1} = \\binom{m_X}{1} = m_X$$\nThis is intuitive: there is only one site, so there are $m_X$ choices for what element to place there.\n\nFor any sublattice where the number of allowed element types $m_X=1$, the formula simplifies to:\n$$N_X = \\binom{n_X + 1 - 1}{n_X} = \\binom{n_X}{n_X} = 1$$\nThis is also intuitive: if only one element type is allowed, there is only one way to decorate the sublattice (by filling all sites with that element).", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport math\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Calculates the number of distinct decorated crystal structures based on\n    combinatorial principles for high-throughput materials screening.\n    \"\"\"\n\n    # Define the Wyckoff orbit multiplicities for each prototype structure.\n    # n_X is the number of symmetry-equivalent sites for sublattice X.\n    prototype_params = {\n        'perovskite': {'n_A': 1, 'n_B': 1, 'n_O': 3},\n        'spinel': {'n_A': 8, 'n_B': 16, 'n_O': 32},\n    }\n\n    # Test suite provided in the problem statement.\n    # Each case is a tuple: (prototype_name, m_A, m_B, m_O)\n    # m_X is the number of allowed element types for sublattice X.\n    test_cases = [\n        ('perovskite', 2, 3, 2),\n        ('perovskite', 1, 1, 1),\n        ('perovskite', 5, 4, 3),\n        ('spinel', 2, 2, 1),\n        ('spinel', 2, 3, 2),\n    ]\n\n    results = []\n\n    def count_decorations(n, m):\n        \"\"\"\n        Calculates the number of ways to decorate a sublattice with n sites\n        using m element types.\n\n        This is a multiset combination problem, also known as \"stars and bars\".\n        The formula is C(n + m - 1, n).\n\n        Args:\n            n (int): The number of indistinguishable sites (orbit multiplicity).\n            m (int): The number of distinct element types to choose from.\n\n        Returns:\n            int: The number of distinct decorations for the sublattice.\n        \"\"\"\n        # Ensure n and m are non-negative integers.\n        if n < 0 or m < 0:\n            raise ValueError(\"Number of sites and element types must be non-negative.\")\n        if m == 0 and n > 0:\n            return 0  # Cannot fill sites if no elements are available.\n        if n == 0:\n            return 1 # One way to have an empty decoration.\n        \n        # Using math.comb for robust calculation of binomial coefficients\n        # C(n, k) = n! / (k! * (n-k)!)\n        return math.comb(n + m - 1, n)\n\n    for case in test_cases:\n        prototype_name, m_A, m_B, m_O = case\n        params = prototype_params[prototype_name]\n        n_A, n_B, n_O = params['n_A'], params['n_B'], params['n_O']\n\n        # Calculate the number of distinct decorations for each sublattice.\n        N_A = count_decorations(n_A, m_A)\n        N_B = count_decorations(n_B, m_B)\n        N_O = count_decorations(n_O, m_O)\n\n        # The total number of distinct structures is the product of the\n        # possibilities for each independent sublattice.\n        total_structures = N_A * N_B * N_O\n        results.append(total_structures)\n\n    # Format the final output as a comma-separated list in square brackets.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3456708"}, {"introduction": "A key screening criterion for new materials is thermodynamic stability, which is assessed by calculating a compound's energy above the convex hull, $E_{\\mathrm{hull}}$. Since the underlying DFT formation enthalpies ($H_f$) are themselves uncertain, this practice explores how to propagate this uncertainty to the final stability prediction using a Monte Carlo bootstrap method [@problem_id:3456702]. Mastering this technique allows for a more rigorous, probabilistic assessment of whether a candidate material is likely to be stable, metastable, or unstable, moving beyond simple point estimates.", "problem": "You are designing an automated workflow to propagate uncertainty in Density Functional Theory (DFT) calculated formation enthalpies to phase stability estimates for a binary system, suitable for high-throughput materials screening. For a binary system composed of elements A and B, consider a set of intermediate compounds indexed by $i$ with composition $x_i \\in (0,1)$ representing the atomic fraction of B, and formation enthalpy per atom $H_{f,i}$ in electronvolts per atom. The pure elements at $x=0$ and $x=1$ have $H_f = 0$ by definition. Each intermediate compound has an independently estimated uncertainty represented by a standard deviation $\\sigma_i$, and uncertainties are modeled as independent Gaussian distributions.\n\nYour task is to implement a Monte Carlo bootstrap to propagate the uncertainties in $H_{f,i}$ to the energy above the convex hull $E_{\\mathrm{hull},i}$ for a specified target compound in each test case. The required steps and definitions are as follows:\n\n- Fundamental base: The formation enthalpy per atom of a mixture along a tie-line between two phases at compositions $x_a$ and $x_b$ with formation enthalpies $H_{f,a}$ and $H_{f,b}$ is given by the convex combination $H_{f,\\mathrm{mix}}(x) = \\lambda H_{f,a} + (1-\\lambda) H_{f,b}$ where $\\lambda = \\dfrac{x_b - x}{x_b - x_a}$ for $x \\in [x_a, x_b]$. The lower convex envelope (the convex hull in one dimension) of the set of points $\\{(x,H_f)\\}$ represents the set of energetically stable mixtures under the lever rule. The energy above hull at composition $x_i$ is defined as $E_{\\mathrm{hull},i} = H_{f,i} - H_{\\mathrm{hull}}(x_i)$, where $H_{\\mathrm{hull}}(x)$ is the hull energy evaluated at $x$ by linear interpolation between adjacent hull vertices that bracket $x$.\n\n- Bootstrap procedure to propagate uncertainty:\n  1. For a given case, form the full set of points that includes the pure elements $(0,0)$ and $(1,0)$ and all intermediate compounds $(x_i, H_{f,i})$. The pure elements have zero uncertainty.\n  2. For each bootstrap replicate $b \\in \\{1,\\dots,B\\}$, draw one sample of $H'_{f,i} \\sim \\mathcal{N}(H_{f,i}, \\sigma_i^2)$ independently for each intermediate compound; keep $(0,0)$ and $(1,0)$ unchanged.\n  3. Compute the lower convex hull of the sampled points $\\{(x_i, H'_{f,i})\\}$ plus $(0,0)$ and $(1,0)$ using a correct convexity condition: for three consecutive candidate hull points $(x_1,y_1)$, $(x_2,y_2)$, $(x_3,y_3)$ with $x_1 < x_2 < x_3$, the slopes must satisfy $\\dfrac{y_2 - y_1}{x_2 - x_1} \\le \\dfrac{y_3 - y_2}{x_3 - x_2}$; otherwise, remove the middle point and recheck, until convexity holds.\n  4. Evaluate $H_{\\mathrm{hull}}(x_t)$ at the target composition $x_t$ by linear interpolation between the two adjacent hull vertices bracketing $x_t$.\n  5. Compute $E^{(b)}_{\\mathrm{hull}} = \\max\\{0, H'_{f,t} - H_{\\mathrm{hull}}(x_t)\\}$ for that bootstrap replicate.\n  6. After $B$ replicates, estimate the sample mean and the two-sided $95\\%$ confidence interval as the empirical quantiles at probabilities $0.025$ and $0.975$ of the set $\\{E^{(b)}_{\\mathrm{hull}}\\}_{b=1}^B$.\n\n- Your program must implement the above with deterministic pseudorandomness by using the provided seeds.\n\n- All energies must be expressed in electronvolts per atom, and all outputs must be reported in electronvolts per atom, rounded to $6$ decimal places.\n\n- For numerical stability, treat any negative value of $E_{\\mathrm{hull}}$ arising from floating-point round-off as $0$.\n\nMathematical and algorithmic specifications:\n\n- You must compute the one-dimensional lower convex hull over the set of points $(x,H_f)$ augmented with $(0,0)$ and $(1,0)$. The hull is the unique minimal convex function that lies below or equal to all points. A practical construction in one dimension is to sort points by $x$ and iteratively enforce the convexity inequality on consecutive triples as given above.\n\n- You must produce results for the specified target compound only for each test case. For a target with composition exactly equal to a hull vertex in a bootstrap replicate, $E_{\\mathrm{hull}}$ is $0$ in that replicate.\n\n- Confidence interval definition: If the sorted $E_{\\mathrm{hull}}$ samples are $e_{(1)} \\le \\dots \\le e_{(B)}$, the lower endpoint is the element at position $\\lceil 0.025 \\times B \\rceil$ and the upper endpoint is the element at position $\\lceil 0.975 \\times B \\rceil$, using standard empirical quantiles with linear interpolation permitted; using a numerical library’s quantile at probabilities $0.025$ and $0.975$ is acceptable.\n\nTest suite to cover general, boundary, and edge cases. In every case below, include the pure elements $(0,0)$ and $(1,0)$ with zero uncertainty in the hull construction; only the listed intermediate compounds are uncertain.\n\n- Case $1$ (general stable interior phase):\n  - Intermediate compositions and statistics: $\\{(x_i, H_{f,i}, \\sigma_i)\\} = \\{(0.25, -0.090, 0.020), (0.50, -0.150, 0.015), (0.75, -0.085, 0.020)\\}$.\n  - Target index: $1$ (that is, the compound at $x = 0.50$).\n  - Bootstrap replicates: $B = 400$.\n  - Seed: $7$.\n\n- Case $2$ (metastable boundary-leaning phase with uncertainty that can cross the hull):\n  - Intermediate compositions and statistics: $\\{(x_i, H_{f,i}, \\sigma_i)\\} = \\{(0.20, -0.030, 0.010), (0.33, -0.070, 0.025), (0.60, -0.040, 0.010), (0.80, -0.010, 0.010)\\}$.\n  - Target index: $0$ (that is, the compound at $x = 0.20$).\n  - Bootstrap replicates: $B = 400$.\n  - Seed: $11$.\n\n- Case $3$ (near-degenerate interior phases testing hull flips):\n  - Intermediate compositions and statistics: $\\{(x_i, H_{f,i}, \\sigma_i)\\} = \\{(0.10, -0.005, 0.004), (0.40, -0.085, 0.012), (0.55, -0.083, 0.012), (0.90, -0.004, 0.004)\\}$.\n  - Target index: $2$ (that is, the compound at $x = 0.55$).\n  - Bootstrap replicates: $B = 600$.\n  - Seed: $13$.\n\nFinal output specification:\n\n- For each case, compute three values for the target compound: the bootstrap sample mean of $E_{\\mathrm{hull}}$, the empirical lower $0.025$ quantile, and the empirical upper $0.975$ quantile.\n- Round each of the three values to $6$ decimal places.\n- Your program should produce a single line of output containing the results as a comma-separated list of lists in the order of the cases, each inner list structured as $[ \\text{mean}, \\text{lower}, \\text{upper} ]$. For example, the printed line should look like $[[m_1,\\ell_1,u_1],[m_2,\\ell_2,u_2],[m_3,\\ell_3,u_3]]$, where every entry is a float in electronvolts per atom, rounded to $6$ decimal places.", "solution": "The problem has been validated and is determined to be a well-posed, scientifically grounded problem in computational materials science. It provides a complete and consistent set of definitions, data, and algorithmic requirements to perform uncertainty quantification for phase stability using a Monte Carlo bootstrap method. All specified parameters are physically reasonable and the methodology aligns with standard practices in the field.\n\nThe task is to propagate uncertainties from Density Functional Theory (DFT) calculated formation enthalpies, $H_{f,i}$, to the derived quantity of energy above the convex hull, $E_{\\mathrm{hull},i}$, for a binary material system. The uncertainty in each $H_{f,i}$ is given by a standard deviation $\\sigma_i$ and modeled by an independent Gaussian distribution, $\\mathcal{N}(H_{f,i}, \\sigma_i^2)$. The solution requires implementing the specified bootstrap procedure.\n\nThe core of the methodology is a Monte Carlo simulation with $B$ replicates for each test case. In each replicate, we perform the following steps:\n\n1.  **Stochastic Sampling**: For each intermediate compound with composition $x_i$ and mean formation enthalpy $H_{f,i}$, a new formation enthalpy sample, $H'_{f,i}$, is drawn from its Gaussian probability distribution. The pure elements at $x=0$ and $x=1$ have their formation enthalpies fixed at $H_f=0$ with zero uncertainty. The reproducibility of this stochastic process is ensured by seeding a pseudorandom number generator as specified for each case.\n\n2.  **Convex Hull Construction**: The set of points $\\{(x_i, H'_{f,i})\\}$ for all intermediate compounds, augmented with the fixed points $(0,0)$ and $(1,0)$ for the pure elements, is used to construct the one-dimensional lower convex hull. This is achieved using a variant of Andrew's monotone chain algorithm, which is well-suited for this one-dimensional problem. The algorithm proceeds as follows:\n    a. All points are sorted by their composition (x-coordinate).\n    b. The points are iterated through, and a candidate hull is built. For each new point $p_3$ being considered, it is checked against the last two points, $p_1$ and $p_2$, on the candidate hull. The three points must form a \"right turn\" or be collinear to maintain the convexity of the lower hull. This corresponds to the sequence of slopes of the segments being non-decreasing: $\\text{slope}(p_1, p_2) \\le \\text{slope}(p_2, p_3)$.\n    c. If the points form a \"left turn\" (i.e., $\\text{slope}(p_1, p_2) > \\text{slope}(p_2, p_3)$), the middle point $p_2$ introduces a concavity and is removed from the candidate hull. This check is repeated until the convexity condition is satisfied, after which the new point $p_3$ is added.\n    d. This process results in a list of vertices that define the lower convex hull, sorted by composition. For numerical stability, the slope comparison is implemented using a cross-product, which avoids division. The condition to remove point $p_2 = (x_2, y_2)$ when considering point $p_3 = (x_3, y_3)$, given the prior hull point $p_1 = (x_1, y_1)$, is when $(y_2 - y_1)(x_3 - x_2) - (x_2 - x_1)(y_3 - y_2) > 0$.\n\n3.  **Energy Above Hull Calculation**: For a specified target compound with composition $x_t$, its energy above the hull, $E_{\\mathrm{hull}}$, must be computed.\n    a. First, the energy of the hull at the target composition, $H_{\\mathrm{hull}}(x_t)$, is determined. This involves finding the two adjacent vertices on the computed convex hull, $(x_a, H_a)$ and $(x_b, H_b)$, that bracket the target composition, i.e., $x_a \\le x_t \\le x_b$.\n    b. $H_{\\mathrm{hull}}(x_t)$ is then calculated by linear interpolation between these two vertices: $H_{\\mathrm{hull}}(x_t) = H_a + (H_b - H_a) \\frac{x_t - x_a}{x_b - x_a}$.\n    c. The energy above the hull for the current bootstrap replicate, $E^{(b)}_{\\mathrm{hull}}$, is the difference between the target compound's sampled enthalpy and the hull energy at its composition: $E^{(b)}_{\\mathrm{hull}} = H'_{f,t} - H_{\\mathrm{hull}}(x_t)$. As specified, any resulting negative values, which can occur due to floating-point imprecision when a point is on the hull, are set to $0$.\n\n4.  **Statistical Analysis**: After completing all $B$ bootstrap replicates, a distribution of $\\{E^{(b)}_{\\mathrm{hull}}\\}_{b=1}^B$ values is obtained. From this distribution, the sample mean is calculated, representing the best estimate for the energy above the hull. The uncertainty in this estimate is quantified by a two-sided $95\\%$ confidence interval, which is determined by the empirical quantiles of the sample distribution at probabilities $0.025$ and $0.975$.\n\nThe implementation will be encapsulated within a single Python script. The `numpy` library is utilized for efficient numerical operations, specifically for generating Gaussian random numbers, performing array manipulations, and calculating the final statistical measures (mean and quantiles). The final results for each case, consisting of the mean, lower quantile, and upper quantile, are rounded to $6$ decimal places and formatted into a single output line as required.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the uncertainty propagation problem for DFT formation enthalpies\n    using a Monte Carlo bootstrap method as specified.\n    \"\"\"\n    test_cases = [\n        {\n            \"data\": np.array([\n                [0.25, -0.090, 0.020],\n                [0.50, -0.150, 0.015],\n                [0.75, -0.085, 0.020]\n            ]),\n            \"target_idx\": 1,\n            \"B\": 400,\n            \"seed\": 7\n        },\n        {\n            \"data\": np.array([\n                [0.20, -0.030, 0.010],\n                [0.33, -0.070, 0.025],\n                [0.60, -0.040, 0.010],\n                [0.80, -0.010, 0.010]\n            ]),\n            \"target_idx\": 0,\n            \"B\": 400,\n            \"seed\": 11\n        },\n        {\n            \"data\": np.array([\n                [0.10, -0.005, 0.004],\n                [0.40, -0.085, 0.012],\n                [0.55, -0.083, 0.012],\n                [0.90, -0.004, 0.004]\n            ]),\n            \"target_idx\": 2,\n            \"B\": 600,\n            \"seed\": 13\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        data = case[\"data\"]\n        target_idx = case[\"target_idx\"]\n        B = case[\"B\"]\n        seed = case[\"seed\"]\n\n        x_coords = data[:, 0]\n        h_means = data[:, 1]\n        h_stds = data[:, 2]\n        target_x = x_coords[target_idx]\n\n        rng = np.random.default_rng(seed)\n\n        e_hull_samples = []\n        for _ in range(B):\n            # 1. Sample H_f values for intermediate compounds\n            h_samples = rng.normal(loc=h_means, scale=h_stds)\n            \n            # 2. Form the full set of points including endpoints (0,0) and (1,0)\n            points = [(0.0, 0.0)]\n            for i in range(len(x_coords)):\n                points.append((x_coords[i], h_samples[i]))\n            points.append((1.0, 0.0))\n            \n            # Points are already sorted by x-coordinate based on problem input structure\n            # points.sort(key=lambda p: p[0]) is implicitly handled.\n\n            # 3. Compute the lower convex hull using the monotone chain algorithm\n            lower_hull = []\n            for p3 in points:\n                # p1, p2, p3 must form a \"right turn\" for a lower hull. Pop on \"left turn\".\n                while len(lower_hull) >= 2:\n                    p1 = lower_hull[-2]\n                    p2 = lower_hull[-1]\n                    # Cross product check: (y2-y1)(x3-x2) - (x2-x1)(y3-y2) > 0 -> \"left turn\"\n                    # We pop if the turn is counter-clockwise (left turn), which corresponds\n                    # to a concave corner in the lower hull.\n                    if (p2[1] - p1[1]) * (p3[0] - p2[0]) - (p2[0] - p1[0]) * (p3[1] - p2[1]) > 0:\n                        lower_hull.pop()\n                    else:\n                        break\n                lower_hull.append(p3)\n\n            # 4. Evaluate H_hull(x_t) by interpolation on the computed hull\n            h_hull_at_target = 0.0\n            for i in range(len(lower_hull) - 1):\n                p_a = lower_hull[i]\n                p_b = lower_hull[i+1]\n                if p_a[0] <= target_x <= p_b[0]:\n                    xa, ha = p_a\n                    xb, hb = p_b\n                    # Handle cases where target_x is exactly a hull vertex\n                    if abs(target_x - xa) < 1e-12:\n                        h_hull_at_target = ha\n                        break\n                    if abs(target_x - xb) < 1e-12:\n                        h_hull_at_target = hb\n                        break\n                    # Linear interpolation for x_t between two hull vertices\n                    h_hull_at_target = ha + (hb - ha) * (target_x - xa) / (xb - xa)\n                    break\n            \n            # 5. Compute E_hull for the replicate\n            h_prime_t = h_samples[target_idx]\n            e_hull = h_prime_t - h_hull_at_target\n            e_hull = max(0.0, e_hull)\n            e_hull_samples.append(e_hull)\n\n        # 6. Compute statistics (mean, 95% CI)\n        e_hull_samples = np.array(e_hull_samples)\n        mean_e_hull = np.mean(e_hull_samples)\n        lower_ci = np.quantile(e_hull_samples, 0.025)\n        upper_ci = np.quantile(e_hull_samples, 0.975)\n        \n        all_results.append([\n            round(mean_e_hull, 6),\n            round(lower_ci, 6),\n            round(upper_ci, 6)\n        ])\n\n    result_str = \"[\" + \",\".join([f\"[{m},{l},{u}]\" for m, l, u in all_results]) + \"]\"\n    print(result_str)\n\nsolve()\n```", "id": "3456702"}, {"introduction": "High-throughput workflows must operate under finite computational budgets, necessitating intelligent strategies for resource allocation. This practice demonstrates how to frame the problem of selecting which materials to refine as a classic 0-1 knapsack problem [@problem_id:3456745]. Here, the 'value' of each computational task is rigorously defined as the expected information gain, measured by the reduction in Shannon entropy, providing a principled way to maximize scientific insight per core-hour spent.", "problem": "A high-throughput computational materials screening workflow must allocate finite computational resources to convergence refinement tasks across a set of candidate materials. Each candidate $x_i$ has an uncertain property modeled as a latent mean with a Gaussian prior and is measured by computations that yield conditionally independent observations with known Gaussian noise. Refinement consists of performing $m_i$ additional independent computational samples for candidate $i$, each consuming $u_i$ core-hours, so the incremental cost for candidate $i$ is $c_i = m_i u_i$ core-hours. You must cast the decision of which candidates to refine within a total budget $B$ core-hours as a $0$-$1$ knapsack selection problem, where the \"value\" $v_i$ of selecting candidate $i$ is the expected information gain from the refinement.\n\nUse the following foundational base:\n- Shannon differential entropy (SDE) of a univariate Gaussian $\\mathcal{N}(0,\\sigma^2)$ is $H(\\sigma^2) = \\frac{1}{2}\\ln\\!\\big(2\\pi e\\,\\sigma^2\\big)$ measured in nats.\n- Under the normal-normal conjugate model with known observation noise variance $\\tau_i^2$ and prior variance $\\sigma_{0,i}^2$ for candidate $i$, after $m_i$ independent observations, the posterior precision equals the sum of the prior precision and the data precision: $\\sigma_{\\text{post},i}^{-2} = \\sigma_{0,i}^{-2} + m_i \\tau_i^{-2}$, and thus $\\sigma_{\\text{post},i}^{2} = \\big(\\sigma_{0,i}^{-2} + m_i \\tau_i^{-2}\\big)^{-1}$.\n\nFrom these bases, derive the information gain $v_i$ for candidate $i$ as the reduction in SDE between the prior and the posterior, and compute $v_i$ for each candidate using the provided parameters. Formulate the selection problem as maximizing $\\sum_{i \\in S} v_i$ subject to $\\sum_{i \\in S} c_i \\le B$ over subsets $S \\subset \\{0,1,\\dots,N-1\\}$ where each item is either selected or not; fractional selection is not permitted. Resolve ties deterministically by the following rule: among all subsets achieving the maximal total value, choose the subset with the minimal total cost; if there is still a tie, choose the subset with the minimal sum of indices.\n\nYour program must implement an exact $0$-$1$ knapsack dynamic programming algorithm with the above tie-breaking, compute the selected indices for each test case, and output the results in a single line as a comma-separated list enclosed in square brackets. Each result must be the list of selected indices in increasing order for that test case.\n\nPhysical unit specification: all computational budgets and costs must be treated in core-hours, and all entropies must be considered in nats. The final output consists only of integer indices, which are unitless.\n\nTest suite:\n- Test case $1$ (happy path):\n  - $N = 6$\n  - Prior variances $\\sigma_{0}^2$: $\\big[0.09, 0.16, 0.25, 0.04, 0.12, 0.20\\big]$\n  - Noise variances $\\tau^2$: $\\big[0.03, 0.04, 0.05, 0.02, 0.06, 0.08\\big]$\n  - Planned samples $m$: $\\big[2, 3, 1, 4, 2, 3\\big]$\n  - Unit costs $u$ (core-hours per sample): $\\big[4, 5, 2, 3, 6, 4\\big]$\n  - Budget $B = 25$ core-hours.\n- Test case $2$ (boundary $B=0$):\n  - $N = 3$\n  - Prior variances $\\sigma_{0}^2$: $\\big[0.10, 0.20, 0.30\\big]$\n  - Noise variances $\\tau^2$: $\\big[0.05, 0.10, 0.15\\big]$\n  - Planned samples $m$: $\\big[1, 2, 3\\big]$\n  - Unit costs $u$: $\\big[4, 5, 6\\big]$\n  - Budget $B = 0$ core-hours.\n- Test case $3$ (exact fit, unique optimum):\n  - $N = 5$\n  - Prior variances $\\sigma_{0}^2$: $\\big[0.10, 0.08, 0.18, 0.12, 0.05\\big]$\n  - Noise variances $\\tau^2$: $\\big[0.05, 0.04, 0.09, 0.03, 0.05\\big]$\n  - Planned samples $m$: $\\big[2, 1, 2, 1, 3\\big]$\n  - Unit costs $u$: $\\big[3, 6, 4, 5, 2\\big]$\n  - Budget $B = 19$ core-hours.\n- Test case $4$ (edge case with zero-value item, tie on value resolved by minimal index sum):\n  - $N = 4$\n  - Prior variances $\\sigma_{0}^2$: $\\big[0.15, 0.15, 0.30, 0.10\\big]$\n  - Noise variances $\\tau^2$: $\\big[0.05, 0.05, 0.10, 0.10\\big]$\n  - Planned samples $m$: $\\big[0, 2, 1, 3\\big]$\n  - Unit costs $u$: $\\big[3, 3, 6, 2\\big]$\n  - Budget $B = 12$ core-hours.\n- Test case $5$ (tie on total value, resolve by minimal total cost):\n  - $N = 4$\n  - Prior variances $\\sigma_{0}^2$: $\\big[0.20, 0.10, 0.05, 0.15\\big]$\n  - Noise variances $\\tau^2$: $\\big[0.10, 0.05, 0.05, 0.05\\big]$\n  - Planned samples $m$: $\\big[1, 2, 3, 1\\big]$\n  - Unit costs $u$: $\\big[10, 5, 3, 6\\big]$\n  - Budget $B = 19$ core-hours.\n\nFinal output format requirement: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each test case result being a list of selected indices in increasing order, for example, $\\big[[0,1,2],[],[0,2,3],[1,2],[1,3]\\big]$.", "solution": "The user-provided problem statement is assessed to be **valid**. It is scientifically grounded in computational materials science and information theory, mathematically well-posed as a 0-1 knapsack problem with deterministic tie-breaking rules, and provides a complete and consistent set of givens for all test cases.\n\nThe objective is to select a subset of candidate materials for computational refinement to maximize the total information gain within a fixed computational budget $B$. This can be rigorously formulated as a $0$-$1$ knapsack problem.\n\nFirst, we must derive the \"value\" $v_i$ and \"cost\" (or \"weight\") $c_i$ for including each candidate $i$ in the refinement set $S$.\n\nThe cost $c_i$ for refining candidate $i$ is explicitly given as the product of the number of planned computational samples $m_i$ and the unit cost per sample $u_i$ (in core-hours):\n$$c_i = m_i u_i$$\nSince both $m_i$ and $u_i$ are specified as integers in all test cases, the cost $c_i$ is an integer.\n\nThe value $v_i$ is defined as the expected information gain, quantified by the reduction in Shannon Differential Entropy (SDE) of the property's distribution, from the prior to the posterior. The SDE of a univariate Gaussian distribution with variance $\\sigma^2$ is given as:\n$$H(\\sigma^2) = \\frac{1}{2}\\ln(2\\pi e\\,\\sigma^2)$$\nThe prior entropy for candidate $i$ with prior variance $\\sigma_{0,i}^2$ is:\n$$H_{\\text{prior},i} = H(\\sigma_{0,i}^2) = \\frac{1}{2}\\ln(2\\pi e\\,\\sigma_{0,i}^2)$$\nThe Bayesian update rule for the posterior variance $\\sigma_{\\text{post},i}^2$ after $m_i$ independent observations, each with noise variance $\\tau_i^2$, is provided:\n$$\\sigma_{\\text{post},i}^2 = \\left(\\sigma_{0,i}^{-2} + m_i \\tau_i^{-2}\\right)^{-1}$$\nThe posterior entropy is then:\n$$H_{\\text{post},i} = H(\\sigma_{\\text{post},i}^2) = \\frac{1}{2}\\ln(2\\pi e\\,\\sigma_{\\text{post},i}^2)$$\nThe information gain $v_i$ is the difference:\n$$v_i = H_{\\text{prior},i} - H_{\\text{post},i} = \\frac{1}{2}\\ln(2\\pi e\\,\\sigma_{0,i}^2) - \\frac{1}{2}\\ln(2\\pi e\\,\\sigma_{\\text{post},i}^2)$$\nUsing the property of logarithms, $\\ln a - \\ln b = \\ln(a/b)$, we simplify this to:\n$$v_i = \\frac{1}{2}\\ln\\left(\\frac{2\\pi e\\,\\sigma_{0,i}^2}{2\\pi e\\,\\sigma_{\\text{post},i}^2}\\right) = \\frac{1}{2}\\ln\\left(\\frac{\\sigma_{0,i}^2}{\\sigma_{\\text{post},i}^2}\\right)$$\nWe can express the ratio of variances using the posterior variance formula:\n$$\\frac{1}{\\sigma_{\\text{post},i}^2} = \\frac{1}{\\sigma_{0,i}^2} + \\frac{m_i}{\\tau_i^2}$$\nMultiplying by $\\sigma_{0,i}^2$:\n$$\\frac{\\sigma_{0,i}^2}{\\sigma_{\\text{post},i}^2} = \\sigma_{0,i}^2 \\left(\\frac{1}{\\sigma_{0,i}^2} + \\frac{m_i}{\\tau_i^2}\\right) = 1 + m_i \\frac{\\sigma_{0,i}^2}{\\tau_i^2}$$\nSubstituting this back into the expression for $v_i$ gives the final formula for the value of each item:\n$$v_i = \\frac{1}{2}\\ln\\left(1 + m_i \\frac{\\sigma_{0,i}^2}{\\tau_i^2}\\right)$$\nNote that if $m_i=0$, then $v_i = \\frac{1}{2}\\ln(1) = 0$, which is logically consistent as no new information is gained.\n\nThe selection problem is to find a subset of indices $S \\subseteq \\{0, 1, \\dots, N-1\\}$ that maximizes the total value $\\sum_{i \\in S} v_i$ subject to the budget constraint $\\sum_{i \\in S} c_i \\le B$. This is a canonical $0$-$1$ knapsack problem. The problem specifies an exact solution via dynamic programming, with a three-level tie-breaking rule:\n1.  Maximize total value $\\sum v_i$.\n2.  Among solutions with maximal value, minimize total cost $\\sum c_i$.\n3.  If a tie persists, minimize the sum of the indices of the selected candidates $\\sum i$.\n\nTo implement this, we use a dynamic programming approach. Let $dp[i][w]$ be the optimal solution achievable using a subset of the first $i$ items (indices $0, \\dots, i-1$) with a budget of $w$. To handle the tie-breaking rules, each state $dp[i][w]$ will store a tuple $(V, C, I_{\\text{sum}})$, representing the maximum value, minimum cost for that value, and minimum index-sum for that cost, respectively.\n\nThe DP table `dp` of size $(N+1) \\times (B+1)$ is initialized with $(0, 0, 0)$. The table is filled iteratively. For each item $i$ (from $1$ to $N$) and each budget capacity $w$ (from $1$ to $B$), we decide whether to include item $i-1$ (with properties $v_{i-1}, c_{i-1}, \\text{index}=i-1$).\n\nThe recurrence relation is as follows:\n$dp[i][w]$ is the better of two candidates:\n1.  **Not including item $i-1$**: The solution is the same as for the first $i-1$ items, $dp[i-1][w]$.\n2.  **Including item $i-1$** (if $w \\ge c_{i-1}$): The solution is formed by taking item $i-1$ and adding it to the best solution for the first $i-1$ items with the remaining budget, $dp[i-1][w-c_{i-1}]$. The new tuple is $(dp[i-1][w-c_{i-1}].V + v_{i-1}, dp[i-1][w-c_{i-1}].C + c_{i-1}, dp[i-1][w-c_{i-1}].I_{\\text{sum}} + (i-1))$.\n\nThe \"better\" candidate is chosen according to the specified tie-breaking rules. After filling the entire table, the tuple at $dp[N][B]$ represents the optimal solution's properties.\n\nTo reconstruct the set of selected items, a second table, `path[i][w]`, is used to store the decision made at each step. By tracing these decisions backward from `path[N][B]`, we can identify all items included in the optimal set. The resulting list of indices is then sorted in increasing order as required.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves a series of 0-1 knapsack problems derived from a materials screening\n    workflow, with specific tie-breaking rules.\n    \"\"\"\n    \n    test_cases = [\n        # Test case 1 (happy path)\n        {\n            \"N\": 6,\n            \"sig0_sq\": [0.09, 0.16, 0.25, 0.04, 0.12, 0.20],\n            \"tau_sq\": [0.03, 0.04, 0.05, 0.02, 0.06, 0.08],\n            \"m\": [2, 3, 1, 4, 2, 3],\n            \"u\": [4, 5, 2, 3, 6, 4],\n            \"B\": 25,\n        },\n        # Test case 2 (boundary B=0)\n        {\n            \"N\": 3,\n            \"sig0_sq\": [0.10, 0.20, 0.30],\n            \"tau_sq\": [0.05, 0.10, 0.15],\n            \"m\": [1, 2, 3],\n            \"u\": [4, 5, 6],\n            \"B\": 0,\n        },\n        # Test case 3 (exact fit, unique optimum)\n        {\n            \"N\": 5,\n            \"sig0_sq\": [0.10, 0.08, 0.18, 0.12, 0.05],\n            \"tau_sq\": [0.05, 0.04, 0.09, 0.03, 0.05],\n            \"m\": [2, 1, 2, 1, 3],\n            \"u\": [3, 6, 4, 5, 2],\n            \"B\": 19,\n        },\n        # Test case 4 (edge case with zero-value item, tie on value resolved by minimal index sum)\n        {\n            \"N\": 4,\n            \"sig0_sq\": [0.15, 0.15, 0.30, 0.10],\n            \"tau_sq\": [0.05, 0.05, 0.10, 0.10],\n            \"m\": [0, 2, 1, 3],\n            \"u\": [3, 3, 6, 2],\n            \"B\": 12,\n        },\n        # Test case 5 (tie on total value, resolve by minimal total cost)\n        {\n            \"N\": 4,\n            \"sig0_sq\": [0.20, 0.10, 0.05, 0.15],\n            \"tau_sq\": [0.10, 0.05, 0.05, 0.05],\n            \"m\": [1, 2, 3, 1],\n            \"u\": [10, 5, 3, 6],\n            \"B\": 19,\n        },\n    ]\n\n    all_results = []\n    \n    for case in test_cases:\n        N = case[\"N\"]\n        sig0_sq = case[\"sig0_sq\"]\n        tau_sq = case[\"tau_sq\"]\n        m = case[\"m\"]\n        u = case[\"u\"]\n        B = case[\"B\"]\n\n        items = []\n        for i in range(N):\n            cost = int(m[i] * u[i])\n            if m[i] == 0 or tau_sq[i] == 0:\n                value = 0.0\n            else:\n                value = 0.5 * np.log(1 + m[i] * sig0_sq[i] / tau_sq[i])\n            items.append({'v': value, 'c': cost, 'i': i})\n\n        # dp[i][w] = (max_value, min_cost, min_index_sum)\n        # for items 0..i-1 and budget w\n        dp = [[(0.0, 0, 0) for _ in range(B + 1)] for _ in range(N + 1)]\n        path = [[0 for _ in range(B + 1)] for _ in range(N + 1)]\n\n        for i in range(1, N + 1):\n            item = items[i - 1]\n            v_i, c_i, idx_i = item['v'], item['c'], item['i']\n\n            for w in range(B + 1):\n                # Option 1: Don't include item i-1\n                candidate1 = dp[i - 1][w]\n                dp[i][w] = candidate1\n                path[i][w] = 0\n\n                # Option 2: Include item i-1 (if possible)\n                if w >= c_i:\n                    base_sol_for_2 = dp[i - 1][w - c_i]\n                    candidate2 = (\n                        base_sol_for_2[0] + v_i,\n                        base_sol_for_2[1] + c_i,\n                        base_sol_for_2[2] + idx_i\n                    )\n                    \n                    v1, c1, s1 = candidate1\n                    v2, c2, s2 = candidate2\n\n                    # Compare candidates using the specified tie-breaking rules\n                    # A small tolerance is used for floating-point value comparison\n                    is_candidate2_better = False\n                    if v2 > v1 + 1e-9:\n                        is_candidate2_better = True\n                    elif abs(v2 - v1) < 1e-9:\n                        if c2 < c1:\n                            is_candidate2_better = True\n                        elif c2 == c1:\n                            if s2 < s1:\n                                is_candidate2_better = True\n                    \n                    if is_candidate2_better:\n                        dp[i][w] = candidate2\n                        path[i][w] = 1\n\n        # Backtrack to find the selected items\n        selected_indices = []\n        w = B\n        for i in range(N, 0, -1):\n            if path[i][w] == 1:\n                item_index = items[i - 1]['i']\n                item_cost = items[i - 1]['c']\n                selected_indices.append(item_index)\n                w -= item_cost\n\n        selected_indices.sort()\n        all_results.append(selected_indices)\n\n    # Format the final output string as specified\n    result_str = \",\".join(map(str, all_results))\n    print(f\"[{result_str.replace(' ', '')}]\")\n\nsolve()\n```", "id": "3456745"}]}