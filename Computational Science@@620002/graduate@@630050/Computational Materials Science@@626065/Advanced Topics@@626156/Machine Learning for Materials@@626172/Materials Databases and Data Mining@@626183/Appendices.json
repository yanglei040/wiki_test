{"hands_on_practices": [{"introduction": "Before we can mine structural data, we must have a consistent way to represent crystal lattices, as the same periodic structure can be described by infinitely many unit cells. This exercise provides hands-on practice with Niggli reduction, an algorithm that transforms any lattice basis into a unique, canonical form through a series of defined basis transformations. Mastering this is a critical first step in building any structural materials database, as it involves the fundamental linear algebra of crystallography and algorithmic thinking essential for data standardization [@problem_id:3463942].", "problem": "You are given a set of cases, each specified by a real-space lattice basis (a matrix of three linearly independent lattice vectors in Cartesian coordinates) and a set of atomic positions in fractional coordinates. Your task is to write a complete and runnable program that, starting only from core definitions about Bravais lattices and integer change-of-basis operations, constructs a Niggli-reduced primitive cell and consistently transforms all atomic fractional coordinates into that reduced representation.\n\nFundamental base and definitions to be used:\n- A Bravais lattice in three dimensions is the set of vectors $\\{\\mathbf{r} = \\mathbf{L}\\mathbf{f} \\mid \\mathbf{f} \\in \\mathbb{Z}^3\\}$ generated by a basis matrix $\\mathbf{L} \\in \\mathbb{R}^{3 \\times 3}$ whose columns are the lattice vectors $\\mathbf{a}$, $\\mathbf{b}$, $\\mathbf{c}$. The metric tensor is $\\mathbf{G} = \\mathbf{L}^{\\mathsf{T}}\\mathbf{L}$, with elements $G_{ij} = \\mathbf{v}_i \\cdot \\mathbf{v}_j$, where $\\mathbf{v}_1=\\mathbf{a}$, $\\mathbf{v}_2=\\mathbf{b}$, $\\mathbf{v}_3=\\mathbf{c}$.\n- Two bases $\\mathbf{L}$ and $\\mathbf{L}'$ generate the same lattice if and only if there exists an integer matrix $\\mathbf{S} \\in \\mathrm{GL}(3,\\mathbb{Z})$ (the General Linear group over the integers (GL), with determinant $\\det(\\mathbf{S}) = \\pm 1$) such that $\\mathbf{L}' = \\mathbf{L}\\mathbf{S}$. Under this change of basis, fractional coordinates transform contravariantly as $\\mathbf{f}' = \\mathbf{S}^{-1}\\mathbf{f}$.\n- The six-dimensional metric representation (G6) of a cell is the $6$-tuple $(A,B,C,D,E,F)$ defined by $A = \\mathbf{a}\\cdot\\mathbf{a}$, $B = \\mathbf{b}\\cdot\\mathbf{b}$, $C = \\mathbf{c}\\cdot\\mathbf{c}$, $D = 2\\,\\mathbf{b}\\cdot\\mathbf{c}$, $E = 2\\,\\mathbf{a}\\cdot\\mathbf{c}$, $F = 2\\,\\mathbf{a}\\cdot\\mathbf{b}$.\n- A canonical Niggli-reduced primitive cell may be characterized, for the purposes of this problem, as any basis reachable by an integer unimodular transform $\\mathbf{S}\\in\\mathrm{GL}(3,\\mathbb{Z})$ for which: (i) the edge-length squares are nondecreasing, i.e., $A \\leq B \\leq C$, (ii) the interaxial angles are non-acute so that $D \\leq 0$, $E \\leq 0$, $F \\leq 0$ (angles $\\geq 90$ degrees), and (iii) among all such transforms, the $6$-tuple $(A,B,C,D,E,F)$ is lexicographically minimal. You must accomplish this using only integer unimodular transformations. You may assume that, for the supplied test suite, searching over integer unimodular transformations with entries in $\\{-1,0,1\\}$, along with column permutations and independent sign inversions of the basis vectors, suffices to reach such a canonical representative.\n\nDerivation target:\n- Starting from the above definitions, derive an algorithm that searches a finite subset of $\\mathrm{GL}(3,\\mathbb{Z})$, applies the induced change-of-basis to candidate lattices, and chooses the canonical reduced cell by enforcing the ordering $A \\leq B \\leq C$, enforcing non-acute interaxial angles (equivalently, $D \\leq 0$, $E \\leq 0$, $F \\leq 0$), and selecting the lexicographically minimal $(A,B,C,D,E,F)$ among those satisfying these conditions. Then, transform all atomic fractional coordinates via $\\mathbf{f}'=\\mathbf{S}^{-1}\\mathbf{f}$ and reduce them componentwise into the half-open interval $[0,1)$ by applying the fractional part map modulo $1$.\n\nUnits and numerical conventions:\n- Lattice vectors are given in ångströms. Use ångströms consistently in all internal computations involving lengths. Interaxial angles must be interpreted and, if needed, computed in degrees. Your program’s final reported outputs are booleans and integers only, so no unit annotations are required in the output.\n\nValidation criteria to compute for each case:\n- Sorted-edge check: return a boolean indicating whether the reduced cell satisfies $A \\leq B \\leq C$ within a numerical tolerance.\n- Non-acute-angles check: return a boolean indicating whether all three interaxial angles are at least $90$ degrees (equivalently, $D \\leq 0$, $E \\leq 0$, $F \\leq 0$ within tolerance).\n- Volume preservation check: return a boolean indicating whether $|\\det(\\mathbf{L})|$ equals $|\\det(\\mathbf{L}')|$ within tolerance.\n- Atom count: return the integer number of atomic positions after transformation (it must equal the input count when only unimodular transforms are used).\n- Unit-cube containment check: return a boolean indicating whether all transformed fractional coordinates lie in $[0,1)$ componentwise within tolerance.\n\nProgram requirements:\n- Implement the above algorithm using only definitions and facts stated here.\n- Use a fixed small numerical tolerance (for example, $10^{-8}$) for comparisons.\n- For each case, produce a result list of the form $[\\text{sorted\\_edge},\\ \\text{non\\_acute\\_angles},\\ \\text{volume\\_preserved},\\ \\text{atom\\_count},\\ \\text{coords\\_in\\_unit}]$.\n- The test suite below is to be hard-coded in your program. Your program should produce a single line of output containing the results for all cases as a comma-separated list enclosed in square brackets (e.g., $\\texttt{[[...],[...],...]}$), with each case’s result list in order.\n\nTest suite (each lattice is given by its three column vectors; atomic positions are fractional):\n- Case $1$ (simple cubic, already reduced):\n  - $\\mathbf{L}_1$ has columns $(3,0,0)$, $(0,3,0)$, $(0,0,3)$, all in ångströms.\n  - Fractional positions: $(0,0,0)$ and $(0.5,0.5,0.5)$.\n- Case $2$ (orthorhombic cell, permuted with a sign flip):\n  - Start from $(3,0,0)$, $(0,4,0)$, $(0,0,5)$ and rearrange columns to $(0,4,0)$, $(0,0,-5)$, $(3,0,0)$ to form $\\mathbf{L}_2$ in ångströms.\n  - Fractional positions: $(0.25,0.4,0.6)$ and $(0.9,0.1,0.3)$.\n- Case $3$ (acute $\\gamma$ angle that must be made non-acute):\n  - $\\mathbf{L}_3$ has columns $(3,0,0)$, $(1.5,2.598076211,0)$, $(0,0,4)$ in ångströms.\n  - Fractional positions: $(0,0,0)$ and $(1.2,-0.1,0.5)$.\n- Case $4$ (equal edges $a=b$ with right angles; includes out-of-range fractional components to test wrapping):\n  - $\\mathbf{L}_4$ has columns $(3,0,0)$, $(0,3,0)$, $(0,0,4)$ in ångströms.\n  - Fractional positions: $(-0.25,1.75,0.0)$, $(0.5,0.5,0.5)$, and $(1.000000001, -1\\times 10^{-9}, 0.999999999)$.\n\nFinal output format:\n- Your program should produce exactly one line: a single outer list containing one inner list per case, in the same order as above. Each inner list must be of the form $[\\text{boolean},\\ \\text{boolean},\\ \\text{boolean},\\ \\text{integer},\\ \\text{boolean}]$, and the outer list must be printed as standard Python list syntax with commas separating elements and no extra text.", "solution": "The user-provided problem is assessed as **valid**. It is scientifically grounded in the principles of crystallography and linear algebra, well-posed with clear objectives and constraints, and formulated objectively. The problem requires the implementation of an algorithm to find a Niggli-reduced primitive cell for a given crystal lattice and to transform atomic coordinates into this new basis, adhering to a specific set of simplified reduction rules.\n\nThe algorithmic approach is designed based on the principles of basis transformation in a Bravais lattice and optimization under lexicographical ordering.\n\nA Bravais lattice is defined by a set of points $\\mathbf{r} = n_1\\mathbf{a} + n_2\\mathbf{b} + n_3\\mathbf{c}$, where $n_i \\in \\mathbb{Z}$ and $\\{\\mathbf{a}, \\mathbf{b}, \\mathbf{c}\\}$ are three linearly independent lattice vectors. These vectors form the columns of a basis matrix $\\mathbf{L} = [\\mathbf{a}\\ \\mathbf{b}\\ \\mathbf{c}] \\in \\mathbb{R}^{3 \\times 3}$. Any point in the lattice is given by $\\mathbf{r} = \\mathbf{L}\\mathbf{f}$, where $\\mathbf{f} \\in \\mathbb{Z}^3$. The geometry of the cell is fully described by the metric tensor $\\mathbf{G} = \\mathbf{L}^{\\mathsf{T}}\\mathbf{L}$, whose elements are the dot products of the basis vectors, $G_{ij} = \\mathbf{v}_i \\cdot \\mathbf{v}_j$.\n\nAn infinite number of basis matrices $\\mathbf{L}$ can generate the same lattice. Two bases, $\\mathbf{L}$ and $\\mathbf{L}'$, describe the same lattice if they are related by a unimodular integer transformation matrix $\\mathbf{S} \\in \\mathrm{GL}(3,\\mathbb{Z})$, such that $\\mathbf{L}' = \\mathbf{L}\\mathbf{S}$. The matrix $\\mathbf{S}$ must have integer entries and a determinant of $\\det(\\mathbf{S}) = \\pm 1$. The volume of the unit cell, given by $|\\det(\\mathbf{L})|$, is invariant under such transformations, as $|\\det(\\mathbf{L}')| = |\\det(\\mathbf{L}\\mathbf{S})| = |\\det(\\mathbf{L})||\\det(\\mathbf{S})| = |\\det(\\mathbf{L})|$. When the basis is changed via $\\mathbf{S}$, any fractional coordinate vector $\\mathbf{f}$ must transform contravariantly as $\\mathbf{f}' = \\mathbf{S}^{-1}\\mathbf{f}$ to ensure the real-space position $\\mathbf{r} = \\mathbf{L}\\mathbf{f} = \\mathbf{L}'\\mathbf{f}'$ remains invariant.\n\nThe problem requires finding a \"canonical\" Niggli-reduced cell according to a simplified set of rules. This is achieved by searching for an optimal transformation matrix $\\mathbf{S}$ that maps a given initial basis $\\mathbf{L}$ to a reduced basis $\\mathbf{L}'$ that satisfies specific criteria. The problem provides a crucial simplification: the search for $\\mathbf{S}$ can be restricted to the finite set of $3 \\times 3$ matrices whose entries are in $\\{-1, 0, 1\\}$ and whose determinant is $\\pm 1$.\n\nThe algorithm proceeds as follows:\nFirst, a static set $\\mathcal{S}$ of all candidate transformation matrices is generated. This is done by iterating through all $3^9 = 19,683$ possible $3 \\times 3$ matrices with entries in $\\{-1, 0, 1\\}$ and retaining only those for which the determinant is close to $\\pm 1$ within a small numerical tolerance. This set $\\mathcal{S}$ is pre-computed and used for all test cases.\n\nFor each test case, specified by an initial lattice $\\mathbf{L}_{\\text{initial}}$ and a set of initial fractional coordinates $\\{\\mathbf{f}_{\\text{initial}}\\}$:\n1.  Initialize a variable `best_g6` to hold the lexicographically smallest qualifying G6 vector found so far, along with the corresponding `best_L` and `best_S`.\n2.  Iterate through every transformation matrix $\\mathbf{S} \\in \\mathcal{S}$.\n3.  For each $\\mathbf{S}$, compute the candidate transformed basis $\\mathbf{L}_{\\text{cand}} = \\mathbf{L}_{\\text{initial}}\\mathbf{S}$.\n4.  From $\\mathbf{L}_{\\text{cand}}$, calculate the metric tensor $\\mathbf{G}_{\\text{cand}} = \\mathbf{L}_{\\text{cand}}^{\\mathsf{T}}\\mathbf{L}_{\\text{cand}}$ and the corresponding G6 vector $(A,B,C,D,E,F)$, where $A=\\mathbf{a}'\\cdot\\mathbf{a}'$, $B=\\mathbf{b}'\\cdot\\mathbf{b}'$, $C=\\mathbf{c}'\\cdot\\mathbf{c}'$, $D=2\\,\\mathbf{b}'\\cdot\\mathbf{c}'$, $E=2\\,\\mathbf{a}'\\cdot\\mathbf{c}'$, and $F=2\\,\\mathbf{a}'\\cdot\\mathbf{b}'$.\n5.  Apply the reduction criteria from the problem statement to the candidate G6 vector:\n    a.  Sorted edge-length squares: $A \\le B \\le C$.\n    b.  Non-acute interaxial angles: $D \\le 0, E \\le 0, F \\le 0$.\n    Numerical comparisons are performed using a tolerance $\\epsilon = 10^{-8}$.\n6.  If a candidate satisfies these conditions, its G6 vector is compared lexicographically to `best_g6`. If `best_g6` is not yet set or the candidate G6 is strictly smaller, `best_g6`, `best_L`, and `best_S` are updated with the current candidate's properties.\n7.  After searching through all $\\mathbf{S} \\in \\mathcal{S}$, the final `best_L` and `best_S` represent the Niggli-reduced cell and the transformation that produces it.\n\nOnce the optimal transformation $\\mathbf{S}_{\\text{final}}$ is found, the validation checks are performed:\n1.  **Sorted-edge check**: Verifies if the final cell's G6 parameters satisfy $A \\le B \\le C$ within tolerance.\n2.  **Non-acute-angles check**: Verifies if $D \\le 0, E \\le 0, F \\le 0$ within tolerance.\n3.  **Volume preservation check**: Confirms that $|\\det(\\mathbf{L}_{\\text{initial}})|$ is equal to $|\\det(\\mathbf{L}_{\\text{final}})|$ within tolerance.\n4.  **Atom count**: The number of atoms is simply the count of initial fractional coordinate vectors.\n5.  **Unit-cube containment check**: The initial fractional coordinates are transformed via $\\mathbf{f}_{\\text{final}} = \\mathbf{S}_{\\text{final}}^{-1}\\mathbf{f}_{\\text{initial}}$. Each component of $\\mathbf{f}_{\\text{final}}$ is then mapped to the interval $[0, 1)$ using the modulo operator, $f'' = f' \\pmod{1}$. A boolean check confirms that all components of all resulting coordinate vectors lie within $[0, 1)$ with appropriate numerical tolerance, specifically checking if each wrapped coordinate $c$ satisfies $c \\ge -\\epsilon$ and $c < 1.0$.\n\nThis systematic procedure guarantees finding the canonical cell as defined by the problem and correctly transforming all associated properties for validation.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom itertools import product\n\n# Define a global tolerance for floating point comparisons.\nTOL = 1e-8\n\ndef get_s_matrices():\n    \"\"\"\n    Generates and returns the set of all 3x3 matrices with entries in {-1, 0, 1}\n    and determinant of +/-1. This set is pre-calculated once.\n    \"\"\"\n    s_matrices = []\n    for p in product([-1, 0, 1], repeat=9):\n        s = np.array(p, dtype=np.int32).reshape(3, 3)\n        det_s = np.linalg.det(s)\n        if np.isclose(det_s, 1.0) or np.isclose(det_s, -1.0):\n            s_matrices.append(s)\n    return s_matrices\n\n# Pre-compute the set of valid transformation matrices.\nS_MATRICES = get_s_matrices()\n\ndef get_g6_from_L(L):\n    \"\"\"\n    Calculates the 6-dimensional metric representation (G6) from a basis matrix L.\n    G6 = (a*a, b*b, c*c, 2*b*c, 2*a*c, 2*a*b)\n    \"\"\"\n    G = L.T @ L\n    A = G[0, 0]\n    B = G[1, 1]\n    C = G[2, 2]\n    D = 2 * G[1, 2]\n    E = 2 * G[0, 2]\n    F = 2 * G[0, 1]\n    return (A, B, C, D, E, F)\n\ndef solve_case(L_initial, f_initial):\n    \"\"\"\n    Finds the Niggli-reduced cell for a given lattice and performs validation checks.\n    \"\"\"\n    best_g6 = None\n    best_L = None\n    best_S = None\n\n    for s_matrix in S_MATRICES:\n        L_candidate = L_initial @ s_matrix\n        g6_candidate = get_g6_from_L(L_candidate)\n        A, B, C, D, E, F = g6_candidate\n\n        # Condition (i): Sorted edge-length squares (A <= B <= C)\n        if not (A <= B + TOL and B <= C + TOL):\n            continue\n        \n        # Condition (ii): Non-acute interaxial angles (D, E, F <= 0)\n        if not (D <= TOL and E <= TOL and F <= TOL):\n            continue\n\n        # Condition (iii): Lexicographically minimal G6 vector\n        if best_g6 is None or g6_candidate < best_g6:\n            best_g6 = g6_candidate\n            best_L = L_candidate\n            best_S = s_matrix\n    \n    # After checking all S, we have the best reduced cell\n    L_final = best_L\n    S_final = best_S\n    g6_final = best_g6\n\n    # --- Perform Validation Checks ---\n    \n    # 1. Sorted-edge check\n    A_f, B_f, C_f, _, _, _ = g6_final\n    sorted_edge = (A_f <= B_f + TOL) and (B_f <= C_f + TOL)\n\n    # 2. Non-acute-angles check\n    _, _, _, D_f, E_f, F_f = g6_final\n    non_acute_angles = (D_f <= TOL) and (E_f <= TOL) and (F_f <= TOL)\n\n    # 3. Volume preservation check\n    vol_initial = np.abs(np.linalg.det(L_initial))\n    vol_final = np.abs(np.linalg.det(L_final))\n    volume_preserved = np.isclose(vol_initial, vol_final, atol=TOL, rtol=TOL)\n\n    # 4. Atom count\n    f_initial_arr = np.array(f_initial)\n    if f_initial_arr.ndim == 1:\n        atom_count = 1 if f_initial_arr.size > 0 else 0\n    else:\n        atom_count = f_initial_arr.shape[0]\n\n    # 5. Unit-cube containment check\n    coords_in_unit = True # Default to True for case with no atoms\n    if atom_count > 0:\n        S_inv = np.linalg.inv(S_final)\n        # Transform coords: S_inv is 3x3, f_initial_arr.T is 3xN_atoms\n        f_transformed = (S_inv @ f_initial_arr.T).T\n        # Reduce components to [0, 1) interval\n        f_wrapped = np.mod(f_transformed, 1.0)\n        # Check if all components are in [0, 1) within tolerance\n        # Check >= -TOL for lower bound robustness, and < 1.0 for upper bound\n        coords_in_unit = bool(np.all((f_wrapped >= -TOL) & (f_wrapped < 1.0)))\n\n    return [sorted_edge, non_acute_angles, volume_preserved, atom_count, coords_in_unit]\n\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"L\": np.array([[3, 0, 0], [0, 3, 0], [0, 0, 3]], dtype=float).T,\n            \"f\": [[0, 0, 0], [0.5, 0.5, 0.5]]\n        },\n        {\n            \"L\": np.array([[0, 4, 0], [0, 0, -5], [3, 0, 0]], dtype=float).T,\n            \"f\": [[0.25, 0.4, 0.6], [0.9, 0.1, 0.3]]\n        },\n        {\n            \"L\": np.array([[3, 0, 0], [1.5, 2.598076211, 0], [0, 0, 4]], dtype=float).T,\n            \"f\": [[0, 0, 0], [1.2, -0.1, 0.5]]\n        },\n        {\n            \"L\": np.array([[3, 0, 0], [0, 3, 0], [0, 0, 4]], dtype=float).T,\n            \"f\": [[-0.25, 1.75, 0.0], [0.5, 0.5, 0.5], [1.000000001, -1e-9, 0.999999999]]\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        L_initial = case[\"L\"]\n        f_initial = case[\"f\"]\n        result_list = solve_case(L_initial, f_initial)\n        results.append(result_list)\n\n    # Final print statement in the exact required format.\n    print(results)\n\nsolve()\n```", "id": "3463942"}, {"introduction": "When building machine learning models for materials properties, a correct evaluation is crucial to avoid overly optimistic results, especially since datasets often contain non-independent samples like polymorphs of the same composition. Grouped cross-validation is a strategy that prevents this information leakage by ensuring all data points from one group (e.g., a chemical composition) are in the same training or test fold. This practice demonstrates how to implement a robust validation scheme tailored to materials data, a vital skill for building reliable predictive models [@problem_id:3463892].", "problem": "You are given a synthetic materials dataset designed to mimic a materials database containing multiple polymorphs per composition, where a polymorph is a distinct crystal structure of the same chemical composition. The goal is to implement grouped cross-validation so that all polymorphs of the same composition are kept in the same fold, train a regression model for formation energy, and report the Mean Absolute Error (MAE) with a $95\\%$ confidence interval. The task is framed to prevent information leakage by ensuring that polymorphs sharing composition-level descriptors are not split across training and test folds.\n\nDataset construction and physical realism:\n- There are $12$ distinct compositions (groups), indexed by integers $0,1,\\dots,11$.\n- Each composition has exactly $6$ polymorphs, so there are $72$ samples in total.\n- Each composition $g$ has a composition-level descriptor $\\mathbf{c}_g \\in \\mathbb{R}^3$ which is fixed across its polymorphs and generated from a uniform distribution in $[0,1]^3$ using a fixed random seed for reproducibility.\n- Each polymorph within composition $g$ has a structure-level descriptor $\\mathbf{s}_{g,j} \\in \\mathbb{R}^2$ generated independently from a uniform distribution in $[0,1]^2$ using the same fixed random seed for reproducibility.\n- The ground-truth formation energy per atom is generated by a linear model with an intercept and additive noise:\n$$\nE_f(g,j) = b + \\mathbf{c}_g \\cdot \\mathbf{w}_c + \\mathbf{s}_{g,j} \\cdot \\mathbf{w}_s + \\varepsilon_{g,j},\n$$\nwhere $b=-2.0$ (in $\\mathrm{eV/atom}$), $\\mathbf{w}_c = [0.8,-0.6,0.4]$ (in $\\mathrm{eV}$ per descriptor unit), $\\mathbf{w}_s = [0.5,-0.3]$ (in $\\mathrm{eV}$ per descriptor unit), and $\\varepsilon_{g,j} \\sim \\mathcal{N}(0,\\sigma^2)$ is additive Gaussian noise with standard deviation $\\sigma$ specified per test case. All energies are in electronvolts per atom ($\\mathrm{eV/atom}$).\n\nModeling and evaluation requirements:\n- Implement grouped $K$-fold Cross-Validation (CV), where the groups are the composition indices. All polymorphs belonging to the same composition must be assigned to the same fold.\n- For each fold, train a linear regression model for formation energy that minimizes the sum of squared residuals with an $\\ell_2$ (Euclidean) penalty of strength $\\lambda$ on the weights, but does not penalize the intercept. The learned model must predict formation energy on the held-out fold. Use standard linear algebra to obtain the solution; the intercept must be handled correctly and excluded from regularization.\n- Compute the per-fold Mean Absolute Error (MAE) as the average of $|E_f^{\\mathrm{pred}} - E_f^{\\mathrm{true}}|$ over all samples in the test fold. Aggregate the per-fold MAEs into an overall MAE by averaging across folds.\n- Compute a $95\\%$ confidence interval for the overall MAE by treating the per-fold MAEs as $K$ samples and using the Student’s $t$-distribution with $K-1$ degrees of freedom. Specifically, if $m_k$ are the per-fold MAEs for $k=1,\\dots,K$, then with sample mean $\\bar{m}$ and sample standard deviation $s$, report the interval $\\left[\\bar{m} - t^\\star \\frac{s}{\\sqrt{K}}, \\bar{m} + t^\\star \\frac{s}{\\sqrt{K}}\\right]$ where $t^\\star$ is the $0.975$ quantile of the Student’s $t$ distribution with $K-1$ degrees of freedom.\n- All error metrics must be reported in $\\mathrm{eV/atom}$. No angles are used; if any trigonometric functions appear in your implementation, their angles must be in radians.\n\nImplementation details and reproducibility:\n- Use a fixed seed $0$ for generating all composition and structure descriptors so that the features are identical across test cases.\n- Use distinct noise seeds for the additive Gaussian noise in each test case to guarantee reproducible targets while varying the noise level.\n- Use a fixed seed $99$ for shuffling and partitioning the groups into $K$ folds.\n\nTest suite:\nImplement your program to run the following three test cases, each specified by the tuple $(K,\\lambda,\\sigma,\\text{noise\\_seed})$:\n1. $K=3$, $\\lambda=0.1$, $\\sigma=0.05$, $\\text{noise\\_seed}=11$.\n2. $K=2$, $\\lambda=0.0$, $\\sigma=0.10$, $\\text{noise\\_seed}=22$.\n3. $K=4$, $\\lambda=1.0$, $\\sigma=0.20$, $\\text{noise\\_seed}=33$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results for all test cases in order, formatted as a comma-separated list enclosed in square brackets, where each test case contributes a three-element list $[\\bar{m}, \\mathrm{lower}, \\mathrm{upper}]$ with floats in $\\mathrm{eV/atom}$. For example, the output must look like\n$$\n[[\\bar{m}_1,\\mathrm{lower}_1,\\mathrm{upper}_1],[\\bar{m}_2,\\mathrm{lower}_2,\\mathrm{upper}_2],[\\bar{m}_3,\\mathrm{lower}_3,\\mathrm{upper}_3]]\n$$\nprinted on a single line, with no additional text.", "solution": "The core scientific rationale is to avoid information leakage by ensuring that polymorphs sharing the same composition-level descriptors are not split between training and test sets, because they would give the model indirect access to the held-out composition’s descriptor through other polymorphs. Grouped $K$-fold Cross-Validation (CV) directly enforces this by partitioning data on group labels (here, compositions).\n\nWe begin from empirical risk minimization. Let $\\mathbf{x}_{i} \\in \\mathbb{R}^{d}$ denote the descriptor vector for sample $i$ formed by concatenating the composition descriptor $\\mathbf{c}_g \\in \\mathbb{R}^{3}$ and the structure descriptor $\\mathbf{s}_{g,j} \\in \\mathbb{R}^{2}$, so $d=5$. Let $y_{i} \\in \\mathbb{R}$ be the formation energy in $\\mathrm{eV/atom}$. The ridge regression objective with intercept $b$ and weights $\\mathbf{w} \\in \\mathbb{R}^{d}$ is\n$$\nJ(b,\\mathbf{w}) = \\sum_{i=1}^{n} \\left(y_i - b - \\mathbf{x}_i^\\top \\mathbf{w}\\right)^2 + \\lambda \\lVert \\mathbf{w} \\rVert_2^2,\n$$\nwhere $\\lambda \\ge 0$ is the regularization strength and the intercept $b$ is not penalized. This is a convex quadratic problem whose minimizer satisfies the normal equations. Defining the augmented design matrix\n$$\n\\mathbf{X}_{\\mathrm{aug}} = \\begin{bmatrix}\n\\mathbf{1} & \\mathbf{X}\n\\end{bmatrix},\n$$\nwhere $\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$ stacks $\\mathbf{x}_i^\\top$ and $\\mathbf{1} \\in \\mathbb{R}^{n}$ is a column of ones, and the augmented parameter vector\n$$\n\\boldsymbol{\\beta} = \\begin{bmatrix}\nb \\\\ \\mathbf{w}\n\\end{bmatrix} \\in \\mathbb{R}^{d+1},\n$$\nthe first-order optimality condition yields\n$$\n\\left(\\mathbf{X}_{\\mathrm{aug}}^\\top \\mathbf{X}_{\\mathrm{aug}} + \\mathbf{L}\\right)\\boldsymbol{\\beta} = \\mathbf{X}_{\\mathrm{aug}}^\\top \\mathbf{y},\n$$\nwith $\\mathbf{L} \\in \\mathbb{R}^{(d+1)\\times(d+1)}$ a diagonal matrix that has $0$ in the intercept entry and $\\lambda$ on the remaining $d$ diagonal entries. Solving this linear system gives the unique minimizer when $\\lambda>0$, and the ordinary least squares (OLS) solution when $\\lambda=0$ and $\\mathbf{X}_{\\mathrm{aug}}^\\top \\mathbf{X}_{\\mathrm{aug}}$ is invertible. Predictions on a test set are obtained by $\\hat{\\mathbf{y}} = \\mathbf{X}_{\\mathrm{aug,test}} \\boldsymbol{\\beta}$.\n\nFor grouped $K$-fold CV, let the dataset be indexed by $i=1,\\dots,n$ and let $g(i)$ be the group label (composition index) for sample $i$. We randomly permute the $G$ unique groups using a fixed seed and partition them into $K$ disjoint folds $\\mathcal{G}_1,\\dots,\\mathcal{G}_K$ of nearly equal size. For each fold $k$, the training set is all samples with group labels not in $\\mathcal{G}_k$, and the test set is all samples with group labels in $\\mathcal{G}_k$. This ensures that for any composition $g$, either all its polymorphs are in training or all are in testing for a given fold.\n\nThe Mean Absolute Error (MAE) on fold $k$ is defined as\n$$\nm_k = \\frac{1}{|\\mathcal{I}_k|} \\sum_{i \\in \\mathcal{I}_k} \\left| \\hat{y}_i - y_i \\right|,\n$$\nwhere $\\mathcal{I}_k$ is the index set of samples in the test fold $k$. The overall MAE is the average across folds,\n$$\n\\bar{m} = \\frac{1}{K} \\sum_{k=1}^{K} m_k.\n$$\nTo obtain a $95\\%$ confidence interval for $\\bar{m}$, we treat $\\{m_k\\}_{k=1}^K$ as $K$ samples and rely on the Student’s $t$-distribution. Compute the sample standard deviation\n$$\ns = \\sqrt{\\frac{1}{K-1}\\sum_{k=1}^{K} \\left(m_k - \\bar{m}\\right)^2},\n$$\nand let $t^\\star$ be the $0.975$ quantile of the Student’s $t$ distribution with $K-1$ degrees of freedom. Then the $95\\%$ confidence interval is\n$$\n\\left[\\bar{m} - t^\\star \\frac{s}{\\sqrt{K}}, \\bar{m} + t^\\star \\frac{s}{\\sqrt{K}}\\right].\n$$\nAll error values are in $\\mathrm{eV/atom}$.\n\nAlgorithmic steps consistent with the above principles:\n1. Generate composition descriptors $\\mathbf{c}_g$ for $g=0,\\dots,11$ from a uniform distribution on $[0,1]^3$ with seed $0$. Generate structure descriptors $\\mathbf{s}_{g,j}$ for $j=1,\\dots,6$ from a uniform distribution on $[0,1]^2$ with the same seed so features are fixed. Form $\\mathbf{x}_{g,j} = [\\mathbf{c}_g; \\mathbf{s}_{g,j}] \\in \\mathbb{R}^{5}$.\n2. For each test case, generate targets using $E_f(g,j) = b + \\mathbf{c}_g \\cdot \\mathbf{w}_c + \\mathbf{s}_{g,j} \\cdot \\mathbf{w}_s + \\varepsilon_{g,j}$ with the specified $\\sigma$ and noise seed, yielding $y_{g,j}$ in $\\mathrm{eV/atom}$.\n3. Split groups into $K$ folds using seed $99$. For each fold, fit ridge regression on the training set by solving $\\left(\\mathbf{X}_{\\mathrm{aug}}^\\top \\mathbf{X}_{\\mathrm{aug}} + \\mathbf{L}\\right)\\boldsymbol{\\beta} = \\mathbf{X}_{\\mathrm{aug}}^\\top \\mathbf{y}$, with $\\mathbf{L}=\\mathrm{diag}(0,\\lambda,\\dots,\\lambda)$ to exclude the intercept from regularization.\n4. Compute per-fold MAEs $m_k$, then compute $\\bar{m}$, $s$, and the $95\\%$ confidence interval using the Student’s $t$-distribution with $K-1$ degrees of freedom.\n5. Repeat for all test cases and print the list of triples $[\\bar{m},\\mathrm{lower},\\mathrm{upper}]$ for the three cases on a single line.\n\nThis design is grounded in empirical risk minimization with $\\ell_2$ regularization, proper handling of intercepts, statistical inference via the Student’s $t$ distribution, and group-aware validation to prevent information leakage across polymorphs sharing composition descriptors. The reporting of MAE and its confidence interval in $\\mathrm{eV/atom}$ aligns with physically meaningful units in computational materials science.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import t\n\ndef generate_features(num_groups=12, polymorphs_per_group=6, comp_dim=3, struct_dim=2, seed=0):\n    \"\"\"\n    Generate fixed composition and structure descriptors for reproducibility.\n    Returns:\n        X: (N, d) feature matrix where d = comp_dim + struct_dim\n        groups: (N,) array of group labels (composition indices)\n        comp_desc: (num_groups, comp_dim) array of composition descriptors\n        struct_desc: (N, struct_dim) array of structure descriptors\n    \"\"\"\n    rng = np.random.RandomState(seed)\n    comp_desc = rng.uniform(0.0, 1.0, size=(num_groups, comp_dim))\n    N = num_groups * polymorphs_per_group\n    struct_desc = rng.uniform(0.0, 1.0, size=(N, struct_dim))\n\n    # Build full features by concatenating composition and structure descriptors\n    d = comp_dim + struct_dim\n    X = np.zeros((N, d))\n    groups = np.zeros(N, dtype=int)\n\n    idx = 0\n    for g in range(num_groups):\n        c = comp_desc[g]\n        for j in range(polymorphs_per_group):\n            s = struct_desc[idx]\n            X[idx, :] = np.concatenate([c, s])\n            groups[idx] = g\n            idx += 1\n    return X, groups, comp_desc, struct_desc\n\ndef generate_targets(comp_desc, struct_desc, num_groups=12, polymorphs_per_group=6,\n                     w_c=np.array([0.8, -0.6, 0.4]),\n                     w_s=np.array([0.5, -0.3]),\n                     b=-2.0, sigma=0.05, noise_seed=11):\n    \"\"\"\n    Generate formation energy targets based on ground-truth linear model and Gaussian noise.\n    Returns:\n        y: (N,) target vector in eV/atom\n    \"\"\"\n    N = num_groups * polymorphs_per_group\n    y = np.zeros(N)\n    rng = np.random.RandomState(noise_seed)\n    idx = 0\n    for g in range(num_groups):\n        c = comp_desc[g]\n        for j in range(polymorphs_per_group):\n            s = struct_desc[idx]\n            mean_val = b + np.dot(c, w_c) + np.dot(s, w_s)\n            noise = rng.normal(loc=0.0, scale=sigma)\n            y[idx] = mean_val + noise\n            idx += 1\n    return y\n\ndef grouped_kfold(groups, K, seed=99):\n    \"\"\"\n    Create grouped K-fold splits: returns list of (train_indices, test_indices).\n    Ensures all samples of the same group are in the same fold.\n    \"\"\"\n    unique_groups = np.unique(groups)\n    rng = np.random.RandomState(seed)\n    shuffled = unique_groups.copy()\n    rng.shuffle(shuffled)\n    n_groups = len(shuffled)\n    # Partition groups into K folds as evenly as possible\n    fold_sizes = np.full(K, n_groups // K, dtype=int)\n    fold_sizes[: (n_groups % K)] += 1\n    folds = []\n    start = 0\n    for fs in fold_sizes:\n        folds.append(shuffled[start:start+fs])\n        start += fs\n\n    # Map groups to sample indices\n    group_to_indices = {g: np.where(groups == g)[0] for g in unique_groups}\n    splits = []\n    all_indices = np.arange(len(groups))\n    for test_groups in folds:\n        test_idx_list = []\n        for g in test_groups:\n            test_idx_list.append(group_to_indices[g])\n        test_indices = np.concatenate(test_idx_list)\n        train_mask = np.ones(len(groups), dtype=bool)\n        train_mask[test_indices] = False\n        train_indices = all_indices[train_mask]\n        splits.append((train_indices, test_indices))\n    return splits\n\ndef ridge_fit(X, y, lam):\n    \"\"\"\n    Fit ridge regression with intercept unpenalized.\n    Solve (X_aug^T X_aug + L) beta = X_aug^T y, where L has 0 for intercept and lam for weights.\n    Returns:\n        beta: (d+1,) parameter vector [intercept, weights...]\n    \"\"\"\n    n, d = X.shape\n    X_aug = np.hstack([np.ones((n, 1)), X])\n    # Regularization matrix L: 0 for intercept, lam for other coefficients\n    L = np.diag(np.concatenate(([0.0], np.full(d, lam))))\n    A = X_aug.T @ X_aug + L\n    b = X_aug.T @ y\n    beta = np.linalg.solve(A, b)\n    return beta\n\ndef evaluate_case(K, lam, sigma, noise_seed,\n                  num_groups=12, polymorphs_per_group=6,\n                  comp_dim=3, struct_dim=2):\n    \"\"\"\n    Evaluate grouped K-fold CV for one case.\n    Returns:\n        (mae_mean, ci_lower, ci_upper)\n    \"\"\"\n    # Fixed features across cases\n    X, groups, comp_desc, struct_desc = generate_features(\n        num_groups=num_groups,\n        polymorphs_per_group=polymorphs_per_group,\n        comp_dim=comp_dim,\n        struct_dim=struct_dim,\n        seed=0\n    )\n    # Targets for this case\n    y = generate_targets(\n        comp_desc=comp_desc,\n        struct_desc=struct_desc,\n        num_groups=num_groups,\n        polymorphs_per_group=polymorphs_per_group,\n        b=-2.0,\n        w_c=np.array([0.8, -0.6, 0.4]),\n        w_s=np.array([0.5, -0.3]),\n        sigma=sigma,\n        noise_seed=noise_seed\n    )\n\n    splits = grouped_kfold(groups, K=K, seed=99)\n    fold_maes = []\n    for train_idx, test_idx in splits:\n        X_train, y_train = X[train_idx], y[train_idx]\n        X_test, y_test = X[test_idx], y[test_idx]\n        beta = ridge_fit(X_train, y_train, lam=lam)\n        # Predict\n        X_test_aug = np.hstack([np.ones((X_test.shape[0], 1)), X_test])\n        y_pred = X_test_aug @ beta\n        mae = float(np.mean(np.abs(y_pred - y_test)))\n        fold_maes.append(mae)\n\n    # Aggregate MAE and compute 95% CI via Student's t\n    K_float = float(K)\n    mae_mean = float(np.mean(fold_maes))\n    if K > 1:\n        s = float(np.std(fold_maes, ddof=1))\n        t_star = float(t.ppf(0.975, df=K - 1))\n        margin = t_star * s / np.sqrt(K_float)\n    else:\n        # Degenerate case; no CI with a single fold, margin set to 0\n        margin = 0.0\n\n    ci_lower = mae_mean - margin\n    ci_upper = mae_mean + margin\n    return [mae_mean, ci_lower, ci_upper]\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case: (K, lambda, sigma, noise_seed)\n    test_cases = [\n        (3, 0.1, 0.05, 11),\n        (2, 0.0, 0.10, 22),\n        (4, 1.0, 0.20, 33),\n    ]\n\n    results = []\n    for case in test_cases:\n        K, lam, sigma, noise_seed = case\n        result = evaluate_case(K=K, lam=lam, sigma=sigma, noise_seed=noise_seed)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3463892"}, {"introduction": "The frontier of materials informatics involves creating models that not only learn from data but also respect fundamental physical laws, with Graph Neural Networks (GNNs) offering a powerful framework for this. This exercise guides you through building a GNN from first principles, where the message-passing mechanism is explicitly constrained to enforce charge conservation. By implementing and testing this physical inductive bias, you will gain insight into designing state-of-the-art, physics-informed deep learning models that accelerate materials discovery [@problem_id:3463928].", "problem": "You are tasked with implementing, from first principles, a minimal Graph Neural Network (GNN) for oxidation state prediction on small inorganic compounds represented as graphs. The design must explicitly encode charge conservation by constraining the directed edge message function so that for each source node $j$ the outgoing messages satisfy $\\sum_{i} m_{ji} = 0$. You must implement and compare two models: (i) an unconstrained baseline, and (ii) a constrained model that enforces charge conservation at the message level. The goal is to test the effect of this constraint on oxidation state prediction accuracy.\n\nBegin from the following bases:\n\n- A material structure can be represented as an undirected graph $\\mathcal{G} = (\\mathcal{V}, \\mathcal{E})$ with node set $\\mathcal{V}$ and edge set $\\mathcal{E}$. Each node $i \\in \\mathcal{V}$ carries a feature vector $\\mathbf{x}_i \\in \\mathbb{R}^{d_{\\text{in}}}$ that includes atomic number and Pauling electronegativity.\n- Graph message passing uses directed messages on edges. For each undirected edge $(j,i) \\in \\mathcal{E}$, construct two directed edges $j \\to i$ and $i \\to j$. A scalar message $m_{ji}$ is produced by a learnable function $f_{\\theta}$ applied to the source and target node embeddings.\n- Charge conservation at the message level requires that for each source node $j$, the outgoing messages obey $\\sum_{i \\in \\mathcal{N}(j)} m_{ji} = 0$, where $\\mathcal{N}(j)$ is the set of neighbors of $j$.\n\nYou must derive, implement, and train both models with the following architecture and constraints:\n\n- Node embedding: $\\mathbf{h}_i = W_{\\text{node}} \\mathbf{x}_i$, where $W_{\\text{node}} \\in \\mathbb{R}^{d_h \\times d_{\\text{in}}}$.\n- Edge message pre-activation: $\\mathbf{u}_{ji} = W_{\\phi} [\\mathbf{h}_j ; \\mathbf{h}_i] + \\mathbf{b}_{\\phi}$, where $W_{\\phi} \\in \\mathbb{R}^{d_m \\times 2d_h}$ and $\\mathbf{b}_{\\phi} \\in \\mathbb{R}^{d_m}$, and $[\\cdot ; \\cdot]$ denotes concatenation.\n- Edge hidden: $\\mathbf{z}_{ji} = \\tanh(\\mathbf{u}_{ji})$.\n- Raw scalar message: $r_{ji} = \\mathbf{v}^{\\top} \\mathbf{z}_{ji} + b_m$, with $\\mathbf{v} \\in \\mathbb{R}^{d_m}$ and $b_m \\in \\mathbb{R}$.\n- Constrained message: $m_{ji} = r_{ji} - \\frac{1}{\\deg(j)} \\sum_{k \\in \\mathcal{N}(j)} r_{jk}$, where $\\deg(j)$ is the out-degree of node $j$. This ensures $\\sum_{i} m_{ji} = 0$ for each $j$. The unconstrained baseline uses $m_{ji} = r_{ji}$.\n- Predicted charge accumulation: $q_i = \\sum_{j \\in \\mathcal{N}(i)} m_{ji}$ (incoming messages only).\n- Predicted oxidation state: $y_i = \\alpha q_i$, where $\\alpha \\in \\mathbb{R}$ is a learnable scalar.\n- Training objective on a graph with node labels $o_i$ is Mean Squared Error (MSE): $L = \\frac{1}{|\\mathcal{V}|} \\sum_{i \\in \\mathcal{V}} (y_i - o_i)^2$.\n\nYou must derive the gradients for all parameters by backpropagation using the chain rule. In particular, you must show how the conservation constraint affects the gradient with respect to the raw edge messages $r_{ji}$, and hence the upstream parameters.\n\nImplement the training by batch gradient descent over a small dataset of graphs constructed from the following compounds and oxidation states (assuming typical formal oxidation states):\n\n- Sodium chloride (NaCl): nodes $\\{ \\text{Na}, \\text{Cl} \\}$, oxidation states $[+1,-1]$.\n- Magnesium oxide (MgO): nodes $\\{ \\text{Mg}, \\text{O} \\}$, oxidation states $[+2,-2]$.\n- Aluminum oxide (Al$_2$O$_3$): nodes $\\{ \\text{Al}, \\text{Al}, \\text{O}, \\text{O}, \\text{O} \\}$, oxidation states $[+3,+3,-2,-2,-2]$.\n- Iron(II) oxide (FeO): nodes $\\{ \\text{Fe}, \\text{O} \\}$, oxidation states $[+2,-2]$.\n- Iron(III) oxide (Fe$_2$O$_3$): nodes $\\{ \\text{Fe}, \\text{Fe}, \\text{O}, \\text{O}, \\text{O} \\}$, oxidation states $[+3,+3,-2,-2,-2]$.\n- Sodium oxide (Na$_2$O): nodes $\\{ \\text{Na}, \\text{Na}, \\text{O} \\}$, oxidation states $[+1,+1,-2]$.\n- Aluminum nitride (AlN): nodes $\\{ \\text{Al}, \\text{N} \\}$, oxidation states $[+3,-3]$.\n\nEdges should connect each cation to each anion in the compound (for diatomic compounds, a single undirected edge; for compounds with multiple cations and anions, connect all cation–anion pairs).\n\nUse node features with dimension $d_{\\text{in}} = 2$: atomic number and Pauling electronegativity. Your program must internally supply reasonable values for these features for the elements $\\{ \\text{Na}, \\text{Cl}, \\text{Mg}, \\text{O}, \\text{Al}, \\text{Fe}, \\text{N} \\}$.\n\nTraining and model hyperparameters:\n\n- Hidden dimensions: $d_h = 4$, $d_m = 4$.\n- Learning rate: $\\eta = 0.02$.\n- Training epochs: $T = 200$.\n- Initialization: small random values with a fixed seed.\n\nEvaluation protocol and metric:\n\n- Split the dataset into training and one held-out test graph. Train both the unconstrained and constrained models on the training graphs and evaluate on the test graph.\n- Prediction accuracy is the fraction of nodes on the test graph whose predicted oxidation state, rounded to the nearest integer, equals the target oxidation state. Express accuracy as a decimal fraction.\n\nTest suite:\n\n- Case $1$: Hold out NaCl as the test graph.\n- Case $2$: Hold out Fe$_2$O$_3$ as the test graph.\n- Case $3$: Hold out AlN as the test graph.\n\nYour program should produce a single line of output containing the accuracies for the unconstrained and constrained models for the three cases, in order, as a comma-separated list enclosed in square brackets. For example, your output must be in the format $[a_1,a_2,a_3,a_4,a_5,a_6]$, where $a_{2k-1}$ is the unconstrained accuracy and $a_{2k}$ is the constrained accuracy for case $k$.", "solution": "The problem is validated as scientifically sound, well-posed, and objective. It presents a clearly defined task in computational materials science, requiring the implementation and comparison of two Graph Neural Network (GNN) models for oxidation state prediction. All necessary architectural components, hyperparameters, datasets, and evaluation protocols are specified, forming a self-contained and tractable problem.\n\nThe core of the problem lies in encoding a physical principle—charge conservation—as an inductive bias in the GNN architecture and analyzing its effect on model performance. We will proceed with a full derivation of the model's forward and backward passes, followed by the implementation.\n\n**1. Theoretical Formulation**\n\nWe are tasked with developing a GNN to predict nodal properties (oxidation states) on graphs representing small inorganic compounds. The solution involves defining the forward propagation of information through the network, deriving the backward propagation of gradients for learning, and comparing an unconstrained model with one that explicitly enforces a charge conservation constraint at the message-passing level.\n\n**1.1. Graph Representation and Features**\n\nA compound is represented as a graph $\\mathcal{G} = (\\mathcal{V}, \\mathcal{E})$, where nodes $i \\in \\mathcal{V}$ are atoms and edges $(j,i) \\in \\mathcal{E}$ represent interactions. The model uses directed messages, so each undirected chemical bond (defined as a cation-anion link) corresponds to two directed edges, $j \\to i$ and $i \\to j$.\n\nEach node $i$ is initialized with a feature vector $\\mathbf{x}_i \\in \\mathbb{R}^{d_{\\text{in}}}$ with $d_{\\text{in}} = 2$. The features are atomic number ($Z$) and Pauling electronegativity ($\\chi$). We use the following standard values:\n- Sodium (Na): $\\mathbf{x}_{\\text{Na}} = [11, 0.93]^{\\top}$\n- Chlorine (Cl): $\\mathbf{x}_{\\text{Cl}} = [17, 3.16]^{\\top}$\n- Magnesium (Mg): $\\mathbf{x}_{\\text{Mg}} = [12, 1.31]^{\\top}$\n- Oxygen (O): $\\mathbf{x}_{\\text{O}} = [8, 3.44]^{\\top}$\n- Aluminum (Al): $\\mathbf{x}_{\\text{Al}} = [13, 1.61]^{\\top}$\n- Iron (Fe): $\\mathbf{x}_{\\text{Fe}} = [26, 1.83]^{\\top}$\n- Nitrogen (N): $\\mathbf{x}_{\\text{N}} = [7, 3.04]^{\\top}$\n\n**1.2. Model Architecture: Forward Pass**\n\nThe forward pass computes a predicted oxidation state $y_i$ for each node $i$. The parameters to be learned are the weight matrices $W_{\\text{node}} \\in \\mathbb{R}^{d_h \\times d_{\\text{in}}}$, $W_{\\phi} \\in \\mathbb{R}^{d_m \\times 2d_h}$, the bias vectors $\\mathbf{b}_{\\phi} \\in \\mathbb{R}^{d_m}$, $b_m \\in \\mathbb{R}$, the vector $\\mathbf{v} \\in \\mathbb{R}^{d_m}$, and a scalar $\\alpha \\in \\mathbb{R}$. The hidden dimensions are given as $d_h=4$ and $d_m=4$.\n\nThe computation proceeds as follows:\n\n1.  **Node Embedding**: Each node's raw features $\\mathbf{x}_i$ are projected into a hidden embedding space:\n    $$ \\mathbf{h}_i = W_{\\text{node}} \\mathbf{x}_i $$\n\n2.  **Edge Message Formulation**: For each directed edge $j \\to i$, a multi-dimensional message precursor is formed by concatenating the source and target node embeddings, followed by an affine transformation:\n    $$ \\mathbf{u}_{ji} = W_{\\phi} [\\mathbf{h}_j ; \\mathbf{h}_i] + \\mathbf{b}_{\\phi} $$\n    where $[\\cdot ; \\cdot]$ denotes vector concatenation.\n\n3.  **Non-linear Activation**: The precursor is passed through a hyperbolic tangent activation function to create a hidden edge representation:\n    $$ \\mathbf{z}_{ji} = \\tanh(\\mathbf{u}_{ji}) $$\n\n4.  **Raw Scalar Message**: A raw scalar message $r_{ji}$ is computed via a linear transformation of the hidden edge representation:\n    $$ r_{ji} = \\mathbf{v}^{\\top} \\mathbf{z}_{ji} + b_m $$\n    This value can be interpreted as the potential \"charge\" transferred from node $j$ to node $i$.\n\n5.  **Message Normalization (Model-Dependent)**:\n    - **Unconstrained Model**: The final message $m_{ji}$ is simply the raw message:\n      $$ m_{ji}^{\\text{unconstrained}} = r_{ji} $$\n    - **Constrained Model**: To enforce charge conservation at the source node, the outgoing messages are normalized such that they sum to zero. For a source node $j$ with out-degree $\\deg(j) = |\\mathcal{N}(j)|$, the constrained message is:\n      $$ m_{ji}^{\\text{constrained}} = r_{ji} - \\frac{1}{\\deg(j)} \\sum_{k \\in \\mathcal{N}(j)} r_{jk} $$\n      This constraint ensures that $\\sum_{i \\in \\mathcal{N}(j)} m_{ji}^{\\text{constrained}} = \\sum_{i \\in \\mathcal{N}(j)} r_{ji} - \\sum_{i \\in \\mathcal{N}(j)} \\frac{1}{\\deg(j)} \\sum_{k \\in \\mathcal{N}(j)} r_{jk} = \\sum_{i} r_{ji} - \\deg(j) \\frac{1}{\\deg(j)} \\sum_{k} r_{jk} = 0$. This models the idea that charge is transferred *away* from node $j$, not created or destroyed at $j$.\n\n6.  **Node Update (Charge Accumulation)**: The predicted charge accumulation $q_i$ at node $i$ is the sum of all incoming messages:\n    $$ q_i = \\sum_{j \\in \\mathcal{N}(i)} m_{ji} $$\n    Note the indices: the sum is over source nodes $j$ for incoming edges $j \\to i$.\n\n7.  **Final Prediction**: The predicted oxidation state $y_i$ is a scaled version of the accumulated charge:\n    $$ y_i = \\alpha q_i $$\n\n**1.3. Training Objective and Gradient Descent**\n\nThe model is trained by minimizing the Mean Squared Error (MSE) loss $L$ between the predicted oxidation states $y_i$ and the true oxidation states $o_i$ over all nodes in a graph:\n$$ L = \\frac{1}{|\\mathcal{V}|} \\sum_{i \\in \\mathcal{V}} (y_i - o_i)^2 $$\nWe use full-batch gradient descent, where gradients are accumulated over all graphs in the training set before a parameter update is performed using a learning rate $\\eta = 0.02$. The update rule for any parameter $\\theta$ is $\\theta \\leftarrow \\theta - \\eta \\frac{\\partial L}{\\partial \\theta}$.\n\n**2. Gradient Derivation via Backpropagation**\n\nWe derive the gradients of the loss $L$ with respect to all learnable parameters using the chain rule. Let $N_g = |\\mathcal{V}_g|$ for a given graph $g$. For batch training, the total gradient $\\frac{\\partial L_{\\text{total}}}{\\partial \\theta}$ is the sum of gradients for each graph in the batch, $\\sum_g \\frac{\\partial L_g}{\\partial \\theta}$.\n\n- **Gradient w.r.t. Prediction $y_i$**:\n  $$ \\frac{\\partial L}{\\partial y_i} = \\frac{2}{N_g} (y_i - o_i) $$\n\n- **Gradient w.r.t. $\\alpha$**:\n  $$ \\frac{\\partial L}{\\partial \\alpha} = \\sum_{i \\in \\mathcal{V}} \\frac{\\partial L}{\\partial y_i} \\frac{\\partial y_i}{\\partial \\alpha} = \\sum_{i \\in \\mathcal{V}} \\frac{2}{N_g}(y_i - o_i) q_i $$\n\n- **Gradient w.r.t. Charge Accumulation $q_i$**:\n  $$ \\frac{\\partial L}{\\partial q_i} = \\frac{\\partial L}{\\partial y_i} \\frac{\\partial y_i}{\\partial q_i} = \\frac{2}{N_g}(y_i - o_i) \\alpha $$\n\n- **Gradient w.r.t. Message $m_{ji}$**: The message $m_{ji}$ only affects the charge accumulation at the target node $i$.\n  $$ \\frac{\\partial L}{\\partial m_{ji}} = \\frac{\\partial L}{\\partial q_i} \\frac{\\partial q_i}{\\partial m_{ji}} = \\frac{\\partial L}{\\partial q_i} = \\frac{2\\alpha}{N_g}(y_i - o_i) $$\n\n- **Gradient w.r.t. Raw Message $r_{ab}$**: This is the critical step where the two models diverge. We use the multivariate chain rule: $\\frac{\\partial L}{\\partial r_{ab}} = \\sum_{j,i} \\frac{\\partial L}{\\partial m_{ji}} \\frac{\\partial m_{ji}}{\\partial r_{ab}}$. Let $\\delta_{m_{ji}} = \\frac{\\partial L}{\\partial m_{ji}}$.\n\n  - **Unconstrained Model**: Here, $m_{ji} = r_{ji}$, so $\\frac{\\partial m_{ji}}{\\partial r_{ab}} = \\delta_{ja}\\delta_{ib}$ (Kronecker deltas). The sum collapses to a single term:\n    $$ \\frac{\\partial L}{\\partial r_{ab}}^{\\text{unconstrained}} = \\delta_{m_{ab}} = \\frac{2\\alpha}{N_g}(y_b - o_b) $$\n\n  - **Constrained Model**: Here, $m_{ji} = r_{ji} - \\frac{1}{\\deg(j)} \\sum_{k \\in \\mathcal{N}(j)} r_{jk}$. The derivative $\\frac{\\partial m_{ji}}{\\partial r_{ab}}$ is non-zero if $j=a$.\n    $$ \\frac{\\partial m_{ji}}{\\partial r_{ab}} = \\begin{cases} 1 - 1/\\deg(a) & \\text{if } (j,i) = (a,b) \\\\ -1/\\deg(a) & \\text{if } j=a, i \\ne b \\\\ 0 & \\text{otherwise} \\end{cases} $$\n    Substituting into the chain rule sum:\n    $$ \\frac{\\partial L}{\\partial r_{ab}}^{\\text{constrained}} = \\delta_{m_{ab}}\\left(1 - \\frac{1}{\\deg(a)}\\right) + \\sum_{i \\in \\mathcal{N}(a), i \\ne b} \\delta_{m_{ai}}\\left(-\\frac{1}{\\deg(a)}\\right) $$\n    $$ = \\delta_{m_{ab}} - \\frac{1}{\\deg(a)} \\sum_{i \\in \\mathcal{N}(a)} \\delta_{m_{ai}} $$\n    This result is intuitive: the gradient for a raw message $r_{ab}$ is its direct contribution to the loss (via $m_{ab}$) corrected by subtracting the average gradient of all outgoing messages from node $a$. This correction distributes the error signal, reflecting the conservation constraint during learning.\n\nThe remaining gradients are derived from $\\frac{\\partial L}{\\partial r_{ji}}$, denoted $\\delta_{r_{ji}}$.\n- **Gradients for $b_m$ and $\\mathbf{v}$**:\n  $$ \\frac{\\partial L}{\\partial b_m} = \\sum_{j,i} \\delta_{r_{ji}} \\quad , \\quad \\frac{\\partial L}{\\partial \\mathbf{v}} = \\sum_{j,i} \\delta_{r_{ji}} \\mathbf{z}_{ji} $$\n\n- **Gradient flow through $\\tanh$**: Let $\\delta_{\\mathbf{z}_{ji}} = \\frac{\\partial L}{\\partial \\mathbf{z}_{ji}} = \\delta_{r_{ji}} \\mathbf{v}$. Then the gradient w.r.t the pre-activation $\\mathbf{u}_{ji}$ is:\n  $$ \\delta_{\\mathbf{u}_{ji}} = \\frac{\\partial L}{\\partial \\mathbf{u}_{ji}} = \\delta_{\\mathbf{z}_{ji}} \\odot (1 - \\mathbf{z}_{ji}^2) $$\n  where $\\odot$ is the Hadamard (element-wise) product.\n\n- **Gradients for $W_{\\phi}$ and $\\mathbf{b}_{\\phi}$**:\n  $$ \\frac{\\partial L}{\\partial \\mathbf{b}_{\\phi}} = \\sum_{j,i} \\delta_{\\mathbf{u}_{ji}} \\quad , \\quad \\frac{\\partial L}{\\partial W_{\\phi}} = \\sum_{j,i} \\delta_{\\mathbf{u}_{ji}} [\\mathbf{h}_j ; \\mathbf{h}_i]^{\\top} $$\n\n- **Gradient w.r.t. Node Embedding $\\mathbf{h}_k$**: A node embedding $\\mathbf{h}_k$ contributes to all outgoing messages (as a source, $j=k$) and all incoming messages (as a target, $i=k$). Let $W_\\phi$ be partitioned as $[W_{\\phi, \\text{src}} | W_{\\phi, \\text{tgt}}]$, where $W_{\\phi, \\text{src}}, W_{\\phi, \\text{tgt}} \\in \\mathbb{R}^{d_m \\times d_h}$.\n  $$ \\delta_{\\mathbf{h}_k} = \\frac{\\partial L}{\\partial \\mathbf{h}_k} = \\sum_{i \\in \\mathcal{N}(k)} W_{\\phi, \\text{src}}^{\\top} \\delta_{\\mathbf{u}_{ki}} + \\sum_{j \\in \\mathcal{N}(k)} W_{\\phi, \\text{tgt}}^{\\top} \\delta_{\\mathbf{u}_{jk}} $$\n\n- **Gradient w.r.t. $W_{\\text{node}}$**:\n  $$ \\frac{\\partial L}{\\partial W_{\\text{node}}} = \\sum_{i \\in \\mathcal{V}} \\delta_{\\mathbf{h}_i} \\mathbf{x}_i^{\\top} $$\n\nThis completes the derivation of all necessary gradients for both models. The implementation will follow this mathematical framework, performing full-batch gradient descent over the provided dataset.", "answer": "```python\nimport numpy as np\nfrom collections import defaultdict\n\n# Global constants for configuration and data\nRANDOM_SEED = 42\nELEMENT_FEATURES = {\n    'Na': np.array([11, 0.93]), 'Cl': np.array([17, 3.16]),\n    'Mg': np.array([12, 1.31]), 'O': np.array([8, 3.44]),\n    'Al': np.array([13, 1.61]), 'Fe': np.array([26, 1.83]),\n    'N': np.array([7, 3.04])\n}\nCOMPOUNDS = {\n    'NaCl': {'cations': ['Na'], 'anions': ['Cl'], 'states': [1, -1]},\n    'MgO': {'cations': ['Mg'], 'anions': ['O'], 'states': [2, -2]},\n    'Al2O3': {'cations': ['Al']*2, 'anions': ['O']*3, 'states': [3, 3, -2, -2, -2]},\n    'FeO': {'cations': ['Fe'], 'anions': ['O'], 'states': [2, -2]},\n    'Fe2O3': {'cations': ['Fe']*2, 'anions': ['O']*3, 'states': [3, 3, -2, -2, -2]},\n    'Na2O': {'cations': ['Na']*2, 'anions': ['O']*1, 'states': [1, 1, -2]},\n    'AlN': {'cations': ['Al'], 'anions': ['N'], 'states': [3, -3]}\n}\nHYPERPARAMS = {\n    'd_in': 2, 'd_h': 4, 'd_m': 4, 'lr': 0.02, 'epochs': 200\n}\n\ndef create_graph_data(compounds_dict):\n    \"\"\"Constructs graph representations from the compound dictionary.\"\"\"\n    graph_data = {}\n    for name, data in compounds_dict.items():\n        atom_list = data['cations'] + data['anions']\n        num_atoms = len(atom_list)\n        num_cations = len(data['cations'])\n        \n        node_features = np.array([ELEMENT_FEATURES[atom] for atom in atom_list])\n        targets = np.array(data['states'])\n        \n        adj = []\n        out_degree = np.zeros(num_atoms)\n        outgoing_edges = defaultdict(list)\n        \n        cation_indices = list(range(num_cations))\n        anion_indices = list(range(num_cations, num_atoms))\n\n        for j in cation_indices:\n            for i in anion_indices:\n                adj.extend([(j, i), (i, j)])\n                out_degree[j] += 1\n                out_degree[i] += 1\n                outgoing_edges[j].append((j,i))\n                outgoing_edges[i].append((i,j))\n        \n        graph_data[name] = {\n            'name': name,\n            'node_features': node_features,\n            'targets': targets,\n            'adj': adj,\n            'out_degree': out_degree,\n            'outgoing_edges': outgoing_edges,\n            'num_nodes': num_atoms\n        }\n    return graph_data\n\nclass GNN:\n    \"\"\"A minimal GNN for oxidation state prediction.\"\"\"\n    def __init__(self, d_in, d_h, d_m, constrained, learning_rate, seed):\n        self.d_in, self.d_h, self.d_m = d_in, d_h, d_m\n        self.constrained = constrained\n        self.lr = learning_rate\n        self.rng = np.random.default_rng(seed)\n\n        # Initialize parameters\n        self.W_node = self.rng.standard_normal((d_h, d_in)) * 0.1\n        self.W_phi = self.rng.standard_normal((d_m, 2 * d_h)) * 0.1\n        self.b_phi = np.zeros(d_m)\n        self.v = self.rng.standard_normal(d_m) * 0.1\n        self.b_m = 0.0\n        self.alpha = self.rng.standard_normal(1) * 0.1\n\n    def forward(self, graphs):\n        \"\"\"Performs the forward pass for a batch of graphs.\"\"\"\n        total_loss = 0.0\n        batch_activations = []\n\n        for graph in graphs:\n            x, targets = graph['node_features'], graph['targets']\n            num_nodes = graph['num_nodes']\n            adj = graph['adj']\n\n            # 1. Node embedding\n            h = x @ self.W_node.T\n\n            # 2-4. Edge message computation\n            u, z, r = {}, {}, {}\n            for j, i in adj:\n                h_concat = np.concatenate([h[j], h[i]])\n                u_ji = self.W_phi @ h_concat + self.b_phi\n                z_ji = np.tanh(u_ji)\n                r_ji = self.v @ z_ji + self.b_m\n                u[j,i], z[j,i], r[j,i] = u_ji, z_ji, r_ji\n            \n            # 5. Message normalization\n            m = {}\n            if self.constrained:\n                for j in range(num_nodes):\n                    r_sum = sum(r[edge] for edge in graph['outgoing_edges'][j])\n                    for edge in graph['outgoing_edges'][j]:\n                        m[edge] = r[edge] - r_sum / graph['out_degree'][j]\n            else:\n                m = r\n\n            # 6. Charge accumulation\n            q = np.zeros(num_nodes)\n            for j, i in adj:\n                q[i] += m[j,i]\n            \n            # 7. Final prediction\n            y = self.alpha * q\n            \n            # Loss calculation\n            loss = np.mean((y - targets)**2)\n            total_loss += loss\n            \n            batch_activations.append({'h': h, 'u': u, 'z': z, 'r': r, 'm': m, 'q': q, 'y': y})\n        \n        return total_loss / len(graphs), batch_activations\n\n    def backward(self, graphs, activations):\n        \"\"\"Performs the backward pass and computes gradients.\"\"\"\n        # Initialize aggregate gradients\n        grad_W_node = np.zeros_like(self.W_node)\n        grad_W_phi = np.zeros_like(self.W_phi)\n        grad_b_phi = np.zeros_like(self.b_phi)\n        grad_v = np.zeros_like(self.v)\n        grad_b_m = 0.0\n        grad_alpha = 0.0\n\n        for i, graph in enumerate(graphs):\n            act = activations[i]\n            x, targets, adj, n_nodes = graph['node_features'], graph['targets'], graph['adj'], graph['num_nodes']\n            y, q, m, r, z, h = act['y'], act['q'], act['m'], act['r'], act['z'], act['h']\n\n            # Gradient of Loss w.r.t y\n            dL_dy = 2.0 / n_nodes * (y - targets)\n            \n            # Gradients for alpha and q\n            grad_alpha += np.sum(dL_dy * q)\n            dL_dq = dL_dy * self.alpha\n\n            # Gradient w.r.t messages m\n            dL_dm = {}\n            for j, i_node in adj:\n                dL_dm[j, i_node] = dL_dq[i_node]\n            \n            # Gradient w.r.t raw messages r\n            dL_dr = {}\n            if self.constrained:\n                for j_node in range(n_nodes):\n                    avg_dL_dm_sum = sum(dL_dm[edge] for edge in graph['outgoing_edges'][j_node]) / graph['out_degree'][j_node]\n                    for edge in graph['outgoing_edges'][j_node]:\n                        dL_dr[edge] = dL_dm[edge] - avg_dL_dm_sum\n            else:\n                dL_dr = dL_dm\n\n            # Backprop through r = v.z + b_m\n            dL_dh = np.zeros_like(h)\n            for j, i_node in adj:\n                delta_r_ji = dL_dr[j, i_node]\n                grad_v += delta_r_ji * z[j,i_node]\n                grad_b_m += delta_r_ji\n\n                # Backprop through z = tanh(u)\n                dL_dz_ji = delta_r_ji * self.v\n                dL_du_ji = dL_dz_ji * (1 - z[j,i_node]**2)\n                \n                # Gradients for W_phi, b_phi\n                grad_b_phi += dL_du_ji\n                h_concat = np.concatenate([h[j], h[i_node]])\n                grad_W_phi += np.outer(dL_du_ji, h_concat)\n\n                # Backprop to h\n                W_phi_src = self.W_phi[:, :self.d_h]\n                W_phi_tgt = self.W_phi[:, self.d_h:]\n                dL_dh[j] += W_phi_src.T @ dL_du_ji\n                dL_dh[i_node] += W_phi_tgt.T @ dL_du_ji\n            \n            # Backprop to W_node\n            grad_W_node += dL_dh.T @ x\n\n        num_graphs = len(graphs)\n        return {\n            'W_node': grad_W_node / num_graphs, 'W_phi': grad_W_phi / num_graphs,\n            'b_phi': grad_b_phi / num_graphs, 'v': grad_v / num_graphs,\n            'b_m': grad_b_m / num_graphs, 'alpha': grad_alpha / num_graphs\n        }\n\n    def update(self, grads):\n        \"\"\"Updates model parameters using gradient descent.\"\"\"\n        self.W_node -= self.lr * grads['W_node']\n        self.W_phi -= self.lr * grads['W_phi']\n        self.b_phi -= self.lr * grads['b_phi']\n        self.v -= self.lr * grads['v']\n        self.b_m -= self.lr * grads['b_m']\n        self.alpha -= self.lr * grads['alpha']\n\ndef run_experiment(train_graphs, test_graph, constrained, hyperparams, seed):\n    \"\"\"Trains and evaluates a GNN model.\"\"\"\n    model = GNN(\n        d_in=hyperparams['d_in'], d_h=hyperparams['d_h'], d_m=hyperparams['d_m'],\n        constrained=constrained, learning_rate=hyperparams['lr'], seed=seed\n    )\n    \n    # Training Loop\n    for _ in range(hyperparams['epochs']):\n        loss, activations = model.forward(train_graphs)\n        grads = model.backward(train_graphs, activations)\n        model.update(grads)\n    \n    # Evaluation\n    _, test_activations = model.forward([test_graph])\n    predictions = test_activations[0]['y']\n    targets = test_graph['targets']\n    \n    rounded_preds = np.round(predictions)\n    accuracy = np.mean(rounded_preds == targets)\n    \n    return accuracy\n\ndef solve():\n    \"\"\"Main function to run the experiments and print results.\"\"\"\n    all_graph_data = create_graph_data(COMPOUNDS)\n    test_cases = ['NaCl', 'Fe2O3', 'AlN']\n    results = []\n\n    for test_compound_name in test_cases:\n        train_graphs = [g for name, g in all_graph_data.items() if name != test_compound_name]\n        test_graph = all_graph_data[test_compound_name]\n        \n        # Run unconstrained model\n        acc_unconstrained = run_experiment(train_graphs, test_graph, constrained=False, hyperparams=HYPERPARAMS, seed=RANDOM_SEED)\n        results.append(acc_unconstrained)\n        \n        # Run constrained model\n        acc_constrained = run_experiment(train_graphs, test_graph, constrained=True, hyperparams=HYPERPARAMS, seed=RANDOM_SEED)\n        results.append(acc_constrained)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{acc:.2f}' for acc in results)}]\")\n\nsolve()\n```", "id": "3463928"}]}