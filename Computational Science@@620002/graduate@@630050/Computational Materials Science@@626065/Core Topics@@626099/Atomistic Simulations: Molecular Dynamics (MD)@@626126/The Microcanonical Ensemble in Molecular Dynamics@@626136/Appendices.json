{"hands_on_practices": [{"introduction": "To begin our practical exploration of the microcanonical ensemble, we must first build a bridge between abstract statistical theory and the concrete data produced by a molecular dynamics simulation. This exercise focuses on the most fundamental thermodynamic property: temperature. You will verify the consistency between two different definitions of temperature—the statistical temperature derived from the phase space volume and entropy, and the kinetic temperature derived from the time-averaged kinetic energy of the particles [@problem_id:3494648]. By working through this problem for a simple, analytically solvable system of harmonic oscillators, you will gain a foundational understanding of how MD simulations embody the principles of statistical mechanics.", "problem": "Consider a system of $N$ independent one-dimensional classical harmonic oscillators evolving under deterministic molecular dynamics in the microcanonical ensemble. The Hamiltonian is $H(\\mathbf{x},\\mathbf{p}) = \\sum_{i=1}^{N} \\left( \\frac{p_i^2}{2 m} + \\frac{1}{2} m \\omega^2 x_i^2 \\right)$, where $m$ is the mass (in kilograms), $\\omega$ is the angular frequency (in radians per second), $x_i$ are positions (in meters), and $p_i$ are momenta (in kilogram meters per second). The microcanonical entropy is defined as $S(E) = k_{\\mathrm{B}} \\ln \\Omega(E)$, with Boltzmann constant $k_{\\mathrm{B}}$ (in joules per kelvin), and $\\Omega(E)$ is the phase space volume of microstates with energy less than or equal to $E$. The density of states is $g(E) = \\partial \\Omega(E)/\\partial E$. The microcanonical inverse temperature is defined by $1/T(E) = \\partial S(E) / \\partial E$.\n\nStarting from the definitions above and fundamental principles, derive a procedure to reconstruct the density of states $g(E)$ by sampling energy shells in phase space for this Hamiltonian. From the reconstructed $g(E)$, compute $S(E)$ and then evaluate $1/T(E) = \\partial S(E)/\\partial E$. Independently, perform microcanonical molecular dynamics (constant-energy dynamics governed by Newton’s laws and integrated using the velocity Verlet algorithm) at the same total energy $E$ and estimate the kinetic temperature from the time series of the kinetic energy $K(t)$ via the classical definition of kinetic temperature. Compare the two inverse temperatures, $1/T(E)$ obtained from the entropy derivative and $1/T_{\\mathrm{kin}}$ obtained from $K(t)$.\n\nYour program must implement the following:\n\n- A velocity Verlet integrator for $N$ independent harmonic oscillators with the specified $m$ and $\\omega$, using a time step $\\Delta t$ (in seconds) and a given number of steps. Initialize positions and velocities randomly, then rescale them to match an exact target total energy $E$ (in joules).\n- A reconstruction of $g(E)$ for this Hamiltonian by energy shell sampling in phase space, starting from first principles. Use this reconstruction to compute $S(E)$ and then $1/T(E) = \\partial S/\\partial E$.\n- A computation of the kinetic temperature $T_{\\mathrm{kin}}$ from the time-average of $K(t)$ as obtained from the molecular dynamics trajectory. Use the relation appropriate for classical kinetic energy with $N$ quadratic velocity degrees of freedom.\n\nFor each test case, report the relative difference between the two inverse temperatures as a unitless decimal defined by\n$$\n\\delta = \\frac{\\left| \\frac{1}{T(E)} - \\frac{1}{T_{\\mathrm{kin}}} \\right|}{\\frac{1}{T_{\\mathrm{kin}}}}.\n$$\n\nUse International System of Units (SI) throughout: mass in kilograms, angular frequency in radians per second, time in seconds, energy in joules, temperature in kelvin. Angles are not required beyond the angular frequency specification. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets.\n\nTest suite (each case is a tuple $(N, m, \\omega, E, \\Delta t, \\text{steps})$):\n\n1. $(4,\\;6.6335209\\times 10^{-26}\\,\\mathrm{kg},\\;1.0\\times 10^{13}\\,\\mathrm{rad/s},\\;4 k_{\\mathrm{B}} \\cdot 300\\,\\mathrm{K},\\;1.0\\times 10^{-16}\\,\\mathrm{s},\\;80000)$\n2. $(4,\\;6.6335209\\times 10^{-26}\\,\\mathrm{kg},\\;1.0\\times 10^{13}\\,\\mathrm{rad/s},\\;4 k_{\\mathrm{B}} \\cdot 1\\,\\mathrm{K},\\;1.0\\times 10^{-16}\\,\\mathrm{s},\\;80000)$\n3. $(1,\\;6.6335209\\times 10^{-26}\\,\\mathrm{kg},\\;1.0\\times 10^{13}\\,\\mathrm{rad/s},\\;1 k_{\\mathrm{B}} \\cdot 270\\,\\mathrm{K},\\;1.0\\times 10^{-16}\\,\\mathrm{s},\\;120000)$\n4. $(8,\\;6.6335209\\times 10^{-26}\\,\\mathrm{kg},\\;1.0\\times 10^{13}\\,\\mathrm{rad/s},\\;8 k_{\\mathrm{B}} \\cdot 1200\\,\\mathrm{K},\\;1.0\\times 10^{-16}\\,\\mathrm{s},\\;60000)$\n\nYour program should produce a single line of output containing the four $\\delta$ values as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3,result4]\").", "solution": "The problem requires a comparison between two definitions of temperature for a system of $N$ independent one-dimensional classical harmonic oscillators in the microcanonical ensemble. The first, $T(E)$, is derived from the statistical mechanical definition of entropy. The second, $T_{\\mathrm{kin}}$, is derived from the time-averaged kinetic energy obtained via a molecular dynamics simulation. This solution presents the theoretical derivations for both, outlines the numerical procedure, and establishes their theoretical equivalence, which the accompanying program will verify numerically.\n\n### 1. Statistical Temperature from the Density of States\n\nThe first step is to derive the statistical temperature, $T(E)$, from the microcanonical entropy $S(E) = k_{\\mathrm{B}} \\ln \\Omega(E)$, where $\\Omega(E)$ is the phase space volume for states with energy less than or equal to $E$. The inverse temperature is then given by $1/T(E) = \\partial S(E)/\\partial E$.\n\nThe Hamiltonian for the system is:\n$$\nH(\\mathbf{x},\\mathbf{p}) = \\sum_{i=1}^{N} \\left( \\frac{p_i^2}{2 m} + \\frac{1}{2} m \\omega^2 x_i^2 \\right)\n$$\nThe phase space volume $\\Omega(E)$ is the integral over all positions $x_i$ and momenta $p_i$ such that $H(\\mathbf{x},\\mathbf{p}) \\leq E$:\n$$\n\\Omega(E) = \\int_{H(\\mathbf{x},\\mathbf{p}) \\leq E} \\prod_{i=1}^{N} dx_i dp_i\n$$\nTo evaluate this integral, we perform a change of variables to transform the inequality into the equation of a hypersphere. Let us define new coordinates $q_j$:\n$$\nq_{2i-1} = \\sqrt{\\frac{m \\omega^2}{2}} x_i \\quad \\text{and} \\quad q_{2i} = \\frac{1}{\\sqrt{2m}} p_i\n$$\nIn these new coordinates, the Hamiltonian becomes:\n$$\nH = \\sum_{i=1}^{N} (q_{2i-1}^2 + q_{2i}^2) = \\sum_{j=1}^{2N} q_j^2\n$$\nThe condition $H \\leq E$ is now $\\sum_{j=1}^{2N} q_j^2 \\leq E$, which describes the interior of a $2N$-dimensional hypersphere of radius $R = \\sqrt{E}$.\n\nThe differential volume element transforms as:\n$$\ndx_i = \\sqrt{\\frac{2}{m \\omega^2}} dq_{2i-1} \\quad \\text{and} \\quad dp_i = \\sqrt{2m} dq_{2i}\n$$\nThe Jacobian for each oscillator is $J_i = \\sqrt{\\frac{2}{m \\omega^2}} \\sqrt{2m} = \\frac{2}{\\omega}$. For the entire $N$-oscillator system, the phase space volume element is:\n$$\n\\prod_{i=1}^{N} dx_i dp_i = \\left(\\frac{2}{\\omega}\\right)^N \\prod_{j=1}^{2N} dq_j\n$$\nThe phase space volume integral becomes:\n$$\n\\Omega(E) = \\left(\\frac{2}{\\omega}\\right)^N \\int_{\\sum_{j=1}^{2N} q_j^2 \\leq E} \\prod_{j=1}^{2N} dq_j\n$$\nThe integral is the volume of a $d$-dimensional hypersphere of radius $R$, given by the formula $V_d(R) = \\frac{\\pi^{d/2}}{\\Gamma(d/2 + 1)} R^d$. With $d=2N$ and $R=\\sqrt{E}$, the volume is:\n$$\nV_{2N}(\\sqrt{E}) = \\frac{\\pi^N}{\\Gamma(N+1)} (\\sqrt{E})^{2N} = \\frac{(\\pi E)^N}{N!}\n$$\nSubstituting this back, we obtain the phase space volume:\n$$\n\\Omega(E) = \\left(\\frac{2}{\\omega}\\right)^N \\frac{(\\pi E)^N}{N!} = \\frac{1}{N!} \\left(\\frac{2\\pi E}{\\omega}\\right)^N\n$$\nThe problem asks for the density of states $g(E)$, which is the derivative of $\\Omega(E)$:\n$$\ng(E) = \\frac{\\partial \\Omega(E)}{\\partial E} = \\frac{1}{N!} \\left(\\frac{2\\pi}{\\omega}\\right)^N \\frac{\\partial (E^N)}{\\partial E} = \\frac{N}{N!} \\left(\\frac{2\\pi}{\\omega}\\right)^N E^{N-1} = \\frac{1}{(N-1)!} \\left(\\frac{2\\pi}{\\omega}\\right)^N E^{N-1}\n$$\nThis analytical derivation constitutes the required \"reconstruction\" of $g(E)$ from first principles for this specific Hamiltonian. The entropy is then:\n$$\nS(E) = k_{\\mathrm{B}} \\ln \\Omega(E) = k_{\\mathrm{B}} \\ln \\left[ \\frac{1}{N!} \\left(\\frac{2\\pi}{\\omega}\\right)^N E^N \\right] = k_{\\mathrm{B}} \\left( \\ln\\left[\\frac{1}{N!} \\left(\\frac{2\\pi}{\\omega}\\right)^N\\right] + N \\ln E \\right)\n$$\nThe microcanonical inverse temperature $1/T(E)$ is the derivative of entropy with respect to energy:\n$$\n\\frac{1}{T(E)} = \\frac{\\partial S(E)}{\\partial E} = k_{\\mathrm{B}} \\frac{\\partial}{\\partial E} (N \\ln E) = \\frac{N k_{\\mathrm{B}}}{E}\n$$\nThis provides the first, purely theoretical, value for the inverse temperature.\n\n### 2. Kinetic Temperature from Molecular Dynamics\n\nThe second approach is to compute the kinetic temperature $T_{\\mathrm{kin}}$ from a molecular dynamics (MD) simulation. The classical definition of kinetic temperature arises from the equipartition theorem, which states that each quadratic degree of freedom in the Hamiltonian contributes an average energy of $\\frac{1}{2} k_{\\mathrm{B}} T$ to the system.\n\nThe total kinetic energy of the system is $K = \\sum_{i=1}^{N} \\frac{p_i^2}{2m}$. This is a sum of $N$ quadratic terms (one for each momentum $p_i$). According to the equipartition theorem, the time-averaged total kinetic energy $\\langle K \\rangle$ is related to the kinetic temperature $T_{\\mathrm{kin}}$ by:\n$$\n\\langle K \\rangle = N \\cdot \\frac{1}{2} k_{\\mathrm{B}} T_{\\mathrm{kin}}\n$$\nFrom this, we can express the inverse kinetic temperature as:\n$$\n\\frac{1}{T_{\\mathrm{kin}}} = \\frac{N k_{\\mathrm{B}}}{2 \\langle K \\rangle}\n$$\nThe value of $\\langle K \\rangle$ is obtained by performing a constant-energy MD simulation and averaging the instantaneous kinetic energy over a long trajectory.\n\nThe simulation proceeds as follows:\n1.  **Initialization**: The $N$ positions $x_i$ and $N$ velocities $v_i$ are initialized with random values drawn from a standard normal distribution. The initial momenta are $p_i = m v_i$. The total energy $E_{\\mathrm{init}}$ is calculated from these initial conditions. The positions and momenta are then rescaled by a factor $\\lambda = \\sqrt{E / E_{\\mathrm{init}}}$, where $E$ is the target total energy. This ensures the simulation starts with the precise energy required for the microcanonical ensemble.\n2.  **Integration**: The trajectory is propagated using the velocity Verlet algorithm, a time-reversible and symplectic integrator that provides excellent energy conservation for long simulations. For an oscillator $i$, the force is $F_i = -m \\omega^2 x_i$, so the acceleration is $a_i = -\\omega^2 x_i$. The update steps for positions $x_i$ and velocities $v_i$ over a time step $\\Delta t$ are:\n    $$\n    v_i(t + \\Delta t/2) = v_i(t) + \\frac{1}{2} a_i(t) \\Delta t\n    $$\n    $$\n    x_i(t + \\Delta t) = x_i(t) + v_i(t + \\Delta t/2) \\Delta t\n    $$\n    $$\n    a_i(t + \\Delta t) = -\\omega^2 x_i(t + \\Delta t)\n    $$\n    $$\n    v_i(t + \\Delta t) = v_i(t + \\Delta t/2) + \\frac{1}{2} a_i(t + \\Delta t) \\Delta t\n    $$\n3.  **Averaging**: During the simulation, the instantaneous kinetic energy $K(t) = \\sum_{i=1}^N \\frac{(m v_i(t))^2}{2m}$ is calculated at each step and accumulated. The time average $\\langle K \\rangle$ is computed at the end of the simulation.\n\n### 3. Comparison and Theoretical Equivalence\n\nFor a system of classical harmonic oscillators, the Virial theorem states that the time average of the total kinetic energy is equal to the time average of the total potential energy, $\\langle K \\rangle = \\langle V \\rangle$. Since the total energy $E = K + V$ is conserved in the microcanonical ensemble, we have $E = \\langle K + V \\rangle = \\langle K \\rangle + \\langle V \\rangle$. This implies that $\\langle K \\rangle = E/2$.\n\nSubstituting this expected result into the expression for the kinetic temperature gives:\n$$\n\\frac{1}{T_{\\mathrm{kin}}} = \\frac{N k_{\\mathrm{B}}}{2 \\langle K \\rangle} = \\frac{N k_{\\mathrm{B}}}{2 (E/2)} = \\frac{N k_{\\mathrm{B}}}{E}\n$$\nThis is identical to the expression for $1/T(E)$ derived from statistical mechanics. Thus, the two definitions of temperature are theoretically equivalent for this system. The purpose of the calculation is to numerically verify this equivalence. The relative difference $\\delta$ will quantify any deviation arising from the finite time step and finite duration of the numerical simulation. A small value of $\\delta$ confirms the consistency of the statistical and kinetic pictures of temperature as well as the accuracy of the MD simulation.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.constants import k as k_B\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    # Test suite from the problem statement.\n    # Each case is a tuple (N, m, omega, E_factor, T_factor, dt, steps)\n    # where E = N * k_B * T_factor for cases 1,3,4 and 4*k_B*T_factor for case 2\n    test_cases_raw = [\n        (4, 6.6335209e-26, 1.0e13, 4, 300, 1.0e-16, 80000),\n        (4, 6.6335209e-26, 1.0e13, 4, 1, 1.0e-16, 80000),\n        (1, 6.6335209e-26, 1.0e13, 1, 270, 1.0e-16, 120000),\n        (8, 6.6335209e-26, 1.0e13, 8, 1200, 1.0e-16, 60000),\n    ]\n\n    test_cases = []\n    for i, case in enumerate(test_cases_raw):\n        N, m, omega, E_factor, T_factor, dt, steps = case\n        # Energy E is defined as N * k_B * T for all cases in the problem logic\n        # For N=4, E=4*kB*T. For N=1, E=1*kB*T. For N=8, E=8*kB*T.\n        E = E_factor * k_B * T_factor\n        test_cases.append((N, m, omega, E, dt, steps))\n\n    results = []\n    for case in test_cases:\n        delta = compute_relative_difference(case)\n        results.append(delta)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef compute_relative_difference(case_params):\n    \"\"\"\n    Computes the relative difference delta for a single test case.\n    \"\"\"\n    N, m, omega, E_target, dt, n_steps = case_params\n\n    # ======== 1. Statistical Temperature T(E) ========\n    # From the derivation, 1/T(E) = N * k_B / E\n    inv_T_statistical = (N * k_B) / E_target\n\n    # ======== 2. Kinetic Temperature T_kin from MD ========\n    # Perform MD simulation to get the time-averaged kinetic energy K\n    \n    # Set a seed for reproducibility\n    rng = np.random.default_rng(seed=0)\n\n    # Initialize positions and velocities randomly\n    x = rng.standard_normal(N)\n    v = rng.standard_normal(N)\n    p = m * v\n\n    # Calculate initial energy\n    K_init = 0.5 * np.sum(p**2) / m\n    V_init = 0.5 * m * omega**2 * np.sum(x**2)\n    E_init = K_init + V_init\n\n    # Rescale to target energy E_target\n    if E_init  0:\n        scale_factor = np.sqrt(E_target / E_init)\n        x *= scale_factor\n        p *= scale_factor\n    \n    # Re-calculate v from scaled p\n    v = p / m\n\n    # Velocity Verlet Integration\n    kinetic_energies = []\n    \n    # Initial acceleration\n    a = -omega**2 * x\n\n    for _ in range(n_steps):\n        # First half-step for velocity\n        v_half = v + 0.5 * a * dt\n        \n        # Full step for position\n        x = x + v_half * dt\n        \n        # Update acceleration using new positions\n        a = -omega**2 * x\n        \n        # Second half-step for velocity\n        v = v_half + 0.5 * a * dt\n        \n        # Update momentum\n        p = m * v\n        \n        # Calculate and store instantaneous kinetic energy\n        K_inst = 0.5 * np.sum(p**2) / m\n        kinetic_energies.append(K_inst)\n\n    # Calculate time-averaged kinetic energy\n    avg_K = np.mean(kinetic_energies)\n\n    # Calculate inverse kinetic temperature 1/T_kin = N * k_B / (2 * K)\n    inv_T_kinetic = (N * k_B) / (2 * avg_K)\n\n    # ======== 3. Comparison ========\n    # Calculate the relative difference delta\n    delta = np.abs(inv_T_statistical - inv_T_kinetic) / inv_T_kinetic\n    \n    return delta\n\nsolve()\n```", "id": "3494648"}, {"introduction": "Having established the concept of temperature, we now move to a more complex thermodynamic property: the heat capacity, $C_V$. This practice is designed to highlight a critical and often subtle aspect of computational statistical mechanics: the choice of ensemble matters. You will investigate why the well-known formula relating heat capacity to energy fluctuations in the canonical ($NVT$) ensemble fails dramatically in a microcanonical ($NVE$) simulation [@problem_id:3494674]. This exercise will guide you to derive the correct microcanonical expression for heat capacity from the curvature of the entropy, providing a deeper appreciation for the distinct statistical properties of different ensembles.", "problem": "You are studying the constant–Number, constant–Volume, constant–Energy ($NVE$) microcanonical ensemble for classical molecular dynamics and its implications for estimating the heat capacity $C$ of model materials. In the canonical ($NVT$) ensemble, one often uses the fluctuation formula relating heat capacity to energy fluctuations. However, in the microcanonical ($NVE$) ensemble, the total energy is conserved, so the canonical fluctuation formula fails for $C$. Your goal is to demonstrate this failure by measuring the fluctuations of the kinetic energy $K$ in $NVE$ and then constructing a correct microcanonical alternative for $C$ using the curvature of the entropy $S(E)$ with respect to energy.\n\nBase your reasoning and computations on the following principles and facts:\n- Newtonian Hamiltonian dynamics for classical systems with separable kinetic and potential energies: $H = K + U$, where $K = \\sum_i p_i^2/(2m_i)$ and $U$ is a function of positions.\n- The microcanonical ensemble is defined by the uniform probability density on the energy shell $H(\\mathbf{p},\\mathbf{q}) = E$, and the Gibbs entropy $S(E)$ can be defined as $S(E) = k_{\\mathrm{B}} \\ln \\Phi(E)$, where $\\Phi(E)$ is the phase space volume enclosed by the energy surface $H \\leq E$.\n- Equipartition of energy in classical systems states that each quadratic degree of freedom contributes $k_{\\mathrm{B}} T / 2$ to the mean energy in thermal equilibrium, and for microcanonical systems with many degrees of freedom the same relation holds to leading order when using the Gibbs entropy temperature.\n- For ideal-gas-like systems with only quadratic kinetic terms, the phase space volume scales as $\\Phi(E) \\propto E^{f/2}$, where $f$ is the number of quadratic momentum degrees of freedom.\n- For harmonic network systems with quadratic kinetic and quadratic potential terms (uncoupled or coupled with a linear transform), the phase space volume scales as $\\Phi(E) \\propto E^{f}$, where $f$ is the number of quadratic momentum degrees of freedom (there are $2f$ quadratic contributions in total, $f$ from momenta and $f$ from coordinates).\n\nTasks:\n1. For each test case below, in microcanonical ($NVE$) conditions, generate a measurement of the kinetic energy variance $\\langle K^2 \\rangle - \\langle K \\rangle^2$ by sampling the microcanonical distribution. For harmonic networks with quadratic modes, the fraction $x = K/E$ is distributed according to a Beta law on $(0,1)$ with parameters $a = f/2$, $b = f/2$, due to the uniform measure on the energy sphere split into $f$ momentum and $f$ coordinate quadratic contributions. For ideal gas with only kinetic energy, $K \\equiv E$, so $x \\equiv 1$ and the variance is zero.\n2. Using your measured $\\langle K \\rangle$, define a microcanonical temperature via equipartition as $T = 2 \\langle K \\rangle / (f k_{\\mathrm{B}})$, where $k_{\\mathrm{B}}$ is the Boltzmann constant in joules per kelvin.\n3. Compute a naive canonical fluctuation estimate $C_{\\mathrm{fluct}}$ by substituting the kinetic energy variance into the canonical formula: set $C_{\\mathrm{fluct}} = (\\langle K^2 \\rangle - \\langle K \\rangle^2) / (k_{\\mathrm{B}} T^2)$. This is a deliberate misuse of the canonical formula that you must quantify.\n4. Construct a correct microcanonical heat capacity $C_{\\mathrm{micro}}$ by using the curvature of the Gibbs entropy $S(E)$ with respect to energy. Do not use any canonical fluctuation formula. Instead, begin from the definition of temperature in microcanonical thermodynamics and derive a relation expressing $C_{\\mathrm{micro}}$ in terms of derivatives of $S(E)$ with respect to $E$. Use the given scaling forms of $\\Phi(E)$ for the two system types to make these derivatives explicit, and then compute $C_{\\mathrm{micro}}$.\n5. For each test case, output two values: $C_{\\mathrm{fluct}}$ and $C_{\\mathrm{micro}}$, both in joules per kelvin, as 64-bit floating-point numbers.\n\nRequirements:\n- All energies must be treated in joules ($\\mathrm{J}$), temperatures in kelvin ($\\mathrm{K}$), and heat capacities in joules per kelvin ($\\mathrm{J/K}$).\n- Angles are not involved in this computation.\n- Percentages are not involved; any ratio must be reported as a decimal.\n- Use a fixed random seed for reproducibility when sampling.\n- Test Suite:\n  - Case 1 (boundary behavior, small degrees of freedom): harmonic network with $f = 2$, $E = 1.0 \\times 10^{-20}\\ \\mathrm{J}$.\n  - Case 2 (larger system, many degrees of freedom): harmonic network with $f = 30$, $E = 1.0 \\times 10^{-19}\\ \\mathrm{J}$.\n  - Case 3 (edge case, kinetic-only ideal gas): ideal gas with $f = 6$, $E = 1.0 \\times 10^{-21}\\ \\mathrm{J}$.\n  - Case 4 (moderate size harmonic network): harmonic network with $f = 6$, $E = 1.0 \\times 10^{-21}\\ \\mathrm{J}$.\n- Sampling specification:\n  - For harmonic cases, sample $x$ from $\\mathrm{Beta}(a,b)$ with $a = f/2$, $b = f/2$, then set $K = x E$. Use at least $5 \\times 10^{4}$ samples for accuracy.\n  - For ideal gas, set $K = E$ for all samples.\n- Final Output Format:\n  - Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain $8$ numbers in the order $[C_{\\mathrm{fluct}}^{(1)}, C_{\\mathrm{micro}}^{(1)}, C_{\\mathrm{fluct}}^{(2)}, C_{\\mathrm{micro}}^{(2)}, C_{\\mathrm{fluct}}^{(3)}, C_{\\mathrm{micro}}^{(3)}, C_{\\mathrm{fluct}}^{(4)}, C_{\\mathrm{micro}}^{(4)}]$, corresponding to the four test cases listed above.\n\nEnsure scientific realism by deriving all required formulas from the stated foundations, and do not use any canonical fluctuation formula for $C_{\\mathrm{micro}}$.", "solution": "The problem requires an analysis of heat capacity calculations within the microcanonical ($NVE$) ensemble. Specifically, we must contrast a naively applied canonical fluctuation formula with a rigorously derived microcanonical expression for heat capacity, $C$. The analysis will be performed for two model systems: an ideal gas and a harmonic network.\n\nFirst, we derive the correct expression for the microcanonical heat capacity, $C_{\\mathrm{micro}}$. The foundation of microcanonical thermodynamics is the Gibbs entropy, $S(E)$, defined in terms of the phase space volume $\\Phi(E)$ enclosed by the energy surface $H \\leq E$:\n$$S(E) = k_{\\mathrm{B}} \\ln \\Phi(E)$$\nwhere $k_{\\mathrm{B}}$ is the Boltzmann constant. The thermodynamic temperature, $T$, is defined as the inverse of the change in entropy with respect to energy:\n$$ \\frac{1}{T} = \\frac{\\partial S(E)}{\\partial E} $$\nThe heat capacity at constant volume, $C$, is defined as the energy required to raise the temperature by one unit, $C = \\partial E / \\partial T$. We can express this in terms of derivatives of $S(E)$. Using the chain rule on the definition of temperature:\n$$ \\frac{\\partial}{\\partial E} \\left(\\frac{1}{T}\\right) = \\frac{\\partial}{\\partial T}\\left(\\frac{1}{T}\\right) \\frac{\\partial T}{\\partial E} = -\\frac{1}{T^2} \\frac{\\partial T}{\\partial E} = -\\frac{1}{T^2 C} $$\nSimultaneously, we have:\n$$ \\frac{\\partial}{\\partial E} \\left(\\frac{1}{T}\\right) = \\frac{\\partial}{\\partial E} \\left(\\frac{\\partial S(E)}{\\partial E}\\right) = \\frac{\\partial^2 S(E)}{\\partial E^2} $$\nEquating these two expressions gives a formula for the microcanonical heat capacity based on the curvature of the entropy:\n$$ C_{\\mathrm{micro}} = -\\frac{1}{T^2 \\left(\\frac{\\partial^2 S(E)}{\\partial E^2}\\right)} = - \\frac{\\left(\\frac{\\partial S(E)}{\\partial E}\\right)^2}{\\frac{\\partial^2 S(E)}{\\partial E^2}} $$\nWe now apply this formula to the specific systems, using their given phase space volume scaling laws.\n\nFor a harmonic network with $f$ quadratic momentum degrees of freedom (and implicitly $f$ quadratic potential degrees of freedom), the phase space volume scales as $\\Phi(E) \\propto E^f$. Thus, $S(E) = k_{\\mathrm{B}} \\ln(c_1 E^f) = k_{\\mathrm{B}} \\ln c_1 + f k_{\\mathrm{B}} \\ln E$ for some constant $c_1$. The derivatives are:\n$$ \\frac{\\partial S}{\\partial E} = \\frac{f k_{\\mathrm{B}}}{E} \\quad \\text{and} \\quad \\frac{\\partial^2 S}{\\partial E^2} = -\\frac{f k_{\\mathrm{B}}}{E^2} $$\nSubstituting these into our expression for $C_{\\mathrm{micro}}$:\n$$ C_{\\mathrm{micro}}^{\\mathrm{(harmonic)}} = - \\frac{\\left(\\frac{f k_{\\mathrm{B}}}{E}\\right)^2}{-\\frac{f k_{\\mathrm{B}}}{E^2}} = \\frac{f^2 k_{\\mathrm{B}}^2 / E^2}{f k_{\\mathrm{B}} / E^2} = f k_{\\mathrm{B}} $$\n\nFor an ideal gas with $f$ quadratic momentum degrees of freedom, the phase space volume scales as $\\Phi(E) \\propto E^{f/2}$. The entropy is $S(E) = k_{\\mathrm{B}} \\ln(c_2 E^{f/2}) = k_{\\mathrm{B}} \\ln c_2 + \\frac{f}{2} k_{\\mathrm{B}} \\ln E$. The derivatives are:\n$$ \\frac{\\partial S}{\\partial E} = \\frac{f k_{\\mathrm{B}}}{2E} \\quad \\text{and} \\quad \\frac{\\partial^2 S}{\\partial E^2} = -\\frac{f k_{\\mathrm{B}}}{2E^2} $$\nSubstituting these into the expression for $C_{\\mathrm{micro}}$:\n$$ C_{\\mathrm{micro}}^{\\mathrm{(ideal)}} = - \\frac{\\left(\\frac{f k_{\\mathrm{B}}}{2E}\\right)^2}{-\\frac{f k_{\\mathrm{B}}}{2E^2}} = \\frac{f^2 k_{\\mathrm{B}}^2 / (4E^2)}{f k_{\\mathrm{B}} / (2E^2)} = \\frac{f}{2} k_{\\mathrm{B}} $$\nThese results, $f k_{\\mathrm{B}}$ and $\\frac{f}{2} k_{\\mathrm{B}}$, are the correct, well-established classical heat capacities for these systems.\n\nNext, we evaluate the naive fluctuation-based estimate, $C_{\\mathrm{fluct}}$. This involves deliberately misusing the canonical ensemble formula for heat capacity, but applying it to the kinetic energy fluctuations within the microcanonical ensemble. The prescribed formulas are:\n$$ T = \\frac{2 \\langle K \\rangle}{f k_{\\mathrm{B}}} \\quad \\text{and} \\quad C_{\\mathrm{fluct}} = \\frac{\\langle K^2 \\rangle - \\langle K \\rangle^2}{k_{\\mathrm{B}} T^2} = \\frac{\\mathrm{Var}(K)}{k_{\\mathrm{B}} T^2} $$\nThe expectation values $\\langle K \\rangle$ and $\\langle K^2 \\rangle$ are to be obtained by sampling.\n\nFor the harmonic network, the kinetic energy fraction $x = K/E$ is specified to follow a Beta distribution, $x \\sim \\mathrm{Beta}(a,b)$ with parameters $a = b = f/2$. The mean and variance of this distribution are known analytically:\n$$ \\langle x \\rangle = \\frac{a}{a+b} = \\frac{f/2}{f} = \\frac{1}{2} $$\n$$ \\mathrm{Var}(x) = \\frac{ab}{(a+b)^2(a+b+1)} = \\frac{(f/2)^2}{f^2(f+1)} = \\frac{1}{4(f+1)} $$\nFrom these, we find the mean and variance of the kinetic energy $K = xE$:\n$$ \\langle K \\rangle = E \\langle x \\rangle = \\frac{E}{2} $$\n$$ \\mathrm{Var}(K) = E^2 \\mathrm{Var}(x) = \\frac{E^2}{4(f+1)} $$\nUsing these analytical results (which numerical sampling will approximate), we can find the expression for $C_{\\mathrm{fluct}}$. First, the temperature is:\n$$ T = \\frac{2 (E/2)}{f k_{\\mathrm{B}}} = \\frac{E}{f k_{\\mathrm{B}}} $$\nThis temperature matches the one derived from entropy ($1/T = \\partial S/\\partial E = f k_{\\mathrm{B}}/E$), confirming the consistency of the problem's premises. Now, we compute $C_{\\mathrm{fluct}}$:\n$$ C_{\\mathrm{fluct}}^{\\mathrm{(harmonic)}} = \\frac{\\mathrm{Var}(K)}{k_{\\mathrm{B}} T^2} = \\frac{E^2 / (4(f+1))}{k_{\\mathrm{B}} (E / (f k_{\\mathrm{B}}))^2} = \\frac{E^2}{4(f+1)} \\frac{f^2 k_{\\mathrm{B}}}{E^2} = \\frac{f^2}{4(f+1)} k_{\\mathrm{B}} $$\n\nFor the ideal gas, the system has only kinetic energy, so $K \\equiv E$.\n$$ \\langle K \\rangle = E \\quad \\text{and} \\quad \\mathrm{Var}(K) = \\langle E^2 \\rangle - \\langle E \\rangle^2 = E^2 - E^2 = 0 $$\nThe temperature is $T = 2E/(f k_{\\mathrm{B}})$, which is again consistent with the entropy derivative. The fluctuation-based heat capacity is immediately:\n$$ C_{\\mathrm{fluct}}^{\\mathrm{(ideal)}} = \\frac{0}{k_{\\mathrm{B}} T^2} = 0 $$\n\nThe discrepancy between $C_{\\mathrm{micro}}$ and $C_{\\mathrm{fluct}}$ is now clear. For harmonic networks, $C_{\\mathrm{fluct}} = k_{\\mathrm{B}} f^2 / (4(f+1))$ which for large $f$ approaches $f k_{\\mathrm{B}} / 4$, a factor of $4$ smaller than the correct value $C_{\\mathrm{micro}} = f k_{\\mathrm{B}}$. For the ideal gas, $C_{\\mathrm{fluct}}$ is identically zero, while the correct value is $C_{\\mathrm{micro}} = f k_{\\mathrm{B}} / 2$. This demonstrates that kinetic energy fluctuations in the $NVE$ ensemble do not yield the heat capacity via the simple canonical formula.\n\nThe computational procedure will involve:\n1. For each test case, calculate the analytical $C_{\\mathrm{micro}}$ using $C_{\\mathrm{micro}}^{\\mathrm{(harmonic)}} = f k_{\\mathrm{B}}$ or $C_{\\mathrm{micro}}^{\\mathrm{(ideal)}} = \\frac{f}{2} k_{\\mathrm{B}}$.\n2. For harmonic cases, generate a large number of samples for $x$ from the $\\mathrm{Beta}(f/2, f/2)$ distribution to obtain numerical estimates for $\\langle K \\rangle$ and $\\mathrm{Var}(K)$. For the ideal gas case, set $\\langle K \\rangle = E$ and $\\mathrm{Var}(K) = 0$.\n3. Use the numerically obtained $\\langle K \\rangle$ to compute $T$.\n4. Use the numerically obtained $\\mathrm{Var}(K)$ and the computed $T$ to find $C_{\\mathrm{fluct}}$.\n5. Report both $C_{\\mathrm{fluct}}$ and $C_{\\mathrm{micro}}$ for each test case.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import beta\n\ndef solve():\n    \"\"\"\n    Calculates and compares a naive fluctuation-based heat capacity (C_fluct)\n    with the correct microcanonical heat capacity (C_micro) for different\n    model systems in the NVE ensemble.\n    \"\"\"\n    #\n    # CONSTANTS AND PARAMETERS\n    #\n    # Boltzmann constant in J/K (CODATA 2018)\n    k_B = 1.380649e-23\n\n    # Sampling parameters\n    NUM_SAMPLES = 50000\n    # Use a fixed random seed for reproducibility\n    RNG = np.random.default_rng(seed=42)\n\n    # Test suite from the problem statement\n    # Each tuple: (system_type, degrees_of_freedom_f, total_energy_E)\n    test_cases = [\n        (\"harmonic\", 2, 1.0e-20),\n        (\"harmonic\", 30, 1.0e-19),\n        (\"ideal\", 6, 1.0e-21),\n        (\"harmonic\", 6, 1.0e-21),\n    ]\n\n    results = []\n\n    for system_type, f, E in test_cases:\n        #\n        # Task 4: Correct Microcanonical Heat Capacity (C_micro)\n        # This is calculated from the analytical formulas derived from the\n        # curvature of the Gibbs entropy S(E).\n        #\n        if system_type == \"harmonic\":\n            # For a harmonic network, C_micro = f * k_B\n            C_micro = f * k_B\n        elif system_type == \"ideal\":\n            # For an ideal gas, C_micro = (f / 2) * k_B\n            C_micro = (f / 2) * k_B\n        else:\n            raise ValueError(f\"Unknown system type: {system_type}\")\n\n        #\n        # Tasks 1-3: Naive Fluctuation Heat Capacity (C_fluct)\n        # This is calculated by sampling kinetic energy K and applying a\n        # deliberately misused canonical fluctuation formula.\n        #\n        if system_type == \"harmonic\":\n            # Task 1 (part 1): Sample kinetic energy K for a harmonic network\n            # The kinetic energy fraction x = K/E follows a Beta distribution\n            # with parameters a = f/2, b = f/2.\n            a = f / 2.0\n            b = f / 2.0\n            \n            x_samples = beta.rvs(a, b, size=NUM_SAMPLES, random_state=RNG)\n            K_samples = x_samples * E\n            \n            # Calculate mean and variance of kinetic energy from samples\n            mean_K = np.mean(K_samples)\n            var_K = np.var(K_samples) # np.var uses ddof=0, population variance\n\n        elif system_type == \"ideal\":\n            # Task 1 (part 2): For an ideal gas, K is constant and equal to E\n            mean_K = E\n            var_K = 0.0\n\n        # Task 2: Define microcanonical temperature T from mean kinetic energy\n        # The equipartition theorem gives T = 2 * K / (f * k_B)\n        if f == 0:\n            # Avoid division by zero, though not expected in test cases\n            T = 0.0\n        else:\n            T = (2.0 * mean_K) / (f * k_B)\n\n        # Task 3: Compute the naive fluctuation heat capacity C_fluct\n        if T == 0.0:\n            # If T is zero, C_fluct is undefined or zero if var_K is also zero.\n            C_fluct = 0.0\n        else:\n            C_fluct = var_K / (k_B * T**2)\n\n        # Task 5: Store results\n        results.append(C_fluct)\n        results.append(C_micro)\n\n    # Final print statement in the exact required format.\n    # Output must be a single line: [C_fluct_1,C_micro_1,C_fluct_2,C_micro_2,...]\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3494674"}, {"introduction": "A molecular dynamics simulation generates a single trajectory through phase space, yet we use it to infer properties of the entire system. This practice confronts the crucial assumption underlying this approach: the ergodic hypothesis. This exercise challenges you to implement and apply diagnostics to assess whether a simulation is truly \"mixing\" and exploring the constant-energy surface effectively [@problem_id:3494707]. By analyzing the behavior of integrable, chaotic, and non-ergodic systems, you will learn to use tools like time autocorrelation and Poincaré sections to identify when a simulation is providing reliable ensemble averages and when it might be trapped, producing biased and misleading results.", "problem": "Write a complete, runnable program that implements and applies a principled diagnostic of mixing on a constant-energy manifold for Hamiltonian molecular dynamics in the microcanonical ensemble. Your diagnostic must combine two independent criteria: (i) decay of the time autocorrelation of an observable and (ii) coverage of a Poincaré surface of section on the energy shell. Then quantify the finite-time microcanonical bias for a symmetry-selected observable. The design must start from fundamental definitions and laws without shortcut formulas.\n\nYou must base your derivation and algorithm on the following fundamental principles:\n- Newtonian mechanics for Hamiltonian systems, with position $q$ and momentum $p$ obeying Hamilton's equations $\\dot{q} = \\partial H / \\partial p$ and $\\dot{p} = - \\partial H / \\partial q$, where $H(q,p)$ is the Hamiltonian.\n- The microcanonical ensemble is defined by the uniform invariant density on the energy shell $H(q,p)=E$ for total energy $E$. For a phase function (observable) $A(q,p)$, the microcanonical expectation $\\langle A \\rangle_{\\mu E}$ is the average of $A$ over the energy shell with respect to the invariant measure.\n- Mixing on the energy shell implies that time correlations of centered observables decay to zero at long times, and that trajectories explore the available manifold in a way that fills Poincaré sections in area-like regions (rather than lying on smooth curves characteristic of integrable dynamics).\n- For a stationary time series $a(t)$ constructed from a bounded observable along a Hamiltonian trajectory, define the fluctuation $\\delta a(t) = a(t) - \\overline{a}$, where $\\overline{a}$ is the time average over a finite interval. Define the normalized autocorrelation $\\rho(\\tau) = C(\\tau)/C(0)$, where $C(\\tau) = \\lim_{T \\to \\infty} \\frac{1}{T} \\int_{0}^{T} \\delta a(t)\\,\\delta a(t+\\tau)\\,dt$. In computation with finite $T$, approximate $C(\\tau)$ by the discrete-time estimator using a long trajectory.\n- A Poincaré section is defined by a codimension-one surface, here taken as $y=0$ with $p_y > 0$ in two-dimensional systems. The set of section points $(x,p_x)$ produced by a long trajectory at energy $E$ lies within the energy-allowed domain of the section. For an integrable system, section points lie on smooth invariant curves; for a mixing (chaotic) system, section points fill positive-area regions.\n\nAlgorithmic requirements and observable selection:\n- You must use the symplectic velocity-Verlet scheme to integrate each Hamiltonian system with a constant time step $dt$ to minimize energy drift.\n- For each system, define a scalar observable $A(q,p)$ that has a known microcanonical expectation by symmetry at the specified energy:\n  - Isotropic two-dimensional harmonic oscillator: $A(t) = x(t)^2 - y(t)^2$, whose microcanonical expectation is $\\langle A \\rangle_{\\mu E} = 0$ by symmetry on the energy shell.\n  - Henon–Heiles system: $A(t) = x(t)$, whose microcanonical expectation is $\\langle A \\rangle_{\\mu E} = 0$ by the $x \\mapsto -x$ symmetry of the Hamiltonian.\n  - Symmetric one-dimensional double-well: $A(t) = x(t)$, whose microcanonical expectation is $\\langle A \\rangle_{\\mu E} = 0$ by $x \\mapsto -x$ symmetry.\n- Define a finite-time estimate of bias as $b_T = \\left|\\overline{A}_T - \\langle A \\rangle_{\\mu E}\\right|$, where $\\overline{A}_T$ is the time average of $A$ over the simulated time $T$.\n- Define an integrated absolute correlation time over a fixed lag window as\n$$\n\\tau_{\\mathrm{abs}} = dt \\sum_{k=1}^{K} \\left|\\rho(k\\,dt)\\right|,\n$$\nwith $K$ chosen to correspond to a window $T_{\\mathrm{lag}} = K\\,dt$ that is small compared to the total simulation time but large compared to the microscopic time scales.\n- Define a Poincaré coverage fraction for two-dimensional systems as follows: using the crossings of $y=0$ with $p_y > 0$, record $(x,p_x)$ at the crossing times. Partition the fixed energy-allowed domain on the section into a uniform $G \\times G$ grid and compute the fraction of grid cells visited at least once. For the chosen section $y=0$ in the two-dimensional isotropic harmonic oscillator and Henon–Heiles systems at energy $E$, the allowed domain is the disk $x^2 + p_x^2 \\le 2E$. For one-dimensional systems, report a coverage fraction of $0$ because the chosen section is not applicable.\n\nMixing decision rule:\n- Declare that a trajectory is mixing if and only if both conditions hold: $\\tau_{\\mathrm{abs}}  \\tau_{\\mathrm{thr}}$ and the coverage fraction exceeds a threshold $c_{\\mathrm{thr}}$, with thresholds specified below.\n\nNumerical details, units, and test suite:\n- Use dimensionless units throughout.\n- Use the velocity-Verlet integrator with time step $dt = 10^{-2}$ and total number of steps $N = 60000$ for each system, giving a total simulated time $T = N \\, dt = 600$.\n- Compute the normalized autocorrelation using a fast convolution method and take $T_{\\mathrm{lag}} = 50$ so that $K = T_{\\mathrm{lag}}/dt = 5000$. Compute $\\tau_{\\mathrm{abs}}$ as defined above.\n- For Poincaré coverage, use a uniform $G \\times G$ grid with $G = 64$ over the square $[-R,R] \\times [-R,R]$ where $R = \\sqrt{2E}$, and normalize the coverage by the number of grid cells whose centers lie inside the disk $x^2 + p_x^2 \\le 2E$.\n- Thresholds for the mixing decision: $\\tau_{\\mathrm{thr}} = 10$ and $c_{\\mathrm{thr}} = 0.05$.\n\nSimulate the following three systems (this set of parameter values forms the required test suite):\n- Case $1$ (two-dimensional isotropic harmonic oscillator): Hamiltonian $H = \\frac{1}{2}(p_x^2 + p_y^2) + \\frac{1}{2}(x^2 + y^2)$, total energy $E = 1$. Initial condition $(x_0,y_0,p_{x,0},p_{y,0}) = (1,0,1,0)$. Observable $A(t)=x(t)^2 - y(t)^2$. For this case, the microcanonical value is $\\langle A \\rangle_{\\mu E} = 0$.\n- Case $2$ (Henon–Heiles): Hamiltonian $H = \\frac{1}{2}(p_x^2 + p_y^2) + \\frac{1}{2}(x^2 + y^2) + x^2 y - \\frac{1}{3} y^3$, total energy $E = 0.12$. Initial condition $(x_0,y_0,p_{x,0}) = (0,0.1,0)$ and choose $p_{y,0} > 0$ to satisfy $H=E$. Observable $A(t)=x(t)$. For this case, the microcanonical value is $\\langle A \\rangle_{\\mu E} = 0$.\n- Case $3$ (one-dimensional symmetric double well): Hamiltonian $H = \\frac{1}{2}p^2 + \\frac{1}{4}(x^2 - 1)^2$, total energy $E = 0.1$. Initial condition $x_0 = -1$, choose $p_0 > 0$ to satisfy $H=E$. Observable $A(t)=x(t)$. For this case, the microcanonical value is $\\langle A \\rangle_{\\mu E} = 0$.\n\nProgram output specification:\n- For each case, compute the boolean mixing decision and the absolute bias $b_T$. Your program should produce a single line of output containing a list of three two-element lists in the order of the cases, where each inner list is of the form $[\\text{mixing\\_boolean}, \\text{bias}]$. The mixing boolean must be a programming-language boolean. The bias must be a floating-point number rounded to six decimal places. For example, an output line could look like $[[\\text{True},0.012345],[\\text{False},0.678901],[\\text{False},0.234567]]$.", "solution": "The problem statement is assessed to be valid. It is scientifically grounded in classical and statistical mechanics, well-posed with all necessary parameters and definitions provided, and objective in its formulation. The problem requires the implementation of a sophisticated diagnostic for mixing in Hamiltonian systems, a canonical task in computational physics. It combines temporal analysis (autocorrelation) and phase-space geometric analysis (Poincaré section) to classify the dynamics of three well-understood model systems.\n\nThe solution proceeds by first constructing the necessary components from fundamental principles, and then applying them to the three specified test cases.\n\n### 1. Theoretical Foundation and Numerical Integration\n\nThe dynamics of each system are governed by a Hamiltonian $H(q,p)$, where $q$ are the generalized coordinates and $p$ are the conjugate momenta. The time evolution follows Hamilton's equations of motion:\n$$\n\\dot{q} = \\frac{\\partial H}{\\partial p}, \\quad \\dot{p} = - \\frac{\\partial H}{\\partial q}\n$$\nFor the Hamiltonians specified, which take the form $H(q,p) = \\frac{1}{2m}p^2 + V(q)$, and assuming unit mass ($m=1$), these equations become $\\dot{q} = p$ and $\\dot{p} = -\\nabla V(q) = F(q)$, where $F(q)$ is the force derived from the potential energy $V(q)$.\n\nTo numerically integrate these equations of motion while preserving the geometric structure of Hamiltonian flow and ensuring good energy conservation, we employ the **symplectic velocity-Verlet algorithm**. For a time step $dt$, the algorithm updates positions and momenta from time $t$ to $t+dt$ as follows:\n1.  Update momentum by a half-step: $p(t + dt/2) = p(t) + F(q(t)) \\cdot (dt/2)$.\n2.  Update position by a full step using the half-step momentum: $q(t + dt) = q(t) + p(t + dt/2) \\cdot dt$.\n3.  Compute the new force $F(q(t + dt))$ at the new position.\n4.  Complete the momentum update: $p(t + dt) = p(t + dt/2) + F(q(t + dt)) \\cdot (dt/2)$.\n\nThis process is repeated for $N=60000$ steps with $dt=10^{-2}$ to generate a trajectory of total duration $T=600$.\n\n### 2. Diagnostic Metrics for Mixing\n\nThe microcanonical ensemble describes an isolated system at constant total energy $E$. The ergodic hypothesis posits that for a chaotic system, time averages along a single long trajectory are equivalent to ensemble averages over the constant-energy surface. A system is **mixing** if it is ergodic and correlations between observables decay over time. We use two metrics to test for mixing behavior.\n\n#### 2.1. Autocorrelation Time, $\\tau_{\\mathrm{abs}}$\n\nFor a stationary time series of an observable $A(t)$, we first compute its time average over the trajectory, $\\overline{A}_T$. The fluctuation is $\\delta A(t) = A(t) - \\overline{A}_T$. The normalized time autocorrelation function is:\n$$\n\\rho(\\tau) = \\frac{C(\\tau)}{C(0)}, \\quad \\text{where} \\quad C(\\tau) = \\langle \\delta A(t) \\delta A(t+\\tau) \\rangle_t\n$$\nIn a mixing system, $\\rho(\\tau) \\to 0$ as $\\tau \\to \\infty$. In a non-ergodic or integrable system, $\\rho(\\tau)$ may exhibit persistent oscillations. We quantify the decay rate by computing the integrated absolute correlation time over a lag window $T_{\\mathrm{lag}} = K\\,dt = 50$:\n$$\n\\tau_{\\mathrm{abs}} = dt \\sum_{k=1}^{K} \\left|\\rho(k\\,dt)\\right|\n$$\nA small $\\tau_{\\mathrm{abs}}$ indicates rapid decay and is a signature of mixing. The autocorrelation is computed efficiently using a fast Fourier transform (FFT) based convolution method (the Wiener-Khinchin theorem).\n\n#### 2.2. Poincaré Section Coverage\n\nA Poincaré section provides a geometric view of the phase space dynamics. For our two-dimensional systems, we define the section by the plane $y=0$ and record the state $(x, p_x)$ whenever the trajectory crosses this plane with positive momentum in the $y$-direction ($p_y > 0$).\n-   For an **integrable** system, these points lie on smooth, one-dimensional curves (invariant tori).\n-   For a **mixing** system, the points appear to fill a two-dimensional area on the section, corresponding to chaotic wandering on the energy manifold.\n\nTo quantify this, we discretize the accessible region of the section, $x^2 + p_x^2 \\le 2E$, with a uniform $G \\times G$ grid (where $G=64$). The coverage fraction is the number of grid cells visited by the trajectory at least once, normalized by the total number of grid cell centers that lie within the accessible disk. A high coverage fraction indicates chaotic, area-filling behavior consistent with mixing. For one-dimensional systems, this specific section is not applicable, and the coverage is defined as $0$.\n\n### 3. Finite-Time Bias and Mixing Decision\n\nThe **finite-time bias**, $b_T$, measures the deviation of the computed time average of an observable, $\\overline{A}_T$, from its true microcanonical expectation, $\\langle A \\rangle_{\\mu E}$. The observables are chosen such that symmetry dictates $\\langle A \\rangle_{\\mu E} = 0$. Thus, the bias is simply:\n$$\nb_T = \\left|\\overline{A}_T\\right|\n$$\nA large bias suggests that the trajectory has not explored the phase space ergodically.\n\nThe final **mixing decision** combines our two criteria. A trajectory is declared `mixing` if and only if both conditions are met:\n1.  The integrated correlation time is small: $\\tau_{\\mathrm{abs}}  \\tau_{\\mathrm{thr}}$ (with $\\tau_{\\mathrm{thr}} = 10$).\n2.  The Poincaré coverage is significant: `coverage` $ c_{\\mathrm{thr}}$ (with $c_{\\mathrm{thr}} = 0.05$).\n\n### 4. Application to Test Systems\n\nThis framework is applied to three systems:\n\n1.  **Isotropic Harmonic Oscillator ($E=1$)**: This system is fully integrable. The chosen initial condition, $(1,0,1,0)$, leads to a trivial trajectory along the $x$-axis where $y(t) \\equiv 0$ and $p_y(t) \\equiv 0$. It will never cross the Poincaré section with $p_y > 0$, resulting in zero coverage. The observable $A(t) = x(t)^2 - y(t)^2 = x(t)^2$ is periodic, leading to a non-decaying correlation function and a large $\\tau_{\\mathrm{abs}}$. The system is correctly identified as **non-mixing**.\n\n2.  **Henon-Heiles System ($E=0.12$)**: At this energy, the system is known to be predominantly chaotic. We expect the trajectory to explore a large volume of the energy shell. The autocorrelation of $A(t)=x(t)$ should decay quickly, and the Poincaré section should be densely filled. The system should be identified as **mixing**, and the bias should be small due to the symmetric exploration of phase space.\n\n3.  **Symmetric Double Well ($E=0.1$)**: The potential barrier height is $V(0) = 0.25$. Since the total energy $E=0.1$ is less than the barrier height, a trajectory starting in one well (at $x_0=-1$) remains trapped there. It cannot explore the full energy surface, violating ergodicity. The time average of $A(t)=x(t)$ will be negative, not the symmetric expectation of $0$, leading to a large bias. The motion is periodic, so $\\tau_{\\mathrm{abs}}$ will be large. The Poincaré coverage is defined as $0$. The system is correctly identified as **non-mixing**.\n\nThe implementation details follow the numerical parameters specified in the problem, including the calculation of initial momenta and the specific definitions of the analysis metrics.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.signal import fftconvolve\n\n# This program must be run with:\n# python 3.12\n# numpy 1.23.5\n# scipy 1.11.4\n\ndef solve():\n    \"\"\"\n    Main function to run the mixing diagnostics on three Hamiltonian systems.\n    \"\"\"\n    # Global numerical parameters from the problem statement\n    DT = 1e-2\n    N_STEPS = 60000\n    T_LAG = 50.0\n    K_STEPS = int(T_LAG / DT)\n    GRID_SIZE = 64\n    TAU_THR = 10.0\n    C_THR = 0.05\n\n    # --- System Definitions ---\n\n    # Case 1: Isotropic Harmonic Oscillator\n    def ho_force(q):\n        return -q\n    def ho_observable(q, p):\n        return q[0]**2 - q[1]**2\n\n    # Case 2: Henon-Heiles\n    def hh_force(q):\n        x, y = q\n        fx = -x - 2 * x * y\n        fy = -y - x**2 + y**2\n        return np.array([fx, fy])\n    def hh_observable(q, p):\n        return q[0]\n\n    # Case 3: 1D Symmetric Double Well\n    def dw_force(q):\n        x = q[0]\n        fx = x - x**3\n        return np.array([fx])\n    def dw_observable(q, p):\n        return q[0]\n\n    # --- Core Simulation and Analysis Functions ---\n\n    def velocity_verlet_integrator(z0, dim, force_func, dt, n_steps):\n        \"\"\"\n        Integrates Hamiltonian dynamics using the velocity-Verlet algorithm.\n        Also detects and records Poincaré section crossings for 2D systems.\n        \"\"\"\n        z_history = np.zeros((n_steps + 1, 2 * dim))\n        z = z0.copy()\n        z_history[0] = z\n        \n        poincare_points = []\n\n        q_old, p_old = None, None\n\n        for i in range(n_steps):\n            q = z[:dim]\n            p = z[dim:]\n\n            if dim == 2:\n                q_old = q.copy()\n            \n            # Velocity-Verlet update\n            p_half = p + force_func(q) * (dt / 2.0)\n            q_new = q + p_half * dt\n            p_new = p_half + force_func(q_new) * (dt / 2.0)\n            \n            z[:dim], z[dim:] = q_new, p_new\n            z_history[i + 1] = z\n\n            # Poincaré section detection for 2D systems (y=0, py0)\n            if dim == 2:\n                y_old, y_new = q_old[1], q_new[1]\n                if y_old * y_new  0 and p_new[1]  0:\n                    # Linear interpolation to find the crossing point\n                    alpha = -y_old / (y_new - y_old)\n                    x_cross = q_old[0] + alpha * (q_new[0] - q_old[0])\n                    px_cross = z_history[i, 2] + alpha * (p_new[0] - z_history[i, 2])\n                    poincare_points.append((x_cross, px_cross))\n\n        return z_history, poincare_points\n\n    def calculate_autocorrelation(A_series, k_max, dt):\n        \"\"\"\n        Calculates the normalized autocorrelation and the integrated absolute correlation time.\n        \"\"\"\n        signal = A_series - np.mean(A_series)\n        n = len(signal)\n        # Using FFT for fast convolution\n        autocov = fftconvolve(signal, signal[::-1], mode='full')[n - 1:]\n        \n        # Biased estimator for autocovariance\n        autocov /= n\n        \n        # Avoid division by zero if variance is nil\n        if autocov[0] == 0:\n            return 0.0\n\n        rho = autocov / autocov[0]\n        \n        if len(rho) = k_max:\n             k_max = len(rho) -1\n        \n        tau_abs = dt * np.sum(np.abs(rho[1:k_max + 1]))\n        return tau_abs\n\n    def calculate_poincare_coverage(points, E, G):\n        \"\"\"\n        Calculates the coverage fraction of a Poincaré section.\n        \"\"\"\n        if not points:\n            return 0.0\n\n        R = np.sqrt(2 * E)\n        \n        # Calculate the total number of grid cells within the allowed domain\n        grid_coords = np.linspace(-R, R, G, endpoint=False) + R/G\n        centers_x, centers_y = np.meshgrid(grid_coords, grid_coords)\n        in_disk = centers_x**2 + centers_y**2 = 2 * E\n        total_allowed_cells = np.sum(in_disk)\n\n        if total_allowed_cells == 0:\n            return 0.0\n\n        visited_cells = set()\n        for x, px in points:\n            if -R = x  R and -R = px  R:\n                ix = int((x + R) / (2 * R) * G)\n                ipx = int((px + R) / (2 * R) * G)\n                visited_cells.add((ix, ipx))\n        \n        coverage = len(visited_cells) / total_allowed_cells\n        return coverage\n\n    def process_case(case, dt, n_steps, k_steps, grid_size, tau_thr, c_thr):\n        \"\"\"\n        Processes a single test case: runs simulation and performs analysis.\n        \"\"\"\n        # Run integrator\n        state_history, poincare_points = velocity_verlet_integrator(\n            case[\"ic\"], case[\"dim\"], case[\"force_func\"], dt, n_steps\n        )\n\n        # Calculate observable time series\n        q_history = state_history[:, :case[\"dim\"]]\n        p_history = state_history[:, case[\"dim\"]:]\n        A_series = np.array([case[\"observable_func\"](q, p) for q, p in zip(q_history, p_history)])\n        \n        # Calculate bias\n        A_mean = np.mean(A_series)\n        bias = np.abs(A_mean - case[\"mu_A\"])\n\n        # Calculate correlation time\n        tau_abs = calculate_autocorrelation(A_series, k_steps, dt)\n        \n        # Calculate Poincaré coverage\n        if case[\"dim\"] == 2:\n            coverage = calculate_poincare_coverage(poincare_points, case[\"E\"], grid_size)\n        else:\n            coverage = 0.0\n            \n        # Apply mixing decision rule\n        is_mixing = (tau_abs  tau_thr) and (coverage  c_thr)\n        \n        return is_mixing, bias\n\n    # --- Test Suite Setup and Execution ---\n\n    test_cases = [\n        # Case 1: Harmonic Oscillator\n        {\n            \"name\": \"HO\", \"dim\": 2, \"force_func\": ho_force,\n            \"observable_func\": ho_observable, \"E\": 1.0,\n            \"ic\": np.array([1.0, 0.0, 1.0, 0.0]), \"mu_A\": 0.0\n        },\n        # Case 2: Henon-Heiles\n        {\n            \"name\": \"HH\", \"dim\": 2, \"force_func\": hh_force,\n            \"observable_func\": hh_observable, \"E\": 0.12,\n            \"ic\": np.array([0.0, 0.1, 0.0, 0.0]), \"mu_A\": 0.0\n        },\n        # Case 3: Double Well\n        {\n            \"name\": \"DW\", \"dim\": 1, \"force_func\": dw_force,\n            \"observable_func\": dw_observable, \"E\": 0.1,\n            \"ic\": np.array([-1.0, 0.0]), \"mu_A\": 0.0\n        }\n    ]\n\n    # Complete initial conditions using energy constraint\n    # Case 2: Henon-Heiles\n    E2, q2, p2x = test_cases[1][\"E\"], test_cases[1][\"ic\"][:2], test_cases[1][\"ic\"][2]\n    V2 = 0.5 * (q2[0]**2 + q2[1]**2) + q2[0]**2 * q2[1] - (q2[1]**3) / 3.0\n    p2y_sq = 2 * (E2 - V2) - p2x**2\n    test_cases[1][\"ic\"][3] = np.sqrt(p2y_sq)\n\n    # Case 3: Double Well\n    E3, q3 = test_cases[2][\"E\"], test_cases[2][\"ic\"][0]\n    V3 = 0.25 * (q3**2 - 1.0)**2\n    p3_sq = 2 * (E3 - V3)\n    test_cases[2][\"ic\"][1] = np.sqrt(p3_sq)\n\n    # Process all cases and collect results\n    results = []\n    for case in test_cases:\n        is_mixing, bias = process_case(\n            case, DT, N_STEPS, K_STEPS, GRID_SIZE, TAU_THR, C_THR\n        )\n        results.append([is_mixing, round(bias, 6)])\n\n    # Final print statement in the exact required format\n    # The default str() representation of lists has spaces, which we remove\n    output_str = str(results).replace(\" \", \"\")\n    print(output_str)\n\nsolve()\n```", "id": "3494707"}]}