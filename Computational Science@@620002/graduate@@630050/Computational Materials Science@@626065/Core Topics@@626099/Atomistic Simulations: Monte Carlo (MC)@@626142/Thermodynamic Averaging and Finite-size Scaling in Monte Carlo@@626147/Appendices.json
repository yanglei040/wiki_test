{"hands_on_practices": [{"introduction": "Before analyzing simulation data, it is crucial to establish the correct theoretical framework. This exercise explores the Harris criterion, a fundamental principle that determines whether weak, quenched disorder in a material alters its critical behavior. By deriving and applying this criterion [@problem_id:3495495], you will learn how to predict the stability of a clean critical point and correctly identify the set of scaling exponents to use in your finite-size scaling analysis.", "problem": "A computational materials science group uses large-scale Monte Carlo (MC) simulations to study the ferromagnetic phase transition in a three-dimensional crystalline alloy with weak, quenched, uncorrelated compositional disorder in the exchange couplings. In the clean limit (no disorder), the transition belongs to the three-dimensional Heisenberg universality class with correlation-length exponent $\\nu_{0}$ and susceptibility exponent $\\gamma_{0}$. The disorder is introduced by independently perturbing bond strengths with zero mean and finite variance, and it couples locally to the energy density. The MC data are analyzed using thermodynamic averaging over disorder realizations and finite-size scaling.\n\nStarting from first principles of scaling near continuous phase transitions and the Central Limit Theorem (CLT) applied to block-averaged disorder, derive the Harris criterion that determines whether weak, short-range correlated quenched disorder is relevant or irrelevant at the clean critical point, in terms of the spatial dimension $d$ and the clean correlation-length exponent $\\nu_{0}$. Then, use this criterion to predict how the finite-size scaling of the disorder-averaged zero-field susceptibility $\\chi$ at criticality is modified for large periodic cubic systems of linear size $L$.\n\nAssume the clean-system exponents are $\\nu_{0}=0.7112$ and $\\gamma_{0}=1.3895$, and the system dimension is $d=3$. Under the assumption that the Harris criterion indicates disorder irrelevance for these parameters, use standard finite-size scaling at criticality to obtain the leading power-law dependence of $\\chi(L)$ and report the single exponent controlling this growth, defined by $\\chi(L)\\sim L^{x_{\\chi}}$ for large $L$.\n\nRound your final exponent $x_{\\chi}$ to four significant figures. Express the answer without units.", "solution": "The problem is evaluated to be valid as it is scientifically grounded in the principles of statistical mechanics, specifically the theory of critical phenomena and the effects of quenched disorder. It is well-posed, objective, and contains all necessary information for a unique solution without contradictions.\n\nThe solution proceeds in two stages. First, we derive the Harris criterion for the relevance of weak quenched disorder. Second, we apply this criterion to the given system and use finite-size scaling theory to determine the scaling exponent of the susceptibility.\n\n**1. Derivation of the Harris Criterion**\n\nThe Harris criterion determines whether a clean critical point is stable against the introduction of weak quenched disorder. The derivation relies on a scaling argument that compares the fluctuations induced by disorder to the intrinsic thermal fluctuations near the critical point.\n\nLet us consider a system in $d$ spatial dimensions. In the absence of disorder (the \"clean\" system), the critical point is characterized by a set of critical exponents. We are interested in the correlation-length exponent $\\nu_{0}$. As the system approaches its critical temperature $T_{c,0}$, the correlation length $\\xi$ diverges as:\n$$\n\\xi \\sim |t|^{-\\nu_{0}}\n$$\nwhere $t = (T - T_{c,0}) / T_{c,0}$ is the reduced temperature. This relation implies that the characteristic temperature scale associated with a given length scale $\\xi$ is $|t(\\xi)| \\sim \\xi^{-1/\\nu_{0}}$.\n\nThe problem states that weak, quenched, uncorrelated disorder is introduced, coupling locally to the energy density. This can be conceptualized as causing small, random spatial fluctuations in the local critical temperature. We consider a block of the system of linear size $\\xi$, corresponding to a volume $V \\sim \\xi^d$. Since the disorder is uncorrelated and has zero mean, the Central Limit Theorem can be applied to the sum of the random perturbations within this block. The number of microscopic degrees of freedom (e.g., sites or bonds) in the block is proportional to its volume, $N \\sim \\xi^d$. The fluctuation in the total energy of the block will scale with the square root of the number of independent random variables, i.e., as $\\sqrt{N} \\sim \\xi^{d/2}$.\n\nThe fluctuation in the average energy density, and thus in the local critical temperature $T_c$ over the volume of size $\\xi^d$, is the total fluctuation divided by the volume. Let's denote this fluctuation as $\\delta T_c(\\xi)$:\n$$\n\\delta T_c(\\xi) \\sim \\frac{\\xi^{d/2}}{\\xi^d} = \\xi^{-d/2}\n$$\nThe clean critical point becomes unstable if these disorder-induced fluctuations in the critical temperature are larger than or of the same order as the intrinsic temperature scale $|t(\\xi)|$ that governs the clean critical behavior at that length scale. Disorder is therefore **relevant** if the fluctuations it causes dominate as we approach the critical point ($\\xi \\to \\infty$). The condition for relevance is:\n$$\n\\delta T_c(\\xi) \\gtrsim |t(\\xi)|\n$$\nSubstituting the scaling forms, we get:\n$$\n\\xi^{-d/2} \\gtrsim \\xi^{-1/\\nu_{0}}\n$$\nFor this inequality to hold for large $\\xi$, the exponent on the left-hand side must be smaller than or equal to the exponent on the right-hand side, leading to $-\\frac{d}{2} \\le -\\frac{1}{\\nu_{0}}$. Multiplying by $-1$ reverses the inequality:\n$$\n\\frac{d}{2} \\ge \\frac{1}{\\nu_{0}} \\implies d\\nu_{0} \\ge 2\n$$\nA more rigorous renormalization group analysis shows that the strict inequality determines the behavior. Therefore, the Harris criterion states:\n- If $d\\nu_{0} > 2$, weak quenched disorder is **irrelevant**. The critical behavior is governed by the clean fixed point, and the critical exponents are unchanged.\n- If $d\\nu_{0}  2$, weak quenched disorder is **relevant**. The critical behavior is governed by a new, disordered fixed point, and the critical exponents are modified.\n\n**2. Finite-Size Scaling of the Susceptibility**\n\nThe problem provides the parameters for the system: spatial dimension $d=3$ and the clean correlation-length exponent $\\nu_{0}=0.7112$ for the $3$-dimensional Heisenberg universality class. We first check the Harris criterion for these values:\n$$\nd\\nu_{0} = 3 \\times 0.7112 = 2.1336\n$$\nSince $2.1336 > 2$, the criterion $d\\nu_{0} > 2$ is satisfied. This confirms the problem's assumption that the disorder is irrelevant. Consequently, the critical exponents of the disordered system are the same as those of the clean system.\n\nWe now use the theory of finite-size scaling (FSS) at criticality. FSS states that for a quantity $Q$ that diverges in the thermodynamic limit ($L \\to \\infty$) as $Q \\sim |t|^{-\\rho}$, its value in a finite system of linear size $L$ at the critical point ($t=0$) scales as:\n$$\nQ(L) \\sim L^{\\rho/\\nu}\n$$\nThe quantity of interest is the zero-field magnetic susceptibility, $\\chi$. In the thermodynamic limit, its divergence is described by the exponent $\\gamma$:\n$$\n\\chi \\sim |t|^{-\\gamma}\n$$\nComparing with the general form, we have $\\rho = \\gamma$. Since disorder is irrelevant, the relevant exponents are those of the clean system, $\\gamma_{0}$ and $\\nu_{0}$. Applying the FSS formula to the susceptibility, we find its scaling with system size $L$ at criticality:\n$$\n\\chi(L) \\sim L^{\\gamma_{0}/\\nu_{0}}\n$$\nThe problem defines the scaling exponent $x_{\\chi}$ through the relation $\\chi(L) \\sim L^{x_{\\chi}}$. By direct comparison, we identify the exponent as:\n$$\nx_{\\chi} = \\frac{\\gamma_{0}}{\\nu_{0}}\n$$\nUsing the provided values for the clean $3$-dimensional Heisenberg exponents, $\\gamma_{0} = 1.3895$ and $\\nu_{0} = 0.7112$:\n$$\nx_{\\chi} = \\frac{1.3895}{0.7112} \\approx 1.9537394...\n$$\nThe problem requires the final answer to be rounded to four significant figures.\n$$\nx_{\\chi} \\approx 1.954\n$$\nThis is the exponent controlling the growth of the disorder-averaged susceptibility with system size at the critical point.", "answer": "$$ \\boxed{1.954} $$", "id": "3495495"}, {"introduction": "This practice bridges the gap between theoretical formula and practical data analysis by focusing on the correlation length $\\xi$. You will first derive a practical estimator for $\\xi$ from the static structure factor, a common output of Monte Carlo simulations. You will then apply this estimator to numerical data to test one of the most elegant predictions of finite-size scaling: the emergence of a universal ratio $\\xi/L$ at criticality for all systems within the same universality class [@problem_id:3495541].", "problem": "Consider a periodic, isotropic spin system on a two-dimensional square lattice of linear size $L$ with lattice spacing set to unity so that all lengths are dimensionless. Define the discrete Fourier transform of the spin field $s_{\\mathbf{r}}$ as $m(\\mathbf{k})$ and the static structure factor $S(\\mathbf{k})$ as the thermodynamic average $S(\\mathbf{k}) = L^2 \\langle |m(\\mathbf{k})|^2 \\rangle$. Assume translational invariance and that the two-point correlation function decays sufficiently fast so that its second moment is finite for all finite $L$. Use the discrete minimal wavevector $\\mathbf{k}_{\\min} = (2\\pi/L, 0)$ and the lattice momentum $\\hat{k}^2 = 4 \\sum_{\\alpha=1}^{2} \\sin^2(k_{\\alpha}/2)$ as the appropriate small-$\\mathbf{k}$ variable. You will work within the fundamental framework of equilibrium statistical mechanics and the canonical ensemble, Monte Carlo (MC) thermodynamic averaging, and Finite-Size Scaling (FSS).\n\nTask 1 (derivation from first principles): Starting from the definitions above and the analytic small-$\\mathbf{k}$ expansion implied by a finite second moment of the correlation function, derive an estimator for the finite-size, second-moment correlation length $\\xi(L,T)$ that uses only $S(\\mathbf{0})$ and $S(\\mathbf{k}_{\\min})$ and the lattice momentum. Your derivation must rely only on core definitions and the leading small-$\\mathbf{k}$ expansion of $S(\\mathbf{k})$ consistent with isotropy and periodic boundary conditions, without invoking any pre-packaged formula.\n\nTask 2 (algorithm design): Using your derived estimator, formulate an algorithm that, given $L$, $S(\\mathbf{0})$, and $S(\\mathbf{k}_{\\min})$ for multiple data sets, computes $\\xi(L,T)$ and the dimensionless ratio $\\xi(L,T)/L$. The trigonometric functions must use radians.\n\nTask 3 (universal ratio test at criticality): Finite-Size Scaling predicts that at the continuous phase transition temperature $T_c$ the ratio $\\xi(L,T_c)/L$ approaches a universal constant $R_\\xi$ that depends only on the universality class and boundary conditions, not on microscopic details. Use the provided test suite to numerically extract $\\xi(L,T)/L$ for three different models at their respective $T_c$, estimate the per-model mean $\\overline{R_\\xi}$ across sizes, and quantify the maximum absolute deviation between these means to test universality.\n\nTask 4 (noncritical scaling trend): For a noncritical data set from one model at $T \\neq T_c$ with a finite, $L$-independent intrinsic correlation length, use your estimator to compute $\\xi(L,T)/L$ for several $L$, and report the least-squares slope of $\\xi(L,T)/L$ as a function of $L$ to demonstrate the expected decay with $L$ away from criticality.\n\nUnits and output: All quantities are dimensionless in lattice units. Angles must be in radians. Your program must print a single line containing a comma-separated list enclosed in square brackets with the following five floating-point numbers, in this order and rounded to exactly six digits after the decimal point: $[\\overline{R_\\xi}^{(A)}, \\overline{R_\\xi}^{(B)}, \\overline{R_\\xi}^{(C)}, \\Delta_{\\max}, b_{\\text{nc}}]$, where $\\overline{R_\\xi}^{(X)}$ is the per-model mean of $\\xi/L$ at $T_c$ for model $X \\in \\{A,B,C\\}$, $\\Delta_{\\max}$ is the maximum absolute deviation among these three means, and $b_{\\text{nc}}$ is the least-squares slope of $\\xi/L$ versus $L$ for the noncritical data set.\n\nTest suite: Use the following data sets. Each entry is a tuple $(\\text{model}, \\text{is\\_critical}, L, S(\\mathbf{0}), S(\\mathbf{k}_{\\min}))$. The field $\\text{is\\_critical} \\in \\{\\text{True}, \\text{False}\\}$ indicates whether the data are at $T_c$.\n\n-   Critical, model $A$: `('A', True, 2, 25.0, 5.0)`, `('A', True, 4, 63.0, 7.0)`, `('A', True, 6, 30.0, 3.0)`\n-   Critical, model $B$: `('B', True, 2, 30.0, 6.0)`, `('B', True, 4, 99.0, 11.0)`, `('B', True, 6, 20.0, 2.0)`\n-   Critical, model $C$: `('C', True, 2, 40.0, 8.0)`, `('C', True, 4, 45.0, 5.0)`, `('C', True, 6, 90.0, 9.0)`\n-   Noncritical, model $A$ at $T \\neq T_c$: `('A', False, 2, 40.0, 4.0)`, `('A', False, 4, 22.0, 4.0)`, `('A', False, 6, 13.0, 4.0)`\n\nConstraints and coverage: The data include very small $L$ (edge conditions) and multiple models to test universality of $\\xi/L$ at $T_c$. Your implementation must strictly follow the estimator derived in Task $1$. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[r_A,r_B,r_C,\\delta,b]$), with each entry rounded to six digits after the decimal point.", "solution": "The problem requires the derivation of an estimator for the second-moment correlation length $\\xi(L,T)$ for a finite-size system, followed by its application to several data sets to test principles of finite-size scaling, namely the universality of the ratio $\\xi/L$ at criticality and its behavior away from criticality.\n\n**Task 1: Derivation of the Estimator for the Correlation Length $\\xi(L,T)$**\n\nOur derivation begins from the fundamental definition of the second-moment correlation length, $\\xi$, in a $d$-dimensional isotropic system. It is defined in terms of the two-point spatial correlation function, $G(\\mathbf{r}) = \\langle s_{\\mathbf{0}} s_{\\mathbf{r}} \\rangle$, where $s_{\\mathbf{r}}$ is the spin at site $\\mathbf{r}$. For a discrete lattice, the definition is:\n$$\n\\xi^2 = \\frac{1}{2d} \\frac{\\sum_{\\mathbf{r}} |\\mathbf{r}|^2 G(\\mathbf{r})}{\\sum_{\\mathbf{r}} G(\\mathbf{r})}\n$$\nwhere the sums are over all lattice vectors $\\mathbf{r}$ on the $L \\times L$ grid, and the dimensionality is $d=2$.\n\nThe denominator is the sum of the correlation function over all space, which is directly related to the static structure factor $S(\\mathbf{k})$ at zero wavevector. The structure factor is the discrete Fourier transform of the correlation function:\n$$\nS(\\mathbf{k}) = \\sum_{\\mathbf{r}} G(\\mathbf{r}) e^{-i \\mathbf{k} \\cdot \\mathbf{r}}\n$$\nEvaluating at $\\mathbf{k}=\\mathbf{0}$, we find that the denominator is simply $S(\\mathbf{0})$:\n$$\n\\sum_{\\mathbf{r}} G(\\mathbf{r}) = S(\\mathbf{k}=\\mathbf{0}) \\equiv S(\\mathbf{0})\n$$\nThe numerator, $\\sum_{\\mathbf{r}} |\\mathbf{r}|^2 G(\\mathbf{r})$, can be related to the curvature of $S(\\mathbf{k})$ at $\\mathbf{k}=\\mathbf{0}$. We compute the Laplacian of $S(\\mathbf{k})$ with respect to $\\mathbf{k}$:\n$$\n\\nabla_{\\mathbf{k}}^2 S(\\mathbf{k}) = \\sum_{\\alpha=1}^{d} \\frac{\\partial^2 S(\\mathbf{k})}{\\partial k_{\\alpha}^2} = \\sum_{\\alpha=1}^{d} \\sum_{\\mathbf{r}} G(\\mathbf{r}) (-i r_{\\alpha})^2 e^{-i \\mathbf{k} \\cdot \\mathbf{r}} = - \\sum_{\\mathbf{r}} G(\\mathbf{r}) \\left(\\sum_{\\alpha=1}^{d} r_{\\alpha}^2\\right) e^{-i \\mathbf{k} \\cdot \\mathbf{r}} = - \\sum_{\\mathbf{r}} G(\\mathbf{r}) |\\mathbf{r}|^2 e^{-i \\mathbf{k} \\cdot \\mathbf{r}}\n$$\nEvaluating at $\\mathbf{k}=\\mathbf{0}$:\n$$\n\\nabla_{\\mathbf{k}}^2 S(\\mathbf{k}) \\Big|_{\\mathbf{k}=\\mathbf{0}} = - \\sum_{\\mathbf{r}} |\\mathbf{r}|^2 G(\\mathbf{r})\n$$\nSubstituting these expressions for the numerator and denominator back into the definition of $\\xi^2$ yields a relationship in Fourier space:\n$$\n\\xi^2 = - \\frac{\\nabla_{\\mathbf{k}}^2 S(\\mathbf{k}) \\Big|_{\\mathbf{k}=\\mathbf{0}}}{2d S(\\mathbf{0})}\n$$\nThis exact relation motivates the small-$\\mathbf{k}$ expansion of $S(\\mathbf{k})$. A Taylor expansion of $S(\\mathbf{k})$ around $\\mathbf{k}=\\mathbf{0}$ for an isotropic system gives:\n$$\nS(\\mathbf{k}) \\approx S(\\mathbf{0}) + \\frac{1}{2} \\sum_{\\alpha,\\beta} k_{\\alpha} k_{\\beta} \\frac{\\partial^2 S}{\\partial k_{\\alpha} \\partial k_{\\beta}}\\Big|_{\\mathbf{0}} \\approx S(\\mathbf{0}) + \\frac{1}{2} k^2 \\frac{\\nabla_{\\mathbf{k}}^2 S}{d}\\Big|_{\\mathbf{0}} = S(\\mathbf{0}) \\left(1 + \\frac{k^2}{2d} \\frac{\\nabla_{\\mathbf{k}}^2 S|_{\\mathbf{0}}}{S(\\mathbf{0})}\\right) = S(\\mathbf{0})(1 - k^2 \\xi^2)\n$$\nA more robust approximation for $S(\\mathbf{k})$, known as the Ornstein-Zernike form, is a Padé approximant consistent with the Taylor expansion to leading order. For a discrete lattice, the continuum wavevector magnitude squared, $k^2$, is replaced by the lattice momentum, $\\hat{k}^2$, which is the eigenvalue of the discrete lattice Laplacian. The problem provides this as $\\hat{k}^2 = 4 \\sum_{\\alpha=1}^{2} \\sin^2(k_{\\alpha}/2)$. The Ornstein-Zernike form for the lattice is then:\n$$\nS(\\mathbf{k}) \\approx \\frac{S(\\mathbf{0})}{1 + \\xi^2 \\hat{k}^2}\n$$\nThis form is superior as it is guaranteed to be positive and often holds over a wider range of $\\mathbf{k}$. By rearranging this equation, we can derive an estimator for $\\xi^2$:\n$$\n1 + \\xi^2 \\hat{k}^2 = \\frac{S(\\mathbf{0})}{S(\\mathbf{k})} \\implies \\xi^2 = \\frac{1}{\\hat{k}^2} \\left( \\frac{S(\\mathbf{0})}{S(\\mathbf{k})} - 1 \\right)\n$$\nTo obtain an estimator for the finite-size correlation length, $\\xi(L,T)$, we evaluate this expression at the smallest non-zero wavevector available on the periodic lattice, $\\mathbf{k}_{\\min}$. The problem specifies using $\\mathbf{k}_{\\min} = (2\\pi/L, 0)$. For this wavevector, the lattice momentum is:\n$$\n\\hat{k}_{\\min}^2 = 4 \\left( \\sin^2\\left(\\frac{2\\pi/L}{2}\\right) + \\sin^2\\left(\\frac{0}{2}\\right) \\right) = 4 \\sin^2(\\pi/L)\n$$\nSubstituting this into our expression for $\\xi^2$ provides the final desired estimator:\n$$\n\\xi^2(L,T) = \\frac{1}{4 \\sin^2(\\pi/L)} \\left( \\frac{S(\\mathbf{0})}{S(\\mathbf{k}_{\\min})} - 1 \\right)\n$$\n\n**Task 2: Algorithm Design**\n\nThe derived formula directly translates into an algorithm. For a given data set comprising the linear size $L$, the structure factor at zero wavevector $S(\\mathbf{0})$, and the structure factor at the minimal wavevector $S(\\mathbf{k}_{\\min})$, the algorithm is:\n1.  Compute the lattice momentum for the minimal wavevector: $\\hat{k}_{\\min}^2 = 4 \\sin^2(\\pi/L)$, ensuring angles are in radians.\n2.  Compute the ratio of structure factors $R_S = S(\\mathbf{0}) / S(\\mathbf{k}_{\\min})$.\n3.  Calculate the squared correlation length: $\\xi^2 = (R_S - 1) / \\hat{k}_{\\min}^2$.\n4.  Take the square root to find the correlation length $\\xi(L,T) = \\sqrt{\\xi^2}$.\n5.  Compute the dimensionless ratio $\\xi(L,T)/L$.\n\n**Task 3  4: Application to Data**\n\nFor Task 3, this algorithm is applied to the data sets marked as critical ($T=T_c$). For each model ($A, B, C$), the ratio $\\xi(L,T_c)/L$ is computed for all available sizes $L$. The mean of these ratios, $\\overline{R_\\xi}$, is calculated for each model. The universality of this ratio is tested by computing the maximum absolute difference, $\\Delta_{\\max}$, between the three model-specific mean values.\n\nFor Task 4, the algorithm is applied to the noncritical data set ($T \\neq T_c$). This provides values of the ratio $\\xi(L,T)/L$ as a function of $L$. The problem posits that away from criticality, where the true correlation length is finite and largely independent of $L$, this ratio should decay with system size. This trend is quantified by computing the slope, $b_{\\text{nc}}$, of a least-squares linear fit of $\\xi(L,T)/L$ versus $L$.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import linregress\n\ndef solve():\n    \"\"\"\n    Derives and applies an estimator for the second-moment correlation length \n    to test finite-size scaling predictions.\n    \"\"\"\n\n    def calculate_xi_over_L(L, S0, Skmin):\n        \"\"\"\n        Computes the ratio xi/L based on the derived estimator.\n\n        Args:\n            L (int): Linear size of the lattice.\n            S0 (float): Static structure factor at k=0.\n            Skmin (float): Static structure factor at k_min.\n\n        Returns:\n            float: The dimensionless ratio xi/L.\n        \"\"\"\n        if L = 1:\n            # For L=1, k_min is ill-defined in this context and its denominator would be 0.\n            # The test data is for L=2.\n            return np.nan\n\n        # Calculate the lattice momentum for k_min = (2*pi/L, 0)\n        # Angles for Python's sin function are in radians as required.\n        k_min_sq = 4 * np.sin(np.pi / L)**2\n        \n        # Ensure physical and numerical validity\n        if k_min_sq == 0 or Skmin == 0:\n            return np.nan\n        \n        ratio_S = S0 / Skmin\n        if ratio_S  1:\n            # An unphysical condition implying a complex correlation length.\n            return np.nan\n            \n        # Apply the derived formula for the squared correlation length\n        xi_sq = (ratio_S - 1) / k_min_sq\n        xi = np.sqrt(xi_sq)\n        \n        return xi / L\n\n    # The test suite provided in the problem statement.\n    # Format: (model, is_critical, L, S(0), S(k_min))\n    test_cases = [\n        ('A', True, 2, 25.0, 5.0), ('A', True, 4, 63.0, 7.0), ('A', True, 6, 30.0, 3.0),\n        ('B', True, 2, 30.0, 6.0), ('B', True, 4, 99.0, 11.0), ('B', True, 6, 20.0, 2.0),\n        ('C', True, 2, 40.0, 8.0), ('C', True, 4, 45.0, 5.0), ('C', True, 6, 90.0, 9.0),\n        ('A', False, 2, 40.0, 4.0), ('A', False, 4, 22.0, 4.0), ('A', False, 6, 13.0, 4.0)\n    ]\n\n    # Data structures to store results for each task\n    critical_ratios = {'A': [], 'B': [], 'C': []}\n    noncritical_data = {'L': [], 'ratio': []}\n\n    # Process all test cases\n    for model, is_critical, L, S0, Skmin in test_cases:\n        ratio = calculate_xi_over_L(L, S0, Skmin)\n        if is_critical:\n            critical_ratios[model].append(ratio)\n        else:\n            # Only model 'A' has non-critical data in this suite\n            noncritical_data['L'].append(L)\n            noncritical_data['ratio'].append(ratio)\n\n    # Task 3: Universal ratio test at criticality\n    r_A_mean = np.mean(critical_ratios['A'])\n    r_B_mean = np.mean(critical_ratios['B'])\n    r_C_mean = np.mean(critical_ratios['C'])\n\n    # Calculate the maximum absolute deviation among the mean ratios\n    delta_max = max(\n        abs(r_A_mean - r_B_mean),\n        abs(r_A_mean - r_C_mean),\n        abs(r_B_mean - r_C_mean)\n    )\n\n    # Task 4: Noncritical scaling trend\n    # Perform a linear regression of xi/L vs L for the noncritical data.\n    # linregress returns: slope, intercept, r-value, p-value, std-err\n    slope, _, _, _, _ = linregress(noncritical_data['L'], noncritical_data['ratio'])\n    b_nc = slope\n\n    # Prepare the final list of results for printing.\n    final_results = [r_A_mean, r_B_mean, r_C_mean, delta_max, b_nc]\n    \n    # Print the final answer in the exact specified format.\n    print(f\"[{','.join(f'{x:.6f}' for x in final_results)}]\")\n\n\nsolve()\n```", "id": "3495541"}, {"introduction": "We now move to one of the most powerful and visually intuitive methods in the study of critical phenomena: data collapse. This exercise guides you through the process of scaling entire order-parameter distributions from different system sizes to collapse them onto a single, universal function [@problem_id:3495505]. By numerically optimizing the quality of this collapse, you will gain hands-on experience with a state-of-the-art technique for precisely determining critical exponents and verifying the principle of universality.", "problem": "Consider the order-parameter distribution for a finite system of linear size $L$, denoted $P_L(m)$, where $m$ is the intensive order parameter (for example, magnetization per spin). Assume equilibrium sampling in the canonical ensemble at the critical temperature $T_c$, so that $P_L(m)$ encodes thermodynamic averaging over microscopic configurations drawn from the Boltzmann distribution. Near $T_c$, the finite-size scaling hypothesis posits that there exist positive critical exponents $\\beta$ and $\\nu$, and a model-dependent nonuniversal metric factor $a_H$ for each microscopic Hamiltonian $H$, such that the family $\\{P_L(m)\\}$ obeys a scaling form when expressed in terms of the scaled variable $x = m L^{\\beta/\\nu} / a_H$. In particular, if the correct exponent ratio $\\beta/\\nu$ is used and each Hamiltonian’s metric factor $a_H$ is properly accounted for, the rescaled distributions collapse onto a universal scaling function that depends only on $x$ and not on $L$ or the microscopic details of $H$.\n\nStarting from first principles, the thermodynamic average defining $P_L(m)$ is an equilibrium probability distribution obtained by counting configurations with magnetization $m$ weighted by the Boltzmann factor $\\exp(-\\mathcal{H}/k_{\\mathrm{B}}T)$; no additional untested assumptions are to be used beyond the scaling hypothesis stated above. The goal is to numerically verify scaling collapse at $T_c$ and to estimate the exponent ratio $\\beta/\\nu$ and the relative metric factor between two different microscopic Hamiltonians believed to be in the same universality class, by minimizing a quantitative measure of miscollapse.\n\nYou are given a synthetic but scientifically consistent construction of order-parameter distributions that encode a universal scaling function with finite-size corrections. For a given Hamiltonian label $H \\in \\{A,B\\}$, a system size $L$, and a true exponent ratio $(\\beta/\\nu)_{\\mathrm{true}}$, define\n$$\nk_H(L) = \\frac{(1 + b_H/L)\\, L^{(\\beta/\\nu)_{\\mathrm{true}}}}{a_H},\n$$\nwhere $a_H0$ is the nonuniversal metric factor and $b_H$ controls a subleading analytic correction to scaling. Let the universal shape be generated by the even function\n$$\nf(x) \\propto \\exp\\!\\big(-u x^4 - v x^2\\big),\n$$\nwith fixed positive parameters $u$ and $v$. The synthetic discrete distribution on a uniform magnetization grid $m \\in [-m_{\\max}, m_{\\max}]$ with spacing $\\Delta m$ is defined by\n$$\nP_L^{(H)}(m) \\propto f\\!\\big(k_H(L)\\, m\\big),\n$$\nfollowed by normalization so that $\\sum_m P_L^{(H)}(m)\\,\\Delta m = 1$. This construction enforces a common universal shape $f(x)$ across different $H$ while allowing for different $a_H$ and finite-size corrections through $b_H$.\n\nTo perform scaling collapse for a candidate exponent ratio $\\theta$ and a candidate metric factor $\\tilde{a}_B$ for Hamiltonian $B$ (with $\\tilde{a}_A$ fixed to $1$ to set units), define the scaled variable $x = m L^{\\theta}/\\tilde{a}_H$ and the properly transformed probability density on a common $x$-grid by the probability-conserving change of variables,\n$$\n\\widetilde{P}^{(H)}_{L,\\theta}(x) = \\frac{\\tilde{a}_H}{L^{\\theta}}\\, P_L^{(H)}\\!\\left(\\frac{\\tilde{a}_H}{L^{\\theta}}\\,x\\right).\n$$\nGiven a set of distributions at various sizes $L$ and both $H \\in \\{A,B\\}$, define the collapse error for a given pair $(\\theta,\\tilde{a}_B)$ by the mean integrated square deviation from the ensemble average curve,\n$$\n\\mathcal{E}(\\theta,\\tilde{a}_B) = \\frac{1}{N_{\\mathrm{sets}}}\\sum_{j=1}^{N_{\\mathrm{sets}}}\\int_{x_{\\min}}^{x_{\\max}} \\left[\\widetilde{P}_j(x) - \\overline{P}(x)\\right]^2\\,dx,\n$$\nwhere $j$ runs over all Hamiltonian–size pairs included in the test, $\\overline{P}(x)$ is the arithmetic average of $\\widetilde{P}_j(x)$ at each $x$, and the integration domain $[x_{\\min},x_{\\max}]$ is the largest symmetric interval common to all rescaled grids in the test. The best-fit parameters are those that minimize $\\mathcal{E}(\\theta,\\tilde{a}_B)$ over a specified search domain.\n\nYour program must:\n- Generate $P_L^{(H)}(m)$ deterministically on a uniform $m$-grid using the construction above, normalize it, and then perform the scaling transform to evaluate $\\mathcal{E}(\\theta,\\tilde{a}_B)$ on a common uniform $x$-grid shared by all datasets in the test.\n- Search for a minimum of $\\mathcal{E}(\\theta,\\tilde{a}_B)$ over a rectangular grid of trial parameters.\n- For each independent test case, output three real numbers: the minimizing $\\theta^\\star$, the minimizing $\\tilde{a}_B^\\star$, and the minimized error $\\mathcal{E}^\\star = \\mathcal{E}(\\theta^\\star,\\tilde{a}_B^\\star)$.\n\nPhysical units are dimensionless in this construction. Angles are not used. Percentages are not used.\n\nUse the following fixed numerical settings for the universal shape and the magnetization grid shared by all tests:\n- $u = 0.5$, $v = 0.25$.\n- $m_{\\max} = 1.6$, $\\Delta m = 0.002$.\n\nDefine the trial search domains and sampling for parameter estimation as:\n- $\\theta \\in [0.05, 0.70]$ sampled uniformly with step $0.02$.\n- $\\tilde{a}_B \\in [0.60, 1.60]$ sampled uniformly with step $0.02$.\n- For the rescaled $x$-grid used to evaluate $\\mathcal{E}$ at a given $(\\theta,\\tilde{a}_B)$, choose the largest symmetric interval $[x_{\\min},x_{\\max}]$ contained in every dataset’s rescaled domain at that $(\\theta,\\tilde{a}_B)$, further clipped to $[-x_{\\mathrm{clip}},x_{\\mathrm{clip}}]$ with $x_{\\mathrm{clip}}=5.0$, and sample with uniform spacing $\\Delta x = 0.01$.\n\nTest suite. Run the following three tests; each test consists of two Hamiltonians $H\\in\\{A,B\\}$ and several sizes $L$, with parameters $(\\beta/\\nu)_{\\mathrm{true}}$, $a_H$, and $b_H$ as specified. In all tests, fix $\\tilde{a}_A \\equiv 1$ when estimating, so that the estimated relative scale is $\\tilde{a}_B$.\n\n- Test $1$ (two-dimensional Ising-like exponents, moderate sizes and mild corrections):\n  - $(\\beta/\\nu)_{\\mathrm{true}} = 0.125$.\n  - Sizes $L \\in \\{16, 32, 64\\}$.\n  - Hamiltonian $A$: $a_A = 1.0$, $b_A = 0.7$.\n  - Hamiltonian $B$: $a_B = 1.3$, $b_B = -0.5$.\n\n- Test $2$ (three-dimensional Ising-like exponents, moderate sizes and mild corrections):\n  - $(\\beta/\\nu)_{\\mathrm{true}} = 0.518$.\n  - Sizes $L \\in \\{14, 20, 28\\}$.\n  - Hamiltonian $A$: $a_A = 1.0$, $b_A = 0.5$.\n  - Hamiltonian $B$: $a_B = 0.85$, $b_B = -0.6$.\n\n- Test $3$ (two-dimensional Ising-like exponents, small sizes and stronger corrections):\n  - $(\\beta/\\nu)_{\\mathrm{true}} = 0.125$.\n  - Sizes $L \\in \\{8, 12\\}$.\n  - Hamiltonian $A$: $a_A = 1.0$, $b_A = 1.5$.\n  - Hamiltonian $B$: $a_B = 1.5$, $b_B = -1.2$.\n\nYour program should produce a single line of output containing all test results in a single flat list of real numbers, ordered by test and within each test as $[\\theta^\\star,\\tilde{a}_B^\\star,\\mathcal{E}^\\star]$. For example, if there are three tests, the required format is a single line with a string representation of a Python list of length $9$, namely $[\\theta^\\star_{(1)},\\tilde{a}_B^\\star_{(1)},\\mathcal{E}^\\star_{(1)},\\theta^\\star_{(2)},\\tilde{a}_B^\\star_{(2)},\\mathcal{E}^\\star_{(2)},\\theta^\\star_{(3)},\\tilde{a}_B^\\star_{(3)},\\mathcal{E}^\\star_{(3)}]$.", "solution": "The problem requires a numerical procedure to verify the finite-size scaling hypothesis and estimate critical parameters by minimizing a \"miscollapse\" error function for a set of order-parameter distributions. The core of the solution is to implement the specified error function $\\mathcal{E}(\\theta, \\tilde{a}_B)$ and find its minimum over a discrete grid of trial parameters $(\\theta, \\tilde{a}_B)$. This process is divided into three main stages: data generation, scaling transformation with error calculation, and parameter optimization.\n\nFirst, we generate the synthetic order-parameter distributions, $\\{P_L^{(H)}(m)\\}$. Each distribution is associated with a Hamiltonian label $H \\in \\{A, B\\}$ and a system size $L$. The generation follows the provided model:\n$$\nP_L^{(H)}(m) \\propto f\\big(k_H(L)\\, m\\big),\n$$\nwhere $f(x) \\propto \\exp(-u x^4 - v x^2)$ is the universal scaling function and $k_H(L) = \\frac{(1 + b_H/L)\\, L^{(\\beta/\\nu)_{\\mathrm{true}}}}{a_H}$ is a scaling factor incorporating the true critical exponent ratio $(\\beta/\\nu)_{\\mathrm{true}}$, a non-universal metric factor $a_H$, and a leading-order correction to scaling controlled by $b_H$.\nNumerically, this involves the following steps:\n1.  A uniform grid for the order parameter $m$ is defined on the interval $[-m_{\\max}, m_{\\max}]$ with spacing $\\Delta m$. The problem specifies $m_{\\max}=1.6$ and $\\Delta m=0.002$. The parameters for the universal shape are fixed at $u=0.5$ and $v=0.25$.\n2.  For each pair $(H, L)$ specified in a test case, we calculate the values of the unnormalized distribution, $f(k_H(L)\\, m)$, on the discrete $m$-grid.\n3.  The distribution is then normalized numerically. The continuous normalization condition $\\int P_L^{(H)}(m)\\, dm = 1$ is approximated by the discrete sum $\\sum_m P_L^{(H)}(m)\\, \\Delta m = 1$. The normalization constant is therefore computed as the sum of all unnormalized values on the grid multiplied by the grid spacing $\\Delta m$.\n\nSecond, we implement the scaling transformation and error evaluation. This constitutes the main computational loop, which performs a grid search over the specified domains for the trial parameters: $\\theta \\in [0.05, 0.70]$ with step $0.02$, and $\\tilde{a}_B \\in [0.60, 1.60]$ with step $0.02$. For each trial pair $(\\theta, \\tilde{a}_B)$, the following procedure is executed to compute the collapse error $\\mathcal{E}(\\theta, \\tilde{a}_B)$:\n1.  A common scaled grid for integration is constructed. The objective is to compare all distributions on a common axis for the scaled variable $x = m L^{\\theta} / \\tilde{a}_H$, where $\\tilde{a}_A$ is fixed to $1$ and $\\tilde{a}_B$ is the trial parameter. For each dataset $j$ (corresponding to a specific $H$ and $L$), the original $m$-grid $[-m_{\\max}, m_{\\max}]$ maps to a scaled interval $[-m_{\\max} L^{\\theta}/\\tilde{a}_H, m_{\\max} L^{\\theta}/\\tilde{a}_H]$. The common domain for comparison is the largest symmetric interval $[x_{\\min}, x_{\\max}]$ contained within all these individual scaled intervals. This domain is determined by the most restrictive range, i.e., $x_{\\max} = \\min_{j}(m_{\\max} L_j^{\\theta}/\\tilde{a}_{H_j})$. This common interval is then clipped to the range $[-x_{\\mathrm{clip}}, x_{\\mathrm{clip}}]$ with $x_{\\mathrm{clip}}=5.0$. The resulting final interval is sampled with a uniform spacing of $\\Delta x = 0.01$ to create the shared integration grid.\n2.  Each generated distribution $P_L^{(H)}(m)$, which is represented as a set of values on the discrete $m$-grid, is transformed into a function on this common $x$-grid. The transformation for the probability density is given by the probability conservation law:\n    $$\n    \\widetilde{P}^{(H)}_{L,\\theta}(x) = \\frac{\\tilde{a}_H}{L^{\\theta}}\\, P_L^{(H)}\\!\\left(\\frac{\\tilde{a}_H}{L^{\\theta}}\\,x\\right).\n    $$\n    To evaluate this numerically, for each point $x_i$ on the common $x$-grid, the corresponding order parameter value $m_i = x_i \\tilde{a}_H / L^{\\theta}$ is calculated. Since $m_i$ will not generally coincide with points on the original $m$-grid, we use linear interpolation on the discrete data of $P_L^{(H)}(m)$ to find its value at $m_i$. This process yields a set of rescaled distributions $\\{\\widetilde{P}_j(x)\\}$, all defined and evaluated on the same final $x$-grid.\n3.  The collapse error $\\mathcal{E}$ is computed from this set of rescaled curves. First, the arithmetic average curve $\\overline{P}(x) = (1/N_{\\mathrm{sets}}) \\sum_j \\widetilde{P}_j(x)$ is calculated at each point on the $x$-grid. Then, for each distribution $\\widetilde{P}_j(x)$, the integrated squared deviation from this average, $\\int [\\widetilde{P}_j(x) - \\overline{P}(x)]^2\\,dx$, is computed via a numerical sum over the grid points, approximating the integral as a Riemann sum with width $\\Delta x$. The final error $\\mathcal{E}(\\theta, \\tilde{a}_B)$ is the average of these integrated squared deviations over all $N_{\\mathrm{sets}}$ datasets included in the test.\n\nThird, the optimal parameters $(\\theta^\\star, \\tilde{a}_B^\\star)$ are determined. These are the parameters that minimize the error function $\\mathcal{E}(\\theta, \\tilde{a}_B)$. A straightforward grid search is performed: the error $\\mathcal{E}$ is calculated for every pair $(\\theta, \\tilde{a}_B)$ on the specified two-dimensional search grid. The pair that yields the smallest error is selected as the best-fit estimate $(\\theta^\\star, \\tilde{a}_B^\\star)$, and the corresponding minimized error is recorded as $\\mathcal{E}^\\star$.\n\nThis entire procedure is implemented in a Python program utilizing the `numpy` library for efficient array manipulation, numerical computations, and interpolation. The program processes each of the three test cases in sequence, performing the data generation and optimization routine for each one. The final results $(\\theta^\\star, \\tilde{a}_B^\\star, \\mathcal{E}^\\star)$ from all tests are collected and formatted into a single list for output, as specified by the problem.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\n# Global constants from the problem statement\nU_PARAM = 0.5\nV_PARAM = 0.25\nM_MAX = 1.6\nDM = 0.002\nTHETA_RANGE = (0.05, 0.70, 0.02)\nAB_RANGE = (0.60, 1.60, 0.02)\nX_CLIP = 5.0\nDX = 0.01\n\ndef universal_f(x):\n    \"\"\"\n    Computes the universal shape function f(x) prop. to exp(-u*x^4 - v*x^2).\n    \"\"\"\n    return np.exp(-U_PARAM * x**4 - V_PARAM * x**2)\n\ndef generate_p_dist(L, beta_nu_true, a_H, b_H):\n    \"\"\"\n    Generates a single normalized synthetic distribution P_L(m).\n    \"\"\"\n    m_grid = np.arange(-M_MAX, M_MAX + DM / 2, DM)\n\n    k_H_L = (1 + b_H / L) * L**beta_nu_true / a_H\n    \n    p_unnormalized = universal_f(k_H_L * m_grid)\n    \n    # Normalize with numerical integration: sum(P_i) * dm = 1\n    norm_factor = np.sum(p_unnormalized) * DM\n    p_normalized = p_unnormalized / norm_factor\n    \n    return m_grid, p_normalized\n\ndef calculate_collapse_error(datasets, theta, aB_try):\n    \"\"\"\n    Calculates the collapse error E(theta, aB_try) for a set of distributions.\n    \"\"\"\n    aA_try = 1.0\n\n    # 1. Determine the common rescaled x-grid\n    x_max_individual_bounds = []\n    for _, _, L, H_label in datasets:\n        aH_try = aA_try if H_label == 'A' else aB_try\n        # The m-range [-M_MAX, M_MAX] maps to x-range [-x_max, x_max]\n        x_max = M_MAX * L**theta / aH_try\n        x_max_individual_bounds.append(x_max)\n\n    # The common symmetric interval is bounded by the minimum of the individual max values.\n    common_x_max_bound = np.min(x_max_individual_bounds)\n    \n    # Clip the range\n    final_x_bound = min(common_x_max_bound, X_CLIP)\n\n    # If the common interval is too small for integration, return a large error\n    if final_x_bound = DX:\n        return np.inf\n    \n    x_integrate_grid = np.arange(-final_x_bound, final_x_bound + DX / 2, DX)\n\n    # 2. Transform each distribution to the common x-grid\n    rescaled_p_curves = []\n    for m_grid, p_dist, L, H_label in datasets:\n        aH_try = aA_try if H_label == 'A' else aB_try\n\n        # For each point x_i on the integration grid, find the corresponding m_i\n        # m = x * aH_try / L**theta\n        m_values_to_interp = x_integrate_grid * aH_try / L**theta\n        \n        # Interpolate P(m) at these new m values\n        p_interp_values = np.interp(m_values_to_interp, m_grid, p_dist)\n        \n        # Apply the probability conservation scaling factor (Jacobian of m w.r.t x)\n        # P_tilde(x) = P(m(x)) * dm/dx\n        prob_scale_factor = aH_try / L**theta\n        p_tilde_curve = prob_scale_factor * p_interp_values\n        rescaled_p_curves.append(p_tilde_curve)\n\n    # 3. Compute the mean integrated square deviation\n    all_rescaled_p = np.array(rescaled_p_curves)\n    if all_rescaled_p.shape[0]  2:\n        return 0.0 # Error is zero if there's only one curve to compare\n\n    p_bar = np.mean(all_rescaled_p, axis=0)\n    deviations = (all_rescaled_p - p_bar)**2\n    \n    # Integrate squared deviations for each curve (sum * dx)\n    integrated_sq_devs = np.sum(deviations, axis=1) * DX\n    \n    # Error is the mean of these integrated deviations\n    error = np.mean(integrated_sq_devs)\n\n    return error\n\ndef solve():\n    test_cases = [\n        {\n            \"beta_nu_true\": 0.125,\n            \"sizes\": [16, 32, 64],\n            \"params\": {'A': {'aH': 1.0, 'bH': 0.7}, 'B': {'aH': 1.3, 'bH': -0.5}}\n        },\n        {\n            \"beta_nu_true\": 0.518,\n            \"sizes\": [14, 20, 28],\n            \"params\": {'A': {'aH': 1.0, 'bH': 0.5}, 'B': {'aH': 0.85, 'bH': -0.6}}\n        },\n        {\n            \"beta_nu_true\": 0.125,\n            \"sizes\": [8, 12],\n            \"params\": {'A': {'aH': 1.0, 'bH': 1.5}, 'B': {'aH': 1.5, 'bH': -1.2}}\n        }\n    ]\n\n    theta_grid = np.arange(THETA_RANGE[0], THETA_RANGE[1] + THETA_RANGE[2]/2, THETA_RANGE[2])\n    aB_grid = np.arange(AB_RANGE[0], AB_RANGE[1] + AB_RANGE[2]/2, AB_RANGE[2])\n\n    all_results = []\n    for case in test_cases:\n        # 1. Generate all required P_L(m) distributions for this test case\n        datasets = []\n        for L in case[\"sizes\"]:\n            for H_label in ['A', 'B']:\n                params = case[\"params\"][H_label]\n                m_grid, p_dist = generate_p_dist(L, case[\"beta_nu_true\"], params['aH'], params['bH'])\n                datasets.append((m_grid, p_dist, L, H_label))\n\n        # 2. Perform grid search for (theta*, aB*) that minimizes the error\n        min_error = np.inf\n        best_params = (np.nan, np.nan, np.nan)\n\n        for theta_try in theta_grid:\n            for aB_try in aB_grid:\n                error = calculate_collapse_error(datasets, theta_try, aB_try)\n                if error  min_error:\n                    min_error = error\n                    best_params = (theta_try, aB_try, error)\n        \n        all_results.extend(best_params)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{x:.6f}' for x in all_results)}]\")\n\nsolve()\n```", "id": "3495505"}]}