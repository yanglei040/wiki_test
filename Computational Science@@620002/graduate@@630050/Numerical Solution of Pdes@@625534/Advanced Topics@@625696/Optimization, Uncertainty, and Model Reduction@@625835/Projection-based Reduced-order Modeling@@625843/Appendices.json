{"hands_on_practices": [{"introduction": "This first practice is your entry point into building a reduced-order model from first principles. You will apply the fundamental Galerkin projection to a parametrized elliptic problem, deriving the reduced system of equations and computing the operators explicitly. This exercise reinforces the core mechanics of projection and introduces the critical concept of stability, which you will assess by computing the model's uniform coercivity constant. [@problem_id:2593121]", "problem": "Consider the parametrized symmetric elliptic variational problem: find $u(\\mu) \\in V$ such that $a_{\\mu}(u(\\mu),v) = f(v)$ for all $v \\in V$, where $V = H_{0}^{1}(0,\\pi)$ and\n$$\na_{\\mu}(u,v) = \\int_{0}^{\\pi} \\left( \\mu \\, u'(x) v'(x) + u(x) v(x) \\right) \\,\\mathrm{d}x, \n\\qquad\nf(v) = \\int_{0}^{\\pi} g(x) v(x) \\,\\mathrm{d}x,\n$$\nwith parameter $\\mu \\in [\\mu_{\\min},\\mu_{\\max}]$ and $g(x) = \\sin(x)$. Let the trial space for a projection-based Reduced-Order Model (ROM) be $V_{r} = \\mathrm{span}\\{\\phi_{1},\\phi_{2}\\}$ with $\\phi_{1}(x) = \\sqrt{\\frac{2}{\\pi}} \\sin(x)$ and $\\phi_{2}(x) = \\sqrt{\\frac{2}{\\pi}} \\sin(2x)$. Choose the test space $W_{r}$ and approximate $u(\\mu)$ by $u_{r}(\\mu) = \\sum_{j=1}^{2} c_{j}(\\mu) \\phi_{j}$.\n\nStarting from the weak formulation and the definition of a projection method, impose that the residual is orthogonal to $W_{r}$ to derive the reduced linear system $A_{r}(\\mu) c(\\mu) = b_{r}(\\mu)$, where $c(\\mu) = (c_{1}(\\mu), c_{2}(\\mu))^{\\top}$. Express $A_{r}(\\mu)$ and $b_{r}(\\mu)$ strictly in terms of the bilinear and linear forms, that is $A_{r}(\\mu)_{ij} = a_{\\mu}(\\phi_{j},\\psi_{i})$ and $b_{r}(\\mu)_{i} = f(\\psi_{i})$ for a chosen basis $\\{\\psi_{i}\\}_{i=1}^{2}$ of $W_{r}$.\n\nThen, set $W_{r} = V_{r}$ (Galerkin choice) and explicitly compute $A_{r}(\\mu)$ and $b_{r}(\\mu)$. Next, from first principles based on well-posedness of variational problems, state conditions on the bilinear form $a_{\\mu}(\\cdot,\\cdot)$ and the spaces under which the Galerkin ROM (that is, $W_{r} = V_{r}$) is stable for all $\\mu \\in [\\mu_{\\min},\\mu_{\\max}]$.\n\nFinally, let the norm on $V$ be the $H^{1}$-type norm\n$$\n\\|v\\|_{V}^{2} = \\int_{0}^{\\pi} \\left( |v'(x)|^{2} + |v(x)|^{2} \\right) \\,\\mathrm{d}x,\n$$\nand restrict the parameter to $[\\mu_{\\min},\\mu_{\\max}] = \\left[\\frac{1}{2}, 2\\right]$. Define the reduced coercivity constant\n$$\n\\alpha_{r}(\\mu) = \\inf_{0 \\neq v_{r} \\in V_{r}} \\frac{a_{\\mu}(v_{r},v_{r})}{\\|v_{r}\\|_{V}^{2}},\n$$\nand its uniform counterpart\n$$\n\\alpha_{r}^{\\star} = \\inf_{\\mu \\in [\\frac{1}{2},2]} \\alpha_{r}(\\mu).\n$$\nCompute $\\alpha_{r}^{\\star}$. Your final answer must be a single real number.", "solution": "The problem asks to find an approximate solution $u_{r}(\\mu) \\in V_{r}$ to the weak formulation: find $u(\\mu) \\in V$ such that\n$$a_{\\mu}(u(\\mu),v) = f(v) \\quad \\forall v \\in V.$$\nThe approximation is given by $u_{r}(\\mu) = \\sum_{j=1}^{2} c_{j}(\\mu) \\phi_{j}$, where $V_{r} = \\mathrm{span}\\{\\phi_{1}, \\phi_{2}\\}$. A projection method imposes that the residual of the approximation is orthogonal to a test space $W_{r} = \\mathrm{span}\\{\\psi_{1}, \\psi_{2}\\}$. The residual is defined as $R(v) = f(v) - a_{\\mu}(u_{r}(\\mu), v)$. The orthogonality condition is thus\n$$a_{\\mu}(u_{r}(\\mu), \\psi_{i}) = f(\\psi_{i}), \\quad i = 1, 2.$$\nSubstituting the expression for $u_{r}(\\mu)$ and using the linearity of the bilinear form $a_{\\mu}(\\cdot, \\cdot)$ in its first argument, we obtain\n$$a_{\\mu}\\left(\\sum_{j=1}^{2} c_{j}(\\mu) \\phi_{j}, \\psi_{i}\\right) = \\sum_{j=1}^{2} c_{j}(\\mu) a_{\\mu}(\\phi_{j}, \\psi_{i}) = f(\\psi_{i}), \\quad i=1, 2.$$\nThis is a linear system of two equations for the unknown coefficients $c(\\mu) = (c_{1}(\\mu), c_{2}(\\mu))^{\\top}$. In matrix form, this is $A_{r}(\\mu) c(\\mu) = b_{r}(\\mu)$, with the matrix entries $(A_{r}(\\mu))_{ij} = a_{\\mu}(\\phi_{j}, \\psi_{i})$ and the vector entries $(b_{r}(\\mu))_{i} = f(\\psi_{i})$.\n\nWe now specialize to the Galerkin method, where the test space is identical to the trial space, $W_{r} = V_{r}$, which implies we choose $\\psi_{i} = \\phi_{i}$. We compute the reduced system matrix $A_{r}(\\mu)$ and vector $b_{r}(\\mu)$ explicitly. The basis functions are $\\phi_{1}(x) = \\sqrt{\\frac{2}{\\pi}} \\sin(x)$ and $\\phi_{2}(x) = \\sqrt{\\frac{2}{\\pi}} \\sin(2x)$. Their derivatives are $\\phi_{1}'(x) = \\sqrt{\\frac{2}{\\pi}} \\cos(x)$ and $\\phi_{2}'(x) = 2\\sqrt{\\frac{2}{\\pi}} \\cos(2x)$.\nThe matrix entries are $(A_{r}(\\mu))_{ij} = a_{\\mu}(\\phi_{j}, \\phi_{i}) = \\int_{0}^{\\pi} (\\mu \\phi_{j}'(x) \\phi_{i}'(x) + \\phi_{j}(x) \\phi_{i}(x)) \\,\\mathrm{d}x$. We require the following integrals, which follow from the properties of trigonometric functions over $[0,\\pi]$:\n$$ \\int_{0}^{\\pi} \\sin(kx)\\sin(lx)\\,\\mathrm{d}x = \\frac{\\pi}{2}\\delta_{kl}, \\quad \\int_{0}^{\\pi} \\cos(kx)\\cos(lx)\\,\\mathrm{d}x = \\frac{\\pi}{2}\\delta_{kl} \\quad (k,l \\in \\{1,2\\}), $$\n$$ \\int_{0}^{\\pi} \\sin(x)\\sin(2x)\\,\\mathrm{d}x = 0, \\quad \\int_{0}^{\\pi} \\cos(x)\\cos(2x)\\,\\mathrm{d}x = 0. $$\nThe mass matrix $M_{ij} = \\int_{0}^{\\pi} \\phi_{i}(x)\\phi_{j}(x)\\,\\mathrm{d}x$:\n$M_{11} = \\frac{2}{\\pi}\\int_{0}^{\\pi}\\sin^2(x)\\,\\mathrm{d}x = 1$, $M_{22} = \\frac{2}{\\pi}\\int_{0}^{\\pi}\\sin^2(2x)\\,\\mathrm{d}x = 1$, $M_{12}=M_{21} = 0$. So, $M = I$.\nThe stiffness matrix $K_{ij} = \\int_{0}^{\\pi} \\phi_{i}'(x)\\phi_{j}'(x)\\,\\mathrm{d}x$:\n$K_{11} = \\frac{2}{\\pi}\\int_{0}^{\\pi}\\cos^2(x)\\,\\mathrm{d}x = 1$, $K_{22} = \\frac{2}{\\pi}\\int_{0}^{\\pi}(2\\cos(2x))^2\\,\\mathrm{d}x = \\frac{8}{\\pi}\\frac{\\pi}{2}=4$, $K_{12}=K_{21}=0$.\nThus, $A_{r}(\\mu) = \\mu K + M = \\mu \\begin{pmatrix} 1 & 0 \\\\ 0 & 4 \\end{pmatrix} + \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} \\mu+1 & 0 \\\\ 0 & 4\\mu+1 \\end{pmatrix}$.\n\nThe vector entries are $(b_{r})_{i} = f(\\phi_{i}) = \\int_{0}^{\\pi} g(x)\\phi_{i}(x)\\,\\mathrm{d}x$ with $g(x) = \\sin(x)$.\n$(b_{r})_{1} = \\int_{0}^{\\pi} \\sin(x) \\sqrt{\\frac{2}{\\pi}}\\sin(x)\\,\\mathrm{d}x = \\sqrt{\\frac{2}{\\pi}}\\int_{0}^{\\pi}\\sin^2(x)\\,\\mathrm{d}x = \\sqrt{\\frac{2}{\\pi}}\\frac{\\pi}{2} = \\sqrt{\\frac{\\pi}{2}}$.\n$(b_{r})_{2} = \\int_{0}^{\\pi} \\sin(x) \\sqrt{\\frac{2}{\\pi}}\\sin(2x)\\,\\mathrm{d}x = \\sqrt{\\frac{2}{\\pi}}\\int_{0}^{\\pi}\\sin(x)\\sin(2x)\\,\\mathrm{d}x = 0$.\nSo, $b_{r} = \\begin{pmatrix} \\sqrt{\\frac{\\pi}{2}} \\\\ 0 \\end{pmatrix}$. Note that $b_r$ is independent of $\\mu$.\n\nFor the stability of the Galerkin ROM, we require the reduced linear system to have a unique solution for any right-hand side. This is equivalent to the matrix $A_{r}(\\mu)$ being invertible. Let $v_{r} = \\sum_{j=1}^{2} c_{j}\\phi_{j}$ be an arbitrary function in $V_{r}$, with coefficient vector $c = (c_{1}, c_{2})^{\\top}$. The quadratic form associated with the matrix $A_{r}(\\mu)$ is\n$$c^{\\top}A_{r}(\\mu)c = \\sum_{i=1}^{2}\\sum_{j=1}^{2} c_{i} (A_{r}(\\mu))_{ij} c_{j} = \\sum_{i=1}^{2}\\sum_{j=1}^{2} c_{i} a_{\\mu}(\\phi_{j}, \\phi_{i}) c_{j} = a_{\\mu}\\left(\\sum_{j=1}^{2}c_{j}\\phi_{j}, \\sum_{i=1}^{2}c_{i}\\phi_{i}\\right) = a_{\\mu}(v_{r},v_{r}).$$\nFrom first principles, the matrix $A_{r}(\\mu)$ is positive definite if and only if $c^{\\top}A_{r}(\\mu)c > 0$ for all $c \\neq 0$. This is equivalent to $a_{\\mu}(v_{r},v_{r}) > 0$ for all $v_{r} \\neq 0$. For stability, a stronger condition, coercivity of the bilinear form $a_{\\mu}(\\cdot,\\cdot)$ on the subspace $V_{r}$, is sufficient. This condition states that there exists a constant $\\alpha_{r}(\\mu)>0$ such that $a_{\\mu}(v_{r},v_{r}) \\ge \\alpha_{r}(\\mu) \\|v_{r}\\|_{V}^{2}$ for all $v_{r} \\in V_{r}$. If this holds, $v_r \\neq 0$ implies $\\|v_r\\|_V > 0$, which in turn implies $a_{\\mu}(v_{r},v_{r}) > 0$. The resulting positive definiteness of $A_{r}(\\mu)$ guarantees its invertibility and thus the stability of the Galerkin ROM. Uniform stability for all $\\mu \\in [\\mu_{\\min},\\mu_{\\max}]$ depends on the uniform positivity of the coercivity constant, i.e., $\\inf_{\\mu \\in [\\mu_{\\min},\\mu_{\\max}]} \\alpha_{r}(\\mu) > 0$.\n\nFinally, we compute $\\alpha_{r}^{\\star} = \\inf_{\\mu \\in [\\frac{1}{2},2]} \\alpha_{r}(\\mu)$, where $\\alpha_{r}(\\mu) = \\inf_{0 \\neq v_{r} \\in V_{r}} \\frac{a_{\\mu}(v_{r},v_{r})}{\\|v_{r}\\|_{V}^{2}}$.\nLet $v_{r}(x) = c_{1}\\phi_{1}(x) + c_{2}\\phi_{2}(x)$. The numerator is\n$a_{\\mu}(v_{r},v_{r}) = \\int_{0}^{\\pi} (\\mu (v_{r}')^2 + v_{r}^2) \\,\\mathrm{d}x = \\mu\\int_{0}^{\\pi}(c_{1}\\phi_{1}' + c_{2}\\phi_{2}')^2\\,\\mathrm{d}x + \\int_{0}^{\\pi}(c_{1}\\phi_{1} + c_{2}\\phi_{2})^2\\,\\mathrm{d}x$.\nUsing the orthogonality properties calculated earlier, this simplifies to\n$a_{\\mu}(v_{r},v_{r}) = \\mu(c_{1}^2 K_{11} + c_{2}^2 K_{22}) + (c_{1}^2 M_{11} + c_{2}^2 M_{22}) = \\mu(c_{1}^2 + 4c_{2}^2) + (c_{1}^2+c_{2}^2) = (\\mu+1)c_{1}^2 + (4\\mu+1)c_{2}^2$.\nThe denominator is the squared norm $\\|v_{r}\\|_{V}^{2} = \\int_{0}^{\\pi} ((v_{r}')^2 + v_{r}^2)\\,\\mathrm{d}x$. This corresponds to $a_{\\mu}(v_r, v_r)$ with $\\mu=1$.\n$\\|v_{r}\\|_{V}^{2} = (c_{1}^2 + 4c_{2}^2) + (c_{1}^2+c_{2}^2) = 2c_{1}^2 + 5c_{2}^2$.\nWe need to find the infimum of the ratio:\n$$ \\frac{a_{\\mu}(v_{r},v_{r})}{\\|v_{r}\\|_{V}^{2}} = \\frac{(\\mu+1)c_{1}^2 + (4\\mu+1)c_{2}^2}{2c_{1}^2 + 5c_{2}^2}. $$\nThis expression is homogeneous in $(c_1, c_2)$, so we can minimize over the variable $t = c_{2}^2/c_{1}^2 \\ge 0$. This covers all cases except $c_1=0$, which we consider separately. Let $h(t) = \\frac{(\\mu+1) + (4\\mu+1)t}{2 + 5t}$.\nTo find the infimum of $h(t)$ for $t \\ge 0$, we analyze its derivative:\n$$ h'(t) = \\frac{(4\\mu+1)(2+5t) - 5((\\mu+1)+(4\\mu+1)t)}{(2+5t)^2} = \\frac{8\\mu+2 - (5\\mu+5)}{(2+5t)^2} = \\frac{3\\mu-3}{(2+5t)^2}. $$\nThe sign of $h'(t)$ is determined by the sign of $\\mu-1$.\n- If $\\mu > 1$, $h'(t) > 0$, so $h(t)$ is increasing. The infimum is at $t=0$: $h(0) = \\frac{\\mu+1}{2}$. This corresponds to $c_2=0$.\n- If $\\mu < 1$, $h'(t) < 0$, so $h(t)$ is decreasing. The infimum is the limit as $t \\to \\infty$: $\\lim_{t\\to\\infty}h(t) = \\frac{4\\mu+1}{5}$. This corresponds to $c_1=0$.\n- If $\\mu = 1$, $h'(t) = 0$, so $h(t)$ is constant: $h(t) = \\frac{1+1}{2} = 1$.\n\nSo, the reduced coercivity constant is $\\alpha_{r}(\\mu) = \\min\\left(\\frac{\\mu+1}{2}, \\frac{4\\mu+1}{5}\\right)$.\nWe need to find $\\alpha_{r}^{\\star} = \\inf_{\\mu \\in [\\frac{1}{2},2]} \\alpha_{r}(\\mu)$. Let's analyze the two functions of $\\mu$. They are equal when $\\frac{\\mu+1}{2} = \\frac{4\\mu+1}{5} \\implies 5\\mu+5 = 8\\mu+2 \\implies 3\\mu = 3 \\implies \\mu=1$.\nFor $\\mu < 1$, $\\frac{\\mu+1}{2} > \\frac{4\\mu+1}{5}$. For $\\mu > 1$, $\\frac{\\mu+1}{2} < \\frac{4\\mu+1}{5}$.\nTherefore, $\\alpha_{r}(\\mu)$ is given by:\n$$ \\alpha_{r}(\\mu) = \\begin{cases} \\frac{4\\mu+1}{5} & \\text{if } \\mu \\in [\\frac{1}{2}, 1] \\\\ \\frac{\\mu+1}{2} & \\text{if } \\mu \\in [1, 2] \\end{cases} $$\nThe function $\\frac{4\\mu+1}{5}$ is increasing on $[\\frac{1}{2}, 1]$. The function $\\frac{\\mu+1}{2}$ is increasing on $[1, 2]$. Since $\\alpha_{r}(1)$ is continuous, the entire function $\\alpha_{r}(\\mu)$ is increasing on the interval $[\\frac{1}{2}, 2]$.\nThe infimum of an increasing function over an interval is its value at the left endpoint.\nThus, $\\alpha_{r}^{\\star} = \\alpha_{r}(\\frac{1}{2})$.\nUsing the first branch of the expression for $\\alpha_r(\\mu)$:\n$$ \\alpha_{r}^{\\star} = \\frac{4(\\frac{1}{2}) + 1}{5} = \\frac{2+1}{5} = \\frac{3}{5}. $$", "answer": "$$\\boxed{\\frac{3}{5}}$$", "id": "2593121"}, {"introduction": "While Galerkin projection is effective for linear systems, a computational bottleneck arises in the more common case of nonlinear problems. This practice introduces a solution known as hyper-reduction, specifically the Discrete Empirical Interpolation Method (DEIM), which is a cornerstone technique for efficiently approximating nonlinear terms. You will implement the core DEIM greedy algorithm to select optimal interpolation points, providing a hands-on understanding of how this clever method makes nonlinear ROMs computationally feasible. [@problem_id:3540261]", "problem": "You are tasked with deriving and implementing the Discrete Empirical Interpolation Method for efficient evaluation of nonlinear residuals arising in reduced-order models of nonlinear flow in porous media, in the context of computational geomechanics. Consider the one-dimensional steady Darcy-type flow problem on the domain $[0,1]$ with pressure-dependent permeability, where the pressure field is modeled as $p(x;\\mu) = 1 - x + \\mu \\sin(\\pi x)$, which satisfies Dirichlet boundary conditions $p(0;\\mu)=1$ and $p(1;\\mu)=0$. The permeability is $k(p) = k_0 \\exp(\\alpha p)$ with $k_0=1$ for simplicity, and $\\alpha$ is a dimensionless nonlinearity coefficient. The finite volume residual at interior grid node $i$ (with uniform mesh size $h$) is given by\n$$\nr_i(p) = -\\frac{1}{h}\\left( \\frac{2\\,k_{i+1}k_i}{k_{i+1}+k_i}\\,\\frac{p_{i+1}-p_i}{h} - \\frac{2\\,k_i k_{i-1}}{k_i+k_{i-1}}\\,\\frac{p_i-p_{i-1}}{h}\\right),\n$$\nwhere $k_i = \\exp(\\alpha p_i)$ and the harmonic means at interfaces are used. All quantities are dimensionless.\n\nYour goal is to start from the foundational idea of reduced-order modeling of nonlinear systems: approximate the nonlinear residual vector $f(\\mathbf{u})\\in\\mathbb{R}^n$ by a reduced basis $U\\in\\mathbb{R}^{n\\times m}$ built from snapshots, together with an interpolation operator $P\\in\\mathbb{R}^{n\\times m}$ that selects $m$ rows. Impose the interpolatory constraint $P^\\top f(\\mathbf{u}) = P^\\top U\\,c(\\mathbf{u})$ to ensure exact matching at selected indices, and derive that $c(\\mathbf{u}) = (P^\\top U)^{-1} P^\\top f(\\mathbf{u})$ when $P^\\top U$ is nonsingular. Explain why a greedy, discrete maximum-norm criterion can be used to select the interpolation indices that make $P^\\top U$ well-conditioned, and formulate an algorithm that, given $U$, returns the interpolation index set.\n\nThen, implement this algorithm to compute the Discrete Empirical Interpolation Method (DEIM) indices for the nonlinear residual snapshots of the above problem. Use a uniform grid with $N$ interior nodes, grid spacing $h = 1/(N+1)$, and form snapshot columns $\\mathbf{r}(\\mu_j)$ from the residual evaluated at the pressure field $p(x;\\mu_j)$ for a prescribed set of parameter values $\\{\\mu_j\\}$. Construct the reduced basis $U$ by the leading $m$ left singular vectors of the snapshot matrix (Proper Orthogonal Decomposition), and then compute the DEIM interpolation indices by the greedy algorithm that iteratively selects the row index of the largest absolute value entry of the current basis vector after orthogonal interpolation against previously selected rows.\n\nAll quantities are dimensionless and must be treated as such. The test suite below specifies three cases to cover a general scenario, a boundary case, and a nearly collinear snapshot scenario.\n\nTest Suite:\n- Case $1$ (general, moderate nonlinearity): $N=20$, $\\alpha=4.0$, $m=5$, $\\{\\mu_j\\} = [0.2,\\,0.5,\\,0.8,\\,1.1,\\,1.4]$.\n- Case $2$ (boundary case for minimal selection): $N=20$, $\\alpha=4.0$, $m=1$, $\\{\\mu_j\\} = [0.2,\\,0.5,\\,0.8,\\,1.1,\\,1.4]$.\n- Case $3$ (near-collinearity with weak nonlinearity): $N=20$, $\\alpha=0.5$, $m=3$, $\\{\\mu_j\\} = [0.01,\\,0.02,\\,0.05,\\,0.08]$.\n\nRequirements:\n- Implement the residual assembly exactly as specified, using harmonic averages for permeabilities at cell interfaces and the given pressure field $p(x;\\mu)$ on a uniform grid with Dirichlet boundary conditions embedded in the node values at $x=0$ and $x=1$.\n- Build the snapshot matrix by evaluating the residual for each $\\mu_j$ and stacking the resulting vectors as columns.\n- Compute the Proper Orthogonal Decomposition basis $U$ via the leading $m$ left singular vectors of the snapshot matrix.\n- Implement the Discrete Empirical Interpolation Method greedy algorithm to compute the interpolation indices from $U$.\n- Use zero-based indexing for indices.\n\nYour program must output a single line containing a list of three lists, each being the DEIM index set (as zero-based integers) for the corresponding case in the order Case $1$, Case $2$, Case $3$. The output format must be exactly a single line of the form\n[ [i1_1,i1_2,...,i1_m1], [i2_1,...,i2_m2], [i3_1,...,i3_m3] ]\nwith no additional text, where $i\\ell_k$ are integers. For example, the printed output must look like [[0,3,5],[2],[1,4,7]] but with the actual computed indices for this problem.", "solution": "The core task is to develop and apply the DEIM to construct an efficient reduced-order model for a nonlinear porous media flow problem. This requires a detailed explanation of the DEIM algorithm, followed by its implementation.\n\n**Theoretical Foundation of the Discrete Empirical Interpolation Method (DEIM)**\n\nReduced-order modeling (ROM) aims to replace high-dimensional, computationally expensive models with low-dimensional surrogates that are faster to evaluate. Consider a system of nonlinear ordinary differential equations, often arising from the spatial discretization of a partial differential equation (PDE), of the form $\\dot{\\mathbf{z}}(t) = A\\mathbf{z}(t) + \\mathbf{f}(\\mathbf{z}(t))$, where $\\mathbf{z} \\in \\mathbb{R}^n$ is the state vector of high dimension $n$, $A \\in \\mathbb{R}^{n \\times n}$ is a linear operator, and $\\mathbf{f}: \\mathbb{R}^n \\to \\mathbb{R}^n$ is a nonlinear function.\n\nUsing a projection-based ROM, the state is approximated in a low-dimensional subspace spanned by a reduced basis $U \\in \\mathbb{R}^{n \\times m}$ where $m \\ll n$: $\\mathbf{z}(t) \\approx U \\mathbf{z}_r(t)$, with $\\mathbf{z}_r \\in \\mathbb{R}^m$ being the reduced state vector. A Galerkin projection onto the basis $U$ yields the reduced system:\n$$\n\\dot{\\mathbf{z}}_r(t) = (U^\\top A U) \\mathbf{z}_r(t) + U^\\top \\mathbf{f}(U \\mathbf{z}_r(t))\n$$\nThe reduced linear operator $A_r = U^\\top A U \\in \\mathbb{R}^{m \\times m}$ can be precomputed, leading to an efficient evaluation of the linear term. However, the nonlinear term $U^\\top \\mathbf{f}(U \\mathbf{z}_r(t))$ remains a computational bottleneck. Its evaluation requires first forming the full-order state $U \\mathbf{z}_r(t) \\in \\mathbb{R}^n$, then evaluating the nonlinear function $\\mathbf{f}$ at this state (a cost dependent on $n$), and finally projecting the result back to the reduced space. This $O(n)$ complexity defeats the purpose of ROM.\n\nThe DEIM provides a solution to this problem, a technique often termed hyper-reduction. The central idea is to also approximate the nonlinear vector $\\mathbf{f}(\\mathbf{u})$ (using $\\mathbf{u}$ as a generic state vector) in a low-dimensional subspace. This subspace is spanned by a basis, which we will call $U$ as per the problem statement, though it is crucial to note this basis is for the nonlinear term, not necessarily the state itself. The basis $U \\in \\mathbb{R}^{n \\times m}$ is typically constructed via POD (i.e., the leading left singular vectors) from a collection of snapshots of the nonlinear term, $\\{\\mathbf{f}(\\mathbf{u}_1), \\mathbf{f}(\\mathbf{u}_2), \\dots\\}$. The approximation is thus:\n$$\n\\mathbf{f}(\\mathbf{u}) \\approx U \\mathbf{c}(\\mathbf{u})\n$$\nwhere $\\mathbf{c}(\\mathbf{u}) \\in \\mathbb{R}^m$ is a vector of coefficients. The challenge is to find $\\mathbf{c}(\\mathbf{u})$ efficiently. A Galerkin projection, $U^\\top \\mathbf{f}(\\mathbf{u}) = (U^\\top U) \\mathbf{c}(\\mathbf{u})$, still requires evaluating the full vector $\\mathbf{f}(\\mathbf{u})$.\n\nDEIM circumvents this by replacing the projection with interpolation. It enforces that the approximation be exact at a small, carefully selected set of $m$ indices, $\\{s_1, s_2, \\dots, s_m\\}$. Let $P \\in \\mathbb{R}^{n \\times m}$ be a matrix that selects these rows, constructed as $P = [\\mathbf{e}_{s_1}, \\mathbf{e}_{s_2}, \\dots, \\mathbf{e}_{s_m}]$, where $\\mathbf{e}_{s_j}$ is the canonical basis vector with a $1$ at position $s_j$. The interpolatory constraint is:\n$$\nP^\\top \\mathbf{f}(\\mathbf{u}) = P^\\top (U \\mathbf{c}(\\mathbf{u})) = (P^\\top U) \\mathbf{c}(\\mathbf{u})\n$$\nIf the matrix $(P^\\top U) \\in \\mathbb{R}^{m \\times m}$ is nonsingular, one can solve for the coefficients:\n$$\n\\mathbf{c}(\\mathbf{u}) = (P^\\top U)^{-1} P^\\top \\mathbf{f}(\\mathbf{u})\n$$\nThis is the derivation requested. The computational efficiency arises because evaluating $P^\\top \\mathbf{f}(\\mathbf{u})$ only requires computing the $m$ components of the nonlinear function $\\mathbf{f}$ corresponding to the selected indices, not all $n$ components. The full DEIM approximation is then:\n$$\n\\mathbf{f}(\\mathbf{u}) \\approx U (P^\\top U)^{-1} P^\\top \\mathbf{f}(\\mathbf{u})\n$$\nThe matrices $U$ and $(P^\\top U)^{-1}$ are computed offline. The online evaluation cost is dominated by computing $m$ components of $\\mathbf{f}$ and a matrix-vector product of size $n \\times m$, which is substantially less than the original $O(n)$ cost.\n\n**The Greedy Algorithm for Index Selection**\n\nThe invertibility and conditioning of the matrix $(P^\\top U)$ are paramount. DEIM employs a greedy algorithm to select the interpolation indices $\\{s_1, \\dots, s_m\\}$ to ensure $(P^\\top U)$ is well-conditioned. This procedure is conceptually analogous to LU factorization with partial pivoting, where at each step, the pivot element is chosen to have the largest possible magnitude to control error propagation.\n\nGiven the orthonormal basis $U = [\\mathbf{u}_1, \\mathbf{u}_2, \\dots, \\mathbf{u}_m] \\in \\mathbb{R}^{n \\times m}$, the algorithm proceeds as follows:\n\n$1$. Select the first basis vector $\\mathbf{u}_1$. Find the index $s_1$ where its magnitude is maximal. This index becomes the first interpolation point.\n$$\ns_1 = \\arg\\max_{i \\in \\{1, \\dots, n\\}} |\\mathbf{u}_1^{(i)}|\n$$\nLet $\\mathcal{S} = \\{s_1\\}$.\n\n$2$. For $k = 2, \\dots, m$:\n   a. Take the next basis vector, $\\mathbf{u}_k$.\n   b. Find its best approximation in the span of the previously selected basis vectors, $\\{\\mathbf{u}_1, \\dots, \\mathbf{u}_{k-1}\\}$, using the interpolation points already chosen, $\\mathcal{S}_{k-1} = \\{s_1, \\dots, s_{k-1}\\}$. We solve for coefficients $\\mathbf{c} \\in \\mathbb{R}^{k-1}$ from the interpolatory constraint:\n   $$\n   P_{k-1}^\\top \\mathbf{u}_k = (P_{k-1}^\\top U_{k-1}) \\mathbf{c} \\implies \\mathbf{c} = (P_{k-1}^\\top U_{k-1})^{-1} P_{k-1}^\\top \\mathbf{u}_k\n   $$\n   where $U_{k-1} = [\\mathbf{u}_1, \\dots, \\mathbf{u}_{k-1}]$ and $P_{k-1}$ selects rows from $\\mathcal{S}_{k-1}$.\n   c. Compute the interpolation error, or residual, vector $\\mathbf{r}_k$:\n   $$\n   \\mathbf{r}_k = \\mathbf{u}_k - U_{k-1}\\mathbf{c}\n   $$\n   d. The next interpolation index $s_k$ is chosen as the one where the magnitude of this residual is maximal. This is the point where the current approximation is worst, and thus where new information is most needed.\n   $$\n   s_k = \\arg\\max_{i \\in \\{1, \\dots, n\\}} |\\mathbf{r}_k^{(i)}|\n   $$\n   e. Add the new index to the set: $\\mathcal{S}_k = \\mathcal{S}_{k-1} \\cup \\{s_k\\}$.\n\n$3$. The final set of interpolation indices is $\\mathcal{S}_m = \\{s_1, \\dots, s_m\\}$. By iteratively maximizing the component of the residual, the algorithm ensures that the columns of $P^\\top U$ are as linearly independent as possible, leading to a well-conditioned matrix.\n\n**Implementation for the Porous Media Problem**\n\nWe now apply this framework to the given problem.\n\n$1$. **Discretization and State Representation**: The domain $[0,1]$ is discretized with $N$ interior nodes and $2$ boundary nodes, totaling $N+2$ points. The grid spacing is $h=1/(N+1)$. The spatial coordinates are $x_j = j h$ for $j=0, 1, \\dots, N+1$. The pressure field $p(x;\\mu) = 1 - x + \\mu \\sin(\\pi x)$ is evaluated at these nodes for each parameter $\\mu_j$ from the test suite.\n\n$2$. **Snapshot Generation**: For each $\\mu_j$, we compute the nonlinear residual vector $\\mathbf{r}(\\mu_j) \\in \\mathbb{R}^N$. The components $r_i$ for interior nodes $i=1, \\dots, N$ are given by the finite volume formula:\n$$\nr_i(p) = -\\frac{1}{h}\\left( T_{i+1/2}\\,\\frac{p_{i+1}-p_i}{h} - T_{i-1/2}\\,\\frac{p_i-p_{i-1}}{h}\\right)\n$$\nwhere $T_{i+1/2}$ is the intercell transmissibility, calculated using the harmonic mean of permeabilities $k_i = \\exp(\\alpha p_i)$:\n$$\nT_{i \\pm 1/2} = \\frac{2\\,k_{i \\pm 1}k_i}{k_{i \\pm 1}+k_i}\n$$\nThe pressure values $p_0=1$ and $p_{N+1}=0$ from the Dirichlet boundary conditions are used at the domain ends. The snapshot matrix $R$ is formed by stacking these residual vectors as columns: $R=[\\mathbf{r}(\\mu_1), \\mathbf{r}(\\mu_2), \\dots]$.\n\n$3$. **POD Basis Construction**: A singular value decomposition (SVD) of the snapshot matrix, $R = U \\Sigma V^\\top$, yields the POD modes. The reduced basis for the nonlinear term is formed by the first $m$ left singular vectors, which are the columns of $U \\in \\mathbb{R}^{N \\times m}$.\n\n$4$. **DEIM Index Computation**: The greedy algorithm described above is applied to this basis $U$ to compute the $m$ DEIM interpolation indices. The implementation will use zero-based indexing, so the indices will range from $0$ to $N-1$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef compute_deim_indices(N, alpha, m, mu_values):\n    \"\"\"\n    Computes the DEIM indices for the given porous media problem.\n\n    Args:\n        N (int): Number of interior grid nodes.\n        alpha (float): Nonlinearity coefficient for permeability.\n        m (int): Number of basis vectors / interpolation points.\n        mu_values (list of float): List of parameters for snapshot generation.\n\n    Returns:\n        list of int: The computed zero-based DEIM indices.\n    \"\"\"\n    if m == 0:\n        return []\n\n    # 1. Grid setup\n    h = 1.0 / (N + 1)\n    x_full = np.linspace(0.0, 1.0, N + 2)\n\n    # 2. Snapshot matrix generation\n    snapshots = []\n    for mu in mu_values:\n        # Evaluate pressure and permeability on the full grid (N+2 nodes)\n        p_vec = 1.0 - x_full + mu * np.sin(np.pi * x_full)\n        k_vec = np.exp(alpha * p_vec)\n\n        # Compute the residual vector for the N interior nodes\n        r_vec = np.zeros(N)\n        for i in range(N):\n            # i is the 0-based index for the residual vector,\n            # corresponding to interior node i+1.\n            # We need values from grid nodes i, i+1, and i+2.\n            p_im1, p_i, p_ip1 = p_vec[i], p_vec[i+1], p_vec[i+2]\n            k_im1, k_i, k_ip1 = k_vec[i], k_vec[i+1], k_vec[i+2]\n\n            # Harmonic mean for internodal permeability\n            k_right = (2.0 * k_ip1 * k_i) / (k_ip1 + k_i)\n            k_left = (2.0 * k_i * k_im1) / (k_i + k_im1)\n            \n            # Fluxes\n            flux_right = k_right * (p_ip1 - p_i) / h\n            flux_left = k_left * (p_i - p_im1) / h\n\n            # Residual (Finite Volume formulation)\n            r_vec[i] = -(1.0 / h) * (flux_right - flux_left)\n        \n        snapshots.append(r_vec)\n\n    R = np.array(snapshots).T\n    if R.shape[1] < m:\n        # If number of snapshots is less than desired basis size,\n        # reduce basis size to number of snapshots.\n        # The rank of R is at most min(N, len(mu_values)).\n        m = R.shape[1]\n\n    # 3. POD basis construction (via SVD)\n    U, s, Vt = np.linalg.svd(R, full_matrices=False)\n    U_m = U[:, :m]\n\n    # 4. DEIM greedy algorithm\n    indices = []\n    \n    # First index\n    u1 = U_m[:, 0]\n    p1 = np.argmax(np.abs(u1))\n    indices.append(p1)\n\n    # Subsequent indices\n    for k in range(1, m):\n        u_k = U_m[:, k]\n        \n        # Assemble sub-problem to find coefficients 'c'\n        U_prev = U_m[:, :k]\n        P_T_U_prev = U_prev[indices, :]\n        P_T_u_k = u_k[indices]\n        \n        # Solve c = (P^T U_{k-1})^{-1} (P^T u_k)\n        try:\n            c = np.linalg.solve(P_T_U_prev, P_T_u_k)\n        except np.linalg.LinAlgError:\n            # Fallback for singular matrix, although DEIM is designed to avoid this.\n            c = np.linalg.lstsq(P_T_U_prev, P_T_u_k, rcond=None)[0]\n\n        # Compute residual r = u_k - U_{k-1} c\n        residual = u_k - U_prev @ c\n        \n        # Find index of max absolute value of residual\n        pk = np.argmax(np.abs(residual))\n        indices.append(pk)\n        \n    return [int(i) for i in indices]\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print the results.\n    \"\"\"\n    test_cases = [\n        # Case 1 (general, moderate nonlinearity)\n        {'N': 20, 'alpha': 4.0, 'm': 5, 'mu_values': [0.2, 0.5, 0.8, 1.1, 1.4]},\n        # Case 2 (boundary case for minimal selection)\n        {'N': 20, 'alpha': 4.0, 'm': 1, 'mu_values': [0.2, 0.5, 0.8, 1.1, 1.4]},\n        # Case 3 (near-collinearity with weak nonlinearity)\n        {'N': 20, 'alpha': 0.5, 'm': 3, 'mu_values': [0.01, 0.02, 0.05, 0.08]}\n    ]\n    \n    all_results = []\n    for case in test_cases:\n        indices = compute_deim_indices(\n            case['N'], case['alpha'], case['m'], case['mu_values']\n        )\n        all_results.append(indices)\n        \n    # Final print statement in the exact required format without spaces\n    output_str = repr(all_results).replace(\" \", \"\")\n    print(output_str)\n\nsolve()\n```", "id": "3540261"}, {"introduction": "A key challenge for ROMs is ensuring their accuracy, especially when a system's behavior evolves in ways not fully captured by a pre-computed basis. This advanced practice guides you through implementing an online adaptive ROM that monitors its own error during a time-dependent simulation. You will use a residual-based *a posteriori* error estimator to decide when to enrich the basis, transforming a static ROM into a dynamic and certifiably reliable predictive tool. [@problem_id:3435653]", "problem": "Consider the linear parabolic Partial Differential Equation (PDE) in one spatial dimension with homogeneous Dirichlet boundary conditions and a time-dependent source, discretized in space by central finite differences on a uniform grid, and in time by backward Euler. Let the spatial domain be the interval $\\left[0,1\\right]$ with $N$ interior points, grid spacing $h = \\frac{1}{N+1}$, and grid points $x_i = i h$ for $i = 1,\\dots,N$. Let the diffusion coefficient be $\\nu > 0$. The semi-discrete (in space) representation using the standard second-order centered difference Laplacian with homogeneous Dirichlet boundary conditions yields a system of ordinary differential equations in the unknown vector $u(t) \\in \\mathbb{R}^N$ of the form\n$$\n\\frac{d}{dt} u(t) = \\nu L u(t) + s(t),\n$$\nwhere $L \\in \\mathbb{R}^{N \\times N}$ is the usual finite-difference Laplacian operator with stencil $\\left[\\frac{1}{h^2}, -\\frac{2}{h^2}, \\frac{1}{h^2}\\right]$ and zeros at the first and last rows' off-boundary entries due to homogeneous Dirichlet boundary conditions, and $s(t) \\in \\mathbb{R}^N$ is the vector of the source term sampled at the grid points. The backward Euler time discretization with time step $\\Delta t > 0$ leads to the linear system at each integer time index $k \\ge 1$\n$$\n\\left(\\frac{1}{\\Delta t} I - \\nu L\\right) u^k = \\frac{1}{\\Delta t} u^{k-1} + s^k,\n$$\nwhere $I$ is the identity matrix, $u^k \\approx u(t_k)$ with $t_k = k \\Delta t$, and $s^k \\approx s(t_k)$.\n\nDefine the symmetric positive definite matrix\n$$\nA := -\\nu L \\in \\mathbb{R}^{N \\times N}, \\quad B := \\frac{1}{\\Delta t} I + A \\in \\mathbb{R}^{N \\times N}.\n$$\nLet the Reduced Order Model (ROM) approximate $u^k$ in a trial subspace of dimension $r \\ll N$ spanned by the columns of a basis matrix $V_r \\in \\mathbb{R}^{N \\times r}$ with $V_r^T V_r = I_r$. The Galerkin projection of the backward Euler scheme onto $\\text{span}(V_r)$ yields the reduced system for coefficients $a^k \\in \\mathbb{R}^r$\n$$\n\\left(\\frac{1}{\\Delta t} I_r + V_r^T A V_r\\right) a^k = \\frac{1}{\\Delta t} a^{k-1} + V_r^T s^k.\n$$\nThe ROM state is $u_r^k = V_r a^k$. The corresponding full residual vector at time step $k$ is defined by\n$$\nr(u_r^k) := \\frac{1}{\\Delta t} u_r^{k-1} + s^k - \\left(\\frac{1}{\\Delta t} I + A\\right) u_r^k = \\frac{1}{\\Delta t} u_r^{k-1} + s^k - B u_r^k \\in \\mathbb{R}^N.\n$$\n\nYou are asked to implement an online error control strategy for the ROM time integrator based on a projection residual dual norm and incremental basis enrichment.\n\nTasks:\n- Start from the fundamental notion of Galerkin projection for linear coercive problems and the backward Euler scheme to derive a computable a posteriori error bound per time step in terms of the dual norm of $r(u_r^k)$ with respect to the symmetric positive definite operator $B$. Let the dual norm be defined by\n$$\n\\|r\\|_{B^{-1}} := \\sqrt{r^T B^{-1} r}.\n$$\n- Design a stopping criterion for accepting the ROM solution at time step $k$ based on a certified a posteriori bound that is tied to $\\|r(u_r^k)\\|_{B^{-1}}$. The acceptance criterion at time step $k$ must be: accept if $\\|r(u_r^k)\\|_{B^{-1}} \\le \\tau$, where $\\tau > 0$ is a tolerance. If not accepted, perform an incremental basis update by appending a new direction computed online, and recompute the ROM solution at the same time step, iterating until acceptance or until the basis size reaches a prescribed maximum $r_{\\max}$.\n- The online enrichment direction at time step $k$ must be chosen in a manner justified by the error equation. Use the Riesz representative of the residual in the $B$-inner product, namely the vector $w^k$ solving $B w^k = r(u_r^k)$. Orthogonalize $w^k$ against the current basis columns in the Euclidean inner product, normalize it, append it to $V_r$, and update the reduced operators. Recompute $a^{k-1}$ as $V_r^T u_r^{k-1}$ in the new basis before resolving for $a^k$.\n- The initial reduced basis must be formed by the first $r_0$ discrete sine modes corresponding to homogeneous Dirichlet boundary conditions, namely columns with entries $v_j(i) = \\sin\\left(\\frac{j \\pi i}{N+1}\\right)$ for $j = 1,\\dots,r_0$ and $i = 1,\\dots,N$, orthonormalized in the Euclidean inner product. The initial ROM state $u_r^0$ must be the projection of the initial condition onto $\\text{span}(V_{r_0})$.\n\nSet the data for the full-order model as follows:\n- Spatial dimension: $N = 80$ interior grid points.\n- Diffusion coefficient: $\\nu = 10^{-2}$.\n- Final time: $T = 10^{-1}$.\n- Time step: $\\Delta t = 2 \\cdot 10^{-3}$, thus $K = \\frac{T}{\\Delta t} = 50$ time steps.\n- Initial condition: $u^0(x) = \\sin(\\pi x)$, sampled at the grid points.\n- Source term: $s(x,t) = \\sin(5 \\pi x) e^{-t}$, sampled at the grid points and evaluated at $t_k = k \\Delta t$.\n\nTest suite specifications:\nImplement the described online-adaptive ROM integrator and run it for the following three test cases, each specified by a triple $(r_0, \\tau, r_{\\max})$:\n- Case A (happy path): $r_0 = 1$, $\\tau = 10^{-4}$, $r_{\\max} = 20$.\n- Case B (strict tolerance): $r_0 = 2$, $\\tau = 10^{-5}$, $r_{\\max} = 25$.\n- Case C (tight capacity edge case): $r_0 = 3$, $\\tau = 10^{-6}$, $r_{\\max} = 8$.\n\nFor each case, report:\n- The final reduced dimension $r_{\\text{final}}$ after completing all $K$ time steps with the online adaptation rule.\n- The maximal accepted a posteriori estimator $\\max_{1 \\le k \\le K} \\|r(u_r^k)\\|_{B^{-1}}$ over all accepted time steps.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order\n$$\n\\left[r_{\\text{final},A}, \\max\\_k \\|r(u_{r,A}^k)\\|_{B^{-1}}, r_{\\text{final},B}, \\max\\_k \\|r(u_{r,B}^k)\\|_{B^{-1}}, r_{\\text{final},C}, \\max\\_k \\|r(u_{r,C}^k)\\|_{B^{-1}}\\right],\n$$\nwhere the subscripts $A,B,C$ refer to the three cases. Each $r_{\\text{final}}$ is an integer and each maximal estimator value is a floating-point number. The line must contain exactly these six values in this order, with no additional text.\n\nYour implementation must be fully self-contained and deterministic, using only the specified numerical parameters and the online algorithm described. The numerical linear algebra must solve linear systems with $B$ robustly. Any orthonormalization must use the Euclidean inner product.", "solution": "### 1. Derivation of the A Posteriori Error Estimator\n\nThe time-dependent problem is discretized using backward Euler, leading to a linear system to be solved at each time step $k \\ge 1$:\n$$ \\left(\\frac{1}{\\Delta t} I - \\nu L\\right) u^k = \\frac{1}{\\Delta t} u^{k-1} + s^k $$\nUsing the provided definitions $A := -\\nu L$ and $B := \\frac{1}{\\Delta t} I + A$, the full-order model (FOM) equation is concisely written as:\n$$ B u^k = \\frac{1}{\\Delta t} u^{k-1} + s^k $$\nHere, $u^k \\in \\mathbb{R}^N$ is the FOM solution at time $t_k$. The matrix $B$ is symmetric and positive definite (SPD) because $A$ is SPD (since $L$ is the negative definite discrete Laplacian and $\\nu > 0$) and $\\frac{1}{\\Delta t}I$ is a positive definite diagonal matrix.\n\nThe ROM solution $u_r^k$ is used to define the full-order residual vector $r(u_r^k)$:\n$$ r(u_r^k) := \\frac{1}{\\Delta t} u_r^{k-1} + s^k - B u_r^k $$\nThis residual quantifies the extent to which the ROM solution fails to satisfy the FOM equation.\n\nTo relate this residual to the true error $e^k := u^k - u_r^k$, we derive the error evolution equation. We subtract the rearranged residual definition, $\\frac{1}{\\Delta t} u_r^{k-1} + s^k = B u_r^k + r(u_r^k)$, from the FOM equation:\n$$ B u^k - (B u_r^k + r(u_r^k)) = \\frac{1}{\\Delta t} u^{k-1} - \\frac{1}{\\Delta t} u_r^{k-1} $$\n$$ B (u^k - u_r^k) - r(u_r^k) = \\frac{1}{\\Delta t} (u^{k-1} - u_r^{k-1}) $$\n$$ B e^k = \\frac{1}{\\Delta t} e^{k-1} + r(u_r^k) $$\nThis equation links the error at step $k$ to the error from the previous step, $e^{k-1}$, and the residual generated at the current step.\n\nFor a single-step a posteriori error bound, we are interested in the error generated *within* step $k$. This is commonly estimated by assuming the solution from the previous step was exact, i.e., $u_r^{k-1} = u^{k-1}$, which implies $e^{k-1} = 0$. Under this assumption, the error equation simplifies to:\n$$ B e^k = r(u_r^k) \\implies e^k = B^{-1} r(u_r^k) $$\nTo measure this error, we use the natural energy norm induced by the SPD operator $B$, defined by $\\|x\\|_B := \\sqrt{x^T B x}$. The squared norm of the error is:\n$$ \\|e^k\\|_B^2 = (e^k)^T B e^k = (B^{-1} r(u_r^k))^T B (B^{-1} r(u_r^k)) $$\nSince $B$ is symmetric, its inverse $B^{-1}$ is also symmetric. Therefore, $(B^{-1} r(u_r^k))^T = r(u_r^k)^T (B^{-1})^T = r(u_r^k)^T B^{-1}$. Substituting this back, we get:\n$$ \\|e^k\\|_B^2 = r(u_r^k)^T B^{-1} B B^{-1} r(u_r^k) = r(u_r^k)^T B^{-1} r(u_r^k) $$\nThis is precisely the squared dual norm of the residual, $\\|r(u_r^k)\\|_{B^{-1}}^2$. Thus, we establish the key identity:\n$$ \\|e^k\\|_B = \\|r(u_r^k)\\|_{B^{-1}} $$\nThis equality provides the theoretical justification for using the computable quantity $\\|r(u_r^k)\\|_{B^{-1}}$ as an a posteriori estimator for the error introduced at time step $k$.\n\n### 2. Algorithmic Design and Implementation\n\nThe online-adaptive algorithm executes a loop over time steps. Within each time step, an inner loop performs basis adaptation until the error estimator is below a tolerance $\\tau$ or a maximum basis size $r_{\\max}$ is reached.\n\n**Initialization:**\n1.  **FOM Setup**: The grid, a uniform set of $N$ interior points, is defined. The finite difference Laplacian $L$, the matrix $A = -\\nu L$, and the time-stepping matrix $B = \\frac{1}{\\Delta t}I + A$ are constructed. Since $B$ is constant and is used in solves at every adaptive step, we compute its Cholesky factorization, $B = C^T C$, once at the beginning for efficiency.\n2.  **Initial ROM**: The initial basis $V_{r_0}$ of size $r_0$ is formed by taking the first $r_0$ discrete sine modes, which are eigenvectors of $L$, and orthonormalizing them. The initial condition $u^0$ is projected onto this basis to obtain the initial ROM state $u_r^0$. For this problem, $u^0(x) = \\sin(\\pi x)$ corresponds to the first sine mode, so if $r_0 \\ge 1$, the projection is exact, i.e., $u_r^0 = u^0$.\n\n**Time Integration with Online Adaptation (for each step $k=1, \\dots, K$):**\nAn inner `while` loop is used to manage the adaptation at each time step $k$. Let $u_r^{k-1}$ be the accepted ROM solution from the previous step.\n\n1.  **Solve Reduced System**:\n    - Project the operator $A$ onto the current basis $V_r$: $A_r = V_r^T A V_r$.\n    - Form the reduced system matrix $B_r = \\frac{1}{\\Delta t} I_r + A_r$.\n    - Project the source term $s^k$ and the previous solution's contribution: $s_r^k = V_r^T s^k$ and a coefficient vector $a^{k-1} = V_r^T u_r^{k-1}$.\n    - Solve the $r \\times r$ linear system for the new coefficients $a^k$: $B_r a^k = \\frac{1}{\\Delta t} a^{k-1} + s_r^k$.\n    - Reconstruct the full-space ROM solution: $u_r^k = V_r a^k$.\n\n2.  **Compute Error Estimator**:\n    - Compute the full-order residual: $r_k = \\frac{1}{\\Delta t} u_r^{k-1} + s^k - B u_r^k$.\n    - To compute the estimator $\\eta_k = \\|r_k\\|_{B^{-1}} = \\sqrt{r_k^T B^{-1} r_k}$, we first solve the $N \\times N$ system $B z_k = r_k$ for $z_k = B^{-1} r_k$. This is done efficiently using the pre-computed Cholesky factorization of $B$.\n    - The estimator is then $\\eta_k = \\sqrt{r_k^T z_k}$.\n\n3.  **Decision and Adaptation**:\n    - **Accept**: If $\\eta_k \\le \\tau$ or the basis size has reached its limit ($r \\ge r_{\\max}$), the solution $u_r^k$ is accepted. This $u_r^k$ becomes $u_r^{k-1}$ for the next time step, and the inner loop terminates. The value of $\\eta_k$ is recorded as the accepted estimator for this time step.\n    - **Reject & Enrich**: If $\\eta_k > \\tau$ and $r < r_{\\max}$, the solution is rejected, and the basis is enriched.\n        - The enrichment vector is chosen as $w_k = z_k = B^{-1} r_k$, which is the Riesz representation of the residual. This direction is known to be effective for error reduction.\n        - $w_k$ is orthogonalized against the current basis columns $V_r$ using the Euclidean inner product (Gram-Schmidt process): $\\tilde{w}_k = w_k - V_r(V_r^T w_k)$.\n        - The resulting vector $\\tilde{w}_k$ is normalized to produce $\\hat{w}_k$.\n        - The basis is augmented: $V_{r+1} = [V_r, \\hat{w}_k]$.\n        - The inner loop continues from Step 1 with the larger basis at the same time step $k$.\n\nThis process repeats until a satisfactory solution is found for all $K$ time steps. For each test case, the final basis size and the maximum of all accepted estimator values are reported.", "answer": "```python\nimport numpy as np\nfrom scipy.linalg import cho_factor, cho_solve\n\ndef run_simulation(N, nu, T, dt, r0, tau, r_max):\n    \"\"\"\n    Runs the online-adaptive ROM simulation for one test case.\n\n    Args:\n        N (int): Number of interior spatial grid points.\n        nu (float): Diffusion coefficient.\n        T (float): Final time.\n        dt (float): Time step size.\n        r0 (int): Initial reduced basis size.\n        tau (float): Tolerance for the a posteriori error estimator.\n        r_max (int): Maximum allowed reduced basis size.\n\n    Returns:\n        tuple: A tuple containing:\n            - r_final (int): The final size of the reduced basis.\n            - max_estimator (float): The maximum accepted estimator value over all time steps.\n    \"\"\"\n    # 1. Setup Full-Order Model (FOM)\n    h = 1.0 / (N + 1)\n    x = np.linspace(h, 1.0 - h, N)\n\n    # Construct the 1D discrete Laplacian matrix L with homogeneous Dirichlet BCs\n    main_diag_L = -2.0 / h**2 * np.ones(N)\n    off_diag_L = 1.0 / h**2 * np.ones(N - 1)\n    L = np.diag(main_diag_L) + np.diag(off_diag_L, k=1) + np.diag(off_diag_L, k=-1)\n    \n    # Define system matrices A and B\n    A = -nu * L\n    B = (1.0 / dt) * np.eye(N) + A\n    \n    # B is symmetric positive definite; pre-compute its Cholesky factorization\n    # for efficient solves of the form B*z = r.\n    B_cholesky_factor = cho_factor(B, lower=False)\n    \n    # 2. Setup Reduced-Order Model (ROM)\n    # Initial basis V from the first r0 orthonormalized discrete sine modes\n    V = np.zeros((N, r0))\n    norm_factor = np.sqrt(2.0 / (N + 1)) # Normalization factor is sqrt(2/(N+1))\n    for j in range(1, r0 + 1):\n        # The correct formula for orthonormal discrete sine transform vectors is sqrt(2/(N+1))*sin(...)\n        V[:, j - 1] = norm_factor * np.sin(j * np.pi * x)\n    \n    # Initial condition and its projection onto the initial basis\n    u0 = np.sin(np.pi * x)\n    # As u0 is proportional to the first sine mode, its projection is nearly exact if r0 >= 1\n    u_r_prev = V @ (V.T @ u0)\n    \n    # Time stepping parameters\n    num_steps = int(round(T / dt))\n    time_grid = np.arange(1, num_steps + 1) * dt\n\n    # Source term function\n    source_func = lambda x_coords, time: np.sin(5 * np.pi * x_coords) * np.exp(-time)\n    \n    max_accepted_estimator = 0.0\n    \n    # 3. Time Integration Loop\n    for t_k in time_grid:\n        s_k = source_func(x, t_k)\n        \n        # 4. Adaptive Inner Loop for Basis Enrichment\n        while True:\n            r = V.shape[1]\n            \n            # Project operators and states onto the current reduced basis\n            A_r = V.T @ A @ V\n            B_r = (1.0 / dt) * np.eye(r) + A_r\n            a_prev = V.T @ u_r_prev\n            s_r = V.T @ s_k\n            \n            # Assemble and solve the small r x r reduced system\n            rhs_r = (1.0 / dt) * a_prev + s_r\n            a_k = np.linalg.solve(B_r, rhs_r)\n            u_r_k = V @ a_k\n            \n            # Compute the a posteriori error estimator\n            residual = (1.0 / dt) * u_r_prev + s_k - B @ u_r_k\n            z_k = cho_solve(B_cholesky_factor, residual)\n            \n            # residual.T @ z_k is guaranteed non-negative in theory,\n            # but can be a tiny negative number due to floating point arithmetic.\n            est_squared = residual.T @ z_k\n            estimator = np.sqrt(max(0, est_squared))\n            \n            # Decision: Accept step or enrich basis\n            if estimator <= tau or r >= r_max:\n                if estimator > max_accepted_estimator:\n                    max_accepted_estimator = estimator\n                u_r_prev = u_r_k\n                break  # Exit adaptation loop and proceed to next time step\n            else:\n                # Enrich basis with the Riesz representative of the residual\n                w_k = z_k\n                \n                # Orthogonalize the new direction w_k against the existing basis V\n                proj_w = V @ (V.T @ w_k)\n                w_ortho = w_k - proj_w\n                norm_w_ortho = np.linalg.norm(w_ortho)\n                \n                # Add normalized vector if it's not already in the basis\n                if norm_w_ortho > 1e-12:\n                    w_new = w_ortho / norm_w_ortho\n                    V = np.hstack((V, w_new[:, np.newaxis]))\n                else:\n                    # Enrichment direction is linearly dependent. Accept the step\n                    # to prevent an infinite loop.\n                    if estimator > max_accepted_estimator:\n                        max_accepted_estimator = estimator\n                    u_r_prev = u_r_k\n                    break\n    \n    r_final = V.shape[1]\n    return r_final, max_accepted_estimator\n\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    # Full-order model parameters\n    N = 80\n    nu = 1e-2\n    T = 1e-1\n    dt = 2e-3\n\n    # Test cases: (r0, tau, r_max)\n    test_cases = [\n        (1, 1e-4, 20),   # Case A\n        (2, 1e-5, 25),   # Case B\n        (3, 1e-6, 8),    # Case C\n    ]\n\n    results = []\n    for r0, tau, r_max in test_cases:\n        r_final, max_estimator = run_simulation(N, nu, T, dt, r0, tau, r_max)\n        results.extend([r_final, max_estimator])\n\n    # Print results in the specified single-line format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3435653"}]}