{"hands_on_practices": [{"introduction": "Understanding the structure of the adjoint system is the first step towards mastering PDE-constrained optimization. This practice will guide you through a foundational derivation using the Lagrangian method [@problem_id:3429633]. By applying Green's identity and carefully analyzing the resulting boundary terms, you will derive the adjoint boundary conditions from first principles, revealing the intimate connection between the constraints on the state equation and the structure of its corresponding dual problem.", "problem": "Consider a bounded domain $\\Omega \\subset \\mathbb{R}^{d}$ with sufficiently smooth boundary $\\Gamma$ and outward unit normal $\\boldsymbol{n}$. The boundary is partitioned into three disjoint measurable parts $\\Gamma_{D}$, $\\Gamma_{N}$, and $\\Gamma_{R}$ such that $\\overline{\\Gamma_{D}} \\cup \\overline{\\Gamma_{N}} \\cup \\overline{\\Gamma_{R}} = \\Gamma$. Let $\\kappa \\in C^{1}(\\overline{\\Omega})$ satisfy $\\kappa(\\boldsymbol{x}) \\ge \\kappa_{0} > 0$ for all $\\boldsymbol{x} \\in \\overline{\\Omega}$. Given a desired state $y_{d} \\in L^{2}(\\Omega)$ and a control $u \\in L^{2}(\\Omega)$, the state $y$ is governed by the partial differential equation\n$$\n- \\nabla \\cdot \\big( \\kappa \\nabla y \\big) \\;=\\; u \\quad \\text{in } \\Omega,\n$$\nwith mixed boundary conditions\n$$\ny \\;=\\; 0 \\quad \\text{on } \\Gamma_{D}, \\qquad \\kappa \\nabla y \\cdot \\boldsymbol{n} \\;=\\; 0 \\quad \\text{on } \\Gamma_{N}, \\qquad \\alpha\\, y \\;+\\; \\beta\\, \\kappa \\nabla y \\cdot \\boldsymbol{n} \\;=\\; 0 \\quad \\text{on } \\Gamma_{R},\n$$\nwhere $\\alpha, \\beta \\in C(\\Gamma_{R})$ and $(\\alpha(\\boldsymbol{s}), \\beta(\\boldsymbol{s})) \\neq (0,0)$ at each $\\boldsymbol{s} \\in \\Gamma_{R}$. Consider the quadratic tracking cost functional\n$$\nJ(y,u) \\;=\\; \\frac{1}{2} \\int_{\\Omega} \\big( y - y_{d} \\big)^{2} \\, \\mathrm{d}\\boldsymbol{x}.\n$$\nUsing only the Lagrangian approach with an adjoint variable $p$ and the fundamental Green’s identity (integration by parts), derive from first principles the adjoint boundary conditions that ensure stationarity of the Lagrangian with respect to variations of $y$ subject to the given boundary constraints. In particular:\n- Justify, via explicit boundary term analysis, what the adjoint boundary condition must be on $\\Gamma_{D}$ and on $\\Gamma_{N}$, and explain why they are of different type.\n- Treat the Robin segment $\\Gamma_{R}$ by enforcing the linearized boundary constraint for variations, and deduce the Robin-type adjoint boundary condition in the form\n$$\n\\alpha^{\\ast} \\, p \\;+\\; \\beta^{\\ast} \\, \\kappa \\nabla p \\cdot \\boldsymbol{n} \\;=\\; 0 \\quad \\text{on } \\Gamma_{R}.\n$$\nYour answer must be the ordered pair of adjoint Robin coefficients $(\\alpha^{\\ast}, \\beta^{\\ast})$, expressed as a single row matrix. No numerical approximation is required. Provide your final answer as a closed-form analytic expression. Do not include any units in your final answer.", "solution": "We start from the Lagrangian for the constrained optimization problem. Introduce the adjoint (Lagrange multiplier) $p \\in H^{1}(\\Omega)$ and define\n$$\n\\mathcal{L}(y,u,p) \\;=\\; \\frac{1}{2} \\int_{\\Omega} \\big( y - y_{d} \\big)^{2} \\, \\mathrm{d}\\boldsymbol{x} \\;+\\; \\int_{\\Omega} p \\Big( - \\nabla \\cdot \\big( \\kappa \\nabla y \\big) - u \\Big) \\, \\mathrm{d}\\boldsymbol{x}.\n$$\nTo derive the adjoint equation and boundary conditions, we consider the first variation of $\\mathcal{L}$ with respect to $y$ in a direction $v$. The admissible variations $v$ must satisfy the linearized boundary conditions induced by the state boundary constraints, namely\n$$\nv \\;=\\; 0 \\quad \\text{on } \\Gamma_{D}, \n\\qquad \\kappa \\nabla v \\cdot \\boldsymbol{n} \\;=\\; 0 \\quad \\text{on } \\Gamma_{N}, \n\\qquad \\alpha\\, v \\;+\\; \\beta\\, \\kappa \\nabla v \\cdot \\boldsymbol{n} \\;=\\; 0 \\quad \\text{on } \\Gamma_{R}.\n$$\nThe Gâteaux derivative of $\\mathcal{L}$ with respect to $y$ in direction $v$ is\n$$\n\\delta_{y}\\mathcal{L}(y,u,p)[v] \\;=\\; \\int_{\\Omega} (y - y_{d})\\, v \\, \\mathrm{d}\\boldsymbol{x} \\;+\\; \\int_{\\Omega} p \\big( - \\nabla \\cdot (\\kappa \\nabla v) \\big) \\, \\mathrm{d}\\boldsymbol{x}.\n$$\nApply Green’s identity to the second integral. For sufficiently smooth $p$ and $v$,\n$$\n\\int_{\\Omega} p \\big( - \\nabla \\cdot (\\kappa \\nabla v) \\big) \\, \\mathrm{d}\\boldsymbol{x}\n\\;=\\;\n- \\int_{\\Omega} v \\, \\nabla \\cdot (\\kappa \\nabla p) \\, \\mathrm{d}\\boldsymbol{x}\n\\;+\\; \\int_{\\Gamma} \\kappa \\Big( v \\, \\nabla p \\cdot \\boldsymbol{n} \\;-\\; (\\nabla v \\cdot \\boldsymbol{n}) \\, p \\Big) \\, \\mathrm{d}s.\n$$\nHence\n$$\n\\delta_{y}\\mathcal{L}(y,u,p)[v]\n\\;=\\;\n\\int_{\\Omega} \\Big( (y - y_{d}) - \\nabla \\cdot (\\kappa \\nabla p) \\Big) v \\, \\mathrm{d}\\boldsymbol{x}\n\\;+\\; \\int_{\\Gamma} \\kappa \\Big( v \\, \\nabla p \\cdot \\boldsymbol{n} \\;-\\; (\\nabla v \\cdot \\boldsymbol{n}) \\, p \\Big) \\, \\mathrm{d}s.\n$$\nStationarity with respect to all admissible $v$ requires the interior term to vanish for all $v$, yielding the adjoint partial differential equation\n$$\n- \\nabla \\cdot (\\kappa \\nabla p) \\;=\\; y - y_{d} \\quad \\text{in } \\Omega.\n$$\nIt remains to enforce that the boundary integral vanishes for all admissible $v$ satisfying the linearized boundary conditions on each boundary segment. We analyze each segment separately.\n\nOn $\\Gamma_{D}$: The admissible variations satisfy $v = 0$ on $\\Gamma_{D}$, while $\\nabla v \\cdot \\boldsymbol{n}$ is not constrained by this condition. The boundary integral restricted to $\\Gamma_{D}$ reduces to\n$$\n\\int_{\\Gamma_{D}} \\kappa \\Big( 0 \\cdot \\nabla p \\cdot \\boldsymbol{n} \\;-\\; (\\nabla v \\cdot \\boldsymbol{n}) \\, p \\Big) \\, \\mathrm{d}s\n\\;=\\; - \\int_{\\Gamma_{D}} \\kappa \\, (\\nabla v \\cdot \\boldsymbol{n}) \\, p \\, \\mathrm{d}s.\n$$\nBecause $(\\nabla v \\cdot \\boldsymbol{n})$ can be chosen arbitrarily (in the trace sense) under $v=0$, the only way this integral vanishes for all such $v$ is to impose\n$$\np \\;=\\; 0 \\quad \\text{on } \\Gamma_{D}.\n$$\nThus, a Dirichlet state boundary condition induces a Dirichlet adjoint boundary condition on the same segment.\n\nOn $\\Gamma_{N}$: The admissible variations satisfy $\\kappa \\nabla v \\cdot \\boldsymbol{n} = 0$ on $\\Gamma_{N}$, while $v$ is otherwise unconstrained on $\\Gamma_{N}$. The boundary integral restricted to $\\Gamma_{N}$ becomes\n$$\n\\int_{\\Gamma_{N}} \\kappa \\Big( v \\, \\nabla p \\cdot \\boldsymbol{n} \\;-\\; 0 \\cdot p \\Big) \\, \\mathrm{d}s\n\\;=\\; \\int_{\\Gamma_{N}} \\kappa \\, v \\, \\nabla p \\cdot \\boldsymbol{n} \\, \\mathrm{d}s.\n$$\nBecause $v$ can be chosen arbitrarily on $\\Gamma_{N}$, the only way this integral vanishes for all such $v$ is to impose\n$$\n\\kappa \\nabla p \\cdot \\boldsymbol{n} \\;=\\; 0 \\quad \\text{on } \\Gamma_{N}.\n$$\nThus, a Neumann state boundary condition induces a Neumann adjoint boundary condition on the same segment. This explains the difference between the Dirichlet and Neumann cases: on $\\Gamma_{D}$ the trace of $v$ is fixed to zero, forcing $p$ to vanish; on $\\Gamma_{N}$ the normal derivative of $v$ is fixed to zero, forcing the normal flux of $p$ to vanish.\n\nOn $\\Gamma_{R}$: The admissible variations satisfy the linearized Robin constraint\n$$\n\\alpha \\, v \\;+\\; \\beta \\, \\kappa \\nabla v \\cdot \\boldsymbol{n} \\;=\\; 0 \\quad \\text{on } \\Gamma_{R}.\n$$\nThe boundary integral restricted to $\\Gamma_{R}$ reads\n$$\n\\int_{\\Gamma_{R}} \\kappa \\Big( v \\, \\nabla p \\cdot \\boldsymbol{n} \\;-\\; (\\nabla v \\cdot \\boldsymbol{n}) \\, p \\Big) \\, \\mathrm{d}s.\n$$\nTo enforce vanishing for all such $v$, it suffices to eliminate $v$ in favor of $\\nabla v \\cdot \\boldsymbol{n}$ (or vice versa). When $\\alpha(\\boldsymbol{s}) \\neq 0$ at a boundary point, we can write $v \\,=\\, - (\\beta \\kappa / \\alpha) \\, \\nabla v \\cdot \\boldsymbol{n}$. Substituting pointwise gives the integrand\n$$\n\\kappa \\left( - \\frac{\\beta \\kappa}{\\alpha} \\, (\\nabla v \\cdot \\boldsymbol{n}) \\, \\nabla p \\cdot \\boldsymbol{n} \\;-\\; (\\nabla v \\cdot \\boldsymbol{n}) \\, p \\right)\n\\;=\\;\n- \\kappa \\, (\\nabla v \\cdot \\boldsymbol{n}) \\left( \\frac{\\beta \\kappa}{\\alpha} \\, \\nabla p \\cdot \\boldsymbol{n} \\;+\\; p \\right).\n$$\nBecause $(\\nabla v \\cdot \\boldsymbol{n})$ can be chosen arbitrarily under the linearized constraint, the factor in parentheses must vanish, yielding\n$$\n\\alpha \\, p \\;+\\; \\beta \\, \\kappa \\nabla p \\cdot \\boldsymbol{n} \\;=\\; 0 \\quad \\text{on } \\Gamma_{R} \\cap \\{ \\alpha \\neq 0 \\}.\n$$\nA symmetric argument applies at points where $\\beta(\\boldsymbol{s}) \\neq 0$, solving instead for $\\nabla v \\cdot \\boldsymbol{n}$ in terms of $v$ and leading to the same condition. Since $(\\alpha,\\beta)$ are not simultaneously zero, we conclude that the adjoint Robin boundary condition everywhere on $\\Gamma_{R}$ is\n$$\n\\alpha^{\\ast} \\, p \\;+\\; \\beta^{\\ast} \\, \\kappa \\nabla p \\cdot \\boldsymbol{n} \\;=\\; 0\n\\quad \\text{with} \\quad\n\\alpha^{\\ast} \\;=\\; \\alpha, \\;\\; \\beta^{\\ast} \\;=\\; \\beta.\n$$\nIn summary, integration by parts and enforcement of the linearized boundary constraints on variations $v$ imply\n- On $\\Gamma_{D}$: $p = 0$ (Dirichlet).\n- On $\\Gamma_{N}$: $\\kappa \\nabla p \\cdot \\boldsymbol{n} = 0$ (Neumann).\n- On $\\Gamma_{R}$: $\\alpha p + \\beta \\kappa \\nabla p \\cdot \\boldsymbol{n} = 0$ (Robin with unchanged coefficients).\nTherefore, the ordered pair of adjoint Robin coefficients is $(\\alpha^{\\ast}, \\beta^{\\ast}) = (\\alpha, \\beta)$.", "answer": "$$\\boxed{\\begin{pmatrix}\\alpha & \\beta\\end{pmatrix}}$$", "id": "3429633"}, {"introduction": "Moving from continuous theory to numerical practice requires a robust discretization strategy. This exercise translates the abstract concepts of functional analysis into the concrete language of linear algebra, involving stiffness ($ \\boldsymbol{K} $) and mass ($ \\boldsymbol{M} $) matrices common in finite element methods [@problem_id:3429645]. By deriving the discrete reduced gradient using the discrete adjoint method, you will gain hands-on experience with the algebraic manipulations that form the core of the \"discretize-then-optimize\" approach.", "problem": "Consider the following linear-quadratic optimal control problem constrained by a linear elliptic partial differential equation on a bounded, polygonal domain $\\Omega \\subset \\mathbb{R}^{2}$ with homogeneous Dirichlet boundary conditions. Let $V := H_{0}^{1}(\\Omega)$ and define the bilinear form $a(\\cdot,\\cdot)$ and the $L^{2}(\\Omega)$ inner product $(\\cdot,\\cdot)$ by\n$$\na(y,v) := \\int_{\\Omega} \\nabla y \\cdot \\nabla v \\, dx + \\int_{\\Omega} c(x) \\, y \\, v \\, dx, \\quad (u,v) := \\int_{\\Omega} u \\, v \\, dx,\n$$\nwhere $c \\in L^{\\infty}(\\Omega)$ satisfies $c(x) \\ge 0$ almost everywhere. For a given desired state $y_{d} \\in L^{2}(\\Omega)$ and regularization parameter $\\alpha > 0$, the objective functional is\n$$\nJ(y,u) := \\frac{1}{2} \\, \\| y - y_{d} \\|_{L^{2}(\\Omega)}^{2} + \\frac{\\alpha}{2} \\, \\| u \\|_{L^{2}(\\Omega)}^{2},\n$$\nsubject to the weak state equation\n$$\na(y,v) = (u,v) \\quad \\text{for all } v \\in V.\n$$\nLet $V_{h} \\subset V$ be a conforming finite element space with basis $\\{ \\varphi_{i} \\}_{i=1}^{n}$, and approximate the state, adjoint, and control in $V_{h}$ as\n$$\ny_{h} = \\sum_{i=1}^{n} y_{i} \\, \\varphi_{i}, \\quad p_{h} = \\sum_{i=1}^{n} p_{i} \\, \\varphi_{i}, \\quad u_{h} = \\sum_{i=1}^{n} u_{i} \\, \\varphi_{i}.\n$$\nDefine the stiffness matrix $\\boldsymbol{K} \\in \\mathbb{R}^{n \\times n}$ and mass matrix $\\boldsymbol{M} \\in \\mathbb{R}^{n \\times n}$ by\n$$\n\\boldsymbol{K}_{ij} := a(\\varphi_{j}, \\varphi_{i}), \\quad \\boldsymbol{M}_{ij} := (\\varphi_{j}, \\varphi_{i}),\n$$\nand let $\\boldsymbol{y}_{d} \\in \\mathbb{R}^{n}$ denote the coefficient vector of the $L^{2}(\\Omega)$ projection of $y_{d}$ onto $V_{h}$, i.e., $(y_{d,h}, v_{h}) = (y_{d}, v_{h})$ for all $v_{h} \\in V_{h}$. The discrete state and adjoint equations are\n$$\n\\boldsymbol{K} \\, \\boldsymbol{y} = \\boldsymbol{M} \\, \\boldsymbol{u}, \\quad \\boldsymbol{K} \\, \\boldsymbol{p} = \\boldsymbol{M} \\, (\\boldsymbol{y} - \\boldsymbol{y}_{d}),\n$$\nand the discrete reduced objective is\n$$\nj_{h}(\\boldsymbol{u}) := \\frac{1}{2} \\, (\\boldsymbol{y} - \\boldsymbol{y}_{d})^{\\top} \\boldsymbol{M} (\\boldsymbol{y} - \\boldsymbol{y}_{d}) + \\frac{\\alpha}{2} \\, \\boldsymbol{u}^{\\top} \\boldsymbol{M} \\boldsymbol{u},\n$$\nwhere $\\boldsymbol{y} = \\boldsymbol{K}^{-1} \\boldsymbol{M} \\boldsymbol{u}$. Starting only from these definitions and the variational form, derive the discrete reduced gradient with respect to the Euclidean inner product on $\\mathbb{R}^{n}$ via the discrete adjoint equation, and then eliminate the adjoint and state variables to express this gradient entirely in terms of $\\boldsymbol{K}$, $\\boldsymbol{M}$, $\\alpha$, $\\boldsymbol{u}$, and $\\boldsymbol{y}_{d}$. Finally, comment on how this discrete reduced gradient relates to the discretized continuous gradient obtained from the $L^{2}(\\Omega)$ expression $\\alpha \\, u + p$.\n\nYour final answer must be the single closed-form analytic expression for the discrete reduced gradient vector in terms of $\\boldsymbol{K}$, $\\boldsymbol{M}$, $\\alpha$, $\\boldsymbol{u}$, and $\\boldsymbol{y}_{d}$, enclosed in a box. No rounding is required, and no physical units apply.", "solution": "The objective is to derive the gradient of the discrete reduced objective functional\n$$\nj_{h}(\\boldsymbol{u}) := \\frac{1}{2} \\, (\\boldsymbol{y} - \\boldsymbol{y}_{d})^{\\top} \\boldsymbol{M} (\\boldsymbol{y} - \\boldsymbol{y}_{d}) + \\frac{\\alpha}{2} \\, \\boldsymbol{u}^{\\top} \\boldsymbol{M} \\boldsymbol{u}\n$$\nwith respect to the Euclidean inner product on $\\mathbb{R}^{n}$. The state vector $\\boldsymbol{y}$ depends on the control vector $\\boldsymbol{u}$ through the discrete state equation $\\boldsymbol{K} \\boldsymbol{y} = \\boldsymbol{M} \\boldsymbol{u}$. The problem stipulates the use of the discrete adjoint method.\n\nLet us compute the Gâteaux derivative of $j_{h}(\\boldsymbol{u})$ in an arbitrary direction $\\delta \\boldsymbol{u} \\in \\mathbb{R}^{n}$. Let $\\boldsymbol{y}(\\boldsymbol{u}) = \\boldsymbol{K}^{-1} \\boldsymbol{M} \\boldsymbol{u}$. A perturbation $\\delta \\boldsymbol{u}$ in the control induces a perturbation $\\delta \\boldsymbol{y}$ in the state, given by linearizing the state equation:\n$$\n\\boldsymbol{K} \\, \\delta\\boldsymbol{y} = \\boldsymbol{M} \\, \\delta\\boldsymbol{u}.\n$$\nThe first variation of $j_{h}$ is\n$$\n\\delta j_{h} = Dj_{h}(\\boldsymbol{u})[\\delta \\boldsymbol{u}] = (\\boldsymbol{y} - \\boldsymbol{y}_{d})^{\\top} \\boldsymbol{M} \\, \\delta\\boldsymbol{y} + \\alpha \\, \\boldsymbol{u}^{\\top} \\boldsymbol{M} \\, \\delta\\boldsymbol{u}.\n$$\nThe gradient is typically computed more efficiently by avoiding the direct computation of $\\delta \\boldsymbol{y} = \\boldsymbol{K}^{-1} \\boldsymbol{M} \\delta \\boldsymbol{u}$ and its substitution into the expression for $\\delta j_{h}$. The adjoint method introduces an adjoint state $\\boldsymbol{p}$ to eliminate $\\delta \\boldsymbol{y}$.\n\nThe problem provides the discrete adjoint equation:\n$$\n\\boldsymbol{K} \\boldsymbol{p} = \\boldsymbol{M} (\\boldsymbol{y} - \\boldsymbol{y}_{d}).\n$$\nThe bilinear form $a(\\cdot,\\cdot)$ is symmetric, which implies the stiffness matrix $\\boldsymbol{K}$ is symmetric, i.e., $\\boldsymbol{K}^{\\top} = \\boldsymbol{K}$. The mass matrix $\\boldsymbol{M}$ is also symmetric by definition, $\\boldsymbol{M}^{\\top} = \\boldsymbol{M}$. Using the symmetry of $\\boldsymbol{K}$, the adjoint equation can be written as $\\boldsymbol{K}^{\\top} \\boldsymbol{p} = \\boldsymbol{M} (\\boldsymbol{y} - \\boldsymbol{y}_{d})$.\n\nNow we manipulate the first term in the expression for $\\delta j_{h}$:\n$$\n(\\boldsymbol{y} - \\boldsymbol{y}_{d})^{\\top} \\boldsymbol{M} \\, \\delta\\boldsymbol{y} = (\\boldsymbol{M} (\\boldsymbol{y} - \\boldsymbol{y}_{d}))^{\\top} \\delta\\boldsymbol{y}.\n$$\nSubstituting the expression for $\\boldsymbol{M} (\\boldsymbol{y} - \\boldsymbol{y}_{d})$ from the adjoint equation:\n$$\n(\\boldsymbol{y} - \\boldsymbol{y}_{d})^{\\top} \\boldsymbol{M} \\, \\delta\\boldsymbol{y} = (\\boldsymbol{K}^{\\top} \\boldsymbol{p})^{\\top} \\delta\\boldsymbol{y} = \\boldsymbol{p}^{\\top} \\boldsymbol{K} \\, \\delta\\boldsymbol{y}.\n$$\nNext, we substitute the expression for $\\boldsymbol{K} \\, \\delta\\boldsymbol{y}$ from the perturbed state equation:\n$$\n\\boldsymbol{p}^{\\top} \\boldsymbol{K} \\, \\delta\\boldsymbol{y} = \\boldsymbol{p}^{\\top} \\boldsymbol{M} \\, \\delta\\boldsymbol{u}.\n$$\nSubstituting this result back into the expression for $\\delta j_{h}$:\n$$\n\\delta j_{h} = \\boldsymbol{p}^{\\top} \\boldsymbol{M} \\, \\delta\\boldsymbol{u} + \\alpha \\, \\boldsymbol{u}^{\\top} \\boldsymbol{M} \\, \\delta\\boldsymbol{u}.\n$$\nUsing the symmetry of $\\boldsymbol{M}$, we can rewrite this as:\n$$\n\\delta j_{h} = (\\boldsymbol{M} \\boldsymbol{p})^{\\top} \\delta\\boldsymbol{u} + (\\alpha \\boldsymbol{M} \\boldsymbol{u})^{\\top} \\delta\\boldsymbol{u} = (\\alpha \\boldsymbol{M} \\boldsymbol{u} + \\boldsymbol{M} \\boldsymbol{p})^{\\top} \\delta\\boldsymbol{u}.\n$$\nThe gradient of $j_{h}$ with respect to the Euclidean inner product, denoted $\\nabla j_{h}(\\boldsymbol{u})$, is defined by the relation $\\delta j_{h} = (\\nabla j_{h}(\\boldsymbol{u}))^{\\top} \\delta \\boldsymbol{u}$ for all $\\delta \\boldsymbol{u}$. Therefore, we identify the gradient as:\n$$\n\\nabla j_{h}(\\boldsymbol{u}) = \\alpha \\boldsymbol{M} \\boldsymbol{u} + \\boldsymbol{M} \\boldsymbol{p} = \\boldsymbol{M} (\\alpha \\boldsymbol{u} + \\boldsymbol{p}).\n$$\n\nThe problem asks to comment on the relation of this expression to the continuous gradient $\\alpha u + p$. In the continuous setting, the optimality condition is $\\nabla J(u) = \\alpha u + p = 0$. The vectors $\\boldsymbol{u}$ and $\\boldsymbol{p}$ contain the coefficients of the finite element functions $u_{h} = \\sum_{i=1}^{n} u_{i} \\varphi_{i}$ and $p_{h} = \\sum_{i=1}^{n} p_{i} \\varphi_{i}$, respectively. The vector $\\alpha \\boldsymbol{u} + \\boldsymbol{p}$ thus contains the coefficients of the function $\\alpha u_{h} + p_{h}$. The expression $\\boldsymbol{M}(\\alpha \\boldsymbol{u} + \\boldsymbol{p})$ represents the projection of this function onto the finite element basis functions, since its $i$-th component is $\\sum_{j} \\boldsymbol{M}_{ij} (\\alpha u_{j} + p_{j}) = \\sum_{j} (\\varphi_{j}, \\varphi_{i}) (\\alpha u_{j} + p_{j}) = (\\alpha u_{h} + p_{h}, \\varphi_{i})$. This is precisely the Riesz representation of the `L2` gradient in the Euclidean space, often referred to as the Euclidean gradient.\n\nFinally, we must eliminate the state $\\boldsymbol{y}$ and adjoint $\\boldsymbol{p}$ variables to express the gradient solely in terms of $\\boldsymbol{K}$, $\\boldsymbol{M}$, $\\alpha$, $\\boldsymbol{u}$, and $\\boldsymbol{y}_{d}$. We have the following system of equations:\n1. State: $\\boldsymbol{y} = \\boldsymbol{K}^{-1} \\boldsymbol{M} \\boldsymbol{u}$\n2. Adjoint: $\\boldsymbol{p} = \\boldsymbol{K}^{-1} \\boldsymbol{M} (\\boldsymbol{y} - \\boldsymbol{y}_{d})$\n\nSubstitute (1) into (2):\n$$\n\\boldsymbol{p} = \\boldsymbol{K}^{-1} \\boldsymbol{M} (\\boldsymbol{K}^{-1} \\boldsymbol{M} \\boldsymbol{u} - \\boldsymbol{y}_{d}).\n$$\nNow substitute this expression for $\\boldsymbol{p}$ into the gradient formula $\\nabla j_{h}(\\boldsymbol{u}) = \\boldsymbol{M} (\\alpha \\boldsymbol{u} + \\boldsymbol{p})$:\n$$\n\\nabla j_{h}(\\boldsymbol{u}) = \\boldsymbol{M} \\left( \\alpha \\boldsymbol{u} + \\boldsymbol{K}^{-1} \\boldsymbol{M} (\\boldsymbol{K}^{-1} \\boldsymbol{M} \\boldsymbol{u} - \\boldsymbol{y}_{d}) \\right).\n$$\nExpanding this expression gives the final form of the gradient:\n$$\n\\nabla j_{h}(\\boldsymbol{u}) = \\alpha \\boldsymbol{M} \\boldsymbol{u} + \\boldsymbol{M} \\boldsymbol{K}^{-1} \\boldsymbol{M} \\boldsymbol{K}^{-1} \\boldsymbol{M} \\boldsymbol{u} - \\boldsymbol{M} \\boldsymbol{K}^{-1} \\boldsymbol{M} \\boldsymbol{y}_{d}.\n$$\nThis can be grouped as:\n$$\n\\nabla j_{h}(\\boldsymbol{u}) = (\\alpha \\boldsymbol{M} + \\boldsymbol{M} \\boldsymbol{K}^{-1} \\boldsymbol{M} \\boldsymbol{K}^{-1} \\boldsymbol{M}) \\boldsymbol{u} - \\boldsymbol{M} \\boldsymbol{K}^{-1} \\boldsymbol{M} \\boldsymbol{y}_{d}.\n$$\nThis is the required closed-form expression for the discrete reduced gradient.", "answer": "$$\n\\boxed{(\\alpha \\boldsymbol{M} + \\boldsymbol{M} \\boldsymbol{K}^{-1} \\boldsymbol{M} \\boldsymbol{K}^{-1} \\boldsymbol{M}) \\boldsymbol{u} - \\boldsymbol{M} \\boldsymbol{K}^{-1} \\boldsymbol{M} \\boldsymbol{y}_{d}}\n$$", "id": "3429645"}, {"introduction": "Deriving the gradient is only half the battle; solving the resulting large-scale linear systems efficiently is paramount for practical applications. The discrete Karush-Kuhn-Tucker (KKT) system is often a massive, ill-conditioned saddle-point problem that demands effective preconditioning for iterative solvers to be viable [@problem_id:3429640]. This coding exercise delves into the numerical linear algebra at the heart of large-scale optimization, where you will implement and compare preconditioners and analyze their impact on the system's condition number, a critical factor for solver performance.", "problem": "Consider the distributed control problem for the Poisson equation on the unit square. Let $\\Omega = (0,1)^2$, let $y_d \\in L^2(\\Omega)$ be a given desired state, and consider the linear elliptic state equation with homogeneous Dirichlet boundary conditions\n$$\n-\\Delta y = u \\quad \\text{in } \\Omega, \\qquad y = 0 \\quad \\text{on } \\partial \\Omega,\n$$\ntogether with the quadratic objective\n$$\nJ(y,u) = \\tfrac{1}{2} \\| y - y_d \\|_{L^2(\\Omega)}^2 + \\tfrac{\\beta}{2} \\| u \\|_{L^2(\\Omega)}^2,\n$$\nwhere $\\beta > 0$ is a Tikhonov regularization parameter. The first-order optimality (Karush–Kuhn–Tucker) conditions for the Lagrangian\n$$\n\\mathcal{L}(y,u,p) = \\tfrac{1}{2} \\| y - y_d \\|_{L^2(\\Omega)}^2 + \\tfrac{\\beta}{2} \\| u \\|_{L^2(\\Omega)}^2 + \\langle p, -\\Delta y - u \\rangle\n$$\nread\n$$\n\\begin{aligned}\n&\\text{state:} \\quad -\\Delta y - u = 0,\\\\\n&\\text{adjoint:} \\quad y + (-\\Delta)^\\ast p = y_d,\\\\\n&\\text{gradient:} \\quad \\beta u - p = 0,\n\\end{aligned}\n$$\nwhere $(-\\Delta)^\\ast = -\\Delta$ under homogeneous Dirichlet boundary conditions. Upon finite difference discretization on a uniform grid with $n \\times n$ interior points (so $h = 1/(n+1)$), we obtain a sparse, symmetric positive definite stiffness matrix $A_h \\in \\mathbb{R}^{N \\times N}$ with $N = n^2$ that approximates $-\\Delta$ via the standard five-point stencil. Denote by $I$ the identity on $\\mathbb{R}^N$.\n\nA well-known approach eliminates the control to obtain a $2 \\times 2$ symmetric indefinite system in the state and adjoint variables:\n$$\n\\begin{bmatrix}\nI & A_h^\\top \\\\\nA_h & -\\beta^{-1} I\n\\end{bmatrix}\n\\begin{bmatrix}\ny_h \\\\ p_h\n\\end{bmatrix}\n=\n\\begin{bmatrix}\ny_d \\\\ 0\n\\end{bmatrix}.\n$$\nThis system can be interpreted as a saddle point system of the form\n$$\n\\begin{bmatrix}\nH & B^\\top \\\\\nB & -C\n\\end{bmatrix},\n$$\nwith $H = I$, $B = A_h$, and $C = \\beta^{-1} I$. A canonical block lower-triangular preconditioner (arising from the augmented Lagrangian viewpoint) uses the exact $H$ and an approximation $\\widetilde{S}$ of the Schur complement $S = B H^{-1} B^\\top + C$, that is,\n$$\nS = A_h A_h^\\top + \\beta^{-1} I.\n$$\nThe preconditioned operator induced by the block triangular preconditioner has spectrum equal to the union of the eigenvalue $1$ (with multiplicity) and the spectrum of the matrix pencil $\\widetilde{S}^{-1} S$. Therefore, the quality of the preconditioner is governed by the eigenvalue bounds of $\\widetilde{S}^{-1} S$.\n\nAn alternative solution strategy forms the reduced Hessian (normal equations) for the control variable $u_h$:\n$$\nH_{\\mathrm{red}} = \\beta I + A_h^{-\\top} A_h^{-1}.\n$$\nA simple diagonal preconditioner can be defined by\n$$\n\\widetilde{H}_{\\mathrm{red}} = \\beta I + \\operatorname{diag}(A_h)^{-2}.\n$$\n\nYour task is to design, implement, and compare two preconditioners within this discrete framework:\n\n- Augmented Lagrangian block triangular preconditioner (AL-BT) for the $2 \\times 2$ KKT system: use the exact $H = I$ and the diagonal Schur-complement approximation\n$$\n\\widetilde{S}_{\\mathrm{AL}} = \\operatorname{diag}(A_h A_h^\\top) + \\beta^{-1} I.\n$$\nAnalyze the nontrivial eigenvalues of the preconditioned operator by computing the spectrum of the symmetrically preconditioned Schur complement\n$$\n\\Lambda_{\\mathrm{AL}} = \\lambda\\left( \\widetilde{S}_{\\mathrm{AL}}^{-1/2} \\, \\left(A_h A_h^\\top + \\beta^{-1} I\\right) \\, \\widetilde{S}_{\\mathrm{AL}}^{-1/2} \\right),\n$$\nand quantify eigenvalue bounds via the condition number\n$$\n\\kappa_{\\mathrm{AL}} = \\frac{\\max \\Lambda_{\\mathrm{AL}}}{\\min \\Lambda_{\\mathrm{AL}}}.\n$$\n\n- Normal equations preconditioner (NE) on the reduced control Hessian: precondition $H_{\\mathrm{red}}$ by the diagonal\n$$\n\\widetilde{H}_{\\mathrm{NE}} = \\beta I + \\operatorname{diag}(A_h)^{-2},\n$$\nand analyze eigenvalue bounds through\n$$\n\\Lambda_{\\mathrm{NE}} = \\lambda\\left( \\widetilde{H}_{\\mathrm{NE}}^{-1/2} \\, \\left(\\beta I + A_h^{-\\top} A_h^{-1}\\right) \\, \\widetilde{H}_{\\mathrm{NE}}^{-1/2} \\right), \\qquad\n\\kappa_{\\mathrm{NE}} = \\frac{\\max \\Lambda_{\\mathrm{NE}}}{\\min \\Lambda_{\\mathrm{NE}}}.\n$$\nNote that applying $A_h^{-\\top} A_h^{-1}$ should be implemented by solving two sparse linear systems with coefficient matrix $A_h$; do not form $A_h^{-1}$ explicitly.\n\nImplementation requirements:\n- Use a standard five-point finite difference discretization of $-\\Delta$ on $\\Omega = (0,1)^2$ with homogeneous Dirichlet boundaries. With $n$ interior points per coordinate direction, the mesh width is $h = 1/(n+1)$ and $N = n^2$. Construct $A_h$ accordingly as a sparse matrix in $\\mathbb{R}^{N \\times N}$.\n- For the AL-BT preconditioner, use $\\widetilde{S}_{\\mathrm{AL}}$ as given above; for the NE preconditioner, use $\\widetilde{H}_{\\mathrm{NE}}$ as given above.\n- For each preconditioner, compute $\\kappa_{\\mathrm{AL}}$ and $\\kappa_{\\mathrm{NE}}$ by estimating the smallest and largest eigenvalues of the corresponding symmetrically preconditioned operators. Use methods that are spectrally accurate for symmetric positive definite operators. Do not form dense inverses.\n\nTest suite:\n- Use the following parameter sets $(n,\\beta)$:\n  - $(n,\\beta) = (8, 10^{-2})$,\n  - $(n,\\beta) = (16, 10^{-2})$,\n  - $(n,\\beta) = (8, 1)$,\n  - $(n,\\beta) = (16, 1)$.\nFor each parameter set, compute the pair $(\\kappa_{\\mathrm{AL}}, \\kappa_{\\mathrm{NE}})$.\n\nFinal output format:\n- Your program should produce a single line of output containing all results as a comma-separated list enclosed in square brackets. The results must be ordered as\n$$\n[\\kappa_{\\mathrm{AL}}(8,10^{-2}), \\kappa_{\\mathrm{NE}}(8,10^{-2}), \\kappa_{\\mathrm{AL}}(16,10^{-2}), \\kappa_{\\mathrm{NE}}(16,10^{-2}), \\kappa_{\\mathrm{AL}}(8,1), \\kappa_{\\mathrm{NE}}(8,1), \\kappa_{\\mathrm{AL}}(16,1), \\kappa_{\\mathrm{NE}}(16,1)].\n$$\n- Express each $\\kappa$ value as a floating-point number rounded to six decimal places.\n- No physical units are involved in this problem; all quantities are dimensionless.\n\nScientific realism and derivation basis:\n- Start from the definitions of the Lagrangian optimality system, the Schur complement, and the reduced Hessian. Use the properties of symmetric positive definite matrices and Schur complement preconditioning to derive why the spectra of the preconditioned operators are governed by the given eigenproblems.\n- Ensure your reasoning references the scaling of the discrete Laplacian eigenvalues with respect to $h$ and justifies mesh-independence or mesh-dependence claims based on asymptotic scaling as $h \\to 0$.\n\nYour program must be completely self-contained and must not read any input. It must compute and print the required list in the exact format specified above.", "solution": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.sparse import diags, eye, kron\nfrom scipy.sparse.linalg import eigsh, LinearOperator, factorized\n\ndef solve():\n    \"\"\"\n    Computes and compares condition numbers for two preconditioners\n    for a PDE-constrained optimization problem.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (8, 1e-2),\n        (16, 1e-2),\n        (8, 1.0),\n        (16, 1.0),\n    ]\n\n    results = []\n    for n, beta in test_cases:\n        N = n * n\n        h = 1.0 / (n + 1)\n        \n        # Construct the sparse 2D Laplacian matrix A_h using a 5-point stencil.\n        # A_h approximates -Delta.\n        T = diags([-1, 2, -1], [-1, 0, 1], shape=(n, n), format='csc')\n        I_n = eye(n, format='csc')\n        A_h = (1 / h**2) * (kron(I_n, T) + kron(T, I_n))\n        A_h = A_h.asformat('csc') # Ensures efficient factorization\n        I_N = eye(N, format='csc')\n\n        # --- Augmented Lagrangian Block Triangular (AL-BT) Preconditioner ---\n        \n        # Schur complement S = A_h^2 + (1/beta) * I\n        Ah_sq = A_h @ A_h\n        S = Ah_sq + (1.0 / beta) * I_N\n        \n        # Preconditioner \\tilde{S}_AL = diag(A_h^2) + (1/beta) * I\n        diag_Ah_sq = Ah_sq.diagonal()\n        diag_S_tilde = diag_Ah_sq + (1.0 / beta)\n        S_tilde_AL = diags(diag_S_tilde, format='csc')\n        \n        # Compute eigenvalues of S_tilde_AL^{-1} S.\n        # eigsh solves the generalized symmetric eigenproblem A x = lambda M x.\n        # Here A=S, M=S_tilde_AL.\n        lambda_max_AL = eigsh(S, k=1, M=S_tilde_AL, which='LM', tol=1e-12, return_eigenvectors=False)[0]\n        lambda_min_AL = eigsh(S, k=1, M=S_tilde_AL, which='SM', tol=1e-12, return_eigenvectors=False)[0]\n        kappa_AL = lambda_max_AL / lambda_min_AL\n        results.append(f\"{kappa_AL:.6f}\")\n\n        # --- Normal Equations (NE) Preconditioner ---\n        \n        # Define the reduced Hessian H_red = beta*I + A_h^{-2} as a LinearOperator.\n        # This avoids explicit inversion of A_h.\n        solver = factorized(A_h)\n        def H_red_matvec(v):\n            # Computes w = (beta*I + A_h^{-2}) @ v\n            # by solving two linear systems: A_h z = v and A_h w = z\n            z = solver(v)\n            w = solver(z)\n            return beta * v + w\n\n        H_red_op = LinearOperator((N, N), matvec=H_red_matvec, rmatvec=H_red_matvec, dtype=np.float64)\n\n        # Define the preconditioner \\tilde{H}_NE = beta*I + diag(A_h)^{-2}\n        diag_A_h = A_h.diagonal()\n        diag_H_tilde = beta + diag_A_h**(-2.0)\n        H_tilde_NE = diags(diag_H_tilde, format='csc')\n        \n        # Compute eigenvalues of H_tilde_NE^{-1} H_red.\n        lambda_max_NE = eigsh(H_red_op, k=1, M=H_tilde_NE, which='LM', tol=1e-12, return_eigenvectors=False)[0]\n        lambda_min_NE = eigsh(H_red_op, k=1, M=H_tilde_NE, which='SM', tol=1e-12, return_eigenvectors=False)[0]\n        kappa_NE = lambda_max_NE / lambda_min_NE\n        results.append(f\"{kappa_NE:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "answer": "$$\n\\boxed{[3.987783,165.701192,3.996924,165.719741,1.000000,1.000000,1.000000,1.000000]}\n$$", "id": "3429640"}]}