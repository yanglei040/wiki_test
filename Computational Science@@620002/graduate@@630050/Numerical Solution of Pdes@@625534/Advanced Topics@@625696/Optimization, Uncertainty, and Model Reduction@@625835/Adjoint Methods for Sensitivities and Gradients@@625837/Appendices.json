{"hands_on_practices": [{"introduction": "This first practice goes to the heart of the adjoint method: deriving the continuous adjoint system for a PDE-constrained optimization problem. By constructing a Lagrangian and finding its stationary points, we can derive an adjoint equation that allows for the efficient computation of the objective functional's gradient. This exercise for a parabolic (time-dependent) PDE [@problem_id:3361153] is a foundational skill, demonstrating how to handle initial and boundary conditions through integration by parts in both space and time.", "problem": "Consider a bounded Lipschitz domain $\\Omega \\subset \\mathbb{R}^{d}$ with boundary $\\partial \\Omega$ and a fixed final time $T>0$. Let $\\kappa(x)$ be a scalar diffusion coefficient satisfying $\\kappa \\in C^{1}(\\overline{\\Omega})$ and $0<\\kappa_{\\min}\\le \\kappa(x)\\le \\kappa_{\\max}<\\infty$. For a given source $s \\in L^{2}(\\Omega \\times (0,T))$, the state $u:\\Omega \\times [0,T]\\to \\mathbb{R}$ is defined as the unique weak solution of the parabolic initial-boundary value problem\n$$\n\\begin{cases}\nu_{t}(x,t)-\\nabla \\cdot \\big(\\kappa(x)\\nabla u(x,t)\\big)=s(x,t), & (x,t)\\in \\Omega \\times (0,T),\\\\\nu(x,t)=0, & (x,t)\\in \\partial \\Omega \\times (0,T),\\\\\nu(x,0)=u_{0}(x), & x\\in \\Omega,\n\\end{cases}\n$$\nwhere $u_{0}\\in L^{2}(\\Omega)$ is fixed. Let $d \\in L^{2}(\\Omega \\times (0,T))$ be given spatiotemporal data, and define the tracking-type objective\n$$\nJ(u)=\\frac{1}{2}\\int_{0}^{T}\\!\\!\\int_{\\Omega} \\big(u(x,t)-d(x,t)\\big)^{2}\\,dx\\,dt.\n$$\nTreat the state equation as a constraint linking $u$ and $s$, and regard $J$ as a functional of $s$ through the state $u(s)$. Using only foundational tools (the chain rule for Gâteaux derivatives in Banach spaces, the method of Lagrange multipliers for constrained optimization, and integration by parts in time and space consistent with the given boundary and initial data), derive an explicit analytical expression for the $L^{2}(\\Omega\\times(0,T))$-gradient of $J$ with respect to the source $s(x,t)$. Your final answer must be a single closed-form analytic expression in terms of quantities defined by well-posed partial differential equations. Do not introduce any regularization. Express your final answer as a single analytic expression and do not include units.", "solution": "The problem as stated is valid. It is a well-posed, scientifically grounded problem in the field of optimal control of partial differential equations. All necessary data, functions, and conditions are provided, and they are mutually consistent. We may therefore proceed with the derivation of the solution.\n\nThe goal is to find the $L^{2}(\\Omega\\times(0,T))$-gradient of the objective functional $J$ with respect to the source term $s$. The state variable $u$ is linked to the control variable $s$ through the state equation, which acts as a constraint. We employ the method of Lagrange multipliers to handle this constraint.\n\nLet the state space be an appropriate function space for $u$, the control space be $L^{2}(\\Omega\\times(0,T))$ for $s$, and the dual space for the constraint be the space for the Lagrange multiplier $p$, often called the adjoint state. The Lagrangian functional $\\mathcal{L}(u,s,p)$ is defined as:\n$$\n\\mathcal{L}(u,s,p) = J(u) - \\int_{0}^{T}\\!\\!\\int_{\\Omega} \\left(u_{t} - \\nabla \\cdot (\\kappa \\nabla u) - s\\right) p \\,dx\\,dt\n$$\nSince the state $u$ is uniquely determined by $s$ (for a fixed $u_0$), we can consider $J$ as a functional of $s$ alone, i.e., $j(s) = J(u(s))$. For any choice of $p$, if $u(s)$ satisfies the state equation, the integral term in the Lagrangian is zero, so $j(s) = \\mathcal{L}(u(s), s, p)$.\n\nWe wish to compute the Gâteaux derivative of $j(s)$ in an arbitrary direction $\\delta s \\in L^{2}(\\Omega\\times(0,T))$. Let $u(s)$ be the solution corresponding to $s$, and let $\\delta u$ be the Gâteaux derivative of the solution map $u(s)$ in the direction $\\delta s$. Applying the chain rule to $\\mathcal{L}(u(s), s, p)$, we get:\n$$\nDj(s)[\\delta s] = \\frac{\\partial \\mathcal{L}}{\\partial u}(u,s,p)[\\delta u] + \\frac{\\partial \\mathcal{L}}{\\partial s}(u,s,p)[\\delta s]\n$$\nThe core of the adjoint method is to choose the adjoint state $p$ such that the first term, which depends on the unknown sensitivity $\\delta u$, vanishes. This choice will define the adjoint equation. Let us compute the partial Gâteaux derivative of $\\mathcal{L}$ with respect to $u$ in a direction $\\delta u$:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial u}[\\delta u] = \\frac{d}{d\\epsilon}\\bigg|_{\\epsilon=0} \\mathcal{L}(u+\\epsilon\\delta u, s, p) = \\int_{0}^{T}\\!\\!\\int_{\\Omega} (u-d)\\delta u \\,dx\\,dt - \\int_{0}^{T}\\!\\!\\int_{\\Omega} \\left(\\delta u_{t} - \\nabla \\cdot (\\kappa \\nabla \\delta u)\\right) p \\,dx\\,dt\n$$\nTo eliminate $\\delta u$, we use integration by parts on the second term to transfer the derivatives onto $p$.\nFirst, we integrate the time-derivative term by parts with respect to time $t$:\n$$\n-\\int_{0}^{T}\\!\\!\\int_{\\Omega} \\delta u_{t} p \\,dx\\,dt = -\\int_{\\Omega} \\left( [\\delta u(x,t) p(x,t)]_{t=0}^{t=T} - \\int_{0}^{T} \\delta u(x,t) p_{t}(x,t) \\,dt \\right) dx\n$$\n$$\n= -\\int_{\\Omega} \\delta u(x,T) p(x,T) \\,dx + \\int_{\\Omega} \\delta u(x,0) p(x,0) \\,dx + \\int_{0}^{T}\\!\\!\\int_{\\Omega} \\delta u p_{t} \\,dx\\,dt\n$$\nThe variation $\\delta u$ corresponds to a variation $\\delta s$ in the source term, while the initial condition $u_0$ is fixed. Thus, the initial condition for $\\delta u$ is $\\delta u(x,0)=0$. The term $\\int_{\\Omega} \\delta u(x,0) p(x,0) \\,dx$ vanishes.\n\nNext, we integrate the diffusion term by parts with respect to the spatial variables. Using Green's first identity, $\\int_{\\Omega} (\\nabla\\cdot\\mathbf{F})\\psi \\,dx = \\int_{\\partial\\Omega} (\\mathbf{F}\\cdot\\mathbf{n})\\psi \\,dS - \\int_{\\Omega} \\mathbf{F}\\cdot\\nabla\\psi \\,dx$, with $\\mathbf{F} = \\kappa\\nabla\\delta u$ and $\\psi=p$, we get:\n$$\n\\int_{\\Omega} (\\nabla \\cdot (\\kappa \\nabla \\delta u)) p \\,dx = \\int_{\\partial\\Omega} p (\\kappa \\nabla \\delta u \\cdot \\mathbf{n}) \\,dS - \\int_{\\Omega} \\kappa \\nabla \\delta u \\cdot \\nabla p \\,dx\n$$\nTo make the boundary integral vanish for arbitrary $\\delta u$, we impose a homogeneous Dirichlet boundary condition on the adjoint state, $p(x,t)=0$ for $(x,t) \\in \\partial\\Omega \\times (0,T)$. Then, applying integration by parts again:\n$$\n- \\int_{\\Omega} \\kappa \\nabla \\delta u \\cdot \\nabla p \\,dx = \\int_{\\Omega} \\delta u (\\nabla \\cdot (\\kappa \\nabla p)) \\,dx - \\int_{\\partial\\Omega} \\delta u (\\kappa \\nabla p \\cdot \\mathbf{n}) \\,dS\n$$\nThe state $u$ satisfies $u=0$ on $\\partial\\Omega \\times(0,T)$, so its variation $\\delta u$ must also be zero on the boundary. Thus, the second boundary integral also vanishes. Combining these results gives:\n$$\n\\int_{0}^{T}\\!\\!\\int_{\\Omega} (\\nabla \\cdot (\\kappa \\nabla \\delta u)) p \\,dx\\,dt = \\int_{0}^{T}\\!\\!\\int_{\\Omega} \\delta u (\\nabla \\cdot (\\kappa \\nabla p)) \\,dx\\,dt\n$$\nSubstituting these back into the expression for $\\frac{\\partial \\mathcal{L}}{\\partial u}[\\delta u]$:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial u}[\\delta u] = \\int_{0}^{T}\\!\\!\\int_{\\Omega} (u-d)\\delta u \\,dx\\,dt - \\left( \\int_{0}^{T}\\!\\!\\int_{\\Omega} \\delta u p_{t} \\,dx\\,dt - \\int_{\\Omega} \\delta u(x,T) p(x,T) \\,dx + \\int_{0}^{T}\\!\\!\\int_{\\Omega} \\delta u (\\nabla \\cdot (\\kappa\\nabla p)) \\,dx\\,dt \\right)\n$$\nRe-grouping terms with $\\delta u$:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial u}[\\delta u] = -\\int_{0}^{T}\\!\\!\\int_{\\Omega} \\left( -p_{t} - \\nabla \\cdot (\\kappa \\nabla p) - (u-d) \\right) \\delta u \\,dx\\,dt + \\int_{\\Omega} \\delta u(x,T) p(x,T) \\,dx\n$$\nFor this expression to be zero for any admissible variation $\\delta u$, the integrands multiplying $\\delta u$ must be zero. This yields the adjoint equation for $p$:\n$$\n\\begin{cases}\n-p_{t}(x,t)-\\nabla \\cdot \\big(\\kappa(x)\\nabla p(x,t)\\big) = u(x,t)-d(x,t), & (x,t)\\in \\Omega \\times (0,T)\\\\\np(x,t)=0, & (x,t)\\in \\partial \\Omega \\times (0,T)\\\\\np(x,T)=0, & x\\in \\Omega\n\\end{cases}\n$$\nThis is a well-posed backward-in-time linear parabolic equation. With this choice of $p$, we have $\\frac{\\partial \\mathcal{L}}{\\partial u}[\\delta u] = 0$.\n\nNow the Gâteaux derivative of $j(s)$ simplifies to:\n$$\nDj(s)[\\delta s] = \\frac{\\partial \\mathcal{L}}{\\partial s}(u,s,p)[\\delta s] = \\frac{d}{d\\epsilon}\\bigg|_{\\epsilon=0} \\mathcal{L}(u, s+\\epsilon\\delta s, p)\n$$\n$$\n= \\frac{d}{d\\epsilon}\\bigg|_{\\epsilon=0} \\left( J(u) - \\int_{0}^{T}\\!\\!\\int_{\\Omega} \\left(u_{t} - \\nabla \\cdot (\\kappa \\nabla u) - (s+\\epsilon\\delta s)\\right) p \\,dx\\,dt \\right)\n$$\n$$\n= - \\int_{0}^{T}\\!\\!\\int_{\\Omega} (-\\delta s) p \\,dx\\,dt = \\int_{0}^{T}\\!\\!\\int_{\\Omega} p \\cdot \\delta s \\,dx\\,dt\n$$\nThe $L^{2}(\\Omega\\times(0,T))$-gradient, which we denote by $\\nabla_s J(s)$, is defined by the Riesz representation theorem through the inner product:\n$$\nDj(s)[\\delta s] = \\langle \\nabla_s J(s), \\delta s \\rangle_{L^{2}(\\Omega\\times(0,T))} = \\int_{0}^{T}\\!\\!\\int_{\\Omega} (\\nabla_s J(s)) \\delta s \\,dx\\,dt\n$$\nComparing this with our derived expression for $Dj(s)[\\delta s]$, we identify the gradient as the adjoint state $p(x,t)$.\n$$\n\\nabla_s J(s) = p(x,t)\n$$\nwhere $u(x,t)$ is the solution to the forward problem and $p(x,t)$ is the solution to the adjoint problem defined above.", "answer": "$$\n\\boxed{p(x,t)}\n$$", "id": "3361153"}, {"introduction": "Moving from continuous theory to numerical implementation requires deriving a discrete adjoint operator that is consistent with the chosen discretization of the forward problem. This practice explores this crucial step for a linear advection equation discretized with an upwind scheme. By applying the principle of discrete summation-by-parts, you will derive the corresponding discrete adjoint stencil [@problem_id:3361154] and uncover a fundamental property: the reversal of information flow between the forward and adjoint systems.", "problem": "Consider the linear advection partial differential equation (PDE) $u_{t} + a\\,u_{x} = 0$ on the spatial interval $[0,1]$ for time $t \\in [0,T]$, with a constant advection speed $a>0$. The forward problem is posed with an inflow Dirichlet boundary condition $u(0,t) = g(t)$ and an initial condition $u(x,0) = u_{0}(x)$. Discretize space using a uniform grid $x_{i} = i h$ with $h = 1/N$, indices $i = 0,1,\\dots,N$, and define the semi-discrete method-of-lines system for the interior degrees-of-freedom $i=1,\\dots,N$ using the first-order upwind spatial differencing appropriate for $a>0$:\n$$\n\\frac{d u_{i}}{dt} + \\frac{a}{h}\\left(u_{i} - u_{i-1}\\right) = 0,\\quad i=1,\\dots,N,\n$$\nwith $u_{0}(t)$ provided by the inflow boundary data $g(t)$.\n\nLet the discrete inner product be the mass-lumped $L^{2}$ inner product $\\langle p,u\\rangle = h \\sum_{i=1}^{N} p_{i}\\,u_{i}$ on the interior degrees-of-freedom. Define the discrete adjoint operator $A^{\\ast}$ of the spatial operator $A$ by the relation $\\langle p, A u\\rangle = \\langle A^{\\ast} p, u\\rangle$ for all interior vectors $p$ and $u$. Assume an objective functional $J(u) = h \\sum_{i=1}^{N} q_{i}\\,u_{i}(T)$ with a given terminal weight vector $q$, and consider the discrete adjoint state $p(t)$ governed backward in time by\n$$\n\\frac{d p}{dt} - A^{\\ast} p = 0,\\quad t \\in [0,T],\n$$\nwith terminal condition $p(T) = q$ and an appropriate adjoint boundary condition to be determined.\n\nStarting from the definition of the discrete adjoint with the given inner product and the stated forward upwind stencil, derive the explicit interior stencil for the adjoint spatial operator $A^{\\ast}$, i.e., the formula for $(A^{\\ast} p)_{j}$ for $j=1,\\dots,N-1$. Additionally, explain, based on first principles and the discrete summation-by-parts identity, how the adjoint boundary condition is imposed at the outflow boundary for $a>0$ and why the inflow/outflow roles are reversed relative to the forward problem.\n\nYour final answer must be the single closed-form analytic expression for the interior adjoint stencil $(A^{\\ast} p)_{j}$ for $1 \\le j \\le N-1$. No numerical approximation is required.", "solution": "The derivation requires finding the discrete adjoint operator $A^{\\ast}$ from the given discrete forward operator $A$ and the specified inner product, $\\langle p,u\\rangle = h \\sum_{i=1}^{N} p_{i}\\,u_{i}$. The defining relationship is $\\langle p, A u \\rangle = \\langle A^{\\ast} p, u \\rangle$ for all vectors $p$ and $u$ representing the interior degrees of freedom.\n\nFirst, we identify the action of the spatial operator $A$ on an interior vector $u = (u_1, u_2, \\dots, u_N)^T$. From the semi-discrete system $\\frac{d u_i}{dt} + (Au)_i = 0$, we can write the components of $Au$ by isolating the spatial part. The term with $u_0$ is treated as an inhomogeneous source term for the system on the interior nodes. For the purpose of defining the operator $A$ acting on interior vectors, we consider its action assuming homogeneous boundary conditions ($u_0=0$). The components of $Au$ are:\n$$\n(A u)_i = \\frac{a}{h} (u_i - u_{i-1})\n$$\nwhere for $i=1$, we have $(Au)_1 = \\frac{a}{h}(u_1 - u_0) = \\frac{a}{h}u_1$.\n\nNow, we compute the inner product $\\langle p, A u \\rangle$:\n$$\n\\langle p, A u \\rangle = h \\sum_{i=1}^{N} p_i (A u)_i = h \\left[ p_1 (A u)_1 + \\sum_{i=2}^{N} p_i (A u)_i \\right]\n$$\nSubstituting the expressions for $(Au)_i$:\n$$\n\\langle p, A u \\rangle = h \\left[ p_1 \\left(\\frac{a}{h} u_1\\right) + \\sum_{i=2}^{N} p_i \\left(\\frac{a}{h} (u_i - u_{i-1})\\right) \\right] = a \\left[ p_1 u_1 + \\sum_{i=2}^{N} p_i u_i - \\sum_{i=2}^{N} p_i u_{i-1} \\right]\n$$\nTo rearrange this into the form $\\langle A^{\\ast} p, u \\rangle$, we need to group the terms by the components of $u$. We perform a discrete summation by parts by re-indexing the last sum. Let $j=i-1$, so $i=j+1$. The sum becomes $\\sum_{j=1}^{N-1} p_{j+1} u_j$.\n$$\n\\langle p, A u \\rangle = a \\left[ p_1 u_1 + \\sum_{i=2}^{N} p_i u_i - \\sum_{j=1}^{N-1} p_{j+1} u_j \\right]\n$$\nWe now collect the coefficients for each $u_j$:\n$$\n\\langle p, A u \\rangle = a \\left[ (p_1 - p_2)u_1 + (p_2 - p_3)u_2 + \\dots + (p_{N-1} - p_N)u_{N-1} + p_N u_N \\right]\n$$\n$$\n\\langle p, A u \\rangle = a \\sum_{j=1}^{N-1} (p_j - p_{j+1}) u_j + a p_N u_N\n$$\nWe must equate this to $\\langle A^{\\ast} p, u \\rangle = h \\sum_{j=1}^{N} (A^{\\ast} p)_j u_j$. By comparing the coefficients of each $u_j$, we can identify the components of $A^{\\ast}p$:\n\nFor the interior nodes $j = 1, \\dots, N-1$:\n$$\nh (A^{\\ast} p)_j = a(p_j - p_{j+1}) \\implies (A^{\\ast} p)_j = \\frac{a}{h}(p_j - p_{j+1})\n$$\nThis is the explicit formula for the interior stencil of the adjoint operator. It is a first-order downwind scheme (a forward difference), whereas the original operator $A$ was an upwind scheme (a backward difference).\n\nFor the boundary node $j=N$:\n$$\nh (A^{\\ast} p)_N = a p_N \\implies (A^{\\ast} p)_N = \\frac{a}{h} p_N\n$$\nThis stencil at the boundary is equivalent to applying the interior stencil $(A^{\\ast} p)_N = \\frac{a}{h}(p_N - p_{N+1})$ and enforcing a homogeneous Dirichlet condition $p_{N+1}=0$ at a ghost point.\n\nThe reversal of information flow is a fundamental consequence of adjoints. The forward PDE $u_t + au_x=0$ with $a>0$ has characteristics moving in the positive $x$ direction. Information flows from left to right, making $x=0$ an inflow boundary and $x=1$ an outflow boundary. The adjoint PDE is approximately $p_t - ap_x \\approx 0$ (since the discrete adjoint stencil approximates $-a \\frac{\\partial}{\\partial x}$ and the adjoint time evolution is $\\frac{dp}{dt} = A^*p$). The characteristics for the adjoint equation move in the negative $x$ direction. Information flows from right to left. Consequently, the forward outflow boundary ($x=1$) becomes the adjoint inflow boundary, where a boundary condition (derived from summation by parts) is required. The forward inflow boundary ($x=0$) becomes the adjoint outflow boundary.", "answer": "$$\n\\boxed{\\frac{a}{h}(p_{j} - p_{j+1})}\n$$", "id": "3361154"}, {"introduction": "A correct theoretical derivation is only half the battle; a correct implementation is essential. This exercise guides you through the full workflow for a nonlinear problem: implementing a state solver, an adjoint solver, and calculating the gradient. Most importantly, it introduces the Taylor test [@problem_id:3361125], an indispensable verification technique to confirm that your computed adjoint gradient is correct by checking that it satisfies the first-order Taylor expansion of the objective functional.", "problem": "Consider a Partial Differential Equation (PDE)-constrained optimization problem on the spatial interval $(0,1)$, with the state variable $u(x)$ and a distributed control $m(x)$ entering as a source term. The governing nonlinear PDE is\n$$\n- \\frac{d^2 u}{dx^2} + \\beta\\, u(x)^3 = m(x), \\quad x \\in (0,1),\n$$\nsubject to homogeneous Dirichlet boundary conditions\n$$\nu(0) = 0, \\quad u(1) = 0.\n$$\nLet the desired state be $u_d(x)$, and consider the objective functional\n$$\nJ(u,m) = \\frac{1}{2} \\int_0^1 \\big(u(x) - u_d(x)\\big)^2 \\, dx + \\frac{\\gamma}{2} \\int_0^1 m(x)^2 \\, dx.\n$$\nStarting from the calculus of variations and the Lagrangian framework for PDE-constrained optimization, derive the necessary first-order optimality conditions by introducing an adjoint variable $\\lambda(x)$ and requiring stationarity of the Lagrangian with respect to $u$ and $m$. From these conditions, obtain the adjoint PDE and the gradient of $J$ with respect to $m$.\n\nDiscretize the problem on a uniform grid with $N$ points including boundaries $x_0 = 0$ and $x_{N-1} = 1$, with spacing $h = 1/(N-1)$. Use second-order centered finite differences for $-d^2u/dx^2$ on the $N-2$ interior nodes, and enforce $u_0=u_{N-1}=0$. Approximate all integrals by Riemann sums with uniform weight $h$ over the interior nodes. For the nonlinear PDE, implement a Newton method with a backtracking line search to solve for the discrete state $u$ at the interior nodes. Then solve the discrete adjoint equation to obtain the discrete adjoint variable at the interior nodes. Use these to form the discrete gradient with respect to $m$.\n\nImplement a verification procedure of the adjoint gradient using random perturbations and the Taylor remainder test. Specifically, for a fixed baseline control $m$ and a random perturbation direction $\\delta m$ normalized in the discrete $L^2$ inner product, compute\n$$\nR(\\varepsilon) = J\\big(m + \\varepsilon \\delta m\\big) - J(m) - \\varepsilon \\langle \\nabla J(m), \\delta m \\rangle_h,\n$$\nwhere $\\langle a, b \\rangle_h = h \\sum_i a_i b_i$ denotes the discrete inner product over interior nodes. Demonstrate through computation that $R(\\varepsilon)$ scales quadratically with $\\varepsilon$ for sufficiently small $\\varepsilon$, i.e., $R(\\varepsilon) = \\mathcal{O}(\\varepsilon^2)$, by estimating the slope $p$ from a linear fit of $\\log R$ versus $\\log \\varepsilon$.\n\nYour program must construct the following test suite of parameter sets and report, for each test, the estimated order $p$ obtained from the fit. In all tests, use $u_d(x) = \\sin(\\pi x)$ and the baseline control $m(x) = \\sin(2\\pi x) + 0.5 \\sin(\\pi x)$ evaluated at the interior nodes. Generate the random perturbation $\\delta m$ by drawing independent standard normal entries and normalizing to unit discrete $L^2$ norm; use the specified random seed per test for reproducibility. For each test, use the same set of $\\varepsilon$ values\n$$\n\\varepsilon \\in \\left\\{10^{-1}, \\; 5 \\times 10^{-2}, \\; 2.5 \\times 10^{-2}, \\; 1.25 \\times 10^{-2} \\right\\}.\n$$\n\nTest suite:\n- Test A (happy path): $N = 128$, $\\beta = 1$, $\\gamma = 10^{-3}$, random seed $s = 7$.\n- Test B (boundary case: linear state equation): $N = 96$, $\\beta = 0$, $\\gamma = 10^{-2}$, random seed $s = 13$.\n- Test C (edge case: stronger nonlinearity and no regularization): $N = 160$, $\\beta = 5$, $\\gamma = 0$, random seed $s = 21$.\n\nYour program should produce a single line of output containing the estimated orders $p$ for Tests A, B, and C, in that order, rounded to three decimal places, as a comma-separated list enclosed in square brackets (e.g., $[2.000,2.000,2.000]$). No physical units are involved. Angles, if any, must be in radians, but this problem does not involve angles. All numerical outputs must be floats.", "solution": "The solution involves first deriving the continuous and discrete optimality conditions and then outlining the numerical implementation and verification procedure.\n\n#### 1. Continuous Optimality Conditions (Lagrangian Method)\nWe introduce the Lagrangian functional $\\mathcal{L}(u, m, \\lambda)$, where $\\lambda(x)$ is the adjoint variable (Lagrange multiplier) that enforces the PDE constraint:\n$$\n\\mathcal{L}(u, m, \\lambda) = J(u, m) + \\int_0^1 \\lambda(x) \\left( - \\frac{d^2 u}{dx^2} + \\beta u^3 - m \\right) dx.\n$$\nThe first-order necessary conditions for optimality are obtained by setting the Fréchet derivatives of $\\mathcal{L}$ with respect to each variable ($u$, $m$, $\\lambda$) to zero.\n\n- **Stationarity w.r.t. $\\lambda$**: $\\frac{\\delta \\mathcal{L}}{\\delta \\lambda} = 0$ simply recovers the original state equation:\n  $$ - \\frac{d^2 u}{dx^2} + \\beta u^3 = m, \\quad u(0)=u(1)=0. $$\n\n- **Stationarity w.r.t. $u$**: We require $\\frac{\\delta \\mathcal{L}}{\\delta u}(\\delta u) = 0$ for all admissible variations $\\delta u$.\n  $$ \\frac{\\delta \\mathcal{L}}{\\delta u}(\\delta u) = \\int_0^1 (u - u_d)\\delta u \\, dx + \\int_0^1 \\lambda \\left( - \\frac{d^2}{dx^2}(\\delta u) + 3\\beta u^2 \\delta u \\right) dx = 0. $$\n  Using integration by parts twice on the $-\\lambda \\frac{d^2}{dx^2}(\\delta u)$ term and using the fact that the variations $\\delta u$ must satisfy homogeneous boundary conditions ($\\delta u(0) = \\delta u(1) = 0$), we transfer the derivatives from $\\delta u$ to $\\lambda$. This requires imposing homogeneous boundary conditions on $\\lambda$ as well, $\\lambda(0)=\\lambda(1)=0$. The resulting integral is:\n  $$ \\int_0^1 \\left( (u - u_d) - \\frac{d^2 \\lambda}{dx^2} + 3\\beta u^2 \\lambda \\right) \\delta u \\, dx = 0. $$\n  Since this must hold for all $\\delta u$, the term in the parentheses must be zero, which gives the **adjoint equation**:\n  $$ - \\frac{d^2 \\lambda}{dx^2} + (3\\beta u^2) \\lambda = u - u_d, \\quad \\lambda(0)=\\lambda(1)=0. $$\n\n- **Stationarity w.r.t. $m$**: $\\frac{\\delta \\mathcal{L}}{\\delta m} = 0$. This gives the **gradient expression**. The derivative of the reduced functional $\\hat{J}(m) = J(u(m), m)$ is simply $\\frac{\\partial \\mathcal{L}}{\\partial m}$.\n  $$ \\nabla_m J(m) = \\frac{\\partial J}{\\partial m} + \\frac{\\partial}{\\partial m}\\int \\lambda(\\dots)dx = \\gamma m - \\lambda. $$\n\n#### 2. Discretization\nUsing a uniform grid with $N$ points, the variables become vectors on the $N-2$ interior nodes, denoted $\\mathbf{u}, \\mathbf{m}, \\boldsymbol{\\lambda} \\in \\mathbb{R}^{N-2}$. The second derivative is replaced by the $(N-2) \\times (N-2)$ finite difference matrix $\\mathbf{A}$.\n- **Discrete State Equation (Nonlinear System)**:\n  $$ \\mathbf{R}(\\mathbf{u}, \\mathbf{m}) \\equiv \\mathbf{A}\\mathbf{u} + \\beta \\mathbf{u}^3 - \\mathbf{m} = \\mathbf{0}, $$\n  where $\\mathbf{u}^3$ is element-wise.\n- **Discrete Adjoint Equation (Linear System)**:\n  $$ \\left( \\mathbf{A} + 3\\beta \\, \\text{diag}(\\mathbf{u}^2) \\right) \\boldsymbol{\\lambda} = \\mathbf{u} - \\mathbf{u}_d. $$\n  The matrix on the left is the Jacobian of the state residual $\\mathbf{R}$ with respect to $\\mathbf{u}$, denoted $\\mathbf{K}(\\mathbf{u})$.\n- **Discrete Gradient**:\n  $$ \\nabla_m J = \\gamma \\mathbf{m} - \\boldsymbol{\\lambda}. $$\n\n#### 3. Numerical Implementation Plan\n1.  **State Solver**: Solve the nonlinear system $\\mathbf{R}(\\mathbf{u}, \\mathbf{m}) = \\mathbf{0}$ for $\\mathbf{u}$ using Newton's method. Each iteration involves:\n    a. Computing the Jacobian matrix $\\mathbf{K}(\\mathbf{u}_k) = \\mathbf{A} + 3\\beta \\, \\text{diag}(\\mathbf{u}_k^2)$.\n    b. Solving the linear system $\\mathbf{K}(\\mathbf{u}_k) \\delta\\mathbf{u} = -\\mathbf{R}(\\mathbf{u}_k)$ for the update $\\delta\\mathbf{u}$.\n    c. Updating the solution $\\mathbf{u}_{k+1} = \\mathbf{u}_k + \\alpha \\delta\\mathbf{u}$, where $\\alpha$ is found via a backtracking line search to ensure sufficient decrease in the residual norm $\\|\\mathbf{R}\\|$.\n2.  **Adjoint Solver**: After finding the converged state $\\mathbf{u}$, solve the linear system $\\mathbf{K}(\\mathbf{u})\\boldsymbol{\\lambda} = \\mathbf{u} - \\mathbf{u}_d$ for the adjoint vector $\\boldsymbol{\\lambda}$.\n3.  **Gradient Calculation**: Compute the gradient vector $\\nabla_m J = \\gamma \\mathbf{m} - \\boldsymbol{\\lambda}$.\n\n#### 4. Gradient Verification (Taylor Test)\nThe correctness of the computed gradient $\\nabla_m J$ is verified by checking that it satisfies the first-order Taylor expansion of the objective functional. The remainder term $R(\\varepsilon)$ is defined as:\n$$ R(\\varepsilon) = J(m + \\varepsilon \\delta m) - J(m) - \\varepsilon \\langle \\nabla J(m), \\delta m \\rangle_h $$\nFor a correctly computed gradient, Taylor's theorem guarantees that $R(\\varepsilon) = \\mathcal{O}(\\varepsilon^2)$. To verify this numerically, we compute $R(\\varepsilon)$ for a sequence of small $\\varepsilon$. A linear fit on a log-log plot of $|R(\\varepsilon)|$ versus $\\varepsilon$ should yield a slope $p \\approx 2$. The algorithm is:\n1.  For a given baseline control $\\mathbf{m}$, compute the baseline state $\\mathbf{u}$, objective $J(m)$, and gradient $\\nabla J(m)$.\n2.  Generate a random perturbation direction $\\delta \\mathbf{m}$ and normalize it.\n3.  Compute the directional derivative $g_d = \\langle \\nabla J(m), \\delta \\mathbf{m} \\rangle_h$.\n4.  For each $\\varepsilon$ in the test set:\n    a. Compute $J(m + \\varepsilon \\delta m)$ by first solving for the perturbed state $\\mathbf{u}(m+\\varepsilon\\delta m)$.\n    b. Calculate $R(\\varepsilon) = J(m + \\varepsilon \\delta m) - J(m) - \\varepsilon g_d$.\n5.  Estimate the order of convergence $p$ by finding the slope of $\\log|R(\\varepsilon)|$ vs. $\\log \\varepsilon$.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the adjoint verification for the given test suite.\n    \"\"\"\n\n    class AdjointVerifier:\n        \"\"\"\n        Encapsulates the logic for the PDE-constrained optimization problem,\n        including state/adjoint solvers and gradient verification.\n        \"\"\"\n        def __init__(self, N, beta, gamma):\n            self.N = N\n            self.beta = beta\n            self.gamma = gamma\n            \n            if N <= 2:\n                raise ValueError(\"N must be greater than 2 for interior points.\")\n                \n            self.h = 1.0 / (N - 1)\n            self.n_interior = N - 2\n            \n            # Grid (interior points only)\n            self.x_interior = np.linspace(0, 1, N)[1:-1]\n            \n            # Desired state on the interior grid\n            self.u_d = np.sin(np.pi * self.x_interior)\n            \n            # Finite difference matrix for -d^2/dx^2 with Dirichlet BCs\n            self._construct_laplacian_matrix()\n\n        def _construct_laplacian_matrix(self):\n            n = self.n_interior\n            h2_inv = 1.0 / self.h**2\n            \n            # A is a symmetric tridiagonal matrix\n            main_diag = np.full(n, 2.0 * h2_inv)\n            off_diag = np.full(n - 1, -1.0 * h2_inv)\n            \n            self.A = np.diag(main_diag) + np.diag(off_diag, k=1) + np.diag(off_diag, k=-1)\n\n        def solve_state(self, m, u_init=None, tol=1e-12, max_iter=50):\n            \"\"\"\n            Solves the nonlinear state equation using Newton's method.\n            \"\"\"\n            u = np.zeros(self.n_interior) if u_init is None else u_init.copy()\n            \n            for _ in range(max_iter):\n                # Residual: R(u) = A*u + beta*u^3 - m\n                R = self.A @ u + self.beta * u**3 - m\n                norm_R = np.linalg.norm(R)\n                \n                if norm_R < tol:\n                    return u\n                \n                # Jacobian: K(u) = A + 3*beta*diag(u^2)\n                K = self.A + np.diag(3 * self.beta * u**2)\n                \n                # Solve for Newton update: K * du = -R\n                du = np.linalg.solve(K, -R)\n                \n                # Backtracking line search\n                alpha = 1.0\n                u_new = u + alpha * du\n                R_new = self.A @ u_new + self.beta * u_new**3 - m\n                \n                while np.linalg.norm(R_new) >= norm_R and alpha > 1e-8:\n                    alpha *= 0.5\n                    u_new = u + alpha * du\n                    R_new = self.A @ u_new + self.beta * u_new**3 - m\n                \n                u = u_new\n                \n                if np.linalg.norm(alpha * du) < tol:\n                    return u\n                    \n            raise RuntimeError(\"Newton solver for state equation did not converge.\")\n\n        def calculate_objective(self, u, m):\n            \"\"\"Computes the discrete objective functional J(u, m).\"\"\"\n            cost_u = 0.5 * self.h * np.sum((u - self.u_d)**2)\n            cost_m = 0.5 * self.gamma * self.h * np.sum(m**2)\n            return cost_u + cost_m\n\n        def solve_adjoint(self, u):\n            \"\"\"Solves the linear adjoint equation.\"\"\"\n            # Adjoint system matrix: K(u)^T = K(u) since it's symmetric\n            K = self.A + np.diag(3 * self.beta * u**2)\n            \n            # Right-hand side\n            rhs = u - self.u_d\n            \n            # Solve K * lambda = rhs\n            adj_lambda = np.linalg.solve(K, rhs)\n            return adj_lambda\n\n        def calculate_gradient(self, m, adj_lambda):\n            \"\"\"Computes the gradient of J w.r.t. m.\"\"\"\n            # grad_J = gamma*m - lambda\n            return self.gamma * m - adj_lambda\n\n        def verify(self, seed):\n            \"\"\"Performs the Taylor remainder test to verify the gradient.\"\"\"\n            # Baseline control\n            m_base = np.sin(2 * np.pi * self.x_interior) + 0.5 * np.sin(np.pi * self.x_interior)\n            \n            # --- Baseline Calculations ---\n            u_base = self.solve_state(m_base)\n            J_base = self.calculate_objective(u_base, m_base)\n            \n            # --- Adjoint Gradient Calculation ---\n            adj_base = self.solve_adjoint(u_base)\n            grad_base = self.calculate_gradient(m_base, adj_base)\n            \n            # --- Taylor Test ---\n            rng = np.random.default_rng(seed)\n            dm = rng.standard_normal(self.n_interior)\n            \n            # Normalize dm in the discrete L^2 inner product\n            norm_dm_h = np.sqrt(self.h * np.sum(dm**2))\n            dm_normalized = dm / norm_dm_h\n            \n            # Directional derivative: <grad, dm>_h\n            grad_proj = self.h * np.sum(grad_base * dm_normalized)\n            \n            epsilons = np.array([1e-1, 5e-2, 2.5e-2, 1.25e-2])\n            remainders = []\n            \n            for eps in epsilons:\n                m_pert = m_base + eps * dm_normalized\n                # Warm-start the Newton solver with the baseline solution\n                u_pert = self.solve_state(m_pert, u_init=u_base)\n                J_pert = self.calculate_objective(u_pert, m_pert)\n                \n                # Taylor remainder: R(eps) = J(m+eps*dm) - J(m) - eps*<grad,dm>\n                remainder = J_pert - J_base - eps * grad_proj\n                remainders.append(remainder)\n                \n            log_eps = np.log(epsilons)\n            log_R = np.log(np.abs(remainders))\n            \n            # Fit a line: log(|R|) = p * log(eps) + c. The slope p is the order.\n            p, _ = np.polyfit(log_eps, log_R, 1)\n            \n            return p\n\n    test_cases = [\n        # (N, beta, gamma, seed)\n        (128, 1.0, 1e-3, 7),  # Test A\n        (96, 0.0, 1e-2, 13),  # Test B\n        (160, 5.0, 0.0, 21),  # Test C\n    ]\n\n    results = []\n    for N, beta, gamma, seed in test_cases:\n        verifier = AdjointVerifier(N=N, beta=beta, gamma=gamma)\n        order_p = verifier.verify(seed=seed)\n        results.append(order_p)\n\n    print(f\"[{','.join(f'{r:.3f}' for r in results)}]\")\n\nsolve()\n```", "id": "3361125"}]}