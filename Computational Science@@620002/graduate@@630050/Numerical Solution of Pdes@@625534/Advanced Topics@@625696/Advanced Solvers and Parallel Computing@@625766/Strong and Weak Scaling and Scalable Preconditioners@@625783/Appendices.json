{"hands_on_practices": [{"introduction": "The theoretical cornerstone of scalable two-level preconditioners, such as domain decomposition methods, is the coarse space's ability to handle global error components. This first exercise provides a model for what happens when this principle is violated. By analyzing a preconditioner with a deficient coarse space, you will derive the resulting asymptotic growth in iteration count, connecting a theoretical flaw to a concrete, non-scalable performance signature [@problem_id:3449762].", "problem": "Consider the numerical solution of a symmetric positive-definite discretization of a second-order linear elliptic partial differential equation in $d$ spatial dimensions on a hypercube of side length $L$, with homogeneous Dirichlet boundary conditions. Let the discrete operator be denoted by $A \\in \\mathbb{R}^{n \\times n}$, arising from a conforming finite element method on a quasi-uniform mesh of spacing $h$. Assume a standard two-level domain decomposition preconditioner $M^{-1}$ is employed, built from $P$ nonoverlapping subdomains of equal diameter $H$ (up to shape regularity), together with a coarse space constructed by energy-minimizing extensions of subdomain traces. The preconditioner is used within the Preconditioned Conjugate Gradient (PCG) method.\n\nThe coarse space is intended to span the near-nullspace of the local Neumann operators to ensure scalability, but suppose it is mis-specified: specifically, it fails to include $r$ rigid body modes per subdomain that belong to the near-nullspace associated with the underlying continuous differential operator (for example, translations and rotations in linear elasticity). As a model for the impact of this deficiency on scalability, assume that:\n\n1. Along the subspace $S$ spanned by the missing coarse components, the effective two-level method behaves like a one-level method with a global Poincaré-type constant set by the subdomain graph diameter. More precisely, there exist positive constants $c_{1}$ and $C_{1}$ independent of $P$, $h$, and $H$, such that the extremal eigenvalues of the preconditioned operator $M^{-1}A$ satisfy\n$$\n\\lambda_{\\max}(M^{-1}A) \\leq C_{1}, \\quad \\lambda_{\\min}(M^{-1}A) \\geq c_{1} \\, P^{-2/d}.\n$$\nThis lower bound models that the smallest Rayleigh quotient on $S$ scales like the inverse square of the number of subdomains along a coordinate direction, by invoking the discrete Poincaré inequality on the subdomain graph.\n\n2. Outside $S$, spectral equivalence with bounds independent of $P$ holds, so the growth of the condition number with $P$ is dominated by the behavior on $S$.\n\nAdopt the standard energy-norm PCG error reduction estimate: after $m$ iterations,\n$$\n\\frac{\\|e_{m}\\|_{A}}{\\|e_{0}\\|_{A}} \\leq 2 \\left( \\frac{\\sqrt{\\kappa} - 1}{\\sqrt{\\kappa} + 1} \\right)^{m},\n$$\nwhere $\\kappa$ is the spectral condition number of $M^{-1}A$.\n\nDefine the strong scaling regime as fixing $n$ while increasing $P$ (so $H \\approx L \\, P^{-1/d}$ and $h$ is fixed), and the weak scaling regime as fixing the local degrees of freedom per subdomain (so $H/h$ is fixed) while increasing $P$ (so $L \\approx H \\, P^{1/d}$). Under both regimes, the assumption above implies the same asymptotic dependence of the condition number on $P$.\n\nDerive, from first principles and the given assumptions, the asymptotic expression for the number of PCG iterations $m_{\\varepsilon}(P)$ required to reduce the energy-norm error by a prescribed factor $\\varepsilon \\in (0,1)$, in terms of $P$, $d$, $\\varepsilon$, and the constants $c_{1}$ and $C_{1}$. Express your final answer as a single closed-form analytic expression with no units. No rounding is required.\n\nAdditionally, propose mathematically grounded diagnostics that could autonomously detect the coarse space deficiency described (missing rigid body modes), based on quantities accessible during a parallel solve, and explain why these diagnostics would reveal the problem under both strong and weak scaling.\n\nYour final answer must be only the single analytic expression for $m_{\\varepsilon}(P)$ derived from the above model.", "solution": "The problem asks for two things: first, to derive an asymptotic expression for the number of Preconditioned Conjugate Gradient (PCG) iterations, $m_{\\varepsilon}(P)$, required to achieve a specified error reduction, and second, to propose diagnostics for detecting the coarse space deficiency described in the problem statement.\n\nFirst, we derive the expression for $m_{\\varepsilon}(P)$.\nThe convergence of the PCG method is governed by the spectral condition number, $\\kappa$, of the preconditioned operator $M^{-1}A$. The condition number is defined as the ratio of the largest to the smallest eigenvalue:\n$$\n\\kappa = \\kappa(M^{-1}A) = \\frac{\\lambda_{\\max}(M^{-1}A)}{\\lambda_{\\min}(M^{-1}A)}\n$$\nThe problem provides bounds for the extremal eigenvalues of $M^{-1}A$. It is stated that the behavior on a specific subspace $S$ spanned by missing coarse components dominates the growth of the condition number. The given bounds are:\n$$\n\\lambda_{\\max}(M^{-1}A) \\leq C_{1}\n$$\n$$\n\\lambda_{\\min}(M^{-1}A) \\geq c_{1} \\, P^{-2/d}\n$$\nwhere $C_{1}$ and $c_{1}$ are positive constants independent of the number of subdomains $P$, the mesh spacing $h$, and the subdomain size $H$. The problem states that this dependence on $P$ holds for both strong and weak scaling regimes.\n\nTo find the asymptotic behavior of the condition number as $P$ grows, we use the upper bound for $\\lambda_{\\max}$ and the lower bound for $\\lambda_{\\min}$. This yields an upper bound on $\\kappa$, which determines the worst-case convergence rate.\n$$\n\\kappa(P) \\leq \\frac{C_{1}}{c_{1} \\, P^{-2/d}} = \\frac{C_{1}}{c_{1}} P^{2/d}\n$$\nLet's denote the constant pre-factor as $C = \\frac{C_1}{c_1}$. The asymptotic behavior of the condition number is thus $\\kappa(P) \\propto P^{2/d}$.\n\nThe required number of iterations, $m$, to reduce the energy-norm of the error by a factor $\\varepsilon \\in (0,1)$, i.e., $\\frac{\\|e_{m}\\|_{A}}{\\|e_{0}\\|_{A}} \\leq \\varepsilon$, is given by the standard PCG error estimate:\n$$\n2 \\left( \\frac{\\sqrt{\\kappa} - 1}{\\sqrt{\\kappa} + 1} \\right)^{m} \\leq \\varepsilon\n$$\nWe need to solve this inequality for $m$. Let $m_{\\varepsilon}(P)$ be the smallest integer $m$ satisfying this condition. We seek an asymptotic expression for $m_{\\varepsilon}(P)$ for large $P$.\nRearranging the inequality, we get:\n$$\n\\left( \\frac{\\sqrt{\\kappa} - 1}{\\sqrt{\\kappa} + 1} \\right)^{m} \\leq \\frac{\\varepsilon}{2}\n$$\nTaking the natural logarithm of both sides. Since $\\frac{\\sqrt{\\kappa} - 1}{\\sqrt{\\kappa} + 1} < 1$, its logarithm is negative, so the inequality sign reverses:\n$$\nm \\ln\\left( \\frac{\\sqrt{\\kappa} - 1}{\\sqrt{\\kappa} + 1} \\right) \\leq \\ln\\left( \\frac{\\varepsilon}{2} \\right)\n$$\n$$\nm \\geq \\frac{\\ln(\\varepsilon/2)}{\\ln\\left( \\frac{\\sqrt{\\kappa} - 1}{\\sqrt{\\kappa} + 1} \\right)} = \\frac{-\\ln(2/\\varepsilon)}{-\\ln\\left( \\frac{\\sqrt{\\kappa} + 1}{\\sqrt{\\kappa} - 1} \\right)} = \\frac{\\ln(2/\\varepsilon)}{\\ln\\left( \\frac{\\sqrt{\\kappa} + 1}{\\sqrt{\\kappa} - 1} \\right)}\n$$\nWe are interested in the asymptotic behavior as $P \\rightarrow \\infty$, which implies $\\kappa(P) \\rightarrow \\infty$. For large $\\kappa$, we can approximate the logarithmic term in the denominator. Let $x = 1/\\sqrt{\\kappa}$. As $\\kappa \\rightarrow \\infty$, $x \\rightarrow 0$. The term becomes $\\ln\\left( \\frac{1+x}{1-x} \\right)$.\nUsing the Taylor series expansion $\\ln(1+x) = x - \\frac{x^2}{2} + O(x^3)$ for small $x$:\n$$\n\\ln\\left( \\frac{1+x}{1-x} \\right) = \\ln(1+x) - \\ln(1-x) = \\left(x - \\frac{x^2}{2} + \\dots\\right) - \\left(-x - \\frac{x^2}{2} - \\dots\\right) = 2x + O(x^3)\n$$\nThus, for large $\\kappa$, we have the approximation:\n$$\n\\ln\\left( \\frac{\\sqrt{\\kappa} + 1}{\\sqrt{\\kappa} - 1} \\right) \\approx \\frac{2}{\\sqrt{\\kappa}}\n$$\nSubstituting this into the expression for $m$:\n$$\nm_{\\varepsilon}(P) \\approx \\frac{\\ln(2/\\varepsilon)}{2/\\sqrt{\\kappa(P)}} = \\frac{1}{2} \\ln\\left(\\frac{2}{\\varepsilon}\\right) \\sqrt{\\kappa(P)}\n$$\nNow, we substitute the asymptotic expression for the condition number, $\\kappa(P) \\approx \\frac{C_1}{c_1} P^{2/d}$:\n$$\nm_{\\varepsilon}(P) \\approx \\frac{1}{2} \\ln\\left(\\frac{2}{\\varepsilon}\\right) \\sqrt{\\frac{C_{1}}{c_{1}} P^{2/d}}\n$$\nSimplifying the expression gives the final asymptotic form for the number of iterations:\n$$\nm_{\\varepsilon}(P) \\approx \\frac{1}{2} \\sqrt{\\frac{C_{1}}{c_{1}}} \\ln\\left(\\frac{2}{\\varepsilon}\\right) P^{1/d}\n$$\nThis expression shows that the number of iterations grows as the $d$-th root of the number of subdomains, which is a signature of a non-scalable two-level preconditioner of this type.\n\nNext, we propose mathematically grounded diagnostics to autonomously detect this specific coarse space deficiency. The core issue is the failure of the preconditioner to be spectrally equivalent to the original operator $A$ with constants independent of $P$. The diagnostics should aim to reveal this lack of scalability.\n\n1.  **Iteration Count Scaling Analysis:** This is the most direct and simplest diagnostic. One performs a scaling study by solving the problem for an increasing number of subdomains/processors $P$.\n    -   In a **weak scaling** study, the local problem size per subdomain ($H/h$) is kept constant. For a scalable preconditioner, the condition number $\\kappa$ should be bounded independently of $P$, and thus the number of iterations $m$ to reach a fixed tolerance $\\varepsilon$ should also be roughly constant.\n    -   In a **strong scaling** study, the global problem size $n$ is fixed. For a scalable preconditioner, $m$ should again be roughly constant.\n    -   **Diagnostic:** Plot the measured iteration count $m(P)$ versus $P$ on a log-log scale. If the preconditioner is scalable, the plot will be a horizontal line (slope $0$). The deficiency described in the problem leads to $m(P) \\propto P^{1/d}$. Therefore, observing a line with a positive slope of approximately $1/d$ is strong evidence for the specific type of coarse space failure. This diagnostic works under both weak and strong scaling as the model for $\\kappa(P)$ is assumed to be the same.\n\n2.  **Extremal Eigenvalue Estimation:** The PCG algorithm is based on the Lanczos iteration. The tridiagonal matrix $T_k$ generated after $k$ steps of the Lanczos process has eigenvalues (called Ritz values) that are excellent approximations of the eigenvalues of the operator $M^{-1}A$, especially the extremal ones.\n    -   **Diagnostic:** During the PCG solve, one can compute the eigenvalues of the small tridiagonal matrix $T_k$ at each iteration or after the solve has converged. By tracking the smallest computed Ritz value, $\\theta_{\\min}$, as a function of $P$, one can directly test the model's assumption.\n    -   The model predicts $\\lambda_{\\min}(M^{-1}A) \\propto P^{-2/d}$, while $\\lambda_{\\max}(M^{-1}A)$ remains bounded. Therefore, if one plots the estimated $\\theta_{\\min}(P)$ versus $P$ on a log-log scale, a line with slope $-2/d$ should be observed. The largest estimated Ritz value, $\\theta_{\\max}(P)$, should remain approximately constant. This provides a more fundamental confirmation of the problem than just iteration counts, as it directly probes the spectrum.\n\n3.  **Analysis of the Coarse Problem:** The two-level preconditioner involves solving a coarse problem with the matrix $A_0 = R_0 A R_0^T$, where $R_0^T$ is the interpolation from the coarse space to the fine grid. The purpose of the coarse space is to handle the low-energy modes that are not well-damped by the local subdomain solves.\n    -   Missing rigid body modes in the coarse basis means that these modes are not properly represented on the coarse grid. This makes the coarse problem matrix $A_0$ itself ill-conditioned. The \"floppiness\" of the global problem, which should be constrained by the coarse solve, is instead reflected in a poorly conditioned $A_0$.\n    -   **Diagnostic:** Explicitly form the coarse matrix $A_0$ and compute its condition number $\\kappa(A_0)$. The size of $A_0$ is proportional to $P$ (specifically, $kP \\times kP$ if there are $k$ basis functions per subdomain), so this is often feasible. For a well-designed, scalable preconditioner, $\\kappa(A_0)$ should be bounded or grow very mildly with $P$. For the deficient preconditioner, $\\kappa(A_0)$ will exhibit strong growth with $P$, often with a power-law dependence, e.g., $\\kappa(A_0) \\propto P^{2/d}$, mirroring the scaling of the overall preconditioned system. This diagnostic is more invasive but pinpoints the failure directly to the coarse component of the preconditioner.\n\nIn summary, a practitioner would first notice the poor scaling of iteration counts (Diagnostic 1). To confirm the cause, they would then analyze the spectrum via Ritz values to see a decaying minimal eigenvalue (Diagnostic 2). Finally, to prove the coarse space is the culprit, they would analyze the coarse problem matrix itself (Diagnostic 3).", "answer": "$$\\boxed{\\frac{1}{2} \\sqrt{\\frac{C_{1}}{c_{1}}} \\ln\\left(\\frac{2}{\\varepsilon}\\right) P^{1/d}}$$", "id": "3449762"}, {"introduction": "Moving from theory to practical optimization, this exercise explores a key trade-off in designing parallel multigrid methods. An exact coarse-grid solve offers the best convergence rate but can be a serial bottleneck, while an inexact solve is cheaper per iteration but may require more outer iterations. You will develop a performance model to find the optimal balance for this trade-off under strong scaling, where communication costs become increasingly dominant [@problem_id:3449746].", "problem": "Consider a linear system $A u = b$ arising from a symmetric positive definite discretization of a linear elliptic partial differential equation on a hierarchy of nested meshes used by a geometric multigrid method. Let a single multigrid V-cycle with an exact coarse solve have an error-propagation operator $E_{\\mathrm{ex}}$ on the fine level with spectral radius (contraction factor) $\\rho_{\\mathrm{ex}} = \\rho(E_{\\mathrm{ex}}) \\in (0,1)$. Suppose that, instead of an exact coarse solve at the first coarse level, you replace it by $k$ inner V-cycles on the coarse hierarchy, where one coarse V-cycle reduces the error on the coarse problem by a factor $\\rho_{\\mathrm{c}} \\in (0,1)$ in the operator norm. Denote by $M_{\\mathrm{c}}^{-1}(k)$ the inexact coarse inverse obtained after $k$ coarse V-cycles.\n\nAssume that the error-propagation operator on the fine level with the inexact coarse solve, $E(k)$, satisfies the norm bound\n$$\n\\|E(k)\\| \\le \\|E_{\\mathrm{ex}}\\| + \\beta \\, \\rho_{\\mathrm{c}}^{k},\n$$\nfor some stability constant $\\beta \\in (0,1 - \\rho_{\\mathrm{ex}})$ independent of $k$ and the number of processors, and that the spectral radius obeys the simplified model\n$$\n\\rho(k) = \\rho(E(k)) = \\rho_{\\mathrm{ex}} + \\beta \\, \\rho_{\\mathrm{c}}^{k}.\n$$\nThe total reduction factor for $m$ outer V-cycles is then bounded by $\\rho(k)^{m}$.\n\nAdopt the following parallel time model for a V-cycle on $P$ processors under strong scaling (the global number of unknowns is fixed). Let the fine-grid unknown count be $N_{1} = N$, and let the number of unknowns on level $\\ell$ be $N_{\\ell} = N / q^{\\ell-1}$ with a constant coarsening factor $q \\ge 2$ and a fixed number of levels $L$. Write the time per level $\\ell$ visit as the sum of a computation term proportional to $N_{\\ell}/P$ and a communication term proportional to $\\log P$. Let $v_{1}$ be the number of fine-level visits per V-cycle and $v_{\\mathrm{sub}}$ be the number of visits per V-cycle to each coarse level $\\ell \\ge 2$. Let $v_{1}^{\\mathrm{comm}}$ and $v_{\\mathrm{sub}}^{\\mathrm{comm}}$ be the corresponding communication visit counts. With per-unknown computation cost $c_{\\mathrm{comp}}$ and per-communication-log cost $c_{\\mathrm{comm}}$, define the fine-level cost\n$$\nT_{1}(P) = v_{1} \\, c_{\\mathrm{comp}} \\frac{N}{P} + v_{1}^{\\mathrm{comm}} \\, c_{\\mathrm{comm}} \\, \\log P,\n$$\nand the cost of one pass across the coarse hierarchy (levels $\\ell = 2,3,\\dots,L$)\n$$\nT_{\\mathrm{sub}}(P) = \\sum_{\\ell=2}^{L} \\left( v_{\\mathrm{sub}} \\, c_{\\mathrm{comp}} \\frac{N_{\\ell}}{P} + v_{\\mathrm{sub}}^{\\mathrm{comm}} \\, c_{\\mathrm{comm}} \\, \\log P \\right).\n$$\nModel the total time for one V-cycle with an inexact coarse solve of quality parameter $k \\in \\mathbb{N}_{0}$ as\n$$\nT_{V}(P,k) = T_{1}(P) + k \\, T_{\\mathrm{sub}}(P).\n$$\nAll time quantities are dimensionless abstract time units and must be treated as pure numbers without physical units.\n\nGiven a target reduction factor $\\theta \\in (0,1)$ on the fine-level residual norm, the number of outer V-cycles required is\n$$\nm(k) = \\left\\lceil \\frac{\\log \\theta}{\\log \\rho(k)} \\right\\rceil.\n$$\nThe time-to-solution under strong scaling is then\n$$\nT_{\\mathrm{tot}}(P,k) = m(k) \\, T_{V}(P,k).\n$$\nDefine the strong-scaling parallel efficiency, using the best choice of $k$ at $P=1$ as the serial reference, by\n$$\nE_{\\mathrm{opt}}(P) = \\frac{T_{\\mathrm{tot}}(1,k_{\\mathrm{opt}}(1))}{P \\, T_{\\mathrm{tot}}(P, k_{\\mathrm{opt}}(P))},\n$$\nwhere\n$$\nk_{\\mathrm{opt}}(P) = \\arg\\min_{k \\in \\{0,1,2,\\dots,k_{\\max}\\}} T_{\\mathrm{tot}}(P,k),\n$$\nwith ties broken by choosing the smallest $k$.\n\nImplement a program that, for the parameter set below, computes for each test case the tuple $[P, k_{\\mathrm{opt}}(P), \\rho(k_{\\mathrm{opt}}(P)), E_{\\mathrm{opt}}(P), T_{\\mathrm{tot}}(P, k_{\\mathrm{opt}}(P))]$.\n\nUse the following parameter values:\n- Fine-grid unknown count: $N = 2^{22}$.\n- Coarsening factor: $q = 4$.\n- Number of levels: $L = 7$.\n- Contraction parameters: $\\rho_{\\mathrm{ex}} = 0.08$, $\\rho_{\\mathrm{c}} = 0.2$, $\\beta = 0.56$.\n- Cost parameters: $c_{\\mathrm{comp}} = 10^{-8}$, $c_{\\mathrm{comm}} = 5 \\times 10^{-4}$.\n- Visit counts: $v_{1} = 2$, $v_{\\mathrm{sub}} = 2$, $v_{1}^{\\mathrm{comm}} = 3$, $v_{\\mathrm{sub}}^{\\mathrm{comm}} = 6$.\n- Target reduction: $\\theta = 10^{-8}$.\n- Integer search range for inexact coarse-solve quality: $k \\in \\{0,1,2,\\dots,8\\}$.\n\nTest suite (distinct processor counts):\n- Case $1$: $P = 1$.\n- Case $2$: $P = 8$.\n- Case $3$: $P = 64$.\n- Case $4$: $P = 2048$.\n\nYour program must:\n- For each case, compute $T_{1}(P)$, $T_{\\mathrm{sub}}(P)$, $\\rho(k)$, $m(k)$, $T_{V}(P,k)$, and $T_{\\mathrm{tot}}(P,k)$ for all $k$ in the specified range.\n- Determine $k_{\\mathrm{opt}}(P)$ and then compute $\\rho(k_{\\mathrm{opt}}(P))$, $E_{\\mathrm{opt}}(P)$, and $T_{\\mathrm{tot}}(P, k_{\\mathrm{opt}}(P))$.\n- Produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is the list $[P, k_{\\mathrm{opt}}(P), \\rho(k_{\\mathrm{opt}}(P)), E_{\\mathrm{opt}}(P), T_{\\mathrm{tot}}(P, k_{\\mathrm{opt}}(P))]$ in the same order as the test suite.\n\nAll logarithms must be natural logarithms. All numeric outputs are pure numbers without any units. The final output format must be exactly one line containing a Python-style list of lists, for example $[[\\dots],[\\dots],[\\dots],[\\dots]]$.", "solution": "### 1. Problem Formulation\n\nThe problem requires a performance analysis of a parallel geometric multigrid method for solving a linear system $A u = b$. The core task is to determine the optimal configuration of an inexact coarse-grid solver and to evaluate the strong-scaling parallel efficiency of the resulting algorithm.\n\nThe multigrid V-cycle's convergence is modeled by its spectral radius (contraction factor), which depends on the quality of the coarse-grid solve, parameterized by an integer $k \\ge 0$:\n$$\n\\rho(k) = \\rho_{\\mathrm{ex}} + \\beta \\, \\rho_{\\mathrm{c}}^{k}\n$$\nHere, $\\rho_{\\mathrm{ex}}$ is the contraction factor with an exact coarse solve, $\\rho_{\\mathrm{c}}$ is the contraction factor of the inner V-cycle used as a coarse-level solver, and $\\beta$ is a stability constant. The number of outer V-cycles, $m(k)$, required to achieve a target residual reduction of $\\theta$ is given by:\n$$\nm(k) = \\left\\lceil \\frac{\\log \\theta}{\\log \\rho(k)} \\right\\rceil\n$$\nwhere the logarithm is the natural logarithm.\n\nThe parallel execution time for a single V-cycle, $T_V(P, k)$, depends on the number of processors $P$ and the coarse-solve quality $k$. It is composed of work on the fine grid, $T_1(P)$, and work on the coarse-grid hierarchy, $T_{\\text{sub}}(P)$:\n$$\nT_{V}(P,k) = T_{1}(P) + k \\, T_{\\mathrm{sub}}(P)\n$$\nFinally, the total time-to-solution is the product of the number of iterations and the time per iteration:\n$$\nT_{\\mathrm{tot}}(P,k) = m(k) \\, T_{V}(P,k)\n$$\n\nThe objective is to find the optimal quality parameter $k_{\\mathrm{opt}}(P)$ that minimizes this total time for a given $P$:\n$$\nk_{\\mathrm{opt}}(P) = \\arg\\min_{k \\in \\{0,1,\\dots,8\\}} T_{\\mathrm{tot}}(P,k)\n$$\nand then to compute the strong-scaling parallel efficiency $E_{\\mathrm{opt}}(P)$ relative to the optimal serial configuration:\n$$\nE_{\\mathrm{opt}}(P) = \\frac{T_{\\mathrm{tot}}(1,k_{\\mathrm{opt}}(1))}{P \\, T_{\\mathrm{tot}}(P, k_{\\mathrm{opt}}(P))}\n$$\nThis must be done for a specified set of parameters and processor counts $P \\in \\{1, 8, 64, 2048\\}$.\n\n### 2. Analysis of the Time-to-Solution Components\n\nWe first detail the expressions for the time components using the given parameters: $N = 2^{22}$, $q = 4$, $L = 7$, $c_{\\mathrm{comp}} = 10^{-8}$, $c_{\\mathrm{comm}} = 5 \\times 10^{-4}$, $v_1 = 2$, $v_{\\mathrm{sub}} = 2$, $v_{1}^{\\mathrm{comm}} = 3$, $v_{\\mathrm{sub}}^{\\mathrm{comm}} = 6$.\n\nThe fine-level cost is:\n$$\nT_{1}(P) = v_{1} c_{\\mathrm{comp}} \\frac{N}{P} + v_{1}^{\\mathrm{comm}} c_{\\mathrm{comm}} \\log P = 2 \\cdot 10^{-8} \\frac{2^{22}}{P} + 3 \\cdot 5 \\cdot 10^{-4} \\log P = \\frac{0.08388608}{P} + 0.0015 \\log P\n$$\n\nThe cost of one pass across the coarse hierarchy is a sum over levels $\\ell=2, \\dots, L$:\n$$\nT_{\\mathrm{sub}}(P) = \\sum_{\\ell=2}^{L} \\left( v_{\\mathrm{sub}} c_{\\mathrm{comp}} \\frac{N_{\\ell}}{P} + v_{\\mathrm{sub}}^{\\mathrm{comm}} c_{\\mathrm{comm}} \\log P \\right)\n$$\nwhere $N_{\\ell}=N/q^{\\ell-1}$. We can separate the sum:\n$$\nT_{\\mathrm{sub}}(P) = \\frac{v_{\\mathrm{sub}} c_{\\mathrm{comp}} N}{P} \\left( \\sum_{\\ell=2}^{L} q^{-(\\ell-1)} \\right) + (L-1) v_{\\mathrm{sub}}^{\\mathrm{comm}} c_{\\mathrm{comm}} \\log P\n$$\nThe summation is a finite geometric series:\n$$\n\\sum_{j=1}^{L-1} \\left(\\frac{1}{q}\\right)^j = \\sum_{j=1}^{6} \\left(\\frac{1}{4}\\right)^j = \\frac{1/4(1 - (1/4)^6)}{1 - 1/4} = \\frac{1}{3} \\left(1 - \\frac{1}{4096}\\right) = \\frac{1}{3} \\frac{4095}{4096} = \\frac{1365}{4096}\n$$\nSubstituting the parameters:\n$$\nT_{\\mathrm{sub}}(P) = \\frac{2 \\cdot 10^{-8} \\cdot 2^{22}}{P} \\left(\\frac{1365}{4096}\\right) + (7-1) \\cdot 6 \\cdot 5 \\cdot 10^{-4} \\log P = \\frac{0.0279619140625}{P} + 0.018 \\log P\n$$\nFor large $P$, the computational work (proportional to $1/P$) diminishes, while the communication cost (proportional to $\\log P$) grows, causing it to dominate the total time.\n\n### 3. Computational Procedure\n\nThe solution is found by implementing the following algorithm:\n\n1.  **Set up constants**: Define all given physical and algorithmic parameters.\n2.  **Serial Reference Calculation**: To compute efficiency, we first need the optimal serial performance. We set $P=1$ and find $k_{\\mathrm{opt}}(1)$ by searching for the minimum $T_{\\mathrm{tot}}(1,k)$ over the specified range $k \\in \\{0, 1, \\dots, 8\\}$. The tie-breaking rule (smallest $k$) is handled by iterating $k$ in increasing order. The resulting optimal time $T_{\\mathrm{ref}} = T_{\\mathrm{tot}}(1,k_{\\mathrm{opt}}(1))$ serves as the serial benchmark. For $P=1$, $\\log P = 0$, simplifying the time expressions significantly as communication costs vanish.\n3.  **Processor Sweep**: For each processor count $P$ in the test suite $\\{1, 8, 64, 2048\\}$:\n    a. Find the optimal coarse-solve quality $k_{\\mathrm{opt}}(P)$ by calculating $T_{\\mathrm{tot}}(P,k)$ for each $k \\in \\{0, 1, \\dots, 8\\}$ and identifying the $k$ that yields the minimum time.\n    b. Store the minimal time as $T_{\\mathrm{tot}}(P, k_{\\mathrm{opt}}(P))$.\n    c. Calculate the corresponding contraction factor $\\rho(k_{\\mathrm{opt}}(P))$.\n    d. Compute the parallel efficiency using the stored serial reference time: $E_{\\mathrm{opt}}(P) = T_{\\mathrm{ref}}/(P \\cdot T_{\\mathrm{tot}}(P, k_{\\mathrm{opt}}(P)))$.\n    e. Collect the quintuple $[P, k_{\\mathrm{opt}}(P), \\rho(k_{\\mathrm{opt}}(P)), E_{\\mathrm{opt}}(P), T_{\\mathrm{tot}}(P, k_{\\mathrm{opt}}(P))]$.\n4.  **Final Output**: Assemble the collected results into a single list of lists.\n\nLet's execute this procedure.\nThe convergence parameters are $\\rho_{\\mathrm{ex}} = 0.08$, $\\beta = 0.56$, $\\rho_{\\mathrm{c}} = 0.2$, $\\theta = 10^{-8}$.\n\n**Reference case ($P=1$):**\n- $T_V(1,k) = 0.08388608 + k \\cdot 0.027961914$.\n- We calculate $m(k)$ and $T_{\\mathrm{tot}}(1,k)=m(k)T_V(1,k)$ for $k=0, \\dots, 8$:\n  - $k=0: \\rho(0)=0.64, m(0)=42 \\implies T_{\\mathrm{tot}}(1,0) \\approx 3.523$\n  - $k=1: \\rho(1)=0.192, m(1)=12 \\implies T_{\\mathrm{tot}}(1,1) \\approx 1.342$\n  - $k=2: \\rho(2)=0.1024, m(2)=9 \\implies T_{\\mathrm{tot}}(1,2) \\approx 1.258$\n  - $k=3: \\rho(3)=0.08448, m(3)=8 \\implies T_{\\mathrm{tot}}(1,3) \\approx 1.342$\n- The minimum time occurs at $k=2$. So, $k_{\\mathrm{opt}}(1)=2$ and the reference time is $T_{\\mathrm{ref}} = T_{\\mathrm{tot}}(1,2) \\approx 1.258289$. The efficiency is $E_{\\mathrm{opt}}(1)=1.0$.\n\n**Test cases ($P \\in \\{1, 8, 64, 2048\\}$):**\nBy applying the same minimization procedure for each $P$, we find that as $P$ increases, the $\\log P$ term in the V-cycle cost $T_V(P,k)$ becomes dominant. This term's coefficient, $(0.0015 + 0.018 k)$, grows with $k$. The increasing cost of each iteration for larger $k$ outweighs the benefit of a better convergence rate (smaller $m(k)$), especially since the benefit (the sequence of $m(k)$ values) is independent of $P$. Consequently, the optimal choice of $k$ shifts. For $P=8, 64, 2048$, the analysis reveals that the minimal time-to-solution is achieved at $k_{\\mathrm{opt}}(P)=0$, where the expensive recursive coarse solves are avoided entirely, leaving only a coarse-grid smoothing step. The significant drop in parallel efficiency reflects the classic strong-scaling limit where communication overheads dominate for a fixed-size problem.\n\nThe final computed results for each test case are generated by the provided program.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the parallel performance modeling problem for a multigrid method.\n    \"\"\"\n    #\n    # --- Step 1: Define model parameters from the problem statement ---\n    #\n    N = 2**22                        # Fine-grid unknown count\n    q = 4.0                          # Coarsening factor\n    L = 7                            # Number of levels\n    rho_ex = 0.08                    # Contraction factor with exact coarse solve\n    rho_c = 0.2                      # Contraction factor of coarse-level inner V-cycle\n    beta = 0.56                      # Stability constant for inexact solve model\n    c_comp = 1e-8                    # Per-unknown computation cost\n    c_comm = 5e-4                    # Per-communication-log cost\n    v1 = 2                           # Fine-level visits per V-cycle\n    v_sub = 2                        # Coarse-level visits per V-cycle\n    v1_comm = 3                      # Fine-level communication visits\n    v_sub_comm = 6                   # Coarse-level communication visits\n    theta = 1e-8                     # Target reduction factor\n    k_range = list(range(9))         # Search range for k: {0, 1, ..., 8}\n    test_cases_P = [1, 8, 64, 2048]  # Processor counts for test cases\n\n    # Pre-calculate the geometric sum for the T_sub expression.\n    # The sum is sum_{l=2 to L} q^{-(l-1)} = sum_{j=1 to L-1} (1/q)^j.\n    # This is a geometric series with a=1/q, r=1/q, n=L-1.\n    sum_inv_q = (1/q) * (1 - (1/q)**(L - 1)) / (1 - 1/q) if q != 1 else float(L - 1)\n\n    #\n    # --- Step 2: Define helper functions for the model equations ---\n    #\n\n    def get_rho(k: int) -> float:\n        \"\"\"Calculates the contraction factor rho(k).\"\"\"\n        return rho_ex + beta * rho_c**k\n\n    def get_m(k: int) -> int:\n        \"\"\"Calculates the number of outer V-cycles m(k).\"\"\"\n        rho_k = get_rho(k)\n        # The problem constraints ensure rho_k < 1.\n        return int(np.ceil(np.log(theta) / np.log(rho_k)))\n\n    def get_T_tot(P: int, k: int) -> float:\n        \"\"\"Calculates the total time-to-solution T_tot(P, k).\"\"\"\n        log_P = np.log(P) if P > 1 else 0.0\n\n        # Cost of fine-level work for one V-cycle\n        T1_P = v1 * c_comp * (N / P) + v1_comm * c_comm * log_P\n        \n        # Cost of one pass across the coarse hierarchy\n        T_sub_P = v_sub * c_comp * (N / P) * sum_inv_q + (L - 1) * v_sub_comm * c_comm * log_P\n        \n        # Total cost of one V-cycle with k inner iterations on the coarse problem\n        T_V_Pk = T1_P + k * T_sub_P\n        \n        # Number of outer V-cycles\n        m_k = get_m(k)\n        \n        return m_k * T_V_Pk\n\n    #\n    # --- Step 3: Find the serial reference performance (P=1) ---\n    #\n    P_ref = 1\n    min_time_ref = float('inf')\n    k_opt_ref = -1\n    \n    for k in k_range:\n        time = get_T_tot(P_ref, k)\n        if time < min_time_ref:\n            min_time_ref = time\n            k_opt_ref = k\n\n    # The serial reference time is the optimal time on 1 processor.\n    T_ref = min_time_ref\n\n    #\n    # --- Step 4: Process all test cases to find optimal k and efficiency ---\n    #\n    results = []\n    for P in test_cases_P:\n        min_time_P = float('inf')\n        k_opt_P = -1\n        \n        # Find k_opt(P) by minimizing T_tot(P, k) over the range of k.\n        # The tie-breaking rule (smallest k) is handled by iterating k\n        # in increasing order and updating only on strict inequality.\n        for k in k_range:\n            time = get_T_tot(P, k)\n            if time < min_time_P:\n                min_time_P = time\n                k_opt_P = k\n        \n        T_tot_opt_P = min_time_P\n        rho_opt_P = get_rho(k_opt_P)\n        \n        # Calculate strong-scaling parallel efficiency\n        E_opt_P = T_ref / (P * T_tot_opt_P)\n        \n        results.append([P, k_opt_P, rho_opt_P, E_opt_P, T_tot_opt_P])\n        \n    #\n    # --- Step 5: Format and print the final output ---\n    #\n    print(str(results))\n\nsolve()\n```", "id": "3449746"}, {"introduction": "Our final practice shifts to the weak scaling paradigm and investigates a critical hardware limitation: memory. Ideal weak scaling assumes that work per processor is constant, but for methods like Algebraic Multigrid (AMG), coarse-grid operators can become denser as the total problem size grows, increasing memory usage per processor. This exercise asks you to model this phenomenon and quantify how memory pressure can degrade performance, demonstrating that true scalability requires co-designing for computation, communication, and memory constraints [@problem_id:3449823].", "problem": "Consider an Algebraic Multigrid (AMG) preconditioner used within a Krylov method for the numerical solution of Partial Differential Equations (PDEs). Focus on weak scaling, where the total problem size grows proportionally with the number of processing units, while the local problem size per processing unit remains constant. In this setting, we are interested in quantifying how coarse-grid operator density growth impacts memory per core and, in turn, reduces weak-scaling parallel efficiency.\n\nUse the following fundamental bases and definitions without introducing any shortcut formulas beyond the stated assumptions:\n\n- Weak scaling holds that the local degrees of freedom per core, denoted $n_{\\mathrm{local}}$, remain constant as the number of processing units $P$ increases.\n- Parallel efficiency under weak scaling, denoted $E(P)$, is defined as $E(P) = \\dfrac{T(1)}{T(P)}$, where $T(P)$ is the wall-clock time per AMG V-cycle at $P$ processing units and $T(1)$ is the time per V-cycle on a single processing unit with $n_{\\mathrm{local}}$ degrees of freedom.\n- The AMG operator memory per core is dominated by the nonzero storage across levels. Let the average number of nonzeros per row on level $l$ be $z_l(P)$, which may grow with $P$ due to density increases on coarse levels. Let the bytes per nonzero be $b_{\\mathrm{nz}}$ and the number of auxiliary vectors stored per degree of freedom be $v_{\\mathrm{mult}}$ with $8$ bytes per vector entry. Then the per-core memory footprint $M(P)$ in gibibytes (GiB) is modeled as\n$$\nM(P) = \\frac{n_{\\mathrm{local}} \\, b_{\\mathrm{nz}} \\sum_{l=1}^{L} z_l(P) + n_{\\mathrm{local}} \\cdot v_{\\mathrm{mult}} \\cdot 8}{1024^3}.\n$$\n- The coarse-grid density growth model is given by\n$$\nz_l(P) = z_{l,0} \\left(1 + c_l \\, P^{\\gamma_l}\\right),\n$$\nwhere $z_{l,0}$ is the base nonzeros per row at $P=1$, $c_l \\ge 0$ controls the growth amplitude, and $\\gamma_l \\ge 0$ controls the growth rate.\n- Memory pressure modifies compute time through a multiplicative penalty. Define the penalty factor $\\phi(M)$ by\n$$\n\\phi(M) = \n\\begin{cases}\n1, & M \\le M_{\\mathrm{thr}},\\\\\n1 + \\kappa \\, \\dfrac{M - M_{\\mathrm{thr}}}{M_{\\mathrm{thr}}}, & M > M_{\\mathrm{thr}},\n\\end{cases}\n$$\nwhere $M_{\\mathrm{thr}}$ is the per-core memory threshold (GiB) and $\\kappa > 0$ quantifies sensitivity to memory pressure.\n- The per-core compute time per V-cycle scales in proportion to the operator complexity:\n$$\nT_{\\mathrm{comp}}(P) = t_0 \\, \\frac{\\sum_{l=1}^{L} z_l(P)}{\\sum_{l=1}^{L} z_l(1)},\n$$\nwhere $t_0$ is the baseline compute time at $P=1$.\n- The communication time under weak scaling is modeled as\n$$\nT_{\\mathrm{comm}}(P) = a \\, \\log_2(P),\n$$\nwith $a > 0$ representing communication cost scaling.\n- The total time per V-cycle is\n$$\nT(P) = \\phi\\!\\left(M(P)\\right) \\, T_{\\mathrm{comp}}(P) + T_{\\mathrm{comm}}(P).\n$$\n\nDefine the threshold $P^\\star$ as the smallest integer $P \\ge 1$ such that $M(P) > M_{\\mathrm{thr}}$. If no such $P \\le P_{\\max}$ exists, report $P^\\star = -1$. Consider an aggressive interpolation pruning strategy that scales both the base stencil sizes and the growth coefficients by a factor $\\rho \\in (0,1)$:\n$$\nz^{\\mathrm{prune}}_l(P) = \\rho \\, z_{l,0} \\left(1 + \\rho \\, c_l \\, P^{\\gamma_l}\\right).\n$$\nUnder pruning, recompute $M^{\\mathrm{prune}}(P)$, $T^{\\mathrm{prune}}(P)$, and $E^{\\mathrm{prune}}(P)$ analogously.\n\nTasks:\n1. For each test case, compute the threshold $P^\\star$.\n2. For each test case, compute the weak-scaling parallel efficiency $E(P_{\\max})$ before pruning and after pruning, denoted $E^{\\mathrm{prune}}(P_{\\max})$. Express the times in seconds and memory in gibibytes, and report efficiencies as decimal fractions rounded to six decimal places.\n\nYour program should use the following test suite (each case is independent and uses its own parameters):\n\n- Case A (happy path):\n  - $n_{\\mathrm{local}} = 5 \\times 10^5$, $L = 4$, $z_{l,0} = [20, 15, 12, 400]$, $c_l = [0.02, 0.03, 0.05, 0.10]$, $\\gamma_l = [0.25, 0.25, 0.25, 0.5]$, $b_{\\mathrm{nz}} = 12$, $v_{\\mathrm{mult}} = 6$, $M_{\\mathrm{thr}} = 8$, $t_0 = 1.0$, $a = 0.03$, $\\kappa = 0.5$, $P_{\\max} = 4096$, $\\rho = 0.5$.\n- Case B (boundary, no density growth):\n  - $n_{\\mathrm{local}} = 5 \\times 10^5$, $L = 4$, $z_{l,0} = [20, 15, 12, 200]$, $c_l = [0, 0, 0, 0]$, $\\gamma_l = [0, 0, 0, 0]$, $b_{\\mathrm{nz}} = 12$, $v_{\\mathrm{mult}} = 6$, $M_{\\mathrm{thr}} = 8$, $t_0 = 1.0$, $a = 0.03$, $\\kappa = 0.5$, $P_{\\max} = 4096$, $\\rho = 0.5$.\n- Case C (edge, aggressive growth on coarse level):\n  - $n_{\\mathrm{local}} = 5 \\times 10^5$, $L = 3$, $z_{l,0} = [25, 18, 600]$, $c_l = [0.05, 0.08, 0.20]$, $\\gamma_l = [0.3, 0.3, 0.6]$, $b_{\\mathrm{nz}} = 12$, $v_{\\mathrm{mult}} = 6$, $M_{\\mathrm{thr}} = 12$, $t_0 = 1.4$, $a = 0.04$, $\\kappa = 0.6$, $P_{\\max} = 1024$, $\\rho = 0.4$.\n\nOutput specification:\n- Your program should produce a single line of output containing the results for the three cases as a comma-separated list enclosed in square brackets, with no spaces, where each case result is itself a list in the form $[P^\\star,E(P_{\\max}),E^{\\mathrm{prune}}(P_{\\max})]$. For example: $[[1,0.900000,0.950000],[\\dots],[\\dots]]$.", "solution": "The core of the problem is to compute the weak-scaling parallel efficiency, $E(P)$, defined as:\n$$\nE(P) = \\frac{T(1)}{T(P)}\n$$\nwhere $T(P)$ is the total wall-clock time for one V-cycle on $P$ processing units. The time on a single processor, $T(1)$, serves as the baseline. The total time $T(P)$ is modeled as the sum of a computational component and a communication component:\n$$\nT(P) = T_{\\mathrm{total\\_comp}}(P) + T_{\\mathrm{comm}}(P)\n$$\nThe communication time is modeled by a standard logarithmic term reflecting the cost of global reductions or data exchanges on a network with a bisection-like topology:\n$$\nT_{\\mathrm{comm}}(P) = a \\log_2(P)\n$$\nFor $P=1$, $T_{\\mathrm{comm}}(1) = a \\log_2(1) = 0$, which is physically correct as a single process does not communicate with others.\n\nThe computational time is more complex, as it is affected by memory pressure. It is modeled as a baseline compute time, $T_{\\mathrm{comp}}(P)$, scaled by a memory penalty factor, $\\phi(M(P))$:\n$$\nT_{\\mathrm{total\\_comp}}(P) = \\phi(M(P)) \\, T_{\\mathrm{comp}}(P)\n$$\nThe baseline compute time, $T_{\\mathrm{comp}}(P)$, is proportional to the operator complexity, which is the total number of nonzeros in the AMG hierarchy stored per core. This is normalized by the complexity at $P=1$:\n$$\nT_{\\mathrm{comp}}(P) = t_0 \\, \\frac{\\sum_{l=1}^{L} z_l(P)}{\\sum_{l=1}^{L} z_l(1)}\n$$\nwhere $z_l(P)$ is the average number of nonzeros per row on multigrid level $l$ for a run on $P$ processors. Note that $T_{\\mathrm{comp}}(1) = t_0$.\n\nThe penalty factor $\\phi(M)$ is a piecewise function that becomes greater than $1$ only when the per-core memory footprint $M(P)$ exceeds a threshold $M_{\\mathrm{thr}}$:\n$$\n\\phi(M) = \n\\begin{cases}\n1, & M \\le M_{\\mathrm{thr}} \\\\\n1 + \\kappa \\, \\dfrac{M - M_{\\mathrm{thr}}}{M_{\\mathrm{thr}}}, & M > M_{\\mathrm{thr}}\n\\end{cases}\n$$\nThis models performance degradation when the working set size exceeds a high-speed cache or available DRAM capacity, leading to slower memory accesses.\n\nThe per-core memory footprint $M(P)$ in gibibytes (GiB) depends on the storage for the operator nonzeros and for auxiliary vectors:\n$$\nM(P) = \\frac{n_{\\mathrm{local}} \\, b_{\\mathrm{nz}} \\sum_{l=1}^{L} z_l(P) + n_{\\mathrm{local}} \\cdot v_{\\mathrm{mult}} \\cdot 8}{1024^3}\n$$\nThe key to weak scaling behavior in this model is the growth of coarse-grid operator density, captured by $z_l(P)$:\n$$\nz_l(P) = z_{l,0} \\left(1 + c_l \\, P^{\\gamma_l}\\right)\n$$\nAs $P$ increases, the terms $P^{\\gamma_l}$ cause $z_l(P)$ to grow, which in turn increases both memory usage $M(P)$ and baseline compute time $T_{\\mathrm{comp}}(P)$.\n\nThe calculation procedure for each test case is as follows:\n\n1.  **Determine the memory pressure threshold $P^\\star$**: This is the smallest integer $P \\ge 1$ for which $M(P) > M_{\\mathrm{thr}}$. We can find this by iterating $P$ from $1$ to $P_{\\max}$ and computing $M(P)$ at each step. The first $P$ that satisfies the inequality is $P^\\star$. If no such $P$ exists up to $P_{\\max}$, we report $P^\\star = -1$.\n\n2.  **Calculate parallel efficiency $E(P_{\\max})$**:\n    a.  Calculate the reference time $T(1)$. This involves computing $\\sum z_l(1)$, then $M(1)$, then $\\phi(M(1))$, and finally $T(1) = \\phi(M(1)) \\cdot t_0$.\n    b.  Calculate the scaled time $T(P_{\\max})$. This follows the same sequence: compute $\\sum z_l(P_{\\max})$, then $M(P_{\\max})$, $\\phi(M(P_{\\max}))$, $T_{\\mathrm{comp}}(P_{\\max})$, and $T_{\\mathrm{comm}}(P_{\\max})$ to get the final $T(P_{\\max})$.\n    c.  Compute the efficiency $E(P_{\\max}) = T(1)/T(P_{\\max})$.\n\n3.  **Repeat for the pruned case**: The entire calculation is repeated using the modified model for the number of nonzeros:\n    $$\n    z^{\\mathrm{prune}}_l(P) = \\rho \\, z_{l,0} \\left(1 + \\rho \\, c_l \\, P^{\\gamma_l}\\right)\n    $$\n    All derived quantities ($M^{\\mathrm{prune}}(P)$, $T^{\\mathrm{prune}}(P)$, etc.) are re-evaluated using this formula to find $E^{\\mathrm{prune}}(P_{\\max})$. Note that the search for $P^\\star$ is only performed for the unpruned case.\n\nThis complete, step-by-step procedure is implemented for each test case to obtain the required results.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all given test cases.\n    \"\"\"\n    # Define test cases as per the problem statement.\n    test_cases = [\n        # Case A (happy path)\n        {\n            'n_local': 5e5, 'b_nz': 12, 'v_mult': 6,\n            'z_l0': np.array([20, 15, 12, 400]),\n            'c_l': np.array([0.02, 0.03, 0.05, 0.10]),\n            'gamma_l': np.array([0.25, 0.25, 0.25, 0.5]),\n            'M_thr': 8, 't0': 1.0, 'a': 0.03, 'kappa': 0.5,\n            'P_max': 4096, 'rho': 0.5\n        },\n        # Case B (boundary, no density growth)\n        {\n            'n_local': 5e5, 'b_nz': 12, 'v_mult': 6,\n            'z_l0': np.array([20, 15, 12, 200]),\n            'c_l': np.array([0, 0, 0, 0]),\n            'gamma_l': np.array([0, 0, 0, 0]),\n            'M_thr': 8, 't0': 1.0, 'a': 0.03, 'kappa': 0.5,\n            'P_max': 4096, 'rho': 0.5\n        },\n        # Case C (edge, aggressive growth on coarse level)\n        {\n            'n_local': 5e5, 'b_nz': 12, 'v_mult': 6,\n            'z_l0': np.array([25, 18, 600]),\n            'c_l': np.array([0.05, 0.08, 0.20]),\n            'gamma_l': np.array([0.3, 0.3, 0.6]),\n            'M_thr': 12, 't0': 1.4, 'a': 0.04, 'kappa': 0.6,\n            'P_max': 1024, 'rho': 0.4\n        }\n    ]\n\n    all_results = [process_case(case) for case in test_cases]\n\n    # Format the output as specified\n    result_strings = [f\"[{r[0]},{r[1]:.6f},{r[2]:.6f}]\" for r in all_results]\n    print(f\"[{','.join(result_strings)}]\")\n\ndef get_sum_z(P, z_l0, c_l, gamma_l, rho, pruned):\n    \"\"\"Calculates the sum of non-zeros per row over all levels.\"\"\"\n    if P == 0: return 0\n    if not pruned:\n        return np.sum(z_l0 * (1 + c_l * (P ** gamma_l)))\n    else:\n        return np.sum(rho * z_l0 * (1 + rho * c_l * (P ** gamma_l)))\n\ndef calculate_time(P, params, pruned):\n    \"\"\"Calculates the total time per V-cycle for a given P.\"\"\"\n    # Unpack parameters\n    t0, a, kappa, M_thr = params['t0'], params['a'], params['kappa'], params['M_thr']\n    n_local, b_nz, v_mult = params['n_local'], params['b_nz'], params['v_mult']\n    z_l0, c_l, gamma_l, rho = params['z_l0'], params['c_l'], params['gamma_l'], params['rho']\n    \n    # Operator complexity (sum of non-zeros)\n    sum_z_P = get_sum_z(P, z_l0, c_l, gamma_l, rho, pruned)\n    sum_z_1 = get_sum_z(1, z_l0, c_l, gamma_l, rho, pruned)\n    \n    # Memory footprint\n    M = (n_local * b_nz * sum_z_P + n_local * v_mult * 8) / (1024**3)\n    \n    # Memory penalty factor\n    phi = 1.0\n    if M > M_thr:\n        phi = 1.0 + kappa * (M - M_thr) / M_thr\n        \n    # Compute time\n    T_comp = t0 * (sum_z_P / sum_z_1)\n    \n    # Communication time\n    T_comm = 0.0\n    if P > 1:\n        T_comm = a * np.log2(P)\n        \n    # Total time\n    return phi * T_comp + T_comm\n\ndef process_case(case_params):\n    \"\"\"\n    Processes a single test case to find P_star, E(P_max), and E_pruned(P_max).\n    \"\"\"\n    P_max = case_params['P_max']\n    \n    # Task 1: Find P_star (only for the unpruned case)\n    P_star = -1\n    for P_val in range(1, P_max + 1):\n        sum_z = get_sum_z(P_val, case_params['z_l0'], case_params['c_l'], case_params['gamma_l'], case_params['rho'], pruned=False)\n        M = (case_params['n_local'] * case_params['b_nz'] * sum_z + \n             case_params['n_local'] * case_params['v_mult'] * 8) / (1024**3)\n        if M > case_params['M_thr']:\n            P_star = P_val\n            break\n            \n    # Task 2: Calculate weak-scaling efficiencies at P_max\n    \n    # Unpruned efficiency\n    T1_unpruned = calculate_time(1, case_params, pruned=False)\n    Tpmax_unpruned = calculate_time(P_max, case_params, pruned=False)\n    E_pmax = T1_unpruned / Tpmax_unpruned\n    \n    # Pruned efficiency\n    T1_pruned = calculate_time(1, case_params, pruned=True)\n    Tpmax_pruned = calculate_time(P_max, case_params, pruned=True)\n    E_prune_pmax = T1_pruned / Tpmax_pruned\n    \n    return [P_star, E_pmax, E_prune_pmax]\n\nif __name__ == \"__main__\":\n    solve()\n\n```", "id": "3449823"}]}