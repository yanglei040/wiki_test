{"hands_on_practices": [{"introduction": "At the heart of any adaptive method is a decision engine that must balance gains in accuracy against computational cost. This practice guides you through building a simple but powerful model to choose between $h$-, $p$-, and $r$-refinement strategies [@problem_id:3360869]. By estimating the error reduction per unit of computational cost for each potential action, you will implement a quantitative \"refinement oracle\" that makes optimal local decisions, a foundational skill for developing efficient adaptive solvers.", "problem": "Construct a cost-aware decision model for adaptive mesh refinement in one spatial dimension that evaluates the marginal error reduction per degree of freedom for three refinement strategies: $h$-refinement (element bisection at fixed polynomial degree), $p$-refinement (local polynomial degree elevation), and $r$-refinement (local node relocation that shrinks an element). Work in the setting of approximating a known function $u(x)$ on an interval using continuous, piecewise-polynomial finite elements, and measure the local discretization error on a single element by the best-approximation error in the $H^1$ seminorm. Use this decision model to choose, for each specified test case, the single refinement action that maximizes the estimated marginal error reduction per unit cost.\n\nBase your derivation and algorithm on the following fundamental definitions and facts:\n- The $H^1$ seminorm of a function $v$ on an interval $I = [a,b]$ is $\\|v\\|_{H^1(I)} := \\left(\\int_a^b |v'(x)|^2 \\, dx \\right)^{1/2}$.\n- For a fixed element $I=[a,b]$ and polynomial degree $p \\in \\mathbb{N}$, the local finite element space on $I$ consists of polynomials of degree at most $p$. Its derivatives span all polynomials of degree at most $p-1$ on $I$.\n- The best-approximation error in the $H^1$ seminorm on $I$ equals the $L^2(I)$ projection error of $u'(x)$ onto the subspace of polynomials of degree at most $p-1$ on $I$.\n- Orthogonal projection in a Hilbert space minimizes the norm of the residual over a subspace.\n\nYou must:\n1. For a given element $I=[a,b]$ and degree $p$, compute the current local error $E_0(I,p)$ as the minimum of $\\int_a^b |u'(x)-q(x)|^2\\,dx$ over all polynomials $q$ of degree at most $p-1$.\n2. For $h$-refinement, replace $I$ by two children $I_1=[a,(a+b)/2]$ and $I_2=[(a+b)/2,b]$ with the same $p$. Compute $E_h(I,p) := E_0(I_1,p) + E_0(I_2,p)$.\n3. For $p$-refinement, elevate the degree on $I$ to $p+1$ and compute $E_p(I,p) := E_0(I,p+1)$.\n4. For $r$-refinement, shrink the element length by a factor $s \\in (0,1)$ while keeping its left endpoint fixed, i.e., $I_r = [a, a + s(b-a)]$, keeping the same $p$. Compute $E_r(I,p,s) := E_0(I_r,p)$.\n\nDefine the marginal error reduction per unit cost for each action as\n- $G_h := \\dfrac{\\max\\{E_0(I,p) - E_h(I,p), 0\\}}{C_h(p)}$,\n- $G_p := \\dfrac{\\max\\{E_0(I,p) - E_p(I,p), 0\\}}{C_p}$,\n- $G_r := \\dfrac{\\max\\{E_0(I,p) - E_r(I,p,s), 0\\}}{C_r}$,\n\nwith the following cost model (counted in equivalent degrees of freedom):\n- $C_h(p) := p$ for bisecting one element of degree $p$,\n- $C_p := 1$ for raising $p$ to $p+1$ on one element,\n- $C_r := c_r$ for relocating nodes to shrink an element by the given factor $s$, where $c_r > 0$ is provided.\n\nUse Gauss–Legendre quadrature to evaluate the required integrals with sufficiently high order to ensure numerical stability, and use orthogonal polynomials on the reference interval to construct the $L^2$ projections without relying on any precomputed shortcut formulas. All elements, variables, operators, and constants must be treated with mathematical precision.\n\nDecision rule: for each test case, compute $G_h$, $G_p$, and $G_r$. Select the action that maximizes the gain. In the event of ties within a numerical tolerance of $10^{-12}$ in gain, break ties by choosing the action with the lowest index in the ordering $h \\rightarrow p \\rightarrow r$.\n\nBenchmark functions $u(x)$ to validate distinct regularity regimes:\n- Smooth analytic: $u(x) = \\sin(\\pi x)$ on $[0,1]$.\n- Endpoint singular (but $H^1$-admissible): $u(x) = x^{\\alpha}$ on $[0,1]$ with $\\alpha = 0.6$.\n\nAngle units are not involved. There are no physical units; all quantities are dimensionless.\n\nTest suite:\nFor each test case, you will be given a tuple specifying the benchmark type, the element interval $[a,b]$, the polynomial degree $p$, the $r$-shrink factor $s$, and the $r$-cost $c_r$. Use $\\alpha = 0.6$ for the singular case. The test cases are:\n- Case 1: smooth, $[a,b]=[0,0.5]$, $p=2$, $s=0.7$, $c_r=1.0$.\n- Case 2: smooth, $[a,b]=[0.5,1]$, $p=2$, $s=0.7$, $c_r=1.0$.\n- Case 3: singular, $[a,b]=[0,0.5]$, $p=2$, $s=0.7$, $c_r=1.0$.\n- Case 4: singular, $[a,b]=[0.5,1]$, $p=2$, $s=0.7$, $c_r=1.0$.\n\nYour program should:\n- Implement the model as specified.\n- For each case, output an integer encoding the chosen action, using $0$ for $h$-refinement, $1$ for $p$-refinement, and $2$ for $r$-refinement.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, e.g., $[r_1,r_2,r_3,r_4]$, where each $r_i$ is the integer decision for case $i$ in the order given above. No other output should be produced.", "solution": "The user-provided problem is a valid, well-posed exercise in numerical analysis, specifically in the field of adaptive finite element methods. It requires the construction and application of a decision model to select an optimal mesh refinement strategy among $h$-, $p$-, and $r$-refinement. The decision is based on maximizing the marginal error reduction per unit of computational cost. All components of the problem—the error metric, refinement strategies, cost model, and decision rule—are defined with mathematical precision, making it a solvable problem grounded in established scientific principles.\n\nThe solution proceeds by first formalizing the computation of the local error, then detailing the calculation of gain for each refinement strategy, and finally specifying the decision logic.\n\n### 1. Local Error Computation\n\nThe core of the model is the computation of the local discretization error on a single element $I = [a,b]$ for a polynomial approximation of degree $p$. The problem defines this error as the best-approximation error in the $H^1$ seminorm. A key provided fact simplifies this: the squared $H^1$-seminorm error for the finite element approximation is equal to the squared $L^2$-norm error of its derivative.\n\nSpecifically, we want to compute $E_0(I, p)$, which is the squared $L^2(I)$ error for the best approximation of the function's derivative, $u'(x)$, by a polynomial $q(x)$ of degree at most $p-1$. Let $\\mathcal{P}_{k}(I)$ denote the space of polynomials of degree at most $k$ on the interval $I$. The error is given by:\n$$\nE_0(I, p) = \\min_{q \\in \\mathcal{P}_{p-1}(I)} \\int_a^b |u'(x) - q(x)|^2 \\, dx\n$$\nFrom Hilbert space theory, the minimum is achieved when $q$ is the orthogonal $L^2$ projection of $u'$ onto $\\mathcal{P}_{p-1}(I)$. Let this projection be $\\Pi_{p-1} u'$. The error is then the squared norm of the residual:\n$$\nE_0(I, p) = \\|u' - \\Pi_{p-1} u'\\|_{L^2(I)}^2\n$$\nTo compute this numerically, we employ a standard technique of mapping the physical element $I = [a,b]$ to a reference element $\\hat{I} = [-1,1]$ via the affine transformation $x(\\xi) = a + \\frac{b-a}{2}(\\xi+1)$. The Jacobian of this map is $J = \\frac{b-a}{2}$. An integral transforms as $\\int_a^b f(x) \\, dx = \\int_{-1}^1 f(x(\\xi)) J \\, d\\xi$.\n\nOn the reference element, we use the basis of Legendre polynomials, $\\{\\hat{L}_k(\\xi)\\}_{k=0}^{\\infty}$, which are orthogonal with respect to the standard $L^2$ inner product on $[-1,1]$:\n$$\n\\int_{-1}^1 \\hat{L}_i(\\xi) \\hat{L}_j(\\xi) \\, d\\xi = \\frac{2}{2i+1}\\delta_{ij}\n$$\nwhere $\\delta_{ij}$ is the Kronecker delta. The projection of the transformed derivative, $\\hat{u}'(\\xi) = u'(x(\\xi))$, onto $\\mathcal{P}_{p-1}(\\hat{I})$ is $\\Pi_{p-1}\\hat{u}' = \\sum_{k=0}^{p-1} c_k \\hat{L}_k(\\xi)$, with coefficients $c_k = \\frac{\\langle \\hat{u}', \\hat{L}_k \\rangle}{\\langle \\hat{L}_k, \\hat{L}_k \\rangle}$.\n\nBy the Pythagorean theorem for orthogonal projections, the error on the physical element can be computed as:\n$$\nE_0(I, p) = J \\left( \\|\\hat{u}'\\|_{L^2(\\hat{I})}^2 - \\|\\Pi_{p-1}\\hat{u}'\\|_{L^2(\\hat{I})}^2 \\right)\n$$\nwhere $\\|\\hat{u}'\\|_{L^2(\\hat{I})}^2 = \\int_{-1}^1 |\\hat{u}'(\\xi)|^2 \\, d\\xi$ and $\\|\\Pi_{p-1}\\hat{u}'\\|_{L^2(\\hat{I})}^2 = \\sum_{k=0}^{p-1} \\frac{\\left( \\int_{-1}^1 \\hat{u}'(\\xi)\\hat{L}_k(\\xi) \\, d\\xi \\right)^2}{\\int_{-1}^1 |\\hat{L}_k(\\xi)|^2 \\, d\\xi}$.\nAll integrals are evaluated numerically using high-order Gauss-Legendre quadrature to ensure accuracy.\n\n### 2. Refinement Strategies and Gain Calculation\n\nWith the error computation method established, we evaluate the three refinement strategies for a given element $I=[a,b]$ and degree $p$.\n\n**Current State:** The initial error is $E_{current} = E_0(I, p)$.\n\n**a) $h$-refinement:** The element $I$ is bisected into two children, $I_1 = [a, (a+b)/2]$ and $I_2 = [(a+b)/2, b]$, with the polynomial degree $p$ held constant. The total error after refinement is the sum of errors on the children:\n$$\nE_h(I, p) = E_0(I_1, p) + E_0(I_2, p)\n$$\nThe cost is given as $C_h(p) = p$. The gain is:\n$$\nG_h = \\frac{\\max\\{0, E_{current} - E_h(I, p)\\}}{C_h(p)}\n$$\n\n**b) $p$-refinement:** The polynomial degree on the original element $I$ is increased to $p+1$. The error after refinement is:\n$$\nE_p(I, p) = E_0(I, p+1)\n$$\nThe cost is $C_p = 1$. The gain is:\n$$\nG_p = \\frac{\\max\\{0, E_{current} - E_p(I, p)\\}}{C_p}\n$$\n\n**c) $r$-refinement:** The element $I$ is shrunk to $I_r = [a, a + s(b-a)]$ with a given factor $s$, while the degree $p$ remains unchanged. The resulting error is calculated on this smaller element:\n$$\nE_r(I, p, s) = E_0(I_r, p)\n$$\nThe cost is a given constant $C_r = c_r$. The gain, as defined by the problem, is:\n$$\nG_r = \\frac{\\max\\{0, E_{current} - E_r(I, p, s)\\}}{C_r}\n$$\n\n### 3. Decision Model\n\nFor each test case, the gains $G_h$, $G_p$, and $G_r$ are computed. The decision rule is to select the refinement strategy corresponding to the maximum gain. In case of a tie, where the difference between two or more gains is less than a tolerance of $10^{-12}$, the tie is broken by choosing the strategy with the lowest index in the prescribed order: $h$-refinement (index $0$), $p$-refinement (index $1$), and $r$-refinement (index $2$). This procedure is applied to each test case to determine the optimal action. The implementation will process the specified benchmark functions, one smooth and one with an endpoint singularity, to test the model's behavior under different function regularity conditions.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import special\n\ndef compute_error_squared(u_prime_func, interval, p_degree, n_quad=100):\n    \"\"\"\n    Computes the squared H^1-seminorm best-approximation error on an element.\n\n    This is equivalent to the L^2 projection error of the derivative u' onto\n    the space of polynomials of degree p-1.\n\n    Args:\n        u_prime_func (callable): The derivative of the function to approximate, u'(x).\n        interval (list or tuple): The element interval [a, b].\n        p_degree (int): The polynomial degree of the finite element space. Projection is onto P_{p-1}.\n        n_quad (int): The number of Gauss-Legendre quadrature points.\n\n    Returns:\n        float: The computed squared error E_0(I, p).\n    \"\"\"\n    k_proj = p_degree - 1\n    a, b = interval\n\n    if abs(a - b) < 1e-15:\n        return 0.0\n\n    nodes, weights = np.polynomial.legendre.leggauss(n_quad)\n    \n    jac = (b - a) / 2.0\n    x_phys = jac * nodes + (a + b) / 2.0\n    \n    u_prime_vals_at_ref_nodes = u_prime_func(x_phys)\n    \n    # Compute the squared L2 norm of u' on the reference interval\n    norm_u_prime_sq = np.sum(weights * u_prime_vals_at_ref_nodes**2)\n    \n    # Compute the squared L2 norm of the projection of u'\n    sum_of_proj_coeffs_sq_norm = 0.0\n    if k_proj >= 0:\n        for j in range(k_proj + 1):\n            # Evaluate j-th Legendre polynomial at quadrature nodes\n            L_j_vals = special.eval_legendre(j, nodes)\n            \n            # Compute inner product <u', L_j> on reference interval\n            inner_prod = np.sum(weights * u_prime_vals_at_ref_nodes * L_j_vals)\n            \n            # Squared norm of L_j is 2 / (2j + 1)\n            norm_L_j_sq = 2.0 / (2.0 * j + 1.0)\n            \n            # The j-th term in the sum for the squared norm of the projection is\n            # (<u', L_j>^2) / ||L_j||^2\n            sum_of_proj_coeffs_sq_norm += inner_prod**2 / norm_L_j_sq\n            \n    # Error squared on reference element (by Pythagorean theorem)\n    error_ref_sq = norm_u_prime_sq - sum_of_proj_coeffs_sq_norm\n    \n    # Scale to physical element\n    error_phys_sq = jac * error_ref_sq\n    \n    # Clamp to zero to handle potential small negative values from numerical precision errors\n    return max(0.0, error_phys_sq)\n\ndef solve():\n    \"\"\"\n    Main solver function to run the adaptive refinement decision model on test cases.\n    \"\"\"\n    # Define benchmark functions and their derivatives\n    alpha = 0.6\n    u_prime_smooth = lambda x: np.pi * np.cos(np.pi * x)\n    # The singular function's derivative: u'(x) = alpha * x^(alpha - 1)\n    # Using np.power for safe handling of array inputs and potential negative bases if not careful\n    u_prime_singular = lambda x: alpha * np.power(x, alpha - 1.0, where=x>0, out=np.full_like(x, np.inf))\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (function_type, interval, p, s, c_r)\n        ('smooth', [0.0, 0.5], 2, 0.7, 1.0),\n        ('smooth', [0.5, 1.0], 2, 0.7, 1.0),\n        ('singular', [0.0, 0.5], 2, 0.7, 1.0),\n        ('singular', [0.5, 1.0], 2, 0.7, 1.0),\n    ]\n\n    results = []\n    TOL = 1e-12\n\n    for case in test_cases:\n        func_type, interval, p, s, c_r = case\n        a, b = interval\n        \n        u_prime = u_prime_smooth if func_type == 'smooth' else u_prime_singular\n\n        # 1. Compute current error\n        E_current = compute_error_squared(u_prime, interval, p)\n\n        # 2. Evaluate h-refinement\n        mid = (a + b) / 2.0\n        I1, I2 = [a, mid], [mid, b]\n        E_h = compute_error_squared(u_prime, I1, p) + compute_error_squared(u_prime, I2, p)\n        C_h = float(p)\n        G_h = max(0, E_current - E_h) / C_h\n\n        # 3. Evaluate p-refinement\n        E_p = compute_error_squared(u_prime, interval, p + 1)\n        C_p = 1.0\n        G_p = max(0, E_current - E_p) / C_p\n\n        # 4. Evaluate r-refinement\n        I_r = [a, a + s * (b - a)]\n        E_r = compute_error_squared(u_prime, I_r, p)\n        C_r = c_r\n        G_r = max(0, E_current - E_r) / C_r\n        \n        # 5. Decision logic with tie-breaking\n        gains = [G_h, G_p, G_r]\n        max_gain = -1.0\n        # Find max gain, handling potential -inf/nan if any (not expected here)\n        for g in gains:\n            if g > max_gain:\n                max_gain = g\n\n        best_action_idx = -1\n        # Find best action using tie-breaking rule\n        # Find all actions that are within TOL of the max gain and pick the one with lowest index\n        for i, g in enumerate(gains):\n            if abs(g - max_gain) <= TOL:\n                best_action_idx = i\n                break # Since we iterate in order h, p, r, the first one found is the winner\n        \n        results.append(best_action_idx)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3360869"}, {"introduction": "While *a posteriori* error estimators tell us where refinement is needed, approximation theory can often predict *what kind* of refinement is best. This exercise explores the deep connection between a solution's smoothness, or analyticity, and the effectiveness of $p$-refinement [@problem_id:3360857]. You will use the locations of known solution singularities in the complex plane to map out regions where exponential convergence is expected, providing a powerful *a priori* basis for distinguishing between elements that benefit from higher polynomial degrees ($p$-refinement) and those that require subdivision ($h$-refinement).", "problem": "You are tasked with designing, implementing, and testing an algorithm for $hp$-adaptation informed by analytic continuation estimates in the one-dimensional setting for polynomial approximation of solutions to partial differential equations on a bounded interval. The goal is to determine, on each mesh element, whether to prefer $p$-refinement, $h$-refinement, or suggest an $r$-movement of mesh nodes, based on analyticity information inferred from the nearest singularities of the solution in the complex plane. The method must be principled and arise from fundamental approximation theory for analytic functions.\n\nBegin from the following foundational and well-tested facts:\n\n- If a function is analytic in an open region containing a given closed interval and in particular inside a Bernstein ellipse with foci at the interval endpoints and parameter $\\rho$ with $\\rho > 1$, the best polynomial approximation error on that interval decays geometrically as $\\mathcal{O}(\\rho^{-p})$ in the polynomial degree $p$. This is a classical result in polynomial approximation theory, and a standard tool in spectral methods and the Finite Element Method (FEM). The acronym FEM refers to the Finite Element Method, and $hp$-refinement denotes the combined strategy of mesh-size ($h$) and polynomial-degree ($p$) adaptation.\n- The relationship between complex-plane singularities and the Bernstein ellipse parameter $\\rho$ on a subinterval of the real axis is provided by the inverse of the Joukowsky map, which connects analytic continuation domains in the physical coordinate with circles in an auxiliary complex plane. The ellipse parameter $\\rho$ is determined by the singularity closest to the interval, after mapping the interval to the reference interval and applying the inverse Joukowsky transformation.\n- When a singularity lies on the real axis inside an element, analytic continuation across that interval fails to extend beyond the interval itself, which implies the effective $\\rho$ is at most $1$ and that purely $p$-refinement cannot achieve exponential convergence. This motivates $h$-refinement or $r$-movement strategies.\n\nYour program must implement an $hp$ decision procedure with a quantitative $r$-movement suggestion based on these principles. Consider a one-dimensional domain $[0,1]$ partitioned into $N$ uniform elements. You are given a set of complex singularities $\\{s_k\\}$ in the physical coordinate, a target elementwise error tolerance $\\tau$, a minimum analyticity threshold $\\rho_{\\min} > 1$ indicating the onset of exponential $p$-convergence worth exploiting, a maximum admissible polynomial degree $p_{\\max} \\in \\mathbb{N}$, and an $r$-movement aggressiveness parameter $\\theta \\in (0,1)$.\n\nFor each element $I_j = [a_j,b_j]$, carry out the following, using only the stated base principles and definitions:\n\n- Determine the local Bernstein ellipse parameter $\\rho_j$ induced by the nearest singularity under the affine map from $[a_j,b_j]$ to the reference interval and the inverse Joukowsky map. If a singularity lies on the real axis inside $[a_j,b_j]$, then take $\\rho_j = 1$.\n- From the geometric convergence principle, determine the minimal degree $p_j^\\star$ such that the best polynomial approximation error satisfies a bound of the form $C \\rho_j^{-p_j^\\star} \\le \\tau$ for some benign constant $C$ that you may normalize to $C=1$ for the purpose of a computable rule. If $\\rho_j \\le 1$, set $p_j^\\star = +\\infty$.\n- Classification rule:\n  - If $\\rho_j > \\rho_{\\min}$ and $p_j^\\star \\le p_{\\max}$, classify the element as $p$-admissible (prefer $p$-refinement on that element).\n  - Otherwise, classify it as $h$-candidate (prefer $h$-refinement on that element).\n- $r$-movement suggestion: For each interior interface between adjacent elements, if exactly one of the two adjacent elements is an $h$-candidate and the other is $p$-admissible, compute a suggested interface displacement magnitude $\\delta$ proportional to the local analyticity contrast. Use a dimensionally consistent surrogate based on the contrast in $1/\\rho$ scaled by the union length of the two adjacent elements and by $\\theta$. Aggregate the total normalized $r$-movement suggestion by summing these contributions across all interior interfaces and dividing by the domain length.\n\nYour program must read no input and instead implement exactly the following test suite, each test case defined by $[N, \\{s_k\\}, \\tau, p_{\\max}, \\rho_{\\min}, \\theta]$:\n\n- Test case $1$ (general, analytic far from the real line): $N = 4$, $\\{s_k\\} = \\{0.5 + 1.0\\,\\mathrm{i}\\}$, $\\tau = 10^{-6}$, $p_{\\max} = 12$, $\\rho_{\\min} = 1.05$, $\\theta = 0.25$.\n- Test case $2$ (branch point on the real axis): $N = 4$, $\\{s_k\\} = \\{0.3 + 0.0\\,\\mathrm{i}\\}$, $\\tau = 10^{-3}$, $p_{\\max} = 10$, $\\rho_{\\min} = 1.02$, $\\theta = 0.25$.\n- Test case $3$ (mixed singularities): $N = 8$, $\\{s_k\\} = \\{0.25 + 0.2\\,\\mathrm{i},\\, 0.75 + 0.0\\,\\mathrm{i}\\}$, $\\tau = 10^{-4}$, $p_{\\max} = 10$, $\\rho_{\\min} = 1.05$, $\\theta = 0.30$.\n- Test case $4$ (nearby complex singularity): $N = 4$, $\\{s_k\\} = \\{0.5 + 0.05\\,\\mathrm{i}\\}$, $\\tau = 10^{-2}$, $p_{\\max} = 8$, $\\rho_{\\min} = 1.10$, $\\theta = 0.20$.\n\nFor each test case, your program must output a list of three values:\n\n- $H$: the number of elements classified as $h$-candidates (an integer).\n- $P_{\\max}^{\\mathrm{sel}}$: the maximum of $p_j^\\star$ among elements classified as $p$-admissible (an integer; if there are no $p$-admissible elements, output $0$).\n- $\\mathcal{R}$: the total normalized $r$-movement suggestion computed as described above, rounded to $6$ decimal places (a floating-point number).\n\nYour program should produce a single line of output containing the results for all test cases as a comma-separated list enclosed in square brackets, where each entry is itself the list $[H, P_{\\max}^{\\mathrm{sel}}, \\mathcal{R}]$ for the corresponding test case. For example, an output with two hypothetical cases might look like $[[2,7,0.031415],[0,5,0.0]]$.\n\nAll quantities are dimensionless in this problem; no physical units are used. Angles, where implicit in complex arguments, are in radians by convention, but you will not directly manipulate angles.\n\nYour implementation must be self-contained and must not require any user input, external files, or network access. The numerical results for the specified test suite must be reproducible.", "solution": "The problem requires the design and implementation of an adaptive mesh refinement strategy for one-dimensional numerical methods. Specifically, it asks for a decision procedure to classify elements of a mesh for either polynomial degree refinement ($p$-refinement), mesh size refinement ($h$-refinement), or mesh vertex redistribution ($r$-movement). This decision is to be based on principles of approximation theory concerning the polynomial approximation of analytic functions.\n\nThe foundational principle is that the convergence rate of the best polynomial approximation of a function on an interval $[a, b]$ is dictated by the function's analyticity in the complex plane. If a function is analytic within a Bernstein ellipse with foci at $a$ and $b$, the approximation error in the maximum norm for a polynomial of degree $p$ decays exponentially, with a rate related to the size of this ellipse. The size is parameterized by $\\rho$, the sum of the semi-major and semi-minor axes of the ellipse, where the distance between foci is normalized. The error is bounded by $\\mathcal{O}(\\rho^{-p})$. A larger $\\rho$ implies a larger domain of analyticity and faster convergence. Singularities of the function near the interval $[a, b]$ limit the size of the largest possible Bernstein ellipse, thus degrading the convergence rate.\n\nThe algorithm proceeds in three main stages: element-wise analysis, interface analysis for $r$-movement, and aggregation of results.\n\n**1. Element-wise Analysis**\n\nThe domain, the interval $[0, 1]$, is partitioned into $N$ uniform elements $I_j = [a_j, b_j]$ for $j=0, 1, \\dots, N-1$. For a uniform partition, the vertices are at $x_j = j/N$, so $a_j = j/N$ and $b_j = (j+1)/N$. The length of each element is $h = 1/N$. For each element $I_j$, we perform the following steps:\n\n**a. Determination of the Analyticity Parameter $\\rho_j$**\n\nThe parameter $\\rho_j$ quantifies the local analyticity of the function on element $I_j$. It corresponds to the smallest Bernstein ellipse for that element which contains at least one of the solution's singularities $\\{s_k\\}$.\n\ni. **Mapping to Reference Interval:** We first map the physical element $I_j = [a_j, b_j]$ to the canonical reference interval $\\Xi = [-1, 1]$. The affine map is $z(\\xi) = \\frac{2\\xi - (a_j+b_j)}{b_j-a_j}$. Each singularity $s_k$ in the physical domain is mapped to $z_k = z(s_k)$ in the complex plane relative to the reference interval.\n\nii. **Inverse Joukowsky Map:** The exterior of the interval $\\Xi = [-1, 1]$ can be mapped to the exterior of the unit disk $|w|>1$ in an auxiliary complex plane using the inverse Joukowsky map, $w(z)$. The level sets of $|w(z)| = \\rho > 1$ are precisely the Bernstein ellipses with foci at $\\pm 1$. The parameter $\\rho$ for an ellipse passing through a point $z$ is given by $|w(z)|$. The map is a solution to $z = \\frac{1}{2}(w+w^{-1})$, which gives $w^2 - 2zw + 1 = 0$. The roots are $w = z \\pm \\sqrt{z^2-1}$. To map to the exterior of the unit disk, we must choose the root with magnitude greater than or equal to $1$. A robust way to compute this is to find both roots and take the one with the larger absolute value. For each mapped singularity $z_k$, we compute the corresponding parameter $\\rho_j^{(k)} = \\max(|z_k + \\sqrt{z_k^2-1}|, |z_k - \\sqrt{z_k^2-1}|)$.\n\niii. **Element Parameter $\\rho_j$:** The analyticity of the function on the element $I_j$ is limited by the \"closest\" singularity. Thus, the effective $\\rho_j$ for the element is the minimum of the parameters associated with all singularities: $\\rho_j = \\min_k \\rho_j^{(k)}$.\n\niv. **Special Case (Real Singularity):** If a singularity $s_k$ is real (i.e., $\\text{Im}(s_k)=0$) and lies strictly inside the element, $s_k \\in (a_j, b_j)$, then a Bernstein ellipse cannot be drawn around the interval without enclosing the singularity. In this case, exponential convergence is lost, and we set $\\rho_j = 1$. If a real singularity lies on an element boundary point, it maps to an endpoint of $\\Xi$ ($-1$ or $1$), which yields $\\rho=1$ via the Joukowsky map, correctly capturing the loss of local analyticity.\n\n**b. Estimation of Required Polynomial Degree $p_j^\\star$**\n\nThe best polynomial approximation error $E_p$ of degree $p$ is bounded by $E_p \\le C \\rho_j^{-p}$. To achieve a target tolerance $\\tau$, we need $C \\rho_j^{-p} \\le \\tau$. Following the problem statement, we normalize the constant to $C=1$, yielding the condition $\\rho_j^{-p} \\le \\tau$.\n\n- If $\\rho_j > 1$, we can solve for $p$: $-p \\ln(\\rho_j) \\le \\ln(\\tau)$, which implies $p \\ge -\\frac{\\ln(\\tau)}{\\ln(\\rho_j)}$. The minimal integer degree required is $p_j^\\star = \\lceil -\\frac{\\ln(\\tau)}{\\ln(\\rho_j)} \\rceil$.\n- If $\\rho_j \\le 1$, the condition cannot be satisfied for any finite $p$ (assuming $\\tau < 1$), indicating at best algebraic convergence. In this case, we set $p_j^\\star = +\\infty$.\n\n**c. Element Classification**\n\nBased on $\\rho_j$ and $p_j^\\star$, each element is classified to guide the adaptation strategy.\n\n- **$p$-admissible:** The element is suitable for $p$-refinement if the analyticity is sufficiently strong and the required degree is practical. This holds if $\\rho_j > \\rho_{\\min}$ and $p_j^\\star \\le p_{\\max}$.\n- **$h$-candidate:** Otherwise, the element is marked for $h$-refinement. This occurs if the convergence is too slow ($\\rho_j \\le \\rho_{\\min}$) or if the degree needed to reach the tolerance is prohibitively high ($p_j^\\star > p_{\\max}$).\n\n**2. Interface Analysis for $r$-Movement**\n\nMesh redistribution ($r$-adaptation) aims to move mesh vertices to better resolve solution features, such as regions of low regularity. A suggestion for moving an interface is generated when adjacent elements have a strong contrast in their predicted refinement types.\n\nWe examine each interior interface $x_{j}$ (for $j=1, \\dots, N-1$), which separates elements $I_{j-1}$ and $I_j$. A displacement magnitude $\\delta_j$ is computed for the interface at $x_j$ if and only if one of the adjacent elements is an $h$-candidate and the other is $p$-admissible. If this condition holds, the displacement is:\n$$ \\delta_j = \\theta \\cdot L_{j-1, j} \\cdot \\left|\\frac{1}{\\rho_{j-1}} - \\frac{1}{\\rho_j}\\right| $$\nHere, $\\theta \\in (0, 1)$ is an aggressiveness parameter, $L_{j-1, j}$ is the combined length of the two elements ($L_{j-1, j} = (b_{j-1}-a_{j-1})+(b_j-a_j) = 2/N$ for a uniform mesh), and the term $|\\frac{1}{\\rho_{j-1}} - \\frac{1}{\\rho_j}|$ quantifies the contrast in the geometric convergence factors. If the condition is not met, $\\delta_j=0$.\n\n**3. Aggregation of a Test Case Result**\n\nFinally, for each test case, we compute three summary statistics:\n\n- $H$: The total count of elements classified as $h$-candidates.\n- $P_{\\max}^{\\mathrm{sel}}$: The maximum required degree among all $p$-admissible elements, i.e., $P_{\\max}^{\\mathrm{sel}} = \\max(\\{ p_j^\\star \\mid I_j \\text{ is } p\\text{-admissible} \\} \\cup \\{0\\})$. We include $0$ in the set to handle the case where no elements are $p$-admissible.\n- $\\mathcal{R}$: The total normalized $r$-movement suggestion. This is the sum of all individual interface displacement suggestions, $\\mathcal{R} = \\sum_{j=1}^{N-1} \\delta_j$, normalized by the domain length, which is $1$.\n\nThis procedure provides a quantitative, theoretically-grounded basis for making local decisions in an $hp-r$ adaptive algorithm.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and tests an hp-r adaptation decision algorithm based on\n    analytic continuation estimates.\n    \"\"\"\n\n    test_cases = [\n        # Test case 1\n        (4, {0.5 + 1.0j}, 1e-6, 12, 1.05, 0.25),\n        # Test case 2\n        (4, {0.3 + 0.0j}, 1e-3, 10, 1.02, 0.25),\n        # Test case 3\n        (8, {0.25 + 0.2j, 0.75 + 0.0j}, 1e-4, 10, 1.05, 0.30),\n        # Test case 4\n        (4, {0.5 + 0.05j}, 1e-2, 8, 1.10, 0.20),\n    ]\n\n    all_results = []\n\n    for N, singularities, tau, p_max, rho_min, theta in test_cases:\n        h = 1.0 / N\n        elements_data = []\n\n        # 1. Element-wise Analysis\n        for j in range(N):\n            a_j, b_j = j * h, (j + 1) * h\n            \n            rho_j = np.inf\n            \n            # Check for real singularity inside the element\n            has_real_singularity_inside = False\n            for s in singularities:\n                if np.isreal(s) and a_j < s.real < b_j:\n                    has_real_singularity_inside = True\n                    break\n            \n            if has_real_singularity_inside:\n                rho_j = 1.0\n            else:\n                rho_contributions = []\n                for s_k in singularities:\n                    # Map singularity to reference element coords\n                    z_k = (2 * s_k - (a_j + b_j)) / (b_j - a_j)\n                    \n                    # Handle case where z_k^2 - 1 is negative real\n                    sqrt_val = np.sqrt(z_k**2 - 1)\n                    \n                    # Find the root of w^2 - 2*z_k*w + 1 = 0 with |w| >= 1\n                    w_plus = z_k + sqrt_val\n                    w_minus = z_k - sqrt_val\n                    \n                    # In theory, one has |w|>=1, other |w|<=1.\n                    # Taking max handles branch choice implicitly.\n                    rho_k = max(np.abs(w_plus), np.abs(w_minus))\n                    rho_contributions.append(rho_k)\n                \n                if rho_contributions:\n                    rho_j = min(rho_contributions)\n\n            # Determine required polynomial degree p_star\n            p_star_j = np.inf\n            if rho_j > 1:\n                # Need p >= -log(tau) / log(rho_j)\n                p_star_j = np.ceil(-np.log(tau) / np.log(rho_j))\n            \n            # Classify element\n            is_p_admissible = (rho_j > rho_min) and (p_star_j <= p_max)\n            classification = 'p' if is_p_admissible else 'h'\n            \n            elements_data.append({\n                'rho': rho_j,\n                'p_star': int(p_star_j) if np.isfinite(p_star_j) else np.inf,\n                'class': classification\n            })\n\n        # 2. Aggregation for H and P_max_sel\n        H = sum(1 for el in elements_data if el['class'] == 'h')\n        \n        p_admissible_degrees = [el['p_star'] for el in elements_data if el['class'] == 'p']\n        P_max_sel = max(p_admissible_degrees) if p_admissible_degrees else 0\n\n        # 3. Interface Analysis for r-movement\n        total_delta = 0.0\n        union_length = 2 * h\n        for j in range(1, N):\n            class_left = elements_data[j-1]['class']\n            class_right = elements_data[j]['class']\n            \n            # Check for mismatch in classification\n            if (class_left == 'h' and class_right == 'p') or \\\n               (class_left == 'p' and class_right == 'h'):\n                rho_left = elements_data[j-1]['rho']\n                rho_right = elements_data[j]['rho']\n                \n                # Prevent division by zero if rho can be zero (not possible here)\n                if rho_left > 0 and rho_right > 0:\n                    contrast = abs(1/rho_left - 1/rho_right)\n                    delta_j = theta * union_length * contrast\n                    total_delta += delta_j\n        \n        R = round(total_delta, 6)\n        \n        all_results.append(f\"[{H},{P_max_sel},{R:.6f}]\")\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(all_results)}]\")\n\nsolve()\n```", "id": "3360857"}, {"introduction": "Effective adaptive strategies often combine $h$-, $p$-, and $r$-refinements, but their interactions can introduce subtle challenges. This practice investigates a critical issue: how geometric distortion from $r$-adaptation can compromise the reliability of error indicators used for $p$-adaptation [@problem_id:3360876]. By analyzing a scenario with a near-singular geometric mapping, you will quantify this \"contamination\" and appreciate why robust error estimation must decouple the true approximation error from artifacts of the element's geometry.", "problem": "Consider a single isoparametric quadrilateral finite element obtained by a mapping from a reference square. Let the reference square be $\\hat{\\Omega} = [-1,1] \\times [-1,1]$ with coordinates $(\\xi,\\eta)$, and let the physical element be $\\Omega_\\varepsilon$ with coordinates $(x,y)$ obtained by the $r$-movement mapping $F_\\varepsilon : \\hat{\\Omega} \\to \\Omega_\\varepsilon$ defined by\n$$\nx = \\xi,\\quad y = \\varepsilon\\,\\eta,\n$$\nwhere $\\varepsilon > 0$ controls the geometric distortion and the Jacobian determinant equals $\\det(J_{F_\\varepsilon}) = \\varepsilon$. Let $u:\\Omega_\\varepsilon \\to \\mathbb{R}$ be a smooth target field. In standard $p$-refinement error indication, hierarchical $p$-based indicators are formed from the tail of the polynomial expansion added between degree $p$ and degree $p+1$. However, under near-singular Jacobians (small $\\varepsilon$), the error indicator can be distorted by the geometry mapping rather than reflecting the approximation quality. The goal is to analyze the robustness of such indicators and design modified indicators that decouple approximation error from geometric distortion.\n\nUse the following fundamental bases and definitions:\n- The chain rule for gradients under a smooth mapping $F$: if $v = \\hat{v} \\circ F^{-1}$, then\n$$\n\\nabla_x v = J_{F}^{-T} \\,\\nabla_{\\xi} \\hat{v},\n$$\nwhere $J_{F}$ is the Jacobian matrix of $F$, and the physical integral transforms as\n$$\n\\int_{\\Omega} g(x)\\,dx = \\int_{\\hat{\\Omega}} g\\big(F(\\xi)\\big)\\,\\det(J_{F})\\,d\\xi.\n$$\n- The $H^1$ seminorm in the physical element is\n$$\n|v|_{H^1(\\Omega)}^2 = \\int_{\\Omega} \\|\\nabla_x v\\|_2^2\\,dx.\n$$\n- On the reference element, use a tensor-product hierarchical orthonormal basis formed by normalized Legendre polynomials $\\{L_i(\\xi)\\}_{i=0}^{\\infty}$ and $\\{L_j(\\eta)\\}_{j=0}^{\\infty}$, where $L_n(t) = \\sqrt{\\frac{2n+1}{2}}\\,P_n(t)$ and $P_n$ is the degree-$n$ Legendre polynomial. The $p$-refinement tail $\\delta_{p\\to p+1}$ is the difference between the degree-$(p+1)$ projection and the degree-$p$ projection on this basis.\n\nDefine a family of target fields parameterized by an integer $n_y \\ge 0$:\n$$\nu_{n_y}(x,y) = \\begin{cases}\n\\sin(\\pi x), & n_y = 0,\\\\[4pt]\n\\sin(\\pi x)\\,\\sin(\\pi n_y y), & n_y \\ge 1,\n\\end{cases}\n$$\nall of which are smooth on $\\Omega_\\varepsilon$ for any $\\varepsilon > 0$.\n\nTasks:\n1. Construct the orthonormal Legendre tensor-product basis up to degree $p+1$ in each coordinate on $\\hat{\\Omega}$.\n2. Compute the $L^2$ orthogonal projection coefficients of $u_{n_y}\\circ F_\\varepsilon$ onto degrees up to $p+1$ and up to $p$, and form the hierarchical tail function $\\delta_{p\\to p+1}$ on $\\hat{\\Omega}$.\n3. Starting from the fundamental mapping identities above, analyze the behavior (robustness) of the following two indicators with respect to the geometric distortion parameter $\\varepsilon$:\n   - A naive $p$-based $H^1$-seminorm indicator that measures the tail in the physical space,\n     $$\n     \\mathcal{E}^{\\text{naive}}_{H^1}(\\varepsilon) = \\int_{\\Omega_\\varepsilon} \\|\\nabla_x \\delta_{p\\to p+1}\\|_2^2\\,dx.\n     $$\n   - A modified indicator designed to decouple approximation error from geometric distortion by measuring the tail in the reference space,\n     $$\n     \\mathcal{E}^{\\text{mod}}_{\\hat{H}^1} = \\int_{\\hat{\\Omega}} \\|\\nabla_{\\xi} \\delta_{p\\to p+1}\\|_2^2\\,d\\xi d\\eta.\n     $$\n4. Additionally, measure the $L^2$ energy of the tail in both physical and reference spaces,\n   $$\n   \\mathcal{E}^{\\text{phys}}_{L^2}(\\varepsilon) = \\int_{\\Omega_\\varepsilon} |\\delta_{p\\to p+1}|^2\\,dx,\\quad\n   \\mathcal{E}^{\\text{ref}}_{L^2} = \\int_{\\hat{\\Omega}} |\\delta_{p\\to p+1}|^2\\,d\\xi d\\eta,\n   $$\n   and examine how the ratio $\\mathcal{R}_{L^2}(\\varepsilon) = \\frac{\\mathcal{E}^{\\text{phys}}_{L^2}(\\varepsilon)}{\\mathcal{E}^{\\text{ref}}_{L^2}}$ varies with $\\varepsilon$.\n\nImplementation details:\n- Use Gaussian quadrature on $\\hat{\\Omega}$ for all integrals; choose a sufficiently high order to resolve the expansions. \n- Implement Legendre polynomials via their three-term recurrence and compute their derivatives for gradient calculations. \n- Use the orthonormal basis property to obtain projection coefficients by inner products on $\\hat{\\Omega}$.\n\nTest suite:\n- Fix $p = 4$ and use the following $(\\varepsilon, n_y)$ parameter pairs:\n  1. $(1.0, 1)$, a general case without distortion,\n  2. $(0.1, 1)$, moderate distortion,\n  3. $(0.01, 1)$, near-singular Jacobian,\n  4. $(0.01, 0)$, edge case with no $y$-variation,\n  5. $(0.01, 5)$, strong $y$-variation under near-singular Jacobian.\n\nFor each test case, compute two floats:\n- The ratio $\\mathcal{R}_{H^1}(\\varepsilon) = \\dfrac{\\mathcal{E}^{\\text{naive}}_{H^1}(\\varepsilon)}{\\mathcal{E}^{\\text{mod}}_{\\hat{H}^1}}$,\n- The ratio $\\mathcal{R}_{L^2}(\\varepsilon) = \\dfrac{\\mathcal{E}^{\\text{phys}}_{L^2}(\\varepsilon)}{\\mathcal{E}^{\\text{ref}}_{L^2}}$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, ordered by test case and flattened. That is,\n$[\\mathcal{R}_{H^1}(\\varepsilon_1),\\mathcal{R}_{L^2}(\\varepsilon_1),\\ldots,\\mathcal{R}_{H^1}(\\varepsilon_5),\\mathcal{R}_{L^2}(\\varepsilon_5)]$\nwith each entry as a float. No units are involved; all quantities are dimensionless. The program must be fully self-contained and require no input.", "solution": "The problem requires an analysis of the robustness of $p$-refinement error indicators for finite element methods when subjected to geometric distortion. The core of the problem is to compare a \"naive\" error indicator, which is directly evaluated in the distorted physical space, with a \"modified\" indicator evaluated in the undistorted reference space. This comparison reveals how geometric factors can contaminate the error estimate, which should ideally reflect only the approximation error of the solution by the polynomial basis.\n\nThe analysis is performed on a single quadrilateral element $\\Omega_\\varepsilon$ in the physical $(x,y)$ coordinate system. This element is derived from a canonical reference square $\\hat{\\Omega} = [-1,1] \\times [-1,1]$ in the $(\\xi,\\eta)$ coordinate system via the mapping $F_\\varepsilon: (\\xi,\\eta) \\mapsto (x,y)$ given by:\n$$\nx = \\xi, \\quad y = \\varepsilon\\eta\n$$\nThe parameter $\\varepsilon > 0$ controls the element's aspect ratio. As $\\varepsilon \\to 0$, the element becomes increasingly distorted or \"squashed\". The Jacobian matrix of this mapping, its determinant, and its inverse transpose are fundamental to the analysis:\n$$\nJ_{F_\\varepsilon} = \\begin{pmatrix} 1 & 0 \\\\ 0 & \\varepsilon \\end{pmatrix}, \\quad \\det(J_{F_\\varepsilon}) = \\varepsilon, \\quad J_{F_\\varepsilon}^{-T} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1/\\varepsilon \\end{pmatrix}\n$$\nThe relationship between gradients in the physical and reference spaces is given by the chain rule: $\\nabla_x v = J_{F_\\varepsilon}^{-T} \\nabla_\\xi \\hat{v}$, where $\\hat{v} = v \\circ F_\\varepsilon$.\n\nThe target function to be approximated is $u_{n_y}(x,y)$, which is pulled back to the reference element to define $\\hat{u}_{n_y}(\\xi,\\eta) = u_{n_y}(F_\\varepsilon(\\xi,\\eta))$:\n$$\n\\hat{u}_{n_y}(\\xi,\\eta) =\n\\begin{cases}\n\\sin(\\pi\\xi) & \\text{if } n_y = 0 \\\\\n\\sin(\\pi\\xi)\\sin(\\pi n_y \\varepsilon \\eta) & \\text{if } n_y \\ge 1\n\\end{cases}\n$$\nThe approximation is performed using a tensor-product basis of orthonormal Legendre polynomials, $\\phi_{ij}(\\xi,\\eta) = L_i(\\xi)L_j(\\eta)$. The error indicator is based on the \"tail\" of the expansion, $\\delta_{p\\to p+1}$, which represents the components of the projected function in polynomial degrees from $p$ to $p+1$. This tail function is defined on the reference element $\\hat{\\Omega}$ as\n$$\n\\delta_{p\\to p+1} = \\hat{u}_{p+1} - \\hat{u}_p = \\sum_{i=0}^{p+1}\\sum_{j=0}^{p+1} c_{ij} \\phi_{ij} - \\sum_{i=0}^{p}\\sum_{j=0}^{p} c_{ij} \\phi_{ij} = \\sum_{(i,j) \\in \\text{tail set}} c_{ij} \\phi_{ij}\n$$\nwhere $p=4$, and the coefficients $c_{ij}$ are the $L^2(\\hat{\\Omega})$-projection coefficients of $\\hat{u}_{n_y}$:\n$$\nc_{ij} = \\int_{\\hat{\\Omega}} \\hat{u}_{n_y}(\\xi,\\eta) \\phi_{ij}(\\xi,\\eta) \\,d\\xi d\\eta\n$$\nThese integrals, and all subsequent ones, are computed numerically using high-order Gauss-Legendre quadrature.\n\nWe analyze four quantities:\n1.  **Reference $L^2$ Energy**: $\\mathcal{E}^{\\text{ref}}_{L^2} = \\int_{\\hat{\\Omega}} |\\delta_{p\\to p+1}|^2\\,d\\xi d\\eta$. Due to the orthonormality of the basis $\\{\\phi_{ij}\\}$, this simplifies to the sum of squared coefficients in the tail:\n    $$\n    \\mathcal{E}^{\\text{ref}}_{L^2} = \\sum_{(i,j) \\in \\text{tail set}} c_{ij}^2\n    $$\n2.  **Physical $L^2$ Energy**: $\\mathcal{E}^{\\text{phys}}_{L^2}(\\varepsilon) = \\int_{\\Omega_\\varepsilon} |\\delta_{p\\to p+1}|^2\\,dx dy$. By transforming the integral to the reference element, we find a direct relationship with the reference energy:\n    $$\n    \\mathcal{E}^{\\text{phys}}_{L^2}(\\varepsilon) = \\int_{\\hat{\\Omega}} |\\delta_{p\\to p+1}|^2 \\det(J_{F_\\varepsilon})\\,d\\xi d\\eta = \\varepsilon \\int_{\\hat{\\Omega}} |\\delta_{p\\to p+1}|^2\\,d\\xi d\\eta = \\varepsilon \\mathcal{E}^{\\text{ref}}_{L^2}\n    $$\n    This leads to an analytical result for the first ratio: $\\mathcal{R}_{L^2}(\\varepsilon) = \\mathcal{E}^{\\text{phys}}_{L^2}(\\varepsilon) / \\mathcal{E}^{\\text{ref}}_{L^2} = \\varepsilon$. This provides a valuable sanity check for the numerical implementation.\n\n3.  **Modified $H^1$ Indicator**: $\\mathcal{E}^{\\text{mod}}_{\\hat{H}^1} = \\int_{\\hat{\\Omega}} \\|\\nabla_{\\xi} \\delta_{p\\to p+1}\\|_2^2\\,d\\xi d\\eta$. This indicator measures the gradient of the tail function in the pristine geometry of the reference element, thereby isolating the approximation error from geometric effects. It is computed via quadrature:\n    $$\n    \\mathcal{E}^{\\text{mod}}_{\\hat{H}^1} = \\int_{-1}^1\\int_{-1}^1 \\left( \\left(\\frac{\\partial \\delta_{p\\to p+1}}{\\partial\\xi}\\right)^2 + \\left(\\frac{\\partial \\delta_{p\\to p+1}}{\\partial\\eta}\\right)^2 \\right) \\,d\\xi d\\eta\n    $$\n4.  **Naive $H^1$ Indicator**: $\\mathcal{E}^{\\text{naive}}_{H^1}(\\varepsilon) = \\int_{\\Omega_\\varepsilon} \\|\\nabla_x \\delta_{p\\to p+1}\\|_2^2\\,dx dy$. This indicator is contaminated by the geometry. Transforming it to the reference element reveals this dependency:\n    $$\n    \\mathcal{E}^{\\text{naive}}_{H^1}(\\varepsilon) = \\int_{\\hat{\\Omega}} \\|J_{F_\\varepsilon}^{-T} \\nabla_\\xi \\delta_{p\\to p+1}\\|_2^2 \\det(J_{F_\\varepsilon}) \\,d\\xi d\\eta\n    $$\n    Substituting the specific Jacobian terms for our mapping yields:\n    $$\n    \\nabla_x \\delta \\leftrightarrow \\begin{pmatrix} \\partial_\\xi \\delta \\\\ \\varepsilon^{-1} \\partial_\\eta \\delta \\end{pmatrix}, \\quad \\| \\nabla_x \\delta \\|_2^2 \\leftrightarrow (\\partial_\\xi \\delta)^2 + \\varepsilon^{-2}(\\partial_\\eta \\delta)^2\n    $$\n    The integral becomes:\n    $$\n    \\mathcal{E}^{\\text{naive}}_{H^1}(\\varepsilon) = \\int_{\\hat{\\Omega}} \\left( (\\partial_\\xi \\delta)^2 + \\frac{1}{\\varepsilon^2}(\\partial_\\eta \\delta)^2 \\right) \\varepsilon \\,d\\xi d\\eta = \\int_{\\hat{\\Omega}} \\left( \\varepsilon(\\partial_\\xi \\delta)^2 + \\frac{1}{\\varepsilon}(\\partial_\\eta \\delta)^2 \\right) \\,d\\xi d\\eta\n    $$\n    The term $1/\\varepsilon$ explicitly shows that the naive indicator is sensitive to geometric distortion. As $\\varepsilon \\to 0$, this term can cause $\\mathcal{E}^{\\text{naive}}_{H^1}$ to behave very differently from $\\mathcal{E}^{\\text{mod}}_{\\hat{H}^1}$, which is what we aim to quantify with the ratio $\\mathcal{R}_{H^1}(\\varepsilon) = \\mathcal{E}^{\\text{naive}}_{H^1}(\\varepsilon) / \\mathcal{E}^{\\text{mod}}_{\\hat{H}^1}$.\n\nA special case occurs for $n_y=0$, where $\\hat{u}_0(\\xi,\\eta) = \\sin(\\pi\\xi)$. This function is independent of $\\eta$, so its projection coefficients $c_{ij}$ are non-zero only for $j=0$. The tail function $\\delta_{p\\to p+1}$ will also be independent of $\\eta$, making its derivative $\\partial_\\eta \\delta_{p\\to p+1} = 0$. In this case, the expressions for the $H^1$ indicators simplify to:\n$$\n\\mathcal{E}^{\\text{mod}}_{\\hat{H}^1} = \\int_{\\hat{\\Omega}} (\\partial_\\xi \\delta)^2 \\, d\\xi d\\eta \\quad \\text{and} \\quad \\mathcal{E}^{\\text{naive}}_{H^1}(\\varepsilon) = \\int_{\\hat{\\Omega}} \\varepsilon (\\partial_\\xi \\delta)^2 \\, d\\xi d\\eta = \\varepsilon \\mathcal{E}^{\\text{mod}}_{\\hat{H}^1}\n$$\nTherefore, for the $n_y=0$ case, we expect $\\mathcal{R}_{H^1}(\\varepsilon) = \\varepsilon$. This provides another crucial verification point for the implementation. For $n_y \\ge 1$ and small $\\varepsilon$, the term $(\\partial_\\eta \\delta)^2/\\varepsilon$ is expected to dominate, causing $\\mathcal{R}_{H^1}(\\varepsilon)$ to become large, demonstrating the non-robustness of the naive indicator.\n\nThe implementation proceeds by setting up a high-order 2D Gaussian quadrature rule on $\\hat{\\Omega}$. Orthonormal Legendre basis functions and their derivatives are pre-computed at the quadrature points. For each test case, the projection coefficients $c_{ij}$ are calculated, followed by the tail function $\\delta_{p\\to p+1}$ and its gradient components on the quadrature grid. Finally, the four energy quantities are integrated numerically to compute the desired ratios.", "answer": "```python\nimport numpy as np\nfrom scipy.special import legendre, roots_legendre\n\ndef legendre_basis_and_derivs(max_deg, x):\n    \"\"\"\n    Computes orthonormal Legendre basis functions L_n(x) and their derivatives L'_n(x).\n    L_n(t) = sqrt((2n+1)/2) * P_n(t), where P_n is the standard Legendre polynomial.\n\n    Args:\n        max_deg (int): Maximum degree of polynomials to compute.\n        x (np.ndarray): 1D array of points in [-1, 1] to evaluate the functions at.\n\n    Returns:\n        tuple[np.ndarray, np.ndarray]:\n            - L_vals: (max_deg+1, num_pts) array of L_n(x) values.\n            - L_prime_vals: (max_deg+1, num_pts) array of L'_n(x) values.\n    \"\"\"\n    num_pts = len(x)\n    L_vals = np.zeros((max_deg + 1, num_pts))\n    L_prime_vals = np.zeros((max_deg + 1, num_pts))\n\n    for n in range(max_deg + 1):\n        Pn = legendre(n)\n        Pn_prime = Pn.deriv(1)\n        \n        norm_const = np.sqrt((2 * n + 1) / 2.0)\n        \n        L_vals[n, :] = norm_const * Pn(x)\n        L_prime_vals[n, :] = norm_const * Pn_prime(x)\n        \n    return L_vals, L_prime_vals\n\ndef solve():\n    \"\"\"\n    Solves the problem of analyzing error indicator robustness for a distorted finite element.\n    \"\"\"\n    p = 4\n    test_cases = [\n        (1.0, 1),\n        (0.1, 1),\n        (0.01, 1),\n        (0.01, 0),\n        (0.01, 5),\n    ]\n\n    # Use a quadrature rule that is sufficiently accurate for the integrands.\n    # The integrands involve products of polynomials and transcendental functions.\n    # A high order is chosen for safety.\n    Nq = 32\n    xi_q, w_q = roots_legendre(Nq)\n    \n    # Pre-compute basis function values and derivatives at quadrature points\n    max_deg = p + 1\n    L_vals, L_prime_vals = legendre_basis_and_derivs(max_deg, xi_q)\n    \n    # 2D quadrature points and weights\n    XI, ETA = np.meshgrid(xi_q, xi_q)\n    W_2D = np.outer(w_q, w_q)\n    \n    results = []\n    \n    for eps, ny in test_cases:\n        # Define the target function on the reference element's quadrature grid\n        if ny == 0:\n            u_hat_vals = np.sin(np.pi * XI)\n        else:\n            u_hat_vals = np.sin(np.pi * XI) * np.sin(np.pi * ny * eps * ETA)\n            \n        # Compute L2 projection coefficients C_ij\n        C = np.zeros((max_deg + 1, max_deg + 1))\n        for i in range(max_deg + 1):\n            L_i_vals_2D = np.tile(L_vals[i, :], (Nq, 1))\n            for j in range(max_deg + 1):\n                L_j_vals_2D = np.tile(L_vals[j, :], (Nq, 1)).T\n                phi_ij_vals = L_i_vals_2D * L_j_vals_2D\n                C[i, j] = np.sum(W_2D * u_hat_vals * phi_ij_vals)\n\n        # Compute reference L^2 energy from coefficients (more accurate)\n        # Tail indices: (i,j) where i=p+1 or j=p+1\n        c_tail_sq = np.sum(C[p + 1, :]**2) + np.sum(C[:p + 1, p + 1]**2)\n        E_ref_L2 = c_tail_sq\n        \n        # Compute physical L^2 energy using the analytical relation\n        E_phys_L2 = eps * E_ref_L2\n        \n        # Compute tail function and its gradient on the grid\n        delta = np.zeros((Nq, Nq))\n        delta_xi = np.zeros((Nq, Nq))\n        delta_eta = np.zeros((Nq, Nq))\n        \n        for i in range(max_deg + 1):\n            L_i_vals_2D = np.tile(L_vals[i, :], (Nq, 1))\n            L_prime_i_vals_2D = np.tile(L_prime_vals[i, :], (Nq, 1))\n            for j in range(max_deg + 1):\n                if i > p or j > p:\n                    L_j_vals_2D = np.tile(L_vals[j, :], (Nq, 1)).T\n                    L_prime_j_vals_2D = np.tile(L_prime_vals[j, :], (Nq, 1)).T\n                    \n                    phi_ij = L_i_vals_2D * L_j_vals_2D\n                    grad_phi_ij_xi = L_prime_i_vals_2D * L_j_vals_2D\n                    grad_phi_ij_eta = L_i_vals_2D * L_prime_j_vals_2D\n                    \n                    delta += C[i, j] * phi_ij\n                    delta_xi += C[i, j] * grad_phi_ij_xi\n                    delta_eta += C[i, j] * grad_phi_ij_eta\n\n        # Compute H^1 seminorm indicators using quadrature\n        integrand_mod = delta_xi**2 + delta_eta**2\n        E_mod_H1 = np.sum(W_2D * integrand_mod)\n        \n        integrand_naive = eps * delta_xi**2 + (1/eps) * delta_eta**2\n        E_naive_H1 = np.sum(W_2D * integrand_naive)\n\n        # Compute ratios\n        if np.isclose(E_mod_H1, 0.0):\n            R_H1 = E_naive_H1 # Should be 0 if E_mod_H1 is 0, unless of numerical error\n        else:\n            R_H1 = E_naive_H1 / E_mod_H1\n\n        if np.isclose(E_ref_L2, 0.0):\n            R_L2 = E_phys_L2 # Should be 0 if E_ref_L2 is 0\n        else:\n            R_L2 = E_phys_L2 / E_ref_L2\n        \n        results.extend([R_H1, R_L2])\n        \n    print(f\"[{','.join(f'{r:.8f}' for r in results)}]\")\n\nsolve()\n```", "id": "3360876"}]}