{"hands_on_practices": [{"introduction": "Before any operator can be learned, a high-quality dataset of input-output pairs must be generated. This practice guides you through designing a principled data generation pipeline for a parametric elliptic partial differential equation ($PDE$), a common task in scientific machine learning. By working through the requirements for sampling random coefficient fields and solving the corresponding $PDE$, you will solidify your understanding of how to create a dataset that is both statistically representative and numerically accurate [@problem_id:3427015].", "problem": "Consider the parametric second-order elliptic partial differential equation (PDE) on the unit square domain $\\Omega = [0,1]^2$ with homogeneous Dirichlet boundary conditions,\n$$\n-\\Delta u(x) + a(x)\\,u(x) = f(x), \\quad x \\in \\Omega, \\qquad u|_{\\partial \\Omega} = 0,\n$$\nwhere $a:\\Omega \\to \\mathbb{R}$ is an unknown coefficient field and $f:\\Omega \\to \\mathbb{R}$ is a source field. In the operator learning setting, one seeks to learn the solution operator mapping $(a,f) \\mapsto u$ from training pairs $\\{(a_i,f_i),u_i\\}_{i=1}^{N}$ generated numerically. Suppose the training distribution for $a$ is specified as a mean-zero, stationary Gaussian random field with prescribed correlation length $\\ell_a>0$ and variance $\\sigma_a^2>0$, and $f$ is drawn independently from a mean-zero, stationary Gaussian random field with correlation length $\\ell_f>0$ and variance $\\sigma_f^2>0$. The goal is to describe a principled end-to-end sampling strategy that produces statistically consistent $(a,f)$ fields and numerically accurate $u$ consistent with the PDE.\n\nA principled strategy must (i) ensure that the sampled Gaussian random fields have the target covariance structure with correlation lengths $\\ell_a$ and $\\ell_f$, (ii) guarantee that the PDE is well-posed with the sampled $a$, and (iii) produce numerical solutions $u$ with a discretization that resolves both the PDE scales and the random field correlation scales.\n\nFrom first principles, recall that a mean-zero, stationary Gaussian random field $g$ on $\\mathbb{R}^d$ is characterized by its covariance function $C_g(r) = \\mathbb{E}[g(x)\\,g(x+r)]$, which by Bochner's theorem is positive-definite if and only if the Fourier transform $\\widehat{C}_g(k)$ is a nonnegative finite measure, often represented by a spectral density $S_g(k) \\ge 0$ when $C_g$ is integrable. For a squared-exponential covariance in $d$ dimensions,\n$$\nC_g(r) = \\sigma^2 \\exp\\!\\left(-\\frac{\\|r\\|^2}{2\\,\\ell^2}\\right),\n$$\nthe spectral density is\n$$\nS_g(k) = \\sigma^2 (2\\pi\\ell^2)^{d/2} \\exp\\!\\left(-\\frac{\\ell^2 \\|k\\|^2}{2}\\right).\n$$\nOn a periodic box of side length $L>0$, sampled on a uniform grid with spacing $h>0$ and $n=L/h$ points per direction, a stationary Gaussian random field with spectral density $S(k)$ may be sampled by drawing complex Gaussian Fourier coefficients $\\widehat{g}(k)$ with variance proportional to $S(k)$ at discrete wavevectors $k$ and then imposing Hermitian symmetry to obtain a real-valued field via the inverse discrete Fourier transform, provided the Nyquist wavenumber $k_{\\max}\\approx \\pi/h$ resolves the support of $S(k)$.\n\nFor the PDE, well-posedness is implied by the Lax–Milgram theorem when $a(x)\\ge a_0>0$ almost everywhere, ensuring coercivity of the bilinear form\n$$\n\\mathcal{B}(u,v) = \\int_{\\Omega} \\big( \\nabla u \\cdot \\nabla v + a\\,u\\,v \\big)\\,dx.\n$$\nA conforming finite element method (FEM) on a triangulation with mesh size $h$ yields approximate solutions $u_h$ in a finite-dimensional subspace $V_h \\subset H_0^1(\\Omega)$ by seeking $u_h \\in V_h$ such that\n$$\n\\int_{\\Omega} \\big( \\nabla u_h \\cdot \\nabla v_h + a\\,u_h\\,v_h \\big)\\,dx = \\int_{\\Omega} f\\,v_h\\,dx \\quad \\text{for all } v_h \\in V_h,\n$$\nwith error controlled by standard estimates provided $h$ resolves the scales of variation in $a$ and $f$ relative to $\\ell_a$ and $\\ell_f$.\n\nWhich option below most accurately describes a principled and scientifically sound sampling strategy to generate training pairs $(a,f)\\mapsto u$ consistent with the above requirements?\n\nA. Sample $a$ and $f$ as follows: choose $\\ell_a>0$, $\\sigma_a>0$, and construct a periodic discretization of $\\Omega$ with uniform grid spacing $h$ satisfying $h \\le \\min(\\ell_a,\\ell_f)/4$; define the discrete wavevectors $k$ on the corresponding Fourier grid. For $a$, set $S_a(k) = \\sigma_a^2 (2\\pi \\ell_a^2)^{2/2} \\exp(-\\ell_a^2 \\|k\\|^2/2)$, draw independent complex Gaussian coefficients $\\widehat{g}_a(k) \\sim \\mathcal{N}_{\\mathbb{C}}(0,S_a(k))$ for each $k$ with Hermitian symmetry $\\widehat{g}_a(-k)=\\overline{\\widehat{g}_a(k)}$, and obtain a real-valued mean-zero field $g_a$ by inverse discrete Fourier transform. Define $a(x)=a_0+g_a(x)$ with $a_0>3\\,\\sigma_a$ to ensure $a(x)\\ge a_0/2>0$ with high probability. Sample $f$ independently with the same procedure using $(\\ell_f,\\sigma_f)$ and spectral density $S_f(k)$. Solve $-\\Delta u + a u = f$ using a conforming finite element method on the same mesh (with integration of $a$ and $f$ by projection onto $V_h$), and verify solver tolerances by ensuring the residual in the discrete weak form is below a prescribed threshold. Store $(a,f,u)$ as a training triple. Repeat to generate independent samples.\n\nB. On a coarse grid with spacing $h=\\ell_a$, draw $a(x_i)\\sim \\mathcal{N}(0,\\sigma_a^2)$ i.i.d. and then convolve the grid values with a compactly supported kernel of radius $\\ell_a$ to introduce correlation; draw $f(x_i)\\sim \\mathcal{N}(0,\\sigma_f^2)$ i.i.d.; solve the PDE with a first-order upwind finite difference scheme, and if $a(x_i)<0$, replace it by $|a(x_i)|$ pointwise. The convolution ensures the correlation length and the pointwise absolute value ensures positivity.\n\nC. Use eigenfunctions of the Laplace operator with Dirichlet boundary conditions $\\{-\\Delta e_m = \\lambda_m e_m\\}$ on $\\Omega$. Sample $a$ by $a(x)=\\sum_{m=1}^{M} \\xi_m e_m(x)$ with $\\xi_m\\sim\\mathcal{N}(0,1/m)$ independent, and declare the correlation length to be $\\ell_a=1/\\sqrt{M}$. Sample $f$ similarly with the same $M$ and solve the PDE by a spectral Galerkin method. This uses orthogonal bases, so the fields are Gaussian with the desired correlation length.\n\nD. Sample $a$ in the Fourier domain by drawing $\\widehat{a}(k)\\sim \\mathcal{N}_{\\mathbb{C}}(0,\\sigma_a^2)$ without imposing any symmetry, inverse transform to obtain a complex-valued $a$, and set the real part as the usable $a$. For $f$, use a deterministic oscillatory source $f(x)=\\sin(1000\\,\\pi x_1)$ to excite high-frequency responses. Solve the PDE on a grid with $h=\\ell_a/2$ using a mixed finite difference scheme and discard runs where the solver fails to converge. This captures a broad frequency content and uses randomization, satisfying the sampling demand.\n\nSelect the option that satisfies all three criteria and is grounded in the foundational characterization of stationary Gaussian random fields and standard well-posedness and discretization theory for elliptic PDEs. There may be one or more correct options.", "solution": "The user has requested a critical validation of the problem statement, followed by a derivation of the correct answer and a detailed evaluation of each provided option.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- **PDE:** $-\\Delta u(x) + a(x)\\,u(x) = f(x)$ on the domain $\\Omega = [0,1]^2$.\n- **Boundary Conditions:** Homogeneous Dirichlet, $u|_{\\partial \\Omega} = 0$.\n- **Coefficient Field $a(x)$:** Sampled from a mean-zero, stationary Gaussian random field (GRF) with correlation length $\\ell_a>0$ and variance $\\sigma_a^2>0$.\n- **Source Field $f(x)$:** Sampled independently from a mean-zero, stationary GRF with correlation length $\\ell_f>0$ and variance $\\sigma_f^2>0$.\n- **Task:** Learn the operator $(a,f) \\mapsto u$ from training pairs $\\{(a_i,f_i),u_i\\}_{i=1}^{N}$.\n- **Strategy Requirements:**\n    1. Sampled GRFs $(a, f)$ must have the target covariance structure.\n    2. The PDE must be well-posed with the sampled $a$.\n    3. The numerical solution $u$ must be on a discretization that resolves all relevant scales.\n- **Background on GRF Sampling:** For a squared-exponential covariance $C_g(r) = \\sigma^2 \\exp(-\\|r\\|^2 / (2\\ell^2))$ in $d$ dimensions, the spectral density is $S_g(k) = \\sigma^2 (2\\pi\\ell^2)^{d/2} \\exp(-\\ell^2 \\|k\\|^2 / 2)$. A stationary GRF can be sampled on a periodic grid by generating complex Gaussian Fourier coefficients $\\widehat{g}(k)$ with variance proportional to $S_g(k)$, imposing Hermitian symmetry, and applying an inverse discrete Fourier transform. The discretization must satisfy the Nyquist criterion.\n- **Background on PDE Well-Posedness and Numerics:** Well-posedness is implied by the Lax-Milgram theorem if $a(x) \\ge a_0 > 0$ almost everywhere. A conforming finite element method (FEM) on a mesh of size $h$ provides an approximate solution $u_h$, with accuracy dependent on $h$ resolving the scales of $a$ and $f$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem statement is evaluated against the specified criteria:\n- **Scientifically Grounded:** The problem is firmly rooted in established fields: numerical analysis for PDEs, the theory of stochastic processes (specifically Gaussian random fields), and the emerging area of operator learning. The methods described (spectral synthesis of GRFs, FEM, Lax-Milgram theorem) are standard and correct.\n- **Well-Posed:** The problem itself is a meta-problem asking for a valid procedure. It is well-posed in that it requests an evaluation of procedures against clear, stated criteria. The underlying physical PDE's well-posedness is a key part of the question.\n- **Objective:** The language is technical, precise, and free of subjective claims. The criteria for a \"principled strategy\" are clearly defined.\n- **Complete and Consistent:** The problem provides sufficient theoretical background on both GRFs and the PDE to allow for a rigorous evaluation of the options. There are no internal contradictions. The mention of sampling on a periodic box for a problem on a square domain with Dirichlet boundaries is a standard practical approach in the field, representing a valid approximation rather than a contradiction.\n- **Realistic and Feasible:** This setup is representative of a common workflow in scientific machine learning for creating surrogate models of physical systems governed by PDEs with uncertain inputs.\n- **Other Flaws:** The problem is not trivial, ill-posed, or unverifiable. It presents a legitimate, non-trivial challenge that integrates concepts from multiple STEM disciplines.\n\n**Step 3: Verdict and Action**\nThe problem statement is **valid**. It is a well-formulated question in the field of computational science and machine learning. The solution process will now proceed.\n\n### Solution Derivation and Option Analysis\n\nA principled data generation strategy must correctly implement the three requirements: (1) accurate GRF sampling, (2) ensuring PDE well-posedness, and (3) using an appropriate numerical solver with adequate resolution.\n\n1.  **GRF Sampling:** To generate a mean-zero, stationary GRF with a squared-exponential covariance on a discrete grid, one must use a spectral synthesis method. This involves:\n    - Choosing a grid spacing $h$ fine enough to resolve the correlation length $\\ell$. A common rule of thumb is $h \\ll \\ell$, for instance $h \\le \\ell/c$ for some constant $c>2$. This ensures that the Nyquist frequency $k_{\\max} \\approx \\pi/h$ is large enough to capture the significant part of the spectral density $S(k)$, which decays on a scale of $1/\\ell$.\n    - Calculating the target spectral density $S(k)$ for discrete wavevectors $k$. For $d=2$, the formula is $S(k) = \\sigma^2 (2\\pi\\ell^2) \\exp(-\\ell^2 \\|k\\|^2/2)$.\n    - Drawing complex Gaussian random variables $\\widehat{g}(k)$ for each $k$ with variance proportional to $S(k)$.\n    - Enforcing Hermitian symmetry, $\\widehat{g}(-k) = \\overline{\\widehat{g}(k)}$, to ensure the resulting field is real-valued.\n    - Applying the inverse discrete Fourier transform.\n\n2.  **PDE Well-Posedness:** The PDE $-\\Delta u + a u = f$ with homogeneous Dirichlet boundary conditions is guaranteed to be well-posed by the Lax-Milgram theorem if the bilinear form $\\mathcal{B}(u,v) = \\int_{\\Omega} (\\nabla u \\cdot \\nabla v + a u v) dx$ is coercive. This holds if $a(x) \\ge 0$. A stronger condition, $a(x) \\ge a_0 > 0$, ensures uniform coercivity and well-conditioning. Since the sampling procedure generates a mean-zero GRF $g_a$, which can take negative values, a modification is necessary. The standard principled approach is to shift the mean: define $a(x) = a_0 + g_a(x)$. For $g_a \\sim \\mathcal{N}(0, \\sigma_a^2)$, choosing $a_0$ to be a few standard deviations (e.g., $a_0 > 3\\sigma_a$) makes the probability of $a(x)$ becoming negative extremely small, effectively ensuring well-posedness for the entire dataset with high probability.\n\n3.  **Numerical Solution:** A conforming Finite Element Method (FEM) is an excellent choice for this elliptic PDE. The mesh size $h$ must be fine enough to resolve all significant variations in the problem. The input fields $a(x)$ and $f(x)$ have characteristic length scales $\\ell_a$ and $\\ell_f$. The solution $u(x)$ may have even smaller scales. A necessary condition for accuracy is that the mesh resolves the inputs, so we must have $h < \\min(\\ell_a, \\ell_f)$. Using the same grid for sampling the GRFs and for the FEM solver is a consistent and common practice, provided the grid is sufficiently fine for both tasks.\n\nWe now evaluate each option against this principled framework.\n\n**A. Sample $a$ and $f$ as follows: choose $\\ell_a>0$, $\\sigma_a>0$, and construct a periodic discretization of $\\Omega$ with uniform grid spacing $h$ satisfying $h \\le \\min(\\ell_a,\\ell_f)/4$; define the discrete wavevectors $k$ on the corresponding Fourier grid. For $a$, set $S_a(k) = \\sigma_a^2 (2\\pi \\ell_a^2)^{2/2} \\exp(-\\ell_a^2 \\|k\\|^2/2)$, draw independent complex Gaussian coefficients $\\widehat{g}_a(k) \\sim \\mathcal{N}_{\\mathbb{C}}(0,S_a(k))$ for each $k$ with Hermitian symmetry $\\widehat{g}_a(-k)=\\overline{\\widehat{g}_a(k)}$, and obtain a real-valued mean-zero field $g_a$ by inverse discrete Fourier transform. Define $a(x)=a_0+g_a(x)$ with $a_0>3\\,\\sigma_a$ to ensure $a(x)\\ge a_0/2>0$ with high probability. Sample $f$ independently with the same procedure using $(\\ell_f,\\sigma_f)$ and spectral density $S_f(k)$. Solve $-\\Delta u + a u = f$ using a conforming finite element method on the same mesh (with integration of $a$ and $f$ by projection onto $V_h$), and verify solver tolerances by ensuring the residual in the discrete weak form is below a prescribed threshold. Store $(a,f,u)$ as a training triple. Repeat to generate independent samples.**\n\n- **Analysis:** This option precisely follows the principled strategy outlined above.\n    1.  **GRF Sampling:** It uses the correct spectral synthesis method, including the correct formula for the spectral density ($d=2$), the imposition of Hermitian symmetry, and a reasonable grid resolution criterion ($h \\le \\min(\\ell_a, \\ell_f)/4$).\n    2.  **Well-Posedness:** It uses the standard, statistically sound method of adding a positive offset $a_0 > 3\\sigma_a$ to ensure the coefficient $a(x)$ is positive with high probability.\n    3.  **Numerical Solution:** It employs a suitable method (conforming FEM) on a well-resolved mesh (the same one used for sampling, which is fine enough). It also includes a standard verification step (checking the residual).\n- **Verdict:** **Correct**.\n\n**B. On a coarse grid with spacing $h=\\ell_a$, draw $a(x_i)\\sim \\mathcal{N}(0,\\sigma_a^2)$ i.i.d. and then convolve the grid values with a compactly supported kernel of radius $\\ell_a$ to introduce correlation; draw $f(x_i)\\sim \\mathcal{N}(0,\\sigma_f^2)$ i.i.d.; solve the PDE with a first-order upwind finite difference scheme, and if $a(x_i)<0$, replace it by $|a(x_i)|$ pointwise. The convolution ensures the correlation length and the pointwise absolute value ensures positivity.**\n\n- **Analysis:** This option has multiple fundamental flaws.\n    1.  **GRF Sampling:** Drawing i.i.d. samples and convolving with a kernel is a valid way to generate *some* correlated random field, but it does not guarantee the target squared-exponential covariance structure unless the kernel is specifically chosen (it would have to be a Gaussian kernel). The method is specified with an arbitrary \"compactly supported kernel\". Furthermore, the grid spacing $h=\\ell_a$ is critically undersampled and would lead to severe aliasing.\n    2.  **Well-Posedness:** Replacing $a(x_i)$ with $|a(x_i)|$ is an ad-hoc procedure that distorts the statistics of the random field. The resulting field is no longer Gaussian. This is not a principled way to enforce positivity.\n    3.  **Numerical Solution:** A first-order upwind scheme is inappropriate for a purely elliptic problem; it is designed for advection-dominated problems and introduces excessive numerical diffusion.\n- **Verdict:** **Incorrect**.\n\n**C. Use eigenfunctions of the Laplace operator with Dirichlet boundary conditions $\\{-\\Delta e_m = \\lambda_m e_m\\}$ on $\\Omega$. Sample $a$ by $a(x)=\\sum_{m=1}^{M} \\xi_m e_m(x)$ with $\\xi_m\\sim\\mathcal{N}(0,1/m)$ independent, and declare the correlation length to be $\\ell_a=1/\\sqrt{M}$. Sample $f$ similarly with the same $M$ and solve the PDE by a spectral Galerkin method. This uses orthogonal bases, so the fields are Gaussian with the desired correlation length.**\n\n- **Analysis:** This option describes a Karhunen-Loève expansion, but misspecifies its properties.\n    1.  **GRF Sampling:** The covariance of the resulting field is $C(x,y) = \\sum_{m=1}^M \\text{Var}(\\xi_m) e_m(x) e_m(y)$. Because the Laplacian eigenfunctions depend on position within the domain, this covariance is *not* stationary (i.e., it depends on $x$ and $y$ individually, not just their difference $x-y$). The problem explicitly requires a *stationary* GRF. The statement `declare the correlation length to be \\ell_a=1/\\sqrt{M}` is arbitrary and has no mathematical basis.\n    2.  **Well-Posedness:** The generated field $a(x)$ has a mean of zero, and no procedure is described to ensure positivity. The PDE would be ill-posed for many samples.\n- **Verdict:** **Incorrect**.\n\n**D. Sample $a$ in the Fourier domain by drawing $\\widehat{a}(k)\\sim \\mathcal{N}_{\\mathbb{C}}(0,\\sigma_a^2)$ without imposing any symmetry, inverse transform to obtain a complex-valued $a$, and set the real part as the usable $a$. For $f$, use a deterministic oscillatory source $f(x)=\\sin(1000\\,\\pi x_1)$ to excite high-frequency responses. Solve the PDE on a grid with $h=\\ell_a/2$ using a mixed finite difference scheme and discard runs where the solver fails to converge. This captures a broad frequency content and uses randomization, satisfying the sampling demand.**\n\n- **Analysis:** This option is incorrect on nearly every point.\n    1.  **GRF Sampling:** Drawing Fourier coefficients with constant variance $\\sigma_a^2$ generates a white noise field (delta-correlated in space), not one with a finite correlation length $\\ell_a$ and a squared-exponential covariance. This fails to meet the statistical specification.\n    2.  **Source Field:** The problem specifies that $f$ must be sampled from a GRF. This option uses a single, deterministic function, which violates the problem statement.\n    3.  **Well-Posedness:** No principled method is used to ensure $a(x) > 0$. The strategy to `discard runs where the solver fails to converge` is an inefficient and unprincipled work-around, not a guarantee of well-posedness. Solver failure is a symptom of the ill-posedness that this strategy fails to prevent.\n- **Verdict:** **Incorrect**.\n\n### Conclusion\n\nOnly option A describes a scientifically and numerically sound procedure that correctly implements the generation of stationary Gaussian random fields, ensures the well-posedness of the elliptic PDE, and uses an appropriate, well-resolved numerical method.", "answer": "$$\\boxed{A}$$", "id": "3427015"}, {"introduction": "Evaluating a learned operator requires a careful distinction between the model's intrinsic error and the numerical error from the simulation tools used. This hands-on coding exercise challenges you to separate a model's generalization error from a solver's discretization error. By implementing a mesh refinement study, you will learn a standard verification technique and gain a deeper appreciation for the different sources of error in a computational workflow [@problem_id:3426971].", "problem": "Consider the boundary value problem for the one-dimensional Poisson equation on the unit interval with homogeneous Dirichlet boundary conditions. Let $U = L^{2}(0,1)$ and $V = H_{0}^{1}(0,1)$ be Hilbert spaces. Define the linear operator $T : U \\to V$ by the rule: for any forcing function $f \\in U$, the function $u = T(f)$ is the unique weak solution of the boundary value problem\n$$\n- u''(x) = f(x), \\quad x \\in (0,1), \\quad u(0) = 0, \\quad u(1) = 0.\n$$\nAssume $f$ admits a Fourier sine series $f(x) = \\sum_{n=1}^{\\infty} a_n \\sin(n \\pi x)$ with convergence in $L^2(0,1)$, and similarly $u$ admits a sine series $u(x) = \\sum_{n=1}^{\\infty} b_n \\sin(n \\pi x)$.\n\nAn operator-learning surrogate $\\hat{T}$, inspired by the Fourier Neural Operator (FNO), is defined by truncating the Fourier sine representation of the output to the lowest $k$ modes. More precisely, $\\hat{T}$ acts by keeping only the first $k$ sine modes of $u$, with coefficients identical to those of $u$ for $n \\le k$ and zero for $n > k$. This can be understood as an operator on $U$ that maps $f$ to $\\hat{u} = \\hat{T}(f)$ given by the truncated sine series.\n\nThe generalization error in operator norm is defined as\n$$\nE_{\\mathrm{gen}} = \\| T - \\hat{T} \\|_{\\mathcal{L}(U,V)} = \\sup_{f \\in U, \\, \\| f \\|_{U} = 1} \\| T(f) - \\hat{T}(f) \\|_{V}.\n$$\nIn practice, one estimates $E_{\\mathrm{gen}}$ relative to the $L^2(0,1)$ norm of the output, using a representative family of inputs.\n\nSeparately, the discretization error arises when approximating the continuous operator $T$ by a discrete numerical solver on a mesh. Consider a uniform grid with $N$ nodes, mesh spacing $h = 1/(N-1)$, and the standard second-order centered finite difference scheme for $-u''(x)$ at interior points. Let $T_{N}$ denote the discrete operator mapping a grid function $f$ (obtained by sampling $f$ at grid points) to the grid function $u_{N}$ (obtained by solving the finite difference linear system with $u(0)=u(1)=0$). Define the discretization error for a given mesh size $N$ by\n$$\nE_{\\mathrm{disc}}(N) = \\sup_{f \\in \\mathcal{F}} \\| T(f) - T_{N}(f) \\|_{L^{2}(0,1)},\n$$\nwhere the norm is evaluated by numerical quadrature on the grid, and $\\mathcal{F}$ is a specified test family in $U$.\n\nYour tasks are:\n- Derive from first principles how $T$ acts on Fourier sine coefficients $a_n$ of $f$, and explain the definition of $E_{\\mathrm{gen}}$ in operator norm relative to $L^2(0,1)$.\n- Distinguish conceptually and quantitatively between $E_{\\mathrm{gen}}$ and $E_{\\mathrm{disc}}(N)$.\n- Propose and implement a mesh refinement experiment to separate the contribution of $E_{\\mathrm{gen}}$ from $E_{\\mathrm{disc}}(N)$ by comparing measurements on a coarse mesh and a fine mesh.\n\nExperimental setup:\n- Use the exact continuous operator $T$ determined by the one-dimensional Poisson equation with homogeneous Dirichlet conditions.\n- Define the surrogate operator $\\hat{T}$ that truncates output sine modes beyond the cutoff $k$.\n- Evaluate $\\| \\cdot \\|_{L^2(0,1)}$ by the composite trapezoidal rule on the chosen grid.\n- Construct a finite difference solver for $T_N$ with the standard tridiagonal matrix for the second derivative approximation and boundary conditions $u(0)=u(1)=0$.\n\nTest suite:\nFor each parameter set, define the test family $\\mathcal{F}$ to consist of three inputs, each being a single normalized sine mode $f(x) = \\sqrt{2} \\sin(n \\pi x)$, which has $L^2(0,1)$ norm equal to $1$. For each parameter set, measure the following four quantities:\n- $E_{\\mathrm{gen}}(N_{\\mathrm{low}})$: the maximum over $\\mathcal{F}$ of $\\| T(f) - \\hat{T}(f) \\|_{L^2(0,1)}$ computed on the coarse mesh with $N_{\\mathrm{low}}$ nodes.\n- $E_{\\mathrm{gen}}(N_{\\mathrm{high}})$: the same maximum computed on the fine mesh with $N_{\\mathrm{high}}$ nodes.\n- $E_{\\mathrm{disc}}(N_{\\mathrm{low}})$: the maximum over $\\mathcal{F}$ of $\\| T(f) - T_{N_{\\mathrm{low}}}(f) \\|_{L^2(0,1)}$ on the coarse mesh.\n- $E_{\\mathrm{disc}}(N_{\\mathrm{high}})$: the same maximum on the fine mesh.\n\nThen, for each parameter set, decide whether the mesh refinement separates the two error sources by checking both of the following conditions:\n- $|E_{\\mathrm{gen}}(N_{\\mathrm{low}}) - E_{\\mathrm{gen}}(N_{\\mathrm{high}})| \\le \\tau_{\\mathrm{gen}}$, with $\\tau_{\\mathrm{gen}}$ a small tolerance.\n- $E_{\\mathrm{disc}}(N_{\\mathrm{low}}) > \\gamma \\, E_{\\mathrm{disc}}(N_{\\mathrm{high}})$, with $\\gamma > 1$ indicating a noticeable decrease in discretization error with refinement.\n\nUse the following three parameter sets as the test suite, each provided as a tuple $(M, k, N_{\\mathrm{low}}, N_{\\mathrm{high}}, \\text{frequencies})$:\n- $\\left(64, 5, 33, 257, [6, 64, 2]\\right)$\n- $\\left(64, 64, 33, 257, [10, 20, 40]\\right)$\n- $\\left(64, 10, 17, 33, [11, 64, 5]\\right)$\nHere, $M$ is the maximum number of sine modes used to evaluate the continuous series for $T$, $k$ is the surrogate cutoff for $\\hat{T}$, $N_{\\mathrm{low}}$ and $N_{\\mathrm{high}}$ are the coarse and fine grid sizes (number of nodes), and the list \"frequencies\" specifies the three single-mode inputs in $\\mathcal{F}$.\n\nFinal output specification:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element should be a list of five values corresponding to a parameter set in the order\n$$\n\\left[ E_{\\mathrm{gen}}(N_{\\mathrm{low}}), \\, E_{\\mathrm{gen}}(N_{\\mathrm{high}}), \\, E_{\\mathrm{disc}}(N_{\\mathrm{low}}), \\, E_{\\mathrm{disc}}(N_{\\mathrm{high}}), \\, \\text{separation\\_flag} \\right],\n$$\nwhere the first four are floating-point numbers and the last is a boolean indicating whether both separation conditions hold. The program must not read any input, and it must use the provided test suite exactly as specified. There are no physical units in this task, and angles, if any appear, must be considered in radians. The tolerance should be fixed to $\\tau_{\\mathrm{gen}} = 10^{-3}$ and the discretization improvement factor should be $\\gamma = 1.1$.", "solution": "The problem is assessed to be valid as it is scientifically grounded in the fields of numerical analysis and operator theory, is well-posed, objective, and contains all necessary information for a unique solution.\n\nHerein, a complete, reasoned solution is provided. Throughout this text, every mathematical entity is rendered in LaTeX as required.\n\n### 1. The Continuous Operator $T$ and the Surrogate Operator $\\hat{T}$\n\nThe problem considers the one-dimensional Poisson equation with homogeneous Dirichlet boundary conditions:\n$$\n-u''(x) = f(x), \\quad x \\in (0,1), \\quad u(0) = 0, \\quad u(1) = 0\n$$\nThe linear operator $T$ maps a forcing function $f \\in L^2(0,1)$ to the unique weak solution $u \\in H_0^1(0,1)$. To understand the action of $T$ in the Fourier domain, we represent both $f(x)$ and $u(x)$ by their Fourier sine series, which form a complete orthogonal basis for functions on $(0,1)$ satisfying the boundary conditions:\n$$\nf(x) = \\sum_{n=1}^{\\infty} a_n \\sin(n \\pi x)\n$$\n$$\nu(x) = \\sum_{n=1}^{\\infty} b_n \\sin(n \\pi x)\n$$\nAssuming sufficient regularity to differentiate term-by-term, the second derivative of $u(x)$ is:\n$$\nu''(x) = \\sum_{n=1}^{\\infty} b_n \\frac{d^2}{dx^2} \\sin(n \\pi x) = \\sum_{n=1}^{\\infty} b_n (-(n \\pi)^2) \\sin(n \\pi x)\n$$\nSubstituting these series into the Poisson equation yields:\n$$\n- \\left( \\sum_{n=1}^{\\infty} -b_n (n \\pi)^2 \\sin(n \\pi x) \\right) = \\sum_{n=1}^{\\infty} a_n \\sin(n \\pi x)\n$$\n$$\n\\sum_{n=1}^{\\infty} b_n (n \\pi)^2 \\sin(n \\pi x) = \\sum_{n=1}^{\\infty} a_n \\sin(n \\pi x)\n$$\nBy the uniqueness of Fourier series coefficients, we can equate the coefficients for each mode $n$:\n$$\nb_n (n \\pi)^2 = a_n \\implies b_n = \\frac{a_n}{(n \\pi)^2}\n$$\nThis fundamental relationship shows that the operator $T$ acts as a diagonal operator in the Fourier basis. It transforms the $n$-th Fourier coefficient of the input $f$, $a_n$, into the $n$-th coefficient of the output $u$, $b_n$, by scaling it by a factor of $1/(n \\pi)^2$. This scaling rapidly dampens higher-frequency components.\n\nThe surrogate operator $\\hat{T}$ is inspired by the Fourier Neural Operator (FNO) and is defined by truncating the output's Fourier series at a cutoff mode $k$. For a solution $u = T(f)$ with coefficients $b_n$, the surrogate solution $\\hat{u} = \\hat{T}(f)$ is:\n$$\n\\hat{u}(x) = \\sum_{n=1}^{k} b_n \\sin(n \\pi x)\n$$\nThe coefficients of $\\hat{u}$ are identical to those of $u$ for modes $n \\le k$ and are zero for $n > k$.\n\n### 2. Generalization vs. Discretization Error\n\nThe problem requires a conceptual and quantitative distinction between the generalization error $E_{\\mathrm{gen}}$ and the discretization error $E_{\\mathrm{disc}}(N)$.\n\n**Generalization Error ($E_{\\mathrm{gen}}$):** This is a **modeling error**, inherent to the surrogate model $\\hat{T}$. It quantifies how well the simplified model $\\hat{T}$ approximates the true continuous operator $T$. The error is the difference between the true solution $u$ and the surrogate solution $\\hat{u}$:\n$$\nT(f) - \\hat{T}(f) = u(x) - \\hat{u}(x) = \\sum_{n=k+1}^{\\infty} b_n \\sin(n \\pi x) = \\sum_{n=k+1}^{\\infty} \\frac{a_n}{(n \\pi)^2} \\sin(n \\pi x)\n$$\nThis error arises from the information lost by discarding all solution modes with frequency index $n > k$. Crucially, $E_{\\mathrm{gen}}$ is a property of the continuous operators $T$ and $\\hat{T}$ and does not depend on any numerical discretization or grid size $N$. For a fixed test family $\\mathcal{F}$, its value is constant. The numerical calculation of this error on a grid is an approximation, but this approximation should converge to the true constant value as the grid is refined.\n\n**Discretization Error ($E_{\\mathrm{disc}}(N)$):** This is a **numerical approximation error**, inherent to the discrete solver $T_N$. It quantifies how well the numerical solution $u_N$ on a grid with $N$ nodes approximates the true continuous solution $u = T(f)$. This error arises from replacing the continuous derivative operator $-d^2/dx^2$ with a finite difference approximation. For the second-order centered difference scheme specified, the local truncation error is of order $O(h^2)$, where $h=1/(N-1)$ is the mesh spacing. Consequently, the global error in the solution, $\\|T(f) - T_N(f)\\|_{L^2(0,1)}$, is also expected to decrease as $N$ increases, typically with an order of convergence related to the scheme's accuracy. We expect $E_{\\mathrm{disc}}(N) \\to 0$ as $N \\to \\infty$ (or $h \\to 0$).\n\nIn summary, $E_{\\mathrm{gen}}$ measures the deficiency of the **model**, while $E_{\\mathrm{disc}}(N)$ measures the deficiency of the **numerical method**.\n\n### 3. The Mesh Refinement Experiment\n\nThe proposed experiment is designed to empirically separate these two distinct sources of error. It relies on their different dependencies on the mesh size $N$.\n\n1.  **Measuring $E_{\\mathrm{gen}}$:** The generalization error $\\|T(f) - \\hat{T}(f)\\|_{L^2(0,1)}$ is an intrinsic property of the operators, independent of the mesh. When we compute it numerically on a coarse mesh ($N_{\\mathrm{low}}$) and a fine mesh ($N_{\\mathrm{high}}$), we are introducing a measurement error due to the trapezoidal rule approximation of the integral. However, if both meshes are sufficiently fine to resolve the functions involved, the numerical results should be very close to each other and to the true analytical value. The condition $|E_{\\mathrm{gen}}(N_{\\mathrm{low}}) - E_{\\mathrm{gen}}(N_{\\mathrm{high}})| \\le \\tau_{\\mathrm{gen}}$ checks if the measurement of $E_{\\mathrm{gen}}$ has stabilized, indicating that the discretization aspect of the measurement is negligible compared to the magnitude of $E_{\\mathrm{gen}}$ itself.\n\n2.  **Measuring $E_{\\mathrm{disc}}(N)$:** The discretization error $\\|T(f) - T_N(f)\\|_{L^2(0,1)}$ is, by definition, dependent on the mesh size $N$. As the mesh is refined from $N_{\\mathrm{low}}$ to $N_{\\mathrm{high}}$, the error of the second-order finite difference scheme should decrease. The ratio of errors is expected to be approximately $(h_{\\mathrm{low}}/h_{\\mathrm{high}})^2 = ((N_{\\mathrm{high}}-1)/(N_{\\mathrm{low}}-1))^2$. The condition $E_{\\mathrm{disc}}(N_{\\mathrm{low}}) > \\gamma \\, E_{\\mathrm{disc}}(N_{\\mathrm{high}})$ for a factor $\\gamma > 1$ confirms this expected reduction in error upon mesh refinement.\n\nBy observing that one error measure remains nearly constant while the other systematically decreases with mesh refinement, the experiment effectively distinguishes and separates the modeling error from the numerical solver error.\n\nIt is noted that the parameter $M$ defines the fidelity of our \"ground truth\" operator $T$ by truncating its series representation at $M$ modes. For the specific test inputs $f(x) = \\sqrt{2} \\sin(n \\pi x)$, the exact solution $u(x)$ is also a single sine mode. Since all test frequencies $n$ are less than or equal to the given values of $M$, this truncation has no effect, and the analytical solution can be used directly.", "answer": "```python\nimport numpy as np\nfrom scipy.linalg import solve_banded\n\ndef solve():\n    \"\"\"\n    Main function to run the mesh refinement experiment for the given test suite.\n    \"\"\"\n    test_cases = [\n        # (M, k, N_low, N_high, frequencies)\n        (64, 5, 33, 257, [6, 64, 2]),\n        (64, 64, 33, 257, [10, 20, 40]),\n        (64, 10, 17, 33, [11, 64, 5]),\n    ]\n    tau_gen = 1e-3\n    gamma = 1.1\n\n    final_results = []\n\n    for M, k, N_low, N_high, freqs in test_cases:\n        \n        errors_gen_low, errors_disc_low = [], []\n        errors_gen_high, errors_disc_high = [], []\n\n        for n in freqs:\n            # --- Coarse Mesh Calculations ---\n            egen_low, edisc_low = calculate_errors(n, k, M, N_low)\n            errors_gen_low.append(egen_low)\n            errors_disc_low.append(edisc_low)\n            \n            # --- Fine Mesh Calculations ---\n            egen_high, edisc_high = calculate_errors(n, k, M, N_high)\n            errors_gen_high.append(egen_high)\n            errors_disc_high.append(edisc_high)\n\n        # Find the maximum error over the family of test functions\n        E_gen_low = np.max(errors_gen_low)\n        E_gen_high = np.max(errors_gen_high)\n        E_disc_low = np.max(errors_disc_low)\n        E_disc_high = np.max(errors_disc_high)\n\n        # Check separation conditions\n        cond1 = np.abs(E_gen_low - E_gen_high) = tau_gen\n        cond2 = E_disc_low > gamma * E_disc_high if E_disc_high > 0 else E_disc_low > 0\n        \n        separation_flag = cond1 and cond2\n\n        final_results.append(\n            f\"[{E_gen_low:.6f},{E_gen_high:.6f},{E_disc_low:.6f},{E_disc_high:.6f},{separation_flag}]\"\n        )\n    \n    print(f\"[{','.join(final_results)}]\")\n\ndef calculate_errors(n, k, M, N):\n    \"\"\"\n    Calculates generalization and discretization errors for a given frequency n,\n    surrogate cutoff k, and grid size N.\n    \n    Args:\n        n (int): The frequency mode of the input function.\n        k (int): The cutoff mode for the surrogate operator.\n        M (int): The cutoff for the 'true' continuous operator.\n        N (int): The number of nodes in the grid.\n\n    Returns:\n        tuple: A tuple containing (E_gen, E_disc).\n    \"\"\"\n    # 1. Define grid and analytical solutions\n    x_grid = np.linspace(0.0, 1.0, N)\n    h = 1.0 / (N - 1)\n\n    # Input function f(x) = sqrt(2) * sin(n*pi*x)\n    f_input = lambda x: np.sqrt(2.0) * np.sin(n * np.pi * x)\n\n    # True solution u(x) = T(f)\n    # The coefficient is sqrt(2) / (n*pi)^2.\n    # The problem specifies T is evaluated with M modes.\n    # For a single mode input n, the output is non-zero only if n = M.\n    if n = M:\n        u_true = lambda x: np.sqrt(2.0) / (n * np.pi)**2 * np.sin(n * np.pi * x)\n    else:\n        u_true = lambda x: np.zeros_like(x)\n\n    # Surrogate solution u_hat(x) = hat{T}(f)\n    # This is the true solution truncated at mode k.\n    if n = k:\n        u_surr = u_true # The n-th mode is kept\n    else:\n        u_surr = lambda x: np.zeros_like(x) # The n-th mode is truncated\n    \n    # Evaluate analytical solutions on the grid\n    u_true_grid = u_true(x_grid)\n    u_surr_grid = u_surr(x_grid)\n\n    # 2. Calculate Generalization Error (E_gen)\n    # E_gen = || T(f) - hat{T}(f) ||_L2 = || u_true - u_surr ||_L2\n    # Norm is computed using composite trapezoidal rule.\n    err_vec_gen = u_true_grid - u_surr_grid\n    norm_sq_gen = np.trapz(err_vec_gen**2, x_grid)\n    E_gen = np.sqrt(norm_sq_gen)\n\n    # 3. Calculate Discretization Error (E_disc)\n    # First, find the numerical solution u_N = T_N(f)\n    \n    # Set up the finite difference system Au = f_interior\n    num_interior_points = N - 2\n    if num_interior_points > 0:\n        # Forcing function on interior grid points\n        f_interior = f_input(x_grid[1:-1])\n\n        # Tridiagonal matrix A for -u'' is (1/h^2) * diag(-1, 2, -1)\n        # For scipy.linalg.solve_banded, we represent A in banded format.\n        # It has 1 lower and 1 upper diagonal.\n        ab = np.zeros((3, num_interior_points))\n        ab[0, 1:] = -1.0  # Upper diagonal\n        ab[1, :] = 2.0   # Main diagonal\n        ab[2, :-1] = -1.0 # Lower diagonal\n        \n        # Solve the linear system A * u_interior = f_interior\n        u_interior = solve_banded((1, 1), ab / h**2, f_interior)\n        \n        # Add boundary conditions u(0)=0, u(1)=0\n        u_N_grid = np.pad(u_interior, 1, 'constant')\n    else: # Handles N=1 or N=2 cases\n        u_N_grid = np.zeros(N)\n\n    # E_disc = || T(f) - T_N(f) ||_L2 = || u_true - u_N ||_L2\n    err_vec_disc = u_true_grid - u_N_grid\n    norm_sq_disc = np.trapz(err_vec_disc**2, x_grid)\n    E_disc = np.sqrt(norm_sq_disc)\n\n    return E_gen, E_disc\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3426971"}, {"introduction": "A key challenge for any learned model is its ability to extrapolate, or make accurate predictions for inputs outside its training distribution. This practice explores this critical issue by testing a simple Fourier Neural Operator on input frequencies it has never seen before [@problem_id:3427041]. You will implement and contrast a baseline data-driven model with a regularized version that incorporates physical knowledge, revealing a common failure mode of neural operators and a powerful strategy for improving their robustness.", "problem": "Consider the one-dimensional periodic Poisson problem on the unit interval where the unknown field $u(x)$ satisfies the second-order ordinary differential equation $-u''(x) = f_{\\mu}(x)$ with periodic boundary conditions and zero mean. In this setting, the parameter $\\mu$ selects a forcing pattern $f_{\\mu}(x)$, and the solution operator maps the input field $f_{\\mu}(x)$ to the output field $u(x)$. The goal is to investigate operator extrapolation for a Fourier Neural Operator (FNO), defined as an operator that acts in the frequency domain by scaling Fourier modes of the input field before transforming back to the spatial domain.\n\nYou must implement the following pipeline entirely in code:\n\n1. Construct a periodic grid of size $N$ points over the domain $[0,1)$, with $N$ finite and moderate, using an equispaced discretization. Define a training parameter range $[\\mu_{\\min}, \\mu_{\\max}]$ consisting of integer values to be used for training. The forcing $f_{\\mu}(x)$ must be a single-frequency real signal consistent with periodicity at integer frequencies.\n\n2. Compute ground-truth solutions $u(x)$ for a given input $f_{\\mu}(x)$ using a numerically sound spectral method based only on foundational principles of Fourier series for periodic problems. The spectral method must be implemented explicitly in your code.\n\n3. Design a simplified Fourier Neural Operator (FNO) model with one layer that multiplies each Fourier mode $\\hat{f}(k)$ of the input by a learned complex scalar weight $w(k)$ that depends only on the integer wavenumber magnitude $|k|$, and then applies an inverse Fourier transform to produce the output prediction $\\hat{u}(k) = w(k)\\hat{f}(k)$. Restrict the FNO to operate diagonally in the frequency domain (mode-by-mode scaling), without cross-mode mixing.\n\n4. Train two variants of the FNO:\n   - Baseline FNO: Estimate the discrete weights $w(k)$ only for the training wavenumbers $k \\in \\{\\mu_{\\min},\\mu_{\\min}+1,\\dots,\\mu_{\\max}\\}$ by data-driven fitting from the training set. Leave $w(k)$ as zero for all other wavenumbers not seen in training.\n   - Extrapolation-aware regularized FNO: Impose a frequency-dependent decay structure on the learned weights by fitting a single scalar parameter in a physically motivated family for $w(k)$ over the training set, and then extend this structured weight to all frequencies $k$ (including those not in the training range). The structured form must encode monotone decay with increasing $|k|$ and be chosen to be consistent with the governing differential operator’s order. The regularization must be implemented in a way that is verifiable by code and does not require any external data or precomputed constants.\n\n5. Characterize extrapolation failure modes when testing on parameter values $\\mu$ outside the training range by computing at least the following quantitative diagnostics for each test:\n   - The relative $\\ell^2$ error between the predicted field and the ground-truth field, computed as the ratio of the $\\ell^2$ norm of the prediction error to the $\\ell^2$ norm of the ground truth.\n   - A boolean indicator of whether the error’s spectral energy is predominantly concentrated in wavenumbers above the largest training wavenumber (e.g., more than a fixed fraction of the error energy lies at $|k|\\mu_{\\max}$).\n\n6. Provide a compact test suite that exercises the following cases:\n   - A parameter $\\mu$ strictly below $\\mu_{\\min}$ (outside-range low).\n   - The boundary parameters $\\mu=\\mu_{\\min}$ and $\\mu=\\mu_{\\max}$ (on-range boundaries).\n   - A parameter $\\mu$ strictly above $\\mu_{\\max}$ (outside-range high).\n\nUse the following fixed numerical choices in your program:\n- Grid size $N=128$.\n- Training range $[\\mu_{\\min}, \\mu_{\\max}] = [2,6]$ with integer training parameters $\\mu \\in \\{2,3,4,5,6\\}$.\n- Forcing definition $f_{\\mu}(x)$ must be $f_{\\mu}(x) = \\sin(2\\pi \\mu x)$ to ensure compatibility with periodicity on the unit interval.\n- Test suite parameters $\\mu \\in \\{1,2,6,9\\}$.\n- Define the spectral failure mode boolean threshold as a fraction of the error energy strictly greater than $0.6$ supported by wavenumbers $|k|\\mu_{\\max}$.\n\nYour program must produce, for each test parameter, a list of four entries containing:\n- The baseline FNO relative $\\ell^2$ error (a float).\n- The regularized FNO relative $\\ell^2$ error (a float).\n- A boolean indicating whether the regularized error is strictly lower than the baseline error.\n- A boolean indicating whether the baseline prediction exhibits the high-frequency error failure mode according to the threshold defined above.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element of this list must itself be a list of the four values corresponding to the test parameters in the order $\\mu \\in \\{1,2,6,9\\}$. For example: \"[$[r_{1}^{\\mathrm{base}},r_{1}^{\\mathrm{reg}},b_{1}^{\\mathrm{improve}},b_{1}^{\\mathrm{fail}}],[r_{2}^{\\mathrm{base}},r_{2}^{\\mathrm{reg}},b_{2}^{\\mathrm{improve}},b_{2}^{\\mathrm{fail}}],\\dots$]\". No physical units are involved; all quantities are nondimensional. Angles, if any, are in radians by construction. The final outputs must be fundamental types (floats and booleans) only, as specified above.", "solution": "The problem is assessed to be valid. It is scientifically sound, well-posed, objective, and contains all necessary information to construct a unique, verifiable solution. We proceed with the solution.\n\nThe problem requires an investigation into the extrapolation capabilities of a simplified Fourier Neural Operator (FNO) on the one-dimensional periodic Poisson problem. The core task involves comparing a baseline FNO, trained only on a specific range of input frequencies, with a regularized FNO that incorporates a physically motivated structure.\n\n**1. The Governing Equation and its Spectral Solution**\n\nThe problem is defined by the second-order ordinary differential equation:\n$$\n-u''(x) = f_{\\mu}(x)\n$$\non the domain $x \\in [0, 1)$, with periodic boundary conditions $u(0)=u(1)$ and $u'(0)=u'(1)$. An additional constraint of zero mean, $\\int_0^1 u(x) dx = 0$, is imposed to ensure a unique solution. The forcing term is a single-frequency sinusoid, $f_{\\mu}(x) = \\sin(2\\pi \\mu x)$, where $\\mu$ is an integer.\n\nFor periodic problems, the Fourier series provides a natural basis. A function $g(x)$ can be represented as a sum of complex exponentials:\n$$\ng(x) = \\sum_{k=-\\infty}^{\\infty} \\hat{g}(k) e^{2\\pi i k x}\n$$\nwhere $\\hat{g}(k)$ are the Fourier coefficients and $k$ are the integer wavenumbers. A key property of the Fourier transform is that differentiation in the spatial domain becomes multiplication in the frequency domain. Specifically, the second derivative transforms as:\n$$\n\\mathcal{F}\\{g''(x)\\} = (2\\pi i k)^2 \\hat{g}(k) = -(2\\pi k)^2 \\hat{g}(k)\n$$\nApplying the Fourier transform to the Poisson equation term by term, we convert the differential equation into an algebraic one:\n$$\n- (-(2\\pi k)^2 \\hat{u}(k)) = \\hat{f}_{\\mu}(k) \\implies (2\\pi k)^2 \\hat{u}(k) = \\hat{f}_{\\mu}(k)\n$$\nThis allows us to solve for the Fourier coefficients of the solution $u(x)$ directly:\n$$\n\\hat{u}(k) = \\frac{1}{(2\\pi k)^2} \\hat{f}_{\\mu}(k) \\quad \\text{for } k \\neq 0\n$$\nFor the $k=0$ (zero-frequency) mode, which represents the mean value, the condition $\\int_0^1 f_{\\mu}(x) dx = 0$ for integer $\\mu$ ensures $\\hat{f}_{\\mu}(0)=0$. The problem's constraint that $u(x)$ has a zero mean implies $\\hat{u}(0)=0$. Thus, the solution is well-defined.\n\nThe analytical solution operator, which maps $f$ to $u$, is therefore a diagonal multiplication in the Fourier domain by a Green's function (or filter) $G(k)$:\n$$\n\\hat{u}(k) = G(k) \\hat{f}_{\\mu}(k) \\quad \\text{where} \\quad G(k) = \\begin{cases} \\frac{1}{(2\\pi k)^2}  k \\neq 0 \\\\ 0  k=0 \\end{cases}\n$$\nThis analytical result forms the basis for our ground-truth numerical solver.\n\n**2. Numerical Discretization and Ground-Truth Solver**\n\nWe discretize the domain $[0,1)$ into $N=128$ equispaced points $x_j = j/N$ for $j=0, 1, \\dots, N-1$. The continuous Fourier transform is replaced by the Discrete Fourier Transform (DFT), efficiently computed via the Fast Fourier Transform (FFT) algorithm. The discrete wavenumbers $k$ are provided by standard FFT frequency-generation routines.\n\nThe ground-truth solver numerically implements the analytical spectral solution:\n1.  Compute the DFT of the input grid function $f_{\\mu}(x_j)$ to get $\\hat{f}_{\\mu}(k)$.\n2.  Multiply each coefficient $\\hat{f}_{\\mu}(k)$ by the corresponding discrete filter weight $G(k)$.\n3.  Compute the inverse DFT of the resulting coefficients $\\hat{u}(k)$ to obtain the solution $u(x_j)$ on the grid.\n\n**3. Fourier Neural Operator Models and Training**\n\nThe simplified FNO model is defined by the operation $\\hat{u}_{pred}(k) = w(k)\\hat{f}(k)$, where $w(k)$ are learned complex weights dependent on the wavenumber magnitude $|k|$.\n\n**Baseline FNO:** This model is trained using data generated from forcing terms with integer frequencies $\\mu \\in \\{2, 3, 4, 5, 6\\}$. The input $f_{\\mu}(x) = \\sin(2\\pi \\mu x)$ has spectral energy concentrated exclusively at wavenumbers $k=\\pm\\mu$. Consequently, the \"training\" process can only determine the weights for $|k| \\in \\{2, 3, 4, 5, 6\\}$. Since the true operator is known, we can directly set the weights to their ideal values: $w_{base}(k) = G(k)$ for $|k| \\in \\{2, \\dots, 6\\}$. For all other wavenumbers $|k|$ not seen during training, the weights are set to zero, i.e., $w_{base}(k)=0$. This model effectively memorizes the operator's behavior on the training frequencies and assumes a null response everywhere else.\n\n**Regularized FNO:** This model imposes a physically motivated structure on the weights. The analytical form of the Green's function $G(k) \\propto k^{-2}$ is motivated by the fact that the operator is the inverse of a second-order differential operator. We choose the functional family $w_{reg}(k) = C/k^2$ for $k \\ne 0$, where $C$ is a single scalar parameter to be fitted. To \"fit\" $C$, we perform a least-squares regression against the ideal weights $G(k)$ for the training wavenumbers $|k| \\in \\{2, \\dots, 6\\}$. Due to the perfect match between our model family and the true operator, this fitting process analytically yields $C = 1/(4\\pi^2)$. The resulting regularized weights are $w_{reg}(k) = G(k)$ for all $k \\ne 0$. By incorporating the correct physical scaling, this FNO learns the true operator for all frequencies, not just those in the training set.\n\n**4. Evaluation on Test Cases**\n\nThe two models are evaluated on a test suite with $\\mu \\in \\{1, 2, 6, 9\\}$.\n\n*   **In-Distribution ($\\mu=2, \\mu=6$):** The input frequencies are within the training set. Both the baseline and regularized FNOs have the correct weights for these frequencies. Both models will produce predictions identical to the ground truth, resulting in near-zero error.\n\n*   **Extrapolation (Low Frequency, $\\mu=1$):** The input frequency $\\mu=1$ is outside the training range $[\\mu_{min}, \\mu_{max}]=[2,6]$. The baseline FNO has $w_{base}(k)=0$ for $|k|=1$, so its prediction is $u_{base}(x)=0$. The resulting relative error is $1.0$. The regularized FNO, having learned the general $k^{-2}$ scaling law, has the correct weight and produces a near-perfect prediction with close to zero error. The error for the baseline model is concentrated at $|k|=1$, which is not greater than $\\mu_{max}=6$, so it does not trigger the high-frequency failure mode.\n\n*   **Extrapolation (High Frequency, $\\mu=9$):** The input frequency $\\mu=9$ is also outside the training range. The baseline FNO again predicts $u_{base}(x)=0$, yielding a relative error of $1.0$. The regularized FNO again succeeds. In this case, the baseline model's error is entirely concentrated at $|k|=9$. Since $9  \\mu_{max}=6$, this energy lies in the high-frequency band, and the spectral failure diagnostic will be triggered, correctly identifying the model's inability to generalize to unseen higher frequencies.\n\nThis experimental design clearly illustrates a primary failure mode of naive data-driven models—poor extrapolation—and demonstrates how imposing physics-informed regularization can lead to robust generalization.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements the full pipeline for training and evaluating FNOs on a 1D Poisson problem.\n    \"\"\"\n    # Fixed numerical choices from the problem statement\n    N = 128\n    mu_train_min = 2\n    mu_train_max = 6\n    mu_train = list(range(mu_train_min, mu_train_max + 1))\n    mu_test = [1, 2, 6, 9]\n    failure_threshold = 0.6\n\n    # Step 1: Construct a periodic grid and corresponding wavenumbers\n    x = np.arange(N) / N\n    k = np.fft.fftfreq(N, d=1.0/N)\n\n    # Step 2: Implement the ground-truth spectral solver\n    def solve_poisson_spectral(f_in, k_in):\n        \"\"\"Computes the solution to -u''=f using a spectral method.\"\"\"\n        f_hat = np.fft.fft(f_in)\n        \n        # The solution operator in Fourier space is multiplication by G(k) = 1/((2*pi*k)^2) for k!=0.\n        u_hat = np.zeros_like(f_hat)\n        nonzero_k_mask = k_in != 0\n        \n        k_nonzero = k_in[nonzero_k_mask]\n        # The analytical solution for the Fourier coefficients u_hat\n        u_hat[nonzero_k_mask] = (1.0 / (2 * np.pi * k_nonzero)**2) * f_hat[nonzero_k_mask]\n        \n        # Transform back to the spatial domain\n        u_out = np.fft.ifft(u_hat)\n        return u_out.real\n\n    # Step 3: Design and \"train\" the FNO models\n    # Baseline FNO: learns weights only for training wavenumbers\n    w_base = np.zeros(N, dtype=np.complex128)\n    for mu in mu_train:\n        # The ideal weight is from the Green's function\n        weight_val = 1.0 / (2 * np.pi * mu)**2\n        # Set weights for positive and negative wavenumbers (+mu, -mu)\n        w_base[mu] = weight_val\n        w_base[N - mu] = weight_val\n\n    # Regularized FNO: learns a structured physical model for the weights.\n    # The physically motivated family is w(k) = C/k^2. Fitting C to the training\n    # data analytically yields C = 1/(4*pi^2). Thus, the regularized model\n    # learns the exact analytical Green's function for all k.\n    w_reg = np.zeros(N, dtype=np.complex128)\n    nonzero_k_mask = k != 0\n    k_nonzero = k[nonzero_k_mask]\n    w_reg[nonzero_k_mask] = 1.0 / (2 * np.pi * k_nonzero)**2\n\n    # Step 4  5: Test models on the test suite and compute diagnostics\n    results = []\n    for mu in mu_test:\n        # Generate the forcing function for the current test case\n        f = np.sin(2 * np.pi * mu * x)\n        \n        # Compute the ground-truth solution\n        u_true = solve_poisson_spectral(f, k)\n        u_hat_true = np.fft.fft(u_true)\n\n        # Get predictions from both FNO models\n        f_hat = np.fft.fft(f)\n        \n        # Baseline FNO prediction\n        u_hat_base = w_base * f_hat\n        u_base = np.fft.ifft(u_hat_base).real\n        \n        # Regularized FNO prediction\n        u_hat_reg = w_reg * f_hat\n        u_reg = np.fft.ifft(u_hat_reg).real\n        \n        # Compute quantitative diagnostics\n        norm_u_true = np.linalg.norm(u_true)\n        \n        # Relative l2 error for both models\n        # A small epsilon is added to the denominator to prevent division by zero\n        # if the true solution norm is zero (not the case here, but good practice).\n        rel_err_base = np.linalg.norm(u_base - u_true) / (norm_u_true + 1e-12)\n        rel_err_reg = np.linalg.norm(u_reg - u_true) / (norm_u_true + 1e-12)\n        \n        # Boolean indicator of whether regularized error is strictly lower\n        is_improved = rel_err_reg  rel_err_base\n        \n        # Boolean indicator of the high-frequency error failure mode for the baseline FNO\n        e_hat_base = u_hat_base - u_hat_true\n        e_hat_base_energy = np.abs(e_hat_base)**2\n        total_energy = np.sum(e_hat_base_energy)\n        \n        if total_energy  1e-24:  # If error is numerically zero, no failure mode\n            is_fail_mode = False\n        else:\n            high_freq_mask = np.abs(k) > mu_train_max\n            high_freq_energy = np.sum(e_hat_base_energy[high_freq_mask])\n            is_fail_mode = (high_freq_energy / total_energy) > failure_threshold\n            \n        results.append([rel_err_base, rel_err_reg, is_improved, is_fail_mode])\n\n    # Final print statement in the exact required format.\n    # The default string representation of a list in Python includes spaces,\n    # which is consistent with the problem's boilerplate skeleton.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3427041"}]}