{"hands_on_practices": [{"introduction": "Before constructing an approximation, we must understand the canvas on which we work: the node set. The quality of a meshfree approximation depends critically on how well the nodes are distributed throughout the domain. This practice [@problem_id:3420013] introduces two fundamental metrics to quantify this distribution: the fill distance $h_{X,\\Omega}$, which measures the largest gap in the domain, and the separation distance $q_X$, which characterizes how close nodes are to each other. Calculating these values provides a concrete understanding of what makes a node set well-behaved for numerical approximation.", "problem": "Consider the two-dimensional domain $\\Omega = [0,1]\\times[0,1] \\subset \\mathbb{R}^{2}$ and the node set $X \\subset \\Omega$ given by the tensor-product grid\n$$\nX = \\left\\{ \\left(\\frac{i}{4}, \\frac{j}{4}\\right) : i,j \\in \\{0,1,2,3,4\\} \\right\\}.\n$$\nAdopt the Euclidean norm $|\\cdot|$ on $\\mathbb{R}^{2}$. Using the standard definitions from meshfree approximation,\n$$\nh_{X,\\Omega} = \\sup_{x \\in \\Omega} \\min_{x_{i} \\in X} |x - x_{i}|, \\qquad q_{X} = \\frac{1}{2} \\min_{\\substack{x_{i}, x_{j} \\in X \\\\ i \\neq j}} |x_{i} - x_{j}|,\n$$\ncompute $h_{X,\\Omega}$ and $q_{X}$ from first principles, and then assess whether $X$ is quasi-uniform by estimating the ratio $\\rho = \\dfrac{h_{X,\\Omega}}{q_{X}}$. Provide a clear geometric argument for the location of the point(s) in $\\Omega$ attaining the supremum in the definition of $h_{X,\\Omega}$ and for the pair(s) of points in $X$ attaining the minimum in the definition of $q_{X}$. Express your final answer as an exact symbolic expression for $\\rho$ with no rounding.", "solution": "The analysis proceeds by computing the two quantities, $q_X$ and $h_{X,\\Omega}$, based on their definitions and the provided node set $X$.\n\n**1. Computation of the Separation Distance, $q_X$**\n\nThe separation distance $q_X$ is defined as half the minimum distance between any two distinct nodes in the set $X$:\n$$\nq_{X} = \\frac{1}{2} \\min_{\\substack{x_{i}, x_{j} \\in X \\\\ i \\neq j}} |x_{i} - x_{j}|\n$$\nThe nodes in $X$ are given by $x_{i,j} = \\left(\\frac{i}{4}, \\frac{j}{4}\\right)$ for $i,j \\in \\{0,1,2,3,4\\}$. Let us consider two distinct nodes, $x_{i,j}$ and $x_{k,l}$, where $(i,j) \\neq (k,l)$. The square of the Euclidean distance between them is:\n$$\n|x_{i,j} - x_{k,l}|^2 = \\left(\\frac{i}{4} - \\frac{k}{4}\\right)^2 + \\left(\\frac{j}{4} - \\frac{l}{4}\\right)^2 = \\frac{1}{16} \\left( (i-k)^2 + (j-l)^2 \\right)\n$$\nTo find the minimum distance, we must find the minimum non-zero value of the term $(i-k)^2 + (j-l)^2$, where $i, j, k, l$ are integers from the set $\\{0,1,2,3,4\\}$. Since $(i,j) \\neq (k,l)$, the term $(i-k)^2 + (j-l)^2$ must be a positive integer. The smallest positive integer that can be formed by a sum of two squares is $1$, which occurs when one of $|i-k|$ or $|j-l|$ is $1$ and the other is $0$.\n\nFor example, consider two horizontally adjacent points, such as $x_{0,0}=(0,0)$ and $x_{1,0}=(1/4, 0)$. Here, $i=0, j=0, k=1, l=0$. The distance is:\n$$\n|x_{0,0} - x_{1,0}| = \\sqrt{\\frac{1}{16} \\left( (0-1)^2 + (0-0)^2 \\right)} = \\sqrt{\\frac{1}{16}} = \\frac{1}{4}\n$$\nThis distance, $1/4$, is the grid spacing. Any other pair of non-adjacent points, such as diagonally adjacent points like $x_{0,0}=(0,0)$ and $x_{1,1}=(1/4, 1/4)$, will be farther apart:\n$$\n|x_{0,0} - x_{1,1}| = \\sqrt{\\frac{1}{16} \\left( (0-1)^2 + (0-1)^2 \\right)} = \\sqrt{\\frac{2}{16}} = \\frac{\\sqrt{2}}{4}\n$$\nSince $\\frac{1}{4} < \\frac{\\sqrt{2}}{4}$, the minimum distance is indeed $1/4$. This minimum is attained between any two horizontally or vertically adjacent nodes in the grid.\n$$\n\\min_{\\substack{x_{i}, x_{j} \\in X \\\\ i \\neq j}} |x_{i} - x_{j}| = \\frac{1}{4}\n$$\nTherefore, the separation distance is:\n$$\nq_X = \\frac{1}{2} \\left(\\frac{1}{4}\\right) = \\frac{1}{8}\n$$\n\n**2. Computation of the Fill Distance, $h_{X,\\Omega}$**\n\nThe fill distance $h_{X, \\Omega}$ is the largest distance any point in the domain $\\Omega$ can be from the nearest node in $X$.\n$$\nh_{X,\\Omega} = \\sup_{x \\in \\Omega} \\min_{x_{i} \\in X} |x - x_{i}|\n$$\nThe node set $X$ forms a uniform $5 \\times 5$ grid that includes the boundary of the domain $\\Omega=[0,1]\\times[0,1]$. This grid partitions the domain $\\Omega$ into a $4 \\times 4$ array of smaller, non-overlapping squares. Each of these small squares has a side length of $1/4$. Let us denote a generic such square as:\n$$\nS_{i,j} = \\left[ \\frac{i}{4}, \\frac{i+1}{4} \\right] \\times \\left[ \\frac{j}{4}, \\frac{j+1}{4} \\right] \\quad \\text{for } i,j \\in \\{0,1,2,3\\}\n$$\nThe vertices of each square $S_{i,j}$ are points in the node set $X$. Specifically, the vertices of $S_{i,j}$ are $x_{i,j}$, $x_{i+1,j}$, $x_{i,j+1}$, and $x_{i+1,j+1}$.\n\nFor any point $x \\in S_{i,j}$, the closest node in the entire set $X$ must be one of the four vertices of $S_{i,j}$. This is because any other node in $X$ is farther away than at least one of these four vertices. Therefore, the problem of finding the global supremum over $\\Omega$ can be reduced to finding the maximum of the local suprema over each small square $S_{i,j}$:\n$$\nh_{X,\\Omega} = \\max_{i,j \\in \\{0,1,2,3\\}} \\left( \\sup_{x \\in S_{i,j}} \\min_{v \\in \\{x_{i,j}, x_{i+1,j}, x_{i,j+1}, x_{i+1,j+1}\\}} |x - v| \\right)\n$$\nWithin a single square $S_{i,j}$, the point that is farthest from all four of its vertices is, by symmetry, its geometric center. Let this center be $x^*_{i,j}$.\n$$\nx^*_{i,j} = \\left( \\frac{i}{4} + \\frac{1}{8}, \\frac{j}{4} + \\frac{1}{8} \\right) = \\left( \\frac{2i+1}{8}, \\frac{2j+1}{8} \\right)\n$$\nThe distance from this center point $x^*_{i,j}$ to any of the four vertices is identical. Let's calculate the distance to the vertex $x_{i,j} = (\\frac{i}{4}, \\frac{j}{4})$:\n$$\n|x^*_{i,j} - x_{i,j}| = \\left| \\left( \\frac{2i+1}{8} - \\frac{2i}{8}, \\frac{2j+1}{8} - \\frac{2j}{8} \\right) \\right| = \\left| \\left( \\frac{1}{8}, \\frac{1}{8} \\right) \\right| = \\sqrt{\\left(\\frac{1}{8}\\right)^2 + \\left(\\frac{1}{8}\\right)^2} = \\sqrt{\\frac{1}{64} + \\frac{1}{64}} = \\sqrt{\\frac{2}{64}} = \\frac{\\sqrt{2}}{8}\n$$\nThis distance represents the maximum \"minimum distance\" within the square $S_{i,j}$. Since this value, $\\frac{\\sqrt{2}}{8}$, is the same for all $16$ small squares that constitute $\\Omega$, the supremum over the entire domain must be this value. The points in $\\Omega$ that attain this supremum are the centers of these $16$ squares.\nThus, the fill distance is:\n$$\nh_{X,\\Omega} = \\frac{\\sqrt{2}}{8}\n$$\n\n**3. Computation and Assessment of the Ratio $\\rho$**\n\nThe quasi-uniformity constant $\\rho$ is the ratio of the fill distance to the separation distance:\n$$\n\\rho = \\frac{h_{X,\\Omega}}{q_{X}}\n$$\nSubstituting the computed values:\n$$\n\\rho = \\frac{\\frac{\\sqrt{2}}{8}}{\\frac{1}{8}} = \\sqrt{2}\n$$\nA set of points $X$ is considered quasi-uniform if the ratio $\\rho$ is bounded by a constant that is independent of the number of points or the characteristic spacing between them. In this case, $\\rho = \\sqrt{2}$, a small constant. This indicates that the node set $X$ is indeed quasi-uniform. The value $\\sqrt{2}$ is characteristic for uniform square grids.", "answer": "$$\\boxed{\\sqrt{2}}$$", "id": "3420013"}, {"introduction": "With a properly characterized node set, the next step is to build an approximation. The Moving Least Squares (MLS) method is a cornerstone of many meshfree techniques, allowing for the construction of smooth approximation functions from scattered data points. This exercise [@problem_id:3420003] guides you through the core mechanics of MLS, from forming the moment matrix to deriving the shape functions and their derivatives. Mastering the calculation of shape function derivatives is especially critical, as they form the foundation for discretizing and solving partial differential equations.", "problem": "Consider the Moving Least Squares (MLS) approximation in one spatial dimension for the numerical solution of partial differential equations using meshfree methods. Use the linear polynomial basis $p(x) = [1, x]^{\\top}$ and a Gaussian weight function $w(r) = \\exp\\!\\big(- (r / \\delta)^{2}\\big)$ with shape parameter $\\delta = 0.6$. Three nodes are located at positions $x_1 = 0$, $x_2 = 0.5$, and $x_3 = 1$, and the evaluation point is $x_0 = 0.5$. Let the test function be the smooth function $u(x) = \\exp(x)$.\n\nStarting from the least-squares functional for MLS and the normal equations, derive the MLS shape functions $N_i(x_0)$ for $i \\in \\{1,2,3\\}$ at the point $x_0$, explicitly in terms of the Gaussian weights at $x_0$. Then, using the definition of the MLS shape function derivatives, compute the MLS approximation to the spatial gradient (which in one dimension is the first derivative) at $x_0$:\n$$\nu_x^{h}(x_0) \\;=\\; \\sum_{i=1}^{3} N_i'(x_0)\\,u(x_i).\n$$\nAll intermediate steps must follow from first principles of the MLS construction, including the formation of the moment matrix, its inverse, and the differentiation of the MLS shape functions with respect to $x$ at $x_0$.\n\nExpress the final answer for $u_x^{h}(x_0)$ as a single exact analytic expression (do not decimalize). No units are required, and no rounding is needed.", "solution": "The Moving Least Squares (MLS) approximation of a function $u(x)$ at an evaluation point $x_0$ is given by $u^h(x_0) = \\mathbf{p}^{\\top}(x_0) \\mathbf{a}(x_0)$, where $\\mathbf{p}(x) = \\begin{pmatrix} 1 \\\\ x \\end{pmatrix}$ is the polynomial basis and $\\mathbf{a}(x_0)$ is a vector of coefficients determined by minimizing the weighted discrete $L_2$ norm:\n$$ J(\\mathbf{a}) = \\sum_{i=1}^{3} w(x_0 - x_i) \\left[ \\mathbf{p}^{\\top}(x_i) \\mathbf{a} - u(x_i) \\right]^2 $$\nThe normal equations, $\\frac{\\partial J}{\\partial \\mathbf{a}} = 0$, yield the linear system $\\mathbf{M}(x_0) \\mathbf{a}(x_0) = \\mathbf{C}(x_0) \\mathbf{u}$, where the moment matrix is $\\mathbf{M}(x_0) = \\sum_{i=1}^{3} w(x_0-x_i) \\mathbf{p}(x_i) \\mathbf{p}^{\\top}(x_i)$ and $\\mathbf{C}(x_0)\\mathbf{u} = \\sum_{i=1}^{3} w(x_0-x_i) \\mathbf{p}(x_i) u(x_i)$.\n\nSolving for $\\mathbf{a}(x_0)$ and substituting back gives the MLS approximation in terms of shape functions $N_i(x_0)$:\n$$ u^h(x_0) = \\sum_{i=1}^{3} N_i(x_0) u(x_i) $$\nwhere the shape function for node $i$ is given by:\n$$ N_i(x_0) = \\mathbf{p}^{\\top}(x_0) \\mathbf{M}^{-1}(x_0) \\mathbf{p}(x_i) w(x_0-x_i) $$\n\n**1. Calculation of Shape Functions $N_i(x_0)$**\n\nFirst, we evaluate the necessary components at $x_0=0.5$.\nThe node positions are $x_1 = 0$, $x_2 = 0.5$, $x_3 = 1$. The shape parameter is $\\delta=0.6$.\nThe distances from the evaluation point are $r_1 = |0.5-0|=0.5$, $r_2 = |0.5-0.5|=0$, $r_3 = |0.5-1|=0.5$.\nThe weights are:\n$w_1 = w(r_1) = \\exp(-(0.5/0.6)^2) = \\exp(-(5/6)^2) = \\exp(-25/36)$.\n$w_2 = w(r_2) = \\exp(0) = 1$.\n$w_3 = w(r_3) = \\exp(-(0.5/0.6)^2) = w_1$.\nLet's denote $w_{13} = w_1 = w_3$.\n\nThe basis vectors evaluated at the nodes are:\n$\\mathbf{p}(x_1) = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$, $\\mathbf{p}(x_2) = \\begin{pmatrix} 1 \\\\ 0.5 \\end{pmatrix}$, $\\mathbf{p}(x_3) = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$.\n\nThe moment matrix $\\mathbf{M}(x_0)$ at $x_0=0.5$ is:\n$$ \\mathbf{M}(0.5) = w_1 \\mathbf{p}(x_1)\\mathbf{p}^{\\top}(x_1) + w_2 \\mathbf{p}(x_2)\\mathbf{p}^{\\top}(x_2) + w_3 \\mathbf{p}(x_3)\\mathbf{p}^{\\top}(x_3) $$\n$$ \\mathbf{M}(0.5) = w_{13} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\end{pmatrix} + 1 \\begin{pmatrix} 1 \\\\ 0.5 \\end{pmatrix} \\begin{pmatrix} 1 & 0.5 \\end{pmatrix} + w_{13} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} \\begin{pmatrix} 1 & 1 \\end{pmatrix} $$\n$$ \\mathbf{M}(0.5) = w_{13} \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix} + \\begin{pmatrix} 1 & 0.5 \\\\ 0.5 & 0.25 \\end{pmatrix} + w_{13} \\begin{pmatrix} 1 & 1 \\\\ 1 & 1 \\end{pmatrix} = \\begin{pmatrix} 1+2w_{13} & 0.5+w_{13} \\\\ 0.5+w_{13} & 0.25+w_{13} \\end{pmatrix} $$\nThe determinant of $\\mathbf{M}(0.5)$ is:\n$$ \\det(\\mathbf{M}(0.5)) = (1+2w_{13})(0.25+w_{13}) - (0.5+w_{13})^2 $$\n$$ = (0.25 + 1.5w_{13} + 2w_{13}^2) - (0.25 + w_{13} + w_{13}^2) = 0.5w_{13} + w_{13}^2 = w_{13}(0.5+w_{13}) $$\nThe inverse moment matrix is:\n$$ \\mathbf{M}^{-1}(0.5) = \\frac{1}{w_{13}(0.5+w_{13})} \\begin{pmatrix} 0.25+w_{13} & -(0.5+w_{13}) \\\\ -(0.5+w_{13}) & 1+2w_{13} \\end{pmatrix} $$\nFor convenience, let's compute the vector $\\mathbf{v}^{\\top} = \\mathbf{p}^{\\top}(x_0) \\mathbf{M}^{-1}(x_0)$:\n$$ \\mathbf{v}^{\\top} = \\begin{pmatrix} 1 & 0.5 \\end{pmatrix} \\frac{1}{w_{13}(0.5+w_{13})} \\begin{pmatrix} 0.25+w_{13} & -(0.5+w_{13}) \\\\ -(0.5+w_{13}) & 1+2w_{13} \\end{pmatrix} $$\n$$ \\mathbf{v}^{\\top} = \\frac{1}{w_{13}(0.5+w_{13})} \\begin{pmatrix} (0.25+w_{13}) - 0.5(0.5+w_{13}) & -(0.5+w_{13}) + 0.5(1+2w_{13}) \\end{pmatrix} $$\n$$ \\mathbf{v}^{\\top} = \\frac{1}{w_{13}(0.5+w_{13})} \\begin{pmatrix} 0.5w_{13} & 0 \\end{pmatrix} = \\frac{1}{0.5+w_{13}} \\begin{pmatrix} 0.5 & 0 \\end{pmatrix} $$\nNow we can find the shape functions $N_i(x_0) = \\mathbf{v}^{\\top} \\mathbf{p}(x_i) w_i$:\n$$ N_1(0.5) = \\left( \\frac{1}{0.5+w_{13}} \\begin{pmatrix} 0.5 & 0 \\end{pmatrix} \\right) \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} w_{13} = \\frac{0.5 w_{13}}{0.5+w_{13}} $$\n$$ N_2(0.5) = \\left( \\frac{1}{0.5+w_{13}} \\begin{pmatrix} 0.5 & 0 \\end{pmatrix} \\right) \\begin{pmatrix} 1 \\\\ 0.5 \\end{pmatrix} (1) = \\frac{0.5}{0.5+w_{13}} $$\n$$ N_3(0.5) = \\left( \\frac{1}{0.5+w_{13}} \\begin{pmatrix} 0.5 & 0 \\end{pmatrix} \\right) \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} w_{13} = \\frac{0.5 w_{13}}{0.5+w_{13}} $$\nwhere $w_{13} = \\exp(-25/36)$.\n\n**2. Calculation of Shape Function Derivatives $N_i'(x_0)$**\n\nThe derivative of the shape function is found by applying the product rule:\n$$ N_i'(x) = \\frac{d}{dx} \\left( \\mathbf{p}^{\\top}(x) \\mathbf{M}^{-1}(x) \\mathbf{p}(x_i) w(x-x_i) \\right) $$\nLet $\\mathbf{C}(x)=\\mathbf{M}^{-1}(x)$.\n$$ N_i'(x) = \\left( \\mathbf{p}'^{\\top}\\mathbf{C}\\mathbf{p}_i + \\mathbf{p}^{\\top}\\mathbf{C}'\\mathbf{p}_i \\right)w_i + \\left( \\mathbf{p}^{\\top}\\mathbf{C}\\mathbf{p}_i \\right)w_i' $$\nwhere $w_i = w(x-x_i)$, $w_i' = \\frac{d}{dx}w(x-x_i)$, $\\mathbf{p} = \\mathbf{p}(x)$, $\\mathbf{p}_i = \\mathbf{p}(x_i)$, $\\mathbf{C}' = -\\mathbf{C}\\mathbf{M}'\\mathbf{C}$, and $\\mathbf{M}'(x) = \\sum_j \\frac{d}{dx}w(x-x_j) \\mathbf{p}_j\\mathbf{p}_j^{\\top}$.\nThe derivatives are evaluated at $x_0=0.5$.\n\nThe derivative of the weight function is $\\frac{dw(r)}{dr} = \\exp(-(r/\\delta)^2) \\cdot (-2r/\\delta^2)$.\nThe derivatives with respect to $x$ are $w_i' = \\frac{dw(x-x_i)}{dx}|_{x_0} = \\frac{dw(r)}{dr}|_{r_i} \\cdot \\text{sgn}(x_0-x_i)$.\n$w_1' = \\left( \\exp(-(0.5/\\delta)^2) \\frac{-2(0.5)}{\\delta^2} \\right) \\cdot (1) = -w_{13}/\\delta^2$.\n$w_2' = 0$ since $r_2=0$.\n$w_3' = \\left( \\exp(-(0.5/\\delta)^2) \\frac{-2(0.5)}{\\delta^2} \\right) \\cdot (-1) = w_{13}/\\delta^2$.\nThe derivative of the basis vector is $\\mathbf{p}'(x) = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$.\n\nThe derivative of the moment matrix at $x_0=0.5$ is:\n$$ \\mathbf{M}'(0.5) = w_1' \\mathbf{p}_1 \\mathbf{p}_1^{\\top} + w_2' \\mathbf{p}_2 \\mathbf{p}_2^{\\top} + w_3' \\mathbf{p}_3 \\mathbf{p}_3^{\\top} $$\n$$ \\mathbf{M}'(0.5) = \\frac{-w_{13}}{\\delta^2} \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix} + \\frac{w_{13}}{\\delta^2} \\begin{pmatrix} 1 & 1 \\\\ 1 & 1 \\end{pmatrix} = \\frac{w_{13}}{\\delta^2} \\begin{pmatrix} 0 & 1 \\\\ 1 & 1 \\end{pmatrix} $$\nA detailed calculation reveals that the dependencies on $w_{13}$ and $\\delta$ cancel out perfectly, yielding a very simple result. The consistency conditions for MLS derivatives, $\\sum_i N_i'(x) = 0$ and $\\sum_i N_i'(x) x_i = 1$, can be used to verify the result.\n\nThe derivatives evaluate to:\n$$ N_1'(0.5) = -1 $$\n$$ N_2'(0.5) = 0 $$\n$$ N_3'(0.5) = 1 $$\n\n**3. Compute Approximate Gradient $u_x^h(x_0)$**\n\nThe MLS approximation of the gradient at $x_0=0.5$ is:\n$$ u_x^{h}(x_0) = \\sum_{i=1}^{3} N_i'(x_0) u(x_i) = N_1'(0.5) u(x_1) + N_2'(0.5) u(x_2) + N_3'(0.5) u(x_3) $$\nThe test function is $u(x) = \\exp(x)$. The values at the nodes are:\n$u(x_1) = u(0) = \\exp(0) = 1$.\n$u(x_2) = u(0.5) = \\exp(0.5)$.\n$u(x_3) = u(1) = \\exp(1) = e$.\n\nSubstituting these values and the computed derivatives:\n$$ u_x^{h}(0.5) = (-1)(1) + (0)(\\exp(0.5)) + (1)(e) $$\n$$ u_x^{h}(0.5) = e - 1 $$\nThis result corresponds to the central difference approximation of the derivative at $x=0.5$ over the interval $[0,1]$, i.e., $\\frac{u(1)-u(0)}{1-0}$. This simplification occurs due to the symmetric arrangement of nodes around the evaluation point, which itself is a node.", "answer": "$$\\boxed{e-1}$$", "id": "3420003"}, {"introduction": "A numerical scheme must be able to accurately represent simple functions to be considered reliable. The property of 'polynomial reproduction' serves as a fundamental check for the consistency of a meshfree operator, ensuring that it is exact for the polynomial functions that form the building blocks of smoother solutions. In this exercise [@problem_id:3419991], you will perform a crucial verification step by testing a given discrete functional against a basis of polynomials to determine its order of exactness. This process is fundamental to validating the quality and convergence properties of a numerical method.", "problem": "In meshfree Moving Least Squares (MLS) methods for partial differential equations, a common verification step is to test polynomial reproduction by checking whether a discrete linear functional reproduces pointwise values of all polynomials up to a specified order. Consider the following discrete functional that approximates point evaluation at the origin for a scalar field $u$ in two spatial dimensions. The functional acts on nodal samples $u(\\boldsymbol{x}_{j})$ at a stencil of nodes $\\{\\boldsymbol{x}_{j}\\}_{j=1}^{N}$ via a weight vector $\\boldsymbol{w} = (w_{1},\\dots,w_{N})^{\\top}$:\n$$\\mathcal{L}_{h}[u] \\;=\\; \\sum_{j=1}^{N} w_{j}\\,u(\\boldsymbol{x}_{j}).$$\nPolynomial reproduction up to order $m$ means that for every polynomial $p$ of total degree at most $m$,\n$$\\sum_{j=1}^{N} w_{j}\\,p(\\boldsymbol{x}_{j}) \\;=\\; p(\\boldsymbol{0}).$$\nYou are given a two-dimensional stencil with $N=6$ nodes\n$$\\boldsymbol{x}_{1}=(1,0),\\quad \\boldsymbol{x}_{2}=(-1,0),\\quad \\boldsymbol{x}_{3}=(0,1),\\quad \\boldsymbol{x}_{4}=(0,-1),\\quad \\boldsymbol{x}_{5}=(1,1),\\quad \\boldsymbol{x}_{6}=(-1,-1),$$\nand a weight vector of the form\n$$\\boldsymbol{w} \\;=\\; (a,\\,a,\\,a,\\,a,\\,b,\\,b)^{\\top},$$\nwith $a=\\frac{1}{8}$ and $b=\\frac{1}{4}$.\nLet $m=2$ and adopt the canonical monomial basis\n$$\\mathcal{B} \\;=\\; \\{\\,1,\\,x,\\,y,\\,x^{2},\\,xy,\\,y^{2}\\,\\}.$$\nFor each $p\\in\\mathcal{B}$, define the exactness error\n$$e(p) \\;=\\; \\sum_{j=1}^{6} w_{j}\\,p(\\boldsymbol{x}_{j}) \\;-\\; p(\\boldsymbol{0}).$$\nDefine the worst-case exactness error over this basis by\n$$E \\;=\\; \\max_{p\\in\\mathcal{B}} \\,|\\,e(p)\\,|.$$\nCompute $E$ exactly as a reduced fraction. Provide your final answer as a single real number without units. No rounding is required.", "solution": "The problem asks for the computation of the worst-case exactness error, $E$, for a given discrete linear functional over a canonical monomial basis $\\mathcal{B}$ up to total degree $m=2$. The functional approximates the value of a function at the origin, $p(\\boldsymbol{0})$, using a weighted sum of its values at a stencil of $N=6$ nodes.\n\nThe discrete functional is defined as:\n$$\\mathcal{L}_{h}[p] = \\sum_{j=1}^{6} w_{j}\\,p(\\boldsymbol{x}_{j})$$\nThe exactness error for a polynomial $p$ is:\n$$e(p) = \\mathcal{L}_{h}[p] - p(\\boldsymbol{0})$$\nThe worst-case error over the basis $\\mathcal{B} = \\{\\,1,\\,x,\\,y,\\,x^{2},\\,xy,\\,y^{2}\\,\\}$ is:\n$$E = \\max_{p\\in\\mathcal{B}} \\,|\\,e(p)\\,|$$\nThe given nodes are:\n$\\boldsymbol{x}_{1}=(1,0)$, $\\boldsymbol{x}_{2}=(-1,0)$, $\\boldsymbol{x}_{3}=(0,1)$, $\\boldsymbol{x}_{4}=(0,-1)$, $\\boldsymbol{x}_{5}=(1,1)$, $\\boldsymbol{x}_{6}=(-1,-1)$.\nThe weights are given by $a=\\frac{1}{8}$ and $b=\\frac{1}{4}$, such that the weight vector is $\\boldsymbol{w} = (w_1, w_2, w_3, w_4, w_5, w_6)^{\\top} = (a, a, a, a, b, b)^{\\top}$.\nThus, $w_1 = w_2 = w_3 = w_4 = \\frac{1}{8}$ and $w_5 = w_6 = \\frac{1}{4}$.\n\nWe will systematically compute the error $e(p)$ for each basis polynomial $p \\in \\mathcal{B}$.\n\n1. For $p(x,y)=1$:\nThe value at any node is $p(\\boldsymbol{x}_j)=1$. The value at the origin is $p(\\boldsymbol{0})=1$.\n$$\\mathcal{L}_{h}[1] = \\sum_{j=1}^{6} w_j \\cdot 1 = 4a + 2b = 4\\left(\\frac{1}{8}\\right) + 2\\left(\\frac{1}{4}\\right) = \\frac{1}{2} + \\frac{1}{2} = 1$$\n$$e(1) = \\mathcal{L}_{h}[1] - p(\\boldsymbol{0}) = 1 - 1 = 0$$\n\n2. For $p(x,y)=x$:\nThe values at the nodes are $p(\\boldsymbol{x}_1)=1$, $p(\\boldsymbol{x}_2)=-1$, $p(\\boldsymbol{x}_3)=0$, $p(\\boldsymbol{x}_4)=0$, $p(\\boldsymbol{x}_5)=1$, $p(\\boldsymbol{x}_6)=-1$. The value at the origin is $p(\\boldsymbol{0})=0$.\n$$\\mathcal{L}_{h}[x] = w_1(1) + w_2(-1) + w_3(0) + w_4(0) + w_5(1) + w_6(-1) = a - a + b - b = 0$$\n$$e(x) = \\mathcal{L}_{h}[x] - p(\\boldsymbol{0}) = 0 - 0 = 0$$\n\n3. For $p(x,y)=y$:\nThe values at the nodes are $p(\\boldsymbol{x}_1)=0$, $p(\\boldsymbol{x}_2)=0$, $p(\\boldsymbol{x}_3)=1$, $p(\\boldsymbol{x}_4)=-1$, $p(\\boldsymbol{x}_5)=1$, $p(\\boldsymbol{x}_6)=-1$. The value at the origin is $p(\\boldsymbol{0})=0$.\n$$\\mathcal{L}_{h}[y] = w_1(0) + w_2(0) + w_3(1) + w_4(-1) + w_5(1) + w_6(-1) = a - a + b - b = 0$$\n$$e(y) = \\mathcal{L}_{h}[y] - p(\\boldsymbol{0}) = 0 - 0 = 0$$\nThis demonstrates that the functional reproduces all polynomials of total degree $1$ exactly.\n\n4. For $p(x,y)=x^2$:\nThe values at the nodes are $p(\\boldsymbol{x}_1)=1^2=1$, $p(\\boldsymbol{x}_2)=(-1)^2=1$, $p(\\boldsymbol{x}_3)=0^2=0$, $p(\\boldsymbol{x}_4)=0^2=0$, $p(\\boldsymbol{x}_5)=1^2=1$, $p(\\boldsymbol{x}_6)=(-1)^2=1$. The value at the origin is $p(\\boldsymbol{0})=0$.\n$$\\mathcal{L}_{h}[x^2] = w_1(1) + w_2(1) + w_3(0) + w_4(0) + w_5(1) + w_6(1) = a+a+b+b = 2a+2b$$\nSubstituting the values of $a$ and $b$:\n$$\\mathcal{L}_{h}[x^2] = 2\\left(\\frac{1}{8}\\right) + 2\\left(\\frac{1}{4}\\right) = \\frac{1}{4} + \\frac{1}{2} = \\frac{3}{4}$$\n$$e(x^2) = \\mathcal{L}_{h}[x^2] - p(\\boldsymbol{0}) = \\frac{3}{4} - 0 = \\frac{3}{4}$$\n\n5. For $p(x,y)=xy$:\nThe values at the nodes are $p(\\boldsymbol{x}_1)=0$, $p(\\boldsymbol{x}_2)=0$, $p(\\boldsymbol{x}_3)=0$, $p(\\boldsymbol{x}_4)=0$, $p(\\boldsymbol{x}_5)=1 \\cdot 1 = 1$, $p(\\boldsymbol{x}_6)=(-1) \\cdot (-1) = 1$. The value at the origin is $p(\\boldsymbol{0})=0$.\n$$\\mathcal{L}_{h}[xy] = w_1(0) + w_2(0) + w_3(0) + w_4(0) + w_5(1) + w_6(1) = b+b = 2b$$\nSubstituting the value of $b$:\n$$\\mathcal{L}_{h}[xy] = 2\\left(\\frac{1}{4}\\right) = \\frac{1}{2}$$\n$$e(xy) = \\mathcal{L}_{h}[xy] - p(\\boldsymbol{0}) = \\frac{1}{2} - 0 = \\frac{1}{2}$$\n\n6. For $p(x,y)=y^2$:\nThe values at the nodes are $p(\\boldsymbol{x}_1)=0^2=0$, $p(\\boldsymbol{x}_2)=0^2=0$, $p(\\boldsymbol{x}_3)=1^2=1$, $p(\\boldsymbol{x}_4)=(-1)^2=1$, $p(\\boldsymbol{x}_5)=1^2=1$, $p(\\boldsymbol{x}_6)=(-1)^2=1$. The value at the origin is $p(\\boldsymbol{0})=0$.\n$$\\mathcal{L}_{h}[y^2] = w_1(0) + w_2(0) + w_3(1) + w_4(1) + w_5(1) + w_6(1) = a+a+b+b = 2a+2b$$\nThis is the same expression as for $p(x,y)=x^2$:\n$$\\mathcal{L}_{h}[y^2] = 2\\left(\\frac{1}{8}\\right) + 2\\left(\\frac{1}{4}\\right) = \\frac{1}{4} + \\frac{1}{2} = \\frac{3}{4}$$\n$$e(y^2) = \\mathcal{L}_{h}[y^2] - p(\\boldsymbol{0}) = \\frac{3}{4} - 0 = \\frac{3}{4}$$\n\nThe set of exactness errors for the basis polynomials in $\\mathcal{B}$ is:\n$\\{e(1), e(x), e(y), e(x^2), e(xy), e(y^2)\\} = \\{0, 0, 0, \\frac{3}{4}, \\frac{1}{2}, \\frac{3}{4}\\}$\n\nThe worst-case exactness error $E$ is the maximum of the absolute values of these errors:\n$$E = \\max\\left\\{|0|, |0|, |0|, \\left|\\frac{3}{4}\\right|, \\left|\\frac{1}{2}\\right|, \\left|\\frac{3}{4}\\right|\\right\\} = \\max\\left\\{0, \\frac{3}{4}, \\frac{1}{2}\\right\\}$$\nComparing the non-zero error values, $\\frac{1}{2} = \\frac{2}{4}$. Since $\\frac{3}{4} > \\frac{2}{4}$, the maximum is $\\frac{3}{4}$.\n$$E = \\frac{3}{4}$$\nThe result is already a reduced fraction.", "answer": "$$\\boxed{\\frac{3}{4}}$$", "id": "3419991"}]}