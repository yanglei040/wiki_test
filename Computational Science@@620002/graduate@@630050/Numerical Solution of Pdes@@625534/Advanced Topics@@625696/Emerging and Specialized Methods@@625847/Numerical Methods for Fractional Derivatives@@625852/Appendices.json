{"hands_on_practices": [{"introduction": "A common challenge in solving time-fractional differential equations is that their solutions are often not smooth at the initial time $t=0$, exhibiting a \"start-up singularity\" of the form $t^{\\sigma}$. This lack of regularity can severely degrade the accuracy of standard numerical schemes on uniform time grids. This exercise [@problem_id:3426227] guides you through the analysis of a powerful remedy: the graded mesh, which clusters grid points near the singularity to restore the optimal convergence rate of the popular L1 scheme.", "problem": "Consider the time-fractional diffusion equation with the Caputo fractional derivative of order $\\alpha \\in (0,1)$ on the spatial interval $x \\in (0,1)$ with homogeneous Dirichlet boundary conditions and a source term chosen so that the exact solution is $u(x,t) = \\sin(\\pi x)\\,t^{\\sigma}$ for $t \\in [0,1]$, where $\\sigma \\in (0,1]$. By projecting onto the first eigenmode of the Laplacian, the temporal evolution reduces to a scalar equation for the amplitude $y(t)$ that has the exact solution $y(t) = t^{\\sigma}$. The Caputo fractional derivative of order $\\alpha$ is defined by\n$$\n\\prescript{C}{}{D}_{t}^{\\alpha} y(t) = \\frac{1}{\\Gamma(1-\\alpha)} \\int_{0}^{t} (t-s)^{-\\alpha}\\,y'(s)\\,\\mathrm{d}s.\n$$\nDiscretize the time interval $[0,1]$ using $N$ steps on a graded mesh $\\{t_{n}\\}_{n=0}^{N}$ defined by $t_{n} = \\left(\\frac{n}{N}\\right)^{\\gamma}$ with grading exponent $\\gamma \\geq 1$. Approximate the Caputo derivative at $t=t_{n}$ via the classical $L1$ scheme,\n$$\n\\mathcal{L}_{1}[y](t_{n}) = \\frac{1}{\\Gamma(1-\\alpha)} \\sum_{k=1}^{n} \\left(\\frac{y(t_{k}) - y(t_{k-1})}{t_{k} - t_{k-1}} \\right) \\int_{t_{k-1}}^{t_{k}} (t_{n}-s)^{-\\alpha}\\,\\mathrm{d}s.\n$$\nStarting only from the above definitions and well-tested facts about the smooth-solution accuracy of the $L1$ scheme, analyze the leading-order behavior as $N \\to \\infty$ of the global discretization error at $t=1$,\n$$\nE_{N} = \\left|\\prescript{C}{}{D}_{t}^{\\alpha} y(1) - \\mathcal{L}_{1}[y](1)\\right|,\n$$\nfor the start-up singular solution $y(t) = t^{\\sigma}$. Then determine the minimal grading exponent $\\gamma$ such that the overall error satisfies $E_{N} = \\mathcal{O}\\!\\left(N^{-(2-\\alpha)}\\right)$, i.e., the method achieves its smooth-solution accuracy order. Your final answer must be a single closed-form analytic expression in terms of $\\alpha$ and $\\sigma$ for the minimal $\\gamma$ required to ensure $E_{N} = \\mathcal{O}\\!\\left(N^{-(2-\\alpha)}\\right)$ as $N \\to \\infty$.", "solution": "The problem requires an analysis of the global discretization error, $E_{N}$, for the $L1$ approximation of the Caputo fractional derivative of order $\\alpha$ for the function $y(t) = t^{\\sigma}$ at $t=1$. The time domain $[0,1]$ is discretized using a graded mesh $t_{n} = (n/N)^{\\gamma}$ for $n=0, 1, \\dots, N$ with a grading exponent $\\gamma \\geq 1$. The goal is to find the minimal $\\gamma$ that ensures the error $E_N$ converges at the optimal rate of $\\mathcal{O}(N^{-(2-\\alpha)})$, which is the known rate for sufficiently smooth solutions.\n\nThe global error at $t=t_N=1$ is given by\n$$\nE_{N} = \\left|\\prescript{C}{}{D}_{t}^{\\alpha} y(1) - \\mathcal{L}_{1}[y](1)\\right|.\n$$\nBy substituting the definitions of the Caputo derivative and the $L1$ scheme, we can express the error as a sum of local errors over the subintervals $[t_{k-1}, t_k]$:\n$$\nE_{N} = \\frac{1}{\\Gamma(1-\\alpha)} \\left| \\sum_{k=1}^{N} \\int_{t_{k-1}}^{t_{k}} (1-s)^{-\\alpha} \\left( y'(s) - \\frac{y(t_k) - y(t_{k-1})}{t_k - t_{k-1}} \\right) \\mathrm{d}s \\right|.\n$$\nThe function is $y(t) = t^{\\sigma}$, for which the first derivative is $y'(t) = \\sigma t^{\\sigma-1}$ and the second derivative is $y''(t) = \\sigma(\\sigma-1)t^{\\sigma-2}$. For the given range $\\sigma \\in (0,1]$, the second derivative $y''(t)$ is singular at $t=0$ (unless $\\sigma=1$, in which case $y''(t)=0$, and the problem becomes trivial as the $L1$ scheme is exact for linear functions). This singularity at the origin is the source of the convergence-order reduction when using a uniform mesh. The graded mesh, by clustering points near $t=0$, is designed to mitigate the effect of this singularity.\n\nThe error analysis for the $L1$ scheme on a graded mesh for solutions with an initial algebraic singularity of the form $t^{\\sigma}$ reveals that the global error is determined by two competing factors:\n$1$. The error arising from the approximation of the non-smooth part of the function near $t=0$. This error is controlled by the grading exponent $\\gamma$ and the singularity strength $\\sigma$.\n$2$. The standard truncation error of the numerical method over the smoother part of the domain, which corresponds to the rate observed for regular solutions (e.g., $y \\in C^2[0,1]$).\n\nA standard result from the numerical analysis of fractional equations states that the convergence order of the $L1$ scheme on a graded mesh for a function with the regularity of $y(t)=t^\\sigma$ is given by the balance of these two error sources. The global error $E_N$ takes the form:\n$$\nE_N = \\mathcal{O}(N^{-\\min(\\gamma \\sigma, 2-\\alpha)}).\n$$\nThe term $\\mathcal{O}(N^{-\\gamma \\sigma})$ represents the error contribution from the initial singularity. A finer mesh near $t=0$ (larger $\\gamma$) or a smoother solution (larger $\\sigma$) results in a faster decay of this error component. The term $\\mathcal{O}(N^{-(2-\\alpha)})$ is the canonical convergence rate of the $L1$ scheme for smooth solutions, which the problem statement refers to as the \"smooth-solution accuracy\". The overall accuracy is limited by the slower of these two rates.\n\nOur objective is to determine the minimal grading exponent $\\gamma$ such that the method achieves its optimal smooth-solution accuracy. This means we require the overall error to be $E_N = \\mathcal{O}(N^{-(2-\\alpha)})$. For this to hold, the error component due to the singularity, $\\mathcal{O}(N^{-\\gamma \\sigma})$, must converge at a rate at least as fast as $\\mathcal{O}(N^{-(2-\\alpha)})$. This imposes the condition on the exponents:\n$$\n\\gamma \\sigma \\geq 2-\\alpha.\n$$\nTo find the minimal grading exponent $\\gamma$ that satisfies this condition, we take the equality, as any smaller $\\gamma$ would result in the $\\mathcal{O}(N^{-\\gamma \\sigma})$ term dominating and thus determining a suboptimal convergence rate.\n$$\n\\gamma \\sigma = 2-\\alpha.\n$$\nSolving for $\\gamma$, we obtain the minimal required grading exponent:\n$$\n\\gamma = \\frac{2-\\alpha}{\\sigma}.\n$$\nWe should verify that this value is consistent with the constraint $\\gamma \\geq 1$. Given that $\\alpha \\in (0,1)$ and $\\sigma \\in (0,1]$, the numerator $2-\\alpha$ is in the interval $(1,2)$, and the denominator $\\sigma$ is in $(0,1]$. Therefore, the ratio $\\frac{2-\\alpha}{\\sigma}$ is always greater than $1$. The condition is satisfied. This choice of $\\gamma$ precisely balances the error from the start-up singularity with the intrinsic error of the numerical scheme, thereby restoring the optimal convergence order.", "answer": "$$\n\\boxed{\\frac{2-\\alpha}{\\sigma}}\n$$", "id": "3426227"}, {"introduction": "A hallmark of a robust numerical method is its ability to behave correctly in limiting cases. For fractional calculus, as the order $\\alpha$ approaches an integer, the fractional operator should recover its integer-order counterpart. This coding practice [@problem_id:3426234] challenges you to implement and compare two time-stepping schemes to see firsthand why a naive discretization fails to capture the correct behavior as $\\alpha \\to 0^+$, highlighting the critical role of initial condition treatment in the algorithm's design.", "problem": "Consider the initial value problem for a fractional-in-time ordinary differential equation derived from a time-fractional partial differential equation context, written in Caputo form: find a function $u:[0,T]\\to\\mathbb{R}$ such that\n$\nD_t^\\alpha u(t) = f(t), \\quad t\\in(0,T], \\qquad u(0)=u_0,\n$\nwhere $D_t^\\alpha$ denotes the Caputo fractional derivative of order $\\alpha\\in(0,1)$, and $f$ is a prescribed source term. The Caputo fractional derivative is defined using the Riemann–Liouville integral of order $1-\\alpha$ applied to the first derivative,\n$\nD_t^\\alpha u(t) := I^{1-\\alpha} u'(t), \\qquad I^\\beta v(t) := \\frac{1}{\\Gamma(\\beta)}\\int_0^t (t-s)^{\\beta-1} v(s)\\,\\mathrm{d}s, \\quad \\beta>0.\n$\nIt is a well-tested fact that, for sufficiently regular $u$, the limit $\\alpha\\to 0^+$ yields\n$\n\\lim_{\\alpha\\to 0^+} D_t^\\alpha u(t) = u(t)-u(0) \\quad \\text{for } t>0,\n$\nwhich implies that, formally, the solution of $D_t^\\alpha u = f$ satisfies $u(t)\\to u(0)+f(t)$ as $\\alpha\\to 0^+$ for $t>0$.\n\nYour task is to derive from first principles a stable and consistent time-stepping scheme that correctly captures the limit $\\alpha\\to 0^+$ and to implement it. Additionally, implement a naive discretization that mis-handles the limiting memory and diagnose its failure. Start from the following fundamental base:\n- The definition of the Caputo derivative $D_t^\\alpha u$ in terms of $I^{1-\\alpha} u'(t)$ for $\\alpha\\in(0,1)$.\n- The classical backward Euler method for the first derivative and the concept of Convolution Quadrature (CQ), which for a linear multistep method with generating function $\\delta(z)$ yields discrete convolution weights through the power series of $\\delta(z)^\\alpha$.\n- The Caputo initial-value correction that replaces $u$ by $u-u(0)$ inside the fractional operator, consistently enforcing the initial condition at $t=0$.\n\nDesign the following two fully discrete-in-time schemes on a uniform grid $t_n = n\\tau$, $n=0,1,\\dots,N$, where $\\tau = T/N$:\n- A Caputo-consistent scheme based on backward Euler Convolution Quadrature (CQ) that incorporates the Caputo initial-value correction so that the scheme recovers $u^n \\approx u_0 + f(t_n)$ as $\\alpha\\to 0^+$ for $n\\ge 1$.\n- A naive Grünwald-type scheme that omits the Caputo initial-value correction, so that it fails to recover the correct limit and instead tends to $u^n \\approx f(t_n)$ as $\\alpha\\to 0^+$.\n\nThen, for each discrete time level $n\\in\\{1,2,\\dots,N\\}$, compare $u^n$ from each scheme with the limiting target $u_0+f(t_n)$ and compute the maximum absolute deviation over positive times $t_n>0$:\n$\nE_{\\mathrm{cap}} := \\max_{1\\le n\\le N} \\big|u^n_{\\mathrm{cap}} - (u_0+f(t_n))\\big|, \\qquad E_{\\mathrm{naive}} := \\max_{1\\le n\\le N} \\big|u^n_{\\mathrm{naive}} - (u_0+f(t_n))\\big|.\n$\n\nAngle unit requirement: whenever trigonometric functions are used, interpret their arguments in radians.\n\nTest Suite. Use the following test cases to assess the schemes. For each test, report the two real numbers $E_{\\mathrm{cap}}$ and $E_{\\mathrm{naive}}$ as specified above.\n- Test $1$: $\\alpha=10^{-2}$, $u_0=1.3$, $T=1$, $N=200$, $f(t)=\\sin(t)$, with the argument in radians.\n- Test $2$: $\\alpha=10^{-6}$, $u_0=-0.7$, $T=1$, $N=200$, $f(t)=\\mathrm{e}^{-t}$.\n- Test $3$: $\\alpha=10^{-3}$, $u_0=0$, $T=1$, $N=20$, $f(t)=t^2$.\n- Test $4$: $\\alpha=5\\times 10^{-2}$, $u_0=2$, $T=2$, $N=400$, $f(t)=\\cos(3t)$, with the argument in radians.\n\nFinal Output Format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order\n$\n[E_{\\mathrm{cap}}^{(1)},E_{\\mathrm{naive}}^{(1)},E_{\\mathrm{cap}}^{(2)},E_{\\mathrm{naive}}^{(2)},E_{\\mathrm{cap}}^{(3)},E_{\\mathrm{naive}}^{(3)},E_{\\mathrm{cap}}^{(4)},E_{\\mathrm{naive}}^{(4)}],\n$\nwhere the superscript indicates the test number. All outputs must be real numbers (floating-point). No other text should be printed.", "solution": "The objective is to numerically solve the initial value problem\n$$\nD_t^\\alpha u(t) = f(t), \\quad t\\in(0,T], \\qquad u(0)=u_0,\n$$\non a uniform grid $t_n = n\\tau$ for $n=0, 1, \\dots, N$, with step size $\\tau = T/N$. We denote the numerical approximation of $u(t_n)$ as $u^n$ and $f(t_n)$ as $f_n$.\n\nThe core of the method is the discretization of the Caputo derivative operator $D_t^\\alpha$ using Convolution Quadrature (CQ) based on the backward Euler method. The Laplace symbol of the continuous operator $D_t^\\alpha$ is $s^\\alpha$. For the backward Euler method, the first derivative is approximated by $\\frac{v(t) - v(t-\\tau)}{\\tau}$, which in the $z$-transform domain corresponds to replacing the differentiation operator $\\partial_t$ with $\\delta(z)/\\tau = (1-z^{-1})/\\tau$. We will work with the generating function $\\delta(\\zeta)=1-\\zeta$. The discrete operator corresponding to $s^\\alpha$ is thus $(\\delta(\\zeta)/\\tau)^\\alpha = \\tau^{-\\alpha}(1-\\zeta)^\\alpha$.\n\nThe generating function for the discrete convolution weights, $(1-\\zeta)^\\alpha$, is expanded using the generalized binomial theorem:\n$$\n(1-\\zeta)^\\alpha = \\sum_{j=0}^\\infty \\binom{\\alpha}{j} (-\\zeta)^j =: \\sum_{j=0}^\\infty g_j \\zeta^j.\n$$\nThe convolution weights are $g_j = (-1)^j \\binom{\\alpha}{j}$. These can be computed efficiently via the recurrence relation:\n$$\ng_0 = 1, \\qquad g_j = \\left(1 - \\frac{\\alpha+1}{j}\\right) g_{j-1} \\quad \\text{for } j \\ge 1.\n$$\nA discrete fractional derivative of a time-indexed sequence $\\{v^k\\}_{k=0}^N$, denoted $(\\partial_\\tau^\\alpha v)^n$, is then defined as the discrete convolution:\n$$\n(\\partial_\\tau^\\alpha v)^n := \\tau^{-\\alpha} \\sum_{j=0}^n g_j v^{n-j}.\n$$\nThe crucial aspect, as highlighted in the problem, is how to handle the initial condition $u(0) = u_0$. This leads to two different schemes.\n\n**Scheme 1: Caputo-Consistent Scheme**\n\nThis scheme incorporates the \"initial-value correction.\" The motivation is the identity $D_t^\\alpha u(t) = D_t^\\alpha (u(t) - u_0)$, which holds because the derivative of a constant is zero. We apply the discrete fractional operator to the function $v(t) = u(t) - u_0$, which has a zero initial value, $v(0)=0$. The discrete sequence is $v^n = u^n - u_0$, with $v^0 = u^0 - u_0 = 0$.\n\nThe discretized equation at $t=t_n$ is $(\\partial_\\tau^\\alpha v)^n = f_n$, which expands to:\n$$\n\\tau^{-\\alpha} \\sum_{j=0}^n g_j v^{n-j} = f_n \\implies \\tau^{-\\alpha} \\sum_{j=0}^n g_j (u^{n-j} - u_0) = f_n.\n$$\nRearranging the terms to solve for $u^n$:\n$$\n\\sum_{j=0}^n g_j u^{n-j} - u_0 \\sum_{j=0}^n g_j = \\tau^\\alpha f_n.\n$$\nIsolating the $j=0$ term from the first sum (since $g_0=1$):\n$$\ng_0 u^n + \\sum_{j=1}^n g_j u^{n-j} = \\tau^\\alpha f_n + u_0 \\sum_{j=0}^n g_j.\n$$\n$$\nu^n = \\tau^\\alpha f_n + u_0 \\sum_{j=0}^n g_j - \\sum_{j=1}^n g_j u^{n-j}.\n$$\nThis defines the time-stepping rule for the Caputo-consistent scheme, $u^n_{\\mathrm{cap}}$.\nAs $\\alpha \\to 0^+$, we have $g_j \\to 0$ for $j \\ge 1$ and $\\tau^\\alpha \\to 1$. Heuristically, $\\sum_{j=0}^n g_j \\to 1$. The scheme becomes:\n$$\nu^n_{\\mathrm{cap}} \\to 1 \\cdot f_n + u_0 \\cdot 1 - \\sum_{j=1}^n (0) \\cdot u^{n-j}_{\\mathrm{cap}} = u_0 + f_n.\n$$\nThis correctly captures the desired limiting behavior.\n\n**Scheme 2: Naive Scheme**\n\nThis scheme omits the initial-value correction and applies the discrete operator directly to the sequence $\\{u^n\\}$, which does not start at zero ($u^0=u_0$). The discretized equation is $(\\partial_\\tau^\\alpha u)^n = f_n$:\n$$\n\\tau^{-\\alpha} \\sum_{j=0}^n g_j u^{n-j} = f_n.\n$$\nIsolating the $j=0$ term to solve for $u^n$:\n$$\ng_0 u^n + \\sum_{j=1}^n g_j u^{n-j} = \\tau^\\alpha f_n.\n$$\n$$\nu^n = \\tau^\\alpha f_n - \\sum_{j=1}^n g_j u^{n-j}.\n$$\nThis defines the time-stepping rule for the naive scheme, $u^n_{\\mathrm{naive}}$. As $\\alpha \\to 0^+$, the update rule simplifies to:\n$$\nu^n_{\\mathrm{naive}} \\to 1 \\cdot f_n - \\sum_{j=1}^n (0) \\cdot u^{n-j}_{\\mathrm{naive}} = f_n.\n$$\nThis scheme fails to recover the initial condition contribution, tending to $f_n$ instead of $u_0 + f_n$.\n\nFor implementation, the two schemes are:\n- **Caputo-Consistent Scheme:** $u^n_{\\mathrm{cap}} = \\tau^\\alpha f_n + u_0 S_n - \\sum_{j=1}^n g_j u^{n-j}_{\\mathrm{cap}}$, where $S_n = \\sum_{j=0}^n g_j$.\n- **Naive Scheme:** $u^n_{\\mathrm{naive}} = \\tau^\\alpha f_n - \\sum_{j=1}^n g_j u^{n-j}_{\\mathrm{naive}}$.\nThe implementation will pre-compute the weights $g_j$ and their cumulative sums $S_n$, then proceed with the time-stepping loop. Finally, the errors $E_{\\mathrm{cap}}$ and $E_{\\mathrm{naive}}$ are computed by comparing the numerical solutions against the target limiting solution $u_0 + f(t_n)$ for $n \\ge 1$.", "answer": "```python\nimport numpy as np\n\ndef run_simulation(alpha, u0, T, N, f_func):\n    \"\"\"\n    Solves the fractional ODE using two schemes and computes their error.\n\n    Args:\n        alpha (float): The order of the fractional derivative.\n        u0 (float): The initial condition u(0).\n        T (float): The final time.\n        N (int): The number of time steps.\n        f_func (callable): The source function f(t).\n\n    Returns:\n        tuple[float, float]: A tuple containing E_cap and E_naive.\n    \"\"\"\n    # 1. Setup grid and parameters\n    tau = T / N\n    t = np.linspace(0, T, N + 1)\n    f_vals = f_func(t)\n\n    # 2. Precompute convolution weights g_j and their cumulative sum S_j\n    # The weights g_j are related to the coefficients of (1-zeta)^alpha.\n    # g_j = (-1)^j * binom(alpha, j)\n    # The recurrence g_j = g_{j-1} * (1 - (alpha + 1) / j) is used.\n    g = np.zeros(N + 1)\n    g[0] = 1.0\n    for j in range(1, N + 1):\n        g[j] = g[j - 1] * (1 - (alpha + 1) / j)\n\n    S = np.cumsum(g)\n\n    # 3. Initialize solution arrays\n    u_cap = np.zeros(N + 1)\n    u_naive = np.zeros(N + 1)\n    u_cap[0] = u0\n    u_naive[0] = u0\n\n    tau_alpha = tau**alpha\n\n    # 4. Time-stepping loop\n    for n in range(1, N + 1):\n        # Convolution sum: sum_{j=1 to n} g_j * u_{n-j}\n        # u_cap[n-1::-1] is (u_{n-1}, u_{n-2}, ..., u_0)\n        # g[1:n+1] is (g_1, g_2, ..., g_n)\n        conv_cap = np.dot(g[1:n + 1], u_cap[n - 1::-1])\n        conv_naive = np.dot(g[1:n + 1], u_naive[n - 1::-1])\n\n        # Caputo-consistent scheme update\n        u_cap[n] = tau_alpha * f_vals[n] + u0 * S[n] - conv_cap\n\n        # Naive scheme update\n        u_naive[n] = tau_alpha * f_vals[n] - conv_naive\n\n    # 5. Calculate errors against the limiting target solution u0 + f(t)\n    target_solution = u0 + f_vals\n    \n    # Error is max absolute difference for t_n > 0 (i.e., n >= 1)\n    E_cap = np.max(np.abs(u_cap[1:] - target_solution[1:]))\n    E_naive = np.max(np.abs(u_naive[1:] - target_solution[1:]))\n    \n    return E_cap, E_naive\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    test_cases = [\n        {'alpha': 1e-2, 'u0': 1.3, 'T': 1, 'N': 200, 'f': lambda t: np.sin(t)},\n        {'alpha': 1e-6, 'u0': -0.7, 'T': 1, 'N': 200, 'f': lambda t: np.exp(-t)},\n        {'alpha': 1e-3, 'u0': 0.0, 'T': 1, 'N': 20, 'f': lambda t: t**2},\n        {'alpha': 5e-2, 'u0': 2.0, 'T': 2, 'N': 400, 'f': lambda t: np.cos(3*t)},\n    ]\n\n    results = []\n    for case in test_cases:\n        E_cap, E_naive = run_simulation(case['alpha'], case['u0'], case['T'], case['N'], case['f'])\n        results.append(E_cap)\n        results.append(E_naive)\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3426234"}, {"introduction": "Moving from the temporal to the spatial domain, this practice addresses the numerical solution of equations involving the fractional Laplacian, $(-\\Delta)^s$. We will employ the method of manufactured solutions, a fundamental technique for verifying the correctness and accuracy of a numerical solver. This hands-on exercise [@problem_id:3426256] will guide you through implementing a spectral finite difference method, computing its convergence rate, and confirming that your code works as theoretically predicted.", "problem": "Consider the one-dimensional bounded domain $(0,1)$ with homogeneous Dirichlet boundary conditions. Let $-\\Delta$ denote the classical Laplacian operator associated with the second derivative, and let $(-\\Delta)^s$ for $s \\in (0,1)$ denote the spectral fractional Laplacian defined on $(0,1)$ by functional calculus with respect to the eigenbasis of $-\\Delta$ under Dirichlet boundary conditions. The manufactured solution approach constructs an exact solution with a known right-hand side to validate a numerical method. In this problem, choose the manufactured solution $u(x) = \\sin(m\\pi x)$ for a positive integer $m$ and define $f(x)$ so that the fractional Poisson problem\n$$\n(-\\Delta)^s u(x) = f(x), \\quad x \\in (0,1), \\quad u(0)=u(1)=0,\n$$\nis satisfied exactly. You may assume the spectral definition of $(-\\Delta)^s$ on $(0,1)$ with homogeneous Dirichlet boundary conditions, based on the eigenfunctions and eigenvalues of the classical Dirichlet Laplacian. Angles in trigonometric functions must be interpreted in radians.\n\nStarting from the following fundamental base: \n- The classical Dirichlet Laplacian $-\\Delta$ on $(0,1)$ has a complete orthonormal set of eigenfunctions with real, positive eigenvalues.\n- The spectral fractional Laplacian $(-\\Delta)^s$ is defined on $(0,1)$ by applying the fractional power $s$ to the eigenvalues of $-\\Delta$ in its spectral decomposition.\n- The Finite Difference Method (FDM) for the classical Dirichlet Laplacian on a uniform grid yields a symmetric positive definite matrix that approximates the operator.\n\nDesign a numerical method that:\n- Constructs the discrete Dirichlet Laplacian matrix on a uniform grid with $N$ interior points using a standard second-order centered FDM.\n- Computes the spectral fractional power of this matrix via its eigen-decomposition.\n- Solves the discrete fractional Poisson problem for the manufactured right-hand side $f(x)$ obtained from the chosen $u(x)$ and the spectral definition of $(-\\Delta)^s$.\n- Evaluates the numerical solution $u_h$ at the interior grid points and compares it against the exact solution samples $u(x_i)$ at the same grid points.\n- Uses the discrete $L^2$ error defined by\n$$\nE(N) = \\left( h \\sum_{i=1}^{N} \\left| u_h(x_i) - u(x_i) \\right|^2 \\right)^{1/2},\n$$\nwhere $h = \\frac{1}{N+1}$ and $x_i = i h$ are the interior grid points.\n\nEstimate the observed convergence rate under grid refinement by computing, for two successive refinements $N \\to 2N$, the quantity\n$$\n\\rho(N \\to 2N) = \\frac{\\log\\left(E(N)/E(2N)\\right)}{\\log(2)}.\n$$\nFor each test case, report a single float equal to the mean of the two successive refinement rates $\\rho(N_1 \\to N_2)$ and $\\rho(N_2 \\to N_3)$ over three grid sizes $N_1 < N_2 < N_3$.\n\nTest suite:\n- Case $1$: $s = 0.25$, $m = 1$, $N_1 = 50$, $N_2 = 100$, $N_3 = 200$.\n- Case $2$: $s = 0.50$, $m = 1$, $N_1 = 50$, $N_2 = 100$, $N_3 = 200$.\n- Case $3$: $s = 0.75$, $m = 3$, $N_1 = 50$, $N_2 = 100$, $N_3 = 200$.\n- Case $4$: $s = 0.99$, $m = 1$, $N_1 = 50$, $N_2 = 100$, $N_3 = 200$.\n\nAll angles in $\\sin$ are in radians. No physical units are involved. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, $[r_1,r_2,r_3,r_4]$, where each $r_k$ is the mean of the two successive refinement rates for the $k$-th test case. Each list element must be a float.", "solution": "### 1. Mathematical Formulation\n\nThe problem under consideration is the fractional Poisson equation on the domain $\\Omega = (0,1)$ with homogeneous Dirichlet boundary conditions:\n$$\n\\begin{cases}\n(-\\Delta)^s u(x) = f(x), & x \\in (0,1) \\\\\nu(0) = u(1) = 0\n\\end{cases}\n$$\nwhere $s \\in (0,1)$ is the fractional order. The operator $(-\\Delta)^s$ is the spectral fractional Laplacian.\n\nThe classical Laplacian operator $-\\Delta = -d^2/dx^2$ on $(0,1)$ with homogeneous Dirichlet boundary conditions has a set of known eigenvalues $\\lambda_k$ and corresponding orthonormal eigenfunctions $\\phi_k(x)$:\n$$\n\\lambda_k = (k\\pi)^2, \\quad \\phi_k(x) = \\sqrt{2}\\sin(k\\pi x), \\quad k = 1, 2, 3, \\dots\n$$\nThe spectral fractional Laplacian is defined by its action on the eigenfunction expansion of a function $g(x) = \\sum_{k=1}^{\\infty} c_k \\phi_k(x)$:\n$$\n(-\\Delta)^s g(x) = \\sum_{k=1}^{\\infty} c_k \\lambda_k^s \\phi_k(x)\n$$\nThe problem specifies a manufactured solution $u(x) = \\sin(m\\pi x)$. This is proportional to the $m$-th eigenfunction $\\phi_m(x)$. Applying the fractional operator to $u(x)$, we find the corresponding right-hand side function $f(x)$:\n$$\nf(x) = (-\\Delta)^s u(x) = (m\\pi)^{2s} \\sin(m\\pi x)\n$$\n\n### 2. Numerical Discretization\n\nThe domain $(0,1)$ is discretized with $N$ interior points $x_i = ih$ for $i=1,\\dots,N$, where $h = 1/(N+1)$. The classical Laplacian is approximated using a second-order centered finite difference scheme, resulting in an $N \\times N$ matrix system. The discrete Laplacian operator, $A_h$, is a symmetric positive definite matrix:\n$$\nA_h = \\frac{1}{h^2}\n\\begin{pmatrix}\n2 & -1 & 0 & \\dots & 0 \\\\\n-1 & 2 & -1 & \\dots & 0 \\\\\n0 & \\ddots & \\ddots & \\ddots & 0 \\\\\n\\vdots & & -1 & 2 & -1 \\\\\n0 & \\dots & 0 & -1 & 2\n\\end{pmatrix}\n$$\n\n### 3. Solving the Discrete Fractional System\n\nThe numerical method mimics the continuous spectral definition. We compute the eigendecomposition of $A_h$ as $A_h = V \\Lambda V^T$. The discrete fractional Laplacian $A_h^s$ is then $A_h^s = V \\Lambda^s V^T$.\n\nThe discrete form of the problem is the linear system $A_h^s u_h = f_h$, where $f_h$ is the vector of $f(x)$ evaluated at the grid points. The solution $u_h$ is given by:\n$$\nu_h = (A_h^s)^{-1} f_h = V \\Lambda^{-s} V^T f_h\n$$\nThe algorithm is:\n1. Construct the matrix $A_h$.\n2. Compute its eigenvalues $\\Lambda$ and eigenvectors $V$.\n3. Construct the vector $f_h$.\n4. Transform $f_h$ into the eigenbasis: $\\hat{f}_h = V^T f_h$.\n5. Scale the components: $\\hat{u}_h(k) = \\mu_k^{-s} \\hat{f}_h(k)$.\n6. Transform back to the standard basis: $u_h = V \\hat{u}_h$.\n\n### 4. Error Analysis and Convergence Rate\n\nThe numerical solution $u_h$ is compared with the exact solution $u(x)$ sampled at the grid points, $u_{exact}$. The error is measured using the discrete $L^2$ norm:\n$$\nE(N) = \\left( h \\sum_{i=1}^{N} | u_h(x_i) - u(x_i) |^2 \\right)^{1/2} = \\sqrt{h} \\|u_h - u_{exact}\\|_2\n$$\nThe order of convergence, $\\rho$, is estimated using two grid sizes $N$ and $2N$:\n$$\n\\rho(N \\to 2N) = \\frac{\\log(E(N)/E(2N))}{\\log(2)}\n$$\nThe final result is the mean of two such rates from three successive grid refinements. The expected convergence rate is $\\rho=2$, as the dominant error stems from the second-order finite difference approximation of the Laplacian.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import eigh\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print the results.\n    \"\"\"\n    test_cases = [\n        # (s, m, (N1, N2, N3))\n        {'s': 0.25, 'm': 1, 'N_grid': (50, 100, 200)},\n        {'s': 0.50, 'm': 1, 'N_grid': (50, 100, 200)},\n        {'s': 0.75, 'm': 3, 'N_grid': (50, 100, 200)},\n        {'s': 0.99, 'm': 1, 'N_grid': (50, 100, 200)},\n    ]\n\n    results = []\n    for case in test_cases:\n        mean_rate = compute_mean_convergence_rate(case['s'], case['m'], case['N_grid'])\n        results.append(mean_rate)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef compute_mean_convergence_rate(s, m, N_grid_sizes):\n    \"\"\"\n    Computes the mean convergence rate for a given set of parameters.\n\n    Args:\n        s (float): The fractional order of the Laplacian.\n        m (int): The mode number for the manufactured solution.\n        N_grid_sizes (tuple): A tuple of three integers (N1, N2, N3) for grid sizes.\n\n    Returns:\n        float: The mean of two successive convergence rates.\n    \"\"\"\n    errors = []\n    for N in N_grid_sizes:\n        h = 1.0 / (N + 1)\n        \n        # 1. Define interior grid points\n        x = np.linspace(h, 1.0 - h, N)\n        \n        # 2. Construct the discrete Laplacian matrix A_h\n        main_diag = np.full(N, 2.0)\n        off_diag = np.full(N - 1, -1.0)\n        A_h = (np.diag(main_diag) + np.diag(off_diag, k=1) + np.diag(off_diag, k=-1)) / (h**2)\n        \n        # 3. Compute the eigendecomposition of A_h.\n        # eigh is used for symmetric matrices.\n        eigenvalues, eigenvectors = eigh(A_h)\n        \n        # 4. Construct the right-hand side vector f_h\n        # f(x) = (m*pi)^(2s) * sin(m*pi*x)\n        f_h = (m * np.pi)**(2 * s) * np.sin(m * np.pi * x)\n        \n        # 5. Solve the linear system A_h^s * u_h = f_h using the spectral method\n        # u_h = V * Lambda^(-s) * V^T * f_h\n        # Step 5.1: Transform f_h to the eigenbasis\n        f_hat = eigenvectors.T @ f_h\n        # Step 5.2: Scale by Lambda^(-s)\n        u_hat = f_hat * (eigenvalues**(-s))\n        # Step 5.3: Transform back to the standard basis\n        u_h = eigenvectors @ u_hat\n        \n        # 6. Compute the exact solution vector u_exact\n        u_exact = np.sin(m * np.pi * x)\n        \n        # 7. Calculate the discrete L2 error E(N)\n        error_norm_sq = np.sum((u_h - u_exact)**2)\n        error_L2 = np.sqrt(h * error_norm_sq)\n        errors.append(error_L2)\n        \n    # 8. Calculate convergence rates\n    E1, E2, E3 = errors\n    \n    # Check for grid refinement ratio.\n    grid_ratio1 = N_grid_sizes[1] / N_grid_sizes[0]\n    grid_ratio2 = N_grid_sizes[2] / N_grid_sizes[1]\n\n    rate1 = np.log(E1 / E2) / np.log(grid_ratio1)\n    rate2 = np.log(E2 / E3) / np.log(grid_ratio2)\n    \n    mean_rate = (rate1 + rate2) / 2.0\n    \n    return mean_rate\n\nsolve()\n\n```", "id": "3426256"}]}