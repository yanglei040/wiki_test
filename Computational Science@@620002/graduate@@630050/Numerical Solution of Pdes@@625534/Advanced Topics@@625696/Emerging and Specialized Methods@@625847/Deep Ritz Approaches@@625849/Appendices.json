{"hands_on_practices": [{"introduction": "The Deep Ritz method is founded on the principle of minimum potential energy, transforming a partial differential equation problem into an optimization task. Before employing complex neural networks, it is instructive to ground our understanding in a simple, analytically solvable case. This first practice [@problem_id:3376700] invites you to derive the energy functional for a 1D Poisson equation and find the exact minimum energy for a simple linear trial function, making the abstract concept of variational minimization tangible.", "problem": "Consider the one-dimensional Poisson partial differential equation (PDE) with homogeneous Dirichlet boundary conditions on the unit interval: find $u:(0,1)\\to\\mathbb{R}$ such that\n$$\n- u''(x) = 1 \\quad \\text{for } x\\in (0,1), \\qquad u(0)=0, \\quad u(1)=0.\n$$\nStarting from the weak formulation derived by integration by parts and the principle of minimum potential energy, formulate the energy functional whose minimizer in the Sobolev space $H_0^1(0,1)$ coincides with the weak solution of the PDE. Then, instantiate the Deep Ritz approach by enforcing the boundary conditions through a multiplicative ansatz\n$$\nu_{\\theta}(x) = \\phi(x)\\,N_{\\theta}(x),\n$$\nwhere $\\phi(x)=x(1-x)$ and $N_{\\theta}(x)=a x + b$ with trainable parameters $\\theta=(a,b)\\in\\mathbb{R}^2$. Using exact (not sampled) integration on $(0,1)$, write the energy as a function of $(a,b)$ implied by this variational formulation and compute its minimum value over all $(a,b)\\in\\mathbb{R}^2$. Express the final minimal energy as an exact value with no rounding.", "solution": "The user wants me to critically validate the problem statement before attempting to solve it.\n\n### Step 1: Extract Givens\n- **PDE:** $- u''(x) = 1$ for $x \\in (0,1)$.\n- **Domain:** $\\Omega = (0,1)$.\n- **Boundary Conditions:** $u(0)=0$, $u(1)=0$ (homogeneous Dirichlet).\n- **Function Space:** The solution $u$ is sought in the Sobolev space $H_0^1(0,1)$.\n- **Method:** The problem is to be solved using a variational approach based on the principle of minimum potential energy, specifically the Deep Ritz method.\n- **Ansatz:** The trial function is of the form $u_{\\theta}(x) = \\phi(x)\\,N_{\\theta}(x)$, where $\\theta = (a,b)$ are trainable parameters.\n- **Boundary Enforcement Function:** $\\phi(x) = x(1-x)$.\n- **\"Neural Network\" Approximation:** $N_{\\theta}(x) = a x + b$.\n- **Task:**\n    1. Formulate the energy functional $J(u)$ corresponding to the PDE.\n    2. Substitute the ansatz $u_{\\theta}(x)$ into $J(u)$ to obtain an energy function $E(a,b) = J(u_{\\theta})$.\n    3. Use exact integration to express $E(a,b)$ as a function of $a$ and $b$.\n    4. Find the minimum value of $E(a,b)$ over all $(a,b) \\in \\mathbb{R}^2$.\n    5. The final answer must be an exact value.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded:** The problem is based on the Poisson equation, the principle of minimum potential energy, and the Deep Ritz method, all of which are standard and well-established concepts in applied mathematics and physics.\n- **Well-Posed:** The problem of minimizing a quadratic function of two variables is well-posed. The underlying PDE with Dirichlet boundary conditions is also a classic well-posed problem.\n- **Objective:** The problem is stated using precise mathematical language, with no subjective or ambiguous terms.\n- **Additional Checks:** The problem is self-contained, with all necessary information provided. There are no contradictions. The simplified linear \"network\" makes the problem analytically tractable as requested, and a unique minimum is expected to exist.\n\n### Step 3: Verdict and Action\nThe problem is valid. The solution process will now commence.\n\nThe Poisson equation is given by $-u''(x) = f(x)$ with $f(x)=1$. The principle of minimum potential energy states that the solution to this PDE is the function $u$ in the appropriate space ($H_0^1(0,1)$ in this case) that minimizes the energy functional $J(u)$. The functional is given by\n$$\nJ(u) = \\frac{1}{2} \\int_{0}^{1} (u'(x))^2 \\, dx - \\int_{0}^{1} f(x)u(x) \\, dx\n$$\nFor $f(x)=1$, this becomes\n$$\nJ(u) = \\frac{1}{2} \\int_{0}^{1} (u'(x))^2 \\, dx - \\int_{0}^{1} u(x) \\, dx\n$$\nThe Deep Ritz method seeks to minimize this functional over a family of trial functions parameterized by $\\theta$. The given trial function is $u_{\\theta}(x)$ with parameters $\\theta=(a,b)$:\n$$\nu_{a,b}(x) = \\phi(x) N_{\\theta}(x) = (x(1-x))(ax+b)\n$$\nThis form enforces the boundary conditions $u_{a,b}(0)=0$ and $u_{a,b}(1)=0$ for any choice of $a$ and $b$. We expand $u_{a,b}(x)$ for easier differentiation and integration:\n$$\nu_{a,b}(x) = (x-x^2)(ax+b) = ax^2 + bx - ax^3 - bx^2 = -ax^3 + (a-b)x^2 + bx\n$$\nNext, we compute the derivative $u'_{a,b}(x)$:\n$$\nu'_{a,b}(x) = \\frac{d}{dx} \\left( -ax^3 + (a-b)x^2 + bx \\right) = -3ax^2 + 2(a-b)x + b\n$$\nNow we must compute the two integrals in the energy functional $E(a,b) = J(u_{a,b})$.\n\nFirst integral (linear term):\n$$\n\\int_{0}^{1} u_{a,b}(x) \\, dx = \\int_{0}^{1} (-ax^3 + (a-b)x^2 + bx) \\, dx\n$$\n$$\n= \\left[ -a\\frac{x^4}{4} + (a-b)\\frac{x^3}{3} + b\\frac{x^2}{2} \\right]_{0}^{1} = -\\frac{a}{4} + \\frac{a-b}{3} + \\frac{b}{2}\n$$\n$$\n= \\frac{-3a + 4(a-b) + 6b}{12} = \\frac{-3a + 4a - 4b + 6b}{12} = \\frac{a+2b}{12}\n$$\n\nSecond integral (quadratic term):\n$$\n\\int_{0}^{1} (u'_{a,b}(x))^2 \\, dx = \\int_{0}^{1} (-3ax^2 + 2(a-b)x + b)^2 \\, dx\n$$\nWe expand the integrand:\n$$\n(-3ax^2 + 2(a-b)x + b)^2 = 9a^2x^4 + 4(a-b)^2x^2 + b^2 - 12a(a-b)x^3 - 6abx^2 + 4b(a-b)x\n$$\n$$\n= 9a^2x^4 - 12a(a-b)x^3 + [4(a-b)^2 - 6ab]x^2 + 4b(a-b)x + b^2\n$$\nIntegrating term by term from $x=0$ to $x=1$:\n$$\n\\int_{0}^{1} (u'_{a,b}(x))^2 \\, dx = \\left[ \\frac{9a^2x^5}{5} - \\frac{12a(a-b)x^4}{4} + \\frac{4(a-b)^2 - 6ab}{3}x^3 + \\frac{4b(a-b)x^2}{2} + b^2x \\right]_{0}^{1}\n$$\n$$\n= \\frac{9a^2}{5} - 3a(a-b) + \\frac{4(a^2-2ab+b^2) - 6ab}{3} + 2b(a-b) + b^2\n$$\n$$\n= \\frac{9a^2}{5} - 3a^2 + 3ab + \\frac{4a^2-8ab-6ab+4b^2}{3} + 2ab - 2b^2 + b^2\n$$\n$$\n= \\frac{9a^2}{5} - 3a^2 + 5ab - b^2 + \\frac{4a^2 - 14ab + 4b^2}{3}\n$$\nPutting everything over a common denominator of $15$:\n$$\n= \\frac{3(9a^2) - 15(3a^2) + 15(5ab) - 15b^2 + 5(4a^2 - 14ab + 4b^2)}{15}\n$$\n$$\n= \\frac{27a^2 - 45a^2 + 75ab - 15b^2 + 20a^2 - 70ab + 20b^2}{15}\n$$\n$$\n= \\frac{(27-45+20)a^2 + (75-70)ab + (-15+20)b^2}{15} = \\frac{2a^2 + 5ab + 5b^2}{15}\n$$\nNow, we assemble the energy functional $E(a,b)$:\n$$\nE(a,b) = \\frac{1}{2} \\left( \\frac{2a^2 + 5ab + 5b^2}{15} \\right) - \\left( \\frac{a+2b}{12} \\right) = \\frac{2a^2 + 5ab + 5b^2}{30} - \\frac{a+2b}{12}\n$$\nTo find the minimum, we compute the partial derivatives with respect to $a$ and $b$ and set them to zero.\n$$\n\\frac{\\partial E}{\\partial a} = \\frac{4a+5b}{30} - \\frac{1}{12} = 0\n$$\n$$\n\\frac{\\partial E}{\\partial b} = \\frac{5a+10b}{30} - \\frac{2}{12} = 0\n$$\nThis gives a system of linear equations:\n1. $12(4a+5b) = 30 \\implies 48a + 60b = 30 \\implies 8a + 10b = 5$\n2. $\\frac{5a+10b}{30} = \\frac{1}{6} \\implies 6(5a+10b) = 30 \\implies 30a + 60b = 30 \\implies a + 2b = 1$\n\nFrom the second equation, we have $a=1-2b$. Substituting this into the first equation:\n$$\n8(1-2b) + 10b = 5\n$$\n$$\n8 - 16b + 10b = 5 \\implies 8 - 6b = 5 \\implies 3 = 6b \\implies b = \\frac{1}{2}\n$$\nSubstituting $b=\\frac{1}{2}$ back into $a=1-2b$:\n$$\na = 1 - 2\\left(\\frac{1}{2}\\right) = 1-1=0\n$$\nThe energy is minimized at $(a,b) = (0, \\frac{1}{2})$. Finally, we compute the minimum energy value by substituting these parameters into $E(a,b)$:\n$$\nE\\left(0, \\frac{1}{2}\\right) = \\frac{2(0)^2 + 5(0)(\\frac{1}{2}) + 5(\\frac{1}{2})^2}{30} - \\frac{0+2(\\frac{1}{2})}{12}\n$$\n$$\n= \\frac{5(\\frac{1}{4})}{30} - \\frac{1}{12} = \\frac{\\frac{5}{4}}{30} - \\frac{1}{12} = \\frac{5}{120} - \\frac{1}{12}\n$$\n$$\n= \\frac{1}{24} - \\frac{2}{24} = -\\frac{1}{24}\n$$\nThe minimal energy is $-\\frac{1}{24}$.", "answer": "$$ \\boxed{-\\frac{1}{24}} $$", "id": "3376700"}, {"introduction": "A crucial design choice in variational PDE solvers is how to enforce boundary conditions. While the previous exercise used a \"hard\" constraint by construction, a more flexible and common approach is to use a \"soft\" penalty term in the loss function. This practice [@problem_id:3376716] guides you through the calculus of variations to derive the governing equations and \"natural\" boundary conditions that emerge from such a penalty formulation, revealing the trade-offs involved and demonstrating how the solution behaves as the penalty strength varies.", "problem": "Consider the Deep Ritz approach for the numerical solution of the Poisson Partial Differential Equation (PDE) on the one-dimensional domain $\\Omega=(0,1)$ with homogeneous Dirichlet boundary conditions, where enforcement is performed softly via a boundary penalty. The Deep Ritz method parameterizes the trial function $u$ by an Artificial Neural Network (ANN) and minimizes a Ritz functional over the admissible space. In the continuous limit of infinite model capacity, the exact minimizer $u_{\\alpha}$ of the penalized energy can be studied directly via the calculus of variations.\n\nLet the penalized Ritz functional be\n$$\n\\mathcal{J}_{\\alpha}(u) \\;=\\; \\int_{0}^{1} \\left( \\frac{1}{2}\\,|u'(x)|^{2} \\;-\\; f(x)\\,u(x) \\right)\\,\\mathrm{d}x \\;+\\; \\alpha\\left( |u(0)|^{2} \\,+\\, |u(1)|^{2} \\right),\n$$\nwith $f(x)=1$ and penalty parameter $\\alpha>0$. The unconstrained minimization is taken over $H^{1}(0,1)$, without imposing $u(0)=0$ or $u(1)=0$ as hard constraints.\n\nStarting from the foundational principle that stationary points of $\\mathcal{J}_{\\alpha}$ satisfy the Euler–Lagrange condition with appropriate boundary terms induced by the penalty, derive the governing ordinary differential equation and boundary conditions for $u_{\\alpha}$ using first principles of the calculus of variations. Then solve the resulting boundary value problem explicitly and determine the exact value of $u_{\\alpha}(0)$ as a closed-form expression in terms of $\\alpha$.\n\nProvide your final answer as a single closed-form analytic expression. No rounding is required and no units are involved.", "solution": "The user has provided a well-defined problem in the calculus of variations, arising from the continuous limit of a Deep Ritz method for a one-dimensional Poisson equation. The problem is scientifically grounded, well-posed, objective, and contains all necessary information for a unique solution. Therefore, the problem is valid.\n\nThe task is to find the minimizer $u_{\\alpha}(x)$ of the penalized Ritz functional\n$$\n\\mathcal{J}_{\\alpha}(u) = \\int_{0}^{1} \\left( \\frac{1}{2}\\,|u'(x)|^{2} - f(x)\\,u(x) \\right)\\,\\mathrm{d}x + \\alpha\\left( |u(0)|^{2} + |u(1)|^{2} \\right)\n$$\nwhere the function space for $u$ is $H^{1}(0,1)$, $f(x)=1$, and $\\alpha>0$ is a penalty parameter. We must first derive the governing differential equation and boundary conditions and then solve for $u_{\\alpha}(x)$ to find the value of $u_{\\alpha}(0)$.\n\nThe minimizer $u_{\\alpha}$ of the functional $\\mathcal{J}_{\\alpha}$ is a stationary point. To find this stationary point, we use the calculus of variations. We consider a variation of $u_{\\alpha}$ of the form $u(x) = u_{\\alpha}(x) + \\epsilon \\eta(x)$, where $\\eta(x) \\in H^{1}(0,1)$ is an arbitrary test function and $\\epsilon$ is a small real parameter. The condition for $u_{\\alpha}$ being a stationary point is that the first variation of the functional vanishes, i.e.,\n$$\n\\frac{\\mathrm{d}}{\\mathrm{d}\\epsilon} \\mathcal{J}_{\\alpha}(u_{\\alpha} + \\epsilon \\eta) \\bigg|_{\\epsilon=0} = 0\n$$\nfor all admissible $\\eta$.\n\nLet's compute the derivative. Since the function $u$ is real-valued, we can write $|u|^{2} = u^{2}$. The functional becomes:\n$$\n\\mathcal{J}_{\\alpha}(u_{\\alpha} + \\epsilon \\eta) = \\int_{0}^{1} \\left( \\frac{1}{2}\\,(u'_{\\alpha} + \\epsilon \\eta')^{2} - f(x)\\,(u_{\\alpha} + \\epsilon \\eta) \\right)\\,\\mathrm{d}x + \\alpha\\left( (u_{\\alpha}(0) + \\epsilon \\eta(0))^{2} + (u_{\\alpha}(1) + \\epsilon \\eta(1))^{2} \\right)\n$$\nDifferentiating with respect to $\\epsilon$:\n$$\n\\frac{\\mathrm{d}}{\\mathrm{d}\\epsilon} \\mathcal{J}_{\\alpha}(u_{\\alpha} + \\epsilon \\eta) = \\int_{0}^{1} \\left( (u'_{\\alpha} + \\epsilon \\eta') \\eta' - f(x)\\eta \\right)\\,\\mathrm{d}x + \\alpha\\left( 2(u_{\\alpha}(0) + \\epsilon \\eta(0))\\eta(0) + 2(u_{\\alpha}(1) + \\epsilon \\eta(1))\\eta(1) \\right)\n$$\nSetting $\\epsilon=0$, we obtain the first-variation condition:\n$$\n\\int_{0}^{1} \\left( u'_{\\alpha}(x) \\eta'(x) - f(x) \\eta(x) \\right)\\,\\mathrm{d}x + 2\\alpha u_{\\alpha}(0) \\eta(0) + 2\\alpha u_{\\alpha}(1) \\eta(1) = 0\n$$\nTo find the strong form of the differential equation, we apply integration by parts to the first term in the integral:\n$$\n\\int_{0}^{1} u'_{\\alpha}(x) \\eta'(x) \\,\\mathrm{d}x = \\left[ u'_{\\alpha}(x) \\eta(x) \\right]_{0}^{1} - \\int_{0}^{1} u''_{\\alpha}(x) \\eta(x) \\,\\mathrm{d}x = u'_{\\alpha}(1)\\eta(1) - u'_{\\alpha}(0)\\eta(0) - \\int_{0}^{1} u''_{\\alpha}(x)\\eta(x) \\,\\mathrm{d}x\n$$\nSubstituting this back into the variational equation yields:\n$$\nu'_{\\alpha}(1)\\eta(1) - u'_{\\alpha}(0)\\eta(0) - \\int_{0}^{1} u''_{\\alpha}(x)\\eta(x) \\,\\mathrm{d}x - \\int_{0}^{1} f(x)\\eta(x) \\,\\mathrm{d}x + 2\\alpha u_{\\alpha}(0) \\eta(0) + 2\\alpha u_{\\alpha}(1) \\eta(1) = 0\n$$\nWe can rearrange the terms by grouping the integral and the boundary evaluations:\n$$\n\\int_{0}^{1} \\left( -u''_{\\alpha}(x) - f(x) \\right) \\eta(x) \\,\\mathrm{d}x + \\left( u'_{\\alpha}(1) + 2\\alpha u_{\\alpha}(1) \\right) \\eta(1) + \\left( -u'_{\\alpha}(0) + 2\\alpha u_{\\alpha}(0) \\right) \\eta(0) = 0\n$$\nThis equation must hold for any choice of test function $\\eta(x) \\in H^{1}(0,1)$. By the fundamental lemma of calculus of variations, if we first consider test functions that vanish at the boundaries ($\\eta(0)=0$ and $\\eta(1)=0$), the integral term must be zero. This gives the Euler-Lagrange equation for the interior of the domain:\n$$\n-u''_{\\alpha}(x) - f(x) = 0 \\quad \\text{for } x \\in (0,1)\n$$\nWith this ODE satisfied, the integral term is zero for all $\\eta$, and the variational equation reduces to the boundary terms:\n$$\n\\left( u'_{\\alpha}(1) + 2\\alpha u_{\\alpha}(1) \\right) \\eta(1) + \\left( -u'_{\\alpha}(0) + 2\\alpha u_{\\alpha}(0) \\right) \\eta(0) = 0\n$$\nSince this must hold for arbitrary values of $\\eta(0)$ and $\\eta(1)$, the coefficients of $\\eta(0)$ and $\\eta(1)$ must be zero independently. This yields the natural boundary conditions:\n$$\nu'_{\\alpha}(0) = 2\\alpha u_{\\alpha}(0)\n$$\n$$\nu'_{\\alpha}(1) = -2\\alpha u_{\\alpha}(1)\n$$\nWe now have a complete boundary value problem (BVP) for $u_{\\alpha}(x)$. Given $f(x)=1$, the BVP is:\n$$\n\\begin{cases}\n-u''_{\\alpha}(x) = 1 & \\text{for } x \\in (0,1) \\\\\nu'_{\\alpha}(0) = 2\\alpha u_{\\alpha}(0) \\\\\nu'_{\\alpha}(1) = -2\\alpha u_{\\alpha}(1)\n\\end{cases}\n$$\nWe proceed to solve this BVP. Integrating the ODE $u''_{\\alpha}(x) = -1$ twice with respect to $x$ gives the general solution:\n$$\nu'_{\\alpha}(x) = -x + C_1\n$$\n$$\nu_{\\alpha}(x) = -\\frac{1}{2}x^2 + C_1 x + C_2\n$$\nwhere $C_1$ and $C_2$ are constants of integration. We determine these constants using the boundary conditions.\nFrom the general solution, we have:\n$$\nu_{\\alpha}(0) = C_2 \\quad \\text{and} \\quad u'_{\\alpha}(0) = C_1\n$$\n$$\nu_{\\alpha}(1) = -\\frac{1}{2} + C_1 + C_2 \\quad \\text{and} \\quad u'_{\\alpha}(1) = -1 + C_1\n$$\nApplying the first boundary condition, $u'_{\\alpha}(0) = 2\\alpha u_{\\alpha}(0)$:\n$$\nC_1 = 2\\alpha C_2\n$$\nApplying the second boundary condition, $u'_{\\alpha}(1) = -2\\alpha u_{\\alpha}(1)$:\n$$\n-1 + C_1 = -2\\alpha \\left(-\\frac{1}{2} + C_1 + C_2\\right)\n$$\n$$\n-1 + C_1 = \\alpha - 2\\alpha C_1 - 2\\alpha C_2\n$$\n$$\n(1 + 2\\alpha)C_1 + 2\\alpha C_2 = 1 + \\alpha\n$$\nWe now have a system of two linear equations for $C_1$ and $C_2$:\n1. $C_1 - 2\\alpha C_2 = 0$\n2. $(1 + 2\\alpha)C_1 + 2\\alpha C_2 = 1 + \\alpha$\n\nSubstitute $C_1 = 2\\alpha C_2$ from the first equation into the second:\n$$\n(1 + 2\\alpha)(2\\alpha C_2) + 2\\alpha C_2 = 1 + \\alpha\n$$\n$$\n(2\\alpha + 4\\alpha^2) C_2 + 2\\alpha C_2 = 1 + \\alpha\n$$\n$$\n(4\\alpha + 4\\alpha^2) C_2 = 1 + \\alpha\n$$\n$$\n4\\alpha(1 + \\alpha) C_2 = 1 + \\alpha\n$$\nSince $\\alpha > 0$, we know that $1+\\alpha \\neq 0$, so we can divide by it:\n$$\nC_2 = \\frac{1+\\alpha}{4\\alpha(1 + \\alpha)} = \\frac{1}{4\\alpha}\n$$\nThe problem asks for the value of $u_{\\alpha}(0)$. From our general solution, we know that $u_{\\alpha}(0) = C_2$.\nTherefore, the exact value is:\n$$\nu_{\\alpha}(0) = \\frac{1}{4\\alpha}\n$$\nFor completeness, we can also find $C_1 = 2\\alpha C_2 = 2\\alpha \\left(\\frac{1}{4\\alpha}\\right) = \\frac{1}{2}$. The full solution is $u_{\\alpha}(x) = -\\frac{1}{2}x^2 + \\frac{1}{2}x + \\frac{1}{4\\alpha}$.", "answer": "$$\n\\boxed{\\frac{1}{4\\alpha}}\n$$", "id": "3376716"}, {"introduction": "Moving from analytical exercises to practical application is the final step in mastering a numerical technique. This capstone practice [@problem_id:3376733] requires you to build a complete Deep Ritz solver for a family of parameterized PDEs, translating the theoretical energy functionals into a concrete computational model. You will implement numerical quadrature, solve the resulting linear systems, and explore a meta-learning strategy to find an optimal starting point for solving new problems efficiently, thus synthesizing all the core concepts into a working tool.", "problem": "You are asked to implement a Deep Ritz solver for a family of parameterized elliptic partial differential equations using a neural trial space with fixed hidden layer (random features) and a linear output layer, together with a meta-learning procedure that selects an initialization minimizing the average energy. Specifically, consider the family of problems on the unit square domain $\\Omega = (0,1)^2$ with homogeneous Dirichlet boundary condition:\n$$\n-\\nabla\\cdot\\big(a_{\\mu}(x,y)\\,\\nabla u_{\\mu}(x,y)\\big) = f_{\\mu}(x,y), \\quad (x,y)\\in\\Omega,\\quad u_{\\mu}|_{\\partial\\Omega}=0,\n$$\nwhere the parameter $\\mu$ ranges over a subset of the interval $[0,1]$. The Deep Ritz method seeks to minimize, over a class of trial functions $u$, the energy functional\n$$\n\\mathcal{J}_{\\mu}(u) = \\int_{\\Omega} \\left( \\frac{1}{2}\\, a_{\\mu}(x,y)\\, \\lvert \\nabla u(x,y) \\rvert^2 \\;-\\; f_{\\mu}(x,y)\\,u(x,y) \\right)\\, \\mathrm{d}x\\,\\mathrm{d}y.\n$$\nUse a neural trial space that enforces the Dirichlet boundary conditions exactly via a boundary factor. Let $b(x,y) = x(1-x)\\,y(1-y)$, and define a fixed (not trainable) feature map $\\varphi:\\Omega\\to\\mathbb{R}^{D}$ using a single hidden layer with hyperbolic tangent activation:\n$$\n\\varphi(x,y) = \\tanh\\big(W\\,[x,y]^{\\top} + c\\big),\n$$\nwith $W\\in\\mathbb{R}^{D\\times 2}$ and $c\\in\\mathbb{R}^{D}$ drawn once from a standard normal distribution with a fixed random seed. The trainable functions are restricted to the linear span\n$$\nu_{w}(x,y) \\;=\\; b(x,y)\\,\\varphi(x,y)^{\\top} w, \\quad w\\in \\mathbb{R}^{D}.\n$$\nFor each parameter $\\mu$, the Ritz energy becomes a strictly convex quadratic function in $w$ of the form\n$$\n\\mathcal{J}_{\\mu}(w) \\;=\\; \\frac{1}{2}\\, w^{\\top} H_{\\mu}\\, w \\;-\\; h_{\\mu}^{\\top} w \\;+\\; C_{\\mu},\n$$\nwhere $H_{\\mu}\\in\\mathbb{R}^{D\\times D}$ and $h_{\\mu}\\in\\mathbb{R}^{D}$ are given by interior integrals that depend on $a_{\\mu}$, $f_{\\mu}$, $b$, and $\\varphi$. Approximate these integrals by Monte Carlo quadrature using $N$ i.i.d. samples uniformly distributed in $\\Omega$:\n- For each sample $(x_s,y_s)$, let $J_s\\in\\mathbb{R}^{D\\times 2}$ denote the Jacobian with rows $\\nabla\\!\\big(b\\,\\varphi_i\\big)(x_s,y_s)$, so that $\\lvert \\nabla u_w \\rvert^2 = \\sum_{i,j} w_i w_j \\, J_s(i,:) \\cdot J_s(j,:) = w^{\\top} \\big(J_s J_s^{\\top}\\big) w$.\n- Then\n$$\nH_{\\mu} \\approx \\frac{1}{N}\\sum_{s=1}^{N} a_{\\mu}(x_s,y_s)\\; J_s J_s^{\\top},\\quad\nh_{\\mu} \\approx \\frac{1}{N}\\sum_{s=1}^{N} f_{\\mu}(x_s,y_s)\\; b(x_s,y_s)\\,\\varphi(x_s,y_s).\n$$\n\nGround-truth parameterization. To enable quantitative evaluation, define $a_{\\mu}$ and $f_{\\mu}$ from a smooth, exactly solvable configuration. For any $\\mu\\in[0,1]$, let\n$$\na_{\\mu}(x,y) \\;=\\; 1 \\;+\\; \\tfrac{1}{2}\\,\\mu\\, \\sin(2\\pi x)\\,\\sin(2\\pi y),\n$$\nand define the exact solution\n$$\nu^{\\star}_{\\mu}(x,y) \\;=\\; \\sin(\\pi x)\\,\\sin(\\pi y) \\;+\\; \\mu\\, \\sin(2\\pi x)\\,\\sin(\\pi y).\n$$\nSet\n$$\nf_{\\mu}(x,y) \\;=\\; -\\nabla\\cdot\\big(a_{\\mu}(x,y)\\,\\nabla u^{\\star}_{\\mu}(x,y)\\big),\n$$\nso that $u^{\\star}_{\\mu}$ exactly satisfies the partial differential equation with homogeneous Dirichlet data. You must compute $f_{\\mu}$ analytically using the product rule:\n$$\nf_{\\mu} \\;=\\; -\\big(\\nabla a_{\\mu}\\cdot \\nabla u^{\\star}_{\\mu} \\;+\\; a_{\\mu}\\,\\Delta u^{\\star}_{\\mu}\\big).\n$$\n\nMeta-learning setup. Consider a training set of parameters $\\mathcal{M}_{\\mathrm{train}} = \\{\\, 0.0,\\; 0.4,\\; 0.8 \\,\\}$ and define the average energy\n$$\n\\overline{\\mathcal{J}}(w) \\;=\\; \\frac{1}{\\lvert \\mathcal{M}_{\\mathrm{train}}\\rvert}\\sum_{\\mu\\in \\mathcal{M}_{\\mathrm{train}}} \\mathcal{J}_{\\mu}(w) \\;=\\; \\frac{1}{2}\\, w^{\\top} \\overline{H}\\, w \\;-\\; \\overline{h}^{\\top} w \\;+\\; \\overline{C},\n$$\nwhere $\\overline{H}$ and $\\overline{h}$ are the corresponding averages of $H_{\\mu}$ and $h_{\\mu}$ computed using the same interior samples for all $\\mu$ to reduce variance. Define the meta-initialization as the unique minimizer\n$$\nw_{\\mathrm{meta}} \\;=\\; \\arg\\min_{w\\in\\mathbb{R}^{D}} \\overline{\\mathcal{J}}(w).\n$$\nPer-task adaptation uses gradient descent on $\\mathcal{J}_{\\mu}(w)$ with a common stepsize $\\eta>0$, starting from an initialization $w_0$:\n$$\nw_{t+1} \\;=\\; w_t - \\eta\\, \\nabla \\mathcal{J}_{\\mu}(w_t) \\;=\\; w_t - \\eta\\,(H_{\\mu} w_t - h_{\\mu}),\n$$\nfor $t=0,1,\\dots,T-1$. For strictly convex quadratic objectives, the amortization error (the suboptimality gap) after $T$ steps admits a worst-case bound in terms of the spectral radius of $I-\\eta H_{\\mu}$. Let $\\rho_{\\mu}(\\eta) = \\max_{\\lambda\\in\\mathrm{spec}(H_{\\mu})} \\lvert 1 - \\eta \\lambda\\rvert$. Then for any $w_0$,\n$$\n\\mathcal{J}_{\\mu}(w_T) - \\min_{w}\\mathcal{J}_{\\mu}(w) \\;\\le\\; \\rho_{\\mu}(\\eta)^{2T}\\, \\big(\\mathcal{J}_{\\mu}(w_0) - \\min_{w}\\mathcal{J}_{\\mu}(w)\\big).\n$$\n\nImplementation requirements.\n- Use $D = 30$ features, $N = 5000$ Monte Carlo interior samples, and a fixed random seed equal to $0$ for generating both features and samples.\n- Use the symmetric positive definite ridge-regularized matrices $H_{\\mu}^{\\epsilon} = H_{\\mu} + \\epsilon I$ with $\\epsilon = 10^{-8}$ for all linear solves and eigenvalue computations.\n- Choose a single stepsize $\\eta$ that is guaranteed to satisfy $\\eta \\le 1/L_{\\mu}$ for every $\\mu$ in the evaluation set by setting\n$$\n\\eta \\;=\\; \\frac{9}{10}\\,\\frac{1}{\\max_{\\mu\\in\\mathcal{M}_{\\mathrm{all}}} L_{\\mu}},\\quad L_{\\mu} = \\lambda_{\\max}(H_{\\mu}^{\\epsilon}),\n$$\nwhere $\\mathcal{M}_{\\mathrm{all}} = \\{\\, 0.0,\\; 0.3,\\; 0.4,\\; 0.6,\\; 0.7,\\; 0.8,\\; 0.9 \\,\\}$ covers all train and test parameters used below.\n\nTest suite. Your program must compute the following four boolean quantities, in this order:\n- Case A (amortization bound, one step): For $\\mu = 0.0$ and $T = 1$, with $w_0 = w_{\\mathrm{meta}}$, verify whether the actual gap is less than or equal to the bound:\n$$\n\\mathcal{J}_{\\mu}(w_T) - \\min_w \\mathcal{J}_{\\mu}(w) \\;\\le\\; \\rho_{\\mu}(\\eta)^{2T}\\,\\big(\\mathcal{J}_{\\mu}(w_0) - \\min_w \\mathcal{J}_{\\mu}(w)\\big).\n$$\n- Case B (amortization bound, multiple steps): For $\\mu = 0.7$ and $T = 5$, with $w_0 = w_{\\mathrm{meta}}$, verify the same bound as in Case A.\n- Case C (meta-initialization improves average energy): Check that the meta-initialization improves the average training energy relative to zero initialization,\n$$\n\\overline{\\mathcal{J}}(w_{\\mathrm{meta}}) \\;<\\; \\overline{\\mathcal{J}}(0).\n$$\n- Case D (benefit after one adaptation step on average): Over the evaluation set $\\{\\, 0.0,\\; 0.3,\\; 0.6,\\; 0.9 \\,\\}$, compare the average energy after one gradient step from $w_{\\mathrm{meta}}$ versus from the zero vector:\n$$\n\\frac{1}{4}\\sum_{\\mu\\in\\{0.0,0.3,0.6,0.9\\}} \\mathcal{J}_{\\mu}\\big(\\,w_{\\mathrm{meta}} - \\eta\\,(H_{\\mu} w_{\\mathrm{meta}} - h_{\\mu})\\,\\big) \\;<\\; \\frac{1}{4}\\sum_{\\mu\\in\\{0.0,0.3,0.6,0.9\\}} \\mathcal{J}_{\\mu}\\big(\\,- \\eta\\,(- h_{\\mu})\\,\\big).\n$$\n\nNumerical details.\n- You must compute $f_{\\mu}$ analytically by differentiating $a_{\\mu}$ and $u^{\\star}_{\\mu}$. Use the identities $\\tanh'(z) = 1 - \\tanh^2(z)$ and the product rule to compute the entries of $J_s$.\n- When verifying inequalities, allow a nonnegative tolerance $\\tau = 10^{-10}$ and treat “$\\le$” as “$\\le +\\tau$”.\n\nFinal output format. Your program should produce a single line of output containing a Python-style list with the four boolean results for Cases A–D, in order, for example, “[True,False,True,True]”. No other text should be printed.", "solution": "The user-defined problem is a well-posed numerical exercise in solving a parameterized partial differential equation (PDE) using a specific variant of the Deep Ritz method. This approach combines concepts from the calculus of variations, numerical linear algebra, and meta-learning. The problem is scientifically grounded, internally consistent, and provides all necessary data and definitions for a unique solution.\n\n### Step 1: Analytical Derivations\n\nFirst, we must derive the analytical expressions required for the implementation.\n\n**1.1. Source Term $f_{\\mu}(x,y)$**\n\nThe source term $f_{\\mu}$ is defined by the relation $f_{\\mu} = -\\nabla\\cdot(a_{\\mu}\\nabla u^{\\star}_{\\mu})$, where $u^{\\star}_{\\mu}$ is the exact solution. Using the vector calculus identity $\\nabla \\cdot (\\phi \\mathbf{F}) = (\\nabla \\phi) \\cdot \\mathbf{F} + \\phi (\\nabla \\cdot \\mathbf{F})$, we have $f_{\\mu} = -(\\nabla a_{\\mu} \\cdot \\nabla u^{\\star}_{\\mu} + a_{\\mu} \\Delta u^{\\star}_{\\mu})$. We compute each component:\n\n- The coefficient $a_{\\mu}(x,y) = 1 + \\frac{1}{2}\\mu\\sin(2\\pi x)\\sin(2\\pi y)$. Its gradient is:\n$$\n\\nabla a_{\\mu} = \\pi\\mu \\begin{pmatrix} \\cos(2\\pi x)\\sin(2\\pi y) \\\\ \\sin(2\\pi x)\\cos(2\\pi y) \\end{pmatrix}.\n$$\n\n- The exact solution is $u^{\\star}_{\\mu}(x,y) = \\sin(\\pi x)\\sin(\\pi y) + \\mu\\sin(2\\pi x)\\sin(\\pi y)$. Its gradient is:\n$$\n\\nabla u^{\\star}_{\\mu} = \\begin{pmatrix} \\pi\\cos(\\pi x)\\sin(\\pi y) + 2\\pi\\mu\\cos(2\\pi x)\\sin(\\pi y) \\\\ \\pi\\sin(\\pi x)\\cos(\\pi y) + \\pi\\mu\\sin(2\\pi x)\\cos(\\pi y) \\end{pmatrix}.\n$$\n\n- The Laplacian of the exact solution is $\\Delta u^{\\star}_{\\mu} = \\frac{\\partial^2 u^{\\star}_{\\mu}}{\\partial x^2} + \\frac{\\partial^2 u^{\\star}_{\\mu}}{\\partial y^2}$:\n$$\n\\frac{\\partial^2 u^{\\star}_{\\mu}}{\\partial x^2} = -\\pi^2\\sin(\\pi x)\\sin(\\pi y) - 4\\pi^2\\mu\\sin(2\\pi x)\\sin(\\pi y) \\\\\n\\frac{\\partial^2 u^{\\star}_{\\mu}}{\\partial y^2} = -\\pi^2\\sin(\\pi x)\\sin(\\pi y) - \\pi^2\\mu\\sin(2\\pi x)\\sin(\\pi y) \\\\\n\\Delta u^{\\star}_{\\mu} = -2\\pi^2\\sin(\\pi x)\\sin(\\pi y) - 5\\pi^2\\mu\\sin(2\\pi x)\\sin(\\pi y).\n$$\n\nCombining these terms yields the full expression for $f_{\\mu}(x,y)$, which will be implemented directly.\n\n**1.2. Jacobian of Trial Basis Functions**\n\nThe trial functions are $u_w(x,y) = \\sum_{i=1}^{D} w_i \\psi_i(x,y)$, where the basis functions are $\\psi_i(x,y) = b(x,y)\\varphi_i(x,y)$. The Jacobian $J_s$ at a sample point $(x_s, y_s)$ has rows given by the gradients $\\nabla \\psi_i (x_s, y_s)$. Using the product rule, $\\nabla \\psi_i = (\\nabla b)\\varphi_i + b(\\nabla \\varphi_i)$.\n\n- Boundary factor: $b(x,y) = x(1-x)y(1-y)$. Its gradient is:\n$$\n\\nabla b(x,y) = \\begin{pmatrix} (1-2x)y(1-y) \\\\ x(1-x)(1-2y) \\end{pmatrix}.\n$$\n\n- Feature map: $\\varphi_i(x,y) = \\tanh(z_i(x,y))$ with $z_i(x,y) = W_{i,1}x + W_{i,2}y + c_i$. Using $\\tanh'(z) = 1-\\tanh^2(z)$, its gradient is:\n$$\n\\nabla\\varphi_i(x,y) = (1-\\varphi_i(x,y)^2)\\nabla z_i = (1-\\varphi_i(x,y)^2)\\begin{pmatrix} W_{i,1} \\\\ W_{i,2} \\end{pmatrix}.\n$$\n\nThus, the $i$-th row of the Jacobian matrix $J_s \\in \\mathbb{R}^{D \\times 2}$ is:\n$$\n\\nabla\\psi_i(x_s, y_s) = \\varphi_i(x_s,y_s) \\nabla b(x_s, y_s) + b(x_s,y_s) (1-\\varphi_i(x_s,y_s)^2) \\begin{pmatrix} W_{i,1} \\\\ W_{i,2} \\end{pmatrix}.\n$$\n\n### Step 2: Numerical Scheme\n\nThe core of the problem is to construct and manipulate the quadratic energy functionals $\\mathcal{J}_{\\mu}(w) = \\frac{1}{2}w^{\\top}H_{\\mu}w - h_{\\mu}^{\\top}w$.\n\n**2.1. Assembly of $H_{\\mu}$ and $h_{\\mu}$**\n\nThe matrices $H_{\\mu}$ and vectors $h_{\\mu}$ are assembled via Monte Carlo integration over $N=5000$ samples $(x_s, y_s)$ drawn uniformly from $\\Omega=(0,1)^2$:\n$$\nH_{\\mu} \\approx \\frac{1}{N}\\sum_{s=1}^{N} a_{\\mu}(x_s,y_s)\\; J_s J_s^{\\top},\\quad\nh_{\\mu} \\approx \\frac{1}{N}\\sum_{s=1}^{N} f_{\\mu}(x_s,y_s)\\; b(x_s,y_s)\\,\\varphi(x_s,y_s).\n$$\nA fixed random seed of $0$ is used to generate both the neural network parameters $(W, c)$ and the sample points. To ensure stability, we use a regularized matrix $H_{\\mu}^{\\epsilon} = H_{\\mu} + \\epsilon I$ with $\\epsilon = 10^{-8}$ for linear solves and eigenvalue computations.\n\n**2.2. Meta-initialization and Adaptation**\n\n- **Meta-learning**: The meta-initialization $w_{\\mathrm{meta}}$ is found by minimizing the average energy over the training set $\\mathcal{M}_{\\mathrm{train}} = \\{0.0, 0.4, 0.8\\}$. This average energy is also a quadratic $\\overline{\\mathcal{J}}(w) = \\frac{1}{2}w^{\\top}\\overline{H}w - \\overline{h}^{\\top}w$, where $\\overline{H}$ and $\\overline{h}$ are the averages of $H_{\\mu}$ and $h_{\\mu}$ over $\\mathcal{M}_{\\mathrm{train}}$. The minimizer is the solution to the linear system $\\overline{H}^{\\epsilon}w = \\overline{h}$.\n\n- **Adaptation**: For a given task $\\mu$, the solution is refined using gradient descent starting from an initial guess $w_0$. The update rule is $w_{t+1} = w_t - \\eta \\nabla\\mathcal{J}_{\\mu}(w_t) = w_t - \\eta(H_{\\mu}w_t - h_{\\mu})$. The stepsize $\\eta$ is chosen conservatively as $\\eta = \\frac{9}{10} / L_{\\max}$, where $L_{\\max} = \\max_{\\mu \\in \\mathcal{M}_{\\mathrm{all}}} \\lambda_{\\max}(H_{\\mu}^{\\epsilon})$.\n\n### Step 3: Evaluation of Test Cases\n\nThe four boolean test cases are evaluated as follows:\n\n- **Cases A and B (Amortization Bound)**: We verify the inequality $\\mathcal{J}_{\\mu}(w_T) - \\min \\mathcal{J}_{\\mu} \\le \\rho_{\\mu}(\\eta)^{2T}(\\mathcal{J}_{\\mu}(w_0) - \\min \\mathcal{J}_{\\mu}) + \\tau$.\n    - $w_0 = w_{\\mathrm{meta}}$.\n    - $w_T$ is obtained by running $T$ steps of gradient descent.\n    - $\\min \\mathcal{J}_{\\mu}$ is approximated by $\\mathcal{J}_{\\mu}(w^*_{\\mu})$, where $w^*_{\\mu} = (H_{\\mu}^{\\epsilon})^{-1}h_{\\mu}$.\n    - The spectral radius $\\rho_{\\mu}(\\eta) = \\max_{\\lambda \\in \\mathrm{spec}(H_{\\mu}^{\\epsilon})} |1-\\eta\\lambda|$.\n\n- **Case C (Meta-initialization Performance)**: We check if the average energy at the meta-initialization is lower than at the origin: $\\overline{\\mathcal{J}}(w_{\\mathrm{meta}}) < \\overline{\\mathcal{J}}(0)$. Since $\\overline{\\mathcal{J}}(0)=0$ and $w_{\\mathrm{meta}}$ is the minimizer, this should hold true.\n\n- **Case D (Average Adaptation Benefit)**: We compare the average energy after one gradient step starting from $w_{\\mathrm{meta}}$ versus starting from $w_0=0$, over the evaluation set $\\{0.0, 0.3, 0.6, 0.9\\}$. This tests whether the meta-initialization provides a better starting point for adaptation on average.\n\nThe following code implements this entire procedure.", "answer": "```python\nimport numpy as np\nfrom scipy import linalg\n\ndef solve():\n    \"\"\"\n    Implements a Deep Ritz solver with meta-initialization for a parameterized elliptic PDE.\n    \"\"\"\n    # 1. Problem\n    # Constants\n    D = 30\n    N = 5000\n    epsilon = 1e-8\n    seed = 0\n    tau = 1e-10\n\n    # Parameter sets\n    M_train = [0.0, 0.4, 0.8]\n    M_all = [0.0, 0.3, 0.4, 0.6, 0.7, 0.8, 0.9]\n    M_eval_D = [0.0, 0.3, 0.6, 0.9]\n    \n    # 2. Setup\n    rng = np.random.default_rng(seed)\n    W = rng.standard_normal(size=(D, 2))\n    c = rng.standard_normal(size=D)\n    samples = rng.uniform(0, 1, size=(N, 2))\n\n    # 3. Analytical Functions\n    pi = np.pi\n\n    def a_mu_func(x, y, mu):\n        return 1.0 + 0.5 * mu * np.sin(2 * pi * x) * np.sin(2 * pi * y)\n\n    def f_mu_func(x, y, mu):\n        # Derivatives of u_star\n        grad_u_star_x = pi*np.sin(pi*y)*(np.cos(pi*x) + 2*mu*np.cos(2*pi*x))\n        grad_u_star_y = pi*np.cos(pi*y)*(np.sin(pi*x) + mu*np.sin(2*pi*x))\n        grad_u_star = np.array([grad_u_star_x, grad_u_star_y])\n\n        delta_u_star = -pi**2*np.sin(pi*y)*(2*np.sin(pi*x) + 5*mu*np.sin(2*pi*x))\n        \n        # Derivatives of a_mu\n        a_mu_val = a_mu_func(x, y, mu)\n        grad_a_mu_x = pi*mu*np.cos(2*pi*x)*np.sin(2*pi*y)\n        grad_a_mu_y = pi*mu*np.sin(2*pi*x)*np.cos(2*pi*y)\n        grad_a_mu = np.array([grad_a_mu_x, grad_a_mu_y])\n\n        return -(np.dot(grad_a_mu, grad_u_star) + a_mu_val * delta_u_star)\n\n    # 4. Assembly of H and h\n    def assemble_H_h(mu, samples, W, c):\n        H = np.zeros((D, D))\n        h_vec = np.zeros(D)\n        \n        for s in range(N):\n            x, y = samples[s, 0], samples[s, 1]\n            \n            a_val = a_mu_func(x, y, mu)\n            f_val = f_mu_func(x, y, mu)\n            b_val = x * (1 - x) * y * (1 - y)\n            \n            z = W @ np.array([x, y]) + c\n            phi = np.tanh(z)\n            \n            grad_b = np.array([(1-2*x)*y*(1-y), x*(1-x)*(1-2*y)])\n            grad_phi_factor = 1 - phi**2\n            \n            J_s = np.outer(phi, grad_b) + b_val * (grad_phi_factor[:, np.newaxis] * W)\n            \n            H += a_val * (J_s @ J_s.T)\n            h_vec += f_val * b_val * phi\n        \n        return H / N, h_vec / N\n\n    # 5. Pre-computation for all mu\n    task_data = {}\n    for mu in M_all:\n        H, h = assemble_H_h(mu, samples, W, c)\n        H_eps = H + epsilon * np.eye(D)\n        task_data[mu] = {'H': H, 'h': h, 'H_eps': H_eps}\n\n    # 6. Compute stepsize eta\n    L_mus = [linalg.eigh(task_data[mu]['H_eps'], eigvals_only=True)[-1] for mu in M_all]\n    eta = 0.9 / np.max(L_mus)\n\n    # 7. Compute Meta-Initialization w_meta\n    H_bar = np.mean([task_data[mu]['H'] for mu in M_train], axis=0)\n    h_bar = np.mean([task_data[mu]['h'] for mu in M_train], axis=0)\n    H_bar_eps = H_bar + epsilon * np.eye(D)\n    w_meta = linalg.solve(H_bar_eps, h_bar, assume_a='pos')\n\n    # 8. Helper functions for evaluation\n    def compute_energy(w, H, h):\n        return 0.5 * w.T @ H @ w - h.T @ w\n\n    # 9. Evaluate Test Cases\n    results = []\n\n    # Case A: Amortization bound, T=1\n    mu_A = 0.0\n    T_A = 1\n    w0_A = w_meta\n    H_A, h_A, H_eps_A = task_data[mu_A]['H'], task_data[mu_A]['h'], task_data[mu_A]['H_eps']\n    w_T_A = w0_A - eta * (H_A @ w0_A - h_A)\n    w_opt_A = linalg.solve(H_eps_A, h_A, assume_a='pos')\n    E_min_A = compute_energy(w_opt_A, H_A, h_A)\n    E_0_A = compute_energy(w0_A, H_A, h_A)\n    E_T_A = compute_energy(w_T_A, H_A, h_A)\n    actual_gap_A = E_T_A - E_min_A\n    initial_gap_A = E_0_A - E_min_A\n    eigvals_A = linalg.eigh(H_eps_A, eigvals_only=True)\n    rho_mu_A = np.max(np.abs(1 - eta * eigvals_A))\n    bound_A = (rho_mu_A**(2 * T_A)) * initial_gap_A\n    results.append(actual_gap_A <= bound_A + tau)\n\n    # Case B: Amortization bound, T=5\n    mu_B = 0.7\n    T_B = 5\n    w_t_B = w_meta\n    H_B, h_B, H_eps_B = task_data[mu_B]['H'], task_data[mu_B]['h'], task_data[mu_B]['H_eps']\n    for _ in range(T_B):\n        w_t_B = w_t_B - eta * (H_B @ w_t_B - h_B)\n    w_T_B = w_t_B\n    w_opt_B = linalg.solve(H_eps_B, h_B, assume_a='pos')\n    E_min_B = compute_energy(w_opt_B, H_B, h_B)\n    E_0_B = compute_energy(w_meta, H_B, h_B)\n    E_T_B = compute_energy(w_T_B, H_B, h_B)\n    actual_gap_B = E_T_B - E_min_B\n    initial_gap_B = E_0_B - E_min_B\n    eigvals_B = linalg.eigh(H_eps_B, eigvals_only=True)\n    rho_mu_B = np.max(np.abs(1 - eta * eigvals_B))\n    bound_B = (rho_mu_B**(2 * T_B)) * initial_gap_B\n    results.append(actual_gap_B <= bound_B + tau)\n\n    # Case C: Meta-initialization improves average energy\n    J_bar_w_meta = compute_energy(w_meta, H_bar, h_bar)\n    J_bar_0 = compute_energy(np.zeros(D), H_bar, h_bar)\n    results.append(J_bar_w_meta < J_bar_0)\n\n    # Case D: Benefit after one adaptation step on average\n    E_meta_avg = 0\n    E_zero_avg = 0\n    for mu_D in M_eval_D:\n        H_D, h_D = task_data[mu_D]['H'], task_data[mu_D]['h']\n        w1_meta = w_meta - eta * (H_D @ w_meta - h_D)\n        E_meta_avg += compute_energy(w1_meta, H_D, h_D)\n        w1_zero = eta * h_D\n        E_zero_avg += compute_energy(w1_zero, H_D, h_D)\n    results.append((E_meta_avg / len(M_eval_D)) < (E_zero_avg / len(M_eval_D)))\n\n    # Final print statement\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3376733"}]}