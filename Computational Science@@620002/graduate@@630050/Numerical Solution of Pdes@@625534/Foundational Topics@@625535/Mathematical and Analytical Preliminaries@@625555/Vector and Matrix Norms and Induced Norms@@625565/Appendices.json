{"hands_on_practices": [{"introduction": "In numerical analysis, we often switch between different norms to simplify proofs or obtain tighter bounds. While all norms on a finite-dimensional vector space are equivalent, the equivalence constants are crucial for quantitative estimates of error and stability. This practice [@problem_id:3460922] challenges you to explore the sharp inequalities relating the fundamental $\\ell_1$, $\\ell_2$, and $\\ell_\\infty$ norms. By constructing vectors that achieve equality, you will develop a concrete, geometric intuition for these relationships, visualizing them as the contact points between scaled hypercubes, spheres, and cross-polytopes.", "problem": "In the analysis of stability and error propagation for linear finite difference discretizations of a one-dimensional Partial Differential Equation (PDE) on a uniform grid with $n$ interior nodes, grid functions are identified with vectors $x \\in \\mathbb{R}^{n}$ and measured with standard vector norms. Consider the three norms defined for $x=(x_{1},\\dots,x_{n})^{\\mathsf{T}} \\in \\mathbb{R}^{n}$ by\n$$\n\\|x\\|_{1} \\equiv \\sum_{i=1}^{n} |x_{i}|,\\quad\n\\|x\\|_{2} \\equiv \\left(\\sum_{i=1}^{n} |x_{i}|^{2}\\right)^{1/2},\\quad\n\\|x\\|_{\\infty} \\equiv \\max_{1 \\leq i \\leq n} |x_{i}|.\n$$\nThese norms are used to bound induced operator norms and condition numbers of discretized operators under norm changes. Starting from the core definitions and standard inequalities from linear algebra, carry out the following steps:\n\n1. Using only the definitions of the norms and fundamental inequalities between them (obtained from the Cauchy–Schwarz inequality and from the ordering $|x_{i}| \\leq \\|x\\|_{\\infty}$), derive the norm relations\n$$\n\\|x\\|_{1} \\leq \\sqrt{n}\\,\\|x\\|_{2}, \\qquad \\|x\\|_{2} \\leq \\sqrt{n}\\,\\|x\\|_{\\infty}\n$$\nfor all nonzero $x \\in \\mathbb{R}^{n}$.\n\n2. Characterize precisely the equality cases in each inequality by determining necessary and sufficient conditions on the components of $x$ for which equality is attained.\n\n3. Provide two explicit nonzero vectors $x^{(1)},x^{(2)} \\in \\mathbb{R}^{n}$, expressed as functions of $n$, such that $x^{(1)}$ attains equality in $\\|x\\|_{1} = \\sqrt{n}\\,\\|x\\|_{2}$ and $x^{(2)}$ attains equality in $\\|x\\|_{2} = \\sqrt{n}\\,\\|x\\|_{\\infty}$. Then interpret geometrically why these choices realize equality, in terms of how the $\\ell_{1}$-, $\\ell_{2}$-, and $\\ell_{\\infty}$-unit balls in $\\mathbb{R}^{n}$ touch after the minimal scaling that makes one contain another.\n\nYour final answer should consist only of your explicit $x^{(1)}$ and $x^{(2)}$ as a single row matrix, with each entry being the corresponding $n$-vector. No numerical rounding is required and no physical units are involved. Express your answer in exact symbolic form.", "solution": "We begin with the definitions of the three norms for a vector $x=(x_{1},\\dots,x_{n})^{\\mathsf{T}} \\in \\mathbb{R}^{n}$:\n$$\n\\|x\\|_{1}=\\sum_{i=1}^{n}|x_{i}|,\\qquad\n\\|x\\|_{2}=\\left(\\sum_{i=1}^{n}|x_{i}|^{2}\\right)^{1/2},\\qquad\n\\|x\\|_{\\infty}=\\max_{1\\leq i\\leq n}|x_{i}|.\n$$\n\nStep 1: Derivation of $\\|x\\|_{1} \\leq \\sqrt{n}\\,\\|x\\|_{2}$. Consider the vectors $a=(|x_{1}|,\\dots,|x_{n}|)^{\\mathsf{T}}$ and $b=(1,\\dots,1)^{\\mathsf{T}} \\in \\mathbb{R}^{n}$. By the Cauchy–Schwarz inequality for the Euclidean inner product,\n$$\na \\cdot b \\leq \\|a\\|_{2}\\,\\|b\\|_{2}.\n$$\nThe left-hand side is $a\\cdot b = \\sum_{i=1}^{n}|x_{i}| = \\|x\\|_{1}$. The norms on the right are $\\|a\\|_{2} = \\left(\\sum_{i=1}^{n}|x_{i}|^{2}\\right)^{1/2}=\\|x\\|_{2}$ and $\\|b\\|_{2}=\\sqrt{n}$. Therefore,\n$$\n\\|x\\|_{1} \\leq \\sqrt{n}\\,\\|x\\|_{2}.\n$$\n\nStep 2: Derivation of $\\|x\\|_{2} \\leq \\sqrt{n}\\,\\|x\\|_{\\infty}$. Since $|x_{i}| \\leq \\|x\\|_{\\infty}$ for each $i$, we have $|x_{i}|^{2} \\leq \\|x\\|_{\\infty}^{2}$ and consequently\n$$\n\\|x\\|_{2}^{2}=\\sum_{i=1}^{n}|x_{i}|^{2} \\leq \\sum_{i=1}^{n}\\|x\\|_{\\infty}^{2}\n= n\\,\\|x\\|_{\\infty}^{2}.\n$$\nTaking the positive square root yields\n$$\n\\|x\\|_{2} \\leq \\sqrt{n}\\,\\|x\\|_{\\infty}.\n$$\n\nStep 3: Equality conditions. For the first inequality, Cauchy–Schwarz achieves equality if and only if the vectors $a$ and $b$ are linearly dependent, that is, there exists a scalar $\\alpha \\geq 0$ such that $a=\\alpha b$. In components this means $|x_{i}|=\\alpha$ for all $i=1,\\dots,n$. Therefore, equality in $\\|x\\|_{1} \\leq \\sqrt{n}\\,\\|x\\|_{2}$ holds if and only if all components of $x$ have the same absolute value, i.e., $x_{i} \\in \\{\\pm \\alpha\\}$ with a common $\\alpha0$.\n\nFor the second inequality, equality in $|x_{i}|^{2} \\leq \\|x\\|_{\\infty}^{2}$ for each $i$ and hence in the sum occurs if and only if $|x_{i}|=\\|x\\|_{\\infty}$ for every $i=1,\\dots,n$. Equivalently, all components of $x$ have absolute value equal to the maximal absolute value. Thus, equality in $\\|x\\|_{2} \\leq \\sqrt{n}\\,\\|x\\|_{\\infty}$ also holds if and only if all components have the same absolute value.\n\nStep 4: Explicit vectors attaining equality. A simple explicit choice is to take any nonzero constant vector. Two convenient instances are:\n- $x^{(1)}=(1,1,\\dots,1)^{\\mathsf{T}} \\in \\mathbb{R}^{n}$. Then $\\|x^{(1)}\\|_{1}=n$, $\\|x^{(1)}\\|_{2}=\\sqrt{n}$, and $\\|x^{(1)}\\|_{\\infty}=1$, so $\\|x^{(1)}\\|_{1}=\\sqrt{n}\\,\\|x^{(1)}\\|_{2}$ and $\\|x^{(1)}\\|_{2}=\\sqrt{n}\\,\\|x^{(1)}\\|_{\\infty}$ both hold.\n- $x^{(2)}=(1,-1,1,-1,\\dots)^{\\mathsf{T}} \\in \\mathbb{R}^{n}$, defined by $x^{(2)}_{i}=(-1)^{i-1}$. Then all $|x^{(2)}_{i}|=1$, so $\\|x^{(2)}\\|_{1}=n$, $\\|x^{(2)}\\|_{2}=\\sqrt{n}$, and $\\|x^{(2)}\\|_{\\infty}=1$, giving $\\|x^{(2)}\\|_{2}=\\sqrt{n}\\,\\|x^{(2)}\\|_{\\infty}$ (and also $\\|x^{(2)}\\|_{1}=\\sqrt{n}\\,\\|x^{(2)}\\|_{2}$).\n\nGeometric intuition. The $\\ell_{1}$-unit ball in $\\mathbb{R}^{n}$ is the cross-polytope, the $\\ell_{2}$-unit ball is the Euclidean sphere, and the $\\ell_{\\infty}$-unit ball is the hypercube. The minimal scaling of the $\\ell_{2}$-ball that contains the $\\ell_{1}$-ball occurs by a factor $\\sqrt{n}$ and the contact points are along directions where all coordinates have equal absolute value, corresponding to the vectors identified above. Likewise, the minimal scaling of the $\\ell_{\\infty}$-ball to contain the $\\ell_{2}$-ball is by $\\sqrt{n}$, with contact at points where all coordinates attain the maximal magnitude. In the context of grid functions for numerical PDEs, these extremal vectors correspond to constant-magnitude grid states, which govern the sharp norm-equivalence constants that appear when changing norms in stability and conditioning estimates for discretized operators.", "answer": "$$\\boxed{\\begin{pmatrix}(1,\\dots,1)^{\\mathsf{T}}  \\bigl((-1)^{i-1}\\bigr)_{i=1}^{n}\\end{pmatrix}}$$", "id": "3460922"}, {"introduction": "When we discretize a partial differential equation, the differential operator is replaced by a large matrix. The \"size\" of this matrix, measured by an induced matrix norm, is fundamental to analyzing the stability and conditioning of the numerical scheme. This exercise [@problem_id:3460949] provides hands-on practice by asking you to compute the induced $1$-norm and $\\infty$-norm for the discrete Laplacian on a 2D grid. You will see how the matrix norm is determined directly by the coefficients of the underlying five-point finite difference stencil, providing a tangible link between the local discretization and the global properties of the operator.", "problem": "Consider the two-dimensional Poisson equation $-\\Delta u = f$ on the unit square $[0,1] \\times [0,1]$ with homogeneous Dirichlet boundary conditions. Discretize the domain using a uniform Cartesian grid with mesh spacing $h = \\frac{1}{n+1}$, where $n \\geq 3$ is the number of unknowns per coordinate direction (so there are $n^{2}$ interior unknowns). Using the standard second-order central finite difference scheme, the discrete negative Laplacian operator acting on the vector of interior nodal values is represented by a sparse matrix $L \\in \\mathbb{R}^{n^{2} \\times n^{2}}$ whose nonzero entries correspond to the five-point stencil: for a grid point with four interior neighbors, the diagonal entry is $-4 h^{-2}$ and the four off-diagonal neighbor entries are $h^{-2}$ each; for grid points adjacent to the boundary, any neighbor outside the interior is eliminated from $L$ (its contribution moves to the right-hand side), so such rows have fewer than four off-diagonal entries of magnitude $h^{-2}$. Assume $n \\geq 3$ so that there exist rows corresponding to strictly interior points (with all four neighbors interior).\n\nStarting from the definitions of the induced matrix one-norm and infinity-norm,\n$$\\|A\\|_{1} = \\max_{1 \\leq j \\leq m} \\sum_{i=1}^{m} |a_{ij}|, \\quad \\|A\\|_{\\infty} = \\max_{1 \\leq i \\leq m} \\sum_{j=1}^{m} |a_{ij}|,$$\nderive sharp bounds for $\\|L\\|_{1}$ and $\\|L\\|_{\\infty}$ and then determine their exact values in terms of $h$ under the stated assumption on $n$. Your final answer must be a single analytical expression that lists $\\|L\\|_{1}$ and $\\|L\\|_{\\infty}$ in a single row using the LaTeX $\\mathrm{pmatrix}$ environment. No rounding is required.", "solution": "The problem asks for the induced matrix 1-norm and $\\infty$-norm of the discrete Laplacian matrix $L \\in \\mathbb{R}^{n^{2} \\times n^{2}}$.\n\nThe induced $\\infty$-norm is defined as the maximum absolute row sum:\n$$ \\|L\\|_{\\infty} = \\max_{1 \\leq i \\leq n^2} \\sum_{j=1}^{n^2} |L_{ij}| $$\nA row in the matrix $L$ corresponds to the finite difference equation at a single interior grid point. The sum of the absolute values of the entries in a row depends on the location of that grid point. We consider the three types of interior points for a grid with $n \\geq 3$:\n1.  **Fully interior points:** For a point $(i,j)$ with $1  i  n$ and $1  j  n$, it has four interior neighbors. The corresponding row in $L$ has a diagonal entry of $-4h^{-2}$ and four off-diagonal entries of $h^{-2}$. The absolute row sum is:\n    $$ |-4h^{-2}| + |h^{-2}| + |h^{-2}| + |h^{-2}| + |h^{-2}| = 4h^{-2} + 4h^{-2} = 8h^{-2} $$\n2.  **Edge points (not corners):** For a point on an edge of the interior grid (e.g., $(1,j)$ with $1  j  n$), it has three interior neighbors. The absolute row sum is:\n    $$ |-4h^{-2}| + |h^{-2}| + |h^{-2}| + |h^{-2}| = 4h^{-2} + 3h^{-2} = 7h^{-2} $$\n3.  **Corner points:** For a point at a corner of the interior grid (e.g., $(1,1)$), it has two interior neighbors. The absolute row sum is:\n    $$ |-4h^{-2}| + |h^{-2}| + |h^{-2}| = 4h^{-2} + 2h^{-2} = 6h^{-2} $$\nThe maximum of these values determines the norm. Since $n \\geq 3$, fully interior points exist. Thus,\n$$ \\|L\\|_{\\infty} = \\max\\{8h^{-2}, 7h^{-2}, 6h^{-2}\\} = 8h^{-2} $$\n\nThe induced 1-norm is defined as the maximum absolute column sum:\n$$ \\|L\\|_{1} = \\max_{1 \\leq j \\leq n^2} \\sum_{i=1}^{n^2} |L_{ij}| $$\nThe matrix $L$ that arises from the standard five-point stencil with lexicographical ordering of grid points is symmetric ($L = L^T$). For any symmetric matrix, the absolute column sums are identical to the absolute row sums. Therefore, $\\|L\\|_{1} = \\|L\\|_{\\infty}$.\n$$ \\|L\\|_{1} = 8h^{-2} $$\nThus, both norms are equal to $8h^{-2}$.", "answer": "$$\\boxed{\\begin{pmatrix} 8h^{-2}  8h^{-2} \\end{pmatrix}}$$", "id": "3460949"}, {"introduction": "For iterative methods, the spectral radius $\\rho(A)$ of the iteration matrix $A$ determines the asymptotic rate of convergence. However, the matrix norm $\\|A\\|$ bounds the error amplification in a single step. For nonnormal matrices, which frequently arise in the discretization of advection-dominated PDEs, there can be a large gap between these two quantities. This practice [@problem_id:3460924] guides you through a canonical example where $\\rho(A) \\lt 1$ but $\\|A\\|_2 \\gt 1$, revealing the important phenomenon of transient error growth. Understanding this behavior is essential for correctly interpreting the convergence of numerical methods and diagnosing stability issues that are not apparent from eigenvalue analysis alone.", "problem": "Consider a stationary linear iterative method applied to a two-mode subsystem extracted by Local Fourier Analysis of a finite difference discretization of an anisotropic elliptic Partial Differential Equation (PDE). The local iteration matrix is modeled by the nonnormal upper-triangular block\n$$\nA = \\begin{pmatrix} d  \\alpha \\\\ 0  d \\end{pmatrix},\n$$\nwith $d \\in (0,1)$ the diagonal damping factor and $\\alpha \\neq 0$ a coupling parameter arising from anisotropy and stencil alignment. Take $d=0.9$ and $\\alpha=2$, so that\n$$\nA = \\begin{pmatrix} 0.9  2 \\\\ 0  0.9 \\end{pmatrix}.\n$$\nStarting from first principles, use the core definitions of spectral radius $\\rho(A)$ and induced $2$-norm $\\|A\\|_{2}$ to:\n- establish whether $\\rho(A)  1$,\n- compute the exact induced $2$-norm $\\|A\\|_{2}$ via the singular value characterization.\n\nThen, briefly explain the implication for norm-based convergence tests of the iteration $x^{(k+1)} = A x^{(k)}$ when the iteration matrix is nonnormal. Finally, report the numerical value of $\\|A\\|_{2}$ for the given $A$, rounding your result to four significant figures. No units are required.", "solution": "First, we determine the spectral radius $\\rho(A)$ for the matrix $A = \\begin{pmatrix} 0.9  2 \\\\ 0  0.9 \\end{pmatrix}$. Since $A$ is an upper-triangular matrix, its eigenvalues are the entries on its main diagonal. Thus, the eigenvalues are $\\lambda_1 = \\lambda_2 = 0.9$. The spectral radius is the maximum of the absolute values of the eigenvalues:\n$$ \\rho(A) = \\max(|\\lambda_1|, |\\lambda_2|) = \\max(0.9, 0.9) = 0.9 $$\nSince $\\rho(A) = 0.9  1$, the iterative method is asymptotically convergent.\n\nNext, we compute the induced 2-norm $\\|A\\|_2$. This norm is given by the largest singular value of $A$, which is the square root of the largest eigenvalue of the matrix $A^T A$.\nWe first compute $A^T A$:\n$$ A^T A = \\begin{pmatrix} 0.9  0 \\\\ 2  0.9 \\end{pmatrix} \\begin{pmatrix} 0.9  2 \\\\ 0  0.9 \\end{pmatrix} = \\begin{pmatrix} 0.9^2  0.9 \\cdot 2 \\\\ 2 \\cdot 0.9  2^2 + 0.9^2 \\end{pmatrix} = \\begin{pmatrix} 0.81  1.8 \\\\ 1.8  4.81 \\end{pmatrix} $$\nThe eigenvalues of $A^T A$ are the roots of the characteristic equation $\\det(A^T A - \\lambda I) = 0$:\n$$ (0.81 - \\lambda)(4.81 - \\lambda) - (1.8)^2 = 0 $$\n$$ \\lambda^2 - (0.81 + 4.81)\\lambda + (0.81)(4.81) - 3.24 = 0 $$\n$$ \\lambda^2 - 5.62\\lambda + 3.8961 - 3.24 = 0 $$\n$$ \\lambda^2 - 5.62\\lambda + 0.6561 = 0 $$\nUsing the quadratic formula, the eigenvalues are:\n$$ \\lambda = \\frac{5.62 \\pm \\sqrt{5.62^2 - 4(0.6561)}}{2} = \\frac{5.62 \\pm \\sqrt{31.5844 - 2.6244}}{2} = \\frac{5.62 \\pm \\sqrt{28.96}}{2} $$\nThe largest eigenvalue is $\\lambda_{\\max} = \\frac{5.62 + \\sqrt{28.96}}{2}$.\nThe induced 2-norm is the square root of $\\lambda_{\\max}$:\n$$ \\|A\\|_2 = \\sqrt{\\lambda_{\\max}} = \\sqrt{\\frac{5.62 + \\sqrt{28.96}}{2}} \\approx \\sqrt{\\frac{5.62 + 5.38145}{2}} \\approx \\sqrt{5.500725} \\approx 2.3453624 $$\nRounding to four significant figures, $\\|A\\|_2 \\approx 2.345$.\n\nThe key implication is that while $\\rho(A) = 0.9  1$ guarantees that the iteration $x^{(k+1)} = A x^{(k)}$ converges as $k \\to \\infty$, the fact that $\\|A\\|_2  1$ allows for transient growth of the error. The error $e^{(k)}$ satisfies $\\|e^{(k)}\\|_2 \\le \\|A\\|_2 \\|e^{(k-1)}\\|_2$, so the error norm can increase in the initial steps before it eventually decays. This highlights that for nonnormal matrices, the spectral radius only describes long-term behavior, while the matrix norm is a better indicator of short-term stability and error amplification.", "answer": "$$\n\\boxed{2.345}\n$$", "id": "3460924"}]}