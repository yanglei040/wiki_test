{"hands_on_practices": [{"introduction": "This first practice lays the foundation for accurate numerical implementation of the finite element method on complex geometries. We will determine the minimum numerical quadrature order required to exactly integrate the weak form's bilinear term after it has been pulled back to the reference element. This exercise will illuminate the crucial difference between affine mappings, which yield polynomial integrands amenable to exact quadrature, and non-affine mappings, where the resulting rational functions pose a significant challenge [@problem_id:3439558].", "problem": "Consider the scalar diffusion bilinear form on a single isoparametric quadrilateral element in two spatial dimensions, given by\n$$\na_{K}(u,v) \\;=\\; \\int_{K} \\nabla u \\cdot \\nabla v \\, \\mathrm{d}x,\n$$\nwhere $K = F(\\widehat{K})$ and $\\widehat{K} = [-1,1]^{2}$ is the reference square. The mapping $F : \\widehat{K} \\to K$ is a polynomial isoparametric mapping of degree $k_{g}$, and the trial and test spaces are tensor-product polynomial spaces of degree $p$ on $\\widehat{K}$, that is, $u \\circ F, v \\circ F \\in \\mathbb{Q}_{p}(\\widehat{K})$, the space of polynomials of degree at most $p$ in each reference coordinate. The pullback of the bilinear form to the reference domain is\n$$\na_{K}(u,v) \\;=\\; \\int_{\\widehat{K}} \\big(\\nabla_{\\xi} \\widehat{u}\\big)^{\\top} \\, \\big(J^{-1} J^{-T}\\big) \\, \\nabla_{\\xi} \\widehat{v} \\; |\\det J| \\, \\mathrm{d}\\xi,\n$$\nwhere $\\widehat{u} = u \\circ F$, $\\widehat{v} = v \\circ F$, $J = \\nabla_{\\xi} F$ is the Jacobian matrix of $F$, and $\\nabla_{\\xi}$ denotes the gradient with respect to reference coordinates.\n\nAssume the element integrals are evaluated by tensor-product Gauss–Legendre quadrature on $\\widehat{K}$ with $q$ points per coordinate direction. Determine the minimal integer $q$ (as a function of $p$) that guarantees exact evaluation of $a_{K}(u,v)$ for all $u, v \\in \\mathbb{Q}_{p}(\\widehat{K})$ in the case of an affine geometry mapping, that is, $k_{g} = 1$. Justify the degree counting that leads to your choice of $q$ by starting from the chain rule and the polynomial degree structure of $\\mathbb{Q}_{p}(\\widehat{K})$ and of the Jacobian-based factors. In addition, briefly determine whether such a finite $q$ exists for nonaffine mappings with $k_{g} \\ge 2$, and explain your reasoning.\n\nYour final answer must be a single closed-form expression in terms of $p$ for the affine case $k_{g} = 1$.", "solution": "The problem asks for the minimal number of Gauss-Legendre quadrature points, $q$, per dimension required for the exact evaluation of a scalar diffusion bilinear form on an isoparametric quadrilateral element. The analysis is divided into two cases: an affine mapping and a non-affine mapping.\n\nFirst, we must clarify the meaning of \"affine geometry mapping, that is, $k_g = 1$\". An isoparametric mapping of degree $k_g=1$ on a reference square $\\widehat{K}=[-1,1]^2$ is a bilinear mapping, where each component of the map $F$ is in the space $\\mathbb{Q}_1(\\widehat{K})$. The Jacobian $J$ of a general bilinear map is not constant; its entries are linear polynomials in the reference coordinates $\\xi = (\\xi_1, \\xi_2)$. However, an affine map of the form $F(\\xi) = A\\xi+b$ has a constant Jacobian matrix $J=A$. A bilinear map reduces to an affine map if and only if the physical element $K$ is a parallelogram. In this case, the integrand of the pulled-back bilinear form becomes a polynomial, which can be integrated exactly by Gauss quadrature. For a general (non-parallelogram) bilinear element, the Jacobian $J$ is not constant, its determinant $\\det J$ is not constant, and consequently, the term $(J^{-1}J^{-T})|\\det J|$ is a matrix of rational functions. Gauss quadrature cannot exactly integrate general rational functions. Therefore, the phrase \"affine geometry mapping\" must be interpreted as restricting the case $k_g=1$ to mappings with a constant Jacobian, i.e., elements that are parallelograms.\n\n**Case 1: Affine Mapping ($k_g = 1$, constant Jacobian)**\n\nThe bilinear form pulled back to the reference element $\\widehat{K}$ is:\n$$\na_{K}(u,v) = \\int_{\\widehat{K}} \\underbrace{\\big(\\nabla_{\\xi} \\widehat{u}\\big)^{\\top} \\, \\big(J^{-1} J^{-T}\\big) \\, \\nabla_{\\xi} \\widehat{v} \\; |\\det J|}_{I(\\xi_1, \\xi_2)} \\, \\mathrm{d}\\xi_1 \\mathrm{d}\\xi_2\n$$\nWe need to determine the maximum polynomial degree of the integrand $I(\\xi_1, \\xi_2)$ in each coordinate direction $\\xi_1$ and $\\xi_2$.\n\n1.  **Geometric Factors**: For an affine mapping, the Jacobian matrix $J = \\nabla_{\\xi} F$ is a constant $2 \\times 2$ matrix. Consequently, its inverse $J^{-1}$, its transpose inverse $J^{-T}$, and its determinant $\\det J$ are also constants. The entire geometric factor, $G = (J^{-1} J^{-T}) |\\det J|$, is a constant matrix.\n\n2.  **Trial and Test Function Factors**: The trial function $u$ and test function $v$ are such that their pullbacks $\\widehat{u} = u \\circ F$ and $\\widehat{v} = v \\circ F$ are elements of $\\mathbb{Q}_{p}(\\widehat{K})$. This is the space of tensor-product polynomials of degree at most $p$ in each variable, i.e., $\\widehat{u}(\\xi_1, \\xi_2) = \\sum_{i=0}^{p} \\sum_{j=0}^{p} c_{ij} \\xi_1^i \\xi_2^j$.\n\n3.  **Degree of the Gradients**: We analyze the polynomial degree of the components of the gradient $\\nabla_{\\xi} \\widehat{u} = (\\frac{\\partial \\widehat{u}}{\\partial \\xi_1}, \\frac{\\partial \\widehat{u}}{\\partial \\xi_2})^{\\top}$.\n    -   The partial derivative with respect to $\\xi_1$, $\\frac{\\partial \\widehat{u}}{\\partial \\xi_1}$, is obtained by differentiating $\\widehat{u}$ with respect to $\\xi_1$. This reduces the maximum degree in $\\xi_1$ by $1$ while leaving the maximum degree in $\\xi_2$ unchanged. Thus, $\\frac{\\partial \\widehat{u}}{\\partial \\xi_1}$ is a polynomial of degree at most $p-1$ in $\\xi_1$ and at most $p$ in $\\xi_2$. We denote this space as $\\mathbb{Q}_{p-1, p}$.\n    -   Similarly, the partial derivative $\\frac{\\partial \\widehat{u}}{\\partial \\xi_2}$ is a polynomial of degree at most $p$ in $\\xi_1$ and at most $p-1$ in $\\xi_2$, belonging to the space $\\mathbb{Q}_{p, p-1}$.\n    The same holds for the gradient of $\\widehat{v}$.\n\n4.  **Degree of the Integrand**: The integrand can be expanded as:\n    $$\n    I(\\xi) = G_{11} \\frac{\\partial \\widehat{u}}{\\partial \\xi_1} \\frac{\\partial \\widehat{v}}{\\partial \\xi_1} + G_{12} \\frac{\\partial \\widehat{u}}{\\partial \\xi_1} \\frac{\\partial \\widehat{v}}{\\partial \\xi_2} + G_{21} \\frac{\\partial \\widehat{u}}{\\partial \\xi_2} \\frac{\\partial \\widehat{v}}{\\partial \\xi_1} + G_{22} \\frac{\\partial \\widehat{u}}{\\partial \\xi_2} \\frac{\\partial \\widehat{v}}{\\partial \\xi_2}\n    $$\n    Since the $G_{ij}$ are constants, the degree of each term is determined by the products of the derivatives. Let's find the maximum degree of these products in each variable:\n    -   $\\frac{\\partial \\widehat{u}}{\\partial \\xi_1} \\frac{\\partial \\widehat{v}}{\\partial \\xi_1}$: Degree in $\\xi_1$ is at most $(p-1)+(p-1) = 2p-2$. Degree in $\\xi_2$ is at most $p+p=2p$.\n    -   $\\frac{\\partial \\widehat{u}}{\\partial \\xi_1} \\frac{\\partial \\widehat{v}}{\\partial \\xi_2}$: Degree in $\\xi_1$ is at most $(p-1)+p = 2p-1$. Degree in $\\xi_2$ is at most $p+(p-1)=2p-1$.\n    -   $\\frac{\\partial \\widehat{u}}{\\partial \\xi_2} \\frac{\\partial \\widehat{v}}{\\partial \\xi_1}$: Degree in $\\xi_1$ is at most $p+(p-1) = 2p-1$. Degree in $\\xi_2$ is at most $(p-1)+p=2p-1$.\n    -   $\\frac{\\partial \\widehat{u}}{\\partial \\xi_2} \\frac{\\partial \\widehat{v}}{\\partial \\xi_2}$: Degree in $\\xi_1$ is at most $p+p = 2p$. Degree in $\\xi_2$ is at most $(p-1)+(p-1)=2p-2$.\n\n    The sum of these terms, $I(\\xi_1, \\xi_2)$, is a polynomial. The maximum degree in $\\xi_1$ is $\\max(2p-2, 2p-1, 2p) = 2p$. The maximum degree in $\\xi_2$ is $\\max(2p, 2p-1, 2p-2) = 2p$. Therefore, the integrand $I(\\xi_1, \\xi_2)$ is a polynomial in the space $\\mathbb{Q}_{2p}(\\widehat{K})$.\n\n5.  **Quadrature Rule Requirement**: A one-dimensional Gauss-Legendre quadrature rule with $q$ points integrates polynomials of degree up to $2q-1$ exactly. For the tensor-product rule to be exact for the two-dimensional integral, the one-dimensional rule must be exact for the integrand's maximum polynomial degree in each respective dimension. Thus, we require the rule to be exact for polynomials of degree up to $2p$. The condition is:\n    $$\n    2q - 1 \\ge 2p\n    $$\n    Solving for $q$, we get $2q \\ge 2p+1$, or $q \\ge p + \\frac{1}{2}$. Since $q$ must be an integer, the minimal value for $q$ is $p+1$.\n\n**Case 2: Non-affine Mappings ($k_g \\ge 2$)**\n\nFor a non-affine polynomial mapping of degree $k_g \\ge 2$, or even for a general bilinear mapping ($k_g=1$ on a non-parallelogram element), the Jacobian matrix $J$ is not constant.\n-   The components of $F$ are polynomials in $\\xi_1, \\xi_2$ of degree up to $k_g$ in each variable.\n-   The components of $J = \\nabla_\\xi F$ are polynomials of degree up to $k_g-1$ in one variable and $k_g$ in the other.\n-   The determinant, $\\det J$, is therefore a non-constant polynomial in $\\xi_1, \\xi_2$.\n-   The inverse of the Jacobian, $J^{-1} = \\frac{1}{\\det J} \\text{adj}(J)$, has entries that are rational functions (a ratio of two polynomials), because $\\det J$ appears in the denominator.\n-   The full integrand $I(\\xi)$ is a product of the polynomial terms from the gradients $(\\nabla_\\xi \\widehat{u}, \\nabla_\\xi \\widehat{v})$ and the rational function terms from the geometric factor $(J^{-1}J^{-T})|\\det J|$. The resulting integrand is, in general, a rational function, not a polynomial.\n\nGauss-Legendre quadrature is designed for the exact integration of polynomials. It cannot exactly integrate a general rational function with any finite number of points. Therefore, for non-affine mappings, exact integration is not possible with a finite quadrature rule. A finite value for $q$ that guarantees exactness for all functions $\\widehat{u}, \\widehat{v} \\in \\mathbb{Q}_p$ and all non-affine mappings of degree $k_g \\ge 2$ does not exist. In practice, one must use a sufficiently high-order quadrature rule to approximate the integral to a desired accuracy, a method known as \"over-integration\".\n\nThe final answer required is for the affine case.", "answer": "$$\n\\boxed{p+1}\n$$", "id": "3439558"}, {"introduction": "Building on the principles of integration, this exercise explores a more subtle consequence of how we handle the geometric terms arising from coordinate mappings. We will analytically compute the \"symmetry defect\" that arises from an inconsistent approximation of the Jacobian matrix $J$ within the bilinear form. This practice provides a concrete demonstration of why a rigorous and consistent treatment of the pullback is essential to preserve fundamental algebraic properties, such as the symmetry of the resulting stiffness matrix [@problem_id:3439550].", "problem": "Consider a single isoparametric element with reference domain $\\hat K = [0,1]^{2}$ and a $C^{1}$ isoparametric map $F_{K}:\\hat K \\to K$ with Jacobian matrix $J(\\hat{\\boldsymbol{x}})$, where $\\hat{\\boldsymbol{x}} = (\\xi,\\eta)$. For a scalar second-order elliptic partial differential equation (PDE), the element bilinear form is defined by $a_{K}(u,v) = \\int_{K} \\nabla_{x} u \\cdot \\nabla_{x} v \\, \\mathrm{d}x$ for sufficiently smooth functions $u$ and $v$ on $K$. Starting only from the chain rule and the change-of-variables formula, derive the expression of the pullback bilinear form on the reference element $\\hat K$ in terms of $J(\\hat{\\boldsymbol{x}})$ and gradients with respect to the reference coordinates.\n\nNext, define the specific isoparametric map $F_{K}(\\xi,\\eta) = \\big(\\xi + \\gamma\\,\\xi\\eta,\\;\\eta\\big)$ with parameter $\\gamma \\in \\mathbb{R}$ satisfying $1+\\gamma\\eta0$ for all $\\eta\\in[0,1]$ (to ensure orientation preservation). Let $\\hat J = J(\\xi_{0},\\eta_{0})$ be a constant approximate Jacobian formed by freezing $J$ at the element center $(\\xi_{0},\\eta_{0}) = \\left(\\tfrac{1}{2},\\tfrac{1}{2}\\right)$. Consider the inconsistent approximate pullback in which the metric tensor is formed by mixing the approximate inverse on the “trial” side with the exact inverse on the “test” side, namely\n$$\nG_{\\mathrm{appr}}(\\hat{\\boldsymbol{x}}) = \\det\\big(J(\\hat{\\boldsymbol{x}})\\big)\\,\\hat J^{-1}\\,J(\\hat{\\boldsymbol{x}})^{-T},\n$$\nand the resulting approximate bilinear form\n$$\na_{\\mathrm{appr}}(u,v) = \\int_{\\hat K} \\nabla_{\\hat{\\boldsymbol{x}}} u(\\hat{\\boldsymbol{x}})^{\\!T}\\,G_{\\mathrm{appr}}(\\hat{\\boldsymbol{x}})\\,\\nabla_{\\hat{\\boldsymbol{x}}} v(\\hat{\\boldsymbol{x}})\\,\\mathrm{d}\\hat{\\boldsymbol{x}}.\n$$\nUsing the test functions $u(\\xi,\\eta) = \\xi$ and $v(\\xi,\\eta) = \\eta^{2}$, compute the analytic value of the symmetry defect\n$$\na_{\\mathrm{appr}}(u,v)\\;-\\;a_{\\mathrm{appr}}(v,u)\n$$\nas a closed-form expression in terms of $\\gamma$. No numerical rounding is required; provide the exact expression as your final answer.", "solution": "The problem consists of two parts. First, we derive the general pullback of the bilinear form, and second, we compute the symmetry defect for a specific approximate form.\n\n**Part 1: Derivation of the Pullback Bilinear Form**\n\nLet $\\hat{u}(\\hat{\\boldsymbol{x}}) = u(F_K(\\hat{\\boldsymbol{x}}))$ and $\\hat{v}(\\hat{\\boldsymbol{x}}) = v(F_K(\\hat{\\boldsymbol{x}}))$ be the compositions of the functions $u$ and $v$ with the mapping $F_K$. The gradient of a function with respect to the physical coordinates $\\boldsymbol{x}$ is denoted $\\nabla_x$, and with respect to reference coordinates $\\hat{\\boldsymbol{x}}$ is denoted $\\nabla_{\\hat{\\boldsymbol{x}}}$. The chain rule relates these gradients:\n$$\n\\nabla_{\\hat{\\boldsymbol{x}}} \\hat{u} = J(\\hat{\\boldsymbol{x}})^T \\nabla_x u\n$$\nwhere $J(\\hat{\\boldsymbol{x}})$ is the Jacobian matrix of the transformation $F_K$. To express the physical gradient in terms of the reference gradient, we invert this relationship:\n$$\n\\nabla_x u = J(\\hat{\\boldsymbol{x}})^{-T} \\nabla_{\\hat{\\boldsymbol{x}}} \\hat{u}\n$$\nThe bilinear form on the physical element $K$ is given by $a_{K}(u,v) = \\int_{K} \\nabla_{x} u \\cdot \\nabla_{x} v \\, \\mathrm{d}x$. We can rewrite the dot product using matrix notation as $(\\nabla_x u)^T (\\nabla_x v)$. Substituting the expression for the physical gradients, we get:\n$$\n\\nabla_{x} u \\cdot \\nabla_{x} v = \\left( J^{-T} \\nabla_{\\hat{\\boldsymbol{x}}} \\hat{u} \\right)^T \\left( J^{-T} \\nabla_{\\hat{\\boldsymbol{x}}} \\hat{v} \\right) = (\\nabla_{\\hat{\\boldsymbol{x}}} \\hat{u})^T J^{-1} J^{-T} (\\nabla_{\\hat{\\boldsymbol{x}}} \\hat{v})\n$$\nNext, we apply the change-of-variables formula for integration, which relates the differential volume elements:\n$$\n\\mathrm{d}x = \\det(J(\\hat{\\boldsymbol{x}})) \\, \\mathrm{d}\\hat{\\boldsymbol{x}}\n$$\nSubstituting these into the definition of $a_K(u,v)$, we obtain the pullback bilinear form $\\hat{a}_K(\\hat{u},\\hat{v})$ on the reference element $\\hat{K}$:\n$$\n\\hat{a}_K(\\hat{u},\\hat{v}) = \\int_{\\hat{K}} (\\nabla_{\\hat{\\boldsymbol{x}}} \\hat{u})^T \\left( \\det(J) J^{-1} J^{-T} \\right) (\\nabla_{\\hat{\\boldsymbol{x}}} \\hat{v}) \\, \\mathrm{d}\\hat{\\boldsymbol{x}}\n$$\nThe matrix $G(\\hat{\\boldsymbol{x}}) = \\det(J(\\hat{\\boldsymbol{x}})) J(\\hat{\\boldsymbol{x}})^{-1} J(\\hat{\\boldsymbol{x}})^{-T}$ is often called the metric tensor transformed to reference coordinates.\n\n**Part 2: Calculation of the Symmetry Defect**\n\nWe are given the specific map $F_{K}(\\xi,\\eta) = (x_1, x_2) = (\\xi + \\gamma\\xi\\eta, \\eta)$.\n\n1.  **Compute the Jacobian $J(\\hat{\\boldsymbol{x}})$ and related quantities.**\n    The Jacobian matrix is $J = \\begin{pmatrix} \\frac{\\partial x_1}{\\partial \\xi}  \\frac{\\partial x_1}{\\partial \\eta} \\\\ \\frac{\\partial x_2}{\\partial \\xi}  \\frac{\\partial x_2}{\\partial \\eta} \\end{pmatrix} = \\begin{pmatrix} 1+\\gamma\\eta  \\gamma\\xi \\\\ 0  1 \\end{pmatrix}$.\n    The determinant is $\\det(J) = 1+\\gamma\\eta$. The condition $1+\\gamma\\eta0$ ensures the map is orientation-preserving.\n    The inverse transpose is $J^{-T} = \\left( \\frac{1}{1+\\gamma\\eta} \\begin{pmatrix} 1  -\\gamma\\xi \\\\ 0  1+\\gamma\\eta \\end{pmatrix} \\right)^T = \\begin{pmatrix} \\frac{1}{1+\\gamma\\eta}  0 \\\\ \\frac{-\\gamma\\xi}{1+\\gamma\\eta}  1 \\end{pmatrix}$.\n\n2.  **Compute the constant approximate Jacobian $\\hat{J}$ and its inverse.**\n    $\\hat{J}$ is $J$ evaluated at $(\\xi_0, \\eta_0) = (\\frac{1}{2}, \\frac{1}{2})$:\n    $\\hat{J} = J(\\tfrac{1}{2}, \\tfrac{1}{2}) = \\begin{pmatrix} 1+\\frac{\\gamma}{2}  \\frac{\\gamma}{2} \\\\ 0  1 \\end{pmatrix}$.\n    The inverse of $\\hat{J}$ is $\\hat{J}^{-1} = \\frac{1}{1+\\gamma/2} \\begin{pmatrix} 1  -\\frac{\\gamma}{2} \\\\ 0  1+\\frac{\\gamma}{2} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{1+\\gamma/2}  \\frac{-\\gamma/2}{1+\\gamma/2} \\\\ 0  1 \\end{pmatrix}$.\n\n3.  **Construct the approximate metric tensor $G_{\\mathrm{appr}}(\\hat{\\boldsymbol{x}})$.**\n    $G_{\\mathrm{appr}} = \\det(J) \\hat{J}^{-1} J^{-T}$. After substituting the terms and multiplying, we get:\n    $$\n    G_{\\mathrm{appr}} = \\frac{1}{1+\\gamma/2} \\begin{pmatrix} 1+\\gamma^2\\xi/2  -\\frac{\\gamma}{2}(1+\\gamma\\eta) \\\\ -(1+\\gamma/2)\\gamma\\xi  (1+\\gamma/2)(1+\\gamma\\eta) \\end{pmatrix}\n    $$\n    The matrix $G_{\\mathrm{appr}}$ is not symmetric, which is the origin of the defect.\n\n4.  **Compute the integrands for $a_{\\mathrm{appr}}(u,v)$ and $a_{\\mathrm{appr}}(v,u)$.**\n    The test functions are $u(\\xi, \\eta) = \\xi$ and $v(\\xi, \\eta) = \\eta^2$. Their gradients are:\n    $$\n    \\nabla_{\\hat{\\boldsymbol{x}}} u = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}, \\quad \\nabla_{\\hat{\\boldsymbol{x}}} v = \\begin{pmatrix} 0 \\\\ 2\\eta \\end{pmatrix}\n    $$\n    The integrand for $a_{\\mathrm{appr}}(u,v)$ is $(\\nabla_{\\hat{\\boldsymbol{x}}}u)^T G_{\\mathrm{appr}} (\\nabla_{\\hat{\\boldsymbol{x}}}v)$:\n    $$\n    \\begin{pmatrix} 1  0 \\end{pmatrix} G_{\\mathrm{appr}} \\begin{pmatrix} 0 \\\\ 2\\eta \\end{pmatrix} = 2\\eta \\cdot (G_{\\mathrm{appr}})_{12} = 2\\eta \\left( \\frac{-\\frac{\\gamma}{2}(1+\\gamma\\eta)}{1+\\gamma/2} \\right) = \\frac{-\\gamma\\eta(1+\\gamma\\eta)}{1+\\gamma/2}\n    $$\n    The integrand for $a_{\\mathrm{appr}}(v,u)$ is $(\\nabla_{\\hat{\\boldsymbol{x}}}v)^T G_{\\mathrm{appr}} (\\nabla_{\\hat{\\boldsymbol{x}}}u)$:\n    $$\n    \\begin{pmatrix} 0  2\\eta \\end{pmatrix} G_{\\mathrm{appr}} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = 2\\eta \\cdot (G_{\\mathrm{appr}})_{21} = 2\\eta \\left( - \\gamma\\xi \\right) = -2\\gamma\\xi\\eta\n    $$\n\n5.  **Evaluate the integrals.**\n    The domain of integration is $\\hat{K} = [0,1]^2$.\n    $$\n    a_{\\mathrm{appr}}(u,v) = \\int_0^1 \\int_0^1 \\frac{-\\gamma(\\eta+\\gamma\\eta^2)}{1+\\gamma/2} \\,d\\xi d\\eta = \\frac{-\\gamma}{1+\\gamma/2} \\left( \\int_0^1 d\\xi \\right) \\left( \\int_0^1 (\\eta+\\gamma\\eta^2)d\\eta \\right)\n    $$\n    $$\n    a_{\\mathrm{appr}}(u,v) = \\frac{-\\gamma}{1+\\gamma/2} (1) \\left[ \\frac{\\eta^2}{2} + \\frac{\\gamma\\eta^3}{3} \\right]_0^1 = \\frac{-\\gamma}{1+\\gamma/2} \\left( \\frac{1}{2} + \\frac{\\gamma}{3} \\right)\n    $$\n    $$\n    a_{\\mathrm{appr}}(v,u) = \\int_0^1 \\int_0^1 -2\\gamma\\xi\\eta \\,d\\xi d\\eta = -2\\gamma \\left( \\int_0^1 \\xi d\\xi \\right) \\left( \\int_0^1 \\eta d\\eta \\right)\n    $$\n    $$\n    a_{\\mathrm{appr}}(v,u) = -2\\gamma \\left( \\frac{1}{2} \\right) \\left( \\frac{1}{2} \\right) = -\\frac{\\gamma}{2}\n    $$\n\n6.  **Compute the symmetry defect.**\n    The defect is $a_{\\mathrm{appr}}(u,v) - a_{\\mathrm{appr}}(v,u)$.\n    $$\n    \\text{Defect} = \\frac{-\\gamma}{1+\\gamma/2} \\left( \\frac{1}{2} + \\frac{\\gamma}{3} \\right) - \\left(-\\frac{\\gamma}{2}\\right) = \\frac{-\\gamma(\\frac{3+2\\gamma}{6})}{\\frac{2+\\gamma}{2}} + \\frac{\\gamma}{2}\n    $$\n    $$\n    \\text{Defect} = \\frac{-2\\gamma(3+2\\gamma)}{6(2+\\gamma)} + \\frac{\\gamma}{2} = \\frac{-\\gamma(3+2\\gamma)}{3(2+\\gamma)} + \\frac{\\gamma}{2}\n    $$\n    Bringing to a common denominator of $6(2+\\gamma)$:\n    $$\n    \\text{Defect} = \\frac{-2\\gamma(3+2\\gamma) + 3\\gamma(2+\\gamma)}{6(2+\\gamma)} = \\frac{-6\\gamma - 4\\gamma^2 + 6\\gamma + 3\\gamma^2}{6(2+\\gamma)}\n    $$\n    $$\n    \\text{Defect} = \\frac{-\\gamma^2}{6(2+\\gamma)}\n    $$\n    As a check, if $\\gamma=0$, the defect is $0$, which is expected as the map becomes the identity and all Jacobians are the identity matrix, making $G_{\\mathrm{appr}}$ symmetric.", "answer": "$$\\boxed{\\frac{-\\gamma^2}{6(2+\\gamma)}}$$", "id": "3439550"}, {"introduction": "Our final practice connects the geometry of the mapping to the conditioning and stability of the final algebraic system, a topic of immense practical importance. By analyzing how the mass $M(\\varepsilon)$ and stiffness $K(\\varepsilon)$ matrices scale under uniform element compression, we will uncover the direct relationship between element distortion and the stability constraints of explicit time-integration schemes. This exercise reveals how poorly shaped elements can severely limit computational efficiency, linking abstract geometric concepts to tangible performance metrics in simulations [@problem_id:3439531].", "problem": "Consider the scalar diffusion equation in two spatial dimensions, posed on a single finite element, and its semi-discrete finite element method (FEM) formulation. Let the reference element be the square $\\hat K = [-1,1]^2$ with bilinear shape functions $\\{\\hat N_i\\}_{i=1}^4$ corresponding to the four corners. Use an isoparametric mapping $F_\\varepsilon : \\hat K \\to K_\\varepsilon$ defined by $x = F_\\varepsilon(\\hat x) = s(\\varepsilon) \\hat x$ with $s(\\varepsilon) = \\sqrt{\\varepsilon}$, where $\\varepsilon \\in (0,1]$ is a given parameter. This mapping yields a Jacobian matrix $J(\\hat x) = s(\\varepsilon) I$, where $I$ is the identity matrix, so that $\\det J(\\hat x) = \\varepsilon$ for all $\\hat x \\in \\hat K$, and therefore $\\min_{\\hat K} \\det J = \\varepsilon$. Although this mapping is affine, interpret it as a limiting case of a curved element that is uniformly compressed to achieve the same minimum Jacobian determinant.\n\nStarting from the standard definitions of the local mass matrix $M(\\varepsilon)$ and stiffness matrix $K(\\varepsilon)$ under an isoparametric mapping,\n$$\nM_{ij}(\\varepsilon) = \\int_{K_\\varepsilon} N_i(x) N_j(x) \\, dx = \\int_{\\hat K} \\hat N_i(\\hat x) \\hat N_j(\\hat x)\\, \\det J(\\hat x) \\, d\\hat x,\n$$\n$$\nK_{ij}(\\varepsilon) = \\int_{K_\\varepsilon} \\nabla N_i(x) \\cdot \\nabla N_j(x) \\, dx = \\int_{\\hat K} \\hat \\nabla \\hat N_i(\\hat x)^{\\top} \\Big(J(\\hat x)^{-1} J(\\hat x)^{-T}\\Big) \\hat \\nabla \\hat N_j(\\hat x) \\, \\det J(\\hat x)\\, d\\hat x,\n$$\nderive from first principles how the spectra of $M(\\varepsilon)$ and $K(\\varepsilon)$ scale with $\\varepsilon$. In particular, use the gradient transformation $\\nabla N = J^{-T} \\hat \\nabla \\hat N$ and the change-of-variables formula to show the asymptotic behaviors $M(\\varepsilon) = \\mathcal{O}(\\varepsilon)$ and $K(\\varepsilon) = \\mathcal{O}(1)$ as $\\varepsilon \\to 0^+$.\n\nThen, consider the semi-discrete system for the diffusion equation with unit diffusivity,\n$$\nM(\\varepsilon)\\, \\frac{d u}{d t} + K(\\varepsilon)\\, u = 0,\n$$\nand explain the explicit Forward Euler (FE) time step constraint in terms of the spectral radius of $M(\\varepsilon)^{-1} K(\\varepsilon)$. Show how the largest stable time step $\\Delta t_{\\max}$ scales with $\\varepsilon$.\n\nYour program must:\n- Assemble $M(\\varepsilon)$ and $K(\\varepsilon)$ using Gaussian quadrature on $\\hat K$ with bilinear shape functions $\\{\\hat N_i\\}_{i=1}^4$ and the mapping $F_\\varepsilon(\\hat x) = s(\\varepsilon) \\hat x$.\n- Compute, for each test case, the following three quantities:\n  1. The ratio $\\lambda_{\\max}\\big(M(\\varepsilon)\\big)/\\varepsilon$, where $\\lambda_{\\max}$ denotes the largest eigenvalue.\n  2. The largest eigenvalue $\\lambda_{\\max}\\big(K(\\varepsilon)\\big)$.\n  3. The largest stable Forward Euler time step $\\Delta t_{\\max} = \\frac{2}{\\rho\\big(M(\\varepsilon)^{-1} K(\\varepsilon)\\big)}$, where $\\rho(\\cdot)$ denotes the spectral radius.\n- Use double-precision floating-point arithmetic and report all quantities as dimensionless floats.\n\nTest suite:\n- Evaluate the program for the parameter values $\\varepsilon \\in \\{10^{0}, 10^{-2}, 10^{-6}, 10^{-10}\\}$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is the list $[\\lambda_{\\max}(M(\\varepsilon))/\\varepsilon,\\ \\lambda_{\\max}(K(\\varepsilon)),\\ \\Delta t_{\\max}]$ for the corresponding test case. For example, an output with two cases should look like $[[a_1,b_1,c_1],[a_2,b_2,c_2]]$, but with actual numeric values.", "solution": "### **Theoretical Derivation**\n\nThe core of the problem is to understand how the geometry of a finite element, parameterized by $\\varepsilon$, affects the algebraic properties of the discrete system.\n\n**1. Scaling of the Mass Matrix $M(\\varepsilon)$**\n\nThe local mass matrix $M(\\varepsilon)$ is defined by the integral over the reference element $\\hat K$:\n$$\nM_{ij}(\\varepsilon) = \\int_{\\hat K} \\hat N_i(\\hat x) \\hat N_j(\\hat x)\\, \\det J(\\hat x) \\, d\\hat x\n$$\nFrom the problem statement, the Jacobian determinant is constant, $\\det J(\\hat x) = \\varepsilon$. We can factor this constant out of the integral:\n$$\nM_{ij}(\\varepsilon) = \\varepsilon \\int_{\\hat K} \\hat N_i(\\hat x) \\hat N_j(\\hat x) \\, d\\hat x\n$$\nThe remaining integral is independent of $\\varepsilon$. It defines the reference mass matrix, $\\hat M$:\n$$\n\\hat M_{ij} = \\int_{\\hat K} \\hat N_i(\\hat x) \\hat N_j(\\hat x) \\, d\\hat x\n$$\nThus, the relationship between the mass matrix on the element $K_\\varepsilon$ and the reference mass matrix is a simple scaling:\n$$\nM(\\varepsilon) = \\varepsilon \\hat M\n$$\nThe eigenvalues of $M(\\varepsilon)$ are directly related to the eigenvalues of $\\hat M$. Let $\\lambda_{\\hat M}$ be an eigenvalue of $\\hat M$. Then $M(\\varepsilon) v = (\\varepsilon \\hat M) v = \\varepsilon (\\lambda_{\\hat M} v) = (\\varepsilon \\lambda_{\\hat M}) v$. This shows that the eigenvalues of $M(\\varepsilon)$ are $\\varepsilon$ times the eigenvalues of $\\hat M$. Consequently, the spectrum of $M(\\varepsilon)$ scales linearly with $\\varepsilon$, and we can write $M(\\varepsilon) = \\mathcal{O}(\\varepsilon)$ as $\\varepsilon \\to 0^+$. The ratio $\\lambda_{\\max}(M(\\varepsilon)) / \\varepsilon$ is equal to the constant $\\lambda_{\\max}(\\hat M)$.\n\n**2. Scaling of the Stiffness Matrix $K(\\varepsilon)$**\n\nThe local stiffness matrix $K(\\varepsilon)$ is given by:\n$$\nK_{ij}(\\varepsilon) = \\int_{\\hat K} \\hat \\nabla \\hat N_i(\\hat x)^{\\top} \\Big(J(\\hat x)^{-1} J(\\hat x)^{-T}\\Big) \\hat \\nabla \\hat N_j(\\hat x) \\, \\det J(\\hat x)\\, d\\hat x\n$$\nWe need to evaluate the geometric term $G(\\hat x) = J(\\hat x)^{-1} J(\\hat x)^{-T}$. The Jacobian matrix is $J(\\hat x) = \\sqrt{\\varepsilon}I$. Its inverse is $J(\\hat x)^{-1} = \\frac{1}{\\sqrt{\\varepsilon}}I$, which is also its own transpose, $J(\\hat x)^{-T} = \\frac{1}{\\sqrt{\\varepsilon}}I$.\nTherefore,\n$$\nG(\\hat x) = \\left(\\frac{1}{\\sqrt{\\varepsilon}}I\\right) \\left(\\frac{1}{\\sqrt{\\varepsilon}}I\\right) = \\frac{1}{\\varepsilon}I\n$$\nSubstituting this and $\\det J(\\hat x) = \\varepsilon$ into the integral for $K_{ij}(\\varepsilon)$:\n$$\nK_{ij}(\\varepsilon) = \\int_{\\hat K} \\hat \\nabla \\hat N_i(\\hat x)^{\\top} \\left(\\frac{1}{\\varepsilon}I\\right) \\hat \\nabla \\hat N_j(\\hat x) \\, (\\varepsilon) \\, d\\hat x\n$$\nThe scalar factors $\\frac{1}{\\varepsilon}$ and $\\varepsilon$ cancel each other out:\n$$\nK_{ij}(\\varepsilon) = \\int_{\\hat K} \\hat \\nabla \\hat N_i(\\hat x)^{\\top} I \\hat \\nabla \\hat N_j(\\hat x) \\, d\\hat x = \\int_{\\hat K} \\hat \\nabla \\hat N_i(\\hat x) \\cdot \\hat \\nabla \\hat N_j(\\hat x) \\, d\\hat x\n$$\nThis expression is the definition of the reference stiffness matrix, $\\hat K$, which is independent of $\\varepsilon$.\n$$\nK(\\varepsilon) = \\hat K\n$$\nTherefore, the stiffness matrix $K(\\varepsilon)$ and its spectrum do not change with $\\varepsilon$. This confirms the asymptotic behavior $K(\\varepsilon) = \\mathcal{O}(1)$ as $\\varepsilon \\to 0^+$. The quantity $\\lambda_{\\max}(K(\\varepsilon))$ is equal to the constant $\\lambda_{\\max}(\\hat K)$.\n\n**3. Scaling of the Maximum Stable Time Step $\\Delta t_{\\max}$**\n\nThe semi-discrete system is given by $M(\\varepsilon)\\, \\frac{d u}{d t} + K(\\varepsilon)\\, u = 0$. Applying the Forward Euler time integration scheme, $u^{n+1} = u^n + \\Delta t \\frac{du}{dt}\\big|_{t=t_n}$, gives:\n$$\nM(\\varepsilon) \\left(\\frac{u^{n+1} - u^n}{\\Delta t}\\right) + K(\\varepsilon) u^n = 0\n$$\nSolving for the new state $u^{n+1}$:\n$$\nu^{n+1} = \\left(I - \\Delta t M(\\varepsilon)^{-1} K(\\varepsilon)\\right) u^n\n$$\nFor stability, the spectral radius of the amplification matrix $A = I - \\Delta t M(\\varepsilon)^{-1} K(\\varepsilon)$ must be no greater than $1$. The eigenvalues of $A$ are $1 - \\Delta t \\lambda_G$, where $\\lambda_G$ are the eigenvalues of the matrix $M(\\varepsilon)^{-1} K(\\varepsilon)$, which are the solutions to the generalized eigenvalue problem $K(\\varepsilon)v = \\lambda_G M(\\varepsilon)v$. Since $M$ and $K$ are symmetric and positive (semi-)definite, the eigenvalues $\\lambda_G$ are real and non-negative.\n\nThe stability condition $|1 - \\Delta t \\lambda_G| \\le 1$ for all $\\lambda_G$ implies $-1 \\le 1 - \\Delta t \\lambda_G \\le 1$, which simplifies to $\\Delta t \\lambda_G \\le 2$. This must hold for the largest eigenvalue, so the constraint is:\n$$\n\\Delta t \\le \\frac{2}{\\max(\\lambda_G)} = \\frac{2}{\\rho(M(\\varepsilon)^{-1} K(\\varepsilon))}\n$$\nThe maximum stable time step is thus $\\Delta t_{\\max} = \\frac{2}{\\rho(M(\\varepsilon)^{-1} K(\\varepsilon))}$.\n\nNow we analyze its scaling with $\\varepsilon$. Using our previous results, $M(\\varepsilon) = \\varepsilon \\hat M$ and $K(\\varepsilon) = \\hat K$. The generalized eigenvalue problem becomes:\n$$\n\\hat K v = \\lambda_G (\\varepsilon \\hat M) v\n$$\nRearranging gives:\n$$\n\\frac{1}{\\varepsilon} \\hat K v = \\lambda_G \\hat M v\n$$\nThis shows that the eigenvalues $\\lambda_G$ of $M(\\varepsilon)^{-1}K(\\varepsilon)$ are equal to $\\frac{1}{\\varepsilon}$ times the eigenvalues of the reference system $\\hat M^{-1}\\hat K$. The spectral radius scales accordingly:\n$$\n\\rho(M(\\varepsilon)^{-1} K(\\varepsilon)) = \\frac{1}{\\varepsilon} \\rho(\\hat M^{-1} \\hat K)\n$$\nSubstituting this into the expression for $\\Delta t_{\\max}$:\n$$\n\\Delta t_{\\max}(\\varepsilon) = \\frac{2}{\\frac{1}{\\varepsilon} \\rho(\\hat M^{-1} \\hat K)} = \\varepsilon \\left(\\frac{2}{\\rho(\\hat M^{-1} \\hat K)}\\right)\n$$\nSince the term in parentheses is a constant, we have shown that $\\Delta t_{\\max}(\\varepsilon) = \\mathcal{O}(\\varepsilon)$. This critical result indicates that as the element becomes smaller (as $\\varepsilon \\to 0$), the stability constraint on the explicit time step becomes proportionally more severe.\n\n### **Numerical Implementation**\n\nThe program will implement the following steps:\n1.  Define the bilinear shape functions $\\{\\hat N_i(\\xi, \\eta)\\}_{i=1}^4$ and their gradients $\\{\\hat \\nabla \\hat N_i(\\xi, \\eta)\\}_{i=1}^4$ on the reference square $\\hat K = [-1,1]^2$.\n2.  For each given $\\varepsilon$, assemble the $4 \\times 4$ matrices $M(\\varepsilon)$ and $K(\\varepsilon)$ using a $2 \\times 2$ Gaussian quadrature rule, which is exact for the polynomial integrands encountered.\n3.  The integrands are evaluated using the derived formulas: the integrand for $M_{ij}$ is $\\varepsilon (\\hat N_i \\hat N_j)$ and the integrand for $K_{ij}$ is $(\\hat \\nabla \\hat N_i \\cdot \\hat \\nabla \\hat N_j)$.\n4.  For each assembled pair $(M(\\varepsilon), K(\\varepsilon))$, compute the required quantities:\n    a. The largest eigenvalue of $M(\\varepsilon)$, divided by $\\varepsilon$. This should be a constant equal to $\\lambda_{\\max}(\\hat M)$.\n    b. The largest eigenvalue of $K(\\varepsilon)$. This should be a constant equal to $\\lambda_{\\max}(\\hat K)$.\n    c. The largest stable time step, $\\Delta t_{\\max}$, by first solving the generalized eigenvalue problem $K(\\varepsilon)v = \\lambda_G M(\\varepsilon)v$ to find $\\rho(M(\\varepsilon)^{-1} K(\\varepsilon))$ and then applying the formula.\n5.  The results for each $\\varepsilon$ are collected and formatted into the specified string output.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import linalg\n\ndef solve():\n    \"\"\"\n    Solves the FEM scaling problem for various values of epsilon.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [1.0, 1e-2, 1e-6, 1e-10]\n\n    def get_bilinear_basis(xi, eta):\n        \"\"\"\n        Evaluates bilinear shape functions and their gradients at a point (xi, eta)\n        in the reference element [-1, 1]^2.\n\n        Node ordering:\n        1: (-1, -1)\n        2: ( 1, -1)\n        3: ( 1,  1)\n        4: (-1,  1)\n\n        Returns:\n            - N: A (4,) array of shape function values.\n            - dN: A (4, 2) array of shape function gradients [d/dxi, d/deta].\n        \"\"\"\n        N = 0.25 * np.array([\n            (1 - xi) * (1 - eta),\n            (1 + xi) * (1 - eta),\n            (1 + xi) * (1 + eta),\n            (1 - xi) * (1 + eta)\n        ])\n\n        dN = 0.25 * np.array([\n            [-(1 - eta), -(1 - xi)],\n            [ (1 - eta), -(1 + xi)],\n            [ (1 + eta),  (1 + xi)],\n            [-(1 + eta),  (1 - xi)]\n        ])\n        return N, dN\n\n    def assemble_matrices(epsilon):\n        \"\"\"\n        Assembles the mass matrix M(epsilon) and stiffness matrix K(epsilon)\n        for a given epsilon using 2x2 Gaussian quadrature.\n        \"\"\"\n        M_eps = np.zeros((4, 4), dtype=np.float64)\n        K_eps = np.zeros((4, 4), dtype=np.float64)\n\n        # 2-point Gauss-Legendre quadrature points and weights\n        quad_points = [-1.0 / np.sqrt(3.0), 1.0 / np.sqrt(3.0)]\n        quad_weights = [1.0, 1.0]\n\n        # Get Jacobian related terms (constant over the element)\n        det_J = epsilon\n        # J_inv_J_invT is (J^-1)(J^-T). For J = sqrt(eps)*I, this is (1/eps)*I.\n        # The geometric factor in the stiffness integral is (J^-1)(J^-T) * det(J)\n        # which is ((1/eps)*I) * eps = I.\n        # This means K(eps) is independent of epsilon.\n\n        for i, weight_i in enumerate(quad_weights):\n            for j, weight_j in enumerate(quad_weights):\n                xi = quad_points[i]\n                eta = quad_points[j]\n                weight = weight_i * weight_j\n\n                N_vals, dN_vals = get_bilinear_basis(xi, eta)\n\n                # Assemble mass matrix M(epsilon)\n                # Integrand is N_i * N_j * det(J)\n                M_integrand = np.outer(N_vals, N_vals)\n                M_eps += M_integrand * det_J * weight\n\n                # Assemble stiffness matrix K(epsilon)\n                # Integrand is (grad_N_i^T * J_inv_J_invT * grad_N_j) * det(J)\n                # which simplifies to (grad_N_i . grad_N_j)\n                K_integrand = dN_vals @ dN_vals.T\n                K_eps += K_integrand * weight\n        \n        return M_eps, K_eps\n\n    results = []\n    for epsilon in test_cases:\n        M, K = assemble_matrices(epsilon)\n\n        # 1. Compute lambda_max(M(epsilon)) / epsilon\n        # Use eigvalsh for symmetric matrices\n        lambda_max_M = linalg.eigvalsh(M).max()\n        ratio_1 = lambda_max_M / epsilon\n\n        # 2. Compute lambda_max(K(epsilon))\n        lambda_max_K = linalg.eigvalsh(K).max()\n\n        # 3. Compute largest stable Forward Euler time step\n        # dt_max = 2 / rho(M^-1 * K)\n        # We solve the generalized eigenvalue problem K*v = lambda*M*v\n        # This is more stable than inverting M.\n        # The eigenvalues are real since K, M are symmetric and M is pos-def.\n        gen_eigenvalues = linalg.eigh(K, M, eigvals_only=True)\n        spectral_radius = np.max(gen_eigenvalues)\n        \n        # Handle the case where the spectral radius is zero (e.g., if K is all zeros)\n        if np.isclose(spectral_radius, 0):\n            dt_max = np.inf\n        else:\n            dt_max = 2.0 / spectral_radius\n        \n        results.append([ratio_1, lambda_max_K, dt_max])\n\n    # Format the final output string exactly as required.\n    formatted_results = [f\"[{r[0]},{r[1]},{r[2]}]\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3439531"}]}