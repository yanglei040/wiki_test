{"hands_on_practices": [{"introduction": "This first exercise provides a foundational walkthrough of convergence analysis for a stationary iterative method. By using the known spectral properties of the discretized 2D Poisson equation, you will determine the precise range of the damping parameter $\\omega$ that guarantees convergence for the weighted Jacobi method. This practice is essential for building the core skill of analytically finding an optimal parameter to maximize the rate of convergence [@problem_id:3437780].", "problem": "Consider the two-dimensional Poisson equation $-\\Delta u = f$ on $(0,1)^{2}$ with homogeneous Dirichlet boundary conditions. Discretize the problem using the standard five-point finite difference stencil on an $N \\times N$ interior grid with mesh spacing $h = \\frac{1}{N+1}$, producing a symmetric positive definite (SPD) linear system $A u = b$. Let $A = D - (L + U)$ denote the usual splitting into diagonal $D$ and strictly lower and upper triangular parts $L$ and $U$. The weighted Jacobi method with damping parameter $\\omega$ applies the iteration $x^{(k+1)} = x^{(k)} + \\omega D^{-1}(b - A x^{(k)})$, whose iteration matrix is $T_{\\omega} = I - \\omega D^{-1} A$.\n\nIt is known that $D = \\frac{4}{h^{2}} I$ and that the eigenvalues of $D^{-1} A$ are indexed by integer mode pairs $(p,q)$ with $1 \\leq p,q \\leq N$, and have the form\n$$\n\\mu_{p,q} = \\sin^{2}\\!\\left(\\alpha_{p}\\right) + \\sin^{2}\\!\\left(\\alpha_{q}\\right), \\quad \\text{where} \\quad \\alpha_{k} = \\frac{\\pi k}{2(N+1)}.\n$$\nFrom these, the extremal bounds on the spectrum are\n$$\n\\mu_{\\min} = 2 \\sin^{2}\\!\\left(\\frac{\\pi}{2(N+1)}\\right), \\qquad \\mu_{\\max} = 2 \\cos^{2}\\!\\left(\\frac{\\pi}{2(N+1)}\\right).\n$$\n\nUsing only these bounds and fundamental properties of the spectral radius, do the following:\n- Derive a provable interval for the damping parameter $\\omega$ that guarantees the convergence of the weighted Jacobi iteration (i.e., $\\rho(T_{\\omega})  1$).\n- Under the optimal choice of $\\omega$ based on the given bounds (chosen to minimize the worst-case spectral radius over the interval $[\\mu_{\\min}, \\mu_{\\max}]$), estimate the worst-case asymptotic convergence factor of the iteration.\n\nExpress your final answer as the exact closed-form expression for the worst-case asymptotic convergence factor as a function of $N$. No rounding is required, and no physical units are involved.", "solution": "The user has provided a problem statement regarding the convergence analysis of the weighted Jacobi method for the discretized 2D Poisson equation.\n\n### Step 1: Extract Givens\n- **Governing Equation  Domain**: $-\\Delta u = f$ on the unit square $(0,1)^{2}$ with homogeneous Dirichlet boundary conditions.\n- **Discretization**: Standard five-point finite difference method on an $N \\times N$ interior grid.\n- **Mesh Spacing**: $h = \\frac{1}{N+1}$.\n- **Linear System**: $A u = b$, where $A$ is symmetric positive definite (SPD).\n- **Matrix Splitting**: $A = D - (L + U)$, where $D$ is the diagonal part of $A$, and $L, U$ are the strictly lower and upper triangular parts, respectively.\n- **Weighted Jacobi Iteration**: $x^{(k+1)} = x^{(k)} + \\omega D^{-1}(b - A x^{(k)})$.\n- **Iteration Matrix**: $T_{\\omega} = I - \\omega D^{-1} A$.\n- **Diagonal Matrix**: $D = \\frac{4}{h^{2}} I$, where $I$ is the identity matrix.\n- **Eigenvalues of $D^{-1} A$**: The eigenvalues are denoted $\\mu_{p,q}$ for $1 \\leq p,q \\leq N$, with the form $\\mu_{p,q} = \\sin^{2}\\!\\left(\\alpha_{p}\\right) + \\sin^{2}\\!\\left(\\alpha_{q}\\right)$, where $\\alpha_{k} = \\frac{\\pi k}{2(N+1)}$.\n- **Extremal Eigenvalues of $D^{-1} A$**: The minimum and maximum eigenvalues are given as:\n  $$ \\mu_{\\min} = 2 \\sin^{2}\\!\\left(\\frac{\\pi}{2(N+1)}\\right) $$\n  $$ \\mu_{\\max} = 2 \\cos^{2}\\!\\left(\\frac{\\pi}{2(N+1)}\\right) $$\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientific or Factual Unsoundness**: The problem is grounded in the standard theory of numerical analysis for partial differential equations. The discretization of the Poisson equation, the formulation of the weighted Jacobi method, and the spectral analysis of convergence are cornerstones of this field. The provided formulas for the eigenvalues of the preconditioned matrix $D^{-1}A$ and their bounds are correct and well-documented for this specific problem. No scientific or mathematical flaws are present.\n2.  **Non-Formalizable or Irrelevant**: The problem is a formal and standard exercise in the rate-of-convergence analysis for stationary iterative methods, which is precisely the topic it purports to be about.\n3.  **Incomplete or Contradictory Setup**: The problem is self-contained. It provides the iteration matrix, the full spectrum of the relevant matrix $D^{-1}A$ (via its extremal bounds), and a clear objective. The information is sufficient and consistent.\n4.  **Unrealistic or Infeasible**: All mathematical conditions and data are standard for this model problem. No unrealistic or infeasible requirements are made.\n5.  **Ill-Posed or Poorly Structured**: The problem is well-posed. It asks for the derivation of a convergence interval and an optimal convergence factor, both of which are uniquely determined by the provided spectral bounds. The terminology is precise and unambiguous.\n6.  **Pseudo-Profound, Trivial, or Tautological**: While the problem relies on provided eigenvalue bounds, the task of deriving the optimal parameter and the corresponding convergence rate requires a substantive chain of reasoning involving spectral radius properties and optimization. It is a non-trivial canonical problem.\n7.  **Outside Scientific Verifiability**: The claims and derivations are fully verifiable using the principles of linear algebra and numerical analysis.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\n### Solution Derivation\nThe convergence of the weighted Jacobi iteration is determined by the spectral radius of the iteration matrix $T_{\\omega} = I - \\omega D^{-1} A$. The iteration converges if and only if $\\rho(T_{\\omega})  1$.\n\nLet $\\mu$ be an eigenvalue of the matrix $D^{-1}A$. Then the corresponding eigenvalue $\\lambda$ of $T_{\\omega}$ is given by:\n$$ \\lambda = 1 - \\omega \\mu $$\nThe spectral radius of $T_{\\omega}$ is the maximum absolute value of its eigenvalues. Since the eigenvalues of $D^{-1}A$ are real and lie in the interval $[\\mu_{\\min}, \\mu_{\\max}]$, the spectral radius is:\n$$ \\rho(T_{\\omega}) = \\max_{\\mu \\in [\\mu_{\\min}, \\mu_{\\max}]} |1 - \\omega \\mu| $$\nFor convergence, we require $\\rho(T_{\\omega})  1$, which translates to the condition:\n$$ -1  1 - \\omega \\mu  1 \\quad \\text{for all } \\mu \\in [\\mu_{\\min}, \\mu_{\\max}] $$\nThe right-hand side inequality, $1 - \\omega \\mu  1$, implies $-\\omega \\mu  0$. Since the matrix $A$ is SPD, its eigenvalues are all positive. The matrix $D$ is a positive scalar multiple of the identity, so $D^{-1}$ is also a positive scalar multiple of the identity. Therefore, the eigenvalues $\\mu$ of $D^{-1}A$ are all positive, meaning $\\mu_{\\min}  0$. The inequality $-\\omega \\mu  0$ thus requires $\\omega  0$.\n\nThe left-hand side inequality, $-1  1 - \\omega \\mu$, implies $\\omega \\mu  2$, or $\\omega  \\frac{2}{\\mu}$. This must hold for all eigenvalues $\\mu$, so it must hold for the largest one, $\\mu_{\\max}$. This gives the condition $\\omega  \\frac{2}{\\mu_{\\max}}$.\n\nCombining these, the interval for $\\omega$ that guarantees convergence is:\n$$ 0  \\omega  \\frac{2}{\\mu_{\\max}} $$\nSubstituting the given expression for $\\mu_{\\max}$:\n$$ 0  \\omega  \\frac{2}{2 \\cos^{2}\\!\\left(\\frac{\\pi}{2(N+1)}\\right)} = \\frac{1}{\\cos^{2}\\!\\left(\\frac{\\pi}{2(N+1)}\\right)} = \\sec^{2}\\!\\left(\\frac{\\pi}{2(N+1)}\\right) $$\nThis completes the first part of the task.\n\nFor the second part, we must find the optimal damping parameter $\\omega_{\\text{opt}}$ that minimizes the spectral radius $\\rho(T_{\\omega})$. The function to be minimized is:\n$$ \\rho(T_{\\omega}) = \\max(|1 - \\omega \\mu_{\\min}|, |1 - \\omega \\mu_{\\max}|) $$\nThe function $g(\\mu) = 1 - \\omega \\mu$ is linear in $\\mu$. The minimum of the maximum of $|g(\\mu)|$ over an interval is achieved when the values at the endpoints have equal magnitude, i.e., $|1 - \\omega_{\\text{opt}} \\mu_{\\min}| = |1 - \\omega_{\\text{opt}} \\mu_{\\max}|$. To achieve the smallest possible magnitude, the function $g(\\mu)$ should have opposite signs at the endpoints. Thus, we set:\n$$ 1 - \\omega_{\\text{opt}} \\mu_{\\min} = -(1 - \\omega_{\\text{opt}} \\mu_{\\max}) $$\n$$ 1 - \\omega_{\\text{opt}} \\mu_{\\min} = -1 + \\omega_{\\text{opt}} \\mu_{\\max} $$\n$$ 2 = \\omega_{\\text{opt}}(\\mu_{\\min} + \\mu_{\\max}) $$\nThis yields the optimal damping parameter:\n$$ \\omega_{\\text{opt}} = \\frac{2}{\\mu_{\\min} + \\mu_{\\max}} $$\nThe corresponding minimal spectral radius, which is the worst-case asymptotic convergence factor, is:\n$$ \\rho(T_{\\omega_{\\text{opt}}}) = |1 - \\omega_{\\text{opt}} \\mu_{\\min}| = \\left|1 - \\frac{2 \\mu_{\\min}}{\\mu_{\\min} + \\mu_{\\max}}\\right| = \\left|\\frac{\\mu_{\\min} + \\mu_{\\max} - 2 \\mu_{\\min}}{\\mu_{\\min} + \\mu_{\\max}}\\right| = \\frac{\\mu_{\\max} - \\mu_{\\min}}{\\mu_{\\max} + \\mu_{\\min}} $$\nThe last step holds because $\\mu_{\\max}  \\mu_{\\min}  0$.\n\nNow, we substitute the given expressions for $\\mu_{\\min}$ and $\\mu_{\\max}$ into this formula.\n$$ \\mu_{\\min} = 2 \\sin^{2}\\!\\left(\\frac{\\pi}{2(N+1)}\\right) $$\n$$ \\mu_{\\max} = 2 \\cos^{2}\\!\\left(\\frac{\\pi}{2(N+1)}\\right) $$\nThe denominator is:\n$$ \\mu_{\\max} + \\mu_{\\min} = 2 \\cos^{2}\\!\\left(\\frac{\\pi}{2(N+1)}\\right) + 2 \\sin^{2}\\!\\left(\\frac{\\pi}{2(N+1)}\\right) = 2\\left(\\cos^{2}\\!\\left(\\frac{\\pi}{2(N+1)}\\right) + \\sin^{2}\\!\\left(\\frac{\\pi}{2(N+1)}\\right)\\right) = 2 $$\nThe numerator is:\n$$ \\mu_{\\max} - \\mu_{\\min} = 2 \\cos^{2}\\!\\left(\\frac{\\pi}{2(N+1)}\\right) - 2 \\sin^{2}\\!\\left(\\frac{\\pi}{2(N+1)}\\right) = 2\\left(\\cos^{2}\\!\\left(\\frac{\\pi}{2(N+1)}\\right) - \\sin^{2}\\!\\left(\\frac{\\pi}{2(N+1)}\\right)\\right) $$\nUsing the double-angle identity $\\cos(2\\theta) = \\cos^{2}(\\theta) - \\sin^{2}(\\theta)$, with $\\theta = \\frac{\\pi}{2(N+1)}$, we get:\n$$ \\mu_{\\max} - \\mu_{\\min} = 2 \\cos\\left(2 \\cdot \\frac{\\pi}{2(N+1)}\\right) = 2 \\cos\\left(\\frac{\\pi}{N+1}\\right) $$\nFinally, the worst-case asymptotic convergence factor is:\n$$ \\rho(T_{\\omega_{\\text{opt}}}) = \\frac{2 \\cos\\left(\\frac{\\pi}{N+1}\\right)}{2} = \\cos\\left(\\frac{\\pi}{N+1}\\right) $$\nThis is the final expression for the convergence factor.", "answer": "$$\n\\boxed{\\cos\\left(\\frac{\\pi}{N+1}\\right)}\n$$", "id": "3437780"}, {"introduction": "Building upon the principles of parameter optimization, this exercise explores the more advanced Successive Over-Relaxation (SOR) method. You will analyze the one-dimensional Poisson problem to derive the optimal relaxation parameter $\\omega_{\\mathrm{opt}}$ that minimizes the spectral radius of the SOR iteration matrix. A key aspect of this practice is to investigate the asymptotic behavior of $\\omega_{\\mathrm{opt}}$ and the convergence rate as the mesh size $h$ approaches zero, a critical consideration in practical scientific computing [@problem_id:3437848].", "problem": "Consider the one-dimensional Poisson boundary value problem $-u''(x)=f(x)$ on $(0,1)$ with homogeneous Dirichlet boundary conditions $u(0)=u(1)=0$. Discretize the problem using the standard second-order centered finite difference method on a uniform grid with $n$ interior points and mesh size $h=\\frac{1}{n+1}$. The resulting linear system is $A \\mathbf{u}=\\mathbf{b}$, where $A\\in \\mathbb{R}^{n\\times n}$ is the tridiagonal matrix with $2/h^{2}$ on the diagonal and $-1/h^{2}$ on the sub- and super-diagonals. Let the Successive Over-Relaxation (SOR) method with relaxation parameter $\\omega\\in(0,2)$ be applied to this system using lexicographic ordering.\n\nStarting from core definitions of stationary iterations, the spectral radius, and standard eigen-analysis for Toeplitz matrices, determine an approximation (valid for all $n\\ge 1$ and asymptotically sharp as $h\\to 0$) to the relaxation parameter $\\omega_{\\mathrm{opt}}(h)$ that minimizes the spectral radius of the SOR iteration matrix. Express your final $\\omega_{\\mathrm{opt}}(h)$ as a closed-form analytic expression in terms of $h$ only, and comment in your derivation on its scaling with $h$ as $h\\to 0$ by extracting the leading-order term. No numerical rounding is required. Your final answer must be a single analytic expression in terms of $h$ only.", "solution": "### Problem Validation\nThe problem is a standard exercise in numerical linear algebra, asking for the derivation of the optimal relaxation parameter for the SOR method applied to the 1D Poisson equation. All aspects of the problem are scientifically sound, well-posed, and self-contained. The objective is clear and the required steps are based on fundamental, verifiable theory. The problem is valid and a full solution can be derived.\n\n### Solution Derivation\nThe convergence properties of stationary iterations are invariant with respect to a scaling of the linear system. The matrix $A$ is given as the tridiagonal matrix with $2/h^2$ on the diagonal and $-1/h^2$ on the off-diagonals. We can analyze the scaled matrix $A' = h^2 A$, which is the simpler tridiagonal matrix with $2$ on the diagonal and $-1$ on the off-diagonals:\n$$\nA' = \\begin{pmatrix} 2  -1   \\\\ -1  2  -1  \\\\  \\ddots  \\ddots  \\ddots \\\\   -1  2 \\end{pmatrix}\n$$\nThe Successive Over-Relaxation (SOR) method's convergence rate is related to the spectral properties of the corresponding Jacobi method. Let's first analyze the Jacobi method for $A'$. The splitting is $A' = D' - (L' + U')$, where $D'=2I$ is the diagonal part. The Jacobi iteration matrix is $T_J = (D')^{-1}(L'+U')$.\n$$\nT_J = \\frac{1}{2} \\begin{pmatrix} 0  1   \\\\ 1  0  1  \\\\  \\ddots  \\ddots  \\ddots \\\\   1  0 \\end{pmatrix}\n$$\nThe eigenvalues of this specific tridiagonal matrix are well-known and are given by:\n$$ \\mu_k = \\cos\\left(\\frac{k\\pi}{n+1}\\right), \\quad k=1, 2, \\ldots, n $$\nThe spectral radius of the Jacobi matrix, $\\rho(T_J)$, is the largest absolute value of these eigenvalues:\n$$ \\rho(T_J) = \\max_{k} |\\mu_k| = \\cos\\left(\\frac{\\pi}{n+1}\\right) $$\nThe matrix $A'$ is a consistently ordered matrix. For such matrices, there is a direct relationship between the spectral radius of the Jacobi matrix $\\rho(T_J)$ and the optimal SOR relaxation parameter $\\omega_{\\text{opt}}$ that minimizes the spectral radius of the SOR iteration matrix. The formula is:\n$$ \\omega_{\\text{opt}} = \\frac{2}{1 + \\sqrt{1 - [\\rho(T_J)]^2}} $$\nSubstituting our expression for $\\rho(T_J)$:\n$$ \\omega_{\\text{opt}} = \\frac{2}{1 + \\sqrt{1 - \\cos^2\\left(\\frac{\\pi}{n+1}\\right)}} $$\nUsing the fundamental trigonometric identity $\\sin^2\\theta + \\cos^2\\theta = 1$, the term under the square root becomes $\\sin^2\\left(\\frac{\\pi}{n+1}\\right)$.\n$$ \\omega_{\\text{opt}} = \\frac{2}{1 + \\sqrt{\\sin^2\\left(\\frac{\\pi}{n+1}\\right)}} = \\frac{2}{1 + \\sin\\left(\\frac{\\pi}{n+1}\\right)} $$\nThe sine term is positive since $1 \\le k \\le n$. The problem asks for the answer in terms of the mesh size $h = \\frac{1}{n+1}$. Substituting this into our expression gives the final form of the optimal parameter:\n$$ \\omega_{\\text{opt}}(h) = \\frac{2}{1 + \\sin(\\pi h)} $$\nThis expression is valid for all $n \\ge 1$ and is asymptotically sharp.\n\nFor the asymptotic behavior as $h \\to 0$, we can use the Taylor series for sine: $\\sin(x) = x - x^3/6 + \\dots$.\n$$ \\omega_{\\text{opt}}(h) \\approx \\frac{2}{1 + \\pi h} $$\nUsing the geometric series expansion $(1+x)^{-1} = 1 - x + x^2 - \\dots$:\n$$ \\omega_{\\text{opt}}(h) \\approx 2(1 - \\pi h + (\\pi h)^2 - \\dots) = 2 - 2\\pi h + \\mathcal{O}(h^2) $$\nThis shows that as the mesh becomes finer ($h \\to 0$), the optimal relaxation parameter approaches $2$, with a leading-order scaling proportional to $h$.", "answer": "$$\n\\boxed{\\frac{2}{1 + \\sin(\\pi h)}}\n$$", "id": "3437848"}, {"introduction": "Our final practice tackles a more complex and realistic scenario: anisotropic diffusion, where standard iterative methods can perform poorly. This problem demonstrates that the effectiveness of a solver like Gauss-Seidel can dramatically depend on the ordering of the grid points when the underlying physical problem has strong directional dependencies. Using the powerful technique of Local Fourier Analysis, you will quantify the performance of different node orderings and discover why specialized strategies, such as line relaxation, are crucial for designing efficient solvers for anisotropic problems [@problem_id:3437865].", "problem": "Consider the two-dimensional constant-coefficient anisotropic diffusion operator discretized on a uniform Cartesian grid with mesh spacing $h$ in both directions, acting on a grid function $u_{i,j}$ as\n$$\nA u_{i,j} \\;=\\; \\alpha \\left(u_{i-1,j} - 2 u_{i,j} + u_{i+1,j}\\right) \\;+\\; \\beta \\left(u_{i,j-1} - 2 u_{i,j} + u_{i,j+1}\\right),\n$$\nwhere $\\alpha = a_{xx}/h^{2}$ and $\\beta = a_{yy}/h^{2}$ with $a_{xx} \\gg a_{yy}$, and define the anisotropy ratio $\\varepsilon = \\beta/\\alpha \\ll 1$. We study the error-propagation of Gauss–Seidel (GS) stationary iterations for $A u = f$ under three node orderings:\n- natural lexicographic row-wise ordering (left-to-right within a row, then row-by-row in the vertical direction),\n- red–black two-color ordering on the standard five-point stencil adjacency,\n- multicolor line relaxation that treats each horizontal line (fixed $j$) as one block with exact block solve and performs a block Gauss–Seidel sweep in the vertical direction.\n\nStarting from the iteration error propagation definition $e^{(k+1)} = G e^{(k)}$, the Gauss–Seidel splitting $A = L + D + U$ with $G = -\\left(D + L\\right)^{-1} U$, and Local Fourier Analysis (LFA) on the infinite grid using Fourier modes $e_{i,j}(\\boldsymbol{\\theta}) = \\exp\\!\\big(i(\\theta_{x} i + \\theta_{y} j)\\big)$ and their symbols, derive the corresponding error symbols $g(\\theta_{x},\\theta_{y})$ for each ordering. Define the high-frequency set\n$$\n\\Theta_{\\mathrm{H}} \\;=\\; \\left\\{ (\\theta_{x},\\theta_{y}) \\in (-\\pi,\\pi]^{2} \\;:\\; |\\theta_{x}| \\in \\left[\\tfrac{\\pi}{2},\\pi\\right] \\;\\text{or}\\; |\\theta_{y}| \\in \\left[\\tfrac{\\pi}{2},\\pi\\right] \\right\\},\n$$\nand the smoothing factor (the high-frequency spectral radius)\n$$\n\\rho_{\\mathrm{H}}(G) \\;=\\; \\sup_{(\\theta_{x},\\theta_{y}) \\in \\Theta_{\\mathrm{H}}} \\left| g(\\theta_{x},\\theta_{y}) \\right|.\n$$\nQuantify $\\rho_{\\mathrm{H}}(G)$ for each ordering as a function of $\\varepsilon$, identify the ordering that minimizes $\\rho_{\\mathrm{H}}(G)$ in the regime $\\varepsilon \\ll 1$, and compute the minimal value of $\\rho_{\\mathrm{H}}(G)$ as an exact closed-form quantity. Express your final answer as a single exact value (no rounding).", "solution": "The problem requires a validation and subsequent analysis of the smoothing properties of Gauss-Seidel iterations for an anisotropic diffusion equation under three different node orderings. The tool for this analysis is Local Fourier Analysis (LFA).\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- Discretized operator: $A u_{i,j} = \\alpha (u_{i-1,j} - 2 u_{i,j} + u_{i+1,j}) + \\beta (u_{i,j-1} - 2 u_{i,j} + u_{i,j+1})$\n- Coefficients: $\\alpha = a_{xx}/h^{2}$, $\\beta = a_{yy}/h^{2}$\n- Anisotropy conditions: $a_{xx} \\gg a_{yy}$, and $\\varepsilon = \\beta/\\alpha \\ll 1$\n- Iteration definition: Error propagation $e^{(k+1)} = G e^{(k)}$, with $G = -(D+L)^{-1}U$ for the Gauss-Seidel splitting $A = L+D+U$.\n- LFA modes: $e_{i,j}(\\boldsymbol{\\theta}) = \\exp(i(\\theta_{x} i + \\theta_{y} j))$ for $\\boldsymbol{\\theta} = (\\theta_x, \\theta_y) \\in (-\\pi, \\pi]^2$.\n- Node orderings: (1) Natural lexicographic, (2) Red-black, (3) Horizontal line relaxation.\n- High-frequency set: $\\Theta_{\\mathrm{H}} = \\{ (\\theta_{x},\\theta_{y}) \\in (-\\pi,\\pi]^{2} : |\\theta_{x}| \\in [\\tfrac{\\pi}{2},\\pi] \\;\\text{or}\\; |\\theta_{y}| \\in [\\tfrac{\\pi}{2},\\pi] \\}$.\n- Smoothing factor: $\\rho_{\\mathrm{H}}(G) = \\sup_{(\\theta_{x},\\theta_{y}) \\in \\Theta_{\\mathrm{H}}} | g(\\theta_{x},\\theta_{y}) |$, where $g(\\boldsymbol{\\theta})$ is the symbol of the error propagation operator $G$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, being a standard topic in numerical analysis for partial differential equations, specifically multigrid methods. All terms are defined precisely, and the setup is self-contained and mathematically consistent. The task is well-posed: to derive and compare the smoothing factors for three classical iteration schemes. The problem does not violate any fundamental principles, is objective, and leads to a unique, verifiable solution.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A complete solution will be provided.\n\n### Derivation of Smoothing Factors\n\nFirst, we find the symbol $a(\\boldsymbol{\\theta})$ of the operator $A$ by applying it to a Fourier mode $e_{i,j}(\\boldsymbol{\\theta})$:\n$$\nA e_{i,j}(\\boldsymbol{\\theta}) = \\left[ \\alpha(e^{-i\\theta_x} - 2 + e^{i\\theta_x}) + \\beta(e^{-i\\theta_y} - 2 + e^{i\\theta_y}) \\right] e_{i,j}(\\boldsymbol{\\theta})\n$$\n$$\na(\\theta_x, \\theta_y) = \\alpha(2\\cos\\theta_x - 2) + \\beta(2\\cos\\theta_y - 2) = -4\\alpha\\sin^2(\\tfrac{\\theta_x}{2}) - 4\\beta\\sin^2(\\tfrac{\\theta_y}{2})\n$$\nThe total diagonal element of the operator $A$ at point $(i,j)$ is $-2(\\alpha+\\beta)$. The error propagation equation for a stationary iteration $e^{(k+1)} = G e^{(k)}$ is analyzed by finding the symbol $g(\\boldsymbol{\\theta})$ of $G$, which is the amplification factor of the mode $\\boldsymbol{\\theta}$.\n\n**1. Lexicographic Gauss-Seidel (Lex-GS)**\nIn the lexicographic ordering (row-wise), the update for the error at node $(i,j)$ uses new values from \"past\" nodes $(i-1,j)$ and $(i,j-1)$:\n$$\n-2(\\alpha+\\beta) e_{i,j}^{(k+1)} + \\alpha e_{i-1,j}^{(k+1)} + \\beta e_{i,j-1}^{(k+1)} + \\alpha e_{i+1,j}^{(k)} + \\beta e_{i,j+1}^{(k)} = 0\n$$\nSubstituting $e_{m,n}^{(k)} = g^k \\exp(i(\\theta_x m + \\theta_y n))$ and dividing by common factors:\n$$\ng(-2(\\alpha+\\beta) + \\alpha e^{-i\\theta_x} + \\beta e^{-i\\theta_y}) + \\alpha e^{i\\theta_x} + \\beta e^{i\\theta_y} = 0\n$$\nSolving for the symbol $g_{\\text{Lex}}(\\boldsymbol{\\theta})$:\n$$\ng_{\\text{Lex}}(\\theta_x, \\theta_y) = -\\frac{\\alpha e^{i\\theta_x} + \\beta e^{i\\theta_y}}{-2(\\alpha+\\beta) + \\alpha e^{-i\\theta_x} - \\beta e^{-i\\theta_y}} = \\frac{\\alpha e^{i\\theta_x} + \\beta e^{i\\theta_y}}{2(\\alpha+\\beta) - \\alpha e^{-i\\theta_x} - \\beta e^{-i\\theta_y}}\n$$\nUsing $\\varepsilon = \\beta/\\alpha$:\n$$\ng_{\\text{Lex}}(\\theta_x, \\theta_y) = \\frac{e^{i\\theta_x} + \\varepsilon e^{i\\theta_y}}{2(1+\\varepsilon) - e^{-i\\theta_x} - \\varepsilon e^{-i\\theta_y}}\n$$\nTo find the smoothing factor, we consider the high-frequency modes that are smoothed the least. For strong anisotropy $\\varepsilon \\ll 1$, the coupling in the $x$-direction dominates. Modes that are smooth in $x$ but oscillatory in $y$ (e.g., $\\theta_x \\approx 0, \\theta_y \\approx \\pi$) are poorly damped. Let's evaluate the amplification factor at $(\\theta_x, \\theta_y) = (0, \\pi) \\in \\Theta_{\\mathrm{H}}$:\n$$\ng_{\\text{Lex}}(0, \\pi) = \\frac{1 + \\varepsilon e^{i\\pi}}{2(1+\\varepsilon) - 1 - \\varepsilon e^{-i\\pi}} = \\frac{1 - \\varepsilon}{2+2\\varepsilon-1+\\varepsilon} = \\frac{1-\\varepsilon}{1+3\\varepsilon}\n$$\nIn the limit $\\varepsilon \\to 0$, $|g_{\\text{Lex}}(0, \\pi)| \\to 1$. Thus, Lex-GS is not an effective smoother for this problem.\n$$\n\\rho_{\\mathrm{H}}(G_{\\text{Lex}}) = \\sup_{\\boldsymbol{\\theta} \\in \\Theta_{\\mathrm{H}}} |g_{\\text{Lex}}(\\boldsymbol{\\theta})| \\ge \\left|\\frac{1-\\varepsilon}{1+3\\varepsilon}\\right| \\xrightarrow{\\varepsilon \\to 0} 1\n$$\n\n**2. Red-Black Gauss-Seidel (RB-GS)**\nFor a two-color ordering with the 5-point stencil, the matrix $A$ is consistently ordered. A known result for consistently ordered matrices is that the eigenvalues of the Gauss-Seidel operator, $\\lambda(G_{\\text{GS}})$, are related to the eigenvalues of the Jacobi operator, $\\lambda(J)$, by $\\lambda(G_{\\text{GS}}) = \\lambda(J)^2$ (plus some zero eigenvalues). The symbol of the Jacobi operator $J = -D^{-1}(L+U)$ is:\n$$\nj(\\theta_x, \\theta_y) = -\\frac{\\alpha(e^{i\\theta_x}+e^{-i\\theta_x}) + \\beta(e^{i\\theta_y}+e^{-i\\theta_y})}{-2(\\alpha+\\beta)} = \\frac{\\alpha \\cos\\theta_x + \\beta \\cos\\theta_y}{\\alpha+\\beta} = \\frac{\\cos\\theta_x + \\varepsilon \\cos\\theta_y}{1+\\varepsilon}\n$$\nThe symbol for RB-GS is thus $g_{\\text{RB}}(\\theta_x, \\theta_y) = [j(\\theta_x, \\theta_y)]^2$:\n$$\ng_{\\text{RB}}(\\theta_x, \\theta_y) = \\left( \\frac{\\cos\\theta_x + \\varepsilon \\cos\\theta_y}{1+\\varepsilon} \\right)^2\n$$\nTo find the smoothing factor, we must maximize $|g_{\\text{RB}}|$ over $\\Theta_{\\mathrm{H}}$. Consider the high-frequency mode $(\\theta_x, \\theta_y) = (\\pi, \\pi) \\in \\Theta_{\\mathrm{H}}$:\n$$\ng_{\\text{RB}}(\\pi, \\pi) = \\left( \\frac{\\cos\\pi + \\varepsilon \\cos\\pi}{1+\\varepsilon} \\right)^2 = \\left( \\frac{-1 - \\varepsilon}{1+\\varepsilon} \\right)^2 = (-1)^2 = 1\n$$\nSince the amplification factor for a high-frequency mode is $1$, RB-GS fails to damp this mode entirely. Thus, it is not a smoother under this definition.\n$$\n\\rho_{\\mathrm{H}}(G_{\\text{RB}}) = 1\n$$\n\n**3. Horizontal Line Relaxation**\nThis is a block Gauss-Seidel method where each block is a horizontal line of nodes. The update for line $j$ uses new values from line $j-1$ and old values from line $j+1$. Within each line, the system is solved exactly. For the error equation, the system for line $j$ is:\n$$\n\\alpha(e_{i-1,j}^{(k+1)} - 2e_{i,j}^{(k+1)} + e_{i+1,j}^{(k+1)}) - 2\\beta e_{i,j}^{(k+1)} + \\beta e_{i,j-1}^{(k+1)} + \\beta e_{i,j+1}^{(k)} = 0\n$$\nWe perform a Fourier transform in the $i$-direction, with mode $\\theta_x$ and amplitude $\\hat{e}_j(\\theta_x)$:\n$$\n\\left( \\alpha(2\\cos\\theta_x-2) - 2\\beta \\right) \\hat{e}_j^{(k+1)}(\\theta_x) + \\beta \\hat{e}_{j-1}^{(k+1)}(\\theta_x) + \\beta \\hat{e}_{j+1}^{(k)}(\\theta_x) = 0\n$$\n$$\n\\left( -4\\alpha\\sin^2(\\tfrac{\\theta_x}{2}) - 2\\beta \\right) \\hat{e}_j^{(k+1)}(\\theta_x) = -\\beta \\hat{e}_{j-1}^{(k+1)}(\\theta_x) - \\beta \\hat{e}_{j+1}^{(k)}(\\theta_x)\n$$\nThis is a 1D vertical GS-like update for each $\\theta_x$ mode. Now, we perform a Fourier transform in the $j$-direction with mode $\\theta_y$ and amplification factor $g(\\theta_x, \\theta_y)$:\n$$\ng \\left( -4\\alpha\\sin^2(\\tfrac{\\theta_x}{2}) - 2\\beta \\right) = -g\\beta e^{-i\\theta_y} - \\beta e^{i\\theta_y}\n$$\nSolving for $g_{\\text{Line}}(\\theta_x, \\theta_y)$:\n$$\ng_{\\text{Line}}(\\theta_x, \\theta_y) = \\frac{\\beta e^{i\\theta_y}}{4\\alpha\\sin^2(\\tfrac{\\theta_x}{2}) + 2\\beta - \\beta e^{-i\\theta_y}} = \\frac{\\varepsilon e^{i\\theta_y}}{4\\sin^2(\\tfrac{\\theta_x}{2}) + 2\\varepsilon - \\varepsilon e^{-i\\theta_y}}\n$$\nWe must find the supremum of $|g_{\\text{Line}}|$ over $\\Theta_{\\mathrm{H}}$. Let's analyze its squared magnitude:\n$$\n|g_{\\text{Line}}|^2 = \\frac{\\varepsilon^2}{|4\\sin^2(\\tfrac{\\theta_x}{2}) + 2\\varepsilon - \\varepsilon e^{-i\\theta_y}|^2} = \\frac{\\varepsilon^2}{(4\\sin^2(\\tfrac{\\theta_x}{2}) + 2\\varepsilon - \\varepsilon\\cos\\theta_y)^2 + (\\varepsilon\\sin\\theta_y)^2}\n$$\nTo maximize this value, we need to minimize the denominator $D(\\theta_x, \\theta_y)$. The term $4\\sin^2(\\tfrac{\\theta_x}{2})$ is non-negative. The denominator is an increasing function of $\\sin^2(\\tfrac{\\theta_x}{2})$. Therefore, we must seek the minimum of the denominator for the smallest possible values of $|\\theta_x|$ allowed within $\\Theta_{\\mathrm{H}}$.\nThe set $\\Theta_{\\mathrm{H}}$ is $\\{ |\\theta_x| \\in [\\tfrac{\\pi}{2},\\pi] \\;\\text{or}\\; |\\theta_y| \\in [\\tfrac{\\pi}{2},\\pi] \\}$.\n- If $|\\theta_x| \\in [\\tfrac{\\pi}{2},\\pi]$, then $\\sin^2(\\tfrac{\\theta_x}{2}) \\ge \\sin^2(\\tfrac{\\pi}{4}) = \\frac{1}{2}$. The term $4\\sin^2(\\tfrac{\\theta_x}{2})$ is at least $2$. For $\\varepsilon \\ll 1$, $|g_{\\text{Line}}|$ will be small, of order $O(\\varepsilon)$.\n- If $|\\theta_y| \\in [\\tfrac{\\pi}{2},\\pi]$, we are free to choose $\\theta_x \\in [-\\pi,\\pi]$. To minimize the denominator, we choose $\\theta_x = 0$.\nSo the supremum will be found along the line $\\theta_x = 0$, with $|\\theta_y| \\in [\\tfrac{\\pi}{2},\\pi]$.\nFor $\\theta_x=0$:\n$$\ng_{\\text{Line}}(0, \\theta_y) = \\frac{\\varepsilon e^{i\\theta_y}}{2\\varepsilon - \\varepsilon e^{-i\\theta_y}} = \\frac{e^{i\\theta_y}}{2 - e^{-i\\theta_y}}\n$$\nThe squared magnitude is:\n$$\n|g_{\\text{Line}}(0, \\theta_y)|^2 = \\frac{1}{|2 - (\\cos\\theta_y + i\\sin\\theta_y)|^{-1}} = \\frac{1}{|2 - \\cos\\theta_y + i\\sin\\theta_y|^2} = \\frac{1}{(2-\\cos\\theta_y)^2 + \\sin^2\\theta_y} = \\frac{1}{5 - 4\\cos\\theta_y}\n$$\nWe need to maximize this function for $|\\theta_y| \\in [\\tfrac{\\pi}{2},\\pi]$, which corresponds to $\\cos\\theta_y \\in [-1, 0]$. The function $f(c) = 1/(5-4c)$ is increasing for $c \\in [-1, 0]$. The maximum is attained at the largest value of $c$, which is $c=\\cos\\theta_y = 0$. This occurs at $\\theta_y = \\pm \\frac{\\pi}{2}$.\nThe maximum value of $|g_{\\text{Line}}(0, \\theta_y)|^2$ is $\\frac{1}{5 - 4(0)} = \\frac{1}{5}$.\nThe smoothing factor is the square root of this value.\n$$\n\\rho_{\\mathrm{H}}(G_{\\text{Line}}) = \\sqrt{\\frac{1}{5}} = \\frac{1}{\\sqrt{5}}\n$$\n\n### Conclusion\nComparing the smoothing factors for $\\varepsilon \\ll 1$:\n- Lexicographic GS: $\\rho_{\\mathrm{H}}(G_{\\text{Lex}}) \\approx 1$\n- Red-Black GS: $\\rho_{\\mathrm{H}}(G_{\\text{RB}}) = 1$\n- Horizontal Line Relaxation: $\\rho_{\\mathrm{H}}(G_{\\text{Line}}) = \\frac{1}{\\sqrt{5}}$\n\nThe horizontal line relaxation scheme is the only effective smoother among the three for the given anisotropic problem. It is the ordering that minimizes the smoothing factor. The minimal value of $\\rho_{\\mathrm{H}}(G)$ is $\\frac{1}{\\sqrt{5}}$.", "answer": "$$\\boxed{\\frac{1}{\\sqrt{5}}}$$", "id": "3437865"}]}