{"hands_on_practices": [{"introduction": "To begin our exploration of direct solvers for sparse systems, we first analyze an ideal scenario that arises from one-dimensional problems. This practice examines the LU factorization of a tridiagonal matrix, demonstrating the best-case behavior where Gaussian elimination perfectly preserves the sparse structure. By proving that no \"fill-in\" occurs and calculating the exact storage cost, you will establish a foundational understanding of the mechanics and efficiency of sparse factorization. [@problem_id:3378267]", "problem": "Consider the one-dimensional, second-order, self-adjoint elliptic Partial Differential Equation (PDE) $-(a(x) u'(x))' = f(x)$ on the interval $[0,1]$ with homogeneous Dirichlet boundary conditions $u(0)=0$ and $u(1)=0$, where $a(x)$ is strictly positive and sufficiently smooth so that the continuous problem is well posed. Discretize the PDE by a standard second-order finite difference method on $n$ interior grid points to obtain a linear system $A \\mathbf{u} = \\mathbf{b}$ with $A \\in \\mathbb{R}^{n \\times n}$ that is tridiagonal, strictly diagonally dominant, and nonsingular. Let $A$ have entries $A_{i,i}$ on the main diagonal and $A_{i,i-1}$, $A_{i,i+1}$ on the first sub- and superdiagonals, respectively, with all other entries equal to zero.\n\nGaussian elimination without pivoting is applied to factor $A$ into a Lower-Upper (LU) factorization $A = LU$, where $L$ is unit lower triangular and $U$ is upper triangular. The goal is to examine the fill-in and band structure under this elimination.\n\nUsing only foundational definitions and operations of Gaussian elimination (row operations that eliminate subdiagonal entries by subtracting suitable multiples of preceding rows) and the definition of banded matrices (a matrix with half-bandwidth $\\beta$ has $A_{i,j} = 0$ whenever $|i-j| > \\beta$), perform the following:\n\n1. Prove that, for this tridiagonal $A$ (half-bandwidth $\\beta = 1$), Gaussian elimination without pivoting preserves the band structure in the sense that no fill-in occurs outside the original tridiagonal band. Your argument must be constructed from first principles of the elimination process and a rigorous accounting of which entries are affected at each step, without assuming any pre-known formula for the LU factors.\n\n2. Under the generic assumption that each elimination step produces a nonzero multiplier and that updated pivots remain nonzero (conditions satisfied, for example, by strictly diagonally dominant tridiagonal systems arising from the above PDE discretization), determine the exact total number of nonzero entries in $L$ and in $U$ produced by this elimination. Express your final result as closed-form expressions in $n$.\n\nProvide your final answer as a two-entry row matrix with the first entry equal to the number of nonzeros in $L$ and the second entry equal to the number of nonzeros in $U$. No rounding is required, and no physical units are involved.", "solution": "The problem statement is first validated against the required criteria.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n-   **PDE:** $-(a(x) u'(x))' = f(x)$ on the interval $[0,1]$.\n-   **Boundary Conditions:** Homogeneous Dirichlet conditions $u(0)=0$ and $u(1)=0$.\n-   **Coefficient:** $a(x)$ is strictly positive and sufficiently smooth.\n-   **Discretization:** Second-order finite difference method on $n$ interior grid points.\n-   **Linear System:** $A \\mathbf{u} = \\mathbf{b}$, with $A \\in \\mathbb{R}^{n \\times n}$.\n-   **Matrix Properties:** $A$ is tridiagonal, strictly diagonally dominant, and nonsingular.\n-   **Matrix Structure:** $A$ has nonzero entries $A_{i,i}$ on the main diagonal, and $A_{i,i-1}$, $A_{i,i+1}$ on the first sub- and super-diagonals. All other entries are zero, meaning $A_{i,j} = 0$ for $|i-j| > 1$. The half-bandwidth is $\\beta = 1$.\n-   **Factorization Method:** Gaussian elimination without pivoting to produce $A = LU$, where $L$ is unit lower triangular and $U$ is upper triangular.\n-   **Task 1:** Prove that no fill-in occurs outside the original tridiagonal band. The argument must be from first principles of Gaussian elimination.\n-   **Task 2:** Determine the total number of nonzero entries in $L$ and in $U$, assuming nonzero multipliers and pivots. Express the result in terms of $n$.\n\n**Step 2: Validate Using Extracted Givens**\n-   **Scientific Groundedness:** The problem originates from the numerical solution of partial differential equations, a core topic in scientific computing and applied mathematics. The use of finite differences to discretize a self-adjoint elliptic PDE, the properties of the resulting matrix (tridiagonal, symmetric positive definite, and hence strictly diagonally dominant and nonsingular), and its LU factorization are all standard, well-established concepts.\n-   **Well-Posedness:** The problem is clearly stated and mathematically sound. The assumption that $A$ is strictly diagonally dominant guarantees that Gaussian elimination without pivoting is well-defined (pivots are always non-zero) and numerically stable, leading to a unique $LU$ factorization. The tasks are specific and have definite answers.\n-   **Objectivity:** The problem is expressed in precise mathematical language, free from any subjective or ambiguous terminology.\n-   **Completeness and Consistency:** The problem is self-contained. All necessary information and definitions (tridiagonal structure, Gaussian elimination, LU factorization) are provided or implicitly understood within the context of numerical linear algebra. The properties of matrix $A$ are consistent with the physical problem described.\n-   **Topic Relevance:** The problem is directly relevant to the topic of direct solvers for sparse linear systems, a key component of the numerical solution of PDEs.\n\n**Step 3: Verdict and Action**\n-   **Verdict:** The problem is valid. It is a well-posed, standard theoretical problem in numerical linear algebra.\n-   **Action:** Proceed with the solution.\n\n### Solution\n\nThe problem asks for two things: a proof regarding the preservation of the band structure of a tridiagonal matrix under Gaussian elimination without pivoting, and a count of the nonzero entries in the resulting $L$ and $U$ factors.\n\n**Part 1: Proof of No Fill-In**\n\nLet $A \\in \\mathbb{R}^{n \\times n}$ be a tridiagonal matrix. By definition, $A_{i,j} = 0$ if $|i-j| > 1$. We perform Gaussian elimination to transform $A$ into an upper triangular matrix $U$. Let $A^{(1)} = A$. The process consists of $n-1$ steps. At step $k$, for $k=1, 2, \\dots, n-1$, we eliminate the subdiagonal entry in column $k$.\n\nThe matrix at the beginning of step $k$ is denoted by $A^{(k)}$. The goal of step $k$ is to introduce a zero at the position $(k+1, k)$. This is accomplished by the row operation $R_{k+1} \\leftarrow R_{k+1} - m_{k+1,k} R_k$, where $R_i$ denotes the $i$-th row of the matrix and the multiplier is $m_{k+1,k} = \\frac{A_{k+1,k}^{(k)}}{A_{k,k}^{(k)}}$. All other rows remain unchanged, except for row $k+1$. The entries of the new matrix $A^{(k+1)}$ are given by:\n$$A_{i,j}^{(k+1)} = A_{i,j}^{(k)} \\quad \\text{for } i \\neq k+1$$\n$$A_{k+1,j}^{(k+1)} = A_{k+1,j}^{(k)} - m_{k+1,k} A_{k,j}^{(k)} \\quad \\text{for } j=1, \\dots, n$$\n\nWe will prove by induction that for all steps $k=1, \\dots, n$, the matrix $A^{(k)}$ remains tridiagonal; that is, $A_{i,j}^{(k)} = 0$ for all $(i,j)$ such that $|i-j| > 1$.\n\n**Base Case ($k=1$):**\nThe initial matrix $A^{(1)} = A$ is tridiagonal, so $A_{i,j}^{(1)} = 0$ if $|i-j|>1$.\n\n**Inductive Hypothesis:**\nAssume that after $k-1$ steps of elimination, the matrix $A^{(k)}$ is tridiagonal. This means $A_{i,j}^{(k)} = 0$ if $|i-j|>1$.\n\n**Inductive Step:**\nWe perform the $k$-th step to obtain $A^{(k+1)}$. The only modified row is row $k+1$. We must show that no fill-in occurs in this row, i.e., $A_{k+1,j}^{(k+1)}$ remains $0$ for $|(k+1)-j| > 1$.\n\nThe update formula is $A_{k+1,j}^{(k+1)} = A_{k+1,j}^{(k)} - m_{k+1,k} A_{k,j}^{(k)}$.\n\nLet's inspect the terms on the right-hand side based on the inductive hypothesis:\n1.  **Row $k$ of $A^{(k)}$:** By the inductive hypothesis, $A_{k,j}^{(k)} \\neq 0$ only if $|k-j| \\le 1$, which means $j=k-1, k, k+1$. However, from the $(k-1)$-th step of elimination, the entry $A_{k,k-1}^{(k)}$ was already set to $0$. Thus, row $k$ of $A^{(k)}$ has nonzero entries only at columns $j=k$ and $j=k+1$.\n2.  **Row $k+1$ of $A^{(k)}$:** By the inductive hypothesis, $A_{k+1,j}^{(k)} \\neq 0$ only if $|(k+1)-j| \\le 1$, which means $j=k, k+1, k+2$.\n\nNow, we evaluate the new entries $A_{k+1,j}^{(k+1)}$:\n-   For $j < k$: We have $|(k+1)-j| > 1$ and $|k-j| > 1$. Thus, from the inductive hypothesis, $A_{k+1,j}^{(k)} = 0$ and $A_{k,j}^{(k)} = 0$. The update results in $A_{k+1,j}^{(k+1)} = 0 - m_{k+1,k} \\cdot 0 = 0$.\n-   For $j = k$: $A_{k+1,k}^{(k+1)} = A_{k+1,k}^{(k)} - \\frac{A_{k+1,k}^{(k)}}{A_{k,k}^{(k)}} A_{k,k}^{(k)} = 0$. This is the intended elimination.\n-   For $j=k+1$: $A_{k+1,k+1}^{(k+1)} = A_{k+1,k+1}^{(k)} - m_{k+1,k} A_{k,k+1}^{(k)}$. This entry is updated but remains within the tridiagonal band.\n-   For $j=k+2$: $A_{k+1,k+2}^{(k+1)} = A_{k+1,k+2}^{(k)} - m_{k+1,k} A_{k,k+2}^{(k)}$. From point 1 above, $A_{k,k+2}^{(k)} = 0$ (since $|k-(k+2)|=2>1$). Thus, $A_{k+1,k+2}^{(k+1)} = A_{k+1,k+2}^{(k)} - 0 = A_{k+1,k+2}^{(k)}$. This entry is unchanged and remains within the band.\n-   For $j > k+2$: We have $|(k+1)-j| > 1$ and $|k-j|>1$. Thus, $A_{k+1,j}^{(k)} = 0$ and $A_{k,j}^{(k)} = 0$. The update results in $A_{k+1,j}^{(k+1)} = 0 - m_{k+1,k} \\cdot 0 = 0$.\n\nIn all cases for row $k+1$, if an entry $A_{k+1,j}^{(k)}$ was zero because it was outside the band (i.e., $|(k+1)-j|>1$), the corresponding updated entry $A_{k+1,j}^{(k+1)}$ is also zero. No new nonzero entries are created outside the original tridiagonal structure. Since all other rows are unchanged, the entire matrix $A^{(k+1)}$ remains tridiagonal.\n\nBy the principle of mathematical induction, the property holds for all steps. The final matrix $U = A^{(n)}$ is therefore upper triangular and also tridiagonal, which means it is upper bidiagonal (nonzero entries only on the main diagonal and the first superdiagonal). Consequently, no fill-in occurs outside the original band.\n\n**Part 2: Number of Nonzero Entries in $L$ and $U$**\n\nThe Gaussian elimination process described gives the factorization $A=LU$.\n\n**The Matrix L:**\nThe matrix $L$ is a unit lower triangular matrix whose off-diagonal entries are the multipliers used during elimination. It is defined as:\n$$\nL = \\begin{pmatrix}\n1 & 0 & \\cdots & 0 \\\\\nm_{2,1} & 1 & \\cdots & 0 \\\\\n0 & m_{3,2} & \\ddots & \\vdots \\\\\n\\vdots & \\ddots & \\ddots & 0 \\\\\n0 & \\cdots & m_{n,n-1} & 1\n\\end{pmatrix}\n$$\n-   By its definition as a unit lower triangular matrix, $L$ has $n$ entries equal to $1$ on its main diagonal ($L_{i,i}=1$).\n-   As shown in the proof, at each step $k \\in \\{1, \\dots, n-1\\}$, exactly one multiplier, $m_{k+1,k}$, is computed to eliminate the single subdiagonal entry $A_{k+1,k}$. These multipliers form the entries $L_{k+1,k}$. There are $n-1$ such multipliers. The problem states we should assume these are all nonzero.\n-   All other entries of $L$ are zero.\n-   Therefore, $L$ is a unit lower bidiagonal matrix.\n-   The total number of nonzero entries in $L$ is the sum of the diagonal entries and the subdiagonal entries: $n + (n-1) = 2n-1$.\n\n**The Matrix U:**\nThe matrix $U$ is the final upper triangular matrix resulting from the elimination process.\n-   From the proof in Part 1, $U$ is upper bidiagonal. It has nonzero entries only on its main diagonal and its first superdiagonal.\n-   The main diagonal of $U$ consists of the pivots $A_{k,k}^{(k)}$ for $k=1, \\dots, n$. The problem assumption is that these remain nonzero. There are $n$ such diagonal entries.\n-   The first superdiagonal of $U$ consists of the entries $A_{k,k+1}^{(k+1)}$ for $k=1, \\dots, n-1$. Our proof showed that $A_{k,k+1}^{(k+1)} = A_{k,k+1}^{(k)} = \\dots = A_{k,k+1}^{(1)} = A_{k,k+1}$. The superdiagonal of $U$ is identical to the superdiagonal of the original matrix $A$. Since $A$ is the result of a standard finite difference scheme for the given PDE, these entries are generally nonzero. There are $n-1$ such superdiagonal entries.\n-   The total number of nonzero entries in $U$ is the sum of the diagonal entries and the superdiagonal entries: $n + (n-1) = 2n-1$.\n\nThe number of nonzero entries in $L$ is $2n-1$ and the number of nonzero entries in $U$ is $2n-1$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n2n-1 & 2n-1\n\\end{pmatrix}\n}\n$$", "id": "3378267"}, {"introduction": "Moving from one to two dimensions reveals a critical challenge for direct solvers: the number of nonzero elements in the $L$ and $U$ factors, known as fill-in, heavily depends on the ordering of the unknowns. This exercise contrasts a simple lexicographic ordering, which leads to extensive fill-in, with a superior graph-based strategy. Quantifying the cost of the naive approach provides a powerful motivation for the sophisticated reordering algorithms that are essential for solving large-scale PDE problems efficiently. [@problem_id:3378287]", "problem": "Consider the linear system arising from the finite difference discretization of a second-order elliptic Partial Differential Equation (PDE) on the unit square with homogeneous Dirichlet boundary conditions. Let there be $n$ interior grid points per coordinate direction, so the total number of unknowns is $N = n^{2}$. Use a standard five-point stencil on the structured Cartesian grid, and assume the coefficient field is smooth and positive so that the resulting stiffness matrix is Symmetric Positive Definite (SPD). The sparse matrix pattern is therefore consistent with the underlying two-dimensional mesh graph: each interior node connects only to its four nearest neighbors.\n\nYou are asked to construct a sparse matrix layout and an ordering that together demonstrate worst-case fill growth, and then propose an alternative ordering that mitigates this growth while preserving the original mesh-based sparsity.\n\nTasks:\n1. Construct the sparse matrix pattern implied by the mesh connectivity and impose a lexicographic (row-wise) ordering of the unknowns, where the index increases first along the $x$-direction within a row and then proceeds to the next row in the $y$-direction. Explain briefly, based on basic properties of Gaussian elimination (or equivalently, Cholesky factorization for SPD matrices), why this lexicographic ordering triggers large elimination fronts and fill growth.\n2. Propose a graph-based ordering strategy that reduces fill growth while preserving the mesh structure. Describe at a high level why it mitigates fill compared to lexicographic ordering.\n3. Under the lexicographic ordering described in Task 1, the stiffness matrix is a symmetric banded matrix with half-bandwidth $w$. Using only the mesh connectivity and the definition of bandedness, determine $w$ in terms of $n$. Then, derive the exact total number of nonzeros in the Cholesky factor $L$ (counting the diagonal) as a closed-form expression in $n$, assuming no pivoting is required due to symmetry and positive definiteness.\n\nProvide your final answer as the closed-form analytic expression requested in Task 3. No rounding is required, and no units are involved. The final answer must be a single expression.", "solution": "The problem as stated is scientifically grounded, well-posed, and objective. It presents a standard, canonical problem in numerical linear algebra concerning the analysis of sparse matrix factorization for systems derived from Partial Differential Equation (PDE) discretizations. All necessary information is provided, the terminology is precise, and the tasks are concrete and solvable. The problem is therefore deemed valid.\n\nThe solution proceeds by addressing the three tasks in the specified order.\n\nTask 1: Analysis of Lexicographic Ordering and Fill Growth\n\nA lexicographic (or natural row-wise) ordering of the $N=n^2$ grid points numbers them as if reading a book: left-to-right, then top-to-bottom. If a grid point has coordinates $(i, j)$ where $i, j \\in \\{1, 2, \\dots, n\\}$, its global index $k$ is given by $k = (j-1)n + i$.\n\nThe five-point stencil connects each node $(i,j)$ to its neighbors $(i\\pm 1, j)$ and $(i, j\\pm 1)$, if they are within the domain. This structure gives the stiffness matrix $A$ a specific block tridiagonal form:\n$$\nA = \\begin{pmatrix}\nT & -c_1 I & & & \\\\\n-c_2 I & T & -c_3 I & & \\\\\n& \\ddots & \\ddots & \\ddots & \\\\\n& & -c_{n-2} I & T & -c_{n-1} I \\\\\n& & & -c_{n-1} I & T\n\\end{pmatrix}\n$$\nHere, each block is an $n \\times n$ matrix. The matrix $T$ is itself a tridiagonal matrix representing the connections within a single row of the grid. The identity matrices $I$ (possibly scaled by constants $c_k$ related to the PDE coefficients and grid spacing, which we can consider as $1$ for structural analysis) represent the connections between adjacent rows.\n\nGaussian elimination (or Cholesky factorization, $A=LL^T$, for SPD matrices) systematically eliminates variables. When variable $k$ is eliminated, the Schur complement is formed for the remaining submatrix. This operation introduces new non-zero entries, known as \"fill-in\". In matrix terms, for all $i, j > k$, the update is $A_{ij} \\leftarrow A_{ij} - A_{ik} (A_{kk})^{-1} A_{kj}$. If $A_{ij}$ was zero but both $A_{ik}$ and $A_{kj}$ were non-zero, a new non-zero $A_{ij}$ is created. In graph terms, eliminating node $k$ adds edges between any pair of its neighbors that were not already connected.\n\nLet's analyze this with the block structure of $A$. The Cholesky factorization proceeds by block-wise elimination. Let $A=LL^T$, where $L$ is block lower triangular:\n$$\nL = \\begin{pmatrix}\nL_1 & & \\\\\nB_2 & L_2 & \\\\\n& \\ddots & \\ddots \\\\\n& & B_n & L_n\n\\end{pmatrix}\n$$\nThe first step of block Cholesky factorization is to compute $L_1$ from $A_{11} = L_1 L_1^T$. Here, $A_{11} = T$, which is a tridiagonal matrix. Its Cholesky factor $L_1$ is a lower bidiagonal matrix, which is sparse.\nThe next step is to find the off-diagonal block $B_2$ by solving $B_2 L_1^T = A_{21}$. Given $A_{21} = -I$, we have $B_2 = -I(L_1^T)^{-1} = -(L_1^{-1})^T$.\nThis step is the origin of the catastrophic fill growth. While $L_1$ is a sparse (bidiagonal) matrix, its inverse, $L_1^{-1}$, is a dense lower triangular matrix. Therefore, the block $B_2$ is a dense upper triangular matrix (since it is the transpose of a dense lower triangular matrix). In the Cholesky factor $L$, this corresponds to a dense block. As the factorization proceeds, this density propagates. The elimination front, which is the set of nodes adjacent to the already eliminated nodes, becomes wide. With lexicographic ordering, this front essentially spans an entire row of $n$ nodes, leading to $O(n)$ fill-in for each node eliminated.\n\nTask 2: An Alternative Ordering Strategy\n\nTo mitigate the extensive fill-in observed with lexicographic ordering, a graph-based reordering strategy is necessary. A highly effective strategy for problems on regular grids is **Nested Dissection**.\n\nThe high-level description of the Nested Dissection algorithm is as follows:\n1.  **Find a Separator:** Identify a small set of nodes (a \"separator\") whose removal splits the grid graph into two disconnected subgraphs of approximately equal size. For an $n \\times n$ grid, a natural separator is the middle column of nodes, which contains $n$ nodes. Removing this separator divides the grid into two subgraphs of size approximately $n \\times (n/2)$.\n2.  **Order Subgraphs:** Recursively apply the same nested dissection strategy to the resulting subgraphs. That is, find separators for the subgraphs, and so on, until the remaining subgraphs are small enough to be ordered directly.\n3.  **Order Separators Last:** The key principle is the ordering sequence. All nodes in the two main subgraphs are numbered first (following the recursive ordering), and the nodes in the top-level separator are numbered last.\n\nThis strategy mitigates fill-in because during the elimination process, nodes in one subgraph are eliminated without creating any fill-in connecting to nodes in the other subgraph. The eliminations are contained within each subgraph. Significant fill-in only occurs when the separator nodes, which are numbered last, are finally eliminated. Since the separators are chosen to be small, the amount of fill-in is drastically reduced compared to lexicographic ordering. For a 2D grid, Nested Dissection reduces the fill-in from $O(n^3)$ to $O(n^2 \\log n)$ and the operation count from $O(n^4)$ to $O(n^3)$.\n\nTask 3: Bandwidth and Fill Count for Lexicographic Ordering\n\nFirst, we determine the half-bandwidth $w$. The half-bandwidth is defined as $w = \\max_{A_{k,l} \\neq 0} |k-l|$. The index of a node at grid position $(i,j)$ is $k = (j-1)n + i$. A non-zero entry $A_{k,l}$ exists if nodes $k$ and $l$ are neighbors. Let's examine the index differences for a node $k$ at $(i,j)$:\n-   Neighbor at $(i+1, j)$: index $l = (j-1)n + i+1$. $|k-l| = 1$.\n-   Neighbor at $(i-1, j)$: index $l = (j-1)n + i-1$. $|k-l| = 1$.\n-   Neighbor at $(i, j+1)$: index $l = j \\cdot n + i$. $|k-l| = |(j-1)n + i - (jn+i)| = |-n| = n$.\n-   Neighbor at $(i, j-1)$: index $l = (j-2)n + i$. $|k-l| = |(j-1)n + i - ((j-2)n+i)| = |n| = n$.\nThe maximum index difference is $n$. Thus, the half-bandwidth is $w=n$.\n\nNext, we derive the exact number of nonzeros in the Cholesky factor $L$. For a banded matrix with half-bandwidth $w$, the Cholesky factor $L$ has its nonzeros entirely contained within the same band structure. For the five-point stencil on a rectangular grid with lexicographic ordering, the elimination process completely fills this band. This means that for any entry $L_{ij}$ with $i-j \\le w=n$, we can expect $L_{ij} \\ne 0$.\n\nLet $S$ be the total number of nonzeros in $L$. We can count these nonzeros by summing the number of nonzeros in each column of $L$. The matrix $L$ is lower triangular, so we only consider $i \\ge j$.\nFor a column $j$, the nonzeros are $L_{ij}$ for $i$ ranging from $j$ up to the extent of the band. The band dictates that $i - j \\le n$, or $i \\le j+n$. So, nonzeros potentially exist for $j \\le i \\le j+n$. The total number of rows is $N=n^2$.\nThe number of nonzeros in column $j$ is thus $\\min(N, j+n) - j + 1$.\n\nWe can sum this expression over all columns $j=1, \\dots, N$:\n$$ S = \\sum_{j=1}^{N} \\left( \\min(n^2, j+n) - j + 1 \\right) $$\nWe split the sum into two parts. For $j \\le N-n = n^2-n$, the term $\\min(n^2, j+n)$ is $j+n$. For $j > n^2-n$, the term is $n^2$.\n1.  For $1 \\le j \\le n^2-n$: The number of nonzeros in each column is $(j+n) - j + 1 = n+1$. There are $n^2-n$ such columns.\n    The total contribution from this part is $(n^2-n)(n+1)$.\n2.  For $n^2-n+1 \\le j \\le n^2$: The number of nonzeros in column $j$ is $n^2 - j + 1$.\n    Let's make a change of index $k = j - (n^2-n)$, so $j=k+n^2-n$. As $j$ goes from $n^2-n+1$ to $n^2$, $k$ goes from $1$ to $n$. The term becomes $n^2 - (k+n^2-n) + 1 = n-k+1$.\n    The sum for this part is $\\sum_{k=1}^{n} (n-k+1)$. This is the sum of integers from $1$ to $n$: $n + (n-1) + \\dots + 1 = \\frac{n(n+1)}{2}$.\n\nThe total number of nonzeros is the sum of these two parts:\n$$ S = (n^2-n)(n+1) + \\frac{n(n+1)}{2} $$\nLet's simplify this expression:\n$$ S = n(n-1)(n+1) + \\frac{n(n+1)}{2} $$\n$$ S = n(n^2-1) + \\frac{n^2+n}{2} $$\n$$ S = n^3 - n + \\frac{n^2}{2} + \\frac{n}{2} $$\n$$ S = n^3 + \\frac{1}{2}n^2 - \\frac{1}{2}n $$\nThis is the closed-form expression for the total number of nonzeros in the Cholesky factor $L$.", "answer": "$$\\boxed{n^3 + \\frac{1}{2}n^2 - \\frac{1}{2}n}$$", "id": "3378287"}, {"introduction": "Beyond managing storage, a robust direct solver must also ensure numerical stability, especially in finite-precision arithmetic. This computational practice investigates the concept of element growth, where the magnitude of entries in the factors can become much larger than in the original matrix, leading to inaccurate solutions. By analyzing a physically-motivated anisotropic diffusion problem, you will determine a threshold where pivoting becomes essential, highlighting the crucial trade-off between exploiting a fixed sparsity pattern and guaranteeing a stable factorization. [@problem_id:3378314]", "problem": "Consider the anisotropic diffusion operator on a unit square domain with homogeneous Dirichlet boundary conditions. Let the continuous model be given by the partial differential equation $-\\nabla \\cdot (\\mathbf{K} \\nabla u) = f$ where $\\mathbf{K} = \\operatorname{diag}(k_x, k_y)$ is the conductivity tensor. Assume $k_x = 1$ and $k_y = \\epsilon$ with $\\epsilon \\in (0, 1]$, which yields anisotropy aligned with the coordinate axes.\n\nDiscretize the operator with the standard second-order centered finite difference scheme on an $n \\times n$ interior grid with uniform spacing $h = 1/(n+1)$ in both coordinate directions, and assemble the linear system $\\mathbf{A}(\\epsilon)\\mathbf{u} = \\mathbf{b}$. The resulting matrix $\\mathbf{A}(\\epsilon)$ is symmetric and positive definite. Explicitly, let $\\mathbf{T}_x \\in \\mathbb{R}^{n \\times n}$ be the tridiagonal matrix with main diagonal entries $2/h^2$ and first sub- and super-diagonal entries $-1/h^2$, and let $\\mathbf{T}_y \\in \\mathbb{R}^{n \\times n}$ be the tridiagonal matrix with main diagonal entries $2\\epsilon/h^2$ and first sub- and super-diagonal entries $-\\epsilon/h^2$. Then $\\mathbf{A}(\\epsilon) \\in \\mathbb{R}^{N \\times N}$ with $N = n^2$ is given by the Kronecker sum $\\mathbf{A}(\\epsilon) = \\mathbf{I}_n \\otimes \\mathbf{T}_y + \\mathbf{T}_x \\otimes \\mathbf{I}_n$, where $\\mathbf{I}_n$ is the $n \\times n$ identity matrix and $\\otimes$ denotes the Kronecker product.\n\nDefine the element growth factor of Gaussian elimination with partial pivoting as $\\rho_{\\mathrm{pp}}(\\mathbf{A}) = \\dfrac{\\max_{i,j} |U^{\\mathrm{pp}}_{ij}|}{\\max_{i,j} |\\mathbf{A}_{ij}|}$, where $\\mathbf{U}^{\\mathrm{pp}}$ is the upper triangular factor produced by Gaussian elimination with partial pivoting. Similarly, define $\\rho_{\\mathrm{np}}(\\mathbf{A}) = \\dfrac{\\max_{i,j} |U^{\\mathrm{np}}_{ij}|}{\\max_{i,j} |\\mathbf{A}_{ij}|}$ for elimination without pivoting, where $\\mathbf{U}^{\\mathrm{np}}$ is the upper triangular factor produced by naive Gaussian elimination without row interchanges.\n\nLet the unit roundoff for binary64 (double precision) arithmetic be $u = 2^{-53}$ and consider the standard normwise backward error model for Gaussian elimination, which asserts a bound of the form $\\eta \\lesssim c N \\rho u$ for some modest constant $c$ and matrix dimension $N$, where $\\rho$ is the element growth factor associated with the algorithm used. For the purposes of this problem, use $c = 3$ and interpret the predicted backward error for no-pivot elimination as $\\eta_{\\mathrm{np}} \\approx 3 N \\rho_{\\mathrm{np}} u$. Declare that pivoting becomes essential for numerical stability when $\\eta_{\\mathrm{np}}$ exceeds a tolerance of $10^{-6}$ (a dimensionless relative threshold).\n\nYour tasks:\n\n1. For each test case specified below, assemble $\\mathbf{A}(\\epsilon)$ from the fundamental discretization definitions given, compute $\\rho_{\\mathrm{pp}}(\\mathbf{A}(\\epsilon))$ and $\\rho_{\\mathrm{np}}(\\mathbf{A}(\\epsilon))$, and evaluate the boolean flag indicating whether pivoting is essential under the criterion $3 N \\rho_{\\mathrm{np}} u > 10^{-6}$. Additionally, report the minimum pivot magnitude encountered during naive elimination, normalized by $\\max_{i,j} |\\mathbf{A}_{ij}|$.\n\n2. For each grid size $n$ appearing in the test suite, estimate an anisotropy threshold $\\epsilon^\\star(n)$ by scanning the set $\\{\\epsilon_k = 10^{-k} : k = 0, 1, 2, \\dots, 12\\}$, assembling $\\mathbf{A}(\\epsilon_k)$ and computing $\\rho_{\\mathrm{np}}(\\mathbf{A}(\\epsilon_k))$ to evaluate $3 N \\rho_{\\mathrm{np}} u > 10^{-6}$. Define $\\epsilon^\\star(n)$ as the smallest $\\epsilon_k$ in the scan for which the inequality holds. If no $\\epsilon_k$ in the scan satisfies the inequality, report $\\epsilon^\\star(n) = 0$ to indicate that the threshold lies below $10^{-12}$ or above the scanned range.\n\nTest suite:\n\n- Case $1$: $n = 6$, $\\epsilon = 1$.\n- Case $2$: $n = 6$, $\\epsilon = 10^{-3}$.\n- Case $3$: $n = 10$, $\\epsilon = 10^{-6}$.\n- Case $4$: $n = 8$, $\\epsilon = 10^{-2}$.\n- Case $5$: $n = 4$, $\\epsilon = 10^{-8}$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to a test case and is itself a list of the form $[\\rho_{\\mathrm{pp}}, \\rho_{\\mathrm{np}}, \\text{essential}, \\epsilon^\\star(n), \\text{min\\_pivot\\_ratio}]$. The quantities $\\rho_{\\mathrm{pp}}$, $\\rho_{\\mathrm{np}}$, $\\epsilon^\\star(n)$, and $\\text{min\\_pivot\\_ratio}$ must be reported as floating-point numbers, and $\\text{essential}$ must be reported as a boolean. There are no physical units in this problem, and all angles, if any appear, should be treated as dimensionless. The final line should look like $[\\,[\\rho_{\\mathrm{pp},1}, \\rho_{\\mathrm{np},1}, \\text{essential}_1, \\epsilon^\\star(n_1), \\text{min\\_pivot\\_ratio}_1], \\dots ]$ with no additional text.", "solution": "## Problem Validation\n\n### Step 1: Extract Givens\n- **PDE Model**: Anisotropic diffusion $-\\nabla \\cdot (\\mathbf{K} \\nabla u) = f$ on a unit square domain with homogeneous Dirichlet boundary conditions.\n- **Conductivity Tensor**: $\\mathbf{K} = \\operatorname{diag}(k_x, k_y)$ with $k_x = 1$ and $k_y = \\epsilon$, where $\\epsilon \\in (0, 1]$.\n- **Discretization**: Standard second-order centered finite difference scheme on an $n \\times n$ interior grid.\n- **Grid Spacing**: Uniform spacing $h = 1/(n+1)$ in both directions.\n- **Linear System**: $\\mathbf{A}(\\epsilon)\\mathbf{u} = \\mathbf{b}$, where $\\mathbf{A}(\\epsilon)$ is an $N \\times N$ matrix with $N = n^2$.\n- **Matrix Assembly**: $\\mathbf{A}(\\epsilon) = \\mathbf{I}_n \\otimes \\mathbf{T}_y + \\mathbf{T}_x \\otimes \\mathbf{I}_n$.\n- **Component Matrices**:\n    - $\\mathbf{T}_x \\in \\mathbb{R}^{n \\times n}$: tridiagonal with main diagonal entries $2/h^2$ and sub/super-diagonal entries $-1/h^2$.\n    - $\\mathbf{T}_y \\in \\mathbb{R}^{n \\times n}$: tridiagonal with main diagonal entries $2\\epsilon/h^2$ and sub/super-diagonal entries $-\\epsilon/h^2$.\n    - $\\mathbf{I}_n$: $n \\times n$ identity matrix.\n    - $\\otimes$: Kronecker product.\n- **Growth Factor Definitions**:\n    - Partial Pivoting: $\\rho_{\\mathrm{pp}}(\\mathbf{A}) = \\dfrac{\\max_{i,j} |U^{\\mathrm{pp}}_{ij}|}{\\max_{i,j} |\\mathbf{A}_{ij}|}$.\n    - No Pivoting: $\\rho_{\\mathrm{np}}(\\mathbf{A}) = \\dfrac{\\max_{i,j} |U^{\\mathrm{np}}_{ij}|}{\\max_{i,j} |\\mathbf{A}_{ij}|}$.\n- **Backward Error Model**:\n    - Unit roundoff: $u = 2^{-53}$ (binary64/double precision).\n    - Predicted backward error (no pivot): $\\eta_{\\mathrm{np}} \\approx 3 N \\rho_{\\mathrm{np}} u$.\n- **Stability Criterion**: Pivoting is essential if $\\eta_{\\mathrm{np}} > 10^{-6}$.\n- **Task 1 Quantities**: For each test case $(n, \\epsilon)$:\n    1. $\\rho_{\\mathrm{pp}}(\\mathbf{A}(\\epsilon))$\n    2. $\\rho_{\\mathrm{np}}(\\mathbf{A}(\\epsilon))$\n    3. Boolean flag for \"essential\" pivoting based on the stability criterion.\n    4. Minimum pivot magnitude during naive elimination, normalized by $\\max_{i,j} |\\mathbf{A}_{ij}|$.\n- **Task 2 Quantity**: For each unique grid size $n$, estimate an anisotropy threshold $\\epsilon^\\star(n)$.\n    - **Scan Set**: $\\{\\epsilon_k = 10^{-k} : k = 0, 1, 2, \\dots, 12\\}$.\n    - **Definition**: $\\epsilon^\\star(n)$ is the smallest $\\epsilon_k$ in the scan for which $3 N \\rho_{\\mathrm{np}} u > 10^{-6}$.\n    - **Default Value**: If no such $\\epsilon_k$ exists in the scan, $\\epsilon^\\star(n) = 0$.\n- **Test Suite**:\n    - Case 1: $n = 6$, $\\epsilon = 1$.\n    - Case 2: $n = 6$, $\\epsilon = 10^{-3}$.\n    - Case 3: $n = 10$, $\\epsilon = 10^{-6}$.\n    - Case 4: $n = 8$, $\\epsilon = 10^{-2}$.\n    - Case 5: $n = 4$, $\\epsilon = 10^{-8}$.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientifically Grounded**: The problem describes the standard finite difference discretization of the anisotropic Poisson equation. This is a fundamental and well-established topic in numerical analysis and scientific computing. The matrix construction via Kronecker sums is a standard representation for such problems under lexicographical ordering of grid points. The concept of element growth in Gaussian elimination and its relation to backward error is a cornerstone of numerical linear algebra. The problem is firmly scientifically grounded.\n2.  **Well-Posed**: The problem is well-posed. The matrix $\\mathbf{A}(\\epsilon)$ for $\\epsilon > 0$ is symmetric and positive definite (it is an irreducibly diagonally dominant M-matrix), ensuring that Gaussian elimination without pivoting is well-defined (pivots are non-zero). The tasks are specific computational exercises with clear definitions for all quantities to be calculated. Unique solutions for these quantities exist for each test case.\n3.  **Objective**: The language is precise, using standard mathematical and numerical analysis terminology. All parameters, constants, and criteria are explicitly defined, leaving no room for subjective interpretation.\n4.  **Completeness**: The problem provides all necessary information: the PDE, discretization method, matrix construction, definitions for growth factors and stability criteria, constants, and a complete set of test cases. It is self-contained.\n5.  **No other flaws**: The problem does not exhibit any of the other invalidity flags (e.g., it is not trivial, metaphorical, or unfalsifiable). Although the stability of naive Gaussian elimination for M-matrices is a subtle topic where theoretical guarantees for exact arithmetic can be misleading in finite precision, the premise that instability can occur for highly anisotropic problems with this grid ordering is a known phenomenon in numerical analysis. The problem is a valid numerical experiment designed to probe this effect.\n\n### Step 3: Verdict and Action\nThe problem statement is **valid**. A complete solution will be provided.\n\n---\n\n## Solution Design\n\nThe solution will be structured as a Python script adhering to the specified environment. The core logic involves three main components: matrix assembly, Gaussian elimination routines, and the main processing loop for the test cases.\n\n**1. Matrix Assembly**\n\nA function `make_A(n, epsilon)` will construct the matrix $\\mathbf{A}(\\epsilon) \\in \\mathbb{R}^{N \\times N}$ where $N=n^2$.\n- It first calculates the grid spacing $h = 1/(n+1)$.\n- It then constructs the $n \\times n$ tridiagonal matrices $\\mathbf{T}_x$ and $\\mathbf{T}_y$ as specified, using `numpy.diag` for efficient creation. The entries are scaled by $1/h^2$ for $\\mathbf{T}_x$ and $\\epsilon/h^2$ for $\\mathbf{T}_y$.\n- Finally, it computes the Kronecker sum $\\mathbf{A}(\\epsilon) = \\mathbf{I}_n \\otimes \\mathbf{T}_y + \\mathbf{T}_x \\otimes \\mathbf{I}_n$ using `numpy.kron` and `numpy.identity`.\n\n**2. Gaussian Elimination and Growth Factor Calculation**\n\n- **Naive Gaussian Elimination (No Pivoting)**: A function `naive_ge(A)` will be implemented. It takes a matrix $\\mathbf{A}$ and returns the maximum absolute element in the resulting upper triangular factor $\\mathbf{U}^{\\mathrm{np}}$ and the minimum absolute pivot encountered. The function will operate on a copy of the input matrix. The core of the algorithm is a loop from $k=0$ to $N-2$, where at each step $k$, it calculates multipliers for rows $i > k$ and performs vectorized row updates: `U[i, k:] -= multiplier * U[k, k:]`. The pivots `U[k, k]` are tracked to find the minimum magnitude.\n- **Gaussian Elimination with Partial Pivoting (GEPP)**: The `scipy.linalg.lu` function will be used. Calling `P, L, U = lu(A)` directly yields the desired upper triangular factor $\\mathbf{U}^{\\mathrm{pp}}$.\n- **Growth Factors**: For both methods, the growth factor $\\rho$ will be computed according to the definition: $\\rho = \\frac{\\max_{i,j} |U_{ij}|}{\\max_{i,j} |\\mathbf{A}_{ij}|}$. The maximum absolute value of the original matrix, $\\max_{i,j} |\\mathbf{A}_{ij}|$, is simply the diagonal entry $(2+2\\epsilon)/h^2$ since $\\epsilon \\in (0,1]$ and the matrix is diagonally dominant.\n\n**3. Task Execution**\n\nThe main function `solve()` will orchestrate the entire process.\n\n- **$\\epsilon^\\star(n)$ Calculation**: A helper function `calculate_epsilon_star(n, u, tol)` will be implemented to determine the anisotropy threshold. To find the \"smallest $\\epsilon_k$\" satisfying the instability criterion, and assuming the instability is monotonic with decreasing $\\epsilon$, the most efficient search is to scan from the smallest $\\epsilon$ in the set (i.e., largest $k$) upwards. The first $\\epsilon_k$ that satisfies the condition $3 N \\rho_{\\mathrm{np}} u > 10^{-6}$ will be the minimum such value. If the loop completes without finding such an $\\epsilon_k$, it will return $0.0$.\n- **Main Loop**:\n    1.  The specified constants $u=2^{-53}$ and tolerance $10^{-6}$ are defined.\n    2.  The test cases are stored in a list.\n    3.  To avoid redundant computations, $\\epsilon^\\star(n)$ is pre-calculated for each unique grid size $n$ in the test suite and stored in a dictionary (memoization).\n    4.  The script then iterates through each $(n, \\epsilon)$ test case.\n    5.  For each case, it assembles $\\mathbf{A}(\\epsilon)$, calls the GEPP and naive GE routines to get $\\mathbf{U}^{\\mathrm{pp}}$ and $\\mathbf{U}^{\\mathrm{np}}$, and calculates the required quantities: $\\rho_{\\mathrm{pp}}$, $\\rho_{\\mathrm{np}}$, `min_pivot_ratio`.\n    6.  The boolean flag `essential` is evaluated using the formula $\\eta_{\\mathrm{np}} = 3 N \\rho_{\\mathrm{np}} u$ and the threshold $10^{-6}$.\n    7.  The pre-computed $\\epsilon^\\star(n)$ is retrieved from the cache.\n    8.  A list containing $[\\rho_{\\mathrm{pp}}, \\rho_{\\mathrm{np}}, \\text{essential}, \\epsilon^\\star(n), \\text{min\\_pivot\\_ratio}]$ is appended to a results list.\n- **Output Formatting**: Finally, the standard `print(f\"[{','.join(map(str, results))}]\")` template is used. This converts each sublist (representing a test case's results) into its string representation and joins them with commas, enclosing the entire result in square brackets, exactly matching the required output format.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import linalg\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem, calculate all required quantities,\n    and print the results in the specified format.\n    \"\"\"\n\n    def make_A(n, epsilon):\n        \"\"\"Assembles the matrix A for a given grid size n and anisotropy epsilon.\"\"\"\n        h = 1.0 / (n + 1)\n        h2 = h * h\n        \n        # T_x matrix corresponding to k_x = 1\n        diag_x = np.full(n, 2.0 / h2)\n        offdiag_x = np.full(n - 1, -1.0 / h2)\n        Tx = np.diag(diag_x) + np.diag(offdiag_x, k=1) + np.diag(offdiag_x, k=-1)\n\n        # T_y matrix corresponding to k_y = epsilon\n        diag_y = np.full(n, 2.0 * epsilon / h2)\n        offdiag_y = np.full(n - 1, -epsilon / h2)\n        Ty = np.diag(diag_y) + np.diag(offdiag_y, k=1) + np.diag(offdiag_y, k=-1)\n\n        In = np.identity(n)\n        A = np.kron(In, Ty) + np.kron(Tx, In)\n        return A\n\n    def naive_ge(A):\n        \"\"\"\n        Performs naive Gaussian elimination on matrix A.\n        Returns the max absolute value in the final U matrix and the min absolute pivot.\n        \"\"\"\n        U = A.copy().astype(np.float64) # Use float64 for precision.\n        N = U.shape[0]\n        min_abs_pivot = np.inf\n        \n        for k in range(N):\n            pivot = U[k, k]\n            # Since the matrix is SPD, pivots are guaranteed positive.\n            min_abs_pivot = min(min_abs_pivot, pivot)\n            \n            if k  N - 1:\n                # A practical check although not theoretically needed for these matrices\n                if abs(pivot)  1e-40: \n                    return np.inf, min_abs_pivot\n\n                multipliers = U[k + 1:, k] / pivot\n                U[k + 1:, k:] -= np.outer(multipliers, U[k, k:])\n                \n        max_abs_U = np.max(np.abs(U))\n        return max_abs_U, min_abs_pivot\n\n    def calculate_epsilon_star(n, u, tol):\n        \"\"\"\n        Calculates the anisotropy threshold epsilon* for a given grid size n\n        by scanning eps_k = 10**-k and finding the smallest one for which\n        the instability condition holds.\n        \"\"\"\n        N = n * n\n        h = 1.0 / (n + 1)\n\n        # Scan from largest k downwards (smallest epsilon upwards)\n        # to find the smallest epsilon_k satisfying the condition.\n        for k in range(12, -1, -1):\n            epsilon = 10.0**(-k)\n            A = make_A(n, epsilon)\n            \n            max_abs_A = (2.0 + 2.0 * epsilon) / (h**2)\n            \n            max_abs_U_np, _ = naive_ge(A)\n            \n            rho_np = max_abs_U_np / max_abs_A if max_abs_A > 0 else np.inf\n            \n            eta_np = 3 * N * rho_np * u\n            \n            if eta_np > tol:\n                return epsilon\n                \n        return 0.0\n\n    test_cases = [\n        (6, 1.0),\n        (6, 1e-3),\n        (10, 1e-6),\n        (8, 1e-2),\n        (4, 1e-8),\n    ]\n\n    u = 2.0**(-53)\n    tol = 1e-6\n    \n    results = []\n    \n    # Pre-calculate epsilon_star for unique n values\n    unique_n = sorted(list(set(case[0] for case in test_cases)))\n    epsilon_star_cache = {n: calculate_epsilon_star(n, u, tol) for n in unique_n}\n\n    for n, epsilon in test_cases:\n        N = n * n\n        A = make_A(n, epsilon)\n\n        max_abs_A = np.max(np.abs(A))\n        if max_abs_A == 0: max_abs_A = 1.0\n\n        # GEPP using scipy\n        _, _, U_pp = linalg.lu(A)\n        max_abs_U_pp = np.max(np.abs(U_pp))\n        rho_pp = max_abs_U_pp / max_abs_A\n\n        # Naive GE\n        max_abs_U_np, min_pivot = naive_ge(A)\n        rho_np = max_abs_U_np / max_abs_A\n        min_pivot_ratio = min_pivot / max_abs_A\n        \n        # Stability criterion\n        eta_np = 3 * N * rho_np * u\n        essential = eta_np > tol\n        \n        epsilon_star_n = epsilon_star_cache[n]\n        \n        results.append([rho_pp, rho_np, essential, epsilon_star_n, min_pivot_ratio])\n        \n    # Custom string formatting to match problem requirements\n    result_strings = []\n    for res in results:\n        # Booleans need to be lowercase 'true' or 'false'\n        bool_str = 'true' if res[2] else 'false'\n        # Format the list as a string\n        r_str = f\"[{res[0]},{res[1]},{bool_str},{res[3]},{res[4]}]\"\n        result_strings.append(r_str)\n\n    print(f\"[{','.join(result_strings)}]\")\n\nsolve()\n```", "id": "3378314"}]}