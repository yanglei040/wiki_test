{"hands_on_practices": [{"introduction": "To truly understand an iterative algorithm like GMRES, there is no substitute for performing the calculations by hand on a small scale. This first practice guides you through the core mechanics of GMRES for a simple $2 \\times 2$ system. By manually executing the Arnoldi process to build the Krylov subspace basis and then solving the resulting least-squares problem, you will gain a concrete understanding of how GMRES constructs its solution and minimizes the residual at each step [@problem_id:3237128].", "problem": "Consider the linear system $A x = b$ with \n$$A = \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix}, \\quad b = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}.$$\nStarting from the initial guess $x_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$, and using the standard Euclidean inner product, perform the Generalized Minimal Residual (GMRES) method (Generalized Minimal Residual (GMRES) method) for $k = 1$ and $k = 2$ iterations, without preconditioning. \n\nUse only the foundational characterization that at iteration $k$, GMRES chooses $x_k$ in $x_0 + \\mathcal{K}_k(A, r_0)$, where $\\mathcal{K}_k(A, r_0)$ is the $k$-th Krylov subspace generated by $A$ and $r_0 = b - A x_0$, so as to minimize the Euclidean norm of the residual $r_k = b - A x_k$. Construct the necessary orthonormal basis of the Krylov subspace by the Arnoldi process, and from first principles of least-squares residual minimization on the projected problem, compute the first two GMRES iterates $x_1$ and $x_2$ exactly.\n\nProvide your final answer as a single row matrix $\\begin{pmatrix} x_{1,1} & x_{1,2} & x_{2,1} & x_{2,2} \\end{pmatrix}$ with exact rational entries. No rounding is required.", "solution": "The problem is to perform the first two iterations ($k=1$ and $k=2$) of the Generalized Minimal Residual (GMRES) method for the linear system $Ax=b$, starting from a zero initial guess. The problem is well-defined and all necessary data are provided.\n\nThe given system is defined by:\n$$A = \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix}, \\quad b = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}, \\quad x_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$$\n\nThe GMRES method constructs an approximate solution $x_k$ at iteration $k$ of the form $x_k = x_0 + z_k$, where $z_k$ is chosen from the $k$-th Krylov subspace $\\mathcal{K}_k(A, r_0) = \\text{span}\\{r_0, Ar_0, \\dots, A^{k-1}r_0\\}$. The vector $z_k$ is chosen to minimize the Euclidean norm of the residual $r_k = b - Ax_k$.\n\nFirst, we calculate the initial residual $r_0$:\n$$r_0 = b - A x_0 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$$\n\nThe Arnoldi process is used to generate an orthonormal basis $V_k = [v_1, v_2, \\dots, v_k]$ for the Krylov subspace $\\mathcal{K}_k(A, r_0)$. The process starts by normalizing the initial residual. Let $\\beta = \\|r_0\\|_2$.\n$$\\beta = \\left\\| \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\right\\|_2 = \\sqrt{1^2 + 0^2} = 1$$\nThe first basis vector $v_1$ is:\n$$v_1 = \\frac{r_0}{\\beta} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$$\n\nAt iteration $k$, we seek $x_k = x_0 + V_k y_k$ for some vector $y_k \\in \\mathbb{R}^k$ that minimizes $\\|b - A(x_0 + V_k y_k)\\|_2 = \\|r_0 - A V_k y_k\\|_2$.\nFrom the Arnoldi process, we have the relation $A V_k = V_{k+1} \\bar{H}_k$, where $\\bar{H}_k$ is a $(k+1) \\times k$ upper Hessenberg matrix.\nSubstituting this into the minimization problem, and using $r_0 = \\beta v_1 = \\beta V_{k+1} e_1$ (where $e_1$ is the first standard basis vector in $\\mathbb{R}^{k+1}$), we obtain:\n$$\\| \\beta V_{k+1} e_1 - V_{k+1} \\bar{H}_k y_k \\|_2 = \\| V_{k+1} (\\beta e_1 - \\bar{H}_k y_k) \\|_2$$\nSince $V_{k+1}$ has orthonormal columns, this simplifies to finding $y_k$ that solves the least-squares problem:\n$$y_k = \\arg\\min_{y \\in \\mathbb{R}^k} \\| \\beta e_1 - \\bar{H}_k y \\|_2$$\n\n**Iteration $k=1$**\n\nWe first execute one step of the Arnoldi iteration to find $v_2$ and the matrix $\\bar{H}_1$.\nCompute $w_1 = A v_1$:\n$$w_1 = \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}$$\nThe entry $h_{1,1}$ is the projection of $w_1$ onto $v_1$:\n$$h_{1,1} = v_1^T w_1 = \\begin{pmatrix} 1 & 0 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = 2$$\nThe unnormalized next vector is $\\tilde{w}_1 = w_1 - h_{1,1}v_1$:\n$$\\tilde{w}_1 = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} - 2 \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$$\nIts norm is $h_{2,1} = \\|\\tilde{w}_1\\|_2 = \\sqrt{0^2+1^2} = 1$.\nThe second basis vector is $v_2 = \\tilde{w}_1 / h_{2,1}$:\n$$v_2 = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$$\nThe matrix $\\bar{H}_1$ is a $2 \\times 1$ matrix:\n$$\\bar{H}_1 = \\begin{pmatrix} h_{1,1} \\\\ h_{2,1} \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}$$\nWe now solve the least-squares problem for $y_1 \\in \\mathbb{R}$:\n$$y_1 = \\arg\\min_{y \\in \\mathbb{R}} \\left\\| \\beta e_1 - \\bar{H}_1 y \\right\\|_2 = \\arg\\min_{y_1 \\in \\mathbb{R}} \\left\\| \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} y_1 \\right\\|_2$$\nThe solution is given by the normal equations $(\\bar{H}_1^T \\bar{H}_1) y_1 = \\bar{H}_1^T (\\beta e_1)$.\n$$\\bar{H}_1^T \\bar{H}_1 = \\begin{pmatrix} 2 & 1 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = 2^2 + 1^2 = 5$$\n$$\\bar{H}_1^T (\\beta e_1) = \\begin{pmatrix} 2 & 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = 2$$\nSo, $5 y_1 = 2$, which gives $y_1 = \\frac{2}{5}$.\nThe first GMRES iterate $x_1$ is:\n$$x_1 = x_0 + V_1 y_1 = x_0 + v_1 y_1 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\frac{2}{5} = \\begin{pmatrix} \\frac{2}{5} \\\\ 0 \\end{pmatrix}$$\n\n**Iteration $k=2$**\n\nWe continue the Arnoldi process to find the second column of $\\bar{H}_2$.\nCompute $w_2 = A v_2$:\n$$w_2 = \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$$\nThe entries $h_{1,2}$ and $h_{2,2}$ are the projections of $w_2$ onto $v_1$ and $v_2$:\n$$h_{1,2} = v_1^T w_2 = \\begin{pmatrix} 1 & 0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} = 1$$\n$$h_{2,2} = v_2^T w_2 = \\begin{pmatrix} 0 & 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} = 2$$\nThe unnormalized next vector $\\tilde{w}_2$ is:\n$$\\tilde{w}_2 = w_2 - h_{1,2} v_1 - h_{2,2} v_2 = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} - 1 \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} - 2 \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$$\nThe norm is $h_{3,2} = \\|\\tilde{w}_2\\|_2 = 0$. This indicates that the Krylov subspace $\\mathcal{K}_2(A, r_0)$ is invariant under $A$, and the Arnoldi process terminates. For a $2 \\times 2$ system, this means GMRES(2) will find the exact solution.\nThe matrix $\\bar{H}_2$ is a $3 \\times 2$ matrix:\n$$\\bar{H}_2 = \\begin{pmatrix} h_{1,1} & h_{1,2} \\\\ h_{2,1} & h_{2,2} \\\\ 0 & h_{3,2} \\end{pmatrix} = \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\\\ 0 & 0 \\end{pmatrix}$$\nWe solve the least-squares problem for $y_2 = \\begin{pmatrix} y_{2,1} \\\\ y_{2,2} \\end{pmatrix} \\in \\mathbb{R}^2$:\n$$y_2 = \\arg\\min_{y \\in \\mathbb{R}^2} \\| \\beta e_1 - \\bar{H}_2 y \\|_2 = \\arg\\min_{y_2 \\in \\mathbb{R}^2} \\left\\| \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\\\ 0 & 0 \\end{pmatrix} y_2 \\right\\|_2$$\nSince the last row of $\\bar{H}_2$ is zero, the residual for this problem will be zero if we can solve the upper $2 \\times 2$ system exactly. We need to solve $H_2 y_2 = \\beta e_1^{(2)}$, where $H_2$ is the upper $2 \\times 2$ part of $\\bar{H}_2$ and $e_1^{(2)}$ is the first standard basis vector in $\\mathbb{R}^2$.\n$$\\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix} \\begin{pmatrix} y_{2,1} \\\\ y_{2,2} \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$$\nFrom the second equation, $y_{2,1} + 2y_{2,2} = 0$, so $y_{2,1} = -2y_{2,2}$.\nSubstituting into the first equation: $2(-2y_{2,2}) + y_{2,2} = 1$, which simplifies to $-3y_{2,2} = 1$, so $y_{2,2} = -\\frac{1}{3}$.\nThis gives $y_{2,1} = -2(-\\frac{1}{3}) = \\frac{2}{3}$.\nThus, the coefficient vector is $y_2 = \\begin{pmatrix} \\frac{2}{3} \\\\ -\\frac{1}{3} \\end{pmatrix}$.\nThe second GMRES iterate $x_2$ is:\n$$x_2 = x_0 + V_2 y_2 = x_0 + v_1 y_{2,1} + v_2 y_{2,2} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\frac{2}{3} + \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} \\left(-\\frac{1}{3}\\right) = \\begin{pmatrix} \\frac{2}{3} \\\\ -\\frac{1}{3} \\end{pmatrix}$$\nAs expected, $x_2$ is the exact solution to the system $Ax=b$.\n\nThe computed iterates are $x_1 = \\begin{pmatrix} \\frac{2}{5} \\\\ 0 \\end{pmatrix}$ and $x_2 = \\begin{pmatrix} \\frac{2}{3} \\\\ -\\frac{1}{3} \\end{pmatrix}$. The required components are $x_{1,1} = \\frac{2}{5}$, $x_{1,2} = 0$, $x_{2,1} = \\frac{2}{3}$, and $x_{2,2} = -\\frac{1}{3}$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{2}{5} & 0 & \\frac{2}{3} & -\\frac{1}{3}\n\\end{pmatrix}\n}\n$$", "id": "3237128"}, {"introduction": "A key feature of a practical GMRES implementation is its ability to monitor convergence without explicitly forming the solution vector $x_k$ at every iteration. This efficiency is achieved by updating the residual norm directly from the small Hessenberg matrix. This exercise delves into this mechanism, asking you to derive the updated residual norm after one step using a Givens rotation [@problem_id:3399084]. Mastering this concept reveals how the algorithm's progress is tracked efficiently.", "problem": "Consider the generalized minimal residual method (GMRES, Generalized Minimal Residual) applied to the linear system arising from a stable finite volume discretization of a steady, linear advectionâ€“diffusion partial differential equation on a square domain with Dirichlet boundary conditions. Let the discrete system be $A x = b$, with $A \\in \\mathbb{R}^{n \\times n}$ large, sparse, and nonsymmetric. GMRES constructs the Krylov subspace via the Arnoldi process. After one Arnoldi step starting from an initial guess $x_0$, the method yields the orthonormal vectors $V_2 \\in \\mathbb{R}^{n \\times 2}$ and the $2 \\times 1$ upper Hessenberg column\n$$\n\\bar{H}_1 \\;=\\; \\begin{bmatrix} h_{1,1} \\\\ h_{2,1} \\end{bmatrix},\n$$\nwith the initial residual norm $\\beta = \\|r_0\\|_2 = \\|b - A x_0\\|_2$. GMRES computes the minimizer of the least-squares problem $\\min_{y \\in \\mathbb{R}} \\|\\beta e_1 - \\bar{H}_1 y\\|_2$ by applying a single orthogonal Givens rotation $G_1 \\in \\mathbb{R}^{2 \\times 2}$ with parameters $c_1$ and $s_1$ that annihilates the entry $h_{2,1}$, i.e.,\n$$\nG_1 \\;=\\; \\begin{bmatrix} c_1 & s_1 \\\\ -s_1 & c_1 \\end{bmatrix}, \\quad c_1^2 + s_1^2 = 1, \\quad G_1 \\begin{bmatrix} h_{1,1} \\\\ h_{2,1} \\end{bmatrix} \\;=\\; \\begin{bmatrix} \\rho_1 \\\\ 0 \\end{bmatrix},\n$$\nwith $\\rho_1 \\in \\mathbb{R}$ chosen nonnegative.\n\nUsing only the foundational setup of GMRES (Arnoldi factorization and least-squares residual minimization via orthogonal transformations) and the defining properties of a Givens rotation, derive a closed-form analytic expression for the updated residual norm after this single rotation in terms of $\\beta$, $h_{1,1}$, and $h_{2,1}$. Your final answer must be a single analytic expression and should not require any rounding.", "solution": "The problem statement has been validated and found to be scientifically grounded, well-posed, and objective. It represents a standard derivation in the context of the generalized minimal residual (GMRES) method for solving linear systems. All provided information is consistent and sufficient to derive the requested expression.\n\nThe objective is to derive an expression for the norm of the residual vector after one step of the GMRES algorithm. The GMRES method seeks an approximate solution $x_1$ of the form $x_1 = x_0 + z_0$, where $z_0$ is in the first Krylov subspace $\\mathcal{K}_1(A, r_0) = \\text{span}\\{r_0\\}$. A more convenient basis for this subspace is the single orthonormal vector $v_1 = r_0 / \\|r_0\\|_2 = r_0 / \\beta$. Thus, $x_1 = x_0 + y_1 v_1$ for some scalar coefficient $y_1 \\in \\mathbb{R}$ that minimizes the norm of the new residual, $r_1 = b - A x_1$.\n\nThe residual $r_1$ can be expressed in terms of the initial residual $r_0$:\n$$\nr_1 = b - A(x_0 + y_1 v_1) = (b - A x_0) - y_1 A v_1 = r_0 - y_1 A v_1\n$$\nUsing $r_0 = \\beta v_1$, this becomes:\n$$\nr_1 = \\beta v_1 - y_1 A v_1\n$$\nThe Arnoldi process constructs an orthonormal basis $V_{m+1} = [v_1, \\dots, v_{m+1}]$ for the Krylov subspace $\\mathcal{K}_{m+1}(A, r_0)$ and a corresponding upper Hessenberg matrix $\\bar{H}_m$ such that the relation $A V_m = V_{m+1} \\bar{H}_m$ holds. For the first step ($m=1$), we have $V_1 = [v_1]$, $V_2 = [v_1, v_2]$, and $\\bar{H}_1 \\in \\mathbb{R}^{2 \\times 1}$. The relation is $A v_1 = V_2 \\bar{H}_1$. The problem statement provides $\\bar{H}_1 = \\begin{bmatrix} h_{1,1} \\\\ h_{2,1} \\end{bmatrix}$.\n\nSubstituting $A v_1 = V_2 \\bar{H}_1$ into the expression for $r_1$:\n$$\nr_1 = \\beta v_1 - y_1 (V_2 \\bar{H}_1)\n$$\nThe vector $v_1$ can be written as $V_2 e_1$, where $e_1 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} \\in \\mathbb{R}^2$. This gives:\n$$\nr_1 = \\beta V_2 e_1 - y_1 V_2 \\bar{H}_1 = V_2 (\\beta e_1 - y_1 \\bar{H}_1)\n$$\nWe need to find the norm of this residual, $\\|r_1\\|_2$. Since the columns of $V_2$ are orthonormal, multiplication by $V_2$ is an isometry and preserves the Euclidean norm. Therefore:\n$$\n\\|r_1\\|_2 = \\|V_2 (\\beta e_1 - y_1 \\bar{H}_1)\\|_2 = \\|\\beta e_1 - y_1 \\bar{H}_1\\|_2\n$$\nGMRES finds the scalar $y_1$ (denoted as $y$ in the problem) that minimizes this norm. The norm of the updated residual is this minimum value:\n$$\n\\|r_1\\|_2 = \\min_{y \\in \\mathbb{R}} \\|\\beta e_1 - \\bar{H}_1 y\\|_2\n$$\nLet's express the vector inside the norm explicitly:\n$$\n\\beta e_1 - \\bar{H}_1 y = \\beta \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} - y \\begin{bmatrix} h_{1,1} \\\\ h_{2,1} \\end{bmatrix} = \\begin{bmatrix} \\beta - y h_{1,1} \\\\ -y h_{2,1} \\end{bmatrix}\n$$\nThe problem states that this least-squares problem is solved by applying a Givens rotation $G_1 = \\begin{bmatrix} c_1 & s_1 \\\\ -s_1 & c_1 \\end{bmatrix}$. Since $G_1$ is an orthogonal matrix, it also preserves the Euclidean norm. Thus:\n$$\n\\|\\beta e_1 - \\bar{H}_1 y\\|_2 = \\|G_1 (\\beta e_1 - \\bar{H}_1 y)\\|_2 = \\|\\beta G_1 e_1 - y G_1 \\bar{H}_1\\|_2\n$$\nWe evaluate the transformed components. The problem states that $G_1$ is constructed to annihilate the subdiagonal element of $\\bar{H}_1$:\n$$\nG_1 \\bar{H}_1 = G_1 \\begin{bmatrix} h_{1,1} \\\\ h_{2,1} \\end{bmatrix} = \\begin{bmatrix} \\rho_1 \\\\ 0 \\end{bmatrix}\n$$\nThe transformed right-hand side vector is:\n$$\n\\beta G_1 e_1 = \\beta \\begin{bmatrix} c_1 & s_1 \\\\ -s_1 & c_1 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} = \\beta \\begin{bmatrix} c_1 \\\\ -s_1 \\end{bmatrix}\n$$\nSubstituting these into the norm expression:\n$$\n\\|r_1\\|_2 = \\min_{y \\in \\mathbb{R}} \\left\\| \\beta \\begin{bmatrix} c_1 \\\\ -s_1 \\end{bmatrix} - y \\begin{bmatrix} \\rho_1 \\\\ 0 \\end{bmatrix} \\right\\|_2 = \\min_{y \\in \\mathbb{R}} \\left\\| \\begin{bmatrix} \\beta c_1 - y \\rho_1 \\\\ -\\beta s_1 \\end{bmatrix} \\right\\|_2\n$$\nThe squared norm is $(\\beta c_1 - y \\rho_1)^2 + (-\\beta s_1)^2$. To minimize this with respect to $y$, we set the term involving $y$ to zero. This occurs when $y \\rho_1 = \\beta c_1$. The minimum value of the squared norm is then $(0)^2 + (-\\beta s_1)^2 = \\beta^2 s_1^2$.\nThe updated residual norm is the square root of this minimum value:\n$$\n\\|r_1\\|_2 = \\sqrt{\\beta^2 s_1^2} = |\\beta s_1|\n$$\nSince $\\beta = \\|r_0\\|_2$ is a norm, it is non-negative, so $\\|r_1\\|_2 = \\beta |s_1|$.\n\nThe final step is to find an expression for $|s_1|$ in terms of $h_{1,1}$ and $h_{2,1}$. The definition of the Givens rotation gives the system of equations:\n$$\n\\begin{cases}\nc_1 h_{1,1} + s_1 h_{2,1} = \\rho_1 \\\\\n-s_1 h_{1,1} + c_1 h_{2,1} = 0\n\\end{cases}\n$$\nalong with the property $c_1^2 + s_1^2 = 1$. From the second equation, we have $c_1 h_{2,1} = s_1 h_{1,1}$. Squaring this yields $c_1^2 h_{2,1}^2 = s_1^2 h_{1,1}^2$. Substituting $c_1^2 = 1 - s_1^2$:\n$$\n(1 - s_1^2) h_{2,1}^2 = s_1^2 h_{1,1}^2\n$$\n$$\nh_{2,1}^2 - s_1^2 h_{2,1}^2 = s_1^2 h_{1,1}^2\n$$\n$$\nh_{2,1}^2 = s_1^2 (h_{1,1}^2 + h_{2,1}^2)\n$$\nSolving for $s_1^2$ gives:\n$$\ns_1^2 = \\frac{h_{2,1}^2}{h_{1,1}^2 + h_{2,1}^2}\n$$\nTaking the square root gives the magnitude of $s_1$:\n$$\n|s_1| = \\sqrt{\\frac{h_{2,1}^2}{h_{1,1}^2 + h_{2,1}^2}} = \\frac{|h_{2,1}|}{\\sqrt{h_{1,1}^2 + h_{2,1}^2}}\n$$\nThis expression is sufficient, as the problem does not require determining the sign of $s_1$. The condition $\\rho_1 \\ge 0$ in the problem statement serves to uniquely define the Givens parameters $c_1$ and $s_1$, but their magnitudes are all that is required here.\n\nFinally, substituting the expression for $|s_1|$ into the formula for the residual norm:\n$$\n\\|r_1\\|_2 = \\beta |s_1| = \\beta \\frac{|h_{2,1}|}{\\sqrt{h_{1,1}^2 + h_{2,1}^2}}\n$$\nThis is the closed-form analytic expression for the updated residual norm after one step of GMRES, in terms of the given quantities $\\beta$, $h_{1,1}$, and $h_{2,1}$.", "answer": "$$\n\\boxed{\\beta \\frac{|h_{2,1}|}{\\sqrt{h_{1,1}^{2} + h_{2,1}^{2}}}}\n$$", "id": "3399084"}, {"introduction": "While full GMRES is robust, its practical variant, restarted GMRES($m$), can suffer from a phenomenon known as stagnation, where the method fails to make progress. This final practice challenges you to explore this critical limitation by constructing a specific matrix and initial residual that cause GMRES($2$) to stagnate while full GMRES still converges [@problem_id:3399033]. By analyzing the underlying Arnoldi process and verifying the behavior computationally, you will develop a deeper, more nuanced understanding of the algorithm's practical performance.", "problem": "Let $A \\in \\mathbb{R}^{4 \\times 4}$ be a nonsingular matrix and consider solving the linear system $A x = b$ with an initial guess $x_0$ using the Generalized Minimal Residual method (GMRES). The Generalized Minimal Residual method (GMRES) constructs the $k$-th approximation $x_k$ in the affine space $x_0 + \\mathcal{K}_k(A, r_0)$, where $\\mathcal{K}_k(A, r_0) = \\operatorname{span}\\{r_0, A r_0, A^2 r_0, \\dots, A^{k-1} r_0\\}$ denotes the $k$-step Krylov subspace and $r_0 = b - A x_0$ is the initial residual. Using the Arnoldi process, GMRES forms an orthonormal basis $V_k = [v_1, \\dots, v_k]$ of $\\mathcal{K}_k(A, r_0)$, together with an upper Hessenberg matrix $H_k \\in \\mathbb{R}^{(k+1)\\times k}$ satisfying $A V_k = V_{k+1} H_k$, and determines the correction $y_k \\in \\mathbb{R}^k$ by minimizing the Euclidean norm $\\| \\beta e_1 - H_k y \\|_2$, where $\\beta = \\| r_0 \\|_2$ and $e_1 \\in \\mathbb{R}^{k+1}$ is the first standard basis vector.\n\nA restarted variant, GMRES($m$), performs $m$ steps, updates the iterate, and then restarts the Arnoldi process with the new residual, repeating this procedure over multiple cycles. It is well known that adverse restart behavior can cause stagnation, where the residual norm does not decrease over cycles even when full GMRES would converge.\n\nConstruct a specific $4 \\times 4$ matrix $A$ and initial residual $r_0$ for which full GMRES converges in $4$ steps, but GMRES($2$) stagnates over multiple cycles due to adverse restart behavior. Use the following construction for $A$ with parameters $\\alpha, \\beta, \\gamma \\in \\mathbb{R}$:\n$$\nA(\\alpha,\\beta,\\gamma) = \\begin{bmatrix}\n0 & 0 & \\alpha & 0 \\\\\n1 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0 \\\\\n0 & 0 & \\gamma & \\beta\n\\end{bmatrix},\n$$\nand take $r_0 = e_1 = [1,0,0,0]^\\top$ with $x_0 = 0$ and $b = r_0$. This $A$ is nonsingular for $\\alpha \\neq 0$ and $\\beta \\neq 0$.\n\nStarting from the fundamental definition of Krylov subspaces and the Arnoldi relation $A V_k = V_{k+1} H_k$, derive conditions under which the first row of $H_2$ becomes zero, making GMRES($2$) stagnate in the least-squares problem $\\min_{y\\in\\mathbb{R}^2} \\|\\beta e_1 - H_2 y\\|_2$ because the first component cannot be reduced. Then, analyze how full GMRES with $k=4$ can produce exact convergence by solving $H_4 y = \\beta e_1$ when $H_4$ has full column rank.\n\nYour program must:\n- Implement GMRES and restarted GMRES according to the above definitions, using the Arnoldi process and solving the least-squares problems for the correction vectors.\n- Construct the matrix $A(\\alpha,\\beta,\\gamma)$ and verify the specified convergence and stagnation properties numerically.\n- Include a test suite comprising the following four cases:\n    1. Happy path: $\\alpha = 2$, $\\beta = 3$, $\\gamma = 1$, $r_0 = e_1$, $x_0 = 0$, $b = r_0$. Verify that full GMRES achieves a residual norm below $10^{-12}$ within $4$ steps and that restarted GMRES($2$) stagnates over $5$ cycles (the residual norms at the end of each cycle remain equal to the initial residual norm within a tolerance of $10^{-12}$).\n    2. Boundary condition: Same $A$ and $r_0$ as case $1$, but use GMRES($4$) with a single cycle. Verify that the residual norm at the end of the cycle is below $10^{-12}$.\n    3. Edge case (perturbation breaking stagnation): Modify $A$ by setting $A_{1,2} \\leftarrow \\varepsilon$ with $\\varepsilon = 0.1$ while keeping $\\alpha = 2$, $\\beta = 3$, $\\gamma = 1$, and use $r_0 = e_1$. Verify that restarted GMRES($2$) over $5$ cycles does not stagnate, meaning the residual norm at the end of the last cycle is strictly smaller than the initial residual norm by at least $10^{-6}$.\n    4. Edge case (different initial residual): Use the original $A$ with $\\alpha = 2$, $\\beta = 3$, $\\gamma = 1$ but take $r_0 = \\frac{1}{2}[1,1,1,1]^\\top$, $x_0 = 0$, and $b = r_0$. Verify that restarted GMRES($2$) over $3$ cycles does not stagnate, meaning the residual norm at the end of the last cycle is strictly smaller than the initial residual norm by at least $10^{-6}$.\n\nFor all cases, measure residual norms in the standard Euclidean norm with no physical units involved. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3,result4]\"), where each result is a boolean indicating whether the specified condition for that test case was satisfied.", "solution": "The provided problem is valid. It is scientifically grounded in the well-established theory of Krylov subspace methods for numerical linear algebra, specifically the Generalized Minimal Residual (GMRES) method. The problem is well-posed, objective, and provides a complete, consistent setup for a specific numerical analysis task: to demonstrate and analyze the stagnation phenomenon in restarted GMRES.\n\nThe solution proceeds in two parts. First, a theoretical derivation is provided to explain the conditions leading to stagnation in GMRES($2$) for the given matrix structure, and why full GMRES($4$) still converges. Second, a Python implementation is constructed to verify these findings numerically across the specified test cases.\n\n### Theoretical Analysis of GMRES Behavior\n\nWe analyze the behavior of GMRES for the linear system $A x = b$ with the given matrix $A(\\alpha, \\beta, \\gamma)$ and initial conditions.\nThe matrix is:\n$$\nA = A(\\alpha,\\beta,\\gamma) = \\begin{bmatrix}\n0 & 0 & \\alpha & 0 \\\\\n1 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0 \\\\\n0 & 0 & \\gamma & \\beta\n\\end{bmatrix}\n$$\nThe initial guess is $x_0 = 0$, so the initial residual is $r_0 = b - A x_0 = b$. We are given $r_0 = e_1 = [1,0,0,0]^\\top$, hence $b=e_1$. The norm of the initial residual, which we denote $\\rho_0$ to avoid confusion with the parameter $\\beta$ in the matrix $A$, is $\\rho_0 = \\|r_0\\|_2 = \\|e_1\\|_2 = 1$.\n\nThe GMRES method uses the Arnoldi process to build an orthonormal basis $V_k = [v_1, \\dots, v_k]$ for the Krylov subspace $\\mathcal{K}_k(A, r_0)$. The first basis vector is $v_1 = r_0 / \\rho_0 = e_1$.\n\n#### Arnoldi Process for $k=1, 2$\n\nLet's perform the first two steps of the Arnoldi process.\n\n**Step 1: Construct $v_2$ and the first column of $H_2$.**\nThe process starts with $v_1 = e_1$. We compute $w_1 = A v_1$:\n$$ w_1 = A v_1 = A e_1 = [0, 1, 0, 0]^\\top = e_2 $$\nThe entries of the first column of the Hessenberg matrix $H$ are the projections of $w_1$ onto the existing basis vectors.\n$$ h_{1,1} = v_1^\\top w_1 = e_1^\\top e_2 = 0 $$\nWe orthogonalize $w_1$ against $v_1$:\n$$ \\tilde{w}_1 = w_1 - h_{1,1} v_1 = e_2 - 0 \\cdot e_1 = e_2 $$\nThe subdiagonal element $h_{2,1}$ is the norm of the result:\n$$ h_{2,1} = \\|\\tilde{w}_1\\|_2 = \\|e_2\\|_2 = 1 $$\nThe next basis vector is the normalized $\\tilde{w}_1$:\n$$ v_2 = \\tilde{w}_1 / h_{2,1} = e_2 $$\n\n**Step 2: Construct $v_3$ and the second column of $H_2$.**\nWe compute $w_2 = A v_2$:\n$$ w_2 = A v_2 = A e_2 = [0, 0, 1, 0]^\\top = e_3 $$\nThe entries of the second column of $H$ are the projections of $w_2$ onto $v_1$ and $v_2$:\n$$ h_{1,2} = v_1^\\top w_2 = e_1^\\top e_3 = 0 $$\n$$ h_{2,2} = v_2^\\top w_2 = e_2^\\top e_3 = 0 $$\nSo, for GMRES($2$), the upper Hessenberg matrix $H_2 \\in \\mathbb{R}^{3 \\times 2}$ is:\n$$\nH_2 = \\begin{bmatrix} h_{1,1} & h_{1,2} \\\\ h_{2,1} & h_{2,2} \\\\ 0 & h_{3,2} \\end{bmatrix}\n$$\nTo complete this, we compute $h_{3,2}$. First, we orthogonalize $w_2$:\n$$ \\tilde{w}_2 = w_2 - h_{1,2} v_1 - h_{2,2} v_2 = e_3 - 0 \\cdot e_1 - 0 \\cdot e_2 = e_3 $$\nAnd find its norm:\n$$ h_{3,2} = \\|\\tilde{w}_2\\|_2 = \\|e_3\\|_2 = 1 $$\nTherefore, the Arnoldi process for $k=2$ yields $V_3 = [e_1, e_2, e_3]$ and the matrix:\n$$\nH_2 = \\begin{bmatrix} 0 & 0 \\\\ 1 & 0 \\\\ 0 & 1 \\end{bmatrix}\n$$\n\n#### Stagnation of GMRES($2$)\n\nIn GMRES($m$), the updated iterate is $x_m = x_0 + V_m y_m$, where $y_m$ solves the least-squares problem $\\min_{y \\in \\mathbb{R}^m} \\| \\rho_0 e_1 - H_m y \\|_2$. For our case with $m=2$, we have $\\rho_0=1$ and $e_1 \\in \\mathbb{R}^{3}$. We need to solve:\n$$ \\min_{y \\in \\mathbb{R}^2} \\left\\| \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix} - \\begin{bmatrix} 0 & 0 \\\\ 1 & 0 \\\\ 0 & 1 \\end{bmatrix} \\begin{bmatrix} y_1 \\\\ y_2 \\end{bmatrix} \\right\\|_2 $$\nThe norm of the residual vector is:\n$$ \\left\\| \\begin{bmatrix} 1 - (0 \\cdot y_1 + 0 \\cdot y_2) \\\\ 0 - (1 \\cdot y_1 + 0 \\cdot y_2) \\\\ 0 - (0 \\cdot y_1 + 1 \\cdot y_2) \\end{bmatrix} \\right\\|_2 = \\left\\| \\begin{bmatrix} 1 \\\\ -y_1 \\\\ -y_2 \\end{bmatrix} \\right\\|_2 = \\sqrt{1^2 + y_1^2 + y_2^2} $$\nThis expression is minimized when $y_1=0$ and $y_2=0$. The optimal solution is $y_2 = [0,0]^\\top$.\n\nThe cause of this behavior is that the first row of $H_2$ is entirely zero. This occurs because for $v_1=e_1$ and $v_2=e_2$, we have $h_{1,1} = v_1^\\top A v_1 = A_{11}$ and $h_{1,2} = v_1^\\top A v_2 = A_{12}$, both of which are zero for the given matrix structure. Because the first component of the target vector $\\rho_0 e_1$ is non-zero, but the first row of $H_2$ is zero, this component of the residual cannot be reduced.\n\nWith $y_2 = 0$, the solution update is $x_2 = x_0 + V_2 y_2 = 0 + V_2 \\cdot 0 = 0$. The new solution is the same as the initial guess. The new residual is $r_2 = b - A x_2 = b = r_0$. When GMRES($2$) is restarted, it begins with the exact same residual $r_0$, and the process repeats identically, making no progress. This is stagnation.\n\n#### Convergence of Full GMRES($4$)\n\nFull GMRES does not restart until the subspace spans the entire space (i.e., $k=n=4$). Let's continue the Arnoldi process. We have $v_1=e_1, v_2=e_2, v_3=e_3$.\n\n**Step 3: Construct $v_4$ and the third column of $H_4$.**\nWe compute $w_3 = A v_3 = A e_3 = [\\alpha, 0, 0, \\gamma]^\\top$.\n$$ h_{1,3} = v_1^\\top w_3 = e_1^\\top [\\alpha, 0, 0, \\gamma]^\\top = \\alpha $$\n$$ h_{2,3} = v_2^\\top w_3 = e_2^\\top [\\alpha, 0, 0, \\gamma]^\\top = 0 $$\n$$ h_{3,3} = v_3^\\top w_3 = e_3^\\top [\\alpha, 0, 0, \\gamma]^\\top = 0 $$\nOrthogonalize $w_3$:\n$$ \\tilde{w}_3 = w_3 - h_{1,3} v_1 - h_{2,3} v_2 - h_{3,3} v_3 = [\\alpha, 0, 0, \\gamma]^\\top - \\alpha e_1 = [0, 0, 0, \\gamma]^\\top = \\gamma e_4 $$\n$$ h_{4,3} = \\|\\tilde{w}_3\\|_2 = |\\gamma| $$\nAssuming $\\gamma \\neq 0$, the next basis vector is $v_4 = \\tilde{w}_3 / h_{4,3} = \\operatorname{sgn}(\\gamma) e_4$.\n\n**Step 4: Construct the fourth column of $H_4$.**\nWe compute $w_4 = A v_4 = A (\\operatorname{sgn}(\\gamma) e_4) = \\operatorname{sgn}(\\gamma) [0, 0, 0, \\beta]^\\top = \\operatorname{sgn}(\\gamma) \\beta e_4$.\n$$ h_{1,4} = v_1^\\top w_4 = 0, \\quad h_{2,4} = v_2^\\top w_4 = 0, \\quad h_{3,4} = v_3^\\top w_4 = 0 $$\n$$ h_{4,4} = v_4^\\top w_4 = (\\operatorname{sgn}(\\gamma)e_4)^\\top (\\operatorname{sgn}(\\gamma)\\beta e_4) = \\beta $$\nOrthogonalize $w_4$:\n$$ \\tilde{w}_4 = w_4 - \\sum_{i=1}^4 h_{i,4} v_i = (\\operatorname{sgn}(\\gamma)\\beta e_4) - h_{4,4} v_4 = (\\operatorname{sgn}(\\gamma)\\beta e_4) - \\beta (\\operatorname{sgn}(\\gamma)e_4) = 0 $$\n$$ h_{5,4} = \\|\\tilde{w}_4\\|_2 = 0 $$\nSince $h_{5,4}=0$, the Arnoldi process terminates (a \"happy breakdown\"). The Krylov subspace $\\mathcal{K}_4(A, r_0)$ is invariant under $A$. This implies that the exact solution to $Ax=b$ lies in the affine space $x_0 + \\mathcal{K}_4(A, r_0)$.\n\nThe full $5 \\times 4$ Hessenberg matrix $H_4$ is:\n$$\nH_4 = \\begin{bmatrix}\n0 & 0 & \\alpha & 0 \\\\\n1 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0 \\\\\n0 & 0 & |\\gamma| & \\beta \\\\\n0 & 0 & 0 & 0\n\\end{bmatrix}\n$$\nThe GMRES($4$) least-squares problem is $\\min_{y \\in \\mathbb{R}^4} \\| \\rho_0 e_1 - H_4 y \\|_2$ with $\\rho_0=1$ and $e_1 \\in \\mathbb{R}^5$. Since the last row of $H_4$ is zero, a zero residual is achievable if we can solve the square system $\\bar{H}_4 y = \\rho_0 \\bar{e}_1$, where $\\bar{H}_4$ is the upper $4 \\times 4$ block of $H_4$:\n$$\n\\begin{bmatrix}\n0 & 0 & \\alpha & 0 \\\\\n1 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0 \\\\\n0 & 0 & |\\gamma| & \\beta\n\\end{bmatrix}\n\\begin{bmatrix} y_1 \\\\ y_2 \\\\ y_3 \\\\ y_4 \\end{bmatrix}\n=\n\\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix}\n$$\nThe determinant of this $4 \\times 4$ matrix $\\bar{H}_4$ is $-\\alpha \\beta$. Since the problem specifies that $A$ is nonsingular ($\\alpha \\neq 0, \\beta \\neq 0$), $\\det(\\bar{H}_4) \\neq 0$. Thus, a unique solution $y_4$ exists, which results in a residual norm of zero. Full GMRES converges in $4$ steps.\n\nThe test cases are designed to confirm this analysis: stagnation occurs for the special matrix and $r_0$, but is broken by perturbing the matrix (Case 3, where $A_{12} \\neq 0$ so $h_{1,2} \\neq 0$) or by choosing a different initial residual that does not align with the special structure (Case 4).", "answer": "```python\nimport numpy as np\n\ndef gmres_custom(A, b, x0, m, max_cycles, tol=1e-15):\n    \"\"\"\n    Implements the Generalized Minimal Residual (GMRES) method.\n\n    Args:\n        A (np.ndarray): The coefficient matrix of the linear system.\n        b (np.ndarray): The right-hand side vector.\n        x0 (np.ndarray): The initial guess for the solution.\n        m (int): The restart parameter (dimension of the Krylov subspace).\n        max_cycles (int): The maximum number of restart cycles.\n        tol (float): The tolerance for convergence.\n\n    Returns:\n        tuple: A tuple containing:\n            - x (np.ndarray): The final computed solution.\n            - residual_norms (list): A list of residual norms, starting with the\n              initial norm, followed by the norm after each cycle.\n    \"\"\"\n    n = A.shape[0]\n    x = x0.copy()\n\n    # Store history of residual norms (initial + after each cycle)\n    residual_norms = []\n\n    r = b - A @ x\n    initial_r_norm = np.linalg.norm(r)\n    residual_norms.append(initial_r_norm)\n\n    if initial_r_norm < tol:\n        return x, residual_norms\n\n    for cycle in range(max_cycles):\n        # Arnoldi Process\n        V = np.zeros((n, m + 1), dtype=float)\n        H = np.zeros((m + 1, m), dtype=float)\n\n        current_r_norm = np.linalg.norm(r)\n        if current_r_norm < tol:\n            break\n        V[:, 0] = r / current_r_norm\n\n        # This is the `beta*e_1` vector for the least-squares problem,\n        # where beta is the norm of the current residual.\n        e1_vec = np.zeros(m + 1)\n        e1_vec[0] = current_r_norm\n\n        actual_m = m\n        for j in range(m):\n            w = A @ V[:, j]\n            for i in range(j + 1):\n                H[i, j] = V[:, i].T @ w\n                w = w - H[i, j] * V[:, i]\n\n            H[j + 1, j] = np.linalg.norm(w)\n\n            # Happy breakdown: subspace is A-invariant\n            if H[j + 1, j] < 1e-16:\n                actual_m = j + 1\n                break\n\n            V[:, j + 1] = w / H[j + 1, j]\n\n        # Adjust H and e1_vec to the actual subspace dimension\n        H_m = H[:actual_m + 1, :actual_m]\n        e1_vec_m = e1_vec[:actual_m + 1]\n\n        # Solve the least-squares problem: min || beta*e1 - H*y ||\n        y, _, _, _ = np.linalg.lstsq(H_m, e1_vec_m, rcond=None)\n\n        # Update the solution\n        x += V[:, :actual_m] @ y\n\n        # Update residual for the next cycle\n        r = b - A @ x\n        residual_norms.append(np.linalg.norm(r))\n\n    return x, residual_norms\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite for GMRES behavior.\n    \"\"\"\n    results = []\n\n    # --- Test Case 1: Stagnation ---\n    alpha, beta, gamma = 2.0, 3.0, 1.0\n    A1 = np.array([\n        [0, 0, alpha, 0],\n        [1, 0, 0,     0],\n        [0, 1, 0,     0],\n        [0, 0, gamma, beta]\n    ], dtype=float)\n    x0 = np.zeros(4)\n    r0 = np.array([1.0, 0.0, 0.0, 0.0])\n    b = r0.copy()\n\n    # Full GMRES(4)\n    _, res_norms_full = gmres_custom(A1, b, x0, m=4, max_cycles=1)\n    cond1_full = res_norms_full[-1] < 1e-12\n\n    # Restarted GMRES(2)\n    _, res_norms_restarted = gmres_custom(A1, b, x0, m=2, max_cycles=5)\n    r_norm_initial = res_norms_restarted[0]\n    cond1_stagnation = all(\n        abs(res_norms_restarted[i] - r_norm_initial) < 1e-12 \n        for i in range(1, 6)\n    )\n    results.append(cond1_full and cond1_stagnation)\n\n    # --- Test Case 2: Boundary Condition (GMRES(4)) ---\n    # This is identical to the first part of Case 1, as GMRES(4) is full GMRES for n=4.\n    _, res_norms_boundary = gmres_custom(A1, b, x0, m=4, max_cycles=1)\n    cond2 = res_norms_boundary[-1] < 1e-12\n    results.append(cond2)\n\n    # --- Test Case 3: Perturbation Breaking Stagnation ---\n    epsilon = 0.1\n    A3 = A1.copy()\n    A3[0, 1] = epsilon  # A_{1,2} in 1-based indexing\n\n    _, res_norms_pert = gmres_custom(A3, b, x0, m=2, max_cycles=5)\n    cond3 = res_norms_pert[-1] < res_norms_pert[0] - 1e-6\n    results.append(cond3)\n    \n    # --- Test Case 4: Different Initial Residual ---\n    A4 = A1.copy()  # Original matrix\n    r0_4 = 0.5 * np.ones(4)\n    b4 = r0_4.copy()\n\n    _, res_norms_r0 = gmres_custom(A4, b4, x0, m=2, max_cycles=3)\n    cond4 = res_norms_r0[-1] < res_norms_r0[0] - 1e-6\n    results.append(cond4)\n\n    # Final print statement\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3399033"}]}