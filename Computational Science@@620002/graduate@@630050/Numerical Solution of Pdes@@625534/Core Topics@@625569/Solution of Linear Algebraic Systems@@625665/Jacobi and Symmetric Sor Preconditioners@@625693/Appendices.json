{"hands_on_practices": [{"introduction": "A crucial first step in understanding any preconditioner is to analyze its behavior on a simple, well-understood model problem. This exercise examines the Jacobi preconditioner applied to the matrix arising from the finite difference discretization of the one-dimensional Poisson equation. By analytically deriving the eigenvalues of the original and the preconditioned systems, you will gain fundamental insight into how Jacobi preconditioning works and, more importantly, uncover a key scenario where it provides no benefit to the condition number, motivating the need for more advanced methods.", "problem": "Consider the one-dimensional Poisson equation $-u''(x) = f(x)$ on the interval $(0,1)$ with Dirichlet boundary conditions $u(0)=0$ and $u(1)=0$. Discretize the interval with $n$ interior grid points at locations $x_i = i h$ for $i=1,2,\\dots,n$, where $h = \\frac{1}{n+1}$. Using the standard second-order centered finite difference approximation to $-u''(x)$, one obtains a linear system $A \\, \\mathbf{u} = \\mathbf{b}$, where $\\mathbf{u} \\in \\mathbb{R}^{n}$ and $A \\in \\mathbb{R}^{n \\times n}$ is the tridiagonal matrix with entries $A_{ii} = \\frac{2}{h^{2}}$ and $A_{i,i\\pm1} = -\\frac{1}{h^{2}}$. Let $D$ denote the diagonal of $A$. Define the Jacobi preconditioned operator by $D^{-1} A$.\n\nStarting from first principles—namely, the finite difference construction of $A$ and the definition of eigenvalues and eigenvectors for linear operators—derive the eigenvalues of $A$ and of $D^{-1} A$. Then use these eigenvalues to assess the effect of Jacobi preconditioning on the asymptotic two-norm condition number. Specifically, compute the exact value of\n$$\n\\lim_{n \\to \\infty} \\frac{\\kappa_{2}\\!\\left(D^{-1} A\\right)}{\\kappa_{2}(A)} \\, ,\n$$\nwhere $\\kappa_{2}$ denotes the two-norm condition number for symmetric positive definite matrices, defined as the ratio of the largest to the smallest eigenvalue.\n\nExpress your final answer as a single real number. No rounding is required.", "solution": "Our goal is to compute the ratio of condition numbers for the Jacobi-preconditioned matrix $D^{-1}A$ and the original matrix $A$. This requires finding the eigenvalues of both matrices.\n\nFirst, we determine the eigenvalues of the matrix $A$. The matrix $A$ is defined by the finite difference approximation of the negative second derivative operator. For a grid function $v_j$ defined at the points $x_j$, the eigenvalue problem $A\\mathbf{v} = \\lambda \\mathbf{v}$ corresponds to the set of difference equations:\n$$\n\\frac{1}{h^2}(-v_{j-1} + 2v_j - v_{j+1}) = \\lambda v_j, \\quad \\text{for } j = 1, 2, \\dots, n.\n$$\nThe Dirichlet boundary conditions $u(0)=0$ and $u(1)=0$ imply that the components of the eigenvector $\\mathbf{v}$ satisfy $v_0 = 0$ and $v_{n+1}=0$. The difference equation can be rewritten as:\n$$\n2v_j - (v_{j-1} + v_{j+1}) = (\\lambda h^2) v_j.\n$$\nWe seek solutions of the form $v_j = \\sin(j\\theta)$ for some parameter $\\theta$. This form automatically satisfies the boundary condition $v_0=0$. Substituting this into the difference equation yields:\n$$\n2\\sin(j\\theta) - (\\sin((j-1)\\theta) + \\sin((j+1)\\theta)) = (\\lambda h^2) \\sin(j\\theta).\n$$\nUsing the trigonometric sum-to-product identity $\\sin(\\alpha - \\beta) + \\sin(\\alpha + \\beta) = 2\\sin(\\alpha)\\cos(\\beta)$, the expression simplifies to:\n$$\n2\\sin(j\\theta) - 2\\sin(j\\theta)\\cos(\\theta) = (\\lambda h^2) \\sin(j\\theta).\n$$\nFor a non-trivial eigenvector, $\\sin(j\\theta)$ is not identically zero, so we can divide by it to obtain a relation for the eigenvalue $\\lambda$:\n$$\n2(1 - \\cos(\\theta)) = \\lambda h^2.\n$$\nUsing the half-angle identity $1 - \\cos(\\theta) = 2\\sin^2(\\frac{\\theta}{2})$, this becomes:\n$$\n4\\sin^2\\left(\\frac{\\theta}{2}\\right) = \\lambda h^2.\n$$\nThe second boundary condition, $v_{n+1}=0$, requires $\\sin((n+1)\\theta) = 0$. This implies $(n+1)\\theta = k\\pi$ for an integer $k$. To obtain $n$ linearly independent eigenvectors, we take $k = 1, 2, \\dots, n$. This quantizes the parameter $\\theta$ to the values $\\theta_k = \\frac{k\\pi}{n+1}$.\nSubstituting these values of $\\theta_k$ provides the $n$ distinct eigenvalues of $A$:\n$$\n\\lambda_k(A) = \\frac{4}{h^2} \\sin^2\\left(\\frac{k\\pi}{2(n+1)}\\right), \\quad \\text{for } k=1, 2, \\dots, n.\n$$\nThe condition number $\\kappa_2(A)$ is the ratio of the largest to the smallest eigenvalue. The sine function is monotonically increasing on $[0, \\pi/2]$. The arguments $\\frac{k\\pi}{2(n+1)}$ for $k=1, \\dots, n$ are in this range.\nThe smallest eigenvalue, $\\lambda_{\\min}(A)$, corresponds to $k=1$:\n$$\n\\lambda_{\\min}(A) = \\lambda_1(A) = \\frac{4}{h^2} \\sin^2\\left(\\frac{\\pi}{2(n+1)}\\right).\n$$\nThe largest eigenvalue, $\\lambda_{\\max}(A)$, corresponds to $k=n$:\n$$\n\\lambda_{\\max}(A) = \\lambda_n(A) = \\frac{4}{h^2} \\sin^2\\left(\\frac{n\\pi}{2(n+1)}\\right).\n$$\nThe condition number of $A$ is therefore:\n$$\n\\kappa_2(A) = \\frac{\\lambda_{\\max}(A)}{\\lambda_{\\min}(A)} = \\frac{\\sin^2\\left(\\frac{n\\pi}{2(n+1)}\\right)}{\\sin^2\\left(\\frac{\\pi}{2(n+1)}\\right)}.\n$$\n\nNext, we analyze the preconditioned matrix $D^{-1}A$. The matrix $D$ is the diagonal of $A$. From the problem statement, all diagonal entries of $A$ are identical: $A_{ii} = \\frac{2}{h^2}$. Thus, $D$ is a scalar multiple of the identity matrix $I_n$:\n$$\nD = \\frac{2}{h^2} I_n.\n$$\nIts inverse is simply:\n$$\nD^{-1} = \\frac{h^2}{2} I_n.\n$$\nThe preconditioned matrix is therefore a scalar multiple of $A$:\n$$\nD^{-1}A = \\left(\\frac{h^2}{2} I_n\\right) A = \\frac{h^2}{2} A.\n$$\nSince $D^{-1}A$ is a scalar multiple of $A$, its eigenvectors are the same as the eigenvectors of $A$, and its eigenvalues $\\mu_k$ are simply scaled versions of the eigenvalues of $A$:\n$$\n\\mu_k = \\frac{h^2}{2}\\lambda_k(A).\n$$\nThe condition number of the preconditioned matrix $D^{-1}A$ is:\n$$\n\\kappa_2(D^{-1}A) = \\frac{\\lambda_{\\max}(D^{-1}A)}{\\lambda_{\\min}(D^{-1}A)} = \\frac{\\frac{h^2}{2}\\lambda_{\\max}(A)}{\\frac{h^2}{2}\\lambda_{\\min}(A)} = \\frac{\\lambda_{\\max}(A)}{\\lambda_{\\min}(A)} = \\kappa_2(A).\n$$\nThis demonstrates that for the specific matrix $A$ arising from the 1D Poisson problem with constant coefficients, the Jacobi preconditioner has no effect on the condition number.\n\nFinally, we compute the required limit:\n$$\n\\lim_{n \\to \\infty} \\frac{\\kappa_{2}(D^{-1} A)}{\\kappa_{2}(A)}.\n$$\nSince we have established that $\\kappa_2(D^{-1}A) = \\kappa_2(A)$ for any $n \\ge 1$, the ratio is identically equal to $1$. Therefore:\n$$\n\\lim_{n \\to \\infty} \\frac{\\kappa_{2}(D^{-1} A)}{\\kappa_{2}(A)} = \\lim_{n \\to \\infty} 1 = 1.\n$$", "answer": "$$\n\\boxed{1}\n$$", "id": "3412279"}, {"introduction": "Having seen that simple diagonal scaling is not always sufficient, we now turn to a more sophisticated method: the Symmetric Successive Over-Relaxation (SSOR) preconditioner. This practice introduces the relaxation parameter $\\omega$, a tunable component that can significantly alter performance. You will use local mode analysis—a powerful tool for analyzing translation-invariant operators—to determine how the eigenvalues of the preconditioned system depend on $\\omega$ and to find the optimal parameter value that minimizes the condition number bound.", "problem": "Consider the one-dimensional Poisson equation $-u''(x) = f(x)$ on the interval $(0,1)$ with homogeneous Dirichlet boundary conditions $u(0) = u(1) = 0$. Using the standard second-order centered finite difference method on a uniform grid with $n$ interior points and meshwidth $h = \\frac{1}{n+1}$, the resulting linear system is $A u = b$ with symmetric positive definite tridiagonal matrix $A \\in \\mathbb{R}^{n \\times n}$ given by $A_{ii} = \\frac{2}{h^2}$ and $A_{i,i\\pm 1} = -\\frac{1}{h^2}$. For preconditioning, use the Jacobi splitting $A = D - L - U$, where $D = \\frac{2}{h^2} I$, $L$ is the strictly lower triangular part with entries $L_{i,i-1} = \\frac{1}{h^2}$, and $U$ is the strictly upper triangular part with entries $U_{i,i+1} = \\frac{1}{h^2}$. Define the Symmetric Successive Overrelaxation (SSOR) preconditioner $M_{\\mathrm{SSOR}}(\\omega)$ for relaxation parameter $\\omega \\in (0,2)$ by\n$$\nM_{\\mathrm{SSOR}}(\\omega) = (D - \\omega L) D^{-1} (D - \\omega U).\n$$\nNote that a positive scalar multiple of $M_{\\mathrm{SSOR}}(\\omega)$ is often used in practice, but such scaling does not affect the spectral condition number of the preconditioned system. The spectral condition number of a symmetric positive definite matrix $B$ is defined by $\\kappa(B) = \\frac{\\lambda_{\\max}(B)}{\\lambda_{\\min}(B)}$, where $\\lambda_{\\max}(B)$ and $\\lambda_{\\min}(B)$ denote the largest and smallest eigenvalues of $B$, respectively.\n\nUsing the fact that $A$ is generated by a constant-coefficient nearest-neighbor stencil and the standard symbol-based local mode analysis for Toeplitz operators, derive a mode-dependent expression for the eigenvalues of the preconditioned operator $M_{\\mathrm{SSOR}}(\\omega)^{-1} A$ as a function of the mode angle $\\theta \\in (0,\\pi)$, and use this to construct an explicit upper bound on the condition number of $M_{\\mathrm{SSOR}}(\\omega)^{-1} A$ for the finite Dirichlet problem with $n$ interior points. Then, analyze how the relaxation parameter $\\omega$ influences this upper bound and determine the value of $\\omega \\in (0,2)$ that minimizes the bound in the limiting sense, together with the corresponding minimal bound itself. Express your final answer as a pair containing:\n- the limiting optimal relaxation parameter $\\omega$, and\n- the minimal attainable upper bound on $\\kappa(M_{\\mathrm{SSOR}}(\\omega)^{-1} A)$ in that limit.\nNo rounding is required, and you should provide exact values. Report your final pair as a single row matrix.", "solution": "We perform a symbol-based local mode analysis (also known as local Fourier analysis) to derive an expression for the eigenvalues of the preconditioned operator. This technique examines the action of the discrete operators on a Fourier mode $v_j = \\exp(i j \\theta)$, where $j$ is the grid index and $\\theta \\in (0, \\pi)$ is the mode angle. The symbol of an operator is the complex scalar that multiplies the mode after the operator is applied.\n\n1.  **Symbols of the Matrix Operators**:\n    Based on the problem definition, we derive the symbols for the operators $A$, $D$, $L$, and $U$.\n    -   **Operator A**: The stencil for $A$ is $\\frac{1}{h^2}(-1, 2, -1)$. Applying this to $v_j$:\n        $A v_j = \\frac{1}{h^2} (-v_{j-1} + 2v_j - v_{j+1}) = \\frac{1}{h^2} (-\\exp(-i\\theta) + 2 - \\exp(i\\theta)) v_j = \\frac{1}{h^2}(2 - 2\\cos\\theta) v_j$.\n        The symbol for $A$ is $\\hat{A}(\\theta) = \\frac{2}{h^2}(1 - \\cos\\theta) = \\frac{4}{h^2}\\sin^2(\\frac{\\theta}{2})$.\n    -   **Operator D**: $D = \\frac{2}{h^2}I$. The symbol is a constant: $\\hat{D}(\\theta) = \\frac{2}{h^2}$.\n    -   **Operator L**: The problem defines $L$ with stencil entry $L_{i,i-1}=\\frac{1}{h^2}$.\n        $L v_j = \\frac{1}{h^2} v_{j-1} = \\frac{1}{h^2} \\exp(-i\\theta) v_j$.\n        The symbol for $L$ is $\\hat{L}(\\theta) = \\frac{1}{h^2}\\exp(-i\\theta)$.\n    -   **Operator U**: The problem defines $U$ with stencil entry $U_{i,i+1}=\\frac{1}{h^2}$.\n        $U v_j = \\frac{1}{h^2} v_{j+1} = \\frac{1}{h^2} \\exp(i\\theta) v_j$.\n        The symbol for $U$ is $\\hat{U}(\\theta) = \\frac{1}{h^2}\\exp(i\\theta)$.\n\n2.  **Symbol of the SSOR Preconditioner**:\n    The symbol of a product of operators is the product of their symbols. Using the given formula for $M_{\\mathrm{SSOR}}(\\omega)$:\n    $\\hat{M}_{\\mathrm{SSOR}}(\\omega)(\\theta) = (\\hat{D}(\\theta) - \\omega \\hat{L}(\\theta)) (\\hat{D}(\\theta))^{-1} (\\hat{D}(\\theta) - \\omega \\hat{U}(\\theta))$\n    $= \\left(\\frac{2}{h^2} - \\frac{\\omega}{h^2}\\exp(-i\\theta)\\right) \\left(\\frac{h^2}{2}\\right) \\left(\\frac{2}{h^2} - \\frac{\\omega}{h^2}\\exp(i\\theta)\\right)$\n    $= \\frac{1}{2h^2} (2 - \\omega\\exp(-i\\theta))(2 - \\omega\\exp(i\\theta))$\n    $= \\frac{1}{2h^2} (4 - 2\\omega(\\exp(i\\theta) + \\exp(-i\\theta)) + \\omega^2)$\n    $= \\frac{1}{2h^2} (4 - 4\\omega\\cos\\theta + \\omega^2)$.\n\n3.  **Eigenvalues of the Preconditioned Operator**:\n    Within the local Fourier analysis framework, the eigenvalues of the preconditioned operator $M_{\\mathrm{SSOR}}(\\omega)^{-1} A$ are approximated by the ratio of the symbols, $\\lambda(\\theta) = \\hat{A}(\\theta) / \\hat{M}_{\\mathrm{SSOR}}(\\omega)(\\theta)$:\n    $$\n    \\lambda(\\theta) = \\frac{\\frac{2}{h^2}(1 - \\cos\\theta)}{\\frac{1}{2h^2}(4 - 4\\omega\\cos\\theta + \\omega^2)} = \\frac{4(1 - \\cos\\theta)}{4 - 4\\omega\\cos\\theta + \\omega^2}.\n    $$\n\n4.  **Condition Number Bound**:\n    The spectral condition number is $\\kappa = \\lambda_{\\max} / \\lambda_{\\min}$. We need to find the extrema of $\\lambda(\\theta)$ for $\\theta$ corresponding to the grid modes, which range from $\\theta_1 = \\frac{\\pi}{n+1}$ to $\\theta_n = \\pi - \\theta_1$. Let $c = \\cos\\theta$. For $\\theta \\in (0,\\pi)$, $c$ decreases from $1$ to $-1$. Let's analyze $\\lambda(\\theta)$ as a function of $c$:\n    $g(c) = \\frac{4(1-c)}{4-4\\omega c + \\omega^2}$. The derivative with respect to $c$ is:\n    $g'(c) = \\frac{-4(4-4\\omega c + \\omega^2) - 4(1-c)(-4\\omega)}{(4-4\\omega c + \\omega^2)^2} = \\frac{-4(4-4\\omega+\\omega^2)}{(4-4\\omega c + \\omega^2)^2} = \\frac{-4(2-\\omega)^2}{(4-4\\omega c + \\omega^2)^2}$.\n    For $\\omega \\in (0,2)$, the numerator is negative and the denominator is positive. Thus, $g'(c)  0$, which means $g(c)$ is a strictly decreasing function of $c$. Since $c=\\cos\\theta$ is a decreasing function of $\\theta$ on $(0, \\pi)$, $\\lambda(\\theta) = g(\\cos\\theta)$ is an increasing function of $\\theta$.\n    Therefore, the minimum eigenvalue corresponds to the smallest mode angle $\\theta_1$ and the maximum eigenvalue corresponds to the largest mode angle $\\theta_n = \\pi - \\theta_1$.\n    $\\lambda_{\\min} = \\lambda(\\theta_1) = \\frac{4(1-\\cos\\theta_1)}{4 - 4\\omega\\cos\\theta_1 + \\omega^2}$.\n    $\\lambda_{\\max} = \\lambda(\\theta_n) = \\frac{4(1-\\cos\\theta_n)}{4 - 4\\omega\\cos\\theta_n + \\omega^2} = \\frac{4(1+\\cos\\theta_1)}{4 + 4\\omega\\cos\\theta_1 + \\omega^2}$.\n    The condition number bound is $\\kappa(\\omega, n) = \\frac{\\lambda_{\\max}}{\\lambda_{\\min}}$:\n    $\\kappa(\\omega, n) = \\frac{1+\\cos\\theta_1}{1-\\cos\\theta_1} \\cdot \\frac{4 - 4\\omega\\cos\\theta_1 + \\omega^2}{4 + 4\\omega\\cos\\theta_1 + \\omega^2}$.\n\n5.  **Minimization of the Bound**:\n    We want to find $\\omega \\in (0,2)$ that minimizes this bound. Let $c_1 = \\cos\\theta_1 = \\cos(\\frac{\\pi}{n+1})$. The first term $\\frac{1+c_1}{1-c_1}$ is independent of $\\omega$. We need to minimize the second term:\n    $f(\\omega) = \\frac{\\omega^2 - 4c_1\\omega + 4}{\\omega^2 + 4c_1\\omega + 4}$.\n    Its derivative is $f'(\\omega) = \\frac{8c_1(\\omega^2-4)}{(\\omega^2 + 4c_1\\omega + 4)^2}$.\n    For $\\omega \\in (0,2)$, we have $\\omega^2-4  0$. Also, for $n \\ge 1$, $\\theta_1 \\in (0, \\pi/2)$, so $c_1 = \\cos\\theta_1 > 0$. Thus, $f'(\\omega)  0$.\n    The function $f(\\omega)$ is strictly decreasing for $\\omega \\in (0,2)$. The minimum is approached as $\\omega$ tends to the right endpoint of the interval, i.e., $\\omega \\to 2$.\n    The limiting optimal relaxation parameter is therefore $\\omega = 2$.\n    To find the minimal attainable bound, we evaluate $\\kappa(\\omega, n)$ in the limit as $\\omega \\to 2$:\n    $\\lim_{\\omega \\to 2} \\kappa(\\omega, n) = \\frac{1+\\cos\\theta_1}{1-\\cos\\theta_1} \\cdot \\lim_{\\omega \\to 2} \\frac{\\omega^2 - 4c_1\\omega + 4}{\\omega^2 + 4c_1\\omega + 4}$\n    $= \\frac{1+\\cos\\theta_1}{1-\\cos\\theta_1} \\cdot \\frac{4 - 8c_1 + 4}{4 + 8c_1 + 4} = \\frac{1+\\cos\\theta_1}{1-\\cos\\theta_1} \\cdot \\frac{8(1 - c_1)}{8(1 + c_1)} = 1$.\n    The minimal upper bound on the condition number, within this analysis framework, is 1, achieved in the limit as $\\omega \\to 2$.\n\nThe limiting optimal relaxation parameter is $\\omega=2$, and the corresponding minimal upper bound on the condition number is $1$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n2  1\n\\end{pmatrix}\n}\n$$", "id": "3412246"}, {"introduction": "Theoretical analysis provides invaluable intuition, but practical implementation reveals complexities that arise in higher dimensions. This hands-on coding exercise challenges you to apply the SSOR preconditioner to the two-dimensional Poisson problem, a cornerstone of computational science. You will investigate the profound effect of matrix ordering by comparing the standard lexicographic ordering with the more advanced red-black ordering, quantifying the dramatic improvement in the preconditioner's spectral properties that a change in data structure can achieve.", "problem": "Consider the two-dimensional Poisson equation $-\\Delta u = f$ on the unit square $(0,1)^2$ with homogeneous Dirichlet boundary conditions. Discretize the operator $-\\Delta$ using the standard centered finite difference scheme on a uniform grid of $n \\times n$ interior points, yielding a symmetric positive definite linear system $A_n u = b$, where $A_n \\in \\mathbb{R}^{N \\times N}$ with $N = n^2$. The matrix $A_n$ has the well-known five-point stencil structure and can be constructed from the one-dimensional tridiagonal matrix $T_n \\in \\mathbb{R}^{n \\times n}$ with entries $2/h^2$ on the diagonal and $-1/h^2$ on the first sub- and super-diagonals, where $h = 1/(n+1)$, via the Kronecker sum $A_n = I_n \\otimes T_n + T_n \\otimes I_n$, where $I_n$ is the $n \\times n$ identity and $\\otimes$ denotes the Kronecker product.\n\nDefine the natural lexicographic ordering by mapping grid points $(i,j)$ with $i,j \\in \\{0,1,\\dots,n-1\\}$ to the linear index $k = i n + j$. Define the red–black ordering by the permutation that lists all indices with $(i+j)$ even (red) first, followed by those with $(i+j)$ odd (black). For any ordering, define the splitting $A = D - L - U$, where $D$ is the diagonal of $A$ and $L$ and $U$ are the strictly lower and strictly upper triangular parts of $-A$, respectively, so that $L$ and $U$ have nonnegative entries. The Symmetric Successive Over-Relaxation (SSOR) preconditioner with relaxation parameter $\\omega \\in (0,2)$ is the symmetric positive definite matrix associated with performing a forward Successive Over-Relaxation (SOR) sweep followed by a backward SOR sweep. Let $M_{\\omega}$ denote this SSOR preconditioner.\n\nFor a given ordering and relaxation parameter $\\omega$, the preconditioned spectrum is the set of generalized eigenvalues $\\lambda$ satisfying $A x = \\lambda M_{\\omega} x$. This generalized spectrum coincides with the ordinary spectrum of the symmetrically preconditioned operator $M_{\\omega}^{-1/2} A M_{\\omega}^{-1/2}$ and with the spectrum of the left-preconditioned operator $M_{\\omega}^{-1} A$.\n\nTask:\n- Construct $A_n$ for each specified $n$.\n- For each $n$ and $\\omega$, construct $M_{\\omega}$ under the natural ordering and compute the smallest and largest generalized eigenvalues of $(A_n, M_{\\omega})$, and their ratio (the condition number of the generalized eigenvalue problem) defined as $\\kappa_{\\mathrm{nat}} = \\lambda_{\\max}/\\lambda_{\\min}$.\n- Construct the red–black permutation, form the permuted system $A_{n,\\mathrm{rb}}$ under red–black ordering, construct $M_{\\omega,\\mathrm{rb}}$ accordingly, and compute the smallest and largest generalized eigenvalues of $(A_{n,\\mathrm{rb}}, M_{\\omega,\\mathrm{rb}})$ and their ratio $\\kappa_{\\mathrm{rb}} = \\lambda_{\\max}/\\lambda_{\\min}$.\n- For each test case, aggregate the outputs into the triple $[\\kappa_{\\mathrm{nat}}, \\kappa_{\\mathrm{rb}}, \\kappa_{\\mathrm{nat}}/\\kappa_{\\mathrm{rb}}]$, where the third entry quantifies the improvement factor of red–black ordering relative to natural ordering.\n\nUse the following test suite of parameter values:\n- Case $1$: $n = 8$, $\\omega = 1.0$.\n- Case $2$: $n = 8$, $\\omega = 1.5$.\n- Case $3$: $n = 16$, $\\omega = 1.9$.\n- Case $4$: $n = 4$, $\\omega = 1.2$.\n\nYour program must:\n- Compute the three floats for each case.\n- Round each reported float to six decimal places.\n- Produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each case’s triple is itself a bracketed comma-separated list, in the same order as the test suite. For example, the output format must be of the form $[[a_1,b_1,c_1],[a_2,b_2,c_2],\\dots]$ with all numeric entries rounded to six decimal places.\n\nNo physical units or angle units are involved in the output. Percentages are not to be used; ratios must be reported as decimal numbers.", "solution": "We begin from the discrete model of the two-dimensional Poisson equation. The continuous operator $-\\Delta$ on $(0,1)^2$ under homogeneous Dirichlet boundary conditions yields a symmetric positive definite operator. Discretizing on a uniform grid of $n \\times n$ interior points with spacing $h = 1/(n+1)$ using centered finite differences produces the matrix $A_n \\in \\mathbb{R}^{N \\times N}$ with $N = n^2$ and standard five-point stencil. A stable and widely used construction is via the Kronecker sum\n$$\nA_n = I_n \\otimes T_n + T_n \\otimes I_n,\n$$\nwhere $I_n$ is the $n \\times n$ identity and $T_n \\in \\mathbb{R}^{n \\times n}$ is tridiagonal with entries $2/h^2$ on the diagonal and $-1/h^2$ on the immediate off-diagonals. This matrix $A_n$ is symmetric positive definite.\n\nTo discuss the Symmetric Successive Over-Relaxation (SSOR) preconditioner, recall the matrix splitting\n$$\nA = D - L - U,\n$$\nwhere $D$ is the diagonal of $A$ and $L$ and $U$ are the strictly lower and strictly upper triangular parts of $-A$, respectively, so that $L$ and $U$ have nonnegative entries. The Successive Over-Relaxation (SOR) iteration for solving $A x = b$ with relaxation parameter $\\omega \\in (0,2)$ consists of a forward triangular sweep using $(D - \\omega L)$ and a backward triangular sweep using $(D - \\omega U)$. The SSOR preconditioner $M_{\\omega}$ is the symmetric positive definite matrix formed by composing the forward and backward SOR steps with appropriate scaling. A standard derivation from fundamental definitions of SOR gives the SSOR preconditioner\n$$\nM_{\\omega} = \\frac{1}{\\omega (2 - \\omega)} (D - \\omega L) D^{-1} (D - \\omega U).\n$$\nThis follows by interpreting the forward SOR step as solving $(D - \\omega L) y = r$ and the backward SOR step as solving $(D - \\omega U) z = D y$, then combining and scaling to obtain a symmetric positive definite operator that approximates $A$ and preserves conjugate gradient compatibility. For $0  \\omega  2$, $M_{\\omega}$ is symmetric positive definite.\n\nWhen using a left preconditioner $M_{\\omega}$, one studies the generalized eigenvalue problem\n$$\nA x = \\lambda M_{\\omega} x,\n$$\nwhose eigenvalues $\\lambda$ equal those of the symmetrically preconditioned operator $M_{\\omega}^{-1/2} A M_{\\omega}^{-1/2}$ and also the spectrum of $M_{\\omega}^{-1} A$. Because $A$ is symmetric positive definite and $M_{\\omega}$ is symmetric positive definite, the generalized eigenvalues are real and positive, and the condition number of the generalized eigenvalue problem is\n$$\n\\kappa = \\frac{\\lambda_{\\max}}{\\lambda_{\\min}}.\n$$\nA smaller $\\kappa$ indicates a spectrum more tightly clustered, which is favorable for Krylov methods.\n\nOrdering affects the strictly triangular components $L$ and $U$, hence the SSOR preconditioner. In natural lexicographic ordering, indices increase along one direction and then the next; in red–black ordering, one colors the grid with two colors based on parity of $(i+j)$, placing all red nodes first, followed by all black nodes. Red–black ordering block-diagonalizes the adjacency pattern of the grid graph, which in turn alters the structure of $L$ and $U$ and typically improves the spectral properties of the SSOR preconditioner for elliptic problems.\n\nAlgorithmic steps:\n- Construct $A_n$ using the Kronecker sum $A_n = I_n \\otimes T_n + T_n \\otimes I_n$.\n- Under natural ordering, form $D = \\operatorname{diag}(A)$, $L = -\\operatorname{tril}(A,-1)$, $U = -\\operatorname{triu}(A,1)$, and compute\n$$\nM_{\\omega} = \\frac{1}{\\omega (2 - \\omega)} (D - \\omega L) D^{-1} (D - \\omega U).\n$$\n- Solve the generalized eigenvalue problem $A x = \\lambda M_{\\omega} x$ to obtain $\\lambda_{\\min}$ and $\\lambda_{\\max}$; compute $\\kappa_{\\mathrm{nat}} = \\lambda_{\\max}/\\lambda_{\\min}$.\n- Construct the red–black permutation by listing all indices $(i,j)$ with $(i+j)$ even, followed by those with $(i+j)$ odd. Apply this permutation to $A_n$ to obtain $A_{n,\\mathrm{rb}}$, and construct $M_{\\omega,\\mathrm{rb}}$ from the permuted $A_{n,\\mathrm{rb}}$ via the same SSOR formula.\n- Solve the generalized eigenvalue problem $A_{n,\\mathrm{rb}} x = \\lambda M_{\\omega,\\mathrm{rb}} x$ to obtain $\\lambda_{\\min}$ and $\\lambda_{\\max}$; compute $\\kappa_{\\mathrm{rb}} = \\lambda_{\\max}/\\lambda_{\\min}$.\n- Report, for each test case, the triple $[\\kappa_{\\mathrm{nat}}, \\kappa_{\\mathrm{rb}}, \\kappa_{\\mathrm{nat}}/\\kappa_{\\mathrm{rb}}]$, with each float rounded to six decimal places.\n\nNumerical considerations:\n- The matrices are sparse and symmetric positive definite; use sparse linear algebra for efficiency and numerical stability.\n- In practice, due to floating-point rounding, explicitly symmetrizing $M_{\\omega}$ via $(M_{\\omega} + M_{\\omega}^{\\top})/2$ can help ensure the symmetry required by generalized symmetric eigenvalue solvers.\n- The smallest and largest generalized eigenvalues can be computed using a symmetric eigensolver for the generalized problem $A x = \\lambda M x$ with $A$ symmetric and $M$ symmetric positive definite, returning extremal eigenvalues efficiently.\n\nThe implementation below constructs the matrices, performs the generalized eigenvalue computations, and prints the requested output for the specified test suite.", "answer": "```python\nimport numpy as np\nfrom scipy.sparse import diags, identity, kron, tril, triu, csr_matrix\nfrom scipy.sparse.linalg import eigsh\n\ndef build_2d_poisson(n: int) - csr_matrix:\n    \"\"\"\n    Build the 2D Poisson (negative Laplacian) matrix with Dirichlet boundary conditions\n    on an n x n grid of interior points using the 5-point stencil.\n    A = kron(I, T) + kron(T, I), where T is 1D tridiagonal with [ -1, 2, -1 ] scaled by 1/h^2.\n    \"\"\"\n    h = 1.0 / (n + 1)\n    main = (2.0 / (h * h)) * np.ones(n)\n    off = (-1.0 / (h * h)) * np.ones(n - 1)\n    T = diags([off, main, off], offsets=[-1, 0, 1], format='csr')\n    I = identity(n, format='csr')\n    A = kron(I, T) + kron(T, I)\n    return A.tocsr()\n\ndef ssor_preconditioner(A: csr_matrix, omega: float) - csr_matrix:\n    \"\"\"\n    Construct the SSOR preconditioner M_omega = 1/(omega(2-omega)) * (D - omega L) * D^{-1} * (D - omega U),\n    where A = D - L - U with L = -tril(A,-1), U = -triu(A,1).\n    \"\"\"\n    if not (0.0  omega  2.0):\n        raise ValueError(\"omega must be in (0,2) for SSOR preconditioner.\")\n    # Diagonal and strictly lower/upper parts\n    diagA = A.diagonal()\n    D = diags(diagA, format='csr')\n    L = -tril(A, k=-1).tocsr()\n    U = -triu(A, k=1).tocsr()\n    # Inverse of diagonal\n    D_inv = diags(1.0 / diagA, format='csr')\n    # Assemble SSOR\n    S1 = (D - omega * L).tocsr()\n    S2 = (D - omega * U).tocsr()\n    denom = omega * (2.0 - omega)\n    M = (S1 @ D_inv @ S2) * (1.0 / denom)\n    # Ensure symmetry numerically\n    M = ((M + M.T) * 0.5).tocsr()\n    return M\n\ndef red_black_permutation(n: int) - np.ndarray:\n    \"\"\"\n    Construct red-black ordering permutation indices for an n x n grid.\n    Red: (i+j) even; Black: (i+j) odd. Return permutation list mapping\n    old indices to new order [reds, blacks].\n    \"\"\"\n    red = []\n    black = []\n    for i in range(n):\n        for j in range(n):\n            idx = i * n + j\n            if ((i + j) % 2) == 0:\n                red.append(idx)\n            else:\n                black.append(idx)\n    return np.array(red + black, dtype=int)\n\ndef generalized_extremal_eigs(A: csr_matrix, M: csr_matrix) - tuple[float, float]:\n    \"\"\"\n    Compute smallest and largest generalized eigenvalues of A x = lambda M x.\n    A must be symmetric; M must be symmetric positive definite.\n    \"\"\"\n    # Smallest algebraic eigenvalue\n    lam_min = eigsh(A, k=1, M=M, which='SA', return_eigenvectors=False, tol=1e-10)[0]\n    # Largest algebraic eigenvalue\n    lam_max = eigsh(A, k=1, M=M, which='LA', return_eigenvectors=False, tol=1e-10)[0]\n    # Guard against any tiny negative due to roundoff\n    lam_min = float(lam_min)\n    lam_max = float(lam_max)\n    if lam_min = 0:\n        lam_min = max(lam_min, 1e-14)\n    return lam_min, lam_max\n\ndef compute_condition_numbers(n: int, omega: float) - tuple[float, float, float]:\n    \"\"\"\n    For given n and omega, compute condition numbers for natural ordering and red-black ordering SSOR\n    preconditioned generalized eigenvalue problems, and their improvement ratio.\n    Returns (kappa_nat, kappa_rb, improvement).\n    \"\"\"\n    # Build A for natural ordering\n    A_nat = build_2d_poisson(n)\n    # SSOR for natural ordering\n    M_nat = ssor_preconditioner(A_nat, omega)\n    lam_min_nat, lam_max_nat = generalized_extremal_eigs(A_nat, M_nat)\n    kappa_nat = lam_max_nat / lam_min_nat\n\n    # Red-black permuted system\n    perm = red_black_permutation(n)\n    # Apply permutation to A: A_rb = P^T A P, achieved by indexing rows and cols\n    A_rb = A_nat[perm, :][:, perm]\n    # SSOR for red-black ordered matrix\n    M_rb = ssor_preconditioner(A_rb, omega)\n    lam_min_rb, lam_max_rb = generalized_extremal_eigs(A_rb, M_rb)\n    kappa_rb = lam_max_rb / lam_min_rb\n\n    improvement = kappa_nat / kappa_rb\n    return kappa_nat, kappa_rb, improvement\n\ndef format_results(results: list[tuple[float, float, float]]) - str:\n    \"\"\"\n    Format the list of triples into the required single-line string with six decimal places\n    and no extra spaces: [[a,b,c],[...],...]\n    \"\"\"\n    parts = []\n    for triple in results:\n        a, b, c = triple\n        parts.append(f\"[{a:.6f},{b:.6f},{c:.6f}]\")\n    return f\"[{','.join(parts)}]\"\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case is a tuple (n, omega)\n    test_cases = [\n        (8, 1.0),\n        (8, 1.5),\n        (16, 1.9),\n        (4, 1.2),\n    ]\n\n    results = []\n    for n, omega in test_cases:\n        kappa_nat, kappa_rb, improvement = compute_condition_numbers(n, omega)\n        results.append((kappa_nat, kappa_rb, improvement))\n\n    # Final print statement in the exact required format.\n    print(format_results(results))\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3412295"}]}