{"hands_on_practices": [{"introduction": "Understanding the sensitivity of linear systems derived from Partial Differential Equations (PDEs) begins with analyzing the canonical example: the Poisson equation. This exercise guides you through a first-principles derivation of the condition number's asymptotic behavior for the 1D discrete Laplacian. Mastering this analytical result [@problem_id:3372809] provides the foundational insight into why finer meshes, represented by a smaller grid spacing $h$, inherently lead to more numerically challenging problems.", "problem": "Consider the one-dimensional Poisson Partial Differential Equation (PDE) $-u''(x)=f(x)$ on the interval $(0,1)$ with homogeneous Dirichlet boundary conditions $u(0)=u(1)=0$. Discretize the domain with $n$ interior grid points, uniform spacing $h=1/(n+1)$, and approximate $-u''(x)$ by the centered second difference. This yields the linear system $A\\mathbf{u}=\\mathbf{f}$ with the $n\\times n$ matrix $A=(1/h^{2})\\,\\mathrm{tridiag}(-1,2,-1)$.\n\nStarting only from the fundamental definitions of the Euclidean norm (2-norm), the 2-norm condition number of a matrix, and the discrete second-difference operator with homogeneous Dirichlet boundary conditions, derive the leading-order asymptotic behavior of the 2-norm condition number $\\kappa_{2}(A)$ as $h\\to 0$. Specifically, show that $\\kappa_{2}(A)\\sim C\\,h^{-2}$ and determine the exact constant $C$.\n\nYour final answer must be the exact closed-form value of $C$ (a real number expressed symbolically, not numerically approximated). Do not provide intermediate steps or any other quantities in the final answer.", "solution": "The matrix $A$ arises from the centered second-difference approximation of the negative second derivative $-u''(x)$ on a uniform grid with homogeneous Dirichlet boundary conditions. The matrix $A$ is real, symmetric, and strictly diagonally dominant with positive diagonal entries and negative off-diagonal entries, so it is symmetric positive definite (SPD).\n\nBy definition, the Euclidean norm (2-norm) condition number of $A$ is\n$$\n\\kappa_{2}(A)=\\|A\\|_{2}\\,\\|A^{-1}\\|_{2}.\n$$\nFor a symmetric positive definite matrix, the spectral norm $\\|A\\|_{2}$ equals the largest eigenvalue of $A$, and $\\|A^{-1}\\|_{2}$ equals the reciprocal of the smallest eigenvalue of $A$. Therefore,\n$$\n\\kappa_{2}(A)=\\frac{\\lambda_{\\max}(A)}{\\lambda_{\\min}(A)}.\n$$\n\nTo determine the eigenvalues of $A$, we use the discrete second-difference operator under homogeneous Dirichlet boundary conditions. Let the grid indices be $j=1,2,\\dots,n$, and define the vector components $v^{(k)}_{j}=\\sin\\!\\big(j\\theta_{k}\\big)$, where\n$$\n\\theta_{k}=\\frac{k\\pi}{n+1}, \\quad k=1,2,\\dots,n.\n$$\nThese $n$ vectors form an orthogonal basis consistent with the homogeneous Dirichlet boundary conditions $v^{(k)}_{0}=v^{(k)}_{n+1}=0$.\n\nApply $A$ to $v^{(k)}$:\n$$\n(Av^{(k)})_{j}=\\frac{1}{h^{2}}\\Big(-v^{(k)}_{j-1}+2v^{(k)}_{j}-v^{(k)}_{j+1}\\Big)=\\frac{1}{h^{2}}\\Big(-\\sin((j-1)\\theta_{k})+2\\sin(j\\theta_{k})-\\sin((j+1)\\theta_{k})\\Big).\n$$\nUsing the trigonometric identity\n$$\n-\\sin((j-1)\\theta)+2\\sin(j\\theta)-\\sin((j+1)\\theta)=2\\big(1-\\cos\\theta\\big)\\sin(j\\theta)=4\\sin^{2}\\!\\Big(\\frac{\\theta}{2}\\Big)\\sin(j\\theta),\n$$\nwe find\n$$\n(Av^{(k)})_{j}=\\frac{4}{h^{2}}\\sin^{2}\\!\\Big(\\frac{\\theta_{k}}{2}\\Big)\\sin(j\\theta_{k})=\\lambda_{k}\\,v^{(k)}_{j},\n$$\nwith eigenvalues\n$$\n\\lambda_{k}=\\frac{4}{h^{2}}\\sin^{2}\\!\\Big(\\frac{k\\pi}{2(n+1)}\\Big), \\quad k=1,2,\\dots,n.\n$$\n\nThus,\n$$\n\\lambda_{\\min}(A)=\\lambda_{1}=\\frac{4}{h^{2}}\\sin^{2}\\!\\Big(\\frac{\\pi}{2(n+1)}\\Big), \\qquad \\lambda_{\\max}(A)=\\lambda_{n}=\\frac{4}{h^{2}}\\sin^{2}\\!\\Big(\\frac{n\\pi}{2(n+1)}\\Big).\n$$\n\nWe now extract the leading-order asymptotics as $h\\to 0$. Since $h=1/(n+1)$, we have $n\\to\\infty$ as $h\\to 0$.\n\nFor the smallest eigenvalue,\n$$\n\\lambda_{\\min}(A)=\\frac{4}{h^{2}}\\sin^{2}\\!\\Big(\\frac{\\pi}{2(n+1)}\\Big)=\\frac{4}{h^{2}}\\sin^{2}\\!\\Big(\\frac{\\pi h}{2}\\Big).\n$$\nUsing the small-angle approximation $\\sin x \\sim x$ as $x\\to 0$, we obtain\n$$\n\\sin\\!\\Big(\\frac{\\pi h}{2}\\Big)\\sim \\frac{\\pi h}{2} \\quad \\text{as } h\\to 0,\n$$\nso\n$$\n\\lambda_{\\min}(A)\\sim \\frac{4}{h^{2}}\\Big(\\frac{\\pi h}{2}\\Big)^{2}=\\pi^{2}.\n$$\n\nFor the largest eigenvalue,\n$$\n\\lambda_{\\max}(A)=\\frac{4}{h^{2}}\\sin^{2}\\!\\Big(\\frac{n\\pi}{2(n+1)}\\Big)=\\frac{4}{h^{2}}\\sin^{2}\\!\\Big(\\frac{\\pi}{2}-\\frac{\\pi}{2(n+1)}\\Big).\n$$\nUsing $\\sin\\!\\big(\\frac{\\pi}{2}-\\varepsilon\\big)=\\cos(\\varepsilon)$ and $\\cos(\\varepsilon)\\to 1$ as $\\varepsilon\\to 0$, we get\n$$\n\\sin^{2}\\!\\Big(\\frac{\\pi}{2}-\\frac{\\pi}{2(n+1)}\\Big)\\to 1 \\quad \\text{as } n\\to\\infty,\n$$\nhence\n$$\n\\lambda_{\\max}(A)\\sim \\frac{4}{h^{2}}.\n$$\n\nTherefore, the 2-norm condition number satisfies\n$$\n\\kappa_{2}(A)=\\frac{\\lambda_{\\max}(A)}{\\lambda_{\\min}(A)}\\sim \\frac{(4/h^{2})}{\\pi^{2}}=\\Big(\\frac{4}{\\pi^{2}}\\Big)h^{-2} \\quad \\text{as } h\\to 0.\n$$\nThis identifies the exact constant $C$ in the asymptotic relation $\\kappa_{2}(A)\\sim C\\,h^{-2}$:\n$$\nC=\\frac{4}{\\pi^{2}}.\n$$", "answer": "$$\\boxed{\\frac{4}{\\pi^{2}}}$$", "id": "3372809"}, {"introduction": "While symmetric matrices have a tidy relationship between eigenvalues and conditioning, many physical systems, particularly those involving transport or advection, yield non-normal matrices where this intuition fails. This practice explores a canonical non-normal matrix where all eigenvalues are deceptively simple, yet the condition number is large. By analyzing this system [@problem_id:3372780], you will uncover the crucial difference between eigenvalues and singular values and understand its profound implications for the reliability of iterative solvers.", "problem": "Consider the linear system arising from a coarse two-cell upwind discretization of a steady one-dimensional linear advection equation, which can be modeled by the nonnormal matrix\n$$\nA(\\alpha) = \\begin{pmatrix} 1 & \\alpha \\\\ 0 & 1 \\end{pmatrix},\n$$\nwhere $\\alpha > 0$ represents the strength of the one-way coupling induced by strong advection relative to diffusion. This matrix has all eigenvalues equal to one for every $\\alpha$, yet it is highly nonnormal when $\\alpha$ is large. Starting from the definitions of singular values, the $2$-norm, and the $2$-norm condition number, derive an exact closed-form expression for $\\kappa_2\\!\\left(A(\\alpha)\\right)$ in terms of $\\alpha$. Then, interpret the implications of large $\\kappa_2\\!\\left(A(\\alpha)\\right)$ for residual-based stopping criteria in Krylov subspace iterative solvers such as the Generalized Minimal Residual (GMRES) method, beginning from the identity $e_k = A^{-1} r_k$ that relates the iterate error $e_k$ to the residual $r_k$.\n\nFinally, for the specific parameter choice $\\alpha = 50$ and a residual tolerance of $\\tau = 1 \\times 10^{-8}$ in the stopping criterion $\\|r_k\\|_2 / \\|b\\|_2 \\le \\tau$, compute the bound $\\kappa_2\\!\\left(A(50)\\right)\\,\\tau$ on the relative error $\\|e_k\\|_2/\\|x^\\star\\|_2$. Round your final numerical answer to four significant figures and express it as a decimal in scientific notation.", "solution": "The solution is divided into three parts: first, the derivation of the condition number $\\kappa_2(A(\\alpha))$; second, the interpretation of its implications for iterative solvers; and third, a specific numerical calculation.\n\n**Part 1: Derivation of the $2$-norm Condition Number**\n\nThe $2$-norm condition number of an invertible matrix $A$ is defined as $\\kappa_2(A) = \\|A\\|_2 \\|A^{-1}\\|_2$. The $2$-norm of a matrix $M$, denoted $\\|M\\|_2$, is its largest singular value, $\\sigma_{\\max}(M)$. The singular values of $M$ are the square roots of the eigenvalues of the symmetric positive semidefinite matrix $M^T M$.\n\nThe given matrix is\n$$\nA(\\alpha) = \\begin{pmatrix} 1 & \\alpha \\\\ 0 & 1 \\end{pmatrix}\n$$\nIts transpose is\n$$\nA(\\alpha)^T = \\begin{pmatrix} 1 & 0 \\\\ \\alpha & 1 \\end{pmatrix}\n$$\nWe first compute the product $A(\\alpha)^T A(\\alpha)$:\n$$\nA(\\alpha)^T A(\\alpha) = \\begin{pmatrix} 1 & 0 \\\\ \\alpha & 1 \\end{pmatrix} \\begin{pmatrix} 1 & \\alpha \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 1 & \\alpha \\\\ \\alpha & \\alpha^2 + 1 \\end{pmatrix}\n$$\nThe eigenvalues $\\lambda$ of $A(\\alpha)^T A(\\alpha)$ are found by solving the characteristic equation $\\det(A(\\alpha)^T A(\\alpha) - \\lambda I) = 0$:\n$$\n\\det \\begin{pmatrix} 1-\\lambda & \\alpha \\\\ \\alpha & \\alpha^2 + 1 - \\lambda \\end{pmatrix} = 0\n$$\n$$\n(1-\\lambda)(\\alpha^2 + 1 - \\lambda) - \\alpha^2 = 0\n$$\n$$\n\\lambda^2 - (\\alpha^2+2)\\lambda + (\\alpha^2+1) - \\alpha^2 = 0\n$$\n$$\n\\lambda^2 - (\\alpha^2+2)\\lambda + 1 = 0\n$$\nUsing the quadratic formula, the eigenvalues are:\n$$\n\\lambda = \\frac{(\\alpha^2+2) \\pm \\sqrt{(\\alpha^2+2)^2 - 4}}{2} = \\frac{\\alpha^2+2 \\pm \\sqrt{\\alpha^4 + 4\\alpha^2 + 4 - 4}}{2}\n$$\n$$\n\\lambda = \\frac{\\alpha^2+2 \\pm \\sqrt{\\alpha^4 + 4\\alpha^2}}{2} = \\frac{\\alpha^2+2 \\pm \\sqrt{\\alpha^2(\\alpha^2+4)}}{2}\n$$\nSince the problem states $\\alpha > 0$, we have $\\sqrt{\\alpha^2} = \\alpha$.\n$$\n\\lambda = \\frac{\\alpha^2+2 \\pm \\alpha\\sqrt{\\alpha^2+4}}{2}\n$$\nLet the eigenvalues be $\\lambda_{\\max}$ and $\\lambda_{\\min}$. The singular values of $A(\\alpha)$ are $\\sigma_{\\max} = \\sqrt{\\lambda_{\\max}}$ and $\\sigma_{\\min} = \\sqrt{\\lambda_{\\min}}$. Thus, $\\|A(\\alpha)\\|_2 = \\sigma_{\\max}$.\nThe singular values of the inverse matrix $A^{-1}$ are the reciprocals of the singular values of $A$. Therefore, the singular values of $A(\\alpha)^{-1}$ are $1/\\sigma_{\\max}$ and $1/\\sigma_{\\min}$. The largest singular value of $A(\\alpha)^{-1}$ is $1/\\sigma_{\\min}$.\nSo, $\\|A(\\alpha)^{-1}\\|_2 = 1/\\sigma_{\\min}$.\n\nThe condition number is then:\n$$\n\\kappa_2(A(\\alpha)) = \\|A(\\alpha)\\|_2 \\|A(\\alpha)^{-1}\\|_2 = \\frac{\\sigma_{\\max}}{\\sigma_{\\min}} = \\frac{\\sqrt{\\lambda_{\\max}}}{\\sqrt{\\lambda_{\\min}}} = \\sqrt{\\frac{\\lambda_{\\max}}{\\lambda_{\\min}}}\n$$\nFrom the characteristic equation $\\lambda^2 - (\\alpha^2+2)\\lambda + 1 = 0$, by Vieta's formulas, the product of the roots is $\\lambda_{\\max}\\lambda_{\\min} = 1$. This implies $\\lambda_{\\min} = 1/\\lambda_{\\max}$.\nSubstituting this into the expression for the condition number:\n$$\n\\kappa_2(A(\\alpha)) = \\sqrt{\\frac{\\lambda_{\\max}}{1/\\lambda_{\\max}}} = \\sqrt{\\lambda_{\\max}^2} = \\lambda_{\\max}\n$$\nTherefore, the condition number is exactly the largest eigenvalue of $A(\\alpha)^T A(\\alpha)$.\n$$\n\\kappa_2(A(\\alpha)) = \\frac{\\alpha^2 + 2 + \\alpha\\sqrt{\\alpha^2+4}}{2}\n$$\nFor large $\\alpha$, we can approximate this expression. As $\\alpha \\to \\infty$, $\\sqrt{\\alpha^2+4} \\approx \\alpha$. Thus, $\\kappa_2(A(\\alpha)) \\approx \\frac{\\alpha^2+2+\\alpha^2}{2} = \\frac{2\\alpha^2+2}{2} = \\alpha^2+1$. A more careful expansion gives $\\kappa_2(A(\\alpha)) \\approx \\alpha^2+2$. This shows the condition number grows quadratically with $\\alpha$.\n\n**Part 2: Implications for Iterative Solvers**\n\nWe are asked to start from the identity $e_k = A^{-1} r_k$, where $e_k = x^\\star - x_k$ is the error vector and $r_k = b - A x_k$ is the residual vector at iterate $k$ of an iterative method for solving $A x = b$.\nTaking the $2$-norm of this identity, we get $\\|e_k\\|_2 = \\|A^{-1} r_k\\|_2$.\nBy the definition of an induced matrix norm, we have the inequality:\n$$\n\\|e_k\\|_2 \\le \\|A^{-1}\\|_2 \\|r_k\\|_2\n$$\nTo obtain a bound on the relative error, $\\|e_k\\|_2/\\|x^\\star\\|_2$, we use the fact that $b = A x^\\star$. Taking norms, we get $\\|b\\|_2 = \\|A x^\\star\\|_2 \\le \\|A\\|_2 \\|x^\\star\\|_2$. This implies that $1/\\|x^\\star\\|_2 \\le \\|A\\|_2/\\|b\\|_2$, assuming $x^\\star \\neq 0$.\nCombining these inequalities, we have:\n$$\n\\frac{\\|e_k\\|_2}{\\|x^\\star\\|_2} \\le \\frac{\\|A^{-1}\\|_2 \\|r_k\\|_2}{\\|x^\\star\\|_2} \\le \\|A^{-1}\\|_2 \\|r_k\\|_2 \\frac{\\|A\\|_2}{\\|b\\|_2}\n$$\nRearranging the terms yields the standard bound:\n$$\n\\frac{\\|e_k\\|_2}{\\|x^\\star\\|_2} \\le \\|A\\|_2 \\|A^{-1}\\|_2 \\frac{\\|r_k\\|_2}{\\|b\\|_2} = \\kappa_2(A) \\frac{\\|r_k\\|_2}{\\|b\\|_2}\n$$\nKrylov subspace methods like GMRES are designed to minimize the norm of the residual, $\\|r_k\\|_2$. The stopping criterion is typically based on the relative residual, for instance, $\\|r_k\\|_2 / \\|b\\|_2 \\le \\tau$ for some small tolerance $\\tau$.\n\nThe derived inequality reveals a crucial aspect of solving linear systems with ill-conditioned matrices. If $\\kappa_2(A)$ is large, a small relative residual $\\|r_k\\|_2/\\|b\\|_2$ does not necessarily imply a small relative error $\\|e_k\\|_2/\\|x^\\star\\|_2$. The error can be larger than the residual by a factor of up to $\\kappa_2(A)$.\n\nFor the matrix $A(\\alpha)$, we found that $\\kappa_2(A(\\alpha))$ grows quadratically with $\\alpha$. This means for strong advection (large $\\alpha$), the matrix is extremely ill-conditioned despite its eigenvalues all being $1$. This high non-normality leads to a situation where an iterative solver like GMRES, by driving the relative residual down to a tolerance $\\tau$, may terminate with a solution whose relative error is bounded by $\\kappa_2(A(\\alpha))\\tau$, which can still be unacceptably large. This illustrates the potential unreliability of residual-based stopping criteria for highly nonnormal systems. The residual norm is not a faithful indicator of the error norm in such cases.\n\n**Part 3: Numerical Calculation**\n\nWe are given $\\alpha = 50$ and a residual tolerance $\\tau = 1 \\times 10^{-8}$. We need to compute the upper bound on the relative error, which is $\\kappa_2(A(50)) \\, \\tau$.\nFirst, we compute $\\kappa_2(A(50))$ using the derived formula:\n$$\n\\kappa_2(A(50)) = \\frac{50^2 + 2 + 50\\sqrt{50^2+4}}{2} = \\frac{2500 + 2 + 50\\sqrt{2500+4}}{2}\n$$\n$$\n\\kappa_2(A(50)) = \\frac{2502 + 50\\sqrt{2504}}{2} = 1251 + 25\\sqrt{2504}\n$$\nNow, we calculate the numerical value:\n$$\n\\sqrt{2504} \\approx 50.03998401\n$$\n$$\n\\kappa_2(A(50)) \\approx 1251 + 25 \\times 50.03998401 = 1251 + 1250.99960025 = 2501.99960025\n$$\nThe bound on the relative error is:\n$$\n\\kappa_2(A(50)) \\, \\tau \\approx 2501.99960025 \\times (1 \\times 10^{-8}) \\approx 2.50199960025 \\times 10^{-5}\n$$\nWe are asked to round this result to four significant figures. The first four significant figures are $2$, $5$, $0$, $1$. The fifth significant figure is $9$, which means we must round up the fourth digit.\nRounding $2.5019...$ to four significant figures gives $2.502$.\nThe final result, expressed in scientific notation, is $2.502 \\times 10^{-5}$.", "answer": "$$\\boxed{2.502 \\times 10^{-5}}$$", "id": "3372780"}, {"introduction": "Theoretical insights into ill-conditioning become stark realities when faced with the limitations of finite-precision arithmetic. This hands-on computational problem [@problem_id:3372810] asks you to quantify the precise point at which a numerical solution for the 2D Poisson equation breaks down due to roundoff error accumulation. Furthermore, you will explore preconditioning, the primary strategy for mitigating ill-conditioning, and numerically verify its effectiveness in restoring solution accuracy.", "problem": "Consider the model elliptic partial differential equation (PDE) Poisson problem $-\\Delta u = f$ on the unit square with homogeneous Dirichlet boundary conditions. Let $A_h$ denote the finite difference discretization of the negative Laplacian operator on an $n \\times n$ interior grid using the standard five-point stencil with grid spacing $h = 1/(n+1)$; the resulting linear system is $A_h x = b$. The matrix $A_h$ is symmetric positive definite. The sensitivity of solving $A_h x = b$ in finite precision arithmetic is governed by the condition number $\\kappa_2(A_h)$ in the spectral norm (Euclidean norm), defined by $\\kappa_2(A_h) = \\|A_h\\|_2 \\|A_h^{-1}\\|_2$, which for symmetric positive definite matrices equals the ratio of the largest to the smallest eigenvalue. Machine precision is characterized by the machine epsilon $\\epsilon_{\\mathrm{mach}}$, the smallest positive number such that $1 + \\epsilon_{\\mathrm{mach}} \\neq 1$ in floating-point arithmetic. In the Conjugate Gradient (CG) method, finite precision causes loss of orthogonality in search directions and an attainable accuracy limit that scales with $\\kappa_2(A_h)\\,\\epsilon_{\\mathrm{mach}}$. Work from the following fundamental bases: the discrete operator definition of $A_h$ via the five-point stencil and the definitions of the spectral condition number and machine epsilon. Derive the asymptotic dependence of $\\kappa_2(A_h)$ on $h$ for the two-dimensional Dirichlet problem, and use it to identify grid spacings $h$ at which $\\kappa_2(A_h) \\approx 1/\\epsilon_{\\mathrm{mach}}$, corresponding to the threshold where roundoff error triggers the loss of essentially all significant digits. Quantify these thresholds for IEEE 754 double precision and for extended precision represented by a long double type. In addition, analyze the impact of simple diagonal scaling (Jacobi equilibration) $\\tilde{A}_h = D^{-1/2} A_h D^{-1/2}$, where $D = \\mathrm{diag}(A_h)$, on the condition number for this operator, and evaluate a shifted preconditioning $M = A_h + \\alpha I$ as a right-preconditioner in the sense of analyzing the spectrum of $M^{-1} A_h$ to mitigate sensitivity.\n\nYour tasks are:\n\n1. Construct $A_h$ explicitly as a sparse matrix for a given $n$ using the five-point stencil and $h = 1/(n+1)$. Compute the smallest and largest eigenvalues numerically, and hence compute $\\kappa_2(A_h)$.\n\n2. Starting from the structure of the discrete operator and its spectrum, derive the asymptotic scaling $\\kappa_2(A_h) \\sim c\\,h^{-2}$ for some constant $c > 0$. Use this asymptotic to compute the threshold grid spacing $h^\\star(\\epsilon)$ defined by $\\kappa_2(A_{h^\\star}) = 1/\\epsilon$ for two machine precisions: IEEE 754 double precision and a long double precision in your environment. Express $h^\\star$ as floating-point numbers.\n\n3. For a representative grid with $n = 32$, compute numerically $\\kappa_2(A_h)$ and then estimate the number of trustworthy decimal digits in the CG solution attainable under perfect preconditioning-free CG, using the heuristic $d(\\epsilon) = \\max\\{0,-\\log_{10}(\\kappa_2(A_h)\\,\\epsilon)\\}$. Compute $d(\\epsilon)$ for double precision and for long double precision. Express $d(\\epsilon)$ as floating-point numbers.\n\n4. Analyze diagonal scaling with $D = \\mathrm{diag}(A_h)$ by forming $\\tilde{A}_h = D^{-1/2}A_hD^{-1/2}$. Determine whether this scaling changes $\\kappa_2$ for this operator, and report a boolean indicating change ($\\mathrm{True}$ if $\\kappa_2$ changes, $\\mathrm{False}$ otherwise).\n\n5. Consider the right-preconditioned operator $M^{-1}A_h$ with $M = A_h + \\alpha I$ for $\\alpha > 0$. Show that the eigenvalues of $M^{-1}A_h$ are $\\lambda_i/( \\lambda_i + \\alpha)$, where $\\lambda_i$ are the eigenvalues of $A_h$. Using the numerically computed extreme eigenvalues for $n=32$, evaluate the condition number of $M^{-1}A_h$ for the choice $\\alpha = \\lambda_{\\min}(A_h)$, and compute the corresponding $d(\\epsilon)$ in double precision. Express these as floating-point numbers.\n\nAngle units are not applicable, and there are no physical units in this problem.\n\nTest suite:\n- Case 1 (happy path): $n = 32$ for numerical eigenvalue and condition number computation.\n- Case 2 (boundary thresholds): compute $h^\\star$ for IEEE 754 double precision and for long double precision from the asymptotic analysis.\n- Case 3 (edge case mitigation): preconditioning with $\\alpha = \\lambda_{\\min}(A_h)$ at $n=32$ and its effect on the condition number and digits.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets in the following order:\n$[h^\\star_{\\text{double}}, h^\\star_{\\text{longdouble}}, \\kappa_2(A_h)\\ \\text{at}\\ n=32, d_{\\text{double}}\\ \\text{at}\\ n=32, d_{\\text{longdouble}}\\ \\text{at}\\ n=32, \\text{JacobiChanges}, \\kappa_2(M^{-1}A_h)\\ \\text{with}\\ \\alpha=\\lambda_{\\min}, d_{\\text{double,pre}}]$.\nEach entry must be a float except for $\\text{JacobiChanges}$, which must be a boolean.", "solution": "The problem investigates the sensitivity of the linear system $A_h x = b$ arising from the five-point finite difference discretization of the Poisson equation, $-\\Delta u = f$, on the unit square with homogeneous Dirichlet boundary conditions. The sensitivity is quantified by the spectral condition number $\\kappa_2(A_h)$.\n\n**1. Matrix Construction and Numerical Condition Number**\n\nThe discrete operator $A_h$ acts on a vector of unknown grid values, ordered lexicographically. For an $n \\times n$ grid of interior points, the grid spacing is $h = 1/(n+1)$, and the total number of unknowns is $N = n^2$. The five-point stencil for the negative Laplacian at a grid point $(i,j)$ is given by:\n$$\n(-\\Delta u)_{i,j} \\approx \\frac{1}{h^2} (4u_{i,j} - u_{i-1,j} - u_{i+1,j} - u_{i,j-1} - u_{i,j+1})\n$$\nThis structure leads to a block tridiagonal matrix $A_h$ of size $n^2 \\times n^2$. A more elegant construction utilizes the Kronecker product. Let $I_n$ be the $n \\times n$ identity matrix and $T_n$ be the $n \\times n$ tridiagonal matrix representing the 1D second derivative:\n$$\nT_n = \\begin{pmatrix}\n2 & -1 & & \\\\\n-1 & 2 & \\ddots & \\\\\n& \\ddots & \\ddots & -1 \\\\\n& & -1 & 2\n\\end{pmatrix}\n$$\nThe matrix $A_h$ is then given by:\n$$\nA_h = \\frac{1}{h^2} (I_n \\otimes T_n + T_n \\otimes I_n)\n$$\nFor a symmetric positive definite matrix like $A_h$, the spectral condition number is the ratio of its largest to its smallest eigenvalue, $\\kappa_2(A_h) = \\lambda_{\\max}(A_h) / \\lambda_{\\min}(A_h)$. For $n=32$, we construct this $1024 \\times 1024$ matrix and compute its extreme eigenvalues numerically.\n\n**2. Asymptotic Analysis and Threshold Grid Spacing $h^\\star$**\n\nThe eigenvalues of $T_n$ are $\\mu_k = 2 - 2\\cos(k\\pi/(n+1)) = 2 - 2\\cos(k\\pi h)$ for $k = 1, \\dots, n$. The eigenvalues of $A_h$ are therefore known analytically:\n$$\n\\lambda_{k,j} = \\frac{1}{h^2} (\\mu_k + \\mu_j) = \\frac{1}{h^2} (4 - 2\\cos(k\\pi h) - 2\\cos(j\\pi h)) \\quad \\text{for } k,j = 1, \\dots, n.\n$$\nThe smallest eigenvalue, $\\lambda_{\\min}$, corresponds to the lowest frequencies, i.e., $(k,j)=(1,1)$:\n$$\n\\lambda_{\\min} = \\lambda_{1,1} = \\frac{1}{h^2} (4 - 4\\cos(\\pi h))\n$$\nUsing the Taylor expansion $\\cos(x) \\approx 1 - x^2/2$ for small $x$ (i.e., small $h$), we get:\n$$\n\\lambda_{\\min} \\approx \\frac{1}{h^2} \\left(4 - 4\\left(1 - \\frac{(\\pi h)^2}{2}\\right)\\right) = \\frac{2\\pi^2 h^2}{h^2} = 2\\pi^2\n$$\nThe largest eigenvalue, $\\lambda_{\\max}$, corresponds to the highest frequencies, i.e., $(k,j)=(n,n)$:\n$$\n\\lambda_{\\max} = \\lambda_{n,n} = \\frac{1}{h^2} (4 - 2\\cos(n\\pi h) - 2\\cos(n\\pi h)) = \\frac{1}{h^2}(4 - 4\\cos(n\\pi h))\n$$\nSince $h=1/(n+1)$, we have $n\\pi h = n\\pi/(n+1) = \\pi - \\pi/(n+1) = \\pi - \\pi h$. Thus, $\\cos(n\\pi h) = \\cos(\\pi - \\pi h) = -\\cos(\\pi h)$.\n$$\n\\lambda_{\\max} = \\frac{1}{h^2} (4 + 4\\cos(\\pi h)) \\approx \\frac{1}{h^2} (4 + 4(1)) = \\frac{8}{h^2}\n$$\nThe asymptotic behavior of the condition number for $h \\to 0$ is:\n$$\n\\kappa_2(A_h) = \\frac{\\lambda_{\\max}}{\\lambda_{\\min}} \\approx \\frac{8/h^2}{2\\pi^2} = \\frac{4}{\\pi^2 h^2} = c h^{-2} \\quad \\text{with } c = \\frac{4}{\\pi^2}\n$$\nThe threshold grid spacing $h^\\star$ is defined by $\\kappa_2(A_{h^\\star}) = 1/\\epsilon_{\\mathrm{mach}}$. Using the asymptotic formula:\n$$\n\\frac{4}{\\pi^2 (h^\\star)^2} = \\frac{1}{\\epsilon_{\\mathrm{mach}}} \\implies (h^\\star)^2 = \\frac{4 \\epsilon_{\\mathrm{mach}}}{\\pi^2} \\implies h^\\star(\\epsilon_{\\mathrm{mach}}) = \\frac{2\\sqrt{\\epsilon_{\\mathrm{mach}}}}{\\pi}\n$$\nWe compute this for IEEE 754 double precision ($\\epsilon_{\\text{double}}$) and the available long double precision ($\\epsilon_{\\text{longdouble}}$), whose value depends on the execution environment.\n\n**3. Trustworthy Digits in CG**\n\nThe number of trustworthy decimal digits in a solution computed by an iterative method like Conjugate Gradients (CG) can be heuristically estimated. The relative error in the solution is often bounded by a term proportional to $\\kappa_2(A_h)\\epsilon_{\\mathrm{mach}}$. The number of lost digits is thus related to $\\log_{10}(\\kappa_2(A_h)\\epsilon_{\\mathrm{mach}})$. A common heuristic for the number of trustworthy digits, $d(\\epsilon)$, is:\n$$\nd(\\epsilon) = \\max\\{0, -\\log_{10}(\\kappa_2(A_h)\\epsilon)\\}\n$$\nWe apply this formula using the numerically computed value of $\\kappa_2(A_h)$ for $n=32$ and the machine epsilon values for double and long double precision.\n\n**4. Diagonal Scaling (Jacobi Equilibration)**\n\nDiagonal scaling, or Jacobi equilibration, is a preconditioning technique where the scaled matrix is $\\tilde{A}_h = D^{-1/2}A_hD^{-1/2}$, with $D = \\mathrm{diag}(A_h)$. For the standard five-point stencil on a uniform grid, every diagonal entry of $A_h$ corresponds to the coefficient of the central node $u_{i,j}$, which is uniformly $4/h^2$. Thus, $D$ is a scalar multiple of the identity matrix:\n$$\nD = \\frac{4}{h^2} I\n$$\nTherefore, $D^{-1/2} = (\\sqrt{h^2/4})I = (h/2)I$. The scaled matrix is:\n$$\n\\tilde{A}_h = \\left(\\frac{h}{2}I\\right) A_h \\left(\\frac{h}{2}I\\right) = \\frac{h^2}{4} A_h\n$$\nSince $\\tilde{A}_h$ is a scalar multiple of $A_h$, its eigenvalues are $\\tilde{\\lambda}_i = (h^2/4)\\lambda_i$. The condition number is invariant under scalar multiplication:\n$$\n\\kappa_2(\\tilde{A}_h) = \\frac{\\tilde{\\lambda}_{\\max}}{\\tilde{\\lambda}_{\\min}} = \\frac{(h^2/4)\\lambda_{\\max}}{(h^2/4)\\lambda_{\\min}} = \\frac{\\lambda_{\\max}}{\\lambda_{\\min}} = \\kappa_2(A_h)\n$$\nFor this specific operator, Jacobi scaling has no effect on the condition number. The requested boolean `JacobiChanges` is therefore `False`.\n\n**5. Shifted Preconditioner Analysis**\n\nWe analyze the right-preconditioned operator $P = M^{-1}A_h$, where the preconditioner is $M = A_h + \\alpha I$ for some $\\alpha > 0$. The eigenvalues of $A_h$ are $\\lambda_i$ and the corresponding eigenvectors are $v_i$. Since $A_h$ and $I$ commute, they share the same eigenvectors. Thus, $v_i$ is also an eigenvector of $M$:\n$$\nM v_i = (A_h + \\alpha I) v_i = A_h v_i + \\alpha v_i = (\\lambda_i + \\alpha) v_i\n$$\nThe action of $P$ on an eigenvector $v_i$ is:\n$$\nP v_i = M^{-1}A_h v_i = M^{-1}(\\lambda_i v_i) = \\lambda_i M^{-1}v_i = \\lambda_i \\frac{1}{\\lambda_i + \\alpha} v_i\n$$\nThe eigenvalues of the preconditioned matrix $P$ are $\\mu_i = \\frac{\\lambda_i}{\\lambda_i + \\alpha}$. The function $f(x) = x/(x+\\alpha)$ is monotonically increasing for $x > 0$ and $\\alpha>0$.\nWith the choice $\\alpha = \\lambda_{\\min}(A_h)$, the new eigenvalues are:\n$$\n\\mu_{\\min} = \\frac{\\lambda_{\\min}}{\\lambda_{\\min} + \\alpha} = \\frac{\\lambda_{\\min}}{\\lambda_{\\min} + \\lambda_{\\min}} = \\frac{1}{2}\n$$\n$$\n\\mu_{\\max} = \\frac{\\lambda_{\\max}}{\\lambda_{\\max} + \\alpha} = \\frac{\\lambda_{\\max}}{\\lambda_{\\max} + \\lambda_{\\min}}\n$$\nThe condition number of the preconditioned system is:\n$$\n\\kappa_2(P) = \\frac{\\mu_{\\max}}{\\mu_{\\min}} = \\frac{\\lambda_{\\max}/(\\lambda_{\\max} + \\lambda_{\\min})}{1/2} = \\frac{2\\lambda_{\\max}}{\\lambda_{\\max} + \\lambda_{\\min}} = \\frac{2(\\lambda_{\\max}/\\lambda_{\\min})}{(\\lambda_{\\max}/\\lambda_{\\min}) + 1} = \\frac{2\\kappa_2(A_h)}{\\kappa_2(A_h)+1}\n$$\nThis transformation maps a large condition number $\\kappa_2(A_h) \\gg 1$ to a value close to $2$, which is a dramatic improvement. We compute this new condition number for $n=32$ using the numerically found extreme eigenvalues and calculate the corresponding trustworthy digits in double precision.", "answer": "```python\nimport numpy as np\nfrom scipy.sparse import diags, kron, identity\nfrom scipy.sparse.linalg import eigsh\n\ndef solve():\n    \"\"\"\n    Computes various metrics related to the condition number of the finite difference\n    discretization of the 2D Poisson equation.\n    \"\"\"\n    # Define parameters for the main numerical case\n    n = 32\n\n    # Get machine precisions for double and long double floating point types.\n    # Note: The precision of np.longdouble is platform-dependent and may be\n    # equivalent to np.float64 on some systems.\n    eps_double = np.finfo(np.float64).eps\n    eps_longdouble = np.finfo(np.longdouble).eps\n\n    # ---- Task 2: Asymptotic threshold h_star ----\n    # Derive h_star from kappa_2(A_h) = 4 / (pi^2 * h^2) = 1/epsilon\n    # h_star = 2 * sqrt(epsilon) / pi\n    h_star_double = (2.0 * np.sqrt(eps_double)) / np.pi\n    h_star_longdouble = (2.0 * np.sqrt(eps_longdouble)) / np.pi\n\n    # ---- Numerical computations for n=32 ----\n    h = 1.0 / (n + 1)\n    h_sq = h**2\n\n    # ---- Task 1: Construct A_h and compute its condition number ----\n    # Create the 1D discrete Laplacian matrix T_n\n    diagonals = [-1.0 * np.ones(n - 1), 2.0 * np.ones(n), -1.0 * np.ones(n - 1)]\n    offsets = [-1, 0, 1]\n    T_n = diags(diagonals, offsets, shape=(n, n), format='csr')\n    \n    # Create the 2D discrete Laplacian A_h using Kronecker products\n    I_n = identity(n, format='csr')\n    A_h_unscaled = kron(I_n, T_n) + kron(T_n, I_n)\n    A_h = (1.0 / h_sq) * A_h_unscaled\n\n    # Compute extreme eigenvalues (smallest and largest) of A_h.\n    # For a sparse SPD matrix, eigsh with which='BE' (Both Ends) is efficient.\n    # It returns k/2 smallest and k/2 largest eigenvalues. For k=2, this is\n    # the smallest and the largest.\n    try:\n        # eigsh returns eigenvalues in ascending order.\n        eigenvalues = eigsh(A_h, k=2, which='BE', return_eigenvectors=False)\n        lambda_min_A = eigenvalues[0]\n        lambda_max_A = eigenvalues[1]\n    except Exception as e:\n        # Fallback in case 'BE' is not supported or fails, though it should not.\n        # This is more robust.\n        lambda_min_val = eigsh(A_h, k=1, which='SA', return_eigenvectors=False)\n        lambda_max_val = eigsh(A_h, k=1, which='LA', return_eigenvectors=False)\n        lambda_min_A = lambda_min_val[0]\n        lambda_max_A = lambda_max_val[0]\n\n    # Compute the condition number of A_h\n    kappa_A_h = lambda_max_A / lambda_min_A\n\n    # ---- Task 3: Compute trustworthy digits for the original system ----\n    # d(eps) = max(0, -log10(kappa * eps))\n    arg_double = kappa_A_h * eps_double\n    arg_longdouble = kappa_A_h * eps_longdouble\n\n    d_double = max(0.0, -np.log10(arg_double))\n    d_longdouble = max(0.0, -np.log10(arg_longdouble))\n\n    # ---- Task 4: Analyze Jacobi (diagonal) scaling ----\n    # As derived in the solution, for this specific constant-diagonal operator,\n    # Jacobi scaling is equivalent to scalar multiplication and does not change\n    # the condition number.\n    JacobiChanges = False\n\n    # ---- Task 5: Analyze shifted preconditioning ----\n    # Preconditioner M = A_h + alpha*I, with alpha = lambda_min(A_h)\n    alpha = lambda_min_A\n    \n    # The condition number of M_inv * A_h is kappa_new = (2*kappa) / (kappa+1)\n    kappa_M_inv_A = (2.0 * kappa_A_h) / (kappa_A_h + 1.0)\n    # Direct computation from eigenvalues:\n    # kappa_M_inv_A = (2 * lambda_max_A) / (lambda_max_A + lambda_min_A)\n\n    # Compute trustworthy digits for the preconditioned system\n    arg_pre_double = kappa_M_inv_A * eps_double\n    d_double_pre = max(0.0, -np.log10(arg_pre_double))\n    \n    # ---- Final Output Assembly ----\n    # Assemble results in the specified order.\n    results = [\n        h_star_double,\n        h_star_longdouble,\n        kappa_A_h,\n        d_double,\n        d_longdouble,\n        JacobiChanges,\n        kappa_M_inv_A,\n        d_double_pre\n    ]\n\n    # Format the final output string.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3372810"}]}