{"hands_on_practices": [{"introduction": "We begin with a foundational skill: deriving the finite difference matrix from first principles. While uniform grids are common in textbooks, real-world problems often demand non-uniform grids to resolve features efficiently. This practice guides you through deriving the second-derivative matrix on such a grid using a conservative finite-volume approach, revealing how local grid spacing directly impacts the matrix entries and the scheme's accuracy [@problem_id:3418706].", "problem": "Consider the linear second-derivative operator $L u = \\frac{d^{2}u}{dx^{2}}$ acting on a sufficiently smooth scalar field $u(x)$ defined over a bounded interval $[a,b]$ with Dirichlet boundary conditions. Let $\\{x_{i}\\}_{i=0}^{n}$ be a strictly monotone nonuniform grid with $x_{0} = a$ and $x_{n} = b$, satisfying $x_{i-1} < x_{i} < x_{i+1}$ for all interior indices $i \\in \\{1,\\dots,n-1\\}$. Define the local spacings by $h_{i+\\frac{1}{2}} = x_{i+1} - x_{i} > 0$ and $h_{i-\\frac{1}{2}} = x_{i} - x_{i-1} > 0$. Using only fundamental definitions (limits and Taylor expansions) and the conservative interpretation of $\\frac{d^{2}u}{dx^{2}}$ as the divergence of the gradient, derive a $3$-point finite difference approximation of $L u$ at an interior node $x_{i}$ that depends solely on the nodal values $u_{i-1}$, $u_{i}$, and $u_{i+1}$, and is consistent to second order under smoothness assumptions on $u$. Then, write the corresponding tridiagonal matrix representation $A$ of the discrete operator mapping the vector of nodal values $(u_{0},u_{1},\\dots,u_{n})^{\\top}$ to the vector of discrete second derivatives $(L_{h}u)_{i}$ at interior nodes $\\{1,\\dots,n-1\\}$, and give explicit formulas for the diagonal entry $A_{i,i}$ and off-diagonal entries $A_{i,i-1}$ and $A_{i,i+1}$ in terms of $h_{i-\\frac{1}{2}}$ and $h_{i+\\frac{1}{2}}$. Your final answer must be the single row of these three entries ordered as $(A_{i,i-1}, A_{i,i}, A_{i,i+1})$, expressed as a closed-form analytic expression. No rounding is required.", "solution": "The problem requires the derivation of a $3$-point finite difference approximation for the second-derivative operator $L u = \\frac{d^{2}u}{dx^{2}}$ on a nonuniform grid. The derivation is to be based on the conservative interpretation of the operator, which is fundamental to the finite volume method and ensures conservation properties in numerical solutions to many physical laws expressed as Partial Differential Equations (PDEs).\n\nThe conservative form of the operator is $L u = \\frac{d}{dx} \\left( \\frac{du}{dx} \\right)$, which expresses the second derivative as the divergence of a flux, where the flux is the gradient $F(x) = \\frac{du}{dx}$.\n\nWe construct a control volume (or cell) $C_i$ around each interior node $x_i$. A natural choice for this control volume is the interval bounded by the midpoints between the nodes: $C_i = [x_{i-\\frac{1}{2}}, x_{i+\\frac{1}{2}}]$, where $x_{i-\\frac{1}{2}} = \\frac{x_{i-1} + x_i}{2}$ and $x_{i+\\frac{1}{2}} = \\frac{x_i + x_{i+1}}{2}$.\n\nThe length of this control volume is given by $\\Delta x_i = x_{i+\\frac{1}{2}} - x_{i-\\frac{1}{2}}$. Using the definitions of the local spacings, $h_{i+\\frac{1}{2}} = x_{i+1} - x_i$ and $h_{i-\\frac{1}{2}} = x_i - x_{i-1}$, we can express this length as:\n$$ \\Delta x_i = \\left(\\frac{x_i + x_{i+1}}{2}\\right) - \\left(\\frac{x_{i-1} + x_i}{2}\\right) = \\frac{x_{i+1} - x_{i-1}}{2} = \\frac{(x_{i+1} - x_i) + (x_i - x_{i-1})}{2} = \\frac{h_{i+\\frac{1}{2}} + h_{i-\\frac{1}{2}}}{2} $$\nThe finite difference approximation $(L_h u)_i$ at node $x_i$ is constructed by averaging the operator $L u$ over the control volume $C_i$ and then discretizing the resulting expression.\n$$ (L_h u)_i \\approx \\frac{1}{\\Delta x_i} \\int_{x_{i-\\frac{1}{2}}}^{x_{i+\\frac{1}{2}}} \\frac{d}{dx} \\left( \\frac{du}{dx} \\right) dx $$\nBy the Fundamental Theorem of Calculus, the integral simplifies to the difference of the flux at the boundaries of the control volume:\n$$ (L_h u)_i \\approx \\frac{1}{\\Delta x_i} \\left[ \\left( \\frac{du}{dx} \\right)_{x_{i+\\frac{1}{2}}} - \\left( \\frac{du}{dx} \\right)_{x_{i-\\frac{1}{2}}} \\right] $$\nNext, we approximate the flux (the first derivative) at the cell faces $x_{i+\\frac{1}{2}}$ and $x_{i-\\frac{1}{2}}$ using second-order accurate central difference approximations based on the adjacent nodal values:\n$$ \\left( \\frac{du}{dx} \\right)_{x_{i+\\frac{1}{2}}} \\approx \\frac{u(x_{i+1}) - u(x_i)}{x_{i+1} - x_i} = \\frac{u_{i+1} - u_i}{h_{i+\\frac{1}{2}}} $$\n$$ \\left( \\frac{du}{dx} \\right)_{x_{i-\\frac{1}{2}}} \\approx \\frac{u(x_i) - u(x_{i-1})}{x_i - x_{i-1}} = \\frac{u_i - u_{i-1}}{h_{i-\\frac{1}{2}}} $$\nSubstituting these flux approximations into the expression for $(L_h u)_i$ yields the discrete operator:\n$$ (L_h u)_i = \\frac{1}{\\frac{h_{i+\\frac{1}{2}} + h_{i-\\frac{1}{2}}}{2}} \\left( \\frac{u_{i+1} - u_i}{h_{i+\\frac{1}{2}}} - \\frac{u_i - u_{i-1}}{h_{i-\\frac{1}{2}}} \\right) $$\nTo determine the entries of the corresponding matrix $A$, we rearrange this expression to identify the coefficients of $u_{i-1}$, $u_i$, and $u_{i+1}$:\n$$ (L_h u)_i = \\frac{2}{h_{i+\\frac{1}{2}} + h_{i-\\frac{1}{2}}} \\left[ \\left(\\frac{1}{h_{i-\\frac{1}{2}}}\\right)u_{i-1} - \\left(\\frac{1}{h_{i-\\frac{1}{2}}} + \\frac{1}{h_{i+\\frac{1}{2}}}\\right)u_i + \\left(\\frac{1}{h_{i+\\frac{1}{2}}}\\right)u_{i+1} \\right] $$\nWe can simplify the coefficient of $u_i$:\n$$ -\\left(\\frac{1}{h_{i-\\frac{1}{2}}} + \\frac{1}{h_{i+\\frac{1}{2}}}\\right) = -\\frac{h_{i+\\frac{1}{2}} + h_{i-\\frac{1}{2}}}{h_{i-\\frac{1}{2}}h_{i+\\frac{1}{2}}} $$\nThe discrete operator is $(L_h u)_i = A_{i,i-1}u_{i-1} + A_{i,i}u_i + A_{i,i+1}u_{i+1}$. The coefficients, which form the $i$-th row of the matrix representation $A$, are:\n\nFor the sub-diagonal entry:\n$$ A_{i,i-1} = \\frac{2}{h_{i+\\frac{1}{2}} + h_{i-\\frac{1}{2}}} \\cdot \\frac{1}{h_{i-\\frac{1}{2}}} = \\frac{2}{h_{i-\\frac{1}{2}}(h_{i-\\frac{1}{2}} + h_{i+\\frac{1}{2}})} $$\nFor the super-diagonal entry:\n$$ A_{i,i+1} = \\frac{2}{h_{i+\\frac{1}{2}} + h_{i-\\frac{1}{2}}} \\cdot \\frac{1}{h_{i+\\frac{1}{2}}} = \\frac{2}{h_{i+\\frac{1}{2}}(h_{i-\\frac{1}{2}} + h_{i+\\frac{1}{2}})} $$\nFor the diagonal entry:\n$$ A_{i,i} = \\frac{2}{h_{i+\\frac{1}{2}} + h_{i-\\frac{1}{2}}} \\cdot \\left(-\\frac{h_{i+\\frac{1}{2}} + h_{i-\\frac{1}{2}}}{h_{i-\\frac{1}{2}}h_{i+\\frac{1}{2}}}\\right) = -\\frac{2}{h_{i-\\frac{1}{2}}h_{i+\\frac{1}{2}}} $$\nA Taylor series analysis shows that the local truncation error of this scheme is $T_i = (L_h u)_i - u''_i = \\frac{h_{i+\\frac{1}{2}} - h_{i-\\frac{1}{2}}}{3}u'''_i + O(h^2_{max})$. The scheme is first-order accurate on a general nonuniform grid. It achieves second-order consistency, as requested, under the condition that the grid spacing changes smoothly, i.e., $|h_{i+\\frac{1}{2}} - h_{i-\\frac{1}{2}}| = O(h^2_{max})$, or on a uniform grid where $h_{i+\\frac{1}{2}} = h_{i-\\frac{1}{2}}$. The derivation method itself is robust and leads to a unique $3$-point stencil.\n\nThe requested entries for the $i$-th row of the matrix $A$, ordered as $(A_{i,i-1}, A_{i,i}, A_{i,i+1})$, are therefore given by the expressions derived above.", "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{2}{h_{i-\\frac{1}{2}}(h_{i-\\frac{1}{2}} + h_{i+\\frac{1}{2}})} & -\\frac{2}{h_{i-\\frac{1}{2}}h_{i+\\frac{1}{2}}} & \\frac{2}{h_{i+\\frac{1}{2}}(h_{i-\\frac{1}{2}} + h_{i+\\frac{1}{2}})} \\end{pmatrix}}\n$$", "id": "3418706"}, {"introduction": "How do the properties of a 1D discretization extend to multiple dimensions? This exercise explores the elegant structure of the 2D Poisson matrix on a rectangular grid. By representing the discrete operator as a Kronecker sum of 1D operators, you will discover how to construct its eigenvectors and prove that its eigenvalues are simply sums of the 1D eigenvalues—a profound insight that underpins many fast solvers [@problem_id:3418648].", "problem": "Consider the two-dimensional Poisson equation with homogeneous Dirichlet boundary conditions on a rectangle,\n$$\n-\\frac{\\partial^{2} u}{\\partial x^{2}}(x,y) - \\frac{\\partial^{2} u}{\\partial y^{2}}(x,y) = f(x,y), \\quad (x,y) \\in (0,L_{x}) \\times (0,L_{y}), \\quad u(x,y) = 0 \\text{ on } \\partial\\big((0,L_{x}) \\times (0,L_{y})\\big).\n$$\nDiscretize the interior of the domain using a uniform grid with $n_{x}$ interior points in the $x$-direction and $n_{y}$ interior points in the $y$-direction, with spacings $h_{x} = \\frac{L_{x}}{n_{x}+1}$ and $h_{y} = \\frac{L_{y}}{n_{y}+1}$. Assemble the standard second-order centered finite difference (finite difference is defined here as a method that replaces derivatives by difference quotients on a grid) matrix $A \\in \\mathbb{R}^{n_{x} n_{y} \\times n_{x} n_{y}}$ approximating the negative Laplacian $-\\Delta$, by lexicographic ordering of interior grid points. Let $T_{n}$ denote the $n \\times n$ tridiagonal matrix with $2$ on the diagonal and $-1$ on the sub- and super-diagonals, which arises from the one-dimensional second-order centered finite difference discretization of the negative second derivative with homogeneous Dirichlet conditions. Then $A$ has the Kronecker sum structure\n$$\nA = \\frac{1}{h_{x}^{2}}\\, T_{n_{x}} \\otimes I_{n_{y}} \\;+\\; \\frac{1}{h_{y}^{2}}\\, I_{n_{x}} \\otimes T_{n_{y}},\n$$\nwhere $I_{m}$ denotes the $m \\times m$ identity matrix, and $\\otimes$ denotes the Kronecker product (the Kronecker product of matrices $X \\in \\mathbb{R}^{a \\times b}$ and $Y \\in \\mathbb{R}^{c \\times d}$ is the block matrix in $\\mathbb{R}^{ac \\times bd}$ defined by $X \\otimes Y = [x_{ij} Y]_{i,j}$).\n\nStarting from the definitions of the one-dimensional second-order centered finite difference operator with homogeneous Dirichlet boundary conditions, and standard properties of the Kronecker product—specifically $(X \\otimes I)(v \\otimes w) = (Xv) \\otimes w$ and $(I \\otimes Y)(v \\otimes w) = v \\otimes (Yw)$—derive the eigenpairs of the one-dimensional matrices $T_{n_{x}}$ and $T_{n_{y}}$, and then construct tensor-product eigenvectors for $A$ to rigorously show that the eigenvalues of $A$ are sums of one-dimensional eigenvalues scaled by $h_{x}$ and $h_{y}$.\n\nYour final task is to provide a single closed-form analytic expression for a generic eigenvalue $\\lambda_{p,q}$ of $A$ corresponding to mode indices $p \\in \\{1,\\dots,n_{x}\\}$ and $q \\in \\{1,\\dots,n_{y}\\}$, expressed only in terms of $p$, $q$, $n_{x}$, $n_{y}$, $h_{x}$, and $h_{y}$. No numerical approximation is required, and no units are to be used in the final expression.", "solution": "The derivation proceeds in two main stages as requested: first, determining the eigenpairs of the one-dimensional finite difference matrix $T_n$, and second, using these to construct the eigenpairs of the two-dimensional matrix $A$.\n\n**Part 1: Eigenpairs of the one-dimensional matrix $T_n$**\n\nThe matrix $T_n$ is an $n \\times n$ tridiagonal matrix with $2$ on the main diagonal and $-1$ on the first sub- and super-diagonals. An eigenvalue equation for $T_n$ is given by $T_n v = \\mu v$, where $\\mu$ is an eigenvalue and $v \\in \\mathbb{R}^n$ is the corresponding eigenvector with components $v_j$. The $j$-th row of this matrix-vector equation, for $j \\in \\{1, \\dots, n\\}$, is:\n$$\n-v_{j-1} + 2v_j - v_{j+1} = \\mu v_j.\n$$\nThese equations are complemented by boundary conditions derived from the homogeneous Dirichlet conditions on the continuous problem, which imply $v_0 = 0$ and $v_{n+1} = 0$. The equation can be rewritten as a homogeneous second-order linear difference equation:\n$$\nv_{j+1} + (\\mu - 2)v_j + v_{j-1} = 0.\n$$\nWe seek a solution of the form $v_j = \\sin(j\\theta)$ for some angle $\\theta$. This choice is motivated by the eigenfunctions of the continuous counterpart, the second derivative operator. The boundary condition $v_0 = \\sin(0 \\cdot \\theta) = 0$ is immediately satisfied. The condition at the other boundary, $v_{n+1}=0$, requires:\n$$\n\\sin((n+1)\\theta) = 0.\n$$\nThis implies that $(n+1)\\theta$ must be an integer multiple of $\\pi$. We write $(n+1)\\theta_p = p\\pi$ for an integer index $p$. To obtain $n$ linearly independent eigenvectors, we can choose $p \\in \\{1, 2, \\dots, n\\}$. This gives the angles:\n$$\n\\theta_p = \\frac{p\\pi}{n+1}, \\quad p \\in \\{1, 2, \\dots, n\\}.\n$$\nThus, the components of the $p$-th eigenvector, denoted $v_p^{(n)}$, are given by:\n$$\n(v_p^{(n)})_j = \\sin\\left(\\frac{jp\\pi}{n+1}\\right), \\quad j \\in \\{1, 2, \\dots, n\\}.\n$$\nTo find the corresponding eigenvalue $\\mu_p$, we substitute this solution back into the difference equation:\n$$\n-\\sin\\left(\\frac{(j-1)p\\pi}{n+1}\\right) + 2\\sin\\left(\\frac{jp\\pi}{n+1}\\right) - \\sin\\left(\\frac{(j+1)p\\pi}{n+1}\\right) = \\mu_p \\sin\\left(\\frac{jp\\pi}{n+1}\\right).\n$$\nUsing the trigonometric sum-to-product identity $\\sin(\\alpha-\\beta) + \\sin(\\alpha+\\beta) = 2\\sin(\\alpha)\\cos(\\beta)$, we can simplify the first and third terms on the left-hand side:\n$$\n\\sin\\left(\\frac{jp\\pi}{n+1} - \\frac{p\\pi}{n+1}\\right) + \\sin\\left(\\frac{jp\\pi}{n+1} + \\frac{p\\pi}{n+1}\\right) = 2\\sin\\left(\\frac{jp\\pi}{n+1}\\right)\\cos\\left(\\frac{p\\pi}{n+1}\\right).\n$$\nSubstituting this back, the equation becomes:\n$$\n2\\sin\\left(\\frac{jp\\pi}{n+1}\\right) - 2\\sin\\left(\\frac{jp\\pi}{n+1}\\right)\\cos\\left(\\frac{p\\pi}{n+1}\\right) = \\mu_p \\sin\\left(\\frac{jp\\pi}{n+1}\\right).\n$$\nFactoring out the sine term (which is not identically zero for the eigenvector):\n$$\n\\left[2 - 2\\cos\\left(\\frac{p\\pi}{n+1}\\right)\\right] \\sin\\left(\\frac{jp\\pi}{n+1}\\right) = \\mu_p \\sin\\left(\\frac{jp\\pi}{n+1}\\right).\n$$\nFrom this, we identify the eigenvalue $\\mu_p^{(n)}$ for the matrix $T_n$:\n$$\n\\mu_p^{(n)} = 2 - 2\\cos\\left(\\frac{p\\pi}{n+1}\\right).\n$$\nUsing the half-angle identity $1 - \\cos(2\\alpha) = 2\\sin^2(\\alpha)$, with $\\alpha = \\frac{p\\pi}{2(n+1)}$, we obtain the final form for the eigenvalues of $T_n$:\n$$\n\\mu_p^{(n)} = 4\\sin^2\\left(\\frac{p\\pi}{2(n+1)}\\right), \\quad p \\in \\{1, 2, \\dots, n\\}.\n$$\n\n**Part 2: Eigenpairs of the two-dimensional matrix $A$**\n\nThe matrix $A$ for the 2D problem is given by the Kronecker sum:\n$$\nA = \\frac{1}{h_{x}^{2}}\\, T_{n_{x}} \\otimes I_{n_{y}} \\;+\\; \\frac{1}{h_{y}^{2}}\\, I_{n_{x}} \\otimes T_{n_{y}}.\n$$\nLet $(\\mu_p^{(n_x)}, v_p^{(n_x)})$ be the eigenpairs for $T_{n_x}$ for $p \\in \\{1, \\dots, n_x\\}$ and $(\\mu_q^{(n_y)}, v_q^{(n_y)})$ be the eigenpairs for $T_{n_y}$ for $q \\in \\{1, \\dots, n_y\\}$, as derived in Part 1. We construct candidate eigenvectors for $A$ as tensor products of the one-dimensional eigenvectors:\n$$\nw_{p,q} = v_p^{(n_x)} \\otimes v_q^{(n_y)}.\n$$\nWe now apply the matrix $A$ to this vector $w_{p,q}$:\n$$\nA w_{p,q} = \\left( \\frac{1}{h_{x}^{2}}\\, T_{n_{x}} \\otimes I_{n_{y}} \\;+\\; \\frac{1}{h_{y}^{2}}\\, I_{n_{x}} \\otimes T_{n_{y}} \\right) ( v_p^{(n_x)} \\otimes v_q^{(n_y)} ).\n$$\nBy linearity, we can distribute the vector to each term in the sum:\n$$\nA w_{p,q} = \\frac{1}{h_{x}^{2}}\\, (T_{n_{x}} \\otimes I_{n_{y}}) ( v_p^{(n_x)} \\otimes v_q^{(n_y)} ) \\;+\\; \\frac{1}{h_{y}^{2}}\\, (I_{n_{x}} \\otimes T_{n_{y}}) ( v_p^{(n_x)} \\otimes v_q^{(n_y)} ).\n$$\nUsing the provided properties of the Kronecker product:\n\\begin{align*} (T_{n_{x}} \\otimes I_{n_{y}}) ( v_p^{(n_x)} \\otimes v_q^{(n_y)} ) &= (T_{n_{x}} v_p^{(n_x)}) \\otimes v_q^{(n_y)} \\\\ (I_{n_{x}} \\otimes T_{n_{y}}) ( v_p^{(n_x)} \\otimes v_q^{(n_y)} ) &= v_p^{(n_x)} \\otimes (T_{n_{y}} v_q^{(n_y)}) \\end{align*}\nSince $v_p^{(n_x)}$ and $v_q^{(n_y)}$ are eigenvectors of $T_{n_x}$ and $T_{n_y}$ respectively, we have $T_{n_{x}} v_p^{(n_x)} = \\mu_p^{(n_x)} v_p^{(n_x)}$ and $T_{n_{y}} v_q^{(n_y)} = \\mu_q^{(n_y)} v_q^{(n_y)}$. Substituting these into the expressions above:\n\\begin{align*} (T_{n_{x}} v_p^{(n_x)}) \\otimes v_q^{(n_y)} &= (\\mu_p^{(n_x)} v_p^{(n_x)}) \\otimes v_q^{(n_y)} = \\mu_p^{(n_x)} (v_p^{(n_x)} \\otimes v_q^{(n_y)}) = \\mu_p^{(n_x)} w_{p,q} \\\\ v_p^{(n_x)} \\otimes (T_{n_{y}} v_q^{(n_y)}) &= v_p^{(n_x)} \\otimes (\\mu_q^{(n_y)} v_q^{(n_y)}) = \\mu_q^{(n_y)} (v_p^{(n_x)} \\otimes v_q^{(n_y)}) = \\mu_q^{(n_y)} w_{p,q} \\end{align*}\nSubstituting these results back into the equation for $A w_{p,q}$:\n$$\nA w_{p,q} = \\frac{1}{h_{x}^{2}}\\, (\\mu_p^{(n_x)} w_{p,q}) \\;+\\; \\frac{1}{h_{y}^{2}}\\, (\\mu_q^{(n_y)} w_{p,q}).\n$$\nFactoring out $w_{p,q}$:\n$$\nA w_{p,q} = \\left( \\frac{\\mu_p^{(n_x)}}{h_x^2} + \\frac{\\mu_q^{(n_y)}}{h_y^2} \\right) w_{p,q}.\n$$\nThis is an eigenvalue equation for $A$. The vector $w_{p,q}$ is an eigenvector of $A$, and the corresponding eigenvalue, denoted $\\lambda_{p,q}$, is:\n$$\n\\lambda_{p,q} = \\frac{\\mu_p^{(n_x)}}{h_x^2} + \\frac{\\mu_q^{(n_y)}}{h_y^2}.\n$$\nThis shows that the eigenvalues of the 2D matrix $A$ are sums of the scaled 1D eigenvalues, for $p \\in \\{1, \\dots, n_x\\}$ and $q \\in \\{1, \\dots, n_y\\}$.\n\n**Part 3: Final Expression for the Eigenvalues of $A$**\n\nFinally, we substitute the expressions for the 1D eigenvalues derived in Part 1 into the formula for $\\lambda_{p,q}$:\n$$\n\\mu_p^{(n_x)} = 4\\sin^2\\left(\\frac{p\\pi}{2(n_x+1)}\\right)\n$$\n$$\n\\mu_q^{(n_y)} = 4\\sin^2\\left(\\frac{q\\pi}{2(n_y+1)}\\right)\n$$\nThe eigenvalue $\\lambda_{p,q}$ of $A$ is therefore:\n$$\n\\lambda_{p,q} = \\frac{1}{h_x^2} \\left[ 4\\sin^2\\left(\\frac{p\\pi}{2(n_x+1)}\\right) \\right] + \\frac{1}{h_y^2} \\left[ 4\\sin^2\\left(\\frac{q\\pi}{2(n_y+1)}\\right) \\right].\n$$\nThis is the required closed-form analytic expression for a generic eigenvalue of $A$ in terms of $p$, $q$, $n_{x}$, $n_{y}$, $h_{x}$, and $h_{y}$.\n$$\n\\lambda_{p,q} = \\frac{4}{h_x^2}\\sin^2\\left(\\frac{p\\pi}{2(n_x+1)}\\right) + \\frac{4}{h_y^2}\\sin^2\\left(\\frac{q\\pi}{2(n_y+1)}\\right).\n$$", "answer": "$$\\boxed{\\frac{4}{h_{x}^{2}}\\sin^{2}\\left(\\frac{p\\pi}{2(n_{x}+1)}\\right) + \\frac{4}{h_{y}^{2}}\\sin^{2}\\left(\\frac{q\\pi}{2(n_{y}+1)}\\right)}$$", "id": "3418648"}, {"introduction": "Many physical systems involve both diffusion and directed transport (advection), leading to non-symmetric matrices upon discretization. This practice delves into the crucial consequences of this asymmetry by constructing the matrix for a 1D advection-diffusion problem using an upwind scheme. You will then quantify the matrix's non-normality by analyzing its skew-Hermitian part, a key step in understanding the stability and convergence of numerical methods for these challenging problems [@problem_id:3418668].", "problem": "Consider the one-dimensional linear operator acting on a sufficiently smooth function $u(x)$ on the interval $[0,1]$ given by $-\\nu\\,u_{xx} + b\\,u_{x}$, where $\\nu>0$ is the diffusion coefficient and $b>0$ is the advection speed. Discretize the interval $[0,1]$ with a uniform grid of $n$ interior points and mesh spacing $h = \\frac{1}{n+1}$, imposing homogeneous Dirichlet boundary conditions $u(0)=u(1)=0$. Use the standard second-order central difference approximation for the second derivative and the first-order upwind approximation for the first derivative appropriate for positive advection speed.\n\nStarting from the definitions of these finite difference approximations and the standard Euclidean inner product on $\\mathbb{C}^{n}$, assemble the $n\\times n$ matrix $A$ representing the discrete operator. Then analyze the nonnormality of $A$ through the field of values (also known as the numerical range) defined by $\\{x^{\\ast}A x / x^{\\ast} x : x \\in \\mathbb{C}^{n}, x \\neq 0\\}$ and the skew-Hermitian part $K = \\frac{1}{2}(A - A^{\\ast})$. In particular, determine the exact closed-form expression of the spectral norm of the skew-Hermitian part, $\\|K\\|_{2}$, in terms of $b$, $h$, and $n$.\n\nExpress your final answer as a single closed-form analytic expression. No rounding is required.", "solution": "Let $U_j$ be the numerical approximation of $u(x_j)$ at the grid point $x_j = j h$. The interval $[0,1]$ is discretized with $n$ interior points, so the vector of unknowns is $U = (U_1, U_2, \\dots, U_n)^T$. The boundary conditions are $U_0 = 0$ and $U_{n+1}=0$.\n\nThe second-order central difference approximation for the second derivative at $x_j$ is:\n$$ u_{xx}(x_j) \\approx \\frac{U_{j+1} - 2U_j + U_{j-1}}{h^2} $$\nFor a positive advection speed ($b>0$), the first-order upwind difference approximation for the first derivative at $x_j$ uses the value from the \"upwind\" direction, which is to the left:\n$$ u_x(x_j) \\approx \\frac{U_j - U_{j-1}}{h} $$\nSubstituting these approximations into the operator $-\\nu\\,u_{xx} + b\\,u_{x}$ for an interior point $x_j$ ($j=1, \\dots, n$) gives the discrete equation:\n$$ (AU)_j = -\\nu \\left( \\frac{U_{j+1} - 2U_j + U_{j-1}}{h^2} \\right) + b \\left( \\frac{U_j - U_{j-1}}{h} \\right) $$\nRearranging the terms to identify the matrix coefficients for $U_{j-1}$, $U_j$, and $U_{j+1}$:\n$$ (AU)_j = \\left(-\\frac{\\nu}{h^2} - \\frac{b}{h}\\right) U_{j-1} + \\left(\\frac{2\\nu}{h^2} + \\frac{b}{h}\\right) U_j + \\left(-\\frac{\\nu}{h^2}\\right) U_{j+1} $$\nThis equation defines the entries of the $n \\times n$ matrix $A$. The matrix $A$ is a tridiagonal matrix with real entries.\n\nThe skew-Hermitian part of $A$ is $K = \\frac{1}{2}(A - A^{\\ast})$. Since $A$ has real entries, its adjoint $A^{\\ast}$ is its transpose $A^T$. The entries of the matrix difference $A - A^T$ are:\n-   Diagonal: $(A - A^T)_{j,j} = A_{j,j} - A^T_{j,j} = 0$\n-   Sub-diagonal: $(A - A^T)_{j,j-1} = A_{j,j-1} - A^T_{j,j-1} = \\left(-\\frac{\\nu}{h^2} - \\frac{b}{h}\\right) - \\left(-\\frac{\\nu}{h^2}\\right) = -\\frac{b}{h}$\n-   Super-diagonal: $(A - A^T)_{j,j+1} = A_{j,j+1} - A^T_{j,j+1} = \\left(-\\frac{\\nu}{h^2}\\right) - \\left(-\\frac{\\nu}{h^2} - \\frac{b}{h}\\right) = \\frac{b}{h}$\n\nThe matrix for the skew-Hermitian part $K$ is therefore:\n$$ K = \\frac{1}{2}(A - A^T) = \\frac{b}{2h} \\begin{pmatrix}\n0 & 1 & 0 & \\dots & 0 \\\\\n-1 & 0 & 1 & \\dots & 0 \\\\\n0 & -1 & 0 & \\ddots & \\vdots \\\\\n\\vdots & & \\ddots & \\ddots & 1 \\\\\n0 & \\dots & & -1 & 0\n\\end{pmatrix}_{n \\times n} $$\nA skew-Hermitian matrix is normal, and for a normal matrix, the spectral norm is equal to its spectral radius (the maximum absolute value of its eigenvalues): $\\|K\\|_{2} = \\rho(K) = \\max_{k} |\\lambda_k(K)|$.\n\nThe eigenvalues of the tridiagonal matrix $S_n = \\text{tridiag}(-1, 0, 1)$ are known to be purely imaginary and given by $\\mu_k = 2i \\cos\\left(\\frac{k\\pi}{n+1}\\right)$ for $k = 1, 2, \\dots, n$.\nThe eigenvalues of $K$ are therefore:\n$$ \\lambda_k(K) = \\frac{b}{2h} \\mu_k = \\frac{b}{2h} \\left( 2i \\cos\\left(\\frac{k\\pi}{n+1}\\right) \\right) = \\frac{b i}{h} \\cos\\left(\\frac{k\\pi}{n+1}\\right) $$\nTo find the spectral norm, we take the maximum absolute value of these eigenvalues over all $k$:\n$$ \\|K\\|_2 = \\max_{k=1,\\dots,n} \\left| \\frac{b i}{h} \\cos\\left(\\frac{k\\pi}{n+1}\\right) \\right| $$\nGiven that $b>0$ and $h>0$, this simplifies to:\n$$ \\|K\\|_2 = \\frac{b}{h} \\max_{k=1,\\dots,n} \\left| \\cos\\left(\\frac{k\\pi}{n+1}\\right) \\right| $$\nThe angles $\\frac{k\\pi}{n+1}$ for $k=1, \\dots, n$ lie in the interval $(0, \\pi)$. The function $|\\cos(\\theta)|$ on this interval has its maximum value at the angle closest to $0$ or $\\pi$. This occurs for $k=1$ and $k=n$, where the value is $\\cos\\left(\\frac{\\pi}{n+1}\\right)$.\nThus, the maximum value is:\n$$ \\max_{k=1,\\dots,n} \\left| \\cos\\left(\\frac{k\\pi}{n+1}\\right) \\right| = \\cos\\left(\\frac{\\pi}{n+1}\\right) $$\nSubstituting this back into the expression for the norm gives the final result. The spectral norm of the skew-Hermitian part $K$ is:\n$$ \\|K\\|_2 = \\frac{b}{h} \\cos\\left(\\frac{\\pi}{n+1}\\right) $$", "answer": "$$\\boxed{\\frac{b}{h} \\cos\\left(\\frac{\\pi}{n+1}\\right)}$$", "id": "3418668"}]}