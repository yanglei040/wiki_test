{"hands_on_practices": [{"introduction": "A fundamental skill in numerical analysis is not just implementing a formula, but understanding its limitations. This first practice explores the classic central difference approximation, revealing the critical trade-off between truncation error from the mathematical approximation and round-off error from finite-precision arithmetic. By performing a grid refinement study [@problem_id:3307315], you will empirically measure the order of convergence and witness firsthand how accuracy can degrade when the step size becomes too small, a crucial lesson in validating any numerical simulation.", "problem": "Consider the role of spatial differentiation in Computational Electromagnetics, specifically in finite-difference discretizations of Maxwell's equations. In methods such as the Finite-Difference Time-Domain (FDTD) scheme, spatial derivatives are approximated on a grid by finite differences, and the accuracy and stability of the electromagnetic field update equations depend on how derivative operators are discretized and on the step size. To study these numerical properties in a controlled setting, focus on approximating the derivative of a smooth scalar function using a symmetric stencil.\n\nStarting from the Taylor expansion of a sufficiently smooth function $f$ about a point $x$, derive a symmetric, two-point stencil approximation to the derivative $f'(x)$ that arises from truncating the Taylor series at the lowest order necessary to eliminate odd-order terms in $h$ and achieves second-order consistency in $h$. Use that approximation in your program for the specific function $f(x) = \\cos x$ at the point $x = 1$, with angles measured in radians.\n\nDefine the absolute error for a step size $h$ as $E(h) = \\left|D(h) - f'(1)\\right|$, where $D(h)$ is your numerical approximation for $f'(1)$ obtained using the symmetric stencil and $f'(1)$ is the exact derivative at $x = 1$. Using a sequence of decreasing step sizes $(h_i)$, define the observed order between successive steps as\n$$\np_i = \\frac{\\log\\left(E(h_i)/E(h_{i+1})\\right)}{\\log\\left(h_i/h_{i+1}\\right)}.\n$$\nIn a grid refinement study, theoretical second-order behavior implies $p_i \\approx 2$ when truncation errors dominate. For sufficiently small $h$, subtractive cancellation in $f(x+h) - f(x-h)$ and floating-point round-off (characterized by machine precision) can cause deviations from this ideal behavior. Your solution must explain the origin of these deviations in terms of truncation error and round-off error.\n\nImplement a program that, for each of the following test sequences of $h$, computes:\n- The sequence of numerical approximations $D(h_i)$ for $f'(1)$,\n- The sequence of absolute errors $E(h_i)$,\n- The sequence of observed orders $(p_i)$ for successive pairs $(h_i, h_{i+1})$.\n\nAngles must be in radians. There are no other physical units in this problem. The programâ€™s final output should aggregate the observed orders for all test sequences into a single line containing a list of lists, where each inner list corresponds to one test sequence and contains the $p_i$ values as floating-point numbers.\n\nTest suite of step-size sequences:\n- Case A (moderate refinement): $[10^{-1},\\, 5 \\cdot 10^{-2},\\, 2.5 \\cdot 10^{-2},\\, 1.25 \\cdot 10^{-2}]$.\n- Case B (coarse-to-moderate refinement): $[5 \\cdot 10^{-1},\\, 2.5 \\cdot 10^{-1},\\, 1.25 \\cdot 10^{-1},\\, 6.25 \\cdot 10^{-2}]$.\n- Case C (near the expected optimal $h$ balancing truncation and round-off): $[10^{-5},\\, 5 \\cdot 10^{-6},\\, 2.5 \\cdot 10^{-6},\\, 1.25 \\cdot 10^{-6}]$.\n- Case D (extremely small steps where round-off dominates): $[10^{-8},\\, 5 \\cdot 10^{-9},\\, 2.5 \\cdot 10^{-9},\\, 1.25 \\cdot 10^{-9}]$.\n- Case E (non-uniform ratios to test generality): $[10^{-3},\\, 7.5 \\cdot 10^{-4},\\, 3 \\cdot 10^{-4},\\, 10^{-4},\\, 7.5 \\cdot 10^{-5}]$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is the inner list of observed orders for the corresponding case (for example, $[[p_{A,1},p_{A,2},p_{A,3}],[p_{B,1},\\dots],\\dots]$).", "solution": "The problem requires the derivation of a second-order accurate finite difference approximation for the first derivative, an analysis of its numerical error properties, and an implementation to compute the observed order of convergence for a specific function and several sets of step sizes.\n\n**1. Derivation of the Symmetric Finite Difference Stencil**\n\nTo derive a symmetric, two-point stencil for the first derivative $f'(x)$, we begin with the Taylor series expansions for a sufficiently smooth function $f(x)$ around a point $x$ for the points $x+h$ and $x-h$, where $h > 0$ is the step size.\n\nThe Taylor expansion for $f(x+h)$ about $x$ is:\n$$\nf(x+h) = f(x) + hf'(x) + \\frac{h^2}{2!}f''(x) + \\frac{h^3}{3!}f'''(x) + \\frac{h^4}{4!}f^{(4)}(x) + \\dots\n$$\n\nThe Taylor expansion for $f(x-h)$ about $x$ is:\n$$\nf(x-h) = f(x) - hf'(x) + \\frac{h^2}{2!}f''(x) - \\frac{h^3}{3!}f'''(x) + \\frac{h^4}{4!}f^{(4)}(x) - \\dots\n$$\n\nTo isolate the first derivative term, $f'(x)$, we subtract the second expansion from the first. This conveniently cancels all terms with even powers of $h$:\n$$\nf(x+h) - f(x-h) = (f(x) - f(x)) + (h - (-h))f'(x) + \\left(\\frac{h^2}{2} - \\frac{h^2}{2}\\right)f''(x) + \\left(\\frac{h^3}{6} - \\left(-\\frac{h^3}{6}\\right)\\right)f'''(x) + \\dots\n$$\n$$\nf(x+h) - f(x-h) = 2hf'(x) + \\frac{2h^3}{6}f'''(x) + O(h^5)\n$$\nwhere $O(h^5)$ represents terms of order $h^5$ and higher.\n\nRearranging this equation to solve for $f'(x)$ gives:\n$$\nf'(x) = \\frac{f(x+h) - f(x-h)}{2h} - \\frac{h^2}{6}f'''(x) - O(h^4)\n$$\n\nFrom this, we define the numerical approximation $D(h)$ for $f'(x)$ by truncating the series:\n$$\nD(h) = \\frac{f(x+h) - f(x-h)}{2h}\n$$\nThis is the required symmetric, two-point stencil, commonly known as the central difference formula.\n\nThe error of this approximation, known as the truncation error $E_t(h)$, is the difference between the exact derivative and its approximation:\n$$\nE_t(h) = f'(x) - D(h) = -\\frac{h^2}{6}f'''(x) - O(h^4)\n$$\nSince the leading term of the error is proportional to $h^2$, the method is second-order accurate (or has second-order consistency).\n\n**2. Analysis of Numerical Error Sources**\n\nThe total absolute error $E(h) = |D(h) - f'(x)|$ in a floating-point computation is a combination of two primary sources: truncation error and round-off error.\n\n**Truncation Error**: As derived above, the truncation error results from approximating an infinite process (the Taylor series) with a finite one (the stencil). For small $h$, this error is dominated by its leading term:\n$$\nE_t(h) \\approx \\left|-\\frac{h^2}{6}f'''(x)\\right| = C_t h^2\n$$\nwhere $C_t = |f'''(x)|/6$. This error decreases quadratically as $h$ decreases. For the specified function $f(x) = \\cos x$, we have $f'(x) = -\\sin x$, $f''(x) = -\\cos x$, and $f'''(x) = \\sin x$. At $x=1$, the constant is $C_t = \\sin(1)/6 \\approx 0.140$.\n\n**Round-off Error**: Round-off error arises because digital computers represent real numbers with finite precision. When $h$ is very small, the values of $f(x+h)$ and $f(x-h)$ become very close. The subtraction $f(x+h) - f(x-h)$ then suffers from **subtractive cancellation**, leading to a loss of significant digits. Let $\\epsilon_{mach}$ be the machine epsilon (the upper bound on the relative error due to rounding in floating-point arithmetic). The error in computing the numerator $f(x+h) - f(x-h)$ is roughly proportional to $|f(x)|\\epsilon_{mach}$. This error is then magnified by division by the small number $2h$. The round-off error $E_r(h)$ can therefore be modeled as:\n$$\nE_r(h) \\approx \\frac{C_r}{h}\n$$\nwhere the constant $C_r$ is on the order of $|f(x)|\\epsilon_{mach}$. This error increases as $h$ decreases.\n\n**Total Error and Optimal Step Size**: The total error is the sum of these two components:\n$$\nE(h) \\approx C_t h^2 + \\frac{C_r}{h}\n$$\nFor large $h$, the $C_t h^2$ term dominates. For very small $h$, the $C_r/h$ term dominates. There exists an optimal step size $h_{opt}$ that minimizes this total error, found by setting the derivative of $E(h)$ with respect to $h$ to zero:\n$$\n\\frac{dE}{dh} = 2C_t h - \\frac{C_r}{h^2} = 0 \\implies h_{opt} = \\left(\\frac{C_r}{2C_t}\\right)^{1/3}\n$$\nFor double-precision floating-point arithmetic, $\\epsilon_{mach} \\approx 2.22 \\times 10^{-16}$. At $x=1$ for $f(x)=\\cos x$, $h_{opt}$ is on the order of $10^{-5}$ to $10^{-6}$.\n\n**3. Observed Order of Convergence**\n\nThe observed order of convergence, $p_i$, between two successive step sizes $h_i$ and $h_{i+1}$ is calculated as:\n$$\np_i = \\frac{\\log\\left(E(h_i)/E(h_{i+1})\\right)}{\\log\\left(h_i/h_{i+1}\\right)}\n$$\nThis formula is derived from the assumption that the error behaves as $E(h) \\approx C h^p$. If this holds, then $E(h_i)/E(h_{i+1}) \\approx (h_i/h_{i+1})^p$, and taking the logarithm of both sides allows solving for $p$.\n\nThe expected behavior of $p_i$ for the test cases is as follows:\n- **Cases A, B, and E**: The step sizes $h_i$ are in the regime where truncation error dominates ($E(h) \\propto h^2$). Therefore, we expect the observed order $p_i$ to be approximately $2$.\n- **Case C**: The step sizes are near $h_{opt}$. In this region, both truncation and round-off errors are significant. The error does not follow a simple power law, so $p_i$ will deviate from $2$, likely decreasing.\n- **Case D**: The step sizes are very small, deep in the round-off dominated regime. Error is expected to increase as $h$ decreases, roughly as $E(h) \\propto 1/h$. In this case, $E(h_i)/E(h_{i+1}) \\approx (1/h_i)/(1/h_{i+1}) = h_{i+1}/h_i$. The formula for $p_i$ yields $p_i \\approx \\log(h_{i+1}/h_i) / \\log(h_i/h_{i+1}) = -1$. Due to the stochastic nature of floating-point arithmetic, the observed values may be chaotic but should generally be negative or close to zero, indicating a breakdown of convergence.\nThe implementation will now compute these values according to the specified test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Derives and applies a central difference approximation for f'(x) to study\n    numerical error behavior. The function computes numerical derivatives, errors,\n    and observed orders of convergence for several sequences of step sizes.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A (moderate refinement)\n        [1e-1, 5e-2, 2.5e-2, 1.25e-2],\n        # Case B (coarse-to-moderate refinement)\n        [5e-1, 2.5e-1, 1.25e-1, 6.25e-2],\n        # Case C (near the expected optimal h)\n        [1e-5, 5e-6, 2.5e-6, 1.25e-6],\n        # Case D (extremely small steps where round-off dominates)\n        [1e-8, 5e-9, 2.5e-9, 1.25e-9],\n        # Case E (non-uniform ratios to test generality)\n        [1e-3, 7.5e-4, 3e-4, 1e-4, 7.5e-5],\n    ]\n\n    # The point at which to evaluate the derivative\n    x = 1.0\n    \n    # The function f(x) = cos(x)\n    f = np.cos\n    \n    # The exact derivative f'(x) = -sin(x) at x=1\n    f_prime_exact = -np.sin(x)\n\n    all_p_results = []\n\n    for h_sequence in test_cases:\n        errors = []\n        for h in h_sequence:\n            # Symmetric, two-point stencil (central difference) approximation\n            # D(h) = (f(x+h) - f(x-h)) / (2*h)\n            d_approx = (f(x + h) - f(x - h)) / (2.0 * h)\n            \n            # Absolute error E(h) = |D(h) - f'(x)|\n            error = np.abs(d_approx - f_prime_exact)\n            errors.append(error)\n\n        p_values = []\n        # Calculate observed order p_i for successive pairs (h_i, h_{i+1})\n        for i in range(len(h_sequence) - 1):\n            h_i = h_sequence[i]\n            h_i_plus_1 = h_sequence[i+1]\n            \n            error_i = errors[i]\n            error_i_plus_1 = errors[i+1]\n            \n            # Formula for observed order: p_i = log(E_i/E_{i+1}) / log(h_i/h_{i+1})\n            # Handle cases where error might be zero to avoid log(0)\n            if error_i_plus_1 == 0.0 or error_i == 0.0:\n                # If the error becomes zero, the order is theoretically infinite.\n                # In practice with floating point, this suggests perfect cancellation or\n                # reaching the limits of precision.\n                p = np.inf\n            else:\n                log_error_ratio = np.log(error_i / error_i_plus_1)\n                log_h_ratio = np.log(h_i / h_i_plus_1)\n                p = log_error_ratio / log_h_ratio\n            \n            p_values.append(p)\n        \n        all_p_results.append(p_values)\n\n    # Format the final output string exactly as required: [[...],[...],...]\n    # without extraneous spaces.\n    outer_list_str = []\n    for p_list in all_p_results:\n        inner_list_str = \"[\" + \",\".join(map(str, p_list)) + \"]\"\n        outer_list_str.append(inner_list_str)\n    \n    final_output = \"[\" + \",\".join(outer_list_str) + \"]\"\n    \n    # Final print statement in the exact required format.\n    print(final_output)\n\nsolve()\n```", "id": "3307315"}, {"introduction": "In many physics simulations, particularly in electromagnetics using the Finite-Difference Time-Domain (FDTD) method, field components are evaluated on staggered grids for enhanced accuracy. This practice moves beyond a simple Taylor series error analysis to the more physically insightful concept of numerical dispersion, which quantifies how a numerical scheme distorts waves. By analyzing an evanescent field on both collocated and staggered grids [@problem_id:3307292], you will derive the amplitude and phase errors, providing a concrete demonstration of why staggered grids are often superior for wave propagation problems.", "problem": "A one-dimensional evanescent electromagnetic field along the $x$-axis, representative of below-cutoff behavior in a uniform waveguide, can be modeled as $E(x) = E_{0} \\exp(-\\alpha x)$ with attenuation constant $\\alpha > 0$. In computational electromagnetics, the spatial derivative $\\partial_{x} E$ is approximated on a uniform grid of spacing $\\Delta x$ using either a collocated central-difference stencil (all quantities defined at integer grid nodes $x_{i} = i \\Delta x$) or a staggered central-difference stencil (as in the Yee grid, where the field samples used to approximate the derivative at $x_{i}$ are taken at half-integer positions $x_{i \\pm 1/2} = x_{i} \\pm \\Delta x/2$).\n\nDefine the discrete approximation operators $D_{\\mathrm{col}}$ and $D_{\\mathrm{stag}}$ that produce an approximation to $\\partial_{x} E$ at $x_{i}$ from grid samples on the respective collocated and staggered arrangements, using second-order accurate centered differences in each case. For the evanescent mode $E(x) = E_{0} \\exp(-\\alpha x)$, write each discrete approximation at $x_{i}$ as a multiplicative factor times the exact derivative, i.e.,\n$$\nD_{\\mathrm{col}} E(x_{i}) = G_{\\mathrm{col}}(\\alpha \\Delta x)\\,\\partial_{x} E(x_{i}), \\qquad D_{\\mathrm{stag}} E(x_{i}) = G_{\\mathrm{stag}}(\\alpha \\Delta x)\\,\\partial_{x} E(x_{i}),\n$$\nwhere $G_{\\mathrm{col}}$ and $G_{\\mathrm{stag}}$ are complex-valued functions of the dimensionless parameter $z = \\alpha \\Delta x$ that encode both amplitude error and phase lag of the discrete operator relative to the exact derivative.\n\nStarting from first principles and the definitions above, derive $G_{\\mathrm{col}}(z)$ and $G_{\\mathrm{stag}}(z)$, and then, over the interval $z \\in [0.1,\\,1]$, determine:\n- the maximum amplitude error for the collocated approximation, defined as $\\max_{z \\in [0.1,\\,1]} \\left(|G_{\\mathrm{col}}(z)| - 1\\right)$,\n- the maximum amplitude error for the staggered approximation, defined as $\\max_{z \\in [0.1,\\,1]} \\left(|G_{\\mathrm{stag}}(z)| - 1\\right)$,\n- the phase lag for the collocated approximation, defined as $\\arg\\!\\left(G_{\\mathrm{col}}(z)\\right)$ over $z \\in [0.1,\\,1]$,\n- the phase lag for the staggered approximation, defined as $\\arg\\!\\left(G_{\\mathrm{stag}}(z)\\right)$ over $z \\in [0.1,\\,1]$.\n\nReport your final numerical values in the order listed above as a single row matrix. Round all numerical values to four significant figures. Express phase in radians. The amplitude errors are unitless.", "solution": "The problem is validated as scientifically grounded, well-posed, objective, and self-contained. The provided information is sufficient and consistent for deriving a unique solution.\n\nThe problem asks for an analysis of two finite difference approximations for the first derivative of an evanescent electromagnetic field, modeled as $E(x) = E_{0} \\exp(-\\alpha x)$. The exact first derivative is given by:\n$$\n\\partial_{x} E(x) = \\frac{d}{dx} \\left(E_{0} \\exp(-\\alpha x)\\right) = - \\alpha E_{0} \\exp(-\\alpha x) = -\\alpha E(x)\n$$\nAt a grid point $x_{i}$, the exact derivative is $\\partial_{x} E(x_{i}) = -\\alpha E(x_{i})$.\n\nFirst, we derive the expression for the numerical error factor $G_{\\mathrm{col}}(z)$ for the collocated central-difference scheme. The second-order accurate collocated central-difference operator, $D_{\\mathrm{col}}$, is defined as:\n$$\nD_{\\mathrm{col}} E(x_{i}) = \\frac{E(x_{i+1}) - E(x_{i-1})}{2 \\Delta x} = \\frac{E(x_{i} + \\Delta x) - E(x_{i} - \\Delta x)}{2 \\Delta x}\n$$\nSubstituting the field expression $E(x) = E_{0} \\exp(-\\alpha x)$:\n$$\nD_{\\mathrm{col}} E(x_{i}) = \\frac{E_{0} \\exp(-\\alpha(x_{i} + \\Delta x)) - E_{0} \\exp(-\\alpha(x_{i} - \\Delta x))}{2 \\Delta x}\n$$\nFactoring out the term $E_{0} \\exp(-\\alpha x_{i})$:\n$$\nD_{\\mathrm{col}} E(x_{i}) = \\frac{E_{0} \\exp(-\\alpha x_{i}) \\left[ \\exp(-\\alpha \\Delta x) - \\exp(\\alpha \\Delta x) \\right]}{2 \\Delta x}\n$$\nUsing the definition of the hyperbolic sine function, $\\sinh(y) = \\frac{\\exp(y) - \\exp(-y)}{2}$, we can write $\\exp(-\\alpha \\Delta x) - \\exp(\\alpha \\Delta x) = -2 \\sinh(\\alpha \\Delta x)$.\n$$\nD_{\\mathrm{col}} E(x_{i}) = \\frac{E(x_{i}) \\left[ -2 \\sinh(\\alpha \\Delta x) \\right]}{2 \\Delta x} = -E(x_{i}) \\frac{\\sinh(\\alpha \\Delta x)}{\\Delta x}\n$$\nThe problem defines $G_{\\mathrm{col}}$ through the relation $D_{\\mathrm{col}} E(x_{i}) = G_{\\mathrm{col}}(\\alpha \\Delta x) \\partial_{x} E(x_{i})$. Substituting our expressions for the numerical and exact derivatives:\n$$\n-E(x_{i}) \\frac{\\sinh(\\alpha \\Delta x)}{\\Delta x} = G_{\\mathrm{col}}(\\alpha \\Delta x) \\left( -\\alpha E(x_{i}) \\right)\n$$\nSolving for $G_{\\mathrm{col}}(\\alpha \\Delta x)$:\n$$\nG_{\\mathrm{col}}(\\alpha \\Delta x) = \\frac{\\sinh(\\alpha \\Delta x)}{\\alpha \\Delta x}\n$$\nWith the dimensionless parameter $z = \\alpha \\Delta x$, we have $G_{\\mathrm{col}}(z) = \\frac{\\sinh(z)}{z}$.\n\nNext, we follow the same procedure for the staggered central-difference scheme. The second-order accurate staggered central-difference operator, $D_{\\mathrm{stag}}$, is defined as:\n$$\nD_{\\mathrm{stag}} E(x_{i}) = \\frac{E(x_{i+1/2}) - E(x_{i-1/2})}{\\Delta x} = \\frac{E(x_{i} + \\Delta x/2) - E(x_{i} - \\Delta x/2)}{\\Delta x}\n$$\nSubstituting the field expression:\n$$\nD_{\\mathrm{stag}} E(x_{i}) = \\frac{E_{0} \\exp(-\\alpha(x_{i} + \\Delta x/2)) - E_{0} \\exp(-\\alpha(x_{i} - \\Delta x/2))}{\\Delta x}\n$$\nFactoring out $E_{0} \\exp(-\\alpha x_{i})$:\n$$\nD_{\\mathrm{stag}} E(x_{i}) = \\frac{E_{0} \\exp(-\\alpha x_{i}) \\left[ \\exp(-\\alpha \\Delta x/2) - \\exp(\\alpha \\Delta x/2) \\right]}{\\Delta x}\n$$\nUsing the identity $\\exp(-y) - \\exp(y) = -2 \\sinh(y)$:\n$$\nD_{\\mathrm{stag}} E(x_{i}) = \\frac{E(x_{i}) \\left[ -2 \\sinh(\\alpha \\Delta x/2) \\right]}{\\Delta x}\n$$\nUsing the relation $D_{\\mathrm{stag}} E(x_{i}) = G_{\\mathrm{stag}}(\\alpha \\Delta x) \\partial_{x} E(x_{i})$:\n$$\n-\\frac{2 E(x_{i}) \\sinh(\\alpha \\Delta x/2)}{\\Delta x} = G_{\\mathrm{stag}}(\\alpha \\Delta x) \\left( -\\alpha E(x_{i}) \\right)\n$$\nSolving for $G_{\\mathrm{stag}}(\\alpha \\Delta x)$:\n$$\nG_{\\mathrm{stag}}(\\alpha \\Delta x) = \\frac{2 \\sinh(\\alpha \\Delta x/2)}{\\alpha \\Delta x} = \\frac{\\sinh(\\alpha \\Delta x/2)}{\\alpha \\Delta x/2}\n$$\nWith $z = \\alpha \\Delta x$, this becomes $G_{\\mathrm{stag}}(z) = \\frac{\\sinh(z/2)}{z/2}$.\n\nNow we analyze these functions over the interval $z \\in [0.1, 1]$.\nFor the collocated scheme, the error factor is $G_{\\mathrm{col}}(z) = \\frac{\\sinh(z)}{z}$. For $z > 0$, $\\sinh(z) > 0$, so $G_{\\mathrm{col}}(z)$ is real and positive. The amplitude error is defined as $|G_{\\mathrm{col}}(z)| - 1 = \\frac{\\sinh(z)}{z} - 1$. To find its maximum, we examine its derivative:\n$$\n\\frac{d}{dz} \\left( \\frac{\\sinh(z)}{z} - 1 \\right) = \\frac{z \\cosh(z) - \\sinh(z)}{z^2}\n$$\nLet $h(z) = z \\cosh(z) - \\sinh(z)$. At $z=0$, $h(0) = 0$. The derivative of $h(z)$ is $h'(z) = \\frac{d}{dz}(z \\cosh(z) - \\sinh(z)) = \\cosh(z) + z \\sinh(z) - \\cosh(z) = z \\sinh(z)$. For $z > 0$, both $z$ and $\\sinh(z)$ are positive, so $h'(z) > 0$. This means $h(z)$ is strictly increasing for $z > 0$. Since $h(0) = 0$, $h(z) > 0$ for $z > 0$. Consequently, the derivative of the amplitude error is positive, meaning the error function is monotonically increasing on $z \\in [0.1, 1]$. The maximum error thus occurs at $z=1$.\nMaximum amplitude error (collocated) = $G_{\\mathrm{col}}(1) - 1 = \\sinh(1) - 1$.\n$\\sinh(1) \\approx 1.17520119$. So, the error is $0.17520119 \\approx 0.1752$.\nSince $G_{\\mathrm{col}}(z)$ is a positive real number for $z \\in [0.1, 1]$, its argument is zero. So, the phase lag is $\\arg(G_{\\mathrm{col}}(z)) = 0$ radians for all $z$ in the interval.\n\nFor the staggered scheme, the error factor is $G_{\\mathrm{stag}}(z) = \\frac{\\sinh(z/2)}{z/2}$. This function has the same form as $G_{\\mathrm{col}}(z)$, but with argument $z/2$. The amplitude error is $|G_{\\mathrm{stag}}(z)| - 1 = \\frac{\\sinh(z/2)}{z/2} - 1$. As established, the function $\\frac{\\sinh(u)}{u}$ is monotonically increasing for $u > 0$. Since $z$ increases over $[0.1, 1]$, $u=z/2$ increases over $[0.05, 0.5]$, so the error is also monotonically increasing. The maximum error occurs at $z=1$.\nMaximum amplitude error (staggered) = $G_{\\mathrm{stag}}(1) - 1 = \\frac{\\sinh(0.5)}{0.5} - 1 = 2 \\sinh(0.5) - 1$.\n$\\sinh(0.5) \\approx 0.521095305$. So, the error is $2 \\times 0.521095305 - 1 = 1.04219061 - 1 = 0.04219061 \\approx 0.04219$.\nSimilarly, since $G_{\\mathrm{stag}}(z)$ is a positive real number for $z \\in [0.1, 1]$, its argument is zero. The phase lag is $\\arg(G_{\\mathrm{stag}}(z)) = 0$ radians.\n\nThe four requested values are:\n1. Max amplitude error (collocated): $\\sinh(1) - 1 \\approx 0.1752$\n2. Max amplitude error (staggered): $2 \\sinh(0.5) - 1 \\approx 0.04219$\n3. Phase lag (collocated): $0$ radians\n4. Phase lag (staggered): $0$ radians\n\nWe report these rounded to four significant figures as requested, writing the exact zeros as $0.0000$ for consistency.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0.1752 & 0.04219 & 0.0000 & 0.0000\n\\end{pmatrix}\n}\n$$", "id": "3307292"}, {"introduction": "While increasing a stencil's width can improve accuracy, it complicates boundary conditions and parallelization. High-order compact schemes offer an elegant solution, achieving superior accuracy by creating an implicit relationship between derivatives at neighboring points. This advanced exercise [@problem_id:3307323] guides you through the derivation of a fourth-order compact scheme and the implementation of an efficient solver for the resulting cyclic linear system, a powerful technique used in high-fidelity computational fluid dynamics and electromagnetics.", "problem": "Consider the task of computing the spatial derivative $\\partial_x E_y$ on a one-dimensional periodic grid, as arises in the discretization of the curl operators in Maxwell's equations for Computational Electromagnetics. Work on a uniform, periodic grid on the interval $[0,2\\pi)$ with $N$ points, grid spacing $\\Delta x = 2\\pi/N$, and node locations $x_i = i \\,\\Delta x$ for $i = 0,1,\\dots,N-1$. Angles must be treated in radians. Starting from the definition of the derivative and Taylor series expansions about a grid point, derive a three-point, fourth-order compact finite difference scheme for $\\partial_x E_y$ with the following characteristics:\n\n- The approximation at node $i$ uses a linear system that couples the discrete derivatives $\\{(\\partial_x E_y)_j\\}$ through a symmetric, nearest-neighbor stencil on the left-hand side, and uses a nearest-neighbor antisymmetric stencil in the function values $\\{E_y(x_j)\\}$ on the right-hand side.\n- The scheme must be fourth-order accurate on a smooth periodic function $E_y(x)$, in the sense that the local truncation error is $\\mathcal{O}(\\Delta x^4)$.\n- The resulting global linear system for the vector of discrete derivatives must be circulant-cyclic tridiagonal due to the periodic boundary conditions.\n\nFrom these requirements:\n\n1. Determine the unique coefficients (if any) that produce fourth-order accuracy for a three-point compact scheme consistent with the above constraints. Derive them by matching Taylor series coefficients using only the fundamental definition of the derivative and Taylor expansions for $E_y(x)$ and its derivatives at neighboring nodes.\n\n2. Explicitly write the resulting linear system that must be solved to obtain the vector of discrete derivatives $\\mathbf{d} \\in \\mathbb{R}^N$ where $d_i \\approx (\\partial_x E_y)(x_i)$, emphasizing the circulant-cyclic tridiagonal structure of the system matrix. Give a principled sketch of an efficient solver that exploits this structure without using dense linear algebra. Your outline must be based on fundamental linear algebra identities for low-rank updates to an invertible matrix.\n\n3. Implement the method as a program that:\n   - Constructs the periodic grid $x_i$ with the specified $N$ and uses $\\Delta x = 2\\pi/N$.\n   - For each test function $E_y(x)$ listed below, assembles the corresponding right-hand side and solves the cyclic tridiagonal linear system to obtain the compact fourth-order approximation to $\\partial_x E_y$.\n   - Compares the numerical result to the exact derivative at each grid point and returns the maximum absolute error over the grid for each test case.\n\nUse only pure mathematical units (no physical units are required), and angles must be in radians. The final output must aggregate the results of all test cases as a single line, a comma-separated list enclosed in square brackets, where each entry is the maximum absolute error for the corresponding test case, printed as a floating-point number. You must round each entry to $12$ significant digits in scientific notation.\n\nTest Suite (each case must be evaluated independently and in the given order):\n\n- Case $1$ (smooth, moderate wavenumber): $N = 64$, $E_y(x) = \\sin(3 x)$, exact derivative $\\partial_x E_y = 3 \\cos(3 x)$.\n- Case $2$ (constant field): $N = 32$, $E_y(x) = 5$, exact derivative $\\partial_x E_y = 0$.\n- Case $3$ (high-frequency near-Nyquist): $N = 128$, let $k = N/2 - 1 = 63$, $E_y(x) = \\sin(k x)$, exact derivative $\\partial_x E_y = k \\cos(k x)$.\n- Case $4$ (non-polynomial smooth function): $N = 100$, $E_y(x) = \\exp(\\sin x)$, exact derivative $\\partial_x E_y = \\cos x \\,\\exp(\\sin x)$.\n- Case $5$ (very small grid): $N = 4$, $E_y(x) = \\sin x$, exact derivative $\\partial_x E_y = \\cos x$.\n\nFinal Output Format:\n\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the same order as the test cases (e.g., \"[v1,v2,v3,v4,v5]\"). Each value must be a floating-point number in scientific notation, rounded to $12$ significant digits.\n\nNo input should be read from the user, and no external files should be used. The implementation should be expressed in a general way that would work for any $N \\ge 4$ and any smooth periodic function $E_y(x)$ following the specified stencil, but it should compute and print results only for the test suite given above.", "solution": "The problem requires the derivation, analysis, and implementation of a three-point, fourth-order compact finite difference scheme for the first derivative on a one-dimensional periodic grid. The validation process confirms that the problem is scientifically grounded, well-posed, objective, and contains sufficient information for a unique solution. It is a standard problem in numerical analysis for partial differential equations.\n\n**Part 1: Derivation of the Compact Scheme Coefficients**\n\nWe are tasked with finding a finite difference approximation for the derivative $\\partial_x E_y$ at grid points $x_i = i \\Delta x$ for $i = 0, 1, \\dots, N-1$ on a uniform periodic grid with spacing $\\Delta x = 2\\pi/N$. Let $d_i$ be the numerical approximation to $E_y'(x_i) \\equiv (\\partial_x E_y)(x_i)$, and let $E_{y,i}$ denote $E_y(x_i)$.\n\nThe problem specifies the form of the compact scheme. At each node $i$, the discrete derivatives $\\{d_j\\}$ are coupled through a symmetric, nearest-neighbor stencil, and the function values $\\{E_{y,j}\\}$ are coupled through a nearest-neighbor antisymmetric stencil. This structure can be expressed as:\n$$\n\\alpha d_{i-1} + \\beta d_i + \\alpha d_{i+1} = \\frac{1}{\\Delta x} \\left( c E_{y,i+1} - c E_{y,i-1} \\right)\n$$\nwhere $\\alpha$, $\\beta$, and $c$ are the unknown coefficients to be determined. The symmetry on the left-hand side (LHS) dictates that the coefficients for $d_{i-1}$ and $d_{i+1}$ are identical. The antisymmetry on the right-hand side (RHS) gives the form $E_{y,i+1} - E_{y,i-1}$. Without loss of generality, we can normalize the coefficient of $d_i$ to $\\beta=1$. The factor $1/\\Delta x$ on the RHS is conventional for first-derivative approximations. Let us also rescale the RHS coefficient to be $a/2$ for algebraic convenience, matching the form of a standard centered difference. The scheme becomes:\n$$\n\\alpha d_{i-1} + d_i + \\alpha d_{i+1} = \\frac{a}{2\\Delta x} (E_{y,i+1} - E_{y,i-1})\n$$\n\nTo find the coefficients $\\alpha$ and $a$ that yield fourth-order accuracy, we replace all terms with their Taylor series expansions around the point $x_i$. We denote the exact derivatives of $E_y(x)$ at $x_i$ as $E_y'$, $E_y''$, etc. The numerical derivatives $d_j$ are assumed to be equal to the exact derivatives $E_y'(x_j)$ for this error analysis.\n\nThe Taylor series for $E_y(x)$ at neighboring points $x_{i\\pm1} = x_i \\pm \\Delta x$ are:\n$$\nE_{y,i\\pm1} = E_y \\pm \\Delta x E_y' + \\frac{(\\Delta x)^2}{2!} E_y'' \\pm \\frac{(\\Delta x)^3}{3!} E_y''' + \\frac{(\\Delta x)^4}{4!} E_y^{(4)} \\pm \\frac{(\\Delta x)^5}{5!} E_y^{(5)} + \\mathcal{O}((\\Delta x)^6)\n$$\nThe Taylor series for the derivative $E_y'(x)$ at neighboring points are:\n$$\nd_{i\\pm1} \\approx E_y'(x_{i\\pm1}) = E_y' \\pm \\Delta x E_y'' + \\frac{(\\Delta x)^2}{2!} E_y''' \\pm \\frac{(\\Delta x)^3}{3!} E_y^{(4)} + \\frac{(\\Delta x)^4}{4!} E_y^{(5)} + \\mathcal{O}((\\Delta x)^5)\n$$\n\nFirst, we expand the RHS of the scheme:\n$$\n\\text{RHS} = \\frac{a}{2\\Delta x} (E_{y,i+1} - E_{y,i-1}) = \\frac{a}{2\\Delta x} \\left( 2\\Delta x E_y' + 2\\frac{(\\Delta x)^3}{6} E_y''' + 2\\frac{(\\Delta x)^5}{120} E_y^{(5)} + \\mathcal{O}((\\Delta x)^7) \\right)\n$$\n$$\n\\text{RHS} = a E_y' + \\frac{a(\\Delta x)^2}{6} E_y''' + \\frac{a(\\Delta x)^4}{120} E_y^{(5)} + \\mathcal{O}((\\Delta x)^6)\n$$\n\nNext, we expand the LHS of the scheme:\n$$\n\\text{LHS} = \\alpha (d_{i-1} + d_{i+1}) + d_i \\approx \\alpha (E_y'(x_{i-1}) + E_y'(x_{i+1})) + E_y'(x_i)\n$$\n$$\n\\text{LHS} \\approx \\alpha \\left( 2E_y' + 2\\frac{(\\Delta x)^2}{2} E_y''' + 2\\frac{(\\Delta x)^4}{24} E_y^{(5)} + \\mathcal{O}((\\Delta x)^6) \\right) + E_y'\n$$\n$$\n\\text{LHS} \\approx (1+2\\alpha)E_y' + \\alpha(\\Delta x)^2 E_y''' + \\frac{\\alpha(\\Delta x)^4}{12} E_y^{(5)} + \\mathcal{O}((\\Delta x)^6)\n$$\n\nFor the scheme to be accurate, the expression LHS - RHS must be as close to zero as possible. We equate the coefficients of the derivatives of $E_y$ on both sides.\n\\begin{itemize}\n    \\item Coefficient of $E_y'$: $1+2\\alpha = a$\n    \\item Coefficient of $E_y''$: The odd derivatives of $E_y$ vanish due to symmetry, which is a key property of this centered scheme.\n    \\item Coefficient of $E_y'''$: To achieve an accuracy higher than second-order, the $(\\Delta x)^2$ terms must cancel.\n    $$ \\alpha(\\Delta x)^2 = \\frac{a(\\Delta x)^2}{6} \\implies \\alpha = \\frac{a}{6} $$\n\\end{itemize}\n\nWe now have a system of two linear equations for the two unknowns $\\alpha$ and $a$:\n1. $a = 1 + 2\\alpha$\n2. $a = 6\\alpha$\n\nSubstituting (2) into (1) gives $6\\alpha = 1 + 2\\alpha$, which yields $4\\alpha = 1$, so $\\alpha = 1/4$.\nSubstituting $\\alpha=1/4$ back into (2) gives $a = 6(1/4) = 3/2$.\n\nThe unique coefficients are $\\alpha = 1/4$ and $a = 3/2$. The scheme is:\n$$\n\\frac{1}{4} d_{i-1} + d_i + \\frac{1}{4} d_{i+1} = \\frac{3/2}{2\\Delta x} (E_{y,i+1} - E_{y,i-1}) = \\frac{3}{4\\Delta x} (E_{y,i+1} - E_{y,i-1})\n$$\n\nThe local truncation error (LTE) is determined by the first non-vanishing term in the expansion. We compare the coefficients of the $(\\Delta x)^4$ terms:\n$$\n\\text{LTE} \\propto \\left( \\frac{\\alpha(\\Delta x)^4}{12} - \\frac{a(\\Delta x)^4}{120} \\right) E_y^{(5)} = \\left( \\frac{1/4}{12} - \\frac{3/2}{120} \\right) (\\Delta x)^4 E_y^{(5)}\n$$\n$$\n= \\left( \\frac{1}{48} - \\frac{3}{240} \\right) (\\Delta x)^4 E_y^{(5)} = \\left( \\frac{5}{240} - \\frac{3}{240} \\right) (\\Delta x)^4 E_y^{(5)} = \\frac{2}{240} (\\Delta x)^4 E_y^{(5)} = \\frac{1}{120} (\\Delta x)^4 E_y^{(5)}\n$$\nThe error is proportional to $(\\Delta x)^4$, confirming the scheme is fourth-order accurate. The truncation error is formally defined as the residual when the exact solution is substituted into the normalized difference operator. After dividing by the normalization factor $1+2\\alpha = 3/2$, the error is $T_i = -\\frac{1}{180}(\\Delta x)^4 E_y^{(5)}(x_i) + \\mathcal{O}((\\Delta x)^6)$.\n\n**Part 2: The Linear System and Efficient Solver**\n\nThe scheme must be applied to all $N$ grid points, from $i=0$ to $i=N-1$. This generates a system of $N$ linear equations for the $N$ unknown derivatives $d_0, d_1, \\dots, d_{N-1}$. Let $\\mathbf{d} = [d_0, d_1, \\dots, d_{N-1}]^T$ be the vector of discrete derivatives. The system can be written in matrix form as $A \\mathbf{d} = \\mathbf{b}$, where $A$ is the coefficient matrix and $\\mathbf{b}$ is the right-hand side vector.\n\nThe equation for a generic interior point $i$ is:\n$$\n(\\frac{1}{4}) d_{i-1} + (1) d_i + (\\frac{1}{4}) d_{i+1} = b_i\n$$\nwhere $b_i = \\frac{3}{4\\Delta x} (E_{y,i+1} - E_{y,i-1})$.\n\nDue to the periodic boundary conditions, indices are taken modulo $N$. For $i=0$, $d_{i-1}=d_{-1}$ becomes $d_{N-1}$. For $i=N-1$, $d_{i+1}=d_N$ becomes $d_0$. This \"wraparound\" nature gives the matrix $A$ its characteristic structure:\n$$\nA = \\begin{pmatrix}\n1 & 1/4 & 0 & \\dots & 0 & 1/4 \\\\\n1/4 & 1 & 1/4 & \\dots & 0 & 0 \\\\\n0 & 1/4 & 1 & \\dots & 0 & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\ddots & \\ddots & \\vdots \\\\\n0 & 0 & 0 & \\dots & 1 & 1/4 \\\\\n1/4 & 0 & 0 & \\dots & 1/4 & 1\n\\end{pmatrix}\n$$\nThis is a symmetric, tridiagonal, circulant-cyclic matrix. The right-hand side vector $\\mathbf{b}$ has components $b_i = \\frac{3}{4\\Delta x} (E_{y,(i+1)\\%N} - E_{y,(i-1+N)\\%N})$.\n\nAn efficient solver must exploit this circulant structure.\nA highly efficient method, with complexity $\\mathcal{O}(N \\log N)$, is based on the Discrete Fourier Transform (DFT), as circulant matrices are diagonalized by the DFT.\n1.  Let $\\mathbf{c} = [1, 1/4, 0, \\dots, 0, 1/4]$ be the first row of $A$. The convolution theorem states that for a circulant matrix $A$, a matrix-vector product $A\\mathbf{d}$ is equivalent to the circular convolution of $\\mathbf{c}$ and $\\mathbf{d}$.\n2.  Applying the DFT (denoted by $\\mathcal{F}$) to the system $A\\mathbf{d} = \\mathbf{b}$ gives $\\mathcal{F}(A\\mathbf{d}) = \\mathcal{F}(\\mathbf{b})$.\n3.  This becomes $\\mathcal{F}(\\mathbf{c}) \\odot \\mathcal{F}(\\mathbf{d}) = \\mathcal{F}(\\mathbf{b})$, where $\\odot$ denotes element-wise multiplication. The vector $\\mathbf{\\Lambda} = \\mathcal{F}(\\mathbf{c})$ contains the eigenvalues of $A$.\n4.  The system is solved in the frequency domain by element-wise division: $\\mathcal{F}(\\mathbf{d}) = \\mathcal{F}(\\mathbf{b}) / \\mathbf{\\Lambda}$.\n5.  The solution vector $\\mathbf{d}$ is recovered by applying the inverse DFT: $\\mathbf{d} = \\mathcal{F}^{-1}(\\mathcal{F}(\\mathbf{b}) / \\mathbf{\\Lambda})$. The use of Fast Fourier Transform (FFT) algorithms for the DFT and its inverse makes this method very fast. The eigenvalues of $A$ are given by $\\lambda_j = 1 + (1/4)e^{i 2\\pi j/N} + (1/4)e^{-i 2\\pi j/N} = 1 + (1/2)\\cos(2\\pi j/N)$ for $j=0, \\dots, N-1$. Since $|\\cos(\\theta)| \\le 1$, all eigenvalues are positive, ensuring the matrix is nonsingular.\n\nAnother efficient $\\mathcal{O}(N)$ method, hinted at by the prompt's mention of \"low-rank updates,\" is to use the Sherman-Morrison-Woodbury formula. The matrix $A$ can be written as a simple tridiagonal matrix $T$ plus a rank-$2$ correction $U V^T$ for the corner elements: $A = T + \\alpha(\\mathbf{e}_0 \\mathbf{e}_{N-1}^T + \\mathbf{e}_{N-1} \\mathbf{e}_0^T)$, where $\\alpha=1/4$. The solution to $A\\mathbf{d} = \\mathbf{b}$ can then be found by solving a few tridiagonal systems (using the $\\mathcal{O}(N)$ Thomas algorithm) and a small $2 \\times 2$ system.\n\n**Part 3: Implementation Strategy**\n\nThe implementation will follow the FFT-based approach, which is conveniently provided by `scipy.linalg.solve_circulant`.\nFor each test case specified:\n1.  Set the grid size $N$ and define the function $E_y(x)$ and its analytical derivative.\n2.  Construct the grid $x_i = i (2\\pi/N)$ for $i=0, \\dots, N-1$.\n3.  Evaluate the function on the grid to create the vector $\\mathbf{E}_y$.\n4.  Construct the right-hand side vector $\\mathbf{b}$ using the formula $b_i = \\frac{3}{4\\Delta x} (E_{y,i+1} - E_{y,i-1})$. Periodic indexing is handled efficiently using `numpy.roll`.\n5.  Define the first row of the circulant matrix $A$: $\\mathbf{c} = [1, 1/4, 0, \\dots, 0, 1/4]$.\n6.  Solve the system $A\\mathbf{d} = \\mathbf{b}$ for $\\mathbf{d}$ using `scipy.linalg.solve_circulant(c, b)`.\n7.  Evaluate the exact derivative on the grid to get the vector $\\mathbf{d}_{\\text{exact}}$.\n8.  Calculate the maximum absolute error over the grid: $\\max(|\\mathbf{d} - \\mathbf{d}_{\\text{exact}}|)$.\n9.  The errors for all test cases are collected and formatted into the required output string.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import solve_circulant\n\ndef solve():\n    \"\"\"\n    Derives and implements a fourth-order compact finite difference scheme\n    to compute the first derivative of periodic functions.\n    \"\"\"\n\n    def compact_fourth_order_derivative(Ey_values, N):\n        \"\"\"\n        Computes the derivative using the 4th-order compact scheme.\n\n        Args:\n            Ey_values (np.ndarray): The function values E_y(x_i) on the grid.\n            N (int): The number of grid points.\n\n        Returns:\n            np.ndarray: The numerical derivative d_i at each grid point.\n        \"\"\"\n        # Grid spacing\n        delta_x = 2.0 * np.pi / N\n\n        # Coefficients of the scheme derived from Taylor series analysis.\n        # Scheme: (1/4)d_{i-1} + d_i + (1/4)d_{i+1} = (3/4 * delta_x) * (E_{y,i+1} - E_{y,i-1})\n        alpha = 1.0 / 4.0\n        a = 3.0 / 2.0\n\n        # Construct the right-hand side (RHS) vector b.\n        # np.roll handles the periodic boundary conditions efficiently.\n        Ey_plus_1 = np.roll(Ey_values, -1)\n        Ey_minus_1 = np.roll(Ey_values, 1)\n        \n        rhs_b = (a / (2.0 * delta_x)) * (Ey_plus_1 - Ey_minus_1)\n\n        # Construct the first row of the circulant-cyclic tridiagonal matrix A.\n        # A has 1 on the main diagonal, and alpha on the sub- and super-diagonals,\n        # with cyclic wrap-around.\n        circulant_first_row = np.zeros(N)\n        circulant_first_row[0] = 1.0\n        circulant_first_row[1] = alpha\n        circulant_first_row[N - 1] = alpha  # or circulant_first_row[-1]\n\n        # Solve the linear system A*d = b using the efficient circulant solver,\n        # which is based on FFTs.\n        d_numerical = solve_circulant(circulant_first_row, rhs_b)\n\n        return d_numerical\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"N\": 64,\n            \"Ey_func\": lambda x: np.sin(3 * x),\n            \"dEy_exact_func\": lambda x: 3 * np.cos(3 * x),\n        },\n        {\n            \"N\": 32,\n            \"Ey_func\": lambda x: np.full_like(x, 5.0),\n            \"dEy_exact_func\": lambda x: np.zeros_like(x),\n        },\n        {\n            \"N\": 128,\n            \"k\": 63, # k = N/2 - 1\n            \"Ey_func\": lambda x, k=63: np.sin(k * x),\n            \"dEy_exact_func\": lambda x, k=63: k * np.cos(k * x),\n        },\n        {\n            \"N\": 100,\n            \"Ey_func\": lambda x: np.exp(np.sin(x)),\n            \"dEy_exact_func\": lambda x: np.cos(x) * np.exp(np.sin(x)),\n        },\n        {\n            \"N\": 4,\n            \"Ey_func\": lambda x: np.sin(x),\n            \"dEy_exact_func\": lambda x: np.cos(x),\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        N = case[\"N\"]\n        \n        # Create the periodic grid\n        x_grid = np.arange(N) * (2.0 * np.pi / N)\n\n        # Evaluate the function and its exact derivative on the grid\n        Ey_values = case[\"Ey_func\"](x_grid)\n        dEy_exact = case[\"dEy_exact_func\"](x_grid)\n        \n        # Compute the numerical derivative\n        dEy_numerical = compact_fourth_order_derivative(Ey_values, N)\n        \n        # Calculate the maximum absolute error\n        max_abs_error = np.max(np.abs(dEy_numerical - dEy_exact))\n        \n        results.append(max_abs_error)\n\n    # Format output as a comma-separated list of values in scientific notation\n    # with 12 significant digits.\n    formatted_results = [f\"{res:.11e}\" for res in results]\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n\n```", "id": "3307323"}]}