{"hands_on_practices": [{"introduction": "The Karhunen-Loève (KL) expansion provides a powerful theoretical framework for representing Gaussian processes. To truly grasp its mechanics, it is invaluable to work through a complete analytical derivation for a fundamental process. This practice guides you through deriving the KL expansion for standard Brownian motion, starting from the governing integral equation and transforming it into a solvable boundary value problem, thereby revealing the explicit eigenvalues and eigenfunctions that form the building blocks of this canonical process [@problem_id:3340706].", "problem": "Consider standard Brownian motion $B(t)$ on the interval $[0,1]$, defined as a centered Gaussian process with covariance function $K(s,t)=\\min(s,t)$. The Karhunen-Loève (KL) expansion of a zero-mean, square-integrable Gaussian process expresses the process as an infinite series with deterministic orthonormal functions and independent standard normal coefficients. Starting from the fundamental definition of the KL expansion via the spectral decomposition of the covariance operator, derive the orthonormal eigenfunctions and eigenvalues of the integral operator $T$ on $L^{2}([0,1])$ with kernel $K(s,t)=\\min(s,t)$, that is, solve for functions $\\phi(s)$ and scalars $\\lambda$ such that\n$$\n\\lambda\\,\\phi(s)=\\int_{0}^{1}K(s,t)\\,\\phi(t)\\,dt,\\quad s\\in[0,1].\n$$\nUse only well-tested facts and definitions: the covariance operator associated with $K$ is compact, self-adjoint, and positive, and its orthonormal eigenfunctions form a complete basis in $L^{2}([0,1])$; the KL expansion for a centered Gaussian process $X(t)$ with eigenpairs $\\{(\\lambda_{k},\\phi_{k})\\}$ is $X(t)=\\sum_{k=1}^{\\infty}\\sqrt{\\lambda_{k}}\\,Z_{k}\\,\\phi_{k}(t)$ where $\\{Z_{k}\\}$ are independent standard normal random variables. Derive the boundary conditions implied by the integral equation, solve the resulting ordinary differential equation, and normalize the eigenfunctions in $L^{2}([0,1])$. Then write down the KL expansion of $B(t)$ on $[0,1]$ explicitly as a single closed-form series in $t$ whose coefficients are independent standard normal random variables.\n\nExpress your final answer as one analytic expression for $B(t)$ (do not provide intermediate steps or separate formulas for eigenvalues/eigenfunctions). No rounding is required, and there are no physical units involved. State your final answer explicitly as a series in $t$.", "solution": "The problem asks for the Karhunen-Loève (KL) expansion of standard Brownian motion on the interval $[0,1]$. This requires finding the eigenvalues and orthonormal eigenfunctions of the integral operator associated with its covariance function.\n\n### Step 1: Extract Givens\n- Process: Standard Brownian motion $B(t)$ on the interval $[0,1]$.\n- Statistical Properties: $B(t)$ is a centered Gaussian process, meaning its mean is $E[B(t)] = 0$.\n- Covariance Function: $K(s,t) = \\text{cov}(B(s), B(t)) = \\min(s,t)$ for $s,t \\in [0,1]$.\n- Governing Equation: The eigenpairs $(\\lambda, \\phi)$ of the covariance operator satisfy the Fredholm integral equation of the second kind:\n$$ \\lambda\\,\\phi(s)=\\int_{0}^{1}K(s,t)\\,\\phi(t)\\,dt,\\quad s\\in[0,1] $$\n- Karhunen-Loève Expansion Formula: For a centered Gaussian process $X(t)$ with eigenpairs $\\{(\\lambda_{k},\\phi_{k})\\}$, the expansion is $X(t)=\\sum_{k=1}^{\\infty}\\sqrt{\\lambda_{k}}\\,Z_{k}\\,\\phi_{k}(t)$, where $\\{Z_{k}\\}$ are independent standard normal random variables.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientific Groundedness**: The problem is firmly rooted in the theory of stochastic processes, specifically the analysis of Gaussian processes. The concept of Brownian motion, its covariance function, the Karhunen-Loève expansion, and the associated integral equation are all standard, well-established topics in mathematics and physics.\n- **Well-Posedness**: The covariance kernel $K(s,t) = \\min(s,t)$ is continuous, symmetric, and positive definite. The associated integral operator is a Hilbert-Schmidt operator on $L^2([0,1])$, which guarantees that it is compact and self-adjoint. By Mercer's Theorem, there exists a countable set of positive eigenvalues and a corresponding complete orthonormal basis of continuous eigenfunctions. The problem is therefore well-posed and has a unique solution.\n- **Objectivity**: The problem is stated in precise mathematical language, free from any subjective or ambiguous terminology.\n- **Completeness and Consistency**: The problem statement provides all necessary information: the process, its domain, its covariance function, the integral equation to be solved, and the general form of the desired expansion. There are no contradictions.\n\n### Step 3: Verdict and Action\nThe problem is valid. It is a standard and well-posed problem in the theory of stochastic processes. I will proceed with the solution.\n\n### Derivation of the Eigenvalue Problem\nThe integral equation we must solve is:\n$$ \\lambda \\phi(s) = \\int_{0}^{1} \\min(s,t) \\phi(t) dt $$\nWe can split the integral at $s$ due to the definition of the minimum function:\n$$ \\lambda \\phi(s) = \\int_{0}^{s} t \\phi(t) dt + \\int_{s}^{1} s \\phi(t) dt $$\nThis equation holds for all $s \\in [0,1]$. We can convert this integral equation into an ordinary differential equation (ODE) by differentiating with respect to $s$. Using the Leibniz integral rule, $\\frac{d}{dx} \\int_{a(x)}^{b(x)} f(x,t) dt = f(x,b(x))b'(x) - f(x,a(x))a'(x) + \\int_{a(x)}^{b(x)} \\frac{\\partial f}{\\partial x} dt$, the differentiation of the first term gives $s\\phi(s)$. For the second term, we have $\\frac{d}{ds} (s \\int_s^1 \\phi(t) dt) = \\int_s^1 \\phi(t)dt + s(-\\phi(s))$.\n\nDifferentiating the equation for $\\lambda\\phi(s)$ with respect to $s$ yields:\n$$ \\lambda \\phi'(s) = s \\phi(s) + \\left( \\int_{s}^{1} \\phi(t) dt - s \\phi(s) \\right) = \\int_{s}^{1} \\phi(t) dt $$\nDifferentiating a second time with respect to $s$:\n$$ \\lambda \\phi''(s) = -\\phi(s) $$\nThis gives the second-order linear homogeneous ODE:\n$$ \\phi''(s) + \\frac{1}{\\lambda} \\phi(s) = 0 $$\n\n### Derivation of Boundary Conditions\nWe need two boundary conditions to solve this ODE.\nFirst, evaluate the original integral equation at $s=0$:\n$$ \\lambda \\phi(0) = \\int_{0}^{1} \\min(0,t)\\,\\phi(t)\\,dt = \\int_{0}^{1} 0 \\cdot \\phi(t) dt = 0 $$\nSince the eigenvalues $\\lambda$ of a positive definite operator must be positive (hence non-zero), we must have $\\phi(0) = 0$.\n\nSecond, evaluate the equation for the first derivative, $\\lambda \\phi'(s) = \\int_{s}^{1} \\phi(t) dt$, at $s=1$:\n$$ \\lambda \\phi'(1) = \\int_{1}^{1} \\phi(t) dt = 0 $$\nAgain, since $\\lambda \\neq 0$, we must have $\\phi'(1)=0$.\n\n### Solving the Boundary Value Problem\nWe solve the ODE $\\phi''(s) + \\frac{1}{\\lambda} \\phi(s) = 0$ subject to the boundary conditions $\\phi(0)=0$ and $\\phi'(1)=0$.\nLet $\\omega^2 = \\frac{1}{\\lambda}$. Since $\\lambda > 0$, $\\omega$ is real. The general solution to $\\phi''(s) + \\omega^2 \\phi(s) = 0$ is:\n$$ \\phi(s) = A \\sin(\\omega s) + C \\cos(\\omega s) $$\nApplying the first boundary condition, $\\phi(0)=0$:\n$$ \\phi(0) = A \\sin(0) + C \\cos(0) = C = 0 $$\nThe solution must be of the form $\\phi(s) = A \\sin(\\omega s)$.\n\nNow, apply the second boundary condition, $\\phi'(1)=0$. The derivative is $\\phi'(s) = A \\omega \\cos(\\omega s)$.\n$$ \\phi'(1) = A \\omega \\cos(\\omega) = 0 $$\nFor a non-trivial solution, we require $A \\neq 0$. Also $\\omega=1/\\sqrt{\\lambda} \\neq 0$. Therefore, we must have $\\cos(\\omega)=0$.\nThis condition is satisfied when $\\omega$ is an odd multiple of $\\pi/2$:\n$$ \\omega_k = \\frac{(2k-1)\\pi}{2} \\quad \\text{for } k = 1, 2, 3, \\ldots $$\nThe eigenvalues $\\lambda_k$ are then given by:\n$$ \\lambda_k = \\frac{1}{\\omega_k^2} = \\frac{1}{\\left(\\frac{(2k-1)\\pi}{2}\\right)^2} = \\frac{4}{(2k-1)^2\\pi^2} $$\nThe corresponding unnormalized eigenfunctions are $\\phi_k(s) = A_k \\sin(\\omega_k s) = A_k \\sin\\left(\\frac{(2k-1)\\pi s}{2}\\right)$.\n\n### Normalization of Eigenfunctions\nThe eigenfunctions must form an orthonormal set in $L^2([0,1])$, so we require $\\int_0^1 \\phi_k(s)^2 ds = 1$.\n$$ \\int_0^1 \\left(A_k \\sin\\left(\\frac{(2k-1)\\pi s}{2}\\right)\\right)^2 ds = 1 $$\n$$ A_k^2 \\int_0^1 \\sin^2\\left(\\frac{(2k-1)\\pi s}{2}\\right) ds = 1 $$\nUsing the identity $\\sin^2(\\theta) = \\frac{1-\\cos(2\\theta)}{2}$:\n$$ \\int_0^1 \\sin^2\\left(\\omega_k s\\right) ds = \\int_0^1 \\frac{1 - \\cos(2\\omega_k s)}{2} ds = \\frac{1}{2} \\left[s - \\frac{\\sin(2\\omega_k s)}{2\\omega_k}\\right]_0^1 $$\nSince $2\\omega_k = (2k-1)\\pi$, we have $\\sin(2\\omega_k s)|_{s=1} = \\sin((2k-1)\\pi) = 0$. The integral evaluates to:\n$$ \\frac{1}{2} \\left(1 - \\frac{\\sin((2k-1)\\pi)}{(2k-1)\\pi}\\right) - 0 = \\frac{1}{2} $$\nSubstituting this back into the normalization condition:\n$$ A_k^2 \\cdot \\frac{1}{2} = 1 \\implies A_k^2 = 2 $$\nChoosing the positive root, $A_k = \\sqrt{2}$. The orthonormal eigenfunctions are:\n$$ \\phi_k(s) = \\sqrt{2} \\sin\\left(\\frac{(2k-1)\\pi s}{2}\\right) $$\n\n### Constructing the Karhunen-Loève Expansion\nThe KL expansion for the centered Gaussian process $B(t)$ is given by $B(t) = \\sum_{k=1}^{\\infty} \\sqrt{\\lambda_k} Z_k \\phi_k(t)$, where $Z_k$ are i.i.d. standard normal random variables. We substitute the derived eigenvalues and eigenfunctions (using $t$ as the variable):\n- Eigenvalues: $\\lambda_k = \\frac{4}{(2k-1)^2\\pi^2}$\n- Eigenfunctions: $\\phi_k(t) = \\sqrt{2} \\sin\\left(\\frac{(2k-1)\\pi t}{2}\\right)$\nThe square root of the eigenvalues is $\\sqrt{\\lambda_k} = \\frac{2}{(2k-1)\\pi}$.\nCombining these results, the KL expansion for standard Brownian motion on $[0,1]$ is:\n$$ B(t) = \\sum_{k=1}^{\\infty} \\left( \\frac{2}{(2k-1)\\pi} \\right) Z_k \\left( \\sqrt{2} \\sin\\left(\\frac{(2k-1)\\pi t}{2}\\right) \\right) $$\n$$ B(t) = \\sum_{k=1}^{\\infty} \\frac{2\\sqrt{2}}{(2k-1)\\pi} Z_k \\sin\\left(\\frac{(2k-1)\\pi t}{2}\\right) $$\nThis expression represents the standard Brownian motion $B(t)$ as an infinite series of deterministic sinusoidal functions with random, uncorrelated Gaussian coefficients.", "answer": "$$\n\\boxed{B(t) = \\sum_{k=1}^{\\infty} \\frac{2\\sqrt{2}}{(2k-1)\\pi} Z_k \\sin\\left(\\frac{(2k-1)\\pi t}{2}\\right)}\n$$", "id": "3340706"}, {"introduction": "While analytical solutions to the KL eigenvalue problem are enlightening, they are only available for a few special covariance kernels. For most practical applications, we must turn to numerical methods. This exercise introduces the Galerkin method, a powerful technique for approximating the eigenpairs of the covariance operator by projecting the problem onto a finite-dimensional basis, allowing us to assess the convergence of the resulting Mercer expansion [@problem_id:3340711].", "problem": "You are given the zero-mean Gaussian process on the interval $[0,1]$ with covariance kernel $K(s,t)=\\exp(-|s-t|)$. Consider the associated covariance operator $T:L^2([0,1])\\to L^2([0,1])$ defined by $(Tf)(s)=\\int_0^1 K(s,t)f(t)\\,dt$. The Mercer expansion of $K$ expresses $K(s,t)$ as a sum of products of eigenfunctions of $T$ weighted by the corresponding eigenvalues. Your task is to construct a Galerkin approximation of this operator using a finite-dimensional subspace, compute the leading eigenpairs, and assess the convergence of the truncated approximate Mercer expansion in a quantitative way.\n\nStart from the following foundational base:\n- The covariance operator $T$ is compact, self-adjoint, and positive on $L^2([0,1])$ for continuous, symmetric, positive semidefinite $K$.\n- The Mercer expansion states that $K(s,t)=\\sum_{k=1}^\\infty \\lambda_k \\psi_k(s)\\psi_k(t)$ where $\\{\\lambda_k\\}_{k\\ge 1}$ are the nonnegative eigenvalues of $T$ and $\\{\\psi_k\\}_{k\\ge 1}$ is an orthonormal basis of $L^2([0,1])$ comprised of eigenfunctions of $T$.\n\nImplement the Galerkin method using the following specifications:\n- Use the orthonormal Fourier cosine basis on $[0,1]$ defined by $\\phi_0(s)=1$ and $\\phi_k(s)=\\sqrt{2}\\cos(\\pi k s)$ for $k\\ge 1$. The subspace of dimension $m$ is $V_m=\\text{span}\\{\\phi_0,\\phi_1,\\ldots,\\phi_{m-1}\\}$.\n- Approximate the required integrals by an $n$-point Gauss–Legendre quadrature on $[0,1]$. Let $\\{x_i,w_i\\}_{i=1}^n$ denote the quadrature nodes and weights on $[0,1]$.\n- Assemble the Galerkin matrices\n$$A_{ij}=\\int_0^1\\int_0^1 \\phi_i(s)K(s,t)\\phi_j(t)\\,ds\\,dt,\\quad M_{ij}=\\int_0^1 \\phi_i(s)\\phi_j(s)\\,ds,$$\nusing the quadrature approximation\n$$A_{ij}\\approx \\sum_{p=1}^n\\sum_{q=1}^n w_p w_q\\, \\phi_i(x_p)\\,K(x_p,x_q)\\,\\phi_j(x_q),\\quad M_{ij}\\approx \\sum_{p=1}^n w_p\\,\\phi_i(x_p)\\,\\phi_j(x_p).$$\n- Compute the generalized symmetric positive definite eigenproblem $A \\mathbf{v}=\\lambda M \\mathbf{v}$ with eigenvalues sorted in non-increasing order and $M$-orthonormal eigenvectors. Form the approximate eigenfunctions $\\psi_k(s)\\approx \\sum_{j=0}^{m-1} v_{j}^{(k)}\\phi_j(s)$.\n- For a given truncation rank $r$, form the truncated approximate Mercer expansion\n$$K_r(s,t)=\\sum_{k=1}^{r}\\lambda_k\\,\\psi_k(s)\\,\\psi_k(t),$$\nand measure the approximation error by the weighted $L^2$ norm on $[0,1]\\times[0,1]$:\n$$\\text{err}(m,n,r)=\\left(\\int_0^1\\int_0^1 \\left(K(s,t)-K_r(s,t)\\right)^2\\,ds\\,dt\\right)^{1/2},$$\napproximated by quadrature as\n$$\\text{err}(m,n,r)\\approx \\left(\\sum_{p=1}^n\\sum_{q=1}^n w_p w_q \\left(K(x_p,x_q)-K_r(x_p,x_q)\\right)^2\\right)^{1/2}.$$\n\nImplementation constraints:\n- The integrals must be approximated using Gauss–Legendre quadrature on $[0,1]$ as described above.\n- If $r=0$, define $K_r$ to be identically $0$ and compute the error accordingly.\n- If $r$ exceeds the dimension of $V_m$, truncate $r$ to the number of available eigenpairs.\n- All angles, when present, must be in radians. There are no physical units involved.\n\nTest suite:\nCompute $\\text{err}(m,n,r)$ for each of the following parameter sets $(m,n,r)$:\n- $(5,64,3)$\n- $(10,128,6)$\n- $(20,256,12)$\n- $(5,64,0)$\n- $(5,64,20)$\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test suite above. For example, the output must look like $[e_1,e_2,e_3,e_4,e_5]$ where each $e_k$ is a floating-point number representing $\\text{err}(m,n,r)$ for the corresponding test case.", "solution": "The problem requires the numerical approximation of the Karhunen-Loève expansion for a zero-mean Gaussian process on the interval $[0,1]$ with a specified covariance kernel, $K(s,t)=\\exp(-|s-t|)$. This is accomplished using the Galerkin method, a standard technique for converting an infinite-dimensional operator problem into a finite-dimensional matrix problem. The accuracy of the resulting approximation is then assessed.\n\nThe foundation of this problem is Mercer's theorem, which applies to the covariance operator $T:L^2([0,1])\\to L^2([0,1])$ defined by $(Tf)(s)=\\int_0^1 K(s,t)f(t)\\,dt$. Since the kernel $K(s,t)$ is continuous, symmetric, and positive semidefinite, the operator $T$ is compact, self-adjoint, and positive. Mercer's theorem states that the kernel can be represented by an absolutely and uniformly convergent series:\n$$\nK(s,t)=\\sum_{k=1}^\\infty \\lambda_k \\psi_k(s)\\psi_k(t)\n$$\nwhere $\\{\\lambda_k\\}_{k\\ge 1}$ are the non-negative eigenvalues of $T$ and $\\{\\psi_k\\}_{k\\ge 1}$ is a corresponding orthonormal basis of $L^2([0,1])$ consisting of eigenfunctions. The eigenpairs $(\\lambda_k, \\psi_k)$ are solutions to the integral eigenvalue equation:\n$$\n(T\\psi_k)(s) = \\int_0^1 K(s,t)\\psi_k(t)\\,dt = \\lambda_k \\psi_k(s)\n$$\n\nThe Galerkin method approximates the solution by restricting the problem to a finite-dimensional subspace $V_m \\subset L^2([0,1])$. We seek an approximate eigenfunction $\\psi^{(m)}(s) \\in V_m$, which can be expressed as a linear combination of basis functions spanning the subspace:\n$$\n\\psi^{(m)}(s) = \\sum_{j=0}^{m-1} v_j \\phi_j(s)\n$$\nThe basis for $V_m$ is specified as the first $m$ elements of the orthonormal Fourier cosine basis on $[0,1]$:\n$$\n\\phi_0(s)=1, \\quad \\phi_k(s)=\\sqrt{2}\\cos(\\pi k s) \\text{ for } k\\ge 1\n$$\nSubstituting the approximation $\\psi^{(m)}$ into the eigenvalue equation and projecting the result onto each basis function $\\phi_i(s)$ for $i=0, \\dots, m-1$ yields the weak form of the problem:\n$$\n\\int_0^1 \\phi_i(s) \\left( \\int_0^1 K(s,t) \\sum_{j=0}^{m-1} v_j \\phi_j(t) \\,dt \\right) \\,ds = \\lambda \\int_0^1 \\phi_i(s) \\left( \\sum_{j=0}^{m-1} v_j \\phi_j(s) \\right) \\,ds\n$$\nBy linearity of the integral, this can be rewritten as:\n$$\n\\sum_{j=0}^{m-1} \\left(\\int_0^1\\int_0^1 \\phi_i(s)K(s,t)\\phi_j(t)\\,ds\\,dt\\right) v_j = \\lambda \\sum_{j=0}^{m-1} \\left(\\int_0^1 \\phi_i(s)\\phi_j(s)\\,ds\\right) v_j\n$$\nThis is a generalized matrix eigenvalue problem of the form $A\\mathbf{v} = \\lambda M\\mathbf{v}$, where $\\mathbf{v} = [v_0, \\dots, v_{m-1}]^T$ is the vector of coefficients. The matrices $A$ and $M$ have elements defined by:\n$$\nA_{ij}=\\int_0^1\\int_0^1 \\phi_i(s)K(s,t)\\phi_j(t)\\,ds\\,dt, \\quad M_{ij}=\\int_0^1 \\phi_i(s)\\phi_j(s)\\,ds\n$$\nSince the chosen basis $\\{\\phi_j\\}$ is orthonormal, the exact mass matrix $M$ would be the identity matrix, $M_{ij} = \\delta_{ij}$. However, the problem specifies that these integrals are to be approximated numerically using an $n$-point Gauss-Legendre quadrature on $[0,1]$. Let $\\{x_p, w_p\\}_{p=1}^n$ be the quadrature nodes and weights. The matrices are then assembled as:\n$$\nA_{ij}\\approx \\sum_{p=1}^n\\sum_{q=1}^n w_p w_q\\, \\phi_i(x_p)\\,K(x_p,x_q)\\,\\phi_j(x_q)\n$$\n$$\nM_{ij}\\approx \\sum_{p=1}^n w_p\\,\\phi_i(x_p)\\,\\phi_j(x_p)\n$$\nThe resulting numerical mass matrix $M$ will be a close approximation of the identity matrix, but using it in the generalized eigenproblem $A\\mathbf{v} = \\lambda M\\mathbf{v}$ corrects for the error introduced by the quadrature in the inner product.\n\nSolving this $m \\times m$ generalized symmetric eigenproblem yields $m$ approximate eigenpairs $(\\lambda_k, \\mathbf{v}^{(k)})$. The eigenvalues are sorted in descending order, $\\lambda_1 \\ge \\lambda_2 \\ge \\dots \\ge \\lambda_m \\ge 0$. The eigenvectors $\\mathbf{v}^{(k)}$ are normalized to be $M$-orthonormal, i.e., $(\\mathbf{v}^{(k)})^T M \\mathbf{v}^{(l)} = \\delta_{kl}$. Each eigenvector provides the coefficients for an approximate eigenfunction: $\\psi_k(s) \\approx \\sum_{j=0}^{m-1} v_j^{(k)} \\phi_j(s)$.\n\nWith these approximate eigenpairs, we construct the truncated Mercer expansion of rank $r$:\n$$\nK_r(s,t)=\\sum_{k=1}^{r}\\lambda_k\\,\\psi_k(s)\\,\\psi_k(t)\n$$\nwhere $r$ is truncated to $m$ if $r > m$, and if $r=0$, $K_r(s,t) \\equiv 0$. The approximation error is quantified by the weighted root-mean-square error over the domain $[0,1] \\times [0,1]$:\n$$\n\\text{err}(m,n,r)=\\left(\\int_0^1\\int_0^1 \\left(K(s,t)-K_r(s,t)\\right)^2\\,ds\\,dt\\right)^{1/2}\n$$\nThis integral is also computed using the $n$-point quadrature:\n$$\n\\text{err}(m,n,r)\\approx \\left(\\sum_{p=1}^n\\sum_{q=1}^n w_p w_q \\left(K(x_p,x_q)-K_r(x_p,x_q)\\right)^2\\right)^{1/2}\n$$\nThe value of $K_r(s,t)$ at the quadrature nodes $(x_p, x_q)$ is calculated using the computed eigenvalues and eigenfunctions evaluated at those nodes. The entire procedure is repeated for each parameter set $(m,n,r)$ provided in the test suite.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import roots_legendre\nfrom scipy.linalg import eigh\n\ndef solve():\n    \"\"\"\n    Computes the approximation error of the truncated Mercer expansion for\n    a given covariance kernel using the Galerkin method.\n    \"\"\"\n    test_cases = [\n        (5, 64, 3),\n        (10, 128, 6),\n        (20, 256, 12),\n        (5, 64, 0),\n        (5, 64, 20),\n    ]\n\n    results = []\n    for m, n, r in test_cases:\n        # Step 1: Gauss-Legendre quadrature nodes and weights for [0, 1]\n        lg_nodes, lg_weights = roots_legendre(n)\n        nodes = 0.5 * (lg_nodes + 1.0)\n        weights = 0.5 * lg_weights\n\n        # Step 2: Evaluate basis functions at quadrature nodes\n        # Phi_mat is an n x m matrix where Phi_mat[p, j] = phi_j(x_p)\n        Phi_mat = np.zeros((n, m))\n        Phi_mat[:, 0] = 1.0\n        for j in range(1, m):\n            Phi_mat[:, j] = np.sqrt(2.0) * np.cos(j * np.pi * nodes)\n\n        # Step 3: Assemble the kernel matrix K(s,t) at quadrature nodes\n        # K_mat is an n x n matrix where K_mat[p, q] = exp(-|x_p - x_q|)\n        s_nodes, t_nodes = np.meshgrid(nodes, nodes)\n        K_mat = np.exp(-np.abs(s_nodes - t_nodes))\n\n        # Step 4: Assemble Galerkin matrices A and M using numerical quadrature\n        # A_ij = sum_{p,q} w_p w_q phi_i(x_p) K(x_p,x_q) phi_j(x_q)\n        # M_ij = sum_p w_p phi_i(x_p) phi_j(x_p)\n        A = np.einsum('p,pi,pq,q,qj->ij', weights, Phi_mat, K_mat, weights, Phi_mat)\n        M = np.einsum('p,pi,pj->ij', weights, Phi_mat, Phi_mat)\n\n        # Step 5: Solve the generalized eigenvalue problem A v = lambda M v\n        # eigh returns eigenvalues in ascending order, so we reverse them.\n        eigvals, eigvecs = eigh(A, M)\n        eigvals = eigvals[::-1]\n        eigvecs = eigvecs[:, ::-1]\n\n        # Step 6: Define the effective rank of truncation\n        # If r > m, we can only use the m available eigenpairs.\n        r_eff = min(r, m)\n\n        # Step 7: Construct the truncated kernel approximation K_r\n        if r_eff == 0:\n            K_r_mat = np.zeros((n, n))\n        else:\n            # Select top r_eff eigenpairs\n            top_eigvals = eigvals[:r_eff]\n            top_eigvecs = eigvecs[:, :r_eff]\n\n            # Approximate eigenfunctions evaluated at nodes: Psi = Phi * V\n            # Psi_approx is an n x r_eff matrix\n            Psi_approx = Phi_mat @ top_eigvecs\n            \n            # K_r(s,t) = sum_{k=1..r} lambda_k psi_k(s) psi_k(t)\n            # At nodes: K_r_mat = Psi * diag(lambda) * Psi^T\n            K_r_mat = Psi_approx @ np.diag(top_eigvals) @ Psi_approx.T\n            # A more direct einsum for K_r_mat[p,q] = sum_k lambda_k Psi[p,k] Psi[q,k]\n            # K_r_mat = np.einsum('pk,k,qk->pq', Psi_approx, top_eigvals, Psi_approx)\n        \n        # Step 8: Calculate the approximation error\n        # err^2 = sum_{p,q} w_p w_q (K(p,q) - K_r(p,q))^2\n        diff_mat = K_mat - K_r_mat\n        error_sq = np.einsum('p,q,pq,pq->', weights, weights, diff_mat, diff_mat)\n        error = np.sqrt(error_sq)\n        \n        results.append(error)\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(f'{res:.12f}' for res in results)}]\")\n\nsolve()\n\n```", "id": "3340711"}, {"introduction": "Building upon the theoretical and numerical foundations, this practice focuses on the end-to-end design of a practical simulation pipeline for a Gaussian Process using its discrete KL expansion. You will implement a simulation for a GP with a Matérn kernel, a widely used model in spatial statistics. The exercise emphasizes the critical aspects of computational cost and memory usage that arise when dealing with large-scale problems, and how to quantify the quality of the resulting truncated approximation [@problem_id:3340755].", "problem": "Consider a mean-zero Gaussian Process (GP) defined on the compact interval $[0,1]$ with covariance function given by a Matérn kernel. Let $X(t)$ denote the GP and let $k(r)$ denote its covariance function evaluated at separation $r = |t - s|$. The Matérn kernel is defined by\n$$\nk_{\\text{Matérn}}(r; \\nu, \\ell, \\sigma^2) = \\sigma^2 \\frac{2^{1-\\nu}}{\\Gamma(\\nu)} \\left( \\frac{\\sqrt{2\\nu}\\, r}{\\ell} \\right)^\\nu K_\\nu\\!\\left( \\frac{\\sqrt{2\\nu}\\, r}{\\ell} \\right),\n$$\nwhere $\\nu > 0$ is the smoothness parameter, $\\ell > 0$ is the length-scale, $\\sigma^2 > 0$ is the marginal variance, $\\Gamma(\\cdot)$ is the gamma function, and $K_\\nu(\\cdot)$ is the modified Bessel function of the second kind. On the diagonal, one sets $k_{\\text{Matérn}}(0;\\nu,\\ell,\\sigma^2) = \\sigma^2$ by continuity.\n\nBy the Karhunen-Loève (KL) expansion, under suitable regularity conditions, a mean-zero GP on a compact domain with covariance operator $C$ can be represented as\n$$\nX(t) \\stackrel{d}{=} \\sum_{i=1}^{\\infty} \\sqrt{\\lambda_i}\\, \\xi_i \\, \\phi_i(t),\n$$\nwhere $\\{(\\lambda_i, \\phi_i)\\}_{i\\ge1}$ are the eigenpairs of the covariance operator $C$ (with $\\lambda_i \\ge 0$), and $\\{\\xi_i\\}_{i\\ge1}$ are independent and identically distributed standard normal random variables. Truncating the expansion to $m$ terms yields an approximation\n$$\nX_m(t) = \\sum_{i=1}^{m} \\sqrt{\\lambda_i}\\, \\xi_i \\, \\phi_i(t).\n$$\n\nYou are tasked with designing and implementing a simulation pipeline that approximates one realization of $X_m(t)$ over a uniform grid of $n=2000$ points in $[0,1]$, using the KL expansion induced by the discrete covariance matrix. The pipeline must adhere to the following design, grounded in first principles:\n\n1. Construct a uniform grid $\\{t_j\\}_{j=1}^{n}$ on $[0,1]$ and form the $n \\times n$ covariance matrix $K$ with entries $K_{ij} = k_{\\text{Matérn}}(|t_i - t_j|;\\nu,\\ell,\\sigma^2)$.\n\n2. Compute the leading $m$ eigenpairs $\\{(\\lambda_i, v_i)\\}_{i=1}^{m}$ of the symmetric positive definite matrix $K$ using an iterative method appropriate for large matrices. Interpret $v_i$ as the discrete counterpart to the eigenfunction $\\phi_i(t)$ sampled at the grid points.\n\n3. Draw $\\xi_i \\sim \\mathcal{N}(0,1)$ independently for $i=1,\\dots,m$ and synthesize one truncated KL sample $y \\in \\mathbb{R}^{n}$ via\n$$\ny = \\sum_{i=1}^{m} \\sqrt{\\lambda_i}\\, \\xi_i \\, v_i.\n$$\n\n4. Provide a complexity estimate for the pipeline using a unit-cost model rooted in algebraic operations, where the following approximations are used:\n   - Building the dense covariance matrix costs on the order of $n^2$ pairwise computations.\n   - The iterative eigen-solver requires $s$ matrix-vector multiplications, each costing on the order of $n^2$ for a dense matrix. Assume $s = 30$.\n   - Sampling and synthesis of $y$ costs on the order of $n m$.\n\n   Under this model, report a single scalar count\n   $$\n   C_{\\text{ops}} = 2 n^2 + s n^2 + n m,\n   $$\n   where the factor $2$ accounts for separate contributions from distance evaluation and kernel evaluation in building $K$.\n\n5. Provide a memory footprint estimate for the pipeline expressed as a peak memory usage in megabytes (MB), using the following principled approximation:\n   - During covariance construction, assume three dense $n \\times n$ arrays are simultaneously present in memory (for the distance matrix, an intermediate scaled argument, and the covariance matrix), each stored in 64-bit floating point format.\n   - Additionally, account for storing the $m$ leading eigenvectors (an $n \\times m$ dense array), the $m$ leading eigenvalues (a length-$m$ array), the grid points (length-$n$), and the synthesized sample (length-$n$).\n   - Using $8$ bytes per 64-bit floating point number, the peak memory in bytes is\n     $$\n     M_{\\text{bytes}} = 8 \\left( 3 n^2 + n m + m + 2n \\right),\n     $$\n     and the peak memory in megabytes is $M_{\\text{MB}} = M_{\\text{bytes}} / 2^{20}$.\n\n6. As a measure of KL truncation quality, report the explained variance ratio\n   $$\n   R = \\frac{\\sum_{i=1}^{m} \\lambda_i}{\\operatorname{trace}(K)},\n   $$\n   where $\\operatorname{trace}(K) = \\sum_{j=1}^{n} K_{jj} = n \\sigma^2$ for the Matérn kernel as specified.\n\nYour program must implement the above pipeline and produce results for the following test suite of parameter sets $(\\nu,\\ell,\\sigma^2,m)$:\n- Test 1 (happy path): $(\\nu,\\ell,\\sigma^2,m) = (1.5, 0.2, 1.0, 25)$.\n- Test 2 (boundary case in truncation): $(\\nu,\\ell,\\sigma^2,m) = (0.5, 0.2, 1.0, 1)$.\n- Test 3 (short length-scale edge case): $(\\nu,\\ell,\\sigma^2,m) = (1.5, 0.05, 1.0, 25)$.\n- Test 4 (smoother process): $(\\nu,\\ell,\\sigma^2,m) = (2.5, 0.5, 1.0, 50)$.\n\nFor each test, compute and return the triple $(M_{\\text{MB}}, C_{\\text{ops}}, R)$ as defined above. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where the triples are flattened in test order, namely\n$$\n\\left[ M_{\\text{MB}}^{(1)}, C_{\\text{ops}}^{(1)}, R^{(1)}, M_{\\text{MB}}^{(2)}, C_{\\text{ops}}^{(2)}, R^{(2)}, M_{\\text{MB}}^{(3)}, C_{\\text{ops}}^{(3)}, R^{(3)}, M_{\\text{MB}}^{(4)}, C_{\\text{ops}}^{(4)}, R^{(4)} \\right].\n$$\nAll numeric outputs must be represented as real numbers (floating-point). No physical units are involved; the memory must be expressed in megabytes as per the formula above. Angles do not appear, and no percentages are required since the explained variance ratio is a decimal between $0$ and $1$ inclusive.\n\nYour implementation must be a complete, runnable program in a modern programming language and must not require any user input, external files, or network access.", "solution": "The problem statement has been critically validated and is deemed valid. It presents a scientifically sound, well-posed, and complete task rooted in the established principles of stochastic processes, specifically Gaussian Processes (GPs) and their representation via the Karhunen-Loève (KL) expansion. All parameters, constants, and formulas required for the solution are explicitly and unambiguously defined. The task is a standard exercise in scientific computing and computational statistics.\n\nThe solution involves implementing the specified simulation and analysis pipeline. The methodology is broken down into the steps defined in the problem statement.\n\n**1. Covariance Matrix Construction**\n\nThe foundation of the simulation is the covariance structure of the GP, which is discretized on a grid. A uniform grid $\\{t_j\\}_{j=1}^{n}$ is constructed on the interval $[0,1]$ with $n=2000$ points. The grid points are defined as $t_j = (j-1)/(n-1)$ for $j=1, \\dots, n$.\n\nFrom this grid, an $n \\times n$ distance matrix $D$ is formed, where its entries are $D_{ij} = |t_i - t_j|$.\n\nThe covariance matrix $K$ is then constructed by applying the Matérn kernel to each entry of the distance matrix: $K_{ij} = k_{\\text{Matérn}}(D_{ij}; \\nu, \\ell, \\sigma^2)$. The Matérn kernel is given by:\n$$\nk_{\\text{Matérn}}(r; \\nu, \\ell, \\sigma^2) = \\sigma^2 \\frac{2^{1-\\nu}}{\\Gamma(\\nu)} \\left( \\frac{\\sqrt{2\\nu}\\, r}{\\ell} \\right)^\\nu K_\\nu\\!\\left( \\frac{\\sqrt{2\\nu}\\, r}{\\ell} \\right)\n$$\nwhere $r$ is the separation, $\\nu > 0$ is the smoothness parameter, $\\ell > 0$ is the length-scale, and $\\sigma^2 > 0$ is the marginal variance. $\\Gamma(\\cdot)$ is the gamma function and $K_\\nu(\\cdot)$ is the modified Bessel function of the second kind. For the case where the argument $r=0$, the kernel's value is taken to be its limit, $k_{\\text{Matérn}}(0) = \\sigma^2$. This special handling is necessary because the formula involves terms like $r^\\nu$ and $K_\\nu(Ar)$ which can lead to indeterminate forms like $0 \\times \\infty$ numerically. The resulting matrix $K$ is symmetric and positive definite.\n\n**2. Eigendecomposition of the Covariance Matrix**\n\nThe continuous KL expansion involves the eigenpairs of the covariance operator. In this discrete setting, we find the eigenpairs of the covariance matrix $K$. The problem asks for the $m$ leading eigenpairs, which correspond to the $m$ largest eigenvalues. These eigenpairs $\\{(\\lambda_i, v_i)\\}_{i=1}^{m}$ are computed for the symmetric matrix $K$. The eigenvalues $\\lambda_i$ are real and positive, and the eigenvectors $v_i \\in \\mathbb{R}^n$ are orthonormal. The vector $v_i$ is the discrete analogue of the eigenfunction $\\phi_i(t)$ evaluated at the grid points $\\{t_j\\}$.\n\nFor a large dense matrix of size $n=2000$, a full eigendecomposition would be computationally expensive ($O(n^3)$). Since only the top $m$ eigenpairs are required ($m \\ll n$), an iterative method is appropriate, as specified. Algorithms like the Lanczos method are efficient for this purpose, and we will use a library implementation of such a solver.\n\n**3. Synthesis of a GP Realization**\n\nThe truncated KL expansion provides a method to synthesize an approximate sample from the GP. A single realization, represented by the vector $y \\in \\mathbb{R}^n$, is synthesized by the formula:\n$$\ny = \\sum_{i=1}^{m} \\sqrt{\\lambda_i}\\, \\xi_i \\, v_i\n$$\nHere, $\\{\\xi_i\\}_{i=1}^{m}$ are independent random variables drawn from a standard normal distribution, $\\mathcal{N}(0,1)$. While this step is conceptually part of the pipeline, the specific random sample $y$ is not required for the output metrics, which are deterministic.\n\n**4. Computational Complexity Estimation**\n\nThe problem provides a unit-cost model to estimate the number of algebraic operations. The total cost, $C_{\\text{ops}}$, is given by the sum of costs for three main stages:\n$$\nC_{\\text{ops}} = 2 n^2 + s n^2 + n m\n$$\nwhere:\n- $2n^2$ accounts for constructing the $n \\times n$ distance matrix and then the $n \\times n$ covariance matrix.\n- $sn^2$ models the cost of the iterative eigensolver, assuming it performs $s=30$ matrix-vector multiplications with the dense $n \\times n$ matrix $K$.\n- $nm$ represents the cost of synthesizing the sample vector $y$ from $m$ eigenvectors of length $n$.\n\nThis formula will be evaluated for each test case using $n=2000$, $s=30$, and the specified value of $m$.\n\n**5. Memory Footprint Estimation**\n\nA model is provided to estimate the peak memory usage in megabytes (MB). The memory in bytes, $M_{\\text{bytes}}$, is:\n$$\nM_{\\text{bytes}} = 8 \\left( 3 n^2 + n m + m + 2n \\right)\n$$\nThe terms correspond to storage for:\n- Three $n \\times n$ arrays (e.g., distances, kernel arguments, final covariance values) used during the construction of $K$, stored as $64$-bit floats ($8$ bytes). This is $3n^2$.\n- The $m$ computed eigenvectors, forming an $n \\times m$ array: $nm$.\n- The $m$ computed eigenvalues: $m$.\n- The grid point array ($n$) and the final synthesized sample array ($n$): $2n$.\n\nThe final value in megabytes is calculated as $M_{\\text{MB}} = M_{\\text{bytes}} / 2^{20}$. This formula will be evaluated for each test case.\n\n**6. Explained Variance Ratio**\n\nTo assess the quality of the $m$-term KL truncation, the explained variance ratio, $R$, is computed. It is the fraction of the total variance captured by the leading $m$ components:\n$$\nR = \\frac{\\sum_{i=1}^{m} \\lambda_i}{\\operatorname{trace}(K)}\n$$\nThe trace of the covariance matrix, $\\operatorname{trace}(K) = \\sum_{j=1}^{n} K_{jj}$, represents the total variance in the discretized process. For the Matérn kernel, the diagonal elements are constant, $K_{jj} = k_{\\text{Matérn}}(0) = \\sigma^2$. Therefore, the trace simplifies to $\\operatorname{trace}(K) = n \\sigma^2$. The numerator is the sum of the $m$ largest eigenvalues computed in Step 2.\n$$\nR = \\frac{\\sum_{i=1}^{m} \\lambda_i}{n \\sigma^2}\n$$\nThis quantity will be computed for each set of parameters.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import gamma, kv\nfrom scipy.sparse.linalg import eigsh\n\ndef solve():\n    \"\"\"\n    Implements the GP simulation pipeline and computes specified metrics.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (nu, ell, sigma2, m)\n        (1.5, 0.2, 1.0, 25),  # Test 1\n        (0.5, 0.2, 1.0, 1),   # Test 2\n        (1.5, 0.05, 1.0, 25), # Test 3\n        (2.5, 0.5, 1.0, 50),  # Test 4\n    ]\n\n    # Global parameters\n    n = 2000\n    s = 30\n\n    results = []\n\n    def matern_kernel(r, nu, ell, sigma2):\n        \"\"\"\n        Computes the Matérn kernel.\n        \n        Args:\n            r (np.ndarray): Separation distances.\n            nu (float): Smoothness parameter.\n            ell (float): Length-scale parameter.\n            sigma2 (float): Marginal variance.\n            \n        Returns:\n            np.ndarray: Covariance values.\n        \"\"\"\n        # The formula is undefined for r=0, but the limit is sigma2.\n        # We need to handle this to avoid 0 * inf = nan.\n        r_is_zero = (r == 0)\n        \n        # To avoid division by zero or warnings, we work with a \"safe\" r.\n        # The values for r=0 will be overwritten later.\n        r_safe = np.where(r_is_zero, 1.0, r)\n        \n        # Calculate the argument for the Bessel function\n        arg = np.sqrt(2 * nu) * r_safe / ell\n        \n        # Matern formula components\n        term1 = sigma2 * (2**(1 - nu) / gamma(nu))\n        term2 = arg**nu\n        term3 = kv(nu, arg)\n        \n        # Combine terms for r > 0\n        k = term1 * term2 * term3\n        \n        # Apply the known limit for r = 0\n        if k.ndim > 0:\n            k[r_is_zero] = sigma2\n        elif r_is_zero:\n            k = sigma2\n            \n        return k\n\n    for case in test_cases:\n        nu, ell, sigma2, m = case\n\n        # 4. Complexity estimate\n        c_ops = float(2 * n**2 + s * n**2 + n * m)\n\n        # 5. Memory footprint estimate\n        m_bytes = 8 * (3 * n**2 + n * m + m + 2 * n)\n        m_mb = float(m_bytes / (2**20))\n\n        # 1. Construct covariance matrix\n        t = np.linspace(0, 1, n)\n        # Create the distance matrix using broadcasting\n        dist_matrix = np.abs(t[:, None] - t)\n        \n        K = matern_kernel(dist_matrix, nu, ell, sigma2)\n\n        # 2. Compute leading m eigenpairs\n        # eigsh is suitable for large, symmetric matrices.\n        # 'LA' specifies to find the largest (Largest Algebraic) eigenvalues.\n        # The eigenvalues are returned in ascending order, so we take the top m.\n        if m > 0:\n            eigenvalues, _ = eigsh(K, k=m, which='LA')\n        else: # Handle m=0 case, though not in test suite.\n            eigenvalues = np.array([])\n\n        # 6. Explained variance ratio\n        # Denominator is trace(K) = n * sigma^2\n        trace_K = n * sigma2\n        if trace_K > 0:\n            r_explained_variance = float(np.sum(eigenvalues) / trace_K)\n        else:\n            r_explained_variance = 0.0 # Or nan, depending on convention. 0.0 is safe.\n\n        results.extend([m_mb, c_ops, r_explained_variance])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{val:.6f}' for val in results)}]\")\n\nsolve()\n```", "id": "3340755"}]}