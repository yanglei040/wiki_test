{"hands_on_practices": [{"introduction": "Moving from theory to practice begins with understanding the mechanics of a single algorithmic step. This exercise provides a foundational walkthrough of an SVGD update, forcing a direct engagement with how the particles are moved by the interaction of the target's score function and the kernel's repulsive force. By performing this calculation from first principles for a simple two-particle system, you will build a concrete intuition for the algorithm's core engine. [@problem_id:3348310]", "problem": "Consider Stein Variational Gradient Descent (SVGD), which performs functional gradient descent on the Kullback–Leibler divergence by transporting a particle distribution via a velocity field in a Reproducing Kernel Hilbert Space (RKHS). In one dimension, the Stein operator for a differentiable target density $p(x)$ acting on a scalar test function $f$ is $T_{p} f(x) = f^{\\prime}(x) + f(x)\\,\\nabla_{x} \\ln p(x)$. Let the target be the standard normal density $p(x) = \\mathcal{N}(0,1)$, and let the RKHS be induced by the Radial Basis Function (RBF) kernel with bandwidth parameter $h > 0$ given by $k_{h}(x,x^{\\prime}) = \\exp\\!\\big(-\\frac{(x-x^{\\prime})^{2}}{2 h^{2}}\\big)$. You are given two initial particles at locations $x_{1}^{(0)} = -1$ and $x_{2}^{(0)} = 2$. Using the SVGD framework starting from the Stein operator definition and the RKHS representer property (without assuming any pre-stated closed-form update), derive the empirical SVGD particle update for a single iteration with step size $\\epsilon > 0$, and then compute the updated particle locations $x_{1}^{(1)}$ and $x_{2}^{(1)}$ explicitly as analytic functions of $h$ and $\\epsilon$. Express your final result in exact closed form (no rounding). The final answer must be a single row vector containing $x_{1}^{(1)}$ and $x_{2}^{(1)}$ as functions of $h$ and $\\epsilon$.", "solution": "The user has asked for a derivation of the Stein Variational Gradient Descent (SVGD) update rule and its application to a specific two-particle system.\n\n### Step 1: Problem Validation\nThe problem statement is validated against the specified criteria.\n\n- **Givens**:\n    -   Method: Stein Variational Gradient Descent (SVGD).\n    -   Stein operator ($1D$): $T_{p} f(x) = f^{\\prime}(x) + f(x)\\,\\nabla_{x} \\ln p(x)$.\n    -   Target density: $p(x) = \\mathcal{N}(0,1)$, the standard normal distribution.\n    -   Reproducing Kernel Hilbert Space (RKHS): Induced by the Radial Basis Function (RBF) kernel $k_{h}(x,x^{\\prime}) = \\exp(-\\frac{(x-x^{\\prime})^{2}}{2 h^{2}})$ with bandwidth $h > 0$.\n    -   Initial particle locations: $x_{1}^{(0)} = -1$ and $x_{2}^{(0)} = 2$.\n    -   Number of particles: $N=2$.\n    -   Iteration step size: $\\epsilon > 0$.\n-   **Validation**:\n    -   **Scientifically Grounded:** The problem uses standard, well-established concepts from Bayesian inference, machine learning, and functional analysis (SVGD, KL divergence, RKHS, Stein's method). It is scientifically sound.\n    -   **Well-Posed:** The problem provides all necessary information (target density, kernel, initial conditions, step size) to compute a unique, single-step update for the particle locations.\n    -   **Objective:** The problem is phrased in precise, technical language with no subjective or ambiguous elements.\n    -   The problem is self-contained, consistent, and feasible.\n\n-   **Verdict**: The problem is valid.\n\n### Step 2: Derivation of the SVGD Update Rule\n\nThe objective of SVGD is to transport an initial particle distribution $q_0$ to a target distribution $p$ by applying a velocity field $\\phi(x)$ to the particles. The particles are updated as $x' = x + \\epsilon \\phi(x)$. The optimal velocity field $\\phi(x)$ is the one that maximally decreases the Kullback–Leibler (KL) divergence, $KL(q_k || p)$, at each step. This corresponds to performing functional gradient descent on the KL divergence in an RKHS $\\mathcal{H}$.\n\nThe directional derivative of the KL divergence with respect to a perturbation in the direction of $\\phi$ is given by:\n$$ \\nabla_{\\phi} KL(q || p) = - \\mathbb{E}_{x \\sim q}[T_p \\phi(x)] $$\nwhere $T_p$ is the Stein operator. To achieve the steepest descent, we must choose $\\phi$ to maximize $\\mathbb{E}_{x \\sim q}[T_p \\phi(x)]$ subject to the constraint that $\\phi$ lies within a unit ball in the RKHS, i.e., $\\|\\phi\\|_{\\mathcal{H}} \\le 1$.\n\nThe expression $F[\\phi] = \\mathbb{E}_{x \\sim q}[T_p \\phi(x)]$ is a linear functional of $\\phi$. By the Riesz representation theorem, for any such linear functional on an RKHS, there exists a unique element $\\psi_q \\in \\mathcal{H}$ such that $F[\\phi] = \\langle \\phi, \\psi_q \\rangle_{\\mathcal{H}}$ for all $\\phi \\in \\mathcal{H}$. The function $\\phi$ that maximizes this inner product under the norm constraint is $\\phi = \\psi_q / \\|\\psi_q\\|_{\\mathcal{H}}$. The direction of steepest ascent is therefore given by $\\psi_q$.\n\nWe can find $\\psi_q$ using the reproducing property of the kernel $k(x, x')$. For any function $f \\in \\mathcal{H}$ and point $y$, we have $f(y) = \\langle f(\\cdot), k(y, \\cdot) \\rangle_{\\mathcal{H}}$. Applying this to $\\psi_q$:\n$$ \\psi_q(y) = \\langle \\psi_q(\\cdot), k(y, \\cdot) \\rangle_{\\mathcal{H}} $$\nSince the inner product is symmetric, $\\langle f, g \\rangle_{\\mathcal{H}} = \\langle g, f \\rangle_{\\mathcal{H}}$, and from the definition of $\\psi_q$, we have $\\langle k(y, \\cdot), \\psi_q(\\cdot) \\rangle_{\\mathcal{H}} = F[k(y, \\cdot)]$. Therefore:\n$$ \\psi_q(y) = F[k(y, \\cdot)] = \\mathbb{E}_{x \\sim q}[T_{p,x} k(y, x)] $$\nHere, $T_{p,x}$ denotes that the Stein operator acts on $k(y,x)$ as a function of its second argument, $x$.\nFor a symmetric kernel, $k(y,x) = k(x,y)$, the optimal velocity field at a point $y$ is:\n$$ \\phi^*(y) = \\psi_q(y) = \\mathbb{E}_{x \\sim q}[T_{p,x} k(x,y)] = \\mathbb{E}_{x \\sim q}[\\nabla_x k(x,y) + k(x,y) \\nabla_x \\ln p(x)] $$\nIn practice, the expectation over $q$ is approximated by an empirical average over the current set of $N$ particles $\\{x_i\\}_{i=1}^N$:\n$$ \\phi(y) \\approx \\frac{1}{N} \\sum_{i=1}^N [\\nabla_{x_i} k(x_i, y) + k(x_i, y) \\nabla_{x_i} \\ln p(x_i)] $$\nThe SVGD update for a particle $x_j$ is then $x_j^{(t+1)} = x_j^{(t)} + \\epsilon \\phi(x_j^{(t)})$, where the velocity field is evaluated at the particle's current location $x_j^{(t)}$:\n$$ \\phi(x_j^{(t)}) = \\frac{1}{N} \\sum_{i=1}^N [\\nabla_{x_i} k(x_i^{(t)}, x_j^{(t)}) + k(x_i^{(t)}, x_j^{(t)}) \\nabla_{x_i} \\ln p(x_i^{(t)})] $$\n\n### Step 3: Application to the Given Problem\n\nWe apply the derived update rule to the specifics of the problem.\n1.  **Target Distribution**: $p(x) = \\mathcal{N}(0,1) = \\frac{1}{\\sqrt{2\\pi}} \\exp(-\\frac{x^2}{2})$.\n    The log-density is $\\ln p(x) = -\\frac{x^2}{2} - \\frac{1}{2}\\ln(2\\pi)$.\n    The score function (gradient of the log-density) is $\\nabla_x \\ln p(x) = -x$.\n\n2.  **Kernel**: The RBF kernel is $k(x, x') = \\exp(-\\frac{(x-x')^2}{2h^2})$.\n    Its gradient with respect to the first argument is:\n    $$ \\nabla_x k(x, x') = \\frac{\\partial}{\\partial x} \\exp\\left(-\\frac{(x-x')^2}{2h^2}\\right) = \\exp\\left(-\\frac{(x-x')^2}{2h^2}\\right) \\cdot \\left(-\\frac{2(x-x')}{2h^2}\\right) = -k(x,x') \\frac{x-x'}{h^2} $$\n\n3.  **Particle Update Equation**: Let the particle locations at iteration $t=0$ be $\\{x_i^{(0)}\\}_{i=1}^N$. The velocity field for particle $x_j^{(0)}$ is:\n    $$ \\phi(x_j^{(0)}) = \\frac{1}{N} \\sum_{i=1}^N \\left[ -k(x_i^{(0)}, x_j^{(0)}) \\frac{x_i^{(0)} - x_j^{(0)}}{h^2} + k(x_i^{(0)}, x_j^{(0)}) (-x_i^{(0)}) \\right] $$\n    $$ \\phi(x_j^{(0)}) = \\frac{1}{N} \\sum_{i=1}^N k(x_i^{(0)}, x_j^{(0)}) \\left( -\\frac{x_i^{(0)} - x_j^{(0)}}{h^2} - x_i^{(0)} \\right) $$\n\n4.  **Initial Conditions**: We have $N=2$ particles at $x_1^{(0)} = -1$ and $x_2^{(0)} = 2$.\n\n5.  **Compute update for $x_1^{(1)}$**: Let $j=1$.\n    $$ \\phi(x_1^{(0)}) = \\frac{1}{2} \\left[ k(x_1^{(0)}, x_1^{(0)}) \\left( -\\frac{x_1^{(0)} - x_1^{(0)}}{h^2} - x_1^{(0)} \\right) + k(x_2^{(0)}, x_1^{(0)}) \\left( -\\frac{x_2^{(0)} - x_1^{(0)}}{h^2} - x_2^{(0)} \\right) \\right] $$\n    We compute the terms:\n    -   $x_1^{(0)} = -1$, $x_2^{(0)} = 2$.\n    -   $k(x_1^{(0)}, x_1^{(0)}) = \\exp(0) = 1$.\n    -   $k(x_2^{(0)}, x_1^{(0)}) = \\exp\\left(-\\frac{(2 - (-1))^2}{2h^2}\\right) = \\exp\\left(-\\frac{9}{2h^2}\\right)$.\n    -   Term for $i=1$: $1 \\cdot (0 - (-1)) = 1$.\n    -   Term for $i=2$: $\\exp\\left(-\\frac{9}{2h^2}\\right) \\left( -\\frac{2 - (-1)}{h^2} - 2 \\right) = \\exp\\left(-\\frac{9}{2h^2}\\right) \\left( -\\frac{3}{h^2} - 2 \\right)$.\n    The velocity is:\n    $$ \\phi(x_1^{(0)}) = \\frac{1}{2} \\left[ 1 - \\left( \\frac{3}{h^2} + 2 \\right) \\exp\\left(-\\frac{9}{2h^2}\\right) \\right] $$\n    The updated particle location is:\n    $$ x_1^{(1)} = x_1^{(0)} + \\epsilon \\phi(x_1^{(0)}) = -1 + \\frac{\\epsilon}{2} \\left[ 1 - \\left( 2 + \\frac{3}{h^2} \\right) \\exp\\left(-\\frac{9}{2h^2}\\right) \\right] $$\n\n6.  **Compute update for $x_2^{(1)}$**: Let $j=2$.\n    $$ \\phi(x_2^{(0)}) = \\frac{1}{2} \\left[ k(x_1^{(0)}, x_2^{(0)}) \\left( -\\frac{x_1^{(0)} - x_2^{(0)}}{h^2} - x_1^{(0)} \\right) + k(x_2^{(0)}, x_2^{(0)}) \\left( -\\frac{x_2^{(0)} - x_2^{(0)}}{h^2} - x_2^{(0)} \\right) \\right] $$\n    We compute the terms:\n    -   $k(x_1^{(0)}, x_2^{(0)}) = \\exp\\left(-\\frac{(-1 - 2)^2}{2h^2}\\right) = \\exp\\left(-\\frac{9}{2h^2}\\right)$.\n    -   $k(x_2^{(0)}, x_2^{(0)}) = 1$.\n    -   Term for $i=1$: $\\exp\\left(-\\frac{9}{2h^2}\\right) \\left( -\\frac{-1 - 2}{h^2} - (-1) \\right) = \\exp\\left(-\\frac{9}{2h^2}\\right) \\left( \\frac{3}{h^2} + 1 \\right)$.\n    -   Term for $i=2$: $1 \\cdot (0 - 2) = -2$.\n    The velocity is:\n    $$ \\phi(x_2^{(0)}) = \\frac{1}{2} \\left[ \\left( \\frac{3}{h^2} + 1 \\right) \\exp\\left(-\\frac{9}{2h^2}\\right) - 2 \\right] $$\n    The updated particle location is:\n    $$ x_2^{(1)} = x_2^{(0)} + \\epsilon \\phi(x_2^{(0)}) = 2 + \\frac{\\epsilon}{2} \\left[ \\left( 1 + \\frac{3}{h^2} \\right) \\exp\\left(-\\frac{9}{2h^2}\\right) - 2 \\right] $$\n    This simplifies to:\n    $$ x_2^{(1)} = 2 - \\epsilon + \\frac{\\epsilon}{2} \\left( 1 + \\frac{3}{h^2} \\right) \\exp\\left(-\\frac{9}{2h^2}\\right) $$\nThe final expressions for the updated particle locations are:\n$$ x_1^{(1)} = -1 + \\frac{\\epsilon}{2} \\left[ 1 - \\left( 2 + \\frac{3}{h^2} \\right) \\exp\\left(-\\frac{9}{2h^2}\\right) \\right] $$\n$$ x_2^{(1)} = 2 - \\epsilon + \\frac{\\epsilon}{2} \\left( 1 + \\frac{3}{h^2} \\right) \\exp\\left(-\\frac{9}{2h^2}\\right) $$", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n-1 + \\frac{\\epsilon}{2} \\left( 1 - \\left(2 + \\frac{3}{h^2}\\right) \\exp\\left(-\\frac{9}{2h^2}\\right) \\right) & 2 - \\epsilon + \\frac{\\epsilon}{2} \\left( 1 + \\frac{3}{h^2} \\right) \\exp\\left(-\\frac{9}{2h^2}\\right)\n\\end{pmatrix}\n}\n$$", "id": "3348310"}, {"introduction": "A defining feature of Stein Variational Gradient Descent is the interplay between two forces: an attraction towards regions of high probability and a repulsion that encourages particle diversity. This dynamic balance is key to SVGD's ability to capture complex distributions and avoid the mode collapse that can plague other variational methods. This practice offers a quantitative exploration of this behavior, asking you to derive the critical distance at which these forces equilibrate for a two-particle system. [@problem_id:3348300]", "problem": "Consider Stein Variational Gradient Descent (SVGD), defined by the functional gradient flow that transports a set of particles to approximate a target density $p(x)$. For a finite set of $n$ particles $\\{x_{i}\\}_{i=1}^{n}$ in $\\mathbb{R}^{d}$, the empirical SVGD velocity at particle $x_{i}$ is\n$$\n\\phi(x_{i}) \\;=\\; \\frac{1}{n} \\sum_{j=1}^{n} \\Big( k(x_{j},x_{i}) \\,\\nabla \\ln p(x_{j}) \\;+\\; \\nabla_{x_{j}} k(x_{j},x_{i}) \\Big),\n$$\nwhere $k(x,y)$ is a positive definite kernel and $\\nabla$ denotes gradient with respect to the argument. In SVGD, the first term is the attraction toward high-density regions and the second term is a repulsion that spreads particles. Let the kernel be the Gaussian radial basis function (RBF) kernel $k(x,y) = \\exp\\!\\big(-\\|x-y\\|^{2}/(2 h^{2})\\big)$ with bandwidth $h>0$.\n\nTask A (scaling of repulsion): In one dimension ($d=1$), for $n=2$ particles at locations $x_{1}$ and $x_{2}$ with inter-particle distance $d = |x_{1}-x_{2}|$, derive the magnitude of the pairwise repulsion contribution from $x_{2}$ to the velocity of $x_{1}$ as a function of $d$ and $h$. You must express it in closed form using only $d$ and $h$.\n\nTask B (quantitative avoidance of mode collapse): Consider the one-dimensional Gaussian target $p(x) = \\mathcal{N}(0,\\sigma^{2})$ with variance $\\sigma^{2} > 0$. Place $n=2$ particles symmetrically at $x_{1}=a$ and $x_{2}=-a$ with $a>0$. Using only the SVGD definition above and the RBF kernel given, compute the exact condition under which the radial component of the SVGD velocity at $x_{1}$ points outward (i.e., increases $|a|$). Then, solve for the unique critical separation $a_{c}(h,\\sigma)$ for which the radial SVGD velocity at $x_{1}$ is exactly zero. Your final answer must be a single closed-form analytic expression for $a_{c}(h,\\sigma)$ with no unevaluated limits or implicit definitions.\n\nAnswer format requirement: Provide only the analytic expression for $a_{c}(h,\\sigma)$ as your final answer. Do not include units. No rounding is required.", "solution": "The user has provided a valid problem statement concerning the properties of Stein Variational Gradient Descent (SVGD). The problem is scientifically grounded, well-posed, and contains all necessary information for a rigorous mathematical derivation. I will proceed with a complete solution.\n\nThe problem is divided into two tasks. I will solve them sequentially to arrive at the final required expression for the critical separation $a_{c}(h, \\sigma)$.\n\nFirst, let's establish the necessary mathematical components.\nThe SVGD velocity at particle $x_i$ is given by:\n$$ \\phi(x_{i}) = \\frac{1}{n} \\sum_{j=1}^{n} \\left( k(x_{j},x_{i}) \\nabla \\ln p(x_{j}) + \\nabla_{x_{j}} k(x_{j},x_{i}) \\right) $$\nThe kernel is the one-dimensional ($d=1$) Gaussian RBF kernel:\n$$ k(x,y) = \\exp\\left(-\\frac{(x-y)^2}{2h^2}\\right) $$\nThe gradient of the kernel with respect to its first argument, $x_j$, is:\n$$ \\nabla_{x_{j}} k(x_{j},x_{i}) = \\frac{\\partial}{\\partial x_j} \\exp\\left(-\\frac{(x_j-x_i)^2}{2h^2}\\right) = \\exp\\left(-\\frac{(x_j-x_i)^2}{2h^2}\\right) \\cdot \\left(-\\frac{2(x_j-x_i)}{2h^2}\\right) = -\\frac{x_j-x_i}{h^2} k(x_j,x_i) $$\n\n**Task A: Scaling of Repulsion**\n\nThis task requires the magnitude of the pairwise repulsion contribution from particle $x_2$ to the velocity of particle $x_1$. The number of particles is $n=2$. The repulsion contribution is the part of the velocity update involving the kernel gradient. For the velocity $\\phi(x_1)$, the term corresponding to $j=2$ contains the repulsion from $x_2$. This contribution is given by $\\frac{1}{n} \\nabla_{x_2} k(x_2, x_1)$.\n\nUsing $n=2$ and the derivative calculated above:\n$$ \\text{Repulsion from } x_2 \\text{ on } x_1 = \\frac{1}{2} \\nabla_{x_2} k(x_2, x_1) = \\frac{1}{2} \\left( -\\frac{x_2-x_1}{h^2} k(x_2, x_1) \\right) = \\frac{x_1-x_2}{2h^2} k(x_1, x_2) $$\nThe problem asks for the magnitude of this term. Let $d = |x_1 - x_2|$. The kernel $k(x_1, x_2)$ is always positive.\n$$ k(x_1, x_2) = \\exp\\left(-\\frac{(x_1-x_2)^2}{2h^2}\\right) = \\exp\\left(-\\frac{d^2}{2h^2}\\right) $$\nThe magnitude is therefore:\n$$ \\left| \\frac{x_1-x_2}{2h^2} k(x_1, x_2) \\right| = \\frac{|x_1-x_2|}{2h^2} k(x_1, x_2) = \\frac{d}{2h^2} \\exp\\left(-\\frac{d^2}{2h^2}\\right) $$\nThis expression represents the magnitude of the repulsion force exerted by one particle on another as a function of their separation $d$ and the kernel bandwidth $h$.\n\n**Task B: Quantitative Avoidance of Mode Collapse**\n\nThis task requires us to find the critical separation $a_c(h, \\sigma)$ for a specific setup.\nThe given setup is:\n- Dimension: $d=1$.\n- Number of particles: $n=2$.\n- Particle locations: $x_1 = a$ and $x_2 = -a$, with $a > 0$.\n- Target density: $p(x) = \\mathcal{N}(0, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{x^2}{2\\sigma^2}\\right)$.\n\nWe first compute the gradient of the log-target density:\n$$ \\ln p(x) = -\\ln(\\sqrt{2\\pi}\\sigma) - \\frac{x^2}{2\\sigma^2} $$\n$$ \\nabla \\ln p(x) = \\frac{d}{dx} \\ln p(x) = -\\frac{x}{\\sigma^2} $$\nAt the particle locations, we have:\n$$ \\nabla \\ln p(x_1) = \\nabla \\ln p(a) = -\\frac{a}{\\sigma^2} $$\n$$ \\nabla \\ln p(x_2) = \\nabla \\ln p(-a) = -\\frac{-a}{\\sigma^2} = \\frac{a}{\\sigma^2} $$\n\nNext, we evaluate the SVGD velocity at $x_1 = a$. The velocity expression is:\n$$ \\phi(x_1) = \\frac{1}{2} \\left[ \\left( k(x_1, x_1) \\nabla\\ln p(x_1) + \\nabla_{x_1} k(x_1, x_1) \\right) + \\left( k(x_2, x_1) \\nabla\\ln p(x_2) + \\nabla_{x_2} k(x_2, x_1) \\right) \\right] $$\nWe compute each term in the sum:\n\nTerm 1 (interaction with self, $j=1$):\n- $k(x_1, x_1) = k(a,a) = \\exp(0) = 1$.\n- $\\nabla\\ln p(x_1) = -a/\\sigma^2$.\n- $\\nabla_{x_1} k(x_1, x_1) = -\\frac{x_1-x_1}{h^2} k(x_1,x_1) = 0$.\nSo, the first term in brackets is $(1 \\cdot (-a/\\sigma^2) + 0) = -a/\\sigma^2$.\n\nTerm 2 (interaction with $x_2$, $j=2$):\n- $x_2 - x_1 = -a - a = -2a$.\n- $k(x_2, x_1) = k(-a, a) = \\exp\\left(-\\frac{(-2a)^2}{2h^2}\\right) = \\exp\\left(-\\frac{2a^2}{h^2}\\right)$.\n- $\\nabla\\ln p(x_2) = a/\\sigma^2$.\n- $\\nabla_{x_2} k(x_2, x_1) = -\\frac{x_2-x_1}{h^2} k(x_2, x_1) = -\\frac{-2a}{h^2} \\exp\\left(-\\frac{2a^2}{h^2}\\right) = \\frac{2a}{h^2} \\exp\\left(-\\frac{2a^2}{h^2}\\right)$.\nSo, the second term in brackets is:\n$$ \\exp\\left(-\\frac{2a^2}{h^2}\\right) \\cdot \\left(\\frac{a}{\\sigma^2}\\right) + \\frac{2a}{h^2} \\exp\\left(-\\frac{2a^2}{h^2}\\right) = \\exp\\left(-\\frac{2a^2}{h^2}\\right) \\left( \\frac{a}{\\sigma^2} + \\frac{2a}{h^2} \\right) $$\n\nNow, we assemble the full expression for $\\phi(x_1)$:\n$$ \\phi(x_1) = \\frac{1}{2} \\left[ -\\frac{a}{\\sigma^2} + \\exp\\left(-\\frac{2a^2}{h^2}\\right) \\left( \\frac{a}{\\sigma^2} + \\frac{2a}{h^2} \\right) \\right] $$\nFactoring out $a/2$:\n$$ \\phi(x_1) = \\frac{a}{2} \\left[ -\\frac{1}{\\sigma^2} + \\exp\\left(-\\frac{2a^2}{h^2}\\right) \\left( \\frac{1}{\\sigma^2} + \\frac{2}{h^2} \\right) \\right] $$\nThe critical separation $a_c$ is defined as the value of $a$ for which the velocity is zero, i.e., $\\phi(x_1) = 0$. Since we are given $a>0$, we need the term in the square brackets to be zero. Let $a = a_c$:\n$$ -\\frac{1}{\\sigma^2} + \\exp\\left(-\\frac{2a_c^2}{h^2}\\right) \\left( \\frac{1}{\\sigma^2} + \\frac{2}{h^2} \\right) = 0 $$\n$$ \\exp\\left(-\\frac{2a_c^2}{h^2}\\right) \\left( \\frac{1}{\\sigma^2} + \\frac{2}{h^2} \\right) = \\frac{1}{\\sigma^2} $$\n$$ \\exp\\left(-\\frac{2a_c^2}{h^2}\\right) = \\frac{1/\\sigma^2}{1/\\sigma^2 + 2/h^2} $$\nTo simplify the right-hand side, we find a common denominator:\n$$ \\frac{1/\\sigma^2}{(h^2 + 2\\sigma^2)/(\\sigma^2 h^2)} = \\frac{1}{\\sigma^2} \\cdot \\frac{\\sigma^2 h^2}{h^2 + 2\\sigma^2} = \\frac{h^2}{h^2 + 2\\sigma^2} $$\nThe equation for $a_c$ is:\n$$ \\exp\\left(-\\frac{2a_c^2}{h^2}\\right) = \\frac{h^2}{h^2 + 2\\sigma^2} $$\nTo solve for $a_c$, we take the natural logarithm of both sides:\n$$ -\\frac{2a_c^2}{h^2} = \\ln\\left(\\frac{h^2}{h^2 + 2\\sigma^2}\\right) $$\nUsing the property $\\ln(x/y) = -\\ln(y/x)$:\n$$ \\frac{2a_c^2}{h^2} = \\ln\\left(\\frac{h^2 + 2\\sigma^2}{h^2}\\right) = \\ln\\left(1 + \\frac{2\\sigma^2}{h^2}\\right) $$\nNow, we solve for $a_c^2$:\n$$ a_c^2 = \\frac{h^2}{2} \\ln\\left(1 + \\frac{2\\sigma^2}{h^2}\\right) $$\nSince $a > 0$, we take the positive square root:\n$$ a_c(h, \\sigma) = \\sqrt{\\frac{h^2}{2} \\ln\\left(1 + \\frac{2\\sigma^2}{h^2}\\right)} $$\nThis can be written as:\n$$ a_c(h, \\sigma) = h \\sqrt{\\frac{1}{2} \\ln\\left(1 + \\frac{2\\sigma^2}{h^2}\\right)} $$\nThis is the unique critical separation for which the SVGD velocity at $x_1=a$ is zero. For $a < a_c$, the term $\\exp(-2a^2/h^2)$ is larger, making the velocity positive (outward repulsion dominates). For $a > a_c$, the velocity is negative (inward attraction to the mode at $x=0$ dominates).", "answer": "$$ \\boxed{h \\sqrt{\\frac{1}{2} \\ln\\left(1 + \\frac{2\\sigma^2}{h^2}\\right)}} $$", "id": "3348300"}, {"introduction": "While understanding the mechanics and dynamics of an algorithm is crucial, its practical utility hinges on its computational feasibility. This exercise shifts focus to the scalability of SVGD by analyzing the per-iteration complexity of the standard implementation, which scales quadratically, $O(n^2)$, with the number of particles $n$. You will then investigate Random Fourier Features (RFF), a powerful technique for approximating the kernel to achieve a sub-quadratic complexity, thereby making SVGD practical for larger-scale problems. [@problem_id:3348282]", "problem": "Consider Stein Variational Gradient Descent (SVGD), which updates a set of $n$ particles $\\{x_i\\}_{i=1}^n \\subset \\mathbb{R}^d$ seeking to transport an empirical measure toward a target density $p(x)$ using a deterministic transform built from the Stein operator and a positive definite kernel $k(x, x')$. The SVGD update for a particle $x_i$ at iteration $t$ can be written in terms of a vector field $\\phi(x)$ as $x_i^{(t+1)} = x_i^{(t)} + \\epsilon \\,\\phi(x_i^{(t)})$, where $\\phi(x)$ is constructed from the empirical expectation over particles of the Stein operator applied to $k$ and $\\nabla \\log p(x)$, and where each pairwise kernel evaluation and kernel gradient is computed densely. Assume that computing the score function $\\nabla \\log p(x)$ for one particle costs on the order of $d$ arithmetic operations.\n\nFor a differentiable, shift-invariant kernel $k(x, x') = k(x - x')$, a Random Fourier Feature (RFF) approximation uses $m$ features to approximate $k$ by $k(x, x') \\approx z(x)^\\top z(x')$, where $z(x) \\in \\mathbb{R}^m$ is constructed via random frequencies drawn from the spectral measure of $k$. Gradients of the kernel with respect to particle coordinates are obtained by differentiating the features, using $\\nabla_x k(x, x') \\approx \\nabla_x z(x)^\\top z(x')$. You may assume that evaluating $z(x)$ and $\\nabla_x z(x)$ for one particle requires on the order of $m d$ arithmetic operations when the random frequencies are precomputed and stored.\n\nCompute the per-iteration time and memory complexities of SVGD with $n$ particles and dimension $d$ using dense kernel evaluation, in terms of $n$ and $d$. Then, propose a sub-quadratic approximation via Random Fourier Features (RFF) with $m$ features for a shift-invariant kernel, and derive the per-iteration time and memory complexities of the RFF-based SVGD, in terms of $n$, $d$, and $m$. State conditions on $m$ under which the RFF-based complexity is sub-quadratic in $n$ for fixed $d$.\n\nSelect the option that correctly describes all of these complexities and conditions.\n\nA. Dense SVGD per-iteration time is $O(n^2 d) + O(n d)$ and memory is $O(n d) + O(n^2)$. With $m$ Random Fourier Features, per-iteration time is $O(n m d)$ and memory is $O(n d) + O(n m) + O(m d)$; this is sub-quadratic in $n$ whenever $m = o(n)$ for fixed $d$.\n\nB. Dense SVGD per-iteration time is $O(n^2) + O(d)$ and memory is $O(n^2 d)$. With $m$ Random Fourier Features, per-iteration time is $O(n^2)$ and memory is $O(n m)$; no sub-quadratic regime in $n$ arises from RFF.\n\nC. Dense SVGD per-iteration time is $O(n d)$ and memory is $O(n d)$. With $m$ Random Fourier Features, per-iteration time is $O(n m d)$ and memory is $O(n d) + O(m)$; the method is sub-quadratic in $n$ when $m$ is constant.\n\nD. Dense SVGD per-iteration time is $O(n^2 d)$ and memory is $O(n d)$. With $m$ Random Fourier Features, per-iteration time is $O(n m)$ and memory is $O(n m)$; the method is always sub-quadratic in $n$ regardless of $m$.\n\nE. Dense SVGD per-iteration time is $O(n^2 d)$ and memory is $O(n^2 d)$. With $m$ Random Fourier Features, per-iteration time is $O(n m d) + O(n^2)$ and memory is $O(n d)$; RFF does not reduce the asymptotic time complexity below quadratic.", "solution": "The user wants a detailed analysis of the per-iteration time and memory complexities for Stein Variational Gradient Descent (SVGD), both in its dense formulation and with a Random Fourier Feature (RFF) approximation.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n-   Algorithm: Stein Variational Gradient Descent (SVGD).\n-   Particles: $n$ particles $\\{x_i\\}_{i=1}^n$ in $\\mathbb{R}^d$.\n-   Target density: $p(x)$.\n-   Update rule: $x_i^{(t+1)} = x_i^{(t)} + \\epsilon \\,\\phi(x_i^{(t)})$.\n-   Velocity field $\\phi(x)$: Constructed from an empirical expectation over particles involving the Stein operator, a kernel $k(x, x')$, and the score function $\\nabla \\log p(x)$.\n-   Dense evaluation: Pairwise kernel values $k(x_i, x_j)$ and kernel gradients are computed for all pairs (densely).\n-   Cost of score function: Computing $\\nabla \\log p(x)$ for one particle costs $O(d)$.\n-   RFF Approximation: For a shift-invariant kernel $k(x, x') = k(x-x')$, an approximation using $m$ features is given by $k(x, x') \\approx z(x)^\\top z(x')$, where $z(x) \\in \\mathbb{R}^m$.\n-   RFF Gradient Approximation: $\\nabla_x k(x, x') \\approx \\nabla_x z(x)^\\top z(x')$.\n-   Cost of RFF features: Evaluating $z(x)$ and its Jacobian $\\nabla_x z(x)$ for one particle costs $O(md)$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem statement is scientifically grounded, describing a standard algorithm (SVGD) and a common approximation technique (RFF) from the field of machine learning and computational statistics. The formulation is consistent with established literature. The problem is well-posed, providing cost models for primitive operations ($O(d)$ for the score, $O(md)$ for RFF features) that are sufficient to perform a standard Big-O complexity analysis. The language used is objective and precise. The problem does not violate any of the invalidity criteria (it's not unsound, non-formalizable, incomplete, unrealistic, ill-posed, trivial, or unverifiable).\n\n**Step 3: Verdict and Action**\nThe problem is valid. I will proceed with deriving the solution.\n\n### Derivation of Complexities\n\n**Part 1: Dense SVGD Complexity**\n\nThe velocity field for particle $x_i$ in SVGD is given by the empirical mean:\n$$ \\phi(x_i) = \\frac{1}{n} \\sum_{j=1}^n \\left[ k(x_i, x_j) \\nabla_{x_j} \\log p(x_j) + \\nabla_{x_j} k(x_i, x_j) \\right] $$\nwhere $\\nabla_{x_j} k(x_i, x_j)$ is the gradient of the kernel with respect to its second argument, evaluated at $x_j$.\n\n**Time Complexity (Dense SVGD):**\n1.  **Score Computation**: We must first compute the score function $\\nabla_{x_j} \\log p(x_j)$ for all $j \\in \\{1, \\dots, n\\}$. Since each computation costs $O(d)$, the total cost is $O(nd)$. These $n$ vectors can be stored.\n2.  **Velocity Computation**: To compute the velocity $\\phi(x_i)$ for a single particle $i$, we must sum over all $j \\in \\{1, \\dots, n\\}$.\n    *   For each pair $(i, j)$, we compute the term inside the summation.\n    *   Evaluating the kernel $k(x_i, x_j)$ takes $O(d)$ time (e.g., for an RBF kernel, this involves a distance calculation).\n    *   The score $\\nabla_{x_j} \\log p(x_j)$ is already computed. The scalar-vector product $k(x_i, x_j) \\nabla_{x_j} \\log p(x_j)$ takes $O(d)$ time.\n    *   Evaluating the kernel gradient $\\nabla_{x_j} k(x_i, x_j)$ also takes $O(d)$ time.\n    *   Adding the two resulting $d$-dimensional vectors takes $O(d)$ time.\n    *   Thus, computing the term for a single $j$ costs $O(d)$.\n    *   Summing over all $n$ particles for a fixed $i$ costs $n \\times O(d) = O(nd)$.\n3.  **Total Velocity Computation**: Since we must compute $\\phi(x_i)$ for all $i \\in \\{1, \\dots, n\\}$, the total cost is $n \\times O(nd) = O(n^2 d)$.\n4.  **Particle Updates**: Updating all $n$ particle positions $x_i \\leftarrow x_i + \\epsilon \\phi(x_i)$ takes $n \\times O(d) = O(nd)$.\n\nThe overall per-iteration time complexity is the sum of these costs: $O(nd) + O(n^2 d) + O(nd) = O(n^2 d)$. This can be written as $O(n^2 d) + O(nd)$.\n\n**Memory Complexity (Dense SVGD):**\n1.  **Particles**: Storing the positions $\\{x_i\\}_{i=1}^n$ requires $O(nd)$ memory.\n2.  **Scores**: Storing the precomputed scores $\\{\\nabla_{x_j} \\log p(x_j)\\}_{j=1}^n$ requires $O(nd)$ memory.\n3.  **Kernel Matrix**: A \"dense\" evaluation often implies the computation and storage of the full $n \\times n$ kernel matrix $K$, where $K_{ij} = k(x_i, x_j)$. This requires $O(n^2)$ memory. While not strictly necessary if values are recomputed on the fly, it is a common implementation strategy.\n4.  **Velocities**: Storing the computed velocities $\\{\\phi(x_i)\\}_{i=1}^n$ requires $O(nd)$ memory.\n\nThe dominant memory terms are for the particles and the kernel matrix. The total memory complexity is $O(nd) + O(n^2)$.\n\n**Part 2: RFF-Approximated SVGD Complexity**\n\nUsing the RFF approximation $k(x, x') \\approx z(x)^\\top z(x')$, the gradient approximation becomes $\\nabla_{x'}k(x, x') \\approx (\\nabla_{x'}z(x'))^\\top z(x)$. Substituting these into the SVGD update for $\\phi(x_i)$:\n$$ \\phi(x_i) \\approx \\frac{1}{n} \\sum_{j=1}^n \\left[ (z(x_i)^\\top z(x_j)) \\nabla_{x_j} \\log p(x_j) + (\\nabla_{x_j}z(x_j))^\\top z(x_i) \\right] $$\nThis expression can be rearranged by factoring out $z(x_i)$:\n$$ \\phi(x_i) \\approx \\left(\\frac{1}{n} \\sum_{j=1}^n \\left[ \\nabla_{x_j} \\log p(x_j) \\, z(x_j)^\\top + \\nabla_{x_j}z(x_j) \\right]\\right)^\\top z(x_i) $$\nLet $S_j = \\nabla_{x_j} \\log p(x_j)$ be the $d \\times 1$ score vector. Let $V$ be the $m \\times d$ matrix defined as:\n$$ V = \\frac{1}{n} \\sum_{j=1}^n \\left[ z(x_j) S_j^\\top + \\nabla_{x_j}z(x_j) \\right] $$\nThen the velocity can be computed as $\\phi(x_i) \\approx V^\\top z(x_i)$.\n\n**Time Complexity (RFF-SVGD):**\nThis formulation avoids the quadratic pairwise interaction. The computation proceeds in stages:\n1.  **Preprocessing**: Compute scores $S_j$ for all $j$: $O(nd)$. Store the pre-computed random frequencies for RFF.\n2.  **Compute Matrix V**:\n    *   Initialize $V$ to a zero matrix of size $m \\times d$.\n    *   Loop for $j=1$ to $n$:\n        *   Compute $z(x_j)$ and its Jacobian $\\nabla_{x_j}z(x_j)$. Per the problem statement, this costs $O(md)$.\n        *   Compute the outer product $z(x_j)S_j^\\top$, an $m \\times d$ matrix. This costs $O(md)$.\n        *   Add this and $\\nabla_{x_j}z(x_j)$ to $V$. This costs $O(md)$.\n    *   The total cost for this loop is $n \\times O(md) = O(nmd)$.\n3.  **Compute Velocities**:\n    *   For each particle $i=1$ to $n$:\n        *   Compute $z(x_i)$, which costs $O(md)$. (Or, these can be computed once for all $i$ and stored).\n        *   Compute the matrix-vector product $\\phi(x_i) = V^\\top z(x_i)$. This is a $(d \\times m) \\times (m \\times 1)$ product, costing $O(md)$.\n    *   The total cost is $n \\times O(md) = O(nmd)$. A more efficient way is to compute the matrix of all features $Z \\in \\mathbb{R}^{n \\times m}$, which costs $O(nmd)$ in total. Then compute the full velocity matrix $\\Phi = ZV$, an $(n \\times m) \\times (m \\times d)$ multiplication, which costs $O(nmd)$.\n4.  **Particle Updates**: Update all $n$ particles: $O(nd)$.\n\nThe overall per-iteration time complexity is $O(nd) + O(nmd) + O(nmd) + O(nd) = O(nmd)$.\n\n**Memory Complexity (RFF-SVGD):**\n1.  **Particles and Scores**: $O(nd)$ for $\\{x_i\\}$ and $O(nd)$ for $\\{S_j\\}$.\n2.  **RFF Frequencies**: To generate features $z(x)$, we need to store $m$ random frequency vectors, each of size $d$. This requires $O(md)$ memory.\n3.  **Feature Matrix**: To compute the final velocities efficiently via matrix multiplication, we must store the feature vectors for all particles, forming a matrix $Z \\in \\mathbb{R}^{n \\times m}$. This requires $O(nm)$ memory.\n4.  **Intermediate Matrix**: The matrix $V \\in \\mathbb{R}^{m \\times d}$ must be stored, requiring $O(md)$ memory.\n\nThe total memory complexity is the sum of these requirements: $O(nd) + O(md) + O(nm)$.\n\n**Part 3: Condition for Sub-Quadratic Complexity**\n\nThe goal is for the RFF-based method to have a time complexity that is asymptotically better (sub-quadratic) than the dense method with respect to the number of particles $n$.\n*   Dense time complexity: $O(n^2 d)$.\n*   RFF time complexity: $O(nmd)$.\n\nFor the RFF version to be sub-quadratic in $n$, we require:\n$$ nmd = o(n^2 d) $$\nAssuming $d$ is fixed, we can simplify this to:\n$$ nm = o(n^2) $$\nDividing by $n$ (for $n>0$), we get the condition:\n$$ m = o(n) $$\nThis means that the number of features $m$ must grow slower than the number of particles $n$.\n\n### Option-by-Option Analysis\n\n*   **A. Dense SVGD per-iteration time is $O(n^2 d) + O(n d)$ and memory is $O(n d) + O(n^2)$. With $m$ Random Fourier Features, per-iteration time is $O(n m d)$ and memory is $O(n d) + O(n m) + O(m d)$; this is sub-quadratic in $n$ whenever $m = o(n)$ for fixed $d$.**\n    *   **Dense SVGD**: Time $O(n^2 d) + O(nd) \\equiv O(n^2 d)$ is correct. Memory $O(nd) + O(n^2)$ is correct under the standard interpretation of dense kernel matrix computation.\n    *   **RFF-SVGD**: Time $O(nmd)$ is correct. Memory $O(nd) + O(nm) + O(md)$ is correct.\n    *   **Condition**: $m=o(n)$ is the correct condition.\n    *   **Verdict**: **Correct**.\n\n*   **B. Dense SVGD per-iteration time is $O(n^2) + O(d)$ and memory is $O(n^2 d)$. With $m$ Random Fourier Features, per-iteration time is $O(n^2)$ and memory is $O(n m)$; no sub-quadratic regime in $n$ arises from RFF.**\n    *   **Dense SVGD**: Time $O(n^2) + O(d)$ is incorrect; it misses the factor of $d$. Memory $O(n^2 d)$ corresponds to storing all pairwise gradients, which is possible but an excessively high-memory implementation and less standard than storing the kernel matrix.\n    *   **RFF-SVGD**: Time $O(n^2)$ is incorrect. Memory $O(nm)$ is incomplete.\n    *   **Condition**: The claim that no sub-quadratic regime exists is false.\n    *   **Verdict**: **Incorrect**.\n\n*   **C. Dense SVGD per-iteration time is $O(n d)$ and memory is $O(n d)$. With $m$ Random Fourier Features, per-iteration time is $O(n m d)$ and memory is $O(n d) + O(m)$; the method is sub-quadratic in $n$ when $m$ is constant.**\n    *   **Dense SVGD**: Time $O(nd)$ is incorrect; it ignores the $O(n^2)$ pairwise interactions.\n    *   **RFF-SVGD**: RFF memory of $O(nd) + O(m)$ is incorrect; it omits the crucial $O(nm)$ term. The condition $m$ is constant is only a specific case of the general condition $m=o(n)$.\n    *   **Verdict**: **Incorrect**.\n\n*   **D. Dense SVGD per-iteration time is $O(n^2 d)$ and memory is $O(n d)$. With $m$ Random Fourier Features, per-iteration time is $O(n m)$ and memory is $O(n m)$; the method is always sub-quadratic in $n$ regardless of $m$.**\n    *   **Dense SVGD**: Memory $O(nd)$ is only correct for a memory-optimized implementation that doesn't store the kernel matrix.\n    *   **RFF-SVGD**: Time $O(nm)$ is incorrect; it misses the factor of $d$. Memory $O(nm)$ is incomplete.\n    *   **Condition**: The claim it is \"always sub-quadratic\" is false. If $m = \\Omega(n)$, the complexity is not sub-quadratic.\n    *   **Verdict**: **Incorrect**.\n\n*   **E. Dense SVGD per-iteration time is $O(n^2 d)$ and memory is $O(n^2 d)$. With $m$ Random Fourier Features, per-iteration time is $O(n m d) + O(n^2)$ and memory is $O(n d)$; RFF does not reduce the asymptotic time complexity below quadratic.**\n    *   **Dense SVGD**: Memory $O(n^2 d)$ is likely incorrect, as noted for option B.\n    *   **RFF-SVGD**: Time $O(nmd) + O(n^2)$ is incorrect; the entire point of the RFF reformulation is to eliminate the $O(n^2)$ term. The claim that RFF does not reduce complexity is also false.\n    *   **Verdict**: **Incorrect**.\n\nBased on the detailed derivation, Option A provides a complete and accurate description of all complexities and the condition for sub-quadratic performance.", "answer": "$$\\boxed{A}$$", "id": "3348282"}]}