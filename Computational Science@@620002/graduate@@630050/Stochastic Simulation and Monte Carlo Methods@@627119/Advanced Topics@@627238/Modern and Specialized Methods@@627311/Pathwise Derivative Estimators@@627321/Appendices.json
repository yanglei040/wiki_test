{"hands_on_practices": [{"introduction": "To build a solid foundation, we begin with a canonical example of pathwise differentiation in a well-behaved system. This exercise focuses on the Ornstein-Uhlenbeck process, a cornerstone of stochastic modeling, and asks you to derive the sensitivity of a final-time expectation with respect to a parameter in the drift. By deriving the \"tangent process\" and constructing the estimator from first principles, you will directly engage with the core mechanism of Infinitesimal Perturbation Analysis (IPA). Verifying that your estimator is unbiased by comparing its expectation to the true analytical derivative will build your confidence in the method's application under ideal conditions [@problem_id:3328507].", "problem": "Consider the Ornstein–Uhlenbeck (OU) process parameterized by a scalar parameter $\\theta \\in (0,\\infty)$, defined as the unique strong solution to the linear stochastic differential equation (SDE)\n$$\ndX_{t}^{\\theta} = -\\lambda(\\theta)\\,X_{t}^{\\theta}\\,dt + \\sigma\\,dW_{t}, \\quad X_{0}^{\\theta} = x_{0},\n$$\nwhere $W_{t}$ is a standard Wiener process (also known as Brownian motion), $\\sigma \\in (0,\\infty)$ is a fixed diffusion coefficient, and $x_{0} \\in \\mathbb{R}$ is deterministic and independent of $\\theta$. Let the mean-reversion rate be $\\lambda(\\theta) = \\theta$. Define the performance functional at a fixed terminal time $T \\in (0,\\infty)$ by $f(x) = x^{2}$ and the quantity of interest $F(\\theta) = \\mathbb{E}[f(X_{T}^{\\theta})]$.\n\nUsing only foundational tools from the theory of stochastic processes and stochastic differential equations—namely, existence and uniqueness for linear SDEs, the variation-of-constants method for linear equations, the Itô isometry, and dominated convergence as needed—do the following:\n\n- Derive the pathwise sensitivity process $Y_{t} = \\nabla_{\\theta} X_{t}^{\\theta}$ and obtain a closed-form expression for the pathwise gradient estimator $G(\\theta) = f'(X_{T}^{\\theta})\\,Y_{T}$.\n\n- Compute $\\mathbb{E}[G(\\theta)]$ in closed form and compare it to the analytic derivative $\\nabla_{\\theta}\\,\\mathbb{E}[f(X_{T}^{\\theta})]$ obtained by differentiating the exact expression for $\\mathbb{E}[X_{T}^{\\theta\\,2}]$ with respect to $\\theta$. Conclude whether $G(\\theta)$ is an unbiased estimator of $\\nabla_{\\theta}\\,\\mathbb{E}[f(X_{T}^{\\theta})]$ under the stated regularity assumptions.\n\nExpress your final answer as a single, closed-form analytic expression for $\\nabla_{\\theta}\\,\\mathbb{E}[f(X_{T}^{\\theta})]$ in terms of $\\theta$, $T$, $\\sigma$, and $x_{0}$. No numerical approximation or rounding is required.", "solution": "The problem statement is evaluated as scientifically grounded, well-posed, objective, complete, and feasible. It represents a standard, non-trivial problem in the field of stochastic sensitivity analysis. Therefore, the problem is deemed **valid**, and a full solution is presented below.\n\nThe objective is to derive and analyze the pathwise gradient estimator for a parameter in an Ornstein-Uhlenbeck (OU) process. The procedure involves several steps: solving the primary SDE, deriving and solving the sensitivity SDE, constructing the estimator, calculating its expectation, and comparing this to the true analytical derivative.\n\nLet the OU process be given by the SDE:\n$$\ndX_{t}^{\\theta} = -\\theta X_{t}^{\\theta}\\,dt + \\sigma\\,dW_{t}, \\quad X_{0}^{\\theta} = x_{0}\n$$\nwhere $\\lambda(\\theta) = \\theta$.\n\n**1. Closed-Form Solution for $X_{t}^{\\theta}$**\n\nThis is a linear SDE. We can solve it using an integrating factor, $I_{t} = \\exp(\\theta t)$.\nApplying Itô's product rule to $I_{t}X_{t}^{\\theta}$:\n$$\nd(\\exp(\\theta t) X_{t}^{\\theta}) = \\theta \\exp(\\theta t) X_{t}^{\\theta}\\,dt + \\exp(\\theta t) dX_{t}^{\\theta}\n$$\nSubstituting $dX_{t}^{\\theta}$:\n$$\nd(\\exp(\\theta t) X_{t}^{\\theta}) = \\theta \\exp(\\theta t) X_{t}^{\\theta}\\,dt + \\exp(\\theta t) (-\\theta X_{t}^{\\theta}\\,dt + \\sigma\\,dW_{t}) = \\sigma \\exp(\\theta t)\\,dW_{t}\n$$\nIntegrating from $s=0$ to $s=t$:\n$$\n\\exp(\\theta t) X_{t}^{\\theta} - \\exp(0) X_{0}^{\\theta} = \\int_{0}^{t} \\sigma \\exp(\\theta s)\\,dW_{s}\n$$\nSolving for $X_{t}^{\\theta}$ with $X_{0}^{\\theta} = x_0$:\n$$\nX_{t}^{\\theta} = x_{0}\\exp(-\\theta t) + \\sigma \\int_{0}^{t} \\exp(-\\theta(t-s))\\,dW_{s}\n$$\nAt the terminal time $T$, we have:\n$$\nX_{T}^{\\theta} = x_{0}\\exp(-\\theta T) + \\sigma \\int_{0}^{T} \\exp(-\\theta(T-s))\\,dW_{s}\n$$\n\n**2. Derivation and Solution of the Sensitivity Process $Y_{t} = \\nabla_{\\theta} X_{t}^{\\theta}$**\n\nThe pathwise derivative process $Y_{t} = \\nabla_{\\theta} X_{t}^{\\theta}$ is found by formally differentiating the SDE for $X_{t}^{\\theta}$ with respect to $\\theta$. The interchange of differentiation and stochastic integration is permissible here because the SDE coefficients are smooth in $\\theta$ and the diffusion coefficient $\\sigma$ is independent of $\\theta$.\n$$\ndY_{t} = d(\\nabla_{\\theta} X_{t}^{\\theta}) = \\nabla_{\\theta}(dX_{t}^{\\theta}) = \\nabla_{\\theta}(-\\theta X_{t}^{\\theta}\\,dt + \\sigma\\,dW_{t})\n$$\n$$\ndY_{t} = -(\\nabla_{\\theta}(\\theta X_{t}^{\\theta}))\\,dt = -((\\nabla_{\\theta}\\theta) X_{t}^{\\theta} + \\theta (\\nabla_{\\theta} X_{t}^{\\theta}))\\,dt = -(X_{t}^{\\theta} + \\theta Y_{t})\\,dt\n$$\nThis gives the linear ordinary differential equation (for each sample path) for $Y_{t}$:\n$$\ndY_{t} = -\\theta Y_{t}\\,dt - X_{t}^{\\theta}\\,dt\n$$\nThe initial condition is $Y_{0} = \\nabla_{\\theta} X_{0}^{\\theta} = \\nabla_{\\theta} x_{0} = 0$, since $x_{0}$ is independent of $\\theta$.\nWe solve this ODE using the integrating factor $\\exp(\\theta t)$:\n$$\nd(\\exp(\\theta t) Y_{t}) = \\exp(\\theta t)(dY_{t} + \\theta Y_{t}\\,dt) = -\\exp(\\theta t)X_{t}^{\\theta}\\,dt\n$$\nIntegrating from $s=0$ to $s=t$:\n$$\n\\exp(\\theta t) Y_{t} - Y_0 = -\\int_{0}^{t} \\exp(\\theta s) X_{s}^{\\theta}\\,ds\n$$\nWith $Y_0=0$, we have $Y_{t} = -\\exp(-\\theta t) \\int_{0}^{t} \\exp(\\theta s) X_{s}^{\\theta}\\,ds$.\nSubstituting the expression for $X_{s}^{\\theta}$:\n$$\nY_{t} = -\\exp(-\\theta t) \\int_{0}^{t} \\exp(\\theta s) \\left( x_{0}\\exp(-\\theta s) + \\sigma\\int_{0}^{s} \\exp(-\\theta(s-u))\\,dW_{u} \\right)\\,ds\n$$\n$$\nY_{t} = -\\exp(-\\theta t) \\int_{0}^{t} \\left( x_{0} + \\sigma\\int_{0}^{s} \\exp(\\theta u)\\,dW_{u} \\right)\\,ds\n$$\n$$\nY_{t} = -\\exp(-\\theta t) \\left( x_{0}t + \\sigma \\int_{0}^{t} \\int_{0}^{s} \\exp(\\theta u)\\,dW_{u}\\,ds \\right)\n$$\nUsing the stochastic Fubini theorem to change the order of integration:\n$$\n\\int_{0}^{t} \\int_{0}^{s} \\exp(\\theta u)\\,dW_{u}\\,ds = \\int_{0}^{t} (t-u)\\exp(\\theta u)\\,dW_{u}\n$$\nThus, the sensitivity process is:\n$$\nY_{t} = -x_{0}t\\exp(-\\theta t) - \\sigma\\exp(-\\theta t)\\int_{0}^{t} (t-u)\\exp(\\theta u)\\,dW_{u} = -x_{0}t\\exp(-\\theta t) - \\sigma\\int_{0}^{t} (t-u)\\exp(-\\theta(t-u))\\,dW_{u}\n$$\nAt the terminal time $T$:\n$$\nY_{T} = -x_{0}T\\exp(-\\theta T) - \\sigma\\int_{0}^{T} (T-u)\\exp(-\\theta(T-u))\\,dW_{u}\n$$\n\n**3. The Pathwise Gradient Estimator $G(\\theta)$ and its Expectation**\n\nThe estimator is $G(\\theta) = f'(X_{T}^{\\theta})Y_{T}$. With $f(x)=x^2$, we have $f'(x)=2x$.\n$$\nG(\\theta) = 2X_{T}^{\\theta}Y_{T}\n$$\nWe now compute its expectation, $\\mathbb{E}[G(\\theta)]$:\n$$\n\\mathbb{E}[G(\\theta)] = 2\\,\\mathbb{E}[X_{T}^{\\theta}Y_{T}]\n$$\nLet's expand the product $X_{T}^{\\theta}Y_{T}$:\n$$\nX_{T}^{\\theta}Y_{T} = \\left( x_{0}\\exp(-\\theta T) + \\sigma\\int_{0}^{T} \\exp(-\\theta(T-s))\\,dW_{s} \\right) \\left( -x_{0}T\\exp(-\\theta T) - \\sigma\\int_{0}^{T} (T-u)\\exp(-\\theta(T-u))\\,dW_{u} \\right)\n$$\nTaking the expectation, the cross-terms involving a single Itô integral are zero, because the expectation of an Itô integral with a deterministic integrand is zero.\n$$\n\\mathbb{E}[X_{T}^{\\theta}Y_{T}] = -x_{0}^{2}T\\exp(-2\\theta T) - \\sigma^{2}\\,\\mathbb{E}\\left[ \\left(\\int_{0}^{T} \\exp(-\\theta(T-s))\\,dW_{s}\\right) \\left(\\int_{0}^{T} (T-u)\\exp(-\\theta(T-u))\\,dW_{u}\\right) \\right]\n$$\nUsing the Itô isometry property $\\mathbb{E}[(\\int g_1 dW)(\\int g_2 dW)] = \\int g_1 g_2 ds$:\n$$\n\\mathbb{E}[\\dots] = \\int_{0}^{T} \\exp(-\\theta(T-s)) \\cdot (T-s)\\exp(-\\theta(T-s))\\,ds = \\int_{0}^{T} (T-s)\\exp(-2\\theta(T-s))\\,ds\n$$\nLet $v=T-s$, so $dv=-ds$. The integral becomes $\\int_{T}^{0} v\\exp(-2\\theta v)(-dv) = \\int_{0}^{T} v\\exp(-2\\theta v)\\,dv$.\nWe compute this integral using integration by parts:\n\\begin{align*}\n\\int_{0}^{T} v\\exp(-2\\theta v)\\,dv &= \\left[v \\left(\\frac{\\exp(-2\\theta v)}{-2\\theta}\\right)\\right]_{0}^{T} - \\int_{0}^{T} \\frac{\\exp(-2\\theta v)}{-2\\theta} \\cdot 1 \\,dv \\\\\n&= -\\frac{T}{2\\theta}\\exp(-2\\theta T) + \\frac{1}{2\\theta} \\int_{0}^{T} \\exp(-2\\theta v)\\,dv \\\\\n&= -\\frac{T}{2\\theta}\\exp(-2\\theta T) + \\frac{1}{2\\theta} \\left[\\frac{\\exp(-2\\theta v)}{-2\\theta}\\right]_{0}^{T} \\\\\n&= -\\frac{T}{2\\theta}\\exp(-2\\theta T) - \\frac{1}{4\\theta^2}(\\exp(-2\\theta T) - 1)\n\\end{align*}\nSubstituting this back into the expectation for $G(\\theta)$:\n$$\n\\mathbb{E}[G(\\theta)] = 2 \\left( -x_{0}^{2}T\\exp(-2\\theta T) - \\sigma^{2}\\left(-\\frac{T}{2\\theta}\\exp(-2\\theta T) - \\frac{1}{4\\theta^2}\\exp(-2\\theta T) + \\frac{1}{4\\theta^2}\\right) \\right)\n$$\n$$\n\\mathbb{E}[G(\\theta)] = -2x_{0}^{2}T\\exp(-2\\theta T) + \\frac{\\sigma^{2}T}{\\theta}\\exp(-2\\theta T) + \\frac{\\sigma^{2}}{2\\theta^2}\\exp(-2\\theta T) - \\frac{\\sigma^{2}}{2\\theta^2}\n$$\n\n**4. Analytic Derivative of $\\mathbb{E}[f(X_{T}^{\\theta})]$**\n\nFirst, we find $F(\\theta) = \\mathbb{E}[f(X_{T}^{\\theta})] = \\mathbb{E}[(X_{T}^{\\theta})^2]$. For any random variable $Z$, $\\mathbb{E}[Z^2] = \\text{Var}(Z) + (\\mathbb{E}[Z])^2$.\nFrom the solution for $X_{T}^{\\theta}$, it is a Gaussian random variable with:\nMean: $\\mathbb{E}[X_{T}^{\\theta}] = x_{0}\\exp(-\\theta T)$.\nVariance: $\\text{Var}(X_{T}^{\\theta}) = \\mathbb{E}[(\\sigma \\int_{0}^{T} \\exp(-\\theta(T-s))\\,dW_{s})^2] = \\sigma^2 \\int_{0}^{T} \\exp(-2\\theta(T-s))\\,ds$.\nLetting $v=T-s$, the integral is $\\int_{0}^{T} \\exp(-2\\theta v)\\,dv = \\frac{1-\\exp(-2\\theta T)}{2\\theta}$.\nSo, $\\text{Var}(X_{T}^{\\theta}) = \\frac{\\sigma^2}{2\\theta}(1-\\exp(-2\\theta T))$.\nTherefore, the quantity of interest is:\n$$\nF(\\theta) = \\mathbb{E}[(X_{T}^{\\theta})^2] = \\frac{\\sigma^2}{2\\theta}(1-\\exp(-2\\theta T)) + (x_{0}\\exp(-\\theta T))^2\n$$\n$$\nF(\\theta) = \\frac{\\sigma^2}{2\\theta} - \\frac{\\sigma^2}{2\\theta}\\exp(-2\\theta T) + x_{0}^{2}\\exp(-2\\theta T)\n$$\nNow, we differentiate $F(\\theta)$ with respect to $\\theta$:\n$$\n\\nabla_{\\theta}F(\\theta) = \\frac{d}{d\\theta}\\left(\\frac{\\sigma^2}{2}\\theta^{-1} - \\frac{\\sigma^2}{2}\\theta^{-1}\\exp(-2\\theta T) + x_0^2\\exp(-2\\theta T)\\right)\n$$\nDifferentiating term by term:\n\\begin{align*}\n\\nabla_{\\theta}F(\\theta) &= \\frac{\\sigma^2}{2}(-\\theta^{-2}) - \\frac{\\sigma^2}{2}(-\\theta^{-2}\\exp(-2\\theta T) + \\theta^{-1}(-2T)\\exp(-2\\theta T)) + x_0^2(-2T)\\exp(-2\\theta T) \\\\\n&= -\\frac{\\sigma^2}{2\\theta^2} + \\frac{\\sigma^2}{2\\theta^2}\\exp(-2\\theta T) + \\frac{\\sigma^2 T}{\\theta}\\exp(-2\\theta T) - 2x_0^2 T \\exp(-2\\theta T)\n\\end{align*}\nRearranging terms to match the previous result:\n$$\n\\nabla_{\\theta}F(\\theta) = -2x_0^2 T \\exp(-2\\theta T) + \\frac{\\sigma^2 T}{\\theta}\\exp(-2\\theta T) + \\frac{\\sigma^2}{2\\theta^2}\\exp(-2\\theta T) - \\frac{\\sigma^2}{2\\theta^2}\n$$\n\n**5. Comparison and Conclusion**\n\nComparing the expression for $\\mathbb{E}[G(\\theta)]$ from Section 3 and $\\nabla_{\\theta}F(\\theta)$ from Section 4, we find they are identical:\n$$\n\\mathbb{E}[G(\\theta)] = \\nabla_{\\theta}\\mathbb{E}[f(X_{T}^{\\theta})]\n$$\nThis demonstrates that the pathwise gradient estimator $G(\\theta) = f'(X_{T}^{\\theta})\\,Y_{T}$ is an unbiased estimator of the true gradient $\\nabla_{\\theta}F(\\theta)$. The validity of interchanging the expectation and differentiation operators, $\\nabla_{\\theta}\\mathbb{E}[\\cdot] = \\mathbb{E}[\\nabla_{\\theta}\\cdot]$, is guaranteed under the problem's conditions by the dominated convergence theorem, due to the smoothness of the SDE's coefficients with respect to the parameter $\\theta$.\n\nThe final answer is the closed-form expression for $\\nabla_{\\theta}F(\\theta)$. It can be written more compactly as:\n$$\n\\nabla_{\\theta}\\mathbb{E}[f(X_{T}^{\\theta})] = -2x_0^2 T \\exp(-2\\theta T) + \\frac{\\sigma^2}{2\\theta^2}\\left[(1+2\\theta T)\\exp(-2\\theta T) - 1\\right]\n$$\nThis is the required analytical expression.", "answer": "$$\\boxed{-2x_0^2 T \\exp(-2\\theta T) + \\frac{\\sigma^2}{2\\theta^2} \\left[ (1+2\\theta T)\\exp(-2\\theta T) - 1 \\right]}$$", "id": "3328507"}, {"introduction": "After mastering the basic application, it is crucial to understand the method's limitations. This practice explores what happens when the function of interest—the payoff—is discontinuous, a common scenario in financial engineering and decision theory. You will investigate the pricing of a digital option, whose payoff is an indicator function, and discover why the naive pathwise estimator is fundamentally biased and fails in practice [@problem_id:3328492]. The exercise then guides you to a powerful solution using a technique from Malliavin calculus, which cleverly uses an integration-by-parts formula to circumvent the problematic discontinuity, yielding a correct and unbiased sensitivity estimate.", "problem": "Consider the Black–Scholes–Merton (BSM) model under the risk-neutral measure. The underlying asset price $\\{S_{t}\\}_{t \\in [0,T]}$ satisfies the stochastic differential equation $dS_{t} = r S_{t}\\,dt + \\sigma S_{t}\\,dW_{t}$ with $S_{0} > 0$, risk-free rate $r \\in \\mathbb{R}$, and volatility $\\sigma > 0$, where $\\{W_{t}\\}$ is a standard Brownian motion. Let the maturity be $T > 0$ and consider the cash-or-nothing digital call payoff $f(S_{T}) = \\mathbf{1}_{\\{S_{T} > K\\}}$ with strike $K > 0$. The risk-neutral price is $V(S_{0}) = \\mathbb{E}\\!\\left[\\exp(-rT)\\,f(S_{T})\\right]$ and the Delta is $\\Delta(S_{0}) = \\frac{\\partial}{\\partial S_{0}} V(S_{0})$.\n\nUsing only fundamental principles of risk-neutral pricing, the explicit strong solution of the BSM stochastic differential equation, and the core definitions of the Malliavin derivative and Skorohod integral in Malliavin calculus (no further shortcut formulas may be assumed), do the following:\n\n- Explain, from first principles, why the pathwise (a.k.a. infinitesimal perturbation analysis) estimator $\\mathbb{E}\\!\\left[\\exp(-rT)\\,\\frac{\\partial}{\\partial S_{0}} f(S_{T})\\right]$ is biased for this discontinuous payoff, whereas the Malliavin calculus estimator obtained via an integration-by-parts argument is unbiased.\n\n- Then compute the Delta $\\Delta(S_{0})$ in closed form by deriving a Malliavin integration-by-parts representation that does not differentiate the discontinuous payoff and reducing the resulting expression to a standard normal expectation.\n\nProvide your final answer as a single closed-form analytic expression in terms of $S_{0}$, $K$, $r$, $\\sigma$, and $T$ using the standard normal density function $\\varphi(\\cdot)$ and, if needed, the standard normal cumulative distribution function $\\Phi(\\cdot)$. No numerical rounding is required, and no units are to be included in the final answer.", "solution": "The problem is valid as it is scientifically grounded in the established theories of financial mathematics and stochastic calculus, is well-posed, objective, and self-contained.\n\nThis problem requires a two-part analysis of the Delta of a cash-or-nothing digital call option in the Black-Scholes-Merton (BSM) model. First, we explain the failure of the pathwise derivative estimator for this option's discontinuous payoff. Second, we derive the correct closed-form expression for the Delta using an integration-by-parts technique, which is the foundation of the Malliavin calculus approach for this type of problem.\n\nThe underlying asset price $S_t$ follows a Geometric Brownian Motion under the risk-neutral measure $\\mathbb{Q}$:\n$$dS_{t} = r S_{t}\\,dt + \\sigma S_{t}\\,dW_{t}$$\nThe explicit solution for the asset price at maturity $T$ given an initial price $S_0$ is:\n$$S_{T} = S_{0} \\exp\\left(\\left(r - \\frac{1}{2}\\sigma^{2}\\right)T + \\sigma W_{T}\\right)$$\nwhere $W_T$ is a normally distributed random variable with mean $0$ and variance $T$, i.e., $W_T \\sim \\mathcal{N}(0, T)$.\n\nThe payoff function is that of a cash-or-nothing digital call: $f(S_{T}) = \\mathbf{1}_{\\{S_{T} > K\\}}$, where $\\mathbf{1}_{\\{\\cdot\\}}$ is the indicator function. The price of the option is $V(S_{0}) = \\mathbb{E}_{\\mathbb{Q}}\\!\\left[\\exp(-rT)\\,f(S_{T})\\right]$, and the Delta is defined as $\\Delta(S_{0}) = \\frac{\\partial}{\\partial S_{0}} V(S_{0})$.\n\n**Part 1: Bias of the Pathwise Derivative Estimator**\n\nThe Delta is given by $\\Delta(S_{0}) = \\frac{\\partial}{\\partial S_{0}} \\left( \\exp(-rT) \\mathbb{E}[f(S_T)] \\right)$. Since $\\exp(-rT)$ is a constant with respect to $S_0$, we have:\n$$\\Delta(S_{0}) = \\exp(-rT) \\frac{\\partial}{\\partial S_{0}} \\mathbb{E}[f(S_T)]$$\nThe pathwise derivative estimator, also known as Infinitesimal Perturbation Analysis (IPA), proceeds by interchanging the differentiation and expectation operators. If this interchange were valid, we would have:\n$$\\Delta(S_{0}) \\stackrel{?}{=} \\exp(-rT) \\mathbb{E}\\left[\\frac{\\partial}{\\partial S_{0}} f(S_T)\\right]$$\nUsing the chain rule, the derivative inside the expectation is:\n$$\\frac{\\partial}{\\partial S_{0}} f(S_T) = f'(S_T) \\cdot \\frac{\\partial S_T}{\\partial S_0}$$\nFrom the explicit solution for $S_T$, we compute the second term:\n$$\\frac{\\partial S_T}{\\partial S_0} = \\frac{\\partial}{\\partial S_0} \\left( S_{0} \\exp\\left(\\left(r - \\frac{1}{2}\\sigma^{2}\\right)T + \\sigma W_{T}\\right) \\right) = \\exp\\left(\\left(r - \\frac{1}{2}\\sigma^{2}\\right)T + \\sigma W_{T}\\right) = \\frac{S_T}{S_0}$$\nThe payoff function $f(x) = \\mathbf{1}_{\\{x > K\\}}$ is a Heaviside step function centered at $K$. Its derivative, in the sense of distributions, is the Dirac delta function, $f'(x) = \\delta(x-K)$. Substituting these into the pathwise expression gives:\n$$\\Delta_{\\text{pathwise}} = \\exp(-rT) \\mathbb{E}\\left[\\delta(S_T - K) \\frac{S_T}{S_0}\\right]$$\nThis estimator is fundamentally flawed for two reasons:\n$1$. **Invalid Interchange:** The interchange of a derivative and an expectation requires certain regularity conditions on the integrand, such as those provided by the Dominated Convergence Theorem. The payoff function $f(S_T)$ is discontinuous for any path where $S_T = K$. The function $\\frac{\\partial}{\\partial S_0} f(S_T)$ is a scaled Dirac delta distribution, not a function, and the conditions for interchanging the operators are not met. The interchange is mathematically invalid.\n$2$. **Computational Failure:** In a Monte Carlo simulation, we generate a large number of paths for $S_T$. Since $S_T$ is a continuous random variable, the probability of any single path yielding $S_T = K$ is exactly zero. Therefore, the term $\\delta(S_T - K)$ would evaluate to $0$ for almost every simulated path. The resulting Monte Carlo estimate for Delta would be $0$, which is incorrect. The true Delta is non-zero. This demonstrates the severe bias of the naive pathwise estimator.\n\nThe Malliavin calculus approach circumvents this by using an integration-by-parts formula to transfer the differentiation from the discontinuous payoff function onto a smooth component of the expectation, resulting in an unbiased estimator.\n\n**Part 2: Derivation of Delta using Malliavin Integration-by-Parts**\n\nWe will derive the closed-form expression for Delta by avoiding differentiation of the indicator function. The core idea is to express the expectation as an explicit integral and use standard integration by parts.\n\nLet $Z = W_T / \\sqrt{T}$, so $Z \\sim \\mathcal{N}(0, 1)$ under $\\mathbb{Q}$. We can write $S_T$ as a function of $S_0$ and the random variable $z$:\n$$S_{T}(z) = S_{0} \\exp\\left(\\left(r - \\frac{1}{2}\\sigma^{2}\\right)T + \\sigma\\sqrt{T} z\\right)$$\nThe expectation becomes an integral with respect to the standard normal probability density function, $\\varphi(z) = \\frac{1}{\\sqrt{2\\pi}} \\exp(-z^2/2)$:\n$$\\mathbb{E}[f(S_T)] = \\int_{-\\infty}^{\\infty} f(S_T(z)) \\varphi(z) dz$$\nNow, we can compute the derivative of the expectation with respect to $S_0$. Since the integrand is sufficiently regular with respect to $S_0$, we can differentiate under the integral sign:\n$$\\frac{\\partial}{\\partial S_{0}} \\mathbb{E}[f(S_T)] = \\int_{-\\infty}^{\\infty} \\frac{\\partial}{\\partial S_{0}} \\left[ f(S_T(z)) \\right] \\varphi(z) dz$$\nApplying the chain rule:\n$$\\frac{\\partial}{\\partial S_0} f(S_T(z)) = f'(S_T(z)) \\frac{\\partial S_T(z)}{\\partial S_0} = f'(S_T(z)) \\frac{S_T(z)}{S_0}$$\nThis brings us back to the problematic $f'$ term:\n$$\\frac{\\partial}{\\partial S_{0}} \\mathbb{E}[f(S_T)] = \\frac{1}{S_0} \\int_{-\\infty}^{\\infty} f'(S_T(z)) S_T(z) \\varphi(z) dz$$\nThis is where the integration-by-parts argument, which is a specific instance of the Malliavin calculus principle, is applied. We relate the derivative with respect to the argument of $f$ to the derivative with respect to $z$:\n$$\\frac{df(S_T(z))}{dz} = f'(S_T(z)) \\frac{d S_T(z)}{dz} = f'(S_T(z)) \\left( S_0 \\exp\\left(\\dots\\right) \\cdot \\sigma\\sqrt{T} \\right) = f'(S_T(z)) S_T(z) \\sigma\\sqrt{T}$$\nWe can thus express $f'(S_T(z)) S_T(z)$ without the prime:\n$$f'(S_T(z)) S_T(z) = \\frac{1}{\\sigma\\sqrt{T}} \\frac{df(S_T(z))}{dz}$$\nSubstituting this into the integral for the sensitivity:\n$$\\frac{\\partial}{\\partial S_0} \\mathbb{E}[f(S_T)] = \\frac{1}{S_0 \\sigma \\sqrt{T}} \\int_{-\\infty}^{\\infty} \\frac{df(S_T(z))}{dz} \\varphi(z) dz$$\nNow we perform standard integration by parts on the integral with respect to $z$. Let $u(z) = \\varphi(z)$ and $dv = \\frac{df(S_T(z))}{dz} dz$. Then $du = \\varphi'(z) dz$ and $v = f(S_T(z))$.\n$$ \\int_{-\\infty}^{\\infty} u dv = \\left[ uv \\right]_{-\\infty}^{\\infty} - \\int_{-\\infty}^{\\infty} v du = \\left[ \\varphi(z) f(S_T(z)) \\right]_{-\\infty}^{\\infty} - \\int_{-\\infty}^{\\infty} f(S_T(z)) \\varphi'(z) dz $$\nThe boundary term vanishes because $\\varphi(z) \\to 0$ as $z \\to \\pm\\infty$, and $f(S_T(z))$ is bounded (by $0$ and $1$). For the remaining integral, we use the identity $\\varphi'(z) = -z\\varphi(z)$:\n$$ - \\int_{-\\infty}^{\\infty} f(S_T(z)) (-z\\varphi(z)) dz = \\int_{-\\infty}^{\\infty} f(S_T(z)) z \\varphi(z) dz = \\mathbb{E}[f(S_T) Z] $$\nCombining our results, we have the integration-by-parts formula for the derivative:\n$$\\frac{\\partial}{\\partial S_0} \\mathbb{E}[f(S_T)] = \\frac{1}{S_0 \\sigma \\sqrt{T}} \\mathbb{E}[f(S_T) Z] = \\frac{1}{S_0 \\sigma T} \\mathbb{E}[f(S_T) W_T]$$\nThis gives us a representation for Delta that does not involve differentiating the payoff function:\n$$\\Delta(S_0) = \\exp(-rT) \\frac{\\mathbb{E}[f(S_T) W_T]}{S_0 \\sigma T}$$\nThis representation is unbiased and suitable for Monte Carlo estimation.\n\nFinally, we compute the closed-form solution by evaluating the expectation:\n$$\\mathbb{E}[f(S_T) Z] = \\mathbb{E}[\\mathbf{1}_{\\{S_T > K\\}} Z] = \\int_{-\\infty}^{\\infty} \\mathbf{1}_{\\{S_T(z) > K\\}} z \\varphi(z) dz$$\nThe condition $S_T(z) > K$ defines the integration region for $z$:\n$$S_0 \\exp\\left(\\left(r - \\frac{\\sigma^2}{2}\\right)T + \\sigma\\sqrt{T} z\\right) > K$$\n$$\\ln(S_0) + \\left(r - \\frac{\\sigma^2}{2}\\right)T + \\sigma\\sqrt{T} z > \\ln(K)$$\n$$\\sigma\\sqrt{T} z > \\ln(K/S_0) - \\left(r - \\frac{\\sigma^2}{2}\\right)T$$\n$$z > \\frac{\\ln(K/S_0) - (r - \\frac{\\sigma^2}{2})T}{\\sigma\\sqrt{T}} = - \\frac{\\ln(S_0/K) + (r - \\frac{\\sigma^2}{2})T}{\\sigma\\sqrt{T}}$$\nLet's define the standard BSM term $d_2$:\n$$d_2 = \\frac{\\ln(S_0/K) + (r - \\frac{\\sigma^2}{2})T}{\\sigma\\sqrt{T}}$$\nThe condition becomes $z > -d_2$. The expectation integral is thus:\n$$\\mathbb{E}[f(S_T) Z] = \\int_{-d_2}^{\\infty} z \\varphi(z) dz = \\int_{-d_2}^{\\infty} z \\frac{1}{\\sqrt{2\\pi}} \\exp(-z^2/2) dz$$\nThis integral can be solved directly:\n$$\\int_{-d_2}^{\\infty} z \\frac{1}{\\sqrt{2\\pi}} \\exp(-z^2/2) dz = -\\frac{1}{\\sqrt{2\\pi}} \\left[ \\exp(-z^2/2) \\right]_{-d_2}^{\\infty} = -\\frac{1}{\\sqrt{2\\pi}} (0 - \\exp(-(-d_2)^2/2)) = \\frac{1}{\\sqrt{2\\pi}} \\exp(-d_2^2/2) = \\varphi(d_2)$$\nSubstituting this result back into our expression for Delta:\n$$\\Delta(S_0) = \\exp(-rT) \\frac{1}{S_0 \\sigma \\sqrt{T}} \\mathbb{E}[f(S_T) Z] = \\frac{\\exp(-rT) \\varphi(d_2)}{S_0 \\sigma \\sqrt{T}}$$\n\nThis is the final closed-form expression for the Delta of a cash-or-nothing digital call option. It correctly depends on the probability density at the decision boundary, which is what the $\\varphi(d_2)$ term represents.", "answer": "$$\\boxed{\\frac{\\exp(-rT)\\,\\varphi\\left(\\frac{\\ln(S_{0}/K) + (r - \\frac{1}{2}\\sigma^{2})T}{\\sigma\\sqrt{T}}\\right)}{S_{0}\\sigma\\sqrt{T}}}$$", "id": "3328492"}, {"introduction": "This final exercise exposes a more subtle failure mode of pathwise estimators, where the differentiability breaks down not in the payoff function, but in the very structure of the stochastic process path. You will analyze a reflected process confined to a quadrant, a model often used for queueing systems or other constrained processes. The challenge is to show how an infinitesimal change in a parameter of the reflection mechanism can lead to a non-differentiable change in the path's trajectory [@problem_id:3328514]. By calculating the left-hand and right-hand derivatives of a sample path performance, you will gain a profound insight into the geometric conditions required for pathwise estimators to be valid and appreciate why systems with state-dependent dynamics often require more sophisticated analysis.", "problem": "Consider a two-dimensional reflected process in the nonnegative orthant defined by the Skorokhod problem. Let $X^{\\epsilon}(t)$ be the unique solution to\n$$\nX^{\\epsilon}(t) \\;=\\; Y(t) \\;+\\; R(\\epsilon)\\,L^{\\epsilon}(t),\n$$\nwhere $R(\\epsilon) \\in \\mathbb{R}^{2 \\times 2}$ is the reflection matrix,\n$$\nR(\\epsilon) \\;=\\; \\begin{pmatrix} 1 & \\epsilon \\\\ 0 & 1 \\end{pmatrix},\n$$\n$Y(t)$ is a given driving path, and $L^{\\epsilon}(t) = \\big(L_{1}^{\\epsilon}(t), L_{2}^{\\epsilon}(t)\\big)^{\\top}$ is a vector of nondecreasing regulator processes with $L_{i}^{\\epsilon}(0)=0$ such that $X_{i}^{\\epsilon}(t) \\ge 0$ for all $t \\in [0,2]$, and $L_{i}^{\\epsilon}$ increases only when $X_{i}^{\\epsilon}(t)=0$, enforcing the minimality condition that the cumulative reflection is the smallest necessary to keep $X^{\\epsilon}(t)$ in the orthant.\n\nLet the driving path $Y(t)$ be absolutely continuous with velocity $u(t)$ given by\n$$\nu(t) \\;=\\; \\begin{cases}\n(-1,\\,-1), & t \\in [0,1), \\\\\n(0,\\,-1), & t \\in [1,2],\n\\end{cases}\n$$\nand $X^{\\epsilon}(0) = (0,0)$. Define the performance functional\n$$\nF(\\epsilon) \\;=\\; X_{1}^{\\epsilon}(2),\n$$\nwhich returns the first coordinate of the reflected state at time $t=2$.\n\nStarting from the basic Skorokhod reflection principles and the complementarity conditions, derive the dependence of $F(\\epsilon)$ on $\\epsilon$ for $\\epsilon$ in a neighborhood of $0$, and compute the left-hand and right-hand pathwise derivatives of $F$ at $\\epsilon=0$,\n$$\n\\partial_{-} F(0) \\quad \\text{and} \\quad \\partial_{+} F(0).\n$$\nExpress your final answer as a pair in a single row matrix using LaTeX’s $\\mathrm{pmatrix}$ environment. Then, explain why this example exhibits non-differentiable dependence of the sample path on the reflection matrix and discuss the implications for unbiased pathwise gradient estimation in stochastic simulation and Monte Carlo methods. No rounding is required, and no units are involved in the final answer.", "solution": "The problem asks for the left-hand and right-hand pathwise derivatives of a performance functional $F(\\epsilon) = X_{1}^{\\epsilon}(2)$ with respect to a parameter $\\epsilon$ in the reflection matrix $R(\\epsilon)$. The process $X^{\\epsilon}(t)$ is a two-dimensional reflected process in the nonnegative orthant, governed by the Skorokhod problem:\n$$\nX^{\\epsilon}(t) = Y(t) + R(\\epsilon)L^{\\epsilon}(t)\n$$\nThe components of this equation are:\n1.  The state vector $X^{\\epsilon}(t) = (X_{1}^{\\epsilon}(t), X_{2}^{\\epsilon}(t))^{\\top}$ which must remain in the nonnegative orthant, i.e., $X_{i}^{\\epsilon}(t) \\ge 0$ for $i \\in \\{1, 2\\}$ and for all $t \\ge 0$.\n2.  The driving path $Y(t)$, which is absolutely continuous with initial condition $Y(0)=(0,0)$ and velocity $u(t) = \\frac{dY}{dt}$ given by:\n    $$\n    u(t) = \\begin{cases} (-1, -1), & t \\in [0,1) \\\\ (0, -1), & t \\in [1,2] \\end{cases}\n    $$\n    Integrating the velocity, we obtain the driving path:\n    $$\n    Y(t) = \\begin{cases} (-t, -t), & t \\in [0,1] \\\\ (-1, -t), & t \\in (1,2] \\end{cases}\n    $$\n3.  The reflection matrix $R(\\epsilon) = \\begin{pmatrix} 1 & \\epsilon \\\\ 0 & 1 \\end{pmatrix}$.\n4.  The regulator process $L^{\\epsilon}(t) = (L_{1}^{\\epsilon}(t), L_{2}^{\\epsilon}(t))^{\\top}$, where each component $L_{i}^{\\epsilon}(t)$ is a nondecreasing process starting at $L_{i}^{\\epsilon}(0)=0$. The minimality (or complementarity) condition states that $L_{i}^{\\epsilon}$ can increase only when the state is on the boundary, i.e., $X_{i}^{\\epsilon}(t)=0$. This is formally written as $\\int_0^T X_{i}^{\\epsilon}(t) dL_{i}^{\\epsilon}(t) = 0$ for any $T \\ge 0$.\n\nThe initial condition is $X^{\\epsilon}(0) = (0,0)$. We need to find the value of $F(\\epsilon) = X_{1}^{\\epsilon}(2)$ for $\\epsilon$ in a neighborhood of $0$ and then compute $\\partial_{-} F(0)$ and $\\partial_{+} F(0)$.\n\nIn component form, the system is:\n$$\n\\begin{cases}\nX_{1}^{\\epsilon}(t) = Y_{1}(t) + L_{1}^{\\epsilon}(t) + \\epsilon L_{2}^{\\epsilon}(t) \\\\\nX_{2}^{\\epsilon}(t) = Y_{2}(t) + L_{2}^{\\epsilon}(t)\n\\end{cases}\n$$\n\nFirst, we analyze the case $\\epsilon=0$. The reflection matrix becomes the identity matrix, $R(0)=I$.\n$$\n\\begin{cases}\nX_{1}^{0}(t) = Y_{1}(t) + L_{1}^{0}(t) \\\\\nX_{2}^{0}(t) = Y_{2}(t) + L_{2}^{0}(t)\n\\end{cases}\n$$\nFor $t \\in [0,1]$:\nThe driving path is $Y(t)=(-t, -t)$. Starting from $X^{0}(0)=(0,0)$, both components of $Y(t)$ are decreasing. To keep $X_{1}^{0}(t) \\ge 0$ and $X_{2}^{0}(t) \\ge 0$, both regulators $L_{1}^{0}(t)$ and $L_{2}^{0}(t)$ must act. The minimal reflection keeps the process at the boundary, so $X^{0}(t)=(0,0)$ for $t \\in [0,1]$.\nThis implies $0 = -t + L_{1}^{0}(t)$ and $0 = -t + L_{2}^{0}(t)$.\nThus, $L_{1}^{0}(t) = t$ and $L_{2}^{0}(t) = t$ for $t \\in [0,1]$. These are nondecreasing, satisfying the condition.\nAt $t=1$, we have $X^{0}(1)=(0,0)$, $L_{1}^{0}(1)=1$, and $L_{2}^{0}(1)=1$.\n\nFor $t \\in (1,2]$:\nThe driving path is $Y(t)=(-1, -t)$, with velocity $u(t)=(0,-1)$. Starting from $X^{0}(1)=(0,0)$, the driving velocity for the first component is $0$. Therefore, there is no force pushing $X_{1}^{0}$ to become negative, so $L_{1}^{0}$ does not need to increase. From the complementarity condition, $dL_{1}^{0}(t)=0$ for $t \\in (1,2]$. Thus, $L_{1}^{0}(t)=L_{1}^{0}(1)=1$ for $t \\in [1,2]$.\nThe driving velocity for the second component is $-1$. To keep $X_{2}^{0}(t) \\ge 0$, $L_{2}^{0}$ must increase to counteract this. The minimal reflection keeps $X_{2}^{0}(t)$ on the boundary at $0$.\nSo, $X_{2}^{0}(t) = 0$ for $t \\in [1,2]$, which implies $0 = Y_{2}(t) + L_{2}^{0}(t) = -t + L_{2}^{0}(t)$. This gives $L_{2}^{0}(t)=t$. This is consistent with $L_{2}^{0}(1)=1$ and is nondecreasing.\nLet's verify the state for $t \\in [1,2]$:\n$X_{1}^{0}(t) = Y_{1}(t) + L_{1}^{0}(t) = -1 + 1 = 0$.\n$X_{2}^{0}(t) = Y_{2}(t) + L_{2}^{0}(t) = -t + t = 0$.\nSo, $X^{0}(t)=(0,0)$ for all $t \\in [0,2]$.\nThe performance functional at $\\epsilon=0$ is $F(0) = X_{1}^{0}(2) = 0$.\n\nNext, we analyze the case $\\epsilon > 0$ (for $\\epsilon$ small).\nFor $t \\in [0,1]$:\nWe hypothesize that $X^{\\epsilon}(t)=(0,0)$ as in the $\\epsilon=0$ case.\nFrom $X_{2}^{\\epsilon}(t)=0$, we get $0 = Y_{2}(t) + L_{2}^{\\epsilon}(t) = -t + L_{2}^{\\epsilon}(t)$, so $L_{2}^{\\epsilon}(t)=t$. This is nondecreasing.\nFrom $X_{1}^{\\epsilon}(t)=0$, we get $0 = Y_{1}(t) + L_{1}^{\\epsilon}(t) + \\epsilon L_{2}^{\\epsilon}(t) = -t + L_{1}^{\\epsilon}(t) + \\epsilon t$. This implies $L_{1}^{\\epsilon}(t) = (1-\\epsilon)t$. For small $\\epsilon>0$ (specifically $\\epsilon < 1$), this is nondecreasing. So this solution holds for $t \\in [0,1]$.\nAt $t=1$, we have $X^{\\epsilon}(1)=(0,0)$, $L_{1}^{\\epsilon}(1)=1-\\epsilon$, and $L_{2}^{\\epsilon}(1)=1$.\n\nFor $t \\in (1,2]$:\nThe driving velocity is $u(t)=(0,-1)$. At $t=1$, $X^{\\epsilon}(1)=(0,0)$. The dynamics are given by $dX_{1}^{\\epsilon} = dL_{1}^{\\epsilon} + \\epsilon dL_{2}^{\\epsilon}$ and $dX_{2}^{\\epsilon} = -dt + dL_{2}^{\\epsilon}$.\nSince $Y_{2}(t)$ is decreasing, $L_{2}^{\\epsilon}$ must increase to keep $X_{2}^{\\epsilon}(t) \\ge 0$. Let's assume $X_{2}^{\\epsilon}(t)$ is kept at $0$. Then $dX_{2}^{\\epsilon}=0$, which gives $dL_{2}^{\\epsilon}=dt$.\nSubstituting this into the equation for $X_{1}^{\\epsilon}$: $dX_{1}^{\\epsilon} = dL_{1}^{\\epsilon} + \\epsilon dt$.\nSince $\\epsilon > 0$, the term $\\epsilon dt$ represents a positive drift. As $X_{1}^{\\epsilon}(1)=0$, this drift will push the state into the interior, $X_{1}^{\\epsilon}(t) > 0$. By the complementarity condition, since $X_{1}^{\\epsilon}(t)>0$ for $t>1$, the regulator $L_{1}^{\\epsilon}$ cannot increase. Thus, $dL_{1}^{\\epsilon}=0$ for $t \\in (1,2]$.\nThis gives $dX_{1}^{\\epsilon} = \\epsilon dt$. Integrating from $t=1$ to a general $t \\in (1,2]$, we have:\n$X_{1}^{\\epsilon}(t) = X_{1}^{\\epsilon}(1) + \\int_{1}^{t} \\epsilon ds = 0 + \\epsilon(t-1) = \\epsilon(t-1)$.\nSince $\\epsilon>0$ and $t>1$, $X_{1}^{\\epsilon}(t)>0$, which is consistent with our assumption $dL_{1}^{\\epsilon}=0$.\nFor the regulators, $L_{1}^{\\epsilon}(t) = L_{1}^{\\epsilon}(1) = 1-\\epsilon$ for $t \\in [1,2]$. And $L_{2}^{\\epsilon}(t)$ must satisfy $X_{2}^{\\epsilon}(t) = Y_{2}(t) + L_{2}^{\\epsilon}(t) = -t + L_{2}^{\\epsilon}(t) = 0$, so $L_{2}^{\\epsilon}(t)=t$ for $t \\in [1,2]$.\nThis solution is consistent. The performance functional for $\\epsilon>0$ is $F(\\epsilon) = X_{1}^{\\epsilon}(2) = \\epsilon(2-1) = \\epsilon$.\nThe right-hand derivative at $\\epsilon=0$ is:\n$$\n\\partial_{+} F(0) = \\lim_{\\epsilon \\to 0^+} \\frac{F(\\epsilon) - F(0)}{\\epsilon} = \\lim_{\\epsilon \\to 0^+} \\frac{\\epsilon - 0}{\\epsilon} = 1\n$$\n\nFinally, we analyze the case $\\epsilon < 0$ (for $\\epsilon$ small).\nFor $t \\in [0,1]$:\nThe logic is identical to the $\\epsilon>0$ case. We assume $X^{\\epsilon}(t)=(0,0)$. This yields $L_{2}^{\\epsilon}(t)=t$ and $L_{1}^{\\epsilon}(t)=(1-\\epsilon)t$. Since $\\epsilon < 0$, $1-\\epsilon > 1$, so $L_{1}^{\\epsilon}(t)$ is nondecreasing. This is a valid solution.\nAt $t=1$, we have $X^{\\epsilon}(1)=(0,0)$, $L_{1}^{\\epsilon}(1)=1-\\epsilon$, and $L_{2}^{\\epsilon}(1)=1$.\n\nFor $t \\in (1,2]$:\nThe dynamics are $dX_{1}^{\\epsilon} = dL_{1}^{\\epsilon} + \\epsilon dL_{2}^{\\epsilon}$ and $dX_{2}^{\\epsilon} = -dt + dL_{2}^{\\epsilon}$.\nAs before, $X_{2}^{\\epsilon}$ is pushed negative, so $L_{2}^{\\epsilon}$ must increase. Let's assume $X_{2}^{\\epsilon}(t)=0$, which implies $dL_{2}^{\\epsilon}=dt$.\nSubstituting this into the equation for $X_{1}^{\\epsilon}$: $dX_{1}^{\\epsilon} = dL_{1}^{\\epsilon} + \\epsilon dt$.\nHere, since $\\epsilon < 0$, the term $\\epsilon dt$ represents a negative drift. This pushes $X_{1}^{\\epsilon}$ towards negative values. To prevent this, $X_{1}^{\\epsilon}$ must be held at the boundary $0$, which requires $L_{1}^{\\epsilon}$ to increase.\nSo, we assume both $X_{1}^{\\epsilon}(t)=0$ and $X_{2}^{\\epsilon}(t)=0$ for $t \\in [1,2]$.\nThis means $dX_{1}^{\\epsilon}=0$ and $dX_{2}^{\\epsilon}=0$.\nFrom $dX_{2}^{\\epsilon}=0$, we get $dL_{2}^{\\epsilon}=dt$.\nFrom $dX_{1}^{\\epsilon}=0$, we get $0 = dL_{1}^{\\epsilon} + \\epsilon dt$, so $dL_{1}^{\\epsilon} = -\\epsilon dt$. Since $\\epsilon < 0$, $dL_{1}^{\\epsilon} > 0$, so $L_{1}^{\\epsilon}$ is indeed increasing, which is consistent with $X_{1}^{\\epsilon}(t)=0$.\nLet's verify the full solution. For $t \\in [1,2]$:\n$L_1^{\\epsilon}(t) = L_1^{\\epsilon}(1) + \\int_1^t (-\\epsilon) ds = (1-\\epsilon) - \\epsilon(t-1) = 1-\\epsilon t$.\n$L_2^{\\epsilon}(t) = L_2^{\\epsilon}(1) + \\int_1^t ds = 1 + (t-1) = t$.\nNow check the state variables:\n$X_{1}^{\\epsilon}(t) = Y_{1}(t) + L_{1}^{\\epsilon}(t) + \\epsilon L_{2}^{\\epsilon}(t) = -1 + (1-\\epsilon t) + \\epsilon t = 0$.\n$X_{2}^{\\epsilon}(t) = Y_{2}(t) + L_{2}^{\\epsilon}(t) = -t + t = 0$.\nThe solution is consistent. Therefore, for $\\epsilon < 0$, $X^{\\epsilon}(t)=(0,0)$ for all $t \\in [0,2]$.\nThe performance functional for $\\epsilon < 0$ is $F(\\epsilon) = X_{1}^{\\epsilon}(2) = 0$.\nThe left-hand derivative at $\\epsilon=0$ is:\n$$\n\\partial_{-} F(0) = \\lim_{\\epsilon \\to 0^-} \\frac{F(\\epsilon) - F(0)}{\\epsilon} = \\lim_{\\epsilon \\to 0^-} \\frac{0 - 0}{\\epsilon} = 0\n$$\n\nThe left-hand and right-hand pathwise derivatives at $\\epsilon=0$ are $(\\partial_{-} F(0), \\partial_{+} F(0)) = (0, 1)$.\n\nThe function $F(\\epsilon)$ is not differentiable at $\\epsilon=0$ because its left-hand and right-hand derivatives do not match. This non-differentiability stems from a qualitative change in the behavior of the sample path $X^{\\epsilon}(t)$ at $\\epsilon=0$. For $t \\in (1,2]$, the set of active boundaries changes depending on the sign of $\\epsilon$.\n- When $\\epsilon>0$, the reflection on the $X_2=0$ boundary has a component in the positive $X_1$ direction (since the second column of $R(\\epsilon)$ is $(\\epsilon,1)^{\\top}$ with $\\epsilon>0$). This pushes the path away from the $X_1=0$ boundary, making it inactive.\n- When $\\epsilon \\le 0$, the reflection on the $X_2=0$ boundary pushes the path either parallel to the $X_1$ axis (for $\\epsilon=0$) or into the $X_1=0$ boundary (for $\\epsilon<0$). This forces the $X_1=0$ boundary to also be active to keep the path within the state space.\n\nThis phenomenon has significant implications for unbiased pathwise gradient estimation in stochastic simulation and Monte Carlo methods. Pathwise estimators, often associated with Infinitesimal Perturbation Analysis (IPA), rely on differentiating the sample performance with respect to a parameter. A key requirement for the validity of IPA and the interchange of expectation and differentiation ($\\frac{d}{d\\theta}\\mathbb{E}[f(X_{\\theta})] = \\mathbb{E}[\\frac{d}{d\\theta}f(X_{\\theta})]$) is the existence and continuity of the sample-path derivative. This example demonstrates that for systems with discontinuous dynamics, such as reflected processes, the sample paths themselves can be non-differentiable with respect to system parameters. A naive application of pathwise differentiation would lead to an incorrect or undefined gradient estimator, which is typically biased. This problem necessitates more advanced techniques like the score function (likelihood ratio) method or generalized pathwise derivative estimators that properly account for the analytic difficulties at points of discontinuity.", "answer": "$$\n\\boxed{\\begin{pmatrix} 0 & 1 \\end{pmatrix}}\n$$", "id": "3328514"}]}