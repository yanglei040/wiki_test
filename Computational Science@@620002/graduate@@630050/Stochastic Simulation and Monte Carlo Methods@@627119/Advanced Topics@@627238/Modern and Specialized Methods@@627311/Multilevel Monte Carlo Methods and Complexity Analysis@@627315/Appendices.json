{"hands_on_practices": [{"introduction": "The primary appeal of the Multilevel Monte Carlo (MLMC) method lies in its remarkable computational efficiency. This exercise guides you through the foundational complexity analysis that proves this efficiency. By determining the optimal number of levels and samples per level, you will derive the total computational work required to achieve a target accuracy, revealing how MLMC can dramatically outperform standard Monte Carlo methods, especially in computationally demanding scenarios. [@problem_id:3322273]", "problem": "Consider a Multilevel Monte Carlo (MLMC) estimator for a scalar quantity of interest $Q$ in which discretizations $Q(h_{\\ell})$ at mesh sizes $h_{\\ell}$ are organized on levels $\\ell = 0,1,\\dots,L$. Assume a geometric refinement $h_{\\ell} = h_{0} m^{-\\ell}$ for some $m  1$, and define the level differences $\\Delta_{\\ell} = Q(h_{\\ell}) - Q(h_{\\ell-1})$ with the convention $Q(h_{-1}) \\equiv 0$. The MLMC estimator is constructed as the telescoping sum of sample averages of the $\\Delta_{\\ell}$. Suppose the following widely used scaling assumptions hold:\n- The bias (discretization error) satisfies $\\left|\\mathbb{E}[Q(h_{L})] - \\mathbb{E}[Q]\\right| \\leq c_{b} h_{L}^{\\alpha}$ for some constants $c_{b}  0$ and $\\alpha  0$.\n- The variance of level differences satisfies $\\mathrm{Var}(\\Delta_{\\ell}) \\asymp c_{v} h_{\\ell}^{\\beta}$ for some constants $c_{v}  0$ and $\\beta  0$.\n- The computational cost per independent sample of $\\Delta_{\\ell}$ satisfies $C_{\\ell} \\asymp c_{c} h_{\\ell}^{-\\gamma}$ for some constants $c_{c}  0$ and $\\gamma  0$.\n\nLet $N_{\\ell}$ denote the number of independent samples used on level $\\ell$, so that the MLMC estimator uses a total computational work $W = \\sum_{\\ell=0}^{L} N_{\\ell} C_{\\ell}$, and has mean-squared error $\\mathrm{MSE} = \\left(\\text{bias}\\right)^{2} + \\sum_{\\ell=0}^{L} \\mathrm{Var}(\\Delta_{\\ell})/N_{\\ell}$. The goal is to choose the level $L(\\varepsilon)$ and the allocation $\\{N_{\\ell}(\\varepsilon)\\}$ so that $\\mathrm{MSE} \\leq \\varepsilon^{2}$ while minimizing the work $W$ as $\\varepsilon \\to 0$.\n\nWhich of the following statements about the optimal choice of $L(\\varepsilon)$, the optimal variance allocation across levels, and the resulting asymptotic work complexity (including any logarithmic factors) is correct?\n\nA. Choose $L(\\varepsilon)$ as the minimal integer such that $c_{b} h_{L}^{\\alpha} \\leq \\varepsilon/\\sqrt{2}$; choose the sample sizes by the optimal variance allocation $N_{\\ell} \\propto \\sqrt{\\mathrm{Var}(\\Delta_{\\ell})/C_{\\ell}}$. Then the minimal work scales as $W(\\varepsilon) = \\mathcal{O}(\\varepsilon^{-2})$ if $\\beta  \\gamma$, $W(\\varepsilon) = \\mathcal{O}\\!\\left(\\varepsilon^{-2} \\left(\\log \\varepsilon^{-1}\\right)^{2}\\right)$ if $\\beta = \\gamma$, and $W(\\varepsilon) = \\mathcal{O}\\!\\left(\\varepsilon^{-2 - (\\gamma - \\beta)/\\alpha}\\right)$ if $\\beta  \\gamma$.\n\nB. Choose $L(\\varepsilon)$ as the minimal integer such that $c_{b} h_{L}^{\\alpha} \\leq \\varepsilon^{2}$; allocate samples as $N_{\\ell} \\propto \\mathrm{Var}(\\Delta_{\\ell})$. Then the minimal work scales as $W(\\varepsilon) = \\mathcal{O}(\\varepsilon^{-2})$ for all relations between $\\beta$ and $\\gamma$, with no logarithmic factor.\n\nC. Choose $L(\\varepsilon)$ so that $h_{L} \\asymp \\varepsilon^{1/\\alpha}$; allocate samples as $N_{\\ell} \\propto \\sqrt{\\mathrm{Var}(\\Delta_{\\ell})\\,C_{\\ell}}$. Then the minimal work scales as $W(\\varepsilon) = \\mathcal{O}\\!\\left(\\varepsilon^{-2} \\log \\varepsilon^{-1}\\right)$ when $\\beta = \\gamma$, and $W(\\varepsilon) = \\mathcal{O}\\!\\left(\\varepsilon^{-2 - (\\gamma - \\beta)/(2\\alpha)}\\right)$ when $\\beta  \\gamma$.\n\nD. Choose $L(\\varepsilon)$ so that $h_{L} \\asymp \\varepsilon^{1/\\alpha}$; allocate samples to equalize the per-level variance contributions, i.e., $N_{\\ell} \\propto \\mathrm{Var}(\\Delta_{\\ell})$. Then $W(\\varepsilon) = \\mathcal{O}(\\varepsilon^{-2})$ if $\\beta  \\gamma$, and $W(\\varepsilon) = \\mathcal{O}(\\varepsilon^{-3})$ if $\\beta  \\gamma$.\n\nE. In the borderline case $\\beta = \\gamma$, if one uses the optimal allocation $N_{\\ell} \\propto \\sqrt{\\mathrm{Var}(\\Delta_{\\ell})/C_{\\ell}}$, the minimal work scales as $W(\\varepsilon) = \\mathcal{O}\\!\\left(\\varepsilon^{-2} \\log \\varepsilon^{-1}\\right)$, where the logarithmic factor arises solely because $L(\\varepsilon)$ grows like $\\log \\varepsilon^{-1}$.", "solution": "The problem statement describes a standard setup for the complexity analysis of the Multilevel Monte Carlo (MLMC) method. All provided definitions, assumptions, and goals are consistent with the established literature on this topic (e.g., Giles, M. B. \"Multilevel Monte Carlo path simulation.\" Operations Research 56.3 (2008): 607-617). The problem is scientifically grounded, well-posed, objective, and contains sufficient information for a unique solution regarding the asymptotic complexity. Therefore, the problem statement is valid.\n\nWe proceed to derive the optimal work complexity for the MLMC estimator.\nThe goal is to minimize the total computational work, $W$, subject to the constraint that the mean-squared error, $\\mathrm{MSE}$, is less than or equal to a prescribed tolerance $\\varepsilon^2$.\n$W = \\sum_{\\ell=0}^{L} N_{\\ell} C_{\\ell}$\n$\\mathrm{MSE} = \\left(\\mathbb{E}[Q(h_{L})] - \\mathbb{E}[Q]\\right)^{2} + \\sum_{\\ell=0}^{L} \\frac{\\mathrm{Var}(\\Delta_{\\ell})}{N_{\\ell}} \\leq \\varepsilon^{2}$\n\nThe scaling assumptions are:\n1.  Bias: $|\\mathbb{E}[Q(h_{L})] - \\mathbb{E}[Q]| \\leq c_{b} h_{L}^{\\alpha}$\n2.  Level Variance: $\\mathrm{Var}(\\Delta_{\\ell}) \\asymp c_{v} h_{\\ell}^{\\beta}$ which we treat as $\\mathrm{Var}(\\Delta_{\\ell}) \\approx K_v h_{\\ell}^{\\beta}$ for some constant $K_v  0$.\n3.  Level Cost: $C_{\\ell} \\asymp c_{c} h_{\\ell}^{-\\gamma}$ which we treat as $C_{\\ell} \\approx K_c h_{\\ell}^{-\\gamma}$ for some constant $K_c  0$.\n\nAn optimal strategy to satisfy the MSE constraint is to balance the contributions from the squared bias and the total variance. A standard choice is to require each to be bounded by $\\varepsilon^2/2$:\na) $(\\text{bias})^2 \\leq \\varepsilon^2/2 \\implies |\\mathbb{E}[Q(h_{L})] - \\mathbb{E}[Q]| \\leq \\varepsilon/\\sqrt{2}$.\nb) $\\sum_{\\ell=0}^{L} \\mathrm{Var}(\\Delta_{\\ell})/N_{\\ell} \\leq \\varepsilon^2/2$.\n\nFrom constraint (a) and the bias assumption, we must choose the finest level $L$ such that:\n$c_{b} h_{L}^{\\alpha} \\leq \\varepsilon/\\sqrt{2}$\nSince $h_{L} = h_{0} m^{-L}$, this inequality determines the minimum required value for $L$. Asymptotically, for $\\varepsilon \\to 0$, this implies $h_{L} \\asymp \\varepsilon^{1/\\alpha}$. This in turn gives $h_0 m^{-L} \\asymp \\varepsilon^{1/\\alpha}$, which leads to $-L \\log m \\asymp (1/\\alpha) \\log \\varepsilon$, so $L \\asymp \\frac{1}{\\alpha \\log m} \\log(\\varepsilon^{-1})$. Thus, $L$ grows logarithmically with $\\varepsilon^{-1}$.\n\nNext, we minimize the work $W$ subject to the variance constraint (b). For minimal work, the inequality becomes an equality: $\\sum_{\\ell=0}^{L} \\mathrm{Var}(\\Delta_{\\ell})/N_{\\ell} = \\varepsilon^2/2$. Let $V_{\\ell} = \\mathrm{Var}(\\Delta_{\\ell})$. We minimize $W = \\sum_{\\ell=0}^{L} N_{\\ell} C_{\\ell}$ subject to $\\sum_{\\ell=0}^{L} V_{\\ell}/N_{\\ell} = \\varepsilon^2/2$. We use the method of Lagrange multipliers.\nThe Lagrangian is $\\mathcal{L}(\\{N_{\\ell}\\}, \\lambda) = \\sum_{\\ell=0}^{L} N_{\\ell} C_{\\ell} + \\lambda \\left(\\sum_{\\ell=0}^{L} \\frac{V_{\\ell}}{N_{\\ell}} - \\frac{\\varepsilon^2}{2}\\right)$.\nSetting the partial derivatives with respect to $N_k$ to zero:\n$\\frac{\\partial \\mathcal{L}}{\\partial N_k} = C_k - \\lambda \\frac{V_k}{N_k^2} = 0 \\implies N_k^2 = \\lambda \\frac{V_k}{C_k} \\implies N_k = \\sqrt{\\lambda} \\sqrt{V_k/C_k}$.\nThis shows that the optimal number of samples on level $\\ell$ is proportional to $\\sqrt{V_{\\ell}/C_{\\ell}}$.\n\nTo find the proportionality constant $\\sqrt{\\lambda}$, we substitute $N_{\\ell}$ back into the variance constraint:\n$\\sum_{\\ell=0}^{L} \\frac{V_{\\ell}}{\\sqrt{\\lambda} \\sqrt{V_{\\ell}/C_{\\ell}}} = \\frac{1}{\\sqrt{\\lambda}} \\sum_{\\ell=0}^{L} \\sqrt{V_{\\ell}C_{\\ell}} = \\frac{\\varepsilon^2}{2}$.\nThus, $\\sqrt{\\lambda} = \\frac{2}{\\varepsilon^2} \\sum_{k=0}^{L} \\sqrt{V_k C_k}$.\nThe optimal sample sizes are $N_{\\ell} = \\frac{2}{\\varepsilon^2} \\left(\\sum_{k=0}^{L} \\sqrt{V_k C_k}\\right) \\sqrt{\\frac{V_{\\ell}}{C_{\\ell}}}$.\n\nThe minimal total work is then:\n$W = \\sum_{\\ell=0}^{L} N_{\\ell} C_{\\ell} = \\sum_{\\ell=0}^{L} \\left[ \\frac{2}{\\varepsilon^2} \\left(\\sum_{k=0}^{L} \\sqrt{V_k C_k}\\right) \\sqrt{\\frac{V_{\\ell}}{C_{\\ell}}} \\right] C_{\\ell} = \\frac{2}{\\varepsilon^2} \\left(\\sum_{k=0}^{L} \\sqrt{V_k C_k}\\right) \\left(\\sum_{\\ell=0}^{L} \\sqrt{V_{\\ell}C_{\\ell}}\\right)$.\n$W = \\frac{2}{\\varepsilon^2} \\left(\\sum_{\\ell=0}^{L} \\sqrt{V_{\\ell}C_{\\ell}}\\right)^2$.\n\nNow we analyze the asymptotic complexity by examining the sum $\\sum_{\\ell=0}^{L} \\sqrt{V_{\\ell}C_{\\ell}}$.\nUsing the scaling assumptions, $\\sqrt{V_{\\ell}C_{\\ell}} \\asymp \\sqrt{h_{\\ell}^{\\beta} h_{\\ell}^{-\\gamma}} = h_{\\ell}^{(\\beta-\\gamma)/2}$.\nWith $h_{\\ell} = h_{0} m^{-\\ell}$, we have $h_{\\ell}^{(\\beta-\\gamma)/2} = (h_0 m^{-\\ell})^{(\\beta-\\gamma)/2} = h_0^{(\\beta-\\gamma)/2} (m^{-(\\beta-\\gamma)/2})^{\\ell}$.\nThe sum is a geometric series: $\\sum_{\\ell=0}^{L} \\sqrt{V_{\\ell}C_{\\ell}} \\asymp \\sum_{\\ell=0}^{L} (m^{-(\\beta-\\gamma)/2})^{\\ell}$.\nLet the ratio be $r = m^{-(\\beta-\\gamma)/2}$. The behavior depends on $r$.\n\nCase 1: $\\beta  \\gamma$.\nThen $\\beta - \\gamma  0$, so the exponent $-(\\beta-\\gamma)/2$ is negative. Since $m1$, the ratio $r = m^{-(\\beta-\\gamma)/2}$ satisfies $0  r  1$. As $L \\to \\infty$, the geometric series converges to a constant: $\\sum_{\\ell=0}^{\\infty} r^{\\ell} = \\frac{1}{1-r}$.\nThe sum is $\\mathcal{O}(1)$.\nThe work is $W \\asymp \\varepsilon^{-2} (\\mathcal{O}(1))^2 = \\mathcal{O}(\\varepsilon^{-2})$.\n\nCase 2: $\\beta = \\gamma$.\nThen $\\beta - \\gamma = 0$, so the ratio $r = m^0 = 1$. The sum becomes:\n$\\sum_{\\ell=0}^{L} 1 = L+1$.\nSince $L \\asymp \\log(\\varepsilon^{-1})$, the sum is $\\mathcal{O}(\\log \\varepsilon^{-1})$.\nThe work is $W \\asymp \\varepsilon^{-2} (\\mathcal{O}(\\log \\varepsilon^{-1}))^2 = \\mathcal{O}(\\varepsilon^{-2}(\\log \\varepsilon^{-1})^2)$.\n\nCase 3: $\\beta  \\gamma$.\nThen $\\beta - \\gamma  0$, so the exponent $-(\\beta-\\gamma)/2$ is positive. The ratio $r = m^{(\\gamma-\\beta)/2}$ is greater than $1$. The geometric series is dominated by its last term:\n$\\sum_{\\ell=0}^{L} r^{\\ell} = \\frac{r^{L+1}-1}{r-1} \\asymp r^L$.\nThe sum is $\\asymp (m^{(\\gamma-\\beta)/2})^L = (m^L)^{(\\gamma-\\beta)/2}$.\nFrom $h_L \\asymp \\varepsilon^{1/\\alpha}$, we have $h_0 m^{-L} \\asymp \\varepsilon^{1/\\alpha}$, so $m^L \\asymp \\varepsilon^{-1/\\alpha}$.\nThe sum is $\\asymp (\\varepsilon^{-1/\\alpha})^{(\\gamma-\\beta)/2} = \\varepsilon^{-(\\gamma-\\beta)/(2\\alpha)}$.\nThe work is $W \\asymp \\varepsilon^{-2} \\left( \\sum_{\\ell=0}^{L} \\sqrt{V_{\\ell}C_{\\ell}} \\right)^2 \\asymp \\varepsilon^{-2} \\left( \\varepsilon^{-(\\gamma-\\beta)/(2\\alpha)} \\right)^2 = \\varepsilon^{-2} \\varepsilon^{-(\\gamma-\\beta)/\\alpha} = \\varepsilon^{-2 - (\\gamma-\\beta)/\\alpha}$.\n\nSummary of derived complexity:\n- If $\\beta  \\gamma$: $W(\\varepsilon) = \\mathcal{O}(\\varepsilon^{-2})$.\n- If $\\beta = \\gamma$: $W(\\varepsilon) = \\mathcal{O}(\\varepsilon^{-2} (\\log \\varepsilon^{-1})^2)$.\n- If $\\beta  \\gamma$: $W(\\varepsilon) = \\mathcal{O}(\\varepsilon^{-2 - (\\gamma - \\beta)/\\alpha})$.\n\nNow we evaluate the given options.\n\nA. Choose $L(\\varepsilon)$ as the minimal integer such that $c_{b} h_{L}^{\\alpha} \\leq \\varepsilon/\\sqrt{2}$; choose the sample sizes by the optimal variance allocation $N_{\\ell} \\propto \\sqrt{\\mathrm{Var}(\\Delta_{\\ell})/C_{\\ell}}$. Then the minimal work scales as $W(\\varepsilon) = \\mathcal{O}(\\varepsilon^{-2})$ if $\\beta  \\gamma$, $W(\\varepsilon) = \\mathcal{O}\\!\\left(\\varepsilon^{-2} \\left(\\log \\varepsilon^{-1}\\right)^{2}\\right)$ if $\\beta = \\gamma$, and $W(\\varepsilon) = \\mathcal{O}\\!\\left(\\varepsilon^{-2 - (\\gamma - \\beta)/\\alpha}\\right)$ if $\\beta  \\gamma$.\nThis statement accurately describes the optimal choice of $L$ (by correctly balancing bias and variance error), the optimal allocation of samples $N_{\\ell}$ (derived from Lagrange multipliers), and the resulting work complexities for all three cases. These results match our derivation exactly.\nVerdict: **Correct**.\n\nB. Choose $L(\\varepsilon)$ as the minimal integer such that $c_{b} h_{L}^{\\alpha} \\leq \\varepsilon^{2}$; allocate samples as $N_{\\ell} \\propto \\mathrm{Var}(\\Delta_{\\ell})$. Then the minimal work scales as $W(\\varepsilon) = \\mathcal{O}(\\varepsilon^{-2})$ for all relations between $\\beta$ and $\\gamma$, with no logarithmic factor.\nThe choice for $L$ corresponds to a squared bias of $\\mathcal{O}(\\varepsilon^4)$, which is an inefficient error partition. The allocation $N_{\\ell} \\propto \\mathrm{Var}(\\Delta_{\\ell})$ is not optimal. The claimed complexity is incorrect, as it ignores the dependency on $\\beta$ and $\\gamma$.\nVerdict: **Incorrect**.\n\nC. Choose $L(\\varepsilon)$ so that $h_{L} \\asymp \\varepsilon^{1/\\alpha}$; allocate samples as $N_{\\ell} \\propto \\sqrt{\\mathrm{Var}(\\Delta_{\\ell})\\,C_{\\ell}}$. Then the minimal work scales as $W(\\varepsilon) = \\mathcal{O}\\!\\left(\\varepsilon^{-2} \\log \\varepsilon^{-1}\\right)$ when $\\beta = \\gamma$, and $W(\\varepsilon) = \\mathcal{O}\\!\\left(\\varepsilon^{-2 - (\\gamma - \\beta)/(2\\alpha)}\\right)$ when $\\beta  \\gamma$.\nThe choice for $L$ is correct. However, the sample allocation $N_{\\ell} \\propto \\sqrt{V_{\\ell}C_{\\ell}}$ is incorrect; the optimal is $N_{\\ell} \\propto \\sqrt{V_{\\ell}/C_{\\ell}}$. The asserted complexities are also incorrect: for $\\beta=\\gamma$, the logarithmic factor is squared, not to the power of $1$; for $\\beta\\gamma$, the exponent of $\\varepsilon$ has a denominator of $2\\alpha$, which is incorrect (it should be $\\alpha$).\nVerdict: **Incorrect**.\n\nD. Choose $L(\\varepsilon)$ so that $h_{L} \\asymp \\varepsilon^{1/\\alpha}$; allocate samples to equalize the per-level variance contributions, i.e., $N_{\\ell} \\propto \\mathrm{Var}(\\Delta_{\\ell})$. Then $W(\\varepsilon) = \\mathcal{O}(\\varepsilon^{-2})$ if $\\beta  \\gamma$, and $W(\\varepsilon) = \\mathcal{O}(\\varepsilon^{-3})$ if $\\beta  \\gamma$.\nThe choice for $L$ is correct. The allocation rule \"equalize the per-level variance contributions\" means $V_{\\ell}/N_{\\ell} = \\text{constant}$, which implies $N_{\\ell} \\propto V_{\\ell}$. This is not the cost-optimal allocation. The resulting work complexities are also incorrectly stated. For $\\beta  \\gamma$, this suboptimal allocation leads to $W \\asymp \\varepsilon^{-2} \\log \\varepsilon^{-1}$, not $\\varepsilon^{-2}$. The claim for $\\beta  \\gamma$ is also not generally true.\nVerdict: **Incorrect**.\n\nE. In the borderline case $\\beta = \\gamma$, if one uses the optimal allocation $N_{\\ell} \\propto \\sqrt{\\mathrm{Var}(\\Delta_{\\ell})/C_{\\ell}}$, the minimal work scales as $W(\\varepsilon) = \\mathcal{O}\\!\\left(\\varepsilon^{-2} \\log \\varepsilon^{-1}\\right)$, where the logarithmic factor arises solely because $L(\\varepsilon)$ grows like $\\log \\varepsilon^{-1}$.\nThis option addresses the specific case $\\beta=\\gamma$ and correctly identifies the optimal allocation. However, it states the complexity as $\\mathcal{O}(\\varepsilon^{-2} \\log \\varepsilon^{-1})$, which is incorrect. As derived, the complexity is $\\mathcal{O}(\\varepsilon^{-2} (\\log \\varepsilon^{-1})^2)$ because the work is proportional to the square of a sum that grows like $L+1$.\nVerdict: **Incorrect**.\n\nThe only statement that is entirely correct is A.", "answer": "$$\\boxed{A}$$", "id": "3322273"}, {"introduction": "The impressive complexity results derived in the previous exercise are not automatic; they depend on careful implementation. This practice delves into the core mechanics of the MLMC estimator, examining the crucial roles of inter-level independence and intra-level coupling. You will analyze how these design choices are essential for the simple, additive variance structure that underpins MLMC theory and explore the consequences of accidental correlations that can arise in practice. [@problem_id:3322274]", "problem": "Consider a quantity of interest $P$ defined on a probability space through a stochastic model (for example, the terminal functional of a stochastic differential equation). Let $\\{P_l\\}_{l=0}^L$ be a hierarchy of discretized approximations with increasing resolution indexed by the level $l$, so that $P_L$ is the finest approximation used. Define the Multilevel Monte Carlo (MLMC) estimator as\n$$\n\\widehat{P}_L \\;=\\; \\frac{1}{N_0}\\sum_{i=1}^{N_0} P_0^{(i)} \\;+\\; \\sum_{l=1}^{L} \\frac{1}{N_l}\\sum_{i=1}^{N_l} \\Big(P_l^{(i)} - P_{l-1}^{(i)}\\Big),\n$$\nwhere, for each level $l\\ge 1$, the pairs $\\big(P_l^{(i)},P_{l-1}^{(i)}\\big)$ are generated using a coupling that shares randomness within the pair to reduce the variance of the level-difference. Across different levels $l\\neq k$, the canonical MLMC design employs independent random number streams so that the level averages are mutually independent. In practice, two implementation choices are critical:\n\n- Independent random streams per level average: the random inputs used to form the averages at different levels are independent.\n- Reproducible shared randomness within coupled pairs $\\big(P_l,P_{l-1}\\big)$: the construction uses a consistent mapping of the fine-level random inputs to the coarse-level inputs within each pair (for example, summing Brownian increments on the fine grid to obtain coarse increments), so that the coupling is stable and replicable.\n\nStarting from the basic principles of linearity of expectation, variance additivity under independence, and the covariance decomposition for the variance of a sum, explain why these two implementation choices are needed for standard MLMC complexity claims. Then analyze the effect of accidental correlation across levels (for example, due to reusing a global random number generator state across levels without resetting per-level seeds), specifically on the bias and variance of $\\widehat{P}_L$. You may assume that for each level $l$, the $N_l$ samples $\\{(P_l^{(i)},P_{l-1}^{(i)})\\}_{i=1}^{N_l}$ are identically distributed and that their marginal distributions are correct for the discretizations at that level.\n\nWhich statement best captures the requirements and consequences described above?\n\nA. Independence across level averages is unnecessary because accidental correlation across levels does not change the variance of $\\widehat{P}_L$, while reproducible shared randomness within pairs introduces bias in $\\mathbb{E}[\\widehat{P}_L]$ by breaking the telescoping identity.\n\nB. Independence across level averages is necessary for the standard MLMC complexity analysis because it yields $\\operatorname{Var}[\\widehat{P}_L] = \\sum_{l=0}^L \\operatorname{Var}(Y_l)$ with $Y_0=\\frac{1}{N_0}\\sum_{i=1}^{N_0}P_0^{(i)}$ and $Y_l=\\frac{1}{N_l}\\sum_{i=1}^{N_l}(P_l^{(i)}-P_{l-1}^{(i)})$, and reproducible shared randomness within each coupled pair minimizes $\\operatorname{Var}(P_l - P_{l-1})$ via positive $\\operatorname{Cov}(P_l,P_{l-1})$. Accidental correlation across levels leaves $\\mathbb{E}[\\widehat{P}_L]$ unchanged but introduces covariance terms $2\\sum_{kl}\\operatorname{Cov}(Y_k,Y_l)$ that distort variance and thereby the work-versus-accuracy predictions.\n\nC. Independence across level averages is required to avoid bias in $\\mathbb{E}[\\widehat{P}_L]$, because without it $\\mathbb{E}[\\widehat{P}_L]\\neq \\mathbb{E}[P_L]$. Reproducible shared randomness within pairs increases $\\operatorname{Var}(P_l - P_{l-1})$ due to positive correlation between $P_l$ and $P_{l-1}$.\n\nD. Accidental correlation across levels strictly decreases the variance of $\\widehat{P}_L$ because covariance terms are always negative in MLMC, thereby improving complexity bounds; independence is therefore overly conservative.\n\nE. Using independent random streams per level and shared randomness inside each pair eliminates both variance and bias completely, making $\\widehat{P}_L=P$ almost surely for finite $N_l$ and finite $L$.", "solution": "The problem statement asks for an analysis of a standard theoretical question in computational science and stochastic methods, specifically regarding the implementation details of the Multilevel Monte Carlo (MLMC) method. The problem is scientifically grounded, well-posed, objective, and contains sufficient information for a unique solution based on first principles of probability theory. Therefore, the problem statement is valid.\n\nWe will analyze the bias and variance of the MLMC estimator $\\widehat{P}_L$ with respect to the two key implementation choices: independence across levels and coupling within levels.\n\n**1. Bias Analysis**\nThe expectation of the MLMC estimator is calculated using the linearity of expectation:\n$$\n\\mathbb{E}[\\widehat{P}_L] = \\mathbb{E}\\left[\\frac{1}{N_0}\\sum_{i=1}^{N_0} P_0^{(i)}\\right] + \\sum_{l=1}^{L} \\mathbb{E}\\left[\\frac{1}{N_l}\\sum_{i=1}^{N_l} \\Big(P_l^{(i)} - P_{l-1}^{(i)}\\Big)\\right]\n$$\nSince the samples at each level are identically distributed, this simplifies to:\n$$\n\\mathbb{E}[\\widehat{P}_L] = \\mathbb{E}[P_0] + \\sum_{l=1}^{L} \\mathbb{E}[P_l - P_{l-1}]\n$$\nUsing linearity of expectation again for the difference term, $\\mathbb{E}[P_l - P_{l-1}] = \\mathbb{E}[P_l] - \\mathbb{E}[P_{l-1}]$. This holds regardless of any correlation (coupling) between $P_l$ and $P_{l-1}$. Substituting this back gives a telescoping sum:\n$$\n\\mathbb{E}[\\widehat{P}_L] = \\mathbb{E}[P_0] + \\left(\\mathbb{E}[P_1] - \\mathbb{E}[P_0]\\right) + \\left(\\mathbb{E}[P_2] - \\mathbb{E}[P_1]\\right) + \\cdots + \\left(\\mathbb{E}[P_L] - \\mathbb{E}[P_{L-1}]\\right)\n$$\nAll intermediate terms cancel, leaving:\n$$\n\\mathbb{E}[\\widehat{P}_L] = \\mathbb{E}[P_L]\n$$\nThe bias of the estimator is $\\mathbb{E}[\\widehat{P}_L] - \\mathbb{E}[P] = \\mathbb{E}[P_L] - \\mathbb{E}[P]$, which is the discretization bias of the finest level approximation, $P_L$.\n\nCrucially, this derivation does not require independence *across* levels. The linearity of expectation is a universal property. Therefore, accidental correlation across levels does not affect the bias of the estimator. The \"reproducible shared randomness\" (coupling) within pairs also does not affect the bias, as the expectation of the telescoping sum remains intact.\n\n**2. Variance Analysis**\nLet's define the level averages:\n$$\nY_0 = \\frac{1}{N_0}\\sum_{i=1}^{N_0} P_0^{(i)} \\quad \\text{and} \\quad Y_l = \\frac{1}{N_l}\\sum_{i=1}^{N_l} \\Big(P_l^{(i)} - P_{l-1}^{(i)}\\Big) \\quad \\text{for } l \\ge 1\n$$\nThe estimator is $\\widehat{P}_L = \\sum_{l=0}^L Y_l$. The variance is:\n$$\n\\operatorname{Var}[\\widehat{P}_L] = \\operatorname{Var}\\left[\\sum_{l=0}^L Y_l\\right] = \\sum_{l=0}^L \\operatorname{Var}[Y_l] + 2\\sum_{0 \\le k  l \\le L} \\operatorname{Cov}(Y_k, Y_l)\n$$\n*Case I: Standard MLMC (Independence Across Levels)*\nThe standard implementation explicitly uses independent random streams for different levels. This ensures that $Y_k$ and $Y_l$ are independent for $k \\neq l$, making $\\operatorname{Cov}(Y_k, Y_l) = 0$. The variance formula simplifies to the sum of the variances of the level averages:\n$$\n\\operatorname{Var}[\\widehat{P}_L] = \\sum_{l=0}^L \\operatorname{Var}[Y_l] = \\frac{\\operatorname{Var}[P_0]}{N_0} + \\sum_{l=1}^L \\frac{\\operatorname{Var}[P_l - P_{l-1}]}{N_l}\n$$\nThe standard MLMC complexity analysis relies critically on this additive structure to optimize the sample counts $N_l$ and derive the overall work complexity.\n\n*Case II: Accidental Correlation Across Levels*\nIf random numbers are inadvertently reused across levels, the $Y_l$ terms are no longer independent, and the covariance terms $\\operatorname{Cov}(Y_k, Y_l)$ are generally non-zero. The presence of these unknown covariance terms breaks the simple additive structure of the variance, invalidating the standard MLMC complexity analysis and the associated optimal allocation of samples.\n\n**3. Role of Coupling (Shared Randomness within Pairs)**\nThe purpose of coupling is to reduce the variance of the correction terms, $\\operatorname{Var}[P_l - P_{l-1}]$. We have:\n$$\n\\operatorname{Var}[P_l - P_{l-1}] = \\operatorname{Var}[P_l] + \\operatorname{Var}[P_{l-1}] - 2\\operatorname{Cov}(P_l, P_{l-1})\n$$\nSince $P_l$ and $P_{l-1}$ are approximations of the same underlying quantity, a good coupling (e.g., using the same Brownian path) makes them strongly and positively correlated. A large positive $\\operatorname{Cov}(P_l, P_{l-1})$ term makes the variance of the difference small, which is the central tenet of MLMC.\n\n**Evaluation of Options**\n\n*   **A:** Incorrect. Accidental correlation changes the variance, and coupling does not introduce bias.\n*   **B:** Correct. This statement accurately identifies that independence across levels is necessary for the standard variance formula and complexity analysis. It correctly describes the role of coupling in reducing variance via positive covariance. It also correctly states that accidental cross-level correlation leaves the bias unchanged but distorts the variance by introducing covariance terms, thereby invalidating the standard work-accuracy predictions.\n*   **C:** Incorrect. Independence across levels is not required to avoid bias, and coupling *decreases*, not increases, the variance of the difference.\n*   **D:** Incorrect. The covariance terms from accidental correlation are not guaranteed to be negative; they can be positive, increasing the total variance.\n*   **E:** Incorrect. MLMC reduces error but does not eliminate it for finite computational resources. There is always a statistical error for finite $N_l$ and a bias for finite $L$.\n\nTherefore, statement B provides the most accurate and complete description.", "answer": "$$\n\\boxed{B}\n$$", "id": "3322274"}, {"introduction": "Ultimately, the goal of a simulation is to produce a reliable quantitative result. An MLMC simulation yields a point estimate, but a complete scientific answer must also report the associated uncertainty. This exercise bridges the gap between theory and practice by tasking you with constructing a confidence interval from hypothetical simulation data. You will learn to synthesize the two key sources of error—the statistical error from finite sampling and the systematic bias from discretization—into a single, conservative interval that rigorously quantifies the total uncertainty in your estimate. [@problem_id:3322294]", "problem": "Consider a Multilevel Monte Carlo (MLMC) estimator for the expectation of a real-valued functional $P$ of a stochastic process, constructed via a telescoping sum of level corrections $Y_{l} = P_{l} - P_{l-1}$, with $P_{-1} \\equiv 0$. The MLMC estimator for the truncated expectation $\\mathbb{E}[P_{L}]$ at level $L$ has the form $\\widehat{P}_{\\mathrm{ML}} = \\sum_{l=0}^{L} \\widehat{Y}_{l}$, where $\\widehat{Y}_{l}$ is the sample mean of $N_{l}$ independent and identically distributed samples of $Y_{l}$. Assume independence across levels, and that at each level $l$ the sample variance $s_{l}^{2}$ is computed from the $N_{l}$ samples.\n\nSuppose the following are given:\n- The level count $L = 3$.\n- Sample sizes $(N_{0}, N_{1}, N_{2}, N_{3}) = (1000, 800, 400, 200)$.\n- Sample variances $(s_{0}^{2}, s_{1}^{2}, s_{2}^{2}, s_{3}^{2}) = (0.80, 0.40, 0.10, 0.025)$.\n- The MLMC point estimate $\\widehat{P}_{\\mathrm{ML}} = 1.234$.\n- A weak error (bias) bound based on weak order $\\alpha = 1$ with constant $C_{b} = 0.12$ and dyadic refinement factor $2$, so that the truncation bias satisfies $|\\mathbb{E}[P] - \\mathbb{E}[P_{L}]| \\leq C_{b} \\, 2^{-\\alpha L}$.\n- A confidence level parameter $\\delta = 0.05$.\n\nUse the Central Limit Theorem (CLT) for the MLMC estimator and the variance estimator\n$$\\widehat{\\sigma}^{2} = \\sum_{l=0}^{L} \\frac{s_{l}^{2}}{N_{l}}$$\nto construct a conservative two-sided $(1-\\delta)$ confidence interval for $\\mathbb{E}[P]$ that accounts for both the sampling variance and the truncation bias. Your construction must rely on first principles: the telescoping representation, the CLT for sums of independent mean-zero terms, and the bias bound. Then, compute the numerical endpoints of the interval using the given data. Round your final interval endpoints to four significant figures. Express your final answer as the ordered pair of endpoints $\\bigl[\\text{lower}, \\text{upper}\\bigr]$.", "solution": "The problem requires the construction of a two-sided $(1-\\delta)$ confidence interval for the true expectation $\\mathbb{E}[P]$ of a quantity of interest, using a Multilevel Monte Carlo (MLMC) estimator. The total error of the MLMC point estimate $\\widehat{P}_{\\mathrm{ML}}$ with respect to the true expectation $\\mathbb{E}[P]$ is decomposed into two components: a statistical error and a truncation error (bias).\n\nThe total error is given by:\n$$\n\\mathbb{E}[P] - \\widehat{P}_{\\mathrm{ML}} = \\underbrace{(\\mathbb{E}[P] - \\mathbb{E}[P_{L}])}_{\\text{Bias}} + \\underbrace{(\\mathbb{E}[P_{L}] - \\widehat{P}_{\\mathrm{ML}})}_{\\text{Statistical Error}}\n$$\nHere, $\\mathbb{E}[P_{L}]$ is the expectation of the quantity of interest approximated at the finest level $L$.\n\nThe first term, the bias, is unknown, but we are provided with a bound on its magnitude:\n$$\n|\\mathbb{E}[P] - \\mathbb{E}[P_{L}]| \\leq B_{L}\n$$\nwhere $B_{L} = C_{b} \\, 2^{-\\alpha L}$. The parameters given are the weak order of convergence $\\alpha=1$, the constant $C_b=0.12$, and the number of levels $L=3$. We can compute this bound:\n$$\nB_{3} = C_{b} \\, 2^{-\\alpha L} = 0.12 \\times 2^{-1 \\times 3} = 0.12 \\times 2^{-3} = 0.12 \\times \\frac{1}{8} = 0.015\n$$\n\nThe second term, the statistical error, is a random variable. The MLMC estimator is defined as $\\widehat{P}_{\\mathrm{ML}} = \\sum_{l=0}^{L} \\widehat{Y}_{l}$, where $\\widehat{Y}_{l}$ is the sample mean of $N_l$ realizations of the level correction $Y_l$. The expectation of the MLMC estimator is $\\mathbb{E}[\\widehat{P}_{\\mathrm{ML}}] = \\mathbb{E}[P_L]$, meaning it is an unbiased estimator for the truncated expectation. The variance of the MLMC estimator, assuming independence across levels, is the sum of the variances of the level estimators:\n$$\n\\mathbb{V}[\\widehat{P}_{\\mathrm{ML}}] = \\sum_{l=0}^{L} \\mathbb{V}[\\widehat{Y}_{l}] = \\sum_{l=0}^{L} \\frac{\\mathbb{V}[Y_{l}]}{N_{l}} = \\sum_{l=0}^{L} \\frac{\\sigma_{l}^{2}}{N_{l}}\n$$\nWe are given an estimator for this variance based on the sample variances $s_{l}^{2}$:\n$$\n\\widehat{\\sigma}^{2} = \\sum_{l=0}^{L} \\frac{s_{l}^{2}}{N_{l}}\n$$\nUsing the provided data for $L=3$:\n- Sample sizes $(N_{0}, N_{1}, N_{2}, N_{3}) = (1000, 800, 400, 200)$\n- Sample variances $(s_{0}^{2}, s_{1}^{2}, s_{2}^{2}, s_{3}^{2}) = (0.80, 0.40, 0.10, 0.025)$\nwe compute the estimated variance:\n$$\n\\widehat{\\sigma}^{2} = \\frac{0.80}{1000} + \\frac{0.40}{800} + \\frac{0.10}{400} + \\frac{0.025}{200}\n$$\n$$\n\\widehat{\\sigma}^{2} = 0.0008 + 0.0005 + 0.00025 + 0.000125 = 0.001675\n$$\nThe estimated standard deviation is $\\widehat{\\sigma} = \\sqrt{0.001675}$.\n\nAccording to the Central Limit Theorem, the distribution of the statistical error, normalized by its standard deviation, is approximately standard normal. This allows us to construct a confidence interval for the truncated expectation $\\mathbb{E}[P_{L}]$:\n$$\n\\mathbb{P}\\left(|\\widehat{P}_{\\mathrm{ML}} - \\mathbb{E}[P_{L}]| \\leq z_{1-\\delta/2} \\widehat{\\sigma}\\right) \\approx 1-\\delta\n$$\nwhere $z_{1-\\delta/2}$ is the $(1-\\delta/2)$-quantile of the standard normal distribution. For the given confidence parameter $\\delta = 0.05$, we require a $(1-0.05) = 0.95$ confidence level, which corresponds to $z_{1-0.05/2} = z_{0.975} \\approx 1.96$.\n\nTo construct a confidence interval for the true expectation $\\mathbb{E}[P]$, we must account for both the statistical uncertainty and the bias. A conservative approach is to add the bias bound to the statistical margin of error. The total error is bounded by:\n$$\n|\\mathbb{E}[P] - \\widehat{P}_{\\mathrm{ML}}| \\leq |\\mathbb{E}[P] - \\mathbb{E}[P_L]| + |\\mathbb{E}[P_L] - \\widehat{P}_{\\mathrm{ML}}|\n$$\nThis leads to a confidence interval for $\\mathbb{E}[P]$ of the form $\\widehat{P}_{\\mathrm{ML}} \\pm E$, where the total margin of error $E$ is the sum of the statistical half-width and the bias bound:\n$$\nE = z_{1-\\delta/2} \\widehat{\\sigma} + B_{3}\n$$\nWe proceed to calculate $E$:\n$$\nE = 1.96 \\times \\sqrt{0.001675} + 0.015\n$$\n$$\nE \\approx 1.96 \\times 0.04092676 + 0.015 \\approx 0.08021645 + 0.015 = 0.09521645\n$$\nThe confidence interval for $\\mathbb{E}[P]$ is given by $[\\text{lower}, \\text{upper}]$, where:\n- Lower endpoint: $\\widehat{P}_{\\mathrm{ML}} - E = 1.234 - 0.09521645 = 1.13878355$\n- Upper endpoint: $\\widehat{P}_{\\mathrm{ML}} + E = 1.234 + 0.09521645 = 1.32921645$\n\nThe problem requires rounding the final interval endpoints to four significant figures.\n- Lower endpoint: $1.13878355 \\approx 1.139$\n- Upper endpoint: $1.32921645 \\approx 1.329$\n\nThe resulting confidence interval is $[1.139, 1.329]$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1.139  1.329\n\\end{pmatrix}\n}\n$$", "id": "3322294"}]}