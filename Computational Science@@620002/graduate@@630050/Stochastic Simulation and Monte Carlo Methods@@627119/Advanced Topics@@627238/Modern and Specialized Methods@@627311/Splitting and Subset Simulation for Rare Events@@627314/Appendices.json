{"hands_on_practices": [{"introduction": "Before running a multilevel splitting simulation, you must make key design choices. This exercise guides you through the process of determining the number of simulation levels ($K$) and the sample size per level ($n$) needed to estimate a very small probability with a desired level of statistical precision. By working through this calculation [@problem_id:3346498], you will gain insight into the fundamental trade-offs that govern the efficiency and accuracy of a splitting algorithm.", "problem": "Consider multilevel splitting for estimating a rare-event probability. Let the target rare-event probability be $p$, and suppose we choose a constant per-level conditional survival probability $p_0$ at each level. The algorithm uses $K$ levels, where $K$ is selected so that the product of level-wise probabilities reproduces the target $p$. Each level employs $n$ independent samples to estimate its conditional survival probability by the empirical fraction. Assume independence of sampling across levels, and model the per-level empirical fraction as a binomial estimator. The estimator of $p$ is the product of the $K$ empirical fractions, one per level.\n\nStarting from the definition of the coefficient of variation (CV), namely $\\mathrm{CV} = \\sqrt{\\mathrm{Var}(\\widehat{p})}/\\mathbb{E}[\\widehat{p}]$, and standard facts about the binomial estimator for a proportion, derive an approximation for the coefficient of variation of the product estimator in terms of $K$, $p_0$, and $n$ using a first-order linearization argument. Then, use this approximation to determine the minimal integer $n$ per level required to meet a prescribed coefficient of variation target $c$, given the following quantities:\n- $p = 10^{-6}$,\n- $p_0 = 10^{-1}$,\n- $c = 3 \\times 10^{-1}$.\n\nCompute:\n1. The number of levels $K = \\lceil \\ln(p) / \\ln(p_0) \\rceil$, where $\\ln$ denotes the natural logarithm.\n2. The minimal integer $n$ per level such that the coefficient of variation of the product estimator does not exceed $c$.\n\nProvide your final answer as the ordered pair $(K, n)$. No rounding beyond integer selection for $n$ is necessary.", "solution": "The objective is to find the number of levels $K$ and the number of samples per level $n$ for a multilevel splitting simulation, subject to a constraint on the coefficient of variation of the final probability estimator.\n\nFirst, we derive the approximation for the coefficient of variation (CV). The estimator for the rare-event probability $p$ is the product of $K$ level-wise estimators:\n$$ \\widehat{p} = \\prod_{i=1}^{K} \\widehat{p}_i $$\nEach $\\widehat{p}_i$ is the empirical fraction of successes from $n$ trials, estimating the true conditional probability $p_0$. Thus, $\\widehat{p}_i$ follows a scaled binomial distribution. Specifically, if $S_i \\sim \\mathrm{Binomial}(n, p_0)$, then $\\widehat{p}_i = S_i/n$. The expected value is $\\mathbb{E}[\\widehat{p}_i] = p_0$ and the variance is $\\mathrm{Var}(\\widehat{p}_i) = \\frac{p_0(1-p_0)}{n}$. Since sampling across levels is independent, the estimators $\\widehat{p}_i$ for $i=1, \\dots, K$ are independent and identically distributed.\n\nThe definition of the coefficient of variation is $\\mathrm{CV}(\\widehat{p}) = \\frac{\\sqrt{\\mathrm{Var}(\\widehat{p})}}{\\mathbb{E}[\\widehat{p}]}$. Its square is $\\mathrm{CV}^2(\\widehat{p}) = \\frac{\\mathrm{Var}(\\widehat{p})}{(\\mathbb{E}[\\widehat{p}])^2}$.\n\nA first-order linearization argument is requested. This is most effectively carried out by considering the logarithm of the estimator. For a positive random variable $X$ concentrated around its mean $\\mu$, a first-order Taylor expansion (delta method) for $\\ln(X)$ leads to the approximation $\\mathrm{Var}(\\ln(X)) \\approx \\frac{\\mathrm{Var}(X)}{\\mu^2} = \\mathrm{CV}^2(X)$.\nApplying this approximation to our estimator $\\widehat{p}$, we have:\n$$ \\mathrm{CV}^2(\\widehat{p}) \\approx \\mathrm{Var}(\\ln(\\widehat{p})) $$\nLet's compute $\\mathrm{Var}(\\ln(\\widehat{p}))$.\n$$ \\ln(\\widehat{p}) = \\ln\\left(\\prod_{i=1}^{K} \\widehat{p}_i\\right) = \\sum_{i=1}^{K} \\ln(\\widehat{p}_i) $$\nDue to the independence of the $\\widehat{p}_i$, the variance of the sum is the sum of the variances:\n$$ \\mathrm{Var}(\\ln(\\widehat{p})) = \\sum_{i=1}^{K} \\mathrm{Var}(\\ln(\\widehat{p}_i)) $$\nSince all levels are identical, $\\mathrm{Var}(\\ln(\\widehat{p}_i))$ is the same for all $i$. Let's denote it by $\\mathrm{Var}(\\ln(\\widehat{p}_1))$.\n$$ \\mathrm{Var}(\\ln(\\widehat{p})) = K \\cdot \\mathrm{Var}(\\ln(\\widehat{p}_1)) $$\nWe use the delta method again for the function $g(x) = \\ln(x)$ applied to the random variable $\\widehat{p}_1$. The approximation is $\\mathrm{Var}(g(\\widehat{p}_1)) \\approx (g'(\\mathbb{E}[\\widehat{p}_1]))^2 \\mathrm{Var}(\\widehat{p}_1)$.\nHere, $\\mathbb{E}[\\widehat{p}_1] = p_0$ and $g'(x) = 1/x$, so $g'(p_0) = 1/p_0$.\n$$ \\mathrm{Var}(\\ln(\\widehat{p}_1)) \\approx \\left(\\frac{1}{p_0}\\right)^2 \\mathrm{Var}(\\widehat{p}_1) = \\frac{1}{p_0^2} \\frac{p_0(1-p_0)}{n} = \\frac{1-p_0}{np_0} $$\nSubstituting this back, we obtain the desired approximation for the squared coefficient of variation:\n$$ \\mathrm{CV}^2(\\widehat{p}) \\approx K \\frac{1-p_0}{np_0} $$\nNext, we compute the number of levels $K$. The values given are $p = 10^{-6}$ and $p_0 = 10^{-1}$.\n$$ K = \\left\\lceil \\frac{\\ln(p)}{\\ln(p_0)} \\right\\rceil = \\left\\lceil \\frac{\\ln(10^{-6})}{\\ln(10^{-1})} \\right\\rceil = \\left\\lceil \\frac{-6 \\ln(10)}{-1 \\ln(10)} \\right\\rceil = \\lceil 6 \\rceil = 6 $$\nSo, the number of levels is $K=6$.\n\nFinally, we determine the minimal integer sample size $n$ per level to meet the target coefficient of variation $c = 3 \\times 10^{-1}$. The condition is $\\mathrm{CV}(\\widehat{p}) \\le c$, which in terms of our approximation is:\n$$ \\sqrt{K \\frac{1-p_0}{np_0}} \\le c $$\nSquaring both sides and rearranging to solve for $n$:\n$$ K \\frac{1-p_0}{np_0} \\le c^2 \\implies n \\ge \\frac{K}{c^2} \\frac{1-p_0}{p_0} $$\nWe substitute the known values: $K=6$, $p_0=10^{-1}=0.1$, and $c=3 \\times 10^{-1}=0.3$.\n$$ n \\ge \\frac{6}{(3 \\times 10^{-1})^2} \\frac{1 - 10^{-1}}{10^{-1}} $$\n$$ n \\ge \\frac{6}{9 \\times 10^{-2}} \\frac{0.9}{0.1} $$\n$$ n \\ge \\frac{6}{0.09} \\times 9 = \\frac{54}{0.09} = \\frac{5400}{9} = 600 $$\nThe inequality is $n \\ge 600$. The minimal integer value for $n$ is therefore $600$.\n\nThe required quantities are the number of levels $K=6$ and the minimal samples per level $n=600$. The result is the ordered pair $(K, n)$.", "answer": "$$ \\boxed{\\begin{pmatrix} 6  600 \\end{pmatrix}} $$", "id": "3346498"}, {"introduction": "A common challenge in particle-based methods like splitting is \"sample impoverishment,\" where the statistical power of the particle population degrades as a few particles come to dominate the estimate. This practice introduces the Effective Sample Size (ESS) as a powerful diagnostic tool to monitor the health of the simulation. You will learn how to calculate the ESS from a set of particle weights and use it to make a principled decision about when to perform a resampling step to maintain simulation efficiency [@problem_id:3346519].", "problem": "Consider an adaptive-level splitting scheme for rare-event estimation, where a sequence of intermediate threshold levels $\\{b_{\\ell}\\}_{\\ell=0}^{L}$ is used to guide particles toward the failure set. At level $\\ell$, suppose we have $N$ particles with positions $\\{X_{i}^{(\\ell)}\\}_{i=1}^{N}$ propagated from level $\\ell-1$ via a Markov kernel that preserves the target conditional distribution at level $\\ell$. The incremental importance weights across levels are defined up to a common normalizing constant by the ratio of target-to-proposal densities, and are used to correct the population to approximate the conditional distribution at level $\\ell$.\n\nThe goal is to determine whether population degeneracy has become severe enough to justify resampling at level $\\ell$. A principled approach is to measure concentration of the normalized weights using the concept of Effective Sample Size (ESS), derived from the variance of self-normalized importance sampling estimators, and to trigger resampling based on a threshold for ESS relative to $N$.\n\nAt a particular level $\\ell$, let the unnormalized incremental importance weights be given by the following list for $N=10$ particles:\n$$\n\\{r_{i}\\}_{i=1}^{10} = \\left\\{2,\\ 1,\\ \\frac{1}{2},\\ \\frac{1}{2},\\ \\frac{1}{5},\\ \\frac{1}{5},\\ \\frac{1}{5},\\ \\frac{1}{10},\\ \\frac{1}{10},\\ \\frac{1}{10}\\right\\}.\n$$\nFrom first principles, starting with the definitions of self-normalized importance weights and the variance of the corresponding estimator, derive the expression for the Effective Sample Size (ESS) at level $\\ell$ in terms of the normalized weights, compute the numerical value of the ESS for the given $\\{r_{i}\\}$, and then compute the normalized ESS fraction $\\mathrm{ESS}/N$.\n\nUsing the relationship between weight concentration, estimator variance, and population degeneracy, articulate a principled resampling criterion based on the normalized ESS fraction and discuss whether resampling would be triggered if a target threshold $\\tau=0.6$ were used at level $\\ell$.\n\nExpress your final answer as the single value of the normalized ESS fraction $\\mathrm{ESS}/N$, rounded to four significant figures. No units are required.", "solution": "The problem requires us to derive the expression for the Effective Sample Size (ESS) from first principles related to self-normalized importance sampling, calculate its value for a given set of weights, and use it to decide whether to perform resampling.\n\nFirst, we derive the expression for the ESS. In importance sampling, we aim to estimate an expectation $\\mathbb{E}_{\\pi}[\\phi(X)]$ using samples $\\{X_i\\}_{i=1}^{N}$ drawn from a proposal distribution $q(x)$. The self-normalized importance sampling estimator for the expectation is given by:\n$$\n\\hat{\\mu}_{SN} = \\frac{\\sum_{i=1}^{N} r_i \\phi(X_i)}{\\sum_{j=1}^{N} r_j} = \\sum_{i=1}^{N} w_i \\phi(X_i)\n$$\nwhere $r_i$ are the unnormalized importance weights $r(X_i) \\propto \\pi(X_i)/q(X_i)$, and $w_i$ are the normalized importance weights:\n$$\nw_i = \\frac{r_i}{\\sum_{j=1}^{N} r_j}\n$$\nThe variance of this estimator determines its quality. A key result from the theory of importance sampling, derived using a first-order Taylor series expansion (the delta method) of the ratio estimator, gives the approximate variance of $\\hat{\\mu}_{SN}$:\n$$\n\\mathrm{Var}(\\hat{\\mu}_{SN}) \\approx \\frac{\\mathrm{Var}_{\\pi}(\\phi(X))}{N} \\left(1 + \\mathrm{CV}_q^2(r)\\right)\n$$\nwhere $\\mathrm{Var}_{\\pi}(\\phi(X))$ is the variance of $\\phi(X)$ under the target distribution $\\pi$, and $\\mathrm{CV}_q^2(r)$ is the squared coefficient of variation of the weights, defined as $\\mathrm{CV}_q^2(r) = \\mathrm{Var}_q(r) / (\\mathbb{E}_q[r])^2$.\n\nThe concept of Effective Sample Size (ESS) arises from comparing the variance of the importance sampling estimator to the variance of an ideal Monte Carlo estimator that uses $\\mathrm{ESS}$ independent samples drawn directly from the target distribution $\\pi$. The variance of such an ideal estimator would be:\n$$\n\\mathrm{Var}(\\hat{\\mu}_{ideal}) = \\frac{\\mathrm{Var}_{\\pi}(\\phi(X))}{\\mathrm{ESS}}\n$$\nBy equating the variances, $\\mathrm{Var}(\\hat{\\mu}_{ideal}) \\approx \\mathrm{Var}(\\hat{\\mu}_{SN})$, we can solve for ESS:\n$$\n\\frac{\\mathrm{Var}_{\\pi}(\\phi(X))}{\\mathrm{ESS}} = \\frac{\\mathrm{Var}_{\\pi}(\\phi(X))}{N} \\left(1 + \\mathrm{CV}_q^2(r)\\right) \\implies \\mathrm{ESS} = \\frac{N}{1 + \\mathrm{CV}_q^2(r)}\n$$\nTo use this formula, we need to estimate $\\mathrm{CV}_q^2(r)$ from our sample of weights $\\{r_i\\}_{i=1}^{N}$. A consistent estimator is:\n$$\n\\widehat{\\mathrm{CV}^2(r)} = \\frac{\\widehat{\\mathrm{Var}}_q(r)}{(\\hat{\\mathbb{E}}_q[r])^2} \\approx \\frac{\\frac{1}{N}\\sum_{i=1}^N (r_i - \\bar{r})^2}{\\bar{r}^2}\n$$\nwhere $\\bar{r} = \\frac{1}{N}\\sum_{i=1}^N r_i$. This simplifies to:\n$$\n\\widehat{\\mathrm{CV}^2(r)} = \\frac{\\frac{1}{N}(\\sum r_i^2 - N\\bar{r}^2)}{\\bar{r}^2} = \\frac{\\frac{1}{N}(\\sum r_i^2) - (\\frac{1}{N}\\sum r_i)^2}{(\\frac{1}{N}\\sum r_i)^2} = \\frac{N\\sum r_i^2 - (\\sum r_i)^2}{(\\sum r_i)^2} = N \\frac{\\sum r_i^2}{(\\sum r_i)^2} - 1\n$$\nSubstituting this estimate back into the expression for ESS yields:\n$$\n\\mathrm{ESS} \\approx \\frac{N}{1 + \\left(N \\frac{\\sum r_i^2}{(\\sum r_i)^2} - 1\\right)} = \\frac{N}{N \\frac{\\sum r_i^2}{(\\sum r_i)^2}} = \\frac{(\\sum r_i)^2}{\\sum r_i^2}\n$$\nThis expression can be written in terms of the normalized weights $w_i = r_i / \\sum_j r_j$:\n$$\n\\mathrm{ESS} = \\frac{(\\sum r_i)^2}{\\sum r_i^2} = \\frac{1}{\\frac{\\sum r_i^2}{(\\sum r_i)^2}} = \\frac{1}{\\sum_i \\left(\\frac{r_i}{\\sum_j r_j}\\right)^2} = \\frac{1}{\\sum_{i=1}^{N} w_i^2}\n$$\nThis is the desired expression for the Effective Sample Size, derived from the variance of the self-normalized estimator.\n\nNext, we compute the numerical value of ESS for the given data. We have $N=10$ particles with unnormalized incremental weights:\n$$\n\\{r_{i}\\}_{i=1}^{10} = \\left\\{2,\\ 1,\\ \\frac{1}{2},\\ \\frac{1}{2},\\ \\frac{1}{5},\\ \\frac{1}{5},\\ \\frac{1}{5},\\ \\frac{1}{10},\\ \\frac{1}{10},\\ \\frac{1}{10}\\right\\}\n$$\nFirst, we calculate the sum of the weights:\n$$\n\\sum_{i=1}^{10} r_i = 2 + 1 + 2\\left(\\frac{1}{2}\\right) + 3\\left(\\frac{1}{5}\\right) + 3\\left(\\frac{1}{10}\\right) = 3 + 1 + \\frac{3}{5} + \\frac{3}{10} = 4 + \\frac{6}{10} + \\frac{3}{10} = 4 + \\frac{9}{10} = \\frac{49}{10} = 4.9\n$$\nNext, we calculate the sum of the squares of the weights:\n$$\n\\sum_{i=1}^{10} r_i^2 = 2^2 + 1^2 + 2\\left(\\frac{1}{2}\\right)^2 + 3\\left(\\frac{1}{5}\\right)^2 + 3\\left(\\frac{1}{10}\\right)^2\n$$\n$$\n\\sum_{i=1}^{10} r_i^2 = 4 + 1 + 2\\left(\\frac{1}{4}\\right) + 3\\left(\\frac{1}{25}\\right) + 3\\left(\\frac{1}{100}\\right) = 5 + \\frac{1}{2} + \\frac{3}{25} + \\frac{3}{100}\n$$\n$$\n\\sum_{i=1}^{10} r_i^2 = 5 + \\frac{50}{100} + \\frac{12}{100} + \\frac{3}{100} = 5 + \\frac{65}{100} = 5.65\n$$\nNow we can compute the ESS:\n$$\n\\mathrm{ESS} = \\frac{(\\sum r_i)^2}{\\sum r_i^2} = \\frac{(4.9)^2}{5.65} = \\frac{24.01}{5.65} = \\frac{2401}{565} \\approx 4.2495575\n$$\nThe problem asks for the normalized ESS fraction, $\\mathrm{ESS}/N$:\n$$\n\\frac{\\mathrm{ESS}}{N} = \\frac{1}{10} \\left(\\frac{2401}{565}\\right) = \\frac{2401}{5650} \\approx 0.42495575\n$$\nRounding to four significant figures, we get $0.4250$.\n\nFinally, we articulate a principled resampling criterion and apply it. The ESS measures the effective number of independent samples represented by the weighted particle set. A value of $\\mathrm{ESS} \\approx N$ indicates a healthy population with uniform weights, while $\\mathrm{ESS} \\ll N$ signals \"population degeneracy\" or \"sample impoverishment,\" where a few particles have very high weights and dominate the estimate, rendering the other particles ineffective. The normalized ESS fraction, $\\mathrm{ESS}/N$, which ranges from $1/N$ (worst case) to $1$ (best case), is a standard metric for degeneracy.\n\nA principled resampling criterion is to trigger a resampling step whenever this fraction drops below a predefined threshold, $\\tau$. The criterion is: **Resample if $\\mathrm{ESS}/N  \\tau$**. Resampling discards particles with low weights and duplicates those with high weights, effectively \"rejuvenating\" the population by resetting the weights to a uniform distribution for the next stage.\n\nIn this problem, the target threshold is given as $\\tau=0.6$. We compare our computed value:\n$$\n\\frac{\\mathrm{ESS}}{N} \\approx 0.4250\n$$\nSince $0.4250  0.6$, the condition is met. Therefore, resampling would be triggered at level $\\ell$. This indicates that the concentration of weights is severe enough that the effective number of particles has dropped to approximately $42.5\\%$ of the nominal population size, justifying a resampling step to maintain the efficiency and stability of the simulation.", "answer": "$$\\boxed{0.4250}$$", "id": "3346519"}, {"introduction": "Many real-world rare events occur in continuous-time systems, often modeled by stochastic differential equations (SDEs). This advanced practice challenges you to apply the splitting method to such a system, where the simulation itself introduces discretization errors. You will implement a sophisticated estimator that uses pathwise likelihood ratios to correct for this bias and explore how Brownian bridge conditioning can further reduce temporal errors, showcasing how to build a high-fidelity estimator for complex dynamics [@problem_id:3346543].", "problem": "Consider the one-dimensional Ornstein–Uhlenbeck stochastic differential equation (SDE) defined by the Itô dynamics\n$$\n\\mathrm{d}X_t = -\\kappa X_t\\,\\mathrm{d}t + \\sigma\\,\\mathrm{d}W_t,\\quad X_0 = x_0,\n$$\nwhere $W_t$ is standard Brownian motion, $\\kappa  0$ is the mean-reversion rate, and $\\sigma  0$ is the diffusion coefficient. Let $T  0$ be a fixed time horizon and $b  0$ a fixed barrier. The rare event of interest is the barrier hitting event\n$$\nA = \\left\\{ \\sup_{0 \\le t \\le T} X_t \\ge b \\right\\}.\n$$\n\nYou will estimate the probability $\\mathbb{P}(A)$ under the true continuous-time SDE using a splitting method with fixed intermediate levels and pathwise likelihood ratio correction to account for time-discretization bias introduced by the Euler–Maruyama proposal. Additionally, you will compare this with a variant that uses Brownian bridge conditioning to reduce temporal discretization error for the final barrier crossing detection.\n\nFundamental base:\n- The exact discrete-time transition of the Ornstein–Uhlenbeck process over a time step $\\Delta t$ is Gaussian:\n$$\nX_{t+\\Delta t} \\mid X_t = x \\sim \\mathcal{N}\\left( x e^{-\\kappa \\Delta t}, \\; \\frac{\\sigma^2}{2\\kappa}\\left(1 - e^{-2\\kappa \\Delta t}\\right) \\right).\n$$\n- The Euler–Maruyama proposal with step size $\\Delta t$ for the same SDE yields:\n$$\n\\tilde{X}_{t+\\Delta t} \\mid \\tilde{X}_t = x \\sim \\mathcal{N}\\left( x - \\kappa x \\,\\Delta t, \\; \\sigma^2 \\Delta t \\right).\n$$\n- The per-step pathwise likelihood ratio (Radon–Nikodym derivative) correcting for simulating under the Euler proposal while targeting the true Ornstein–Uhlenbeck transition is given by the ratio of the corresponding Gaussian densities:\n$$\nL(x \\to y; \\Delta t) = \\frac{\\phi\\left(y; \\, x e^{-\\kappa \\Delta t}, \\, \\frac{\\sigma^2}{2\\kappa}\\left(1 - e^{-2\\kappa \\Delta t}\\right) \\right)}{\\phi\\left(y; \\, x - \\kappa x \\Delta t, \\, \\sigma^2 \\Delta t \\right)},\n$$\nwhere $\\phi(\\cdot;\\,m,v)$ denotes the Gaussian probability density function with mean $m$ and variance $v$.\n- For a Brownian bridge over one time step, conditional on endpoints $x$ and $y$, the probability that the continuous path crosses a level $\\ell$ within the step, when both endpoints satisfy $x  \\ell$ and $y  \\ell$, is given by the reflection-principle formula\n$$\np_{\\mathrm{BB}}(x,y;\\ell,\\Delta t) = \\exp\\left( -\\frac{2(\\ell - x)(\\ell - y)}{\\sigma^2 \\Delta t} \\right).\n$$\nIf $\\max(x,y) \\ge \\ell$, then the crossing probability within the step is $1$. For the Ornstein–Uhlenbeck process with additive noise, the conditional law of the path segment given the endpoints is a Brownian bridge with the same variance scaling, making this formula applicable for within-step crossing detection.\n\nSplitting design:\n- Use a fixed set of strictly increasing intermediate levels $\\ell_1  \\ell_2  \\cdots  \\ell_{m-1}  \\ell_m = b$ and a uniform time grid $t_n = n\\,\\Delta t$ for $n = 0,1,\\dots,\\lfloor T/\\Delta t \\rfloor$.\n- Initialize $N$ independent replicas at state $x_0$ and time index $n=0$, each carrying an initial log-weight $w^{(0)} = 0$.\n- For stage $k = 1,2,\\dots,m$:\n  1. For each replica, simulate forward under the Euler–Maruyama proposal until either the discrete-time state $X_{t_n}$ first satisfies $X_{t_n} \\ge \\ell_k$ (a hit) or the time horizon $T$ is reached (no hit). Accumulate the segment log-likelihood ratio by summing $\\log L(x \\to y;\\Delta t)$ at every step. For stages $k  m$, detect hits using only the discrete-time grid states. For the final stage $k = m$, implement two detection variants:\n     - Variant without Brownian bridge (grid-only): detect hits when the discrete-time state exceeds $b$.\n     - Variant with Brownian bridge: at each step, if both endpoints are below $b$, perform within-step crossing detection by flipping a Bernoulli random variable with success probability $p_{\\mathrm{BB}}(x,y;b,\\Delta t)$; a success indicates a hit in continuous time within the step.\n  2. Let $H_k$ denote the set of replicas that hit level $\\ell_k$ during this stage. Compute the weighted conditional probability estimator for stage $k$,\n     $$\n     \\hat{p}_k = \\frac{\\sum_{i=1}^{N} \\exp\\left(w_i^{\\mathrm{parent}} + w_i^{\\mathrm{seg}}\\right) \\,\\mathbf{1}\\{i \\in H_k\\}}{\\sum_{i=1}^{N} \\exp\\left(w_i^{\\mathrm{parent}} + w_i^{\\mathrm{seg}}\\right)},\n     $$\n     where $w_i^{\\mathrm{parent}}$ is the accumulated log-weight carried into stage $k$ by replica $i$ from previous stages, and $w_i^{\\mathrm{seg}}$ is the segment log-weight accumulated during stage $k$.\n  3. If $H_k$ is empty, set the remaining product of conditional probabilities to $0$ and terminate. Otherwise, resample $N$ replicas for the next stage by drawing with replacement from $H_k$ with probabilities proportional to $\\exp\\left(w_i^{\\mathrm{parent}} + w_i^{\\mathrm{seg}}\\right)$, and set each resampled replica’s new parent log-weight to $w_i^{\\mathrm{parent}} + w_i^{\\mathrm{seg}}$, its state to the hit state at the hitting time index, and its time index to the hitting time index.\n- The splitting estimator of $\\mathbb{P}(A)$ is then the product $\\prod_{k=1}^{m} \\hat{p}_k$.\n\nTasks:\n- Implement the splitting estimator with pathwise likelihood ratio weighting per stage:\n  - Estimator $E_{\\mathrm{LR,grid}}$: use discrete-time grid-only detection at all stages, including the final barrier stage $k=m$.\n  - Estimator $E_{\\mathrm{LR,BB}}$: use the same splitting procedure, but at the final barrier stage $k=m$, use Brownian bridge conditioning for within-step barrier crossing detection.\n- For numerical stability in the likelihood ratio sums, use log-sum-exp computations.\n\nYour program must compute both estimators $E_{\\mathrm{LR,grid}}$ and $E_{\\mathrm{LR,BB}}$ for each of the following test cases. All mathematical constants and parameters must be treated exactly as specified.\n\nTest suite:\n- Case $1$: $\\kappa = 1.5$, $\\sigma = 0.6$, $x_0 = 0.0$, $T = 1.0$, $\\Delta t = 0.01$, levels $\\ell = [0.6, 1.2, 1.5, 1.8]$, $N = 800$.\n- Case $2$: $\\kappa = 1.5$, $\\sigma = 0.6$, $x_0 = 0.0$, $T = 1.0$, $\\Delta t = 0.05$, levels $\\ell = [0.6, 1.2, 1.5, 1.8]$, $N = 1000$.\n- Case $3$: $\\kappa = 2.5$, $\\sigma = 0.5$, $x_0 = 0.0$, $T = 1.5$, $\\Delta t = 0.02$, levels $\\ell = [0.7, 1.3, 1.8, 2.2]$, $N = 1000$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results for the test suite as a list of lists, where each inner list is $\\left[E_{\\mathrm{LR,grid}}, E_{\\mathrm{LR,BB}}\\right]$ for the corresponding case, in the order of the cases listed above. For example, print a single line of the form\n$$\n\\left[ [a_1, b_1], [a_2, b_2], [a_3, b_3] \\right]\n$$\nwith numerical float values $a_i$ and $b_i$.", "solution": "We aim to estimate the rare-event probability $\\mathbb{P}(A)$ for the Ornstein–Uhlenbeck process under the true continuous-time dynamics, while simulation is driven by the Euler–Maruyama proposal. The estimation framework uses splitting across a ladder of levels, pathwise likelihood ratios to correct for the proposal-target mismatch of discrete-time transitions, and a final-stage option to reduce temporal discretization error via Brownian bridge conditioning.\n\nPrinciples and derivations:\n\n1. Transition kernels and likelihood ratio:\n   The Ornstein–Uhlenbeck process has a known exact discrete-time transition over one step $\\Delta t$:\n   $$\n   X_{t+\\Delta t} \\mid X_t = x \\sim \\mathcal{N}\\left( \\mu_{\\mathrm{OU}}(x), \\, v_{\\mathrm{OU}} \\right),\n   $$\n   with $\\mu_{\\mathrm{OU}}(x) = x e^{-\\kappa \\Delta t}$ and $v_{\\mathrm{OU}} = \\frac{\\sigma^2}{2\\kappa}\\left(1 - e^{-2\\kappa \\Delta t}\\right)$. The Euler–Maruyama proposal yields\n   $$\n   \\tilde{X}_{t+\\Delta t} \\mid \\tilde{X}_t = x \\sim \\mathcal{N}\\left( \\mu_{\\mathrm{EM}}(x), \\, v_{\\mathrm{EM}} \\right),\n   $$\n   with $\\mu_{\\mathrm{EM}}(x) = x - \\kappa x \\Delta t$ and $v_{\\mathrm{EM}} = \\sigma^2 \\Delta t$. When simulating stepwise under the proposal, the pathwise Radon–Nikodym derivative, discretized per step, is the product of the per-step likelihood ratios\n   $$\n   L(x \\to y; \\Delta t) = \\frac{\\phi\\left(y; \\mu_{\\mathrm{OU}}(x), v_{\\mathrm{OU}} \\right)}{\\phi\\left(y; \\mu_{\\mathrm{EM}}(x), v_{\\mathrm{EM}} \\right)}.\n   $$\n   Accumulating over a segment with states $\\{x_0, x_1, \\dots, x_n\\}$ yields the segment weight $W = \\prod_{j=0}^{n-1} L(x_j \\to x_{j+1}; \\Delta t)$, or equivalently the segment log-weight $w = \\sum_{j=0}^{n-1} \\log L(x_j \\to x_{j+1}; \\Delta t)$ for numerical stability.\n\n2. Splitting estimator:\n   Consider a fixed ladder $\\ell_1  \\ell_2  \\cdots  \\ell_m = b$. At stage $k$, replicas begin at states that have reached $\\ell_{k-1}$ and attempt to reach $\\ell_k$ before the time horizon. Let $w_i^{\\mathrm{parent}}$ be the log-weight accumulated up to the start of this stage, and $w_i^{\\mathrm{seg}}$ the additional segment log-weight. The stage’s weighted conditional probability estimator is\n   $$\n   \\hat{p}_k = \\frac{\\sum_{i=1}^{N} \\exp(w_i^{\\mathrm{parent}} + w_i^{\\mathrm{seg}})\\,\\mathbf{1}\\{i \\in H_k\\}}{\\sum_{i=1}^{N} \\exp(w_i^{\\mathrm{parent}} + w_i^{\\mathrm{seg}})},\n   $$\n   which is the importance sampling estimate of $\\mathbb{P}(\\text{hit } \\ell_k \\mid \\text{start at } \\ell_{k-1})$ under the true dynamics, using samples generated by the proposal and corrected by the pathwise likelihood ratios. The overall splitting estimator is\n   $$\n   \\widehat{\\mathbb{P}}(A) = \\prod_{k=1}^{m} \\hat{p}_k.\n   $$\n   After computing $\\hat{p}_k$, we resample $N$ replicas for the next stage from the hitters $H_k$, using probabilities proportional to $\\exp(w_i^{\\mathrm{parent}} + w_i^{\\mathrm{seg}})$. This resampling pushes the empirical distribution closer to the target conditional distribution (under the true dynamics) of hitting $\\ell_k$.\n\n3. Brownian bridge conditioning for temporal discretization:\n   Discrete-time grid detection underestimates barrier crossing probabilities because crossings may occur between time steps. The conditional law of the path segment given endpoints $x$ and $y$ for an additive-noise diffusion is a Brownian bridge. The probability that a Brownian bridge crosses level $\\ell$ within a step $\\Delta t$, given both endpoints are below $\\ell$, is\n   $$\n   p_{\\mathrm{BB}}(x,y;\\ell,\\Delta t) = \\exp\\left( -\\frac{2(\\ell - x)(\\ell - y)}{\\sigma^2 \\Delta t} \\right).\n   $$\n   For the final barrier stage $k=m$, we incorporate Brownian bridge: at each step, if the grid endpoints $x$ and $y$ both lie below $b$, we flip a Bernoulli random variable with success probability $p_{\\mathrm{BB}}(x,y;b,\\Delta t)$; a success indicates a continuous-time crossing within the step. If either endpoint exceeds $b$, we declare a hit deterministically. This reduces the temporal discretization error in the final stage’s detection while preserving the pathwise likelihood ratio correction for the simulated proposal transitions.\n\n4. Numerical stability and implementation details:\n   - Compute segment weights in log-space to avoid underflow:\n     $$\n     w_i^{\\mathrm{seg}} = \\sum \\left[ \\log \\phi\\left(y; \\mu_{\\mathrm{OU}}(x), v_{\\mathrm{OU}}\\right) - \\log \\phi\\left(y; \\mu_{\\mathrm{EM}}(x), v_{\\mathrm{EM}}\\right) \\right].\n     $$\n   - For the stage-weighted probability $\\hat{p}_k$, use the log-sum-exp trick:\n     $$\n     \\hat{p}_k = \\frac{\\sum_{i} \\exp(\\ell_i)\\,\\mathbf{1}\\{i \\in H_k\\}}{\\sum_{i} \\exp(\\ell_i)} = \\frac{\\sum_{i} \\exp(\\ell_i - M)\\,\\mathbf{1}\\{i \\in H_k\\}}{\\sum_{i} \\exp(\\ell_i - M)},\n     $$\n     where $\\ell_i = w_i^{\\mathrm{parent}} + w_i^{\\mathrm{seg}}$ and $M = \\max_i \\ell_i$.\n   - Resample $N$ hitters with probabilities proportional to $\\exp(\\ell_i)$ and carry forward the new parent log-weight $w_i^{\\mathrm{parent}} \\gets \\ell_i$ and the hit state and time index as the stage start for the next level.\n\n5. Comparison of estimators:\n   - $E_{\\mathrm{LR,grid}}$ uses discrete grid detection at all stages $k=1,\\dots,m$.\n   - $E_{\\mathrm{LR,BB}}$ uses the same splitting and likelihood ratio scheme but replaces the final stage detection by Brownian bridge conditioning. Because within-step crossing probabilities are properly treated in the last stage, $E_{\\mathrm{LR,BB}}$ reduces temporal discretization error relative to $E_{\\mathrm{LR,grid}}$, especially for coarser $\\Delta t$.\n\nAlgorithmic steps for each test case:\n- Initialize a random number generator with a fixed seed for reproducibility.\n- For each case, run splitting with log-weights and resampling across levels $\\ell_1,\\dots,\\ell_m$ twice:\n  - Once with grid-only detection at all levels to obtain $E_{\\mathrm{LR,grid}}$.\n  - Once with Brownian bridge detection at the final level to obtain $E_{\\mathrm{LR,BB}}$.\n- Aggregate results in the specified single-line list-of-lists format.\n\nThe differences across the test suite are designed to expose:\n- A “happy path” case with relatively fine $\\Delta t$ ($\\Delta t = 0.01$), where both estimators should be close.\n- A coarse $\\Delta t$ ($\\Delta t = 0.05$), where Brownian bridge correction is expected to increase the estimated hitting probability compared to grid-only detection.\n- A more strongly mean-reverting case ($\\kappa = 2.5$) with moderate $\\Delta t$ ($\\Delta t = 0.02$), where the rare-event nature is more pronounced and the splitting with likelihood ratio correction is essential for a stable estimate.\n\nAll outputs are floats without physical units, and angles are not involved. The final output is a list of lists, each containing two floats $\\left[E_{\\mathrm{LR,grid}}, E_{\\mathrm{LR,BB}}\\right]$ per case, printed as a single line.\n\n```python\nimport numpy as np\n\n# Fixed RNG for reproducibility\nrng = np.random.default_rng(123456789)\n\ndef normal_log_pdf(y, mean, var):\n    # Numerically stable log pdf for Gaussian\n    return -0.5 * (np.log(2.0 * np.pi * var) + ((y - mean) ** 2) / var)\n\ndef ou_true_params(x, kappa, sigma, dt):\n    mean_true = x * np.exp(-kappa * dt)\n    var_true = (sigma * sigma) / (2.0 * kappa) * (1.0 - np.exp(-2.0 * kappa * dt))\n    # Guard against numerical zero variance\n    var_true = max(var_true, 1e-16)\n    return mean_true, var_true\n\ndef euler_params(x, kappa, sigma, dt):\n    mean_euler = x * (1.0 - kappa * dt)\n    var_euler = sigma * sigma * dt\n    var_euler = max(var_euler, 1e-16)\n    return mean_euler, var_euler\n\ndef brownian_bridge_cross_prob(x, y, level, sigma, dt):\n    # If either endpoint is already above level, crossing is certain\n    if x = level or y = level:\n        return 1.0\n    # Both endpoints below: use reflection principle for Brownian bridge\n    return np.exp(-2.0 * (level - x) * (level - y) / (sigma * sigma * dt))\n\nclass Replica:\n    __slots__ = (\"x\", \"idx\", \"logw\")\n    def __init__(self, x, idx, logw):\n        self.x = x\n        self.idx = idx\n        self.logw = logw\n\ndef simulate_stage_segment(replica, level, T, dt, kappa, sigma, use_bb=False, is_final_stage=False):\n    \"\"\"\n    Simulate one replica forward from its current state/time until it hits the given level\n    or reaches time T. Accumulate the segment log-weight via pathwise likelihood ratios.\n    Detection:\n      - For non-final stages: grid-based detection only.\n      - For final stage: if use_bb=True, apply Brownian bridge within-step crossing detection.\n    Returns:\n      hit (bool), new_x (float), new_idx (int), seg_logw (float)\n    If hit=True, new_x/new_idx represent the discrete-time hit state/index used to seed next stage.\n    \"\"\"\n    x = replica.x\n    idx = replica.idx\n    n_steps = int(np.floor(T / dt))\n    seg_logw = 0.0\n\n    # If starting already above level (unlikely in splitting), treat as immediate hit\n    if x = level:\n        return True, x, idx, seg_logw\n\n    while idx  n_steps:\n        # Propose next step via Euler\n        mean_e, var_e = euler_params(x, kappa, sigma, dt)\n        y = mean_e + np.sqrt(var_e) * rng.standard_normal()\n        # Compute per-step log-likelihood ratio\n        mean_t, var_t = ou_true_params(x, kappa, sigma, dt)\n        seg_logw += normal_log_pdf(y, mean_t, var_t) - normal_log_pdf(y, mean_e, var_e)\n\n        # Detection\n        if not is_final_stage or not use_bb:\n            # Grid-only detection\n            if y = level:\n                # Hit at discrete time\n                return True, y, idx + 1, seg_logw\n        else:\n            # Final stage with Brownian bridge detection\n            if x = level or y = level:\n                return True, y, idx + 1, seg_logw\n            # Both endpoints below: stochastic within-step hit\n            p_hit = brownian_bridge_cross_prob(x, y, level, sigma, dt)\n            u = rng.random()\n            if u  p_hit:\n                # Hit occurred within the step; we do not know precise crossing point,\n                # but for final stage we only need to record hit. Use y and idx+1 as placeholder.\n                return True, y, idx + 1, seg_logw\n\n        # Move forward\n        x = y\n        idx += 1\n\n    # Reached time horizon without hitting\n    return False, x, idx, seg_logw\n\ndef logsumexp(log_values):\n    if len(log_values) == 0:\n        return -np.inf\n    m = np.max(log_values)\n    return m + np.log(np.sum(np.exp(log_values - m)))\n\ndef splitting_estimate(kappa, sigma, x0, T, dt, levels, N, use_bb_final):\n    \"\"\"\n    Perform splitting across fixed levels with pathwise likelihood ratio correction.\n    Returns the product of weighted conditional probability estimates across stages.\n    If use_bb_final=True, apply Brownian bridge detection only at the final barrier level.\n    \"\"\"\n    # Initialize replicas at x0, time 0, logw=0\n    replicas = [Replica(x0, 0, 0.0) for _ in range(N)]\n    est = 1.0\n\n    m = len(levels)\n    for k in range(m):\n        level = levels[k]\n        # Simulate each replica's segment to reach current level\n        hit_logLs = []\n        hit_states = []\n        hit_indices = []\n        all_logLs = []\n\n        for rep in replicas:\n            is_final = (k == m - 1)\n            hit, new_x, new_idx, seg_logw = simulate_stage_segment(rep, level, T, dt, kappa, sigma,\n                                                                   use_bb=use_bb_final, is_final_stage=is_final)\n            total_logw = rep.logw + seg_logw\n            all_logLs.append(total_logw)\n            if hit:\n                hit_logLs.append(total_logw)\n                hit_states.append(new_x)\n                hit_indices.append(new_idx)\n\n        # Compute weighted conditional probability for this stage via log-sum-exp\n        if len(hit_logLs) == 0:\n            est *= 0.0\n            break\n\n        denom_log = logsumexp(np.array(all_logLs))\n        numer_log = logsumexp(np.array(hit_logLs))\n        p_k = np.exp(numer_log - denom_log)\n        est *= p_k\n\n        # Resample N replicas from hitters with weights proportional to exp(total_logw)\n        # Normalize weights stably\n        hit_logLs_arr = np.array(hit_logLs)\n        w_max = np.max(hit_logLs_arr)\n        weights = np.exp(hit_logLs_arr - w_max)\n        probs = weights / np.sum(weights)\n        # Multinomial resampling indices\n        resample_idx = rng.choice(len(hit_states), size=N, replace=True, p=probs)\n\n        # Prepare replicas for next stage; carry forward parent log-weight and hit state/index\n        new_replicas = []\n        for j in resample_idx:\n            r = Replica(hit_states[j], hit_indices[j], hit_logLs[j])\n            new_replicas.append(r)\n        replicas = new_replicas\n\n    return est\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1\n        {\n            \"kappa\": 1.5, \"sigma\": 0.6, \"x0\": 0.0, \"T\": 1.0, \"dt\": 0.01,\n            \"levels\": [0.6, 1.2, 1.5, 1.8], \"N\": 800\n        },\n        # Case 2\n        {\n            \"kappa\": 1.5, \"sigma\": 0.6, \"x0\": 0.0, \"T\": 1.0, \"dt\": 0.05,\n            \"levels\": [0.6, 1.2, 1.5, 1.8], \"N\": 1000\n        },\n        # Case 3\n        {\n            \"kappa\": 2.5, \"sigma\": 0.5, \"x0\": 0.0, \"T\": 1.5, \"dt\": 0.02,\n            \"levels\": [0.7, 1.3, 1.8, 2.2], \"N\": 1000\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        kappa = case[\"kappa\"]\n        sigma = case[\"sigma\"]\n        x0 = case[\"x0\"]\n        T = case[\"T\"]\n        dt = case[\"dt\"]\n        levels = case[\"levels\"]\n        N = case[\"N\"]\n\n        # Estimator with LR and grid-only detection (all stages)\n        est_grid = splitting_estimate(kappa, sigma, x0, T, dt, levels, N, use_bb_final=False)\n        # Estimator with LR and Brownian bridge detection at final barrier\n        est_bb = splitting_estimate(kappa, sigma, x0, T, dt, levels, N, use_bb_final=True)\n\n        results.append([est_grid, est_bb])\n\n    # Final print statement in the exact required format.\n    # Single line containing list of lists of floats.\n    def format_list(lst):\n        # Ensure standard Python list formatting without extra spaces\n        return \"[\" + \",\".join(str(x) if not isinstance(x, list) else format_list(x) for x in lst) + \"]\"\n    print(format_list(results))\n\nsolve()\n```", "answer": "$$ \\boxed{\\text{A list of lists with numerical float values, as requested}} $$", "id": "3346543"}]}