{"hands_on_practices": [{"introduction": "To build a robust understanding of resampling, we must first analyze the statistical properties of its most fundamental form: multinomial resampling. This exercise guides you through a first-principles derivation of the variance and covariance of the offspring counts. By completing this practice [@problem_id:3336496], you will gain a crucial insight into the nature of resampling: while it combats weight degeneracy, it introduces correlation among the particle population, a key trade-off in the design of particle filters.", "problem": "Consider a Sequential Monte Carlo (SMC) algorithm maintaining a population of $M$ particles with normalized weights $\\{w_{i}\\}_{i=1}^{M}$ satisfying $w_{i}>0$ and $\\sum_{i=1}^{M} w_{i} = 1$. A standard multinomial resampling step produces a new set of $N$ offspring by drawing indices $I_{1}, I_{2}, \\dots, I_{N}$ independently from the categorical distribution on $\\{1,2,\\dots,M\\}$ with probabilities $\\mathbb{P}(I_{n}=i)=w_{i}$. For each $i \\in \\{1,\\dots,M\\}$, define the offspring count\n$$\nA_{i} := \\sum_{n=1}^{N} \\mathbf{1}\\{I_{n}=i\\}.\n$$\nUsing only fundamental definitions of expectation, variance, and covariance, along with independence across draws and the mutual exclusivity of category indicators within a single draw, derive closed-form analytic expressions for $\\operatorname{Var}(A_{i})$ and $\\operatorname{Cov}(A_{i}, A_{j})$ for $i \\neq j$, and use them to demonstrate that the pairwise covariances are negative. Express your final answer as symbolic expressions; no numerical approximation is required.", "solution": "The problem is valid as it is scientifically grounded, well-posed, objective, and self-contained. We can proceed with the derivation.\n\nThe problem describes a multinomial sampling process. We have $N$ independent draws from a categorical distribution over $M$ categories, where the probability of drawing category $i$ is $w_i$. The variable $A_i$ represents the total number of times category $i$ was drawn in $N$ trials. The set of random variables $(A_1, A_2, \\dots, A_M)$ follows a multinomial distribution with parameters $N$ and $(w_1, w_2, \\dots, w_M)$. We are asked to derive the variance of $A_i$ and the covariance of $A_i$ and $A_j$ for $i \\neq j$ from first principles.\n\nLet us define an indicator random variable for each draw $n \\in \\{1, 2, \\dots, N\\}$ and each category $i \\in \\{1, 2, \\dots, M\\}$.\nLet $X_{ni} = \\mathbf{1}\\{I_n = i\\}$, where $I_n$ is the outcome of the $n$-th draw. $X_{ni}=1$ if the $n$-th draw selects particle $i$, and $X_{ni}=0$ otherwise.\nThe total count for particle $i$ is the sum of these indicators over all $N$ draws:\n$$\nA_i = \\sum_{n=1}^{N} X_{ni}\n$$\nThe probability that the $n$-th draw selects particle $i$ is given as $\\mathbb{P}(I_n=i) = w_i$.\nThus, for any $n$ and $i$, $X_{ni}$ is a Bernoulli random variable with parameter $w_i$, i.e., $X_{ni} \\sim \\operatorname{Bernoulli}(w_i)$.\n\nFirst, let's find the expectation and variance of a single indicator $X_{ni}$.\nThe expectation is:\n$$\n\\mathbb{E}[X_{ni}] = 1 \\cdot \\mathbb{P}(X_{ni}=1) + 0 \\cdot \\mathbb{P}(X_{ni}=0) = \\mathbb{P}(I_n=i) = w_i\n$$\nFor the variance, we first find $\\mathbb{E}[X_{ni}^2]$. Since $X_{ni}$ is an indicator, $X_{ni}^2 = X_{ni}$.\n$$\n\\mathbb{E}[X_{ni}^2] = \\mathbb{E}[X_{ni}] = w_i\n$$\nThe variance is then:\n$$\n\\operatorname{Var}(X_{ni}) = \\mathbb{E}[X_{ni}^2] - (\\mathbb{E}[X_{ni}])^2 = w_i - w_i^2 = w_i(1-w_i)\n$$\n\nNow we derive the expression for $\\operatorname{Var}(A_i)$.\nThe variance of $A_i$ is given by:\n$$\n\\operatorname{Var}(A_i) = \\operatorname{Var}\\left(\\sum_{n=1}^{N} X_{ni}\\right)\n$$\nThe draws $I_1, I_2, \\dots, I_N$ are independent. Therefore, for a fixed category $i$, the random variables $X_{1i}, X_{2i}, \\dots, X_{Ni}$ are independent. The variance of a sum of independent random variables is the sum of their variances.\n$$\n\\operatorname{Var}(A_i) = \\sum_{n=1}^{N} \\operatorname{Var}(X_{ni})\n$$\nSince all $X_{ni}$ for $n=1, \\dots, N$ are identically distributed, their variances are the same.\n$$\n\\operatorname{Var}(A_i) = \\sum_{n=1}^{N} w_i(1-w_i) = N w_i(1-w_i)\n$$\n\nNext, we derive the expression for $\\operatorname{Cov}(A_i, A_j)$ for $i \\neq j$.\nThe definition of covariance is $\\operatorname{Cov}(A_i, A_j) = \\mathbb{E}[A_i A_j] - \\mathbb{E}[A_i]\\mathbb{E}[A_j]$.\nFirst, we find the expectation of $A_i$:\n$$\n\\mathbb{E}[A_i] = \\mathbb{E}\\left[\\sum_{n=1}^{N} X_{ni}\\right] = \\sum_{n=1}^{N} \\mathbb{E}[X_{ni}] = \\sum_{n=1}^{N} w_i = N w_i\n$$\nSimilarly, $\\mathbb{E}[A_j] = N w_j$. The second term of the covariance is $\\mathbb{E}[A_i]\\mathbb{E}[A_j] = (N w_i)(N w_j) = N^2 w_i w_j$.\n\nNow we compute the first term, $\\mathbb{E}[A_i A_j]$.\n$$\nA_i A_j = \\left(\\sum_{n=1}^{N} X_{ni}\\right) \\left(\\sum_{m=1}^{N} X_{mj}\\right) = \\sum_{n=1}^{N} \\sum_{m=1}^{N} X_{ni} X_{mj}\n$$\nBy linearity of expectation:\n$$\n\\mathbb{E}[A_i A_j] = \\mathbb{E}\\left[\\sum_{n=1}^{N} \\sum_{m=1}^{N} X_{ni} X_{mj}\\right] = \\sum_{n=1}^{N} \\sum_{m=1}^{N} \\mathbb{E}[X_{ni} X_{mj}]\n$$\nWe must evaluate the expectation $\\mathbb{E}[X_{ni} X_{mj}]$. We consider two cases for the indices $n$ and $m$.\n\nCase 1: $n \\neq m$.\nThe draws $I_n$ and $I_m$ are independent. Therefore, the indicator variables $X_{ni} = \\mathbf{1}\\{I_n=i\\}$ and $X_{mj} = \\mathbf{1}\\{I_m=j\\}$ are independent.\nFor independent variables, the expectation of the product is the product of the expectations:\n$$\n\\mathbb{E}[X_{ni} X_{mj}] = \\mathbb{E}[X_{ni}] \\mathbb{E}[X_{mj}] = w_i w_j\n$$\nThere are $N^2 - N = N(N-1)$ such pairs $(n,m)$ where $n \\neq m$.\n\nCase 2: $n = m$.\nWe are considering $\\mathbb{E}[X_{ni} X_{nj}]$ for $i \\neq j$. The product of the indicators is:\n$$\nX_{ni} X_{nj} = \\mathbf{1}\\{I_n=i\\} \\mathbf{1}\\{I_n=j\\}\n$$\nSince $i \\neq j$, a single draw $I_n$ cannot be equal to both $i$ and $j$ simultaneously. The events $\\{I_n=i\\}$ and $\\{I_n=j\\}$ are mutually exclusive.\nTherefore, the product $\\mathbf{1}\\{I_n=i\\} \\mathbf{1}\\{I_n=j\\}$ is always $0$.\nThis means $\\mathbb{E}[X_{ni} X_{nj}] = \\mathbb{E}[0] = 0$.\nThere are $N$ such pairs $(n,m)$ where $n=m$.\n\nNow we can calculate $\\mathbb{E}[A_i A_j]$ by splitting the double summation:\n$$\n\\mathbb{E}[A_i A_j] = \\sum_{n \\neq m} \\mathbb{E}[X_{ni} X_{mj}] + \\sum_{n=m} \\mathbb{E}[X_{ni} X_{nj}]\n$$\nSubstituting the results from the two cases:\n$$\n\\mathbb{E}[A_i A_j] = \\sum_{n \\neq m} (w_i w_j) + \\sum_{n=m} (0) = N(N-1) w_i w_j\n$$\nFinally, we compute the covariance:\n$$\n\\operatorname{Cov}(A_i, A_j) = \\mathbb{E}[A_i A_j] - \\mathbb{E}[A_i]\\mathbb{E}[A_j] = N(N-1) w_i w_j - N^2 w_i w_j\n$$\n$$\n\\operatorname{Cov}(A_i, A_j) = (N^2 - N) w_i w_j - N^2 w_i w_j = -N w_i w_j\n$$\n\nTo demonstrate that the pairwise covariances are negative for $i \\neq j$:\nThe derived expression is $\\operatorname{Cov}(A_i, A_j) = -N w_i w_j$.\nAccording to the problem statement:\n1. $N$ is the number of offspring, which is a positive integer, so $N > 0$.\n2. The weights satisfy $w_k > 0$ for all $k \\in \\{1, \\dots, M\\}$. This means $w_i > 0$ and $w_j > 0$.\nThe product $N w_i w_j$ is a product of three positive numbers, hence it is positive.\nTherefore, $\\operatorname{Cov}(A_i, A_j) = -N w_i w_j < 0$. This confirms that the pairwise covariances are negative. This is an expected result due to the constraint $\\sum_{k=1}^{M} A_k = N$; an increase in the count of one category must be compensated by a decrease in the count of others.\n\nThe final expressions are:\n$\\operatorname{Var}(A_i) = N w_i(1 - w_i)$\n$\\operatorname{Cov}(A_i, A_j) = -N w_i w_j$ for $i \\neq j$.", "answer": "$$\n\\boxed{\\begin{pmatrix} N w_i(1-w_i) & -N w_i w_j \\end{pmatrix}}\n$$", "id": "3336496"}, {"introduction": "While multinomial resampling is simple to implement, more sophisticated methods often yield better performance. This practice [@problem_id:3336501] challenges you to quantitatively compare multinomial and stratified resampling on a critical performance metric: the expected number of unique particles that survive the resampling step. Deriving this from basic principles will solidify your understanding of how stratified sampling reduces the stochasticity of the selection process, thereby preserving greater particle diversity.", "problem": "Consider a single resampling step in a Sequential Monte Carlo (SMC) algorithm with $N \\geq 2$ particles and normalized weights $\\tilde w_{1:N}$ satisfying $\\tilde w_i \\geq 0$ and $\\sum_{i=1}^{N} \\tilde w_i = 1$. Define the cumulative weights $C_0 = 0$ and $C_i = \\sum_{k=1}^{i} \\tilde w_k$ for $i \\in \\{1,\\dots,N\\}$. Let $I_i = [C_{i-1}, C_i)$ denote the weight interval associated with particle $i$. One resampling step produces $N$ offspring from the weighted population according to one of the following schemes:\n\n- Multinomial resampling: draw $N$ independent samples from the discrete distribution on $\\{1,\\dots,N\\}$ with probabilities $\\tilde w_{1:N}$.\n\n- Stratified resampling: draw $V_1,\\dots,V_N$ independently with $V_j \\sim \\mathrm{Uniform}(0,1)$, set $U_j = \\frac{j-1+V_j}{N}$ for $j \\in \\{1,\\dots,N\\}$, and assign offspring $j$ to the unique $i$ such that $U_j \\in I_i$.\n\nFor a resampling scheme, define the offspring counts $N_i = \\sum_{j=1}^{N} \\mathbf{1}\\{U_j \\in I_i\\}$ and the number of distinct ancestors $K = \\sum_{i=1}^{N} \\mathbf{1}\\{N_i \\geq 1\\}$. You may use the following basic definitions: for any event $A$, $\\mathbf{1}\\{A\\}$ denotes the indicator function; for any real $x$, $\\lfloor x \\rfloor$ is the floor (greatest integer $\\leq x$) and $\\{x\\} = x - \\lfloor x \\rfloor$ is the fractional part.\n\nStarting from these definitions alone, derive the conditional expectation $\\mathbb{E}[K \\mid \\tilde w_{1:N}]$ under multinomial resampling and under stratified resampling. Express your final expressions solely in terms of $N$, $\\tilde w_{1:N}$, and the cumulative sums $C_i$. Your derivations must justify each step from first principles of probability and the construction of the two resamplers.\n\nReport your final answer as a single row matrix containing the two expressions $[\\mathbb{E}_{\\mathrm{mult}}(K \\mid \\tilde w_{1:N}),\\, \\mathbb{E}_{\\mathrm{strat}}(K \\mid \\tilde w_{1:N})]$. No numerical evaluation is required, and no rounding is needed.", "solution": "The object of interest is the conditional expectation of the number of distinct ancestors, $K$, given the normalized weights $\\tilde w_{1:N}$. For brevity, we denote this expectation as $\\mathbb{E}[K]$, as the conditioning on the weights is implicit throughout the derivation.\n\nThe number of distinct ancestors is defined as $K = \\sum_{i=1}^{N} \\mathbf{1}\\{N_i \\geq 1\\}$, where $N_i$ is the number of offspring of particle $i$. Using the linearity of expectation, we have:\n$$\n\\mathbb{E}[K] = \\mathbb{E}\\left[\\sum_{i=1}^{N} \\mathbf{1}\\{N_i \\geq 1\\}\\right] = \\sum_{i=1}^{N} \\mathbb{E}[\\mathbf{1}\\{N_i \\geq 1\\}]\n$$\nThe expectation of an indicator function is the probability of the event it indicates. Therefore:\n$$\n\\mathbb{E}[K] = \\sum_{i=1}^{N} P(N_i \\geq 1)\n$$\nIt is often more convenient to calculate the probability of the complementary event, $\\{N_i = 0\\}$, which is the event that particle $i$ has no offspring.\n$$\nP(N_i \\geq 1) = 1 - P(N_i = 0)\n$$\nSubstituting this into the expression for $\\mathbb{E}[K]$ gives:\n$$\n\\mathbb{E}[K] = \\sum_{i=1}^{N} (1 - P(N_i = 0)) = N - \\sum_{i=1}^{N} P(N_i = 0)\n$$\nThe remainder of the derivation consists of calculating $P(N_i = 0)$ for each of the two resampling schemes.\n\n### Multinomial Resampling\n\nIn multinomial resampling, $N$ offspring are drawn independently from the categorical distribution defined by the weights $\\tilde w_{1:N}$. For a single draw, the probability of selecting particle $i$ is $\\tilde w_i$. Consequently, the probability of *not* selecting particle $i$ in a single draw is $1 - \\tilde w_i$.\n\nThe event $\\{N_i = 0\\}$ occurs if and only if particle $i$ is not selected in any of the $N$ independent draws. The probability of this event is the product of the probabilities of not selecting particle $i$ in each of the $N$ draws:\n$$\nP_{\\mathrm{mult}}(N_i = 0) = (1 - \\tilde w_i)^N\n$$\nSubstituting this result into the general formula for $\\mathbb{E}[K]$:\n$$\n\\mathbb{E}_{\\mathrm{mult}}[K \\mid \\tilde w_{1:N}] = N - \\sum_{i=1}^{N} (1 - \\tilde w_i)^N\n$$\n\n### Stratified Resampling\n\nIn stratified resampling, $N$ ordered uniform random numbers $U_j = \\frac{j-1+V_j}{N}$ are generated, where $V_j \\sim \\mathrm{Uniform}(0,1)$ are independent. Each $U_j$ is thus drawn uniformly from its corresponding stratum $J_j = [\\frac{j-1}{N}, \\frac{j}{N})$. The $j$-th offspring is a copy of particle $i$ if $U_j$ falls into the interval $I_i = [C_{i-1}, C_i)$.\n\nThe number of offspring for particle $i$ is $N_i = \\sum_{j=1}^{N} \\mathbf{1}\\{U_j \\in I_i\\}$. The event $\\{N_i=0\\}$ occurs if no $U_j$ falls into the interval $I_i$. Since the $V_j$ are independent, the $U_j$ are also independent. Therefore, the probability of particle $i$ having no offspring is:\n$$\nP_{\\mathrm{strat}}(N_i = 0) = P\\left(\\bigcap_{j=1}^{N} \\{U_j \\notin I_i\\}\\right) = \\prod_{j=1}^{N} P(U_j \\notin I_i) = \\prod_{j=1}^{N} (1 - P(U_j \\in I_i))\n$$\nThe probability $P(U_j \\in I_i)$ is the ratio of the length of the intersection of the two intervals to the length of the support of $U_j$:\n$$\np_{ij} \\equiv P(U_j \\in I_i) = \\frac{\\text{length}(I_i \\cap J_j)}{\\text{length}(J_j)} = \\frac{\\text{length}([C_{i-1}, C_i) \\cap [\\frac{j-1}{N}, \\frac{j}{N}))}{1/N} = N \\cdot \\text{length}([C_{i-1}, C_i) \\cap [\\frac{j-1}{N}, \\frac{j}{N}))\n$$\nThe value of $p_{ij}$ depends on the relationship between the interval $I_i$ and the strata $J_j$. Let us scale the problem by a factor of $N$. Let $a = NC_{i-1}$ and $b = NC_i$. The scaled interval is $[a, b)$ of length $N\\tilde w_i$. The scaled strata are $[j-1, j)$. Then $p_{ij} = \\text{length}([a,b)\\cap[j-1,j))$.\n\nWe analyze $P(N_i=0)$ based on how many stratum boundaries $k/N$ (for integers $k$) lie within the interval $I_i$. This is determined by the value $\\lfloor N C_i \\rfloor - \\lfloor N C_{i-1} \\rfloor$.\n\n**Case 1: $\\lfloor NC_i \\rfloor = \\lfloor NC_{i-1} \\rfloor$.**\nLet this common integer value be $k$. This means $k \\leq NC_{i-1}$ and $NC_i < k+1$. The entire interval $I_i$ is contained within a single stratum $J_{k+1} = [\\frac{k}{N}, \\frac{k+1}{N})$.\nFor $j \\neq k+1$, the intersection $I_i \\cap J_j$ is empty, so $p_{ij}=0$.\nFor $j=k+1$, $p_{i,k+1} = N \\cdot \\text{length}([C_{i-1}, C_i)) = N(C_i-C_{i-1}) = N\\tilde w_i$.\nIn this case, only one term in the product for $P(N_i=0)$ is not $1$:\n$$\nP(N_i=0) = 1 - p_{i,k+1} = 1 - N\\tilde w_i\n$$\n\n**Case 2: $\\lfloor NC_i \\rfloor = \\lfloor NC_{i-1} \\rfloor + 1$.**\nLet $k=\\lfloor NC_{i-1} \\rfloor$. Then $\\lfloor NC_i \\rfloor = k+1$. The interval $I_i$ crosses exactly one stratum boundary, $\\frac{k+1}{N}$. It intersects two strata, $J_{k+1}$ and $J_{k+2}$.\nFor $j \\notin \\{k+1, k+2\\}$, $p_{ij}=0$. The product for $P(N_i=0)$ reduces to $(1-p_{i,k+1})(1-p_{i,k+2})$.\nThe probability for the first intersecting stratum is:\n$p_{i,k+1} = N \\cdot \\text{length}([C_{i-1}, \\frac{k+1}{N})) = N(\\frac{k+1}{N} - C_{i-1}) = k+1 - NC_{i-1} = 1 - (NC_{i-1} - k) = 1 - \\{NC_{i-1}\\}$.\nThe probability for the second intersecting stratum is:\n$p_{i,k+2} = N \\cdot \\text{length}([\\frac{k+1}{N}, C_i)) = N(C_i - \\frac{k+1}{N}) = NC_i - (k+1) = NC_i - \\lfloor NC_i \\rfloor = \\{NC_i\\}$.\nThus, the probability of no offspring is:\n$$\nP(N_i=0) = (1 - p_{i,k+1})(1-p_{i,k+2}) = (1 - (1-\\{NC_{i-1}\\}))(1 - \\{NC_i\\}) = \\{NC_{i-1}\\}(1 - \\{NC_i\\})\n$$\n\n**Case 3: $\\lfloor NC_i \\rfloor \\geq \\lfloor NC_{i-1} \\rfloor + 2$.**\nIn this case, the interval $I_i$ is wide enough to entirely contain at least one stratum. Let $k = \\lfloor NC_{i-1} \\rfloor + 1$. The stratum $J_{k+1} = [\\frac{k}{N}, \\frac{k+1}{N})$ is fully contained within $I_i$. This is because $C_{i-1} < \\frac{k}{N}$ and $C_i > \\frac{k+1}{N}$.\nFor $j = k+1$, $p_{ij} = N \\cdot \\text{length}([\\frac{k}{N}, \\frac{k+1}{N})) = N \\cdot \\frac{1}{N} = 1$.\nThe product for $P(N_i = 0)$ contains a factor $(1-p_{i,k+1}) = (1-1) = 0$. Therefore:\n$$\nP(N_i=0) = 0\n$$\n\nCombining these three cases using indicator functions, we obtain a single expression for $P(N_i=0)$:\n$$\nP_{\\mathrm{strat}}(N_i=0) = (1-N\\tilde{w}_i)\\mathbf{1}_{\\{\\lfloor NC_i \\rfloor = \\lfloor NC_{i-1} \\rfloor\\}} + \\{NC_{i-1}\\}(1 - \\{NC_i\\})\\mathbf{1}_{\\{\\lfloor NC_i \\rfloor = \\lfloor NC_{i-1} \\rfloor+1\\}}\n$$\nwhere $\\{x\\} = x-\\lfloor x \\rfloor$.\nThe total expected number of distinct ancestors under stratified resampling is:\n$$\n\\mathbb{E}_{\\mathrm{strat}}[K \\mid \\tilde w_{1:N}] = N - \\sum_{i=1}^{N} \\left[ (1-N\\tilde{w}_i)\\mathbf{1}_{\\{\\lfloor NC_i \\rfloor = \\lfloor NC_{i-1} \\rfloor\\}} + \\{NC_{i-1}\\}(1 - \\{NC_i\\})\\mathbf{1}_{\\{\\lfloor NC_i \\rfloor = \\lfloor NC_{i-1} \\rfloor+1\\}} \\right]\n$$\nThis expression can be written entirely in terms of $N$, cumulative sums $C_i$, and the floor function as requested:\n$$\n\\mathbb{E}_{\\mathrm{strat}}[K \\mid \\tilde w_{1:N}] = N - \\sum_{i=1}^{N} \\left[ (1-N(C_i-C_{i-1}))\\mathbf{1}_{\\{\\lfloor NC_i \\rfloor = \\lfloor NC_{i-1} \\rfloor\\}} + (NC_{i-1}-\\lfloor NC_{i-1} \\rfloor)(1 - (NC_i-\\lfloor NC_i \\rfloor))\\mathbf{1}_{\\{\\lfloor NC_i \\rfloor = \\lfloor NC_{i-1} \\rfloor+1\\}} \\right]\n$$\n\nThe final answer is a row matrix containing the two expressions.", "answer": "$$\n\\boxed{\\begin{pmatrix} N - \\sum_{i=1}^{N} (1 - \\tilde w_i)^N & N - \\sum_{i=1}^{N} \\left[ (1 - N \\tilde w_i) \\mathbf{1}_{\\{\\lfloor N C_i \\rfloor = \\lfloor N C_{i-1} \\rfloor\\}} + (N C_{i-1} - \\lfloor N C_{i-1} \\rfloor) (1 - (N C_i - \\lfloor N C_i \\rfloor)) \\mathbf{1}_{\\{\\lfloor N C_i \\rfloor = \\lfloor N C_{i-1} \\rfloor + 1\\}} \\right] \\end{pmatrix}}\n$$", "id": "3336501"}, {"introduction": "The choice between advanced resampling schemes like stratified and systematic resampling involves subtle trade-offs that can significantly impact estimator accuracy. This advanced problem [@problem_id:3336458] asks you to analyze the difference in the conditional variance of a linear statistic produced by these two methods. By deriving this difference, you will uncover the structural conditions under which the commonly preferred systematic resampling can paradoxically yield higher variance, a vital lesson in selecting the right algorithm for a given problem structure.", "problem": "Consider a fixed array of normalized importance weights $(w_{1},\\dots,w_{m})$ with $\\sum_{i=1}^{m} w_{i} = 1$ and a fixed array of real numbers $(f_{1},\\dots,f_{m})$. Let $N \\in \\mathbb{N}$ be the resampling size, and define cumulative weights $c_{0} := 0$ and $c_{i} := \\sum_{k=1}^{i} w_{k}$ for $i=1,\\dots,m$. Let the resampling counts $(A_{1},\\dots,A_{m})$ be constructed under two schemes:\n\n- Stratified resampling: Independently sample $U_{j} \\sim \\mathrm{Uniform}\\big((j-1)/N,\\, j/N\\big)$ for $j=1,\\dots,N$, and set $A_{i} := \\#\\{j: U_{j} \\in (c_{i-1}, c_{i}]\\}$.\n- Systematic resampling: Sample a single $U \\sim \\mathrm{Uniform}(0,1)$, set $u_{j} := \\big(U + (j-1)/N\\big) \\bmod 1$ for $j=1,\\dots,N$, and define $A_{i} := \\#\\{j: u_{j} \\in (c_{i-1}, c_{i}]\\}$.\n\nFor the linear statistic $S := \\sum_{i=1}^{m} A_{i} f_{i}$, derive from first principles an exact expression for the conditional variance difference\n$$\nD := \\mathrm{Var}\\!\\left(S \\,\\middle|\\, w_{1:m}, f_{1:m}\\right)_{\\mathrm{systematic}} - \\mathrm{Var}\\!\\left(S \\,\\middle|\\, w_{1:m}, f_{1:m}\\right)_{\\mathrm{stratified}}.\n$$\nYour derivation must start from the definitions above and the elementary properties of variance and of the floor function, without invoking pre-packaged resampling variance formulas. Express your final answer as a closed-form analytic expression that depends only on $(f_{i})$, $(w_{i})$, and $N$, using quantities you define along the way.\n\nIn your explanation, also identify (qualitatively, without numerical computation) a structural condition on $(f_{i})$ and $(w_{i})$ under which the systematic resampling scheme can have larger conditional variance than the stratified scheme. The final answer must be a single analytic expression for $D$. No numerical rounding is required, and no physical units apply.", "solution": "The problem asks for the difference in conditional variances of a linear statistic $S$ under two resampling schemes: systematic and stratified. The statistic is defined as $S := \\sum_{i=1}^{m} A_{i} f_{i}$, where $A_i$ are the resampling counts. The arrays $(w_{1:m})$ and $(f_{1:m})$ are fixed.\n\nLet us define a function $g: [0,1] \\to \\mathbb{R}$ as a staircase function constructed from the given values:\n$$\ng(u) := \\sum_{i=1}^{m} f_{i} \\mathbf{1}_{(c_{i-1}, c_{i}]}(u)\n$$\nwhere $\\mathbf{1}_I(u)$ is the indicator function for the interval $I$. The cumulative weights are $c_0=0$ and $c_i = \\sum_{k=1}^i w_k$.\n\nThe resampling count $A_i$ is the number of resampled points that fall into the interval $(c_{i-1}, c_i]$. For any resampled point $u_j$, the value of the function $g(u_j)$ is $f_i$ if and only if $u_j \\in (c_{i-1}, c_i]$. Thus, we can express the statistic $S$ as a sum over the resampled points:\n$$\nS = \\sum_{j=1}^{N} g(u_j)\n$$\nwhere $u_1, \\dots, u_N$ are the $N$ resampled points. The distribution of these points depends on the resampling scheme.\n\nFor both schemes, the expectation of $S$ is the same. Let's verify this.\nFor stratified resampling, $U_j \\sim \\mathrm{Uniform}(I_j)$ independently, where $I_j = ((j-1)/N, j/N)$.\n$$\n\\mathrm{E}[S]_{\\mathrm{strat}} = \\mathrm{E}\\left[\\sum_{j=1}^{N} g(U_j)\\right] = \\sum_{j=1}^{N} \\mathrm{E}[g(U_j)] = \\sum_{j=1}^{N} \\int_{(j-1)/N}^{j/N} g(u) \\frac{1}{1/N} du = N \\sum_{j=1}^{N} \\int_{I_j} g(u)du = N \\int_0^1 g(u)du\n$$\nFor systematic resampling, $u_j = (U+(j-1)/N) \\pmod 1$ where $U \\sim \\mathrm{Uniform}(0,1)$.\n$$\n\\mathrm{E}[S]_{\\mathrm{sys}} = \\mathrm{E}\\left[\\sum_{j=1}^{N} g(u_j)\\right] = \\sum_{j=1}^{N} \\mathrm{E}[g(u_j)]\n$$\nFor any $j$, $u_j$ is uniform on $(0,1)$, so $\\mathrm{E}[g(u_j)] = \\int_0^1 g(u)du$. Thus,\n$$\n\\mathrm{E}[S]_{\\mathrm{sys}} = \\sum_{j=1}^{N} \\int_0^1 g(u)du = N \\int_0^1 g(u)du\n$$\nThe integral $\\int_0^1 g(u)du = \\sum_{i=1}^m f_i (c_i - c_{i-1}) = \\sum_{i=1}^m f_i w_i$. So, $\\mathrm{E}[S] = N \\sum_{i=1}^m f_i w_i$ for both schemes.\n\nNow we compute the variances.\n\n**Variance for Stratified Resampling**\nIn stratified resampling, the random variables $U_j$ are independent. Therefore, the random variables $F_j := g(U_j)$ are independent.\nThe variance of the sum is the sum of the variances:\n$$\n\\mathrm{Var}(S)_{\\mathrm{strat}} = \\mathrm{Var}\\left(\\sum_{j=1}^{N} F_j\\right) = \\sum_{j=1}^{N} \\mathrm{Var}(F_j)\n$$\nThe variance of each $F_j$ is calculated with respect to its own distribution, $U_j \\sim \\mathrm{Uniform}(I_j)$:\n$$\n\\mathrm{Var}(F_j) = \\mathrm{E}[F_j^2] - (\\mathrm{E}[F_j])^2\n$$\n$$\n\\mathrm{E}[F_j] = N \\int_{I_j} g(u)du\n$$\n$$\n\\mathrm{E}[F_j^2] = N \\int_{I_j} g(u)^2 du\n$$\nSo, the total variance for the stratified scheme is:\n$$\n\\mathrm{Var}(S)_{\\mathrm{strat}} = \\sum_{j=1}^{N} \\left( N \\int_{I_j} g(u)^2 du - \\left(N \\int_{I_j} g(u)du\\right)^2 \\right)\n$$\nThis can be written as:\n$$\n\\mathrm{Var}(S)_{\\mathrm{strat}} = N \\int_0^1 g(u)^2 du - N^2 \\sum_{j=1}^{N} \\left(\\int_{(j-1)/N}^{j/N} g(u)du\\right)^2\n$$\n\n**Variance for Systematic Resampling**\nIn systematic resampling, all points $u_j$ depend on a single random variable $U \\sim \\mathrm{Uniform}(0,1)$. The statistic $S(U) = \\sum_{j=1}^N g((U+(j-1)/N) \\pmod 1)$ is a function of $U$.\nThe function $S(U)$ is periodic with period $1/N$. To see this:\n$$\nS(U+1/N) = \\sum_{j=1}^N g\\left(\\left(U+\\frac{1}{N}+\\frac{j-1}{N}\\right) \\pmod 1\\right) = \\sum_{j=1}^N g\\left(\\left(U+\\frac{j}{N}\\right) \\pmod 1\\right)\n$$\nLet $k=j+1$. The sum becomes $\\sum_{k=2}^{N+1} g((U+(k-1)/N)\\pmod 1)$. Since $g((U+N/N)\\pmod 1) = g(U+1 \\pmod 1) = g(U) = g((U+1-1)/N) \\pmod 1)$, the set of summands is merely permuted. Thus $S(U+1/N)=S(U)$.\n\nBecause $S(U)$ is $1/N$-periodic, its variance can be computed by considering $U$ to be uniform on any interval of length $1/N$, for instance $U \\sim \\mathrm{Uniform}(0, 1/N)$. For $U \\in [0, 1/N)$, the modulo operation is not needed for the points $u_j = U+(j-1)/N$.\nLet us redefine $U \\sim \\mathrm{Uniform}(0,1/N)$. The variance is given by:\n$$\n\\mathrm{Var}(S)_{\\mathrm{sys}} = \\mathrm{E}[S(U)^2] - (\\mathrm{E}[S(U)])^2\n$$\nThe expectation is over $U \\sim \\mathrm{Uniform}(0,1/N)$. The PDF is $N$ on this interval.\n$$\n\\mathrm{E}[S(U)] = N \\int_0^{1/N} S(u)du = N \\int_0^{1/N} \\sum_{j=1}^N g(u+\\frac{j-1}{N})du\n$$\n$$\n= N \\sum_{j=1}^N \\int_0^{1/N} g(u+\\frac{j-1}{N})du = N \\sum_{j=1}^N \\int_{(j-1)/N}^{j/N} g(v)dv = N \\int_0^1 g(v)dv\n$$\nThis matches the expectation calculated earlier.\nThe second moment is:\n$$\n\\mathrm{E}[S(U)^2] = N \\int_0^{1/N} \\left( \\sum_{j=1}^N g(u+\\frac{j-1}{N}) \\right)^2 du\n$$\nSo, the variance for the systematic scheme is:\n$$\n\\mathrm{Var}(S)_{\\mathrm{sys}} = N \\int_0^{1/N} \\left( \\sum_{j=1}^N g\\left(u+\\frac{j-1}{N}\\right) \\right)^2 du - \\left( N \\int_0^1 g(u)du \\right)^2\n$$\n\n**Difference in Variances**\nLet's find the difference $D = \\mathrm{Var}(S)_{\\mathrm{sys}} - \\mathrm{Var}(S)_{\\mathrm{strat}}$.\nLet $G_j = \\int_{(j-1)/N}^{j/N} g(u)du$. Then $\\int_0^1 g(u)du = \\sum_j G_j$.\n$$\n\\mathrm{Var}(S)_{\\mathrm{sys}} = N \\int_0^{1/N} \\left( \\sum_{j=1}^N g\\left(u+\\frac{j-1}{N}\\right) \\right)^2 du - N^2 \\left( \\sum_{j=1}^N G_j \\right)^2\n$$\n$$\n\\mathrm{Var}(S)_{\\mathrm{strat}} = N \\sum_{j=1}^N \\int_{(j-1)/N}^{j/N} g(u)^2 du - N^2 \\sum_{j=1}^N G_j^2\n$$\n$D = N \\int_0^{1/N} \\left(\\sum_j g(u+\\frac{j-1}{N})\\right)^2 du - N \\sum_j \\int_{I_j} g^2(u)du - N^2 [(\\sum_j G_j)^2 - \\sum_j G_j^2]$.\nLet's expand the squared sum in the first term:\n$\\left(\\sum_j g(u+\\frac{j-1}{N})\\right)^2 = \\sum_j g(u+\\frac{j-1}{N})^2 + \\sum_{j \\neq k} g(u+\\frac{j-1}{N})g(u+\\frac{k-1}{N})$.\nIntegrating from $0$ to $1/N$:\n$\\int_0^{1/N} \\left(\\sum_j g(u+\\frac{j-1}{N})\\right)^2 du = \\sum_j \\int_0^{1/N} g(u+\\frac{j-1}{N})^2 du + \\sum_{j \\neq k} \\int_0^{1/N} g(u+\\frac{j-1}{N})g(u+\\frac{k-1}{N}) du$.\nA change of variables $v=u+(j-1)/N$ gives $\\int_0^{1/N} g(u+\\frac{j-1}{N})^2 du = \\int_{I_j} g(v)^2 dv$.\nSo, $\\int_0^{1/N} \\left(\\sum_j \\dots\\right)^2 du = \\sum_j \\int_{I_j} g(v)^2 dv + \\sum_{j \\neq k} \\int_0^{1/N} g(u+\\frac{j-1}{N})g(u+\\frac{k-1}{N}) du$.\nSubstituting this into the expression for $D$:\n$$\nD = N \\left(\\sum_j \\int_{I_j} g^2 dv + \\sum_{j \\neq k} \\dots \\right) - N\\sum_j \\int_{I_j}g^2 dv - N^2 \\sum_{j \\neq k} G_j G_k\n$$\n$$\nD = N \\sum_{j \\neq k} \\int_0^{1/N} g\\left(u+\\frac{j-1}{N}\\right) g\\left(u+\\frac{k-1}{N}\\right) du - N^2 \\sum_{j \\neq k} G_j G_k\n$$\nThis expression can be nicely formulated as a sum of covariances. Let $U \\sim \\mathrm{Uniform}(0, 1/N)$ and define random variables $C_j(U) = g(U+\\frac{j-1}{N})$ for $j=1,\\dots,N$.\n$\\mathrm{E}[C_j] = N \\int_0^{1/N} g(u+\\frac{j-1}{N})du = N G_j$.\n$\\mathrm{Var}(S)_{\\mathrm{strat}} = \\sum_j \\mathrm{Var}(C_j(U_j))$ where $U_j$ are iid $\\sim U(0,1/N)$. Due to iid, this is $\\sum_j \\mathrm{Var}(C_j(U))$.\n$\\mathrm{Var}(S)_{\\mathrm{sys}} = \\mathrm{Var}(\\sum_j C_j(U)) = \\sum_j \\mathrm{Var}(C_j(U)) + \\sum_{j \\neq k} \\mathrm{Cov}(C_j(U), C_k(U))$.\nTherefore, $D = \\sum_{j \\neq k} \\mathrm{Cov}(C_j(U), C_k(U))$.\nThe covariance term is:\n$\\mathrm{Cov}(C_j, C_k) = \\mathrm{E}[C_j C_k] - \\mathrm{E}[C_j]\\mathrm{E}[C_k] = N \\int_0^{1/N} g(u+\\frac{j-1}{N})g(u+\\frac{k-1}{N})du - (N G_j)(N G_k)$.\nSumming over $j \\neq k$ gives the final expression for $D$.\n\n**Condition for $D > 0$**\nThe difference $D$ is the sum of cross-covariances $\\mathrm{Cov}(C_j, C_k)$. $D > 0$ if these covariances are, on average, positive.\nThe random variable $C_j(U) = g(U+(j-1)/N)$ represents the value of $g$ as we traverse the $j$-th stratum $I_j=((j-1)/N, j/N)$. $C_k(U)$ is the value of $g$ as we traverse the $k$-th stratum $I_k$. A positive covariance means that the functions $u \\mapsto g(u+(j-1)/N)$ and $u \\mapsto g(u+(k-1)/N)$ tend to move above or below their respective means together.\nThis occurs if the function $g(u)$ exhibits periodic behavior, or strong harmonic content at frequencies matching integer multiples of $N$. Consider the case where the pairs $(w_i, f_i)$ are ordered such that $g(u)$ is approximately periodic with a period of $1/N$. Then $g(u+\\frac{j-1}{N}) \\approx g(u)$ for all $j$. This would make $C_j(U) \\approx C_k(U)$ for all $j, k$, resulting in $\\mathrm{Cov}(C_j, C_k) \\approx \\mathrm{Var}(C_j) > 0$. This positive correlation across all strata inflates the systematic variance compared to the stratified variance (which only contains the variance terms). Such a periodic structure in $g(u)$ can be created by arranging the $f_i$ values in an oscillatory pattern that is resonant with the stratification grid. For example, if the weights $w_i$ are roughly equal, and the sequence of $f_i$ values alternates between high and low values with a period that aligns with $N$, $D$ can become positive.\n\nThe final expression for $D$ is:\n$$\nD = \\sum_{j=1, k=1, j \\neq k}^{N} \\left( N \\int_{0}^{1/N} g\\left(u+\\frac{j-1}{N}\\right) g\\left(u+\\frac{k-1}{N}\\right) du - N^2 \\left(\\int_{\\frac{j-1}{N}}^{\\frac{j}{N}} g(v)dv\\right) \\left(\\int_{\\frac{k-1}{N}}^{\\frac{k}{N}} g(w)dw\\right) \\right)\n$$\nwhere $g(x) = \\sum_{i=1}^m f_i \\mathbf{1}_{(c_{i-1}, c_i]}(x)$.", "answer": "$$\n\\boxed{\\sum_{j=1, k=1, j \\neq k}^{N} \\left( N \\int_{0}^{\\frac{1}{N}} g\\left(u+\\frac{j-1}{N}\\right) g\\left(u+\\frac{k-1}{N}\\right) du - N^2 \\left(\\int_{\\frac{j-1}{N}}^{\\frac{j}{N}} g(v)dv\\right) \\left(\\int_{\\frac{k-1}{N}}^{\\frac{k}{N}} g(w)dw\\right) \\right)}\n$$", "id": "3336458"}]}