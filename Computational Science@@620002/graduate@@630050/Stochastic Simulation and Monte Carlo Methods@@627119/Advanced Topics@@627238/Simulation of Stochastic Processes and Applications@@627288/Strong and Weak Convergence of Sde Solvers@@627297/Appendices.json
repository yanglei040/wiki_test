{"hands_on_practices": [{"introduction": "A common challenge in stochastic simulation is the slow convergence of numerical methods. This practice demonstrates a powerful technique, Richardson extrapolation, to accelerate the weak convergence of the Euler–Maruyama scheme. By strategically combining results from two different step sizes, you will learn how to cancel the leading-order error term, effectively improving the method's accuracy without significant additional complexity [@problem_id:3349725].", "problem": "Consider the scalar stochastic differential equation (SDE) $dX_{t} = a(X_{t})\\,dt + b(X_{t})\\,dW_{t}$ on $t \\in [0,T]$ with initial condition $X_{0} = x_{0}$, where $a$ and $b$ are sufficiently smooth and with polynomial growth, and $W_{t}$ is a standard Wiener process. Let $\\varphi:\\mathbb{R}\\to\\mathbb{R}$ be a bounded smooth test function. Define $P := \\mathbb{E}[\\varphi(X_{T})]$ and, for a time step $h := T/N$ with integer $N \\geq 1$, let $X^{h}_{n+1} = X^{h}_{n} + a(X^{h}_{n})\\,h + b(X^{h}_{n})\\,\\Delta W_{n}$, where $\\Delta W_{n} \\sim \\mathcal{N}(0,h)$ are independent, and set $P_{h} := \\mathbb{E}[\\varphi(X^{h}_{N})]$. Assume the Talay–Tubaro expansion holds for the Euler–Maruyama weak error up to terms of order $O(h^{3})$, namely that there exist constants $c_{1}$ and $c_{2}$ (depending on $a$, $b$, $\\varphi$, and $T$) such that\n$$\nP_{h} = P + c_{1}\\,h + c_{2}\\,h^{2} + O(h^{3}).\n$$\nUsing only the above assumptions and definitions, construct a Richardson-extrapolated approximation of $P$ of the form $R(h) = \\alpha\\,P_{h} + \\beta\\,P_{h/2}$ that improves the weak order of the Euler–Maruyama method from order $1$ to order $2$. Derive the explicit values of the coefficients $\\alpha$ and $\\beta$ by cancelling the leading-order bias using the Talay–Tubaro expansion up to $O(h^{2})$ terms. Provide your final answer as the row vector $(\\alpha,\\beta)$, written as a $1 \\times 2$ matrix using the $\\mathrm{pmatrix}$ environment. No rounding is required.", "solution": "The problem requires the construction of a Richardson-extrapolated approximation, $R(h)$, of a quantity $P = \\mathbb{E}[\\varphi(X_{T})]$, where $X_T$ is the solution of a scalar stochastic differential equation at time $T$. The approximation is to be constructed from two Euler-Maruyama approximations, $P_{h}$ and $P_{h/2}$, computed with step sizes $h$ and $h/2$, respectively. The goal is to determine the coefficients $\\alpha$ and $\\beta$ in the linear combination $R(h) = \\alpha\\,P_{h} + \\beta\\,P_{h/2}$ such that the weak order of convergence is improved from $1$ to $2$.\n\nThe foundation of the derivation is the provided Talay–Tubaro expansion for the weak error of the Euler–Maruyama method. The expansion for the approximation $P_{h}$ with step size $h$ is given as:\n$$\nP_{h} = P + c_{1}\\,h + c_{2}\\,h^{2} + O(h^{3})\n$$\nwhere $c_{1}$ and $c_{2}$ are constants independent of the step size $h$.\n\nTo apply Richardson extrapolation, we also need the expansion for the approximation computed with a different step size. We choose the step size $h/2$. Replacing $h$ with $h/2$ in the above expansion gives the expression for $P_{h/2}$:\n$$\nP_{h/2} = P + c_{1}\\left(\\frac{h}{2}\\right) + c_{2}\\left(\\frac{h}{2}\\right)^{2} + O\\left(\\left(\\frac{h}{2}\\right)^{3}\\right)\n$$\nSimplifying the terms involving $h$, we get:\n$$\nP_{h/2} = P + \\frac{c_{1}}{2}\\,h + \\frac{c_{2}}{4}\\,h^{2} + O(h^{3})\n$$\nThe Richardson-extrapolated approximation is defined as $R(h) = \\alpha\\,P_{h} + \\beta\\,P_{h/2}$. We substitute the expansions for $P_h$ and $P_{h/2}$ into this definition:\n$$\nR(h) = \\alpha\\left(P + c_{1}\\,h + c_{2}\\,h^{2} + O(h^{3})\\right) + \\beta\\left(P + \\frac{c_{1}}{2}\\,h + \\frac{c_{2}}{4}\\,h^{2} + O(h^{3})\\right)\n$$\nWe now collect terms based on their dependence on $P$ and powers of $h$:\n$$\nR(h) = (\\alpha + \\beta)P + \\left(\\alpha c_{1} + \\frac{\\beta c_{1}}{2}\\right)h + \\left(\\alpha c_{2} + \\frac{\\beta c_{2}}{4}\\right)h^{2} + O(h^{3})\n$$\nThis can be rewritten by factoring out the constants $c_1$ and $c_2$:\n$$\nR(h) = (\\alpha + \\beta)P + c_{1}\\left(\\alpha + \\frac{\\beta}{2}\\right)h + c_{2}\\left(\\alpha + \\frac{\\beta}{4}\\right)h^{2} + O(h^{3})\n$$\nThe goal is to make $R(h)$ a better approximation of $P$ than either $P_h$ or $P_{h/2}$. Specifically, we want the error $R(h) - P$ to be of order $O(h^{2})$. The error is given by:\n$$\nR(h) - P = (\\alpha + \\beta - 1)P + c_{1}\\left(\\alpha + \\frac{\\beta}{2}\\right)h + c_{2}\\left(\\alpha + \\frac{\\beta}{4}\\right)h^{2} + O(h^{3})\n$$\nFor $R(h)$ to be a consistent approximation of $P$, the expression for $R(h)$ must converge to $P$ as $h \\to 0$. This requires that the coefficient of $P$ is $1$. Therefore, we must have:\n$$\n\\alpha + \\beta = 1\n$$\nThis ensures that $R(h) - P \\to 0$ as $h \\to 0$. To achieve a weak order of $2$, the leading-order error term, which is the term of order $h$, must be eliminated. Assuming $c_{1} \\neq 0$ (which is true in the general case for the Euler-Maruyama method), we must set its coefficient to zero:\n$$\n\\alpha + \\frac{\\beta}{2} = 0\n$$\nWe now have a system of two linear equations with two unknowns, $\\alpha$ and $\\beta$:\n\\begin{align*}\n\\alpha + \\beta = 1 \\quad (1) \\\\\n\\alpha + \\frac{1}{2}\\beta = 0 \\quad (2)\n\\end{align*}\nFrom equation $(2)$, we can express $\\alpha$ in terms of $\\beta$:\n$$\n\\alpha = -\\frac{1}{2}\\beta\n$$\nSubstituting this expression for $\\alpha$ into equation $(1)$:\n$$\n\\left(-\\frac{1}{2}\\beta\\right) + \\beta = 1\n$$\n$$\n\\frac{1}{2}\\beta = 1\n$$\nSolving for $\\beta$ yields:\n$$\n\\beta = 2\n$$\nNow, we substitute the value of $\\beta$ back into the expression for $\\alpha$:\n$$\n\\alpha = -\\frac{1}{2}(2) = -1\n$$\nThus, the coefficients are $\\alpha = -1$ and $\\beta = 2$.\n\nWith these coefficients, the extrapolated estimator is $R(h) = 2P_{h/2} - P_{h}$. The error becomes:\n$$\nR(h) - P = c_{2}\\left((-1) + \\frac{2}{4}\\right)h^{2} + O(h^{3}) = c_{2}\\left(-1 + \\frac{1}{2}\\right)h^{2} + O(h^{3}) = -\\frac{c_{2}}{2}h^{2} + O(h^{3})\n$$\nThe error $|R(h) - P|$ is of order $O(h^{2})$, which confirms that the weak order of convergence has been improved to $2$. The required coefficients are $(\\alpha, \\beta) = (-1, 2)$.", "answer": "$$\n\\boxed{\\begin{pmatrix} -1  2 \\end{pmatrix}}\n$$", "id": "3349725"}, {"introduction": "When moving to multidimensional systems, pathwise accuracy, or strong convergence, often becomes critical. This exercise delves into the Milstein method, a scheme with strong order one, and explores the subtleties that arise in multiple dimensions. You will investigate the conditions under which a simplified component-wise scheme is sufficient, revealing the fundamental role of the commutativity of diffusion coefficients in determining the solver's structure [@problem_id:3349759].", "problem": "Consider the two-dimensional stochastic differential equation (SDE) with diagonal diffusion\n$$\n\\begin{aligned}\ndX_{1}(t) = a_{1}\\!\\left(X_{1}(t)\\right)\\,dt + b_{1}\\!\\left(X_{1}(t)\\right)\\,dW_{1}(t),\\\\\ndX_{2}(t) = a_{2}\\!\\left(X_{2}(t)\\right)\\,dt + b_{2}\\!\\left(X_{2}(t)\\right)\\,dW_{2}(t),\n\\end{aligned}\n$$\nwhere $W_{1}(t)$ and $W_{2}(t)$ are independent standard Brownian motions, and $a_{i}$, $b_{i}$ are globally Lipschitz with bounded derivatives up to second order and linear growth, ensuring a unique strong solution with finite moments. Let $X(0) = (x_{1},x_{2})$ be deterministic. Let $T0$ be fixed and consider a uniform partition $0=t_{0}t_{1}\\cdotst_{N}=T$ with step size $h=T/N$ and Brownian increments $\\Delta W_{i,n} = W_{i}(t_{n+1})-W_{i}(t_{n})$.\n\nDefine the following two numerical schemes:\n\n1. The component-wise Milstein scheme, applied independently to each scalar SDE:\n$$\n\\begin{aligned}\nX^{\\mathrm{comp}}_{1,n+1} = X^{\\mathrm{comp}}_{1,n} + a_{1}\\!\\left(X^{\\mathrm{comp}}_{1,n}\\right) h + b_{1}\\!\\left(X^{\\mathrm{comp}}_{1,n}\\right) \\Delta W_{1,n} + \\tfrac{1}{2} b_{1}\\!\\left(X^{\\mathrm{comp}}_{1,n}\\right) b_{1}'\\!\\left(X^{\\mathrm{comp}}_{1,n}\\right)\\left((\\Delta W_{1,n})^{2}-h\\right),\\\\\nX^{\\mathrm{comp}}_{2,n+1} = X^{\\mathrm{comp}}_{2,n} + a_{2}\\!\\left(X^{\\mathrm{comp}}_{2,n}\\right) h + b_{2}\\!\\left(X^{\\mathrm{comp}}_{2,n}\\right) \\Delta W_{2,n} + \\tfrac{1}{2} b_{2}\\!\\left(X^{\\mathrm{comp}}_{2,n}\\right) b_{2}'\\!\\left(X^{\\mathrm{comp}}_{2,n}\\right)\\left((\\Delta W_{2,n})^{2}-h\\right),\n\\end{aligned}\n$$\nwith $X^{\\mathrm{comp}}_{i,0}=x_{i}$.\n\n2. The full multidimensional Milstein scheme with Lévy areas for the vector SDE $dX = a(X)\\,dt + \\sum_{j=1}^{2} g_{j}(X)\\,dW_{j}$, where $a(x) = (a_{1}(x_{1}), a_{2}(x_{2}))^{\\top}$ and $g_{1}(x)=(b_{1}(x_{1}),0)^{\\top}$, $g_{2}(x)=(0,b_{2}(x_{2}))^{\\top}$:\n$$\nX^{\\mathrm{full}}_{n+1} = X^{\\mathrm{full}}_{n} + a\\!\\left(X^{\\mathrm{full}}_{n}\\right) h + \\sum_{j=1}^{2} g_{j}\\!\\left(X^{\\mathrm{full}}_{n}\\right) \\Delta W_{j,n} + \\sum_{j,k=1}^{2} \\left[D g_{k}\\!\\left(X^{\\mathrm{full}}_{n}\\right) g_{j}\\!\\left(X^{\\mathrm{full}}_{n}\\right)\\right] I_{j,k}^{(n)},\n$$\nwith $X^{\\mathrm{full}}_{0}=(x_{1},x_{2})$, Jacobians $Dg_{k}$, and iterated integrals\n$$\nI_{j,k}^{(n)} = \\int_{t_{n}}^{t_{n+1}} \\int_{t_{n}}^{s_{2}} dW_{j}(s_{1})\\,dW_{k}(s_{2}),\n$$\nfor which $I_{j,j}^{(n)} = \\tfrac{1}{2}\\left((\\Delta W_{j,n})^{2}-h\\right)$ and, for $j\\neq k$, $I_{j,k}^{(n)}$ is approximated by any antisymmetric Lévy area approximation $\\tilde{I}_{j,k}^{(n)}$ satisfying $\\tilde{I}_{j,k}^{(n)}=-\\tilde{I}_{k,j}^{(n)}$, $\\mathbb{E}[\\tilde{I}_{j,k}^{(n)}]=0$, $\\mathbb{E}[|\\tilde{I}_{j,k}^{(n)}|^{2}]=\\theta h^{2}$ with a finite constant $\\theta0$, and independent of $\\Delta W_{j,n}$ and $\\Delta W_{k,n}$.\n\nStarting from the fundamental Itô expansion and the definition of the Milstein method via the first-order Itô–Taylor scheme, reason about the structure of $Dg_{k}(x)g_{j}(x)$ for $j,k \\in \\{1,2\\}$ under this diagonal diffusion. Carefully justify under which algebraic condition on the diffusion vector fields $g_{1}$ and $g_{2}$ the cross terms involving the Lévy areas $I_{j,k}^{(n)}$ with $j\\neq k$ vanish in the full Milstein update.\n\nLet $X^{\\mathrm{comp}}_{T}$ and $X^{\\mathrm{full}}_{T}$ be the terminal values at time $T$ produced by the two schemes with the same Brownian path and the same time step $h$. Compute the exact value of the mean-square difference\n$$\n\\Delta(h) \\equiv \\mathbb{E}\\!\\left[\\left\\|X^{\\mathrm{comp}}_{T} - X^{\\mathrm{full}}_{T}\\right\\|^{2}\\right],\n$$\nas a function of $h$, under the stated diagonal diffusion setting and assumptions on $a_{i}$ and $b_{i}$. Your answer must be a single real number or a closed-form analytic expression. No rounding is required. Also, clearly state the condition under which the simpler component-wise Milstein scheme suffices to achieve the same strong convergence order as the full Milstein with approximate Lévy areas, but do not include this condition in the final boxed answer.", "solution": "The problem requires an analysis of two numerical schemes for a specific two-dimensional stochastic differential equation (SDE) with diagonal diffusion, followed by the computation of the mean-square difference between their outputs.\n\nThe SDE system is given by:\n$$\n\\begin{aligned}\ndX_{1}(t) = a_{1}\\!\\left(X_{1}(t)\\right)\\,dt + b_{1}\\!\\left(X_{1}(t)\\right)\\,dW_{1}(t) \\\\\ndX_{2}(t) = a_{2}\\!\\left(X_{2}(t)\\right)\\,dt + b_{2}\\!\\left(X_{2}(t)\\right)\\,dW_{2}(t)\n\\end{aligned}\n$$\nThis system is fully decoupled, meaning the evolution of $X_{1}$ is independent of $X_{2}$ and vice versa. In vector form, $dX = a(X)dt + \\sum_{j=1}^{2} g_j(X) dW_j(t)$, with $X = (X_1, X_2)^\\top$. The coefficient vector fields are defined as $a(x) = (a_{1}(x_{1}), a_{2}(x_{2}))^{\\top}$, $g_{1}(x)=(b_{1}(x_{1}),0)^{\\top}$, and $g_{2}(x)=(0,b_{2}(x_{2}))^{\\top}$, where $x=(x_1, x_2)^\\top$.\n\nFirst, we address the condition under which the cross terms in the full multidimensional Milstein scheme vanish. The full Milstein scheme is given by:\n$$\nX^{\\mathrm{full}}_{n+1} = X^{\\mathrm{full}}_{n} + a\\!\\left(X^{\\mathrm{full}}_{n}\\right) h + \\sum_{j=1}^{2} g_{j}\\!\\left(X^{\\mathrm{full}}_{n}\\right) \\Delta W_{j,n} + \\sum_{j,k=1}^{2} \\left[D g_{k}\\!\\left(X^{\\mathrm{full}}_{n}\\right) g_{j}\\!\\left(X^{\\mathrm{full}}_{n}\\right)\\right] I_{j,k}^{(n)}\n$$\nThe cross terms are those for which $j \\neq k$. Their coefficients are $[Dg_k(X) g_j(X)]$. We must compute these vectors for $j=1, k=2$ and $j=2, k=1$.\nThe vector fields are $g_{1}(x) = \\begin{pmatrix} b_{1}(x_{1}) \\\\ 0 \\end{pmatrix}$ and $g_{2}(x) = \\begin{pmatrix} 0 \\\\ b_{2}(x_{2}) \\end{pmatrix}$.\nTheir Jacobians are the matrices of first partial derivatives:\n$$\nDg_{1}(x) = \\frac{\\partial g_{1}}{\\partial x} = \\begin{pmatrix} \\frac{\\partial b_{1}(x_{1})}{\\partial x_{1}}  \\frac{\\partial b_{1}(x_{1})}{\\partial x_{2}} \\\\ \\frac{\\partial 0}{\\partial x_{1}}  \\frac{\\partial 0}{\\partial x_{2}} \\end{pmatrix} = \\begin{pmatrix} b_{1}'(x_{1})  0 \\\\ 0  0 \\end{pmatrix}\n$$\n$$\nDg_{2}(x) = \\frac{\\partial g_{2}}{\\partial x} = \\begin{pmatrix} \\frac{\\partial 0}{\\partial x_{1}}  \\frac{\\partial 0}{\\partial x_{2}} \\\\ \\frac{\\partial b_{2}(x_{2})}{\\partial x_{1}}  \\frac{\\partial b_{2}(x_{2})}{\\partial x_{2}} \\end{pmatrix} = \\begin{pmatrix} 0  0 \\\\ 0  b_{2}'(x_{2}) \\end{pmatrix}\n$$\nNow we compute the matrix-vector products for the cross-term coefficients:\nFor $j=1, k=2$:\n$$\nDg_{2}(x) g_{1}(x) = \\begin{pmatrix} 0  0 \\\\ 0  b_{2}'(x_{2}) \\end{pmatrix} \\begin{pmatrix} b_{1}(x_{1}) \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n$$\nFor $j=2, k=1$:\n$$\nDg_{1}(x) g_{2}(x) = \\begin{pmatrix} b_{1}'(x_{1})  0 \\\\ 0  0 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ b_{2}(x_{2}) \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n$$\nSince both coefficient vectors are identically zero, the cross terms involving the iterated integrals $I_{1,2}^{(n)}$ and $I_{2,1}^{(n)}$ vanish regardless of the value of these integrals. This occurs due to the specific structure of the diffusion, where the vector field $g_j$ depends only on the $j$-th component of the state, $x_j$. The general algebraic condition for these terms to vanish is that the diffusion vector fields commute, i.e., their Lie bracket is zero: $[g_j, g_k](x) \\equiv Dg_k(x)g_j(x) - Dg_j(x)g_k(x) = 0$. In our case, $[g_1, g_2] = 0 - 0 = 0$.\n\nNext, we compute the mean-square difference $\\Delta(h) = \\mathbb{E}\\!\\left[\\left\\|X^{\\mathrm{comp}}_{T} - X^{\\mathrm{full}}_{T}\\right\\|^{2}\\right]$.\nWith the cross terms being zero, the full Milstein scheme simplifies to:\n$$\nX^{\\mathrm{full}}_{n+1} = X^{\\mathrm{full}}_{n} + a(X^{\\mathrm{full}}_{n}) h + \\sum_{j=1}^{2} g_{j}(X^{\\mathrm{full}}_{n}) \\Delta W_{j,n} + [Dg_{1}(X^{\\mathrm{full}}_{n}) g_{1}(X^{\\mathrm{full}}_{n})] I_{1,1}^{(n)} + [Dg_{2}(X^{\\mathrm{full}}_{n}) g_{2}(X^{\\mathrm{full}}_{n})] I_{2,2}^{(n)}\n$$\nLet's compute the diagonal term coefficients:\n$$\nDg_{1}(x) g_{1}(x) = \\begin{pmatrix} b_{1}'(x_{1})  0 \\\\ 0  0 \\end{pmatrix} \\begin{pmatrix} b_{1}(x_{1}) \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} b_{1}(x_{1}) b_{1}'(x_{1}) \\\\ 0 \\end{pmatrix}\n$$\n$$\nDg_{2}(x) g_{2}(x) = \\begin{pmatrix} 0  0 \\\\ 0  b_{2}'(x_{2}) \\end{pmatrix} \\begin{pmatrix} 0 \\\\ b_{2}(x_{2}) \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ b_{2}(x_{2}) b_{2}'(x_{2}) \\end{pmatrix}\n$$\nSubstituting these results and the expressions for $a$, $g_j$, and $I_{j,j}^{(n)} = \\frac{1}{2}((\\Delta W_{j,n})^2 - h)$ into the simplified scheme, we can write it out component-wise. Let $X^{\\mathrm{full}}_{n} = (X^{\\mathrm{full}}_{1,n}, X^{\\mathrm{full}}_{2,n})^{\\top}$.\n\nFor the first component:\n$$\nX^{\\mathrm{full}}_{1,n+1} = X^{\\mathrm{full}}_{1,n} + a_{1}(X^{\\mathrm{full}}_{1,n})h + b_{1}(X^{\\mathrm{full}}_{1,n})\\Delta W_{1,n} + b_{1}(X^{\\mathrm{full}}_{1,n})b_{1}'(X^{\\mathrm{full}}_{1,n}) \\left( \\frac{1}{2}((\\Delta W_{1,n})^2 - h) \\right)\n$$\nFor the second component:\n$$\nX^{\\mathrm{full}}_{2,n+1} = X^{\\mathrm{full}}_{2,n} + a_{2}(X^{\\mathrm{full}}_{2,n})h + b_{2}(X^{\\mathrm{full}}_{2,n})\\Delta W_{2,n} + b_{2}(X^{\\mathrm{full}}_{2,n})b_{2}'(X^{\\mathrm{full}}_{2,n}) \\left( \\frac{1}{2}((\\Delta W_{2,n})^2 - h) \\right)\n$$\nNow we compare this to the component-wise Milstein scheme, $X^{\\mathrm{comp}}$, given in the problem:\n$$\nX^{\\mathrm{comp}}_{1,n+1} = X^{\\mathrm{comp}}_{1,n} + a_{1}(X^{\\mathrm{comp}}_{1,n}) h + b_{1}(X^{\\mathrm{comp}}_{1,n}) \\Delta W_{1,n} + \\tfrac{1}{2} b_{1}(X^{\\mathrm{comp}}_{1,n}) b_{1}'(X^{\\mathrm{comp}}_{1,n})\\left((\\Delta W_{1,n})^{2}-h\\right)\n$$\n$$\nX^{\\mathrm{comp}}_{2,n+1} = X^{\\mathrm{comp}}_{2,n} + a_{2}(X^{\\mathrm{comp}}_{2,n}) h + b_{2}(X^{\\mathrm{comp}}_{2,n}) \\Delta W_{2,n} + \\tfrac{1}{2} b_{2}(X^{\\mathrm{comp}}_{2,n}) b_{2}'(X^{\\mathrm{comp}}_{2,n})\\left((\\Delta W_{2,n})^{2}-h\\right)\n$$\nThe update rules for $X^{\\mathrm{full}}$ and $X^{\\mathrm{comp}}$ are functionally identical for each component. Let us denote the update function for the first component as $F_1(y, \\Delta w, h)$ and for the second as $F_2(z, \\Delta w, h)$. Then we have:\n$$\nX^{\\mathrm{full}}_{1,n+1} = F_1(X^{\\mathrm{full}}_{1,n}, \\Delta W_{1,n}, h) \\quad \\text{and} \\quad X^{\\mathrm{comp}}_{1,n+1} = F_1(X^{\\mathrm{comp}}_{1,n}, \\Delta W_{1,n}, h)\n$$\n$$\nX^{\\mathrm{full}}_{2,n+1} = F_2(X^{\\mathrm{full}}_{2,n}, \\Delta W_{2,n}, h) \\quad \\text{and} \\quad X^{\\mathrm{comp}}_{2,n+1} = F_2(X^{\\mathrm{comp}}_{2,n}, \\Delta W_{2,n}, h)\n$$\nThe problem states that the initial conditions are identical, $X^{\\mathrm{comp}}_{0} = (x_1, x_2)$ and $X^{\\mathrm{full}}_{0} = (x_1, x_2)$, and that the computations use the same Brownian path. We can prove by induction that the two schemes produce identical trajectories.\nBase case ($n=0$): $X^{\\mathrm{comp}}_{i,0} = x_{i} = X^{\\mathrm{full}}_{i,0}$ for $i=1,2$.\nInductive step: Assume $X^{\\mathrm{comp}}_{i,n} = X^{\\mathrm{full}}_{i,n}$ for $i=1,2$.\nThen, for the next step, since the update functions and the inputs (the state at step $n$ and the Brownian increment $\\Delta W_{i,n}$) are identical for both schemes, the outputs must be identical:\n$$\nX^{\\mathrm{comp}}_{1,n+1} = F_1(X^{\\mathrm{comp}}_{1,n}, \\dots) = F_1(X^{\\mathrm{full}}_{1,n}, \\dots) = X^{\\mathrm{full}}_{1,n+1}\n$$\n$$\nX^{\\mathrm{comp}}_{2,n+1} = F_2(X^{\\mathrm{comp}}_{2,n}, \\dots) = F_2(X^{\\mathrm{full}}_{2,n}, \\dots) = X^{\\mathrm{full}}_{2,n+1}\n$$\nThus, $X^{\\mathrm{comp}}_{n} = X^{\\mathrm{full}}_{n}$ for all $n=0, 1, \\dots, N$. This means that at the terminal time $T=t_N$, we have $X^{\\mathrm{comp}}_{T} = X^{\\mathrm{comp}}_{N}$ and $X^{\\mathrm{full}}_{T} = X^{\\mathrm{full}}_{N}$, and so $X^{\\mathrm{comp}}_{T} = X^{\\mathrm{full}}_{T}$.\nThe difference vector is $X^{\\mathrm{comp}}_{T} - X^{\\mathrm{full}}_{T} = 0$.\nThe squared norm is $\\left\\|X^{\\mathrm{comp}}_{T} - X^{\\mathrm{full}}_{T}\\right\\|^{2} = \\|0\\|^{2} = 0$.\nThe mean-square difference is therefore:\n$$\n\\Delta(h) = \\mathbb{E}[0] = 0\n$$\nThe value is exactly zero, independent of the step size $h$.\n\nFinally, the condition under which the simpler component-wise Milstein scheme suffices to achieve the same strong convergence order as the full Milstein scheme (strong order 1) is precisely the condition that makes the two schemes equivalent. This occurs when the cross-term coefficients in the full Milstein scheme are zero. As shown, this is true if the diffusion vector fields commute: $[g_j, g_k]=0$ for all $j \\neq k$. This condition, known as commutative noise, ensures that the component-wise scheme does not omit any terms of order $h$ that would otherwise degrade its strong convergence order to $0.5$. The \"diagonal diffusion\" as specified in this problem is a special case of commutative noise.", "answer": "$$\n\\boxed{0}\n$$", "id": "3349759"}, {"introduction": "The theoretical assumptions underlying numerical methods are not mere formalities; they define the boundary between stability and failure. This practice presents a critical case study where the widely-used Euler-Maruyama method fails to converge in the strong sense due to superlinear coefficients in the SDE. By analyzing the growth of the moments of the numerical solution, you will quantify the mechanism of this divergence and determine the exact time at which the approximation breaks down [@problem_id:3349735].", "problem": "Consider the scalar stochastic differential equation (SDE) with superlinear drift and multiplicative diffusion\n$$\ndX_{t} \\;=\\; X_{t}^{3}\\,dt \\;+\\; X_{t}\\,dW_{t},\\qquad X_{0}=x_{0}0,\n$$\nwhere $W_{t}$ is a standard one-dimensional Brownian motion and $x_{0}$ is deterministic. Let $T0$ be a fixed terminal time and consider the time discretization $t_{n}=n h$ with $h=T/N$, $n=0,1,\\dots,N$. The explicit Euler–Maruyama (EM) method (Euler–Maruyama (EM)) for this SDE is given by the recursion\n$$\nY_{n+1} \\;=\\; Y_{n} \\;+\\; h\\,a(Y_{n}) \\;+\\; b(Y_{n})\\,\\Delta W_{n},\\qquad \\Delta W_{n}=W_{t_{n+1}}-W_{t_{n}},\n$$\nwith $a(x)=x^{3}$ and $b(x)=x$, where the Brownian increments satisfy $\\Delta W_{n}\\sim\\mathcal{N}(0,h)$ and are independent of $\\{Y_{k}\\}_{k\\le n}$.\n\nStarting from the fundamental properties of Brownian motion, the definition of the EM method, and basic inequalities for moments (such as those implied by the Cauchy–Schwarz inequality), perform the following tasks:\n\n1. Derive an exact expression for the conditional second moment $\\mathbb{E}\\!\\left[\\,|Y_{n+1}|^{2}\\,\\middle|\\,Y_{n}\\,\\right]$ in terms of $Y_{n}$ and $h$, and use it to obtain a deterministic recursive lower bound for the unconditional second moments $s_{n}:=\\mathbb{E}\\!\\left[\\,|Y_{n}|^{2}\\,\\right]$ of the form $s_{n+1}\\ge s_{n}+\\text{(terms depending on $h$ and $s_{n}$)}$.\n\n2. Introduce a continuous-time lower-bound model $s:[0,\\infty)\\to(0,\\infty)$ that captures the growth mechanism of the EM second moments through a differential inequality obtained in the limit of small $h$ (viewing $(s_{n+1}-s_{n})/h$ as a time derivative at $t_{n}$). Solve the corresponding autonomous initial-value problem exactly to identify the earliest time $t^{\\ast}\\in(0,\\infty)$ at which the lower bound loses finiteness, starting from $s(0)=x_{0}^{2}$.\n\nYour final answer must be the closed-form analytic expression for the critical blow-up time $t^{\\ast}$ as a function of $x_{0}$. No approximation or rounding is required, and no units should be included in the final expression.", "solution": "The SDE under consideration is\n$$\ndX_{t} \\;=\\; X_{t}^{3}\\,dt \\;+\\; X_{t}\\,dW_{t},\\qquad X_{0}=x_{0}0.\n$$\nThe Euler–Maruyama (EM) numerical scheme for this SDE is given by the recursion\n$$\nY_{n+1} \\;=\\; Y_{n} \\;+\\; h\\,Y_{n}^{3} \\;+\\; Y_{n}\\,\\Delta W_{n},\n$$\nwhere $Y_{0}=x_{0}$, $h$ is the time step, and $\\Delta W_{n}$ are independent and identically distributed random variables following a normal distribution $\\mathcal{N}(0,h)$.\n\n**Part 1: Derivation of a recursive lower bound**\n\nFirst, we derive an exact expression for the conditional second moment, $\\mathbb{E}[|Y_{n+1}|^{2} | Y_{n}]$. We square the EM recursion formula:\n$$\n|Y_{n+1}|^{2} = Y_{n+1}^{2} = \\left( (Y_{n} + hY_{n}^{3}) + Y_{n}\\Delta W_{n} \\right)^{2} = (Y_{n} + hY_{n}^{3})^{2} + 2Y_{n}(Y_{n} + hY_{n}^{3})\\Delta W_{n} + Y_{n}^{2}(\\Delta W_{n})^{2}.\n$$\nWe now take the conditional expectation with respect to $Y_{n}$. In this context, $Y_{n}$ is treated as a known value. The increment $\\Delta W_{n}$ is independent of $Y_{n}$, so we can use its known moments: $\\mathbb{E}[\\Delta W_{n}] = 0$ and $\\mathbb{E}[(\\Delta W_{n})^{2}] = h$.\n\\begin{align*}\n\\mathbb{E}[|Y_{n+1}|^{2} | Y_{n}] = \\mathbb{E}[(Y_{n} + hY_{n}^{3})^{2} | Y_{n}] + \\mathbb{E}[2Y_{n}(Y_{n} + hY_{n}^{3})\\Delta W_{n} | Y_{n}] + \\mathbb{E}[Y_{n}^{2}(\\Delta W_{n})^{2} | Y_{n}] \\\\\n= (Y_{n} + hY_{n}^{3})^{2} + 2Y_{n}(Y_{n} + hY_{n}^{3})\\mathbb{E}[\\Delta W_{n}] + Y_{n}^{2}\\mathbb{E}[(\\Delta W_{n})^{2}] \\\\\n= (Y_{n}^{2} + 2hY_{n}^{4} + h^{2}Y_{n}^{6}) + 2Y_{n}(Y_{n} + hY_{n}^{3})(0) + Y_{n}^{2}(h) \\\\\n= Y_{n}^{2} + hY_{n}^{2} + 2hY_{n}^{4} + h^{2}Y_{n}^{6}.\n\\end{align*}\nThis is the exact expression for the conditional second moment.\n\nNext, we derive a recursive lower bound for the unconditional second moments $s_{n} := \\mathbb{E}[|Y_{n}|^{2}]$. Using the law of total expectation, we have $s_{n+1} = \\mathbb{E}[|Y_{n+1}|^{2}] = \\mathbb{E}[\\mathbb{E}[|Y_{n+1}|^{2} | Y_{n}]]$.\n$$\ns_{n+1} = \\mathbb{E}[Y_{n}^{2} + hY_{n}^{2} + 2hY_{n}^{4} + h^{2}Y_{n}^{6}].\n$$\nBy linearity of expectation,\n$$\ns_{n+1} = \\mathbb{E}[Y_{n}^{2}] + h\\mathbb{E}[Y_{n}^{2}] + 2h\\mathbb{E}[Y_{n}^{4}] + h^{2}\\mathbb{E}[Y_{n}^{6}] = s_{n} + hs_{n} + 2h\\mathbb{E}[Y_{n}^{4}] + h^{2}\\mathbb{E}[Y_{n}^{6}].\n$$\nTo obtain a lower bound, we apply two inequalities. First, as $h0$ and $Y_{n}^{6} \\ge 0$, the term $h^{2}\\mathbb{E}[Y_{n}^{6}]$ is non-negative, so we can drop it:\n$$\ns_{n+1} \\ge s_{n} + hs_{n} + 2h\\mathbb{E}[Y_{n}^{4}].\n$$\nSecond, by Jensen's inequality applied to the convex function $\\phi(x)=x^2$ and the random variable $Y_{n}^{2}$, we have $\\mathbb{E}[(Y_{n}^{2})^{2}] \\ge (\\mathbb{E}[Y_{n}^{2}])^{2}$, which is $\\mathbb{E}[Y_{n}^{4}] \\ge s_{n}^{2}$. Substituting this into the inequality for $s_{n+1}$ gives the required recursive lower bound:\n$$\ns_{n+1} \\ge s_{n} + hs_{n} + 2hs_{n}^{2}.\n$$\n\n**Part 2: Continuous-time model and blow-up time**\n\nWe rearrange the recursive inequality to resemble a finite difference approximation of a derivative:\n$$\n\\frac{s_{n+1} - s_{n}}{h} \\ge s_{n} + 2s_{n}^{2}.\n$$\nWe define a continuous-time model $s(t)$ for $s_{n}$ at times $t_{n}=nh$. In the limit as $h \\to 0$, the left-hand side approximates the derivative $\\frac{ds}{dt}$, leading to the differential inequality:\n$$\n\\frac{ds}{dt} \\ge s(t) + 2s(t)^{2}.\n$$\nThe earliest possible blow-up time for this lower bound is found by solving the corresponding ordinary differential equation (ODE), which describes the fastest growth rate allowed by the inequality:\n$$\n\\frac{ds}{dt} = s + 2s^{2},\n$$\nwith the initial condition $s(0) = s_{0} = \\mathbb{E}[|Y_{0}|^{2}] = \\mathbb{E}[x_{0}^{2}] = x_{0}^{2}$.\n\nThis ODE is separable. Since $s(0) = x_{0}^{2}  0$ and $\\frac{ds}{dt}  0$ for $s0$, $s(t)$ will remain positive. We separate variables:\n$$\n\\frac{ds}{s(1+2s)} = dt.\n$$\nUsing partial fraction decomposition, $\\frac{1}{s(1+2s)} = \\frac{1}{s} - \\frac{2}{1+2s}$. Integrating both sides gives:\n$$\n\\int \\left(\\frac{1}{s} - \\frac{2}{1+2s}\\right) ds = \\int dt \\quad\\implies\\quad \\ln(s) - \\ln(1+2s) = t + C,\n$$\nwhere $C$ is an integration constant. We use the initial condition $s(0)=x_{0}^{2}$ to find $C$:\n$$\n\\ln(x_{0}^{2}) - \\ln(1+2x_{0}^{2}) = 0 + C \\quad\\implies\\quad C = \\ln\\left(\\frac{x_{0}^{2}}{1+2x_{0}^{2}}\\right).\n$$\nThe implicit solution is $\\ln\\left(\\frac{s(t)}{1+2s(t)}\\right) = t + \\ln\\left(\\frac{x_{0}^{2}}{1+2x_{0}^{2}}\\right)$. Exponentiating both sides yields:\n$$\n\\frac{s(t)}{1+2s(t)} = \\frac{x_{0}^{2}}{1+2x_{0}^{2}}\\exp(t).\n$$\nWe solve for $s(t)$:\n$$\ns(t) = (1+2s(t))\\frac{x_{0}^{2}\\exp(t)}{1+2x_{0}^{2}} \\implies s(t)\\left(1 - \\frac{2x_{0}^{2}\\exp(t)}{1+2x_{0}^{2}}\\right) = \\frac{x_{0}^{2}\\exp(t)}{1+2x_{0}^{2}}.\n$$\nThis gives the explicit solution:\n$$\ns(t) = \\frac{x_{0}^{2}\\exp(t)}{1+2x_{0}^{2} - 2x_{0}^{2}\\exp(t)}.\n$$\nThe solution $s(t)$ experiences a finite-time blow-up. The blow-up time, $t^{\\ast}$, is the time at which the denominator becomes zero.\n$$\n1+2x_{0}^{2} - 2x_{0}^{2}\\exp(t^{\\ast}) = 0.\n$$\nSolving for $t^{\\ast}$:\n$$\n2x_{0}^{2}\\exp(t^{\\ast}) = 1+2x_{0}^{2} \\implies \\exp(t^{\\ast}) = \\frac{1+2x_{0}^{2}}{2x_{0}^{2}} = 1 + \\frac{1}{2x_{0}^{2}}.\n$$\nTaking the natural logarithm of both sides gives the critical blow-up time:\n$$\nt^{\\ast} = \\ln\\left(1 + \\frac{1}{2x_{0}^{2}}\\right).\n$$", "answer": "$$\\boxed{\\ln\\left(1 + \\frac{1}{2x_{0}^{2}}\\right)}$$", "id": "3349735"}]}