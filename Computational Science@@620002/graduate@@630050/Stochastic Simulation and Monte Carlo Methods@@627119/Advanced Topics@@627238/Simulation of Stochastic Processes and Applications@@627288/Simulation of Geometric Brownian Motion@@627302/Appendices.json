{"hands_on_practices": [{"introduction": "To simulate a stochastic differential equation, the Euler-Maruyama scheme is often the first method one learns due to its simplicity. This exercise explores the direct application of this method to Geometric Brownian Motion (GBM). By deriving the probability that a simulated path for a strictly positive process becomes negative in a single step, you will uncover a critical flaw in this naive discretization, highlighting the importance of choosing numerical methods that preserve the fundamental properties of the model [@problem_id:3342006]. This analysis provides essential motivation for using more sophisticated, exact simulation schemes in practice.", "problem": "Consider the stochastic differential equation (SDE) for a positive-valued process $S_{t}$ driven by a standard Brownian motion $W_{t}$,\n$$\ndS_{t}=\\mu S_{t}\\,dt+\\sigma S_{t}\\,dW_{t},\n$$\nwhere $\\mu\\in\\mathbb{R}$ and $\\sigma0$ are constants. You aim to simulate this process on a uniform time grid $t_{n}=n\\,\\Delta t$ with fixed step size $\\Delta t0$ using the Euler–Maruyama method. Assume the Brownian increments are approximated as $W_{t_{n+1}}-W_{t_{n}}=\\sqrt{\\Delta t}\\,Z_{n}$ with $Z_{n}$ independent and identically distributed standard normal random variables.\n\nStarting from the SDE and the definition of Brownian motion and its increments, derive the one-step Euler–Maruyama update for $S_{n+1}$ in terms of $S_{n}$, $\\mu$, $\\sigma$, $\\Delta t$, and $Z_{n}$. Then, under the condition $S_{n}0$, derive a closed-form analytical expression, in terms of the standard normal cumulative distribution function, for the probability that the next step produced by this Euler–Maruyama scheme is negative, i.e., the probability that $S_{n+1}0$ given $S_{n}0$.\n\nYour final answer must be a single closed-form analytic expression. Do not provide a numerical approximation, and do not use a percentage sign. If you introduce any named function, clearly define it in your derivation.", "solution": "The supplied problem requires the derivation of the one-step Euler-Maruyama update rule for the Geometric Brownian Motion process and subsequently, the derivation of a closed-form expression for the probability that a single step of this numerical scheme results in a negative value, given a positive starting value.\n\nThe stochastic differential equation (SDE) for the process $S_t$ is given as:\n$$\ndS_{t}=\\mu S_{t}\\,dt+\\sigma S_{t}\\,dW_{t}\n$$\nwhere $S_t0$, $\\mu$ is a real constant representing the drift, $\\sigma0$ is a constant representing the volatility, and $W_t$ is a standard Brownian motion.\n\nFirst, we derive the one-step Euler–Maruyama update rule. This numerical scheme approximates the solution of the SDE over a discrete time grid $t_n = n \\Delta t$. To obtain the update from $S_n = S_{t_n}$ to $S_{n+1} = S_{t_{n+1}}$, we consider the integral form of the SDE over the time interval $[t_n, t_{n+1}]$:\n$$\nS_{t_{n+1}} - S_{t_n} = \\int_{t_n}^{t_{n+1}} \\mu S_t \\, dt + \\int_{t_n}^{t_{n+1}} \\sigma S_t \\, dW_t\n$$\nThe Euler–Maruyama method approximates the integrands by their values at the beginning of the interval, $t_n$. That is, for $t \\in [t_n, t_{n+1}]$, we approximate $S_t \\approx S_{t_n} = S_n$. The equation becomes:\n$$\nS_{n+1} - S_n \\approx \\mu S_n \\int_{t_n}^{t_{n+1}} dt + \\sigma S_n \\int_{t_n}^{t_{n+1}} dW_t\n$$\nThe integrals evaluate to:\n$$\n\\int_{t_n}^{t_{n+1}} dt = t_{n+1} - t_n = \\Delta t\n$$\n$$\n\\int_{t_n}^{t_{n+1}} dW_t = W_{t_{n+1}} - W_{t_n}\n$$\nThe problem states that the Brownian increment $W_{t_{n+1}}-W_{t_{n}}$ is modeled as $\\sqrt{\\Delta t}\\,Z_{n}$, where $Z_n$ are independent and identically distributed standard normal random variables, $Z_n \\sim N(0, 1)$. Substituting these results, the Euler-Maruyama discretization is:\n$$\nS_{n+1} - S_n = \\mu S_n \\Delta t + \\sigma S_n \\sqrt{\\Delta t} Z_n\n$$\nRearranging to solve for $S_{n+1}$ gives the one-step update rule:\n$$\nS_{n+1} = S_n + \\mu S_n \\Delta t + \\sigma S_n \\sqrt{\\Delta t} Z_n = S_n (1 + \\mu \\Delta t + \\sigma \\sqrt{\\Delta t} Z_n)\n$$\nThis completes the first part of the derivation.\n\nNext, we derive the probability that the next step, $S_{n+1}$, is negative, under the condition that the current step, $S_n$, is positive. We want to find the probability $P(S_{n+1}  0 | S_n  0)$.\nUsing the update rule derived above, the condition $S_{n+1}  0$ translates to:\n$$\nS_n (1 + \\mu \\Delta t + \\sigma \\sqrt{\\Delta t} Z_n)  0\n$$\nGiven the condition $S_n  0$, we can divide both sides of the inequality by $S_n$ without changing the direction of the inequality:\n$$\n1 + \\mu \\Delta t + \\sigma \\sqrt{\\Delta t} Z_n  0\n$$\nNow, we must isolate the random variable $Z_n$ to determine the condition on its value.\n$$\n\\sigma \\sqrt{\\Delta t} Z_n  -(1 + \\mu \\Delta t)\n$$\nSince $\\sigma  0$ and $\\Delta t  0$ are given, the coefficient $\\sigma\\sqrt{\\Delta t}$ is positive. We can divide by it without altering the inequality sign:\n$$\nZ_n  -\\frac{1 + \\mu \\Delta t}{\\sigma \\sqrt{\\Delta t}}\n$$\nThe probability we seek is the probability that the standard normal random variable $Z_n$ is less than this specific value. This probability is given by the cumulative distribution function (CDF) of the standard normal distribution, which we will denote by $\\Phi(x)$. The function $\\Phi(x)$ is defined as the probability that a standard normal variable $Z \\sim N(0,1)$ is less than or equal to $x$:\n$$\n\\Phi(x) = P(Z \\le x) = \\int_{-\\infty}^{x} \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{z^2}{2}\\right) dz\n$$\nTherefore, the probability that $S_{n+1}  0$ given $S_n  0$ is:\n$$\nP(S_{n+1}  0 | S_n  0) = P\\left(Z_n  -\\frac{1 + \\mu \\Delta t}{\\sigma \\sqrt{\\Delta t}}\\right) = \\Phi\\left(-\\frac{1 + \\mu \\Delta t}{\\sigma \\sqrt{\\Delta t}}\\right)\n$$\nThis provides the required closed-form analytical expression for the probability of the process becoming negative in one step using the Euler-Maruyama scheme. It is noteworthy that the true solution to the SDE, $S_t = S_0 \\exp\\left((\\mu - \\sigma^2/2)t + \\sigma W_t\\right)$, is always positive for $S_00$. The possibility of $S_{n+1}0$ is an artifact of this particular numerical discretization method.", "answer": "$$\n\\boxed{\\Phi\\left(-\\frac{1 + \\mu \\Delta t}{\\sigma \\sqrt{\\Delta t}}\\right)}\n$$", "id": "3342006"}, {"introduction": "Beyond basic simulation, a crucial skill is the design of efficient Monte Carlo estimators to obtain precise results with minimal computational effort. This practice delves into the powerful variance reduction technique known as Common Random Numbers (CRN), applied to the practical problem of pricing an option spread. You will see how inducing positive correlation between estimators by using the same source of randomness can dramatically reduce the variance of their difference [@problem_id:3341965]. This hands-on coding problem will guide you through implementing an exact GBM simulation and empirically quantifying the effectiveness of CRN.", "problem": "You are asked to design, analyze, and implement a Monte Carlo estimator for the price difference of two European call options under a Geometric Brownian Motion (GBM) model, using Common Random Numbers (CRN) to reduce variance. The underlying asset price process is modeled as a Geometric Brownian Motion (GBM) with stochastic differential equation (SDE) given by\n$$\ndS_t = \\mu S_t \\, dt + \\sigma S_t \\, dB_t,\n$$\nwhere $S_t$ is the asset price at time $t$, $\\mu$ is the drift, $\\sigma$ is the volatility, and $B_t$ is a standard Wiener process. The target quantity is the difference of expected payoffs at a fixed horizon $T$, namely\n$$\n\\Delta P = \\mathbb{E}\\big[(S_T - K_1)^+\\big] - \\mathbb{E}\\big[(S_T - K_2)^+\\big],\n$$\nfor two strikes $K_1$ and $K_2$, where $(x)^+ = \\max\\{x,0\\}$.\n\nStarting from the SDE definition and the properties of the Wiener process, derive a correct exact-time simulation scheme for $S_T$ that does not rely on time discretization. Then, design a Monte Carlo (MC) estimator for $\\Delta P$ that uses Common Random Numbers (CRN), meaning that for each simulated terminal draw you must evaluate both payoffs $(S_T - K_1)^+$ and $(S_T - K_2)^+$ using the same underlying randomness. Derive, from first principles and properties of variance and covariance, the form of the estimator variance under CRN and under a baseline \"independent streams\" approach in which the two option payoffs are simulated independently with the same total number of payoff evaluations. Use this to obtain an empirical estimate of a variance reduction factor.\n\nYour program must:\n- Implement exact simulation of $S_T$ under GBM.\n- For each test case, construct:\n  - A CRN estimator of $\\Delta P$ using $n$ independent draws of the terminal shock and evaluating both payoffs on each draw.\n  - An empirical estimate of the per-path variance under CRN, $\\widehat{\\mathrm{Var}}_{\\mathrm{CRN}} = \\widehat{\\mathrm{Var}}\\big((S_T - K_1)^+ - (S_T - K_2)^+\\big)$.\n  - An empirical estimate of the per-path variance under independent streams, $\\widehat{\\mathrm{Var}}_{\\mathrm{ind}} = \\widehat{\\mathrm{Var}}\\big((S_T - K_1)^+\\big) + \\widehat{\\mathrm{Var}}\\big((S_T - K_2)^+\\big)$, estimated from the same simulated terminal values so that total payoff evaluations are matched.\n  - The empirical variance reduction factor $R = \\widehat{\\mathrm{Var}}_{\\mathrm{ind}} / \\widehat{\\mathrm{Var}}_{\\mathrm{CRN}}$.\n  - The empirical correlation coefficient between the two payoffs, $\\widehat{\\rho} \\in [-1,1]$.\n- Use a fixed seed for the pseudorandom number generator in each test case to ensure reproducibility.\n\nTest suite:\n- Case $1$: $S_0 = 100$, $\\mu = 0.03$, $\\sigma = 0.2$, $T = 1$, $K_1 = 90$, $K_2 = 110$, $n = 200000$, seed $= 1729$.\n- Case $2$: $S_0 = 100$, $\\mu = 0.03$, $\\sigma = 0.2$, $T = 1$, $K_1 = 100$, $K_2 = 100.001$, $n = 300000$, seed $= 2029$.\n- Case $3$: $S_0 = 100$, $\\mu = 0.0$, $\\sigma = 0.8$, $T = 1$, $K_1 = 100$, $K_2 = 120$, $n = 250000$, seed $= 2222$.\n- Case $4$: $S_0 = 100$, $\\mu = 0.03$, $\\sigma = 0.4$, $T = 0.05$, $K_1 = 95$, $K_2 = 105$, $n = 300000$, seed $= 3333$.\n\nAll parameters are dimensionless scalars. There are no physical units and no angles. All reported numerical results must be real numbers. For each test case, your program must output a list of three floats:\n- the CRN estimate of $\\Delta P$,\n- the variance reduction factor $R$,\n- the payoff correlation estimate $\\widehat{\\rho}$,\n\neach rounded to $6$ decimal places.\n\nFinal output format:\n- Your program should produce a single line of output containing a list with one entry per test case; each entry should itself be a three-element list in the order specified above.\n- For example, the overall printed object should look like\n$$\n[\\,[\\delta_1, R_1, \\rho_1], [\\delta_2, R_2, \\rho_2], [\\delta_3, R_3, \\rho_3], [\\delta_4, R_4, \\rho_4]\\,],\n$$\nwith each $\\delta_i$, $R_i$, and $\\rho_i$ printed to $6$ decimal places.", "solution": "The user has provided a well-posed problem in computational finance. I will first validate the problem statement and then provide a complete solution.\n\n### Step 1: Extract Givens\n- **Model**: The underlying asset price $S_t$ follows a Geometric Brownian Motion (GBM) with the stochastic differential equation (SDE):\n$$\ndS_t = \\mu S_t \\, dt + \\sigma S_t \\, dB_t\n$$\nwhere $\\mu$ is the drift, $\\sigma$ is the volatility, and $B_t$ is a standard Wiener process.\n- **Target Quantity**: The price difference of two European call options, $\\Delta P = \\mathbb{E}\\big[(S_T - K_1)^+\\big] - \\mathbb{E}\\big[(S_T - K_2)^+\\big]$, where $(x)^+ = \\max\\{x,0\\}$, $T$ is the expiry time, and $K_1$, $K_2$ are the strike prices.\n- **Methodology**: A Monte Carlo estimator using Common Random Numbers (CRN) for variance reduction.\n- **Required Derivations**:\n    1. An exact-time simulation scheme for $S_T$.\n    2. The form of the estimator variance under CRN and independent streams.\n- **Required Outputs for each test case**:\n    1. The CRN estimate of $\\Delta P$.\n    2. The empirical variance reduction factor $R = \\widehat{\\mathrm{Var}}_{\\mathrm{ind}} / \\widehat{\\mathrm{Var}}_{\\mathrm{CRN}}$.\n    3. The empirical correlation coefficient $\\widehat{\\rho}$ between the payoffs.\n- **Definitions for Empirical Estimates**:\n    - $\\widehat{\\mathrm{Var}}_{\\mathrm{CRN}} = \\widehat{\\mathrm{Var}}\\big((S_T - K_1)^+ - (S_T - K_2)^+\\big)$.\n    - $\\widehat{\\mathrm{Var}}_{\\mathrm{ind}} = \\widehat{\\mathrm{Var}}\\big((S_T - K_1)^+\\big) + \\widehat{\\mathrm{Var}}\\big((S_T - K_2)^+\\big)$.\n- **Test Suite**:\n    - Case $1$: $S_0 = 100$, $\\mu = 0.03$, $\\sigma = 0.2$, $T = 1$, $K_1 = 90$, $K_2 = 110$, $n = 200000$, seed $= 1729$.\n    - Case $2$: $S_0 = 100$, $\\mu = 0.03$, $\\sigma = 0.2$, $T = 1$, $K_1 = 100$, $K_2 = 100.001$, $n = 300000$, seed $= 2029$.\n    - Case $3$: $S_0 = 100$, $\\mu = 0.0$, $\\sigma = 0.8$, $T = 1$, $K_1 = 100$, $K_2 = 120$, $n = 250000$, seed $= 2222$.\n    - Case $4$: $S_0 = 100$, $\\mu = 0.03$, $\\sigma = 0.4$, $T = 0.05$, $K_1 = 95$, $K_2 = 105$, $n = 300000$, seed $= 3333$.\n- **Output Formatting**: A single-line list of lists, `[[delta_1, R_1, rho_1], [delta_2, R_2, rho_2], ...]`, with each number printed to $6$ decimal places.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem is based on the standard GBM model from financial mathematics and employs standard Monte Carlo simulation techniques (CRN). All concepts are firmly rooted in stochastic calculus and statistics.\n- **Well-Posed**: All necessary parameters ($S_0, \\mu, \\sigma, T, K_1, K_2, n$, seed) are provided for each test case. The objective function and the quantities to be calculated are defined precisely. A unique numerical solution can be computed.\n- **Objective**: The problem is stated using precise, objective mathematical language.\n- **Conclusion**: The problem is valid. It is scientifically sound, well-posed, objective, complete, and non-trivial.\n\n### Step 3: Verdict and Action\nThe problem is valid. I will proceed with the solution.\n\n### Principle-Based Design and Derivations\n\n#### 1. Exact Simulation Scheme for Geometric Brownian Motion\nThe price process $S_t$ is governed by the SDE:\n$$\ndS_t = \\mu S_t \\, dt + \\sigma S_t \\, dB_t\n$$\nTo find an exact solution for $S_T$, we consider the process for $\\ln S_t$. Let $f(x) = \\ln x$. The first two derivatives are $f'(x) = 1/x$ and $f''(x) = -1/x^2$. By Itô's lemma, the dynamics of $f(S_t) = \\ln S_t$ are given by:\n$$\nd(\\ln S_t) = f'(S_t) dS_t + \\frac{1}{2} f''(S_t) (dS_t)^2\n$$\nAccording to Itô calculus rules, $(dS_t)^2 = (\\mu S_t dt + \\sigma S_t dB_t)^2 = \\sigma^2 S_t^2 dt$, as terms with $dt^2$ and $dt \\, dB_t$ are zero. Substituting the derivatives and $(dS_t)^2$:\n$$\nd(\\ln S_t) = \\frac{1}{S_t} (\\mu S_t dt + \\sigma S_t dB_t) + \\frac{1}{2} \\left(-\\frac{1}{S_t^2}\\right) (\\sigma^2 S_t^2 dt)\n$$\n$$\nd(\\ln S_t) = (\\mu dt + \\sigma dB_t) - \\frac{1}{2} \\sigma^2 dt = \\left(\\mu - \\frac{1}{2}\\sigma^2\\right) dt + \\sigma dB_t\n$$\nThis is an SDE with constant coefficients. We can integrate it from $t=0$ to $t=T$:\n$$\n\\int_0^T d(\\ln S_t) = \\int_0^T \\left(\\mu - \\frac{1}{2}\\sigma^2\\right) dt + \\int_0^T \\sigma dB_t\n$$\n$$\n\\ln S_T - \\ln S_0 = \\left(\\mu - \\frac{1}{2}\\sigma^2\\right) T + \\sigma (B_T - B_0)\n$$\nGiven that $B_0=0$, the increment $B_T$ is a random variable distributed as a normal distribution with mean $0$ and variance $T$, i.e., $B_T \\sim \\mathcal{N}(0, T)$. We can write $B_T = \\sqrt{T} Z$, where $Z \\sim \\mathcal{N}(0, 1)$ is a standard normal random variable.\nExponentiating both sides gives the exact solution for $S_T$:\n$$\nS_T = S_0 \\exp\\left( \\left(\\mu - \\frac{1}{2}\\sigma^2\\right) T + \\sigma \\sqrt{T} Z \\right)\n$$\nThis formula provides an \"exact-time\" simulation scheme because it allows us to draw samples of the terminal price $S_T$ directly from its known distribution without discretizing the time interval $[0, T]$.\n\n#### 2. Monte Carlo Estimation and Variance Reduction\nLet $Y_1 = (S_T - K_1)^+$ and $Y_2 = (S_T - K_2)^+$ be the payoffs for the two call options. The quantity of interest is $\\Delta P = \\mathbb{E}[Y_1] - \\mathbb{E}[Y_2] = \\mathbb{E}[Y_1 - Y_2]$.\n\n**Common Random Numbers (CRN) Estimator**\nThe CRN technique involves using the same stream of random numbers to simulate both quantities being compared. For each of $n$ Monte Carlo trials, we generate a an independent standard normal random variate $Z_i$ for $i=1, \\dots, n$. We then compute a single terminal price $S_{T,i}$ using the scheme derived above. From this single $S_{T,i}$, we evaluate both payoffs, $Y_{1,i} = (S_{T,i} - K_1)^+$ and $Y_{2,i} = (S_{T,i} - K_2)^+$. The CRN estimator for $\\Delta P$ is the sample mean of the differences:\n$$\n\\widehat{\\Delta P}_{\\mathrm{CRN}} = \\frac{1}{n} \\sum_{i=1}^n (Y_{1,i} - Y_{2,i})\n$$\nThe variance of this estimator is given by:\n$$\n\\mathrm{Var}(\\widehat{\\Delta P}_{\\mathrm{CRN}}) = \\frac{1}{n} \\mathrm{Var}(Y_1 - Y_2)\n$$\nThe term $\\mathrm{Var}_{\\mathrm{CRN}} \\equiv \\mathrm{Var}(Y_1 - Y_2)$ is the per-path variance under CRN.\n\n**Baseline Independent Streams Estimator**\nFor comparison, consider an estimator where $\\mathbb{E}[Y_1]$ and $\\mathbb{E}[Y_2]$ are estimated independently. To match the total number of $2n$ payoff evaluations used in the CRN approach, one would use $n$ independent paths for $Y_1$ and another $n$ independent paths for $Y_2$. The estimator would be $\\widehat{\\Delta P}_{\\mathrm{ind}} = \\frac{1}{n}\\sum_{i=1}^n Y_{1,i} - \\frac{1}{n}\\sum_{j=1}^n Y'_{2,j}$, where $Y'$ is based on an independent simulation. The variance of this estimator is:\n$$\n\\mathrm{Var}(\\widehat{\\Delta P}_{\\mathrm{ind}}) = \\mathrm{Var}\\left(\\frac{1}{n}\\sum_{i=1}^n Y_{1,i}\\right) + \\mathrm{Var}\\left(\\frac{1}{n}\\sum_{j=1}^n Y'_{2,j}\\right) = \\frac{1}{n}\\mathrm{Var}(Y_1) + \\frac{1}{n}\\mathrm{Var}(Y_2)\n$$\nThe corresponding per-path variance is $\\mathrm{Var}_{\\mathrm{ind}} \\equiv \\mathrm{Var}(Y_1) + \\mathrm{Var}(Y_2)$.\n\n**Variance Analysis and Reduction Factor**\nUsing the property $\\mathrm{Var}(X-Y) = \\mathrm{Var}(X) + \\mathrm{Var}(Y) - 2\\mathrm{Cov}(X,Y)$, we can relate the two variances:\n$$\n\\mathrm{Var}_{\\mathrm{CRN}} = \\mathrm{Var}_{\\mathrm{ind}} - 2\\mathrm{Cov}(Y_1, Y_2)\n$$\nThe payoff functions $Y_1 = \\max(S_T - K_1, 0)$ and $Y_2 = \\max(S_T - K_2, 0)$ are both non-decreasing functions of the same underlying random variable $S_T$. Therefore, $Y_1$ and $Y_2$ are positively correlated, meaning $\\mathrm{Cov}(Y_1, Y_2)  0$. This implies that $\\mathrm{Var}_{\\mathrm{CRN}}  \\mathrm{Var}_{\\mathrm{ind}}$, so CRN achieves variance reduction.\n\nThe variance reduction factor is the ratio of the per-path variances:\n$$\nR = \\frac{\\mathrm{Var}_{\\mathrm{ind}}}{\\mathrm{Var}_{\\mathrm{CRN}}} = \\frac{\\mathrm{Var}(Y_1) + \\mathrm{Var}(Y_2)}{\\mathrm{Var}(Y_1 - Y_2)}\n$$\nA value of $R  1$ indicates that CRN is effective.\n\n**Empirical Quantities**\nIn practice, we compute empirical estimates from the simulated samples. Given the $n$ pairs of payoffs $\\{(Y_{1,i}, Y_{2,i})\\}_{i=1}^n$ generated using CRN:\n- The estimate of $\\Delta P$ is $\\widehat{\\Delta P} = \\bar{Y}_1 - \\bar{Y}_2$, where $\\bar{Y}_k = \\frac{1}{n}\\sum_{i=1}^n Y_{k,i}$.\n- The empirical per-path CRN variance is the sample variance of the differences:\n$\\widehat{\\mathrm{Var}}_{\\mathrm{CRN}} = \\widehat{\\mathrm{Var}}(Y_1 - Y_2) = \\frac{1}{n-1}\\sum_{i=1}^n \\left( (Y_{1,i}-Y_{2,i}) - (\\bar{Y}_1 - \\bar{Y}_2) \\right)^2$.\n- The empirical per-path independent variance is the sum of the individual sample variances:\n$\\widehat{\\mathrm{Var}}_{\\mathrm{ind}} = \\widehat{\\mathrm{Var}}(Y_1) + \\widehat{\\mathrm{Var}}(Y_2)$, where $\\widehat{\\mathrm{Var}}(Y_k) = \\frac{1}{n-1}\\sum_{i=1}^n (Y_{k,i} - \\bar{Y}_k)^2$.\n- The empirical variance reduction factor is $R = \\widehat{\\mathrm{Var}}_{\\mathrm{ind}} / \\widehat{\\mathrm{Var}}_{\\mathrm{CRN}}$.\n- The empirical correlation coefficient is calculated as:\n$$\n\\widehat{\\rho} = \\frac{\\widehat{\\mathrm{Cov}}(Y_1, Y_2)}{\\sqrt{\\widehat{\\mathrm{Var}}(Y_1) \\widehat{\\mathrm{Var}}(Y_2)}}\n$$\nwhere $\\widehat{\\mathrm{Cov}}(Y_1, Y_2) = \\frac{1}{n-1}\\sum_{i=1}^n (Y_{1,i} - \\bar{Y}_1)(Y_{2,i} - \\bar{Y}_2)$.\n\n#### 3. Algorithmic Implementation\nFor each test case with parameters $S_0, \\mu, \\sigma, T, K_1, K_2, n, \\text{seed}$:\n1. Initialize a pseudorandom number generator with the specified seed.\n2. Generate a vector of $n$ independent standard normal random variates $Z_i \\sim \\mathcal{N}(0, 1)$.\n3. Compute a vector of $n$ terminal stock prices $S_{T,i}$ using the exact formula: $S_{T,i} = S_0 \\exp\\left( (\\mu - 0.5\\sigma^2)T + \\sigma\\sqrt{T}Z_i \\right)$.\n4. Compute two vectors of payoffs, $Y_{1,i} = \\max(S_{T,i} - K_1, 0)$ and $Y_{2,i} = \\max(S_{T,i} - K_2, 0)$.\n5. Calculate the vector of payoff differences, $\\Delta_i = Y_{1,i} - Y_{2,i}$.\n6. Calculate the CRN estimate of the price difference: $\\widehat{\\Delta P} = \\mathrm{mean}(\\Delta_i)$.\n7. Calculate the empirical variances:\n    - $\\widehat{\\mathrm{Var}}_{\\mathrm{CRN}} = \\mathrm{var}(\\Delta_i, \\text{ddof}=1)$.\n    - $\\widehat{\\mathrm{Var}}(Y_1) = \\mathrm{var}(Y_{1,i}, \\text{ddof}=1)$.\n    - $\\widehat{\\mathrm{Var}}(Y_2) = \\mathrm{var}(Y_{2,i}, \\text{ddof}=1)$.\n    - $\\widehat{\\mathrm{Var}}_{\\mathrm{ind}} = \\widehat{\\mathrm{Var}}(Y_1) + \\widehat{\\mathrm{Var}}(Y_2)$.\n8. Calculate the empirical variance reduction factor $R = \\widehat{\\mathrm{Var}}_{\\mathrm{ind}} / \\widehat{\\mathrm{Var}}_{\\mathrm{CRN}}$.\n9. Calculate the empirical correlation coefficient $\\widehat{\\rho} = \\mathrm{corrcoef}(Y_{1,i}, Y_{2,i})$.\n10. Store the results $[\\widehat{\\Delta P}, R, \\widehat{\\rho}]$ for the current test case.\n11. After processing all cases, format and print the results as specified.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Designs, analyzes, and implements a Monte Carlo estimator for the price\n    difference of two European call options under a Geometric Brownian Motion\n    (GBM) model, using Common Random Numbers (CRN) to reduce variance.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (S0, mu, sigma, T, K1, K2, n, seed)\n        (100, 0.03, 0.2, 1, 90, 110, 200000, 1729),\n        (100, 0.03, 0.2, 1, 100, 100.001, 300000, 2029),\n        (100, 0.0, 0.8, 1, 100, 120, 250000, 2222),\n        (100, 0.03, 0.4, 0.05, 95, 105, 300000, 3333),\n    ]\n\n    all_results = []\n    for case in test_cases:\n        S0, mu, sigma, T, K1, K2, n, seed = case\n\n        # 1. Initialize a pseudorandom number generator with the specified seed.\n        # Use default_rng for modern, best-practice random number generation.\n        rng = np.random.default_rng(seed)\n\n        # 2. Generate a vector of n independent standard normal random variates.\n        Z = rng.standard_normal(n)\n\n        # 3. Compute a vector of n terminal stock prices using the exact formula.\n        drift = (mu - 0.5 * sigma**2) * T\n        diffusion = sigma * np.sqrt(T) * Z\n        S_T = S0 * np.exp(drift + diffusion)\n\n        # 4. Compute two vectors of payoffs.\n        Y1 = np.maximum(S_T - K1, 0)\n        Y2 = np.maximum(S_T - K2, 0)\n\n        # 5. Calculate the vector of payoff differences.\n        delta_payoffs = Y1 - Y2\n\n        # 6. Calculate the CRN estimate of the price difference.\n        delta_p_crn = np.mean(delta_payoffs)\n\n        # 7. Calculate the empirical variances. ddof=1 for sample variance.\n        # Per-path variance under CRN\n        var_crn = np.var(delta_payoffs, ddof=1)\n        \n        # Individual payoff variances\n        var_y1 = np.var(Y1, ddof=1)\n        var_y2 = np.var(Y2, ddof=1)\n        \n        # Per-path variance under hypothetical independent streams\n        var_ind = var_y1 + var_y2\n\n        # 8. Calculate the empirical variance reduction factor R.\n        # Check for division by zero, though highly unlikely here.\n        if var_crn > 0:\n            R = var_ind / var_crn\n        else:\n            # This case happens if all payoff differences are identical,\n            # indicating perfect correlation and infinite reduction.\n            # R can be set to infinity or a large number.\n            # For this problem, it's safe to assume var_crn will be positive.\n            R = np.inf\n\n\n        # 9. Calculate the empirical correlation coefficient.\n        # np.corrcoef returns a 2x2 matrix. The off-diagonal element is the correlation.\n        # A check is needed for cases where one variance is zero.\n        if var_y1 > 0 and var_y2 > 0:\n            rho = np.corrcoef(Y1, Y2)[0, 1]\n        else:\n            # If one variance is zero, the payoff is constant (likely 0),\n            # and correlation is undefined. Set to 1 as it's the limit.\n            rho = 1.0\n\n        # 10. Store the results for the current test case.\n        all_results.append([delta_p_crn, R, rho])\n\n    # 11. After processing all cases, format and print the results as specified.\n    # The output format requires printing each number to 6 decimal places.\n    # This must be done via string formatting.\n    output_parts = []\n    for result_group in all_results:\n        # result_group is like [delta, R, rho]\n        formatted_group = [f'{val:.6f}' for val in result_group]\n        output_parts.append(f\"[{', '.join(formatted_group)}]\")\n\n    final_output = f\"[{', '.join(output_parts)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "3341965"}, {"introduction": "In real-world applications, the parameters of a stochastic model, such as the drift $\\mu$ and volatility $\\sigma$ of a GBM, are unknown and must be estimated from data. This exercise bridges the gap between simulation and statistical inference by tasking you with deriving the Maximum Likelihood Estimator (MLE) for the volatility parameter. You will then implement a Monte Carlo study to investigate the estimator's finite-sample properties, such as its bias and variance, and compare these empirical results to the asymptotic predictions derived from Fisher information theory [@problem_id:3341959]. This practice provides a complete workflow from theoretical modeling to practical parameter estimation and validation.", "problem": "You are asked to design and implement a complete, runnable program that uses Monte Carlo simulation to study Maximum Likelihood Estimation (MLE) of the volatility parameter under the geometric Brownian motion (GBM) model from discrete-time observations, and to compare the finite-sample variance of the estimator to the inverse of the Fisher information computed from the corresponding log-price process. The problem must be solved from first principles of stochastic calculus and statistical inference, without using closed-form estimators as a starting point. The fundamental base you may assume includes the stochastic differential equation definition of GBM, properties of Brownian motion, Itô’s lemma, the distribution of normally distributed increments, and the general form of the log-likelihood for a multivariate normal sample.\n\nStart from the following base. A geometric Brownian motion for a strictly positive process $S_t$ satisfies the stochastic differential equation\n$$\n\\mathrm{d}S_t = \\mu S_t \\,\\mathrm{d}t + \\sigma S_t \\,\\mathrm{d}W_t,\n$$\nwhere $W_t$ is a standard Brownian motion, $\\mu$ is the drift, and $\\sigma$ is the volatility. Define the log-price $X_t = \\log S_t$. Based on Itô’s lemma and standard properties of Brownian motion, the discrete increments $R_i = X_{t_i} - X_{t_{i-1}}$ over equally spaced times $t_i = i \\Delta$ for $i \\in \\{1,\\dots,n\\}$ are independent and normally distributed:\n$$\nR_i \\sim \\mathcal{N}\\!\\big(\\Delta(\\mu - \\tfrac{1}{2}\\sigma^2),\\, \\sigma^2 \\Delta\\big).\n$$\nUse only this base to derive the MLE for $\\sigma$ when the drift parameter is unknown. Then implement a Monte Carlo (MC) study to assess the finite-sample bias and variance of the resulting estimator, and compare the empirical variance to the asymptotic variance predicted by the Fisher information computed for the log-price model.\n\nYour program must do the following for each test case below:\n1. Simulate $M$ independent samples of length $n$ of the log-returns $R_1,\\dots,R_n$ from the exact normal transition law described above, under the given parameters $(\\mu,\\sigma,\\Delta)$.\n2. For each sample, compute the MLE of $\\sigma$ from the discrete observations of $S_{t_i}$, which are equivalently summarized by $R_i = \\log S_{t_i} - \\log S_{t_{i-1}}$.\n3. Compute the Monte Carlo estimate of the finite-sample bias of the estimator as the sample average of $\\widehat{\\sigma}$ minus the true $\\sigma$.\n4. Compute the Monte Carlo estimate of the finite-sample variance of the estimator as the empirical variance of $\\widehat{\\sigma}$ across the $M$ replications.\n5. Compute the asymptotic variance predicted by the Fisher information for $\\sigma$ based on the log-price model.\n6. Aggregate the results for all test cases into a single line of output.\n\nAll computations must be expressed in purely mathematical terms without physical units. Angles do not appear in this problem. Percentages, if any, must be expressed as decimals.\n\nTest suite. Use the following three test cases, each specified by a tuple $(n,\\Delta,\\mu,\\sigma,M,\\text{seed})$:\n- Case A (general, one trading year at daily spacing): $(n,\\Delta,\\mu,\\sigma,M,\\text{seed}) = (\\,252,\\, 1/252,\\, 0.08,\\, 0.25,\\, 20000,\\, 12345\\,)$.\n- Case B (small sample, coarse spacing): $(n,\\Delta,\\mu,\\sigma,M,\\text{seed}) = (\\,5,\\, 1,\\, 0.00,\\, 0.50,\\, 200000,\\, 67890\\,)$.\n- Case C (longer sample, fine spacing): $(n,\\Delta,\\mu,\\sigma,M,\\text{seed}) = (\\,1000,\\, 1/252,\\, 0.02,\\, 0.20,\\, 5000,\\, 54321\\,)$.\n\nFor each case, your program must output a triple $[\\text{bias}, \\text{variance}, \\text{asymptotic\\_variance}]$ in this order, where:\n- $\\text{bias}$ is the Monte Carlo estimate of $E[\\widehat{\\sigma}] - \\sigma$,\n- $\\text{variance}$ is the Monte Carlo estimate of $\\mathrm{Var}(\\widehat{\\sigma})$ using the empirical variance with divisor $M$,\n- $\\text{asymptotic\\_variance}$ is the inverse of the Fisher information for $\\sigma$ derived from the log-price model for a single sample of size $n$, expressed as a function of $\\sigma$ and $n$.\n\nFinal output format. Your program should produce a single line of output containing a list of the three triples for Cases A, B, and C in this order, as a comma-separated list enclosed in square brackets of lists, for example:\n\"[ [bias_A,variance_A,asymptotic_variance_A] , [bias_B,variance_B,asymptotic_variance_B] , [bias_C,variance_C,asymptotic_variance_C] ]\"\nYour program must be self-contained and must not require any input. It must use the exact parameter values above and the random seeds provided for reproducibility. The final output must be printed exactly once and must contain only the described list, with no additional text.", "solution": "The problem requires the derivation of the Maximum Likelihood Estimator (MLE) for the volatility parameter $\\sigma$ of a geometric Brownian motion (GBM) process, the derivation of its asymptotic variance via the Fisher information, and the implementation of a Monte Carlo simulation to assess the estimator's finite-sample properties.\n\nLet the stock price process $S_t$ follow a GBM:\n$$\n\\mathrm{d}S_t = \\mu S_t \\,\\mathrm{d}t + \\sigma S_t \\,\\mathrm{d}W_t\n$$\nwhere $\\mu$ is the drift, $\\sigma  0$ is the volatility, and $W_t$ is a standard Brownian motion. The log-price process is $X_t = \\log S_t$. By applying Itô's lemma to $f(S_t) = \\log S_t$, we find that $X_t$ follows an arithmetic Brownian motion:\n$$\n\\mathrm{d}X_t = (\\mu - \\tfrac{1}{2}\\sigma^2) \\,\\mathrm{d}t + \\sigma \\,\\mathrm{d}W_t\n$$\nFor discrete, equally spaced time intervals of length $\\Delta$, the log-returns $R_i = X_{t_i} - X_{t_{i-1}}$ for $t_i = i\\Delta$ are independent and identically distributed (i.i.d.) normal random variables:\n$$\nR_i \\sim \\mathcal{N}\\!\\left(\\nu, \\tau^2\\right)\n$$\nwhere the mean is $\\nu = \\Delta(\\mu - \\tfrac{1}{2}\\sigma^2)$ and the variance is $\\tau^2 = \\sigma^2 \\Delta$.\n\n### 1. Derivation of the Maximum Likelihood Estimator (MLE)\n\nWe aim to find the MLE for $\\sigma$ when $\\mu$ is unknown, based on a sample of $n$ log-returns $R_1, \\dots, R_n$. The parameters of the model are $(\\mu, \\sigma)$. The log-likelihood function for the sample is:\n$$\n\\ell(\\mu, \\sigma; R_1, \\dots, R_n) = \\sum_{i=1}^n \\log f(R_i; \\mu, \\sigma)\n$$\nwhere $f(R_i; \\mu, \\sigma)$ is the probability density function of the normal distribution with mean $\\nu = \\Delta(\\mu - \\frac{1}{2}\\sigma^2)$ and variance $\\sigma^2\\Delta$. The log-likelihood can be written as:\n$$\n\\ell(\\mu, \\sigma) = \\sum_{i=1}^n \\left( -\\frac{1}{2}\\log(2\\pi) - \\frac{1}{2}\\log(\\sigma^2 \\Delta) - \\frac{(R_i - \\Delta(\\mu - \\frac{1}{2}\\sigma^2))^2}{2\\sigma^2 \\Delta} \\right)\n$$\n$$\n\\ell(\\mu, \\sigma) = -\\frac{n}{2}\\log(2\\pi\\Delta) - n\\log(\\sigma) - \\frac{1}{2\\sigma^2 \\Delta} \\sum_{i=1}^n \\left(R_i - \\Delta\\mu + \\frac{1}{2}\\Delta\\sigma^2\\right)^2\n$$\nTo find the MLEs $(\\widehat{\\mu}, \\widehat{\\sigma})$, we take the partial derivatives of $\\ell$ with respect to $\\mu$ and $\\sigma$ and set them to zero.\n\nThe partial derivative with respect to $\\mu$ is:\n$$\n\\frac{\\partial\\ell}{\\partial\\mu} = -\\frac{1}{2\\sigma^2 \\Delta} \\sum_{i=1}^n 2\\left(R_i - \\Delta\\mu + \\frac{1}{2}\\Delta\\sigma^2\\right)(-\\Delta) = \\frac{1}{\\sigma^2} \\sum_{i=1}^n \\left(R_i - \\Delta\\mu + \\frac{1}{2}\\Delta\\sigma^2\\right)\n$$\nSetting $\\frac{\\partial\\ell}{\\partial\\mu} = 0$ implies $\\sum (R_i - \\widehat{\\nu}) = 0$, where $\\widehat{\\nu}$ is the MLE of the mean of the returns. This yields $\\widehat{\\nu} = \\frac{1}{n} \\sum R_i = \\bar{R}$. So, at the maximum, the parameters must satisfy:\n$$\n\\Delta(\\widehat{\\mu} - \\tfrac{1}{2}\\widehat{\\sigma}^2) = \\bar{R}\n$$\nThis establishes a relationship between $\\widehat{\\mu}$ and $\\widehat{\\sigma}$. We can now profile the likelihood by substituting $\\bar{R}$ for the mean parameter $\\nu$. It's more direct to work with the parameter $\\sigma^2$. The partial derivative with respect to $\\sigma^2$ is:\n$$\n\\frac{\\partial\\ell}{\\partial(\\sigma^2)} = -\\frac{n}{2\\sigma^2} + \\frac{1}{2(\\sigma^2)^2 \\Delta} \\sum_{i=1}^n \\left(R_i - \\nu\\right)^2\n$$\nSetting this to zero and substituting $\\widehat{\\nu} = \\bar{R}$, we get:\n$$\n\\frac{n}{2\\widehat{\\sigma}^2} = \\frac{1}{2(\\widehat{\\sigma}^2)^2 \\Delta} \\sum_{i=1}^n (R_i - \\bar{R})^2 \\implies \\widehat{\\sigma}^2 = \\frac{1}{n\\Delta} \\sum_{i=1}^n (R_i - \\bar{R})^2\n$$\nBy the invariance property of MLEs, the MLE for $\\sigma$ is the square root of the MLE for $\\sigma^2$:\n$$\n\\widehat{\\sigma} = \\sqrt{\\frac{1}{n\\Delta} \\sum_{i=1}^n (R_i - \\bar{R})^2}\n$$\nThis estimator will be used in the simulation. Note that $E[\\widehat{\\sigma}^2] = \\frac{n-1}{n}\\sigma^2$, so $\\widehat{\\sigma}^2$ is a biased estimator for $\\sigma^2$. Consequently, $\\widehat{\\sigma}$ is also a biased estimator for $\\sigma$.\n\n### 2. Derivation of the Asymptotic Variance\nThe asymptotic variance of $\\widehat{\\sigma}$ is given by the corresponding diagonal element of the inverse of the total Fisher information matrix. The total Fisher information for $n$ i.i.d. observations is $I_n(\\mu, \\sigma) = n I(\\mu, \\sigma)$, where $I(\\mu, \\sigma)$ is the Fisher information for a single observation.\n\nTo simplify the calculation, we use a reparameterization. Let $\\theta_1 = \\nu = \\Delta(\\mu - \\frac{1}{2}\\sigma^2)$ and $\\theta_2 = \\tau^2 = \\sigma^2 \\Delta$. The log-likelihood for one observation $R_i$ is $\\ell_i(\\theta_1, \\theta_2) = C - \\frac{1}{2}\\log(\\theta_2) - \\frac{(R_i - \\theta_1)^2}{2\\theta_2}$. The Fisher information matrix for $(\\theta_1, \\theta_2)$ is diagonal:\n$$\nI(\\theta_1, \\theta_2) = \\begin{pmatrix} -E[\\frac{\\partial^2 \\ell_i}{\\partial \\theta_1^2}]  0 \\\\ 0  -E[\\frac{\\partial^2 \\ell_i}{\\partial \\theta_2^2}] \\end{pmatrix} = \\begin{pmatrix} 1/\\theta_2  0 \\\\ 0  1/(2\\theta_2^2) \\end{pmatrix} = \\begin{pmatrix} 1/(\\sigma^2\\Delta)  0 \\\\ 0  1/(2\\sigma^4\\Delta^2) \\end{pmatrix}\n$$\nThe transformation from $(\\mu, \\sigma)$ to $(\\theta_1, \\sigma^2 \\Delta)$ has the Jacobian matrix $J$:\n$$\nJ = \\frac{\\partial(\\theta_1, \\theta_2)}{\\partial(\\mu, \\sigma)} = \\begin{pmatrix} \\partial\\theta_1/\\partial\\mu  \\partial\\theta_1/\\partial\\sigma \\\\ \\partial\\theta_2/\\partial\\mu  \\partial\\theta_2/\\partial\\sigma \\end{pmatrix} = \\begin{pmatrix} \\Delta  -\\Delta\\sigma \\\\ 0  2\\sigma\\Delta \\end{pmatrix}\n$$\nThe Fisher information matrix for $(\\mu, \\sigma)$ is $I(\\mu, \\sigma) = J^T I(\\theta_1, \\theta_2) J$:\n$$\nI(\\mu, \\sigma) = \\begin{pmatrix} \\Delta  0 \\\\ -\\Delta\\sigma  2\\sigma\\Delta \\end{pmatrix} \\begin{pmatrix} 1/(\\sigma^2\\Delta)  0 \\\\ 0  1/(2\\sigma^4\\Delta^2) \\end{pmatrix} \\begin{pmatrix} \\Delta  -\\Delta\\sigma \\\\ 0  2\\sigma\\Delta \\end{pmatrix} = \\begin{pmatrix} \\Delta/\\sigma^2  -\\Delta/\\sigma \\\\ -\\Delta/\\sigma  \\Delta + 2/\\sigma^2 \\end{pmatrix}\n$$\nThe determinant of this matrix is $\\det(I) = (\\Delta/\\sigma^2)(\\Delta + 2/\\sigma^2) - (-\\Delta/\\sigma)^2 = 2\\Delta/\\sigma^4$. The inverse is:\n$$\n[I(\\mu, \\sigma)]^{-1} = \\frac{\\sigma^4}{2\\Delta} \\begin{pmatrix} \\Delta + 2/\\sigma^2  \\Delta/\\sigma \\\\ \\Delta/\\sigma  \\Delta/\\sigma^2 \\end{pmatrix} = \\begin{pmatrix} \\sigma^4/2 + \\sigma^2/\\Delta  \\sigma^3/2 \\\\ \\sigma^3/2  \\sigma^2/2 \\end{pmatrix}\n$$\nThe asymptotic variance-covariance matrix for $(\\widehat{\\mu}, \\widehat{\\sigma})$ is $[n I(\\mu, \\sigma)]^{-1}$. The asymptotic variance of $\\widehat{\\sigma}$ is the $(2,2)$ element of this matrix:\n$$\n\\text{Asymptotic Var}(\\widehat{\\sigma}) = \\frac{\\sigma^2}{2n}\n$$\nThis result is independent of $\\mu$ and $\\Delta$, and depends only on the true volatility $\\sigma$ and the sample size $n$.\n\n### 3. Monte Carlo Simulation Design\nFor each test case $(n, \\Delta, \\mu, \\sigma, M, \\text{seed})$, the simulation proceeds as follows:\n1.  Initialize a random number generator with the given `seed`.\n2.  Repeat $M$ times:\n    a. Generate a sample of $n$ log-returns $\\{R_i\\}_{i=1}^n$ from the distribution $\\mathcal{N}(\\Delta(\\mu - \\frac{1}{2}\\sigma^2), \\sigma^2 \\Delta)$.\n    b. Calculate the MLE $\\widehat{\\sigma}$ using the formula $\\widehat{\\sigma} = \\sqrt{\\frac{1}{n\\Delta} \\sum_{i=1}^n (R_i - \\bar{R})^2}$.\n    c. Store the computed value of $\\widehat{\\sigma}$.\n3.  After $M$ replications, we have a sample of estimates $\\{\\widehat{\\sigma}_j\\}_{j=1}^M$.\n4.  Estimate the bias as the sample mean of the estimation error: $\\text{bias} = \\left(\\frac{1}{M} \\sum_{j=1}^M \\widehat{\\sigma}_j\\right) - \\sigma$.\n5.  Estimate the variance as the empirical variance of the estimates, using a divisor of $M$ as specified: $\\text{variance} = \\frac{1}{M} \\sum_{j=1}^M (\\widehat{\\sigma}_j - \\overline{\\widehat{\\sigma}})^2$, where $\\overline{\\widehat{\\sigma}} = \\frac{1}{M}\\sum_j \\widehat{\\sigma}_j$.\n6.  Calculate the theoretical asymptotic variance as $\\text{asymptotic\\_variance} = \\sigma^2 / (2n)$.\n7.  The final output for each case is the triplet $[\\text{bias}, \\text{variance}, \\text{asymptotic\\_variance}]$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_simulation(n, delta, mu, sigma, M, seed):\n    \"\"\"\n    Runs the Monte Carlo simulation for a single test case to assess the\n    properties of the MLE for volatility in a GBM model.\n\n    Args:\n        n (int): Number of observations in each sample path.\n        delta (float): Time step between observations.\n        mu (float): Drift parameter of the GBM.\n        sigma (float): Volatility parameter of the GBM.\n        M (int): Number of Monte Carlo replications.\n        seed (int): Seed for the random number generator.\n\n    Returns:\n        list: A list containing [bias, variance, asymptotic_variance].\n    \"\"\"\n    # Use the modern recommended way to manage random state\n    rng = np.random.default_rng(seed)\n\n    # Pre-calculate true parameters of the normal distribution for log-returns\n    # R_i ~ N(nu, sd_R^2)\n    nu = delta * (mu - 0.5 * sigma**2)\n    sd_R = sigma * np.sqrt(delta)\n\n    # Array to store the M estimates of sigma\n    sigma_hats = np.zeros(M)\n    \n    # Generate all random numbers at once for efficiency\n    all_R = rng.normal(loc=nu, scale=sd_R, size=(M, n))\n\n    # Perform calculations for each of the M samples\n    for i in range(M):\n        R = all_R[i, :]\n        \n        # The MLE for the variance of returns, var(R) = (1/n) * sum((R_i - R_bar)^2)\n        # using ddof=0 for the MLE (divisor n)\n        var_R_mle = np.var(R, ddof=0)\n        \n        # The MLE for sigma^2 is derived from var_R_mle = sigma_hat^2 * delta\n        sigma_sq_hat = var_R_mle / delta\n        \n        # By invariance property of MLEs, hat(sigma) = sqrt(hat(sigma^2))\n        sigma_hats[i] = np.sqrt(sigma_sq_hat)\n\n    # 3. Compute the Monte Carlo estimate of the bias\n    bias = np.mean(sigma_hats) - sigma\n\n    # 4. Compute the Monte Carlo estimate of the variance\n    # The problem specifies using divisor M, which corresponds to ddof=0\n    variance = np.var(sigma_hats, ddof=0)\n\n    # 5. Compute the theoretical asymptotic variance from Fisher Information\n    # Asymptotic Var(hat(sigma)) = sigma^2 / (2*n)\n    asymptotic_variance = sigma**2 / (2 * n)\n    \n    return [bias, variance, asymptotic_variance]\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (n, Delta, mu, sigma, M, seed)\n        (252, 1/252.0, 0.08, 0.25, 20000, 12345),  # Case A\n        (5, 1.0, 0.00, 0.50, 200000, 67890),    # Case B\n        (1000, 1/252.0, 0.02, 0.20, 5000, 54321), # Case C\n    ]\n\n    results = []\n    for case in test_cases:\n        n, delta, mu, sigma, M, seed = case\n        result = run_simulation(n, delta, mu, sigma, M, seed)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    # The str() representation of a list is '[item1, item2, ...]', which matches the required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3341959"}]}