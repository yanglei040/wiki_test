{"hands_on_practices": [{"introduction": "Understanding a stochastic process begins with its fundamental properties. This first practice takes you back to the very definition of the Brownian bridge as a standard Brownian motion conditioned to return to zero. By deriving the distribution of the bridge at a single point in time, you will translate the abstract concept of conditioning a Gaussian process into a concrete mean and variance, providing the essential building block for simulation [@problem_id:3350886].", "problem": "Consider a standard Brownian motion $W_t$ for $t \\in [0,1]$ with $W_0 = 0$, zero mean, and covariance function $\\mathbb{E}[W_s W_t] = \\min(s,t)$. Define the Brownian bridge $B_t$ on the interval $[0,1]$ as the process obtained from $W_t$ by conditioning on the event $W_1 = 0$, so that $B_0 = 0$ and $B_1 = 0$. Starting from the foundational properties of Gaussian processes and the joint Gaussian law of $(W_t, W_1)$, derive the distribution of $B_t$ for a fixed $t \\in (0,1)$, including its mean and variance, using the conditional Gaussian law. After deriving the distribution, design an exact sampling method for $B_t$ at a fixed $t$ that relies solely on the conditional Gaussian representation, without invoking any asymptotic or approximate arguments.\n\nYour program must implement the exact sampler you derive and then, for each test case specified below, perform the following steps:\n1. Use the derived conditional Gaussian distribution to generate $n$ independent samples of $B_t$.\n2. Compute the sample mean $\\hat{m}(t)$ and the sample variance $\\hat{v}(t)$ (use the population variance with divisor $n$).\n3. Compute the theoretical mean $m(t)$ and variance $v(t)$ of $B_t$ from your derivation.\n4. For each test case, output two values in the following order:\n   - The sample mean $\\hat{m}(t)$.\n   - The difference $\\hat{v}(t) - v(t)$.\n\nThe test suite consists of four parameter sets $(t, n, \\text{seed})$ that must be used exactly as given:\n- Case $1$: $t = 0.5$, $n = 20000$, $\\text{seed} = 1234567$.\n- Case $2$: $t = 0.01$, $n = 50000$, $\\text{seed} = 202311$.\n- Case $3$: $t = 0.99$, $n = 50000$, $\\text{seed} = 314159$.\n- Case $4$: $t = 0.2$, $n = 30000$, $\\text{seed} = 271828$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test cases and values specified above. For example, the output format must be exactly of the form\n$[\\hat{m}(t_1),\\hat{v}(t_1)-v(t_1),\\hat{m}(t_2),\\hat{v}(t_2)-v(t_2),\\hat{m}(t_3),\\hat{v}(t_3)-v(t_3),\\hat{m}(t_4),\\hat{v}(t_4)-v(t_4)]$.", "solution": "The problem requires the derivation of the distribution of a Brownian bridge $B_t$ at a fixed time $t \\in (0,1)$ and the design of an exact sampling algorithm based on this derivation.\n\n### Part 1: Derivation of the Distribution of the Brownian Bridge\n\nA standard Brownian motion $W_t$ for $t \\in [0,1]$ is a Gaussian process with $W_0=0$, mean function $\\mathbb{E}[W_t] = 0$, and covariance function $\\mathbb{E}[W_s W_t] = \\min(s,t)$. The Brownian bridge $B_t$ is defined as the process $W_t$ conditioned on the event $W_1=0$. To find the distribution of $B_t$ for a fixed $t \\in (0,1)$, we use the properties of conditional Gaussian distributions.\n\nConsider the joint distribution of the random variables $W_t$ and $W_1$. Since $W_t$ is a Gaussian process, the random vector $\\mathbf{X} = \\begin{pmatrix} W_t \\\\ W_1 \\end{pmatrix}$ follows a multivariate normal distribution.\n\nThe mean vector $\\boldsymbol{\\mu}$ is given by:\n$$\n\\boldsymbol{\\mu} = \\mathbb{E}[\\mathbf{X}] = \\begin{pmatrix} \\mathbb{E}[W_t] \\\\ \\mathbb{E}[W_1] \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n$$\n\nThe covariance matrix $\\boldsymbol{\\Sigma}$ is given by:\n$$\n\\boldsymbol{\\Sigma} = \\begin{pmatrix} \\mathbb{E}[W_t^2]  \\mathbb{E}[W_t W_1] \\\\ \\mathbb{E}[W_1 W_t]  \\mathbb{E}[W_1^2] \\end{pmatrix}\n$$\nUsing the covariance function $\\mathbb{E}[W_s W_u] = \\min(s,u)$, we compute the elements of $\\boldsymbol{\\Sigma}$:\n- $\\mathbb{E}[W_t^2] = \\min(t,t) = t$\n- $\\mathbb{E}[W_1^2] = \\min(1,1) = 1$\n- $\\mathbb{E}[W_t W_1] = \\min(t,1) = t$ (since $t \\in [0,1]$)\n\nThus, the covariance matrix is:\n$$\n\\boldsymbol{\\Sigma} = \\begin{pmatrix} t  t \\\\ t  1 \\end{pmatrix}\n$$\nSo, $\\begin{pmatrix} W_t \\\\ W_1 \\end{pmatrix} \\sim \\mathcal{N}\\left(\\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} t  t \\\\ t  1 \\end{pmatrix}\\right)$.\n\nWe want to find the distribution of $B_t$, which is the distribution of $W_t$ given $W_1=0$. For a general partitioned Gaussian vector $\\begin{pmatrix} \\mathbf{X}_1 \\\\ \\mathbf{X}_2 \\end{pmatrix} \\sim \\mathcal{N}\\left(\\begin{pmatrix} \\boldsymbol{\\mu}_1 \\\\ \\boldsymbol{\\mu}_2 \\end{pmatrix}, \\begin{pmatrix} \\boldsymbol{\\Sigma}_{11}  \\boldsymbol{\\Sigma}_{12} \\\\ \\boldsymbol{\\Sigma}_{21}  \\boldsymbol{\\Sigma}_{22} \\end{pmatrix}\\right)$, the conditional distribution of $\\mathbf{X}_1$ given $\\mathbf{X}_2 = \\mathbf{x}_2$ is also Gaussian, with mean and covariance:\n$$\n\\mathbb{E}[\\mathbf{X}_1 | \\mathbf{X}_2 = \\mathbf{x}_2] = \\boldsymbol{\\mu}_1 + \\boldsymbol{\\Sigma}_{12} \\boldsymbol{\\Sigma}_{22}^{-1} (\\mathbf{x}_2 - \\boldsymbol{\\mu}_2)\n$$\n$$\n\\text{Cov}(\\mathbf{X}_1 | \\mathbf{X}_2 = \\mathbf{x}_2) = \\boldsymbol{\\Sigma}_{11} - \\boldsymbol{\\Sigma}_{12} \\boldsymbol{\\Sigma}_{22}^{-1} \\boldsymbol{\\Sigma}_{21}\n$$\n\nIn our specific case, $\\mathbf{X}_1 = W_t$, $\\mathbf{X}_2 = W_1$, and $\\mathbf{x}_2=0$. The scalar components are $\\boldsymbol{\\mu}_1=0$, $\\boldsymbol{\\mu}_2=0$, $\\boldsymbol{\\Sigma}_{11}=t$, $\\boldsymbol{\\Sigma}_{12}=t$, $\\boldsymbol{\\Sigma}_{21}=t$, and $\\boldsymbol{\\Sigma}_{22}=1$. The inverse $\\boldsymbol{\\Sigma}_{22}^{-1}$ is simply $1^{-1}=1$.\n\nThe theoretical mean of $B_t$, denoted $m(t)$, is:\n$$\nm(t) = \\mathbb{E}[W_t | W_1 = 0] = 0 + t \\cdot 1^{-1} \\cdot (0 - 0) = 0\n$$\n\nThe theoretical variance of $B_t$, denoted $v(t)$, is:\n$$\nv(t) = \\text{Var}(W_t | W_1 = 0) = t - t \\cdot 1^{-1} \\cdot t = t - t^2 = t(1-t)\n$$\nTherefore, the distribution of the Brownian bridge $B_t$ at a fixed time $t$ is a normal distribution with mean $0$ and variance $t(1-t)$:\n$$\nB_t \\sim \\mathcal{N}(0, t(1-t))\n$$\n\n### Part 2: Exact Sampling Method Design\n\nAn exact sampler for a Gaussian random variable $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$ can be constructed from a standard normal random variable $Z \\sim \\mathcal{N}(0,1)$. The transformation is $X = \\mu + \\sigma Z$.\n\nBased on the derived distribution $B_t \\sim \\mathcal{N}(0, t(1-t))$, we have $\\mu=0$ and $\\sigma^2 = t(1-t)$. The standard deviation is $\\sigma = \\sqrt{t(1-t)}$. Thus, an exact sample of $B_t$ can be generated using the formula:\n$$\nB_t = \\sqrt{t(1-t)} \\cdot Z, \\quad \\text{where } Z \\sim \\mathcal{N}(0,1)\n$$\nThis method is exact as it relies on a direct transformation and does not involve any approximations or asymptotic assumptions, contingent on the availability of a sampler for the standard normal distribution.\n\n### Part 3: Algorithmic Implementation\n\nThe program will implement the following algorithm for each test case $(t, n, \\text{seed})$:\n1.  Set the random number generator's seed to the given `seed` for reproducibility.\n2.  Calculate the theoretical mean $m(t) = 0$ and theoretical variance $v(t) = t(1-t)$.\n3.  Generate $n$ independent samples $Z_1, Z_2, \\dots, Z_n$ from the standard normal distribution $\\mathcal{N}(0,1)$.\n4.  Transform these samples to obtain $n$ samples of the Brownian bridge $b_i$ using the formula $b_i = \\sqrt{t(1-t)} \\cdot Z_i$.\n5.  Compute the sample mean $\\hat{m}(t) = \\frac{1}{n} \\sum_{i=1}^n b_i$.\n6.  Compute the sample variance $\\hat{v}(t) = \\frac{1}{n} \\sum_{i=1}^n (b_i - \\hat{m}(t))^2$, using the divisor $n$ as specified.\n7.  Calculate the two required output values: the sample mean $\\hat{m}(t)$ and the difference between the sample and theoretical variances, $\\hat{v}(t) - v(t)$.\n8.  These values are collected from all test cases and formatted into a single output line.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Derives and implements an exact sampler for a Brownian Bridge B_t\n    at a fixed time t, based on its conditional Gaussian distribution.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (t, n, seed)\n        (0.5, 20000, 1234567),\n        (0.01, 50000, 202311),\n        (0.99, 50000, 314159),\n        (0.2, 30000, 271828),\n    ]\n\n    results = []\n    for t, n, seed in test_cases:\n        # Set seed for reproducibility.\n        rng = np.random.default_rng(seed)\n\n        # Theoretical properties of B_t from derivation B_t ~ N(0, t(1-t)).\n        # Theoretical mean m(t) = 0.\n        # Theoretical variance v(t) = t * (1 - t).\n        theoretical_variance = t * (1.0 - t)\n\n        # Generate n samples from a standard normal distribution Z ~ N(0,1).\n        z_samples = rng.normal(size=n)\n        \n        # Transform standard normal samples to samples of B_t.\n        # B_t = sqrt(t*(1-t)) * Z\n        std_dev = np.sqrt(theoretical_variance)\n        b_t_samples = std_dev * z_samples\n\n        # Compute the sample mean, m_hat(t).\n        sample_mean = np.mean(b_t_samples)\n\n        # Compute the sample variance, v_hat(t).\n        # The problem specifies using the population variance with divisor n.\n        # numpy.var() uses ddof=0 by default, which corresponds to a divisor of n.\n        sample_variance = np.var(b_t_samples)\n\n        # Calculate the difference between sample and theoretical variance.\n        variance_diff = sample_variance - theoretical_variance\n\n        # Append results for this test case.\n        results.append(sample_mean)\n        results.append(variance_diff)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3350886"}, {"introduction": "While simulating a process at discrete points is useful, we are often interested in properties of the entire continuous path, such as its maximum value. This exercise contrasts a common but biased discretization-based approach with a more elegant and powerful exact sampling method. By leveraging the known analytical distribution for the maximum of a Brownian bridge, you will develop an appreciation for exact simulation techniques and the importance of recognizing and avoiding discretization bias in your models [@problem_id:3350866].", "problem": "Let $\\{W_t\\}_{t \\in [0,1]}$ be a standard Brownian motion (BM) and define the standard Brownian bridge $\\{B^{\\mathrm{br}}_t\\}_{t \\in [0,1]}$ by $B^{\\mathrm{br}}_t = W_t - t W_1$. Let $M = \\max_{t \\in [0,1]} B^{\\mathrm{br}}_t$. You will construct an exact sampler for $M$ by exploiting the law of $M$, then compare Monte Carlo (MC) estimators of $E[g(M)]$ based on this exact sampler against naive discretization-based estimators that approximate $M$ by the maximum of the bridge observed on a uniform grid of times.\n\nTasks:\n\n1) Starting only from the fundamental definitions of Brownian motion and the Brownian bridge, together with the reflection principle for Brownian motion and standard properties of Gaussian processes, derive the cumulative distribution function of $M$. In particular, express $F_M(x) = \\mathbb{P}(M \\le x)$ for $x \\ge 0$ in closed form by first principles. Then derive the inverse transform required to construct an exact sampler for $M$.\n\n2) Design an exact sampling algorithm for $M$ using the inverse transform sampling method applied to the law derived in Task $1$. The algorithm must produce independent and identically distributed samples from the exact distribution of $M$ without any discretization error.\n\n3) Design a naive discretization-based estimator as follows. For a given integer grid size $n_{\\text{grid}} \\in \\mathbb{N}$, simulate the Brownian bridge $\\{B^{\\mathrm{br}}_t\\}$ on the uniform grid $t_i = i/n_{\\text{grid}}$ for $i \\in \\{0,1,\\dots,n_{\\text{grid}}\\}$ using the relation $B^{\\mathrm{br}}_{t_i} = W_{t_i} - t_i W_1$ with Brownian motion increments that are independent mean-zero Gaussians with variances equal to the time steps. Approximate the maximum $M$ by $\\max_{0 \\le i \\le n_{\\text{grid}}} B^{\\mathrm{br}}_{t_i}$, and use this approximation to estimate $E[g(M)]$ by Monte Carlo. Explain why this estimator has a discretization bias and what its direction is when $g$ is nondecreasing.\n\n4) Implement a program that:\n- Uses your exact sampler to compute MC estimates of $E[g(M)]$ with associated Monte Carlo standard errors.\n- Uses the naive discretization to compute MC estimates of $E[g(M)]$ with associated Monte Carlo standard errors.\n- Reports, for each test case, the exact-sampler estimate, the discretization-based estimate, the estimated bias (discretization minus exact), the standard error of the exact-sampler estimate, the standard error of the discretization-based estimate, and a two-sided confidence interval half-width for the bias computed as $z_{0.975} \\sqrt{\\mathrm{SE}_{\\text{exact}}^2 + \\mathrm{SE}_{\\text{disc}}^2}$ with $z_{0.975} = 1.96$.\n\nNumerical specifications and test suite:\n\n- Use independent pseudorandom number generator (PRNG) seeds for the exact and discretization paths to ensure independence of the two MC estimators. Use seed values $\\mathrm{seed}_{\\mathrm{exact}} = 12345$ and $\\mathrm{seed}_{\\mathrm{disc}} = 67890$.\n- For each test case, use the following function $g$, grid size $n_{\\text{grid}}$, and sample sizes $N_{\\text{exact}}$ and $N_{\\text{disc}}$:\n  - Test case $1$: $g(x) = x$, $n_{\\text{grid}} = 16$, $N_{\\text{exact}} = 200000$, $N_{\\text{disc}} = 100000$.\n  - Test case $2$: $g(x) = \\exp(0.75 x)$, $n_{\\text{grid}} = 64$, $N_{\\text{exact}} = 200000$, $N_{\\text{disc}} = 50000$.\n  - Test case $3$: $g(x) = x^2$, $n_{\\text{grid}} = 256$, $N_{\\text{exact}} = 200000$, $N_{\\text{disc}} = 20000$.\n  - Test case $4$: $g(x) = \\mathbf{1}\\{x \\le 0.5\\}$, $n_{\\text{grid}} = 64$, $N_{\\text{exact}} = 200000$, $N_{\\text{disc}} = 50000$.\n- All expectations and standard errors must be reported as dimensionless real numbers.\n\nOutput format:\n\nYour program should produce a single line of output containing the results as a JSON-like nested list with one entry per test case. Each entry must be a list of six real numbers in the following order:\n$[\\mathrm{exact\\_mean}, \\mathrm{disc\\_mean}, \\mathrm{bias}, \\mathrm{se\\_exact}, \\mathrm{se\\_disc}, \\mathrm{ci95\\_bias}]$,\nwhere $\\mathrm{bias} = \\mathrm{disc\\_mean} - \\mathrm{exact\\_mean}$ and $\\mathrm{ci95\\_bias} = 1.96 \\sqrt{\\mathrm{se\\_exact}^2 + \\mathrm{se\\_disc}^2}$. For example, the output should have the form\n$[[a_1,b_1,c_1,d_1,e_1,f_1],[a_2,b_2,c_2,d_2,e_2,f_2],[a_3,b_3,c_3,d_3,e_3,f_3],[a_4,b_4,c_4,d_4,e_4,f_4]]$,\nwith all entries being real numbers. There must be no other printed text.", "solution": "We begin by recalling the fundamental definitions. A standard Brownian motion (BM) $\\{W_t\\}_{t \\in [0,1]}$ satisfies: $W_0 = 0$, has independent increments, and for $0 \\le s  t \\le 1$, the increment $W_t - W_s$ is Gaussian with mean $0$ and variance $t-s$. The standard Brownian bridge (BB) on $[0,1]$ is the Gaussian process $\\{B^{\\mathrm{br}}_t\\}$ defined by $B^{\\mathrm{br}}_t = W_t - t W_1$, which pins the process at both endpoints since $B^{\\mathrm{br}}_0 = 0$ and $B^{\\mathrm{br}}_1 = W_1 - W_1 = 0$. The process is centered, $\\mathbb{E}[B^{\\mathrm{br}}_t] = 0$, with covariance $\\mathrm{Cov}(B^{\\mathrm{br}}_s,B^{\\mathrm{br}}_t) = \\min(s,t) - s t$.\n\nDefine the maximum $M = \\max_{t \\in [0,1]} B^{\\mathrm{br}}_t$. We will derive the distribution of $M$ via elementary properties of BM and the reflection principle.\n\nStep $1$: Law of the maximum. Consider the event $\\{ M \\le x \\}$ for $x \\ge 0$. By definition,\n$$\nM \\le x \\quad \\Longleftrightarrow \\quad B^{\\mathrm{br}}_t \\le x \\text{ for all } t \\in [0,1].\n$$\nUsing the representation $B^{\\mathrm{br}}_t = W_t - t W_1$, we can analyze $\\{ \\max_{t \\in [0,1]} (W_t - t W_1) \\le x \\}$ by conditioning on $W_1 = y$. Conditional on $W_1 = y$, the process $t \\mapsto W_t - t y$ equals a Brownian motion with deterministic linear drift $-y$ starting from $0$, i.e., $\\widetilde{W}_t = W_t - t y$. Thus\n$$\n\\mathbb{P}(M \\le x) = \\int_{-\\infty}^{\\infty} \\mathbb{P}\\Big( \\sup_{t \\in [0,1]} (W_t - t y) \\le x \\,\\Big|\\, W_1 = y \\Big) \\, \\varphi(y) \\, dy,\n$$\nwhere $\\varphi(y)$ is the standard normal density.\n\nFor a Brownian motion with drift $-y$, the probability that it crosses the level $x$ by time $1$ admits a classical expression derived from the reflection principle and Cameronâ€“Martin change of measure. Specifically, for $x \\ge 0$,\n$$\n\\mathbb{P}\\Big( \\sup_{t \\in [0,1]} (W_t - t y) \\ge x \\,\\Big|\\, W_1 = y \\Big) = \\exp\\big(-2 x (x + y) \\big)\\, \\mathbf{1}\\{x + y \\ge 0\\}.\n$$\nHence\n$$\n\\mathbb{P}(M \\le x \\mid W_1 = y) = 1 - \\exp\\big(-2 x (x + y) \\big)\\, \\mathbf{1}\\{x + y \\ge 0\\}.\n$$\nIntegrating with respect to $y \\sim \\mathcal{N}(0,1)$, the term involving the indicator and exponential simplifies by Gaussian integration (a standard computation in the theory of bridges) to yield\n$$\nF_M(x) = \\mathbb{P}(M \\le x) = 1 - \\exp(-2 x^2), \\quad x \\ge 0.\n$$\nThis formula is a well-established law of the maximum of the standard Brownian bridge on $[0,1]$ and follows from the reflection principle adapted to the pinned process.\n\nStep $2$: Inverse transform sampler. The cumulative distribution function $F_M$ is continuous and strictly increasing on $[0,\\infty)$ with $F_M(0) = 0$ and $\\lim_{x \\to \\infty} F_M(x) = 1$. For $U \\sim \\mathrm{Uniform}(0,1)$, the inverse transform $M = F_M^{-1}(U)$ gives an exact sample from the law of $M$. Since $F_M(x) = 1 - \\exp(-2 x^2)$ for $x \\ge 0$, we invert:\n$$\nU = 1 - \\exp(-2 x^2) \\;\\Longleftrightarrow\\; 1-U = \\exp(-2 x^2) \\;\\Longleftrightarrow\\; -\\log(1-U) = 2 x^2\n$$\nand thus\n$$\nx = \\sqrt{\\frac{-\\log(1-U)}{2}}.\n$$\nBecause $1-U$ is also $\\mathrm{Uniform}(0,1)$, we can equivalently set $M = \\sqrt{-\\tfrac{1}{2}\\log U}$. Either form produces exact independent and identically distributed samples of $M$.\n\nStep $3$: Naive discretization-based estimator. For a given integer grid size $n_{\\text{grid}} \\ge 1$, define $t_i = i/n_{\\text{grid}}$ for $i \\in \\{0,1,\\dots,n_{\\text{grid}}\\}$. Simulate Brownian motion at these times by $W_{t_0} = 0$ and increments $W_{t_i} - W_{t_{i-1}} \\sim \\mathcal{N}(0, t_i - t_{i-1}) = \\mathcal{N}(0, 1/n_{\\text{grid}})$ independently. Then construct the bridge values $B^{\\mathrm{br}}_{t_i} = W_{t_i} - t_i W_1$ and approximate the maximum by\n$$\nM^{(n_{\\text{grid}})} = \\max_{0 \\le i \\le n_{\\text{grid}}} B^{\\mathrm{br}}_{t_i}.\n$$\nGiven a measurable function $g:\\mathbb{R}_+ \\to \\mathbb{R}$, the naive discretization-based MC estimator of $E[g(M)]$ uses the sample mean of $g(M^{(n_{\\text{grid}})})$. Because $M^{(n_{\\text{grid}})} \\le M$ almost surely (the grid overlooks potential local maxima between grid points), when $g$ is nondecreasing the estimator has a nonpositive bias:\n$$\n\\mathbb{E}[g(M^{(n_{\\text{grid}})})] \\le \\mathbb{E}[g(M)].\n$$\nThe bias magnitude tends to $0$ as $n_{\\text{grid}} \\to \\infty$ but can be non-negligible for moderate $n_{\\text{grid}}$.\n\nStep $4$: Monte Carlo estimators and precision. For a sample $\\{X_j\\}_{j=1}^N$ of independent and identically distributed realizations of a random variable $X$, the Monte Carlo estimator of $\\mathbb{E}[X]$ is the sample mean $\\overline{X}_N = \\frac{1}{N} \\sum_{j=1}^N X_j$, with Monte Carlo standard error estimated by $\\widehat{\\mathrm{SE}}(\\overline{X}_N) = \\widehat{\\sigma}_X/\\sqrt{N}$ where $\\widehat{\\sigma}_X^2$ is the sample variance with Bessel's correction. Applying this to $X = g(M)$ using the exact sampler yields an unbiased estimator with finite variance for bounded or suitably integrable $g$. Applying it to $X = g(M^{(n_{\\text{grid}})})$ yields a biased estimator whose bias is negative when $g$ is nondecreasing. Using independent PRNG streams for the exact and discretization-based runs ensures that the difference of the two Monte Carlo estimates has variance equal to the sum of their variances; therefore, a two-sided confidence interval half-width for the bias can be computed as $1.96 \\sqrt{\\mathrm{SE}_{\\text{exact}}^2 + \\mathrm{SE}_{\\text{disc}}^2}$.\n\nAlgorithmic design:\n\n- Exact sampler:\n  - Generate $U \\sim \\mathrm{Uniform}(0,1)$.\n  - Set $M = \\sqrt{-\\tfrac{1}{2} \\log U}$.\n  - Repeat independently to obtain the desired sample size.\n\n- Naive discretization sampler:\n  - For each replication:\n    - Simulate Brownian motion on the grid by cumulative sums of independent $\\mathcal{N}(0, 1/n_{\\text{grid}})$ increments.\n    - Compute $B^{\\mathrm{br}}_{t_i} = W_{t_i} - t_i W_1$.\n    - Take the maximum over $i$ to obtain $M^{(n_{\\text{grid}})}$.\n  - Repeat to obtain the desired sample size. For computational efficiency and memory control, process replications in batches and vectorize the computations over the grid dimension.\n\nImplementation notes:\n\n- Use separate seeds $\\mathrm{seed}_{\\mathrm{exact}} = 12345$ and $\\mathrm{seed}_{\\mathrm{disc}} = 67890$ to ensure independence of the two MC estimators.\n- For each test case specified in the problem statement, compute and report:\n  - exact\\_mean $= \\frac{1}{N_{\\text{exact}}} \\sum_{j=1}^{N_{\\text{exact}}} g(M_j)$,\n  - disc\\_mean $= \\frac{1}{N_{\\text{disc}}} \\sum_{j=1}^{N_{\\text{disc}}} g(M^{(n_{\\text{grid}})}_j)$,\n  - bias $= \\text{disc\\_mean} - \\text{exact\\_mean}$,\n  - Monte Carlo standard errors $\\text{se\\_exact}$ and $\\text{se\\_disc}$ computed from sample variances,\n  - $\\text{ci95\\_bias} = 1.96 \\sqrt{\\text{se\\_exact}^2 + \\text{se\\_disc}^2}$.\n\nThe final program aggregates the results for all test cases into a single JSON-like nested list on one line, as required.", "answer": "```python\nimport numpy as np\n\ndef sample_max_bridge_exact(rng: np.random.Generator, n: int) - np.ndarray:\n    # Exact sampler for M = max_{t in [0,1]} B^{br}_t; CDF F(x) = 1 - exp(-2 x^2), x=0\n    # Inverse transform: M = sqrt(-0.5 * log(U)), U ~ Uniform(0,1)\n    U = rng.random(n)\n    # Guard against U == 0 (extremely unlikely)\n    U = np.where(U == 0.0, np.nextafter(0.0, 1.0), U)\n    M = np.sqrt(-0.5 * np.log(U))\n    return M\n\ndef sample_max_bridge_discretized(rng: np.random.Generator, n_samples: int, n_grid: int, batch_cap: int = None) - np.ndarray:\n    # Simulate maxima of Brownian bridge on a uniform grid of n_grid intervals (n_grid+1 points)\n    # t_i = i / n_grid, increments dW ~ N(0, 1/n_grid)\n    # Process in batches for memory efficiency\n    if batch_cap is None:\n        # target about 5e6 doubles per batch (approx 40 MB)\n        target_elems = 5_000_000\n        batch_cap = max(1, int(target_elems // max(1, n_grid)))\n    t = np.linspace(0.0, 1.0, n_grid + 1, dtype=np.float64)  # shape (n_grid+1,)\n    maxima = np.empty(n_samples, dtype=np.float64)\n    produced = 0\n    var_inc = 1.0 / n_grid\n    while produced  n_samples:\n        bsz = min(batch_cap, n_samples - produced)\n        # Generate Brownian increments and cumulative sum to get W at grid points\n        dW = rng.normal(loc=0.0, scale=np.sqrt(var_inc), size=(bsz, n_grid))\n        W = np.concatenate([np.zeros((bsz, 1), dtype=np.float64), np.cumsum(dW, axis=1)], axis=1)  # shape (bsz, n_grid+1)\n        W1 = W[:, [-1]]  # shape (bsz, 1)\n        # Bridge values: B = W - t * W1\n        B = W - t * W1  # broadcasting over t\n        max_batch = np.max(B, axis=1)\n        maxima[produced:produced + bsz] = max_batch\n        produced += bsz\n    return maxima\n\ndef mc_stats(values: np.ndarray):\n    mean = float(np.mean(values))\n    # Sample standard deviation with Bessel's correction\n    sd = float(np.std(values, ddof=1)) if values.size  1 else 0.0\n    se = sd / np.sqrt(values.size) if values.size  0 else 0.0\n    return mean, sd, se\n\ndef solve():\n    # Define test cases: (g_id, g_func, n_grid, N_exact, N_disc)\n    def g1(x): return x  # g(x) = x\n    def g2(x): return np.exp(0.75 * x)  # g(x) = exp(0.75 x)\n    def g3(x): return x**2  # g(x) = x^2\n    def g4(x): return (x = 0.5).astype(np.float64)  # g(x) = 1{x = 0.5}\n\n    test_cases = [\n        (\"case1\", g1, 16, 200_000, 100_000),\n        (\"case2\", g2, 64, 200_000, 50_000),\n        (\"case3\", g3, 256, 200_000, 20_000),\n        (\"case4\", g4, 64, 200_000, 50_000),\n    ]\n\n    # Independent RNGs for exact and discretized estimators\n    rng_exact = np.random.default_rng(12345)\n    rng_disc = np.random.default_rng(67890)\n\n    results = []\n    for _, g, n_grid, N_exact, N_disc in test_cases:\n        # Exact sampling\n        M_exact = sample_max_bridge_exact(rng_exact, N_exact)\n        g_exact = g(M_exact)\n        exact_mean, exact_sd, se_exact = mc_stats(g_exact)\n\n        # Discretization-based sampling\n        M_disc = sample_max_bridge_discretized(rng_disc, N_disc, n_grid)\n        g_disc = g(M_disc)\n        disc_mean, disc_sd, se_disc = mc_stats(g_disc)\n\n        # Bias and CI half-width for bias (independent estimates)\n        bias = disc_mean - exact_mean\n        ci95_bias = 1.96 * np.sqrt(se_exact**2 + se_disc**2)\n\n        results.append([\n            exact_mean,\n            disc_mean,\n            bias,\n            se_exact,\n            se_disc,\n            ci95_bias\n        ])\n\n    # Print as a single JSON-like nested list\n    # Ensure deterministic formatting with default str conversion\n    def fmt_list(lst):\n        return \"[\" + \",\".join(str(x) if not isinstance(x, list) else fmt_list(x) for x in lst) + \"]\"\n\n    print(fmt_list(results))\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3350866"}, {"introduction": "The Brownian bridge is not just a process to be studied in isolation; it is a fundamental tool in the broader toolkit of stochastic simulation. This advanced practice demonstrates how the bridge can be instrumental in constructing efficient Monte Carlo estimators through variance reduction. You will use the bridge to generate conditional paths within a stratified sampling scheme, applying the law of total variance to optimize the simulation of a related process and minimize computational cost for a given level of precision [@problem_id:3350869].", "problem": "Let $\\{W_{t}: 0 \\leq t \\leq T\\}$ be a standard Brownian motion with $W_{0} = 0$ on a filtered probability space satisfying the usual conditions, and let the terminal time $T  0$ be fixed. Consider the path functional\n$$\nI \\;=\\; \\int_{0}^{T} W_{t} \\, dt.\n$$\nYou wish to estimate $\\theta = \\mathbb{E}[I]$ by Monte Carlo with stratification on the terminal value $W_{T}$. A stratified design is specified by:\n- a number of strata $m \\in \\mathbb{N}$, and\n- a partition of $\\mathbb{R}$ into $m$ Borel sets that are intervals determined by cut points on $\\mathbb{R}$ applied to $W_{T}$.\n\nWithin each stratum, you generate samples by first drawing $W_{T}$ from its normal law truncated to the stratum, and then simulating a Brownian bridge conditional on $W_{T}$ to evaluate a realization of $I$.\n\nYou use optimal Neyman allocation of samples across strata for any fixed partition, and assume a computational budget model with a fixed overhead cost $c_{f}  0$ per stratum and a per-path cost $c_{p}  0$. The total budget is $B = 4\\,c_{f}$, so any feasible design must satisfy $m\\,c_{f} + c_{p}\\sum_{k=1}^{m} n_{k} \\leq B$, where $n_{k}$ is the number of paths sampled in stratum $k$.\n\nStarting only from:\n- the definition of Brownian motion and the Brownian bridge (the conditional process of $W_{t}$ given $W_{T}$),\n- the law of total variance,\n- and the standard normal distribution and its truncations,\n\ndo the following tasks:\n1. Derive the conditional mean and variance of $I$ given $W_{T} = x$, and show that they are affine in $x$ and constant (respectively).\n2. For a given two-stratum partition defined by a single cut $c \\in \\mathbb{R}$, namely $A_{1} = (-\\infty, c]$ and $A_{2} = (c,\\infty)$, express the stratum probabilities $p_{k} = \\mathbb{P}(W_{T} \\in A_{k})$ and the stratum variances $S_{k}^{2} = \\operatorname{Var}(I \\mid W_{T} \\in A_{k})$ in terms of $T$ and the truncated normal moments of $W_{T}$.\n3. Using optimal Neyman allocation for fixed strata and the budget model above, express the minimal achievable variance in terms of $m$, the cut points, $T$, $c_{f}$, $c_{p}$, and $B$.\n4. Under the given budget $B = 4\\,c_{f}$, determine the optimal number of strata $m^{\\star} \\in \\{1,2\\}$ and, if $m^{\\star} = 2$, the optimal cut $c^{\\star} \\in \\mathbb{R}$ that minimizes the minimal achievable variance. Express your final answer as a single row matrix $\\begin{pmatrix} m^{\\star}  c^{\\star} \\end{pmatrix}$.\n\nYour final answer must be a single matrix as specified. No rounding is required, and no physical units are involved. All mathematical expressions must be written in LaTeX. Do not use any special formatting beyond standard mathematical notation. Provide complete reasoning from first principles as specified above, and do not assume or cite any stratification formula that is not derived from the stated bases.", "solution": "The user wants me to solve a problem regarding a stratified Monte Carlo simulation.\n\n### Step 1: Extract Givens\n-   Stochastic process: $\\{W_{t}: 0 \\leq t \\leq T\\}$ is a standard Brownian motion with $W_{0} = 0$.\n-   Path functional: $I = \\int_{0}^{T} W_{t} \\, dt$.\n-   Estimand: $\\theta = \\mathbb{E}[I]$.\n-   Method: Monte Carlo with stratification on the terminal value $W_{T}$.\n-   Strata: A partition of $\\mathbb{R}$ into $m$ intervals, defining events $A_k = \\{W_T \\in \\text{interval}_k\\}$.\n-   Sampling: Generate $W_T$ from its law truncated to a stratum, then simulate a Brownian bridge conditional on $W_T$.\n-   Allocation: Optimal Neyman allocation.\n-   Budget: $m\\,c_{f} + c_{p}\\sum_{k=1}^{m} n_{k} \\leq B$, with $n_k$ being the number of samples in stratum $k$.\n-   Costs: $c_{f}  0$ (fixed per stratum), $c_{p}  0$ (per path).\n-   Specific Budget: $B = 4\\,c_{f}$.\n-   Allowed bases for derivation: Definition of Brownian motion and Brownian bridge, law of total variance, standard normal distribution and its truncations.\n-   Tasks:\n    1.  Derive the conditional mean $\\mathbb{E}[I | W_{T} = x]$ and variance $\\operatorname{Var}(I | W_{T} = x)$, and analyze their functional form.\n    2.  For a two-stratum partition $A_{1} = (-\\infty, c]$, $A_{2} = (c,\\infty)$, find the probabilities $p_{k} = \\mathbb{P}(W_T \\in A_k)$ and variances $S_{k}^{2} = \\operatorname{Var}(I \\mid W_{T} in A_{k})$.\n    3.  Derive the formula for the minimal achievable variance under Neyman allocation and the given budget model.\n    4.  Find the optimal number of strata $m^{\\star} \\in \\{1,2\\}$ and the optimal cut $c^{\\star}$ (if $m^{\\star}=2$) for the budget $B=4c_f$.\n\n### Step 2: Validate Using Extracted Givens\n-   **Scientifically Grounded**: The problem is a standard exercise in stochastic simulation and quantitative finance, based on the well-established theories of Brownian motion and Monte Carlo methods.\n-   **Well-Posed**: The problem is stated with sufficient detail and clear objectives, leading to a definite answer.\n-   **Objective**: The language is formal and mathematical, with no subjective or ambiguous components.\n-   **Flaw Checklist**: The problem does not violate any of the specified invalidity criteria. It is scientifically sound, formalizable, complete, and solvable. The premises are consistent and the question is a non-trivial but standard application of the theory.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. I will proceed with the full solution.\n\nHere is the detailed solution.\n\nThe problem asks for a four-part derivation and optimization concerning the stratified estimation of $\\theta = \\mathbb{E}[\\int_{0}^{T} W_t dt]$.\n\n**Task 1: Conditional Mean and Variance of $I$**\n\nLet $I = \\int_{0}^{T} W_{t} \\, dt$. We need to find the conditional mean and variance of $I$ given $W_T = x$.\n\nThe conditional process $\\{W_t | W_T=x\\}$ for $t \\in [0,T]$ is a Brownian bridge. The conditional expectation of $W_t$ given $W_T=x$ is a standard result from the properties of multivariate normal distributions, as $(W_t, W_T)$ is a bivariate normal vector. The conditional expectation is linear:\n$$\n\\mathbb{E}[W_t | W_T = x] = \\mathbb{E}[W_t] + \\frac{\\operatorname{Cov}(W_t, W_T)}{\\operatorname{Var}(W_T)}(x - \\mathbb{E}[W_T])\n$$\nFor a standard Brownian motion, $\\mathbb{E}[W_t]=0$, $\\operatorname{Var}(W_T)=T$, and for $t \\leq T$, $\\operatorname{Cov}(W_t, W_T) = \\mathbb{E}[W_t W_T] = t$. Thus,\n$$\n\\mathbb{E}[W_t | W_T = x] = \\frac{t}{T}x\n$$\nUsing the linearity of expectation and Fubini's theorem to interchange expectation and integration, the conditional mean of $I$ is:\n$$\n\\mathbb{E}[I | W_T = x] = \\mathbb{E}\\left[\\int_0^T W_t dt \\Bigg| W_T=x\\right] = \\int_0^T \\mathbb{E}[W_t | W_T=x] dt = \\int_0^T \\frac{t}{T}x \\, dt = \\frac{x}{T} \\left[\\frac{t^2}{2}\\right]_0^T = \\frac{xT^2}{2T} = \\frac{Tx}{2}\n$$\nThis expression is affine in $x$ (specifically, linear with zero intercept).\n\nFor the conditional variance, we use the representation of the Brownian bridge process $W_t^x$ (denoting $W_t$ conditioned on $W_T=x$) as $W_t^x = \\frac{t}{T}x + B_t$, where $B_t = W_t - \\frac{t}{T}W_T$ is a standard Brownian bridge (from $0$ to $0$) which is independent of the value $x$.\n$$\n\\operatorname{Var}(I | W_T = x) = \\operatorname{Var}\\left(\\int_0^T W_t^x dt\\right) = \\operatorname{Var}\\left(\\int_0^T \\left(\\frac{t}{T}x + B_t\\right) dt\\right)\n$$\nSince $\\int_0^T \\frac{t}{T}x dt$ is a deterministic quantity given $x$, it does not contribute to the variance.\n$$\n\\operatorname{Var}(I | W_T = x) = \\operatorname{Var}\\left(\\int_0^T B_t dt\\right)\n$$\nThis expression is independent of $x$, so the conditional variance is constant. Let's compute this constant, which we denote $S^2_c$.\nSince $\\mathbb{E}[B_t] = \\mathbb{E}[W_t] - \\frac{t}{T}\\mathbb{E}[W_T] = 0$, we have $\\mathbb{E}[\\int_0^T B_t dt] = 0$.\nThe variance is then the second moment:\n$$\nS^2_c = \\mathbb{E}\\left[\\left(\\int_0^T B_t dt\\right)^2\\right] = \\mathbb{E}\\left[\\int_0^T \\int_0^T B_s B_t ds dt\\right] = \\int_0^T \\int_0^T \\mathbb{E}[B_s B_t] ds dt\n$$\nThe covariance function of a standard Brownian bridge is $\\mathbb{E}[B_s B_t] = \\min(s,t) - \\frac{st}{T}$. The integral is:\n$$\nS^2_c = \\int_0^T \\int_0^T \\left(\\min(s,t) - \\frac{st}{T}\\right) ds dt = \\int_0^T \\int_0^T \\min(s,t) ds dt - \\frac{1}{T}\\int_0^T \\int_0^T st ds dt\n$$\nThe first integral is $2\\int_0^T \\int_0^s t dt ds = 2\\int_0^T \\frac{s^2}{2} ds = \\int_0^T s^2 ds = \\frac{T^3}{3}$.\nThe second integral is $\\frac{1}{T}(\\int_0^T s ds)(\\int_0^T t dt) = \\frac{1}{T}(\\frac{T^2}{2})(\\frac{T^2}{2}) = \\frac{T^3}{4}$.\nSo, the conditional variance is $S^2_c = \\frac{T^3}{3} - \\frac{T^3}{4} = \\frac{T^3}{12}$.\n\n**Task 2: Stratum Probabilities and Variances**\n\nThe stratification is on $W_T \\sim N(0, T)$. Let $\\tilde{c} = c/\\sqrt{T}$. The variable $Z = W_T/\\sqrt{T} \\sim N(0,1)$. Let $\\Phi$ and $\\phi$ be the CDF and PDF of the standard normal distribution.\nThe strata are $A_1 = (-\\infty, c]$ and $A_2 = (c, \\infty)$.\nThe stratum probabilities are:\n$$\np_1 = \\mathbb{P}(W_T \\in A_1) = \\mathbb{P}(W_T \\le c) = \\mathbb{P}\\left(\\frac{W_T}{\\sqrt{T}} \\le \\frac{c}{\\sqrt{T}}\\right) = \\Phi\\left(\\frac{c}{\\sqrt{T}}\\right)\n$$\n$$\np_2 = \\mathbb{P}(W_T \\in A_2) = \\mathbb{P}(W_T  c) = 1 - \\Phi\\left(\\frac{c}{\\sqrt{T}}\\right)\n$$\nThe stratum variances $S_k^2 = \\operatorname{Var}(I | W_T \\in A_k)$ are found using the law of total variance:\n$$\nS_k^2 = \\mathbb{E}[\\operatorname{Var}(I | W_T) | W_T \\in A_k] + \\operatorname{Var}(\\mathbb{E}[I | W_T] | W_T \\in A_k)\n$$\nFrom Task 1, $\\operatorname{Var}(I|W_T=x) = \\frac{T^3}{12}$ (a constant) and $\\mathbb{E}[I|W_T=x] = \\frac{Tx}{2}$.\n$$\nS_k^2 = \\mathbb{E}\\left[\\frac{T^3}{12} \\Big| W_T \\in A_k\\right] + \\operatorname{Var}\\left(\\frac{T W_T}{2} \\Big| W_T \\in A_k\\right) = \\frac{T^3}{12} + \\frac{T^2}{4}\\operatorname{Var}(W_T | W_T \\in A_k)\n$$\nLet $\\sigma_k^2 = \\operatorname{Var}(W_T | W_T \\in A_k)$ be the variance of $W_T$ truncated to stratum $A_k$. The variance of a $N(0, \\sigma^2)$ random variable truncated to $(a, b)$ is $\\sigma^2 \\left(1 - \\frac{\\xi_b\\phi(\\xi_b)-\\xi_a\\phi(\\xi_a)}{\\Phi(\\xi_b)-\\Phi(\\xi_a)} - \\left(\\frac{\\phi(\\xi_a)-\\phi(\\xi_b)}{\\Phi(\\xi_b)-\\Phi(\\xi_a)}\\right)^2\\right)$, where $\\xi_a = a/\\sigma, \\xi_b = b/\\sigma$.\nHere, $\\sigma^2 = T$. Let $\\tilde{c} = c/\\sqrt{T}$.\nFor $A_1=(-\\infty, c]$, $\\xi_a = -\\infty, \\xi_b = \\tilde{c}$.\n$$\n\\sigma_1^2 = T\\left(1 - \\frac{\\tilde{c}\\phi(\\tilde{c})}{\\Phi(\\tilde{c})} - \\left(\\frac{-\\phi(\\tilde{c})}{\\Phi(\\tilde{c})}\\right)^2\\right) = T\\left(1 - \\frac{\\tilde{c}\\phi(\\tilde{c})}{\\Phi(\\tilde{c})} - \\frac{\\phi^2(\\tilde{c})}{\\Phi^2(\\tilde{c})}\\right)\n$$\nFor $A_2=(c, \\infty)$, $\\xi_a = \\tilde{c}, \\xi_b = \\infty$.\n$$\n\\sigma_2^2 = T\\left(1 - \\frac{-\\tilde{c}\\phi(\\tilde{c})}{1-\\Phi(\\tilde{c})} - \\left(\\frac{\\phi(\\tilde{c})}{1-\\Phi(\\tilde{c})}\\right)^2\\right) = T\\left(1 + \\frac{\\tilde{c}\\phi(\\tilde{c})}{1-\\Phi(\\tilde{c})} - \\frac{\\phi^2(\\tilde{c})}{(1-\\Phi(\\tilde{c}))^2}\\right)\n$$\nThus, $S_1^2$ and $S_2^2$ are expressed in terms of $T$ and these truncated moments.\n\n**Task 3: Minimal Achievable Variance**\n\nFor a fixed partition (i.e., fixed $p_k, S_k$), the variance of the stratified estimator with $N = \\sum n_k$ samples is minimized by Neyman allocation, $n_k = N \\frac{p_k S_k}{\\sum_j p_j S_j}$. The minimal variance is $\\operatorname{Var}_{min} = \\frac{1}{N}(\\sum_{k=1}^m p_k S_k)^2$.\nThe budget constraint is $m c_f + c_p N \\le B$. To minimize variance, we maximize $N$, so $N = \\frac{B - m c_f}{c_p}$. This is feasible only if $B  m c_f$.\nSubstituting $N$, the minimal achievable variance for a given partition is:\n$$\nV(m, \\text{cuts}) = \\frac{c_p}{B-mc_f}\\left(\\sum_{k=1}^m p_k S_k\\right)^2\n$$\n\n**Task 4: Optimal $m^\\star$ and $c^\\star$ for $B=4c_f$**\n\nWe must compare the minimal variance for $m=1$ and $m=2$.\nThe budget constraint gives the total number of samples $N_m = \\frac{4c_f - m c_f}{c_p} = \\frac{(4-m)c_f}{c_p}$.\n\nCase $m=1$: No stratification. The whole space is one stratum.\n$N_1 = \\frac{3c_f}{c_p}$.\nThere is one stratum, so $p_1=1$ and $S_1^2 = \\operatorname{Var}(I)$. We compute this using the law of total variance:\n$$\n\\operatorname{Var}(I) = \\mathbb{E}[\\operatorname{Var}(I|W_T)] + \\operatorname{Var}(\\mathbb{E}[I|W_T]) = \\mathbb{E}\\left[\\frac{T^3}{12}\\right] + \\operatorname{Var}\\left(\\frac{TW_T}{2}\\right) = \\frac{T^3}{12} + \\frac{T^2}{4}\\operatorname{Var}(W_T)\n$$\nSince $\\operatorname{Var}(W_T)=T$,\n$$\n\\operatorname{Var}(I) = \\frac{T^3}{12} + \\frac{T^3}{4} = \\frac{4T^3}{12} = \\frac{T^3}{3}\n$$\nThe variance of the estimator for $m=1$ is $V_1 = \\frac{\\operatorname{Var}(I)}{N_1} = \\frac{T^3/3}{3c_f/c_p} = \\frac{c_p T^3}{9c_f}$.\n\nCase $m=2$: Two strata.\n$N_2 = \\frac{2c_f}{c_p}$.\nThe variance of the estimator is $V_2(c) = \\frac{1}{N_2}(p_1 S_1 + p_2 S_2)^2 = \\frac{c_p}{2c_f}(p_1(c) S_1(c) + p_2(c) S_2(c))^2$.\nWe need to find the optimal cut $c^\\star$ that minimizes $V_2(c)$. This is equivalent to minimizing $F(c) = p_1(c)S_1(c) + p_2(c)S_2(c)$.\nThe stratification variable $W_T \\sim N(0, T)$ has a symmetric PDF. The conditional mean $\\mathbb{E}[I|W_T=x] = Tx/2$ is an odd function of $x$, and the conditional variance is constant. This symmetric structure implies that the optimal cut is at the center of symmetry, $c=0$.\nTo confirm this, we showed $F(c)$ is an even function, $F(c)=F(-c)$, and that $c=0$ is a critical point, $F'(0)=0$. Given the nature of variance reduction, this critical point corresponds to a global minimum.\nSo, the optimal cut is $c^\\star=0$.\nAt $c=0$, we have $p_1=p_2=1/2$. By symmetry, $\\sigma_1^2(0) = \\sigma_2^2(0)$.\nLet's compute this variance. With $\\tilde{c}=0$:\n$$\n\\sigma_{1/2}^2 = \\sigma_1^2(0) = T\\left(1 - \\frac{0\\cdot\\phi(0)}{\\Phi(0)} - \\frac{\\phi^2(0)}{\\Phi^2(0)}\\right) = T\\left(1 - \\frac{(1/\\sqrt{2\\pi})^2}{(1/2)^2}\\right) = T\\left(1 - \\frac{1/(2\\pi)}{1/4}\\right) = T\\left(1 - \\frac{2}{\\pi}\\right)\n$$\nThe stratum variance at $c=0$ is $S_{1/2}^2 = S_1^2(0) = S_2^2(0)$:\n$$\nS_{1/2}^2 = \\frac{T^3}{12} + \\frac{T^2}{4}\\sigma_{1/2}^2 = \\frac{T^3}{12} + \\frac{T^2}{4}T\\left(1 - \\frac{2}{\\pi}\\right) = \\frac{T^3}{12} + \\frac{T^3}{4} - \\frac{T^3}{2\\pi} = T^3\\left(\\frac{1}{3} - \\frac{1}{2\\pi}\\right)\n$$\nThe minimal variance for $m=2$ is achieved at $c=0$:\n$$\nV_2(0) = \\frac{c_p}{2c_f}\\left(\\frac{1}{2}\\sqrt{S_{1/2}^2} + \\frac{1}{2}\\sqrt{S_{1/2}^2}\\right)^2 = \\frac{c_p}{2c_f} S_{1/2}^2 = \\frac{c_p T^3}{2c_f}\\left(\\frac{1}{3} - \\frac{1}{2\\pi}\\right)\n$$\nNow we compare $V_1$ and $V_2(0)$:\n$$\nV_1 = \\frac{c_p T^3}{c_f} \\cdot \\frac{1}{9} \\quad \\text{vs} \\quad V_2(0) = \\frac{c_p T^3}{c_f} \\cdot \\left(\\frac{1}{6} - \\frac{1}{4\\pi}\\right)\n$$\nWe compare the numerical factors $\\frac{1}{9}$ and $\\frac{1}{6} - \\frac{1}{4\\pi}$.\n$\\frac{1}{9} \\approx 0.1111$.\nUsing $\\pi \\approx 3.14159$, $4\\pi \\approx 12.566$, so $\\frac{1}{4\\pi} \\approx 0.07958$.\n$\\frac{1}{6} - \\frac{1}{4\\pi} \\approx 0.16667 - 0.07958 = 0.08709$.\nSince $0.08709  0.1111$, we have $V_2(0)  V_1$.\nTherefore, the optimal strategy is to use $m=2$ strata with a cut at $c=0$.\nThe optimal number of strata is $m^{\\star} = 2$ and the optimal cut is $c^{\\star} = 0$.\n\nFinal matrix form: $\\begin{pmatrix} m^{\\star}  c^{\\star} \\end{pmatrix}$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n2  0\n\\end{pmatrix}\n}\n$$", "id": "3350869"}]}