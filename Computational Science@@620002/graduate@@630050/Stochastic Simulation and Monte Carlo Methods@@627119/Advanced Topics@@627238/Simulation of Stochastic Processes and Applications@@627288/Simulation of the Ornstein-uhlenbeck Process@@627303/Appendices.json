{"hands_on_practices": [{"introduction": "Before embarking on complex simulations, it is essential to understand the error introduced at each step of the numerical integration. This practice provides a foundational analysis of the one-step strong mean-square error of the Euler-Maruyama scheme. By comparing this error to the intrinsic variance of the Ornstein-Uhlenbeck process, you will develop a crucial intuition for how the choice of time step $\\Delta$ relative to the mean-reversion time scale $1/\\kappa$ impacts simulation accuracy [@problem_id:3344350].", "problem": "Consider the Ornstein–Uhlenbeck (OU) process defined by the stochastic differential equation (SDE) $dX_{t} = -\\kappa \\left(X_{t} - \\mu \\right) \\, dt + \\sigma \\, dW_{t}$, where $\\kappa > 0$ is the mean-reversion rate, $\\mu \\in \\mathbb{R}$ is the long-term mean, $\\sigma > 0$ is the diffusion coefficient, and $W_{t}$ is a standard Wiener process. You will simulate the process over a single step of length $\\Delta > 0$ starting from $t = 0$ with the initial condition $X_{0} = \\mu$. Two one-step approximations are considered:\n\n- The exact discretization obtained by solving the linear SDE on $[0,\\Delta]$.\n- The Euler–Maruyama method driven by the same underlying Wiener increment to form a strong coupling.\n\nStarting from the SDE and the definition of the Euler–Maruyama scheme, derive the one-step strong mean-square error $E\\!\\left[\\left(X_{\\Delta}^{\\mathrm{EM}} - X_{\\Delta}^{\\mathrm{exact}}\\right)^{2}\\right]$ as a closed-form function of $\\kappa$, $\\sigma$, and $\\Delta$, and compare it to the exact one-step conditional variance $\\operatorname{Var}\\!\\left(X_{\\Delta} \\mid X_{0} = \\mu \\right)$. Define the relative error $R(\\kappa \\Delta)$ to be the ratio of the one-step strong mean-square error to $\\operatorname{Var}\\!\\left(X_{\\Delta} \\mid X_{0} = \\mu \\right)$, and provide a single closed-form analytic expression for $R(\\kappa \\Delta)$ in terms of $\\kappa$ and $\\Delta$. Identify, based on this expression, the regime of $\\kappa \\Delta$ for which the relative error is largest.\n\nYour final answer must be the single analytic expression for $R(\\kappa \\Delta)$, written in terms of $\\kappa$ and $\\Delta$. No numerical rounding is required.", "solution": "The Ornstein–Uhlenbeck (OU) process is governed by the linear stochastic differential equation $dX_{t} = -\\kappa \\left(X_{t} - \\mu \\right) \\, dt + \\sigma \\, dW_{t}$. For such linear SDEs, the exact solution over one step can be obtained via the integrating-factor method. Over the interval $[0,\\Delta]$ with initial condition $X_{0} = \\mu$, the mild solution reads\n$$\nX_{\\Delta}^{\\mathrm{exact}} = \\mu + \\sigma \\int_{0}^{\\Delta} \\exp\\!\\left(-\\kappa (\\Delta - s)\\right) \\, dW_{s}.\n$$\nThis follows by writing $Y_{t} = X_{t} - \\mu$, which satisfies $dY_{t} = -\\kappa Y_{t} \\, dt + \\sigma \\, dW_{t}$, and applying the integrating factor $\\exp(\\kappa t)$.\n\nThe Euler–Maruyama one-step scheme applied to the same SDE, driven by the same Wiener increment for strong coupling, yields\n$$\nX_{\\Delta}^{\\mathrm{EM}} = X_{0} - \\kappa \\left(X_{0} - \\mu\\right) \\Delta + \\sigma \\left(W_{\\Delta} - W_{0}\\right).\n$$\nWith $X_{0} = \\mu$, this simplifies to\n$$\nX_{\\Delta}^{\\mathrm{EM}} = \\mu + \\sigma \\left(W_{\\Delta} - W_{0}\\right) = \\mu + \\sigma \\int_{0}^{\\Delta} 1 \\, dW_{s}.\n$$\nHence, the one-step strong error under common driving $W$ is\n$$\nE_{\\Delta} := X_{\\Delta}^{\\mathrm{EM}} - X_{\\Delta}^{\\mathrm{exact}} = \\sigma \\int_{0}^{\\Delta} \\left(1 - \\exp\\!\\left(-\\kappa (\\Delta - s)\\right)\\right) \\, dW_{s}.\n$$\nBy Itô isometry, the one-step strong mean-square error is\n$$\n\\mathrm{MSE}(\\Delta) := E\\!\\left[\\left(E_{\\Delta}\\right)^{2}\\right] = \\sigma^{2} \\int_{0}^{\\Delta} \\left(1 - \\exp\\!\\left(-\\kappa (\\Delta - s)\\right)\\right)^{2} \\, ds.\n$$\nTo evaluate the integral, perform the change of variables $u = \\Delta - s$, so that $s = \\Delta - u$, $ds = -du$, and as $s$ runs from $0$ to $\\Delta$, $u$ runs from $\\Delta$ down to $0$. Thus\n$$\n\\int_{0}^{\\Delta} \\left(1 - \\exp\\!\\left(-\\kappa (\\Delta - s)\\right)\\right)^{2} \\, ds\n= \\int_{0}^{\\Delta} \\left(1 - \\exp(-\\kappa u)\\right)^{2} \\, du.\n$$\nExpanding the square and integrating termwise,\n$$\n\\int_{0}^{\\Delta} \\left(1 - 2 \\exp(-\\kappa u) + \\exp(-2 \\kappa u)\\right) \\, du\n= \\Delta - 2 \\int_{0}^{\\Delta} \\exp(-\\kappa u) \\, du + \\int_{0}^{\\Delta} \\exp(-2 \\kappa u) \\, du.\n$$\nUsing $\\int_{0}^{\\Delta} \\exp(-\\alpha u) \\, du = \\frac{1 - \\exp(-\\alpha \\Delta)}{\\alpha}$ for $\\alpha > 0$, we obtain\n$$\n\\int_{0}^{\\Delta} \\left(1 - \\exp(-\\kappa u)\\right)^{2} \\, du\n= \\Delta - \\frac{2 \\left(1 - \\exp(-\\kappa \\Delta)\\right)}{\\kappa}\n+ \\frac{1 - \\exp(-2 \\kappa \\Delta)}{2 \\kappa}.\n$$\nTherefore,\n$$\n\\mathrm{MSE}(\\Delta) = \\sigma^{2} \\left[ \\Delta - \\frac{2 \\left(1 - \\exp(-\\kappa \\Delta)\\right)}{\\kappa}\n+ \\frac{1 - \\exp(-2 \\kappa \\Delta)}{2 \\kappa} \\right].\n$$\n\nNext, compute the exact one-step conditional variance given $X_{0} = \\mu$. From the exact solution,\n$$\nX_{\\Delta}^{\\mathrm{exact}} - \\mu = \\sigma \\int_{0}^{\\Delta} \\exp\\!\\left(-\\kappa (\\Delta - s)\\right) \\, dW_{s},\n$$\nand by Itô isometry,\n$$\n\\operatorname{Var}\\!\\left(X_{\\Delta} \\mid X_{0} = \\mu \\right)\n= \\sigma^{2} \\int_{0}^{\\Delta} \\exp\\!\\left(-2 \\kappa (\\Delta - s)\\right) \\, ds\n= \\sigma^{2} \\int_{0}^{\\Delta} \\exp(-2 \\kappa u) \\, du\n= \\sigma^{2} \\frac{1 - \\exp(-2 \\kappa \\Delta)}{2 \\kappa}.\n$$\n\nDefine the relative error $R(\\kappa \\Delta)$ as the ratio\n$$\nR(\\kappa \\Delta) := \\frac{\\mathrm{MSE}(\\Delta)}{\\operatorname{Var}\\!\\left(X_{\\Delta} \\mid X_{0} = \\mu \\right)}.\n$$\nSubstituting the expressions above and simplifying yields\n$$\nR(\\kappa \\Delta) = \\frac{\\Delta - \\frac{2 \\left(1 - \\exp(-\\kappa \\Delta)\\right)}{\\kappa}\n+ \\frac{1 - \\exp(-2 \\kappa \\Delta)}{2 \\kappa}}{\\frac{1 - \\exp(-2 \\kappa \\Delta)}{2 \\kappa}}\n= \\frac{2 \\kappa \\Delta - 4 \\left(1 - \\exp(-\\kappa \\Delta)\\right) + \\left(1 - \\exp(-2 \\kappa \\Delta)\\right)}{1 - \\exp(-2 \\kappa \\Delta)}.\n$$\nEquivalently, introducing the dimensionless parameter $\\theta := \\kappa \\Delta$,\n$$\nR(\\theta) = \\frac{2 \\theta - 3 + 4 \\exp(-\\theta) - \\exp(-2 \\theta)}{1 - \\exp(-2 \\theta)}.\n$$\n\nTo identify the regime of largest relative error, inspect the limiting behavior. As $\\theta \\to 0$, use the expansions $\\exp(-\\theta) = 1 - \\theta + \\frac{\\theta^{2}}{2} - \\frac{\\theta^{3}}{6} + \\cdots$ and $\\exp(-2 \\theta) = 1 - 2 \\theta + 2 \\theta^{2} - \\frac{4}{3} \\theta^{3} + \\cdots$ to find $R(\\theta) \\sim \\frac{1}{3} \\theta^{2} \\to 0$. As $\\theta \\to \\infty$, $\\exp(-\\theta)$ and $\\exp(-2 \\theta)$ vanish, and $R(\\theta) \\sim \\frac{2 \\theta - 3}{1} = 2 \\theta - 3$, which grows without bound. Thus, the relative error is largest for large $\\kappa \\Delta$, that is, when the time step $\\Delta$ is large relative to the mean-reversion time scale $1 / \\kappa$.", "answer": "$$\\boxed{\\frac{2 \\kappa \\Delta - 3 + 4 \\exp\\!\\left(-\\kappa \\Delta\\right) - \\exp\\!\\left(-2 \\kappa \\Delta\\right)}{1 - \\exp\\!\\left(-2 \\kappa \\Delta\\right)}}$$", "id": "3344350"}, {"introduction": "Building on the analysis of simulation error, we now explore a subtle but critical consequence: discretization bias in parameter estimation. This exercise presents a hypothetical scenario where an analyst uses data generated by the approximate Euler-Maruyama scheme but applies a parameter estimation formula derived from the exact process dynamics. By calculating the resulting systematic bias, you will learn why the choice of simulation method has profound implications for statistical inference and model calibration [@problem_id:3344342].", "problem": "Consider the Ornstein–Uhlenbeck (OU) process defined by the stochastic differential equation (SDE) $dX_{t} = \\theta (\\mu - X_{t}) \\, dt + \\sigma \\, dW_{t}$, where $\\theta > 0$, $\\mu \\in \\mathbb{R}$, $\\sigma > 0$, and $W_{t}$ is a standard Wiener process. Suppose one simulates $\\{X_{n\\Delta}\\}_{n=0}^{N}$ using the Euler–Maruyama (EM) scheme with fixed time step $\\Delta > 0$, that is, $X_{(n+1)\\Delta} = X_{n\\Delta} + \\theta(\\mu - X_{n\\Delta}) \\Delta + \\sigma \\sqrt{\\Delta} \\,\\varepsilon_{n}$ with independent standard normal $\\varepsilon_{n}$. Then, ignoring the fact that the data were generated by Euler–Maruyama, an analyst fits an Autoregressive model of order one (AR(1)) to $\\{X_{n\\Delta}\\}$ and converts the estimated autoregressive coefficient $\\hat{\\varphi}$ into a continuous-time mean-reversion rate via the exact-discretization mapping $\\hat{\\theta} = -\\Delta^{-1} \\ln(\\hat{\\varphi})$. Assume that the sample size $N$ is arbitrarily large so that estimation variability can be neglected and $\\hat{\\varphi}$ converges to the pseudo-true autoregressive coefficient of the data-generating model. \n\nUsing only fundamental definitions of the EM scheme, the exact discrete-time transition of the OU process, and asymptotic series for smooth functions, derive the leading-order term in $\\Delta$ of the asymptotic bias $B(\\Delta) = \\hat{\\theta} - \\theta$ induced by using the exact-discretization mapping on EM-simulated data. Provide your final result as a single closed-form analytic expression for the leading-order term in $\\Delta$ (that is, the coefficient of the smallest positive power of $\\Delta$ in $B(\\Delta)$), expressed in terms of $\\theta$ and $\\Delta$. Do not include units and do not round.", "solution": "The problem asks for the leading-order term of the asymptotic bias in estimating the mean-reversion parameter $\\theta$ of an Ornstein-Uhlenbeck (OU) process when the estimation is based on data generated by the Euler-Maruyama (EM) scheme, but the parameter mapping assumes an exact discretization.\n\nThe OU process is described by the stochastic differential equation (SDE):\n$$dX_{t} = \\theta (\\mu - X_{t}) \\, dt + \\sigma \\, dW_{t}$$\nwhere $\\theta > 0$, $\\mu \\in \\mathbb{R}$, $\\sigma > 0$, and $W_{t}$ is a standard Wiener process.\n\nThe problem states that the process is simulated using the Euler-Maruyama scheme with a fixed time step $\\Delta > 0$. The discrete-time update rule is given by:\n$$X_{(n+1)\\Delta} = X_{n\\Delta} + \\theta(\\mu - X_{n\\Delta}) \\Delta + \\sigma \\sqrt{\\Delta} \\,\\varepsilon_{n}$$\nwhere $\\varepsilon_{n}$ are independent random variables from a standard normal distribution, $\\varepsilon_{n} \\sim N(0, 1)$.\n\nThis equation can be rearranged to highlight its structure as an Autoregressive model of order one (AR(1)):\n$$X_{(n+1)\\Delta} = \\theta\\mu\\Delta + (1 - \\theta\\Delta) X_{n\\Delta} + \\sigma \\sqrt{\\Delta} \\,\\varepsilon_{n}$$\nThis is a standard AR(1) process of the form $Y_{n+1} = c + \\phi Y_n + e_{n+1}$, where $Y_n = X_{n\\Delta}$, the constant term is $c = \\theta\\mu\\Delta$, the autoregressive coefficient is $\\phi = 1 - \\theta\\Delta$, and the innovation term is $e_{n+1} = \\sigma \\sqrt{\\Delta} \\,\\varepsilon_{n}$.\n\nThe problem specifies that the sample size $N$ is arbitrarily large, which implies that any statistical estimation variability can be ignored. An analyst fits an AR(1) model to the simulated data $\\{X_{n\\Delta}\\}$. In the large sample limit, the estimated autoregressive coefficient, denoted $\\hat{\\varphi}$, will converge to the true autoregressive coefficient of the data-generating process. In this case, the data are generated by the EM scheme, so the pseudo-true parameter is:\n$$\\hat{\\varphi} = 1 - \\theta\\Delta$$\n\nThe analyst, however, converts this estimated coefficient $\\hat{\\varphi}$ into an estimate of the continuous-time parameter, $\\hat{\\theta}$, using the mapping derived from the *exact* discretization of the OU process. The exact solution to the OU SDE over a time interval $\\Delta$ gives the following exact AR(1) representation:\n$$X_{(n+1)\\Delta} = \\mu(1 - e^{-\\theta\\Delta}) + e^{-\\theta\\Delta} X_{n\\Delta} + \\text{noise term}$$\nFrom this exact form, the true relationship between the continuous-time parameter $\\theta$ and the exact discrete-time autoregressive coefficient $\\varphi_{\\text{exact}} = e^{-\\theta\\Delta}$ is $\\theta = -\\frac{1}{\\Delta} \\ln(\\varphi_{\\text{exact}})$.\n\nThe analyst erroneously applies this mapping to the coefficient $\\hat{\\varphi}$ obtained from the EM-simulated data:\n$$\\hat{\\theta} = -\\frac{1}{\\Delta} \\ln(\\hat{\\varphi})$$\nSubstituting the expression for $\\hat{\\varphi}$:\n$$\\hat{\\theta} = -\\frac{1}{\\Delta} \\ln(1 - \\theta\\Delta)$$\nThe asymptotic bias is defined as $B(\\Delta) = \\hat{\\theta} - \\theta$. Therefore,\n$$B(\\Delta) = -\\frac{1}{\\Delta} \\ln(1 - \\theta\\Delta) - \\theta$$\nTo find the leading-order term of the bias for small $\\Delta$, we perform a Taylor series expansion of the logarithmic term around $\\Delta = 0$. The Taylor series for $\\ln(1-x)$ around $x=0$ is:\n$$\\ln(1-x) = -x - \\frac{x^2}{2} - \\frac{x^3}{3} - \\dots = -\\sum_{k=1}^{\\infty} \\frac{x^k}{k}$$\nLetting $x = \\theta\\Delta$, we have:\n$$\\ln(1 - \\theta\\Delta) = -(\\theta\\Delta) - \\frac{(\\theta\\Delta)^2}{2} - \\frac{(\\theta\\Delta)^3}{3} - O(\\Delta^4)$$\n$$\\ln(1 - \\theta\\Delta) = -\\theta\\Delta - \\frac{\\theta^2\\Delta^2}{2} - O(\\Delta^3)$$\nNow we substitute this expansion back into the expression for $\\hat{\\theta}$:\n$$\\hat{\\theta} = -\\frac{1}{\\Delta} \\left( -\\theta\\Delta - \\frac{\\theta^2\\Delta^2}{2} - O(\\Delta^3) \\right)$$\n$$\\hat{\\theta} = \\theta + \\frac{\\theta^2\\Delta}{2} + O(\\Delta^2)$$\nThe bias $B(\\Delta)$ is then:\n$$B(\\Delta) = \\hat{\\theta} - \\theta = \\left( \\theta + \\frac{\\theta^2\\Delta}{2} + O(\\Delta^2) \\right) - \\theta$$\n$$B(\\Delta) = \\frac{\\theta^2\\Delta}{2} + O(\\Delta^2)$$\nThe leading-order term in the expansion of the bias $B(\\Delta)$ is the term with the smallest positive power of $\\Delta$. This term is $\\frac{\\theta^2\\Delta}{2}$. This represents the dominant part of the bias for small time steps $\\Delta$.", "answer": "$$\\boxed{\\frac{\\theta^2 \\Delta}{2}}$$", "id": "3344342"}, {"introduction": "This final practice moves from analytical exercises to a full-fledged computational problem, where theory meets application. You will estimate the probability of the process crossing a barrier, a common task in financial engineering and other fields. This exercise makes the abstract concept of discretization bias tangible by showing how naive, discrete-time sampling systematically underestimates the true probability, and then guides you through implementing a principled correction to obtain a more accurate result [@problem_id:3344309].", "problem": "Consider the Ornstein–Uhlenbeck process defined by the stochastic differential equation $dX_t=-\\kappa\\left(X_t-\\mu\\right)\\,dt+\\sigma\\,dW_t,$ where $W_t$ is a standard Wiener process, $\\kappa>0$ is the mean-reversion rate, $\\mu\\in\\mathbb{R}$ is the long-run mean, and $\\sigma>0$ is the diffusion coefficient. The goal is to estimate the probability $\\mathbb{P}\\!\\left(\\max_{0\\le s\\le T}X_s\\ge b\\right)$ for a fixed threshold $b\\in\\mathbb{R}$ using Monte Carlo simulation (MC), discuss the systematic downward bias caused by time discretization when detecting barrier crossings via a discretized path, and implement a refinement that reduces this bias using a principled crossing-probability correction between sampling times. You must proceed from first principles of stochastic simulation and construct all derivations needed for your algorithmic choices.\n\nRequirements:\n\n1. Use the Euler–Maruyama method as the fundamental discrete-time simulator of the process over a uniform grid $t_0=0<t_1<\\cdots<t_N=T$ with step size $\\Delta t=T/N$. For each path, record whether the discrete-time maximum $\\max_{0\\le i\\le N}X_{t_i}$ exceeds the barrier $b$. This defines an estimator for $\\mathbb{P}\\!\\left(\\max_{0\\le s\\le T}X_s\\ge b\\right)$ based on discrete-time sampling alone.\n\n2. Explain why, when sampling at discrete times, the event of crossing the barrier between two grid points may be missed, and therefore the discrete-time estimator is systematically biased downward. Provide a rigorous argument based on the properties of the Euler–Maruyama increment over a single step and the continuity of sample paths.\n\n3. Design and implement a refinement that reduces the downward bias by incorporating, for each interval $\\left[t_i,t_{i+1}\\right]$, a principled estimate of the probability that a continuous path crosses the barrier $b$ between $t_i$ and $t_{i+1}$, conditioned on the two endpoints $X_{t_i}$ and $X_{t_{i+1}}$. Derive the crossing probability starting from the decomposition of the Euler–Maruyama increment into deterministic drift and a scaled Wiener increment, and use this to construct an interval-wise correction. Combine the interval corrections along the path to obtain a per-path crossing-probability estimate, and then aggregate over paths to produce a refined MC estimator.\n\n4. To gauge the magnitude of the discretization bias and the effectiveness of the refinement, also compute a high-fidelity reference by simulating the process on a much finer grid using the exact Gaussian transition of the Ornstein–Uhlenbeck process over a time step $\\Delta t$, namely the transition $X_{t+\\Delta t}\\mid X_t\\sim\\mathcal{N}\\!\\left(\\mu+(X_t-\\mu)\\,e^{-\\kappa \\Delta t},\\;\\frac{\\sigma^2}{2\\kappa}\\left(1-e^{-2\\kappa \\Delta t}\\right)\\right),$ and estimating the crossing probability from the fine-grid discrete-time maximum.\n\n5. For all simulations, ensure that the initial value $X_0=x_0$ is included when computing the discrete maximum, so that if $x_0\\ge b$ then the crossing event is recorded at $t=0$.\n\n6. Numerical output units: the desired probability is unitless; all outputs must be real numbers in $[0,1]$ expressed as decimals.\n\n7. Implement the following test suite. For each test case, produce three outputs: the coarse-grid discrete-time estimator, the refined coarse-grid estimator with interval crossing correction, and the fine-grid reference estimator. Use the specified seeds to ensure reproducibility.\n\n   Test case A (happy path):\n   - Parameters: $\\kappa=1.5$, $\\mu=0.0$, $\\sigma=0.5$, $x_0=0.0$, $T=1.0$, $b=1.0$.\n   - Coarse grid: $\\Delta t=0.05$, $N=20$, $N_{\\text{paths}}=100000$, random seed $42$.\n   - Fine grid reference: $\\Delta t_{\\text{ref}}=0.005$, $N_{\\text{ref}}=200$, $N_{\\text{paths,ref}}=40000$, random seed $1042$.\n\n   Test case B (boundary condition):\n   - Parameters: $\\kappa=1.0$, $\\mu=0.0$, $\\sigma=0.5$, $x_0=0.3$, $T=1.0$, $b=0.3$.\n   - Coarse grid: $\\Delta t=0.1$, $N=10$, $N_{\\text{paths}}=100000$, random seed $43$.\n   - Fine grid reference: $\\Delta t_{\\text{ref}}=0.01$, $N_{\\text{ref}}=100$, $N_{\\text{paths,ref}}=40000$, random seed $1043$.\n\n   Test case C (rare event with coarse sampling):\n   - Parameters: $\\kappa=1.5$, $\\mu=0.0$, $\\sigma=0.5$, $x_0=0.0$, $T=1.0$, $b=2.0$.\n   - Coarse grid: $\\Delta t=0.1$, $N=10$, $N_{\\text{paths}}=100000$, random seed $44$.\n   - Fine grid reference: $\\Delta t_{\\text{ref}}=0.01$, $N_{\\text{ref}}=100$, $N_{\\text{paths,ref}}=40000$, random seed $1044$.\n\n   Test case D (short horizon with fast mean reversion):\n   - Parameters: $\\kappa=4.0$, $\\mu=0.0$, $\\sigma=1.0$, $x_0=-1.0$, $T=0.5$, $b=0.0$.\n   - Coarse grid: $\\Delta t=0.05$, $N=10$, $N_{\\text{paths}}=100000$, random seed $45$.\n   - Fine grid reference: $\\Delta t_{\\text{ref}}=0.005$, $N_{\\text{ref}}=100$, $N_{\\text{paths,ref}}=40000$, random seed $1045$.\n\n8. Final output specification: Your program should produce a single line of output containing the $12$ results for the four test cases, in the order\n   $\\left[\\widehat{p}_{\\text{coarse}}^{(A)},\\;\\widehat{p}_{\\text{refined}}^{(A)},\\;\\widehat{p}_{\\text{fine}}^{(A)},\\;\\widehat{p}_{\\text{coarse}}^{(B)},\\;\\widehat{p}_{\\text{refined}}^{(B)},\\;\\widehat{p}_{\\text{fine}}^{(B)},\\;\\widehat{p}_{\\text{coarse}}^{(C)},\\;\\widehat{p}_{\\text{refined}}^{(C)},\\;\\widehat{p}_{\\text{fine}}^{(C)},\\;\\widehat{p}_{\\text{coarse}}^{(D)},\\;\\widehat{p}_{\\text{refined}}^{(D)},\\;\\widehat{p}_{\\text{fine}}^{(D)}\\right],$ formatted as a comma-separated list enclosed in square brackets, for example, $\\left[0.123456,0.234567,0.345678,\\ldots\\right].$", "solution": "The problem requires the estimation of the probability that an Ornstein-Uhlenbeck (OU) process, $X_t$, exceeds a certain barrier, $b$, within a given time horizon $[0, T]$. The OU process is described by the stochastic differential equation (SDE):\n$$dX_t = -\\kappa(X_t - \\mu)dt + \\sigma dW_t, \\quad X_0 = x_0$$\nwhere $\\kappa > 0$ is the rate of mean reversion, $\\mu$ is the long-term mean, $\\sigma > 0$ is the volatility, and $W_t$ is a standard Wiener process. We are tasked with estimating $\\mathbb{P}(\\max_{0 \\le s \\le T} X_s \\ge b)$ using three different Monte Carlo (MC) methods to highlight and address the issue of discretization bias. The time interval $[0, T]$ is discretized into $N$ steps of size $\\Delta t = T/N$, yielding a time grid $t_i = i \\Delta t$ for $i = 0, 1, \\dots, N$.\n\nThe three methods are: $1)$ a coarse estimator based on the discrete-time maximum of an Euler-Maruyama simulation, $2)$ a refined version of this estimator that corrects for missed barrier crossings between grid points, and $3)$ a high-fidelity reference estimator computed on a fine grid using the exact transition law of the OU process.\n\n**Method 1: Coarse Estimator via Euler-Maruyama**\n\nThe Euler-Maruyama method is a fundamental scheme for the numerical integration of SDEs. Applying it to the OU process SDE gives the discrete-time update rule:\n$$X_{t_{i+1}} = X_{t_i} - \\kappa(X_{t_i} - \\mu)\\Delta t + \\sigma \\Delta W_i$$\nwhere $\\Delta W_i = W_{t_{i+1}} - W_{t_i}$ is an increment of the Wiener process. These increments are independent and identically distributed normal random variables with mean $0$ and variance $\\Delta t$. We can write $\\Delta W_i = \\sqrt{\\Delta t} Z_i$, where $Z_i \\sim \\mathcal{N}(0, 1)$ are independent standard normal random variables.\n\nThe simulation proceeds by generating a large number of paths, $N_{\\text{paths}}$. For each path, we generate a sequence of states $X_0, X_{t_1}, \\dots, X_{t_N}$. The simplest MC estimator for the crossing probability is based on observing only the values of the process at these discrete time points. We define an indicator variable for each path $j$:\n$$I_j = \\mathbb{I}\\left(\\max_{0 \\le i \\le N} X_{t_i}^{(j)} \\ge b\\right)$$\nwhere $\\mathbb{I}(\\cdot)$ is the indicator function. The coarse estimator, $\\hat{p}_{\\text{coarse}}$, is the sample mean of these indicators:\n$$\\hat{p}_{\\text{coarse}} = \\frac{1}{N_{\\text{paths}}} \\sum_{j=1}^{N_{\\text{paths}}} I_j$$\n\n**Analysis of Discretization Bias**\n\nThis coarse estimator is systematically biased. The sample paths of an OU process are continuous. It is possible for a path to cross the barrier $b$ between two grid points, $t_i$ and $t_{i+1}$, and then return below the barrier, such that $X_{t_i} < b$ and $X_{t_{i+1}} < b$, but $\\max_{s \\in [t_i, t_{i+1}]} X_s \\ge b$. The discrete-time maximum check at points $t_i$ would fail to detect such a crossing.\n\nFormally, the event that the discrete-time maximum exceeds the barrier, $E_{\\text{discrete}} = \\{\\max_{0 \\le i \\le N} X_{t_i} \\ge b\\}$, is a subset of the true event of interest, which is that the continuous-time maximum exceeds the barrier, $E_{\\text{continuous}} = \\{\\max_{0 \\le s \\le T} X_s \\ge b\\}$. That is, $E_{\\text{discrete}} \\subseteq E_{\\text{continuous}}$. This implies that the probability of the discrete event is less than or equal to the probability of the continuous event:\n$$\\mathbb{P}(E_{\\text{discrete}}) \\le \\mathbb{P}(E_{\\text{continuous}})$$\nThe estimator $\\hat{p}_{\\text{coarse}}$ is a consistent estimator of $\\mathbb{P}(E_{\\text{discrete}})$, not $\\mathbb{P}(E_{\\text{continuous}})$. Therefore, $\\hat{p}_{\\text{coarse}}$ systematically underestimates the true crossing probability, exhibiting a downward bias that decreases as the time step $\\Delta t$ approaches $0$.\n\n**Method 2: Refined Estimator with Interval Correction**\n\nTo mitigate this bias, we can refine the estimator by accounting for the probability of a crossing occurring within an interval $[t_i, t_{i+1}]$, conditioned on the observed endpoints $X_{t_i} = u$ and $X_{t_{i+1}} = v$.\n\nFor a small time interval $\\Delta t$, the drift and diffusion coefficients of the OU process can be approximated as constant. Specifically, we approximate the process $X_s$ over $s \\in [t_i, t_{i+1}]$ as a Brownian motion with constant volatility $\\sigma$, starting from $X_{t_i}=u$. The drift term $-\\kappa(X_s-\\mu)$ is of order $dt$, while the diffusive part $\\sigma dW_s$ is of order $\\sqrt{dt}$. For small $\\Delta t$, the path's local behavior is dominated by the diffusion term. This justifies approximating the process locally as $Y_s = u + \\sigma(W_s - W_{t_i})$.\n\nWe require the probability that this process crosses a barrier $b$, given that it starts at $Y_{t_i} = u$ and ends at $Y_{t_{i+1}} = v$. This is a classic result for a Brownian bridge. Let's analyze the process $B_\\tau = W_{t_i+\\tau} - W_{t_i}$ for $\\tau \\in [0, \\Delta t]$. $B_\\tau$ is a standard Wiener process starting at $B_0=0$. The process is $Y_{t_i+\\tau} = u + \\sigma B_\\tau$. The endpoint condition $Y_{t_{i+1}}=v$ fixes the Wiener increment: $B_{\\Delta t} = (v-u)/\\sigma$. The crossing event is $\\max_{0 \\le \\tau \\le \\Delta t} Y_{t_i+\\tau} \\ge b$, which is equivalent to $\\max_{0 \\le \\tau \\le \\Delta t} B_\\tau \\ge (b-u)/\\sigma$.\n\nThe probability that the maximum of a standard Wiener process $B_\\tau$ on $[0, T]$ exceeds a level $a > 0$, conditional on $B_T = x < a$, is given by:\n$$\\mathbb{P}(\\max_{0 \\le \\tau \\le T} B_\\tau \\ge a \\mid B_T = x) = \\exp\\left(-\\frac{2a(a-x)}{T}\\right)$$\nMapping our variables, we have $T=\\Delta t$, $a = (b-u)/\\sigma$, and $x=(v-u)/\\sigma$. We need to consider the case where both endpoints are below the barrier, i.e., $u<b$ and $v<b$. Then $a>0$. The conditional probability of a crossing within the interval $[t_i, t_{i+1}]$ is:\n$$p_i(u, v) = \\exp\\left(-\\frac{2\\frac{b-u}{\\sigma}\\left(\\frac{b-u}{\\sigma} - \\frac{v-u}{\\sigma}\\right)}{\\Delta t}\\right) = \\exp\\left(-\\frac{2(b-u)(b-v)}{\\sigma^2 \\Delta t}\\right)$$\nThis formula provides the probability of a missed crossing in an interval, given both endpoints are below the barrier.\n\nThe refined estimator is constructed as follows. For each simulated path $j$:\n1. If $\\max_{0 \\le i \\le N} X_{t_i}^{(j)} \\ge b$, the path has definitely crossed. The crossing probability for this path, $P^{(j)}$, is $1$.\n2. If $\\max_{0 \\le i \\le N} X_{t_i}^{(j)} < b$, the path might have crossed in one or more of the $N$ intervals. The probability of *not* crossing in interval $i$ is $1 - p_i(X_{t_i}^{(j)}, X_{t_{i+1}}^{(j)})$. Assuming the crossing events in disjoint intervals are approximately independent for small $\\Delta t$, the probability of *not* crossing anywhere in $[0, T]$ is the product of these non-crossing probabilities. The probability of at least one crossing is therefore:\n$$P^{(j)} = 1 - \\prod_{i=0}^{N-1} \\left(1 - p_i(X_{t_i}^{(j)}, X_{t_{i+1}}^{(j)})\\right)$$\nThe refined estimator, $\\hat{p}_{\\text{refined}}$, is the average of these path probabilities over all simulated paths:\n$$\\hat{p}_{\\text{refined}} = \\frac{1}{N_{\\text{paths}}} \\sum_{j=1}^{N_{\\text{paths}}} P^{(j)}$$\nThis is a form of Rao-Blackwellization, where a simple indicator is replaced by its conditional expectation, which can reduce variance and bias.\n\n**Method 3: High-Fidelity Reference Estimator**\n\nTo assess the accuracy of the coarse and refined estimators, a benchmark value is needed. We can compute a high-fidelity estimate by using a much finer time grid ($\\Delta t_{\\text{ref}} \\ll \\Delta t$) and replacing the Euler-Maruyama approximation with the exact transition law of the OU process. The OU process is a Gaussian process, and the exact distribution of $X_{t+\\Delta t}$ conditional on $X_t$ is known:\n$$X_{t+\\Delta t} \\mid X_t \\sim \\mathcal{N}\\left(\\mu + (X_t - \\mu)e^{-\\kappa \\Delta t}, \\frac{\\sigma^2}{2\\kappa}(1 - e^{-2\\kappa \\Delta t})\\right)$$\nWe can generate paths using the exact update step:\n$$X_{t_{i+1}} = \\mu + (X_{t_i} - \\mu)e^{-\\kappa \\Delta t_{\\text{ref}}} + \\sqrt{\\frac{\\sigma^2}{2\\kappa}(1 - e^{-2\\kappa \\Delta t_{\\text{ref}}})} Z_i, \\quad Z_i \\sim \\mathcal{N}(0, 1)$$\nBy using a very small $\\Delta t_{\\text{ref}}$, the probability of a missed crossing between grid points becomes negligible. The reference estimator, $\\hat{p}_{\\text{fine}}$, is then the standard discrete maximum estimator computed on these fine-grid paths:\n$$\\hat{p}_{\\text{fine}} = \\frac{1}{N_{\\text{paths,ref}}} \\sum_{j=1}^{N_{\\text{paths,ref}}} \\mathbb{I}\\left(\\max_{0 \\le i \\le N_{\\text{ref}}} X_{t_i}^{(j)} \\ge b\\right)$$\nThis provides a reliable benchmark against which to judge the performance of the coarse and refined estimators.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    test_cases = [\n        # Test case A (happy path)\n        {\n            \"params\": {\"kappa\": 1.5, \"mu\": 0.0, \"sigma\": 0.5, \"x0\": 0.0, \"T\": 1.0, \"b\": 1.0},\n            \"coarse_grid\": {\"dt\": 0.05, \"N_paths\": 100000, \"seed\": 42},\n            \"fine_grid\": {\"dt\": 0.005, \"N_paths\": 40000, \"seed\": 1042},\n        },\n        # Test case B (boundary condition)\n        {\n            \"params\": {\"kappa\": 1.0, \"mu\": 0.0, \"sigma\": 0.5, \"x0\": 0.3, \"T\": 1.0, \"b\": 0.3},\n            \"coarse_grid\": {\"dt\": 0.1, \"N_paths\": 100000, \"seed\": 43},\n            \"fine_grid\": {\"dt\": 0.01, \"N_paths\": 40000, \"seed\": 1043},\n        },\n        # Test case C (rare event with coarse sampling)\n        {\n            \"params\": {\"kappa\": 1.5, \"mu\": 0.0, \"sigma\": 0.5, \"x0\": 0.0, \"T\": 1.0, \"b\": 2.0},\n            \"coarse_grid\": {\"dt\": 0.1, \"N_paths\": 100000, \"seed\": 44},\n            \"fine_grid\": {\"dt\": 0.01, \"N_paths\": 40000, \"seed\": 1044},\n        },\n        # Test case D (short horizon with fast mean reversion)\n        {\n            \"params\": {\"kappa\": 4.0, \"mu\": 0.0, \"sigma\": 1.0, \"x0\": -1.0, \"T\": 0.5, \"b\": 0.0},\n            \"coarse_grid\": {\"dt\": 0.05, \"N_paths\": 100000, \"seed\": 45},\n            \"fine_grid\": {\"dt\": 0.005, \"N_paths\": 40000, \"seed\": 1045},\n        },\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        p = case[\"params\"]\n        cg = case[\"coarse_grid\"]\n        fg = case[\"fine_grid\"]\n\n        # --- Coarse grid simulations ---\n        rng_coarse = np.random.default_rng(cg[\"seed\"])\n        N_coarse = int(p[\"T\"] / cg[\"dt\"])\n\n        # Generate paths using Euler-Maruyama\n        paths_coarse = np.zeros((cg[\"N_paths\"], N_coarse + 1))\n        paths_coarse[:, 0] = p[\"x0\"]\n        \n        for i in range(N_coarse):\n            Z = rng_coarse.standard_normal(cg[\"N_paths\"])\n            drift = -p[\"kappa\"] * (paths_coarse[:, i] - p[\"mu\"]) * cg[\"dt\"]\n            diffusion = p[\"sigma\"] * np.sqrt(cg[\"dt\"]) * Z\n            paths_coarse[:, i+1] = paths_coarse[:, i] + drift + diffusion\n\n        # --- 1. Coarse Estimator ---\n        max_discrete = np.max(paths_coarse, axis=1)\n        p_coarse = np.mean(max_discrete >= p[\"b\"])\n        results.append(p_coarse)\n\n        # --- 2. Refined Estimator ---\n        # Paths that did not cross at discrete points\n        sub_barrier_paths_mask = max_discrete < p[\"b\"]\n        sub_barrier_paths = paths_coarse[sub_barrier_paths_mask]\n        \n        # Calculate interval crossing probabilities for these paths\n        if sub_barrier_paths.shape[0] > 0:\n            u = sub_barrier_paths[:, :-1]\n            v = sub_barrier_paths[:, 1:]\n            \n            # This is p_i(u,v) = exp(-2*(b-u)*(b-v) / (sigma^2 * dt))\n            # Handle potential overflow in exp by clipping exponent for stability\n            exponent = -2.0 * (p[\"b\"] - u) * (p[\"b\"] - v) / (p[\"sigma\"]**2 * cg[\"dt\"])\n            # The exponent is always non-positive since u,v < b\n            p_interval_cross = np.exp(np.maximum(exponent, -700)) # clip to avoid underflow being exactly 0\n            \n            # Probability of at least one crossing for paths that were below barrier at nodes\n            # P_path = 1 - product(1 - p_interval_cross)\n            # log(1-P_path) = sum(log(1-p_interval_cross))\n            # Use log-sum-exp trick for numerical stability\n            p_path_cross = 1.0 - np.exp(np.sum(np.log1p(-p_interval_cross), axis=1))\n\n            path_probabilities = np.ones(cg[\"N_paths\"])\n            path_probabilities[sub_barrier_paths_mask] = p_path_cross\n        else: # All paths crossed at nodes\n            path_probabilities = np.ones(cg[\"N_paths\"])\n            \n        p_refined = np.mean(path_probabilities)\n        results.append(p_refined)\n        \n        # --- 3. Fine Grid Reference Estimator ---\n        rng_fine = np.random.default_rng(fg[\"seed\"])\n        N_fine = int(p[\"T\"] / fg[\"dt\"])\n\n        # Generate paths using exact transition\n        paths_fine = np.zeros((fg[\"N_paths\"], N_fine + 1))\n        paths_fine[:, 0] = p[\"x0\"]\n        \n        exp_k_dt = np.exp(-p[\"kappa\"] * fg[\"dt\"])\n        # Variance term\n        if p[\"kappa\"] > 1e-9:\n            var = (p[\"sigma\"]**2 / (2 * p[\"kappa\"])) * (1 - np.exp(-2 * p[\"kappa\"] * fg[\"dt\"]))\n        else: # Taylor expansion for small kappa to avoid 0/0\n            var = p[\"sigma\"]**2 * fg[\"dt\"]\n\n        std_dev = np.sqrt(var)\n\n        for i in range(N_fine):\n            Z = rng_fine.standard_normal(fg[\"N_paths\"])\n            mean = p[\"mu\"] + (paths_fine[:, i] - p[\"mu\"]) * exp_k_dt\n            paths_fine[:, i+1] = mean + std_dev * Z\n\n        p_fine = np.mean(np.max(paths_fine, axis=1) >= p[\"b\"])\n        results.append(p_fine)\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3344309"}]}