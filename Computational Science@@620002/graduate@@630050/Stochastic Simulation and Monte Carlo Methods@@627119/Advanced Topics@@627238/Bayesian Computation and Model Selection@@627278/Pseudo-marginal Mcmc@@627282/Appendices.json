{"hands_on_practices": [{"introduction": "The pseudo-marginal Metropolis-Hastings algorithm's core mechanism involves a randomized acceptance probability. A rigorous analysis of the algorithm's efficiency begins with understanding the statistical properties of this probability. This exercise provides a foundational workout by asking you to derive the full distribution and expected value of the acceptance probability, $\\mathbb{E}[\\alpha]$, under the common and analytically tractable assumption of multiplicative log-normal noise. By working through this derivation, you will establish a precise mathematical link between the variance of the likelihood estimator, $\\sigma^2$, and the sampler's behavior, a cornerstone for all advanced PMMCMC theory [@problem_id:3332960].", "problem": "Consider a pseudo-marginal Markov Chain Monte Carlo (MCMC) algorithm that uses an unbiased positive estimator of the likelihood, implemented via multiplicative log-normal noise. Let the Metropolis–Hastings (MH) proposal produce a deterministic exact log-ratio $r \\in \\mathbb{R}$ of the target times proposal reverse-to-forward ratio, and suppose the current and proposed likelihood estimators use independent log-noise variables $\\epsilon$ and $\\epsilon'$ that are identically distributed as $\\mathcal{N}(-\\sigma^{2}/2,\\sigma^{2})$, with $\\sigma^{2}>0$. The pseudo-marginal acceptance probability satisfies\n$$\n\\alpha \\;=\\; \\min\\{1, \\exp(r + \\epsilon' - \\epsilon)\\}.\n$$\nStarting from first principles of the MH algorithm and the pseudo-marginal construction (unbiasedness of the likelihood estimator on the natural scale), derive the distribution of the random acceptance probability $\\alpha$ explicitly, including any point mass and the density on $(0,1)$. Then, compute the exact analytic expression for the expectation $\\mathbb{E}[\\alpha]$ and examine its dependence on $\\sigma^{2}$, characterizing the limiting behavior as $\\sigma^{2} \\to 0$ and as $\\sigma^{2} \\to \\infty$. You may use the standard normal cumulative distribution function (CDF) $\\Phi(\\cdot)$ and probability density function (PDF) $\\varphi(\\cdot)$ wherever appropriate, and you must express all results in closed form.\n\nProvide as your final answer the exact closed-form analytic expression for $\\mathbb{E}[\\alpha]$ in terms of $r$ and $\\sigma$. No numerical rounding is required.", "solution": "The pseudo-marginal construction replaces the exact likelihood $L(\\theta)$ by a positive unbiased estimator $\\widehat{L}(\\theta)$ such that $\\mathbb{E}[\\widehat{L}(\\theta)] = L(\\theta)$. A common device is to write $\\widehat{L}(\\theta) = L(\\theta)\\exp(\\epsilon)$ where $\\epsilon$ is Gaussian with mean $-\\sigma^{2}/2$ and variance $\\sigma^{2}$, ensuring $\\mathbb{E}[\\exp(\\epsilon)] = \\exp(-\\sigma^{2}/2 + \\sigma^{2}/2) = 1$ so that unbiasedness on the natural scale holds.\n\nGiven a proposal, the exact Metropolis–Hastings (MH) acceptance ratio is $\\exp(r)$, where $r$ is the exact log-ratio. In the pseudo-marginal MH, the acceptance ratio is perturbed by the ratio of independent likelihood estimators, so the log-ratio becomes\n$$\nR_{\\log} \\;=\\; r + \\epsilon' - \\epsilon.\n$$\nSince $\\epsilon$ and $\\epsilon'$ are independent and identically distributed as $\\mathcal{N}(-\\sigma^{2}/2,\\sigma^{2})$, their difference\n$$\nZ \\;=\\; \\epsilon' - \\epsilon\n$$\nis Gaussian with mean $0$ and variance $2\\sigma^{2}$, i.e., $Z \\sim \\mathcal{N}(0,2\\sigma^{2})$. The acceptance probability is thus\n$$\n\\alpha \\;=\\; \\min\\{1,\\exp(r+Z)\\}.\n$$\n\nTo derive the distribution of $\\alpha$, we observe that $\\alpha=1$ occurs when $r+Z \\geq 0$, i.e., when $Z \\geq -r$. Otherwise, when $r+Z < 0$, one has $\\alpha = \\exp(r+Z) \\in (0,1)$. Let $v = 2\\sigma^{2}$ for brevity. Then $Z \\sim \\mathcal{N}(0,v)$, and the probability of a point mass at $1$ is\n$$\n\\mathbb{P}(\\alpha = 1) \\;=\\; \\mathbb{P}(Z \\geq -r) \\;=\\; \\Phi\\!\\left(\\frac{r}{\\sqrt{v}}\\right) \\;=\\; \\Phi\\!\\left(\\frac{r}{\\sqrt{2}\\,\\sigma}\\right),\n$$\nwhere $\\Phi$ is the standard normal cumulative distribution function. On the continuous part $(0,1)$, the mapping between $Z$ and $\\alpha$ when $r+Z<0$ is $\\alpha = \\exp(r+Z) \\in (0,1)$, so $Z = \\ln(\\alpha) - r$. The density of $Z$ is\n$$\nf_{Z}(z) \\;=\\; \\frac{1}{\\sqrt{2\\pi v}}\\exp\\!\\left(-\\frac{z^{2}}{2v}\\right) \\;=\\; \\frac{1}{\\sqrt{4\\pi \\sigma^{2}}}\\exp\\!\\left(-\\frac{z^{2}}{4\\sigma^{2}}\\right).\n$$\nBy change of variables, for $\\alpha \\in (0,1)$,\n$$\nf_{\\alpha}(\\alpha) \\;=\\; f_{Z}\\!\\left(\\ln(\\alpha)-r\\right)\\left|\\frac{d}{d\\alpha}\\big(\\ln(\\alpha)-r\\big)\\right|\n\\;=\\; \\frac{1}{\\sqrt{4\\pi \\sigma^{2}}}\\exp\\!\\left(-\\frac{(\\ln(\\alpha)-r)^{2}}{4\\sigma^{2}}\\right)\\cdot \\frac{1}{\\alpha}.\n$$\nTherefore, the acceptance probability $\\alpha$ has a mixed distribution consisting of a point mass at $1$ with probability $\\Phi(r/(\\sqrt{2}\\,\\sigma))$, and a continuous density on $(0,1)$ given by\n$$\nf_{\\alpha}(\\alpha) \\;=\\; \\frac{1}{\\alpha\\,\\sqrt{4\\pi \\sigma^{2}}}\\exp\\!\\left(-\\frac{(\\ln(\\alpha)-r)^{2}}{4\\sigma^{2}}\\right), \\qquad 0<\\alpha<1.\n$$\n\nWe now compute the expectation $\\mathbb{E}[\\alpha]$. Write\n$$\n\\mathbb{E}[\\alpha] \\;=\\; \\mathbb{E}\\big[\\min\\{1,\\exp(r+Z)\\}\\big] \\;=\\; \\mathbb{P}(r+Z \\geq 0) \\;+\\; \\mathbb{E}\\big[\\exp(r+Z)\\,\\mathbf{1}\\{r+Z<0\\}\\big].\n$$\nThe first term is $\\Phi\\!\\left(r/\\sqrt{v}\\right)$. For the second term, note\n$$\n\\mathbb{E}\\big[\\exp(r+Z)\\,\\mathbf{1}\\{Z<-r\\}\\big] \\;=\\; \\exp(r)\\,\\mathbb{E}\\big[\\exp(Z)\\,\\mathbf{1}\\{Z<-r\\}\\big].\n$$\nLet $Z=\\sqrt{v}\\,Y$ where $Y\\sim \\mathcal{N}(0,1)$. Then\n$$\n\\mathbb{E}\\big[\\exp(Z)\\,\\mathbf{1}\\{Z<-r\\}\\big] \\;=\\; \\mathbb{E}\\big[\\exp(\\sqrt{v}\\,Y)\\,\\mathbf{1}\\{Y< -r/\\sqrt{v}\\}\\big].\n$$\nA standard Gaussian identity for truncated exponential moments is\n$$\n\\mathbb{E}\\big[\\exp(tY)\\,\\mathbf{1}\\{Y<x\\}\\big] \\;=\\; \\exp\\!\\left(\\frac{t^{2}}{2}\\right)\\,\\Phi(x-t), \\qquad Y\\sim \\mathcal{N}(0,1).\n$$\nApplying this with $t=\\sqrt{v}$ and $x=-r/\\sqrt{v}$ gives\n$$\n\\mathbb{E}\\big[\\exp(Z)\\,\\mathbf{1}\\{Z<-r\\}\\big] \\;=\\; \\exp\\!\\left(\\frac{v}{2}\\right)\\,\\Phi\\!\\left(-\\frac{r}{\\sqrt{v}} - \\sqrt{v}\\right).\n$$\nHence,\n$$\n\\mathbb{E}[\\alpha] \\;=\\; \\Phi\\!\\left(\\frac{r}{\\sqrt{v}}\\right) \\;+\\; \\exp\\!\\left(r+\\frac{v}{2}\\right)\\,\\Phi\\!\\left(-\\frac{r}{\\sqrt{v}} - \\sqrt{v}\\right).\n$$\nRestoring $v=2\\sigma^{2}$ yields the compact closed form\n$$\n\\mathbb{E}[\\alpha] \\;=\\; \\Phi\\!\\left(\\frac{r}{\\sqrt{2}\\,\\sigma}\\right) \\;+\\; \\exp\\!\\left(r+\\sigma^{2}\\right)\\,\\Phi\\!\\left(\\frac{-r-2\\sigma^{2}}{\\sqrt{2}\\,\\sigma}\\right).\n$$\n\nTo examine the dependence on $\\sigma^{2}$, consider the limiting regimes. As $\\sigma^{2}\\to 0$, one has $Z\\to 0$ almost surely, so $\\alpha \\to \\min\\{1,\\exp(r)\\}$ deterministically. Therefore,\n$$\n\\lim_{\\sigma^{2}\\to 0}\\mathbb{E}[\\alpha] \\;=\\; \\min\\{1,\\exp(r)\\}.\n$$\nAs $\\sigma^{2}\\to \\infty$, we have $\\frac{r}{\\sqrt{2}\\,\\sigma}\\to 0$, so $\\Phi\\!\\left(\\frac{r}{\\sqrt{2}\\,\\sigma}\\right)\\to \\frac{1}{2}$. Meanwhile, with $v=2\\sigma^{2}$,\n$$\n\\exp\\!\\left(r+\\frac{v}{2}\\right)\\,\\Phi\\!\\left(-\\frac{r}{\\sqrt{v}} - \\sqrt{v}\\right) \\;\\sim\\; \\exp\\!\\left(r+\\frac{v}{2}\\right)\\,\\frac{\\varphi(\\sqrt{v})}{\\sqrt{v}}\n\\;=\\; \\exp(r)\\,\\frac{1}{\\sqrt{2\\pi v}} \\;\\to\\; 0,\n$$\nso\n$$\n\\lim_{\\sigma^{2}\\to \\infty}\\mathbb{E}[\\alpha] \\;=\\; \\frac{1}{2}.\n$$\nThus, $\\mathbb{E}[\\alpha]$ continuously moves from $\\min\\{1,\\exp(r)\\}$ at $\\sigma^{2}=0$ to $1/2$ as $\\sigma^{2}\\to\\infty$. Consequently, for $r$ such that $\\exp(r) > 1/2$ (equivalently, $r > -\\ln 2$), the expectation $\\mathbb{E}[\\alpha]$ decreases with increasing $\\sigma^{2}$, whereas for $\\exp(r) < 1/2$ (equivalently, $r < -\\ln 2$), it increases toward $1/2$ as $\\sigma^{2}$ grows. In all cases, the limit as $\\sigma^{2}\\to\\infty$ is $1/2$.", "answer": "$$\\boxed{\\Phi\\!\\left(\\frac{r}{\\sqrt{2}\\,\\sigma}\\right) + \\exp\\!\\left(r+\\sigma^{2}\\right)\\,\\Phi\\!\\left(\\frac{-r-2\\sigma^{2}}{\\sqrt{2}\\,\\sigma}\\right)}$$", "id": "3332960"}, {"introduction": "Building on the relationship between estimator variance and acceptance rate, we now address a critical practical question: how should one choose the number of particles, $m$, for the likelihood estimate? Using a larger $m$ reduces the estimator's variance but linearly increases the computational cost per iteration. This practice challenges you to analyze this trade-off to maximize overall algorithmic efficiency, defined as the expected acceptance rate per unit of computational cost. You will discover a fundamental and somewhat counter-intuitive principle governing the optimal choice of $m$, revealing that simply minimizing estimator variance is often a suboptimal strategy in PMMCMC [@problem_id:3333004].", "problem": "Consider the pseudo-marginal Markov Chain Monte Carlo (MCMC) method that targets a posterior density with unknown likelihood and uses an unbiased estimator of the likelihood at a proposed parameter $ \\theta' $. Suppose a single Metropolis-Hastings (MH) step uses a symmetric proposal so that the Hastings correction cancels, and focus on one such step in which the true log-posterior difference between the proposed and current state is a fixed value $ \\Delta = -1 $. At both the current and proposed points, the likelihood estimator is constructed by averaging $ m $ independent and identically distributed unbiased estimators. Assume the following scientifically standard approximation: by the Central Limit Theorem and the Delta method, the error in the log of the averaged likelihood estimator at any fixed parameter value is approximately Gaussian with mean $ 0 $ and variance $ \\sigma^{2}/m $. Further assume the errors at the current and proposed points are independent, and that $ \\sigma^{2} = 1 $.\n\nUnder these assumptions, the pseudo-marginal acceptance probability for the proposed move can be written as $ \\alpha = \\min\\{1, \\exp(\\Delta + Z)\\} $, where $ Z $ is the difference of two independent Gaussian log-estimation errors from the proposed and current points. Given this setting:\n\n- Derive an exact closed-form expression for the expected acceptance probability $ A(m) = \\mathbb{E}[\\alpha] $ as a function of the integer $ m \\geq 1 $.\n- Define the computational cost of using $ m $ averages to be proportional to $ m $, and define the efficiency per unit cost to be $ \\mathrm{Eff}(m) = A(m)/m $.\n- Determine the integer $ m $ that maximizes $ \\mathrm{Eff}(m) $.\n\nYour final answer must be the single integer that maximizes $ \\mathrm{Eff}(m) $. No rounding is required. Express the final answer as an integer without units.", "solution": "The problem requires finding the integer number of estimators, $m$, that maximizes the efficiency of a pseudo-marginal MCMC step. The efficiency is defined as the expected acceptance probability per unit of computational cost.\n\nFirst, we formalize the components of the problem.\nThe acceptance probability for a single Metropolis-Hastings step is given by $\\alpha = \\min\\{1, \\exp(\\Delta + Z)\\}$, where $\\Delta = -1$ is the true log-posterior difference.\nThe term $Z$ represents the noise in the log-likelihood ratio estimate. It is the difference between two independent and identically distributed (i.i.d.) random variables, each representing the error in the log of an averaged likelihood estimator.\nLet the error at the proposed point be $\\epsilon_{\\text{prop}}$ and at the current point be $\\epsilon_{\\text{curr}}$. The problem states that by the Central Limit Theorem and the Delta method, these errors are approximately Gaussian with mean $0$ and variance $\\frac{\\sigma^2}{m}$.\nGiven $\\sigma^2 = 1$, we have $\\epsilon_{\\text{prop}} \\sim \\mathcal{N}(0, 1/m)$ and $\\epsilon_{\\text{curr}} \\sim \\mathcal{N}(0, 1/m)$.\nThe variable $Z$ is their difference, $Z = \\epsilon_{\\text{prop}} - \\epsilon_{\\text{curr}}$. Since they are independent, $Z$ is also a Gaussian random variable.\nThe mean of $Z$ is $\\mathbb{E}[Z] = \\mathbb{E}[\\epsilon_{\\text{prop}}] - \\mathbb{E}[\\epsilon_{\\text{curr}}] = 0 - 0 = 0$.\nThe variance of $Z$ is $\\mathrm{Var}(Z) = \\mathrm{Var}(\\epsilon_{\\text{prop}}) + \\mathrm{Var}(\\epsilon_{\\text{curr}}) = \\frac{1}{m} + \\frac{1}{m} = \\frac{2}{m}$.\nSo, $Z \\sim \\mathcal{N}(0, 2/m)$. Let us denote the variance of $Z$ as $v_m = 2/m$. The probability density function (PDF) of $Z$ is $f_Z(z) = \\frac{1}{\\sqrt{2\\pi v_m}} \\exp\\left(-\\frac{z^2}{2v_m}\\right)$.\n\nThe first task is to derive the expected acceptance probability, $A(m) = \\mathbb{E}[\\alpha]$.\n$$A(m) = \\mathbb{E}[\\min\\{1, \\exp(\\Delta + Z)\\}] = \\int_{-\\infty}^{\\infty} \\min\\{1, \\exp(\\Delta + z)\\} f_Z(z) dz$$\nThe term $\\min\\{1, \\exp(\\Delta + z)\\}$ depends on the sign of $\\Delta + z$.\n$\\exp(\\Delta + z) \\leq 1$ when $\\Delta + z \\leq 0$, which is $z \\leq -\\Delta$.\n$\\exp(\\Delta + z) > 1$ when $\\Delta + z > 0$, which is $z > -\\Delta$.\nSo we split the integral at $z = -\\Delta$:\n$$A(m) = \\int_{-\\infty}^{-\\Delta} \\exp(\\Delta + z) f_Z(z) dz + \\int_{-\\Delta}^{\\infty} 1 \\cdot f_Z(z) dz$$\nThe second integral is the probability $P(Z \\geq -\\Delta)$. Let $\\Phi(\\cdot)$ be the cumulative distribution function (CDF) of the standard normal distribution $\\mathcal{N}(0, 1)$.\n$$\\int_{-\\Delta}^{\\infty} f_Z(z) dz = P(Z \\geq -\\Delta) = 1 - P(Z < -\\Delta) = 1 - \\Phi\\left(\\frac{-\\Delta}{\\sqrt{v_m}}\\right) = \\Phi\\left(\\frac{\\Delta}{\\sqrt{v_m}}\\right)$$\nThe first integral is:\n$$I_1 = \\int_{-\\infty}^{-\\Delta} \\exp(\\Delta + z) \\frac{1}{\\sqrt{2\\pi v_m}} \\exp\\left(-\\frac{z^2}{2v_m}\\right) dz$$\n$$I_1 = \\frac{\\exp(\\Delta)}{\\sqrt{2\\pi v_m}} \\int_{-\\infty}^{-\\Delta} \\exp\\left(z - \\frac{z^2}{2v_m}\\right) dz$$\nWe complete the square in the exponent:\n$$z - \\frac{z^2}{2v_m} = -\\frac{1}{2v_m}(z^2 - 2v_m z) = -\\frac{1}{2v_m}((z-v_m)^2 - v_m^2) = -\\frac{(z-v_m)^2}{2v_m} + \\frac{v_m}{2}$$\nSubstituting this back into the integral:\n$$I_1 = \\frac{\\exp(\\Delta)}{\\sqrt{2\\pi v_m}} \\int_{-\\infty}^{-\\Delta} \\exp\\left(-\\frac{(z-v_m)^2}{2v_m} + \\frac{v_m}{2}\\right) dz$$\n$$I_1 = \\exp(\\Delta + v_m/2) \\int_{-\\infty}^{-\\Delta} \\frac{1}{\\sqrt{2\\pi v_m}} \\exp\\left(-\\frac{(z-v_m)^2}{2v_m}\\right) dz$$\nThe integrand is the PDF of a normal distribution with mean $v_m$ and variance $v_m$, i.e., $\\mathcal{N}(v_m, v_m)$. The integral is the probability that a random variable $W \\sim \\mathcal{N}(v_m, v_m)$ is less than or equal to $-\\Delta$.\n$$I_1 = \\exp(\\Delta + v_m/2) P(W \\leq -\\Delta) = \\exp(\\Delta + v_m/2) \\Phi\\left(\\frac{-\\Delta - v_m}{\\sqrt{v_m}}\\right)$$\nCombining the two parts, we get the expression for $A(m)$:\n$$A(m) = \\Phi\\left(\\frac{\\Delta}{\\sqrt{v_m}}\\right) + \\exp(\\Delta + v_m/2) \\Phi\\left(\\frac{-\\Delta - v_m}{\\sqrt{v_m}}\\right)$$\nSubstituting the given values $\\Delta = -1$ and $v_m = 2/m$:\n$$A(m) = \\Phi\\left(\\frac{-1}{\\sqrt{2/m}}\\right) + \\exp\\left(-1 + \\frac{1}{m}\\right) \\Phi\\left(\\frac{1 - 2/m}{\\sqrt{2/m}}\\right)$$\n$$A(m) = \\Phi\\left(-\\sqrt{\\frac{m}{2}}\\right) + \\exp\\left(\\frac{1}{m} - 1\\right) \\Phi\\left(\\frac{m-2}{\\sqrt{2m}}\\right)$$\n\nThe computational cost is proportional to $m$, so we can write it as $C(m) = k \\cdot m$ for some constant $k>0$. The efficiency per unit cost is defined as $\\mathrm{Eff}(m) = A(m)/C(m)$. Since we are maximizing this function, the constant $k$ does not affect the optimal value of $m$, so we can set $k=1$ and use the given definition:\n$$\\mathrm{Eff}(m) = \\frac{A(m)}{m}$$\nWe need to find the integer $m \\geq 1$ that maximizes this function. Instead of computing the derivative of this complicated expression, we can analyze the behavior of its components.\n\nLet us analyze the function $A(v) = \\Phi(\\Delta/\\sqrt{v}) + \\exp(\\Delta + v/2) \\Phi((-\\Delta-v)/\\sqrt{v})$, where we treat the variance $v$ as the variable. For the case $\\Delta < 0$, it is a known result in the study of pseudo-marginal methods that the expected acceptance probability $A(v)$ is a strictly increasing function of the variance $v$.\nIn our problem, $v_m = 2/m$. As $m$ increases, $v_m$ decreases. Since $A(v_m)$ is an increasing function of $v_m$, and $v_m$ is a decreasing function of $m$, the composite function $A(m) = A(v_m(m))$ must be a strictly decreasing function of $m$ for $m \\geq 1$.\n\nWe are looking to maximize $\\mathrm{Eff}(m) = \\frac{A(m)}{m}$.\nThis function is a product of two positive, strictly decreasing functions for $m \\in \\{1, 2, 3, \\ldots\\}$:\n1. $A(m)$: As established, this is a strictly decreasing function of $m$.\n2. $1/m$: This is also a strictly decreasing function of $m$.\n\nThe product of two positive, strictly decreasing functions is also a strictly decreasing function. Let $f(m) = A(m)$ and $g(m)=1/m$. Both are positive and decreasing for $m \\ge 1$. Then $f(m+1) < f(m)$ and $g(m+1) < g(m)$. Therefore, their product $\\mathrm{Eff}(m+1) = f(m+1)g(m+1) < f(m)g(m) = \\mathrm{Eff}(m)$.\nThus, $\\mathrm{Eff}(m)$ is a strictly decreasing function of $m$ for $m \\geq 1$.\n\nFor a strictly decreasing function over the set of positive integers, the maximum value must occur at the smallest possible integer in its domain. The problem states we are considering integers $m \\geq 1$.\nTherefore, the maximum efficiency is achieved at $m=1$.\n\nTo confirm our reasoning, we can evaluate the efficiency for the first few integer values of $m$:\nFor $m=1$: $v_1=2$.\n$A(1) = \\Phi(-1/\\sqrt{2}) + \\exp(0) \\Phi((1-2)/\\sqrt{2}) = 2\\Phi(-1/\\sqrt{2})$.\nUsing $\\Phi(-x) = 1-\\Phi(x)$, $A(1) = 2(1-\\Phi(1/\\sqrt{2}))$. Numerically, $1/\\sqrt{2} \\approx 0.707$, and $\\Phi(0.707) \\approx 0.760$. So $A(1) \\approx 2(1 - 0.760) = 0.48$. $\\mathrm{Eff}(1) \\approx 0.48$.\n\nFor $m=2$: $v_2=1$.\n$A(2) = \\Phi(-1) + \\exp(-1/2)\\Phi(0) = \\Phi(-1) + 0.5\\exp(-1/2)$.\nNumerically, $\\Phi(-1) \\approx 0.159$ and $\\exp(-1/2) \\approx 0.607$. So $A(2) \\approx 0.159 + 0.5(0.607) = 0.159 + 0.3035 = 0.4625$.\n$\\mathrm{Eff}(2) = A(2)/2 \\approx 0.4625/2 = 0.23125$.\n\nThe numerical evaluation shows $\\mathrm{Eff}(1) > \\mathrm{Eff}(2)$, which is consistent with our conclusion that $\\mathrm{Eff}(m)$ is a decreasing function of $m$. The integer $m \\geq 1$ that maximizes $\\mathrm{Eff}(m)$ is thus $m=1$.", "answer": "$$\\boxed{1}$$", "id": "3333004"}, {"introduction": "Theoretical insights are most potent when paired with practical implementation. This final exercise serves as a capstone, guiding you through the development and analysis of a PMMCMC sampler for a complete Bayesian model with a heavy-tailed posterior. You will construct an unbiased likelihood estimator for a Student-$t$ model via its latent variable representation and deploy it within a Metropolis-Hastings framework. By coding the algorithm and diagnosing its convergence, you will gain direct experience with the practical challenges and essential tools used in applying PMMCMC to real-world statistical inference [@problem_id:3332969].", "problem": "You are given a one-dimensional Bayesian posterior with intentionally heavy tails. The unknown parameter is denoted by $\\theta \\in \\mathbb{R}$. The prior for $\\theta$ is Cauchy with scale $s_0$, given by the density $p(\\theta) \\propto \\left(1 + (\\theta/s_0)^2\\right)^{-1}$. The likelihood arises from a Student-$t$ observation model with degrees-of-freedom $\\nu$ and known scale $\\sigma$, such that conditional on $\\theta$, the observations $y_1,\\dots,y_n$ are independent and follow $y_i \\sim \\text{Student-}t_\\nu(\\text{location}=\\theta,\\text{scale}=\\sigma)$. For the test suite in this problem, use $n=20$ observations,\n$$\ny = \\left(0.5,-1.2,0.3,2.1,-0.7,1.5,-2.4,0.8,-0.1,3.0,-3.5,0.2,1.1,-0.4,0.0,2.6,-1.8,0.9,-0.9,0.4\\right),\n$$\nand hyperparameters $\\nu=3$, $\\sigma=1$, $s_0=5$.\n\nYou must implement a pseudo-marginal Metropolis–Hastings (MH) Markov chain Monte Carlo algorithm to sample $\\theta$ using an unbiased likelihood estimator constructed from a latent variable representation. Model the Student-$t$ likelihood through the Gaussian-inverse-gamma mixture representation:\n- Introduce latent precision variables $\\lambda_i \\sim \\text{Gamma}(\\nu/2,\\nu/2)$ independently, where the parameterization is shape-rate.\n- Given $\\lambda_i$ and $\\theta$, the conditional likelihood is $y_i \\mid \\lambda_i,\\theta \\sim \\mathcal{N}\\!\\left(\\theta, \\sigma^2/\\lambda_i\\right)$ independently across $i$.\n\nFrom this, for a fixed $\\theta$, define an unbiased estimator of the marginal likelihood $L(\\theta) = \\prod_{i=1}^n \\int \\phi\\!\\left(y_i;\\theta,\\sigma^2/\\lambda_i\\right)\\,\\text{Gamma}(\\lambda_i;\\nu/2,\\nu/2)\\,d\\lambda_i$, where $\\phi(\\cdot;\\mu,\\tau^2)$ denotes the Gaussian density with mean $\\mu$ and variance $\\tau^2$. Use independent importance sampling for each factor with proposal $\\lambda_i \\sim \\text{Gamma}(\\nu/2,\\nu/2)$ to obtain the unbiased factor estimator\n$$\n\\widehat{\\ell}_i(\\theta) \\;=\\; \\frac{1}{K} \\sum_{k=1}^K \\phi\\!\\left(y_i;\\theta,\\sigma^2/\\lambda_{i,k}\\right),\n$$\nand take the product $\\widehat{L}(\\theta) = \\prod_{i=1}^n \\widehat{\\ell}_i(\\theta)$. The pseudo-marginal MH algorithm uses $\\widehat{L}(\\theta)$ in place of $L(\\theta)$ in the acceptance ratio. Use a symmetric Gaussian random-walk proposal $\\theta' \\sim \\mathcal{N}(\\theta,s_{\\text{prop}}^2)$.\n\nFor a fixed random seed, analyze how different initializations $\\theta_0$ affect the empirical time to stationarity. For a given chain, define an empirical target functional $f(\\theta) = \\theta$ and a reference posterior mean $\\mu_{\\text{ref}}$ computed deterministically via numerical quadrature on a finite grid. Specifically, define an unnormalized posterior\n$$\n\\widetilde{\\pi}(\\theta) \\;=\\; \\left(1 + (\\theta/s_0)^2\\right)^{-1} \\prod_{i=1}^n \\left(1 + \\frac{(y_i - \\theta)^2}{\\nu \\sigma^2}\\right)^{-(\\nu+1)/2},\n$$\nand approximate\n$$\n\\mu_{\\text{ref}} \\;\\approx\\; \\frac{\\sum_{j=1}^M \\theta_j \\,\\widetilde{\\pi}(\\theta_j)}{\\sum_{j=1}^M \\widetilde{\\pi}(\\theta_j)},\n$$\nusing a uniform grid $\\{\\theta_j\\}_{j=1}^M$ that spans from $\\theta_{\\min}$ to $\\theta_{\\max}$ with constant spacing $\\Delta \\theta$. Your implementation must use $\\theta_{\\min}=-50$, $\\theta_{\\max}=50$, and a constant grid spacing that yields $M$ equally spaced points. It is acceptable to ignore any multiplicative constants that do not depend on $\\theta$ in the likelihood and prior when computing $\\widetilde{\\pi}(\\theta)$ since they cancel in the ratio.\n\nGiven a pseudo-marginal MH chain $\\{\\theta_t\\}_{t=1}^T$ with proposal standard deviation $s_{\\text{prop}}$, define the running average $m_t = t^{-1}\\sum_{s=1}^t \\theta_s$. For a tolerance $\\delta$ and a stabilization window length $w$, define the empirical time to stationarity as the smallest index $t^\\star$ such that\n$$\n\\max_{s \\in \\{t^\\star, t^\\star+1,\\dots, t^\\star+w-1\\}} \\left| m_s - \\mu_{\\text{ref}} \\right| \\le \\delta.\n$$\nIf no such $t^\\star$ exists within the allotted simulation length, report $T$.\n\nBase your derivation and implementation on the following foundational concepts:\n- The pseudo-marginal method replaces the intractable likelihood by a nonnegative unbiased estimator and preserves the exact posterior as the invariant distribution of the Markov chain.\n- The Gaussian-inverse-gamma representation of the Student-$t$ distribution is valid and widely used to construct unbiased estimators.\n- The Metropolis–Hastings acceptance probability for a symmetric proposal kernel $q(\\theta' \\mid \\theta)$ equals $\\alpha(\\theta,\\theta') = \\min\\{1, \\pi(\\theta') \\widehat{L}(\\theta') / (\\pi(\\theta)\\widehat{L}(\\theta))\\}$, where $\\pi(\\theta)$ denotes the prior, and $\\widehat{L}(\\theta)$ is the unbiased likelihood estimator.\n\nUse the following fixed constants and test suite:\n- Data and hyperparameters:\n  - $y$ is given above.\n  - $\\nu = 3$, $\\sigma = 1$, $s_0 = 5$.\n- Proposal standard deviation: $s_{\\text{prop}} = 0.6$.\n- Random seed: $12345$.\n- Quadrature grid: $\\theta_{\\min} = -50$, $\\theta_{\\max} = 50$, with $M = 10001$ equally spaced points.\n- Pseudo-marginal MH run length: $T = 20000$ iterations.\n- Tolerance and stabilization window: $\\delta = 0.05$, $w = 500$.\n- For each test case, the unbiased likelihood estimator uses $K$ draws per observation as defined above.\n- Test cases (initial value $\\theta_0$ and $K$):\n  1. $\\theta_0 = 0.0$, $K = 5$.\n  2. $\\theta_0 = 20.0$, $K = 5$.\n  3. $\\theta_0 = -20.0$, $K = 5$.\n  4. $\\theta_0 = 20.0$, $K = 1$.\n\nYour program must:\n- Compute $\\mu_{\\text{ref}}$ via the specified grid quadrature using $\\widetilde{\\pi}(\\theta)$ as defined above.\n- For each test case, run the pseudo-marginal MH chain for $T$ iterations with the specified $s_{\\text{prop}}$, $\\theta_0$, and $K$, and compute $t^\\star$ using the rule above. If the rule is never satisfied, return $T$ for that test case.\n- Use only the specified random seed for all Monte Carlo randomness.\n\nFinal output format:\n- Your program should produce a single line containing the results as a comma-separated list enclosed in square brackets. The results must be the four integers $t^\\star$ for the four test cases, ordered as listed above. For example, the output line must look like $[a,b,c,d]$ where $a$, $b$, $c$, and $d$ are integers.", "solution": "The user requires the implementation and analysis of a pseudo-marginal Metropolis-Hastings (PMMH) algorithm for a Bayesian inference problem with a heavy-tailed posterior distribution. The solution involves several distinct, principled steps: calculating a reference posterior mean, implementing the PMMH sampler with a custom unbiased likelihood estimator, and evaluating its convergence.\n\n### 1. Bayesian Model Specification\n\nThe problem is to infer a one-dimensional parameter $\\theta \\in \\mathbb{R}$. The posterior distribution is proportional to the product of the prior and the likelihood, $p(\\theta|y) \\propto p(\\theta) L(\\theta; y)$.\n\n*   **Prior:** The prior on $\\theta$ is a Cauchy distribution with location $0$ and scale $s_0=5$. Its probability density function (PDF) is $p(\\theta) \\propto \\left(1 + (\\theta/s_0)^2\\right)^{-1}$. This is a heavy-tailed distribution, expressing weak prior knowledge about $\\theta$.\n\n*   **Likelihood:** The data $y = \\{y_1, \\dots, y_n\\}$ consists of $n=20$ observations. The likelihood model posits that each observation $y_i$ is drawn independently from a Student-$t$ distribution with $\\nu=3$ degrees of freedom, location $\\theta$, and scale $\\sigma=1$. The likelihood function is $L(\\theta; y) = \\prod_{i=1}^n p(y_i|\\theta)$, where the Student-$t$ PDF is\n    $$\n    p(y_i|\\theta) = C_{\\nu,\\sigma} \\left(1 + \\frac{(y_i - \\theta)^2}{\\nu \\sigma^2}\\right)^{-\\frac{\\nu+1}{2}}\n    $$\n    with $C_{\\nu,\\sigma}$ being a normalization constant independent of $\\theta$.\n\n### 2. Pseudo-Marginal Metropolis-Hastings (PMMH)\n\nThe Student-$t$ likelihood does not have a simple analytical form that is convenient for some MCMC methods. The PMMH algorithm circumvents the need to evaluate the likelihood $L(\\theta)$ directly. Instead, it uses a non-negative, unbiased estimator $\\widehat{L}(\\theta)$ such that $\\mathbb{E}[\\widehat{L}(\\theta)] = L(\\theta)$.\n\nThe algorithm proceeds as follows:\n1.  At iteration $t$, given the current state $(\\theta_{t-1}, \\widehat{L}_{t-1})$, where $\\widehat{L}_{t-1}$ is the likelihood estimate for $\\theta_{t-1}$.\n2.  Propose a new state $\\theta' \\sim q(\\cdot|\\theta_{t-1})$. The problem specifies a symmetric Gaussian random-walk proposal, $\\theta' \\sim \\mathcal{N}(\\theta_{t-1}, s_{\\text{prop}}^2)$.\n3.  Generate a new, independent likelihood estimate $\\widehat{L}'$ for the proposed state $\\theta'$ using the specified estimation procedure.\n4.  Accept the proposal with probability\n    $$\n    \\alpha(\\theta_{t-1}, \\theta') = \\min\\left\\{1, \\frac{p(\\theta') \\widehat{L}'}{p(\\theta_{t-1}) \\widehat{L}_{t-1}}\\right\\}\n    $$\n    where $p(\\cdot)$ is the prior density. If accepted, the new state is $(\\theta_t, \\widehat{L}_t) = (\\theta', \\widehat{L}')$. Otherwise, the chain remains at the current state, so $(\\theta_t, \\widehat{L}_t) = (\\theta_{t-1}, \\widehat{L}_{t-1})$.\n\nCrucially, the random numbers used to generate $\\widehat{L}'$ must be drawn anew at each proposal, but the old estimate $\\widehat{L}_{t-1}$ is reused if the proposal is rejected. This ensures that the invariant distribution of the Markov chain is the true posterior $p(\\theta|y)$. To avoid numerical underflow, all calculations are performed on the logarithmic scale.\n\n### 3. Unbiased Likelihood Estimator\n\nThe estimator $\\widehat{L}(\\theta)$ is constructed based on the Gaussian-inverse-gamma mixture representation of the Student-$t$ distribution. Each $y_i \\sim \\text{Student-}t_\\nu(\\theta, \\sigma)$ can be modeled hierarchically:\n-   An unobserved precision variable $\\lambda_i$ is drawn from a Gamma distribution: $\\lambda_i \\sim \\text{Gamma}(\\nu/2, \\nu/2)$, where the parameterization is (shape, rate).\n-   Conditional on $\\lambda_i$ and $\\theta$, the observation is Gaussian: $y_i \\mid \\lambda_i, \\theta \\sim \\mathcal{N}(\\theta, \\sigma^2/\\lambda_i)$.\n\nThe marginal likelihood $p(y_i|\\theta)$ is an integral over the latent variable $\\lambda_i$:\n$p(y_i|\\theta) = \\int_0^\\infty \\mathcal{N}(y_i; \\theta, \\sigma^2/\\lambda_i) \\, \\text{Gamma}(\\lambda_i; \\nu/2, \\nu/2) \\, d\\lambda_i$.\n\nThis integral is estimated using Monte Carlo integration. The problem specifies using the prior $\\text{Gamma}(\\nu/2, \\nu/2)$ as the importance sampling proposal distribution. For each observation $y_i$, $K$ samples $\\lambda_{i,k}$ are drawn from this Gamma distribution. The estimator for the $i$-th likelihood factor is:\n$$\n\\widehat{\\ell}_i(\\theta) = \\frac{1}{K} \\sum_{k=1}^K \\mathcal{N}(y_i; \\theta, \\sigma^2/\\lambda_{i,k})\n$$\nThis estimator is unbiased for $p(y_i|\\theta)$. The total likelihood estimator is the product of these independent estimators, $\\widehat{L}(\\theta) = \\prod_{i=1}^n \\widehat{\\ell}_i(\\theta)$, which is unbiased for the true likelihood $L(\\theta)$. For numerical stability, we compute the logarithm of this estimator using the log-sum-exp trick:\n$\\log \\widehat{\\ell}_i(\\theta) = -\\log K + \\text{logsumexp}_{k=1}^K(\\log \\mathcal{N}(y_i; \\theta, \\sigma^2/\\lambda_{i,k}))$.\n\n### 4. Reference Mean and Convergence Diagnostic\n\nTo assess the convergence of the MCMC chains, a high-accuracy reference value for the posterior mean of $\\theta$ is required.\n\n*   **Reference Mean ($\\mu_{\\text{ref}}$):** This is computed by numerical quadrature over a fine grid of $\\theta$ values from $\\theta_{\\min}=-50$ to $\\theta_{\\max}=50$ with $M=10001$ points. The unnormalized posterior density $\\widetilde{\\pi}(\\theta) = p(\\theta) L(\\theta)$ is evaluated at each grid point. The mean is then approximated by the weighted average:\n    $$\n    \\mu_{\\text{ref}} \\approx \\frac{\\sum_{j=1}^M \\theta_j \\,\\widetilde{\\pi}(\\theta_j)}{\\sum_{j=1}^M \\widetilde{\\pi}(\\theta_j)}\n    $$\n    Calculation is done in log-space to prevent numerical errors, and the true Student-$t$ likelihood is used.\n\n*   **Time to Stationarity ($t^\\star$):** For a given MCMC chain $\\{\\theta_t\\}_{t=1}^T$, we compute the running averages $m_t = \\frac{1}{t}\\sum_{s=1}^t \\theta_s$. The time to stationarity, $t^\\star$, is defined as the first iteration index such that for a window of length $w=500$, the running average remains within a tolerance $\\delta=0.05$ of the reference mean:\n    $$\n    t^\\star = \\min \\left\\{ t \\in [1, T-w+1] \\;\\middle|\\; \\max_{s \\in \\{t, \\dots, t+w-1\\}} |m_s - \\mu_{\\text{ref}}| \\le \\delta \\right\\}\n    $$\nIf this condition is not met by $T-w+1$, $t^\\star$ is reported as the total run length $T=20000$. This diagnostic measures how quickly the chain's ergodic average converges to the true posterior mean according to this criterion.\n\nThe analysis across different initial values $\\theta_0$ and numbers of particles $K$ demonstrates how PMMH performance is sensitive to initialization (burn-in) and estimator variance. A smaller $K$ leads to a higher variance in $\\log\\widehat{L}(\\theta)$, which can cause the sampler to become \"stuck,\" yielding poor mixing and slow convergence.", "answer": "```python\nimport numpy as np\nfrom scipy import stats, special\n\ndef solve():\n    \"\"\"\n    Implements and analyzes the pseudo-marginal Metropolis-Hastings algorithm.\n    \"\"\"\n    \n    # ------------------- GIVENS -------------------\n    # Data and hyperparameters\n    y = np.array([\n        0.5, -1.2, 0.3, 2.1, -0.7, 1.5, -2.4, 0.8, -0.1, 3.0,\n        -3.5, 0.2, 1.1, -0.4, 0.0, 2.6, -1.8, 0.9, -0.9, 0.4\n    ])\n    n = len(y)\n    nu = 3.0\n    sigma = 1.0\n    s0 = 5.0\n    \n    # MCMC and proposal settings\n    s_prop = 0.6\n    T = 20000\n    \n    # Quadrature grid for reference mean\n    theta_min = -50.0\n    theta_max = 50.0\n    M = 10001\n    \n    # Stationarity diagnostic settings\n    delta = 0.05\n    w = 500\n    \n    # Random seed\n    RANDOM_SEED = 12345\n    rng = np.random.default_rng(RANDOM_SEED)\n\n    # Test cases\n    test_cases = [\n        (0.0, 5),    # theta_0, K\n        (20.0, 5),\n        (-20.0, 5),\n        (20.0, 1),\n    ]\n    \n    # ------------------- HELPER FUNCTIONS -------------------\n\n    def log_cauchy_prior_unnorm(theta, s0_val):\n        \"\"\"Computes the unnormalized log-density of a Cauchy prior.\"\"\"\n        return -np.log(1 + (theta / s0_val)**2)\n\n    def compute_mu_ref():\n        \"\"\"Computes the reference posterior mean via grid quadrature.\"\"\"\n        theta_grid = np.linspace(theta_min, theta_max, M)\n        \n        # Log-prior on the grid\n        log_prior_vals = log_cauchy_prior_unnorm(theta_grid, s0)\n        \n        # Log-likelihood on the grid using broadcasting\n        # y reshaped to (n, 1) and theta_grid to (1, M) broadcasts to (n, M)\n        log_like_matrix = stats.t.logpdf(y[:, None], df=nu, loc=theta_grid, scale=sigma)\n        log_like_vals = np.sum(log_like_matrix, axis=0) # Sum over n\n        \n        # Unnormalized log-posterior\n        log_post_vals = log_prior_vals + log_like_vals\n        \n        # Stabilize by subtracting max before exponentiating\n        log_post_vals -= np.max(log_post_vals)\n        post_vals = np.exp(log_post_vals)\n        \n        # Compute mean via sum approximation\n        numerator = np.sum(theta_grid * post_vals)\n        denominator = np.sum(post_vals)\n        \n        return numerator / denominator\n\n    def estimate_log_likelihood(theta, K_val):\n        \"\"\"Computes the unbiased estimator of the log-likelihood.\"\"\"\n        shape_gamma = nu / 2.0\n        rate_gamma = nu / 2.0\n        scale_gamma = 1.0 / rate_gamma\n\n        # Draw latent variables lambda_ik ~ Gamma(nu/2, nu/2)\n        lambda_samples = rng.gamma(shape_gamma, scale_gamma, size=(n, K_val))\n        \n        # Variances of conditional Gaussians: sigma^2 / lambda_ik\n        variances = sigma**2 / lambda_samples\n        \n        # Log of Gaussian PDF: log(phi(y_i; theta, sigma^2/lambda_ik))\n        # Broadcasting y[:, None] (n,1) with variances (n, K) -> (n, K)\n        log_phi_vals = -0.5 * np.log(2 * np.pi * variances) - (y[:, None] - theta)**2 / (2 * variances)\n        \n        # Use log-sum-exp trick for numerical stability\n        # axis=1 sums over the K samples for each observation i\n        log_sum_exp_vals = special.logsumexp(log_phi_vals, axis=1)\n        log_ell_hat_vals = log_sum_exp_vals - np.log(K_val)\n        \n        # Total log-likelihood estimator is the sum of the logs of factor estimators\n        return np.sum(log_ell_hat_vals)\n\n    def run_pmmh(theta0, K_val):\n        \"\"\"Runs the PMMH sampler.\"\"\"\n        chain = np.zeros(T)\n        \n        # Initial state\n        theta_current = theta0\n        log_L_current = estimate_log_likelihood(theta_current, K_val)\n        \n        for t in range(T):\n            # Propose a new theta\n            theta_proposal = rng.normal(loc=theta_current, scale=s_prop)\n            \n            # Generate a new likelihood estimate for the proposal\n            log_L_proposal = estimate_log_likelihood(theta_proposal, K_val)\n            \n            # Log-priors for current and-proposed theta\n            log_prior_current = log_cauchy_prior_unnorm(theta_current, s0)\n            log_prior_proposal = log_cauchy_prior_unnorm(theta_proposal, s0)\n            \n            # Log of the Metropolis-Hastings acceptance ratio\n            log_alpha = (log_prior_proposal + log_L_proposal) - \\\n                        (log_prior_current + log_L_current)\n            \n            # Accept/reject step\n            if np.log(rng.uniform()) < log_alpha:\n                theta_current = theta_proposal\n                log_L_current = log_L_proposal\n            \n            chain[t] = theta_current\n            \n        return chain\n\n    def compute_t_star(chain, mu_ref_val):\n        \"\"\"Computes the empirical time to stationarity.\"\"\"\n        running_sum = np.cumsum(chain)\n        t_values = np.arange(1, T + 1)\n        running_averages = running_sum / t_values\n        \n        t_star_val = T  # Default if condition is never met\n        \n        # Search for the smallest t_star (1-based index)\n        max_t_candidate = T - w + 1\n        for t_candidate in range(1, max_t_candidate + 1):\n            # Window indices are 0-based\n            start_idx = t_candidate - 1\n            end_idx = t_candidate + w - 1\n            window_slice = running_averages[start_idx:end_idx]\n            \n            max_abs_diff = np.max(np.abs(window_slice - mu_ref_val))\n            \n            if max_abs_diff <= delta:\n                t_star_val = t_candidate\n                break\n                \n        return int(t_star_val)\n\n    # ------------------- MAIN EXECUTION -------------------\n    \n    # 1. Compute reference posterior mean\n    mu_ref = compute_mu_ref()\n    \n    results = []\n    \n    # 2. Run simulation for each test case\n    for theta_0, K in test_cases:\n        # Run the PMMH sampler\n        chain = run_pmmh(theta0=theta_0, K_val=K)\n        \n        # Compute the time to stationarity\n        t_star = compute_t_star(chain, mu_ref)\n        \n        results.append(t_star)\n\n    # 3. Format and print the final output\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3332969"}]}