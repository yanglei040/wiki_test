{"hands_on_practices": [{"introduction": "To build a solid understanding of Annealed Importance Sampling (AIS), we begin with a foundational exercise. This practice explores the core mechanism of AIS in an idealized setting where the Markov chain Monte Carlo (MCMC) transitions mix perfectly at each stage. By working through an analytically tractable case—annealing between two Gaussian distributions—you will verify the fundamental principle that the expected incremental weight is precisely the ratio of the corresponding normalization constants, or partition functions [@problem_id:3288040].", "problem": "Consider the following setting for Annealed Importance Sampling (AIS), formally introduced here as Annealed Importance Sampling (AIS) with a geometric path. Let the target energy be $E(x)=\\lambda x^2$ for $x\\in\\mathbb{R}$ and a fixed constant $\\lambda>0$. Define the base distribution $b(x)$ to be a Gaussian with zero mean and variance $s^2$, that is $b(x)=\\frac{1}{\\sqrt{2\\pi}\\,s}\\exp\\left(-\\frac{x^2}{2s^2}\\right)$, and the target distribution $t(x)$ to be the normalized Gibbs distribution associated with the energy $E(x)$, given by $t(x)=\\sqrt{\\frac{\\lambda}{\\pi}}\\exp(-\\lambda x^2)$. Consider the geometric annealing path of intermediate unnormalized densities $u_\\beta(x)=b(x)^{1-\\beta}t(x)^{\\beta}$ parameterized by an inverse-temperature schedule $\\{\\beta_k\\}_{k=0}^K$ with $0=\\beta_0\\beta_1\\cdots\\beta_K=1$.\n\nStart from the foundational principle of importance sampling and normalization constants: for any unnormalized density $u(x)$ with normalization constant $Z=\\int_\\mathbb{R}u(x)\\,\\mathrm{d}x$, the normalized density is $p(x)=\\frac{u(x)}{Z}$. In AIS with a geometric path and perfect mixing at each stage, the expected incremental weight from stage $k$ to stage $k+1$ is equal to the ratio of normalization constants $\\frac{Z_{\\beta_{k+1}}}{Z_{\\beta_k}}$, where $Z_\\beta=\\int_\\mathbb{R}u_\\beta(x)\\,\\mathrm{d}x$. Your task is to derive, from first principles, the normalization constant $Z_\\beta$ for the given $b(x)$ and $t(x)$, and then implement a program that, given $(\\lambda,s,\\{\\beta_k\\}_{k=0}^K)$, computes the expected incremental weight at each stage, defined as $\\frac{Z_{\\beta_{k+1}}}{Z_{\\beta_k}}$ for $k=0,1,\\dots,K-1$.\n\nThe derivation must begin from the definitions of $b(x)$, $t(x)$, the geometric path $u_\\beta(x)$, and the normalization constant $Z_\\beta$, and use only algebraic manipulation and well-tested facts about Gaussian integrals, avoiding any shortcut formulas not derived in your solution.\n\nYour program must process the following test suite of parameter sets, each provided as $(\\lambda,s,\\text{schedule})$:\n- Test case $1$: $(\\lambda,s,\\{\\beta_k\\})=\\left(0.7,1.5,\\{0.0,0.2,0.5,0.9,1.0\\}\\right)$.\n- Test case $2$ (base equals target boundary): $(\\lambda,s,\\{\\beta_k\\})=\\left(0.5,1.0,\\{0.0,0.3,0.6,0.9,1.0\\}\\right)$.\n- Test case $3$ (small energy scale and wide base): $(\\lambda,s,\\{\\beta_k\\})=\\left(10^{-6},100.0,\\{0.0,0.01,0.1,0.5,1.0\\}\\right)$.\n- Test case $4$ (sharp target and narrow base): $(\\lambda,s,\\{\\beta_k\\})=\\left(10.0,0.1,\\{0.0,0.4,0.8,1.0\\}\\right)$.\n\nFor each test case, compute the list $\\left[\\frac{Z_{\\beta_1}}{Z_{\\beta_0}},\\frac{Z_{\\beta_2}}{Z_{\\beta_1}},\\dots,\\frac{Z_{\\beta_K}}{Z_{\\beta_{K-1}}}\\right]$ of expected incremental weights. There are no physical units in this problem. Your program should produce a single line of output containing the results as a comma-separated list of lists enclosed in square brackets, with no spaces, where each inner list corresponds to one test case in the order listed above, for example $[[w_{1,1},w_{1,2}], [w_{2,1},\\dots]]$ but without spaces: $[[w_{1,1},w_{1,2}],[w_{2,1},\\dots]]$. Each $w$ must be a floating-point number in decimal representation. Your implementation must be a complete, runnable program.", "solution": "The problem is valid as it is scientifically grounded, well-posed, objective, and internally consistent. It presents a standard theoretical exercise in the field of computational statistics, specifically concerning Annealed Importance Sampling (AIS). All provided definitions and parameters are standard and sufficient for deriving a unique solution.\n\nThe task is to derive the normalization constant $Z_\\beta$ for an intermediate distribution in an AIS scheme and then use it to compute the expected incremental weights. The derivation proceeds from first principles as stipulated.\n\nLet the base distribution be a zero-mean Gaussian with variance $s^2$, with its probability density function (PDF) given by:\n$$\nb(x) = \\frac{1}{\\sqrt{2\\pi}s} \\exp\\left(-\\frac{x^2}{2s^2}\\right)\n$$\nThe target distribution is the normalized Gibbs distribution for the energy $E(x) = \\lambda x^2$ with $\\lambda  0$. The unnormalized Gibbs distribution is $\\exp(-E(x)) = \\exp(-\\lambda x^2)$. To normalize this, we compute the integral $\\int_{-\\infty}^{\\infty} \\exp(-\\lambda x^2) \\mathrm{d}x$. This is a standard Gaussian integral of the form $\\int_{-\\infty}^{\\infty} \\exp(-ax^2) \\mathrm{d}x = \\sqrt{\\pi/a}$, which for $a=\\lambda$ yields $\\sqrt{\\pi/\\lambda}$. Thus, the normalized target PDF is:\n$$\nt(x) = \\frac{\\exp(-\\lambda x^2)}{\\sqrt{\\pi/\\lambda}} = \\sqrt{\\frac{\\lambda}{\\pi}} \\exp(-\\lambda x^2)\n$$\nThis matches the provided expression for $t(x)$.\n\nThe geometric annealing path defines a sequence of unnormalized intermediate densities $u_\\beta(x)$ for a schedule $0 = \\beta_0  \\beta_1  \\dots  \\beta_K = 1$:\n$$\nu_\\beta(x) = b(x)^{1-\\beta} t(x)^\\beta\n$$\nOur first step is to substitute the expressions for $b(x)$ and $t(x)$ into this definition:\n$$\nu_\\beta(x) = \\left[ \\frac{1}{\\sqrt{2\\pi}s} \\exp\\left(-\\frac{x^2}{2s^2}\\right) \\right]^{1-\\beta} \\left[ \\sqrt{\\frac{\\lambda}{\\pi}} \\exp(-\\lambda x^2) \\right]^{\\beta}\n$$\nWe can separate the terms that are constant with respect to $x$ from the exponential terms involving $x$:\n$$\nu_\\beta(x) = \\left( (\\sqrt{2\\pi}s)^{-1} \\right)^{1-\\beta} \\left( (\\pi/\\lambda)^{-1/2} \\right)^{\\beta} \\times \\exp\\left(-\\frac{x^2}{2s^2}\\right)^{1-\\beta} \\exp(-\\lambda x^2)^{\\beta}\n$$\nLet's simplify the constant pre-factor and the exponent separately.\nThe constant part is:\n$$\nC_\\beta = (2\\pi s^2)^{-(1-\\beta)/2} (\\pi/\\lambda)^{-\\beta/2}\n$$\nThe exponential part is:\n$$\n\\exp\\left( -(1-\\beta)\\frac{x^2}{2s^2} - \\beta\\lambda x^2 \\right) = \\exp\\left( -x^2 \\left[ \\frac{1-\\beta}{2s^2} + \\beta\\lambda \\right] \\right)\n$$\nSo, the unnormalized density $u_\\beta(x)$ is an unnormalized Gaussian distribution of the form $u_\\beta(x) = C_\\beta \\exp(-A_\\beta x^2)$, where the coefficient $A_\\beta$ in the exponent is:\n$$\nA_\\beta = \\frac{1-\\beta}{2s^2} + \\beta\\lambda\n$$\nThe normalization constant $Z_\\beta$ is defined as the integral of $u_\\beta(x)$ over its domain $\\mathbb{R}$:\n$$\nZ_\\beta = \\int_{-\\infty}^{\\infty} u_\\beta(x) \\, \\mathrm{d}x = \\int_{-\\infty}^{\\infty} C_\\beta \\exp(-A_\\beta x^2) \\, \\mathrm{d}x = C_\\beta \\int_{-\\infty}^{\\infty} \\exp(-A_\\beta x^2) \\, \\mathrm{d}x\n$$\nSince $\\lambda  0$, $s^2  0$, and $\\beta \\in [0, 1]$, the coefficient $A_\\beta$ is always positive. This allows us to use the aforementioned Gaussian integral formula $\\int_{-\\infty}^{\\infty} e^{-ax^2} \\mathrm{d}x = \\sqrt{\\pi/a}$ with $a = A_\\beta$:\n$$\nZ_\\beta = C_\\beta \\sqrt{\\frac{\\pi}{A_\\beta}}\n$$\nSubstituting the expression for $C_\\beta$:\n$$\nZ_\\beta = (2\\pi s^2)^{-(1-\\beta)/2} (\\pi/\\lambda)^{-\\beta/2} \\pi^{1/2} A_\\beta^{-1/2}\n$$\nLet's simplify the powers of the constants:\n$$\nZ_\\beta = 2^{-(1-\\beta)/2} \\pi^{-(1-\\beta)/2} s^{-(1-\\beta)} \\pi^{-\\beta/2} \\lambda^{\\beta/2} \\pi^{1/2} A_\\beta^{-1/2}\n$$\n$$\nZ_\\beta = 2^{-(1-\\beta)/2} s^{-(1-\\beta)} \\lambda^{\\beta/2} \\pi^{(-1+\\beta-\\beta+1)/2} A_\\beta^{-1/2}\n$$\nThe exponent of $\\pi$ simplifies to $0$, so $\\pi^0=1$:\n$$\nZ_\\beta = 2^{-(1-\\beta)/2} s^{-(1-\\beta)} \\lambda^{\\beta/2} A_\\beta^{-1/2}\n$$\nNow, substituting the expression for $A_\\beta = \\frac{1-\\beta}{2s^2} + \\beta\\lambda = \\frac{1-\\beta + 2s^2\\beta\\lambda}{2s^2}$:\n$$\nZ_\\beta = 2^{-(1-\\beta)/2} s^{-(1-\\beta)} \\lambda^{\\beta/2} \\left( \\frac{1-\\beta + 2s^2\\beta\\lambda}{2s^2} \\right)^{-1/2}\n$$\n$$\nZ_\\beta = 2^{-(1-\\beta)/2} s^{-(1-\\beta)} \\lambda^{\\beta/2} \\left( \\frac{2s^2}{1-\\beta + 2s^2\\beta\\lambda} \\right)^{1/2}\n$$\n$$\nZ_\\beta = 2^{-1/2+\\beta/2} s^{-1+\\beta} \\lambda^{\\beta/2} \\cdot 2^{1/2} s^1 \\cdot (1-\\beta + 2s^2\\beta\\lambda)^{-1/2}\n$$\nCombining terms with the same base:\n$$\nZ_\\beta = 2^{\\beta/2} s^\\beta \\lambda^{\\beta/2} (1-\\beta + 2s^2\\beta\\lambda)^{-1/2}\n$$\n$$\nZ_\\beta = (2s^2\\lambda)^{\\beta/2} (1 - \\beta + (2s^2\\lambda)\\beta)^{-1/2}\n$$\nLet's define a constant $\\alpha = 2s^2\\lambda$. The expression for $Z_\\beta$ becomes markedly simpler:\n$$\nZ_\\beta = \\alpha^{\\beta/2} (1 - \\beta + \\alpha\\beta)^{-1/2} = \\sqrt{\\frac{\\alpha^\\beta}{1 + (\\alpha-1)\\beta}}\n$$\nWe verify this formula for the boundary cases. For $\\beta=0$, $u_0(x) = b(x)$, which is normalized, so $Z_0=1$. Our formula gives $Z_0 = \\sqrt{\\frac{\\alpha^0}{1+(\\alpha-1)0}} = \\sqrt{\\frac{1}{1}} = 1$. For $\\beta=1$, $u_1(x) = t(x)$, which is also normalized, so $Z_1=1$. Our formula gives $Z_1 = \\sqrt{\\frac{\\alpha^1}{1+(\\alpha-1)1}} = \\sqrt{\\frac{\\alpha}{1+\\alpha-1}} = \\sqrt{\\frac{\\alpha}{\\alpha}} = 1$. The formula is correct.\n\nThe expected incremental weight from stage $k$ to stage $k+1$ is the ratio of normalization constants $W_k = \\frac{Z_{\\beta_{k+1}}}{Z_{\\beta_k}}$. Using our derived expression for $Z_\\beta$:\n$$\nW_k = \\frac{\\sqrt{\\frac{\\alpha^{\\beta_{k+1}}}{1 + (\\alpha-1)\\beta_{k+1}}}}{\\sqrt{\\frac{\\alpha^{\\beta_k}}{1 + (\\alpha-1)\\beta_k}}} = \\sqrt{\\frac{\\alpha^{\\beta_{k+1}}}{1 + (\\alpha-1)\\beta_{k+1}} \\cdot \\frac{1 + (\\alpha-1)\\beta_k}{\\alpha^{\\beta_k}}}\n$$\n$$\nW_k = \\sqrt{\\alpha^{\\beta_{k+1}-\\beta_k} \\frac{1 + (\\alpha-1)\\beta_k}{1 + (\\alpha-1)\\beta_{k+1}}}\n$$\nThis is the final expression to be implemented. The term $\\alpha = 2s^2\\lambda$ is computed once per test case. For each step in the annealing schedule from $\\beta_k$ to $\\beta_{k+1}$, the weight $W_k$ is calculated using this formula.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the expected incremental weights for Annealed Importance Sampling (AIS)\n    with a geometric path between two Gaussian distributions.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (lambda, s, schedule)\n        (0.7, 1.5, [0.0, 0.2, 0.5, 0.9, 1.0]),\n        (0.5, 1.0, [0.0, 0.3, 0.6, 0.9, 1.0]),\n        (1e-6, 100.0, [0.0, 0.01, 0.1, 0.5, 1.0]),\n        (10.0, 0.1, [0.0, 0.4, 0.8, 1.0]),\n    ]\n\n    all_results = []\n    for case in test_cases:\n        lam, s, schedule = case\n        \n        # The derived formula depends on the constant alpha = 2 * s^2 * lambda.\n        alpha = 2.0 * s**2 * lam\n        \n        case_weights = []\n        # Iterate through the annealing schedule to compute each incremental weight.\n        for i in range(len(schedule) - 1):\n            beta_k = schedule[i]\n            beta_k_plus_1 = schedule[i+1]\n            \n            # The incremental weight from beta_k to beta_k_plus_1 is Z_{k+1}/Z_k.\n            # Handle the special case where alpha is 1, which means the base and\n            # target distributions are identical, so all weights are 1.\n            if np.isclose(alpha, 1.0):\n                weight = 1.0\n            else:\n                # Derived formula:\n                # W_k = sqrt(alpha^(beta_{k+1}-beta_k) * (1+(alpha-1)beta_k) / (1+(alpha-1)beta_{k+1}))\n                beta_diff = beta_k_plus_1 - beta_k\n                term1 = alpha**beta_diff\n                \n                numerator = 1.0 + (alpha - 1.0) * beta_k\n                denominator = 1.0 + (alpha - 1.0) * beta_k_plus_1\n                \n                # As derived, with alpha  0 and beta in [0, 1], the denominator is always positive.\n                term2 = numerator / denominator\n                \n                weight = np.sqrt(term1 * term2)\n            \n            case_weights.append(weight)\n        \n        all_results.append(case_weights)\n\n    # Final print statement in the exact required format:\n    # A single line, comma-separated list of lists, with no spaces.\n    # Example: [[w1,w2],[w3,w4,w5]]\n    inner_lists_str = [f\"[{','.join(map(str, res))}]\" for res in all_results]\n    final_output = f\"[{','.join(inner_lists_str)}]\"\n    \n    print(final_output)\n\nsolve()\n```", "id": "3288040"}, {"introduction": "Standard implementations of AIS often use a 'geometric' annealing path, which is simple but not always robust. This practice exposes a critical failure mode that occurs when bridging from a distribution with light tails (e.g., a Gaussian) to one with heavy tails, a common scenario in statistics and physics. You will mathematically analyze why this mismatch leads to catastrophic weight degeneracy and then explore a more robust 'tail-aware' polynomial path, learning how to design annealing strategies that respect the geometry of the problem space [@problem_id:3288124].", "problem": "You are to investigate the failure modes of Annealed Importance Sampling (AIS) when bridging from an exponentially light-tailed base distribution to a heavy-tailed target, and to design a tail-aware alternative. All mathematical definitions below use the standard setup of AIS with unnormalized densities.\n\nLet $d \\in \\mathbb{N}$ denote the dimension. Consider a base density $p_0(x) \\propto f_0(x)$ on $\\mathbb{R}^d$ and a target $p_1(x) \\propto f_1(x)$. AIS constructs a sequence of intermediate unnormalized densities $\\{f_k\\}_{k=0}^K$ where $f_0=f_0$ and $f_K=f_1$, with corresponding normalized densities $p_k(x) \\propto f_k(x)$. For a schedule $\\{\\beta_k\\}_{k=0}^K$ with $\\beta_0=0$, $\\beta_K=1$, $\\beta_k$ increasing, the standard geometric annealing is $f_k(x) = f_0(x)^{1-\\beta_k} f_1(x)^{\\beta_k}$. The AIS incremental weight from $k$ to $k+1$ is $r_{k \\to k+1}(x_k) = f_{k+1}(x_k)/f_k(x_k)$. Catastrophic weight degeneracy occurs when the second moment $\\mathbb{E}_{p_k}[r_{k \\to k+1}(X)^2]$ is infinite for some step $k$, causing the variance of importance weights to be infinite and rendering effective sample size arbitrarily close to zero in the limit.\n\nIn this problem, you will study the case of a heavy-tailed target\n$$\np_1(x) \\propto f_1(x) \\propto \\left(1 + \\lVert x \\rVert^2 \\right)^{-\\nu},\n$$\nwith parameter $\\nu > d/2$ to ensure normalizability. You will compare two AIS paths:\n\n- Standard geometric annealing from a Gaussian base:\n  $$\n  f_0(x) \\propto \\exp\\!\\left(-\\frac{\\lVert x \\rVert^2}{2}\\right), \\quad f_k(x) = f_0(x)^{1-\\beta_k} f_1(x)^{\\beta_k}, \\quad \\beta_k = \\frac{k}{K}.\n  $$\n- A tail-aware polynomial path that replaces the base with a heavy-tailed Student-like density:\n  $$\n  q_\\kappa(x) \\propto \\left(1 + \\lVert x \\rVert^2 \\right)^{-\\kappa}, \\quad \\kappa > d/2, \\quad \\tilde f_k(x) = q_\\kappa(x)^{1-\\beta_k} f_1(x)^{\\beta_k}, \\quad \\beta_k = \\frac{k}{K}.\n  $$\n\nYour tasks are purely mathematical and algorithmic:\n- Starting from the fundamental definitions of importance sampling moments, derive tail conditions for the finiteness of the second moment of the incremental weight at the final step $k=K-1$ for each path. For standard geometric annealing, show how the exponential factor causes divergence. For the tail-aware polynomial path, derive a condition involving $d$, $\\nu$, $\\kappa$, and $\\beta_{K-1}$ for finiteness.\n- Use these conditions to write a program that, for each specified test case, computes:\n  1. A boolean indicating whether the second moment of the final incremental weight under the standard geometric path is infinite.\n  2. A boolean indicating whether the second moment of the final incremental weight under the tail-aware polynomial path is infinite.\n  3. A nonnegative real-valued \"exponential divergence coefficient\" for the standard geometric path at the final step, defined as\n     $$\n     \\theta_{\\mathrm{geo}} := \\frac{a(\\beta_{K-1})}{2} - a(\\beta_{K}), \\quad a(\\beta) := 1 - \\beta,\n     $$\n     where $a(\\beta)$ parameterizes the Gaussian factor $\\exp(-a(\\beta)\\lVert x \\rVert^2/2)$ in the geometric path. Positive $\\theta_{\\mathrm{geo}}$ implies exponential explosion of the integrand and infinite second moment.\n  4. A real-valued \"polynomial integrability margin\" for the tail-aware path at the final step, defined as\n     $$\n     \\rho_{\\mathrm{poly}} := \\bigl(2 m(\\beta_K) - m(\\beta_{K-1})\\bigr) - \\frac{d}{2}, \\quad m(\\beta) := \\nu \\beta + \\kappa (1 - \\beta).\n     $$\n     A positive margin implies the integral of the squared incremental weight is finite; a nonpositive value signals divergence or borderline divergence.\n\nImplementation details:\n- Use the linear schedule $\\beta_k = k/K$ for a given integer $K \\ge 2$.\n- Use the following test suite, each specified as a tuple $(d,\\nu,\\kappa,K)$:\n  - Test case $1$: $(2, 1.1, 1.5, 10)$.\n  - Test case $2$: $(10, 6.0, 6.0, 20)$.\n  - Test case $3$: $(50, 26.0, 26.0, 30)$.\n- For each test case, compute and return a list with four entries in the order described above: $[\\text{geo\\_inf}, \\text{poly\\_inf}, \\theta_{\\mathrm{geo}}, \\rho_{\\mathrm{poly}}]$, where $\\text{geo\\_inf}$ and $\\text{poly\\_inf}$ are booleans and $\\theta_{\\mathrm{geo}}$, $\\rho_{\\mathrm{poly}}$ are floats.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list of the three per-test-case lists, enclosed in square brackets, for example\n$$\n[[\\text{geo\\_inf}_1, \\text{poly\\_inf}_1, \\theta_{\\mathrm{geo},1}, \\rho_{\\mathrm{poly},1}],[\\text{geo\\_inf}_2, \\text{poly\\_inf}_2, \\theta_{\\mathrm{geo},2}, \\rho_{\\mathrm{poly},2}],[\\text{geo\\_inf}_3, \\text{poly\\_inf}_3, \\theta_{\\mathrm{geo},3}, \\rho_{\\mathrm{poly},3}]].\n$$\nNo additional text should be printed. There are no physical units involved. All angles, if any, should be in radians. All numbers must be printed in standard decimal notation inherent to Python's default string conversion for floats and booleans.", "solution": "The problem requires an analysis of the failure modes of Annealed Importance Sampling (AIS) by examining the second moment of the final incremental importance weight. The finiteness of this moment is a crucial diagnostic for the stability and efficiency of the AIS estimator. We will analyze two annealing paths: the standard geometric path from a light-tailed Gaussian to a heavy-tailed target, and a tail-aware path from one heavy-tailed distribution to another.\n\nThe central quantity of interest is the second moment of the incremental weight update from an intermediate distribution $p_k$ to $p_{k+1}$, given by $\\mathbb{E}_{p_k}[r_{k \\to k+1}(X)^2]$. The incremental weight is defined as $r_{k \\to k+1}(x) = f_{k+1}(x)/f_k(x)$, where $p_j(x) \\propto f_j(x)$ for any $j$. The expectation is taken with respect to the normalized density $p_k(x) = f_k(x) / Z_k$, where $Z_k = \\int f_k(x) dx$.\n\nThe expression for the second moment is:\n$$\n\\mathbb{E}_{p_k}[r_{k \\to k+1}(X)^2] = \\int p_k(x) \\left(\\frac{f_{k+1}(x)}{f_k(x)}\\right)^2 dx = \\int \\frac{f_k(x)}{Z_k} \\frac{f_{k+1}(x)^2}{f_k(x)^2} dx = \\frac{1}{Z_k} \\int \\frac{f_{k+1}(x)^2}{f_k(x)} dx\n$$\nThe second moment is finite if and only if the integral $\\int \\frac{f_{k+1}(x)^2}{f_k(x)} dx$ is finite, as $Z_k$ is a finite normalization constant (assuming $p_k$ is a proper distribution). We are specifically interested in the final step of the annealing schedule, from $k=K-1$ to $k=K$. The integral to analyze is thus $\\int \\frac{f_K(x)^2}{f_{K-1}(x)} dx$.\n\nThe annealing schedule is given by $\\{\\beta_k\\}_{k=0}^K$ with $\\beta_0=0$ and $\\beta_K=1$. The intermediate densities are constructed as $f_k \\propto f_{\\text{base}}^{1-\\beta_k} f_{\\text{target}}^{\\beta_k}$. For the final step, $f_K(x) \\propto f_{\\text{base}}^{1-\\beta_K} f_{\\text{target}}^{\\beta_K} = f_{\\text{base}}^{0} f_{\\text{target}}^{1} = f_1(x)$. Therefore, the integral controlling the second moment simplifies to:\n$$\nI = \\int \\frac{f_1(x)^2}{f_{K-1}(x)} dx\n$$\n\nWe will now analyze this integral for the two specified paths.\n\n### Standard Geometric Annealing Path\nFor this path, the base and target unnormalized densities are:\n- Base: $f_0(x) \\propto \\exp(-\\frac{\\lVert x \\rVert^2}{2})$\n- Target: $f_1(x) \\propto (1 + \\lVert x \\rVert^2)^{-\\nu}$\n\nThe intermediate density at step $k=K-1$ is $f_{K-1}(x) = f_0(x)^{1-\\beta_{K-1}} f_1(x)^{\\beta_{K-1}}$.\nSubstituting this into the integral $I$:\n$$\nI_{\\text{geo}} = \\int \\frac{f_1(x)^2}{f_0(x)^{1-\\beta_{K-1}} f_1(x)^{\\beta_{K-1}}} dx = \\int f_0(x)^{-(1-\\beta_{K-1})} f_1(x)^{2-\\beta_{K-1}} dx\n$$\nNow, substituting the functional forms for $f_0$ and $f_1$:\n$$\nI_{\\text{geo}} \\propto \\int \\left( \\exp\\left(-\\frac{\\lVert x \\rVert^2}{2}\\right) \\right)^{-(1-\\beta_{K-1})} \\left( (1 + \\lVert x \\rVert^2)^{-\\nu} \\right)^{2-\\beta_{K-1}} dx\n$$\n$$\nI_{\\text{geo}} \\propto \\int \\exp\\left(\\frac{1-\\beta_{K-1}}{2} \\lVert x \\rVert^2\\right) (1 + \\lVert x \\rVert^2)^{-\\nu(2-\\beta_{K-1})} dx\n$$\nFor the integral to converge, the integrand must decay to zero sufficiently fast as $\\lVert x \\rVert \\to \\infty$. However, since $\\beta_k$ is an increasing schedule with $\\beta_K=1$, we have $\\beta_{K-1}  1$. This implies that the coefficient $1-\\beta_{K-1}$ in the exponential term is strictly positive. The term $\\exp(\\frac{1-\\beta_{K-1}}{2} \\lVert x \\rVert^2)$ grows exponentially as $\\lVert x \\rVert \\to \\infty$. This exponential growth will always dominate the polynomial decay of $(1 + \\lVert x \\rVert^2)^{-\\nu(2-\\beta_{K-1})}$. The integrand diverges, and thus the integral $I_{\\text{geo}}$ is infinite.\nThis demonstrates a catastrophic failure of standard AIS when bridging from a light-tailed (Gaussian) base to a heavy-tailed (Student-like) target. The second moment of the final incremental weight is always infinite. Thus, the boolean `geo_inf` is always `True`.\n\nThe \"exponential divergence coefficient\" is defined as $\\theta_{\\mathrm{geo}} := \\frac{a(\\beta_{K-1})}{2} - a(\\beta_{K})$. With $a(\\beta) = 1-\\beta$, we have $a(\\beta_{K-1})=1-\\beta_{K-1}$ and $a(\\beta_K)=a(1)=0$. So, $\\theta_{\\mathrm{geo}} = \\frac{1-\\beta_{K-1}}{2}$. This is precisely the positive coefficient of $\\lVert x \\rVert^2$ in the diverging exponential term, confirming its role in causing the integral to explode. For a linear schedule $\\beta_k = k/K$, we have $\\beta_{K-1}=(K-1)/K$, so $\\theta_{\\mathrm{geo}} = \\frac{1-(K-1)/K}{2} = \\frac{1/K}{2} = \\frac{1}{2K}$.\n\n### Tail-Aware Polynomial Path\nFor this path, the base and target unnormalized densities are:\n- Base: $q_\\kappa(x) \\propto (1 + \\lVert x \\rVert^2)^{-\\kappa}$\n- Target: $f_1(x) \\propto (1 + \\lVert x \\rVert^2)^{-\\nu}$\n\nThe intermediate density is $\\tilde f_{K-1}(x) = q_\\kappa(x)^{1-\\beta_{K-1}} f_1(x)^{\\beta_{K-1}}$.\nThe integral to analyze is:\n$$\nI_{\\text{poly}} = \\int \\frac{f_1(x)^2}{\\tilde f_{K-1}(x)} dx = \\int q_\\kappa(x)^{-(1-\\beta_{K-1})} f_1(x)^{2-\\beta_{K-1}} dx\n$$\nSubstituting the functional forms:\n$$\nI_{\\text{poly}} \\propto \\int \\left( (1 + \\lVert x \\rVert^2)^{-\\kappa} \\right)^{-(1-\\beta_{K-1})} \\left( (1 + \\lVert x \\rVert^2)^{-\\nu} \\right)^{2-\\beta_{K-1}} dx\n$$\n$$\nI_{\\text{poly}} \\propto \\int (1 + \\lVert x \\rVert^2)^{\\kappa(1-\\beta_{K-1})} (1 + \\lVert x \\rVert^2)^{-\\nu(2-\\beta_{K-1})} dx\n$$\n$$\nI_{\\text{poly}} \\propto \\int (1 + \\lVert x \\rVert^2)^{-\\left[\\nu(2-\\beta_{K-1}) - \\kappa(1-\\beta_{K-1})\\right]} dx\n$$\nThis is an integral of the form $\\int (1+r^2)^{-p} d^d x$ over $\\mathbb{R}^d$, where $r = \\lVert x \\rVert$. Using hyperspherical coordinates, the integral is finite if and only if the integrand decays sufficiently fast. The asymptotic behavior for large $r$ is $r^{-2p}$. The integral over $\\mathbb{R}^d$ behaves as $\\int^\\infty r^{-2p} r^{d-1} dr = \\int^\\infty r^{d-1-2p} dr$. This integral converges if and only if the exponent is less than $-1$, i.e., $d-1-2p  -1$, which simplifies to $2p  d$.\n\nThe exponent in our case is $p = \\nu(2-\\beta_{K-1}) - \\kappa(1-\\beta_{K-1})$. So, the condition for a finite second moment is:\n$$\n2\\left[\\nu(2-\\beta_{K-1}) - \\kappa(1-\\beta_{K-1})\\right]  d\n$$\nLet's relate this to the \"polynomial integrability margin\" $\\rho_{\\mathrm{poly}} := (2 m(\\beta_K) - m(\\beta_{K-1})) - d/2$, where $m(\\beta) := \\nu \\beta + \\kappa (1 - \\beta)$.\nLet's expand the term $2 m(\\beta_K) - m(\\beta_{K-1})$:\n- $m(\\beta_K) = m(1) = \\nu(1) + \\kappa(1-1) = \\nu$.\n- $m(\\beta_{K-1}) = \\nu \\beta_{K-1} + \\kappa (1 - \\beta_{K-1})$.\n- $2 m(\\beta_K) - m(\\beta_{K-1}) = 2\\nu - (\\nu \\beta_{K-1} + \\kappa(1-\\beta_{K-1})) = 2\\nu - \\nu\\beta_{K-1} - \\kappa + \\kappa\\beta_{K-1} = \\nu(2-\\beta_{K-1}) - \\kappa(1-\\beta_{K-1})$.\nThis is exactly the exponent $p$ from our derivation.\n\nThe convergence condition $2p  d$ can be rewritten as $p  d/2$. Substituting $p = 2 m(\\beta_K) - m(\\beta_{K-1})$, the condition becomes:\n$$\n(2 m(\\beta_K) - m(\\beta_{K-1}))  \\frac{d}{2} \\iff (2 m(\\beta_K) - m(\\beta_{K-1})) - \\frac{d}{2}  0\n$$\nThis is precisely the condition $\\rho_{\\mathrm{poly}}  0$. Therefore, the second moment of the incremental weight is finite if and only if the polynomial integrability margin is positive. Divergence occurs if $\\rho_{\\mathrm{poly}} \\le 0$. The boolean `poly_inf` is `True` if $\\rho_{\\mathrm{poly}} \\le 0$ and `False` otherwise.\n\nWith these derivations, we can now implement a program to compute the required values for the given test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes AIS failure diagnostics for given test cases.\n\n    For each test case (d, nu, kappa, K), this function calculates four values:\n    1. geo_inf: Boolean, True if the second moment of the final incremental weight\n                under the standard geometric path is infinite.\n    2. poly_inf: Boolean, True if the second moment of the final incremental weight\n                 under the tail-aware polynomial path is infinite.\n    3. theta_geo: The exponential divergence coefficient for the geometric path.\n    4. rho_poly: The polynomial integrability margin for the polynomial path.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each case is a tuple (d, nu, kappa, K).\n    test_cases = [\n        (2, 1.1, 1.5, 10),\n        (10, 6.0, 6.0, 20),\n        (50, 26.0, 26.0, 30),\n    ]\n\n    results = []\n    for case in test_cases:\n        d, nu, kappa, K = case\n\n        # --- Standard Geometric Path Analysis ---\n        \n        # As derived, bridging a light-tailed exponential base to a heavy-tailed\n        # polynomial target always results in an infinite second moment for the\n        # final importance weight. The integrand contains a term exp(c*||x||^2)\n        # where c  0, which causes divergence.\n        geo_inf = True\n\n        # Calculate the exponential divergence coefficient theta_geo.\n        # beta_k = k / K. beta_{K-1} = (K-1)/K.\n        # theta_geo = (1 - beta_{K-1}) / 2 = (1 - (K-1)/K) / 2 = (1/K) / 2 = 1 / (2*K).\n        theta_geo = 1.0 / (2.0 * K)\n\n        # --- Tail-Aware Polynomial Path Analysis ---\n\n        # Calculate the polynomial integrability margin rho_poly.\n        # beta_K = 1\n        # beta_{K-1} = (K-1)/K\n        # m(beta) = nu*beta + kappa*(1-beta)\n        # m(beta_K) = nu\n        # m(beta_{K-1}) = nu * (K-1)/K + kappa * (1 - (K-1)/K) = nu * (K-1)/K + kappa/K\n        # rho_poly = (2*m(beta_K) - m(beta_{K-1})) - d/2\n        #          = (2*nu - (nu*(K-1)/K + kappa/K)) - d/2\n        #          = nu*(2 - (K-1)/K) - kappa/K - d/2\n        #          = nu*( (2K - K + 1)/K ) - kappa/K - d/2\n        #          = nu*(K+1)/K - kappa/K - d/2\n        \n        rho_poly = nu * (K + 1.0) / K - kappa / K - d / 2.0\n        \n        # The second moment is infinite if and only if rho_poly is non-positive.\n        poly_inf = rho_poly = 0\n\n        # Append the results for the current test case.\n        results.append([geo_inf, poly_inf, theta_geo, rho_poly])\n\n    # Final print statement in the exact required format.\n    # Convert each inner list to its string representation and join them.\n    # Python's default string conversion for bools and floats is used as required.\n    formatted_results = [f\"[{res[0]},{res[1]},{res[2]},{res[3]}]\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3288124"}, {"introduction": "Beyond the path's functional form, the performance of AIS hinges on the 'annealing schedule'—the specific sequence of intermediate temperatures. A poorly chosen schedule, where steps are too large in 'difficult' regions, can ruin the efficiency of the sampler by inducing high-variance weights. This problem addresses how to intelligently automate schedule design by using information from pilot runs to adaptively refine the schedule, a crucial technique for making AIS a practical and efficient tool [@problem_id:3288085].", "problem": "Consider annealed importance sampling between a source distribution $p_0(x)$ and a target distribution $p_1(x)$, with a tempered path $p_t(x) \\propto p_0(x)^{1-t} p_1(x)^t$ for $t \\in [0,1]$. Let a schedule be given by $\\{t_k\\}_{k=0}^K$ with $t_0 = 0$ and $t_K = 1$. For each stage $k \\in \\{1,\\dots,K\\}$, the incremental importance weight for a state $x$ is\n$$\nw_k(x) = \\frac{p_{t_k}(x)}{p_{t_{k-1}}(x)} = \\exp\\!\\big((t_k - t_{k-1}) \\big(\\log p_1(x) - \\log p_0(x)\\big)\\big).\n$$\nSuppose you run $R$ pilot annealed importance sampling trajectories, producing incremental weights $\\{w_k^{(i)}\\}_{i=1}^R$ at each stage $k$. Define the stage-wise Effective Sample Size (ESS) as\n$$\nESS_k = \\frac{\\left(\\sum_{i=1}^R w_k^{(i)}\\right)^2}{\\sum_{i=1}^R \\left(w_k^{(i)}\\right)^2}.\n$$\nYou are given a total budget of $M$ intermediate points to distribute across the path $t \\in [0,1]$ by refining the schedule (that is, by splitting the existing intervals $[t_{k-1},t_k]$ into subintervals) so that, heuristically, the stage-wise ESS is not lower than a prescribed minimum $ESS_{\\min}$ at any stage. The goal is to choose a heuristic that uses pilot-run information to adaptively redistribute these $M$ intermediate points across the schedule to target the minimum per-stage ESS.\n\nWhich option below describes a scientifically sound heuristic that uses pilot-run estimates to adaptively refine the schedule to target a minimum per-stage ESS, while keeping the total number of intermediate points equal to $M$?\n\nA. From the pilot runs, compute $\\widehat{ESS}_k$ at each stage $k$ and set $\\hat{\\sigma}_k^2 = \\log\\!\\big(R/\\widehat{ESS}_k\\big)$. Treating the variance of $\\log w_k$ as scaling quadratically with the step size, choose a local step size target\n$$\n\\Delta t_k^\\star = \\Delta t_k \\sqrt{\\frac{\\log\\!\\big(R/ESS_{\\min}\\big)}{\\hat{\\sigma}_k^2}},\n$$\nwhere $\\Delta t_k = t_k - t_{k-1}$ is the current step. Allocate the number of substeps $m_k$ in $[t_{k-1},t_k]$ proportionally to $(\\Delta t_k/\\Delta t_k^\\star)$, i.e., $m_k \\propto \\sqrt{\\hat{\\sigma}_k^2 / \\log\\!\\big(R/ESS_{\\min}\\big)}$, and rescale $\\{m_k\\}$ so that $\\sum_{k=1}^K m_k = M$. Split each interval into $m_k$ equal subintervals to form the refined schedule.\n\nB. Redistribute the $M$ intermediate points proportional to $\\widehat{ESS}_k$ across stages, giving more points to stages with larger $\\widehat{ESS}_k$, because those stages produce more reliable incremental weights and thus should be further refined.\n\nC. Use the average Metropolis–Hastings acceptance probability of the transition kernel at each stage (from pilot runs) as the sole criterion, allocating more intermediate points to stages with lower acceptance rates, irrespective of the incremental weights, because improving mixing automatically yields higher Effective Sample Size.\n\nD. Keep the $M$ intermediate points equally spaced in $t$ by setting $t_k = k/K$ after refinement, ignoring pilot-run ESS information, because equal spacing avoids bias and overfitting to pilot runs and therefore maintains estimator quality uniformly across stages.", "solution": "The user has requested a an analysis of a problem concerning adaptive schedule selection for Annealed Importance Sampling (AIS). The problem statement is first validated for scientific and logical integrity.\n\n### Problem Validation\n\nThe problem statement describes a standard setup for Annealed Importance Sampling (AIS) and poses a question about designing an adaptive schedule.\n- **Givens**:\n    - Source distribution $p_0(x)$ and target distribution $p_1(x)$.\n    - A tempered path $p_t(x) \\propto p_0(x)^{1-t} p_1(x)^t$ for $t \\in [0,1]$.\n    - An initial schedule $\\{t_k\\}_{k=0}^K$ with $t_0 = 0$ and $t_K = 1$.\n    - The incremental importance weight is defined as $w_k(x) = \\exp\\!\\big((t_k - t_{k-1}) \\big(\\log p_1(x) - \\log p_0(x)\\big)\\big)$. This corresponds to the ratio of unnormalized densities, $\\tilde{p}_{t_k}(x)/\\tilde{p}_{t_{k-1}}(x)$, which is standard in AIS, despite the potentially confusing notation $w_k(x) = p_{t_k}(x)/p_{t_{k-1}}(x)$.\n    - $R$ pilot runs producing weights $\\{w_k^{(i)}\\}_{i=1}^R$ at each stage $k$.\n    - The stage-wise Effective Sample Size (ESS) is defined as $ESS_k = \\frac{\\left(\\sum_{i=1}^R w_k^{(i)}\\right)^2}{\\sum_{i=1}^R \\left(w_k^{(i)}\\right)^2}$. This is a standard definition.\n    - A budget of $M$ intermediate points for schedule refinement.\n    - A target minimum stage-wise ESS, $ESS_{\\min}$.\n- **Validation Verdict**: The problem is scientifically grounded in the theory of Monte Carlo methods, specifically AIS. It is well-posed, asking for the evaluation of several heuristics against established principles of variance reduction. The terminology and formulas are standard in the field. The problem is objective and contains sufficient information to determine the most sound heuristic among the choices. Therefore, the problem is **valid**.\n\n### Derivation of the Correct Heuristic\n\nThe primary goal of refining the annealing schedule is to manage the variance of the importance weights. A high variance in the weights at any stage leads to a low Effective Sample Size (ESS), which in turn degrades the quality of the final estimator of the partition function ratio $Z_1/Z_0$ or expectations under $p_1(x)$. The goal is to distribute the intermediate schedule points to equalize the difficulty across all steps, which can be measured by keeping the stage-wise ESS above a certain threshold.\n\nThe stage-wise $ESS_k$ is related to the relative variance of the weights $w_k^{(i)}$. A more stable and theoretically convenient quantity to analyze is the variance of the log-weights. Let $\\sigma_k^2 = \\text{Var}_{x \\sim p_{t_{k-1}}}[\\log w_k(x)]$. Assuming the weights $w_k$ are approximately log-normally distributed, the ESS and log-weight variance are related by:\n$$\nESS_k \\approx R \\exp(-\\sigma_k^2)\n$$\nFrom this relationship, we can estimate the log-weight variance from the pilot run's empirical $\\widehat{ESS}_k$:\n$$\n\\hat{\\sigma}_k^2 \\approx \\log\\left(\\frac{R}{\\widehat{ESS}_k}\\right)\n$$\nA low $\\widehat{ESS}_k$ corresponds to a high estimated variance $\\hat{\\sigma}_k^2$, indicating a \"difficult\" transition from $t_{k-1}$ to $t_k$.\n\nThe log-weight is given by $\\log w_k(x) = (t_k - t_{k-1})(\\log p_1(x) - \\log p_0(x))$. Let $\\Delta t_k = t_k - t_{k-1}$ be the step size and $L(x) = \\log p_1(x) - \\log p_0(x)$. Then, the variance of the log-weight is:\n$$\n\\sigma_k^2 = \\text{Var}_{p_{t_{k-1}}}[\\log w_k(x)] = (\\Delta t_k)^2 \\text{Var}_{p_{t_{k-1}}}[L(x)]\n$$\nThis shows that the log-weight variance scales quadratically with the step size $\\Delta t_k$. The term $\\text{Var}_{p_{t_{k-1}}}[L(x)]$ can be thought of as the intrinsic difficulty of the path at temperature $t_{k-1}$.\n\nThe heuristic aims to create a new schedule where each step has roughly the same target log-weight variance, $\\sigma_{\\min}^2$, corresponding to the target $ESS_{\\min}$. From our approximation, this target variance is $\\sigma_{\\min}^2 = \\log(R/ESS_{\\min})$.\n\nConsider the original $k$-th interval $[t_{k-1}, t_k]$ of length $\\Delta t_k$. We want to subdivide it into $m_k$ smaller intervals, each of length $\\delta t_k = \\Delta t_k / m_k$. For each new small step, the variance of its log-weight will be:\n$$\n\\sigma_{\\text{new}}^2 \\approx (\\delta t_k)^2 \\text{Var}_{p_{t_{k-1}}}[L(x)]\n$$\nHere, we use the approximation that the intrinsic variance $\\text{Var}_{p_t}[L(x)]$ is roughly constant for $t \\in [t_{k-1}, t_k]$.\nFrom the pilot run, we have the estimate $\\hat{\\sigma}_k^2 = (\\Delta t_k)^2 \\text{Var}_{p_{t_{k-1}}}[L(x)]$, which implies $\\text{Var}_{p_{t_{k-1}}}[L(x)] \\approx \\hat{\\sigma}_k^2 / (\\Delta t_k)^2$.\nSubstituting this into the expression for the new variance:\n$$\n\\sigma_{\\text{new}}^2 \\approx \\left(\\frac{\\Delta t_k}{m_k}\\right)^2 \\frac{\\hat{\\sigma}_k^2}{(\\Delta t_k)^2} = \\frac{\\hat{\\sigma}_k^2}{m_k^2}\n$$\nTo achieve the target variance $\\sigma_{\\min}^2$, we set $\\sigma_{\\text{new}}^2 = \\sigma_{\\min}^2$:\n$$\n\\frac{\\hat{\\sigma}_k^2}{m_k^2} = \\sigma_{\\min}^2 \\implies m_k^2 = \\frac{\\hat{\\sigma}_k^2}{\\sigma_{\\min}^2} \\implies m_k = \\frac{\\hat{\\sigma}_k}{\\sigma_{\\min}}\n$$\nThis result is the core of the adaptive heuristic: the number of substeps $m_k$ to place in the original interval $[t_{k-1}, t_k]$ should be proportional to the estimated standard deviation of the log-weights, $\\hat{\\sigma}_k$.\n$$\nm_k \\propto \\hat{\\sigma}_k\n$$\nThe total number of substeps is constrained by the budget $M$ (assuming $\\sum m_k = M$), so the final allocation is $m_k = M \\frac{\\hat{\\sigma}_k}{\\sum_{j=1}^K \\hat{\\sigma}_j}$. This strategy allocates more points to intervals that were observed to have high variance (low ESS), which is scientifically sound.\n\n### Option-by-Option Analysis\n\n**A. From the pilot runs, compute $\\widehat{ESS}_k$ at each stage $k$ and set $\\hat{\\sigma}_k^2 = \\log\\!\\big(R/\\widehat{ESS}_k\\big)$. Treating the variance of $\\log w_k$ as scaling quadratically with the step size, choose a local step size target $\\Delta t_k^\\star = \\Delta t_k \\sqrt{\\frac{\\log\\!\\big(R/ESS_{\\min}\\big)}{\\hat{\\sigma}_k^2}}$, where $\\Delta t_k = t_k - t_{k-1}$ is the current step. Allocate the number of substeps $m_k$ in $[t_{k-1},t_k]$ proportionally to $(\\Delta t_k/\\Delta t_k^\\star)$, i.e., $m_k \\propto \\sqrt{\\hat{\\sigma}_k^2 / \\log\\!\\big(R/ESS_{\\min}\\big)}$, and rescale $\\{m_k\\}$ so that $\\sum_{k=1}^K m_k = M$. Split each interval into $m_k$ equal subintervals to form the refined schedule.**\n\nThis option follows the derived logic precisely.\n1.  The estimation $\\hat{\\sigma}_k^2 = \\log(R/\\widehat{ESS}_k)$ is standard.\n2.  The target step size $\\Delta t_k^{\\star}$ is defined as the step that would yield the target variance $\\sigma_{\\min}^2 = \\log(R/ESS_{\\min})$. As shown in the derivation, the ideal number of substeps is $m_k = \\hat{\\sigma}_k / \\sigma_{\\min}$. The target step size is $\\Delta t_k^\\star = \\Delta t_k / m_k = \\Delta t_k \\sigma_{\\min} / \\hat{\\sigma}_k = \\Delta t_k \\sqrt{\\sigma_{\\min}^2 / \\hat{\\sigma}_k^2}$, which matches the formula.\n3.  The allocation rule $m_k \\propto (\\Delta t_k / \\Delta t_k^\\star)$ implies $m_k$ substeps of size $\\Delta t_k^\\star$ are needed to cover the interval of size $\\Delta t_k$. This is correct.\n4.  The proportionality $m_k \\propto \\sqrt{\\hat{\\sigma}_k^2 / \\log(R/ESS_{\\min})}$ is equivalent to $m_k \\propto \\hat{\\sigma}_k / \\sigma_{\\min}$, which simplifies to $m_k \\propto \\hat{\\sigma}_k$ since $\\sigma_{\\min}$ is a constant. This perfectly matches our derivation.\nThe entire procedure is a textbook example of a sound, variance-based adaptive scheduling heuristic for AIS.\n\n**Verdict: Correct**\n\n**B. Redistribute the $M$ intermediate points proportional to $\\widehat{ESS}_k$ across stages, giving more points to stages with larger $\\widehat{ESS}_k$, because those stages produce more reliable incremental weights and thus should be further refined.**\n\nThis heuristic is fundamentally flawed. A large $\\widehat{ESS}_k$ indicates a small weight variance, meaning the step from $t_{k-1}$ to $t_k$ was \"easy\". These stages require *fewer* points, not more. Conversely, stages with low $\\widehat{ESS}_k$ are \"hard\" and need to be broken down into smaller substeps (i.e., allocated more points). This option proposes the exact opposite of the correct strategy.\n\n**Verdict: Incorrect**\n\n**C. Use the average Metropolis–Hastings acceptance probability of the transition kernel at each stage (from pilot runs) as the sole criterion, allocating more intermediate points to stages with lower acceptance rates, irrespective of the incremental weights, because improving mixing automatically yields higher Effective Sample Size.**\n\nThis heuristic addresses a different aspect of AIS performance. The MCMC transition kernel $T_k(x'|x)$ is used to generate a sample from $p_{t_k}(x)$. A low acceptance rate indicates poor mixing, which is indeed a problem. However, the dominant factor determining the variance of the overall AIS estimator is the variance of the product of importance weights. The stage-wise ESS is a direct measure of this variance contribution. While poor mixing can affect the pilot-run estimate of ESS, the two quantities (acceptance rate and weight variance) are not directly equivalent. It is possible to have high acceptance rates but still have large weight variance if the distributions $p_{t_{k-1}}$ and $p_{t_k}$ are very different. The problem asks specifically to target a minimum per-stage ESS, so using a direct estimate of ESS (or its proxy, $\\hat{\\sigma}_k^2$) is the most scientifically sound approach. Ignoring the incremental weights is a critical mistake.\n\n**Verdict: Incorrect**\n\n**D. Keep the $M$ intermediate points equally spaced in $t$ by setting $t_k = k/K$ after refinement, ignoring pilot-run ESS information, because equal spacing avoids bias and overfitting to pilot runs and therefore maintains estimator quality uniformly across stages.**\n\nThis describes a non-adaptive, or static, schedule. The purpose of pilot runs is to gather information to improve the schedule. Ignoring this information defeats the purpose of an adaptive procedure. While static schedules are simple, they can be highly inefficient if the \"difficulty\" of the annealing path is not uniform. The problem explicitly asks for a heuristic that *uses* pilot-run information to *adaptively* refine the schedule. This option proposes to do neither.\n\n**Verdict: Incorrect**", "answer": "$$\\boxed{A}$$", "id": "3288085"}]}