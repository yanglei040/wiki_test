{"hands_on_practices": [{"introduction": "Self-Normalized Importance Sampling (SNIS) is a cornerstone of Bayesian computation, allowing us to approximate posterior expectations even when the normalizing constant is unknown. This first practice grounds the theory in a tangible example by comparing a SNIS approximation against an exact, analytically derivable posterior in a conjugate Poisson-Gamma model [@problem_id:3289041]. By implementing the sampler and quantifying its finite-sample bias, you will develop a practical intuition for the performance and limitations of importance sampling.", "problem": "You are given a single observation modelled by a Poisson likelihood with a Gamma prior on the unknown rate parameter. Your task is to derive the exact posterior distribution from first principles, compute exact posterior expectations for specified functionals, and then quantify the finite-sample bias of a Self-Normalized Importance Sampling (SNIS) approximation using a Lognormal proposal distribution. Your program must implement the full pipeline and produce a single-line output aggregating the bias estimates for a provided test suite.\n\nThe fundamental base you may use includes:\n- Bayes' theorem: for prior density $p(\\lambda)$, likelihood $p(y \\mid \\lambda)$, and posterior $p(\\lambda \\mid y)$, the identity $p(\\lambda \\mid y) \\propto p(y \\mid \\lambda) p(\\lambda)$.\n- The Poisson probability mass function: $p(y \\mid \\lambda) = \\exp(-\\lambda) \\lambda^{y} / y!$ for $y \\in \\{0,1,2,\\dots\\}$ and $\\lambda > 0$.\n- The Gamma probability density function with shape-rate parameterization: $p(\\lambda \\mid \\alpha, \\beta) = \\beta^{\\alpha} \\lambda^{\\alpha - 1} \\exp(-\\beta \\lambda) / \\Gamma(\\alpha)$ for $\\alpha > 0$, $\\beta > 0$, and $\\lambda > 0$.\n- The Lognormal probability density function with parameters $(\\mu, \\sigma)$: $q(\\lambda \\mid \\mu, \\sigma) = \\left[ \\lambda \\sigma \\sqrt{2 \\pi} \\right]^{-1} \\exp\\left( - \\frac{(\\log \\lambda - \\mu)^2}{2 \\sigma^2} \\right)$ for $\\sigma > 0$ and $\\lambda > 0$.\n\nTasks to complete:\n1) Starting only from Bayes' theorem and the definitions of the Poisson and Gamma distributions, derive the exact posterior distribution $p(\\lambda \\mid y, \\alpha, \\beta)$ for the model with a single observation $y \\in \\{0,1,2,\\dots\\}$, prior $\\lambda \\sim \\mathrm{Gamma}(\\alpha, \\beta)$ with shape $\\alpha$ and rate $\\beta$, and likelihood $y \\mid \\lambda \\sim \\mathrm{Poisson}(\\lambda)$. Implement a function that returns the posterior parameters in the same shape-rate form.\n2) For the posterior from step $1$, consider the functionals $h_1(\\lambda) = \\lambda$ and $h_2(\\lambda) = \\log \\lambda$. Using properties of the Gamma distribution that qualify as well-tested facts, compute the exact expectations $\\mathbb{E}[h_1(\\lambda) \\mid y, \\alpha, \\beta]$ and $\\mathbb{E}[h_2(\\lambda) \\mid y, \\alpha, \\beta]$ under the exact posterior.\n3) Implement Self-Normalized Importance Sampling (SNIS) using a Lognormal proposal $q(\\lambda \\mid \\mu, \\sigma)$ to approximate $\\mathbb{E}[h(\\lambda)]$ for $h \\in \\{h_1, h_2\\}$. Given $N$ independent and identically distributed (IID) draws $\\{\\lambda_i\\}_{i=1}^N$ from $q$, form unnormalized importance weights $w_i \\propto \\frac{p(\\lambda_i \\mid y, \\alpha, \\beta)}{q(\\lambda_i \\mid \\mu, \\sigma)}$ using the posterior density up to a multiplicative constant, normalize the weights $\\tilde{w}_i = \\frac{w_i}{\\sum_{j=1}^N w_j}$, and compute the SNIS estimate $\\hat{I}_N(h) = \\sum_{i=1}^N \\tilde{w}_i h(\\lambda_i)$. Use numerically stable computations in the log domain for weights.\n4) Quantify the finite-sample bias of the SNIS estimator for both $h_1$ and $h_2$ by Monte Carlo replication. Specifically, with $R$ IID replicates of the SNIS procedure, compute the empirical bias estimate $\\widehat{\\mathrm{Bias}}_R(h) = \\left( \\frac{1}{R} \\sum_{r=1}^R \\hat{I}_{N,r}(h) \\right) - \\mathbb{E}[h(\\lambda) \\mid y, \\alpha, \\beta]$, where $\\hat{I}_{N,r}(h)$ is the SNIS estimate in replicate $r$. Use the base random seed $s_0$ and, for test case index $k \\in \\{0,1,2\\}$, use seed $s_0 + k$ to initialize the random number generator for that test case. All randomness must be generated from a Lognormal distribution parameterized by $(\\mu, \\sigma)$ for the proposal $q$.\n5) Implement the following test suite. For each test case, compute the two empirical biases in the order $[ \\widehat{\\mathrm{Bias}}_R(h_1), \\widehat{\\mathrm{Bias}}_R(h_2) ]$, and aggregate across test cases in the order case $1$, case $2$, case $3$ into a single flattened list.\n- Use $N = 2000$ and $R = 400$.\n- Use base seed $s_0 = 20251010$.\n- Test Case $1$: $y = 12$, $\\alpha = 2.5$, $\\beta = 1.3$, $\\mu = \\log\\left( \\frac{\\alpha + y}{\\beta + 1} \\right)$, $\\sigma = 0.7$.\n- Test Case $2$: $y = 1$, $\\alpha = 0.6$, $\\beta = 0.2$, $\\mu = -0.5$, $\\sigma = 1.1$.\n- Test Case $3$: $y = 100$, $\\alpha = 10.0$, $\\beta = 5.0$, $\\mu = \\log\\left( \\frac{\\alpha + y}{\\beta + 1} \\right) - 0.5$, $\\sigma = 0.5$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain $6$ floating-point numbers in the order $[\\widehat{\\mathrm{Bias}}_R(h_1)\\_\\mathrm{case1}, \\widehat{\\mathrm{Bias}}_R(h_2)\\_\\mathrm{case1}, \\widehat{\\mathrm{Bias}}_R(h_1)\\_\\mathrm{case2}, \\widehat{\\mathrm{Bias}}_R(h_2)\\_\\mathrm{case2}, \\widehat{\\mathrm{Bias}}_R(h_1)\\_\\mathrm{case3}, \\widehat{\\mathrm{Bias}}_R(h_2)\\_\\mathrm{case3}]$.", "solution": "The problem statement has been critically examined and is determined to be valid. It is scientifically grounded, well-posed, objective, self-contained, and consistent. All provided data and definitions are clear and sufficient for deriving a unique and meaningful solution. The problem represents a standard, albeit comprehensive, exercise in Bayesian inference and Monte Carlo simulation. We now proceed with the solution.\n\n### Step 1: Derivation of the Posterior Distribution\n\nThe model is defined by a Poisson likelihood for a single observation $y$, and a Gamma prior for the rate parameter $\\lambda$.\nThe prior distribution is $\\lambda \\sim \\mathrm{Gamma}(\\alpha, \\beta)$, with probability density function (PDF):\n$$\np(\\lambda \\mid \\alpha, \\beta) = \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} \\lambda^{\\alpha - 1} e^{-\\beta \\lambda}\n$$\nThe likelihood for an observation $y$ is $y \\mid \\lambda \\sim \\mathrm{Poisson}(\\lambda)$, with probability mass function (PMF):\n$$\np(y \\mid \\lambda) = \\frac{e^{-\\lambda} \\lambda^{y}}{y!}\n$$\nAccording to Bayes' theorem, the posterior distribution $p(\\lambda \\mid y, \\alpha, \\beta)$ is proportional to the product of the likelihood and the prior:\n$$\np(\\lambda \\mid y, \\alpha, \\beta) \\propto p(y \\mid \\lambda) p(\\lambda \\mid \\alpha, \\beta)\n$$\nSubstituting the functional forms of the likelihood and prior, we have:\n$$\np(\\lambda \\mid y, \\alpha, \\beta) \\propto \\left( \\frac{e^{-\\lambda} \\lambda^{y}}{y!} \\right) \\left( \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} \\lambda^{\\alpha - 1} e^{-\\beta \\lambda} \\right)\n$$\nWe can collect all terms that do not depend on $\\lambda$ into the proportionality constant. These terms are $\\frac{1}{y!}$, $\\beta^{\\alpha}$, and $\\frac{1}{\\Gamma(\\alpha)}$. Combining the terms that are functions of $\\lambda$:\n$$\np(\\lambda \\mid y, \\alpha, \\beta) \\propto \\lambda^{y} e^{-\\lambda} \\cdot \\lambda^{\\alpha - 1} e^{-\\beta \\lambda}\n$$\nUsing the properties of exponents, we combine the powers of $\\lambda$ and the arguments of the exponential function:\n$$\np(\\lambda \\mid y, \\alpha, \\beta) \\propto \\lambda^{y + \\alpha - 1} e^{-(\\lambda + \\beta\\lambda)}\n$$\n$$\np(\\lambda \\mid y, \\alpha, \\beta) \\propto \\lambda^{(\\alpha + y) - 1} e^{-(\\beta + 1)\\lambda}\n$$\nThis resulting expression is the kernel of a Gamma distribution. By inspection, we can identify the new parameters of this posterior Gamma distribution. Let the posterior parameters be $\\alpha'$ and $\\beta'$. We can see that:\n$$\n\\alpha' = \\alpha + y\n$$\n$$\n\\beta' = \\beta + 1\n$$\nThus, the posterior distribution for $\\lambda$ given the observation $y$ and prior parameters $\\alpha$ and $\\beta$ is a Gamma distribution with updated parameters:\n$$\n\\lambda \\mid y, \\alpha, \\beta \\sim \\mathrm{Gamma}(\\alpha' = \\alpha + y, \\beta' = \\beta + 1)\n$$\nThis demonstrates the conjugacy of the Gamma prior with the Poisson likelihood.\n\n### Step 2: Computation of Exact Posterior Expectations\n\nWith the posterior distribution identified as $\\lambda \\mid y \\sim \\mathrm{Gamma}(\\alpha', \\beta')$, we can compute the exact expectations of the specified functionals $h_1(\\lambda) = \\lambda$ and $h_2(\\lambda) = \\log\\lambda$.\n\nFor a random variable $X \\sim \\mathrm{Gamma}(k, \\theta)$ with shape $k$ and scale $\\theta$, the mean is $\\mathbb{E}[X] = k\\theta$. In our shape-rate parameterization where the rate is $\\beta_p = 1/\\theta$, the mean is $\\mathbb{E}[X] = k/\\beta_p$.\nFor our posterior distribution $\\mathrm{Gamma}(\\alpha', \\beta')$, the expectation of $h_1(\\lambda) = \\lambda$ is:\n$$\n\\mathbb{E}[h_1(\\lambda) \\mid y, \\alpha, \\beta] = \\mathbb{E}[\\lambda \\mid y] = \\frac{\\alpha'}{\\beta'} = \\frac{\\alpha + y}{\\beta + 1}\n$$\nFor the functional $h_2(\\lambda) = \\log\\lambda$, the expectation for a Gamma-distributed variable $X \\sim \\mathrm{Gamma}(\\alpha_p, \\beta_p)$ is given by $\\mathbb{E}[\\log X] = \\psi(\\alpha_p) - \\log(\\beta_p)$, where $\\psi(\\cdot)$ is the digamma function, defined as the logarithmic derivative of the gamma function, $\\psi(z) = \\frac{d}{dz}\\log\\Gamma(z)$.\nApplying this to our posterior distribution:\n$$\n\\mathbb{E}[h_2(\\lambda) \\mid y, \\alpha, \\beta] = \\mathbb{E}[\\log \\lambda \\mid y] = \\psi(\\alpha') - \\log(\\beta') = \\psi(\\alpha + y) - \\log(\\beta + 1)\n$$\n\n### Step 3: Self-Normalized Importance Sampling (SNIS)\n\nThe goal is to estimate the posterior expectation $\\mathbb{E}[h(\\lambda) \\mid y] = \\frac{\\int h(\\lambda) p(y|\\lambda)p(\\lambda) d\\lambda}{\\int p(y|\\lambda)p(\\lambda) d\\lambda}$ using a proposal distribution $q(\\lambda)$. Let $\\tilde{p}(\\lambda) = p(y|\\lambda)p(\\lambda)$ be the unnormalized posterior.\nThe SNIS estimator for a functional $h(\\lambda)$ based on $N$ samples $\\{\\lambda_i\\}_{i=1}^N$ from a proposal distribution $q(\\lambda)$ is:\n$$\n\\hat{I}_N(h) = \\sum_{i=1}^{N} \\tilde{w}_i h(\\lambda_i)\n$$\nwhere the normalized weights $\\tilde{w}_i$ are given by:\n$$\n\\tilde{w}_i = \\frac{w_i}{\\sum_{j=1}^{N} w_j} \\quad \\text{with raw weights} \\quad w_i = \\frac{\\tilde{p}(\\lambda_i)}{q(\\lambda_i)}\n$$\nOur target distribution (unnormalized posterior) is $\\tilde{p}(\\lambda) \\propto \\lambda^{\\alpha' - 1} e^{-\\beta'\\lambda}$, and our proposal is the Lognormal distribution $q(\\lambda \\mid \\mu, \\sigma)$.\nTo prevent numerical underflow or overflow, computations are performed in the log domain. The log of the raw weight is:\n$$\n\\log w_i = \\log \\tilde{p}(\\lambda_i) - \\log q(\\lambda_i)\n$$\nWe only need the kernel of the target density, so we use $\\log \\tilde{p}_{\\text{kernel}}(\\lambda) = (\\alpha' - 1)\\log\\lambda - \\beta'\\lambda$. The log of the proposal PDF is $\\log q(\\lambda \\mid \\mu, \\sigma) = -\\log\\lambda - \\log\\sigma - \\frac{1}{2}\\log(2\\pi) - \\frac{(\\log\\lambda - \\mu)^2}{2\\sigma^2}$. So,\n$$\n\\log w_i = \\left( (\\alpha' - 1)\\log\\lambda_i - \\beta'\\lambda_i \\right) - \\left( -\\log\\lambda_i - \\log\\sigma - \\frac{1}{2}\\log(2\\pi) - \\frac{(\\log\\lambda_i - \\mu)^2}{2\\sigma^2} \\right)\n$$\nFor normalization, we use the log-sum-exp trick. Let $L_i = \\log w_i$ and $L_{\\max} = \\max_i \\{L_i\\}$. The normalized weights are:\n$$\n\\tilde{w}_i = \\frac{e^{L_i}}{\\sum_{j=1}^N e^{L_j}} = \\frac{e^{L_i - L_{\\max}}}{\\sum_{j=1}^N e^{L_j - L_{\\max}}}\n$$\nThis stabilized computation avoids floating-point errors.\n\n### Step 4: Quantifying Finite-Sample Bias\n\nThe SNIS estimator is generally biased for finite $N$. The bias is defined as $\\mathrm{Bias}(\\hat{I}_N(h)) = \\mathbb{E}[\\hat{I}_N(h)] - \\mathbb{E}[h(\\lambda) \\mid y]$. We estimate this bias using Monte Carlo simulation. By generating $R$ independent replicates of the SNIS estimate, $\\{\\hat{I}_{N,r}(h)\\}_{r=1}^R$, we can approximate the expectation $\\mathbb{E}[\\hat{I}_N(h)]$ with the sample mean $\\frac{1}{R}\\sum_{r=1}^R \\hat{I}_{N,r}(h)$. The empirical bias is then:\n$$\n\\widehat{\\mathrm{Bias}}_R(h) = \\left( \\frac{1}{R} \\sum_{r=1}^R \\hat{I}_{N,r}(h) \\right) - \\mathbb{E}[h(\\lambda) \\mid y]\n$$\nThis procedure will be implemented for the functionals $h_1(\\lambda)$ and $h_2(\\lambda)$ for each test case specified in the problem. The random number generator is seeded with $s_0 + k$ for test case $k$ to ensure reproducibility.", "answer": "```python\nimport numpy as np\nfrom scipy.special import digamma\n\ndef solve():\n    \"\"\"\n    Implements the full pipeline to derive posterior, compute exact expectations,\n    and quantify the finite-sample bias of a Self-Normalized Importance Sampling\n    (SNIS) approximation for a Poisson-Gamma model.\n    \"\"\"\n\n    # Global parameters from the problem statement\n    N = 2000\n    R = 400\n    s0 = 20251010\n\n    def calculate_biases(y, alpha, beta, mu, sigma, seed):\n        \"\"\"\n        Calculates the SNIS bias for a single test case.\n\n        Args:\n            y (int): The observed Poisson count.\n            alpha (float): The shape parameter of the Gamma prior.\n            beta (float): The rate parameter of the Gamma prior.\n            mu (float): The mean parameter of the Lognormal proposal (on the log scale).\n            sigma (float): The standard deviation of the Lognormal proposal (on the log scale).\n            seed (int): The random seed for this test case.\n\n        Returns:\n            A tuple (bias_h1, bias_h2) containing the empirical biases for\n            h1(lambda) = lambda and h2(lambda) = log(lambda).\n        \"\"\"\n        # Set the seed for this specific test case to ensure reproducibility.\n        rng = np.random.default_rng(seed)\n\n        # Step 1: Derive posterior parameters\n        # Posterior is Gamma(alpha', beta')\n        alpha_post = alpha + y\n        beta_post = beta + 1\n\n        # Step 2: Compute exact posterior expectations\n        exact_exp_h1 = alpha_post / beta_post\n        exact_exp_h2 = digamma(alpha_post) - np.log(beta_post)\n\n        # Accumulators for the mean of the SNIS estimates over R replicates\n        total_snis_h1 = 0.0\n        total_snis_h2 = 0.0\n\n        # Step 4: Quantify bias via Monte Carlo replication\n        for _ in range(R):\n            # Step 3: Implement one replicate of the SNIS estimator\n\n            # Draw N samples from the Lognormal proposal distribution q(lambda | mu, sigma)\n            # numpy.random.lognormal uses mu and sigma of the underlying normal distribution.\n            lambda_samples = rng.lognormal(mean=mu, sigma=sigma, size=N)\n            log_lambda_samples = np.log(lambda_samples)\n\n            # Calculate log of the unnormalized posterior (target) density kernel\n            # log p(lambda|y) \\propto (alpha_post - 1) * log(lambda) - beta_post * lambda\n            log_target_unnorm = (alpha_post - 1) * log_lambda_samples - beta_post * lambda_samples\n\n            # Calculate log of the Lognormal proposal density\n            # log q(lambda) = -log(lambda) - log(sigma) - 0.5*log(2*pi) - (log(lambda)-mu)^2 / (2*sigma^2)\n            log_proposal = -log_lambda_samples - np.log(sigma) - 0.5 * np.log(2 * np.pi) - \\\n                           (log_lambda_samples - mu)**2 / (2 * sigma**2)\n\n            # Calculate log of the unnormalized importance weights\n            log_weights = log_target_unnorm - log_proposal\n\n            # Normalize weights in a numerically stable way (log-sum-exp trick)\n            # This prevents underflow/overflow when exponentiating.\n            log_weights_max = np.max(log_weights)\n            weights = np.exp(log_weights - log_weights_max)\n            normalized_weights = weights / np.sum(weights)\n\n            # Compute the SNIS estimates for this replicate for h1 and h2\n            snis_h1 = np.sum(normalized_weights * lambda_samples)\n            snis_h2 = np.sum(normalized_weights * log_lambda_samples)\n\n            # Accumulate the estimates\n            total_snis_h1 += snis_h1\n            total_snis_h2 += snis_h2\n\n        # Calculate the average SNIS estimates over all R replicates\n        avg_snis_h1 = total_snis_h1 / R\n        avg_snis_h2 = total_snis_h2 / R\n\n        # Compute the final empirical bias estimates\n        bias_h1 = avg_snis_h1 - exact_exp_h1\n        bias_h2 = avg_snis_h2 - exact_exp_h2\n\n        return bias_h1, bias_h2\n\n    # Step 5: Implement the test suite\n    test_cases_params = [\n        # Test Case 1\n        {'y': 12, 'alpha': 2.5, 'beta': 1.3, 'mu_func': lambda a, y, b: np.log((a + y) / (b + 1)), 'sigma': 0.7},\n        # Test Case 2\n        {'y': 1, 'alpha': 0.6, 'beta': 0.2, 'mu_func': lambda a, y, b: -0.5, 'sigma': 1.1},\n        # Test Case 3\n        {'y': 100, 'alpha': 10.0, 'beta': 5.0, 'mu_func': lambda a, y, b: np.log((a + y) / (b + 1)) - 0.5, 'sigma': 0.5},\n    ]\n\n    all_biases = []\n    \n    for i, params in enumerate(test_cases_params):\n        y_val = params['y']\n        alpha_val = params['alpha']\n        beta_val = params['beta']\n        mu_val = params['mu_func'](alpha_val, y_val, beta_val)\n        sigma_val = params['sigma']\n        seed_val = s0 + i\n\n        bias_h1, bias_h2 = calculate_biases(y_val, alpha_val, beta_val, mu_val, sigma_val, seed_val)\n        all_biases.extend([bias_h1, bias_h2])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, all_biases))}]\")\n\nsolve()\n```", "id": "3289041"}, {"introduction": "The Bernstein-von Mises theorem underpins many approximation techniques by guaranteeing that, under certain regularity conditions, posterior distributions become asymptotically Gaussian. This exercise challenges you to explore the boundaries of this powerful theorem by contrasting a model that satisfies these conditions with a \"nonregular\" one that does not [@problem_id:3289088]. By implementing exact samplers for both and using statistical diagnostics, you will gain a deep, practical understanding of when asymptotic normality can be trusted and when it fails.", "problem": "Consider the task of diagnosing the failure of the Bernstein–von Mises theorem (asymptotic normality of the posterior) in a nonregular model versus a regular baseline model. Work from first principles: Bayes’ rule for posterior construction, the definition of a likelihood function, and basic probability transformations. Do not assume any asymptotic normality result a priori.\n\nYour goal is to implement a program that, for a small test suite of cases, constructs posterior distributions, generates independent and identically distributed samples from them, and computes quantitative diagnostics that reveal whether the posterior shape is close to Gaussian or decidedly non-Gaussian even for large sample sizes.\n\nModels and tasks:\n\n1) Nonregular model (support depends on the parameter):\n- Data model: for a parameter $\\theta \\in (0, b)$ with $b &gt; 0$ fixed and known, observations $X_1, \\dots, X_n$ are independent and identically distributed with $X_i \\mid \\theta \\sim \\mathrm{Uniform}(0,\\theta)$.\n- Prior: $\\pi(\\theta)$ is constant on $(0, b)$ and zero outside.\n- Fundamental base to use: Bayes’ rule and the elementary form of the likelihood of independent and identically distributed observations.\n- Tasks:\n  - Derive the posterior density $\\pi(\\theta \\mid x_{1:n})$ using Bayes’ rule.\n  - Show that $\\pi(\\theta \\mid x_{1:n})$ depends on the data only through the sample maximum $X_{(n)} = \\max_i X_i$.\n  - Design and justify a method to generate independent and identically distributed draws from $\\pi(\\theta \\mid x_{1:n})$ by transforming independent $\\mathrm{Uniform}(0,1)$ variables via an analytically derived inverse cumulative distribution function. Your method must be exact (no Markov chain Monte Carlo).\n  - Using your sampler, generate $S$ independent and identically distributed posterior draws, standardize them to zero mean and unit variance using the empirical posterior mean $\\hat{\\mu}$ and empirical posterior standard deviation $\\hat{\\sigma}$, i.e., compute $Z_j = (\\Theta_j - \\hat{\\mu})/\\hat{\\sigma}$, and compute both:\n    - the standardized third central moment (skewness), defined as the empirical average of $Z_j^3$;\n    - the Kolmogorov–Smirnov sup-norm distance between the empirical cumulative distribution function of $\\{Z_j\\}$ and the standard normal cumulative distribution function, i.e., $\\sup_z \\lvert \\hat{F}_S(z) - \\Phi(z) \\rvert$.\n  - Explain in your solution why this model is a counterexample to asymptotic normality of the posterior, and why the two diagnostics above remain far from their Gaussian ideals even for large $n$.\n\n2) Regular baseline model:\n- Data model: for a parameter $\\theta \\in \\mathbb{R}$, observations $Y_1, \\dots, Y_n$ are independent and identically distributed with $Y_i \\mid \\theta \\sim \\mathcal{N}(\\theta, 1)$.\n- Prior: $\\theta \\sim \\mathcal{N}(0, \\tau^2)$ with a large but finite $\\tau^2$.\n- Fundamental base to use: Bayes’ rule with a conjugate Gaussian prior for a Gaussian likelihood.\n- Tasks:\n  - Derive the posterior $\\pi(\\theta \\mid y_{1:n})$.\n  - Generate $S$ independent and identically distributed draws from this posterior in closed form.\n  - Standardize to zero mean and unit variance using the empirical posterior mean and empirical posterior standard deviation and compute the same two diagnostics as above.\n\nDiagnostics to compute and return:\n- For each test case below, return two numbers:\n  - the empirical skewness (standardized third central moment) of the standardized posterior draws;\n  - the Kolmogorov–Smirnov sup-norm distance to the standard normal cumulative distribution function after standardization.\n\nImplementation details:\n- Use only the provided test suite below. For all random simulation, set the pseudorandom number generator seed to $20231011$ to ensure reproducibility.\n- For all posterior simulations, use $S = 120000$ independent and identically distributed draws.\n- For the Kolmogorov–Smirnov distance, compute $\\sup_z \\lvert \\hat{F}_S(z) - \\Phi(z) \\rvert$ on the standardized draws by comparing the empirical cumulative distribution function to the standard normal cumulative distribution function at the sorted sample points.\n\nTest suite:\n- Case 1 (nonregular, large $n$): $\\theta^\\star = 1.0$, $n = 1000$, $b = 10.0$.\n- Case 2 (nonregular, even larger $n$): $\\theta^\\star = 1.0$, $n = 10000$, $b = 10.0$.\n- Case 3 (regular baseline): $\\theta^\\star = 1.0$, $n = 1000$, $\\tau^2 = 10^6$.\n\nData generation:\n- For Cases 1 and 2, generate data $X_i \\sim \\mathrm{Uniform}(0, \\theta^\\star)$ independently and identically distributed; form the posterior conditioned on the realized $X_{(n)}$.\n- For Case 3, generate data $Y_i \\sim \\mathcal{N}(\\theta^\\star, 1)$ independently and identically distributed; form the posterior conditioned on the realized data.\n\nFinal output format:\n- Your program should produce a single line of output containing the results for the three test cases as a comma-separated list of three pairs, with each pair of numbers formatted to six digits after the decimal point and enclosed in square brackets. Specifically, print a single line of the form:\n  - [[skew1,ks1],[skew2,ks2],[skew3,ks3]]\n- No additional text should be printed.\n\nAngle and physical units:\n- There are no physical units or angles in this problem.\n\nAnswer types:\n- Each reported number is a real number (float). The final output is a single list containing three lists of two floats each, as described above.\n\nYour program must be self-contained and must not require any user input. It must adhere to the execution environment specified in the final answer section.", "solution": "The objective is to analyze the asymptotic behavior of posterior distributions for two distinct models: a nonregular model where the data's support depends on the parameter, and a standard regular model. We will derive the posteriors, implement exact sampling methods, and compute diagnostics (skewness and Kolmogorov-Smirnov distance) to test for asymptotic normality, as predicted for regular models by the Bernstein-von Mises (BvM) theorem.\n\n### Model 1: Nonregular Uniform Model\n\nThis model serves as a canonical counterexample to the BvM theorem.\n\n**1. Data Specification and Prior**\n- Data model: $X_1, \\dots, X_n$ are independent and identically distributed (i.i.d.) draws from a $\\mathrm{Uniform}(0,\\theta)$ distribution, i.e., $X_i \\mid \\theta \\sim \\mathcal{U}(0,\\theta)$. The parameter $\\theta$ is unknown but is restricted to the interval $(0, b)$, where $b > 0$ is a known constant.\n- Prior distribution: A non-informative prior is chosen for $\\theta$, which is constant over its support: $\\pi(\\theta) \\propto \\mathbb{I}(0 < \\theta < b)$. This is a $\\mathrm{Uniform}(0,b)$ distribution, so the prior density is $\\pi(\\theta) = \\frac{1}{b} \\mathbb{I}(0 < \\theta < b)$, where $\\mathbb{I}(\\cdot)$ is the indicator function.\n\n**2. Derivation of the Posterior Distribution**\nWe use Bayes' rule, which states that the posterior density is proportional to the product of the likelihood and the prior:\n$$\n\\pi(\\theta \\mid x_{1:n}) \\propto p(x_{1:n} \\mid \\theta) \\pi(\\theta)\n$$\nThe likelihood function $p(x_{1:n} \\mid \\theta)$ for i.i.d. observations is the product of individual densities:\n$$\np(x_{1:n} \\mid \\theta) = \\prod_{i=1}^n p(x_i \\mid \\theta) = \\prod_{i=1}^n \\left( \\frac{1}{\\theta} \\mathbb{I}(0 < x_i < \\theta) \\right)\n$$\nThe product of indicator functions $\\prod_{i=1}^n \\mathbb{I}(0 < x_i < \\theta)$ is equal to $1$ if and only if all $x_i$ are less than $\\theta$. This condition can be succinctly expressed using the sample maximum, $X_{(n)} = \\max\\{X_1, \\dots, X_n\\}$. The condition holds if and only if $X_{(n)} < \\theta$. We assume all $x_i > 0$. Therefore, the likelihood function is:\n$$\np(x_{1:n} \\mid \\theta) = \\left(\\frac{1}{\\theta}\\right)^n \\mathbb{I}(X_{(n)} < \\theta) = \\theta^{-n} \\mathbb{I}(\\theta > X_{(n)})\n$$\nAs shown, the likelihood, and thus the posterior, depends on the data $x_{1:n}$ only through the single sufficient statistic $X_{(n)}$.\n\nCombining the likelihood and the prior, the unnormalized posterior is:\n$$\n\\pi(\\theta \\mid x_{1:n}) \\propto \\left( \\theta^{-n} \\mathbb{I}(\\theta > X_{(n)}) \\right) \\times \\left( \\frac{1}{b} \\mathbb{I}(0 < \\theta < b) \\right)\n$$\nThis simplifies to:\n$$\n\\pi(\\theta \\mid x_{1:n}) \\propto \\theta^{-n} \\mathbb{I}(X_{(n)} < \\theta < b)\n$$\nTo find the normalized posterior density, we compute the normalization constant $C$:\n$$\nC = \\int_{X_{(n)}}^b \\theta^{-n} d\\theta = \\left[ \\frac{\\theta^{-n+1}}{-n+1} \\right]_{X_{(n)}}^b = \\frac{1}{n-1} \\left( X_{(n)}^{-n+1} - b^{-n+1} \\right) \\quad (\\text{for } n>1)\n$$\nThe exact posterior density is a truncated Pareto distribution:\n$$\n\\pi(\\theta \\mid x_{1:n}) = \\frac{\\theta^{-n}}{C} = \\frac{(n-1)\\theta^{-n}}{X_{(n)}^{-n+1} - b^{-n+1}} \\mathbb{I}(X_{(n)} < \\theta < b)\n$$\n\n**3. Inverse CDF Sampling Method**\nTo generate exact i.i.d. draws from this posterior, we use inverse transform sampling. First, we derive the posterior cumulative distribution function (CDF), $F(\\theta_0) = P(\\theta \\le \\theta_0 \\mid x_{1:n})$:\n$$\nF(\\theta_0) = \\int_{X_{(n)}}^{\\theta_0} \\pi(\\theta \\mid x_{1:n}) d\\theta = \\frac{1}{C} \\int_{X_{(n)}}^{\\theta_0} \\theta^{-n} d\\theta = \\frac{\\frac{1}{n-1}(X_{(n)}^{-n+1} - \\theta_0^{-n+1})}{\\frac{1}{n-1}(X_{(n)}^{-n+1} - b^{-n+1})} = \\frac{X_{(n)}^{-n+1} - \\theta_0^{-n+1}}{X_{(n)}^{-n+1} - b^{-n+1}}\n$$\nfor $\\theta_0 \\in [X_{(n)}, b]$. We set $F(\\theta_0) = u$, where $u \\sim \\mathcal{U}(0,1)$, and solve for $\\theta_0$:\n$$\nu = \\frac{X_{(n)}^{-n+1} - \\theta_0^{-n+1}}{X_{(n)}^{-n+1} - b^{-n+1}} \\implies u(X_{(n)}^{-n+1} - b^{-n+1}) = X_{(n)}^{-n+1} - \\theta_0^{-n+1}\n$$\n$$\n\\theta_0^{-n+1} = X_{(n)}^{-n+1} - u(X_{(n)}^{-n+1} - b^{-n+1}) = (1-u)X_{(n)}^{-n+1} + u b^{-n+1}\n$$\n$$\n\\theta_0 = \\left( (1-u)X_{(n)}^{-n+1} + u b^{-n+1} \\right)^{\\frac{1}{1-n}}\n$$\nThis is the inverse CDF, $F^{-1}(u)$. A numerically stable form for computation is derived by factoring out $X_{(n)}$:\n$$\n\\theta_0 = X_{(n)} \\left( 1 - u \\left(1 - \\left(\\frac{X_{(n)}}{b}\\right)^{n-1}\\right) \\right)^{\\frac{-1}{n-1}}\n$$\nGenerating a uniform random variate $u$ and applying this transformation yields an exact draw from the posterior.\n\n**4. Failure of the Bernstein-von Mises Theorem**\nThe BvM theorem states that under certain \"regularity conditions,\" a posterior distribution converges in shape to a Gaussian distribution as the sample size $n \\to \\infty$. A key regularity condition is that the support of the data distribution $p(x|\\theta)$ must not depend on the parameter $\\theta$. In our model, the support is $(0, \\theta)$, which violates this condition.\n\nConsequently, the posterior does not become Gaussian. As $n \\to \\infty$, $X_{(n)}$ converges to the true parameter value $\\theta^{\\star}$, and the term $b^{-n+1}$ (with $b > X_{(n)}$) goes to zero much faster than $X_{(n)}^{-n+1}$. The posterior becomes increasingly concentrated just above $X_{(n)}$. The density $\\pi(\\theta | x_{1:n}) \\propto \\theta^{-n}$ is maximized at the lower boundary of its support, $\\theta=X_{(n)}$, and decreases sharply. This shape is highly asymmetric and resembles a reversed (and truncated) power-law distribution. With increasing $n$, this asymmetry does not vanish. A proper rescaling of the posterior, such as $n( \\theta - X_{(n)} )$, can be shown to converge to an Exponential distribution, not a Gaussian one. Therefore, diagnostics designed to measure Gaussianity, such as skewness (which is $0$ for a Gaussian) and the K-S distance to a normal CDF, will not converge to their ideal values. They will instead converge to the values characteristic of this non-Gaussian limiting shape, remaining far from the Gaussian ideals even for very large $n$.\n\n### Model 2: Regular Gaussian Model\n\nThis model satisfies the BvM regularity conditions and serves as a baseline for comparison.\n\n**1. Data Specification and Prior**\n- Data model: $Y_1, \\dots, Y_n$ are i.i.d. draws from a $\\mathcal{N}(\\theta, 1)$ distribution.\n- Prior distribution: A conjugate prior for the mean of a Gaussian is a Gaussian. We use $\\theta \\sim \\mathcal{N}(0, \\tau^2)$, with a large variance $\\tau^2$ to make it weakly informative.\n\n**2. Derivation of the Posterior Distribution**\nAgain, we use Bayes' rule: $\\pi(\\theta \\mid y_{1:n}) \\propto p(y_{1:n} \\mid \\theta) \\pi(\\theta)$.\n- The likelihood is: $p(y_{1:n} \\mid \\theta) \\propto \\exp\\left(-\\frac{1}{2} \\sum_{i=1}^n (y_i - \\theta)^2 \\right)$.\n- The prior is: $\\pi(\\theta) \\propto \\exp\\left(-\\frac{\\theta^2}{2\\tau^2}\\right)$.\nThe posterior is proportional to the product:\n$$\n\\pi(\\theta \\mid y_{1:n}) \\propto \\exp\\left(-\\frac{1}{2} \\sum_{i=1}^n (y_i - \\theta)^2 - \\frac{\\theta^2}{2\\tau^2}\\right)\n$$\nThe term in the exponent is a quadratic in $\\theta$: $\\sum(y_i - \\theta)^2 + \\frac{\\theta^2}{\\tau^2} = \\sum y_i^2 - 2\\theta \\sum y_i + n\\theta^2 + \\frac{\\theta^2}{\\tau^2}$. Ignoring terms not involving $\\theta$, we focus on:\n$$\n-\\frac{1}{2} \\left[ \\left(n + \\frac{1}{\\tau^2}\\right)\\theta^2 - 2(n\\bar{y})\\theta \\right]\n$$\nwhere $\\bar{y} = \\frac{1}{n}\\sum y_i$. Completing the square for $\\theta$ shows this is the kernel of a Gaussian density. The posterior distribution for $\\theta$ is also a Gaussian, $\\mathcal{N}(\\mu_n, \\sigma_n^2)$, with parameters:\n$$\n\\sigma_n^2 = \\left(n + \\frac{1}{\\tau^2}\\right)^{-1} \\qquad \\mu_n = \\sigma_n^2 (n\\bar{y}) = \\frac{n\\bar{y}}{n + 1/\\tau^2}\n$$\nSampling from this posterior is straightforward: one simply generates draws from a normal distribution with the calculated mean $\\mu_n$ and variance $\\sigma_n^2$. As this posterior is itself perfectly Gaussian, generating samples, standardizing them, and comparing to a standard normal distribution should yield a skewness\nvery close to $0$ and a very small K-S distance, limited only by Monte Carlo error from the finite number of posterior draws $S$.\n\n### Diagnostic Computations\n\nFor both models, after generating $S$ posterior draws $\\{\\Theta_j\\}_{j=1}^S$, we compute:\n1.  Empirical mean $\\hat{\\mu} = \\frac{1}{S} \\sum_{j=1}^S \\Theta_j$ and standard deviation $\\hat{\\sigma} = \\sqrt{\\frac{1}{S-1} \\sum_{j=1}^S (\\Theta_j - \\hat{\\mu})^2}$.\n2.  Standardized draws $Z_j = (\\Theta_j - \\hat{\\mu})/\\hat{\\sigma}$.\n3.  **Skewness**: The standardized third central moment, $\\mathrm{Skew} = \\frac{1}{S} \\sum_{j=1}^S Z_j^3$.\n4.  **Kolmogorov-Smirnov distance**: $D_S = \\sup_z \\lvert \\hat{F}_S(z) - \\Phi(z) \\rvert$, where $\\hat{F}_S$ is the empirical CDF of the $\\{Z_j\\}$ and $\\Phi$ is the standard normal CDF. This is computed by finding the maximum deviation at the sorted sample points.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Solves the problem of diagnosing the failure of the Bernstein–von Mises theorem\n    by comparing a nonregular Uniform model with a regular Gaussian model.\n    \"\"\"\n    RNG_SEED = 20231011\n    S = 120000  # Number of posterior draws\n\n    rng = np.random.default_rng(RNG_SEED)\n\n    test_cases = [\n        {'model': 'nonregular', 'theta_star': 1.0, 'n': 1000, 'b': 10.0},\n        {'model': 'nonregular', 'theta_star': 1.0, 'n': 10000, 'b': 10.0},\n        {'model': 'regular', 'theta_star': 1.0, 'n': 1000, 'tau_sq': 1e6}\n    ]\n\n    results = []\n\n    for case in test_cases:\n        if case['model'] == 'nonregular':\n            # Parameters for the nonregular case\n            theta_star = case['theta_star']\n            n = case['n']\n            b = case['b']\n\n            # 1. Generate data\n            # X_i ~ Uniform(0, theta_star)\n            data = rng.uniform(0, theta_star, n)\n            x_n_max = np.max(data)\n\n            # 2. Generate samples from the posterior using inverse CDF sampling\n            u = rng.uniform(0, 1, S)\n            \n            # Numerically stable inverse CDF formula:\n            # theta = x_n_max * (1 - u * (1 - (x_n_max/b)**(n-1)))**(-1/(n-1))\n            power_term = (x_n_max / b)**(n - 1)\n            base = 1 - u * (1 - power_term)\n            exponent = -1 / (n - 1)\n            theta_samples = x_n_max * (base**exponent)\n\n        elif case['model'] == 'regular':\n            # Parameters for the regular case\n            theta_star = case['theta_star']\n            n = case['n']\n            tau_sq = case['tau_sq']\n\n            # 1. Generate data\n            # Y_i ~ Normal(theta_star, 1)\n            data = rng.normal(theta_star, 1, n)\n            y_bar = np.mean(data)\n\n            # 2. Calculate posterior parameters (Normal-Normal conjugate model)\n            post_var = 1 / (n + 1 / tau_sq)\n            post_mean = post_var * (n * y_bar)\n            post_std = np.sqrt(post_var)\n\n            # 3. Generate samples from the posterior\n            theta_samples = rng.normal(post_mean, post_std, S)\n\n        # 4. Compute diagnostics for all cases\n        # Standardize samples to zero mean and unit variance\n        mu_hat = np.mean(theta_samples)\n        # Using ddof=0 for population standard deviation of the sample\n        sigma_hat = np.std(theta_samples) \n        z_samples = (theta_samples - mu_hat) / sigma_hat\n\n        # Compute empirical skewness (standardized third central moment)\n        skewness = np.mean(z_samples**3)\n\n        # Compute Kolmogorov-Smirnov distance\n        z_sorted = np.sort(z_samples)\n        \n        # Empirical CDF values at each sorted sample point\n        ecdf = np.arange(1, S + 1) / S\n        \n        # Standard normal CDF values at the same points\n        norm_cdf_vals = norm.cdf(z_sorted)\n\n        # The KS statistic is the max difference between ECDF and the true CDF.\n        # The difference can be maximal just before or at the ECDF jump points.\n        dist1 = np.abs(ecdf - norm_cdf_vals)\n        dist2 = np.abs((ecdf - 1/S) - norm_cdf_vals)\n        ks_dist = np.max(np.maximum(dist1, dist2))\n\n        results.append([skewness, ks_dist])\n\n    # Format the final output as specified\n    formatted_results = [f\"[{s:.6f},{k:.6f}]\" for s, k in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3289088"}, {"introduction": "Real-world posteriors are often multimodal, a feature that can cause simple MCMC and importance samplers to fail. This exercise guides you through a sophisticated strategy that combines local Laplace approximations, importance sampling, and control variates to approximate a multimodal target [@problem_id:3289121]. By deliberately constructing imperfect proposals—such as missing a mode or misstating its weight—you will quantify the resulting bias and learn firsthand about the critical importance of a well-specified proposal distribution for complex targets.", "problem": "Consider a one-dimensional Bayesian posterior distribution $\\,\\pi(x)\\,$ that is multimodal. You will approximate $\\,\\pi(x)\\,$ using local quadratic (Laplace) expansions around multiple maximum a posteriori points (modes), thereby forming a Gaussian mixture approximation $\\,\\tilde{\\pi}(x)\\,$, and then use self-normalized importance sampling with a control variate constructed from these local quadratic approximations. You must evaluate the quality of the posterior approximation and quantify the bias introduced when modes are missed or underweighted in $\\,\\tilde{\\pi}(x)\\,$.\n\nThe foundational base for the derivation and algorithm must begin from the following well-tested definitions:\n\n- The posterior distribution $\\,\\pi(x)\\,$ is defined up to proportionality by an unnormalized density $\\,\\pi^\\star(x)\\,$ and, for computational purposes, may be represented by a normalized mixture of Gaussian components for which moments can be computed exactly.\n- The self-normalized importance sampling estimator for $\\,\\mathbb{E}_\\pi[f(X)]\\,$ using proposal distribution $\\,q(x)\\,$ with weights $\\,w(x) = \\pi(x)/q(x)\\,$ is\n$$\n\\widehat{\\mu} \\equiv \\frac{\\sum_{i=1}^n w(X_i)\\,f(X_i)}{\\sum_{i=1}^n w(X_i)} \\quad \\text{with } X_i \\stackrel{\\text{i.i.d.}}{\\sim} q.\n$$\n- Control variates reduce variance by incorporating functions with known expectations. In self-normalized importance sampling, if $\\,h(x)\\,$ has a known target expectation $\\,\\mathbb{E}_\\pi[h(X)]\\,$, then for any constant $\\,c\\,$,\n$$\n\\widehat{\\mu}_{\\text{cv}}(c) \\equiv \\frac{\\sum_{i=1}^n w(X_i)\\,\\big(f(X_i)-c\\big(h(X_i)-\\mathbb{E}_\\pi[h(X)]\\big)\\big)}{\\sum_{i=1}^n w(X_i)}\n$$\nretains the same large-sample limit as $\\,\\widehat{\\mu}\\,$. If, instead, the expectation $\\,\\mathbb{E}_\\pi[h(X)]\\,$ is replaced by an approximation $\\,\\mathbb{E}_{\\tilde{\\pi}}[h(X)]\\,$ derived from local quadratic approximations around multiple modes, then a bias is introduced that can be quantified from first principles.\n\nYou are to implement and evaluate this methodology on a scientifically sound and fully specified target posterior and proposal approximations as follows.\n\nTarget posterior $\\,\\pi(x)\\,$:\n- Let $\\,\\pi(x)\\,$ be the normalized mixture of two Gaussian components:\n$$\n\\pi(x) = \\omega_1\\,\\mathcal{N}(x;\\mu_1,\\sigma_1^2) + \\omega_2\\,\\mathcal{N}(x;\\mu_2,\\sigma_2^2),\n$$\nwith parameters $\\,\\mu_1=-2\\,$, $\\,\\sigma_1=0.5\\,$, $\\,\\mu_2=2\\,$, $\\,\\sigma_2=0.7\\,$, and weights $\\,\\omega_1=0.6\\,$, $\\,\\omega_2=0.4\\,$. Here $\\,\\mathcal{N}(x;\\mu,\\sigma^2)\\,$ denotes the Gaussian probability density function with mean $\\,\\mu\\,$ and variance $\\,\\sigma^2\\,$.\n\nControl variate and estimand:\n- Let the function of interest be $\\,f(x)=x^3\\,$. The control variate is chosen as the quadratic $\\,h(x)=x^2\\,$, motivated by local quadratic approximations of the log-posterior around each mode; its expectation under any Gaussian mixture is available in closed form.\n\nProposal distributions $\\,q(x)\\,$ and approximate posterior $\\,\\tilde{\\pi}(x)\\,$:\n- Construct $\\,q(x) \\equiv \\tilde{\\pi}(x)\\,$ as a Gaussian mixture formed from local quadratic Laplace approximations around mode locations $\\,\\mu_k\\,$. For each test case below, define $\\,\\tilde{\\pi}(x)\\,$ via component means $\\,\\tilde{\\mu}_k\\,$, variances $\\,\\tilde{\\sigma}_k^2\\,$, and weights $\\,\\tilde{\\omega}_k\\,$. Sampling from $\\,q(x)\\,$ is done by first choosing a component index according to weights $\\,\\tilde{\\omega}_k\\,$ and then sampling a Gaussian with its corresponding $\\,\\tilde{\\mu}_k\\,$ and $\\,\\tilde{\\sigma}_k^2\\,$. The importance weights are $\\,w(x)=\\pi(x)/q(x)\\,$.\n\nBias quantification:\n- Using the control variate estimator with $\\,c=1\\,$ but replacing $\\,\\mathbb{E}_\\pi[h]\\,$ by $\\,\\mathbb{E}_{\\tilde{\\pi}}[h]\\,$, its large-sample limit equals\n$$\n\\lim_{n\\to\\infty}\\widehat{\\mu}_{\\text{cv}}(1) = \\mathbb{E}_\\pi[f] - \\big(\\mathbb{E}_\\pi[h] - \\mathbb{E}_{\\tilde{\\pi}}[h]\\big),\n$$\nso the asymptotic bias is\n$$\n\\text{Bias}_{\\infty} = -\\big(\\mathbb{E}_\\pi[h] - \\mathbb{E}_{\\tilde{\\pi}}[h]\\big).\n$$\nYou must compute both the empirical Monte Carlo bias for finite $\\,n\\,$ and the analytical asymptotic bias for each test case.\n\nAnalytical expectations:\n- For a Gaussian $\\,X\\sim\\mathcal{N}(\\mu,\\sigma^2)\\,$, the moments $\\,\\mathbb{E}[X^2]=\\sigma^2+\\mu^2\\,$ and $\\,\\mathbb{E}[X^3]=\\mu^3+3\\mu\\sigma^2\\,$ are well-tested facts. For a mixture $\\,\\sum_k \\omega_k \\mathcal{N}(\\mu_k,\\sigma_k^2)\\,$, $\\,\\mathbb{E}[X^2]=\\sum_k \\omega_k(\\sigma_k^2+\\mu_k^2)\\,$ and $\\,\\mathbb{E}[X^3]=\\sum_k \\omega_k(\\mu_k^3+3\\mu_k\\sigma_k^2)\\,$.\n\nTest suite:\n- Use sample size $\\,n=200000\\,$ and a deterministic random seed $\\,12345\\,$.\n- The four proposal cases $\\,\\tilde{\\pi}(x)\\,$ are:\n    1. Case A (happy path): $\\,\\tilde{\\omega}_1=0.6\\,$, $\\,\\tilde{\\sigma}_1=0.5\\,$, $\\,\\tilde{\\mu}_1=-2\\,$; $\\,\\tilde{\\omega}_2=0.4\\,$, $\\,\\tilde{\\sigma}_2=0.7\\,$, $\\,\\tilde{\\mu}_2=2\\,$.\n    2. Case B (missed mode): $\\,\\tilde{\\omega}_1=1.0\\,$, $\\,\\tilde{\\sigma}_1=0.5\\,$, $\\,\\tilde{\\mu}_1=-2\\,$; $\\,\\tilde{\\omega}_2=0.0\\,$, $\\,\\tilde{\\sigma}_2=0.7\\,$, $\\,\\tilde{\\mu}_2=2\\,$.\n    3. Case C (underweighted minor mode): $\\,\\tilde{\\omega}_1=0.85\\,$, $\\,\\tilde{\\sigma}_1=0.5\\,$, $\\,\\tilde{\\mu}_1=-2\\,$; $\\,\\tilde{\\omega}_2=0.15\\,$, $\\,\\tilde{\\sigma}_2=0.7\\,$, $\\,\\tilde{\\mu}_2=2\\,$.\n    4. Case D (variance mis-specified in minor mode): $\\,\\tilde{\\omega}_1=0.6\\,$, $\\,\\tilde{\\sigma}_1=0.5\\,$, $\\,\\tilde{\\mu}_1=-2\\,$; $\\,\\tilde{\\omega}_2=0.4\\,$, $\\,\\tilde{\\sigma}_2=0.35\\,$, $\\,\\tilde{\\mu}_2=2\\,$.\n- For every case, set $\\,c=1\\,$ in the control variate estimator and $\\,h(x)=x^2\\,$.\n\nRequired outputs:\n- For each test case, compute three quantities:\n    1. The empirical Monte Carlo bias of the control variate estimator: $\\,\\widehat{\\mu}_{\\text{cv}}(1) - \\mathbb{E}_\\pi[f]\\,$.\n    2. The empirical Monte Carlo bias of the plain self-normalized importance sampling estimator: $\\,\\widehat{\\mu} - \\mathbb{E}_\\pi[f]\\,$.\n    3. The analytical asymptotic bias of the control variate estimator: $\\, -\\big(\\mathbb{E}_\\pi[h]-\\mathbb{E}_{\\tilde{\\pi}}[h]\\big)\\,$.\n- Your program should produce a single line of output containing these $\\,12\\,$ results, ordered case-by-case and flattened, as a comma-separated list enclosed in square brackets, for example $\\,\\big[\\text{A1},\\text{A2},\\text{A3},\\text{B1},\\dots,\\text{D3}\\big]\\,$. There are no physical units involved in this problem.", "solution": "The problem requires an evaluation of self-normalized importance sampling with control variates, specifically focusing on the bias introduced by an imperfect approximation of the target posterior distribution. The target posterior $\\,\\pi(x)\\,$ is a bimodal Gaussian mixture. The proposal distribution $\\,q(x)\\,$ is also a Gaussian mixture, denoted $\\,\\tilde{\\pi}(x)\\,$, which serves as a Laplace-approximation-based model of $\\,\\pi(x)\\,$. We will compute the analytical and empirical biases of estimators for $\\,\\mathbb{E}_\\pi[f(X)]\\,$ where $\\,f(x)=x^3\\,$ for four different proposal distributions.\n\nFirst, we establish the ground truth by calculating the exact expectations of the estimand $\\,f(x)=x^3\\,$ and the control variate $\\,h(x)=x^2\\,$ with respect to the target posterior distribution $\\,\\pi(x)\\,$.\n\nThe target posterior is $\\,\\pi(x) = \\omega_1\\,\\mathcal{N}(x;\\mu_1,\\sigma_1^2) + \\omega_2\\,\\mathcal{N}(x;\\mu_2,\\sigma_2^2)\\,$ with parameters:\n- Component 1: $\\,\\mu_1=-2\\,$, $\\,\\sigma_1=0.5\\,$, $\\,\\omega_1=0.6\\,$\n- Component 2: $\\,\\mu_2=2\\,$, $\\,\\sigma_2=0.7\\,$, $\\,\\omega_2=0.4\\,$\n\nFor a Gaussian distribution $\\,X \\sim \\mathcal{N}(\\mu, \\sigma^2)\\,$, the second and third moments are $\\,\\mathbb{E}[X^2]=\\sigma^2+\\mu^2\\,$ and $\\,\\mathbb{E}[X^3]=\\mu^3+3\\mu\\sigma^2\\,$. For a mixture, the moments are the weighted sum of the component moments.\n\nFor component 1:\n$\\,\\mathbb{E}_1[X^2] = \\sigma_1^2 + \\mu_1^2 = (0.5)^2 + (-2)^2 = 0.25 + 4 = 4.25\\,$\n$\\,\\mathbb{E}_1[X^3] = \\mu_1^3 + 3\\mu_1\\sigma_1^2 = (-2)^3 + 3(-2)(0.5)^2 = -8 - 1.5 = -9.5\\,$\n\nFor component 2:\n$\\,\\mathbb{E}_2[X^2] = \\sigma_2^2 + \\mu_2^2 = (0.7)^2 + (2)^2 = 0.49 + 4 = 4.49\\,$\n$\\,\\mathbb{E}_2[X^3] = \\mu_2^3 + 3\\mu_2\\sigma_2^2 = (2)^3 + 3(2)(0.7)^2 = 8 + 2.94 = 10.94\\,$\n\nThe moments of the target posterior $\\,\\pi(x)\\,$ are:\n$\\,\\mathbb{E}_\\pi[h(X)] = \\mathbb{E}_\\pi[X^2] = \\omega_1\\mathbb{E}_1[X^2] + \\omega_2\\mathbb{E}_2[X^2] = (0.6)(4.25) + (0.4)(4.49) = 2.55 + 1.796 = 4.346\\,$\n$\\,\\mathbb{E}_\\pi[f(X)] = \\mathbb{E}_\\pi[X^3] = \\omega_1\\mathbb{E}_1[X^3] + \\omega_2\\mathbb{E}_2[X^3] = (0.6)(-9.5) + (0.4)(10.94) = -5.7 + 4.376 = -1.324\\,$\n\nThese are the true values used for calculating biases. The true value of the estimand is $\\,\\mathbb{E}_\\pi[f] = -1.324\\,$.\n\nThe procedure for each test case is as follows:\n1.  Define the proposal distribution $\\,q(x) = \\tilde{\\pi}(x)\\,$ from the case parameters.\n2.  Calculate the expectation of the control variate under the proposal, $\\,\\mathbb{E}_{\\tilde{\\pi}}[h(X)]\\,$.\n3.  Calculate the analytical asymptotic bias of the control variate estimator: $\\,\\text{Bias}_{\\infty} = -\\big(\\mathbb{E}_\\pi[h] - \\mathbb{E}_{\\tilde{\\pi}}[h]\\big)\\,$. This is the third required output for the case.\n4.  Perform a Monte Carlo simulation with $\\,n=200000\\,$ samples $\\,X_i \\sim q(x)\\,$ using a random seed of $\\,12345\\,$.\n5.  For each sample $\\,X_i\\,$, compute the importance weight $\\,w(X_i) = \\pi(X_i)/q(X_i)\\,$.\n6.  Compute the plain self-normalized importance sampling estimate: $\\,\\widehat{\\mu} = \\frac{\\sum_i w(X_i) f(X_i)}{\\sum_i w(X_i)}\\,$.\n7.  Compute the control variate estimate (with $\\,c=1\\,$ and approximate expectation for $\\,h\\,$): $\\,\\widehat{\\mu}_{\\text{cv}}(1) = \\widehat{\\mu} - \\left(\\frac{\\sum_i w(X_i) h(X_i)}{\\sum_i w(X_i)} - \\mathbb{E}_{\\tilde{\\pi}}[h]\\right)\\,$.\n8.  Calculate the empirical bias for the plain estimator: $\\,\\widehat{\\mu} - \\mathbb{E}_\\pi[f]\\,$. This is the second required output.\n9.  Calculate the empirical bias for the control variate estimator: $\\,\\widehat{\\mu}_{\\text{cv}}(1) - \\mathbb{E}_\\pi[f]\\,$. This is the first required output.\n\n### Case A: Happy Path\nThe proposal $\\,\\tilde{\\pi}(x)\\,$ is identical to the target $\\,\\pi(x)\\,$.\n- Parameters: $\\,\\tilde{\\omega}_1=0.6, \\tilde{\\sigma}_1=0.5, \\tilde{\\mu}_1=-2\\,$; $\\,\\tilde{\\omega}_2=0.4, \\tilde{\\sigma}_2=0.7, \\tilde{\\mu}_2=2\\,$.\n- $\\,\\mathbb{E}_{\\tilde{\\pi}}[h] = \\mathbb{E}_{\\pi}[h] = 4.346\\,$.\n- Analytical asymptotic bias: $\\,\\text{Bias}_{\\infty} = -(4.346 - 4.346) = 0\\,$.\n- In simulation, $\\,q(x) = \\pi(x)\\,$, so all importance weights $\\,w(x_i)=1\\,$. The estimators become standard Monte Carlo estimators. The empirical biases for both $\\,\\widehat{\\mu}\\,$ and $\\,\\widehat{\\mu}_{\\text{cv}}(1)\\,$ should be small, arising solely from sampling variability.\n\n### Case B: Missed Mode\nThe proposal $\\,\\tilde{\\pi}(x)\\,$ consists only of the first mode.\n- Parameters: $\\,\\tilde{\\omega}_1=1.0, \\tilde{\\sigma}_1=0.5, \\tilde{\\mu}_1=-2\\,$; $\\,\\tilde{\\omega}_2=0.0\\,$.\n- The proposal is $\\,\\tilde{\\pi}(x) = \\mathcal{N}(x; -2, 0.5^2)\\,$.\n- $\\,\\mathbb{E}_{\\tilde{\\pi}}[h] = \\tilde{\\sigma}_1^2 + \\tilde{\\mu}_1^2 = (0.5)^2 + (-2)^2 = 4.25\\,$.\n- Analytical asymptotic bias: $\\,\\text{Bias}_{\\infty} = -(4.346 - 4.25) = -0.096\\,$.\n- The simulation samples exclusively from the first mode. The importance weights $\\,w(X_i) = \\pi(X_i)/\\mathcal{N}(X_i; -2, 0.5^2)\\,$ will have very high variance, as no samples explore the region of the second mode of $\\,\\pi(x)\\,$. We expect large empirical biases for both estimators.\n\n### Case C: Underweighted Minor Mode\nThe proposal $\\,\\tilde{\\pi}(x)\\,$ correctly identifies both modes but misjudges their relative weights.\n- Parameters: $\\,\\tilde{\\omega}_1=0.85, \\tilde{\\sigma}_1=0.5, \\tilde{\\mu}_1=-2\\,$; $\\,\\tilde{\\omega}_2=0.15, \\tilde{\\sigma}_2=0.7, \\tilde{\\mu}_2=2\\,$.\n- The component expectations are $\\,\\mathbb{E}_1[X^2] = 4.25\\,$ and $\\,\\mathbb{E}_2[X^2] = 4.49\\,$.\n- $\\,\\mathbb{E}_{\\tilde{\\pi}}[h] = (0.85)(4.25) + (0.15)(4.49) = 3.6125 + 0.6735 = 4.286\\,$.\n- Analytical asymptotic bias: $\\,\\text{Bias}_{\\infty} = -(4.346 - 4.286) = -0.06\\,$.\n- Sampling undersamples the second mode. Importance weights for samples from the second mode will be larger on average ($\\,\\approx\\omega_2/\\tilde{\\omega}_2 = 0.4/0.15 \\approx 2.67\\,$), increasing the variance of the estimators compared to Case A. We expect moderate empirical biases.\n\n### Case D: Variance Mis-specified in Minor Mode\nThe proposal $\\,\\tilde{\\pi}(x)\\,$ has correct weights and means, but the variance of the second component is too small.\n- Parameters: $\\,\\tilde{\\omega}_1=0.6, \\tilde{\\sigma}_1=0.5, \\tilde{\\mu}_1=-2\\,$; $\\,\\tilde{\\omega}_2=0.4, \\tilde{\\sigma}_2=0.35, \\tilde{\\mu}_2=2\\,$.\n- Component 1 expectation is $\\,\\mathbb{E}_1[X^2] = 4.25\\,$. For component 2 of the proposal: $\\,\\mathbb{E}_{\\tilde{\\pi},2}[X^2] = (0.35)^2 + 2^2 = 0.1225 + 4 = 4.1225\\,$.\n- $\\,\\mathbb{E}_{\\tilde{\\pi}}[h] = (0.6)(4.25) + (0.4)(4.1225) = 2.55 + 1.649 = 4.199\\,$.\n- Analytical asymptotic bias: $\\,\\text{Bias}_{\\infty} = -(4.346 - 4.199) = -0.147\\,$.\n- The proposal for the second mode is narrower than the target. Samples drawn from this component that fall in the tails of the true component distribution will receive extremely high weights, leading to high estimator variance. We expect significant empirical biases.\n\nThe empirical results from the simulation will reflect not only the asymptotic bias but also the finite-sample bias and variance of the estimators. The asymptotic bias provides a theoretical baseline for the bias of the control variate estimator, which is a deterministic consequence of the mismatch between the approximate and true posteriors.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Implements and evaluates self-normalized importance sampling with a control variate\n    for a bimodal target distribution, quantifying biases from imperfect proposals.\n    \"\"\"\n    # Problem-wide parameters\n    N_SAMPLES = 200000\n    SEED = 12345\n    ESTIMAND_FUNC = lambda x: x**3\n    CV_FUNC = lambda x: x**2\n    C_VAL = 1.0\n\n    # Target posterior distribution pi(x) parameters\n    pi_params = {\n        'mus': np.array([-2.0, 2.0]),\n        'sigmas': np.array([0.5, 0.7]),\n        'weights': np.array([0.6, 0.4])\n    }\n\n    # Test cases for the proposal distribution q(x) = pi_tilde(x)\n    test_cases = [\n        # Case A: Happy path (q = pi)\n        {\n            'mus': np.array([-2.0, 2.0]),\n            'sigmas': np.array([0.5, 0.7]),\n            'weights': np.array([0.6, 0.4])\n        },\n        # Case B: Missed mode\n        {\n            'mus': np.array([-2.0, 2.0]),\n            'sigmas': np.array([0.5, 0.7]),\n            'weights': np.array([1.0, 0.0])\n        },\n        # Case C: Underweighted minor mode\n        {\n            'mus': np.array([-2.0, 2.0]),\n            'sigmas': np.array([0.5, 0.7]),\n            'weights': np.array([0.85, 0.15])\n        },\n        # Case D: Variance mis-specified in minor mode\n        {\n            'mus': np.array([-2.0, 2.0]),\n            'sigmas': np.array([0.5, 0.35]),\n            'weights': np.array([0.6, 0.4])\n        }\n    ]\n\n    def gaussian_mixture_pdf(x, params):\n        \"\"\"Calculates the PDF of a Gaussian mixture.\"\"\"\n        pdf_val = 0.0\n        for mu, sigma, weight in zip(params['mus'], params['sigmas'], params['weights']):\n            if weight > 0:\n                pdf_val += weight * norm.pdf(x, loc=mu, scale=sigma)\n        return pdf_val\n\n    def gaussian_mixture_moment(k, params):\n        \"\"\"Calculates the k-th moment of a Gaussian mixture.\"\"\"\n        total_moment = 0.0\n        for mu, sigma, weight in zip(params['mus'], params['sigmas'], params['weights']):\n            if weight > 0:\n                if k == 2:\n                    moment = sigma**2 + mu**2\n                elif k == 3:\n                    moment = mu**3 + 3 * mu * sigma**2\n                else:\n                    raise ValueError(\"Only k=2 and k=3 are supported.\")\n                total_moment += weight * moment\n        return total_moment\n    \n    def sample_from_mixture(n, params, rng):\n        \"\"\"Generates n samples from a Gaussian mixture.\"\"\"\n        n_components = len(params['mus'])\n        # Handle cases where some weights are 0\n        active_indices = np.where(params['weights'] > 0)[0]\n        active_weights = params['weights'][active_indices]\n        \n        # Normalize weights in case they don't sum to 1 after filtering\n        active_weights /= np.sum(active_weights)\n\n        component_choices = rng.choice(active_indices, size=n, p=active_weights)\n        samples = rng.normal(\n            loc=params['mus'][component_choices],\n            scale=params['sigmas'][component_choices]\n        )\n        return samples\n\n    # Calculate true expectations for the target distribution pi\n    true_f_exp = gaussian_mixture_moment(3, pi_params)  # E_pi[f] = E_pi[x^3]\n    true_h_exp = gaussian_mixture_moment(2, pi_params)  # E_pi[h] = E_pi[x^2]\n\n    # Initialize random number generator\n    rng = np.random.default_rng(SEED)\n    \n    results = []\n    \n    for q_params in test_cases:\n        # 1. Analytical calculations for the proposal q\n        exp_h_q = gaussian_mixture_moment(2, q_params)\n        \n        # 2. Analytical asymptotic bias for the CV estimator\n        analytical_bias_cv = -(true_h_exp - exp_h_q)\n        \n        # 3. Monte Carlo Simulation\n        samples = sample_from_mixture(N_SAMPLES, q_params, rng)\n        \n        # 4. Calculate importance weights\n        pi_pdf_vals = gaussian_mixture_pdf(samples, pi_params)\n        q_pdf_vals = gaussian_mixture_pdf(samples, q_params)\n        \n        # Avoid division by zero if q_pdf is zero (should not happen with this setup)\n        # However, it's good practice. A zero q_pdf where pi_pdf is non-zero implies\n        # infinite variance, which the results will reflect via large weights.\n        weights = np.divide(pi_pdf_vals, q_pdf_vals, out=np.zeros_like(pi_pdf_vals), where=q_pdf_vals!=0)\n\n        # 5. Calculate estimators\n        f_vals = ESTIMAND_FUNC(samples)\n        h_vals = CV_FUNC(samples)\n        \n        sum_weights = np.sum(weights)\n        \n        # Plain self-normalized IS estimator for E[f]\n        mu_hat = np.sum(weights * f_vals) / sum_weights\n        \n        # Self-normalized IS estimator for E[h]\n        mu_hat_h = np.sum(weights * h_vals) / sum_weights\n        \n        # Control variate estimator\n        mu_hat_cv = mu_hat - C_VAL * (mu_hat_h - exp_h_q)\n\n        # 6. Calculate empirical biases\n        empirical_bias_cv = mu_hat_cv - true_f_exp\n        empirical_bias_plain = mu_hat - true_f_exp\n        \n        # Store results in the required order\n        results.extend([empirical_bias_cv, empirical_bias_plain, analytical_bias_cv])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.8f}' for r in results)}]\")\n\nsolve()\n```", "id": "3289121"}]}