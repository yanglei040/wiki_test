{"hands_on_practices": [{"introduction": "The efficiency of path sampling methods like Thermodynamic Integration depends critically on the statistical properties of the chosen path. This exercise provides a foundational understanding of this principle by asking you to derive a closed-form expression for the total variance of the log-density increment along a path connecting two Gaussian distributions. By solving this problem analytically, you will gain direct insight into how the dissimilarity between the initial and final distributions governs the difficulty of the estimation task [@problem_id:3328156].", "problem": "Consider two unnormalized densities on the real line, $q_{0}(x) = \\exp\\!\\big(-x^{2}/(2\\sigma_{0}^{2})\\big)$ and $q_{1}(x) = \\exp\\!\\big(-x^{2}/(2\\sigma_{1}^{2})\\big)$, where $\\sigma_{0} > 0$ and $\\sigma_{1} > 0$. Define the geometric path $\\{p_{t}(x) : t \\in [0,1]\\}$ by\n$$\np_{t}(x) \\propto q_{0}(x)^{1-t}\\,q_{1}(x)^{t},\n$$\nand the log-density increment along the path by\n$$\nU(x) \\equiv \\ln q_{1}(x) - \\ln q_{0}(x).\n$$\nA path-stability functional that quantifies overlap for thermodynamic integration (TI) and geometric-bridge path sampling is\n$$\nS(\\sigma_{0},\\sigma_{1}) \\equiv \\int_{0}^{1} \\mathrm{Var}_{p_{t}}\\!\\big[U(X)\\big] \\,\\mathrm{d}t,\n$$\nwhere the variance is taken with respect to $p_{t}$. Starting only from the definitions above and standard properties of the Gaussian distribution, derive a closed-form analytic expression for $S(\\sigma_{0},\\sigma_{1})$ in terms of $\\sigma_{0}$ and $\\sigma_{1}$ only. Your final answer must be a single simplified analytic expression. Do not include any units. No numerical rounding is required.", "solution": "The problem is valid as it is scientifically grounded in statistical mechanics, well-posed with all necessary information, and stated objectively. We can proceed to derive the closed-form expression for the path-stability functional $S(\\sigma_{0},\\sigma_{1})$.\n\nThe derivation proceeds in three main steps:\n1.  Determine the normalized probability distribution $p_t(x)$ for any given $t \\in [0,1]$.\n2.  Calculate the variance of the log-density increment, $\\mathrm{Var}_{p_{t}}[U(X)]$, as a function of $t$.\n3.  Integrate this variance over $t$ from $0$ to $1$ to find $S(\\sigma_{0},\\sigma_{1})$.\n\nStep 1: Characterize the probability distribution $p_t(x)$.\nThe unnormalized density path is given by $p_{t}(x) \\propto q_{0}(x)^{1-t}\\,q_{1}(x)^{t}$. Substituting the expressions for $q_{0}(x)$ and $q_{1}(x)$:\n$$\np_t(x) \\propto \\left(\\exp\\left(-\\frac{x^2}{2\\sigma_0^2}\\right)\\right)^{1-t} \\left(\\exp\\left(-\\frac{x^2}{2\\sigma_1^2}\\right)\\right)^{t}\n$$\nUsing the property $(\\exp(a))^b = \\exp(ab)$, we combine the exponents:\n$$\np_t(x) \\propto \\exp\\left(-\\frac{(1-t)x^2}{2\\sigma_0^2} - \\frac{tx^2}{2\\sigma_1^2}\\right) = \\exp\\left(-\\frac{x^2}{2}\\left(\\frac{1-t}{\\sigma_0^2} + \\frac{t}{\\sigma_1^2}\\right)\\right)\n$$\nThis functional form is that of a zero-mean Gaussian distribution, $\\mathcal{N}(0, \\sigma_t^2)$, with an unnormalized density kernel $\\exp(-x^2/(2\\sigma_t^2))$. By comparing the exponents, we can define the variance $\\sigma_t^2$ of the distribution $p_t(x)$ for each $t$:\n$$\n\\frac{1}{2\\sigma_t^2} = \\frac{1}{2}\\left(\\frac{1-t}{\\sigma_0^2} + \\frac{t}{\\sigma_1^2}\\right) \\implies \\frac{1}{\\sigma_t^2} = \\frac{1-t}{\\sigma_0^2} + \\frac{t}{\\sigma_1^2}\n$$\nThe normalized probability density function is therefore:\n$$\np_t(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma_t} \\exp\\left(-\\frac{x^2}{2\\sigma_t^2}\\right)\n$$\n\nStep 2: Calculate the variance $\\mathrm{Var}_{p_{t}}[U(X)]$.\nFirst, we find an explicit expression for the log-density increment $U(x)$:\n$$\nU(x) = \\ln q_{1}(x) - \\ln q_{0}(x) = \\ln\\left(\\exp\\left(-\\frac{x^2}{2\\sigma_1^2}\\right)\\right) - \\ln\\left(\\exp\\left(-\\frac{x^2}{2\\sigma_0^2}\\right)\\right)\n$$\n$$\nU(x) = -\\frac{x^2}{2\\sigma_1^2} + \\frac{x^2}{2\\sigma_0^2} = x^2 \\left(\\frac{1}{2\\sigma_0^2} - \\frac{1}{2\\sigma_1^2}\\right)\n$$\nLet's define a constant $C$ that is independent of $x$ and $t$:\n$$\nC \\equiv \\frac{1}{2\\sigma_0^2} - \\frac{1}{2\\sigma_1^2} = \\frac{\\sigma_1^2 - \\sigma_0^2}{2\\sigma_0^2\\sigma_1^2}\n$$\nSo, $U(x) = C x^2$. We need to compute the variance of $U(X)$ where $X$ is a random variable drawn from $p_t(x)$.\n$$\n\\mathrm{Var}_{p_{t}}[U(X)] = \\mathrm{Var}_{p_{t}}[C X^2] = C^2 \\mathrm{Var}_{p_{t}}[X^2]\n$$\nThe variance of $X^2$ is given by $\\mathrm{Var}_{p_{t}}[X^2] = \\mathbb{E}_{p_t}[(X^2)^2] - (\\mathbb{E}_{p_t}[X^2])^2 = \\mathbb{E}_{p_t}[X^4] - (\\mathbb{E}_{p_t}[X^2])^2$.\nFor a random variable $X \\sim \\mathcal{N}(0, \\sigma_t^2)$, the second and fourth moments are known to be:\n$$\n\\mathbb{E}_{p_t}[X^2] = \\sigma_t^2\n$$\n$$\n\\mathbb{E}_{p_t}[X^4] = 3(\\sigma_t^2)^2 = 3\\sigma_t^4\n$$\nSubstituting these moments into the expression for $\\mathrm{Var}_{p_{t}}[X^2]$:\n$$\n\\mathrm{Var}_{p_{t}}[X^2] = 3\\sigma_t^4 - (\\sigma_t^2)^2 = 2\\sigma_t^4\n$$\nNow we can express $\\mathrm{Var}_{p_{t}}[U(X)]$ in terms of $C$ and $\\sigma_t$:\n$$\n\\mathrm{Var}_{p_{t}}[U(X)] = C^2 (2\\sigma_t^4) = 2 \\left(\\frac{\\sigma_1^2 - \\sigma_0^2}{2\\sigma_0^2\\sigma_1^2}\\right)^2 \\sigma_t^4 = \\frac{(\\sigma_1^2 - \\sigma_0^2)^2}{2(\\sigma_0^2\\sigma_1^2)^2}\\sigma_t^4\n$$\nWe express $\\sigma_t^4$ using the relationship for $1/\\sigma_t^2$:\n$$\n\\sigma_t^2 = \\left(\\frac{1-t}{\\sigma_0^2} + \\frac{t}{\\sigma_1^2}\\right)^{-1} = \\left(\\frac{(1-t)\\sigma_1^2 + t\\sigma_0^2}{\\sigma_0^2\\sigma_1^2}\\right)^{-1} = \\frac{\\sigma_0^2\\sigma_1^2}{(1-t)\\sigma_1^2 + t\\sigma_0^2}\n$$\nTherefore,\n$$\n\\sigma_t^4 = \\frac{(\\sigma_0^2\\sigma_1^2)^2}{((1-t)\\sigma_1^2 + t\\sigma_0^2)^2}\n$$\nSubstituting this into the expression for the variance:\n$$\n\\mathrm{Var}_{p_{t}}[U(X)] = \\frac{(\\sigma_1^2 - \\sigma_0^2)^2}{2(\\sigma_0^2\\sigma_1^2)^2} \\frac{(\\sigma_0^2\\sigma_1^2)^2}{((1-t)\\sigma_1^2 + t\\sigma_0^2)^2} = \\frac{(\\sigma_1^2 - \\sigma_0^2)^2}{2((1-t)\\sigma_1^2 + t\\sigma_0^2)^2}\n$$\n\nStep 3: Integrate $\\mathrm{Var}_{p_{t}}[U(X)]$ over $t \\in [0,1]$.\nThe path-stability functional is $S(\\sigma_{0},\\sigma_{1}) = \\int_{0}^{1} \\mathrm{Var}_{p_{t}}[U(X)] \\mathrm{d}t$.\n$$\nS(\\sigma_{0},\\sigma_{1}) = \\int_{0}^{1} \\frac{(\\sigma_1^2 - \\sigma_0^2)^2}{2((1-t)\\sigma_1^2 + t\\sigma_0^2)^2} \\mathrm{d}t\n$$\nThe numerator is constant with respect to $t$, so we can pull it out of the integral:\n$$\nS(\\sigma_{0},\\sigma_{1}) = \\frac{(\\sigma_1^2 - \\sigma_0^2)^2}{2} \\int_{0}^{1} \\frac{1}{(\\sigma_1^2 + t(\\sigma_0^2 - \\sigma_1^2))^2} \\mathrm{d}t\n$$\nLet's evaluate the integral. Let $u = \\sigma_1^2 + t(\\sigma_0^2 - \\sigma_1^2)$. Then $\\mathrm{d}u = (\\sigma_0^2 - \\sigma_1^2) \\mathrm{d}t$. We change the integration variable from $t$ to $u$.\nAt $t=0$, $u = \\sigma_1^2$. At $t=1$, $u = \\sigma_1^2 + (\\sigma_0^2 - \\sigma_1^2) = \\sigma_0^2$.\nThe integral becomes:\n$$\n\\int_{\\sigma_1^2}^{\\sigma_0^2} \\frac{1}{u^2} \\frac{\\mathrm{d}u}{\\sigma_0^2 - \\sigma_1^2} = \\frac{1}{\\sigma_0^2 - \\sigma_1^2} \\int_{\\sigma_1^2}^{\\sigma_0^2} u^{-2} \\mathrm{d}u\n$$\nAssuming $\\sigma_0^2 \\neq \\sigma_1^2$:\n$$\n= \\frac{1}{\\sigma_0^2 - \\sigma_1^2} \\left[-\\frac{1}{u}\\right]_{\\sigma_1^2}^{\\sigma_0^2} = \\frac{1}{\\sigma_0^2 - \\sigma_1^2} \\left(-\\frac{1}{\\sigma_0^2} - \\left(-\\frac{1}{\\sigma_1^2}\\right)\\right)\n$$\n$$\n= \\frac{1}{\\sigma_0^2 - \\sigma_1^2} \\left(\\frac{1}{\\sigma_1^2} - \\frac{1}{\\sigma_0^2}\\right) = \\frac{1}{\\sigma_0^2 - \\sigma_1^2} \\left(\\frac{\\sigma_0^2 - \\sigma_1^2}{\\sigma_0^2\\sigma_1^2}\\right) = \\frac{1}{\\sigma_0^2\\sigma_1^2}\n$$\nNow, substitute this result back into the expression for $S(\\sigma_{0},\\sigma_{1})$:\n$$\nS(\\sigma_{0},\\sigma_{1}) = \\frac{(\\sigma_1^2 - \\sigma_0^2)^2}{2} \\left(\\frac{1}{\\sigma_0^2\\sigma_1^2}\\right) = \\frac{(\\sigma_1^2 - \\sigma_0^2)^2}{2\\sigma_0^2\\sigma_1^2}\n$$\nThis expression can be further simplified. We can rewrite it as:\n$$\nS(\\sigma_{0},\\sigma_{1}) = \\frac{1}{2} \\left(\\frac{\\sigma_1^2 - \\sigma_0^2}{\\sigma_0\\sigma_1}\\right)^2 = \\frac{1}{2} \\left(\\frac{\\sigma_1^2}{\\sigma_0\\sigma_1} - \\frac{\\sigma_0^2}{\\sigma_0\\sigma_1}\\right)^2 = \\frac{1}{2} \\left(\\frac{\\sigma_1}{\\sigma_0} - \\frac{\\sigma_0}{\\sigma_1}\\right)^2\n$$\nThis is the final closed-form analytic expression. If $\\sigma_0 = \\sigma_1$, then $U(x)=0$, so $\\mathrm{Var}_{p_t}[U(X)]=0$ and $S(\\sigma_0, \\sigma_0)=0$, which is consistent with the derived formula.", "answer": "$$\n\\boxed{\\frac{1}{2}\\left(\\frac{\\sigma_{1}}{\\sigma_{0}} - \\frac{\\sigma_{0}}{\\sigma_{1}}\\right)^{2}}\n$$", "id": "3328156"}, {"introduction": "Building upon the theoretical link between path choice and variance, this practice explores a concrete question of estimator design. You will compare the standard power posterior path, which tempers the likelihood, with an alternative path that tempers the prior distribution. Through a combination of analytical derivation and implementation for a conjugate Gaussian model, you will discover the conditions under which one path is superior to the other, a crucial skill for constructing efficient estimators in practice [@problem_id:3328093].", "problem": "You are to implement and analyze two path-based estimators of the log-evidence (log marginal likelihood) in a Bayesian conjugate Gaussian model, and determine conditions under which tempering the prior within a path defined by $\\pi_\\beta \\propto p(y \\mid x) \\, p(x)^\\beta$ can reduce the Monte Carlo variance of thermodynamic integration relative to the standard power posterior path $\\pi_\\beta \\propto p(y \\mid x)^\\beta \\, p(x)$.\n\nConsider the model with prior $p(x) = \\mathcal{N}(x; 0, \\tau^2)$ and likelihood $p(y \\mid x) = \\mathcal{N}(y; x, \\sigma^2)$, where $\\mathcal{N}(a; b, c)$ denotes a Gaussian density in $a$ with mean $b$ and variance $c$. The goal is to estimate $\\log p(y)$ using thermodynamic integration along two different paths and to compute the asymptotic Monte Carlo variance of the corresponding numerical quadrature estimators when the expectation at each inverse-temperature parameter $\\beta$ is estimated by an independent Monte Carlo average of size $n$.\n\nBase definitions to use:\n- Bayes’ rule and normalizing constants: for any unnormalized density $q_\\beta(x)$ with normalizing constant $Z(\\beta) = \\int q_\\beta(x) \\, dx$, the normalized density is $\\pi_\\beta(x) = q_\\beta(x) / Z(\\beta)$.\n- The thermodynamic integration identity: if $q_\\beta(x)$ is differentiable in $\\beta$ and $\\partial_\\beta \\log q_\\beta(x)$ exists with sufficient integrability, then $\\frac{d}{d\\beta} \\log Z(\\beta) = \\mathbb{E}_{\\pi_\\beta} \\left[ \\partial_\\beta \\log q_\\beta(X) \\right]$ and hence $\\log Z(1) - \\log Z(0) = \\int_0^1 \\mathbb{E}_{\\pi_\\beta} \\left[ \\partial_\\beta \\log q_\\beta(X) \\right] \\, d\\beta$.\n- For a numerical quadrature with nodes $\\{\\beta_j\\}_{j=0}^{K-1}$ and weights $\\{w_j\\}_{j=0}^{K-1}$ approximating $\\int_0^1 g(\\beta) \\, d\\beta \\approx \\sum_{j=0}^{K-1} w_j g(\\beta_j)$, and independent Monte Carlo estimates $\\widehat{g}(\\beta_j)$ with $\\operatorname{Var}(\\widehat{g}(\\beta_j)) = \\operatorname{Var}_{\\pi_{\\beta_j}}(f(X))/n$, the asymptotic variance of the quadrature estimator is $\\sum_{j=0}^{K-1} w_j^2 \\, \\operatorname{Var}_{\\pi_{\\beta_j}}(f(X))/n$.\n\nPaths to compare:\n- Power posterior path: $q_\\beta^{\\mathrm{PP}}(x) = p(y \\mid x)^\\beta \\, p(x)$, so that $\\partial_\\beta \\log q_\\beta^{\\mathrm{PP}}(x) = \\log p(y \\mid x)$ and $\\pi_\\beta^{\\mathrm{PP}}(x) \\propto p(y \\mid x)^\\beta \\, p(x)$.\n- Prior-tempering path: $q_\\beta^{\\mathrm{PT}}(x) = p(y \\mid x) \\, p(x)^\\beta$, so that $\\partial_\\beta \\log q_\\beta^{\\mathrm{PT}}(x) = \\log p(x)$ and $\\pi_\\beta^{\\mathrm{PT}}(x) \\propto p(y \\mid x) \\, p(x)^\\beta$.\n\nIn the conjugate Gaussian setting, both $\\pi_\\beta^{\\mathrm{PP}}$ and $\\pi_\\beta^{\\mathrm{PT}}$ are Gaussian for each $\\beta \\in [0,1]$. You will exploit this to derive closed-form expressions for the variance, under $\\pi_\\beta$, of the relevant integrands $f_{\\mathrm{PP}}(x) = \\log p(y \\mid x)$ and $f_{\\mathrm{PT}}(x) = \\log p(x)$ at each $\\beta$.\n\nTask:\n- Derive, from first principles and the Gaussian identities, explicit formulas for the mean and variance of $\\pi_\\beta^{\\mathrm{PP}}$ and $\\pi_\\beta^{\\mathrm{PT}}$ as functions of $\\beta$, $y$, $\\tau^2$, and $\\sigma^2$.\n- From these, derive $\\operatorname{Var}_{\\pi_\\beta^{\\mathrm{PP}}}(\\log p(y \\mid X))$ and $\\operatorname{Var}_{\\pi_\\beta^{\\mathrm{PT}}}(\\log p(X))$ as functions of $\\beta$, $y$, $\\tau^2$, and $\\sigma^2$.\n- Using the uniform trapezoidal rule with $K$ grid points $\\beta_j = j/(K-1)$ for $j \\in \\{0, \\dots, K-1\\}$ and weights $w_0 = w_{K-1} = \\Delta \\beta / 2$, $w_j = \\Delta \\beta$ for $j \\in \\{1, \\dots, K-2\\}$, where $\\Delta \\beta = 1/(K-1)$, compute the asymptotic Monte Carlo variance of the thermodynamic integration estimator for each path, assuming $n$ independent samples are used at each grid point and independence across grid points.\n- State conditions on $\\sigma^2$, $\\tau^2$, and $y$ under which the prior-tempering path reduces the variance relative to the power posterior path. Your program must implement the variance computations for a finite set of parameter values and report which path is better in each case. The numerical value of the asymptotic variance for each path must be reported.\n\nTest suite:\nProvide results for the following parameter sets $(y, \\tau^2, \\sigma^2, K, n)$:\n- Case $1$: $(y, \\tau^2, \\sigma^2, K, n) = (0.5, 1.0, 1.0, 41, 1000)$.\n- Case $2$: $(y, \\tau^2, \\sigma^2, K, n) = (3.0, 10.0, 1.0, 41, 1000)$.\n- Case $3$: $(y, \\tau^2, \\sigma^2, K, n) = (3.0, 0.2, 5.0, 41, 1000)$.\n- Case $4$ (boundary discretization check): $(y, \\tau^2, \\sigma^2, K, n) = (1.0, 1.0, 0.1, 2, 1000)$.\n\nRequired final output format:\n- Your program should produce a single line of output containing a list of per-case results. Each per-case result must be a list of three entries: the asymptotic variance for the power posterior path (a float), the asymptotic variance for the prior-tempering path (a float), and a boolean indicating whether the prior-tempering path has strictly smaller variance than the power posterior path. The overall output must be a single list of these per-case lists, printed without extra whitespace beyond what Python’s default list-to-string conversion includes.\n- For example, the output must look like $[[v_{1,\\mathrm{PP}}, v_{1,\\mathrm{PT}}, b_1], [v_{2,\\mathrm{PP}}, v_{2,\\mathrm{PT}}, b_2], [v_{3,\\mathrm{PP}}, v_{3,\\mathrm{PT}}, b_3], [v_{4,\\mathrm{PP}}, v_{4,\\mathrm{PT}}, b_4]]$, where $v_{i,\\cdot}$ are floats and $b_i$ booleans. You must print the floats with at most $6$ decimal places.", "solution": "The problem requires the derivation and comparison of the asymptotic Monte Carlo variance for two thermodynamic integration (TI) paths used to estimate the log marginal likelihood, $\\log p(y)$, for a conjugate Gaussian model. The two paths are the standard power posterior (PP) path and a prior-tempering (PT) path.\n\nThe model is defined by a Gaussian prior and likelihood:\n- Prior: $p(x) = \\mathcal{N}(x; 0, \\tau^2) = \\frac{1}{\\sqrt{2\\pi\\tau^2}} \\exp\\left(-\\frac{x^2}{2\\tau^2}\\right)$\n- Likelihood: $p(y \\mid x) = \\mathcal{N}(y; x, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y-x)^2}{2\\sigma^2}\\right)$\n\nThe core of thermodynamic integration is the identity $\\log Z(1) - \\log Z(0) = \\int_0^1 \\mathbb{E}_{\\pi_\\beta} \\left[ \\partial_\\beta \\log q_\\beta(X) \\right] \\, d\\beta$, where $\\pi_\\beta(x) = q_\\beta(x)/Z(\\beta)$ is a family of distributions indexed by $\\beta \\in [0,1]$ that connects a tractable initial distribution ($\\beta=0$) to the target distribution ($\\beta=1$). The asymptotic variance of a numerical quadrature estimator of this integral is $\\mathcal{V} = \\frac{1}{n} \\sum_{j=0}^{K-1} w_j^2 \\operatorname{Var}_{\\pi_{\\beta_j}}(f(X))$, where $f(X) = \\partial_\\beta \\log q_\\beta(X)$ and $n$ is the number of Monte Carlo samples per grid point $\\beta_j$.\n\nWe will first derive the parameters of the intermediate distributions $\\pi_\\beta(x)$ and the variance of the integrand for each path. A key mathematical tool is the property that the product of Gaussian densities results in another (unnormalized) Gaussian density. A generic unnormalized log-Gaussian density of the form $-\\frac{1}{2v}x^2 + \\frac{\\mu}{v}x + C$ corresponds to a Gaussian distribution $\\mathcal{N}(x; \\mu, v)$.\n\n### 1. Power Posterior (PP) Path Analysis\nFor the PP path, the unnormalized density is $q_\\beta^{\\mathrm{PP}}(x) = p(y \\mid x)^\\beta p(x)$. The corresponding normalized density is $\\pi_\\beta^{\\mathrm{PP}}(x)$. The log-density is:\n$$ \\log q_\\beta^{\\mathrm{PP}}(x) = \\beta \\log p(y \\mid x) + \\log p(x) + C_1 = -\\beta \\frac{(x-y)^2}{2\\sigma^2} - \\frac{x^2}{2\\tau^2} + C_2 $$\nExpanding and collecting terms in $x$:\n$$ \\log q_\\beta^{\\mathrm{PP}}(x) = -\\frac{1}{2} \\left[ \\left(\\frac{\\beta}{\\sigma^2} + \\frac{1}{\\tau^2}\\right)x^2 - \\frac{2\\beta y}{\\sigma^2} x \\right] + C_3 $$\nThis shows that $\\pi_\\beta^{\\mathrm{PP}}(x)$ is a Gaussian distribution $\\mathcal{N}(x; \\mu_\\beta^{\\mathrm{PP}}, v_\\beta^{\\mathrm{PP}})$. By completing the square, we identify the inverse variance and the mean-variance product:\n$$ \\frac{1}{v_\\beta^{\\mathrm{PP}}} = \\frac{\\beta}{\\sigma^2} + \\frac{1}{\\tau^2} \\implies v_\\beta^{\\mathrm{PP}} = \\frac{\\sigma^2\\tau^2}{\\beta\\tau^2 + \\sigma^2} $$\n$$ \\frac{\\mu_\\beta^{\\mathrm{PP}}}{v_\\beta^{\\mathrm{PP}}} = \\frac{\\beta y}{\\sigma^2} \\implies \\mu_\\beta^{\\mathrm{PP}} = v_\\beta^{\\mathrm{PP}} \\frac{\\beta y}{\\sigma^2} = \\frac{\\beta y \\tau^2}{\\beta\\tau^2 + \\sigma^2} $$\nThe TI integrand for this path is $f_{\\mathrm{PP}}(x) = \\partial_\\beta \\log q_\\beta^{\\mathrm{PP}}(x) = \\log p(y \\mid x) = -\\frac{(x-y)^2}{2\\sigma^2} - \\frac{1}{2}\\log(2\\pi\\sigma^2)$. We need its variance, $\\operatorname{Var}_{\\pi_\\beta^{\\mathrm{PP}}}(f_{\\mathrm{PP}}(X))$. The constant term does not affect variance.\n$$ \\operatorname{Var}_{\\pi_\\beta^{\\mathrm{PP}}}(\\log p(y \\mid X)) = \\operatorname{Var}_{\\pi_\\beta^{\\mathrm{PP}}}\\left(-\\frac{(X-y)^2}{2\\sigma^2}\\right) = \\frac{1}{4\\sigma^4} \\operatorname{Var}((X-y)^2) $$\nFor $X \\sim \\mathcal{N}(\\mu, v)$, the random variable $Y = X-y$ follows $\\mathcal{N}(\\mu-y, v)$. The variance of $Y^2$ is $\\operatorname{Var}(Y^2) = 2v(v + 2(\\mu-y)^2)$. Substituting $\\mu = \\mu_\\beta^{\\mathrm{PP}}$ and $v = v_\\beta^{\\mathrm{PP}}$:\n$$ \\operatorname{Var}_{\\pi_\\beta^{\\mathrm{PP}}} = \\frac{1}{4\\sigma^4} \\left[ 2v_\\beta^{\\mathrm{PP}} (v_\\beta^{\\mathrm{PP}} + 2(\\mu_\\beta^{\\mathrm{PP}}-y)^2) \\right] = \\frac{v_\\beta^{\\mathrm{PP}}}{2\\sigma^4} (v_\\beta^{\\mathrm{PP}} + 2(\\mu_\\beta^{\\mathrm{PP}}-y)^2) $$\nSubstituting the expressions for $\\mu_\\beta^{\\mathrm{PP}}$ and $v_\\beta^{\\mathrm{PP}}$ and simplifying yields the per-stratum variance for the PP path:\n$$ V_{\\mathrm{PP}}(\\beta) = \\frac{y^2 \\sigma^2 \\tau^2}{(\\beta\\tau^2 + \\sigma^2)^3} + \\frac{\\tau^4}{2(\\beta\\tau^2 + \\sigma^2)^2} $$\n\n### 2. Prior-Tempering (PT) Path Analysis\nFor the PT path, $q_\\beta^{\\mathrm{PT}}(x) = p(y \\mid x) p(x)^\\beta$. The log-density is:\n$$ \\log q_\\beta^{\\mathrm{PT}}(x) = \\log p(y \\mid x) + \\beta \\log p(x) + C_4 = -\\frac{(x-y)^2}{2\\sigma^2} - \\beta\\frac{x^2}{2\\tau^2} + C_5 $$\n$$ \\log q_\\beta^{\\mathrm{PT}}(x) = -\\frac{1}{2} \\left[ \\left(\\frac{1}{\\sigma^2} + \\frac{\\beta}{\\tau^2}\\right)x^2 - \\frac{2y}{\\sigma^2} x \\right] + C_6 $$\nThis corresponds to a Gaussian distribution $\\pi_\\beta^{\\mathrm{PT}}(x) = \\mathcal{N}(x; \\mu_\\beta^{\\mathrm{PT}}, v_\\beta^{\\mathrm{PT}})$ with parameters:\n$$ \\frac{1}{v_\\beta^{\\mathrm{PT}}} = \\frac{1}{\\sigma^2} + \\frac{\\beta}{\\tau^2} \\implies v_\\beta^{\\mathrm{PT}} = \\frac{\\sigma^2\\tau^2}{\\tau^2 + \\beta\\sigma^2} $$\n$$ \\frac{\\mu_\\beta^{\\mathrm{PT}}}{v_\\beta^{\\mathrm{PT}}} = \\frac{y}{\\sigma^2} \\implies \\mu_\\beta^{\\mathrm{PT}} = v_\\beta^{\\mathrm{PT}} \\frac{y}{\\sigma^2} = \\frac{y \\tau^2}{\\tau^2 + \\beta\\sigma^2} $$\nThe TI integrand is $f_{\\mathrm{PT}}(x) = \\partial_\\beta \\log q_\\beta^{\\mathrm{PT}}(x) = \\log p(x) = -\\frac{x^2}{2\\tau^2} - \\frac{1}{2}\\log(2\\pi\\tau^2)$. We require its variance under $\\pi_\\beta^{\\mathrm{PT}}$:\n$$ \\operatorname{Var}_{\\pi_\\beta^{\\mathrm{PT}}}(\\log p(X)) = \\operatorname{Var}_{\\pi_\\beta^{\\mathrm{PT}}}\\left(-\\frac{X^2}{2\\tau^2}\\right) = \\frac{1}{4\\tau^4} \\operatorname{Var}(X^2) $$\nUsing the identity $\\operatorname{Var}(X^2) = 2v(v+2\\mu^2)$ for $X \\sim \\mathcal{N}(\\mu,v)$, with $\\mu = \\mu_\\beta^{\\mathrm{PT}}$ and $v = v_\\beta^{\\mathrm{PT}}$:\n$$ \\operatorname{Var}_{\\pi_\\beta^{\\mathrm{PT}}} = \\frac{1}{4\\tau^4} \\left[ 2v_\\beta^{\\mathrm{PT}} (v_\\beta^{\\mathrm{PT}} + 2(\\mu_\\beta^{\\mathrm{PT}})^2) \\right] = \\frac{v_\\beta^{\\mathrm{PT}}}{2\\tau^4} (v_\\beta^{\\mathrm{PT}} + 2(\\mu_\\beta^{\\mathrm{PT}})^2) $$\nSubstituting and simplifying leads to the per-stratum variance for the PT path:\n$$ V_{\\mathrm{PT}}(\\beta) = \\frac{y^2 \\sigma^2 \\tau^2}{(\\tau^2 + \\beta\\sigma^2)^3} + \\frac{\\sigma^4}{2(\\tau^2 + \\beta\\sigma^2)^2} $$\n\n### 3. Asymptotic Variance and Path Comparison\nThe total asymptotic variance for an estimator using the trapezoidal rule with $K$ points is:\n$$ \\mathcal{V} = \\frac{1}{n} \\sum_{j=0}^{K-1} w_j^2 V(\\beta_j) = \\frac{(\\Delta\\beta)^2}{n} \\left[ \\frac{V(0)}{4} + \\sum_{j=1}^{K-2} V(\\beta_j) + \\frac{V(1)}{4} \\right] $$\nwhere $\\beta_j = j/(K-1)$, $\\Delta\\beta = 1/(K-1)$, and $V(\\beta)$ is either $V_{\\mathrm{PP}}(\\beta)$ or $V_{\\mathrm{PT}}(\\beta)$.\n\nTo determine when the PT path is superior (i.e., has lower variance), we compare $V_{\\mathrm{PP}}(\\beta)$ and $V_{\\mathrm{PT}}(\\beta)$. The variance expressions are complex, but analysis at the endpoints $\\beta=0$ and $\\beta=1$ is revealing.\nAt $\\beta=0$:\n$$ V_{\\mathrm{PP}}(0) = \\frac{2y^2\\tau^2 + \\tau^4}{2\\sigma^4}, \\quad V_{\\mathrm{PT}}(0) = \\frac{2y^2\\sigma^2 + \\sigma^4}{2\\tau^4} $$\n$V_{\\mathrm{PT}}(0) < V_{\\mathrm{PP}}(0)$ if and only if $\\tau^4(2y^2\\sigma^2 + \\sigma^4) < \\sigma^4(2y^2\\tau^2 + \\tau^4)$, which simplifies to $2y^2\\sigma^6+\\sigma^8 < 2y^2\\tau^6+\\tau^8$. Since the function $g(v) = 2y^2v^3 + v^4$ is monotonically increasing for $v > 0$, this inequality holds if and only if $\\sigma^2 < \\tau^2$.\n\nAt $\\beta=1$:\n$$ V_{\\mathrm{PP}}(1) = \\frac{y^2 \\sigma^2 \\tau^2}{(\\tau^2 + \\sigma^2)^3} + \\frac{\\tau^4}{2(\\tau^2 + \\sigma^2)^2}, \\quad V_{\\mathrm{PT}}(1) = \\frac{y^2 \\sigma^2 \\tau^2}{(\\tau^2 + \\sigma^2)^3} + \\frac{\\sigma^4}{2(\\tau^2 + \\sigma^2)^2} $$\n$V_{\\mathrm{PT}}(1) < V_{\\mathrm{PP}}(1)$ if and only if $\\sigma^4 < \\tau^4$, which is equivalent to $\\sigma^2 < \\tau^2$.\n\nSince the variance for the PT path is lower at both endpoints if and only if $\\sigma^2 < \\tau^2$, and given that the variance is typically dominated by the behavior near $\\beta=0$ (especially when there is a mismatch between prior and likelihood), the general condition for the prior-tempering path to have lower variance than the power posterior path is $\\sigma^2 < \\tau^2$. This corresponds to the case where the likelihood is more concentrated (informative) than the prior. The PT path is advantageous because it avoids evaluating a narrow likelihood using samples from a broad prior, which is the high-variance scenario for the PP path near $\\beta=0$.\n\nThe implementation will compute the total asymptotic variance for each path and each test case based on the derived formulas and the trapezoidal rule sum.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and compares the asymptotic variance of thermodynamic integration\n    for a conjugate Gaussian model using two different paths: power posterior and\n    prior tempering.\n    \"\"\"\n    test_cases = [\n        # (y, tau^2, sigma^2, K, n)\n        (0.5, 1.0, 1.0, 41, 1000),\n        (3.0, 10.0, 1.0, 41, 1000),\n        (3.0, 0.2, 5.0, 41, 1000),\n        (1.0, 1.0, 0.1, 2, 1000),\n    ]\n\n    def v_pp_func(beta, y, tau2, sigma2):\n        \"\"\"Calculates Var_pi_beta^PP(log p(y|X)) for a given beta.\"\"\"\n        denominator = beta * tau2 + sigma2\n        term1 = (y**2 * sigma2 * tau2) / (denominator**3)\n        term2 = (tau2**2) / (2 * denominator**2)\n        return term1 + term2\n\n    def v_pt_func(beta, y, tau2, sigma2):\n        \"\"\"Calculates Var_pi_beta^PT(log p(X)) for a given beta.\"\"\"\n        denominator = tau2 + beta * sigma2\n        term1 = (y**2 * sigma2 * tau2) / (denominator**3)\n        term2 = (sigma2**2) / (2 * denominator**2)\n        return term1 + term2\n\n    final_results = []\n\n    for case in test_cases:\n        y, tau2, sigma2, K, n = case\n\n        if K < 2:\n            # The trapezoidal rule as defined requires at least 2 points (K>=2).\n            # A result of NaN indicates this invalid parameter.\n            final_results.append([np.nan, np.nan, False])\n            continue\n        \n        # Grid points for numerical integration\n        betas = np.linspace(0.0, 1.0, K)\n        \n        # Calculate integrand variances at each grid point\n        V_pp_values = v_pp_func(betas, y, tau2, sigma2)\n        V_pt_values = v_pt_func(betas, y, tau2, sigma2)\n\n        # Calculate total asymptotic variance using trapezoidal rule weights\n        # V_total = (1/n) * sum(w_j^2 * V(beta_j))\n        # w_j = delta_beta for interior, delta_beta/2 for endpoints\n        # So w_j^2 = (delta_beta)^2 for interior, (delta_beta/2)^2 for endpoints\n        delta_beta = 1.0 / (K - 1)\n        \n        if K == 2:\n            # sum over j=1...K-2 is empty\n            sum_V_pp_interior = 0\n            sum_V_pt_interior = 0\n        else:\n            sum_V_pp_interior = np.sum(V_pp_values[1:-1])\n            sum_V_pt_interior = np.sum(V_pt_values[1:-1])\n\n        total_var_pp = (delta_beta**2 / n) * (\n            V_pp_values[0] / 4.0 + sum_V_pp_interior + V_pp_values[-1] / 4.0\n        )\n        total_var_pt = (delta_beta**2 / n) * (\n            V_pt_values[0] / 4.0 + sum_V_pt_interior + V_pt_values[-1] / 4.0\n        )\n\n        is_pt_better = total_var_pt < total_var_pp\n        \n        final_results.append([total_var_pp, total_var_pt, is_pt_better])\n\n    # Format the final output string as specified\n    results_to_print = []\n    for res in final_results:\n        v_pp, v_pt, b = res\n        # Round to 6 decimal places for printing as per instruction \"at most 6\"\n        # str(round(val, 6)) achieves this.\n        results_to_print.append([round(v_pp, 6), round(v_pt, 6), b])\n\n    # Convert list of lists to string and remove spaces\n    print(str(results_to_print).replace(\" \", \"\"))\n\nsolve()\n```", "id": "3328093"}, {"introduction": "Beyond the statistical variance of the integrand, the accuracy of Thermodynamic Integration is also determined by the numerical quadrature scheme used to approximate the path integral. A fixed, uniform grid of temperatures may be inefficient or inaccurate if the integrand's shape is complex. This hands-on exercise guides you in implementing an adaptive quadrature algorithm that automatically concentrates integration points in regions of high curvature, equipping you with a powerful tool to control discretization error and improve the robustness of your evidence estimates [@problem_id:3328092].", "problem": "You are tasked with designing and implementing a diagnostic for Thermodynamic Integration (TI) in the context of path-based evidence estimation within stochastic simulation and Monte Carlo methods. The diagnostic must estimate local quadrature error on each interval of a temperature grid by comparing left and right Riemann sums to the trapezoidal approximation, and adaptively refine the temperature grid where the estimated error exceeds a threshold.\n\nStart from the following fundamental base:\n\n- Thermodynamic Integration (TI) is based on the identity that for a tempered family of distributions with inverse temperature $\\beta \\in [0,1]$, the logarithm of the evidence (also called marginal likelihood) is given by\n$$\n\\log Z = \\int_{0}^{1} \\mu(\\beta)\\, \\mathrm{d}\\beta,\n$$\nwhere $\\mu(\\beta) = \\mathbb{E}_{\\pi_{\\beta}}[\\log p(y \\mid \\theta)]$ and $\\pi_{\\beta}(\\theta) \\propto p(y \\mid \\theta)^{\\beta} p(\\theta)$ denotes the power posterior.\n\n- A numerical quadrature rule over a partition $0 = \\beta_{0} < \\beta_{1} < \\cdots < \\beta_{K} = 1$ approximates the integral via a sum of interval contributions. For any interval $[\\beta_{i}, \\beta_{i+1}]$ of width $\\Delta_{i} = \\beta_{i+1} - \\beta_{i}$, define the left Riemann sum contribution as $L_{i} = \\mu(\\beta_{i}) \\Delta_{i}$, the right Riemann sum contribution as $R_{i} = \\mu(\\beta_{i+1}) \\Delta_{i}$, and the trapezoidal contribution as $T_{i} = \\frac{1}{2}\\left(\\mu(\\beta_{i}) + \\mu(\\beta_{i+1})\\right)\\Delta_{i}$.\n\nYour program must implement the following diagnostic and adaptive refinement procedure:\n\n1. Given an initial grid $\\{\\beta_{i}\\}_{i=0}^{K}$ over $[0,1]$, compute, for each interval $[\\beta_{i}, \\beta_{i+1}]$, a local quadrature error estimate defined by the maximum absolute deviation between the trapezoidal contribution and the left/right Riemann contributions:\n$$\ne_{i} = \\max\\left(\\left|L_{i} - T_{i}\\right|,\\left|R_{i} - T_{i}\\right|\\right).\n$$\n\n2. Adaptively refine the grid by bisecting any interval $[\\beta_{i}, \\beta_{i+1}]$ for which $e_{i}$ exceeds a specified threshold $\\tau$. Continue refinement until all $e_{i} \\le \\tau$, or until a specified maximum number of grid points $K_{\\max}+1$ is reached. On refinement, evaluate $\\mu(\\beta)$ at newly inserted midpoints and update the local error estimates.\n\n3. After refinement terminates, compute the final trapezoidal approximation to $\\int_{0}^{1} \\mu(\\beta)\\, \\mathrm{d}\\beta$ using the refined grid, and report the maximum local error estimate $\\max_{i} e_{i}$.\n\nThe diagnostic must be implemented as a complete, runnable program. For testing, use the following suite of deterministic integrand functions $\\mu(\\beta)$, initial grids, refinement thresholds, and maximum grid sizes. In every case, the domain is $\\beta \\in [0,1]$.\n\n- Test Case $1$ (general monotone, smooth):\n    - $\\mu(\\beta) = \\log\\!\\left(1 + 10\\beta\\right) + 0.2\\,\\beta^{2}$.\n    - Initial grid: $K+1 = 5$ equispaced points on $[0,1]$.\n    - Threshold: $\\tau = 10^{-3}$.\n    - Maximum grid size: $K_{\\max}+1 = 4097$.\n\n- Test Case $2$ (constant function, zero curvature):\n    - $\\mu(\\beta) = 1.5$.\n    - Initial grid: $K+1 = 3$ equispaced points on $[0,1]$.\n    - Threshold: $\\tau = 10^{-6}$.\n    - Maximum grid size: $K_{\\max}+1 = 4097$.\n\n- Test Case $3$ (sharp transition near $\\beta = 0.5$):\n    - $\\mu(\\beta) = \\dfrac{3}{1 + \\exp\\!\\left(-50(\\beta - 0.5)\\right)}$.\n    - Initial grid: $K+1 = 2$ points $\\{0,1\\}$.\n    - Threshold: $\\tau = 5 \\times 10^{-3}$.\n    - Maximum grid size: $K_{\\max}+1 = 4097$.\n\n- Test Case $4$ (high curvature near $\\beta = 0$):\n    - $\\mu(\\beta) = \\sqrt{\\beta}$.\n    - Initial grid: $K+1 = 3$ equispaced points on $[0,1]$.\n    - Threshold: $\\tau = 10^{-3}$.\n    - Maximum grid size: $K_{\\max}+1 = 4097$.\n\nYour program must:\n\n- Implement the adaptive refinement diagnostic described above exactly as stated.\n- For each test case, return a list $[I, N, E]$ where $I$ is the final trapezoidal integral estimate (a floating-point number), $N$ is the final number of grid points (an integer), and $E$ is the final maximum local error estimate (a floating-point number).\n- Produce a single line of output containing the results for all provided test cases as a comma-separated list enclosed in square brackets, with each per-case result itself enclosed in square brackets, in the exact format\n$[\\,[I_{1},N_{1},E_{1}],\\,[I_{2},N_{2},E_{2}],\\,[I_{3},N_{3},E_{3}],\\,[I_{4},N_{4},E_{4}]\\,]$.\n\nNo physical units, angle units, or percentages are involved in this problem. All results must be pure numbers.", "solution": "The problem requires the design and implementation of an adaptive grid refinement procedure for the numerical evaluation of the Thermodynamic Integration (TI) formula. This procedure serves as a diagnostic tool to assess and control quadrature error.\n\nThe core of Thermodynamic Integration is the evaluation of the integral for the log evidence, $\\log Z$:\n$$\n\\log Z = \\int_{0}^{1} \\mu(\\beta)\\, \\mathrm{d}\\beta\n$$\nwhere $\\mu(\\beta) = \\mathbb{E}_{\\pi_{\\beta}}[\\log p(y \\mid \\theta)]$ is the expected log-likelihood under a power posterior $\\pi_{\\beta}$. Since $\\mu(\\beta)$ is typically not known analytically, this integral must be computed numerically. The standard approach is to use a quadrature rule, such as the trapezoidal rule, on a discrete grid of temperature values $0 = \\beta_{0} < \\beta_{1} < \\cdots < \\beta_{K} = 1$. The total integral is approximated as the sum of contributions from each interval $[\\beta_{i}, \\beta_{i+1}]$:\n$$\n\\log Z \\approx \\sum_{i=0}^{K-1} T_{i} = \\sum_{i=0}^{K-1} \\frac{1}{2}\\left(\\mu(\\beta_{i}) + \\mu(\\beta_{i+1})\\right)\\left(\\beta_{i+1} - \\beta_{i}\\right)\n$$\n\nThe accuracy of this approximation depends critically on the density of the grid points $\\{\\beta_i\\}$, especially in regions where $\\mu(\\beta)$ has high curvature. A uniform grid may be inefficient, wasting points in flat regions and being too coarse in steep regions. An adaptive grid refinement strategy addresses this by concentrating grid points where the quadrature error is largest.\n\nTo implement such a strategy, a local error estimate is required. The problem specifies a heuristic error diagnostic for each interval $[\\beta_{i}, \\beta_{i+1}]$. This diagnostic, $e_i$, is defined as the maximum absolute difference between the trapezoidal rule contribution, $T_i$, and the simpler left and right Riemann sum contributions, $L_i$ and $R_i$.\nLet $\\Delta_i = \\beta_{i+1} - \\beta_i$. The contributions are:\n- Left Riemann sum: $L_{i} = \\mu(\\beta_{i}) \\Delta_{i}$\n- Right Riemann sum: $R_{i} = \\mu(\\beta_{i+1}) \\Delta_{i}$\n- Trapezoidal sum: $T_{i} = \\frac{1}{2}\\left(\\mu(\\beta_{i}) + \\mu(\\beta_{i+1})\\right)\\Delta_{i}$\n\nThe local error estimate is $e_{i} = \\max\\left(\\left|L_{i} - T_{i}\\right|,\\left|R_{i} - T_{i}\\right|\\right)$. We can simplify the terms inside the absolute value:\n$$\nL_{i} - T_{i} = \\mu(\\beta_{i})\\Delta_{i} - \\frac{1}{2}\\left(\\mu(\\beta_{i}) + \\mu(\\beta_{i+1})\\right)\\Delta_{i} = \\frac{1}{2}\\left(\\mu(\\beta_{i}) - \\mu(\\beta_{i+1})\\right)\\Delta_{i}\n$$\n$$\nR_{i} - T_{i} = \\mu(\\beta_{i+1})\\Delta_{i} - \\frac{1}{2}\\left(\\mu(\\beta_{i}) + \\mu(\\beta_{i+1})\\right)\\Delta_{i} = \\frac{1}{2}\\left(\\mu(\\beta_{i+1}) - \\mu(\\beta_{i})\\right)\\Delta_{i}\n$$\nSince $|L_i - T_i| = |R_i - T_i|$, the local error simplifies to:\n$$\ne_{i} = \\frac{1}{2}\\left|\\mu(\\beta_{i+1}) - \\mu(\\beta_{i})\\right|\\Delta_{i}\n$$\nThis expression is proportional to the absolute change in the function value across the interval, scaled by the interval width. It is a simple and effective heuristic for the local integration error, as it will be large where the function is steep, which is precisely where the trapezoidal approximation is least accurate.\n\nThe adaptive refinement algorithm proceeds iteratively:\n1.  Start with an initial grid of $\\beta$ values and a dictionary to store computed $\\mu(\\beta)$ values to avoid redundant calculations. The grid points are maintained in a sorted list.\n2.  Enter a loop that continues until a termination condition is met. In each iteration:\n    a. For every interval $[\\beta_i, \\beta_{i+1}]$ in the current grid, calculate the local error estimate $e_i$.\n    b. Identify all intervals where the error $e_i$ exceeds the specified threshold $\\tau$.\n    c. Check for termination conditions:\n        i. If no intervals have an error greater than $\\tau$, the grid is sufficiently refined. The loop terminates.\n        ii. If bisecting all \"offending\" intervals would cause the total number of grid points to exceed the maximum allowed size, $K_{\\max}+1$, the refinement budget is exhausted. The loop terminates.\n    d. If the loop has not terminated, perform the refinement. For each interval identified in step 2b, calculate its midpoint $\\beta_{\\text{mid}} = (\\beta_i + \\beta_{i+1})/2$. Add all such new midpoints to the grid. Compute and store the corresponding $\\mu(\\beta_{\\text{mid}})$ values. The list of grid points is then re-sorted.\n3.  Upon loop termination, the final adapted grid is established. The final estimate for the integral, $I$, is calculated by summing the trapezoidal contributions $T_i$ over all intervals in this final grid. The final number of grid points, $N$, is the size of the final grid. The final maximum local error, $E$, is the maximum $e_i$ found across all intervals of the final grid. These three quantities, $[I, N, E]$, are reported.\n\nThis procedure ensures that computational effort, in the form of $\\mu(\\beta)$ evaluations (which can be very expensive in a real application), is focused on the regions of $\\beta \\in [0, 1]$ that contribute most to the overall quadrature error. The algorithm is implemented to handle the four specified test cases, each with a different integrand $\\mu(\\beta)$ and set of parameters, demonstrating its behavior on functions with varying smoothness and structure.", "answer": "```python\nimport numpy as np\n\ndef adaptive_integrator(mu_func, initial_betas, tau, k_max_plus_1):\n    \"\"\"\n    Performs adaptive grid refinement for thermodynamic integration.\n\n    Args:\n        mu_func (callable): The integrand function mu(beta).\n        initial_betas (list or np.ndarray): The initial grid of beta values.\n        tau (float): The error threshold for refinement.\n        k_max_plus_1 (int): The maximum allowed number of grid points.\n\n    Returns:\n        list: A list containing [I, N, E] where I is the final integral estimate,\n              N is the final number of grid points, and E is the final max local error.\n    \"\"\"\n    # Use a sorted list for betas to allow for easy insertion\n    betas = sorted(list(initial_betas))\n    # Use a dictionary to cache mu(beta) values\n    mu_vals = {b: mu_func(b) for b in betas}\n\n    while True:\n        # Step 1: Find all intervals that need refinement\n        indices_to_refine = []\n        if len(betas) > 1:\n            for i in range(len(betas) - 1):\n                b1, b2 = betas[i], betas[i+1]\n                mu1, mu2 = mu_vals[b1], mu_vals[b2]\n                delta = b2 - b1\n                error = 0.5 * abs(mu2 - mu1) * delta\n                if error > tau:\n                    indices_to_refine.append(i)\n\n        # Step 2: Check termination conditions\n        # Condition a: All errors are below the threshold\n        if not indices_to_refine:\n            break\n        \n        # Condition b: Adding new points would exceed the max size\n        if len(betas) + len(indices_to_refine) > k_max_plus_1:\n            break\n\n        # Step 3: Perform refinement if no termination condition is met\n        midpoints_to_add = []\n        for i in indices_to_refine:\n            b1, b2 = betas[i], betas[i+1]\n            midpoint = (b1 + b2) / 2.0\n            midpoints_to_add.append(midpoint)\n        \n        for bp in midpoints_to_add:\n            # Check for existence is technically not needed for bisection\n            # but is good practice.\n            if bp not in mu_vals:\n                mu_vals[bp] = mu_func(bp)\n\n        betas.extend(midpoints_to_add)\n        betas.sort()\n\n    # Step 4: After refinement terminates, compute final quantities\n    final_integral = 0.0\n    final_max_error = 0.0\n    if len(betas) > 1:\n        for i in range(len(betas) - 1):\n            b1, b2 = betas[i], betas[i+1]\n            mu1, mu2 = mu_vals[b1], mu_vals[b2]\n            delta = b2 - b1\n            \n            # Trapezoidal contribution to integral\n            final_integral += 0.5 * (mu1 + mu2) * delta\n            \n            # Local error\n            error = 0.5 * abs(mu2 - mu1) * delta\n            if error > final_max_error:\n                final_max_error = error\n\n    final_num_points = len(betas)\n\n    return [final_integral, final_num_points, final_max_error]\n\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    # Define the integrand functions for each test case\n    mu_1 = lambda beta: np.log(1 + 10 * beta) + 0.2 * beta**2\n    mu_2 = lambda beta: 1.5\n    mu_3 = lambda beta: 3.0 / (1.0 + np.exp(-50.0 * (beta - 0.5)))\n    mu_4 = lambda beta: np.sqrt(beta)\n\n    # Define the test cases as specified in the problem\n    test_cases = [\n        {\n            'mu_func': mu_1,\n            'initial_betas': np.linspace(0, 1, 5),\n            'tau': 1e-3,\n            'k_max_plus_1': 4097\n        },\n        {\n            'mu_func': mu_2,\n            'initial_betas': np.linspace(0, 1, 3),\n            'tau': 1e-6,\n            'k_max_plus_1': 4097\n        },\n        {\n            'mu_func': mu_3,\n            'initial_betas': [0.0, 1.0],\n            'tau': 5e-3,\n            'k_max_plus_1': 4097\n        },\n        {\n            'mu_func': mu_4,\n            'initial_betas': np.linspace(0, 1, 3),\n            'tau': 1e-3,\n            'k_max_plus_1': 4097\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        result = adaptive_integrator(\n            case['mu_func'],\n            case['initial_betas'],\n            case['tau'],\n            case['k_max_plus_1']\n        )\n        results.append(result)\n\n    # Format the results into the exact required string format\n    # e.g., [[I1,N1,E1],[I2,N2,E2],...]\n    # Using f-strings to avoid spaces that `str(list)` would introduce\n    formatted_results = [f\"[{r[0]},{r[1]},{r[2]}]\" for r in results]\n    final_output = f\"[{','.join(formatted_results)}]\"\n    \n    print(final_output)\n\nsolve()\n```", "id": "3328092"}]}