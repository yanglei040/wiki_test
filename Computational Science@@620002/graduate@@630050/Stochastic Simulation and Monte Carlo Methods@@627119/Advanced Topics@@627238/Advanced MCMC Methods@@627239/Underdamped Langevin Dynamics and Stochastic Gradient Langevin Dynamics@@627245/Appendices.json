{"hands_on_practices": [{"introduction": "Before we can simulate a system's dynamics, we must first understand the properties of the target equilibrium state it is expected to reach. Underdamped Langevin dynamics are designed to sample from the Gibbs-Boltzmann distribution, where the probability of a state is related to its energy. This first exercise provides foundational practice in connecting the physical parameters of a model, such as its potential energy and temperature, to the statistical properties of its stationary distribution [@problem_id:3359276]. By analyzing a simple quadratic potential, you will derive the exact covariances for position and velocity, reinforcing your understanding of how the target distribution is structured in phase space.", "problem": "Consider the $d$-dimensional underdamped Langevin Stochastic Differential Equation (SDE) for position $x \\in \\mathbb{R}^{d}$ and velocity $v \\in \\mathbb{R}^{d}$,\n$$\n\\mathrm{d}x_{t} = v_{t}\\,\\mathrm{d}t, \n\\qquad\n\\mathrm{d}v_{t} = -\\gamma\\, v_{t}\\,\\mathrm{d}t - \\nabla U(x_{t})\\,\\mathrm{d}t + \\sqrt{\\frac{2\\gamma}{\\beta}}\\,\\mathrm{d}W_{t},\n$$\nwhere $W_{t}$ is a $d$-dimensional standard Wiener process, $\\gamma>0$ is the friction coefficient, and $\\beta>0$ is the inverse temperature. Assume the potential energy is quadratic,\n$$\nU(x) = \\frac{1}{2}\\, x^{\\top} A x,\n$$\nwhere $A \\in \\mathbb{R}^{d \\times d}$ is symmetric positive definite. Starting from the stationary Fokker–Planck equation and the fluctuation–dissipation balance encoded in the SDE above, determine the stationary density $\\pi(x,v)$ and use it to calculate the stationary covariances $\\mathrm{Cov}(x)$ and $\\mathrm{Cov}(v)$ implied by $\\pi$. Express your final answer as a single row matrix whose two entries are the closed-form expressions for $\\mathrm{Cov}(x)$ and $\\mathrm{Cov}(v)$, respectively. No numerical rounding is required.", "solution": "The problem is valid. It is a well-posed, scientifically grounded problem in statistical mechanics and stochastic calculus, with all necessary information provided for a unique solution.\n\nThe underdamped Langevin dynamics for the state vector $(x_t, v_t)$ are given by the stochastic differential equations (SDEs):\n$$\n\\mathrm{d}x_{t} = v_{t}\\,\\mathrm{d}t\n$$\n$$\n\\mathrm{d}v_{t} = -\\gamma\\, v_{t}\\,\\mathrm{d}t - \\nabla U(x_{t})\\,\\mathrm{d}t + \\sqrt{\\frac{2\\gamma}{\\beta}}\\,\\mathrm{d}W_{t}\n$$\nThis system describes the motion of a particle with unit mass ($m=1$) in a potential $U(x)$, subject to friction and random thermal fluctuations. The form of the noise term is consistent with the fluctuation-dissipation theorem, which guarantees that the system will thermalize to a stationary state described by the Gibbs-Boltzmann distribution at inverse temperature $\\beta$.\n\nThe total energy of the system is given by the Hamiltonian $H(x, v)$, which is the sum of the potential energy $U(x)$ and the kinetic energy $K(v)$.\nThe potential energy is given as a quadratic form:\n$$\nU(x) = \\frac{1}{2}\\, x^{\\top} A x\n$$\nwhere $A$ is a symmetric positive definite matrix.\nThe kinetic energy for a particle with unit mass is:\n$$\nK(v) = \\frac{1}{2}\\, v^{\\top} v\n$$\nThus, the Hamiltonian is:\n$$\nH(x,v) = U(x) + K(v) = \\frac{1}{2}\\, x^{\\top} A x + \\frac{1}{2}\\, v^{\\top} v\n$$\nThe stationary distribution, or invariant measure, $\\pi(x, v)$ of the Langevin SDE is the Gibbs-Boltzmann distribution associated with this Hamiltonian:\n$$\n\\pi(x, v) = Z^{-1} \\exp(-\\beta H(x, v))\n$$\nwhere $Z$ is the normalization constant, also known as the partition function. Substituting the expression for $H(x,v)$:\n$$\n\\pi(x, v) = Z^{-1} \\exp\\left(-\\beta \\left(\\frac{1}{2}\\, x^{\\top} A x + \\frac{1}{2}\\, v^{\\top} v\\right)\\right)\n$$\nWe can separate the terms involving $x$ and $v$:\n$$\n\\pi(x, v) = Z^{-1} \\exp\\left(-\\frac{\\beta}{2}\\, x^{\\top} A x\\right) \\exp\\left(-\\frac{\\beta}{2}\\, v^{\\top} v\\right)\n$$\nThis demonstrates that the stationary distribution is a product of two functions, one depending only on $x$ and the other only on $v$. This implies that at stationarity, the position $x$ and velocity $v$ are statistically independent random variables. We can write $\\pi(x, v) = \\pi(x) \\pi(v)$, where:\n$$\n\\pi(x) \\propto \\exp\\left(-\\frac{1}{2}\\, x^{\\top} (\\beta A) x\\right)\n$$\n$$\n\\pi(v) \\propto \\exp\\left(-\\frac{1}{2}\\, v^{\\top} (\\beta I) v\\right)\n$$\nwhere $I$ is the $d \\times d$ identity matrix.\n\nBoth $\\pi(x)$ and $\\pi(v)$ are probability densities of multivariate normal distributions. A general $d$-dimensional multivariate normal distribution with mean $\\mu$ and covariance matrix $\\Sigma$ has a probability density function proportional to $\\exp\\left(-\\frac{1}{2} (z - \\mu)^{\\top} \\Sigma^{-1} (z - \\mu)\\right)$.\nFor both distributions, the quadratic form is centered, which means the mean vectors are zero:\n$$\n\\mathbb{E}[x] = \\int_{\\mathbb{R}^d} x \\, \\pi(x) \\, \\mathrm{d}x = 0\n$$\n$$\n\\mathbb{E}[v] = \\int_{\\mathbb{R}^d} v \\, \\pi(v) \\, \\mathrm{d}v = 0\n$$\nThe covariance matrices are defined as $\\mathrm{Cov}(x) = \\mathbb{E}[(x - \\mathbb{E}[x])(x - \\mathbb{E}[x])^{\\top}] = \\mathbb{E}[xx^{\\top}]$ and $\\mathrm{Cov}(v) = \\mathbb{E}[vv^{\\top}]$.\n\nTo find $\\mathrm{Cov}(x)$, we compare the exponent in $\\pi(x)$ with the standard form. We identify the inverse covariance matrix of $x$, denoted $\\Sigma_{x}^{-1}$, by the relation:\n$$\n-\\frac{1}{2}\\, x^{\\top} \\Sigma_{x}^{-1} x = -\\frac{1}{2}\\, x^{\\top} (\\beta A) x\n$$\nThis gives $\\Sigma_{x}^{-1} = \\beta A$. Since $A$ is symmetric positive definite and $\\beta > 0$, $\\beta A$ is invertible. The covariance matrix for $x$ is therefore:\n$$\n\\mathrm{Cov}(x) = \\Sigma_x = (\\beta A)^{-1} = \\frac{1}{\\beta} A^{-1}\n$$\n\nSimilarly, for $\\mathrm{Cov}(v)$, we compare the exponent in $\\pi(v)$ with the standard form. We identify the inverse covariance matrix of $v$, denoted $\\Sigma_{v}^{-1}$, by the relation:\n$$\n-\\frac{1}{2}\\, v^{\\top} \\Sigma_{v}^{-1} v = -\\frac{1}{2}\\, v^{\\top} (\\beta I) v\n$$\nThis gives $\\Sigma_{v}^{-1} = \\beta I$. The covariance matrix for $v$ is therefore:\n$$\n\\mathrm{Cov}(v) = \\Sigma_v = (\\beta I)^{-1} = \\frac{1}{\\beta} I\n$$\nThe friction coefficient $\\gamma$ influences the dynamics (i.e., the rate of convergence to the stationary distribution), but it does not affect the stationary covariances themselves, as they are solely determined by the potential energy $U(x)$ and the temperature $\\beta^{-1}$.\n\nThe stationary covariances are $\\mathrm{Cov}(x) = \\frac{1}{\\beta} A^{-1}$ and $\\mathrm{Cov}(v) = \\frac{1}{\\beta} I$.", "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{1}{\\beta} A^{-1} & \\frac{1}{\\beta} I \\end{pmatrix}}\n$$", "id": "3359276"}, {"introduction": "Having characterized the target equilibrium state, our focus now shifts to the dynamics that guide the system toward it. The underdamped Langevin equation can be split into components, and the velocity update is governed by an Ornstein-Uhlenbeck (OU) process, which models the balance between friction and random thermal kicks. This exercise asks you to analyze the discrete-time update rule for this OU process [@problem_id:3359245]. You will verify that the numerical scheme correctly preserves the equilibrium variance, a crucial property known as discrete-time fluctuation-dissipation consistency, which ensures the stability and accuracy of the simulation.", "problem": "Consider the scalar velocity sub-dynamics in underdamped Langevin dynamics, modeled as an Ornstein–Uhlenbeck (OU) step. The continuous-time stochastic differential equation is\n$$\n\\mathrm{d}v(t) \\;=\\; -\\,\\gamma\\,v(t)\\,\\mathrm{d}t \\;+\\; \\sqrt{\\frac{2\\,\\gamma}{\\beta}}\\,\\mathrm{d}W(t),\n$$\nwhere $v(t)$ is the velocity, $\\gamma \\!>\\! 0$ is the friction coefficient, $\\beta \\!>\\! 0$ is the inverse thermal energy, and $W(t)$ is a standard Wiener process. Over a finite time-step $\\Delta t \\!>\\! 0$, the exact OU update can be written as\n$$\nv_{+} \\;=\\; \\exp(-\\gamma\\,\\Delta t)\\,v_{-} \\;+\\; \\sqrt{\\frac{1 - \\exp(-2\\,\\gamma\\,\\Delta t)}{\\beta}}\\,\\xi,\n$$\nwhere $\\xi \\sim \\mathcal{N}(0,1)$ is independent of $v_{-}$. Assume $\\mathbb{E}[v_{-}] = 0$ and $\\mathrm{Var}(v_{-}) = \\sigma_{0}^{2}$.\n\nUsing only fundamental properties of linear stochastic differential equations and the independence of Gaussian increments, derive the variance of $v_{+}$ in closed form in terms of $\\sigma_{0}^{2}$, $\\gamma$, $\\Delta t$, and $\\beta$. Then, by specializing your expression to the equilibrium value $\\sigma_{0}^{2} = 1/\\beta$, verify the discrete-time fluctuation–dissipation consistency (invariance of the equilibrium variance under the OU update). \n\nProvide your final answer as the single closed-form analytic expression for $\\mathrm{Var}(v_{+})$. No rounding is required and no units are involved.", "solution": "The problem statement is critically validated and found to be valid. It is scientifically grounded, well-posed, objective, and self-contained. The model presented is the standard Ornstein–Uhlenbeck process, which accurately describes the velocity component in underdamped Langevin dynamics. All parameters and conditions are clearly defined and consistent with established principles of statistical physics and stochastic calculus.\n\nThe primary objective is to derive the variance of the updated velocity, $\\mathrm{Var}(v_{+})$, given the discrete-time update rule:\n$$\nv_{+} \\;=\\; \\exp(-\\gamma\\,\\Delta t)\\,v_{-} \\;+\\; \\sqrt{\\frac{1 - \\exp(-2\\,\\gamma\\,\\Delta t)}{\\beta}}\\,\\xi\n$$\nWe are given that $\\mathbb{E}[v_{-}] = 0$ and $\\mathrm{Var}(v_{-}) = \\sigma_{0}^{2}$. The random variable $\\xi$ is drawn from a standard normal distribution, $\\xi \\sim \\mathcal{N}(0,1)$, which implies $\\mathbb{E}[\\xi] = 0$ and $\\mathrm{Var}(\\xi) = 1$. A crucial piece of information is that $\\xi$ is independent of $v_{-}$.\n\nThe expression for $v_{+}$ is a linear combination of the two random variables $v_{-}$ and $\\xi$. Let us define two new random variables for clarity:\n$$\nA = \\exp(-\\gamma\\,\\Delta t)\\,v_{-}\n$$\n$$\nB = \\sqrt{\\frac{1 - \\exp(-2\\,\\gamma\\,\\Delta t)}{\\beta}}\\,\\xi\n$$\nSo, $v_{+} = A + B$.\n\nThe variance of a sum of two random variables is given by $\\mathrm{Var}(A+B) = \\mathrm{Var}(A) + \\mathrm{Var}(B) + 2\\,\\mathrm{Cov}(A,B)$. Since $v_{-}$ and $\\xi$ are independent, any deterministic functions of them, such as $A$ and $B$, are also independent. For independent variables, their covariance is zero, i.e., $\\mathrm{Cov}(A,B) = 0$. Therefore, the variance of the sum simplifies to:\n$$\n\\mathrm{Var}(v_{+}) = \\mathrm{Var}(A) + \\mathrm{Var}(B)\n$$\nNow, we compute the variance of $A$ and $B$ separately. For any random variable $X$ and constant $c$, the variance property is $\\mathrm{Var}(cX) = c^{2}\\mathrm{Var}(X)$.\n\nFor variable $A$:\n$$\n\\mathrm{Var}(A) = \\mathrm{Var}\\left(\\exp(-\\gamma\\,\\Delta t)\\,v_{-}\\right) = \\left(\\exp(-\\gamma\\,\\Delta t)\\right)^{2} \\mathrm{Var}(v_{-})\n$$\nSubstituting the given $\\mathrm{Var}(v_{-}) = \\sigma_{0}^{2}$, we get:\n$$\n\\mathrm{Var}(A) = \\exp(-2\\,\\gamma\\,\\Delta t)\\,\\sigma_{0}^{2}\n$$\n\nFor variable $B$:\n$$\n\\mathrm{Var}(B) = \\mathrm{Var}\\left(\\sqrt{\\frac{1 - \\exp(-2\\,\\gamma\\,\\Delta t)}{\\beta}}\\,\\xi\\right) = \\left(\\sqrt{\\frac{1 - \\exp(-2\\,\\gamma\\,\\Delta t)}{\\beta}}\\right)^{2} \\mathrm{Var}(\\xi)\n$$\nSubstituting $\\mathrm{Var}(\\xi) = 1$, we get:\n$$\n\\mathrm{Var}(B) = \\frac{1 - \\exp(-2\\,\\gamma\\,\\Delta t)}{\\beta}\n$$\n\nCombining these two results, we obtain the expression for the variance of $v_{+}$:\n$$\n\\mathrm{Var}(v_{+}) = \\mathrm{Var}(A) + \\mathrm{Var}(B) = \\sigma_{0}^{2}\\,\\exp(-2\\,\\gamma\\,\\Delta t) + \\frac{1 - \\exp(-2\\,\\gamma\\,\\Delta t)}{\\beta}\n$$\nThis is the closed-form expression for $\\mathrm{Var}(v_{+})$ in terms of $\\sigma_{0}^{2}$, $\\gamma$, $\\Delta t$, and $\\beta$.\n\nThe second part of the problem asks to verify the discrete-time fluctuation–dissipation consistency. This involves checking if the equilibrium variance is invariant under the time evolution step. The equilibrium variance for the velocity in the Langevin system is known to be $\\frac{1}{\\beta}$, corresponding to the equipartition theorem where the average kinetic energy $\\frac{1}{2}m\\langle v^2 \\rangle = \\frac{1}{2}k_B T$. In our notation with unit mass ($m=1$) and $\\beta = (k_B T)^{-1}$, this is $\\frac{1}{2}\\mathrm{Var}(v) = \\frac{1}{2\\beta}$, so $\\mathrm{Var}(v) = \\frac{1}{\\beta}$.\n\nLet's specialize our derived expression by setting the initial variance to this equilibrium value, $\\sigma_{0}^{2} = \\frac{1}{\\beta}$:\n$$\n\\mathrm{Var}(v_{+}) = \\left(\\frac{1}{\\beta}\\right)\\,\\exp(-2\\,\\gamma\\,\\Delta t) + \\frac{1 - \\exp(-2\\,\\gamma\\,\\Delta t)}{\\beta}\n$$\nWe can factor out the common term $\\frac{1}{\\beta}$:\n$$\n\\mathrm{Var}(v_{+}) = \\frac{1}{\\beta} \\left( \\exp(-2\\,\\gamma\\,\\Delta t) + 1 - \\exp(-2\\,\\gamma\\,\\Delta t) \\right)\n$$\nThe exponential terms cancel each other out:\n$$\n\\mathrm{Var}(v_{+}) = \\frac{1}{\\beta} \\left( 1 \\right) = \\frac{1}{\\beta}\n$$\nThis result confirms that if the velocity distribution has the equilibrium variance $\\frac{1}{\\beta}$ at the beginning of the time step, it will retain this variance after the Ornstein–Uhlenbeck update. This demonstrates the consistency of the discrete update rule with the stationary (equilibrium) distribution, which is a key aspect of the fluctuation-dissipation theorem. The final answer required is the general expression for $\\mathrm{Var}(v_{+})$ derived initially.", "answer": "$$\\boxed{\\sigma_{0}^{2} \\exp(-2 \\gamma \\Delta t) + \\frac{1 - \\exp(-2 \\gamma \\Delta t)}{\\beta}}$$", "id": "3359245"}, {"introduction": "In many real-world applications, especially in machine learning, the exact gradient of the potential energy is intractable or too costly to compute. Stochastic Gradient Langevin Dynamics (SGLD) addresses this by using noisy gradient estimates calculated from small subsets of data, or \"mini-batches.\" This practice explores the statistical nature of this approximation [@problem_id:3359225]. By calculating the variance of the mini-batch gradient estimator, you will uncover the fundamental relationship between the mini-batch size and the level of noise injected into the dynamics, a key principle for tuning the efficiency and convergence of SGLD samplers.", "problem": "Consider the underdamped Langevin dynamics for a target potential $U(x)$ in $d$ dimensions, where the true gradient $\\nabla U(x)$ is replaced by a stochastic mini-batch estimator in stochastic gradient Langevin dynamics (SGLD). Let the per-sample contribution to the gradient at a fixed $x$ be a random vector $G(x)$ with mean $\\mathbb{E}[G(x)] = \\nabla U(x)$ and covariance matrix $\\Sigma(x) = \\operatorname{Cov}(G(x))$. Assume that a mini-batch of size $b$ is formed by sampling with replacement, producing $b$ independent and identically distributed copies $\\{G_j(x)\\}_{j=1}^{b}$ of $G(x)$, and define the mini-batch gradient estimator\n$$\n\\widehat{\\nabla U}(x) \\;=\\; \\frac{1}{b}\\sum_{j=1}^{b} G_j(x).\n$$\nTreating $x$ as fixed, compute the variance (i.e., the covariance matrix) of $\\widehat{\\nabla U}(x)$ as a function of $b$ and $\\Sigma(x)$. Express your final answer as a single closed-form analytic expression in terms of $b$ and $\\Sigma(x)$ only. No rounding is required, and no units should be included in the final answer.", "solution": "The problem requires the computation of the variance, which for a vector-valued random variable is its covariance matrix, of the stochastic mini-batch gradient estimator $\\widehat{\\nabla U}(x)$. The analysis will treat the position $x$ as a fixed parameter.\n\nThe mini-batch gradient estimator is defined as the average of $b$ independent and identically distributed (i.i.d.) per-sample gradient estimators $\\{G_j(x)\\}_{j=1}^{b}$:\n$$\n\\widehat{\\nabla U}(x) = \\frac{1}{b}\\sum_{j=1}^{b} G_j(x)\n$$\nWe are asked to compute $\\operatorname{Var}(\\widehat{\\nabla U}(x))$, which is equivalent to the covariance matrix $\\operatorname{Cov}(\\widehat{\\nabla U}(x))$.\n$$\n\\operatorname{Var}(\\widehat{\\nabla U}(x)) = \\operatorname{Cov}\\left( \\frac{1}{b}\\sum_{j=1}^{b} G_j(x) \\right)\n$$\nWe use a fundamental property of the covariance operator. For a constant scalar $c$ and a random vector $X$, $\\operatorname{Cov}(cX) = c^2 \\operatorname{Cov}(X)$. In this case, the scalar is $c = \\frac{1}{b}$. Thus, we can factor it out of the covariance operator:\n$$\n\\operatorname{Cov}\\left( \\frac{1}{b}\\sum_{j=1}^{b} G_j(x) \\right) = \\left(\\frac{1}{b}\\right)^2 \\operatorname{Cov}\\left( \\sum_{j=1}^{b} G_j(x) \\right) = \\frac{1}{b^2} \\operatorname{Cov}\\left( \\sum_{j=1}^{b} G_j(x) \\right)\n$$\nNext, we use another key property of covariance. For a set of mutually independent random vectors $\\{X_j\\}_{j=1}^{b}$, the covariance of their sum is the sum of their individual covariances:\n$$\n\\operatorname{Cov}\\left(\\sum_{j=1}^{b} X_j\\right) = \\sum_{j=1}^{b} \\operatorname{Cov}(X_j)\n$$\nThe problem statement explicitly mentions that the samples $\\{G_j(x)\\}_{j=1}^{b}$ are independent. Therefore, we can apply this property:\n$$\n\\operatorname{Cov}\\left( \\sum_{j=1}^{b} G_j(x) \\right) = \\sum_{j=1}^{b} \\operatorname{Cov}(G_j(x))\n$$\nThe problem also states that the samples are identically distributed. This implies that the covariance matrix is the same for each sample $G_j(x)$. We are given that the covariance matrix of a single per-sample gradient $G(x)$ is $\\operatorname{Cov}(G(x)) = \\Sigma(x)$. Since all $G_j(x)$ are identically distributed copies of $G(x)$, it follows that for any $j \\in \\{1, 2, \\ldots, b\\}$:\n$$\n\\operatorname{Cov}(G_j(x)) = \\Sigma(x)\n$$\nSubstituting this into the sum, we get:\n$$\n\\sum_{j=1}^{b} \\operatorname{Cov}(G_j(x)) = \\sum_{j=1}^{b} \\Sigma(x)\n$$\nThis is a sum of $b$ identical matrices $\\Sigma(x)$, which equals $b\\Sigma(x)$.\n\nFinally, we combine all the pieces.\n$$\n\\operatorname{Var}(\\widehat{\\nabla U}(x)) = \\frac{1}{b^2} \\left( \\sum_{j=1}^{b} \\operatorname{Cov}(G_j(x)) \\right) = \\frac{1}{b^2} (b \\Sigma(x))\n$$\nSimplifying the expression by canceling one factor of $b$ gives the final result:\n$$\n\\operatorname{Var}(\\widehat{\\nabla U}(x)) = \\frac{1}{b} \\Sigma(x)\n$$\nThis result shows that the variance of the mini-batch gradient estimator is inversely proportional to the batch size $b$. Increasing the batch size reduces the noise in the gradient estimate, a principle central to stochastic optimization methods.", "answer": "$$\\boxed{\\frac{1}{b} \\Sigma(x)}$$", "id": "3359225"}]}