{"hands_on_practices": [{"introduction": "The efficiency of Parallel Tempering critically depends on the successful exchange of configurations between replicas at different temperatures. This exercise delves into the theoretical underpinnings of this process by deriving an approximation for the swap acceptance probability in a high-dimensional system ([@problem_id:3326637]). By applying principles from statistical mechanics and the Central Limit Theorem, you will see how this probability is governed by the dimension of the problem and the spacing of the temperature ladder, providing essential insights for designing effective tempering schemes.", "problem": "Consider Parallel Tempering within the framework of Markov Chain Monte Carlo (MCMC), where the target at inverse temperature $\\beta$ is the $d$-dimensional Gaussian product with energy $U(x) = \\frac{1}{2}\\|x\\|^{2}$ and density $\\pi_{\\beta}(x) \\propto \\exp(-\\beta U(x))$. Let two adjacent inverse temperatures be $\\beta_{1}$ and $\\beta_{2}$ with $\\beta_{2} - \\beta_{1} = \\Delta \\beta > 0$ and mean inverse temperature $\\bar{\\beta} = (\\beta_{1} + \\beta_{2})/2$. In a swap move, independent states $x \\sim \\pi_{\\beta_{1}}$ and $y \\sim \\pi_{\\beta_{2}}$ are proposed to be exchanged, with Metropolis acceptance probability\n$$\n\\alpha = \\min\\left\\{1, \\exp\\big((\\beta_{1} - \\beta_{2})(U(x) - U(y))\\big)\\right\\}.\n$$\nStarting from the definitions of the Gaussian product and the properties of $\\chi^{2}$ distributions, approximate the difference of energies $U(x) - U(y)$ by a normal distribution for large $d$ and derive the resulting leading-order approximation (for small $\\Delta \\beta$) to the expected swap acceptance $\\mathbb{E}[\\alpha]$ between these adjacent inverse temperatures. Your derivation must clearly show how the approximation depends on the dimension $d$ and the mean inverse temperature $\\bar{\\beta}$. Express your final answer as a single closed-form analytic expression involving $d$, $\\bar{\\beta}$, and $\\Delta \\beta$ using the standard normal cumulative distribution function $\\Phi$. No rounding is required, and no units are involved.", "solution": "The user wants to derive an approximation for the expected swap acceptance probability in a Parallel Tempering simulation.\n\n### Step 1: Problem Validation\n\n**1.1. Extraction of Givens:**\n- Framework: Parallel Tempering, Markov Chain Monte Carlo (MCMC).\n- Target density at inverse temperature $\\beta$: $\\pi_{\\beta}(x) \\propto \\exp(-\\beta U(x))$ for $x \\in \\mathbb{R}^d$.\n- Energy function: $U(x) = \\frac{1}{2}\\|x\\|^{2}$, which is $\\frac{1}{2}\\sum_{i=1}^{d} x_i^2$.\n- Inverse temperatures: Two adjacent levels, $\\beta_{1}$ and $\\beta_{2}$.\n- Temperature difference: $\\beta_{2} - \\beta_{1} = \\Delta \\beta > 0$.\n- Mean inverse temperature: $\\bar{\\beta} = (\\beta_{1} + \\beta_{2})/2$.\n- Swap proposal: Exchange independent states $x \\sim \\pi_{\\beta_{1}}$ and $y \\sim \\pi_{\\beta_{2}}$.\n- Metropolis acceptance probability: $\\alpha = \\min\\left\\{1, \\exp\\big((\\beta_{1} - \\beta_{2})(U(x) - U(y))\\big)\\right\\}$.\n- Task: Derive the leading-order approximation for the expected swap acceptance, $\\mathbb{E}[\\alpha]$, for large dimension $d$ and small $\\Delta \\beta$.\n- Method: Approximate the energy difference $U(x) - U(y)$ by a normal distribution, using properties of $\\chi^{2}$ distributions.\n- Required final form: A closed-form expression depending on $d$, $\\bar{\\beta}$, $\\Delta \\beta$, and the standard normal cumulative distribution function $\\Phi$.\n\n**1.2. Validation using Givens:**\n- **Scientific Grounding:** The problem is firmly rooted in the theory of MCMC methods, specifically Parallel Tempering. The choice of a multidimensional Gaussian as the target distribution is a standard and analytically tractable model. The swap acceptance probability is the correct Metropolis-Hastings probability for this exchange. The use of the Central Limit Theorem (via $\\chi^2$ distributions) for high-dimensional systems is a standard and valid technique in statistical physics and computational statistics. All concepts are scientifically sound.\n- **Well-Posedness:** The problem statement is clear, self-contained, and provides a well-defined objective: to derive a specific approximation. The constraints (large $d$, small $\\Delta \\beta$) guide the derivation process. A unique analytical solution is expected.\n- **Objectivity:** The problem is formulated in precise mathematical language, free from ambiguity or subjective elements.\n\n**1.3. Verdict and Action:**\nThe problem is valid. It is scientifically sound, well-posed, objective, and contains no discernible flaws. I will proceed with the derivation.\n\n### Step 2: Solution Derivation\n\nThe target probability density at inverse temperature $\\beta$ is given by\n$$ \\pi_{\\beta}(x) = C_{\\beta} \\exp(-\\beta U(x)) = C_{\\beta} \\exp\\left(-\\frac{\\beta}{2} \\sum_{i=1}^d x_i^2\\right) = \\prod_{i=1}^d \\left(\\sqrt{\\frac{\\beta}{2\\pi}} \\exp\\left(-\\frac{\\beta}{2} x_i^2\\right)\\right) $$\nwhere $C_{\\beta}$ is the normalization constant. This shows that for a state $x \\sim \\pi_{\\beta}(x)$, each component $x_i$ is an independent random variable drawn from a zero-mean Gaussian distribution $\\mathcal{N}(0, \\sigma^2)$ with variance $\\sigma^2 = 1/\\beta$.\n\nLet's characterize the distribution of the energy $U(x)$. If $x_i \\sim \\mathcal{N}(0, 1/\\beta)$, then $\\sqrt{\\beta}x_i \\sim \\mathcal{N}(0, 1)$. The square of a standard normal variable follows a chi-squared distribution with one degree of freedom, so $(\\sqrt{\\beta}x_i)^2 = \\beta x_i^2 \\sim \\chi^2_1$.\n\nThe energy is $U(x) = \\frac{1}{2}\\sum_{i=1}^d x_i^2$. Let's consider the scaled energy $2\\beta U(x)$:\n$$ 2\\beta U(x) = \\beta \\sum_{i=1}^d x_i^2 = \\sum_{i=1}^d (\\beta x_i^2) $$\nSince each term $\\beta x_i^2$ is an independent $\\chi^2_1$ variable, their sum is a chi-squared variable with $d$ degrees of freedom: $2\\beta U(x) \\sim \\chi^2_d$.\nThis implies that the energy $U(x)$ for a state drawn from $\\pi_{\\beta}(x)$ follows the distribution $U(x) \\sim \\frac{1}{2\\beta}\\chi^2_d$. This is a Gamma distribution with shape $k=d/2$ and scale $\\theta=1/\\beta$.\n\nThe mean and variance of a $\\chi^2_d$ distribution are $d$ and $2d$, respectively. We can find the mean and variance of the energy $U(x)$:\n$$ \\mathbb{E}[U(x)] = \\mathbb{E}\\left[\\frac{1}{2\\beta}\\chi^2_d\\right] = \\frac{1}{2\\beta}\\mathbb{E}[\\chi^2_d] = \\frac{d}{2\\beta} $$\n$$ \\text{Var}(U(x)) = \\text{Var}\\left(\\frac{1}{2\\beta}\\chi^2_d\\right) = \\left(\\frac{1}{2\\beta}\\right)^2\\text{Var}(\\chi^2_d) = \\frac{2d}{4\\beta^2} = \\frac{d}{2\\beta^2} $$\nFor large $d$, the Central Limit Theorem states that the $\\chi^2_d$ distribution can be approximated by a normal distribution. Thus, for large $d$, the distribution of $U(x)$ is approximately normal:\n$$ U(x) \\approx \\mathcal{N}\\left(\\frac{d}{2\\beta}, \\frac{d}{2\\beta^2}\\right) $$\nWe are interested in the difference of energies $\\Delta U = U(x) - U(y)$, where $x \\sim \\pi_{\\beta_1}$ and $y \\sim \\pi_{\\beta_2}$ are independent. Using the normal approximation for large $d$:\n$$ U(x) \\approx \\mathcal{N}\\left(\\frac{d}{2\\beta_1}, \\frac{d}{2\\beta_1^2}\\right) \\quad \\text{and} \\quad U(y) \\approx \\mathcal{N}\\left(\\frac{d}{2\\beta_2}, \\frac{d}{2\\beta_2^2}\\right) $$\nSince $U(x)$ and $U(y)$ are independent, their difference $\\Delta U$ is also approximately normal, $\\Delta U \\approx \\mathcal{N}(\\mu_{\\Delta U}, \\sigma^2_{\\Delta U})$, with mean and variance:\n$$ \\mu_{\\Delta U} = \\mathbb{E}[U(x)] - \\mathbb{E}[U(y)] = \\frac{d}{2\\beta_1} - \\frac{d}{2\\beta_2} = \\frac{d(\\beta_2 - \\beta_1)}{2\\beta_1\\beta_2} = \\frac{d \\Delta\\beta}{2\\beta_1\\beta_2} $$\n$$ \\sigma^2_{\\Delta U} = \\text{Var}(U(x)) + \\text{Var}(U(y)) = \\frac{d}{2\\beta_1^2} + \\frac{d}{2\\beta_2^2} = \\frac{d}{2}\\left(\\frac{1}{\\beta_1^2} + \\frac{1}{\\beta_2^2}\\right) $$\nThe problem requires a leading-order approximation for small $\\Delta\\beta$. We use $\\beta_1 = \\bar{\\beta} - \\Delta\\beta/2$ and $\\beta_2 = \\bar{\\beta} + \\Delta\\beta/2$. For small $\\Delta\\beta$:\n$$ \\beta_1\\beta_2 = \\bar{\\beta}^2 - (\\Delta\\beta/2)^2 \\approx \\bar{\\beta}^2 $$\n$$ \\frac{1}{\\beta_1^2} + \\frac{1}{\\beta_2^2} \\approx \\frac{1}{\\bar{\\beta}^2} + \\frac{1}{\\bar{\\beta}^2} = \\frac{2}{\\bar{\\beta}^2} $$\nSubstituting these into the expressions for the mean and variance gives the leading-order approximations:\n$$ \\mu_{\\Delta U} \\approx \\frac{d \\Delta\\beta}{2\\bar{\\beta}^2} $$\n$$ \\sigma^2_{\\Delta U} \\approx \\frac{d}{2}\\left(\\frac{2}{\\bar{\\beta}^2}\\right) = \\frac{d}{\\bar{\\beta}^2} $$\nThe swap acceptance probability is $\\alpha = \\min\\{1, \\exp((\\beta_1 - \\beta_2)\\Delta U)\\}$. Since $\\beta_1 - \\beta_2 = -\\Delta\\beta$, we have:\n$$ \\alpha = \\min\\{1, \\exp(-\\Delta\\beta \\cdot \\Delta U)\\} $$\nLet's define a new random variable $K = \\Delta\\beta \\cdot \\Delta U$. Since $\\Delta U$ is approximately normal, $K$ is also approximately normal, $K \\approx \\mathcal{N}(\\mu_K, \\sigma_K^2)$, where:\n$$ \\mu_K = \\Delta\\beta \\cdot \\mu_{\\Delta U} \\approx \\Delta\\beta \\cdot \\frac{d \\Delta\\beta}{2\\bar{\\beta}^2} = \\frac{d(\\Delta\\beta)^2}{2\\bar{\\beta}^2} $$\n$$ \\sigma_K^2 = (\\Delta\\beta)^2 \\cdot \\sigma^2_{\\Delta U} \\approx (\\Delta\\beta)^2 \\cdot \\frac{d}{\\bar{\\beta}^2} = \\frac{d(\\Delta\\beta)^2}{\\bar{\\beta}^2} $$\nA crucial relationship emerges from these approximations: $\\mu_K \\approx \\frac{1}{2}\\sigma_K^2$. We will use this exact relationship for our approximated variables from now on.\n\nWe need to calculate $\\mathbb{E}[\\alpha] = \\mathbb{E}[\\min\\{1, \\exp(-K)\\}]$. Let $f_K(k)$ be the probability density function of $K \\approx \\mathcal{N}(\\mu_K, \\sigma_K^2)$. The expectation is given by:\n$$ \\mathbb{E}[\\alpha] = \\int_{-\\infty}^{\\infty} \\min\\{1, e^{-k}\\} f_K(k) \\,dk $$\nWe split the integral based on the value of $k$:\n$$ \\mathbb{E}[\\alpha] = \\int_{-\\infty}^{0} 1 \\cdot f_K(k) \\,dk + \\int_{0}^{\\infty} e^{-k} f_K(k) \\,dk $$\nThe first integral is the cumulative probability $P(K \\le 0)$:\n$$ \\int_{-\\infty}^{0} f_K(k) \\,dk = P(K \\le 0) = \\Phi\\left(\\frac{0 - \\mu_K}{\\sigma_K}\\right) = \\Phi\\left(-\\frac{\\mu_K}{\\sigma_K}\\right) $$\nFor the second integral, we have:\n$$ \\int_{0}^{\\infty} e^{-k} f_K(k) \\,dk = \\int_{0}^{\\infty} e^{-k} \\frac{1}{\\sqrt{2\\pi\\sigma_K^2}} \\exp\\left(-\\frac{(k - \\mu_K)^2}{2\\sigma_K^2}\\right) \\,dk $$\nWe complete the square in the exponent of the integrand. The term inside the exponential is:\n$$ -k - \\frac{(k - \\mu_K)^2}{2\\sigma_K^2} = -\\frac{2k\\sigma_K^2 + k^2 - 2k\\mu_K + \\mu_K^2}{2\\sigma_K^2} = -\\frac{k^2 - 2k(\\mu_K - \\sigma_K^2) + \\mu_K^2}{2\\sigma_K^2} $$\n$$ = -\\frac{(k - (\\mu_K - \\sigma_K^2))^2 - (\\mu_K - \\sigma_K^2)^2 + \\mu_K^2}{2\\sigma_K^2} = -\\frac{(k - (\\mu_K - \\sigma_K^2))^2}{2\\sigma_K^2} - \\mu_K + \\frac{\\sigma_K^2}{2} $$\nSo the integrand becomes:\n$$ \\exp\\left(-\\mu_K + \\frac{\\sigma_K^2}{2}\\right) \\frac{1}{\\sqrt{2\\pi\\sigma_K^2}} \\exp\\left(-\\frac{(k - (\\mu_K - \\sigma_K^2))^2}{2\\sigma_K^2}\\right) $$\nThe second part is the PDF of a normal distribution $\\mathcal{N}(\\mu_K - \\sigma_K^2, \\sigma_K^2)$. Let's call a random variable with this distribution $K'$. The integral becomes:\n$$ \\exp\\left(-\\mu_K + \\frac{\\sigma_K^2}{2}\\right) \\int_{0}^{\\infty} f_{K'}(k) \\,dk = \\exp\\left(-\\mu_K + \\frac{\\sigma_K^2}{2}\\right) P(K'  0) $$\n$$ P(K'  0) = 1 - \\Phi\\left(\\frac{0 - (\\mu_K - \\sigma_K^2)}{\\sigma_K}\\right) = 1 - \\Phi\\left(\\frac{-\\mu_K + \\sigma_K^2}{\\sigma_K}\\right) = \\Phi\\left(\\frac{\\mu_K - \\sigma_K^2}{\\sigma_K}\\right) $$\nNow, we use the key relation $\\mu_K = \\frac{1}{2}\\sigma_K^2$. The prefactor becomes $\\exp(-\\frac{1}{2}\\sigma_K^2 + \\frac{1}{2}\\sigma_K^2) = \\exp(0) = 1$.\nThe argument of $\\Phi$ in the first integral part becomes:\n$$ -\\frac{\\mu_K}{\\sigma_K} = -\\frac{\\frac{1}{2}\\sigma_K^2}{\\sigma_K} = -\\frac{\\sigma_K}{2} $$\nThe argument of $\\Phi$ in the second integral part becomes:\n$$ \\frac{\\mu_K - \\sigma_K^2}{\\sigma_K} = \\frac{\\frac{1}{2}\\sigma_K^2 - \\sigma_K^2}{\\sigma_K} = \\frac{-\\frac{1}{2}\\sigma_K^2}{\\sigma_K} = -\\frac{\\sigma_K}{2} $$\nCombining the results, the expected acceptance probability is:\n$$ \\mathbb{E}[\\alpha] \\approx \\Phi\\left(-\\frac{\\sigma_K}{2}\\right) + 1 \\cdot \\Phi\\left(-\\frac{\\sigma_K}{2}\\right) = 2\\Phi\\left(-\\frac{\\sigma_K}{2}\\right) $$\nFinally, we substitute the expression for $\\sigma_K$. Since $\\Delta\\beta  0$ and $\\bar{\\beta}  0$, the standard deviation is:\n$$ \\sigma_K = \\sqrt{\\frac{d(\\Delta\\beta)^2}{\\bar{\\beta}^2}} = \\frac{\\sqrt{d}\\Delta\\beta}{\\bar{\\beta}} $$\nThe resulting approximation for the expected acceptance probability is:\n$$ \\mathbb{E}[\\alpha] \\approx 2\\Phi\\left(-\\frac{\\sqrt{d}\\Delta\\beta}{2\\bar{\\beta}}\\right) $$\nThis expression relates the swap acceptance rate to the dimension $d$ and the temperature schedule parameters $\\bar{\\beta}$ and $\\Delta\\beta$.", "answer": "$$\\boxed{2\\Phi\\left(-\\frac{\\sqrt{d}\\Delta\\beta}{2\\bar{\\beta}}\\right)}$$", "id": "3326637"}, {"introduction": "While tempering methods are powerful tools for overcoming energy barriers, it is a common misconception that they universally improve sampling efficiency. This exercise provides a crucial counterexample by analyzing a scenario where tempering can actually increase the asymptotic variance of a Monte Carlo estimator ([@problem_id:3326619]). Working through this problem will foster a more nuanced and critical understanding of tempering, highlighting the importance of considering the properties of both the target distribution and the observable of interest.", "problem": "Let $X$ be a one-dimensional random variable with the Student-$t$ target distribution $\\pi_{\\nu}$ having $\\nu>2$ degrees of freedom, location $0$, and scale $1$, with probability density\n$$\n\\pi_{\\nu}(x) \\;=\\; c_{\\nu}\\,\\Big(1+\\frac{x^{2}}{\\nu}\\Big)^{-(\\nu+1)/2}, \\quad c_{\\nu} \\;=\\; \\frac{\\Gamma\\!\\big(\\frac{\\nu+1}{2}\\big)}{\\sqrt{\\nu\\pi}\\,\\Gamma\\!\\big(\\frac{\\nu}{2}\\big)}.\n$$\nConsider the observable $h(x)=x$. You will compare the asymptotic variance $\\sigma^{2}(h)$ of the Monte Carlo estimator of $\\mathbb{E}_{\\pi_{\\nu}}[h(X)]$ under two algorithms, measured in the usual central limit theorem scale where $\\sqrt{N}(\\hat{\\mu}_{N}-\\mu)\\Rightarrow \\mathcal{N}(0,\\sigma^{2}(h))$ as $N\\to\\infty$.\n\n1. Base algorithm: run an independence sampler whose proposal equals $\\pi_{\\nu}$, so that $\\{X_{t}\\}_{t\\ge 1}$ are independent and identically distributed draws from $\\pi_{\\nu}$. Use the standard sample mean $\\hat{\\mu}_{N}^{\\mathrm{base}}=\\frac{1}{N}\\sum_{t=1}^{N}h(X_{t})$.\n\n2. Tempered algorithm: run a two-temperature simulated tempering (ST) scheme with inverse-temperature set $\\{\\beta_{0},\\beta_{1}\\}=\\{1,b\\}$, where $b\\in(0,1)$ is fixed. At each iteration $t$, first draw $\\beta_{t}$ uniformly from $\\{1,b\\}$, then draw $X_{t}\\sim \\pi_{\\nu,\\beta_{t}}$ exactly, where $\\pi_{\\nu,\\beta}(x)\\propto \\pi_{\\nu}(x)^{\\beta}$ denotes the tempered target at inverse temperature $\\beta$. Construct the Horvitz–Thompson estimator\n$$\n\\hat{\\mu}_{N}^{\\mathrm{ST}} \\;=\\; \\frac{2}{N}\\sum_{t=1}^{N}\\mathbf{1}\\{\\beta_{t}=1\\}\\,h(X_{t}),\n$$\nwhich is unbiased for $\\mathbb{E}_{\\pi_{\\nu}}[h(X)]$ given the inclusion probability $1/2$ of $\\beta_{t}=1$.\n\nStarting only from fundamental facts about the Student-$t$ distribution and the central limit theorem for independent sequences, derive closed-form expressions for the asymptotic variances $\\sigma_{\\mathrm{base}}^{2}(h)$ and $\\sigma_{\\mathrm{ST}}^{2}(h)$ as functions of $\\nu$. Your final answer must be the pair of expressions for $\\sigma_{\\mathrm{base}}^{2}(h)$ and $\\sigma_{\\mathrm{ST}}^{2}(h)$, in that order, each in closed form as a function of $\\nu$. No numerical approximations are allowed. If any regularity condition is needed, state it clearly and verify it holds. The final answer must be a single analytical expression or a pair of analytical expressions; do not provide inequalities or equations as the final answer.", "solution": "The problem requires the derivation of the asymptotic variances for two different Monte Carlo estimators of the mean of a random variable $X$ following a Student-$t$ distribution. Let the observable be $h(x)=x$. The target distribution is $\\pi_\\nu$, the Student-$t$ distribution with $\\nu > 2$ degrees of freedom, location $0$, and scale $1$. The quantity to be estimated is $\\mu = \\mathbb{E}_{\\pi_{\\nu}}[h(X)] = \\mathbb{E}_{\\pi_{\\nu}}[X]$. Since the probability density function $\\pi_{\\nu}(x)$ is symmetric about $x=0$ and the first moment exists for $\\nu>1$ (a condition satisfied by $\\nu>2$), the mean is $\\mu=0$.\n\nThe asymptotic variance $\\sigma^{2}(h)$ is defined in the context of the Central Limit Theorem (CLT), which states that for a suitably constructed estimator $\\hat{\\mu}_N$, the quantity $\\sqrt{N}(\\hat{\\mu}_{N}-\\mu)$ converges in distribution to a normal distribution $\\mathcal{N}(0,\\sigma^{2}(h))$. For an estimator that is an average of $N$ independent and identically distributed (i.i.d.) random variables, $\\hat{\\mu}_N = \\frac{1}{N}\\sum_{t=1}^{N} Z_t$, where $\\mathbb{E}[Z_t]=\\mu$ and $\\mathrm{Var}(Z_t)  \\infty$, the CLT states that the asymptotic variance $\\sigma^2(h)$ is simply the variance of the individual term, $\\mathrm{Var}(Z_t)$. The regularity condition for this version of the CLT is that the variance of the summands must be finite.\n\nFirst, we calculate the asymptotic variance for the base algorithm, $\\sigma_{\\mathrm{base}}^{2}(h)$.\nThe estimator is the standard sample mean $\\hat{\\mu}_{N}^{\\mathrm{base}}=\\frac{1}{N}\\sum_{t=1}^{N}h(X_{t})$. The samples $\\{X_{t}\\}_{t\\ge 1}$ are i.i.d. draws from the target distribution $\\pi_{\\nu}$. Here, the terms in the sum are $Z_t = h(X_t) = X_t$. The sequence $\\{Z_t\\}$ is i.i.d.\nThe asymptotic variance is thus given by the variance of a single term:\n$$\n\\sigma_{\\mathrm{base}}^{2}(h) \\;=\\; \\mathrm{Var}_{\\pi_{\\nu}}(h(X)) \\;=\\; \\mathrm{Var}_{\\pi_{\\nu}}(X)\n$$\nA fundamental property of the standard Student-$t$ distribution with $\\nu$ degrees of freedom is that its variance is well-defined and finite for $\\nu > 2$, and its value is given by:\n$$\n\\mathrm{Var}_{\\pi_{\\nu}}(X) \\;=\\; \\mathbb{E}_{\\pi_{\\nu}}[X^2] - (\\mathbb{E}_{\\pi_{\\nu}}[X])^2 \\;=\\; \\mathbb{E}_{\\pi_{\\nu}}[X^2] - 0^2 \\;=\\; \\frac{\\nu}{\\nu-2}\n$$\nThe problem states that $\\nu>2$, so this variance is finite. This satisfies the regularity condition for the CLT.\nTherefore, the asymptotic variance for the base algorithm is:\n$$\n\\sigma_{\\mathrm{base}}^{2}(h) \\;=\\; \\frac{\\nu}{\\nu-2}\n$$\n\nNext, we calculate the asymptotic variance for the simulated tempering (ST) algorithm, $\\sigma_{\\mathrm{ST}}^{2}(h)$.\nThe estimator is the Horvitz-Thompson estimator $\\hat{\\mu}_{N}^{\\mathrm{ST}} = \\frac{2}{N}\\sum_{t=1}^{N}\\mathbf{1}\\{\\beta_{t}=1\\}\\,h(X_{t})$. We can write this as an average $\\hat{\\mu}_{N}^{\\mathrm{ST}} = \\frac{1}{N} \\sum_{t=1}^N Y_t$, where $Y_t = 2 \\cdot \\mathbf{1}\\{\\beta_{t}=1\\}\\,h(X_{t})$.\nAt each step $t$, the pair $(\\beta_t, X_t)$ is generated independently of all other steps. Specifically, $\\beta_t$ is drawn uniformly from $\\{1,b\\}$, and then $X_t$ is drawn from $\\pi_{\\nu, \\beta_t}$. This implies that the sequence of random variables $\\{Y_t\\}_{t \\ge 1}$ is i.i.d.\nThe asymptotic variance is therefore the variance of a single term $Y_t$:\n$$\n\\sigma_{\\mathrm{ST}}^{2}(h) \\;=\\; \\mathrm{Var}(Y_t) \\;=\\; \\mathbb{E}[Y_t^2] - (\\mathbb{E}[Y_t])^2\n$$\nFirst, we find the expectation of $Y_t$. We use the law of total expectation, conditioning on the value of $\\beta_t$:\n$$\n\\mathbb{E}[Y_t] \\;=\\; \\mathbb{E}[\\mathbb{E}[Y_t|\\beta_t]] \\;=\\; P(\\beta_t=1)\\mathbb{E}[Y_t|\\beta_t=1] + P(\\beta_t=b)\\mathbb{E}[Y_t|\\beta_t=b]\n$$\nThe probability of selecting either temperature is $P(\\beta_t=1)=P(\\beta_t=b)=1/2$.\nGiven $\\beta_t=1$, $Y_t = 2 \\cdot 1 \\cdot h(X_t) = 2X_t$, where $X_t \\sim \\pi_{\\nu,1} = \\pi_{\\nu}$. So, $\\mathbb{E}[Y_t|\\beta_t=1] = \\mathbb{E}[2X_t] = 2\\mathbb{E}_{\\pi_{\\nu}}[X] = 2 \\cdot 0 = 0$.\nGiven $\\beta_t=b$, $Y_t = 2 \\cdot 0 \\cdot h(X_t) = 0$. So, $\\mathbb{E}[Y_t|\\beta_t=b] = 0$.\nThus, $\\mathbb{E}[Y_t] = \\frac{1}{2} \\cdot 0 + \\frac{1}{2} \\cdot 0 = 0$. This confirms the estimator is unbiased for $\\mu=0$.\nNow we compute the second moment $\\mathbb{E}[Y_t^2]$ to find the variance:\n$$\n\\mathrm{Var}(Y_t) \\;=\\; \\mathbb{E}[Y_t^2] \\;=\\; \\mathbb{E}[(2 \\cdot \\mathbf{1}\\{\\beta_{t}=1\\}\\,h(X_{t}))^2] \\;=\\; \\mathbb{E}[4 \\cdot (\\mathbf{1}\\{\\beta_{t}=1\\})^2 \\cdot (h(X_t))^2]\n$$\nSince $(\\mathbf{1}\\{\\beta_{t}=1\\})^2 = \\mathbf{1}\\{\\beta_{t}=1\\}$ and $h(x)=x$, this simplifies to:\n$$\n\\mathrm{Var}(Y_t) \\;=\\; 4 \\cdot \\mathbb{E}[\\mathbf{1}\\{\\beta_{t}=1\\} \\cdot X_t^2]\n$$\nAgain, using the law of total expectation:\n$$\n\\mathbb{E}[\\mathbf{1}\\{\\beta_{t}=1\\} \\cdot X_t^2] \\;=\\; P(\\beta_t=1)\\mathbb{E}[\\mathbf{1}\\{\\beta_{t}=1\\} \\cdot X_t^2|\\beta_t=1] + P(\\beta_t=b)\\mathbb{E}[\\mathbf{1}\\{\\beta_{t}=1\\} \\cdot X_t^2|\\beta_t=b]\n$$\nIf $\\beta_t=1$, the expression inside the expectation is $1 \\cdot X_t^2$, where $X_t \\sim \\pi_{\\nu,1} = \\pi_{\\nu}$. The expectation is $\\mathbb{E}_{\\pi_{\\nu}}[X^2] = \\mathrm{Var}_{\\pi_{\\nu}}(X) = \\frac{\\nu}{\\nu-2}$.\nIf $\\beta_t=b$, the expression inside the expectation is $0 \\cdot X_t^2 = 0$, so its expectation is $0$.\nSubstituting these results:\n$$\n\\mathbb{E}[\\mathbf{1}\\{\\beta_{t}=1\\} \\cdot X_t^2] \\;=\\; \\frac{1}{2} \\cdot \\frac{\\nu}{\\nu-2} + \\frac{1}{2} \\cdot 0 \\;=\\; \\frac{1}{2} \\frac{\\nu}{\\nu-2}\n$$\nFinally, we can find the variance of $Y_t$:\n$$\n\\sigma_{\\mathrm{ST}}^{2}(h) \\;=\\; \\mathrm{Var}(Y_t) \\;=\\; 4 \\cdot \\left(\\frac{1}{2} \\frac{\\nu}{\\nu-2}\\right) \\;=\\; 2 \\frac{\\nu}{\\nu-2}\n$$\nThe variance $\\mathrm{Var}(Y_t)$ is finite since $\\nu>2$, so the regularity condition for the CLT is satisfied for the tempered algorithm as well.\n\nThe pair of closed-form expressions for the asymptotic variances are $\\sigma_{\\mathrm{base}}^{2}(h) = \\frac{\\nu}{\\nu-2}$ and $\\sigma_{\\mathrm{ST}}^{2}(h) = 2\\frac{\\nu}{\\nu-2}$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{\\nu}{\\nu-2}  2\\frac{\\nu}{\\nu-2}\n\\end{pmatrix}\n}\n$$", "id": "3326619"}, {"introduction": "Parallel Tempering and Simulated Tempering offer two distinct strategies for navigating rugged sampling landscapes. This computational practice highlights a fundamental difference in their implementation and performance, focusing on the role of the free energy estimates (or log-weights $g_k$) in Simulated Tempering ([@problem_id:3326582]). By comparing the mixing behavior of the two algorithms, you will gain practical insight into the robustness of Parallel Tempering and the critical importance of accurate weight-setting for an effective Simulated Tempering simulation.", "problem": "Consider a discrete energy landscape with three microstates, denoted by $L$, $C$, and $R$, with energy function $U$ specified by $U(L) = 0$, $U(C) = h$, and $U(R) = 0$, where $h > 0$ represents a barrier height. A macro-step of the local Metropolis–Hastings update at inverse temperature $\\beta$ is defined as two successive proposals that attempt to move from a well to the barrier and then from the barrier to the opposite well. Under this macro-step definition, and assuming standard Metropolis accept/reject with proposals that deterministically attempt to move to $C$ from either well and deterministically attempt to move to the opposite well from the barrier, the probability to flip from $L$ to $R$ (and symmetrically from $R$ to $L$) in one macro-step at inverse temperature $\\beta$ is $e^{-\\beta h}$.\n\nSimulated Tempering (ST) targets the joint distribution over state $x \\in \\{L,C,R\\}$ and inverse temperature index $k \\in \\{1,\\dots,K\\}$ with the density proportional to $\\exp(-\\beta_k U(x) + g_k)$, where $\\beta_k$ is the inverse temperature ladder and $g_k$ are scalar log-weights. The stationary marginal over temperatures in ST is proportional to the partition function $Z(\\beta_k)$ times the exponential of the log-weights, that is\n$$\n\\pi_k^{\\mathrm{ST}} \\propto Z(\\beta_k)\\, e^{g_k}, \\quad \\text{where} \\quad Z(\\beta) = \\sum_{x \\in \\{L,C,R\\}} e^{-\\beta U(x)} = 2 + e^{-\\beta h}.\n$$\nParallel Tempering (PT) uses $K$ replicas, one at each inverse temperature $\\beta_k$, and attempts neighbor swaps; in the present construction, for states restricted to $\\{L,R\\}$, swaps are always accepted because $U(L) = U(R)$, so the replica identity performing updates at $\\beta=1$ is uniformly distributed over the ladder in stationarity. Thus, the per macro-step probability that the well state at $\\beta=1$ changes under PT equals the average transition probability across the ladder,\n$$\nM_{\\mathrm{PT}} = \\frac{1}{K} \\sum_{k=1}^K e^{-\\beta_k h}.\n$$\nIn contrast, the per macro-step probability that the well state changes at $\\beta=1$ under ST equals the product of the stationary occupancy at $\\beta=1$ and the local flip probability there,\n$$\nM_{\\mathrm{ST}} = \\pi_1^{\\mathrm{ST}}\\, e^{-\\beta_1 h}, \\quad \\text{where} \\quad \\pi_1^{\\mathrm{ST}} = \\frac{Z(\\beta_1)\\, e^{g_1}}{\\sum_{j=1}^K Z(\\beta_j) e^{g_j}}.\n$$\nThis construction isolates the effect of inaccurate $g_k$ in ST, which induces skewed occupancy across temperatures via the stationary distribution and thereby changes $M_{\\mathrm{ST}}$ at $\\beta=1$, while $M_{\\mathrm{PT}}$ depends only on the ladder.\n\nYour task is to implement a program that, for a given barrier height $h$, inverse temperature ladder $(\\beta_1,\\dots,\\beta_K)$, and log-weights $(g_1,\\dots,g_K)$, computes the degradation ratio\n$$\nR = \\frac{M_{\\mathrm{ST}}}{M_{\\mathrm{PT}}}.\n$$\nA smaller value of $R$ indicates worse mixing at $\\beta=1$ under ST relative to PT with the same ladder. Derive this ratio from first principles based on the specified joint ST target, the partition function, and the macro-step flip probability $e^{-\\beta h}$, without using any additional formulas that are not implied by these definitions.\n\nUse the following test suite of parameter values:\n\n- Test case 1 (general case with inaccurate weights favoring hot temperatures by default): $h = 6.0$, $(\\beta_1,\\beta_2,\\beta_3) = (1.0,0.5,0.2)$, $(g_1,g_2,g_3) = (0.0,0.0,0.0)$.\n- Test case 2 (more severely inaccurate $g_k$ skewing towards hot temperatures): $h = 6.0$, $(\\beta_1,\\beta_2,\\beta_3) = (1.0,0.5,0.2)$, $(g_1,g_2,g_3) = (0.0,1.5,2.0)$.\n- Test case 3 (lower barrier with the same inaccurate weights as test case 1): $h = 3.0$, $(\\beta_1,\\beta_2,\\beta_3) = (1.0,0.5,0.2)$, $(g_1,g_2,g_3) = (0.0,0.0,0.0)$.\n- Test case 4 (control case with perfect weights in ST): $h = 6.0$, $(\\beta_1,\\beta_2,\\beta_3) = (1.0,0.5,0.2)$, and $g_k = -\\log Z(\\beta_k)$ for $k=1,2,3$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test cases, where each element is the floating-point value of $R$ for the corresponding test case. For example, the output format must be exactly of the form $[r_1,r_2,r_3,r_4]$ with no additional text.\n\nNo physical units are involved in this computation; all quantities are dimensionless. Angles do not appear in this problem. Express all numbers as decimal floating-point values.", "solution": "The problem requires the computation of the degradation ratio $R$, defined as the ratio of the mixing probabilities for Simulated Tempering ($M_{\\mathrm{ST}}$) and Parallel Tempering ($M_{\\mathrm{PT}}$), $R = M_{\\mathrm{ST}}/M_{\\mathrm{PT}}$. This ratio quantifies the performance of Simulated Tempering at the target inverse temperature $\\beta_1=1$ relative to an idealized Parallel Tempering setup on the same temperature ladder. The derivation and computation are based entirely on the principles and definitions provided.\n\nFirst, we recall the definitions for the per macro-step flip probabilities for the two methods. For Parallel Tempering (PT), the problem specifies a construction where the replica at the target inverse temperature $\\beta_1$ samples uniformly from the entire ladder of $K$ inverse temperatures $\\{\\beta_k\\}_{k=1}^K$. Consequently, its effective flip probability is the average of the local flip probabilities, $e^{-\\beta_k h}$, across the ladder:\n$$\nM_{\\mathrm{PT}} = \\frac{1}{K} \\sum_{k=1}^K e^{-\\beta_k h}\n$$\n\nFor Simulated Tempering (ST), the system evolves in a joint space of state $x$ and temperature index $k$. The probability of a state flip at the target inverse temperature $\\beta_1$ is the product of the stationary probability of the system being at that temperature, $\\pi_1^{\\mathrm{ST}}$, and the local flip probability at that temperature, $e^{-\\beta_1 h}$.\n$$\nM_{\\mathrm{ST}} = \\pi_1^{\\mathrm{ST}}\\, e^{-\\beta_1 h}\n$$\nThe stationary probability $\\pi_1^{\\mathrm{ST}}$ depends on the log-weights $g_k$ and the partition functions $Z(\\beta_k)$ at each temperature. The problem provides the expression for the stationary marginal probability over temperatures:\n$$\n\\pi_k^{\\mathrm{ST}} \\propto Z(\\beta_k)\\, e^{g_k}\n$$\nNormalizing this gives the probability for $k=1$:\n$$\n\\pi_1^{\\mathrm{ST}} = \\frac{Z(\\beta_1)\\, e^{g_1}}{\\sum_{j=1}^K Z(\\beta_j) e^{g_j}}\n$$\nWe are also given the partition function $Z(\\beta)$ for the three-state system with energy barrier $h$:\n$$\nZ(\\beta) = \\sum_{x \\in \\{L,C,R\\}} e^{-\\beta U(x)} = e^{-\\beta \\cdot 0} + e^{-\\beta h} + e^{-\\beta \\cdot 0} = 2 + e^{-\\beta h}\n$$\n\nTo derive the degradation ratio $R$, we substitute the expressions for $M_{\\mathrm{ST}}$ and $M_{\\mathrm{PT}}$ into its definition:\n$$\nR = \\frac{M_{\\mathrm{ST}}}{M_{\\mathrm{PT}}} = \\frac{\\pi_1^{\\mathrm{ST}}\\, e^{-\\beta_1 h}}{\\frac{1}{K} \\sum_{k=1}^K e^{-\\beta_k h}}\n$$\nNext, we substitute the full expression for $\\pi_1^{\\mathrm{ST}}$:\n$$\nR = \\frac{\\left( \\frac{Z(\\beta_1)\\, e^{g_1}}{\\sum_{j=1}^K Z(\\beta_j) e^{g_j}} \\right) e^{-\\beta_1 h}}{\\frac{1}{K} \\sum_{k=1}^K e^{-\\beta_k h}}\n$$\nRearranging this expression yields the final formula for the degradation ratio, which can be directly implemented programmatically:\n$$\nR = \\frac{K \\cdot Z(\\beta_1) \\cdot e^{g_1} \\cdot e^{-\\beta_1 h}}{\\left(\\sum_{j=1}^K Z(\\beta_j) e^{g_j}\\right) \\left(\\sum_{k=1}^K e^{-\\beta_k h}\\right)}\n$$\nThis formula depends only on the given parameters: the barrier height $h$, the inverse temperature ladder $\\boldsymbol{\\beta} = (\\beta_1, \\dots, \\beta_K)$, and the log-weights $\\mathbf{g} = (g_1, \\dots, g_K)$.\n\nThe algorithmic implementation will follow these steps for each test case:\n1.  Given $h$, $\\boldsymbol{\\beta}$, and $\\mathbf{g}$, determine the number of temperatures $K$. For Test Case 4, first compute the partition functions $Z(\\beta_k)$ to determine the values for $g_k = -\\log(Z(\\beta_k))$.\n2.  Compute an array of partition function values, $Z(\\beta_k) = 2 + e^{-\\beta_k h}$, for all $k=1, \\dots, K$.\n3.  Compute the terms in the denominator of the expression for $R$:\n    - The ST normalization term: $\\sum_{j=1}^K Z(\\beta_j) e^{g_j}$.\n    - The PT mixing term: $\\sum_{k=1}^K e^{-\\beta_k h}$.\n4.  Compute the term in the numerator of the expression for $R$: $K \\cdot Z(\\beta_1) \\cdot e^{g_1} \\cdot e^{-\\beta_1 h}$.\n5.  Calculate the final ratio $R$.\n\nThis procedure is applied to each of the four specified test cases to generate the final list of results.", "answer": "```python\nimport numpy as np\n\ndef calculate_ratio(h, betas, gs):\n    \"\"\"\n    Computes the degradation ratio R for a given set of parameters.\n\n    Args:\n        h (float): The energy barrier height.\n        betas (list or np.ndarray): The inverse temperature ladder.\n        gs (list or np.ndarray): The log-weights for ST.\n\n    Returns:\n        float: The degradation ratio R.\n    \"\"\"\n    betas = np.array(betas, dtype=float)\n    gs = np.array(gs, dtype=float)\n    K = len(betas)\n\n    # --- Calculation for M_PT ---\n    # The term e^(-beta_k * h) appears in both PT and ST calculations.\n    exp_beta_h = np.exp(-betas * h)\n    \n    # PT mixing probability is the average transition probability over the ladder.\n    m_pt_sum_term = np.sum(exp_beta_h)\n    m_pt = (1.0 / K) * m_pt_sum_term\n\n    # --- Calculation for M_ST ---\n    # The partition function Z(beta) = 2 + e^(-beta * h).\n    z_vals = 2.0 + exp_beta_h\n    \n    # Stationary probability pi_1^ST at the target temperature beta_1.\n    st_normalization_term = np.sum(z_vals * np.exp(gs))\n    \n    # Handle the case where the normalization term might be zero, although\n    # it is positive definite for finite z_vals and gs.\n    if st_normalization_term == 0.0:\n        pi_1_st = 0.0\n    else:\n        pi_1_st_numerator = z_vals[0] * np.exp(gs[0])\n        pi_1_st = pi_1_st_numerator / st_normalization_term\n\n    # ST mixing probability at the target temperature.\n    m_st = pi_1_st * exp_beta_h[0]\n\n    # --- Degradation Ratio R ---\n    # The ratio R = M_ST / M_PT.\n    # Handle the division-by-zero case, though unlikely with given problem constraints.\n    # If m_pt is 0, exp_beta_h are all 0, so m_st is also 0. R -> 0/0.\n    # In this limit, R can be shown to be K * pi_1_st, but here we can just return 0.\n    if m_pt == 0.0:\n        return 0.0\n    \n    return m_st / m_pt\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test suite.\n    \"\"\"\n    # Test case 1 (general case with inaccurate weights favoring hot temperatures by default): \n    # h = 6.0, (beta_1,beta_2,beta_3) = (1.0,0.5,0.2), (g_1,g_2,g_3) = (0.0,0.0,0.0).\n    tc1 = (6.0, [1.0, 0.5, 0.2], [0.0, 0.0, 0.0])\n\n    # Test case 2 (more severely inaccurate g_k skewing towards hot temperatures): \n    # h = 6.0, (beta_1,beta_2,beta_3) = (1.0,0.5,0.2), (g_1,g_2,g_3) = (0.0,1.5,2.0).\n    tc2 = (6.0, [1.0, 0.5, 0.2], [0.0, 1.5, 2.0])\n\n    # Test case 3 (lower barrier with the same inaccurate weights as test case 1): \n    # h = 3.0, (beta_1,beta_2,beta_3) = (1.0,0.5,0.2), (g_1,g_2,g_3) = (0.0,0.0,0.0).\n    tc3 = (3.0, [1.0, 0.5, 0.2], [0.0, 0.0, 0.0])\n\n    # Test case 4 (control case with perfect weights in ST): \n    # h = 6.0, (beta_1,beta_2,beta_3) = (1.0,0.5,0.2), and g_k = -log Z(beta_k) for k=1,2,3.\n    h4 = 6.0\n    betas4 = np.array([1.0, 0.5, 0.2])\n    z_vals4 = 2.0 + np.exp(-betas4 * h4)\n    gs4 = -np.log(z_vals4)\n    tc4 = (h4, betas4.tolist(), gs4.tolist())\n    \n    test_cases = [tc1, tc2, tc3, tc4]\n    \n    results = []\n    for h_val, betas_val, gs_val in test_cases:\n        ratio = calculate_ratio(h_val, betas_val, gs_val)\n        results.append(ratio)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3326582"}]}