{"hands_on_practices": [{"introduction": "To build a solid understanding of Piecewise Deterministic Markov Processes (PDMPs), we begin with the most fundamental and analytically tractable example: the Bouncy Particle Sampler (BPS) targeting a multivariate Gaussian distribution. For this model, the negative log-density $U(x)$ is a quadratic function, which simplifies the event rate calculation significantly. This exercise [@problem_id:3323706] will guide you through deriving the exact event times from first principles, providing a concrete foundation before we tackle more complex scenarios.", "problem": "Consider the bouncy particle sampler for a target density proportional to $\\exp(-U(x))$ on $\\mathbb{R}^{d}$, where $U(x)=\\frac{1}{2}x^{\\top}A x$ with $A$ symmetric positive definite. The sampler is a piecewise deterministic Markov process (PDMP) with flow $\\dot{x}=v$ and $\\dot{v}=0$ between events, event (bounce) rate $\\lambda(x,v)=\\max\\{0, v^{\\top}\\nabla U(x)\\}$, and a specular reflection of the velocity at event times across the hyperplane orthogonal to $\\nabla U(x)$ that preserves the component of $v$ tangent to the level set of $U$ and flips the component normal to $\\nabla U(x)$.\n\n1. Using only the geometric characterization of specular reflection across a hyperplane with normal vector $\\nabla U(x)$, derive the explicit expression for the post-bounce velocity $v'$ in terms of $x$ and $v$.\n\n2. Let the process start at $(x(0),v(0))=(x_{0},v)$ with $x_{0}\\in\\mathbb{R}^{d}$ and $v\\in\\mathbb{R}^{d}$ such that $v^{\\top}A x_{0}>0$ and $v^{\\top}A v>0$. Using the definition of the event rate and the deterministic flow, derive an explicit inverse-transform sampling formula for the first event time $\\tau$, expressed as a deterministic function of a unit-rate exponential random variable $E$. Your derivation must start from the definition of the inhomogeneous Poisson process with rate $t\\mapsto \\lambda(x(t),v(t))$ and the fundamental change-of-variables principle for integrated hazard. Express the final formula for $\\tau$ as a closed-form analytic expression in terms of $A$, $x_{0}$, $v$, and $E$.\n\nState your final answer as the single explicit expression for $\\tau$ as a function of $A$, $x_{0}$, $v$, and $E$. No numerical evaluation is required.", "solution": "The problem is evaluated as valid, as it is scientifically grounded, well-posed, and objective. It consists of two parts concerning the Bouncy Particle Sampler (BPS) for a multivariate Gaussian target distribution. We will solve both parts sequentially.\n\nThe target probability density is proportional to $\\exp(-U(x))$ for $x \\in \\mathbb{R}^{d}$, where the potential energy is given by $U(x) = \\frac{1}{2}x^{\\top}A x$. The matrix $A$ is specified as symmetric and positive definite. The gradient of the potential, which will be frequently used, is $\\nabla U(x) = Ax$. The BPS evolves according to the state $(x(t), v(t))$, with deterministic dynamics between discrete bounce events. The flow is defined by $\\dot{x}(t) = v(t)$ and $\\dot{v}(t) = 0$, implying linear motion at a constant velocity between events. The rate of these events is $\\lambda(x,v) = \\max\\{0, v^{\\top}\\nabla U(x)\\}$.\n\nPart 1: Derivation of the post-bounce velocity.\nThe problem describes the bounce event as a specular reflection of the velocity vector $v$ across the hyperplane that is orthogonal to the gradient vector $\\nabla U(x)$. This means the component of $v$ parallel to $\\nabla U(x)$ is inverted, while the component orthogonal to $\\nabla U(x)$ is preserved.\n\nLet $n = \\nabla U(x) = Ax$ be the normal vector to the reflection hyperplane at position $x$. We can decompose the incoming velocity vector $v$ into two components: $v_{\\parallel}$ parallel to $n$, and $v_{\\perp}$ perpendicular to $n$.\n$$v = v_{\\parallel} + v_{\\perp}$$\nThe component parallel to $n$ is the orthogonal projection of $v$ onto $n$:\n$$v_{\\parallel} = \\frac{v^{\\top}n}{n^{\\top}n} n$$\nThe component perpendicular to $n$ is then given by:\n$$v_{\\perp} = v - v_{\\parallel} = v - \\frac{v^{\\top}n}{n^{\\top}n} n$$\nAccording to the rule of specular reflection, the post-bounce velocity $v'$ is obtained by reversing the parallel component and keeping the perpendicular component unchanged:\n$$v' = v_{\\perp} - v_{\\parallel} = \\left(v - v_{\\parallel}\\right) - v_{\\parallel} = v - 2v_{\\parallel}$$\nSubstituting the expression for $v_{\\parallel}$, we get:\n$$v' = v - 2 \\frac{v^{\\top}n}{n^{\\top}n} n$$\nFinally, substituting $n = Ax$, we obtain the explicit expression for the post-bounce velocity $v'$ in terms of $x$, $v$, and $A$:\n$$v' = v - 2 \\frac{v^{\\top}(Ax)}{(Ax)^{\\top}(Ax)} (Ax) = v - 2 \\frac{v^{\\top}Ax}{x^{\\top}A^{\\top}Ax} Ax$$\nSince $A$ is symmetric ($A^{\\top}=A$), this can be written as:\n$$v' = v - 2 \\frac{v^{\\top}Ax}{x^{\\top}A^2 x} Ax$$\nThis completes the derivation for the first part.\n\nPart 2: Derivation of the first event time $\\tau$.\nWe are tasked with deriving a formula to sample the first event time, $\\tau$, using inverse transform sampling. The process starts at $(x(0), v(0)) = (x_0, v)$. Between events, the particle's position evolves as $x(t) = x_0 + vt$. The velocity $v$ remains constant.\n\nThe event rate at time $t$ is given by $\\lambda(t) = \\lambda(x(t), v)$. We substitute the expression for $x(t)$ into the rate function:\n$$\\lambda(t) = \\max\\{0, v^{\\top}\\nabla U(x(t))\\} = \\max\\{0, v^{\\top}A(x_0 + vt)\\}$$\nExpanding the term inside the maximum gives:\n$$\\lambda(t) = \\max\\{0, v^{\\top}Ax_0 + t(v^{\\top}Av)\\}$$\nThe problem provides the initial conditions $v^{\\top}Ax_0 > 0$ and $v^{\\top}Av > 0$. Let us define the constants $a = v^{\\top}Ax_0$ and $b = v^{\\top}Av$. With these definitions, $a>0$ and $b>0$. The rate function for $t \\ge 0$ simplifies to:\n$$\\lambda(t) = a + bt$$\nThe first event time $\\tau$ of an inhomogeneous Poisson process with rate $\\lambda(t)$ can be sampled using the inverse transform method. The fundamental principle states that the integrated hazard up to time $\\tau$, $\\Lambda(\\tau)$, follows a standard exponential distribution. Let $E$ be a random variable drawn from an exponential distribution with rate $1$. Then we must solve the equation $\\Lambda(\\tau) = E$ for $\\tau$.\n\nThe integrated hazard function $\\Lambda(t)$ is the integral of the rate function from $0$ to $t$:\n$$\\Lambda(t) = \\int_0^t \\lambda(s) ds = \\int_0^t (a + bs) ds = \\left[as + \\frac{1}{2}bs^2\\right]_0^t = at + \\frac{1}{2}bt^2$$\nSetting $\\Lambda(\\tau) = E$, we obtain a quadratic equation for $\\tau$:\n$$a\\tau + \\frac{1}{2}b\\tau^2 = E$$\nRearranging into standard form:\n$$\\frac{b}{2}\\tau^2 + a\\tau - E = 0$$\nWe solve this equation for $\\tau$ using the quadratic formula:\n$$\\tau = \\frac{-a \\pm \\sqrt{a^2 - 4\\left(\\frac{b}{2}\\right)(-E)}}{2\\left(\\frac{b}{2}\\right)} = \\frac{-a \\pm \\sqrt{a^2 + 2bE}}{b}$$\nSince $\\tau$ represents a time, it must be non-negative. We are given $a > 0$ and $b > 0$, and $E$ is a non-negative random variable. The term under the square root, $\\sqrt{a^2 + 2bE}$, is always greater than or equal to $\\sqrt{a^2} = a$.\nThe root corresponding to the minus sign, $\\frac{-a - \\sqrt{a^2 + 2bE}}{b}$, is always non-positive.\nThe root corresponding to the plus sign, $\\frac{-a + \\sqrt{a^2 + 2bE}}{b}$, is always non-negative.\nTherefore, the physically meaningful solution for the event time $\\tau$ is:\n$$\\tau = \\frac{-a + \\sqrt{a^2 + 2bE}}{b}$$\nSubstituting back the expressions for $a$ and $b$ in terms of $A$, $x_0$, and $v$:\n$$\\tau = \\frac{-(v^{\\top}Ax_0) + \\sqrt{(v^{\\top}Ax_0)^2 + 2(v^{\\top}Av)E}}{v^{\\top}Av}$$\nThis is the final closed-form expression for the first event time $\\tau$.", "answer": "$$\n\\boxed{\\frac{-(v^{\\top}Ax_{0}) + \\sqrt{(v^{\\top}Ax_{0})^{2} + 2(v^{\\top}Av)E}}{v^{\\top}Av}}\n$$", "id": "3323706"}, {"introduction": "While Gaussian targets provide excellent intuition, most real-world potentials are not simple quadratics, making exact event time calculation impossible. This practice [@problem_id:3323688] introduces the workhorse algorithm for simulating BPS for general smooth potentials: the thinning method. You will learn to construct a local linear upper bound for the event rate using the Lipschitz continuity of the potential's gradient, a powerful technique that enables efficient and exact simulation without needing to solve complex integrals.", "problem": "Consider a continuously differentiable potential $U:\\mathbb{R}^d\\to\\mathbb{R}$ whose gradient $\\nabla U$ is globally Lipschitz with constant $L>0$, meaning $\\|\\nabla U(x)-\\nabla U(y)\\|\\le L\\|x-y\\|$ for all $x,y\\in\\mathbb{R}^d$. In the Bouncy Particle Sampler (BPS), a Piecewise Deterministic Markov Process (PDMP) used in nonreversible Markov Chain Monte Carlo (MCMC), the event rate at state $(x,v)$ is $\\lambda(x,v)=\\max\\{0,\\,v\\cdot\\nabla U(x)\\}$, where $v\\in\\mathbb{R}^d$ is the velocity vector. Along the deterministic trajectory $x(t)=x_0+t\\,v$, the instantaneous event rate becomes $\\lambda(t)=\\max\\{0,\\,v\\cdot\\nabla U(x_0+t\\,v)\\}$.\n\nStarting from the Lipschitz property and basic calculus along lines, derive an upper envelope $\\bar{r}(t)$ for the rate $\\lambda(t)$ that is suitable for standard thinning-based simulation of the first event time. Specifically:\n\n1. Use the Lipschitz property of $\\nabla U$ and the definition of directional derivatives to obtain a bound of the form $v\\cdot\\nabla U(x_0+t\\,v)\\le a+b\\,t$ for constants $a$ and $b$ that you must define explicitly in terms of $x_0$, $v$, $L$, and $\\nabla U(x_0)$. Then define the envelope function $\\bar{r}(t)=\\max\\{0,\\,a+b\\,t\\}$ and state precisely its piecewise structure in terms of the time at which $a+b\\,t$ changes sign.\n\n2. Provide explicit formulas for the integrated envelope $\\bar{R}(t)=\\int_0^t\\bar{r}(s)\\,ds$ over time, emphasizing its piecewise quadratic form, and show how to invert $\\bar{R}(t)$ to solve for the first proposed event time $t$ given an exponential variate $E>0$ (with $E$ dimensionless). Your inversion formula must handle both branches corresponding to $a\\ge 0$ and $a<0$.\n\n3. Give explicit formulas for bound update intervals when the envelope is refreshed at a new anchor point $x_{\\mathrm{anc}}=x_0+s\\,v$. Define the interval length $\\delta$ over which $\\bar{r}(t)$ remains identically zero before becoming positive. Express $\\delta$ in terms of $L$, $v$, and $v\\cdot\\nabla U(x_{\\mathrm{anc}})$.\n\nYour program must implement these constructions and verify their correctness on the following test suite of quadratic potentials $U(x)=\\tfrac{1}{2}x^\\top A x$ in $d=2$, where $A$ is symmetric:\n\n- For each test matrix, the Lipschitz constant is the operator norm of the Hessian, i.e., $L=\\|A\\|_2$, which for symmetric $A$ equals the largest magnitude eigenvalue.\n\nUse the following test cases:\n\n- Test 1 (general case): $A=\\mathrm{diag}(3.0,1.0)$, $x_0=(1.0,-0.5)$, $v=(0.6,-0.8)$. Verify that $\\bar{r}(t)\\ge \\lambda(t)$ for $101$ equally spaced $t\\in[0,1]$. The output is a boolean.\n\n- Test 2 (negative intercept and bound-update interval): $A=\\mathrm{diag}(2.0,2.0)$, $x_0=(-0.2,0.1)$, $v=(0.5,0.5)$. Compute the bound-update interval length $\\delta=\\max\\{0,\\,-a/b\\}$, where $a=v\\cdot\\nabla U(x_0)$ and $b=L\\|v\\|^2$. The output is a float. Use units consistent with time in the PDMP (dimensionless).\n\n- Test 3 (inversion of integrated envelope with initial zero branch): Reuse Test 2 and compute the first proposed event time by solving $\\bar{R}(t)=E$ with $E=0.05$. The output is a float.\n\n- Test 4 (indefinite quadratic but globally Lipschitz gradient): $A=\\mathrm{diag}(3.0,-2.0)$, $x_0=(0.4,0.6)$, $v=(0.5,-0.3)$. Verify $\\bar{r}(t)\\ge \\lambda(t)$ for $101$ equally spaced $t\\in[0,1]$. The output is a boolean.\n\n- Test 5 (tight envelope when $v$ aligns with the largest-eigenvalue direction): $A=\\mathrm{diag}(5.0,1.0)$, $x_0=(0.2,0.1)$, $v=(1.0,0.0)$. Compute the ratio $\\bar{R}(1)/\\int_0^1\\lambda(s)\\,ds$. The output is a float.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3,result4,result5]\").", "solution": "The problem statement is evaluated as scientifically grounded, well-posed, and objective. It is based on established principles of Markov Chain Monte Carlo methods, specifically the Bouncy Particle Sampler, and the mathematical tools used for its analysis, such as Lipschitz continuity and simulation via thinning. The tasks are specified with sufficient detail and formal rigor to admit a unique and verifiable solution. We may therefore proceed with the derivation and implementation.\n\nThe core of the problem is to construct a tractable upper bound for the BPS event rate $\\lambda(t) = \\max\\{0, v \\cdot \\nabla U(x_0+tv)\\}$ along a linear trajectory $x(t) = x_0+tv$. This bound, denoted $\\bar{r}(t)$, facilitates the simulation of event times using the thinning method.\n\n### Part 1: Derivation of the Upper Envelope $\\bar{r}(t)$\n\nWe aim to find a linear upper bound for the quantity $g(t) = v \\cdot \\nabla U(x_0+tv)$. We begin by expressing $g(t)$ in terms of its value at $t=0$:\n$$g(t) = v \\cdot \\nabla U(x_0) + v \\cdot \\left( \\nabla U(x_0+tv) - \\nabla U(x_0) \\right)$$\nBy applying the Cauchy-Schwarz inequality to the second term, we get:\n$$v \\cdot \\left( \\nabla U(x_0+tv) - \\nabla U(x_0) \\right) \\le \\|v\\| \\cdot \\| \\nabla U(x_0+tv) - \\nabla U(x_0) \\|$$\nThe problem states that the gradient $\\nabla U$ is globally Lipschitz with constant $L>0$. Applying this property with positions $x = x_0+tv$ and $y=x_0$, we have:\n$$\\| \\nabla U(x_0+tv) - \\nabla U(x_0) \\| \\le L \\|(x_0+tv) - x_0\\| = L \\|tv\\| = L|t| \\|v\\|$$\nFor forward time evolution, we consider $t \\ge 0$, so $|t|=t$. Combining these inequalities yields:\n$$v \\cdot \\left( \\nabla U(x_0+tv) - \\nabla U(x_0) \\right) \\le \\|v\\| \\cdot (L t \\|v\\|) = L\\|v\\|^2 t$$\nSubstituting this back into the expression for $g(t)$:\n$$v \\cdot \\nabla U(x_0+tv) \\le v \\cdot \\nabla U(x_0) + L\\|v\\|^2 t$$\nThis gives a linear upper bound on $v \\cdot \\nabla U(x_0+tv)$. We define the constants $a$ and $b$ as:\n$$a = v \\cdot \\nabla U(x_0)$$\n$$b = L\\|v\\|^2$$\nWith these definitions, the inequality is $v \\cdot \\nabla U(x_0+tv) \\le a+bt$. Since the event rate is $\\lambda(t) = \\max\\{0, v \\cdot \\nabla U(x_0+tv)\\}$, a valid upper envelope $\\bar{r}(t)$ is obtained by taking the positive part of our linear bound:\n$$\\bar{r}(t) = \\max\\{0, a+bt\\}$$\nThis envelope has a piecewise structure determined by the sign of $a+bt$. The term $a+bt$ is zero when $t=-a/b$. Since $L>0$ and we assume a non-zero velocity $v$, we have $b>0$.\n\nThe piecewise structure is as follows:\n- If $a \\ge 0$, then $-a/b \\le 0$. For all $t \\ge 0$, we have $a+bt \\ge 0$, so $\\bar{r}(t) = a+bt$.\n- If $a < 0$, then $-a/b > 0$. Let $\\delta = -a/b$.\n    - For $t \\in [0, \\delta)$, we have $a+bt < 0$, so $\\bar{r}(t)=0$.\n    - For $t \\ge \\delta$, we have $a+bt \\ge 0$, so $\\bar{r}(t)=a+bt$.\n\n### Part 2: Integrated Envelope $\\bar{R}(t)$ and Its Inversion\n\nTo simulate an event time using thinning, we generate a proposed time $t$ by inverting the integrated envelope $\\bar{R}(t) = \\int_0^t \\bar{r}(s)ds$. We draw a standard exponential variate $E \\sim \\mathrm{Exp}(1)$, which is dimensionless, and solve $\\bar{R}(t)=E$ for $t$.\n\nWe analyze the two cases for the structure of $\\bar{r}(t)$:\n\n**Case 1: $a \\ge 0$**\nFor $t \\ge 0$, $\\bar{r}(t) = a+bt$. The integrated rate is:\n$$\\bar{R}(t) = \\int_0^t (a+bs)ds = \\left[as + \\frac{1}{2}bs^2\\right]_0^t = at + \\frac{1}{2}bt^2$$\nTo find the proposed event time, we solve the quadratic equation $\\frac{1}{2}bt^2 + at - E = 0$ for $t > 0$. The quadratic formula gives:\n$$t = \\frac{-a \\pm \\sqrt{a^2 - 4(\\frac{1}{2}b)(-E)}}{b} = \\frac{-a \\pm \\sqrt{a^2 + 2bE}}{b}$$\nSince $t$ must be positive and $b>0$, we take the positive root:\n$$t = \\frac{-a + \\sqrt{a^2 + 2bE}}{b}$$\n\n**Case 2: $a < 0$**\nLet $\\delta = -a/b > 0$. The rate $\\bar{r}(s)$ is $0$ for $s \\in [0, \\delta)$ and $a+bs$ for $s \\ge \\delta$.\n- If $t \\in [0, \\delta)$, $\\bar{R}(t) = \\int_0^t 0 ds = 0$.\n- If $t \\ge \\delta$, the integrated rate is:\n$$\\bar{R}(t) = \\int_0^\\delta 0 ds + \\int_\\delta^t (a+bs)ds = \\left[as+\\frac{1}{2}bs^2\\right]_\\delta^t = (at+\\frac{1}{2}bt^2) - (a\\delta+\\frac{1}{2}b\\delta^2)$$\nSubstituting $\\delta = -a/b$, the second term is $a(-a/b) + \\frac{1}{2}b(-a/b)^2 = -a^2/b + a^2/(2b) = -a^2/(2b)$.\nThus, for $t \\ge \\delta$, $\\bar{R}(t) = at + \\frac{1}{2}bt^2 + \\frac{a^2}{2b} = \\frac{1}{2b}(2abt+b^2t^2+a^2) = \\frac{(a+bt)^2}{2b}$.\nA simpler way to derive this is to integrate from $\\delta$: $\\bar{R}(t) = \\int_{\\delta}^t (a+bs)ds$. Let $u=s-\\delta$. Then $s=u+\\delta$ and $ds=du$.\n$$\\bar{R}(t) = \\int_0^{t-\\delta} (a+b(u+\\delta))du = \\int_0^{t-\\delta} (a+b\\delta+bu)du$$\nSince $a+b\\delta=0$, the integral becomes $\\int_0^{t-\\delta} bu du = \\frac{1}{2}b(t-\\delta)^2$.\nTo solve $\\bar{R}(t)=E$, we must have $t \\ge \\delta$. We solve $\\frac{1}{2}b(t-\\delta)^2 = E$:\n$$(t-\\delta)^2 = \\frac{2E}{b} \\implies t-\\delta = \\sqrt{\\frac{2E}{b}} \\quad (\\text{since } t \\ge \\delta)$$\n$$t = \\delta + \\sqrt{\\frac{2E}{b}} = -\\frac{a}{b} + \\sqrt{\\frac{2E}{b}}$$\n\n### Part 3: Bound Update Interval $\\delta$\n\nWhen the BPS simulation advances time without an event, the linear bound on the rate can become loose. It is common practice to refresh the bound at a new anchor point $x_{\\mathrm{anc}} = x_0+s v$. The new bound is constructed for time $\\tau = t-s \\ge 0$ relative to the new anchor point.\nThe new linear bound for the rate is based on the new intercept $a_{\\mathrm{new}} = v \\cdot \\nabla U(x_{\\mathrm{anc}})$ and the same slope parameter $b = L\\|v\\|^2$. The new rate envelope is $\\bar{r}(\\tau) = \\max\\{0, a_{\\mathrm{new}} + b\\tau\\}$.\nThis envelope is identically zero if $a_{\\mathrm{new}} < 0$, over the time interval $\\tau \\in [0, -a_{\\mathrm{new}}/b)$. The length of this interval, which we call the bound-update interval, is $\\delta = -a_{\\mathrm{new}}/b$. If $a_{\\mathrm{new}} \\ge 0$, the rate is positive from $\\tau=0$ and this interval has length $0$.\nTherefore, the length of the interval is given by:\n$$\\delta = \\max\\left\\{0, -\\frac{a_{\\mathrm{new}}}{b}\\right\\} = \\max\\left\\{0, -\\frac{v \\cdot \\nabla U(x_{\\mathrm{anc}})}{L\\|v\\|^2}\\right\\}$$\nThis formula precisely matches the quantity requested in Test 2, where the anchor point is $x_0$.", "answer": "```python\nimport numpy as np\nimport math\n\ndef solve():\n    \"\"\"\n    Implements and verifies the BPS rate envelope calculations.\n    \"\"\"\n    results = []\n\n    # ===== Test 1: General case verification =====\n    A1 = np.diag([3.0, 1.0])\n    x0_1 = np.array([1.0, -0.5])\n    v_1 = np.array([0.6, -0.8])\n    \n    L1 = np.max(np.abs(np.linalg.eigvals(A1)))\n    grad_U_x0_1 = A1 @ x0_1\n    a1 = v_1 @ grad_U_x0_1\n    b1 = L1 * (v_1 @ v_1)\n    \n    t_vals = np.linspace(0, 1, 101)\n    lambda_t_1 = np.maximum(0, a1 + t_vals * (v_1 @ A1 @ v_1))\n    r_bar_t_1 = np.maximum(0, a1 + t_vals * b1)\n    \n    # Tiny tolerance for floating point comparisons\n    is_valid_bound_1 = np.all(r_bar_t_1 >= lambda_t_1 - 1e-9)\n    results.append(bool(is_valid_bound_1))\n\n    # ===== Test 2: Negative intercept and bound-update interval =====\n    A2 = np.diag([2.0, 2.0])\n    x0_2 = np.array([-0.2, 0.1])\n    v_2 = np.array([0.5, 0.5])\n    \n    L2 = np.max(np.abs(np.linalg.eigvals(A2)))\n    grad_U_x0_2 = A2 @ x0_2\n    a2 = v_2 @ grad_U_x0_2\n    b2 = L2 * (v_2 @ v_2)\n\n    delta_2 = max(0, -a2 / b2)\n    results.append(delta_2)\n\n    # ===== Test 3: Inversion of integrated envelope (from Test 2) =====\n    E3 = 0.05\n    # Using parameters a2, b2 from Test 2. Since a2 < 0:\n    t3 = (-a2 / b2) + math.sqrt(2 * E3 / b2)\n    results.append(t3)\n    \n    # ===== Test 4: Indefinite quadratic potential =====\n    A4 = np.diag([3.0, -2.0])\n    x0_4 = np.array([0.4, 0.6])\n    v_4 = np.array([0.5, -0.3])\n    \n    L4 = np.max(np.abs(np.linalg.eigvals(A4)))\n    grad_U_x0_4 = A4 @ x0_4\n    a4 = v_4 @ grad_U_x0_4\n    b4 = L4 * (v_4 @ v_4)\n    \n    lambda_t_4 = np.maximum(0, a4 + t_vals * (v_4 @ A4 @ v_4))\n    r_bar_t_4 = np.maximum(0, a4 + t_vals * b4)\n    \n    is_valid_bound_4 = np.all(r_bar_t_4 >= lambda_t_4 - 1e-9)\n    results.append(bool(is_valid_bound_4))\n    \n    # ===== Test 5: Tight envelope case =====\n    A5 = np.diag([5.0, 1.0])\n    x0_5 = np.array([0.2, 0.1])\n    v_5 = np.array([1.0, 0.0])\n    \n    L5 = np.max(np.abs(np.linalg.eigvals(A5)))\n    grad_U_x0_5 = A5 @ x0_5\n    a5 = v_5 @ grad_U_x0_5\n    b5 = L5 * (v_5 @ v_5)\n    \n    # Since a5 > 0, the integrated envelope over [0, 1] is a + b/2.\n    R_bar_1 = a5 * 1.0 + 0.5 * b5 * 1.0**2\n    \n    # The actual rate is lambda(t) = a + t * (v'Av).\n    vTAv5 = v_5 @ A5 @ v_5\n    # Since a5 > 0 and vTAv5 > 0, the max(0, ...) is redundant.\n    # Integrated actual rate over [0, 1] is a + (v'Av)/2.\n    R_actual_1 = a5 * 1.0 + 0.5 * vTAv5 * 1.0**2\n    \n    ratio_5 = R_bar_1 / R_actual_1\n    results.append(ratio_5)\n    \n    # Print the final results in the specified format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3323688"}, {"introduction": "A major challenge for modern samplers is scalability to massive datasets, where computing the full gradient at each step is prohibitively expensive. This exercise [@problem_id:3323722] tackles this problem head-on within the context of the Zig-Zag sampler. You will develop an advanced and computationally efficient bounding scheme that combines exact calculations on a small data subsample with deterministic bounds on the remainder, creating an unbiased rate estimate suitable for exact thinning and making these nonreversible PDMPs practical for large-scale Bayesian inference.", "problem": "Consider a Piecewise Deterministic Markov Process (PDMP) Zig-Zag sampler targeting a differentiable probability density with negative log-posterior potential $U(x) = \\sum_{n=1}^{N} \\ell_n(x)$ on $\\mathbb{R}^d$, where $x \\in \\mathbb{R}^d$ and each $\\ell_n(x)$ is a differentiable loss term. In the Zig-Zag process, velocities $v \\in \\{-1,+1\\}^d$ are constant between events, positions evolve deterministically as $x(t) = x_0 + t v$, and each coordinate $i \\in \\{1,\\dots,d\\}$ has an event intensity (hazard) $\\lambda_i(x(t), v) = \\max\\{0,\\, v_i \\,\\partial_i U(x(t))\\}$.\n\nYou are asked to derive and implement a stochastic upper bound function $M_i(t)$ for the intensity $\\lambda_i(x(t), v)$, suitable for exact Poisson thinning (that is, $M_i(t) \\ge \\lambda_i(x(t), v)$ holds almost surely for all $t \\ge 0$ under the randomness used to construct $M_i(t)$). The construction must be based on:\n- an unbiased subsampling estimator for $\\partial_i U(x)$,\n- concentration inequalities applied to bounded random variables, and\n- deterministic Lipschitz envelope bounds to ensure exactness.\n\nTo ensure scientific realism and a concrete instantiation, work with a logistic-regression negative log-posterior:\n$$\nU(x) \\;=\\; \\sum_{n=1}^{N} \\ell_n(x) \\;+\\; \\frac{\\alpha}{2}\\,\\|x\\|_2^2,\n$$\nwhere the prior precision is $\\alpha > 0$ and\n$$\n\\ell_n(x) \\;=\\; -y_n \\log \\sigma(w_n^\\top x) \\;-\\; (1-y_n)\\,\\log(1-\\sigma(w_n^\\top x)), \\quad \\sigma(a) = \\frac{1}{1+e^{-a}}.\n$$\nHere $y_n \\in \\{0,1\\}$ and $w_n \\in \\mathbb{R}^d$ are given data. For this model,\n$$\n\\partial_i \\ell_n(x) \\;=\\; (\\sigma(w_n^\\top x) - y_n)\\,w_{n,i}, \\quad \\partial_i U(x) \\;=\\; \\sum_{n=1}^{N} (\\sigma(w_n^\\top x) - y_n)\\,w_{n,i} \\;+\\; \\alpha\\,x_i.\n$$\n\nYou must:\n1. Derive from first principles a subsampling-based unbiased estimator $\\widehat{G}_i(x)$ of $\\partial_i U(x)$ and a one-sided upper confidence correction using a concentration inequality that applies to bounded summands. Explicitly specify bounds on each per-datum summand that justify the concentration inequality.\n2. Recognize that a high-probability upper confidence bound alone is insufficient for exact thinning. Derive a deterministic Lipschitz envelope for $|\\partial_i U(x(t))|$ along a Zig-Zag segment using the facts that $|\\sigma'(a)| \\le \\frac{1}{4}$ and $|(\\sigma(w_n^\\top x)-y_n)w_{n,i}| \\le |w_{n,i}|$. From this, construct an almost-sure upper bound $M_i(t)$ that incorporates:\n   - the subsampled exact absolute sum over a chosen subset $S \\subset \\{1,\\dots,N\\}$,\n   - a deterministic bound for the unsampled complement $S^c$,\n   - and a linear-in-$t$ Lipschitz envelope capturing the growth of $|\\partial_i U(x(t))|$ along the direction $v$.\n3. Show that the resulting $M_i(t)$ satisfies $M_i(t) \\ge \\lambda_i(x(t), v)$ for all $t \\ge 0$ and for all realizations of the subsampling randomness (that is, it is suitable for exact thinning).\n4. Implement a program that, for a synthetic dataset generated with a fixed random seed, verifies numerically that your derived $M_i(t)$ dominates the true $\\lambda_i(x(t), v)$ across a uniform grid of times in $[0,T]$. Your verification must compute the true intensity $\\lambda_i(x(t), v)$ using the full data (no approximation) and check $M_i(t) \\ge \\lambda_i(x(t), v)$ for each grid point.\n5. The program must internally generate the dataset and parameters and run the tests with no external input. Use the following test suite of cases, each specified by a tuple $(N, d, m, i, T, \\alpha, \\text{seed})$, where $m$ is the subsample size, $i$ is the coordinate index (zero-based), $T$ is the time horizon, and $\\alpha$ is the prior precision. The velocity $v$ for all tests must be $v \\in \\{-1,+1\\}^d$ with entries alternating in sign, starting with $+1$.\n   - Case A (happy path): $(200, 3, 20, 0, 0.5, 0.1, 123)$.\n   - Case B (boundary, no subsampling): $(200, 3, 0, 1, 0.25, 0.1, 456)$.\n   - Case C (edge, full subsample): $(200, 3, 200, 2, 0.1, 0.1, 789)$.\n   - Case D (stress, larger horizon): $(200, 3, 50, 1, 1.0, 0.1, 321)$.\n\nDetails and constraints:\n- Your derivation must start from the PDMP Zig-Zag intensity definition and only use fundamental properties of $\\sigma(\\cdot)$, boundedness of the per-datum contribution, and Lipschitz continuity implied by $\\sigma'(\\cdot)$.\n- You must define the subsampling estimator and the concentration correction clearly, including the bounding interval for each summand used in the inequality.\n- Angles are not involved. Physical units are not involved.\n- For numerical verification, use a uniform grid of $K=50$ points in $[0,T]$ to check the inequality. The output for each case must be a boolean that is true if $M_i(t) \\ge \\lambda_i(x(t), v)$ for all grid points and false otherwise.\n- Final Output Format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[True,False,True,False]\"). The boolean values must be in Python's canonical format (\"True\" or \"False\").", "solution": "The user has provided a valid problem concerning the construction of a stochastic thinning bound for a Zig-Zag sampler. The problem is scientifically grounded, well-posed, and all necessary components for its solution are specified. I will proceed with a full derivation and implementation.\n\n### Derivation of the Upper Bound for the Zig-Zag Intensity\n\nThe problem asks for an almost-surely correct upper bound $M_i(t)$ for the event intensity $\\lambda_i(x(t), v)$ of the $i$-th coordinate of a Zig-Zag sampler. The process targets a density with negative log-posterior $U(x)$, and the intensity is given by:\n$$\n\\lambda_i(x(t), v) = \\max\\{0, v_i \\partial_i U(x(t))\\}\n$$\nwhere $x(t) = x_0 + t v$ is the deterministic trajectory between events. Finding a bound $M_i(t) \\ge \\lambda_i(x(t), v)$ is equivalent to finding an upper bound on the quantity $h(t) = v_i \\partial_i U(x(t))$. The final bound for the intensity will then be $M_i(t) = \\max\\{0, \\text{upper\\_bound}(h(t))\\}$.\n\nThe potential is given by $U(x) = \\sum_{n=1}^{N} \\ell_n(x) + \\frac{\\alpha}{2}\\|x\\|_2^2$, leading to the partial derivative:\n$$\n\\partial_i U(x) = \\sum_{n=1}^{N} \\underbrace{(\\sigma(w_n^\\top x) - y_n)w_{n,i}}_{f_{n,i}(x)} + \\alpha x_i\n$$\n\nThe construction of the bound will proceed in two main steps:\n1. Deriving a linear-in-$t$ envelope for the time evolution of $\\partial_i U(x(t))$ using a Lipschitz constant.\n2. Constructing a bound for the initial value $v_i \\partial_i U(x_0)$ using subsampling and deterministic bounds.\n\n#### Part 1: High-Probability vs. Almost-Sure Bounds\n\nThe prompt first suggests employing an unbiased subsampling estimator for $\\partial_i U(x)$ and using concentration inequalities. Let $S \\subset \\{1, \\dots, N\\}$ be a random subset of size $m$. An unbiased estimator for $\\sum_{n=1}^N f_{n,i}(x)$ is $\\frac{N}{m}\\sum_{n \\in S} f_{n,i}(x)$. This gives an estimator for the full gradient:\n$$\n\\widehat{G}_i(x) = \\frac{N}{m} \\sum_{n \\in S} f_{n,i}(x) + \\alpha x_i\n$$\nThe terms $f_{n,i}(x)$ are bounded: since $y_n \\in \\{0,1\\}$ and $\\sigma(\\cdot) \\in (0,1)$, we have $|\\sigma(w_n^\\top x) - y_n| < 1$. This implies $|f_{n,i}(x)| \\le |w_{n,i}|$. The bounded nature of these terms allows the use of concentration inequalities (e.g., Hoeffding's inequality for sampling without replacement) to construct a high-probability confidence interval around the estimator. Such an approach would yield a bound $B_i(x,S)$ such that $P(v_i \\partial_i U(x) \\le B_i(x,S)) \\ge 1-\\delta$ for some small $\\delta > 0$.\n\nHowever, as correctly noted in the problem description, such a high-probability bound is insufficient for *exact* simulation via Poisson thinning. An exact thinning algorithm requires an upper bound $M_i(t)$ that holds **almost surely**, meaning with probability $1$ with respect to any randomness used in its construction (here, the choice of the subsample $S$). A bound that fails with non-zero probability would lead to a biased sampler. We therefore proceed with a construction that guarantees an almost-sure bound.\n\n#### Part 2: Almost-Sure Bound via Deterministic Bounding\n\nWe construct the bound by splitting sums over the full dataset $\\{1, \\dots, N\\}$ into a subsample $S$ (of size $m$) and its complement $S^c$. Terms over $S$ are computed exactly, while terms over $S^c$ are bounded deterministically.\n\n**Step 2.1: Lipschitz Envelope for Time Evolution**\n\nWe first establish a bound on the rate of change of $\\partial_i U(x(t))$. The time derivative of $\\partial_i U(x(t))$ is:\n$$\n\\frac{d}{dt} \\partial_i U(x(t)) = \\nabla(\\partial_i U(x(t)))^\\top \\frac{dx}{dt} = \\sum_{j=1}^d (\\partial_j \\partial_i U(x(t))) v_j\n$$\nThe Hessian entries are $\\partial_j \\partial_i U(x) = \\sum_{n=1}^N \\sigma'(w_n^\\top x)w_{n,i}w_{n,j} + \\alpha \\delta_{ij}$. Substituting this gives:\n$$\n\\frac{d}{dt} \\partial_i U(x(t)) = \\sum_{j=1}^d v_j \\left(\\sum_{n=1}^N \\sigma'(w_n^\\top x(t))w_{n,i}w_{n,j} + \\alpha \\delta_{ij}\\right) = \\sum_{n=1}^N \\sigma'(w_n^\\top x(t)) w_{n,i} (w_n^\\top v) + \\alpha v_i\n$$\nWe use the property $|\\sigma'(a)| \\le 1/4$ to bound the absolute value of this derivative:\n$$\n\\left|\\frac{d}{dt} \\partial_i U(x(t))\\right| \\le \\sum_{n=1}^N \\left|\\sigma'(w_n^\\top x(t))\\right| |w_{n,i}| |w_n^\\top v| + |\\alpha v_i| \\le \\frac{1}{4} \\sum_{n=1}^N |w_{n,i}| |w_n^\\top v| + \\alpha\n$$\nLet's define the constant Lipschitz bound $L_i$:\n$$\nL_i := \\frac{1}{4} \\sum_{n=1}^N |w_{n,i}| |w_n^\\top v| + \\alpha\n$$\nThis constant bounds the magnitude of the rate of change of $\\partial_i U(x(t))$ for all $t \\ge 0$. By integrating this bound, we have for any $t \\ge 0$:\n$$\n|\\partial_i U(x(t)) - \\partial_i U(x_0)| \\le L_i t\n$$\nThis implies $\\partial_i U(x(t)) \\le \\partial_i U(x_0) + L_i t$. Multiplying by $v_i \\in \\{-1, 1\\}$, we get an upper bound for $h(t) = v_i \\partial_i U(x(t))$:\n$$\nv_i \\partial_i U(x(t)) \\le v_i \\partial_i U(x_0) + L_i t\n$$\n\n**Step 2.2: Subsampling-based Bound at $t=0$**\n\nThe remaining task is to find an almost-sure upper bound for the initial term $v_i \\partial_i U(x_0)$. We do this by splitting the sum in $\\partial_i U(x_0)$ over the subsample $S$ and its complement $S^c$.\n$$\n\\partial_i U(x_0) = \\sum_{n \\in S} f_{n,i}(x_0) + \\sum_{n \\in S^c} f_{n,i}(x_0) + \\alpha x_{0,i}\n$$\nMultiplying by $v_i$:\n$$\nv_i \\partial_i U(x_0) = v_i \\left( \\sum_{n \\in S} f_{n,i}(x_0) + \\alpha x_{0,i} \\right) + v_i \\sum_{n \\in S^c} f_{n,i}(x_0)\n$$\nThe first part, involving the sum over $S$, can be computed exactly since $S$ is known. For the second part, we use a deterministic bound. As specified in the problem, we use the fact that $|f_{n,i}(x_0)| = |(\\sigma(w_n^\\top x_0)-y_n)w_{n,i}| \\le |w_{n,i}|$.\n$$\nv_i \\sum_{n \\in S^c} f_{n,i}(x_0) \\le \\left|v_i \\sum_{n \\in S^c} f_{n,i}(x_0)\\right| \\le \\sum_{n \\in S^c} |f_{n,i}(x_0)| \\le \\sum_{n \\in S^c} |w_{n,i}|\n$$\nThis bound holds for any $x_0$ and any choice of $S$. Substituting this back, we get an upper bound for $v_i \\partial_i U(x_0)$:\n$$\nv_i \\partial_i U(x_0) \\le v_i \\left( \\sum_{n \\in S} f_{n,i}(x_0) + \\alpha x_{0,i} \\right) + \\sum_{n \\in S^c} |w_{n,i}|\n$$\nLet us denote this initial bound by $M_0$:\n$$\nM_0 := v_i \\left( \\sum_{n \\in S} (\\sigma(w_n^\\top x_0) - y_n)w_{n,i} + \\alpha x_{0,i} \\right) + \\sum_{n \\in S^c} |w_{n,i}|\n$$\nNote that $M_0$ depends on the specific realization of the random sample $S$.\n\n**Step 2.3: The Final Bound $M_i(t)$**\n\nCombining the results from the previous steps, we obtain an upper bound for $v_i \\partial_i U(x(t))$:\n$$\nv_i \\partial_i U(x(t)) \\le v_i \\partial_i U(x_0) + L_i t \\le M_0 + L_i t\n$$\nLet $\\bar{M}_i(t) = M_0 + L_i t$. This bound holds for all $t \\ge 0$ and for any choice of subsample $S$. Finally, to bound the intensity $\\lambda_i(x(t),v) = \\max\\{0, v_i \\partial_i U(x(t))\\}$, we take the positive part of our bound:\n$$\nM_i(t) = \\max\\{0, \\bar{M}_i(t)\\} = \\max\\{0, M_0 + L_i t\\}\n$$\nThis completes the derivation. The function $M_i(t)$ satisfies $M_i(t) \\ge \\lambda_i(x(t), v)$ almost surely, as required for exact Poisson thinning. This construction incorporates an exact calculation on a subset $S$, a deterministic bound on the complement $S^c$, and a linear-in-$t$ Lipschitz envelope.\n\nThe special cases are handled correctly:\n- If $m=0$, $S$ is empty and the first sum in $M_0$ is zero. The second sum is over all $n \\in \\{1, \\dots, N\\}$.\n- If $m=N$, $S$ is the full set and $S^c$ is empty. The second sum in $M_0$ is zero, and $M_0 = v_i \\partial_i U(x_0)$ becomes an exact evaluation.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import expit as sigma\n\ndef solve():\n    \"\"\"\n    Derives and verifies a stochastic upper bound for the Zig-Zag sampler intensity\n    in a logistic regression model.\n    \"\"\"\n    # Test cases: (N, d, m, i, T, alpha, seed)\n    test_cases = [\n        (200, 3, 20, 0, 0.5, 0.1, 123),    # Case A: happy path\n        (200, 3, 0, 1, 0.25, 0.1, 456),    # Case B: boundary, no subsampling\n        (200, 3, 200, 2, 0.1, 0.1, 789),   # Case C: edge, full subsample\n        (200, 3, 50, 1, 1.0, 0.1, 321),    # Case D: stress, larger horizon\n    ]\n\n    results = []\n    # Number of grid points for numerical verification\n    K = 50\n\n    for N, d, m, i, T, alpha, seed in test_cases:\n        # Set up RNG for reproducibility\n        rng = np.random.default_rng(seed)\n\n        # Generate synthetic data for logistic regression\n        # Assumes a true parameter vector to generate plausible labels\n        x_true = rng.standard_normal(size=d)\n        w = rng.standard_normal(size=(N, d))\n        # Probabilities for y_n = 1\n        p_y = sigma(w @ x_true)\n        y = rng.binomial(1, p_y)\n\n        # Initial state for the Zig-Zag segment\n        x0 = rng.standard_normal(size=d)\n        # Velocity v has alternating signs: +1, -1, +1, ...\n        v = np.array([(-1)**k for k in range(d)])\n\n        # Select subsample S and its complement Sc\n        if m > N or m < 0:\n            raise ValueError(\"Subsample size m must be between 0 and N.\")\n        \n        S_indices = rng.choice(N, m, replace=False)\n        S_mask = np.zeros(N, dtype=bool)\n        S_mask[S_indices] = True\n        Sc_mask = ~S_mask\n\n        w_S, y_S = w[S_mask], y[S_mask]\n        w_Sc = w[Sc_mask]\n        \n        # --- Bound Construction M_i(t) ---\n\n        # Calculate M_0, the bound at t=0\n        # Term from the subsample S (computed exactly) + prior\n        f_ni_S = (sigma(w_S @ x0) - y_S) * w_S[:, i]\n        term_S_part = np.sum(f_ni_S)\n        term_prior = alpha * x0[i]\n        M0_S_part = v[i] * (term_S_part + term_prior)\n        \n        # Deterministic bounding term from the complement Sc\n        M0_Sc_part = np.sum(np.abs(w_Sc[:, i]))\n\n        M0 = M0_S_part + M0_Sc_part\n\n        # Calculate the Lipschitz constant L_i\n        w_dot_v = w @ v\n        L_i_sum_term = 0.25 * np.sum(np.abs(w[:, i]) * np.abs(w_dot_v))\n        L_i = L_i_sum_term + alpha\n\n        def bound_M_i(t: float) -> float:\n            \"\"\"Calculates the upper bound M_i(t) on the intensity.\"\"\"\n            m_bar = M0 + L_i * t\n            return np.maximum(0.0, m_bar)\n\n        # --- True Intensity lambda_i(t) ---\n        \n        def true_lambda_i(t: float) -> float:\n            \"\"\"Calculates the true intensity lambda_i(t) using the full dataset.\"\"\"\n            xt = x0 + t * v\n            # Gradient of log-likelihood for coordinate i\n            grad_log_lik_i = np.sum((sigma(w @ xt) - y) * w[:, i])\n            # Gradient of prior for coordinate i\n            grad_prior_i = alpha * xt[i]\n            # Full gradient of negative log-posterior\n            grad_U_i = grad_log_lik_i + grad_prior_i\n            \n            return np.maximum(0.0, v[i] * grad_U_i)\n            \n        # --- Numerical Verification ---\n        \n        t_grid = np.linspace(0.0, T, K)\n        \n        bound_holds = True\n        for t in t_grid:\n            true_val = true_lambda_i(t)\n            bound_val = bound_M_i(t)\n            # Check if the bound holds, with a small tolerance for floating point errors\n            if bound_val < true_val - 1e-9:\n                bound_holds = False\n                break\n        \n        results.append(bound_holds)\n        \n    # Format the final output as specified\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3323722"}]}