{"hands_on_practices": [{"introduction": "To begin our hands-on exploration, we first need to master the core mechanism that makes Delayed Rejection (DR) work. This exercise focuses on deriving the acceptance probability for a multi-stage DR algorithm directly from the principle of detailed balance [@problem_id:3302333]. By carefully constructing the ratio of forward and reverse path probabilities, you will see how the algorithm compensates for each rejected proposal to ensure the final chain still targets the correct distribution.", "problem": "Consider a target probability density known up to a normalizing constant, denoted by $\\pi(x)$ on $\\mathbb{R}^{d}$, and a Markov chain constructed by the Metropolis–Hastings (MH) method within the broader framework of Markov Chain Monte Carlo (MCMC). A delayed rejection (DR) scheme extends MH by allowing a second attempt, third attempt, and so on, after a rejection, using stage-specific proposal kernels with potentially increasing local adaptation. In DR of order $k$, one uses a family of proposal densities $\\{q_{j}(\\cdot)\\}_{j=1}^{k}$, where $q_{1}(x,\\cdot)$ is the first-stage kernel and subsequently $q_{j}(x,y_{1},\\ldots,y_{j-1},\\cdot)$ for $j \\ge 2$ may depend on the current state and the previously proposed points. Acceptance probabilities are chosen so that the resulting Markov transition kernel satisfies detailed balance with respect to $\\pi(\\cdot)$.\n\nSuppose we operate a DR scheme with $k=3$ stages. At a given iteration, from a current state $x$, we draw $y_{1} \\sim q_{1}(x,\\cdot)$ and attempt a standard MH accept/reject decision. If the proposal is rejected, we draw $y_{2} \\sim q_{2}(x,y_{1},\\cdot)$ and attempt a second-stage accept/reject. If rejected again, we draw $y_{3} \\sim q_{3}(x,y_{1},y_{2},\\cdot)$ and attempt a third-stage accept/reject, with acceptance probability denoted $\\alpha_{3}(x,y_{1},y_{2},y_{3})$. The acceptance probabilities at earlier stages are denoted by $\\alpha_{1}(x,y_{1})$ and $\\alpha_{2}(x,y_{1},y_{2})$, respectively. Assume that all proposal densities are everywhere positive on their supports and that all acceptance probabilities are chosen to enforce detailed balance and build a reversible Markov chain with respect to $\\pi(\\cdot)$.\n\nUsing the detailed balance principle and the fact that each stage-$j$ proposal in the forward path must be paired with the corresponding reverse path composed of $j$ nested proposals and $j-1$ rejections when viewed from the proposed state back to the current state, identify the correct explicit formula for the third-stage acceptance probability $\\alpha_{3}(x,y_{1},y_{2},y_{3})$ and describe the computational implications of evaluating the reverse-path quantities and the need for stored proposals.\n\nWhich option is correct?\n\nA. $\\alpha_{3}(x,y_{1},y_{2},y_{3}) = \\min\\Bigg\\{1,\\; \\dfrac{\\pi(y_{3})\\, q_{1}(y_{3},y_{2})\\, q_{2}(y_{3},y_{2},y_{1})\\, q_{3}(y_{3},y_{2},y_{1},x)\\, \\big(1-\\alpha_{1}(y_{3},y_{2})\\big)\\, \\big(1-\\alpha_{2}(y_{3},y_{2},y_{1})\\big)}{\\pi(x)\\, q_{1}(x,y_{1})\\, q_{2}(x,y_{1},y_{2})\\, q_{3}(x,y_{1},y_{2},y_{3})\\, \\big(1-\\alpha_{1}(x,y_{1})\\big)\\, \\big(1-\\alpha_{2}(x,y_{1},y_{2})\\big)} \\Bigg\\}$. Computationally, one must evaluate reverse-path proposal densities and reverse-path rejections, which generally requires target evaluations at $y_{1}$, $y_{2}$, and $y_{3}$; forward-path quantities $\\alpha_{1}(x,y_{1})$ and $\\alpha_{2}(x,y_{1},y_{2})$ should be stored to avoid recomputation. Memory and arithmetic costs grow on the order of $\\mathcal{O}(k)$ with the number of stages, and caching of $\\pi(\\cdot)$ and $q_{j}(\\cdot)$ evaluations is crucial to control overhead.\n\nB. $\\alpha_{3}(x,y_{1},y_{2},y_{3}) = \\min\\Bigg\\{1,\\; \\dfrac{\\pi(y_{3})\\, q_{1}(y_{3},x)\\, q_{2}(y_{3},x)\\, q_{3}(y_{3},x)}{\\pi(x)\\, q_{1}(x,y_{1})\\, q_{2}(x,y_{1},y_{2})\\, q_{3}(x,y_{1},y_{2},y_{3})} \\Bigg\\}$. Computationally, since only proposal densities appear, there is no need to evaluate any acceptance probabilities from previous stages nor to store intermediate proposals, so the memory and computational costs are negligible beyond evaluating the proposals.\n\nC. $\\alpha_{3}(x,y_{1},y_{2},y_{3}) = \\min\\Bigg\\{1,\\; \\dfrac{\\pi(y_{3})\\, q_{1}(y_{2},y_{3})\\, q_{2}(y_{1},y_{2},y_{3})\\, q_{3}(x,y_{1},y_{2},y_{3})\\, \\big(1-\\alpha_{1}(y_{2},y_{3})\\big)\\, \\big(1-\\alpha_{2}(y_{1},y_{2},y_{3})\\big)}{\\pi(x)\\, q_{1}(x,y_{1})\\, q_{2}(x,y_{1},y_{2})\\, q_{3}(x,y_{1},y_{2},y_{3})\\, \\big(1-\\alpha_{1}(x,y_{1})\\big)\\, \\big(1-\\alpha_{2}(x,y_{1},y_{2})\\big)} \\Bigg\\}$. Computationally, reverse-path rejections are unnecessary because the third stage directly mirrors the forward proposals; thus, only forward rejections should be stored.\n\nD. $\\alpha_{3}(x,y_{1},y_{2},y_{3}) = \\min\\Bigg\\{1,\\; \\dfrac{\\pi(y_{3})\\, q_{1}(y_{3},y_{2})\\, q_{2}(y_{3},y_{2},y_{1})\\, q_{3}(y_{3},y_{2},y_{1},x)\\, \\alpha_{1}(y_{3},y_{2})\\, \\alpha_{2}(y_{3},y_{2},y_{1})}{\\pi(x)\\, q_{1}(x,y_{1})\\, q_{2}(x,y_{1},y_{2})\\, q_{3}(x,y_{1},y_{2},y_{3})\\, \\alpha_{1}(x,y_{1})\\, \\alpha_{2}(x,y_{1},y_{2})} \\Bigg\\}$. Computationally, because acceptance probabilities rather than rejection probabilities appear, one can discard earlier proposals without loss, and no reverse-path evaluations are required as they cancel by symmetry.", "solution": "The core principle of the Delayed Rejection (DR) algorithm is generalized detailed balance. This principle requires that the probability flow for an entire sequence of moves from state $x$ to state $y_j$ must be balanced by the probability flow of the reverse sequence of moves from $y_j$ back to $x$.\n\nFor a stage-3 acceptance ($x \\to y_3$), the forward path consists of proposing $y_1$ and rejecting it, then proposing $y_2$ and rejecting it, and finally proposing $y_3$ and accepting it. The probability density for this forward path (up to the acceptance probability $\\alpha_3$) is:\n$$ \\text{Forward} = \\pi(x) \\cdot q_1(x, y_1) \\cdot \\big(1-\\alpha_1(x, y_1)\\big) \\cdot q_2(x, y_1, y_2) \\cdot \\big(1-\\alpha_2(x, y_1, y_2)\\big) \\cdot q_3(x, y_1, y_2, y_3) $$\n\nThe reverse path must be constructed to mirror this sequence. Starting from $y_3$, we must propose $y_2$ and reject it, then propose $y_1$ and reject it, and finally propose $x$ and accept it. The probability density for this reverse path is:\n$$ \\text{Reverse} = \\pi(y_3) \\cdot q_1(y_3, y_2) \\cdot \\big(1-\\alpha_1(y_3, y_2)\\big) \\cdot q_2(y_3, y_2, y_1) \\cdot \\big(1-\\alpha_2(y_3, y_2, y_1)\\big) \\cdot q_3(y_3, y_2, y_1, x) $$\n\nThe third-stage acceptance probability $\\alpha_3(x, y_1, y_2, y_3)$ is then defined in the standard Metropolis-Hastings form as the minimum of 1 and the ratio of the reverse path density to the forward path density:\n$$ \\alpha_3 = \\min\\left\\{1, \\frac{\\text{Reverse}}{\\text{Forward}}\\right\\} $$\n\nSubstituting the expressions gives the formula in option A.\n\n- **Option A** provides the correct formula derived from generalized detailed balance. It correctly includes the target densities, the sequence of forward and reverse proposal densities, and, crucially, the probabilities of rejection ($1-\\alpha_j$) for the intermediate stages of both the forward and reverse paths. The computational analysis is also correct: calculating the reverse-path terms requires evaluating $\\pi$ at the intermediate proposals ($y_1, y_2$) and storing previously computed forward-path quantities.\n- **Option B** is incorrect. It omits the critical rejection probability terms ($1-\\alpha_j$) and uses incorrect reverse proposal densities.\n- **Option C** is incorrect. It scrambles the arguments of the reverse proposal densities.\n- **Option D** is incorrect. It incorrectly uses acceptance probabilities ($\\alpha_j$) instead of rejection probabilities ($1-\\alpha_j$) for the intermediate steps. A move to a later stage only occurs after a rejection.\n\nTherefore, option A is the only correct choice.", "answer": "$$\\boxed{A}$$", "id": "3302333"}, {"introduction": "While Delayed Rejection can increase the overall acceptance rate, this benefit does not come for free, as each additional rejection stage adds computational overhead. This practice challenges you to move beyond a simple analysis of acceptance rates and perform a more rigorous cost-benefit analysis [@problem_id:3302307]. You will use the Expected Squared Jump Distance (ESJD) per unit time, a standard metric for sampler efficiency, to derive a quantitative criterion for when DR provides a net performance gain over a standard Metropolis-Hastings sampler.", "problem": "Consider a two-stage Delayed Rejection (DR) variant of the Metropolis–Hastings algorithm within Markov Chain Monte Carlo (MCMC), targeting a probability density $\\pi(x)$ on $\\mathbb{R}^d$. Let the current state be $X \\in \\mathbb{R}^d$. The stage-$1$ proposal is $Y \\sim q_1(X,\\cdot)$ with acceptance probability\n$$\n\\alpha_1(X,Y) \\;=\\; \\min\\!\\left\\{1, \\;\\frac{\\pi(Y)\\,q_1(Y,X)}{\\pi(X)\\,q_1(X,Y)}\\right\\}.\n$$\nIf the stage-$1$ proposal is rejected, a stage-$2$ proposal $Z \\sim q_2(X,Y,\\cdot)$ is drawn and accepted with probability $\\alpha_2(X,Y,Z)$ chosen so that the resulting Markov chain is reversible with respect to $\\pi$. A common choice (Mira’s two-stage delayed rejection acceptance) is\n$$\n\\alpha_2(X,Y,Z) \\;=\\; \\min\\!\\left\\{1, \\; \\frac{\\pi(Z)\\,q_1(Z,Y)\\,\\big(1-\\alpha_1(Z,Y)\\big)\\,q_2(Z,Y,X)}{\\pi(X)\\,q_1(X,Y)\\,\\big(1-\\alpha_1(X,Y)\\big)\\,q_2(X,Y,Z)}\\right\\}.\n$$\nDefine the following expected quantities under the stationary regime:\n- $p_1 \\;=\\; \\mathbb{E}\\!\\left[\\alpha_1(X,Y)\\right]$, the stage-$1$ acceptance probability.\n- $p_2 \\;=\\; \\mathbb{E}\\!\\left[\\alpha_2(X,Y,Z)\\,\\big|\\,\\text{stage-$1$ rejection}\\right]$, the stage-$2$ acceptance probability conditional on stage-$1$ rejection.\n- $J_1 \\;=\\; \\mathbb{E}\\!\\left[\\|Y-X\\|^2 \\,\\big|\\,\\text{stage-$1$ acceptance}\\right]$, the conditional expected squared jump distance at stage 1, where $\\|\\cdot\\|$ is the Euclidean norm.\n- $J_2 \\;=\\; \\mathbb{E}\\!\\left[\\|Z-X\\|^2 \\,\\big|\\,\\text{stage-$2$ acceptance}\\right]$, the conditional expected squared jump distance at stage 2$.\n\nAssume a cost model in which the expected computational time per iteration comprises a stage-$1$ cost $c_1>0$ that is always incurred, plus an incremental stage-$2$ cost $c_2>0$ that is incurred only when stage-$1$ rejects. For the one-stage baseline that uses only $q_1$ (no delayed rejection), the efficiency is measured by the expected squared jump distance per unit time, defined as the expected squared jump per iteration divided by the expected computational time per iteration.\n\nWhich option correctly identifies a quantitative criterion for when DR yields a net efficiency gain over the one-stage baseline, expressed in terms of the expected squared jump distance per unit time, and also highlights realistic cases where DR may not improve efficiency because of a poor choice of $q_2$ or increased computational cost?\n\nA. DR is more efficient than the baseline if\n$$\n\\frac{p_1\\,J_1 + (1-p_1)\\,p_2\\,J_2}{\\,c_1 + (1-p_1)\\,c_2\\,} \\;>\\; \\frac{p_1\\,J_1}{\\,c_1\\,}.\n$$\nDR may fail to improve efficiency when $q_2$ is overly local so that $J_2$ is negligible relative to $J_1$ (for example, a very small step size yielding $J_2 \\approx 0$), or when the stage-$2$ mechanism substantially increases cost (for example, $c_2 \\gg c_1$ due to expensive local curvature computations), so the left-hand side falls below the baseline.\n\nB. DR always improves efficiency because any additional acceptance strictly increases the expected squared jump distance per unit time. A valid criterion is\n$$\n\\frac{p_1\\,J_1 + p_2\\,J_2}{\\,c_1 + c_2\\,} \\;>\\; \\frac{p_1\\,J_1}{\\,c_1\\,},\n$$\nand no choice of $q_2$ or costs $c_2$ can make DR worse than the baseline.\n\nC. DR is beneficial if and only if $p_2>p_1$ and $c_2<c_1$. Poor choices of $q_2$ only matter through the acceptance rate $p_2$, not through the jump distance, so the only relevant criterion is $p_2>p_1$.\n\nD. DR is more efficient than the baseline if\n$$\n\\frac{p_1\\,J_1}{\\,c_1 + c_2\\,} \\;<\\; \\frac{p_1\\,J_1 + p_2\\,J_2}{\\,c_1\\,}.\n$$\nDR may fail when $q_2$ has low acceptance, but its jump magnitude $J_2$ does not enter the decision because only acceptance rates drive efficiency.\n\nE. DR may be harmful because the stage-$2$ correction can change the invariant distribution away from $\\pi$, so efficiency comparisons are ill-posed. A conservative criterion is to ensure $q_2$ leaves $\\pi$ invariant by itself, in which case DR improves efficiency unconditionally.", "solution": "To determine when Delayed Rejection (DR) is more efficient, we must compare the efficiency of the DR sampler to a baseline one-stage Metropolis-Hastings (MH) sampler. The efficiency is defined as the expected squared jump distance (ESJD) per unit of computational time.\n\n**1. Baseline MH Sampler Efficiency**\n- The expected computational time per iteration is constant: $\\mathbb{E}[\\text{Time}] = c_1$.\n- A jump occurs only upon acceptance, which happens with probability $p_1$. The conditional expected squared jump distance is $J_1$. The ESJD per iteration is therefore $p_1 J_1$.\n- The efficiency of the baseline MH sampler is:\n$$ \\text{Eff}_{\\text{MH}} = \\frac{p_1 J_1}{c_1} $$\n\n**2. Two-Stage DR Sampler Efficiency**\n- The expected computational time per iteration depends on whether the second stage is needed. The stage-1 cost $c_1$ is always incurred. The stage-2 cost $c_2$ is incurred only if stage 1 rejects, which happens with probability $(1-p_1)$.\n$$ \\mathbb{E}[\\text{Time}] = c_1 + (1-p_1)c_2 $$\n- The ESJD per iteration has two components: a jump from stage 1 (with probability $p_1$ and distance-squared $J_1$) and a jump from stage 2 (with probability $(1-p_1)p_2$ and distance-squared $J_2$).\n$$ \\text{ESJD}_{\\text{DR}} = p_1 J_1 + (1-p_1)p_2 J_2 $$\n- The efficiency of the DR sampler is:\n$$ \\text{Eff}_{\\text{DR}} = \\frac{p_1 J_1 + (1-p_1) p_2 J_2}{c_1 + (1-p_1) c_2} $$\n\n**3. Criterion for Efficiency Gain**\nThe DR scheme provides a net efficiency gain over the baseline when $\\text{Eff}_{\\text{DR}} > \\text{Eff}_{\\text{MH}}$:\n$$ \\frac{p_1 J_1 + (1-p_1) p_2 J_2}{c_1 + (1-p_1) c_2} > \\frac{p_1 J_1}{c_1} $$\nThis inequality correctly balances the potential gain in jump distance from the second stage against the additional computational cost.\n\nThe analysis in option A correctly identifies this criterion. It also correctly points out that DR may not be beneficial if the second stage offers negligible jump distance ($J_2 \\approx 0$) or is too computationally expensive ($c_2 \\gg c_1$), as either scenario can make the left-hand side of the inequality smaller than the right-hand side.", "answer": "$$\\boxed{A}$$", "id": "3302307"}, {"introduction": "The acceptance probability formulas for Delayed Rejection and Delayed Acceptance are complex, making implementation errors a common and serious concern. A buggy sampler can produce biased results that appear plausible, so how can we verify our code is correct? This final practice introduces you to several powerful diagnostic tests designed to detect a lack of reversibility, a key property that a correct implementation must satisfy [@problem_id:3302329]. These methods provide practical, hands-on tools for validating and debugging complex MCMC algorithms.", "problem": "Consider a Markov chain Monte Carlo (MCMC) implementation that mixes two mechanisms: a two-stage Delayed Acceptance (DA) scheme and a two-stage Delayed Rejection (DR) scheme. The target distribution has unnormalized density $\\pi(x)$ on $\\mathbb{R}^d$. In DA, the first-stage filter uses a cheap approximation $\\tilde{\\pi}(x)$, and in DR, a second-stage proposal is drawn conditionally on a first-stage rejection. The one-step Markov kernel of the implemented chain is $P(x,dy)$, possibly involving auxiliary randomness and mixtures of DA and DR moves.\n\nBy construction, a correctly implemented DA or DR algorithm should leave $\\pi$ invariant and be reversible with respect to $\\pi$, that is, satisfy detailed balance $\\pi(x) P(x,dy) = \\pi(y) P(y,dx)$. In practice, coding errors or numerical approximations can bias acceptance probabilities or disrupt reversibility.\n\nYou are tasked with selecting valid diagnostics to detect bias or lack of reversibility in such implementations, and with specifying the computations required to carry them out on a single long run $\\{X_t\\}_{t=1}^T$ for some large $T \\in \\mathbb{N}$. You may assume that for any recorded proposed move, you can evaluate the unnormalized target $\\pi(x)$ at visited states, the approximate target $\\tilde{\\pi}(x)$ used by DA at those states, and the proposal densities $q_1(x,y)$ and stage-$2$ conditional proposal densities $q_2(x,y;z)$ used by DR, for any triplet $(x,y,z)$ that actually occurred in the program. You may also assume you can recompute any acceptance ratio used by the code at the time of a proposal. No further structural formulas should be assumed beyond the definitions of detailed balance and reversibility.\n\nWhich of the following are valid and informative diagnostics to detect bias or lack of reversibility, and what computations do they require?\n\nA. Bidirectional composite-ratio identity check: For each recorded proposed pair $(x,y)$ at the stage where the algorithm would ultimately accept or reject the move, compute the composite Hastings ratio $R(x,y)$ actually used by the code. Recompute the same composite ratio but with roles reversed, $R(y,x)$, using the same stage-specific ingredients (including $\\tilde{\\pi}$ in DA and conditional $q_2$ in DR) as they would appear if the code were at $y$ proposing back to $x$. Report the distribution of the arithmetic errors $E(x,y) = \\log R(x,y) + \\log R(y,x)$ across all such pairs; under correct implementation, $E(x,y)$ should concentrate near $0$ up to numerical precision. This requires evaluating $\\pi(\\cdot)$ and $\\tilde{\\pi}(\\cdot)$ at $x$ and $y$, and all proposal densities that the code uses at the relevant stages for $(x,y)$ and $(y,x)$.\n\nB. Empirical detailed-balance flow symmetry via binning: Select a partition $\\{B_i\\}_{i=1}^m$ of state space into $m \\in \\mathbb{N}$ disjoint bins. From the path $\\{X_t\\}$, compute counts $N_i = \\sum_{t=1}^T \\mathbf{1}\\{X_t \\in B_i\\}$ and $N_{ij} = \\sum_{t=1}^{T-1} \\mathbf{1}\\{X_t \\in B_i, X_{t+1} \\in B_j\\}$. Form $\\hat{\\pi}(B_i) = N_i / \\sum_{k=1}^m N_k$ and $\\hat{P}(B_i,B_j) = N_{ij}/\\sum_{k=1}^m N_{ik}$. Test the null constraints $\\hat{\\pi}(B_i)\\hat{P}(B_i,B_j) = \\hat{\\pi}(B_j)\\hat{P}(B_j,B_i)$ for all $i \\neq j$ using a paired $z$-test or a chi-squared goodness-of-fit statistic with estimated variance from batch means over blocks of length $b \\in \\mathbb{N}$. Significant deviations signal lack of reversibility or bias. This requires only counting transitions and bin occupancies, plus estimating sampling variability.\n\nC. Time-reversal symmetry of lag-$1$ cross-moments: For bounded test functions $f:\\mathbb{R}^d \\to \\mathbb{R}$ and $g:\\mathbb{R}^d \\to \\mathbb{R}$, form the antisymmetric statistic $S_T(f,g) = \\frac{1}{T-1}\\sum_{t=1}^{T-1}\\left[f(X_t)g(X_{t+1}) - f(X_{t+1})g(X_t)\\right]$. Under stationarity and reversibility, $\\mathbb{E}[S_T(f,g)] = 0$. Compute $S_T(f,g)$ for a suite of functions (for example, coordinate projections and low-order polynomials), estimate its variance via batch means, and form studentized $t$-statistics for testing equality to $0$. Persistent nonzero values indicate lack of reversibility or bias. This requires only evaluations of $f$ and $g$ on the trajectory and an autocovariance-robust variance estimator.\n\nD. Kolmogorov cycle condition on short cycles using proposal-aware transition estimates: For randomly sampled triplets of distinct states $(x,y,z)$ drawn from the path, estimate the one-step transition mass $P(x,y)$ that your code would induce between these specific states by Monte Carlo conditioning on $x$: from $x$, simulate $M \\in \\mathbb{N}$ mock proposals following the program’s logic (including DA filters and DR second-stage proposals), tally the fraction that land exactly at $y$ when proposals are discrete, or within a small ball $B_\\epsilon(y)$ of radius $\\epsilon > 0$ when continuous, divided by the corresponding ball volume to yield a local density estimate. Do the same to get $\\hat{P}(y,z)$, $\\hat{P}(z,x)$, $\\hat{P}(x,z)$, $\\hat{P}(z,y)$, and $\\hat{P}(y,x)$. Form the cycle sum $C(x,y,z) = \\log \\hat{P}(x,y) + \\log \\hat{P}(y,z) + \\log \\hat{P}(z,x) - \\log \\hat{P}(x,z) - \\log \\hat{P}(z,y) - \\log \\hat{P}(y,x)$. Under reversibility, $\\mathbb{E}[C(x,y,z)] = 0$. Aggregate $C(x,y,z)$ across sampled triplets and test whether its mean differs from $0$ beyond Monte Carlo error. This requires proposal simulation from the implemented code, evaluation of acceptance decisions, and careful control of $\\epsilon$ and $M$.\n\nE. Stage-wise acceptance equality: In a correctly implemented DA or DR, the marginal first-stage acceptance rate must equal the marginal second-stage acceptance rate; therefore, test reversibility by checking whether the empirical stage-$1$ acceptance frequency equals the stage-$2$ acceptance frequency. If they differ materially, declare lack of reversibility.\n\nF. Universal acceptance level: Correctly implemented reversible high-dimensional random-walk schemes have optimal average acceptance rate near $0.234$. Therefore, as a reversibility diagnostic, check whether the observed long-run acceptance rate is approximately $0.234$; otherwise conclude the implementation is biased or nonreversible.\n\nSelect all that apply.", "solution": "A correct MCMC implementation designed to be reversible with respect to a target distribution $\\pi$ must satisfy the detailed balance condition, $\\pi(x) P(x,dy) = \\pi(y) P(y,dx)$. Violations of this condition indicate an implementation error. We evaluate each proposed diagnostic based on its ability to test this condition or its direct consequences.\n\n**A. Bidirectional composite-ratio identity check:** This test is based on the property that for any Metropolis-like algorithm satisfying detailed balance, the acceptance ratio $R(x,y)$ must satisfy $R(x,y)R(y,x)=1$. Checking if $\\log R(x,y) + \\log R(y,x) = 0$ is a direct and powerful \"white-box\" test of the core acceptance logic. A deviation from zero beyond numerical precision signals a bug. This is a valid diagnostic.\n\n**B. Empirical detailed-balance flow symmetry via binning:** This test checks a macroscopic consequence of detailed balance: for a stationary chain, the number of transitions from bin $B_i$ to bin $B_j$ should equal the number of transitions from $B_j$ to $B_i$ ($N_{ij} \\approx N_{ji}$). This is a standard and effective \"black-box\" diagnostic for reversibility. This is a valid diagnostic.\n\n**C. Time-reversal symmetry of lag-1 cross-moments:** Reversibility implies that the joint distribution of $(X_t, X_{t+1})$ is symmetric. This leads to the identity $\\mathbb{E}[f(X_t)g(X_{t+1}) - f(X_{t+1})g(X_t)] = 0$. The statistic $S_T(f,g)$ is the empirical average of this quantity. A statistically significant non-zero value for $S_T(f,g)$ is evidence against reversibility. This is a valid diagnostic.\n\n**D. Kolmogorov cycle condition on short cycles:** Reversibility is equivalent to Kolmogorov's criterion, which states that for any cycle of states, the product of transition probabilities in one direction equals the product in the reverse direction. For a 3-cycle, $p(x,y)p(y,z)p(z,x) = p(x,z)p(z,y)p(y,x)$. While computationally intensive to estimate the transition densities $p(\\cdot,\\cdot)$, checking this condition is a direct test of reversibility. This is a valid diagnostic.\n\n**E. Stage-wise acceptance equality:** This is based on a false premise. There is no theoretical reason why the acceptance rate of the first stage of a DR or DA sampler should equal that of the second stage. Their purposes and underlying proposal mechanisms are different. This is not a valid diagnostic.\n\n**F. Universal acceptance level:** This is also based on a false premise. The optimal acceptance rate of $\\approx 0.234$ is a result about the *efficiency* of a specific class of algorithms (high-dimensional random-walk Metropolis) under specific assumptions. It is not a condition for the *correctness* (reversibility) of a general MCMC sampler. A correct sampler can have any acceptance rate, and an incorrect one could happen to have this rate. This is not a valid diagnostic.\n\nTherefore, options A, B, C, and D are all valid diagnostics.", "answer": "$$\\boxed{ABCD}$$", "id": "3302329"}]}