{"hands_on_practices": [{"introduction": "To truly grasp the concept of a low-discrepancy sequence, there is no substitute for building one from first principles. This exercise guides you through the manual construction of the Halton sequence, a classic example rooted in number theory. By generating points using the radical inverse function, you will develop a concrete intuition for how properties of base representations translate directly into the uniform spatial distribution that makes Quasi-Monte Carlo methods so effective. [@problem_id:3313764]", "problem": "In the context of deterministic low-discrepancy sequences used in Quasi-Monte Carlo (QMC) methods, consider the two-dimensional Halton sequence constructed from the radical inverse functions. The radical inverse function in base $b$, denoted $\\phi_{b}(n)$, is defined by writing the positive integer $n$ in base $b$ as $n = \\sum_{k=0}^{m} a_{k} b^{k}$ with digits $a_{k} \\in \\{0,1,\\dots,b-1\\}$ and then reversing the digits to obtain $\\phi_{b}(n) = \\sum_{k=0}^{m} a_{k} b^{-(k+1)}$. The Halton sequence in dimension $2$ with coprime bases $b_{1}=2$ and $b_{2}=3$ is given by the points $\\big(\\phi_{2}(n), \\phi_{3}(n)\\big)$ for $n \\in \\mathbb{N}$.\n\nTasks:\n1. Using only the definition above, construct explicitly the first $10$ points of the two-dimensional Halton sequence with bases $b_{1}=2$ and $b_{2}=3$.\n2. Based on fundamental concepts of equidistribution and discrepancy, identify and justify at least two visible structural patterns of these $10$ points that are relevant to numerical integration over $[0,1]^{2}$, such as uniform coverage, avoidance of clustering, or regularity of one-dimensional projections. Your discussion must be grounded in the definitions and observable properties of the constructed points, not heuristic claims.\n3. Let $f:[0,1]^{2} \\to \\mathbb{R}$ be defined by $f(x,y) = x y$. Using the $10$ constructed Halton points, form the QMC estimator $\\widehat{I}_{10} = \\frac{1}{10} \\sum_{n=1}^{10} f\\big(\\phi_{2}(n), \\phi_{3}(n)\\big)$ for the integral $I = \\int_{0}^{1} \\int_{0}^{1} x y \\, dx \\, dy$. Provide the absolute error $|\\widehat{I}_{10} - I|$ as an exact rational number.\n\nThe final answer must be a single real-valued number given in exact rational form. No rounding is required.", "solution": "The solution is organized into three parts as requested by the problem statement.\n\n### Part 1: Construction of the Halton Sequence Points\n\nThe two-dimensional Halton sequence is formed by the points $\\big(\\phi_{b_{1}}(n), \\phi_{b_{2}}(n)\\big)$ for $n \\in \\mathbb{N}$, with coprime bases $b_{1}=2$ and $b_{2}=3$. The radical inverse function $\\phi_{b}(n)$ is found by writing $n$ in base $b$, $n = \\sum_{k=0}^{m} a_{k} b^{k}$, and reversing the digits around the radix point to form the fraction $\\phi_{b}(n) = \\sum_{k=0}^{m} a_{k} b^{-(k+1)}$. We compute the first $10$ points of this sequence for $n=1, 2, \\ldots, 10$.\n\nThe following table details the construction for each value of $n$:\n| $n$ | Base $2$ Rep. ($n$) | $\\phi_{2}(n)$ | Base $3$ Rep. ($n$) | $\\phi_{3}(n)$ |\n|:---:|:-------------------:|:-------------:|:-------------------:|:-------------:|\n| $1$ | $(1)_{2}$ | $\\frac{1}{2}$ | $(1)_{3}$ | $\\frac{1}{3}$ |\n| $2$ | $(10)_{2}$ | $\\frac{1}{4}$ | $(2)_{3}$ | $\\frac{2}{3}$ |\n| $3$ | $(11)_{2}$ | $\\frac{3}{4}$ | $(10)_{3}$ | $\\frac{1}{9}$ |\n| $4$ | $(100)_{2}$ | $\\frac{1}{8}$ | $(11)_{3}$ | $\\frac{4}{9}$ |\n| $5$ | $(101)_{2}$ | $\\frac{5}{8}$ | $(12)_{3}$ | $\\frac{7}{9}$ |\n| $6$ | $(110)_{2}$ | $\\frac{3}{8}$ | $(20)_{3}$ | $\\frac{2}{9}$ |\n| $7$ | $(111)_{2}$ | $\\frac{7}{8}$ | $(21)_{3}$ | $\\frac{5}{9}$ |\n| $8$ | $(1000)_{2}$ | $\\frac{1}{16}$ | $(22)_{3}$ | $\\frac{8}{9}$ |\n| $9$ | $(1001)_{2}$ | $\\frac{9}{16}$ | $(100)_{3}$ | $\\frac{1}{27}$ |\n| $10$| $(1010)_{2}$| $\\frac{5}{16}$ | $(101)_{3}$ | $\\frac{10}{27}$ |\n\nThe first $10$ points, denoted $P_{n} = \\big(\\phi_{2}(n), \\phi_{3}(n)\\big)$, are:\n$P_{1} = \\big(\\frac{1}{2}, \\frac{1}{3}\\big)$\n$P_{2} = \\big(\\frac{1}{4}, \\frac{2}{3}\\big)$\n$P_{3} = \\big(\\frac{3}{4}, \\frac{1}{9}\\big)$\n$P_{4} = \\big(\\frac{1}{8}, \\frac{4}{9}\\big)$\n$P_{5} = \\big(\\frac{5}{8}, \\frac{7}{9}\\big)$\n$P_{6} = \\big(\\frac{3}{8}, \\frac{2}{9}\\big)$\n$P_{7} = \\big(\\frac{7}{8}, \\frac{5}{9}\\big)$\n$P_{8} = \\big(\\frac{1}{16}, \\frac{8}{9}\\big)$\n$P_{9} = \\big(\\frac{9}{16}, \\frac{1}{27}\\big)$\n$P_{10} = \\big(\\frac{5}{16}, \\frac{10}{27}\\big)$\n\n### Part 2: Structural Patterns of the Halton Points\n\nBased on the $10$ points constructed above, we can identify and justify the following structural patterns, which are fundamental to the effectiveness of QMC methods.\n\n1.  **Regularity of One-Dimensional Projections:**\n    The set of the first $10$ values for the first coordinate, $\\{\\phi_{2}(n)\\}_{n=1}^{10}$, is $\\{\\frac{1}{2}, \\frac{1}{4}, \\frac{3}{4}, \\frac{1}{8}, \\frac{5}{8}, \\frac{3}{8}, \\frac{7}{8}, \\frac{1}{16}, \\frac{9}{16}, \\frac{5}{16}\\}$. A key property of the one-dimensional radical inverse sequence in base $b$ is that the set of the first $b^k-1$ points, $\\{\\phi_b(n)\\}_{n=1}^{b^k-1}$, is a permutation of the set of equidistributed points $\\{\\frac{1}{b^k}, \\frac{2}{b^k}, \\dots, \\frac{b^k-1}{b^k}\\}$. For instance, the first $2^3-1=7$ points of the sequence $\\{\\phi_2(n)\\}$ are $\\{\\frac{1}{2}, \\frac{1}{4}, \\frac{3}{4}, \\frac{1}{8}, \\frac{5}{8}, \\frac{3}{8}, \\frac{7}{8}\\}$. This set is precisely $\\{\\frac{1}{8}, \\frac{2}{8}, \\frac{3}{8}, \\frac{4}{8}, \\frac{5}{8}, \\frac{6}{8}, \\frac{7}{8}\\}$, just in a different order. This highly structured stratification of the interval $[0,1]$ ensures that the one-dimensional projections are very evenly distributed. A similar property holds for the second coordinate based on powers of $3$. For example, the first $3^2-1=8$ points of $\\{\\phi_3(n)\\}$ are a permutation of the set $\\{\\frac{1}{9}, \\frac{2}{9}, \\dots, \\frac{8}{9}\\}$. This regularity, visible even in a small sample of $10$ points, is crucial for accurate numerical integration, as it ensures all parts of the interval are sampled systematically.\n\n2.  **Uniform Coverage and Avoidance of Clustering in Two Dimensions:**\n    The Halton sequence is constructed to fill the unit square $[0,1]^2$ in a highly uniform manner, avoiding the clustering seen in pseudorandom sequences. This can be observed by partitioning the unit square and counting the points in each region. Let us divide the square $[0,1]^2$ into four equal quadrants at $(\\frac{1}{2}, \\frac{1}{2})$: Lower-Left (LL) $[0, \\frac{1}{2}) \\times [0, \\frac{1}{2})$, Lower-Right (LR) $[\\frac{1}{2}, 1] \\times [0, \\frac{1}{2})$, Upper-Left (UL) $[0, \\frac{1}{2}) \\times [\\frac{1}{2}, 1]$, and Upper-Right (UR) $[\\frac{1}{2}, 1] \\times [\\frac{1}{2}, 1]$.\n    The distribution of the first $10$ points is as follows (treating points on boundaries like $x=\\frac{1}{2}$ as being in the right-hand or upper region):\n    -   LL: $P_4(\\frac{1}{8}, \\frac{4}{9})$, $P_6(\\frac{3}{8}, \\frac{2}{9})$, $P_{10}(\\frac{5}{16}, \\frac{10}{27})$ --- Total: $3$ points.\n    -   LR: $P_1(\\frac{1}{2}, \\frac{1}{3})$, $P_3(\\frac{3}{4}, \\frac{1}{9})$, $P_9(\\frac{9}{16}, \\frac{1}{27})$ --- Total: $3$ points.\n    -   UL: $P_2(\\frac{1}{4}, \\frac{2}{3})$, $P_8(\\frac{1}{16}, \\frac{8}{9})$ --- Total: $2$ points.\n    -   UR: $P_5(\\frac{5}{8}, \\frac{7}{9})$, $P_7(\\frac{7}{8}, \\frac{5}{9})$ --- Total: $2$ points.\n    The number of points in each quadrant is $(3, 3, 2, 2)$, which is very close to the ideal uniform distribution of $2.5$ points per quadrant. This demonstrates the sequence's low discrepancy property: the points are spread out to cover the domain evenly, without large gaps or dense clusters. This is a direct result of using coprime bases, which ensures that the filling patterns in each dimension do not correlate in a way that would create regular but empty regions (a known problem if bases are not coprime).\n\n### Part 3: Calculation of the Absolute Error\n\nThe integral to be estimated is $I = \\int_{0}^{1} \\int_{0}^{1} f(x,y) \\, dx \\, dy$ with $f(x,y) = xy$.\nFirst, we compute the exact value of the integral:\n$$I = \\int_{0}^{1} \\int_{0}^{1} x y \\, dx \\, dy = \\left( \\int_{0}^{1} x \\, dx \\right) \\left( \\int_{0}^{1} y \\, dy \\right)$$\n$$I = \\left[ \\frac{x^2}{2} \\right]_{0}^{1} \\left[ \\frac{y^2}{2} \\right]_{0}^{1} = \\left(\\frac{1}{2} - 0\\right) \\left(\\frac{1}{2} - 0\\right) = \\frac{1}{4}$$\n\nNext, we compute the QMC estimator $\\widehat{I}_{10}$ using the $N=10$ constructed Halton points $P_n = (\\phi_2(n), \\phi_3(n))$:\n$$\\widehat{I}_{10} = \\frac{1}{10} \\sum_{n=1}^{10} f(P_n) = \\frac{1}{10} \\sum_{n=1}^{10} \\phi_{2}(n) \\phi_{3}(n)$$\nWe compute the products for each point:\n$f(P_1) = \\frac{1}{2} \\cdot \\frac{1}{3} = \\frac{1}{6}$\n$f(P_2) = \\frac{1}{4} \\cdot \\frac{2}{3} = \\frac{2}{12} = \\frac{1}{6}$\n$f(P_3) = \\frac{3}{4} \\cdot \\frac{1}{9} = \\frac{3}{36} = \\frac{1}{12}$\n$f(P_4) = \\frac{1}{8} \\cdot \\frac{4}{9} = \\frac{4}{72} = \\frac{1}{18}$\n$f(P_5) = \\frac{5}{8} \\cdot \\frac{7}{9} = \\frac{35}{72}$\n$f(P_6) = \\frac{3}{8} \\cdot \\frac{2}{9} = \\frac{6}{72} = \\frac{1}{12}$\n$f(P_7) = \\frac{7}{8} \\cdot \\frac{5}{9} = \\frac{35}{72}$\n$f(P_8) = \\frac{1}{16} \\cdot \\frac{8}{9} = \\frac{8}{144} = \\frac{1}{18}$\n$f(P_9) = \\frac{9}{16} \\cdot \\frac{1}{27} = \\frac{1}{16 \\cdot 3} = \\frac{1}{48}$\n$f(P_{10}) = \\frac{5}{16} \\cdot \\frac{10}{27} = \\frac{50}{432} = \\frac{25}{216}$\n\nNow we sum these values:\n$$\\sum_{n=1}^{10} f(P_n) = \\frac{1}{6} + \\frac{1}{6} + \\frac{1}{12} + \\frac{1}{18} + \\frac{35}{72} + \\frac{1}{12} + \\frac{35}{72} + \\frac{1}{18} + \\frac{1}{48} + \\frac{25}{216}$$\nTo sum these fractions, we find a common denominator. The least common multiple of $\\{6, 12, 18, 72, 48, 216\\}$ is $432$.\n$$ \\sum_{n=1}^{10} f(P_n) = \\frac{72}{432} + \\frac{72}{432} + \\frac{36}{432} + \\frac{24}{432} + \\frac{210}{432} + \\frac{36}{432} + \\frac{210}{432} + \\frac{24}{432} + \\frac{9}{432} + \\frac{50}{432} $$\n$$ \\sum_{n=1}^{10} f(P_n) = \\frac{72+72+36+24+210+36+210+24+9+50}{432} = \\frac{743}{432} $$\nThe QMC estimator is:\n$$\\widehat{I}_{10} = \\frac{1}{10} \\left(\\frac{743}{432}\\right) = \\frac{743}{4320}$$\n\nFinally, we compute the absolute error $|\\widehat{I}_{10} - I|$:\n$$ |\\widehat{I}_{10} - I| = \\left| \\frac{743}{4320} - \\frac{1}{4} \\right| $$\nTo subtract, we use the common denominator $4320$:\n$$ \\frac{1}{4} = \\frac{1 \\cdot 1080}{4 \\cdot 1080} = \\frac{1080}{4320} $$\n$$ |\\widehat{I}_{10} - I| = \\left| \\frac{743}{4320} - \\frac{1080}{4320} \\right| = \\left| \\frac{743 - 1080}{4320} \\right| = \\left| \\frac{-337}{4320} \\right| = \\frac{337}{4320} $$\nThe integer $337$ is prime, and the prime factorization of $4320$ is $2^5 \\cdot 3^3 \\cdot 5$. Thus, the fraction $\\frac{337}{4320}$ is irreducible.", "answer": "$$\\boxed{\\frac{337}{4320}}$$", "id": "3313764"}, {"introduction": "Having constructed a QMC point set, we now explore the theoretical justification for its superior performance over random sampling. This practice contrasts the probabilistic error framework of standard Monte Carlo (MC) methods with the deterministic error bounds of QMC. By analyzing the MC variance for a canonical integrand and then engaging with the Koksma-Hlawka inequality, you will connect the geometric uniformity of a point set (discrepancy) to the analytic properties of the integrand (variation) to understand QMC's faster convergence rate. [@problem_id:3313753]", "problem": "Let $d \\in \\mathbb{N}$ and consider the integrand $f(x)=\\prod_{j=1}^{d} x_{j}$ defined on the unit hypercube $[0,1]^{d}$. Let $\\{U_{i}\\}_{i=1}^{n}$ be independent and identically distributed random vectors with $U_{i} \\sim \\mathrm{Uniform}([0,1]^{d})$, and define the Monte Carlo estimator $\\hat{I}_{n}=\\frac{1}{n} \\sum_{i=1}^{n} f(U_{i})$ for the integral $I=\\int_{[0,1]^{d}} f(x) \\,\\mathrm{d}x$. Starting from core definitions of expectation and variance and the independence of $\\{U_{i}\\}_{i=1}^{n}$, derive a closed-form expression for $\\mathrm{Var}(\\hat{I}_{n})$ as a function of $n$ and $d$. Then, without appealing to variance, explain how Quasi-Monte Carlo (QMC) methods—based on deterministic low-discrepancy point sets—can reduce integration error for this integrand, by connecting the star discrepancy and the Hardy–Krause variation via foundational inequalities. Your explanation should start from definitions of discrepancy and variation and culminate in a rigorous statement of how the QMC error bound scales with $n$ for suitable low-discrepancy constructions. The final answer must be your closed-form expression for $\\mathrm{Var}(\\hat{I}_{n})$. No rounding is required.", "solution": "The problem consists of two parts. First, we must derive the variance of the Monte Carlo estimator $\\hat{I}_{n}$ for the given integrand. Second, we must explain how Quasi-Monte Carlo (QMC) methods can achieve a better error convergence rate for this specific integrand, using the concepts of star discrepancy and Hardy-Krause variation.\n\nPart 1: Derivation of $\\mathrm{Var}(\\hat{I}_{n})$\n\nThe Monte Carlo estimator for the integral $I=\\int_{[0,1]^{d}} f(x) \\,\\mathrm{d}x$ is given by $\\hat{I}_{n}=\\frac{1}{n} \\sum_{i=1}^{n} f(U_{i})$, where $\\{U_{i}\\}_{i=1}^{n}$ are independent and identically distributed (i.i.d.) random vectors with $U_{i} \\sim \\mathrm{Uniform}([0,1]^{d})$.\n\nThe variance of the estimator $\\hat{I}_{n}$ is given by:\n$$\n\\mathrm{Var}(\\hat{I}_{n}) = \\mathrm{Var}\\left(\\frac{1}{n} \\sum_{i=1}^{n} f(U_{i})\\right)\n$$\nSince the random vectors $\\{U_{i}\\}_{i=1}^{n}$ are i.i.d., the random variables $Y_{i} = f(U_{i})$ are also i.i.d. Using the properties of variance, we have:\n$$\n\\mathrm{Var}(\\hat{I}_{n}) = \\frac{1}{n^{2}} \\mathrm{Var}\\left(\\sum_{i=1}^{n} f(U_{i})\\right) = \\frac{1}{n^{2}} \\sum_{i=1}^{n} \\mathrm{Var}(f(U_{i}))\n$$\nSince the $f(U_{i})$ are identically distributed, $\\mathrm{Var}(f(U_{i}))$ is the same for all $i$. Let's denote this common variance as $\\mathrm{Var}(f(U))$.\n$$\n\\mathrm{Var}(\\hat{I}_{n}) = \\frac{1}{n^{2}} \\cdot n \\cdot \\mathrm{Var}(f(U)) = \\frac{1}{n} \\mathrm{Var}(f(U))\n$$\nThe variance of the random variable $f(U)$ is defined as $\\mathrm{Var}(f(U)) = \\mathbb{E}[f(U)^{2}] - (\\mathbb{E}[f(U)])^{2}$. We need to compute these two expectations.\n\nThe random vector $U$ is uniformly distributed on $[0,1]^{d}$, meaning its components $U_j$ for $j=1, \\dots, d$ are i.i.d. random variables with $U_j \\sim \\mathrm{Uniform}([0,1])$. The integrand is $f(x) = \\prod_{j=1}^{d} x_{j}$.\n\nFirst, we compute the expectation $\\mathbb{E}[f(U)]$:\n$$\n\\mathbb{E}[f(U)] = \\mathbb{E}\\left[\\prod_{j=1}^{d} U_{j}\\right]\n$$\nDue to the independence of the components $U_j$, the expectation of the product is the product of the expectations:\n$$\n\\mathbb{E}[f(U)] = \\prod_{j=1}^{d} \\mathbb{E}[U_{j}]\n$$\nFor a standard uniform random variable $X \\sim \\mathrm{Uniform}([0,1])$, the expectation is $\\mathbb{E}[X] = \\int_{0}^{1} x \\,\\mathrm{d}x = \\left[\\frac{x^{2}}{2}\\right]_{0}^{1} = \\frac{1}{2}$.\nTherefore,\n$$\n\\mathbb{E}[f(U)] = \\prod_{j=1}^{d} \\frac{1}{2} = \\left(\\frac{1}{2}\\right)^{d} = 2^{-d}\n$$\nThis value is also the true value of the integral $I$, confirming that the Monte Carlo estimator is unbiased, i.e., $\\mathbb{E}[\\hat{I}_{n}] = I$.\n\nNext, we compute the expectation of the square, $\\mathbb{E}[f(U)^{2}]$:\n$$\n\\mathbb{E}[f(U)^{2}] = \\mathbb{E}\\left[\\left(\\prod_{j=1}^{d} U_{j}\\right)^{2}\\right] = \\mathbb{E}\\left[\\prod_{j=1}^{d} U_{j}^{2}\\right]\n$$\nAgain, by independence of the $U_j$:\n$$\n\\mathbb{E}[f(U)^{2}] = \\prod_{j=1}^{d} \\mathbb{E}[U_{j}^{2}]\n$$\nFor $X \\sim \\mathrm{Uniform}([0,1])$, the second moment is $\\mathbb{E}[X^{2}] = \\int_{0}^{1} x^{2} \\,\\mathrm{d}x = \\left[\\frac{x^{3}}{3}\\right]_{0}^{1} = \\frac{1}{3}$.\nTherefore,\n$$\n\\mathbb{E}[f(U)^{2}] = \\prod_{j=1}^{d} \\frac{1}{3} = \\left(\\frac{1}{3}\\right)^{d} = 3^{-d}\n$$\nNow we can compute the variance of $f(U)$:\n$$\n\\mathrm{Var}(f(U)) = \\mathbb{E}[f(U)^{2}] - (\\mathbb{E}[f(U)])^{2} = 3^{-d} - (2^{-d})^{2} = 3^{-d} - 4^{-d}\n$$\nFinally, we substitute this back into the expression for the variance of the estimator:\n$$\n\\mathrm{Var}(\\hat{I}_{n}) = \\frac{1}{n} \\mathrm{Var}(f(U)) = \\frac{1}{n} (3^{-d} - 4^{-d})\n$$\n\nPart 2: QMC Error Reduction via Discrepancy and Variation\n\nThe Monte Carlo method uses random points, and its error is probabilistic, typically characterized by the standard deviation of the estimator, which is $\\mathcal{O}(n^{-1/2})$. Quasi-Monte Carlo methods use deterministic, highly uniform point sets, known as low-discrepancy sets, to achieve faster convergence. The error in QMC is deterministic and is bounded using the concepts of function variation and point set discrepancy.\n\nThe error of a QMC approximation using a point set $P_{n} = \\{p_{1}, \\dots, p_{n}\\} \\subset [0,1]^{d}$ is given by $|I - Q_{n}(f)|$, where $Q_{n}(f) = \\frac{1}{n}\\sum_{i=1}^{n}f(p_{i})$. This error is bounded by the Koksma-Hlawka inequality:\n$$\n\\left|\\int_{[0,1]^{d}} f(x) \\,\\mathrm{d}x - \\frac{1}{n} \\sum_{i=1}^{n} f(p_{i})\\right| \\leq V_{HK}(f) \\cdot D_{n}^{*}(P_{n})\n$$\nwhere $V_{HK}(f)$ is the Hardy-Krause variation of the function $f$, and $D_{n}^{*}(P_{n})$ is the star discrepancy of the point set $P_{n}$.\n\nThe star discrepancy $D_{n}^{*}(P_{n})$ measures the uniformity of the point set. It is defined as the maximum deviation between the fraction of points falling into any axis-aligned box anchored at the origin and the volume of that box:\n$$\nD_{n}^{*}(P_{n}) = \\sup_{y \\in [0,1]^{d}} \\left| \\frac{1}{n} \\sum_{i=1}^{n} \\mathbb{1}_{[0,y)}(p_{i}) - \\prod_{j=1}^{d} y_j \\right|\n$$\nwhere $[0,y) = [0, y_1) \\times \\dots \\times [0, y_d)$. Low-discrepancy point sets are constructed to minimize $D_{n}^{*}$.\n\nThe Hardy-Krause variation $V_{HK}(f)$ measures the \"wiggliness\" of the function $f$. For a function with continuous mixed partial derivatives, it is defined as:\n$$\nV_{HK}(f) = \\sum_{\\emptyset \\neq u \\subseteq \\{1, \\dots, d\\}} \\int_{[0,1]^{|u|}} \\left| \\frac{\\partial^{|u|}f}{\\prod_{j \\in u} \\partial x_j} (x_u; 1) \\right| \\,\\mathrm{d}x_u\n$$\nwhere $(x_u; 1)$ denotes that variables $x_j$ with $j \\notin u$ are set to $1$.\n\nFor our integrand, $f(x) = \\prod_{j=1}^{d} x_j$, we first compute the mixed partial derivatives. For any non-empty subset of indices $u \\subseteq \\{1, \\dots, d\\}$, the differentiation gives:\n$$\n\\frac{\\partial^{|u|}f}{\\prod_{j \\in u} \\partial x_j} = \\prod_{k \\notin u} x_k\n$$\nEvaluating this at $(x_u; 1)$, we set $x_k = 1$ for all $k \\notin u$, which results in:\n$$\n\\frac{\\partial^{|u|}f}{\\prod_{j \\in u} \\partial x_j} (x_u; 1) = 1\n$$\nThis result is independent of $x_u$ and positive. The corresponding integral term in the variation formula is:\n$$\n\\int_{[0,1]^{|u|}} |1| \\,\\mathrm{d}x_u = \\int_{[0,1]^{|u|}} 1 \\,\\mathrm{d}x_u = 1\n$$\nThe total Hardy-Krause variation is the sum of these terms over all non-empty subsets $u$:\n$$\nV_{HK}(f) = \\sum_{\\emptyset \\neq u \\subseteq \\{1, \\dots, d\\}} 1\n$$\nThe number of non-empty subsets of a set with $d$ elements is $2^{d} - 1$. Thus, the variation is finite:\n$$\nV_{HK}(f) = 2^{d} - 1\n$$\nSince $f$ has finite Hardy-Krause variation, the Koksma-Hlawka inequality applies. The QMC error is bounded by:\n$$\n|I - Q_{n}(f)| \\leq (2^{d} - 1) D_{n}^{*}(P_{n})\n$$\nFor well-known low-discrepancy constructions (e.g., Halton or Sobol' sequences), the star discrepancy has an asymptotic bound of the form:\n$$\nD_{n}^{*}(P_{n}) = \\mathcal{O}\\left(\\frac{(\\ln n)^{d-1}}{n}\\right) \\quad \\text{or} \\quad D_{n}^{*}(P_{n}) = \\mathcal{O}\\left(\\frac{(\\ln n)^{d}}{n}\\right)\n$$\n(The exact power of the logarithm depends on the sequence and the theoretical bound). In either case, for a fixed dimension $d$, the QMC error bound is approximately $\\mathcal{O}(n^{-1})$, ignoring logarithmic factors. This demonstrates a deterministically guaranteed rate of convergence that is asymptotically superior to the probabilistic convergence rate $\\mathcal{O}(n^{-1/2})$ of the standard Monte Carlo method. The integrand $f(x)=\\prod x_j$ is an ideal candidate for QMC because its low-order mixed derivatives are constant, keeping its Hardy-Krause variation finite and relatively small for moderate $d$.", "answer": "$$\n\\boxed{\\frac{1}{n}(3^{-d} - 4^{-d})}\n$$", "id": "3313753"}, {"introduction": "A key challenge with deterministic QMC is the difficulty of estimating the integration error for a general function, as its Hardy-Krause variation is often unknown. This final practice introduces Randomized Quasi-Monte Carlo (RQMC), a powerful hybrid approach that resolves this issue by blending QMC's structure with statistical sampling. This exercise will lead you through the derivation of the statistical machinery needed to obtain unbiased variance estimates, transforming QMC from a theoretical construct into a robust and practical tool for computational science. [@problem_id:3313803]", "problem": "Consider integration of an integrable function $f:[0,1]^s \\to \\mathbb{R}$ with respect to the uniform distribution on $[0,1]^s$. Let $\\mu = \\int_{[0,1]^s} f(\\boldsymbol{u}) \\,\\mathrm{d}\\boldsymbol{u}$. In a randomized quasi-Monte Carlo (RQMC) scheme based on Owen-scrambled digital nets, suppose that for a fixed integer $N \\ge 1$ you generate $R \\ge 2$ independent replicates of a fully Owen-scrambled $(t,m,s)$-net, each replicate providing $N$ dependent points that are marginally uniform on $[0,1]^s$ and independent across replicates. For each replicate $r \\in \\{1,\\dots,R\\}$, define the replicate estimator\n$$\n\\hat{\\mu}_r \\;=\\; \\frac{1}{N}\\sum_{n=1}^{N} f(\\boldsymbol{U}_{r,n}),\n$$\nwhere within each replicate $r$, the points $\\{\\boldsymbol{U}_{r,n}\\}_{n=1}^N$ form a scrambled digital net and across replicates the point sets are independent. Let the grand RQMC estimator be\n$$\n\\bar{\\mu}_R \\;=\\; \\frac{1}{R}\\sum_{r=1}^{R} \\hat{\\mu}_r.\n$$\nAssume that $f$ is square-integrable so that $\\mathbb{E}[(\\hat{\\mu}_r - \\mu)^2] < \\infty$, and that the fourth moment $\\mathbb{E}[(\\hat{\\mu}_r - \\mu)^4]$ is finite for the given $N$ under the scrambling. Starting only from the definitions of unbiasedness, variance, independence across replicates, and the classical properties of the sample mean and sample variance for independent and identically distributed random variables, do the following:\n\n1. Prove that $\\bar{\\mu}_R$ is an unbiased estimator of $\\mu$ under the given RQMC construction and independence of the $R$ scrambled replicates.\n\n2. Derive, from first principles and using only the $R$ replicate values $\\{\\hat{\\mu}_r\\}_{r=1}^R$, an estimator for the variance of $\\bar{\\mu}_R$ that is unbiased under the stated assumptions. Your derivation must not rely on any pre-quoted formula for sample variance; it must start from the definitions and independence across replicates.\n\n3. Establish the consistency properties of your variance estimator as $R \\to \\infty$ with $N$ fixed, under the finite fourth moment assumption. Discuss how the behavior changes as $N \\to \\infty$ with $R$ fixed, given known variance contraction of scrambled nets for square-integrable $f$.\n\nYour final answer must be the single closed-form expression of the variance estimator you derived in item 2, simplified as much as possible. No numerical approximation is required and no units are involved.", "solution": "This problem asks for three distinct but related results concerning an RQMC integration scheme: proving the unbiasedness of the grand mean estimator, deriving an unbiased estimator for its variance, and discussing the asymptotic properties of this variance estimator. We will address each part in turn, adhering strictly to first principles as requested.\n\nLet $\\mu = \\int_{[0,1]^s} f(\\boldsymbol{u}) \\,\\mathrm{d}\\boldsymbol{u}$ be the integral of interest. The replicate estimators are $\\hat{\\mu}_r = \\frac{1}{N}\\sum_{n=1}^{N} f(\\boldsymbol{U}_{r,n})$ for $r \\in \\{1, \\dots, R\\}$, and the grand estimator is $\\bar{\\mu}_R = \\frac{1}{R}\\sum_{r=1}^{R} \\hat{\\mu}_r$. The point sets $\\{\\boldsymbol{U}_{r,n}\\}_{n=1}^N$ are independent across replicates $r$, and for a fixed $r$, the points constitute a scrambled digital net. A key property of such a net is that each point $\\boldsymbol{U}_{r,n}$ is marginally distributed uniformly over the hypercube $[0,1]^s$.\n\n1. Unbiasedness of the Grand Estimator $\\bar{\\mu}_R$\n\nTo prove that $\\bar{\\mu}_R$ is an unbiased estimator of $\\mu$, we must show that its expected value is equal to $\\mu$. The expectation is taken over the randomization of the $R$ independent scrambled nets.\n\nFirst, we determine the expected value of a single replicate estimator, $\\hat{\\mu}_r$. By the linearity of the expectation operator, we have:\n$$\n\\mathbb{E}[\\hat{\\mu}_r] = \\mathbb{E}\\left[\\frac{1}{N}\\sum_{n=1}^{N} f(\\boldsymbol{U}_{r,n})\\right] = \\frac{1}{N}\\sum_{n=1}^{N} \\mathbb{E}[f(\\boldsymbol{U}_{r,n})]\n$$\nThe problem states that each point $\\boldsymbol{U}_{r,n}$ is marginally uniform on $[0,1]^s$. This means the probability density function of any individual point $\\boldsymbol{U}_{r,n}$ is the uniform density on $[0,1]^s$. Therefore, the expected value of $f$ applied to such a point is, by definition, the integral of $f$ over the domain:\n$$\n\\mathbb{E}[f(\\boldsymbol{U}_{r,n})] = \\int_{[0,1]^s} f(\\boldsymbol{u}) \\cdot 1 \\,\\mathrm{d}\\boldsymbol{u} = \\mu\n$$\nThis holds for every $n \\in \\{1, \\dots, N\\}$ and for every replicate $r \\in \\{1, \\dots, R\\}$. Substituting this result back into the expression for $\\mathbb{E}[\\hat{\\mu}_r]$ yields:\n$$\n\\mathbb{E}[\\hat{\\mu}_r] = \\frac{1}{N}\\sum_{n=1}^{N} \\mu = \\frac{1}{N}(N\\mu) = \\mu\n$$\nThis demonstrates that each individual replicate estimator $\\hat{\\mu}_r$ is an unbiased estimator of $\\mu$.\n\nNow, we can find the expected value of the grand estimator $\\bar{\\mu}_R$. Again using the linearity of expectation:\n$$\n\\mathbb{E}[\\bar{\\mu}_R] = \\mathbb{E}\\left[\\frac{1}{R}\\sum_{r=1}^{R} \\hat{\\mu}_r\\right] = \\frac{1}{R}\\sum_{r=1}^{R} \\mathbb{E}[\\hat{\\mu}_r]\n$$\nSince $\\mathbb{E}[\\hat{\\mu}_r] = \\mu$ for all $r$, we have:\n$$\n\\mathbb{E}[\\bar{\\mu}_R] = \\frac{1}{R}\\sum_{r=1}^{R} \\mu = \\frac{1}{R}(R\\mu) = \\mu\n$$\nThis completes the proof. The grand RQMC estimator $\\bar{\\mu}_R$ is an unbiased estimator of $\\mu$. This property relies on the linearity of expectation and the marginal uniformity of the points, but not on independence between points within a replicate.\n\n2. Derivation of an Unbiased Variance Estimator for $\\bar{\\mu}_R$\n\nWe are tasked to derive an unbiased estimator for $\\mathbb{V}(\\bar{\\mu}_R)$ using only the replicate values $\\{\\hat{\\mu}_r\\}_{r=1}^R$.\n\nFirst, let us find the true variance of $\\bar{\\mu}_R$. The replicate estimators $\\{\\hat{\\mu}_r\\}_{r=1}^R$ are constructed from independent point sets. Therefore, the random variables $\\hat{\\mu}_1, \\hat{\\mu}_2, \\dots, \\hat{\\mu}_R$ are independent. Furthermore, since the scrambling procedure is statistically identical for each replicate, they are also identically distributed (i.i.d.). Let the variance of each replicate estimator be denoted by $\\sigma_N^2$:\n$$\n\\sigma_N^2 = \\mathbb{V}(\\hat{\\mu}_r) = \\mathbb{E}[(\\hat{\\mu}_r - \\mu)^2]\n$$\nThe problem assumption of square-integrability of $f$ ensures that this variance is finite.\n\nThe variance of the grand estimator $\\bar{\\mu}_R$ is:\n$$\n\\mathbb{V}(\\bar{\\mu}_R) = \\mathbb{V}\\left(\\frac{1}{R}\\sum_{r=1}^{R} \\hat{\\mu}_r\\right) = \\frac{1}{R^2} \\mathbb{V}\\left(\\sum_{r=1}^{R} \\hat{\\mu}_r\\right)\n$$\nDue to the independence of the replicates $\\{\\hat{\\mu}_r\\}$, the variance of the sum is the sum of the variances:\n$$\n\\mathbb{V}(\\bar{\\mu}_R) = \\frac{1}{R^2} \\sum_{r=1}^{R} \\mathbb{V}(\\hat{\\mu}_r) = \\frac{1}{R^2} \\sum_{r=1}^{R} \\sigma_N^2 = \\frac{1}{R^2}(R\\sigma_N^2) = \\frac{\\sigma_N^2}{R}\n$$\nOur goal is to find an estimator $\\hat{\\mathbb{V}}(\\bar{\\mu}_R)$ such that $\\mathbb{E}[\\hat{\\mathbb{V}}(\\bar{\\mu}_R)] = \\sigma_N^2/R$. The standard approach for i.i.d. samples is to use the sample variance. Let us derive this from first principles as required.\n\nThe task is equivalent to finding an unbiased estimator of $\\sigma_N^2/R$ based on the $R$ i.i.d. observations $\\hat{\\mu}_1, \\dots, \\hat{\\mu}_R$. We begin by constructing the sample variance of these observations:\n$$\nS_R^2 = \\frac{1}{R-1} \\sum_{r=1}^{R} (\\hat{\\mu}_r - \\bar{\\mu}_R)^2\n$$\nLet us prove that $S_R^2$ is an unbiased estimator of $\\sigma_N^2$. We compute its expectation:\n$$\n\\mathbb{E}[S_R^2] = \\frac{1}{R-1} \\mathbb{E}\\left[\\sum_{r=1}^{R} (\\hat{\\mu}_r - \\bar{\\mu}_R)^2\\right]\n$$\nWe expand the term inside the expectation:\n$$\n\\sum_{r=1}^{R} (\\hat{\\mu}_r - \\bar{\\mu}_R)^2 = \\sum_{r=1}^{R} \\hat{\\mu}_r^2 - 2\\bar{\\mu}_R \\sum_{r=1}^{R} \\hat{\\mu}_r + \\sum_{r=1}^{R} \\bar{\\mu}_R^2 = \\sum_{r=1}^{R} \\hat{\\mu}_r^2 - 2\\bar{\\mu}_R(R\\bar{\\mu}_R) + R\\bar{\\mu}_R^2 = \\sum_{r=1}^{R} \\hat{\\mu}_r^2 - R\\bar{\\mu}_R^2\n$$\nTaking the expectation gives:\n$$\n\\mathbb{E}\\left[\\sum_{r=1}^{R} (\\hat{\\mu}_r - \\bar{\\mu}_R)^2\\right] = \\sum_{r=1}^{R} \\mathbb{E}[\\hat{\\mu}_r^2] - R\\mathbb{E}[\\bar{\\mu}_R^2]\n$$\nWe use the relation $\\mathbb{E}[X^2] = \\mathbb{V}(X) + (\\mathbb{E}[X])^2$. For each $\\hat{\\mu}_r$, we have $\\mathbb{E}[\\hat{\\mu}_r] = \\mu$ and $\\mathbb{V}(\\hat{\\mu}_r) = \\sigma_N^2$. Thus, $\\mathbb{E}[\\hat{\\mu}_r^2] = \\sigma_N^2 + \\mu^2$.\nFor $\\bar{\\mu}_R$, we have $\\mathbb{E}[\\bar{\\mu}_R] = \\mu$ and $\\mathbb{V}(\\bar{\\mu}_R) = \\sigma_N^2/R$. Thus, $\\mathbb{E}[\\bar{\\mu}_R^2] = \\sigma_N^2/R + \\mu^2$.\nSubstituting these into the equation for the expectation:\n$$\n\\mathbb{E}\\left[\\sum_{r=1}^{R} (\\hat{\\mu}_r - \\bar{\\mu}_R)^2\\right] = \\sum_{r=1}^{R} (\\sigma_N^2 + \\mu^2) - R\\left(\\frac{\\sigma_N^2}{R} + \\mu^2\\right)\n$$\n$$\n= R(\\sigma_N^2 + \\mu^2) - (\\sigma_N^2 + R\\mu^2) = R\\sigma_N^2 + R\\mu^2 - \\sigma_N^2 - R\\mu^2 = (R-1)\\sigma_N^2\n$$\nTherefore, the expectation of the sample variance is:\n$$\n\\mathbb{E}[S_R^2] = \\frac{1}{R-1} (R-1)\\sigma_N^2 = \\sigma_N^2\n$$\nThis shows that $S_R^2$ is an unbiased estimator of the variance of a single replicate, $\\sigma_N^2$.\nOur target is to estimate $\\mathbb{V}(\\bar{\\mu}_R) = \\sigma_N^2/R$. We can construct the estimator as $\\hat{\\mathbb{V}}(\\bar{\\mu}_R) = S_R^2/R$. Let's verify its unbiasedness:\n$$\n\\mathbb{E}[\\hat{\\mathbb{V}}(\\bar{\\mu}_R)] = \\mathbb{E}\\left[\\frac{S_R^2}{R}\\right] = \\frac{1}{R}\\mathbb{E}[S_R^2] = \\frac{\\sigma_N^2}{R}\n$$\nThis matches the true variance $\\mathbb{V}(\\bar{\\mu}_R)$. Thus, the desired unbiased estimator is:\n$$\n\\hat{\\mathbb{V}}(\\bar{\\mu}_R) = \\frac{S_R^2}{R} = \\frac{1}{R(R-1)} \\sum_{r=1}^{R} (\\hat{\\mu}_r - \\bar{\\mu}_R)^2\n$$\nThis expression depends only on the replicate values, as required.\n\n3. Consistency Properties of the Variance Estimator\n\nWe examine the behavior of the derived estimator in two asymptotic regimes.\n\nCase 1: $R \\to \\infty$ with $N$ fixed.\nWe are investigating the consistency of our estimator for $\\mathbb{V}(\\bar{\\mu}_R) = \\sigma_N^2/R$. The estimator is $\\hat{\\mathbb{V}}(\\bar{\\mu}_R) = S_R^2/R$. The variables $\\{\\hat{\\mu}_r\\}_{r=1}^R$ are i.i.d. with finite mean $\\mu$ and finite variance $\\sigma_N^2$. The problem states that the fourth moment $\\mathbb{E}[(\\hat{\\mu}_r - \\mu)^4]$ is also finite.\nBy the Law of Large Numbers, the sample variance $S_R^2$ is a consistent estimator of the population variance $\\sigma_N^2$. That is, as $R \\to \\infty$:\n$$\nS_R^2 = \\frac{1}{R-1} \\sum_{r=1}^{R} (\\hat{\\mu}_r - \\bar{\\mu}_R)^2 \\xrightarrow{p} \\sigma_N^2\n$$\nwhere $\\xrightarrow{p}$ denotes convergence in probability. The finite fourth moment assumption ensures this convergence.\nThe quantity being estimated is $\\mathbb{V}(\\bar{\\mu}_R) = \\sigma_N^2/R$. Both the estimator $\\hat{\\mathbb{V}}(\\bar{\\mu}_R) = S_R^2/R$ and the true variance $\\sigma_N^2/R$ converge to $0$ as $R \\to \\infty$. The quality of the estimation is assessed by the convergence of their difference to $0$. Let's consider the scaled difference:\n$$\nR \\left( \\hat{\\mathbb{V}}(\\bar{\\mu}_R) - \\mathbb{V}(\\bar{\\mu}_R) \\right) = R \\left( \\frac{S_R^2}{R} - \\frac{\\sigma_N^2}{R} \\right) = S_R^2 - \\sigma_N^2\n$$\nSince $S_R^2 - \\sigma_N^2 \\xrightarrow{p} 0$, our variance estimator is consistent in the sense that it tracks the true variance to zero. Specifically, the relative error converges to zero:\n$$\n\\frac{\\hat{\\mathbb{V}}(\\bar{\\mu}_R) - \\mathbb{V}(\\bar{\\mu}_R)}{\\mathbb{V}(\\bar{\\mu}_R)} = \\frac{S_R^2/R - \\sigma_N^2/R}{\\sigma_N^2/R} = \\frac{S_R^2 - \\sigma_N^2}{\\sigma_N^2} \\xrightarrow{p} 0\n$$\nSo, the sample variance of the replicates provides a consistent estimate of the per-replicate variance $\\sigma_N^2$, which in turn gives a consistent estimate of the variance of the grand mean.\n\nCase 2: $N \\to \\infty$ with $R$ fixed.\nHere, we rely on the known properties of scrambled digital nets. For any square-integrable function $f$, the variance of the RQMC estimator converges to zero faster than the standard Monte Carlo rate of $O(N^{-1})$. That is:\n$$\n\\sigma_N^2 = \\mathbb{V}(\\hat{\\mu}_r) = o(N^{-1}) \\quad \\text{as } N \\to \\infty\n$$\n(For smoother functions, the rate is even faster, e.g., $O(N^{-2+\\epsilon})$ or better).\nAs $N \\to \\infty$, $\\sigma_N^2 \\to 0$. The true variance of the grand mean, $\\mathbb{V}(\\bar{\\mu}_R) = \\sigma_N^2/R$, also converges to $0$.\nLet's analyze the behavior of our estimator $\\hat{\\mathbb{V}}(\\bar{\\mu}_R) = S_R^2/R$. The expected value of the estimator is $\\mathbb{E}[S_R^2/R] = \\sigma_N^2/R$. As $N \\to \\infty$, this expected value tends to $0$.\nBecause $S_R^2$ is a non-negative random variable, its expectation converging to zero implies that $S_R^2$ itself must converge to $0$ in probability.\n$$\n\\mathbb{E}[S_R^2] = \\sigma_N^2 \\to 0 \\implies S_R^2 \\xrightarrow{p} 0 \\quad \\text{as } N \\to \\infty\n$$\nConsequently, our estimator $\\hat{\\mathbb{V}}(\\bar{\\mu}_R) = S_R^2/R$ also converges to $0$ in probability. This is the correct and desired behavior. As $N$ increases, each replicate $\\hat{\\mu}_r$ becomes a highly accurate estimate of $\\mu$. The variation between the replicates diminishes, and the ensemble of $R$ replicates becomes tightly clustered around $\\mu$. Our variance estimator correctly captures this loss of variation by producing an estimate that tends to $0$, reflecting the increasing certainty in the grand mean $\\bar{\\mu}_R$.", "answer": "$$\\boxed{\\frac{1}{R(R-1)} \\sum_{r=1}^{R} (\\hat{\\mu}_r - \\bar{\\mu}_R)^2}$$", "id": "3313803"}]}