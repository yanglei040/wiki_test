{"hands_on_practices": [{"introduction": "The foundation of using orthogonal arrays in practice begins with understanding their construction. This exercise guides you through building a mixed-level orthogonal array from first principles, a crucial skill for creating bespoke experimental designs. By applying the product method to combine two smaller, well-known arrays, you will gain a tangible understanding of how the defining properties of strength and balance are achieved and verified [@problem_id:3325882].", "problem": "In the context of variance reduction for Monte Carlo integration via orthogonal array sampling, consider constructing a mixed-level orthogonal array that can be used to generate quasi-random designs with exact two-way stratification. Let there be $k_1$ factors with $s_1=2$ levels and $k_2$ factors with $s_2=3$ levels, and require strength $t=2$. Take $k_1=3$, $k_2=4$, and restrict attention to the smallest admissible run size $N$ that allows strength $t=2$ for all two-column projections across the mixed levels.\n\nTasks:\n1. Starting from the definition of a strength-$t$ orthogonal array and basic counting, determine the smallest $N$ consistent with pairwise exact balance across all unordered pairs of columns, including pairs of two-level columns, pairs of three-level columns, and mixed pairs.\n2. Construct, in a mathematically explicit way, an orthogonal array with these parameters $N$, $k_1$, $k_2$, $t$, and verify from first principles (by counting) that all two-way projections are exactly balanced.\n3. Define the following global two-way imbalance measure\n$$\nD \\;=\\; \\sum_{\\{i,j\\}} \\;\\sum_{a=0}^{s_i-1}\\;\\sum_{b=0}^{s_j-1} \\left(n_{ab}^{(i,j)} \\;-\\; \\frac{N}{s_i s_j}\\right)^{2},\n$$\nwhere the outer sum is over all unordered pairs $\\{i,j\\}$ of distinct columns, $s_i \\in \\{2,3\\}$ is the number of levels of column $i$, and $n_{ab}^{(i,j)}$ is the observed count of runs in which column $i$ is at level $a$ and column $j$ is at level $b$. Compute the exact value of $D$ for your constructed design.\n\nYour final answer must be the single real-valued number equal to the value of $D$. If a numerical approximation were required, rounding instructions would be provided; here, give the exact value with no units.", "solution": "The problem asks for three tasks to be completed in sequence.\n\n#### Task 1: Determination of the Smallest Run Size $N$\n\nAn orthogonal array of strength $t=2$ requires that for any pair of columns, all possible combinations of levels appear with the same frequency. This imposes constraints on the total number of runs, $N$. We consider the three types of column pairs.\n\n1.  **Pair of two-level columns:** For any pair of the $k_1=3$ columns with $s_1=2$ levels, there are $2 \\times 2 = 4$ level combinations. For balance, $N$ must be a multiple of 4.\n\n2.  **Pair of three-level columns:** For any pair of the $k_2=4$ columns with $s_2=3$ levels, there are $3 \\times 3 = 9$ level combinations. For balance, $N$ must be a multiple of 9.\n\n3.  **Mixed pair of columns:** For a pair with one 2-level and one 3-level column, there are $2 \\times 3 = 6$ level combinations. For balance, $N$ must be a multiple of 6.\n\nFor an orthogonal array with these properties to exist, $N$ must be a multiple of 4, 9, and 6. The smallest such positive integer is the least common multiple of these numbers.\n$$\nN = \\text{lcm}(4, 6, 9) = \\text{lcm}(2^2, 2 \\cdot 3, 3^2) = 2^2 \\cdot 3^2 = 36\n$$\nThe smallest admissible run size is $N=36$.\n\n#### Task 2: Construction and Verification of the Orthogonal Array\n\nWe can construct the required mixed-level orthogonal array, an $OA(36, 2^3 3^4, 2)$, using the product method on two smaller, well-known orthogonal arrays.\n\n1.  **Component Array A:** An orthogonal array $OA(4, 2^3, 2)$. This is a strength-2 array with $N_1=4$ runs and $k_1=3$ factors, each with $s_1=2$ levels. A standard representation is:\n$$\nA = \\begin{pmatrix} 0 & 0 & 0 \\\\ 0 & 1 & 1 \\\\ 1 & 0 & 1 \\\\ 1 & 1 & 0 \\end{pmatrix}\n$$\n\n2.  **Component Array B:** An orthogonal array $OA(9, 3^4, 2)$. This is a strength-2 array with $N_2=9$ runs and $k_2=4$ factors, each with $s_2=3$ levels. This can be constructed from the affine plane of order $3$.\n$$\nB = \\begin{pmatrix}\n0 & 0 & 0 & 0 \\\\\n0 & 1 & 1 & 2 \\\\\n0 & 2 & 2 & 1 \\\\\n1 & 0 & 1 & 1 \\\\\n1 & 1 & 2 & 0 \\\\\n1 & 2 & 0 & 2 \\\\\n2 & 0 & 2 & 2 \\\\\n2 & 1 & 0 & 1 \\\\\n2 & 2 & 1 & 0\n\\end{pmatrix}\n$$\n\n**Construction:** The final array $C$ of size $36 \\times 7$ is constructed by taking the product of $A$ and $B$. Each of the $N_1=4$ rows of $A$ is combined with each of the $N_2=9$ rows of $B$ to form the $N_1 \\times N_2 = 36$ rows of $C$.\n\n**Verification:**\n- **Two 2-level columns:** In $A$, any pair of columns contains each of the 4 level combinations exactly once. In $C$, each row of $A$ is repeated 9 times. So, any pair of 2-level columns in $C$ contains each of the 4 combinations $1 \\times 9 = 9$ times, which matches $N/(s_1 s_1) = 36/4 = 9$.\n- **Two 3-level columns:** In $B$, any pair of columns contains each of the 9 level combinations exactly once. In $C$, the array $B$ is repeated 4 times. So, any pair of 3-level columns in $C$ contains each of the 9 combinations $1 \\times 4 = 4$ times, which matches $N/(s_2 s_2) = 36/9 = 4$.\n- **One 2-level and one 3-level column:** The number of times a level $u \\in \\{0, 1\\}$ appears in a column of $A$ is $N_1/s_1 = 4/2 = 2$. The number of times a level $v \\in \\{0, 1, 2\\}$ appears in a column of $B$ is $N_2/s_2 = 9/3 = 3$. In the product construction, the number of occurrences of the pair $(u, v)$ is the product of these counts: $2 \\times 3 = 6$. This matches $N/(s_1 s_2) = 36/6 = 6$.\n\nThe construction is successful and yields a design with exact two-way stratification.\n\n#### Task 3: Computation of the Imbalance Measure $D$\n\nThe global two-way imbalance measure is defined as:\n$$\nD \\;=\\; \\sum_{\\{i,j\\}} \\;\\sum_{a=0}^{s_i-1}\\;\\sum_{b=0}^{s_j-1} \\left(n_{ab}^{(i,j)} \\;-\\; \\frac{N}{s_i s_j}\\right)^{2}\n$$\nAs verified in Task 2, our constructed array is an orthogonal array of strength $t=2$. By definition, this means that for every pair of columns $\\{i,j\\}$, the observed counts $n_{ab}^{(i,j)}$ are exactly equal to the expected counts for perfect balance, $N/(s_i s_j)$.\nTherefore, for every term in the summation for $D$, the quantity inside the parentheses is exactly zero:\n$$\nn_{ab}^{(i,j)} - \\frac{N}{s_i s_j} = 0\n$$\nConsequently, the entire sum is a sum of zeros.\n$$\nD = 0\n$$\nThe exact value of $D$ for the constructed design is $0$.", "answer": "$$\\boxed{0}$$", "id": "3325882"}, {"introduction": "After mastering the construction of an orthogonal array, the natural next question is to ask what statistical advantages it confers. This practice delves into the theoretical heart of orthogonal array sampling, connecting the combinatorial property of 'strength' to a profound reduction in estimator variance. By working through the analysis of variance (ANOVA) decomposition for a specific class of functions, you will derive the significantly improved convergence rate that makes OA-based sampling a premier technique in Monte Carlo integration [@problem_id:3325870].", "problem": "Consider a function $f:[0,1]^d \\to \\mathbb{R}$ with the Hoeffding–Sobol analysis of variance (ANOVA) decomposition $f(\\boldsymbol{x})=\\sum_{u \\subseteq \\{1,\\dots,d\\}} f_u(\\boldsymbol{x}_u)$, where, for $u \\neq \\varnothing$, each component $f_u$ has zero mean with respect to the uniform measure on $[0,1]^{|u|}$ and the components are mutually orthogonal in $L^2([0,1]^d)$. Let the superposition dimension of $f$ be at most $t$, meaning $f_u \\equiv 0$ for all index sets $u$ with $|u|>t$. Denote $\\sigma_u^2=\\operatorname{Var}(f_u(\\boldsymbol{U}))$ where $\\boldsymbol{U}\\sim \\operatorname{Unif}([0,1]^d)$, and assume an a priori bound $B$ such that\n$$\n\\sum_{u:1\\le |u|\\le t} \\sigma_u^2 \\le B,\n$$\nwith $B<\\infty$ known.\n\nYou approximate $\\mu=\\mathbb{E}[f(\\boldsymbol{U})]$ using $N$ function evaluations generated by a randomized Orthogonal Array-based Latin Hypercube Sampling (OA-LHS) design of strength $t$, constructed from an orthogonal array that balances all projections onto any $t$ coordinates and employs independent uniform jitter within the induced $t$-dimensional strata. Let $\\widehat{\\mu}_N$ denote the resulting unbiased OA-LHS estimator of $\\mu$.\n\nStarting from the ANOVA decomposition and the defining properties of orthogonal arrays and stratified sampling, derive a nonasymptotic upper bound on the mean squared error $\\mathbb{E}\\big[(\\widehat{\\mu}_N-\\mu)^2\\big]$ in terms of $N$, $B$, and a finite design-dependent constant $\\kappa_t>0$ (independent of $N$ and $B$) that encapsulates the effect of $t$-wise balancing and within-stratum randomization. Then, using that bound, determine the minimal real-valued sample size $N$ required to guarantee a target mean squared error tolerance $\\epsilon^2$, where $\\epsilon>0$ is given. Express your final answer as a single closed-form analytic expression in terms of $\\epsilon$, $B$, and $\\kappa_t$. Ignore integrality and any feasibility congruences required by a specific orthogonal array construction, and provide $N$ as a positive real number.", "solution": "We begin with the Hoeffding–Sobol ANOVA decomposition $f(\\boldsymbol{x})=\\sum_{u\\subseteq\\{1,\\dots,d\\}} f_u(\\boldsymbol{x}_u)$. The properties of the decomposition imply that\n$$\n\\operatorname{Var}(f(\\boldsymbol{U}))=\\sum_{u:1\\le |u|\\le d}\\sigma_u^2,\n$$\nwhere $\\sigma_u^2=\\operatorname{Var}(f_u(\\boldsymbol{U}))$. Since the superposition dimension is at most $t$, $f_u\\equiv 0$ for $|u|>t$, so we have\n$$\n\\operatorname{Var}(f(\\boldsymbol{U}))=\\sum_{u:1\\le |u|\\le t}\\sigma_u^2\\le B.\n$$\nThe OA-LHS estimator $\\widehat{\\mu}_N=\\frac{1}{N}\\sum_{i=1}^N f(\\boldsymbol{X}_i)$ is unbiased, so $\\mathbb{E}[\\widehat{\\mu}_N]=\\mu$. Thus, the mean squared error is equal to the variance:\n$$\n\\mathbb{E}\\big[(\\widehat{\\mu}_N-\\mu)^2\\big]=\\operatorname{Var}(\\widehat{\\mu}_N).\n$$\nThe key property of a strength-$t$ OA-LHS design is that it eliminates the leading-order variance contributions from all ANOVA terms $f_u$ with $|u| \\le t$. For such designs, a standard result from Quasi-Monte Carlo theory states that the variance is bounded by\n$$\n\\operatorname{Var}(\\widehat{\\mu}_N)\\le \\frac{\\kappa_t}{N^2}\\sum_{u:1\\le |u|\\le t}\\sigma_u^2,\n$$\nwhere $\\kappa_t>0$ is a constant that depends on the specific construction of the OA-LHS design but not on $N$ or the function $f$. This improved $N^{-2}$ convergence rate (compared to the $N^{-1}$ rate of simple Monte Carlo) is a direct consequence of the combinatorial cancellation of error terms enforced by the strength-$t$ balance.\n\nUsing the given a priori bound $\\sum_{u:1\\le |u|\\le t}\\sigma_u^2 \\le B$, we obtain the nonasymptotic inequality:\n$$\n\\mathbb{E}\\big[(\\widehat{\\mu}_N-\\mu)^2\\big] = \\operatorname{Var}(\\widehat{\\mu}_N) \\le \\frac{\\kappa_t B}{N^2}.\n$$\nTo achieve the target mean squared error tolerance $\\epsilon^2$, we require:\n$$\n\\frac{\\kappa_t B}{N^2} \\le \\epsilon^2.\n$$\nSolving this inequality for $N$ gives:\n$$\nN^2 \\ge \\frac{\\kappa_t B}{\\epsilon^2} \\implies N \\ge \\sqrt{\\frac{\\kappa_t B}{\\epsilon^2}} = \\frac{\\sqrt{\\kappa_t B}}{\\epsilon}.\n$$\nThe problem asks for the minimal real-valued sample size, so we take the lower bound as the answer.\n$$\nN_{\\min}=\\frac{\\sqrt{\\kappa_t B}}{\\epsilon}.\n$$", "answer": "$$\\boxed{\\frac{\\sqrt{\\kappa_t B}}{\\epsilon}}$$", "id": "3325870"}, {"introduction": "Theoretical designs are often perfect, but real-world applications frequently involve imperfections like budget limitations or data loss. This exercise addresses the practical consequences of deviating from a complete orthogonal array by randomly thinning the design points. You will analyze how this process impacts the statistical properties of the sample, quantifying the loss of balance and the resulting variance inflation, thereby developing a deeper understanding of the robustness of orthogonal array sampling [@problem_id:3325867].", "problem": "Consider an orthogonal array (OA) of strength $t$, denoted $OA(N,k,s,t)$, where $N$ is the number of design points, $k$ is the number of factors, each factor has $s$ levels, and strength $t$ means that for any selection of $t$ columns, all $s^{t}$ level combinations occur equally often. Let $t'$ be an integer with $1 \\leq t' < t$. By the defining property of the $OA(N,k,s,t)$, for any fixed selection $J$ of $t'$ columns, each of the $s^{t'}$ level combinations on $J$ appears exactly $N/s^{t'}$ times in the full array.\n\nSuppose a random thinning is performed: select $n$ points uniformly at random without replacement from the $N$ points, with $1 \\leq n < N$. For the fixed $t'$-subset $J$, index the $s^{t'}$ level combinations by $u \\in \\{1,2,\\dots,s^{t'}\\}$. Let $X_{u}$ denote the count of the selected $n$ points that fall into level combination $u$ on $J$. Define the empirical proportion for cell $u$ by $\\hat{p}_{u} = X_{u}/n$, and define the residual $t'$-wise imbalance measure by\n$$\nD_{t'} \\;=\\; \\sum_{u=1}^{s^{t'}} \\left( X_{u} - \\frac{n}{s^{t'}} \\right)^{2}.\n$$\nNow consider a function $f$ on the $k$-factor design space that depends only on the $t'$ coordinates in $J$ and is constant within each $t'$-wise level combination on $J$. That is, there exist real constants $\\{c_{u}\\}_{u=1}^{s^{t'}}$ such that $f$ takes value $c_{u}$ on all points whose $J$-coordinates form level combination $u$. The Monte Carlo estimator of the mean of $f$ based on the $n$ thinned points is the sample average\n$$\n\\hat{\\mu}_{f} \\;=\\; \\frac{1}{n} \\sum_{i=1}^{n} f(X^{(i)}),\n$$\nwhere $X^{(i)}$ are the $n$ selected points.\n\nStarting from fundamental definitions of orthogonal arrays and standard sampling without replacement, derive:\n- The induced bias in the empirical cell proportion for any fixed cell $u$, defined as $B_{t'} = \\mathbb{E}[\\hat{p}_{u}] - \\frac{1}{s^{t'}}$.\n- The expected residual imbalance $\\mathbb{E}[D_{t'}]$.\n- The variance inflation for the estimator $\\hat{\\mu}_{f}$, quantified by $\\operatorname{Var}(\\hat{\\mu}_{f})$ in terms of $N$, $n$, $s$, $t'$, and the constants $\\{c_{u}\\}$.\n\nExpress the final answer as a single row matrix containing, in order, the three quantities $B_{t'}$, $\\mathbb{E}[D_{t'}]$, and $\\operatorname{Var}(\\hat{\\mu}_{f})$. No rounding is required, and no physical units are involved. Your derivation must be based on the foundational properties of orthogonal arrays and well-tested facts of multivariate hypergeometric sampling.", "solution": "The problem describes a sampling process where a sample of size $n$ is drawn without replacement from a population of size $N$. The population is partitioned into $K = s^{t'}$ strata, each of size $N_u = N/s^{t'}$. The vector of counts $(X_1, \\dots, X_{s^{t'}})$ from each stratum follows a multivariate hypergeometric distribution.\n\n**1. Induced Bias $B_{t'}$**\n\nThe bias is $B_{t'} = \\mathbb{E}[\\hat{p}_{u}] - \\frac{1}{s^{t'}}$. The expected count for a single cell $u$ from a sample of size $n$ drawn without replacement is given by the sample size times the population proportion:\n$$\n\\mathbb{E}[X_u] = n \\frac{N_u}{N}\n$$\nSubstituting $N_u = N/s^{t'}$:\n$$\n\\mathbb{E}[X_u] = n \\frac{N/s^{t'}}{N} = \\frac{n}{s^{t'}}\n$$\nThe expectation of the empirical proportion $\\hat{p}_u = X_u/n$ is:\n$$\n\\mathbb{E}[\\hat{p}_u] = \\frac{1}{n}\\mathbb{E}[X_u] = \\frac{1}{n} \\left(\\frac{n}{s^{t'}}\\right) = \\frac{1}{s^{t'}}\n$$\nTherefore, the bias is zero:\n$$\nB_{t'} = \\frac{1}{s^{t'}} - \\frac{1}{s^{t'}} = 0\n$$\n\n**2. Expected Residual Imbalance $\\mathbb{E}[D_{t'}]$**\n\nThe residual imbalance is $D_{t'} = \\sum_{u=1}^{s^{t'}} \\left( X_{u} - \\frac{n}{s^{t'}} \\right)^{2}$. Since $\\mathbb{E}[X_u] = n/s^{t'}$, its expectation is the sum of the variances of the cell counts:\n$$\n\\mathbb{E}[D_{t'}] = \\mathbb{E}\\left[ \\sum_{u=1}^{s^{t'}} \\left( X_{u} - \\mathbb{E}[X_u] \\right)^{2} \\right] = \\sum_{u=1}^{s^{t'}} \\operatorname{Var}(X_u)\n$$\nThe variance of a hypergeometric random variable is:\n$$\n\\operatorname{Var}(X_u) = n \\frac{N_u}{N} \\left(1 - \\frac{N_u}{N}\\right) \\frac{N-n}{N-1}\n$$\nSubstituting the population proportion $\\frac{N_u}{N} = \\frac{1}{s^{t'}}$:\n$$\n\\operatorname{Var}(X_u) = n \\frac{1}{s^{t'}} \\left(1 - \\frac{1}{s^{t'}}\\right) \\frac{N-n}{N-1}\n$$\nSince this variance is the same for all $s^{t'}$ cells, we sum them up:\n$$\n\\mathbb{E}[D_{t'}] = s^{t'} \\times \\operatorname{Var}(X_u) = s^{t'} \\left[ n \\frac{1}{s^{t'}} \\left(1 - \\frac{1}{s^{t'}}\\right) \\frac{N-n}{N-1} \\right]\n$$\nSimplifying gives:\n$$\n\\mathbb{E}[D_{t'}] = n \\left(1 - \\frac{1}{s^{t'}}\\right) \\frac{N-n}{N-1}\n$$\n\n**3. Variance of the Estimator $\\operatorname{Var}(\\hat{\\mu}_{f})$**\n\nThe estimator can be written as $\\hat{\\mu}_{f} = \\frac{1}{n} \\sum_{u=1}^{s^{t'}} c_u X_u$. Its variance is:\n$$\n\\operatorname{Var}(\\hat{\\mu}_{f}) = \\frac{1}{n^2} \\operatorname{Var}\\left( \\sum_{u=1}^{s^{t'}} c_u X_u \\right) = \\frac{1}{n^2} \\left( \\sum_u c_u^2 \\operatorname{Var}(X_u) + \\sum_{u \\neq v} c_u c_v \\operatorname{Cov}(X_u, X_v) \\right)\n$$\nThe covariance for the multivariate hypergeometric distribution is:\n$$\n\\operatorname{Cov}(X_u, X_v) = -n \\frac{N_u}{N} \\frac{N_v}{N} \\frac{N-n}{N-1} = -n \\frac{1}{(s^{t'})^2} \\frac{N-n}{N-1}\n$$\nSubstituting the variance and covariance terms and factoring out the common term $\\frac{n(N-n)}{N-1}$ gives:\n$$\n\\operatorname{Var}\\left( \\sum c_u X_u \\right) = \\frac{n(N-n)}{N-1} \\left[ \\sum_{u} c_u^2 \\frac{1}{s^{t'}}\\left(1 - \\frac{1}{s^{t'}}\\right) - \\sum_{u \\neq v} c_u c_v \\frac{1}{(s^{t'})^2} \\right]\n$$\n$$\n= \\frac{n(N-n)}{N-1} \\left[ \\frac{1}{s^{t'}} \\sum_{u} c_u^2 - \\frac{1}{(s^{t'})^2} \\sum_{u} c_u^2 - \\frac{1}{(s^{t'})^2} \\sum_{u \\neq v} c_u c_v \\right]\n$$\nUsing the identity $\\left(\\sum_u c_u\\right)^2 = \\sum_u c_u^2 + \\sum_{u \\neq v} c_u c_v$, this simplifies to:\n$$\n= \\frac{n(N-n)}{N-1} \\left[ \\frac{1}{s^{t'}} \\sum_{u=1}^{s^{t'}} c_u^2 - \\frac{1}{(s^{t'})^2} \\left( \\sum_{u=1}^{s^{t'}} c_u \\right)^2 \\right]\n$$\nFinally, dividing by $n^2$ gives $\\operatorname{Var}(\\hat{\\mu}_{f})$:\n$$\n\\operatorname{Var}(\\hat{\\mu}_{f}) = \\frac{N-n}{n(N-1)} \\left[ \\frac{1}{s^{t'}} \\sum_{u=1}^{s^{t'}} c_u^2 - \\frac{1}{(s^{t'})^2} \\left( \\sum_{u=1}^{s^{t'}} c_u \\right)^2 \\right]\n$$\nThe term in brackets is the population variance of the cell values $\\{c_u\\}$.\n\nThe three requested quantities are thus:\n1. $B_{t'} = 0$\n2. $\\mathbb{E}[D_{t'}] = n \\left(1 - \\frac{1}{s^{t'}}\\right) \\frac{N-n}{N-1}$\n3. $\\operatorname{Var}(\\hat{\\mu}_{f}) = \\frac{N-n}{n(N-1)} \\left[ \\frac{1}{s^{t'}} \\sum_{u=1}^{s^{t'}} c_u^2 - \\frac{1}{(s^{t'})^2} \\left( \\sum_{u=1}^{s^{t'}} c_u \\right)^2 \\right]$", "answer": "$$\n\\boxed{\\begin{pmatrix} 0 & n \\left(1 - \\frac{1}{s^{t'}}\\right) \\frac{N-n}{N-1} & \\frac{N-n}{n(N-1)} \\left[ \\frac{1}{s^{t'}} \\sum_{u=1}^{s^{t'}} c_u^2 - \\frac{1}{(s^{t'})^2} \\left( \\sum_{u=1}^{s^{t'}} c_u \\right)^2 \\right] \\end{pmatrix}}\n$$", "id": "3325867"}]}