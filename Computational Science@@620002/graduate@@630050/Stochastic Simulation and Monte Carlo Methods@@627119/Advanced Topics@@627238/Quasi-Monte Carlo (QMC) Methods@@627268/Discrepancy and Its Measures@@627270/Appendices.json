{"hands_on_practices": [{"introduction": "To truly grasp the concept of discrepancy, it is essential to move from abstract definitions to concrete calculations. This first exercise provides a foundational workout, guiding you through the exact computation of the one-dimensional star discrepancy, $D_N^*$, for a simple, equispaced point set [@problem_id:3303337]. By analyzing the behavior of the discrepancy function piecewise, you will develop a direct intuition for how this crucial metric quantifies uniformity and discover that even highly regular patterns are not always optimal.", "problem": "Let $\\{x_{n}\\}_{n=1}^{N} \\subset [0,1]$ be a point set used in Quasi-Monte Carlo (QMC) methods, and recall the star discrepancy in $s$ dimensions is defined by\n$$\nD_{N}^{*}(\\{x_{n}\\}_{n=1}^{N}) \\;=\\; \\sup_{t \\in [0,1]^{s}} \\left| \\frac{1}{N} \\sum_{n=1}^{N} \\mathbf{1}\\{x_{n} \\in [0,t)\\} \\;-\\; \\prod_{j=1}^{s} t_{j} \\right|,\n$$\nwhere $[0,t) = \\prod_{j=1}^{s} [0,t_{j})$ and $\\mathbf{1}\\{\\cdot\\}$ denotes the indicator function. Specialize to the one-dimensional case $s=1$, so that\n$$\nD_{N}^{*}(\\{x_{n}\\}_{n=1}^{N}) \\;=\\; \\sup_{t \\in [0,1]} \\left| \\frac{1}{N} \\sum_{n=1}^{N} \\mathbf{1}\\{x_{n} < t\\} \\;-\\; t \\right|.\n$$\nConsider the equispaced point set $x_{n} = \\frac{n}{N}$ for $n=1,2,\\dots,N$. Starting from the above definition, derive an exact closed-form expression for $D_{N}^{*}$ as a function of $N$. Then, interpret your result in terms of one-dimensional optimality: compare the value you obtained to the minimal possible star discrepancy achievable by any $N$-point set on $[0,1]$, and state whether the equispaced endpoints design is optimal in one dimension.\n\nExpress your final answer for $D_{N}^{*}$ as a closed-form function of $N$. No rounding is required.", "solution": "The problem is valid as it is scientifically grounded, well-posed, objective, and self-contained. It presents a standard exercise in the theory of uniform distribution and Quasi-Monte Carlo methods.\n\nWe are asked to derive a closed-form expression for the star discrepancy $D_N^*$ of the one-dimensional point set $\\{x_n\\}_{n=1}^N$ where $x_n = \\frac{n}{N}$ for $n=1, 2, \\dots, N$. The one-dimensional star discrepancy is defined as:\n$$\nD_{N}^{*}(\\{x_{n}\\}_{n=1}^{N}) = \\sup_{t \\in [0,1]} \\left| \\frac{1}{N} \\sum_{n=1}^{N} \\mathbf{1}\\{x_{n} < t\\} - t \\right|\n$$\nLet us define the function whose supremum we seek:\n$$\nf(t) = E_N(t) - t = \\frac{1}{N} \\sum_{n=1}^{N} \\mathbf{1}\\{x_{n} < t\\} - t\n$$\nHere, $E_N(t)$ is the empirical distribution function of the point set. The points are $x_1 = \\frac{1}{N}, x_2 = \\frac{2}{N}, \\dots, x_N = \\frac{N}{N} = 1$. These points partition the interval $[0,1]$ into subintervals. We will analyze the function $f(t)$ on these subintervals.\n\nThe domain $[0,1]$ can be written as the union of the point $\\{0\\}$ and the intervals $(\\frac{k}{N}, \\frac{k+1}{N}]$ for $k=0, 1, \\dots, N-1$.\n\nCase 1: $t=0$.\nAt $t=0$, no points $x_n = \\frac{n}{N}$ (which are all positive) are strictly less than $0$. Thus, the sum is $0$.\n$$\nf(0) = \\frac{1}{N} \\sum_{n=1}^{N} \\mathbf{1}\\{x_{n} < 0\\} - 0 = \\frac{0}{N} - 0 = 0\n$$\n\nCase 2: $t \\in (0, \\frac{1}{N}]$.\nFor any $t$ in this interval, there are no points $x_n = \\frac{n}{N}$ that are strictly less than $t$. The smallest point is $x_1 = \\frac{1}{N}$, and for any $t \\in (0, \\frac{1}{N}]$, we have $x_n \\ge \\frac{1}{N} \\ge t$. Thus, $\\mathbf{1}\\{x_n < t\\} = 0$ for all $n$.\nThe empirical distribution function is $E_N(t) = 0$.\nThe function $f(t)$ becomes:\n$$\nf(t) = 0 - t = -t\n$$\nWe are interested in $|f(t)| = t$. On the interval $(0, \\frac{1}{N}]$, this function is maximized at $t=\\frac{1}{N}$, where $|f(\\frac{1}{N})| = \\frac{1}{N}$.\n\nCase 3: $t \\in (\\frac{k}{N}, \\frac{k+1}{N}]$ for $k=1, 2, \\dots, N-1$.\nFor any $t$ in such an interval, we need to count how many points $x_n = \\frac{n}{N}$ are strictly less than $t$.\nThe condition $x_n < t$ is $\\frac{n}{N} < t$.\nSince $t > \\frac{k}{N}$, the points $x_1=\\frac{1}{N}, x_2=\\frac{2}{N}, \\dots, x_k=\\frac{k}{N}$ are all strictly less than $t$. This accounts for $k$ points.\nSince $t \\le \\frac{k+1}{N}$, for any $n \\ge k+1$, we have $x_n = \\frac{n}{N} \\ge \\frac{k+1}{N} \\ge t$. So these points are not strictly less than $t$.\nTherefore, for any $t \\in (\\frac{k}{N}, \\frac{k+1}{N}]$, there are exactly $k$ points satisfying $x_n < t$.\nThe empirical distribution function is $E_N(t) = \\frac{k}{N}$.\nThe function $f(t)$ on this interval is:\n$$\nf(t) = \\frac{k}{N} - t\n$$\nThis is a linear function of $t$ with a slope of $-1$. To find the supremum of its absolute value on the interval $(\\frac{k}{N}, \\frac{k+1}{N}]$, we examine the endpoints.\nAs $t$ approaches the left endpoint from the right, $t \\to (\\frac{k}{N})^{+}$:\n$$\n\\lim_{t \\to (\\frac{k}{N})^{+}} f(t) = \\frac{k}{N} - \\frac{k}{N} = 0\n$$\nAt the right endpoint, $t = \\frac{k+1}{N}$:\n$$\nf\\left(\\frac{k+1}{N}\\right) = \\frac{k}{N} - \\frac{k+1}{N} = -\\frac{1}{N}\n$$\nOn the interval $(\\frac{k}{N}, \\frac{k+1}{N}]$, $f(t)$ ranges from (but not including) $0$ down to $-\\frac{1}{N}$. Thus, $|f(t)|$ ranges from $0$ up to $\\frac{1}{N}$. The supremum of $|f(t)|$ on this interval is $\\frac{1}{N}$.\n\nCombining all cases, for any $t \\in (0,1]$, there is a $k \\in \\{0, \\dots, N-1\\}$ such that $t \\in (\\frac{k}{N}, \\frac{k+1}{N}]$. On each of these intervals, the supremum of $|f(t)|$ is $\\frac{1}{N}$. At $t=0$, $|f(0)|=0$.\nThe overall supremum over the entire interval $[0,1]$ is therefore the maximum of these values.\n$$\nD_N^* = \\sup_{t \\in [0,1]} |f(t)| = \\max\\left( |f(0)|, \\sup_{t \\in (0,1]} |f(t)| \\right) = \\max\\left(0, \\frac{1}{N}\\right) = \\frac{1}{N}\n$$\nThe closed-form expression for the star discrepancy of this point set is $D_N^* = \\frac{1}{N}$.\n\nFor the second part of the problem, we must interpret this result. In one dimension, it is a classical result that for any set of $N$ points $\\{y_n\\}_{n=1}^N \\subset [0,1]$, the star discrepancy is bounded from below:\n$$\nD_N^*(\\{y_n\\}_{n=1}^N) \\ge \\frac{1}{2N}\n$$\nThis lower bound is sharp. It is achieved by the point set $y_n = \\frac{2n-1}{2N}$ for $n=1, \\dots, N$. This set consists of the midpoints of the intervals $[\\frac{n-1}{N}, \\frac{n}{N}]$. The star discrepancy for this set is exactly $\\frac{1}{2N}$. A point set that achieves the minimal possible discrepancy is called optimal.\nThe point set given in the problem, $x_n = \\frac{n}{N}$, yields a discrepancy of $D_N^* = \\frac{1}{N}$.\nComparing our result to the optimal value, we have $\\frac{1}{N}$ versus $\\frac{1}{2N}$. For any $N > 1$, we see that $\\frac{1}{N} > \\frac{1}{2N}$.\nTherefore, the equispaced endpoint design $x_n = \\frac{n}{N}$ is not optimal in one dimension, as its star discrepancy is twice the minimum possible value.", "answer": "$$\n\\boxed{\\frac{1}{N}}\n$$", "id": "3303337"}, {"introduction": "Having calculated discrepancy for a given set, the next logical step is to construct a sequence designed to be uniform and then measure its quality. This practice bridges theory and application by tasking you with generating the Halton sequence, a cornerstone of Quasi-Monte Carlo methods, from its number-theoretic roots in the radical inverse function [@problem_id:3303284]. You will then implement an exact algorithm to compute its star discrepancy, transforming an abstract quality metric into a tangible number and confronting the practical challenges of uniformity in higher dimensions.", "problem": "You are tasked with constructing low-discrepancy points for use in Quasi-Monte Carlo (QMC) methods and quantifying their uniformity via star discrepancy. Use the following foundational definitions.\n\nLet $p \\geq 2$ be an integer base and let $n \\in \\mathbb{N}$ be a positive integer. Write $n$ in base $p$ as $n = \\sum_{k=0}^{K} a_k p^k$ with digits $a_k \\in \\{0,1,\\dots,p-1\\}$. The radical inverse function in base $p$ is the map $\\phi_p : \\mathbb{N} \\to [0,1)$ defined by\n$$\n\\phi_p(n) = \\sum_{k=0}^{K} a_k p^{-(k+1)}.\n$$\nFor a dimension $s \\in \\mathbb{N}$ and pairwise coprime integer bases $(p_1,\\dots,p_s)$ (traditionally distinct primes), the $s$-dimensional Halton sequence is defined by points\n$$\n\\mathbf{x}_n = \\big(\\phi_{p_1}(n),\\dots,\\phi_{p_s}(n)\\big), \\quad n = 1,2,\\dots.\n$$\n\nFor a finite point set $P_N = \\{\\mathbf{x}_1,\\dots,\\mathbf{x}_N\\} \\subset [0,1)^s$, the star discrepancy is defined by\n$$\nD_N^*(P_N) = \\sup_{\\mathbf{u} \\in [0,1]^s} \\left| \\frac{1}{N} \\sum_{i=1}^N \\mathbf{1}\\{\\mathbf{x}_i \\in [\\mathbf{0},\\mathbf{u})\\} - \\prod_{j=1}^s u_j \\right|,\n$$\nwhere $[\\mathbf{0},\\mathbf{u}) = \\prod_{j=1}^s [0,u_j)$ and $\\mathbf{1}\\{\\cdot\\}$ denotes the indicator function.\n\nYour tasks are:\n\n1. Implement the radical inverse $\\phi_p(n)$ from its base-$p$ definition and construct the first $N$ points $\\mathbf{x}_1,\\dots,\\mathbf{x}_N$ of the Halton sequence in dimension $s$ using bases $(p_1,\\dots,p_s)$.\n2. Compute the exact star discrepancy $D_N^*(P_N)$ by evaluating the supremum over a finite candidate set defined as follows: let $U_j$ be the set consisting of all coordinates $\\{x_{i,j} : i=1,\\dots,N\\}$ augmented with $1$, and consider the Cartesian product $U = U_1 \\times \\dots \\times U_s$. Evaluate the discrepancy expression on all $\\mathbf{u} \\in U$ and take the maximum value. This computes $D_N^*(P_N)$ exactly for star discrepancy because the supremum is attained at such grid points for anchored boxes.\n3. In addition to implementing the computation, argue qualitatively (in your solution write-up, not in code) how $D_N^*(P_N)$ grows with the dimension $s$ when $(p_1,\\dots,p_s)$ are distinct small primes and $N$ increases, grounded in the definitions above and the combinatorial structure of anchored boxes. No physical units are involved, and angles are not part of this task.\n\nTest suite:\n- Case A: $N = 1$, $(p_1) = (2)$, $s = 1$.\n- Case B: $N = 10$, $(p_1,p_2) = (2,3)$, $s = 2$.\n- Case C: $N = 12$, $(p_1,p_2,p_3) = (2,5,3)$, $s = 3$.\n- Case D: $N = 20$, $(p_1,p_2,p_3,p_4) = (2,3,5,7)$, $s = 4$.\n\nYour program should:\n- Construct $P_N$ for each case,\n- Compute $D_N^*(P_N)$ exactly using the candidate set $U$ described above,\n- Produce a single line of output containing the star discrepancy values for the four cases as a comma-separated list enclosed in square brackets, in the order A, B, C, D, for example, \"[0.5,0.123,0.456,0.789]\". Each value must be a real number (float); no additional text should be printed.", "solution": "The problem requires the implementation and analysis of Halton sequences and their star discrepancy. The solution proceeds in three parts: first, we formulate the algorithm for generating the points of a Halton sequence; second, we detail the procedure for computing the star discrepancy as specified; and third, we provide a qualitative argument regarding the behavior of discrepancy in higher dimensions.\n\nA Halton sequence is a deterministic, low-discrepancy sequence used in Quasi-Monte Carlo methods to generate points that are more uniformly distributed than pseudo-random points. Its construction is based on the radical inverse function.\n\nLet $p \\geq 2$ be an integer base. Any positive integer $n$ has a unique base-$p$ expansion $n = \\sum_{k=0}^{K} a_k p^k$, where the digits $a_k$ are in $\\{0, 1, \\dots, p-1\\}$. The radical inverse function, $\\phi_p: \\mathbb{N} \\to [0,1)$, is defined by reflecting this expansion about the decimal point:\n$$\n\\phi_p(n) = \\sum_{k=0}^{K} a_k p^{-(k+1)}.\n$$\nTo implement this function for a given integer $n$ and base $p$, one can iteratively extract the base-$p$ digits of $n$. The standard algorithm for obtaining base-$p$ digits starts with the least significant digit, $a_0 = n \\pmod p$. The next digit is $a_1 = (n // p) \\pmod p$, and so on. The algorithm for $\\phi_p(n)$ proceeds as follows: initialize a result $r$ to $0$ and a factor $f$ to $1/p$. While $n>0$, compute the digit $d = n \\pmod p$, add $d \\cdot f$ to $r$, update $f$ to $f/p$, and update $n$ to $n // p$. This correctly computes the summation, as the factor $f$ takes values $p^{-1}, p^{-2}, \\dots$ corresponding to digits $a_0, a_1, \\dots$.\n\nAn $s$-dimensional Halton sequence is constructed using $s$ pairwise coprime integer bases, $(p_1, p_2, \\dots, p_s)$, which are typically chosen as the first $s$ prime numbers. The $n$-th point in the sequence (for $n=1, 2, \\dots, N$) is given by:\n$$\n\\mathbf{x}_n = \\big(\\phi_{p_1}(n), \\phi_{p_2}(n), \\dots, \\phi_{p_s}(n)\\big).\n$$\nGenerating the point set $P_N = \\{\\mathbf{x}_1, \\dots, \\mathbf{x}_N\\}$ involves iterating $n$ from $1$ to $N$ and, for each $n$, computing the $s$ components of $\\mathbf{x}_n$ using the radical inverse function with the corresponding bases.\n\nThe uniformity of the point set $P_N$ is quantified by its star discrepancy, $D_N^*(P_N)$. This metric measures the largest deviation between the empirical distribution of points and the uniform distribution over all anchored hyper-rectangles (boxes) of the form $[\\mathbf{0}, \\mathbf{u}) = [0, u_1) \\times \\dots \\times [0, u_s)$, where $\\mathbf{u} = (u_1, \\dots, u_s) \\in [0,1]^s$. The definition is:\n$$\nD_N^*(P_N) = \\sup_{\\mathbf{u} \\in [0,1]^s} \\left| \\frac{1}{N} \\sum_{i=1}^N \\mathbf{1}\\{\\mathbf{x}_i \\in [\\mathbf{0},\\mathbf{u})\\} - \\text{Vol}([\\mathbf{0},\\mathbf{u})) \\right|,\n$$\nwhere $\\mathbf{1}\\{\\cdot\\}$ is the indicator function and the volume of the box is $\\text{Vol}([\\mathbf{0},\\mathbf{u})) = \\prod_{j=1}^s u_j$. The term $\\frac{1}{N} \\sum_{i=1}^N \\mathbf{1}\\{\\mathbf{x}_i \\in [\\mathbf{0},\\mathbf{u})\\}$ represents the fraction of points from $P_N$ that fall inside the box $[\\mathbf{0}, \\mathbf{u})$.\n\nA direct computation of the supremum over the continuous domain $[0,1]^s$ is intractable. However, it is a known result in discrepancy theory that the supremum is always attained at a point $\\mathbf{u}$ whose coordinates are taken from the coordinates of the points in $P_N$. The problem specifies an exact computational procedure based on this principle. We define a finite candidate set of test points $\\mathbf{u}$. For each dimension $j \\in \\{1,\\dots,s\\}$, let $U_j$ be the set of $j$-th coordinates of all points in $P_N$, augmented with the value $1$: $U_j = \\{x_{i,j} : i=1,\\dots,N\\} \\cup \\{1\\}$. The full set of candidate points is the Cartesian product $U = U_1 \\times \\dots \\times U_s$. The star discrepancy is then computed by finding the maximum of the discrepancy function over all $\\mathbf{u} \\in U$. The algorithm is:\n1. Generate the point set $P_N = \\{\\mathbf{x}_1, \\dots, \\mathbf{x}_N\\}$.\n2. For each dimension $j=1, \\dots, s$, form the set of unique coordinates $U_j = \\text{unique}(\\{x_{i,j}\\}_{i=1}^N) \\cup \\{1\\}$.\n3. Initialize a variable `max_discrepancy` to $0$.\n4. For each vector $\\mathbf{u}$ in the Cartesian product $U_1 \\times \\dots \\times U_s$:\n   a. Calculate the volume $V = \\prod_{j=1}^s u_j$.\n   b. Count the number of points $C = \\sum_{i=1}^N \\mathbf{1}\\{\\mathbf{x}_i < \\mathbf{u}\\}$, where $\\mathbf{x}_i < \\mathbf{u}$ is shorthand for $x_{i,j} < u_j$ for all $j=1, \\dots, s$.\n   c. Compute the local discrepancy $d = |C/N - V|$.\n   d. Update `max_discrepancy` = $\\max(\\text{max\\_discrepancy}, d)$.\nThe final `max_discrepancy` is the exact value of $D_N^*(P_N)$. The number of candidate points in $U$ is at most $(N+1)^s$, which is computationally feasible for the parameters specified in the test suite.\n\nFinally, we consider the qualitative behavior of $D_N^*(P_N)$ as the dimension $s$ increases, for a fixed number of points $N$. As $s$ grows, the volume of the unit hypercube $[0,1)^s$ becomes increasingly concentrated near its boundary, an effect known as the \"curse of dimensionality\". A fixed number of points $N$ becomes progressively sparse in this high-dimensional space, making it much harder to maintain uniformity. The number of potential \"empty\" regions or regions with a disproportionate number of points escalates. The combinatorial complexity of the test space for discrepancy also grows, as the number of vertices in the test grid $U$ is bounded by $(N+1)^s$. This explosive growth implies that it is much easier to find a box $[\\mathbf{0}, \\mathbf{u})$ for which the empirical measure $C/N$ significantly deviates from its volume $V$. This intuition is confirmed by theoretical bounds on the star discrepancy of Halton sequences, which are of the form $D_N^*(P_N) = O\\left(\\frac{(\\log N)^s}{N}\\right)$. For a fixed $N$, this bound grows exponentially with $s$ (as $(\\log N)^s$). Therefore, we expect $D_N^*(P_N)$ to increase—and the quality of the point set to degrade—as the dimension $s$ increases, particularly if $N$ is not increased appropriately.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom itertools import product\n\ndef radical_inverse(n, base):\n    \"\"\"\n    Computes the radical inverse of an integer n in a given base.\n    This corresponds to the van der Corput sequence, which is the 1D Halton sequence.\n    \"\"\"\n    if n == 0:\n        return 0.0\n    \n    res = 0.0\n    p_inv = 1.0 / base\n    temp_n = n\n    while temp_n > 0:\n        digit = temp_n % base\n        res += digit * p_inv\n        p_inv /= base\n        temp_n //= base\n    return res\n\ndef halton_sequence(n_points, dim, bases):\n    \"\"\"\n    Generates the first n_points of an s-dimensional Halton sequence.\n    \"\"\"\n    points = np.zeros((n_points, dim))\n    for i in range(n_points):\n        # The sequence is defined for n = 1, 2, ...\n        n_val = i + 1\n        for d in range(dim):\n            points[i, d] = radical_inverse(n_val, bases[d])\n    return points\n\ndef compute_star_discrepancy(points, n_points, dim):\n    \"\"\"\n    Computes the exact star discrepancy for a given point set P_N.\n    The supremum is evaluated over a finite grid of test points U,\n    as specified in the problem statement.\n    \"\"\"\n    if n_points == 0:\n        return 0.0\n\n    # Build the candidate coordinate sets U_j for each dimension j\n    # U_j = {x_{i,j} for i=1..N} U {1}\n    candidate_coords = []\n    for j in range(dim):\n        # Get unique coordinates for dimension j and add 1.0\n        coords_j = np.unique(points[:, j]).tolist()\n        coords_j.append(1.0)\n        candidate_coords.append(coords_j)\n    \n    max_discrepancy = 0.0\n    \n    # Iterate through all test vectors u in the Cartesian product U\n    # U = U_1 x U_2 x ... x U_s\n    for u in product(*candidate_coords):\n        u_vec = np.array(u)\n        \n        # Volume of the box [0, u) is the product of its side lengths\n        volume = np.prod(u_vec)\n        \n        # Count points x_i such that x_i < u (element-wise)\n        count = np.sum(np.all(points < u_vec, axis=1))\n        \n        # Empirical measure: fraction of points in the box\n        empirical_measure = count / n_points\n        \n        # Local discrepancy at this test point u\n        discrepancy = np.abs(empirical_measure - volume)\n        \n        # Update the maximum discrepancy found so far\n        if discrepancy > max_discrepancy:\n            max_discrepancy = discrepancy\n            \n    return max_discrepancy\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test cases.\n    \"\"\"\n    test_cases = [\n        # Case A: N=1, s=1, p=(2)\n        {'N': 1, 's': 1, 'bases': [2]},\n        # Case B: N=10, s=2, p=(2,3)\n        {'N': 10, 's': 2, 'bases': [2, 3]},\n        # Case C: N=12, s=3, p=(2,5,3)\n        {'N': 12, 's': 3, 'bases': [2, 5, 3]},\n        # Case D: N=20, s=4, p=(2,3,5,7)\n        {'N': 20, 's': 4, 'bases': [2, 3, 5, 7]},\n    ]\n\n    results = []\n    for case in test_cases:\n        N = case['N']\n        s = case['s']\n        bases = case['bases']\n\n        # 1. Construct the Halton point set P_N\n        points = halton_sequence(N, s, bases)\n        \n        # 2. Compute the star discrepancy D_N*(P_N)\n        discrepancy = compute_star_discrepancy(points, N, s)\n        results.append(discrepancy)\n\n    # Format the final output as a comma-separated list in brackets\n    print(f\"[{','.join(map(str, results))}]\")\n\n# Execute the solver\nsolve()\n```", "id": "3303284"}, {"introduction": "This final practice demonstrates the practical power of discrepancy-related ideas by applying them to a challenging problem in Monte Carlo integration. You will derive and implement a method for constructing rigorous error bounds for integrals of discontinuous functions, a scenario where the classic Koksma-Hlawka inequality fails [@problem_id:3303333]. The key is to leverage the connection between integration error and the one-dimensional Kolmogorov-Smirnov discrepancy of the function's output values, using the Dvoretzky-Kiefer-Wolfowitz (DKW) inequality to create a robust, practical tool that works even when the function has infinite variation.", "problem": "You are given the task of constructing discrepancy-aware error bars for Monte Carlo estimates of integrals of discontinuous functions in a way that remains valid even when the Hardy–Krause variation is infinite. Consider the integration of a bounded measurable function $f:[0,1]^d \\to \\mathbb{R}$ under the uniform probability measure on $[0,1]^d$, so that the target integral is the expectation $\\mathbb{E}[f(X)]$ with $X \\sim \\text{Uniform}([0,1]^d)$. The classical Koksma–Hlawka inequality uses Hardy–Krause variation to bound integration error for quasi-Monte Carlo methods, but it becomes vacuous when $V_{\\mathrm{HK}}(f)=\\infty$ for discontinuous $f$ in dimensions $d \\geq 2$. Your goal is to derive and implement an alternative approach that uses discrepancy of the pushforward sample values $Y_i = f(X_i)$ and yields finite, rigorous error bars without relying on Hardy–Krause variation.\n\nFrom first principles, proceed as follows. Start from the definition of the empirical cumulative distribution function (ECDF) of the pushforward random variable $Y = f(X)$, denoted $F_n(t) = \\frac{1}{n}\\sum_{i=1}^n \\mathbf{1}\\{Y_i \\le t\\}$ with $Y_i = f(X_i)$ and $X_i$ independent and identically distributed draws from the uniform distribution on $[0,1]^d$. Use well-tested facts about almost sure uniform convergence of the ECDF to the true cumulative distribution function $F_Y(t)$, and rigorously relate the integration error $\\left|\\frac{1}{n}\\sum_{i=1}^n f(X_i) - \\mathbb{E}[f(X)]\\right|$ to a bound expressed using the Kolmogorov–Smirnov discrepancy for the ECDF of $Y_i$. Derive high-confidence error bars for $\\mathbb{E}[f(X)]$ solely in terms of:\n- the sample values $Y_1,\\dots,Y_n$,\n- the fact that $f$ is bounded with known bounds $a \\le f(x) \\le b$ for all $x$ in $[0,1]^d$,\n- and standard, distribution-free inequalities for the ECDF.\n\nYou must implement the resulting error bars in a program and demonstrate their behavior on discontinuous functions $f$ for which $V_{\\mathrm{HK}}(f)=\\infty$ in dimension $d \\ge 2$. In all test cases below, $f$ is an indicator function of a geometric region, so $Y = f(X) \\in \\{0,1\\}$ and $(a,b) = (0,1)$. Use independent and identically distributed sampling for $X_i$ with a fixed random seed, and compute:\n- the Monte Carlo estimate $\\hat{\\mu} = \\frac{1}{n}\\sum_{i=1}^n Y_i$,\n- a rigorous two-sided confidence interval $[\\mathrm{L}, \\mathrm{U}]$ for $\\mathbb{E}[f(X)]$ at confidence level $1 - \\alpha$ based on the discrepancy of the ECDF of $Y_i$,\n- the true integral $\\mu^\\star$ (known analytically for the chosen regions),\n- and a boolean indicating whether $\\mu^\\star \\in [\\mathrm{L},\\mathrm{U}]$.\n\nFundamental base to be used in your derivation:\n- The definition of the empirical cumulative distribution function $F_n$ and the true cumulative distribution function $F_Y$ for a real-valued random variable $Y$.\n- The identity expressing the expectation of a bounded random variable in terms of its distribution function via tail integrals.\n- The fact that the empirical cumulative distribution function converges uniformly to the true cumulative distribution function and admits nonasymptotic distribution-free exponential tail bounds.\n\nYou are not allowed to rely on any bound derived from Hardy–Krause variation, and you must ensure the resulting error bars are valid when $V_{\\mathrm{HK}}(f) = \\infty$. Your program must compute these error bars using only the discrepancy of the ECDF of $Y_i$ and produce the final results as specified below.\n\nTest suite:\nFor each test case, generate $n$ independent and identically distributed points $X_i$ uniformly on $[0,1]^d$ using a fixed seed, evaluate $Y_i = f(X_i)$, and compute the requested outputs.\n\n- Test case $1$ (happy path, moderate sample size):\n  - Dimension: $d = 2$.\n  - Function: $f(x) = \\mathbf{1}\\{\\|x - c\\|_2 \\le r\\}$ where $c = (0.5, 0.5)$ and $r = 0.3$.\n  - This $f$ is discontinuous on a curved boundary in $d \\ge 2$, so $V_{\\mathrm{HK}}(f) = \\infty$.\n  - Analytical integral: the area of a disk of radius $r$ centered in the unit square, equal to $\\mu^\\star = \\pi r^2$ since the disk lies entirely inside $[0,1]^2$ for $r = 0.3$.\n  - Sample size: $n = 4096$.\n  - Confidence parameter: $\\alpha = 0.05$.\n  - Seed: $12345$.\n\n- Test case $2$ (larger dimension, more samples):\n  - Dimension: $d = 3$.\n  - Function: $f(x) = \\mathbf{1}\\{\\|x - c\\|_2 \\le r\\}$ where $c = (0.5, 0.5, 0.5)$ and $r = 0.3$.\n  - Discontinuity on a curved boundary implies $V_{\\mathrm{HK}}(f) = \\infty$.\n  - Analytical integral: the volume of a ball of radius $r$ centered in the unit cube, equal to $\\mu^\\star = \\frac{4}{3}\\pi r^3$ since the ball lies entirely inside $[0,1]^3$ for $r = 0.3$.\n  - Sample size: $n = 8192$.\n  - Confidence parameter: $\\alpha = 0.01$.\n  - Seed: $67890$.\n\n- Test case $3$ (edge case, small sample size and a different discontinuous geometry):\n  - Dimension: $d = 2$.\n  - Function: $f(x) = \\mathbf{1}\\{r_1 \\le \\|x - c\\|_2 \\le r_2\\}$ (an annulus), where $c = (0.5, 0.5)$, $r_1 = 0.2$, and $r_2 = 0.4$.\n  - Discontinuity on two curved boundaries implies $V_{\\mathrm{HK}}(f) = \\infty$.\n  - Analytical integral: the area of the annulus, equal to $\\mu^\\star = \\pi(r_2^2 - r_1^2)$ since the annulus lies entirely inside $[0,1]^2$ for $r_2 = 0.4$ and $c = (0.5, 0.5)$.\n  - Sample size: $n = 64$.\n  - Confidence parameter: $\\alpha = 0.2$.\n  - Seed: $24680$.\n\nOutput specification:\nYour program should produce a single line of output containing a list of results, one per test case, with each result structured as a list $[\\mathrm{L},\\mathrm{U},\\mu^\\star,\\mathrm{inside}]$, where:\n- $\\mathrm{L}$ and $\\mathrm{U}$ are floats representing the lower and upper ends of the discrepancy-aware confidence interval for $\\mathbb{E}[f(X)]$,\n- $\\mu^\\star$ is a float representing the analytical true integral,\n- $\\mathrm{inside}$ is a boolean indicating whether $\\mu^\\star$ lies within $[\\mathrm{L},\\mathrm{U}]$.\n\nFor example, the final output should look like $[[\\mathrm{L}_1,\\mathrm{U}_1,\\mu^\\star_1,\\mathrm{inside}_1],[\\mathrm{L}_2,\\mathrm{U}_2,\\mu^\\star_2,\\mathrm{inside}_2],[\\mathrm{L}_3,\\mathrm{U}_3,\\mu^\\star_3,\\mathrm{inside}_3]]$ with the actual numerical values computed by your program.", "solution": "The problem requires the derivation and implementation of a method to construct rigorous confidence intervals for Monte Carlo estimates of integrals for discontinuous, bounded functions. This is to be achieved without relying on the Hardy–Krause variation, which is infinite for the functions under consideration, but instead by leveraging the properties of the empirical cumulative distribution function (ECDF) of the pushforward samples.\n\nLet the integral of interest be $I = \\int_{[0,1]^d} f(x) dx$, where $f: [0,1]^d \\to \\mathbb{R}$ is a bounded, measurable function. Let $X$ be a random variable uniformly distributed on $[0,1]^d$. The integral can be expressed as the expectation $I = \\mu^\\star = \\mathbb{E}[f(X)]$. The Monte Carlo method estimates this integral using the sample mean of $n$ independent and identically distributed (i.i.d.) evaluations of $f$. Let $X_1, \\dots, X_n$ be i.i.d. draws from $\\text{Uniform}([0,1]^d)$, and let $Y_i = f(X_i)$ be the corresponding pushforward samples. The Monte Carlo estimate is $\\hat{\\mu} = \\frac{1}{n} \\sum_{i=1}^n Y_i$.\n\nThe function $f$ is bounded, so there exist constants $a$ and $b$ such that $a \\le f(x) \\le b$ for all $x \\in [0,1]^d$. Consequently, the random variable $Y = f(X)$ has its support contained within the interval $[a, b]$.\n\nThe derivation proceeds from the fundamental relationship between the expectation of a random variable and its cumulative distribution function (CDF). For any random variable $Y$ with support in $[a,b]$, its expectation can be written as:\n$$ \\mu^\\star = \\mathbb{E}[Y] = a + \\int_a^b (1 - F_Y(t)) dt $$\nwhere $F_Y(t) = P(Y \\le t)$ is the true CDF of $Y$. This identity is derived from integration by parts of $\\mathbb{E}[Y] = \\int_a^b t dF_Y(t)$ and holds for any random variable with bounded support, regardless of whether its CDF is continuous.\n\nThe Monte Carlo estimate $\\hat{\\mu}$ is the expectation with respect to the empirical measure of the sample $\\{Y_1, \\dots, Y_n\\}$. The empirical cumulative distribution function (ECDF) is defined as $F_n(t) = \\frac{1}{n}\\sum_{i=1}^n \\mathbf{1}\\{Y_i \\le t\\}$. The same identity relating expectation to the CDF can be applied to the empirical measure, yielding an exact expression for the sample mean:\n$$ \\hat{\\mu} = \\mathbb{E}_{F_n}[Y] = a + \\int_a^b (1 - F_n(t)) dt $$\n\nThe error of the Monte Carlo estimate is the difference between $\\hat{\\mu}$ and $\\mu^\\star$. Subtracting the two expectation identities gives:\n$$ \\hat{\\mu} - \\mu^\\star = \\left(a + \\int_a^b (1 - F_n(t)) dt\\right) - \\left(a + \\int_a^b (1 - F_Y(t)) dt\\right) = \\int_a^b (F_Y(t) - F_n(t)) dt $$\nTo obtain a bound on the error, we take the absolute value:\n$$ |\\hat{\\mu} - \\mu^\\star| = \\left| \\int_a^b (F_Y(t) - F_n(t)) dt \\right| \\le \\int_a^b |F_Y(t) - F_n(t)| dt $$\nThe term $|F_Y(t) - F_n(t)|$ is the pointwise difference between the true CDF and the ECDF. This difference is uniformly bounded by the Kolmogorov–Smirnov discrepancy, $D_n = \\sup_{t \\in \\mathbb{R}} |F_Y(t) - F_n(t)|$.\nApplying this uniform bound to the integral, we get:\n$$ |\\hat{\\mu} - \\mu^\\star| \\le \\int_a^b D_n dt = D_n (b-a) $$\nThis inequality connects the integration error directly to the discrepancy of the one-dimensional pushforward samples, avoiding any dependence on the multi-dimensional structure of $f$ or its Hardy–Krause variation.\n\nTo make this bound useful, we require a high-probability bound on the random variable $D_n$. The Dvoretzky–Kiefer–Wolfowitz (DKW) inequality provides a non-asymptotic, distribution-free bound on the probability that $D_n$ exceeds some value $\\epsilon$. For any $\\epsilon > 0$, the inequality states:\n$$ P(D_n > \\epsilon) \\le 2e^{-2n\\epsilon^2} $$\nWe want to construct a confidence interval for $\\mu^\\star$ with a confidence level of $1-\\alpha$. We can set the upper bound on the probability of a large deviation to $\\alpha$:\n$$ \\alpha = 2e^{-2n\\epsilon^2} $$\nSolving for $\\epsilon$ gives the critical value, which we denote $\\epsilon_{n, \\alpha}$:\n$$ \\ln\\left(\\frac{\\alpha}{2}\\right) = -2n\\epsilon^2 \\implies \\epsilon_{n, \\alpha} = \\sqrt{\\frac{1}{2n}\\ln\\left(\\frac{2}{\\alpha}\\right)} $$\nThe DKW inequality implies that with probability at least $1-\\alpha$, the event $D_n \\le \\epsilon_{n, \\alpha}$ occurs.\n\nCombining these results, we can state that with probability at least $1-\\alpha$:\n$$ |\\mu^\\star - \\hat{\\mu}| \\le (b-a) D_n \\le (b-a) \\epsilon_{n, \\alpha} $$\nThis inequality defines a symmetric confidence interval for $\\mu^\\star$ around the estimate $\\hat{\\mu}$. The lower bound $\\mathrm{L}$ and upper bound $\\mathrm{U}$ of this confidence interval are:\n$$ \\mathrm{L} = \\hat{\\mu} - (b-a)\\sqrt{\\frac{1}{2n}\\ln\\left(\\frac{2}{\\alpha}\\right)} $$\n$$ \\mathrm{U} = \\hat{\\mu} + (b-a)\\sqrt{\\frac{1}{2n}\\ln\\left(\\frac{2}{\\alpha}\\right)} $$\nThis confidence interval is valid for any bounded function $f$, including discontinuous ones for which the Hardy–Krause variation $V_{\\mathrm{HK}}(f)$ is infinite. For the test cases provided, $f$ is an indicator function, so its output $Y$ is a Bernoulli random variable with values in $\\{0, 1\\}$. Thus, the bounds are $a=0$ and $b=1$, and the term $(b-a)$ simplifies to $1$. The sample mean $\\hat{\\mu}$ is the proportion of samples for which $f(X_i)=1$. This rigorous, distribution-free method provides a practical way to assess Monte Carlo integration error in challenging scenarios.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes discrepancy-aware confidence intervals for Monte Carlo estimates\n    of integrals for several discontinuous functions.\n    \"\"\"\n    test_cases = [\n        {\n            \"d\": 2,\n            \"f_params\": {\"c\": np.array([0.5, 0.5]), \"r\": 0.3},\n            \"f_type\": \"disk\",\n            \"mu_star\": np.pi * 0.3**2,\n            \"n\": 4096,\n            \"alpha\": 0.05,\n            \"seed\": 12345,\n        },\n        {\n            \"d\": 3,\n            \"f_params\": {\"c\": np.array([0.5, 0.5, 0.5]), \"r\": 0.3},\n            \"f_type\": \"ball\",\n            \"mu_star\": (4/3) * np.pi * 0.3**3,\n            \"n\": 8192,\n            \"alpha\": 0.01,\n            \"seed\": 67890,\n        },\n        {\n            \"d\": 2,\n            \"f_params\": {\"c\": np.array([0.5, 0.5]), \"r1\": 0.2, \"r2\": 0.4},\n            \"f_type\": \"annulus\",\n            \"mu_star\": np.pi * (0.4**2 - 0.2**2),\n            \"n\": 64,\n            \"alpha\": 0.2,\n            \"seed\": 24680,\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        d = case[\"d\"]\n        n = case[\"n\"]\n        alpha = case[\"alpha\"]\n        seed = case[\"seed\"]\n        f_params = case[\"f_params\"]\n        f_type = case[\"f_type\"]\n        mu_star = case[\"mu_star\"]\n\n        # Set the random seed for reproducibility.\n        rng = np.random.default_rng(seed)\n\n        # Generate n i.i.d. points in the d-dimensional unit cube.\n        X = rng.random((n, d))\n        \n        # Evaluate the function f(x) on the sample points to get Y_i.\n        # Since f is an indicator function, its bounds are a=0, b=1.\n        if f_type == \"disk\" or f_type == \"ball\":\n            c = f_params[\"c\"]\n            r = f_params[\"r\"]\n            # Compute squared Euclidean distance from the center c.\n            dist_sq = np.sum((X - c)**2, axis=1)\n            Y = (dist_sq <= r**2).astype(float)\n        elif f_type == \"annulus\":\n            c = f_params[\"c\"]\n            r1 = f_params[\"r1\"]\n            r2 = f_params[\"r2\"]\n            dist_sq = np.sum((X - c)**2, axis=1)\n            Y = ((dist_sq >= r1**2) & (dist_sq <= r2**2)).astype(float)\n\n        # Calculate the Monte Carlo estimate (sample mean).\n        mu_hat = np.mean(Y)\n\n        # The function is bounded with a=0 and b=1.\n        a, b = 0.0, 1.0\n\n        # Calculate the error term for the confidence interval based on the DKW inequality.\n        # delta = (b-a) * sqrt( (1/(2n)) * ln(2/alpha) )\n        delta = (b - a) * np.sqrt((1.0 / (2.0 * n)) * np.log(2.0 / alpha))\n\n        # Compute the lower and upper bounds of the confidence interval.\n        L = mu_hat - delta\n        U = mu_hat + delta\n\n        # Check if the true integral value lies within the computed interval.\n        inside = (L <= mu_star <= U)\n\n        # Append the results for this test case.\n        results.append([L, U, mu_star, inside])\n    \n    # Custom string conversion to match required output format.\n    def format_results(data):\n        outer_list = []\n        for sublist in data:\n            inner_list = []\n            for item in sublist:\n                if isinstance(item, bool):\n                    inner_list.append(str(item))\n                else:\n                    inner_list.append(f\"{item:.17f}\") # Use sufficient precision for floats\n            outer_list.append(f\"[{','.join(inner_list)}]\")\n        return f\"[{','.join(outer_list)}]\"\n\n    # The problem asks for the output of `str(list_of_lists)`.\n    # Let's check `','.join(map(str, results))` against this.\n    # str(results) produces \"[[-0.1, ...], ...]\" with spaces.\n    # The prompt's example format is `[[L_1,U_1...], [L_2...]]`, so `','.join(map(str, results))` is correct.\n    # It will produce a string like `'[...], [...]'`, which then gets wrapped in `[` and `]`. Perfect.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3303333"}]}