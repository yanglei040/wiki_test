{"hands_on_practices": [{"introduction": "The cornerstone of assessing MCMC efficiency is the Integrated Autocorrelation Time (IAT), which measures how many correlated samples are equivalent to one independent sample. This first exercise provides a fundamental mathematical workout, asking you to derive the IAT for an idealized process. By calculating the IAT for both an original and a thinned chain under a hypothetical first-order autoregressive model, you will gain a precise understanding of how thinning alters the autocorrelation structure [@problem_id:3357397].", "problem": "Consider a stationary Markov Chain Monte Carlo (MCMC) process $\\{X_{n}\\}_{n \\geq 0}$ targeting an invariant distribution $\\pi$, and a square-integrable function $f$ with $\\mathbb{E}_{\\pi}[f(X_{n})]=\\mu$ and $\\operatorname{Var}_{\\pi}(f(X_{n}))=\\sigma^{2} \\in (0,\\infty)$. Define the centered process $Y_{n}=f(X_{n})-\\mu$, and assume $\\{Y_{n}\\}$ is strictly stationary with autocorrelation function $\\rho_{t}=\\operatorname{Corr}(Y_{0},Y_{t})$. The Markov chain central limit theorem (MC-CLT) states that the variance of the ergodic average $N^{-1}\\sum_{n=1}^{N} f(X_{n})$ inflates by the integrated autocorrelation time (IAT) $\\tau$, which is defined by the convergent series $\\tau=1+2\\sum_{t=1}^{\\infty}\\rho_{t}$ under absolute summability of $\\{\\rho_{t}\\}$.\n\nSuppose $f(X_{n})$ follows a first-order autoregressive correlation model in the sense that $\\rho_{t}=\\phi^{t}$ for some $0<\\phi<1$. Derive the closed-form expression for the integrated autocorrelation time $\\tau$ of the unthinned chain. Next, consider $k$-thinning, which keeps every $k$-th iterate and discards others, producing the thinned series $\\{Y_{0},Y_{k},Y_{2k},\\dots\\}$. Under the same autoregressive correlation model, derive the integrated autocorrelation time $\\tau_{k}$ for the $k$-thinned chain in closed form.\n\nExpress your final answer as analytic expressions for $\\tau$ and $\\tau_{k}$, with no rounding.", "solution": "The problem is first validated to ensure it is scientifically sound, well-posed, and objective.\n\n### Step 1: Extract Givens\n-   A stationary Markov Chain Monte Carlo (MCMC) process $\\{X_{n}\\}_{n \\geq 0}$ with invariant distribution $\\pi$.\n-   A square-integrable function $f$ such that $\\mathbb{E}_{\\pi}[f(X_{n})]=\\mu$ and $\\operatorname{Var}_{\\pi}(f(X_{n}))=\\sigma^{2} \\in (0,\\infty)$.\n-   A centered, strictly stationary process $Y_{n}=f(X_{n})-\\mu$.\n-   The autocorrelation function (ACF) of $\\{Y_{n}\\}$ is $\\rho_{t}=\\operatorname{Corr}(Y_{0},Y_{t})$.\n-   The integrated autocorrelation time (IAT) for the unthinned chain is $\\tau=1+2\\sum_{t=1}^{\\infty}\\rho_{t}$. The series is assumed to be absolutely convergent.\n-   The specific model for the ACF is $\\rho_{t}=\\phi^{t}$ for some constant $\\phi$ where $0<\\phi<1$.\n-   A $k$-thinned chain is formed by taking every $k$-th iterate, yielding the series $\\{Y_{0},Y_{k},Y_{2k},\\dots\\}$.\n-   The task is to derive the closed-form expression for the IAT of the unthinned chain, $\\tau$, and the IAT of the $k$-thinned chain, $\\tau_k$.\n\n### Step 2: Validate Using Extracted Givens\n-   **Scientifically Grounded:** The problem is rooted in the established theory of Markov Chain Monte Carlo methods and time series analysis. The definitions provided for the integrated autocorrelation time, stationary processes, and autocorrelation are standard in statistics. The first-order autoregressive correlation model ($\\rho_{t}=\\phi^{t}$) is a fundamental and widely used model for time series data. The condition $0 < \\phi < 1$ ensures the stationarity of the process and the convergence of the series defining the IAT, which is mathematically essential. The problem is a standard theoretical exercise in this field.\n-   **Well-Posed:** The problem clearly defines all terms and provides a specific model for the autocorrelation function. It asks for the derivation of two specific quantities, $\\tau$ and $\\tau_k$, based on the given information. The problem is self-contained and has a unique, derivable solution.\n-   **Objective:** The problem is stated using precise mathematical and statistical language, with no subjectivity or ambiguity.\n\n### Step 3: Verdict and Action\nThe problem is valid. It is a well-defined theoretical problem in the field of MCMC analysis. A full solution will be provided.\n\n### Derivation of the Integrated Autocorrelation Time (IAT) for the Unthinned Chain\n\nThe integrated autocorrelation time, $\\tau$, is defined as:\n$$\n\\tau = 1 + 2\\sum_{t=1}^{\\infty}\\rho_{t}\n$$\nThe problem specifies that the autocorrelation function follows the model of a first-order autoregressive process, $\\rho_{t} = \\phi^{t}$, for $t \\geq 1$. Substituting this into the definition of $\\tau$, we get:\n$$\n\\tau = 1 + 2\\sum_{t=1}^{\\infty}\\phi^{t}\n$$\nThe summation term is a geometric series $\\sum_{t=1}^{\\infty}\\phi^{t} = \\phi + \\phi^{2} + \\phi^{3} + \\dots$. The sum of this series is known. For any $|r|<1$, the sum of the infinite geometric series $\\sum_{n=1}^{\\infty} ar^{n-1} = a + ar + ar^2 + \\dots$ is $\\frac{a}{1-r}$. In our case, the first term is $a=\\phi$ and the common ratio is $r=\\phi$. Therefore, the sum is:\n$$\n\\sum_{t=1}^{\\infty}\\phi^{t} = \\frac{\\phi}{1-\\phi}\n$$\nThis is valid because the problem states $0 < \\phi < 1$, which satisfies the condition $|\\phi|<1$.\n\nSubstituting this result back into the expression for $\\tau$:\n$$\n\\tau = 1 + 2\\left(\\frac{\\phi}{1-\\phi}\\right)\n$$\nTo simplify, we find a common denominator:\n$$\n\\tau = \\frac{1-\\phi}{1-\\phi} + \\frac{2\\phi}{1-\\phi} = \\frac{1-\\phi+2\\phi}{1-\\phi} = \\frac{1+\\phi}{1-\\phi}\n$$\nThis is the closed-form expression for the IAT of the unthinned chain.\n\n### Derivation of the IAT for the k-Thinned Chain\n\nThe $k$-thinned chain is constructed by keeping every $k$-th sample from the original chain. Let the new thinned process be $\\{Z_{m}\\}_{m \\geq 0}$, where $Z_{m} = Y_{mk}$. The process $\\{Z_{m}\\}$ is also stationary. We need to find its IAT, $\\tau_{k}$.\n\nFirst, we determine the autocorrelation function for the thinned process, which we denote by $\\rho_{m}^{(k)}$.\n$$\n\\rho_{m}^{(k)} = \\operatorname{Corr}(Z_{0}, Z_{m}) = \\operatorname{Corr}(Y_{0}, Y_{mk})\n$$\nBy the definition of the ACF for the original process, $\\operatorname{Corr}(Y_{0}, Y_{mk}) = \\rho_{mk}$.\nUsing the given model $\\rho_{t} = \\phi^{t}$, we find the ACF for the thinned process:\n$$\n\\rho_{m}^{(k)} = \\rho_{mk} = \\phi^{mk} = (\\phi^{k})^{m}\n$$\nThe IAT for the thinned chain, $\\tau_{k}$, is defined analogously to $\\tau$:\n$$\n\\tau_{k} = 1 + 2\\sum_{m=1}^{\\infty}\\rho_{m}^{(k)}\n$$\nSubstituting the expression for $\\rho_{m}^{(k)}$:\n$$\n\\tau_{k} = 1 + 2\\sum_{m=1}^{\\infty}(\\phi^{k})^{m}\n$$\nThis sum is again a geometric series, but with a common ratio of $\\phi^{k}$. Since $0 < \\phi < 1$ and $k$ is an integer (presumably $k \\geq 1$), we have $0 < \\phi^{k} < 1$. Thus, the series converges.\nThe sum of this geometric series is:\n$$\n\\sum_{m=1}^{\\infty}(\\phi^{k})^{m} = \\frac{\\phi^{k}}{1-\\phi^{k}}\n$$\nSubstituting this back into the expression for $\\tau_{k}$:\n$$\n\\tau_{k} = 1 + 2\\left(\\frac{\\phi^{k}}{1-\\phi^{k}}\\right)\n$$\nSimplifying by finding a common denominator:\n$$\n\\tau_{k} = \\frac{1-\\phi^{k}}{1-\\phi^{k}} + \\frac{2\\phi^{k}}{1-\\phi^{k}} = \\frac{1-\\phi^{k}+2\\phi^{k}}{1-\\phi^{k}} = \\frac{1+\\phi^{k}}{1-\\phi^{k}}\n$$\nThis is the closed-form expression for the IAT of the $k$-thinned chain.\n\nThe final results are the expressions for $\\tau$ and $\\tau_{k}$.\nFor the unthinned chain: $\\tau = \\frac{1+\\phi}{1-\\phi}$.\nFor the $k$-thinned chain: $\\tau_{k} = \\frac{1+\\phi^{k}}{1-\\phi^{k}}$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{1+\\phi}{1-\\phi} & \\frac{1+\\phi^k}{1-\\phi^k}\n\\end{pmatrix}\n}\n$$", "id": "3357397"}, {"introduction": "While the previous exercise explored the IAT of the *retained* samples, a more practical question is how thinning affects the overall computational efficiency for a fixed budget of MCMC iterations. This practice delves into a more realistic high-dimensional scenario involving a random-walk Metropolis-Hastings algorithm. By deriving a formal efficiency metric and finding the thinning interval that maximizes it, you will directly confront the question of whether discarding samples can ever be beneficial, leading to a crucial conclusion about statistical efficiency [@problem_id:3357394].", "problem": "Consider a $d$-dimensional Gaussian target distribution $\\pi(\\mathbf{x}) \\propto \\exp\\!\\big(-\\|\\mathbf{x}\\|^{2}/(2\\tau^{2})\\big)$ with scale $\\tau > 0$, and a random-walk Metropolis–Hastings algorithm, a method within Markov chain Monte Carlo (MCMC), that proposes $\\mathbf{y} \\sim \\mathcal{N}(\\mathbf{x}, s^{2}\\mathbf{I}_{d})$ from the current state $\\mathbf{x}$. Suppose the practitioner chooses the proposal scale using a proxy $\\tau_{\\text{prop}}$ not equal to the true $\\tau$, so that $s^{2} = (l^{2}\\tau_{\\text{prop}}^{2})/d$ for some dimensionless step-size parameter $l > 0$, inducing a scale mismatch ratio $c = \\tau_{\\text{prop}}/\\tau$. Assume the high-dimensional product structure and regularity conditions under which the random-walk Metropolis–Hastings chain admits a diffusion limit for any fixed coordinate as $d \\to \\infty$.\n\nStarting from the fundamental definitions of the Metropolis–Hastings acceptance rule for a symmetric proposal and the diffusion limit for product targets, perform the following:\n\n- Derive how the proposal scale $s$ (through $l$ and $c$) controls the first-coordinate autocorrelation function $\\rho_{t}$ at lag $t \\in \\mathbb{N}$ under the diffusion approximation for large $d$. Your derivation must begin from the acceptance probability definition and the generator form of the diffusion limit for random-walk Metropolis, and conclude with a closed-form expression for $\\rho_{t}$ as a function of $l$, $c$, $\\tau$, and $d$.\n- Define thinning by a positive integer factor $m \\in \\mathbb{N}$ as keeping every $m$-th iterate and discarding the rest. Let the computational efficiency per saved sample be defined as\n$$\nE(m) \\;=\\; \\frac{1/m}{1 \\;+\\; 2\\sum_{t=1}^{\\infty} \\rho_{tm}} \\, ,\n$$\nwhich is the reciprocal of the integrated autocorrelation time per saved sample, scaled by the cost of $m$ steps per saved iterate. Using your derived form of $\\rho_{t}$, obtain a closed-form expression for $E(m)$ and determine the value of $m$ that maximizes $E(m)$ over $m \\in \\mathbb{N}$.\n\nYour final answer must be the maximizing value $m^{\\star}$, as a single number. No rounding is required, and no units are involved. In your derivation, treat all constants symbolically; do not substitute numerical values for any of $l$, $c$, $\\tau$, or $d$.", "solution": "The problem asks for an analysis of thinning an MCMC chain generated by a random-walk Metropolis–Hastings (RWMH) algorithm for a high-dimensional Gaussian target. We are asked to derive the autocorrelation function under a diffusion approximation, and then use it to find the optimal thinning factor $m$ that maximizes a given computational efficiency metric.\n\n### Part 1: Derivation of the Autocorrelation Function $\\rho_t$\n\nThe target distribution is a $d$-dimensional Gaussian $\\pi(\\mathbf{x}) \\propto \\exp(-\\|\\mathbf{x}\\|^{2}/(2\\tau^{2}))$, which corresponds to $\\mathcal{N}(\\mathbf{0}, \\tau^{2}\\mathbf{I}_{d})$. The RWMH proposal is $\\mathbf{y} \\sim \\mathcal{N}(\\mathbf{x}, s^{2}\\mathbf{I}_{d})$, where $\\mathbf{x}$ is the current state. The proposal is symmetric, so the acceptance probability is $\\alpha(\\mathbf{x}, \\mathbf{y}) = \\min\\left(1, \\frac{\\pi(\\mathbf{y})}{\\pi(\\mathbf{x})}\\right)$.\n\nThe log-ratio of the densities is:\n$$ \\ln\\left(\\frac{\\pi(\\mathbf{y})}{\\pi(\\mathbf{x})}\\right) = -\\frac{\\|\\mathbf{y}\\|^{2}}{2\\tau^{2}} - \\left(-\\frac{\\|\\mathbf{x}\\|^{2}}{2\\tau^{2}}\\right) = \\frac{1}{2\\tau^{2}}\\left(\\|\\mathbf{x}\\|^{2} - \\|\\mathbf{y}\\|^{2}\\right) $$\nLet the proposal be written as $\\mathbf{y} = \\mathbf{x} + s\\mathbf{Z}$, where $\\mathbf{Z} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I}_{d})$.\nThen, $\\|\\mathbf{y}\\|^{2} - \\|\\mathbf{x}\\|^{2} = \\|\\mathbf{x} + s\\mathbf{Z}\\|^{2} - \\|\\mathbf{x}\\|^{2} = 2s(\\mathbf{x} \\cdot \\mathbf{Z}) + s^{2}\\|\\mathbf{Z}\\|^{2}$.\nThe log-acceptance ratio is:\n$$ A = -\\frac{1}{2\\tau^{2}}\\left(2s(\\mathbf{x} \\cdot \\mathbf{Z}) + s^{2}\\|\\mathbf{Z}\\|^{2}\\right) = -\\frac{s(\\mathbf{x} \\cdot \\mathbf{Z})}{\\tau^{2}} - \\frac{s^{2}\\|\\mathbf{Z}\\|^{2}}{2\\tau^{2}} $$\nThe proposal variance is given as $s^{2} = (l^{2}\\tau_{\\text{prop}}^{2})/d$. With the scale mismatch ratio $c = \\tau_{\\text{prop}}/\\tau$, we can write $s^{2} = (l^{2}c^{2}\\tau^{2})/d$, so $s = (lc\\tau)/\\sqrt{d}$. Substituting this into the expression for $A$:\n$$ A = -\\frac{lc\\tau}{\\sqrt{d}\\tau^{2}}(\\mathbf{x} \\cdot \\mathbf{Z}) - \\frac{l^{2}c^{2}\\tau^{2}}{d(2\\tau^{2})}\\|\\mathbf{Z}\\|^{2} = -\\frac{lc}{\\sqrt{d}\\tau}(\\mathbf{x} \\cdot \\mathbf{Z}) - \\frac{l^{2}c^{2}}{2d}\\|\\mathbf{Z}\\|^{2} $$\nIn the high-dimensional limit ($d \\to \\infty$), we analyze the behavior of these terms. When the chain is in its stationary distribution $\\pi$, each component $x_i \\sim \\mathcal{N}(0, \\tau^2)$ independently. The term $\\mathbf{x} \\cdot \\mathbf{Z} = \\sum_{i=1}^d x_i Z_i$ is a sum of i.i.d. random variables with mean $0$ and variance $E[x_i^2]E[Z_i^2] = \\tau^2 \\cdot 1 = \\tau^2$. Thus, by the Central Limit Theorem, $(\\mathbf{x} \\cdot \\mathbf{Z}) / \\sqrt{d\\tau^2}$ converges in distribution to a standard normal variable $W \\sim \\mathcal{N}(0,1)$. The first term in $A$ becomes:\n$$ -\\frac{lc}{\\sqrt{d}\\tau}(\\mathbf{x} \\cdot \\mathbf{Z}) = -lc \\left(\\frac{\\mathbf{x} \\cdot \\mathbf{Z}}{\\sqrt{d}\\tau}\\right) \\xrightarrow{d} -lcW $$\nFor the second term, by the Law of Large Numbers, $\\|\\mathbf{Z}\\|^{2}/d = \\frac{1}{d}\\sum_{i=1}^d Z_i^2 \\xrightarrow{p} E[Z_1^2] = 1$.\nSo, in the limit $d \\to \\infty$, the log-acceptance ratio $A$ converges in distribution to a random variable $V$:\n$$ V = -lcW - \\frac{l^{2}c^{2}}{2} $$\nwhere $W \\sim \\mathcal{N}(0,1)$. $V$ is a Gaussian random variable with mean $E[V] = -l^{2}c^{2}/2$ and variance $\\text{Var}(V) = (-lc)^{2}\\text{Var}(W) = l^{2}c^{2}$.\n\nThe limiting average acceptance probability $\\bar{\\alpha}$ is the expectation of $\\min(1, \\exp(V))$:\n$$ \\bar{\\alpha}(l,c) = E[\\min(1, \\exp(V))] = E[\\exp(\\min(0,V))] $$\nFor a normal variable $V \\sim \\mathcal{N}(\\mu, \\sigma^{2})$, this expectation is known to be $\\exp(\\mu+\\sigma^{2}/2)\\Phi(-\\mu/\\sigma - \\sigma) + \\Phi(\\mu/\\sigma)$, where $\\Phi$ is the standard normal CDF. Here, $\\mu = -l^{2}c^{2}/2$ and $\\sigma = lc$.\n$$ \\bar{\\alpha}(l,c) = \\exp\\left(-\\frac{l^2c^2}{2} + \\frac{l^2c^2}{2}\\right) \\Phi\\left(-\\frac{-l^2c^2/2}{lc} - lc \\right) + \\Phi\\left(\\frac{-l^2c^2/2}{lc}\\right) $$\n$$ \\bar{\\alpha}(l,c) = \\Phi\\left(\\frac{lc}{2} - lc\\right) + \\Phi\\left(-\\frac{lc}{2}\\right) = \\Phi\\left(-\\frac{lc}{2}\\right) + \\Phi\\left(-\\frac{lc}{2}\\right) = 2\\Phi(-lc/2) $$\nUnder the specified regularity conditions, the process for any single coordinate, scaled in time as $u = k/d$ (where $k$ is the MCMC iteration number), converges weakly to an Ornstein-Uhlenbeck (OU) process as $d \\to \\infty$. The SDE for the vector process $\\mathbf{X}(u)$ is:\n$$ d\\mathbf{X}(u) = -\\frac{h}{2\\tau^2}\\mathbf{X}(u)du + \\sqrt{h} d\\mathbf{W}(u) $$\nwhere $h = \\lim_{d\\to\\infty} d s^2 \\bar{\\alpha}$ is the limiting diffusion constant and $\\mathbf{W}(u)$ is a standard $d$-dimensional Wiener process.\nSubstituting the expressions for $s^2$ and $\\bar{\\alpha}$:\n$$ h = \\lim_{d\\to\\infty} d \\left(\\frac{l^2c^2\\tau^2}{d}\\right) \\left(2\\Phi(-lc/2)\\right) = 2l^2c^2\\tau^2\\Phi(-lc/2) $$\nThe SDE for a single coordinate $X_i(u)$ is:\n$$ dX_i(u) = -\\Lambda X_i(u)du + \\sqrt{2\\Lambda\\tau^2}dW_i(u) $$\nwhere $\\Lambda = h/(2\\tau^2)$ is the rate parameter.\n$$ \\Lambda = \\frac{2l^2c^2\\tau^2\\Phi(-lc/2)}{2\\tau^2} = l^2c^2\\Phi(-lc/2) $$\nThe autocorrelation function of an OU process with rate $\\Lambda$ at a time lag of $\\Delta u$ is $\\exp(-\\Lambda \\Delta u)$. The discrete MCMC chain has a lag of $t$ steps. In the continuous-time scaling, this corresponds to a time lag of $\\Delta u = t/d$. Thus, the autocorrelation at lag $t$ for the original MCMC chain is:\n$$ \\rho_t = \\exp(-\\Lambda t/d) = \\exp\\left(-\\frac{t}{d} l^2c^2\\Phi(-lc/2)\\right) $$\nThis expression for $\\rho_t$ depends on $l$, $c$, and $d$, but not on $\\tau$, as the target scale $\\tau$ cancels out in the rate parameter $\\Lambda$.\n\n### Part 2: Maximization of Computational Efficiency $E(m)$\n\nThe computational efficiency is given by:\n$$ E(m) = \\frac{1/m}{1 + 2\\sum_{t=1}^{\\infty} \\rho_{tm}} $$\nThe autocorrelation function is of the form $\\rho_t = r^t$, where $r = \\exp(-\\gamma)$ and $\\gamma = \\frac{1}{d} l^2c^2\\Phi(-lc/2)$.\nThe sum in the denominator is a geometric series:\n$$ \\sum_{t=1}^{\\infty} \\rho_{tm} = \\sum_{t=1}^{\\infty} (r^m)^t = \\frac{r^m}{1-r^m} $$\nThe denominator, which is the integrated autocorrelation time of the thinned chain, is:\n$$ 1 + 2\\sum_{t=1}^{\\infty} \\rho_{tm} = 1 + \\frac{2r^m}{1-r^m} = \\frac{1-r^m+2r^m}{1-r^m} = \\frac{1+r^m}{1-r^m} $$\nSubstituting this back into the expression for $E(m)$:\n$$ E(m) = \\frac{1/m}{(1+r^m)/(1-r^m)} = \\frac{1 - r^m}{m(1 + r^m)} $$\nLet $x = \\gamma m$. Since $l, c > 0$, $\\Phi(-lc/2) > 0$, so $\\gamma > 0$. For $m \\in \\mathbb{N}$, $x > 0$.\n$$ E(m) = \\frac{1 - \\exp(-\\gamma m)}{m(1 + \\exp(-\\gamma m))} = \\frac{\\gamma(1 - \\exp(-x))}{x(1 + \\exp(-x))} $$\nTo maximize $E(m)$ with respect to $m \\in \\mathbb{N}$, we analyze the function $g(x) = \\frac{1 - \\exp(-x)}{x(1 + \\exp(-x))}$ for $x>0$.\nWe can rewrite $g(x)$ using the hyperbolic tangent function, $\\tanh(z) = \\frac{\\exp(z)-\\exp(-z)}{\\exp(z)+\\exp(-z)} = \\frac{1-\\exp(-2z)}{1+\\exp(-2z)}$:\n$$ g(x) = \\frac{1}{x} \\frac{\\exp(x/2)(1 - \\exp(-x))}{\\exp(x/2)(1 + \\exp(-x))} = \\frac{1}{x} \\frac{\\exp(x/2)-\\exp(-x/2)}{\\exp(x/2)+\\exp(-x/2)} = \\frac{\\tanh(x/2)}{x} $$\nWe find the derivative of $g(x)$ with respect to $x$:\n$$ g'(x) = \\frac{\\frac{1}{2}\\text{sech}^2(x/2) \\cdot x - \\tanh(x/2) \\cdot 1}{x^2} = \\frac{1}{x^2} \\left(\\frac{x}{2\\cosh^2(x/2)} - \\frac{\\sinh(x/2)}{\\cosh(x/2)}\\right) $$\n$$ g'(x) = \\frac{1}{x^2\\cosh^2(x/2)} \\left(\\frac{x}{2} - \\sinh(x/2)\\cosh(x/2)\\right) $$\nUsing the identity $\\sinh(z)\\cosh(z) = \\frac{1}{2}\\sinh(2z)$:\n$$ g'(x) = \\frac{1}{x^2\\cosh^2(x/2)} \\left(\\frac{x}{2} - \\frac{1}{2}\\sinh(x)\\right) = \\frac{1}{2x^2\\cosh^2(x/2)} (x - \\sinh(x)) $$\nFor any real $x>0$, it is a standard result that $\\sinh(x) > x$. This can be shown by considering the function $h(x) = \\sinh(x) - x$, for which $h(0)=0$ and $h'(x) = \\cosh(x) - 1 > 0$ for $x>0$.\nTherefore, for $x>0$, the term $(x - \\sinh(x))$ is strictly negative. The pre-factor $\\frac{1}{2x^2\\cosh^2(x/2)}$ is strictly positive.\nThis implies that $g'(x) < 0$ for all $x > 0$.\nThe function $g(x)$ is strictly decreasing for $x > 0$. Since $x = \\gamma m$ and $\\gamma > 0$, the efficiency $E(m)$ is a strictly decreasing function of $m$ for $m \\ge 1$.\n\nWe are asked to maximize $E(m)$ over the set of positive integers $m \\in \\mathbb{N} = \\{1, 2, 3, \\dots\\}$. For a strictly decreasing function defined on the positive integers, the maximum value is attained at the smallest possible integer.\nTherefore, the value of $m$ that maximizes $E(m)$ is $m=1$. This implies that no thinning is the optimal strategy for maximizing this definition of computational efficiency.", "answer": "$$\\boxed{1}$$", "id": "3357394"}, {"introduction": "The mathematical conclusion that thinning reduces statistical efficiency often clashes with the visual intuition that thinned trace plots look \"better\". This final practice challenges you to synthesize your understanding and establish a robust diagnostic protocol. By analyzing the trade-offs between visual appearance, statistical efficiency, and the risk of misinterpretation, you will learn to identify flawed diagnostic procedures and adopt best practices for reliable MCMC convergence assessment [@problem_id:3357343].", "problem": "Consider a time-homogeneous Markov chain Monte Carlo (MCMC) algorithm producing a stationary, ergodic Markov chain $\\{X_n\\}_{n \\geq 0}$ with stationary distribution $\\pi$. For a square-integrable function $g$ with respect to $\\pi$, let $\\mu = \\mathbb{E}_{\\pi}[g(X)]$, and define the autocovariance function $\\gamma_k = \\mathrm{Cov}_{\\pi}(g(X_0), g(X_k))$ and autocorrelation function $\\rho_k = \\gamma_k / \\gamma_0$ for $k \\geq 0$. The Markov chain central limit theorem states that under appropriate mixing conditions,\n$$\n\\sqrt{n}\\,(\\bar{g}_n - \\mu) \\xrightarrow{d} \\mathcal{N}\\big(0,\\, \\sigma^2 \\tau_{\\mathrm{int}}\\big),\n$$\nwhere $\\bar{g}_n = \\frac{1}{n} \\sum_{i=1}^{n} g(X_i)$, $\\sigma^2 = \\gamma_0$, and the integrated autocorrelation time is\n$$\n\\tau_{\\mathrm{int}} = 1 + 2 \\sum_{k=1}^{\\infty} \\rho_k,\n$$\nassumed finite. The effective sample size is defined by $\\mathrm{ESS} = n / \\tau_{\\mathrm{int}}$.\n\nThinning by an integer factor $t \\geq 1$ produces the subsequence $Y_j = X_{j t}$, $j = 1, 2, \\dots$, with the same marginal $\\pi$ but altered dependence: its lag-$k$ autocorrelation is $\\rho^{(t)}_k = \\rho_{k t}$ and its integrated autocorrelation time is $\\tau_{\\mathrm{int}}^{(t)} = 1 + 2 \\sum_{k=1}^{\\infty} \\rho_{k t}$.\n\nAddress the following in a unified analysis:\n\n- Starting from the definitions above, explain why visual trace plots of $\\{Y_j\\}$ may appear “better” (less sticky or more rapidly wandering) than those of $\\{X_n\\}$ when $t$ is large, even though the underlying mixing of $\\{X_n\\}$ has not changed.\n- Using the Markov chain central limit theorem and the dependence structure of thinning, analyze the effect of thinning on the effective sample size per unit computational budget. In particular, assume a geometric autocorrelation model $\\rho_k = \\lambda^k$ with $0 \\leq \\lambda < 1$ and fixed total number $n$ of transition steps computed. Derive the ratio\n$$\nR(t, \\lambda) = \\frac{\\mathrm{ESS}_{\\mathrm{thin}}}{\\mathrm{ESS}_{\\mathrm{full}}}\n$$\nand determine whether thinning can improve $\\mathrm{ESS}$ per unit computational budget under this model.\n- Based on your analysis, select the best protocol that avoids misinterpretation caused by thinning when performing convergence and efficiency diagnostics.\n\nChoose the single best option.\n\nA. Provide thinned trace plots with a large thinning interval $t$ so that adjacent samples have autocorrelation approximately $0$; treat the kept samples as independent and compute all diagnostics (autocorrelation function, integrated autocorrelation time, effective sample size, and potential scale reduction factor) on the thinned output only. If the thinned trace appears “flat,” declare convergence.\n\nB. Recognize that thinning reduces adjacent autocorrelation from $\\rho_1$ to $\\rho_t$, which can make trace plots look visually improved, but does not change the underlying mixing of the chain. Compute autocorrelation functions, integrated autocorrelation time using a consistent estimator, and effective sample size from the unthinned chains; use multiple independent chains to compute the potential scale reduction factor; when displaying trace plots, prefer unthinned traces or supplement thinned plots with batch-means overlays and annotate with effective sample size computed from the raw chain. Do not base convergence decisions on thinned trace appearance.\n\nC. Because thinning reduces autocorrelation, it always increases effective sample size approximately linearly in $t$. Therefore, aggressively thin before any diagnostics, compute effective sample size from the count of kept samples, and rely primarily on the visual independence of the thinned trace to judge convergence.\n\nD. Thin until the lag-$1$ sample autocorrelation is below a chosen threshold, then compute the effective sample size and integrated autocorrelation time on the thinned output; to avoid pessimism from autocorrelation, compute the potential scale reduction factor using only the thinned chains, and accept convergence when the thinned trace looks less sticky. This protocol reduces estimator bias and increases precision compared to using the raw chain.", "solution": "The user has provided a problem statement concerning the practice of thinning in Markov chain Monte Carlo (MCMC) methods. I will first validate the problem statement according to the specified criteria.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- A time-homogeneous, stationary, ergodic Markov chain $\\{X_n\\}_{n \\geq 0}$ with stationary distribution $\\pi$.\n- A square-integrable function $g$ with mean $\\mu = \\mathbb{E}_{\\pi}[g(X)]$.\n- Autocovariance function $\\gamma_k = \\mathrm{Cov}_{\\pi}(g(X_0), g(X_k))$.\n- Autocorrelation function $\\rho_k = \\gamma_k / \\gamma_0$ for $k \\geq 0$.\n- Markov chain central limit theorem: $\\sqrt{n}\\,(\\bar{g}_n - \\mu) \\xrightarrow{d} \\mathcal{N}\\big(0,\\, \\sigma^2 \\tau_{\\mathrm{int}}\\big)$, where $\\bar{g}_n = \\frac{1}{n} \\sum_{i=1}^{n} g(X_i)$ and $\\sigma^2 = \\gamma_0$.\n- Integrated autocorrelation time (IAT): $\\tau_{\\mathrm{int}} = 1 + 2 \\sum_{k=1}^{\\infty} \\rho_k$, assumed finite.\n- Effective sample size (ESS): $\\mathrm{ESS} = n / \\tau_{\\mathrm{int}}$.\n- Thinning by an integer factor $t \\geq 1$ produces a subsequence $Y_j = X_{j t}$.\n- The thinned chain has $n' = \\lfloor n/t \\rfloor$ samples, marginal distribution $\\pi$, lag-$k$ autocorrelation $\\rho^{(t)}_k = \\rho_{k t}$, and IAT $\\tau_{\\mathrm{int}}^{(t)} = 1 + 2 \\sum_{k=1}^{\\infty} \\rho_{k t}$.\n- The analysis assumes a geometric autocorrelation model $\\rho_k = \\lambda^k$ with $0 \\leq \\lambda < 1$.\n- The goal is a unified analysis of: (1) the visual appearance of thinned trace plots, (2) the effect of thinning on ESS per computational budget by deriving $R(t, \\lambda) = \\mathrm{ESS}_{\\mathrm{thin}}/\\mathrm{ESS}_{\\mathrm{full}}$, and (3) the best protocol for diagnostics.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem statement is based on standard, well-established theory of MCMC methods. All definitions—stationarity, ergodicity, autocorrelation, integrated autocorrelation time, effective sample size, and the Markov chain CLT—are fundamental concepts in this field. The model of geometric autocorrelation decay is a common and useful theoretical tool for analysis. The problem is scientifically sound.\n- **Well-Posed:** The questions posed are precise and admit a definite answer based on the provided definitions. The derivation of the ratio $R(t, \\lambda)$ is a clear-cut mathematical exercise, and its analysis directly informs the recommendation for a diagnostic protocol. The problem is well-posed.\n- **Objective:** The language is formal, mathematical, and free of subjective claims. It poses an objective question about the mathematical and statistical consequences of a specific procedure (thinning).\n\n**Step 3: Verdict and Action**\n- **Verdict:** The problem is valid. It is scientifically sound, well-posed, and objective.\n- **Action:** I will proceed with the unified analysis and evaluation of the options.\n\n### Unified Analysis\n\nThe problem requires a three-part analysis.\n\n**1. Visual Appearance of Thinned Trace Plots**\n\nA trace plot of an MCMC chain typically displays the values of a quantity of interest, $g(X_n)$, versus the iteration number $n$. High positive autocorrelation, especially at lag $k=1$, means that successive samples $g(X_n)$ and $g(X_{n+1})$ tend to be close to each other. This causes the trace plot to appear \"sticky\" or to meander slowly, which is a visual sign of slow mixing. The lag-$1$ autocorrelation of the original chain $\\{X_n\\}$ is $\\rho_1$.\n\nThinning by a factor $t$ creates a new sequence $\\{Y_j\\}$ where $Y_j = X_{jt}$. When plotting the trace of $\\{Y_j\\}$, we plot $g(Y_j)$ against the new index $j$. The lag-$1$ autocorrelation of this new sequence is $\\rho^{(t)}_1 = \\rho_{1 \\cdot t} = \\rho_t$. For any MCMC chain that is not perfectly mixing in one step (i.e., $\\rho_k \\neq 0$ for some $k \\geq 1$), and assuming positive correlations that decay with lag, we have $\\rho_t < \\rho_1$ for $t > 1$. If the thinning interval $t$ is chosen to be large, $\\rho_t$ will be close to $0$.\n\nConsequently, adjacent points in the thinned trace plot, $g(Y_j)$ and $g(Y_{j+1})$, will be nearly uncorrelated. A plot of nearly uncorrelated samples from the stationary distribution $\\pi$ will resemble white noise, appearing to wander rapidly and cover the posterior landscape efficiently. This visual impression of \"better\" mixing is an artifact. The underlying MCMC algorithm's mixing properties have not changed; we have simply omitted the intermediate, highly correlated states, creating a visual illusion of independence. The actual time (in terms of computational steps) between $g(Y_j)$ and $g(Y_{j+1})$ is $t$ steps of the original, slow-mixing chain.\n\n**2. Effect of Thinning on Effective Sample Size (ESS)**\n\nWe analyze the effect of thinning on the statistical efficiency of the MCMC estimate for a fixed computational budget, which corresponds to generating a total of $n$ MCMC samples. The efficiency is measured by the Effective Sample Size (ESS). For simplicity, let $n$ be an integer multiple of $t$, so the thinned chain has exactly $n' = n/t$ samples.\n\nThe ESS for the full chain is $\\mathrm{ESS}_{\\mathrm{full}} = n / \\tau_{\\mathrm{int}}$.\nThe ESS for the thinned chain is $\\mathrm{ESS}_{\\mathrm{thin}} = n' / \\tau_{\\mathrm{int}}^{(t)} = (n/t) / \\tau_{\\mathrm{int}}^{(t)}$.\n\nWe now use the geometric autocorrelation model, $\\rho_k = \\lambda^k$, for $0 \\leq \\lambda < 1$.\n\nFirst, we calculate the IAT for the full chain:\n$$ \\tau_{\\mathrm{int}} = 1 + 2 \\sum_{k=1}^{\\infty} \\rho_k = 1 + 2 \\sum_{k=1}^{\\infty} \\lambda^k = 1 + 2 \\left( \\frac{\\lambda}{1-\\lambda} \\right) = \\frac{1-\\lambda+2\\lambda}{1-\\lambda} = \\frac{1+\\lambda}{1-\\lambda} $$\nThus, $\\mathrm{ESS}_{\\mathrm{full}} = n \\left( \\frac{1-\\lambda}{1+\\lambda} \\right)$.\n\nNext, we calculate the IAT for the thinned chain. The autocorrelation is $\\rho^{(t)}_k = \\rho_{kt} = (\\lambda^t)^k$.\n$$ \\tau_{\\mathrm{int}}^{(t)} = 1 + 2 \\sum_{k=1}^{\\infty} \\rho_{kt} = 1 + 2 \\sum_{k=1}^{\\infty} (\\lambda^t)^k = 1 + 2 \\left( \\frac{\\lambda^t}{1-\\lambda^t} \\right) = \\frac{1-\\lambda^t+2\\lambda^t}{1-\\lambda^t} = \\frac{1+\\lambda^t}{1-\\lambda^t} $$\nThus, $\\mathrm{ESS}_{\\mathrm{thin}} = \\frac{n/t}{\\tau_{\\mathrm{int}}^{(t)}} = \\frac{n}{t} \\left( \\frac{1-\\lambda^t}{1+\\lambda^t} \\right)$.\n\nNow, we derive the ratio $R(t, \\lambda)$:\n$$ R(t, \\lambda) = \\frac{\\mathrm{ESS}_{\\mathrm{thin}}}{\\mathrm{ESS}_{\\mathrm{full}}} = \\frac{\\frac{n}{t} \\left( \\frac{1-\\lambda^t}{1+\\lambda^t} \\right)}{n \\left( \\frac{1-\\lambda}{1+\\lambda} \\right)} = \\frac{1}{t} \\frac{(1-\\lambda^t)(1+\\lambda)}{(1+\\lambda^t)(1-\\lambda)} $$\nTo determine if thinning can improve ESS, we must check if $R(t, \\lambda) > 1$ is possible for $t > 1$ and $\\lambda \\in (0,1)$. This is equivalent to asking if the variance of the sample mean from the thinned chain is smaller than that from the full chain. The asymptotic variance of the sample mean is proportional to $\\tau_{\\mathrm{int}}/(\\text{number of samples})$.\nFor the full chain, variance is proportional to $\\tau_{\\mathrm{int}}/n$.\nFor the thinned chain, variance is proportional to $\\tau_{\\mathrm{int}}^{(t)}/n' = \\tau_{\\mathrm{int}}^{(t)}/(n/t) = t \\tau_{\\mathrm{int}}^{(t)}/n$.\nThinning is disadvantageous if $t \\tau_{\\mathrm{int}}^{(t)}/n > \\tau_{\\mathrm{int}}/n$, which simplifies to $t \\tau_{\\mathrm{int}}^{(t)} > \\tau_{\\mathrm{int}}$. Using our derived expressions, this is equivalent to showing:\n$$ t \\left( \\frac{1+\\lambda^t}{1-\\lambda^t} \\right) > \\frac{1+\\lambda}{1-\\lambda} \\quad \\text{for } t > 1, \\lambda \\in (0,1) $$\nLet $h(x) = x \\left( \\frac{1+\\lambda^x}{1-\\lambda^x} \\right)$ for $x \\geq 1$. We have established that $h(1) = \\frac{1+\\lambda}{1-\\lambda}$. A careful analysis of the derivative, $h'(x)$, shows that $h'(x) > 0$ for all $x \\geq 1$ and $\\lambda \\in (0,1)$. Therefore, $h(t)$ is a strictly increasing function of $t$. This implies that for any $t>1$, $h(t) > h(1)$, which proves the inequality above.\nThe conclusion is that $t \\tau_{\\mathrm{int}}^{(t)} > \\tau_{\\mathrm{int}}$, meaning the variance of the estimator from the thinned chain is always larger. This implies that $\\mathrm{ESS}_{\\mathrm{thin}} < \\mathrm{ESS}_{\\mathrm{full}}$ for any $t>1$. Thinning always reduces the statistical efficiency and the effective sample size for a fixed computational budget. It amounts to discarding useful information.\n\n**3. Recommended Diagnostic Protocol**\n\nBased on the above analysis:\n- Thinning produces visually misleading trace plots that suggest better mixing than what has actually been achieved. Decisions based on these plots are unreliable.\n- Thinning is statistically inefficient. It discards samples, leading to a loss of precision in posterior estimates for a given number of MCMC iterations.\n- Therefore, all quantitative diagnostics—including the autocorrelation function (ACF), integrated autocorrelation time (IAT), effective sample size (ESS), and multi-chain diagnostics like the potential scale reduction factor (PSRF, or $\\hat{R}$)—should be computed using the original, unthinned chain. This provides the most accurate and reliable assessment of the sampler's performance and the Monte Carlo error of the estimates.\n- If trace plots are presented, unthinned plots, while \"messy,\" are a more honest representation of the chain's behavior. Techniques like plotting running means or batch means over the unthinned trace can aid visualization without the misleading effect of thinning. If a thinned plot is used for clarity, it must be clearly labeled as such, and any reported efficiency metrics (like ESS) must be derived from the original, unthinned chain.\n\n### Evaluation of Options\n\n**A. Provide thinned trace plots with a large thinning interval $t$ so that adjacent samples have autocorrelation approximately $0$; treat the kept samples as independent and compute all diagnostics (autocorrelation function, integrated autocorrelation time, effective sample size, and potential scale reduction factor) on the thinned output only. If the thinned trace appears “flat,” declare convergence.**\nThis option advises implementing the flawed practices identified in our analysis. It relies on the visual illusion of thinning, wrongly computes diagnostics on the statistically inefficient thinned chain, and incorrectly assumes independence.\n**Verdict: Incorrect.**\n\n**B. Recognize that thinning reduces adjacent autocorrelation from $\\rho_1$ to $\\rho_t$, which can make trace plots look visually improved, but does not change the underlying mixing of the chain. Compute autocorrelation functions, integrated autocorrelation time using a consistent estimator, and effective sample size from the unthinned chains; use multiple independent chains to compute the potential scale reduction factor; when displaying trace plots, prefer unthinned traces or supplement thinned plots with batch-means overlays and annotate with effective sample size computed from the raw chain. Do not base convergence decisions on thinned trace appearance.**\nThis option correctly describes the visual effect of thinning as a potential illusion that does not alter true mixing. It correctly mandates that all key diagnostics (ACF, IAT, ESS, PSRF) be computed from the full, unthinned chain to ensure statistical validity and efficiency. It also recommends responsible visualization practices. This protocol is entirely consistent with our rigorous analysis.\n**Verdict: Correct.**\n\n**C. Because thinning reduces autocorrelation, it always increases effective sample size approximately linearly in $t$. Therefore, aggressively thin before any diagnostics, compute effective sample size from the count of kept samples, and rely primarily on the visual independence of the thinned trace to judge convergence.**\nThis statement makes a factually incorrect claim. Our derivation of $R(t, \\lambda)$ proved that thinning *decreases* the total effective sample size for a fixed computational budget. The premise of this option is false. The subsequent recommendations are consequently flawed.\n**Verdict: Incorrect.**\n\n**D. Thin until the lag-$1$ sample autocorrelation is below a chosen threshold, then compute the effective sample size and integrated autocorrelation time on the thinned output; to avoid pessimism from autocorrelation, compute the potential scale reduction factor using only the thinned chains, and accept convergence when the thinned trace looks less sticky. This protocol reduces estimator bias and increases precision compared to using the raw chain.**\nThis option contains multiple falsehoods. Computing diagnostics on the thinned output gives a deceptively optimistic view of performance. High autocorrelation is not \"pessimism\" but a real property of the chain that correctly reduces the ESS; ignoring it introduces bias in the *assessment* of precision. Our analysis shows that thinning *decreases* precision (i.e., increases variance) for a fixed compute budget.\n**Verdict: Incorrect.**", "answer": "$$\\boxed{B}$$", "id": "3357343"}]}