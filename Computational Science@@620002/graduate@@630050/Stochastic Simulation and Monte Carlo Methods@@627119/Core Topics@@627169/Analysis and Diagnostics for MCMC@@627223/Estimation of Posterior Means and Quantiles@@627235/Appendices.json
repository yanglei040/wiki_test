{"hands_on_practices": [{"introduction": "Markov Chain Monte Carlo (MCMC) methods, such as the Gibbs sampler, are foundational tools for approximating posterior distributions. However, the raw output from an MCMC chain can often be used more efficiently. This practice ([@problem_id:3306479]) delves into the powerful technique of Rao-Blackwellization, which reduces the variance of an estimator by replacing a random variable with its conditional expectation. You will implement a Gibbs sampler for a standard Bayesian model and compare a naive Monte Carlo estimator against its superior Rao-Blackwellized counterpart for both posterior means and quantiles, providing a concrete demonstration of this essential variance reduction strategy.", "problem": "Consider a Bayesian normal location-scale model with conjugate priors. Let data $y = (y_1,\\dots,y_n)$ be conditionally independent and identically distributed given parameter $\\theta = (\\mu,\\sigma^2)$ as $y_i \\mid \\mu,\\sigma^2 \\sim \\mathcal{N}(\\mu,\\sigma^2)$ for all $i \\in \\{1,\\dots,n\\}$. Assume the Normal-Inverse-Gamma prior: $\\mu \\mid \\sigma^2 \\sim \\mathcal{N}(\\mu_0,\\sigma^2/\\kappa_0)$ and $\\sigma^2 \\sim \\mathrm{Inverse\\text{-}Gamma}(a_0,b_0)$, where the Inverse-Gamma density is proportional to $(\\sigma^2)^{-a_0-1}\\exp(-b_0/\\sigma^2)$ on $(0,\\infty)$. The posterior is proper for all finite data.\n\nThe task is to estimate the posterior mean $\\mathbb{E}[g(\\theta)\\mid y]$ for the choice $g(\\theta)=\\mu$, and to estimate the posterior $\\alpha$-quantile $q_\\alpha(g(\\theta)\\mid y)$ for the same $g(\\theta)=\\mu$, using a Gibbs sampler and Rao–Blackwellization via conditional Monte Carlo. Begin from first principles: the factorization of the joint posterior via Bayes’ theorem, the definitions of conditional expectations, and the definition of quantiles as inverses of cumulative distribution functions.\n\nDesign a Gibbs sampler that alternates between sampling from the two full conditional distributions $p(\\mu\\mid \\sigma^2,y)$ and $p(\\sigma^2\\mid \\mu,y)$. Using the sampler, construct the following estimators:\n- A naive posterior mean estimator based on direct Monte Carlo averaging of $\\mu$ draws from the Gibbs chain.\n- A Rao–Blackwellized posterior mean estimator that analytically averages over one conditional by replacing each sampled $\\mu$ with the conditional expectation $\\mathbb{E}[\\mu\\mid \\sigma^2,y]$ before averaging over the chain’s $\\sigma^2$ values.\n- A naive posterior $\\alpha$-quantile estimator based on the sample quantile of the $\\mu$ draws.\n- A conditional Monte Carlo “smoothed” posterior $\\alpha$-quantile estimator obtained by first estimating the marginal posterior cumulative distribution function of $\\mu$ at any threshold $t$ via $\\widehat{F}(t) = \\frac{1}{T}\\sum_{t=1}^T \\mathbb{P}(\\mu\\le t\\mid \\sigma_t^2,y)$, where $\\{\\sigma_t^2\\}$ are $\\sigma^2$ draws from the Gibbs sampler and $T$ is the number of retained iterations, and then numerically inverting $t \\mapsto \\widehat{F}(t)$ to obtain the $\\alpha$-quantile.\n\nExamine, by simulation, whether the Rao–Blackwellized estimator reduces Monte Carlo variance relative to the naive estimator for:\n- The posterior mean.\n- The posterior cumulative distribution function at the true posterior $\\alpha$-quantile.\n- The posterior $\\alpha$-quantile itself.\n\nTo make this testable, use the following test suite of parameter settings. For each test case, the program must:\n- Use the specified data $y$ and hyperparameters $(\\mu_0,\\kappa_0,a_0,b_0)$.\n- Run $R$ independent replications of the Gibbs sampler to empirically estimate variances and mean squared errors across replications. Each replication should use $M$ total iterations with a burn-in of $B$ iterations, then retain the next $T=M-B$ draws.\n- Compute the true posterior $\\alpha$-quantile of $\\mu$ in closed form using the known marginal posterior distribution of $\\mu$ under conjugacy, and evaluate the naive and conditional Monte Carlo estimators relative to this target.\n\nUse the following concrete test cases (all numbers are real scalars):\n\n- Case A:\n  - Data length $n = 20$ and data vector $y = [\\,1.3,\\,0.7,\\,1.8,\\,1.5,\\,0.9,\\,2.1,\\,0.4,\\,1.2,\\,0.8,\\,1.0,\\,1.7,\\,1.1,\\,0.5,\\,1.4,\\,0.6,\\,1.9,\\,0.3,\\,1.6,\\,1.2,\\,0.95\\,]$.\n  - Hyperparameters $(\\mu_0,\\kappa_0,a_0,b_0) = (0,\\,1,\\,2,\\,2)$.\n  - Quantile level $\\alpha = 0.5$.\n\n- Case B:\n  - Data length $n = 3$ and data vector $y = [\\,0.5,\\,1.2,\\,-0.1\\,]$.\n  - Hyperparameters $(\\mu_0,\\kappa_0,a_0,b_0) = (0,\\,1,\\,2,\\,2)$.\n  - Quantile level $\\alpha = 0.9$.\n\n- Case C:\n  - Data length $n = 30$ and data vector $y = [\\,-1.1,\\,-0.7,\\,-1.4,\\,-1.3,\\,-1.0,\\,-0.6,\\,-1.2,\\,-0.8,\\,-1.5,\\,-0.9,\\,-1.3,\\,-1.1,\\,-0.4,\\,-0.7,\\,-1.0,\\,-1.6,\\,-1.3,\\,-0.5,\\,-0.8,\\,-1.2,\\,-1.0,\\,-0.9,\\,-1.4,\\,-1.1,\\,-0.6,\\,-1.2,\\,-0.7,\\,-1.0,\\,-1.3,\\,-0.8\\,]$.\n  - Hyperparameters $(\\mu_0,\\kappa_0,a_0,b_0) = (0,\\,1,\\,2,\\,2)$.\n  - Quantile level $\\alpha = 0.1$.\n\nIn all cases, your program must:\n- Use a Gibbs sampler that draws alternately from $p(\\mu\\mid \\sigma^2,y)$ and $p(\\sigma^2\\mid \\mu,y)$, without introducing any external randomness beyond a pseudorandom generator with fixed seeds to ensure reproducibility.\n- For each case, run $R$ independent replications; in each replication, run $M$ iterations with a burn-in of $B$ and retain $T=M-B$ draws.\n- Across the $R$ replications, compute:\n  - The sample variance of the naive posterior mean estimator of $\\mu$ and the sample variance of the Rao–Blackwellized posterior mean estimator of $\\mu$.\n  - The sample variance across replications of the naive estimator of the posterior cumulative distribution function at the true posterior $\\alpha$-quantile of $\\mu$, and the sample variance across replications of the conditional Monte Carlo estimator of the same quantity at the same threshold.\n  - The sample mean squared error across replications of the naive posterior $\\alpha$-quantile estimator of $\\mu$ and the sample mean squared error across replications of the conditional Monte Carlo smoothed posterior $\\alpha$-quantile estimator of $\\mu$, both relative to the true posterior $\\alpha$-quantile.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain, in order for cases A, B, and C, the following nine boolean entries:\n- For each case, first a boolean indicating whether the Rao–Blackwellized posterior mean estimator has smaller variance than the naive posterior mean estimator across replications.\n- Second, a boolean indicating whether the conditional Monte Carlo estimator of the posterior cumulative distribution function at the true posterior $\\alpha$-quantile has smaller variance than the naive empirical cumulative distribution function estimator at the same threshold across replications.\n- Third, a boolean indicating whether the conditional Monte Carlo smoothed posterior $\\alpha$-quantile estimator has smaller mean squared error than the naive empirical quantile estimator across replications.\n\nThus the required final output must be of the form $[b_1,b_2,b_3,b_4,b_5,b_6,b_7,b_8,b_9]$ with each $b_i \\in \\{\\mathrm{True},\\mathrm{False}\\}$.", "solution": "The user wants a detailed analysis and implementation of Gibbs sampling with Rao-Blackwellization for a Bayesian normal location-scale model. The solution must be derived from first principles, and the performance of standard vs. variance-reduced estimators must be compared via simulation for three specific test cases.\n\n### 1. Bayesian Model and Posterior Distributions\n\nThe model is defined by a normal likelihood for the data $y = (y_1, \\dots, y_n)$ and a conjugate Normal-Inverse-Gamma prior for the parameters $\\theta = (\\mu, \\sigma^2)$.\n\n-   **Likelihood**: The data are conditionally independent and identically distributed, $y_i \\mid \\mu, \\sigma^2 \\sim \\mathcal{N}(\\mu, \\sigma^2)$. The likelihood function is:\n    $$p(y \\mid \\mu, \\sigma^2) = \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - \\mu)^2}{2\\sigma^2}\\right) \\propto (\\sigma^2)^{-n/2} \\exp\\left(-\\frac{1}{2\\sigma^2} \\sum_{i=1}^n (y_i - \\mu)^2\\right)$$\n-   **Prior**: The prior is a Normal-Inverse-Gamma distribution, specified hierarchically as $\\mu \\mid \\sigma^2 \\sim \\mathcal{N}(\\mu_0, \\sigma^2/\\kappa_0)$ and $\\sigma^2 \\sim \\mathrm{Inverse\\text{-}Gamma}(a_0, b_0)$. The joint prior density is:\n    $$p(\\mu, \\sigma^2) = p(\\mu \\mid \\sigma^2) p(\\sigma^2) \\propto (\\sigma^2)^{-1/2} \\exp\\left(-\\frac{\\kappa_0(\\mu - \\mu_0)^2}{2\\sigma^2}\\right) \\cdot (\\sigma^2)^{-a_0-1} \\exp\\left(-\\frac{b_0}{\\sigma^2}\\right)$$\n-   **Posterior**: The joint posterior distribution $p(\\mu, \\sigma^2 \\mid y)$ is proportional to the product of the likelihood and the prior, $p(\\mu, \\sigma^2 \\mid y) \\propto p(y \\mid \\mu, \\sigma^2) p(\\mu, \\sigma^2)$. Due to conjugacy, the posterior is also a Normal-Inverse-Gamma distribution, $p(\\mu, \\sigma^2 \\mid y) \\sim \\mathrm{NIG}(\\mu_n, \\kappa_n, a_n, b_n)$, with updated hyperparameters:\n    -   $\\kappa_n = \\kappa_0 + n$\n    -   $\\mu_n = \\frac{\\kappa_0\\mu_0 + n\\bar{y}}{\\kappa_0 + n}$, where $\\bar{y} = \\frac{1}{n} \\sum_{i=1}^n y_i$\n    -   $a_n = a_0 + \\frac{n}{2}$\n    -   $b_n = b_0 + \\frac{1}{2} \\sum_{i=1}^n (y_i - \\bar{y})^2 + \\frac{n\\kappa_0}{2(\\kappa_0+n)}(\\bar{y} - \\mu_0)^2$\n\n### 2. Gibbs Sampler\n\nThe Gibbs sampler generates a Markov chain whose stationary distribution is the joint posterior $p(\\mu, \\sigma^2 \\mid y)$ by iteratively sampling from the full conditional distributions $p(\\mu \\mid \\sigma^2, y)$ and $p(\\sigma^2 \\mid \\mu, y)$.\n\n-   **Full conditional for $\\mu$**: The distribution of $\\mu$ given $\\sigma^2$ and $y$ is derived from the joint posterior $p(\\mu, \\sigma^2 \\mid y) = p(\\mu \\mid \\sigma^2, y) p(\\sigma^2 \\mid y)$. This implies that $p(\\mu \\mid \\sigma^2, y)$ is the normal part of the joint Normal-Inverse-Gamma posterior.\n    $$ \\mu \\mid \\sigma^2, y \\sim \\mathcal{N}(\\mu_n, \\sigma^2/\\kappa_n) $$\n-   **Full conditional for $\\sigma^2$**: Similarly, the full conditional for $\\sigma^2$ can be derived from the joint posterior. We use $p(\\sigma^2 \\mid \\mu, y) \\propto p(\\mu, \\sigma^2 \\mid y)$. The terms involving $\\sigma^2$ in the NIG posterior density are $(\\sigma^2)^{-a_n-1} \\exp(-b_n/\\sigma^2)$ from the marginal part, and $(\\sigma^2)^{-1/2}\\exp(-\\frac{\\kappa_n(\\mu-\\mu_n)^2}{2\\sigma^2})$ from the conditional part. Combining these yields an Inverse-Gamma distribution.\n    $$ \\sigma^2 \\mid \\mu, y \\sim \\mathrm{Inverse\\text{-}Gamma}\\left(a_n + \\frac{1}{2}, b_n + \\frac{\\kappa_n(\\mu - \\mu_n)^2}{2}\\right) $$\n    This form is computationally efficient as the posterior hyperparameters $(\\mu_n, \\kappa_n, a_n, b_n)$ are computed once.\n\n**Gibbs Sampling Algorithm**:\n1.  Initialize $\\mu^{(0)}$ and $(\\sigma^2)^{(0)}$.\n2.  For $k = 1, 2, \\dots, M$:\n    a. Draw $(\\sigma^2)^{(k)}$ from the distribution $\\mathrm{Inverse\\text{-}Gamma}\\left(a_n + \\frac{1}{2}, b_n + \\frac{\\kappa_n(\\mu^{(k-1)} - \\mu_n)^2}{2}\\right)$.\n    b. Draw $\\mu^{(k)}$ from the distribution $\\mathcal{N}(\\mu_n, (\\sigma^2)^{(k)}/\\kappa_n)$.\n3.  Discard the first $B$ samples as burn-in and retain the subsequent $T=M-B$ samples $\\{(\\mu^{(k)}, (\\sigma^2)^{(k)})\\}_{k=B+1}^M$.\n\n### 3. Estimators for Posterior Mean $\\mathbb{E}[\\mu \\mid y]$\n\nThe true posterior mean is known analytically from the marginal posterior of $\\mu$, which is a Student's t-distribution. The mean of this t-distribution is its location parameter, $\\mu_n$. Thus, $\\mathbb{E}[\\mu \\mid y] = \\mu_n$.\n\n-   **Naive Estimator**: A direct Monte Carlo average of the $\\mu$ samples from the Gibbs chain.\n    $$ \\hat{\\mathbb{E}}[\\mu \\mid y]_{\\text{naive}} = \\frac{1}{T} \\sum_{k=1}^T \\mu^{(k)} $$\n-   **Rao-Blackwellized Estimator**: This estimator leverages the law of total expectation, $\\mathbb{E}[\\mu \\mid y] = \\mathbb{E}_{\\sigma^2|y}[\\mathbb{E}[\\mu \\mid \\sigma^2, y]]$. The Monte Carlo approximation averages the inner conditional expectation over the draws of $\\sigma^2$.\n    $$ \\hat{\\mathbb{E}}[\\mu \\mid y]_{\\text{RB}} = \\frac{1}{T} \\sum_{k=1}^T \\mathbb{E}[\\mu \\mid (\\sigma^2)^{(k)}, y] $$\n    Since we know $\\mathbb{E}[\\mu \\mid \\sigma^2, y] = \\mu_n$, which is a constant independent of $\\sigma^2$, the estimator simplifies to:\n    $$ \\hat{\\mathbb{E}}[\\mu \\mid y]_{\\text{RB}} = \\frac{1}{T} \\sum_{k=1}^T \\mu_n = \\mu_n $$\n    The Rao-Blackwellized estimator is simply the exact posterior mean. Its variance across simulation replications is $0$, while the naive estimator's variance is positive. Therefore, the Rao-Blackwellized estimator will have a strictly smaller variance.\n\n### 4. Estimators for Posterior Quantile $q_\\alpha(\\mu \\mid y)$\n\nFirst, we determine the true quantile for comparison. The marginal posterior distribution of $\\mu$ is a location-scale Student's t-distribution, $\\mu \\mid y \\sim t(\\nu, \\mu_n, s^2)$, with:\n-   Degrees of freedom: $\\nu = 2a_n = 2a_0 + n$\n-   Location: $\\mu_loc = \\mu_n$\n-   Scale: $s = \\sqrt{b_n / (a_n \\kappa_n)}$\n\nThe true $\\alpha$-quantile, $q_\\alpha^*$, is $q_\\alpha^* = \\mu_n + s \\cdot t_{\\nu, \\alpha}$, where $t_{\\nu, \\alpha}$ is the $\\alpha$-quantile of a standard Student's t-distribution with $\\nu$ degrees of freedom.\n\n-   **Naive Estimator**: The sample $\\alpha$-quantile of the collected $\\mu$ draws.\n    $$ \\hat{q}_{\\alpha, \\text{naive}} = \\text{SampleQuantile}(\\{\\mu^{(k)}\\}_{k=1}^T, \\alpha) $$\n-   **Conditional Monte Carlo (Smoothed) Estimator**: This method first estimates the posterior CDF of $\\mu$, $F(t) = \\mathbb{P}(\\mu \\le t \\mid y)$, and then inverts it.\n    The CDF is estimated using Rao-Blackwellization: $F(t) = \\mathbb{E}_{\\sigma^2|y}[\\mathbb{P}(\\mu \\le t \\mid \\sigma^2, y)]$.\n    -   The inner term is the CDF of a normal distribution: $\\mathbb{P}(\\mu \\le t \\mid \\sigma^2, y) = \\Phi\\left(\\frac{t-\\mu_n}{\\sqrt{\\sigma^2/\\kappa_n}}\\right)$, where $\\Phi$ is the standard normal CDF.\n    -   The estimated CDF is: $\\hat{F}(t) = \\frac{1}{T} \\sum_{k=1}^T \\Phi\\left(\\frac{t-\\mu_n}{\\sqrt{(\\sigma^2)^{(k)}/\\kappa_n}}\\right)$.\n    -   The smoothed quantile estimator, $\\hat{q}_{\\alpha, \\text{cmc}}$, is then found by numerically solving the equation $\\hat{F}(t) = \\alpha$ for $t$.\n\n### 5. Estimators for Posterior CDF at $q_\\alpha^*$\n\nTo assess variance reduction on the CDF estimation itself, we evaluate the CDF estimators at the true quantile $q_\\alpha^*$.\n\n-   **Naive Estimator (Empirical CDF)**:\n    $$ \\hat{F}_{\\text{naive}}(q_\\alpha^*) = \\frac{1}{T} \\sum_{k=1}^T \\mathbf{1}_{\\{\\mu^{(k)} \\le q_\\alpha^*\\}}, \\quad \\text{where } \\mathbf{1}_{\\{\\cdot\\}} \\text{ is the indicator function.} $$\n-   **Conditional Monte Carlo Estimator**:\n    $$ \\hat{F}_{\\text{cmc}}(q_\\alpha^*) = \\frac{1}{T} \\sum_{k=1}^T \\Phi\\left(\\frac{q_\\alpha^* - \\mu_n}{\\sqrt{(\\sigma^2)^{(k)}/\\kappa_n}}\\right) $$\nBy the Rao-Blackwell theorem, $\\text{Var}(\\hat{F}_{\\text{cmc}}(q_\\alpha^*)) \\le \\text{Var}(\\hat{F}_{\\text{naive}}(q_\\alpha^*))$ because $\\hat{F}_{\\text{cmc}}$ is formed by averaging the conditional expectation of the random variable $\\mathbf{1}_{\\{\\mu \\le q_\\alpha^*\\}}$.\n\n### 6. Simulation and Comparison Logic\n\nFor each test case, we run $R$ independent replications of the Gibbs sampler. In each replication, we compute the four estimators: $\\hat{\\mathbb{E}}_{\\text{naive}}$, $\\hat{\\mathbb{E}}_{\\text{RB}}$, $\\hat{q}_{\\alpha, \\text{naive}}$, and $\\hat{q}_{\\alpha, \\text{cmc}}$. We also compute the two CDF estimates, $\\hat{F}_{\\text{naive}}(q_\\alpha^*)$ and $\\hat{F}_{\\text{cmc}}(q_\\alpha^*)$.\n\n-   **Comparison 1 (Mean Estimator Variance)**: We compute the sample variance of the $R$ estimates of $\\hat{\\mathbb{E}}_{\\text{naive}}$ and compare it to the sample variance of the $R$ estimates of $\\hat{\\mathbb{E}}_{\\text{RB}}$. Since $\\hat{\\mathbb{E}}_{\\text{RB}}$ is the constant $\\mu_n$, its variance is $0$, which will be less than the positive variance of the naive estimator.\n-   **Comparison 2 (CDF Estimator Variance)**: We compute the sample variance of the $R$ estimates of $\\hat{F}_{\\text{naive}}(q_\\alpha^*)$ and compare it to the sample variance of the $R$ estimates of $\\hat{F}_{\\text{cmc}}(q_\\alpha^*)$. Theory predicts the latter will be smaller.\n-   **Comparison 3 (Quantile Estimator MSE)**: We compute the Mean Squared Error (MSE) for both quantile estimators with respect to the true quantile $q_\\alpha^*$.\n    -   $\\text{MSE}_{\\text{naive}} = \\frac{1}{R} \\sum_{r=1}^R (\\hat{q}_{\\alpha, \\text{naive}}^{(r)} - q_\\alpha^*)^2$\n    -   $\\text{MSE}_{\\text{cmc}} = \\frac{1}{R} \\sum_{r=1}^R (\\hat{q}_{\\alpha, \\text{cmc}}^{(r)} - q_\\alpha^*)^2$\nWe compare these two MSE values. Since the smoothed CDF is a better estimator, its inverse (the quantile estimator) is expected to have lower MSE.\n\nFor all three comparisons, the Rao-Blackwellized / Conditional Monte Carlo method is theoretically guaranteed to reduce variance. Therefore, we expect all resulting booleans to be `True`. The simulation serves as a numerical verification of this principle. The simulation parameters will be set to $R=500$ replications, $M=2500$ total iterations, and a burn-in of $B=500$, yielding $T=2000$ samples per replication.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm, t as student_t\nfrom scipy.optimize import root_scalar\n\ndef run_simulation_for_case(data, mu0, kappa0, a0, b0, alpha, R, M, B, seed):\n    \"\"\"\n    Runs the full simulation for one test case.\n\n    Args:\n        data (np.ndarray): The observed data vector y.\n        mu0, kappa0, a0, b0 (float): Hyperparameters for the prior.\n        alpha (float): The quantile level.\n        R (int): Number of independent replications.\n        M (int): Total number of Gibbs iterations per replication.\n        B (int): Number of burn-in iterations.\n        seed (int): Seed for the main random number generator.\n\n    Returns:\n        tuple[bool, bool, bool]: The three boolean comparison results.\n    \"\"\"\n    n = len(data)\n    y_bar = np.mean(data)\n    sum_sq_dev = np.sum((data - y_bar)**2)\n    T = M - B\n\n    # --- 1. Compute posterior and true analytical values ---\n    # Posterior hyperparameters for NIG(mu_n, kappa_n, a_n, b_n)\n    kappa_n = kappa0 + n\n    mu_n = (kappa0 * mu0 + n * y_bar) / kappa_n\n    a_n = a0 + n / 2.0\n    b_n = b0 + 0.5 * sum_sq_dev + (n * kappa0 * (y_bar - mu0)**2) / (2.0 * (kappa0 + n))\n    \n    # True posterior mean is mu_n\n    true_posterior_mean = mu_n\n    \n    # Marginal posterior for mu is a Student's t distribution\n    # p(mu|y) = t(df=2*a_n, loc=mu_n, scale=sqrt(b_n/(a_n*kappa_n)))\n    nu = 2 * a_n\n    scale = np.sqrt(b_n / (a_n * kappa_n))\n    \n    # True posterior alpha-quantile\n    q_alpha_true = student_t.ppf(alpha, df=nu, loc=mu_n, scale=scale)\n\n    # --- 2. Setup for simulation replications ---\n    # Create a seeded RNG to generate seeds for each replication for reproducibility\n    main_rng = np.random.default_rng(seed)\n    replication_seeds = main_rng.integers(low=1, high=2**31, size=R)\n    \n    # Lists to store results from each replication\n    naive_mean_estimates = np.zeros(R)\n    rb_mean_estimates = np.zeros(R)\n    \n    naive_cdf_estimates_at_true_q = np.zeros(R)\n    cmc_cdf_estimates_at_true_q = np.zeros(R)\n    \n    naive_quantile_estimates = np.zeros(R)\n    cmc_quantile_estimates = np.zeros(R)\n\n    for r in range(R):\n        rng = np.random.default_rng(replication_seeds[r])\n        \n        # --- Gibbs Sampler ---\n        mu_samples = np.zeros(T)\n        sigma2_samples = np.zeros(T)\n        \n        # Initialize chain\n        mu_curr = y_bar\n        \n        for i in range(M):\n            # Sample sigma^2 | mu, y\n            a_cond = a_n + 0.5\n            b_cond = b_n + 0.5 * kappa_n * (mu_curr - mu_n)**2\n            # Inv-Gamma(a, b) draw is 1/Gamma(a, 1/b)\n            sigma2_curr = 1.0 / rng.gamma(shape=a_cond, scale=1.0/b_cond)\n            \n            # Sample mu | sigma^2, y\n            mu_curr = rng.normal(loc=mu_n, scale=np.sqrt(sigma2_curr / kappa_n))\n            \n            if i >= B:\n                mu_samples[i - B] = mu_curr\n                sigma2_samples[i - B] = sigma2_curr\n\n        # --- 3. Compute estimators for this replication ---\n        \n        # Mean estimators\n        naive_mean_estimates[r] = np.mean(mu_samples)\n        rb_mean_estimates[r] = true_posterior_mean # This is constant\n\n        # CDF estimators at the true quantile q_alpha_true\n        naive_cdf_estimates_at_true_q[r] = np.mean(mu_samples <= q_alpha_true)\n        phi_args = (q_alpha_true - mu_n) / np.sqrt(sigma2_samples / kappa_n)\n        cmc_cdf_estimates_at_true_q[r] = np.mean(norm.cdf(phi_args))\n\n        # Quantile estimators\n        naive_quantile_estimates[r] = np.quantile(mu_samples, alpha)\n        \n        # CMC quantile estimator via root finding\n        def cdf_to_invert(t):\n            phi_args_t = (t - mu_n) / np.sqrt(sigma2_samples / kappa_n)\n            return np.mean(norm.cdf(phi_args_t)) - alpha\n        \n        # Provide a reasonable search bracket based on the t-distribution\n        bracket_low = student_t.ppf(0.001, df=nu, loc=mu_n, scale=scale)\n        bracket_high = student_t.ppf(0.999, df=nu, loc=mu_n, scale=scale)\n        sol = root_scalar(cdf_to_invert, bracket=[bracket_low, bracket_high], method='brentq')\n        cmc_quantile_estimates[r] = sol.root\n\n    # --- 4. Compute variances and MSEs across replications ---\n    var_mean_naive = np.var(naive_mean_estimates, ddof=1)\n    var_mean_rb = np.var(rb_mean_estimates, ddof=1) # Will be 0\n\n    var_cdf_naive = np.var(naive_cdf_estimates_at_true_q, ddof=1)\n    var_cdf_cmc = np.var(cmc_cdf_estimates_at_true_q, ddof=1)\n\n    mse_quantile_naive = np.mean((naive_quantile_estimates - q_alpha_true)**2)\n    mse_quantile_cmc = np.mean((cmc_quantile_estimates - q_alpha_true)**2)\n\n    # --- 5. Perform comparisons ---\n    b1 = var_mean_rb < var_mean_naive\n    b2 = var_cdf_cmc < var_cdf_naive\n    b3 = mse_quantile_cmc < mse_quantile_naive\n\n    return b1, b2, b3\n\ndef solve():\n    # Simulation Parameters\n    R_reps = 500\n    M_iters = 2500\n    B_burn = 500\n\n    # Test Case A\n    y_A = np.array([1.3, 0.7, 1.8, 1.5, 0.9, 2.1, 0.4, 1.2, 0.8, 1.0, 1.7, 1.1, 0.5, 1.4, 0.6, 1.9, 0.3, 1.6, 1.2, 0.95])\n    params_A = {'data': y_A, 'mu0': 0.0, 'kappa0': 1.0, 'a0': 2.0, 'b0': 2.0, 'alpha': 0.5, 'R': R_reps, 'M': M_iters, 'B': B_burn, 'seed': 123}\n    \n    # Test Case B\n    y_B = np.array([0.5, 1.2, -0.1])\n    params_B = {'data': y_B, 'mu0': 0.0, 'kappa0': 1.0, 'a0': 2.0, 'b0': 2.0, 'alpha': 0.9, 'R': R_reps, 'M': M_iters, 'B': B_burn, 'seed': 456}\n\n    # Test Case C\n    y_C = np.array([-1.1, -0.7, -1.4, -1.3, -1.0, -0.6, -1.2, -0.8, -1.5, -0.9, -1.3, -1.1, -0.4, -0.7, -1.0, -1.6, -1.3, -0.5, -0.8, -1.2, -1.0, -0.9, -1.4, -1.1, -0.6, -1.2, -0.7, -1.0, -1.3, -0.8])\n    params_C = {'data': y_C, 'mu0': 0.0, 'kappa0': 1.0, 'a0': 2.0, 'b0': 2.0, 'alpha': 0.1, 'R': R_reps, 'M': M_iters, 'B': B_burn, 'seed': 789}\n    \n    test_cases = [params_A, params_B, params_C]\n    \n    all_results = []\n    for case_params in test_cases:\n        results = run_simulation_for_case(**case_params)\n        all_results.extend(results)\n\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```", "id": "3306479"}, {"introduction": "While MCMC methods aim to generate samples from the target posterior, importance sampling offers a powerful alternative when direct sampling is challenging but the posterior density can be evaluated up to a constant. This exercise ([@problem_id:3306507]) guides you to build estimators for posterior means, CDFs, and quantiles from the ground up using the principle of self-normalized importance sampling. By drawing samples from a simpler proposal distribution and re-weighting them, you will approximate complex posteriors across several canonical Bayesian models, gaining practical experience with the versatility and mechanics of this fundamental Monte Carlo technique.", "problem": "Design and implement a complete program that uses self-normalized importance sampling to estimate posterior means, cumulative distribution functions, and quantiles for specified Bayesian models. The solution must be constructed from the following fundamental bases: Bayes' theorem, the definition of a posterior expectation as an integral under a probability measure, the definition of a quantile as the generalized inverse of a cumulative distribution function, and the change-of-measure identity for transforming integrals. No other formulas are assumed.\n\nGiven a target posterior density $\\pi(\\theta \\mid y)$ known up to a normalizing constant and a proposal density $q(\\theta)$ from which sampling is feasible, use self-normalized importance sampling to approximate posterior expectations $\\mathbb{E}_{\\pi}[h(\\theta)]$, posterior cumulative distribution function values $F_{\\pi}(x) = \\mathbb{P}_{\\pi}(\\theta \\le x)$, and posterior quantiles $Q_{\\pi}(\\alpha)$ defined by $Q_{\\pi}(\\alpha) = \\inf\\{x \\in \\mathbb{R} : F_{\\pi}(x) \\ge \\alpha\\}$. Your estimators must start from the definition of expectation under a change of measure and must not assume access to the normalizing constant of $\\pi(\\theta \\mid y)$.\n\nYour program must implement the following test suite of three Bayesian models. For each model, you must:\n- Sample $N$ proposal draws from $q(\\theta)$ with a fixed random seed.\n- Compute self-normalized importance weights using the unnormalized posterior kernel.\n- Estimate three posterior functionals: mean, a single cumulative distribution function value at a specified point $x$, and a single quantile at a specified probability level $\\alpha$.\n- Compute the exact ground-truth values using the closed-form posterior for the conjugate cases specified.\n- Return booleans indicating whether the absolute errors of the three estimates are within the specified tolerances.\n\nGlobal Monte Carlo settings:\n- Number of proposal draws: $N = 200000$.\n- Random seed for reproducibility: $s = 20231115$.\n- Tolerances: absolute mean error threshold $\\varepsilon_{\\mathrm{mean}} = 0.01$, absolute cumulative distribution function error threshold $\\varepsilon_{\\mathrm{cdf}} = 0.005$, absolute quantile error threshold $\\varepsilon_{\\mathrm{q}} = 0.02$.\n\nTest case A (Gaussian likelihood with Gaussian prior; real-valued parameter):\n- Prior: $\\theta \\sim \\mathcal{N}(\\mu_0, \\tau_0^2)$ with $\\mu_0 = 0$ and $\\tau_0 = 2$.\n- Likelihood: $y_1,\\dots,y_n \\mid \\theta \\overset{\\text{i.i.d.}}{\\sim} \\mathcal{N}(\\theta, \\sigma^2)$ with $\\sigma = 1$.\n- Observed summary statistics: $n = 30$ and $\\bar{y} = 0.25$.\n- Known closed-form posterior: $\\theta \\mid y \\sim \\mathcal{N}(\\mu_{\\mathrm{post}}, v_{\\mathrm{post}})$, where $v_{\\mathrm{post}} = \\left(\\frac{n}{\\sigma^2} + \\frac{1}{\\tau_0^2}\\right)^{-1}$ and $\\mu_{\\mathrm{post}} = v_{\\mathrm{post}}\\left(\\frac{n \\bar{y}}{\\sigma^2} + \\frac{\\mu_0}{\\tau_0^2}\\right)$.\n- Proposal: $q(\\theta) = \\mathcal{N}(m_q, s_q^2)$ with $m_q = \\mu_{\\mathrm{post}} + 0.5$ and $s_q = \\sqrt{2}\\sqrt{v_{\\mathrm{post}}}$.\n- Targets to estimate: posterior mean, posterior cumulative distribution function at $x_0 = 0$, and posterior quantile at probability level $\\alpha_0 = 0.95$.\n\nTest case B (Poisson likelihood with Gamma prior; positive parameter):\n- Prior: $\\theta \\sim \\mathrm{Gamma}(a, b)$ with shape $a = 2$ and rate $b = 1$.\n- Likelihood: $y_1,\\dots,y_n \\mid \\theta \\overset{\\text{i.i.d.}}{\\sim} \\mathrm{Poisson}(\\theta)$.\n- Observed summary statistics: $n = 40$ and $\\sum_{i=1}^n y_i = 55$.\n- Known closed-form posterior: $\\theta \\mid y \\sim \\mathrm{Gamma}(a', b')$ with $a' = a + \\sum_{i=1}^n y_i$ and $b' = b + n$.\n- Proposal: $q(\\theta) = \\mathrm{Gamma}(\\tilde{a}, \\tilde{b})$ with shape $\\tilde{a} = 50$ and rate $\\tilde{b} = 35$.\n- Targets to estimate: posterior mean, posterior cumulative distribution function at $x_1 = 1.2$, and posterior quantile at probability level $\\alpha_1 = 0.5$.\n\nTest case C (Bernoulli likelihood with Beta prior; unit-interval parameter):\n- Prior: $\\theta \\sim \\mathrm{Beta}(\\alpha, \\beta)$ with $\\alpha = 0.5$ and $\\beta = 0.5$.\n- Likelihood: $y_1,\\dots,y_n \\mid \\theta \\overset{\\text{i.i.d.}}{\\sim} \\mathrm{Bernoulli}(\\theta)$.\n- Observed summary statistics: $n = 80$ and $\\sum_{i=1}^n y_i = 20$.\n- Known closed-form posterior: $\\theta \\mid y \\sim \\mathrm{Beta}(\\alpha', \\beta')$ with $\\alpha' = \\alpha + \\sum_{i=1}^n y_i$ and $\\beta' = \\beta + n - \\sum_{i=1}^n y_i$.\n- Proposal: $q(\\theta) = \\mathrm{Beta}(\\tilde{\\alpha}, \\tilde{\\beta})$ with $\\tilde{\\alpha} = 1.0$ and $\\tilde{\\beta} = 1.5$.\n- Targets to estimate: posterior mean, posterior cumulative distribution function at $x_2 = 0.3$, and posterior quantile at probability level $\\alpha_2 = 0.95$.\n\nExact reference values for comparison:\n- For Test case A, use the closed-form Gaussian posterior to compute the exact posterior mean, cumulative distribution function, and quantile.\n- For Test case B, use the closed-form Gamma posterior to compute the exact posterior mean, cumulative distribution function, and quantile.\n- For Test case C, use the closed-form Beta posterior to compute the exact posterior mean, cumulative distribution function, and quantile.\n\nOutput specification:\n- For each test case, produce three booleans in order: mean accuracy, cumulative distribution function accuracy, quantile accuracy. Each boolean is true if and only if the absolute error is less than or equal to the corresponding tolerance $\\varepsilon_{\\mathrm{mean}}$, $\\varepsilon_{\\mathrm{cdf}}$, or $\\varepsilon_{\\mathrm{q}}$.\n- Aggregate all booleans across the three test cases into a single flat list in the order Test case A then Test case B then Test case C.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[r_1,r_2,\\dots,r_9]$), with each $r_i$ being either the string literal $\\texttt{True}$ or $\\texttt{False}$.", "solution": "The present task requires the design and implementation of a program to estimate posterior functionals using self-normalized importance sampling (SNIS). The solution must be derived from fundamental principles, starting with Bayes' theorem and the definition of expectation.\n\n### Theoretical Foundation of Self-Normalized Importance Sampling\n\nLet $\\theta$ be a parameter vector and $y$ be the observed data. According to Bayes' theorem, the posterior probability density function (PDF) $\\pi(\\theta \\mid y)$ is proportional to the product of the likelihood $p(y \\mid \\theta)$ and the prior $p(\\theta)$:\n$$\n\\pi(\\theta \\mid y) = \\frac{p(y \\mid \\theta) p(\\theta)}{p(y)}\n$$\nwhere $p(y) = \\int p(y \\mid \\theta) p(\\theta) d\\theta$ is the marginal likelihood, serving as a normalizing constant. We denote the unnormalized posterior kernel as $f(\\theta) = p(y \\mid \\theta) p(\\theta)$, so that $\\pi(\\theta \\mid y) = f(\\theta) / p(y)$. In many practical scenarios, $f(\\theta)$ is known, but its integral $p(y)$ is intractable.\n\nThe posterior expectation of a function $h(\\theta)$ is defined as:\n$$\n\\mathbb{E}_{\\pi}[h(\\theta)] = \\int h(\\theta) \\pi(\\theta \\mid y) d\\theta\n$$\nTo estimate this integral using Monte Carlo methods when sampling directly from $\\pi(\\theta \\mid y)$ is difficult, we introduce a proposal distribution $q(\\theta)$ from which we can easily draw samples. The support of $q(\\theta)$ must cover the support of $\\pi(\\theta \\mid y)$, i.e., $q(\\theta) > 0$ whenever $\\pi(\\theta \\mid y) > 0$.\n\nWe apply the change-of-measure identity by multiplying and dividing the integrand by $q(\\theta)$:\n$$\n\\mathbb{E}_{\\pi}[h(\\theta)] = \\int h(\\theta) \\frac{\\pi(\\theta \\mid y)}{q(\\theta)} q(\\theta) d\\theta = \\mathbb{E}_{q}\\left[h(\\theta) \\frac{\\pi(\\theta \\mid y)}{q(\\theta)}\\right]\n$$\nwhere $\\mathbb{E}_{q}[\\cdot]$ denotes the expectation with respect to the proposal distribution $q$. Substituting $\\pi(\\theta \\mid y) = f(\\theta) / p(y)$, we have:\n$$\n\\mathbb{E}_{\\pi}[h(\\theta)] = \\mathbb{E}_{q}\\left[h(\\theta) \\frac{f(\\theta)/p(y)}{q(\\theta)}\\right] = \\frac{1}{p(y)} \\mathbb{E}_{q}[h(\\theta) w(\\theta)]\n$$\nwhere $w(\\theta) = \\frac{f(\\theta)}{q(\\theta)}$ are the unnormalized importance weights. The unknown constant $p(y)$ remains. We can, however, express $p(y)$ itself as an expectation with respect to $q$:\n$$\np(y) = \\int f(\\theta) d\\theta = \\int \\frac{f(\\theta)}{q(\\theta)} q(\\theta) d\\theta = \\mathbb{E}_{q}[w(\\theta)]\n$$\nCombining these two expressions yields the self-normalized form:\n$$\n\\mathbb{E}_{\\pi}[h(\\theta)] = \\frac{\\mathbb{E}_{q}[h(\\theta) w(\\theta)]}{\\mathbb{E}_{q}[w(\\theta)]}\n$$\nGiven $N$ independent and identically distributed samples $\\theta_1, \\dots, \\theta_N$ drawn from $q(\\theta)$, we can approximate the expectations using Monte Carlo averages. Let $w_i = w(\\theta_i) = f(\\theta_i)/q(\\theta_i)$. The SNIS estimator for $\\mathbb{E}_{\\pi}[h(\\theta)]$ is:\n$$\n\\widehat{\\mathbb{E}}_{\\pi}[h(\\theta)] = \\frac{\\frac{1}{N}\\sum_{i=1}^{N} h(\\theta_i) w_i}{\\frac{1}{N}\\sum_{i=1}^{N} w_i} = \\frac{\\sum_{i=1}^{N} h(\\theta_i) w_i}{\\sum_{i=1}^{N} w_i}\n$$\nThis estimator is a ratio of two estimators and is consistent for $\\mathbb{E}_{\\pi}[h(\\theta)]$ as $N \\to \\infty$.\n\n### Estimation of Posterior Functionals\n\n1.  **Posterior Mean**: To estimate the posterior mean, we set $h(\\theta) = \\theta$. The estimator is:\n    $$\n    \\hat{\\mu}_{\\pi} = \\frac{\\sum_{i=1}^{N} \\theta_i w_i}{\\sum_{i=1}^{N} w_i}\n    $$\n\n2.  **Posterior Cumulative Distribution Function (CDF)**: The posterior CDF at a point $x$, $F_{\\pi}(x) = \\mathbb{P}_{\\pi}(\\theta \\le x)$, is an expectation where the function is an indicator, $h(\\theta) = \\mathbb{I}(\\theta \\le x)$. The estimator is:\n    $$\n    \\hat{F}_{\\pi}(x) = \\frac{\\sum_{i=1}^{N} \\mathbb{I}(\\theta_i \\le x) w_i}{\\sum_{i=1}^{N} w_i}\n    $$\n\n3.  **Posterior Quantile**: The $\\alpha$-quantile, $Q_{\\pi}(\\alpha)$, is the generalized inverse of the CDF: $Q_{\\pi}(\\alpha) = \\inf\\{x: F_{\\pi}(x) \\ge \\alpha\\}$. We estimate this by inverting the estimated CDF, $\\hat{F}_{\\pi}(x)$. This can be implemented algorithmically. First, compute the normalized weights $\\bar{w}_i = w_i / \\sum_{j=1}^{N} w_j$. Then, pair the samples $\\{\\theta_i\\}$ with their normalized weights $\\{\\bar{w}_i\\}$. Sort these pairs based on the values of $\\theta_i$ to obtain $(\\theta_{(1)}, \\bar{w}_{(1)}), \\dots, (\\theta_{(N)}, \\bar{w}_{(N)})$, where $\\theta_{(1)} \\le \\theta_{(2)} \\le \\dots \\le \\theta_{(N)}$. The estimated quantile $\\hat{Q}_{\\pi}(\\alpha)$ is the first ordered sample $\\theta_{(k)}$ for which the cumulative sum of weights is at least $\\alpha$:\n    $$\n    \\hat{Q}_{\\pi}(\\alpha) = \\theta_{(k)} \\quad \\text{where} \\quad k = \\min\\left\\{j \\in \\{1, \\dots, N\\} : \\sum_{i=1}^{j} \\bar{w}_{(i)} \\ge \\alpha\\right\\}\n    $$\n\n### Numerical Implementation and Test Cases\n\nFor numerical stability, computations involving products of probabilities are performed in the log domain. The log-weight is $\\log w_i = \\log f(\\theta_i) - \\log q(\\theta_i)$. To prevent overflow when exponentiating, a common stabilization technique is to subtract the maximum log-weight: $w_i = \\exp(\\log w_i - \\max_j(\\log w_j))$. This rescales the weights but does not change the final SNIS estimates.\n\n**Test Case A: Gaussian-Gaussian Model**\n- Prior: $\\theta \\sim \\mathcal{N}(\\mu_0, \\tau_0^2)$, so $p(\\theta) \\propto \\exp\\left(-\\frac{(\\theta-\\mu_0)^2}{2\\tau_0^2}\\right)$.\n- Likelihood using summary statistics: $p(y \\mid \\theta) \\propto \\exp\\left(-\\frac{n(\\theta-\\bar{y})^2}{2\\sigma^2}\\right)$.\n- Unnormalized posterior kernel: $f(\\theta) \\propto \\exp\\left(-\\frac{n(\\theta-\\bar{y})^2}{2\\sigma^2} - \\frac{(\\theta-\\mu_0)^2}{2\\tau_0^2}\\right)$.\n- Proposal: $q(\\theta) \\sim \\mathcal{N}(m_q, s_q^2)$, so $q(\\theta) \\propto \\exp\\left(-\\frac{(\\theta-m_q)^2}{2s_q^2}\\right)$.\n- The log-weight is $\\log w(\\theta) = \\left(-\\frac{n(\\theta-\\bar{y})^2}{2\\sigma^2} - \\frac{(\\theta-\\mu_0)^2}{2\\tau_0^2}\\right) - \\left(-\\frac{(\\theta-m_q)^2}{2s_q^2}\\right)$.\n\n**Test Case B: Poisson-Gamma Model**\n- Prior: $\\theta \\sim \\mathrm{Gamma}(a, b)$, so $p(\\theta) \\propto \\theta^{a-1} e^{-b\\theta}$ for $\\theta > 0$.\n- Likelihood using summary statistics: $p(y \\mid \\theta) \\propto \\theta^{\\sum y_i} e^{-n\\theta}$.\n- Unnormalized posterior kernel: $f(\\theta) \\propto \\theta^{a+\\sum y_i-1} e^{-(b+n)\\theta}$.\n- Proposal: $q(\\theta) \\sim \\mathrm{Gamma}(\\tilde{a}, \\tilde{b})$, so $q(\\theta) \\propto \\theta^{\\tilde{a}-1} e^{-\\tilde{b}\\theta}$.\n- The log-weight is $\\log w(\\theta) = ((a+\\sum y_i-1)\\log\\theta - (b+n)\\theta) - ((\\tilde{a}-1)\\log\\theta - \\tilde{b}\\theta)$.\n\n**Test Case C: Bernoulli-Beta Model**\n- Prior: $\\theta \\sim \\mathrm{Beta}(\\alpha, \\beta)$, so $p(\\theta) \\propto \\theta^{\\alpha-1} (1-\\theta)^{\\beta-1}$ for $\\theta \\in (0, 1)$.\n- Likelihood using summary statistics: $p(y \\mid \\theta) \\propto \\theta^{\\sum y_i} (1-\\theta)^{n-\\sum y_i}$.\n- Unnormalized posterior kernel: $f(\\theta) \\propto \\theta^{\\alpha+\\sum y_i-1} (1-\\theta)^{\\beta+n-\\sum y_i-1}$.\n- Proposal: $q(\\theta) \\sim \\mathrm{Beta}(\\tilde{\\alpha}, \\tilde{\\beta})$, so $q(\\theta) \\propto \\theta^{\\tilde{\\alpha}-1} (1-\\theta)^{\\tilde{\\beta}-1}$.\n- The log-weight is $\\log w(\\theta) = ((\\alpha+\\sum y_i-1)\\log\\theta + (\\beta+n-\\sum y_i-1)\\log(1-\\theta)) - ((\\tilde{\\alpha}-1)\\log\\theta + (\\tilde{\\beta}-1)\\log(1-\\theta))$.\n\nThe program will implement these models, generate $N=200000$ samples from each proposal, compute the SNIS estimates for the mean, a specified CDF value, and a specified quantile, and compare them against the exact values derived from the known conjugate posteriors. The comparison is based on absolute error thresholds $\\varepsilon_{\\mathrm{mean}} = 0.01$, $\\varepsilon_{\\mathrm{cdf}} = 0.005$, and $\\varepsilon_{\\mathrm{q}} = 0.02$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import stats\n\ndef solve():\n    # Global Monte Carlo settings\n    N = 200000\n    seed = 20231115\n    E_MEAN = 0.01\n    E_CDF = 0.005\n    E_Q = 0.02\n\n    rng = np.random.default_rng(seed)\n\n    def estimate_functionals(theta_samples, weights):\n        \"\"\"\n        Computes posterior mean, CDF, and quantile from weighted samples.\n        \"\"\"\n        if np.sum(weights) == 0:\n            # This can happen if all weights underflow to zero\n            return np.nan, lambda x: np.nan, lambda alpha: np.nan\n            \n        # 1. Normalize weights\n        norm_weights = weights / np.sum(weights)\n\n        # 2. Estimate posterior mean\n        est_mean = np.sum(norm_weights * theta_samples)\n\n        # 3. Estimator for posterior CDF\n        def est_cdf_func(x):\n            return np.sum(norm_weights * (theta_samples <= x))\n\n        # 4. Estimator for posterior quantile\n        def est_quantile_func(alpha):\n            sort_indices = np.argsort(theta_samples)\n            sorted_theta = theta_samples[sort_indices]\n            sorted_weights = norm_weights[sort_indices]\n            cum_weights = np.cumsum(sorted_weights)\n            # Find the first index where cumulative weight is >= alpha\n            quantile_index = np.searchsorted(cum_weights, alpha, side='left')\n            # Handle edge case where alpha is 1 or very close\n            if quantile_index >= len(sorted_theta):\n                return sorted_theta[-1]\n            return sorted_theta[quantile_index]\n\n        return est_mean, est_cdf_func, est_quantile_func\n\n    # --- Test Case A: Gaussian-Gaussian ---\n    def run_case_A():\n        # Parameters\n        mu0, tau0 = 0.0, 2.0\n        n, y_bar, sigma = 30.0, 0.25, 1.0\n        x0, alpha0 = 0.0, 0.95\n\n        # Closed-form posterior parameters\n        v_post = 1.0 / (n / sigma**2 + 1.0 / tau0**2)\n        mu_post = v_post * (n * y_bar / sigma**2 + mu0 / tau0**2)\n\n        # Proposal parameters\n        m_q = mu_post + 0.5\n        s_q = np.sqrt(2.0 * v_post)\n\n        # Ground truth using closed-form posterior\n        posterior_dist = stats.norm(loc=mu_post, scale=np.sqrt(v_post))\n        true_mean = posterior_dist.mean()\n        true_cdf = posterior_dist.cdf(x0)\n        true_quantile = posterior_dist.ppf(alpha0)\n\n        # Unnormalized log-posterior kernel\n        def log_f(theta):\n            log_prior = stats.norm.logpdf(theta, loc=mu0, scale=tau0)\n            # We use the sufficient statistic for the log-likelihood\n            log_lik = -n * (theta - y_bar)**2 / (2 * sigma**2)\n            return log_lik + log_prior\n\n        # Sample from proposal\n        proposal_dist = stats.norm(loc=m_q, scale=s_q)\n        theta_samples = proposal_dist.rvs(size=N, random_state=rng)\n\n        # Compute importance weights in log-space for stability\n        log_weights = log_f(theta_samples) - proposal_dist.logpdf(theta_samples)\n        \n        # Stabilize weights by subtracting the max log-weight before exponentiating\n        stable_log_weights = log_weights - np.max(log_weights)\n        weights = np.exp(stable_log_weights)\n\n        # Estimate functionals\n        est_mean, est_cdf_func, est_quantile_func = estimate_functionals(theta_samples, weights)\n        est_cdf = est_cdf_func(x0)\n        est_quantile = est_quantile_func(alpha0)\n\n        # Compare and return booleans\n        mean_ok = np.abs(est_mean - true_mean) <= E_MEAN\n        cdf_ok = np.abs(est_cdf - true_cdf) <= E_CDF\n        q_ok = np.abs(est_quantile - true_quantile) <= E_Q\n        \n        return [mean_ok, cdf_ok, q_ok]\n\n    # --- Test Case B: Poisson-Gamma ---\n    def run_case_B():\n        # Parameters\n        a, b = 2.0, 1.0\n        n, sum_y = 40.0, 55.0\n        x1, alpha1 = 1.2, 0.5\n\n        # Closed-form posterior parameters\n        a_prime = a + sum_y\n        b_prime = b + n\n\n        # Proposal parameters\n        a_tilde, b_tilde = 50.0, 35.0\n\n        # Ground truth using closed-form posterior\n        posterior_dist = stats.gamma(a=a_prime, scale=1.0/b_prime)\n        true_mean = posterior_dist.mean()\n        true_cdf = posterior_dist.cdf(x1)\n        true_quantile = posterior_dist.ppf(alpha1)\n\n        # Unnormalized log-posterior kernel\n        def log_f(theta):\n            # Using logsumexp trick is not necessary here, but checking for theta > 0\n            # is important for log.\n            with np.errstate(divide='ignore'):\n                log_prior = stats.gamma.logpdf(theta, a=a, scale=1.0/b)\n                log_lik = sum_y * np.log(theta) - n * theta\n            # Replace -inf from log(0) with a very small number to avoid issues\n            log_prior[theta <= 0] = -np.inf\n            log_lik[theta <= 0] = -np.inf\n            return log_lik + log_prior\n\n        # Sample from proposal\n        proposal_dist = stats.gamma(a=a_tilde, scale=1.0/b_tilde)\n        theta_samples = proposal_dist.rvs(size=N, random_state=rng)\n        \n        # Compute importance weights\n        log_weights = log_f(theta_samples) - proposal_dist.logpdf(theta_samples)\n        stable_log_weights = log_weights - np.nanmax(log_weights)\n        weights = np.exp(stable_log_weights)\n\n        # Estimate functionals\n        est_mean, est_cdf_func, est_quantile_func = estimate_functionals(theta_samples, weights)\n        est_cdf = est_cdf_func(x1)\n        est_quantile = est_quantile_func(alpha1)\n        \n        # Compare and return booleans\n        mean_ok = np.abs(est_mean - true_mean) <= E_MEAN\n        cdf_ok = np.abs(est_cdf - true_cdf) <= E_CDF\n        q_ok = np.abs(est_quantile - true_quantile) <= E_Q\n        \n        return [mean_ok, cdf_ok, q_ok]\n\n    # --- Test Case C: Bernoulli-Beta ---\n    def run_case_C():\n        # Parameters\n        alpha_prior, beta_prior = 0.5, 0.5\n        n, sum_y = 80.0, 20.0\n        x2, alpha2 = 0.3, 0.95\n\n        # Closed-form posterior parameters\n        alpha_prime = alpha_prior + sum_y\n        beta_prime = beta_prior + n - sum_y\n\n        # Proposal parameters\n        alpha_tilde, beta_tilde = 1.0, 1.5\n\n        # Ground truth using closed-form posterior\n        posterior_dist = stats.beta(a=alpha_prime, b=beta_prime)\n        true_mean = posterior_dist.mean()\n        true_cdf = posterior_dist.cdf(x2)\n        true_quantile = posterior_dist.ppf(alpha2)\n\n        # Unnormalized log-posterior kernel\n        def log_f(theta):\n            with np.errstate(divide='ignore'):\n                log_prior = stats.beta.logpdf(theta, a=alpha_prior, b=beta_prior)\n                log_lik = sum_y * np.log(theta) + (n - sum_y) * np.log(1 - theta)\n            log_prior[~np.isfinite(log_prior)] = -np.inf\n            log_lik[~np.isfinite(log_lik)] = -np.inf\n            return log_lik + log_prior\n            \n        # Sample from proposal\n        proposal_dist = stats.beta(a=alpha_tilde, b=beta_tilde)\n        theta_samples = proposal_dist.rvs(size=N, random_state=rng)\n        \n        # Compute importance weights\n        log_weights = log_f(theta_samples) - proposal_dist.logpdf(theta_samples)\n        stable_log_weights = log_weights - np.nanmax(log_weights)\n        weights = np.exp(stable_log_weights)\n\n        # Estimate functionals\n        est_mean, est_cdf_func, est_quantile_func = estimate_functionals(theta_samples, weights)\n        est_cdf = est_cdf_func(x2)\n        est_quantile = est_quantile_func(alpha2)\n\n        # Compare and return booleans\n        mean_ok = np.abs(est_mean - true_mean) <= E_MEAN\n        cdf_ok = np.abs(est_cdf - true_cdf) <= E_CDF\n        q_ok = np.abs(est_quantile - true_quantile) <= E_Q\n\n        return [mean_ok, cdf_ok, q_ok]\n\n    # Run all test cases\n    results = []\n    results.extend(run_case_A())\n    results.extend(run_case_B())\n    results.extend(run_case_C())\n\n    # Final print statement\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3306507"}]}