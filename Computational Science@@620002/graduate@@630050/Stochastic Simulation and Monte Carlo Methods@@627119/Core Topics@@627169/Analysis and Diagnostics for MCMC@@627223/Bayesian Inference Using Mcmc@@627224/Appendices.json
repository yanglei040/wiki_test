{"hands_on_practices": [{"introduction": "This problem serves as a foundational exercise in interpreting the output of a Markov Chain Monte Carlo (MCMC) simulation. Before delving into the complexities of designing samplers, it is crucial to understand what to do with the samples once they are generated. This practice demonstrates how to use a set of draws from a posterior distribution to compute summary statistics, such as the posterior mean, which is a cornerstone of Bayesian parameter estimation. [@problem_id:1319931]", "problem": "A quality control engineer is assessing a manufacturing process for a new type of semiconductor chip. The quality of the process is characterized by the probability, $p$, that a single chip is non-defective. The engineer's prior belief about $p$ is that it is equally likely to be any value in the discrete set $S = \\{0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9\\}$.\n\nTo update this belief, the engineer tests a random batch of $n=20$ chips and finds that $k=15$ of them are non-defective. The number of non-defective chips is assumed to follow a binomial distribution.\n\nTo analyze the posterior distribution of $p$ given the data, a Markov Chain Monte Carlo (MCMC) simulation using the Metropolis-Hastings algorithm was performed. A total of 5000 samples were generated from the posterior distribution. After discarding the first 1000 samples as a \"burn-in\" period, the remaining 4000 samples were tallied. The counts for each possible value of $p$ in the set $S$ are as follows:\n- $p=0.1$: 0 samples\n- $p=0.2$: 0 samples\n- $p=0.3$: 0 samples\n- $p=0.4$: 0 samples\n- $p=0.5$: 19 samples\n- $p=0.6$: 301 samples\n- $p=0.7$: 1690 samples\n- $p=0.8$: 1845 samples\n- $p=0.9$: 145 samples\n\nUsing the results of this simulation, calculate the estimated posterior mean of the probability $p$. Report your answer rounded to three significant figures.", "solution": "We want the posterior mean of $p$ estimated from the MCMC output after burn-in. Let $N$ be the total number of retained samples and $N_{j}$ the count of samples at $p_{j} \\in S$. The Monte Carlo estimator of the posterior mean is\n$$\n\\hat{\\mu}=\\frac{1}{N}\\sum_{j} p_{j} N_{j}=\\sum_{j} p_{j}\\left(\\frac{N_{j}}{N}\\right).\n$$\nFrom the tallies, $N=4000$ and only the following $N_{j}$ are nonzero:\n$$\nN_{0.5}=19,\\quad N_{0.6}=301,\\quad N_{0.7}=1690,\\quad N_{0.8}=1845,\\quad N_{0.9}=145.\n$$\nThus,\n$$\n\\hat{\\mu}=\\frac{0.5 \\cdot 19+0.6 \\cdot 301+0.7 \\cdot 1690+0.8 \\cdot 1845+0.9 \\cdot 145}{4000}.\n$$\nCompute the numerator step by step:\n$$\n0.5 \\cdot 19=9.5,\\quad 0.6 \\cdot 301=180.6,\\quad 0.7 \\cdot 1690=1183,\\quad 0.8 \\cdot 1845=1476,\\quad 0.9 \\cdot 145=130.5,\n$$\n$$\n9.5+180.6=190.1,\\quad 190.1+1183=1373.1,\\quad 1373.1+1476=2849.1,\\quad 2849.1+130.5=2979.6.\n$$\nTherefore,\n$$\n\\hat{\\mu}=\\frac{2979.6}{4000}=0.7449.\n$$\nRounding to three significant figures gives\n$$\n0.7449 \\approx 0.745.\n$$", "answer": "$$\\boxed{0.745}$$", "id": "1319931"}, {"introduction": "Building upon the basics, this practice explores the design of more sophisticated MCMC algorithms that improve sampling efficiency. It focuses on the Metropolis-Adjusted Langevin Algorithm (MALA), which incorporates gradient information from the target posterior to propose more intelligent moves. You will derive the MALA proposal mechanism from its underlying stochastic differential equation and then apply it in a concrete scenario, bridging the gap between continuous-time stochastic processes and practical, discrete-time samplers. [@problem_id:3289331]", "problem": "A central task in computational systems biology is parameter inference for stochastic models of gene regulation from noisy observations. Consider a two-parameter transcription-degradation model whose Bayesian posterior over a parameter vector $\\theta \\in \\mathbb{R}^{2}$ is smooth and strictly positive. Suppose we wish to construct a Markov chain Monte Carlo (MCMC) method that targets this posterior using dynamics inspired by the overdamped Langevin Stochastic Differential Equation (SDE). The overdamped Langevin SDE for a target density $\\pi(\\theta)$ is given by\n$$\nd\\theta_{t} \\;=\\; \\frac{1}{2}\\,\\nabla \\log \\pi(\\theta_{t})\\,dt \\;+\\; dW_{t},\n$$\nwhere $W_{t}$ is a standard $2$-dimensional Wiener process and $\\nabla \\log \\pi(\\theta)$ denotes the gradient with respect to $\\theta$.\n\nTask:\n- Starting only from the SDE above and fundamental definitions of the Metropolis-Hastings (MH) algorithm, derive a proposal mechanism of the form $\\theta' = \\theta + \\frac{\\delta^{2}}{2}\\,\\nabla \\log \\pi(\\theta) + \\delta\\,\\eta$ with $\\eta \\sim \\mathcal{N}(0, I)$, and derive the corresponding MH acceptance probability in terms of $\\pi$, the current state $\\theta$, the proposed state $\\theta'$, and the Gaussian proposal densities. Define any acronyms you introduce on their first appearance.\n\nThen, specialize to the following scientifically realistic posterior arising from a linear-Gaussian approximation to a stochastic gene expression model. The posterior $\\pi(\\theta)$ is proportional to a Gaussian density with mean $\\mu$ and covariance $\\Sigma$:\n$$\n\\pi(\\theta) \\;\\propto\\; \\exp\\!\\left(-\\frac{1}{2}(\\theta - \\mu)^{\\top}\\Sigma^{-1}(\\theta - \\mu)\\right),\n$$\nwith\n$$\n\\mu \\;=\\; \\begin{pmatrix}0.5 \\\\ -0.3\\end{pmatrix}, \n\\qquad \n\\Sigma \\;=\\; \\begin{pmatrix}0.5 & 0.1 \\\\ 0.1 & 0.4\\end{pmatrix}.\n$$\nAssume the current state is\n$$\n\\theta \\;=\\; \\begin{pmatrix}0.6 \\\\ -0.2\\end{pmatrix},\n$$\nthe step size parameter is $\\delta = 0.2$, and the realized Gaussian noise draw is\n$$\n\\eta \\;=\\; \\begin{pmatrix}0.5 \\\\ -1.2\\end{pmatrix}.\n$$\nUsing the proposal you derived and the general MH acceptance probability expression, compute the Metropolis-Adjusted Langevin Algorithm (MALA) acceptance probability for moving from $\\theta$ to the proposed $\\theta'$. Express your final answer as a decimal and round your answer to four significant figures.", "solution": "The task is to derive a proposal mechanism and acceptance probability for a specific Markov chain Monte Carlo (MCMC) method and then apply it to a given Bayesian posterior.\n\nFirst, we derive the proposal mechanism. The problem specifies dynamics inspired by the overdamped Langevin Stochastic Differential Equation (SDE):\n$$d\\theta_{t} = \\frac{1}{2}\\nabla \\log \\pi(\\theta_{t})\\,dt + dW_{t}$$\nwhere $\\theta_t \\in \\mathbb{R}^2$ is the parameter vector, $\\pi(\\theta)$ is the target posterior density, and $W_t$ is a standard $2$-dimensional Wiener process.\n\nA discrete-time approximation of this SDE can be obtained using the Euler-Maruyama method. We discretize time with a step size $\\Delta t > 0$. The update from a state $\\theta$ at time $t$ to a state $\\theta'$ at time $t+\\Delta t$ is:\n$$\\theta' \\approx \\theta + \\frac{1}{2}\\nabla \\log \\pi(\\theta) \\Delta t + \\Delta W_t$$\nThe increment of the Wiener process, $\\Delta W_t = W_{t+\\Delta t} - W_t$, is a Gaussian random variable with mean $0$ and covariance matrix $(\\Delta t)I$, where $I$ is the $2 \\times 2$ identity matrix.\nWe can write $\\Delta W_t = \\sqrt{\\Delta t}\\,\\eta$, where $\\eta \\sim \\mathcal{N}(0, I)$. Let the step size parameter be $\\delta = \\sqrt{\\Delta t}$, so $\\Delta t = \\delta^2$. Substituting this into the discretized equation yields the proposal mechanism:\n$$\\theta' = \\theta + \\frac{\\delta^2}{2}\\nabla \\log \\pi(\\theta) + \\delta\\eta, \\quad \\eta \\sim \\mathcal{N}(0, I)$$\nThis is a draw from a Gaussian proposal density, $q(\\theta'|\\theta)$, centered at $\\theta + \\frac{\\delta^2}{2}\\nabla \\log \\pi(\\theta)$ with covariance $\\delta^2 I$. The proposal density function is:\n$$q(\\theta'|\\theta) = \\frac{1}{(2\\pi\\delta^2)^{2/2}} \\exp\\left( -\\frac{1}{2\\delta^2} \\left\\| \\theta' - \\left( \\theta + \\frac{\\delta^2}{2}\\nabla \\log \\pi(\\theta) \\right) \\right\\|^2 \\right)$$\nwhere $||\\cdot||$ denotes the Euclidean norm.\n\nNext, we derive the Metropolis-Hastings (MH) acceptance probability. The MH algorithm ensures that the resulting Markov chain has $\\pi(\\theta)$ as its stationary distribution. The acceptance probability $\\alpha(\\theta', \\theta)$ for a move from state $\\theta$ to a proposed state $\\theta'$ is given by:\n$$\\alpha(\\theta', \\theta) = \\min\\left(1, \\frac{\\pi(\\theta')q(\\theta|\\theta')}{\\pi(\\theta)q(\\theta'|\\theta)}\\right)$$\nThe term $q(\\theta|\\theta')$ is the density of proposing $\\theta$ starting from $\\theta'$. Based on our proposal mechanism, this is:\n$$q(\\theta|\\theta') = \\frac{1}{(2\\pi\\delta^2)^{2/2}} \\exp\\left( -\\frac{1}{2\\delta^2} \\left\\| \\theta - \\left( \\theta' + \\frac{\\delta^2}{2}\\nabla \\log \\pi(\\theta') \\right) \\right\\|^2 \\right)$$\nSubstituting the expressions for $q(\\theta'|\\theta)$ and $q(\\theta|\\theta')$ into the acceptance probability formula, the normalization constants cancel, leading to:\n$$\\alpha(\\theta', \\theta) = \\min\\left(1, \\frac{\\pi(\\theta')}{\\pi(\\theta)} \\frac{\\exp\\left( -\\frac{1}{2\\delta^2} \\left\\| \\theta - \\theta' - \\frac{\\delta^2}{2}\\nabla \\log \\pi(\\theta') \\right\\|^2 \\right)}{\\exp\\left( -\\frac{1}{2\\delta^2} \\left\\| \\theta' - \\theta - \\frac{\\delta^2}{2}\\nabla \\log \\pi(\\theta) \\right\\|^2 \\right)}\\right)$$\nThis algorithm, which uses a Langevin-based proposal with an MH correction step, is known as the Metropolis-Adjusted Langevin Algorithm (MALA).\n\nNow, we specialize to the given posterior, which is proportional to a multivariate Gaussian density:\n$$\\pi(\\theta) \\propto \\exp\\left(-\\frac{1}{2}(\\theta - \\mu)^{\\top}\\Sigma^{-1}(\\theta - \\mu)\\right)$$\nThe log-posterior is $\\log\\pi(\\theta) = C - \\frac{1}{2}(\\theta - \\mu)^{\\top}\\Sigma^{-1}(\\theta - \\mu)$ for some constant $C$. The gradient of the log-posterior with respect to $\\theta$, which we denote as $g(\\theta)$, is:\n$$g(\\theta) = \\nabla \\log \\pi(\\theta) = \\nabla_\\theta \\left( -\\frac{1}{2}(\\theta - \\mu)^{\\top}\\Sigma^{-1}(\\theta - \\mu) \\right) = -\\Sigma^{-1}(\\theta - \\mu)$$\nThe given values are:\n$$\\mu = \\begin{pmatrix}0.5 \\\\ -0.3\\end{pmatrix}, \\quad \\Sigma = \\begin{pmatrix}0.5 & 0.1 \\\\ 0.1 & 0.4\\end{pmatrix}, \\quad \\theta = \\begin{pmatrix}0.6 \\\\ -0.2\\end{pmatrix}, \\quad \\delta = 0.2, \\quad \\eta = \\begin{pmatrix}0.5 \\\\ -1.2\\end{pmatrix}$$\nFirst, we compute the inverse of the covariance matrix $\\Sigma$:\n$$\\det(\\Sigma) = (0.5)(0.4) - (0.1)(0.1) = 0.2 - 0.01 = 0.19$$\n$$\\Sigma^{-1} = \\frac{1}{0.19} \\begin{pmatrix}0.4 & -0.1 \\\\ -0.1 & 0.5\\end{pmatrix}$$\nNext, we compute the proposed state $\\theta'$:\n$$\\theta' = \\theta + \\frac{\\delta^2}{2}g(\\theta) + \\delta\\eta$$\nWe calculate each term:\n$$\\theta - \\mu = \\begin{pmatrix}0.6 \\\\ -0.2\\end{pmatrix} - \\begin{pmatrix}0.5 \\\\ -0.3\\end{pmatrix} = \\begin{pmatrix}0.1 \\\\ 0.1\\end{pmatrix}$$\n$$g(\\theta) = -\\Sigma^{-1}(\\theta - \\mu) = -\\frac{1}{0.19} \\begin{pmatrix}0.4 & -0.1 \\\\ -0.1 & 0.5\\end{pmatrix} \\begin{pmatrix}0.1 \\\\ 0.1\\end{pmatrix} = -\\frac{1}{0.19} \\begin{pmatrix}0.03 \\\\ 0.04\\end{pmatrix}$$\nThe drift term is, with $\\delta^2 = (0.2)^2 = 0.04$:\n$$\\frac{\\delta^2}{2}g(\\theta) = \\frac{0.04}{2} \\left( -\\frac{1}{0.19} \\begin{pmatrix}0.03 \\\\ 0.04\\end{pmatrix} \\right) = -\\frac{0.02}{0.19} \\begin{pmatrix}0.03 \\\\ 0.04\\end{pmatrix} = -\\frac{1}{19} \\begin{pmatrix}0.06 \\\\ 0.08\\end{pmatrix}$$\nThe diffusion term is:\n$$\\delta\\eta = 0.2 \\begin{pmatrix}0.5 \\\\ -1.2\\end{pmatrix} = \\begin{pmatrix}0.1 \\\\ -0.24\\end{pmatrix}$$\nSo, the proposed state is:\n$$\\theta' = \\begin{pmatrix}0.6 \\\\ -0.2\\end{pmatrix} - \\frac{1}{19} \\begin{pmatrix}0.06 \\\\ 0.08\\end{pmatrix} + \\begin{pmatrix}0.1 \\\\ -0.24\\end{pmatrix} = \\begin{pmatrix}0.7 \\\\ -0.44\\end{pmatrix} - \\begin{pmatrix}0.06/19 \\\\ 0.08/19\\end{pmatrix} = \\begin{pmatrix}13.3/19 - 0.06/19 \\\\ -8.36/19 - 0.08/19\\end{pmatrix} = \\frac{1}{19}\\begin{pmatrix}13.24 \\\\ -8.44\\end{pmatrix}$$\n\nTo compute the acceptance probability, we evaluate the log of the acceptance ratio $R = \\frac{\\pi(\\theta')q(\\theta|\\theta')}{\\pi(\\theta)q(\\theta'|\\theta)}$. For the specific case of a Gaussian target and a MALA proposal, this expression simplifies to:\n$$\\log R = - \\frac{\\delta^2}{8} \\left( \\|g(\\theta')\\|^2 - \\|g(\\theta)\\|^2 \\right)$$\nWe need to compute the squared norms of the gradients at $\\theta$ and $\\theta'$.\n$$g(\\theta) = -\\frac{1}{0.19} \\begin{pmatrix}0.03 \\\\ 0.04\\end{pmatrix} = -\\frac{1}{19} \\begin{pmatrix}3 \\\\ 4\\end{pmatrix}$$\n$$\\|g(\\theta)\\|^2 = \\left(-\\frac{1}{19}\\right)^2 (3^2 + 4^2) = \\frac{1}{361}(9 + 16) = \\frac{25}{361}$$\nNext, we compute $g(\\theta')$:\n$$\\theta'-\\mu = \\frac{1}{19}\\begin{pmatrix}13.24 \\\\ -8.44\\end{pmatrix} - \\begin{pmatrix}0.5 \\\\ -0.3\\end{pmatrix} = \\frac{1}{19}\\begin{pmatrix}13.24 \\\\ -8.44\\end{pmatrix} - \\frac{1}{19}\\begin{pmatrix}9.5 \\\\ -5.7\\end{pmatrix} = \\frac{1}{19}\\begin{pmatrix}3.74 \\\\ -2.74\\end{pmatrix}$$\n$$g(\\theta') = -\\Sigma^{-1}(\\theta'-\\mu) = -\\frac{1}{0.19} \\begin{pmatrix}0.4 & -0.1 \\\\ -0.1 & 0.5\\end{pmatrix} \\frac{1}{19}\\begin{pmatrix}3.74 \\\\ -2.74\\end{pmatrix} = -\\frac{100}{361} \\begin{pmatrix}0.4(3.74) - 0.1(-2.74) \\\\ -0.1(3.74) + 0.5(-2.74)\\end{pmatrix}$$\n$$g(\\theta') = -\\frac{100}{361} \\begin{pmatrix}1.496 + 0.274 \\\\ -0.374 - 1.37\\end{pmatrix} = -\\frac{100}{361} \\begin{pmatrix}1.77 \\\\ -1.744\\end{pmatrix}$$\n$$\\|g(\\theta')\\|^2 = \\left(-\\frac{100}{361}\\right)^2 (1.77^2 + (-1.744)^2) = \\frac{10000}{130321} (3.1329 + 3.041536) = \\frac{10000}{130321} (6.174436) = \\frac{61744.36}{130321}$$\nNow we compute the numerical values:\n$$\\|g(\\theta)\\|^2 = \\frac{25}{361} \\approx 0.06925208$$\n$$\\|g(\\theta')\\|^2 = \\frac{61744.36}{130321} \\approx 0.47378602$$\nWith $\\delta^2/8 = 0.04/8 = 0.005$, we find $\\log R$:\n$$\\log R = -0.005 \\times (0.47378602 - 0.06925208) = -0.005 \\times (0.40453394) = -0.00202267$$\nThe acceptance probability is $\\alpha = \\min(1, \\exp(\\log R))$. Since $\\log R  0$, we have:\n$$\\alpha = \\exp(-0.00202267) \\approx 0.99797936$$\nRounding to four significant figures, the result is $0.9980$.", "answer": "$$\\boxed{0.9980}$$", "id": "3289331"}, {"introduction": "The final step in mastering MCMC is translating theory into robust, working code. This comprehensive exercise challenges you to implement a complete Metropolis-Hastings sampler for a realistic model from computational physics, with a critical focus on numerical stability. By performing all calculations in the log-domain, you will learn how to prevent common numerical errors like underflow and overflow, a vital skill for applying Bayesian methods to real-world problems where probabilities can become vanishingly small. [@problem_id:3604531]", "problem": "You are tasked with implementing a numerically stable Metropolis–Hastings algorithm to perform Bayesian inference for a simple transmission experiment model used in computational nuclear physics. Your implementation must be a complete, runnable program, and all computations that could suffer from underflow or overflow must be performed in the log-domain where appropriate.\n\nConsider a monoenergetic beam transmission through a thin target at selected beam energies. Let the effective energy-dependent attenuation be modeled by\n$$\n\\Sigma(E;\\sigma_0,A) = \\sigma_0 + \\frac{A}{(E - E_r)^2 + \\gamma^2}\n$$\nwhere $E$ is the beam energy, $\\sigma_0 \\ge 0$ and $A \\ge 0$ are unknown nonnegative parameters to be inferred, and $E_r$ and $\\gamma$ are known constants. The expected transmitted counts at energy $E_i$ are modeled as\n$$\n\\lambda_i = I_0 \\exp\\left(-k \\, \\Sigma(E_i; \\sigma_0, A)\\right)\n$$\nwith known $I_0  0$ and $k  0$. The observed transmitted counts $y_i$ at each selected $E_i$ are independent and follow a Poisson distribution with mean $\\lambda_i$.\n\nYour task is to:\n- Represent the unknowns via a log-transformation $\\theta = (\\theta_0,\\theta_1)$ with $\\sigma_0 = \\exp(\\theta_0)$ and $A = \\exp(\\theta_1)$ to enforce nonnegativity.\n- Use independent Gaussian priors on $\\theta_j$ with given means and standard deviations.\n- Implement a random-walk Metropolis–Hastings sampler in the transformed space $\\theta$ using Gaussian proposals. Because the proposal distribution is symmetric, the Hastings correction cancels.\n- Compute all acceptance decisions using log-domain arithmetic to ensure numerical stability. In particular, do not compute the acceptance probability directly in linear scale; instead, compare $\\log u$ to the log-acceptance ratio, where $u$ is drawn from a Uniform distribution on $(0,1)$.\n- Compute the log-likelihood by summing the Poisson log-probabilities using only log-safe quantities. You must not directly evaluate $\\log(\\lambda_i)$ by first forming $\\lambda_i$ if doing so risks underflow; instead, compute $\\log(\\lambda_i)$ as $\\log(I_0) - k \\,\\Sigma(E_i; \\sigma_0, A)$ and $-\\lambda_i$ as $-\\exp(\\log(\\lambda_i))$. Use a numerically stable function for $\\log(y_i!)$.\n\nUse the following test suite, which specifies three synthetic experiments with different regimes. In each case, you must generate the synthetic data $y_i$ via the stated Poisson model using the specified global seed and seed offsets and then perform inference.\n\nCommon settings for all tests:\n- Energies $E = [6.0, 6.3, 6.6, 6.9, 7.2]$.\n- Resonance parameters $E_r = 6.6$ and $\\gamma = 0.05$.\n- Global base seed $s = 12345$. For test case index $c \\in \\{0,1,2\\}$, use the data-generation and sampling random number generator seed $s + c$.\n\nTest case 1 (moderate regime):\n- $I_0 = 200000$, $k = 5.0$.\n- True parameters for data generation: $\\sigma_0^{\\mathrm{true}} = 0.3$, $A^{\\mathrm{true}} = 3.0$.\n- Prior on $\\theta$: means $(\\mu_0,\\mu_1) = (\\log(0.3), \\log(3.0))$, standard deviations $(\\tau_0,\\tau_1) = (1.0, 1.0)$.\n- Proposal standard deviations for $(\\theta_0,\\theta_1)$: $(0.05, 0.10)$.\n- Metropolis–Hastings run length $N = 12000$, burn-in $B = 2000$.\n\nTest case 2 (extreme attenuation, many zeros expected):\n- $I_0 = 5000000$, $k = 50.0$.\n- True parameters for data generation: $\\sigma_0^{\\mathrm{true}} = 0.1$, $A^{\\mathrm{true}} = 10.0$.\n- Prior on $\\theta$: means $(\\mu_0,\\mu_1) = (\\log(0.2), \\log(2.0))$, standard deviations $(\\tau_0,\\tau_1) = (1.0, 1.0)$.\n- Proposal standard deviations for $(\\theta_0,\\theta_1)$: $(0.10, 0.20)$.\n- Metropolis–Hastings run length $N = 12000$, burn-in $B = 2000$.\n\nTest case 3 (nearly flat background, no resonance contribution):\n- $I_0 = 150000$, $k = 2.0$.\n- True parameters for data generation: $\\sigma_0^{\\mathrm{true}} = 0.25$, $A^{\\mathrm{true}} = 0.0$.\n- Prior on $\\theta$: means $(\\mu_0,\\mu_1) = (\\log(0.2), \\log(1.0))$, standard deviations $(\\tau_0,\\tau_1) = (1.0, 0.5)$.\n- Proposal standard deviations for $(\\theta_0,\\theta_1)$: $(0.05, 0.10)$.\n- Metropolis–Hastings run length $N = 12000$, burn-in $B = 2000$.\n\nRequirements:\n- Use the stated seeds to generate the synthetic counts and to drive the sampler.\n- For numerical stability, all acceptance decisions must be performed fully in the log domain, i.e., accept if $\\log u  \\min\\{0, \\log \\pi(\\theta') - \\log \\pi(\\theta)\\}$ where $\\pi$ is the unnormalized posterior density in the transformed space and $u \\sim \\mathrm{Uniform}(0,1)$.\n- For each test case, compute the posterior mean estimates of $\\sigma_0$ and $A$ from the post-burn-in samples, as well as the overall acceptance rate across the full run. These parameters are dimensionless in this simplified model, so no physical units are required.\n- Angle units are not applicable.\n- The final results must be aggregated across the three test cases and printed as a single line: a comma-separated Python-style list containing, in order, for each case, the posterior mean of $\\sigma_0$, the posterior mean of $A$, and the acceptance rate, i.e.,\n$$\n[\\overline{\\sigma_0}^{(1)}, \\overline{A}^{(1)}, \\alpha^{(1)}, \\overline{\\sigma_0}^{(2)}, \\overline{A}^{(2)}, \\alpha^{(2)}, \\overline{\\sigma_0}^{(3)}, \\overline{A}^{(3)}, \\alpha^{(3)}].\n$$\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]$). The numerical results must be floats.\n\nTest suite coverage intent:\n- Test case $1$ is a standard well-behaved regime.\n- Test case $2$ stresses log-domain stability because $\\lambda_i$ can be extremely small, generating many zeros.\n- Test case $3$ probes identifiability when the resonance contribution vanishes ($A \\approx 0$) and tests prior influence.\n\nImplement the program to carry out these tasks exactly and print the required single-line output. No user input is permitted, and the program must be fully deterministic under the specified seeds.", "solution": "The problem requires the implementation of a Metropolis-Hastings Markov Chain Monte Carlo (MCMC) algorithm to perform Bayesian inference on the parameters of a nuclear transmission model. The solution must be numerically stable, particularly through the consistent use of log-domain arithmetic. The process can be broken down into the formulation of the Bayesian model, the description of the MCMC sampling algorithm, and the final analysis of the generated samples.\n\n### 1. Bayesian Model Formulation\n\nThe core of the Bayesian approach is the posterior distribution, which combines prior knowledge about the parameters with information from the observed data (the likelihood).\n\n**Parameter Transformation:**\nThe physical parameters to be inferred are the background attenuation $\\sigma_0$ and the resonance amplitude $A$, both of which must be non-negative. To enforce this constraint naturally, we work with a transformed parameter space $\\theta = (\\theta_0, \\theta_1)$, where:\n$$\n\\sigma_0 = \\exp(\\theta_0) \\quad \\text{and} \\quad A = \\exp(\\theta_1)\n$$\nThis transformation maps the entire real line $(-\\infty, \\infty)$ for each $\\theta_j$ to the required positive domain $(0, \\infty)$ for $\\sigma_0$ and $A$.\n\n**Likelihood Function:**\nThe observed data consist of counts $y = \\{y_i\\}$ at a set of energies $E = \\{E_i\\}$. Each $y_i$ is modeled as an independent draw from a Poisson distribution with a mean $\\lambda_i$ that depends on the parameters $\\theta$:\n$$\ny_i \\sim \\text{Poisson}(\\lambda_i(\\theta))\n$$\nThe full likelihood for the dataset $y$ is the product of individual Poisson probabilities:\n$$\np(y | \\theta) = \\prod_{i} \\frac{\\lambda_i(\\theta)^{y_i} e^{-\\lambda_i(\\theta)}}{y_i!}\n$$\nThe mean count rate $\\lambda_i$ is determined by the Beer-Lambert law for attenuation:\n$$\n\\lambda_i(\\theta) = I_0 \\exp\\left(-k \\, \\Sigma(E_i; \\sigma_0, A)\\right)\n$$\nwhere $I_0$ is the incident beam intensity and $k$ is a constant related to the target thickness and number density. The energy-dependent attenuation cross-section $\\Sigma$ is modeled as a constant background plus a Lorentzian resonance:\n$$\n\\Sigma(E_i; \\sigma_0, A) = \\sigma_0 + \\frac{A}{(E_i - E_r)^2 + \\gamma^2}\n$$\nHere, $E_r$ and $\\gamma$ are known constants representing the resonance energy and width, respectively.\n\n**Prior Distribution:**\nThe prior distribution represents our knowledge about the parameters before observing the data. For the transformed parameters $\\theta_j$, independent Gaussian priors are specified:\n$$\np(\\theta) = p(\\theta_0)p(\\theta_1) = \\mathcal{N}(\\theta_0 | \\mu_0, \\tau_0^2) \\, \\mathcal{N}(\\theta_1 | \\mu_1, \\tau_1^2)\n$$\nwhere $(\\mu_0, \\mu_1)$ are the prior means and $(\\tau_0, \\tau_1)$ are the prior standard deviations for $\\theta_0$ and $\\theta_1$.\n\n### 2. Log-Posterior and Numerical Stability\n\nThe posterior distribution $\\pi(\\theta | y)$ is given by Bayes' theorem:\n$$\n\\pi(\\theta | y) \\propto p(y | \\theta) \\, p(\\theta)\n$$\nFor computational purposes, it is far more stable and convenient to work with the logarithm of the posterior distribution. Additive constants can be ignored since the MCMC algorithm only depends on ratios of posterior densities.\n$$\n\\log \\pi(\\theta | y) = \\log p(y | \\theta) + \\log p(\\theta) + \\text{constant}\n$$\nThe log-likelihood term is:\n$$\n\\log p(y | \\theta) = \\sum_i \\left( y_i \\log(\\lambda_i(\\theta)) - \\lambda_i(\\theta) - \\log(y_i!) \\right)\n$$\nTo prevent numerical underflow/overflow, especially in test cases with extreme attenuation where $\\lambda_i$ can be very close to zero, we must compute these terms carefully:\n1.  The term $\\log(\\lambda_i(\\theta))$ is computed directly from its definition to avoid first computing a potentially very small $\\lambda_i$:\n    $$\n    \\log(\\lambda_i(\\theta)) = \\log(I_0) - k \\, \\Sigma(E_i; \\exp(\\theta_0), \\exp(\\theta_1))\n    $$\n2.  The term $-\\lambda_i(\\theta)$ is computed as $-\\exp(\\log(\\lambda_i(\\theta)))$.\n3.  The log-factorial term $\\log(y_i!)$ is computed via the numerically stable log-gamma function, $\\log \\Gamma(y_i+1)$.\n\nThe log-prior term is the sum of two Gaussian log-probability densities (ignoring normalization constants):\n$$\n\\log p(\\theta) \\propto -\\frac{1}{2} \\left[ \\left(\\frac{\\theta_0 - \\mu_0}{\\tau_0}\\right)^2 + \\left(\\frac{\\theta_1 - \\mu_1}{\\tau_1}\\right)^2 \\right]\n$$\n\n### 3. The Metropolis-Hastings Algorithm\n\nWe employ a random-walk Metropolis-Hastings (RWMH) algorithm to draw samples from the posterior distribution $\\pi(\\theta | y)$.\n\n1.  **Initialization**: The chain is initialized at a starting point $\\theta^{(0)}$, chosen as the mean of the prior, $(\\mu_0, \\mu_1)$.\n2.  **Iteration**: For each step $t$ from $0$ to $N-1$:\n    a.  **Proposal**: A new candidate state $\\theta'$ is proposed by adding a random perturbation from a symmetric Gaussian distribution to the current state $\\theta^{(t)}$:\n        $$\n        \\theta' = \\theta^{(t)} + \\epsilon, \\quad \\text{where} \\quad \\epsilon \\sim \\mathcal{N}(0, \\Sigma_p)\n        $$\n        The proposal covariance $\\Sigma_p$ is diagonal, with variances given by the specified proposal standard deviations. Since the proposal is symmetric, the Hastings correction term is $1$.\n\n    b.  **Acceptance**: The candidate $\\theta'$ is accepted with probability $\\alpha(\\theta', \\theta^{(t)}) = \\min\\left(1, \\frac{\\pi(\\theta'|y)}{\\pi(\\theta^{(t)}|y)}\\right)$. To implement this decision stably, we compute the log of the ratio:\n        $$\n        \\log r = \\log\\pi(\\theta'|y) - \\log\\pi(\\theta^{(t)}|y)\n        $$\n        We then draw a uniform random number $u \\sim \\mathcal{U}(0,1)$ and accept the proposal if $\\log u  \\log r$. The problem specifies the equivalent and robust condition: accept if $\\log u  \\min(0, \\log r)$. If accepted, we set $\\theta^{(t+1)} = \\theta'$; otherwise, we set $\\theta^{(t+1)} = \\theta^{(t)}$ and the chain does not move.\n\n### 4. Data Simulation and Posterior Analysis\n\nFor each of the three test cases, we first simulate synthetic data and then perform inference.\n\n-   **Data Generation**: For a given test case with true parameters $(\\sigma_0^{\\text{true}}, A^{\\text{true}})$, we compute the true mean counts $\\lambda_i^{\\text{true}}$ for each energy $E_i$. The synthetic dataset $\\{y_i\\}$ is then generated by drawing from Poisson distributions $y_i \\sim \\text{Poisson}(\\lambda_i^{\\text{true}})$, using the specified random number generator seed for reproducibility.\n\n-   **Inference and Analysis**:\n    1.  The RWMH sampler is run for $N$ total iterations to produce a chain of samples $\\{\\theta^{(t)}\\}_{t=0}^{N-1}$.\n    2.  The initial $B$ samples (the burn-in period) are discarded to ensure that the remaining samples represent the stationary posterior distribution.\n    3.  The post-burn-in samples of $\\theta$ are transformed back to the original parameter space: $(\\sigma_0^{(t)}, A^{(t)}) = (\\exp(\\theta_0^{(t)}), \\exp(\\theta_1^{(t)}))$.\n    4.  The posterior means are estimated by averaging these samples:\n        $$\n        \\overline{\\sigma_0} = \\frac{1}{N-B} \\sum_{t=B}^{N-1} \\sigma_0^{(t)}, \\quad \\overline{A} = \\frac{1}{N-B} \\sum_{t=B}^{N-1} A^{(t)}\n        $$\n    5.  The overall acceptance rate $\\alpha$ is computed as the fraction of accepted proposals over the entire run of $N$ iterations.\n\nThis entire procedure is implemented in a single program, executed for each test case, and the final results are aggregated into the specified output format.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import gammaln\n\ndef solve():\n    \"\"\"\n    Main function to run the full analysis for all test cases and print the final result.\n    \"\"\"\n\n    # Common settings for all test cases\n    common_settings = {\n        'E': np.array([6.0, 6.3, 6.6, 6.9, 7.2]),\n        'Er': 6.6,\n        'gamma': 0.05,\n        'N': 12000,\n        'B': 2000,\n        'base_seed': 12345\n    }\n\n    # Test suite definition\n    test_cases = [\n        {\n            # Test case 1: Moderate regime\n            'case_idx': 0,\n            'I0': 200000.0,\n            'k': 5.0,\n            'true_params': {'sigma0': 0.3, 'A': 3.0},\n            'prior_means': (np.log(0.3), np.log(3.0)),\n            'prior_stds': (1.0, 1.0),\n            'proposal_stds': (0.05, 0.10)\n        },\n        {\n            # Test case 2: Extreme attenuation\n            'case_idx': 1,\n            'I0': 5000000.0,\n            'k': 50.0,\n            'true_params': {'sigma0': 0.1, 'A': 10.0},\n            'prior_means': (np.log(0.2), np.log(2.0)),\n            'prior_stds': (1.0, 1.0),\n            'proposal_stds': (0.10, 0.20)\n        },\n        {\n            # Test case 3: Nearly flat background\n            'case_idx': 2,\n            'I0': 150000.0,\n            'k': 2.0,\n            'true_params': {'sigma0': 0.25, 'A': 0.0},\n            'prior_means': (np.log(0.2), np.log(1.0)),\n            'prior_stds': (1.0, 0.5),\n            'proposal_stds': (0.05, 0.10)\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        # Combine common settings with case-specific settings\n        params = {**common_settings, **case}\n        \n        # Run the MCMC simulation for the current case\n        mean_sigma0, mean_A, acceptance_rate = run_mcmc_for_case(params)\n        \n        # Append results for this case\n        all_results.extend([mean_sigma0, mean_A, acceptance_rate])\n\n    # Print results in the required format\n    print(f\"[{','.join(map(str, all_results))}]\")\n\ndef sigma_attenuation(E, sigma0, A, Er, gamma):\n    \"\"\"\n    Computes the effective energy-dependent attenuation Sigma.\n    \"\"\"\n    return sigma0 + A / ((E - Er)**2 + gamma**2)\n\ndef log_posterior(theta, y, E, I0, k, Er, gamma, prior_means, prior_stds):\n    \"\"\"\n    Computes the log of the unnormalized posterior density.\n    All calculations are performed in the log domain for numerical stability.\n    \"\"\"\n    theta0, theta1 = theta\n    sigma0 = np.exp(theta0)\n    A = np.exp(theta1)\n    \n    # Log-prior calculation\n    log_prior_val = -0.5 * np.sum(((theta - prior_means) / prior_stds)**2)\n\n    # Log-likelihood calculation (vectorized over energies E)\n    Sigma_vec = sigma_attenuation(E, sigma0, A, Er, gamma)\n    log_lambda_vec = np.log(I0) - k * Sigma_vec\n    \n    # Poisson log-likelihood: y*log(lambda) - lambda - log(y!)\n    log_likelihood_val = np.sum(y * log_lambda_vec - np.exp(log_lambda_vec) - gammaln(y + 1))\n    \n    return log_likelihood_val + log_prior_val\n\ndef run_mcmc_for_case(params):\n    \"\"\"\n    Generates synthetic data and runs the Metropolis-Hastings sampler for a single test case.\n    \"\"\"\n    # Unpack parameters\n    E = params['E']\n    Er = params['Er']\n    gamma = params['gamma']\n    I0 = params['I0']\n    k = params['k']\n    true_params = params['true_params']\n    prior_means = np.array(params['prior_means'])\n    prior_stds = np.array(params['prior_stds'])\n    proposal_stds = np.array(params['proposal_stds'])\n    N = params['N']\n    B = params['B']\n    seed = params['base_seed'] + params['case_idx']\n\n    # Initialize random number generator with the specified seed\n    rng = np.random.default_rng(seed)\n\n    # --- 1. Generate Synthetic Data ---\n    sigma0_true = true_params['sigma0']\n    A_true = true_params['A']\n    \n    Sigma_true = sigma_attenuation(E, sigma0_true, A_true, Er, gamma)\n    lambda_true = I0 * np.exp(-k * Sigma_true)\n    y = rng.poisson(lambda_true)\n\n    # --- 2. Run Metropolis-Hastings Sampler ---\n    # Initialization\n    theta_current = np.array(prior_means)\n    log_post_current = log_posterior(theta_current, y, E, I0, k, Er, gamma, prior_means, prior_stds)\n\n    samples = np.zeros((N, 2))\n    n_accepted = 0\n\n    for i in range(N):\n        # Propose a new state (random walk)\n        theta_proposal = theta_current + rng.normal(0, proposal_stds)\n\n        # Calculate log posterior for the proposal\n        log_post_proposal = log_posterior(theta_proposal, y, E, I0, k, Er, gamma, prior_means, prior_stds)\n\n        # Acceptance step in log-domain\n        log_r = log_post_proposal - log_post_current\n        log_u = np.log(rng.uniform())\n\n        if log_u  log_r: # Equivalent to log_u  min(0, log_r)\n            theta_current = theta_proposal\n            log_post_current = log_post_proposal\n            n_accepted += 1\n        \n        samples[i] = theta_current\n\n    # --- 3. Post-processing ---\n    # Discard burn-in samples\n    post_burn_in_samples = samples[B:]\n\n    # Transform samples from log-space (theta) to original space (sigma0, A)\n    sigma0_samples = np.exp(post_burn_in_samples[:, 0])\n    A_samples = np.exp(post_burn_in_samples[:, 1])\n\n    # Compute posterior means\n    mean_sigma0 = np.mean(sigma0_samples)\n    mean_A = np.mean(A_samples)\n\n    # Compute acceptance rate\n    acceptance_rate = n_accepted / N\n\n    return mean_sigma0, mean_A, acceptance_rate\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3604531"}]}