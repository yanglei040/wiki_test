{"hands_on_practices": [{"introduction": "To effectively apply spectral variance estimation, we must first build a solid theoretical foundation. This practice provides a crucial first step by connecting the definition of a stochastic process to its spectral properties. By deriving the spectral density and long-run variance for the canonical autoregressive AR(1) process from first principles, you will gain a deep understanding of how the model's parameters directly shape the autocorrelation structure and, consequently, the variance of time-average estimators.", "problem": "Consider a strictly stationary, zero-mean, Gaussian autoregressive of order one (AR(1)) process, defined by $X_{t}=\\phi X_{t-1}+\\epsilon_{t}$ with $|\\phi|<1$, where the innovations $\\epsilon_{t}$ are independent and identically distributed with $\\epsilon_{t}\\sim \\mathcal{N}(0,\\sigma_{\\epsilon}^{2})$. In the context of spectral variance estimation for Monte Carlo time-average estimators, the spectral density $f(\\omega)$ of $\\{X_{t}\\}$ is defined as the Fourier transform of the autocovariance function $\\gamma_{k}=\\operatorname{Cov}(X_{t},X_{t+k})$,\n$$\nf(\\omega)=\\frac{1}{2\\pi}\\sum_{k=-\\infty}^{\\infty}\\gamma_{k}\\exp(-\\mathrm{i}k\\omega),\n$$\nand the long-run variance $\\sigma^{2}$ (also called the spectral variance) is defined either as the absolutely convergent series $\\sigma^{2}=\\gamma_{0}+2\\sum_{k=1}^{\\infty}\\gamma_{k}$ or equivalently as $\\sigma^{2}=2\\pi f(0)$.\n\nStarting from these definitions and the given stochastic difference equation, derive $f(0)$ explicitly in terms of $\\phi$ and $\\sigma_{\\epsilon}^{2}$, and hence obtain a closed-form expression for $\\sigma^{2}$. Provide your final answer as exact analytic expressions in terms of $\\phi$ and $\\sigma_{\\epsilon}^{2}$, listing $f(0)$ and $\\sigma^{2}$ in that order. No numerical approximation or rounding is required.", "solution": "The problem asks for the derivation of the spectral density at frequency zero, $f(0)$, and the long-run variance, $\\sigma^2$, for a strictly stationary, zero-mean, Gaussian AR(1) process defined by $X_{t}=\\phi X_{t-1}+\\epsilon_{t}$. The innovations $\\epsilon_{t}$ are i.i.d. with distribution $\\mathcal{N}(0,\\sigma_{\\epsilon}^{2})$, and the stationarity condition $|\\phi|<1$ holds.\n\nFirst, we determine the autocovariance function $\\gamma_{k}=\\operatorname{Cov}(X_{t},X_{t+k})$. Since the process is zero-mean, $\\gamma_{k} = \\operatorname{E}[X_{t}X_{t+k}]$.\nThe variance of the process, $\\gamma_{0}$, can be found by taking the variance of the defining equation:\n$$\n\\operatorname{Var}(X_{t}) = \\operatorname{Var}(\\phi X_{t-1} + \\epsilon_{t})\n$$\nDue to stationarity, $\\operatorname{Var}(X_{t}) = \\operatorname{Var}(X_{t-1}) = \\gamma_{0}$. The term $X_{t-1}$ is a function of past innovations $\\{\\epsilon_{t-1}, \\epsilon_{t-2}, \\dots\\}$ and is therefore uncorrelated with the current innovation $\\epsilon_{t}$. Thus, $\\operatorname{Cov}(X_{t-1}, \\epsilon_{t}) = 0$.\nThe variance of the sum of uncorrelated random variables is the sum of their variances:\n$$\n\\gamma_{0} = \\operatorname{Var}(\\phi X_{t-1}) + \\operatorname{Var}(\\epsilon_{t}) = \\phi^{2}\\operatorname{Var}(X_{t-1}) + \\sigma_{\\epsilon}^{2}\n$$\n$$\n\\gamma_{0} = \\phi^{2}\\gamma_{0} + \\sigma_{\\epsilon}^{2}\n$$\nSolving for $\\gamma_{0}$ yields:\n$$\n\\gamma_{0}(1-\\phi^{2}) = \\sigma_{\\epsilon}^{2} \\implies \\gamma_{0} = \\frac{\\sigma_{\\epsilon}^{2}}{1-\\phi^{2}}\n$$\nThe condition $|\\phi|<1$ ensures that the variance $\\gamma_{0}$ is finite and positive.\n\nNext, we find the autocovariance $\\gamma_{k}$ for lag $k > 0$:\n$$\n\\gamma_{k} = \\operatorname{E}[X_{t}X_{t+k}] = \\operatorname{E}[X_{t}(\\phi X_{t+k-1} + \\epsilon_{t+k})]\n$$\nBy linearity of expectation:\n$$\n\\gamma_{k} = \\phi \\operatorname{E}[X_{t}X_{t+k-1}] + \\operatorname{E}[X_{t}\\epsilon_{t+k}]\n$$\nFor $k>0$, the innovation $\\epsilon_{t+k}$ is uncorrelated with $X_{t}$. Since both have zero mean, their covariance equals their expectation product: $\\operatorname{E}[X_{t}\\epsilon_{t+k}]=\\operatorname{E}[X_t]\\operatorname{E}[\\epsilon_{t+k}]=0 \\cdot 0 = 0$.\nThe expression simplifies to a recurrence relation:\n$$\n\\gamma_{k} = \\phi \\operatorname{E}[X_{t}X_{t+k-1}] = \\phi \\gamma_{k-1}\n$$\nfor $k \\ge 1$. This implies $\\gamma_{k} = \\phi^{k}\\gamma_{0}$ for $k \\ge 0$. The autocovariance function is symmetric, $\\gamma_{-k} = \\gamma_{k}$, so we can write the general form as $\\gamma_{k} = \\phi^{|k|}\\gamma_{0}$ for any integer $k$.\n\nThe spectral density is defined as $f(\\omega)=\\frac{1}{2\\pi}\\sum_{k=-\\infty}^{\\infty}\\gamma_{k}\\exp(-\\mathrm{i}k\\omega)$. We are asked to find $f(0)$, which corresponds to setting $\\omega=0$:\n$$\nf(0) = \\frac{1}{2\\pi}\\sum_{k=-\\infty}^{\\infty}\\gamma_{k}\\exp(0) = \\frac{1}{2\\pi}\\sum_{k=-\\infty}^{\\infty}\\gamma_{k}\n$$\nSubstituting the expression for $\\gamma_k$:\n$$\nf(0) = \\frac{1}{2\\pi}\\sum_{k=-\\infty}^{\\infty}\\phi^{|k|}\\gamma_{0} = \\frac{\\gamma_{0}}{2\\pi}\\sum_{k=-\\infty}^{\\infty}\\phi^{|k|}\n$$\nThe summation can be split into two parts:\n$$\n\\sum_{k=-\\infty}^{\\infty}\\phi^{|k|} = \\sum_{k=0}^{\\infty}\\phi^{k} + \\sum_{k=-\\infty}^{-1}\\phi^{-k}\n$$\nThe first part is a standard geometric series which, for $|\\phi|<1$, converges to $\\frac{1}{1-\\phi}$. For the second part, let $j=-k$. As $k$ goes from $-1$ to $-\\infty$, $j$ goes from $1$ to $\\infty$:\n$$\n\\sum_{k=-\\infty}^{-1}\\phi^{-k} = \\sum_{j=1}^{\\infty}\\phi^{j} = \\frac{\\phi}{1-\\phi}\n$$\nCombining the two parts:\n$$\n\\sum_{k=-\\infty}^{\\infty}\\phi^{|k|} = \\frac{1}{1-\\phi} + \\frac{\\phi}{1-\\phi} = \\frac{1+\\phi}{1-\\phi}\n$$\nNow, substitute this sum back into the expression for $f(0)$:\n$$\nf(0) = \\frac{\\gamma_{0}}{2\\pi} \\left( \\frac{1+\\phi}{1-\\phi} \\right)\n$$\nFinally, substituting the expression for $\\gamma_{0} = \\frac{\\sigma_{\\epsilon}^{2}}{1-\\phi^{2}} = \\frac{\\sigma_{\\epsilon}^{2}}{(1-\\phi)(1+\\phi)}$:\n$$\nf(0) = \\frac{1}{2\\pi} \\left( \\frac{\\sigma_{\\epsilon}^{2}}{(1-\\phi)(1+\\phi)} \\right) \\left( \\frac{1+\\phi}{1-\\phi} \\right) = \\frac{\\sigma_{\\epsilon}^{2}}{2\\pi(1-\\phi)^{2}}\n$$\nThis is the closed-form expression for $f(0)$.\n\nThe long-run variance $\\sigma^2$ is given by the relation $\\sigma^2 = 2\\pi f(0)$. Using our result for $f(0)$:\n$$\n\\sigma^2 = 2\\pi \\left( \\frac{\\sigma_{\\epsilon}^{2}}{2\\pi(1-\\phi)^{2}} \\right) = \\frac{\\sigma_{\\epsilon}^{2}}{(1-\\phi)^{2}}\n$$\nThis is the closed-form expression for the long-run variance $\\sigma^2$.\nThe requested quantities are $f(0)$ and $\\sigma^2$.", "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{\\sigma_{\\epsilon}^{2}}{2\\pi(1-\\phi)^{2}} & \\frac{\\sigma_{\\epsilon}^{2}}{(1- \\phi)^{2}} \\end{pmatrix}}\n$$", "id": "3346157"}, {"introduction": "Theoretical derivations assume ideal conditions, but real-world Monte Carlo simulations can suffer from subtle algorithmic flaws, such as those arising from imperfect random number generators. This practice transitions from theory to diagnostics, challenging you to become a data detective. You will implement several spectral estimators from scratch and use them to identify characteristic signatures of lattice artifacts injected into a Markov chain Monte Carlo (MCMC) sampler, learning how tools like the periodogram and batch means can reveal pathologies that are invisible to simpler convergence metrics.", "problem": "Consider a stationary scalar time series generated by a reversible Markov Chain Monte Carlo (MCMC) sampler targeting a standard normal distribution. Let the time series be denoted by $\\{Y_t\\}_{t=1}^n$, where $Y_t = g(X_t)$ and $\\{X_t\\}$ is a Markov chain with invariant density proportional to $\\exp(-x^2/2)$. Assume $g(x) = x$ so that $Y_t = X_t$. Define the spectral density $S(\\omega)$ of $\\{Y_t\\}$ as the Fourier transform of its autocovariance function $\\gamma_k = \\mathrm{Cov}(Y_t,Y_{t+k})$, that is, $S(\\omega) = \\sum_{k=-\\infty}^{\\infty} \\gamma_k e^{-i \\omega k}$ for $\\omega \\in [-\\pi,\\pi]$. The zero-frequency spectral density $S(0)$ governs the asymptotic variance of the sample mean $\\bar{Y}_n$, in the sense that $\\mathrm{Var}(\\bar{Y}_n) \\approx S(0)/n$ under appropriate mixing conditions.\n\nYou will investigate the sensitivity of an estimator $\\hat{S}(0)$ to flaws in the Random Number Generator (RNG) by injecting lattice artifacts into MCMC transitions and quantifying deviations in the periodogram $I(\\omega)$ near $\\omega = 0$. Specifically:\n\n- Construct a Random-Walk Metropolis sampler with proposal $Y = X + \\sigma Z$, where $Z$ is standard normal. Use a symmetric proposal so that the Metropolis acceptance probability is $\\alpha(X,Y) = \\min\\{1, \\exp(-(Y^2 - X^2)/2)\\}$.\n- To inject lattice artifacts, quantize proposals to a grid of spacing $\\delta > 0$ by replacing $Y$ with $Y_q = \\delta \\,\\mathrm{round}(Y/\\delta)$ before applying the acceptance rule. This quantization is intended to mimic lattice structure introduced by flawed RNGs or deterministic artifacts.\n- Consider two types of RNGs for driving the proposal noise: a high-quality generator and a flawed linear congruential generator with a small modulus that exhibits short-period lattice behavior. Implement the flawed generator explicitly and use it to produce uniform variates transformed to normal variates via a method of your choice.\n\nYour tasks are:\n\n1. Generate the Markov chain for a burn-in period followed by $n$ iterations, and extract the time series $\\{Y_t\\}_{t=1}^n$ after centering by subtracting its sample mean.\n2. Estimate the zero-frequency spectral density using a lag-window estimator with a Bartlett kernel and a finite bandwidth. The estimator must be constructed from first principles, starting from autocovariance estimates of the centered series and an explicit taper function supported on a finite set of lags. Do not assume any shortcut formulas; use the core definitions of autocovariance and kernel weighting to build the estimator.\n3. Compute the periodogram $I(\\omega_j)$ at the discrete Fourier frequencies $\\omega_j = 2\\pi j/n$ for $j=0,\\dots,\\lfloor n/2 \\rfloor$ using the Fast Fourier Transform (FFT). Quantify deviations near zero frequency by forming the ratio $R = \\bar{I}_{\\text{low}}/\\bar{I}_{\\text{mid}}$, where $\\bar{I}_{\\text{low}}$ is the average of $I(\\omega_j)$ for the $J$ smallest positive frequencies and $\\bar{I}_{\\text{mid}}$ is the average over $J$ frequencies centered at $j \\approx n/4$.\n4. Form an independent reference estimator of $S(0)$ using the non-overlapping batch means method with a batch size that grows with $n$. Starting from the definition of batch means and their variance, derive and implement a consistent estimator for $S(0)$.\n5. Design a decision rule that flags under-dispersed spectral variance by comparing the Bartlett lag-window estimate of $S(0)$ to the batch-means reference and the near-zero periodogram ratio. Your rule should produce a boolean flag that is true if and only if both the Bartlett estimate is substantially smaller than the batch-means estimate and the near-zero periodogram ratio indicates suppressed low-frequency power.\n\nFor reproducibility and coverage, use the following test suite of parameter sets $(n,\\sigma,\\delta,\\text{rng},J)$:\n\n- Test 1 (happy path): $(8192, 1.25, 0.0, \\text{good}, 16)$\n- Test 2 (lattice artifact, moderate): $(8192, 1.25, 0.5, \\text{lcg}, 16)$\n- Test 3 (lattice artifact, strong): $(8192, 1.25, 1.0, \\text{lcg}, 16)$\n- Test 4 (boundary case, shorter chain): $(2048, 1.25, 0.0, \\text{good}, 8)$\n\nAll angles must be in radians; no physical units are involved. For each test case, your program must compute the boolean flag described in item 5. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[true,false,true,false]\"). The exact output must be a list of Python booleans \"True\" or \"False\" in order corresponding to the tests above.", "solution": "The problem requires an investigation into the sensitivity of spectral variance estimators to artifacts in a Random Number Generator (RNG) used within a Markov Chain Monte Carlo (MCMC) simulation. This will be accomplished by implementing a Metropolis sampler, injecting specific artifacts, and evaluating the resulting time series using several spectral analysis techniques. The final output is a decision rule that flags potential issues based on these analyses.\n\n### 1. MCMC Simulation Framework\n\nA Random-Walk Metropolis (RWM) sampler is constructed to generate a time series $\\{X_t\\}$ from a target distribution with probability density function $p(x) \\propto \\exp(-x^2/2)$, which is the standard normal distribution $\\mathcal{N}(0,1)$. The observable is $g(x)=x$, so the time series is $Y_t = X_t$.\n\nThe RWM algorithm proceeds as follows:\n1.  Given the current state $X_t$, a proposal $Y$ is generated from a proposal distribution $q(Y|X_t)$. We use a normal random walk: $Y = X_t + \\sigma Z$, where $Z \\sim \\mathcal{N}(0,1)$ and $\\sigma$ is the proposal step size.\n2.  An optional artifact is introduced by quantizing the proposal: $Y_q = \\delta \\cdot \\mathrm{round}(Y/\\delta)$. If $\\delta = 0$, no quantization is performed, and $Y_q = Y$.\n3.  The proposal $Y_q$ is accepted with probability $\\alpha(X_t, Y_q) = \\min\\left(1, \\frac{p(Y_q)}{p(X_t)}\\right)$. Since the proposal is symmetric, the Hastings ratio is $1$. The acceptance probability simplifies to $\\alpha(X_t, Y_q) = \\min\\left(1, \\exp\\left(-\\frac{1}{2}(Y_q^2 - X_t^2)\\right)\\right)$.\n4.  If the proposal is accepted, $X_{t+1} = Y_q$. Otherwise, the chain remains at its current state, $X_{t+1} = X_t$.\n\nA burn-in period of $2000$ iterations is used to allow the chain to converge to its stationary distribution before collecting $n$ samples for analysis. The initial state of the chain is $X_0 = 0$.\n\n### 2. Random Number Generators and Artifacts\n\nTwo types of RNGs are used to generate the standard normal variates $Z$ for the proposals.\n\n-   **High-Quality RNG**: The `numpy.random.default_rng()` generator, which is based on the PCG64 algorithm, serves as the high-quality, \"good\" RNG.\n-   **Flawed RNG**: A Linear Congruential Generator (LCG) with a small modulus is implemented to simulate a flawed RNG. The LCG is defined by the recurrence relation $S_{i+1} = (a S_i + c) \\pmod m$. We choose parameters known to produce poor results: $a=1229$, $c=1$, and $m=2^{16}$. Uniform variates $U_i \\in [0, 1)$ are produced via $U_i = S_i / m$. Pairs of these uniform variates are then transformed into standard normal variates using the Box-Muller transform:\n    $$Z_1 = \\sqrt{-2 \\ln U_1} \\cos(2\\pi U_2)$$\n    $$Z_2 = \\sqrt{-2 \\ln U_1} \\sin(2\\pi U_2)$$\n    This combination of a short-period LCG and the Box-Muller transform is known to produce structured artifacts in the resulting normal deviates.\n\n### 3. Spectral Analysis and Estimators of $S(0)$\n\nThe primary goal is to estimate the spectral density at zero frequency, $S(0) = \\sum_{k=-\\infty}^{\\infty} \\gamma_k$, where $\\gamma_k$ is the autocovariance of the stationary process at lag $k$. For the analysis, the raw time series $\\{Y_t\\}_{t=1}^n$ is first centered by subtracting its sample mean $\\bar{Y}_n = \\frac{1}{n}\\sum_{t=1}^n Y_t$. Let the centered series be $\\{y_t\\}_{t=1}^n$ where $y_t = Y_t - \\bar{Y}_n$.\n\n#### 3.1. Bartlett Lag-Window Estimator\n\nThis estimator is based on a weighted sum of sample autocovariances.\nThe sample autocovariance at lag $k \\ge 0$ is estimated as:\n$$ \\hat{\\gamma}_k = \\frac{1}{n} \\sum_{t=1}^{n-k} y_t y_{t+k} $$\nThe Bartlett lag-window estimator of $S(0)$ is given by:\n$$ \\hat{S}_B(0) = \\sum_{k=-(M-1)}^{M-1} \\lambda_B(k/M) \\hat{\\gamma}_k $$\nwhere $M$ is the truncation lag (bandwidth parameter) and $\\lambda_B(u) = 1 - |u|$ for $|u| \\le 1$ ($0$ otherwise) is the Bartlett kernel. Due to the symmetry $\\hat{\\gamma}_k = \\hat{\\gamma}_{-k}$, the estimator simplifies to:\n$$ \\hat{S}_B(0) = \\hat{\\gamma}_0 + 2 \\sum_{k=1}^{M-1} \\left(1 - \\frac{k}{M}\\right) \\hat{\\gamma}_k $$\nFor consistency, the truncation lag $M$ must grow with $n$ such that $M \\to \\infty$ and $M/n \\to 0$. A standard choice is $M = \\lfloor n^{1/3} \\rfloor$.\n\n#### 3.2. Non-Overlapping Batch Means (NOBM) Estimator\n\nThe NOBM method provides an alternative, consistent estimator for $S(0)$ under weak mixing conditions. It serves as a more robust reference estimator.\n1.  The time series $\\{Y_t\\}_{t=1}^n$ is divided into $a_n$ non-overlapping batches, each of size $m_n$. For simplicity, we assume $n = a_n m_n$.\n2.  The mean of each batch is calculated:\n    $$ B_j = \\frac{1}{m_n} \\sum_{i=1}^{m_n} Y_{(j-1)m_n + i} \\quad \\text{for} \\quad j = 1, \\dots, a_n $$\n3.  Under stationarity and mixing, for large $m_n$, the batch means $B_j$ are approximately independent and identically distributed normal random variables with $\\mathrm{Var}(B_j) \\approx S(0)/m_n$.\n4.  The variance of the grand sample mean $\\bar{Y}_n$ is related to the variance of the batch means: $\\mathrm{Var}(\\bar{Y}_n) \\approx \\frac{1}{a_n} \\mathrm{Var}(B_j)$.\n5.  Estimating $\\mathrm{Var}(B_j)$ by the sample variance of the batch means, we get an estimator for $S(0)$:\n    $$ \\hat{S}_{NOBM}(0) = m_n \\cdot \\frac{1}{a_n-1} \\sum_{j=1}^{a_n} (B_j - \\bar{Y}_n)^2 $$\n    where $\\bar{Y}_n$ is the overall sample mean.\nFor consistency, the batch size $m_n$ must grow with $n$. A standard choice is $m_n = \\lfloor n^{1/2} \\rfloor$, which implies $a_n = \\lfloor n/m_n \\rfloor \\approx \\lfloor n^{1/2} \\rfloor$.\n\n#### 3.3. Periodogram Analysis\n\nThe periodogram is an estimator of the spectral density at the discrete Fourier frequencies $\\omega_j = 2\\pi j/n$. It is calculated from the centered series $\\{y_t\\}$:\n$$ I(\\omega_j) = \\frac{1}{n} \\left| \\sum_{t=1}^n y_t e^{-i \\omega_j t} \\right|^2 $$\nThis is efficiently computed using the Fast Fourier Transform (FFT).\nTo quantify the suppression of power near zero frequency, we compute the ratio $R$:\n$$ R = \\frac{\\bar{I}_{\\text{low}}}{\\bar{I}_{\\text{mid}}} $$\nwhere:\n-   $\\bar{I}_{\\text{low}} = \\frac{1}{J} \\sum_{j=1}^{J} I(\\omega_j)$ is the average periodogram value over the $J$ lowest positive frequencies.\n-   $\\bar{I}_{\\text{mid}} = \\frac{1}{J} \\sum_{k} I(\\omega_k)$ is the average periodogram value over $J$ frequencies centered at the index $j_{\\text{mid}} = \\lfloor n/4 \\rfloor$.\n\nFor a process with positive autocorrelation, we expect $S(0) > S(\\omega)$ for $\\omega > 0$, which would typically yield $R > 1$. A value of $R < 1$ indicates an unusual depression of low-frequency power relative to mid-range frequencies, a potential sign of artifacts.\n\n### 4. Decision Rule for Artifact Detection\n\nA decision rule is formulated to flag a time series as potentially flawed. The rule is designed to detect under-dispersion in the spectral variance, a characteristic symptom of lattice artifacts. An alert is triggered (flag is `True`) if and only if both of the following conditions are met:\n\n1.  **Discrepancy between Estimators**: The Bartlett estimator is substantially smaller than the more robust NOBM estimator. The Bartlett estimator, being based on short-lag correlations, is sensitive to the local chain behavior altered by quantization, while the NOBM estimator captures longer-term variance. We formalize this as:\n    $$ \\hat{S}_B(0) < 0.5 \\cdot \\hat{S}_{NOBM}(0) $$\n2.  **Suppressed Low-Frequency Power**: The periodogram ratio $R$ indicates a deficit of power near zero frequency. This is quantified by the condition:\n    $$ R < 0.7 $$\n\nThis two-pronged rule combines evidence from both the time domain (via autocovariances and batching) and the frequency domain (via the periodogram) to make a robust decision.", "answer": "```python\nimport numpy as np\nfrom scipy.fft import fft\n\ndef solve():\n    \"\"\"\n    Solves the problem of detecting MCMC artifacts via spectral analysis.\n    \"\"\"\n\n    # Constants for the flawed Linear Congruential Generator (LCG)\n    LCG_A = 1229\n    LCG_C = 1\n    LCG_M = 2**16\n\n    # MCMC simulation parameters\n    N_BURN = 2000\n    X0 = 0.0\n\n    # Decision rule thresholds\n    BARTLETT_RATIO_THRESHOLD = 0.5\n    PERIODOGRAM_RATIO_THRESHOLD = 0.7\n\n    def lcg_generator(seed):\n        \"\"\"A generator for the flawed LCG yielding uniform variates.\"\"\"\n        state = seed\n        while True:\n            state = (LCG_A * state + LCG_C) % LCG_M\n            yield state / LCG_M\n\n    def box_muller_generator(uniform_gen):\n        \"\"\"A generator for normal variates using the Box-Muller transform.\"\"\"\n        while True:\n            u1 = next(uniform_gen)\n            u2 = next(uniform_gen)\n            if u1 == 0.0: continue\n            R = np.sqrt(-2 * np.log(u1))\n            theta = 2 * np.pi * u2\n            yield R * np.cos(theta)\n            yield R * np.sin(theta)\n\n    def generate_mcmc_series(n, sigma, delta, rng_type, seed):\n        \"\"\"\n        Generates a time series using a Random-Walk Metropolis sampler.\n        \"\"\"\n        if rng_type == 'good':\n            rng = np.random.default_rng(seed)\n            proposal_noise_gen = rng.standard_normal\n        elif rng_type == 'lcg':\n            uniform_gen = lcg_generator(seed=1) # Fixed seed for LCG\n            normal_gen = box_muller_generator(uniform_gen)\n            proposal_noise_gen = lambda: next(normal_gen)\n        else:\n            raise ValueError(\"Unknown RNG type\")\n\n        # Use a separate RNG for acceptance probability to avoid interaction\n        accept_rng = np.random.default_rng(seed + 1)\n\n        x = X0\n        # Burn-in phase\n        for _ in range(N_BURN):\n            z = proposal_noise_gen()\n            y = x + sigma * z\n            if delta > 0:\n                y = delta * np.round(y / delta)\n            \n            alpha = min(1.0, np.exp(-0.5 * (y**2 - x**2)))\n            if accept_rng.uniform() < alpha:\n                x = y\n        \n        # Sampling phase\n        series = np.zeros(n)\n        for i in range(n):\n            z = proposal_noise_gen()\n            y = x + sigma * z\n            if delta > 0:\n                y = delta * np.round(y / delta)\n\n            alpha = min(1.0, np.exp(-0.5 * (y**2 - x**2)))\n            if accept_rng.uniform() < alpha:\n                x = y\n            series[i] = x\n            \n        return series\n\n    def bartlett_estimator(series_centered, M):\n        \"\"\"Computes the Bartlett lag-window estimator for S(0).\"\"\"\n        n = len(series_centered)\n        if M >= n: M = n - 1\n\n        gamma_0 = np.var(series_centered) # equivalent to sum(y_t^2)/n\n        s0 = gamma_0\n        for k in range(1, M):\n            # Biased autocovariance estimator\n            gamma_k = np.dot(series_centered[:n-k], series_centered[k:]) / n\n            s0 += 2 * (1 - k / M) * gamma_k\n        return s0\n\n    def nobm_estimator(series, m_n):\n        \"\"\"Computes the Non-Overlapping Batch Means estimator for S(0).\"\"\"\n        n = len(series)\n        a_n = n // m_n\n        if a_n < 2: return np.nan\n\n        # Reshape into a_n batches of size m_n and compute batch means\n        batches = series[:a_n * m_n].reshape((a_n, m_n))\n        batch_means = np.mean(batches, axis=1)\n\n        # Variance of batch means\n        var_batch_means = np.var(batch_means, ddof=1)\n        \n        # S(0) estimate\n        s0 = m_n * var_batch_means\n        return s0\n    \n    def periodogram_ratio(series_centered, J):\n        \"\"\"Computes the ratio of low-frequency to mid-frequency periodogram averages.\"\"\"\n        n = len(series_centered)\n        \n        # Compute periodogram using FFT\n        fft_vals = fft(series_centered)\n        periodogram = (np.abs(fft_vals)**2) / n\n\n        # Indices for low frequencies (positive frequencies only, j=1 to J)\n        i_low = periodogram[1:J+1]\n        \n        # Indices for mid frequencies\n        j_mid = n // 4\n        j_start = j_mid - (J - 1) // 2\n        j_end = j_mid + J // 2\n        i_mid = periodogram[j_start:j_end+1]\n\n        if len(i_low) == 0 or len(i_mid) == 0 or np.mean(i_mid) == 0:\n            return np.nan\n            \n        return np.mean(i_low) / np.mean(i_mid)\n\n    # Test cases from the problem statement\n    test_cases = [\n        (8192, 1.25, 0.0, 'good', 16),\n        (8192, 1.25, 0.5, 'lcg', 16),\n        (8192, 1.25, 1.0, 'lcg', 16),\n        (2048, 1.25, 0.0, 'good', 8),\n    ]\n\n    results = []\n    for i, case in enumerate(test_cases):\n        n, sigma, delta, rng_type, J = case\n        \n        # Seed for reproducibility of each test case\n        case_seed = 42 + i\n\n        series = generate_mcmc_series(n, sigma, delta, rng_type, seed=case_seed)\n        \n        # Center the series for estimators that require it\n        centered_series = series - np.mean(series)\n        \n        # Determine parameters for estimators\n        M_bartlett = int(n**(1/3))\n        m_nobm = int(n**(1/2))\n        \n        # Compute estimators\n        s0_bartlett = bartlett_estimator(centered_series, M_bartlett)\n        # NOBM uses the original series to correctly compute batch means vs grand mean\n        s0_nobm = nobm_estimator(series, m_nobm)\n        p_ratio = periodogram_ratio(centered_series, J)\n\n        # Apply decision rule\n        if np.isnan(s0_bartlett) or np.isnan(s0_nobm) or np.isnan(p_ratio):\n            flag = False # Could not compute, assume no artifact\n        else:\n            cond1 = s0_bartlett < BARTLETT_RATIO_THRESHOLD * s0_nobm\n            cond2 = p_ratio < PERIODOGRAM_RATIO_THRESHOLD        \n            flag = cond1 and cond2\n        \n        results.append(flag)\n\n    # Format the final output exactly as specified\n    print(f\"[{','.join(str(r) for r in results)}]\")\n\nsolve()\n```", "id": "3346100"}, {"introduction": "Modern computational statistics often relies on running multiple simulations in parallel, which introduces the risk of subtle, hidden dependencies between supposedly independent chains. This advanced practice elevates our diagnostic toolkit to address this multi-chain context. You will design and implement a sophisticated test that uses block-wise spectral variance estimates as a new time series, employing correlation analysis and a permutation test to detect synchrony, thereby developing a powerful method for ensuring the integrity of large-scale parallel simulations.", "problem": "You are given a collection of $m$ discrete-time stochastic processes (chains) $\\{X_t^{(i)}\\}_{t=1}^n$ for $i \\in \\{1,\\dots,m\\}$, each intended to represent an independent realization of a stationary, ergodic Markov chain under the Monte Carlo paradigm. For each chain, you are interested in the spectral density at frequency zero, denoted $S^{(i)}(0)$, which governs the asymptotic variance of sample averages. Your task is to design and implement a diagnostic that compares estimates $\\hat{S}^{(i)}(0)$ across chains in order to detect hidden coupling induced by shared randomness or resource contention, which may create synchronous low-frequency variability across the chains. You must validate the diagnostic by comparing scenarios with and without injected synchronization.\n\nFundamental base and definitions to use:\n\n- A discrete-time, mean-zero, second-order stationary process $\\{Y_t\\}$ has autocovariance sequence $\\{\\gamma_k\\}_{k \\in \\mathbb{Z}}$ defined by $\\gamma_k = \\mathrm{Cov}(Y_t, Y_{t+k})$ and spectral density $S(\\omega)$ defined as the Fourier transform of $\\gamma_k$, that is\n$$\nS(\\omega) = \\frac{1}{2\\pi} \\sum_{k=-\\infty}^{\\infty} \\gamma_k e^{-i \\omega k}, \\quad \\omega \\in [-\\pi,\\pi].\n$$\n- The value at zero frequency satisfies\n$$\nS(0) = \\frac{1}{2\\pi} \\sum_{k=-\\infty}^{\\infty} \\gamma_k = \\frac{1}{2\\pi}\\left(\\gamma_0 + 2 \\sum_{k=1}^{\\infty} \\gamma_k\\right).\n$$\n- For a stationary, ergodic process with finite second moments, the variance of the sample mean $\\bar{Y}_n = \\frac{1}{n} \\sum_{t=1}^n Y_t$ satisfies\n$$\n\\mathrm{Var}(\\bar{Y}_n) \\approx \\frac{1}{n} \\sum_{k=-\\infty}^{\\infty} \\gamma_k = \\frac{2\\pi}{n} S(0),\n$$\nunder standard mixing conditions as $n \\to \\infty$, linking $S(0)$ to the long-run variance.\n\nSpectral variance estimation to implement:\n\n- Given observed data $y_1,\\dots,y_n$ from a chain, define the centered series $z_t = y_t - \\bar{y}_n$, where $\\bar{y}_n = \\frac{1}{n} \\sum_{t=1}^n y_t$.\n- The sample autocovariance at lag $k \\in \\{0,1,\\dots,n-1\\}$ is\n$$\n\\hat{\\gamma}_k = \\frac{1}{n} \\sum_{t=1}^{n-k} z_t z_{t+k}.\n$$\n- A lag-window estimator with a symmetric window $w_k$ and bandwidth $b$ estimates the spectral density at zero as\n$$\n\\hat{S}(0) = \\hat{\\gamma}_0 + 2 \\sum_{k=1}^b w_k \\hat{\\gamma}_k,\n$$\nwith the Bartlett window $w_k = 1 - \\frac{k}{b+1}$ for $k \\in \\{0,1,\\dots,b\\}$ and $w_k = 0$ for $k > b$.\n\nDiagnostic design constraints:\n\n- Under independence across chains, the blockwise estimates of $S(0)$ for different chains should not be systematically synchronized over time beyond estimation noise. Hidden coupling (for example, due to shared randomness or synchronized slowdowns) can induce common low-frequency variability, causing the blockwise $\\hat{S}(0)$ estimates to co-move across chains.\n- Segment each chain into $B$ non-overlapping contiguous blocks of equal length $L = \\left\\lfloor \\frac{n}{B} \\right\\rfloor$. For each chain $i$ and block $b \\in \\{1,\\dots,B\\}$, compute a block-level estimate $\\hat{S}_b^{(i)}(0)$ using the lag-window estimator with bandwidth $b^\\star = \\min\\{b, L-1\\}$ and the data from that block.\n- Form the $B \\times m$ matrix whose $(b,i)$ entry is $\\hat{S}_b^{(i)}(0)$, and compute the average off-diagonal Pearson correlation across chains using the $m \\times m$ sample correlation matrix of the blockwise columns. Denote the observed statistic by\n$$\n\\bar{r}_{\\mathrm{obs}} = \\frac{2}{m(m-1)} \\sum_{1 \\le i < j \\le m} r_{ij},\n$$\nwhere $r_{ij}$ is the sample correlation between the sequences $\\{\\hat{S}_b^{(i)}(0)\\}_{b=1}^B$ and $\\{\\hat{S}_b^{(j)}(0)\\}_{b=1}^B$.\n- To obtain a data-driven detection threshold while preserving the chain-wise marginal distributions of $\\{\\hat{S}_b^{(i)}(0)\\}$, perform a block-permutation test that breaks synchrony but leaves the per-chain block distributions intact: independently permute the $B$ blocks within each chain, recompute the average off-diagonal correlation $\\bar{r}^{(\\ell)}$ for permutation replicate $\\ell$, and estimate the $p$-value as\n$$\np = \\frac{1 + \\sum_{\\ell=1}^R \\mathbf{1}\\{\\bar{r}^{(\\ell)} \\ge \\bar{r}_{\\mathrm{obs}}\\}}{1 + R},\n$$\nwith $R$ random permutations. Declare hidden coupling detected if $p < \\alpha$ for a chosen significance level $\\alpha$.\n\nData-generating mechanisms to implement:\n\n- For each chain $i \\in \\{1,\\dots,m\\}$, simulate a process using an autoregressive model of order one with a possible shared component:\n$$\nX_t^{(i)} = \\phi X_{t-1}^{(i)} + \\varepsilon_t^{(i)} + a \\, S_t,\\quad \\varepsilon_t^{(i)} \\sim \\mathcal{N}(0,\\sigma^2)\\ \\text{i.i.d. across } t \\text{ and } i,\n$$\nwhere the shared component $S_t$ evolves as\n$$\nS_t = \\rho S_{t-1} + \\eta_t,\\quad \\eta_t \\sim \\mathcal{N}(0,\\sigma_c^2)\\ \\text{i.i.d.},\n$$\nwith $S_0 = 0$ and $X_0^{(i)} = 0$. The parameter $a$ controls the strength of hidden coupling across chains: $a = 0$ corresponds to independence, while $a > 0$ injects common low-frequency variability across all chains.\n- Use $g(x) = x$ as the function of interest; that is, estimate $S(0)$ directly on $\\{X_t^{(i)}\\}$.\n\nTest suite:\n\nImplement and evaluate the diagnostic on the following four test cases, each specified as a tuple $(m, n, \\phi, \\sigma, \\rho, \\sigma_c, a, b, B, R, \\alpha)$:\n\n1. Case A (independence baseline): $(6, 12000, 0.6, 1.0, 0.98, 1.0, 0.0, 150, 12, 199, 0.01)$.\n2. Case B (subtle coupling): $(6, 12000, 0.6, 1.0, 0.98, 1.0, 0.3, 150, 12, 199, 0.01)$.\n3. Case C (stronger coupling): $(6, 12000, 0.6, 1.0, 0.98, 1.0, 0.55, 150, 12, 199, 0.01)$.\n4. Case D (boundary robustness under high persistence, no coupling): $(6, 12000, 0.95, 1.0, 0.98, 1.0, 0.0, 300, 12, 199, 0.01)$.\n\nRequired outputs:\n\n- For each case, output a boolean that is $\\mathrm{True}$ if hidden coupling is detected by the diagnostic (that is, $p < \\alpha$) and $\\mathrm{False}$ otherwise.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, \"[$\\mathrm{False}$,$\\mathrm{True}$,$\\mathrm{True}$,$\\mathrm{False}$]\"). There are no physical units or angles involved; all outputs are unitless booleans.\n\nImplementation requirements:\n\n- Use the Bartlett lag-window estimator for $S(0)$ with bandwidth $b^\\star = \\min\\{b, L-1\\}$ within each block.\n- Use the permutation test described above with exactly $R$ permutations for each case.\n- Use a fixed random seed to ensure reproducibility across runs.\n\nYour final program must be a complete, runnable script that implements the simulation, estimation, diagnostic, and test suite as described, and prints the list of four booleans on a single line in the exact format specified. No additional text or formatting should be printed.", "solution": "The problem requires the design and implementation of a statistical diagnostic to detect hidden coupling among a set of $m$ parallel Monte Carlo simulation chains. The diagnostic is based on the principle that if chains are truly independent, their long-run variance estimates, when computed over successive time blocks, should not exhibit systematic correlation. Conversely, shared randomness or resource contention can induce common low-frequency variability, causing these estimates to co-move. We will first formalize the data generation process, then the estimation of the spectral variance, followed by the design of the correlation-based diagnostic and its validation via a permutation test.\n\n**1. Data-Generating Process**\n\nThe foundation of our test is a simulated dataset comprising $m$ stochastic processes, or chains. Each chain is modeled as a first-order autoregressive process, AR($1$), with an additive shared component that introduces cross-chain coupling.\n\nFirst, a common shock process, $\\{S_t\\}_{t=1}^n$, is generated. This process is itself an AR($1$) model, intended to represent a source of low-frequency variability that may affect all chains simultaneously. It is defined by:\n$$\nS_t = \\rho S_{t-1} + \\eta_t, \\quad \\text{for } t=1, \\dots, n\n$$\nwith initial condition $S_0 = 0$. The innovations $\\eta_t$ are independent and identically distributed (i.i.d.) Gaussian random variables with mean $0$ and variance $\\sigma_c^2$, i.e., $\\eta_t \\sim \\mathcal{N}(0, \\sigma_c^2)$. The parameter $\\rho \\in [0, 1)$ governs the persistence of the shared shocks; values of $\\rho$ close to $1$ generate strong, low-frequency fluctuations.\n\nNext, for each chain $i \\in \\{1, \\dots, m\\}$, the process $\\{X_t^{(i)}\\}_{t=1}^n$ is generated as:\n$$\nX_t^{(i)} = \\phi X_{t-1}^{(i)} + \\varepsilon_t^{(i)} + a \\, S_t, \\quad \\text{for } t=1, \\dots, n\n$$\nwith initial conditions $X_0^{(i)} = 0$. The innovations $\\varepsilon_t^{(i)}$ are i.i.d. Gaussian random variables, $\\varepsilon_t^{(i)} \\sim \\mathcal{N}(0, \\sigma^2)$, and are independent across both time $t$ and chains $i$. The parameter $\\phi \\in [0, 1)$ is the autoregressive coefficient for the individual chain's dynamics. The parameter $a \\ge 0$ controls the strength of the coupling:\n- If $a=0$, the term $a S_t$ vanishes, and the chains $\\{X_t^{(i)}\\}$ are mutually independent AR($1$) processes. This corresponds to the null hypothesis of no coupling.\n- If $a > 0$, the shared component $S_t$ is injected into all chains, inducing a positive correlation among them. This corresponds to the alternative hypothesis of hidden coupling.\n\n**2. Blockwise Spectral Variance Estimation**\n\nThe diagnostic quanitity is the spectral density at frequency zero, $S(0)$, which is proportional to the asymptotic variance of the sample mean of a stationary process. We do not estimate $S(0)$ from the entire chain at once. Instead, to capture time-varying synchrony, we divide each chain of length $n$ into $B$ non-overlapping blocks of length $L = \\lfloor n/B \\rfloor$.\n\nFor each chain $i$ and each block $b \\in \\{1, \\dots, B\\}$, we compute a block-level estimate of the spectral variance, denoted $\\hat{S}_b^{(i)}(0)$. Let the data for block $b$ of chain $i$ be $\\{y_t\\}_{t=1}^L$. The estimation proceeds as follows:\n\n- **Centering:** The data are centered by subtracting the block mean: $z_t = y_t - \\bar{y}_L$, where $\\bar{y}_L = \\frac{1}{L} \\sum_{t=1}^L y_t$.\n- **Bandwidth Selection:** The bandwidth for the lag-window estimator is set to $b^\\star = \\min(b_{\\text{param}}, L-1)$, where $b_{\\text{param}}$ is the base bandwidth parameter provided in the test case. This ensures the number of lags does not exceed the available data in the block.\n- **Sample Autocovariances:** The sample autocovariances $\\hat{\\gamma}_k$ are computed for lags $k = 0, 1, \\dots, b^\\star$:\n$$\n\\hat{\\gamma}_k = \\frac{1}{L} \\sum_{t=1}^{L-k} z_t z_{t+k}\n$$\n- **Lag-Window Estimator:** The spectral variance at frequency zero (defined in the problem as $2\\pi S(0)$) is estimated using the Bartlett lag-window estimator:\n$$\n\\hat{S}_b^{(i)}(0) = \\hat{\\gamma}_0 + 2 \\sum_{k=1}^{b^\\star} w_k \\hat{\\gamma}_k\n$$\nwhere the Bartlett window weights are given by $w_k = 1 - \\frac{k}{b^\\star+1}$ for $k \\in \\{1, \\dots, b^\\star\\}$.\n\nThis procedure yields a $B \\times m$ matrix of estimates, where the entry $(b,i)$ is $\\hat{S}_b^{(i)}(0)$. The sequence $\\{\\hat{S}_b^{(i)}(0)\\}_{b=1}^B$ is a time series representing the evolution of the estimated spectral variance for chain $i$.\n\n**3. Diagnostic Test Statistic and Permutation Test**\n\nThe core idea of the diagnostic is that if the chains are coupled ($a>0$), the time series of their blockwise spectral variance estimates, $\\{\\hat{S}_b^{(i)}(0)\\}_{b=1}^B$, will tend to move together. We quantify this co-movement using the average off-diagonal Pearson correlation.\n\n- **Observed Test Statistic:** Let $M$ be the $B \\times m$ matrix of blockwise estimates $\\hat{S}_b^{(i)}(0)$. We compute the $m \\times m$ sample correlation matrix of the columns of $M$. Let $r_{ij}$ be the correlation between the $i$-th and $j$-th columns. The observed test statistic, $\\bar{r}_{\\mathrm{obs}}$, is the average of the upper (or lower) triangular elements of this correlation matrix:\n$$\n\\bar{r}_{\\mathrm{obs}} = \\frac{2}{m(m-1)} \\sum_{1 \\le i < j \\le m} r_{ij}\n$$\n A large positive value of $\\bar{r}_{\\mathrm{obs}}$ suggests synchrony.\n\n- **Permutation Test for Significance:** To determine if $\\bar{r}_{\\mathrm{obs}}$ is statistically significant, we compare it to a null distribution generated via a permutation test. This non-parametric approach is crucial because the theoretical distribution of $\\bar{r}_{\\mathrm{obs}}$ is complex. The permutation test preserves the marginal distribution of spectral variance estimates within each chain but breaks any temporal synchrony across chains.\nThe procedure is as follows:\n    1. For a large number of replications, $R$:\n    2. For each replicate $\\ell \\in \\{1, \\dots, R\\}$, create a new matrix $M^{(\\ell)}$ by independently permuting the $B$ block estimates within each of the $m$ columns of the original matrix $M$.\n    3. Compute the test statistic $\\bar{r}^{(\\ell)}$ from this permuted matrix $M^{(\\ell)}$.\n    4. The collection $\\{\\bar{r}^{(\\ell)}\\}_{\\ell=1}^R$ forms an empirical distribution of the test statistic under the null hypothesis of no synchrony.\n    5. The $p$-value is estimated as the proportion of permuted statistics that are greater than or equal to the observed statistic, with an adjustment to avoid a $p$-value of $0$:\n$$\np = \\frac{1 + \\sum_{\\ell=1}^R \\mathbf{1}\\{\\bar{r}^{(\\ell)} \\ge \\bar{r}_{\\mathrm{obs}}\\}}{1 + R}\n$$\nwhere $\\mathbf{1}\\{\\cdot\\}$ is the indicator function.\n\n- **Decision:** Given a significance level $\\alpha$, we reject the null hypothesis of independence and declare that hidden coupling is detected if the computed $p$-value is less than $\\alpha$.\n\nThis complete procedure is applied to each of the four test cases specified in the problem, yielding a boolean result (True for detection, False otherwise) for each case. The fixed random seed ensures that the simulation and permutation test are reproducible.", "answer": "```python\nimport numpy as np\n\ndef generate_data(m, n, phi, sigma, rho, sigma_c, a):\n    \"\"\"\n    Generates m chains of data according to the specified AR(1) model with a shared component.\n    \"\"\"\n    # Generate the shared component S_t\n    S = np.zeros(n + 1)\n    eta = np.random.normal(0, sigma_c, size=n)\n    for t in range(1, n + 1):\n        S[t] = rho * S[t - 1] + eta[t - 1]\n\n    # Generate the m chains X_t^(i)\n    X = np.zeros((m, n + 1))\n    epsilon = np.random.normal(0, sigma, size=(m, n))\n    for i in range(m):\n        for t in range(1, n + 1):\n            X[i, t] = phi * X[i, t - 1] + epsilon[i, t - 1] + a * S[t]\n\n    return X[:, 1:]  # Return m x n array, discarding initial zeros\n\ndef estimate_spectral_variance(series, bandwidth):\n    \"\"\"\n    Computes the Bartlett lag-window estimator for spectral variance at frequency zero.\n    \"\"\"\n    L = len(series)\n    if L < 2:\n        return 0.0\n    \n    # Effective bandwidth cannot exceed L-1 lags\n    b_star = min(bandwidth, L - 1)\n    \n    # Center the series\n    z = series - np.mean(series)\n\n    # Compute sample autocovariances\n    # gamma_0\n    gamma_0 = np.sum(z * z) / L\n    \n    # gamma_k for k > 0\n    sum_term = 0.0\n    if b_star > 0:\n        for k in range(1, b_star + 1):\n            gamma_k = np.sum(z[:-k] * z[k:]) / L\n            w_k = 1.0 - k / (b_star + 1.0)\n            sum_term += w_k * gamma_k\n\n    return gamma_0 + 2.0 * sum_term\n\ndef avg_off_diag_corr(matrix):\n    \"\"\"\n    Computes the average off-diagonal Pearson correlation of a matrix's columns.\n    \"\"\"\n    m = matrix.shape[1]\n    if m < 2:\n        return 0.0\n    \n    # rowvar=False treats columns as variables\n    corr_matrix = np.corrcoef(matrix, rowvar=False)\n    \n    # For a single variable or constant series, corrcoef can return nan or a scalar\n    if corr_matrix.ndim < 2:\n        return 0.0\n\n    # Sum of off-diagonal elements\n    np.fill_diagonal(corr_matrix, 0)\n    sum_off_diag = np.sum(corr_matrix)\n    \n    return sum_off_diag / (m * (m - 1))\n\ndef run_diagnostic(params):\n    \"\"\"\n    Runs the full diagnostic test for a given set of parameters.\n    \"\"\"\n    m, n, phi, sigma, rho, sigma_c, a, b_param, B, R, alpha = params\n\n    # 1. Generate Data\n    chains = generate_data(m, n, phi, sigma, rho, sigma_c, a)\n\n    # 2. Blockwise Spectral Variance Estimation\n    L = n // B\n    if L == 0:\n      raise ValueError(\"Block length L must be positive.\")\n      \n    S_hat_matrix = np.zeros((B, m))\n\n    for i in range(m):\n        for j in range(B):\n            block_data = chains[i, j * L : (j + 1) * L]\n            S_hat_matrix[j, i] = estimate_spectral_variance(block_data, b_param)\n\n    # 3. Compute Observed Test Statistic\n    r_obs = avg_off_diag_corr(S_hat_matrix)\n\n    # 4. Perform Permutation Test\n    perm_r_values = np.zeros(R)\n    perm_S_hat_matrix = np.copy(S_hat_matrix)\n    \n    # Use a generator for permutations for efficiency and cleaner state management\n    rng = np.random.default_rng()\n\n    for l in range(R):\n        for i in range(m):\n            rng.shuffle(perm_S_hat_matrix[:, i])\n        perm_r_values[l] = avg_off_diag_corr(perm_S_hat_matrix)\n\n    # 5. Calculate p-value and make a decision\n    p_value = (1.0 + np.sum(perm_r_values >= r_obs)) / (1.0 + R)\n    detection = p_value < alpha\n    \n    return detection\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    # Set a fixed random seed for reproducibility\n    np.random.seed(42)\n\n    # Define the test cases from the problem statement\n    test_cases = [\n        # (m, n, phi, sigma, rho, sigma_c, a, b, B, R, alpha)\n        (6, 12000, 0.6, 1.0, 0.98, 1.0, 0.0, 150, 12, 199, 0.01),   # Case A\n        (6, 12000, 0.6, 1.0, 0.98, 1.0, 0.3, 150, 12, 199, 0.01),   # Case B\n        (6, 12000, 0.6, 1.0, 0.98, 1.0, 0.55, 150, 12, 199, 0.01), # Case C\n        (6, 12000, 0.95, 1.0, 0.98, 1.0, 0.0, 300, 12, 199, 0.01)   # Case D\n    ]\n\n    results = []\n    for case in test_cases:\n        result = run_diagnostic(case)\n        results.append(result)\n\n    # Final print statement in the exact required format\n    # The boolean values are converted to lowercase strings for Python's `str()`\n    # and then capitalized to match the example format `[False,True,...]`\n    print(f\"[{','.join(map(lambda b: str(b).capitalize(), results))}]\")\n\nsolve()\n```", "id": "3346140"}]}