{"hands_on_practices": [{"introduction": "A deep understanding of crude Monte Carlo methods begins with analyzing the estimator's variance. This exercise [@problem_id:3301580] provides a foundational look at this concept by asking you to derive the exact variance for a simple but important integrand: the indicator function. By working through this problem, you will see how the estimator's variability is directly tied to the properties of the integrand, specifically the probability of a random sample being \"successful,\" which is a core principle in analyzing Monte Carlo efficiency.", "problem": "Consider crude Monte Carlo integration on the domain $D=[0,1]$ for the integrand $h(x)=\\mathbf{1}\\{x\\le t\\}$, where $t\\in[0,1]$ is fixed and $\\mathbf{1}\\{\\cdot\\}$ denotes the indicator function. Let $X_{1},X_{2},\\dots,X_{n}$ be independent and identically distributed samples from the continuous uniform distribution on $[0,1]$, denoted $X_{i}\\sim\\mathrm{Uniform}(0,1)$, and define the crude Monte Carlo estimator of the integral $I=\\int_{0}^{1}h(x)\\,dx$ by\n$$\n\\hat I_{n}=\\frac{1}{n}\\sum_{i=1}^{n}h(X_{i}).\n$$\nStarting from the core definitions of expectation and variance for independent and identically distributed random variables, derive an exact closed-form expression for $\\mathrm{Var}(\\hat I_{n})$ as a function of $n$ and $t$ when $h(x)=\\mathbf{1}\\{x\\le t\\}$. Then, purely in terms of $t$, interpret how the choice of $t$ affects the variability of $\\hat I_{n}$ by identifying the value(s) of $t$ that minimize and maximize the variance and explaining the qualitative behavior across $t\\in[0,1]$. The final answer must be the closed-form expression for $\\mathrm{Var}(\\hat I_{n})$. No rounding is required.", "solution": "The problem statement is well-defined, self-contained, and scientifically grounded in the principles of probability theory and Monte Carlo methods. It is a standard problem in stochastic simulation. Therefore, the problem is valid, and a solution will be derived.\n\nThe objective is to derive a closed-form expression for the variance of the crude Monte Carlo estimator $\\mathrm{Var}(\\hat I_{n})$. The estimator is given by\n$$\n\\hat I_{n}=\\frac{1}{n}\\sum_{i=1}^{n}h(X_{i}),\n$$\nwhere $X_{1}, X_{2}, \\dots, X_{n}$ are independent and identically distributed (i.i.d.) random variables from a $\\mathrm{Uniform}(0,1)$ distribution, and the integrand is $h(x)=\\mathbf{1}\\{x\\le t\\}$ for a fixed $t\\in[0,1]$.\n\nWe begin by applying the properties of variance. Given that the samples $X_i$ are i.i.d., the transformed random variables $Y_i = h(X_i)$ are also i.i.d.\nThe variance of the estimator $\\hat I_n$ is:\n$$\n\\mathrm{Var}(\\hat I_{n}) = \\mathrm{Var}\\left(\\frac{1}{n}\\sum_{i=1}^{n}h(X_{i})\\right)\n$$\nUsing the property $\\mathrm{Var}(aZ) = a^2\\mathrm{Var}(Z)$ with $a = \\frac{1}{n}$ and $Z = \\sum_{i=1}^{n}h(X_{i})$, we get:\n$$\n\\mathrm{Var}(\\hat I_{n}) = \\frac{1}{n^2}\\mathrm{Var}\\left(\\sum_{i=1}^{n}h(X_{i})\\right)\n$$\nSince the random variables $h(X_i)$ are independent, the variance of their sum is the sum of their variances:\n$$\n\\mathrm{Var}\\left(\\sum_{i=1}^{n}h(X_{i})\\right) = \\sum_{i=1}^{n}\\mathrm{Var}(h(X_{i}))\n$$\nFurthermore, since the $h(X_i)$ are identically distributed, their variances are all equal. Let $\\mathrm{Var}(h(X_i)) = \\sigma_h^2$ for all $i=1,\\dots,n$.\n$$\n\\sum_{i=1}^{n}\\mathrm{Var}(h(X_{i})) = n \\cdot \\mathrm{Var}(h(X_1))\n$$\nSubstituting this back into the expression for $\\mathrm{Var}(\\hat I_{n})$:\n$$\n\\mathrm{Var}(\\hat I_{n}) = \\frac{1}{n^2} \\left( n \\cdot \\mathrm{Var}(h(X_1)) \\right) = \\frac{1}{n}\\mathrm{Var}(h(X_1))\n$$\nNow, we must compute $\\mathrm{Var}(h(X_1))$. We use the formula $\\mathrm{Var}(Z) = \\mathrm{E}[Z^2] - (\\mathrm{E}[Z])^2$. In our case, $Z = h(X_1) = \\mathbf{1}\\{X_1 \\le t\\}$.\n\nFirst, we calculate the expectation $\\mathrm{E}[h(X_1)]$.\nThe random variable $h(X_1)$ can take only two values: $1$ if $X_1 \\le t$, and $0$ if $X_1  t$. This is a Bernoulli random variable. Its expectation is the probability that it equals $1$.\n$$\np = P(h(X_1) = 1) = P(X_1 \\le t)\n$$\nSince $X_1 \\sim \\mathrm{Uniform}(0,1)$, its probability density function (PDF) is $f(x) = 1$ for $x \\in [0,1]$ and $0$ otherwise. Its cumulative distribution function (CDF) for $x \\in [0,1]$ is $F(x) = x$. For the given $t \\in [0,1]$:\n$$\nP(X_1 \\le t) = \\int_0^t f(x) \\, dx = \\int_0^t 1 \\, dx = t\n$$\nThus, $h(X_1)$ is a Bernoulli random variable with parameter $p=t$.\nThe expectation is:\n$$\n\\mathrm{E}[h(X_1)] = t\n$$\nNext, we calculate $\\mathrm{E}[(h(X_1))^2]$. Since $h(X_1)$ is an indicator function, its values are either $0$ or $1$. Therefore, $(h(X_1))^2$ is identical to $h(X_1)$ because $0^2=0$ and $1^2=1$.\n$$\n(h(X_1))^2 = h(X_1)\n$$\nThis implies:\n$$\n\\mathrm{E}[(h(X_1))^2] = \\mathrm{E}[h(X_1)] = t\n$$\nNow we can compute the variance of $h(X_1)$:\n$$\n\\mathrm{Var}(h(X_1)) = \\mathrm{E}[(h(X_1))^2] - (\\mathrm{E}[h(X_1)])^2 = t - t^2 = t(1-t)\n$$\nThis is the well-known variance of a Bernoulli($t$) random variable.\n\nFinally, we substitute this result into our expression for $\\mathrm{Var}(\\hat I_{n})$:\n$$\n\\mathrm{Var}(\\hat I_{n}) = \\frac{1}{n} \\mathrm{Var}(h(X_1)) = \\frac{t(1-t)}{n}\n$$\nThis is the exact closed-form expression for the variance of the estimator as a function of $n$ and $t$.\n\nFor the interpretation, we analyze how the variance $\\mathrm{Var}(\\hat I_{n})$ changes with $t \\in [0,1]$ for a fixed number of samples $n$. The variability is determined by the term $v(t) = t(1-t)$. This is a quadratic function of $t$, representing a downward-opening parabola with roots at $t=0$ and $t=1$.\n- **Minimization**: The minimum value of $v(t)$ on the interval $[0,1]$ occurs at the endpoints.\n  - At $t=0$, $v(0) = 0(1-0) = 0$, so $\\mathrm{Var}(\\hat I_n) = 0$. In this case, $h(x) = \\mathbf{1}\\{x \\le 0\\}$, which is $0$ for all $x \\in (0,1]$. All samples $h(X_i)$ will be $0$ with probability $1$, so the estimator has zero variance.\n  - At $t=1$, $v(1) = 1(1-1) = 0$, so $\\mathrm{Var}(\\hat I_n) = 0$. In this case, $h(x) = \\mathbf{1}\\{x \\le 1\\}$, which is $1$ for all $x \\in [0,1]$. All samples $h(X_i)$ will be $1$, so the estimator again has zero variance.\nIn both cases, the integrand is constant over the effective support of the sampling distribution, leading to no sampling error.\n- **Maximization**: The maximum value of the parabola $v(t) = -t^2 + t$ occurs at its vertex, which is at $t = -\\frac{1}{2(-1)} = \\frac{1}{2}$. At this point, the value is $v(\\frac{1}{2}) = \\frac{1}{2}(1-\\frac{1}{2}) = \\frac{1}{4}$.\n  - The variance is maximized at $t=1/2$, yielding $\\mathrm{Var}(\\hat I_n) = \\frac{1}{4n}$. This corresponds to the case of maximum uncertainty for the Bernoulli trials $h(X_i)$. When $t=1/2$, a sample $X_i$ is equally likely to be less than or greater than $1/2$, meaning $h(X_i)$ is equally likely to be $1$ or $0$. This maximal randomness in the individual samples results in the highest possible variance for their average.\n\nIn summary, the variability of the Monte Carlo estimator is zero when the problem is deterministic ($t=0$ or $t=1$) and is maximal when the uncertainty of each sample is highest ($t=1/2$).", "answer": "$$\\boxed{\\frac{t(1-t)}{n}}$$", "id": "3301580"}, {"introduction": "While the standard Central Limit Theorem provides the theoretical bedrock for many Monte Carlo applications, it is crucial to understand its limitations. This thought experiment [@problem_id:3301536] confronts a scenario where the integrand is \"heavy-tailed,\" leading to infinite variance in the samples. You will investigate how this violation of a key assumption invalidates the standard CLT and instead leads to different convergence rates and limit distributions, a critical concept for robust numerical work.", "problem": "Consider crude Monte Carlo integration of the integral $I=\\int_{0}^{1} h(x)\\,dx$ over the domain $D=[0,1]$. Let $(X_i)_{i\\ge 1}$ be an independent and identically distributed sequence with $X_i\\sim \\mathrm{Uniform}(0,1)$. The crude Monte Carlo estimator is $\\widehat I_n=\\frac{1}{n}\\sum_{i=1}^n h(X_i)$. You are asked to select the correct statements regarding a specific heavy-tailed integrand $h$ and its implications for variance estimation and limit theorems.\n\nTake the integrand $h:D\\to [1,\\infty)$ defined by $h(x)=x^{-3/4}$ for $x\\in(0,1]$ and $h(0)=+\\infty$ (interpreted in the Lebesgue sense so that $h\\in L^1([0,1])$ but the point singularity at $x=0$ is integrable). For the estimator $\\widehat I_n$, consider the following statements:\n\nA. $h\\in L^1([0,1])$ but $h\\notin L^2([0,1])$.\n\nB. $\\widehat I_n$ is unbiased and satisfies the Strong Law of Large Numbers (SLLN), and the usual Central Limit Theorem (CLT) with $\\sqrt{n}$-normalization and finite asymptotic variance applies.\n\nC. $\\mathrm{Var}(h(X_1))=+\\infty$, and no consistent estimator of $\\mathrm{Var}(h(X_1))$ exists; in particular, the sample variance $S_n^2=\\frac{1}{n-1}\\sum_{i=1}^n\\big(h(X_i)-\\widehat I_n\\big)^2$ diverges to $+\\infty$ almost surely as $n\\to\\infty$.\n\nD. The fluctuations of $\\widehat I_n$ obey a heavy-tailed generalized limit theorem: $n^{1/4}\\big(\\widehat I_n - I\\big)$ converges in distribution to a non-degenerate $\\alpha$-stable law with tail index $\\alpha=\\frac{4}{3}$ (up to centering), whereas $\\sqrt{n}\\big(\\widehat I_n - I\\big)$ diverges in probability.\n\nSelect all correct statements.", "solution": "The user wants a meticulous validation of the problem statement, followed by a full solution deriving the correct answer and evaluating all options.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n-   The integral to be estimated is $I=\\int_{0}^{1} h(x)\\,dx$.\n-   The domain of integration is $D=[0,1]$.\n-   The sampling is done using an independent and identically distributed (i.i.d.) sequence of random variables $(X_i)_{i\\ge 1}$, where $X_i\\sim \\mathrm{Uniform}(0,1)$.\n-   The crude Monte Carlo estimator is given by $\\widehat I_n=\\frac{1}{n}\\sum_{i=1}^n h(X_i)$.\n-   The integrand is the function $h:D\\to [1,\\infty)$ defined by $h(x)=x^{-3/4}$ for $x\\in(0,1]$ and $h(0)=+\\infty$.\n-   It is specified that $h$ is interpreted in the Lebesgue sense and $h\\in L^1([0,1])$.\n\n**Step 2: Validate Using Extracted Givens**\n-   **Scientific Grounding:** The problem is firmly situated in the mathematical fields of numerical integration, probability theory, and statistics, specifically concerning Monte Carlo methods. It tests the understanding of $L^p$ spaces, the Law of Large Numbers (LLN), the Central Limit Theorem (CLT), and the Generalized Central Limit Theorem for heavy-tailed distributions (stable laws). All concepts are standard and rigorously defined.\n-   **Well-Posedness:** The problem defines a specific integrand, a sampling distribution, and an estimator. It then asks to verify several statements about the mathematical properties of this setup. These are well-posed questions with unique, verifiable answers.\n-   **Objectivity:** The problem is stated using precise, objective mathematical language. Terms like \"unbiased\", \"Strong Law of Large Numbers\", \"converges in distribution\", and \"$\\alpha$-stable law\" have unambiguous definitions.\n-   **Flaw Checklist:**\n    1.  **Scientific/Factual Unsoundness:** None. The setup is a canonical example used to illustrate the breakdown of the standard CLT and the emergence of stable laws.\n    2.  **Non-Formalizable or Irrelevant:** None. The problem is perfectly formalizable.\n    3.  **Incomplete or Contradictory Setup:** None. The problem provides all necessary information. The statement that $h\\in L^1([0,1])$ is a premise that can be independently verified, confirming the consistency of the setup.\n    4.  **Unrealistic or Infeasible:** Not applicable as it's a purely mathematical problem. The mathematical constructs are valid.\n    5.  **Ill-Posed or Poorly Structured:** None.\n    6.  **Pseudo-Profound, Trivial, or Tautological:** The problem is neither trivial nor tautological. It requires a solid understanding of a non-trivial interplay between integrability conditions and limit theorems in probability.\n    7.  **Outside Scientific Verifiability:** All claims are mathematically verifiable.\n\n**Step 3: Verdict and Action**\nThe problem statement is valid. I will proceed with the solution.\n\n### Solution Derivation\n\nLet $Y_i = h(X_i)$ be the sampled values. Since the $X_i$ are i.i.d. $\\mathrm{Uniform}(0,1)$, the $Y_i$ are also i.i.d. random variables. The crude Monte Carlo estimator is the sample mean of these variables, $\\widehat I_n = \\frac{1}{n}\\sum_{i=1}^n Y_i$.\n\nThe core of the problem lies in analyzing the moments of the random variable $Y_1 = h(X_1)$. The $k$-th moment is given by $E[Y_1^k] = E[(h(X_1))^k]$. By the law of the unconscious statistician, this is:\n$$E[(h(X_1))^k] = \\int_0^1 (h(x))^k f_{X_1}(x) dx$$\nSince $X_1 \\sim \\mathrm{Uniform}(0,1)$, its probability density function is $f_{X_1}(x) = 1$ for $x \\in [0,1]$. Thus:\n$$E[(h(X_1))^k] = \\int_0^1 (h(x))^k dx = \\int_0^1 (x^{-3/4})^k dx = \\int_0^1 x^{-3k/4} dx$$\nThis integral is a standard $p$-integral of the form $\\int_0^1 x^{-p} dx$, which converges if and only if $p  1$. In our case, the condition for convergence is $\\frac{3k}{4}  1$, or $k  \\frac{4}{3}$.\n\n**1. First Moment (Expectation):**\nFor $k=1$, we have $p = \\frac{3(1)}{4} = \\frac{3}{4}  1$. The integral converges.\n$$I = E[Y_1] = \\int_0^1 x^{-3/4} dx = \\left[ \\frac{x^{-3/4+1}}{-3/4+1} \\right]_0^1 = \\left[ \\frac{x^{1/4}}{1/4} \\right]_0^1 = [4x^{1/4}]_0^1 = 4(1) - 4(0) = 4$$\nThe expectation of $h(X_1)$ is finite and equal to $I=4$. This confirms that $h \\in L^1([0,1])$.\n\n**2. Second Moment and Variance:**\nFor $k=2$, we have $p = \\frac{3(2)}{4} = \\frac{3}{2}  1$. The integral diverges.\n$$E[Y_1^2] = \\int_0^1 x^{-3/2} dx = \\left[ \\frac{x^{-1/2}}{-1/2} \\right]_0^1 = [-2x^{-1/2}]_0^1$$\nThis expression diverges to $+\\infty$ as $x \\to 0^+$.\nTherefore, $E[Y_1^2] = +\\infty$.\nThe variance of $Y_1$ is $\\mathrm{Var}(Y_1) = E[Y_1^2] - (E[Y_1])^2 = +\\infty - 4^2 = +\\infty$. The variance is infinite.\n\nWith these foundational results, we can evaluate each statement.\n\n### Option-by-Option Analysis\n\n**Statement A: $h\\in L^1([0,1])$ but $h\\notin L^2([0,1])$.**\n-   The space $L^p([0,1])$ consists of functions $f$ such that $\\int_0^1 |f(x)|^p dx  \\infty$.\n-   For $p=1$, we check $\\int_0^1 |h(x)| dx$. Since $h(x) = x^{-3/4} \\ge 1  0$, this is $\\int_0^1 x^{-3/4} dx$. As calculated above, this integral evaluates to $4$, which is finite. Thus, $h \\in L^1([0,1])$.\n-   For $p=2$, we check $\\int_0^1 |h(x)|^2 dx = \\int_0^1 (x^{-3/4})^2 dx = \\int_0^1 x^{-3/2} dx$. As calculated above, this integral diverges. Thus, $h \\notin L^2([0,1])$.\n-   The statement is a conjunction of two true facts.\n-   Verdict: **Correct**.\n\n**Statement B: $\\widehat I_n$ is unbiased and satisfies the Strong Law of Large Numbers (SLLN), and the usual Central Limit Theorem (CLT) with $\\sqrt{n}$-normalization and finite asymptotic variance applies.**\n-   **Unbiasedness:** The expected value of the estimator is $E[\\widehat I_n] = E[\\frac{1}{n}\\sum_{i=1}^n h(X_i)] = \\frac{1}{n}\\sum_{i=1}^n E[h(X_i)]$. Since $E[h(X_i)] = I = 4$ for all $i$, we have $E[\\widehat I_n] = \\frac{1}{n}(n \\cdot I) = I$. The estimator is unbiased. This part is true.\n-   **SLLN:** Kolmogorov's Strong Law of Large Numbers states that for a sequence of i.i.d. random variables $Y_i$, if $E[|Y_1|]  \\infty$, then the sample mean converges almost surely to the expectation. Here $Y_i = h(X_i)$. Since $h(x) \\ge 1$, $|h(X_1)| = h(X_1)$, and we found $E[h(X_1)] = 4  \\infty$. Therefore, the SLLN applies, and $\\widehat I_n \\to I$ almost surely. This part is true.\n-   **CLT:** The standard Lindeberg-Lévy Central Limit Theorem states that for a sequence of i.i.d. random variables $Y_i$ with finite mean $\\mu$ and finite variance $\\sigma^2  0$, the normalized sum $\\sqrt{n}(\\frac{1}{n}\\sum Y_i - \\mu)$ converges in distribution to a normal distribution $N(0, \\sigma^2)$. A necessary condition for this theorem is that the variance $\\sigma^2 = \\mathrm{Var}(Y_1)$ must be finite. We have established that $\\mathrm{Var}(h(X_1)) = +\\infty$. Therefore, the standard CLT does not apply.\n-   The statement incorrectly claims that the usual CLT applies.\n-   Verdict: **Incorrect**.\n\n**Statement C: $\\mathrm{Var}(h(X_1))=+\\infty$, and no consistent estimator of $\\mathrm{Var}(h(X_1))$ exists; in particular, the sample variance $S_n^2=\\frac{1}{n-1}\\sum_{i=1}^n\\big(h(X_i)-\\widehat I_n\\big)^2$ diverges to $+\\infty$ almost surely as $n\\to\\infty$.**\n-   **Variance:** As shown, $\\mathrm{Var}(h(X_1)) = E[(h(X_1))^2] - (E[h(X_1)])^2 = +\\infty$. This part is true.\n-   **Sample Variance Behavior:** Let $Y_i = h(X_i)$. The sample variance is $S_n^2 = \\frac{1}{n-1}\\left(\\sum_{i=1}^n Y_i^2 - n(\\widehat I_n)^2\\right) = \\frac{n}{n-1}\\left(\\frac{1}{n}\\sum_{i=1}^n Y_i^2 - (\\widehat I_n)^2\\right)$.\n    -   As $n\\to\\infty$, the pre-factor $\\frac{n}{n-1} \\to 1$.\n    -   By the SLLN (Statement B), $\\widehat I_n \\to I = 4$ almost surely. Thus $(\\widehat I_n)^2 \\to 16$ almost surely. This term is convergent and bounded.\n    -   Consider the term $\\frac{1}{n}\\sum_{i=1}^n Y_i^2$. The variables $Z_i = Y_i^2 = (h(X_i))^2$ are i.i.d. and non-negative. Their expectation is $E[Z_i] = E[Y_i^2] = +\\infty$. A known extension of the SLLN states that if $Z_i$ are i.i.d. non-negative random variables with $E[Z_1] = +\\infty$, then $\\frac{1}{n}\\sum_{i=1}^n Z_i \\to +\\infty$ almost surely.\n    -   Combining these results, $S_n^2$ behaves like $1 \\cdot (+\\infty - 16)$, which diverges to $+\\infty$ almost surely.\n-   The claim that $S_n^2$ diverges to $+\\infty$ a.s. is correct. This is the behavior one would expect from an estimator of an infinite quantity.\n-   Verdict: **Correct**.\n\n**Statement D: The fluctuations of $\\widehat I_n$ obey a heavy-tailed generalized limit theorem: $n^{1/4}\\big(\\widehat I_n - I\\big)$ converges in distribution to a non-degenerate $\\alpha$-stable law with tail index $\\alpha=\\frac{4}{3}$ (up to centering), whereas $\\sqrt{n}\\big(\\widehat I_n - I\\big)$ diverges in probability.**\n-   **Generalized CLT:** Since the variance of $Y_1=h(X_1)$ is infinite, we must investigate if it lies in the domain of attraction of a stable law. We analyze the tail probability $P(Y_1  y)$. For $y \\ge 1$:\n    $$P(Y_1  y) = P(h(X_1)  y) = P(X_1^{-3/4}  y) = P(X_1  y^{-4/3})$$\n    Since $X_1 \\sim \\mathrm{Uniform}(0,1)$, this probability is $P(Y_1  y) = y^{-4/3}$.\n    This shows that the tail of the distribution of $Y_1$ is regularly varying with index $\\alpha = 4/3$.\n-   Since $1  \\alpha  2$, the random variable $Y_1$ is in the domain of attraction of an $\\alpha$-stable law. The Generalized CLT applies. For a sum of i.i.d. variables from such a distribution, the proper normalization for convergence to a stable law is $n^{1/\\alpha}$.\n-   The convergence result is for the centered sum: $\\frac{1}{n^{1/\\alpha}} \\sum_{i=1}^n (Y_i - E[Y_i]) \\xrightarrow{d} S$, where $S$ is an $\\alpha$-stable random variable.\n-   With $\\alpha = 4/3$, the normalization is $n^{1/(4/3)} = n^{3/4}$. The limit theorem is: $\\frac{1}{n^{3/4}} \\sum_{i=1}^n (h(X_i) - I) \\xrightarrow{d} S$.\n-   Let's check the expression in the statement:\n    $n^{1/4}(\\widehat I_n - I) = n^{1/4}\\left(\\frac{1}{n}\\sum_{i=1}^n h(X_i) - I\\right) = \\frac{n^{1/4}}{n} \\sum_{i=1}^n (h(X_i) - I) = \\frac{1}{n^{3/4}} \\sum_{i=1}^n (h(X_i) - I)$.\n    This is precisely the correctly normalized quantity. The statement that it converges in distribution to a non-degenerate $\\alpha$-stable law with $\\alpha=4/3$ is correct.\n-   **Divergence of $\\sqrt{n}$-normalized term:** We can write $\\sqrt{n}(\\widehat I_n - I)$ as:\n    $$\\sqrt{n}(\\widehat I_n - I) = n^{1/2}(\\widehat I_n - I) = n^{1/2-1/4} \\cdot \\left(n^{1/4}(\\widehat I_n - I)\\right) = n^{1/4} \\cdot Z_n$$\n    where $Z_n = n^{1/4}(\\widehat I_n - I)$. We just showed that $Z_n$ converges in distribution to a non-degenerate stable law $S$. A sequence of random variables that converges in distribution is stochastically bounded (tight). Since $n^{1/4} \\to \\infty$ and $Z_n$ converges to a random variable that is not zero almost surely, the product $n^{1/4} Z_n$ must diverge in probability.\n-   Verdict: **Correct**.", "answer": "$$\\boxed{ACD}$$", "id": "3301536"}, {"introduction": "Theoretical understanding is best solidified through practical application. This exercise [@problem_id:3301576] challenges you to build a complete Monte Carlo pipeline, from designing a custom sampler to estimating integrals. You will implement a rejection sampling algorithm to draw points from a non-standard domain and then use those samples to perform crude Monte Carlo integration, bridging the gap between abstract formulas and concrete computational results.", "problem": "You are asked to design, analyze, and implement a rejection sampling algorithm to draw samples from the Uniform distribution (Unif) on the set $$D=\\{x\\in[0,1]^2:\\,x_1+x_2\\le 1\\}$$ and to use these samples for crude Monte Carlo (MC) integration. Begin from first principles: the definition of the Uniform distribution on a measurable set, the definition of rejection sampling, and the definition of crude Monte Carlo integration as the estimation of integrals by sampling from a distribution whose expectation is related to the integral of interest.\n\nConstruct a rejection sampling scheme that uses proposals from the Uniform distribution on the square $$S=[0,1]^2,$$ with acceptance determined by whether a proposed point lies in $$D.$$ Derive, from the foundational definitions, the acceptance probability of this scheme, and justify that the accepted draws are distributed as Uniform on $$D.$$\n\nImplement the algorithm to generate a specified number of accepted samples, denote this number by $$N_{\\text{acc}},$$ and record the total number of proposals $$N_{\\text{prop}}$$ used. Compute the empirical acceptance rate $$\\hat{\\alpha}=N_{\\text{acc}}/N_{\\text{prop}}.$$ Using the accepted samples $$\\{X^{(i)}\\}_{i=1}^{N_{\\text{acc}}}$$ with $$X^{(i)}\\in D,$$ estimate the integrals\n$$I_1=\\int_D x_1 x_2\\,\\mathrm{d}x \\quad\\text{and}\\quad I_2=\\int_D (x_1+x_2)\\,\\mathrm{d}x,$$\nvia crude Monte Carlo integration by relating these integrals to expectations under the Uniform distribution on $$D$$ and the area of $$D.$$\n\nYour program must implement the following deterministic test suite and produce results for each test case:\n\n- Test case $$1$$: Random number generator (RNG) seed $$12345$$ and accepted sample target $$N_{\\text{acc}}=1.$$\n- Test case $$2$$: RNG seed $$202310$$ and accepted sample target $$N_{\\text{acc}}=5000.$$\n- Test case $$3$$: RNG seed $$42$$ and accepted sample target $$N_{\\text{acc}}=20000.$$\n\nFor each test case, run the rejection sampler until exactly $$N_{\\text{acc}}$$ points have been accepted, then:\n- Compute and return the empirical acceptance rate $$\\hat{\\alpha}$$ as a floating-point number.\n- Compute and return the crude Monte Carlo estimates of $$I_1$$ and $$I_2$$ as floating-point numbers.\n- Return the total number of proposals $$N_{\\text{prop}}$$ as an integer.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case’s result is itself a list $$[\\hat{\\alpha},\\,\\widehat{I}_1,\\,\\widehat{I}_2,\\,N_{\\text{prop}}].$$ For example, the final output should look like $$[[\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot]].$$ No physical units or angles are involved in this problem. All randomness must be controlled by the specified RNG seeds to ensure reproducibility. Use inclusive acceptance on the boundary, i.e., accept when $$x_1+x_2\\le 1.$$", "solution": "The problem is valid as it is scientifically grounded in probability theory and numerical methods, is well-posed, objective, self-contained, and computationally feasible. We proceed with a complete solution.\n\nThe solution will be developed from first principles, beginning with the foundational definitions of the relevant mathematical and algorithmic concepts.\n\n**1. Theoretical Framework**\n\n**a. The Target Distribution**\nThe objective is to draw samples from the Uniform distribution on the set $$D = \\{x \\in [0,1]^2 : x_1+x_2 \\le 1\\}$$. The set $$D$$ is a right triangle in the plane with vertices at $$(0,0)$$, $$(1,0)$$, and $$(0,1)$$. The area of this set, denoted as $$\\text{Area}(D)$$, is a standard geometric result:\n$$\n\\text{Area}(D) = \\frac{1}{2} \\times \\text{base} \\times \\text{height} = \\frac{1}{2} \\times 1 \\times 1 = \\frac{1}{2}\n$$\nThe probability density function (PDF) of a random variable $$X$$ that is uniformly distributed on a measurable set $$A \\subset \\mathbb{R}^d$$ is defined as:\n$$\nf_X(x) = \\begin{cases} 1/\\text{Area}(A)  \\text{if } x \\in A \\\\ 0  \\text{otherwise} \\end{cases}\n$$\nTherefore, the target PDF, which we denote as $$p(x)$$, for the Uniform distribution on $$D$$ is:\n$$\np(x) = \\begin{cases} 1/(1/2) = 2  \\text{if } x \\in D \\\\ 0  \\text{otherwise} \\end{cases}\n$$\nThis can be written compactly using an indicator function, $$p(x) = 2 \\cdot \\mathbb{I}_{D}(x)$$.\n\n**b. Rejection Sampling**\nRejection sampling is a method for generating samples from a target distribution with PDF $$p(x)$$ when we can easily sample from another distribution, the proposal distribution, with PDF $$g(x)$$. A necessary condition is that the support of $$g(x)$$ must contain the support of $$p(x)$$. Furthermore, there must exist a constant $$M  \\infty$$ such that $$p(x) \\le M \\cdot g(x)$$ for all $$x$$. The algorithm proceeds as follows:\n1. Draw a sample $$Y$$ from the proposal distribution $$g(x)$$.\n2. Draw a sample $$U$$ from the Uniform distribution on $$[0, 1]$$.\n3. If $$U \\le \\frac{p(Y)}{M g(Y)}$$, accept the sample $$Y$$ (i.e., set $$X=Y$$). Otherwise, reject $$Y$$ and return to step 1.\n\n**c. Constructing the Sampler for $$D$$**\nThe problem specifies using a proposal distribution that is Uniform on the square $$S = [0,1]^2$$. The area of $$S$$ is $$\\text{Area}(S) = 1^2 = 1$$. The proposal PDF, $$g(x)$$, is therefore:\n$$\ng(x) = \\begin{cases} 1/\\text{Area}(S) = 1  \\text{if } x \\in S \\\\ 0  \\text{otherwise} \\end{cases}\n$$\nThe support of $$g(x)$$ is $$S$$, and since $$D \\subset S$$, the support of $$g(x)$$ contains the support of $$p(x)$$. We now find the constant $$M$$. We require $$p(x) \\le M \\cdot g(x)$$ for all $$x$$.\n- If $$x \\in D$$, then $$p(x)=2$$ and $$g(x)=1$$. The condition is $$2 \\le M \\cdot 1$$, so $$M \\ge 2$$.\n- If $$x \\in S \\setminus D$$, then $$p(x)=0$$ and $$g(x)=1$$. The condition is $$0 \\le M \\cdot 1$$, which is true for any non-negative $$M$$.\n- If $$x \\notin S$$, then $$p(x)=0$$ and $$g(x)=0$$. The condition holds.\nThe minimal value for $$M$$ that satisfies the inequality everywhere is $$M=2$$.\n\nNow, we analyze the acceptance condition $$U \\le \\frac{p(Y)}{M g(Y)}$$ with $$M=2$$:\n- If the proposed sample $$Y$$ is in $$D$$, then $$p(Y)=2$$ and $$g(Y)=1$$. The ratio is $$\\frac{p(Y)}{M g(Y)} = \\frac{2}{2 \\cdot 1} = 1$$. The condition is $$U \\le 1$$, which is always true for a sample $$U$$ from $$\\text{Unif}(0,1)$$.\n- If the proposed sample $$Y$$ is in $$S \\setminus D$$, then $$p(Y)=0$$ and $$g(Y)=1$$. The ratio is $$\\frac{p(Y)}{M g(Y)} = \\frac{0}{2 \\cdot 1} = 0$$. The condition is $$U \\le 0$$, which is almost surely false.\n\nThis simplifies the rejection sampling algorithm significantly:\n1. Propose a sample $$Y=(Y_1, Y_2)$$ from the Uniform distribution on $$S=[0,1]^2$$. This is achieved by drawing $$Y_1 \\sim \\text{Unif}(0,1)$$ and $$Y_2 \\sim \\text{Unif}(0,1)$$ independently.\n2. If $$Y \\in D$$ (i.e., if $$Y_1 + Y_2 \\le 1$$), accept $$Y$$.\n3. Otherwise, reject $$Y$$ and repeat.\n\n**d. Acceptance Probability and Distribution of Accepted Samples**\nThe probability of accepting a proposal $$Y$$ in a single trial, denoted by $$\\alpha$$, is the probability that $$Y$$ falls into the acceptance region, which is $$D$$.\n$$\n\\alpha = P(Y \\in D) = \\int_S \\mathbb{I}_D(y) g(y) \\, \\mathrm{d}y = \\int_D g(y) \\, \\mathrm{d}y\n$$\nSince $$g(y)=1$$ for all $$y \\in D \\subset S$$, we have:\n$$\n\\alpha = \\int_D 1 \\, \\mathrm{d}y = \\text{Area}(D) = \\frac{1}{2}\n$$\nThe theoretical acceptance probability is $$\\alpha = 0.5$$. The number of proposals $$N_{\\text{prop}}$$ required to obtain one accepted sample follows a Geometric distribution with success probability $$\\alpha$$. The expected number of proposals to get $$N_{\\text{acc}}$$ samples is $$N_{\\text{acc}} / \\alpha$$.\n\nTo confirm that the accepted samples $$X$$ are indeed uniformly distributed on $$D$$, we consider the probability that an accepted sample falls into an arbitrary measurable subset $$A \\subseteq D$$.\n$$\nP(X \\in A) = P(Y \\in A \\mid Y \\text{ is accepted}) = \\frac{P(Y \\in A \\text{ and } Y \\text{ is accepted})}{P(Y \\text{ is accepted})}\n$$\nSince $$A \\subseteq D$$, a sample $$Y \\in A$$ is always accepted. Thus, $$P(Y \\in A \\text{ and } Y \\text{ is accepted}) = P(Y \\in A)$$.\n$$\nP(Y \\in A) = \\int_A g(y) \\, \\mathrm{d}y = \\int_A 1 \\, \\mathrm{d}y = \\text{Area}(A)\n$$\nThe denominator is the overall acceptance probability, $$P(Y \\text{ is accepted}) = \\alpha = \\text{Area}(D)$$. Therefore,\n$$\nP(X \\in A) = \\frac{\\text{Area}(A)}{\\text{Area}(D)}\n$$\nThis is precisely the definition of the uniform probability measure on $$D$$. The resulting samples are correctly distributed.\n\n**2. Monte Carlo Integration**\n\nWe wish to estimate an integral of the form $$I = \\int_D h(x) \\, \\mathrm{d}x$$. This integral can be related to the expectation of $$h(X)$$ where $$X$$ is a random variable with PDF $$p(x) = \\text{Unif}(D)$$.\n$$\nE[h(X)] = \\int_D h(x) p(x) \\, \\mathrm{d}x = \\int_D h(x) \\frac{1}{\\text{Area}(D)} \\, \\mathrm{d}x\n$$\nRearranging this gives the identity:\n$$\nI = \\int_D h(x) \\, \\mathrm{d}x = \\text{Area}(D) \\cdot E[h(X)]\n$$\nBy the Law of Large Numbers, the expectation $$E[h(X)]$$ can be estimated by the sample mean of $$h$$ evaluated over a set of $$N_{\\text{acc}}$$ independent samples $$\\{X^{(i)}\\}_{i=1}^{N_{\\text{acc}}}$$ drawn from $$p(x)$$:\n$$\nE[h(X)] \\approx \\frac{1}{N_{\\text{acc}}} \\sum_{i=1}^{N_{\\text{acc}}} h(X^{(i)})\n$$\nSubstituting this into the identity for $$I$$ yields the crude Monte Carlo estimator, $$\\widehat{I}$$:\n$$\n\\widehat{I} = \\text{Area}(D) \\cdot \\frac{1}{N_{\\text{acc}}} \\sum_{i=1}^{N_{\\text{acc}}} h(X^{(i)})\n$$\nGiven $$\\text{Area}(D)=1/2$$, the specific estimators for $$I_1 = \\int_D x_1 x_2 \\, \\mathrm{d}x$$ and $$I_2 = \\int_D (x_1+x_2) \\, \\mathrm{d}x$$ are:\n- For $$I_1$$, let $$h_1(x) = x_1 x_2$$. The estimator is:\n$$\n\\widehat{I}_1 = \\frac{1}{2} \\cdot \\frac{1}{N_{\\text{acc}}} \\sum_{i=1}^{N_{\\text{acc}}} X^{(i)}_1 X^{(i)}_2\n$$\n- For $$I_2$$, let $$h_2(x) = x_1 + x_2$$. The estimator is:\n$$\n\\widehat{I}_2 = \\frac{1}{2} \\cdot \\frac{1}{N_{\\text{acc}}} \\sum_{i=1}^{N_{\\text{acc}}} (X^{(i)}_1 + X^{(i)}_2)\n$$\n\n**3. Implementation Plan**\nThe implementation will consist of a function that takes an RNG seed and a target number of accepted samples, $$N_{\\text{acc}}$$, as input.\n1. Initialize a random number generator with the specified seed for reproducibility.\n2. Initialize an empty list for accepted samples and set the proposal count, $$N_{\\text{prop}}$$, to $$0$$.\n3. Loop until the number of accepted samples reaches $$N_{\\text{acc}}$$:\n    a. Increment $$N_{\\text{prop}}$$.\n    b. Generate a $$2$$-dimensional proposal point $$x = (x_1, x_2)$$ by drawing two numbers from $$\\text{Unif}(0,1)$$.\n    c. If $$x_1 + x_2 \\le 1$$, add the point $$x$$ to the list of accepted samples.\n4. Once the loop terminates, calculate the required quantities:\n    a. Empirical acceptance rate: $$\\hat{\\alpha} = N_{\\text{acc}} / N_{\\text{prop}}$$.\n    b. Monte Carlo estimates $$\\widehat{I}_1$$ and $$\\widehat{I}_2$$ using the formulas derived above, applied to the collected samples.\n    c. The total number of proposals, $$N_{\\text{prop}}$$.\n5. Return these four values: $$[\\hat{\\alpha}, \\widehat{I}_1, \\widehat{I}_2, N_{\\text{prop}}]$$.\nThe main program will execute this logic for each of the specified test cases and format the results as a single string.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements the rejection sampling and Monte Carlo integration as per the problem description.\n    Runs a deterministic test suite and prints the results in the specified format.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (RNG seed, Number of accepted samples N_acc)\n        (12345, 1),\n        (202310, 5000),\n        (42, 20000),\n    ]\n\n    all_results = []\n    for seed, N_acc in test_cases:\n        # Initialize the random number generator for reproducibility.\n        rng = np.random.default_rng(seed)\n\n        accepted_samples = []\n        N_prop = 0\n\n        # Run the rejection sampler until exactly N_acc samples are collected.\n        while len(accepted_samples)  N_acc:\n            N_prop += 1\n            # Propose a point from the Uniform distribution on the unit square S = [0,1]^2.\n            # This is done by drawing two independent samples from Unif(0,1).\n            proposal_point = rng.random(size=2)\n            \n            # Acceptance condition: check if the point lies in the target set D.\n            # D = {x in [0,1]^2: x_1 + x_2 = 1}.\n            # The boundary condition x_1 + x_2 = 1 is inclusive as specified.\n            if proposal_point[0] + proposal_point[1] = 1.0:\n                accepted_samples.append(proposal_point)\n\n        # 1. Compute the empirical acceptance rate.\n        # This is the ratio of accepted samples to total proposals.\n        # The theoretical rate is Area(D)/Area(S) = (1/2)/1 = 0.5.\n        alpha_hat = N_acc / N_prop\n\n        # 2. Compute the crude Monte Carlo estimates of the integrals.\n        # The general formula is: I_hat = Area(D) * (1/N_acc) * sum(h(X_i)).\n        # Here, Area(D) = 0.5.\n        area_d = 0.5\n        \n        # Convert the list of samples to a NumPy array for efficient, vectorized calculations.\n        if N_acc  0:\n            samples_np = np.array(accepted_samples)\n            \n            # For I_1 = integral(x_1 * x_2), the function is h_1(x) = x_1 * x_2.\n            h1_values = samples_np[:, 0] * samples_np[:, 1]\n            mean_h1 = np.mean(h1_values)\n            I1_hat = area_d * mean_h1\n            \n            # For I_2 = integral(x_1 + x_2), the function is h_2(x) = x_1 + x_2.\n            h2_values = samples_np[:, 0] + samples_np[:, 1]\n            mean_h2 = np.mean(h2_values)\n            I2_hat = area_d * mean_h2\n        else: # This branch is not hit by the given test cases but is robust.\n            I1_hat = 0.0\n            I2_hat = 0.0\n\n        # 3. N_prop is the total number of proposals.\n        # It's an integer.\n\n        # Package the results for this test case.\n        case_result = [alpha_hat, I1_hat, I2_hat, N_prop]\n        all_results.append(case_result)\n\n    # Format the final output according to the problem specification.\n    # e.g., [[val,val,val,val],[val,val,val,val]]\n    inner_lists_str = []\n    for res in all_results:\n        # Format each inner list as '[v1,v2,v3,v4]' without extra spaces.\n        inner_str = '[' + ','.join(map(str, res)) + ']'\n        inner_lists_str.append(inner_str)\n    \n    final_output_str = '[' + ','.join(inner_lists_str) + ']'\n\n    # Final print statement in the exact required format.\n    print(final_output_str)\n\nsolve()\n```", "id": "3301576"}]}