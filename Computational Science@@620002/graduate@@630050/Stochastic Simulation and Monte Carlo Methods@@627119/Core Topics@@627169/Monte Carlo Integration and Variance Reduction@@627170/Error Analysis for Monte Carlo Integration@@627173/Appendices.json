{"hands_on_practices": [{"introduction": "The cornerstone of Monte Carlo error analysis is the Central Limit Theorem, which predicts a specific rate of convergence for the estimator's error. This first practice provides a direct, computational way to witness this fundamental principle in action, building intuition for how sample size affects accuracy. By simulating Monte Carlo estimates across various scenarios, you will empirically confirm the hallmark $n^{-1/2}$ decay of the root mean squared error, a result that underpins the reliability of these methods [@problem_id:3306236].", "problem": "Consider Monte Carlo integration of a function $f$ under a probability distribution of a real-valued random variable $X$. Let $\\mu = \\mathbb{E}[f(X)]$ and for each integer $n \\ge 1$, let $\\hat{\\mu}_n = \\frac{1}{n}\\sum_{i=1}^n f(X_i)$ where $\\{X_i\\}_{i=1}^n$ are independent and identically distributed copies of $X$. Define the root mean squared error $\\operatorname{RMSE}(n)$ of the estimator $\\hat{\\mu}_n$ as $\\operatorname{RMSE}(n) = \\sqrt{\\mathbb{E}\\big[(\\hat{\\mu}_n - \\mu)^2\\big]}$. Assume throughout that $\\mathbb{E}[f(X)^2]$ is finite.\n\nYour task is to design a program that estimates the empirical decay rate of $\\operatorname{RMSE}(n)$ as a function of $n$ by simulation. Use the following fundamental base:\n- The linearity of expectation for independent and identically distributed random variables.\n- The additivity of variance for independent random variables.\n- The definition of the sample mean and mean squared error.\n\nAlgorithmic specification:\n- For each test case described below, fix a list of sample sizes $\\mathcal{N}$ and a replication count $R$. For each $n \\in \\mathcal{N}$, generate $R$ independent Monte Carlo estimators $\\hat{\\mu}_n$ by drawing $n$ independent and identically distributed samples of $X$ per replication, evaluating $f$ on those samples, and averaging. Use a single pool of $R \\times n_{\\max}$ samples to efficiently obtain all $\\hat{\\mu}_n$ across $n \\in \\mathcal{N}$, where $n_{\\max} = \\max \\mathcal{N}$, by taking the first $n$ samples per replication for each $n$. Compute the empirical root mean squared error as $\\widehat{\\operatorname{RMSE}}(n) = \\sqrt{\\frac{1}{R}\\sum_{r=1}^R (\\hat{\\mu}_n^{(r)} - \\mu)^2}$, where $\\mu$ is known analytically for each test case below.\n- Estimate the scaling exponent $\\alpha$ for the relation $\\widehat{\\operatorname{RMSE}}(n) \\approx C n^{\\alpha}$ by performing an ordinary least squares fit of $\\log \\widehat{\\operatorname{RMSE}}(n)$ versus $\\log n$ across all $n \\in \\mathcal{N}$. Report the slope $\\alpha$.\n\nAngle convention: any appearance of $\\sin(\\cdot)$ must interpret its argument in radians.\n\nTest suite (three test cases):\n1. Case A (bounded integrand under a Gaussian law):\n   - Distribution: $X \\sim \\mathcal{N}(0,1)$.\n   - Integrand: $f(x) = \\exp(-x^2)$.\n   - Analytical mean: $\\mu = \\mathbb{E}[\\exp(-X^2)] = \\frac{1}{\\sqrt{1 + 2 \\cdot 1}} = \\frac{1}{\\sqrt{3}}$.\n   - Replications: $R = 4096$.\n   - Sample sizes: $\\mathcal{N} = \\{100, 400, 1600\\}$.\n   - Random seed: $s = 13579$.\n\n2. Case B (heavy-tailed sampling with finite integrand variance):\n   - Distribution: $X \\sim t_{\\nu}$ with degrees of freedom $\\nu = 6$.\n   - Integrand: $f(x) = x^2$.\n   - Analytical mean: $\\mu = \\mathbb{E}[X^2] = \\frac{\\nu}{\\nu - 2} = \\frac{6}{4} = 1.5$.\n   - Replications: $R = 4096$.\n   - Sample sizes: $\\mathcal{N} = \\{100, 400, 1600\\}$.\n   - Random seed: $s = 24680$.\n\n3. Case C (skewed law with mixed oscillatory-linear integrand):\n   - Distribution: $X \\sim \\operatorname{Exponential}(\\lambda)$ with rate $\\lambda = 1$ (mean $1$).\n   - Integrand: $f(x) = \\sin(x) + x$ with $x$ in radians.\n   - Analytical mean: since $\\mathbb{E}[e^{iX}] = \\frac{1}{1 - i}$ for $X \\sim \\operatorname{Exponential}(1)$, we have $\\mathbb{E}[\\sin(X)] = \\operatorname{Im}\\big(\\frac{1}{1 - i}\\big) = \\frac{1}{2}$ and $\\mathbb{E}[X] = 1$, hence $\\mu = \\frac{1}{2} + 1 = 1.5$.\n   - Replications: $R = 4096$.\n   - Sample sizes: $\\mathcal{N} = \\{100, 400, 1600\\}$.\n   - Random seed: $s = 98765$.\n\nOutput specification:\n- For each test case, compute the slope $\\alpha$ of the least squares fit of $\\log \\widehat{\\operatorname{RMSE}}(n)$ versus $\\log n$ over $\\mathcal{N}$.\n- Your program should produce a single line of output containing the three slopes for Cases A, B, and C, respectively, as a comma-separated list enclosed in square brackets, for example, $[\\alpha_A,\\alpha_B,\\alpha_C]$.\n- The outputs must be real numbers (floats). No other text or units should be printed.\n\nAll random number generation must be deterministic given the specified seeds. No user input is required. The program must be self-contained and must not access external resources.", "solution": "The task is to perform an empirical error analysis for Monte Carlo integration and estimate the convergence rate of the root mean squared error. The theoretical foundation for this analysis rests upon fundamental principles of probability theory, namely the Law of Large Numbers and the Central Limit Theorem.\n\nLet $X$ be a real-valued random variable with a given probability distribution, and let $f$ be a real-valued function. We are interested in computing the expectation $\\mu = \\mathbb{E}[f(X)]$. The Monte Carlo method approximates this value by drawing $n$ independent and identically distributed (i.i.d.) samples $\\{X_1, X_2, \\dots, X_n\\}$ from the distribution of $X$, and then computing the sample mean of the function evaluated at these points:\n$$\n\\hat{\\mu}_n = \\frac{1}{n}\\sum_{i=1}^n f(X_i)\n$$\nThis estimator $\\hat{\\mu}_n$ is itself a random variable, as its value depends on the random samples drawn. To assess its quality, we analyze its statistical properties.\n\nFirst, we determine the bias of the estimator. By the linearity of expectation, and since each $X_i$ has the same distribution as $X$:\n$$\n\\mathbb{E}[\\hat{\\mu}_n] = \\mathbb{E}\\left[\\frac{1}{n}\\sum_{i=1}^n f(X_i)\\right] = \\frac{1}{n}\\sum_{i=1}^n \\mathbb{E}[f(X_i)] = \\frac{1}{n}\\sum_{i=1}^n \\mu = \\frac{n\\mu}{n} = \\mu\n$$\nSince $\\mathbb{E}[\\hat{\\mu}_n] = \\mu$, the estimator is unbiased.\n\nNext, we analyze the variance of the estimator. Let $\\sigma_f^2 = \\operatorname{Var}(f(X)) = \\mathbb{E}[f(X)^2] - (\\mathbb{E}[f(X)])^2 = \\mathbb{E}[f(X)^2] - \\mu^2$. The problem states that $\\mathbb{E}[f(X)^2]$ is finite, which ensures $\\sigma_f^2$ is also finite. Since the samples $\\{X_i\\}$ are independent, the function values $\\{f(X_i)\\}$ are also independent random variables. For independent variables, the variance of the sum is the sum of the variances:\n$$\n\\operatorname{Var}(\\hat{\\mu}_n) = \\operatorname{Var}\\left(\\frac{1}{n}\\sum_{i=1}^n f(X_i)\\right) = \\frac{1}{n^2}\\operatorname{Var}\\left(\\sum_{i=1}^n f(X_i)\\right) = \\frac{1}{n^2}\\sum_{i=1}^n \\operatorname{Var}(f(X_i)) = \\frac{1}{n^2} \\sum_{i=1}^n \\sigma_f^2 = \\frac{n\\sigma_f^2}{n^2} = \\frac{\\sigma_f^2}{n}\n$$\nThe problem defines the estimator's error using the Root Mean Squared Error (RMSE), $\\operatorname{RMSE}(n) = \\sqrt{\\mathbb{E}[(\\hat{\\mu}_n - \\mu)^2]}$. The quantity inside the square root is the Mean Squared Error (MSE), which can be decomposed into variance and squared bias:\n$$\n\\operatorname{MSE}(n) = \\mathbb{E}[(\\hat{\\mu}_n - \\mu)^2] = \\operatorname{Var}(\\hat{\\mu}_n) + (\\mathbb{E}[\\hat{\\mu}_n] - \\mu)^2\n$$\nSince the estimator is unbiased, its bias is $0$. Therefore, the MSE is equal to the variance:\n$$\n\\operatorname{MSE}(n) = \\frac{\\sigma_f^2}{n}\n$$\nTaking the square root gives the theoretical RMSE:\n$$\n\\operatorname{RMSE}(n) = \\sqrt{\\frac{\\sigma_f^2}{n}} = \\frac{\\sigma_f}{\\sqrt{n}} = \\sigma_f n^{-1/2}\n$$\nThis result shows that the RMSE of the Monte Carlo estimator decays with the sample size $n$ as $n^{-1/2}$. This corresponds to the general form $\\operatorname{RMSE}(n) = C n^{\\alpha}$ with a constant $C = \\sigma_f$ and a decay exponent $\\alpha = -1/2$.\n\nThe task requires estimating this exponent $\\alpha$ empirically. This is done by simulating the estimation process multiple times. For each sample size $n$ from a given set $\\mathcal{N}$, we generate $R$ independent Monte Carlo estimates, denoted $\\{\\hat{\\mu}_n^{(1)}, \\hat{\\mu}_n^{(2)}, \\dots, \\hat{\\mu}_n^{(R)}\\}$. We then compute an empirical estimate of the RMSE, denoted $\\widehat{\\operatorname{RMSE}}(n)$, which approximates the true $\\operatorname{RMSE}(n)$:\n$$\n\\widehat{\\operatorname{RMSE}}(n) = \\sqrt{\\frac{1}{R}\\sum_{r=1}^R (\\hat{\\mu}_n^{(r)} - \\mu)^2}\n$$\nHere, $\\mu$ is the known analytical mean of $f(X)$, which is provided for each test case.\n\nTo estimate $\\alpha$, we assume the relationship $\\widehat{\\operatorname{RMSE}}(n) \\approx C n^{\\alpha}$ holds for our empirical data. By taking the natural logarithm of both sides, we obtain a linear relationship:\n$$\n\\log(\\widehat{\\operatorname{RMSE}}(n)) \\approx \\log(C) + \\alpha \\log(n)\n$$\nThis corresponds to a linear equation $y = m x + c$, where $y = \\log(\\widehat{\\operatorname{RMSE}}(n))$, the slope $m = \\alpha$, the independent variable $x = \\log(n)$, and the intercept $c = \\log(C)$. We can therefore estimate $\\alpha$ by performing an ordinary least squares (OLS) linear regression on the set of data points $\\{(\\log(n), \\log(\\widehat{\\operatorname{RMSE}}(n)))\\}$ for all $n \\in \\mathcal{N}$. The slope of the resulting regression line is our empirical estimate of $\\alpha$.\n\nThe algorithmic procedure is as follows:\n1. For each test case, initialize a random number generator with the specified seed for reproducibility.\n2. Generate a single large block of random samples of size $R \\times n_{\\max}$, where $R$ is the number of replications and $n_{\\max}$ is the maximum sample size in $\\mathcal{N}$. Let this matrix of samples be $S$.\n3. Apply the integrand function $f$ element-wise to $S$ to obtain a matrix of function values, $F$.\n4. For each sample size $n \\in \\mathcal{N}$:\n   a. Extract the first $n$ columns of $F$, corresponding to $R$ sets of $n$ function evaluations.\n   b. For each of the $R$ rows, compute the mean to obtain the vector of $R$ estimates $\\{\\hat{\\mu}_n^{(r)}\\}_{r=1}^R$.\n   c. Compute the empirical RMSE, $\\widehat{\\operatorname{RMSE}}(n)$, using the provided formula and the known true mean $\\mu$.\n5. After computing $\\widehat{\\operatorname{RMSE}}(n)$ for all $n \\in \\mathcal{N}$, create two vectors: one with the logarithms of the sample sizes, $\\log(n)$, and another with the logarithms of the empirical RMSEs, $\\log(\\widehat{\\operatorname{RMSE}}(n))$.\n6. Perform a linear regression on these two vectors to find the slope, which is the desired estimate for $\\alpha$.\n7. Repeat this process for all test cases and report the resulting slopes.\nThe theoretically expected value for $\\alpha$ in all cases is $-0.5$, provided the variance $\\sigma_f^2$ is finite and non-zero. The empirical results should be close to this value.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import linregress, t as t_dist\n\ndef run_simulation_and_regression(params):\n    \"\"\"\n    Runs a Monte Carlo simulation for a single test case and computes the RMSE decay rate.\n\n    Args:\n        params (dict): A dictionary containing all parameters for the test case.\n\n    Returns:\n        float: The estimated decay exponent alpha.\n    \"\"\"\n    R = params['R']\n    N_list = params['N_list']\n    mu = params['mu']\n    seed = params['seed']\n    integrand = params['integrand']\n    sampler = params['sampler']\n    \n    # Set the random seed for reproducibility using a Generator object\n    rng = np.random.default_rng(seed)\n    \n    n_max = max(N_list)\n    \n    # Generate the base pool of random samples\n    # For scipy.stats distributions, the `random_state` argument accepts a Generator.\n    samples = sampler(rng, (R, n_max))\n    \n    # Evaluate the integrand on all samples\n    f_samples = integrand(samples)\n    \n    rmses = []\n    for n in N_list:\n        # Take the first n samples for each replication\n        f_samples_n = f_samples[:, :n]\n        \n        # Compute the R Monte Carlo estimators\n        mu_hat_n_r = np.mean(f_samples_n, axis=1)\n        \n        # Compute the empirical RMSE\n        # RMSE_hat(n) = sqrt(1/R * sum_{r=1 to R} (mu_hat_n^(r) - mu)^2)\n        rmse_n = np.sqrt(np.mean((mu_hat_n_r - mu)**2))\n        rmses.append(rmse_n)\n        \n    # Perform ordinary least squares fit on the log-log data\n    # log(RMSE_hat(n)) vs log(n)\n    log_N = np.log(N_list)\n    log_rmse = np.log(rmses)\n    \n    # linregress returns (slope, intercept, r_value, p_value, stderr)\n    regression_result = linregress(log_N, log_rmse)\n    \n    # The slope of the log-log plot is the scaling exponent alpha\n    alpha = regression_result.slope\n    return alpha\n\ndef solve():\n    \"\"\"\n    Defines and runs all test cases, then prints the results in the specified format.\n    \"\"\"\n    test_cases = [\n        {\n            # Case A: Bounded integrand, Gaussian law\n            'sampler': lambda rng, size: rng.normal(loc=0.0, scale=1.0, size=size),\n            'integrand': lambda x: np.exp(-x**2),\n            'mu': 1.0 / np.sqrt(3.0),\n            'R': 4096,\n            'N_list': [100, 400, 1600],\n            'seed': 13579\n        },\n        {\n            # Case B: Heavy-tailed sampling, finite integrand variance\n            'sampler': lambda rng, size: t_dist.rvs(df=6, size=size, random_state=rng),\n            'integrand': lambda x: x**2,\n            'mu': 1.5,\n            'R': 4096,\n            'N_list': [100, 400, 1600],\n            'seed': 24680\n        },\n        {\n            # Case C: Skewed law, mixed integrand\n            'sampler': lambda rng, size: rng.exponential(scale=1.0, size=size),\n            'integrand': lambda x: np.sin(x) + x,\n            'mu': 1.5,\n            'R': 4096,\n            'N_list': [100, 400, 1600],\n            'seed': 98765\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        alpha = run_simulation_and_regression(case)\n        results.append(alpha)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3306236"}, {"introduction": "Moving beyond the standard estimator, many practical scenarios involve unnormalized probability distributions, necessitating the use of self-normalized importance sampling. This practice delves into the error characteristics of such estimators, which, unlike their standard counterparts, are biased for finite sample sizes. By simulating this scenario, you will empirically decompose the total error—the Mean Squared Error ($MSE$)—into its two key components: variance and squared bias, offering a deeper understanding of the estimator's performance [@problem_id:3306258].", "problem": "Consider estimating the expectation of a function under an unnormalized target density using self-normalized importance sampling. Let the target density on the real line be defined up to a normalizing constant by the unnormalized density $\\tilde{\\pi}(x) = \\exp\\!\\left(-\\tfrac{x^{4}}{2}\\right)$ for all real $x$. The normalized target density is $\\pi(x) = \\tilde{\\pi}(x)/Z$, where the normalizing constant $Z$ is $Z = \\int_{-\\infty}^{\\infty} \\tilde{\\pi}(x)\\,dx$. The quantity of interest is the expectation $\\mu = \\mathbb{E}_{\\pi}[f(X)]$ with $f(x) = x^{2}$, that is,\n$$\n\\mu = \\frac{\\int_{-\\infty}^{\\infty} x^{2}\\,\\tilde{\\pi}(x)\\,dx}{\\int_{-\\infty}^{\\infty} \\tilde{\\pi}(x)\\,dx}.\n$$\nIn importance sampling with proposal density $q(x)$, the self-normalized importance sampling estimator with $N$ samples is\n$$\n\\hat{\\mu}_{\\mathrm{SNIS}} = \\frac{\\sum_{i=1}^{N} w_{i} f(x_{i})}{\\sum_{i=1}^{N} w_{i}},\n$$\nwhere $x_{i} \\sim q$ independently and the unnormalized importance weights are $w_{i} = \\tilde{\\pi}(x_{i})/q(x_{i})$. This estimator is biased for finite $N$ because it is a ratio of random sums.\n\nStarting from the fundamental definitions of expectation, variance, mean squared error, and importance sampling, write a complete, runnable program that:\n- Computes a high-accuracy numerical approximation of the ground-truth value $\\mu$ using numerical quadrature over the real line for both the numerator and denominator integrals.\n- For each test case in the test suite below, generates $R$ independent replicates of $\\hat{\\mu}_{\\mathrm{SNIS}}$ using the specified proposal $q$ and sample size $N$, with a fixed pseudorandom number generator seed for reproducibility. For a given replicate $r$ in $\\{1,\\dots,R\\}$, draw $N$ independent samples $x_{i}^{(r)} \\sim q$, compute the corresponding weights $w_{i}^{(r)} = \\tilde{\\pi}(x_{i}^{(r)})/q(x_{i}^{(r)})$, and form the self-normalized estimator $\\hat{\\mu}_{r} = \\left(\\sum_{i=1}^{N} w_{i}^{(r)} f(x_{i}^{(r)})\\right)\\Big/\\left(\\sum_{i=1}^{N} w_{i}^{(r)}\\right)$.\n- Computes the empirical mean squared error, empirical variance, and squared empirical bias across the $R$ replicates:\n  - The empirical mean squared error is\n    $$\n    \\widehat{\\mathrm{MSE}} = \\frac{1}{R} \\sum_{r=1}^{R} \\left(\\hat{\\mu}_{r} - \\mu\\right)^{2}.\n    $$\n  - The empirical variance uses the population normalization,\n    $$\n    \\widehat{\\mathrm{Var}} = \\frac{1}{R} \\sum_{r=1}^{R} \\left(\\hat{\\mu}_{r} - \\bar{\\mu}\\right)^{2}, \\quad \\text{where} \\quad \\bar{\\mu} = \\frac{1}{R}\\sum_{r=1}^{R} \\hat{\\mu}_{r}.\n    $$\n  - The squared empirical bias is\n    $$\n    \\widehat{\\mathrm{Bias}}^{2} = \\left(\\bar{\\mu} - \\mu\\right)^{2}.\n    $$\n- Verifies the decomposition by reporting $\\widehat{\\mathrm{MSE}}$, $\\widehat{\\mathrm{Var}}$, and $\\widehat{\\mathrm{Bias}}^{2}$ (the sum $\\widehat{\\mathrm{Var}} + \\widehat{\\mathrm{Bias}}^{2}$ should numerically match $\\widehat{\\mathrm{MSE}}$ up to Monte Carlo error).\n\nUse the following scientifically sound and diverse test suite of cases, specified by $(N, R, \\text{proposal}, \\text{degrees-of-freedom}, \\text{seed})$:\n- Case $1$: $(N = 1, R = 50000, \\text{proposal} = \\text{Normal}, \\text{degrees-of-freedom} = \\text{None}, \\text{seed} = 12345)$. This is a boundary case illustrating maximal finite-sample bias since $\\hat{\\mu}_{\\mathrm{SNIS}}$ reduces to $f(x)$ when $N = 1$.\n- Case $2$: $(N = 20, R = 10000, \\text{proposal} = \\text{Normal}, \\text{degrees-of-freedom} = \\text{None}, \\text{seed} = 23456)$. This is a moderate-sample case with a light-tailed proposal.\n- Case $3$: $(N = 50, R = 5000, \\text{proposal} = \\text{Student-t}, \\text{degrees-of-freedom} = 3, \\text{seed} = 34567)$. This case uses a heavier-tailed proposal to examine variance reduction in the tails.\n- Case $4$: $(N = 200, R = 2000, \\text{proposal} = \\text{Normal}, \\text{degrees-of-freedom} = \\text{None}, \\text{seed} = 45678)$. This is a larger-sample case for the light-tailed proposal.\n\nFor the proposal densities $q(x)$, use:\n- The standard Normal proposal with density $q(x) = (2\\pi)^{-1/2}\\exp\\!\\left(-x^{2}/2\\right)$.\n- The Student-$t$ proposal with $\\nu$ degrees of freedom, with density\n$$\nq(x) = \\frac{\\Gamma\\!\\left(\\frac{\\nu+1}{2}\\right)}{\\sqrt{\\nu\\pi}\\,\\Gamma\\!\\left(\\frac{\\nu}{2}\\right)}\\left(1 + \\frac{x^{2}}{\\nu}\\right)^{-\\frac{\\nu+1}{2}},\n$$\nfor real $x$.\n\nYour program should produce a single line of output containing the results for all cases as a list of triples in the following format:\n$$\n\\big[\\,[\\widehat{\\mathrm{MSE}}_{1},\\,\\widehat{\\mathrm{Var}}_{1},\\,\\widehat{\\mathrm{Bias}}^{2}_{1}],\\,[\\widehat{\\mathrm{MSE}}_{2},\\,\\widehat{\\mathrm{Var}}_{2},\\,\\widehat{\\mathrm{Bias}}^{2}_{2}],\\,[\\widehat{\\mathrm{MSE}}_{3},\\,\\widehat{\\mathrm{Var}}_{3},\\,\\widehat{\\mathrm{Bias}}^{2}_{3}],\\,[\\widehat{\\mathrm{MSE}}_{4},\\,\\widehat{\\mathrm{Var}}_{4},\\,\\widehat{\\mathrm{Bias}}^{2}_{4}]\\,\\big].\n$$\nAll numerical results must be reported as floating-point numbers in the order of the test suite specified above, with no additional text printed.", "solution": "The problem asks us to analyze the error of the self-normalized importance sampling (SNIS) estimator, which is commonly used when the target probability density $\\pi(x)$ is only known up to a constant, i.e., we have an unnormalized density $\\tilde{\\pi}(x)$ where $\\pi(x) = \\tilde{\\pi}(x)/Z$. The quantity of interest is $\\mu = \\mathbb{E}_\\pi[f(X)]$.\n\nThe SNIS estimator is constructed using samples $x_i$ drawn from a different, known proposal distribution $q(x)$. It is given by the ratio:\n$$\n\\hat{\\mu}_{\\mathrm{SNIS}} = \\frac{\\sum_{i=1}^{N} w_{i} f(x_{i})}{\\sum_{i=1}^{N} w_{i}}, \\quad \\text{where the weights are } w_i = \\frac{\\tilde{\\pi}(x_i)}{q(x_i)}.\n$$\nThis estimator is a ratio of two random variables, let's call them $\\hat{A} = \\frac{1}{N}\\sum w_i f(x_i)$ and $\\hat{B} = \\frac{1}{N}\\sum w_i$. The estimator is $\\hat{\\mu}_{\\mathrm{SNIS}} = \\hat{A}/\\hat{B}$.\nThe expectation of the numerator is $\\mathbb{E}_q[\\hat{A}] = \\mathbb{E}_q\\left[\\frac{1}{N}\\sum \\frac{\\tilde{\\pi}(X_i)}{q(X_i)}f(X_i)\\right] = \\int \\frac{\\tilde{\\pi}(x)}{q(x)}f(x)q(x)dx = \\int f(x)\\tilde{\\pi}(x)dx$.\nThe expectation of the denominator is $\\mathbb{E}_q[\\hat{B}] = \\mathbb{E}_q\\left[\\frac{1}{N}\\sum \\frac{\\tilde{\\pi}(X_i)}{q(X_i)}\\right] = \\int \\frac{\\tilde{\\pi}(x)}{q(x)}q(x)dx = \\int \\tilde{\\pi}(x)dx = Z$.\nThe true mean is $\\mu = \\frac{\\int f(x)\\tilde{\\pi}(x)dx}{Z} = \\frac{\\mathbb{E}_q[\\hat{A}]}{\\mathbb{E}_q[\\hat{B}]}$.\n\nHowever, for a finite sample size $N$, the expectation of the ratio is not equal to the ratio of the expectations:\n$$\n\\mathbb{E}[\\hat{\\mu}_{\\mathrm{SNIS}}] = \\mathbb{E}\\left[\\frac{\\hat{A}}{\\hat{B}}\\right] \\neq \\frac{\\mathbb{E}_q[\\hat{A}]}{\\mathbb{E}_q[\\hat{B}]} = \\mu\n$$\nThis inequality is the source of the finite-sample bias of the SNIS estimator. The bias, $\\mathrm{Bias}(\\hat{\\mu}_{\\mathrm{SNIS}}) = \\mathbb{E}[\\hat{\\mu}_{\\mathrm{SNIS}}] - \\mu$, is generally non-zero for any finite $N$, though it typically vanishes as $N \\to \\infty$.\n\nThe total error of an estimator is quantified by the Mean Squared Error (MSE), which can be decomposed into contributions from variance and bias:\n$$\n\\mathrm{MSE}(\\hat{\\mu}) = \\mathbb{E}[(\\hat{\\mu} - \\mu)^2] = \\mathrm{Var}(\\hat{\\mu}) + (\\mathrm{Bias}(\\hat{\\mu}))^2\n$$\nThe goal of the simulation is to compute empirical estimates of these three quantities to understand their relative contributions.\n\nGiven $R$ independent replicates of the estimator, $\\{\\hat{\\mu}_1, \\dots, \\hat{\\mu}_R\\}$, and the true value $\\mu$ (computed via high-precision numerical quadrature), we can compute:\n1.  An estimate of $\\mathbb{E}[\\hat{\\mu}]$ using the sample mean of the replicates: $\\bar{\\mu} = \\frac{1}{R}\\sum_{r=1}^R \\hat{\\mu}_r$.\n2.  The **squared empirical bias**: $\\widehat{\\mathrm{Bias}}^2 = (\\bar{\\mu} - \\mu)^2$. This estimates the squared bias of the estimator for a given sample size $N$.\n3.  The **empirical variance**: $\\widehat{\\mathrm{Var}} = \\frac{1}{R}\\sum_{r=1}^R (\\hat{\\mu}_r - \\bar{\\mu})^2$. This estimates the variance of the estimator, $\\mathrm{Var}(\\hat{\\mu})$.\n4.  The **empirical Mean Squared Error**: $\\widehat{\\mathrm{MSE}} = \\frac{1}{R}\\sum_{r=1}^R (\\hat{\\mu}_r - \\mu)^2$. This estimates the total MSE.\n\nBy simple algebra, it can be shown that these empirical estimators satisfy the decomposition exactly:\n$$\n\\widehat{\\mathrm{MSE}} = \\frac{1}{R}\\sum (\\hat{\\mu}_r - \\mu)^2 = \\frac{1}{R}\\sum (\\hat{\\mu}_r - \\bar{\\mu} + \\bar{\\mu} - \\mu)^2 = \\frac{1}{R}\\sum (\\hat{\\mu}_r - \\bar{\\mu})^2 + (\\bar{\\mu} - \\mu)^2 + \\frac{2}{R}(\\bar{\\mu}-\\mu)\\sum(\\hat{\\mu}_r - \\bar{\\mu})\n$$\nSince $\\sum(\\hat{\\mu}_r - \\bar{\\mu}) = 0$ by definition of $\\bar{\\mu}$, the cross-term vanishes, leaving:\n$$\n\\widehat{\\mathrm{MSE}} = \\widehat{\\mathrm{Var}} + \\widehat{\\mathrm{Bias}}^2\n$$\nThe program computes these three values for different scenarios (varying $N$ and the proposal $q$). The results will show how the bias is significant for small $N$ and decreases as $N$ increases, while the variance also decreases with $N$. This illustrates the balance of bias and variance in the estimator's total error.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import integrate\nfrom scipy import stats\n\ndef solve():\n    \"\"\"\n    Solves the self-normalized importance sampling problem.\n    1. Computes the ground-truth value mu.\n    2. Iterates through test cases to run Monte Carlo simulations.\n    3. For each case, computes and stores the empirical MSE, variance, and squared bias.\n    4. Prints the final results in the specified format.\n    \"\"\"\n\n    # Define the function of interest f(x) = x^2 and the unnormalized target density pi_tilde(x).\n    f_x = lambda x: x**2\n    pi_tilde = lambda x: np.exp(-x**4 / 2)\n\n    def calculate_ground_truth():\n        \"\"\"\n        Computes the ground truth value mu = E_pi[f(X)] using numerical quadrature.\n        \"\"\"\n        numerator_integrand = lambda x: f_x(x) * pi_tilde(x)\n        denominator_integrand = pi_tilde\n\n        numerator_val, _ = integrate.quad(numerator_integrand, -np.inf, np.inf)\n        denominator_val, _ = integrate.quad(denominator_integrand, -np.inf, np.inf)\n\n        if denominator_val == 0:\n            raise ValueError(\"Normalizing constant Z is zero.\")\n            \n        return numerator_val / denominator_val\n\n    def run_simulation(N, R, proposal_type, dof, seed, mu_true):\n        \"\"\"\n        Runs the SNIS simulation for a single test case.\n\n        Args:\n            N (int): Number of samples per replicate.\n            R (int): Number of replicates.\n            proposal_type (str): Type of proposal distribution ('Normal' or 'Student-t').\n            dof (int or None): Degrees of freedom for Student-t proposal.\n            seed (int): Seed for the random number generator.\n            mu_true (float): The ground-truth value of the expectation.\n\n        Returns:\n            list: A list containing [MSE, Var, Bias^2].\n        \"\"\"\n        rng = np.random.default_rng(seed)\n        mu_hat_replicates = np.zeros(R)\n\n        for r in range(R):\n            if proposal_type == 'Normal':\n                x_samples = rng.normal(size=N)\n                q_pdf_vals = stats.norm.pdf(x_samples)\n            elif proposal_type == 'Student-t':\n                x_samples = rng.standard_t(df=dof, size=N)\n                q_pdf_vals = stats.t.pdf(x_samples, df=dof)\n            else:\n                raise ValueError(\"Unknown proposal type specified.\")\n\n            pi_tilde_vals = pi_tilde(x_samples)\n            f_vals = f_x(x_samples)\n            \n            # Unnormalized importance weights\n            weights = pi_tilde_vals / q_pdf_vals\n\n            numerator = np.sum(weights * f_vals)\n            denominator = np.sum(weights)\n            \n            if denominator > 0:\n                mu_hat_replicates[r] = numerator / denominator\n            else:\n                # This case is highly unlikely for the given proposals as weights are non-negative.\n                # It would only occur if all weights underflow to zero. We assign NaN, which\n                # will be filtered out before computing statistics.\n                mu_hat_replicates[r] = np.nan\n\n        # Filter out any potential NaN values before computing statistics.\n        valid_replicates = mu_hat_replicates[~np.isnan(mu_hat_replicates)]\n        \n        # Empirical mean of the estimates\n        mu_bar = np.mean(valid_replicates)\n        \n        # Empirical Mean Squared Error (MSE)\n        # Note: MSE is computed over the set of valid (non-NaN) replicates for consistency.\n        mse_hat = np.mean((valid_replicates - mu_true)**2)\n\n        # Empirical Variance (with 1/R-style population normalization)\n        var_hat = np.var(valid_replicates)\n        \n        # Squared Empirical Bias\n        bias_sq_hat = (mu_bar - mu_true)**2\n        \n        return [mse_hat, var_hat, bias_sq_hat]\n\n    # Calculate the ground truth value for mu\n    mu_true = calculate_ground_truth()\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (1, 50000, 'Normal', None, 12345),\n        (20, 10000, 'Normal', None, 23456),\n        (50, 5000, 'Student-t', 3, 34567),\n        (200, 2000, 'Normal', None, 45678)\n    ]\n\n    results = []\n    for case in test_cases:\n        N, R, prop_type, dof, seed = case\n        result_triple = run_simulation(N, R, prop_type, dof, seed, mu_true)\n        results.append(result_triple)\n\n    # Final print statement in the exact required format.\n    result_strs = [f\"[{res[0]},{res[1]},{res[2]}]\" for res in results]\n    print(f\"[{','.join(result_strs)}]\")\n\nsolve()\n```", "id": "3306258"}, {"introduction": "The efficiency of Monte Carlo integration is critically dependent on the function being integrated, and singularities can pose a major threat by causing infinite variance. This exercise combines theoretical analysis with computation to explore this very challenge, demonstrating why variance reduction is not just an optimization but often a necessity. You will first determine the conditions under which a standard estimator fails and then analyze how a targeted importance sampling strategy can be used to control the variance and restore convergence [@problem_id:3306284].", "problem": "Consider the integration of a spherically symmetric integrand with a singularity at the origin over a hypercube. Let $d \\in \\mathbb{N}$ with $d \\geq 1$, and let $\\alpha \\in \\mathbb{R}$ with $\\alpha \\geq 0$. Define the function $f:[0,1]^d \\to [0,\\infty)$ by $f(x) = \\|x\\|^{-\\alpha}$, where $\\|x\\|$ is the Euclidean norm in $\\mathbb{R}^d$. We are interested in Monte Carlo integration of $\\int_{[0,1]^d} f(x)\\,dx$ and the error properties of estimators when the sampling distribution is either uniform or designed via importance sampling.\n\nStarting from fundamental definitions:\n- The Monte Carlo estimator under the uniform distribution on $[0,1]^d$ for an integrable function $f$ is the sample mean of independent evaluations of $f$, and its variance (if finite) equals the variance of $f(X)$ when $X$ is uniformly distributed on $[0,1]^d$.\n- The second moment of $f$ under the uniform distribution is $\\int_{[0,1]^d} f(x)^2\\,dx$.\n- The error of the Monte Carlo estimator, under finite variance, decays on the order of $N^{-1/2}$ where $N$ is the number of independent samples, and the mean squared error scales as the variance divided by $N$.\n\nYou must analyze the singularity at $x=0$ using the following base principles only:\n- Spherical coordinates in $\\mathbb{R}^d$ with volume element $r^{d-1}\\,dr\\,d\\Omega$, where $r \\geq 0$ and $d\\Omega$ is the surface area element on the unit sphere $S^{d-1}$.\n- The surface area of $S^{d-1}$ is a known constant denoted by $S_{d-1}$.\n- The integral $\\int_0^\\varepsilon r^\\beta\\,dr$ converges if and only if $\\beta > -1$ for any fixed $\\varepsilon \\in (0,1)$.\n\nYour tasks:\n1. Derive the necessary and sufficient thresholds on $\\alpha$ (in terms of $d$) for:\n   - The integrability of $f$ over $[0,1]^d$ under the uniform measure (i.e., the existence of the mean of $f(X)$ for $X$ uniform on $[0,1]^d$).\n   - The finiteness of the variance of the uniform Monte Carlo estimator (i.e., the finiteness of the second moment $\\int_{[0,1]^d} f(x)^2\\,dx$).\n   Your derivation must reason from the behavior near $x=0$ using spherical coordinates and the above base principles, without quoting the final threshold formulas directly.\n\n2. Propose an importance sampling transformation that specifically targets the singularity near $x=0$ by splitting the domain into a small ball around the origin and its complement. For the inner region of radius $\\varepsilon \\in (0,1)$, restricted to the positive orthant $[0,1]^d$, define a radial importance sampling scheme that samples the radius $r \\in [0,\\varepsilon]$ with a probability density proportional to $r^q$ for some exponent $q$, and samples directions uniformly over the positive orthant of the unit sphere. Provide an error analysis for the inner-region estimator based on this scheme:\n   - Express the importance weight in terms of $r$, $q$, $d$, and $\\alpha$.\n   - Derive the threshold condition on $q$ and $\\alpha$ (in terms of $d$) ensuring the finiteness of the variance of the inner-region estimator, again using only the base principles listed above.\n   - Identify the value of $q$ that would make the inner-region estimator have zero variance if it were admissible, and state the condition on $\\alpha$ for which this is admissible.\n\n3. For a fixed choice $\\varepsilon = 1/2$, express the exact inner-region contribution to the mean and the inner-region contribution to the second moment of $f$ under uniform sampling, both restricted to the positive orthant intersection with the ball of radius $\\varepsilon$. These exact expressions must be written in terms of $S_{d-1}$, $d$, $\\alpha$, and $\\varepsilon$, and should account for the fact that within $[0,1]^d$ the directions are restricted to the positive orthant. If a quantity diverges, indicate it as $+\\infty$.\n\nYour program must implement the following test suite and compute for each case:\n- A boolean indicating whether $f$ is integrable under uniform sampling on $[0,1]^d$.\n- A boolean indicating whether the variance of the uniform Monte Carlo estimator is finite.\n- A boolean indicating, for the inner-region radial importance sampling with exponent choice $q = d - 1 - \\alpha + \\delta$ and $\\delta = (d - \\alpha)/2$ (when $\\alpha < d$; otherwise take this importance sampling boolean to be false), whether the theoretical inner-region variance is finite.\n- A float equal to the exact inner-region mean contribution on $\\{x \\in [0,1]^d: \\|x\\| \\le \\varepsilon\\}$ with $\\varepsilon = 1/2$; if it diverges, return positive infinity.\n- A float equal to the exact inner-region second moment contribution under uniform sampling on $\\{x \\in [0,1]^d: \\|x\\| \\le \\varepsilon\\}$ with $\\varepsilon = 1/2$; if it diverges, return positive infinity.\n\nUse the test suite:\n- Case $1$: $(d,\\alpha) = (2,0.6)$.\n- Case $2$: $(d,\\alpha) = (2,1.2)$.\n- Case $3$: $(d,\\alpha) = (3,1.4)$.\n- Case $4$: $(d,\\alpha) = (3,1.5)$.\n- Case $5$: $(d,\\alpha) = (2,2.0)$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results for the five test cases as a comma-separated list of lists, in the order of the cases above. Each inner list must be of the form $[b_1,b_2,b_3,m,s]$, where $b_1$, $b_2$, and $b_3$ are booleans, $m$ is the inner-region mean float, and $s$ is the inner-region second moment float. Use a plain list-of-lists notation and represent divergence by positive infinity. For example, the printed output must look like\n  \"[[True,False,True,0.123,inf],[...],...]\"\nwith the five inner lists in order.", "solution": "The problem requires an analysis of the convergence properties of Monte Carlo estimators for the integral of $f(x) = \\|x\\|^{-\\alpha}$ over the hypercube $[0,1]^d$. The analysis must be derived from first principles involving spherical coordinates and properties of radial integrals.\n\n### Part 1: Integrability and Variance Thresholds\n\nThe function to be integrated is $f(x) = \\|x\\|^{-\\alpha}$ over the domain $D = [0,1]^d$. The singularity of this function is located at $x=0$. The convergence of the integral $\\int_D f(x) dx$ depends solely on the behavior of the integrand in an arbitrarily small neighborhood of this singularity. Let's consider a small region around the origin, $B_\\varepsilon^+ = \\{x \\in [0,1]^d : \\|x\\| \\le \\varepsilon\\}$ for some small $\\varepsilon \\in (0,1)$. For such an $\\varepsilon$ (e.g., $\\varepsilon \\le 1$), this region is the portion of a $d$-dimensional ball of radius $\\varepsilon$ that lies in the positive orthant. The integral over the remainder of the domain, $D \\setminus B_\\varepsilon^+$, is finite because $f(x)$ is continuous and bounded there. Therefore, the convergence of the integral over $D$ is equivalent to the convergence of the integral over $B_\\varepsilon^+$.\n\nWe use spherical coordinates in $\\mathbb{R}^d$. A point $x$ is represented by its radial distance $r = \\|x\\|$ and a set of angles $\\Omega$ on the unit $(d-1)$-sphere, $S^{d-1}$. The volume element is $dx = r^{d-1} dr d\\Omega$. The function $f(x)$ becomes $f(r) = r^{-\\alpha}$. The region $B_\\varepsilon^+$ corresponds to $r \\in [0, \\varepsilon]$ and angles $\\Omega$ spanning the positive orthant. The surface area of the unit sphere in the positive orthant is $\\int_{S^{d-1}^+} d\\Omega = S_{d-1} / 2^d$, where $S_{d-1}$ is the total surface area of $S^{d-1}$.\n\n**1. Condition for Integrability of $f(x)$**\nThe integral of $f(x)$ is given by $I = \\int_{[0,1]^d} \\|x\\|^{-\\alpha} dx$. Its convergence depends on the integral near the origin:\n$$\n\\int_{B_\\varepsilon^+} f(x) dx = \\int_0^\\varepsilon \\int_{S_{d-1}^+} (r^{-\\alpha}) (r^{d-1} dr d\\Omega) = \\left(\\int_{S_{d-1}^+} d\\Omega\\right) \\left(\\int_0^\\varepsilon r^{-\\alpha} r^{d-1} dr\\right)\n$$\n$$\n= \\frac{S_{d-1}}{2^d} \\int_0^\\varepsilon r^{d-1-\\alpha} dr\n$$\nAccording to the provided base principle, the integral $\\int_0^\\varepsilon r^\\beta dr$ converges if and only if the exponent $\\beta > -1$. In our case, $\\beta = d-1-\\alpha$. Thus, for the integral to converge, we require:\n$$\nd-1-\\alpha > -1 \\implies d-\\alpha > 0 \\implies \\alpha  d\n$$\nThis is the necessary and sufficient condition for the integrability of $f(x)$ over $[0,1]^d$.\n\n**2. Condition for Finiteness of Variance**\nThe variance of the uniform Monte Carlo estimator is finite if and only if the second moment of the function, $E[f(X)^2]$, is finite. For a uniform distribution on $[0,1]^d$ (which has volume $1$), this is equivalent to the convergence of the integral $\\int_{[0,1]^d} f(x)^2 dx$. The integrand is $f(x)^2 = (\\|x\\|^{-\\alpha})^2 = \\|x\\|^{-2\\alpha}$.\nAgain, we analyze the behavior near the origin:\n$$\n\\int_{B_\\varepsilon^+} f(x)^2 dx = \\int_0^\\varepsilon \\int_{S_{d-1}^+} (r^{-2\\alpha}) (r^{d-1} dr d\\Omega) = \\frac{S_{d-1}}{2^d} \\int_0^\\varepsilon r^{d-1-2\\alpha} dr\n$$\nUsing the same convergence principle, the exponent $\\beta = d-1-2\\alpha$ must satisfy $\\beta > -1$:\n$$\nd-1-2\\alpha > -1 \\implies d-2\\alpha > 0 \\implies \\alpha  \\frac{d}{2}\n$$\nThis is the necessary and sufficient condition for the finiteness of the variance of the uniform Monte Carlo estimator.\n\n### Part 2: Importance Sampling Analysis\n\nWe are considering an importance sampling scheme on the inner region $B_\\varepsilon^+$. The radius $r$ is sampled from a probability density function $p_r(r) \\propto r^q$ for $r \\in [0,\\varepsilon]$, and the direction $\\Omega$ is sampled uniformly from the positive orthant of the unit sphere.\n\nThe normalized radial PDF is $p_r(r) = C r^q$. We find $C$ from $\\int_0^\\varepsilon C r^q dr = 1$, which gives $C[\\frac{r^{q+1}}{q+1}]_0^\\varepsilon = 1$. This requires $q > -1$ for convergence at $r=0$. Then $C = \\frac{q+1}{\\varepsilon^{q+1}}$, so $p_r(r) = \\frac{q+1}{\\varepsilon^{q+1}} r^q$.\n\nThe integral of interest is $I_{inner} = \\int_{B_\\varepsilon^+} f(x) dx = \\frac{S_{d-1}}{2^d} \\int_0^\\varepsilon r^{-\\alpha} r^{d-1} dr = \\frac{S_{d-1}}{2^d} \\int_0^\\varepsilon r^{d-1-\\alpha} dr$.\nThe Monte Carlo estimator for this integral based on sampling $r$ from $p_r(r)$ is based on the expectation $E_{p_r}[\\frac{g(r)}{p_r(r)}]$, where $g(r)$ is the function being integrated with respect to $r$. Here, $g(r) = \\frac{S_{d-1}}{2^d} r^{d-1-\\alpha}$.\n\n**1. Importance Weight**\nThe term that is averaged in the estimator is the integrand divided by the sampling density. For a single sample $x=(r, \\Omega)$, the value contributed to the sum is $\\frac{f(x)}{p(x)} = \\frac{r^{-\\alpha}}{p(x)}$, where the full sampling density is $p(x) = p(r, \\Omega) = p_r(r) p_\\Omega(\\Omega) = \\left(\\frac{q+1}{\\varepsilon^{q+1}} r^q\\right) / \\left(\\frac{S_{d-1}}{2^d}\\right)$. The importance weight term in the estimator is proportional to $\\frac{f(x) \\times (\\text{volume element})}{p(x) \\times (\\text{sampling measure})}$, which simplifies to analyzing the radial part. The value of each sample in the sum for the estimator is $\\frac{g(r)}{p_r(r)} = \\frac{(\\frac{S_{d-1}}{2^d}) r^{d-1-\\alpha}}{(\\frac{q+1}{\\varepsilon^{q+1}}) r^q} = \\frac{S_{d-1}}{2^d} \\frac{\\varepsilon^{q+1}}{q+1} r^{d-1-\\alpha-q}$.\n\n**2. Variance Finiteness for Importance Sampling**\nThe variance of the estimator is finite if $\\int_0^\\varepsilon \\frac{g(r)^2}{p_r(r)} dr$ is finite.\n$$\n\\int_0^\\varepsilon \\frac{g(r)^2}{p_r(r)} dr = \\int_0^\\varepsilon \\frac{\\left( \\frac{S_{d-1}}{2^d} r^{d-1-\\alpha} \\right)^2}{\\frac{q+1}{\\varepsilon^{q+1}} r^q} dr = \\left( \\frac{S_{d-1}}{2^d} \\right)^2 \\frac{\\varepsilon^{q+1}}{q+1} \\int_0^\\varepsilon \\frac{r^{2(d-1-\\alpha)}}{r^q} dr\n$$\n$$\n\\propto \\int_0^\\varepsilon r^{2(d-1-\\alpha)-q} dr = \\int_0^\\varepsilon r^{2d-2-2\\alpha-q} dr\n$$\nFor this integral to converge, the exponent $\\beta = 2d-2-2\\alpha-q$ must be greater than $-1$:\n$$\n2d-2-2\\alpha-q > -1 \\implies 2(d-1-\\alpha) > q-1\n$$\n\n**3. Optimal Choice of $q$**\nA zero-variance estimator is achieved if the quantity being averaged, $\\frac{g(r)}{p_r(r)}$, is a constant. This means $p_r(r)$ must be proportional to $g(r)$.\n$$\np_r(r) \\propto g(r) \\propto r^{d-1-\\alpha}\n$$\nComparing this with the sampling form $p_r(r) \\propto r^q$, the optimal choice is $q = d-1-\\alpha$.\nFor this choice of $q$ to define a valid probability distribution, the normalization integral $\\int_0^\\varepsilon r^q dr$ must converge, which requires $q > -1$.\nTherefore, the condition for this optimal choice to be admissible is:\n$$\nd-1-\\alpha > -1 \\implies \\alpha  d\n$$\nThis is the same as the condition for the original integral to be finite. If the integral is infinite, no importance sampling scheme can yield a finite estimate with finite variance.\n\n### Part 3: Exact Inner-Region Expressions\n\nWe are given $\\varepsilon = 1/2$. The inner region is $B_{1/2}^+ = \\{x \\in [0,1]^d : \\|x\\| \\le 1/2\\}$.\n\n**1. Inner-Region Mean Contribution**\nThe mean contribution is $m = \\int_{B_{1/2}^+} f(x) dx$. From Part 1, this is:\n$$\nm = \\frac{S_{d-1}}{2^d} \\int_0^{1/2} r^{d-1-\\alpha} dr\n$$\nIf $\\alpha  d$ (i.e., $d-1-\\alpha > -1$), the integral evaluates to:\n$$\n\\int_0^{1/2} r^{d-1-\\alpha} dr = \\left[ \\frac{r^{d-\\alpha}}{d-\\alpha} \\right]_0^{1/2} = \\frac{(1/2)^{d-\\alpha}}{d-\\alpha}\n$$\nSo, for $\\alpha  d$, $m = \\frac{S_{d-1}}{2^d} \\frac{(0.5)^{d-\\alpha}}{d-\\alpha}$.\nIf $\\alpha \\ge d$, the integral diverges, and $m = +\\infty$.\n\n**2. Inner-Region Second Moment Contribution**\nThe second moment contribution is $s = \\int_{B_{1/2}^+} f(x)^2 dx$. From Part 1, this is:\n$$\ns = \\frac{S_{d-1}}{2^d} \\int_0^{1/2} r^{d-1-2\\alpha} dr\n$$\nIf $\\alpha  d/2$ (i.e., $d-1-2\\alpha > -1$), the integral evaluates to:\n$$\n\\int_0^{1/2} r^{d-1-2\\alpha} dr = \\left[ \\frac{r^{d-2\\alpha}}{d-2\\alpha} \\right]_0^{1/2} = \\frac{(1/2)^{d-2\\alpha}}{d-2\\alpha}\n$$\nSo, for $\\alpha  d/2$, $s = \\frac{S_{d-1}}{2^d} \\frac{(0.5)^{d-2\\alpha}}{d-2\\alpha}$.\nIf $\\alpha \\ge d/2$, the integral diverges, and $s = +\\infty$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import gamma\n\ndef solve():\n    \"\"\"\n    Solves the problem for the given test cases based on the derived theoretical thresholds\n    and formulas for Monte Carlo integration of a singular function.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (2, 0.6),  # Case 1\n        (2, 1.2),  # Case 2\n        (3, 1.4),  # Case 3\n        (3, 1.5),  # Case 4\n        (2, 2.0),  # Case 5\n    ]\n\n    results = []\n    \n    for d, alpha in test_cases:\n        # Task 1: Check integrability and finite variance for uniform sampling.\n        # Condition for integrability: alpha  d\n        b1 = alpha  d\n        \n        # Condition for finite variance: alpha  d / 2\n        b2 = alpha  d / 2\n\n        # Task 2: Check finite variance for the specified importance sampling scheme.\n        # The scheme is defined for alpha  d.\n        # The condition for finite variance of this scheme was derived to be alpha  d.\n        # Therefore, b3 is true if and only if alpha  d.\n        b3 = alpha  d\n\n        # Task 3: Calculate inner-region mean and second moment contributions.\n        epsilon = 0.5\n        \n        # Calculate surface area of (d-1)-sphere, S_{d-1} = 2 * pi^(d/2) / Gamma(d/2)\n        try:\n            sd_minus_1 = 2 * (np.pi**(d/2)) / gamma(d/2)\n        except (ValueError, TypeError):\n            # This case shouldn't be hit with integer d = 1\n            sd_minus_1 = np.nan\n\n        # Calculate inner-region mean contribution 'm'\n        # Formula: (S_{d-1} / 2^d) * (epsilon^(d-alpha) / (d-alpha))\n        # This is finite if and only if alpha  d.\n        if alpha  d:\n            m = (sd_minus_1 / (2**d)) * (epsilon**(d - alpha)) / (d - alpha)\n        else:\n            m = np.inf\n\n        # Calculate inner-region second moment contribution 's'\n        # Formula: (S_{d-1} / 2^d) * (epsilon^(d-2*alpha) / (d-2*alpha))\n        # This is finite if and only if alpha  d / 2.\n        if alpha  d / 2:\n            s = (sd_minus_1 / (2**d)) * (epsilon**(d - 2 * alpha)) / (d - 2 * alpha)\n        else:\n            s = np.inf\n            \n        # Append the list of results for the current test case.\n        results.append([b1, b2, b3, m, s])\n\n    # Custom string formatting to match the required output format \"[[True,False,True,0.123,inf],...]\"\n    def format_list(lst):\n        items = []\n        for item in lst:\n            if isinstance(item, bool):\n                items.append(str(item))\n            elif isinstance(item, float):\n                if np.isinf(item):\n                    items.append('inf')\n                else:\n                    items.append(str(item))\n            else: # Should not happen based on logic\n                items.append(str(item))\n        return f\"[{','.join(items)}]\"\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(format_list, results))}]\")\n\nsolve()\n```", "id": "3306284"}]}