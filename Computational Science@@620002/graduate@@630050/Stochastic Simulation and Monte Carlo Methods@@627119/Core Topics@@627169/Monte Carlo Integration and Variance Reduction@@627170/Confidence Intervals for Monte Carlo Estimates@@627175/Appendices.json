{"hands_on_practices": [{"introduction": "Effective Monte Carlo simulation is not just about running more trials, but about running them smartly. This practice tackles the problem of optimal resource allocation in stratified sampling, a powerful variance reduction technique. By working through this exercise [@problem_id:3298330], you will derive the celebrated Neyman allocation, which provides a blueprint for distributing a fixed computational budget across different strata to minimize the variance, and thus the confidence interval width, of your final estimate.", "problem": "A computational scientist seeks to estimate the overall mean of a quantity of interest under a stratified Monte Carlo design. The population is partitioned into $H \\geq 2$ disjoint strata indexed by $h \\in \\{1,\\dots,H\\}$. The stratum weights are known and given by $\\{p_h\\}_{h=1}^{H}$ with $\\sum_{h=1}^{H} p_h = 1$ and $p_h \\in (0,1)$ for each $h$. Within stratum $h$, independent and identically distributed random variables $\\{X_{h,i}\\}_{i=1}^{n_h}$ are sampled with finite variance $\\sigma_h^2 \\in (0,\\infty)$. The aim is to estimate the global mean $\\mu = \\sum_{h=1}^{H} p_h \\mu_h$, where $\\mu_h = \\mathbb{E}[X_{h,1}]$, using the stratified Monte Carlo estimator $\\hat{\\mu} = \\sum_{h=1}^{H} p_h \\bar{X}_h$ with $\\bar{X}_h = n_h^{-1} \\sum_{i=1}^{n_h} X_{h,i}$. The allocations $\\{n_h\\}$ are design variables to be chosen by the scientist prior to sampling.\n\nEach sample drawn in stratum $h$ incurs a known, positive per-unit cost $c_h \\in (0,\\infty)$. The total available budget is a fixed amount $C \\in (0,\\infty)$, and the design must satisfy the deterministic budget constraint $\\sum_{h=1}^{H} c_h n_h = C$. Assume that $n_h$ are treated as real-valued design variables for the purpose of optimization, and that $\\{X_{h,i}\\}$ are independent across $h$ and $i$.\n\nUsing only foundational principles, namely: (i) the variance of a stratum sample mean is $\\sigma_h^2 / n_h$ under independence and identical distribution within each stratum, and (ii) asymptotic normality of $\\hat{\\mu}$ follows from the Central Limit Theorem (CLT) so that the two-sided $(1-\\alpha)$ confidence interval has half-width proportional to $\\sqrt{\\operatorname{Var}(\\hat{\\mu})}$ for fixed $\\alpha \\in (0,1)$, derive the allocation rule $\\{n_h\\}$ that minimizes the asymptotic confidence interval half-width subject to the budget constraint. Then, express the resulting minimal variance $\\operatorname{Var}(\\hat{\\mu})$ in closed form as a function of $C$, $\\{p_h\\}$, $\\{\\sigma_h\\}$, and $\\{c_h\\}$.\n\nProvide your final answer as a single closed-form analytic expression that simultaneously specifies: (i) the optimal allocation $n_h$ for each stratum $h$, and (ii) the minimized variance of $\\hat{\\mu}$. No numerical evaluation is required, and no rounding is needed. Express your final answer in terms of $C$, $\\{p_h\\}$, $\\{\\sigma_h\\}$, and $\\{c_h\\}$ only.", "solution": "The problem is to determine the optimal allocation of samples, $\\{n_h\\}_{h=1}^H$, in a stratified Monte Carlo setting to minimize the variance of the estimator for the global mean, subject to a total budget constraint. The minimization of the variance is equivalent to minimizing the asymptotic confidence interval half-width, as stated in the problem.\n\nFirst, we formalize the optimization problem. The estimator for the global mean $\\mu = \\sum_{h=1}^{H} p_h \\mu_h$ is given by $\\hat{\\mu} = \\sum_{h=1}^{H} p_h \\bar{X}_h$. The variance of this estimator, which we aim to minimize, is the objective function.\n$$ \\operatorname{Var}(\\hat{\\mu}) = \\operatorname{Var}\\left( \\sum_{h=1}^{H} p_h \\bar{X}_h \\right) $$\nGiven that the samples $\\{X_{h,i}\\}$ are independent across different strata $h$, the variance of the sum is the sum of the variances:\n$$ \\operatorname{Var}(\\hat{\\mu}) = \\sum_{h=1}^{H} \\operatorname{Var}(p_h \\bar{X}_h) = \\sum_{h=1}^{H} p_h^2 \\operatorname{Var}(\\bar{X}_h) $$\nThe problem provides that the variance of the sample mean within stratum $h$ is $\\operatorname{Var}(\\bar{X}_h) = \\frac{\\sigma_h^2}{n_h}$. Substituting this expression, we obtain the objective function $V(\\{n_h\\})$:\n$$ V(\\{n_h\\}) = \\sum_{h=1}^{H} \\frac{p_h^2 \\sigma_h^2}{n_h} $$\nThis objective function is to be minimized with respect to the allocation vector $\\{n_1, n_2, \\dots, n_H\\}$, subject to the budget constraint:\n$$ \\sum_{h=1}^{H} c_h n_h = C $$\nWe treat the sample sizes $n_h$ as continuous positive real variables for the purpose of this optimization.\n\nThis is a constrained optimization problem that can be solved using the method of Lagrange multipliers. The Lagrangian function $\\mathcal{L}$ is defined as:\n$$ \\mathcal{L}(\\{n_h\\}, \\lambda) = \\sum_{h=1}^{H} \\frac{p_h^2 \\sigma_h^2}{n_h} + \\lambda \\left( \\sum_{h=1}^{H} c_h n_h - C \\right) $$\nwhere $\\lambda$ is the Lagrange multiplier. To find the minimum, we set the partial derivatives of $\\mathcal{L}$ with respect to each $n_k$ (for $k \\in \\{1,\\dots,H\\}$) to zero:\n$$ \\frac{\\partial \\mathcal{L}}{\\partial n_k} = -\\frac{p_k^2 \\sigma_k^2}{n_k^2} + \\lambda c_k = 0 $$\nSince all $p_k, \\sigma_k, c_k$ are positive, $\\lambda$ must also be positive. Rearranging this equation to solve for $n_k$:\n$$ n_k^2 = \\frac{p_k^2 \\sigma_k^2}{\\lambda c_k} $$\nSince sample sizes $n_k$ must be positive, we take the positive square root:\n$$ n_k = \\frac{p_k \\sigma_k}{\\sqrt{\\lambda} \\sqrt{c_k}} $$\nThis relation shows that the optimal sample size for a stratum is proportional to its weight $p_k$ and standard deviation $\\sigma_k$, and inversely proportional to the square root of its sampling cost $\\sqrt{c_k}$.\n\nTo find the constant of proportionality (which depends on $\\lambda$), we substitute this expression for $n_k$ into the budget constraint equation:\n$$ \\sum_{k=1}^{H} c_k \\left( \\frac{p_k \\sigma_k}{\\sqrt{\\lambda} \\sqrt{c_k}} \\right) = C $$\n$$ \\frac{1}{\\sqrt{\\lambda}} \\sum_{k=1}^{H} p_k \\sigma_k \\sqrt{c_k} = C $$\nWe can now solve for the term $1/\\sqrt{\\lambda}$:\n$$ \\frac{1}{\\sqrt{\\lambda}} = \\frac{C}{\\sum_{j=1}^{H} p_j \\sigma_j \\sqrt{c_j}} $$\nSubstituting this back into the expression for $n_k$ gives the explicit optimal allocation rule for each stratum $k$:\n$$ n_k = \\left( \\frac{C}{\\sum_{j=1}^{H} p_j \\sigma_j \\sqrt{c_j}} \\right) \\frac{p_k \\sigma_k}{\\sqrt{c_k}} = C \\frac{p_k \\sigma_k / \\sqrt{c_k}}{\\sum_{j=1}^{H} p_j \\sigma_j \\sqrt{c_j}} $$\n\nNext, we derive the minimized variance, $\\operatorname{Var}(\\hat{\\mu})_{min}$, by substituting the optimal allocations back into the variance formula. A more direct path is to use the first-order condition from the Lagrangian:\nFrom $\\lambda c_k = \\frac{p_k^2 \\sigma_k^2}{n_k^2}$, we can multiply by $n_k$ to get $\\lambda c_k n_k = \\frac{p_k^2 \\sigma_k^2}{n_k}$.\nThe total variance is the sum of these terms over all strata:\n$$ \\operatorname{Var}(\\hat{\\mu})_{min} = \\sum_{k=1}^{H} \\frac{p_k^2 \\sigma_k^2}{n_k} = \\sum_{k=1}^{H} \\lambda c_k n_k = \\lambda \\sum_{k=1}^{H} c_k n_k $$\nUsing the budget constraint $\\sum_{k=1}^{H} c_k n_k = C$, we find a simple relationship:\n$$ \\operatorname{Var}(\\hat{\\mu})_{min} = \\lambda C $$\nTo get the final expression, we need $\\lambda$. From our earlier result for $1/\\sqrt{\\lambda}$, we square both sides to find $1/\\lambda$:\n$$ \\frac{1}{\\lambda} = \\frac{C^2}{\\left( \\sum_{j=1}^{H} p_j \\sigma_j \\sqrt{c_j} \\right)^2} \\implies \\lambda = \\frac{1}{C^2} \\left( \\sum_{j=1}^{H} p_j \\sigma_j \\sqrt{c_j} \\right)^2 $$\nSubstituting this expression for $\\lambda$ into the equation for the minimal variance:\n$$ \\operatorname{Var}(\\hat{\\mu})_{min} = \\left( \\frac{1}{C^2} \\left( \\sum_{h=1}^{H} p_h \\sigma_h \\sqrt{c_h} \\right)^2 \\right) C = \\frac{1}{C} \\left( \\sum_{h=1}^{H} p_h \\sigma_h \\sqrt{c_h} \\right)^2 $$\nThis provides the closed-form expression for the minimal variance under optimal allocation.\n\nThe final answer comprises two parts: the formula for the optimal allocation $n_h$ for any stratum $h$, and the formula for the resulting minimal variance of the estimator $\\hat{\\mu}$. These are expressed as requested.", "answer": "$$ \\boxed{\\begin{pmatrix} n_h = C \\frac{p_h \\sigma_h / \\sqrt{c_h}}{\\sum_{k=1}^{H} p_k \\sigma_k \\sqrt{c_k}} & \\operatorname{Var}(\\hat{\\mu}) = \\frac{1}{C} \\left( \\sum_{h=1}^{H} p_h \\sigma_h \\sqrt{c_h} \\right)^2 \\end{pmatrix}} $$", "id": "3298330"}, {"introduction": "While the Central Limit Theorem provides the basis for the most common type of confidence interval, its guarantees are asymptotic and depend on the underlying distribution. This hands-on coding exercise [@problem_id:3298410] challenges you to implement and compare this standard CLT-based interval against non-asymptotic alternatives derived from Hoeffding's and Bernstein's inequalities. Through simulation, you will gain a practical understanding of the trade-offs between these methods in terms of coverage and efficiency, particularly for finite sample sizes or skewed distributions.", "problem": "You are given a bounded independent and identically distributed sequence $\\{X_i\\}_{i=1}^n$ with $X_i \\in [a,b]$ and finite mean $\\mu = \\mathbb{E}[X_i]$. The Monte Carlo estimator of the mean is the sample average $\\bar{X}_n = \\frac{1}{n}\\sum_{i=1}^n X_i$. A $(1-\\alpha)$ confidence interval is any random interval centered at $\\bar{X}_n$ that contains the true mean $\\mu$ with probability at least $1-\\alpha$ under the data generating mechanism.\n\nThe goal is to compare the empirical coverage properties and average half-widths of three confidence intervals for $\\mu$ in the bounded setting, each constructed from a different foundational principle: one based on Hoeffding’s inequality, one based on Bernstein’s inequality with a data-driven variance proxy, and one based on the Central Limit Theorem (CLT) using a normal approximation. You must begin from foundational definitions of the Monte Carlo estimator, basic probability inequalities for bounded random variables, and the Central Limit Theorem; deduce appropriate half-widths for each interval; and then implement a simulation to estimate coverage probabilities and average half-widths.\n\nYour program must:\n- Implement three two-sided $(1-\\alpha)$ confidence intervals for $\\mu$, each centered at $\\bar{X}_n$, obtained respectively from:\n  1. A bound derived from Hoeffding’s inequality for bounded random variables in $[a,b]$.\n  2. A bound derived from Bernstein’s inequality that incorporates a variance term and is operationalized using the sample variance as a plug-in proxy.\n  3. A CLT-based interval using a normal quantile and the sample standard deviation.\n- For each interval construction, empirically estimate its coverage probability, defined as the fraction of Monte Carlo replications in which the interval contains the true mean $\\mu$, and compute its average half-width across replications.\n- Use a fixed random seed for reproducibility.\n\nYou must simulate from bounded distributions to create diverse regimes of range, variance, skewness, and sample size. Use the following test suite, where each test case specifies the distribution, its parameters, the bounds $[a,b]$, the sample size $n$, the nominal level $\\alpha$, and the number of Monte Carlo replications $R$. For each case below, the true mean $\\mu$ is determined by the distribution and parameters, and should be used in coverage checks.\n\nTest Suite (four cases):\n1. Case U-small: Uniform distribution on $[a,b]$ with $a=0$, $b=1$, $n=20$, $\\alpha=0.05$, $R=5000$.\n2. Case T-skew: Triangular distribution on $[a,b]$ with $a=0$, $b=1$, mode $m=0.9$, $n=40$, $\\alpha=0.05$, $R=5000$. The triangular distribution has density supported on $[a,b]$ and mode at $m \\in [a,b]$; its mean is $(a+b+m)/3$.\n3. Case B-wide: Beta distribution on $[0,1]$ with shape parameters $(\\alpha_{\\text{B}},\\beta_{\\text{B}})=(0.5,3)$, linearly scaled to $[a,b]$ with $a=-1$, $b=2$, $n=100$, $\\alpha=0.05$, $R=5000$. The mean on $[0,1]$ is $\\alpha_{\\text{B}}/(\\alpha_{\\text{B}}+\\beta_{\\text{B}})$; after scaling to $[a,b]$ it becomes $a + (b-a)\\, \\alpha_{\\text{B}}/(\\alpha_{\\text{B}}+\\beta_{\\text{B}})$.\n4. Case B-large-n: Beta distribution on $[0,1]$ with $(\\alpha_{\\text{B}},\\beta_{\\text{B}})=(2,2)$, scaled to $[a,b]$ with $a=-0.5$, $b=0.5$, $n=1000$, $\\alpha=0.05$, $R=2000$.\n\nImplementation requirements:\n- For each replication, draw $n$ independent samples $X_1,\\dots,X_n$ according to the specified case, compute $\\bar{X}_n$, compute the sample variance $s^2$ with $n-1$ degrees of freedom, and construct the three intervals as described above.\n- Use a fixed random seed for all simulations.\n- For the Bernstein-based interval, use a plug-in approach with the sample variance within the inequality to determine the half-width. For the CLT-based interval, use the $(1-\\alpha/2)$ standard normal quantile and the sample standard deviation.\n- For each case, return:\n  - The empirical coverage probabilities for the Hoeffding, Bernstein, and CLT intervals as three floats.\n  - The average half-widths of the Hoeffding, Bernstein, and CLT intervals as three floats.\n  - A single integer code indicating the “preferable” method according to the following rule: among the intervals whose empirical coverage is at least $1-\\alpha$, select the method with the smallest average half-width; if none achieve coverage at least $1-\\alpha$, select the method with the largest empirical coverage, breaking ties by smallest average half-width. Encode Hoeffding as $0$, Bernstein as $1$, and CLT as $2$.\n\nFinal Output Format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case and is itself a list of the form:\n  - $[\\,[\\text{cov}_{\\text{H}},\\text{cov}_{\\text{B}},\\text{cov}_{\\text{C}}],\\,[\\text{hw}_{\\text{H}},\\text{hw}_{\\text{B}},\\text{hw}_{\\text{C}}],\\,\\text{pref}\\,]$\n  - Here, $\\text{cov}_{\\text{H}}$, $\\text{cov}_{\\text{B}}$, $\\text{cov}_{\\text{C}}$ are floats for empirical coverage of Hoeffding, Bernstein, and CLT intervals respectively; $\\text{hw}_{\\text{H}}$, $\\text{hw}_{\\text{B}}$, $\\text{hw}_{\\text{C}}$ are floats for average half-widths; and $\\text{pref}$ is the integer preference code in $\\{0,1,2\\}$.\nYour program must print exactly one line in this format and must not read any input.", "solution": "The problem requires a comparison of three distinct methods for constructing a $(1-\\alpha)$ confidence interval for the mean $\\mu$ of a bounded random variable, estimated using the Monte Carlo sample mean $\\bar{X}_n$. The methods are based on Hoeffding's inequality, Bernstein's inequality, and the Central Limit Theorem (CLT). A confidence interval is of the form $[\\bar{X}_n - \\epsilon, \\bar{X}_n + \\epsilon]$, where $\\epsilon$ is the half-width. The core of the task is to derive the expression for $\\epsilon$ for each method and then empirically evaluate the performance (coverage probability and average half-width) via simulation.\n\nLet $\\{X_i\\}_{i=1}^n$ be a sequence of independent and identically distributed (i.i.d.) random variables, with $X_i \\in [a, b]$ for known bounds $a$ and $b$. The estimator for the mean $\\mu = \\mathbb{E}[X_i]$ is the sample mean $\\bar{X}_n = \\frac{1}{n}\\sum_{i=1}^n X_i$. A two-sided $(1-\\alpha)$ confidence interval must satisfy $\\mathbb{P}(\\mu \\in [\\bar{X}_n - \\epsilon, \\bar{X}_n + \\epsilon]) \\ge 1-\\alpha$, which is equivalent to controlling the tail probability $\\mathbb{P}(|\\bar{X}_n - \\mu| \\ge \\epsilon) \\le \\alpha$.\n\n### 1. Confidence Interval Derivations\n\n#### 1.1. Hoeffding's Interval\nHoeffding's inequality provides a distribution-free bound on the deviation of a sample mean from its expected value for bounded random variables. For i.i.d. $X_i \\in [a, b]$, the inequality is:\n$$ \\mathbb{P}(|\\bar{X}_n - \\mu| \\ge \\epsilon) \\le 2 \\exp\\left(-\\frac{2n\\epsilon^2}{(b-a)^2}\\right) $$\nTo construct a $(1-\\alpha)$ confidence interval, we set the right-hand side to $\\alpha$ and solve for the half-width, which we denote $\\epsilon_H$:\n$$ 2 \\exp\\left(-\\frac{2n\\epsilon_H^2}{(b-a)^2}\\right) = \\alpha $$\n$$ \\exp\\left(-\\frac{2n\\epsilon_H^2}{(b-a)^2}\\right) = \\frac{\\alpha}{2} $$\nTaking the natural logarithm of both sides:\n$$ -\\frac{2n\\epsilon_H^2}{(b-a)^2} = \\ln\\left(\\frac{\\alpha}{2}\\right) = -\\ln\\left(\\frac{2}{\\alpha}\\right) $$\nSolving for $\\epsilon_H$:\n$$ \\epsilon_H^2 = \\frac{(b-a)^2}{2n}\\ln\\left(\\frac{2}{\\alpha}\\right) $$\n$$ \\epsilon_H = (b-a) \\sqrt{\\frac{\\ln(2/\\alpha)}{2n}} $$\nThis half-width is deterministic, depending only on the problem parameters $n$, $\\alpha$, and the range $(b-a)$, not on the observed data. It guarantees coverage of at least $(1-\\alpha)$ but is often conservative (i.e., wider than necessary).\n\n#### 1.2. Bernstein's Interval\nBernstein's inequality is a more refined concentration inequality that incorporates the variance of the random variables, leading to tighter bounds when the variance is small. A common form of Bernstein's inequality for the sample mean is:\n$$ \\mathbb{P}(|\\bar{X}_n - \\mu| \\ge \\epsilon) \\le 2 \\exp\\left(-\\frac{n\\epsilon^2}{2(\\sigma^2 + C\\epsilon/3)}\\right) $$\nwhere $\\sigma^2 = \\text{Var}(X_i)$ and $C$ is an upper bound on $|X_i - \\mu|$. A safe, though potentially loose, choice for $C$ is the range of the data, $C = b-a$.\n\nSince the true variance $\\sigma^2$ is unknown, the problem specifies using the sample variance $s^2 = \\frac{1}{n-1}\\sum_{i=1}^n(X_i - \\bar{X}_n)^2$ as a \"plug-in proxy\". This makes the resulting half-width, $\\epsilon_B$, data-dependent. We set the bound to $\\alpha$ and solve for $\\epsilon_B$:\n$$ 2 \\exp\\left(-\\frac{n\\epsilon_B^2}{2(s^2 + (b-a)\\epsilon_B/3)}\\right) = \\alpha $$\nLetting $K = \\ln(2/\\alpha)$, this simplifies to:\n$$ \\frac{n\\epsilon_B^2}{2(s^2 + (b-a)\\epsilon_B/3)} = K $$\n$$ n\\epsilon_B^2 = 2K(s^2 + (b-a)\\epsilon_B/3) $$\nRearranging gives a quadratic equation in $\\epsilon_B$:\n$$ n\\epsilon_B^2 - \\left(\\frac{2K(b-a)}{3}\\right)\\epsilon_B - 2Ks^2 = 0 $$\nThe positive root of this quadratic equation, found using the quadratic formula, gives the half-width:\n$$ \\epsilon_B = \\frac{\\frac{2K(b-a)}{3} + \\sqrt{\\left(\\frac{2K(b-a)}{3}\\right)^2 + 8nKs^2}}{2n} $$\nThis \"empirical Bernstein\" interval is expected to be tighter than Hoeffding's, adapting to the observed sample variance.\n\n#### 1.3. Central Limit Theorem (CLT) Interval\nThe Central Limit Theorem states that for large $n$, the distribution of the standardized sample mean converges to a standard normal distribution:\n$$ \\frac{\\bar{X}_n - \\mu}{\\sigma/\\sqrt{n}} \\xrightarrow{d} \\mathcal{N}(0, 1) $$\nThis justifies the approximation:\n$$ \\mathbb{P}\\left(-z_{1-\\alpha/2} \\le \\frac{\\bar{X}_n - \\mu}{\\sigma/\\sqrt{n}} \\le z_{1-\\alpha/2}\\right) \\approx 1-\\alpha $$\nwhere $z_{1-\\alpha/2}$ is the $(1-\\alpha/2)$-quantile of the standard normal distribution. By Slutsky's theorem, we can replace the unknown population standard deviation $\\sigma$ with the sample standard deviation $s = \\sqrt{s^2}$ without changing the asymptotic distribution. Rearranging the inequality gives the familiar CLT-based confidence interval with half-width $\\epsilon_C$:\n$$ \\epsilon_C = z_{1-\\alpha/2} \\frac{s}{\\sqrt{n}} $$\nThis interval is asymptotically exact, meaning its coverage probability converges to exactly $(1-\\alpha)$ as $n \\to \\infty$. However, for finite $n$, especially with skewed or heavy-tailed distributions, its actual coverage may deviate from the nominal level.\n\n### 2. Simulation and Evaluation\n\nThe program implements a Monte Carlo simulation to evaluate these three intervals for a suite of test cases. For each case, the simulation proceeds as follows:\n1.  A total of $R$ replications are performed.\n2.  In each replication, a random sample of size $n$ is drawn from the specified distribution.\n3.  The sample mean $\\bar{X}_n$ and sample variance $s^2$ (with $n-1$ degrees of freedom) are computed.\n4.  The three half-widths—$\\epsilon_H$ (constant), $\\epsilon_B$ (depends on $s^2$), and $\\epsilon_C$ (depends on $s$)—are calculated.\n5.  For each of the three intervals, we check if it contains the true mean $\\mu$ (i.e., if $|\\bar{X}_n - \\mu| \\le \\epsilon$).\n6.  After all replications, the empirical coverage probability for each method is the fraction of times the interval contained $\\mu$. The average half-width is the mean of the calculated half-widths over all replications.\n\nFinally, a \"preferable\" method is selected based on a two-tiered rule: among methods achieving at least the nominal coverage $(1-\\alpha)$, the one with the smallest average half-width is chosen. If no method achieves this coverage, the one with the highest empirical coverage is chosen, with ties broken by the smallest average half-width.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation study and print results.\n    \"\"\"\n    # Use a fixed random seed for all simulations for reproducibility.\n    # The RNG is created once and passed to the simulation function.\n    rng = np.random.default_rng(12345)\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: Uniform distribution, small sample size\n        {'dist': 'uniform', 'params': {}, 'a': 0.0, 'b': 1.0, 'n': 20, 'alpha': 0.05, 'R': 5000},\n        # Case 2: Skewed triangular distribution\n        {'dist': 'triangular', 'params': {'mode': 0.9}, 'a': 0.0, 'b': 1.0, 'n': 40, 'alpha': 0.05, 'R': 5000},\n        # Case 3: Scaled and skewed Beta distribution\n        {'dist': 'beta', 'params': {'alpha_B': 0.5, 'beta_B': 3.0}, 'a': -1.0, 'b': 2.0, 'n': 100, 'alpha': 0.05, 'R': 5000},\n        # Case 4: Scaled symmetric Beta distribution, large sample size\n        {'dist': 'beta', 'params': {'alpha_B': 2.0, 'beta_B': 2.0}, 'a': -0.5, 'b': 0.5, 'n': 1000, 'alpha': 0.05, 'R': 2000},\n    ]\n\n    results = []\n    for case in test_cases:\n        result = run_simulation_for_case(case, rng)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef run_simulation_for_case(case, rng):\n    \"\"\"\n    Runs the simulation for a single test case.\n    \"\"\"\n    dist, params, a, b, n, alpha, R = case['dist'], case['params'], case['a'], case['b'], case['n'], case['alpha'], case['R']\n    \n    # Calculate the true mean (mu) for the distribution\n    if dist == 'uniform':\n        mu = (a + b) / 2.0\n    elif dist == 'triangular':\n        mu = (a + b + params['mode']) / 3.0\n    elif dist == 'beta':\n        mu_01 = params['alpha_B'] / (params['alpha_B'] + params['beta_B'])\n        mu = a + (b - a) * mu_01\n\n    # Pre-calculate constants for interval calculations\n    z_quantile = norm.ppf(1.0 - alpha / 2.0)\n    hoeffding_hw = (b - a) * np.sqrt(np.log(2.0 / alpha) / (2.0 * n))\n    bernstein_K = np.log(2.0 / alpha)\n    bernstein_C_param = b - a\n\n    # Initialize accumulators for metrics\n    coverage_counts = np.zeros(3)  # [Hoeffding, Bernstein, CLT]\n    hw_sums = np.zeros(3)          # [Hoeffding, Bernstein, CLT]\n\n    for _ in range(R):\n        # 1. Generate a sample of size n from the specified distribution\n        if dist == 'uniform':\n            samples = rng.uniform(low=a, high=b, size=n)\n        elif dist == 'triangular':\n            samples = rng.triangular(left=a, mode=params['mode'], right=b, size=n)\n        elif dist == 'beta':\n            samples_01 = rng.beta(a=params['alpha_B'], b=params['beta_B'], size=n)\n            samples = a + (b - a) * samples_01\n\n        # 2. Compute sample statistics\n        x_bar = np.mean(samples)\n        s_squared = np.var(samples, ddof=1)\n        # Ensure variance is non-negative for robustness\n        s_squared = max(0.0, s_squared)\n        s = np.sqrt(s_squared)\n\n        # 3. Calculate interval half-widths for this sample\n        # Hoeffding half-width is constant\n        \n        # Bernstein half-width\n        quad_A = float(n)\n        quad_B = -2.0 * bernstein_C_param * bernstein_K / 3.0\n        quad_C = -2.0 * s_squared * bernstein_K\n        discriminant = quad_B**2 - 4.0 * quad_A * quad_C\n        bernstein_hw = (-quad_B + np.sqrt(discriminant)) / (2.0 * quad_A)\n\n        # CLT half-width\n        clt_hw = z_quantile * s / np.sqrt(n)\n        \n        half_widths = [hoeffding_hw, bernstein_hw, clt_hw]\n        hw_sums += half_widths\n\n        # 4. Check coverage for each method\n        deviation = np.abs(x_bar - mu)\n        if deviation = half_widths[0]:\n            coverage_counts[0] += 1\n        if deviation = half_widths[1]:\n            coverage_counts[1] += 1\n        if deviation = half_widths[2]:\n            coverage_counts[2] += 1\n\n    # 5. Calculate final empirical metrics\n    empirical_coverages = (coverage_counts / R).tolist()\n    average_half_widths = (hw_sums / R).tolist()\n\n    # 6. Apply the preference rule\n    methods = [\n        (empirical_coverages[0], average_half_widths[0], 0), # Hoeffding\n        (empirical_coverages[1], average_half_widths[1], 1), # Bernstein\n        (empirical_coverages[2], average_half_widths[2], 2)  # CLT\n    ]\n\n    target_coverage = 1.0 - alpha\n    valid_methods = [m for m in methods if m[0] >= target_coverage]\n    \n    if valid_methods:\n        # Among methods with sufficient coverage, choose the one with the smallest half-width\n        best_method = min(valid_methods, key=lambda x: x[1])\n    else:\n        # If no method has sufficient coverage, choose the one with the highest coverage\n        # Break ties using the smallest half-width.\n        # Sorting by (-coverage, half-width) achieves this.\n        best_method = max(methods, key=lambda x: (x[0], -x[1]))\n\n    pref_code = best_method[2]\n\n    # Format result for this case\n    return [empirical_coverages, average_half_widths, pref_code]\n\nsolve()\n```", "id": "3298410"}, {"introduction": "Many advanced Monte Carlo methods, such as self-normalized importance sampling, require estimating a ratio of two expectations, $\\rho = \\mu_Y / \\mu_Z$. This seemingly simple task can be fraught with peril, especially when the denominator's mean $\\mu_Z$ is close to zero. This problem [@problem_id:3298323] guides you through a case study comparing the standard delta method with the more robust Fieller's method, revealing why the former can be misleading and how the latter provides a more reliable confidence interval in such challenging scenarios.", "problem": "Consider a self-normalized importance sampling estimate of a ratio of expectations. Let $\\{(Y_i,Z_i)\\}_{i=1}^n$ be independent and identically distributed with $Y_i = h(X_i) w(X_i)$ and $Z_i = w(X_i)$, where $w(X_i)  0$ almost surely and $h$ is a real-valued function. The target parameter is the ratio $\\rho = \\mu_Y / \\mu_Z$, where $\\mu_Y = \\mathbb{E}[Y_i]$ and $\\mu_Z = \\mathbb{E}[Z_i]$. A Monte Carlo run with $n = 400$ draws produced the following summary statistics:\n- Sample means: $\\bar{Y} = 0.6$ and $\\bar{Z} = 0.02$.\n- Sample variances and covariance (computed in the usual unbiased way): $s_{YY} = 1.0$, $s_{ZZ} = 0.09$, $s_{YZ} = 0.02$.\n\nAssume the joint Central Limit Theorem (CLT) applies so that $\\sqrt{n}\\big((\\bar{Y},\\bar{Z}) - (\\mu_Y,\\mu_Z)\\big)$ is approximately bivariate normal with covariance matrix equal to the population covariance matrix, which we estimate by the sample covariance matrix. Using large-sample normal theory at the $95\\%$ level with critical value $z_{0.975} = 1.96$, compare the Fieller confidence interval (CI) and the delta-method CI for $\\rho$ based on these data, and identify the correct statements below. In particular, analyze how the variability of the denominator impacts the reliability of the delta-method CI.\n\nChoose all that apply.\n\nA. The $95\\%$ Fieller CI is approximately $(-\\infty,\\,-63.0] \\cup [12.0,\\,\\infty)$, while the delta-method CI is approximately $[-14.0,\\,74.0]$. In this setting, the delta-method CI can be misleading because the denominator is not significantly different from zero.\n\nB. Both Fieller and delta-method CIs are finite and nearly identical, roughly $[28.0,\\,32.0]$; therefore the delta-method is reliable here.\n\nC. The Fieller CI is the entire real line at $95\\%$ confidence because the associated quadratic discriminant is negative.\n\nD. If all $Y_i$ and $Z_i$ are multiplied by a common positive constant, both the Fieller and delta-method CIs for $\\rho$ are unchanged.", "solution": "The problem requires a comparison of the Fieller and delta-method confidence intervals (CIs) for a ratio of expectations, based on provided sample statistics from a Monte Carlo simulation. We must first validate the problem statement.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- Data source: $n = 400$ independent and identically distributed draws of $(Y_i, Z_i)$.\n- Definitions: $Y_i = h(X_i) w(X_i)$, $Z_i = w(X_i)$, with $w(X_i) > 0$ almost surely.\n- Target parameter: $\\rho = \\mu_Y / \\mu_Z$, where $\\mu_Y = \\mathbb{E}[Y_i]$ and $\\mu_Z = \\mathbb{E}[Z_i]$.\n- Sample statistics:\n    - Sample means: $\\bar{Y} = 0.6$, $\\bar{Z} = 0.02$.\n    - Sample variances and covariance (unbiased): $s_{YY} = 1.0$, $s_{ZZ} = 0.09$, $s_{YZ} = 0.02$.\n- Assumptions:\n    - Joint Central Limit Theorem (CLT) applies: $\\sqrt{n}\\big((\\bar{Y},\\bar{Z}) - (\\mu_Y,\\mu_Z)\\big)$ is approximately bivariate normal.\n    - Population covariance matrix is estimated by the sample covariance matrix.\n- Confidence level: $95\\%$.\n- Critical value: $z_{0.975} = 1.96$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is well-defined within the domain of large-sample statistical theory and Monte Carlo methods.\n- **Scientifically Grounded:** The concepts of self-normalized importance sampling, the delta method, and Fieller's method for ratio estimation are standard and mathematically rigorous topics in statistics.\n- **Well-Posed:** The problem provides a complete set of numerical data ($n$, $\\bar{Y}$, $\\bar{Z}$, $s_{YY}$, $s_{ZZ}$, $s_{YZ}$) and a clearly-defined statistical framework (CLT, $95\\%$ confidence) sufficient to construct the required confidence intervals.\n- **Objective:** The problem statement is composed of objective, quantitative information and established statistical terminology.\n- **Consistency Check:** We verify that the provided sample covariance matrix is valid (positive semi-definite). The sample covariance matrix is \n$$ S = \\begin{pmatrix} s_{YY}  s_{YZ} \\\\ s_{YZ}  s_{ZZ} \\end{pmatrix} = \\begin{pmatrix} 1.0  0.02 \\\\ 0.02  0.09 \\end{pmatrix} $$\nIts determinant is $(1.0)(0.09) - (0.02)^2 = 0.09 - 0.0004 = 0.0896 > 0$. Since the principal minors are positive ($1.0 > 0$ and $0.0896 > 0$), the matrix is positive definite and thus represents a valid sample covariance structure.\n\n**Step 3: Verdict and Action**\nThe problem statement is internally consistent, scientifically sound, and well-posed. We may proceed to the solution.\n\n### Solution Derivation\n\nThe parameter of interest is $\\rho = \\mu_Y / \\mu_Z$, estimated by $\\hat{\\rho} = \\bar{Y} / \\bar{Z}$.\nGiven the data:\n$\\hat{\\rho} = 0.6 / 0.02 = 30$.\n$n = 400$, $z \\equiv z_{0.975} = 1.96$.\nThe sample variances are given as $s_{YY} = s_Y^2 = 1.0$, $s_{ZZ} = s_Z^2 = 0.09$, and the sample covariance is $s_{YZ} = 0.02$.\n\n**1. Delta-Method Confidence Interval**\n\nThe delta method provides an approximation for the variance of a function of random variables. For $\\hat{\\rho} = g(\\bar{Y}, \\bar{Z}) = \\bar{Y} / \\bar{Z}$, the large-sample variance is approximated by:\n$$ \\text{Var}(\\hat{\\rho}) \\approx \\frac{1}{n} (\\nabla g)^T \\Sigma (\\nabla g) \\Big|_{(\\mu_Y, \\mu_Z)} $$\nwhere $\\Sigma$ is the covariance matrix of $(Y_i, Z_i)$. We estimate this variance by plugging in sample estimates for all unknown quantities:\n$$ \\hat{\\text{Var}}(\\hat{\\rho}) = \\frac{1}{n\\bar{Z}^2} (s_{YY} - 2\\hat{\\rho}s_{YZ} + \\hat{\\rho}^2 s_{ZZ}) $$\nSubstituting the given values:\n$$ \\hat{\\text{Var}}(\\hat{\\rho}) = \\frac{1}{400(0.02)^2} (1.0 - 2(30)(0.02) + (30)^2(0.09)) $$\n$$ \\hat{\\text{Var}}(\\hat{\\rho}) = \\frac{1}{400(0.0004)} (1.0 - 1.2 + 900(0.09)) $$\n$$ \\hat{\\text{Var}}(\\hat{\\rho}) = \\frac{1}{0.16} (1.0 - 1.2 + 81) = \\frac{80.8}{0.16} = 505 $$\nThe standard error of the estimate is $SE(\\hat{\\rho}) = \\sqrt{505} \\approx 22.472$.\nThe $95\\%$ delta-method CI is $\\hat{\\rho} \\pm z \\cdot SE(\\hat{\\rho})$:\n$$ CI_{\\text{delta}} = 30 \\pm 1.96 \\times 22.472 $$\n$$ CI_{\\text{delta}} = 30 \\pm 44.045 $$\n$$ CI_{\\text{delta}} \\approx [-14.0, 74.0] $$\n\n**2. Fieller's Confidence Interval**\n\nFieller's method constructs a CI by finding the set of all values of $\\rho_0$ for which the hypothesis $H_0: \\rho = \\rho_0$ is not rejected. This is based on the statistic $\\bar{Y} - \\rho\\bar{Z}$, which is approximately normally distributed with mean $\\mu_Y - \\rho\\mu_Z = 0$ and variance $\\frac{1}{n} (\\sigma_Y^2 - 2\\rho\\sigma_{YZ} + \\rho^2\\sigma_Z^2)$.\nThe $(1-\\alpha)$ CI for $\\rho$ is the set of values solving the inequality:\n$$ \\frac{(\\bar{Y} - \\rho\\bar{Z})^2}{\\frac{1}{n} (s_{YY} - 2\\rho s_{YZ} + \\rho^2 s_{ZZ})} \\le z^2 $$\nRearranging this leads to a quadratic inequality $A\\rho^2 + B\\rho + C \\le 0$, where:\n- $A = n\\bar{Z}^2 - z^2 s_{ZZ}$\n- $B = -2(n\\bar{Y}\\bar{Z} - z^2 s_{YZ})$\n- $C = n\\bar{Y}^2 - z^2 s_{YY}$\n\nWe compute the coefficients using $z^2 = (1.96)^2 = 3.8416$:\n- $A = 400(0.02)^2 - 3.8416(0.09) = 0.16 - 0.345744 = -0.185744$\n- $B = -2(400(0.6)(0.02) - 3.8416(0.02)) = -2(4.8 - 0.076832) = -9.446336$\n- $C = 400(0.6)^2 - 3.8416(1.0) = 144 - 3.8416 = 140.1584$\n\nThe sign of $A$ is critical. A negative $A$ indicates that the sample mean of the denominator, $\\bar{Z}$, is not significantly different from zero. Specifically, the test statistic for $H_0: \\mu_Z = 0$ is $t_Z = \\sqrt{n}\\bar{Z}/s_Z$, and $A$ can be written as $s_{ZZ}(t_Z^2 - z^2)$. Here, $t_Z^2 = 400(0.02)^2/0.09 = 0.16/0.09 \\approx 1.778$, which is less than $z^2 = 3.8416$, confirming $A  0$. This implies the parabola $f(\\rho) = A\\rho^2 + B\\rho + C$ opens downwards.\n\nNext, we find the roots of $A\\rho^2 + B\\rho + C = 0$ by computing the discriminant $D = B^2 - 4AC$:\n$$ D = (-9.446336)^2 - 4(-0.185744)(140.1584) $$\n$$ D = 89.233 + 104.149 = 193.382 $$\nSince $D > 0$, there are two distinct real roots. Because the parabola opens downwards ($A  0$), the inequality $A\\rho^2 + B\\rho + C \\le 0$ is satisfied *outside* the roots.\nThe roots are:\n$$ \\rho = \\frac{-B \\pm \\sqrt{D}}{2A} = \\frac{9.446336 \\pm \\sqrt{193.382}}{2(-0.185744)} = \\frac{9.446336 \\pm 13.906186}{-0.371488} $$\n$$ \\rho_1 = \\frac{9.446336 - 13.906186}{-0.371488} = \\frac{-4.45985}{-0.371488} \\approx 12.005 $$\n$$ \\rho_2 = \\frac{9.446336 + 13.906186}{-0.371488} = \\frac{23.352522}{-0.371488} \\approx -62.862 $$\nThe $95\\%$ Fieller CI is the set $(-\\infty, \\rho_2] \\cup [\\rho_1, \\infty)$, which is approximately $(-\\infty, -62.9] \\cup [12.0, \\infty)$.\n\n### Evaluation of Options\n\n**A. The $95\\%$ Fieller CI is approximately $(-\\infty,\\,-63.0] \\cup [12.0,\\,\\infty)$, while the delta-method CI is approximately $[-14.0,\\,74.0]$. In this setting, the delta-method CI can be misleading because the denominator is not significantly different from zero.**\nOur derived Fieller CI is $(-\\infty, -62.9] \\cup [12.0, \\infty)$, which matches the provided interval. Our derived delta-method CI is $[-14.0, 74.0]$, which also matches. The reason for the disjoint Fieller CI is indeed that the denominator's mean $\\mu_Z$ is not statistically different from zero at the $5\\%$ level, as shown by $A  0$. This is the precise scenario where the linear approximation underlying the delta method fails, leading to a misleadingly compact and finite CI. Thus, this statement is entirely correct.\n**Verdict: Correct.**\n\n**B. Both Fieller and delta-method CIs are finite and nearly identical, roughly $[28.0,\\,32.0]$; therefore the delta-method is reliable here.**\nThis is false. Our calculations show the CIs are drastically different in both form and magnitude. The Fieller CI is not finite, and the delta-method CI is $[-14.0, 74.0]$, which is very wide. The interval $[28.0, 32.0]$ is a narrow interval around the point estimate of $30$ and does not reflect the large uncertainty in the data.\n**Verdict: Incorrect.**\n\n**C. The Fieller CI is the entire real line at $95\\%$ confidence because the associated quadratic discriminant is negative.**\nFor the Fieller CI to be the entire real line, the conditions are $A  0$ and $D \\le 0$. While $A$ is indeed negative, we calculated the discriminant $D \\approx 193.38$, which is positive. A positive discriminant implies the quadratic has real roots, which bound a finite region or its complement. Therefore, the premise (negative discriminant) and the conclusion (entire real line) are both false.\n**Verdict: Incorrect.**\n\n**D. If all $Y_i$ and $Z_i$ are multiplied by a common positive constant, both the Fieller and delta-method CIs for $\\rho$ are unchanged.**\nLet's analyze the effect of a transformation $Y'_i = c Y_i$ and $Z'_i = c Z_i$ for $c > 0$. The parameter $\\rho' = \\mathbb{E}[Y'_i] / \\mathbb{E}[Z'_i] = c\\mu_Y / (c\\mu_Z) = \\rho$ is invariant. The sample statistics transform as: $\\bar{Y}' = c\\bar{Y}$, $\\bar{Z}' = c\\bar{Z}$, $s'_{YY} = c^2 s_{YY}$, $s'_{ZZ} = c^2 s_{ZZ}$, and $s'_{YZ} = c^2 s_{YZ}$.\n- **Delta Method:** The point estimate $\\hat{\\rho}' = \\bar{Y}'/\\bar{Z}' = (c\\bar{Y})/(c\\bar{Z}) = \\hat{\\rho}$ is invariant. The new variance is $\\hat{\\text{Var}}(\\hat{\\rho}') = \\frac{1}{n(\\bar{Z}')^2} (s'_{YY} - 2\\hat{\\rho}'s'_{YZ} + (\\hat{\\rho}')^2 s'_{ZZ}) = \\frac{1}{n(c\\bar{Z})^2} (c^2 s_{YY} - 2\\hat{\\rho}c^2 s_{YZ} + \\hat{\\rho}^2 c^2 s_{ZZ}) = \\frac{c^2}{c^2} \\hat{\\text{Var}}(\\hat{\\rho}) = \\hat{\\text{Var}}(\\hat{\\rho})$. Since both $\\hat{\\rho}$ and its standard error are invariant, the CI is unchanged.\n- **Fieller Method:** The coefficients of the quadratic $A'\\rho^2 + B'\\rho + C' \\le 0$ become $A' = c^2 A$, $B' = c^2 B$, and $C' = c^2 C$. The inequality is $c^2(A\\rho^2 + B\\rho + C) \\le 0$. Since $c^2 > 0$, this is equivalent to the original inequality $A\\rho^2 + B\\rho + C \\le 0$. The solution set for $\\rho$ is therefore identical.\nBoth CIs are invariant to this scaling.\n**Verdict: Correct.**", "answer": "$$\\boxed{AD}$$", "id": "3298323"}]}