{"hands_on_practices": [{"introduction": "Before applying a statistical tool, it is crucial to understand its theoretical underpinnings. This first practice establishes the most common formula for the Effective Sample Size (ESS) from a foundational statistical principle: equating the variance of a self-normalized importance sampling estimator to the variance of an idealized, unweighted estimator. Completing this derivation solidifies the intuition that ESS represents the number of independent samples from the target distribution that would provide equivalent statistical precision [@problem_id:3304977].", "problem": "Consider a target distribution with density $\\pi(x)$ on $\\mathbb{R}^{d}$ and an integrand $h:\\mathbb{R}^{d}\\to\\mathbb{R}$ with finite variance under $\\pi$. Let $x_{1},\\dots,x_{N}$ be independent draws from a proposal density $q(x)$, and define the unnormalized importance weights $w_{i}=\\pi(x_{i})/q(x_{i})0$. The Self-Normalized Importance Sampling (SNIS) estimator of $I=\\mathbb{E}_{\\pi}[h(X)]$ is $\\hat{I}_{\\mathrm{SN}}=\\sum_{i=1}^{N}\\tilde{w}_{i}h(x_{i})$ with normalized weights $\\tilde{w}_{i}=w_{i}/\\sum_{j=1}^{N}w_{j}$. Define the Effective Sample Size (ESS) as the number $N_{\\mathrm{eff}}$ such that the variance of $\\hat{I}_{\\mathrm{SN}}$ is matched to that of an equal-weight average of $N_{\\mathrm{eff}}$ independent target draws, in the following sense: use the variance property of weighted averages of independent and identically distributed random variables to express $\\operatorname{Var}(\\hat{I}_{\\mathrm{SN}})$ in terms of the $\\tilde{w}_{i}$ and equate it to the variance of an equal-weight average of $N_{\\mathrm{eff}}$ independent copies of $h(X)$ under $\\pi$. Derive an explicit expression for $N_{\\mathrm{eff}}$ in terms of the unnormalized weights $\\{w_{i}\\}_{i=1}^{N}$.\n\nNext, assess the sensitivity of $N_{\\mathrm{eff}}$ to scaling of the unnormalized weights: for a constant $c0$, consider the scaled weights $w_{i}\\mapsto c\\,w_{i}$ and compute the derivative $\\frac{\\mathrm{d}}{\\mathrm{d}c}N_{\\mathrm{eff}}(c\\,w_{1},\\dots,c\\,w_{N})$ at an arbitrary $c0$.\n\nExpress your final answer as a two-entry row vector containing, in order: the derived expression for $N_{\\mathrm{eff}}$ in terms of $\\{w_{i}\\}$ and the simplified analytic expression for $\\frac{\\mathrm{d}}{\\mathrm{d}c}N_{\\mathrm{eff}}(c\\,w_{1},\\dots,c\\,w_{N})$. No rounding is required.", "solution": "The problem presents two tasks: first, to derive an expression for the effective sample size ($N_{\\mathrm{eff}}$) for a Self-Normalized Importance Sampling (SNIS) estimator based on a specified variance-matching condition; second, to evaluate the sensitivity of this $N_{\\mathrm{eff}}$ to a global scaling of the unnormalized weights.\n\nWe begin with the first task: the derivation of $N_{\\mathrm{eff}}$.\n\nThe problem defines the effective sample size $N_{\\mathrm{eff}}$ as the number of independent samples from the target distribution $\\pi(x)$ that would yield an estimator with the same variance as the SNIS estimator $\\hat{I}_{\\mathrm{SN}}$.\n\nFirst, let us establish the variance of the benchmark estimator. An ideal Monte Carlo estimator for $I = \\mathbb{E}_{\\pi}[h(X)]$ based on $N_{\\mathrm{eff}}$ independent and identically distributed (i.i.d.) samples $\\{X'_j\\}_{j=1}^{N_{\\mathrm{eff}}}$ drawn directly from the target distribution $\\pi(x)$ would be the simple mean $\\hat{I}_{\\mathrm{MC}} = \\frac{1}{N_{\\mathrm{eff}}} \\sum_{j=1}^{N_{\\mathrm{eff}}} h(X'_j)$. The variance of this estimator is given by:\n$$\n\\operatorname{Var}(\\hat{I}_{\\mathrm{MC}}) = \\operatorname{Var}\\left(\\frac{1}{N_{\\mathrm{eff}}} \\sum_{j=1}^{N_{\\mathrm{eff}}} h(X'_j)\\right) = \\frac{1}{N_{\\mathrm{eff}}^2} \\sum_{j=1}^{N_{\\mathrm{eff}}} \\operatorname{Var}_{\\pi}(h(X)) = \\frac{N_{\\mathrm{eff}}}{N_{\\mathrm{eff}}^2} \\operatorname{Var}_{\\pi}(h(X)) = \\frac{\\sigma_{\\pi}^2}{N_{\\mathrm{eff}}}\n$$\nwhere $\\sigma_{\\pi}^2 = \\operatorname{Var}_{\\pi}(h(X))$ is the variance of the integrand $h(X)$ under the target distribution $\\pi$.\n\nNext, we consider the SNIS estimator, $\\hat{I}_{\\mathrm{SN}} = \\sum_{i=1}^{N}\\tilde{w}_{i}h(x_{i})$, where $x_i$ are i.i.d. draws from the proposal distribution $q(x)$ and $\\tilde{w}_i$ are the normalized importance weights. A rigorous derivation of $\\operatorname{Var}(\\hat{I}_{\\mathrm{SN}})$ is complex because the weights $\\tilde{w}_i$ are themselves random variables and are correlated. However, the problem statement provides a specific instruction: \"use the variance property of weighted averages of independent and identically distributed random variables to express $\\operatorname{Var}(\\hat{I}_{\\mathrm{SN}})$ in terms of the $\\tilde{w}_{i}$\".\n\nThis directs us to adopt a common heuristic model where the estimator's variance is approximated by treating the normalized weights $\\tilde{w}_i$ as fixed, pre-computed constants and pretending the random variables being averaged, let's call them $Z_i$, are i.i.d. draws from the *target* distribution $\\pi$. Under this simplifying model, the variance of the weighted sum $\\sum_{i=1}^{N} \\tilde{w}_i Z_i$, where $Z_i$ are i.i.d. with variance $\\sigma_{\\pi}^2$, is:\n$$\n\\operatorname{Var}\\left(\\sum_{i=1}^{N} \\tilde{w}_i Z_i\\right) = \\sum_{i=1}^{N} \\operatorname{Var}(\\tilde{w}_i Z_i) = \\sum_{i=1}^{N} \\tilde{w}_i^2 \\operatorname{Var}(Z_i) = \\left(\\sum_{i=1}^{N} \\tilde{w}_i^2\\right) \\sigma_{\\pi}^2\n$$\nThis is the expression for the variance of $\\hat{I}_{\\mathrm{SN}}$ that we are instructed to use.\n\nAccording to the problem's definition of $N_{\\mathrm{eff}}$, we must equate the two variance expressions:\n$$\n\\frac{\\sigma_{\\pi}^2}{N_{\\mathrm{eff}}} = \\left(\\sum_{i=1}^{N} \\tilde{w}_i^2\\right) \\sigma_{\\pi}^2\n$$\nAssuming that $h(x)$ is not constant almost everywhere under $\\pi$, we have $\\sigma_{\\pi}^2  0$, and we can divide both sides by $\\sigma_{\\pi}^2$ to solve for $N_{\\mathrm{eff}}$:\n$$\nN_{\\mathrm{eff}} = \\frac{1}{\\sum_{i=1}^{N} \\tilde{w}_i^2}\n$$\nThe problem requires this expression in terms of the unnormalized weights $w_i = \\pi(x_i)/q(x_i)$. The normalized weights are $\\tilde{w}_i = w_i / \\sum_{j=1}^{N} w_j$. Substituting this into our expression for $N_{\\mathrm{eff}}$:\n$$\nN_{\\mathrm{eff}} = \\frac{1}{\\sum_{i=1}^{N} \\left(\\frac{w_i}{\\sum_{j=1}^{N} w_j}\\right)^2} = \\frac{1}{\\frac{\\sum_{i=1}^{N} w_i^2}{\\left(\\sum_{j=1}^{N} w_j\\right)^2}}\n$$\nSimplifying this yields the first part of our answer:\n$$\nN_{\\mathrm{eff}} = \\frac{\\left(\\sum_{i=1}^{N} w_i\\right)^2}{\\sum_{i=1}^{N} w_i^2}\n$$\n\nNow, for the second task, we must assess the sensitivity of $N_{\\mathrm{eff}}$ to a scaling of the unnormalized weights. Let's consider a set of scaled weights $w'_i = c w_i$ for some constant $c  0$. We define $N_{\\mathrm{eff}}(c)$ as the effective sample size computed using these scaled weights:\n$$\nN_{\\mathrm{eff}}(c) = N_{\\mathrm{eff}}(c w_1, \\dots, c w_N) = \\frac{\\left(\\sum_{i=1}^{N} (c w_i)\\right)^2}{\\sum_{i=1}^{N} (c w_i)^2}\n$$\nWe can factor the constant $c$ out of the sums:\n$$\nN_{\\mathrm{eff}}(c) = \\frac{\\left(c \\sum_{i=1}^{N} w_i\\right)^2}{\\sum_{i=1}^{N} c^2 w_i^2} = \\frac{c^2 \\left(\\sum_{i=1}^{N} w_i\\right)^2}{c^2 \\sum_{i=1}^{N} w_i^2}\n$$\nSince $c  0$, we have $c^2 \\neq 0$, so we can cancel the $c^2$ terms in the numerator and denominator:\n$$\nN_{\\mathrm{eff}}(c) = \\frac{\\left(\\sum_{i=1}^{N} w_i\\right)^2}{\\sum_{i=1}^{N} w_i^2}\n$$\nThis expression is identical to the original expression for $N_{\\mathrm{eff}}$ and is independent of the scaling factor $c$. This is expected, as the process of normalizing weights, $\\tilde{w}_i = w_i / \\sum_j w_j$, inherently removes any global scaling factor. The scaled normalized weights are $\\tilde{w}'_i = \\frac{c w_i}{\\sum_j (c w_j)} = \\frac{c w_i}{c \\sum_j w_j} = \\tilde{w}_i$, so the quantity $N_{\\mathrm{eff}} = 1/\\sum_i \\tilde{w}_i^2$ is manifestly invariant to scaling.\n\nSince $N_{\\mathrm{eff}}(c)$ is a constant function with respect to $c$, its derivative with respect to $c$ must be zero for any $c  0$.\n$$\n\\frac{\\mathrm{d}}{\\mathrm{d}c}N_{\\mathrm{eff}}(c\\,w_{1},\\dots,c\\,w_{N}) = \\frac{\\mathrm{d}}{\\mathrm{d}c} \\left( \\frac{\\left(\\sum_{i=1}^{N} w_i\\right)^2}{\\sum_{i=1}^{N} w_i^2} \\right) = 0\n$$\nThis completes the second part of the problem. We assemble the two results into a row vector as requested.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{\\left(\\sum_{i=1}^{N} w_i\\right)^2}{\\sum_{i=1}^{N} w_i^2}  0\n\\end{pmatrix}\n}\n$$", "id": "3304977"}, {"introduction": "A mathematical formula is only as useful as its practical implementation. This exercise bridges the gap between theory and code by addressing the critical issue of numerical stability when computing the ESS, especially from the log-weights that are prevalent in statistical models. You will implement the indispensable \"log-sum-exp\" trick, a cornerstone technique for preventing numerical overflow and underflow in robust statistical software [@problem_id:3304971].", "problem": "You are given a collection of weighted samples with unnormalized weights known only through their natural logarithms. Let there be $n$ samples indexed by $i \\in \\{1,\\dots,n\\}$, with unnormalized positive weights $W_i \\in (0,\\infty)$, and define the log-weights $\\ell_i = \\log W_i \\in \\mathbb{R} \\cup \\{-\\infty\\}$. The weighted estimator is formed with normalized weights $w_i = W_i / \\sum_{j=1}^{n} W_j$, and the Effective Sample Size (ESS) is the quantity that equates the variance of this weighted estimator to that of an unweighted estimator with the same underlying samples but equal weights. Your task is to start from the variance properties of independent random variables and derive the ESS in terms of the weights, and then design a numerically stable algorithm that operates only on $\\ell_i$ to compute the ESS without explicit formation of $W_i$ and without sacrificing numerical accuracy when $\\ell_i$ are very large (positive or negative) or equal to $-\\infty$.\n\nUse the following fundamental base in your derivation:\n- Variance of a linear combination of independent, mean-zero random variables $Z_i$ with common variance $\\sigma^2$ is $\\mathrm{Var}\\left(\\sum_{i=1}^{n} a_i Z_i\\right) = \\sigma^2 \\sum_{i=1}^{n} a_i^2$.\n- For unweighted averaging with $n$ samples, equal weights are $a_i = 1/n$, yielding variance $\\sigma^2 / n$.\n\nUsing these foundations, do all of the following:\n1. Derive, from first principles, a closed-form expression for the Effective Sample Size (ESS) in terms of the weights $W_i$ and their normalized versions $w_i$. Do not assume any shortcut formula; show how ESS arises by equating the variance of the weighted estimator to that of an unweighted estimator.\n2. Starting from the derived expression, show how ESS can be computed using only the log-weights $\\ell_i$ by manipulating sums and products in the logarithmic domain. Your derivation must explicitly exhibit a transformation that avoids direct exponentiation of very large or very small numbers.\n3. Explain the numerical instability that arises if one naively exponentiates $\\ell_i$ to form $W_i$, including cases of overflow when $\\ell_i$ are large and underflow when $\\ell_i$ are very negative. Provide the precise numerically stable transformation that mitigates these issues using the logarithmic domain.\n4. Specify a robust convention for the degenerate case where all $\\ell_i = -\\infty$ (which implies all $W_i = 0$). Under this convention, define the ESS value your program must return.\n\nThen, implement a complete and runnable program that:\n- Consumes no input and uses only the specified test suite below.\n- For each test case, computes the ESS from the given list of log-weights using your numerically stable method that does not form $W_i$ explicitly and does not require normalization of $w_i$ in floating-point arithmetic.\n- Produces as output a single line containing a comma-separated list enclosed in square brackets with the ESS values for all test cases, rounded to $10$ decimal places. The output must be exactly in the format $[r_1,r_2,\\dots,r_m]$, where each $r_k$ is a decimal string with $10$ digits after the decimal point.\n\nNo physical units are involved; no angles are involved. All answers must be expressed as floating-point decimal numbers as specified above.\n\nTest suite (each case is a vector of log-weights):\n- Case 1 (uniform weights): $\\ell = [0.0, 0.0, 0.0, 0.0, 0.0]$.\n- Case 2 (extremely large positive log-weights with hierarchy): $\\ell = [1000.0, 999.0, 990.0, 900.0, 800.0]$.\n- Case 3 (extremely negative but equal log-weights): $\\ell = [-1000.0, -1000.0, -1000.0]$.\n- Case 4 (one dominant weight, others negligible): $\\ell = [0.0, -1000.0, -1000.0, -1000.0]$.\n- Case 5 (mixture of finite and negative infinity): $\\ell = [0.0, -\\infty, -\\infty, -10.0]$.\n- Case 6 (all negative infinity; degenerate): $\\ell = [-\\infty, -\\infty, -\\infty]$.\n- Case 7 (single sample): $\\ell = [123.456]$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[r_1,r_2,\\dots,r_7]$), where each $r_k$ is the ESS for the corresponding case, rounded to $10$ decimal places.", "solution": "The problem of computing the Effective Sample Size (ESS) for a set of weighted samples from their log-weights is a standard task in computational statistics, particularly in the context of importance sampling and sequential Monte Carlo methods. The problem as stated is scientifically grounded, well-posed, objective, and complete. We proceed with the solution by first deriving the required expressions from fundamental principles and then designing a numerically stable algorithm.\n\n### Step 1: Derivation of the Effective Sample Size (ESS)\n\nWe begin from the foundational principles laid out in the problem statement. Consider a set of $n$ independent, identically distributed, mean-zero random variables $Z_i$, each with a common variance $\\mathrm{Var}(Z_i) = \\sigma^2$.\n\nA weighted estimator for the mean of the underlying distribution from which the $Z_i$ are sampled is given by $\\hat{\\mu} = \\sum_{i=1}^{n} w_i Z_i$, where $w_i$ are normalized weights such that $\\sum_{i=1}^{n} w_i = 1$. The variance of this estimator is derived using the provided formula for the variance of a linear combination of independent random variables, $\\mathrm{Var}\\left(\\sum_{i=1}^{n} a_i Z_i\\right) = \\sigma^2 \\sum_{i=1}^{n} a_i^2$. Setting the coefficients $a_i = w_i$, we get:\n$$ \\mathrm{Var}(\\hat{\\mu}) = \\sigma^2 \\sum_{i=1}^{n} w_i^2 $$\n\nFor a standard unweighted estimator with $N$ samples, the weights are uniform, $a_i = 1/N$, and its variance is $\\mathrm{Var}(\\hat{\\mu}_{\\text{unweighted}}) = \\sigma^2 \\sum_{i=1}^{N} (1/N)^2 = \\sigma^2 \\cdot N \\cdot (1/N^2) = \\sigma^2 / N$.\n\nThe Effective Sample Size (ESS) is defined as the number of samples $n_{eff}$ that an unweighted estimator would need to have the same variance as our weighted estimator. We denote $\\mathrm{ESS} = n_{eff}$. Therefore, we equate the two variances:\n$$ \\mathrm{Var}(\\hat{\\mu}) = \\frac{\\sigma^2}{\\mathrm{ESS}} $$\n$$ \\sigma^2 \\sum_{i=1}^{n} w_i^2 = \\frac{\\sigma^2}{\\mathrm{ESS}} $$\nSolving for ESS, we arrive at the first required expression in terms of the normalized weights $w_i$:\n$$ \\mathrm{ESS} = \\frac{1}{\\sum_{i=1}^{n} w_i^2} $$\n\nThe normalized weights $w_i$ are related to the unnormalized positive weights $W_i$ by the relation $w_i = W_i / S_W$, where $S_W = \\sum_{j=1}^{n} W_j$ is the sum of unnormalized weights. Substituting this into the denominator of the ESS expression gives:\n$$ \\sum_{i=1}^{n} w_i^2 = \\sum_{i=1}^{n} \\left( \\frac{W_i}{\\sum_{j=1}^{n} W_j} \\right)^2 = \\frac{\\sum_{i=1}^{n} W_i^2}{\\left(\\sum_{j=1}^{n} W_j\\right)^2} $$\nSubstituting this back into the expression for ESS yields the closed-form expression in terms of the unnormalized weights $W_i$:\n$$ \\mathrm{ESS} = \\frac{\\left(\\sum_{j=1}^{n} W_j\\right)^2}{\\sum_{i=1}^{n} W_i^2} $$\nThis completes the first part of the derivation.\n\n### Step 2  3: Numerically Stable Computation from Log-Weights\n\nWe are given the natural logarithms of the unnormalized weights, $\\ell_i = \\log W_i$, which implies $W_i = e^{\\ell_i}$. Substituting this into the derived ESS formula gives:\n$$ \\mathrm{ESS} = \\frac{\\left(\\sum_{i=1}^{n} e^{\\ell_i}\\right)^2}{\\sum_{i=1}^{n} e^{2\\ell_i}} $$\n\n**Numerical Instability:**\nA naive implementation would first compute the weights $W_i = e^{\\ell_i}$ and then use the formula. This approach is numerically unstable.\n1.  **Overflow:** If any $\\ell_i$ is large and positive (e.g., $\\ell_i  709.78$ for IEEE 754 double precision), the computation of $e^{\\ell_i}$ will result in overflow (infinity). This contaminates the sums, leading to an incorrect or undefined result like $\\infty/\\infty$.\n2.  **Underflow:** If any $\\ell_i$ is large and negative (e.g., $\\ell_i  -745$), $e^{\\ell_i}$ will underflow to $0$. While this may be acceptable for a single weight, if all weights are small, their sum might also underflow to $0$, leading to a division by zero. Furthermore, this leads to a catastrophic loss of relative precision if the weights have a large dynamic range.\n\n**Numerically Stable Transformation:**\nTo mitigate these issues, we employ a standard technique often called the \"log-sum-exp\" trick. Let $\\ell_{\\max} = \\max_{i \\in \\{1,\\dots,n\\}} \\ell_i$. We can factor out $e^{\\ell_{\\max}}$ from the numerator sum and $e^{2\\ell_{\\max}}$ from the denominator sum.\n\nLet $S_1 = \\sum_{i=1}^{n} W_i = \\sum_{i=1}^{n} e^{\\ell_i}$ and $S_2 = \\sum_{i=1}^{n} W_i^2 = \\sum_{i=1}^{n} e^{2\\ell_i}$.\nWe rewrite $S_1$ as:\n$$ S_1 = \\sum_{i=1}^{n} e^{\\ell_i - \\ell_{\\max}} e^{\\ell_{\\max}} = e^{\\ell_{\\max}} \\sum_{i=1}^{n} e^{\\ell_i - \\ell_{\\max}} $$\nAnd we rewrite $S_2$ as:\n$$ S_2 = \\sum_{i=1}^{n} e^{2\\ell_i - 2\\ell_{\\max}} e^{2\\ell_{\\max}} = e^{2\\ell_{\\max}} \\sum_{i=1}^{n} e^{2(\\ell_i - \\ell_{\\max})} $$\n\nNow, substituting these factored forms back into the ESS expression $\\mathrm{ESS} = S_1^2 / S_2$:\n$$ \\mathrm{ESS} = \\frac{\\left(e^{\\ell_{\\max}} \\sum_{i=1}^{n} e^{\\ell_i - \\ell_{\\max}}\\right)^2}{e^{2\\ell_{\\max}} \\sum_{i=1}^{n} e^{2(\\ell_i - \\ell_{\\max})}} = \\frac{e^{2\\ell_{\\max}} \\left(\\sum_{i=1}^{n} e^{\\ell_i - \\ell_{\\max}}\\right)^2}{e^{2\\ell_{\\max}} \\sum_{i=1}^{n} e^{2(\\ell_i - \\ell_{\\max})}} $$\nThe term $e^{2\\ell_{\\max}}$ cancels out algebraically, yielding the final, numerically stable formula:\n$$ \\mathrm{ESS} = \\frac{\\left(\\sum_{i=1}^{n} e^{\\ell_i - \\ell_{\\max}}\\right)^2}{\\sum_{i=1}^{n} e^{2(\\ell_i - \\ell_{\\max})}} $$\nThis expression is stable because the arguments of the exponentiation, $\\ell_i - \\ell_{\\max}$, are always less than or equal to $0$. This prevents overflow. The largest term in the sum is $e^0 = 1$, which preserves numerical precision for other terms that would have otherwise underflowed relative to a very large number. This transformation allows us to compute the ESS using only operations on the log-weights, without ever forming the potentially problematic $W_i$ or normalized $w_i$ values directly.\n\n### Step 4: Convention for the Degenerate Case\n\nThe degenerate case occurs when all log-weights are negative infinity, i.e., $\\ell_i = -\\infty$ for all $i \\in \\{1,\\dots,n\\}$. This corresponds to all unnormalized weights being zero, $W_i = 0$.\nIn this scenario, the sum of weights $\\sum W_i = 0$ and the sum of squared weights $\\sum W_i^2 = 0$. The ESS formula becomes $\\mathrm{ESS} = 0^2 / 0$, which is an indeterminate form.\n\nFrom a statistical perspective, a set of all-zero weights provides no information, as no sample contributes to the estimator. It is equivalent to having zero effective samples. Therefore, the most sensible and robust convention is to define the ESS as $0$ for this case.\n\nIn our numerically stable algorithm, this case corresponds to $\\ell_{\\max} = -\\infty$. We must explicitly check for this condition before proceeding with the main computation to avoid the indeterminate form $\\ell_i - \\ell_{\\max} = -\\infty - (-\\infty)$. If $\\ell_{\\max} = -\\infty$, the algorithm should return $0.0$. For cases where only some, but not all, $\\ell_i$ are $-\\infty$, the stable formula handles them correctly: if $\\ell_{\\max}$ is finite, then for an $\\ell_i = -\\infty$, the term $\\ell_i - \\ell_{\\max} = -\\infty$, and $e^{-\\infty}$ evaluates to $0$, correctly excluding that sample from the calculation.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the effective sample size problem for a predefined test suite.\n    \"\"\"\n\n    def compute_ess(log_weights: np.ndarray) - float:\n        \"\"\"\n        Computes the Effective Sample Size (ESS) from a vector of log-weights\n        using a numerically stable algorithm.\n\n        Args:\n            log_weights: A 1D numpy array of unnormalized log-weights.\n\n        Returns:\n            The calculated ESS as a float.\n        \"\"\"\n        # Ensure input is a numpy array for vectorized operations\n        log_weights = np.asarray(log_weights, dtype=np.float64)\n\n        # Handle the trivial case of a single sample\n        if log_weights.size == 1:\n            return 1.0\n        \n        # Handle the case of an empty set of weights\n        if log_weights.size == 0:\n            return 0.0\n\n        # Find the maximum log-weight\n        max_log_weight = np.max(log_weights)\n\n        # Handle the degenerate case where all log-weights are -inf\n        # This implies all weights are 0, so ESS is 0.\n        if max_log_weight == -np.inf:\n            return 0.0\n\n        # Shift log-weights to prevent overflow/underflow\n        # This is the core of the log-sum-exp trick\n        shifted_log_weights = log_weights - max_log_weight\n        \n        # Calculate the terms for the sums in the stable ESS formula.\n        # Numerator sum's terms: exp(ell_i - ell_max)\n        # Denominator sum's terms: exp(2 * (ell_i - ell_max))\n        # This avoids direct calculation of W_i = exp(ell_i)\n        \n        # Sum of exp(l_i - l_max)\n        s1 = np.sum(np.exp(shifted_log_weights))\n        \n        # Sum of exp(2 * (l_i - l_max)) = sum of (exp(l_i - l_max))^2\n        s2 = np.sum(np.exp(2 * shifted_log_weights))\n        \n        # The terms exp(2 * l_max) from numerator and denominator cancel out\n        # ESS = (sum(W_i))^2 / sum(W_i^2)\n        #     = (exp(l_max) * sum(exp(l_i-l_max)))^2 / (exp(2*l_max) * sum(exp(2*(l_i-l_max))))\n        #     = (sum(exp(l_i-l_max)))^2 / sum(exp(2*(l_i-l_max)))\n        \n        # The denominator s2 cannot be zero if there is at least one finite log-weight,\n        # because the maximum shifted log-weight is 0, making its term exp(0)=1.\n        ess = s1**2 / s2\n        \n        return ess\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        np.array([0.0, 0.0, 0.0, 0.0, 0.0]),\n        np.array([1000.0, 999.0, 990.0, 900.0, 800.0]),\n        np.array([-1000.0, -1000.0, -1000.0]),\n        np.array([0.0, -1000.0, -1000.0, -1000.0]),\n        np.array([0.0, -np.inf, -np.inf, -10.0]),\n        np.array([-np.inf, -np.inf, -np.inf]),\n        np.array([123.456])\n    ]\n\n    results = []\n    for case in test_cases:\n        result = compute_ess(case)\n        # Format to 10 decimal places as specified\n        results.append(f\"{result:.10f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3304971"}, {"introduction": "The Effective Sample Size is not merely a post-hoc diagnostic; it is a powerful tool for algorithmic design. This advanced practice places you in the role of designing a pseudo-marginal MCMC algorithm, a powerful but delicate inference method. Here, you will use the principles of ESS to determine the optimal number of particles needed to balance computational effort against the statistical efficiency of the sampler, a fundamental trade-off in modern Monte Carlo methods [@problem_id:3304999].", "problem": "Consider a pseudo-marginal Markov chain Monte Carlo (MCMC) algorithm in which, at each iteration, a proposed parameter value $X$ is evaluated with a noisy importance-sampling estimate of its likelihood. Specifically, given $X$, draw $M$ independent auxiliary variables $U^{(1)},\\dots,U^{(M)}$ and form weights $w^{(m)} \\propto L(X,U^{(m)})$, with the unbiased likelihood estimator defined by $\\hat{L}(X) = \\frac{1}{M} \\sum_{m=1}^{M} w^{(m)}$. Suppose that for any fixed $X$, the weights follow a log-normal model $w^{(m)} = c(X)\\,\\exp(\\epsilon^{(m)})$ with $\\epsilon^{(m)} \\sim \\mathcal{N}(0,\\tau^{2})$, independently across $m$ and iterations, and $c(X)0$ is a deterministic scale factor depending on $X$.\n\nPer iteration, the internal importance weights admit an effective sample size (ESS), defined by\n$$\nN_{\\mathrm{eff}} = \\frac{\\left(\\sum_{m=1}^{M} w^{(m)}\\right)^{2}}{\\sum_{m=1}^{M} \\left(w^{(m)}\\right)^{2}}.\n$$\nYour aims are:\n1. Achieve a target expected effective sample size $E\\!\\left[N_{\\mathrm{eff}}\\right] \\geq N_{\\mathrm{eff}}^{\\star}$ per iteration.\n2. Balance against acceptance rate by enforcing the design constraint that the variance of the log of the likelihood estimator satisfies $\\operatorname{Var}\\!\\left(\\ln \\hat{L}(X)\\right) \\leq \\sigma_{\\max}^{2}$.\n\nStarting from well-tested definitions and facts about moments of log-normal random variables and asymptotic approximations for sample sums, derive closed-form constraints on $M$ implied by these two aims. Then, given $\\tau^{2} = 1.1$, $N_{\\mathrm{eff}}^{\\star} = 1000$, and $\\sigma_{\\max}^{2} = 0.3$, compute the smallest integer $M$ that simultaneously satisfies both constraints. Express the final answer as a single integer. No rounding by significant figures is required; report the exact minimal integer.", "solution": "We begin from the definitions of effective sample size for weighted samples and the behavior of sums of independent and identically distributed random variables.\n\nLet $w^{(m)} = c(X)\\,\\exp(\\epsilon^{(m)})$ with $\\epsilon^{(m)} \\sim \\mathcal{N}(0,\\tau^{2})$. Denote $Y^{(m)} = \\exp(\\epsilon^{(m)})$ so that $w^{(m)} = c(X) Y^{(m)}$ and $Y^{(m)}$ are independent and identically distributed log-normal random variables. The moments of $Y^{(m)}$ are\n$$\nE\\!\\left[Y^{(m)}\\right] = \\exp\\!\\left(\\frac{\\tau^{2}}{2}\\right), \\quad E\\!\\left[\\left(Y^{(m)}\\right)^{2}\\right] = \\exp\\!\\left(2\\tau^{2}\\right), \\quad \\operatorname{Var}\\!\\left(Y^{(m)}\\right) = \\exp\\!\\left(\\tau^{2}\\right)\\left(\\exp\\!\\left(\\tau^{2}\\right) - 1\\right).\n$$\nSince $w^{(m)} = c(X) Y^{(m)}$, we have\n$$\nE\\!\\left[w^{(m)}\\right] = c(X) \\exp\\!\\left(\\frac{\\tau^{2}}{2}\\right), \\quad E\\!\\left[\\left(w^{(m)}\\right)^{2}\\right] = c(X)^{2} \\exp\\!\\left(2\\tau^{2}\\right).\n$$\n\nEffective sample size constraint. By the law of large numbers, for large $M$,\n$$\n\\sum_{m=1}^{M} w^{(m)} \\approx M\\,E\\!\\left[w^{(m)}\\right], \\quad \\sum_{m=1}^{M} \\left(w^{(m)}\\right)^{2} \\approx M\\,E\\!\\left[\\left(w^{(m)}\\right)^{2}\\right].\n$$\nTherefore,\n$$\nE\\!\\left[N_{\\mathrm{eff}}\\right] \\approx \\frac{\\left(M\\,E[w^{(m)}]\\right)^{2}}{M\\,E[(w^{(m)})^{2}]} = M\\,\\frac{\\left(E[w^{(m)}]\\right)^{2}}{E[(w^{(m)})^{2}]}.\n$$\nSubstituting the moments of $w^{(m)}$ yields\n$$\n\\frac{\\left(E[w^{(m)}]\\right)^{2}}{E[(w^{(m)})^{2}]} = \\frac{\\left(c(X)\\,\\exp(\\tau^{2}/2)\\right)^{2}}{c(X)^{2}\\,\\exp(2\\tau^{2})} = \\exp(-\\tau^{2}),\n$$\nso\n$$\nE\\!\\left[N_{\\mathrm{eff}}\\right] \\approx M\\,\\exp(-\\tau^{2}).\n$$\nTo achieve $E\\!\\left[N_{\\mathrm{eff}}\\right] \\geq N_{\\mathrm{eff}}^{\\star}$, it suffices to enforce\n$$\nM \\geq N_{\\mathrm{eff}}^{\\star}\\,\\exp(\\tau^{2}).\n$$\n\nAcceptance balance via variance of $\\ln \\hat{L}(X)$. The estimator is\n$$\n\\hat{L}(X) = \\frac{1}{M}\\sum_{m=1}^{M} w^{(m)} = c(X)\\,\\bar{Y}, \\quad \\text{where } \\bar{Y} = \\frac{1}{M}\\sum_{m=1}^{M} Y^{(m)}.\n$$\nWe approximate $\\operatorname{Var}\\!\\left(\\ln \\hat{L}(X)\\right)$ using the delta method. First compute $\\operatorname{Var}(\\bar{Y})$:\n$$\nE[\\bar{Y}] = E[Y^{(1)}] = \\exp\\!\\left(\\frac{\\tau^{2}}{2}\\right), \\quad \\operatorname{Var}(\\bar{Y}) = \\frac{1}{M}\\operatorname{Var}(Y^{(1)}) = \\frac{1}{M}\\exp(\\tau^{2})\\left(\\exp(\\tau^{2}) - 1\\right).\n$$\nLet $g(y) = \\ln y$. The delta method gives, for large $M$,\n$$\n\\operatorname{Var}\\!\\left(\\ln \\bar{Y}\\right) \\approx \\left(g'(E[\\bar{Y}])\\right)^{2}\\operatorname{Var}(\\bar{Y}) = \\left(\\frac{1}{E[\\bar{Y}]}\\right)^{2} \\operatorname{Var}(\\bar{Y}) = \\frac{\\operatorname{Var}(\\bar{Y})}{\\left(E[\\bar{Y}]\\right)^{2}}.\n$$\nSubstituting the moments,\n$$\n\\operatorname{Var}\\!\\left(\\ln \\bar{Y}\\right) \\approx \\frac{\\frac{1}{M}\\exp(\\tau^{2})\\left(\\exp(\\tau^{2}) - 1\\right)}{\\exp(\\tau^{2})} = \\frac{\\exp(\\tau^{2}) - 1}{M}.\n$$\nBecause $\\hat{L}(X) = c(X)\\,\\bar{Y}$ and $c(X)$ is deterministic, $\\ln \\hat{L}(X) = \\ln c(X) + \\ln \\bar{Y}$, and hence\n$$\n\\operatorname{Var}\\!\\left(\\ln \\hat{L}(X)\\right) = \\operatorname{Var}\\!\\left(\\ln \\bar{Y}\\right) \\approx \\frac{\\exp(\\tau^{2}) - 1}{M}.\n$$\nImposing $\\operatorname{Var}\\!\\left(\\ln \\hat{L}(X)\\right) \\leq \\sigma_{\\max}^{2}$ yields\n$$\n\\frac{\\exp(\\tau^{2}) - 1}{M} \\leq \\sigma_{\\max}^{2} \\quad \\Longrightarrow \\quad M \\geq \\frac{\\exp(\\tau^{2}) - 1}{\\sigma_{\\max}^{2}}.\n$$\n\nCombining constraints. The smallest integer $M$ that satisfies both aims is\n$$\nM = \\left\\lceil \\max\\left\\{N_{\\mathrm{eff}}^{\\star}\\,\\exp(\\tau^{2}), \\, \\frac{\\exp(\\tau^{2}) - 1}{\\sigma_{\\max}^{2}}\\right\\} \\right\\rceil.\n$$\n\nNumerical evaluation. With $\\tau^{2} = 1.1$, $N_{\\mathrm{eff}}^{\\star} = 1000$, and $\\sigma_{\\max}^{2} = 0.3$:\n$$\n\\exp(1.1) \\approx 3.004166,\n$$\nso\n$$\nN_{\\mathrm{eff}}^{\\star}\\,\\exp(\\tau^{2}) \\approx 1000 \\times 3.004166 = 3004.166,\n$$\nand\n$$\n\\frac{\\exp(\\tau^{2}) - 1}{\\sigma_{\\max}^{2}} \\approx \\frac{3.004166 - 1}{0.3} = \\frac{2.004166}{0.3} \\approx 6.680553.\n$$\nTaking the maximum and the smallest integer greater than or equal to it,\n$$\nM = \\left\\lceil \\max\\{3004.166,\\,6.680553\\} \\right\\rceil = \\left\\lceil 3004.166 \\right\\rceil = 3005.\n$$\nThus, $M = 3005$ is the minimal choice that achieves the target expected effective sample size while keeping the variance of the log-likelihood estimator within the specified bound to preserve acceptance behavior.", "answer": "$$\\boxed{3005}$$", "id": "3304999"}]}