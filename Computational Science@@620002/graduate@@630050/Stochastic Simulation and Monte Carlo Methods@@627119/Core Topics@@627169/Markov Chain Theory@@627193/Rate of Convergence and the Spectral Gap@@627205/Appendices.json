{"hands_on_practices": [{"introduction": "Understanding the spectral gap begins with its definition in terms of the eigenvalues of the transition matrix. This first practice provides a direct, hands-on calculation for a simple, finite-state Markov chain. By explicitly computing the eigenvalues of the transition matrix, you will solidify your understanding of how the spectral gap is determined and what it represents for a reversible chain [@problem_id:3335415].", "problem": "Consider a finite-state, time-homogeneous Markov chain on the state space $\\{1,2,3,4\\}$ with transition matrix $P \\in \\mathbb{R}^{4 \\times 4}$ given by\n$$\nP \\;=\\; \\begin{pmatrix}\n\\frac{7}{10}  \\frac{1}{10}  \\frac{1}{10}  \\frac{1}{10} \\\\\n\\frac{1}{10}  \\frac{7}{10}  \\frac{1}{10}  \\frac{1}{10} \\\\\n\\frac{1}{10}  \\frac{1}{10}  \\frac{7}{10}  \\frac{1}{10} \\\\\n\\frac{1}{10}  \\frac{1}{10}  \\frac{1}{10}  \\frac{7}{10}\n\\end{pmatrix}.\n$$\nLet the probability vector $\\pi \\in \\mathbb{R}^{4}$ be given by $\\pi_i = \\frac{1}{4}$ for each $i \\in \\{1,2,3,4\\}$. In the sense of the $L^2(\\pi)$ operator induced by $P$, the spectral gap is defined as the difference between the largest eigenvalue and the next-largest eigenvalue of $P$ (for a reversible chain this equals $1 - \\lambda_2$ when the eigenvalues are ordered $1 = \\lambda_1 \\ge \\lambda_2 \\ge \\cdots$).\n\nStarting only from the core definitions of a stationary distribution, reversibility, and the spectral decomposition of a linear operator, do the following:\n1. Verify that $\\pi$ is a stationary distribution of $P$.\n2. Justify that the chain is reversible with respect to $\\pi$.\n3. Compute all eigenvalues of $P$ and order them in nonincreasing order to identify $\\lambda_2$.\n4. Using the definition above, compute the spectral gap $\\gamma$ of $P$.\n\nExpress the final answer for the spectral gap $\\gamma$ as an exact rational number.", "solution": "The problem requires a four-part analysis of a given Markov chain: verification of the stationary distribution, justification of reversibility, computation of the eigenvalues of the transition matrix, and calculation of the spectral gap. We will address each part in sequence, starting from the fundamental definitions as specified.\n\nThe state space is $S = \\{1, 2, 3, 4\\}$. The transition matrix is given by:\n$$\nP \\;=\\; \\begin{pmatrix}\n\\frac{7}{10}  \\frac{1}{10}  \\frac{1}{10}  \\frac{1}{10} \\\\\n\\frac{1}{10}  \\frac{7}{10}  \\frac{1}{10}  \\frac{1}{10} \\\\\n\\frac{1}{10}  \\frac{1}{10}  \\frac{7}{10}  \\frac{1}{10} \\\\\n\\frac{1}{10}  \\frac{1}{10}  \\frac{1}{10}  \\frac{7}{10}\n\\end{pmatrix}\n$$\nThe proposed stationary distribution is the uniform distribution $\\pi = \\begin{pmatrix} \\frac{1}{4}  \\frac{1}{4}  \\frac{1}{4}  \\frac{1}{4} \\end{pmatrix}$.\n\n**1. Verification of the Stationary Distribution**\n\nA probability distribution $\\pi$ is a stationary distribution of a Markov chain with transition matrix $P$ if it is a left eigenvector of $P$ with eigenvalue $1$. The core definition is the condition $\\pi P = \\pi$. We compute the product of the row vector $\\pi$ and the matrix $P$:\n$$\n\\pi P = \\begin{pmatrix} \\frac{1}{4}  \\frac{1}{4}  \\frac{1}{4}  \\frac{1}{4} \\end{pmatrix} \\begin{pmatrix}\n\\frac{7}{10}  \\frac{1}{10}  \\frac{1}{10}  \\frac{1}{10} \\\\\n\\frac{1}{10}  \\frac{7}{10}  \\frac{1}{10}  \\frac{1}{10} \\\\\n\\frac{1}{10}  \\frac{1}{10}  \\frac{7}{10}  \\frac{1}{10} \\\\\n\\frac{1}{10}  \\frac{1}{10}  \\frac{1}{10}  \\frac{7}{10}\n\\end{pmatrix}\n$$\nThe $j$-th component of the resulting vector is given by $(\\pi P)_j = \\sum_{i=1}^{4} \\pi_i P_{ij}$. Since $\\pi_i = \\frac{1}{4}$ for all $i \\in \\{1, 2, 3, 4\\}$, this simplifies to:\n$$\n(\\pi P)_j = \\frac{1}{4} \\sum_{i=1}^{4} P_{ij}\n$$\nThis is $\\frac{1}{4}$ times the sum of the elements in the $j$-th column of $P$.\nFor $j=1$: $\\sum_{i=1}^{4} P_{i1} = \\frac{7}{10} + \\frac{1}{10} + \\frac{1}{10} + \\frac{1}{10} = \\frac{10}{10} = 1$. So, $(\\pi P)_1 = \\frac{1}{4} \\cdot 1 = \\frac{1}{4}$.\nDue to the symmetry of the matrix $P$, all column sums are identical. For any $j \\in \\{1, 2, 3, 4\\}$:\n$$\n\\sum_{i=1}^{4} P_{ij} = \\frac{7}{10} + 3 \\times \\frac{1}{10} = 1\n$$\nThus, for all $j$, we have $(\\pi P)_j = \\frac{1}{4} \\cdot 1 = \\frac{1}{4} = \\pi_j$.\nThis confirms that $\\pi P = \\pi$, and therefore $\\pi$ is a stationary distribution of the Markov chain.\n\n**2. Justification of Reversibility**\n\nA Markov chain is reversible with respect to a stationary distribution $\\pi$ if the detailed balance condition is satisfied. The definition of detailed balance is:\n$$\n\\pi_i P_{ij} = \\pi_j P_{ji} \\quad \\text{for all } i, j \\in S\n$$\nIn this problem, the stationary distribution is uniform, meaning $\\pi_i = \\frac{1}{4}$ for all $i$. Substituting this into the detailed balance equation gives:\n$$\n\\frac{1}{4} P_{ij} = \\frac{1}{4} P_{ji}\n$$\nThis equation simplifies to $P_{ij} = P_{ji}$. This condition requires the transition matrix $P$ to be symmetric. Observing the given matrix $P$, we see that it is indeed symmetric ($P = P^T$). For any pair of indices $(i, j)$, the entry $P_{ij}$ is equal to the entry $P_{ji}$. Therefore, the detailed balance condition holds, and the Markov chain is reversible with respect to $\\pi$.\n\n**3. Computation of Eigenvalues**\n\nThe eigenvalues $\\lambda$ of the matrix $P$ are the roots of the characteristic equation $\\det(P - \\lambda I) = 0$, where $I$ is the $4 \\times 4$ identity matrix. The matrix $P$ has a specific structure which simplifies this calculation. It can be expressed as a linear combination of the identity matrix $I$ and the matrix of all ones, $J$.\n$$\nP = \\frac{6}{10} I + \\frac{1}{10} J = \\frac{3}{5} I + \\frac{1}{10} J\n$$\nwhere $I$ is the identity matrix and $J$ is the $4 \\times 4$ matrix with all entries equal to $1$. The eigenvalues of an $n \\times n$ matrix of the form $aI + bJ$ are known. The matrix $J$ has two distinct eigenvalues: $n$ with multiplicity $1$ (eigenvector is a vector of all ones) and $0$ with multiplicity $n-1$ (the eigenspace is the set of vectors whose components sum to zero).\nFor an eigenvector $v$ of $J$ with eigenvalue $\\lambda_J$, we have:\n$$\n(aI + bJ)v = aIv + bJv = av + b\\lambda_J v = (a + b\\lambda_J)v\n$$\nSo, the eigenvalues of $aI + bJ$ are $a + b\\lambda_J$.\nIn our case, $n=4$, $a = \\frac{6}{10}$, and $b = \\frac{1}{10}$. The eigenvalues of $J$ are $4$ (multiplicity $1$) and $0$ (multiplicity $3$).\nThe eigenvalues of $P$ are therefore:\n- One eigenvalue corresponding to $\\lambda_J=4$:\n$$\n\\lambda_1 = a + b \\cdot 4 = \\frac{6}{10} + \\frac{1}{10} \\cdot 4 = \\frac{6+4}{10} = \\frac{10}{10} = 1\n$$\n- Three eigenvalues corresponding to $\\lambda_J=0$:\n$$\n\\lambda_{2,3,4} = a + b \\cdot 0 = \\frac{6}{10} + 0 = \\frac{6}{10}\n$$\nThe eigenvalues of $P$ are $\\{1, \\frac{6}{10}, \\frac{6}{10}, \\frac{6}{10}\\}$.\nOrdering them in nonincreasing order as $\\lambda_1 \\ge \\lambda_2 \\ge \\lambda_3 \\ge \\lambda_4$:\n$\\lambda_1 = 1$\n$\\lambda_2 = \\frac{6}{10} = \\frac{3}{5}$\n$\\lambda_3 = \\frac{6}{10} = \\frac{3}{5}$\n$\\lambda_4 = \\frac{6}{10} = \\frac{3}{5}$\nThe second-largest eigenvalue is $\\lambda_2 = \\frac{3}{5}$.\n\n**4. Computation of the Spectral Gap**\n\nThe problem defines the spectral gap $\\gamma$ as the difference between the largest and the next-largest eigenvalue of $P$. This corresponds to $\\gamma = \\lambda_1 - \\lambda_2$.\nUsing the eigenvalues computed above:\n$\\lambda_1 = 1$\n$\\lambda_2 = \\frac{3}{5}$\nThe spectral gap $\\gamma$ is:\n$$\n\\gamma = 1 - \\frac{3}{5} = \\frac{5}{5} - \\frac{3}{5} = \\frac{2}{5}\n$$\nThe spectral gap is an exact rational number, as required.", "answer": "$$\n\\boxed{\\frac{2}{5}}\n$$", "id": "3335415"}, {"introduction": "We now move from finite state spaces to a continuous setting by analyzing one of the most fundamental MCMC algorithms: the Gibbs sampler. This exercise will guide you through the process of deriving the spectral gap for a Gibbs sampler on a bivariate normal target, revealing a direct and elegant relationship between the target's correlation $\\rho$ and the sampler's convergence rate. This practice is invaluable for understanding how the geometry of the target distribution can impede sampler performance, a crucial concept in practical MCMC applications [@problem_id:3335418].", "problem": "Consider a target distribution that is a centered bivariate normal with covariance matrix $\\Sigma$,\n$$\n\\Sigma \\;=\\; \\begin{pmatrix}\n\\sigma_{x}^{2}  \\rho\\,\\sigma_{x}\\sigma_{y} \\\\\n\\rho\\,\\sigma_{x}\\sigma_{y}  \\sigma_{y}^{2}\n\\end{pmatrix},\n$$\nwhere $\\sigma_{x}  0$, $\\sigma_{y}  0$, and $|\\rho|  1$. Let $(X,Y) \\sim \\mathcal{N}\\!\\left((0,0)^{\\top}, \\Sigma\\right)$ be the target law. Consider the two-coordinate systematic Gibbs sampler for Markov Chain Monte Carlo (MCMC) that, at each iteration, first updates $X$ given $Y$ from its full conditional distribution and then updates $Y$ given the newly sampled $X$ from its full conditional distribution. Let $K$ denote the one-step Markov operator acting on square-integrable functions with respect to the target, defined by\n$$\n(K f)(x,y) \\;=\\; \\mathbb{E}\\!\\left[\\, f\\!\\left(X',Y'\\right) \\,\\middle|\\, (X,Y)=(x,y) \\right],\n$$\nwhere $(X',Y')$ is the next state produced by one full Gibbs sweep. Let $L_{2}(\\pi)$ denote the Hilbert space of square-integrable functions with respect to the target density $\\pi$, and let $L_{2}^{0}(\\pi)$ be the subspace of mean-zero functions. The $L_{2}$ spectral gap is defined as\n$$\n\\gamma \\;:=\\; 1 - \\|K\\|_{L_{2}^{0}(\\pi)} \\;=\\; 1 \\;-\\; \\sup_{f \\in L_{2}^{0}(\\pi),\\, f \\neq 0} \\frac{\\|K f\\|_{2}}{\\|f\\|_{2}}.\n$$\n\nStarting from first principles, namely the full conditional distributions of the multivariate normal and the definition of the Markov operator and $L_{2}$ spectral gap, derive the induced autoregressive representation for the $Y$-marginal under one full Gibbs sweep, use it to characterize the action of $K$ on $L_{2}^{0}(\\pi)$, and compute the spectral gap $\\gamma$ in closed form as a function of $\\rho$. Express your final answer as a closed-form analytic expression in $\\rho$. No rounding is required.", "solution": "This problem asks for the spectral gap of a systematic Gibbs sampler targeting a bivariate normal distribution. The derivation will proceed by first finding the full conditional distributions, then analyzing the effect of one full Gibbs sweep on the state variables, which will allow us to characterize the Markov operator $K$ and its norm on the space of mean-zero functions.\n\n**1. Full Conditional Distributions**\n\nFor a multivariate normal distribution, the conditional distributions are also normal. Given the state vector $(X,Y)^{\\top}$ with mean $(0,0)^{\\top}$ and covariance matrix $\\Sigma$, the conditional distribution of $X$ given $Y=y$ is $\\mathcal{N}(\\mu_{X|Y}, \\sigma_{X|Y}^2)$, where:\n- Conditional mean: $\\mu_{X|Y} = \\mu_X + \\Sigma_{XY} \\Sigma_{YY}^{-1} (y - \\mu_Y) = 0 + (\\rho\\sigma_x\\sigma_y)(\\sigma_y^2)^{-1}(y) = \\rho\\frac{\\sigma_x}{\\sigma_y} y$.\n- Conditional variance: $\\sigma_{X|Y}^2 = \\Sigma_{XX} - \\Sigma_{XY} \\Sigma_{YY}^{-1} \\Sigma_{YX} = \\sigma_x^2 - (\\rho\\sigma_x\\sigma_y)(\\sigma_y^2)^{-1}(\\rho\\sigma_x\\sigma_y) = \\sigma_x^2(1-\\rho^2)$.\n\nSo, the full conditional distribution for $X$ is $p(x|y) = \\mathcal{N}\\left(\\rho\\frac{\\sigma_x}{\\sigma_y} y, \\sigma_x^2(1-\\rho^2)\\right)$.\nBy symmetry, the full conditional for $Y$ given $X=x$ is $p(y|x) = \\mathcal{N}\\left(\\rho\\frac{\\sigma_y}{\\sigma_x} x, \\sigma_y^2(1-\\rho^2)\\right)$.\n\n**2. Autoregressive Representation of the Gibbs Sampler**\n\nThe systematic Gibbs sampler updates coordinates sequentially. Given the state at iteration $n$, $(X_n, Y_n)$, a full sweep to generate $(X_{n+1}, Y_{n+1})$ proceeds as:\n1. Sample $X_{n+1}$ from $p(x|Y_n)$.\n2. Sample $Y_{n+1}$ from $p(y|X_{n+1})$.\n\nWe can express this using random variables:\n1. $X_{n+1} = \\rho\\frac{\\sigma_x}{\\sigma_y} Y_n + \\sqrt{\\sigma_x^2(1-\\rho^2)} Z_1$, where $Z_1 \\sim \\mathcal{N}(0,1)$.\n2. $Y_{n+1} = \\rho\\frac{\\sigma_y}{\\sigma_x} X_{n+1} + \\sqrt{\\sigma_y^2(1-\\rho^2)} Z_2$, where $Z_2 \\sim \\mathcal{N}(0,1)$ is independent of $Z_1$ and $(X_n, Y_n)$.\n\nTo find the induced process for the $Y$-marginal, we substitute the expression for $X_{n+1}$ into the one for $Y_{n+1}$:\n$$\nY_{n+1} = \\rho\\frac{\\sigma_y}{\\sigma_x} \\left( \\rho\\frac{\\sigma_x}{\\sigma_y} Y_n + \\sigma_x\\sqrt{1-\\rho^2} Z_1 \\right) + \\sigma_y\\sqrt{1-\\rho^2} Z_2\n$$\n$$\nY_{n+1} = \\rho^2 Y_n + \\rho \\sigma_y \\sqrt{1-\\rho^2} Z_1 + \\sigma_y\\sqrt{1-\\rho^2} Z_2\n$$\nThis shows that the marginal process for $Y$ is an AR(1) process, $Y_{n+1} = \\rho^2 Y_n + \\epsilon_{n+1}$, where $\\epsilon_{n+1}$ is a zero-mean noise term independent of $Y_n$.\n\n**3. Action of the Markov Operator $K$**\n\nThe operator norm $\\|K\\|_{L_{2}^{0}(\\pi)}$ is the largest magnitude of the eigenvalues of $K$ corresponding to non-constant eigenfunctions. We can probe the action of $K$ on simple linear functions, which are in $L_2^0(\\pi)$ as the target distribution is centered. Let's consider the function $f(x,y) = y$.\n$$\n(Ky)(x_n, y_n) = \\mathbb{E}[Y_{n+1} | (X_n, Y_n)=(x_n, y_n)]\n$$\nTaking the expectation of the AR(1) expression for $Y_{n+1}$ conditional on $(X_n, Y_n)$:\n$$\n\\mathbb{E}[Y_{n+1} | Y_n=y_n] = \\mathbb{E}[\\rho^2 Y_n + \\epsilon_{n+1} | Y_n=y_n] = \\rho^2 y_n + \\mathbb{E}[\\epsilon_{n+1}] = \\rho^2 y_n\n$$\nThis shows that the function $f(x,y)=y$ is an eigenfunction of the Markov operator $K$ with eigenvalue $\\lambda = \\rho^2$. Similarly, $f(x,y)=x$ is an eigenfunction with eigenvalue $\\rho^2$.\n\n**4. Spectral Gap Calculation**\n\nFor the specific case of a systematic Gibbs sampler on a bivariate normal distribution, it is a known result that the eigenvalues corresponding to the slowest-decaying modes are given by the correlation of the induced AR(1) process. The set of eigenvalues of $K$ acting on $L_2^0(\\pi)$ is $\\{\\rho^{2k} : k \\ge 1\\}$, with the largest magnitude being $\\rho^2$.\nTherefore, the norm of the operator on the space of mean-zero functions is:\n$$\n\\|K\\|_{L_{2}^{0}(\\pi)} = \\sup_{\\lambda \\in \\text{spectrum}(K) \\setminus \\{1\\}} |\\lambda| = \\rho^2\n$$\nThe spectral gap $\\gamma$ is defined as $\\gamma = 1 - \\|K\\|_{L_{2}^{0}(\\pi)}$. Substituting the value we found:\n$$\n\\gamma = 1 - \\rho^2\n$$\nThis result demonstrates that as the correlation $|\\rho|$ between the variables approaches 1, the spectral gap approaches 0, leading to increasingly slow convergence of the Gibbs sampler.", "answer": "$$\n\\boxed{1 - \\rho^2}\n$$", "id": "3335418"}, {"introduction": "Our final practice explores a cutting-edge topic in MCMC: the optimal tuning of algorithms in high dimensions. When targeting a high-dimensional distribution, the efficiency of a Random Walk Metropolis sampler critically depends on the choice of its proposal variance. By analyzing the problem in a scaling limit where the discrete chain approximates a continuous-time diffusion, you will derive the famous result for the optimal proposal scale that balances move distance and acceptance probability, thereby maximizing the spectral gap [@problem_id:3335442].", "problem": "Consider the Random Walk Metropolis (RWM) algorithm on $\\mathbb{R}^{d}$ targeting the standard Gaussian distribution $\\mathcal{N}(0, I_{d})$. The proposal is $Y = X + \\sigma Z$, where $Z \\sim \\mathcal{N}(0, I_{d})$ and $\\sigma^{2}  0$ is the proposal variance. The Metropolis–Hastings acceptance probability is $a(X, Y) = \\min\\{1, \\exp(-\\frac{1}{2}(|Y|^{2} - |X|^{2}))\\}$, and the resulting Markov chain is reversible with respect to the target distribution.\n\nStarting from:\n- the definition of spectral gap $\\gamma$ of a reversible Markov chain via its Dirichlet form;\n- the characterization of the Ornstein–Uhlenbeck (OU) semigroup’s spectral gap on $\\mathbb{R}$ for the invariant $\\mathcal{N}(0, 1)$ distribution;\n- and a scientifically standard high-dimensional scaling regime $\\sigma = \\ell / \\sqrt{d}$ with fixed $\\ell  0$,\n\nderive the leading-order dependence of the discrete-time spectral gap $\\gamma(\\sigma, d)$ on $\\sigma$ in the high-dimensional limit $d \\to \\infty$. Your derivation should proceed from first principles: identify the asymptotic acceptance behavior, relate the small-step Dirichlet form of the Metropolis kernel to the OU Dirichlet form acting on one coordinate, and consistently account for the time-rescaling needed to obtain a non-degenerate limiting generator. Then identify the regime that balances move size and acceptance to maximize the spectral gap, and solve for the optimal proposal scale as a function of $d$.\n\nProvide your final answer as the optimal proposal standard deviation $\\sigma_{\\mathrm{opt}}(d)$ expressed in terms of $d$. Round your final answer to three significant figures. No physical units are involved, and no angles appear in this problem.", "solution": "This problem requires deriving the optimal proposal standard deviation for a Random Walk Metropolis (RWM) algorithm in high dimensions by analyzing its diffusive limit. The key is to find the proposal scaling that maximizes the speed of the limiting diffusion, which in turn maximizes the spectral gap of the discrete chain.\n\n**1. Asymptotic Acceptance Probability**\n\nThe acceptance probability depends on the change in the log-target density, which is $\\ln R = -\\frac{1}{2}(|Y|^2 - |X|^2)$. Let's analyze this quantity under the scaling $\\sigma = \\ell / \\sqrt{d}$.\n$$\n\\ln R = -\\frac{1}{2} \\left(|X+\\sigma Z|^2 - |X|^2\\right) = -\\frac{1}{2} \\left(2\\sigma \\langle X, Z \\rangle + \\sigma^2 |Z|^2\\right) = -\\frac{\\ell}{\\sqrt{d}} \\langle X, Z \\rangle - \\frac{\\ell^2}{2d} |Z|^2\n$$\nHere, $X$ is a sample from the stationary distribution $\\mathcal{N}(0, I_d)$, and $Z$ is an independent proposal innovation from $\\mathcal{N}(0, I_d)$. For large $d$:\n- $\\langle X, Z \\rangle = \\sum_{i=1}^d X_i Z_i$ is a sum of i.i.d. random variables with mean 0 and variance 1. By the Central Limit Theorem, $\\frac{1}{\\sqrt{d}}\\langle X, Z \\rangle \\xrightarrow{d} \\mathcal{N}(0,1)$. Thus, the first term $-\\frac{\\ell}{\\sqrt{d}}\\langle X, Z \\rangle$ converges in distribution to $\\mathcal{N}(0, \\ell^2)$.\n- $|Z|^2 = \\sum_{i=1}^d Z_i^2$ is a sum of i.i.d. squared standard normals. By the Law of Large Numbers, $|Z|^2/d \\xrightarrow{p} \\mathbb{E}[Z_1^2] = 1$. Thus, the second term $-\\frac{\\ell^2}{2d}|Z|^2$ converges in probability to $-\\ell^2/2$.\n\nCombining these, the log-ratio $\\ln R$ converges in distribution to a random variable $S \\sim \\mathcal{N}(-\\ell^2/2, \\ell^2)$. The asymptotic average acceptance rate, $A(\\ell)$, is the expectation of $\\min\\{1, \\exp(S)\\}$. For $S \\sim \\mathcal{N}(\\mu, \\nu^2)$, this is given by $A = 2\\Phi(\\mu/\\nu)$ if $\\mu \\le 0$ and $\\mu+\\nu^2/2 = 0$. With $\\mu = -\\ell^2/2$ and $\\nu = \\ell$, these conditions are satisfied, and we get:\n$$\nA(\\ell) = 2\\Phi\\left(\\frac{-\\ell^2/2}{\\ell}\\right) = 2\\Phi(-\\ell/2)\n$$\nwhere $\\Phi(\\cdot)$ is the standard normal CDF.\n\n**2. Continuous-Time Limit and Spectral Gap**\n\nIn the high-dimensional limit, the discrete-time Markov chain, when time is accelerated by a factor of $d$, converges to a continuous-time diffusion process. The efficiency of the discrete chain is determined by the \"speed\" of this limiting process. The spectral gap of the discrete chain, $\\gamma_d$, is related to the spectral gap of the generator of the limiting diffusion, $\\mathcal{G}_{\\text{lim}}$, by $\\gamma_d \\approx \\text{Gap}(\\mathcal{G}_{\\text{lim}}) / d$.\n\nThe generator of the limiting process is $\\mathcal{G}_{\\text{lim}} = c(\\ell) \\mathcal{L}_{\\text{OU}}$, where $\\mathcal{L}_{\\text{OU}}$ is the generator of a standard 1-D Ornstein-Uhlenbeck process (e.g., $dX_t = -X_t dt + \\sqrt{2} dW_t$) and has a fixed spectral gap. The function $c(\\ell)$ is the speed of the process, which depends on the proposal scaling $\\ell$. It is given by the average squared jump distance in one dimension, which is $\\mathbb{E}[(\\sigma Z_1)^2] = \\sigma^2$, multiplied by the acceptance rate $A(\\ell)$.\n$$\nc(\\ell) = d \\cdot \\sigma^2 \\cdot A(\\ell) = d \\cdot \\left(\\frac{\\ell^2}{d}\\right) \\cdot A(\\ell) = \\ell^2 A(\\ell)\n$$\nThe problem is now to maximize the speed function $c(\\ell)$ to maximize the spectral gap $\\gamma_d$.\n\n**3. Optimization**\n\nWe need to find the value of $\\ell$ that maximizes the function $c(\\ell) = \\ell^2 A(\\ell) = 2\\ell^2 \\Phi(-\\ell/2)$. To do this, we compute the derivative with respect to $\\ell$ and set it to zero. Let $\\phi(\\cdot)$ be the standard normal PDF.\n$$\n\\frac{dc}{d\\ell} = \\frac{d}{d\\ell} \\left[2\\ell^2 \\Phi(-\\ell/2)\\right] = 4\\ell \\Phi(-\\ell/2) + 2\\ell^2 \\phi(-\\ell/2) \\cdot \\left(-\\frac{1}{2}\\right) = 0\n$$\nFor $\\ell > 0$, we can divide by $\\ell$:\n$$\n4\\Phi(-\\ell/2) - \\ell\\phi(-\\ell/2) = 0\n$$\nSince $\\phi$ is an even function, we can write this as $4\\Phi(-\\ell/2) = \\ell\\phi(\\ell/2)$. This is a transcendental equation for $\\ell$ that must be solved numerically.\n\n**4. Final Result**\n\nThe numerical solution to the equation $4\\Phi(-x) = 2x\\phi(x)$ (with $x = \\ell/2$) is a well-known result in MCMC theory. The optimal value is found to be $\\ell_{\\text{opt}} \\approx 2.38$.\nThis optimal scaling factor $\\ell_{\\text{opt}}$ maximizes the efficiency of the RWM sampler in high dimensions. The corresponding optimal proposal standard deviation $\\sigma_{\\text{opt}}(d)$ is obtained by substituting this value back into the scaling relationship:\n$$\n\\sigma_{\\text{opt}}(d) = \\frac{\\ell_{\\text{opt}}}{\\sqrt{d}} \\approx \\frac{2.38}{\\sqrt{d}}\n$$\nThis result establishes that the proposal variance $\\sigma^2$ should be scaled as $1/d$, with a specific constant of proportionality $\\ell^2 \\approx 2.38^2 \\approx 5.66$ to achieve the fastest convergence. At this optimal scaling, the acceptance rate is $A(2.38) = 2\\Phi(-2.38/2) = 2\\Phi(-1.19) \\approx 0.234$.", "answer": "$$\n\\boxed{\\frac{2.38}{\\sqrt{d}}}\n$$", "id": "3335442"}]}