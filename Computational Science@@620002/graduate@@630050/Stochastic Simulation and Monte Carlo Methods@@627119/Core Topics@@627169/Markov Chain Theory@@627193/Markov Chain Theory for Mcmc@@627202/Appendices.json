{"hands_on_practices": [{"introduction": "This first exercise explores the most fundamental aspect of a Markov chain: its evolution over time. By analyzing a simple random walk on the real line with Gaussian steps, you will see how the distribution of the chain's state changes after multiple transitions. This practice builds direct intuition for the Chapman-Kolmogorov equations and the concept of an n-step transition kernel from first principles. [@problem_id:3319838]", "problem": "Consider a time-homogeneous Markov chain $\\{X_{n}\\}_{n \\geq 0}$ on $\\mathbb{R}$ with transition kernel $P(x, \\cdot) = \\mathcal{N}(x, \\sigma^{2})$, where $\\sigma^{2} \\in (0, \\infty)$ is fixed, and with deterministic initialization $X_{0} = x_{0} \\in \\mathbb{R}$. That is, for each $n \\in \\mathbb{N}$ and each Borel set $A \\subset \\mathbb{R}$, the one-step transition probability satisfies $P(x, A) = \\int_{A} \\varphi_{\\sigma}(y - x) \\, dy$ with $\\varphi_{\\sigma}(u) = (2 \\pi \\sigma^{2})^{-1/2} \\exp\\!\\big(-u^{2}/(2 \\sigma^{2})\\big)$. Starting only from the fundamental definitions of a time-homogeneous Markov chain, the Chapman–Kolmogorov equations, and basic facts about the normal distribution and convolution of independent random variables, derive the $n$-step law of $X_{n}$ under $P^{n}(x_{0}, \\cdot)$, identify its distribution in closed form, and obtain its mean and variance in terms of $x_{0}$, $n$, and $\\sigma^{2}$. \n\nYour final reported answer must be the ordered pair $\\big(\\mathbb{E}[X_{n}], \\operatorname{Var}(X_{n})\\big)$ written as a single row matrix using the $\\mathrm{pmatrix}$ environment. Do not include the distributional form in the final answer; provide that within your derivation. No rounding is required. Express the final answer using exact symbolic quantities $x_{0}$, $n$, and $\\sigma^{2}$.", "solution": "We begin from the definition of a time-homogeneous Markov chain on $\\mathbb{R}$ with transition kernel $P(x, \\cdot)$. The chain satisfies, for all $n \\in \\mathbb{N}$, all bounded measurable $f$, and all $x \\in \\mathbb{R}$,\n$$\n\\mathbb{E}\\!\\left[f(X_{n+1}) \\mid X_{n} = x \\right] \\equiv \\int_{\\mathbb{R}} f(y) \\, P(x, dy).\n$$\nHere the kernel is $P(x, \\cdot) = \\mathcal{N}(x, \\sigma^{2})$, meaning that, conditional on $X_{n} = x$, the next state $X_{n+1}$ has the normal distribution with mean $x$ and variance $\\sigma^{2}$. Equivalently, one may realize the dynamics by introducing an independent and identically distributed sequence $\\{Z_{k}\\}_{k \\geq 1}$ with $Z_{k} \\sim \\mathcal{N}(0, \\sigma^{2})$, independent of $X_{0}$, and writing\n$$\nX_{n+1} = X_{n} + Z_{n+1}, \\quad n \\geq 0.\n$$\nThis representation is consistent with $P(x, \\cdot)$ because for any Borel set $A \\subset \\mathbb{R}$,\n$$\n\\mathbb{P}\\!\\left(X_{n+1} \\in A \\mid X_{n} = x \\right) = \\mathbb{P}\\!\\left(x + Z_{n+1} \\in A \\right) = \\int_{A} \\varphi_{\\sigma}(y - x) \\, dy = P(x, A),\n$$\nwhere $\\varphi_{\\sigma}(u) = (2 \\pi \\sigma^{2})^{-1/2} \\exp\\!\\big(-u^{2}/(2 \\sigma^{2})\\big)$ is the normal density with mean $0$ and variance $\\sigma^{2}$.\n\nIterating the update shows that for any $n \\in \\mathbb{N}$,\n$$\nX_{n} = X_{0} + \\sum_{k=1}^{n} Z_{k} = x_{0} + \\sum_{k=1}^{n} Z_{k}.\n$$\nSince $\\{Z_{k}\\}_{k \\geq 1}$ are independent and each $Z_{k} \\sim \\mathcal{N}(0, \\sigma^{2})$, their sum $\\sum_{k=1}^{n} Z_{k}$ is normal with mean $0$ and variance $n \\sigma^{2}$. There are several equivalent fundamental ways to establish this:\n\n- Convolution of independent normal distributions: the convolution of $\\mathcal{N}(0, \\sigma^{2})$ with itself $n$ times is $\\mathcal{N}(0, n \\sigma^{2})$.\n\n- Characteristic functions: the characteristic function of $Z_{k}$ is $\\phi_{Z}(t) = \\mathbb{E}[\\exp(i t Z_{k})] = \\exp\\!\\big(-\\tfrac{1}{2} \\sigma^{2} t^{2}\\big)$ for $t \\in \\mathbb{R}$. Independence gives\n$$\n\\phi_{\\sum_{k=1}^{n} Z_{k}}(t) = \\prod_{k=1}^{n} \\phi_{Z}(t) = \\left(\\exp\\!\\big(-\\tfrac{1}{2} \\sigma^{2} t^{2}\\big)\\right)^{n} = \\exp\\!\\big(-\\tfrac{1}{2} n \\sigma^{2} t^{2}\\big),\n$$\nwhich is the characteristic function of $\\mathcal{N}(0, n \\sigma^{2})$.\n\nAdding the constant shift $x_{0}$ translates the distribution by $x_{0}$. Hence the law of $X_{n}$ is\n$$\nX_{n} \\sim \\mathcal{N}\\!\\big(x_{0}, \\, n \\sigma^{2}\\big).\n$$\nThis identifies the $n$-step transition kernel as\n$$\nP^{n}(x_{0}, \\cdot) = \\mathcal{N}\\!\\big(x_{0}, \\, n \\sigma^{2}\\big),\n$$\nwhich also follows from the Chapman–Kolmogorov equations by $n$-fold convolution of the one-step kernel with itself.\n\nFrom the identified distribution, the mean and variance follow immediately:\n$$\n\\mathbb{E}[X_{n}] = x_{0}, \\qquad \\operatorname{Var}(X_{n}) = n \\sigma^{2}.\n$$\nThese formulas are consistent with the base case $n = 0$, where $X_{0} = x_{0}$ is degenerate with variance $0$, and they satisfy the additive variance growth expected from the sum of $n$ independent increments each of variance $\\sigma^{2}$.\n\nTherefore, the requested ordered pair $\\big(\\mathbb{E}[X_{n}], \\operatorname{Var}(X_{n})\\big)$ is $\\big(x_{0}, \\, n \\sigma^{2}\\big)$.", "answer": "$$\\boxed{\\begin{pmatrix} x_{0}  n \\sigma^{2} \\end{pmatrix}}$$", "id": "3319838"}, {"introduction": "After understanding how a chain evolves, a crucial question is whether it converges to a stable, long-term behavior described by a stationary distribution. This exercise delves into this concept by analyzing a birth-death process, a common and important class of Markov chains. You will use the principle of detailed balance—a cornerstone of MCMC design—to derive the chain's invariant measure and determine the conditions under which it represents a proper equilibrium state. [@problem_id:3319836]", "problem": "Consider a discrete-time birth–death Markov chain on the countable state space $\\{0,1,2,\\dots\\}$, used as a proposal kernel within a Markov chain Monte Carlo (MCMC) algorithm. The transition probabilities are nearest-neighbor and satisfy $P(i,i+1)=\\lambda_{i}$, $P(i,i-1)=\\mu_{i}$ for $i\\geq 1$, and $P(0,1)=\\lambda_{0}$, $P(0,0)=1-\\lambda_{0}$. Assume the parameterized family\n$$\n\\lambda_{i}=p \\quad \\text{for all } i\\geq 0, \\qquad \\mu_{0}=0, \\qquad \\mu_{i}=q\\,\\frac{i}{i+1} \\quad \\text{for } i\\geq 1,\n$$\nwith $p,q\\in(0,1)$ satisfying $p+q1$ to ensure $P(i,i)\\geq 0$ for all $i$. Starting from the definition of an invariant measure for a Markov chain, derive an invariant measure (up to normalization) for this chain, and determine precisely for which values of the parameters $p$ and $q$ the invariant measure is summable (i.e., defines a proper probability distribution upon normalization).\n\nFinally, compute the closed-form analytic expression for the normalizing constant\n$$\nS(p,q)=\\sum_{i=0}^{\\infty} w_{i},\n$$\nwhere $(w_{i})_{i\\geq 0}$ is your derived invariant measure up to normalization. Your final answer must be the single analytic expression for $S(p,q)$. No rounding is required, and no physical units are involved.", "solution": "An invariant measure $(w_i)_{i\\geq 0}$ for a Markov chain with transition matrix $P$ on a state space $S$ is a set of non-negative numbers satisfying the global balance equations $w = wP$. In component form, this means $w_k = \\sum_{j \\in S} w_j P(j,k)$ for each state $k \\in S$. For a birth–death process, which is a special type of Markov chain on the integers where transitions are restricted to nearest neighbors, a sufficient condition for a measure to be invariant is that it satisfies the detailed balance equations:\n$$\nw_i P(i, j) = w_j P(j, i) \\quad \\text{ for all neighboring states } i, j.\n$$\nIn this problem, the state space is $S = \\{\\,0, 1, 2, \\dots\\,\\}$ and the only allowed transitions are between $i$ and $i+1$. The detailed balance equations thus take the form:\n$$\nw_i P(i, i+1) = w_{i+1} P(i+1, i) \\quad \\text{for all } i \\geq 0.\n$$\nThese equations are sufficient because they imply the global balance equations. For any state $i \\ge 1$, the global balance equation is $w_i = w_{i-1}P(i-1,i) + w_i P(i,i) + w_{i+1}P(i+1,i)$. Using the fact that probabilities sum to one, $P(i,i) = 1 - P(i,i-1) - P(i,i+1)$, this simplifies to $w_i(P(i,i-1) + P(i,i+1)) = w_{i-1}P(i-1,i) + w_{i+1}P(i+1,i)$. The detailed balance condition for the pair $(i-1, i)$ is $w_{i-1}P(i-1,i) = w_i P(i, i-1)$. Substituting this into the simplified global balance equation yields $w_i P(i,i+1) = w_{i+1}P(i+1,i)$, which is the detailed balance condition for the pair $(i, i+1)$. For the state $i=0$, the global balance equation is $w_0 = w_0 P(0,0) + w_1 P(1,0)$. This simplifies to $w_0(1-P(0,0)) = w_1 P(1,0)$, which is $w_0 P(0,1) = w_1 P(1,0)$, exactly the detailed balance equation for the pair $(0,1)$. Hence, it is sufficient to solve the detailed balance equations.\n\nUsing the problem's notation, $\\lambda_i = P(i,i+1)$ and $\\mu_i = P(i,i-1)$, the detailed balance equations become $w_i \\lambda_i = w_{i+1} \\mu_{i+1}$ for $i \\geq 0$. This gives a recurrence relation for the components of the invariant measure:\n$$\nw_{i+1} = w_i \\frac{\\lambda_i}{\\mu_{i+1}} \\quad \\text{for } i \\geq 0.\n$$\nThe problem defines the transition parameters as $\\lambda_i = p$ for all $i \\ge 0$, and $\\mu_i = q\\,\\frac{i}{i+1}$ for $i \\ge 1$. We have $\\mu_0=0$.\nWe can solve for $w_i$ iteratively. Let us set $w_0 = 1$ to find the measure up to a multiplicative constant.\nFor $i=0$: $w_1 = w_0 \\frac{\\lambda_0}{\\mu_1} = 1 \\cdot \\frac{p}{q\\,\\frac{1}{1+1}} = \\frac{p}{q/2} = 2 \\frac{p}{q}$.\nFor $i=1$: $w_2 = w_1 \\frac{\\lambda_1}{\\mu_2} = \\left(2 \\frac{p}{q}\\right) \\frac{p}{q\\,\\frac{2}{2+1}} = \\left(2 \\frac{p}{q}\\right) \\frac{3p}{2q} = 3 \\left(\\frac{p}{q}\\right)^2$.\nFor $i=2$: $w_3 = w_2 \\frac{\\lambda_2}{\\mu_3} = \\left(3 \\left(\\frac{p}{q}\\right)^2\\right) \\frac{p}{q\\,\\frac{3}{3+1}} = \\left(3 \\left(\\frac{p}{q}\\right)^2\\right) \\frac{4p}{3q} = 4 \\left(\\frac{p}{q}\\right)^3$.\nThe emerging pattern suggests the general form $w_i = (i+1) \\left(\\frac{p}{q}\\right)^i$ for $i \\ge 0$. We can verify this formula by induction. The base case $i=0$ gives $w_0 = (0+1)\\left(\\frac{p}{q}\\right)^0 = 1$, which matches our initial choice.\nAssume the formula is true for index $i$. For index $i+1$, the recurrence gives:\n$$\nw_{i+1} = w_i \\frac{\\lambda_i}{\\mu_{i+1}} = \\left((i+1) \\left(\\frac{p}{q}\\right)^i\\right) \\frac{p}{q\\frac{i+1}{i+2}} = (i+1) \\left(\\frac{p}{q}\\right)^i \\frac{p}{q} \\frac{i+2}{i+1} = (i+2) \\left(\\frac{p}{q}\\right)^{i+1}.\n$$\nThis is the proposed formula for the index $i+1$. Thus, by induction, the unnormalized invariant measure is $w_i = C \\cdot (i+1) \\left(\\frac{p}{q}\\right)^i$ for any constant $C$. We take $C=1$ for the subsequent calculation.\n\nFor this invariant measure to be summable, which is the condition for it to correspond to a proper stationary probability distribution upon normalization, the series $S(p,q) = \\sum_{i=0}^{\\infty} w_i$ must converge.\n$$\nS(p,q) = \\sum_{i=0}^{\\infty} (i+1) \\left(\\frac{p}{q}\\right)^i.\n$$\nLet $x = p/q$. The series is an arithmetico-geometric series, $\\sum_{i=0}^{\\infty} (i+1) x^i$. A power series of this form converges if and only if $|x|1$. Since the problem specifies $p, q \\in (0,1)$, we have $x = p/q  0$. The convergence condition is therefore $x  1$, which is $p/q  1$, or $pq$. Along with the given constraints $p,q \\in (0,1)$ and $p+q  1$, the invariant measure is summable if and only if $0  p  q$ and $p+q  1$.\n\nFinally, we compute the closed-form expression for the normalizing constant $S(p,q)$ for the case $pq$. We use the well-known formula for a geometric series, valid for $|x|1$:\n$$\n\\sum_{i=0}^{\\infty} x^i = \\frac{1}{1-x}.\n$$\nDifferentiating both sides with respect to $x$ allows us to evaluate our target sum:\n$$\n\\frac{d}{dx} \\left(\\sum_{i=0}^{\\infty} x^i\\right) = \\sum_{i=0}^{\\infty} \\frac{d}{dx} (x^i) = \\sum_{i=1}^{\\infty} i x^{i-1}.\n$$\nBy re-indexing with $j=i-1$, the sum becomes $\\sum_{j=0}^{\\infty} (j+1) x^j$, which is exactly the form of our series. Therefore, the sum is equal to the derivative of the geometric series sum:\n$$\nS(p,q) = \\sum_{i=0}^{\\infty} (i+1) x^i = \\frac{d}{dx} \\left(\\frac{1}{1-x}\\right) = \\frac{d}{dx} (1-x)^{-1} = (-1)(1-x)^{-2}(-1) = \\frac{1}{(1-x)^2}.\n$$\nSubstituting $x=p/q$ back into this result provides the final analytic expression for the normalizing constant:\n$$\nS(p,q) = \\frac{1}{\\left(1 - \\frac{p}{q}\\right)^2} = \\frac{1}{\\left(\\frac{q-p}{q}\\right)^2} = \\frac{q^2}{(q-p)^2}.\n$$", "answer": "$$\\boxed{\\frac{q^2}{(q-p)^2}}$$", "id": "3319836"}, {"introduction": "For MCMC methods, knowing that a chain converges is not enough; we must also understand the efficiency of the sampling process. This final practice provides a concrete link between the spectral properties of a transition kernel and the statistical performance of the corresponding sampler. By computing the asymptotic variance for a simple two-state reversible chain, you will quantify how temporal correlations in the chain affect the precision of Monte Carlo estimates. [@problem_id:3319839]", "problem": "Consider a time-homogeneous, irreducible, aperiodic Markov chain with finite state space $\\mathcal{X}$, transition kernel $P(x,y)$, and stationary distribution $\\pi$. Assume $P$ is reversible with respect to $\\pi$, meaning $P$ satisfies detailed balance: $\\pi(x) P(x,y) = \\pi(y) P(y,x)$ for all $x,y \\in \\mathcal{X}$. Let $P$ act on $L^{2}(\\pi)$ via $(P g)(x) = \\sum_{y \\in \\mathcal{X}} P(x,y) g(y)$, and define the $L^{2}(\\pi)$ inner product by $\\langle g, h \\rangle_{\\pi} = \\sum_{x \\in \\mathcal{X}} \\pi(x) g(x) h(x)$. For a measurable function $f : \\mathcal{X} \\to \\mathbb{R}$ with $\\mathbb{E}_{\\pi}[f] = \\sum_{x \\in \\mathcal{X}} \\pi(x) f(x)$ finite, consider the ergodic averages $\\bar{f}_{n} = \\frac{1}{n} \\sum_{t=1}^{n} f(X_{t})$, where $(X_{t})$ is the Markov chain with transition kernel $P$ started in stationarity.\n\nAssume the central limit theorem for Markov chains applies to $f$ so that $\\sqrt{n}(\\bar{f}_{n} - \\mathbb{E}_{\\pi}[f])$ converges in distribution to a centered normal random variable with asymptotic variance $\\sigma^{2}_{f}$. Starting from the definitions above and using only foundational facts about self-adjoint linear operators on Hilbert spaces and spectral measures, derive a representation of $\\sigma^{2}_{f}$ in terms of the spectral measure associated with $P$ and the centered function $g = f - \\mathbb{E}_{\\pi}[f]$.\n\nThen, specialize to a two-state reversible chain $\\mathcal{X} = \\{0,1\\}$ with\n$$\nP = \\begin{pmatrix}\n1 - \\alpha  \\alpha \\\\\n\\beta  1 - \\beta\n\\end{pmatrix},\n$$\nwhere $\\alpha \\in (0,1)$ and $\\beta \\in (0,1)$, so the stationary distribution is $\\pi(0) = \\frac{\\beta}{\\alpha + \\beta}$ and $\\pi(1) = \\frac{\\alpha}{\\alpha + \\beta}$. Let $f(0) = u$ and $f(1) = v$ with $u, v \\in \\mathbb{R}$. Compute a closed-form expression for the asymptotic variance $\\sigma^{2}_{f}$ of $\\bar{f}_{n}$ as a function of $\\alpha$, $\\beta$, $u$, and $v$. Express your final answer as a single analytic expression. If you carry out any intermediate simplifications, ensure all steps are valid for $\\alpha \\in (0,1)$ and $\\beta \\in (0,1)$.", "solution": "### Part 1: Derivation of the Asymptotic Variance in Spectral Form\n\nThe asymptotic variance of the ergodic average $\\bar{f}_n$ is given by the sum of autocovariances:\n$$ \\sigma_f^2 = \\sum_{k=-\\infty}^{\\infty} \\text{Cov}_{\\pi}(f(X_0), f(X_k)) $$\nSince the chain is stationary, $\\text{Cov}_{\\pi}(f(X_0), f(X_k)) = \\text{Cov}_{\\pi}(f(X_0), f(X_{-k}))$, so we can write:\n$$ \\sigma_f^2 = \\text{Var}_{\\pi}(f(X_0)) + 2 \\sum_{k=1}^{\\infty} \\text{Cov}_{\\pi}(f(X_0), f(X_k)) $$\nLet $g = f - \\mathbb{E}_{\\pi}[f]$ be the centered function. We have $\\mathbb{E}_{\\pi}[g] = 0$, so $\\sigma_f^2 = \\sigma_g^2$. The variance and covariance terms can be expressed using the $L^2(\\pi)$ inner product.\n$\\text{Var}_{\\pi}(f(X_0)) = \\mathbb{E}_{\\pi}[g(X_0)^2] = \\sum_x \\pi(x) g(x)^2 = \\langle g, g \\rangle_{\\pi}$.\nThe covariance is $\\text{Cov}_{\\pi}(f(X_0), f(X_k)) = \\mathbb{E}_{\\pi}[g(X_0) g(X_k)]$. By the law of total expectation and the definition of the operator $P$:\n$$ \\mathbb{E}_{\\pi}[g(X_0) g(X_k)] = \\mathbb{E}_{\\pi} \\left[ g(X_0) \\mathbb{E}[g(X_k) | X_0] \\right] = \\mathbb{E}_{\\pi} [g(X_0) (P^k g)(X_0)] = \\langle g, P^k g \\rangle_{\\pi} $$\nThus, the asymptotic variance can be written in operator notation as:\n$$ \\sigma_f^2 = \\langle g, g \\rangle_{\\pi} + 2 \\sum_{k=1}^{\\infty} \\langle g, P^k g \\rangle_{\\pi} = \\left\\langle g, \\left(I + 2 \\sum_{k=1}^{\\infty} P^k \\right) g \\right\\rangle_{\\pi} $$\nThe reversibility condition implies that the operator $P$ is self-adjoint on the Hilbert space $L^2(\\pi)$. Since the state space $\\mathcal{X}$ is finite, $L^2(\\pi)$ is finite-dimensional. By the spectral theorem, there exists an orthonormal basis of eigenfunctions $\\{\\psi_i\\}_{i=1}^{|\\mathcal{X}|}$ such that $P\\psi_i = \\lambda_i \\psi_i$ and $\\langle \\psi_i, \\psi_j \\rangle_{\\pi} = \\delta_{ij}$. The eigenvalues $\\lambda_i$ are real.\nFor an irreducible, aperiodic chain, the eigenvalue $\\lambda_1=1$ is simple, and the corresponding eigenfunction is a constant, which we can take to be $\\psi_1(x) = 1$. All other eigenvalues satisfy $|\\lambda_i|  1$.\nThe centered function $g$ is orthogonal to the space of constant functions:\n$$ \\langle g, \\psi_1 \\rangle_{\\pi} = \\sum_x \\pi(x) g(x) \\psi_1(x) = \\sum_x \\pi(x) g(x) = \\mathbb{E}_{\\pi}[g] = 0 $$\nWe can expand $g$ in the eigenbasis: $g = \\sum_{i=1}^{|\\mathcal{X}|} \\langle g, \\psi_i \\rangle_{\\pi} \\psi_i$. Since $\\langle g, \\psi_1 \\rangle_{\\pi} = 0$, this sum is effectively over $i=2, \\dots, |\\mathcal{X}|$.\nUsing this expansion, we compute $\\langle g, P^k g \\rangle_{\\pi}$:\n$$ \\langle g, P^k g \\rangle_{\\pi} = \\left\\langle \\sum_i \\langle g, \\psi_i \\rangle_{\\pi} \\psi_i, \\sum_j \\langle g, \\psi_j \\rangle_{\\pi} \\lambda_j^k \\psi_j \\right\\rangle_{\\pi} = \\sum_{i,j} \\langle g, \\psi_i \\rangle_{\\pi} \\langle g, \\psi_j \\rangle_{\\pi} \\lambda_j^k \\langle \\psi_i, \\psi_j \\rangle_{\\pi} $$\nUsing orthonormality, $\\langle \\psi_i, \\psi_j \\rangle_{\\pi} = \\delta_{ij}$, this simplifies to:\n$$ \\langle g, P^k g \\rangle_{\\pi} = \\sum_{i=2}^{|\\mathcal{X}|} (\\langle g, \\psi_i \\rangle_{\\pi})^2 \\lambda_i^k $$\nSubstituting this back into the expression for $\\sigma_f^2$:\n$$ \\sigma_f^2 = \\sum_{i=2}^{|\\mathcal{X}|} (\\langle g, \\psi_i \\rangle_{\\pi})^2 + 2 \\sum_{k=1}^{\\infty} \\sum_{i=2}^{|\\mathcal{X}|} (\\langle g, \\psi_i \\rangle_{\\pi})^2 \\lambda_i^k = \\sum_{i=2}^{|\\mathcal{X}|} (\\langle g, \\psi_i \\rangle_{\\pi})^2 \\left( 1 + 2 \\sum_{k=1}^{\\infty} \\lambda_i^k \\right) $$\nFor each $i \\ge 2$, $|\\lambda_i|  1$, so the geometric series converges: $\\sum_{k=1}^{\\infty} \\lambda_i^k = \\frac{\\lambda_i}{1 - \\lambda_i}$. The term in parentheses becomes:\n$$ 1 + 2 \\frac{\\lambda_i}{1 - \\lambda_i} = \\frac{1 - \\lambda_i + 2\\lambda_i}{1 - \\lambda_i} = \\frac{1 + \\lambda_i}{1 - \\lambda_i} $$\nSo the spectral representation of the asymptotic variance is:\n$$ \\sigma_f^2 = \\sum_{i=2}^{|\\mathcal{X}|} \\frac{1 + \\lambda_i}{1 - \\lambda_i} (\\langle g, \\psi_i \\rangle_{\\pi})^2 $$\nThis can be expressed using the projection-valued spectral measure $E$ of $P$. $P = \\int \\lambda dE(\\lambda)$. The measure $\\mu_g(A) = \\langle g, E(A)g \\rangle_{\\pi} = \\|E(A)g\\|_{\\pi}^2$ captures the distribution of $g$ across the spectrum. Since $E(\\{\\lambda_i\\})$ is the projector onto the $\\lambda_i$-eigenspace $\\Pi_i$, we have $\\|\\Pi_i g\\|_{\\pi}^2 = (\\langle g, \\psi_i \\rangle_{\\pi})^2$ for simple eigenvalues. The sum becomes an integral over the spectrum of $P$, excluding the eigenvalue $1$ because $g$ has no component in that eigenspace:\n$$ \\sigma_f^2 = \\int_{\\sigma(P) \\setminus \\{1\\}} \\frac{1+\\lambda}{1-\\lambda} \\, d\\mu_g(\\lambda) $$\n\n### Part 2: Calculation for the Two-State Chain\n\nWe specialize the result to the given two-state chain. The state space is $\\mathcal{X}=\\{0,1\\}$.\nFirst, we find the eigenvalues of the transition matrix $P = \\begin{pmatrix} 1-\\alpha  \\alpha \\\\ \\beta  1-\\beta \\end{pmatrix}$. The characteristic equation is $\\det(P - \\lambda I)=0$:\n$$ (1-\\alpha-\\lambda)(1-\\beta-\\lambda) - \\alpha\\beta = \\lambda^2 - (2-\\alpha-\\beta)\\lambda + (1-\\alpha-\\beta) = 0 $$\nThis factors as $(\\lambda-1)(\\lambda - (1-\\alpha-\\beta)) = 0$. The eigenvalues are $\\lambda_1 = 1$ and $\\lambda_2 = 1-\\alpha-\\beta$. Since $\\alpha, \\beta \\in (0,1)$, we have $-1  \\lambda_2  1$.\n\nFor a two-state space, $|\\mathcal{X}|=2$, the sum for $\\sigma_f^2$ has only one term, for $i=2$:\n$$ \\sigma_f^2 = \\frac{1+\\lambda_2}{1-\\lambda_2} (\\langle g, \\psi_2 \\rangle_{\\pi})^2 $$\nwhere $\\psi_2$ is the normalized eigenfunction for $\\lambda_2$.\nSince $g$ is orthogonal to the constant eigenfunction $\\psi_1$, it must lie entirely in the eigenspace of $\\lambda_2$. Therefore, the coefficient squared $(\\langle g, \\psi_2 \\rangle_{\\pi})^2$ is simply the total squared norm of $g$, $\\|g\\|_{\\pi}^2 = \\langle g, g \\rangle_{\\pi}$.\nSo, we have the simplified formula:\n$$ \\sigma_f^2 = \\frac{1+\\lambda_2}{1-\\lambda_2} \\|g\\|_{\\pi}^2 $$\nNext, we compute the two components of this formula. The term involving eigenvalues is:\n$$ \\frac{1+\\lambda_2}{1-\\lambda_2} = \\frac{1+(1-\\alpha-\\beta)}{1-(1-\\alpha-\\beta)} = \\frac{2-\\alpha-\\beta}{\\alpha+\\beta} $$\nNext, we compute $\\|g\\|_{\\pi}^2$. First, we find $g = f - \\mathbb{E}_{\\pi}[f]$. The expectation is:\n$$ \\mathbb{E}_{\\pi}[f] = \\pi(0)f(0) + \\pi(1)f(1) = \\frac{\\beta}{\\alpha+\\beta} u + \\frac{\\alpha}{\\alpha+\\beta} v = \\frac{\\beta u + \\alpha v}{\\alpha+\\beta} $$\nThe components of the centered function $g$ are:\n$g(0) = f(0) - \\mathbb{E}_{\\pi}[f] = u - \\frac{\\beta u + \\alpha v}{\\alpha+\\beta} = \\frac{(\\alpha+\\beta)u - \\beta u - \\alpha v}{\\alpha+\\beta} = \\frac{\\alpha(u-v)}{\\alpha+\\beta}$.\n$g(1) = f(1) - \\mathbb{E}_{\\pi}[f] = v - \\frac{\\beta u + \\alpha v}{\\alpha+\\beta} = \\frac{(\\alpha+\\beta)v - \\beta u - \\alpha v}{\\alpha+\\beta} = \\frac{-\\beta(u-v)}{\\alpha+\\beta}$.\nThe squared norm $\\|g\\|_{\\pi}^2 = \\pi(0)g(0)^2 + \\pi(1)g(1)^2$ is:\n$$ \\|g\\|_{\\pi}^2 = \\frac{\\beta}{\\alpha+\\beta} \\left(\\frac{\\alpha(u-v)}{\\alpha+\\beta}\\right)^2 + \\frac{\\alpha}{\\alpha+\\beta} \\left(\\frac{-\\beta(u-v)}{\\alpha+\\beta}\\right)^2 $$\n$$ = \\frac{(u-v)^2}{(\\alpha+\\beta)^3} \\left( \\beta\\alpha^2 + \\alpha\\beta^2 \\right) = \\frac{(u-v)^2}{(\\alpha+\\beta)^3} \\alpha\\beta(\\alpha+\\beta) = \\frac{\\alpha\\beta(u-v)^2}{(\\alpha+\\beta)^2} $$\nFinally, we combine the two components to find $\\sigma_f^2$:\n$$ \\sigma_f^2 = \\left(\\frac{2-\\alpha-\\beta}{\\alpha+\\beta}\\right) \\left(\\frac{\\alpha\\beta(u-v)^2}{(\\alpha+\\beta)^2}\\right) = \\frac{\\alpha\\beta(2-\\alpha-\\beta)(u-v)^2}{(\\alpha+\\beta)^3} $$\nThis is the closed-form expression for the asymptotic variance.", "answer": "$$\\boxed{\\frac{\\alpha\\beta(2 - \\alpha - \\beta)(u-v)^2}{(\\alpha+\\beta)^3}}$$", "id": "3319839"}]}