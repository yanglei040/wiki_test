{"hands_on_practices": [{"introduction": "The rate at which a Markov chain converges to its stationary distribution is fundamentally governed by the eigenvalues of its transition matrix. This first exercise provides a concrete opportunity to forge the connection between a chain's dynamics and its spectral properties. You will derive the transition rule for a new Markov chain formed by the difference of two independent random walks and then use this to calculate a specific eigenvalue, a key step in analyzing its convergence speed. [@problem_id:787957]", "problem": "Consider two independent random walks, $X_t$ and $Y_t$, on the vertices of a cycle graph $\\mathbb{Z}_N = \\{0, 1, \\dots, N-1\\}$, where $N$ is an integer multiple of 4.\nThe first walker, $X_t$, moves according to the following rule: from its current position $i$, it jumps to $(i+1) \\pmod N$ with probability $p$, and stays at $i$ with probability $1-p$.\nThe second walker, $Y_t$, moves as follows: from its current position $j$, it jumps to $(j-1) \\pmod N$ with probability $q$, and stays at $j$ with probability $1-q$. Here $p, q \\in (0, 1)$ are constant probabilities.\n\nA new stochastic process, $Z_t$, is defined as the difference between the positions of the two walkers, taken modulo $N$:\n$$Z_t = (X_t - Y_t) \\pmod N$$\nThis process $Z_t$ forms a Markov chain on the state space $\\mathbb{Z}_N$. The rate of convergence of this Markov chain to its stationary distribution is governed by the magnitudes of the eigenvalues of its transition matrix.\n\nLet $\\lambda_k$ for $k \\in \\{0, 1, \\dots, N-1\\}$ be the eigenvalues of the transition matrix for the Markov chain $Z_t$. Find the exact value of the modulus of the eigenvalue $\\lambda_{N/4}$.", "solution": "1. Transition probabilities for $Z_t$:\n   $$P(\\Delta Z=0)=(1-p)(1-q),\\quad P(\\Delta Z=1)=p(1-q)+(1-p)q,\\quad P(\\Delta Z=2)=pq.$$\n2. Eigenvalue formula on $\\mathbb{Z}_N$:\n   $$\\lambda_k = (1-p)(1-q) + \\bigl[p(1-q)+(1-p)q\\bigr]e^{2\\pi i k/N} + pq\\,e^{4\\pi i k/N}.$$\n3. For $k=N/4$, set $\\theta=2\\pi k/N=\\pi/2$, so $e^{i\\theta}=i$, $e^{2i\\theta}=-1$. Then\n   $$\\lambda_{N/4} = (1-p)(1-q) + \\bigl[p(1-q)+(1-p)q\\bigr]\\,i + pq\\,(-1).$$\n4. Write real and imaginary parts:\n   $$\\Re\\lambda_{N/4} = (1-p)(1-q)-pq = 1 - p - q,\\quad\n     \\Im\\lambda_{N/4} = p(1-q)+(1-p)q = p+q-2pq.$$\n5. Modulus:\n   $$|\\lambda_{N/4}| = \\sqrt{\\bigl(1-p-q\\bigr)^2 + \\bigl(p+q-2pq\\bigr)^2}.$$", "answer": "$$\\boxed{\\sqrt{(1-p-q)^2 + (p+q-2pq)^2}}$$", "id": "787957"}, {"introduction": "While direct analytical calculation of eigenvalues is instructive, it is often intractable for the large, complex chains encountered in real-world applications. This practice shifts our focus to numerical methods, demonstrating how to estimate the spectral gap using the power method with deflation. You will not only implement this fundamental algorithm but also use your estimate to derive a rigorous upper bound on the mixing time, bridging the gap between numerical linear algebra and Markov chain theory. [@problem_id:3283319]", "problem": "You are given a collection of finite-state Markov chains, each represented by a transition matrix $P \\in \\mathbb{R}^{n \\times n}$ with nonnegative entries and unit column sums. In all provided test cases, $P$ is symmetric and doubly stochastic (both row and column sums are $1$), so the stationary distribution is uniform. Your task is to design and implement a program that estimates the spectral gap using the power method with deflation, and then uses this estimate to compute an upper bound on the mixing time in total variation distance.\n\nBegin from fundamental facts:\n- A real symmetric doubly stochastic matrix $P$ has eigenvalues in $[-1,1]$, with the largest eigenvalue equal to $1$ and the corresponding eigenvector proportional to the all-ones vector. By the Perron–Frobenius theorem, the dominant eigenvalue $\\lambda_1$ of a nonnegative, irreducible column-stochastic matrix is $1$.\n- The spectral gap is defined as $\\gamma = 1 - \\lambda_\\ast$, where $\\lambda_\\ast = \\max\\{|\\lambda| : \\lambda \\in \\sigma(P), \\lambda \\ne 1\\}$ is the magnitude of the second-largest eigenvalue in absolute value.\n- The power method iterates $x_{k+1} = P x_k / \\|P x_k\\|_2$ from an initial $x_0 \\ne 0$, and converges to a dominant eigenvector. To estimate a subdominant eigenvalue (here $\\lambda_\\ast$), you must deflate by projecting iterates onto the subspace orthogonal to the dominant eigenvector so that they converge to an eigenvector associated with $\\lambda_\\ast$.\n- For a reversible, symmetric, doubly stochastic, and lazy chain, the spectral decomposition yields an $L^2$-contraction governed by $\\lambda_\\ast$, and norms satisfy $\\|\\cdot\\|_1 \\le \\sqrt{n}\\,\\|\\cdot\\|_2$. Total variation distance is $\\|\\cdot\\|_{\\mathrm{TV}} = \\tfrac{1}{2}\\|\\cdot\\|_1$. Use these facts to derive an upper bound on the mixing time to achieve total variation distance at most a given tolerance $\\varepsilon$, starting from the worst-case initial state.\n\nImplement the following steps for each test case:\n1. Use the power method to approximate the dominant eigenvector $v_1$ of $P$ and its eigenvalue. Normalize $v_1$ to have unit Euclidean norm.\n2. Use deflated power iteration: start from a random nonzero vector $x_0$, orthogonalize it against $v_1$ using the Euclidean inner product, and iterate $x_{k+1} = P x_k$, orthogonalizing after each multiplication against $v_1$, followed by normalization. Estimate the subdominant eigenvalue $\\lambda_\\ast$ via the Rayleigh quotient $r(x) = \\frac{x^\\top P x}{x^\\top x}$ when $x$ has converged.\n3. Compute the spectral gap $\\gamma = 1 - \\lambda_\\ast$.\n4. Using the inequality chain from the spectral contraction in $\\ell^2$ and $\\|\\cdot\\|_1 \\le \\sqrt{n}\\,\\|\\cdot\\|_2$, derive a worst-case upper bound on the total variation mixing time $t_{\\mathrm{mix}}(\\varepsilon)$ for initial distribution equal to a point mass (one-hot vector), that is, a bound on the smallest integer $t$ such that $\\|\\mu_t - \\pi\\|_{\\mathrm{TV}} \\le \\varepsilon$. Express your final bound solely in terms of $n$, $\\varepsilon$, and the estimated $\\lambda_\\ast$.\n\nUse the following test suite of matrices and tolerances, written in LaTeX:\n- Case A: Two-state, strongly lazy and symmetric,\n$$\nP_A = \\begin{bmatrix}\n0.95 & 0.05\\\\\n0.05 & 0.95\n\\end{bmatrix},\\quad \\varepsilon_A = 0.01.\n$$\n- Case B: Five-state cycle with lazy random walk, where $S$ is the simple random walk on the cycle graph with probability $\\tfrac{1}{2}$ to each neighbor and $0$ otherwise, and $P_B = \\tfrac{1}{2}I + \\tfrac{1}{2}S$. Explicitly,\n$$\nS_B = \\begin{bmatrix}\n0 & \\tfrac{1}{2} & 0 & 0 & \\tfrac{1}{2}\\\\\n\\tfrac{1}{2} & 0 & \\tfrac{1}{2} & 0 & 0\\\\\n0 & \\tfrac{1}{2} & 0 & \\tfrac{1}{2} & 0\\\\\n0 & 0 & \\tfrac{1}{2} & 0 & \\tfrac{1}{2}\\\\\n\\tfrac{1}{2} & 0 & 0 & \\tfrac{1}{2} & 0\n\\end{bmatrix},\\quad\nP_B = \\tfrac{1}{2}I + \\tfrac{1}{2}S_B,\\quad \\varepsilon_B = 0.01.\n$$\n- Case C: Three-state symmetric, doubly stochastic tridiagonal,\n$$\nP_C = \\begin{bmatrix}\n0.9 & 0.1 & 0.0\\\\\n0.1 & 0.8 & 0.1\\\\\n0.0 & 0.1 & 0.9\n\\end{bmatrix},\\quad \\varepsilon_C = 0.01.\n$$\n- Case D: Four-state complete graph with lazy random walk,\n$$\nJ_4 = \\begin{bmatrix}\n1 & 1 & 1 & 1\\\\\n1 & 1 & 1 & 1\\\\\n1 & 1 & 1 & 1\\\\\n1 & 1 & 1 & 1\n\\end{bmatrix},\\quad\nP_D = \\tfrac{1}{2}I + \\tfrac{1}{2}\\tfrac{1}{4}J_4,\\quad \\varepsilon_D = 0.01.\n$$\n\nYour program must:\n- Implement the power method and its deflation as described, using a stopping tolerance for the Rayleigh quotient difference of $10^{-12}$ or a maximum of $10{,}000$ iterations per phase.\n- For each case, compute the spectral gap $\\gamma$ and an upper bound on the mixing time $t_{\\mathrm{mix}}(\\varepsilon)$ derived from the above principles, assuming worst-case initialization as a point mass.\n- Aggregate the results for all cases into a single line. The final output format must be a comma-separated Python-style list of lists. Each inner list corresponds to one case and contains two entries $[\\gamma, t]$, where $\\gamma$ is rounded to six decimal places and $t$ is an integer bound. For example, the output should look like\n`[[gamma_A, t_A],[gamma_B, t_B],[gamma_C, t_C],[gamma_D, t_D]]`.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[[gap1,time1],[gap2,time2],[gap3,time3],[gap4,time4]]\"). No external input should be read. No physical units or angles are involved. All answers must be numerical floats or integers as specified.", "solution": "We start from the core definitions and properties for the provided test matrices. A transition matrix $P$ for a finite-state Markov chain with nonnegative entries and unit column sums acts on column probability vectors by $\\mu_{t+1} = P\\,\\mu_t$. In the symmetric, doubly stochastic setting, $P = P^\\top$ and both row and column sums are $1$, which implies that the stationary distribution is uniform: $\\pi = \\frac{1}{n}\\mathbf{1}$. The spectrum of $P$ lies in $[-1,1]$, with the dominant eigenvalue $\\lambda_1 = 1$ and eigenvector proportional to $\\mathbf{1}$.\n\nThe spectral gap is defined by\n$$\n\\gamma = 1 - \\lambda_\\ast,\\quad \\text{where}\\quad \\lambda_\\ast = \\max\\{|\\lambda|:\\lambda\\in\\sigma(P), \\lambda\\ne 1\\}.\n$$\nIn the symmetric setting, the eigenvalues are real, and for a lazy chain (for example, $P = \\tfrac{1}{2}I + \\tfrac{1}{2}S$ with $S$ column-stochastic), all eigenvalues lie in $[0,1]$, so $\\lambda_\\ast$ is the second-largest eigenvalue in $[0,1]$.\n\nTo approximate $\\lambda_1$ and its eigenvector $v_1$, we apply the power method. Let $x_0\\in\\mathbb{R}^n$ be a nonzero vector. Define the iteration\n$$\nx_{k+1} = \\frac{P x_k}{\\|P x_k\\|_2}.\n$$\nUnder the assumptions that $P$ is nonnegative and irreducible with dominant eigenvalue $\\lambda_1=1$ strictly greater in magnitude than other eigenvalues, the sequence $x_k$ converges to an eigenvector $v_1$ associated with $\\lambda_1$. A numerically stable estimate of the eigenvalue is given by the Rayleigh quotient,\n$$\n\\rho(x) = \\frac{x^\\top P x}{x^\\top x},\n$$\nwhich converges to $\\lambda_1$ when $x$ converges to $v_1$.\n\nTo compute the subdominant eigenvalue $\\lambda_\\ast$ via deflation, we restrict the iteration to the subspace orthogonal to $v_1$. Specifically, choose an initial vector $y_0$, project it onto the orthogonal complement of $v_1$ using the Euclidean inner product, and iterate:\n$$\n\\tilde{y}_k = P y_k,\\quad\n\\tilde{y}_k^\\perp = \\tilde{y}_k - (v_1^\\top \\tilde{y}_k) v_1,\\quad\ny_{k+1} = \\frac{\\tilde{y}_k^\\perp}{\\|\\tilde{y}_k^\\perp\\|_2}.\n$$\nIn the symmetric case, this orthogonal iteration converges to an eigenvector $v_2$ associated with the largest eigenvalue in the orthogonal complement to $v_1$, namely $\\lambda_\\ast$. Again, the Rayleigh quotient\n$$\n\\rho(y) = \\frac{y^\\top P y}{y^\\top y}\n$$\nevaluated at the converged $y$ gives an accurate estimate of $\\lambda_\\ast$.\n\nWe now derive an upper bound on the mixing time in total variation distance. For reversible chains (which include symmetric doubly stochastic matrices), the spectral decomposition implies $L^2$ contraction relative to the stationary distribution. Let $\\mu_t = P^t \\mu_0$ be the distribution after $t$ steps from initial distribution $\\mu_0$. Writing the deviation from stationarity as $d_t = \\mu_t - \\pi$, and letting $\\lambda_\\ast$ be the largest eigenvalue magnitude below $1$, we have the $L^2$ bound\n$$\n\\|d_t\\|_2 \\le \\lambda_\\ast^t \\|d_0\\|_2.\n$$\nUsing norm inequalities, we relate the total variation distance to the $L^2$ norm. Since $\\|\\cdot\\|_1 \\le \\sqrt{n}\\,\\|\\cdot\\|_2$ and total variation is defined by $\\|\\cdot\\|_{\\mathrm{TV}} = \\tfrac{1}{2}\\|\\cdot\\|_1$, we obtain\n$$\n\\|\\mu_t - \\pi\\|_{\\mathrm{TV}} = \\tfrac{1}{2}\\|d_t\\|_1 \\le \\tfrac{1}{2} \\sqrt{n}\\,\\|d_t\\|_2 \\le \\tfrac{1}{2} \\sqrt{n}\\,\\lambda_\\ast^t \\|d_0\\|_2.\n$$\nFor worst-case initialization as a point mass, say $\\mu_0 = e_i$ (the $i$-th standard basis vector), and uniform stationary distribution $\\pi = \\tfrac{1}{n}\\mathbf{1}$, the initial deviation has\n$$\n\\|d_0\\|_2 = \\left\\|e_i - \\tfrac{1}{n}\\mathbf{1}\\right\\|_2 = \\sqrt{1 - \\tfrac{1}{n}}.\n$$\nThus,\n$$\n\\|\\mu_t - \\pi\\|_{\\mathrm{TV}} \\le \\frac{1}{2} \\sqrt{n}\\,\\sqrt{1 - \\frac{1}{n}}\\;\\lambda_\\ast^t.\n$$\nTo achieve a tolerance $\\varepsilon$, it suffices to choose $t$ such that\n$$\n\\lambda_\\ast^t \\le \\frac{2\\varepsilon}{\\sqrt{n}\\,\\sqrt{1 - \\frac{1}{n}}}.\n$$\nSolving for $t$ yields\n$$\nt \\ge \\frac{\\ln\\left(\\frac{2\\varepsilon}{\\sqrt{n}\\,\\sqrt{1 - \\frac{1}{n}}}\\right)}{\\ln(\\lambda_\\ast)}.\n$$\nSince $\\lambda_\\ast \\in (0,1)$ for lazy chains, $\\ln(\\lambda_\\ast) < 0$ and the right-hand side is positive. We take the ceiling to obtain an integer upper bound:\n$$\nt_{\\mathrm{mix}}(\\varepsilon) = \\left\\lceil \\frac{\\ln\\left(\\frac{2\\varepsilon}{\\sqrt{n}\\,\\sqrt{1 - \\frac{1}{n}}}\\right)}{\\ln(\\lambda_\\ast)} \\right\\rceil.\n$$\n\nAlgorithmic design:\n- Implement a power method to approximate $v_1$ and $\\lambda_1$ with stopping criterion based on the change in the Rayleigh quotient below $10^{-12}$ or reaching $10{,}000$ iterations.\n- Implement deflated power iteration: orthogonalize iterates against $v_1$ at each step, normalize, and monitor the Rayleigh quotient for convergence to estimate $\\lambda_\\ast$.\n- Compute the spectral gap $\\gamma = 1 - \\lambda_\\ast$.\n- Compute $t_{\\mathrm{mix}}(\\varepsilon)$ using the derived formula with $n$ equal to the dimension of $P$ and the estimated $\\lambda_\\ast$. In the numerically degenerate case when $\\lambda_\\ast$ is extremely small (e.g., due to machine precision), one can safely bound $t_{\\mathrm{mix}}(\\varepsilon)$ by $1$ since the contraction is immediate; however, the provided test suite avoids this edge case.\n\nTest suite matrices and $\\varepsilon$ are:\n- Case A:\n$$\nP_A = \\begin{bmatrix}\n0.95 & 0.05\\\\\n0.05 & 0.95\n\\end{bmatrix},\\quad \\varepsilon_A = 0.01.\n$$\n- Case B:\n$$\nS_B = \\begin{bmatrix}\n0 & \\tfrac{1}{2} & 0 & 0 & \\tfrac{1}{2}\\\\\n\\tfrac{1}{2} & 0 & \\tfrac{1}{2} & 0 & 0\\\\\n0 & \\tfrac{1}{2} & 0 & \\tfrac{1}{2} & 0\\\\\n0 & 0 & \\tfrac{1}{2} & 0 & \\tfrac{1}{2}\\\\\n\\tfrac{1}{2} & 0 & 0 & \\tfrac{1}{2} & 0\n\\end{bmatrix},\\quad\nP_B = \\tfrac{1}{2}I + \\tfrac{1}{2}S_B,\\quad \\varepsilon_B = 0.01.\n$$\n- Case C:\n$$\nP_C = \\begin{bmatrix}\n0.9 & 0.1 & 0.0\\\\\n0.1 & 0.8 & 0.1\\\\\n0.0 & 0.1 & 0.9\n\\end{bmatrix},\\quad \\varepsilon_C = 0.01.\n$$\n- Case D:\n$$\nJ_4 = \\begin{bmatrix}\n1 & 1 & 1 & 1\\\\\n1 & 1 & 1 & 1\\\\\n1 & 1 & 1 & 1\\\\\n1 & 1 & 1 & 1\n\\end{bmatrix},\\quad\nP_D = \\tfrac{1}{2}I + \\tfrac{1}{2}\\tfrac{1}{4}J_4,\\quad \\varepsilon_D = 0.01.\n$$\n\nThe program computes $[\\gamma, t_{\\mathrm{mix}}(\\varepsilon)]$ for each case and prints a single line in the format `[[gamma_A, t_A],[gamma_B, t_B],[gamma_C, t_C],[gamma_D, t_D]]`, with $\\gamma$ rounded to six decimal places and $t$ an integer obtained by ceiling.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef power_method(P, max_iter=10000, tol=1e-12, rng=None):\n    \"\"\"\n    Compute the dominant eigenvector and Rayleigh quotient via the power method.\n    Returns (v1, lambda1).\n    \"\"\"\n    n = P.shape[0]\n    if rng is None:\n        rng = np.random.default_rng(42)\n    x = rng.random(n)\n    x = x / np.linalg.norm(x)\n    prev_rayleigh = None\n    for _ in range(max_iter):\n        y = P @ x\n        ny = np.linalg.norm(y)\n        if ny == 0:\n            # Degenerate case: choose a new random vector\n            x = rng.random(n)\n            x = x / np.linalg.norm(x)\n            continue\n        x = y / ny\n        rayleigh = float(x.T @ (P @ x))\n        if prev_rayleigh is not None and abs(rayleigh - prev_rayleigh) < tol:\n            break\n        prev_rayleigh = rayleigh\n    return x, prev_rayleigh if prev_rayleigh is not None else float(x.T @ (P @ x))\n\ndef deflated_power_method(P, v1, max_iter=10000, tol=1e-12, rng=None):\n    \"\"\"\n    Compute the subdominant eigenvector/eigenvalue by deflating along v1.\n    Returns (v2, lambda2_est).\n    \"\"\"\n    n = P.shape[0]\n    if rng is None:\n        rng = np.random.default_rng(123)\n    # Start with a random vector orthogonal to v1\n    x = rng.random(n)\n    x = x - (v1.T @ x) * v1\n    nx = np.linalg.norm(x)\n    if nx == 0:\n        # If by chance we chose collinear, pick another random\n        x = rng.standard_normal(n)\n        x = x - (v1.T @ x) * v1\n        nx = np.linalg.norm(x)\n        if nx == 0:\n            # Fallback to a fixed vector orthogonal to v1\n            # Construct any vector then subtract projection\n            x = np.ones(n)\n            x = x - (v1.T @ x) * v1\n            nx = np.linalg.norm(x)\n    x = x / nx\n    prev_rayleigh = None\n    for _ in range(max_iter):\n        y = P @ x\n        # Orthogonalize against v1\n        y = y - (v1.T @ y) * v1\n        ny = np.linalg.norm(y)\n        if ny == 0:\n            # Reinitialize if vector collapses\n            x = rng.random(n)\n            x = x - (v1.T @ x) * v1\n            nx = np.linalg.norm(x)\n            if nx == 0:\n                continue\n            x = x / nx\n            prev_rayleigh = None\n            continue\n        x = y / ny\n        rayleigh = float(x.T @ (P @ x))\n        if prev_rayleigh is not None and abs(rayleigh - prev_rayleigh) < tol:\n            break\n        prev_rayleigh = rayleigh\n    lambda2_est = prev_rayleigh if prev_rayleigh is not None else float(x.T @ (P @ x))\n    return x, lambda2_est\n\ndef mixing_time_bound(n, lambda_star, epsilon):\n    \"\"\"\n    Compute the upper bound on mixing time t_mix(epsilon) derived from:\n    ||mu_t - pi||_TV <= (1/2) * sqrt(n) * sqrt(1 - 1/n) * lambda_star^t <= epsilon\n    Solve for t: t >= ln(2*epsilon / (sqrt(n)*sqrt(1 - 1/n))) / ln(lambda_star)\n    Return ceil of the bound; handle edge cases.\n    \"\"\"\n    # Guard against numerical issues when lambda_star is extremely small or zero\n    if lambda_star <= 1e-15:\n        return 1  # immediate contraction\n    numerator = np.log(2.0 * epsilon / (np.sqrt(n) * np.sqrt(1.0 - 1.0 / n)))\n    denominator = np.log(lambda_star)\n    # denominator is negative (since 0 < lambda_star < 1), numerator is negative\n    t = numerator / denominator\n    # Ensure nonnegative\n    t = max(0.0, t)\n    return int(np.ceil(t))\n\ndef build_case_B_matrix():\n    # Five-state cycle random walk S_B, then lazy P_B = 0.5*I + 0.5*S_B\n    n = 5\n    S = np.zeros((n, n))\n    # Each node transitions to its two neighbors with probability 1/2 each\n    for i in range(n):\n        S[(i + 1) % n, i] = 0.5  # to next\n        S[(i - 1) % n, i] = 0.5  # to previous\n    P = 0.5 * np.eye(n) + 0.5 * S\n    return P\n\ndef solve():\n    # Define the test cases from the problem statement.\n    P_A = np.array([[0.95, 0.05],\n                    [0.05, 0.95]], dtype=float)\n    epsilon_A = 0.01\n\n    P_B = build_case_B_matrix()\n    epsilon_B = 0.01\n\n    P_C = np.array([[0.9, 0.1, 0.0],\n                    [0.1, 0.8, 0.1],\n                    [0.0, 0.1, 0.9]], dtype=float)\n    epsilon_C = 0.01\n\n    J4 = np.ones((4, 4), dtype=float)\n    P_D = 0.5 * np.eye(4) + 0.5 * (J4 / 4.0)\n    epsilon_D = 0.01\n\n    test_cases = [\n        (P_A, epsilon_A),\n        (P_B, epsilon_B),\n        (P_C, epsilon_C),\n        (P_D, epsilon_D),\n    ]\n\n    results = []\n    for P, eps in test_cases:\n        n = P.shape[0]\n        # Dominant eigenpair via power method\n        v1, _ = power_method(P, max_iter=10000, tol=1e-12)\n        # Deflated power method to estimate lambda2\n        v2, lambda2 = deflated_power_method(P, v1, max_iter=10000, tol=1e-12)\n        lambda_star = abs(lambda2)\n        gap = 1.0 - lambda_star\n        t_mix = mixing_time_bound(n, lambda_star, eps)\n        # Round gap to 6 decimals\n        gap_rounded = round(gap, 6)\n        results.append([gap_rounded, t_mix])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(str(item) for item in results)}]\")\n\nsolve()\n```", "id": "3283319"}, {"introduction": "The mixing time of a random walk is not just a property of the underlying graph, but also of the specific rules of the walk itself. This advanced exercise guides you through a comparative study of two important models: the simple random walk (SRW) and the non-backtracking random walk (NBRW). By implementing both and analyzing their performance on various graphs, you will gain direct insight into how modifying the walk's dynamics can drastically alter its convergence rate and spectral gap. [@problem_id:3320491]", "problem": "Consider finite, undirected, simple graphs with no self-loops and no multiple edges. Let $A$ denote the adjacency matrix of a graph on $n$ vertices, and let $D$ denote the diagonal degree matrix. The simple random walk transition matrix on vertices is $P_{\\mathrm{srw}} = D^{-1} A$. The nonbacktracking random walk is defined on the state space of directed edges. For an undirected edge $\\{u,v\\}$, the corresponding directed edges are $(u \\to v)$ and $(v \\to u)$. The Hashimoto (nonbacktracking) matrix $B$ is the square matrix indexed by directed edges with entries\n$$\nB_{(u \\to v),(x \\to y)} =\n\\begin{cases}\n1, & \\text{if } v = x \\text{ and } y \\neq u, \\\\\n0, & \\text{otherwise}.\n\\end{cases}\n$$\nThe nonbacktracking random walk transition matrix on directed edges is obtained by normalizing the rows of $B$: for a row corresponding to $(u \\to v)$, each allowed transition $(v \\to w)$ with $w \\neq u$ has probability $1/(d(v)-1)$, where $d(v)$ is the degree of $v$. To ensure aperiodicity and convergence to a stationary distribution, consider the lazy versions of both chains:\n$$\nP_{\\mathrm{srw}}^{\\mathrm{lazy}} = \\frac{I + P_{\\mathrm{srw}}}{2}, \\qquad\nP_{\\mathrm{nbrw}}^{\\mathrm{lazy}} = \\frac{I + P_{\\mathrm{nbrw}}}{2}.\n$$\nFor a finite Markov chain with transition matrix $P$ and stationary distribution $\\pi$, the total variation distance between two distributions $\\mu$ and $\\nu$ on the state space is\n$$\n\\|\\mu - \\nu\\|_{\\mathrm{TV}} = \\frac{1}{2} \\sum_{s} |\\mu(s) - \\nu(s)|.\n$$\nThe $\\epsilon$-mixing time is defined as\n$$\nt_{\\mathrm{mix}}(\\epsilon) = \\min \\left\\{ t \\in \\mathbb{N} : \\max_{s} \\left\\| \\delta_{s} P^{t} - \\pi \\right\\|_{\\mathrm{TV}} \\le \\epsilon \\right\\},\n$$\nwhere $\\delta_{s}$ is the point mass at state $s$. The spectral gap is defined as\n$$\n\\gamma(P) = 1 - \\max_{i \\ge 2} |\\lambda_i(P)|,\n$$\nwhere $\\lambda_i(P)$ are the eigenvalues of $P$ ordered by decreasing magnitude, and $\\lambda_1(P) = 1$ is the largest eigenvalue for a lazy chain.\n\nYour task is to write a complete program that:\n- Constructs $P_{\\mathrm{srw}}^{\\mathrm{lazy}}$ on vertices and $P_{\\mathrm{nbrw}}^{\\mathrm{lazy}}$ on directed edges for each specified graph.\n- Computes the stationary distribution $\\pi$ for each chain using first principles: find the unique probability vector that is a left eigenvector of $P$ with eigenvalue $1$ by iterative multiplication and normalization.\n- Computes $t_{\\mathrm{mix}}(\\epsilon)$ for each given $\\epsilon$ by explicitly evaluating the worst-case total variation distance over all initial states and finding the smallest $t \\le T_{\\max}$ that achieves the bound; if no such $t$ exists up to $T_{\\max}$, return $T_{\\max} + 1$ for that case.\n- Computes the spectral gap $\\gamma(P)$ by evaluating $1 - \\max_{i \\ge 2} |\\lambda_i(P)|$.\n\nUse the following test suite of graphs and parameters. For each graph $G_k$, define the vertex set $\\{0,1,\\dots,n-1\\}$ and the undirected edge set $E$:\n\n- Graph $G_1$: cycle on $n = 4$ vertices with edges $E = \\{(0,1),(1,2),(2,3),(3,0)\\}$.\n- Graph $G_2$: cycle on $n = 5$ vertices with edges $E = \\{(0,1),(1,2),(2,3),(3,4),(4,0)\\}$.\n- Graph $G_3$: complete graph on $n = 5$ vertices with edges $E = \\{(i,j) : 0 \\le i < j \\le 4\\}$.\n\nFor each graph, use two accuracy thresholds $\\epsilon \\in \\{0.05, 0.01\\}$ and a maximum time horizon $T_{\\max} = 200$. For each graph $G_k$, your program must produce a list of $6$ numbers in the following order:\n- $t_{\\mathrm{mix}}^{\\mathrm{srw}}(0.05)$ as an integer,\n- $t_{\\mathrm{mix}}^{\\mathrm{nbrw}}(0.05)$ as an integer,\n- $t_{\\mathrm{mix}}^{\\mathrm{srw}}(0.01)$ as an integer,\n- $t_{\\mathrm{mix}}^{\\mathrm{nbrw}}(0.01)$ as an integer,\n- $\\gamma(P_{\\mathrm{srw}}^{\\mathrm{lazy}})$ as a float rounded to $6$ decimal places,\n- $\\gamma(P_{\\mathrm{nbrw}}^{\\mathrm{lazy}})$ as a float rounded to $6$ decimal places.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each item corresponds to one graph $G_k$ and is itself a list as specified above. For example, the output must look like\n`[[result_1],[result_2],[result_3]]`\nwith the actual numeric values in place of the placeholders.\n\nNo physical units or angles are involved in this task. All answers are expressed as integers or floats. The program must not read any input and must run self-contained.", "solution": "The problem statement is a valid and well-posed exercise in computational Markov chain theory. It requires the construction and analysis of two types of random walks—the simple random walk (SRW) and the non-backtracking random walk (NBRW)—on specified graphs. For each walk, we are tasked with computing its stationary distribution, mixing time for given precision levels, and spectral gap. All definitions and required computations are standard within the fields of stochastic processes and theoretical computer science. The provided test cases consist of small, finite graphs for which the required calculations are computationally feasible. The problem is scientifically grounded, objective, and self-contained.\n\nThe methodology for solving this problem involves several distinct steps for each graph provided in the test suite.\n\n**1. Graph Representation**\nFor each graph $G=(V, E)$ with $n = |V|$ vertices, we first construct its $n \\times n$ adjacency matrix $A$, where $A_{ij} = 1$ if an edge $\\{i,j\\} \\in E$ and $A_{ij} = 0$ otherwise. We also compute the degree of each vertex, $d(v) = \\sum_{j} A_{vj}$, which forms the diagonal entries of the degree matrix $D$.\n\n**2. Simple Random Walk (SRW) Analysis**\nThe SRW is a Markov chain on the vertex set $V$.\n- **Transition Matrix:** The standard SRW transition matrix is $P_{\\mathrm{srw}} = D^{-1}A$. To ensure aperiodicity, which guarantees convergence to a unique stationary distribution, we use the lazy version of the chain, defined as $P_{\\mathrm{srw}}^{\\mathrm{lazy}} = \\frac{1}{2}(I + P_{\\mathrm{srw}})$, where $I$ is the identity matrix.\n- **Stationary Distribution ($\\pi_{\\mathrm{srw}}$):** For an undirected, connected graph, the stationary distribution of the SRW is given by $\\pi_{\\mathrm{srw}}(v) = \\frac{d(v)}{2|E|}$, where $|E|$ is the total number of edges. We will compute this distribution numerically using the power method, as specified: starting with an initial probability vector $\\pi^{(0)}$, we iterate $\\pi^{(k+1)} = \\pi^{(k)} P_{\\mathrm{srw}}^{\\mathrm{lazy}}$ and normalize until convergence.\n\n**3. Non-Backtracking Random Walk (NBRW) Analysis**\nThe NBRW is a Markov chain on the set of directed edges of the graph. For each undirected edge $\\{u,v\\}$, there are two directed edges, $(u \\to v)$ and $(v \\to u)$. The size of the state space is $M = 2|E|$.\n- **Transition Matrix:** The NBRW from state $(u \\to v)$ can only transition to a state $(v \\to w)$ where $w$ is a neighbor of $v$ and $w \\neq u$. The probability of each such transition is uniform, $1/(d(v)-1)$, assuming $d(v) > 1$. This defines the transition matrix $P_{\\mathrm{nbrw}}$. The lazy version is $P_{\\mathrm{nbrw}}^{\\mathrm{lazy}} = \\frac{1}{2}(I + P_{\\mathrm{nbrw}})$.\n- **Stationary Distribution ($\\pi_{\\mathrm{nbrw}}$):** The stationary distribution for the NBRW on a connected, non-bipartite regular graph is uniform over all $2|E|$ directed edges: $\\pi_{\\mathrm{nbrw}}(u \\to v) = \\frac{1}{2|E|}$. This will also be computed numerically via the power method.\n\n**4. Performance Metrics Calculation**\nFor both the SRW and NBRW lazy chains, we compute the following metrics:\n- **$\\epsilon$-Mixing Time ($t_{\\mathrm{mix}}(\\epsilon)$):** This is the first time step $t$ at which the distribution of the chain, starting from the worst possible initial state, is within $\\epsilon$ of the stationary distribution $\\pi$ in total variation distance. It is calculated as $t_{\\mathrm{mix}}(\\epsilon) = \\min \\{ t \\in \\mathbb{N} : \\max_{s} \\|\\delta_s P^t - \\pi\\|_{\\mathrm{TV}} \\le \\epsilon \\}$. We compute this by iterating $t$ from $1$ to a maximum horizon $T_{\\max}$, calculating the matrix power $P^t$, and evaluating the maximum total variation distance over all starting states $s$. The distribution after $t$ steps starting from state $s$ is the $s$-th row of $P^t$.\n- **Spectral Gap ($\\gamma$):** The spectral gap of a lazy transition matrix $P$ is defined as $\\gamma(P) = 1 - \\lambda_2^*$, where $\\lambda_2^* = \\max_{i \\ge 2} |\\lambda_i(P)|$ and $\\lambda_i(P)$ are the eigenvalues of $P$. Since the chain is lazy and the graph is connected, $\\lambda_1(P) = 1$ is the unique eigenvalue of magnitude $1$. The remaining eigenvalues determine the rate of convergence to the stationary distribution. The eigenvalues will be computed numerically using standard linear algebra routines.\n\nThe entire procedure is implemented for each of the three specified graphs ($C_4$, $C_5$, and $K_5$) using the given parameters $\\epsilon \\in \\{0.05, 0.01\\}$ and $T_{\\max} = 200$. The final results are then collated and formatted as required.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef compute_pi(P):\n    \"\"\"\n    Computes the stationary distribution of a Markov chain with transition matrix P\n    using the power method.\n    \"\"\"\n    n_states = P.shape[0]\n    pi = np.ones(n_states) / n_states\n    for _ in range(1000):  # Iterate to converge\n        pi_new = pi @ P\n        if np.allclose(pi_new, pi):\n            break\n        pi = pi_new\n    pi /= np.sum(pi)  # Normalize to ensure it's a probability distribution\n    return pi\n\ndef compute_tmix(P, pi, eps, T_max):\n    \"\"\"\n    Computes the epsilon-mixing time for a Markov chain.\n    \"\"\"\n    n_states = P.shape[0]\n    P_t = np.copy(P)\n    for t in range(1, T_max + 1):\n        max_tv_dist = 0.0\n        for s in range(n_states):\n            # The s-th row of P_t is the distribution after t steps starting from s.\n            dist_t = P_t[s, :]\n            tv_dist = 0.5 * np.sum(np.abs(dist_t - pi))\n            if tv_dist > max_tv_dist:\n                max_tv_dist = tv_dist\n        \n        if max_tv_dist <= eps:\n            return t\n        \n        if t < T_max:\n            P_t = P_t @ P\n\n    return T_max + 1\n\ndef compute_gamma(P):\n    \"\"\"\n    Computes the spectral gap of a transition matrix P.\n    \"\"\"\n    eigvals = np.linalg.eigvals(P)\n    eig_mags = np.abs(eigvals)\n    # Sort magnitudes in descending order\n    sorted_mags = np.sort(eig_mags)[::-1]\n    # The largest is 1. The second largest is at index 1.\n    second_largest_mag = sorted_mags[1]\n    gamma = 1 - second_largest_mag\n    return gamma\n\ndef analyze_graph(n, edges, epsilons, T_max):\n    \"\"\"\n    Analyzes a graph for both SRW and NBRW, computing metrics.\n    \"\"\"\n    # 1. Common graph properties\n    A = np.zeros((n, n), dtype=float)\n    for u, v in edges:\n        A[u, v] = 1\n        A[v, u] = 1\n    degrees = A.sum(axis=1)\n\n    # 2. SRW Analysis\n    D_inv = np.diag(1.0 / degrees)\n    P_srw = D_inv @ A\n    P_srw_lazy = 0.5 * (np.eye(n) + P_srw)\n    \n    pi_srw = compute_pi(P_srw_lazy)\n    tmix_srw_eps1 = compute_tmix(P_srw_lazy, pi_srw, epsilons[0], T_max)\n    tmix_srw_eps2 = compute_tmix(P_srw_lazy, pi_srw, epsilons[1], T_max)\n    gamma_srw = compute_gamma(P_srw_lazy)\n\n    # 3. NBRW Analysis\n    directed_edges = []\n    for u, v in edges:\n        directed_edges.append((u, v))\n        directed_edges.append((v, u))\n    \n    M = len(directed_edges)\n    edge_to_idx = {edge: i for i, edge in enumerate(directed_edges)}\n    \n    P_nbrw = np.zeros((M, M), dtype=float)\n    for i, (u, v) in enumerate(directed_edges):\n        deg_v = degrees[v]\n        if deg_v > 1:\n            norm = 1.0 / (deg_v - 1)\n            # Find neighbors of v\n            for w in range(n):\n                if A[v, w] == 1 and w != u:\n                    j = edge_to_idx[(v, w)]\n                    P_nbrw[i, j] = norm\n\n    P_nbrw_lazy = 0.5 * (np.eye(M) + P_nbrw)\n    \n    pi_nbrw = compute_pi(P_nbrw_lazy)\n    tmix_nbrw_eps1 = compute_tmix(P_nbrw_lazy, pi_nbrw, epsilons[0], T_max)\n    tmix_nbrw_eps2 = compute_tmix(P_nbrw_lazy, pi_nbrw, epsilons[1], T_max)\n    gamma_nbrw = compute_gamma(P_nbrw_lazy)\n\n    return [\n        tmix_srw_eps1,\n        tmix_nbrw_eps1,\n        tmix_srw_eps2,\n        tmix_nbrw_eps2,\n        round(gamma_srw, 6),\n        round(gamma_nbrw, 6)\n    ]\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"n\": 4, \n            \"edges\": [(0, 1), (1, 2), (2, 3), (3, 0)],\n            \"epsilons\": [0.05, 0.01],\n            \"T_max\": 200\n        },\n        {\n            \"n\": 5, \n            \"edges\": [(0, 1), (1, 2), (2, 3), (3, 4), (4, 0)],\n            \"epsilons\": [0.05, 0.01],\n            \"T_max\": 200\n        },\n        {\n            \"n\": 5, \n            \"edges\": [(i, j) for i in range(5) for j in range(i + 1, 5)],\n            \"epsilons\": [0.05, 0.01],\n            \"T_max\": 200\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        results = analyze_graph(case[\"n\"], case[\"edges\"], case[\"epsilons\"], case[\"T_max\"])\n        all_results.append(results)\n\n    # Format the final output\n    def format_list(lst):\n        items = []\n        for item in lst:\n            if isinstance(item, float):\n                items.append(f\"{item:.6f}\")\n            else:\n                items.append(str(item))\n        return f\"[{','.join(items)}]\"\n\n    output_str = f\"[{','.join(map(format_list, all_results))}]\"\n    print(output_str)\n\n# Run the solver\nsolve()\n```", "id": "3320491"}]}