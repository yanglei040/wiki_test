{"hands_on_practices": [{"introduction": "The PageRank algorithm is a cornerstone application of stationary distributions in a massive, real-world context. This exercise moves from theory to practice by having you simulate the convergence of the PageRank vector for various web graph structures. By implementing the power iteration and varying parameters like the teleportation probability $\\alpha$, you will gain concrete intuition for how the spectral properties of the transition matrix, influenced by graph structure and algorithmic choices, dictate the speed of convergence [@problem_id:3300074].", "problem": "Consider the Google PageRank formulation as the stationary distribution of a Markov chain on a finite directed graph with teleportation. Let $n \\in \\mathbb{N}$ denote the number of nodes, let $W \\in \\{0,1\\}^{n \\times n}$ be the adjacency matrix defined by columns as follows: $W_{ij} = 1$ if there is a directed edge from node $j$ to node $i$, and $W_{ij} = 0$ otherwise. Define the teleportation distribution $v \\in \\mathbb{R}^n$ to be the uniform probability vector, $v_i = 1/n$ for all $i \\in \\{1,\\dots,n\\}$. Define the column-stochastic matrix $S \\in \\mathbb{R}^{n \\times n}$ by normalizing each column of $W$; if a column $j$ has zero sum (a dangling node), set $S_{\\cdot j} = v$. For a teleportation probability $\\alpha \\in [0,1)$, consider the PageRank iteration\n$$\nx_{k+1} \\;=\\; \\alpha S x_k \\;+\\; (1 - \\alpha)\\, v,\\quad k = 0,1,2,\\dots,\n$$\nwith initial distribution $x_0 = v$. The stationary distribution $x^\\star \\in \\mathbb{R}^n$ is the unique solution to\n$$\nx^\\star \\;=\\; \\alpha S x^\\star \\;+\\; (1-\\alpha)\\, v, \\qquad x^\\star \\ge 0, \\;\\; \\sum_{i=1}^n x^\\star_i = 1,\n$$\nequivalently,\n$$\n(I - \\alpha S)\\, x^\\star \\;=\\; (1-\\alpha)\\, v.\n$$\nUse the total variation distance $d_{\\mathrm{TV}}(p,q) = \\tfrac{1}{2}\\,\\lVert p - q\\rVert_1$ to quantify convergence. The goal is to simulate the convergence to $x^\\star$ by power iteration and to quantify how the convergence speed depends on $\\alpha$ and the presence of dangling nodes.\n\nStarting from the fundamental facts that (i) $S$ is column-stochastic by construction, (ii) $x^\\star$ exists and is unique for any $\\alpha \\in [0,1)$, and (iii) the iteration is linear, design and implement a program that, for each test case below, performs the following tasks:\n1. Construct $S$ from $W$ and $v$ as defined above.\n2. Compute the exact stationary distribution $x^\\star$ by solving $(I - \\alpha S)\\,x^\\star = (1-\\alpha) v$ and normalizing to a probability vector.\n3. Run the power iteration $x_{k+1} = \\alpha S x_k + (1 - \\alpha) v$ from $x_0 = v$ until the stopping criterion $d_{\\mathrm{TV}}(x_k, x^\\star) \\le \\varepsilon$ is met, or until a maximum of $K_{\\max} = 100000$ iterations is reached. Use $\\varepsilon = 10^{-10}$ for all cases.\n4. Record $K$, the number of iterations performed when the stopping criterion is first met (if the initial $x_0$ already satisfies the criterion, then $K=0$). If the stopping criterion is not met within $K_{\\max}$ iterations, set $K=K_{\\max}$.\n5. Estimate an empirical linear convergence rate $r_{\\mathrm{hat}}$ from the sequence of $\\ell_1$-errors $e_k = \\lVert x_k - x^\\star \\rVert_1$ by computing the ratios $e_k/e_{k-1}$ for $k \\ge 1$ over the run and reporting the median of the last $\\min\\{50, \\text{number of ratios}\\}$ ratios. If fewer than two iterates are available, set $r_{\\mathrm{hat}}=0.0$. This $r_{\\mathrm{hat}}$ approximates the asymptotic contraction factor on the zero-sum subspace and reflects sensitivity to $\\alpha$ and dangling nodes.\n\nYou must implement the above for the following test suite of graphs and parameters. Each graph is specified by its adjacency matrix $W$ with the column-based convention described above, and each test case uses $v$ uniform and $\\varepsilon = 10^{-10}$:\n\n- Test case 1 (no dangling nodes, moderate teleportation): $n=4$, $\\alpha=0.85$, \n$$\nW = \\begin{bmatrix}\n0  0  0  1 \\\\\n1  0  0  0 \\\\\n0  1  0  0 \\\\\n0  0  1  0\n\\end{bmatrix}.\n$$\n- Test case 2 (no dangling nodes, slow mixing): $n=4$, $\\alpha=0.99$, same $W$ as in test case 1.\n- Test case 3 (one dangling node, moderate teleportation): $n=3$, $\\alpha=0.85$,\n$$\nW = \\begin{bmatrix}\n0  1  0 \\\\\n1  0  0 \\\\\n0  0  0\n\\end{bmatrix}.\n$$\n- Test case 4 (one dangling node, slow mixing): $n=3$, $\\alpha=0.99$, same $W$ as in test case 3.\n- Test case 5 (reducible graph without dangling nodes, moderate teleportation): $n=4$, $\\alpha=0.90$,\n$$\nW = \\begin{bmatrix}\n0  1  0  0 \\\\\n1  0  0  0 \\\\\n0  0  0  1 \\\\\n0  0  1  0\n\\end{bmatrix}.\n$$\n- Test case 6 (teleportation dominates): $n=4$, $\\alpha=0.00$, same $W$ as in test case 1.\n\nYour program must produce a single line of output containing the results for the six test cases as a comma-separated list enclosed in square brackets. Each test case result must be a two-element list $[K, r_{\\mathrm{hat}}]$ in that order. Thus, the output format must be exactly\n\"[ [K1,rhat1],[K2,rhat2],[K3,rhat3],[K4,rhat4],[K5,rhat5],[K6,rhat6] ]\"\nwith no spaces required beyond those shown and with floating-point numbers expressed in decimal or scientific notation. No physical units or angles are involved, and no percentages appear in the output. All computations are purely numerical and dimensionless.", "solution": "The problem statement has been critically validated and is deemed sound. It is scientifically grounded in the theory of Markov chains and numerical linear algebra, specifically the PageRank algorithm. The problem is well-posed, with all necessary data, definitions, and constraints provided to ensure a unique, stable, and meaningful solution. The objectives are clear and expressed in precise mathematical language. We may therefore proceed with the solution.\n\nThe problem requires the implementation and analysis of the PageRank algorithm for a set of specified graphs and parameters. The solution involves five main tasks for each test case:\n1.  Constructing the Google matrix component $S$.\n2.  Computing the exact stationary PageRank vector $x^\\star$.\n3.  Simulating the convergence to $x^\\star$ using power iteration.\n4.  Determining the number of iterations $K$ required to reach a specified tolerance.\n5.  Estimating the empirical convergence rate $r_{\\mathrm{hat}}$.\n\nWe will now detail the methodology for each of these tasks.\n\n**1. Construction of the Stochastic Matrix $S$**\n\nThe matrix $S \\in \\mathbb{R}^{n \\times n}$ is derived from the graph's adjacency matrix $W \\in \\{0,1\\}^{n \\times n}$. The problem specifies a column-based convention for $W$, where $W_{ij} = 1$ indicates a directed edge from node $j$ to node $i$. $S$ is a column-stochastic matrix constructed as follows:\n\nFor each column $j \\in \\{1, \\dots, n\\}$ of $W$:\n- Let $c_j = \\sum_{i=1}^n W_{ij}$ be the sum of the elements in column $j$, which corresponds to the out-degree of node $j$.\n- If $c_j  0$, the node $j$ has outgoing links. The corresponding column $S_{\\cdot j}$ of $S$ is obtained by normalizing the column $W_{\\cdot j}$:\n$$S_{ij} = \\frac{W_{ij}}{c_j} \\quad \\text{for } i=1, \\dots, n.$$\n- If $c_j = 0$, node $j$ is a \"dangling node\" with no outgoing links. To ensure that the resulting matrix is stochastic (i.e., its columns sum to $1$), the column $S_{\\cdot j}$ is set to the uniform teleportation vector $v \\in \\mathbb{R}^n$, where $v_i = 1/n$ for all $i$.\n$$S_{\\cdot j} = v = \\begin{bmatrix} 1/n  1/n  \\dots  1/n \\end{bmatrix}^T.$$\nBy this construction, every column of $S$ is non-negative and sums to $1$, making $S$ a column-stochastic matrix.\n\n**2. Computation of the Exact Stationary Distribution $x^\\star$**\n\nThe stationary distribution, or PageRank vector, $x^\\star$, is the unique probability vector that is a fixed point of the PageRank iteration. It satisfies the equation:\n$$x^\\star = \\alpha S x^\\star + (1-\\alpha)v$$\nwhere $\\alpha \\in [0,1)$ is the teleportation probability. This equation can be rearranged into a standard linear system:\n$$I x^\\star - \\alpha S x^\\star = (1-\\alpha)v$$\n$$(I - \\alpha S) x^\\star = (1-\\alpha)v$$\nwhere $I$ is the $n \\times n$ identity matrix.\n\nThe existence and uniqueness of the solution $x^\\star$ are guaranteed for $\\alpha \\in [0,1)$. The matrix $S$ is stochastic, so its spectral radius (the maximum modulus of its eigenvalues) is $\\rho(S)=1$. The spectral radius of $\\alpha S$ is $\\rho(\\alpha S) = \\alpha \\rho(S) = \\alpha$. Since $\\alpha  1$, all eigenvalues of $\\alpha S$ are strictly inside the unit circle in the complex plane. This ensures that $1$ is not an eigenvalue of $\\alpha S$, and thus the matrix $(I - \\alpha S)$ is invertible.\n\nThe unique solution $x^\\star$ can be computed by solving this linear system. While the problem statement suggests normalizing the result, the solution to this system is guaranteed to be a probability vector (its components sum to $1$), so normalization is redundant but harmless.\n\n**3. Power Iteration Simulation**\n\nThe PageRank vector $x^\\star$ is computed iteratively using the power method. The iteration is defined as:\n$$x_{k+1} = \\alpha S x_k + (1-\\alpha)v$$\nstarting with an initial distribution $x_0 = v$. The process continues until the iterate $x_k$ is sufficiently close to the true stationary distribution $x^\\star$. Convergence is measured using the total variation distance, $d_{\\mathrm{TV}}(p,q) = \\frac{1}{2} \\lVert p - q \\rVert_1$. The iteration stops when the condition $d_{\\mathrm{TV}}(x_k, x^\\star) \\le \\varepsilon$ is met, for a given tolerance $\\varepsilon = 10^{-10}$. The number of iterations, $K$, required to meet this criterion is recorded. If the criterion is not met within a maximum of $K_{\\max} = 100000$ iterations, $K$ is set to $K_{\\max}$. If the initial vector $x_0$ already satisfies the criterion, $K=0$.\n\n**4. Estimation of Empirical Convergence Rate $r_{\\mathrm{hat}}$**\n\nThe convergence of the power iteration is linear. The error vector at step $k$ is $e_k = x_k - x^\\star$. The error propagates as:\n$$e_{k+1} = x_{k+1} - x^\\star = (\\alpha S x_k + (1-\\alpha)v) - (\\alpha S x^\\star + (1-\\alpha)v) = \\alpha S (x_k - x^\\star) = \\alpha S e_k.$$\nTaking the $\\ell_1$-norm, we get $\\lVert e_{k+1} \\rVert_1 = \\alpha \\lVert S e_k \\rVert_1$. The asymptotic rate of convergence of the $\\ell_1$-error, $\\lVert e_k \\rVert_1$, is determined by the action of the operator $\\alpha S$ on the subspace of zero-sum vectors (since $x_k$ and $x^\\star$ are probability vectors, their difference $e_k$ sums to zero). The theoretical rate is related to $\\alpha$ and the spectral properties of $S$.\n\nThe problem requires an *empirical* estimate of this rate, $r_{\\mathrm{hat}}$. This is calculated from the sequence of $\\ell_1$-errors, $e_k = \\lVert x_k - x^\\star \\rVert_1$, generated during the power iteration. The ratio $e_k/e_{k-1}$ for $k \\ge 1$ approximates the per-step reduction in error. To obtain a stable estimate of the asymptotic rate, we compute the median of the last $\\min\\{50, K\\}$ of these ratios, where $K$ is the total number of iterations performed. If fewer than two iterates are generated (i.e., $K=0$), implying less than one ratio can be computed, $r_{\\mathrm{hat}}$ is set to $0.0$.\n\nThe following program implements this entire procedure for the six specified test cases, adhering strictly to the methodology described.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the PageRank convergence problem for all test cases.\n    \"\"\"\n\n    def solve_one_case(n, alpha, W, epsilon, k_max):\n        \"\"\"\n        Solves a single test case of the PageRank problem.\n\n        Args:\n            n (int): Number of nodes.\n            alpha (float): Teleportation probability.\n            W (np.ndarray): Adjacency matrix (column-based).\n            epsilon (float): Convergence tolerance for total variation distance.\n            k_max (int): Maximum number of iterations.\n\n        Returns:\n            list: A list containing [K, r_hat], the number of iterations and the\n                  empirical convergence rate.\n        \"\"\"\n        # 1. Construct the stochastic matrix S\n        v = np.ones((n, 1)) / n\n        S = np.zeros((n, n), dtype=np.float64)\n        col_sums = np.sum(W, axis=0)\n        for j in range(n):\n            if col_sums[j] == 0:\n                S[:, j] = v.flatten()\n            else:\n                S[:, j] = W[:, j] / col_sums[j]\n\n        # 2. Compute the exact stationary distribution x_star\n        I = np.identity(n, dtype=np.float64)\n        A = I - alpha * S\n        b = (1 - alpha) * v\n        x_star = np.linalg.solve(A, b)\n        # Normalization is generally redundant but specified as a safeguard\n        x_star /= np.sum(x_star)\n\n        # 3.  4. Run power iteration and find K\n        x_k = v.copy()\n        errors_l1 = []\n        K = 0\n\n        # Loop from k=0 to k_max\n        for k_iter in range(k_max + 1):\n            e_k = np.linalg.norm(x_k - x_star, ord=1)\n            errors_l1.append(e_k)\n            d_tv = 0.5 * e_k\n\n            if d_tv = epsilon:\n                K = k_iter\n                break\n            \n            if k_iter == k_max:\n                K = k_max\n                break\n\n            # Update for next iteration\n            x_k = alpha * S @ x_k + (1 - alpha) * v\n\n        # 5. Estimate empirical convergence rate r_hat\n        if len(errors_l1)  2:  # K=0 case, not enough data for a ratio\n            r_hat = 0.0\n        else:\n            # Calculate ratios e_k / e_{k-1}\n            ratios = [errors_l1[i] / errors_l1[i-1] \n                      for i in range(1, len(errors_l1)) if errors_l1[i-1] > 0]\n\n            if not ratios:\n                r_hat = 0.0\n            else:\n                num_ratios_to_consider = min(50, len(ratios))\n                last_ratios = ratios[-num_ratios_to_consider:]\n                r_hat = np.median(last_ratios)\n\n        return [K, r_hat]\n\n    # --- Test Case Definitions ---\n    # Epsilon and K_max are common for all cases\n    epsilon = 1e-10\n    k_max = 100000\n\n    test_cases = [\n        # Case 1: n=4, alpha=0.85, W for a 4-cycle\n        (4, 0.85, np.array([[0, 0, 0, 1], [1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0]], dtype=np.float64)),\n        # Case 2: n=4, alpha=0.99, W for a 4-cycle (slower mixing)\n        (4, 0.99, np.array([[0, 0, 0, 1], [1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0]], dtype=np.float64)),\n        # Case 3: n=3, alpha=0.85, W with one dangling node\n        (3, 0.85, np.array([[0, 1, 0], [1, 0, 0], [0, 0, 0]], dtype=np.float64)),\n        # Case 4: n=3, alpha=0.99, W with one dangling node (slower mixing)\n        (3, 0.99, np.array([[0, 1, 0], [1, 0, 0], [0, 0, 0]], dtype=np.float64)),\n        # Case 5: n=4, alpha=0.90, reducible graph (two 2-cycles)\n        (4, 0.90, np.array([[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]], dtype=np.float64)),\n        # Case 6: n=4, alpha=0.00, teleportation only\n        (4, 0.00, np.array([[0, 0, 0, 1], [1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0]], dtype=np.float64)),\n    ]\n\n    results = []\n    for n, alpha, W in test_cases:\n        result = solve_one_case(n, alpha, W, epsilon, k_max)\n        results.append(result)\n\n    # Format the final output string exactly as required.\n    # The template `print(f\"[{','.join(map(str, results))}]\")` will produce a string like\n    # '[[71, 0.85],[498, 0.99]]' which is a standard compact representation.\n    print(f\"[{','.join(map(str, results))}]\".replace(\" \", \"\"))\n\nsolve()\n```", "id": "3300074"}, {"introduction": "When we state that a Markov chain converges, the choice of metric is not merely a technical detail; it has profound practical consequences. This problem presents a classic example of a chain whose distributions converge in the $1$-Wasserstein metric ($W_1$) but fail to converge in the stronger total variation (TV) metric. By analyzing this system, you will explore a scenario where the law of the iterates remains discrete while the stationary distribution is continuous, a situation that guarantees the failure of TV convergence [@problem_id:3300048]. This practice will sharpen your understanding of different modes of convergence and why they dictate which types of expectations can be reliably estimated from a simulation.", "problem": "Consider the time-homogeneous Markov chain $(X_n)_{n \\ge 0}$ on the compact metric space $([0,1], d)$ with $d(x,y) = |x - y|$, defined by the random iterated function system\n$$\nX_{n+1} \\in \\left\\{ \\tfrac{1}{3} X_n,\\ \\tfrac{2 + X_n}{3} \\right\\} \\quad \\text{with equal probability } \\tfrac{1}{2}, \\text{ independently over time.}\n$$\nLet $P$ denote its Markov transition kernel and let $\\pi$ denote a stationary distribution if it exists. Let $\\|\\cdot\\|_{\\mathrm{TV}}$ denote the total variation distance and $W_1$ denote the $1$-Wasserstein distance on probability measures on $([0,1], d)$.\n\nSelect all statements that are true.\n\nA. The Markov operator is a contraction in $W_1$ with modulus strictly less than $1$, so there exists a unique stationary distribution $\\pi$ and, for any initial distribution $\\mu$, one has\n$$\nW_1(\\mu P^n, \\pi) \\le \\left(\\tfrac{1}{3}\\right)^n W_1(\\mu, \\pi) \\quad \\text{for all } n \\ge 0.\n$$\n\nB. For any $x \\in [0,1]$, the sequence of laws $P^n(x, \\cdot)$ does not converge to $\\pi$ in total variation; in fact,\n$$\n\\|P^n(x, \\cdot) - \\pi\\|_{\\mathrm{TV}} = 1 \\quad \\text{for all } n \\ge 1.\n$$\n\nC. For any bounded Lipschitz function $f \\colon [0,1] \\to \\mathbb{R}$ with Lipschitz constant $\\mathrm{Lip}(f)$, the bias decays at the $W_1$-rate:\n$$\n\\big|\\mathbb{E}_x[f(X_n)] - \\pi(f)\\big| \\le \\mathrm{Lip}(f)\\, W_1\\!\\big(P^n(x, \\cdot), \\pi\\big) \\quad \\text{for all } n \\ge 0,\n$$\nso in particular $\\mathbb{E}_x[f(X_n)] \\to \\pi(f)$ as $n \\to \\infty$.\n\nD. For any bounded measurable function $f$,\n$$\n\\big|\\mathbb{E}_x[f(X_n)] - \\pi(f)\\big| \\le \\|f\\|_\\infty\\, W_1\\!\\big(P^n(x, \\cdot), \\pi\\big) \\quad \\text{for all } n \\ge 0,\n$$\nso $W_1$-convergence implies uniform control of bias over all bounded measurable $f$.\n\nE. If one simulates the chain and forms time averages $n^{-1}\\sum_{k=1}^n f(X_k)$, then for any indicator function $f = \\mathbf{1}_A$ with $\\pi(\\partial A) = 0$, one has the almost sure convergence\n$$\n\\frac{1}{n}\\sum_{k=1}^n \\mathbf{1}_A(X_k) \\longrightarrow \\pi(A) \\quad \\text{as } n \\to \\infty,\n$$\nfor any starting point $X_0 = x \\in [0,1]$.\n\nChoose all that apply.", "solution": "The objective is to determine which of the provided statements about the given Markov chain are true. This chain is a classic example of an iterated function system (IFS) whose unique stationary distribution is the Cantor distribution.\n\n**A. Correct.** The two functions defining the IFS, $f_1(x) = \\frac{1}{3}x$ and $f_2(x) = \\frac{2+x}{3}$, are both contractions on $[0,1]$ with a Lipschitz constant of $\\frac{1}{3}$. A standard argument using a synchronous coupling shows that the Markov operator $P$ contracts the $1$-Wasserstein distance: $W_1(\\mu_1 P, \\mu_2 P) \\le \\frac{1}{3} W_1(\\mu_1, \\mu_2)$ for any two probability measures $\\mu_1, \\mu_2$. Since the space of probability measures on $[0,1]$ is a complete metric space under $W_1$, the Banach fixed-point theorem guarantees the existence of a unique fixed point, which is the stationary distribution $\\pi$. Applying the contraction property iteratively with one measure being the stationary distribution ($\\pi P = \\pi$) gives the stated bound.\n\n**B. Correct.** For any starting point $x \\in [0,1]$, the law of $X_n$, denoted $P^n(x, \\cdot)$, is a discrete probability measure supported on at most $2^n$ points. In contrast, the stationary distribution $\\pi$ (the standard Cantor distribution) is a non-atomic (or continuous) measure, meaning $\\pi(\\{y\\})=0$ for any single point $y$. The total variation distance between a discrete measure $\\mu_d$ and a non-atomic measure $\\mu_c$ is always 1. This can be seen by choosing the set $A$ to be the support of the discrete measure $\\mu_d$; then $\\|\\mu_d - \\mu_c\\|_{\\mathrm{TV}} \\ge |\\mu_d(A) - \\mu_c(A)| = |1 - 0| = 1$. Since the TV distance cannot exceed 1, it must be exactly 1. Thus, the chain fails to converge in total variation.\n\n**C. Correct.** This statement is a direct consequence of the Kantorovich-Rubinstein duality theorem for the $1$-Wasserstein distance, which states that $W_1(\\mu, \\nu) = \\sup_{g: \\mathrm{Lip}(g) \\le 1} |\\int g \\, d\\mu - \\int g \\, d\\nu|$. For any function $f$ with Lipschitz constant $\\mathrm{Lip}(f)$, this implies $|\\int f \\, d\\mu - \\int f \\, d\\nu| \\le \\mathrm{Lip}(f) W_1(\\mu, \\nu)$. Let $\\mu = P^n(x, \\cdot)$ and $\\nu = \\pi$. This gives the inequality $|\\mathbb{E}_x[f(X_n)] - \\pi(f)| \\le \\mathrm{Lip}(f) W_1(P^n(x, \\cdot), \\pi)$. As established in A, $W_1(P^n(x, \\cdot), \\pi)$ converges to zero, so the bias also converges to zero for all bounded Lipschitz functions.\n\n**D. Incorrect.** Convergence in the $W_1$ metric is equivalent to weak convergence of probability measures plus convergence of the first moments. This type of convergence does not imply uniform control over all bounded measurable functions. The error for general bounded measurable functions is controlled by the total variation distance via the inequality $|\\int f \\, d\\mu - \\int f \\, d\\nu| \\le \\|f\\|_\\infty \\|\\mu - \\nu\\|_{\\mathrm{TV}}$. As established in B, the TV distance remains 1 for all $n \\ge 1$. This means there must exist bounded measurable functions (e.g., the indicator function for the support of $P^n(x,\\cdot)$) for which the bias does not converge to zero in the manner suggested by the fallacious inequality.\n\n**E. Correct.** This statement pertains to the Strong Law of Large Numbers (SLLN), or Ergodic Theorem, for the Markov chain. The chain's strong contractive property in the $W_1$ metric ensures it is ergodic. This implies that for any starting point $X_0=x$ and any function $f \\in L^1(\\pi)$ (which includes all bounded functions like indicators), the time averages converge almost surely to the spatial average: $\\frac{1}{n}\\sum_{k=1}^n f(X_k) \\to \\int f d\\pi = \\pi(A)$ for $f = \\mathbf{1}_A$. The condition $\\pi(\\partial A)=0$ is a standard condition for ensuring weak convergence of measures of a set, but the SLLN is a stronger result that holds for the indicator of any measurable set $A$. Therefore, the statement is true.", "answer": "$$\\boxed{ABCE}$$", "id": "3300024"}, {"introduction": "When we state that a Markov chain converges, the choice of metric is not merely a technical detail; it has profound practical consequences. This problem presents a classic example of a chain whose distributions converge in the $1$-Wasserstein metric ($W_1$) but fail to converge in the stronger total variation (TV) metric. By analyzing this system, you will explore a scenario where the law of the iterates remains discrete while the stationary distribution is continuous, a situation that guarantees the failure of TV convergence [@problem_id:3300048]. This practice will sharpen your understanding of different modes of convergence and why they dictate which types of expectations can be reliably estimated from a simulation.", "problem": "Consider the time-homogeneous Markov chain $(X_n)_{n \\ge 0}$ on the compact metric space $([0,1], d)$ with $d(x,y) = |x - y|$, defined by the random iterated function system\n$$\nX_{n+1} \\in \\left\\{ \\tfrac{1}{3} X_n,\\ \\tfrac{2 + X_n}{3} \\right\\} \\quad \\text{with equal probability } \\tfrac{1}{2}, \\text{ independently over time.}\n$$\nLet $P$ denote its Markov transition kernel and let $\\pi$ denote a stationary distribution if it exists. Let $\\|\\cdot\\|_{\\mathrm{TV}}$ denote the total variation distance and $W_1$ denote the $1$-Wasserstein distance on probability measures on $([0,1], d)$.\n\nSelect all statements that are true.\n\nA. The Markov operator is a contraction in $W_1$ with modulus strictly less than $1$, so there exists a unique stationary distribution $\\pi$ and, for any initial distribution $\\mu$, one has\n$$\nW_1(\\mu P^n, \\pi) \\le \\left(\\tfrac{1}{3}\\right)^n W_1(\\mu, \\pi) \\quad \\text{for all } n \\ge 0.\n$$\n\nB. For any $x \\in [0,1]$, the sequence of laws $P^n(x, \\cdot)$ does not converge to $\\pi$ in total variation; in fact,\n$$\n\\|P^n(x, \\cdot) - \\pi\\|_{\\mathrm{TV}} = 1 \\quad \\text{for all } n \\ge 1.\n$$\n\nC. For any bounded Lipschitz function $f \\colon [0,1] \\to \\mathbb{R}$ with Lipschitz constant $\\mathrm{Lip}(f)$, the bias decays at the $W_1$-rate:\n$$\n\\big|\\mathbb{E}_x[f(X_n)] - \\pi(f)\\big| \\le \\mathrm{Lip}(f)\\, W_1\\!\\big(P^n(x, \\cdot), \\pi\\big) \\quad \\text{for all } n \\ge 0,\n$$\nso in particular $\\mathbb{E}_x[f(X_n)] \\to \\pi(f)$ as $n \\to \\infty$.\n\nD. For any bounded measurable function $f$,\n$$\n\\big|\\mathbb{E}_x[f(X_n)] - \\pi(f)\\big| \\le \\|f\\|_\\infty\\, W_1\\!\\big(P^n(x, \\cdot), \\pi\\big) \\quad \\text{for all } n \\ge 0,\n$$\nso $W_1$-convergence implies uniform control of bias over all bounded measurable $f$.\n\nE. If one simulates the chain and forms time averages $n^{-1}\\sum_{k=1}^n f(X_k)$, then for any indicator function $f = \\mathbf{1}_A$ with $\\pi(\\partial A) = 0$, one has the almost sure convergence\n$$\n\\frac{1}{n}\\sum_{k=1}^n \\mathbf{1}_A(X_k) \\longrightarrow \\pi(A) \\quad \\text{as } n \\to \\infty,\n$$\nfor any starting point $X_0 = x \\in [0,1]$.\n\nChoose all that apply.", "solution": "The objective is to find an upper bound for the total variation distance $\\|\\pi - \\tilde{\\pi}\\|_{\\mathrm{TV}}$ between the stationary distributions $\\pi$ and $\\tilde{\\pi}$ of the kernels $P$ and $\\tilde{P}$, respectively.\n\nBy definition, $\\pi$ and $\\tilde{\\pi}$ are stationary distributions, meaning they are probability measures on $\\mathcal{X}$ satisfying $\\pi P = \\pi$ and $\\tilde{\\pi} \\tilde{P} = \\tilde{\\pi}$. We start with the quantity we wish to bound and use these stationarity properties:\n$$\n\\pi - \\tilde{\\pi} = \\pi P - \\tilde{\\pi} \\tilde{P}\n$$\nWe add and subtract a cross-term, $\\tilde{\\pi} P$, to separate the effects of the kernel perturbation and the operator's contractivity. Applying the triangle inequality for the TV norm gives:\n$$\n\\|\\pi - \\tilde{\\pi}\\|_{\\mathrm{TV}} = \\|(\\pi P - \\tilde{\\pi} P) + (\\tilde{\\pi} P - \\tilde{\\pi} \\tilde{P})\\|_{\\mathrm{TV}} \\le \\|\\pi P - \\tilde{\\pi} P\\|_{\\mathrm{TV}} + \\|\\tilde{\\pi}(P - \\tilde{P})\\|_{\\mathrm{TV}}\n$$\nWe bound each of the two terms on the right-hand side.\n1. For the first term, we use the definition of the Dobrushin coefficient $\\delta(P)$. Since $\\pi$ and $\\tilde{\\pi}$ are probability measures, $\\|\\pi P - \\tilde{\\pi} P\\|_{\\mathrm{TV}} \\le \\delta(P) \\|\\pi - \\tilde{\\pi}\\|_{\\mathrm{TV}}$.\n\n2. For the second term, $\\|\\tilde{\\pi}(P - \\tilde{P})\\|_{\\mathrm{TV}}$, we use the definition of the push-forward measure and the kernel seminorm. Let $K = P - \\tilde{P}$.\n\\begin{align*}\n\\|\\tilde{\\pi}K\\|_{\\mathrm{TV}} = \\frac{1}{2} \\sum_{y \\in \\mathcal{X}} \\left| \\sum_{x \\in \\mathcal{X}} \\tilde{\\pi}(x) K(x,y) \\right| \\le \\sum_{x \\in \\mathcal{X}} \\tilde{\\pi}(x) \\left( \\frac{1}{2} \\sum_{y \\in \\mathcal{X}} |K(x,y)| \\right) \\\\\n= \\sum_{x \\in \\mathcal{X}} \\tilde{\\pi}(x) \\|K(x, \\cdot)\\|_{\\mathrm{TV}} \\le \\sum_{x \\in \\mathcal{X}} \\tilde{\\pi}(x) \\|K\\|_{\\mathrm{ker}} \\\\\n= \\|K\\|_{\\mathrm{ker}} \\left(\\sum_{x \\in \\mathcal{X}} \\tilde{\\pi}(x)\\right) = \\|P - \\tilde{P}\\|_{\\mathrm{ker}}\n\\end{align*}\nGiven that $\\|P - \\tilde{P}\\|_{\\mathrm{ker}} \\le \\epsilon$, we have $\\|\\tilde{\\pi}(P - \\tilde{P})\\|_{\\mathrm{TV}} \\le \\epsilon$.\n\nSubstituting these bounds back into the main inequality:\n$$\n\\|\\pi - \\tilde{\\pi}\\|_{\\mathrm{TV}} \\le \\delta(P) \\|\\pi - \\tilde{\\pi}\\|_{\\mathrm{TV}} + \\epsilon\n$$\nRearranging the terms to solve for $\\|\\pi - \\tilde{\\pi}\\|_{\\mathrm{TV}}$ (and using the fact that $1 - \\delta(P) > 0$):\n$$\n(1 - \\delta(P)) \\|\\pi - \\tilde{\\pi}\\|_{\\mathrm{TV}} \\le \\epsilon \\implies \\|\\pi - \\tilde{\\pi}\\|_{\\mathrm{TV}} \\le \\frac{\\epsilon}{1 - \\delta(P)}\n$$\nThis is the desired explicit upper bound.\n\nFinally, we evaluate this bound for the given numerical values: $\\delta(P) = 0.87$ and $\\epsilon = 3.0 \\times 10^{-2}$.\n$$\n\\|\\pi - \\tilde{\\pi}\\|_{\\mathrm{TV}} \\le \\frac{3.0 \\times 10^{-2}}{1 - 0.87} = \\frac{0.03}{0.13} = \\frac{3}{13} \\approx 0.230769...\n$$\nRounding to four significant figures gives the final numerical bound.", "answer": "$$\\boxed{0.2308}$$", "id": "3300048"}]}