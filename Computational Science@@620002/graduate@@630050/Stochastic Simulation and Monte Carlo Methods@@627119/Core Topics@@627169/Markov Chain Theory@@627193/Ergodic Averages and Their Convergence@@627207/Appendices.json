{"hands_on_practices": [{"introduction": "The Central Limit Theorem is a cornerstone of statistical inference, but its standard form applies to independent samples. This practice extends the concept to dependent data, a common scenario in time series analysis and MCMC. By working with the canonical first-order autoregressive (AR(1)) process, you will rigorously establish the conditions for a CLT to hold and compute the exact long-run variance, gaining a deep understanding of how temporal correlations affect the precision of ergodic averages [@problem_id:3305618].", "problem": "Consider the first-order autoregressive process defined by $X_{k+1}=\\phi X_{k}+\\epsilon_{k+1}$, where $(\\epsilon_{k})_{k\\geq 1}$ are independent and identically distributed real-valued random variables with $\\mathbb{E}[\\epsilon_{1}]=0$ and $\\mathbb{V}\\mathrm{ar}(\\epsilon_{1})=\\sigma_{\\epsilon}^{2}\\in(0,\\infty)$, and where $|\\phi|1$. Assume the initial state $X_{0}$ has the unique stationary distribution of the chain. Define the strong mixing (also called $\\alpha$-mixing) coefficients by $\\alpha(n)=\\sup_{k\\geq 0}\\sup_{A\\in\\mathcal{F}_{-\\infty}^{k},\\,B\\in\\mathcal{F}_{k+n}^{\\infty}}|\\mathbb{P}(A\\cap B)-\\mathbb{P}(A)\\mathbb{P}(B)|$, where $\\mathcal{F}_{a}^{b}$ denotes the $\\sigma$-algebra generated by $\\{X_{t}:a\\leq t\\leq b\\}$.\n\nYou may use as foundational starting points only the following:\n- Definitions of Markov chains, stationarity, strong mixing coefficients, total variation norm, and the Central Limit Theorem (CLT) for stationary strongly mixing sequences under summable mixing coefficients and finite second moments.\n- Standard facts about autoregressive processes of order $1$ with $|\\phi|1$, including existence and uniqueness of a stationary distribution, and that the stationary autocovariance function of a Gaussian autoregressive process of order $1$ has the form $\\gamma(h)=\\gamma(0)\\phi^{|h|}$.\n\nTasks:\n- First, starting from the Markov property and contractivity of the affine recursion, establish that the $\\alpha$-mixing coefficients $\\alpha(n)$ decay at an exponential rate in $n$.\n- Next, using only the exponential decay of $\\alpha$-mixing and boundedness of $f$, deduce a Central Limit Theorem for ergodic averages of the form $\\frac{1}{n}\\sum_{k=1}^{n}f(X_{k})$ with any bounded measurable $f$, and identify the form of the asymptotic variance as a covariance series.\n- Finally, specialize to the case $\\epsilon_{k}\\sim\\mathcal{N}(0,\\sigma_{\\epsilon}^{2})$ and $f(x)=x$, and compute the exact closed-form expression for the asymptotic variance in the CLT for $\\sqrt{n}\\big(\\frac{1}{n}\\sum_{k=1}^{n}X_{k}-\\mathbb{E}[X_{0}]\\big)$.\n\nYour final answer must be a single closed-form analytic expression for this asymptotic variance in terms of $\\phi$ and $\\sigma_{\\epsilon}^{2}$. No numerical approximation is required.", "solution": "The problem is analyzed in three parts as requested. First, we establish the exponential decay of the strong mixing coefficients. Second, we use this result to deduce a Central Limit Theorem (CLT) for ergodic averages. Third, we compute the explicit asymptotic variance for a specific case.\n\nPart 1: Exponential Decay of $\\alpha$-mixing Coefficients\n\nThe process is defined by the first-order autoregressive (AR($1$)) equation $X_{k+1}=\\phi X_{k}+\\epsilon_{k+1}$, where $|\\phi|1$. This is a time-homogeneous Markov process. The problem asks to establish the exponential decay of the $\\alpha$-mixing coefficients, $\\alpha(n)$, starting from the Markov property and the contractivity of the recursion.\n\nConsider two realizations of the process, $(X_k^x)$ and $(X_k^y)$, starting from two different initial states $X_0=x$ and $X_0=y$, but driven by the same sequence of innovations $(\\epsilon_k)_{k\\geq 1}$. By iterating the recursion, we can express the state at time $k$ as:\n$$X_k^x = \\phi^k x + \\sum_{j=1}^{k} \\phi^{k-j}\\epsilon_j$$\n$$X_k^y = \\phi^k y + \\sum_{j=1}^{k} \\phi^{k-j}\\epsilon_j$$\nThe difference between these two paths at time $k$ is:\n$$X_k^x - X_k^y = \\phi^k (x - y)$$\nTaking the absolute value, we observe the contractivity property:\n$$|X_k^x - X_k^y| = |\\phi|^k |x - y|$$\nSince $|\\phi|1$, the distance between the two paths contracts to zero at an exponential rate. This property is fundamental to proving geometric ergodicity.\n\nFor a stationary Markov process, the strong mixing coefficient $\\alpha(n)$ is defined as $\\alpha(n)=\\sup_{k\\geq 0}\\sup_{A\\in\\mathcal{F}_{-\\infty}^{k},\\,B\\in\\mathcal{F}_{k+n}^{\\infty}}|\\mathbb{P}(A\\cap B)-\\mathbb{P}(A)\\mathbb{P}(B)|$. Due to stationarity and the Markov property, the decay rate of $\\alpha(n)$ is governed by the rate at which the $n$-step transition probability distribution $P^n(x, \\cdot) = \\mathbb{P}(X_n \\in \\cdot | X_0=x)$ converges to the unique stationary distribution $\\pi(\\cdot)$.\n\nA standard theorem in the theory of Markov chains states that if a process is geometrically ergodic, meaning that the total variation distance between the $n$-step transition distribution and the stationary distribution decays exponentially, i.e., $\\sup_x \\|P^n(x, \\cdot) - \\pi(\\cdot) \\|_{TV} \\le M r^n$ for some $M0$ and $r \\in (0,1)$, then its $\\alpha$-mixing coefficients also decay exponentially. The contractivity shown above is a sufficient condition for geometric ergodicity for models of this type. This connection is formalized through coupling arguments. Therefore, we can conclude that there exist constants $C0$ and $\\rho \\in (0,1)$ such that for all $n \\ge 1$:\n$$\\alpha(n) \\le C \\rho^n$$\nThis establishes that the $\\alpha$-mixing coefficients decay at an exponential rate.\n\nPart 2: Central Limit Theorem for Ergodic Averages\n\nWe are asked to deduce a Central Limit Theorem for ergodic averages of the form $\\frac{1}{n}\\sum_{k=1}^{n}f(X_{k})$ for a bounded, measurable function $f$. Let $Y_k = f(X_k)$.\nThe foundational CLT for stationary, strongly-mixing sequences states that if $(Y_k)$ is a stationary sequence with mean $\\mu = \\mathbb{E}[Y_k]$, finite second moments $\\mathbb{E}[Y_k^2]  \\infty$, and strong mixing coefficients $\\alpha_Y(n)$ that are summable, i.e., $\\sum_{n=1}^\\infty \\alpha_Y(n)  \\infty$, then\n$$ \\sqrt{n}\\left(\\frac{1}{n}\\sum_{k=1}^{n}Y_k - \\mu\\right) \\xrightarrow{d} \\mathcal{N}(0, \\sigma^2) $$\nwhere $\\xrightarrow{d}$ denotes convergence in distribution, and the asymptotic variance $\\sigma^2$ is given by\n$$ \\sigma^2 = \\sum_{h=-\\infty}^{\\infty} \\mathrm{Cov}(Y_0, Y_h) = \\mathrm{Var}(Y_0) + 2\\sum_{h=1}^{\\infty}\\mathrm{Cov}(Y_0, Y_h) $$\nWe must verify the conditions for the sequence $Y_k = f(X_k)$.\n1.  **Stationarity:** Since the process $(X_k)$ is assumed to be stationary, the transformed process $(Y_k = f(X_k))$ is also stationary.\n2.  **Finite Second Moment:** The function $f$ is bounded, so there exists a constant $M$ such that $|f(x)| \\le M$ for all $x$. Thus, $\\mathbb{E}[Y_k^2] = \\mathbb{E}[f(X_k)^2] \\le M^2  \\infty$.\n3.  **Summable Mixing Coefficients:** The mixing coefficients for the sequence $(Y_k)$, let's call them $\\alpha_Y(n)$, are bounded by the mixing coefficients of the original sequence $(X_k)$, i.e., $\\alpha_Y(n) \\le \\alpha_X(n)$. From Part 1, we established that $\\alpha_X(n) \\le C \\rho^n$ for $\\rho \\in (0,1)$. The geometric series $\\sum_{n=1}^\\infty C\\rho^n$ converges, so $\\sum_{n=1}^\\infty \\alpha_X(n)  \\infty$, which implies $\\sum_{n=1}^\\infty \\alpha_Y(n)  \\infty$.\n\nAll conditions are met. Thus, the CLT holds for $\\frac{1}{n}\\sum_{k=1}^{n}f(X_{k})$. The asymptotic variance, denoted $\\sigma_f^2$, takes the form of a covariance series:\n$$ \\sigma_f^2 = \\mathrm{Var}(f(X_0)) + 2\\sum_{k=1}^{\\infty}\\mathrm{Cov}(f(X_0), f(X_k)) $$\n\nPart 3: Computation of the Asymptotic Variance\n\nWe now specialize to the case where $\\epsilon_k \\sim \\mathcal{N}(0, \\sigma_\\epsilon^2)$ and the function is $f(x)=x$. The problem asks for the asymptotic variance in the CLT for $\\sqrt{n}\\big(\\frac{1}{n}\\sum_{k=1}^{n}X_{k}-\\mathbb{E}[X_{0}]\\big)$. The general formula for the asymptotic variance $\\sigma^2$ is:\n$$ \\sigma^2 = \\mathrm{Var}(X_0) + 2\\sum_{k=1}^{\\infty}\\mathrm{Cov}(X_0, X_k) $$\nLet $\\gamma(h) = \\mathrm{Cov}(X_0, X_h)$ be the autocovariance function. The formula is $\\sigma^2 = \\gamma(0) + 2\\sum_{k=1}^{\\infty}\\gamma(k)$.\n\nWe first find the mean and variance of the stationary process. Taking the expectation of the AR($1$) equation in stationarity yields $\\mathbb{E}[X_k] = \\phi \\mathbb{E}[X_{k-1}] + \\mathbb{E}[\\epsilon_k]$. With $\\mathbb{E}[\\epsilon_k]=0$ and $\\mathbb{E}[X_k]=\\mathbb{E}[X_{k-1}]=\\mu_X$, we get $\\mu_X = \\phi \\mu_X$, which implies $(1-\\phi)\\mu_X=0$. Since $|\\phi|1$, we must have $\\mu_X=0$. So, $\\mathbb{E}[X_k]=0$ for all $k$.\n\nNext, we find the variance, $\\gamma(0) = \\mathrm{Var}(X_k)$.\n$ \\mathrm{Var}(X_k) = \\mathrm{Var}(\\phi X_{k-1} + \\epsilon_k) $. Since $X_{k-1}$ is determined by innovations up to time $k-1$, it is independent of $\\epsilon_k$. Thus,\n$ \\mathrm{Var}(X_k) = \\phi^2 \\mathrm{Var}(X_{k-1}) + \\mathrm{Var}(\\epsilon_k) $. In stationarity, $\\mathrm{Var}(X_k)=\\mathrm{Var}(X_{k-1})=\\gamma(0)$ and $\\mathrm{Var}(\\epsilon_k)=\\sigma_\\epsilon^2$.\n$ \\gamma(0) = \\phi^2 \\gamma(0) + \\sigma_\\epsilon^2 \\implies \\gamma(0)(1-\\phi^2) = \\sigma_\\epsilon^2 \\implies \\gamma(0) = \\frac{\\sigma_\\epsilon^2}{1-\\phi^2} $.\n\nNow we find the autocovariance $\\gamma(k) = \\mathrm{Cov}(X_0, X_k) = \\mathbb{E}[X_0 X_k]$ for $k0$. We use the provided fact that for a stationary Gaussian AR($1$) process, $\\gamma(h) = \\gamma(0)\\phi^{|h|}$. So for $k \\ge 1$, $\\gamma(k) = \\gamma(0)\\phi^k$.\n\nWe substitute these into the formula for the asymptotic variance:\n$$ \\sigma^2 = \\gamma(0) + 2\\sum_{k=1}^{\\infty} \\gamma(0)\\phi^k = \\gamma(0) \\left(1 + 2\\sum_{k=1}^{\\infty}\\phi^k\\right) $$\nThe sum is a geometric series: $\\sum_{k=1}^{\\infty}\\phi^k = \\frac{\\phi}{1-\\phi}$, which converges because $|\\phi|1$.\nSubstituting this sum:\n$$ \\sigma^2 = \\gamma(0) \\left(1 + 2\\frac{\\phi}{1-\\phi}\\right) = \\gamma(0) \\left(\\frac{1-\\phi+2\\phi}{1-\\phi}\\right) = \\gamma(0) \\left(\\frac{1+\\phi}{1-\\phi}\\right) $$\nFinally, substitute the expression for $\\gamma(0)$:\n$$ \\sigma^2 = \\left(\\frac{\\sigma_\\epsilon^2}{1-\\phi^2}\\right) \\left(\\frac{1+\\phi}{1-\\phi}\\right) $$\nUsing the factorization $1-\\phi^2 = (1-\\phi)(1+\\phi)$:\n$$ \\sigma^2 = \\frac{\\sigma_\\epsilon^2}{(1-\\phi)(1+\\phi)} \\frac{1+\\phi}{1-\\phi} = \\frac{\\sigma_\\epsilon^2}{(1-\\phi)^2} $$\nThis is the closed-form expression for the asymptotic variance.", "answer": "$$\\boxed{\\frac{\\sigma_{\\epsilon}^{2}}{(1-\\phi)^{2}}}$$", "id": "3305618"}, {"introduction": "While the asymptotic variance of an ergodic average is formally a sum of all autocovariances, this representation can be made more intuitive and practical. This exercise introduces the concept of the integrated autocorrelation time, $\\tau_{\\mathrm{int}}$, which elegantly bundles the effect of temporal dependence into a single number [@problem_id:3305623]. You will derive the famous relationship between an estimator's variance, the stationary variance, and $\\tau_{\\mathrm{int}}$, a formula that is fundamental to error analysis in MCMC.", "problem": "Consider a time-homogeneous, irreducible, positive recurrent Markov chain $\\,\\{X_t\\}_{t \\geq 0}\\,$ on a measurable state space with unique invariant distribution $\\,\\pi\\,$. Assume the chain is initialized in stationarity, so that $\\,X_0 \\sim \\pi\\,$ and $\\,\\{X_t\\}\\,$ is strictly stationary. Let $\\,f:\\text{state space}\\to\\mathbb{R}\\,$ be integrable with finite variance under $\\,\\pi\\,$, denoted $\\,\\operatorname{Var}_{\\pi}(f) = \\sigma_f^2 \\in (0,\\infty)\\,$. Define the ergodic average\n$$\n\\hat{\\pi}_n(f) \\;=\\; \\frac{1}{n}\\sum_{t=1}^{n} f(X_t).\n$$\nLet the autocovariance function be $\\,\\gamma_k = \\operatorname{Cov}_{\\pi}(f(X_0), f(X_k))\\,$ and the autocorrelation function be $\\,\\rho_k = \\gamma_k/\\gamma_0\\,$, where $\\,\\gamma_0 = \\sigma_f^2\\,$. Assume $\\,\\sum_{k=1}^{\\infty} |\\rho_k|  \\infty\\,$.\n\n1) Starting from first principles and the definition of variance and covariance, express $\\,\\operatorname{Var}(\\hat{\\pi}_n(f))\\,$ exactly in terms of the finite-sample sum of autocovariances $\\,\\gamma_k\\,$. Then, under the given summability assumption, extract the leading $\\,1/n\\,$ term as $\\,n \\to \\infty\\,$ in terms of the infinite series in $\\,\\rho_k\\,$ and $\\,\\sigma_f^2\\,$.\n\n2) Introduce the integrated autocorrelation time $\\,\\tau_{\\mathrm{int}}\\,$ as a functional of the autocorrelation sequence, and rewrite your leading-order expression for $\\,\\operatorname{Var}(\\hat{\\pi}_n(f))\\,$ succinctly in terms of $\\,\\sigma_f^2\\,$, $\\,\\tau_{\\mathrm{int}}\\,$, and $\\,n\\,$.\n\n3) Now specialize to a first-order autoregressive correlation structure $\\,\\rho_k = \\alpha^{k}\\,$ for $\\,k \\geq 1\\,$ with $\\,|\\alpha|1\\,$. Let $\\,\\alpha = \\tfrac{1}{2}\\,$, $\\,\\sigma_f^2 = 2\\,$, and $\\,n = 10^{4}\\,$. Using your expression from part $\\,2)\\,$, compute the approximate value of $\\,\\operatorname{Var}(\\hat{\\pi}_n(f))\\,$. Round your final numerical answer to four significant figures.", "solution": "The problem is divided into three parts. We will address them sequentially.\n\nPart 1: Derivation of the exact and asymptotic variance of the ergodic average.\n\nThe ergodic average is defined as $\\hat{\\pi}_n(f) = \\frac{1}{n}\\sum_{t=1}^{n} f(X_t)$. We start from the definition of variance:\n$$\n\\operatorname{Var}(\\hat{\\pi}_n(f)) = \\operatorname{Var}\\left(\\frac{1}{n}\\sum_{t=1}^{n} f(X_t)\\right)\n$$\nUsing the property $\\operatorname{Var}(cY) = c^2\\operatorname{Var}(Y)$ for a constant $c$, we can factor out $\\frac{1}{n^2}$:\n$$\n\\operatorname{Var}(\\hat{\\pi}_n(f)) = \\frac{1}{n^2} \\operatorname{Var}\\left(\\sum_{t=1}^{n} f(X_t)\\right)\n$$\nThe variance of a sum of random variables is the sum of all entries in their covariance matrix:\n$$\n\\operatorname{Var}\\left(\\sum_{t=1}^{n} f(X_t)\\right) = \\sum_{s=1}^{n} \\sum_{t=1}^{n} \\operatorname{Cov}(f(X_s), f(X_t))\n$$\nThe problem states that the Markov chain $\\{X_t\\}_{t \\geq 0}$ is strictly stationary. This implies that the covariance between $f(X_s)$ and $f(X_t)$ depends only on the time lag $|t-s|$. Let $\\gamma_k = \\operatorname{Cov}_{\\pi}(f(X_0), f(X_k))$ be the autocovariance at lag $k$. Then, by stationarity:\n$$\n\\operatorname{Cov}(f(X_s), f(X_t)) = \\operatorname{Cov}(f(X_0), f(X_{|t-s|})) = \\gamma_{|t-s|}\n$$\nSubstituting this into the double summation, we have:\n$$\n\\operatorname{Var}(\\hat{\\pi}_n(f)) = \\frac{1}{n^2} \\sum_{s=1}^{n} \\sum_{t=1}^{n} \\gamma_{|t-s|}\n$$\nTo evaluate this sum, we re-index it by the lag $k = t-s$. The lag $k$ ranges from $-(n-1)$ to $n-1$. For a given lag $k$, there are $n-|k|$ pairs of indices $(s,t)$ in the sum (where $1 \\leq s,t \\leq n$) such that $t-s=k$. The sum can thus be written as:\n$$\n\\sum_{s=1}^{n} \\sum_{t=1}^{n} \\gamma_{|t-s|} = \\sum_{k=-(n-1)}^{n-1} (n-|k|) \\gamma_{|k|}\n$$\nThe autocovariance function is even, meaning $\\gamma_k = \\gamma_{-k}$, so $\\gamma_{|k|}$ is simply $\\gamma_k$ for any integer $k$. We can split the sum:\n$$\n\\sum_{k=-(n-1)}^{n-1} (n-|k|) \\gamma_k = (n-0)\\gamma_0 + \\sum_{k=1}^{n-1} (n-k)\\gamma_k + \\sum_{k=-(n-1)}^{-1} (n-|k|)\\gamma_k\n$$\nLetting $j = -k$ in the last term, it becomes $\\sum_{j=1}^{n-1} (n-j)\\gamma_{-j} = \\sum_{j=1}^{n-1} (n-j)\\gamma_j$. The total sum is therefore:\n$$\nn\\gamma_0 + 2\\sum_{k=1}^{n-1} (n-k)\\gamma_k\n$$\nSubstituting this back into the expression for the variance yields the exact expression:\n$$\n\\operatorname{Var}(\\hat{\\pi}_n(f)) = \\frac{1}{n^2} \\left[ n\\gamma_0 + 2\\sum_{k=1}^{n-1} (n-k)\\gamma_k \\right] = \\frac{\\gamma_0}{n} + \\frac{2}{n}\\sum_{k=1}^{n-1}\\left(1 - \\frac{k}{n}\\right)\\gamma_k\n$$\nNow, we find the leading term as $n \\to \\infty$. Using the definitions $\\gamma_0 = \\sigma_f^2$ and $\\rho_k = \\gamma_k/\\gamma_0$, we rewrite the variance as:\n$$\n\\operatorname{Var}(\\hat{\\pi}_n(f)) = \\frac{\\sigma_f^2}{n} \\left[ 1 + 2\\sum_{k=1}^{n-1}\\left(1 - \\frac{k}{n}\\right)\\rho_k \\right]\n$$\nFor large $n$, the term $(1 - k/n)$ approaches $1$ for any fixed $k$, and the upper limit of the sum goes to infinity. The problem states that $\\sum_{k=1}^{\\infty} |\\rho_k|  \\infty$, which means the series $\\sum \\rho_k$ converges absolutely. This allows us to interchange the limit and the sum (via the Dominated Convergence Theorem for series, since $|(1-k/n)\\rho_k| \\le |\\rho_k|$ and $\\sum|\\rho_k|$ is a convergent dominating series).\n$$\n\\lim_{n\\to\\infty} \\sum_{k=1}^{n-1}\\left(1 - \\frac{k}{n}\\right)\\rho_k = \\sum_{k=1}^{\\infty} \\rho_k\n$$\nTherefore, for large $n$, we have $n \\operatorname{Var}(\\hat{\\pi}_n(f)) \\to \\sigma_f^2 (1 + 2\\sum_{k=1}^{\\infty} \\rho_k)$. The leading order term in $1/n$ is:\n$$\n\\operatorname{Var}(\\hat{\\pi}_n(f)) \\approx \\frac{\\sigma_f^2}{n} \\left( 1 + 2\\sum_{k=1}^{\\infty} \\rho_k \\right)\n$$\n\nPart 2: Introduction of the integrated autocorrelation time.\n\nThe integrated autocorrelation time, $\\tau_{\\mathrm{int}}$, is defined as the sum of the autocorrelation function over all lags:\n$$\n\\tau_{\\mathrm{int}} = \\sum_{k=-\\infty}^{\\infty} \\rho_k\n$$\nBy definition, $\\rho_0 = \\gamma_0/\\gamma_0 = 1$. The autocorrelation function is also an even function, $\\rho_k = \\rho_{-k}$. Thus, we can write $\\tau_{\\mathrm{int}}$ as:\n$$\n\\tau_{\\mathrm{int}} = \\rho_0 + \\sum_{k=1}^{\\infty} \\rho_k + \\sum_{k=-\\infty}^{-1} \\rho_k = 1 + 2\\sum_{k=1}^{\\infty} \\rho_k\n$$\nThis is precisely the factor appearing in the asymptotic expression for the variance. The condition $\\sum_{k=1}^{\\infty} |\\rho_k|  \\infty$ ensures that $\\tau_{\\mathrm{int}}$ is finite. Substituting this definition into our result from Part 1, we obtain the succinct expression:\n$$\n\\operatorname{Var}(\\hat{\\pi}_n(f)) \\approx \\frac{\\sigma_f^2 \\tau_{\\mathrm{int}}}{n}\n$$\n\nPart 3: Specialization to an AR(1) correlation structure and numerical calculation.\n\nWe are given a first-order autoregressive correlation structure, $\\rho_k = \\alpha^k$ for $k \\geq 1$, with $|\\alpha|  1$. We first compute $\\tau_{\\mathrm{int}}$ for this structure using its definition from Part 2:\n$$\n\\tau_{\\mathrm{int}} = 1 + 2\\sum_{k=1}^{\\infty} \\rho_k = 1 + 2\\sum_{k=1}^{\\infty} \\alpha^k\n$$\nThe sum is a standard geometric series: $\\sum_{k=1}^{\\infty} \\alpha^k = \\frac{\\alpha}{1-\\alpha}$. Substituting this into the expression for $\\tau_{\\mathrm{int}}$ gives:\n$$\n\\tau_{\\mathrm{int}} = 1 + 2\\left(\\frac{\\alpha}{1-\\alpha}\\right) = \\frac{1-\\alpha+2\\alpha}{1-\\alpha} = \\frac{1+\\alpha}{1-\\alpha}\n$$\nThe problem provides the values $\\alpha = \\frac{1}{2}$, $\\sigma_f^2 = 2$, and $n = 10^4$. We first calculate $\\tau_{\\mathrm{int}}$ with $\\alpha = \\frac{1}{2}$:\n$$\n\\tau_{\\mathrm{int}} = \\frac{1+\\frac{1}{2}}{1-\\frac{1}{2}} = \\frac{\\frac{3}{2}}{\\frac{1}{2}} = 3\n$$\nNow, we use the approximate formula for the variance with the given numerical values:\n$$\n\\operatorname{Var}(\\hat{\\pi}_n(f)) \\approx \\frac{\\sigma_f^2 \\tau_{\\mathrm{int}}}{n} = \\frac{2 \\times 3}{10^4} = \\frac{6}{10000} = 0.0006\n$$\nThe problem requires the final answer to be rounded to four significant figures. We express $0.0006$ in scientific notation to explicitly show the significant figures: $6.000 \\times 10^{-4}$.", "answer": "$$ \\boxed{6.000 \\times 10^{-4}} $$", "id": "3305623"}, {"introduction": "Ergodic theorems are powerful, but they rely on crucial assumptions about the underlying process, such as aperiodicity. This practice explores what happens when this assumption is violated by constructing a simple periodic Markov chain where the standard time averages fail to converge to the correct stationary expectation. This thought experiment highlights a common pitfall and deepens your understanding of the necessary conditions for ergodicity by showing precisely how their failure manifests [@problem_id:3305645].", "problem": "Consider a discrete-time Markov chain $\\{X_{t}\\}_{t \\geq 0}$ on the state space $\\{0,1\\}$ with transition kernel $P$ defined by\n$$\nP(0,1)=1, \\quad P(1,0)=1, \\quad P(0,0)=0, \\quad P(1,1)=0.\n$$\nLet $\\pi$ be an invariant distribution for $P$, and let $f:\\{0,1\\}\\to\\mathbb{R}$ be a bounded function with $f(0)=\\alpha$ and $f(1)=\\beta$, where $\\alpha,\\beta\\in\\mathbb{R}$. Assume an arbitrary initial distribution for $X_{0}$.\n\nUsing only the fundamental definitions of a Markov chain, invariant distribution, periodicity, and ergodic averages, perform the following steps:\n\n- Construct $\\pi$ and verify that it is indeed invariant under $P$.\n- Establish the period of the chain and describe the structure of trajectories $\\{X_{t}\\}_{t\\geq 0}$.\n- Define the residue-class ergodic averages for residue $r\\in\\{0,1\\}$ by\n$$\nA_{n}^{(r)} \\equiv \\frac{1}{n}\\sum_{t=0}^{n-1} f\\big(X_{2t+r}\\big).\n$$\nShow that for each fixed $r\\in\\{0,1\\}$, the sequence $\\{A_{n}^{(r)}\\}_{n\\geq 1}$ converges almost surely, identify its almost sure limit, and explain why these limits need not coincide nor equal $\\sum_{x\\in\\{0,1\\}}\\pi(x)f(x)$ when $\\alpha\\neq\\beta$. Conclude why residue-class averaging “without averaging over a full period” does not yield a deterministic almost sure limit equal to the stationary expectation.\n- Now “average over a full period” by defining the period-averaged ergodic average\n$$\n\\overline{A}_{n} \\equiv \\frac{1}{2}\\sum_{r=0}^{1}\\left(\\frac{1}{n}\\sum_{t=0}^{n-1} f\\big(X_{2t+r}\\big)\\right).\n$$\nDerive the almost sure limit of $\\overline{A}_{n}$ and relate it to $\\pi$ and $f$.\n\nAnswer specification:\n- Your final answer must be the closed-form expression of $\\lim_{n\\to\\infty}\\overline{A}_{n}$ in terms of $\\alpha$ and $\\beta$.\n- No numerical rounding is required.", "solution": "The problem asks for an analysis of ergodic averages for a periodic Markov chain. We proceed by following the specified steps.\n\nFirst, we construct the invariant distribution $\\pi$ for the given transition kernel $P$. The state space is $S=\\{0,1\\}$. The transition kernel is defined by $P(0,1)=1$, $P(1,0)=1$, and $P(0,0)=P(1,1)=0$. The corresponding transition matrix is\n$$\nP = \\begin{pmatrix} 0  1 \\\\ 1  0 \\end{pmatrix}.\n$$\nAn invariant distribution $\\pi = (\\pi(0), \\pi(1))$ must satisfy the stationarity condition $\\pi P = \\pi$, along with the normalization condition $\\sum_{x\\in S} \\pi(x) = 1$. The matrix equation is\n$$\n(\\pi(0), \\pi(1)) \\begin{pmatrix} 0  1 \\\\ 1  0 \\end{pmatrix} = (\\pi(0), \\pi(1)).\n$$\nThis yields the vector equation $(\\pi(1), \\pi(0)) = (\\pi(0), \\pi(1))$, which implies $\\pi(0) = \\pi(1)$.\nCombining this with the normalization condition $\\pi(0) + \\pi(1) = 1$, we solve the system of equations:\n$$\n\\begin{cases}\n\\pi(0) = \\pi(1) \\\\\n\\pi(0) + \\pi(1) = 1\n\\end{cases}\n\\implies 2\\pi(0) = 1 \\implies \\pi(0) = \\frac{1}{2}.\n$$\nThus, $\\pi(1) = \\frac{1}{2}$. The unique invariant distribution is $\\pi = (\\frac{1}{2}, \\frac{1}{2})$.\nWe verify its invariance: $\\pi P = (\\frac{1}{2}, \\frac{1}{2}) \\begin{pmatrix} 0  1 \\\\ 1  0 \\end{pmatrix} = (\\frac{1}{2}, \\frac{1}{2}) = \\pi$.\n\nNext, we establish the period of the chain. The period of a state $i$ is the greatest common divisor (GCD) of the set of all possible return times $\\{n \\geq 1 : P^n(i, i)  0\\}$. We compute the powers of $P$:\n$$\nP^2 = P \\cdot P = \\begin{pmatrix} 0  1 \\\\ 1  0 \\end{pmatrix} \\begin{pmatrix} 0  1 \\\\ 1  0 \\end{pmatrix} = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} = I.\n$$\nIt follows that $P^n = P$ for odd $n$ and $P^n = I$ for even $n$.\nFor state $0$, the set of return times is $\\{n \\geq 1 : P^n(0,0)  0\\} = \\{2, 4, 6, \\dots\\}$. The GCD of this set is $2$.\nFor state $1$, the set of return times is $\\{n \\geq 1 : P^n(1,1)  0\\} = \\{2, 4, 6, \\dots\\}$. The GCD of this set is also $2$.\nThe chain is irreducible since it is possible to transition between any two states. Thus, all states have the same period, and the period of the chain is $d=2$.\nThe trajectory of the chain $\\{X_t\\}_{t\\geq 0}$ is deterministic given the initial state $X_0$. With probability $1$, $X_{t+1}=1$ if $X_t=0$ and $X_{t+1}=0$ if $X_t=1$. The state flips at each step. This implies that for any realization originating from $X_0$, the trajectory is given by $X_t = (X_0 + t) \\pmod 2$.\n\nNow, we analyze the residue-class ergodic averages $A_{n}^{(r)} = \\frac{1}{n}\\sum_{t=0}^{n-1} f(X_{2t+r})$ for $r \\in \\{0,1\\}$, where $f(0)=\\alpha$ and $f(1)=\\beta$.\nFor a given trajectory starting at $X_0$, the state at time $2t+r$ is\n$$\nX_{2t+r} = (X_0 + (2t+r)) \\pmod 2 = (X_0 + r) \\pmod 2.\n$$\nThis value is constant for all $t \\geq 0$ and depends only on the initial state $X_0$ and the residue $r$. Let's call this constant state $x^*(X_0, r) = (X_0 + r) \\pmod 2$.\nThe average $A_{n}^{(r)}$ for a given path is then\n$$\nA_{n}^{(r)} = \\frac{1}{n} \\sum_{t=0}^{n-1} f(x^*(X_0, r)) = f(x^*(X_0, r)).\n$$\nThe sequence $\\{A_n^{(r)}\\}_{n \\geq 1}$ is constant for any specific trajectory. Therefore, it converges almost surely to that constant value. The almost sure limit is:\n$$\n\\lim_{n\\to\\infty} A_{n}^{(r)} = f((X_0+r) \\pmod 2).\n$$\nThis limit is a random variable depending on the random initial state $X_0$:\nFor $r=0$: $\\lim_{n\\to\\infty} A_n^{(0)} = f(X_0)$. This is $\\alpha$ if $X_0=0$, and $\\beta$ if $X_0=1$.\nFor $r=1$: $\\lim_{n\\to\\infty} A_n^{(1)} = f((X_0+1) \\pmod 2)$. This is $f(1)=\\beta$ if $X_0=0$, and $f(0)=\\alpha$ if $X_0=1$.\nIf $\\alpha \\neq \\beta$, these limits do not coincide. Furthermore, the stationary expectation is $\\mathbb{E}_{\\pi}[f] = \\sum_{x \\in \\{0,1\\}} \\pi(x)f(x) = \\frac{1}{2}f(0) + \\frac{1}{2}f(1) = \\frac{\\alpha+\\beta}{2}$. The limits $\\alpha$ and $\\beta$ are not equal to $\\frac{\\alpha+\\beta}{2}$ (unless $\\alpha=\\beta$). This failure to converge to a deterministic limit equal to the stationary expectation is a hallmark of periodic chains. The residue-class sampling observes the process on only one of the cyclically permuting sets of states, leading to a biased, path-dependent result.\n\nFinally, we analyze the period-averaged ergodic average $\\overline{A}_{n} = \\frac{1}{2}\\sum_{r=0}^{1} A_{n}^{(r)}$. We seek its almost sure limit. Using the linearity of limits:\n$$\n\\lim_{n\\to\\infty} \\overline{A}_{n} = \\text{a.s.-}\\lim_{n\\to\\infty} \\frac{1}{2}(A_n^{(0)} + A_n^{(1)}) = \\frac{1}{2} \\left( \\text{a.s.-}\\lim_{n\\to\\infty} A_n^{(0)} + \\text{a.s.-}\\lim_{n\\to\\infty} A_n^{(1)} \\right).\n$$\nWe evaluate this limit based on the initial state $X_0$.\nIf $X_0=0$: The limits for $A_n^{(0)}$ and $A_n^{(1)}$ are $\\alpha$ and $\\beta$ respectively. So, $\\lim_{n\\to\\infty} \\overline{A}_{n} = \\frac{1}{2}(\\alpha + \\beta)$.\nIf $X_0=1$: The limits for $A_n^{(0)}$ and $A_n^{(1)}$ are $\\beta$ and $\\alpha$ respectively. So, $\\lim_{n\\to\\infty} \\overline{A}_{n} = \\frac{1}{2}(\\beta + \\alpha)$.\nIn both cases, the limit is identical. Therefore, the almost sure limit of $\\overline{A}_{n}$ is deterministic and is given by $\\frac{\\alpha+\\beta}{2}$, regardless of the initial distribution. This limit equals the stationary expectation $\\mathbb{E}_{\\pi}[f]$. By averaging over a full period, we correctly recover the ergodic property. Note that $\\overline{A}_n$ can be expressed as the standard time average:\n$$\n\\overline{A}_{n} = \\frac{1}{2n} \\left(\\sum_{t=0}^{n-1} f(X_{2t}) + \\sum_{t=0}^{n-1} f(X_{2t+1})\\right) = \\frac{1}{2n} \\sum_{k=0}^{2n-1} f(X_k).\n$$\nThis demonstrates that the standard ergodic average converges to the stationary expectation even for this periodic chain.\nThe closed-form expression for the limit is $\\frac{\\alpha+\\beta}{2}$.", "answer": "$$\n\\boxed{\\frac{\\alpha+\\beta}{2}}\n$$", "id": "3305645"}]}