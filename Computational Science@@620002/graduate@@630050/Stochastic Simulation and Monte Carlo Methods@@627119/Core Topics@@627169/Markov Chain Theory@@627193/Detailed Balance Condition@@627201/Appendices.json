{"hands_on_practices": [{"introduction": "The abstract definition of detailed balance, $\\pi_i P_{ij} = \\pi_j P_{ji}$, connects a stationary distribution $\\pi$ to a transition matrix $P$. A crucial preliminary question is whether a given Markov chain *can* be reversible for *some* positive stationary distribution. This exercise moves from theory to application, using Kolmogorov's cycle criterion as a practical tool to test for reversibility and, upon confirmation, to solve for the unique stationary distribution.", "problem": "Consider a discrete-time Markov chain (MC) on the state space $\\{1,2,3\\}$ with transition probabilities given by the following stochastic matrix $P = (P_{ij})$:\n$$\nP \\;=\\;\n\\begin{pmatrix}\nP_{11} & P_{12} & P_{13} \\\\\nP_{21} & P_{22} & P_{23} \\\\\nP_{31} & P_{32} & P_{33}\n\\end{pmatrix}\n\\;=\\;\n\\begin{pmatrix}\n\\frac{2}{5} & \\frac{2}{5} & \\frac{1}{5} \\\\\n\\frac{1}{2} & \\frac{1}{6} & \\frac{1}{3} \\\\\n\\frac{3}{10} & \\frac{2}{5} & \\frac{3}{10}\n\\end{pmatrix}.\n$$\nReversibility (detailed balance) with respect to a strictly positive invariant distribution $\\pi = (\\pi_{1},\\pi_{2},\\pi_{3})$ means $\\pi_{i} P_{ij} = \\pi_{j} P_{ji}$ for all $i,j \\in \\{1,2,3\\}$. A classical criterion for reversibility involves cycle constraints along closed loops. Starting from core definitions of a discrete-time Markov chain, stationarity, and detailed balance, derive the cycle criterion for a three-state chain and then use it to decide whether detailed balance can hold for some strictly positive $\\pi$ for the above $P$. If it can, solve for $\\pi$ with the normalization $\\sum_{i=1}^{3} \\pi_{i} = 1$. If detailed balance cannot hold, justify the conclusion by the cycle criterion. Express your final answer as the normalized vector $\\pi$; no rounding is required and the answer must be exact.", "solution": "The problem statement is a valid exercise in the theory of discrete-time Markov chains. It is self-contained, mathematically sound, and well-posed. We shall proceed with the solution.\n\nFirst, we establish the theoretical foundation as requested by the problem statement.\n\nA discrete-time Markov chain on a finite state space $\\mathcal{S}$ is a sequence of random variables $\\{X_n\\}_{n \\ge 0}$ taking values in $\\mathcal{S}$ that satisfies the Markov property: for any states $i, j, i_0, \\ldots, i_{n-1} \\in \\mathcal{S}$,\n$$\nP(X_{n+1}=j | X_n=i, X_{n-1}=i_{n-1}, \\dots, X_0=i_0) = P(X_{n+1}=j | X_n=i).\n$$\nFor a time-homogeneous chain, this probability is independent of $n$ and is denoted by the transition probability $P_{ij} = P(X_{n+1}=j | X_n=i)$. These probabilities form the transition matrix $P = (P_{ij})$.\n\nA probability distribution $\\pi = (\\pi_i)_{i \\in \\mathcal{S}}$ is called a stationary or invariant distribution if it is a fixed point of the transition operation. That is, if the chain is in state $i$ with probability $\\pi_i$ at time $n$, it will be in state $j$ with probability $\\pi_j$ at time $n+1$. This is expressed by the equation $\\pi = \\pi P$, which in component form is:\n$$\n\\pi_j = \\sum_{i \\in \\mathcal{S}} \\pi_i P_{ij} \\quad \\text{for all } j \\in \\mathcal{S}.\n$$\n\nA Markov chain is said to be reversible, or to satisfy the detailed balance condition with respect to a distribution $\\pi$, if for all states $i, j \\in \\mathcal{S}$:\n$$\n\\pi_i P_{ij} = \\pi_j P_{ji}.\n$$\nThe detailed balance condition is a sufficient, but not necessary, condition for stationarity. To prove this, we sum the detailed balance equation over all states $j$:\n$$\n\\sum_{j \\in \\mathcal{S}} \\pi_i P_{ij} = \\sum_{j \\in \\mathcal{S}} \\pi_j P_{ji}.\n$$\nThe left side simplifies to $\\pi_i \\sum_{j \\in \\mathcal{S}} P_{ij} = \\pi_i \\cdot 1 = \\pi_i$, since the rows of a stochastic matrix sum to $1$. The equation thus becomes $\\pi_i = \\sum_{j \\in \\mathcal{S}} \\pi_j P_{ji}$. Relabeling the indices by letting $i \\to j$ and $j \\to i$ yields $\\pi_j = \\sum_{i \\in \\mathcal{S}} \\pi_i P_{ij}$, which is precisely the stationarity condition.\n\nNext, we derive the cycle criterion (Kolmogorov's criterion for reversibility). Assume the detailed balance condition $\\pi_i P_{ij} = \\pi_j P_{ji}$ holds for a strictly positive distribution $\\pi > 0$. Consider any closed loop (cycle) of states $i_1, i_2, \\ldots, i_k, i_1$. From the detailed balance condition, we can write:\n$$\n\\frac{P_{i_m i_{m+1}}}{P_{i_{m+1} i_m}} = \\frac{\\pi_{i_{m+1}}}{\\pi_{i_m}}\n$$\nfor any adjacent pair of states in the cycle, provided the transition probabilities in the denominator are non-zero.\nTaking the product around the cycle $i_1 \\to i_2 \\to \\dots \\to i_k \\to i_1$ (where $i_{k+1}=i_1$):\n$$\n\\prod_{m=1}^{k} \\frac{P_{i_m i_{m+1}}}{P_{i_{m+1} i_m}} = \\frac{P_{i_1 i_2}}{P_{i_2 i_1}} \\cdot \\frac{P_{i_2 i_3}}{P_{i_3 i_2}} \\cdots \\frac{P_{i_k i_1}}{P_{i_1 i_k}} = \\frac{\\pi_{i_2}}{\\pi_{i_1}} \\cdot \\frac{\\pi_{i_3}}{\\pi_{i_2}} \\cdots \\frac{\\pi_{i_1}}{\\pi_{i_k}} = 1.\n$$\nThis implies that the product of transition probabilities along any cycle must equal the product of transition probabilities along the same cycle in the reverse direction:\n$$\nP_{i_1 i_2} P_{i_2 i_3} \\cdots P_{i_k i_1} = P_{i_1 i_k} P_{i_k i_{k-1}} \\cdots P_{i_2 i_1}.\n$$\nFor a three-state chain on $\\mathcal{S}=\\{1, 2, 3\\}$, any cycle longer than $3$ is composed of smaller cycles. The cycles of length $2$ (e.g., $1 \\to 2 \\to 1$) give the trivial identity $P_{12}P_{21} = P_{12}P_{21}$. Thus, it suffices to check the condition for the cycle of length $3$: $1 \\to 2 \\to 3 \\to 1$. The criterion is:\n$$\nP_{12} P_{23} P_{31} = P_{13} P_{32} P_{21}.\n$$\nThe product on the left corresponds to the path $1 \\to 2 \\to 3 \\to 1$, and the product on the right corresponds to the reverse path $1 \\to 3 \\to 2 \\to 1$.\n\nWe now apply this criterion to the given transition matrix:\n$$\nP =\n\\begin{pmatrix}\n\\frac{2}{5} & \\frac{2}{5} & \\frac{1}{5} \\\\\n\\frac{1}{2} & \\frac{1}{6} & \\frac{1}{3} \\\\\n\\frac{3}{10} & \\frac{2}{5} & \\frac{3}{10}\n\\end{pmatrix}.\n$$\nThe relevant transition probabilities are:\n$P_{12} = \\frac{2}{5}$, $P_{23} = \\frac{1}{3}$, $P_{31} = \\frac{3}{10}$.\n$P_{13} = \\frac{1}{5}$, $P_{32} = \\frac{2}{5}$, $P_{21} = \\frac{1}{2}$.\n\nWe compute the product for the cycle $1 \\to 2 \\to 3 \\to 1$:\n$$\nP_{12} P_{23} P_{31} = \\left(\\frac{2}{5}\\right) \\left(\\frac{1}{3}\\right) \\left(\\frac{3}{10}\\right) = \\frac{6}{150} = \\frac{1}{25}.\n$$\nNext, we compute the product for the reverse cycle $1 \\to 3 \\to 2 \\to 1$:\n$$\nP_{13} P_{32} P_{21} = \\left(\\frac{1}{5}\\right) \\left(\\frac{2}{5}\\right) \\left(\\frac{1}{2}\\right) = \\frac{2}{50} = \\frac{1}{25}.\n$$\nSince $P_{12} P_{23} P_{31} = P_{13} P_{32} P_{21}$, the cycle criterion is satisfied. Therefore, a strictly positive invariant distribution $\\pi$ satisfying the detailed balance condition exists for this Markov chain.\n\nWe now solve for this distribution $\\pi = (\\pi_1, \\pi_2, \\pi_3)$. The detailed balance equations are:\n1. $\\pi_1 P_{12} = \\pi_2 P_{21} \\implies \\pi_1 \\left(\\frac{2}{5}\\right) = \\pi_2 \\left(\\frac{1}{2}\\right)$\n2. $\\pi_2 P_{23} = \\pi_3 P_{32} \\implies \\pi_2 \\left(\\frac{1}{3}\\right) = \\pi_3 \\left(\\frac{2}{5}\\right)$\n\nFrom equation (1), we express $\\pi_2$ in terms of $\\pi_1$:\n$$\n\\pi_2 = \\pi_1 \\frac{2/5}{1/2} = \\pi_1 \\left(\\frac{4}{5}\\right).\n$$\nFrom equation (2), we express $\\pi_3$ in terms of $\\pi_2$:\n$$\n\\pi_3 = \\pi_2 \\frac{1/3}{2/5} = \\pi_2 \\left(\\frac{5}{6}\\right).\n$$\nSubstituting the expression for $\\pi_2$:\n$$\n\\pi_3 = \\left(\\frac{4}{5}\\pi_1\\right) \\left(\\frac{5}{6}\\right) = \\frac{4}{6}\\pi_1 = \\frac{2}{3}\\pi_1.\n$$\nNow we have $\\pi_2 = \\frac{4}{5}\\pi_1$ and $\\pi_3 = \\frac{2}{3}\\pi_1$. We use the normalization condition $\\sum_{i=1}^{3} \\pi_i = 1$:\n$$\n\\pi_1 + \\pi_2 + \\pi_3 = 1\n$$\n$$\n\\pi_1 + \\frac{4}{5}\\pi_1 + \\frac{2}{3}\\pi_1 = 1\n$$\n$$\n\\pi_1 \\left(1 + \\frac{4}{5} + \\frac{2}{3}\\right) = 1\n$$\nTo sum the fractions, we find a common denominator, which is $15$:\n$$\n\\pi_1 \\left(\\frac{15}{15} + \\frac{12}{15} + \\frac{10}{15}\\right) = 1\n$$\n$$\n\\pi_1 \\left(\\frac{15+12+10}{15}\\right) = 1\n$$\n$$\n\\pi_1 \\left(\\frac{37}{15}\\right) = 1 \\implies \\pi_1 = \\frac{15}{37}.\n$$\nNow we find $\\pi_2$ and $\\pi_3$:\n$$\n\\pi_2 = \\frac{4}{5}\\pi_1 = \\frac{4}{5} \\left(\\frac{15}{37}\\right) = \\frac{12}{37}.\n$$\n$$\n\\pi_3 = \\frac{2}{3}\\pi_1 = \\frac{2}{3} \\left(\\frac{15}{37}\\right) = \\frac{10}{37}.\n$$\nThus, the unique normalized stationary distribution that satisfies detailed balance is $\\pi = \\left(\\frac{15}{37}, \\frac{12}{37}, \\frac{10}{37}\\right)$. All components are strictly positive, as required. A final check confirms normalization: $\\frac{15}{37} + \\frac{12}{37} + \\frac{10}{37} = \\frac{37}{37} = 1$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{15}{37} & \\frac{12}{37} & \\frac{10}{37}\n\\end{pmatrix}\n}\n$$", "id": "3302632"}, {"introduction": "The Metropolis-Hastings algorithm is the workhorse of modern simulation, ingeniously enforcing detailed balance through its acceptance probability. However, its power lies in its precise formulation, which accounts for any asymmetries in the proposal mechanism. This exercise presents a cautionary tale by demonstrating how a seemingly plausible but naive acceptance rule can break detailed balance, challenging you to derive the correct, robust formula from first principles.", "problem": "Consider a finite-state Markov Chain Monte Carlo (MCMC) method on the state space $\\{0,1,2\\}$ targeting the distribution $\\pi$ with $\\pi(0)=\\frac{1}{2}$, $\\pi(1)=\\frac{1}{3}$, and $\\pi(2)=\\frac{1}{6}$. Two deterministic proposal kernels are available: a clockwise kernel $K^{+}$ defined by $K^{+}(x,y)=1$ if $y\\equiv x+1\\ (\\mathrm{mod}\\ 3)$ and $K^{+}(x,y)=0$ otherwise, and a counterclockwise kernel $K^{-}$ defined by $K^{-}(x,y)=1$ if $y\\equiv x-1\\ (\\mathrm{mod}\\ 3)$ and $K^{-}(x,y)=0$ otherwise. The proposal mechanism is a state-dependent mixture: at the current state $x$, $K^{+}$ is selected with weight $w(x)$ and $K^{-}$ with weight $1-w(x)$, where $w(0)=\\frac{3}{4}$, $w(1)=\\frac{1}{2}$, and $w(2)=\\frac{1}{4}$. Thus the one-step proposal probability is $q(x,y)=w(x)K^{+}(x,y)+\\big(1-w(x)\\big)K^{-}(x,y)$.\n\nSuppose a practitioner implements a naive acceptance rule that incorrectly assumes a symmetric proposal and uses $\\alpha_{\\mathrm{naive}}(x,y)=\\min\\!\\big(1,\\frac{\\pi(y)}{\\pi(x)}\\big)$ for all $x\\neq y$. Using only the definition of detailed balance, namely that $\\pi(x)P(x,y)=\\pi(y)P(y,x)$ for all states $x,y$ with $x\\neq y$ where $P(x,y)=q(x,y)\\alpha(x,y)$ denotes the transition kernel, and the rules of probability, compute the ratio\n$$\\rho=\\frac{\\pi(0)\\,q(0,1)\\,\\alpha_{\\mathrm{naive}}(0,1)}{\\pi(1)\\,q(1,0)\\,\\alpha_{\\mathrm{naive}}(1,0)}$$\nto demonstrate that detailed balance is broken for the pair $(0,1)$ under this naive rule.\n\nThen, starting from the same foundational definition of detailed balance and without assuming any shortcut formulas, derive the acceptance probability $\\alpha_{\\mathrm{corr}}(0,1)$ that restores reversibility for the transition from $0$ to $1$ under the given state-dependent mixture proposal $q(x,y)$. Express your final answer as a single reduced fraction. No rounding is required.", "solution": "The analysis of the problem will be conducted in two parts. First, we will demonstrate that the naive acceptance rule violates the detailed balance condition by computing the specified ratio $\\rho$. Second, we will derive the correct acceptance probability $\\alpha_{\\mathrm{corr}}(0,1)$ that restores detailed balance for the transition from state $0$ to $1$.\n\nThe given parameters are:\nState space: $S = \\{0, 1, 2\\}$\nTarget distribution: $\\pi(0) = \\frac{1}{2}$, $\\pi(1) = \\frac{1}{3}$, $\\pi(2) = \\frac{1}{6}$\nClockwise proposal: $K^{+}(x,y)=1$ if $y \\equiv x+1 \\pmod{3}$\nCounterclockwise proposal: $K^{-}(x,y)=1$ if $y \\equiv x-1 \\pmod{3}$\nState-dependent mixture weights: $w(0)=\\frac{3}{4}$, $w(1)=\\frac{1}{2}$, $w(2)=\\frac{1}{4}$\nCombined proposal probability: $q(x,y)=w(x)K^{+}(x,y)+\\big(1-w(x)\\big)K^{-}(x,y)$\n\nThe fundamental condition for detailed balance (reversibility) states that for a Markov chain with stationary distribution $\\pi$ and transition probabilities $P(x,y)$, the rate of transition from state $x$ to $y$ must equal the rate of transition from $y$ to $x$ in equilibrium. For any pair of distinct states $x, y \\in S$, this is expressed as:\n$$ \\pi(x) P(x,y) = \\pi(y) P(y,x) $$\nFor a Metropolis-Hastings-type algorithm, the transition probability for $x \\neq y$ is given by $P(x,y) = q(x,y)\\alpha(x,y)$, where $q(x,y)$ is the proposal probability and $\\alpha(x,y)$ is the acceptance probability.\n\n### Part 1: Violation of Detailed Balance with the Naive Rule\n\nWe are asked to compute the ratio $\\rho = \\frac{\\pi(0)\\,q(0,1)\\,\\alpha_{\\mathrm{naive}}(0,1)}{\\pi(1)\\,q(1,0)\\,\\alpha_{\\mathrm{naive}}(1,0)}$ to show that detailed balance is broken for the pair of states $(0,1)$. Detailed balance holds if and only if this ratio is equal to $1$.\n\nFirst, we compute the necessary proposal probabilities $q(0,1)$ and $q(1,0)$.\nFor the transition $0 \\to 1$:\nThe proposed state $y=1$ is clockwise from the current state $x=0$, since $1 \\equiv 0+1 \\pmod{3}$. Thus, $K^{+}(0,1)=1$ and $K^{-}(0,1)=0$.\nThe proposal probability is:\n$q(0,1) = w(0)K^{+}(0,1) + (1-w(0))K^{-}(0,1) = w(0) \\cdot 1 + (1-w(0)) \\cdot 0 = w(0) = \\frac{3}{4}$.\n\nFor the transition $1 \\to 0$:\nThe proposed state $y=0$ is counterclockwise from the current state $x=1$, since $0 \\equiv 1-1 \\pmod{3}$. Thus, $K^{+}(1,0)=0$ and $K^{-}(1,0)=1$.\nThe proposal probability is:\n$q(1,0) = w(1)K^{+}(1,0) + (1-w(1))K^{-}(1,0) = w(1) \\cdot 0 + (1-w(1)) \\cdot 1 = 1-w(1) = 1-\\frac{1}{2} = \\frac{1}{2}$.\n\nNext, we compute the acceptance probabilities using the naive rule $\\alpha_{\\mathrm{naive}}(x,y)=\\min\\left(1,\\frac{\\pi(y)}{\\pi(x)}\\right)$.\nFor the transition $0 \\to 1$:\n$\\alpha_{\\mathrm{naive}}(0,1) = \\min\\left(1, \\frac{\\pi(1)}{\\pi(0)}\\right) = \\min\\left(1, \\frac{1/3}{1/2}\\right) = \\min\\left(1, \\frac{2}{3}\\right) = \\frac{2}{3}$.\n\nFor the transition $1 \\to 0$:\n$\\alpha_{\\mathrm{naive}}(1,0) = \\min\\left(1, \\frac{\\pi(0)}{\\pi(1)}\\right) = \\min\\left(1, \\frac{1/2}{1/3}\\right) = \\min\\left(1, \\frac{3}{2}\\right) = 1$.\n\nNow we can compute the numerator and denominator of $\\rho$.\nThe forward flux (proportional to probability flow from $0$ to $1$) is:\n$\\pi(0)\\,q(0,1)\\,\\alpha_{\\mathrm{naive}}(0,1) = \\frac{1}{2} \\cdot \\frac{3}{4} \\cdot \\frac{2}{3} = \\frac{6}{24} = \\frac{1}{4}$.\n\nThe reverse flux (proportional to probability flow from $1$ to $0$) is:\n$\\pi(1)\\,q(1,0)\\,\\alpha_{\\mathrm{naive}}(1,0) = \\frac{1}{3} \\cdot \\frac{1}{2} \\cdot 1 = \\frac{1}{6}$.\n\nThe ratio $\\rho$ is therefore:\n$$ \\rho = \\frac{1/4}{1/6} = \\frac{6}{4} = \\frac{3}{2} $$\nSince $\\rho = \\frac{3}{2} \\neq 1$, the forward and reverse fluxes are not balanced. This demonstrates that the naive acceptance rule, which ignores the asymmetry in the proposal distribution $q(x,y)$, fails to satisfy the detailed balance condition for the pair $(0,1)$.\n\n### Part 2: Derivation of the Correct Acceptance Probability\n\nTo restore detailed balance, the acceptance probability $\\alpha_{\\mathrm{corr}}(x,y)$ must be chosen such that for all $x \\neq y$:\n$$ \\pi(x) q(x,y) \\alpha_{\\mathrm{corr}}(x,y) = \\pi(y) q(y,x) \\alpha_{\\mathrm{corr}}(y,x) $$\nWe must find a solution for $\\alpha_{\\mathrm{corr}}(x,y)$ and $\\alpha_{\\mathrm{corr}}(y,x)$ that satisfies this equation while also adhering to the constraint that acceptance probabilities must be in the interval $[0,1]$. A standard method, which maximizes acceptance rates, is to set one of the probabilities to its maximum possible value, $1$.\n\nLet's rearrange the equation:\n$$ \\frac{\\alpha_{\\mathrm{corr}}(x,y)}{\\alpha_{\\mathrm{corr}}(y,x)} = \\frac{\\pi(y) q(y,x)}{\\pi(x) q(x,y)} $$\nDefine the Metropolis-Hastings ratio $R_{xy} = \\frac{\\pi(y) q(y,x)}{\\pi(x) q(x,y)}$.\nCase 1: $R_{xy} \\le 1$. We wish to maximize the acceptance probabilities. We can set the larger of the two, $\\alpha_{\\mathrm{corr}}(y,x)$, to its maximum value of $1$. The equation then dictates that $\\alpha_{\\mathrm{corr}}(x,y) = R_{xy}$. Since $R_{xy} \\le 1$, this choice is valid. So, $\\alpha_{\\mathrm{corr}}(x,y) = R_{xy}$ and $\\alpha_{\\mathrm{corr}}(y,x) = 1$.\n\nCase 2: $R_{xy} > 1$. In this case, $\\frac{1}{R_{xy}} < 1$. We can set $\\alpha_{\\mathrm{corr}}(x,y)$ to its maximum value, $1$. The equation then requires that $1 = R_{xy} \\cdot \\alpha_{\\mathrm{corr}}(y,x)$, which gives $\\alpha_{\\mathrm{corr}}(y,x) = \\frac{1}{R_{xy}}$. Since $\\frac{1}{R_{xy}} < 1$, this choice is valid. So, $\\alpha_{\\mathrm{corr}}(x,y) = 1$ and $\\alpha_{\\mathrm{corr}}(y,x) = \\frac{1}{R_{xy}}$.\n\nThese two cases can be expressed compactly using the minimum function:\n$$ \\alpha_{\\mathrm{corr}}(x,y) = \\min\\left(1, R_{xy}\\right) = \\min\\left(1, \\frac{\\pi(y) q(y,x)}{\\pi(x) q(x,y)}\\right) $$\nThis is the general form of the Metropolis-Hastings acceptance probability.\n\nWe now apply this formula to find the specific value of $\\alpha_{\\mathrm{corr}}(0,1)$. We need to compute the ratio $R_{01}$:\n$$ R_{01} = \\frac{\\pi(1) q(1,0)}{\\pi(0) q(0,1)} $$\nUsing the values we have already calculated:\n$\\pi(0) = \\frac{1}{2}$\n$\\pi(1) = \\frac{1}{3}$\n$q(0,1) = \\frac{3}{4}$\n$q(1,0) = \\frac{1}{2}$\n\nThe numerator is $\\pi(1) q(1,0) = \\frac{1}{3} \\cdot \\frac{1}{2} = \\frac{1}{6}$.\nThe denominator is $\\pi(0) q(0,1) = \\frac{1}{2} \\cdot \\frac{3}{4} = \\frac{3}{8}$.\n\nThe ratio is:\n$$ R_{01} = \\frac{1/6}{3/8} = \\frac{1}{6} \\cdot \\frac{8}{3} = \\frac{8}{18} = \\frac{4}{9} $$\nFinally, the correct acceptance probability for the transition from $0$ to $1$ is:\n$$ \\alpha_{\\mathrm{corr}}(0,1) = \\min\\left(1, \\frac{4}{9}\\right) = \\frac{4}{9} $$\nThis value, when used in the MCMC simulation, will ensure that the detailed balance condition is satisfied for the pair of states $(0,1)$.", "answer": "$$ \\boxed{\\frac{4}{9}} $$", "id": "3302655"}, {"introduction": "While ensuring detailed balance is fundamental to constructing a reversible MCMC sampler, not all samplers that satisfy this condition are created equal. The choice of acceptance rule directly impacts the algorithm's efficiency and convergence speed. This practice delves into this crucial aspect by comparing the standard Metropolis-Hastings acceptance rule to an alternative, Barker's rule, using the powerful concept of Peskun ordering to prove which method yields superior performance.", "problem": "Consider a target probability distribution with density $\\,\\pi(x)\\,$ on a measurable state space $\\,\\mathcal{X}\\,$ and a proposal Markov kernel with density $\\,q(x,y)\\,$ with respect to a common $\\sigma$-finite measure on $\\,\\mathcal{X}\\times\\mathcal{X}\\,$. A Markov chain Monte Carlo (MCMC) transition kernel based on proposals $\\,q(x,\\cdot)\\,$ and an acceptance function $\\,\\alpha(x,y)\\in[0,1]\\,$ has off-diagonal transition probability density $\\,P(x,y)=q(x,y)\\alpha(x,y)\\,$ for $\\,y\\neq x\\,$ and diagonal probability $\\,P(x,x)=1-\\int_{\\mathcal{X}\\setminus\\{x\\}} q(x,y)\\alpha(x,y)\\,dy\\,$, ensuring stochasticity.\n\nA chain is $\\pi$-reversible if it satisfies the detailed balance condition\n$$\n\\pi(x)P(x,y)=\\pi(y)P(y,x)\\quad\\text{for all }x\\neq y.\n$$\nFor Metropolis–Hastings type kernels, detailed balance reduces on off-diagonals to\n$$\n\\pi(x)q(x,y)\\,\\alpha(x,y)\\;=\\;\\pi(y)q(y,x)\\,\\alpha(y,x)\\quad\\text{for all }x\\neq y.\n$$\n\nDefine the Barker acceptance function\n$$\n\\alpha_B(x,y)\\;=\\;\\frac{\\pi(y)\\,q(y,x)}{\\pi(y)\\,q(y,x)+\\pi(x)\\,q(x,y)}\n$$\nand the Metropolis–Hastings acceptance function\n$$\n\\alpha_M(x,y)\\;=\\;\\min\\!\\left\\{\\,1\\,,\\,\\frac{\\pi(y)\\,q(y,x)}{\\pi(x)\\,q(x,y)}\\,\\right\\}.\n$$\nLet $\\,r(x,y):=\\frac{\\pi(y)\\,q(y,x)}{\\pi(x)\\,q(x,y)}\\,$ denote the Hastings ratio. The Peskun ordering states that for two $\\pi$-reversible Markov kernels $\\,P_1\\,$ and $\\,P_2\\,$, if $\\,P_1(x,y)\\ge P_2(x,y)\\,$ for all $\\,x\\neq y\\,$, then for any function $\\,f\\in L^2(\\pi)\\,$, the asymptotic variance of the ergodic average under $\\,P_1\\,$ is less than or equal to that under $\\,P_2\\,$.\n\nWhich of the following statements are correct?\n\nA. For any proposal density $\\,q(x,y)\\,$ and target density $\\,\\pi(x)\\,$ such that $\\,q(x,y)>0\\,$ implies $\\,q(y,x)>0\\,$, both $\\,\\alpha_B(x,y)\\,$ and $\\,\\alpha_M(x,y)\\,$ generate $\\pi$-reversible Markov kernels that satisfy the detailed balance condition.\n\nB. For all $\\,x\\neq y\\,$ and all pairs $\\,(\\pi,q)\\,$ with $\\,q(x,y)>0\\,$, one has $\\,q(x,y)\\alpha_M(x,y)\\ge q(x,y)\\alpha_B(x,y)\\,$; by Peskun ordering, the Metropolis–Hastings kernel has asymptotic variance no larger than the Barker kernel for the estimation of any $\\,f\\in L^2(\\pi)\\,$.\n\nC. There exist target–proposal pairs $\\,(\\pi,q)\\,$ for which the Barker acceptance can exceed the Metropolis–Hastings acceptance for some transitions, implying that Barker can yield smaller asymptotic variance than Metropolis–Hastings for some functions $\\,f\\in L^2(\\pi)\\,$.\n\nD. When the proposal is symmetric, i.e., $\\,q(x,y)=q(y,x)\\,$ for all $\\,x,y\\,$, Barker acceptance reduces to $\\,\\alpha_B(x,y)=\\frac{\\pi(y)}{\\pi(x)+\\pi(y)}\\,$ and Metropolis acceptance reduces to $\\,\\alpha_M(x,y)=\\min\\{1,\\frac{\\pi(y)}{\\pi(x)}\\}\\,$; in this setting Barker avoids truncation at $\\,1\\,$ and therefore is not Peskun-dominated by Metropolis–Hastings.\n\nE. Equality $\\,\\alpha_M(x,y)=\\alpha_B(x,y)\\,$ holds if and only if $\\,\\pi(y)\\,q(y,x)=0\\,$ or $\\,\\pi(x)\\,q(x,y)=0\\,$ (or both); otherwise $\\,\\alpha_M(x,y)>\\alpha_B(x,y)\\,$ for all transitions with finite, positive Hastings ratio $\\,r(x,y)\\in(0,\\infty)\\,$.", "solution": "The user wants me to act as a meticulous STEM professor to validate and solve a problem on Markov chain Monte Carlo (MCMC) methods, specifically comparing the Barker and Metropolis-Hastings acceptance functions.\n\n### Problem Validation\n\nFirst, I will validate the problem statement.\n\n**Step 1: Extract Givens**\n- Target probability density: $\\pi(x)$ on a measurable state space $\\mathcal{X}$.\n- Proposal Markov kernel density: $q(x,y)$.\n- MCMC transition kernel density (off-diagonal): $P(x,y) = q(x,y)\\alpha(x,y)$ for $y \\neq x$.\n- MCMC transition probability (diagonal): $P(x,x) = 1 - \\int_{\\mathcal{X}\\setminus\\{x\\}} q(x,y)\\alpha(x,y)\\,dy$.\n- Detailed balance condition (for $\\pi$-reversibility): $\\pi(x)P(x,y) = \\pi(y)P(y,x)$ for all $x \\neq y$.\n- Reduced detailed balance for Metropolis-Hastings type kernels: $\\pi(x)q(x,y)\\alpha(x,y) = \\pi(y)q(y,x)\\alpha(y,x)$ for all $x \\neq y$.\n- Barker acceptance function: $\\alpha_B(x,y) = \\frac{\\pi(y)q(y,x)}{\\pi(y)q(y,x) + \\pi(x)q(x,y)}$.\n- Metropolis-Hastings acceptance function: $\\alpha_M(x,y) = \\min\\!\\left\\{\\,1\\,,\\,\\frac{\\pi(y)q(y,x)}{\\pi(x)q(x,y)}\\,\\right\\}$.\n- Hastings ratio: $r(x,y) := \\frac{\\pi(y)q(y,x)}{\\pi(x)q(x,y)}$.\n- Peskun ordering principle: For two $\\pi$-reversible Markov kernels $P_1$ and $P_2$, if $P_1(x,y) \\ge P_2(x,y)$ for all $x \\neq y$, then the asymptotic variance of the ergodic average under $P_1$ is less than or equal to that under $P_2$ for any function $f \\in L^2(\\pi)$.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded**: The problem is firmly based on the established theory of MCMC methods. All definitions (detailed balance, acceptance functions, Peskun ordering) are standard and correctly stated.\n- **Well-Posed**: The problem is well-structured. The definitions provided are sufficient to analyze the assertions in the options.\n- **Objective**: The language is precise and free of subjective claims.\n\n**Step 3: Verdict and Action**\nThe problem statement is valid. I will proceed to the solution.\n\n### Solution Derivation\n\nThe analysis will proceed on an option-by-option basis. The Hastings ratio $r(x,y)$ is central to the comparison. Using its definition, the acceptance functions can be written as:\n$$\n\\alpha_M(x,y) = \\min\\{1, r(x,y)\\}\n$$\n$$\n\\alpha_B(x,y) = \\frac{\\pi(y)q(y,x)}{\\pi(y)q(y,x) + \\pi(x)q(x,y)} = \\frac{\\frac{\\pi(y)q(y,x)}{\\pi(x)q(x,y)}}{\\frac{\\pi(y)q(y,x)}{\\pi(x)q(x,y)} + 1} = \\frac{r(x,y)}{1+r(x,y)}\n$$\nWe must assume that where these ratios are used, the denominators are non-zero. The domain of $r(x,y)$ is $[0, \\infty]$.\n\n**Option A Evaluation**\n\nStatement A claims that both acceptance functions generate $\\pi$-reversible kernels satisfying the detailed balance condition, given that $q(x,y)>0$ implies $q(y,x)>0$. The detailed balance condition is $\\pi(x)q(x,y)\\alpha(x,y) = \\pi(y)q(y,x)\\alpha(y,x)$.\n\n1.  **Barker Acceptance $\\alpha_B$**:\n    - Left-hand side (LHS): $\\pi(x)q(x,y)\\alpha_B(x,y) = \\pi(x)q(x,y) \\left( \\frac{\\pi(y)q(y,x)}{\\pi(y)q(y,x) + \\pi(x)q(x,y)} \\right) = \\frac{\\pi(x)q(x,y)\\pi(y)q(y,x)}{\\pi(y)q(y,x) + \\pi(x)q(x,y)}$.\n    - Right-hand side (RHS): $\\pi(y)q(y,x)\\alpha_B(y,x)$. Note that $\\alpha_B(y,x) = \\frac{\\pi(x)q(x,y)}{\\pi(x)q(x,y) + \\pi(y)q(y,x)}$.\n    - RHS = $\\pi(y)q(y,x) \\left( \\frac{\\pi(x)q(x,y)}{\\pi(x)q(x,y) + \\pi(y)q(y,x)} \\right) = \\frac{\\pi(y)q(y,x)\\pi(x)q(x,y)}{\\pi(x)q(x,y) + \\pi(y)q(y,x)}$.\n    - LHS = RHS. The condition holds.\n\n2.  **Metropolis-Hastings Acceptance $\\alpha_M$**:\n    - Let $A = \\pi(x)q(x,y)$ and $B = \\pi(y)q(y,x)$. The condition is $A \\alpha_M(x,y) = B \\alpha_M(y,x)$. Note that $\\alpha_M(x,y) = \\min(1, B/A)$ and $\\alpha_M(y,x) = \\min(1, A/B)$.\n    - Case 1: $B/A \\le 1$. Then $\\alpha_M(x,y) = B/A$, and since $A/B \\ge 1$, $\\alpha_M(y,x) = 1$.\n        - LHS = $A(B/A) = B$.\n        - RHS = $B(1) = B$. Equality holds.\n    - Case 2: $B/A > 1$. Then $\\alpha_M(x,y) = 1$, and since $A/B < 1$, $\\alpha_M(y,x) = A/B$.\n        - LHS = $A(1) = A$.\n        - RHS = $B(A/B) = A$. Equality holds.\n    - The detailed balance condition is satisfied in all cases.\n\nThe provided condition $q(x,y)>0 \\implies q(y,x)>0$ is sufficient. If $q(x,y)>0$, then $q(y,x)>0$, and the argument above holds. If $q(x,y)=0$, the LHS of the detailed balance equation is $0$. The RHS involves $\\alpha(y,x)$, which depends on $r(y,x) = \\frac{\\pi(x)q(x,y)}{\\pi(y)q(y,x)}=0$ (assuming the denominator is non-zero). This gives $\\alpha_B(y,x)=0$ and $\\alpha_M(y,x)=0$, making the RHS zero as well. Thus, the condition holds universally.\n\n**Verdict for A: Correct**\n\n**Option B Evaluation**\n\nStatement B claims that $q(x,y)\\alpha_M(x,y) \\ge q(x,y)\\alpha_B(x,y)$ for all $x \\neq y$ (with $q(x,y)>0$), and this implies by Peskun ordering that the Metropolis-Hastings kernel has an asymptotic variance no larger than the Barker kernel.\n\n1.  **Inequality Verification**: Since $q(x,y)>0$, the inequality is equivalent to $\\alpha_M(x,y) \\ge \\alpha_B(x,y)$.\n    - We must check if $\\min\\{1, r\\} \\ge \\frac{r}{1+r}$ for all $r \\ge 0$.\n    - Case 1: $0 \\le r \\le 1$. We check if $r \\ge \\frac{r}{1+r}$. Multiplying by $(1+r) > 0$ gives $r(1+r) \\ge r$, which simplifies to $r^2+r \\ge r$, or $r^2 \\ge 0$. This is true.\n    - Case 2: $r > 1$. We check if $1 \\ge \\frac{r}{1+r}$. Multiplying by $(1+r) > 0$ gives $1+r \\ge r$, which simplifies to $1 \\ge 0$. This is also true.\n    - The inequality $\\alpha_M(x,y) \\ge \\alpha_B(x,y)$ holds for all $r(x,y) \\ge 0$.\n\n2.  **Peskun Ordering Application**:\n    - Let $P_M$ be the kernel with $\\alpha_M$ and $P_B$ be the kernel with $\\alpha_B$.\n    - The inequality $\\alpha_M \\ge \\alpha_B$ implies $P_M(x,y) = q(x,y)\\alpha_M(x,y) \\ge q(x,y)\\alpha_B(x,y) = P_B(x,y)$ for all $x \\neq y$.\n    - From A, both kernels are $\\pi$-reversible.\n    - The problem states the Peskun ordering principle: these conditions imply that the asymptotic variance for any $f \\in L^2(\\pi)$ under $P_M$ is less than or equal to that under $P_B$.\n    - The conclusion in statement B is a direct and correct application of the stated principle. Although a full proof of the variance inequality requires ergodicity, the statement claims the result \"by Peskun ordering\", which is a valid line of reasoning within standard MCMC theory where ergodicity is a design goal.\n\n**Verdict for B: Correct**\n\n**Option C Evaluation**\n\nStatement C claims that Barker acceptance can exceed Metropolis-Hastings acceptance for some transitions. This is a direct contradiction to the inequality $\\alpha_M(x,y) \\ge \\alpha_B(x,y)$ proven in the analysis of option B. That inequality holds for all possible values of the Hastings ratio $r(x,y) \\ge 0$. Therefore, there are no target-proposal pairs or transitions for which $\\alpha_B(x,y) > \\alpha_M(x,y)$. The premise is false, so the entire statement is incorrect.\n\n**Verdict for C: Incorrect**\n\n**Option D Evaluation**\n\nStatement D considers the special case of a symmetric proposal, $q(x,y)=q(y,x)$.\n1.  **Reduction of Acceptance Functions**:\n    - The Hastings ratio becomes $r(x,y) = \\frac{\\pi(y)q(y,x)}{\\pi(x)q(x,y)} = \\frac{\\pi(y)}{\\pi(x)}$.\n    - Barker: $\\alpha_B(x,y) = \\frac{r}{1+r} = \\frac{\\pi(y)/\\pi(x)}{1+\\pi(y)/\\pi(x)} = \\frac{\\pi(y)}{\\pi(x)+\\pi(y)}$. This is correct.\n    - Metropolis: $\\alpha_M(x,y) = \\min\\{1, r\\} = \\min\\{1, \\frac{\\pi(y)}{\\pi(x)}\\}$. This is also correct.\n2.  **Analysis of the Conclusion**:\n    - The statement claims that because Barker acceptance, $\\frac{\\pi(y)}{\\pi(x)+\\pi(y)}$, is always less than $1$ (it \"avoids truncation at $1$\"), it is \"therefore is not Peskun-dominated by Metropolis-Hastings\".\n    - This reasoning is fallacious. Peskun domination of $P_B$ by $P_M$ requires $P_M(x,y) \\ge P_B(x,y)$ for $x \\neq y$, which is equivalent to $\\alpha_M(x,y) \\ge \\alpha_B(x,y)$.\n    - As shown in the analysis of option B, this inequality holds universally, including for the symmetric case. The fact that $\\alpha_B(x,y) < 1$ while $\\alpha_M(x,y)$ can be $1$ is precisely a reason why the domination holds, not a reason why it fails. Metropolis-Hastings allows for a higher acceptance rate.\n\n**Verdict for D: Incorrect**\n\n**Option E Evaluation**\n\nStatement E provides a detailed analysis of the equality and inequality conditions between $\\alpha_M(x,y)$ and $\\alpha_B(x,y)$.\n\n1.  **Equality Condition**:\n    - We test for $\\alpha_M(x,y) = \\alpha_B(x,y)$, which is $\\min\\{1, r\\} = \\frac{r}{1+r}$.\n    - Case 1: $0 \\le r \\le 1$. Equality is $r = \\frac{r}{1+r}$, which simplifies to $r^2=0$, meaning $r=0$.\n    - Case 2: $r > 1$. Equality is $1 = \\frac{r}{1+r}$, which leads to $1+r=r$ or $1=0$, an impossibility. The equality holds in the limit as $r \\to \\infty$.\n    - Thus, equality holds only for $r=0$ or $r=\\infty$.\n    - $r(x,y) = \\frac{\\pi(y)q(y,x)}{\\pi(x)q(x,y)}$.\n    - $r=0$ corresponds to $\\pi(y)q(y,x)=0$ (assuming $\\pi(x)q(x,y) \\neq 0$).\n    - $r=\\infty$ corresponds to $\\pi(x)q(x,y)=0$ (assuming $\\pi(y)q(y,x) \\neq 0$).\n    - If both terms are zero, the ratio $r$ is undefined an takes the form $0/0$. However, in this case $P_M(x,y)=0$ and $P_B(x,y)=0$, so the resulting transition probabilities are equal. The statement about the equality of the acceptance functions themselves is technically imprecise for this $0/0$ case, but in spirit, the condition described leads to equal outcomes. The statement is correct under a practical interpretation.\n\n2.  **Strict Inequality Condition**:\n    - The statement claims that otherwise, for finite, positive Hastings ratio $r \\in (0, \\infty)$, we have $\\alpha_M(x,y) > \\alpha_B(x,y)$.\n    - Case 1: $0 < r \\le 1$. We check if $r > \\frac{r}{1+r}$, which is equivalent to $r^2 > 0$. This is true for $r>0$.\n    - Case 2: $r > 1$. We check if $1 > \\frac{r}{1+r}$, which is equivalent to $1+r > r$, or $1 > 0$. This is true.\n    - So, for all $r \\in (0, \\infty)$, the strict inequality $\\alpha_M(x,y) > \\alpha_B(x,y)$ holds. This part of the statement is correct.\n\nBoth parts of the statement accurately describe the relationship between the two functions.\n\n**Verdict for E: Correct**\n\n### Summary of Correct Statements\n\nThe analysis shows that statements A, B, and E are correct.\n- **A** correctly states that both methods satisfy detailed balance, a fundamental requirement.\n- **B** correctly states the Peskun superiority of Metropolis-Hastings over Barker, a key result in MCMC efficiency.\n- **E** correctly provides the detailed mathematical analysis of the inequality that underpins the result in B.\n\nD and C are demonstrably incorrect. The question asks which statements are correct, plural.", "answer": "$$\\boxed{ABE}$$", "id": "3302595"}]}