{"hands_on_practices": [{"introduction": "This practice guides you through the foundational derivation of the Metropolis-Hastings acceptance rule, starting from the principle of detailed balance. By applying this rule to a specific target distribution with an asymmetric proposal, you will gain hands-on experience in constructing the acceptance ratio [@problem_id:3336061]. The exercise culminates in demonstrating how Gibbs sampling emerges as a special case where the proposal distribution is chosen so intelligently that the acceptance probability simplifies to unity, cementing the core relationship between these two pivotal MCMC methods.", "problem": "Consider a Markov chain Monte Carlo transition of the Metropolis–Hastings (MH) type designed to leave a target distribution $\\,\\pi(x)\\,$ invariant on the real line. The proposal mechanism is characterized by a conditional density $\\,q(y \\mid x)\\,$ that may be asymmetric. The acceptance function $\\,a(x,y)\\,$, which satisfies $\\,0 \\leq a(x,y) \\leq 1\\,$, is to be chosen so that the chain is reversible with respect to $\\,\\pi\\,$ and hence has $\\,\\pi\\,$ as its stationary distribution. Starting from the principle of detailed balance for reversible Markov kernels, derive the explicit Hastings acceptance ratio $\\,r(x,y)\\,$ in terms of $\\,\\pi\\,$ and $\\,q\\,$, and thereby obtain the functional form of the acceptance probability $\\,a(x,y)\\,$.\n\nThen, instantiate your derivation for the following scientifically realistic setting:\n- The unnormalized target density is given by $\\,\\tilde{\\pi}(x) = \\exp\\!\\left(-\\frac{1}{4}x^{4} + \\frac{1}{2}x^{2}\\right)\\,$, which is log-concave in the tails and integrable on $\\,\\mathbb{R}\\,$. The normalized $\\,\\pi(x)\\,$ is proportional to $\\,\\tilde{\\pi}(x)\\,$.\n- The proposal is an additive drifted Gaussian random walk, $\\,q(y \\mid x) = \\mathcal{N}(y; x+\\delta, \\sigma^{2})\\,$ with fixed drift $\\,\\delta \\in \\mathbb{R}\\,$ and scale $\\,\\sigma > 0\\,$. Here $\\,\\mathcal{N}(y; m, s^{2})\\,$ denotes the normal density in $\\,y\\,$ with mean $\\,m\\,$ and variance $\\,s^{2}\\,$.\n\nProvide the closed-form expression for the Hastings ratio $\\,r(x,y)\\,$ for this specific $\\,\\tilde{\\pi}\\,$ and $\\,q\\,$, expressed only in terms of $\\,x\\,$, $\\,y\\,$, $\\,\\delta\\,$, and $\\,\\sigma\\,$. Finally, explain from first principles why symmetry of $\\,q\\,$ (for example, $\\,\\delta=0\\,$ in the above proposal) simplifies the MH acceptance rule, and briefly relate this simplification to the special case of Gibbs sampling, where proposals are drawn from full conditional distributions.\n\nYour final answer must be a single closed-form analytic expression for $\\,r(x,y)\\,$. No rounding is required, and no units are involved.", "solution": "The problem as stated is scientifically grounded, well-posed, and contains all necessary information for a complete and unique solution. It is a standard exercise in the theory of Markov Chain Monte Carlo (MCMC) methods. We will proceed with a solution.\n\nThe core principle underpinning the Metropolis–Hastings (MH) algorithm is the concept of detailed balance, which is a sufficient condition for a Markov chain's transition kernel to leave a target distribution $\\pi(x)$ invariant. For a discrete-time Markov process on a continuous state space $\\mathcal{X}$, with transition kernel density $K(y \\mid x)$, the detailed balance condition with respect to a stationary distribution $\\pi(x)$ is given by:\n$$\n\\pi(x) K(y \\mid x) = \\pi(y) K(x \\mid y) \\quad \\forall x,y \\in \\mathcal{X}\n$$\nIntegrating this equation over $x$ shows that if it holds, $\\pi$ is indeed a stationary distribution of the chain.\n\nThe Metropolis–Hastings algorithm constructs the transition kernel $K(y \\mid x)$ from two components: a proposal distribution a_priori designated by the conditional density $q(y \\mid x)$, and an acceptance probability $a(x,y)$. A transition from state $x$ to a new state $y$ occurs in two steps: first, a candidate state $y$ is proposed by drawing from $q(\\cdot \\mid x)$, and second, this proposed move is accepted with probability $a(x,y)$. For $x \\neq y$, the density of making a transition from $x$ to $y$ is the product of the density of proposing $y$ and the probability of accepting it:\n$$\nK(y \\mid x) = q(y \\mid x) a(x, y)\n$$\nSubstituting this form into the detailed balance equation yields:\n$$\n\\pi(x) q(y \\mid x) a(x, y) = \\pi(y) q(x \\mid y) a(y, x)\n$$\nwhere $a(y,x)$ is the probability of accepting a move from $y$ to $x$. This equation can be rearranged to isolate the ratio of the acceptance probabilities:\n$$\n\\frac{a(x, y)}{a(y, x)} = \\frac{\\pi(y) q(x \\mid y)}{\\pi(x) q(y \\mid x)}\n$$\nWe define the quantity on the right-hand side as the Hastings ratio, $r(x,y)$:\n$$\nr(x, y) \\equiv \\frac{\\pi(y) q(x \\mid y)}{\\pi(x) q(y \\mid x)}\n$$\nTo satisfy the detailed balance condition while maximizing the acceptance rate (and thus the efficiency of the chain), we select the functional form for $a(x,y)$ as proposed by Hastings:\n$$\na(x, y) = \\min(1, r(x, y))\n$$\nThis choice satisfies the constraint $0 \\leq a(x, y) \\leq 1$ and fulfills the ratio condition. For instance, if $r(x,y) \\geq 1$, then $a(x,y) = 1$. It follows that $r(y,x) = 1/r(x,y) \\leq 1$, so $a(y,x) = r(y,x)$. The ratio is $a(x,y)/a(y,x) = 1/r(y,x) = r(x,y)$, which is correct. A symmetric argument holds if $r(x,y) < 1$. This completes the general derivation of the acceptance probability.\n\nWe now instantiate this for the specific problem setting. The unnormalized target density is $\\tilde{\\pi}(x) = \\exp(-\\frac{1}{4}x^4 + \\frac{1}{2}x^2)$, and the proposal density is a drifted Gaussian, $q(y \\mid x) = \\mathcal{N}(y; x+\\delta, \\sigma^2)$. The Hastings ratio $r(x,y)$ can be computed using the unnormalized density $\\tilde{\\pi}(x)$, because any normalization constant $C$ in $\\pi(x) = C\\tilde{\\pi}(x)$ will cancel in the ratio $\\pi(y)/\\pi(x)$.\n$$\nr(x, y) = \\frac{\\tilde{\\pi}(y)}{\\tilde{\\pi}(x)} \\frac{q(x \\mid y)}{q(y \\mid x)}\n$$\nFirst, let's compute the ratio of the target densities:\n$$\n\\frac{\\tilde{\\pi}(y)}{\\tilde{\\pi}(x)} = \\frac{\\exp(-\\frac{1}{4}y^4 + \\frac{1}{2}y^2)}{\\exp(-\\frac{1}{4}x^4 + \\frac{1}{2}x^2)} = \\exp\\left(-\\frac{1}{4}(y^4 - x^4) + \\frac{1}{2}(y^2 - x^2)\\right)\n$$\nNext, we compute the ratio of the proposal densities. The proposal density from $x$ to $y$ is:\n$$\nq(y \\mid x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y - (x+\\delta))^2}{2\\sigma^2}\\right)\n$$\nThe reverse proposal density, from $y$ to $x$, is:\n$$\nq(x \\mid y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x - (y+\\delta))^2}{2\\sigma^2}\\right)\n$$\nThe ratio is therefore:\n$$\n\\frac{q(x \\mid y)}{q(y \\mid x)} = \\frac{\\exp\\left(-\\frac{(x - y - \\delta)^2}{2\\sigma^2}\\right)}{\\exp\\left(-\\frac{(y - x - \\delta)^2}{2\\sigma^2}\\right)} = \\exp\\left(\\frac{1}{2\\sigma^2} \\left[ (y - x - \\delta)^2 - (x - y - \\delta)^2 \\right] \\right)\n$$\nLet's simplify the term in the exponent. Using the identity $A^2 - B^2 = (A-B)(A+B)$:\n$$\n(y - x - \\delta)^2 - (x - y - \\delta)^2 = (-(x-y) - \\delta)^2 - (x-y-\\delta)^2 = ((x-y)+\\delta)^2 - ((x-y)-\\delta)^2\n$$\n$$\n= \\left[ ((x-y)+\\delta) - ((x-y)-\\delta) \\right] \\left[ ((x-y)+\\delta) + ((x-y)-\\delta) \\right]\n$$\n$$\n= [2\\delta] [2(x-y)] = 4\\delta(x-y)\n$$\nSubstituting this back into the exponent of the proposal ratio gives:\n$$\n\\frac{4\\delta(x-y)}{2\\sigma^2} = \\frac{2\\delta(x-y)}{\\sigma^2}\n$$\nSo, the proposal density ratio is:\n$$\n\\frac{q(x \\mid y)}{q(y \\mid x)} = \\exp\\left(\\frac{2\\delta(x-y)}{\\sigma^2}\\right)\n$$\nFinally, we combine the target and proposal ratios to obtain the full Hastings ratio $r(x,y)$:\n$$\nr(x, y) = \\exp\\left(-\\frac{1}{4}(y^4 - x^4) + \\frac{1}{2}(y^2 - x^2)\\right) \\exp\\left(\\frac{2\\delta(x-y)}{\\sigma^2}\\right)\n$$\n$$\nr(x, y) = \\exp\\left(-\\frac{1}{4}(y^4 - x^4) + \\frac{1}{2}(y^2 - x^2) + \\frac{2\\delta(x-y)}{\\sigma^2}\\right)\n$$\nThis is the closed-form expression for the Hastings ratio for the specified dynamics.\n\nNow, we consider the simplification that occurs when the proposal distribution $q$ is symmetric, i.e., $q(y \\mid x) = q(x \\mid y)$ for all $x,y$. In our specific example, this corresponds to setting the drift $\\delta = 0$. With a symmetric proposal, the ratio $q(x \\mid y) / q(y \\mid x)$ becomes exactly $1$. The Hastings ratio then simplifies to:\n$$\nr(x,y) = \\frac{\\pi(y)}{\\pi(x)}\n$$\nThe acceptance probability becomes $a(x, y) = \\min(1, \\pi(y)/\\pi(x))$. This is the acceptance rule of the original Metropolis algorithm, a special case of the Metropolis-Hastings algorithm. The simplification is that one no longer needs to compute the proposal density ratio, which can be computationally non-trivial, especially for complex proposal mechanisms.\n\nFinally, we relate this to Gibbs sampling. Gibbs sampling is an MCMC method for sampling from a multivariate distribution $\\pi(\\mathbf{x}) = \\pi(x_1, \\dots, x_d)$. A single Gibbs update consists of replacing one component, say $x_i$, with a new value $x_i'$ drawn from its full conditional distribution, $\\pi(x_i \\mid \\mathbf{x}_{-i})$, where $\\mathbf{x}_{-i}$ denotes all variables except $x_i$.\nWe can frame this procedure as a special case of the Metropolis-Hastings algorithm. The proposal for a new state $\\mathbf{x}' = (x_1, \\dots, x_i', \\dots, x_d)$ from state $\\mathbf{x}$ is made by drawing from the distribution $q(\\mathbf{x}' \\mid \\mathbf{x}) = \\pi(x_i' \\mid \\mathbf{x}_{-i})$. To find the acceptance probability, we compute the Hastings ratio:\n$$\nr(\\mathbf{x}, \\mathbf{x}') = \\frac{\\pi(\\mathbf{x}')}{\\pi(\\mathbf{x})} \\frac{q(\\mathbf{x} \\mid \\mathbf{x}')}{q(\\mathbf{x}' \\mid \\mathbf{x})}\n$$\nUsing the definition of conditional probability, $\\pi(\\mathbf{x}) = \\pi(x_i \\mid \\mathbf{x}_{-i}) \\pi(\\mathbf{x}_{-i})$. Thus, the ratio of the target densities is:\n$$\n\\frac{\\pi(\\mathbf{x}')}{\\pi(\\mathbf{x})} = \\frac{\\pi(x_i' \\mid \\mathbf{x}_{-i}') \\pi(\\mathbf{x}_{-i}')}{\\pi(x_i \\mid \\mathbf{x}_{-i}) \\pi(\\mathbf{x}_{-i})}\n$$\nSince only the $i$-th component changes, $\\mathbf{x}_{-i}' = \\mathbf{x}_{-i}$, which simplifies the ratio to:\n$$\n\\frac{\\pi(\\mathbf{x}')}{\\pi(\\mathbf{x})} = \\frac{\\pi(x_i' \\mid \\mathbf{x}_{-i}) \\pi(\\mathbf{x}_{-i})}{\\pi(x_i \\mid \\mathbf{x}_{-i}) \\pi(\\mathbf{x}_{-i})} = \\frac{\\pi(x_i' \\mid \\mathbf{x}_{-i})}{\\pi(x_i \\mid \\mathbf{x}_{-i})}\n$$\nThe reverse proposal density is $q(\\mathbf{x} \\mid \\mathbf{x}') = \\pi(x_i \\mid \\mathbf{x}_{-i}')$. Again, since $\\mathbf{x}_{-i}' = \\mathbf{x}_{-i}$, we have $q(\\mathbf{x} \\mid \\mathbf{x}') = \\pi(x_i \\mid \\mathbf{x}_{-i})$. The proposal ratio is:\n$$\n\\frac{q(\\mathbf{x} \\mid \\mathbf{x}')}{q(\\mathbf{x}' \\mid \\mathbf{x})} = \\frac{\\pi(x_i \\mid \\mathbf{x}_{-i})}{\\pi(x_i' \\mid \\mathbf{x}_{-i})}\n$$\nSubstituting these two ratios into the expression for $r(\\mathbf{x}, \\mathbf{x}')$:\n$$\nr(\\mathbf{x}, \\mathbf{x}') = \\left(\\frac{\\pi(x_i' \\mid \\mathbf{x}_{-i})}{\\pi(x_i \\mid \\mathbf{x}_{-i})}\\right) \\left(\\frac{\\pi(x_i \\mid \\mathbf{x}_{-i})}{\\pi(x_i' \\mid \\mathbf{x}_{-i})}\\right) = 1\n$$\nThe Hastings ratio is identically equal to $1$. Consequently, the acceptance probability $a(\\mathbf{x}, \\mathbf{x}') = \\min(1, 1) = 1$. This demonstrates that proposals drawn from the full conditional distribution are always accepted. Therefore, Gibbs sampling is an instance of the Metropolis-Hastings algorithm where the intelligent choice of a proposal distribution leads to an acceptance probability of $1$, obviating the need for an explicit rejection step. This is the ultimate simplification of the MH rule.", "answer": "$$\n\\boxed{\\exp\\left(-\\frac{1}{4}(y^{4} - x^{4}) + \\frac{1}{2}(y^{2} - x^{2}) + \\frac{2\\delta(x-y)}{\\sigma^{2}}\\right)}\n$$", "id": "3336061"}, {"introduction": "To make the abstract link between Gibbs sampling and the Metropolis-Hastings framework more tangible, this practice explores the classic case of a bivariate normal distribution. You will derive the famous Gaussian full conditional distributions and then formally treat a Gibbs update as a Metropolis-Hastings step, calculating its acceptance probability explicitly [@problem_id:3336066]. This exercise provides a clear, step-by-step algebraic confirmation that the Gibbs update is indeed an MH step with a guaranteed acceptance rate of $1$.", "problem": "Let $\\pi(x_{1}, x_{2})$ be a target probability density on $\\mathbb{R}^{2}$ given by a bivariate normal distribution with mean vector $0$ and covariance matrix $\\Sigma$, where\n$$\n\\Sigma \\;=\\; \\begin{pmatrix} \\sigma_{11} & \\sigma_{12} \\\\ \\sigma_{12} & \\sigma_{22} \\end{pmatrix},\n$$\nwith $\\sigma_{11} \\,>\\, 0$, $\\sigma_{22} \\,>\\, 0$, and $\\sigma_{11}\\sigma_{22} - \\sigma_{12}^{2} \\,>\\, 0$. Consider the corresponding Markov chain Monte Carlo procedure that alternates coordinate-wise updates using the full conditionals (Gibbs sampling). Starting from the fundamental definition of a multivariate normal density and the properties of quadratic forms, derive the full conditional distributions $\\pi(x_{1} \\mid x_{2})$ and $\\pi(x_{2} \\mid x_{1})$ by explicitly completing the square, and identify their Gaussian means and variances as functions of $\\sigma_{11}$, $\\sigma_{22}$, $\\sigma_{12}$, $x_{1}$, and $x_{2}$.\n\nThen, interpret a single-coordinate Gibbs update as a Metropolis-Hastings proposal within the Metropolis-Hastings (MH) algorithm framework, where the proposal for $x_{1}$ given $x_{2}$ is exactly $\\pi(\\cdot \\mid x_{2})$, and similarly for $x_{2}$ given $x_{1}$. Using only the definitions of the MH acceptance probability and conditional densities, derive the exact acceptance probability for such a proposal in terms of $\\pi$ and the proposal density, and simplify it to a numerical value that holds for all states.\n\nReport your answer as a single row matrix containing, in this order:\n- the mean of $\\pi(x_{1} \\mid x_{2})$,\n- the variance of $\\pi(x_{1} \\mid x_{2})$,\n- the mean of $\\pi(x_{2} \\mid x_{1})$,\n- the variance of $\\pi(x_{2} \\mid x_{1})$,\n- the MH acceptance probability for either coordinate update.\n\nNo rounding is required. The final answer must be a single closed-form analytic expression.", "solution": "This problem involves two main tasks: first, deriving the full conditional distributions for a bivariate normal, and second, showing that a Gibbs update based on these conditionals has a Metropolis-Hastings acceptance probability of 1.\n\n#### Derivation of Full Conditional Distributions\n\nThe probability density function (PDF) for a bivariate normal distribution with mean vector $\\boldsymbol{\\mu}=0$ and covariance matrix $\\Sigma$ is proportional to the exponential of a quadratic form:\n$$ \\pi(x_1, x_2) \\propto \\exp\\left(-\\frac{1}{2}\\begin{pmatrix} x_1 & x_2 \\end{pmatrix} \\Sigma^{-1} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}\\right) $$\nThe inverse of the covariance matrix $\\Sigma = \\begin{pmatrix} \\sigma_{11} & \\sigma_{12} \\\\ \\sigma_{12} & \\sigma_{22} \\end{pmatrix}$ is:\n$$ \\Sigma^{-1} = \\frac{1}{\\sigma_{11}\\sigma_{22} - \\sigma_{12}^2} \\begin{pmatrix} \\sigma_{22} & -\\sigma_{12} \\\\ -\\sigma_{12} & \\sigma_{11} \\end{pmatrix} $$\nThe term in the exponent is:\n$$ -\\frac{1}{2} \\mathbf{x}^T \\Sigma^{-1} \\mathbf{x} = -\\frac{1}{2(\\sigma_{11}\\sigma_{22} - \\sigma_{12}^2)} (\\sigma_{22}x_1^2 - 2\\sigma_{12}x_1x_2 + \\sigma_{11}x_2^2) $$\nTo find the full conditional distribution $\\pi(x_1 \\mid x_2)$, we treat all terms not involving $x_1$ as constants:\n$$ \\pi(x_1 \\mid x_2) \\propto \\exp\\left( -\\frac{1}{2(\\sigma_{11}\\sigma_{22} - \\sigma_{12}^2)} (\\sigma_{22}x_1^2 - 2\\sigma_{12}x_1x_2) \\right) $$\nWe complete the square for the terms in $x_1$:\n$$ \\sigma_{22}x_1^2 - 2\\sigma_{12}x_2 x_1 = \\sigma_{22}\\left(x_1^2 - 2\\frac{\\sigma_{12}}{\\sigma_{22}}x_2 x_1\\right) = \\sigma_{22}\\left( \\left(x_1 - \\frac{\\sigma_{12}}{\\sigma_{22}}x_2\\right)^2 - \\left(\\frac{\\sigma_{12}}{\\sigma_{22}}x_2\\right)^2 \\right) $$\nSubstituting this back and dropping terms constant in $x_1$ from the proportionality, we get:\n$$ \\pi(x_1 \\mid x_2) \\propto \\exp\\left( -\\frac{\\sigma_{22}}{2(\\sigma_{11}\\sigma_{22} - \\sigma_{12}^2)} \\left(x_1 - \\frac{\\sigma_{12}}{\\sigma_{22}}x_2\\right)^2 \\right) $$\nThis is the kernel of a normal distribution for $x_1$, which has the form $\\exp\\left(-\\frac{(x_1-\\mu)^2}{2\\sigma^2}\\right)$. By comparing the forms, we identify the mean and variance:\n- Mean: $E[x_1 \\mid x_2] = \\frac{\\sigma_{12}}{\\sigma_{22}}x_2$\n- Variance: $\\mathrm{Var}(x_1 \\mid x_2) = \\frac{\\sigma_{11}\\sigma_{22} - \\sigma_{12}^2}{\\sigma_{22}} = \\sigma_{11} - \\frac{\\sigma_{12}^2}{\\sigma_{22}}$\n\nBy the symmetry of the problem, we can find the parameters for $\\pi(x_2 \\mid x_1)$ by swapping the indices $1 \\leftrightarrow 2$:\n- Mean: $E[x_2 \\mid x_1] = \\frac{\\sigma_{12}}{\\sigma_{11}}x_1$\n- Variance: $\\mathrm{Var}(x_2 \\mid x_1) = \\sigma_{22} - \\frac{\\sigma_{12}^2}{\\sigma_{11}}$\n\n#### Derivation of the Metropolis-Hastings Acceptance Probability\n\nNow, let's treat a Gibbs update of $x_1$ as a Metropolis-Hastings (MH) step. The state moves from $\\mathbf{x}=(x_1, x_2)$ to $\\mathbf{x}'=(x_1', x_2)$. The proposal distribution for the new component $x_1'$ is the full conditional distribution:\n$$ q(\\mathbf{x}' \\mid \\mathbf{x}) = \\pi(x_1' \\mid x_2) $$\nThe reverse proposal, from $\\mathbf{x}'$ back to $\\mathbf{x}$, is similarly:\n$$ q(\\mathbf{x} \\mid \\mathbf{x}') = \\pi(x_1 \\mid x_2) $$\nThe MH acceptance probability $\\alpha$ is determined by the ratio:\n$$ R = \\frac{\\pi(\\mathbf{x}') q(\\mathbf{x}\\mid\\mathbf{x}')}{\\pi(\\mathbf{x}) q(\\mathbf{x}'\\mid\\mathbf{x})} = \\frac{\\pi(x_1', x_2) \\pi(x_1 \\mid x_2)}{\\pi(x_1, x_2) \\pi(x_1' \\mid x_2)} $$\nUsing the definition of conditional probability, $\\pi(A,B) = \\pi(A\\mid B)\\pi(B)$, we can write:\n- $\\pi(x_1, x_2) = \\pi(x_1 \\mid x_2) \\pi(x_2)$\n- $\\pi(x_1', x_2) = \\pi(x_1' \\mid x_2) \\pi(x_2)$\nSubstituting these into the ratio $R$:\n$$ R = \\frac{\\big(\\pi(x_1' \\mid x_2) \\pi(x_2)\\big) \\cdot \\pi(x_1 \\mid x_2)}{\\big(\\pi(x_1 \\mid x_2) \\pi(x_2)\\big) \\cdot \\pi(x_1' \\mid x_2)} = 1 $$\nEvery term in the numerator cancels with a corresponding term in the denominator. The acceptance probability is therefore:\n$$ \\alpha = \\min(1, R) = \\min(1, 1) = 1 $$\nThis proves that a Gibbs sampling step is a special case of the MH algorithm where the acceptance probability is always 1. A symmetric argument holds for updating $x_2$.\n\nThe five requested quantities are thus:\n1. Mean of $\\pi(x_1 \\mid x_2)$: $\\frac{\\sigma_{12}}{\\sigma_{22}}x_2$\n2. Variance of $\\pi(x_1 \\mid x_2)$: $\\sigma_{11} - \\frac{\\sigma_{12}^2}{\\sigma_{22}}$\n3. Mean of $\\pi(x_2 \\mid x_1)$: $\\frac{\\sigma_{12}}{\\sigma_{11}}x_1$\n4. Variance of $\\pi(x_2 \\mid x_1)$: $\\sigma_{22} - \\frac{\\sigma_{12}^2}{\\sigma_{11}}$\n5. MH acceptance probability: $1$", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{\\sigma_{12}}{\\sigma_{22}} x_2 & \\sigma_{11} - \\frac{\\sigma_{12}^{2}}{\\sigma_{22}} & \\frac{\\sigma_{12}}{\\sigma_{11}} x_1 & \\sigma_{22} - \\frac{\\sigma_{12}^{2}}{\\sigma_{11}} & 1\n\\end{pmatrix}\n}\n$$", "id": "3336066"}, {"introduction": "This final practice showcases the practical power of the unified MH-Gibbs perspective by tackling a standard Bayesian logistic regression model. In many real-world models, some parameter conditionals are tractable for Gibbs sampling while others are not, a challenge you will address here [@problem_id:3336126]. By designing a hybrid \"Metropolis-within-Gibbs\" sampler, you will learn how to combine the strengths of both algorithms, a cornerstone technique for efficient sampling in modern computational statistics.", "problem": "Consider a binary response model with observations $\\{(y_{i}, x_{i})\\}_{i=1}^{n}$, where $y_{i} \\in \\{0,1\\}$ and $x_{i} \\in \\mathbb{R}^{p}$. The likelihood is given by a logistic regression:\n- $y_{i} \\mid \\beta \\sim \\mathrm{Bernoulli}(\\sigma(x_{i}^{\\top}\\beta))$ independently for $i=1,\\dots,n$, where $\\sigma(t) = \\frac{1}{1+\\exp(-t)}$.\n\nAssume a Gaussian prior with a global variance hyperparameter:\n- $\\beta \\mid \\tau^{2} \\sim \\mathcal{N}\\!\\left(0, \\tau^{2}\\Sigma_{0}\\right)$ with known positive definite matrix $\\Sigma_{0} \\in \\mathbb{R}^{p \\times p}$ and unknown $\\tau^{2} > 0$.\n- $\\tau^{2} \\sim \\mathrm{Inverse-Gamma}(a_{0}, b_{0})$ with density $p(\\tau^{2}) = \\dfrac{b_{0}^{a_{0}}}{\\Gamma(a_{0})} (\\tau^{2})^{-(a_{0}+1)} \\exp\\!\\left(-\\dfrac{b_{0}}{\\tau^{2}}\\right)$ for $a_{0} > 0$ and $b_{0} > 0$.\n\nTasks:\n- Identify which full conditional distributions in this model are available in closed form and which are not.\n- Design a Metropolis-within-Gibbs sampler that alternates between updates for $\\tau^{2}$ and $\\beta$.\n- For the $\\beta$-update, suppose you employ a Gaussian random-walk proposal $\\beta^{\\star} \\sim \\mathcal{N}(\\beta, \\Sigma_{\\mathrm{prop}})$ with a fixed symmetric positive definite matrix $\\Sigma_{\\mathrm{prop}} \\in \\mathbb{R}^{p \\times p}$. Derive a closed-form expression for the Metropolis-Hastings acceptance probability $\\alpha(\\beta \\to \\beta^{\\star} \\mid \\tau^{2}, X, y)$ as a function of $(X, y, \\beta, \\beta^{\\star}, \\tau^{2}, \\Sigma_{0})$, where $X$ is the $n \\times p$ design matrix with rows $x_{i}^{\\top}$.\n\nProvide your final answer as a single closed-form analytical expression for the acceptance probability. Do not approximate or round your answer.", "solution": "The problem statement is first validated for scientific and formal correctness.\n\n### Step 1: Extract Givens\n-   **Observations**: $\\{(y_{i}, x_{i})\\}_{i=1}^{n}$, where $y_{i} \\in \\{0,1\\}$ is a binary response and $x_{i} \\in \\mathbb{R}^{p}$ is a vector of predictors.\n-   **Likelihood Model**: $y_{i} \\mid \\beta \\sim \\mathrm{Bernoulli}(\\pi_i)$ independently for $i=1,\\dots,n$, with $\\pi_i = \\sigma(x_{i}^{\\top}\\beta)$. The logistic function is defined as $\\sigma(t) = \\frac{1}{1+\\exp(-t)}$.\n-   **Prior on $\\beta$**: $\\beta \\mid \\tau^{2} \\sim \\mathcal{N}\\!\\left(0, \\tau^{2}\\Sigma_{0}\\right)$, where $\\Sigma_{0} \\in \\mathbb{R}^{p \\times p}$ is a known positive definite matrix.\n-   **Hyperprior on $\\tau^2$**: $\\tau^{2} \\sim \\mathrm{Inverse-Gamma}(a_{0}, b_{0})$, with known shape $a_{0} > 0$ and rate $b_{0} > 0$. The probability density function is given as $p(\\tau^{2}) = \\dfrac{b_{0}^{a_{0}}}{\\Gamma(a_{0})} (\\tau^{2})^{-(a_{0}+1)} \\exp\\!\\left(-\\dfrac{b_{0}}{\\tau^{2}}\\right)$.\n-   **Metropolis-Hastings Proposal**: For updating $\\beta$, a Gaussian random-walk proposal is used: $\\beta^{\\star} \\sim \\mathcal{N}(\\beta, \\Sigma_{\\mathrm{prop}})$, where $\\Sigma_{\\mathrm{prop}} \\in \\mathbb{R}^{p \\times p}$ is a fixed, symmetric, positive definite matrix.\n-   **Tasks**:\n    1.  Identify which full conditional distributions are available in closed form.\n    2.  Design a Metropolis-within-Gibbs sampler.\n    3.  Derive the Metropolis-Hastings acceptance probability $\\alpha(\\beta \\to \\beta^{\\star} \\mid \\tau^{2}, X, y)$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem describes a standard Bayesian hierarchical model for logistic regression. All components—the logistic likelihood, Gaussian prior, and Inverse-Gamma hyperprior—are well-established in statistics and machine learning. The tasks are concrete, mathematically well-defined, and follow standard procedures for constructing MCMC samplers. The problem is self-contained, with all necessary distributions and parameters specified. It is objective, scientifically grounded, and well-posed.\n\n### Step 3: Verdict and Action\nThe problem is valid. A full solution will be provided.\n\n### Solution Derivations\n\nThe joint posterior distribution of the parameters $(\\beta, \\tau^2)$ is given by Bayes' theorem:\n$$p(\\beta, \\tau^2 \\mid y, X) \\propto p(y \\mid \\beta, X) p(\\beta \\mid \\tau^2) p(\\tau^2)$$\nwhere $y = (y_1, \\dots, y_n)^\\top$ and $X$ is the $n \\times p$ matrix with rows $x_i^\\top$.\n\n**Task 1: Full Conditional Distributions**\n\nWe analyze the full conditional for each parameter block, $\\beta$ and $\\tau^2$.\n\n1.  **Full conditional for $\\beta$**:\n    The full conditional density for $\\beta$ is proportional to the terms in the joint posterior that involve $\\beta$:\n    $$p(\\beta \\mid \\tau^2, y, X) \\propto p(y \\mid \\beta, X) p(\\beta \\mid \\tau^2)$$\n    The likelihood term is $p(y \\mid \\beta, X) = \\prod_{i=1}^{n} \\sigma(x_{i}^{\\top}\\beta)^{y_{i}} (1-\\sigma(x_{i}^{\\top}\\beta))^{1-y_{i}}$. The prior term is $p(\\beta \\mid \\tau^2) \\propto \\exp\\left(-\\frac{1}{2\\tau^2}\\beta^\\top\\Sigma_0^{-1}\\beta\\right)$.\n    Combining these gives:\n    $$p(\\beta \\mid \\tau^2, y, X) \\propto \\left( \\prod_{i=1}^{n} \\frac{(\\exp(x_i^\\top\\beta))^{y_i}}{1+\\exp(x_i^\\top\\beta)} \\right) \\exp\\left(-\\frac{1}{2\\tau^2}\\beta^\\top\\Sigma_0^{-1}\\beta\\right)$$\n    The product of the logistic likelihood terms and the Gaussian prior density does not result in a standard, recognizable distribution from which we can sample directly. Therefore, the full conditional distribution for $\\beta$ is **not available in closed form**.\n\n2.  **Full conditional for $\\tau^2$**:\n    The full conditional for $\\tau^2$ is proportional to terms involving $\\tau^2$:\n    $$p(\\tau^2 \\mid \\beta, y, X) \\propto p(\\beta \\mid \\tau^2) p(\\tau^2)$$\n    Note that the likelihood $p(y \\mid \\beta, X)$ is not a function of $\\tau^2$.\n    The density of the prior $p(\\beta \\mid \\tau^2)$ is:\n    $$p(\\beta \\mid \\tau^2) = (2\\pi)^{-p/2} |\\tau^2\\Sigma_0|^{-1/2} \\exp\\left(-\\frac{1}{2}\\beta^\\top(\\tau^2\\Sigma_0)^{-1}\\beta\\right) \\propto (\\tau^2)^{-p/2} \\exp\\left(-\\frac{\\beta^\\top\\Sigma_0^{-1}\\beta}{2\\tau^2}\\right)$$\n    The density of the hyperprior $p(\\tau^2)$ is:\n    $$p(\\tau^2) \\propto (\\tau^2)^{-(a_0+1)} \\exp\\left(-\\frac{b_0}{\\tau^2}\\right)$$\n    Multiplying these two expressions gives the kernel of the full conditional for $\\tau^2$:\n    $$p(\\tau^2 \\mid \\beta) \\propto (\\tau^2)^{-p/2} \\exp\\left(-\\frac{\\beta^\\top\\Sigma_0^{-1}\\beta}{2\\tau^2}\\right) \\cdot (\\tau^2)^{-(a_0+1)} \\exp\\left(-\\frac{b_0}{\\tau^2}\\right)$$\n    $$p(\\tau^2 \\mid \\beta) \\propto (\\tau^2)^{-(a_0+p/2+1)} \\exp\\left(-\\frac{1}{\\tau^2}\\left(b_0 + \\frac{1}{2}\\beta^\\top\\Sigma_0^{-1}\\beta\\right)\\right)$$\n    This is the kernel of an Inverse-Gamma distribution. Thus, the full conditional for $\\tau^2$ **is available in closed form**:\n    $$\\tau^2 \\mid \\beta, y, X \\sim \\mathrm{Inverse-Gamma}\\left(a_0 + \\frac{p}{2},\\ b_0 + \\frac{1}{2}\\beta^\\top\\Sigma_0^{-1}\\beta\\right)$$\n\n**Task 2: Metropolis-within-Gibbs Sampler Design**\n\nA hybrid sampler is required, combining a Gibbs step for $\\tau^2$ and a Metropolis-Hastings step for $\\beta$. Let $(\\beta^{(k)}, (\\tau^2)^{(k)})$ be the state of the chain at iteration $k$. The update to iteration $k+1$ proceeds as follows:\n\n1.  **Gibbs Step for $\\tau^2$**: Sample $(\\tau^2)^{(k+1)}$ from its full conditional distribution, given the current value of $\\beta^{(k)}$:\n    $$(\\tau^2)^{(k+1)} \\sim \\mathrm{Inverse-Gamma}\\left(a_0 + \\frac{p}{2}, b_0 + \\frac{1}{2}(\\beta^{(k)})^\\top\\Sigma_0^{-1}\\beta^{(k)}\\right)$$\n2.  **Metropolis-Hastings Step for $\\beta$**: Update $\\beta$ using the newly sampled $(\\tau^2)^{(k+1)}$.\n    a. **Propose**: Generate a candidate value $\\beta^\\star$ from the specified random-walk proposal distribution:\n    $$\\beta^\\star \\sim \\mathcal{N}\\left(\\beta^{(k)}, \\Sigma_{\\mathrm{prop}}\\right)$$\n    b. **Compute Acceptance Probability**: Calculate the acceptance probability $\\alpha = \\alpha(\\beta^{(k)} \\to \\beta^\\star \\mid (\\tau^2)^{(k+1)}, y, X)$. The derivation is detailed in the next section.\n    c. **Accept or Reject**: Generate a random number $u \\sim \\mathrm{Uniform}(0,1)$.\n    If $u < \\alpha$, accept the proposal: $\\beta^{(k+1)} = \\beta^\\star$.\n    Otherwise, reject the proposal: $\\beta^{(k+1)} = \\beta^{(k)}$.\n\n**Task 3: Derivation of the Acceptance Probability**\n\nThe Metropolis-Hastings acceptance probability $\\alpha(\\beta \\to \\beta^\\star)$ is given by:\n$$\\alpha = \\min\\left(1, \\frac{p(\\beta^\\star \\mid \\tau^2, y, X)q(\\beta \\mid \\beta^\\star)}{p(\\beta \\mid \\tau^2, y, X)q(\\beta^\\star \\mid \\beta)}\\right)$$\nHere, the target distribution $p(\\cdot \\mid \\tau^2, y, X)$ is the full conditional for $\\beta$, and $q(\\cdot \\mid \\cdot)$ is the proposal distribution.\n\nThe proposal is a Gaussian random walk, $q(\\beta^\\star \\mid \\beta) = f(\\beta^\\star; \\beta, \\Sigma_{\\mathrm{prop}})$, where $f(\\cdot; \\mu, \\Sigma)$ is the multivariate normal PDF. Since $\\Sigma_{\\mathrm{prop}}$ is symmetric, the normal density is symmetric with respect to its mean, meaning $f(\\beta^\\star; \\beta, \\Sigma_{\\mathrm{prop}}) = f(\\beta; \\beta^\\star, \\Sigma_{\\mathrm{prop}})$. Thus, $q(\\beta^\\star \\mid \\beta) = q(\\beta \\mid \\beta^\\star)$, and the proposal ratio simplifies to $1$.\n\nThe acceptance probability becomes the ratio of the target densities:\n$$\\alpha = \\min\\left(1, \\frac{p(\\beta^\\star \\mid \\tau^2, y, X)}{p(\\beta \\mid \\tau^2, y, X)}\\right)$$\nLet $R$ denote the ratio. As $p(\\beta \\mid \\tau^2, y, X) \\propto p(y \\mid \\beta, X) p(\\beta \\mid \\tau^2)$, the ratio is:\n$$R = \\frac{p(y \\mid \\beta^\\star, X) p(\\beta^\\star \\mid \\tau^2)}{p(y \\mid \\beta, X) p(\\beta \\mid \\tau^2)}$$\nLet's evaluate the likelihood ratio and the prior ratio separately.\n\n-   **Likelihood Ratio**:\n    $$p(y \\mid \\beta, X) = \\prod_{i=1}^{n} p(y_i \\mid \\beta, x_i) = \\prod_{i=1}^{n} \\frac{\\exp(y_i x_i^\\top\\beta)}{1+\\exp(x_i^\\top\\beta)}$$\n    The ratio is:\n    $$\\frac{p(y \\mid \\beta^\\star, X)}{p(y \\mid \\beta, X)} = \\prod_{i=1}^{n} \\frac{\\exp(y_i x_i^\\top\\beta^\\star) / (1+\\exp(x_i^\\top\\beta^\\star))}{\\exp(y_i x_i^\\top\\beta) / (1+\\exp(x_i^\\top\\beta))} = \\left(\\prod_{i=1}^{n} \\exp(y_i x_i^\\top(\\beta^\\star - \\beta))\\right) \\left(\\prod_{i=1}^{n} \\frac{1+\\exp(x_i^\\top\\beta)}{1+\\exp(x_i^\\top\\beta^\\star)}\\right)$$\n    The first part can be written using matrix notation as $\\exp(\\sum_{i=1}^{n} y_i x_i^\\top(\\beta^\\star - \\beta)) = \\exp(y^\\top X(\\beta^\\star - \\beta))$.\n\n-   **Prior Ratio**:\n    $$p(\\beta \\mid \\tau^2) \\propto \\exp\\left(-\\frac{1}{2\\tau^2}\\beta^\\top\\Sigma_0^{-1}\\beta\\right)$$\n    The ratio is:\n    $$\\frac{p(\\beta^\\star \\mid \\tau^2)}{p(\\beta \\mid \\tau^2)} = \\frac{\\exp\\left(-\\frac{1}{2\\tau^2}(\\beta^\\star)^\\top\\Sigma_0^{-1}\\beta^\\star\\right)}{\\exp\\left(-\\frac{1}{2\\tau^2}\\beta^\\top\\Sigma_0^{-1}\\beta\\right)} = \\exp\\left(-\\frac{1}{2\\tau^2}\\left((\\beta^\\star)^\\top\\Sigma_0^{-1}\\beta^\\star - \\beta^\\top\\Sigma_0^{-1}\\beta\\right)\\right)$$\n    This can be rewritten as $\\exp\\left(\\frac{1}{2\\tau^2}\\left(\\beta^\\top\\Sigma_0^{-1}\\beta - (\\beta^\\star)^\\top\\Sigma_0^{-1}\\beta^\\star\\right)\\right)$.\n\n-   **Combined Ratio**:\n    Multiplying the likelihood and prior ratios gives $R$:\n    $$R = \\exp\\left(y^\\top X(\\beta^\\star - \\beta)\\right) \\left(\\prod_{i=1}^{n} \\frac{1+\\exp(x_i^\\top\\beta)}{1+\\exp(x_i^\\top\\beta^\\star)}\\right) \\exp\\left(\\frac{1}{2\\tau^2}\\left(\\beta^\\top\\Sigma_0^{-1}\\beta - (\\beta^\\star)^\\top\\Sigma_0^{-1}\\beta^\\star\\right)\\right)$$\n    Combining the exponential terms:\n    $$R = \\exp\\left(y^\\top X(\\beta^\\star - \\beta) + \\frac{1}{2\\tau^2}\\left(\\beta^\\top\\Sigma_0^{-1}\\beta - (\\beta^\\star)^\\top\\Sigma_0^{-1}\\beta^\\star\\right)\\right) \\prod_{i=1}^{n} \\frac{1+\\exp(x_i^\\top\\beta)}{1+\\exp(x_i^\\top\\beta^\\star)}$$\nThe acceptance probability is $\\alpha = \\min\\{1, R\\}$. This expression is a function of the givens $(X, y, \\beta, \\beta^{\\star}, \\tau^{2}, \\Sigma_{0})$, as required.", "answer": "$$\\boxed{\\min\\left\\{1, \\exp\\left(y^{\\top}X(\\beta^{\\star} - \\beta) + \\frac{1}{2\\tau^2}\\left(\\beta^{\\top}\\Sigma_{0}^{-1}\\beta - (\\beta^{\\star})^{\\top}\\Sigma_{0}^{-1}\\beta^{\\star}\\right)\\right) \\prod_{i=1}^{n}\\frac{1+\\exp(x_{i}^{\\top}\\beta)}{1+\\exp(x_{i}^{\\top}\\beta^{\\star})}\\right\\}}$$", "id": "3336126"}]}