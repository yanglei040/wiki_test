{"hands_on_practices": [{"introduction": "The heart of the independence sampler, like any Metropolis-Hastings method, is the acceptance probability $\\alpha(x,y)$ that governs whether a proposed state is accepted. This first exercise takes you back to first principles, asking you to derive this probability directly from the detailed balance condition rather than simply applying a pre-packaged formula [@problem_id:3354112]. By calculating the acceptance probability for a specific target and proposal, you will gain a concrete understanding of how the sampler makes its decisions on a step-by-step basis.", "problem": "Consider the independence sampler, a special case of the Metropolis–Hastings algorithm within Markov chain Monte Carlo (MCMC), targeting a distribution on $\\mathbb{R}$ with unnormalized density $\\pi(x) \\propto \\exp(-x^{4}/2)$. The proposal distribution is independent of the current state and given by the standard normal density $g(x) = (2\\pi)^{-1/2} \\exp(-x^{2}/2)$. A proposed move from a current state $x \\in \\mathbb{R}$ to a candidate $y \\in \\mathbb{R}$ is accepted with probability $\\alpha(x,y)$, where the construction must satisfy detailed balance with respect to $\\pi$.\n\nStarting from the detailed balance requirement and the general principles of Metropolis–Hastings (without using any pre-derived acceptance formula), determine the acceptance probability $\\alpha(x,y)$ for the independence sampler. Then, evaluate this acceptance probability at the specific values $x = \\tfrac{1}{2}$ and $y = \\tfrac{3}{2}$. Provide the exact value in closed form; do not approximate or round.", "solution": "The problem requires the derivation of the acceptance probability for an independence sampler and its subsequent evaluation at specific points. The derivation must start from the principle of detailed balance.\n\nLet the target probability density function be $p(x)$. We are given its unnormalized form, $\\pi(x) \\propto \\exp(-x^{4}/2)$. The normalized density is $p(x) = \\frac{1}{Z}\\pi(x)$, where $Z$ is a normalization constant. The proposal distribution is given by the density $g(x) = (2\\pi)^{-1/2} \\exp(-x^{2}/2)$, which is the standard normal distribution. Since this is an independence sampler, the proposal for a new state $y$ is drawn from $g(y)$ independently of the current state $x$. Thus, the transition proposal density from $x$ to $y$ is $q(y|x) = g(y)$.\n\nThe Metropolis-Hastings algorithm generates a Markov chain whose stationary distribution is the target distribution $p(x)$. A sufficient condition for this is the detailed balance condition, which states that for any two states $x$ and $y$:\n$$\np(x) K(y|x) = p(y) K(x|y)\n$$\nwhere $K(y|x)$ is the overall transition probability (or density) of moving from state $x$ to state $y$. For $y \\neq x$, the transition kernel is the product of proposing the move and accepting it:\n$$\nK(y|x) = q(y|x) \\alpha(x,y)\n$$\nHere, $q(y|x)$ is the proposal density and $\\alpha(x,y)$ is the acceptance probability. Substituting this into the detailed balance equation gives:\n$$\np(x) q(y|x) \\alpha(x,y) = p(y) q(x|y) \\alpha(y,x)\n$$\nWe can use the unnormalized density $\\pi(x)$ because the normalization constant $Z$ cancels out:\n$$\n\\frac{\\pi(x)}{Z} q(y|x) \\alpha(x,y) = \\frac{\\pi(y)}{Z} q(x|y) \\alpha(y,x)\n$$\n$$\n\\pi(x) q(y|x) \\alpha(x,y) = \\pi(y) q(x|y) \\alpha(y,x)\n$$\nThis equation relates the acceptance probabilities for the forward move ($x \\to y$) and the reverse move ($y \\to x$):\n$$\n\\frac{\\alpha(x,y)}{\\alpha(y,x)} = \\frac{\\pi(y) q(x|y)}{\\pi(x) q(y|x)}\n$$\nTo satisfy this condition, a common and effective choice, proposed by Hastings, is:\n$$\n\\alpha(x,y) = \\min\\left(1, \\frac{\\pi(y) q(x|y)}{\\pi(x) q(y|x)}\\right)\n$$\nThis choice for $\\alpha(x,y)$ (and symmetrically, $\\alpha(y,x)$) ensures the detailed balance equation holds. To verify, if the ratio $r = \\frac{\\pi(y) q(x|y)}{\\pi(x) q(y|x)} \\le 1$, then $\\alpha(x,y) = r$ and $\\alpha(y,x) = \\min(1, 1/r) = 1$. The detailed balance equation becomes $\\pi(x) q(y|x) \\cdot r = \\pi(y) q(x|y) \\cdot 1$, which simplifies to $\\pi(x) q(y|x) \\frac{\\pi(y) q(x|y)}{\\pi(x) q(y|x)} = \\pi(y) q(x|y)$, a true statement. A similar verification holds if $r > 1$.\n\nNow, we specialize this general formula for the given independence sampler.\nThe proposal densities are:\n$q(y|x) = g(y)$\n$q(x|y) = g(x)$\nSubstituting these into the acceptance probability formula yields:\n$$\n\\alpha(x,y) = \\min\\left(1, \\frac{\\pi(y) g(x)}{\\pi(x) g(y)}\\right)\n$$\nWe are given $\\pi(x) \\propto \\exp(-x^{4}/2)$ and $g(x) = (2\\pi)^{-1/2} \\exp(-x^{2}/2)$. Let's compute the ratio inside the $\\min$ function. The proportionality constant for $\\pi(x)$ cancels, as does the constant $(2\\pi)^{-1/2}$ for $g(x)$.\n$$\n\\frac{\\pi(y) g(x)}{\\pi(x) g(y)} = \\frac{\\exp(-y^{4}/2) \\exp(-x^{2}/2)}{\\exp(-x^{4}/2) \\exp(-y^{2}/2)}\n$$\nCombining the exponents, we get:\n$$\n\\frac{\\pi(y) g(x)}{\\pi(x) g(y)} = \\exp\\left(-\\frac{y^{4}}{2} - \\frac{x^{2}}{2} - \\left(-\\frac{x^{4}}{2} - \\frac{y^{2}}{2}\\right)\\right) = \\exp\\left(\\frac{x^{4} - y^{4} + y^{2} - x^{2}}{2}\\right)\n$$\nThus, the acceptance probability is:\n$$\n\\alpha(x,y) = \\min\\left(1, \\exp\\left(\\frac{x^{4} - y^{4} + y^{2} - x^{2}}{2}\\right)\\right)\n$$\nThis completes the first part of the problem.\n\nNext, we must evaluate this expression for the specific values $x = \\frac{1}{2}$ and $y = \\frac{3}{2}$. We need to calculate the argument of the exponential function:\n$$\n\\text{Exponent} = \\frac{x^{4} - y^{4} + y^{2} - x^{2}}{2}\n$$\nFirst, let's compute the powers of $x$ and $y$:\n$x = \\frac{1}{2} \\implies x^{2} = \\left(\\frac{1}{2}\\right)^{2} = \\frac{1}{4}$\n$x^{4} = \\left(\\frac{1}{4}\\right)^{2} = \\frac{1}{16}$\n\n$y = \\frac{3}{2} \\implies y^{2} = \\left(\\frac{3}{2}\\right)^{2} = \\frac{9}{4}$\n$y^{4} = \\left(\\frac{9}{4}\\right)^{2} = \\frac{81}{16}$\n\nNow, we substitute these values into the expression for the exponent:\n$x^{4} - y^{4} = \\frac{1}{16} - \\frac{81}{16} = -\\frac{80}{16} = -5$\n$y^{2} - x^{2} = \\frac{9}{4} - \\frac{1}{4} = \\frac{8}{4} = 2$\n\nSo, the full numerator of the exponent is $(x^{4} - y^{4}) + (y^{2} - x^{2}) = -5 + 2 = -3$.\nThe exponent is therefore $\\frac{-3}{2} = -\\frac{3}{2}$.\n\nThe acceptance probability is:\n$$\n\\alpha\\left(\\frac{1}{2}, \\frac{3}{2}\\right) = \\min\\left(1, \\exp\\left(-\\frac{3}{2}\\right)\\right)\n$$\nSince the base of the natural logarithm $e \\approx 2.718 > 1$, any negative power of $e$ will be less than $1$. Specifically, $\\exp(-\\frac{3}{2}) = \\frac{1}{e^{3/2}} < 1$.\nTherefore, the minimum of $1$ and $\\exp(-\\frac{3}{2})$ is $\\exp(-\\frac{3}{2})$.\nThe exact value of the acceptance probability is $\\exp(-\\frac{3}{2})$.", "answer": "$$\\boxed{\\exp\\left(-\\frac{3}{2}\\right)}$$", "id": "3354112"}, {"introduction": "Theoretical understanding is essential, but the real power of MCMC methods is unlocked through implementation. This practice guides you through the process of creating a robust, single-step independence sampler function from its mathematical definition [@problem_id:3354075]. You will focus on a crucial practical consideration: performing calculations in log-space to ensure numerical stability, a standard and vital technique when dealing with products of small probability densities.", "problem": "You are to formalize and implement one step of the independence sampler, a special case of the Metropolis–Hastings algorithm in Markov Chain Monte Carlo (MCMC), starting from fundamental principles. The independence sampler uses a proposal distribution that does not depend on the current state. Your task is to derive the acceptance probability from the detailed balance condition and to encode a single update step that correctly reflects this derivation, using numerically stable computations.\n\nStarting point and foundational base:\n- A Markov chain with target probability density or mass function $\\,\\pi(x)\\,$ should be constructed so that $\\,\\pi(x)\\,$ is its invariant distribution.\n- The detailed balance condition (also called reversibility) for a transition kernel $\\,P(x,\\mathrm{d}y)\\,$ with respect to $\\,\\pi\\,$ is\n$$\n\\pi(x) P(x,\\mathrm{d}y) = \\pi(y) P(y,\\mathrm{d}x),\n$$\nfor all appropriate $\\,x,y\\,$ in the state space.\n- In the Metropolis–Hastings framework, one draws a proposal $\\,y\\,$ from a proposal kernel $\\,q(x,\\mathrm{d}y)\\,$ and accepts it with some acceptance probability $\\,\\alpha(x,y)\\,$. For the independence sampler, $\\,q(x,\\mathrm{d}y)\\,$ factorizes as $\\,g(\\mathrm{d}y)\\,$; that is, the proposal $\\,y\\,$ is drawn from a fixed distribution $\\,g\\,$ that does not depend on $\\,x\\,$.\n\nYour tasks:\n1. From the detailed balance condition and the Metropolis–Hastings construction with proposal density $\\,g(y)\\,$ that is independent of $\\,x\\,$, derive the correct acceptance mechanism for the independence sampler. You must show how to obtain $\\,\\alpha(x,y)\\,$ starting from detailed balance, without assuming any pre-given formula.\n2. Provide clear pseudocode for one step of the independence sampler that identifies the input current state $\\,x\\,$, the draw $\\,y \\sim g\\,$, the computation of $\\,\\alpha(x,y)\\,$, and the update rule that produces $\\,X_{n+1}\\,$.\n3. Implement a complete, runnable program that:\n   - Encodes your pseudocode for one step in a function that returns three outputs: the acceptance probability $\\,\\alpha(x,y)\\,$, the acceptance decision (a boolean), and the updated state $\\,X_{n+1}\\,$.\n   - Uses log-densities throughout to improve numerical stability. If any log-density evaluates to negative infinity due to zero density outside support, the acceptance probability must be handled consistently with the derived rule.\n   - For random draws, uses a fixed seed per test case to ensure reproducibility.\n\nTest suite specification:\nImplement the single-step independence sampler for the following three cases. In each case, the current state $\\,x\\,$, the target log-density $\\,\\log \\pi(\\cdot)\\,$, the proposal sampler (for $\\,y \\sim g\\,$), and the proposal log-density $\\,\\log g(\\cdot)\\,$ are fully specified. The program must output, in order for each case, the tuple $[\\alpha(x,y),\\text{accepted},X_{n+1}]$ flattened into a single list.\n\n- Case A (happy path, well-matched tails):\n  - Current state: $\\,x = 0.5\\,$.\n  - Target density: standard normal $\\,\\pi(x) = \\varphi(x;0,1)\\,$, where\n    $$\n    \\varphi(x;\\mu,\\sigma) = \\frac{1}{\\sqrt{2\\pi}\\,\\sigma}\\,\\exp\\!\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right).\n    $$\n  - Proposal $\\,g\\,$: normal $\\,\\mathcal{N}(0,2^2)\\,$, i.e., draw $\\,y \\sim \\varphi(y;0,2)\\,$ and use its log-density accordingly.\n  - Seed: use the fixed integer $\\,12345\\,$ for random number generation in this case.\n\n- Case B (mismatch: multimodal target, heavy-tailed proposal):\n  - Current state: $\\,x = 2.5\\,$.\n  - Target density: equally weighted mixture of two normals,\n    $$\n    \\pi(x) = \\tfrac{1}{2}\\,\\varphi(x;-2,0.5) + \\tfrac{1}{2}\\,\\varphi(x;2,0.5),\n    $$\n    implemented via log-sum-exp for $\\,\\log \\pi(x)\\,$.\n  - Proposal $\\,g\\,$: Student's t distribution with $\\,\\nu=3\\,$ degrees of freedom (standard scale),\n    $$\n    f_t(x;\\nu) = \\frac{\\Gamma\\!\\left(\\frac{\\nu+1}{2}\\right)}{\\sqrt{\\nu\\pi}\\,\\Gamma\\!\\left(\\frac{\\nu}{2}\\right)}\\left(1+\\frac{x^2}{\\nu}\\right)^{-\\frac{\\nu+1}{2}}.\n    $$\n    Use this for $\\,\\log g(x)\\,$ and sample $\\,y \\sim t_{\\nu=3}\\,$.\n  - Seed: use the fixed integer $\\,67890\\,$.\n\n- Case C (bounded support target, uniform proposal):\n  - Current state: $\\,x = 0.5\\,$.\n  - Target density: truncated standard normal on $[0,1]$,\n    $$\n    \\pi(x) = \\begin{cases}\n    \\dfrac{\\varphi(x;0,1)}{Z}, & x \\in [0,1], \\\\[6pt]\n    0, & \\text{otherwise},\n    \\end{cases}\n    \\quad \\text{where}\\quad Z = \\Phi(1) - \\Phi(0),\n    $$\n    and $\\,\\Phi(\\cdot)\\,$ is the standard normal cumulative distribution function.\n  - Proposal $\\,g\\,$: uniform on $[0,1]$, i.e., draw $\\,y \\sim \\text{Uniform}(0,1)\\,$ with $\\,g(y)=1\\,$ for $\\,y \\in [0,1]\\,$ and $\\,g(y)=0\\,$ otherwise; use the corresponding log-density.\n  - Seed: use the fixed integer $\\,13579\\,$.\n\nOutput format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain nine entries in the following order:\n$[\\alpha_A,\\text{accepted}_A,X_{n+1,A},\\alpha_B,\\text{accepted}_B,X_{n+1,B},\\alpha_C,\\text{accepted}_C,X_{n+1,C}]$,\nwhere $\\,\\alpha_\\cdot\\,$ are floats, $\\,\\text{accepted}_\\cdot\\,$ are booleans, and $\\,X_{n+1,\\cdot}\\,$ are floats.\n\nNo physical units or angles are involved in this problem. All results are pure numbers. The program must be deterministic given the specified seeds and must not read any input.", "solution": "The problem statement is a valid and well-posed exercise in computational statistics. It requires the derivation, formulation, and implementation of a single step of the independence sampler, a specific algorithm within the Metropolis-Hastings framework. All necessary constants, distributions, and initial conditions are provided, and the task is scientifically grounded and objective.\n\n### 1. Derivation of the Acceptance Probability\n\nThe objective is to construct a Markov chain whose stationary distribution is a given target probability density function (PDF) $\\pi(x)$. The Metropolis-Hastings algorithm achieves this by defining a transition kernel $P(x, \\mathrm{d}y)$ that satisfies the detailed balance condition with respect to $\\pi(x)$:\n$$\n\\pi(x) P(x, \\mathrm{d}y) = \\pi(y) P(y, \\mathrm{d}x)\n$$\nFor continuous state spaces, the transition kernel $P(x, \\mathrm{d}y)$ is a mixture of an absolutely continuous component (for accepted moves) and a discrete component (for rejected moves). A move from state $x$ to a proposed state $y$ is generated from a proposal distribution with density $q(x,y)$, and this move is accepted with probability $\\alpha(x,y)$. The density of the continuous part of the transition kernel is thus $q(x,y)\\alpha(x,y)$. For detailed balance to hold for any pair of distinct states $(x,y)$, the densities of forward and reverse transitions must balance:\n$$\n\\pi(x) q(x,y) \\alpha(x,y) = \\pi(y) q(y,x) \\alpha(y,x)\n$$\nThis equation is satisfied by the Metropolis-Hastings choice for the acceptance probability:\n$$\n\\alpha(x,y) = \\min \\left( 1, \\frac{\\pi(y) q(y,x)}{\\pi(x) q(x,y)} \\right)\n$$\nWe can verify this. Let the ratio be $r = \\frac{\\pi(y) q(y,x)}{\\pi(x) q(x,y)}$. Then $\\alpha(x,y) = \\min(1, r)$ and $\\alpha(y,x) = \\min(1, 1/r)$.\n- If $r \\le 1$, then $\\alpha(x,y) = r$ and $\\alpha(y,x) = 1$. The detailed balance equation becomes $\\pi(x)q(x,y)r = \\pi(y)q(y,x) \\cdot 1$, which simplifies to $\\pi(x)q(x,y)\\frac{\\pi(y)q(y,x)}{\\pi(x)q(x,y)} = \\pi(y)q(y,x)$, an identity.\n- If $r > 1$, then $\\alpha(x,y) = 1$ and $\\alpha(y,x) = 1/r$. The equation becomes $\\pi(x)q(x,y) \\cdot 1 = \\pi(y)q(y,x) (1/r)$, which simplifies to $\\pi(x)q(x,y) = \\pi(y)q(y,x) \\frac{\\pi(x)q(x,y)}{\\pi(y)q(y,x)}$, also an identity.\n\nThe independence sampler is a special case where the proposal distribution is independent of the current state $x$. That is, the proposal density $q(x,y)$ can be written as $g(y)$ for some fixed PDF $g$. Consequently, the proposal density for the reverse move, $q(y,x)$, is simply $g(x)$.\n\nSubstituting $q(x,y) = g(y)$ and $q(y,x) = g(x)$ into the general Metropolis-Hastings acceptance probability formula yields the acceptance probability for the independence sampler:\n$$\n\\alpha(x,y) = \\min \\left( 1, \\frac{\\pi(y) g(x)}{\\pi(x) g(y)} \\right)\n$$\nThis is the required derivation. For numerical stability, computations are performed in the logarithmic domain. Let the acceptance ratio be $r(x,y) = \\frac{\\pi(y)g(x)}{\\pi(x)g(y)}$. Its logarithm is:\n$$\n\\log r(x,y) = \\log(\\pi(y)) + \\log(g(x)) - \\log(\\pi(x)) - \\log(g(y))\n$$\nThe acceptance probability can then be written as $\\alpha(x,y) = \\min(1, \\exp(\\log r(x,y)))$. A more numerically stable form is $\\alpha(x,y) = \\exp(\\min(0, \\log r(x,y)))$. This avoids computing a large exponential which would be subsequently capped at $1$.\n\nSpecial care is needed for states where densities are zero. If a proposed state $y$ has $\\pi(y)=0$, this implies $\\log(\\pi(y))=-\\infty$, making $\\log r(x,y) = -\\infty$ and thus $\\alpha(x,y)=0$. The move is always rejected, which is correct. If the current state $x$ has $g(x)=0$ (but $\\pi(x)>0$), implies $\\log(g(x))=-\\infty$. The ratio $r(x,y)$ becomes infinite and $\\alpha(x,y)=1$, so the move is always accepted (provided $\\pi(y)>0$). The log-space calculation must correctly handle these cases.\n\n### 2. Pseudocode for One Step of the Independence Sampler\n\n**Inputs:**\n- Current state: $X_n$\n- Target log-density function: $\\log \\pi(\\cdot)$\n- Proposal sampler: a function to draw samples from $g(\\cdot)$\n- Proposal log-density function: $\\log g(\\cdot)$\n\n**Algorithm:**\n1.  Draw a proposal state $y$ from the proposal distribution: $y \\sim g(\\cdot)$.\n2.  Calculate the log-densities at the current state $X_n$ and the proposed state $y$:\n    - $\\log\\pi_n \\leftarrow \\log\\pi(X_n)$\n    - $\\log g_n \\leftarrow \\log g(X_n)$\n    - $\\log\\pi_y \\leftarrow \\log\\pi(y)$\n    - $\\log g_y \\leftarrow \\log g(y)$\n3.  Calculate the logarithm of the acceptance ratio $r$:\n    - $\\log r \\leftarrow (\\log\\pi_y + \\log g_n) - (\\log\\pi_n + \\log g_y)$\n    *Handle cases where any log-density is $-\\infty$ to ensure $\\log r$ becomes $+\\infty$ or $-\\infty$ appropriately.*\n4.  Calculate the acceptance probability $\\alpha$:\n    - $\\alpha \\leftarrow \\min(1, \\exp(\\log r))$ or, for better stability, $\\alpha \\leftarrow \\exp(\\min(0, \\log r))$.\n5.  Draw a random number $u$ from a uniform distribution on $[0, 1)$: $u \\sim \\text{Uniform}(0,1)$.\n6.  Update the state:\n    - If $u < \\alpha$:\n        - $X_{n+1} \\leftarrow y$\n        - accepted $\\leftarrow \\text{True}$\n    - Else:\n        - $X_{n+1} \\leftarrow X_n$\n        - accepted $\\leftarrow \\text{False}$\n7.  **Return:** The tuple $(\\alpha, \\text{accepted}, X_{n+1})$.\n\n### 3. Implementation\nThe following Python code implements the pseudocode for the specified test cases, adhering to a numerically stable approach using log-densities.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm, t\nfrom scipy.special import logsumexp\n\ndef independence_sampler_step(x_current, log_pi_func, proposal_sampler, log_g_func, rng):\n    \"\"\"\n    Performs a single step of the independence sampler.\n\n    Args:\n        x_current (float): The current state of the Markov chain.\n        log_pi_func (callable): The log-density of the target distribution pi.\n        proposal_sampler (callable): A function that draws a sample from the proposal g.\n        log_g_func (callable): The log-density of the proposal distribution g.\n        rng (np.random.Generator): A random number generator object.\n\n    Returns:\n        tuple: (acceptance_prob, accepted, next_state)\n    \"\"\"\n    # 1. Draw a proposal state y\n    y_proposal = proposal_sampler(rng)\n\n    # 2. Calculate log-densities\n    log_pi_current = log_pi_func(x_current)\n    log_g_current = log_g_func(x_current)\n    log_pi_proposal = log_pi_func(y_proposal)\n    log_g_proposal = log_g_func(y_proposal)\n\n    # 3. Calculate the log of the acceptance ratio\n    # log r = log(pi(y)g(x)) - log(pi(x)g(y))\n    # We must handle -inf cases carefully.\n    \n    # Python's float arithmetic with np.inf handles most cases correctly,\n    # but the case where g(x) = 0 (log_g_current = -np.inf) leading to\n    # alpha = 1 must be handled explicitly because (-inf - inf) is nan.\n    \n    # If a proposal has zero target density, it should never be accepted.\n    if log_pi_proposal == -np.inf:\n        log_r = -np.inf\n    # If the current state has zero proposal density, the reverse move is impossible.\n    # The proposal should be accepted (if it's valid under the target).\n    elif log_g_current == -np.inf:\n        log_r = np.inf\n    # All other cases are handled correctly by standard float arithmetic.\n    # We assume x_current is a valid state, so log_pi_current != -np.inf.\n    # We assume y_proposal is drawn from g, so log_g_proposal != -np.inf.\n    else:\n        log_r = (log_pi_proposal + log_g_current) - (log_pi_current + log_g_proposal)\n\n    # 4. Calculate acceptance probability\n    alpha = np.exp(min(0.0, log_r))\n\n    # 5. Draw a uniform random number\n    u = rng.uniform(0, 1)\n\n    # 6. Update the state\n    if u < alpha:\n        next_state = y_proposal\n        accepted = True\n    else:\n        next_state = x_current\n        accepted = False\n\n    return alpha, accepted, next_state\n\n\ndef solve():\n    \"\"\"\n    Runs the test suite for the independence sampler and prints the results.\n    \"\"\"\n    # Case A: Normal target, Normal proposal\n    def log_pi_A(x):\n        return norm.logpdf(x, loc=0, scale=1)\n    \n    def sampler_g_A(rng):\n        return norm.rvs(loc=0, scale=2, random_state=rng)\n\n    def log_g_A(x):\n        return norm.logpdf(x, loc=0, scale=2)\n\n    # Case B: Mixture Normal target, Student's t proposal\n    def log_pi_B(x):\n        log_pdf1 = norm.logpdf(x, loc=-2, scale=0.5)\n        log_pdf2 = norm.logpdf(x, loc=2, scale=0.5)\n        return np.log(0.5) + logsumexp([log_pdf1, log_pdf2])\n\n    def sampler_g_B(rng):\n        return t.rvs(df=3, random_state=rng)\n        \n    def log_g_B(x):\n        return t.logpdf(x, df=3)\n\n    # Case C: Truncated Normal target, Uniform proposal\n    Z_C = norm.cdf(1) - norm.cdf(0)\n    log_Z_C = np.log(Z_C)\n\n    def log_pi_C(x):\n        if 0 <= x <= 1:\n            return norm.logpdf(x, loc=0, scale=1) - log_Z_C\n        else:\n            return -np.inf\n\n    def sampler_g_C(rng):\n        return rng.uniform(0, 1)\n\n    def log_g_C(x):\n        if 0 <= x <= 1:\n            return 0.0  # log(1)\n        else:\n            return -np.inf\n    \n    test_cases = [\n        # (current_x, log_pi, sampler_g, log_g, seed)\n        (0.5, log_pi_A, sampler_g_A, log_g_A, 12345),\n        (2.5, log_pi_B, sampler_g_B, log_g_B, 67890),\n        (0.5, log_pi_C, sampler_g_C, log_g_C, 13579),\n    ]\n\n    results = []\n    for x_curr, log_pi, sampler_g, log_g, seed in test_cases:\n        rng = np.random.default_rng(seed)\n        alpha, accepted, x_next = independence_sampler_step(\n            x_curr, log_pi, sampler_g, log_g, rng\n        )\n        results.extend([alpha, accepted, x_next])\n\n    # Format the final output as a comma-separated list in brackets\n    # The str() of a boolean is 'True' or 'False' which is standard\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3354075"}, {"introduction": "A correctly implemented sampler is not guaranteed to be an efficient one, as its performance is critically dependent on how well the proposal distribution $g$ approximates the target $\\pi$. This final practice explores a common and important failure scenario: what happens when the proposal distribution has 'lighter' tails than the target distribution [@problem_id:3354091]. By analyzing the expected acceptance probability far from the center of the distribution, you will learn to diagnose why the sampler can become 'stuck' and fail to explore the state space effectively, a key insight for designing practical algorithms.", "problem": "Consider a one-dimensional independence sampler within the framework of Markov chain Monte Carlo (MCMC), targeting the Laplace distribution with density $\\pi(x)=\\frac{1}{2}\\exp(-|x|)$ on $\\mathbb{R}$ and using a zero-mean Gaussian proposal density $g(x)=\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\!\\left(-\\frac{x^{2}}{2\\sigma^{2}}\\right)$ with fixed variance parameter $\\sigma^{2}\\in(0,\\infty)$. Define the importance weight $w(x)=\\pi(x)/g(x)$ and the acceptance probability $\\alpha(x,y)=\\min\\{1,\\,w(y)/w(x)\\}$ for a proposed $y\\sim g$ independent of the current state $x$.\n\nStarting from first principles, namely the definition of the independence sampler, the explicit forms of the Laplace and Gaussian densities, and basic properties of exponential functions, derive the asymptotic form of the ratio $w(y)/w(x)$ as $|x|\\to\\infty$ and use it to determine the limit\n$$\nL \\;=\\; \\lim_{|x|\\to\\infty}\\,\\mathbb{E}_{Y\\sim g}\\!\\left[\\alpha(x,Y)\\right].\n$$\nClearly justify each step of your derivation by appealing only to the stated definitions and standard asymptotic reasoning for exponential functions. State briefly, in words, what this limit implies for the large-$|x|$ acceptance behavior of the independence sampler under this target–proposal pairing.\n\nYour final answer must be the value of $L$. No rounding is required.", "solution": "The problem as stated is valid. It is a well-posed question within the established mathematical framework of Markov chain Monte Carlo methods, specifically concerning the properties of the independence sampler. All provided definitions and distributions are standard and mathematically sound, and the problem is self-contained. We may therefore proceed with the derivation.\n\nThe target probability density function is the Laplace distribution, given by\n$$ \\pi(x) = \\frac{1}{2}\\exp(-|x|) $$\nfor $x \\in \\mathbb{R}$. The proposal density is the zero-mean Gaussian distribution,\n$$ g(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left(-\\frac{x^{2}}{2\\sigma^{2}}\\right) $$\nwhere the variance $\\sigma^{2}$ is a fixed positive constant.\n\nThe independence sampler's performance is critically linked to the properties of the importance weight function, defined as the ratio of the target density to the proposal density:\n$$ w(x) = \\frac{\\pi(x)}{g(x)} = \\frac{\\frac{1}{2}\\exp(-|x|)}{\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left(-\\frac{x^{2}}{2\\sigma^{2}}\\right)} $$\nLet us define the constant $C = \\frac{\\sqrt{2\\pi}\\sigma}{2}$. The weight function can then be written as:\n$$ w(x) = C \\exp\\left(\\frac{x^{2}}{2\\sigma^{2}} - |x|\\right) $$\nThe acceptance probability for a move from a state $x$ to a proposed state $y \\sim g$ is given by\n$$ \\alpha(x,y) = \\min\\left\\{1, \\frac{w(y)}{w(x)}\\right\\} $$\nWe are asked to find the limit of the expected acceptance probability as the current state $x$ goes to infinity, i.e.,\n$$ L = \\lim_{|x|\\to\\infty}\\,\\mathbb{E}_{Y\\sim g}\\!\\left[\\alpha(x,Y)\\right] $$\nThe expectation is taken with respect to the proposal distribution of $Y$, so we can write this as an integral:\n$$ L = \\lim_{|x|\\to\\infty} \\int_{-\\infty}^{\\infty} \\alpha(x,y) g(y) \\,dy = \\lim_{|x|\\to\\infty} \\int_{-\\infty}^{\\infty} \\min\\left\\{1, \\frac{w(y)}{w(x)}\\right\\} g(y) \\,dy $$\nTo evaluate this limit, we first analyze the asymptotic behavior of the weight function $w(x)$ as $|x| \\to \\infty$. The argument of the exponential in $w(x)$ is the function $f(x) = \\frac{x^{2}}{2\\sigma^{2}} - |x|$. For large values of $|x|$, the quadratic term $x^{2}/(2\\sigma^{2})$ dominates the linear term $|x|$. Since $\\sigma^{2} > 0$, the coefficient of the quadratic term is positive. Consequently,\n$$ \\lim_{|x|\\to\\infty} f(x) = \\lim_{|x|\\to\\infty} \\left(\\frac{x^{2}}{2\\sigma^{2}} - |x|\\right) = \\infty $$\nBased on the properties of the exponential function, this implies that the weight function $w(x)$ also tends to infinity:\n$$ \\lim_{|x|\\to\\infty} w(x) = \\lim_{|x|\\to\\infty} C \\exp(f(x)) = \\infty $$\nNow, consider the ratio $w(y)/w(x)$ which appears inside the acceptance probability. For any fixed value of $y$, the numerator $w(y)$ is a finite positive constant. As $|x| \\to \\infty$, the denominator $w(x)$ approaches infinity. Therefore, for any fixed $y$, the ratio tends to zero:\n$$ \\lim_{|x|\\to\\infty} \\frac{w(y)}{w(x)} = 0 $$\nThis suggests that the integrand in the expression for $L$ converges pointwise to zero. To formalize the evaluation of the limit $L$, we can apply the Dominated Convergence Theorem. Let the sequence of functions under the integral be denoted by $h_x(y)$:\n$$ h_x(y) = \\min\\left\\{1, \\frac{w(y)}{w(x)}\\right\\} g(y) $$\nFor each fixed $y \\in \\mathbb{R}$, we have established that $\\lim_{|x|\\to\\infty} w(x) = \\infty$, which implies $\\lim_{|x|\\to\\infty} \\frac{w(y)}{w(x)} = 0$. Thus, the pointwise limit of the integrand is:\n$$ \\lim_{|x|\\to\\infty} h_x(y) = \\lim_{|x|\\to\\infty} \\min\\left\\{1, \\frac{w(y)}{w(x)}\\right\\} g(y) = \\min\\{1, 0\\} \\cdot g(y) = 0 \\cdot g(y) = 0 $$\nNext, we must find an integrable function that dominates $|h_x(y)|$ for all $x$. The term $\\min\\{1, \\cdot\\}$ is by definition always less than or equal to $1$. Since $w(y)$ and $w(x)$ are ratios of positive densities, they are positive, so the ratio is positive. And $g(y)$ is a probability density, so it is non-negative. Therefore, we have the bound:\n$$ 0 \\le \\min\\left\\{1, \\frac{w(y)}{w(x)}\\right\\} \\le 1 $$\nThis implies that the integrand is bounded by the proposal density itself:\n$$ |h_x(y)| = h_x(y) \\le 1 \\cdot g(y) = g(y) $$\nThe function $g(y)$ is a valid dominating function because it is a probability density function and its integral over $\\mathbb{R}$ is finite:\n$$ \\int_{-\\infty}^{\\infty} g(y) \\,dy = 1 < \\infty $$\nSince the conditions for the Dominated Convergence Theorem are met, we can exchange the limit and the integral:\n$$ L = \\lim_{|x|\\to\\infty} \\int_{-\\infty}^{\\infty} h_x(y) \\,dy = \\int_{-\\infty}^{\\infty} \\lim_{|x|\\to\\infty} h_x(y) \\,dy $$\nSubstituting the pointwise limit we found:\n$$ L = \\int_{-\\infty}^{\\infty} 0 \\,dy = 0 $$\nThe limit is $L=0$.\n\nThis result has a critical implication for the behavior of the sampler. The expected acceptance probability, given that the chain is currently in the far tails of the state space (large $|x|$), is zero. This means that if the Markov chain happens to move to a state with a large absolute value, it will become \"stuck\". Almost every subsequent proposal drawn from the Gaussian $g$ will be rejected. This is a classic symptom of a poorly performing MCMC algorithm, caused by using a proposal distribution ($g$) with much lighter tails (Gaussian, decaying like $\\exp(-x^2)$) than the target distribution ($\\pi$, decaying like $\\exp(-|x|)$). The sampler cannot efficiently explore the tails of the target distribution, leading to a lack of ergodicity and very poor mixing.", "answer": "$$\n\\boxed{0}\n$$", "id": "3354091"}]}