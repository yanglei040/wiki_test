{"hands_on_practices": [{"introduction": "To master the Random-Walk Metropolis algorithm, we must first understand its fundamental mechanics. This practice begins by deriving the one-step transition kernel from first principles, illustrating how the acceptance probability and a symmetric proposal combine to ensure the target distribution is left invariant. By calculating the probability of the chain remaining in its current state, you will gain a concrete understanding of the trade-off between exploration and rejection that defines the algorithm's behavior [@problem_id:3334157].", "problem": "Consider a Random-Walk Metropolis construction within the Metropolis-Hastings framework for Markov Chain Monte Carlo (MCMC), targeting a probability density $\\pi(x)$ on $\\mathbb{R}^{d}$, and using a proposal kernel of the form $q(y \\mid x) = q(y - x)$ that is symmetric in the sense that $q(u) = q(-u)$. Starting from the requirement that the Markov transition kernel must leave $\\pi$ invariant by satisfying detailed balance, derive the one-step transition kernel $P(x,dy)$ of the resulting Markov chain, and express the probability of remaining at the current state $x$ as an explicit integral involving $q$ and $\\pi$.\n\nThen specialize to $d=1$ with target density proportional to a centered Gaussian, $\\pi(x) \\propto \\exp\\!\\big(-x^{2}/(2 \\tau^{2})\\big)$, and a Gaussian random-walk proposal $q(\\epsilon) = \\frac{1}{\\sqrt{2 \\pi} \\sigma} \\exp\\!\\big(-\\epsilon^{2}/(2 \\sigma^{2})\\big)$, with fixed constants $\\tau  0$ and $\\sigma  0$. Using your derived expression, find the “stay” probability $r(0)$ at $x=0$ in closed form, and then evaluate it numerically for $\\tau = 2$ and $\\sigma = 1$. Round your final numerical answer to four significant figures. The final answer must be a single real number with no units.", "solution": "First, we derive the general form of the one-step transition kernel $P(x, dy)$ for a Random-Walk Metropolis algorithm. The algorithm proceeds from a current state $x$ by proposing a new state $y$ from a proposal distribution $q(y \\mid x)$. The proposal is then accepted with probability $\\alpha(x, y)$ or rejected, in which case the chain remains at $x$. The general Metropolis-Hastings acceptance probability is\n$$\n\\alpha(x, y) = \\min\\left(1, \\frac{\\pi(y) q(x \\mid y)}{\\pi(x) q(y \\mid x)}\\right)\n$$\nThe problem specifies a random-walk proposal of the form $q(y \\mid x) = q(y - x)$ with the symmetry property $q(u) = q(-u)$. This implies that the proposal kernel is symmetric:\n$$\nq(x \\mid y) = q(x - y) = q(-(y - x)) = q(y - x) = q(y \\mid x)\n$$\nThus, the proposal ratio $q(x \\mid y) / q(y \\mid x) = 1$, and the acceptance probability simplifies to the Metropolis form:\n$$\n\\alpha(x, y) = \\min\\left(1, \\frac{\\pi(y)}{\\pi(x)}\\right)\n$$\nThe one-step transition kernel $P(x, A)$ gives the probability of moving from state $x$ into a measurable set $A \\subseteq \\mathbb{R}^d$. It is composed of two parts: a continuous part for accepted moves to a new state $y \\neq x$, and a discrete part for rejected moves where the state remains $x$. In differential form, the kernel $P(x, dy)$ is:\n$$\nP(x, dy) = \\alpha(x, y) q(y \\mid x) \\,dy + \\delta_x(dy) \\left(1 - \\int_{\\mathbb{R}^d} \\alpha(x, z) q(z \\mid x) \\,dz \\right)\n$$\nwhere $\\delta_x(dy)$ is the Dirac measure at $x$. Substituting the specific forms of $\\alpha(x,y)$ and $q(y|x)$, we obtain the transition kernel:\n$$\nP(x, dy) = \\min\\left(1, \\frac{\\pi(y)}{\\pi(x)}\\right) q(y-x) \\,dy + \\delta_x(dy) \\left(1 - \\int_{\\mathbb{R}^d} \\min\\left(1, \\frac{\\pi(z)}{\\pi(x)}\\right) q(z-x) \\,dz \\right)\n$$\nThe probability of remaining at the current state $x$, which we denote as $r(x)$, is the coefficient of the Dirac mass $\\delta_x(dy)$. This corresponds to the total probability of rejecting a proposal.\n$$\nr(x) = 1 - \\int_{\\mathbb{R}^d} \\alpha(x, y) q(y \\mid x) \\,dy\n$$\nThe integral represents the total probability of accepting a proposal. Substituting our expressions for $\\alpha$ and $q$:\n$$\nr(x) = 1 - \\int_{\\mathbb{R}^d} \\min\\left(1, \\frac{\\pi(y)}{\\pi(x)}\\right) q(y - x) \\,dy\n$$\nThis is the required expression for the probability of remaining at state $x$.\n\nNext, we specialize to the given case: $d=1$, target density $\\pi(x) \\propto \\exp\\big(-x^{2}/(2 \\tau^{2})\\big)$, and proposal $q(\\epsilon) = \\frac{1}{\\sqrt{2 \\pi} \\sigma} \\exp\\big(-\\epsilon^{2}/(2 \\sigma^{2})\\big)$. We seek the \"stay\" probability $r(0)$ at $x=0$.\nUsing the derived formula for $r(x)$ with $x=0$:\n$$\nr(0) = 1 - \\int_{-\\infty}^{\\infty} \\min\\left(1, \\frac{\\pi(y)}{\\pi(0)}\\right) q(y - 0) \\,dy\n$$\nFirst, we compute the ratio of the target densities. Let the proportionality constant be $C$.\n$$\n\\frac{\\pi(y)}{\\pi(0)} = \\frac{C \\exp(-y^2 / (2\\tau^2))}{C \\exp(-0^2 / (2\\tau^2))} = \\exp\\left(-\\frac{y^2}{2\\tau^2}\\right)\n$$\nSince $y^2 \\geq 0$ and $\\tau^2 > 0$, the exponent is non-positive, which means $\\exp(-y^2 / (2\\tau^2)) \\le 1$ for all $y \\in \\mathbb{R}$. Therefore, the minimum term simplifies:\n$$\n\\min\\left(1, \\frac{\\pi(y)}{\\pi(0)}\\right) = \\frac{\\pi(y)}{\\pi(0)} = \\exp\\left(-\\frac{y^2}{2\\tau^2}\\right)\n$$\nThe probability of staying at $0$ is $r(0) = 1 - a(0)$, where $a(0)$ is the acceptance probability at $0$. Let's calculate $a(0)$:\n$$\na(0) = \\int_{-\\infty}^{\\infty} \\exp\\left(-\\frac{y^2}{2\\tau^2}\\right) q(y) \\,dy\n$$\nSubstituting the given Gaussian proposal density for $q(y)$:\n$$\na(0) = \\int_{-\\infty}^{\\infty} \\exp\\left(-\\frac{y^2}{2\\tau^2}\\right) \\frac{1}{\\sqrt{2 \\pi} \\sigma} \\exp\\left(-\\frac{y^2}{2\\sigma^2}\\right) \\,dy\n$$\nWe combine the exponential terms:\n$$\na(0) = \\frac{1}{\\sqrt{2 \\pi} \\sigma} \\int_{-\\infty}^{\\infty} \\exp\\left(-\\frac{y^2}{2\\tau^2} - \\frac{y^2}{2\\sigma^2}\\right) \\,dy\n= \\frac{1}{\\sqrt{2 \\pi} \\sigma} \\int_{-\\infty}^{\\infty} \\exp\\left(-\\frac{y^2}{2} \\left(\\frac{1}{\\tau^2} + \\frac{1}{\\sigma^2}\\right)\\right) \\,dy\n$$\nLet's simplify the coefficient of $y^2$:\n$$\n\\frac{1}{\\tau^2} + \\frac{1}{\\sigma^2} = \\frac{\\sigma^2 + \\tau^2}{\\tau^2 \\sigma^2}\n$$\nThe integral becomes:\n$$\na(0) = \\frac{1}{\\sqrt{2 \\pi} \\sigma} \\int_{-\\infty}^{\\infty} \\exp\\left(-\\frac{y^2}{2} \\frac{\\tau^2 + \\sigma^2}{\\tau^2 \\sigma^2}\\right) \\,dy\n$$\nThis is a standard Gaussian integral of the form $\\int_{-\\infty}^{\\infty} \\exp(-z^2 / (2s^2)) \\,dz = \\sqrt{2\\pi}s$. In our case, the variance $s^2$ is given by $s^{-2} = (\\tau^2+\\sigma^2)/(\\tau^2\\sigma^2)$, so $s = \\frac{\\tau\\sigma}{\\sqrt{\\tau^2+\\sigma^2}}$. Thus, the integral evaluates to:\n$$\n\\int_{-\\infty}^{\\infty} \\exp\\left(-\\frac{y^2}{2} \\frac{\\tau^2 + \\sigma^2}{\\tau^2 \\sigma^2}\\right) \\,dy = \\sqrt{2\\pi} \\left(\\frac{\\tau\\sigma}{\\sqrt{\\tau^2+\\sigma^2}}\\right)\n$$\nSubstituting this back into the expression for $a(0)$:\n$$\na(0) = \\frac{1}{\\sqrt{2 \\pi} \\sigma} \\left(\\sqrt{2\\pi} \\frac{\\tau\\sigma}{\\sqrt{\\tau^2+\\sigma^2}}\\right) = \\frac{\\tau}{\\sqrt{\\tau^2+\\sigma^2}}\n$$\nThe probability of remaining at the state $x=0$ is therefore:\n$$\nr(0) = 1 - a(0) = 1 - \\frac{\\tau}{\\sqrt{\\tau^2+\\sigma^2}}\n$$\nFinally, we evaluate this expression for the given values $\\tau = 2$ and $\\sigma = 1$:\n$$\nr(0) = 1 - \\frac{2}{\\sqrt{2^2 + 1^2}} = 1 - \\frac{2}{\\sqrt{4 + 1}} = 1 - \\frac{2}{\\sqrt{5}}\n$$\nNumerically, $\\sqrt{5} \\approx 2.236067977$.\n$$\nr(0) = 1 - \\frac{2}{2.236067977} \\approx 1 - 0.894427191 \\approx 0.105572809\n$$\nRounding to four significant figures, we get $0.1056$.", "answer": "$$\n\\boxed{0.1056}\n$$", "id": "3334157"}, {"introduction": "A well-tuned sampler balances making bold proposals with maintaining a reasonable acceptance rate. This exercise moves from the single-step mechanics to a crucial global property by calculating the expected acceptance rate of the algorithm at stationarity [@problem_id:3334210]. Through a clever geometric argument, you will derive a closed-form expression that explicitly links the proposal's standard deviation to the sampler's efficiency, providing deep insight into the art of tuning MCMC methods.", "problem": "Consider a one-dimensional Random-Walk Metropolis algorithm targeting the standard normal distribution with density $\\pi(x) \\propto \\exp(-x^{2}/2)$ and a symmetric Gaussian proposal $q(y \\mid x) = \\mathcal{N}(x,\\sigma^{2})$. Let $X \\sim \\pi$ denote the current state at stationarity and $Y \\mid X \\sim q(\\cdot \\mid X)$ the proposal. The Metropolis acceptance probability is\n$$\n\\alpha(x,y) \\;=\\; \\min\\!\\left\\{1,\\;\\frac{\\pi(y) q(x \\mid y)}{\\pi(x) q(y \\mid x)}\\right\\},\n$$\nwhich reduces to $\\alpha(x,y) = \\min\\{1, \\pi(y)/\\pi(x)\\}$ due to proposal symmetry. Using only fundamental definitions and first principles, derive the closed-form analytic expression of the expected acceptance rate\n$$\nE[\\alpha(X,Y)]\n$$\nas a function of the proposal standard deviation $\\sigma  0$ by integrating over the joint distribution of $(X,Y)$ under stationarity. Your final answer must be a single closed-form expression in $\\sigma$ involving only elementary functions. Do not provide a numerical approximation and do not introduce any special functions beyond the elementary inverse trigonometric functions. Express your final answer as an analytic function of $\\sigma$ with no units.", "solution": "The expected acceptance rate $E[\\alpha(X,Y)]$ is computed by integrating the acceptance probability $\\alpha(x,y)$ over the joint stationary distribution of the current state $X$ and the proposed state $Y$. The joint probability density is $p(x,y) = q(y \\mid x) \\pi(x)$.\n$$\nE[\\alpha(X,Y)] = \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} \\alpha(x,y) q(y \\mid x) \\pi(x) \\, dy \\, dx\n$$\nThe acceptance probability is $\\alpha(x,y) = \\min\\{1, \\pi(y)/\\pi(x)\\}$. We can split the integral into two regions: one where $\\pi(y)/\\pi(x) \\ge 1$ and one where it is less than 1.\n$$\nE[\\alpha(X,Y)] = \\iint_{\\pi(y) \\ge \\pi(x)} 1 \\cdot q(y \\mid x) \\pi(x) \\, dy \\, dx + \\iint_{\\pi(y)  \\pi(x)} \\frac{\\pi(y)}{\\pi(x)} q(y \\mid x) \\pi(x) \\, dy \\, dx\n$$\nThis simplifies to:\n$$\nE[\\alpha(X,Y)] = \\iint_{\\pi(y) \\ge \\pi(x)} q(y \\mid x) \\pi(x) \\, dy \\, dx + \\iint_{\\pi(y)  \\pi(x)} \\pi(y) q(y \\mid x) \\, dy \\, dx\n$$\nSince the proposal $q(y \\mid x)$ is symmetric, we have $q(y \\mid x) = q(x \\mid y)$. The second integral's integrand can be written as $\\pi(y) q(x \\mid y)$. By swapping the integration variables $(x, y) \\to (y, x)$ in the second integral, the integrand becomes $\\pi(x) q(y \\mid x)$ and the integration domain becomes $\\{(y,x) \\mid \\pi(x)  \\pi(y)\\}$, which is the same as the first domain. Thus, the two integrals are equal.\n$$\nE[\\alpha(X,Y)] = 2 \\iint_{\\pi(y) \\ge \\pi(x)} q(y \\mid x) \\pi(x) \\, dy \\, dx = 2 P(\\pi(Y) \\ge \\pi(X))\n$$\nFor the target $\\pi(x) \\propto \\exp(-x^2/2)$, the condition $\\pi(y) \\ge \\pi(x)$ is equivalent to $\\exp(-y^2/2) \\ge \\exp(-x^2/2)$, which simplifies to $y^2 \\le x^2$, or $|y| \\le |x|$. The problem is now to compute $2 P(|Y| \\le |X|)$.\n\nThe proposal is $Y \\sim \\mathcal{N}(X, \\sigma^2)$, which can be written as $Y = X + Z$ where $Z \\sim \\mathcal{N}(0, \\sigma^2)$ is independent of $X \\sim \\mathcal{N}(0, 1)$. We need to calculate $P(|X+Z| \\le |X|)$. This is equivalent to $P((X+Z)^2 \\le X^2)$, which simplifies to:\n$$\nP(X^2 + 2XZ + Z^2 \\le X^2) = P(Z^2 + 2XZ \\le 0) = P(Z(Z+2X) \\le 0)\n$$\nThis inequality defines a region in the $(X, Z)$ plane. To simplify, we transform to a coordinate system where the joint density is circularly symmetric. Let $u=X$ and $v=Z/\\sigma$. Then both $u$ and $v$ are independent standard normal variables, and their joint density $p(u,v) = \\frac{1}{2\\pi}\\exp(-(u^2+v^2)/2)$ is circularly symmetric.\n\nThe inequality $Z(Z+2X) \\le 0$ becomes $(\\sigma v)(\\sigma v + 2u) \\le 0$, which simplifies to $v(v + \\frac{2}{\\sigma}u) \\le 0$. This defines a double-wedge region in the $(u,v)$-plane, with boundaries given by the lines $v=0$ and $v = - \\frac{2}{\\sigma}u$.\n\nBecause the joint distribution of $(u,v)$ is circularly symmetric, the probability of falling within this region is the total angle of the wedges divided by $2\\pi$. The angle $\\theta$ of one wedge (e.g., in the fourth quadrant) is the angle between the positive $u$-axis and the line $v = - \\frac{2}{\\sigma}u$. This angle is given by:\n$$\n\\theta = \\arctan\\left(\\frac{2}{\\sigma}\\right)\n$$\nThe total angle for the two wedges is $2\\theta$. The probability is:\n$$\nP(|Y| \\le |X|) = \\frac{2\\theta}{2\\pi} = \\frac{1}{\\pi}\\arctan\\left(\\frac{2}{\\sigma}\\right)\n$$\nThe final expected acceptance rate is twice this value:\n$$\nE[\\alpha(X,Y)] = 2 P(|Y| \\le |X|) = \\frac{2}{\\pi}\\arctan\\left(\\frac{2}{\\sigma}\\right)\n$$", "answer": "$$\n\\boxed{\\frac{2}{\\pi} \\arctan\\left(\\frac{2}{\\sigma}\\right)}\n$$", "id": "3334210"}, {"introduction": "The ultimate test of a sampler is the quality of the samples it generates. This final practice shifts our focus from the algorithm's design to the statistical analysis of its output, focusing on the integrated autocorrelation time ($\\tau_{\\text{int}}$) which quantifies sampling efficiency [@problem_id:3334186]. You will derive a practical estimator for this crucial diagnostic and analyze its finite-sample bias, revealing the fundamental trade-off between bias and variance that governs MCMC output analysis.", "problem": "Consider a one-dimensional Random-Walk Metropolis (RWM) method with symmetric proposal density, which yields a reversible, stationary Markov chain $\\{X_t\\}_{t=1}^{N}$ with respect to a target density $\\pi(x)$. Let $h:\\mathbb{R}\\to\\mathbb{R}$ be a square-integrable observable, and define the centered time series $Y_t = h(X_t) - \\mathbb{E}_{\\pi}[h(X)]$, assumed to be strictly stationary with variance $\\sigma^{2} = \\mathbb{E}_{\\pi}[Y_t^{2}]$ and autocorrelation function $\\rho_{k} = \\mathbb{E}_{\\pi}[Y_t Y_{t+k}]/\\sigma^{2}$.\n\nThe integrated autocorrelation time is defined by the absolutely convergent series\n$$\n\\tau_{\\text{int}} \\equiv 1 + 2 \\sum_{k=1}^{\\infty} \\rho_{k},\n$$\nwhich, for reversible Markov chains arising from symmetric RWM proposals, quantifies the variance inflation of the sample mean $\\bar{Y}_{N} = \\frac{1}{N} \\sum_{t=1}^{N} Y_t$ relative to independent sampling via $\\operatorname{Var}(\\bar{Y}_{N}) \\approx \\sigma^{2}\\,\\tau_{\\text{int}}/N$ under standard mixing assumptions.\n\nDerive, from first principles, a practical lag-window estimator of $\\tau_{\\text{int}}$ based on the Bartlett window. Concretely, define the sample autocovariance \n$$\n\\hat{\\gamma}_{k}=\\frac{1}{N}\\sum_{t=1}^{N-k}(Y_{t}-\\bar{Y}_{N})(Y_{t+k}-\\bar{Y}_{N}),\n$$\nthe sample variance $\\hat{\\gamma}_{0}$, and the sample autocorrelation $\\hat{\\rho}_{k}=\\hat{\\gamma}_{k}/\\hat{\\gamma}_{0}$. For a chosen truncation lag $L \\in \\{1,2,\\dots,N-1\\}$ and Bartlett weights $w_{k}=1-\\frac{k}{L+1}$ for $1 \\le k \\le L$, define the estimator\n$$\n\\hat{\\tau}_{\\text{Bart}}(L)=1+2\\sum_{k=1}^{L}\\left(1-\\frac{k}{L+1}\\right)\\hat{\\rho}_{k}.\n$$\n\nAssume that, for the observable $Y_t$ generated by the symmetric RWM, the autocorrelation function is exactly geometric, $\\rho_{k}=\\phi^{k}$ for some $|\\phi|1$, which is a common and scientifically plausible working model for reversible chains when $h$ is dominated by a leading eigenmode of the Markov operator. Under this assumption and ignoring $O(N^{-1})$ corrections due to estimating the mean, compute the finite-sample truncation-and-weighting bias\n$$\n\\mathbb{E}[\\hat{\\tau}_{\\text{Bart}}(L)]-\\tau_{\\text{int}}\n$$\nin closed form as a function of $L$ and $\\phi$.\n\nIn addition, using the same working model, analyze how the leading-order variance of $\\hat{\\tau}_{\\text{Bart}}(L)$ scales with $N$ and $L$, clearly explaining the balance between truncation bias and sampling variance in terms of $L$ and $N$; your analysis should begin from the definitions above and standard properties of stationary Gaussian-like processes but must not rely on any shortcut formulas.\n\nExpress the requested bias as a single, simplified analytic expression in terms of $L$ and $\\phi$. No numerical rounding is required. Do not include units.", "solution": "We begin from the definition of the integrated autocorrelation time for a stationary time series,\n$$\n\\tau_{\\text{int}}=1+2\\sum_{k=1}^{\\infty}\\rho_{k},\n$$\nwhich captures the variance inflation factor for the sample mean $\\bar{Y}_{N}$ under standard assumptions for reversible Markov chains such as those induced by Random-Walk Metropolis (RWM) with symmetric proposals. The reversibility implies a self-adjoint Markov operator on $L^{2}(\\pi)$, which ensures that the autocorrelation function is absolutely summable for sufficiently mixing chains, and validates the series for $\\tau_{\\text{int}}$.\n\nA direct plug-in estimator that replaces $\\rho_{k}$ by $\\hat{\\rho}_{k}$ and truncates the infinite sum is generally noisy at large lags; thus, lag-window estimators are used to stabilize estimation by smoothly downweighting large lags. The Bartlett (triangular) window is a classical choice with weights $w_{k}=1-\\frac{k}{L+1}$ for $1\\le k\\le L$ and $w_{k}=0$ for $k > L$. From definitions,\n$$\n\\hat{\\tau}_{\\text{Bart}}(L)=1+2\\sum_{k=1}^{L}\\left(1-\\frac{k}{L+1}\\right)\\hat{\\rho}_{k}.\n$$\nThis is a practical estimator: it is positive for nonnegative empirical autocorrelations, reduces variance by downweighting higher lags, and under mild conditions is consistent as $L\\to\\infty$ with $L/N\\to 0$.\n\nTo analyze finite-sample bias and variance, we adopt a working model that is often appropriate in reversible Markov chains: the autocorrelation function is geometric, $\\rho_{k}=\\phi^{k}$ with $|\\phi|1$, corresponding to a dominant eigenmode. This arises, for instance, when $h$ is aligned with the leading nontrivial eigenfunction of the Markov operator so that $Y_{t}$ resembles a first-order autoregression. Under this model, the true integrated autocorrelation time is\n$$\n\\tau_{\\text{int}}=1+2\\sum_{k=1}^{\\infty}\\phi^{k}=1+2\\cdot\\frac{\\phi}{1-\\phi}=\\frac{1+\\phi}{1-\\phi}.\n$$\n\nWe first compute the bias. Ignoring $O(N^{-1})$ corrections due to mean estimation (we return to $N$-dependence in the variance section), an approximate expected value of the estimator is obtained by replacing $\\hat{\\rho}_{k}$ with its expectation $\\rho_{k}$:\n$$\n\\mathbb{E}[\\hat{\\tau}_{\\text{Bart}}(L)]\\approx 1+2\\sum_{k=1}^{L}\\left(1-\\frac{k}{L+1}\\right)\\rho_{k}.\n$$\nTherefore the bias due to truncation and weighting is\n$$\n\\mathbb{E}[\\hat{\\tau}_{\\text{Bart}}(L)]-\\tau_{\\text{int}}\\approx \\left[1+2\\sum_{k=1}^{L}\\left(1-\\frac{k}{L+1}\\right)\\rho_{k}\\right]-\\left[1+2\\sum_{k=1}^{\\infty}\\rho_{k}\\right].\n$$\nSimplifying,\n$$\n\\mathbb{E}[\\hat{\\tau}_{\\text{Bart}}(L)]-\\tau_{\\text{int}}\\approx 2\\sum_{k=1}^{L}\\left(1-\\frac{k}{L+1}-1\\right)\\rho_{k}-2\\sum_{k=L+1}^{\\infty}\\rho_{k}=-\\frac{2}{L+1}\\sum_{k=1}^{L}k\\,\\rho_{k}-2\\sum_{k=L+1}^{\\infty}\\rho_{k}.\n$$\nUnder $\\rho_{k}=\\phi^{k}$, both sums admit closed forms. The weighted partial sum is\n$$\n\\sum_{k=1}^{L}k\\,\\phi^{k}=\\phi\\,\\frac{1-(L+1)\\phi^{L}+L\\phi^{L+1}}{(1-\\phi)^{2}},\n$$\nand the tail sum is\n$$\n\\sum_{k=L+1}^{\\infty}\\phi^{k}=\\frac{\\phi^{L+1}}{1-\\phi}.\n$$\nTherefore,\n$$\n\\mathbb{E}[\\hat{\\tau}_{\\text{Bart}}(L)]-\\tau_{\\text{int}}\\approx -\\frac{2}{L+1}\\cdot \\phi\\,\\frac{1-(L+1)\\phi^{L}+L\\phi^{L+1}}{(1-\\phi)^{2}}-2\\cdot \\frac{\\phi^{L+1}}{1-\\phi}.\n$$\nThis is the exact truncation-and-weighting bias under the geometric correlation model, neglecting $O(N^{-1})$ corrections.\n\nWe now discuss the variance. The estimator is a linear combination of sample autocorrelations:\n$$\n\\hat{\\tau}_{\\text{Bart}}(L)-1=2\\sum_{k=1}^{L}w_{k}\\hat{\\rho}_{k}.\n$$\nFor a strictly stationary, approximately Gaussian time series $Y_{t}$ with short memory (as is typical for well-tuned RWM), the vector $(\\hat{\\gamma}_{1},\\dots,\\hat{\\gamma}_{L})$ and hence $(\\hat{\\rho}_{1},\\dots,\\hat{\\rho}_{L})$ is asymptotically normal with covariance scaling like $1/N$. More precisely, for $1\\le j,k\\le L$,\n$$\n\\operatorname{Cov}(\\hat{\\rho}_{j},\\hat{\\rho}_{k})\\approx \\frac{1}{N}\\,\\Xi_{jk},\n$$\nwhere the matrix $\\Xi$ depends on the true autocovariances of $Y_{t}$. Consequently,\n$$\n\\operatorname{Var}(\\hat{\\tau}_{\\text{Bart}}(L))\\approx \\frac{4}{N}\\sum_{j=1}^{L}\\sum_{k=1}^{L}w_{j}w_{k}\\,\\Xi_{jk}.\n$$\nUnder the AR(1) working model with unit variance $\\sigma^{2}=1$ and $\\rho_{k}=\\phi^{k}$, one obtains a Toeplitz-plus-Hankel structure for $\\Xi$ that is bounded uniformly in $L$; the double sum scales like $\\sum_{k=1}^{L}\\sum_{j=1}^{L}w_{j}w_{k}\\,\\rho_{|j-k|}$, which is $O(L)$ for $|\\phi|1$ because the Bartlett weights satisfy $0\\le w_{k}\\le 1$ and $\\rho_{|j-k|}$ is summable. Thus,\n$$\n\\operatorname{Var}(\\hat{\\tau}_{\\text{Bart}}(L))=O\\!\\left(\\frac{L}{N}\\right).\n$$\nThe mean-squared error balances the squared bias, which from the closed-form expression decays geometrically in $L$ as $|\\phi|^{2L}$, against the sampling variance, which grows linearly in $L/N$. An optimal choice of $L$ that minimizes the leading-order mean-squared error therefore grows like $\\log N$ for fixed $\\phi$, in contrast to settings with slowly decaying correlations where polynomial growth might be preferable. This selection reflects the reversible RWM’s fast decorrelation for well-chosen step sizes and acceptance rates.\n\nIn summary, for symmetric RWM with observable $Y_{t}$ modeled by $\\rho_{k}=\\phi^{k}$, the Bartlett estimator is practical and its finite-sample bias is explicitly given by the derived closed form. The variance scales as $L/N$, guiding the trade-off between truncation and sampling noise.", "answer": "$$\\boxed{-\\frac{2\\phi\\left[1-(L+1)\\phi^{L}+L\\phi^{L+1}\\right]}{(L+1)(1-\\phi)^{2}}-\\frac{2\\phi^{L+1}}{1-\\phi}}$$", "id": "3334186"}]}