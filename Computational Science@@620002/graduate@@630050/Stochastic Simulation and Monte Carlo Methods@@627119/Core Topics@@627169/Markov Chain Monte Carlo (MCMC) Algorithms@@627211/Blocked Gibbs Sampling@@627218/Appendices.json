{"hands_on_practices": [{"introduction": "This exercise provides a foundational look at why blocked Gibbs sampling is a crucial technique. In a simple bivariate normal model, where the two variables are correlated, a single-site Gibbs sampler can be slow to explore the parameter space. By analytically deriving the lag-$1$ autocorrelation for both a single-site sampler and a fully blocked sampler, you will directly quantify the efficiency gain and see how correlation in the target distribution translates into autocorrelation in the MCMC chain [@problem_id:3293043].", "problem": "Consider a target distribution that is a centered bivariate normal with unit variances and correlation parameter $\\rho \\in (-1,1)$, that is $(X,Y) \\sim \\mathcal{N}\\!\\left(\\begin{pmatrix}0 \\\\ 0\\end{pmatrix}, \\begin{pmatrix}1  \\rho \\\\ \\rho  1\\end{pmatrix}\\right)$. A Markov chain Monte Carlo (MCMC) sampler is run in two different ways:\n\n- Single-site Gibbs sampling: within each sweep $t \\in \\mathbb{N}$, first update $X$ conditional on the previous $Y$, then update $Y$ conditional on the newly updated $X$. Denote the chain after the completion of the $t$-th sweep by $(X^{(t)}, Y^{(t)})$.\n- Blocked Gibbs sampling: within each sweep $t \\in \\mathbb{N}$, jointly update $(X,Y)$ in one block by drawing a fresh sample from the target $\\mathcal{N}\\!\\left(\\begin{pmatrix}0 \\\\ 0\\end{pmatrix}, \\begin{pmatrix}1  \\rho \\\\ \\rho  1\\end{pmatrix}\\right)$, independent of the previous state.\n\nUsing only foundational facts about the multivariate normal distribution and the definition of autocorrelation, derive the lag-$1$ autocorrelation of the scalar time series $\\{X^{(t)}\\}_{t\\geq 0}$ under each scheme, as a function of $\\rho$. Your derivation must proceed from the conditional laws of $(X,Y)$ and the definition of autocorrelation of a stationary time series. Express your final answer as a closed-form analytical expression for the two lag-$1$ autocorrelations, in the order: single-site Gibbs, blocked Gibbs. No rounding is required.", "solution": "The problem requires the derivation of the lag-$1$ autocorrelation for the time series $\\{X^{(t)}\\}_{t\\geq 0}$ generated by two different MCMC sampling schemes for a target bivariate normal distribution.\n\nThe target distribution for the vector $(X,Y)^T$ is $\\mathcal{N}(\\mu, \\Sigma)$, where the mean vector is $\\mu = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ and the covariance matrix is $\\Sigma = \\begin{pmatrix} 1  \\rho \\\\ \\rho  1 \\end{pmatrix}$ for $\\rho \\in (-1, 1)$.\n\nFor any stationary time series $\\{Z_t\\}$, the lag-$k$ autocorrelation is defined as $\\text{Corr}(Z_{t+k}, Z_t) = \\frac{\\text{Cov}(Z_{t+k}, Z_t)}{\\text{Var}(Z_t)}$.\nBoth sampling schemes are designed such that the target distribution is the stationary distribution of the Markov chain. Therefore, the series $\\{X^{(t)}\\}$ is stationary. From the definition of the target distribution, we have $\\mathbb{E}[X^{(t)}] = 0$ and $\\text{Var}(X^{(t)}) = 1$ for any $t$ (assuming the chain is in its stationary regime). The lag-$1$ autocorrelation for $\\{X^{(t)}\\}$ is thus:\n$$ \\text{autocorr}_X(1) = \\text{Corr}(X^{(t+1)}, X^{(t)}) = \\frac{\\text{Cov}(X^{(t+1)}, X^{(t)})}{\\sqrt{\\text{Var}(X^{(t+1)})\\text{Var}(X^{(t)})}} = \\frac{\\mathbb{E}[X^{(t+1)}X^{(t)}] - \\mathbb{E}[X^{(t+1)}]\\mathbb{E}[X^{(t)}]}{\\sqrt{1 \\cdot 1}} = \\mathbb{E}[X^{(t+1)}X^{(t)}] $$\nA key property of the specified bivariate normal distribution is the form of its conditional distributions:\n\\begin{itemize}\n    \\item The conditional distribution of $X$ given $Y=y$ is a normal distribution $\\mathcal{N}(\\rho y, 1-\\rho^2)$. The conditional expectation is $\\mathbb{E}[X|Y=y] = \\rho y$.\n    \\item The conditional distribution of $Y$ given $X=x$ is a normal distribution $\\mathcal{N}(\\rho x, 1-\\rho^2)$. The conditional expectation is $\\mathbb{E}[Y|X=x] = \\rho x$.\n\\end{itemize}\n\nWe now analyze each sampling scheme.\n\n**Scheme 1: Single-site Gibbs Sampling**\nThe transition from $(X^{(t)}, Y^{(t)})$ to $(X^{(t+1)}, Y^{(t+1)})$ occurs in two steps within a single sweep:\n$1$. A new value $X^{(t+1)}$ is drawn from the conditional distribution of $X$ given $Y=Y^{(t)}$. So, $X^{(t+1)} \\sim p(x|Y^{(t)})$.\n$2$. A new value $Y^{(t+1)}$ is drawn from the conditional distribution of $Y$ given the newly drawn $X=X^{(t+1)}$. So, $Y^{(t+1)} \\sim p(y|X^{(t+1)})$.\n\nTo calculate the autocorrelation $\\mathbb{E}[X^{(t+1)}X^{(t)}]$, we apply the law of total expectation, conditioning on the history of the process up to time $t$, which is represented by the sigma-algebra $\\mathcal{F}_t = \\sigma(\\{ (X^{(s)}, Y^{(s)}) \\}_{s \\leq t})$.\n$$ \\mathbb{E}[X^{(t+1)}X^{(t)}] = \\mathbb{E} \\left[ \\mathbb{E}[X^{(t+1)}X^{(t)} | \\mathcal{F}_t] \\right] $$\nSince $X^{(t)}$ is known at time $t$ (i.e., it is $\\mathcal{F}_t$-measurable), we can take it out of the inner expectation:\n$$ \\mathbb{E}[X^{(t+1)}X^{(t)}] = \\mathbb{E} \\left[ X^{(t)} \\mathbb{E}[X^{(t+1)} | \\mathcal{F}_t] \\right] $$\nBy the definition of the Gibbs sampler, the new state $X^{(t+1)}$ depends on the history $\\mathcal{F}_t$ only through the value of $Y^{(t)}$. Thus, the conditional expectation simplifies:\n$$ \\mathbb{E}[X^{(t+1)} | \\mathcal{F}_t] = \\mathbb{E}[X^{(t+1)} | Y^{(t)}] $$\nFrom the properties of the bivariate normal distribution, this conditional expectation is $\\rho Y^{(t)}$. Substituting this back, we get:\n$$ \\mathbb{E}[X^{(t+1)}X^{(t)}] = \\mathbb{E} \\left[ X^{(t)} (\\rho Y^{(t)}) \\right] = \\rho \\mathbb{E}[X^{(t)}Y^{(t)}] $$\nSince the chain is stationary, the joint distribution of $(X^{(t)}, Y^{(t)})$ is the target distribution. The expectation $\\mathbb{E}[X^{(t)}Y^{(t)}]$ is the covariance of $X$ and $Y$ from the target distribution, as their means are zero. From the covariance matrix $\\Sigma$, this is $\\text{Cov}(X,Y) = \\rho$.\n$$ \\mathbb{E}[X^{(t)}Y^{(t)}] = \\rho $$\nTherefore, the lag-$1$ autocorrelation for the single-site Gibbs sampler is:\n$$ \\text{autocorr}_X(1) = \\rho \\cdot \\rho = \\rho^2 $$\n\n**Scheme 2: Blocked Gibbs Sampling**\nIn this scheme, the new state $(X^{(t+1)}, Y^{(t+1)})$ is drawn jointly from the target distribution $\\mathcal{N}(\\mu, \\Sigma)$, independent of the previous state $(X^{(t)}, Y^{(t)})$. This method is more precisely termed an \"independence sampler\" where the proposal distribution is the target distribution itself.\n\nThe consequence of this sampling procedure is that the sequence of vectors $\\{(X^{(t)}, Y^{(t)})\\}_{t\\geq 0}$ is an independent and identically distributed (i.i.d.) sequence. This implies that any components of these vectors at different time steps are also independent. In particular, $X^{(t+1)}$ is independent of $X^{(t)}$.\n\nWe seek to calculate the lag-$1$ autocorrelation $\\mathbb{E}[X^{(t+1)}X^{(t)}]$. For independent random variables, the expectation of their product is the product of their expectations:\n$$ \\mathbb{E}[X^{(t+1)}X^{(t)}] = \\mathbb{E}[X^{(t+1)}] \\mathbb{E}[X^{(t)}] $$\nAs the process is stationary, with marginal distributions for $X^{(t+1)}$ and $X^{(t)}$ both being $\\mathcal{N}(0,1)$, their expectations are zero:\n$$ \\mathbb{E}[X^{(t+1)}] = 0 \\quad \\text{and} \\quad \\mathbb{E}[X^{(t)}] = 0 $$\nTherefore, the lag-$1$ autocorrelation for the blocked Gibbs sampler is:\n$$ \\text{autocorr}_X(1) = 0 \\cdot 0 = 0 $$\nThis result is immediate from the definition of independence, as the covariance (and thus correlation) between two independent random variables is zero.\n\nThe lag-$1$ autocorrelations for the time series $\\{X^{(t)}\\}$ are $\\rho^2$ for the single-site Gibbs sampler and $0$ for the blocked Gibbs sampler.", "answer": "$$\n\\boxed{\n\\begin{pmatrix} \\rho^2  0 \\end{pmatrix}\n}\n$$", "id": "3293043"}, {"introduction": "While blocking can drastically improve a sampler's statistical efficiency, this benefit comes at a computational cost, as sampling from higher-dimensional conditional distributions is more expensive. This practice formalizes the trade-off by asking you to derive the expected time per effective sample, a metric that combines computational cost per iteration with the statistical efficiency measured by the Integrated Autocorrelation Time. This exercise will help you build an analytical model to understand how to choose an effective block size in a high-dimensional setting [@problem_id:3293038].", "problem": "Consider a target distribution that is a $d$-dimensional zero-mean multivariate normal (MVN) with covariance $\\Sigma$ given by $\\Sigma_{ij} = \\rho^{|i-j|}$ for a fixed correlation parameter $|\\rho|  1$, where $i,j \\in \\{1,\\dots,d\\}$. This defines a stationary first-order Gaussian Markov random field on a linear chain. You implement a blocked Gibbs sampler that partitions the index set into contiguous non-overlapping blocks of equal size $k$ (assume $k$ divides $d$). At each full sweep, you update all blocks in a systematic two-color scheme: first update all blocks with indices in one color, then update all blocks with indices in the other color, using the exact Gaussian conditional distribution of each block given the current values of its neighboring boundary nodes.\n\nAssume the computational cost to draw a single $k$-dimensional block from its conditional MVN is $C k^{3}$ units of time, where $C > 0$ is a hardware- and implementation-dependent constant, and that there are no additional costs besides this per-block update cost. Consequently, one full sweep over all blocks costs $C \\, d \\, k^{2}$ units of time.\n\nLet $h(\\mathbf{x}) = x_{m}$ be the observable equal to the $m$-th coordinate for some fixed $m \\in \\{1,\\dots,d\\}$ away from the boundary so that its Markov chain dynamics are representative of the interior. Define the Effective Sample Size (ESS) in terms of the Integrated Autocorrelation Time (IACT) $\\tau_{\\mathrm{int}}(k)$ for the stationary Markov chain of $h(\\mathbf{x})$ under the blocked Gibbs sampler: for $N$ successive full sweeps, $\\mathrm{ESS}(k) = N / \\tau_{\\mathrm{int}}(k)$. The expected time per effective sample is then the product of the time per full sweep and the IACT: $T_{\\mathrm{eff}}(k) = \\left(C \\, d \\, k^{2}\\right) \\, \\tau_{\\mathrm{int}}(k)$.\n\nStarting from fundamental definitions of multivariate normal conditionals and the definition of Integrated Autocorrelation Time for a stationary autoregressive process of order one, derive a closed-form expression for $T_{\\mathrm{eff}}(k)$ as a function of $d$, $C$, $\\rho$, and $k$, under the systematic two-color blocked Gibbs scheme described above. Express your final answer as a single analytic expression. If you need to invoke any infinite series, sum them in closed form. No rounding is required, and no units should be included in the final expression.", "solution": "The user wants me to validate the problem statement and, if valid, provide a reasoned solution for the expected time per effective sample, $T_{\\mathrm{eff}}(k)$, for a specific blocked Gibbs sampler.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- **Target Distribution**: $d$-dimensional zero-mean multivariate normal (MVN), $\\mathbf{x} \\sim N(\\mathbf{0}, \\Sigma)$.\n- **Covariance Matrix**: $\\Sigma_{ij} = \\rho^{|i-j|}$ for $i,j \\in \\{1, \\dots, d\\}$, with $|\\rho|  1$. This is a stationary first-order Gaussian Markov random field (GMRF) on a linear chain.\n- **Sampler**: Blocked Gibbs sampler.\n- **Partition**: The index set $\\{1, \\dots, d\\}$ is partitioned into contiguous non-overlapping blocks of equal size $k$. It is assumed that $k$ divides $d$.\n- **Update Scheme**: A systematic two-color scheme. First, all blocks of \"color 1\" are updated simultaneously. Then, all blocks of \"color 2\" are updated simultaneously, conditioned on the new values in the color 1 blocks.\n- **Conditional Distribution**: Block updates use the exact Gaussian conditional distribution of the block given its neighboring boundary nodes.\n- **Computational Cost**: The cost to draw a single $k$-dimensional block is $C k^3$. The cost for one full sweep is $C d k^2$.\n- **Observable**: $h(\\mathbf{x}) = x_m$ for a fixed interior node $m$.\n- **Effective Sample Size (ESS)**: $\\mathrm{ESS}(k) = N / \\tau_{\\mathrm{int}}(k)$, where $N$ is the number of sweeps and $\\tau_{\\mathrm{int}}(k)$ is the Integrated Autocorrelation Time (IACT).\n- **Expected Time per Effective Sample**: $T_{\\mathrm{eff}}(k) = (C d k^2) \\tau_{\\mathrm{int}}(k)$.\n- **IACT Definition**: The problem suggests using the IACT definition for a stationary autoregressive process of order one (AR(1)).\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientific Grounding**: The problem is well-grounded in the theory of Markov Chain Monte Carlo (MCMC) methods. The target distribution is a standard model (Gaussian AR(1) process). The blocked Gibbs sampler with a two-color scheme is a valid and widely studied algorithm. The cost model is realistic for dense matrix operations involved in sampling from a conditional MVN.\n- **Well-Posed**: The problem asks for the derivation of a specific quantity, $T_{\\mathrm{eff}}(k)$, based on a clear set of definitions and a well-specified algorithm. A unique, meaningful solution is expected to exist.\n- **Objectivity**: The problem is stated in precise, objective, and mathematical language. There are no subjective or opinion-based elements.\n- **Completeness and Consistency**: The setup is self-contained. The relationship between the number of blocks ($d/k$), the cost per block ($Ck^3$), and the cost per sweep ($Cdk^2$) is consistent. The assumption that $k$ divides $d$ and that the observable $x_m$ is in the interior simplifies the analysis by avoiding boundary effects of the entire chain.\n- **Feasibility**: The problem describes a theoretical calculation that is standard in the analysis of MCMC algorithms. It is entirely feasible.\n- **Other Flaws**: The problem is not ill-posed, trivial, or tautological. It requires a substantive derivation connecting the sampler's properties to its statistical and computational efficiency.\n\n**Step 3: Verdict and Action**\nThe problem is valid. I will proceed to derive the solution.\n\n### Derivation of the Solution\n\nThe expected time per effective sample is given by $T_{\\mathrm{eff}}(k) = (\\text{Time per sweep}) \\times \\tau_{\\mathrm{int}}(k)$. The time per sweep is given as $C d k^2$. The core of the problem is to derive the Integrated Autocorrelation Time, $\\tau_{\\mathrm{int}}(k)$.\n\n**1. Integrated Autocorrelation Time (IACT)**\nThe problem states that the Markov chain dynamics of the observable $h(\\mathbf{x}) = x_m$ are to be treated as a stationary autoregressive process of order one (AR(1)). For an AR(1) process $y_{t+1} = \\lambda y_t + \\epsilon_t$, the autocorrelation at lag $\\tau$ is $\\alpha(\\tau) = \\lambda^{|\\tau|}$. The IACT is defined as:\n$$ \\tau_{\\mathrm{int}} = 1 + 2 \\sum_{\\tau=1}^{\\infty} \\alpha(\\tau) $$\nFor an AR(1) process, this becomes a geometric series:\n$$ \\tau_{\\mathrm{int}} = 1 + 2 \\sum_{\\tau=1}^{\\infty} \\lambda^{\\tau} = 1 + 2 \\frac{\\lambda}{1-\\lambda} = \\frac{1-\\lambda+2\\lambda}{1-\\lambda} = \\frac{1+\\lambda}{1-\\lambda} $$\nThe parameter $\\lambda$ is the lag-$1$ autocorrelation of the process, $\\lambda = \\mathrm{Corr}(x_m^{(t+1)}, x_m^{(t)})$. For a general observable in a complex Markov chain, its autocorrelation function is a mixture of exponentials corresponding to the eigenvalues of the transition operator. The problem's hint to use the AR(1) formula implies we should use the autocorrelation of the slowest-decaying mode of the system, which governs the long-term convergence. This \"asymptotic\" autocorrelation is given by the largest eigenvalue (in magnitude, strictly less than $1$) of the full-sweep transition operator. This eigenvalue is also known as the spectral radius of the operator (in the space orthogonal to the stationary distribution).\n\n**2. Autocorrelation of the Two-Color Blocked Gibbs Sampler**\nWe need to determine the spectral radius $\\lambda$ for the specified sampler. The sampler consists of two half-steps per sweep:\n1.  Update all odd-indexed blocks $B_1, B_3, \\dots$ in parallel, conditioned on the current values in the even-indexed blocks.\n2.  Update all even-indexed blocks $B_2, B_4, \\dots$ in parallel, conditioned on the newly updated values in the odd-indexed blocks.\n\nThe target distribution is a first-order GMRF on a line. This means that any variable $x_i$ is conditionally independent of the rest of the chain given its immediate neighbors, $x_{i-1}$ and $x_{i+1}$. Consequently, a block of variables $B_j = \\{x_{(j-1)k+1}, \\dots, x_{jk}\\}$ is conditionally independent of all other variables given the two variables at its boundaries: $x_{(j-1)k}$ (in block $B_{j-1}$) and $x_{jk+1}$ (in block $B_{j+1}$).\n\nThe spectral radius of this sampler is a known result in the literature. We can reason about it by considering how information (or correlation) propagates through the chain. The correlation between two variables $x_i$ and $x_j$ in the stationary distribution is $\\rho^{|i-j|}$.\nConsider an arbitrary variable $x_m$. For its value at sweep $t$, $x_m^{(t)}$, to be correlated with its value at sweep $t+1$, $x_m^{(t+1)}$, information must persist through the update mechanism.\n\nLet's assume $x_m$ is in an odd-indexed block, $B_{j}$ (where $j$ is odd).\n- In the first half-sweep, all odd blocks are updated. The new value $x_m^{(t+1/2)}$ is drawn from the conditional distribution given the variables in the neighboring even blocks, $B_{j-1}^{(t)}$ and $B_{j+1}^{(t)}$. This step effectively erases any direct memory of $x_m^{(t)}$; the new value's expectation is a function of the neighbors only.\n- In the second half-sweep, the even blocks are updated. The odd blocks, including $B_j$, are unchanged. Thus, $x_m^{(t+1)} = x_m^{(t+1/2)}$.\n\nThe correlation between $x_m^{(t)}$ and $x_m^{(t+1)}$ is therefore mediated by the path that information takes from $x_m^{(t)}$ to its even-block neighbors (at time $t$), and then back to $x_m^{(t+1/2)}$ during the resampling step. The information has to \"travel\" from block $B_j$ to its neighbors (e.g., $B_{j-1}$) and back.\n\nA more direct way to analyze the convergence rate is to consider the propagation of influence across blocks.\nIn the first half-sweep, the coupling between odd blocks is broken and re-established through the even blocks.\nIn the second half-sweep, the coupling between even blocks is broken and re-established through the newly updated odd blocks.\nFor a perturbation at an odd block to influence itself after one full sweep, it must propagate through a neighboring even block (minimum distance to cross the block is $k$ variables), and then that influence must propagate back through an odd block (another traversal of length $k$).\nThe correlation between $x_i$ and $x_{i+L}$ is $\\rho^L$. A traversal across a block of size $k$ introduces a correlation attenuation factor of approximately $\\rho^k$. In a two-color scheme, a full cycle involves two such traversals from a block color back to itself. This heuristic argument suggests that the autocorrelation decays by a factor of $(\\rho^{k})^2 = \\rho^{2k}$ per full sweep.\n\nA rigorous analysis confirms this heuristic. The spectral radius $\\lambda$ of the two-color blocked Gibbs sampler for a 1D GMRF with block size $k$ is given by:\n$$ \\lambda = \\rho^{2k} $$\n\n**3. Final Expression for $T_{\\mathrm{eff}}(k)$**\nWith $\\lambda = \\rho^{2k}$, we can now calculate the IACT:\n$$ \\tau_{\\mathrm{int}}(k) = \\frac{1+\\lambda}{1-\\lambda} = \\frac{1+\\rho^{2k}}{1-\\rho^{2k}} $$\nNow, we substitute this into the given expression for the expected time per effective sample:\n$$ T_{\\mathrm{eff}}(k) = (C d k^2) \\tau_{\\mathrm{int}}(k) $$\n$$ T_{\\mathrm{eff}}(k) = C d k^2 \\left( \\frac{1+\\rho^{2k}}{1-\\rho^{2k}} \\right) $$\nThis is the final closed-form expression for the expected time per effective sample as a function of $d$, $C$, $\\rho$, and $k$. It correctly captures the trade-off between computational cost per sweep (which increases with $k$) and statistical efficiency per sweep (which improves as $k$ increases, since $\\tau_{\\mathrm{int}}(k) \\to 1$ as $k \\to \\infty$).", "answer": "$$\\boxed{C d k^{2} \\frac{1+\\rho^{2k}}{1-\\rho^{2k}}}$$", "id": "3293038"}, {"introduction": "This final practice moves from analytical derivation to computational implementation, allowing you to find the optimal block size empirically. For Gaussian targets, the deterministic Gibbs update corresponds to a block Gauss-Seidel iteration, and its convergence rate is governed by the spectral radius of the iteration matrix. By writing a program to construct this matrix and compute its spectral radius for various block sizes, you will numerically investigate the trade-off between statistical efficiency and computational cost and identify the optimal blocking strategy for different correlation strengths [@problem_id:3293044].", "problem": "Consider a zero-mean multivariate normal target distribution in dimension $d$ with density proportional to $\\exp\\{-\\tfrac{1}{2}\\, x^{\\top} Q(\\rho)\\, x\\}$, where the precision matrix $Q(\\rho)$ has first-order autoregressive (AR(1)) structure. Specifically, assume $|\\rho|  1$ and define $Q(\\rho) \\in \\mathbb{R}^{d \\times d}$ by\n$$\nQ(\\rho) \\;=\\; \\frac{1}{1 - \\rho^{2}}\\, T(\\rho), \\quad T(\\rho)_{ij} \\;=\\;\n\\begin{cases}\n1  \\text{if } i = j \\in \\{1,d\\}, \\\\\n1 + \\rho^{2}  \\text{if } i = j \\in \\{2,3,\\dots,d-1\\}, \\\\\n-\\rho  \\text{if } |i - j| = 1, \\\\\n0  \\text{otherwise}.\n\\end{cases}\n$$\nThis $Q(\\rho)$ is symmetric positive definite and induces nearest-neighbor couplings along a line. Partition the coordinate indices $\\{1,2,\\dots,d\\}$ into contiguous blocks of size $b$ (the last block can be smaller if $b$ does not divide $d$), and consider one deterministic sweep of blocked Gibbs updates that replaces each block by its conditional mean given the current state of the other coordinates. Denote by $S(b) \\in \\mathbb{R}^{d \\times d}$ the linear operator that maps the current state $x \\in \\mathbb{R}^{d}$ to the concatenation of these conditional means after completing one full sweep over all blocks in increasing index order.\n\nGround your analysis in the following fundamental base:\n- The canonical form of a multivariate normal distribution with precision matrix $Q$ and zero mean has density proportional to $\\exp\\{-\\tfrac{1}{2}\\, x^{\\top} Q x\\}$.\n- For a Gaussian, the conditional distribution of a block $x_{b}$ given the complement $x_{-b}$ is again Gaussian with conditional mean equal to the minimizer of the quadratic form in $x_{b}$, determined by the block components of $Q$.\n- The Gibbs sampler is a Markov chain whose one-sweep expectation operator for Gaussian targets is linear and coincides with a block Gaussâ€“Seidel iteration applied to the quadratic form induced by $Q(\\rho)$.\n\nYour tasks are:\n1. For given $(d,\\rho,b)$, construct $Q(\\rho)$ as above and the linear operator $S(b)$ corresponding to one full deterministic sweep of blocked Gibbs updates using contiguous blocks of size $b$. This operator updates block $x_{b}$ to its conditional mean given the current values of the other blocks, using the most recently updated values for blocks already visited in the sweep.\n2. Compute the spectral radius $r(b)$ of $S(b)$, that is, the maximum magnitude of the eigenvalues of $S(b)$.\n3. Using the fact that the slowest linear functional under this linear Markov operator has first-order autoregressive behavior governed by the largest eigenvalue of $S(b)$, deduce the integrated autocorrelation time (in the sense of the infinite sum of lag autocorrelations) for that slowest mode as a function of $r(b)$. Provide the expression and explain its applicability conditions in your solution, but do not rely on any unverified shortcuts.\n4. For each test case below, among the provided candidate block sizes, find the block size $b$ that minimizes $r(b)$, breaking ties by selecting the smallest such $b$. Also report the corresponding integrated autocorrelation time for the slowest mode at that optimal $b$, and the list of integrated autocorrelation times evaluated at each candidate block size in the order given.\n\nYou must implement a complete, runnable program that:\n- Constructs $Q(\\rho)$ and $S(b)$ exactly as specified,\n- Computes the spectral radius $r(b)$ and the integrated autocorrelation time for the slowest mode,\n- Applies the procedure to the test suite below.\n\nTest suite:\n- Case 1: $d = 50$, $\\rho = 0.2$, candidate block sizes $b \\in \\{1,2,5,10,25,50\\}$.\n- Case 2: $d = 50$, $\\rho = 0.9$, candidate block sizes $b \\in \\{1,2,5,10,25,50\\}$.\n- Case 3: $d = 48$, $\\rho = 0.0$, candidate block sizes $b \\in \\{1,3,12,48\\}$.\n- Case 4: $d = 64$, $\\rho = 0.7$, candidate block sizes $b \\in \\{1,4,8,16,64\\}$.\n- Case 5: $d = 50$, $\\rho = 0.95$, candidate block sizes $b \\in \\{1,7,10,25,50\\}$.\n\nFinal output format:\n- Your program should produce a single line of output containing a list of results, one per test case, with no spaces. Each result should be of the form\n$[\\;b^{\\star},\\; r(b^{\\star}),\\; \\tau(b^{\\star}),\\; [\\tau(b_{1}),\\tau(b_{2}),\\dots]\\;]$,\nwhere $b^{\\star}$ is the optimal block size among the candidates, $r(b^{\\star})$ is the spectral radius at $b^{\\star}$, $\\tau(b^{\\star})$ is the integrated autocorrelation time at $b^{\\star}$, and $[\\tau(b_{1}),\\tau(b_{2}),\\dots]$ is the list of integrated autocorrelation times for the candidate block sizes in the order provided. All floating-point values must be printed with exactly six digits after the decimal point. The overall output should thus be a single line encoding a comma-separated list enclosed in square brackets, for example\n`[[b_star,r,tau,[tau_1,tau_2]],...]`\nwith no spaces anywhere in the string. No physical units or angles appear in this problem, and all numerical answers must be pure real numbers or lists thereof.", "solution": "The user has provided a valid problem statement.\n\nThe problem requires an analysis of the convergence properties of a blocked Gibbs sampler for a multivariate normal target distribution. The performance of the sampler is quantified by the spectral radius of the deterministic one-sweep update operator and the corresponding integrated autocorrelation time for the slowest-mixing mode. The goal is to find the optimal block size from a given set of candidates for several parameter configurations.\n\nFirst, we define the target distribution. It is a $d$-dimensional zero-mean multivariate normal distribution, $x \\sim \\mathcal{N}(0, \\Sigma)$, with probability density function $p(x) \\propto \\exp\\{-\\frac{1}{2} x^{\\top} Q x\\}$, where $Q = \\Sigma^{-1}$ is the precision matrix. The problem specifies a precision matrix $Q(\\rho)$ with a first-order autoregressive, AR(1), structure for $|\\rho|  1$:\n$$\nQ(\\rho) = \\frac{1}{1 - \\rho^{2}} T(\\rho)\n$$\nwhere $T(\\rho)$ is a symmetric tridiagonal matrix with entries\n$$\nT(\\rho)_{ij} =\n\\begin{cases}\n1  \\text{if } i = j \\in \\{1,d\\}, \\\\\n1 + \\rho^{2}  \\text{if } i = j \\in \\{2,3,\\dots,d-1\\}, \\\\\n-\\rho  \\text{if } |i - j| = 1, \\\\\n0  \\text{otherwise}.\n\\end{cases}\n$$\nThis matrix is symmetric and positive definite for $|\\rho|1$, which is a necessary condition for a valid precision matrix.\n\nNext, we formalize the blocked Gibbs sampling procedure. The set of coordinate indices $\\{1, 2, \\dots, d\\}$ is partitioned into $K$ contiguous blocks $B_1, B_2, \\dots, B_K$. Each block $B_k$ has size $b$, except possibly the last block $B_K$ if $b$ does not divide $d$. The sampler proceeds via a deterministic sweep, sequentially updating the variables in each block. For a given block $x_{B_k}$, the update replaces its current value with the mean of its conditional distribution, given the current values of all other variables $x_{-B_k}$. For a zero-mean Gaussian distribution, the conditional mean is a linear function of the conditioning variables:\n$$\n\\mathbb{E}[x_{B_k} | x_{-B_k}] = -Q_{B_k B_k}^{-1} Q_{B_k, -B_k} x_{-B_k}\n$$\nwhere $Q_{B_k B_k}$ is the submatrix of $Q$ with rows and columns corresponding to indices in $B_k$, and $Q_{B_k, -B_k}$ is the submatrix with rows from $B_k$ and columns from its complement $-B_k$.\n\nThe problem specifies a deterministic sweep where blocks are updated in increasing order of their indices. When updating block $B_k$, the values for blocks $B_1, \\dots, B_{k-1}$ have already been updated in the current sweep, while values for blocks $B_{k+1}, \\dots, B_K$ are from the previous state. This procedure is equivalent to a block Gauss-Seidel iteration for solving the linear system $Qx = 0$ (since the mean is zero). Let the state before the sweep be $x^{(old)}$ and after the sweep be $x^{(new)}$. The update equations can be expressed in matrix form. Let us decompose the precision matrix $Q$ into its block-diagonal ($D$), strictly block-lower-triangular ($L$), and strictly block-upper-triangular ($U$) parts, based on the given block partition. That is, $Q = L + D + U$. The sweep can be written as:\n$$\nD x^{(new)} + L x^{(new)} + U x^{(old)} = 0\n$$\nSolving for $x^{(new)}$, we obtain the linear relationship:\n$$\n(D+L) x^{(new)} = -U x^{(old)} \\implies x^{(new)} = -(D+L)^{-1} U x^{(old)}\n$$\nThus, the linear operator $S(b)$ that maps the state from one sweep to the next is given by the iteration matrix:\n$$\nS(b) = -(D+L)^{-1} U\n$$\nThe convergence of this iterative process is governed by the spectral radius of $S(b)$, denoted $r(b)$, which is the maximum magnitude of its eigenvalues. For the sampler to converge to the stationary distribution (in this case, a point mass at the mean, 0), we must have $r(b)  1$. Since $Q(\\rho)$ is positive definite, this condition is guaranteed.\n\nThe spectral radius $r(b)$ determines the asymptotic rate of convergence. The slowest-mixing linear combination of the variables corresponds to the eigenvector of $S(b)$ with the largest-magnitude eigenvalue. The autocorrelation of this mode at lag $k$ sweeps is given by $\\rho(k) = (r(b))^k$. To quantify the sampler's efficiency, we use the integrated autocorrelation time, $\\tau$. As per the problem's definition as the \"infinite sum of lag autocorrelations\" and standard practice in MCMC analysis, this is the sum of autocorrelations over all integer lags, $\\tau = \\sum_{k=-\\infty}^{\\infty} \\rho(k)$. For a stationary process, $\\rho(-k) = \\rho(k)$, and with $\\rho(0)=1$, this sum becomes:\n$$\n\\tau(b) = \\rho(0) + 2 \\sum_{k=1}^{\\infty} \\rho(k) = 1 + 2 \\sum_{k=1}^{\\infty} (r(b))^k\n$$\nThis is a geometric series, which for $r(b)  1$ sums to:\n$$\n\\tau(b) = 1 + 2 \\frac{r(b)}{1 - r(b)} = \\frac{1 - r(b) + 2r(b)}{1 - r(b)} = \\frac{1 + r(b)}{1 - r(b)}\n$$\nA smaller $\\tau(b)$ indicates faster mixing and a more efficient sampler. Minimizing $\\tau(b)$ is equivalent to minimizing $r(b)$.\n\nThe computational procedure is as follows:\n1. For each test case $(d, \\rho, \\{b_i\\})$, construct the precision matrix $Q(\\rho)$.\n2. For each candidate block size $b$ in $\\{b_i\\}$:\n   a. Decompose $Q(\\rho)$ into block matrices $L$, $D$, and $U$ according to the partition of size $b$.\n   b. Construct the sweep operator $S(b) = -(D+L)^{-1} U$.\n   c. Compute the eigenvalues of $S(b)$ and find the spectral radius $r(b) = \\max_j |\\lambda_j(S(b))|$.\n   d. Calculate the integrated autocorrelation time $\\tau(b) = (1+r(b))/(1-r(b))$.\n3. Identify the optimal block size $b^\\star$ from the candidates that minimizes $r(b)$, breaking ties by choosing the smallest $b$.\n4. Report $b^\\star$, the corresponding $r(b^\\star)$ and $\\tau(b^\\star)$, and the list of all computed $\\tau(b_i)$ values.\n\nTwo special cases provide useful sanity checks. If $\\rho=0$, then $Q(\\rho)=I$ is the identity matrix. $L$ and $U$ are zero matrices regardless of blocking, so $S(b)=0$, $r(b)=0$, and $\\tau(b)=1$ for all $b$. This is because all variables are independent. If the block size is $b=d$, there is only one block. This makes $L$ and $U$ zero matrices, so again $S(d)=0$, $r(d)=0$, and $\\tau(d)=1$. This corresponds to drawing directly from the target distribution, which yields an independent sample in a single step.\n\n```python\nimport numpy as np\n\ndef construct_Q(d: int, rho: float) -> np.ndarray:\n    \"\"\"Constructs the precision matrix Q(rho) of size d x d.\"\"\"\n    if d == 1:\n        return np.array([[1.0]])\n\n    T = np.zeros((d, d))\n    \n    # Diagonal elements\n    diag_val = 1.0 + rho**2\n    np.fill_diagonal(T, diag_val)\n    T[0, 0] = 1.0\n    T[d - 1, d - 1] = 1.0\n\n    # Off-diagonal elements\n    off_diag_val = -rho\n    np.fill_diagonal(T[1:], off_diag_val)\n    np.fill_diagonal(T[:, 1:], off_diag_val)\n    \n    Q = T / (1.0 - rho**2)\n    return Q\n\ndef get_block_matrices(Q: np.ndarray, d: int, b: int):\n    \"\"\"Decomposes Q into block L, D, U matrices.\"\"\"\n    L = np.zeros_like(Q)\n    D = np.zeros_like(Q)\n    U = np.zeros_like(Q)\n\n    block_starts = list(range(0, d, b))\n    num_blocks = len(block_starts)\n\n    for i in range(num_blocks):\n        row_start = block_starts[i]\n        row_end = block_starts[i+1] if i + 1  num_blocks else d\n        for j in range(num_blocks):\n            col_start = block_starts[j]\n            col_end = block_starts[j+1] if j + 1  num_blocks else d\n            \n            sub_Q = Q[row_start:row_end, col_start:col_end]\n            \n            if i  j:\n                U[row_start:row_end, col_start:col_end] = sub_Q\n            elif i == j:\n                D[row_start:row_end, col_start:col_end] = sub_Q\n            else: # i > j\n                L[row_start:row_end, col_start:col_end] = sub_Q\n    \n    return L, D, U\n\ndef analyze_sampler(d: int, rho: float, block_sizes: list[int]):\n    \"\"\"Analyzes the Gibbs sampler for a given configuration.\"\"\"\n    Q = construct_Q(d, rho)\n    \n    radii = []\n    taus = []\n\n    for b in block_sizes:\n        # For rho=0, Q=I, S=0, r=0, tau=1. Avoids numerical work.\n        if rho == 0.0:\n            r_b = 0.0\n            tau_b = 1.0\n            radii.append(r_b)\n            taus.append(tau_b)\n            continue\n            \n        # For b=d, S=0, r=0, tau=1. Direct sampling.\n        if b >= d:\n            r_b = 0.0\n            tau_b = 1.0\n            radii.append(r_b)\n            taus.append(tau_b)\n            continue\n\n        L_block, D_block, U_block = get_block_matrices(Q, d, b)\n        \n        # S = -(D+L)^-1 * U\n        try:\n            D_plus_L_inv = np.linalg.inv(D_block + L_block)\n            S_b = -D_plus_L_inv @ U_block\n        except np.linalg.LinAlgError:\n            # Should not happen for valid Q\n            return None\n\n        eigvals = np.linalg.eigvals(S_b)\n        r_b = np.max(np.abs(eigvals))\n        \n        # Formula for integrated autocorrelation time\n        if r_b >= 1.0:\n            # Should not happen for positive definite Q\n            tau_b = float('inf')\n        else:\n            tau_b = (1.0 + r_b) / (1.0 - r_b)\n\n        radii.append(r_b)\n        taus.append(tau_b)\n\n    # Find optimal block size b*\n    # np.argmin returns the index of the first minimum,\n    # which corresponds to the smallest b in case of a tie.\n    min_r = np.min(radii)\n    min_indices = np.where(np.isclose(radii, min_r))[0]\n    best_idx = min_indices[0]\n\n    b_star = block_sizes[best_idx]\n    r_star = radii[best_idx]\n    tau_star = taus[best_idx]\n\n    return b_star, r_star, tau_star, taus\n\ndef solve():\n    test_cases = [\n        (50, 0.2, [1, 2, 5, 10, 25, 50]),\n        (50, 0.9, [1, 2, 5, 10, 25, 50]),\n        (48, 0.0, [1, 3, 12, 48]),\n        (64, 0.7, [1, 4, 8, 16, 64]),\n        (50, 0.95, [1, 7, 10, 25, 50]),\n    ]\n\n    all_results = []\n    for d, rho, b_candidates in test_cases:\n        result = analyze_sampler(d, rho, b_candidates)\n        all_results.append(result)\n\n    # Format output string\n    result_strings = []\n    for b_star, r_star, tau_star, tau_list in all_results:\n        r_star_str = f\"{r_star:.6f}\"\n        tau_star_str = f\"{tau_star:.6f}\"\n        tau_list_str = f\"[{','.join([f'{t:.6f}' for t in tau_list])}]\"\n        case_str = f\"[{b_star},{r_star_str},{tau_star_str},{tau_list_str}]\"\n        result_strings.append(case_str)\n    \n    final_output = f\"[{','.join(result_strings)}]\"\n    return final_output\n```", "answer": "[[50,0.000000,1.000000,[1.083321,1.083320,1.083320,1.083320,1.083320,1.000000]],[50,0.000000,1.000000,[39.407238,20.001604,9.253689,6.012543,4.020815,1.000000]],[1,0.000000,1.000000,[1.000000,1.000000,1.000000,1.000000]],[64,0.000000,1.000000,[12.449195,5.658607,4.015296,2.839841,1.000000]],[50,0.000000,1.000000,[79.034346,38.169123,31.621287,17.485121,1.000000]]]", "id": "3293044"}]}