{"hands_on_practices": [{"introduction": "The Ziggurat method relies on a pre-computed set of rectangles that approximate the target probability density function. This exercise guides you through a foundational step of building these tables by implementing an algorithm to create the necessary equal-area partitions [@problem_id:3356972]. By using numerical integration and root-finding, you will translate the core mathematical theory of a Ziggurat-like method into a practical construction algorithm, which is the essential first step in creating a functional sampler.", "problem": "Consider a nonnegative, integrable, monotonically non-increasing probability density function $f$ on the nonnegative real line $[0,\\infty)$, with total mass $$S = \\int_{0}^{\\infty} f(x)\\,dx.$$ A Ziggurat-like method can be constructed by partitioning the area under the curve using vertical slices of equal area. For a given integer $n \\ge 1$, we define a sequence of abscissae $$0 = x_0  x_1  \\dots  x_{n-1}  x_n$$ and corresponding ordinates $$y_i = f(x_i)\\quad \\text{for}\\quad i=0,1,\\dots,n,$$ such that the equal-area condition $$\\int_{x_{i-1}}^{x_i} f(x)\\,dx = A \\quad \\text{for all}\\quad i=1,2,\\dots,n$$ holds, where $$A = \\frac{S}{n}.$$ Because $f$ is integrable and non-increasing, the cumulative integral $$F(x) = \\int_{0}^{x} f(u)\\,du$$ is continuous, strictly increasing on intervals where $f0$, and satisfies $\\lim_{x\\to\\infty}F(x)=S$. Therefore, for $i=1,2,\\dots,n-1$, the equation $$F(x_i) - F(x_{i-1}) = A$$ has a unique finite solution $x_i$. For the terminal index $i=n$, the solution is $x_n = +\\infty$ in exact mathematics; in numerical computation one must approximate $x_n$ by a finite surrogate that attains a prescribed tail tolerance.\n\nYour task is to design and implement a constructive algorithm, using numerical root-finding and numerical quadrature of $f$, to precompute the arrays $\\{x_i\\}_{i=0}^{n}$ and $\\{y_i\\}_{i=0}^{n}$ satisfying the above properties. Your algorithm must:\n\n- Compute $S$ by numerical integration of $f$ over $[0,\\infty)$.\n- Set $A = S/n$.\n- Initialize $x_0 = 0$, $y_0 = f(0)$.\n- For $i=1,\\dots,n-1$, solve $$F(x_i) = i \\cdot A$$ for $x_i$ by bracketed root-finding, where $$F(x) = \\int_{0}^{x} f(u)\\,du,$$ and then set $y_i = f(x_i)$.\n- For the terminal index $i=n$, numerically approximate $x_n$ by solving $$F(x_n) = S - \\varepsilon$$ for a specified tolerance $\\varepsilon  0$, and set $y_n = f(x_n)$.\n- Verify numerically that the equal-area condition holds to within a small numerical error by computing $$E = \\max_{1\\le i\\le n}\\left|\\int_{x_{i-1}}^{x_i} f(x)\\,dx - A\\right|.$$\n\nThe algorithm must be implemented generically for any $f$ that satisfies the above assumptions, and it must use numerical quadrature for integrals and bracketed root-finding for the inversion tasks.\n\nTest Suite:\nImplement and run your algorithm for the following test cases. In each case, the final quantity to report is the real number $E$ defined above.\n\n- Case 1 (happy path): $f(x) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\!\\big(-\\frac{x^2}{2}\\big)$ for $x\\ge 0$ (the standard normal density restricted to $[0,\\infty)$), with $n=8$, and tail tolerance $\\varepsilon = 10^{-12}$.\n- Case 2 (smooth exponential): $f(x) = e^{-x}$ for $x\\ge 0$ (the rate-$1$ exponential density), with $n=64$, and tail tolerance $\\varepsilon = 10^{-12}$.\n- Case 3 (boundary condition): $f(x) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\!\\big(-\\frac{x^2}{2}\\big)$ for $x\\ge 0$, with $n=1$, and tail tolerance $\\varepsilon = 10^{-12}$.\n- Case 4 (large $n$ stress): $f(x) = e^{-x}$ for $x\\ge 0$, with $n=256$, and tail tolerance $\\varepsilon = 10^{-12}$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the cases listed above. Precisely, the printed line must be of the form\n$$[E_1,E_2,E_3,E_4],$$\nwhere $E_k$ is the real number computed for the $k$-th case. No other text should be printed.", "solution": "The user has provided a well-posed problem statement from the field of numerical methods for stochastic simulation. The task is to implement and test a constructive algorithm for precomputing the parameters of an equal-area vertical partition, which can be used in a Ziggurat-like method for random variate generation. The problem is scientifically grounded, formally specified, and internally consistent. It is therefore deemed valid.\n\nThe problem asks for the construction of a sequence of abscissae $$0 = x_0  x_1  \\dots  x_n$$ and corresponding ordinates $$y_i = f(x_i)$$ for a given non-negative, integrable, and monotonically non-increasing function $$f(x)$$ on $$[0, \\infty)$$. The construction must satisfy an equal-area condition for $$n$$ vertical strips.\n\nThe algorithm proceeds as follows:\n\n**1. Computation of Total Mass and Target Area**\nFirst, the total mass (integral) of the function $$f$$ over its domain $$[0, \\infty)$$ is computed using numerical quadrature:\n$$\nS = \\int_{0}^{\\infty} f(x)\\,dx\n$$\nThe domain is partitioned into $$n$$ segments, each intended to contain an equal portion of the total mass. The target area for each segment is therefore:\n$$\nA = \\frac{S}{n}\n$$\n\n**2. The Cumulative Integral Function**\nThe core of the construction relies on the cumulative integral function, $$F(x)$$, defined as:\n$$\nF(x) = \\int_{0}^{x} f(u)\\,du\n$$\nSince $$f(x) \\ge 0$$, $$F(x)$$ is a monotonically non-decreasing function. Given that $$f$$ is also specified as non-increasing, $$F(x)$$ is a concave function. The strict monotonicity of $$F(x)$$ on intervals where $$f(x)  0$$ guarantees that its inverse is well-defined, which is essential for finding the abscissae $$x_i$$. In this implementation, $$F(x)$$ will be evaluated at any given $$x$$ using numerical quadrature.\n\n**3. Iterative Construction of Abscissae**\nThe abscissae $$x_i$$ are determined sequentially.\n\n- **Initialization**: The sequence starts with the boundary condition at the origin:\n  $$\n  x_0 = 0, \\quad y_0 = f(0)\n  $$\n\n- **Interior Abscissae ($$i = 1, \\dots, n-1$$)**: For each interior index $$i$$, the abscissa $$x_i$$ is defined by the equal-area condition. The cumulative area up to $$x_i$$ should ideally be $$i \\cdot A$$. This leads to the equation:\n  $$\n  F(x_i) = \\int_{0}^{x_i} f(u)\\,du = i \\cdot A\n  $$\n  This equation must be solved for $$x_i$$. We can define a function $$g_i(x) = F(x) - iA$$ and find its root. Since $$F(x)$$ is monotonic, a unique root exists. We employ a bracketed root-finding algorithm, such as Brent's method, which is robust and efficient. To find a root of $$g_i(x) = 0$$, we must supply a bracket $$[a, b]$$ such that $$g_i(a) \\cdot g_i(b)  0$$.\n    - A natural lower bound is $$a = x_{i-1}$$, since the sequence of $$x_i$$ must be strictly increasing. At this point, $$F(x_{i-1}) \\approx (i-1)A$$, which implies $$g_i(x_{i-1}) = F(x_{i-1}) - iA \\approx (i-1)A - iA = -A  0$$.\n    - An upper bound $$b  x_{i-1}$$ can be found using an expanding search until $$g_i(b)  0$$.\n  Once $$x_i$$ is numerically determined, the corresponding ordinate is simply $$y_i = f(x_i)$$.\n\n- **Terminal Abscissa ($$i=n$$)**: In exact mathematics, $$x_n = \\infty$$ would ensure the last segment contains the remaining area $$A$$. For numerical computation, $$x_n$$ is approximated by a finite value that captures almost all the remaining mass in the tail of the distribution, leaving a small, prescribed residual area $$\\varepsilon$$. The condition is:\n  $$\n  \\int_{x_n}^{\\infty} f(x)\\,dx = \\varepsilon\n  $$\n  This is equivalent to finding $$x_n$$ such that $$F(x_n) = S - \\varepsilon$$. We solve for $$x_n$$ by finding the root of the function $$h(x) = F(x) - (S - \\varepsilon)$$. The bracketing and root-finding procedure is analogous to the one used for the interior abscissae, using $$x_{n-1}$$ as the lower bound for the search. Finally, $$y_n = f(x_n)$$.\n\n**4. Numerical Verification**\nThe final step is to verify the accuracy of the construction by computing the maximum absolute error $$E$$. For each segment $$i \\in \\{1, \\dots, n\\}$$, the actual area under the curve is computed via numerical quadrature:\n$$\nA_i = \\int_{x_{i-1}}^{x_i} f(x)\\,dx\n$$\nThe error for each segment is $$|A_i - A|$$. The overall error metric $$E$$ is the maximum of these individual errors:\n$$\nE = \\max_{1 \\le i \\le n} |A_i - A|\n$$\nDue to the definition of $$x_n$$, the area of the last segment $$A_n = F(x_n) - F(x_{n-1})$$ will be approximately $$(S-\\varepsilon) - (n-1)A = (nA-\\varepsilon) - (n-1)A = A-\\varepsilon$$. Thus, its contribution to the error is $$| (A-\\varepsilon) - A | = \\varepsilon$$. The total error $$E$$ is therefore expected to be at least $$\\varepsilon$$, and will be dominated by $$\\varepsilon$$ if the numerical errors from quadrature and root-finding for the other segments are smaller. To ensure this, the tolerance settings for the numerical integration routines must be more stringent than $$\\varepsilon$$.\n\nThis procedure is implemented using the `scipy` library, specifically `scipy.integrate.quad` for all numerical integrations and `scipy.optimize.brentq` for the bracketed root-finding.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.integrate import quad\nfrom scipy.optimize import brentq\nfrom typing import Callable\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n\n    def compute_ziggurat_params(f: Callable[[float], float], n: int, epsilon: float) -> float:\n        \"\"\"\n        Constructs the Ziggurat partitions and computes the maximum area error.\n\n        Args:\n            f: The probability density function on [0, inf).\n            n: The number of partitions.\n            epsilon: The tail tolerance for the last partition.\n\n        Returns:\n            The maximum error E.\n        \"\"\"\n        # Define high-precision quadrature options to ensure numerical errors\n        # do not dominate the prescribed tolerance epsilon.\n        quad_opts = {'epsabs': 1e-14, 'epsrel': 1e-14}\n        \n        # Step 1: Compute the total mass S.\n        S, _ = quad(f, 0, np.inf, **quad_opts)\n\n        # Step 2: Compute the target area A.\n        A = S / n\n\n        # Initialize arrays for abscissae and ordinates.\n        x = np.zeros(n + 1)\n        y = np.zeros(n + 1)\n\n        # Step 3: Initialize x_0 and y_0.\n        x[0] = 0.0\n        y[0] = f(x[0])\n\n        memoized_F = {}\n        def F(val: float) -> float:\n            \"\"\"Cumulative integral F(x) with memoization for performance.\"\"\"\n            if val in memoized_F:\n                return memoized_F[val]\n            if val == 0:\n                return 0.0\n            res, _ = quad(f, 0, val, **quad_opts)\n            memoized_F[val] = res\n            return res\n\n        # Step 4: Compute interior abscissae x_i for i = 1, ..., n-1.\n        if n > 1:\n            for i in range(1, n):\n                target_F = i * A\n                \n                # Define the function whose root we need to find.\n                g = lambda val: F(val) - target_F\n                \n                # Bracket the root. The lower bound is the previous abscissa.\n                a = x[i-1]\n                \n                # The value at the lower bound should be negative.\n                # g(a) = F(x_{i-1}) - i*A ≈ (i-1)*A - i*A = -A\n                \n                # Perform an expanding search for the upper bound.\n                b = a + 1.0 \n                # In the rare case of starting at a root...\n                if g(a) == 0:\n                    a -= 1e-9 \n                \n                while g(b)  0:\n                    a = b\n                    b *= 1.5\n                \n                # Solve for x_i using Brent's method.\n                x[i] = brentq(g, a, b)\n                y[i] = f(x[i])\n\n        # Step 5: Compute the terminal abscissa x_n.\n        target_F_n = S - epsilon\n        \n        # Define the function for the terminal root-finding problem.\n        h = lambda val: F(val) - target_F_n\n        \n        # Bracket the root for x_n. Lower bound is x_{n-1}.\n        a_n = x[n-1]\n        \n        # Expanding search for the upper bound.\n        b_n = a_n + 1.0\n        # h(a_n) = F(x_{n-1}) - (S - epsilon) ≈ (n-1)*A - S + epsilon = -A + epsilon  0\n        if h(a_n) >= 0: # Handle edge case where n=1 and S-epsilon is very small\n            b_n = a_n\n            a_n -= 1.0\n            while h(a_n) >= 0: a_n -= 1.0\n\n        while h(b_n)  0:\n            a_n = b_n\n            b_n *= 1.5\n\n        # Solve for x_n.\n        x[n] = brentq(h, a_n, b_n)\n        y[n] = f(x[n])\n\n        # Step 6: Verify the equal-area condition numerically.\n        errors = []\n        for i in range(1, n + 1):\n            slice_area, _ = quad(f, x[i-1], x[i], **quad_opts)\n            errors.append(abs(slice_area - A))\n            \n        E = max(errors)\n        return E\n\n    # Test Suite\n    # Case 1: Standard normal density (half-normal)\n    f_normal = lambda x: (1.0 / np.sqrt(2.0 * np.pi)) * np.exp(-x**2 / 2.0)\n    \n    # Case 2: Standard exponential density\n    f_exp = lambda x: np.exp(-x)\n\n    test_cases = [\n        {'f': f_normal, 'n': 8, 'epsilon': 1e-12},\n        {'f': f_exp, 'n': 64, 'epsilon': 1e-12},\n        {'f': f_normal, 'n': 1, 'epsilon': 1e-12},\n        {'f': f_exp, 'n': 256, 'epsilon': 1e-12},\n    ]\n\n    results = []\n    for case in test_cases:\n        E = compute_ziggurat_params(case['f'], case['n'], case['epsilon'])\n        results.append(E)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.15e}' for r in results)}]\")\n\nsolve()\n```", "id": "3356972"}, {"introduction": "The remarkable speed of the Ziggurat method stems from its use of integer-only operations, which avoids costly floating-point arithmetic in the main sampling loop. This practice challenges you to design and implement this critical optimization: a \"bit-slicing\" mapping from a single random integer to a layer index and intra-rectangle coordinates [@problem_id:3357000]. You will connect low-level implementation details to fundamental probability theory by proving the uniformity of your mapping and empirically verifying its statistical properties.", "problem": "You are implementing the core integer-only mapping primitive used in the Ziggurat method for transforming uniformly distributed $32$-bit integers into tuples comprising a layer index and intra-rectangle coordinates. Consider the following general requirement. You must design a mapping from a $32$-bit uniformly distributed integer $R \\in \\{0,1,\\dots,2^{32}-1\\}$ to a triple $(i,u,v)$ where $i$ is a layer index and $(u,v)$ are coordinates within the unit square to be later scaled to the width and height of the chosen layer’s rectangle. The mapping must satisfy the following constraints.\n\n1. Bit-slicing design:\n   - Choose nonnegative integers $b,m,n$ such that $b + m + n = 32$.\n   - The mapping must depend only on bit slicing, integer shifts, masking, and comparisons. Specifically, it must not use floating-point multiplications when extracting $i,u,v$ from $R$.\n   - Interpret $(u,v)$ as fixed-point fractions, that is, as rational numbers of the form $u = U / 2^m$ and $v = V / 2^n$ for integers $U \\in \\{0,1,\\dots,2^m-1\\}$ and $V \\in \\{0,1,\\dots,2^n-1\\}$.\n\n2. Layer-count support:\n   - For a number of layers $K = 2^b$, the mapping must be a bijection from $\\{0,1,\\dots,2^{32}-1\\}$ onto $\\{0,1,\\dots,K-1\\} \\times \\{0,1,\\dots,2^m-1\\} \\times \\{0,1,\\dots,2^n-1\\}$ under the fixed-point interpretation described above, and it must induce independence between $i$, $u$, and $v$.\n   - For a number of layers $K \\le 2^b$ that is not a power of two, the mapping must be extended by rejection on the layer index so that $i$ is exactly uniform on $\\{0,1,\\dots,K-1\\}$, while still using only integer operations and preserving independence between the accepted $i$ and the fixed-point coordinates $(u,v)$. The rejection must be based solely on the sliced index bits.\n\n3. Proof obligation:\n   - Starting from the foundational facts that a $32$-bit uniform integer is uniform over its state space and that distinct bit positions are independent Bernoulli with parameter $1/2$, prove that your mapping yields marginal uniformity of $i$ and of $(u,v)$ over their discrete grids, and prove the independence properties in both the $K = 2^b$ case and the rejection case $K \\le 2^b$.\n\n4. Program requirements:\n   - Implement your mapping for two scenarios:\n     (a) $K = 2^b$ with $(b,m,n) = (8,12,12)$.\n     (b) $K = 250 \\le 2^8$ with $(b,m,n) = (8,12,12)$ using rejection on the top $b$ bits only.\n   - For testing, you will use Monte Carlo simulation with a fixed seed. All randomness arises from independent draws of $32$-bit uniform integers.\n\n5. Statistical test suite and outputs:\n   Implement the following tests. Each test must produce a boolean based on a specified statistical decision rule. Use a significance level $\\alpha = 0.01$. When a chi-square test is required, use the standard chi-square goodness-of-fit test or chi-square test of independence without Yates’ continuity correction.\n\n   Test set A (power-of-two case):\n   - Parameters: $K = 2^8$, $b = 8$, $m = 12$, $n = 12$, sample size $N_A = 400000$.\n   - A1: Uniformity of layer index $i$ over $\\{0,1,\\dots,255\\}$ via chi-square goodness-of-fit with equal expected counts. Output boolean $T_{A1}$ indicating whether the null is not rejected at level $\\alpha$.\n   - A2: Uniformity of horizontal fixed-point coordinate $U \\in \\{0,1,\\dots,4095\\}$ via chi-square goodness-of-fit with equal expected counts. Output boolean $T_{A2}$.\n   - A3: Independence between $i$ and a coarsened horizontal coordinate $U_{\\mathrm{coarse}} \\in \\{0,1,\\dots,15\\}$ defined by the top $4$ bits of $U$, via chi-square test of independence on the $256 \\times 16$ contingency table. Output boolean $T_{A3}$.\n\n   Test set B (boundary determinism under bit slicing for power-of-two case):\n   - Parameters: $K = 2^8$, $b = 8$, $m = 12$, $n = 12$.\n   - B1: For $R = 0$, verify that $(i,U,V) = (0,0,0)$. Output boolean $T_{B1}$.\n   - B2: For $R = 2^{32}-1$, verify that $(i,U,V) = (255,4095,4095)$. Output boolean $T_{B2}$.\n\n   Test set C (non-power-of-two case with rejection on index bits):\n   - Parameters: $K = 250$, $b = 8$, $m = 12$, $n = 12$, sample size $N_C = 300000$.\n   - C1: Using only rejection on the index bits, test uniformity of $i$ on $\\{0,1,\\dots,249\\}$ by chi-square goodness-of-fit with equal expected counts. Output boolean $T_{C1}$.\n   - C2: Uniformity of $U \\in \\{0,1,\\dots,4095\\}$ via chi-square goodness-of-fit with equal expected counts. Output boolean $T_{C2}$.\n   - C3: Independence between $i$ and $U_{\\mathrm{coarse}} \\in \\{0,1,\\dots,15\\}$ via chi-square test of independence on the $250 \\times 16$ contingency table. Output boolean $T_{C3}$.\n\n6. Final output format:\n   Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[true,false,...]\") in the following order:\n   - $[T_{A1}, T_{A2}, T_{A3}, T_{B1}, T_{B2}, T_{C1}, T_{C2}, T_{C3}]$.\n\nAll angles are dimensionless and no physical units are involved. All answers must be the lowercase boolean strings 'true' or 'false'. The entire program must run without input, using its own internal test suite and a fixed random seed for reproducibility. Ensure all computations relevant to the mapping use only integer bit slicing, shifts, masking, comparisons, and integer arithmetic. Floating-point operations may be used only for statistical testing and do not affect the mapping design itself.", "solution": "We begin with the foundational facts for discrete uniform random variables on binary representations. Let $R$ be uniformly distributed on $\\{0,1,\\dots,2^{32}-1\\}$. Then:\n\n1. The mapping from $R$ to its bit vector $(B_{31},B_{30},\\dots,B_{0}) \\in \\{0,1\\}^{32}$, with $R = \\sum_{j=0}^{31} B_j 2^j$, is a bijection between $\\{0,1,\\dots,2^{32}-1\\}$ and $\\{0,1\\}^{32}$. Therefore $R$ uniform implies that the bit vector is uniform over $\\{0,1\\}^{32}$.\n\n2. Independence of distinct bit positions: For any subset $S \\subset \\{0,1,\\dots,31\\}$, the bits $(B_j)_{j \\in S}$ are independent and each marginal $B_j$ is Bernoulli with parameter $1/2$. This follows from the uniformity of the bit vector over the Cartesian product $\\{0,1\\}^{32}$.\n\nUsing these facts, we design an integer-only mapping from $R$ to the integer components $(I, U, V)$ by bit slicing. Choose nonnegative integers $b,m,n$ such that $b+m+n=32$, and define integer-valued coordinates\n$$\nI(R) \\equiv \\left\\lfloor \\frac{R}{2^{32-b}} \\right\\rfloor, \\quad\nU(R) \\equiv \\left\\lfloor \\frac{R \\bmod 2^{32-b}}{2^n} \\right\\rfloor, \\quad\nV(R) \\equiv R \\bmod 2^n.\n$$\nEquivalently, using shifts and masks,\n$$\nI(R) = R \\gg (32-b), \\quad\nU(R) = (R \\gg n) \\;\\\\; (2^m-1), \\quad\nV(R) = R \\;\\\\; (2^n-1),\n$$\nwhere $\\gg$ is the arithmetic right-shift on unsigned integers and $\\$ is the bitwise AND. The fixed-point coordinates are $u(R) = U(R)/2^m$ and $v(R) = V(R)/2^n$.\n\nCase 1: Power-of-two layer count $K = 2^b$.\nWe claim that the mapping\n$$\n\\Phi: \\{0,1,\\dots,2^{32}-1\\} \\to \\{0,1,\\dots,2^b-1\\} \\times \\{0,1,\\dots,2^m-1\\} \\times \\{0,1,\\dots,2^n-1\\},\n$$\ngiven by $\\Phi(R) = \\big(I(R), U(R), V(R)\\big)$, is a bijection. This follows because bit slicing partitions the $32$ bits into three disjoint blocks of lengths $b,m,n$; every triple $(i,u,v)$ corresponds to exactly one $R$ that has those bits in those positions. Therefore:\n- Uniformity of $I$: Since $\\Phi$ is a bijection and $R$ is uniform on a set of size $2^{32}$, the marginal distribution of $I$ is uniform on $\\{0,\\dots,2^b-1\\}$, since each $i$ appears in exactly $2^{m+n} = 2^{32-b}$ preimages.\n- Uniformity of $U$ and $V$: Similarly, each value for $U$ and each value for $V$ appears in exactly $2^{b+n}$ and $2^{b+m}$ preimages, respectively, yielding uniform discrete distributions on their grids.\n- Independence: Because the bit blocks are disjoint and the bit vector is uniform over $\\{0,1\\}^{32}$, the triple $(I,U,V)$ factors as a Cartesian product of independent coordinates. More explicitly, for any $(i,u,v)$,\n$$\n\\mathbb{P}(I=i, U=u, V=v) = \\frac{1}{2^{32}} = \\frac{1}{2^b} \\cdot \\frac{1}{2^m} \\cdot \\frac{1}{2^n} = \\mathbb{P}(I=i)\\, \\mathbb{P}(U=u)\\, \\mathbb{P}(V=v).\n$$\nThus $I$, $U$, and $V$ are mutually independent, and the fixed-point fractions $u = U/2^m$ and $v = V/2^n$ inherit these properties.\n\nCase 2: Non-power-of-two layer count $K \\le 2^b$ with rejection on index bits.\nDefine $I^\\ast(R) = \\left\\lfloor R/2^{32-b} \\right\\rfloor$ as before, but now accept the draw only if $I^\\ast(R) \\in \\{0,1,\\dots,K-1\\}$; otherwise, resample a fresh $R$ independently and repeat. Upon acceptance, define $U(R)$ and $V(R)$ from the same accepted $R$ using the lower $m$ and $n$ bits as above.\n\nWe now show that:\n- The accepted index $I$ is uniform on $\\{0,1,\\dots,K-1\\}$. Indeed, for each $i \\in \\{0,\\dots,K-1\\}$, the acceptance event is $\\{I^\\ast(R) = i\\}$, which has probability $2^{32-b}/2^{32} = 2^{-b}$. Conditional on acceptance, the probability that $I=i$ is $\\frac{2^{-b}}{K \\cdot 2^{-b}} = 1/K$.\n- Independence between $(U,V)$ and $I$: The decision to accept depends only on the top $b$ bits. By independence of disjoint bit blocks, the lower $m+n$ bits are independent of the top $b$ bits. Conditioning on the acceptance event (which is a function only of the top bits) does not alter the distribution of the lower bits; hence $(U,V)$ remain independent of the acceptance indicator and of the realized $I$. In particular, $\\mathbb{P}(U=u, V=v \\mid I=i) = 2^{-(m+n)}$, so $(U,V)$ are uniform on their grids and independent of $I$.\n- Uniformity of $U$ and $V$: Follows directly from the independence argument above.\n\nThus, this rejection scheme preserves the required uniformity and independence, while using only integer shifts, masks, and comparisons. No floating-point multiplication is used in the mapping itself; only at the point of interpreting fixed-point fractions might one divide by $2^m$ or $2^n$, which is not required for the mapping correctness and can be avoided if one retains integer fixed-point coordinates.\n\nStatistical validation design.\nTo empirically validate the mapping, we perform chi-square goodness-of-fit tests for uniformity and chi-square tests of independence at significance $\\alpha = 0.01$. We use:\n- Test set A for $K=2^8$, $(b,m,n)=(8,12,12)$ and sample size $N_A = 400000$, checking uniformity of $I$, uniformity of $U$, and independence between $I$ and a coarsened $U_{\\mathrm{coarse}}$ formed by the top $4$ bits of $U$.\n- Test set B for deterministic boundaries $R=0$ and $R=2^{32}-1$, verifying exact bit-slicing outputs.\n- Test set C for $K=250 \\le 2^8$ with rejection, $(b,m,n)=(8,12,12)$ and sample size $N_C = 300000$, checking uniformity of $I$, uniformity of $U$, and independence between $I$ and $U_{\\mathrm{coarse}}$.\n\nWe fix the pseudorandom number generator seed to ensure reproducibility. The expected outcomes are booleans:\n- $T_{A1}, T_{A2}, T_{A3}$ should be true if the mapping produces distributions consistent with uniformity and independence at level $\\alpha$.\n- $T_{B1}, T_{B2}$ should both be true if boundary bit slicing is implemented correctly.\n- $T_{C1}, T_{C2}, T_{C3}$ should be true for the rejection-based mapping.\n\nAll mapping computations use only integer bit slicing. Floating-point arithmetic appears only inside the statistical testing functions and does not affect the mapping’s uniformity properties. The final program aggregates the eight booleans in the specified order and prints them on a single line as required.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import chisquare, chi2_contingency\n\n# Mapping functions using integer-only bit slicing.\n\ndef slice_bits_power_of_two(R_uint32, b, m, n):\n    \"\"\"\n    Vectorized bit-slicing mapping for K = 2^b.\n    Input:\n        R_uint32: np.ndarray of dtype np.uint32\n        b, m, n: nonnegative integers with b+m+n = 32\n    Output:\n        i: np.ndarray of indices in [0, 2^b-1]\n        U: np.ndarray of ints in [0, 2^m-1] (fixed-point horizontal coordinate)\n        V: np.ndarray of ints in [0, 2^n-1] (fixed-point vertical coordinate)\n    \"\"\"\n    assert b + m + n == 32\n    R = R_uint32.astype(np.uint32)\n    if b == 0:\n        i = np.zeros_like(R, dtype=np.uint32)\n    else:\n        i = (R >> np.uint32(32 - b)).astype(np.uint32)\n    if n == 0:\n        V = np.zeros_like(R, dtype=np.uint32)\n        U = R  ((np.uint32(1)  np.uint32(m)) - np.uint32(1))\n    else:\n        V = (R  ((np.uint32(1)  np.uint32(n)) - np.uint32(1))).astype(np.uint32)\n        if m == 0:\n            U = np.zeros_like(R, dtype=np.uint32)\n        else:\n            U = ((R >> np.uint32(n))  ((np.uint32(1)  np.uint32(m)) - np.uint32(1))).astype(np.uint32)\n    return i, U, V\n\ndef slice_bits_with_rejection_K_le_2b(size, K, b, m, n, rng):\n    \"\"\"\n    Vectorized rejection scheme on the index bits only for K = 2^b.\n    Returns exactly 'size' accepted samples of (i, U, V).\n    \"\"\"\n    assert b + m + n == 32\n    assert 1 = K = (1  b)\n    i_out = np.empty(size, dtype=np.uint32)\n    U_out = np.empty(size, dtype=np.uint32)\n    V_out = np.empty(size, dtype=np.uint32)\n    filled = 0\n    # Use batches to reduce overhead; batch size chosen adaptively.\n    # Expected acceptance probability is K / 2^b.\n    acc_prob = K / (1  b)\n    # Choose a batch size that likely yields enough acceptances per batch.\n    base_batch = max(10000, int(1.5 * size / max(acc_prob, 1e-6)))\n    # Cap the batch size to keep memory reasonable\n    base_batch = min(base_batch, 1_000_000)\n\n    while filled  size:\n        batch = min(base_batch, size - filled)\n        R = rng.integers(0, 1  32, size=batch, dtype=np.uint32)\n        i, U, V = slice_bits_power_of_two(R, b, m, n)\n        mask = (i  K)\n        num_acc = int(mask.sum())\n        if num_acc > 0:\n            end_idx = filled + num_acc\n            if end_idx > size: # In case we over-generate\n                num_acc = size - filled\n                end_idx = size\n            i_out[filled:end_idx] = i[mask][:num_acc]\n            U_out[filled:end_idx] = U[mask][:num_acc]\n            V_out[filled:end_idx] = V[mask][:num_acc]\n            filled += num_acc\n    return i_out, U_out, V_out\n\n# Statistical tests.\n\ndef test_uniform_counts(values, num_categories, alpha=0.01):\n    \"\"\"\n    Chi-square goodness-of-fit test for uniformity over num_categories.\n    values: np.ndarray of nonnegative integers expected in [0, num_categories-1]\n    Returns True if p-value > alpha, else False.\n    \"\"\"\n    counts = np.bincount(values, minlength=num_categories).astype(np.int64)\n    # Ensure we restrict to exact length\n    counts = counts[:num_categories]\n    expected = np.full(num_categories, counts.sum() / num_categories)\n    stat, p = chisquare(counts, f_exp=expected)\n    return bool(p > alpha)\n\ndef test_independence(x, x_categories, y, y_categories, alpha=0.01):\n    \"\"\"\n    Chi-square test of independence on the contingency table formed by (x,y).\n    Returns True if p-value > alpha else False.\n    \"\"\"\n    # Build contingency table via flat indexing\n    flat = (x.astype(np.int64) * y_categories + y.astype(np.int64))\n    counts = np.bincount(flat, minlength=x_categories * y_categories).astype(np.int64)\n    table = counts.reshape((x_categories, y_categories))\n    chi2, p, dof, expected = chi2_contingency(table, correction=False)\n    return bool(p > alpha)\n\ndef main():\n    rng = np.random.default_rng(20250110)\n\n    # Test set A: K=2^8, (b,m,n)=(8,12,12), N_A=400000\n    b = 8\n    m = 12\n    n = 12\n    K_pow2 = 1  b\n    N_A = 400_000\n    R_A = rng.integers(0, 1  32, size=N_A, dtype=np.uint32)\n    i_A, U_A, V_A = slice_bits_power_of_two(R_A, b, m, n)\n\n    T_A1 = test_uniform_counts(i_A, K_pow2, alpha=0.01)\n    T_A2 = test_uniform_counts(U_A, 1  m, alpha=0.01)\n    # Coarse U: top 4 bits of U\n    U_A_coarse = (U_A >> np.uint32(m - 4)).astype(np.uint32)\n    T_A3 = test_independence(i_A, K_pow2, U_A_coarse, 1  4, alpha=0.01)\n\n    # Test set B: Boundary determinism for power-of-two case\n    R_low = np.uint32(0)\n    i_low, U_low, V_low = slice_bits_power_of_two(np.array([R_low], dtype=np.uint32), b, m, n)\n    T_B1 = bool((i_low[0] == 0) and (U_low[0] == 0) and (V_low[0] == 0))\n\n    R_high = np.uint32((1  32) - 1)\n    i_high, U_high, V_high = slice_bits_power_of_two(np.array([R_high], dtype=np.uint32), b, m, n)\n    T_B2 = bool((i_high[0] == (K_pow2 - 1)) and (U_high[0] == ((1  m) - 1)) and (V_high[0] == ((1  n) - 1)))\n\n    # Test set C: K=250 = 2^8, rejection on index bits only, N_C=300000\n    K_nonpow = 250\n    N_C = 300_000\n    i_C, U_C, V_C = slice_bits_with_rejection_K_le_2b(N_C, K_nonpow, b, m, n, rng)\n\n    T_C1 = test_uniform_counts(i_C, K_nonpow, alpha=0.01)\n    T_C2 = test_uniform_counts(U_C, 1  m, alpha=0.01)\n    U_C_coarse = (U_C >> np.uint32(m - 4)).astype(np.uint32)\n    T_C3 = test_independence(i_C, K_nonpow, U_C_coarse, 1  4, alpha=0.01)\n\n    results = [T_A1, T_A2, T_A3, T_B1, T_B2, T_C1, T_C2, T_C3]\n    print(f\"[{','.join(map(str, results)).lower()}]\")\n\nif __name__ == \"__main__\":\n    main()\n```", "id": "3357000"}, {"introduction": "Implementing a random number generator is only half the battle; rigorously verifying its correctness is a crucial and demanding task. This final practice shifts focus from implementation to validation, asking you to design a comprehensive statistical test suite capable of detecting subtle flaws in a Ziggurat sampler's output [@problem_id:3357047]. You will perform power analysis to determine the required sample sizes for detecting common errors like mean bias or incorrect tail probabilities, embodying the standards of professional scientific computing.", "problem": "A developer implements a piecewise-constant and tail-exponential accept–reject sampler based on the ziggurat method to generate independent and identically distributed samples that should follow a standard normal distribution. The sampler is suspected to have the following subtle deviations relative to the ideal standard normal target: a small mean bias of magnitude $\\delta = 5 \\times 10^{-4}$, a small variance inflation of magnitude $\\epsilon = 10^{-3}$ so that the true variance is $\\sigma^{2} = 1 + \\epsilon$, and a tail under-sampling where the probability of exceeding a fixed threshold is reduced by a factor of $1 - \\eta$ with $\\eta = 0.05$ for the event $\\{|X|  t\\}$ at $t = 4$. The samples are independent and the target distribution under the null hypothesis is $\\mathcal{N}(0,1)$. The testing objective is to empirically validate correctness of the sampler by constructing a scientifically rigorous test suite that controls the family-wise error rate at level $\\alpha = 0.01$ across all tests and achieves power at least $0.8$ for each of the three deviations described above. The suite should include tests with complementary sensitivity in the center, scale, global shape, and tails. Assume that planning may conservatively use a Bonferroni per-test level $\\alpha^{\\star} = \\alpha/m$ for $m$ tests, and that normal and chi-square approximations are acceptable for sample size calculations at large $n$.\n\nWhich option most correctly specifies such a test suite and provides order-of-magnitude sample size calculations that meet the power requirement for the given alternatives, together with scientifically justified reasoning for sensitivity and error control?\n\nA. Use only the one-sample Kolmogorov–Smirnov test at level $\\alpha = 0.01$ with $n = 10^{6}$; this single omnibus test has sufficient sensitivity to detect mean, variance, and tail deviations with power exceeding $0.8$ and controls family-wise error by testing once.\n\nB. Use a suite comprising a one-sample $z$-test for the mean at per-test level $\\alpha^{\\star}$, a chi-square test for variance at per-test level $\\alpha^{\\star}$, a binomial proportion test for tail exceedances at threshold $t = 4$ at per-test level $\\alpha^{\\star}$, plus both Kolmogorov–Smirnov and Anderson–Darling goodness-of-fit tests for global shape, with family-wise error controlled via Holm–Bonferroni at $\\alpha = 0.01$. Planning with Bonferroni $\\alpha^{\\star} = \\alpha/5$ yields the following sample sizes to achieve power at least $0.8$ under the stated alternatives: for the mean bias $\\delta = 5 \\times 10^{-4}$, $n \\approx 6.2 \\times 10^{7}$; for the variance inflation $\\epsilon = 10^{-3}$, $n \\approx 3.1 \\times 10^{7}$; for the tail under-sampling $\\eta = 0.05$ at $t = 4$, using $p_{t} = 2 \\Phi(-4) \\approx 6.334 \\times 10^{-5}$, $n \\approx 9.8 \\times 10^{7}$. Selecting $n = 10^{8}$ ensures power at least $0.8$ across all three deviations while controlling family-wise error, with Anderson–Darling providing enhanced tail sensitivity complementary to Kolmogorov–Smirnov.\n\nC. Use a Student’s $t$-test for the mean at level $\\alpha = 0.05$ and an Anderson–Darling test at level $\\alpha = 0.05$; ignore multiple-testing adjustments to preserve power. With $n = 10^{7}$, both tests will surpass $0.8$ power for the specified mean, variance, and tail deviations due to the omnibus sensitivity of Anderson–Darling.\n\nD. Replace global distribution tests with moment tests up to the fourth central moment at level $\\alpha = 0.01$, combined with a variance test; omit tail-specific exceedance tests as redundant once kurtosis is controlled. With $n = 10^{6}$, the combined tests provide $0.8$ power for the given deviations and control family-wise error by virtue of fewer tests.", "solution": "This problem requires designing a rigorous statistical test suite to validate a standard normal random number generator. The key constraints are controlling the family-wise error rate (FWER) at $\\alpha = 0.01$ while achieving a power of at least $0.8$ for three specific, subtle deviations: a mean bias, a variance inflation, and a tail probability error. We must perform sample size calculations to verify the feasibility of the proposed tests.\n\n### Power Analysis and Sample Size Calculation\n\nLet's plan for a test suite with $m$ tests. To control FWER, we use a Bonferroni-corrected significance level for planning each test: $\\alpha^{\\star} = \\alpha/m$. The required power is $1-\\beta = 0.8$, so the Type II error rate is $\\beta = 0.2$. The corresponding standard normal quantiles are $z_{1-\\beta} = z_{0.8} \\approx 0.8416$. Option B suggests a suite of $m=5$ tests, so we use $\\alpha^\\star = 0.01/5 = 0.002$. For a two-sided test, the critical value is determined by $\\alpha^\\star/2 = 0.001$, giving $z_{1-\\alpha^\\star/2} = z_{0.999} \\approx 3.0902$.\n\n1.  **Mean Bias Test:**\n    *   Hypotheses: $H_0: \\mu = 0$ vs. $H_A: \\mu = \\delta = 5 \\times 10^{-4}$.\n    *   Test: A two-sided one-sample z-test is most powerful. The required sample size $n_{\\text{mean}}$ is:\n        $$ n_{\\text{mean}} \\approx \\left( \\frac{z_{1-\\alpha^\\star/2} + z_{1-\\beta}}{\\delta} \\right)^2 = \\left( \\frac{3.0902 + 0.8416}{5 \\times 10^{-4}} \\right)^2 \\approx (7863.6)^2 \\approx 6.18 \\times 10^7 $$\n\n2.  **Variance Inflation Test:**\n    *   Hypotheses: $H_0: \\sigma^2 = 1$ vs. $H_A: \\sigma^2 = 1+\\epsilon$ with $\\epsilon = 10^{-3}$.\n    *   Test: A $\\chi^2$-test for variance is appropriate. For large $n$, we can use a normal approximation. The sample size $n_{\\text{var}}$ is approximately:\n        $$ n_{\\text{var}} \\approx 1 + \\frac{2(z_{1-\\alpha^\\star/2} + z_{1-\\beta})^2}{\\epsilon^2} = 1 + \\frac{2(3.0902 + 0.8416)^2}{(10^{-3})^2} \\approx 3.09 \\times 10^7 $$\n\n3.  **Tail Probability Test:**\n    *   Hypotheses: $H_0: p = p_0$ vs. $H_A: p = p_A = p_0(1-\\eta)$, where $p = P(|X| > 4)$, $\\eta = 0.05$.\n    *   Test: A test of proportions, using a normal approximation.\n    *   First, calculate the null probability: $p_0 = 2\\Phi(-4) \\approx 2 \\times (3.167 \\times 10^{-5}) = 6.334 \\times 10^{-5}$.\n    *   The alternative probability is $p_A = p_0(1-0.05) \\approx 6.017 \\times 10^{-5}$.\n    *   The sample size $n_{\\text{tail}}$ for a test of proportions is:\n        $$ n_{\\text{tail}} \\approx \\frac{\\left( z_{1-\\alpha^\\star/2}\\sqrt{p_0} + z_{1-\\beta}\\sqrt{p_A} \\right)^2}{(p_0 - p_A)^2} $$\n        $$ n_{\\text{tail}} \\approx \\frac{\\left( 3.0902\\sqrt{6.334 \\times 10^{-5}} + 0.8416\\sqrt{6.017 \\times 10^{-5}} \\right)^2}{(6.334 \\times 10^{-5} \\times 0.05)^2} \\approx 9.66 \\times 10^7 $$\n\n**Conclusion on Sample Size:** To achieve the desired power for all three specific tests, we must choose a sample size that is at least the maximum of the individual requirements: $n = \\max(6.18 \\times 10^7, 3.09 \\times 10^7, 9.66 \\times 10^7) = 9.66 \\times 10^7$. A choice of $n=10^8$ comfortably satisfies this.\n\n### Evaluation of Options\n\nA. This option is incorrect. The Kolmogorov-Smirnov test, while being an omnibus test, lacks the statistical power of specific tests for detecting small deviations in mean, variance, or tail probabilities. Our calculations show that a sample size of $n=10^6$ is insufficient by more than an order of magnitude to detect the specified mean bias, even with a much more powerful z-test. The KS test is also notoriously weak for detecting errors in the far tails.\n\nB. This option is correct.\n    *   **Test Suite:** It proposes a comprehensive suite using the most powerful specific tests for each targeted flaw (z-test for mean, $\\chi^2$-test for variance, binomial/proportion test for tails). It correctly supplements these with omnibus tests (KS and AD) for general validation, noting the enhanced tail sensitivity of the Anderson-Darling test.\n    *   **Error Control:** It correctly states the need to control the FWER and proposes a valid method (Holm-Bonferroni), using the standard Bonferroni correction for the conservative planning phase.\n    *   **Sample Size:** The sample size calculations are accurate and consistent with our derivations. The final choice of $n=10^8$ is scientifically justified, as it exceeds the minimum requirement of $9.66 \\times 10^7$ needed for the most demanding test (the tail probability test).\n\nC. This option is incorrect. It makes two critical errors: it explicitly advises ignoring multiple testing correction, which violates the FWER constraint of $\\alpha=0.01$. Secondly, it vastly underestimates the required sample size; $n=10^7$ is far too small to achieve the desired power for any of the specified defects.\n\nD. This option is incorrect. It wrongly claims that a test on kurtosis makes a specific tail exceedance test redundant. Kurtosis is a global measure and is not a substitute for a direct test of probability at a specific threshold. More importantly, it proposes a sample size of $n=10^6$, which our power analysis shows is inadequate by more than an order of magnitude to detect the specified variance inflation or mean bias.\n\nTherefore, option B is the only one that presents a scientifically rigorous, quantitatively justified, and complete plan that meets all requirements of the problem.", "answer": "$$\\boxed{B}$$", "id": "3357047"}]}