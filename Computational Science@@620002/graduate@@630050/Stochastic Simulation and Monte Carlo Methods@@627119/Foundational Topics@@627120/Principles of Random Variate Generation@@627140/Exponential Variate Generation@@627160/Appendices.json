{"hands_on_practices": [{"introduction": "The cornerstone of generating random variates from many distributions is the inverse transform method. This practice guides you through the fundamental process of deriving and implementing a sampler for the exponential distribution based on this powerful theorem [@problem_id:2403697]. You will not only generate the samples but also apply a rigorous statistical test to validate that your implementation correctly produces variates from the target distribution, a crucial skill in any simulation study.", "problem": "You are asked to formalize, implement, and validate sampling from the exponential distribution using first principles. Consider the exponential distribution with rate parameter $\\lambda \\in (0,\\infty)$ and cumulative distribution function $F(x) = \\mathbb{P}(X \\le x)$.\n\nTask:\n1. Using the inverse transform principle, generate $n$ independent samples from the exponential distribution with rate $\\lambda$ from independent draws of a variable $U$ that is uniformly distributed on the unit interval. You must rely on the property that if $U$ is uniformly distributed on the unit interval, then $F^{-1}(U)$ has cumulative distribution function $F$.\n2. For each parameter triple $(\\lambda,n,\\alpha)$, with $\\lambda \\in (0,\\infty)$, $n \\in \\mathbb{N}$, and $\\alpha \\in (0,1)$ as a significance level, conduct a one-sample distributional goodness-of-fit test for the exponential distribution with rate $\\lambda$ based on the $n$ samples you generated. Report the $p$-value and a boolean decision that is True if you do not reject the null hypothesis that the data follow an exponential distribution with rate $\\lambda$ at significance level $\\alpha$, and False otherwise. The $p$-value must be reported as a real number.\n3. For reproducibility, initialize your pseudo-random number generator once at the beginning with the fixed integer seed $s=123456789$.\n\nTest suite:\n- Case $1$: $(\\lambda,n,\\alpha) = (0.7,1000,0.05)$\n- Case $2$: $(\\lambda,n,\\alpha) = (2.3,200,0.01)$\n- Case $3$: $(\\lambda,n,\\alpha) = (0.1,500,0.10)$\n- Case $4$: $(\\lambda,n,\\alpha) = (10.0,100,0.05)$\n- Case $5$: $(\\lambda,n,\\alpha) = (0.5,5,0.20)$\n\nYour program must:\n- Implement the sampler described in item $1$ to generate the $n$ samples for each case.\n- Implement the test described in item $2$ and compute the $p$-value for each case.\n- For each case $i \\in \\{1,2,3,4,5\\}$, output two values in order: the $p$-value rounded to six decimal places and the boolean decision as specified above.\n\nFinal output format:\n- Your program should produce a single line of output containing all results as a comma-separated list enclosed in square brackets. The list must have length $10$ and be ordered as $[p_1,d_1,p_2,d_2,p_3,d_3,p_4,d_4,p_5,d_5]$, where $p_i$ is the $p$-value (rounded to six decimal places) for case $i$ and $d_i$ is the corresponding boolean decision.", "solution": "The problem requires the generation of random samples from an exponential distribution using the inverse transform method, followed by a statistical validation of these samples using a goodness-of-fit test. The solution is developed in two parts: first, the derivation of the sampling algorithm, and second, the specification of the statistical test.\n\n**1. Inverse Transform Sampling for the Exponential Distribution**\n\nThe exponential distribution is characterized by a rate parameter $\\lambda > 0$. Its probability density function (PDF) for a random variable $X$ is given by:\n$$\nf(x; \\lambda) = \\begin{cases} \\lambda e^{-\\lambda x} & \\text{if } x \\ge 0 \\\\ 0 & \\text{if } x < 0 \\end{cases}\n$$\nThe cumulative distribution function (CDF), $F(x)$, represents the probability that the random variable $X$ takes a value less than or equal to $x$. It is obtained by integrating the PDF from $0$ to $x$:\n$$\nF(x) = \\mathbb{P}(X \\le x) = \\int_0^x \\lambda e^{-\\lambda t} dt = \\left[ -e^{-\\lambda t} \\right]_0^x = 1 - e^{-\\lambda x} \\quad \\text{for } x \\ge 0\n$$\nThe inverse transform sampling method is founded on the principle that if $U$ is a random variable drawn from a standard uniform distribution on the interval $(0, 1)$, then the random variable $X = F^{-1}(U)$ will have the distribution characterized by the CDF $F$.\n\nTo apply this method, we must first find the inverse of the CDF, denoted $F^{-1}(u)$. We set $u = F(x)$ and solve for $x$:\n$$\nu = 1 - e^{-\\lambda x}\n$$\nRearranging the terms to isolate $x$:\n$$\ne^{-\\lambda x} = 1 - u\n$$\nTaking the natural logarithm of both sides:\n$$\n-\\lambda x = \\ln(1 - u)\n$$\nFinally, solving for $x$ yields the inverse CDF:\n$$\nx = F^{-1}(u) = -\\frac{1}{\\lambda} \\ln(1 - u)\n$$\nTherefore, to generate a sample $x$ from an exponential distribution with rate $\\lambda$, one generates a random number $u$ from $\\text{Uniform}(0,1)$ and applies the transformation $x = -\\frac{1}{\\lambda} \\ln(1 - u)$.\n\nA useful simplification arises from the property that if $U \\sim \\text{Uniform}(0,1)$, then the random variable $V = 1 - U$ is also distributed as $\\text{Uniform}(0,1)$. This allows us to replace $1 - u$ with $u$ in the formula, leading to the computationally equivalent and more direct expression:\n$$\nx = -\\frac{1}{\\lambda} \\ln(u)\n$$\nThis formula will be implemented to generate the $n$ samples for each test case. The procedure commences by initializing a pseudo-random number generator with the specified seed $s=123456789$ to ensure reproducibility. Then, for each test case $(\\lambda, n, \\alpha)$, we draw $n$ independent values $u_1, u_2, \\dots, u_n$ from $\\text{Uniform}(0,1)$ and compute the corresponding exponential samples $x_i = -\\frac{1}{\\lambda} \\ln(u_i)$.\n\n**2. Goodness-of-Fit Testing**\n\nAfter generating the sample data $\\{x_1, \\dots, x_n\\}$, a one-sample distributional goodness-of-fit test must be performed. The null hypothesis, $H_0$, is that the data are drawn from an exponential distribution with the specified rate parameter $\\lambda$. The alternative hypothesis, $H_1$, is that the data are not from this distribution.\n\nA standard and appropriate test for this scenario is the one-sample Kolmogorov-Smirnov (K-S) test. This test compares the empirical cumulative distribution function (ECDF) of the sample data, $F_n(x)$, with the theoretical CDF of the hypothesized distribution, $F(x) = 1 - e^{-\\lambda x}$. The ECDF is defined as:\n$$\nF_n(x) = \\frac{1}{n} \\sum_{i=1}^n \\mathbb{I}(x_i \\le x)\n$$\nwhere $\\mathbb{I}(\\cdot)$ is the indicator function. The K-S test statistic, $D_n$, is the maximum absolute difference between the ECDF and the theoretical CDF over all possible values of $x$:\n$$\nD_n = \\sup_x | F_n(x) - F(x) |\n$$\nThe distribution of the $D_n$ statistic under the null hypothesis is known and is independent of the specific continuous distribution being tested. This property allows for the calculation of a $p$-value, defined as the probability of observing a test statistic as extreme as, or more extreme than, the one computed from the data, assuming $H_0$ is true.\n\nThe decision rule is based on the significance level $\\alpha$:\n- If the $p$-value is less than $\\alpha$, we reject the null hypothesis $H_0$.\n- If the $p$-value is greater than or equal to $\\alpha$, we do not reject the null hypothesis $H_0$.\n\nThe problem requires a boolean decision that is True if we do not reject $H_0$. Thus, the decision for a given test case will be the result of the comparison $p\\text{-value} \\ge \\alpha$. For implementation, we will use the `scipy.stats.kstest` function. The exponential distribution implementation in `scipy` is parameterized by a location `loc` and a `scale` parameter. To match our target PDF $\\lambda e^{-\\lambda x}$, we must set `loc`=0 and `scale`=$1/\\lambda$.\n\nThe complete algorithm for each test case $(\\lambda, n, \\alpha)$ is:\n1. Generate $n$ uniform random numbers $u_1, \\dots, u_n$ using a generator seeded with $s=123456789$.\n2. Transform these into exponential samples $x_i = -\\frac{1}{\\lambda} \\ln(u_i)$.\n3. Perform the K-S test on the samples $\\{x_i\\}$ against the 'expon' distribution with parameters `loc=0` and `scale`=$1/\\lambda$.\n4. Obtain the $p$-value from the test result.\n5. Compute the boolean decision as $p\\text{-value} \\ge \\alpha$.\n6. Report the $p$-value rounded to six decimal places and the boolean decision.\n\nThis procedure will be executed for all five specified test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import stats\n\ndef solve():\n    \"\"\"\n    Solves the problem by generating exponential samples via inverse transform\n    and validating them with a Kolmogorov-Smirnov goodness-of-fit test.\n\n    The process adheres to the problem's requirements for validation,\n    reproducibility with a fixed seed, and specific output formatting.\n    \"\"\"\n    \n    # Global seed for reproducibility, as per problem statement.\n    # The modern `default_rng` is used for generating pseudo-random numbers.\n    seed = 123456789\n    rng = np.random.default_rng(seed)\n\n    # Test suite: each tuple is (lambda, n, alpha)\n    test_cases = [\n        (0.7, 1000, 0.05),\n        (2.3, 200, 0.01),\n        (0.1, 500, 0.10),\n        (10.0, 100, 0.05),\n        (0.5, 5, 0.20),\n    ]\n\n    results = []\n    for lambda_val, n, alpha in test_cases:\n        # Task 1: Generate n samples from the exponential distribution with rate lambda.\n        # This is implemented using the inverse transform method.\n        # If U is a random variable from Uniform(0, 1), then X = -ln(U)/lambda\n        # is a random variable from Exponential(lambda).\n        \n        # Generate n uniform random numbers in the interval [0.0, 1.0).\n        uniform_samples = rng.uniform(size=n)\n        \n        # Apply the inverse CDF transformation.\n        exponential_samples = -np.log(uniform_samples) / lambda_val\n\n        # Task 2: Conduct a one-sample Kolmogorov-Smirnov test.\n        # The null hypothesis (H0) is that the generated data follows an\n        # exponential distribution with the given rate lambda_val.\n        # The `scipy.stats.expon` distribution is parameterized by `loc` and `scale`.\n        # For a rate lambda, the scale parameter is 1/lambda and loc is 0.\n        scale_param = 1.0 / lambda_val\n        ks_statistic, p_value = stats.kstest(exponential_samples, 'expon', args=(0, scale_param))\n        \n        # The decision is True if we do not reject H0 at significance level alpha.\n        # This occurs when the p-value is greater than or equal to alpha.\n        decision = p_value >= alpha\n        \n        # Format results as specified in the problem statement.\n        p_value_rounded = round(p_value, 6)\n        \n        results.append(p_value_rounded)\n        results.append(decision)\n\n    # Final print statement in the exact required format.\n    # The map(str, ...) function correctly converts boolean values to \"True\"/\"False\".\n    print(f\"[{','.join(map(str, results))}]\")\n\n# Execute the main function to solve the problem and print the output.\nsolve()\n```", "id": "2403697"}, {"introduction": "Once samples are generated, they are often used to estimate the parameters of the underlying model. This exercise challenges you to move from data generation to statistical inference by analyzing the properties of the maximum likelihood estimator (MLE) for the exponential rate parameter $\\lambda$ [@problem_id:3307724]. By deriving the estimator's exact variance and comparing it to the theoretical limit of precision given by the Cramér-Rao lower bound, you will gain a deeper understanding of estimator efficiency and performance.", "problem": "A stochastic simulation produces independent and identically distributed (i.i.d.) samples from an exponential distribution with unknown rate parameter $\\lambda>0$ using inverse transform sampling applied to uniform pseudorandom numbers on $[0,1]$. Let $X_{1},\\dots,X_{n}$ denote the resulting i.i.d. sample from the exponential distribution with probability density function $f(x;\\lambda)=\\lambda \\exp(-\\lambda x)$ for $x\\geq 0$ and $f(x;\\lambda)=0$ otherwise. Assume $n\\geq 3$.\n\nStarting only from the model specification and the definition of the likelihood function, perform the following:\n\n- Derive the maximum likelihood estimator (MLE) $\\hat{\\lambda}$ of $\\lambda$.\n- Derive the exact finite-sample variance $\\operatorname{Var}(\\hat{\\lambda})$.\n- Compute the Fisher information for $n$ observations and the corresponding Cramér–Rao lower bound (CRLB) for unbiased estimators of $\\lambda$.\n- Assess efficiency by determining whether the MLE attains the CRLB for finite $n$, and compute the asymptotic efficiency defined as $\\lim_{n\\to\\infty}\\frac{\\text{CRLB}}{\\operatorname{Var}(\\hat{\\lambda})}$.\n\nProvide your final result as a single row matrix in the order $\\bigl(\\hat{\\lambda},\\,\\operatorname{Var}(\\hat{\\lambda}),\\,\\text{CRLB},\\,\\lim_{n\\to\\infty}\\frac{\\text{CRLB}}{\\operatorname{Var}(\\hat{\\lambda})}\\bigr)$. Do not include any units. The answer must be given in exact symbolic form with no numerical rounding.", "solution": "The problem is to derive the maximum likelihood estimator (MLE) for the rate parameter $\\lambda$ of an exponential distribution, its variance, the Cramér–Rao lower bound (CRLB), and its asymptotic efficiency. We are given a sample $X_1, \\dots, X_n$ of independent and identically distributed (i.i.d.) random variables from an exponential distribution with probability density function (PDF) $f(x;\\lambda) = \\lambda \\exp(-\\lambda x)$ for $x \\geq 0$. We are given that the sample size is $n \\geq 3$.\n\n**Part 1: Derivation of the Maximum Likelihood Estimator (MLE) $\\hat{\\lambda}$**\n\nThe likelihood function $L(\\lambda)$ for the i.i.d. sample $X_1, \\dots, X_n$ is the product of the individual probability density functions:\n$$L(\\lambda; X_1, \\dots, X_n) = \\prod_{i=1}^{n} f(X_i; \\lambda) = \\prod_{i=1}^{n} \\lambda \\exp(-\\lambda X_i)$$\nAssuming all $X_i \\geq 0$, this simplifies to:\n$$L(\\lambda) = \\lambda^n \\exp\\left(-\\lambda \\sum_{i=1}^{n} X_i\\right)$$\nTo find the MLE, we maximize $L(\\lambda)$ with respect to $\\lambda$. It is more convenient to maximize the log-likelihood function, $\\ell(\\lambda) = \\ln L(\\lambda)$, as the logarithm is a monotonically increasing function.\n$$\\ell(\\lambda) = \\ln\\left(\\lambda^n \\exp\\left(-\\lambda \\sum_{i=1}^{n} X_i\\right)\\right) = n\\ln\\lambda - \\lambda \\sum_{i=1}^{n} X_i$$\nTo find the maximum, we compute the derivative of $\\ell(\\lambda)$ with respect to $\\lambda$ and set it to zero. This is the score function.\n$$\\frac{d\\ell}{d\\lambda} = \\frac{n}{\\lambda} - \\sum_{i=1}^{n} X_i$$\nSetting the derivative to zero and solving for $\\lambda$ gives the estimator $\\hat{\\lambda}$:\n$$\\frac{n}{\\hat{\\lambda}} - \\sum_{i=1}^{n} X_i = 0 \\implies \\hat{\\lambda} = \\frac{n}{\\sum_{i=1}^{n} X_i}$$\nThis can also be written in terms of the sample mean $\\bar{X} = \\frac{1}{n} \\sum_{i=1}^{n} X_i$ as $\\hat{\\lambda} = \\frac{1}{\\bar{X}}$. To confirm this is a maximum, we check the second derivative:\n$$\\frac{d^2\\ell}{d\\lambda^2} = -\\frac{n}{\\lambda^2}$$\nSince $\\lambda > 0$ and $n \\geq 3$, the second derivative is always negative, which confirms that $\\hat{\\lambda}$ is indeed the maximum likelihood estimator.\nThe first result is $\\hat{\\lambda} = \\frac{n}{\\sum_{i=1}^{n} X_i}$.\n\n**Part 2: Derivation of the Variance $\\operatorname{Var}(\\hat{\\lambda})$**\n\nThe estimator is $\\hat{\\lambda} = \\frac{n}{S_n}$, where $S_n = \\sum_{i=1}^{n} X_i$. Since each $X_i$ is an i.i.d. exponential random variable with rate $\\lambda$, i.e., $X_i \\sim \\text{Exponential}(\\lambda)$, an equivalent description is $X_i \\sim \\text{Gamma}(1, \\lambda)$. The sum of $n$ i.i.d. $\\text{Gamma}(1, \\lambda)$ random variables is a $\\text{Gamma}(n, \\lambda)$ random variable. Thus, $S_n \\sim \\text{Gamma}(n, \\lambda)$.\nThe PDF of $S_n$ is $f_{S_n}(s) = \\frac{\\lambda^n s^{n-1} \\exp(-\\lambda s)}{\\Gamma(n)}$ for $s \\geq 0$.\nTo find $\\operatorname{Var}(\\hat{\\lambda}) = \\operatorname{Var}\\left(\\frac{n}{S_n}\\right)$, we first need the moments of $S_n^{-1}$. The $k$-th moment of an inverse Gamma variable $S_n^{-1}$ is given by $E[S_n^k]$. For $S_n \\sim \\text{Gamma}(\\alpha, \\beta)$ with PDF $f(s) = \\frac{\\beta^\\alpha s^{\\alpha-1} e^{-\\beta s}}{\\Gamma(\\alpha)}$, we have $E[S_n^k] = \\frac{\\Gamma(\\alpha+k)}{\\Gamma(\\alpha)\\beta^k}$, provided $\\alpha+k>0$.\nIn our case, $\\alpha=n$ and the rate is $\\beta=\\lambda$. So, for any $k$ such that $n+k>0$:\n$$E[S_n^k] = \\frac{\\Gamma(n+k)}{\\Gamma(n)\\lambda^k}$$\nTo calculate the variance, we need $E[\\hat{\\lambda}]$ and $E[\\hat{\\lambda}^2]$.\nFirst, $E[\\hat{\\lambda}] = E\\left[\\frac{n}{S_n}\\right] = n E[S_n^{-1}]$. Using $k=-1$:\n$$E[S_n^{-1}] = \\frac{\\Gamma(n-1)}{\\Gamma(n)\\lambda^{-1}} = \\frac{(n-2)!}{(n-1)!}\\lambda = \\frac{\\lambda}{n-1}$$\nThis is valid for $n-1>0$, or $n>1$. Since we are given $n \\geq 3$, this condition is met.\n$$E[\\hat{\\lambda}] = \\frac{n\\lambda}{n-1}$$\nNext, $E[\\hat{\\lambda}^2] = E\\left[\\left(\\frac{n}{S_n}\\right)^2\\right] = n^2 E[S_n^{-2}]$. Using $k=-2$:\n$$E[S_n^{-2}] = \\frac{\\Gamma(n-2)}{\\Gamma(n)\\lambda^{-2}} = \\frac{(n-3)!}{(n-1)!}\\lambda^2 = \\frac{\\lambda^2}{(n-1)(n-2)}$$\nThis is valid for $n-2>0$, or $n>2$. Since we are given $n \\geq 3$, this condition is also met.\n$$E[\\hat{\\lambda}^2] = \\frac{n^2\\lambda^2}{(n-1)(n-2)}$$\nThe variance is $\\operatorname{Var}(\\hat{\\lambda}) = E[\\hat{\\lambda}^2] - (E[\\hat{\\lambda}])^2$.\n$$\\operatorname{Var}(\\hat{\\lambda}) = \\frac{n^2\\lambda^2}{(n-1)(n-2)} - \\left(\\frac{n\\lambda}{n-1}\\right)^2 = \\frac{n^2\\lambda^2}{(n-1)(n-2)} - \\frac{n^2\\lambda^2}{(n-1)^2}$$\n$$= n^2\\lambda^2 \\left( \\frac{n-1 - (n-2)}{(n-1)^2(n-2)} \\right) = n^2\\lambda^2 \\left( \\frac{1}{(n-1)^2(n-2)} \\right)$$\nThus, the second result is $\\operatorname{Var}(\\hat{\\lambda}) = \\frac{n^2\\lambda^2}{(n-1)^2(n-2)}$.\n\n**Part 3: Fisher Information and Cramér–Rao Lower Bound (CRLB)**\n\nThe Fisher information for a single observation, $I(\\lambda)$, is given by $I(\\lambda) = -E\\left[\\frac{d^2}{d\\lambda^2} \\ln f(X;\\lambda)\\right]$.\nThe log-PDF for one observation is $\\ln f(x;\\lambda) = \\ln\\lambda - \\lambda x$. The derivatives are:\n$$\\frac{d}{d\\lambda} \\ln f(x;\\lambda) = \\frac{1}{\\lambda} - x$$\n$$\\frac{d^2}{d\\lambda^2} \\ln f(x;\\lambda) = -\\frac{1}{\\lambda^2}$$\nSince the second derivative is a constant with respect to $x$, its expectation is the constant itself:\n$$I(\\lambda) = -E\\left[-\\frac{1}{\\lambda^2}\\right] = \\frac{1}{\\lambda^2}$$\nFor $n$ i.i.d. observations, the total Fisher information is additive: $I_n(\\lambda) = n I(\\lambda) = \\frac{n}{\\lambda^2}$.\nThe Cramér–Rao Lower Bound (CRLB) provides a lower bound on the variance of any unbiased estimator of $\\lambda$. The bound is given by the reciprocal of the Fisher information:\n$$\\text{CRLB} = \\frac{1}{I_n(\\lambda)} = \\frac{1}{n/\\lambda^2} = \\frac{\\lambda^2}{n}$$\nThe third result is $\\text{CRLB} = \\frac{\\lambda^2}{n}$.\n\n**Part 4: Efficiency Assessment**\n\nFirst, we assess if the MLE attains the CRLB for finite $n$. The CRLB applies to unbiased estimators. We found that $E[\\hat{\\lambda}] = \\frac{n\\lambda}{n-1}$, which is not equal to $\\lambda$ for any finite $n$. Since $\\hat{\\lambda}$ is a biased estimator, it cannot attain the CRLB for unbiased estimators.\n\nNext, we compute the asymptotic efficiency, which is defined as the limit of the ratio of the CRLB to the variance of the estimator as $n \\to \\infty$.\n$$\\text{Asymptotic Efficiency} = \\lim_{n\\to\\infty} \\frac{\\text{CRLB}}{\\operatorname{Var}(\\hat{\\lambda})}$$\nSubstituting the expressions for CRLB and $\\operatorname{Var}(\\hat{\\lambda})$:\n$$\\lim_{n\\to\\infty} \\frac{\\frac{\\lambda^2}{n}}{\\frac{n^2\\lambda^2}{(n-1)^2(n-2)}} = \\lim_{n\\to\\infty} \\frac{\\lambda^2}{n} \\cdot \\frac{(n-1)^2(n-2)}{n^2\\lambda^2} = \\lim_{n\\to\\infty} \\frac{(n-1)^2(n-2)}{n^3}$$\nTo evaluate the limit, we look at the ratio of the leading terms of the polynomials in $n$:\n$$\\lim_{n\\to\\infty} \\frac{(n^2 - 2n + 1)(n-2)}{n^3} = \\lim_{n\\to\\infty} \\frac{n^3 - 4n^2 + 5n - 2}{n^3}$$\n$$= \\lim_{n\\to\\infty} \\left(1 - \\frac{4}{n} + \\frac{5}{n^2} - \\frac{2}{n^3}\\right) = 1$$\nThe asymptotic efficiency is $1$, meaning the MLE $\\hat{\\lambda}$ is asymptotically efficient.\nThe fourth result is $1$.\n\n**Final Result Collation**\nThe four results, in the specified order, are:\n$1$. $\\hat{\\lambda} = \\frac{n}{\\sum_{i=1}^n X_i}$\n$2$. $\\operatorname{Var}(\\hat{\\lambda}) = \\frac{n^2 \\lambda^2}{(n-1)^2(n-2)}$\n$3$. $\\text{CRLB} = \\frac{\\lambda^2}{n}$\n$4$. $\\lim_{n\\to\\infty}\\frac{\\text{CRLB}}{\\operatorname{Var}(\\hat{\\lambda})} = 1$\nThese are to be presented as a single row matrix.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{n}{\\sum_{i=1}^{n} X_i} & \\frac{n^2 \\lambda^2}{(n-1)^2(n-2)} & \\frac{\\lambda^2}{n} & 1\n\\end{pmatrix}\n}\n$$", "id": "3307724"}, {"introduction": "A mathematically correct algorithm can sometimes fail in practice due to the limitations of finite-precision computer arithmetic. This problem explores a critical, real-world pitfall in implementing the inverse transform method: numerical instability arising from catastrophic cancellation [@problem_id:3314503]. By analyzing the behavior of the standard formula under floating-point arithmetic, you will learn to identify and remedy such issues, a vital skill for developing robust and reliable scientific software.", "problem": "You are asked to analyze the numerical stability of inverse transform sampling for continuous variables under finite-precision arithmetic. Consider generating samples from the exponential distribution with rate parameter $\\lambda>0$ by applying the inverse transform to a uniform variate $U\\sim\\mathrm{Uniform}(0,1)$, so that $X=F^{-1}(U)$ where $F(x)=1-\\exp(-\\lambda x)$. A direct algebraic rearrangement gives $X=-\\log(1-U)/\\lambda$. Assume standard floating-point arithmetic conforming to the Institute of Electrical and Electronics Engineers (IEEE) 754 standard in base $2$ with rounding to nearest, and let the machine precision be $\\varepsilon_{\\mathrm{mach}}$ (for IEEE 754 binary64, $\\varepsilon_{\\mathrm{mach}}=2^{-53}$ is half of the unit roundoff in the sense that if $x$ is a real number and $\\mathrm{fl}(x)$ denotes its rounded representation, then $\\mathrm{fl}(x)=x(1+\\delta)$ for some $|\\delta|\\le \\varepsilon_{\\mathrm{mach}}$ for one correctly rounded elementary operation without overflow/underflow). In the sequel, let $\\mathrm{fl}(\\cdot)$ denote the rounded result of a single floating-point operation, and let $\\log1p(z)$ denote the library function that evaluates $\\log(1+z)$ with high relative accuracy for $z$ near $0$.\n\nStarting strictly from the definitions of inverse transform sampling and the floating-point rounding model, reason about the numerical behavior of computing $X$ via $-\\log(1-U)$ and via alternatives that avoid forming $1-U$ explicitly. In particular, determine which of the following statements are correct. There may be more than one correct option.\n\nA. In IEEE 754 binary64 with rounding to nearest, if $u2^{-53}$ then $\\mathrm{fl}(1-u)=1$ exactly. Consequently, the naive inverse transform using $x=-\\log(\\mathrm{fl}(1-u))$ returns $x=0$ while the true value is $x=-\\log(1-u)\\approx u>0$. Using $x=-\\log1p(-u)$ avoids this loss of information because it does not form $1-u$ as an intermediate quantity.\n\nB. For $u$ close to $1$, the subtraction $1-u$ is well-conditioned and never loses relative accuracy, so $-\\log(1-u)$ is always as accurate as $-\\log1p(-u)$.\n\nC. Replacing $-\\log(1-u)$ by $-\\log(u)$ yields identically distributed samples and strictly avoids subtractive cancellation; however, to prevent evaluating $\\log(0)$ one must ensure $u\\in(0,1]$, for example by mapping an exact zero to the next representable number above $0$ before applying the logarithm.\n\nD. Adding a fixed positive constant $\\varepsilon$ to $u$ (for instance, $u\\leftarrow u+2^{-53}$) before evaluating $-\\log(1-u)$ is a sound way to repair cancellation without biasing the target distribution.\n\nE. For general inverse transforms of the form $F^{-1}(u)$ with differentiable $F$, the only place where rounding in $u$ can create catastrophic error is when $u\\approx 1$; values $u\\approx 0$ are numerically harmless for $-\\log(1-u)$.\n\nSelect all options that are correct and justify your choices using first principles regarding the inverse transform method, the behavior of $\\log(1-x)$ for small $x$, and the floating-point error model.", "solution": "The problem asks for an analysis of the numerical stability of generating exponential random variates using the inverse transform method.\n\nThe core of the problem lies in the computation of $X = -\\frac{\\log(1-U)}{\\lambda}$ where $U$ is a random variate from the $\\mathrm{Uniform}(0,1)$ distribution. The multiplicative factor $1/\\lambda$ is numerically stable (assuming $\\lambda$ is not near overflow or underflow limits), so we focus on the term $-\\log(1-U)$. Numerical issues in this computation depend on the value of $U$.\n\nThe two critical regimes for $U$ are $U \\to 0$ and $U \\to 1$.\n\nCase 1: $U = u$ where $u$ is close to $0$.\nThe mathematical expression is $x = -\\log(1-u)/\\lambda$. Using the Taylor series for $\\log(1-z)$ around $z=0$, which is $\\log(1-z) = -z - z^2/2 - \\dots$, we find that for small $u>0$, $x \\approx -(-u)/\\lambda = u/\\lambda$. The desired result is a small positive number.\nThe naive floating-point computation is $\\mathrm{fl}(-\\log(\\mathrm{fl}(1-u))/\\lambda)$. The first step is the subtraction $\\mathrm{fl}(1-u)$. According to the problem's description of IEEE 754 binary64 arithmetic, the unit roundoff is $\\varepsilon_{\\mathrm{mach}} = 2^{-53}$. The machine epsilon, which is the distance from $1$ to the next larger representable floating-point number, is $\\varepsilon = 2\\varepsilon_{\\mathrm{mach}} = 2^{-52}$.\nWhen a real number is rounded to the nearest floating-point number, any number in the interval $(1 - \\varepsilon/2, 1]$ will be rounded to $1$. This interval is $(1 - 2^{-53}, 1]$. If $u$ is a positive floating-point number such that $u  2^{-53}$, then $1-u$ falls into this interval, and thus $\\mathrm{fl}(1-u) = 1$.\nConsequently, for such small $u$, the computation yields $\\mathrm{fl}(-\\log(1)/\\lambda) = 0$. This is a catastrophic loss of precision, as the true value is approximately $u/\\lambda > 0$, but the computed result is exactly $0$. This type of error, where significant information is lost due to subtraction, is a form of catastrophic cancellation.\n\nCase 2: $U = u$ where $u$ is close to $1$.\nLet $u = 1-\\delta$ for some small $\\delta > 0$. The mathematical expression is $x = -\\log(1-(1-\\delta))/\\lambda = -\\log(\\delta)/\\lambda$. As $\\delta \\to 0^+$, $x \\to +\\infty$.\nThe computation involves the subtraction $1-u$. The condition number of the problem of computing $f(u)=1-u$ is $\\kappa(u) = |u f'(u) / f(u)| = |u(-1)/(1-u)| = |u/(1-u)|$. As $u \\to 1$, $\\kappa(u) \\to \\infty$. This means the problem is ill-conditioned: small relative errors in the input $u$ can be amplified into large relative errors in the output $1-u$. For example, the rounding error in representing $U$ as a float, even if just one $\\varepsilon_{\\mathrm{mach}}$, can lead to a very large relative error in the quantity $1-U$, which then propagates through the logarithm.\n\nNow we evaluate each option.\n\nA. In IEEE 754 binary64 with rounding to nearest, if $u2^{-53}$ then $\\mathrm{fl}(1-u)=1$ exactly. Consequently, the naive inverse transform using $x=-\\log(\\mathrm{fl}(1-u))$ returns $x=0$ while the true value is $x=-\\log(1-u)\\approx u>0$. Using $x=-\\log1p(-u)$ avoids this loss of information because it does not form $1-u$ as an intermediate quantity.\n\nThis statement is correct. As analyzed in Case 1 above, if $u$ is a positive number smaller than the unit roundoff $\\varepsilon_{\\mathrm{mach}} = 2^{-53}$, the subtraction $1-u$ when performed in floating-point arithmetic yields exactly $1$. This results in $\\log(1)=0$, completely losing the information contained in $u$. The true value $-\\log(1-u)$ is approximately $u$, so this is a failure of the naive algorithm. The function $\\log1p(z)$ is specifically designed to compute $\\log(1+z)$ accurately for small $|z|$. By rewriting $-\\log(1-u)$ as $-\\log1p(-u)$, we pass the small quantity $-u$ directly to this specialized function, which can use a numerically stable method (like a Taylor series approximation) to compute the result without performing the intermediate subtraction $1-u$ that leads to cancellation. This accurately reflects the purpose and function of `log1p`. **Correct**.\n\nB. For $u$ close to $1$, the subtraction $1-u$ is well-conditioned and never loses relative accuracy, so $-\\log(1-u)$ is always as accurate as $-\\log1p(-u)$.\n\nThis statement is incorrect. The first claim, that the subtraction $1-u$ is well-conditioned for $u$ close to $1$, is false. As shown in the analysis of Case 2, the condition number of computing $1-u$ is $|u/(1-u)|$, which approaches infinity as $u \\to 1$. An ill-conditioned problem is one where small relative input errors can cause large relative output errors. While the floating-point subtraction itself might be exact under certain conditions (e.g., Sterbenz's Lemma), the underlying problem is ill-conditioned, meaning any error in $u$ (e.g. from the original random number generation) will be greatly amplified. Since the premise of the statement is false, the statement is not sound. **Incorrect**.\n\nC. Replacing $-\\log(1-u)$ by $-\\log(u)$ yields identically distributed samples and strictly avoids subtractive cancellation; however, to prevent evaluating $\\log(0)$ one must ensure $u\\in(0,1]$, for example by mapping an exact zero to the next representable number above $0$ before applying the logarithm.\n\nThis statement is correct. If a random variable $U$ follows a $\\mathrm{Uniform}(0,1)$ distribution, then the random variable $V=1-U$ also follows a $\\mathrm{Uniform}(0,1)$ distribution. This is a fundamental property of the uniform distribution. Therefore, generating samples using $X = -\\log(U)/\\lambda$ is statistically equivalent to using $X = -\\log(1-U)/\\lambda$. The expression $-\\log(U)/\\lambda$ does not involve the subtraction $1-U$, thus completely avoiding the catastrophic cancellation issue that occurs when $U$ is near $0$ in the original formulation. This alternative method is numerically superior. The caveat is also correct: since a uniform random number generator might produce an exact $0$, one must handle the case $U=0$ to prevent an error from $\\log(0)$. This is a necessary practical consideration for robust implementation. **Correct**.\n\nD. Adding a fixed positive constant $\\varepsilon$ to $u$ (for instance, $u\\leftarrow u+2^{-53}$) before evaluating $-\\log(1-u)$ is a sound way to repair cancellation without biasing the target distribution.\n\nThis statement is incorrect. The proposed modification is $X' = -\\log(1-(U+\\varepsilon))/\\lambda$. If $U \\sim \\mathrm{Uniform}(0,1)$, then $U' = U+\\varepsilon$ is distributed as $\\mathrm{Uniform}(\\varepsilon, 1+\\varepsilon)$. Applying the inverse transform method with a random variable that is not $\\mathrm{Uniform}(0,1)$ will not produce samples from the original target distribution. The resulting samples $X'$ will have a different distribution from the desired exponential distribution. The claim that this can be done \"without biasing the target distribution\" is false. This method introduces a systematic error, or bias. **Incorrect**.\n\nE. For general inverse transforms of the form $F^{-1}(u)$ with differentiable $F$, the only place where rounding in $u$ can create catastrophic error is when $u\\approx 1$; values $u\\approx 0$ are numerically harmless for $-\\log(1-u)$.\n\nThis statement is incorrect. It makes two false claims. First, as established in the analysis of option A, values of $u \\approx 0$ are precisely where the naive computation of $-\\log(1-u)$ suffers from catastrophic cancellation, so this situation is not \"numerically harmless.\" Second, the generalization that error can only be created when $u \\approx 1$ is false. For a general distribution, the sensitivity of $F^{-1}(u)$ to changes in $u$ is given by the derivative $(F^{-1})'(u) = 1/f(F^{-1}(u))$, where $f$ is the probability density function. Large errors can occur wherever this derivative is large, which happens in the tails of the distribution where $f(x)$ is small. For many distributions, like the normal distribution, there are two tails, corresponding to $u \\to 0$ and $u \\to 1$. Therefore, numerical issues can arise at both ends of the $(0,1)$ interval, not just near $u \\approx 1$. **Incorrect**.", "answer": "$$\\boxed{AC}$$", "id": "3314503"}]}